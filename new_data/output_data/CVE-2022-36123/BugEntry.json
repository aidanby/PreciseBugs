{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n * This code is used on x86_64 to create page table identity mappings on\n * demand by building up a new set of page tables (or appending to the\n * existing ones), and then switching over to them when ready.\n *\n * Copyright (C) 2015-2016  Yinghai Lu\n * Copyright (C)      2016  Kees Cook\n */\n\n/*\n * Since we're dealing with identity mappings, physical and virtual\n * addresses are the same, so override these defines which are ultimately\n * used by the headers in misc.h.\n */\n#define __pa(x)  ((unsigned long)(x))\n#define __va(x)  ((void *)((unsigned long)(x)))\n\n/* No PAGE_TABLE_ISOLATION support needed either: */\n#undef CONFIG_PAGE_TABLE_ISOLATION\n\n#include \"error.h\"\n#include \"misc.h\"\n\n/* These actually do the work of building the kernel identity maps. */\n#include <linux/pgtable.h>\n#include <asm/cmpxchg.h>\n#include <asm/trap_pf.h>\n#include <asm/trapnr.h>\n#include <asm/init.h>\n/* Use the static base for this part of the boot process */\n#undef __PAGE_OFFSET\n#define __PAGE_OFFSET __PAGE_OFFSET_BASE\n#include \"../../mm/ident_map.c\"\n\n#define _SETUP\n#include <asm/setup.h>\t/* For COMMAND_LINE_SIZE */\n#undef _SETUP\n\nextern unsigned long get_cmd_line_ptr(void);\n\n/* Used by PAGE_KERN* macros: */\npteval_t __default_kernel_pte_mask __read_mostly = ~0;\n\n/* Used to track our page table allocation area. */\nstruct alloc_pgt_data {\n\tunsigned char *pgt_buf;\n\tunsigned long pgt_buf_size;\n\tunsigned long pgt_buf_offset;\n};\n\n/*\n * Allocates space for a page table entry, using struct alloc_pgt_data\n * above. Besides the local callers, this is used as the allocation\n * callback in mapping_info below.\n */\nstatic void *alloc_pgt_page(void *context)\n{\n\tstruct alloc_pgt_data *pages = (struct alloc_pgt_data *)context;\n\tunsigned char *entry;\n\n\t/* Validate there is space available for a new page. */\n\tif (pages->pgt_buf_offset >= pages->pgt_buf_size) {\n\t\tdebug_putstr(\"out of pgt_buf in \" __FILE__ \"!?\\n\");\n\t\tdebug_putaddr(pages->pgt_buf_offset);\n\t\tdebug_putaddr(pages->pgt_buf_size);\n\t\treturn NULL;\n\t}\n\n\tentry = pages->pgt_buf + pages->pgt_buf_offset;\n\tpages->pgt_buf_offset += PAGE_SIZE;\n\n\treturn entry;\n}\n\n/* Used to track our allocated page tables. */\nstatic struct alloc_pgt_data pgt_data;\n\n/* The top level page table entry pointer. */\nstatic unsigned long top_level_pgt;\n\nphys_addr_t physical_mask = (1ULL << __PHYSICAL_MASK_SHIFT) - 1;\n\n/*\n * Mapping information structure passed to kernel_ident_mapping_init().\n * Due to relocation, pointers must be assigned at run time not build time.\n */\nstatic struct x86_mapping_info mapping_info;\n\n/*\n * Adds the specified range to the identity mappings.\n */\nvoid kernel_add_identity_map(unsigned long start, unsigned long end)\n{\n\tint ret;\n\n\t/* Align boundary to 2M. */\n\tstart = round_down(start, PMD_SIZE);\n\tend = round_up(end, PMD_SIZE);\n\tif (start >= end)\n\t\treturn;\n\n\t/* Build the mapping. */\n\tret = kernel_ident_mapping_init(&mapping_info, (pgd_t *)top_level_pgt, start, end);\n\tif (ret)\n\t\terror(\"Error: kernel_ident_mapping_init() failed\\n\");\n}\n\n/* Locates and clears a region for a new top level page table. */\nvoid initialize_identity_maps(void *rmode)\n{\n\tunsigned long cmdline;\n\n\t/* Exclude the encryption mask from __PHYSICAL_MASK */\n\tphysical_mask &= ~sme_me_mask;\n\n\t/* Init mapping_info with run-time function/buffer pointers. */\n\tmapping_info.alloc_pgt_page = alloc_pgt_page;\n\tmapping_info.context = &pgt_data;\n\tmapping_info.page_flag = __PAGE_KERNEL_LARGE_EXEC | sme_me_mask;\n\tmapping_info.kernpg_flag = _KERNPG_TABLE;\n\n\t/*\n\t * It should be impossible for this not to already be true,\n\t * but since calling this a second time would rewind the other\n\t * counters, let's just make sure this is reset too.\n\t */\n\tpgt_data.pgt_buf_offset = 0;\n\n\t/*\n\t * If we came here via startup_32(), cr3 will be _pgtable already\n\t * and we must append to the existing area instead of entirely\n\t * overwriting it.\n\t *\n\t * With 5-level paging, we use '_pgtable' to allocate the p4d page table,\n\t * the top-level page table is allocated separately.\n\t *\n\t * p4d_offset(top_level_pgt, 0) would cover both the 4- and 5-level\n\t * cases. On 4-level paging it's equal to 'top_level_pgt'.\n\t */\n\ttop_level_pgt = read_cr3_pa();\n\tif (p4d_offset((pgd_t *)top_level_pgt, 0) == (p4d_t *)_pgtable) {\n\t\tpgt_data.pgt_buf = _pgtable + BOOT_INIT_PGT_SIZE;\n\t\tpgt_data.pgt_buf_size = BOOT_PGT_SIZE - BOOT_INIT_PGT_SIZE;\n\t\tmemset(pgt_data.pgt_buf, 0, pgt_data.pgt_buf_size);\n\t} else {\n\t\tpgt_data.pgt_buf = _pgtable;\n\t\tpgt_data.pgt_buf_size = BOOT_PGT_SIZE;\n\t\tmemset(pgt_data.pgt_buf, 0, pgt_data.pgt_buf_size);\n\t\ttop_level_pgt = (unsigned long)alloc_pgt_page(&pgt_data);\n\t}\n\n\t/*\n\t * New page-table is set up - map the kernel image, boot_params and the\n\t * command line. The uncompressed kernel requires boot_params and the\n\t * command line to be mapped in the identity mapping. Map them\n\t * explicitly here in case the compressed kernel does not touch them,\n\t * or does not touch all the pages covering them.\n\t */\n\tkernel_add_identity_map((unsigned long)_head, (unsigned long)_end);\n\tboot_params = rmode;\n\tkernel_add_identity_map((unsigned long)boot_params, (unsigned long)(boot_params + 1));\n\tcmdline = get_cmd_line_ptr();\n\tkernel_add_identity_map(cmdline, cmdline + COMMAND_LINE_SIZE);\n\n\tsev_prep_identity_maps(top_level_pgt);\n\n\t/* Load the new page-table. */\n\twrite_cr3(top_level_pgt);\n}\n\nstatic pte_t *split_large_pmd(struct x86_mapping_info *info,\n\t\t\t      pmd_t *pmdp, unsigned long __address)\n{\n\tunsigned long page_flags;\n\tunsigned long address;\n\tpte_t *pte;\n\tpmd_t pmd;\n\tint i;\n\n\tpte = (pte_t *)info->alloc_pgt_page(info->context);\n\tif (!pte)\n\t\treturn NULL;\n\n\taddress     = __address & PMD_MASK;\n\t/* No large page - clear PSE flag */\n\tpage_flags  = info->page_flag & ~_PAGE_PSE;\n\n\t/* Populate the PTEs */\n\tfor (i = 0; i < PTRS_PER_PMD; i++) {\n\t\tset_pte(&pte[i], __pte(address | page_flags));\n\t\taddress += PAGE_SIZE;\n\t}\n\n\t/*\n\t * Ideally we need to clear the large PMD first and do a TLB\n\t * flush before we write the new PMD. But the 2M range of the\n\t * PMD might contain the code we execute and/or the stack\n\t * we are on, so we can't do that. But that should be safe here\n\t * because we are going from large to small mappings and we are\n\t * also the only user of the page-table, so there is no chance\n\t * of a TLB multihit.\n\t */\n\tpmd = __pmd((unsigned long)pte | info->kernpg_flag);\n\tset_pmd(pmdp, pmd);\n\t/* Flush TLB to establish the new PMD */\n\twrite_cr3(top_level_pgt);\n\n\treturn pte + pte_index(__address);\n}\n\nstatic void clflush_page(unsigned long address)\n{\n\tunsigned int flush_size;\n\tchar *cl, *start, *end;\n\n\t/*\n\t * Hardcode cl-size to 64 - CPUID can't be used here because that might\n\t * cause another #VC exception and the GHCB is not ready to use yet.\n\t */\n\tflush_size = 64;\n\tstart      = (char *)(address & PAGE_MASK);\n\tend        = start + PAGE_SIZE;\n\n\t/*\n\t * First make sure there are no pending writes on the cache-lines to\n\t * flush.\n\t */\n\tasm volatile(\"mfence\" : : : \"memory\");\n\n\tfor (cl = start; cl != end; cl += flush_size)\n\t\tclflush(cl);\n}\n\nstatic int set_clr_page_flags(struct x86_mapping_info *info,\n\t\t\t      unsigned long address,\n\t\t\t      pteval_t set, pteval_t clr)\n{\n\tpgd_t *pgdp = (pgd_t *)top_level_pgt;\n\tp4d_t *p4dp;\n\tpud_t *pudp;\n\tpmd_t *pmdp;\n\tpte_t *ptep, pte;\n\n\t/*\n\t * First make sure there is a PMD mapping for 'address'.\n\t * It should already exist, but keep things generic.\n\t *\n\t * To map the page just read from it and fault it in if there is no\n\t * mapping yet. kernel_add_identity_map() can't be called here because\n\t * that would unconditionally map the address on PMD level, destroying\n\t * any PTE-level mappings that might already exist. Use assembly here\n\t * so the access won't be optimized away.\n\t */\n\tasm volatile(\"mov %[address], %%r9\"\n\t\t     :: [address] \"g\" (*(unsigned long *)address)\n\t\t     : \"r9\", \"memory\");\n\n\t/*\n\t * The page is mapped at least with PMD size - so skip checks and walk\n\t * directly to the PMD.\n\t */\n\tp4dp = p4d_offset(pgdp, address);\n\tpudp = pud_offset(p4dp, address);\n\tpmdp = pmd_offset(pudp, address);\n\n\tif (pmd_large(*pmdp))\n\t\tptep = split_large_pmd(info, pmdp, address);\n\telse\n\t\tptep = pte_offset_kernel(pmdp, address);\n\n\tif (!ptep)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Changing encryption attributes of a page requires to flush it from\n\t * the caches.\n\t */\n\tif ((set | clr) & _PAGE_ENC) {\n\t\tclflush_page(address);\n\n\t\t/*\n\t\t * If the encryption attribute is being cleared, change the page state\n\t\t * to shared in the RMP table.\n\t\t */\n\t\tif (clr)\n\t\t\tsnp_set_page_shared(__pa(address & PAGE_MASK));\n\t}\n\n\t/* Update PTE */\n\tpte = *ptep;\n\tpte = pte_set_flags(pte, set);\n\tpte = pte_clear_flags(pte, clr);\n\tset_pte(ptep, pte);\n\n\t/*\n\t * If the encryption attribute is being set, then change the page state to\n\t * private in the RMP entry. The page state change must be done after the PTE\n\t * is updated.\n\t */\n\tif (set & _PAGE_ENC)\n\t\tsnp_set_page_private(__pa(address & PAGE_MASK));\n\n\t/* Flush TLB after changing encryption attribute */\n\twrite_cr3(top_level_pgt);\n\n\treturn 0;\n}\n\nint set_page_decrypted(unsigned long address)\n{\n\treturn set_clr_page_flags(&mapping_info, address, 0, _PAGE_ENC);\n}\n\nint set_page_encrypted(unsigned long address)\n{\n\treturn set_clr_page_flags(&mapping_info, address, _PAGE_ENC, 0);\n}\n\nint set_page_non_present(unsigned long address)\n{\n\treturn set_clr_page_flags(&mapping_info, address, 0, _PAGE_PRESENT);\n}\n\nstatic void do_pf_error(const char *msg, unsigned long error_code,\n\t\t\tunsigned long address, unsigned long ip)\n{\n\terror_putstr(msg);\n\n\terror_putstr(\"\\nError Code: \");\n\terror_puthex(error_code);\n\terror_putstr(\"\\nCR2: 0x\");\n\terror_puthex(address);\n\terror_putstr(\"\\nRIP relative to _head: 0x\");\n\terror_puthex(ip - (unsigned long)_head);\n\terror_putstr(\"\\n\");\n\n\terror(\"Stopping.\\n\");\n}\n\nvoid do_boot_page_fault(struct pt_regs *regs, unsigned long error_code)\n{\n\tunsigned long address = native_read_cr2();\n\tunsigned long end;\n\tbool ghcb_fault;\n\n\tghcb_fault = sev_es_check_ghcb_fault(address);\n\n\taddress   &= PMD_MASK;\n\tend        = address + PMD_SIZE;\n\n\t/*\n\t * Check for unexpected error codes. Unexpected are:\n\t *\t- Faults on present pages\n\t *\t- User faults\n\t *\t- Reserved bits set\n\t */\n\tif (error_code & (X86_PF_PROT | X86_PF_USER | X86_PF_RSVD))\n\t\tdo_pf_error(\"Unexpected page-fault:\", error_code, address, regs->ip);\n\telse if (ghcb_fault)\n\t\tdo_pf_error(\"Page-fault on GHCB page:\", error_code, address, regs->ip);\n\n\t/*\n\t * Error code is sane - now identity map the 2M region around\n\t * the faulting address.\n\t */\n\tkernel_add_identity_map(address, end);\n}\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_X86_SETUP_H\n#define _ASM_X86_SETUP_H\n\n#include <uapi/asm/setup.h>\n\n#define COMMAND_LINE_SIZE 2048\n\n#include <linux/linkage.h>\n#include <asm/page_types.h>\n#include <asm/ibt.h>\n\n#ifdef __i386__\n\n#include <linux/pfn.h>\n/*\n * Reserved space for vmalloc and iomap - defined in asm/page.h\n */\n#define MAXMEM_PFN\tPFN_DOWN(MAXMEM)\n#define MAX_NONPAE_PFN\t(1 << 20)\n\n#endif /* __i386__ */\n\n#define PARAM_SIZE 4096\t\t/* sizeof(struct boot_params) */\n\n#define OLD_CL_MAGIC\t\t0xA33F\n#define OLD_CL_ADDRESS\t\t0x020\t/* Relative to real mode data */\n#define NEW_CL_POINTER\t\t0x228\t/* Relative to real mode data */\n\n#ifndef __ASSEMBLY__\n#include <asm/bootparam.h>\n#include <asm/x86_init.h>\n\nextern u64 relocated_ramdisk;\n\n/* Interrupt control for vSMPowered x86_64 systems */\n#ifdef CONFIG_X86_64\nvoid vsmp_init(void);\n#else\nstatic inline void vsmp_init(void) { }\n#endif\n\nstruct pt_regs;\n\nvoid setup_bios_corruption_check(void);\nvoid early_platform_quirks(void);\n\nextern unsigned long saved_video_mode;\n\nextern void reserve_standard_io_resources(void);\nextern void i386_reserve_resources(void);\nextern unsigned long __startup_64(unsigned long physaddr, struct boot_params *bp);\nextern void startup_64_setup_env(unsigned long physbase);\nextern void early_setup_idt(void);\nextern void __init do_early_exception(struct pt_regs *regs, int trapnr);\n\n#ifdef CONFIG_X86_INTEL_MID\nextern void x86_intel_mid_early_setup(void);\n#else\nstatic inline void x86_intel_mid_early_setup(void) { }\n#endif\n\n#ifdef CONFIG_X86_INTEL_CE\nextern void x86_ce4100_early_setup(void);\n#else\nstatic inline void x86_ce4100_early_setup(void) { }\n#endif\n\n#ifndef _SETUP\n\n#include <asm/espfix.h>\n#include <linux/kernel.h>\n\n/*\n * This is set up by the setup-routine at boot-time\n */\nextern struct boot_params boot_params;\nextern char _text[];\n\nstatic inline bool kaslr_enabled(void)\n{\n\treturn IS_ENABLED(CONFIG_RANDOMIZE_MEMORY) &&\n\t\t!!(boot_params.hdr.loadflags & KASLR_FLAG);\n}\n\n/*\n * Apply no randomization if KASLR was disabled at boot or if KASAN\n * is enabled. KASAN shadow mappings rely on regions being PGD aligned.\n */\nstatic inline bool kaslr_memory_enabled(void)\n{\n\treturn kaslr_enabled() && !IS_ENABLED(CONFIG_KASAN);\n}\n\nstatic inline unsigned long kaslr_offset(void)\n{\n\treturn (unsigned long)&_text - __START_KERNEL;\n}\n\n/*\n * Do NOT EVER look at the BIOS memory size location.\n * It does not work on many machines.\n */\n#define LOWMEMSIZE()\t(0x9f000)\n\n/* exceedingly early brk-like allocator */\nextern unsigned long _brk_end;\nvoid *extend_brk(size_t size, size_t align);\n\n/*\n * Reserve space in the .brk section, which is a block of memory from which the\n * caller is allowed to allocate very early (before even memblock is available)\n * by calling extend_brk().  All allocated memory will be eventually converted\n * to memblock.  Any leftover unallocated memory will be freed.\n *\n * The size is in bytes.\n */\n#define RESERVE_BRK(name, size)\t\t\t\t\t\\\n\t__section(\".bss..brk\") __aligned(1) __used\t\\\n\tstatic char __brk_##name[size]\n\nextern void probe_roms(void);\n#ifdef __i386__\n\nasmlinkage void __init i386_start_kernel(void);\n\n#else\nasmlinkage void __init x86_64_start_kernel(char *real_mode);\nasmlinkage void __init x86_64_start_reservations(char *real_mode_data);\n\n#endif /* __i386__ */\n#endif /* _SETUP */\n\n#else  /* __ASSEMBLY */\n\n.macro __RESERVE_BRK name, size\n\t.pushsection .bss..brk, \"aw\"\nSYM_DATA_START(__brk_\\name)\n\t.skip \\size\nSYM_DATA_END(__brk_\\name)\n\t.popsection\n.endm\n\n#define RESERVE_BRK(name, size) __RESERVE_BRK name, size\n\n#endif /* __ASSEMBLY__ */\n\n#endif /* _ASM_X86_SETUP_H */\n", "/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */\n#ifndef _ASM_X86_BOOTPARAM_H\n#define _ASM_X86_BOOTPARAM_H\n\n/* setup_data/setup_indirect types */\n#define SETUP_NONE\t\t\t0\n#define SETUP_E820_EXT\t\t\t1\n#define SETUP_DTB\t\t\t2\n#define SETUP_PCI\t\t\t3\n#define SETUP_EFI\t\t\t4\n#define SETUP_APPLE_PROPERTIES\t\t5\n#define SETUP_JAILHOUSE\t\t\t6\n#define SETUP_CC_BLOB\t\t\t7\n\n#define SETUP_INDIRECT\t\t\t(1<<31)\n\n/* SETUP_INDIRECT | max(SETUP_*) */\n#define SETUP_TYPE_MAX\t\t\t(SETUP_INDIRECT | SETUP_JAILHOUSE)\n\n/* ram_size flags */\n#define RAMDISK_IMAGE_START_MASK\t0x07FF\n#define RAMDISK_PROMPT_FLAG\t\t0x8000\n#define RAMDISK_LOAD_FLAG\t\t0x4000\n\n/* loadflags */\n#define LOADED_HIGH\t(1<<0)\n#define KASLR_FLAG\t(1<<1)\n#define QUIET_FLAG\t(1<<5)\n#define KEEP_SEGMENTS\t(1<<6)\n#define CAN_USE_HEAP\t(1<<7)\n\n/* xloadflags */\n#define XLF_KERNEL_64\t\t\t(1<<0)\n#define XLF_CAN_BE_LOADED_ABOVE_4G\t(1<<1)\n#define XLF_EFI_HANDOVER_32\t\t(1<<2)\n#define XLF_EFI_HANDOVER_64\t\t(1<<3)\n#define XLF_EFI_KEXEC\t\t\t(1<<4)\n#define XLF_5LEVEL\t\t\t(1<<5)\n#define XLF_5LEVEL_ENABLED\t\t(1<<6)\n\n#ifndef __ASSEMBLY__\n\n#include <linux/types.h>\n#include <linux/screen_info.h>\n#include <linux/apm_bios.h>\n#include <linux/edd.h>\n#include <asm/ist.h>\n#include <video/edid.h>\n\n/* extensible setup data list node */\nstruct setup_data {\n\t__u64 next;\n\t__u32 type;\n\t__u32 len;\n\t__u8 data[0];\n};\n\n/* extensible setup indirect data node */\nstruct setup_indirect {\n\t__u32 type;\n\t__u32 reserved;  /* Reserved, must be set to zero. */\n\t__u64 len;\n\t__u64 addr;\n};\n\nstruct setup_header {\n\t__u8\tsetup_sects;\n\t__u16\troot_flags;\n\t__u32\tsyssize;\n\t__u16\tram_size;\n\t__u16\tvid_mode;\n\t__u16\troot_dev;\n\t__u16\tboot_flag;\n\t__u16\tjump;\n\t__u32\theader;\n\t__u16\tversion;\n\t__u32\trealmode_swtch;\n\t__u16\tstart_sys_seg;\n\t__u16\tkernel_version;\n\t__u8\ttype_of_loader;\n\t__u8\tloadflags;\n\t__u16\tsetup_move_size;\n\t__u32\tcode32_start;\n\t__u32\tramdisk_image;\n\t__u32\tramdisk_size;\n\t__u32\tbootsect_kludge;\n\t__u16\theap_end_ptr;\n\t__u8\text_loader_ver;\n\t__u8\text_loader_type;\n\t__u32\tcmd_line_ptr;\n\t__u32\tinitrd_addr_max;\n\t__u32\tkernel_alignment;\n\t__u8\trelocatable_kernel;\n\t__u8\tmin_alignment;\n\t__u16\txloadflags;\n\t__u32\tcmdline_size;\n\t__u32\thardware_subarch;\n\t__u64\thardware_subarch_data;\n\t__u32\tpayload_offset;\n\t__u32\tpayload_length;\n\t__u64\tsetup_data;\n\t__u64\tpref_address;\n\t__u32\tinit_size;\n\t__u32\thandover_offset;\n\t__u32\tkernel_info_offset;\n} __attribute__((packed));\n\nstruct sys_desc_table {\n\t__u16 length;\n\t__u8  table[14];\n};\n\n/* Gleaned from OFW's set-parameters in cpu/x86/pc/linux.fth */\nstruct olpc_ofw_header {\n\t__u32 ofw_magic;\t/* OFW signature */\n\t__u32 ofw_version;\n\t__u32 cif_handler;\t/* callback into OFW */\n\t__u32 irq_desc_table;\n} __attribute__((packed));\n\nstruct efi_info {\n\t__u32 efi_loader_signature;\n\t__u32 efi_systab;\n\t__u32 efi_memdesc_size;\n\t__u32 efi_memdesc_version;\n\t__u32 efi_memmap;\n\t__u32 efi_memmap_size;\n\t__u32 efi_systab_hi;\n\t__u32 efi_memmap_hi;\n};\n\n/*\n * This is the maximum number of entries in struct boot_params::e820_table\n * (the zeropage), which is part of the x86 boot protocol ABI:\n */\n#define E820_MAX_ENTRIES_ZEROPAGE 128\n\n/*\n * The E820 memory region entry of the boot protocol ABI:\n */\nstruct boot_e820_entry {\n\t__u64 addr;\n\t__u64 size;\n\t__u32 type;\n} __attribute__((packed));\n\n/*\n * Smallest compatible version of jailhouse_setup_data required by this kernel.\n */\n#define JAILHOUSE_SETUP_REQUIRED_VERSION\t1\n\n/*\n * The boot loader is passing platform information via this Jailhouse-specific\n * setup data structure.\n */\nstruct jailhouse_setup_data {\n\tstruct {\n\t\t__u16\tversion;\n\t\t__u16\tcompatible_version;\n\t} __attribute__((packed)) hdr;\n\tstruct {\n\t\t__u16\tpm_timer_address;\n\t\t__u16\tnum_cpus;\n\t\t__u64\tpci_mmconfig_base;\n\t\t__u32\ttsc_khz;\n\t\t__u32\tapic_khz;\n\t\t__u8\tstandard_ioapic;\n\t\t__u8\tcpu_ids[255];\n\t} __attribute__((packed)) v1;\n\tstruct {\n\t\t__u32\tflags;\n\t} __attribute__((packed)) v2;\n} __attribute__((packed));\n\n/* The so-called \"zeropage\" */\nstruct boot_params {\n\tstruct screen_info screen_info;\t\t\t/* 0x000 */\n\tstruct apm_bios_info apm_bios_info;\t\t/* 0x040 */\n\t__u8  _pad2[4];\t\t\t\t\t/* 0x054 */\n\t__u64  tboot_addr;\t\t\t\t/* 0x058 */\n\tstruct ist_info ist_info;\t\t\t/* 0x060 */\n\t__u64 acpi_rsdp_addr;\t\t\t\t/* 0x070 */\n\t__u8  _pad3[8];\t\t\t\t\t/* 0x078 */\n\t__u8  hd0_info[16];\t/* obsolete! */\t\t/* 0x080 */\n\t__u8  hd1_info[16];\t/* obsolete! */\t\t/* 0x090 */\n\tstruct sys_desc_table sys_desc_table; /* obsolete! */\t/* 0x0a0 */\n\tstruct olpc_ofw_header olpc_ofw_header;\t\t/* 0x0b0 */\n\t__u32 ext_ramdisk_image;\t\t\t/* 0x0c0 */\n\t__u32 ext_ramdisk_size;\t\t\t\t/* 0x0c4 */\n\t__u32 ext_cmd_line_ptr;\t\t\t\t/* 0x0c8 */\n\t__u8  _pad4[112];\t\t\t\t/* 0x0cc */\n\t__u32 cc_blob_address;\t\t\t\t/* 0x13c */\n\tstruct edid_info edid_info;\t\t\t/* 0x140 */\n\tstruct efi_info efi_info;\t\t\t/* 0x1c0 */\n\t__u32 alt_mem_k;\t\t\t\t/* 0x1e0 */\n\t__u32 scratch;\t\t/* Scratch field! */\t/* 0x1e4 */\n\t__u8  e820_entries;\t\t\t\t/* 0x1e8 */\n\t__u8  eddbuf_entries;\t\t\t\t/* 0x1e9 */\n\t__u8  edd_mbr_sig_buf_entries;\t\t\t/* 0x1ea */\n\t__u8  kbd_status;\t\t\t\t/* 0x1eb */\n\t__u8  secure_boot;\t\t\t\t/* 0x1ec */\n\t__u8  _pad5[2];\t\t\t\t\t/* 0x1ed */\n\t/*\n\t * The sentinel is set to a nonzero value (0xff) in header.S.\n\t *\n\t * A bootloader is supposed to only take setup_header and put\n\t * it into a clean boot_params buffer. If it turns out that\n\t * it is clumsy or too generous with the buffer, it most\n\t * probably will pick up the sentinel variable too. The fact\n\t * that this variable then is still 0xff will let kernel\n\t * know that some variables in boot_params are invalid and\n\t * kernel should zero out certain portions of boot_params.\n\t */\n\t__u8  sentinel;\t\t\t\t\t/* 0x1ef */\n\t__u8  _pad6[1];\t\t\t\t\t/* 0x1f0 */\n\tstruct setup_header hdr;    /* setup header */\t/* 0x1f1 */\n\t__u8  _pad7[0x290-0x1f1-sizeof(struct setup_header)];\n\t__u32 edd_mbr_sig_buffer[EDD_MBR_SIG_MAX];\t/* 0x290 */\n\tstruct boot_e820_entry e820_table[E820_MAX_ENTRIES_ZEROPAGE]; /* 0x2d0 */\n\t__u8  _pad8[48];\t\t\t\t/* 0xcd0 */\n\tstruct edd_info eddbuf[EDDMAXNR];\t\t/* 0xd00 */\n\t__u8  _pad9[276];\t\t\t\t/* 0xeec */\n} __attribute__((packed));\n\n/**\n * enum x86_hardware_subarch - x86 hardware subarchitecture\n *\n * The x86 hardware_subarch and hardware_subarch_data were added as of the x86\n * boot protocol 2.07 to help distinguish and support custom x86 boot\n * sequences. This enum represents accepted values for the x86\n * hardware_subarch.  Custom x86 boot sequences (not X86_SUBARCH_PC) do not\n * have or simply *cannot* make use of natural stubs like BIOS or EFI, the\n * hardware_subarch can be used on the Linux entry path to revector to a\n * subarchitecture stub when needed. This subarchitecture stub can be used to\n * set up Linux boot parameters or for special care to account for nonstandard\n * handling of page tables.\n *\n * These enums should only ever be used by x86 code, and the code that uses\n * it should be well contained and compartmentalized.\n *\n * KVM and Xen HVM do not have a subarch as these are expected to follow\n * standard x86 boot entries. If there is a genuine need for \"hypervisor\" type\n * that should be considered separately in the future. Future guest types\n * should seriously consider working with standard x86 boot stubs such as\n * the BIOS or EFI boot stubs.\n *\n * WARNING: this enum is only used for legacy hacks, for platform features that\n *\t    are not easily enumerated or discoverable. You should not ever use\n *\t    this for new features.\n *\n * @X86_SUBARCH_PC: Should be used if the hardware is enumerable using standard\n *\tPC mechanisms (PCI, ACPI) and doesn't need a special boot flow.\n * @X86_SUBARCH_LGUEST: Used for x86 hypervisor demo, lguest, deprecated\n * @X86_SUBARCH_XEN: Used for Xen guest types which follow the PV boot path,\n * \twhich start at asm startup_xen() entry point and later jump to the C\n * \txen_start_kernel() entry point. Both domU and dom0 type of guests are\n * \tcurrently supported through this PV boot path.\n * @X86_SUBARCH_INTEL_MID: Used for Intel MID (Mobile Internet Device) platform\n *\tsystems which do not have the PCI legacy interfaces.\n * @X86_SUBARCH_CE4100: Used for Intel CE media processor (CE4100) SoC\n * \tfor settop boxes and media devices, the use of a subarch for CE4100\n * \tis more of a hack...\n */\nenum x86_hardware_subarch {\n\tX86_SUBARCH_PC = 0,\n\tX86_SUBARCH_LGUEST,\n\tX86_SUBARCH_XEN,\n\tX86_SUBARCH_INTEL_MID,\n\tX86_SUBARCH_CE4100,\n\tX86_NR_SUBARCHS,\n};\n\n#endif /* __ASSEMBLY__ */\n\n#endif /* _ASM_X86_BOOTPARAM_H */\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n *  prepare to run common code\n *\n *  Copyright (C) 2000 Andrea Arcangeli <andrea@suse.de> SuSE\n */\n\n#define DISABLE_BRANCH_PROFILING\n\n/* cpu_feature_enabled() cannot be used this early */\n#define USE_EARLY_PGTABLE_L5\n\n#include <linux/init.h>\n#include <linux/linkage.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/percpu.h>\n#include <linux/start_kernel.h>\n#include <linux/io.h>\n#include <linux/memblock.h>\n#include <linux/cc_platform.h>\n#include <linux/pgtable.h>\n\n#include <asm/processor.h>\n#include <asm/proto.h>\n#include <asm/smp.h>\n#include <asm/setup.h>\n#include <asm/desc.h>\n#include <asm/tlbflush.h>\n#include <asm/sections.h>\n#include <asm/kdebug.h>\n#include <asm/e820/api.h>\n#include <asm/bios_ebda.h>\n#include <asm/bootparam_utils.h>\n#include <asm/microcode.h>\n#include <asm/kasan.h>\n#include <asm/fixmap.h>\n#include <asm/realmode.h>\n#include <asm/extable.h>\n#include <asm/trapnr.h>\n#include <asm/sev.h>\n#include <asm/tdx.h>\n\n/*\n * Manage page tables very early on.\n */\nextern pmd_t early_dynamic_pgts[EARLY_DYNAMIC_PAGE_TABLES][PTRS_PER_PMD];\nstatic unsigned int __initdata next_early_pgt;\npmdval_t early_pmd_flags = __PAGE_KERNEL_LARGE & ~(_PAGE_GLOBAL | _PAGE_NX);\n\n#ifdef CONFIG_X86_5LEVEL\nunsigned int __pgtable_l5_enabled __ro_after_init;\nunsigned int pgdir_shift __ro_after_init = 39;\nEXPORT_SYMBOL(pgdir_shift);\nunsigned int ptrs_per_p4d __ro_after_init = 1;\nEXPORT_SYMBOL(ptrs_per_p4d);\n#endif\n\n#ifdef CONFIG_DYNAMIC_MEMORY_LAYOUT\nunsigned long page_offset_base __ro_after_init = __PAGE_OFFSET_BASE_L4;\nEXPORT_SYMBOL(page_offset_base);\nunsigned long vmalloc_base __ro_after_init = __VMALLOC_BASE_L4;\nEXPORT_SYMBOL(vmalloc_base);\nunsigned long vmemmap_base __ro_after_init = __VMEMMAP_BASE_L4;\nEXPORT_SYMBOL(vmemmap_base);\n#endif\n\n/*\n * GDT used on the boot CPU before switching to virtual addresses.\n */\nstatic struct desc_struct startup_gdt[GDT_ENTRIES] = {\n\t[GDT_ENTRY_KERNEL32_CS]         = GDT_ENTRY_INIT(0xc09b, 0, 0xfffff),\n\t[GDT_ENTRY_KERNEL_CS]           = GDT_ENTRY_INIT(0xa09b, 0, 0xfffff),\n\t[GDT_ENTRY_KERNEL_DS]           = GDT_ENTRY_INIT(0xc093, 0, 0xfffff),\n};\n\n/*\n * Address needs to be set at runtime because it references the startup_gdt\n * while the kernel still uses a direct mapping.\n */\nstatic struct desc_ptr startup_gdt_descr = {\n\t.size = sizeof(startup_gdt),\n\t.address = 0,\n};\n\n#define __head\t__section(\".head.text\")\n\nstatic void __head *fixup_pointer(void *ptr, unsigned long physaddr)\n{\n\treturn ptr - (void *)_text + (void *)physaddr;\n}\n\nstatic unsigned long __head *fixup_long(void *ptr, unsigned long physaddr)\n{\n\treturn fixup_pointer(ptr, physaddr);\n}\n\n#ifdef CONFIG_X86_5LEVEL\nstatic unsigned int __head *fixup_int(void *ptr, unsigned long physaddr)\n{\n\treturn fixup_pointer(ptr, physaddr);\n}\n\nstatic bool __head check_la57_support(unsigned long physaddr)\n{\n\t/*\n\t * 5-level paging is detected and enabled at kernel decompression\n\t * stage. Only check if it has been enabled there.\n\t */\n\tif (!(native_read_cr4() & X86_CR4_LA57))\n\t\treturn false;\n\n\t*fixup_int(&__pgtable_l5_enabled, physaddr) = 1;\n\t*fixup_int(&pgdir_shift, physaddr) = 48;\n\t*fixup_int(&ptrs_per_p4d, physaddr) = 512;\n\t*fixup_long(&page_offset_base, physaddr) = __PAGE_OFFSET_BASE_L5;\n\t*fixup_long(&vmalloc_base, physaddr) = __VMALLOC_BASE_L5;\n\t*fixup_long(&vmemmap_base, physaddr) = __VMEMMAP_BASE_L5;\n\n\treturn true;\n}\n#else\nstatic bool __head check_la57_support(unsigned long physaddr)\n{\n\treturn false;\n}\n#endif\n\nstatic unsigned long __head sme_postprocess_startup(struct boot_params *bp, pmdval_t *pmd)\n{\n\tunsigned long vaddr, vaddr_end;\n\tint i;\n\n\t/* Encrypt the kernel and related (if SME is active) */\n\tsme_encrypt_kernel(bp);\n\n\t/*\n\t * Clear the memory encryption mask from the .bss..decrypted section.\n\t * The bss section will be memset to zero later in the initialization so\n\t * there is no need to zero it after changing the memory encryption\n\t * attribute.\n\t */\n\tif (sme_get_me_mask()) {\n\t\tvaddr = (unsigned long)__start_bss_decrypted;\n\t\tvaddr_end = (unsigned long)__end_bss_decrypted;\n\n\t\tfor (; vaddr < vaddr_end; vaddr += PMD_SIZE) {\n\t\t\t/*\n\t\t\t * On SNP, transition the page to shared in the RMP table so that\n\t\t\t * it is consistent with the page table attribute change.\n\t\t\t *\n\t\t\t * __start_bss_decrypted has a virtual address in the high range\n\t\t\t * mapping (kernel .text). PVALIDATE, by way of\n\t\t\t * early_snp_set_memory_shared(), requires a valid virtual\n\t\t\t * address but the kernel is currently running off of the identity\n\t\t\t * mapping so use __pa() to get a *currently* valid virtual address.\n\t\t\t */\n\t\t\tearly_snp_set_memory_shared(__pa(vaddr), __pa(vaddr), PTRS_PER_PMD);\n\n\t\t\ti = pmd_index(vaddr);\n\t\t\tpmd[i] -= sme_get_me_mask();\n\t\t}\n\t}\n\n\t/*\n\t * Return the SME encryption mask (if SME is active) to be used as a\n\t * modifier for the initial pgdir entry programmed into CR3.\n\t */\n\treturn sme_get_me_mask();\n}\n\n/* Code in __startup_64() can be relocated during execution, but the compiler\n * doesn't have to generate PC-relative relocations when accessing globals from\n * that function. Clang actually does not generate them, which leads to\n * boot-time crashes. To work around this problem, every global pointer must\n * be adjusted using fixup_pointer().\n */\nunsigned long __head __startup_64(unsigned long physaddr,\n\t\t\t\t  struct boot_params *bp)\n{\n\tunsigned long load_delta, *p;\n\tunsigned long pgtable_flags;\n\tpgdval_t *pgd;\n\tp4dval_t *p4d;\n\tpudval_t *pud;\n\tpmdval_t *pmd, pmd_entry;\n\tpteval_t *mask_ptr;\n\tbool la57;\n\tint i;\n\tunsigned int *next_pgt_ptr;\n\n\tla57 = check_la57_support(physaddr);\n\n\t/* Is the address too large? */\n\tif (physaddr >> MAX_PHYSMEM_BITS)\n\t\tfor (;;);\n\n\t/*\n\t * Compute the delta between the address I am compiled to run at\n\t * and the address I am actually running at.\n\t */\n\tload_delta = physaddr - (unsigned long)(_text - __START_KERNEL_map);\n\n\t/* Is the address not 2M aligned? */\n\tif (load_delta & ~PMD_PAGE_MASK)\n\t\tfor (;;);\n\n\t/* Include the SME encryption mask in the fixup value */\n\tload_delta += sme_get_me_mask();\n\n\t/* Fixup the physical addresses in the page table */\n\n\tpgd = fixup_pointer(&early_top_pgt, physaddr);\n\tp = pgd + pgd_index(__START_KERNEL_map);\n\tif (la57)\n\t\t*p = (unsigned long)level4_kernel_pgt;\n\telse\n\t\t*p = (unsigned long)level3_kernel_pgt;\n\t*p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;\n\n\tif (la57) {\n\t\tp4d = fixup_pointer(&level4_kernel_pgt, physaddr);\n\t\tp4d[511] += load_delta;\n\t}\n\n\tpud = fixup_pointer(&level3_kernel_pgt, physaddr);\n\tpud[510] += load_delta;\n\tpud[511] += load_delta;\n\n\tpmd = fixup_pointer(level2_fixmap_pgt, physaddr);\n\tfor (i = FIXMAP_PMD_TOP; i > FIXMAP_PMD_TOP - FIXMAP_PMD_NUM; i--)\n\t\tpmd[i] += load_delta;\n\n\t/*\n\t * Set up the identity mapping for the switchover.  These\n\t * entries should *NOT* have the global bit set!  This also\n\t * creates a bunch of nonsense entries but that is fine --\n\t * it avoids problems around wraparound.\n\t */\n\n\tnext_pgt_ptr = fixup_pointer(&next_early_pgt, physaddr);\n\tpud = fixup_pointer(early_dynamic_pgts[(*next_pgt_ptr)++], physaddr);\n\tpmd = fixup_pointer(early_dynamic_pgts[(*next_pgt_ptr)++], physaddr);\n\n\tpgtable_flags = _KERNPG_TABLE_NOENC + sme_get_me_mask();\n\n\tif (la57) {\n\t\tp4d = fixup_pointer(early_dynamic_pgts[(*next_pgt_ptr)++],\n\t\t\t\t    physaddr);\n\n\t\ti = (physaddr >> PGDIR_SHIFT) % PTRS_PER_PGD;\n\t\tpgd[i + 0] = (pgdval_t)p4d + pgtable_flags;\n\t\tpgd[i + 1] = (pgdval_t)p4d + pgtable_flags;\n\n\t\ti = physaddr >> P4D_SHIFT;\n\t\tp4d[(i + 0) % PTRS_PER_P4D] = (pgdval_t)pud + pgtable_flags;\n\t\tp4d[(i + 1) % PTRS_PER_P4D] = (pgdval_t)pud + pgtable_flags;\n\t} else {\n\t\ti = (physaddr >> PGDIR_SHIFT) % PTRS_PER_PGD;\n\t\tpgd[i + 0] = (pgdval_t)pud + pgtable_flags;\n\t\tpgd[i + 1] = (pgdval_t)pud + pgtable_flags;\n\t}\n\n\ti = physaddr >> PUD_SHIFT;\n\tpud[(i + 0) % PTRS_PER_PUD] = (pudval_t)pmd + pgtable_flags;\n\tpud[(i + 1) % PTRS_PER_PUD] = (pudval_t)pmd + pgtable_flags;\n\n\tpmd_entry = __PAGE_KERNEL_LARGE_EXEC & ~_PAGE_GLOBAL;\n\t/* Filter out unsupported __PAGE_KERNEL_* bits: */\n\tmask_ptr = fixup_pointer(&__supported_pte_mask, physaddr);\n\tpmd_entry &= *mask_ptr;\n\tpmd_entry += sme_get_me_mask();\n\tpmd_entry +=  physaddr;\n\n\tfor (i = 0; i < DIV_ROUND_UP(_end - _text, PMD_SIZE); i++) {\n\t\tint idx = i + (physaddr >> PMD_SHIFT);\n\n\t\tpmd[idx % PTRS_PER_PMD] = pmd_entry + i * PMD_SIZE;\n\t}\n\n\t/*\n\t * Fixup the kernel text+data virtual addresses. Note that\n\t * we might write invalid pmds, when the kernel is relocated\n\t * cleanup_highmap() fixes this up along with the mappings\n\t * beyond _end.\n\t *\n\t * Only the region occupied by the kernel image has so far\n\t * been checked against the table of usable memory regions\n\t * provided by the firmware, so invalidate pages outside that\n\t * region. A page table entry that maps to a reserved area of\n\t * memory would allow processor speculation into that area,\n\t * and on some hardware (particularly the UV platform) even\n\t * speculative access to some reserved areas is caught as an\n\t * error, causing the BIOS to halt the system.\n\t */\n\n\tpmd = fixup_pointer(level2_kernel_pgt, physaddr);\n\n\t/* invalidate pages before the kernel image */\n\tfor (i = 0; i < pmd_index((unsigned long)_text); i++)\n\t\tpmd[i] &= ~_PAGE_PRESENT;\n\n\t/* fixup pages that are part of the kernel image */\n\tfor (; i <= pmd_index((unsigned long)_end); i++)\n\t\tif (pmd[i] & _PAGE_PRESENT)\n\t\t\tpmd[i] += load_delta;\n\n\t/* invalidate pages after the kernel image */\n\tfor (; i < PTRS_PER_PMD; i++)\n\t\tpmd[i] &= ~_PAGE_PRESENT;\n\n\t/*\n\t * Fixup phys_base - remove the memory encryption mask to obtain\n\t * the true physical address.\n\t */\n\t*fixup_long(&phys_base, physaddr) += load_delta - sme_get_me_mask();\n\n\treturn sme_postprocess_startup(bp, pmd);\n}\n\n/* Wipe all early page tables except for the kernel symbol map */\nstatic void __init reset_early_page_tables(void)\n{\n\tmemset(early_top_pgt, 0, sizeof(pgd_t)*(PTRS_PER_PGD-1));\n\tnext_early_pgt = 0;\n\twrite_cr3(__sme_pa_nodebug(early_top_pgt));\n}\n\n/* Create a new PMD entry */\nbool __init __early_make_pgtable(unsigned long address, pmdval_t pmd)\n{\n\tunsigned long physaddr = address - __PAGE_OFFSET;\n\tpgdval_t pgd, *pgd_p;\n\tp4dval_t p4d, *p4d_p;\n\tpudval_t pud, *pud_p;\n\tpmdval_t *pmd_p;\n\n\t/* Invalid address or early pgt is done ?  */\n\tif (physaddr >= MAXMEM || read_cr3_pa() != __pa_nodebug(early_top_pgt))\n\t\treturn false;\n\nagain:\n\tpgd_p = &early_top_pgt[pgd_index(address)].pgd;\n\tpgd = *pgd_p;\n\n\t/*\n\t * The use of __START_KERNEL_map rather than __PAGE_OFFSET here is\n\t * critical -- __PAGE_OFFSET would point us back into the dynamic\n\t * range and we might end up looping forever...\n\t */\n\tif (!pgtable_l5_enabled())\n\t\tp4d_p = pgd_p;\n\telse if (pgd)\n\t\tp4d_p = (p4dval_t *)((pgd & PTE_PFN_MASK) + __START_KERNEL_map - phys_base);\n\telse {\n\t\tif (next_early_pgt >= EARLY_DYNAMIC_PAGE_TABLES) {\n\t\t\treset_early_page_tables();\n\t\t\tgoto again;\n\t\t}\n\n\t\tp4d_p = (p4dval_t *)early_dynamic_pgts[next_early_pgt++];\n\t\tmemset(p4d_p, 0, sizeof(*p4d_p) * PTRS_PER_P4D);\n\t\t*pgd_p = (pgdval_t)p4d_p - __START_KERNEL_map + phys_base + _KERNPG_TABLE;\n\t}\n\tp4d_p += p4d_index(address);\n\tp4d = *p4d_p;\n\n\tif (p4d)\n\t\tpud_p = (pudval_t *)((p4d & PTE_PFN_MASK) + __START_KERNEL_map - phys_base);\n\telse {\n\t\tif (next_early_pgt >= EARLY_DYNAMIC_PAGE_TABLES) {\n\t\t\treset_early_page_tables();\n\t\t\tgoto again;\n\t\t}\n\n\t\tpud_p = (pudval_t *)early_dynamic_pgts[next_early_pgt++];\n\t\tmemset(pud_p, 0, sizeof(*pud_p) * PTRS_PER_PUD);\n\t\t*p4d_p = (p4dval_t)pud_p - __START_KERNEL_map + phys_base + _KERNPG_TABLE;\n\t}\n\tpud_p += pud_index(address);\n\tpud = *pud_p;\n\n\tif (pud)\n\t\tpmd_p = (pmdval_t *)((pud & PTE_PFN_MASK) + __START_KERNEL_map - phys_base);\n\telse {\n\t\tif (next_early_pgt >= EARLY_DYNAMIC_PAGE_TABLES) {\n\t\t\treset_early_page_tables();\n\t\t\tgoto again;\n\t\t}\n\n\t\tpmd_p = (pmdval_t *)early_dynamic_pgts[next_early_pgt++];\n\t\tmemset(pmd_p, 0, sizeof(*pmd_p) * PTRS_PER_PMD);\n\t\t*pud_p = (pudval_t)pmd_p - __START_KERNEL_map + phys_base + _KERNPG_TABLE;\n\t}\n\tpmd_p[pmd_index(address)] = pmd;\n\n\treturn true;\n}\n\nstatic bool __init early_make_pgtable(unsigned long address)\n{\n\tunsigned long physaddr = address - __PAGE_OFFSET;\n\tpmdval_t pmd;\n\n\tpmd = (physaddr & PMD_MASK) + early_pmd_flags;\n\n\treturn __early_make_pgtable(address, pmd);\n}\n\nvoid __init do_early_exception(struct pt_regs *regs, int trapnr)\n{\n\tif (trapnr == X86_TRAP_PF &&\n\t    early_make_pgtable(native_read_cr2()))\n\t\treturn;\n\n\tif (IS_ENABLED(CONFIG_AMD_MEM_ENCRYPT) &&\n\t    trapnr == X86_TRAP_VC && handle_vc_boot_ghcb(regs))\n\t\treturn;\n\n\tif (trapnr == X86_TRAP_VE && tdx_early_handle_ve(regs))\n\t\treturn;\n\n\tearly_fixup_exception(regs, trapnr);\n}\n\n/* Don't add a printk in there. printk relies on the PDA which is not initialized \n   yet. */\nstatic void __init clear_bss(void)\n{\n\tmemset(__bss_start, 0,\n\t       (unsigned long) __bss_stop - (unsigned long) __bss_start);\n}\n\nstatic unsigned long get_cmd_line_ptr(void)\n{\n\tunsigned long cmd_line_ptr = boot_params.hdr.cmd_line_ptr;\n\n\tcmd_line_ptr |= (u64)boot_params.ext_cmd_line_ptr << 32;\n\n\treturn cmd_line_ptr;\n}\n\nstatic void __init copy_bootdata(char *real_mode_data)\n{\n\tchar * command_line;\n\tunsigned long cmd_line_ptr;\n\n\t/*\n\t * If SME is active, this will create decrypted mappings of the\n\t * boot data in advance of the copy operations.\n\t */\n\tsme_map_bootdata(real_mode_data);\n\n\tmemcpy(&boot_params, real_mode_data, sizeof(boot_params));\n\tsanitize_boot_params(&boot_params);\n\tcmd_line_ptr = get_cmd_line_ptr();\n\tif (cmd_line_ptr) {\n\t\tcommand_line = __va(cmd_line_ptr);\n\t\tmemcpy(boot_command_line, command_line, COMMAND_LINE_SIZE);\n\t}\n\n\t/*\n\t * The old boot data is no longer needed and won't be reserved,\n\t * freeing up that memory for use by the system. If SME is active,\n\t * we need to remove the mappings that were created so that the\n\t * memory doesn't remain mapped as decrypted.\n\t */\n\tsme_unmap_bootdata(real_mode_data);\n}\n\nasmlinkage __visible void __init x86_64_start_kernel(char * real_mode_data)\n{\n\t/*\n\t * Build-time sanity checks on the kernel image and module\n\t * area mappings. (these are purely build-time and produce no code)\n\t */\n\tBUILD_BUG_ON(MODULES_VADDR < __START_KERNEL_map);\n\tBUILD_BUG_ON(MODULES_VADDR - __START_KERNEL_map < KERNEL_IMAGE_SIZE);\n\tBUILD_BUG_ON(MODULES_LEN + KERNEL_IMAGE_SIZE > 2*PUD_SIZE);\n\tBUILD_BUG_ON((__START_KERNEL_map & ~PMD_MASK) != 0);\n\tBUILD_BUG_ON((MODULES_VADDR & ~PMD_MASK) != 0);\n\tBUILD_BUG_ON(!(MODULES_VADDR > __START_KERNEL));\n\tMAYBE_BUILD_BUG_ON(!(((MODULES_END - 1) & PGDIR_MASK) ==\n\t\t\t\t(__START_KERNEL & PGDIR_MASK)));\n\tBUILD_BUG_ON(__fix_to_virt(__end_of_fixed_addresses) <= MODULES_END);\n\n\tcr4_init_shadow();\n\n\t/* Kill off the identity-map trampoline */\n\treset_early_page_tables();\n\n\tclear_bss();\n\n\t/*\n\t * This needs to happen *before* kasan_early_init() because latter maps stuff\n\t * into that page.\n\t */\n\tclear_page(init_top_pgt);\n\n\t/*\n\t * SME support may update early_pmd_flags to include the memory\n\t * encryption mask, so it needs to be called before anything\n\t * that may generate a page fault.\n\t */\n\tsme_early_init();\n\n\tkasan_early_init();\n\n\t/*\n\t * Flush global TLB entries which could be left over from the trampoline page\n\t * table.\n\t *\n\t * This needs to happen *after* kasan_early_init() as KASAN-enabled .configs\n\t * instrument native_write_cr4() so KASAN must be initialized for that\n\t * instrumentation to work.\n\t */\n\t__native_tlb_flush_global(this_cpu_read(cpu_tlbstate.cr4));\n\n\tidt_setup_early_handler();\n\n\t/* Needed before cc_platform_has() can be used for TDX */\n\ttdx_early_init();\n\n\tcopy_bootdata(__va(real_mode_data));\n\n\t/*\n\t * Load microcode early on BSP.\n\t */\n\tload_ucode_bsp();\n\n\t/* set init_top_pgt kernel high mapping*/\n\tinit_top_pgt[511] = early_top_pgt[511];\n\n\tx86_64_start_reservations(real_mode_data);\n}\n\nvoid __init x86_64_start_reservations(char *real_mode_data)\n{\n\t/* version is always not zero if it is copied */\n\tif (!boot_params.hdr.version)\n\t\tcopy_bootdata(__va(real_mode_data));\n\n\tx86_early_init_platform_quirks();\n\n\tswitch (boot_params.hdr.hardware_subarch) {\n\tcase X86_SUBARCH_INTEL_MID:\n\t\tx86_intel_mid_early_setup();\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tstart_kernel();\n}\n\n/*\n * Data structures and code used for IDT setup in head_64.S. The bringup-IDT is\n * used until the idt_table takes over. On the boot CPU this happens in\n * x86_64_start_kernel(), on secondary CPUs in start_secondary(). In both cases\n * this happens in the functions called from head_64.S.\n *\n * The idt_table can't be used that early because all the code modifying it is\n * in idt.c and can be instrumented by tracing or KASAN, which both don't work\n * during early CPU bringup. Also the idt_table has the runtime vectors\n * configured which require certain CPU state to be setup already (like TSS),\n * which also hasn't happened yet in early CPU bringup.\n */\nstatic gate_desc bringup_idt_table[NUM_EXCEPTION_VECTORS] __page_aligned_data;\n\nstatic struct desc_ptr bringup_idt_descr = {\n\t.size\t\t= (NUM_EXCEPTION_VECTORS * sizeof(gate_desc)) - 1,\n\t.address\t= 0, /* Set at runtime */\n};\n\nstatic void set_bringup_idt_handler(gate_desc *idt, int n, void *handler)\n{\n#ifdef CONFIG_AMD_MEM_ENCRYPT\n\tstruct idt_data data;\n\tgate_desc desc;\n\n\tinit_idt_data(&data, n, handler);\n\tidt_init_desc(&desc, &data);\n\tnative_write_idt_entry(idt, n, &desc);\n#endif\n}\n\n/* This runs while still in the direct mapping */\nstatic void startup_64_load_idt(unsigned long physbase)\n{\n\tstruct desc_ptr *desc = fixup_pointer(&bringup_idt_descr, physbase);\n\tgate_desc *idt = fixup_pointer(bringup_idt_table, physbase);\n\n\n\tif (IS_ENABLED(CONFIG_AMD_MEM_ENCRYPT)) {\n\t\tvoid *handler;\n\n\t\t/* VMM Communication Exception */\n\t\thandler = fixup_pointer(vc_no_ghcb, physbase);\n\t\tset_bringup_idt_handler(idt, X86_TRAP_VC, handler);\n\t}\n\n\tdesc->address = (unsigned long)idt;\n\tnative_load_idt(desc);\n}\n\n/* This is used when running on kernel addresses */\nvoid early_setup_idt(void)\n{\n\t/* VMM Communication Exception */\n\tif (IS_ENABLED(CONFIG_AMD_MEM_ENCRYPT)) {\n\t\tsetup_ghcb();\n\t\tset_bringup_idt_handler(bringup_idt_table, X86_TRAP_VC, vc_boot_ghcb);\n\t}\n\n\tbringup_idt_descr.address = (unsigned long)bringup_idt_table;\n\tnative_load_idt(&bringup_idt_descr);\n}\n\n/*\n * Setup boot CPU state needed before kernel switches to virtual addresses.\n */\nvoid __head startup_64_setup_env(unsigned long physbase)\n{\n\t/* Load GDT */\n\tstartup_gdt_descr.address = (unsigned long)fixup_pointer(startup_gdt, physbase);\n\tnative_load_gdt(&startup_gdt_descr);\n\n\t/* New GDT is live - reload data segment registers */\n\tasm volatile(\"movl %%eax, %%ds\\n\"\n\t\t     \"movl %%eax, %%ss\\n\"\n\t\t     \"movl %%eax, %%es\\n\" : : \"a\"(__KERNEL_DS) : \"memory\");\n\n\tstartup_64_load_idt(physbase);\n}\n", "/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * ld script for the x86 kernel\n *\n * Historic 32-bit version written by Martin Mares <mj@atrey.karlin.mff.cuni.cz>\n *\n * Modernisation, unification and other changes and fixes:\n *   Copyright (C) 2007-2009  Sam Ravnborg <sam@ravnborg.org>\n *\n *\n * Don't define absolute symbols until and unless you know that symbol\n * value is should remain constant even if kernel image is relocated\n * at run time. Absolute symbols are not relocated. If symbol value should\n * change if kernel is relocated, make the symbol section relative and\n * put it inside the section definition.\n */\n\n#ifdef CONFIG_X86_32\n#define LOAD_OFFSET __PAGE_OFFSET\n#else\n#define LOAD_OFFSET __START_KERNEL_map\n#endif\n\n#define RUNTIME_DISCARD_EXIT\n#define EMITS_PT_NOTE\n#define RO_EXCEPTION_TABLE_ALIGN\t16\n\n#include <asm-generic/vmlinux.lds.h>\n#include <asm/asm-offsets.h>\n#include <asm/thread_info.h>\n#include <asm/page_types.h>\n#include <asm/orc_lookup.h>\n#include <asm/cache.h>\n#include <asm/boot.h>\n\n#undef i386     /* in case the preprocessor is a 32bit one */\n\nOUTPUT_FORMAT(CONFIG_OUTPUT_FORMAT)\n\n#ifdef CONFIG_X86_32\nOUTPUT_ARCH(i386)\nENTRY(phys_startup_32)\n#else\nOUTPUT_ARCH(i386:x86-64)\nENTRY(phys_startup_64)\n#endif\n\njiffies = jiffies_64;\n\n#if defined(CONFIG_X86_64)\n/*\n * On 64-bit, align RODATA to 2MB so we retain large page mappings for\n * boundaries spanning kernel text, rodata and data sections.\n *\n * However, kernel identity mappings will have different RWX permissions\n * to the pages mapping to text and to the pages padding (which are freed) the\n * text section. Hence kernel identity mappings will be broken to smaller\n * pages. For 64-bit, kernel text and kernel identity mappings are different,\n * so we can enable protection checks as well as retain 2MB large page\n * mappings for kernel text.\n */\n#define X86_ALIGN_RODATA_BEGIN\t. = ALIGN(HPAGE_SIZE);\n\n#define X86_ALIGN_RODATA_END\t\t\t\t\t\\\n\t\t. = ALIGN(HPAGE_SIZE);\t\t\t\t\\\n\t\t__end_rodata_hpage_align = .;\t\t\t\\\n\t\t__end_rodata_aligned = .;\n\n#define ALIGN_ENTRY_TEXT_BEGIN\t. = ALIGN(PMD_SIZE);\n#define ALIGN_ENTRY_TEXT_END\t. = ALIGN(PMD_SIZE);\n\n/*\n * This section contains data which will be mapped as decrypted. Memory\n * encryption operates on a page basis. Make this section PMD-aligned\n * to avoid splitting the pages while mapping the section early.\n *\n * Note: We use a separate section so that only this section gets\n * decrypted to avoid exposing more than we wish.\n */\n#define BSS_DECRYPTED\t\t\t\t\t\t\\\n\t. = ALIGN(PMD_SIZE);\t\t\t\t\t\\\n\t__start_bss_decrypted = .;\t\t\t\t\\\n\t*(.bss..decrypted);\t\t\t\t\t\\\n\t. = ALIGN(PAGE_SIZE);\t\t\t\t\t\\\n\t__start_bss_decrypted_unused = .;\t\t\t\\\n\t. = ALIGN(PMD_SIZE);\t\t\t\t\t\\\n\t__end_bss_decrypted = .;\t\t\t\t\\\n\n#else\n\n#define X86_ALIGN_RODATA_BEGIN\n#define X86_ALIGN_RODATA_END\t\t\t\t\t\\\n\t\t. = ALIGN(PAGE_SIZE);\t\t\t\t\\\n\t\t__end_rodata_aligned = .;\n\n#define ALIGN_ENTRY_TEXT_BEGIN\n#define ALIGN_ENTRY_TEXT_END\n#define BSS_DECRYPTED\n\n#endif\n\nPHDRS {\n\ttext PT_LOAD FLAGS(5);          /* R_E */\n\tdata PT_LOAD FLAGS(6);          /* RW_ */\n#ifdef CONFIG_X86_64\n#ifdef CONFIG_SMP\n\tpercpu PT_LOAD FLAGS(6);        /* RW_ */\n#endif\n\tinit PT_LOAD FLAGS(7);          /* RWE */\n#endif\n\tnote PT_NOTE FLAGS(0);          /* ___ */\n}\n\nSECTIONS\n{\n#ifdef CONFIG_X86_32\n\t. = LOAD_OFFSET + LOAD_PHYSICAL_ADDR;\n\tphys_startup_32 = ABSOLUTE(startup_32 - LOAD_OFFSET);\n#else\n\t. = __START_KERNEL;\n\tphys_startup_64 = ABSOLUTE(startup_64 - LOAD_OFFSET);\n#endif\n\n\t/* Text and read-only data */\n\t.text :  AT(ADDR(.text) - LOAD_OFFSET) {\n\t\t_text = .;\n\t\t_stext = .;\n\t\t/* bootstrapping code */\n\t\tHEAD_TEXT\n\t\tTEXT_TEXT\n\t\tSCHED_TEXT\n\t\tCPUIDLE_TEXT\n\t\tLOCK_TEXT\n\t\tKPROBES_TEXT\n\t\tALIGN_ENTRY_TEXT_BEGIN\n\t\tENTRY_TEXT\n\t\tALIGN_ENTRY_TEXT_END\n\t\tSOFTIRQENTRY_TEXT\n\t\tSTATIC_CALL_TEXT\n\t\t*(.gnu.warning)\n\n#ifdef CONFIG_RETPOLINE\n\t\t__indirect_thunk_start = .;\n\t\t*(.text.__x86.indirect_thunk)\n\t\t__indirect_thunk_end = .;\n#endif\n\t} :text =0xcccc\n\n\t/* End of text section, which should occupy whole number of pages */\n\t_etext = .;\n\t. = ALIGN(PAGE_SIZE);\n\n\tX86_ALIGN_RODATA_BEGIN\n\tRO_DATA(PAGE_SIZE)\n\tX86_ALIGN_RODATA_END\n\n\t/* Data */\n\t.data : AT(ADDR(.data) - LOAD_OFFSET) {\n\t\t/* Start of data section */\n\t\t_sdata = .;\n\n\t\t/* init_task */\n\t\tINIT_TASK_DATA(THREAD_SIZE)\n\n#ifdef CONFIG_X86_32\n\t\t/* 32 bit has nosave before _edata */\n\t\tNOSAVE_DATA\n#endif\n\n\t\tPAGE_ALIGNED_DATA(PAGE_SIZE)\n\n\t\tCACHELINE_ALIGNED_DATA(L1_CACHE_BYTES)\n\n\t\tDATA_DATA\n\t\tCONSTRUCTORS\n\n\t\t/* rarely changed data like cpu maps */\n\t\tREAD_MOSTLY_DATA(INTERNODE_CACHE_BYTES)\n\n\t\t/* End of data section */\n\t\t_edata = .;\n\t} :data\n\n\tBUG_TABLE\n\n\tORC_UNWIND_TABLE\n\n\t. = ALIGN(PAGE_SIZE);\n\t__vvar_page = .;\n\n\t.vvar : AT(ADDR(.vvar) - LOAD_OFFSET) {\n\t\t/* work around gold bug 13023 */\n\t\t__vvar_beginning_hack = .;\n\n\t\t/* Place all vvars at the offsets in asm/vvar.h. */\n#define EMIT_VVAR(name, offset)\t\t\t\t\\\n\t\t. = __vvar_beginning_hack + offset;\t\\\n\t\t*(.vvar_ ## name)\n#include <asm/vvar.h>\n#undef EMIT_VVAR\n\n\t\t/*\n\t\t * Pad the rest of the page with zeros.  Otherwise the loader\n\t\t * can leave garbage here.\n\t\t */\n\t\t. = __vvar_beginning_hack + PAGE_SIZE;\n\t} :data\n\n\t. = ALIGN(__vvar_page + PAGE_SIZE, PAGE_SIZE);\n\n\t/* Init code and data - will be freed after init */\n\t. = ALIGN(PAGE_SIZE);\n\t.init.begin : AT(ADDR(.init.begin) - LOAD_OFFSET) {\n\t\t__init_begin = .; /* paired with __init_end */\n\t}\n\n#if defined(CONFIG_X86_64) && defined(CONFIG_SMP)\n\t/*\n\t * percpu offsets are zero-based on SMP.  PERCPU_VADDR() changes the\n\t * output PHDR, so the next output section - .init.text - should\n\t * start another segment - init.\n\t */\n\tPERCPU_VADDR(INTERNODE_CACHE_BYTES, 0, :percpu)\n\tASSERT(SIZEOF(.data..percpu) < CONFIG_PHYSICAL_START,\n\t       \"per-CPU data too large - increase CONFIG_PHYSICAL_START\")\n#endif\n\n\tINIT_TEXT_SECTION(PAGE_SIZE)\n#ifdef CONFIG_X86_64\n\t:init\n#endif\n\n\t/*\n\t * Section for code used exclusively before alternatives are run. All\n\t * references to such code must be patched out by alternatives, normally\n\t * by using X86_FEATURE_ALWAYS CPU feature bit.\n\t *\n\t * See static_cpu_has() for an example.\n\t */\n\t.altinstr_aux : AT(ADDR(.altinstr_aux) - LOAD_OFFSET) {\n\t\t*(.altinstr_aux)\n\t}\n\n\tINIT_DATA_SECTION(16)\n\n\t.x86_cpu_dev.init : AT(ADDR(.x86_cpu_dev.init) - LOAD_OFFSET) {\n\t\t__x86_cpu_dev_start = .;\n\t\t*(.x86_cpu_dev.init)\n\t\t__x86_cpu_dev_end = .;\n\t}\n\n#ifdef CONFIG_X86_INTEL_MID\n\t.x86_intel_mid_dev.init : AT(ADDR(.x86_intel_mid_dev.init) - \\\n\t\t\t\t\t\t\t\tLOAD_OFFSET) {\n\t\t__x86_intel_mid_dev_start = .;\n\t\t*(.x86_intel_mid_dev.init)\n\t\t__x86_intel_mid_dev_end = .;\n\t}\n#endif\n\n\t/*\n\t * start address and size of operations which during runtime\n\t * can be patched with virtualization friendly instructions or\n\t * baremetal native ones. Think page table operations.\n\t * Details in paravirt_types.h\n\t */\n\t. = ALIGN(8);\n\t.parainstructions : AT(ADDR(.parainstructions) - LOAD_OFFSET) {\n\t\t__parainstructions = .;\n\t\t*(.parainstructions)\n\t\t__parainstructions_end = .;\n\t}\n\n#ifdef CONFIG_RETPOLINE\n\t/*\n\t * List of instructions that call/jmp/jcc to retpoline thunks\n\t * __x86_indirect_thunk_*(). These instructions can be patched along\n\t * with alternatives, after which the section can be freed.\n\t */\n\t. = ALIGN(8);\n\t.retpoline_sites : AT(ADDR(.retpoline_sites) - LOAD_OFFSET) {\n\t\t__retpoline_sites = .;\n\t\t*(.retpoline_sites)\n\t\t__retpoline_sites_end = .;\n\t}\n#endif\n\n#ifdef CONFIG_X86_KERNEL_IBT\n\t. = ALIGN(8);\n\t.ibt_endbr_seal : AT(ADDR(.ibt_endbr_seal) - LOAD_OFFSET) {\n\t\t__ibt_endbr_seal = .;\n\t\t*(.ibt_endbr_seal)\n\t\t__ibt_endbr_seal_end = .;\n\t}\n#endif\n\n\t/*\n\t * struct alt_inst entries. From the header (alternative.h):\n\t * \"Alternative instructions for different CPU types or capabilities\"\n\t * Think locking instructions on spinlocks.\n\t */\n\t. = ALIGN(8);\n\t.altinstructions : AT(ADDR(.altinstructions) - LOAD_OFFSET) {\n\t\t__alt_instructions = .;\n\t\t*(.altinstructions)\n\t\t__alt_instructions_end = .;\n\t}\n\n\t/*\n\t * And here are the replacement instructions. The linker sticks\n\t * them as binary blobs. The .altinstructions has enough data to\n\t * get the address and the length of them to patch the kernel safely.\n\t */\n\t.altinstr_replacement : AT(ADDR(.altinstr_replacement) - LOAD_OFFSET) {\n\t\t*(.altinstr_replacement)\n\t}\n\n\t. = ALIGN(8);\n\t.apicdrivers : AT(ADDR(.apicdrivers) - LOAD_OFFSET) {\n\t\t__apicdrivers = .;\n\t\t*(.apicdrivers);\n\t\t__apicdrivers_end = .;\n\t}\n\n\t. = ALIGN(8);\n\t/*\n\t * .exit.text is discarded at runtime, not link time, to deal with\n\t *  references from .altinstructions\n\t */\n\t.exit.text : AT(ADDR(.exit.text) - LOAD_OFFSET) {\n\t\tEXIT_TEXT\n\t}\n\n\t.exit.data : AT(ADDR(.exit.data) - LOAD_OFFSET) {\n\t\tEXIT_DATA\n\t}\n\n#if !defined(CONFIG_X86_64) || !defined(CONFIG_SMP)\n\tPERCPU_SECTION(INTERNODE_CACHE_BYTES)\n#endif\n\n\t. = ALIGN(PAGE_SIZE);\n\n\t/* freed after init ends here */\n\t.init.end : AT(ADDR(.init.end) - LOAD_OFFSET) {\n\t\t__init_end = .;\n\t}\n\n\t/*\n\t * smp_locks might be freed after init\n\t * start/end must be page aligned\n\t */\n\t. = ALIGN(PAGE_SIZE);\n\t.smp_locks : AT(ADDR(.smp_locks) - LOAD_OFFSET) {\n\t\t__smp_locks = .;\n\t\t*(.smp_locks)\n\t\t. = ALIGN(PAGE_SIZE);\n\t\t__smp_locks_end = .;\n\t}\n\n#ifdef CONFIG_X86_64\n\t.data_nosave : AT(ADDR(.data_nosave) - LOAD_OFFSET) {\n\t\tNOSAVE_DATA\n\t}\n#endif\n\n\t/* BSS */\n\t. = ALIGN(PAGE_SIZE);\n\t.bss : AT(ADDR(.bss) - LOAD_OFFSET) {\n\t\t__bss_start = .;\n\t\t*(.bss..page_aligned)\n\t\t. = ALIGN(PAGE_SIZE);\n\t\t*(BSS_MAIN)\n\t\tBSS_DECRYPTED\n\t\t. = ALIGN(PAGE_SIZE);\n\t\t__bss_stop = .;\n\t}\n\n\t/*\n\t * The memory occupied from _text to here, __end_of_kernel_reserve, is\n\t * automatically reserved in setup_arch(). Anything after here must be\n\t * explicitly reserved using memblock_reserve() or it will be discarded\n\t * and treated as available memory.\n\t */\n\t__end_of_kernel_reserve = .;\n\n\t. = ALIGN(PAGE_SIZE);\n\t.brk (NOLOAD) : AT(ADDR(.brk) - LOAD_OFFSET) {\n\t\t__brk_base = .;\n\t\t. += 64 * 1024;\t\t/* 64k alignment slop space */\n\t\t*(.bss..brk)\t\t/* areas brk users have reserved */\n\t\t__brk_limit = .;\n\t}\n\n\t. = ALIGN(PAGE_SIZE);\t\t/* keep VO_INIT_SIZE page aligned */\n\t_end = .;\n\n#ifdef CONFIG_AMD_MEM_ENCRYPT\n\t/*\n\t * Early scratch/workarea section: Lives outside of the kernel proper\n\t * (_text - _end).\n\t *\n\t * Resides after _end because even though the .brk section is after\n\t * __end_of_kernel_reserve, the .brk section is later reserved as a\n\t * part of the kernel. Since it is located after __end_of_kernel_reserve\n\t * it will be discarded and become part of the available memory. As\n\t * such, it can only be used by very early boot code and must not be\n\t * needed afterwards.\n\t *\n\t * Currently used by SME for performing in-place encryption of the\n\t * kernel during boot. Resides on a 2MB boundary to simplify the\n\t * pagetable setup used for SME in-place encryption.\n\t */\n\t. = ALIGN(HPAGE_SIZE);\n\t.init.scratch : AT(ADDR(.init.scratch) - LOAD_OFFSET) {\n\t\t__init_scratch_begin = .;\n\t\t*(.init.scratch)\n\t\t. = ALIGN(HPAGE_SIZE);\n\t\t__init_scratch_end = .;\n\t}\n#endif\n\n\tSTABS_DEBUG\n\tDWARF_DEBUG\n\tELF_DETAILS\n\n\tDISCARDS\n\n\t/*\n\t * Make sure that the .got.plt is either completely empty or it\n\t * contains only the lazy dispatch entries.\n\t */\n\t.got.plt (INFO) : { *(.got.plt) }\n\tASSERT(SIZEOF(.got.plt) == 0 ||\n#ifdef CONFIG_X86_64\n\t       SIZEOF(.got.plt) == 0x18,\n#else\n\t       SIZEOF(.got.plt) == 0xc,\n#endif\n\t       \"Unexpected GOT/PLT entries detected!\")\n\n\t/*\n\t * Sections that should stay zero sized, which is safer to\n\t * explicitly check instead of blindly discarding.\n\t */\n\t.got : {\n\t\t*(.got) *(.igot.*)\n\t}\n\tASSERT(SIZEOF(.got) == 0, \"Unexpected GOT entries detected!\")\n\n\t.plt : {\n\t\t*(.plt) *(.plt.*) *(.iplt)\n\t}\n\tASSERT(SIZEOF(.plt) == 0, \"Unexpected run-time procedure linkages detected!\")\n\n\t.rel.dyn : {\n\t\t*(.rel.*) *(.rel_*)\n\t}\n\tASSERT(SIZEOF(.rel.dyn) == 0, \"Unexpected run-time relocations (.rel) detected!\")\n\n\t.rela.dyn : {\n\t\t*(.rela.*) *(.rela_*)\n\t}\n\tASSERT(SIZEOF(.rela.dyn) == 0, \"Unexpected run-time relocations (.rela) detected!\")\n}\n\n/*\n * The ASSERT() sink to . is intentional, for binutils 2.14 compatibility:\n */\n. = ASSERT((_end - LOAD_OFFSET <= KERNEL_IMAGE_SIZE),\n\t   \"kernel image bigger than KERNEL_IMAGE_SIZE\");\n\n#ifdef CONFIG_X86_64\n/*\n * Per-cpu symbols which need to be offset from __per_cpu_load\n * for the boot processor.\n */\n#define INIT_PER_CPU(x) init_per_cpu__##x = ABSOLUTE(x) + __per_cpu_load\nINIT_PER_CPU(gdt_page);\nINIT_PER_CPU(fixed_percpu_data);\nINIT_PER_CPU(irq_stack_backing_store);\n\n#ifdef CONFIG_SMP\n. = ASSERT((fixed_percpu_data == 0),\n           \"fixed_percpu_data is not at start of per-cpu area\");\n#endif\n\n#endif /* CONFIG_X86_64 */\n\n#ifdef CONFIG_KEXEC_CORE\n#include <asm/kexec.h>\n\n. = ASSERT(kexec_control_code_size <= KEXEC_CONTROL_CODE_MAX_SIZE,\n           \"kexec control code size is too big\");\n#endif\n\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * Core of Xen paravirt_ops implementation.\n *\n * This file contains the xen_paravirt_ops structure itself, and the\n * implementations for:\n * - privileged instructions\n * - interrupt flags\n * - segment operations\n * - booting and setup\n *\n * Jeremy Fitzhardinge <jeremy@xensource.com>, XenSource Inc, 2007\n */\n\n#include <linux/cpu.h>\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/smp.h>\n#include <linux/preempt.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/delay.h>\n#include <linux/start_kernel.h>\n#include <linux/sched.h>\n#include <linux/kprobes.h>\n#include <linux/memblock.h>\n#include <linux/export.h>\n#include <linux/mm.h>\n#include <linux/page-flags.h>\n#include <linux/pci.h>\n#include <linux/gfp.h>\n#include <linux/edd.h>\n#include <linux/reboot.h>\n\n#include <xen/xen.h>\n#include <xen/events.h>\n#include <xen/interface/xen.h>\n#include <xen/interface/version.h>\n#include <xen/interface/physdev.h>\n#include <xen/interface/vcpu.h>\n#include <xen/interface/memory.h>\n#include <xen/interface/nmi.h>\n#include <xen/interface/xen-mca.h>\n#include <xen/features.h>\n#include <xen/page.h>\n#include <xen/hvc-console.h>\n#include <xen/acpi.h>\n\n#include <asm/paravirt.h>\n#include <asm/apic.h>\n#include <asm/page.h>\n#include <asm/xen/pci.h>\n#include <asm/xen/hypercall.h>\n#include <asm/xen/hypervisor.h>\n#include <asm/xen/cpuid.h>\n#include <asm/fixmap.h>\n#include <asm/processor.h>\n#include <asm/proto.h>\n#include <asm/msr-index.h>\n#include <asm/traps.h>\n#include <asm/setup.h>\n#include <asm/desc.h>\n#include <asm/pgalloc.h>\n#include <asm/tlbflush.h>\n#include <asm/reboot.h>\n#include <asm/stackprotector.h>\n#include <asm/hypervisor.h>\n#include <asm/mach_traps.h>\n#include <asm/mwait.h>\n#include <asm/pci_x86.h>\n#include <asm/cpu.h>\n#ifdef CONFIG_X86_IOPL_IOPERM\n#include <asm/io_bitmap.h>\n#endif\n\n#ifdef CONFIG_ACPI\n#include <linux/acpi.h>\n#include <asm/acpi.h>\n#include <acpi/pdc_intel.h>\n#include <acpi/processor.h>\n#include <xen/interface/platform.h>\n#endif\n\n#include \"xen-ops.h\"\n#include \"mmu.h\"\n#include \"smp.h\"\n#include \"multicalls.h\"\n#include \"pmu.h\"\n\n#include \"../kernel/cpu/cpu.h\" /* get_cpu_cap() */\n\nvoid *xen_initial_gdt;\n\nstatic int xen_cpu_up_prepare_pv(unsigned int cpu);\nstatic int xen_cpu_dead_pv(unsigned int cpu);\n\nstruct tls_descs {\n\tstruct desc_struct desc[3];\n};\n\n/*\n * Updating the 3 TLS descriptors in the GDT on every task switch is\n * surprisingly expensive so we avoid updating them if they haven't\n * changed.  Since Xen writes different descriptors than the one\n * passed in the update_descriptor hypercall we keep shadow copies to\n * compare against.\n */\nstatic DEFINE_PER_CPU(struct tls_descs, shadow_tls_desc);\n\nstatic void __init xen_pv_init_platform(void)\n{\n\txen_set_restricted_virtio_memory_access();\n\n\tpopulate_extra_pte(fix_to_virt(FIX_PARAVIRT_BOOTMAP));\n\n\tset_fixmap(FIX_PARAVIRT_BOOTMAP, xen_start_info->shared_info);\n\tHYPERVISOR_shared_info = (void *)fix_to_virt(FIX_PARAVIRT_BOOTMAP);\n\n\t/* xen clock uses per-cpu vcpu_info, need to init it for boot cpu */\n\txen_vcpu_info_reset(0);\n\n\t/* pvclock is in shared info area */\n\txen_init_time_ops();\n}\n\nstatic void __init xen_pv_guest_late_init(void)\n{\n#ifndef CONFIG_SMP\n\t/* Setup shared vcpu info for non-smp configurations */\n\txen_setup_vcpu_info_placement();\n#endif\n}\n\nstatic __read_mostly unsigned int cpuid_leaf5_ecx_val;\nstatic __read_mostly unsigned int cpuid_leaf5_edx_val;\n\nstatic void xen_cpuid(unsigned int *ax, unsigned int *bx,\n\t\t      unsigned int *cx, unsigned int *dx)\n{\n\tunsigned maskebx = ~0;\n\n\t/*\n\t * Mask out inconvenient features, to try and disable as many\n\t * unsupported kernel subsystems as possible.\n\t */\n\tswitch (*ax) {\n\tcase CPUID_MWAIT_LEAF:\n\t\t/* Synthesize the values.. */\n\t\t*ax = 0;\n\t\t*bx = 0;\n\t\t*cx = cpuid_leaf5_ecx_val;\n\t\t*dx = cpuid_leaf5_edx_val;\n\t\treturn;\n\n\tcase 0xb:\n\t\t/* Suppress extended topology stuff */\n\t\tmaskebx = 0;\n\t\tbreak;\n\t}\n\n\tasm(XEN_EMULATE_PREFIX \"cpuid\"\n\t\t: \"=a\" (*ax),\n\t\t  \"=b\" (*bx),\n\t\t  \"=c\" (*cx),\n\t\t  \"=d\" (*dx)\n\t\t: \"0\" (*ax), \"2\" (*cx));\n\n\t*bx &= maskebx;\n}\n\nstatic bool __init xen_check_mwait(void)\n{\n#ifdef CONFIG_ACPI\n\tstruct xen_platform_op op = {\n\t\t.cmd\t\t\t= XENPF_set_processor_pminfo,\n\t\t.u.set_pminfo.id\t= -1,\n\t\t.u.set_pminfo.type\t= XEN_PM_PDC,\n\t};\n\tuint32_t buf[3];\n\tunsigned int ax, bx, cx, dx;\n\tunsigned int mwait_mask;\n\n\t/* We need to determine whether it is OK to expose the MWAIT\n\t * capability to the kernel to harvest deeper than C3 states from ACPI\n\t * _CST using the processor_harvest_xen.c module. For this to work, we\n\t * need to gather the MWAIT_LEAF values (which the cstate.c code\n\t * checks against). The hypervisor won't expose the MWAIT flag because\n\t * it would break backwards compatibility; so we will find out directly\n\t * from the hardware and hypercall.\n\t */\n\tif (!xen_initial_domain())\n\t\treturn false;\n\n\t/*\n\t * When running under platform earlier than Xen4.2, do not expose\n\t * mwait, to avoid the risk of loading native acpi pad driver\n\t */\n\tif (!xen_running_on_version_or_later(4, 2))\n\t\treturn false;\n\n\tax = 1;\n\tcx = 0;\n\n\tnative_cpuid(&ax, &bx, &cx, &dx);\n\n\tmwait_mask = (1 << (X86_FEATURE_EST % 32)) |\n\t\t     (1 << (X86_FEATURE_MWAIT % 32));\n\n\tif ((cx & mwait_mask) != mwait_mask)\n\t\treturn false;\n\n\t/* We need to emulate the MWAIT_LEAF and for that we need both\n\t * ecx and edx. The hypercall provides only partial information.\n\t */\n\n\tax = CPUID_MWAIT_LEAF;\n\tbx = 0;\n\tcx = 0;\n\tdx = 0;\n\n\tnative_cpuid(&ax, &bx, &cx, &dx);\n\n\t/* Ask the Hypervisor whether to clear ACPI_PDC_C_C2C3_FFH. If so,\n\t * don't expose MWAIT_LEAF and let ACPI pick the IOPORT version of C3.\n\t */\n\tbuf[0] = ACPI_PDC_REVISION_ID;\n\tbuf[1] = 1;\n\tbuf[2] = (ACPI_PDC_C_CAPABILITY_SMP | ACPI_PDC_EST_CAPABILITY_SWSMP);\n\n\tset_xen_guest_handle(op.u.set_pminfo.pdc, buf);\n\n\tif ((HYPERVISOR_platform_op(&op) == 0) &&\n\t    (buf[2] & (ACPI_PDC_C_C1_FFH | ACPI_PDC_C_C2C3_FFH))) {\n\t\tcpuid_leaf5_ecx_val = cx;\n\t\tcpuid_leaf5_edx_val = dx;\n\t}\n\treturn true;\n#else\n\treturn false;\n#endif\n}\n\nstatic bool __init xen_check_xsave(void)\n{\n\tunsigned int cx, xsave_mask;\n\n\tcx = cpuid_ecx(1);\n\n\txsave_mask = (1 << (X86_FEATURE_XSAVE % 32)) |\n\t\t     (1 << (X86_FEATURE_OSXSAVE % 32));\n\n\t/* Xen will set CR4.OSXSAVE if supported and not disabled by force */\n\treturn (cx & xsave_mask) == xsave_mask;\n}\n\nstatic void __init xen_init_capabilities(void)\n{\n\tsetup_force_cpu_cap(X86_FEATURE_XENPV);\n\tsetup_clear_cpu_cap(X86_FEATURE_DCA);\n\tsetup_clear_cpu_cap(X86_FEATURE_APERFMPERF);\n\tsetup_clear_cpu_cap(X86_FEATURE_MTRR);\n\tsetup_clear_cpu_cap(X86_FEATURE_ACC);\n\tsetup_clear_cpu_cap(X86_FEATURE_X2APIC);\n\tsetup_clear_cpu_cap(X86_FEATURE_SME);\n\n\t/*\n\t * Xen PV would need some work to support PCID: CR3 handling as well\n\t * as xen_flush_tlb_others() would need updating.\n\t */\n\tsetup_clear_cpu_cap(X86_FEATURE_PCID);\n\n\tif (!xen_initial_domain())\n\t\tsetup_clear_cpu_cap(X86_FEATURE_ACPI);\n\n\tif (xen_check_mwait())\n\t\tsetup_force_cpu_cap(X86_FEATURE_MWAIT);\n\telse\n\t\tsetup_clear_cpu_cap(X86_FEATURE_MWAIT);\n\n\tif (!xen_check_xsave()) {\n\t\tsetup_clear_cpu_cap(X86_FEATURE_XSAVE);\n\t\tsetup_clear_cpu_cap(X86_FEATURE_OSXSAVE);\n\t}\n}\n\nstatic noinstr void xen_set_debugreg(int reg, unsigned long val)\n{\n\tHYPERVISOR_set_debugreg(reg, val);\n}\n\nstatic noinstr unsigned long xen_get_debugreg(int reg)\n{\n\treturn HYPERVISOR_get_debugreg(reg);\n}\n\nstatic void xen_end_context_switch(struct task_struct *next)\n{\n\txen_mc_flush();\n\tparavirt_end_context_switch(next);\n}\n\nstatic unsigned long xen_store_tr(void)\n{\n\treturn 0;\n}\n\n/*\n * Set the page permissions for a particular virtual address.  If the\n * address is a vmalloc mapping (or other non-linear mapping), then\n * find the linear mapping of the page and also set its protections to\n * match.\n */\nstatic void set_aliased_prot(void *v, pgprot_t prot)\n{\n\tint level;\n\tpte_t *ptep;\n\tpte_t pte;\n\tunsigned long pfn;\n\tunsigned char dummy;\n\tvoid *va;\n\n\tptep = lookup_address((unsigned long)v, &level);\n\tBUG_ON(ptep == NULL);\n\n\tpfn = pte_pfn(*ptep);\n\tpte = pfn_pte(pfn, prot);\n\n\t/*\n\t * Careful: update_va_mapping() will fail if the virtual address\n\t * we're poking isn't populated in the page tables.  We don't\n\t * need to worry about the direct map (that's always in the page\n\t * tables), but we need to be careful about vmap space.  In\n\t * particular, the top level page table can lazily propagate\n\t * entries between processes, so if we've switched mms since we\n\t * vmapped the target in the first place, we might not have the\n\t * top-level page table entry populated.\n\t *\n\t * We disable preemption because we want the same mm active when\n\t * we probe the target and when we issue the hypercall.  We'll\n\t * have the same nominal mm, but if we're a kernel thread, lazy\n\t * mm dropping could change our pgd.\n\t *\n\t * Out of an abundance of caution, this uses __get_user() to fault\n\t * in the target address just in case there's some obscure case\n\t * in which the target address isn't readable.\n\t */\n\n\tpreempt_disable();\n\n\tcopy_from_kernel_nofault(&dummy, v, 1);\n\n\tif (HYPERVISOR_update_va_mapping((unsigned long)v, pte, 0))\n\t\tBUG();\n\n\tva = __va(PFN_PHYS(pfn));\n\n\tif (va != v && HYPERVISOR_update_va_mapping((unsigned long)va, pte, 0))\n\t\tBUG();\n\n\tpreempt_enable();\n}\n\nstatic void xen_alloc_ldt(struct desc_struct *ldt, unsigned entries)\n{\n\tconst unsigned entries_per_page = PAGE_SIZE / LDT_ENTRY_SIZE;\n\tint i;\n\n\t/*\n\t * We need to mark the all aliases of the LDT pages RO.  We\n\t * don't need to call vm_flush_aliases(), though, since that's\n\t * only responsible for flushing aliases out the TLBs, not the\n\t * page tables, and Xen will flush the TLB for us if needed.\n\t *\n\t * To avoid confusing future readers: none of this is necessary\n\t * to load the LDT.  The hypervisor only checks this when the\n\t * LDT is faulted in due to subsequent descriptor access.\n\t */\n\n\tfor (i = 0; i < entries; i += entries_per_page)\n\t\tset_aliased_prot(ldt + i, PAGE_KERNEL_RO);\n}\n\nstatic void xen_free_ldt(struct desc_struct *ldt, unsigned entries)\n{\n\tconst unsigned entries_per_page = PAGE_SIZE / LDT_ENTRY_SIZE;\n\tint i;\n\n\tfor (i = 0; i < entries; i += entries_per_page)\n\t\tset_aliased_prot(ldt + i, PAGE_KERNEL);\n}\n\nstatic void xen_set_ldt(const void *addr, unsigned entries)\n{\n\tstruct mmuext_op *op;\n\tstruct multicall_space mcs = xen_mc_entry(sizeof(*op));\n\n\ttrace_xen_cpu_set_ldt(addr, entries);\n\n\top = mcs.args;\n\top->cmd = MMUEXT_SET_LDT;\n\top->arg1.linear_addr = (unsigned long)addr;\n\top->arg2.nr_ents = entries;\n\n\tMULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);\n\n\txen_mc_issue(PARAVIRT_LAZY_CPU);\n}\n\nstatic void xen_load_gdt(const struct desc_ptr *dtr)\n{\n\tunsigned long va = dtr->address;\n\tunsigned int size = dtr->size + 1;\n\tunsigned long pfn, mfn;\n\tint level;\n\tpte_t *ptep;\n\tvoid *virt;\n\n\t/* @size should be at most GDT_SIZE which is smaller than PAGE_SIZE. */\n\tBUG_ON(size > PAGE_SIZE);\n\tBUG_ON(va & ~PAGE_MASK);\n\n\t/*\n\t * The GDT is per-cpu and is in the percpu data area.\n\t * That can be virtually mapped, so we need to do a\n\t * page-walk to get the underlying MFN for the\n\t * hypercall.  The page can also be in the kernel's\n\t * linear range, so we need to RO that mapping too.\n\t */\n\tptep = lookup_address(va, &level);\n\tBUG_ON(ptep == NULL);\n\n\tpfn = pte_pfn(*ptep);\n\tmfn = pfn_to_mfn(pfn);\n\tvirt = __va(PFN_PHYS(pfn));\n\n\tmake_lowmem_page_readonly((void *)va);\n\tmake_lowmem_page_readonly(virt);\n\n\tif (HYPERVISOR_set_gdt(&mfn, size / sizeof(struct desc_struct)))\n\t\tBUG();\n}\n\n/*\n * load_gdt for early boot, when the gdt is only mapped once\n */\nstatic void __init xen_load_gdt_boot(const struct desc_ptr *dtr)\n{\n\tunsigned long va = dtr->address;\n\tunsigned int size = dtr->size + 1;\n\tunsigned long pfn, mfn;\n\tpte_t pte;\n\n\t/* @size should be at most GDT_SIZE which is smaller than PAGE_SIZE. */\n\tBUG_ON(size > PAGE_SIZE);\n\tBUG_ON(va & ~PAGE_MASK);\n\n\tpfn = virt_to_pfn(va);\n\tmfn = pfn_to_mfn(pfn);\n\n\tpte = pfn_pte(pfn, PAGE_KERNEL_RO);\n\n\tif (HYPERVISOR_update_va_mapping((unsigned long)va, pte, 0))\n\t\tBUG();\n\n\tif (HYPERVISOR_set_gdt(&mfn, size / sizeof(struct desc_struct)))\n\t\tBUG();\n}\n\nstatic inline bool desc_equal(const struct desc_struct *d1,\n\t\t\t      const struct desc_struct *d2)\n{\n\treturn !memcmp(d1, d2, sizeof(*d1));\n}\n\nstatic void load_TLS_descriptor(struct thread_struct *t,\n\t\t\t\tunsigned int cpu, unsigned int i)\n{\n\tstruct desc_struct *shadow = &per_cpu(shadow_tls_desc, cpu).desc[i];\n\tstruct desc_struct *gdt;\n\txmaddr_t maddr;\n\tstruct multicall_space mc;\n\n\tif (desc_equal(shadow, &t->tls_array[i]))\n\t\treturn;\n\n\t*shadow = t->tls_array[i];\n\n\tgdt = get_cpu_gdt_rw(cpu);\n\tmaddr = arbitrary_virt_to_machine(&gdt[GDT_ENTRY_TLS_MIN+i]);\n\tmc = __xen_mc_entry(0);\n\n\tMULTI_update_descriptor(mc.mc, maddr.maddr, t->tls_array[i]);\n}\n\nstatic void xen_load_tls(struct thread_struct *t, unsigned int cpu)\n{\n\t/*\n\t * In lazy mode we need to zero %fs, otherwise we may get an\n\t * exception between the new %fs descriptor being loaded and\n\t * %fs being effectively cleared at __switch_to().\n\t */\n\tif (paravirt_get_lazy_mode() == PARAVIRT_LAZY_CPU)\n\t\tloadsegment(fs, 0);\n\n\txen_mc_batch();\n\n\tload_TLS_descriptor(t, cpu, 0);\n\tload_TLS_descriptor(t, cpu, 1);\n\tload_TLS_descriptor(t, cpu, 2);\n\n\txen_mc_issue(PARAVIRT_LAZY_CPU);\n}\n\nstatic void xen_load_gs_index(unsigned int idx)\n{\n\tif (HYPERVISOR_set_segment_base(SEGBASE_GS_USER_SEL, idx))\n\t\tBUG();\n}\n\nstatic void xen_write_ldt_entry(struct desc_struct *dt, int entrynum,\n\t\t\t\tconst void *ptr)\n{\n\txmaddr_t mach_lp = arbitrary_virt_to_machine(&dt[entrynum]);\n\tu64 entry = *(u64 *)ptr;\n\n\ttrace_xen_cpu_write_ldt_entry(dt, entrynum, entry);\n\n\tpreempt_disable();\n\n\txen_mc_flush();\n\tif (HYPERVISOR_update_descriptor(mach_lp.maddr, entry))\n\t\tBUG();\n\n\tpreempt_enable();\n}\n\nvoid noist_exc_debug(struct pt_regs *regs);\n\nDEFINE_IDTENTRY_RAW(xenpv_exc_nmi)\n{\n\t/* On Xen PV, NMI doesn't use IST.  The C part is the same as native. */\n\texc_nmi(regs);\n}\n\nDEFINE_IDTENTRY_RAW_ERRORCODE(xenpv_exc_double_fault)\n{\n\t/* On Xen PV, DF doesn't use IST.  The C part is the same as native. */\n\texc_double_fault(regs, error_code);\n}\n\nDEFINE_IDTENTRY_RAW(xenpv_exc_debug)\n{\n\t/*\n\t * There's no IST on Xen PV, but we still need to dispatch\n\t * to the correct handler.\n\t */\n\tif (user_mode(regs))\n\t\tnoist_exc_debug(regs);\n\telse\n\t\texc_debug(regs);\n}\n\nDEFINE_IDTENTRY_RAW(exc_xen_unknown_trap)\n{\n\t/* This should never happen and there is no way to handle it. */\n\tinstrumentation_begin();\n\tpr_err(\"Unknown trap in Xen PV mode.\");\n\tBUG();\n\tinstrumentation_end();\n}\n\n#ifdef CONFIG_X86_MCE\nDEFINE_IDTENTRY_RAW(xenpv_exc_machine_check)\n{\n\t/*\n\t * There's no IST on Xen PV, but we still need to dispatch\n\t * to the correct handler.\n\t */\n\tif (user_mode(regs))\n\t\tnoist_exc_machine_check(regs);\n\telse\n\t\texc_machine_check(regs);\n}\n#endif\n\nstruct trap_array_entry {\n\tvoid (*orig)(void);\n\tvoid (*xen)(void);\n\tbool ist_okay;\n};\n\n#define TRAP_ENTRY(func, ist_ok) {\t\t\t\\\n\t.orig\t\t= asm_##func,\t\t\t\\\n\t.xen\t\t= xen_asm_##func,\t\t\\\n\t.ist_okay\t= ist_ok }\n\n#define TRAP_ENTRY_REDIR(func, ist_ok) {\t\t\\\n\t.orig\t\t= asm_##func,\t\t\t\\\n\t.xen\t\t= xen_asm_xenpv_##func,\t\t\\\n\t.ist_okay\t= ist_ok }\n\nstatic struct trap_array_entry trap_array[] = {\n\tTRAP_ENTRY_REDIR(exc_debug,\t\t\ttrue  ),\n\tTRAP_ENTRY_REDIR(exc_double_fault,\t\ttrue  ),\n#ifdef CONFIG_X86_MCE\n\tTRAP_ENTRY_REDIR(exc_machine_check,\t\ttrue  ),\n#endif\n\tTRAP_ENTRY_REDIR(exc_nmi,\t\t\ttrue  ),\n\tTRAP_ENTRY(exc_int3,\t\t\t\tfalse ),\n\tTRAP_ENTRY(exc_overflow,\t\t\tfalse ),\n#ifdef CONFIG_IA32_EMULATION\n\t{ entry_INT80_compat,          xen_entry_INT80_compat,          false },\n#endif\n\tTRAP_ENTRY(exc_page_fault,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_divide_error,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_bounds,\t\t\t\tfalse ),\n\tTRAP_ENTRY(exc_invalid_op,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_device_not_available,\t\tfalse ),\n\tTRAP_ENTRY(exc_coproc_segment_overrun,\t\tfalse ),\n\tTRAP_ENTRY(exc_invalid_tss,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_segment_not_present,\t\tfalse ),\n\tTRAP_ENTRY(exc_stack_segment,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_general_protection,\t\tfalse ),\n\tTRAP_ENTRY(exc_spurious_interrupt_bug,\t\tfalse ),\n\tTRAP_ENTRY(exc_coprocessor_error,\t\tfalse ),\n\tTRAP_ENTRY(exc_alignment_check,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_simd_coprocessor_error,\t\tfalse ),\n#ifdef CONFIG_X86_KERNEL_IBT\n\tTRAP_ENTRY(exc_control_protection,\t\tfalse ),\n#endif\n};\n\nstatic bool __ref get_trap_addr(void **addr, unsigned int ist)\n{\n\tunsigned int nr;\n\tbool ist_okay = false;\n\tbool found = false;\n\n\t/*\n\t * Replace trap handler addresses by Xen specific ones.\n\t * Check for known traps using IST and whitelist them.\n\t * The debugger ones are the only ones we care about.\n\t * Xen will handle faults like double_fault, so we should never see\n\t * them.  Warn if there's an unexpected IST-using fault handler.\n\t */\n\tfor (nr = 0; nr < ARRAY_SIZE(trap_array); nr++) {\n\t\tstruct trap_array_entry *entry = trap_array + nr;\n\n\t\tif (*addr == entry->orig) {\n\t\t\t*addr = entry->xen;\n\t\t\tist_okay = entry->ist_okay;\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (nr == ARRAY_SIZE(trap_array) &&\n\t    *addr >= (void *)early_idt_handler_array[0] &&\n\t    *addr < (void *)early_idt_handler_array[NUM_EXCEPTION_VECTORS]) {\n\t\tnr = (*addr - (void *)early_idt_handler_array[0]) /\n\t\t     EARLY_IDT_HANDLER_SIZE;\n\t\t*addr = (void *)xen_early_idt_handler_array[nr];\n\t\tfound = true;\n\t}\n\n\tif (!found)\n\t\t*addr = (void *)xen_asm_exc_xen_unknown_trap;\n\n\tif (WARN_ON(found && ist != 0 && !ist_okay))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int cvt_gate_to_trap(int vector, const gate_desc *val,\n\t\t\t    struct trap_info *info)\n{\n\tunsigned long addr;\n\n\tif (val->bits.type != GATE_TRAP && val->bits.type != GATE_INTERRUPT)\n\t\treturn 0;\n\n\tinfo->vector = vector;\n\n\taddr = gate_offset(val);\n\tif (!get_trap_addr((void **)&addr, val->bits.ist))\n\t\treturn 0;\n\tinfo->address = addr;\n\n\tinfo->cs = gate_segment(val);\n\tinfo->flags = val->bits.dpl;\n\t/* interrupt gates clear IF */\n\tif (val->bits.type == GATE_INTERRUPT)\n\t\tinfo->flags |= 1 << 2;\n\n\treturn 1;\n}\n\n/* Locations of each CPU's IDT */\nstatic DEFINE_PER_CPU(struct desc_ptr, idt_desc);\n\n/* Set an IDT entry.  If the entry is part of the current IDT, then\n   also update Xen. */\nstatic void xen_write_idt_entry(gate_desc *dt, int entrynum, const gate_desc *g)\n{\n\tunsigned long p = (unsigned long)&dt[entrynum];\n\tunsigned long start, end;\n\n\ttrace_xen_cpu_write_idt_entry(dt, entrynum, g);\n\n\tpreempt_disable();\n\n\tstart = __this_cpu_read(idt_desc.address);\n\tend = start + __this_cpu_read(idt_desc.size) + 1;\n\n\txen_mc_flush();\n\n\tnative_write_idt_entry(dt, entrynum, g);\n\n\tif (p >= start && (p + 8) <= end) {\n\t\tstruct trap_info info[2];\n\n\t\tinfo[1].address = 0;\n\n\t\tif (cvt_gate_to_trap(entrynum, g, &info[0]))\n\t\t\tif (HYPERVISOR_set_trap_table(info))\n\t\t\t\tBUG();\n\t}\n\n\tpreempt_enable();\n}\n\nstatic unsigned xen_convert_trap_info(const struct desc_ptr *desc,\n\t\t\t\t      struct trap_info *traps, bool full)\n{\n\tunsigned in, out, count;\n\n\tcount = (desc->size+1) / sizeof(gate_desc);\n\tBUG_ON(count > 256);\n\n\tfor (in = out = 0; in < count; in++) {\n\t\tgate_desc *entry = (gate_desc *)(desc->address) + in;\n\n\t\tif (cvt_gate_to_trap(in, entry, &traps[out]) || full)\n\t\t\tout++;\n\t}\n\n\treturn out;\n}\n\nvoid xen_copy_trap_info(struct trap_info *traps)\n{\n\tconst struct desc_ptr *desc = this_cpu_ptr(&idt_desc);\n\n\txen_convert_trap_info(desc, traps, true);\n}\n\n/* Load a new IDT into Xen.  In principle this can be per-CPU, so we\n   hold a spinlock to protect the static traps[] array (static because\n   it avoids allocation, and saves stack space). */\nstatic void xen_load_idt(const struct desc_ptr *desc)\n{\n\tstatic DEFINE_SPINLOCK(lock);\n\tstatic struct trap_info traps[257];\n\tunsigned out;\n\n\ttrace_xen_cpu_load_idt(desc);\n\n\tspin_lock(&lock);\n\n\tmemcpy(this_cpu_ptr(&idt_desc), desc, sizeof(idt_desc));\n\n\tout = xen_convert_trap_info(desc, traps, false);\n\tmemset(&traps[out], 0, sizeof(traps[0]));\n\n\txen_mc_flush();\n\tif (HYPERVISOR_set_trap_table(traps))\n\t\tBUG();\n\n\tspin_unlock(&lock);\n}\n\n/* Write a GDT descriptor entry.  Ignore LDT descriptors, since\n   they're handled differently. */\nstatic void xen_write_gdt_entry(struct desc_struct *dt, int entry,\n\t\t\t\tconst void *desc, int type)\n{\n\ttrace_xen_cpu_write_gdt_entry(dt, entry, desc, type);\n\n\tpreempt_disable();\n\n\tswitch (type) {\n\tcase DESC_LDT:\n\tcase DESC_TSS:\n\t\t/* ignore */\n\t\tbreak;\n\n\tdefault: {\n\t\txmaddr_t maddr = arbitrary_virt_to_machine(&dt[entry]);\n\n\t\txen_mc_flush();\n\t\tif (HYPERVISOR_update_descriptor(maddr.maddr, *(u64 *)desc))\n\t\t\tBUG();\n\t}\n\n\t}\n\n\tpreempt_enable();\n}\n\n/*\n * Version of write_gdt_entry for use at early boot-time needed to\n * update an entry as simply as possible.\n */\nstatic void __init xen_write_gdt_entry_boot(struct desc_struct *dt, int entry,\n\t\t\t\t\t    const void *desc, int type)\n{\n\ttrace_xen_cpu_write_gdt_entry(dt, entry, desc, type);\n\n\tswitch (type) {\n\tcase DESC_LDT:\n\tcase DESC_TSS:\n\t\t/* ignore */\n\t\tbreak;\n\n\tdefault: {\n\t\txmaddr_t maddr = virt_to_machine(&dt[entry]);\n\n\t\tif (HYPERVISOR_update_descriptor(maddr.maddr, *(u64 *)desc))\n\t\t\tdt[entry] = *(struct desc_struct *)desc;\n\t}\n\n\t}\n}\n\nstatic void xen_load_sp0(unsigned long sp0)\n{\n\tstruct multicall_space mcs;\n\n\tmcs = xen_mc_entry(0);\n\tMULTI_stack_switch(mcs.mc, __KERNEL_DS, sp0);\n\txen_mc_issue(PARAVIRT_LAZY_CPU);\n\tthis_cpu_write(cpu_tss_rw.x86_tss.sp0, sp0);\n}\n\n#ifdef CONFIG_X86_IOPL_IOPERM\nstatic void xen_invalidate_io_bitmap(void)\n{\n\tstruct physdev_set_iobitmap iobitmap = {\n\t\t.bitmap = NULL,\n\t\t.nr_ports = 0,\n\t};\n\n\tnative_tss_invalidate_io_bitmap();\n\tHYPERVISOR_physdev_op(PHYSDEVOP_set_iobitmap, &iobitmap);\n}\n\nstatic void xen_update_io_bitmap(void)\n{\n\tstruct physdev_set_iobitmap iobitmap;\n\tstruct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);\n\n\tnative_tss_update_io_bitmap();\n\n\tiobitmap.bitmap = (uint8_t *)(&tss->x86_tss) +\n\t\t\t  tss->x86_tss.io_bitmap_base;\n\tif (tss->x86_tss.io_bitmap_base == IO_BITMAP_OFFSET_INVALID)\n\t\tiobitmap.nr_ports = 0;\n\telse\n\t\tiobitmap.nr_ports = IO_BITMAP_BITS;\n\n\tHYPERVISOR_physdev_op(PHYSDEVOP_set_iobitmap, &iobitmap);\n}\n#endif\n\nstatic void xen_io_delay(void)\n{\n}\n\nstatic DEFINE_PER_CPU(unsigned long, xen_cr0_value);\n\nstatic unsigned long xen_read_cr0(void)\n{\n\tunsigned long cr0 = this_cpu_read(xen_cr0_value);\n\n\tif (unlikely(cr0 == 0)) {\n\t\tcr0 = native_read_cr0();\n\t\tthis_cpu_write(xen_cr0_value, cr0);\n\t}\n\n\treturn cr0;\n}\n\nstatic void xen_write_cr0(unsigned long cr0)\n{\n\tstruct multicall_space mcs;\n\n\tthis_cpu_write(xen_cr0_value, cr0);\n\n\t/* Only pay attention to cr0.TS; everything else is\n\t   ignored. */\n\tmcs = xen_mc_entry(0);\n\n\tMULTI_fpu_taskswitch(mcs.mc, (cr0 & X86_CR0_TS) != 0);\n\n\txen_mc_issue(PARAVIRT_LAZY_CPU);\n}\n\nstatic void xen_write_cr4(unsigned long cr4)\n{\n\tcr4 &= ~(X86_CR4_PGE | X86_CR4_PSE | X86_CR4_PCE);\n\n\tnative_write_cr4(cr4);\n}\n\nstatic u64 xen_read_msr_safe(unsigned int msr, int *err)\n{\n\tu64 val;\n\n\tif (pmu_msr_read(msr, &val, err))\n\t\treturn val;\n\n\tval = native_read_msr_safe(msr, err);\n\tswitch (msr) {\n\tcase MSR_IA32_APICBASE:\n\t\tval &= ~X2APIC_ENABLE;\n\t\tbreak;\n\t}\n\treturn val;\n}\n\nstatic int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)\n{\n\tint ret;\n\tunsigned int which;\n\tu64 base;\n\n\tret = 0;\n\n\tswitch (msr) {\n\tcase MSR_FS_BASE:\t\twhich = SEGBASE_FS; goto set;\n\tcase MSR_KERNEL_GS_BASE:\twhich = SEGBASE_GS_USER; goto set;\n\tcase MSR_GS_BASE:\t\twhich = SEGBASE_GS_KERNEL; goto set;\n\n\tset:\n\t\tbase = ((u64)high << 32) | low;\n\t\tif (HYPERVISOR_set_segment_base(which, base) != 0)\n\t\t\tret = -EIO;\n\t\tbreak;\n\n\tcase MSR_STAR:\n\tcase MSR_CSTAR:\n\tcase MSR_LSTAR:\n\tcase MSR_SYSCALL_MASK:\n\tcase MSR_IA32_SYSENTER_CS:\n\tcase MSR_IA32_SYSENTER_ESP:\n\tcase MSR_IA32_SYSENTER_EIP:\n\t\t/* Fast syscall setup is all done in hypercalls, so\n\t\t   these are all ignored.  Stub them out here to stop\n\t\t   Xen console noise. */\n\t\tbreak;\n\n\tdefault:\n\t\tif (!pmu_msr_write(msr, low, high, &ret))\n\t\t\tret = native_write_msr_safe(msr, low, high);\n\t}\n\n\treturn ret;\n}\n\nstatic u64 xen_read_msr(unsigned int msr)\n{\n\t/*\n\t * This will silently swallow a #GP from RDMSR.  It may be worth\n\t * changing that.\n\t */\n\tint err;\n\n\treturn xen_read_msr_safe(msr, &err);\n}\n\nstatic void xen_write_msr(unsigned int msr, unsigned low, unsigned high)\n{\n\t/*\n\t * This will silently swallow a #GP from WRMSR.  It may be worth\n\t * changing that.\n\t */\n\txen_write_msr_safe(msr, low, high);\n}\n\n/* This is called once we have the cpu_possible_mask */\nvoid __init xen_setup_vcpu_info_placement(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\t/* Set up direct vCPU id mapping for PV guests. */\n\t\tper_cpu(xen_vcpu_id, cpu) = cpu;\n\t\txen_vcpu_setup(cpu);\n\t}\n\n\tpv_ops.irq.save_fl = __PV_IS_CALLEE_SAVE(xen_save_fl_direct);\n\tpv_ops.irq.irq_disable = __PV_IS_CALLEE_SAVE(xen_irq_disable_direct);\n\tpv_ops.irq.irq_enable = __PV_IS_CALLEE_SAVE(xen_irq_enable_direct);\n\tpv_ops.mmu.read_cr2 = __PV_IS_CALLEE_SAVE(xen_read_cr2_direct);\n}\n\nstatic const struct pv_info xen_info __initconst = {\n\t.extra_user_64bit_cs = FLAT_USER_CS64,\n\t.name = \"Xen\",\n};\n\nstatic const typeof(pv_ops) xen_cpu_ops __initconst = {\n\t.cpu = {\n\t\t.cpuid = xen_cpuid,\n\n\t\t.set_debugreg = xen_set_debugreg,\n\t\t.get_debugreg = xen_get_debugreg,\n\n\t\t.read_cr0 = xen_read_cr0,\n\t\t.write_cr0 = xen_write_cr0,\n\n\t\t.write_cr4 = xen_write_cr4,\n\n\t\t.wbinvd = native_wbinvd,\n\n\t\t.read_msr = xen_read_msr,\n\t\t.write_msr = xen_write_msr,\n\n\t\t.read_msr_safe = xen_read_msr_safe,\n\t\t.write_msr_safe = xen_write_msr_safe,\n\n\t\t.read_pmc = xen_read_pmc,\n\n\t\t.load_tr_desc = paravirt_nop,\n\t\t.set_ldt = xen_set_ldt,\n\t\t.load_gdt = xen_load_gdt,\n\t\t.load_idt = xen_load_idt,\n\t\t.load_tls = xen_load_tls,\n\t\t.load_gs_index = xen_load_gs_index,\n\n\t\t.alloc_ldt = xen_alloc_ldt,\n\t\t.free_ldt = xen_free_ldt,\n\n\t\t.store_tr = xen_store_tr,\n\n\t\t.write_ldt_entry = xen_write_ldt_entry,\n\t\t.write_gdt_entry = xen_write_gdt_entry,\n\t\t.write_idt_entry = xen_write_idt_entry,\n\t\t.load_sp0 = xen_load_sp0,\n\n#ifdef CONFIG_X86_IOPL_IOPERM\n\t\t.invalidate_io_bitmap = xen_invalidate_io_bitmap,\n\t\t.update_io_bitmap = xen_update_io_bitmap,\n#endif\n\t\t.io_delay = xen_io_delay,\n\n\t\t.start_context_switch = paravirt_start_context_switch,\n\t\t.end_context_switch = xen_end_context_switch,\n\t},\n};\n\nstatic void xen_restart(char *msg)\n{\n\txen_reboot(SHUTDOWN_reboot);\n}\n\nstatic void xen_machine_halt(void)\n{\n\txen_reboot(SHUTDOWN_poweroff);\n}\n\nstatic void xen_machine_power_off(void)\n{\n\tdo_kernel_power_off();\n\txen_reboot(SHUTDOWN_poweroff);\n}\n\nstatic void xen_crash_shutdown(struct pt_regs *regs)\n{\n\txen_reboot(SHUTDOWN_crash);\n}\n\nstatic const struct machine_ops xen_machine_ops __initconst = {\n\t.restart = xen_restart,\n\t.halt = xen_machine_halt,\n\t.power_off = xen_machine_power_off,\n\t.shutdown = xen_machine_halt,\n\t.crash_shutdown = xen_crash_shutdown,\n\t.emergency_restart = xen_emergency_restart,\n};\n\nstatic unsigned char xen_get_nmi_reason(void)\n{\n\tunsigned char reason = 0;\n\n\t/* Construct a value which looks like it came from port 0x61. */\n\tif (test_bit(_XEN_NMIREASON_io_error,\n\t\t     &HYPERVISOR_shared_info->arch.nmi_reason))\n\t\treason |= NMI_REASON_IOCHK;\n\tif (test_bit(_XEN_NMIREASON_pci_serr,\n\t\t     &HYPERVISOR_shared_info->arch.nmi_reason))\n\t\treason |= NMI_REASON_SERR;\n\n\treturn reason;\n}\n\nstatic void __init xen_boot_params_init_edd(void)\n{\n#if IS_ENABLED(CONFIG_EDD)\n\tstruct xen_platform_op op;\n\tstruct edd_info *edd_info;\n\tu32 *mbr_signature;\n\tunsigned nr;\n\tint ret;\n\n\tedd_info = boot_params.eddbuf;\n\tmbr_signature = boot_params.edd_mbr_sig_buffer;\n\n\top.cmd = XENPF_firmware_info;\n\n\top.u.firmware_info.type = XEN_FW_DISK_INFO;\n\tfor (nr = 0; nr < EDDMAXNR; nr++) {\n\t\tstruct edd_info *info = edd_info + nr;\n\n\t\top.u.firmware_info.index = nr;\n\t\tinfo->params.length = sizeof(info->params);\n\t\tset_xen_guest_handle(op.u.firmware_info.u.disk_info.edd_params,\n\t\t\t\t     &info->params);\n\t\tret = HYPERVISOR_platform_op(&op);\n\t\tif (ret)\n\t\t\tbreak;\n\n#define C(x) info->x = op.u.firmware_info.u.disk_info.x\n\t\tC(device);\n\t\tC(version);\n\t\tC(interface_support);\n\t\tC(legacy_max_cylinder);\n\t\tC(legacy_max_head);\n\t\tC(legacy_sectors_per_track);\n#undef C\n\t}\n\tboot_params.eddbuf_entries = nr;\n\n\top.u.firmware_info.type = XEN_FW_DISK_MBR_SIGNATURE;\n\tfor (nr = 0; nr < EDD_MBR_SIG_MAX; nr++) {\n\t\top.u.firmware_info.index = nr;\n\t\tret = HYPERVISOR_platform_op(&op);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tmbr_signature[nr] = op.u.firmware_info.u.disk_mbr_signature.mbr_signature;\n\t}\n\tboot_params.edd_mbr_sig_buf_entries = nr;\n#endif\n}\n\n/*\n * Set up the GDT and segment registers for -fstack-protector.  Until\n * we do this, we have to be careful not to call any stack-protected\n * function, which is most of the kernel.\n */\nstatic void __init xen_setup_gdt(int cpu)\n{\n\tpv_ops.cpu.write_gdt_entry = xen_write_gdt_entry_boot;\n\tpv_ops.cpu.load_gdt = xen_load_gdt_boot;\n\n\tswitch_to_new_gdt(cpu);\n\n\tpv_ops.cpu.write_gdt_entry = xen_write_gdt_entry;\n\tpv_ops.cpu.load_gdt = xen_load_gdt;\n}\n\nstatic void __init xen_dom0_set_legacy_features(void)\n{\n\tx86_platform.legacy.rtc = 1;\n}\n\nstatic void __init xen_domu_set_legacy_features(void)\n{\n\tx86_platform.legacy.rtc = 0;\n}\n\nextern void early_xen_iret_patch(void);\n\n/* First C function to be called on Xen boot */\nasmlinkage __visible void __init xen_start_kernel(void)\n{\n\tstruct physdev_set_iopl set_iopl;\n\tunsigned long initrd_start = 0;\n\tint rc;\n\n\tif (!xen_start_info)\n\t\treturn;\n\n\t__text_gen_insn(&early_xen_iret_patch,\n\t\t\tJMP32_INSN_OPCODE, &early_xen_iret_patch, &xen_iret,\n\t\t\tJMP32_INSN_SIZE);\n\n\txen_domain_type = XEN_PV_DOMAIN;\n\txen_start_flags = xen_start_info->flags;\n\n\txen_setup_features();\n\n\t/* Install Xen paravirt ops */\n\tpv_info = xen_info;\n\tpv_ops.cpu = xen_cpu_ops.cpu;\n\txen_init_irq_ops();\n\n\t/*\n\t * Setup xen_vcpu early because it is needed for\n\t * local_irq_disable(), irqs_disabled(), e.g. in printk().\n\t *\n\t * Don't do the full vcpu_info placement stuff until we have\n\t * the cpu_possible_mask and a non-dummy shared_info.\n\t */\n\txen_vcpu_info_reset(0);\n\n\tx86_platform.get_nmi_reason = xen_get_nmi_reason;\n\n\tx86_init.resources.memory_setup = xen_memory_setup;\n\tx86_init.irqs.intr_mode_select\t= x86_init_noop;\n\tx86_init.irqs.intr_mode_init\t= x86_init_noop;\n\tx86_init.oem.arch_setup = xen_arch_setup;\n\tx86_init.oem.banner = xen_banner;\n\tx86_init.hyper.init_platform = xen_pv_init_platform;\n\tx86_init.hyper.guest_late_init = xen_pv_guest_late_init;\n\n\t/*\n\t * Set up some pagetable state before starting to set any ptes.\n\t */\n\n\txen_setup_machphys_mapping();\n\txen_init_mmu_ops();\n\n\t/* Prevent unwanted bits from being set in PTEs. */\n\t__supported_pte_mask &= ~_PAGE_GLOBAL;\n\t__default_kernel_pte_mask &= ~_PAGE_GLOBAL;\n\n\t/* Get mfn list */\n\txen_build_dynamic_phys_to_machine();\n\n\t/* Work out if we support NX */\n\tget_cpu_cap(&boot_cpu_data);\n\tx86_configure_nx();\n\n\t/*\n\t * Set up kernel GDT and segment registers, mainly so that\n\t * -fstack-protector code can be executed.\n\t */\n\txen_setup_gdt(0);\n\n\t/* Determine virtual and physical address sizes */\n\tget_cpu_address_sizes(&boot_cpu_data);\n\n\t/* Let's presume PV guests always boot on vCPU with id 0. */\n\tper_cpu(xen_vcpu_id, 0) = 0;\n\n\tidt_setup_early_handler();\n\n\txen_init_capabilities();\n\n#ifdef CONFIG_X86_LOCAL_APIC\n\t/*\n\t * set up the basic apic ops.\n\t */\n\txen_init_apic();\n#endif\n\n\tmachine_ops = xen_machine_ops;\n\n\t/*\n\t * The only reliable way to retain the initial address of the\n\t * percpu gdt_page is to remember it here, so we can go and\n\t * mark it RW later, when the initial percpu area is freed.\n\t */\n\txen_initial_gdt = &per_cpu(gdt_page, 0);\n\n\txen_smp_init();\n\n#ifdef CONFIG_ACPI_NUMA\n\t/*\n\t * The pages we from Xen are not related to machine pages, so\n\t * any NUMA information the kernel tries to get from ACPI will\n\t * be meaningless.  Prevent it from trying.\n\t */\n\tdisable_srat();\n#endif\n\tWARN_ON(xen_cpuhp_setup(xen_cpu_up_prepare_pv, xen_cpu_dead_pv));\n\n\tlocal_irq_disable();\n\tearly_boot_irqs_disabled = true;\n\n\txen_raw_console_write(\"mapping kernel into physical memory\\n\");\n\txen_setup_kernel_pagetable((pgd_t *)xen_start_info->pt_base,\n\t\t\t\t   xen_start_info->nr_pages);\n\txen_reserve_special_pages();\n\n\t/*\n\t * We used to do this in xen_arch_setup, but that is too late\n\t * on AMD were early_cpu_init (run before ->arch_setup()) calls\n\t * early_amd_init which pokes 0xcf8 port.\n\t */\n\tset_iopl.iopl = 1;\n\trc = HYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);\n\tif (rc != 0)\n\t\txen_raw_printk(\"physdev_op failed %d\\n\", rc);\n\n\n\tif (xen_start_info->mod_start) {\n\t    if (xen_start_info->flags & SIF_MOD_START_PFN)\n\t\tinitrd_start = PFN_PHYS(xen_start_info->mod_start);\n\t    else\n\t\tinitrd_start = __pa(xen_start_info->mod_start);\n\t}\n\n\t/* Poke various useful things into boot_params */\n\tboot_params.hdr.type_of_loader = (9 << 4) | 0;\n\tboot_params.hdr.ramdisk_image = initrd_start;\n\tboot_params.hdr.ramdisk_size = xen_start_info->mod_len;\n\tboot_params.hdr.cmd_line_ptr = __pa(xen_start_info->cmd_line);\n\tboot_params.hdr.hardware_subarch = X86_SUBARCH_XEN;\n\n\tif (!xen_initial_domain()) {\n\t\tif (pci_xen)\n\t\t\tx86_init.pci.arch_init = pci_xen_init;\n\t\tx86_platform.set_legacy_features =\n\t\t\t\txen_domu_set_legacy_features;\n\t} else {\n\t\tconst struct dom0_vga_console_info *info =\n\t\t\t(void *)((char *)xen_start_info +\n\t\t\t\t xen_start_info->console.dom0.info_off);\n\t\tstruct xen_platform_op op = {\n\t\t\t.cmd = XENPF_firmware_info,\n\t\t\t.interface_version = XENPF_INTERFACE_VERSION,\n\t\t\t.u.firmware_info.type = XEN_FW_KBD_SHIFT_FLAGS,\n\t\t};\n\n\t\tx86_platform.set_legacy_features =\n\t\t\t\txen_dom0_set_legacy_features;\n\t\txen_init_vga(info, xen_start_info->console.dom0.info_size);\n\t\txen_start_info->console.domU.mfn = 0;\n\t\txen_start_info->console.domU.evtchn = 0;\n\n\t\tif (HYPERVISOR_platform_op(&op) == 0)\n\t\t\tboot_params.kbd_status = op.u.firmware_info.u.kbd_shift_flags;\n\n\t\t/* Make sure ACS will be enabled */\n\t\tpci_request_acs();\n\n\t\txen_acpi_sleep_register();\n\n\t\txen_boot_params_init_edd();\n\n#ifdef CONFIG_ACPI\n\t\t/*\n\t\t * Disable selecting \"Firmware First mode\" for correctable\n\t\t * memory errors, as this is the duty of the hypervisor to\n\t\t * decide.\n\t\t */\n\t\tacpi_disable_cmcff = 1;\n#endif\n\t}\n\n\txen_add_preferred_consoles();\n\n#ifdef CONFIG_PCI\n\t/* PCI BIOS service won't work from a PV guest. */\n\tpci_probe &= ~PCI_PROBE_BIOS;\n#endif\n\txen_raw_console_write(\"about to get started...\\n\");\n\n\t/* We need this for printk timestamps */\n\txen_setup_runstate_info(0);\n\n\txen_efi_init(&boot_params);\n\n\t/* Start the world */\n\tcr4_init_shadow(); /* 32b kernel does this in i386_start_kernel() */\n\tx86_64_start_reservations((char *)__pa_symbol(&boot_params));\n}\n\nstatic int xen_cpu_up_prepare_pv(unsigned int cpu)\n{\n\tint rc;\n\n\tif (per_cpu(xen_vcpu, cpu) == NULL)\n\t\treturn -ENODEV;\n\n\txen_setup_timer(cpu);\n\n\trc = xen_smp_intr_init(cpu);\n\tif (rc) {\n\t\tWARN(1, \"xen_smp_intr_init() for CPU %d failed: %d\\n\",\n\t\t     cpu, rc);\n\t\treturn rc;\n\t}\n\n\trc = xen_smp_intr_init_pv(cpu);\n\tif (rc) {\n\t\tWARN(1, \"xen_smp_intr_init_pv() for CPU %d failed: %d\\n\",\n\t\t     cpu, rc);\n\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\nstatic int xen_cpu_dead_pv(unsigned int cpu)\n{\n\txen_smp_intr_free(cpu);\n\txen_smp_intr_free_pv(cpu);\n\n\txen_teardown_timer(cpu);\n\n\treturn 0;\n}\n\nstatic uint32_t __init xen_platform_pv(void)\n{\n\tif (xen_pv_domain())\n\t\treturn xen_cpuid_base();\n\n\treturn 0;\n}\n\nconst __initconst struct hypervisor_x86 x86_hyper_xen_pv = {\n\t.name                   = \"Xen PV\",\n\t.detect                 = xen_platform_pv,\n\t.type\t\t\t= X86_HYPER_XEN_PV,\n\t.runtime.pin_vcpu       = xen_pin_vcpu,\n\t.ignore_nopv\t\t= true,\n};\n", "/* SPDX-License-Identifier: GPL-2.0 */\n/* Xen-specific pieces of head.S, intended to be included in the right\n\tplace in head.S */\n\n#ifdef CONFIG_XEN\n\n#include <linux/elfnote.h>\n#include <linux/init.h>\n\n#include <asm/boot.h>\n#include <asm/asm.h>\n#include <asm/msr.h>\n#include <asm/page_types.h>\n#include <asm/percpu.h>\n#include <asm/unwind_hints.h>\n\n#include <xen/interface/elfnote.h>\n#include <xen/interface/features.h>\n#include <xen/interface/xen.h>\n#include <xen/interface/xen-mca.h>\n#include <asm/xen/interface.h>\n\n.pushsection .noinstr.text, \"ax\"\n\t.balign PAGE_SIZE\nSYM_CODE_START(hypercall_page)\n\t.rept (PAGE_SIZE / 32)\n\t\tUNWIND_HINT_FUNC\n\t\tANNOTATE_NOENDBR\n\t\tret\n\t\t/*\n\t\t * Xen will write the hypercall page, and sort out ENDBR.\n\t\t */\n\t\t.skip 31, 0xcc\n\t.endr\n\n#define HYPERCALL(n) \\\n\t.equ xen_hypercall_##n, hypercall_page + __HYPERVISOR_##n * 32; \\\n\t.type xen_hypercall_##n, @function; .size xen_hypercall_##n, 32\n#include <asm/xen-hypercalls.h>\n#undef HYPERCALL\nSYM_CODE_END(hypercall_page)\n.popsection\n\n#ifdef CONFIG_XEN_PV\n\t__INIT\nSYM_CODE_START(startup_xen)\n\tUNWIND_HINT_EMPTY\n\tANNOTATE_NOENDBR\n\tcld\n\n\t/* Clear .bss */\n\txor %eax,%eax\n\tmov $__bss_start, %rdi\n\tmov $__bss_stop, %rcx\n\tsub %rdi, %rcx\n\tshr $3, %rcx\n\trep stosq\n\n\tmov %rsi, xen_start_info\n\tmov initial_stack(%rip), %rsp\n\n\t/* Set up %gs.\n\t *\n\t * The base of %gs always points to fixed_percpu_data.  If the\n\t * stack protector canary is enabled, it is located at %gs:40.\n\t * Note that, on SMP, the boot cpu uses init data section until\n\t * the per cpu areas are set up.\n\t */\n\tmovl\t$MSR_GS_BASE,%ecx\n\tmovq\t$INIT_PER_CPU_VAR(fixed_percpu_data),%rax\n\tcdq\n\twrmsr\n\n\tcall xen_start_kernel\nSYM_CODE_END(startup_xen)\n\t__FINIT\n\n#ifdef CONFIG_XEN_PV_SMP\n.pushsection .text\nSYM_CODE_START(asm_cpu_bringup_and_idle)\n\tUNWIND_HINT_EMPTY\n\tENDBR\n\n\tcall cpu_bringup_and_idle\nSYM_CODE_END(asm_cpu_bringup_and_idle)\n.popsection\n#endif\n#endif\n\n\tELFNOTE(Xen, XEN_ELFNOTE_GUEST_OS,       .asciz \"linux\")\n\tELFNOTE(Xen, XEN_ELFNOTE_GUEST_VERSION,  .asciz \"2.6\")\n\tELFNOTE(Xen, XEN_ELFNOTE_XEN_VERSION,    .asciz \"xen-3.0\")\n#ifdef CONFIG_X86_32\n\tELFNOTE(Xen, XEN_ELFNOTE_VIRT_BASE,      _ASM_PTR __PAGE_OFFSET)\n#else\n\tELFNOTE(Xen, XEN_ELFNOTE_VIRT_BASE,      _ASM_PTR __START_KERNEL_map)\n\t/* Map the p2m table to a 512GB-aligned user address. */\n\tELFNOTE(Xen, XEN_ELFNOTE_INIT_P2M,       .quad (PUD_SIZE * PTRS_PER_PUD))\n#endif\n#ifdef CONFIG_XEN_PV\n\tELFNOTE(Xen, XEN_ELFNOTE_ENTRY,          _ASM_PTR startup_xen)\n#endif\n\tELFNOTE(Xen, XEN_ELFNOTE_HYPERCALL_PAGE, _ASM_PTR hypercall_page)\n\tELFNOTE(Xen, XEN_ELFNOTE_FEATURES,\n\t\t.ascii \"!writable_page_tables|pae_pgdir_above_4gb\")\n\tELFNOTE(Xen, XEN_ELFNOTE_SUPPORTED_FEATURES,\n\t\t.long (1 << XENFEAT_writable_page_tables) |       \\\n\t\t      (1 << XENFEAT_dom0) |                       \\\n\t\t      (1 << XENFEAT_linux_rsdp_unrestricted))\n\tELFNOTE(Xen, XEN_ELFNOTE_PAE_MODE,       .asciz \"yes\")\n\tELFNOTE(Xen, XEN_ELFNOTE_LOADER,         .asciz \"generic\")\n\tELFNOTE(Xen, XEN_ELFNOTE_L1_MFN_VALID,\n\t\t.quad _PAGE_PRESENT; .quad _PAGE_PRESENT)\n\tELFNOTE(Xen, XEN_ELFNOTE_SUSPEND_CANCEL, .long 1)\n\tELFNOTE(Xen, XEN_ELFNOTE_MOD_START_PFN,  .long 1)\n\tELFNOTE(Xen, XEN_ELFNOTE_HV_START_LOW,   _ASM_PTR __HYPERVISOR_VIRT_START)\n\tELFNOTE(Xen, XEN_ELFNOTE_PADDR_OFFSET,   _ASM_PTR 0)\n\n#endif /*CONFIG_XEN */\n", "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * Copyright (C) 2015-2017 Josh Poimboeuf <jpoimboe@redhat.com>\n */\n\n#include <string.h>\n#include <stdlib.h>\n#include <inttypes.h>\n#include <sys/mman.h>\n\n#include <arch/elf.h>\n#include <objtool/builtin.h>\n#include <objtool/cfi.h>\n#include <objtool/arch.h>\n#include <objtool/check.h>\n#include <objtool/special.h>\n#include <objtool/warn.h>\n#include <objtool/endianness.h>\n\n#include <linux/objtool.h>\n#include <linux/hashtable.h>\n#include <linux/kernel.h>\n#include <linux/static_call_types.h>\n\nstruct alternative {\n\tstruct list_head list;\n\tstruct instruction *insn;\n\tbool skip_orig;\n};\n\nstatic unsigned long nr_cfi, nr_cfi_reused, nr_cfi_cache;\n\nstatic struct cfi_init_state initial_func_cfi;\nstatic struct cfi_state init_cfi;\nstatic struct cfi_state func_cfi;\n\nstruct instruction *find_insn(struct objtool_file *file,\n\t\t\t      struct section *sec, unsigned long offset)\n{\n\tstruct instruction *insn;\n\n\thash_for_each_possible(file->insn_hash, insn, hash, sec_offset_hash(sec, offset)) {\n\t\tif (insn->sec == sec && insn->offset == offset)\n\t\t\treturn insn;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct instruction *next_insn_same_sec(struct objtool_file *file,\n\t\t\t\t\t      struct instruction *insn)\n{\n\tstruct instruction *next = list_next_entry(insn, list);\n\n\tif (!next || &next->list == &file->insn_list || next->sec != insn->sec)\n\t\treturn NULL;\n\n\treturn next;\n}\n\nstatic struct instruction *next_insn_same_func(struct objtool_file *file,\n\t\t\t\t\t       struct instruction *insn)\n{\n\tstruct instruction *next = list_next_entry(insn, list);\n\tstruct symbol *func = insn->func;\n\n\tif (!func)\n\t\treturn NULL;\n\n\tif (&next->list != &file->insn_list && next->func == func)\n\t\treturn next;\n\n\t/* Check if we're already in the subfunction: */\n\tif (func == func->cfunc)\n\t\treturn NULL;\n\n\t/* Move to the subfunction: */\n\treturn find_insn(file, func->cfunc->sec, func->cfunc->offset);\n}\n\nstatic struct instruction *prev_insn_same_sym(struct objtool_file *file,\n\t\t\t\t\t       struct instruction *insn)\n{\n\tstruct instruction *prev = list_prev_entry(insn, list);\n\n\tif (&prev->list != &file->insn_list && prev->func == insn->func)\n\t\treturn prev;\n\n\treturn NULL;\n}\n\n#define func_for_each_insn(file, func, insn)\t\t\t\t\\\n\tfor (insn = find_insn(file, func->sec, func->offset);\t\t\\\n\t     insn;\t\t\t\t\t\t\t\\\n\t     insn = next_insn_same_func(file, insn))\n\n#define sym_for_each_insn(file, sym, insn)\t\t\t\t\\\n\tfor (insn = find_insn(file, sym->sec, sym->offset);\t\t\\\n\t     insn && &insn->list != &file->insn_list &&\t\t\t\\\n\t\tinsn->sec == sym->sec &&\t\t\t\t\\\n\t\tinsn->offset < sym->offset + sym->len;\t\t\t\\\n\t     insn = list_next_entry(insn, list))\n\n#define sym_for_each_insn_continue_reverse(file, sym, insn)\t\t\\\n\tfor (insn = list_prev_entry(insn, list);\t\t\t\\\n\t     &insn->list != &file->insn_list &&\t\t\t\t\\\n\t\tinsn->sec == sym->sec && insn->offset >= sym->offset;\t\\\n\t     insn = list_prev_entry(insn, list))\n\n#define sec_for_each_insn_from(file, insn)\t\t\t\t\\\n\tfor (; insn; insn = next_insn_same_sec(file, insn))\n\n#define sec_for_each_insn_continue(file, insn)\t\t\t\t\\\n\tfor (insn = next_insn_same_sec(file, insn); insn;\t\t\\\n\t     insn = next_insn_same_sec(file, insn))\n\nstatic bool is_jump_table_jump(struct instruction *insn)\n{\n\tstruct alt_group *alt_group = insn->alt_group;\n\n\tif (insn->jump_table)\n\t\treturn true;\n\n\t/* Retpoline alternative for a jump table? */\n\treturn alt_group && alt_group->orig_group &&\n\t       alt_group->orig_group->first_insn->jump_table;\n}\n\nstatic bool is_sibling_call(struct instruction *insn)\n{\n\t/*\n\t * Assume only ELF functions can make sibling calls.  This ensures\n\t * sibling call detection consistency between vmlinux.o and individual\n\t * objects.\n\t */\n\tif (!insn->func)\n\t\treturn false;\n\n\t/* An indirect jump is either a sibling call or a jump to a table. */\n\tif (insn->type == INSN_JUMP_DYNAMIC)\n\t\treturn !is_jump_table_jump(insn);\n\n\t/* add_jump_destinations() sets insn->call_dest for sibling calls. */\n\treturn (is_static_jump(insn) && insn->call_dest);\n}\n\n/*\n * This checks to see if the given function is a \"noreturn\" function.\n *\n * For global functions which are outside the scope of this object file, we\n * have to keep a manual list of them.\n *\n * For local functions, we have to detect them manually by simply looking for\n * the lack of a return instruction.\n */\nstatic bool __dead_end_function(struct objtool_file *file, struct symbol *func,\n\t\t\t\tint recursion)\n{\n\tint i;\n\tstruct instruction *insn;\n\tbool empty = true;\n\n\t/*\n\t * Unfortunately these have to be hard coded because the noreturn\n\t * attribute isn't provided in ELF data.\n\t */\n\tstatic const char * const global_noreturns[] = {\n\t\t\"__stack_chk_fail\",\n\t\t\"panic\",\n\t\t\"do_exit\",\n\t\t\"do_task_dead\",\n\t\t\"kthread_exit\",\n\t\t\"make_task_dead\",\n\t\t\"__module_put_and_kthread_exit\",\n\t\t\"kthread_complete_and_exit\",\n\t\t\"__reiserfs_panic\",\n\t\t\"lbug_with_loc\",\n\t\t\"fortify_panic\",\n\t\t\"usercopy_abort\",\n\t\t\"machine_real_restart\",\n\t\t\"rewind_stack_and_make_dead\",\n\t\t\"kunit_try_catch_throw\",\n\t\t\"xen_start_kernel\",\n\t\t\"cpu_bringup_and_idle\",\n\t\t\"do_group_exit\",\n\t\t\"stop_this_cpu\",\n\t\t\"__invalid_creds\",\n\t\t\"cpu_startup_entry\",\n\t\t\"__ubsan_handle_builtin_unreachable\",\n\t\t\"ex_handler_msr_mce\",\n\t};\n\n\tif (!func)\n\t\treturn false;\n\n\tif (func->bind == STB_WEAK)\n\t\treturn false;\n\n\tif (func->bind == STB_GLOBAL)\n\t\tfor (i = 0; i < ARRAY_SIZE(global_noreturns); i++)\n\t\t\tif (!strcmp(func->name, global_noreturns[i]))\n\t\t\t\treturn true;\n\n\tif (!func->len)\n\t\treturn false;\n\n\tinsn = find_insn(file, func->sec, func->offset);\n\tif (!insn->func)\n\t\treturn false;\n\n\tfunc_for_each_insn(file, func, insn) {\n\t\tempty = false;\n\n\t\tif (insn->type == INSN_RETURN)\n\t\t\treturn false;\n\t}\n\n\tif (empty)\n\t\treturn false;\n\n\t/*\n\t * A function can have a sibling call instead of a return.  In that\n\t * case, the function's dead-end status depends on whether the target\n\t * of the sibling call returns.\n\t */\n\tfunc_for_each_insn(file, func, insn) {\n\t\tif (is_sibling_call(insn)) {\n\t\t\tstruct instruction *dest = insn->jump_dest;\n\n\t\t\tif (!dest)\n\t\t\t\t/* sibling call to another file */\n\t\t\t\treturn false;\n\n\t\t\t/* local sibling call */\n\t\t\tif (recursion == 5) {\n\t\t\t\t/*\n\t\t\t\t * Infinite recursion: two functions have\n\t\t\t\t * sibling calls to each other.  This is a very\n\t\t\t\t * rare case.  It means they aren't dead ends.\n\t\t\t\t */\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\treturn __dead_end_function(file, dest->func, recursion+1);\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic bool dead_end_function(struct objtool_file *file, struct symbol *func)\n{\n\treturn __dead_end_function(file, func, 0);\n}\n\nstatic void init_cfi_state(struct cfi_state *cfi)\n{\n\tint i;\n\n\tfor (i = 0; i < CFI_NUM_REGS; i++) {\n\t\tcfi->regs[i].base = CFI_UNDEFINED;\n\t\tcfi->vals[i].base = CFI_UNDEFINED;\n\t}\n\tcfi->cfa.base = CFI_UNDEFINED;\n\tcfi->drap_reg = CFI_UNDEFINED;\n\tcfi->drap_offset = -1;\n}\n\nstatic void init_insn_state(struct objtool_file *file, struct insn_state *state,\n\t\t\t    struct section *sec)\n{\n\tmemset(state, 0, sizeof(*state));\n\tinit_cfi_state(&state->cfi);\n\n\t/*\n\t * We need the full vmlinux for noinstr validation, otherwise we can\n\t * not correctly determine insn->call_dest->sec (external symbols do\n\t * not have a section).\n\t */\n\tif (opts.link && opts.noinstr && sec)\n\t\tstate->noinstr = sec->noinstr;\n}\n\nstatic struct cfi_state *cfi_alloc(void)\n{\n\tstruct cfi_state *cfi = calloc(sizeof(struct cfi_state), 1);\n\tif (!cfi) {\n\t\tWARN(\"calloc failed\");\n\t\texit(1);\n\t}\n\tnr_cfi++;\n\treturn cfi;\n}\n\nstatic int cfi_bits;\nstatic struct hlist_head *cfi_hash;\n\nstatic inline bool cficmp(struct cfi_state *cfi1, struct cfi_state *cfi2)\n{\n\treturn memcmp((void *)cfi1 + sizeof(cfi1->hash),\n\t\t      (void *)cfi2 + sizeof(cfi2->hash),\n\t\t      sizeof(struct cfi_state) - sizeof(struct hlist_node));\n}\n\nstatic inline u32 cfi_key(struct cfi_state *cfi)\n{\n\treturn jhash((void *)cfi + sizeof(cfi->hash),\n\t\t     sizeof(*cfi) - sizeof(cfi->hash), 0);\n}\n\nstatic struct cfi_state *cfi_hash_find_or_add(struct cfi_state *cfi)\n{\n\tstruct hlist_head *head = &cfi_hash[hash_min(cfi_key(cfi), cfi_bits)];\n\tstruct cfi_state *obj;\n\n\thlist_for_each_entry(obj, head, hash) {\n\t\tif (!cficmp(cfi, obj)) {\n\t\t\tnr_cfi_cache++;\n\t\t\treturn obj;\n\t\t}\n\t}\n\n\tobj = cfi_alloc();\n\t*obj = *cfi;\n\thlist_add_head(&obj->hash, head);\n\n\treturn obj;\n}\n\nstatic void cfi_hash_add(struct cfi_state *cfi)\n{\n\tstruct hlist_head *head = &cfi_hash[hash_min(cfi_key(cfi), cfi_bits)];\n\n\thlist_add_head(&cfi->hash, head);\n}\n\nstatic void *cfi_hash_alloc(unsigned long size)\n{\n\tcfi_bits = max(10, ilog2(size));\n\tcfi_hash = mmap(NULL, sizeof(struct hlist_head) << cfi_bits,\n\t\t\tPROT_READ|PROT_WRITE,\n\t\t\tMAP_PRIVATE|MAP_ANON, -1, 0);\n\tif (cfi_hash == (void *)-1L) {\n\t\tWARN(\"mmap fail cfi_hash\");\n\t\tcfi_hash = NULL;\n\t}  else if (opts.stats) {\n\t\tprintf(\"cfi_bits: %d\\n\", cfi_bits);\n\t}\n\n\treturn cfi_hash;\n}\n\nstatic unsigned long nr_insns;\nstatic unsigned long nr_insns_visited;\n\n/*\n * Call the arch-specific instruction decoder for all the instructions and add\n * them to the global instruction list.\n */\nstatic int decode_instructions(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct symbol *func;\n\tunsigned long offset;\n\tstruct instruction *insn;\n\tint ret;\n\n\tfor_each_sec(file, sec) {\n\n\t\tif (!(sec->sh.sh_flags & SHF_EXECINSTR))\n\t\t\tcontinue;\n\n\t\tif (strcmp(sec->name, \".altinstr_replacement\") &&\n\t\t    strcmp(sec->name, \".altinstr_aux\") &&\n\t\t    strncmp(sec->name, \".discard.\", 9))\n\t\t\tsec->text = true;\n\n\t\tif (!strcmp(sec->name, \".noinstr.text\") ||\n\t\t    !strcmp(sec->name, \".entry.text\"))\n\t\t\tsec->noinstr = true;\n\n\t\tfor (offset = 0; offset < sec->sh.sh_size; offset += insn->len) {\n\t\t\tinsn = malloc(sizeof(*insn));\n\t\t\tif (!insn) {\n\t\t\t\tWARN(\"malloc failed\");\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tmemset(insn, 0, sizeof(*insn));\n\t\t\tINIT_LIST_HEAD(&insn->alts);\n\t\t\tINIT_LIST_HEAD(&insn->stack_ops);\n\t\t\tINIT_LIST_HEAD(&insn->call_node);\n\n\t\t\tinsn->sec = sec;\n\t\t\tinsn->offset = offset;\n\n\t\t\tret = arch_decode_instruction(file, sec, offset,\n\t\t\t\t\t\t      sec->sh.sh_size - offset,\n\t\t\t\t\t\t      &insn->len, &insn->type,\n\t\t\t\t\t\t      &insn->immediate,\n\t\t\t\t\t\t      &insn->stack_ops);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\t\t/*\n\t\t\t * By default, \"ud2\" is a dead end unless otherwise\n\t\t\t * annotated, because GCC 7 inserts it for certain\n\t\t\t * divide-by-zero cases.\n\t\t\t */\n\t\t\tif (insn->type == INSN_BUG)\n\t\t\t\tinsn->dead_end = true;\n\n\t\t\thash_add(file->insn_hash, &insn->hash, sec_offset_hash(sec, insn->offset));\n\t\t\tlist_add_tail(&insn->list, &file->insn_list);\n\t\t\tnr_insns++;\n\t\t}\n\n\t\tlist_for_each_entry(func, &sec->symbol_list, list) {\n\t\t\tif (func->type != STT_FUNC || func->alias != func)\n\t\t\t\tcontinue;\n\n\t\t\tif (!find_insn(file, sec, func->offset)) {\n\t\t\t\tWARN(\"%s(): can't find starting instruction\",\n\t\t\t\t     func->name);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tsym_for_each_insn(file, func, insn) {\n\t\t\t\tinsn->func = func;\n\t\t\t\tif (insn->type == INSN_ENDBR && list_empty(&insn->call_node)) {\n\t\t\t\t\tif (insn->offset == insn->func->offset) {\n\t\t\t\t\t\tlist_add_tail(&insn->call_node, &file->endbr_list);\n\t\t\t\t\t\tfile->nr_endbr++;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tfile->nr_endbr_int++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (opts.stats)\n\t\tprintf(\"nr_insns: %lu\\n\", nr_insns);\n\n\treturn 0;\n\nerr:\n\tfree(insn);\n\treturn ret;\n}\n\n/*\n * Read the pv_ops[] .data table to find the static initialized values.\n */\nstatic int add_pv_ops(struct objtool_file *file, const char *symname)\n{\n\tstruct symbol *sym, *func;\n\tunsigned long off, end;\n\tstruct reloc *rel;\n\tint idx;\n\n\tsym = find_symbol_by_name(file->elf, symname);\n\tif (!sym)\n\t\treturn 0;\n\n\toff = sym->offset;\n\tend = off + sym->len;\n\tfor (;;) {\n\t\trel = find_reloc_by_dest_range(file->elf, sym->sec, off, end - off);\n\t\tif (!rel)\n\t\t\tbreak;\n\n\t\tfunc = rel->sym;\n\t\tif (func->type == STT_SECTION)\n\t\t\tfunc = find_symbol_by_offset(rel->sym->sec, rel->addend);\n\n\t\tidx = (rel->offset - sym->offset) / sizeof(unsigned long);\n\n\t\tobjtool_pv_add(file, idx, func);\n\n\t\toff = rel->offset + 1;\n\t\tif (off > end)\n\t\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Allocate and initialize file->pv_ops[].\n */\nstatic int init_pv_ops(struct objtool_file *file)\n{\n\tstatic const char *pv_ops_tables[] = {\n\t\t\"pv_ops\",\n\t\t\"xen_cpu_ops\",\n\t\t\"xen_irq_ops\",\n\t\t\"xen_mmu_ops\",\n\t\tNULL,\n\t};\n\tconst char *pv_ops;\n\tstruct symbol *sym;\n\tint idx, nr;\n\n\tif (!opts.noinstr)\n\t\treturn 0;\n\n\tfile->pv_ops = NULL;\n\n\tsym = find_symbol_by_name(file->elf, \"pv_ops\");\n\tif (!sym)\n\t\treturn 0;\n\n\tnr = sym->len / sizeof(unsigned long);\n\tfile->pv_ops = calloc(sizeof(struct pv_state), nr);\n\tif (!file->pv_ops)\n\t\treturn -1;\n\n\tfor (idx = 0; idx < nr; idx++)\n\t\tINIT_LIST_HEAD(&file->pv_ops[idx].targets);\n\n\tfor (idx = 0; (pv_ops = pv_ops_tables[idx]); idx++)\n\t\tadd_pv_ops(file, pv_ops);\n\n\treturn 0;\n}\n\nstatic struct instruction *find_last_insn(struct objtool_file *file,\n\t\t\t\t\t  struct section *sec)\n{\n\tstruct instruction *insn = NULL;\n\tunsigned int offset;\n\tunsigned int end = (sec->sh.sh_size > 10) ? sec->sh.sh_size - 10 : 0;\n\n\tfor (offset = sec->sh.sh_size - 1; offset >= end && !insn; offset--)\n\t\tinsn = find_insn(file, sec, offset);\n\n\treturn insn;\n}\n\n/*\n * Mark \"ud2\" instructions and manually annotated dead ends.\n */\nstatic int add_dead_ends(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct reloc *reloc;\n\tstruct instruction *insn;\n\n\t/*\n\t * Check for manually annotated dead ends.\n\t */\n\tsec = find_section_by_name(file->elf, \".rela.discard.unreachable\");\n\tif (!sec)\n\t\tgoto reachable;\n\n\tlist_for_each_entry(reloc, &sec->reloc_list, list) {\n\t\tif (reloc->sym->type != STT_SECTION) {\n\t\t\tWARN(\"unexpected relocation symbol type in %s\", sec->name);\n\t\t\treturn -1;\n\t\t}\n\t\tinsn = find_insn(file, reloc->sym->sec, reloc->addend);\n\t\tif (insn)\n\t\t\tinsn = list_prev_entry(insn, list);\n\t\telse if (reloc->addend == reloc->sym->sec->sh.sh_size) {\n\t\t\tinsn = find_last_insn(file, reloc->sym->sec);\n\t\t\tif (!insn) {\n\t\t\t\tWARN(\"can't find unreachable insn at %s+0x%\" PRIx64,\n\t\t\t\t     reloc->sym->sec->name, reloc->addend);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t} else {\n\t\t\tWARN(\"can't find unreachable insn at %s+0x%\" PRIx64,\n\t\t\t     reloc->sym->sec->name, reloc->addend);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn->dead_end = true;\n\t}\n\nreachable:\n\t/*\n\t * These manually annotated reachable checks are needed for GCC 4.4,\n\t * where the Linux unreachable() macro isn't supported.  In that case\n\t * GCC doesn't know the \"ud2\" is fatal, so it generates code as if it's\n\t * not a dead end.\n\t */\n\tsec = find_section_by_name(file->elf, \".rela.discard.reachable\");\n\tif (!sec)\n\t\treturn 0;\n\n\tlist_for_each_entry(reloc, &sec->reloc_list, list) {\n\t\tif (reloc->sym->type != STT_SECTION) {\n\t\t\tWARN(\"unexpected relocation symbol type in %s\", sec->name);\n\t\t\treturn -1;\n\t\t}\n\t\tinsn = find_insn(file, reloc->sym->sec, reloc->addend);\n\t\tif (insn)\n\t\t\tinsn = list_prev_entry(insn, list);\n\t\telse if (reloc->addend == reloc->sym->sec->sh.sh_size) {\n\t\t\tinsn = find_last_insn(file, reloc->sym->sec);\n\t\t\tif (!insn) {\n\t\t\t\tWARN(\"can't find reachable insn at %s+0x%\" PRIx64,\n\t\t\t\t     reloc->sym->sec->name, reloc->addend);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t} else {\n\t\t\tWARN(\"can't find reachable insn at %s+0x%\" PRIx64,\n\t\t\t     reloc->sym->sec->name, reloc->addend);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn->dead_end = false;\n\t}\n\n\treturn 0;\n}\n\nstatic int create_static_call_sections(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct static_call_site *site;\n\tstruct instruction *insn;\n\tstruct symbol *key_sym;\n\tchar *key_name, *tmp;\n\tint idx;\n\n\tsec = find_section_by_name(file->elf, \".static_call_sites\");\n\tif (sec) {\n\t\tINIT_LIST_HEAD(&file->static_call_list);\n\t\tWARN(\"file already has .static_call_sites section, skipping\");\n\t\treturn 0;\n\t}\n\n\tif (list_empty(&file->static_call_list))\n\t\treturn 0;\n\n\tidx = 0;\n\tlist_for_each_entry(insn, &file->static_call_list, call_node)\n\t\tidx++;\n\n\tsec = elf_create_section(file->elf, \".static_call_sites\", SHF_WRITE,\n\t\t\t\t sizeof(struct static_call_site), idx);\n\tif (!sec)\n\t\treturn -1;\n\n\tidx = 0;\n\tlist_for_each_entry(insn, &file->static_call_list, call_node) {\n\n\t\tsite = (struct static_call_site *)sec->data->d_buf + idx;\n\t\tmemset(site, 0, sizeof(struct static_call_site));\n\n\t\t/* populate reloc for 'addr' */\n\t\tif (elf_add_reloc_to_insn(file->elf, sec,\n\t\t\t\t\t  idx * sizeof(struct static_call_site),\n\t\t\t\t\t  R_X86_64_PC32,\n\t\t\t\t\t  insn->sec, insn->offset))\n\t\t\treturn -1;\n\n\t\t/* find key symbol */\n\t\tkey_name = strdup(insn->call_dest->name);\n\t\tif (!key_name) {\n\t\t\tperror(\"strdup\");\n\t\t\treturn -1;\n\t\t}\n\t\tif (strncmp(key_name, STATIC_CALL_TRAMP_PREFIX_STR,\n\t\t\t    STATIC_CALL_TRAMP_PREFIX_LEN)) {\n\t\t\tWARN(\"static_call: trampoline name malformed: %s\", key_name);\n\t\t\treturn -1;\n\t\t}\n\t\ttmp = key_name + STATIC_CALL_TRAMP_PREFIX_LEN - STATIC_CALL_KEY_PREFIX_LEN;\n\t\tmemcpy(tmp, STATIC_CALL_KEY_PREFIX_STR, STATIC_CALL_KEY_PREFIX_LEN);\n\n\t\tkey_sym = find_symbol_by_name(file->elf, tmp);\n\t\tif (!key_sym) {\n\t\t\tif (!opts.module) {\n\t\t\t\tWARN(\"static_call: can't find static_call_key symbol: %s\", tmp);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * For modules(), the key might not be exported, which\n\t\t\t * means the module can make static calls but isn't\n\t\t\t * allowed to change them.\n\t\t\t *\n\t\t\t * In that case we temporarily set the key to be the\n\t\t\t * trampoline address.  This is fixed up in\n\t\t\t * static_call_add_module().\n\t\t\t */\n\t\t\tkey_sym = insn->call_dest;\n\t\t}\n\t\tfree(key_name);\n\n\t\t/* populate reloc for 'key' */\n\t\tif (elf_add_reloc(file->elf, sec,\n\t\t\t\t  idx * sizeof(struct static_call_site) + 4,\n\t\t\t\t  R_X86_64_PC32, key_sym,\n\t\t\t\t  is_sibling_call(insn) * STATIC_CALL_SITE_TAIL))\n\t\t\treturn -1;\n\n\t\tidx++;\n\t}\n\n\treturn 0;\n}\n\nstatic int create_retpoline_sites_sections(struct objtool_file *file)\n{\n\tstruct instruction *insn;\n\tstruct section *sec;\n\tint idx;\n\n\tsec = find_section_by_name(file->elf, \".retpoline_sites\");\n\tif (sec) {\n\t\tWARN(\"file already has .retpoline_sites, skipping\");\n\t\treturn 0;\n\t}\n\n\tidx = 0;\n\tlist_for_each_entry(insn, &file->retpoline_call_list, call_node)\n\t\tidx++;\n\n\tif (!idx)\n\t\treturn 0;\n\n\tsec = elf_create_section(file->elf, \".retpoline_sites\", 0,\n\t\t\t\t sizeof(int), idx);\n\tif (!sec) {\n\t\tWARN(\"elf_create_section: .retpoline_sites\");\n\t\treturn -1;\n\t}\n\n\tidx = 0;\n\tlist_for_each_entry(insn, &file->retpoline_call_list, call_node) {\n\n\t\tint *site = (int *)sec->data->d_buf + idx;\n\t\t*site = 0;\n\n\t\tif (elf_add_reloc_to_insn(file->elf, sec,\n\t\t\t\t\t  idx * sizeof(int),\n\t\t\t\t\t  R_X86_64_PC32,\n\t\t\t\t\t  insn->sec, insn->offset)) {\n\t\t\tWARN(\"elf_add_reloc_to_insn: .retpoline_sites\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tidx++;\n\t}\n\n\treturn 0;\n}\n\nstatic int create_ibt_endbr_seal_sections(struct objtool_file *file)\n{\n\tstruct instruction *insn;\n\tstruct section *sec;\n\tint idx;\n\n\tsec = find_section_by_name(file->elf, \".ibt_endbr_seal\");\n\tif (sec) {\n\t\tWARN(\"file already has .ibt_endbr_seal, skipping\");\n\t\treturn 0;\n\t}\n\n\tidx = 0;\n\tlist_for_each_entry(insn, &file->endbr_list, call_node)\n\t\tidx++;\n\n\tif (opts.stats) {\n\t\tprintf(\"ibt: ENDBR at function start: %d\\n\", file->nr_endbr);\n\t\tprintf(\"ibt: ENDBR inside functions:  %d\\n\", file->nr_endbr_int);\n\t\tprintf(\"ibt: superfluous ENDBR:       %d\\n\", idx);\n\t}\n\n\tif (!idx)\n\t\treturn 0;\n\n\tsec = elf_create_section(file->elf, \".ibt_endbr_seal\", 0,\n\t\t\t\t sizeof(int), idx);\n\tif (!sec) {\n\t\tWARN(\"elf_create_section: .ibt_endbr_seal\");\n\t\treturn -1;\n\t}\n\n\tidx = 0;\n\tlist_for_each_entry(insn, &file->endbr_list, call_node) {\n\n\t\tint *site = (int *)sec->data->d_buf + idx;\n\t\t*site = 0;\n\n\t\tif (elf_add_reloc_to_insn(file->elf, sec,\n\t\t\t\t\t  idx * sizeof(int),\n\t\t\t\t\t  R_X86_64_PC32,\n\t\t\t\t\t  insn->sec, insn->offset)) {\n\t\t\tWARN(\"elf_add_reloc_to_insn: .ibt_endbr_seal\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tidx++;\n\t}\n\n\treturn 0;\n}\n\nstatic int create_mcount_loc_sections(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tunsigned long *loc;\n\tstruct instruction *insn;\n\tint idx;\n\n\tsec = find_section_by_name(file->elf, \"__mcount_loc\");\n\tif (sec) {\n\t\tINIT_LIST_HEAD(&file->mcount_loc_list);\n\t\tWARN(\"file already has __mcount_loc section, skipping\");\n\t\treturn 0;\n\t}\n\n\tif (list_empty(&file->mcount_loc_list))\n\t\treturn 0;\n\n\tidx = 0;\n\tlist_for_each_entry(insn, &file->mcount_loc_list, call_node)\n\t\tidx++;\n\n\tsec = elf_create_section(file->elf, \"__mcount_loc\", 0, sizeof(unsigned long), idx);\n\tif (!sec)\n\t\treturn -1;\n\n\tidx = 0;\n\tlist_for_each_entry(insn, &file->mcount_loc_list, call_node) {\n\n\t\tloc = (unsigned long *)sec->data->d_buf + idx;\n\t\tmemset(loc, 0, sizeof(unsigned long));\n\n\t\tif (elf_add_reloc_to_insn(file->elf, sec,\n\t\t\t\t\t  idx * sizeof(unsigned long),\n\t\t\t\t\t  R_X86_64_64,\n\t\t\t\t\t  insn->sec, insn->offset))\n\t\t\treturn -1;\n\n\t\tidx++;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Warnings shouldn't be reported for ignored functions.\n */\nstatic void add_ignores(struct objtool_file *file)\n{\n\tstruct instruction *insn;\n\tstruct section *sec;\n\tstruct symbol *func;\n\tstruct reloc *reloc;\n\n\tsec = find_section_by_name(file->elf, \".rela.discard.func_stack_frame_non_standard\");\n\tif (!sec)\n\t\treturn;\n\n\tlist_for_each_entry(reloc, &sec->reloc_list, list) {\n\t\tswitch (reloc->sym->type) {\n\t\tcase STT_FUNC:\n\t\t\tfunc = reloc->sym;\n\t\t\tbreak;\n\n\t\tcase STT_SECTION:\n\t\t\tfunc = find_func_by_offset(reloc->sym->sec, reloc->addend);\n\t\t\tif (!func)\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tWARN(\"unexpected relocation symbol type in %s: %d\", sec->name, reloc->sym->type);\n\t\t\tcontinue;\n\t\t}\n\n\t\tfunc_for_each_insn(file, func, insn)\n\t\t\tinsn->ignore = true;\n\t}\n}\n\n/*\n * This is a whitelist of functions that is allowed to be called with AC set.\n * The list is meant to be minimal and only contains compiler instrumentation\n * ABI and a few functions used to implement *_{to,from}_user() functions.\n *\n * These functions must not directly change AC, but may PUSHF/POPF.\n */\nstatic const char *uaccess_safe_builtin[] = {\n\t/* KASAN */\n\t\"kasan_report\",\n\t\"kasan_check_range\",\n\t/* KASAN out-of-line */\n\t\"__asan_loadN_noabort\",\n\t\"__asan_load1_noabort\",\n\t\"__asan_load2_noabort\",\n\t\"__asan_load4_noabort\",\n\t\"__asan_load8_noabort\",\n\t\"__asan_load16_noabort\",\n\t\"__asan_storeN_noabort\",\n\t\"__asan_store1_noabort\",\n\t\"__asan_store2_noabort\",\n\t\"__asan_store4_noabort\",\n\t\"__asan_store8_noabort\",\n\t\"__asan_store16_noabort\",\n\t\"__kasan_check_read\",\n\t\"__kasan_check_write\",\n\t/* KASAN in-line */\n\t\"__asan_report_load_n_noabort\",\n\t\"__asan_report_load1_noabort\",\n\t\"__asan_report_load2_noabort\",\n\t\"__asan_report_load4_noabort\",\n\t\"__asan_report_load8_noabort\",\n\t\"__asan_report_load16_noabort\",\n\t\"__asan_report_store_n_noabort\",\n\t\"__asan_report_store1_noabort\",\n\t\"__asan_report_store2_noabort\",\n\t\"__asan_report_store4_noabort\",\n\t\"__asan_report_store8_noabort\",\n\t\"__asan_report_store16_noabort\",\n\t/* KCSAN */\n\t\"__kcsan_check_access\",\n\t\"__kcsan_mb\",\n\t\"__kcsan_wmb\",\n\t\"__kcsan_rmb\",\n\t\"__kcsan_release\",\n\t\"kcsan_found_watchpoint\",\n\t\"kcsan_setup_watchpoint\",\n\t\"kcsan_check_scoped_accesses\",\n\t\"kcsan_disable_current\",\n\t\"kcsan_enable_current_nowarn\",\n\t/* KCSAN/TSAN */\n\t\"__tsan_func_entry\",\n\t\"__tsan_func_exit\",\n\t\"__tsan_read_range\",\n\t\"__tsan_write_range\",\n\t\"__tsan_read1\",\n\t\"__tsan_read2\",\n\t\"__tsan_read4\",\n\t\"__tsan_read8\",\n\t\"__tsan_read16\",\n\t\"__tsan_write1\",\n\t\"__tsan_write2\",\n\t\"__tsan_write4\",\n\t\"__tsan_write8\",\n\t\"__tsan_write16\",\n\t\"__tsan_read_write1\",\n\t\"__tsan_read_write2\",\n\t\"__tsan_read_write4\",\n\t\"__tsan_read_write8\",\n\t\"__tsan_read_write16\",\n\t\"__tsan_atomic8_load\",\n\t\"__tsan_atomic16_load\",\n\t\"__tsan_atomic32_load\",\n\t\"__tsan_atomic64_load\",\n\t\"__tsan_atomic8_store\",\n\t\"__tsan_atomic16_store\",\n\t\"__tsan_atomic32_store\",\n\t\"__tsan_atomic64_store\",\n\t\"__tsan_atomic8_exchange\",\n\t\"__tsan_atomic16_exchange\",\n\t\"__tsan_atomic32_exchange\",\n\t\"__tsan_atomic64_exchange\",\n\t\"__tsan_atomic8_fetch_add\",\n\t\"__tsan_atomic16_fetch_add\",\n\t\"__tsan_atomic32_fetch_add\",\n\t\"__tsan_atomic64_fetch_add\",\n\t\"__tsan_atomic8_fetch_sub\",\n\t\"__tsan_atomic16_fetch_sub\",\n\t\"__tsan_atomic32_fetch_sub\",\n\t\"__tsan_atomic64_fetch_sub\",\n\t\"__tsan_atomic8_fetch_and\",\n\t\"__tsan_atomic16_fetch_and\",\n\t\"__tsan_atomic32_fetch_and\",\n\t\"__tsan_atomic64_fetch_and\",\n\t\"__tsan_atomic8_fetch_or\",\n\t\"__tsan_atomic16_fetch_or\",\n\t\"__tsan_atomic32_fetch_or\",\n\t\"__tsan_atomic64_fetch_or\",\n\t\"__tsan_atomic8_fetch_xor\",\n\t\"__tsan_atomic16_fetch_xor\",\n\t\"__tsan_atomic32_fetch_xor\",\n\t\"__tsan_atomic64_fetch_xor\",\n\t\"__tsan_atomic8_fetch_nand\",\n\t\"__tsan_atomic16_fetch_nand\",\n\t\"__tsan_atomic32_fetch_nand\",\n\t\"__tsan_atomic64_fetch_nand\",\n\t\"__tsan_atomic8_compare_exchange_strong\",\n\t\"__tsan_atomic16_compare_exchange_strong\",\n\t\"__tsan_atomic32_compare_exchange_strong\",\n\t\"__tsan_atomic64_compare_exchange_strong\",\n\t\"__tsan_atomic8_compare_exchange_weak\",\n\t\"__tsan_atomic16_compare_exchange_weak\",\n\t\"__tsan_atomic32_compare_exchange_weak\",\n\t\"__tsan_atomic64_compare_exchange_weak\",\n\t\"__tsan_atomic8_compare_exchange_val\",\n\t\"__tsan_atomic16_compare_exchange_val\",\n\t\"__tsan_atomic32_compare_exchange_val\",\n\t\"__tsan_atomic64_compare_exchange_val\",\n\t\"__tsan_atomic_thread_fence\",\n\t\"__tsan_atomic_signal_fence\",\n\t/* KCOV */\n\t\"write_comp_data\",\n\t\"check_kcov_mode\",\n\t\"__sanitizer_cov_trace_pc\",\n\t\"__sanitizer_cov_trace_const_cmp1\",\n\t\"__sanitizer_cov_trace_const_cmp2\",\n\t\"__sanitizer_cov_trace_const_cmp4\",\n\t\"__sanitizer_cov_trace_const_cmp8\",\n\t\"__sanitizer_cov_trace_cmp1\",\n\t\"__sanitizer_cov_trace_cmp2\",\n\t\"__sanitizer_cov_trace_cmp4\",\n\t\"__sanitizer_cov_trace_cmp8\",\n\t\"__sanitizer_cov_trace_switch\",\n\t/* UBSAN */\n\t\"ubsan_type_mismatch_common\",\n\t\"__ubsan_handle_type_mismatch\",\n\t\"__ubsan_handle_type_mismatch_v1\",\n\t\"__ubsan_handle_shift_out_of_bounds\",\n\t/* misc */\n\t\"csum_partial_copy_generic\",\n\t\"copy_mc_fragile\",\n\t\"copy_mc_fragile_handle_tail\",\n\t\"copy_mc_enhanced_fast_string\",\n\t\"ftrace_likely_update\", /* CONFIG_TRACE_BRANCH_PROFILING */\n\tNULL\n};\n\nstatic void add_uaccess_safe(struct objtool_file *file)\n{\n\tstruct symbol *func;\n\tconst char **name;\n\n\tif (!opts.uaccess)\n\t\treturn;\n\n\tfor (name = uaccess_safe_builtin; *name; name++) {\n\t\tfunc = find_symbol_by_name(file->elf, *name);\n\t\tif (!func)\n\t\t\tcontinue;\n\n\t\tfunc->uaccess_safe = true;\n\t}\n}\n\n/*\n * FIXME: For now, just ignore any alternatives which add retpolines.  This is\n * a temporary hack, as it doesn't allow ORC to unwind from inside a retpoline.\n * But it at least allows objtool to understand the control flow *around* the\n * retpoline.\n */\nstatic int add_ignore_alternatives(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct reloc *reloc;\n\tstruct instruction *insn;\n\n\tsec = find_section_by_name(file->elf, \".rela.discard.ignore_alts\");\n\tif (!sec)\n\t\treturn 0;\n\n\tlist_for_each_entry(reloc, &sec->reloc_list, list) {\n\t\tif (reloc->sym->type != STT_SECTION) {\n\t\t\tWARN(\"unexpected relocation symbol type in %s\", sec->name);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn = find_insn(file, reloc->sym->sec, reloc->addend);\n\t\tif (!insn) {\n\t\t\tWARN(\"bad .discard.ignore_alts entry\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn->ignore_alts = true;\n\t}\n\n\treturn 0;\n}\n\n__weak bool arch_is_retpoline(struct symbol *sym)\n{\n\treturn false;\n}\n\n#define NEGATIVE_RELOC\t((void *)-1L)\n\nstatic struct reloc *insn_reloc(struct objtool_file *file, struct instruction *insn)\n{\n\tif (insn->reloc == NEGATIVE_RELOC)\n\t\treturn NULL;\n\n\tif (!insn->reloc) {\n\t\tif (!file)\n\t\t\treturn NULL;\n\n\t\tinsn->reloc = find_reloc_by_dest_range(file->elf, insn->sec,\n\t\t\t\t\t\t       insn->offset, insn->len);\n\t\tif (!insn->reloc) {\n\t\t\tinsn->reloc = NEGATIVE_RELOC;\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn insn->reloc;\n}\n\nstatic void remove_insn_ops(struct instruction *insn)\n{\n\tstruct stack_op *op, *tmp;\n\n\tlist_for_each_entry_safe(op, tmp, &insn->stack_ops, list) {\n\t\tlist_del(&op->list);\n\t\tfree(op);\n\t}\n}\n\nstatic void annotate_call_site(struct objtool_file *file,\n\t\t\t       struct instruction *insn, bool sibling)\n{\n\tstruct reloc *reloc = insn_reloc(file, insn);\n\tstruct symbol *sym = insn->call_dest;\n\n\tif (!sym)\n\t\tsym = reloc->sym;\n\n\t/*\n\t * Alternative replacement code is just template code which is\n\t * sometimes copied to the original instruction. For now, don't\n\t * annotate it. (In the future we might consider annotating the\n\t * original instruction if/when it ever makes sense to do so.)\n\t */\n\tif (!strcmp(insn->sec->name, \".altinstr_replacement\"))\n\t\treturn;\n\n\tif (sym->static_call_tramp) {\n\t\tlist_add_tail(&insn->call_node, &file->static_call_list);\n\t\treturn;\n\t}\n\n\tif (sym->retpoline_thunk) {\n\t\tlist_add_tail(&insn->call_node, &file->retpoline_call_list);\n\t\treturn;\n\t}\n\n\t/*\n\t * Many compilers cannot disable KCOV or sanitizer calls with a function\n\t * attribute so they need a little help, NOP out any such calls from\n\t * noinstr text.\n\t */\n\tif (opts.hack_noinstr && insn->sec->noinstr && sym->profiling_func) {\n\t\tif (reloc) {\n\t\t\treloc->type = R_NONE;\n\t\t\telf_write_reloc(file->elf, reloc);\n\t\t}\n\n\t\telf_write_insn(file->elf, insn->sec,\n\t\t\t       insn->offset, insn->len,\n\t\t\t       sibling ? arch_ret_insn(insn->len)\n\t\t\t               : arch_nop_insn(insn->len));\n\n\t\tinsn->type = sibling ? INSN_RETURN : INSN_NOP;\n\n\t\tif (sibling) {\n\t\t\t/*\n\t\t\t * We've replaced the tail-call JMP insn by two new\n\t\t\t * insn: RET; INT3, except we only have a single struct\n\t\t\t * insn here. Mark it retpoline_safe to avoid the SLS\n\t\t\t * warning, instead of adding another insn.\n\t\t\t */\n\t\t\tinsn->retpoline_safe = true;\n\t\t}\n\n\t\treturn;\n\t}\n\n\tif (opts.mcount && sym->fentry) {\n\t\tif (sibling)\n\t\t\tWARN_FUNC(\"Tail call to __fentry__ !?!?\", insn->sec, insn->offset);\n\n\t\tif (reloc) {\n\t\t\treloc->type = R_NONE;\n\t\t\telf_write_reloc(file->elf, reloc);\n\t\t}\n\n\t\telf_write_insn(file->elf, insn->sec,\n\t\t\t       insn->offset, insn->len,\n\t\t\t       arch_nop_insn(insn->len));\n\n\t\tinsn->type = INSN_NOP;\n\n\t\tlist_add_tail(&insn->call_node, &file->mcount_loc_list);\n\t\treturn;\n\t}\n\n\tif (!sibling && dead_end_function(file, sym))\n\t\tinsn->dead_end = true;\n}\n\nstatic void add_call_dest(struct objtool_file *file, struct instruction *insn,\n\t\t\t  struct symbol *dest, bool sibling)\n{\n\tinsn->call_dest = dest;\n\tif (!dest)\n\t\treturn;\n\n\t/*\n\t * Whatever stack impact regular CALLs have, should be undone\n\t * by the RETURN of the called function.\n\t *\n\t * Annotated intra-function calls retain the stack_ops but\n\t * are converted to JUMP, see read_intra_function_calls().\n\t */\n\tremove_insn_ops(insn);\n\n\tannotate_call_site(file, insn, sibling);\n}\n\nstatic void add_retpoline_call(struct objtool_file *file, struct instruction *insn)\n{\n\t/*\n\t * Retpoline calls/jumps are really dynamic calls/jumps in disguise,\n\t * so convert them accordingly.\n\t */\n\tswitch (insn->type) {\n\tcase INSN_CALL:\n\t\tinsn->type = INSN_CALL_DYNAMIC;\n\t\tbreak;\n\tcase INSN_JUMP_UNCONDITIONAL:\n\t\tinsn->type = INSN_JUMP_DYNAMIC;\n\t\tbreak;\n\tcase INSN_JUMP_CONDITIONAL:\n\t\tinsn->type = INSN_JUMP_DYNAMIC_CONDITIONAL;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tinsn->retpoline_safe = true;\n\n\t/*\n\t * Whatever stack impact regular CALLs have, should be undone\n\t * by the RETURN of the called function.\n\t *\n\t * Annotated intra-function calls retain the stack_ops but\n\t * are converted to JUMP, see read_intra_function_calls().\n\t */\n\tremove_insn_ops(insn);\n\n\tannotate_call_site(file, insn, false);\n}\n\nstatic bool same_function(struct instruction *insn1, struct instruction *insn2)\n{\n\treturn insn1->func->pfunc == insn2->func->pfunc;\n}\n\nstatic bool is_first_func_insn(struct objtool_file *file, struct instruction *insn)\n{\n\tif (insn->offset == insn->func->offset)\n\t\treturn true;\n\n\tif (opts.ibt) {\n\t\tstruct instruction *prev = prev_insn_same_sym(file, insn);\n\n\t\tif (prev && prev->type == INSN_ENDBR &&\n\t\t    insn->offset == insn->func->offset + prev->len)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * Find the destination instructions for all jumps.\n */\nstatic int add_jump_destinations(struct objtool_file *file)\n{\n\tstruct instruction *insn, *jump_dest;\n\tstruct reloc *reloc;\n\tstruct section *dest_sec;\n\tunsigned long dest_off;\n\n\tfor_each_insn(file, insn) {\n\t\tif (insn->jump_dest) {\n\t\t\t/*\n\t\t\t * handle_group_alt() may have previously set\n\t\t\t * 'jump_dest' for some alternatives.\n\t\t\t */\n\t\t\tcontinue;\n\t\t}\n\t\tif (!is_static_jump(insn))\n\t\t\tcontinue;\n\n\t\treloc = insn_reloc(file, insn);\n\t\tif (!reloc) {\n\t\t\tdest_sec = insn->sec;\n\t\t\tdest_off = arch_jump_destination(insn);\n\t\t} else if (reloc->sym->type == STT_SECTION) {\n\t\t\tdest_sec = reloc->sym->sec;\n\t\t\tdest_off = arch_dest_reloc_offset(reloc->addend);\n\t\t} else if (reloc->sym->retpoline_thunk) {\n\t\t\tadd_retpoline_call(file, insn);\n\t\t\tcontinue;\n\t\t} else if (insn->func) {\n\t\t\t/*\n\t\t\t * External sibling call or internal sibling call with\n\t\t\t * STT_FUNC reloc.\n\t\t\t */\n\t\t\tadd_call_dest(file, insn, reloc->sym, true);\n\t\t\tcontinue;\n\t\t} else if (reloc->sym->sec->idx) {\n\t\t\tdest_sec = reloc->sym->sec;\n\t\t\tdest_off = reloc->sym->sym.st_value +\n\t\t\t\t   arch_dest_reloc_offset(reloc->addend);\n\t\t} else {\n\t\t\t/* non-func asm code jumping to another file */\n\t\t\tcontinue;\n\t\t}\n\n\t\tjump_dest = find_insn(file, dest_sec, dest_off);\n\t\tif (!jump_dest) {\n\t\t\tWARN_FUNC(\"can't find jump dest instruction at %s+0x%lx\",\n\t\t\t\t  insn->sec, insn->offset, dest_sec->name,\n\t\t\t\t  dest_off);\n\t\t\treturn -1;\n\t\t}\n\n\t\t/*\n\t\t * Cross-function jump.\n\t\t */\n\t\tif (insn->func && jump_dest->func &&\n\t\t    insn->func != jump_dest->func) {\n\n\t\t\t/*\n\t\t\t * For GCC 8+, create parent/child links for any cold\n\t\t\t * subfunctions.  This is _mostly_ redundant with a\n\t\t\t * similar initialization in read_symbols().\n\t\t\t *\n\t\t\t * If a function has aliases, we want the *first* such\n\t\t\t * function in the symbol table to be the subfunction's\n\t\t\t * parent.  In that case we overwrite the\n\t\t\t * initialization done in read_symbols().\n\t\t\t *\n\t\t\t * However this code can't completely replace the\n\t\t\t * read_symbols() code because this doesn't detect the\n\t\t\t * case where the parent function's only reference to a\n\t\t\t * subfunction is through a jump table.\n\t\t\t */\n\t\t\tif (!strstr(insn->func->name, \".cold\") &&\n\t\t\t    strstr(jump_dest->func->name, \".cold\")) {\n\t\t\t\tinsn->func->cfunc = jump_dest->func;\n\t\t\t\tjump_dest->func->pfunc = insn->func;\n\n\t\t\t} else if (!same_function(insn, jump_dest) &&\n\t\t\t\t   is_first_func_insn(file, jump_dest)) {\n\t\t\t\t/*\n\t\t\t\t * Internal sibling call without reloc or with\n\t\t\t\t * STT_SECTION reloc.\n\t\t\t\t */\n\t\t\t\tadd_call_dest(file, insn, jump_dest->func, true);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tinsn->jump_dest = jump_dest;\n\t}\n\n\treturn 0;\n}\n\nstatic struct symbol *find_call_destination(struct section *sec, unsigned long offset)\n{\n\tstruct symbol *call_dest;\n\n\tcall_dest = find_func_by_offset(sec, offset);\n\tif (!call_dest)\n\t\tcall_dest = find_symbol_by_offset(sec, offset);\n\n\treturn call_dest;\n}\n\n/*\n * Find the destination instructions for all calls.\n */\nstatic int add_call_destinations(struct objtool_file *file)\n{\n\tstruct instruction *insn;\n\tunsigned long dest_off;\n\tstruct symbol *dest;\n\tstruct reloc *reloc;\n\n\tfor_each_insn(file, insn) {\n\t\tif (insn->type != INSN_CALL)\n\t\t\tcontinue;\n\n\t\treloc = insn_reloc(file, insn);\n\t\tif (!reloc) {\n\t\t\tdest_off = arch_jump_destination(insn);\n\t\t\tdest = find_call_destination(insn->sec, dest_off);\n\n\t\t\tadd_call_dest(file, insn, dest, false);\n\n\t\t\tif (insn->ignore)\n\t\t\t\tcontinue;\n\n\t\t\tif (!insn->call_dest) {\n\t\t\t\tWARN_FUNC(\"unannotated intra-function call\", insn->sec, insn->offset);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tif (insn->func && insn->call_dest->type != STT_FUNC) {\n\t\t\t\tWARN_FUNC(\"unsupported call to non-function\",\n\t\t\t\t\t  insn->sec, insn->offset);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t} else if (reloc->sym->type == STT_SECTION) {\n\t\t\tdest_off = arch_dest_reloc_offset(reloc->addend);\n\t\t\tdest = find_call_destination(reloc->sym->sec, dest_off);\n\t\t\tif (!dest) {\n\t\t\t\tWARN_FUNC(\"can't find call dest symbol at %s+0x%lx\",\n\t\t\t\t\t  insn->sec, insn->offset,\n\t\t\t\t\t  reloc->sym->sec->name,\n\t\t\t\t\t  dest_off);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tadd_call_dest(file, insn, dest, false);\n\n\t\t} else if (reloc->sym->retpoline_thunk) {\n\t\t\tadd_retpoline_call(file, insn);\n\n\t\t} else\n\t\t\tadd_call_dest(file, insn, reloc->sym, false);\n\t}\n\n\treturn 0;\n}\n\n/*\n * The .alternatives section requires some extra special care over and above\n * other special sections because alternatives are patched in place.\n */\nstatic int handle_group_alt(struct objtool_file *file,\n\t\t\t    struct special_alt *special_alt,\n\t\t\t    struct instruction *orig_insn,\n\t\t\t    struct instruction **new_insn)\n{\n\tstruct instruction *last_orig_insn, *last_new_insn = NULL, *insn, *nop = NULL;\n\tstruct alt_group *orig_alt_group, *new_alt_group;\n\tunsigned long dest_off;\n\n\n\torig_alt_group = malloc(sizeof(*orig_alt_group));\n\tif (!orig_alt_group) {\n\t\tWARN(\"malloc failed\");\n\t\treturn -1;\n\t}\n\torig_alt_group->cfi = calloc(special_alt->orig_len,\n\t\t\t\t     sizeof(struct cfi_state *));\n\tif (!orig_alt_group->cfi) {\n\t\tWARN(\"calloc failed\");\n\t\treturn -1;\n\t}\n\n\tlast_orig_insn = NULL;\n\tinsn = orig_insn;\n\tsec_for_each_insn_from(file, insn) {\n\t\tif (insn->offset >= special_alt->orig_off + special_alt->orig_len)\n\t\t\tbreak;\n\n\t\tinsn->alt_group = orig_alt_group;\n\t\tlast_orig_insn = insn;\n\t}\n\torig_alt_group->orig_group = NULL;\n\torig_alt_group->first_insn = orig_insn;\n\torig_alt_group->last_insn = last_orig_insn;\n\n\n\tnew_alt_group = malloc(sizeof(*new_alt_group));\n\tif (!new_alt_group) {\n\t\tWARN(\"malloc failed\");\n\t\treturn -1;\n\t}\n\n\tif (special_alt->new_len < special_alt->orig_len) {\n\t\t/*\n\t\t * Insert a fake nop at the end to make the replacement\n\t\t * alt_group the same size as the original.  This is needed to\n\t\t * allow propagate_alt_cfi() to do its magic.  When the last\n\t\t * instruction affects the stack, the instruction after it (the\n\t\t * nop) will propagate the new state to the shared CFI array.\n\t\t */\n\t\tnop = malloc(sizeof(*nop));\n\t\tif (!nop) {\n\t\t\tWARN(\"malloc failed\");\n\t\t\treturn -1;\n\t\t}\n\t\tmemset(nop, 0, sizeof(*nop));\n\t\tINIT_LIST_HEAD(&nop->alts);\n\t\tINIT_LIST_HEAD(&nop->stack_ops);\n\n\t\tnop->sec = special_alt->new_sec;\n\t\tnop->offset = special_alt->new_off + special_alt->new_len;\n\t\tnop->len = special_alt->orig_len - special_alt->new_len;\n\t\tnop->type = INSN_NOP;\n\t\tnop->func = orig_insn->func;\n\t\tnop->alt_group = new_alt_group;\n\t\tnop->ignore = orig_insn->ignore_alts;\n\t}\n\n\tif (!special_alt->new_len) {\n\t\t*new_insn = nop;\n\t\tgoto end;\n\t}\n\n\tinsn = *new_insn;\n\tsec_for_each_insn_from(file, insn) {\n\t\tstruct reloc *alt_reloc;\n\n\t\tif (insn->offset >= special_alt->new_off + special_alt->new_len)\n\t\t\tbreak;\n\n\t\tlast_new_insn = insn;\n\n\t\tinsn->ignore = orig_insn->ignore_alts;\n\t\tinsn->func = orig_insn->func;\n\t\tinsn->alt_group = new_alt_group;\n\n\t\t/*\n\t\t * Since alternative replacement code is copy/pasted by the\n\t\t * kernel after applying relocations, generally such code can't\n\t\t * have relative-address relocation references to outside the\n\t\t * .altinstr_replacement section, unless the arch's\n\t\t * alternatives code can adjust the relative offsets\n\t\t * accordingly.\n\t\t */\n\t\talt_reloc = insn_reloc(file, insn);\n\t\tif (alt_reloc &&\n\t\t    !arch_support_alt_relocation(special_alt, insn, alt_reloc)) {\n\n\t\t\tWARN_FUNC(\"unsupported relocation in alternatives section\",\n\t\t\t\t  insn->sec, insn->offset);\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (!is_static_jump(insn))\n\t\t\tcontinue;\n\n\t\tif (!insn->immediate)\n\t\t\tcontinue;\n\n\t\tdest_off = arch_jump_destination(insn);\n\t\tif (dest_off == special_alt->new_off + special_alt->new_len) {\n\t\t\tinsn->jump_dest = next_insn_same_sec(file, last_orig_insn);\n\t\t\tif (!insn->jump_dest) {\n\t\t\t\tWARN_FUNC(\"can't find alternative jump destination\",\n\t\t\t\t\t  insn->sec, insn->offset);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!last_new_insn) {\n\t\tWARN_FUNC(\"can't find last new alternative instruction\",\n\t\t\t  special_alt->new_sec, special_alt->new_off);\n\t\treturn -1;\n\t}\n\n\tif (nop)\n\t\tlist_add(&nop->list, &last_new_insn->list);\nend:\n\tnew_alt_group->orig_group = orig_alt_group;\n\tnew_alt_group->first_insn = *new_insn;\n\tnew_alt_group->last_insn = nop ? : last_new_insn;\n\tnew_alt_group->cfi = orig_alt_group->cfi;\n\treturn 0;\n}\n\n/*\n * A jump table entry can either convert a nop to a jump or a jump to a nop.\n * If the original instruction is a jump, make the alt entry an effective nop\n * by just skipping the original instruction.\n */\nstatic int handle_jump_alt(struct objtool_file *file,\n\t\t\t   struct special_alt *special_alt,\n\t\t\t   struct instruction *orig_insn,\n\t\t\t   struct instruction **new_insn)\n{\n\tif (orig_insn->type != INSN_JUMP_UNCONDITIONAL &&\n\t    orig_insn->type != INSN_NOP) {\n\n\t\tWARN_FUNC(\"unsupported instruction at jump label\",\n\t\t\t  orig_insn->sec, orig_insn->offset);\n\t\treturn -1;\n\t}\n\n\tif (opts.hack_jump_label && special_alt->key_addend & 2) {\n\t\tstruct reloc *reloc = insn_reloc(file, orig_insn);\n\n\t\tif (reloc) {\n\t\t\treloc->type = R_NONE;\n\t\t\telf_write_reloc(file->elf, reloc);\n\t\t}\n\t\telf_write_insn(file->elf, orig_insn->sec,\n\t\t\t       orig_insn->offset, orig_insn->len,\n\t\t\t       arch_nop_insn(orig_insn->len));\n\t\torig_insn->type = INSN_NOP;\n\t}\n\n\tif (orig_insn->type == INSN_NOP) {\n\t\tif (orig_insn->len == 2)\n\t\t\tfile->jl_nop_short++;\n\t\telse\n\t\t\tfile->jl_nop_long++;\n\n\t\treturn 0;\n\t}\n\n\tif (orig_insn->len == 2)\n\t\tfile->jl_short++;\n\telse\n\t\tfile->jl_long++;\n\n\t*new_insn = list_next_entry(orig_insn, list);\n\treturn 0;\n}\n\n/*\n * Read all the special sections which have alternate instructions which can be\n * patched in or redirected to at runtime.  Each instruction having alternate\n * instruction(s) has them added to its insn->alts list, which will be\n * traversed in validate_branch().\n */\nstatic int add_special_section_alts(struct objtool_file *file)\n{\n\tstruct list_head special_alts;\n\tstruct instruction *orig_insn, *new_insn;\n\tstruct special_alt *special_alt, *tmp;\n\tstruct alternative *alt;\n\tint ret;\n\n\tret = special_get_alts(file->elf, &special_alts);\n\tif (ret)\n\t\treturn ret;\n\n\tlist_for_each_entry_safe(special_alt, tmp, &special_alts, list) {\n\n\t\torig_insn = find_insn(file, special_alt->orig_sec,\n\t\t\t\t      special_alt->orig_off);\n\t\tif (!orig_insn) {\n\t\t\tWARN_FUNC(\"special: can't find orig instruction\",\n\t\t\t\t  special_alt->orig_sec, special_alt->orig_off);\n\t\t\tret = -1;\n\t\t\tgoto out;\n\t\t}\n\n\t\tnew_insn = NULL;\n\t\tif (!special_alt->group || special_alt->new_len) {\n\t\t\tnew_insn = find_insn(file, special_alt->new_sec,\n\t\t\t\t\t     special_alt->new_off);\n\t\t\tif (!new_insn) {\n\t\t\t\tWARN_FUNC(\"special: can't find new instruction\",\n\t\t\t\t\t  special_alt->new_sec,\n\t\t\t\t\t  special_alt->new_off);\n\t\t\t\tret = -1;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tif (special_alt->group) {\n\t\t\tif (!special_alt->orig_len) {\n\t\t\t\tWARN_FUNC(\"empty alternative entry\",\n\t\t\t\t\t  orig_insn->sec, orig_insn->offset);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = handle_group_alt(file, special_alt, orig_insn,\n\t\t\t\t\t       &new_insn);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t} else if (special_alt->jump_or_nop) {\n\t\t\tret = handle_jump_alt(file, special_alt, orig_insn,\n\t\t\t\t\t      &new_insn);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\talt = malloc(sizeof(*alt));\n\t\tif (!alt) {\n\t\t\tWARN(\"malloc failed\");\n\t\t\tret = -1;\n\t\t\tgoto out;\n\t\t}\n\n\t\talt->insn = new_insn;\n\t\talt->skip_orig = special_alt->skip_orig;\n\t\torig_insn->ignore_alts |= special_alt->skip_alt;\n\t\tlist_add_tail(&alt->list, &orig_insn->alts);\n\n\t\tlist_del(&special_alt->list);\n\t\tfree(special_alt);\n\t}\n\n\tif (opts.stats) {\n\t\tprintf(\"jl\\\\\\tNOP\\tJMP\\n\");\n\t\tprintf(\"short:\\t%ld\\t%ld\\n\", file->jl_nop_short, file->jl_short);\n\t\tprintf(\"long:\\t%ld\\t%ld\\n\", file->jl_nop_long, file->jl_long);\n\t}\n\nout:\n\treturn ret;\n}\n\nstatic int add_jump_table(struct objtool_file *file, struct instruction *insn,\n\t\t\t    struct reloc *table)\n{\n\tstruct reloc *reloc = table;\n\tstruct instruction *dest_insn;\n\tstruct alternative *alt;\n\tstruct symbol *pfunc = insn->func->pfunc;\n\tunsigned int prev_offset = 0;\n\n\t/*\n\t * Each @reloc is a switch table relocation which points to the target\n\t * instruction.\n\t */\n\tlist_for_each_entry_from(reloc, &table->sec->reloc_list, list) {\n\n\t\t/* Check for the end of the table: */\n\t\tif (reloc != table && reloc->jump_table_start)\n\t\t\tbreak;\n\n\t\t/* Make sure the table entries are consecutive: */\n\t\tif (prev_offset && reloc->offset != prev_offset + 8)\n\t\t\tbreak;\n\n\t\t/* Detect function pointers from contiguous objects: */\n\t\tif (reloc->sym->sec == pfunc->sec &&\n\t\t    reloc->addend == pfunc->offset)\n\t\t\tbreak;\n\n\t\tdest_insn = find_insn(file, reloc->sym->sec, reloc->addend);\n\t\tif (!dest_insn)\n\t\t\tbreak;\n\n\t\t/* Make sure the destination is in the same function: */\n\t\tif (!dest_insn->func || dest_insn->func->pfunc != pfunc)\n\t\t\tbreak;\n\n\t\talt = malloc(sizeof(*alt));\n\t\tif (!alt) {\n\t\t\tWARN(\"malloc failed\");\n\t\t\treturn -1;\n\t\t}\n\n\t\talt->insn = dest_insn;\n\t\tlist_add_tail(&alt->list, &insn->alts);\n\t\tprev_offset = reloc->offset;\n\t}\n\n\tif (!prev_offset) {\n\t\tWARN_FUNC(\"can't find switch jump table\",\n\t\t\t  insn->sec, insn->offset);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\n/*\n * find_jump_table() - Given a dynamic jump, find the switch jump table\n * associated with it.\n */\nstatic struct reloc *find_jump_table(struct objtool_file *file,\n\t\t\t\t      struct symbol *func,\n\t\t\t\t      struct instruction *insn)\n{\n\tstruct reloc *table_reloc;\n\tstruct instruction *dest_insn, *orig_insn = insn;\n\n\t/*\n\t * Backward search using the @first_jump_src links, these help avoid\n\t * much of the 'in between' code. Which avoids us getting confused by\n\t * it.\n\t */\n\tfor (;\n\t     insn && insn->func && insn->func->pfunc == func;\n\t     insn = insn->first_jump_src ?: prev_insn_same_sym(file, insn)) {\n\n\t\tif (insn != orig_insn && insn->type == INSN_JUMP_DYNAMIC)\n\t\t\tbreak;\n\n\t\t/* allow small jumps within the range */\n\t\tif (insn->type == INSN_JUMP_UNCONDITIONAL &&\n\t\t    insn->jump_dest &&\n\t\t    (insn->jump_dest->offset <= insn->offset ||\n\t\t     insn->jump_dest->offset > orig_insn->offset))\n\t\t    break;\n\n\t\ttable_reloc = arch_find_switch_table(file, insn);\n\t\tif (!table_reloc)\n\t\t\tcontinue;\n\t\tdest_insn = find_insn(file, table_reloc->sym->sec, table_reloc->addend);\n\t\tif (!dest_insn || !dest_insn->func || dest_insn->func->pfunc != func)\n\t\t\tcontinue;\n\n\t\treturn table_reloc;\n\t}\n\n\treturn NULL;\n}\n\n/*\n * First pass: Mark the head of each jump table so that in the next pass,\n * we know when a given jump table ends and the next one starts.\n */\nstatic void mark_func_jump_tables(struct objtool_file *file,\n\t\t\t\t    struct symbol *func)\n{\n\tstruct instruction *insn, *last = NULL;\n\tstruct reloc *reloc;\n\n\tfunc_for_each_insn(file, func, insn) {\n\t\tif (!last)\n\t\t\tlast = insn;\n\n\t\t/*\n\t\t * Store back-pointers for unconditional forward jumps such\n\t\t * that find_jump_table() can back-track using those and\n\t\t * avoid some potentially confusing code.\n\t\t */\n\t\tif (insn->type == INSN_JUMP_UNCONDITIONAL && insn->jump_dest &&\n\t\t    insn->offset > last->offset &&\n\t\t    insn->jump_dest->offset > insn->offset &&\n\t\t    !insn->jump_dest->first_jump_src) {\n\n\t\t\tinsn->jump_dest->first_jump_src = insn;\n\t\t\tlast = insn->jump_dest;\n\t\t}\n\n\t\tif (insn->type != INSN_JUMP_DYNAMIC)\n\t\t\tcontinue;\n\n\t\treloc = find_jump_table(file, func, insn);\n\t\tif (reloc) {\n\t\t\treloc->jump_table_start = true;\n\t\t\tinsn->jump_table = reloc;\n\t\t}\n\t}\n}\n\nstatic int add_func_jump_tables(struct objtool_file *file,\n\t\t\t\t  struct symbol *func)\n{\n\tstruct instruction *insn;\n\tint ret;\n\n\tfunc_for_each_insn(file, func, insn) {\n\t\tif (!insn->jump_table)\n\t\t\tcontinue;\n\n\t\tret = add_jump_table(file, insn, insn->jump_table);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n/*\n * For some switch statements, gcc generates a jump table in the .rodata\n * section which contains a list of addresses within the function to jump to.\n * This finds these jump tables and adds them to the insn->alts lists.\n */\nstatic int add_jump_table_alts(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct symbol *func;\n\tint ret;\n\n\tif (!file->rodata)\n\t\treturn 0;\n\n\tfor_each_sec(file, sec) {\n\t\tlist_for_each_entry(func, &sec->symbol_list, list) {\n\t\t\tif (func->type != STT_FUNC)\n\t\t\t\tcontinue;\n\n\t\t\tmark_func_jump_tables(file, func);\n\t\t\tret = add_func_jump_tables(file, func);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void set_func_state(struct cfi_state *state)\n{\n\tstate->cfa = initial_func_cfi.cfa;\n\tmemcpy(&state->regs, &initial_func_cfi.regs,\n\t       CFI_NUM_REGS * sizeof(struct cfi_reg));\n\tstate->stack_size = initial_func_cfi.cfa.offset;\n}\n\nstatic int read_unwind_hints(struct objtool_file *file)\n{\n\tstruct cfi_state cfi = init_cfi;\n\tstruct section *sec, *relocsec;\n\tstruct unwind_hint *hint;\n\tstruct instruction *insn;\n\tstruct reloc *reloc;\n\tint i;\n\n\tsec = find_section_by_name(file->elf, \".discard.unwind_hints\");\n\tif (!sec)\n\t\treturn 0;\n\n\trelocsec = sec->reloc;\n\tif (!relocsec) {\n\t\tWARN(\"missing .rela.discard.unwind_hints section\");\n\t\treturn -1;\n\t}\n\n\tif (sec->sh.sh_size % sizeof(struct unwind_hint)) {\n\t\tWARN(\"struct unwind_hint size mismatch\");\n\t\treturn -1;\n\t}\n\n\tfile->hints = true;\n\n\tfor (i = 0; i < sec->sh.sh_size / sizeof(struct unwind_hint); i++) {\n\t\thint = (struct unwind_hint *)sec->data->d_buf + i;\n\n\t\treloc = find_reloc_by_dest(file->elf, sec, i * sizeof(*hint));\n\t\tif (!reloc) {\n\t\t\tWARN(\"can't find reloc for unwind_hints[%d]\", i);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn = find_insn(file, reloc->sym->sec, reloc->addend);\n\t\tif (!insn) {\n\t\t\tWARN(\"can't find insn for unwind_hints[%d]\", i);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn->hint = true;\n\n\t\tif (opts.ibt && hint->type == UNWIND_HINT_TYPE_REGS_PARTIAL) {\n\t\t\tstruct symbol *sym = find_symbol_by_offset(insn->sec, insn->offset);\n\n\t\t\tif (sym && sym->bind == STB_GLOBAL &&\n\t\t\t    insn->type != INSN_ENDBR && !insn->noendbr) {\n\t\t\t\tWARN_FUNC(\"UNWIND_HINT_IRET_REGS without ENDBR\",\n\t\t\t\t\t  insn->sec, insn->offset);\n\t\t\t}\n\t\t}\n\n\t\tif (hint->type == UNWIND_HINT_TYPE_FUNC) {\n\t\t\tinsn->cfi = &func_cfi;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->cfi)\n\t\t\tcfi = *(insn->cfi);\n\n\t\tif (arch_decode_hint_reg(hint->sp_reg, &cfi.cfa.base)) {\n\t\t\tWARN_FUNC(\"unsupported unwind_hint sp base reg %d\",\n\t\t\t\t  insn->sec, insn->offset, hint->sp_reg);\n\t\t\treturn -1;\n\t\t}\n\n\t\tcfi.cfa.offset = bswap_if_needed(hint->sp_offset);\n\t\tcfi.type = hint->type;\n\t\tcfi.end = hint->end;\n\n\t\tinsn->cfi = cfi_hash_find_or_add(&cfi);\n\t}\n\n\treturn 0;\n}\n\nstatic int read_noendbr_hints(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct instruction *insn;\n\tstruct reloc *reloc;\n\n\tsec = find_section_by_name(file->elf, \".rela.discard.noendbr\");\n\tif (!sec)\n\t\treturn 0;\n\n\tlist_for_each_entry(reloc, &sec->reloc_list, list) {\n\t\tinsn = find_insn(file, reloc->sym->sec, reloc->sym->offset + reloc->addend);\n\t\tif (!insn) {\n\t\t\tWARN(\"bad .discard.noendbr entry\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (insn->type == INSN_ENDBR)\n\t\t\tWARN_FUNC(\"ANNOTATE_NOENDBR on ENDBR\", insn->sec, insn->offset);\n\n\t\tinsn->noendbr = 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int read_retpoline_hints(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct instruction *insn;\n\tstruct reloc *reloc;\n\n\tsec = find_section_by_name(file->elf, \".rela.discard.retpoline_safe\");\n\tif (!sec)\n\t\treturn 0;\n\n\tlist_for_each_entry(reloc, &sec->reloc_list, list) {\n\t\tif (reloc->sym->type != STT_SECTION) {\n\t\t\tWARN(\"unexpected relocation symbol type in %s\", sec->name);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn = find_insn(file, reloc->sym->sec, reloc->addend);\n\t\tif (!insn) {\n\t\t\tWARN(\"bad .discard.retpoline_safe entry\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (insn->type != INSN_JUMP_DYNAMIC &&\n\t\t    insn->type != INSN_CALL_DYNAMIC) {\n\t\t\tWARN_FUNC(\"retpoline_safe hint not an indirect jump/call\",\n\t\t\t\t  insn->sec, insn->offset);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn->retpoline_safe = true;\n\t}\n\n\treturn 0;\n}\n\nstatic int read_instr_hints(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct instruction *insn;\n\tstruct reloc *reloc;\n\n\tsec = find_section_by_name(file->elf, \".rela.discard.instr_end\");\n\tif (!sec)\n\t\treturn 0;\n\n\tlist_for_each_entry(reloc, &sec->reloc_list, list) {\n\t\tif (reloc->sym->type != STT_SECTION) {\n\t\t\tWARN(\"unexpected relocation symbol type in %s\", sec->name);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn = find_insn(file, reloc->sym->sec, reloc->addend);\n\t\tif (!insn) {\n\t\t\tWARN(\"bad .discard.instr_end entry\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn->instr--;\n\t}\n\n\tsec = find_section_by_name(file->elf, \".rela.discard.instr_begin\");\n\tif (!sec)\n\t\treturn 0;\n\n\tlist_for_each_entry(reloc, &sec->reloc_list, list) {\n\t\tif (reloc->sym->type != STT_SECTION) {\n\t\t\tWARN(\"unexpected relocation symbol type in %s\", sec->name);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn = find_insn(file, reloc->sym->sec, reloc->addend);\n\t\tif (!insn) {\n\t\t\tWARN(\"bad .discard.instr_begin entry\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn->instr++;\n\t}\n\n\treturn 0;\n}\n\nstatic int read_intra_function_calls(struct objtool_file *file)\n{\n\tstruct instruction *insn;\n\tstruct section *sec;\n\tstruct reloc *reloc;\n\n\tsec = find_section_by_name(file->elf, \".rela.discard.intra_function_calls\");\n\tif (!sec)\n\t\treturn 0;\n\n\tlist_for_each_entry(reloc, &sec->reloc_list, list) {\n\t\tunsigned long dest_off;\n\n\t\tif (reloc->sym->type != STT_SECTION) {\n\t\t\tWARN(\"unexpected relocation symbol type in %s\",\n\t\t\t     sec->name);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn = find_insn(file, reloc->sym->sec, reloc->addend);\n\t\tif (!insn) {\n\t\t\tWARN(\"bad .discard.intra_function_call entry\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (insn->type != INSN_CALL) {\n\t\t\tWARN_FUNC(\"intra_function_call not a direct call\",\n\t\t\t\t  insn->sec, insn->offset);\n\t\t\treturn -1;\n\t\t}\n\n\t\t/*\n\t\t * Treat intra-function CALLs as JMPs, but with a stack_op.\n\t\t * See add_call_destinations(), which strips stack_ops from\n\t\t * normal CALLs.\n\t\t */\n\t\tinsn->type = INSN_JUMP_UNCONDITIONAL;\n\n\t\tdest_off = insn->offset + insn->len + insn->immediate;\n\t\tinsn->jump_dest = find_insn(file, insn->sec, dest_off);\n\t\tif (!insn->jump_dest) {\n\t\t\tWARN_FUNC(\"can't find call dest at %s+0x%lx\",\n\t\t\t\t  insn->sec, insn->offset,\n\t\t\t\t  insn->sec->name, dest_off);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * Return true if name matches an instrumentation function, where calls to that\n * function from noinstr code can safely be removed, but compilers won't do so.\n */\nstatic bool is_profiling_func(const char *name)\n{\n\t/*\n\t * Many compilers cannot disable KCOV with a function attribute.\n\t */\n\tif (!strncmp(name, \"__sanitizer_cov_\", 16))\n\t\treturn true;\n\n\t/*\n\t * Some compilers currently do not remove __tsan_func_entry/exit nor\n\t * __tsan_atomic_signal_fence (used for barrier instrumentation) with\n\t * the __no_sanitize_thread attribute, remove them. Once the kernel's\n\t * minimum Clang version is 14.0, this can be removed.\n\t */\n\tif (!strncmp(name, \"__tsan_func_\", 12) ||\n\t    !strcmp(name, \"__tsan_atomic_signal_fence\"))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int classify_symbols(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct symbol *func;\n\n\tfor_each_sec(file, sec) {\n\t\tlist_for_each_entry(func, &sec->symbol_list, list) {\n\t\t\tif (func->bind != STB_GLOBAL)\n\t\t\t\tcontinue;\n\n\t\t\tif (!strncmp(func->name, STATIC_CALL_TRAMP_PREFIX_STR,\n\t\t\t\t     strlen(STATIC_CALL_TRAMP_PREFIX_STR)))\n\t\t\t\tfunc->static_call_tramp = true;\n\n\t\t\tif (arch_is_retpoline(func))\n\t\t\t\tfunc->retpoline_thunk = true;\n\n\t\t\tif (!strcmp(func->name, \"__fentry__\"))\n\t\t\t\tfunc->fentry = true;\n\n\t\t\tif (is_profiling_func(func->name))\n\t\t\t\tfunc->profiling_func = true;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void mark_rodata(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tbool found = false;\n\n\t/*\n\t * Search for the following rodata sections, each of which can\n\t * potentially contain jump tables:\n\t *\n\t * - .rodata: can contain GCC switch tables\n\t * - .rodata.<func>: same, if -fdata-sections is being used\n\t * - .rodata..c_jump_table: contains C annotated jump tables\n\t *\n\t * .rodata.str1.* sections are ignored; they don't contain jump tables.\n\t */\n\tfor_each_sec(file, sec) {\n\t\tif (!strncmp(sec->name, \".rodata\", 7) &&\n\t\t    !strstr(sec->name, \".str1.\")) {\n\t\t\tsec->rodata = true;\n\t\t\tfound = true;\n\t\t}\n\t}\n\n\tfile->rodata = found;\n}\n\nstatic int decode_sections(struct objtool_file *file)\n{\n\tint ret;\n\n\tmark_rodata(file);\n\n\tret = init_pv_ops(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = decode_instructions(file);\n\tif (ret)\n\t\treturn ret;\n\n\tadd_ignores(file);\n\tadd_uaccess_safe(file);\n\n\tret = add_ignore_alternatives(file);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Must be before read_unwind_hints() since that needs insn->noendbr.\n\t */\n\tret = read_noendbr_hints(file);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Must be before add_{jump_call}_destination.\n\t */\n\tret = classify_symbols(file);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Must be before add_jump_destinations(), which depends on 'func'\n\t * being set for alternatives, to enable proper sibling call detection.\n\t */\n\tret = add_special_section_alts(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = add_jump_destinations(file);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Must be before add_call_destination(); it changes INSN_CALL to\n\t * INSN_JUMP.\n\t */\n\tret = read_intra_function_calls(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = add_call_destinations(file);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Must be after add_call_destinations() such that it can override\n\t * dead_end_function() marks.\n\t */\n\tret = add_dead_ends(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = add_jump_table_alts(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = read_unwind_hints(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = read_retpoline_hints(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = read_instr_hints(file);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nstatic bool is_fentry_call(struct instruction *insn)\n{\n\tif (insn->type == INSN_CALL &&\n\t    insn->call_dest &&\n\t    insn->call_dest->fentry)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool has_modified_stack_frame(struct instruction *insn, struct insn_state *state)\n{\n\tstruct cfi_state *cfi = &state->cfi;\n\tint i;\n\n\tif (cfi->cfa.base != initial_func_cfi.cfa.base || cfi->drap)\n\t\treturn true;\n\n\tif (cfi->cfa.offset != initial_func_cfi.cfa.offset)\n\t\treturn true;\n\n\tif (cfi->stack_size != initial_func_cfi.cfa.offset)\n\t\treturn true;\n\n\tfor (i = 0; i < CFI_NUM_REGS; i++) {\n\t\tif (cfi->regs[i].base != initial_func_cfi.regs[i].base ||\n\t\t    cfi->regs[i].offset != initial_func_cfi.regs[i].offset)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool check_reg_frame_pos(const struct cfi_reg *reg,\n\t\t\t\tint expected_offset)\n{\n\treturn reg->base == CFI_CFA &&\n\t       reg->offset == expected_offset;\n}\n\nstatic bool has_valid_stack_frame(struct insn_state *state)\n{\n\tstruct cfi_state *cfi = &state->cfi;\n\n\tif (cfi->cfa.base == CFI_BP &&\n\t    check_reg_frame_pos(&cfi->regs[CFI_BP], -cfi->cfa.offset) &&\n\t    check_reg_frame_pos(&cfi->regs[CFI_RA], -cfi->cfa.offset + 8))\n\t\treturn true;\n\n\tif (cfi->drap && cfi->regs[CFI_BP].base == CFI_BP)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int update_cfi_state_regs(struct instruction *insn,\n\t\t\t\t  struct cfi_state *cfi,\n\t\t\t\t  struct stack_op *op)\n{\n\tstruct cfi_reg *cfa = &cfi->cfa;\n\n\tif (cfa->base != CFI_SP && cfa->base != CFI_SP_INDIRECT)\n\t\treturn 0;\n\n\t/* push */\n\tif (op->dest.type == OP_DEST_PUSH || op->dest.type == OP_DEST_PUSHF)\n\t\tcfa->offset += 8;\n\n\t/* pop */\n\tif (op->src.type == OP_SRC_POP || op->src.type == OP_SRC_POPF)\n\t\tcfa->offset -= 8;\n\n\t/* add immediate to sp */\n\tif (op->dest.type == OP_DEST_REG && op->src.type == OP_SRC_ADD &&\n\t    op->dest.reg == CFI_SP && op->src.reg == CFI_SP)\n\t\tcfa->offset -= op->src.offset;\n\n\treturn 0;\n}\n\nstatic void save_reg(struct cfi_state *cfi, unsigned char reg, int base, int offset)\n{\n\tif (arch_callee_saved_reg(reg) &&\n\t    cfi->regs[reg].base == CFI_UNDEFINED) {\n\t\tcfi->regs[reg].base = base;\n\t\tcfi->regs[reg].offset = offset;\n\t}\n}\n\nstatic void restore_reg(struct cfi_state *cfi, unsigned char reg)\n{\n\tcfi->regs[reg].base = initial_func_cfi.regs[reg].base;\n\tcfi->regs[reg].offset = initial_func_cfi.regs[reg].offset;\n}\n\n/*\n * A note about DRAP stack alignment:\n *\n * GCC has the concept of a DRAP register, which is used to help keep track of\n * the stack pointer when aligning the stack.  r10 or r13 is used as the DRAP\n * register.  The typical DRAP pattern is:\n *\n *   4c 8d 54 24 08\t\tlea    0x8(%rsp),%r10\n *   48 83 e4 c0\t\tand    $0xffffffffffffffc0,%rsp\n *   41 ff 72 f8\t\tpushq  -0x8(%r10)\n *   55\t\t\t\tpush   %rbp\n *   48 89 e5\t\t\tmov    %rsp,%rbp\n *\t\t\t\t(more pushes)\n *   41 52\t\t\tpush   %r10\n *\t\t\t\t...\n *   41 5a\t\t\tpop    %r10\n *\t\t\t\t(more pops)\n *   5d\t\t\t\tpop    %rbp\n *   49 8d 62 f8\t\tlea    -0x8(%r10),%rsp\n *   c3\t\t\t\tretq\n *\n * There are some variations in the epilogues, like:\n *\n *   5b\t\t\t\tpop    %rbx\n *   41 5a\t\t\tpop    %r10\n *   41 5c\t\t\tpop    %r12\n *   41 5d\t\t\tpop    %r13\n *   41 5e\t\t\tpop    %r14\n *   c9\t\t\t\tleaveq\n *   49 8d 62 f8\t\tlea    -0x8(%r10),%rsp\n *   c3\t\t\t\tretq\n *\n * and:\n *\n *   4c 8b 55 e8\t\tmov    -0x18(%rbp),%r10\n *   48 8b 5d e0\t\tmov    -0x20(%rbp),%rbx\n *   4c 8b 65 f0\t\tmov    -0x10(%rbp),%r12\n *   4c 8b 6d f8\t\tmov    -0x8(%rbp),%r13\n *   c9\t\t\t\tleaveq\n *   49 8d 62 f8\t\tlea    -0x8(%r10),%rsp\n *   c3\t\t\t\tretq\n *\n * Sometimes r13 is used as the DRAP register, in which case it's saved and\n * restored beforehand:\n *\n *   41 55\t\t\tpush   %r13\n *   4c 8d 6c 24 10\t\tlea    0x10(%rsp),%r13\n *   48 83 e4 f0\t\tand    $0xfffffffffffffff0,%rsp\n *\t\t\t\t...\n *   49 8d 65 f0\t\tlea    -0x10(%r13),%rsp\n *   41 5d\t\t\tpop    %r13\n *   c3\t\t\t\tretq\n */\nstatic int update_cfi_state(struct instruction *insn,\n\t\t\t    struct instruction *next_insn,\n\t\t\t    struct cfi_state *cfi, struct stack_op *op)\n{\n\tstruct cfi_reg *cfa = &cfi->cfa;\n\tstruct cfi_reg *regs = cfi->regs;\n\n\t/* stack operations don't make sense with an undefined CFA */\n\tif (cfa->base == CFI_UNDEFINED) {\n\t\tif (insn->func) {\n\t\t\tWARN_FUNC(\"undefined stack state\", insn->sec, insn->offset);\n\t\t\treturn -1;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (cfi->type == UNWIND_HINT_TYPE_REGS ||\n\t    cfi->type == UNWIND_HINT_TYPE_REGS_PARTIAL)\n\t\treturn update_cfi_state_regs(insn, cfi, op);\n\n\tswitch (op->dest.type) {\n\n\tcase OP_DEST_REG:\n\t\tswitch (op->src.type) {\n\n\t\tcase OP_SRC_REG:\n\t\t\tif (op->src.reg == CFI_SP && op->dest.reg == CFI_BP &&\n\t\t\t    cfa->base == CFI_SP &&\n\t\t\t    check_reg_frame_pos(&regs[CFI_BP], -cfa->offset)) {\n\n\t\t\t\t/* mov %rsp, %rbp */\n\t\t\t\tcfa->base = op->dest.reg;\n\t\t\t\tcfi->bp_scratch = false;\n\t\t\t}\n\n\t\t\telse if (op->src.reg == CFI_SP &&\n\t\t\t\t op->dest.reg == CFI_BP && cfi->drap) {\n\n\t\t\t\t/* drap: mov %rsp, %rbp */\n\t\t\t\tregs[CFI_BP].base = CFI_BP;\n\t\t\t\tregs[CFI_BP].offset = -cfi->stack_size;\n\t\t\t\tcfi->bp_scratch = false;\n\t\t\t}\n\n\t\t\telse if (op->src.reg == CFI_SP && cfa->base == CFI_SP) {\n\n\t\t\t\t/*\n\t\t\t\t * mov %rsp, %reg\n\t\t\t\t *\n\t\t\t\t * This is needed for the rare case where GCC\n\t\t\t\t * does:\n\t\t\t\t *\n\t\t\t\t *   mov    %rsp, %rax\n\t\t\t\t *   ...\n\t\t\t\t *   mov    %rax, %rsp\n\t\t\t\t */\n\t\t\t\tcfi->vals[op->dest.reg].base = CFI_CFA;\n\t\t\t\tcfi->vals[op->dest.reg].offset = -cfi->stack_size;\n\t\t\t}\n\n\t\t\telse if (op->src.reg == CFI_BP && op->dest.reg == CFI_SP &&\n\t\t\t\t (cfa->base == CFI_BP || cfa->base == cfi->drap_reg)) {\n\n\t\t\t\t/*\n\t\t\t\t * mov %rbp, %rsp\n\t\t\t\t *\n\t\t\t\t * Restore the original stack pointer (Clang).\n\t\t\t\t */\n\t\t\t\tcfi->stack_size = -cfi->regs[CFI_BP].offset;\n\t\t\t}\n\n\t\t\telse if (op->dest.reg == cfa->base) {\n\n\t\t\t\t/* mov %reg, %rsp */\n\t\t\t\tif (cfa->base == CFI_SP &&\n\t\t\t\t    cfi->vals[op->src.reg].base == CFI_CFA) {\n\n\t\t\t\t\t/*\n\t\t\t\t\t * This is needed for the rare case\n\t\t\t\t\t * where GCC does something dumb like:\n\t\t\t\t\t *\n\t\t\t\t\t *   lea    0x8(%rsp), %rcx\n\t\t\t\t\t *   ...\n\t\t\t\t\t *   mov    %rcx, %rsp\n\t\t\t\t\t */\n\t\t\t\t\tcfa->offset = -cfi->vals[op->src.reg].offset;\n\t\t\t\t\tcfi->stack_size = cfa->offset;\n\n\t\t\t\t} else if (cfa->base == CFI_SP &&\n\t\t\t\t\t   cfi->vals[op->src.reg].base == CFI_SP_INDIRECT &&\n\t\t\t\t\t   cfi->vals[op->src.reg].offset == cfa->offset) {\n\n\t\t\t\t\t/*\n\t\t\t\t\t * Stack swizzle:\n\t\t\t\t\t *\n\t\t\t\t\t * 1: mov %rsp, (%[tos])\n\t\t\t\t\t * 2: mov %[tos], %rsp\n\t\t\t\t\t *    ...\n\t\t\t\t\t * 3: pop %rsp\n\t\t\t\t\t *\n\t\t\t\t\t * Where:\n\t\t\t\t\t *\n\t\t\t\t\t * 1 - places a pointer to the previous\n\t\t\t\t\t *     stack at the Top-of-Stack of the\n\t\t\t\t\t *     new stack.\n\t\t\t\t\t *\n\t\t\t\t\t * 2 - switches to the new stack.\n\t\t\t\t\t *\n\t\t\t\t\t * 3 - pops the Top-of-Stack to restore\n\t\t\t\t\t *     the original stack.\n\t\t\t\t\t *\n\t\t\t\t\t * Note: we set base to SP_INDIRECT\n\t\t\t\t\t * here and preserve offset. Therefore\n\t\t\t\t\t * when the unwinder reaches ToS it\n\t\t\t\t\t * will dereference SP and then add the\n\t\t\t\t\t * offset to find the next frame, IOW:\n\t\t\t\t\t * (%rsp) + offset.\n\t\t\t\t\t */\n\t\t\t\t\tcfa->base = CFI_SP_INDIRECT;\n\n\t\t\t\t} else {\n\t\t\t\t\tcfa->base = CFI_UNDEFINED;\n\t\t\t\t\tcfa->offset = 0;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\telse if (op->dest.reg == CFI_SP &&\n\t\t\t\t cfi->vals[op->src.reg].base == CFI_SP_INDIRECT &&\n\t\t\t\t cfi->vals[op->src.reg].offset == cfa->offset) {\n\n\t\t\t\t/*\n\t\t\t\t * The same stack swizzle case 2) as above. But\n\t\t\t\t * because we can't change cfa->base, case 3)\n\t\t\t\t * will become a regular POP. Pretend we're a\n\t\t\t\t * PUSH so things don't go unbalanced.\n\t\t\t\t */\n\t\t\t\tcfi->stack_size += 8;\n\t\t\t}\n\n\n\t\t\tbreak;\n\n\t\tcase OP_SRC_ADD:\n\t\t\tif (op->dest.reg == CFI_SP && op->src.reg == CFI_SP) {\n\n\t\t\t\t/* add imm, %rsp */\n\t\t\t\tcfi->stack_size -= op->src.offset;\n\t\t\t\tif (cfa->base == CFI_SP)\n\t\t\t\t\tcfa->offset -= op->src.offset;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (op->dest.reg == CFI_SP && op->src.reg == CFI_BP) {\n\n\t\t\t\t/* lea disp(%rbp), %rsp */\n\t\t\t\tcfi->stack_size = -(op->src.offset + regs[CFI_BP].offset);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (!cfi->drap && op->src.reg == CFI_SP &&\n\t\t\t    op->dest.reg == CFI_BP && cfa->base == CFI_SP &&\n\t\t\t    check_reg_frame_pos(&regs[CFI_BP], -cfa->offset + op->src.offset)) {\n\n\t\t\t\t/* lea disp(%rsp), %rbp */\n\t\t\t\tcfa->base = CFI_BP;\n\t\t\t\tcfa->offset -= op->src.offset;\n\t\t\t\tcfi->bp_scratch = false;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (op->src.reg == CFI_SP && cfa->base == CFI_SP) {\n\n\t\t\t\t/* drap: lea disp(%rsp), %drap */\n\t\t\t\tcfi->drap_reg = op->dest.reg;\n\n\t\t\t\t/*\n\t\t\t\t * lea disp(%rsp), %reg\n\t\t\t\t *\n\t\t\t\t * This is needed for the rare case where GCC\n\t\t\t\t * does something dumb like:\n\t\t\t\t *\n\t\t\t\t *   lea    0x8(%rsp), %rcx\n\t\t\t\t *   ...\n\t\t\t\t *   mov    %rcx, %rsp\n\t\t\t\t */\n\t\t\t\tcfi->vals[op->dest.reg].base = CFI_CFA;\n\t\t\t\tcfi->vals[op->dest.reg].offset = \\\n\t\t\t\t\t-cfi->stack_size + op->src.offset;\n\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (cfi->drap && op->dest.reg == CFI_SP &&\n\t\t\t    op->src.reg == cfi->drap_reg) {\n\n\t\t\t\t /* drap: lea disp(%drap), %rsp */\n\t\t\t\tcfa->base = CFI_SP;\n\t\t\t\tcfa->offset = cfi->stack_size = -op->src.offset;\n\t\t\t\tcfi->drap_reg = CFI_UNDEFINED;\n\t\t\t\tcfi->drap = false;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (op->dest.reg == cfi->cfa.base && !(next_insn && next_insn->hint)) {\n\t\t\t\tWARN_FUNC(\"unsupported stack register modification\",\n\t\t\t\t\t  insn->sec, insn->offset);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tbreak;\n\n\t\tcase OP_SRC_AND:\n\t\t\tif (op->dest.reg != CFI_SP ||\n\t\t\t    (cfi->drap_reg != CFI_UNDEFINED && cfa->base != CFI_SP) ||\n\t\t\t    (cfi->drap_reg == CFI_UNDEFINED && cfa->base != CFI_BP)) {\n\t\t\t\tWARN_FUNC(\"unsupported stack pointer realignment\",\n\t\t\t\t\t  insn->sec, insn->offset);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tif (cfi->drap_reg != CFI_UNDEFINED) {\n\t\t\t\t/* drap: and imm, %rsp */\n\t\t\t\tcfa->base = cfi->drap_reg;\n\t\t\t\tcfa->offset = cfi->stack_size = 0;\n\t\t\t\tcfi->drap = true;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Older versions of GCC (4.8ish) realign the stack\n\t\t\t * without DRAP, with a frame pointer.\n\t\t\t */\n\n\t\t\tbreak;\n\n\t\tcase OP_SRC_POP:\n\t\tcase OP_SRC_POPF:\n\t\t\tif (op->dest.reg == CFI_SP && cfa->base == CFI_SP_INDIRECT) {\n\n\t\t\t\t/* pop %rsp; # restore from a stack swizzle */\n\t\t\t\tcfa->base = CFI_SP;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (!cfi->drap && op->dest.reg == cfa->base) {\n\n\t\t\t\t/* pop %rbp */\n\t\t\t\tcfa->base = CFI_SP;\n\t\t\t}\n\n\t\t\tif (cfi->drap && cfa->base == CFI_BP_INDIRECT &&\n\t\t\t    op->dest.reg == cfi->drap_reg &&\n\t\t\t    cfi->drap_offset == -cfi->stack_size) {\n\n\t\t\t\t/* drap: pop %drap */\n\t\t\t\tcfa->base = cfi->drap_reg;\n\t\t\t\tcfa->offset = 0;\n\t\t\t\tcfi->drap_offset = -1;\n\n\t\t\t} else if (cfi->stack_size == -regs[op->dest.reg].offset) {\n\n\t\t\t\t/* pop %reg */\n\t\t\t\trestore_reg(cfi, op->dest.reg);\n\t\t\t}\n\n\t\t\tcfi->stack_size -= 8;\n\t\t\tif (cfa->base == CFI_SP)\n\t\t\t\tcfa->offset -= 8;\n\n\t\t\tbreak;\n\n\t\tcase OP_SRC_REG_INDIRECT:\n\t\t\tif (!cfi->drap && op->dest.reg == cfa->base &&\n\t\t\t    op->dest.reg == CFI_BP) {\n\n\t\t\t\t/* mov disp(%rsp), %rbp */\n\t\t\t\tcfa->base = CFI_SP;\n\t\t\t\tcfa->offset = cfi->stack_size;\n\t\t\t}\n\n\t\t\tif (cfi->drap && op->src.reg == CFI_BP &&\n\t\t\t    op->src.offset == cfi->drap_offset) {\n\n\t\t\t\t/* drap: mov disp(%rbp), %drap */\n\t\t\t\tcfa->base = cfi->drap_reg;\n\t\t\t\tcfa->offset = 0;\n\t\t\t\tcfi->drap_offset = -1;\n\t\t\t}\n\n\t\t\tif (cfi->drap && op->src.reg == CFI_BP &&\n\t\t\t    op->src.offset == regs[op->dest.reg].offset) {\n\n\t\t\t\t/* drap: mov disp(%rbp), %reg */\n\t\t\t\trestore_reg(cfi, op->dest.reg);\n\n\t\t\t} else if (op->src.reg == cfa->base &&\n\t\t\t    op->src.offset == regs[op->dest.reg].offset + cfa->offset) {\n\n\t\t\t\t/* mov disp(%rbp), %reg */\n\t\t\t\t/* mov disp(%rsp), %reg */\n\t\t\t\trestore_reg(cfi, op->dest.reg);\n\n\t\t\t} else if (op->src.reg == CFI_SP &&\n\t\t\t\t   op->src.offset == regs[op->dest.reg].offset + cfi->stack_size) {\n\n\t\t\t\t/* mov disp(%rsp), %reg */\n\t\t\t\trestore_reg(cfi, op->dest.reg);\n\t\t\t}\n\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tWARN_FUNC(\"unknown stack-related instruction\",\n\t\t\t\t  insn->sec, insn->offset);\n\t\t\treturn -1;\n\t\t}\n\n\t\tbreak;\n\n\tcase OP_DEST_PUSH:\n\tcase OP_DEST_PUSHF:\n\t\tcfi->stack_size += 8;\n\t\tif (cfa->base == CFI_SP)\n\t\t\tcfa->offset += 8;\n\n\t\tif (op->src.type != OP_SRC_REG)\n\t\t\tbreak;\n\n\t\tif (cfi->drap) {\n\t\t\tif (op->src.reg == cfa->base && op->src.reg == cfi->drap_reg) {\n\n\t\t\t\t/* drap: push %drap */\n\t\t\t\tcfa->base = CFI_BP_INDIRECT;\n\t\t\t\tcfa->offset = -cfi->stack_size;\n\n\t\t\t\t/* save drap so we know when to restore it */\n\t\t\t\tcfi->drap_offset = -cfi->stack_size;\n\n\t\t\t} else if (op->src.reg == CFI_BP && cfa->base == cfi->drap_reg) {\n\n\t\t\t\t/* drap: push %rbp */\n\t\t\t\tcfi->stack_size = 0;\n\n\t\t\t} else {\n\n\t\t\t\t/* drap: push %reg */\n\t\t\t\tsave_reg(cfi, op->src.reg, CFI_BP, -cfi->stack_size);\n\t\t\t}\n\n\t\t} else {\n\n\t\t\t/* push %reg */\n\t\t\tsave_reg(cfi, op->src.reg, CFI_CFA, -cfi->stack_size);\n\t\t}\n\n\t\t/* detect when asm code uses rbp as a scratch register */\n\t\tif (opts.stackval && insn->func && op->src.reg == CFI_BP &&\n\t\t    cfa->base != CFI_BP)\n\t\t\tcfi->bp_scratch = true;\n\t\tbreak;\n\n\tcase OP_DEST_REG_INDIRECT:\n\n\t\tif (cfi->drap) {\n\t\t\tif (op->src.reg == cfa->base && op->src.reg == cfi->drap_reg) {\n\n\t\t\t\t/* drap: mov %drap, disp(%rbp) */\n\t\t\t\tcfa->base = CFI_BP_INDIRECT;\n\t\t\t\tcfa->offset = op->dest.offset;\n\n\t\t\t\t/* save drap offset so we know when to restore it */\n\t\t\t\tcfi->drap_offset = op->dest.offset;\n\t\t\t} else {\n\n\t\t\t\t/* drap: mov reg, disp(%rbp) */\n\t\t\t\tsave_reg(cfi, op->src.reg, CFI_BP, op->dest.offset);\n\t\t\t}\n\n\t\t} else if (op->dest.reg == cfa->base) {\n\n\t\t\t/* mov reg, disp(%rbp) */\n\t\t\t/* mov reg, disp(%rsp) */\n\t\t\tsave_reg(cfi, op->src.reg, CFI_CFA,\n\t\t\t\t op->dest.offset - cfi->cfa.offset);\n\n\t\t} else if (op->dest.reg == CFI_SP) {\n\n\t\t\t/* mov reg, disp(%rsp) */\n\t\t\tsave_reg(cfi, op->src.reg, CFI_CFA,\n\t\t\t\t op->dest.offset - cfi->stack_size);\n\n\t\t} else if (op->src.reg == CFI_SP && op->dest.offset == 0) {\n\n\t\t\t/* mov %rsp, (%reg); # setup a stack swizzle. */\n\t\t\tcfi->vals[op->dest.reg].base = CFI_SP_INDIRECT;\n\t\t\tcfi->vals[op->dest.reg].offset = cfa->offset;\n\t\t}\n\n\t\tbreak;\n\n\tcase OP_DEST_MEM:\n\t\tif (op->src.type != OP_SRC_POP && op->src.type != OP_SRC_POPF) {\n\t\t\tWARN_FUNC(\"unknown stack-related memory operation\",\n\t\t\t\t  insn->sec, insn->offset);\n\t\t\treturn -1;\n\t\t}\n\n\t\t/* pop mem */\n\t\tcfi->stack_size -= 8;\n\t\tif (cfa->base == CFI_SP)\n\t\t\tcfa->offset -= 8;\n\n\t\tbreak;\n\n\tdefault:\n\t\tWARN_FUNC(\"unknown stack-related instruction\",\n\t\t\t  insn->sec, insn->offset);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\n/*\n * The stack layouts of alternatives instructions can sometimes diverge when\n * they have stack modifications.  That's fine as long as the potential stack\n * layouts don't conflict at any given potential instruction boundary.\n *\n * Flatten the CFIs of the different alternative code streams (both original\n * and replacement) into a single shared CFI array which can be used to detect\n * conflicts and nicely feed a linear array of ORC entries to the unwinder.\n */\nstatic int propagate_alt_cfi(struct objtool_file *file, struct instruction *insn)\n{\n\tstruct cfi_state **alt_cfi;\n\tint group_off;\n\n\tif (!insn->alt_group)\n\t\treturn 0;\n\n\tif (!insn->cfi) {\n\t\tWARN(\"CFI missing\");\n\t\treturn -1;\n\t}\n\n\talt_cfi = insn->alt_group->cfi;\n\tgroup_off = insn->offset - insn->alt_group->first_insn->offset;\n\n\tif (!alt_cfi[group_off]) {\n\t\talt_cfi[group_off] = insn->cfi;\n\t} else {\n\t\tif (cficmp(alt_cfi[group_off], insn->cfi)) {\n\t\t\tWARN_FUNC(\"stack layout conflict in alternatives\",\n\t\t\t\t  insn->sec, insn->offset);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int handle_insn_ops(struct instruction *insn,\n\t\t\t   struct instruction *next_insn,\n\t\t\t   struct insn_state *state)\n{\n\tstruct stack_op *op;\n\n\tlist_for_each_entry(op, &insn->stack_ops, list) {\n\n\t\tif (update_cfi_state(insn, next_insn, &state->cfi, op))\n\t\t\treturn 1;\n\n\t\tif (!insn->alt_group)\n\t\t\tcontinue;\n\n\t\tif (op->dest.type == OP_DEST_PUSHF) {\n\t\t\tif (!state->uaccess_stack) {\n\t\t\t\tstate->uaccess_stack = 1;\n\t\t\t} else if (state->uaccess_stack >> 31) {\n\t\t\t\tWARN_FUNC(\"PUSHF stack exhausted\",\n\t\t\t\t\t  insn->sec, insn->offset);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tstate->uaccess_stack <<= 1;\n\t\t\tstate->uaccess_stack  |= state->uaccess;\n\t\t}\n\n\t\tif (op->src.type == OP_SRC_POPF) {\n\t\t\tif (state->uaccess_stack) {\n\t\t\t\tstate->uaccess = state->uaccess_stack & 1;\n\t\t\t\tstate->uaccess_stack >>= 1;\n\t\t\t\tif (state->uaccess_stack == 1)\n\t\t\t\t\tstate->uaccess_stack = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic bool insn_cfi_match(struct instruction *insn, struct cfi_state *cfi2)\n{\n\tstruct cfi_state *cfi1 = insn->cfi;\n\tint i;\n\n\tif (!cfi1) {\n\t\tWARN(\"CFI missing\");\n\t\treturn false;\n\t}\n\n\tif (memcmp(&cfi1->cfa, &cfi2->cfa, sizeof(cfi1->cfa))) {\n\n\t\tWARN_FUNC(\"stack state mismatch: cfa1=%d%+d cfa2=%d%+d\",\n\t\t\t  insn->sec, insn->offset,\n\t\t\t  cfi1->cfa.base, cfi1->cfa.offset,\n\t\t\t  cfi2->cfa.base, cfi2->cfa.offset);\n\n\t} else if (memcmp(&cfi1->regs, &cfi2->regs, sizeof(cfi1->regs))) {\n\t\tfor (i = 0; i < CFI_NUM_REGS; i++) {\n\t\t\tif (!memcmp(&cfi1->regs[i], &cfi2->regs[i],\n\t\t\t\t    sizeof(struct cfi_reg)))\n\t\t\t\tcontinue;\n\n\t\t\tWARN_FUNC(\"stack state mismatch: reg1[%d]=%d%+d reg2[%d]=%d%+d\",\n\t\t\t\t  insn->sec, insn->offset,\n\t\t\t\t  i, cfi1->regs[i].base, cfi1->regs[i].offset,\n\t\t\t\t  i, cfi2->regs[i].base, cfi2->regs[i].offset);\n\t\t\tbreak;\n\t\t}\n\n\t} else if (cfi1->type != cfi2->type) {\n\n\t\tWARN_FUNC(\"stack state mismatch: type1=%d type2=%d\",\n\t\t\t  insn->sec, insn->offset, cfi1->type, cfi2->type);\n\n\t} else if (cfi1->drap != cfi2->drap ||\n\t\t   (cfi1->drap && cfi1->drap_reg != cfi2->drap_reg) ||\n\t\t   (cfi1->drap && cfi1->drap_offset != cfi2->drap_offset)) {\n\n\t\tWARN_FUNC(\"stack state mismatch: drap1=%d(%d,%d) drap2=%d(%d,%d)\",\n\t\t\t  insn->sec, insn->offset,\n\t\t\t  cfi1->drap, cfi1->drap_reg, cfi1->drap_offset,\n\t\t\t  cfi2->drap, cfi2->drap_reg, cfi2->drap_offset);\n\n\t} else\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool func_uaccess_safe(struct symbol *func)\n{\n\tif (func)\n\t\treturn func->uaccess_safe;\n\n\treturn false;\n}\n\nstatic inline const char *call_dest_name(struct instruction *insn)\n{\n\tstatic char pvname[19];\n\tstruct reloc *rel;\n\tint idx;\n\n\tif (insn->call_dest)\n\t\treturn insn->call_dest->name;\n\n\trel = insn_reloc(NULL, insn);\n\tif (rel && !strcmp(rel->sym->name, \"pv_ops\")) {\n\t\tidx = (rel->addend / sizeof(void *));\n\t\tsnprintf(pvname, sizeof(pvname), \"pv_ops[%d]\", idx);\n\t\treturn pvname;\n\t}\n\n\treturn \"{dynamic}\";\n}\n\nstatic bool pv_call_dest(struct objtool_file *file, struct instruction *insn)\n{\n\tstruct symbol *target;\n\tstruct reloc *rel;\n\tint idx;\n\n\trel = insn_reloc(file, insn);\n\tif (!rel || strcmp(rel->sym->name, \"pv_ops\"))\n\t\treturn false;\n\n\tidx = (arch_dest_reloc_offset(rel->addend) / sizeof(void *));\n\n\tif (file->pv_ops[idx].clean)\n\t\treturn true;\n\n\tfile->pv_ops[idx].clean = true;\n\n\tlist_for_each_entry(target, &file->pv_ops[idx].targets, pv_target) {\n\t\tif (!target->sec->noinstr) {\n\t\t\tWARN(\"pv_ops[%d]: %s\", idx, target->name);\n\t\t\tfile->pv_ops[idx].clean = false;\n\t\t}\n\t}\n\n\treturn file->pv_ops[idx].clean;\n}\n\nstatic inline bool noinstr_call_dest(struct objtool_file *file,\n\t\t\t\t     struct instruction *insn,\n\t\t\t\t     struct symbol *func)\n{\n\t/*\n\t * We can't deal with indirect function calls at present;\n\t * assume they're instrumented.\n\t */\n\tif (!func) {\n\t\tif (file->pv_ops)\n\t\t\treturn pv_call_dest(file, insn);\n\n\t\treturn false;\n\t}\n\n\t/*\n\t * If the symbol is from a noinstr section; we good.\n\t */\n\tif (func->sec->noinstr)\n\t\treturn true;\n\n\t/*\n\t * The __ubsan_handle_*() calls are like WARN(), they only happen when\n\t * something 'BAD' happened. At the risk of taking the machine down,\n\t * let them proceed to get the message out.\n\t */\n\tif (!strncmp(func->name, \"__ubsan_handle_\", 15))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int validate_call(struct objtool_file *file,\n\t\t\t struct instruction *insn,\n\t\t\t struct insn_state *state)\n{\n\tif (state->noinstr && state->instr <= 0 &&\n\t    !noinstr_call_dest(file, insn, insn->call_dest)) {\n\t\tWARN_FUNC(\"call to %s() leaves .noinstr.text section\",\n\t\t\t\tinsn->sec, insn->offset, call_dest_name(insn));\n\t\treturn 1;\n\t}\n\n\tif (state->uaccess && !func_uaccess_safe(insn->call_dest)) {\n\t\tWARN_FUNC(\"call to %s() with UACCESS enabled\",\n\t\t\t\tinsn->sec, insn->offset, call_dest_name(insn));\n\t\treturn 1;\n\t}\n\n\tif (state->df) {\n\t\tWARN_FUNC(\"call to %s() with DF set\",\n\t\t\t\tinsn->sec, insn->offset, call_dest_name(insn));\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int validate_sibling_call(struct objtool_file *file,\n\t\t\t\t struct instruction *insn,\n\t\t\t\t struct insn_state *state)\n{\n\tif (has_modified_stack_frame(insn, state)) {\n\t\tWARN_FUNC(\"sibling call from callable instruction with modified stack frame\",\n\t\t\t\tinsn->sec, insn->offset);\n\t\treturn 1;\n\t}\n\n\treturn validate_call(file, insn, state);\n}\n\nstatic int validate_return(struct symbol *func, struct instruction *insn, struct insn_state *state)\n{\n\tif (state->noinstr && state->instr > 0) {\n\t\tWARN_FUNC(\"return with instrumentation enabled\",\n\t\t\t  insn->sec, insn->offset);\n\t\treturn 1;\n\t}\n\n\tif (state->uaccess && !func_uaccess_safe(func)) {\n\t\tWARN_FUNC(\"return with UACCESS enabled\",\n\t\t\t  insn->sec, insn->offset);\n\t\treturn 1;\n\t}\n\n\tif (!state->uaccess && func_uaccess_safe(func)) {\n\t\tWARN_FUNC(\"return with UACCESS disabled from a UACCESS-safe function\",\n\t\t\t  insn->sec, insn->offset);\n\t\treturn 1;\n\t}\n\n\tif (state->df) {\n\t\tWARN_FUNC(\"return with DF set\",\n\t\t\t  insn->sec, insn->offset);\n\t\treturn 1;\n\t}\n\n\tif (func && has_modified_stack_frame(insn, state)) {\n\t\tWARN_FUNC(\"return with modified stack frame\",\n\t\t\t  insn->sec, insn->offset);\n\t\treturn 1;\n\t}\n\n\tif (state->cfi.bp_scratch) {\n\t\tWARN_FUNC(\"BP used as a scratch register\",\n\t\t\t  insn->sec, insn->offset);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic struct instruction *next_insn_to_validate(struct objtool_file *file,\n\t\t\t\t\t\t struct instruction *insn)\n{\n\tstruct alt_group *alt_group = insn->alt_group;\n\n\t/*\n\t * Simulate the fact that alternatives are patched in-place.  When the\n\t * end of a replacement alt_group is reached, redirect objtool flow to\n\t * the end of the original alt_group.\n\t */\n\tif (alt_group && insn == alt_group->last_insn && alt_group->orig_group)\n\t\treturn next_insn_same_sec(file, alt_group->orig_group->last_insn);\n\n\treturn next_insn_same_sec(file, insn);\n}\n\n/*\n * Follow the branch starting at the given instruction, and recursively follow\n * any other branches (jumps).  Meanwhile, track the frame pointer state at\n * each instruction and validate all the rules described in\n * tools/objtool/Documentation/stack-validation.txt.\n */\nstatic int validate_branch(struct objtool_file *file, struct symbol *func,\n\t\t\t   struct instruction *insn, struct insn_state state)\n{\n\tstruct alternative *alt;\n\tstruct instruction *next_insn, *prev_insn = NULL;\n\tstruct section *sec;\n\tu8 visited;\n\tint ret;\n\n\tsec = insn->sec;\n\n\twhile (1) {\n\t\tnext_insn = next_insn_to_validate(file, insn);\n\n\t\tif (func && insn->func && func != insn->func->pfunc) {\n\t\t\tWARN(\"%s() falls through to next function %s()\",\n\t\t\t     func->name, insn->func->name);\n\t\t\treturn 1;\n\t\t}\n\n\t\tif (func && insn->ignore) {\n\t\t\tWARN_FUNC(\"BUG: why am I validating an ignored function?\",\n\t\t\t\t  sec, insn->offset);\n\t\t\treturn 1;\n\t\t}\n\n\t\tvisited = 1 << state.uaccess;\n\t\tif (insn->visited) {\n\t\t\tif (!insn->hint && !insn_cfi_match(insn, &state.cfi))\n\t\t\t\treturn 1;\n\n\t\t\tif (insn->visited & visited)\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tnr_insns_visited++;\n\t\t}\n\n\t\tif (state.noinstr)\n\t\t\tstate.instr += insn->instr;\n\n\t\tif (insn->hint) {\n\t\t\tstate.cfi = *insn->cfi;\n\t\t} else {\n\t\t\t/* XXX track if we actually changed state.cfi */\n\n\t\t\tif (prev_insn && !cficmp(prev_insn->cfi, &state.cfi)) {\n\t\t\t\tinsn->cfi = prev_insn->cfi;\n\t\t\t\tnr_cfi_reused++;\n\t\t\t} else {\n\t\t\t\tinsn->cfi = cfi_hash_find_or_add(&state.cfi);\n\t\t\t}\n\t\t}\n\n\t\tinsn->visited |= visited;\n\n\t\tif (propagate_alt_cfi(file, insn))\n\t\t\treturn 1;\n\n\t\tif (!insn->ignore_alts && !list_empty(&insn->alts)) {\n\t\t\tbool skip_orig = false;\n\n\t\t\tlist_for_each_entry(alt, &insn->alts, list) {\n\t\t\t\tif (alt->skip_orig)\n\t\t\t\t\tskip_orig = true;\n\n\t\t\t\tret = validate_branch(file, func, alt->insn, state);\n\t\t\t\tif (ret) {\n\t\t\t\t\tif (opts.backtrace)\n\t\t\t\t\t\tBT_FUNC(\"(alt)\", insn);\n\t\t\t\t\treturn ret;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (skip_orig)\n\t\t\t\treturn 0;\n\t\t}\n\n\t\tif (handle_insn_ops(insn, next_insn, &state))\n\t\t\treturn 1;\n\n\t\tswitch (insn->type) {\n\n\t\tcase INSN_RETURN:\n\t\t\treturn validate_return(func, insn, &state);\n\n\t\tcase INSN_CALL:\n\t\tcase INSN_CALL_DYNAMIC:\n\t\t\tret = validate_call(file, insn, &state);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\tif (opts.stackval && func && !is_fentry_call(insn) &&\n\t\t\t    !has_valid_stack_frame(&state)) {\n\t\t\t\tWARN_FUNC(\"call without frame pointer save/setup\",\n\t\t\t\t\t  sec, insn->offset);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\tif (insn->dead_end)\n\t\t\t\treturn 0;\n\n\t\t\tbreak;\n\n\t\tcase INSN_JUMP_CONDITIONAL:\n\t\tcase INSN_JUMP_UNCONDITIONAL:\n\t\t\tif (is_sibling_call(insn)) {\n\t\t\t\tret = validate_sibling_call(file, insn, &state);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\n\t\t\t} else if (insn->jump_dest) {\n\t\t\t\tret = validate_branch(file, func,\n\t\t\t\t\t\t      insn->jump_dest, state);\n\t\t\t\tif (ret) {\n\t\t\t\t\tif (opts.backtrace)\n\t\t\t\t\t\tBT_FUNC(\"(branch)\", insn);\n\t\t\t\t\treturn ret;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (insn->type == INSN_JUMP_UNCONDITIONAL)\n\t\t\t\treturn 0;\n\n\t\t\tbreak;\n\n\t\tcase INSN_JUMP_DYNAMIC:\n\t\tcase INSN_JUMP_DYNAMIC_CONDITIONAL:\n\t\t\tif (is_sibling_call(insn)) {\n\t\t\t\tret = validate_sibling_call(file, insn, &state);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\n\t\t\tif (insn->type == INSN_JUMP_DYNAMIC)\n\t\t\t\treturn 0;\n\n\t\t\tbreak;\n\n\t\tcase INSN_CONTEXT_SWITCH:\n\t\t\tif (func && (!next_insn || !next_insn->hint)) {\n\t\t\t\tWARN_FUNC(\"unsupported instruction in callable function\",\n\t\t\t\t\t  sec, insn->offset);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\treturn 0;\n\n\t\tcase INSN_STAC:\n\t\t\tif (state.uaccess) {\n\t\t\t\tWARN_FUNC(\"recursive UACCESS enable\", sec, insn->offset);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\tstate.uaccess = true;\n\t\t\tbreak;\n\n\t\tcase INSN_CLAC:\n\t\t\tif (!state.uaccess && func) {\n\t\t\t\tWARN_FUNC(\"redundant UACCESS disable\", sec, insn->offset);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\tif (func_uaccess_safe(func) && !state.uaccess_stack) {\n\t\t\t\tWARN_FUNC(\"UACCESS-safe disables UACCESS\", sec, insn->offset);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\tstate.uaccess = false;\n\t\t\tbreak;\n\n\t\tcase INSN_STD:\n\t\t\tif (state.df) {\n\t\t\t\tWARN_FUNC(\"recursive STD\", sec, insn->offset);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\tstate.df = true;\n\t\t\tbreak;\n\n\t\tcase INSN_CLD:\n\t\t\tif (!state.df && func) {\n\t\t\t\tWARN_FUNC(\"redundant CLD\", sec, insn->offset);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\tstate.df = false;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tif (insn->dead_end)\n\t\t\treturn 0;\n\n\t\tif (!next_insn) {\n\t\t\tif (state.cfi.cfa.base == CFI_UNDEFINED)\n\t\t\t\treturn 0;\n\t\t\tWARN(\"%s: unexpected end of section\", sec->name);\n\t\t\treturn 1;\n\t\t}\n\n\t\tprev_insn = insn;\n\t\tinsn = next_insn;\n\t}\n\n\treturn 0;\n}\n\nstatic int validate_unwind_hints(struct objtool_file *file, struct section *sec)\n{\n\tstruct instruction *insn;\n\tstruct insn_state state;\n\tint ret, warnings = 0;\n\n\tif (!file->hints)\n\t\treturn 0;\n\n\tinit_insn_state(file, &state, sec);\n\n\tif (sec) {\n\t\tinsn = find_insn(file, sec, 0);\n\t\tif (!insn)\n\t\t\treturn 0;\n\t} else {\n\t\tinsn = list_first_entry(&file->insn_list, typeof(*insn), list);\n\t}\n\n\twhile (&insn->list != &file->insn_list && (!sec || insn->sec == sec)) {\n\t\tif (insn->hint && !insn->visited && !insn->ignore) {\n\t\t\tret = validate_branch(file, insn->func, insn, state);\n\t\t\tif (ret && opts.backtrace)\n\t\t\t\tBT_FUNC(\"<=== (hint)\", insn);\n\t\t\twarnings += ret;\n\t\t}\n\n\t\tinsn = list_next_entry(insn, list);\n\t}\n\n\treturn warnings;\n}\n\nstatic int validate_retpoline(struct objtool_file *file)\n{\n\tstruct instruction *insn;\n\tint warnings = 0;\n\n\tfor_each_insn(file, insn) {\n\t\tif (insn->type != INSN_JUMP_DYNAMIC &&\n\t\t    insn->type != INSN_CALL_DYNAMIC)\n\t\t\tcontinue;\n\n\t\tif (insn->retpoline_safe)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * .init.text code is ran before userspace and thus doesn't\n\t\t * strictly need retpolines, except for modules which are\n\t\t * loaded late, they very much do need retpoline in their\n\t\t * .init.text\n\t\t */\n\t\tif (!strcmp(insn->sec->name, \".init.text\") && !opts.module)\n\t\t\tcontinue;\n\n\t\tWARN_FUNC(\"indirect %s found in RETPOLINE build\",\n\t\t\t  insn->sec, insn->offset,\n\t\t\t  insn->type == INSN_JUMP_DYNAMIC ? \"jump\" : \"call\");\n\n\t\twarnings++;\n\t}\n\n\treturn warnings;\n}\n\nstatic bool is_kasan_insn(struct instruction *insn)\n{\n\treturn (insn->type == INSN_CALL &&\n\t\t!strcmp(insn->call_dest->name, \"__asan_handle_no_return\"));\n}\n\nstatic bool is_ubsan_insn(struct instruction *insn)\n{\n\treturn (insn->type == INSN_CALL &&\n\t\t!strcmp(insn->call_dest->name,\n\t\t\t\"__ubsan_handle_builtin_unreachable\"));\n}\n\nstatic bool ignore_unreachable_insn(struct objtool_file *file, struct instruction *insn)\n{\n\tint i;\n\tstruct instruction *prev_insn;\n\n\tif (insn->ignore || insn->type == INSN_NOP || insn->type == INSN_TRAP)\n\t\treturn true;\n\n\t/*\n\t * Ignore alternative replacement instructions.  This can happen\n\t * when a whitelisted function uses one of the ALTERNATIVE macros.\n\t */\n\tif (!strcmp(insn->sec->name, \".altinstr_replacement\") ||\n\t    !strcmp(insn->sec->name, \".altinstr_aux\"))\n\t\treturn true;\n\n\t/*\n\t * Whole archive runs might encounter dead code from weak symbols.\n\t * This is where the linker will have dropped the weak symbol in\n\t * favour of a regular symbol, but leaves the code in place.\n\t *\n\t * In this case we'll find a piece of code (whole function) that is not\n\t * covered by a !section symbol. Ignore them.\n\t */\n\tif (opts.link && !insn->func) {\n\t\tint size = find_symbol_hole_containing(insn->sec, insn->offset);\n\t\tunsigned long end = insn->offset + size;\n\n\t\tif (!size) /* not a hole */\n\t\t\treturn false;\n\n\t\tif (size < 0) /* hole until the end */\n\t\t\treturn true;\n\n\t\tsec_for_each_insn_continue(file, insn) {\n\t\t\t/*\n\t\t\t * If we reach a visited instruction at or before the\n\t\t\t * end of the hole, ignore the unreachable.\n\t\t\t */\n\t\t\tif (insn->visited)\n\t\t\t\treturn true;\n\n\t\t\tif (insn->offset >= end)\n\t\t\t\tbreak;\n\n\t\t\t/*\n\t\t\t * If this hole jumps to a .cold function, mark it ignore too.\n\t\t\t */\n\t\t\tif (insn->jump_dest && insn->jump_dest->func &&\n\t\t\t    strstr(insn->jump_dest->func->name, \".cold\")) {\n\t\t\t\tstruct instruction *dest = insn->jump_dest;\n\t\t\t\tfunc_for_each_insn(file, dest->func, dest)\n\t\t\t\t\tdest->ignore = true;\n\t\t\t}\n\t\t}\n\n\t\treturn false;\n\t}\n\n\tif (!insn->func)\n\t\treturn false;\n\n\tif (insn->func->static_call_tramp)\n\t\treturn true;\n\n\t/*\n\t * CONFIG_UBSAN_TRAP inserts a UD2 when it sees\n\t * __builtin_unreachable().  The BUG() macro has an unreachable() after\n\t * the UD2, which causes GCC's undefined trap logic to emit another UD2\n\t * (or occasionally a JMP to UD2).\n\t *\n\t * It may also insert a UD2 after calling a __noreturn function.\n\t */\n\tprev_insn = list_prev_entry(insn, list);\n\tif ((prev_insn->dead_end || dead_end_function(file, prev_insn->call_dest)) &&\n\t    (insn->type == INSN_BUG ||\n\t     (insn->type == INSN_JUMP_UNCONDITIONAL &&\n\t      insn->jump_dest && insn->jump_dest->type == INSN_BUG)))\n\t\treturn true;\n\n\t/*\n\t * Check if this (or a subsequent) instruction is related to\n\t * CONFIG_UBSAN or CONFIG_KASAN.\n\t *\n\t * End the search at 5 instructions to avoid going into the weeds.\n\t */\n\tfor (i = 0; i < 5; i++) {\n\n\t\tif (is_kasan_insn(insn) || is_ubsan_insn(insn))\n\t\t\treturn true;\n\n\t\tif (insn->type == INSN_JUMP_UNCONDITIONAL) {\n\t\t\tif (insn->jump_dest &&\n\t\t\t    insn->jump_dest->func == insn->func) {\n\t\t\t\tinsn = insn->jump_dest;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tbreak;\n\t\t}\n\n\t\tif (insn->offset + insn->len >= insn->func->offset + insn->func->len)\n\t\t\tbreak;\n\n\t\tinsn = list_next_entry(insn, list);\n\t}\n\n\treturn false;\n}\n\nstatic int validate_symbol(struct objtool_file *file, struct section *sec,\n\t\t\t   struct symbol *sym, struct insn_state *state)\n{\n\tstruct instruction *insn;\n\tint ret;\n\n\tif (!sym->len) {\n\t\tWARN(\"%s() is missing an ELF size annotation\", sym->name);\n\t\treturn 1;\n\t}\n\n\tif (sym->pfunc != sym || sym->alias != sym)\n\t\treturn 0;\n\n\tinsn = find_insn(file, sec, sym->offset);\n\tif (!insn || insn->ignore || insn->visited)\n\t\treturn 0;\n\n\tstate->uaccess = sym->uaccess_safe;\n\n\tret = validate_branch(file, insn->func, insn, *state);\n\tif (ret && opts.backtrace)\n\t\tBT_FUNC(\"<=== (sym)\", insn);\n\treturn ret;\n}\n\nstatic int validate_section(struct objtool_file *file, struct section *sec)\n{\n\tstruct insn_state state;\n\tstruct symbol *func;\n\tint warnings = 0;\n\n\tlist_for_each_entry(func, &sec->symbol_list, list) {\n\t\tif (func->type != STT_FUNC)\n\t\t\tcontinue;\n\n\t\tinit_insn_state(file, &state, sec);\n\t\tset_func_state(&state.cfi);\n\n\t\twarnings += validate_symbol(file, sec, func, &state);\n\t}\n\n\treturn warnings;\n}\n\nstatic int validate_noinstr_sections(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tint warnings = 0;\n\n\tsec = find_section_by_name(file->elf, \".noinstr.text\");\n\tif (sec) {\n\t\twarnings += validate_section(file, sec);\n\t\twarnings += validate_unwind_hints(file, sec);\n\t}\n\n\tsec = find_section_by_name(file->elf, \".entry.text\");\n\tif (sec) {\n\t\twarnings += validate_section(file, sec);\n\t\twarnings += validate_unwind_hints(file, sec);\n\t}\n\n\treturn warnings;\n}\n\nstatic int validate_functions(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tint warnings = 0;\n\n\tfor_each_sec(file, sec) {\n\t\tif (!(sec->sh.sh_flags & SHF_EXECINSTR))\n\t\t\tcontinue;\n\n\t\twarnings += validate_section(file, sec);\n\t}\n\n\treturn warnings;\n}\n\nstatic void mark_endbr_used(struct instruction *insn)\n{\n\tif (!list_empty(&insn->call_node))\n\t\tlist_del_init(&insn->call_node);\n}\n\nstatic int validate_ibt_insn(struct objtool_file *file, struct instruction *insn)\n{\n\tstruct instruction *dest;\n\tstruct reloc *reloc;\n\tunsigned long off;\n\tint warnings = 0;\n\n\t/*\n\t * Looking for function pointer load relocations.  Ignore\n\t * direct/indirect branches:\n\t */\n\tswitch (insn->type) {\n\tcase INSN_CALL:\n\tcase INSN_CALL_DYNAMIC:\n\tcase INSN_JUMP_CONDITIONAL:\n\tcase INSN_JUMP_UNCONDITIONAL:\n\tcase INSN_JUMP_DYNAMIC:\n\tcase INSN_JUMP_DYNAMIC_CONDITIONAL:\n\tcase INSN_RETURN:\n\tcase INSN_NOP:\n\t\treturn 0;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tfor (reloc = insn_reloc(file, insn);\n\t     reloc;\n\t     reloc = find_reloc_by_dest_range(file->elf, insn->sec,\n\t\t\t\t\t      reloc->offset + 1,\n\t\t\t\t\t      (insn->offset + insn->len) - (reloc->offset + 1))) {\n\n\t\t/*\n\t\t * static_call_update() references the trampoline, which\n\t\t * doesn't have (or need) ENDBR.  Skip warning in that case.\n\t\t */\n\t\tif (reloc->sym->static_call_tramp)\n\t\t\tcontinue;\n\n\t\toff = reloc->sym->offset;\n\t\tif (reloc->type == R_X86_64_PC32 || reloc->type == R_X86_64_PLT32)\n\t\t\toff += arch_dest_reloc_offset(reloc->addend);\n\t\telse\n\t\t\toff += reloc->addend;\n\n\t\tdest = find_insn(file, reloc->sym->sec, off);\n\t\tif (!dest)\n\t\t\tcontinue;\n\n\t\tif (dest->type == INSN_ENDBR) {\n\t\t\tmark_endbr_used(dest);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (dest->func && dest->func == insn->func) {\n\t\t\t/*\n\t\t\t * Anything from->to self is either _THIS_IP_ or\n\t\t\t * IRET-to-self.\n\t\t\t *\n\t\t\t * There is no sane way to annotate _THIS_IP_ since the\n\t\t\t * compiler treats the relocation as a constant and is\n\t\t\t * happy to fold in offsets, skewing any annotation we\n\t\t\t * do, leading to vast amounts of false-positives.\n\t\t\t *\n\t\t\t * There's also compiler generated _THIS_IP_ through\n\t\t\t * KCOV and such which we have no hope of annotating.\n\t\t\t *\n\t\t\t * As such, blanket accept self-references without\n\t\t\t * issue.\n\t\t\t */\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (dest->noendbr)\n\t\t\tcontinue;\n\n\t\tWARN_FUNC(\"relocation to !ENDBR: %s\",\n\t\t\t  insn->sec, insn->offset,\n\t\t\t  offstr(dest->sec, dest->offset));\n\n\t\twarnings++;\n\t}\n\n\treturn warnings;\n}\n\nstatic int validate_ibt_data_reloc(struct objtool_file *file,\n\t\t\t\t   struct reloc *reloc)\n{\n\tstruct instruction *dest;\n\n\tdest = find_insn(file, reloc->sym->sec,\n\t\t\t reloc->sym->offset + reloc->addend);\n\tif (!dest)\n\t\treturn 0;\n\n\tif (dest->type == INSN_ENDBR) {\n\t\tmark_endbr_used(dest);\n\t\treturn 0;\n\t}\n\n\tif (dest->noendbr)\n\t\treturn 0;\n\n\tWARN_FUNC(\"data relocation to !ENDBR: %s\",\n\t\t  reloc->sec->base, reloc->offset,\n\t\t  offstr(dest->sec, dest->offset));\n\n\treturn 1;\n}\n\n/*\n * Validate IBT rules and remove used ENDBR instructions from the seal list.\n * Unused ENDBR instructions will be annotated for sealing (i.e., replaced with\n * NOPs) later, in create_ibt_endbr_seal_sections().\n */\nstatic int validate_ibt(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct reloc *reloc;\n\tstruct instruction *insn;\n\tint warnings = 0;\n\n\tfor_each_insn(file, insn)\n\t\twarnings += validate_ibt_insn(file, insn);\n\n\tfor_each_sec(file, sec) {\n\n\t\t/* Already done by validate_ibt_insn() */\n\t\tif (sec->sh.sh_flags & SHF_EXECINSTR)\n\t\t\tcontinue;\n\n\t\tif (!sec->reloc)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * These sections can reference text addresses, but not with\n\t\t * the intent to indirect branch to them.\n\t\t */\n\t\tif (!strncmp(sec->name, \".discard\", 8)\t\t\t||\n\t\t    !strncmp(sec->name, \".debug\", 6)\t\t\t||\n\t\t    !strcmp(sec->name, \".altinstructions\")\t\t||\n\t\t    !strcmp(sec->name, \".ibt_endbr_seal\")\t\t||\n\t\t    !strcmp(sec->name, \".orc_unwind_ip\")\t\t||\n\t\t    !strcmp(sec->name, \".parainstructions\")\t\t||\n\t\t    !strcmp(sec->name, \".retpoline_sites\")\t\t||\n\t\t    !strcmp(sec->name, \".smp_locks\")\t\t\t||\n\t\t    !strcmp(sec->name, \".static_call_sites\")\t\t||\n\t\t    !strcmp(sec->name, \"_error_injection_whitelist\")\t||\n\t\t    !strcmp(sec->name, \"_kprobe_blacklist\")\t\t||\n\t\t    !strcmp(sec->name, \"__bug_table\")\t\t\t||\n\t\t    !strcmp(sec->name, \"__ex_table\")\t\t\t||\n\t\t    !strcmp(sec->name, \"__jump_table\")\t\t\t||\n\t\t    !strcmp(sec->name, \"__mcount_loc\")\t\t\t||\n\t\t    !strcmp(sec->name, \"__tracepoints\"))\n\t\t\tcontinue;\n\n\t\tlist_for_each_entry(reloc, &sec->reloc->reloc_list, list)\n\t\t\twarnings += validate_ibt_data_reloc(file, reloc);\n\t}\n\n\treturn warnings;\n}\n\nstatic int validate_sls(struct objtool_file *file)\n{\n\tstruct instruction *insn, *next_insn;\n\tint warnings = 0;\n\n\tfor_each_insn(file, insn) {\n\t\tnext_insn = next_insn_same_sec(file, insn);\n\n\t\tif (insn->retpoline_safe)\n\t\t\tcontinue;\n\n\t\tswitch (insn->type) {\n\t\tcase INSN_RETURN:\n\t\t\tif (!next_insn || next_insn->type != INSN_TRAP) {\n\t\t\t\tWARN_FUNC(\"missing int3 after ret\",\n\t\t\t\t\t  insn->sec, insn->offset);\n\t\t\t\twarnings++;\n\t\t\t}\n\n\t\t\tbreak;\n\t\tcase INSN_JUMP_DYNAMIC:\n\t\t\tif (!next_insn || next_insn->type != INSN_TRAP) {\n\t\t\t\tWARN_FUNC(\"missing int3 after indirect jump\",\n\t\t\t\t\t  insn->sec, insn->offset);\n\t\t\t\twarnings++;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn warnings;\n}\n\nstatic int validate_reachable_instructions(struct objtool_file *file)\n{\n\tstruct instruction *insn;\n\n\tif (file->ignore_unreachables)\n\t\treturn 0;\n\n\tfor_each_insn(file, insn) {\n\t\tif (insn->visited || ignore_unreachable_insn(file, insn))\n\t\t\tcontinue;\n\n\t\tWARN_FUNC(\"unreachable instruction\", insn->sec, insn->offset);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nint check(struct objtool_file *file)\n{\n\tint ret, warnings = 0;\n\n\tarch_initial_func_cfi_state(&initial_func_cfi);\n\tinit_cfi_state(&init_cfi);\n\tinit_cfi_state(&func_cfi);\n\tset_func_state(&func_cfi);\n\n\tif (!cfi_hash_alloc(1UL << (file->elf->symbol_bits - 3)))\n\t\tgoto out;\n\n\tcfi_hash_add(&init_cfi);\n\tcfi_hash_add(&func_cfi);\n\n\tret = decode_sections(file);\n\tif (ret < 0)\n\t\tgoto out;\n\n\twarnings += ret;\n\n\tif (list_empty(&file->insn_list))\n\t\tgoto out;\n\n\tif (opts.retpoline) {\n\t\tret = validate_retpoline(file);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\twarnings += ret;\n\t}\n\n\tif (opts.stackval || opts.orc || opts.uaccess) {\n\t\tret = validate_functions(file);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\n\t\tret = validate_unwind_hints(file, NULL);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\n\t\tif (!warnings) {\n\t\t\tret = validate_reachable_instructions(file);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\twarnings += ret;\n\t\t}\n\n\t} else if (opts.noinstr) {\n\t\tret = validate_noinstr_sections(file);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\t}\n\n\tif (opts.ibt) {\n\t\tret = validate_ibt(file);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\t}\n\n\tif (opts.sls) {\n\t\tret = validate_sls(file);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\t}\n\n\tif (opts.static_call) {\n\t\tret = create_static_call_sections(file);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\t}\n\n\tif (opts.retpoline) {\n\t\tret = create_retpoline_sites_sections(file);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\t}\n\n\tif (opts.mcount) {\n\t\tret = create_mcount_loc_sections(file);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\t}\n\n\tif (opts.ibt) {\n\t\tret = create_ibt_endbr_seal_sections(file);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\t}\n\n\tif (opts.orc && !list_empty(&file->insn_list)) {\n\t\tret = orc_create(file);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\t}\n\n\n\tif (opts.stats) {\n\t\tprintf(\"nr_insns_visited: %ld\\n\", nr_insns_visited);\n\t\tprintf(\"nr_cfi: %ld\\n\", nr_cfi);\n\t\tprintf(\"nr_cfi_reused: %ld\\n\", nr_cfi_reused);\n\t\tprintf(\"nr_cfi_cache: %ld\\n\", nr_cfi_cache);\n\t}\n\nout:\n\t/*\n\t *  For now, don't fail the kernel build on fatal warnings.  These\n\t *  errors are still fairly common due to the growing matrix of\n\t *  supported toolchains and their recent pace of change.\n\t */\n\treturn 0;\n}\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n * This code is used on x86_64 to create page table identity mappings on\n * demand by building up a new set of page tables (or appending to the\n * existing ones), and then switching over to them when ready.\n *\n * Copyright (C) 2015-2016  Yinghai Lu\n * Copyright (C)      2016  Kees Cook\n */\n\n/*\n * Since we're dealing with identity mappings, physical and virtual\n * addresses are the same, so override these defines which are ultimately\n * used by the headers in misc.h.\n */\n#define __pa(x)  ((unsigned long)(x))\n#define __va(x)  ((void *)((unsigned long)(x)))\n\n/* No PAGE_TABLE_ISOLATION support needed either: */\n#undef CONFIG_PAGE_TABLE_ISOLATION\n\n#include \"error.h\"\n#include \"misc.h\"\n\n/* These actually do the work of building the kernel identity maps. */\n#include <linux/pgtable.h>\n#include <asm/cmpxchg.h>\n#include <asm/trap_pf.h>\n#include <asm/trapnr.h>\n#include <asm/init.h>\n/* Use the static base for this part of the boot process */\n#undef __PAGE_OFFSET\n#define __PAGE_OFFSET __PAGE_OFFSET_BASE\n#include \"../../mm/ident_map.c\"\n\n#define _SETUP\n#include <asm/setup.h>\t/* For COMMAND_LINE_SIZE */\n#undef _SETUP\n\nextern unsigned long get_cmd_line_ptr(void);\n\n/* Used by PAGE_KERN* macros: */\npteval_t __default_kernel_pte_mask __read_mostly = ~0;\n\n/* Used to track our page table allocation area. */\nstruct alloc_pgt_data {\n\tunsigned char *pgt_buf;\n\tunsigned long pgt_buf_size;\n\tunsigned long pgt_buf_offset;\n};\n\n/*\n * Allocates space for a page table entry, using struct alloc_pgt_data\n * above. Besides the local callers, this is used as the allocation\n * callback in mapping_info below.\n */\nstatic void *alloc_pgt_page(void *context)\n{\n\tstruct alloc_pgt_data *pages = (struct alloc_pgt_data *)context;\n\tunsigned char *entry;\n\n\t/* Validate there is space available for a new page. */\n\tif (pages->pgt_buf_offset >= pages->pgt_buf_size) {\n\t\tdebug_putstr(\"out of pgt_buf in \" __FILE__ \"!?\\n\");\n\t\tdebug_putaddr(pages->pgt_buf_offset);\n\t\tdebug_putaddr(pages->pgt_buf_size);\n\t\treturn NULL;\n\t}\n\n\tentry = pages->pgt_buf + pages->pgt_buf_offset;\n\tpages->pgt_buf_offset += PAGE_SIZE;\n\n\treturn entry;\n}\n\n/* Used to track our allocated page tables. */\nstatic struct alloc_pgt_data pgt_data;\n\n/* The top level page table entry pointer. */\nstatic unsigned long top_level_pgt;\n\nphys_addr_t physical_mask = (1ULL << __PHYSICAL_MASK_SHIFT) - 1;\n\n/*\n * Mapping information structure passed to kernel_ident_mapping_init().\n * Due to relocation, pointers must be assigned at run time not build time.\n */\nstatic struct x86_mapping_info mapping_info;\n\n/*\n * Adds the specified range to the identity mappings.\n */\nvoid kernel_add_identity_map(unsigned long start, unsigned long end)\n{\n\tint ret;\n\n\t/* Align boundary to 2M. */\n\tstart = round_down(start, PMD_SIZE);\n\tend = round_up(end, PMD_SIZE);\n\tif (start >= end)\n\t\treturn;\n\n\t/* Build the mapping. */\n\tret = kernel_ident_mapping_init(&mapping_info, (pgd_t *)top_level_pgt, start, end);\n\tif (ret)\n\t\terror(\"Error: kernel_ident_mapping_init() failed\\n\");\n}\n\n/* Locates and clears a region for a new top level page table. */\nvoid initialize_identity_maps(void *rmode)\n{\n\tunsigned long cmdline;\n\tstruct setup_data *sd;\n\n\t/* Exclude the encryption mask from __PHYSICAL_MASK */\n\tphysical_mask &= ~sme_me_mask;\n\n\t/* Init mapping_info with run-time function/buffer pointers. */\n\tmapping_info.alloc_pgt_page = alloc_pgt_page;\n\tmapping_info.context = &pgt_data;\n\tmapping_info.page_flag = __PAGE_KERNEL_LARGE_EXEC | sme_me_mask;\n\tmapping_info.kernpg_flag = _KERNPG_TABLE;\n\n\t/*\n\t * It should be impossible for this not to already be true,\n\t * but since calling this a second time would rewind the other\n\t * counters, let's just make sure this is reset too.\n\t */\n\tpgt_data.pgt_buf_offset = 0;\n\n\t/*\n\t * If we came here via startup_32(), cr3 will be _pgtable already\n\t * and we must append to the existing area instead of entirely\n\t * overwriting it.\n\t *\n\t * With 5-level paging, we use '_pgtable' to allocate the p4d page table,\n\t * the top-level page table is allocated separately.\n\t *\n\t * p4d_offset(top_level_pgt, 0) would cover both the 4- and 5-level\n\t * cases. On 4-level paging it's equal to 'top_level_pgt'.\n\t */\n\ttop_level_pgt = read_cr3_pa();\n\tif (p4d_offset((pgd_t *)top_level_pgt, 0) == (p4d_t *)_pgtable) {\n\t\tpgt_data.pgt_buf = _pgtable + BOOT_INIT_PGT_SIZE;\n\t\tpgt_data.pgt_buf_size = BOOT_PGT_SIZE - BOOT_INIT_PGT_SIZE;\n\t\tmemset(pgt_data.pgt_buf, 0, pgt_data.pgt_buf_size);\n\t} else {\n\t\tpgt_data.pgt_buf = _pgtable;\n\t\tpgt_data.pgt_buf_size = BOOT_PGT_SIZE;\n\t\tmemset(pgt_data.pgt_buf, 0, pgt_data.pgt_buf_size);\n\t\ttop_level_pgt = (unsigned long)alloc_pgt_page(&pgt_data);\n\t}\n\n\t/*\n\t * New page-table is set up - map the kernel image, boot_params and the\n\t * command line. The uncompressed kernel requires boot_params and the\n\t * command line to be mapped in the identity mapping. Map them\n\t * explicitly here in case the compressed kernel does not touch them,\n\t * or does not touch all the pages covering them.\n\t */\n\tkernel_add_identity_map((unsigned long)_head, (unsigned long)_end);\n\tboot_params = rmode;\n\tkernel_add_identity_map((unsigned long)boot_params, (unsigned long)(boot_params + 1));\n\tcmdline = get_cmd_line_ptr();\n\tkernel_add_identity_map(cmdline, cmdline + COMMAND_LINE_SIZE);\n\n\t/*\n\t * Also map the setup_data entries passed via boot_params in case they\n\t * need to be accessed by uncompressed kernel via the identity mapping.\n\t */\n\tsd = (struct setup_data *)boot_params->hdr.setup_data;\n\twhile (sd) {\n\t\tunsigned long sd_addr = (unsigned long)sd;\n\n\t\tkernel_add_identity_map(sd_addr, sd_addr + sizeof(*sd) + sd->len);\n\t\tsd = (struct setup_data *)sd->next;\n\t}\n\n\tsev_prep_identity_maps(top_level_pgt);\n\n\t/* Load the new page-table. */\n\twrite_cr3(top_level_pgt);\n}\n\nstatic pte_t *split_large_pmd(struct x86_mapping_info *info,\n\t\t\t      pmd_t *pmdp, unsigned long __address)\n{\n\tunsigned long page_flags;\n\tunsigned long address;\n\tpte_t *pte;\n\tpmd_t pmd;\n\tint i;\n\n\tpte = (pte_t *)info->alloc_pgt_page(info->context);\n\tif (!pte)\n\t\treturn NULL;\n\n\taddress     = __address & PMD_MASK;\n\t/* No large page - clear PSE flag */\n\tpage_flags  = info->page_flag & ~_PAGE_PSE;\n\n\t/* Populate the PTEs */\n\tfor (i = 0; i < PTRS_PER_PMD; i++) {\n\t\tset_pte(&pte[i], __pte(address | page_flags));\n\t\taddress += PAGE_SIZE;\n\t}\n\n\t/*\n\t * Ideally we need to clear the large PMD first and do a TLB\n\t * flush before we write the new PMD. But the 2M range of the\n\t * PMD might contain the code we execute and/or the stack\n\t * we are on, so we can't do that. But that should be safe here\n\t * because we are going from large to small mappings and we are\n\t * also the only user of the page-table, so there is no chance\n\t * of a TLB multihit.\n\t */\n\tpmd = __pmd((unsigned long)pte | info->kernpg_flag);\n\tset_pmd(pmdp, pmd);\n\t/* Flush TLB to establish the new PMD */\n\twrite_cr3(top_level_pgt);\n\n\treturn pte + pte_index(__address);\n}\n\nstatic void clflush_page(unsigned long address)\n{\n\tunsigned int flush_size;\n\tchar *cl, *start, *end;\n\n\t/*\n\t * Hardcode cl-size to 64 - CPUID can't be used here because that might\n\t * cause another #VC exception and the GHCB is not ready to use yet.\n\t */\n\tflush_size = 64;\n\tstart      = (char *)(address & PAGE_MASK);\n\tend        = start + PAGE_SIZE;\n\n\t/*\n\t * First make sure there are no pending writes on the cache-lines to\n\t * flush.\n\t */\n\tasm volatile(\"mfence\" : : : \"memory\");\n\n\tfor (cl = start; cl != end; cl += flush_size)\n\t\tclflush(cl);\n}\n\nstatic int set_clr_page_flags(struct x86_mapping_info *info,\n\t\t\t      unsigned long address,\n\t\t\t      pteval_t set, pteval_t clr)\n{\n\tpgd_t *pgdp = (pgd_t *)top_level_pgt;\n\tp4d_t *p4dp;\n\tpud_t *pudp;\n\tpmd_t *pmdp;\n\tpte_t *ptep, pte;\n\n\t/*\n\t * First make sure there is a PMD mapping for 'address'.\n\t * It should already exist, but keep things generic.\n\t *\n\t * To map the page just read from it and fault it in if there is no\n\t * mapping yet. kernel_add_identity_map() can't be called here because\n\t * that would unconditionally map the address on PMD level, destroying\n\t * any PTE-level mappings that might already exist. Use assembly here\n\t * so the access won't be optimized away.\n\t */\n\tasm volatile(\"mov %[address], %%r9\"\n\t\t     :: [address] \"g\" (*(unsigned long *)address)\n\t\t     : \"r9\", \"memory\");\n\n\t/*\n\t * The page is mapped at least with PMD size - so skip checks and walk\n\t * directly to the PMD.\n\t */\n\tp4dp = p4d_offset(pgdp, address);\n\tpudp = pud_offset(p4dp, address);\n\tpmdp = pmd_offset(pudp, address);\n\n\tif (pmd_large(*pmdp))\n\t\tptep = split_large_pmd(info, pmdp, address);\n\telse\n\t\tptep = pte_offset_kernel(pmdp, address);\n\n\tif (!ptep)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Changing encryption attributes of a page requires to flush it from\n\t * the caches.\n\t */\n\tif ((set | clr) & _PAGE_ENC) {\n\t\tclflush_page(address);\n\n\t\t/*\n\t\t * If the encryption attribute is being cleared, change the page state\n\t\t * to shared in the RMP table.\n\t\t */\n\t\tif (clr)\n\t\t\tsnp_set_page_shared(__pa(address & PAGE_MASK));\n\t}\n\n\t/* Update PTE */\n\tpte = *ptep;\n\tpte = pte_set_flags(pte, set);\n\tpte = pte_clear_flags(pte, clr);\n\tset_pte(ptep, pte);\n\n\t/*\n\t * If the encryption attribute is being set, then change the page state to\n\t * private in the RMP entry. The page state change must be done after the PTE\n\t * is updated.\n\t */\n\tif (set & _PAGE_ENC)\n\t\tsnp_set_page_private(__pa(address & PAGE_MASK));\n\n\t/* Flush TLB after changing encryption attribute */\n\twrite_cr3(top_level_pgt);\n\n\treturn 0;\n}\n\nint set_page_decrypted(unsigned long address)\n{\n\treturn set_clr_page_flags(&mapping_info, address, 0, _PAGE_ENC);\n}\n\nint set_page_encrypted(unsigned long address)\n{\n\treturn set_clr_page_flags(&mapping_info, address, _PAGE_ENC, 0);\n}\n\nint set_page_non_present(unsigned long address)\n{\n\treturn set_clr_page_flags(&mapping_info, address, 0, _PAGE_PRESENT);\n}\n\nstatic void do_pf_error(const char *msg, unsigned long error_code,\n\t\t\tunsigned long address, unsigned long ip)\n{\n\terror_putstr(msg);\n\n\terror_putstr(\"\\nError Code: \");\n\terror_puthex(error_code);\n\terror_putstr(\"\\nCR2: 0x\");\n\terror_puthex(address);\n\terror_putstr(\"\\nRIP relative to _head: 0x\");\n\terror_puthex(ip - (unsigned long)_head);\n\terror_putstr(\"\\n\");\n\n\terror(\"Stopping.\\n\");\n}\n\nvoid do_boot_page_fault(struct pt_regs *regs, unsigned long error_code)\n{\n\tunsigned long address = native_read_cr2();\n\tunsigned long end;\n\tbool ghcb_fault;\n\n\tghcb_fault = sev_es_check_ghcb_fault(address);\n\n\taddress   &= PMD_MASK;\n\tend        = address + PMD_SIZE;\n\n\t/*\n\t * Check for unexpected error codes. Unexpected are:\n\t *\t- Faults on present pages\n\t *\t- User faults\n\t *\t- Reserved bits set\n\t */\n\tif (error_code & (X86_PF_PROT | X86_PF_USER | X86_PF_RSVD))\n\t\tdo_pf_error(\"Unexpected page-fault:\", error_code, address, regs->ip);\n\telse if (ghcb_fault)\n\t\tdo_pf_error(\"Page-fault on GHCB page:\", error_code, address, regs->ip);\n\n\t/*\n\t * Error code is sane - now identity map the 2M region around\n\t * the faulting address.\n\t */\n\tkernel_add_identity_map(address, end);\n}\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_X86_SETUP_H\n#define _ASM_X86_SETUP_H\n\n#include <uapi/asm/setup.h>\n\n#define COMMAND_LINE_SIZE 2048\n\n#include <linux/linkage.h>\n#include <asm/page_types.h>\n#include <asm/ibt.h>\n\n#ifdef __i386__\n\n#include <linux/pfn.h>\n/*\n * Reserved space for vmalloc and iomap - defined in asm/page.h\n */\n#define MAXMEM_PFN\tPFN_DOWN(MAXMEM)\n#define MAX_NONPAE_PFN\t(1 << 20)\n\n#endif /* __i386__ */\n\n#define PARAM_SIZE 4096\t\t/* sizeof(struct boot_params) */\n\n#define OLD_CL_MAGIC\t\t0xA33F\n#define OLD_CL_ADDRESS\t\t0x020\t/* Relative to real mode data */\n#define NEW_CL_POINTER\t\t0x228\t/* Relative to real mode data */\n\n#ifndef __ASSEMBLY__\n#include <asm/bootparam.h>\n#include <asm/x86_init.h>\n\nextern u64 relocated_ramdisk;\n\n/* Interrupt control for vSMPowered x86_64 systems */\n#ifdef CONFIG_X86_64\nvoid vsmp_init(void);\n#else\nstatic inline void vsmp_init(void) { }\n#endif\n\nstruct pt_regs;\n\nvoid setup_bios_corruption_check(void);\nvoid early_platform_quirks(void);\n\nextern unsigned long saved_video_mode;\n\nextern void reserve_standard_io_resources(void);\nextern void i386_reserve_resources(void);\nextern unsigned long __startup_64(unsigned long physaddr, struct boot_params *bp);\nextern void startup_64_setup_env(unsigned long physbase);\nextern void early_setup_idt(void);\nextern void __init do_early_exception(struct pt_regs *regs, int trapnr);\n\n#ifdef CONFIG_X86_INTEL_MID\nextern void x86_intel_mid_early_setup(void);\n#else\nstatic inline void x86_intel_mid_early_setup(void) { }\n#endif\n\n#ifdef CONFIG_X86_INTEL_CE\nextern void x86_ce4100_early_setup(void);\n#else\nstatic inline void x86_ce4100_early_setup(void) { }\n#endif\n\n#ifndef _SETUP\n\n#include <asm/espfix.h>\n#include <linux/kernel.h>\n\n/*\n * This is set up by the setup-routine at boot-time\n */\nextern struct boot_params boot_params;\nextern char _text[];\n\nstatic inline bool kaslr_enabled(void)\n{\n\treturn IS_ENABLED(CONFIG_RANDOMIZE_MEMORY) &&\n\t\t!!(boot_params.hdr.loadflags & KASLR_FLAG);\n}\n\n/*\n * Apply no randomization if KASLR was disabled at boot or if KASAN\n * is enabled. KASAN shadow mappings rely on regions being PGD aligned.\n */\nstatic inline bool kaslr_memory_enabled(void)\n{\n\treturn kaslr_enabled() && !IS_ENABLED(CONFIG_KASAN);\n}\n\nstatic inline unsigned long kaslr_offset(void)\n{\n\treturn (unsigned long)&_text - __START_KERNEL;\n}\n\n/*\n * Do NOT EVER look at the BIOS memory size location.\n * It does not work on many machines.\n */\n#define LOWMEMSIZE()\t(0x9f000)\n\n/* exceedingly early brk-like allocator */\nextern unsigned long _brk_end;\nvoid *extend_brk(size_t size, size_t align);\n\n/*\n * Reserve space in the .brk section, which is a block of memory from which the\n * caller is allowed to allocate very early (before even memblock is available)\n * by calling extend_brk().  All allocated memory will be eventually converted\n * to memblock.  Any leftover unallocated memory will be freed.\n *\n * The size is in bytes.\n */\n#define RESERVE_BRK(name, size)\t\t\t\t\t\\\n\t__section(\".bss..brk\") __aligned(1) __used\t\\\n\tstatic char __brk_##name[size]\n\nextern void probe_roms(void);\n\nvoid clear_bss(void);\n\n#ifdef __i386__\n\nasmlinkage void __init i386_start_kernel(void);\n\n#else\nasmlinkage void __init x86_64_start_kernel(char *real_mode);\nasmlinkage void __init x86_64_start_reservations(char *real_mode_data);\n\n#endif /* __i386__ */\n#endif /* _SETUP */\n\n#else  /* __ASSEMBLY */\n\n.macro __RESERVE_BRK name, size\n\t.pushsection .bss..brk, \"aw\"\nSYM_DATA_START(__brk_\\name)\n\t.skip \\size\nSYM_DATA_END(__brk_\\name)\n\t.popsection\n.endm\n\n#define RESERVE_BRK(name, size) __RESERVE_BRK name, size\n\n#endif /* __ASSEMBLY__ */\n\n#endif /* _ASM_X86_SETUP_H */\n", "/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */\n#ifndef _ASM_X86_BOOTPARAM_H\n#define _ASM_X86_BOOTPARAM_H\n\n/* setup_data/setup_indirect types */\n#define SETUP_NONE\t\t\t0\n#define SETUP_E820_EXT\t\t\t1\n#define SETUP_DTB\t\t\t2\n#define SETUP_PCI\t\t\t3\n#define SETUP_EFI\t\t\t4\n#define SETUP_APPLE_PROPERTIES\t\t5\n#define SETUP_JAILHOUSE\t\t\t6\n#define SETUP_CC_BLOB\t\t\t7\n\n#define SETUP_INDIRECT\t\t\t(1<<31)\n\n/* SETUP_INDIRECT | max(SETUP_*) */\n#define SETUP_TYPE_MAX\t\t\t(SETUP_INDIRECT | SETUP_CC_BLOB)\n\n/* ram_size flags */\n#define RAMDISK_IMAGE_START_MASK\t0x07FF\n#define RAMDISK_PROMPT_FLAG\t\t0x8000\n#define RAMDISK_LOAD_FLAG\t\t0x4000\n\n/* loadflags */\n#define LOADED_HIGH\t(1<<0)\n#define KASLR_FLAG\t(1<<1)\n#define QUIET_FLAG\t(1<<5)\n#define KEEP_SEGMENTS\t(1<<6)\n#define CAN_USE_HEAP\t(1<<7)\n\n/* xloadflags */\n#define XLF_KERNEL_64\t\t\t(1<<0)\n#define XLF_CAN_BE_LOADED_ABOVE_4G\t(1<<1)\n#define XLF_EFI_HANDOVER_32\t\t(1<<2)\n#define XLF_EFI_HANDOVER_64\t\t(1<<3)\n#define XLF_EFI_KEXEC\t\t\t(1<<4)\n#define XLF_5LEVEL\t\t\t(1<<5)\n#define XLF_5LEVEL_ENABLED\t\t(1<<6)\n\n#ifndef __ASSEMBLY__\n\n#include <linux/types.h>\n#include <linux/screen_info.h>\n#include <linux/apm_bios.h>\n#include <linux/edd.h>\n#include <asm/ist.h>\n#include <video/edid.h>\n\n/* extensible setup data list node */\nstruct setup_data {\n\t__u64 next;\n\t__u32 type;\n\t__u32 len;\n\t__u8 data[0];\n};\n\n/* extensible setup indirect data node */\nstruct setup_indirect {\n\t__u32 type;\n\t__u32 reserved;  /* Reserved, must be set to zero. */\n\t__u64 len;\n\t__u64 addr;\n};\n\nstruct setup_header {\n\t__u8\tsetup_sects;\n\t__u16\troot_flags;\n\t__u32\tsyssize;\n\t__u16\tram_size;\n\t__u16\tvid_mode;\n\t__u16\troot_dev;\n\t__u16\tboot_flag;\n\t__u16\tjump;\n\t__u32\theader;\n\t__u16\tversion;\n\t__u32\trealmode_swtch;\n\t__u16\tstart_sys_seg;\n\t__u16\tkernel_version;\n\t__u8\ttype_of_loader;\n\t__u8\tloadflags;\n\t__u16\tsetup_move_size;\n\t__u32\tcode32_start;\n\t__u32\tramdisk_image;\n\t__u32\tramdisk_size;\n\t__u32\tbootsect_kludge;\n\t__u16\theap_end_ptr;\n\t__u8\text_loader_ver;\n\t__u8\text_loader_type;\n\t__u32\tcmd_line_ptr;\n\t__u32\tinitrd_addr_max;\n\t__u32\tkernel_alignment;\n\t__u8\trelocatable_kernel;\n\t__u8\tmin_alignment;\n\t__u16\txloadflags;\n\t__u32\tcmdline_size;\n\t__u32\thardware_subarch;\n\t__u64\thardware_subarch_data;\n\t__u32\tpayload_offset;\n\t__u32\tpayload_length;\n\t__u64\tsetup_data;\n\t__u64\tpref_address;\n\t__u32\tinit_size;\n\t__u32\thandover_offset;\n\t__u32\tkernel_info_offset;\n} __attribute__((packed));\n\nstruct sys_desc_table {\n\t__u16 length;\n\t__u8  table[14];\n};\n\n/* Gleaned from OFW's set-parameters in cpu/x86/pc/linux.fth */\nstruct olpc_ofw_header {\n\t__u32 ofw_magic;\t/* OFW signature */\n\t__u32 ofw_version;\n\t__u32 cif_handler;\t/* callback into OFW */\n\t__u32 irq_desc_table;\n} __attribute__((packed));\n\nstruct efi_info {\n\t__u32 efi_loader_signature;\n\t__u32 efi_systab;\n\t__u32 efi_memdesc_size;\n\t__u32 efi_memdesc_version;\n\t__u32 efi_memmap;\n\t__u32 efi_memmap_size;\n\t__u32 efi_systab_hi;\n\t__u32 efi_memmap_hi;\n};\n\n/*\n * This is the maximum number of entries in struct boot_params::e820_table\n * (the zeropage), which is part of the x86 boot protocol ABI:\n */\n#define E820_MAX_ENTRIES_ZEROPAGE 128\n\n/*\n * The E820 memory region entry of the boot protocol ABI:\n */\nstruct boot_e820_entry {\n\t__u64 addr;\n\t__u64 size;\n\t__u32 type;\n} __attribute__((packed));\n\n/*\n * Smallest compatible version of jailhouse_setup_data required by this kernel.\n */\n#define JAILHOUSE_SETUP_REQUIRED_VERSION\t1\n\n/*\n * The boot loader is passing platform information via this Jailhouse-specific\n * setup data structure.\n */\nstruct jailhouse_setup_data {\n\tstruct {\n\t\t__u16\tversion;\n\t\t__u16\tcompatible_version;\n\t} __attribute__((packed)) hdr;\n\tstruct {\n\t\t__u16\tpm_timer_address;\n\t\t__u16\tnum_cpus;\n\t\t__u64\tpci_mmconfig_base;\n\t\t__u32\ttsc_khz;\n\t\t__u32\tapic_khz;\n\t\t__u8\tstandard_ioapic;\n\t\t__u8\tcpu_ids[255];\n\t} __attribute__((packed)) v1;\n\tstruct {\n\t\t__u32\tflags;\n\t} __attribute__((packed)) v2;\n} __attribute__((packed));\n\n/* The so-called \"zeropage\" */\nstruct boot_params {\n\tstruct screen_info screen_info;\t\t\t/* 0x000 */\n\tstruct apm_bios_info apm_bios_info;\t\t/* 0x040 */\n\t__u8  _pad2[4];\t\t\t\t\t/* 0x054 */\n\t__u64  tboot_addr;\t\t\t\t/* 0x058 */\n\tstruct ist_info ist_info;\t\t\t/* 0x060 */\n\t__u64 acpi_rsdp_addr;\t\t\t\t/* 0x070 */\n\t__u8  _pad3[8];\t\t\t\t\t/* 0x078 */\n\t__u8  hd0_info[16];\t/* obsolete! */\t\t/* 0x080 */\n\t__u8  hd1_info[16];\t/* obsolete! */\t\t/* 0x090 */\n\tstruct sys_desc_table sys_desc_table; /* obsolete! */\t/* 0x0a0 */\n\tstruct olpc_ofw_header olpc_ofw_header;\t\t/* 0x0b0 */\n\t__u32 ext_ramdisk_image;\t\t\t/* 0x0c0 */\n\t__u32 ext_ramdisk_size;\t\t\t\t/* 0x0c4 */\n\t__u32 ext_cmd_line_ptr;\t\t\t\t/* 0x0c8 */\n\t__u8  _pad4[112];\t\t\t\t/* 0x0cc */\n\t__u32 cc_blob_address;\t\t\t\t/* 0x13c */\n\tstruct edid_info edid_info;\t\t\t/* 0x140 */\n\tstruct efi_info efi_info;\t\t\t/* 0x1c0 */\n\t__u32 alt_mem_k;\t\t\t\t/* 0x1e0 */\n\t__u32 scratch;\t\t/* Scratch field! */\t/* 0x1e4 */\n\t__u8  e820_entries;\t\t\t\t/* 0x1e8 */\n\t__u8  eddbuf_entries;\t\t\t\t/* 0x1e9 */\n\t__u8  edd_mbr_sig_buf_entries;\t\t\t/* 0x1ea */\n\t__u8  kbd_status;\t\t\t\t/* 0x1eb */\n\t__u8  secure_boot;\t\t\t\t/* 0x1ec */\n\t__u8  _pad5[2];\t\t\t\t\t/* 0x1ed */\n\t/*\n\t * The sentinel is set to a nonzero value (0xff) in header.S.\n\t *\n\t * A bootloader is supposed to only take setup_header and put\n\t * it into a clean boot_params buffer. If it turns out that\n\t * it is clumsy or too generous with the buffer, it most\n\t * probably will pick up the sentinel variable too. The fact\n\t * that this variable then is still 0xff will let kernel\n\t * know that some variables in boot_params are invalid and\n\t * kernel should zero out certain portions of boot_params.\n\t */\n\t__u8  sentinel;\t\t\t\t\t/* 0x1ef */\n\t__u8  _pad6[1];\t\t\t\t\t/* 0x1f0 */\n\tstruct setup_header hdr;    /* setup header */\t/* 0x1f1 */\n\t__u8  _pad7[0x290-0x1f1-sizeof(struct setup_header)];\n\t__u32 edd_mbr_sig_buffer[EDD_MBR_SIG_MAX];\t/* 0x290 */\n\tstruct boot_e820_entry e820_table[E820_MAX_ENTRIES_ZEROPAGE]; /* 0x2d0 */\n\t__u8  _pad8[48];\t\t\t\t/* 0xcd0 */\n\tstruct edd_info eddbuf[EDDMAXNR];\t\t/* 0xd00 */\n\t__u8  _pad9[276];\t\t\t\t/* 0xeec */\n} __attribute__((packed));\n\n/**\n * enum x86_hardware_subarch - x86 hardware subarchitecture\n *\n * The x86 hardware_subarch and hardware_subarch_data were added as of the x86\n * boot protocol 2.07 to help distinguish and support custom x86 boot\n * sequences. This enum represents accepted values for the x86\n * hardware_subarch.  Custom x86 boot sequences (not X86_SUBARCH_PC) do not\n * have or simply *cannot* make use of natural stubs like BIOS or EFI, the\n * hardware_subarch can be used on the Linux entry path to revector to a\n * subarchitecture stub when needed. This subarchitecture stub can be used to\n * set up Linux boot parameters or for special care to account for nonstandard\n * handling of page tables.\n *\n * These enums should only ever be used by x86 code, and the code that uses\n * it should be well contained and compartmentalized.\n *\n * KVM and Xen HVM do not have a subarch as these are expected to follow\n * standard x86 boot entries. If there is a genuine need for \"hypervisor\" type\n * that should be considered separately in the future. Future guest types\n * should seriously consider working with standard x86 boot stubs such as\n * the BIOS or EFI boot stubs.\n *\n * WARNING: this enum is only used for legacy hacks, for platform features that\n *\t    are not easily enumerated or discoverable. You should not ever use\n *\t    this for new features.\n *\n * @X86_SUBARCH_PC: Should be used if the hardware is enumerable using standard\n *\tPC mechanisms (PCI, ACPI) and doesn't need a special boot flow.\n * @X86_SUBARCH_LGUEST: Used for x86 hypervisor demo, lguest, deprecated\n * @X86_SUBARCH_XEN: Used for Xen guest types which follow the PV boot path,\n * \twhich start at asm startup_xen() entry point and later jump to the C\n * \txen_start_kernel() entry point. Both domU and dom0 type of guests are\n * \tcurrently supported through this PV boot path.\n * @X86_SUBARCH_INTEL_MID: Used for Intel MID (Mobile Internet Device) platform\n *\tsystems which do not have the PCI legacy interfaces.\n * @X86_SUBARCH_CE4100: Used for Intel CE media processor (CE4100) SoC\n * \tfor settop boxes and media devices, the use of a subarch for CE4100\n * \tis more of a hack...\n */\nenum x86_hardware_subarch {\n\tX86_SUBARCH_PC = 0,\n\tX86_SUBARCH_LGUEST,\n\tX86_SUBARCH_XEN,\n\tX86_SUBARCH_INTEL_MID,\n\tX86_SUBARCH_CE4100,\n\tX86_NR_SUBARCHS,\n};\n\n#endif /* __ASSEMBLY__ */\n\n#endif /* _ASM_X86_BOOTPARAM_H */\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n *  prepare to run common code\n *\n *  Copyright (C) 2000 Andrea Arcangeli <andrea@suse.de> SuSE\n */\n\n#define DISABLE_BRANCH_PROFILING\n\n/* cpu_feature_enabled() cannot be used this early */\n#define USE_EARLY_PGTABLE_L5\n\n#include <linux/init.h>\n#include <linux/linkage.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/percpu.h>\n#include <linux/start_kernel.h>\n#include <linux/io.h>\n#include <linux/memblock.h>\n#include <linux/cc_platform.h>\n#include <linux/pgtable.h>\n\n#include <asm/processor.h>\n#include <asm/proto.h>\n#include <asm/smp.h>\n#include <asm/setup.h>\n#include <asm/desc.h>\n#include <asm/tlbflush.h>\n#include <asm/sections.h>\n#include <asm/kdebug.h>\n#include <asm/e820/api.h>\n#include <asm/bios_ebda.h>\n#include <asm/bootparam_utils.h>\n#include <asm/microcode.h>\n#include <asm/kasan.h>\n#include <asm/fixmap.h>\n#include <asm/realmode.h>\n#include <asm/extable.h>\n#include <asm/trapnr.h>\n#include <asm/sev.h>\n#include <asm/tdx.h>\n\n/*\n * Manage page tables very early on.\n */\nextern pmd_t early_dynamic_pgts[EARLY_DYNAMIC_PAGE_TABLES][PTRS_PER_PMD];\nstatic unsigned int __initdata next_early_pgt;\npmdval_t early_pmd_flags = __PAGE_KERNEL_LARGE & ~(_PAGE_GLOBAL | _PAGE_NX);\n\n#ifdef CONFIG_X86_5LEVEL\nunsigned int __pgtable_l5_enabled __ro_after_init;\nunsigned int pgdir_shift __ro_after_init = 39;\nEXPORT_SYMBOL(pgdir_shift);\nunsigned int ptrs_per_p4d __ro_after_init = 1;\nEXPORT_SYMBOL(ptrs_per_p4d);\n#endif\n\n#ifdef CONFIG_DYNAMIC_MEMORY_LAYOUT\nunsigned long page_offset_base __ro_after_init = __PAGE_OFFSET_BASE_L4;\nEXPORT_SYMBOL(page_offset_base);\nunsigned long vmalloc_base __ro_after_init = __VMALLOC_BASE_L4;\nEXPORT_SYMBOL(vmalloc_base);\nunsigned long vmemmap_base __ro_after_init = __VMEMMAP_BASE_L4;\nEXPORT_SYMBOL(vmemmap_base);\n#endif\n\n/*\n * GDT used on the boot CPU before switching to virtual addresses.\n */\nstatic struct desc_struct startup_gdt[GDT_ENTRIES] = {\n\t[GDT_ENTRY_KERNEL32_CS]         = GDT_ENTRY_INIT(0xc09b, 0, 0xfffff),\n\t[GDT_ENTRY_KERNEL_CS]           = GDT_ENTRY_INIT(0xa09b, 0, 0xfffff),\n\t[GDT_ENTRY_KERNEL_DS]           = GDT_ENTRY_INIT(0xc093, 0, 0xfffff),\n};\n\n/*\n * Address needs to be set at runtime because it references the startup_gdt\n * while the kernel still uses a direct mapping.\n */\nstatic struct desc_ptr startup_gdt_descr = {\n\t.size = sizeof(startup_gdt),\n\t.address = 0,\n};\n\n#define __head\t__section(\".head.text\")\n\nstatic void __head *fixup_pointer(void *ptr, unsigned long physaddr)\n{\n\treturn ptr - (void *)_text + (void *)physaddr;\n}\n\nstatic unsigned long __head *fixup_long(void *ptr, unsigned long physaddr)\n{\n\treturn fixup_pointer(ptr, physaddr);\n}\n\n#ifdef CONFIG_X86_5LEVEL\nstatic unsigned int __head *fixup_int(void *ptr, unsigned long physaddr)\n{\n\treturn fixup_pointer(ptr, physaddr);\n}\n\nstatic bool __head check_la57_support(unsigned long physaddr)\n{\n\t/*\n\t * 5-level paging is detected and enabled at kernel decompression\n\t * stage. Only check if it has been enabled there.\n\t */\n\tif (!(native_read_cr4() & X86_CR4_LA57))\n\t\treturn false;\n\n\t*fixup_int(&__pgtable_l5_enabled, physaddr) = 1;\n\t*fixup_int(&pgdir_shift, physaddr) = 48;\n\t*fixup_int(&ptrs_per_p4d, physaddr) = 512;\n\t*fixup_long(&page_offset_base, physaddr) = __PAGE_OFFSET_BASE_L5;\n\t*fixup_long(&vmalloc_base, physaddr) = __VMALLOC_BASE_L5;\n\t*fixup_long(&vmemmap_base, physaddr) = __VMEMMAP_BASE_L5;\n\n\treturn true;\n}\n#else\nstatic bool __head check_la57_support(unsigned long physaddr)\n{\n\treturn false;\n}\n#endif\n\nstatic unsigned long __head sme_postprocess_startup(struct boot_params *bp, pmdval_t *pmd)\n{\n\tunsigned long vaddr, vaddr_end;\n\tint i;\n\n\t/* Encrypt the kernel and related (if SME is active) */\n\tsme_encrypt_kernel(bp);\n\n\t/*\n\t * Clear the memory encryption mask from the .bss..decrypted section.\n\t * The bss section will be memset to zero later in the initialization so\n\t * there is no need to zero it after changing the memory encryption\n\t * attribute.\n\t */\n\tif (sme_get_me_mask()) {\n\t\tvaddr = (unsigned long)__start_bss_decrypted;\n\t\tvaddr_end = (unsigned long)__end_bss_decrypted;\n\n\t\tfor (; vaddr < vaddr_end; vaddr += PMD_SIZE) {\n\t\t\t/*\n\t\t\t * On SNP, transition the page to shared in the RMP table so that\n\t\t\t * it is consistent with the page table attribute change.\n\t\t\t *\n\t\t\t * __start_bss_decrypted has a virtual address in the high range\n\t\t\t * mapping (kernel .text). PVALIDATE, by way of\n\t\t\t * early_snp_set_memory_shared(), requires a valid virtual\n\t\t\t * address but the kernel is currently running off of the identity\n\t\t\t * mapping so use __pa() to get a *currently* valid virtual address.\n\t\t\t */\n\t\t\tearly_snp_set_memory_shared(__pa(vaddr), __pa(vaddr), PTRS_PER_PMD);\n\n\t\t\ti = pmd_index(vaddr);\n\t\t\tpmd[i] -= sme_get_me_mask();\n\t\t}\n\t}\n\n\t/*\n\t * Return the SME encryption mask (if SME is active) to be used as a\n\t * modifier for the initial pgdir entry programmed into CR3.\n\t */\n\treturn sme_get_me_mask();\n}\n\n/* Code in __startup_64() can be relocated during execution, but the compiler\n * doesn't have to generate PC-relative relocations when accessing globals from\n * that function. Clang actually does not generate them, which leads to\n * boot-time crashes. To work around this problem, every global pointer must\n * be adjusted using fixup_pointer().\n */\nunsigned long __head __startup_64(unsigned long physaddr,\n\t\t\t\t  struct boot_params *bp)\n{\n\tunsigned long load_delta, *p;\n\tunsigned long pgtable_flags;\n\tpgdval_t *pgd;\n\tp4dval_t *p4d;\n\tpudval_t *pud;\n\tpmdval_t *pmd, pmd_entry;\n\tpteval_t *mask_ptr;\n\tbool la57;\n\tint i;\n\tunsigned int *next_pgt_ptr;\n\n\tla57 = check_la57_support(physaddr);\n\n\t/* Is the address too large? */\n\tif (physaddr >> MAX_PHYSMEM_BITS)\n\t\tfor (;;);\n\n\t/*\n\t * Compute the delta between the address I am compiled to run at\n\t * and the address I am actually running at.\n\t */\n\tload_delta = physaddr - (unsigned long)(_text - __START_KERNEL_map);\n\n\t/* Is the address not 2M aligned? */\n\tif (load_delta & ~PMD_PAGE_MASK)\n\t\tfor (;;);\n\n\t/* Include the SME encryption mask in the fixup value */\n\tload_delta += sme_get_me_mask();\n\n\t/* Fixup the physical addresses in the page table */\n\n\tpgd = fixup_pointer(&early_top_pgt, physaddr);\n\tp = pgd + pgd_index(__START_KERNEL_map);\n\tif (la57)\n\t\t*p = (unsigned long)level4_kernel_pgt;\n\telse\n\t\t*p = (unsigned long)level3_kernel_pgt;\n\t*p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;\n\n\tif (la57) {\n\t\tp4d = fixup_pointer(&level4_kernel_pgt, physaddr);\n\t\tp4d[511] += load_delta;\n\t}\n\n\tpud = fixup_pointer(&level3_kernel_pgt, physaddr);\n\tpud[510] += load_delta;\n\tpud[511] += load_delta;\n\n\tpmd = fixup_pointer(level2_fixmap_pgt, physaddr);\n\tfor (i = FIXMAP_PMD_TOP; i > FIXMAP_PMD_TOP - FIXMAP_PMD_NUM; i--)\n\t\tpmd[i] += load_delta;\n\n\t/*\n\t * Set up the identity mapping for the switchover.  These\n\t * entries should *NOT* have the global bit set!  This also\n\t * creates a bunch of nonsense entries but that is fine --\n\t * it avoids problems around wraparound.\n\t */\n\n\tnext_pgt_ptr = fixup_pointer(&next_early_pgt, physaddr);\n\tpud = fixup_pointer(early_dynamic_pgts[(*next_pgt_ptr)++], physaddr);\n\tpmd = fixup_pointer(early_dynamic_pgts[(*next_pgt_ptr)++], physaddr);\n\n\tpgtable_flags = _KERNPG_TABLE_NOENC + sme_get_me_mask();\n\n\tif (la57) {\n\t\tp4d = fixup_pointer(early_dynamic_pgts[(*next_pgt_ptr)++],\n\t\t\t\t    physaddr);\n\n\t\ti = (physaddr >> PGDIR_SHIFT) % PTRS_PER_PGD;\n\t\tpgd[i + 0] = (pgdval_t)p4d + pgtable_flags;\n\t\tpgd[i + 1] = (pgdval_t)p4d + pgtable_flags;\n\n\t\ti = physaddr >> P4D_SHIFT;\n\t\tp4d[(i + 0) % PTRS_PER_P4D] = (pgdval_t)pud + pgtable_flags;\n\t\tp4d[(i + 1) % PTRS_PER_P4D] = (pgdval_t)pud + pgtable_flags;\n\t} else {\n\t\ti = (physaddr >> PGDIR_SHIFT) % PTRS_PER_PGD;\n\t\tpgd[i + 0] = (pgdval_t)pud + pgtable_flags;\n\t\tpgd[i + 1] = (pgdval_t)pud + pgtable_flags;\n\t}\n\n\ti = physaddr >> PUD_SHIFT;\n\tpud[(i + 0) % PTRS_PER_PUD] = (pudval_t)pmd + pgtable_flags;\n\tpud[(i + 1) % PTRS_PER_PUD] = (pudval_t)pmd + pgtable_flags;\n\n\tpmd_entry = __PAGE_KERNEL_LARGE_EXEC & ~_PAGE_GLOBAL;\n\t/* Filter out unsupported __PAGE_KERNEL_* bits: */\n\tmask_ptr = fixup_pointer(&__supported_pte_mask, physaddr);\n\tpmd_entry &= *mask_ptr;\n\tpmd_entry += sme_get_me_mask();\n\tpmd_entry +=  physaddr;\n\n\tfor (i = 0; i < DIV_ROUND_UP(_end - _text, PMD_SIZE); i++) {\n\t\tint idx = i + (physaddr >> PMD_SHIFT);\n\n\t\tpmd[idx % PTRS_PER_PMD] = pmd_entry + i * PMD_SIZE;\n\t}\n\n\t/*\n\t * Fixup the kernel text+data virtual addresses. Note that\n\t * we might write invalid pmds, when the kernel is relocated\n\t * cleanup_highmap() fixes this up along with the mappings\n\t * beyond _end.\n\t *\n\t * Only the region occupied by the kernel image has so far\n\t * been checked against the table of usable memory regions\n\t * provided by the firmware, so invalidate pages outside that\n\t * region. A page table entry that maps to a reserved area of\n\t * memory would allow processor speculation into that area,\n\t * and on some hardware (particularly the UV platform) even\n\t * speculative access to some reserved areas is caught as an\n\t * error, causing the BIOS to halt the system.\n\t */\n\n\tpmd = fixup_pointer(level2_kernel_pgt, physaddr);\n\n\t/* invalidate pages before the kernel image */\n\tfor (i = 0; i < pmd_index((unsigned long)_text); i++)\n\t\tpmd[i] &= ~_PAGE_PRESENT;\n\n\t/* fixup pages that are part of the kernel image */\n\tfor (; i <= pmd_index((unsigned long)_end); i++)\n\t\tif (pmd[i] & _PAGE_PRESENT)\n\t\t\tpmd[i] += load_delta;\n\n\t/* invalidate pages after the kernel image */\n\tfor (; i < PTRS_PER_PMD; i++)\n\t\tpmd[i] &= ~_PAGE_PRESENT;\n\n\t/*\n\t * Fixup phys_base - remove the memory encryption mask to obtain\n\t * the true physical address.\n\t */\n\t*fixup_long(&phys_base, physaddr) += load_delta - sme_get_me_mask();\n\n\treturn sme_postprocess_startup(bp, pmd);\n}\n\n/* Wipe all early page tables except for the kernel symbol map */\nstatic void __init reset_early_page_tables(void)\n{\n\tmemset(early_top_pgt, 0, sizeof(pgd_t)*(PTRS_PER_PGD-1));\n\tnext_early_pgt = 0;\n\twrite_cr3(__sme_pa_nodebug(early_top_pgt));\n}\n\n/* Create a new PMD entry */\nbool __init __early_make_pgtable(unsigned long address, pmdval_t pmd)\n{\n\tunsigned long physaddr = address - __PAGE_OFFSET;\n\tpgdval_t pgd, *pgd_p;\n\tp4dval_t p4d, *p4d_p;\n\tpudval_t pud, *pud_p;\n\tpmdval_t *pmd_p;\n\n\t/* Invalid address or early pgt is done ?  */\n\tif (physaddr >= MAXMEM || read_cr3_pa() != __pa_nodebug(early_top_pgt))\n\t\treturn false;\n\nagain:\n\tpgd_p = &early_top_pgt[pgd_index(address)].pgd;\n\tpgd = *pgd_p;\n\n\t/*\n\t * The use of __START_KERNEL_map rather than __PAGE_OFFSET here is\n\t * critical -- __PAGE_OFFSET would point us back into the dynamic\n\t * range and we might end up looping forever...\n\t */\n\tif (!pgtable_l5_enabled())\n\t\tp4d_p = pgd_p;\n\telse if (pgd)\n\t\tp4d_p = (p4dval_t *)((pgd & PTE_PFN_MASK) + __START_KERNEL_map - phys_base);\n\telse {\n\t\tif (next_early_pgt >= EARLY_DYNAMIC_PAGE_TABLES) {\n\t\t\treset_early_page_tables();\n\t\t\tgoto again;\n\t\t}\n\n\t\tp4d_p = (p4dval_t *)early_dynamic_pgts[next_early_pgt++];\n\t\tmemset(p4d_p, 0, sizeof(*p4d_p) * PTRS_PER_P4D);\n\t\t*pgd_p = (pgdval_t)p4d_p - __START_KERNEL_map + phys_base + _KERNPG_TABLE;\n\t}\n\tp4d_p += p4d_index(address);\n\tp4d = *p4d_p;\n\n\tif (p4d)\n\t\tpud_p = (pudval_t *)((p4d & PTE_PFN_MASK) + __START_KERNEL_map - phys_base);\n\telse {\n\t\tif (next_early_pgt >= EARLY_DYNAMIC_PAGE_TABLES) {\n\t\t\treset_early_page_tables();\n\t\t\tgoto again;\n\t\t}\n\n\t\tpud_p = (pudval_t *)early_dynamic_pgts[next_early_pgt++];\n\t\tmemset(pud_p, 0, sizeof(*pud_p) * PTRS_PER_PUD);\n\t\t*p4d_p = (p4dval_t)pud_p - __START_KERNEL_map + phys_base + _KERNPG_TABLE;\n\t}\n\tpud_p += pud_index(address);\n\tpud = *pud_p;\n\n\tif (pud)\n\t\tpmd_p = (pmdval_t *)((pud & PTE_PFN_MASK) + __START_KERNEL_map - phys_base);\n\telse {\n\t\tif (next_early_pgt >= EARLY_DYNAMIC_PAGE_TABLES) {\n\t\t\treset_early_page_tables();\n\t\t\tgoto again;\n\t\t}\n\n\t\tpmd_p = (pmdval_t *)early_dynamic_pgts[next_early_pgt++];\n\t\tmemset(pmd_p, 0, sizeof(*pmd_p) * PTRS_PER_PMD);\n\t\t*pud_p = (pudval_t)pmd_p - __START_KERNEL_map + phys_base + _KERNPG_TABLE;\n\t}\n\tpmd_p[pmd_index(address)] = pmd;\n\n\treturn true;\n}\n\nstatic bool __init early_make_pgtable(unsigned long address)\n{\n\tunsigned long physaddr = address - __PAGE_OFFSET;\n\tpmdval_t pmd;\n\n\tpmd = (physaddr & PMD_MASK) + early_pmd_flags;\n\n\treturn __early_make_pgtable(address, pmd);\n}\n\nvoid __init do_early_exception(struct pt_regs *regs, int trapnr)\n{\n\tif (trapnr == X86_TRAP_PF &&\n\t    early_make_pgtable(native_read_cr2()))\n\t\treturn;\n\n\tif (IS_ENABLED(CONFIG_AMD_MEM_ENCRYPT) &&\n\t    trapnr == X86_TRAP_VC && handle_vc_boot_ghcb(regs))\n\t\treturn;\n\n\tif (trapnr == X86_TRAP_VE && tdx_early_handle_ve(regs))\n\t\treturn;\n\n\tearly_fixup_exception(regs, trapnr);\n}\n\n/* Don't add a printk in there. printk relies on the PDA which is not initialized \n   yet. */\nvoid __init clear_bss(void)\n{\n\tmemset(__bss_start, 0,\n\t       (unsigned long) __bss_stop - (unsigned long) __bss_start);\n\tmemset(__brk_base, 0,\n\t       (unsigned long) __brk_limit - (unsigned long) __brk_base);\n}\n\nstatic unsigned long get_cmd_line_ptr(void)\n{\n\tunsigned long cmd_line_ptr = boot_params.hdr.cmd_line_ptr;\n\n\tcmd_line_ptr |= (u64)boot_params.ext_cmd_line_ptr << 32;\n\n\treturn cmd_line_ptr;\n}\n\nstatic void __init copy_bootdata(char *real_mode_data)\n{\n\tchar * command_line;\n\tunsigned long cmd_line_ptr;\n\n\t/*\n\t * If SME is active, this will create decrypted mappings of the\n\t * boot data in advance of the copy operations.\n\t */\n\tsme_map_bootdata(real_mode_data);\n\n\tmemcpy(&boot_params, real_mode_data, sizeof(boot_params));\n\tsanitize_boot_params(&boot_params);\n\tcmd_line_ptr = get_cmd_line_ptr();\n\tif (cmd_line_ptr) {\n\t\tcommand_line = __va(cmd_line_ptr);\n\t\tmemcpy(boot_command_line, command_line, COMMAND_LINE_SIZE);\n\t}\n\n\t/*\n\t * The old boot data is no longer needed and won't be reserved,\n\t * freeing up that memory for use by the system. If SME is active,\n\t * we need to remove the mappings that were created so that the\n\t * memory doesn't remain mapped as decrypted.\n\t */\n\tsme_unmap_bootdata(real_mode_data);\n}\n\nasmlinkage __visible void __init x86_64_start_kernel(char * real_mode_data)\n{\n\t/*\n\t * Build-time sanity checks on the kernel image and module\n\t * area mappings. (these are purely build-time and produce no code)\n\t */\n\tBUILD_BUG_ON(MODULES_VADDR < __START_KERNEL_map);\n\tBUILD_BUG_ON(MODULES_VADDR - __START_KERNEL_map < KERNEL_IMAGE_SIZE);\n\tBUILD_BUG_ON(MODULES_LEN + KERNEL_IMAGE_SIZE > 2*PUD_SIZE);\n\tBUILD_BUG_ON((__START_KERNEL_map & ~PMD_MASK) != 0);\n\tBUILD_BUG_ON((MODULES_VADDR & ~PMD_MASK) != 0);\n\tBUILD_BUG_ON(!(MODULES_VADDR > __START_KERNEL));\n\tMAYBE_BUILD_BUG_ON(!(((MODULES_END - 1) & PGDIR_MASK) ==\n\t\t\t\t(__START_KERNEL & PGDIR_MASK)));\n\tBUILD_BUG_ON(__fix_to_virt(__end_of_fixed_addresses) <= MODULES_END);\n\n\tcr4_init_shadow();\n\n\t/* Kill off the identity-map trampoline */\n\treset_early_page_tables();\n\n\tclear_bss();\n\n\t/*\n\t * This needs to happen *before* kasan_early_init() because latter maps stuff\n\t * into that page.\n\t */\n\tclear_page(init_top_pgt);\n\n\t/*\n\t * SME support may update early_pmd_flags to include the memory\n\t * encryption mask, so it needs to be called before anything\n\t * that may generate a page fault.\n\t */\n\tsme_early_init();\n\n\tkasan_early_init();\n\n\t/*\n\t * Flush global TLB entries which could be left over from the trampoline page\n\t * table.\n\t *\n\t * This needs to happen *after* kasan_early_init() as KASAN-enabled .configs\n\t * instrument native_write_cr4() so KASAN must be initialized for that\n\t * instrumentation to work.\n\t */\n\t__native_tlb_flush_global(this_cpu_read(cpu_tlbstate.cr4));\n\n\tidt_setup_early_handler();\n\n\t/* Needed before cc_platform_has() can be used for TDX */\n\ttdx_early_init();\n\n\tcopy_bootdata(__va(real_mode_data));\n\n\t/*\n\t * Load microcode early on BSP.\n\t */\n\tload_ucode_bsp();\n\n\t/* set init_top_pgt kernel high mapping*/\n\tinit_top_pgt[511] = early_top_pgt[511];\n\n\tx86_64_start_reservations(real_mode_data);\n}\n\nvoid __init x86_64_start_reservations(char *real_mode_data)\n{\n\t/* version is always not zero if it is copied */\n\tif (!boot_params.hdr.version)\n\t\tcopy_bootdata(__va(real_mode_data));\n\n\tx86_early_init_platform_quirks();\n\n\tswitch (boot_params.hdr.hardware_subarch) {\n\tcase X86_SUBARCH_INTEL_MID:\n\t\tx86_intel_mid_early_setup();\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tstart_kernel();\n}\n\n/*\n * Data structures and code used for IDT setup in head_64.S. The bringup-IDT is\n * used until the idt_table takes over. On the boot CPU this happens in\n * x86_64_start_kernel(), on secondary CPUs in start_secondary(). In both cases\n * this happens in the functions called from head_64.S.\n *\n * The idt_table can't be used that early because all the code modifying it is\n * in idt.c and can be instrumented by tracing or KASAN, which both don't work\n * during early CPU bringup. Also the idt_table has the runtime vectors\n * configured which require certain CPU state to be setup already (like TSS),\n * which also hasn't happened yet in early CPU bringup.\n */\nstatic gate_desc bringup_idt_table[NUM_EXCEPTION_VECTORS] __page_aligned_data;\n\nstatic struct desc_ptr bringup_idt_descr = {\n\t.size\t\t= (NUM_EXCEPTION_VECTORS * sizeof(gate_desc)) - 1,\n\t.address\t= 0, /* Set at runtime */\n};\n\nstatic void set_bringup_idt_handler(gate_desc *idt, int n, void *handler)\n{\n#ifdef CONFIG_AMD_MEM_ENCRYPT\n\tstruct idt_data data;\n\tgate_desc desc;\n\n\tinit_idt_data(&data, n, handler);\n\tidt_init_desc(&desc, &data);\n\tnative_write_idt_entry(idt, n, &desc);\n#endif\n}\n\n/* This runs while still in the direct mapping */\nstatic void startup_64_load_idt(unsigned long physbase)\n{\n\tstruct desc_ptr *desc = fixup_pointer(&bringup_idt_descr, physbase);\n\tgate_desc *idt = fixup_pointer(bringup_idt_table, physbase);\n\n\n\tif (IS_ENABLED(CONFIG_AMD_MEM_ENCRYPT)) {\n\t\tvoid *handler;\n\n\t\t/* VMM Communication Exception */\n\t\thandler = fixup_pointer(vc_no_ghcb, physbase);\n\t\tset_bringup_idt_handler(idt, X86_TRAP_VC, handler);\n\t}\n\n\tdesc->address = (unsigned long)idt;\n\tnative_load_idt(desc);\n}\n\n/* This is used when running on kernel addresses */\nvoid early_setup_idt(void)\n{\n\t/* VMM Communication Exception */\n\tif (IS_ENABLED(CONFIG_AMD_MEM_ENCRYPT)) {\n\t\tsetup_ghcb();\n\t\tset_bringup_idt_handler(bringup_idt_table, X86_TRAP_VC, vc_boot_ghcb);\n\t}\n\n\tbringup_idt_descr.address = (unsigned long)bringup_idt_table;\n\tnative_load_idt(&bringup_idt_descr);\n}\n\n/*\n * Setup boot CPU state needed before kernel switches to virtual addresses.\n */\nvoid __head startup_64_setup_env(unsigned long physbase)\n{\n\t/* Load GDT */\n\tstartup_gdt_descr.address = (unsigned long)fixup_pointer(startup_gdt, physbase);\n\tnative_load_gdt(&startup_gdt_descr);\n\n\t/* New GDT is live - reload data segment registers */\n\tasm volatile(\"movl %%eax, %%ds\\n\"\n\t\t     \"movl %%eax, %%ss\\n\"\n\t\t     \"movl %%eax, %%es\\n\" : : \"a\"(__KERNEL_DS) : \"memory\");\n\n\tstartup_64_load_idt(physbase);\n}\n", "/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * ld script for the x86 kernel\n *\n * Historic 32-bit version written by Martin Mares <mj@atrey.karlin.mff.cuni.cz>\n *\n * Modernisation, unification and other changes and fixes:\n *   Copyright (C) 2007-2009  Sam Ravnborg <sam@ravnborg.org>\n *\n *\n * Don't define absolute symbols until and unless you know that symbol\n * value is should remain constant even if kernel image is relocated\n * at run time. Absolute symbols are not relocated. If symbol value should\n * change if kernel is relocated, make the symbol section relative and\n * put it inside the section definition.\n */\n\n#ifdef CONFIG_X86_32\n#define LOAD_OFFSET __PAGE_OFFSET\n#else\n#define LOAD_OFFSET __START_KERNEL_map\n#endif\n\n#define RUNTIME_DISCARD_EXIT\n#define EMITS_PT_NOTE\n#define RO_EXCEPTION_TABLE_ALIGN\t16\n\n#include <asm-generic/vmlinux.lds.h>\n#include <asm/asm-offsets.h>\n#include <asm/thread_info.h>\n#include <asm/page_types.h>\n#include <asm/orc_lookup.h>\n#include <asm/cache.h>\n#include <asm/boot.h>\n\n#undef i386     /* in case the preprocessor is a 32bit one */\n\nOUTPUT_FORMAT(CONFIG_OUTPUT_FORMAT)\n\n#ifdef CONFIG_X86_32\nOUTPUT_ARCH(i386)\nENTRY(phys_startup_32)\n#else\nOUTPUT_ARCH(i386:x86-64)\nENTRY(phys_startup_64)\n#endif\n\njiffies = jiffies_64;\n\n#if defined(CONFIG_X86_64)\n/*\n * On 64-bit, align RODATA to 2MB so we retain large page mappings for\n * boundaries spanning kernel text, rodata and data sections.\n *\n * However, kernel identity mappings will have different RWX permissions\n * to the pages mapping to text and to the pages padding (which are freed) the\n * text section. Hence kernel identity mappings will be broken to smaller\n * pages. For 64-bit, kernel text and kernel identity mappings are different,\n * so we can enable protection checks as well as retain 2MB large page\n * mappings for kernel text.\n */\n#define X86_ALIGN_RODATA_BEGIN\t. = ALIGN(HPAGE_SIZE);\n\n#define X86_ALIGN_RODATA_END\t\t\t\t\t\\\n\t\t. = ALIGN(HPAGE_SIZE);\t\t\t\t\\\n\t\t__end_rodata_hpage_align = .;\t\t\t\\\n\t\t__end_rodata_aligned = .;\n\n#define ALIGN_ENTRY_TEXT_BEGIN\t. = ALIGN(PMD_SIZE);\n#define ALIGN_ENTRY_TEXT_END\t. = ALIGN(PMD_SIZE);\n\n/*\n * This section contains data which will be mapped as decrypted. Memory\n * encryption operates on a page basis. Make this section PMD-aligned\n * to avoid splitting the pages while mapping the section early.\n *\n * Note: We use a separate section so that only this section gets\n * decrypted to avoid exposing more than we wish.\n */\n#define BSS_DECRYPTED\t\t\t\t\t\t\\\n\t. = ALIGN(PMD_SIZE);\t\t\t\t\t\\\n\t__start_bss_decrypted = .;\t\t\t\t\\\n\t*(.bss..decrypted);\t\t\t\t\t\\\n\t. = ALIGN(PAGE_SIZE);\t\t\t\t\t\\\n\t__start_bss_decrypted_unused = .;\t\t\t\\\n\t. = ALIGN(PMD_SIZE);\t\t\t\t\t\\\n\t__end_bss_decrypted = .;\t\t\t\t\\\n\n#else\n\n#define X86_ALIGN_RODATA_BEGIN\n#define X86_ALIGN_RODATA_END\t\t\t\t\t\\\n\t\t. = ALIGN(PAGE_SIZE);\t\t\t\t\\\n\t\t__end_rodata_aligned = .;\n\n#define ALIGN_ENTRY_TEXT_BEGIN\n#define ALIGN_ENTRY_TEXT_END\n#define BSS_DECRYPTED\n\n#endif\n\nPHDRS {\n\ttext PT_LOAD FLAGS(5);          /* R_E */\n\tdata PT_LOAD FLAGS(6);          /* RW_ */\n#ifdef CONFIG_X86_64\n#ifdef CONFIG_SMP\n\tpercpu PT_LOAD FLAGS(6);        /* RW_ */\n#endif\n\tinit PT_LOAD FLAGS(7);          /* RWE */\n#endif\n\tnote PT_NOTE FLAGS(0);          /* ___ */\n}\n\nSECTIONS\n{\n#ifdef CONFIG_X86_32\n\t. = LOAD_OFFSET + LOAD_PHYSICAL_ADDR;\n\tphys_startup_32 = ABSOLUTE(startup_32 - LOAD_OFFSET);\n#else\n\t. = __START_KERNEL;\n\tphys_startup_64 = ABSOLUTE(startup_64 - LOAD_OFFSET);\n#endif\n\n\t/* Text and read-only data */\n\t.text :  AT(ADDR(.text) - LOAD_OFFSET) {\n\t\t_text = .;\n\t\t_stext = .;\n\t\t/* bootstrapping code */\n\t\tHEAD_TEXT\n\t\tTEXT_TEXT\n\t\tSCHED_TEXT\n\t\tCPUIDLE_TEXT\n\t\tLOCK_TEXT\n\t\tKPROBES_TEXT\n\t\tALIGN_ENTRY_TEXT_BEGIN\n\t\tENTRY_TEXT\n\t\tALIGN_ENTRY_TEXT_END\n\t\tSOFTIRQENTRY_TEXT\n\t\tSTATIC_CALL_TEXT\n\t\t*(.gnu.warning)\n\n#ifdef CONFIG_RETPOLINE\n\t\t__indirect_thunk_start = .;\n\t\t*(.text.__x86.indirect_thunk)\n\t\t__indirect_thunk_end = .;\n#endif\n\t} :text =0xcccc\n\n\t/* End of text section, which should occupy whole number of pages */\n\t_etext = .;\n\t. = ALIGN(PAGE_SIZE);\n\n\tX86_ALIGN_RODATA_BEGIN\n\tRO_DATA(PAGE_SIZE)\n\tX86_ALIGN_RODATA_END\n\n\t/* Data */\n\t.data : AT(ADDR(.data) - LOAD_OFFSET) {\n\t\t/* Start of data section */\n\t\t_sdata = .;\n\n\t\t/* init_task */\n\t\tINIT_TASK_DATA(THREAD_SIZE)\n\n#ifdef CONFIG_X86_32\n\t\t/* 32 bit has nosave before _edata */\n\t\tNOSAVE_DATA\n#endif\n\n\t\tPAGE_ALIGNED_DATA(PAGE_SIZE)\n\n\t\tCACHELINE_ALIGNED_DATA(L1_CACHE_BYTES)\n\n\t\tDATA_DATA\n\t\tCONSTRUCTORS\n\n\t\t/* rarely changed data like cpu maps */\n\t\tREAD_MOSTLY_DATA(INTERNODE_CACHE_BYTES)\n\n\t\t/* End of data section */\n\t\t_edata = .;\n\t} :data\n\n\tBUG_TABLE\n\n\tORC_UNWIND_TABLE\n\n\t. = ALIGN(PAGE_SIZE);\n\t__vvar_page = .;\n\n\t.vvar : AT(ADDR(.vvar) - LOAD_OFFSET) {\n\t\t/* work around gold bug 13023 */\n\t\t__vvar_beginning_hack = .;\n\n\t\t/* Place all vvars at the offsets in asm/vvar.h. */\n#define EMIT_VVAR(name, offset)\t\t\t\t\\\n\t\t. = __vvar_beginning_hack + offset;\t\\\n\t\t*(.vvar_ ## name)\n#include <asm/vvar.h>\n#undef EMIT_VVAR\n\n\t\t/*\n\t\t * Pad the rest of the page with zeros.  Otherwise the loader\n\t\t * can leave garbage here.\n\t\t */\n\t\t. = __vvar_beginning_hack + PAGE_SIZE;\n\t} :data\n\n\t. = ALIGN(__vvar_page + PAGE_SIZE, PAGE_SIZE);\n\n\t/* Init code and data - will be freed after init */\n\t. = ALIGN(PAGE_SIZE);\n\t.init.begin : AT(ADDR(.init.begin) - LOAD_OFFSET) {\n\t\t__init_begin = .; /* paired with __init_end */\n\t}\n\n#if defined(CONFIG_X86_64) && defined(CONFIG_SMP)\n\t/*\n\t * percpu offsets are zero-based on SMP.  PERCPU_VADDR() changes the\n\t * output PHDR, so the next output section - .init.text - should\n\t * start another segment - init.\n\t */\n\tPERCPU_VADDR(INTERNODE_CACHE_BYTES, 0, :percpu)\n\tASSERT(SIZEOF(.data..percpu) < CONFIG_PHYSICAL_START,\n\t       \"per-CPU data too large - increase CONFIG_PHYSICAL_START\")\n#endif\n\n\tINIT_TEXT_SECTION(PAGE_SIZE)\n#ifdef CONFIG_X86_64\n\t:init\n#endif\n\n\t/*\n\t * Section for code used exclusively before alternatives are run. All\n\t * references to such code must be patched out by alternatives, normally\n\t * by using X86_FEATURE_ALWAYS CPU feature bit.\n\t *\n\t * See static_cpu_has() for an example.\n\t */\n\t.altinstr_aux : AT(ADDR(.altinstr_aux) - LOAD_OFFSET) {\n\t\t*(.altinstr_aux)\n\t}\n\n\tINIT_DATA_SECTION(16)\n\n\t.x86_cpu_dev.init : AT(ADDR(.x86_cpu_dev.init) - LOAD_OFFSET) {\n\t\t__x86_cpu_dev_start = .;\n\t\t*(.x86_cpu_dev.init)\n\t\t__x86_cpu_dev_end = .;\n\t}\n\n#ifdef CONFIG_X86_INTEL_MID\n\t.x86_intel_mid_dev.init : AT(ADDR(.x86_intel_mid_dev.init) - \\\n\t\t\t\t\t\t\t\tLOAD_OFFSET) {\n\t\t__x86_intel_mid_dev_start = .;\n\t\t*(.x86_intel_mid_dev.init)\n\t\t__x86_intel_mid_dev_end = .;\n\t}\n#endif\n\n\t/*\n\t * start address and size of operations which during runtime\n\t * can be patched with virtualization friendly instructions or\n\t * baremetal native ones. Think page table operations.\n\t * Details in paravirt_types.h\n\t */\n\t. = ALIGN(8);\n\t.parainstructions : AT(ADDR(.parainstructions) - LOAD_OFFSET) {\n\t\t__parainstructions = .;\n\t\t*(.parainstructions)\n\t\t__parainstructions_end = .;\n\t}\n\n#ifdef CONFIG_RETPOLINE\n\t/*\n\t * List of instructions that call/jmp/jcc to retpoline thunks\n\t * __x86_indirect_thunk_*(). These instructions can be patched along\n\t * with alternatives, after which the section can be freed.\n\t */\n\t. = ALIGN(8);\n\t.retpoline_sites : AT(ADDR(.retpoline_sites) - LOAD_OFFSET) {\n\t\t__retpoline_sites = .;\n\t\t*(.retpoline_sites)\n\t\t__retpoline_sites_end = .;\n\t}\n#endif\n\n#ifdef CONFIG_X86_KERNEL_IBT\n\t. = ALIGN(8);\n\t.ibt_endbr_seal : AT(ADDR(.ibt_endbr_seal) - LOAD_OFFSET) {\n\t\t__ibt_endbr_seal = .;\n\t\t*(.ibt_endbr_seal)\n\t\t__ibt_endbr_seal_end = .;\n\t}\n#endif\n\n\t/*\n\t * struct alt_inst entries. From the header (alternative.h):\n\t * \"Alternative instructions for different CPU types or capabilities\"\n\t * Think locking instructions on spinlocks.\n\t */\n\t. = ALIGN(8);\n\t.altinstructions : AT(ADDR(.altinstructions) - LOAD_OFFSET) {\n\t\t__alt_instructions = .;\n\t\t*(.altinstructions)\n\t\t__alt_instructions_end = .;\n\t}\n\n\t/*\n\t * And here are the replacement instructions. The linker sticks\n\t * them as binary blobs. The .altinstructions has enough data to\n\t * get the address and the length of them to patch the kernel safely.\n\t */\n\t.altinstr_replacement : AT(ADDR(.altinstr_replacement) - LOAD_OFFSET) {\n\t\t*(.altinstr_replacement)\n\t}\n\n\t. = ALIGN(8);\n\t.apicdrivers : AT(ADDR(.apicdrivers) - LOAD_OFFSET) {\n\t\t__apicdrivers = .;\n\t\t*(.apicdrivers);\n\t\t__apicdrivers_end = .;\n\t}\n\n\t. = ALIGN(8);\n\t/*\n\t * .exit.text is discarded at runtime, not link time, to deal with\n\t *  references from .altinstructions\n\t */\n\t.exit.text : AT(ADDR(.exit.text) - LOAD_OFFSET) {\n\t\tEXIT_TEXT\n\t}\n\n\t.exit.data : AT(ADDR(.exit.data) - LOAD_OFFSET) {\n\t\tEXIT_DATA\n\t}\n\n#if !defined(CONFIG_X86_64) || !defined(CONFIG_SMP)\n\tPERCPU_SECTION(INTERNODE_CACHE_BYTES)\n#endif\n\n\t. = ALIGN(PAGE_SIZE);\n\n\t/* freed after init ends here */\n\t.init.end : AT(ADDR(.init.end) - LOAD_OFFSET) {\n\t\t__init_end = .;\n\t}\n\n\t/*\n\t * smp_locks might be freed after init\n\t * start/end must be page aligned\n\t */\n\t. = ALIGN(PAGE_SIZE);\n\t.smp_locks : AT(ADDR(.smp_locks) - LOAD_OFFSET) {\n\t\t__smp_locks = .;\n\t\t*(.smp_locks)\n\t\t. = ALIGN(PAGE_SIZE);\n\t\t__smp_locks_end = .;\n\t}\n\n#ifdef CONFIG_X86_64\n\t.data_nosave : AT(ADDR(.data_nosave) - LOAD_OFFSET) {\n\t\tNOSAVE_DATA\n\t}\n#endif\n\n\t/* BSS */\n\t. = ALIGN(PAGE_SIZE);\n\t.bss : AT(ADDR(.bss) - LOAD_OFFSET) {\n\t\t__bss_start = .;\n\t\t*(.bss..page_aligned)\n\t\t. = ALIGN(PAGE_SIZE);\n\t\t*(BSS_MAIN)\n\t\tBSS_DECRYPTED\n\t\t. = ALIGN(PAGE_SIZE);\n\t\t__bss_stop = .;\n\t}\n\n\t/*\n\t * The memory occupied from _text to here, __end_of_kernel_reserve, is\n\t * automatically reserved in setup_arch(). Anything after here must be\n\t * explicitly reserved using memblock_reserve() or it will be discarded\n\t * and treated as available memory.\n\t */\n\t__end_of_kernel_reserve = .;\n\n\t. = ALIGN(PAGE_SIZE);\n\t.brk : AT(ADDR(.brk) - LOAD_OFFSET) {\n\t\t__brk_base = .;\n\t\t. += 64 * 1024;\t\t/* 64k alignment slop space */\n\t\t*(.bss..brk)\t\t/* areas brk users have reserved */\n\t\t__brk_limit = .;\n\t}\n\n\t. = ALIGN(PAGE_SIZE);\t\t/* keep VO_INIT_SIZE page aligned */\n\t_end = .;\n\n#ifdef CONFIG_AMD_MEM_ENCRYPT\n\t/*\n\t * Early scratch/workarea section: Lives outside of the kernel proper\n\t * (_text - _end).\n\t *\n\t * Resides after _end because even though the .brk section is after\n\t * __end_of_kernel_reserve, the .brk section is later reserved as a\n\t * part of the kernel. Since it is located after __end_of_kernel_reserve\n\t * it will be discarded and become part of the available memory. As\n\t * such, it can only be used by very early boot code and must not be\n\t * needed afterwards.\n\t *\n\t * Currently used by SME for performing in-place encryption of the\n\t * kernel during boot. Resides on a 2MB boundary to simplify the\n\t * pagetable setup used for SME in-place encryption.\n\t */\n\t. = ALIGN(HPAGE_SIZE);\n\t.init.scratch : AT(ADDR(.init.scratch) - LOAD_OFFSET) {\n\t\t__init_scratch_begin = .;\n\t\t*(.init.scratch)\n\t\t. = ALIGN(HPAGE_SIZE);\n\t\t__init_scratch_end = .;\n\t}\n#endif\n\n\tSTABS_DEBUG\n\tDWARF_DEBUG\n\tELF_DETAILS\n\n\tDISCARDS\n\n\t/*\n\t * Make sure that the .got.plt is either completely empty or it\n\t * contains only the lazy dispatch entries.\n\t */\n\t.got.plt (INFO) : { *(.got.plt) }\n\tASSERT(SIZEOF(.got.plt) == 0 ||\n#ifdef CONFIG_X86_64\n\t       SIZEOF(.got.plt) == 0x18,\n#else\n\t       SIZEOF(.got.plt) == 0xc,\n#endif\n\t       \"Unexpected GOT/PLT entries detected!\")\n\n\t/*\n\t * Sections that should stay zero sized, which is safer to\n\t * explicitly check instead of blindly discarding.\n\t */\n\t.got : {\n\t\t*(.got) *(.igot.*)\n\t}\n\tASSERT(SIZEOF(.got) == 0, \"Unexpected GOT entries detected!\")\n\n\t.plt : {\n\t\t*(.plt) *(.plt.*) *(.iplt)\n\t}\n\tASSERT(SIZEOF(.plt) == 0, \"Unexpected run-time procedure linkages detected!\")\n\n\t.rel.dyn : {\n\t\t*(.rel.*) *(.rel_*)\n\t}\n\tASSERT(SIZEOF(.rel.dyn) == 0, \"Unexpected run-time relocations (.rel) detected!\")\n\n\t.rela.dyn : {\n\t\t*(.rela.*) *(.rela_*)\n\t}\n\tASSERT(SIZEOF(.rela.dyn) == 0, \"Unexpected run-time relocations (.rela) detected!\")\n}\n\n/*\n * The ASSERT() sink to . is intentional, for binutils 2.14 compatibility:\n */\n. = ASSERT((_end - LOAD_OFFSET <= KERNEL_IMAGE_SIZE),\n\t   \"kernel image bigger than KERNEL_IMAGE_SIZE\");\n\n#ifdef CONFIG_X86_64\n/*\n * Per-cpu symbols which need to be offset from __per_cpu_load\n * for the boot processor.\n */\n#define INIT_PER_CPU(x) init_per_cpu__##x = ABSOLUTE(x) + __per_cpu_load\nINIT_PER_CPU(gdt_page);\nINIT_PER_CPU(fixed_percpu_data);\nINIT_PER_CPU(irq_stack_backing_store);\n\n#ifdef CONFIG_SMP\n. = ASSERT((fixed_percpu_data == 0),\n           \"fixed_percpu_data is not at start of per-cpu area\");\n#endif\n\n#endif /* CONFIG_X86_64 */\n\n#ifdef CONFIG_KEXEC_CORE\n#include <asm/kexec.h>\n\n. = ASSERT(kexec_control_code_size <= KEXEC_CONTROL_CODE_MAX_SIZE,\n           \"kexec control code size is too big\");\n#endif\n\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * Core of Xen paravirt_ops implementation.\n *\n * This file contains the xen_paravirt_ops structure itself, and the\n * implementations for:\n * - privileged instructions\n * - interrupt flags\n * - segment operations\n * - booting and setup\n *\n * Jeremy Fitzhardinge <jeremy@xensource.com>, XenSource Inc, 2007\n */\n\n#include <linux/cpu.h>\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/smp.h>\n#include <linux/preempt.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/delay.h>\n#include <linux/start_kernel.h>\n#include <linux/sched.h>\n#include <linux/kprobes.h>\n#include <linux/memblock.h>\n#include <linux/export.h>\n#include <linux/mm.h>\n#include <linux/page-flags.h>\n#include <linux/pci.h>\n#include <linux/gfp.h>\n#include <linux/edd.h>\n#include <linux/reboot.h>\n\n#include <xen/xen.h>\n#include <xen/events.h>\n#include <xen/interface/xen.h>\n#include <xen/interface/version.h>\n#include <xen/interface/physdev.h>\n#include <xen/interface/vcpu.h>\n#include <xen/interface/memory.h>\n#include <xen/interface/nmi.h>\n#include <xen/interface/xen-mca.h>\n#include <xen/features.h>\n#include <xen/page.h>\n#include <xen/hvc-console.h>\n#include <xen/acpi.h>\n\n#include <asm/paravirt.h>\n#include <asm/apic.h>\n#include <asm/page.h>\n#include <asm/xen/pci.h>\n#include <asm/xen/hypercall.h>\n#include <asm/xen/hypervisor.h>\n#include <asm/xen/cpuid.h>\n#include <asm/fixmap.h>\n#include <asm/processor.h>\n#include <asm/proto.h>\n#include <asm/msr-index.h>\n#include <asm/traps.h>\n#include <asm/setup.h>\n#include <asm/desc.h>\n#include <asm/pgalloc.h>\n#include <asm/tlbflush.h>\n#include <asm/reboot.h>\n#include <asm/stackprotector.h>\n#include <asm/hypervisor.h>\n#include <asm/mach_traps.h>\n#include <asm/mwait.h>\n#include <asm/pci_x86.h>\n#include <asm/cpu.h>\n#ifdef CONFIG_X86_IOPL_IOPERM\n#include <asm/io_bitmap.h>\n#endif\n\n#ifdef CONFIG_ACPI\n#include <linux/acpi.h>\n#include <asm/acpi.h>\n#include <acpi/pdc_intel.h>\n#include <acpi/processor.h>\n#include <xen/interface/platform.h>\n#endif\n\n#include \"xen-ops.h\"\n#include \"mmu.h\"\n#include \"smp.h\"\n#include \"multicalls.h\"\n#include \"pmu.h\"\n\n#include \"../kernel/cpu/cpu.h\" /* get_cpu_cap() */\n\nvoid *xen_initial_gdt;\n\nstatic int xen_cpu_up_prepare_pv(unsigned int cpu);\nstatic int xen_cpu_dead_pv(unsigned int cpu);\n\nstruct tls_descs {\n\tstruct desc_struct desc[3];\n};\n\n/*\n * Updating the 3 TLS descriptors in the GDT on every task switch is\n * surprisingly expensive so we avoid updating them if they haven't\n * changed.  Since Xen writes different descriptors than the one\n * passed in the update_descriptor hypercall we keep shadow copies to\n * compare against.\n */\nstatic DEFINE_PER_CPU(struct tls_descs, shadow_tls_desc);\n\nstatic void __init xen_pv_init_platform(void)\n{\n\txen_set_restricted_virtio_memory_access();\n\n\tpopulate_extra_pte(fix_to_virt(FIX_PARAVIRT_BOOTMAP));\n\n\tset_fixmap(FIX_PARAVIRT_BOOTMAP, xen_start_info->shared_info);\n\tHYPERVISOR_shared_info = (void *)fix_to_virt(FIX_PARAVIRT_BOOTMAP);\n\n\t/* xen clock uses per-cpu vcpu_info, need to init it for boot cpu */\n\txen_vcpu_info_reset(0);\n\n\t/* pvclock is in shared info area */\n\txen_init_time_ops();\n}\n\nstatic void __init xen_pv_guest_late_init(void)\n{\n#ifndef CONFIG_SMP\n\t/* Setup shared vcpu info for non-smp configurations */\n\txen_setup_vcpu_info_placement();\n#endif\n}\n\nstatic __read_mostly unsigned int cpuid_leaf5_ecx_val;\nstatic __read_mostly unsigned int cpuid_leaf5_edx_val;\n\nstatic void xen_cpuid(unsigned int *ax, unsigned int *bx,\n\t\t      unsigned int *cx, unsigned int *dx)\n{\n\tunsigned maskebx = ~0;\n\n\t/*\n\t * Mask out inconvenient features, to try and disable as many\n\t * unsupported kernel subsystems as possible.\n\t */\n\tswitch (*ax) {\n\tcase CPUID_MWAIT_LEAF:\n\t\t/* Synthesize the values.. */\n\t\t*ax = 0;\n\t\t*bx = 0;\n\t\t*cx = cpuid_leaf5_ecx_val;\n\t\t*dx = cpuid_leaf5_edx_val;\n\t\treturn;\n\n\tcase 0xb:\n\t\t/* Suppress extended topology stuff */\n\t\tmaskebx = 0;\n\t\tbreak;\n\t}\n\n\tasm(XEN_EMULATE_PREFIX \"cpuid\"\n\t\t: \"=a\" (*ax),\n\t\t  \"=b\" (*bx),\n\t\t  \"=c\" (*cx),\n\t\t  \"=d\" (*dx)\n\t\t: \"0\" (*ax), \"2\" (*cx));\n\n\t*bx &= maskebx;\n}\n\nstatic bool __init xen_check_mwait(void)\n{\n#ifdef CONFIG_ACPI\n\tstruct xen_platform_op op = {\n\t\t.cmd\t\t\t= XENPF_set_processor_pminfo,\n\t\t.u.set_pminfo.id\t= -1,\n\t\t.u.set_pminfo.type\t= XEN_PM_PDC,\n\t};\n\tuint32_t buf[3];\n\tunsigned int ax, bx, cx, dx;\n\tunsigned int mwait_mask;\n\n\t/* We need to determine whether it is OK to expose the MWAIT\n\t * capability to the kernel to harvest deeper than C3 states from ACPI\n\t * _CST using the processor_harvest_xen.c module. For this to work, we\n\t * need to gather the MWAIT_LEAF values (which the cstate.c code\n\t * checks against). The hypervisor won't expose the MWAIT flag because\n\t * it would break backwards compatibility; so we will find out directly\n\t * from the hardware and hypercall.\n\t */\n\tif (!xen_initial_domain())\n\t\treturn false;\n\n\t/*\n\t * When running under platform earlier than Xen4.2, do not expose\n\t * mwait, to avoid the risk of loading native acpi pad driver\n\t */\n\tif (!xen_running_on_version_or_later(4, 2))\n\t\treturn false;\n\n\tax = 1;\n\tcx = 0;\n\n\tnative_cpuid(&ax, &bx, &cx, &dx);\n\n\tmwait_mask = (1 << (X86_FEATURE_EST % 32)) |\n\t\t     (1 << (X86_FEATURE_MWAIT % 32));\n\n\tif ((cx & mwait_mask) != mwait_mask)\n\t\treturn false;\n\n\t/* We need to emulate the MWAIT_LEAF and for that we need both\n\t * ecx and edx. The hypercall provides only partial information.\n\t */\n\n\tax = CPUID_MWAIT_LEAF;\n\tbx = 0;\n\tcx = 0;\n\tdx = 0;\n\n\tnative_cpuid(&ax, &bx, &cx, &dx);\n\n\t/* Ask the Hypervisor whether to clear ACPI_PDC_C_C2C3_FFH. If so,\n\t * don't expose MWAIT_LEAF and let ACPI pick the IOPORT version of C3.\n\t */\n\tbuf[0] = ACPI_PDC_REVISION_ID;\n\tbuf[1] = 1;\n\tbuf[2] = (ACPI_PDC_C_CAPABILITY_SMP | ACPI_PDC_EST_CAPABILITY_SWSMP);\n\n\tset_xen_guest_handle(op.u.set_pminfo.pdc, buf);\n\n\tif ((HYPERVISOR_platform_op(&op) == 0) &&\n\t    (buf[2] & (ACPI_PDC_C_C1_FFH | ACPI_PDC_C_C2C3_FFH))) {\n\t\tcpuid_leaf5_ecx_val = cx;\n\t\tcpuid_leaf5_edx_val = dx;\n\t}\n\treturn true;\n#else\n\treturn false;\n#endif\n}\n\nstatic bool __init xen_check_xsave(void)\n{\n\tunsigned int cx, xsave_mask;\n\n\tcx = cpuid_ecx(1);\n\n\txsave_mask = (1 << (X86_FEATURE_XSAVE % 32)) |\n\t\t     (1 << (X86_FEATURE_OSXSAVE % 32));\n\n\t/* Xen will set CR4.OSXSAVE if supported and not disabled by force */\n\treturn (cx & xsave_mask) == xsave_mask;\n}\n\nstatic void __init xen_init_capabilities(void)\n{\n\tsetup_force_cpu_cap(X86_FEATURE_XENPV);\n\tsetup_clear_cpu_cap(X86_FEATURE_DCA);\n\tsetup_clear_cpu_cap(X86_FEATURE_APERFMPERF);\n\tsetup_clear_cpu_cap(X86_FEATURE_MTRR);\n\tsetup_clear_cpu_cap(X86_FEATURE_ACC);\n\tsetup_clear_cpu_cap(X86_FEATURE_X2APIC);\n\tsetup_clear_cpu_cap(X86_FEATURE_SME);\n\n\t/*\n\t * Xen PV would need some work to support PCID: CR3 handling as well\n\t * as xen_flush_tlb_others() would need updating.\n\t */\n\tsetup_clear_cpu_cap(X86_FEATURE_PCID);\n\n\tif (!xen_initial_domain())\n\t\tsetup_clear_cpu_cap(X86_FEATURE_ACPI);\n\n\tif (xen_check_mwait())\n\t\tsetup_force_cpu_cap(X86_FEATURE_MWAIT);\n\telse\n\t\tsetup_clear_cpu_cap(X86_FEATURE_MWAIT);\n\n\tif (!xen_check_xsave()) {\n\t\tsetup_clear_cpu_cap(X86_FEATURE_XSAVE);\n\t\tsetup_clear_cpu_cap(X86_FEATURE_OSXSAVE);\n\t}\n}\n\nstatic noinstr void xen_set_debugreg(int reg, unsigned long val)\n{\n\tHYPERVISOR_set_debugreg(reg, val);\n}\n\nstatic noinstr unsigned long xen_get_debugreg(int reg)\n{\n\treturn HYPERVISOR_get_debugreg(reg);\n}\n\nstatic void xen_end_context_switch(struct task_struct *next)\n{\n\txen_mc_flush();\n\tparavirt_end_context_switch(next);\n}\n\nstatic unsigned long xen_store_tr(void)\n{\n\treturn 0;\n}\n\n/*\n * Set the page permissions for a particular virtual address.  If the\n * address is a vmalloc mapping (or other non-linear mapping), then\n * find the linear mapping of the page and also set its protections to\n * match.\n */\nstatic void set_aliased_prot(void *v, pgprot_t prot)\n{\n\tint level;\n\tpte_t *ptep;\n\tpte_t pte;\n\tunsigned long pfn;\n\tunsigned char dummy;\n\tvoid *va;\n\n\tptep = lookup_address((unsigned long)v, &level);\n\tBUG_ON(ptep == NULL);\n\n\tpfn = pte_pfn(*ptep);\n\tpte = pfn_pte(pfn, prot);\n\n\t/*\n\t * Careful: update_va_mapping() will fail if the virtual address\n\t * we're poking isn't populated in the page tables.  We don't\n\t * need to worry about the direct map (that's always in the page\n\t * tables), but we need to be careful about vmap space.  In\n\t * particular, the top level page table can lazily propagate\n\t * entries between processes, so if we've switched mms since we\n\t * vmapped the target in the first place, we might not have the\n\t * top-level page table entry populated.\n\t *\n\t * We disable preemption because we want the same mm active when\n\t * we probe the target and when we issue the hypercall.  We'll\n\t * have the same nominal mm, but if we're a kernel thread, lazy\n\t * mm dropping could change our pgd.\n\t *\n\t * Out of an abundance of caution, this uses __get_user() to fault\n\t * in the target address just in case there's some obscure case\n\t * in which the target address isn't readable.\n\t */\n\n\tpreempt_disable();\n\n\tcopy_from_kernel_nofault(&dummy, v, 1);\n\n\tif (HYPERVISOR_update_va_mapping((unsigned long)v, pte, 0))\n\t\tBUG();\n\n\tva = __va(PFN_PHYS(pfn));\n\n\tif (va != v && HYPERVISOR_update_va_mapping((unsigned long)va, pte, 0))\n\t\tBUG();\n\n\tpreempt_enable();\n}\n\nstatic void xen_alloc_ldt(struct desc_struct *ldt, unsigned entries)\n{\n\tconst unsigned entries_per_page = PAGE_SIZE / LDT_ENTRY_SIZE;\n\tint i;\n\n\t/*\n\t * We need to mark the all aliases of the LDT pages RO.  We\n\t * don't need to call vm_flush_aliases(), though, since that's\n\t * only responsible for flushing aliases out the TLBs, not the\n\t * page tables, and Xen will flush the TLB for us if needed.\n\t *\n\t * To avoid confusing future readers: none of this is necessary\n\t * to load the LDT.  The hypervisor only checks this when the\n\t * LDT is faulted in due to subsequent descriptor access.\n\t */\n\n\tfor (i = 0; i < entries; i += entries_per_page)\n\t\tset_aliased_prot(ldt + i, PAGE_KERNEL_RO);\n}\n\nstatic void xen_free_ldt(struct desc_struct *ldt, unsigned entries)\n{\n\tconst unsigned entries_per_page = PAGE_SIZE / LDT_ENTRY_SIZE;\n\tint i;\n\n\tfor (i = 0; i < entries; i += entries_per_page)\n\t\tset_aliased_prot(ldt + i, PAGE_KERNEL);\n}\n\nstatic void xen_set_ldt(const void *addr, unsigned entries)\n{\n\tstruct mmuext_op *op;\n\tstruct multicall_space mcs = xen_mc_entry(sizeof(*op));\n\n\ttrace_xen_cpu_set_ldt(addr, entries);\n\n\top = mcs.args;\n\top->cmd = MMUEXT_SET_LDT;\n\top->arg1.linear_addr = (unsigned long)addr;\n\top->arg2.nr_ents = entries;\n\n\tMULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);\n\n\txen_mc_issue(PARAVIRT_LAZY_CPU);\n}\n\nstatic void xen_load_gdt(const struct desc_ptr *dtr)\n{\n\tunsigned long va = dtr->address;\n\tunsigned int size = dtr->size + 1;\n\tunsigned long pfn, mfn;\n\tint level;\n\tpte_t *ptep;\n\tvoid *virt;\n\n\t/* @size should be at most GDT_SIZE which is smaller than PAGE_SIZE. */\n\tBUG_ON(size > PAGE_SIZE);\n\tBUG_ON(va & ~PAGE_MASK);\n\n\t/*\n\t * The GDT is per-cpu and is in the percpu data area.\n\t * That can be virtually mapped, so we need to do a\n\t * page-walk to get the underlying MFN for the\n\t * hypercall.  The page can also be in the kernel's\n\t * linear range, so we need to RO that mapping too.\n\t */\n\tptep = lookup_address(va, &level);\n\tBUG_ON(ptep == NULL);\n\n\tpfn = pte_pfn(*ptep);\n\tmfn = pfn_to_mfn(pfn);\n\tvirt = __va(PFN_PHYS(pfn));\n\n\tmake_lowmem_page_readonly((void *)va);\n\tmake_lowmem_page_readonly(virt);\n\n\tif (HYPERVISOR_set_gdt(&mfn, size / sizeof(struct desc_struct)))\n\t\tBUG();\n}\n\n/*\n * load_gdt for early boot, when the gdt is only mapped once\n */\nstatic void __init xen_load_gdt_boot(const struct desc_ptr *dtr)\n{\n\tunsigned long va = dtr->address;\n\tunsigned int size = dtr->size + 1;\n\tunsigned long pfn, mfn;\n\tpte_t pte;\n\n\t/* @size should be at most GDT_SIZE which is smaller than PAGE_SIZE. */\n\tBUG_ON(size > PAGE_SIZE);\n\tBUG_ON(va & ~PAGE_MASK);\n\n\tpfn = virt_to_pfn(va);\n\tmfn = pfn_to_mfn(pfn);\n\n\tpte = pfn_pte(pfn, PAGE_KERNEL_RO);\n\n\tif (HYPERVISOR_update_va_mapping((unsigned long)va, pte, 0))\n\t\tBUG();\n\n\tif (HYPERVISOR_set_gdt(&mfn, size / sizeof(struct desc_struct)))\n\t\tBUG();\n}\n\nstatic inline bool desc_equal(const struct desc_struct *d1,\n\t\t\t      const struct desc_struct *d2)\n{\n\treturn !memcmp(d1, d2, sizeof(*d1));\n}\n\nstatic void load_TLS_descriptor(struct thread_struct *t,\n\t\t\t\tunsigned int cpu, unsigned int i)\n{\n\tstruct desc_struct *shadow = &per_cpu(shadow_tls_desc, cpu).desc[i];\n\tstruct desc_struct *gdt;\n\txmaddr_t maddr;\n\tstruct multicall_space mc;\n\n\tif (desc_equal(shadow, &t->tls_array[i]))\n\t\treturn;\n\n\t*shadow = t->tls_array[i];\n\n\tgdt = get_cpu_gdt_rw(cpu);\n\tmaddr = arbitrary_virt_to_machine(&gdt[GDT_ENTRY_TLS_MIN+i]);\n\tmc = __xen_mc_entry(0);\n\n\tMULTI_update_descriptor(mc.mc, maddr.maddr, t->tls_array[i]);\n}\n\nstatic void xen_load_tls(struct thread_struct *t, unsigned int cpu)\n{\n\t/*\n\t * In lazy mode we need to zero %fs, otherwise we may get an\n\t * exception between the new %fs descriptor being loaded and\n\t * %fs being effectively cleared at __switch_to().\n\t */\n\tif (paravirt_get_lazy_mode() == PARAVIRT_LAZY_CPU)\n\t\tloadsegment(fs, 0);\n\n\txen_mc_batch();\n\n\tload_TLS_descriptor(t, cpu, 0);\n\tload_TLS_descriptor(t, cpu, 1);\n\tload_TLS_descriptor(t, cpu, 2);\n\n\txen_mc_issue(PARAVIRT_LAZY_CPU);\n}\n\nstatic void xen_load_gs_index(unsigned int idx)\n{\n\tif (HYPERVISOR_set_segment_base(SEGBASE_GS_USER_SEL, idx))\n\t\tBUG();\n}\n\nstatic void xen_write_ldt_entry(struct desc_struct *dt, int entrynum,\n\t\t\t\tconst void *ptr)\n{\n\txmaddr_t mach_lp = arbitrary_virt_to_machine(&dt[entrynum]);\n\tu64 entry = *(u64 *)ptr;\n\n\ttrace_xen_cpu_write_ldt_entry(dt, entrynum, entry);\n\n\tpreempt_disable();\n\n\txen_mc_flush();\n\tif (HYPERVISOR_update_descriptor(mach_lp.maddr, entry))\n\t\tBUG();\n\n\tpreempt_enable();\n}\n\nvoid noist_exc_debug(struct pt_regs *regs);\n\nDEFINE_IDTENTRY_RAW(xenpv_exc_nmi)\n{\n\t/* On Xen PV, NMI doesn't use IST.  The C part is the same as native. */\n\texc_nmi(regs);\n}\n\nDEFINE_IDTENTRY_RAW_ERRORCODE(xenpv_exc_double_fault)\n{\n\t/* On Xen PV, DF doesn't use IST.  The C part is the same as native. */\n\texc_double_fault(regs, error_code);\n}\n\nDEFINE_IDTENTRY_RAW(xenpv_exc_debug)\n{\n\t/*\n\t * There's no IST on Xen PV, but we still need to dispatch\n\t * to the correct handler.\n\t */\n\tif (user_mode(regs))\n\t\tnoist_exc_debug(regs);\n\telse\n\t\texc_debug(regs);\n}\n\nDEFINE_IDTENTRY_RAW(exc_xen_unknown_trap)\n{\n\t/* This should never happen and there is no way to handle it. */\n\tinstrumentation_begin();\n\tpr_err(\"Unknown trap in Xen PV mode.\");\n\tBUG();\n\tinstrumentation_end();\n}\n\n#ifdef CONFIG_X86_MCE\nDEFINE_IDTENTRY_RAW(xenpv_exc_machine_check)\n{\n\t/*\n\t * There's no IST on Xen PV, but we still need to dispatch\n\t * to the correct handler.\n\t */\n\tif (user_mode(regs))\n\t\tnoist_exc_machine_check(regs);\n\telse\n\t\texc_machine_check(regs);\n}\n#endif\n\nstruct trap_array_entry {\n\tvoid (*orig)(void);\n\tvoid (*xen)(void);\n\tbool ist_okay;\n};\n\n#define TRAP_ENTRY(func, ist_ok) {\t\t\t\\\n\t.orig\t\t= asm_##func,\t\t\t\\\n\t.xen\t\t= xen_asm_##func,\t\t\\\n\t.ist_okay\t= ist_ok }\n\n#define TRAP_ENTRY_REDIR(func, ist_ok) {\t\t\\\n\t.orig\t\t= asm_##func,\t\t\t\\\n\t.xen\t\t= xen_asm_xenpv_##func,\t\t\\\n\t.ist_okay\t= ist_ok }\n\nstatic struct trap_array_entry trap_array[] = {\n\tTRAP_ENTRY_REDIR(exc_debug,\t\t\ttrue  ),\n\tTRAP_ENTRY_REDIR(exc_double_fault,\t\ttrue  ),\n#ifdef CONFIG_X86_MCE\n\tTRAP_ENTRY_REDIR(exc_machine_check,\t\ttrue  ),\n#endif\n\tTRAP_ENTRY_REDIR(exc_nmi,\t\t\ttrue  ),\n\tTRAP_ENTRY(exc_int3,\t\t\t\tfalse ),\n\tTRAP_ENTRY(exc_overflow,\t\t\tfalse ),\n#ifdef CONFIG_IA32_EMULATION\n\t{ entry_INT80_compat,          xen_entry_INT80_compat,          false },\n#endif\n\tTRAP_ENTRY(exc_page_fault,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_divide_error,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_bounds,\t\t\t\tfalse ),\n\tTRAP_ENTRY(exc_invalid_op,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_device_not_available,\t\tfalse ),\n\tTRAP_ENTRY(exc_coproc_segment_overrun,\t\tfalse ),\n\tTRAP_ENTRY(exc_invalid_tss,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_segment_not_present,\t\tfalse ),\n\tTRAP_ENTRY(exc_stack_segment,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_general_protection,\t\tfalse ),\n\tTRAP_ENTRY(exc_spurious_interrupt_bug,\t\tfalse ),\n\tTRAP_ENTRY(exc_coprocessor_error,\t\tfalse ),\n\tTRAP_ENTRY(exc_alignment_check,\t\t\tfalse ),\n\tTRAP_ENTRY(exc_simd_coprocessor_error,\t\tfalse ),\n#ifdef CONFIG_X86_KERNEL_IBT\n\tTRAP_ENTRY(exc_control_protection,\t\tfalse ),\n#endif\n};\n\nstatic bool __ref get_trap_addr(void **addr, unsigned int ist)\n{\n\tunsigned int nr;\n\tbool ist_okay = false;\n\tbool found = false;\n\n\t/*\n\t * Replace trap handler addresses by Xen specific ones.\n\t * Check for known traps using IST and whitelist them.\n\t * The debugger ones are the only ones we care about.\n\t * Xen will handle faults like double_fault, so we should never see\n\t * them.  Warn if there's an unexpected IST-using fault handler.\n\t */\n\tfor (nr = 0; nr < ARRAY_SIZE(trap_array); nr++) {\n\t\tstruct trap_array_entry *entry = trap_array + nr;\n\n\t\tif (*addr == entry->orig) {\n\t\t\t*addr = entry->xen;\n\t\t\tist_okay = entry->ist_okay;\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (nr == ARRAY_SIZE(trap_array) &&\n\t    *addr >= (void *)early_idt_handler_array[0] &&\n\t    *addr < (void *)early_idt_handler_array[NUM_EXCEPTION_VECTORS]) {\n\t\tnr = (*addr - (void *)early_idt_handler_array[0]) /\n\t\t     EARLY_IDT_HANDLER_SIZE;\n\t\t*addr = (void *)xen_early_idt_handler_array[nr];\n\t\tfound = true;\n\t}\n\n\tif (!found)\n\t\t*addr = (void *)xen_asm_exc_xen_unknown_trap;\n\n\tif (WARN_ON(found && ist != 0 && !ist_okay))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int cvt_gate_to_trap(int vector, const gate_desc *val,\n\t\t\t    struct trap_info *info)\n{\n\tunsigned long addr;\n\n\tif (val->bits.type != GATE_TRAP && val->bits.type != GATE_INTERRUPT)\n\t\treturn 0;\n\n\tinfo->vector = vector;\n\n\taddr = gate_offset(val);\n\tif (!get_trap_addr((void **)&addr, val->bits.ist))\n\t\treturn 0;\n\tinfo->address = addr;\n\n\tinfo->cs = gate_segment(val);\n\tinfo->flags = val->bits.dpl;\n\t/* interrupt gates clear IF */\n\tif (val->bits.type == GATE_INTERRUPT)\n\t\tinfo->flags |= 1 << 2;\n\n\treturn 1;\n}\n\n/* Locations of each CPU's IDT */\nstatic DEFINE_PER_CPU(struct desc_ptr, idt_desc);\n\n/* Set an IDT entry.  If the entry is part of the current IDT, then\n   also update Xen. */\nstatic void xen_write_idt_entry(gate_desc *dt, int entrynum, const gate_desc *g)\n{\n\tunsigned long p = (unsigned long)&dt[entrynum];\n\tunsigned long start, end;\n\n\ttrace_xen_cpu_write_idt_entry(dt, entrynum, g);\n\n\tpreempt_disable();\n\n\tstart = __this_cpu_read(idt_desc.address);\n\tend = start + __this_cpu_read(idt_desc.size) + 1;\n\n\txen_mc_flush();\n\n\tnative_write_idt_entry(dt, entrynum, g);\n\n\tif (p >= start && (p + 8) <= end) {\n\t\tstruct trap_info info[2];\n\n\t\tinfo[1].address = 0;\n\n\t\tif (cvt_gate_to_trap(entrynum, g, &info[0]))\n\t\t\tif (HYPERVISOR_set_trap_table(info))\n\t\t\t\tBUG();\n\t}\n\n\tpreempt_enable();\n}\n\nstatic unsigned xen_convert_trap_info(const struct desc_ptr *desc,\n\t\t\t\t      struct trap_info *traps, bool full)\n{\n\tunsigned in, out, count;\n\n\tcount = (desc->size+1) / sizeof(gate_desc);\n\tBUG_ON(count > 256);\n\n\tfor (in = out = 0; in < count; in++) {\n\t\tgate_desc *entry = (gate_desc *)(desc->address) + in;\n\n\t\tif (cvt_gate_to_trap(in, entry, &traps[out]) || full)\n\t\t\tout++;\n\t}\n\n\treturn out;\n}\n\nvoid xen_copy_trap_info(struct trap_info *traps)\n{\n\tconst struct desc_ptr *desc = this_cpu_ptr(&idt_desc);\n\n\txen_convert_trap_info(desc, traps, true);\n}\n\n/* Load a new IDT into Xen.  In principle this can be per-CPU, so we\n   hold a spinlock to protect the static traps[] array (static because\n   it avoids allocation, and saves stack space). */\nstatic void xen_load_idt(const struct desc_ptr *desc)\n{\n\tstatic DEFINE_SPINLOCK(lock);\n\tstatic struct trap_info traps[257];\n\tunsigned out;\n\n\ttrace_xen_cpu_load_idt(desc);\n\n\tspin_lock(&lock);\n\n\tmemcpy(this_cpu_ptr(&idt_desc), desc, sizeof(idt_desc));\n\n\tout = xen_convert_trap_info(desc, traps, false);\n\tmemset(&traps[out], 0, sizeof(traps[0]));\n\n\txen_mc_flush();\n\tif (HYPERVISOR_set_trap_table(traps))\n\t\tBUG();\n\n\tspin_unlock(&lock);\n}\n\n/* Write a GDT descriptor entry.  Ignore LDT descriptors, since\n   they're handled differently. */\nstatic void xen_write_gdt_entry(struct desc_struct *dt, int entry,\n\t\t\t\tconst void *desc, int type)\n{\n\ttrace_xen_cpu_write_gdt_entry(dt, entry, desc, type);\n\n\tpreempt_disable();\n\n\tswitch (type) {\n\tcase DESC_LDT:\n\tcase DESC_TSS:\n\t\t/* ignore */\n\t\tbreak;\n\n\tdefault: {\n\t\txmaddr_t maddr = arbitrary_virt_to_machine(&dt[entry]);\n\n\t\txen_mc_flush();\n\t\tif (HYPERVISOR_update_descriptor(maddr.maddr, *(u64 *)desc))\n\t\t\tBUG();\n\t}\n\n\t}\n\n\tpreempt_enable();\n}\n\n/*\n * Version of write_gdt_entry for use at early boot-time needed to\n * update an entry as simply as possible.\n */\nstatic void __init xen_write_gdt_entry_boot(struct desc_struct *dt, int entry,\n\t\t\t\t\t    const void *desc, int type)\n{\n\ttrace_xen_cpu_write_gdt_entry(dt, entry, desc, type);\n\n\tswitch (type) {\n\tcase DESC_LDT:\n\tcase DESC_TSS:\n\t\t/* ignore */\n\t\tbreak;\n\n\tdefault: {\n\t\txmaddr_t maddr = virt_to_machine(&dt[entry]);\n\n\t\tif (HYPERVISOR_update_descriptor(maddr.maddr, *(u64 *)desc))\n\t\t\tdt[entry] = *(struct desc_struct *)desc;\n\t}\n\n\t}\n}\n\nstatic void xen_load_sp0(unsigned long sp0)\n{\n\tstruct multicall_space mcs;\n\n\tmcs = xen_mc_entry(0);\n\tMULTI_stack_switch(mcs.mc, __KERNEL_DS, sp0);\n\txen_mc_issue(PARAVIRT_LAZY_CPU);\n\tthis_cpu_write(cpu_tss_rw.x86_tss.sp0, sp0);\n}\n\n#ifdef CONFIG_X86_IOPL_IOPERM\nstatic void xen_invalidate_io_bitmap(void)\n{\n\tstruct physdev_set_iobitmap iobitmap = {\n\t\t.bitmap = NULL,\n\t\t.nr_ports = 0,\n\t};\n\n\tnative_tss_invalidate_io_bitmap();\n\tHYPERVISOR_physdev_op(PHYSDEVOP_set_iobitmap, &iobitmap);\n}\n\nstatic void xen_update_io_bitmap(void)\n{\n\tstruct physdev_set_iobitmap iobitmap;\n\tstruct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);\n\n\tnative_tss_update_io_bitmap();\n\n\tiobitmap.bitmap = (uint8_t *)(&tss->x86_tss) +\n\t\t\t  tss->x86_tss.io_bitmap_base;\n\tif (tss->x86_tss.io_bitmap_base == IO_BITMAP_OFFSET_INVALID)\n\t\tiobitmap.nr_ports = 0;\n\telse\n\t\tiobitmap.nr_ports = IO_BITMAP_BITS;\n\n\tHYPERVISOR_physdev_op(PHYSDEVOP_set_iobitmap, &iobitmap);\n}\n#endif\n\nstatic void xen_io_delay(void)\n{\n}\n\nstatic DEFINE_PER_CPU(unsigned long, xen_cr0_value);\n\nstatic unsigned long xen_read_cr0(void)\n{\n\tunsigned long cr0 = this_cpu_read(xen_cr0_value);\n\n\tif (unlikely(cr0 == 0)) {\n\t\tcr0 = native_read_cr0();\n\t\tthis_cpu_write(xen_cr0_value, cr0);\n\t}\n\n\treturn cr0;\n}\n\nstatic void xen_write_cr0(unsigned long cr0)\n{\n\tstruct multicall_space mcs;\n\n\tthis_cpu_write(xen_cr0_value, cr0);\n\n\t/* Only pay attention to cr0.TS; everything else is\n\t   ignored. */\n\tmcs = xen_mc_entry(0);\n\n\tMULTI_fpu_taskswitch(mcs.mc, (cr0 & X86_CR0_TS) != 0);\n\n\txen_mc_issue(PARAVIRT_LAZY_CPU);\n}\n\nstatic void xen_write_cr4(unsigned long cr4)\n{\n\tcr4 &= ~(X86_CR4_PGE | X86_CR4_PSE | X86_CR4_PCE);\n\n\tnative_write_cr4(cr4);\n}\n\nstatic u64 xen_read_msr_safe(unsigned int msr, int *err)\n{\n\tu64 val;\n\n\tif (pmu_msr_read(msr, &val, err))\n\t\treturn val;\n\n\tval = native_read_msr_safe(msr, err);\n\tswitch (msr) {\n\tcase MSR_IA32_APICBASE:\n\t\tval &= ~X2APIC_ENABLE;\n\t\tbreak;\n\t}\n\treturn val;\n}\n\nstatic int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)\n{\n\tint ret;\n\tunsigned int which;\n\tu64 base;\n\n\tret = 0;\n\n\tswitch (msr) {\n\tcase MSR_FS_BASE:\t\twhich = SEGBASE_FS; goto set;\n\tcase MSR_KERNEL_GS_BASE:\twhich = SEGBASE_GS_USER; goto set;\n\tcase MSR_GS_BASE:\t\twhich = SEGBASE_GS_KERNEL; goto set;\n\n\tset:\n\t\tbase = ((u64)high << 32) | low;\n\t\tif (HYPERVISOR_set_segment_base(which, base) != 0)\n\t\t\tret = -EIO;\n\t\tbreak;\n\n\tcase MSR_STAR:\n\tcase MSR_CSTAR:\n\tcase MSR_LSTAR:\n\tcase MSR_SYSCALL_MASK:\n\tcase MSR_IA32_SYSENTER_CS:\n\tcase MSR_IA32_SYSENTER_ESP:\n\tcase MSR_IA32_SYSENTER_EIP:\n\t\t/* Fast syscall setup is all done in hypercalls, so\n\t\t   these are all ignored.  Stub them out here to stop\n\t\t   Xen console noise. */\n\t\tbreak;\n\n\tdefault:\n\t\tif (!pmu_msr_write(msr, low, high, &ret))\n\t\t\tret = native_write_msr_safe(msr, low, high);\n\t}\n\n\treturn ret;\n}\n\nstatic u64 xen_read_msr(unsigned int msr)\n{\n\t/*\n\t * This will silently swallow a #GP from RDMSR.  It may be worth\n\t * changing that.\n\t */\n\tint err;\n\n\treturn xen_read_msr_safe(msr, &err);\n}\n\nstatic void xen_write_msr(unsigned int msr, unsigned low, unsigned high)\n{\n\t/*\n\t * This will silently swallow a #GP from WRMSR.  It may be worth\n\t * changing that.\n\t */\n\txen_write_msr_safe(msr, low, high);\n}\n\n/* This is called once we have the cpu_possible_mask */\nvoid __init xen_setup_vcpu_info_placement(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\t/* Set up direct vCPU id mapping for PV guests. */\n\t\tper_cpu(xen_vcpu_id, cpu) = cpu;\n\t\txen_vcpu_setup(cpu);\n\t}\n\n\tpv_ops.irq.save_fl = __PV_IS_CALLEE_SAVE(xen_save_fl_direct);\n\tpv_ops.irq.irq_disable = __PV_IS_CALLEE_SAVE(xen_irq_disable_direct);\n\tpv_ops.irq.irq_enable = __PV_IS_CALLEE_SAVE(xen_irq_enable_direct);\n\tpv_ops.mmu.read_cr2 = __PV_IS_CALLEE_SAVE(xen_read_cr2_direct);\n}\n\nstatic const struct pv_info xen_info __initconst = {\n\t.extra_user_64bit_cs = FLAT_USER_CS64,\n\t.name = \"Xen\",\n};\n\nstatic const typeof(pv_ops) xen_cpu_ops __initconst = {\n\t.cpu = {\n\t\t.cpuid = xen_cpuid,\n\n\t\t.set_debugreg = xen_set_debugreg,\n\t\t.get_debugreg = xen_get_debugreg,\n\n\t\t.read_cr0 = xen_read_cr0,\n\t\t.write_cr0 = xen_write_cr0,\n\n\t\t.write_cr4 = xen_write_cr4,\n\n\t\t.wbinvd = native_wbinvd,\n\n\t\t.read_msr = xen_read_msr,\n\t\t.write_msr = xen_write_msr,\n\n\t\t.read_msr_safe = xen_read_msr_safe,\n\t\t.write_msr_safe = xen_write_msr_safe,\n\n\t\t.read_pmc = xen_read_pmc,\n\n\t\t.load_tr_desc = paravirt_nop,\n\t\t.set_ldt = xen_set_ldt,\n\t\t.load_gdt = xen_load_gdt,\n\t\t.load_idt = xen_load_idt,\n\t\t.load_tls = xen_load_tls,\n\t\t.load_gs_index = xen_load_gs_index,\n\n\t\t.alloc_ldt = xen_alloc_ldt,\n\t\t.free_ldt = xen_free_ldt,\n\n\t\t.store_tr = xen_store_tr,\n\n\t\t.write_ldt_entry = xen_write_ldt_entry,\n\t\t.write_gdt_entry = xen_write_gdt_entry,\n\t\t.write_idt_entry = xen_write_idt_entry,\n\t\t.load_sp0 = xen_load_sp0,\n\n#ifdef CONFIG_X86_IOPL_IOPERM\n\t\t.invalidate_io_bitmap = xen_invalidate_io_bitmap,\n\t\t.update_io_bitmap = xen_update_io_bitmap,\n#endif\n\t\t.io_delay = xen_io_delay,\n\n\t\t.start_context_switch = paravirt_start_context_switch,\n\t\t.end_context_switch = xen_end_context_switch,\n\t},\n};\n\nstatic void xen_restart(char *msg)\n{\n\txen_reboot(SHUTDOWN_reboot);\n}\n\nstatic void xen_machine_halt(void)\n{\n\txen_reboot(SHUTDOWN_poweroff);\n}\n\nstatic void xen_machine_power_off(void)\n{\n\tdo_kernel_power_off();\n\txen_reboot(SHUTDOWN_poweroff);\n}\n\nstatic void xen_crash_shutdown(struct pt_regs *regs)\n{\n\txen_reboot(SHUTDOWN_crash);\n}\n\nstatic const struct machine_ops xen_machine_ops __initconst = {\n\t.restart = xen_restart,\n\t.halt = xen_machine_halt,\n\t.power_off = xen_machine_power_off,\n\t.shutdown = xen_machine_halt,\n\t.crash_shutdown = xen_crash_shutdown,\n\t.emergency_restart = xen_emergency_restart,\n};\n\nstatic unsigned char xen_get_nmi_reason(void)\n{\n\tunsigned char reason = 0;\n\n\t/* Construct a value which looks like it came from port 0x61. */\n\tif (test_bit(_XEN_NMIREASON_io_error,\n\t\t     &HYPERVISOR_shared_info->arch.nmi_reason))\n\t\treason |= NMI_REASON_IOCHK;\n\tif (test_bit(_XEN_NMIREASON_pci_serr,\n\t\t     &HYPERVISOR_shared_info->arch.nmi_reason))\n\t\treason |= NMI_REASON_SERR;\n\n\treturn reason;\n}\n\nstatic void __init xen_boot_params_init_edd(void)\n{\n#if IS_ENABLED(CONFIG_EDD)\n\tstruct xen_platform_op op;\n\tstruct edd_info *edd_info;\n\tu32 *mbr_signature;\n\tunsigned nr;\n\tint ret;\n\n\tedd_info = boot_params.eddbuf;\n\tmbr_signature = boot_params.edd_mbr_sig_buffer;\n\n\top.cmd = XENPF_firmware_info;\n\n\top.u.firmware_info.type = XEN_FW_DISK_INFO;\n\tfor (nr = 0; nr < EDDMAXNR; nr++) {\n\t\tstruct edd_info *info = edd_info + nr;\n\n\t\top.u.firmware_info.index = nr;\n\t\tinfo->params.length = sizeof(info->params);\n\t\tset_xen_guest_handle(op.u.firmware_info.u.disk_info.edd_params,\n\t\t\t\t     &info->params);\n\t\tret = HYPERVISOR_platform_op(&op);\n\t\tif (ret)\n\t\t\tbreak;\n\n#define C(x) info->x = op.u.firmware_info.u.disk_info.x\n\t\tC(device);\n\t\tC(version);\n\t\tC(interface_support);\n\t\tC(legacy_max_cylinder);\n\t\tC(legacy_max_head);\n\t\tC(legacy_sectors_per_track);\n#undef C\n\t}\n\tboot_params.eddbuf_entries = nr;\n\n\top.u.firmware_info.type = XEN_FW_DISK_MBR_SIGNATURE;\n\tfor (nr = 0; nr < EDD_MBR_SIG_MAX; nr++) {\n\t\top.u.firmware_info.index = nr;\n\t\tret = HYPERVISOR_platform_op(&op);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tmbr_signature[nr] = op.u.firmware_info.u.disk_mbr_signature.mbr_signature;\n\t}\n\tboot_params.edd_mbr_sig_buf_entries = nr;\n#endif\n}\n\n/*\n * Set up the GDT and segment registers for -fstack-protector.  Until\n * we do this, we have to be careful not to call any stack-protected\n * function, which is most of the kernel.\n */\nstatic void __init xen_setup_gdt(int cpu)\n{\n\tpv_ops.cpu.write_gdt_entry = xen_write_gdt_entry_boot;\n\tpv_ops.cpu.load_gdt = xen_load_gdt_boot;\n\n\tswitch_to_new_gdt(cpu);\n\n\tpv_ops.cpu.write_gdt_entry = xen_write_gdt_entry;\n\tpv_ops.cpu.load_gdt = xen_load_gdt;\n}\n\nstatic void __init xen_dom0_set_legacy_features(void)\n{\n\tx86_platform.legacy.rtc = 1;\n}\n\nstatic void __init xen_domu_set_legacy_features(void)\n{\n\tx86_platform.legacy.rtc = 0;\n}\n\nextern void early_xen_iret_patch(void);\n\n/* First C function to be called on Xen boot */\nasmlinkage __visible void __init xen_start_kernel(struct start_info *si)\n{\n\tstruct physdev_set_iopl set_iopl;\n\tunsigned long initrd_start = 0;\n\tint rc;\n\n\tif (!si)\n\t\treturn;\n\n\tclear_bss();\n\n\txen_start_info = si;\n\n\t__text_gen_insn(&early_xen_iret_patch,\n\t\t\tJMP32_INSN_OPCODE, &early_xen_iret_patch, &xen_iret,\n\t\t\tJMP32_INSN_SIZE);\n\n\txen_domain_type = XEN_PV_DOMAIN;\n\txen_start_flags = xen_start_info->flags;\n\n\txen_setup_features();\n\n\t/* Install Xen paravirt ops */\n\tpv_info = xen_info;\n\tpv_ops.cpu = xen_cpu_ops.cpu;\n\txen_init_irq_ops();\n\n\t/*\n\t * Setup xen_vcpu early because it is needed for\n\t * local_irq_disable(), irqs_disabled(), e.g. in printk().\n\t *\n\t * Don't do the full vcpu_info placement stuff until we have\n\t * the cpu_possible_mask and a non-dummy shared_info.\n\t */\n\txen_vcpu_info_reset(0);\n\n\tx86_platform.get_nmi_reason = xen_get_nmi_reason;\n\n\tx86_init.resources.memory_setup = xen_memory_setup;\n\tx86_init.irqs.intr_mode_select\t= x86_init_noop;\n\tx86_init.irqs.intr_mode_init\t= x86_init_noop;\n\tx86_init.oem.arch_setup = xen_arch_setup;\n\tx86_init.oem.banner = xen_banner;\n\tx86_init.hyper.init_platform = xen_pv_init_platform;\n\tx86_init.hyper.guest_late_init = xen_pv_guest_late_init;\n\n\t/*\n\t * Set up some pagetable state before starting to set any ptes.\n\t */\n\n\txen_setup_machphys_mapping();\n\txen_init_mmu_ops();\n\n\t/* Prevent unwanted bits from being set in PTEs. */\n\t__supported_pte_mask &= ~_PAGE_GLOBAL;\n\t__default_kernel_pte_mask &= ~_PAGE_GLOBAL;\n\n\t/* Get mfn list */\n\txen_build_dynamic_phys_to_machine();\n\n\t/* Work out if we support NX */\n\tget_cpu_cap(&boot_cpu_data);\n\tx86_configure_nx();\n\n\t/*\n\t * Set up kernel GDT and segment registers, mainly so that\n\t * -fstack-protector code can be executed.\n\t */\n\txen_setup_gdt(0);\n\n\t/* Determine virtual and physical address sizes */\n\tget_cpu_address_sizes(&boot_cpu_data);\n\n\t/* Let's presume PV guests always boot on vCPU with id 0. */\n\tper_cpu(xen_vcpu_id, 0) = 0;\n\n\tidt_setup_early_handler();\n\n\txen_init_capabilities();\n\n#ifdef CONFIG_X86_LOCAL_APIC\n\t/*\n\t * set up the basic apic ops.\n\t */\n\txen_init_apic();\n#endif\n\n\tmachine_ops = xen_machine_ops;\n\n\t/*\n\t * The only reliable way to retain the initial address of the\n\t * percpu gdt_page is to remember it here, so we can go and\n\t * mark it RW later, when the initial percpu area is freed.\n\t */\n\txen_initial_gdt = &per_cpu(gdt_page, 0);\n\n\txen_smp_init();\n\n#ifdef CONFIG_ACPI_NUMA\n\t/*\n\t * The pages we from Xen are not related to machine pages, so\n\t * any NUMA information the kernel tries to get from ACPI will\n\t * be meaningless.  Prevent it from trying.\n\t */\n\tdisable_srat();\n#endif\n\tWARN_ON(xen_cpuhp_setup(xen_cpu_up_prepare_pv, xen_cpu_dead_pv));\n\n\tlocal_irq_disable();\n\tearly_boot_irqs_disabled = true;\n\n\txen_raw_console_write(\"mapping kernel into physical memory\\n\");\n\txen_setup_kernel_pagetable((pgd_t *)xen_start_info->pt_base,\n\t\t\t\t   xen_start_info->nr_pages);\n\txen_reserve_special_pages();\n\n\t/*\n\t * We used to do this in xen_arch_setup, but that is too late\n\t * on AMD were early_cpu_init (run before ->arch_setup()) calls\n\t * early_amd_init which pokes 0xcf8 port.\n\t */\n\tset_iopl.iopl = 1;\n\trc = HYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);\n\tif (rc != 0)\n\t\txen_raw_printk(\"physdev_op failed %d\\n\", rc);\n\n\n\tif (xen_start_info->mod_start) {\n\t    if (xen_start_info->flags & SIF_MOD_START_PFN)\n\t\tinitrd_start = PFN_PHYS(xen_start_info->mod_start);\n\t    else\n\t\tinitrd_start = __pa(xen_start_info->mod_start);\n\t}\n\n\t/* Poke various useful things into boot_params */\n\tboot_params.hdr.type_of_loader = (9 << 4) | 0;\n\tboot_params.hdr.ramdisk_image = initrd_start;\n\tboot_params.hdr.ramdisk_size = xen_start_info->mod_len;\n\tboot_params.hdr.cmd_line_ptr = __pa(xen_start_info->cmd_line);\n\tboot_params.hdr.hardware_subarch = X86_SUBARCH_XEN;\n\n\tif (!xen_initial_domain()) {\n\t\tif (pci_xen)\n\t\t\tx86_init.pci.arch_init = pci_xen_init;\n\t\tx86_platform.set_legacy_features =\n\t\t\t\txen_domu_set_legacy_features;\n\t} else {\n\t\tconst struct dom0_vga_console_info *info =\n\t\t\t(void *)((char *)xen_start_info +\n\t\t\t\t xen_start_info->console.dom0.info_off);\n\t\tstruct xen_platform_op op = {\n\t\t\t.cmd = XENPF_firmware_info,\n\t\t\t.interface_version = XENPF_INTERFACE_VERSION,\n\t\t\t.u.firmware_info.type = XEN_FW_KBD_SHIFT_FLAGS,\n\t\t};\n\n\t\tx86_platform.set_legacy_features =\n\t\t\t\txen_dom0_set_legacy_features;\n\t\txen_init_vga(info, xen_start_info->console.dom0.info_size);\n\t\txen_start_info->console.domU.mfn = 0;\n\t\txen_start_info->console.domU.evtchn = 0;\n\n\t\tif (HYPERVISOR_platform_op(&op) == 0)\n\t\t\tboot_params.kbd_status = op.u.firmware_info.u.kbd_shift_flags;\n\n\t\t/* Make sure ACS will be enabled */\n\t\tpci_request_acs();\n\n\t\txen_acpi_sleep_register();\n\n\t\txen_boot_params_init_edd();\n\n#ifdef CONFIG_ACPI\n\t\t/*\n\t\t * Disable selecting \"Firmware First mode\" for correctable\n\t\t * memory errors, as this is the duty of the hypervisor to\n\t\t * decide.\n\t\t */\n\t\tacpi_disable_cmcff = 1;\n#endif\n\t}\n\n\txen_add_preferred_consoles();\n\n#ifdef CONFIG_PCI\n\t/* PCI BIOS service won't work from a PV guest. */\n\tpci_probe &= ~PCI_PROBE_BIOS;\n#endif\n\txen_raw_console_write(\"about to get started...\\n\");\n\n\t/* We need this for printk timestamps */\n\txen_setup_runstate_info(0);\n\n\txen_efi_init(&boot_params);\n\n\t/* Start the world */\n\tcr4_init_shadow(); /* 32b kernel does this in i386_start_kernel() */\n\tx86_64_start_reservations((char *)__pa_symbol(&boot_params));\n}\n\nstatic int xen_cpu_up_prepare_pv(unsigned int cpu)\n{\n\tint rc;\n\n\tif (per_cpu(xen_vcpu, cpu) == NULL)\n\t\treturn -ENODEV;\n\n\txen_setup_timer(cpu);\n\n\trc = xen_smp_intr_init(cpu);\n\tif (rc) {\n\t\tWARN(1, \"xen_smp_intr_init() for CPU %d failed: %d\\n\",\n\t\t     cpu, rc);\n\t\treturn rc;\n\t}\n\n\trc = xen_smp_intr_init_pv(cpu);\n\tif (rc) {\n\t\tWARN(1, \"xen_smp_intr_init_pv() for CPU %d failed: %d\\n\",\n\t\t     cpu, rc);\n\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\nstatic int xen_cpu_dead_pv(unsigned int cpu)\n{\n\txen_smp_intr_free(cpu);\n\txen_smp_intr_free_pv(cpu);\n\n\txen_teardown_timer(cpu);\n\n\treturn 0;\n}\n\nstatic uint32_t __init xen_platform_pv(void)\n{\n\tif (xen_pv_domain())\n\t\treturn xen_cpuid_base();\n\n\treturn 0;\n}\n\nconst __initconst struct hypervisor_x86 x86_hyper_xen_pv = {\n\t.name                   = \"Xen PV\",\n\t.detect                 = xen_platform_pv,\n\t.type\t\t\t= X86_HYPER_XEN_PV,\n\t.runtime.pin_vcpu       = xen_pin_vcpu,\n\t.ignore_nopv\t\t= true,\n};\n", "/* SPDX-License-Identifier: GPL-2.0 */\n/* Xen-specific pieces of head.S, intended to be included in the right\n\tplace in head.S */\n\n#ifdef CONFIG_XEN\n\n#include <linux/elfnote.h>\n#include <linux/init.h>\n\n#include <asm/boot.h>\n#include <asm/asm.h>\n#include <asm/msr.h>\n#include <asm/page_types.h>\n#include <asm/percpu.h>\n#include <asm/unwind_hints.h>\n\n#include <xen/interface/elfnote.h>\n#include <xen/interface/features.h>\n#include <xen/interface/xen.h>\n#include <xen/interface/xen-mca.h>\n#include <asm/xen/interface.h>\n\n.pushsection .noinstr.text, \"ax\"\n\t.balign PAGE_SIZE\nSYM_CODE_START(hypercall_page)\n\t.rept (PAGE_SIZE / 32)\n\t\tUNWIND_HINT_FUNC\n\t\tANNOTATE_NOENDBR\n\t\tret\n\t\t/*\n\t\t * Xen will write the hypercall page, and sort out ENDBR.\n\t\t */\n\t\t.skip 31, 0xcc\n\t.endr\n\n#define HYPERCALL(n) \\\n\t.equ xen_hypercall_##n, hypercall_page + __HYPERVISOR_##n * 32; \\\n\t.type xen_hypercall_##n, @function; .size xen_hypercall_##n, 32\n#include <asm/xen-hypercalls.h>\n#undef HYPERCALL\nSYM_CODE_END(hypercall_page)\n.popsection\n\n#ifdef CONFIG_XEN_PV\n\t__INIT\nSYM_CODE_START(startup_xen)\n\tUNWIND_HINT_EMPTY\n\tANNOTATE_NOENDBR\n\tcld\n\n\tmov initial_stack(%rip), %rsp\n\n\t/* Set up %gs.\n\t *\n\t * The base of %gs always points to fixed_percpu_data.  If the\n\t * stack protector canary is enabled, it is located at %gs:40.\n\t * Note that, on SMP, the boot cpu uses init data section until\n\t * the per cpu areas are set up.\n\t */\n\tmovl\t$MSR_GS_BASE,%ecx\n\tmovq\t$INIT_PER_CPU_VAR(fixed_percpu_data),%rax\n\tcdq\n\twrmsr\n\n\tmov\t%rsi, %rdi\n\tcall xen_start_kernel\nSYM_CODE_END(startup_xen)\n\t__FINIT\n\n#ifdef CONFIG_XEN_PV_SMP\n.pushsection .text\nSYM_CODE_START(asm_cpu_bringup_and_idle)\n\tUNWIND_HINT_EMPTY\n\tENDBR\n\n\tcall cpu_bringup_and_idle\nSYM_CODE_END(asm_cpu_bringup_and_idle)\n.popsection\n#endif\n#endif\n\n\tELFNOTE(Xen, XEN_ELFNOTE_GUEST_OS,       .asciz \"linux\")\n\tELFNOTE(Xen, XEN_ELFNOTE_GUEST_VERSION,  .asciz \"2.6\")\n\tELFNOTE(Xen, XEN_ELFNOTE_XEN_VERSION,    .asciz \"xen-3.0\")\n#ifdef CONFIG_X86_32\n\tELFNOTE(Xen, XEN_ELFNOTE_VIRT_BASE,      _ASM_PTR __PAGE_OFFSET)\n#else\n\tELFNOTE(Xen, XEN_ELFNOTE_VIRT_BASE,      _ASM_PTR __START_KERNEL_map)\n\t/* Map the p2m table to a 512GB-aligned user address. */\n\tELFNOTE(Xen, XEN_ELFNOTE_INIT_P2M,       .quad (PUD_SIZE * PTRS_PER_PUD))\n#endif\n#ifdef CONFIG_XEN_PV\n\tELFNOTE(Xen, XEN_ELFNOTE_ENTRY,          _ASM_PTR startup_xen)\n#endif\n\tELFNOTE(Xen, XEN_ELFNOTE_HYPERCALL_PAGE, _ASM_PTR hypercall_page)\n\tELFNOTE(Xen, XEN_ELFNOTE_FEATURES,\n\t\t.ascii \"!writable_page_tables|pae_pgdir_above_4gb\")\n\tELFNOTE(Xen, XEN_ELFNOTE_SUPPORTED_FEATURES,\n\t\t.long (1 << XENFEAT_writable_page_tables) |       \\\n\t\t      (1 << XENFEAT_dom0) |                       \\\n\t\t      (1 << XENFEAT_linux_rsdp_unrestricted))\n\tELFNOTE(Xen, XEN_ELFNOTE_PAE_MODE,       .asciz \"yes\")\n\tELFNOTE(Xen, XEN_ELFNOTE_LOADER,         .asciz \"generic\")\n\tELFNOTE(Xen, XEN_ELFNOTE_L1_MFN_VALID,\n\t\t.quad _PAGE_PRESENT; .quad _PAGE_PRESENT)\n\tELFNOTE(Xen, XEN_ELFNOTE_SUSPEND_CANCEL, .long 1)\n\tELFNOTE(Xen, XEN_ELFNOTE_MOD_START_PFN,  .long 1)\n\tELFNOTE(Xen, XEN_ELFNOTE_HV_START_LOW,   _ASM_PTR __HYPERVISOR_VIRT_START)\n\tELFNOTE(Xen, XEN_ELFNOTE_PADDR_OFFSET,   _ASM_PTR 0)\n\n#endif /*CONFIG_XEN */\n", "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * Copyright (C) 2015-2017 Josh Poimboeuf <jpoimboe@redhat.com>\n */\n\n#include <string.h>\n#include <stdlib.h>\n#include <inttypes.h>\n#include <sys/mman.h>\n\n#include <arch/elf.h>\n#include <objtool/builtin.h>\n#include <objtool/cfi.h>\n#include <objtool/arch.h>\n#include <objtool/check.h>\n#include <objtool/special.h>\n#include <objtool/warn.h>\n#include <objtool/endianness.h>\n\n#include <linux/objtool.h>\n#include <linux/hashtable.h>\n#include <linux/kernel.h>\n#include <linux/static_call_types.h>\n\nstruct alternative {\n\tstruct list_head list;\n\tstruct instruction *insn;\n\tbool skip_orig;\n};\n\nstatic unsigned long nr_cfi, nr_cfi_reused, nr_cfi_cache;\n\nstatic struct cfi_init_state initial_func_cfi;\nstatic struct cfi_state init_cfi;\nstatic struct cfi_state func_cfi;\n\nstruct instruction *find_insn(struct objtool_file *file,\n\t\t\t      struct section *sec, unsigned long offset)\n{\n\tstruct instruction *insn;\n\n\thash_for_each_possible(file->insn_hash, insn, hash, sec_offset_hash(sec, offset)) {\n\t\tif (insn->sec == sec && insn->offset == offset)\n\t\t\treturn insn;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct instruction *next_insn_same_sec(struct objtool_file *file,\n\t\t\t\t\t      struct instruction *insn)\n{\n\tstruct instruction *next = list_next_entry(insn, list);\n\n\tif (!next || &next->list == &file->insn_list || next->sec != insn->sec)\n\t\treturn NULL;\n\n\treturn next;\n}\n\nstatic struct instruction *next_insn_same_func(struct objtool_file *file,\n\t\t\t\t\t       struct instruction *insn)\n{\n\tstruct instruction *next = list_next_entry(insn, list);\n\tstruct symbol *func = insn->func;\n\n\tif (!func)\n\t\treturn NULL;\n\n\tif (&next->list != &file->insn_list && next->func == func)\n\t\treturn next;\n\n\t/* Check if we're already in the subfunction: */\n\tif (func == func->cfunc)\n\t\treturn NULL;\n\n\t/* Move to the subfunction: */\n\treturn find_insn(file, func->cfunc->sec, func->cfunc->offset);\n}\n\nstatic struct instruction *prev_insn_same_sym(struct objtool_file *file,\n\t\t\t\t\t       struct instruction *insn)\n{\n\tstruct instruction *prev = list_prev_entry(insn, list);\n\n\tif (&prev->list != &file->insn_list && prev->func == insn->func)\n\t\treturn prev;\n\n\treturn NULL;\n}\n\n#define func_for_each_insn(file, func, insn)\t\t\t\t\\\n\tfor (insn = find_insn(file, func->sec, func->offset);\t\t\\\n\t     insn;\t\t\t\t\t\t\t\\\n\t     insn = next_insn_same_func(file, insn))\n\n#define sym_for_each_insn(file, sym, insn)\t\t\t\t\\\n\tfor (insn = find_insn(file, sym->sec, sym->offset);\t\t\\\n\t     insn && &insn->list != &file->insn_list &&\t\t\t\\\n\t\tinsn->sec == sym->sec &&\t\t\t\t\\\n\t\tinsn->offset < sym->offset + sym->len;\t\t\t\\\n\t     insn = list_next_entry(insn, list))\n\n#define sym_for_each_insn_continue_reverse(file, sym, insn)\t\t\\\n\tfor (insn = list_prev_entry(insn, list);\t\t\t\\\n\t     &insn->list != &file->insn_list &&\t\t\t\t\\\n\t\tinsn->sec == sym->sec && insn->offset >= sym->offset;\t\\\n\t     insn = list_prev_entry(insn, list))\n\n#define sec_for_each_insn_from(file, insn)\t\t\t\t\\\n\tfor (; insn; insn = next_insn_same_sec(file, insn))\n\n#define sec_for_each_insn_continue(file, insn)\t\t\t\t\\\n\tfor (insn = next_insn_same_sec(file, insn); insn;\t\t\\\n\t     insn = next_insn_same_sec(file, insn))\n\nstatic bool is_jump_table_jump(struct instruction *insn)\n{\n\tstruct alt_group *alt_group = insn->alt_group;\n\n\tif (insn->jump_table)\n\t\treturn true;\n\n\t/* Retpoline alternative for a jump table? */\n\treturn alt_group && alt_group->orig_group &&\n\t       alt_group->orig_group->first_insn->jump_table;\n}\n\nstatic bool is_sibling_call(struct instruction *insn)\n{\n\t/*\n\t * Assume only ELF functions can make sibling calls.  This ensures\n\t * sibling call detection consistency between vmlinux.o and individual\n\t * objects.\n\t */\n\tif (!insn->func)\n\t\treturn false;\n\n\t/* An indirect jump is either a sibling call or a jump to a table. */\n\tif (insn->type == INSN_JUMP_DYNAMIC)\n\t\treturn !is_jump_table_jump(insn);\n\n\t/* add_jump_destinations() sets insn->call_dest for sibling calls. */\n\treturn (is_static_jump(insn) && insn->call_dest);\n}\n\n/*\n * This checks to see if the given function is a \"noreturn\" function.\n *\n * For global functions which are outside the scope of this object file, we\n * have to keep a manual list of them.\n *\n * For local functions, we have to detect them manually by simply looking for\n * the lack of a return instruction.\n */\nstatic bool __dead_end_function(struct objtool_file *file, struct symbol *func,\n\t\t\t\tint recursion)\n{\n\tint i;\n\tstruct instruction *insn;\n\tbool empty = true;\n\n\t/*\n\t * Unfortunately these have to be hard coded because the noreturn\n\t * attribute isn't provided in ELF data.\n\t */\n\tstatic const char * const global_noreturns[] = {\n\t\t\"__stack_chk_fail\",\n\t\t\"panic\",\n\t\t\"do_exit\",\n\t\t\"do_task_dead\",\n\t\t\"kthread_exit\",\n\t\t\"make_task_dead\",\n\t\t\"__module_put_and_kthread_exit\",\n\t\t\"kthread_complete_and_exit\",\n\t\t\"__reiserfs_panic\",\n\t\t\"lbug_with_loc\",\n\t\t\"fortify_panic\",\n\t\t\"usercopy_abort\",\n\t\t\"machine_real_restart\",\n\t\t\"rewind_stack_and_make_dead\",\n\t\t\"kunit_try_catch_throw\",\n\t\t\"xen_start_kernel\",\n\t\t\"cpu_bringup_and_idle\",\n\t\t\"do_group_exit\",\n\t\t\"stop_this_cpu\",\n\t\t\"__invalid_creds\",\n\t\t\"cpu_startup_entry\",\n\t\t\"__ubsan_handle_builtin_unreachable\",\n\t\t\"ex_handler_msr_mce\",\n\t};\n\n\tif (!func)\n\t\treturn false;\n\n\tif (func->bind == STB_WEAK)\n\t\treturn false;\n\n\tif (func->bind == STB_GLOBAL)\n\t\tfor (i = 0; i < ARRAY_SIZE(global_noreturns); i++)\n\t\t\tif (!strcmp(func->name, global_noreturns[i]))\n\t\t\t\treturn true;\n\n\tif (!func->len)\n\t\treturn false;\n\n\tinsn = find_insn(file, func->sec, func->offset);\n\tif (!insn->func)\n\t\treturn false;\n\n\tfunc_for_each_insn(file, func, insn) {\n\t\tempty = false;\n\n\t\tif (insn->type == INSN_RETURN)\n\t\t\treturn false;\n\t}\n\n\tif (empty)\n\t\treturn false;\n\n\t/*\n\t * A function can have a sibling call instead of a return.  In that\n\t * case, the function's dead-end status depends on whether the target\n\t * of the sibling call returns.\n\t */\n\tfunc_for_each_insn(file, func, insn) {\n\t\tif (is_sibling_call(insn)) {\n\t\t\tstruct instruction *dest = insn->jump_dest;\n\n\t\t\tif (!dest)\n\t\t\t\t/* sibling call to another file */\n\t\t\t\treturn false;\n\n\t\t\t/* local sibling call */\n\t\t\tif (recursion == 5) {\n\t\t\t\t/*\n\t\t\t\t * Infinite recursion: two functions have\n\t\t\t\t * sibling calls to each other.  This is a very\n\t\t\t\t * rare case.  It means they aren't dead ends.\n\t\t\t\t */\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\treturn __dead_end_function(file, dest->func, recursion+1);\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic bool dead_end_function(struct objtool_file *file, struct symbol *func)\n{\n\treturn __dead_end_function(file, func, 0);\n}\n\nstatic void init_cfi_state(struct cfi_state *cfi)\n{\n\tint i;\n\n\tfor (i = 0; i < CFI_NUM_REGS; i++) {\n\t\tcfi->regs[i].base = CFI_UNDEFINED;\n\t\tcfi->vals[i].base = CFI_UNDEFINED;\n\t}\n\tcfi->cfa.base = CFI_UNDEFINED;\n\tcfi->drap_reg = CFI_UNDEFINED;\n\tcfi->drap_offset = -1;\n}\n\nstatic void init_insn_state(struct objtool_file *file, struct insn_state *state,\n\t\t\t    struct section *sec)\n{\n\tmemset(state, 0, sizeof(*state));\n\tinit_cfi_state(&state->cfi);\n\n\t/*\n\t * We need the full vmlinux for noinstr validation, otherwise we can\n\t * not correctly determine insn->call_dest->sec (external symbols do\n\t * not have a section).\n\t */\n\tif (opts.link && opts.noinstr && sec)\n\t\tstate->noinstr = sec->noinstr;\n}\n\nstatic struct cfi_state *cfi_alloc(void)\n{\n\tstruct cfi_state *cfi = calloc(sizeof(struct cfi_state), 1);\n\tif (!cfi) {\n\t\tWARN(\"calloc failed\");\n\t\texit(1);\n\t}\n\tnr_cfi++;\n\treturn cfi;\n}\n\nstatic int cfi_bits;\nstatic struct hlist_head *cfi_hash;\n\nstatic inline bool cficmp(struct cfi_state *cfi1, struct cfi_state *cfi2)\n{\n\treturn memcmp((void *)cfi1 + sizeof(cfi1->hash),\n\t\t      (void *)cfi2 + sizeof(cfi2->hash),\n\t\t      sizeof(struct cfi_state) - sizeof(struct hlist_node));\n}\n\nstatic inline u32 cfi_key(struct cfi_state *cfi)\n{\n\treturn jhash((void *)cfi + sizeof(cfi->hash),\n\t\t     sizeof(*cfi) - sizeof(cfi->hash), 0);\n}\n\nstatic struct cfi_state *cfi_hash_find_or_add(struct cfi_state *cfi)\n{\n\tstruct hlist_head *head = &cfi_hash[hash_min(cfi_key(cfi), cfi_bits)];\n\tstruct cfi_state *obj;\n\n\thlist_for_each_entry(obj, head, hash) {\n\t\tif (!cficmp(cfi, obj)) {\n\t\t\tnr_cfi_cache++;\n\t\t\treturn obj;\n\t\t}\n\t}\n\n\tobj = cfi_alloc();\n\t*obj = *cfi;\n\thlist_add_head(&obj->hash, head);\n\n\treturn obj;\n}\n\nstatic void cfi_hash_add(struct cfi_state *cfi)\n{\n\tstruct hlist_head *head = &cfi_hash[hash_min(cfi_key(cfi), cfi_bits)];\n\n\thlist_add_head(&cfi->hash, head);\n}\n\nstatic void *cfi_hash_alloc(unsigned long size)\n{\n\tcfi_bits = max(10, ilog2(size));\n\tcfi_hash = mmap(NULL, sizeof(struct hlist_head) << cfi_bits,\n\t\t\tPROT_READ|PROT_WRITE,\n\t\t\tMAP_PRIVATE|MAP_ANON, -1, 0);\n\tif (cfi_hash == (void *)-1L) {\n\t\tWARN(\"mmap fail cfi_hash\");\n\t\tcfi_hash = NULL;\n\t}  else if (opts.stats) {\n\t\tprintf(\"cfi_bits: %d\\n\", cfi_bits);\n\t}\n\n\treturn cfi_hash;\n}\n\nstatic unsigned long nr_insns;\nstatic unsigned long nr_insns_visited;\n\n/*\n * Call the arch-specific instruction decoder for all the instructions and add\n * them to the global instruction list.\n */\nstatic int decode_instructions(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct symbol *func;\n\tunsigned long offset;\n\tstruct instruction *insn;\n\tint ret;\n\n\tfor_each_sec(file, sec) {\n\n\t\tif (!(sec->sh.sh_flags & SHF_EXECINSTR))\n\t\t\tcontinue;\n\n\t\tif (strcmp(sec->name, \".altinstr_replacement\") &&\n\t\t    strcmp(sec->name, \".altinstr_aux\") &&\n\t\t    strncmp(sec->name, \".discard.\", 9))\n\t\t\tsec->text = true;\n\n\t\tif (!strcmp(sec->name, \".noinstr.text\") ||\n\t\t    !strcmp(sec->name, \".entry.text\"))\n\t\t\tsec->noinstr = true;\n\n\t\tfor (offset = 0; offset < sec->sh.sh_size; offset += insn->len) {\n\t\t\tinsn = malloc(sizeof(*insn));\n\t\t\tif (!insn) {\n\t\t\t\tWARN(\"malloc failed\");\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tmemset(insn, 0, sizeof(*insn));\n\t\t\tINIT_LIST_HEAD(&insn->alts);\n\t\t\tINIT_LIST_HEAD(&insn->stack_ops);\n\t\t\tINIT_LIST_HEAD(&insn->call_node);\n\n\t\t\tinsn->sec = sec;\n\t\t\tinsn->offset = offset;\n\n\t\t\tret = arch_decode_instruction(file, sec, offset,\n\t\t\t\t\t\t      sec->sh.sh_size - offset,\n\t\t\t\t\t\t      &insn->len, &insn->type,\n\t\t\t\t\t\t      &insn->immediate,\n\t\t\t\t\t\t      &insn->stack_ops);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\t\t/*\n\t\t\t * By default, \"ud2\" is a dead end unless otherwise\n\t\t\t * annotated, because GCC 7 inserts it for certain\n\t\t\t * divide-by-zero cases.\n\t\t\t */\n\t\t\tif (insn->type == INSN_BUG)\n\t\t\t\tinsn->dead_end = true;\n\n\t\t\thash_add(file->insn_hash, &insn->hash, sec_offset_hash(sec, insn->offset));\n\t\t\tlist_add_tail(&insn->list, &file->insn_list);\n\t\t\tnr_insns++;\n\t\t}\n\n\t\tlist_for_each_entry(func, &sec->symbol_list, list) {\n\t\t\tif (func->type != STT_FUNC || func->alias != func)\n\t\t\t\tcontinue;\n\n\t\t\tif (!find_insn(file, sec, func->offset)) {\n\t\t\t\tWARN(\"%s(): can't find starting instruction\",\n\t\t\t\t     func->name);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tsym_for_each_insn(file, func, insn) {\n\t\t\t\tinsn->func = func;\n\t\t\t\tif (insn->type == INSN_ENDBR && list_empty(&insn->call_node)) {\n\t\t\t\t\tif (insn->offset == insn->func->offset) {\n\t\t\t\t\t\tlist_add_tail(&insn->call_node, &file->endbr_list);\n\t\t\t\t\t\tfile->nr_endbr++;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tfile->nr_endbr_int++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (opts.stats)\n\t\tprintf(\"nr_insns: %lu\\n\", nr_insns);\n\n\treturn 0;\n\nerr:\n\tfree(insn);\n\treturn ret;\n}\n\n/*\n * Read the pv_ops[] .data table to find the static initialized values.\n */\nstatic int add_pv_ops(struct objtool_file *file, const char *symname)\n{\n\tstruct symbol *sym, *func;\n\tunsigned long off, end;\n\tstruct reloc *rel;\n\tint idx;\n\n\tsym = find_symbol_by_name(file->elf, symname);\n\tif (!sym)\n\t\treturn 0;\n\n\toff = sym->offset;\n\tend = off + sym->len;\n\tfor (;;) {\n\t\trel = find_reloc_by_dest_range(file->elf, sym->sec, off, end - off);\n\t\tif (!rel)\n\t\t\tbreak;\n\n\t\tfunc = rel->sym;\n\t\tif (func->type == STT_SECTION)\n\t\t\tfunc = find_symbol_by_offset(rel->sym->sec, rel->addend);\n\n\t\tidx = (rel->offset - sym->offset) / sizeof(unsigned long);\n\n\t\tobjtool_pv_add(file, idx, func);\n\n\t\toff = rel->offset + 1;\n\t\tif (off > end)\n\t\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Allocate and initialize file->pv_ops[].\n */\nstatic int init_pv_ops(struct objtool_file *file)\n{\n\tstatic const char *pv_ops_tables[] = {\n\t\t\"pv_ops\",\n\t\t\"xen_cpu_ops\",\n\t\t\"xen_irq_ops\",\n\t\t\"xen_mmu_ops\",\n\t\tNULL,\n\t};\n\tconst char *pv_ops;\n\tstruct symbol *sym;\n\tint idx, nr;\n\n\tif (!opts.noinstr)\n\t\treturn 0;\n\n\tfile->pv_ops = NULL;\n\n\tsym = find_symbol_by_name(file->elf, \"pv_ops\");\n\tif (!sym)\n\t\treturn 0;\n\n\tnr = sym->len / sizeof(unsigned long);\n\tfile->pv_ops = calloc(sizeof(struct pv_state), nr);\n\tif (!file->pv_ops)\n\t\treturn -1;\n\n\tfor (idx = 0; idx < nr; idx++)\n\t\tINIT_LIST_HEAD(&file->pv_ops[idx].targets);\n\n\tfor (idx = 0; (pv_ops = pv_ops_tables[idx]); idx++)\n\t\tadd_pv_ops(file, pv_ops);\n\n\treturn 0;\n}\n\nstatic struct instruction *find_last_insn(struct objtool_file *file,\n\t\t\t\t\t  struct section *sec)\n{\n\tstruct instruction *insn = NULL;\n\tunsigned int offset;\n\tunsigned int end = (sec->sh.sh_size > 10) ? sec->sh.sh_size - 10 : 0;\n\n\tfor (offset = sec->sh.sh_size - 1; offset >= end && !insn; offset--)\n\t\tinsn = find_insn(file, sec, offset);\n\n\treturn insn;\n}\n\n/*\n * Mark \"ud2\" instructions and manually annotated dead ends.\n */\nstatic int add_dead_ends(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct reloc *reloc;\n\tstruct instruction *insn;\n\n\t/*\n\t * Check for manually annotated dead ends.\n\t */\n\tsec = find_section_by_name(file->elf, \".rela.discard.unreachable\");\n\tif (!sec)\n\t\tgoto reachable;\n\n\tlist_for_each_entry(reloc, &sec->reloc_list, list) {\n\t\tif (reloc->sym->type != STT_SECTION) {\n\t\t\tWARN(\"unexpected relocation symbol type in %s\", sec->name);\n\t\t\treturn -1;\n\t\t}\n\t\tinsn = find_insn(file, reloc->sym->sec, reloc->addend);\n\t\tif (insn)\n\t\t\tinsn = list_prev_entry(insn, list);\n\t\telse if (reloc->addend == reloc->sym->sec->sh.sh_size) {\n\t\t\tinsn = find_last_insn(file, reloc->sym->sec);\n\t\t\tif (!insn) {\n\t\t\t\tWARN(\"can't find unreachable insn at %s+0x%\" PRIx64,\n\t\t\t\t     reloc->sym->sec->name, reloc->addend);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t} else {\n\t\t\tWARN(\"can't find unreachable insn at %s+0x%\" PRIx64,\n\t\t\t     reloc->sym->sec->name, reloc->addend);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn->dead_end = true;\n\t}\n\nreachable:\n\t/*\n\t * These manually annotated reachable checks are needed for GCC 4.4,\n\t * where the Linux unreachable() macro isn't supported.  In that case\n\t * GCC doesn't know the \"ud2\" is fatal, so it generates code as if it's\n\t * not a dead end.\n\t */\n\tsec = find_section_by_name(file->elf, \".rela.discard.reachable\");\n\tif (!sec)\n\t\treturn 0;\n\n\tlist_for_each_entry(reloc, &sec->reloc_list, list) {\n\t\tif (reloc->sym->type != STT_SECTION) {\n\t\t\tWARN(\"unexpected relocation symbol type in %s\", sec->name);\n\t\t\treturn -1;\n\t\t}\n\t\tinsn = find_insn(file, reloc->sym->sec, reloc->addend);\n\t\tif (insn)\n\t\t\tinsn = list_prev_entry(insn, list);\n\t\telse if (reloc->addend == reloc->sym->sec->sh.sh_size) {\n\t\t\tinsn = find_last_insn(file, reloc->sym->sec);\n\t\t\tif (!insn) {\n\t\t\t\tWARN(\"can't find reachable insn at %s+0x%\" PRIx64,\n\t\t\t\t     reloc->sym->sec->name, reloc->addend);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t} else {\n\t\t\tWARN(\"can't find reachable insn at %s+0x%\" PRIx64,\n\t\t\t     reloc->sym->sec->name, reloc->addend);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn->dead_end = false;\n\t}\n\n\treturn 0;\n}\n\nstatic int create_static_call_sections(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct static_call_site *site;\n\tstruct instruction *insn;\n\tstruct symbol *key_sym;\n\tchar *key_name, *tmp;\n\tint idx;\n\n\tsec = find_section_by_name(file->elf, \".static_call_sites\");\n\tif (sec) {\n\t\tINIT_LIST_HEAD(&file->static_call_list);\n\t\tWARN(\"file already has .static_call_sites section, skipping\");\n\t\treturn 0;\n\t}\n\n\tif (list_empty(&file->static_call_list))\n\t\treturn 0;\n\n\tidx = 0;\n\tlist_for_each_entry(insn, &file->static_call_list, call_node)\n\t\tidx++;\n\n\tsec = elf_create_section(file->elf, \".static_call_sites\", SHF_WRITE,\n\t\t\t\t sizeof(struct static_call_site), idx);\n\tif (!sec)\n\t\treturn -1;\n\n\tidx = 0;\n\tlist_for_each_entry(insn, &file->static_call_list, call_node) {\n\n\t\tsite = (struct static_call_site *)sec->data->d_buf + idx;\n\t\tmemset(site, 0, sizeof(struct static_call_site));\n\n\t\t/* populate reloc for 'addr' */\n\t\tif (elf_add_reloc_to_insn(file->elf, sec,\n\t\t\t\t\t  idx * sizeof(struct static_call_site),\n\t\t\t\t\t  R_X86_64_PC32,\n\t\t\t\t\t  insn->sec, insn->offset))\n\t\t\treturn -1;\n\n\t\t/* find key symbol */\n\t\tkey_name = strdup(insn->call_dest->name);\n\t\tif (!key_name) {\n\t\t\tperror(\"strdup\");\n\t\t\treturn -1;\n\t\t}\n\t\tif (strncmp(key_name, STATIC_CALL_TRAMP_PREFIX_STR,\n\t\t\t    STATIC_CALL_TRAMP_PREFIX_LEN)) {\n\t\t\tWARN(\"static_call: trampoline name malformed: %s\", key_name);\n\t\t\treturn -1;\n\t\t}\n\t\ttmp = key_name + STATIC_CALL_TRAMP_PREFIX_LEN - STATIC_CALL_KEY_PREFIX_LEN;\n\t\tmemcpy(tmp, STATIC_CALL_KEY_PREFIX_STR, STATIC_CALL_KEY_PREFIX_LEN);\n\n\t\tkey_sym = find_symbol_by_name(file->elf, tmp);\n\t\tif (!key_sym) {\n\t\t\tif (!opts.module) {\n\t\t\t\tWARN(\"static_call: can't find static_call_key symbol: %s\", tmp);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * For modules(), the key might not be exported, which\n\t\t\t * means the module can make static calls but isn't\n\t\t\t * allowed to change them.\n\t\t\t *\n\t\t\t * In that case we temporarily set the key to be the\n\t\t\t * trampoline address.  This is fixed up in\n\t\t\t * static_call_add_module().\n\t\t\t */\n\t\t\tkey_sym = insn->call_dest;\n\t\t}\n\t\tfree(key_name);\n\n\t\t/* populate reloc for 'key' */\n\t\tif (elf_add_reloc(file->elf, sec,\n\t\t\t\t  idx * sizeof(struct static_call_site) + 4,\n\t\t\t\t  R_X86_64_PC32, key_sym,\n\t\t\t\t  is_sibling_call(insn) * STATIC_CALL_SITE_TAIL))\n\t\t\treturn -1;\n\n\t\tidx++;\n\t}\n\n\treturn 0;\n}\n\nstatic int create_retpoline_sites_sections(struct objtool_file *file)\n{\n\tstruct instruction *insn;\n\tstruct section *sec;\n\tint idx;\n\n\tsec = find_section_by_name(file->elf, \".retpoline_sites\");\n\tif (sec) {\n\t\tWARN(\"file already has .retpoline_sites, skipping\");\n\t\treturn 0;\n\t}\n\n\tidx = 0;\n\tlist_for_each_entry(insn, &file->retpoline_call_list, call_node)\n\t\tidx++;\n\n\tif (!idx)\n\t\treturn 0;\n\n\tsec = elf_create_section(file->elf, \".retpoline_sites\", 0,\n\t\t\t\t sizeof(int), idx);\n\tif (!sec) {\n\t\tWARN(\"elf_create_section: .retpoline_sites\");\n\t\treturn -1;\n\t}\n\n\tidx = 0;\n\tlist_for_each_entry(insn, &file->retpoline_call_list, call_node) {\n\n\t\tint *site = (int *)sec->data->d_buf + idx;\n\t\t*site = 0;\n\n\t\tif (elf_add_reloc_to_insn(file->elf, sec,\n\t\t\t\t\t  idx * sizeof(int),\n\t\t\t\t\t  R_X86_64_PC32,\n\t\t\t\t\t  insn->sec, insn->offset)) {\n\t\t\tWARN(\"elf_add_reloc_to_insn: .retpoline_sites\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tidx++;\n\t}\n\n\treturn 0;\n}\n\nstatic int create_ibt_endbr_seal_sections(struct objtool_file *file)\n{\n\tstruct instruction *insn;\n\tstruct section *sec;\n\tint idx;\n\n\tsec = find_section_by_name(file->elf, \".ibt_endbr_seal\");\n\tif (sec) {\n\t\tWARN(\"file already has .ibt_endbr_seal, skipping\");\n\t\treturn 0;\n\t}\n\n\tidx = 0;\n\tlist_for_each_entry(insn, &file->endbr_list, call_node)\n\t\tidx++;\n\n\tif (opts.stats) {\n\t\tprintf(\"ibt: ENDBR at function start: %d\\n\", file->nr_endbr);\n\t\tprintf(\"ibt: ENDBR inside functions:  %d\\n\", file->nr_endbr_int);\n\t\tprintf(\"ibt: superfluous ENDBR:       %d\\n\", idx);\n\t}\n\n\tif (!idx)\n\t\treturn 0;\n\n\tsec = elf_create_section(file->elf, \".ibt_endbr_seal\", 0,\n\t\t\t\t sizeof(int), idx);\n\tif (!sec) {\n\t\tWARN(\"elf_create_section: .ibt_endbr_seal\");\n\t\treturn -1;\n\t}\n\n\tidx = 0;\n\tlist_for_each_entry(insn, &file->endbr_list, call_node) {\n\n\t\tint *site = (int *)sec->data->d_buf + idx;\n\t\t*site = 0;\n\n\t\tif (elf_add_reloc_to_insn(file->elf, sec,\n\t\t\t\t\t  idx * sizeof(int),\n\t\t\t\t\t  R_X86_64_PC32,\n\t\t\t\t\t  insn->sec, insn->offset)) {\n\t\t\tWARN(\"elf_add_reloc_to_insn: .ibt_endbr_seal\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tidx++;\n\t}\n\n\treturn 0;\n}\n\nstatic int create_mcount_loc_sections(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tunsigned long *loc;\n\tstruct instruction *insn;\n\tint idx;\n\n\tsec = find_section_by_name(file->elf, \"__mcount_loc\");\n\tif (sec) {\n\t\tINIT_LIST_HEAD(&file->mcount_loc_list);\n\t\tWARN(\"file already has __mcount_loc section, skipping\");\n\t\treturn 0;\n\t}\n\n\tif (list_empty(&file->mcount_loc_list))\n\t\treturn 0;\n\n\tidx = 0;\n\tlist_for_each_entry(insn, &file->mcount_loc_list, call_node)\n\t\tidx++;\n\n\tsec = elf_create_section(file->elf, \"__mcount_loc\", 0, sizeof(unsigned long), idx);\n\tif (!sec)\n\t\treturn -1;\n\n\tidx = 0;\n\tlist_for_each_entry(insn, &file->mcount_loc_list, call_node) {\n\n\t\tloc = (unsigned long *)sec->data->d_buf + idx;\n\t\tmemset(loc, 0, sizeof(unsigned long));\n\n\t\tif (elf_add_reloc_to_insn(file->elf, sec,\n\t\t\t\t\t  idx * sizeof(unsigned long),\n\t\t\t\t\t  R_X86_64_64,\n\t\t\t\t\t  insn->sec, insn->offset))\n\t\t\treturn -1;\n\n\t\tidx++;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Warnings shouldn't be reported for ignored functions.\n */\nstatic void add_ignores(struct objtool_file *file)\n{\n\tstruct instruction *insn;\n\tstruct section *sec;\n\tstruct symbol *func;\n\tstruct reloc *reloc;\n\n\tsec = find_section_by_name(file->elf, \".rela.discard.func_stack_frame_non_standard\");\n\tif (!sec)\n\t\treturn;\n\n\tlist_for_each_entry(reloc, &sec->reloc_list, list) {\n\t\tswitch (reloc->sym->type) {\n\t\tcase STT_FUNC:\n\t\t\tfunc = reloc->sym;\n\t\t\tbreak;\n\n\t\tcase STT_SECTION:\n\t\t\tfunc = find_func_by_offset(reloc->sym->sec, reloc->addend);\n\t\t\tif (!func)\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tWARN(\"unexpected relocation symbol type in %s: %d\", sec->name, reloc->sym->type);\n\t\t\tcontinue;\n\t\t}\n\n\t\tfunc_for_each_insn(file, func, insn)\n\t\t\tinsn->ignore = true;\n\t}\n}\n\n/*\n * This is a whitelist of functions that is allowed to be called with AC set.\n * The list is meant to be minimal and only contains compiler instrumentation\n * ABI and a few functions used to implement *_{to,from}_user() functions.\n *\n * These functions must not directly change AC, but may PUSHF/POPF.\n */\nstatic const char *uaccess_safe_builtin[] = {\n\t/* KASAN */\n\t\"kasan_report\",\n\t\"kasan_check_range\",\n\t/* KASAN out-of-line */\n\t\"__asan_loadN_noabort\",\n\t\"__asan_load1_noabort\",\n\t\"__asan_load2_noabort\",\n\t\"__asan_load4_noabort\",\n\t\"__asan_load8_noabort\",\n\t\"__asan_load16_noabort\",\n\t\"__asan_storeN_noabort\",\n\t\"__asan_store1_noabort\",\n\t\"__asan_store2_noabort\",\n\t\"__asan_store4_noabort\",\n\t\"__asan_store8_noabort\",\n\t\"__asan_store16_noabort\",\n\t\"__kasan_check_read\",\n\t\"__kasan_check_write\",\n\t/* KASAN in-line */\n\t\"__asan_report_load_n_noabort\",\n\t\"__asan_report_load1_noabort\",\n\t\"__asan_report_load2_noabort\",\n\t\"__asan_report_load4_noabort\",\n\t\"__asan_report_load8_noabort\",\n\t\"__asan_report_load16_noabort\",\n\t\"__asan_report_store_n_noabort\",\n\t\"__asan_report_store1_noabort\",\n\t\"__asan_report_store2_noabort\",\n\t\"__asan_report_store4_noabort\",\n\t\"__asan_report_store8_noabort\",\n\t\"__asan_report_store16_noabort\",\n\t/* KCSAN */\n\t\"__kcsan_check_access\",\n\t\"__kcsan_mb\",\n\t\"__kcsan_wmb\",\n\t\"__kcsan_rmb\",\n\t\"__kcsan_release\",\n\t\"kcsan_found_watchpoint\",\n\t\"kcsan_setup_watchpoint\",\n\t\"kcsan_check_scoped_accesses\",\n\t\"kcsan_disable_current\",\n\t\"kcsan_enable_current_nowarn\",\n\t/* KCSAN/TSAN */\n\t\"__tsan_func_entry\",\n\t\"__tsan_func_exit\",\n\t\"__tsan_read_range\",\n\t\"__tsan_write_range\",\n\t\"__tsan_read1\",\n\t\"__tsan_read2\",\n\t\"__tsan_read4\",\n\t\"__tsan_read8\",\n\t\"__tsan_read16\",\n\t\"__tsan_write1\",\n\t\"__tsan_write2\",\n\t\"__tsan_write4\",\n\t\"__tsan_write8\",\n\t\"__tsan_write16\",\n\t\"__tsan_read_write1\",\n\t\"__tsan_read_write2\",\n\t\"__tsan_read_write4\",\n\t\"__tsan_read_write8\",\n\t\"__tsan_read_write16\",\n\t\"__tsan_atomic8_load\",\n\t\"__tsan_atomic16_load\",\n\t\"__tsan_atomic32_load\",\n\t\"__tsan_atomic64_load\",\n\t\"__tsan_atomic8_store\",\n\t\"__tsan_atomic16_store\",\n\t\"__tsan_atomic32_store\",\n\t\"__tsan_atomic64_store\",\n\t\"__tsan_atomic8_exchange\",\n\t\"__tsan_atomic16_exchange\",\n\t\"__tsan_atomic32_exchange\",\n\t\"__tsan_atomic64_exchange\",\n\t\"__tsan_atomic8_fetch_add\",\n\t\"__tsan_atomic16_fetch_add\",\n\t\"__tsan_atomic32_fetch_add\",\n\t\"__tsan_atomic64_fetch_add\",\n\t\"__tsan_atomic8_fetch_sub\",\n\t\"__tsan_atomic16_fetch_sub\",\n\t\"__tsan_atomic32_fetch_sub\",\n\t\"__tsan_atomic64_fetch_sub\",\n\t\"__tsan_atomic8_fetch_and\",\n\t\"__tsan_atomic16_fetch_and\",\n\t\"__tsan_atomic32_fetch_and\",\n\t\"__tsan_atomic64_fetch_and\",\n\t\"__tsan_atomic8_fetch_or\",\n\t\"__tsan_atomic16_fetch_or\",\n\t\"__tsan_atomic32_fetch_or\",\n\t\"__tsan_atomic64_fetch_or\",\n\t\"__tsan_atomic8_fetch_xor\",\n\t\"__tsan_atomic16_fetch_xor\",\n\t\"__tsan_atomic32_fetch_xor\",\n\t\"__tsan_atomic64_fetch_xor\",\n\t\"__tsan_atomic8_fetch_nand\",\n\t\"__tsan_atomic16_fetch_nand\",\n\t\"__tsan_atomic32_fetch_nand\",\n\t\"__tsan_atomic64_fetch_nand\",\n\t\"__tsan_atomic8_compare_exchange_strong\",\n\t\"__tsan_atomic16_compare_exchange_strong\",\n\t\"__tsan_atomic32_compare_exchange_strong\",\n\t\"__tsan_atomic64_compare_exchange_strong\",\n\t\"__tsan_atomic8_compare_exchange_weak\",\n\t\"__tsan_atomic16_compare_exchange_weak\",\n\t\"__tsan_atomic32_compare_exchange_weak\",\n\t\"__tsan_atomic64_compare_exchange_weak\",\n\t\"__tsan_atomic8_compare_exchange_val\",\n\t\"__tsan_atomic16_compare_exchange_val\",\n\t\"__tsan_atomic32_compare_exchange_val\",\n\t\"__tsan_atomic64_compare_exchange_val\",\n\t\"__tsan_atomic_thread_fence\",\n\t\"__tsan_atomic_signal_fence\",\n\t/* KCOV */\n\t\"write_comp_data\",\n\t\"check_kcov_mode\",\n\t\"__sanitizer_cov_trace_pc\",\n\t\"__sanitizer_cov_trace_const_cmp1\",\n\t\"__sanitizer_cov_trace_const_cmp2\",\n\t\"__sanitizer_cov_trace_const_cmp4\",\n\t\"__sanitizer_cov_trace_const_cmp8\",\n\t\"__sanitizer_cov_trace_cmp1\",\n\t\"__sanitizer_cov_trace_cmp2\",\n\t\"__sanitizer_cov_trace_cmp4\",\n\t\"__sanitizer_cov_trace_cmp8\",\n\t\"__sanitizer_cov_trace_switch\",\n\t/* UBSAN */\n\t\"ubsan_type_mismatch_common\",\n\t\"__ubsan_handle_type_mismatch\",\n\t\"__ubsan_handle_type_mismatch_v1\",\n\t\"__ubsan_handle_shift_out_of_bounds\",\n\t/* misc */\n\t\"csum_partial_copy_generic\",\n\t\"copy_mc_fragile\",\n\t\"copy_mc_fragile_handle_tail\",\n\t\"copy_mc_enhanced_fast_string\",\n\t\"ftrace_likely_update\", /* CONFIG_TRACE_BRANCH_PROFILING */\n\tNULL\n};\n\nstatic void add_uaccess_safe(struct objtool_file *file)\n{\n\tstruct symbol *func;\n\tconst char **name;\n\n\tif (!opts.uaccess)\n\t\treturn;\n\n\tfor (name = uaccess_safe_builtin; *name; name++) {\n\t\tfunc = find_symbol_by_name(file->elf, *name);\n\t\tif (!func)\n\t\t\tcontinue;\n\n\t\tfunc->uaccess_safe = true;\n\t}\n}\n\n/*\n * FIXME: For now, just ignore any alternatives which add retpolines.  This is\n * a temporary hack, as it doesn't allow ORC to unwind from inside a retpoline.\n * But it at least allows objtool to understand the control flow *around* the\n * retpoline.\n */\nstatic int add_ignore_alternatives(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct reloc *reloc;\n\tstruct instruction *insn;\n\n\tsec = find_section_by_name(file->elf, \".rela.discard.ignore_alts\");\n\tif (!sec)\n\t\treturn 0;\n\n\tlist_for_each_entry(reloc, &sec->reloc_list, list) {\n\t\tif (reloc->sym->type != STT_SECTION) {\n\t\t\tWARN(\"unexpected relocation symbol type in %s\", sec->name);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn = find_insn(file, reloc->sym->sec, reloc->addend);\n\t\tif (!insn) {\n\t\t\tWARN(\"bad .discard.ignore_alts entry\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn->ignore_alts = true;\n\t}\n\n\treturn 0;\n}\n\n__weak bool arch_is_retpoline(struct symbol *sym)\n{\n\treturn false;\n}\n\n#define NEGATIVE_RELOC\t((void *)-1L)\n\nstatic struct reloc *insn_reloc(struct objtool_file *file, struct instruction *insn)\n{\n\tif (insn->reloc == NEGATIVE_RELOC)\n\t\treturn NULL;\n\n\tif (!insn->reloc) {\n\t\tif (!file)\n\t\t\treturn NULL;\n\n\t\tinsn->reloc = find_reloc_by_dest_range(file->elf, insn->sec,\n\t\t\t\t\t\t       insn->offset, insn->len);\n\t\tif (!insn->reloc) {\n\t\t\tinsn->reloc = NEGATIVE_RELOC;\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn insn->reloc;\n}\n\nstatic void remove_insn_ops(struct instruction *insn)\n{\n\tstruct stack_op *op, *tmp;\n\n\tlist_for_each_entry_safe(op, tmp, &insn->stack_ops, list) {\n\t\tlist_del(&op->list);\n\t\tfree(op);\n\t}\n}\n\nstatic void annotate_call_site(struct objtool_file *file,\n\t\t\t       struct instruction *insn, bool sibling)\n{\n\tstruct reloc *reloc = insn_reloc(file, insn);\n\tstruct symbol *sym = insn->call_dest;\n\n\tif (!sym)\n\t\tsym = reloc->sym;\n\n\t/*\n\t * Alternative replacement code is just template code which is\n\t * sometimes copied to the original instruction. For now, don't\n\t * annotate it. (In the future we might consider annotating the\n\t * original instruction if/when it ever makes sense to do so.)\n\t */\n\tif (!strcmp(insn->sec->name, \".altinstr_replacement\"))\n\t\treturn;\n\n\tif (sym->static_call_tramp) {\n\t\tlist_add_tail(&insn->call_node, &file->static_call_list);\n\t\treturn;\n\t}\n\n\tif (sym->retpoline_thunk) {\n\t\tlist_add_tail(&insn->call_node, &file->retpoline_call_list);\n\t\treturn;\n\t}\n\n\t/*\n\t * Many compilers cannot disable KCOV or sanitizer calls with a function\n\t * attribute so they need a little help, NOP out any such calls from\n\t * noinstr text.\n\t */\n\tif (opts.hack_noinstr && insn->sec->noinstr && sym->profiling_func) {\n\t\tif (reloc) {\n\t\t\treloc->type = R_NONE;\n\t\t\telf_write_reloc(file->elf, reloc);\n\t\t}\n\n\t\telf_write_insn(file->elf, insn->sec,\n\t\t\t       insn->offset, insn->len,\n\t\t\t       sibling ? arch_ret_insn(insn->len)\n\t\t\t               : arch_nop_insn(insn->len));\n\n\t\tinsn->type = sibling ? INSN_RETURN : INSN_NOP;\n\n\t\tif (sibling) {\n\t\t\t/*\n\t\t\t * We've replaced the tail-call JMP insn by two new\n\t\t\t * insn: RET; INT3, except we only have a single struct\n\t\t\t * insn here. Mark it retpoline_safe to avoid the SLS\n\t\t\t * warning, instead of adding another insn.\n\t\t\t */\n\t\t\tinsn->retpoline_safe = true;\n\t\t}\n\n\t\treturn;\n\t}\n\n\tif (opts.mcount && sym->fentry) {\n\t\tif (sibling)\n\t\t\tWARN_FUNC(\"Tail call to __fentry__ !?!?\", insn->sec, insn->offset);\n\n\t\tif (reloc) {\n\t\t\treloc->type = R_NONE;\n\t\t\telf_write_reloc(file->elf, reloc);\n\t\t}\n\n\t\telf_write_insn(file->elf, insn->sec,\n\t\t\t       insn->offset, insn->len,\n\t\t\t       arch_nop_insn(insn->len));\n\n\t\tinsn->type = INSN_NOP;\n\n\t\tlist_add_tail(&insn->call_node, &file->mcount_loc_list);\n\t\treturn;\n\t}\n\n\tif (!sibling && dead_end_function(file, sym))\n\t\tinsn->dead_end = true;\n}\n\nstatic void add_call_dest(struct objtool_file *file, struct instruction *insn,\n\t\t\t  struct symbol *dest, bool sibling)\n{\n\tinsn->call_dest = dest;\n\tif (!dest)\n\t\treturn;\n\n\t/*\n\t * Whatever stack impact regular CALLs have, should be undone\n\t * by the RETURN of the called function.\n\t *\n\t * Annotated intra-function calls retain the stack_ops but\n\t * are converted to JUMP, see read_intra_function_calls().\n\t */\n\tremove_insn_ops(insn);\n\n\tannotate_call_site(file, insn, sibling);\n}\n\nstatic void add_retpoline_call(struct objtool_file *file, struct instruction *insn)\n{\n\t/*\n\t * Retpoline calls/jumps are really dynamic calls/jumps in disguise,\n\t * so convert them accordingly.\n\t */\n\tswitch (insn->type) {\n\tcase INSN_CALL:\n\t\tinsn->type = INSN_CALL_DYNAMIC;\n\t\tbreak;\n\tcase INSN_JUMP_UNCONDITIONAL:\n\t\tinsn->type = INSN_JUMP_DYNAMIC;\n\t\tbreak;\n\tcase INSN_JUMP_CONDITIONAL:\n\t\tinsn->type = INSN_JUMP_DYNAMIC_CONDITIONAL;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tinsn->retpoline_safe = true;\n\n\t/*\n\t * Whatever stack impact regular CALLs have, should be undone\n\t * by the RETURN of the called function.\n\t *\n\t * Annotated intra-function calls retain the stack_ops but\n\t * are converted to JUMP, see read_intra_function_calls().\n\t */\n\tremove_insn_ops(insn);\n\n\tannotate_call_site(file, insn, false);\n}\n\nstatic bool same_function(struct instruction *insn1, struct instruction *insn2)\n{\n\treturn insn1->func->pfunc == insn2->func->pfunc;\n}\n\nstatic bool is_first_func_insn(struct objtool_file *file, struct instruction *insn)\n{\n\tif (insn->offset == insn->func->offset)\n\t\treturn true;\n\n\tif (opts.ibt) {\n\t\tstruct instruction *prev = prev_insn_same_sym(file, insn);\n\n\t\tif (prev && prev->type == INSN_ENDBR &&\n\t\t    insn->offset == insn->func->offset + prev->len)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * Find the destination instructions for all jumps.\n */\nstatic int add_jump_destinations(struct objtool_file *file)\n{\n\tstruct instruction *insn, *jump_dest;\n\tstruct reloc *reloc;\n\tstruct section *dest_sec;\n\tunsigned long dest_off;\n\n\tfor_each_insn(file, insn) {\n\t\tif (insn->jump_dest) {\n\t\t\t/*\n\t\t\t * handle_group_alt() may have previously set\n\t\t\t * 'jump_dest' for some alternatives.\n\t\t\t */\n\t\t\tcontinue;\n\t\t}\n\t\tif (!is_static_jump(insn))\n\t\t\tcontinue;\n\n\t\treloc = insn_reloc(file, insn);\n\t\tif (!reloc) {\n\t\t\tdest_sec = insn->sec;\n\t\t\tdest_off = arch_jump_destination(insn);\n\t\t} else if (reloc->sym->type == STT_SECTION) {\n\t\t\tdest_sec = reloc->sym->sec;\n\t\t\tdest_off = arch_dest_reloc_offset(reloc->addend);\n\t\t} else if (reloc->sym->retpoline_thunk) {\n\t\t\tadd_retpoline_call(file, insn);\n\t\t\tcontinue;\n\t\t} else if (insn->func) {\n\t\t\t/*\n\t\t\t * External sibling call or internal sibling call with\n\t\t\t * STT_FUNC reloc.\n\t\t\t */\n\t\t\tadd_call_dest(file, insn, reloc->sym, true);\n\t\t\tcontinue;\n\t\t} else if (reloc->sym->sec->idx) {\n\t\t\tdest_sec = reloc->sym->sec;\n\t\t\tdest_off = reloc->sym->sym.st_value +\n\t\t\t\t   arch_dest_reloc_offset(reloc->addend);\n\t\t} else {\n\t\t\t/* non-func asm code jumping to another file */\n\t\t\tcontinue;\n\t\t}\n\n\t\tjump_dest = find_insn(file, dest_sec, dest_off);\n\t\tif (!jump_dest) {\n\t\t\tWARN_FUNC(\"can't find jump dest instruction at %s+0x%lx\",\n\t\t\t\t  insn->sec, insn->offset, dest_sec->name,\n\t\t\t\t  dest_off);\n\t\t\treturn -1;\n\t\t}\n\n\t\t/*\n\t\t * Cross-function jump.\n\t\t */\n\t\tif (insn->func && jump_dest->func &&\n\t\t    insn->func != jump_dest->func) {\n\n\t\t\t/*\n\t\t\t * For GCC 8+, create parent/child links for any cold\n\t\t\t * subfunctions.  This is _mostly_ redundant with a\n\t\t\t * similar initialization in read_symbols().\n\t\t\t *\n\t\t\t * If a function has aliases, we want the *first* such\n\t\t\t * function in the symbol table to be the subfunction's\n\t\t\t * parent.  In that case we overwrite the\n\t\t\t * initialization done in read_symbols().\n\t\t\t *\n\t\t\t * However this code can't completely replace the\n\t\t\t * read_symbols() code because this doesn't detect the\n\t\t\t * case where the parent function's only reference to a\n\t\t\t * subfunction is through a jump table.\n\t\t\t */\n\t\t\tif (!strstr(insn->func->name, \".cold\") &&\n\t\t\t    strstr(jump_dest->func->name, \".cold\")) {\n\t\t\t\tinsn->func->cfunc = jump_dest->func;\n\t\t\t\tjump_dest->func->pfunc = insn->func;\n\n\t\t\t} else if (!same_function(insn, jump_dest) &&\n\t\t\t\t   is_first_func_insn(file, jump_dest)) {\n\t\t\t\t/*\n\t\t\t\t * Internal sibling call without reloc or with\n\t\t\t\t * STT_SECTION reloc.\n\t\t\t\t */\n\t\t\t\tadd_call_dest(file, insn, jump_dest->func, true);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tinsn->jump_dest = jump_dest;\n\t}\n\n\treturn 0;\n}\n\nstatic struct symbol *find_call_destination(struct section *sec, unsigned long offset)\n{\n\tstruct symbol *call_dest;\n\n\tcall_dest = find_func_by_offset(sec, offset);\n\tif (!call_dest)\n\t\tcall_dest = find_symbol_by_offset(sec, offset);\n\n\treturn call_dest;\n}\n\n/*\n * Find the destination instructions for all calls.\n */\nstatic int add_call_destinations(struct objtool_file *file)\n{\n\tstruct instruction *insn;\n\tunsigned long dest_off;\n\tstruct symbol *dest;\n\tstruct reloc *reloc;\n\n\tfor_each_insn(file, insn) {\n\t\tif (insn->type != INSN_CALL)\n\t\t\tcontinue;\n\n\t\treloc = insn_reloc(file, insn);\n\t\tif (!reloc) {\n\t\t\tdest_off = arch_jump_destination(insn);\n\t\t\tdest = find_call_destination(insn->sec, dest_off);\n\n\t\t\tadd_call_dest(file, insn, dest, false);\n\n\t\t\tif (insn->ignore)\n\t\t\t\tcontinue;\n\n\t\t\tif (!insn->call_dest) {\n\t\t\t\tWARN_FUNC(\"unannotated intra-function call\", insn->sec, insn->offset);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tif (insn->func && insn->call_dest->type != STT_FUNC) {\n\t\t\t\tWARN_FUNC(\"unsupported call to non-function\",\n\t\t\t\t\t  insn->sec, insn->offset);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t} else if (reloc->sym->type == STT_SECTION) {\n\t\t\tdest_off = arch_dest_reloc_offset(reloc->addend);\n\t\t\tdest = find_call_destination(reloc->sym->sec, dest_off);\n\t\t\tif (!dest) {\n\t\t\t\tWARN_FUNC(\"can't find call dest symbol at %s+0x%lx\",\n\t\t\t\t\t  insn->sec, insn->offset,\n\t\t\t\t\t  reloc->sym->sec->name,\n\t\t\t\t\t  dest_off);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tadd_call_dest(file, insn, dest, false);\n\n\t\t} else if (reloc->sym->retpoline_thunk) {\n\t\t\tadd_retpoline_call(file, insn);\n\n\t\t} else\n\t\t\tadd_call_dest(file, insn, reloc->sym, false);\n\t}\n\n\treturn 0;\n}\n\n/*\n * The .alternatives section requires some extra special care over and above\n * other special sections because alternatives are patched in place.\n */\nstatic int handle_group_alt(struct objtool_file *file,\n\t\t\t    struct special_alt *special_alt,\n\t\t\t    struct instruction *orig_insn,\n\t\t\t    struct instruction **new_insn)\n{\n\tstruct instruction *last_orig_insn, *last_new_insn = NULL, *insn, *nop = NULL;\n\tstruct alt_group *orig_alt_group, *new_alt_group;\n\tunsigned long dest_off;\n\n\n\torig_alt_group = malloc(sizeof(*orig_alt_group));\n\tif (!orig_alt_group) {\n\t\tWARN(\"malloc failed\");\n\t\treturn -1;\n\t}\n\torig_alt_group->cfi = calloc(special_alt->orig_len,\n\t\t\t\t     sizeof(struct cfi_state *));\n\tif (!orig_alt_group->cfi) {\n\t\tWARN(\"calloc failed\");\n\t\treturn -1;\n\t}\n\n\tlast_orig_insn = NULL;\n\tinsn = orig_insn;\n\tsec_for_each_insn_from(file, insn) {\n\t\tif (insn->offset >= special_alt->orig_off + special_alt->orig_len)\n\t\t\tbreak;\n\n\t\tinsn->alt_group = orig_alt_group;\n\t\tlast_orig_insn = insn;\n\t}\n\torig_alt_group->orig_group = NULL;\n\torig_alt_group->first_insn = orig_insn;\n\torig_alt_group->last_insn = last_orig_insn;\n\n\n\tnew_alt_group = malloc(sizeof(*new_alt_group));\n\tif (!new_alt_group) {\n\t\tWARN(\"malloc failed\");\n\t\treturn -1;\n\t}\n\n\tif (special_alt->new_len < special_alt->orig_len) {\n\t\t/*\n\t\t * Insert a fake nop at the end to make the replacement\n\t\t * alt_group the same size as the original.  This is needed to\n\t\t * allow propagate_alt_cfi() to do its magic.  When the last\n\t\t * instruction affects the stack, the instruction after it (the\n\t\t * nop) will propagate the new state to the shared CFI array.\n\t\t */\n\t\tnop = malloc(sizeof(*nop));\n\t\tif (!nop) {\n\t\t\tWARN(\"malloc failed\");\n\t\t\treturn -1;\n\t\t}\n\t\tmemset(nop, 0, sizeof(*nop));\n\t\tINIT_LIST_HEAD(&nop->alts);\n\t\tINIT_LIST_HEAD(&nop->stack_ops);\n\n\t\tnop->sec = special_alt->new_sec;\n\t\tnop->offset = special_alt->new_off + special_alt->new_len;\n\t\tnop->len = special_alt->orig_len - special_alt->new_len;\n\t\tnop->type = INSN_NOP;\n\t\tnop->func = orig_insn->func;\n\t\tnop->alt_group = new_alt_group;\n\t\tnop->ignore = orig_insn->ignore_alts;\n\t}\n\n\tif (!special_alt->new_len) {\n\t\t*new_insn = nop;\n\t\tgoto end;\n\t}\n\n\tinsn = *new_insn;\n\tsec_for_each_insn_from(file, insn) {\n\t\tstruct reloc *alt_reloc;\n\n\t\tif (insn->offset >= special_alt->new_off + special_alt->new_len)\n\t\t\tbreak;\n\n\t\tlast_new_insn = insn;\n\n\t\tinsn->ignore = orig_insn->ignore_alts;\n\t\tinsn->func = orig_insn->func;\n\t\tinsn->alt_group = new_alt_group;\n\n\t\t/*\n\t\t * Since alternative replacement code is copy/pasted by the\n\t\t * kernel after applying relocations, generally such code can't\n\t\t * have relative-address relocation references to outside the\n\t\t * .altinstr_replacement section, unless the arch's\n\t\t * alternatives code can adjust the relative offsets\n\t\t * accordingly.\n\t\t */\n\t\talt_reloc = insn_reloc(file, insn);\n\t\tif (alt_reloc &&\n\t\t    !arch_support_alt_relocation(special_alt, insn, alt_reloc)) {\n\n\t\t\tWARN_FUNC(\"unsupported relocation in alternatives section\",\n\t\t\t\t  insn->sec, insn->offset);\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (!is_static_jump(insn))\n\t\t\tcontinue;\n\n\t\tif (!insn->immediate)\n\t\t\tcontinue;\n\n\t\tdest_off = arch_jump_destination(insn);\n\t\tif (dest_off == special_alt->new_off + special_alt->new_len) {\n\t\t\tinsn->jump_dest = next_insn_same_sec(file, last_orig_insn);\n\t\t\tif (!insn->jump_dest) {\n\t\t\t\tWARN_FUNC(\"can't find alternative jump destination\",\n\t\t\t\t\t  insn->sec, insn->offset);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!last_new_insn) {\n\t\tWARN_FUNC(\"can't find last new alternative instruction\",\n\t\t\t  special_alt->new_sec, special_alt->new_off);\n\t\treturn -1;\n\t}\n\n\tif (nop)\n\t\tlist_add(&nop->list, &last_new_insn->list);\nend:\n\tnew_alt_group->orig_group = orig_alt_group;\n\tnew_alt_group->first_insn = *new_insn;\n\tnew_alt_group->last_insn = nop ? : last_new_insn;\n\tnew_alt_group->cfi = orig_alt_group->cfi;\n\treturn 0;\n}\n\n/*\n * A jump table entry can either convert a nop to a jump or a jump to a nop.\n * If the original instruction is a jump, make the alt entry an effective nop\n * by just skipping the original instruction.\n */\nstatic int handle_jump_alt(struct objtool_file *file,\n\t\t\t   struct special_alt *special_alt,\n\t\t\t   struct instruction *orig_insn,\n\t\t\t   struct instruction **new_insn)\n{\n\tif (orig_insn->type != INSN_JUMP_UNCONDITIONAL &&\n\t    orig_insn->type != INSN_NOP) {\n\n\t\tWARN_FUNC(\"unsupported instruction at jump label\",\n\t\t\t  orig_insn->sec, orig_insn->offset);\n\t\treturn -1;\n\t}\n\n\tif (opts.hack_jump_label && special_alt->key_addend & 2) {\n\t\tstruct reloc *reloc = insn_reloc(file, orig_insn);\n\n\t\tif (reloc) {\n\t\t\treloc->type = R_NONE;\n\t\t\telf_write_reloc(file->elf, reloc);\n\t\t}\n\t\telf_write_insn(file->elf, orig_insn->sec,\n\t\t\t       orig_insn->offset, orig_insn->len,\n\t\t\t       arch_nop_insn(orig_insn->len));\n\t\torig_insn->type = INSN_NOP;\n\t}\n\n\tif (orig_insn->type == INSN_NOP) {\n\t\tif (orig_insn->len == 2)\n\t\t\tfile->jl_nop_short++;\n\t\telse\n\t\t\tfile->jl_nop_long++;\n\n\t\treturn 0;\n\t}\n\n\tif (orig_insn->len == 2)\n\t\tfile->jl_short++;\n\telse\n\t\tfile->jl_long++;\n\n\t*new_insn = list_next_entry(orig_insn, list);\n\treturn 0;\n}\n\n/*\n * Read all the special sections which have alternate instructions which can be\n * patched in or redirected to at runtime.  Each instruction having alternate\n * instruction(s) has them added to its insn->alts list, which will be\n * traversed in validate_branch().\n */\nstatic int add_special_section_alts(struct objtool_file *file)\n{\n\tstruct list_head special_alts;\n\tstruct instruction *orig_insn, *new_insn;\n\tstruct special_alt *special_alt, *tmp;\n\tstruct alternative *alt;\n\tint ret;\n\n\tret = special_get_alts(file->elf, &special_alts);\n\tif (ret)\n\t\treturn ret;\n\n\tlist_for_each_entry_safe(special_alt, tmp, &special_alts, list) {\n\n\t\torig_insn = find_insn(file, special_alt->orig_sec,\n\t\t\t\t      special_alt->orig_off);\n\t\tif (!orig_insn) {\n\t\t\tWARN_FUNC(\"special: can't find orig instruction\",\n\t\t\t\t  special_alt->orig_sec, special_alt->orig_off);\n\t\t\tret = -1;\n\t\t\tgoto out;\n\t\t}\n\n\t\tnew_insn = NULL;\n\t\tif (!special_alt->group || special_alt->new_len) {\n\t\t\tnew_insn = find_insn(file, special_alt->new_sec,\n\t\t\t\t\t     special_alt->new_off);\n\t\t\tif (!new_insn) {\n\t\t\t\tWARN_FUNC(\"special: can't find new instruction\",\n\t\t\t\t\t  special_alt->new_sec,\n\t\t\t\t\t  special_alt->new_off);\n\t\t\t\tret = -1;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tif (special_alt->group) {\n\t\t\tif (!special_alt->orig_len) {\n\t\t\t\tWARN_FUNC(\"empty alternative entry\",\n\t\t\t\t\t  orig_insn->sec, orig_insn->offset);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = handle_group_alt(file, special_alt, orig_insn,\n\t\t\t\t\t       &new_insn);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t} else if (special_alt->jump_or_nop) {\n\t\t\tret = handle_jump_alt(file, special_alt, orig_insn,\n\t\t\t\t\t      &new_insn);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\talt = malloc(sizeof(*alt));\n\t\tif (!alt) {\n\t\t\tWARN(\"malloc failed\");\n\t\t\tret = -1;\n\t\t\tgoto out;\n\t\t}\n\n\t\talt->insn = new_insn;\n\t\talt->skip_orig = special_alt->skip_orig;\n\t\torig_insn->ignore_alts |= special_alt->skip_alt;\n\t\tlist_add_tail(&alt->list, &orig_insn->alts);\n\n\t\tlist_del(&special_alt->list);\n\t\tfree(special_alt);\n\t}\n\n\tif (opts.stats) {\n\t\tprintf(\"jl\\\\\\tNOP\\tJMP\\n\");\n\t\tprintf(\"short:\\t%ld\\t%ld\\n\", file->jl_nop_short, file->jl_short);\n\t\tprintf(\"long:\\t%ld\\t%ld\\n\", file->jl_nop_long, file->jl_long);\n\t}\n\nout:\n\treturn ret;\n}\n\nstatic int add_jump_table(struct objtool_file *file, struct instruction *insn,\n\t\t\t    struct reloc *table)\n{\n\tstruct reloc *reloc = table;\n\tstruct instruction *dest_insn;\n\tstruct alternative *alt;\n\tstruct symbol *pfunc = insn->func->pfunc;\n\tunsigned int prev_offset = 0;\n\n\t/*\n\t * Each @reloc is a switch table relocation which points to the target\n\t * instruction.\n\t */\n\tlist_for_each_entry_from(reloc, &table->sec->reloc_list, list) {\n\n\t\t/* Check for the end of the table: */\n\t\tif (reloc != table && reloc->jump_table_start)\n\t\t\tbreak;\n\n\t\t/* Make sure the table entries are consecutive: */\n\t\tif (prev_offset && reloc->offset != prev_offset + 8)\n\t\t\tbreak;\n\n\t\t/* Detect function pointers from contiguous objects: */\n\t\tif (reloc->sym->sec == pfunc->sec &&\n\t\t    reloc->addend == pfunc->offset)\n\t\t\tbreak;\n\n\t\tdest_insn = find_insn(file, reloc->sym->sec, reloc->addend);\n\t\tif (!dest_insn)\n\t\t\tbreak;\n\n\t\t/* Make sure the destination is in the same function: */\n\t\tif (!dest_insn->func || dest_insn->func->pfunc != pfunc)\n\t\t\tbreak;\n\n\t\talt = malloc(sizeof(*alt));\n\t\tif (!alt) {\n\t\t\tWARN(\"malloc failed\");\n\t\t\treturn -1;\n\t\t}\n\n\t\talt->insn = dest_insn;\n\t\tlist_add_tail(&alt->list, &insn->alts);\n\t\tprev_offset = reloc->offset;\n\t}\n\n\tif (!prev_offset) {\n\t\tWARN_FUNC(\"can't find switch jump table\",\n\t\t\t  insn->sec, insn->offset);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\n/*\n * find_jump_table() - Given a dynamic jump, find the switch jump table\n * associated with it.\n */\nstatic struct reloc *find_jump_table(struct objtool_file *file,\n\t\t\t\t      struct symbol *func,\n\t\t\t\t      struct instruction *insn)\n{\n\tstruct reloc *table_reloc;\n\tstruct instruction *dest_insn, *orig_insn = insn;\n\n\t/*\n\t * Backward search using the @first_jump_src links, these help avoid\n\t * much of the 'in between' code. Which avoids us getting confused by\n\t * it.\n\t */\n\tfor (;\n\t     insn && insn->func && insn->func->pfunc == func;\n\t     insn = insn->first_jump_src ?: prev_insn_same_sym(file, insn)) {\n\n\t\tif (insn != orig_insn && insn->type == INSN_JUMP_DYNAMIC)\n\t\t\tbreak;\n\n\t\t/* allow small jumps within the range */\n\t\tif (insn->type == INSN_JUMP_UNCONDITIONAL &&\n\t\t    insn->jump_dest &&\n\t\t    (insn->jump_dest->offset <= insn->offset ||\n\t\t     insn->jump_dest->offset > orig_insn->offset))\n\t\t    break;\n\n\t\ttable_reloc = arch_find_switch_table(file, insn);\n\t\tif (!table_reloc)\n\t\t\tcontinue;\n\t\tdest_insn = find_insn(file, table_reloc->sym->sec, table_reloc->addend);\n\t\tif (!dest_insn || !dest_insn->func || dest_insn->func->pfunc != func)\n\t\t\tcontinue;\n\n\t\treturn table_reloc;\n\t}\n\n\treturn NULL;\n}\n\n/*\n * First pass: Mark the head of each jump table so that in the next pass,\n * we know when a given jump table ends and the next one starts.\n */\nstatic void mark_func_jump_tables(struct objtool_file *file,\n\t\t\t\t    struct symbol *func)\n{\n\tstruct instruction *insn, *last = NULL;\n\tstruct reloc *reloc;\n\n\tfunc_for_each_insn(file, func, insn) {\n\t\tif (!last)\n\t\t\tlast = insn;\n\n\t\t/*\n\t\t * Store back-pointers for unconditional forward jumps such\n\t\t * that find_jump_table() can back-track using those and\n\t\t * avoid some potentially confusing code.\n\t\t */\n\t\tif (insn->type == INSN_JUMP_UNCONDITIONAL && insn->jump_dest &&\n\t\t    insn->offset > last->offset &&\n\t\t    insn->jump_dest->offset > insn->offset &&\n\t\t    !insn->jump_dest->first_jump_src) {\n\n\t\t\tinsn->jump_dest->first_jump_src = insn;\n\t\t\tlast = insn->jump_dest;\n\t\t}\n\n\t\tif (insn->type != INSN_JUMP_DYNAMIC)\n\t\t\tcontinue;\n\n\t\treloc = find_jump_table(file, func, insn);\n\t\tif (reloc) {\n\t\t\treloc->jump_table_start = true;\n\t\t\tinsn->jump_table = reloc;\n\t\t}\n\t}\n}\n\nstatic int add_func_jump_tables(struct objtool_file *file,\n\t\t\t\t  struct symbol *func)\n{\n\tstruct instruction *insn;\n\tint ret;\n\n\tfunc_for_each_insn(file, func, insn) {\n\t\tif (!insn->jump_table)\n\t\t\tcontinue;\n\n\t\tret = add_jump_table(file, insn, insn->jump_table);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n/*\n * For some switch statements, gcc generates a jump table in the .rodata\n * section which contains a list of addresses within the function to jump to.\n * This finds these jump tables and adds them to the insn->alts lists.\n */\nstatic int add_jump_table_alts(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct symbol *func;\n\tint ret;\n\n\tif (!file->rodata)\n\t\treturn 0;\n\n\tfor_each_sec(file, sec) {\n\t\tlist_for_each_entry(func, &sec->symbol_list, list) {\n\t\t\tif (func->type != STT_FUNC)\n\t\t\t\tcontinue;\n\n\t\t\tmark_func_jump_tables(file, func);\n\t\t\tret = add_func_jump_tables(file, func);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void set_func_state(struct cfi_state *state)\n{\n\tstate->cfa = initial_func_cfi.cfa;\n\tmemcpy(&state->regs, &initial_func_cfi.regs,\n\t       CFI_NUM_REGS * sizeof(struct cfi_reg));\n\tstate->stack_size = initial_func_cfi.cfa.offset;\n}\n\nstatic int read_unwind_hints(struct objtool_file *file)\n{\n\tstruct cfi_state cfi = init_cfi;\n\tstruct section *sec, *relocsec;\n\tstruct unwind_hint *hint;\n\tstruct instruction *insn;\n\tstruct reloc *reloc;\n\tint i;\n\n\tsec = find_section_by_name(file->elf, \".discard.unwind_hints\");\n\tif (!sec)\n\t\treturn 0;\n\n\trelocsec = sec->reloc;\n\tif (!relocsec) {\n\t\tWARN(\"missing .rela.discard.unwind_hints section\");\n\t\treturn -1;\n\t}\n\n\tif (sec->sh.sh_size % sizeof(struct unwind_hint)) {\n\t\tWARN(\"struct unwind_hint size mismatch\");\n\t\treturn -1;\n\t}\n\n\tfile->hints = true;\n\n\tfor (i = 0; i < sec->sh.sh_size / sizeof(struct unwind_hint); i++) {\n\t\thint = (struct unwind_hint *)sec->data->d_buf + i;\n\n\t\treloc = find_reloc_by_dest(file->elf, sec, i * sizeof(*hint));\n\t\tif (!reloc) {\n\t\t\tWARN(\"can't find reloc for unwind_hints[%d]\", i);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn = find_insn(file, reloc->sym->sec, reloc->addend);\n\t\tif (!insn) {\n\t\t\tWARN(\"can't find insn for unwind_hints[%d]\", i);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn->hint = true;\n\n\t\tif (opts.ibt && hint->type == UNWIND_HINT_TYPE_REGS_PARTIAL) {\n\t\t\tstruct symbol *sym = find_symbol_by_offset(insn->sec, insn->offset);\n\n\t\t\tif (sym && sym->bind == STB_GLOBAL &&\n\t\t\t    insn->type != INSN_ENDBR && !insn->noendbr) {\n\t\t\t\tWARN_FUNC(\"UNWIND_HINT_IRET_REGS without ENDBR\",\n\t\t\t\t\t  insn->sec, insn->offset);\n\t\t\t}\n\t\t}\n\n\t\tif (hint->type == UNWIND_HINT_TYPE_FUNC) {\n\t\t\tinsn->cfi = &func_cfi;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->cfi)\n\t\t\tcfi = *(insn->cfi);\n\n\t\tif (arch_decode_hint_reg(hint->sp_reg, &cfi.cfa.base)) {\n\t\t\tWARN_FUNC(\"unsupported unwind_hint sp base reg %d\",\n\t\t\t\t  insn->sec, insn->offset, hint->sp_reg);\n\t\t\treturn -1;\n\t\t}\n\n\t\tcfi.cfa.offset = bswap_if_needed(hint->sp_offset);\n\t\tcfi.type = hint->type;\n\t\tcfi.end = hint->end;\n\n\t\tinsn->cfi = cfi_hash_find_or_add(&cfi);\n\t}\n\n\treturn 0;\n}\n\nstatic int read_noendbr_hints(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct instruction *insn;\n\tstruct reloc *reloc;\n\n\tsec = find_section_by_name(file->elf, \".rela.discard.noendbr\");\n\tif (!sec)\n\t\treturn 0;\n\n\tlist_for_each_entry(reloc, &sec->reloc_list, list) {\n\t\tinsn = find_insn(file, reloc->sym->sec, reloc->sym->offset + reloc->addend);\n\t\tif (!insn) {\n\t\t\tWARN(\"bad .discard.noendbr entry\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (insn->type == INSN_ENDBR)\n\t\t\tWARN_FUNC(\"ANNOTATE_NOENDBR on ENDBR\", insn->sec, insn->offset);\n\n\t\tinsn->noendbr = 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int read_retpoline_hints(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct instruction *insn;\n\tstruct reloc *reloc;\n\n\tsec = find_section_by_name(file->elf, \".rela.discard.retpoline_safe\");\n\tif (!sec)\n\t\treturn 0;\n\n\tlist_for_each_entry(reloc, &sec->reloc_list, list) {\n\t\tif (reloc->sym->type != STT_SECTION) {\n\t\t\tWARN(\"unexpected relocation symbol type in %s\", sec->name);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn = find_insn(file, reloc->sym->sec, reloc->addend);\n\t\tif (!insn) {\n\t\t\tWARN(\"bad .discard.retpoline_safe entry\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (insn->type != INSN_JUMP_DYNAMIC &&\n\t\t    insn->type != INSN_CALL_DYNAMIC) {\n\t\t\tWARN_FUNC(\"retpoline_safe hint not an indirect jump/call\",\n\t\t\t\t  insn->sec, insn->offset);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn->retpoline_safe = true;\n\t}\n\n\treturn 0;\n}\n\nstatic int read_instr_hints(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct instruction *insn;\n\tstruct reloc *reloc;\n\n\tsec = find_section_by_name(file->elf, \".rela.discard.instr_end\");\n\tif (!sec)\n\t\treturn 0;\n\n\tlist_for_each_entry(reloc, &sec->reloc_list, list) {\n\t\tif (reloc->sym->type != STT_SECTION) {\n\t\t\tWARN(\"unexpected relocation symbol type in %s\", sec->name);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn = find_insn(file, reloc->sym->sec, reloc->addend);\n\t\tif (!insn) {\n\t\t\tWARN(\"bad .discard.instr_end entry\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn->instr--;\n\t}\n\n\tsec = find_section_by_name(file->elf, \".rela.discard.instr_begin\");\n\tif (!sec)\n\t\treturn 0;\n\n\tlist_for_each_entry(reloc, &sec->reloc_list, list) {\n\t\tif (reloc->sym->type != STT_SECTION) {\n\t\t\tWARN(\"unexpected relocation symbol type in %s\", sec->name);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn = find_insn(file, reloc->sym->sec, reloc->addend);\n\t\tif (!insn) {\n\t\t\tWARN(\"bad .discard.instr_begin entry\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn->instr++;\n\t}\n\n\treturn 0;\n}\n\nstatic int read_intra_function_calls(struct objtool_file *file)\n{\n\tstruct instruction *insn;\n\tstruct section *sec;\n\tstruct reloc *reloc;\n\n\tsec = find_section_by_name(file->elf, \".rela.discard.intra_function_calls\");\n\tif (!sec)\n\t\treturn 0;\n\n\tlist_for_each_entry(reloc, &sec->reloc_list, list) {\n\t\tunsigned long dest_off;\n\n\t\tif (reloc->sym->type != STT_SECTION) {\n\t\t\tWARN(\"unexpected relocation symbol type in %s\",\n\t\t\t     sec->name);\n\t\t\treturn -1;\n\t\t}\n\n\t\tinsn = find_insn(file, reloc->sym->sec, reloc->addend);\n\t\tif (!insn) {\n\t\t\tWARN(\"bad .discard.intra_function_call entry\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (insn->type != INSN_CALL) {\n\t\t\tWARN_FUNC(\"intra_function_call not a direct call\",\n\t\t\t\t  insn->sec, insn->offset);\n\t\t\treturn -1;\n\t\t}\n\n\t\t/*\n\t\t * Treat intra-function CALLs as JMPs, but with a stack_op.\n\t\t * See add_call_destinations(), which strips stack_ops from\n\t\t * normal CALLs.\n\t\t */\n\t\tinsn->type = INSN_JUMP_UNCONDITIONAL;\n\n\t\tdest_off = insn->offset + insn->len + insn->immediate;\n\t\tinsn->jump_dest = find_insn(file, insn->sec, dest_off);\n\t\tif (!insn->jump_dest) {\n\t\t\tWARN_FUNC(\"can't find call dest at %s+0x%lx\",\n\t\t\t\t  insn->sec, insn->offset,\n\t\t\t\t  insn->sec->name, dest_off);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * Return true if name matches an instrumentation function, where calls to that\n * function from noinstr code can safely be removed, but compilers won't do so.\n */\nstatic bool is_profiling_func(const char *name)\n{\n\t/*\n\t * Many compilers cannot disable KCOV with a function attribute.\n\t */\n\tif (!strncmp(name, \"__sanitizer_cov_\", 16))\n\t\treturn true;\n\n\t/*\n\t * Some compilers currently do not remove __tsan_func_entry/exit nor\n\t * __tsan_atomic_signal_fence (used for barrier instrumentation) with\n\t * the __no_sanitize_thread attribute, remove them. Once the kernel's\n\t * minimum Clang version is 14.0, this can be removed.\n\t */\n\tif (!strncmp(name, \"__tsan_func_\", 12) ||\n\t    !strcmp(name, \"__tsan_atomic_signal_fence\"))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int classify_symbols(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct symbol *func;\n\n\tfor_each_sec(file, sec) {\n\t\tlist_for_each_entry(func, &sec->symbol_list, list) {\n\t\t\tif (func->bind != STB_GLOBAL)\n\t\t\t\tcontinue;\n\n\t\t\tif (!strncmp(func->name, STATIC_CALL_TRAMP_PREFIX_STR,\n\t\t\t\t     strlen(STATIC_CALL_TRAMP_PREFIX_STR)))\n\t\t\t\tfunc->static_call_tramp = true;\n\n\t\t\tif (arch_is_retpoline(func))\n\t\t\t\tfunc->retpoline_thunk = true;\n\n\t\t\tif (!strcmp(func->name, \"__fentry__\"))\n\t\t\t\tfunc->fentry = true;\n\n\t\t\tif (is_profiling_func(func->name))\n\t\t\t\tfunc->profiling_func = true;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void mark_rodata(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tbool found = false;\n\n\t/*\n\t * Search for the following rodata sections, each of which can\n\t * potentially contain jump tables:\n\t *\n\t * - .rodata: can contain GCC switch tables\n\t * - .rodata.<func>: same, if -fdata-sections is being used\n\t * - .rodata..c_jump_table: contains C annotated jump tables\n\t *\n\t * .rodata.str1.* sections are ignored; they don't contain jump tables.\n\t */\n\tfor_each_sec(file, sec) {\n\t\tif (!strncmp(sec->name, \".rodata\", 7) &&\n\t\t    !strstr(sec->name, \".str1.\")) {\n\t\t\tsec->rodata = true;\n\t\t\tfound = true;\n\t\t}\n\t}\n\n\tfile->rodata = found;\n}\n\nstatic int decode_sections(struct objtool_file *file)\n{\n\tint ret;\n\n\tmark_rodata(file);\n\n\tret = init_pv_ops(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = decode_instructions(file);\n\tif (ret)\n\t\treturn ret;\n\n\tadd_ignores(file);\n\tadd_uaccess_safe(file);\n\n\tret = add_ignore_alternatives(file);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Must be before read_unwind_hints() since that needs insn->noendbr.\n\t */\n\tret = read_noendbr_hints(file);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Must be before add_{jump_call}_destination.\n\t */\n\tret = classify_symbols(file);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Must be before add_jump_destinations(), which depends on 'func'\n\t * being set for alternatives, to enable proper sibling call detection.\n\t */\n\tret = add_special_section_alts(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = add_jump_destinations(file);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Must be before add_call_destination(); it changes INSN_CALL to\n\t * INSN_JUMP.\n\t */\n\tret = read_intra_function_calls(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = add_call_destinations(file);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Must be after add_call_destinations() such that it can override\n\t * dead_end_function() marks.\n\t */\n\tret = add_dead_ends(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = add_jump_table_alts(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = read_unwind_hints(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = read_retpoline_hints(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = read_instr_hints(file);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nstatic bool is_fentry_call(struct instruction *insn)\n{\n\tif (insn->type == INSN_CALL &&\n\t    insn->call_dest &&\n\t    insn->call_dest->fentry)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool has_modified_stack_frame(struct instruction *insn, struct insn_state *state)\n{\n\tstruct cfi_state *cfi = &state->cfi;\n\tint i;\n\n\tif (cfi->cfa.base != initial_func_cfi.cfa.base || cfi->drap)\n\t\treturn true;\n\n\tif (cfi->cfa.offset != initial_func_cfi.cfa.offset)\n\t\treturn true;\n\n\tif (cfi->stack_size != initial_func_cfi.cfa.offset)\n\t\treturn true;\n\n\tfor (i = 0; i < CFI_NUM_REGS; i++) {\n\t\tif (cfi->regs[i].base != initial_func_cfi.regs[i].base ||\n\t\t    cfi->regs[i].offset != initial_func_cfi.regs[i].offset)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool check_reg_frame_pos(const struct cfi_reg *reg,\n\t\t\t\tint expected_offset)\n{\n\treturn reg->base == CFI_CFA &&\n\t       reg->offset == expected_offset;\n}\n\nstatic bool has_valid_stack_frame(struct insn_state *state)\n{\n\tstruct cfi_state *cfi = &state->cfi;\n\n\tif (cfi->cfa.base == CFI_BP &&\n\t    check_reg_frame_pos(&cfi->regs[CFI_BP], -cfi->cfa.offset) &&\n\t    check_reg_frame_pos(&cfi->regs[CFI_RA], -cfi->cfa.offset + 8))\n\t\treturn true;\n\n\tif (cfi->drap && cfi->regs[CFI_BP].base == CFI_BP)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int update_cfi_state_regs(struct instruction *insn,\n\t\t\t\t  struct cfi_state *cfi,\n\t\t\t\t  struct stack_op *op)\n{\n\tstruct cfi_reg *cfa = &cfi->cfa;\n\n\tif (cfa->base != CFI_SP && cfa->base != CFI_SP_INDIRECT)\n\t\treturn 0;\n\n\t/* push */\n\tif (op->dest.type == OP_DEST_PUSH || op->dest.type == OP_DEST_PUSHF)\n\t\tcfa->offset += 8;\n\n\t/* pop */\n\tif (op->src.type == OP_SRC_POP || op->src.type == OP_SRC_POPF)\n\t\tcfa->offset -= 8;\n\n\t/* add immediate to sp */\n\tif (op->dest.type == OP_DEST_REG && op->src.type == OP_SRC_ADD &&\n\t    op->dest.reg == CFI_SP && op->src.reg == CFI_SP)\n\t\tcfa->offset -= op->src.offset;\n\n\treturn 0;\n}\n\nstatic void save_reg(struct cfi_state *cfi, unsigned char reg, int base, int offset)\n{\n\tif (arch_callee_saved_reg(reg) &&\n\t    cfi->regs[reg].base == CFI_UNDEFINED) {\n\t\tcfi->regs[reg].base = base;\n\t\tcfi->regs[reg].offset = offset;\n\t}\n}\n\nstatic void restore_reg(struct cfi_state *cfi, unsigned char reg)\n{\n\tcfi->regs[reg].base = initial_func_cfi.regs[reg].base;\n\tcfi->regs[reg].offset = initial_func_cfi.regs[reg].offset;\n}\n\n/*\n * A note about DRAP stack alignment:\n *\n * GCC has the concept of a DRAP register, which is used to help keep track of\n * the stack pointer when aligning the stack.  r10 or r13 is used as the DRAP\n * register.  The typical DRAP pattern is:\n *\n *   4c 8d 54 24 08\t\tlea    0x8(%rsp),%r10\n *   48 83 e4 c0\t\tand    $0xffffffffffffffc0,%rsp\n *   41 ff 72 f8\t\tpushq  -0x8(%r10)\n *   55\t\t\t\tpush   %rbp\n *   48 89 e5\t\t\tmov    %rsp,%rbp\n *\t\t\t\t(more pushes)\n *   41 52\t\t\tpush   %r10\n *\t\t\t\t...\n *   41 5a\t\t\tpop    %r10\n *\t\t\t\t(more pops)\n *   5d\t\t\t\tpop    %rbp\n *   49 8d 62 f8\t\tlea    -0x8(%r10),%rsp\n *   c3\t\t\t\tretq\n *\n * There are some variations in the epilogues, like:\n *\n *   5b\t\t\t\tpop    %rbx\n *   41 5a\t\t\tpop    %r10\n *   41 5c\t\t\tpop    %r12\n *   41 5d\t\t\tpop    %r13\n *   41 5e\t\t\tpop    %r14\n *   c9\t\t\t\tleaveq\n *   49 8d 62 f8\t\tlea    -0x8(%r10),%rsp\n *   c3\t\t\t\tretq\n *\n * and:\n *\n *   4c 8b 55 e8\t\tmov    -0x18(%rbp),%r10\n *   48 8b 5d e0\t\tmov    -0x20(%rbp),%rbx\n *   4c 8b 65 f0\t\tmov    -0x10(%rbp),%r12\n *   4c 8b 6d f8\t\tmov    -0x8(%rbp),%r13\n *   c9\t\t\t\tleaveq\n *   49 8d 62 f8\t\tlea    -0x8(%r10),%rsp\n *   c3\t\t\t\tretq\n *\n * Sometimes r13 is used as the DRAP register, in which case it's saved and\n * restored beforehand:\n *\n *   41 55\t\t\tpush   %r13\n *   4c 8d 6c 24 10\t\tlea    0x10(%rsp),%r13\n *   48 83 e4 f0\t\tand    $0xfffffffffffffff0,%rsp\n *\t\t\t\t...\n *   49 8d 65 f0\t\tlea    -0x10(%r13),%rsp\n *   41 5d\t\t\tpop    %r13\n *   c3\t\t\t\tretq\n */\nstatic int update_cfi_state(struct instruction *insn,\n\t\t\t    struct instruction *next_insn,\n\t\t\t    struct cfi_state *cfi, struct stack_op *op)\n{\n\tstruct cfi_reg *cfa = &cfi->cfa;\n\tstruct cfi_reg *regs = cfi->regs;\n\n\t/* stack operations don't make sense with an undefined CFA */\n\tif (cfa->base == CFI_UNDEFINED) {\n\t\tif (insn->func) {\n\t\t\tWARN_FUNC(\"undefined stack state\", insn->sec, insn->offset);\n\t\t\treturn -1;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (cfi->type == UNWIND_HINT_TYPE_REGS ||\n\t    cfi->type == UNWIND_HINT_TYPE_REGS_PARTIAL)\n\t\treturn update_cfi_state_regs(insn, cfi, op);\n\n\tswitch (op->dest.type) {\n\n\tcase OP_DEST_REG:\n\t\tswitch (op->src.type) {\n\n\t\tcase OP_SRC_REG:\n\t\t\tif (op->src.reg == CFI_SP && op->dest.reg == CFI_BP &&\n\t\t\t    cfa->base == CFI_SP &&\n\t\t\t    check_reg_frame_pos(&regs[CFI_BP], -cfa->offset)) {\n\n\t\t\t\t/* mov %rsp, %rbp */\n\t\t\t\tcfa->base = op->dest.reg;\n\t\t\t\tcfi->bp_scratch = false;\n\t\t\t}\n\n\t\t\telse if (op->src.reg == CFI_SP &&\n\t\t\t\t op->dest.reg == CFI_BP && cfi->drap) {\n\n\t\t\t\t/* drap: mov %rsp, %rbp */\n\t\t\t\tregs[CFI_BP].base = CFI_BP;\n\t\t\t\tregs[CFI_BP].offset = -cfi->stack_size;\n\t\t\t\tcfi->bp_scratch = false;\n\t\t\t}\n\n\t\t\telse if (op->src.reg == CFI_SP && cfa->base == CFI_SP) {\n\n\t\t\t\t/*\n\t\t\t\t * mov %rsp, %reg\n\t\t\t\t *\n\t\t\t\t * This is needed for the rare case where GCC\n\t\t\t\t * does:\n\t\t\t\t *\n\t\t\t\t *   mov    %rsp, %rax\n\t\t\t\t *   ...\n\t\t\t\t *   mov    %rax, %rsp\n\t\t\t\t */\n\t\t\t\tcfi->vals[op->dest.reg].base = CFI_CFA;\n\t\t\t\tcfi->vals[op->dest.reg].offset = -cfi->stack_size;\n\t\t\t}\n\n\t\t\telse if (op->src.reg == CFI_BP && op->dest.reg == CFI_SP &&\n\t\t\t\t (cfa->base == CFI_BP || cfa->base == cfi->drap_reg)) {\n\n\t\t\t\t/*\n\t\t\t\t * mov %rbp, %rsp\n\t\t\t\t *\n\t\t\t\t * Restore the original stack pointer (Clang).\n\t\t\t\t */\n\t\t\t\tcfi->stack_size = -cfi->regs[CFI_BP].offset;\n\t\t\t}\n\n\t\t\telse if (op->dest.reg == cfa->base) {\n\n\t\t\t\t/* mov %reg, %rsp */\n\t\t\t\tif (cfa->base == CFI_SP &&\n\t\t\t\t    cfi->vals[op->src.reg].base == CFI_CFA) {\n\n\t\t\t\t\t/*\n\t\t\t\t\t * This is needed for the rare case\n\t\t\t\t\t * where GCC does something dumb like:\n\t\t\t\t\t *\n\t\t\t\t\t *   lea    0x8(%rsp), %rcx\n\t\t\t\t\t *   ...\n\t\t\t\t\t *   mov    %rcx, %rsp\n\t\t\t\t\t */\n\t\t\t\t\tcfa->offset = -cfi->vals[op->src.reg].offset;\n\t\t\t\t\tcfi->stack_size = cfa->offset;\n\n\t\t\t\t} else if (cfa->base == CFI_SP &&\n\t\t\t\t\t   cfi->vals[op->src.reg].base == CFI_SP_INDIRECT &&\n\t\t\t\t\t   cfi->vals[op->src.reg].offset == cfa->offset) {\n\n\t\t\t\t\t/*\n\t\t\t\t\t * Stack swizzle:\n\t\t\t\t\t *\n\t\t\t\t\t * 1: mov %rsp, (%[tos])\n\t\t\t\t\t * 2: mov %[tos], %rsp\n\t\t\t\t\t *    ...\n\t\t\t\t\t * 3: pop %rsp\n\t\t\t\t\t *\n\t\t\t\t\t * Where:\n\t\t\t\t\t *\n\t\t\t\t\t * 1 - places a pointer to the previous\n\t\t\t\t\t *     stack at the Top-of-Stack of the\n\t\t\t\t\t *     new stack.\n\t\t\t\t\t *\n\t\t\t\t\t * 2 - switches to the new stack.\n\t\t\t\t\t *\n\t\t\t\t\t * 3 - pops the Top-of-Stack to restore\n\t\t\t\t\t *     the original stack.\n\t\t\t\t\t *\n\t\t\t\t\t * Note: we set base to SP_INDIRECT\n\t\t\t\t\t * here and preserve offset. Therefore\n\t\t\t\t\t * when the unwinder reaches ToS it\n\t\t\t\t\t * will dereference SP and then add the\n\t\t\t\t\t * offset to find the next frame, IOW:\n\t\t\t\t\t * (%rsp) + offset.\n\t\t\t\t\t */\n\t\t\t\t\tcfa->base = CFI_SP_INDIRECT;\n\n\t\t\t\t} else {\n\t\t\t\t\tcfa->base = CFI_UNDEFINED;\n\t\t\t\t\tcfa->offset = 0;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\telse if (op->dest.reg == CFI_SP &&\n\t\t\t\t cfi->vals[op->src.reg].base == CFI_SP_INDIRECT &&\n\t\t\t\t cfi->vals[op->src.reg].offset == cfa->offset) {\n\n\t\t\t\t/*\n\t\t\t\t * The same stack swizzle case 2) as above. But\n\t\t\t\t * because we can't change cfa->base, case 3)\n\t\t\t\t * will become a regular POP. Pretend we're a\n\t\t\t\t * PUSH so things don't go unbalanced.\n\t\t\t\t */\n\t\t\t\tcfi->stack_size += 8;\n\t\t\t}\n\n\n\t\t\tbreak;\n\n\t\tcase OP_SRC_ADD:\n\t\t\tif (op->dest.reg == CFI_SP && op->src.reg == CFI_SP) {\n\n\t\t\t\t/* add imm, %rsp */\n\t\t\t\tcfi->stack_size -= op->src.offset;\n\t\t\t\tif (cfa->base == CFI_SP)\n\t\t\t\t\tcfa->offset -= op->src.offset;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (op->dest.reg == CFI_SP && op->src.reg == CFI_BP) {\n\n\t\t\t\t/* lea disp(%rbp), %rsp */\n\t\t\t\tcfi->stack_size = -(op->src.offset + regs[CFI_BP].offset);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (!cfi->drap && op->src.reg == CFI_SP &&\n\t\t\t    op->dest.reg == CFI_BP && cfa->base == CFI_SP &&\n\t\t\t    check_reg_frame_pos(&regs[CFI_BP], -cfa->offset + op->src.offset)) {\n\n\t\t\t\t/* lea disp(%rsp), %rbp */\n\t\t\t\tcfa->base = CFI_BP;\n\t\t\t\tcfa->offset -= op->src.offset;\n\t\t\t\tcfi->bp_scratch = false;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (op->src.reg == CFI_SP && cfa->base == CFI_SP) {\n\n\t\t\t\t/* drap: lea disp(%rsp), %drap */\n\t\t\t\tcfi->drap_reg = op->dest.reg;\n\n\t\t\t\t/*\n\t\t\t\t * lea disp(%rsp), %reg\n\t\t\t\t *\n\t\t\t\t * This is needed for the rare case where GCC\n\t\t\t\t * does something dumb like:\n\t\t\t\t *\n\t\t\t\t *   lea    0x8(%rsp), %rcx\n\t\t\t\t *   ...\n\t\t\t\t *   mov    %rcx, %rsp\n\t\t\t\t */\n\t\t\t\tcfi->vals[op->dest.reg].base = CFI_CFA;\n\t\t\t\tcfi->vals[op->dest.reg].offset = \\\n\t\t\t\t\t-cfi->stack_size + op->src.offset;\n\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (cfi->drap && op->dest.reg == CFI_SP &&\n\t\t\t    op->src.reg == cfi->drap_reg) {\n\n\t\t\t\t /* drap: lea disp(%drap), %rsp */\n\t\t\t\tcfa->base = CFI_SP;\n\t\t\t\tcfa->offset = cfi->stack_size = -op->src.offset;\n\t\t\t\tcfi->drap_reg = CFI_UNDEFINED;\n\t\t\t\tcfi->drap = false;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (op->dest.reg == cfi->cfa.base && !(next_insn && next_insn->hint)) {\n\t\t\t\tWARN_FUNC(\"unsupported stack register modification\",\n\t\t\t\t\t  insn->sec, insn->offset);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tbreak;\n\n\t\tcase OP_SRC_AND:\n\t\t\tif (op->dest.reg != CFI_SP ||\n\t\t\t    (cfi->drap_reg != CFI_UNDEFINED && cfa->base != CFI_SP) ||\n\t\t\t    (cfi->drap_reg == CFI_UNDEFINED && cfa->base != CFI_BP)) {\n\t\t\t\tWARN_FUNC(\"unsupported stack pointer realignment\",\n\t\t\t\t\t  insn->sec, insn->offset);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tif (cfi->drap_reg != CFI_UNDEFINED) {\n\t\t\t\t/* drap: and imm, %rsp */\n\t\t\t\tcfa->base = cfi->drap_reg;\n\t\t\t\tcfa->offset = cfi->stack_size = 0;\n\t\t\t\tcfi->drap = true;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Older versions of GCC (4.8ish) realign the stack\n\t\t\t * without DRAP, with a frame pointer.\n\t\t\t */\n\n\t\t\tbreak;\n\n\t\tcase OP_SRC_POP:\n\t\tcase OP_SRC_POPF:\n\t\t\tif (op->dest.reg == CFI_SP && cfa->base == CFI_SP_INDIRECT) {\n\n\t\t\t\t/* pop %rsp; # restore from a stack swizzle */\n\t\t\t\tcfa->base = CFI_SP;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (!cfi->drap && op->dest.reg == cfa->base) {\n\n\t\t\t\t/* pop %rbp */\n\t\t\t\tcfa->base = CFI_SP;\n\t\t\t}\n\n\t\t\tif (cfi->drap && cfa->base == CFI_BP_INDIRECT &&\n\t\t\t    op->dest.reg == cfi->drap_reg &&\n\t\t\t    cfi->drap_offset == -cfi->stack_size) {\n\n\t\t\t\t/* drap: pop %drap */\n\t\t\t\tcfa->base = cfi->drap_reg;\n\t\t\t\tcfa->offset = 0;\n\t\t\t\tcfi->drap_offset = -1;\n\n\t\t\t} else if (cfi->stack_size == -regs[op->dest.reg].offset) {\n\n\t\t\t\t/* pop %reg */\n\t\t\t\trestore_reg(cfi, op->dest.reg);\n\t\t\t}\n\n\t\t\tcfi->stack_size -= 8;\n\t\t\tif (cfa->base == CFI_SP)\n\t\t\t\tcfa->offset -= 8;\n\n\t\t\tbreak;\n\n\t\tcase OP_SRC_REG_INDIRECT:\n\t\t\tif (!cfi->drap && op->dest.reg == cfa->base &&\n\t\t\t    op->dest.reg == CFI_BP) {\n\n\t\t\t\t/* mov disp(%rsp), %rbp */\n\t\t\t\tcfa->base = CFI_SP;\n\t\t\t\tcfa->offset = cfi->stack_size;\n\t\t\t}\n\n\t\t\tif (cfi->drap && op->src.reg == CFI_BP &&\n\t\t\t    op->src.offset == cfi->drap_offset) {\n\n\t\t\t\t/* drap: mov disp(%rbp), %drap */\n\t\t\t\tcfa->base = cfi->drap_reg;\n\t\t\t\tcfa->offset = 0;\n\t\t\t\tcfi->drap_offset = -1;\n\t\t\t}\n\n\t\t\tif (cfi->drap && op->src.reg == CFI_BP &&\n\t\t\t    op->src.offset == regs[op->dest.reg].offset) {\n\n\t\t\t\t/* drap: mov disp(%rbp), %reg */\n\t\t\t\trestore_reg(cfi, op->dest.reg);\n\n\t\t\t} else if (op->src.reg == cfa->base &&\n\t\t\t    op->src.offset == regs[op->dest.reg].offset + cfa->offset) {\n\n\t\t\t\t/* mov disp(%rbp), %reg */\n\t\t\t\t/* mov disp(%rsp), %reg */\n\t\t\t\trestore_reg(cfi, op->dest.reg);\n\n\t\t\t} else if (op->src.reg == CFI_SP &&\n\t\t\t\t   op->src.offset == regs[op->dest.reg].offset + cfi->stack_size) {\n\n\t\t\t\t/* mov disp(%rsp), %reg */\n\t\t\t\trestore_reg(cfi, op->dest.reg);\n\t\t\t}\n\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tWARN_FUNC(\"unknown stack-related instruction\",\n\t\t\t\t  insn->sec, insn->offset);\n\t\t\treturn -1;\n\t\t}\n\n\t\tbreak;\n\n\tcase OP_DEST_PUSH:\n\tcase OP_DEST_PUSHF:\n\t\tcfi->stack_size += 8;\n\t\tif (cfa->base == CFI_SP)\n\t\t\tcfa->offset += 8;\n\n\t\tif (op->src.type != OP_SRC_REG)\n\t\t\tbreak;\n\n\t\tif (cfi->drap) {\n\t\t\tif (op->src.reg == cfa->base && op->src.reg == cfi->drap_reg) {\n\n\t\t\t\t/* drap: push %drap */\n\t\t\t\tcfa->base = CFI_BP_INDIRECT;\n\t\t\t\tcfa->offset = -cfi->stack_size;\n\n\t\t\t\t/* save drap so we know when to restore it */\n\t\t\t\tcfi->drap_offset = -cfi->stack_size;\n\n\t\t\t} else if (op->src.reg == CFI_BP && cfa->base == cfi->drap_reg) {\n\n\t\t\t\t/* drap: push %rbp */\n\t\t\t\tcfi->stack_size = 0;\n\n\t\t\t} else {\n\n\t\t\t\t/* drap: push %reg */\n\t\t\t\tsave_reg(cfi, op->src.reg, CFI_BP, -cfi->stack_size);\n\t\t\t}\n\n\t\t} else {\n\n\t\t\t/* push %reg */\n\t\t\tsave_reg(cfi, op->src.reg, CFI_CFA, -cfi->stack_size);\n\t\t}\n\n\t\t/* detect when asm code uses rbp as a scratch register */\n\t\tif (opts.stackval && insn->func && op->src.reg == CFI_BP &&\n\t\t    cfa->base != CFI_BP)\n\t\t\tcfi->bp_scratch = true;\n\t\tbreak;\n\n\tcase OP_DEST_REG_INDIRECT:\n\n\t\tif (cfi->drap) {\n\t\t\tif (op->src.reg == cfa->base && op->src.reg == cfi->drap_reg) {\n\n\t\t\t\t/* drap: mov %drap, disp(%rbp) */\n\t\t\t\tcfa->base = CFI_BP_INDIRECT;\n\t\t\t\tcfa->offset = op->dest.offset;\n\n\t\t\t\t/* save drap offset so we know when to restore it */\n\t\t\t\tcfi->drap_offset = op->dest.offset;\n\t\t\t} else {\n\n\t\t\t\t/* drap: mov reg, disp(%rbp) */\n\t\t\t\tsave_reg(cfi, op->src.reg, CFI_BP, op->dest.offset);\n\t\t\t}\n\n\t\t} else if (op->dest.reg == cfa->base) {\n\n\t\t\t/* mov reg, disp(%rbp) */\n\t\t\t/* mov reg, disp(%rsp) */\n\t\t\tsave_reg(cfi, op->src.reg, CFI_CFA,\n\t\t\t\t op->dest.offset - cfi->cfa.offset);\n\n\t\t} else if (op->dest.reg == CFI_SP) {\n\n\t\t\t/* mov reg, disp(%rsp) */\n\t\t\tsave_reg(cfi, op->src.reg, CFI_CFA,\n\t\t\t\t op->dest.offset - cfi->stack_size);\n\n\t\t} else if (op->src.reg == CFI_SP && op->dest.offset == 0) {\n\n\t\t\t/* mov %rsp, (%reg); # setup a stack swizzle. */\n\t\t\tcfi->vals[op->dest.reg].base = CFI_SP_INDIRECT;\n\t\t\tcfi->vals[op->dest.reg].offset = cfa->offset;\n\t\t}\n\n\t\tbreak;\n\n\tcase OP_DEST_MEM:\n\t\tif (op->src.type != OP_SRC_POP && op->src.type != OP_SRC_POPF) {\n\t\t\tWARN_FUNC(\"unknown stack-related memory operation\",\n\t\t\t\t  insn->sec, insn->offset);\n\t\t\treturn -1;\n\t\t}\n\n\t\t/* pop mem */\n\t\tcfi->stack_size -= 8;\n\t\tif (cfa->base == CFI_SP)\n\t\t\tcfa->offset -= 8;\n\n\t\tbreak;\n\n\tdefault:\n\t\tWARN_FUNC(\"unknown stack-related instruction\",\n\t\t\t  insn->sec, insn->offset);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\n/*\n * The stack layouts of alternatives instructions can sometimes diverge when\n * they have stack modifications.  That's fine as long as the potential stack\n * layouts don't conflict at any given potential instruction boundary.\n *\n * Flatten the CFIs of the different alternative code streams (both original\n * and replacement) into a single shared CFI array which can be used to detect\n * conflicts and nicely feed a linear array of ORC entries to the unwinder.\n */\nstatic int propagate_alt_cfi(struct objtool_file *file, struct instruction *insn)\n{\n\tstruct cfi_state **alt_cfi;\n\tint group_off;\n\n\tif (!insn->alt_group)\n\t\treturn 0;\n\n\tif (!insn->cfi) {\n\t\tWARN(\"CFI missing\");\n\t\treturn -1;\n\t}\n\n\talt_cfi = insn->alt_group->cfi;\n\tgroup_off = insn->offset - insn->alt_group->first_insn->offset;\n\n\tif (!alt_cfi[group_off]) {\n\t\talt_cfi[group_off] = insn->cfi;\n\t} else {\n\t\tif (cficmp(alt_cfi[group_off], insn->cfi)) {\n\t\t\tWARN_FUNC(\"stack layout conflict in alternatives\",\n\t\t\t\t  insn->sec, insn->offset);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int handle_insn_ops(struct instruction *insn,\n\t\t\t   struct instruction *next_insn,\n\t\t\t   struct insn_state *state)\n{\n\tstruct stack_op *op;\n\n\tlist_for_each_entry(op, &insn->stack_ops, list) {\n\n\t\tif (update_cfi_state(insn, next_insn, &state->cfi, op))\n\t\t\treturn 1;\n\n\t\tif (!insn->alt_group)\n\t\t\tcontinue;\n\n\t\tif (op->dest.type == OP_DEST_PUSHF) {\n\t\t\tif (!state->uaccess_stack) {\n\t\t\t\tstate->uaccess_stack = 1;\n\t\t\t} else if (state->uaccess_stack >> 31) {\n\t\t\t\tWARN_FUNC(\"PUSHF stack exhausted\",\n\t\t\t\t\t  insn->sec, insn->offset);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tstate->uaccess_stack <<= 1;\n\t\t\tstate->uaccess_stack  |= state->uaccess;\n\t\t}\n\n\t\tif (op->src.type == OP_SRC_POPF) {\n\t\t\tif (state->uaccess_stack) {\n\t\t\t\tstate->uaccess = state->uaccess_stack & 1;\n\t\t\t\tstate->uaccess_stack >>= 1;\n\t\t\t\tif (state->uaccess_stack == 1)\n\t\t\t\t\tstate->uaccess_stack = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic bool insn_cfi_match(struct instruction *insn, struct cfi_state *cfi2)\n{\n\tstruct cfi_state *cfi1 = insn->cfi;\n\tint i;\n\n\tif (!cfi1) {\n\t\tWARN(\"CFI missing\");\n\t\treturn false;\n\t}\n\n\tif (memcmp(&cfi1->cfa, &cfi2->cfa, sizeof(cfi1->cfa))) {\n\n\t\tWARN_FUNC(\"stack state mismatch: cfa1=%d%+d cfa2=%d%+d\",\n\t\t\t  insn->sec, insn->offset,\n\t\t\t  cfi1->cfa.base, cfi1->cfa.offset,\n\t\t\t  cfi2->cfa.base, cfi2->cfa.offset);\n\n\t} else if (memcmp(&cfi1->regs, &cfi2->regs, sizeof(cfi1->regs))) {\n\t\tfor (i = 0; i < CFI_NUM_REGS; i++) {\n\t\t\tif (!memcmp(&cfi1->regs[i], &cfi2->regs[i],\n\t\t\t\t    sizeof(struct cfi_reg)))\n\t\t\t\tcontinue;\n\n\t\t\tWARN_FUNC(\"stack state mismatch: reg1[%d]=%d%+d reg2[%d]=%d%+d\",\n\t\t\t\t  insn->sec, insn->offset,\n\t\t\t\t  i, cfi1->regs[i].base, cfi1->regs[i].offset,\n\t\t\t\t  i, cfi2->regs[i].base, cfi2->regs[i].offset);\n\t\t\tbreak;\n\t\t}\n\n\t} else if (cfi1->type != cfi2->type) {\n\n\t\tWARN_FUNC(\"stack state mismatch: type1=%d type2=%d\",\n\t\t\t  insn->sec, insn->offset, cfi1->type, cfi2->type);\n\n\t} else if (cfi1->drap != cfi2->drap ||\n\t\t   (cfi1->drap && cfi1->drap_reg != cfi2->drap_reg) ||\n\t\t   (cfi1->drap && cfi1->drap_offset != cfi2->drap_offset)) {\n\n\t\tWARN_FUNC(\"stack state mismatch: drap1=%d(%d,%d) drap2=%d(%d,%d)\",\n\t\t\t  insn->sec, insn->offset,\n\t\t\t  cfi1->drap, cfi1->drap_reg, cfi1->drap_offset,\n\t\t\t  cfi2->drap, cfi2->drap_reg, cfi2->drap_offset);\n\n\t} else\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool func_uaccess_safe(struct symbol *func)\n{\n\tif (func)\n\t\treturn func->uaccess_safe;\n\n\treturn false;\n}\n\nstatic inline const char *call_dest_name(struct instruction *insn)\n{\n\tstatic char pvname[19];\n\tstruct reloc *rel;\n\tint idx;\n\n\tif (insn->call_dest)\n\t\treturn insn->call_dest->name;\n\n\trel = insn_reloc(NULL, insn);\n\tif (rel && !strcmp(rel->sym->name, \"pv_ops\")) {\n\t\tidx = (rel->addend / sizeof(void *));\n\t\tsnprintf(pvname, sizeof(pvname), \"pv_ops[%d]\", idx);\n\t\treturn pvname;\n\t}\n\n\treturn \"{dynamic}\";\n}\n\nstatic bool pv_call_dest(struct objtool_file *file, struct instruction *insn)\n{\n\tstruct symbol *target;\n\tstruct reloc *rel;\n\tint idx;\n\n\trel = insn_reloc(file, insn);\n\tif (!rel || strcmp(rel->sym->name, \"pv_ops\"))\n\t\treturn false;\n\n\tidx = (arch_dest_reloc_offset(rel->addend) / sizeof(void *));\n\n\tif (file->pv_ops[idx].clean)\n\t\treturn true;\n\n\tfile->pv_ops[idx].clean = true;\n\n\tlist_for_each_entry(target, &file->pv_ops[idx].targets, pv_target) {\n\t\tif (!target->sec->noinstr) {\n\t\t\tWARN(\"pv_ops[%d]: %s\", idx, target->name);\n\t\t\tfile->pv_ops[idx].clean = false;\n\t\t}\n\t}\n\n\treturn file->pv_ops[idx].clean;\n}\n\nstatic inline bool noinstr_call_dest(struct objtool_file *file,\n\t\t\t\t     struct instruction *insn,\n\t\t\t\t     struct symbol *func)\n{\n\t/*\n\t * We can't deal with indirect function calls at present;\n\t * assume they're instrumented.\n\t */\n\tif (!func) {\n\t\tif (file->pv_ops)\n\t\t\treturn pv_call_dest(file, insn);\n\n\t\treturn false;\n\t}\n\n\t/*\n\t * If the symbol is from a noinstr section; we good.\n\t */\n\tif (func->sec->noinstr)\n\t\treturn true;\n\n\t/*\n\t * The __ubsan_handle_*() calls are like WARN(), they only happen when\n\t * something 'BAD' happened. At the risk of taking the machine down,\n\t * let them proceed to get the message out.\n\t */\n\tif (!strncmp(func->name, \"__ubsan_handle_\", 15))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int validate_call(struct objtool_file *file,\n\t\t\t struct instruction *insn,\n\t\t\t struct insn_state *state)\n{\n\tif (state->noinstr && state->instr <= 0 &&\n\t    !noinstr_call_dest(file, insn, insn->call_dest)) {\n\t\tWARN_FUNC(\"call to %s() leaves .noinstr.text section\",\n\t\t\t\tinsn->sec, insn->offset, call_dest_name(insn));\n\t\treturn 1;\n\t}\n\n\tif (state->uaccess && !func_uaccess_safe(insn->call_dest)) {\n\t\tWARN_FUNC(\"call to %s() with UACCESS enabled\",\n\t\t\t\tinsn->sec, insn->offset, call_dest_name(insn));\n\t\treturn 1;\n\t}\n\n\tif (state->df) {\n\t\tWARN_FUNC(\"call to %s() with DF set\",\n\t\t\t\tinsn->sec, insn->offset, call_dest_name(insn));\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int validate_sibling_call(struct objtool_file *file,\n\t\t\t\t struct instruction *insn,\n\t\t\t\t struct insn_state *state)\n{\n\tif (has_modified_stack_frame(insn, state)) {\n\t\tWARN_FUNC(\"sibling call from callable instruction with modified stack frame\",\n\t\t\t\tinsn->sec, insn->offset);\n\t\treturn 1;\n\t}\n\n\treturn validate_call(file, insn, state);\n}\n\nstatic int validate_return(struct symbol *func, struct instruction *insn, struct insn_state *state)\n{\n\tif (state->noinstr && state->instr > 0) {\n\t\tWARN_FUNC(\"return with instrumentation enabled\",\n\t\t\t  insn->sec, insn->offset);\n\t\treturn 1;\n\t}\n\n\tif (state->uaccess && !func_uaccess_safe(func)) {\n\t\tWARN_FUNC(\"return with UACCESS enabled\",\n\t\t\t  insn->sec, insn->offset);\n\t\treturn 1;\n\t}\n\n\tif (!state->uaccess && func_uaccess_safe(func)) {\n\t\tWARN_FUNC(\"return with UACCESS disabled from a UACCESS-safe function\",\n\t\t\t  insn->sec, insn->offset);\n\t\treturn 1;\n\t}\n\n\tif (state->df) {\n\t\tWARN_FUNC(\"return with DF set\",\n\t\t\t  insn->sec, insn->offset);\n\t\treturn 1;\n\t}\n\n\tif (func && has_modified_stack_frame(insn, state)) {\n\t\tWARN_FUNC(\"return with modified stack frame\",\n\t\t\t  insn->sec, insn->offset);\n\t\treturn 1;\n\t}\n\n\tif (state->cfi.bp_scratch) {\n\t\tWARN_FUNC(\"BP used as a scratch register\",\n\t\t\t  insn->sec, insn->offset);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic struct instruction *next_insn_to_validate(struct objtool_file *file,\n\t\t\t\t\t\t struct instruction *insn)\n{\n\tstruct alt_group *alt_group = insn->alt_group;\n\n\t/*\n\t * Simulate the fact that alternatives are patched in-place.  When the\n\t * end of a replacement alt_group is reached, redirect objtool flow to\n\t * the end of the original alt_group.\n\t */\n\tif (alt_group && insn == alt_group->last_insn && alt_group->orig_group)\n\t\treturn next_insn_same_sec(file, alt_group->orig_group->last_insn);\n\n\treturn next_insn_same_sec(file, insn);\n}\n\n/*\n * Follow the branch starting at the given instruction, and recursively follow\n * any other branches (jumps).  Meanwhile, track the frame pointer state at\n * each instruction and validate all the rules described in\n * tools/objtool/Documentation/stack-validation.txt.\n */\nstatic int validate_branch(struct objtool_file *file, struct symbol *func,\n\t\t\t   struct instruction *insn, struct insn_state state)\n{\n\tstruct alternative *alt;\n\tstruct instruction *next_insn, *prev_insn = NULL;\n\tstruct section *sec;\n\tu8 visited;\n\tint ret;\n\n\tsec = insn->sec;\n\n\twhile (1) {\n\t\tnext_insn = next_insn_to_validate(file, insn);\n\n\t\tif (func && insn->func && func != insn->func->pfunc) {\n\t\t\tWARN(\"%s() falls through to next function %s()\",\n\t\t\t     func->name, insn->func->name);\n\t\t\treturn 1;\n\t\t}\n\n\t\tif (func && insn->ignore) {\n\t\t\tWARN_FUNC(\"BUG: why am I validating an ignored function?\",\n\t\t\t\t  sec, insn->offset);\n\t\t\treturn 1;\n\t\t}\n\n\t\tvisited = 1 << state.uaccess;\n\t\tif (insn->visited) {\n\t\t\tif (!insn->hint && !insn_cfi_match(insn, &state.cfi))\n\t\t\t\treturn 1;\n\n\t\t\tif (insn->visited & visited)\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tnr_insns_visited++;\n\t\t}\n\n\t\tif (state.noinstr)\n\t\t\tstate.instr += insn->instr;\n\n\t\tif (insn->hint) {\n\t\t\tstate.cfi = *insn->cfi;\n\t\t} else {\n\t\t\t/* XXX track if we actually changed state.cfi */\n\n\t\t\tif (prev_insn && !cficmp(prev_insn->cfi, &state.cfi)) {\n\t\t\t\tinsn->cfi = prev_insn->cfi;\n\t\t\t\tnr_cfi_reused++;\n\t\t\t} else {\n\t\t\t\tinsn->cfi = cfi_hash_find_or_add(&state.cfi);\n\t\t\t}\n\t\t}\n\n\t\tinsn->visited |= visited;\n\n\t\tif (propagate_alt_cfi(file, insn))\n\t\t\treturn 1;\n\n\t\tif (!insn->ignore_alts && !list_empty(&insn->alts)) {\n\t\t\tbool skip_orig = false;\n\n\t\t\tlist_for_each_entry(alt, &insn->alts, list) {\n\t\t\t\tif (alt->skip_orig)\n\t\t\t\t\tskip_orig = true;\n\n\t\t\t\tret = validate_branch(file, func, alt->insn, state);\n\t\t\t\tif (ret) {\n\t\t\t\t\tif (opts.backtrace)\n\t\t\t\t\t\tBT_FUNC(\"(alt)\", insn);\n\t\t\t\t\treturn ret;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (skip_orig)\n\t\t\t\treturn 0;\n\t\t}\n\n\t\tif (handle_insn_ops(insn, next_insn, &state))\n\t\t\treturn 1;\n\n\t\tswitch (insn->type) {\n\n\t\tcase INSN_RETURN:\n\t\t\treturn validate_return(func, insn, &state);\n\n\t\tcase INSN_CALL:\n\t\tcase INSN_CALL_DYNAMIC:\n\t\t\tret = validate_call(file, insn, &state);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\tif (opts.stackval && func && !is_fentry_call(insn) &&\n\t\t\t    !has_valid_stack_frame(&state)) {\n\t\t\t\tWARN_FUNC(\"call without frame pointer save/setup\",\n\t\t\t\t\t  sec, insn->offset);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\tif (insn->dead_end)\n\t\t\t\treturn 0;\n\n\t\t\tbreak;\n\n\t\tcase INSN_JUMP_CONDITIONAL:\n\t\tcase INSN_JUMP_UNCONDITIONAL:\n\t\t\tif (is_sibling_call(insn)) {\n\t\t\t\tret = validate_sibling_call(file, insn, &state);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\n\t\t\t} else if (insn->jump_dest) {\n\t\t\t\tret = validate_branch(file, func,\n\t\t\t\t\t\t      insn->jump_dest, state);\n\t\t\t\tif (ret) {\n\t\t\t\t\tif (opts.backtrace)\n\t\t\t\t\t\tBT_FUNC(\"(branch)\", insn);\n\t\t\t\t\treturn ret;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (insn->type == INSN_JUMP_UNCONDITIONAL)\n\t\t\t\treturn 0;\n\n\t\t\tbreak;\n\n\t\tcase INSN_JUMP_DYNAMIC:\n\t\tcase INSN_JUMP_DYNAMIC_CONDITIONAL:\n\t\t\tif (is_sibling_call(insn)) {\n\t\t\t\tret = validate_sibling_call(file, insn, &state);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\n\t\t\tif (insn->type == INSN_JUMP_DYNAMIC)\n\t\t\t\treturn 0;\n\n\t\t\tbreak;\n\n\t\tcase INSN_CONTEXT_SWITCH:\n\t\t\tif (func && (!next_insn || !next_insn->hint)) {\n\t\t\t\tWARN_FUNC(\"unsupported instruction in callable function\",\n\t\t\t\t\t  sec, insn->offset);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\treturn 0;\n\n\t\tcase INSN_STAC:\n\t\t\tif (state.uaccess) {\n\t\t\t\tWARN_FUNC(\"recursive UACCESS enable\", sec, insn->offset);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\tstate.uaccess = true;\n\t\t\tbreak;\n\n\t\tcase INSN_CLAC:\n\t\t\tif (!state.uaccess && func) {\n\t\t\t\tWARN_FUNC(\"redundant UACCESS disable\", sec, insn->offset);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\tif (func_uaccess_safe(func) && !state.uaccess_stack) {\n\t\t\t\tWARN_FUNC(\"UACCESS-safe disables UACCESS\", sec, insn->offset);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\tstate.uaccess = false;\n\t\t\tbreak;\n\n\t\tcase INSN_STD:\n\t\t\tif (state.df) {\n\t\t\t\tWARN_FUNC(\"recursive STD\", sec, insn->offset);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\tstate.df = true;\n\t\t\tbreak;\n\n\t\tcase INSN_CLD:\n\t\t\tif (!state.df && func) {\n\t\t\t\tWARN_FUNC(\"redundant CLD\", sec, insn->offset);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\tstate.df = false;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tif (insn->dead_end)\n\t\t\treturn 0;\n\n\t\tif (!next_insn) {\n\t\t\tif (state.cfi.cfa.base == CFI_UNDEFINED)\n\t\t\t\treturn 0;\n\t\t\tWARN(\"%s: unexpected end of section\", sec->name);\n\t\t\treturn 1;\n\t\t}\n\n\t\tprev_insn = insn;\n\t\tinsn = next_insn;\n\t}\n\n\treturn 0;\n}\n\nstatic int validate_unwind_hints(struct objtool_file *file, struct section *sec)\n{\n\tstruct instruction *insn;\n\tstruct insn_state state;\n\tint ret, warnings = 0;\n\n\tif (!file->hints)\n\t\treturn 0;\n\n\tinit_insn_state(file, &state, sec);\n\n\tif (sec) {\n\t\tinsn = find_insn(file, sec, 0);\n\t\tif (!insn)\n\t\t\treturn 0;\n\t} else {\n\t\tinsn = list_first_entry(&file->insn_list, typeof(*insn), list);\n\t}\n\n\twhile (&insn->list != &file->insn_list && (!sec || insn->sec == sec)) {\n\t\tif (insn->hint && !insn->visited && !insn->ignore) {\n\t\t\tret = validate_branch(file, insn->func, insn, state);\n\t\t\tif (ret && opts.backtrace)\n\t\t\t\tBT_FUNC(\"<=== (hint)\", insn);\n\t\t\twarnings += ret;\n\t\t}\n\n\t\tinsn = list_next_entry(insn, list);\n\t}\n\n\treturn warnings;\n}\n\nstatic int validate_retpoline(struct objtool_file *file)\n{\n\tstruct instruction *insn;\n\tint warnings = 0;\n\n\tfor_each_insn(file, insn) {\n\t\tif (insn->type != INSN_JUMP_DYNAMIC &&\n\t\t    insn->type != INSN_CALL_DYNAMIC)\n\t\t\tcontinue;\n\n\t\tif (insn->retpoline_safe)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * .init.text code is ran before userspace and thus doesn't\n\t\t * strictly need retpolines, except for modules which are\n\t\t * loaded late, they very much do need retpoline in their\n\t\t * .init.text\n\t\t */\n\t\tif (!strcmp(insn->sec->name, \".init.text\") && !opts.module)\n\t\t\tcontinue;\n\n\t\tWARN_FUNC(\"indirect %s found in RETPOLINE build\",\n\t\t\t  insn->sec, insn->offset,\n\t\t\t  insn->type == INSN_JUMP_DYNAMIC ? \"jump\" : \"call\");\n\n\t\twarnings++;\n\t}\n\n\treturn warnings;\n}\n\nstatic bool is_kasan_insn(struct instruction *insn)\n{\n\treturn (insn->type == INSN_CALL &&\n\t\t!strcmp(insn->call_dest->name, \"__asan_handle_no_return\"));\n}\n\nstatic bool is_ubsan_insn(struct instruction *insn)\n{\n\treturn (insn->type == INSN_CALL &&\n\t\t!strcmp(insn->call_dest->name,\n\t\t\t\"__ubsan_handle_builtin_unreachable\"));\n}\n\nstatic bool ignore_unreachable_insn(struct objtool_file *file, struct instruction *insn)\n{\n\tint i;\n\tstruct instruction *prev_insn;\n\n\tif (insn->ignore || insn->type == INSN_NOP || insn->type == INSN_TRAP)\n\t\treturn true;\n\n\t/*\n\t * Ignore alternative replacement instructions.  This can happen\n\t * when a whitelisted function uses one of the ALTERNATIVE macros.\n\t */\n\tif (!strcmp(insn->sec->name, \".altinstr_replacement\") ||\n\t    !strcmp(insn->sec->name, \".altinstr_aux\"))\n\t\treturn true;\n\n\t/*\n\t * Whole archive runs might encounter dead code from weak symbols.\n\t * This is where the linker will have dropped the weak symbol in\n\t * favour of a regular symbol, but leaves the code in place.\n\t *\n\t * In this case we'll find a piece of code (whole function) that is not\n\t * covered by a !section symbol. Ignore them.\n\t */\n\tif (opts.link && !insn->func) {\n\t\tint size = find_symbol_hole_containing(insn->sec, insn->offset);\n\t\tunsigned long end = insn->offset + size;\n\n\t\tif (!size) /* not a hole */\n\t\t\treturn false;\n\n\t\tif (size < 0) /* hole until the end */\n\t\t\treturn true;\n\n\t\tsec_for_each_insn_continue(file, insn) {\n\t\t\t/*\n\t\t\t * If we reach a visited instruction at or before the\n\t\t\t * end of the hole, ignore the unreachable.\n\t\t\t */\n\t\t\tif (insn->visited)\n\t\t\t\treturn true;\n\n\t\t\tif (insn->offset >= end)\n\t\t\t\tbreak;\n\n\t\t\t/*\n\t\t\t * If this hole jumps to a .cold function, mark it ignore too.\n\t\t\t */\n\t\t\tif (insn->jump_dest && insn->jump_dest->func &&\n\t\t\t    strstr(insn->jump_dest->func->name, \".cold\")) {\n\t\t\t\tstruct instruction *dest = insn->jump_dest;\n\t\t\t\tfunc_for_each_insn(file, dest->func, dest)\n\t\t\t\t\tdest->ignore = true;\n\t\t\t}\n\t\t}\n\n\t\treturn false;\n\t}\n\n\tif (!insn->func)\n\t\treturn false;\n\n\tif (insn->func->static_call_tramp)\n\t\treturn true;\n\n\t/*\n\t * CONFIG_UBSAN_TRAP inserts a UD2 when it sees\n\t * __builtin_unreachable().  The BUG() macro has an unreachable() after\n\t * the UD2, which causes GCC's undefined trap logic to emit another UD2\n\t * (or occasionally a JMP to UD2).\n\t *\n\t * It may also insert a UD2 after calling a __noreturn function.\n\t */\n\tprev_insn = list_prev_entry(insn, list);\n\tif ((prev_insn->dead_end || dead_end_function(file, prev_insn->call_dest)) &&\n\t    (insn->type == INSN_BUG ||\n\t     (insn->type == INSN_JUMP_UNCONDITIONAL &&\n\t      insn->jump_dest && insn->jump_dest->type == INSN_BUG)))\n\t\treturn true;\n\n\t/*\n\t * Check if this (or a subsequent) instruction is related to\n\t * CONFIG_UBSAN or CONFIG_KASAN.\n\t *\n\t * End the search at 5 instructions to avoid going into the weeds.\n\t */\n\tfor (i = 0; i < 5; i++) {\n\n\t\tif (is_kasan_insn(insn) || is_ubsan_insn(insn))\n\t\t\treturn true;\n\n\t\tif (insn->type == INSN_JUMP_UNCONDITIONAL) {\n\t\t\tif (insn->jump_dest &&\n\t\t\t    insn->jump_dest->func == insn->func) {\n\t\t\t\tinsn = insn->jump_dest;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tbreak;\n\t\t}\n\n\t\tif (insn->offset + insn->len >= insn->func->offset + insn->func->len)\n\t\t\tbreak;\n\n\t\tinsn = list_next_entry(insn, list);\n\t}\n\n\treturn false;\n}\n\nstatic int validate_symbol(struct objtool_file *file, struct section *sec,\n\t\t\t   struct symbol *sym, struct insn_state *state)\n{\n\tstruct instruction *insn;\n\tint ret;\n\n\tif (!sym->len) {\n\t\tWARN(\"%s() is missing an ELF size annotation\", sym->name);\n\t\treturn 1;\n\t}\n\n\tif (sym->pfunc != sym || sym->alias != sym)\n\t\treturn 0;\n\n\tinsn = find_insn(file, sec, sym->offset);\n\tif (!insn || insn->ignore || insn->visited)\n\t\treturn 0;\n\n\tstate->uaccess = sym->uaccess_safe;\n\n\tret = validate_branch(file, insn->func, insn, *state);\n\tif (ret && opts.backtrace)\n\t\tBT_FUNC(\"<=== (sym)\", insn);\n\treturn ret;\n}\n\nstatic int validate_section(struct objtool_file *file, struct section *sec)\n{\n\tstruct insn_state state;\n\tstruct symbol *func;\n\tint warnings = 0;\n\n\tlist_for_each_entry(func, &sec->symbol_list, list) {\n\t\tif (func->type != STT_FUNC)\n\t\t\tcontinue;\n\n\t\tinit_insn_state(file, &state, sec);\n\t\tset_func_state(&state.cfi);\n\n\t\twarnings += validate_symbol(file, sec, func, &state);\n\t}\n\n\treturn warnings;\n}\n\nstatic int validate_noinstr_sections(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tint warnings = 0;\n\n\tsec = find_section_by_name(file->elf, \".noinstr.text\");\n\tif (sec) {\n\t\twarnings += validate_section(file, sec);\n\t\twarnings += validate_unwind_hints(file, sec);\n\t}\n\n\tsec = find_section_by_name(file->elf, \".entry.text\");\n\tif (sec) {\n\t\twarnings += validate_section(file, sec);\n\t\twarnings += validate_unwind_hints(file, sec);\n\t}\n\n\treturn warnings;\n}\n\nstatic int validate_functions(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tint warnings = 0;\n\n\tfor_each_sec(file, sec) {\n\t\tif (!(sec->sh.sh_flags & SHF_EXECINSTR))\n\t\t\tcontinue;\n\n\t\twarnings += validate_section(file, sec);\n\t}\n\n\treturn warnings;\n}\n\nstatic void mark_endbr_used(struct instruction *insn)\n{\n\tif (!list_empty(&insn->call_node))\n\t\tlist_del_init(&insn->call_node);\n}\n\nstatic int validate_ibt_insn(struct objtool_file *file, struct instruction *insn)\n{\n\tstruct instruction *dest;\n\tstruct reloc *reloc;\n\tunsigned long off;\n\tint warnings = 0;\n\n\t/*\n\t * Looking for function pointer load relocations.  Ignore\n\t * direct/indirect branches:\n\t */\n\tswitch (insn->type) {\n\tcase INSN_CALL:\n\tcase INSN_CALL_DYNAMIC:\n\tcase INSN_JUMP_CONDITIONAL:\n\tcase INSN_JUMP_UNCONDITIONAL:\n\tcase INSN_JUMP_DYNAMIC:\n\tcase INSN_JUMP_DYNAMIC_CONDITIONAL:\n\tcase INSN_RETURN:\n\tcase INSN_NOP:\n\t\treturn 0;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tfor (reloc = insn_reloc(file, insn);\n\t     reloc;\n\t     reloc = find_reloc_by_dest_range(file->elf, insn->sec,\n\t\t\t\t\t      reloc->offset + 1,\n\t\t\t\t\t      (insn->offset + insn->len) - (reloc->offset + 1))) {\n\n\t\t/*\n\t\t * static_call_update() references the trampoline, which\n\t\t * doesn't have (or need) ENDBR.  Skip warning in that case.\n\t\t */\n\t\tif (reloc->sym->static_call_tramp)\n\t\t\tcontinue;\n\n\t\toff = reloc->sym->offset;\n\t\tif (reloc->type == R_X86_64_PC32 || reloc->type == R_X86_64_PLT32)\n\t\t\toff += arch_dest_reloc_offset(reloc->addend);\n\t\telse\n\t\t\toff += reloc->addend;\n\n\t\tdest = find_insn(file, reloc->sym->sec, off);\n\t\tif (!dest)\n\t\t\tcontinue;\n\n\t\tif (dest->type == INSN_ENDBR) {\n\t\t\tmark_endbr_used(dest);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (dest->func && dest->func == insn->func) {\n\t\t\t/*\n\t\t\t * Anything from->to self is either _THIS_IP_ or\n\t\t\t * IRET-to-self.\n\t\t\t *\n\t\t\t * There is no sane way to annotate _THIS_IP_ since the\n\t\t\t * compiler treats the relocation as a constant and is\n\t\t\t * happy to fold in offsets, skewing any annotation we\n\t\t\t * do, leading to vast amounts of false-positives.\n\t\t\t *\n\t\t\t * There's also compiler generated _THIS_IP_ through\n\t\t\t * KCOV and such which we have no hope of annotating.\n\t\t\t *\n\t\t\t * As such, blanket accept self-references without\n\t\t\t * issue.\n\t\t\t */\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (dest->noendbr)\n\t\t\tcontinue;\n\n\t\tWARN_FUNC(\"relocation to !ENDBR: %s\",\n\t\t\t  insn->sec, insn->offset,\n\t\t\t  offstr(dest->sec, dest->offset));\n\n\t\twarnings++;\n\t}\n\n\treturn warnings;\n}\n\nstatic int validate_ibt_data_reloc(struct objtool_file *file,\n\t\t\t\t   struct reloc *reloc)\n{\n\tstruct instruction *dest;\n\n\tdest = find_insn(file, reloc->sym->sec,\n\t\t\t reloc->sym->offset + reloc->addend);\n\tif (!dest)\n\t\treturn 0;\n\n\tif (dest->type == INSN_ENDBR) {\n\t\tmark_endbr_used(dest);\n\t\treturn 0;\n\t}\n\n\tif (dest->noendbr)\n\t\treturn 0;\n\n\tWARN_FUNC(\"data relocation to !ENDBR: %s\",\n\t\t  reloc->sec->base, reloc->offset,\n\t\t  offstr(dest->sec, dest->offset));\n\n\treturn 1;\n}\n\n/*\n * Validate IBT rules and remove used ENDBR instructions from the seal list.\n * Unused ENDBR instructions will be annotated for sealing (i.e., replaced with\n * NOPs) later, in create_ibt_endbr_seal_sections().\n */\nstatic int validate_ibt(struct objtool_file *file)\n{\n\tstruct section *sec;\n\tstruct reloc *reloc;\n\tstruct instruction *insn;\n\tint warnings = 0;\n\n\tfor_each_insn(file, insn)\n\t\twarnings += validate_ibt_insn(file, insn);\n\n\tfor_each_sec(file, sec) {\n\n\t\t/* Already done by validate_ibt_insn() */\n\t\tif (sec->sh.sh_flags & SHF_EXECINSTR)\n\t\t\tcontinue;\n\n\t\tif (!sec->reloc)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * These sections can reference text addresses, but not with\n\t\t * the intent to indirect branch to them.\n\t\t */\n\t\tif (!strncmp(sec->name, \".discard\", 8)\t\t\t||\n\t\t    !strncmp(sec->name, \".debug\", 6)\t\t\t||\n\t\t    !strcmp(sec->name, \".altinstructions\")\t\t||\n\t\t    !strcmp(sec->name, \".ibt_endbr_seal\")\t\t||\n\t\t    !strcmp(sec->name, \".orc_unwind_ip\")\t\t||\n\t\t    !strcmp(sec->name, \".parainstructions\")\t\t||\n\t\t    !strcmp(sec->name, \".retpoline_sites\")\t\t||\n\t\t    !strcmp(sec->name, \".smp_locks\")\t\t\t||\n\t\t    !strcmp(sec->name, \".static_call_sites\")\t\t||\n\t\t    !strcmp(sec->name, \"_error_injection_whitelist\")\t||\n\t\t    !strcmp(sec->name, \"_kprobe_blacklist\")\t\t||\n\t\t    !strcmp(sec->name, \"__bug_table\")\t\t\t||\n\t\t    !strcmp(sec->name, \"__ex_table\")\t\t\t||\n\t\t    !strcmp(sec->name, \"__jump_table\")\t\t\t||\n\t\t    !strcmp(sec->name, \"__mcount_loc\"))\n\t\t\tcontinue;\n\n\t\tlist_for_each_entry(reloc, &sec->reloc->reloc_list, list)\n\t\t\twarnings += validate_ibt_data_reloc(file, reloc);\n\t}\n\n\treturn warnings;\n}\n\nstatic int validate_sls(struct objtool_file *file)\n{\n\tstruct instruction *insn, *next_insn;\n\tint warnings = 0;\n\n\tfor_each_insn(file, insn) {\n\t\tnext_insn = next_insn_same_sec(file, insn);\n\n\t\tif (insn->retpoline_safe)\n\t\t\tcontinue;\n\n\t\tswitch (insn->type) {\n\t\tcase INSN_RETURN:\n\t\t\tif (!next_insn || next_insn->type != INSN_TRAP) {\n\t\t\t\tWARN_FUNC(\"missing int3 after ret\",\n\t\t\t\t\t  insn->sec, insn->offset);\n\t\t\t\twarnings++;\n\t\t\t}\n\n\t\t\tbreak;\n\t\tcase INSN_JUMP_DYNAMIC:\n\t\t\tif (!next_insn || next_insn->type != INSN_TRAP) {\n\t\t\t\tWARN_FUNC(\"missing int3 after indirect jump\",\n\t\t\t\t\t  insn->sec, insn->offset);\n\t\t\t\twarnings++;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn warnings;\n}\n\nstatic int validate_reachable_instructions(struct objtool_file *file)\n{\n\tstruct instruction *insn;\n\n\tif (file->ignore_unreachables)\n\t\treturn 0;\n\n\tfor_each_insn(file, insn) {\n\t\tif (insn->visited || ignore_unreachable_insn(file, insn))\n\t\t\tcontinue;\n\n\t\tWARN_FUNC(\"unreachable instruction\", insn->sec, insn->offset);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nint check(struct objtool_file *file)\n{\n\tint ret, warnings = 0;\n\n\tarch_initial_func_cfi_state(&initial_func_cfi);\n\tinit_cfi_state(&init_cfi);\n\tinit_cfi_state(&func_cfi);\n\tset_func_state(&func_cfi);\n\n\tif (!cfi_hash_alloc(1UL << (file->elf->symbol_bits - 3)))\n\t\tgoto out;\n\n\tcfi_hash_add(&init_cfi);\n\tcfi_hash_add(&func_cfi);\n\n\tret = decode_sections(file);\n\tif (ret < 0)\n\t\tgoto out;\n\n\twarnings += ret;\n\n\tif (list_empty(&file->insn_list))\n\t\tgoto out;\n\n\tif (opts.retpoline) {\n\t\tret = validate_retpoline(file);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\twarnings += ret;\n\t}\n\n\tif (opts.stackval || opts.orc || opts.uaccess) {\n\t\tret = validate_functions(file);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\n\t\tret = validate_unwind_hints(file, NULL);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\n\t\tif (!warnings) {\n\t\t\tret = validate_reachable_instructions(file);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\twarnings += ret;\n\t\t}\n\n\t} else if (opts.noinstr) {\n\t\tret = validate_noinstr_sections(file);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\t}\n\n\tif (opts.ibt) {\n\t\tret = validate_ibt(file);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\t}\n\n\tif (opts.sls) {\n\t\tret = validate_sls(file);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\t}\n\n\tif (opts.static_call) {\n\t\tret = create_static_call_sections(file);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\t}\n\n\tif (opts.retpoline) {\n\t\tret = create_retpoline_sites_sections(file);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\t}\n\n\tif (opts.mcount) {\n\t\tret = create_mcount_loc_sections(file);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\t}\n\n\tif (opts.ibt) {\n\t\tret = create_ibt_endbr_seal_sections(file);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\t}\n\n\tif (opts.orc && !list_empty(&file->insn_list)) {\n\t\tret = orc_create(file);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\twarnings += ret;\n\t}\n\n\n\tif (opts.stats) {\n\t\tprintf(\"nr_insns_visited: %ld\\n\", nr_insns_visited);\n\t\tprintf(\"nr_cfi: %ld\\n\", nr_cfi);\n\t\tprintf(\"nr_cfi_reused: %ld\\n\", nr_cfi_reused);\n\t\tprintf(\"nr_cfi_cache: %ld\\n\", nr_cfi_cache);\n\t}\n\nout:\n\t/*\n\t *  For now, don't fail the kernel build on fatal warnings.  These\n\t *  errors are still fairly common due to the growing matrix of\n\t *  supported toolchains and their recent pace of change.\n\t */\n\treturn 0;\n}\n"], "filenames": ["arch/x86/boot/compressed/ident_map_64.c", "arch/x86/include/asm/setup.h", "arch/x86/include/uapi/asm/bootparam.h", "arch/x86/kernel/head64.c", "arch/x86/kernel/vmlinux.lds.S", "arch/x86/xen/enlighten_pv.c", "arch/x86/xen/xen-head.S", "tools/objtool/check.c"], "buggy_code_start_loc": [112, 122, 18, 429, 388, 1186, 51, 3829], "buggy_code_end_loc": [165, 122, 19, 432, 389, 1193, 73, 3831], "fixing_code_start_loc": [113, 123, 18, 429, 388, 1186, 50, 3829], "fixing_code_end_loc": [179, 126, 19, 435, 389, 1198, 66, 3830], "type": "NVD-CWE-noinfo", "message": "The Linux kernel before 5.18.13 lacks a certain clear operation for the block starting symbol (.bss). This allows Xen PV guest OS users to cause a denial of service or gain privileges.", "other": {"cve": {"id": "CVE-2022-36123", "sourceIdentifier": "cve@mitre.org", "published": "2022-07-29T14:15:08.303", "lastModified": "2022-09-04T19:29:34.133", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The Linux kernel before 5.18.13 lacks a certain clear operation for the block starting symbol (.bss). This allows Xen PV guest OS users to cause a denial of service or gain privileges."}, {"lang": "es", "value": "El kernel de Linux versiones anteriores a 5.18.13, carece de una determinada operaci\u00f3n de borrado para el s\u00edmbolo de inicio de bloque (.bss). Esto permite a usuarios del SO hu\u00e9sped Xen PV causar una denegaci\u00f3n de servicio o conseguir privilegios"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}], "configurations": [{"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.18.13", "matchCriteriaId": "5E7A257D-D28B-46EB-BC21-AD8C60095E96"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h300s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "6770B6C3-732E-4E22-BF1C-2D2FD610061C"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h300s:-:*:*:*:*:*:*:*", "matchCriteriaId": "9F9C8C20-42EB-4AB5-BD97-212DEB070C43"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h500s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "7FFF7106-ED78-49BA-9EC5-B889E3685D53"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h500s:-:*:*:*:*:*:*:*", "matchCriteriaId": "E63D8B0F-006E-4801-BF9D-1C001BBFB4F9"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h700s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "56409CEC-5A1E-4450-AA42-641E459CC2AF"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h700s:-:*:*:*:*:*:*:*", "matchCriteriaId": "B06F4839-D16A-4A61-9BB5-55B13F41E47F"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h410s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "D0B4AD8A-F172-4558-AEC6-FF424BA2D912"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h410s:-:*:*:*:*:*:*:*", "matchCriteriaId": "8497A4C9-8474-4A62-8331-3FE862ED4098"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h410c_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "234DEFE0-5CE5-4B0A-96B8-5D227CB8ED31"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h410c:-:*:*:*:*:*:*:*", "matchCriteriaId": "CDDF61B7-EC5C-467C-B710-B89F502CD04F"}]}]}], "references": [{"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.18.13", "source": "cve@mitre.org", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/sickcodes/security/blob/master/advisories/SICK-2022-128.md", "source": "cve@mitre.org", "tags": ["Exploit", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/74a0032b8524ee2bd4443128c0bf9775928680b0", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/96e8fc5818686d4a1591bb6907e7fdb64ef29884", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20220901-0003/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://sick.codes/sick-2022-128", "source": "cve@mitre.org", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/74a0032b8524ee2bd4443128c0bf9775928680b0"}}