{"buggy_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// Implements a quantized eight-bit version of the matmul operation.\n\n#define EIGEN_USE_THREADS\n\n#define GEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\n#include \"public/gemmlowp.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/kernels/meta_support.h\"\n#include \"tensorflow/core/kernels/quantization_utils.h\"\n#include \"tensorflow/core/kernels/reference_gemm.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n\nnamespace tensorflow {\n\n// We have to break this out as a separate function because there are multiple\n// combinations of transpose attributes we need to support, and they have to be\n// compile-time constants to work with the templates used internally.\ntemplate <bool TransposeA, bool TransposeB, bool TransposeC>\nvoid GemmlowpMultiply(OpKernelContext* op_context, const quint8* a_data,\n                      const quint8* b_data, qint32* c_data, int m, int n, int k,\n                      int offset_a, int offset_b, int lda, int ldb, int ldc) {\n  const uint8* a_data_as_uint8 = &(a_data->value);\n  const uint8* b_data_as_uint8 = &(b_data->value);\n  int32* c_data_as_int32 = &(c_data->value);\n  static const gemmlowp::MapOrder ResultOrder =\n      !TransposeC ? gemmlowp::MapOrder::RowMajor : gemmlowp::MapOrder::ColMajor;\n  static const gemmlowp::MapOrder LhsOrder =\n      !TransposeA ? gemmlowp::MapOrder::RowMajor : gemmlowp::MapOrder::ColMajor;\n  static const gemmlowp::MapOrder RhsOrder =\n      !TransposeB ? gemmlowp::MapOrder::RowMajor : gemmlowp::MapOrder::ColMajor;\n  gemmlowp::MatrixMap<const std::uint8_t, LhsOrder> lhs(a_data_as_uint8, m, k,\n                                                        lda);\n  gemmlowp::MatrixMap<const std::uint8_t, RhsOrder> rhs(b_data_as_uint8, k, n,\n                                                        ldb);\n  gemmlowp::MatrixMap<std::int32_t, ResultOrder> result(c_data_as_int32, m, n,\n                                                        ldc);\n  const std::tuple<> empty_pipeline = {};\n  auto& worker_threads =\n      *(op_context->device()->tensorflow_cpu_worker_threads());\n  TensorflowGemmContext context(worker_threads.num_threads,\n                                worker_threads.workers);\n  gemmlowp::GemmWithOutputPipeline<std::uint8_t, std::int32_t,\n                                   gemmlowp::DefaultL8R8BitDepthParams>(\n      &context, lhs, rhs, &result, -offset_a, -offset_b, empty_pipeline);\n  // Since gemmlowp uses assembly to write to the output, msan won't detect\n  // the output buffer as written to, so we mark it manually.\n  TF_ANNOTATE_MEMORY_IS_INITIALIZED(c_data_as_int32, m * n * sizeof(int32));\n}\n\ntemplate <class T1, class T2, class Toutput>\nclass QuantizedMatMulOp : public OpKernel {\n public:\n  explicit QuantizedMatMulOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"transpose_a\", &transpose_a_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"transpose_b\", &transpose_b_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& a = context->input(0);\n    const Tensor& b = context->input(1);\n    const float min_a = context->input(2).flat<float>()(0);\n    const float max_a = context->input(3).flat<float>()(0);\n    const float min_b = context->input(4).flat<float>()(0);\n    const float max_b = context->input(5).flat<float>()(0);\n\n    // Make sure that we have valid quantization ranges for the input buffers.\n    // If the difference between the min and max is negative or zero, it makes\n    // it hard to do meaningful intermediate operations on the values.\n    OP_REQUIRES(context, (max_a > min_a),\n                errors::InvalidArgument(\"max_a must be larger than min_a.\"));\n    OP_REQUIRES(context, (max_b > min_b),\n                errors::InvalidArgument(\"max_b must be larger than min_b.\"));\n    const int32_t offset_a = FloatToQuantizedUnclamped<T1>(0.0f, min_a, max_a);\n    const int32_t offset_b = FloatToQuantizedUnclamped<T2>(0.0f, min_b, max_b);\n    const int32_t offset_c = 0;\n    const int32_t mult_c = 1;\n    const int32_t shift_c = 0;\n\n    // Check that the dimensions of the two matrices are valid.\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(a.shape()),\n                errors::InvalidArgument(\"In[0] is not a matrix\"));\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(b.shape()),\n                errors::InvalidArgument(\"In[1] is not a matrix\"));\n    Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\n    dim_pair[0].first = transpose_a_ ? 0 : 1;\n    dim_pair[0].second = transpose_b_ ? 1 : 0;\n\n    OP_REQUIRES(context,\n                a.dim_size(dim_pair[0].first) == b.dim_size(dim_pair[0].second),\n                errors::InvalidArgument(\"Matrix size-incompatible: In[0]: \",\n                                        a.shape().DebugString(),\n                                        \", In[1]: \", b.shape().DebugString()));\n\n    OP_REQUIRES(context, ((shift_c >= 0) && (shift_c <= 31)),\n                errors::InvalidArgument(\"shift_c must be between 0 and 31, \"\n                                        \"inclusive.\"));\n\n    int a_dim_remaining = 1 - dim_pair[0].first;\n    int b_dim_remaining = 1 - dim_pair[0].second;\n    TensorShape out_shape(\n        {a.dim_size(a_dim_remaining), b.dim_size(b_dim_remaining)});\n    Tensor* c = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &c));\n    CHECK(c);\n\n    const T1* a_data = a.flat<T1>().data();\n    const T2* b_data = b.flat<T2>().data();\n    Toutput* c_data = c->flat<Toutput>().data();\n\n    const bool transpose_c = false;\n    const size_t m = a.dim_size(a_dim_remaining);\n    const size_t n = b.dim_size(b_dim_remaining);\n    const size_t k = a.dim_size(dim_pair[0].first);\n    const size_t lda = a.dim_size(1);\n    const size_t ldb = b.dim_size(1);\n    const size_t ldc = n;\n\n    if (meta::IsSupportedAndEnabled() && std::is_same<T1, quint8>() &&\n        std::is_same<T2, quint8>() && std::is_same<Toutput, qint32>() &&\n        (offset_c == 0) && (mult_c == 1) && (shift_c == 0) &&\n        (transpose_c == false) && (k <= 2048)) {\n      // Gemmlowp/meta code path works on 32 & 64 bit Arm with NEON Simd and\n      // allows optimized quantized 8bit to 32bit gemm.\n      meta::QuantizedGemm(context, transpose_a_, transpose_b_, a_data, b_data,\n                          c_data, m, n, k, -offset_a, -offset_b, lda, ldb, ldc);\n    } else if (std::is_same<T1, quint8>() && std::is_same<T2, quint8>() &&\n               std::is_same<Toutput, qint32>() && (offset_c == 0) &&\n               (mult_c == 1) && (shift_c == 0) && (transpose_c == false)) {\n      // The gemmlowp optimized library only works for a particular set of data\n      // types, so check if we meet those requirements and fall back to a slower\n      // reference implementation if not.\n      if (transpose_a_) {\n        if (transpose_b_) {\n          GemmlowpMultiply<true, true, false>(context, a_data, b_data, c_data,\n                                              m, n, k, offset_a, offset_b, lda,\n                                              ldb, ldc);\n        } else {\n          GemmlowpMultiply<true, false, false>(context, a_data, b_data, c_data,\n                                               m, n, k, offset_a, offset_b, lda,\n                                               ldb, ldc);\n        }\n      } else {\n        if (transpose_b_) {\n          GemmlowpMultiply<false, true, false>(context, a_data, b_data, c_data,\n                                               m, n, k, offset_a, offset_b, lda,\n                                               ldb, ldc);\n        } else {\n          GemmlowpMultiply<false, false, false>(context, a_data, b_data, c_data,\n                                                m, n, k, offset_a, offset_b,\n                                                lda, ldb, ldc);\n        }\n      }\n    } else {\n      ReferenceGemm<T1, T2, Toutput>(\n          transpose_a_, transpose_b_, transpose_c, m, n, k, a_data, offset_a,\n          lda, b_data, offset_b, ldb, c_data, shift_c, offset_c, mult_c, ldc);\n    }\n\n    float min_c_value;\n    float max_c_value;\n    QuantizationRangeForMultiplication<T1, T2, Toutput>(\n        min_a, max_a, min_b, max_b, &min_c_value, &max_c_value);\n    Tensor* c_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &c_min));\n    c_min->flat<float>()(0) = min_c_value;\n\n    Tensor* c_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &c_max));\n    c_max->flat<float>()(0) = max_c_value;\n  }\n\n private:\n  bool transpose_a_;\n  bool transpose_b_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"QuantizedMatMul\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint8>(\"T1\")\n                            .TypeConstraint<quint8>(\"T2\")\n                            .TypeConstraint<qint32>(\"Toutput\"),\n                        QuantizedMatMulOp<quint8, quint8, qint32>);\n\n}  // namespace tensorflow\n", "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#include <functional>\n#include <memory>\n#include <vector>\n\n#include \"tensorflow/core/framework/allocator.h\"\n#include \"tensorflow/core/framework/fake_input.h\"\n#include \"tensorflow/core/framework/node_def_builder.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_testutil.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/framework/types.pb.h\"\n#include \"tensorflow/core/kernels/ops_testutil.h\"\n#include \"tensorflow/core/kernels/ops_util.h\"\n#include \"tensorflow/core/kernels/quantization_utils.h\"\n#include \"tensorflow/core/lib/core/status_test_util.h\"\n#include \"tensorflow/core/platform/test.h\"\n\nnamespace tensorflow {\n\nclass QuantizedMatMulTest : public OpsTestBase {\n protected:\n};\n\n// Runs two small matrices through the operator, and leaves all the parameters\n// at their default values.\nTEST_F(QuantizedMatMulTest, Small_NoParams) {\n  TF_ASSERT_OK(NodeDefBuilder(\"quantized_mat_mul_op\", \"QuantizedMatMul\")\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Attr(\"Toutput\", DataTypeToEnum<qint32>::v())\n                   .Finalize(node_def()));\n  TF_ASSERT_OK(InitOp());\n  // A matrix is:\n  // |  1 |  2 |  3 |\n  // |  4 |  5 |  6 |\n  AddInputFromArray<quint8>(TensorShape({2, 3}), {1, 2, 3, 4, 5, 6});\n  // B matrix is:\n  // |  7 |  8 |  9 | 10 |\n  // | 11 | 12 | 13 | 14 |\n  // | 15 | 16 | 17 | 18 |\n  AddInputFromArray<quint8>(TensorShape({3, 4}),\n                            {7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18});\n  AddInputFromArray<float>(TensorShape({1}), {0});\n  AddInputFromArray<float>(TensorShape({1}), {255.0f});\n  AddInputFromArray<float>(TensorShape({1}), {0});\n  AddInputFromArray<float>(TensorShape({1}), {255.0f});\n\n  TF_ASSERT_OK(RunOpKernel());\n  // Here are the results we expect, from hand calculations:\n  // (1 * 7) + (2 * 11) + (3 * 15) = 74\n  // (1 * 8) + (2 * 12) + (3 * 16) = 80\n  // (1 * 9) + (2 * 13) + (3 * 17) = 86\n  // (1 * 10) + (2 * 14) + (3 * 18) = 92\n  // (4 * 7) + (5 * 11) + (6 * 15) = 173\n  // (4 * 8) + (5 * 12) + (6 * 16) = 188\n  // (4 * 9) + (5 * 13) + (6 * 17) = 203\n  // (4 * 10) + (5 * 14) + (6 * 18) = 218\n  Tensor expected(allocator(), DT_QINT32, TensorShape({2, 4}));\n  test::FillValues<qint32>(&expected, {74, 80, 86, 92, 173, 188, 203, 218});\n  test::ExpectTensorEqual<qint32>(expected, *GetOutput(0));\n}\n\n// This test multiplies two 1x1 8bit matrices, and compares the\n// results with hand-calculated expectations.\nTEST_F(QuantizedMatMulTest, VerySmall_WithParams) {\n  // These parameters reflect a typical production usage of eight-bit matmuls\n  // in an Inception-style network.\n  const bool transpose_a = true;\n  const int a_rows = 1;\n  const int a_cols = 1;\n  const int b_rows = 1;\n  const int b_cols = 1;\n  const bool transpose_b = false;\n  TF_ASSERT_OK(NodeDefBuilder(\"quantized_mat_mul_op\", \"QuantizedMatMul\")\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Attr(\"Toutput\", DataTypeToEnum<qint32>::v())\n                   .Attr(\"transpose_a\", transpose_a)\n                   .Attr(\"transpose_b\", transpose_b)\n                   .Finalize(node_def()));\n  TF_ASSERT_OK(InitOp());\n  // The A matrix is:\n  // |  -1 |\n  // The input array only contains unsigned bytes, so we specify the actual\n  // values as n+a_offset, where a_offset is 12 above. For example that means -1\n  // is represented as -1 + 12, or 11.\n  // We have set the transpose_a flag to true, so the matrix is transposed, and\n  // for filling the values the in-memory storage order is effectively\n  // column major, rather than the default row-major.\n  AddInputFromArray<quint8>(TensorShape({a_rows, a_cols}), {11});\n\n  // The B matrix is:\n  // |   1 |\n  AddInputFromArray<quint8>(TensorShape({b_rows, b_cols}), {0});\n  AddInputFromArray<float>(TensorShape({1}), {-12.0f});\n  AddInputFromArray<float>(TensorShape({1}), {243.0f});\n  AddInputFromArray<float>(TensorShape({1}), {1.0f});\n  AddInputFromArray<float>(TensorShape({1}), {256.0f});\n  TF_ASSERT_OK(RunOpKernel());\n  // We're requesting C = A.transposed() * B,\n  // so we expect to get these results:\n  // 1*-1 = -1\n  // | -1 |\n  Tensor expected(allocator(), DT_QINT32, TensorShape({a_cols, b_cols}));\n  test::FillValues<qint32>(&expected, {-1});\n  test::ExpectTensorEqual<qint32>(expected, *GetOutput(0));\n}\n\n// This test multiplies two 1x1 8bit matrices, but sets an invalid quantization\n// range, so we expect to get an error\nTEST_F(QuantizedMatMulTest, VerySmall_BadRange) {\n  // These parameters reflect a typical production usage of eight-bit matmuls\n  // in an Inception-style network.\n  const bool transpose_a = true;\n  const int a_rows = 1;\n  const int a_cols = 1;\n  const int b_rows = 1;\n  const int b_cols = 1;\n  const bool transpose_b = false;\n  TF_ASSERT_OK(NodeDefBuilder(\"quantized_mat_mul_op\", \"QuantizedMatMul\")\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Attr(\"Toutput\", DataTypeToEnum<qint32>::v())\n                   .Attr(\"transpose_a\", transpose_a)\n                   .Attr(\"transpose_b\", transpose_b)\n                   .Finalize(node_def()));\n  TF_ASSERT_OK(InitOp());\n  // The A matrix is:\n  // |  -1 |\n  AddInputFromArray<quint8>(TensorShape({a_rows, a_cols}), {11});\n\n  // The B matrix is:\n  // |   1 |\n  AddInputFromArray<quint8>(TensorShape({b_rows, b_cols}), {0});\n  AddInputFromArray<float>(TensorShape({1}), {-12.0f});\n  AddInputFromArray<float>(TensorShape({1}), {243.0f});\n  // Here we set the range so that the min and max are equal, so we expect to\n  // see an error when we run.\n  AddInputFromArray<float>(TensorShape({1}), {1.0f});\n  AddInputFromArray<float>(TensorShape({1}), {1.0f});\n  EXPECT_EQ(::tensorflow::error::INVALID_ARGUMENT, RunOpKernel().code());\n}\n\n// This test multiplies a couple of small 8-bit matrices, and compares the\n// results with hand-calculated expectations. It uses shifts and offsets to\n// control the range of the outputs.\nTEST_F(QuantizedMatMulTest, Small_WithParams) {\n  // These parameters reflect a typical production usage of eight-bit matmuls\n  // in an Inception-style network.\n  const bool transpose_a = true;\n  const int a_rows = 3;\n  const int a_cols = 4;\n  const int b_rows = 3;\n  const int b_cols = 2;\n  const bool transpose_b = false;\n  TF_ASSERT_OK(NodeDefBuilder(\"quantized_mat_mul_op\", \"QuantizedMatMul\")\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Attr(\"Toutput\", DataTypeToEnum<qint32>::v())\n                   .Attr(\"transpose_a\", transpose_a)\n                   .Attr(\"transpose_b\", transpose_b)\n                   .Finalize(node_def()));\n  TF_ASSERT_OK(InitOp());\n  // The A matrix is:\n  // |  -1 |  -5 |  -9 |\n  // |  -2 |  -6 | -10 |\n  // |  -3 |  -7 | -11 |\n  // |  -4 |  -8 | -12 |\n  // The input array only contains unsigned bytes, so we specify the actual\n  // values as n+a_offset, where a_offset is 12 above. For example that means -1\n  // is represented as -1 + 12, or 11.\n  // We have set the transpose_a flag to true, so the matrix is transposed, and\n  // for filling the values the in-memory storage order is effectively\n  // column major, rather than the default row-major.\n  AddInputFromArray<quint8>(TensorShape({a_rows, a_cols}), {\n                                                               11,\n                                                               10,\n                                                               9,\n                                                               8,\n                                                               7,\n                                                               6,\n                                                               5,\n                                                               4,\n                                                               3,\n                                                               2,\n                                                               1,\n                                                               0,\n                                                           });\n\n  // The B matrix is:\n  // |   1 |   4|\n  // |   2 |   5|\n  // |   3 |   6|\n  AddInputFromArray<quint8>(TensorShape({b_rows, b_cols}), {\n                                                               1,\n                                                               4,\n                                                               2,\n                                                               5,\n                                                               3,\n                                                               6,\n                                                           });\n  AddInputFromArray<float>(TensorShape({1}), {-12.0f});\n  AddInputFromArray<float>(TensorShape({1}), {243.0f});\n  AddInputFromArray<float>(TensorShape({1}), {0});\n  AddInputFromArray<float>(TensorShape({1}), {255.0f});\n  TF_ASSERT_OK(RunOpKernel());\n  // We're requesting C = A.transposed() * B,\n  // so we expect to get these results:\n  // 1*-1 + 2*-5 + 3*-9 = -38\n  // 4*-1 + 5*-5 + 6*-9 = -83\n  // 1*-2 + 2*-6 + 3*-10 = -44\n  // 4*-2 + 5*-6 + 6*-10 = -98\n  // 1*-3 + 2*-7 + 3*-11 = -50\n  // 4*-3 + 5*-7 + 6*-11 = -113\n  // 1*-4 + 2*-8 + 3*-12 = -56\n  // 4*-4 + 5*-8 + 6*-12 = -128\n  // |  -38 |  -83 |\n  // |  -44 |  -98 |\n  // |  -50 | -113 |\n  // |  -56 | -128 |\n  Tensor expected(allocator(), DT_QINT32, TensorShape({a_cols, b_cols}));\n  test::FillValues<qint32>(&expected, {\n                                          -38,\n                                          -83,\n                                          -44,\n                                          -98,\n                                          -50,\n                                          -113,\n                                          -56,\n                                          -128,\n                                      });\n  test::ExpectTensorEqual<qint32>(expected, *GetOutput(0));\n}\n\n// This test multiplies a couple of medium-sized 8-bit matrices, and tests the\n// results against what we saw from running a float MatMul with equivalent\n// inputs.\nTEST_F(QuantizedMatMulTest, Medium_WithParams) {\n  const bool transpose_a = true;\n  const bool transpose_b = false;\n  TF_ASSERT_OK(NodeDefBuilder(\"quantized_mat_mul_op\", \"QuantizedMatMul\")\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Attr(\"Toutput\", DataTypeToEnum<qint32>::v())\n                   .Attr(\"transpose_a\", transpose_a)\n                   .Attr(\"transpose_b\", transpose_b)\n                   .Finalize(node_def()));\n  TF_ASSERT_OK(InitOp());\n\n  const int a_rows = 8;\n  const int a_cols = 8;\n  const float a_min = -2164.25f;\n  const float a_max = 2006.27f;\n  Tensor a_float(DT_FLOAT, {a_rows, a_cols});\n  test::FillValues<float>(\n      &a_float,\n      {-1014.12, -157.382, -810.17,  1435.28,  1016.37,  219.684,  -316.054,\n       -2164.25, 2006.27,  -547.444, 857.376,  404.376,  9.72115,  332.588,\n       194.385,  -286.57,  26.062,   23.1125,  110.436,  247.055,  -127.683,\n       -376.275, -124.81,  -846.826, -77.1507, 305.581,  -202.747, 12.9528,\n       9.64886,  872.686,  40.9069,  197.816,  44.16,    -306.768, -1457.52,\n       -368.939, -1049.42, -486.353, 1745.87,  95.7695,  395.773,  -254.333,\n       -404.27,  787.16,   -2.44114, 199.37,   -1024.08, 784.901,  235.055,\n       -42.7295, 241.498,  -245.365, 470.763,  186.159,  186.579,  -220.163,\n       1304.58,  386.272,  -358.853, -755.996, 360.109,  -866.007, 55.2828,\n       -508.801});\n  Tensor a_quantized = FloatTensorToQuantized<quint8>(a_float, a_min, a_max);\n\n  const int b_rows = 8;\n  const int b_cols = 8;\n  const float b_min = -0.739539f;\n  const float b_max = 0.641057f;\n  Tensor b_float(DT_FLOAT, {b_rows, b_cols});\n  test::FillValues<float>(\n      &b_float,\n      {-0.294619, -0.0670519, 0.261507,   -0.126274, 0.127229,   -0.176945,\n       -0.251223, 0.231086,   0.453694,   0.415666,  -0.288733,  0.508717,\n       0.211551,  0.0435907,  -0.582383,  -0.308779, 0.0696883,  -0.438122,\n       0.114,     0.433964,   0.109883,   0.284931,  -0.149661,  0.108657,\n       0.458333,  -0.130231,  -0.35805,   -0.123206, -0.437968,  0.0282411,\n       0.628818,  -0.0522173, -0.0233403, 0.124863,  0.217165,   0.262294,\n       -0.171005, -0.254693,  -0.200433,  -0.287354, 0.488166,   -0.0354688,\n       -0.118091, -0.590444,  0.491537,   -0.739539, 0.083117,   0.282482,\n       0.275269,  -0.36574,   0.107476,   0.0511428, -0.136887,  -0.0149852,\n       -0.259694, 0.641057,   0.264054,   -0.295126, -0.0218791, 0.361211,\n       0.012448,  0.0709718,  -0.392394,  -0.434215});\n  Tensor b_quantized = FloatTensorToQuantized<quint8>(b_float, b_min, b_max);\n\n  AddInputFromArray<quint8>(a_quantized.shape(), a_quantized.flat<quint8>());\n  AddInputFromArray<quint8>(b_quantized.shape(), b_quantized.flat<quint8>());\n  AddInputFromArray<float>(TensorShape({1}), {a_min});\n  AddInputFromArray<float>(TensorShape({1}), {a_max});\n  AddInputFromArray<float>(TensorShape({1}), {b_min});\n  AddInputFromArray<float>(TensorShape({1}), {b_max});\n  TF_ASSERT_OK(RunOpKernel());\n\n  Tensor expected_float(DT_FLOAT, {a_cols, b_cols});\n  test::FillValues<float>(\n      &expected_float,\n      {1776.82f,  421.058f,  -854.308f, 1430.65f,  503.105f,  57.2744f,\n       -1514.97f, -1163.66f, -87.0979f, -394.577f, -39.4983f, -79.1938f,\n       -329.029f, 313.475f,  446.929f,  -59.5855f, 350.837f,  238.655f,\n       -609.21f,  350.499f,  192.238f,  847.576f,  -103.177f, 185.886f,\n       -90.5335f, 200.787f,  99.1981f,  -717.076f, 763.815f,  -703.726f,\n       -125.164f, 732.325f,  -51.5303f, -418.826f, 60.0783f,  -299.658f,\n       231.41f,   72.0622f,  -289.244f, 663.776f,  391.177f,  294.415f,\n       -484.148f, -677.932f, -180.342f, -194.764f, 761.715f,  553.061f,\n       -283.355f, 321.109f,  351.269f,  1171.7f,   -857.497f, 343.804f,\n       -494.599f, -844.119f, 725.237f,  586.052f,  -735.013f, -897.723f,\n       -122.434f, -502.907f, 1264.6f,   -239.991f});\n\n  const Tensor& output_quantized = *GetOutput(0);\n  const float output_min = GetOutput(1)->flat<float>()(0);\n  const float output_max = GetOutput(2)->flat<float>()(0);\n  Tensor output_float =\n      QuantizedTensorToFloat<qint32>(output_quantized, output_min, output_max);\n  test::ExpectTensorNear<float>(expected_float, output_float, 15.0);\n}\n\n}  // namespace tensorflow\n"], "fixing_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// Implements a quantized eight-bit version of the matmul operation.\n\n#define EIGEN_USE_THREADS\n\n#define GEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\n#include \"public/gemmlowp.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/op_requires.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/kernels/meta_support.h\"\n#include \"tensorflow/core/kernels/quantization_utils.h\"\n#include \"tensorflow/core/kernels/reference_gemm.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/errors.h\"\n\nnamespace tensorflow {\n\n// We have to break this out as a separate function because there are multiple\n// combinations of transpose attributes we need to support, and they have to be\n// compile-time constants to work with the templates used internally.\ntemplate <bool TransposeA, bool TransposeB, bool TransposeC>\nvoid GemmlowpMultiply(OpKernelContext* op_context, const quint8* a_data,\n                      const quint8* b_data, qint32* c_data, int m, int n, int k,\n                      int offset_a, int offset_b, int lda, int ldb, int ldc) {\n  const uint8* a_data_as_uint8 = &(a_data->value);\n  const uint8* b_data_as_uint8 = &(b_data->value);\n  int32* c_data_as_int32 = &(c_data->value);\n  static const gemmlowp::MapOrder ResultOrder =\n      !TransposeC ? gemmlowp::MapOrder::RowMajor : gemmlowp::MapOrder::ColMajor;\n  static const gemmlowp::MapOrder LhsOrder =\n      !TransposeA ? gemmlowp::MapOrder::RowMajor : gemmlowp::MapOrder::ColMajor;\n  static const gemmlowp::MapOrder RhsOrder =\n      !TransposeB ? gemmlowp::MapOrder::RowMajor : gemmlowp::MapOrder::ColMajor;\n  gemmlowp::MatrixMap<const std::uint8_t, LhsOrder> lhs(a_data_as_uint8, m, k,\n                                                        lda);\n  gemmlowp::MatrixMap<const std::uint8_t, RhsOrder> rhs(b_data_as_uint8, k, n,\n                                                        ldb);\n  gemmlowp::MatrixMap<std::int32_t, ResultOrder> result(c_data_as_int32, m, n,\n                                                        ldc);\n  const std::tuple<> empty_pipeline = {};\n  auto& worker_threads =\n      *(op_context->device()->tensorflow_cpu_worker_threads());\n  TensorflowGemmContext context(worker_threads.num_threads,\n                                worker_threads.workers);\n  gemmlowp::GemmWithOutputPipeline<std::uint8_t, std::int32_t,\n                                   gemmlowp::DefaultL8R8BitDepthParams>(\n      &context, lhs, rhs, &result, -offset_a, -offset_b, empty_pipeline);\n  // Since gemmlowp uses assembly to write to the output, msan won't detect\n  // the output buffer as written to, so we mark it manually.\n  TF_ANNOTATE_MEMORY_IS_INITIALIZED(c_data_as_int32, m * n * sizeof(int32));\n}\n\ntemplate <class T1, class T2, class Toutput>\nclass QuantizedMatMulOp : public OpKernel {\n public:\n  explicit QuantizedMatMulOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"transpose_a\", &transpose_a_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"transpose_b\", &transpose_b_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& a = context->input(0);\n    const Tensor& b = context->input(1);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(context->input(2).shape()),\n                errors::InvalidArgument(\"min_a must be a scalar, but got shape\",\n                                        context->input(2).shape()));\n    const float min_a = context->input(2).flat<float>()(0);\n    OP_REQUIRES(context, context->input(3).NumElements() == 1,\n                errors::InvalidArgument(\"max_a must be a scalar, but got shape\",\n                                        context->input(3).shape()));\n    const float max_a = context->input(3).flat<float>()(0);\n    OP_REQUIRES(context, context->input(4).NumElements() == 1,\n                errors::InvalidArgument(\"min_b must be a scalar, but got shape\",\n                                        context->input(4).shape()));\n    const float min_b = context->input(4).flat<float>()(0);\n    OP_REQUIRES(context, context->input(5).NumElements() == 1,\n                errors::InvalidArgument(\"max_b must be a scalar, but got shape\",\n                                        context->input(5).shape()));\n    const float max_b = context->input(5).flat<float>()(0);\n\n    // Make sure that we have valid quantization ranges for the input buffers.\n    // If the difference between the min and max is negative or zero, it makes\n    // it hard to do meaningful intermediate operations on the values.\n    OP_REQUIRES(context, (max_a > min_a),\n                errors::InvalidArgument(\"max_a must be larger than min_a.\"));\n    OP_REQUIRES(context, (max_b > min_b),\n                errors::InvalidArgument(\"max_b must be larger than min_b.\"));\n    const int32_t offset_a = FloatToQuantizedUnclamped<T1>(0.0f, min_a, max_a);\n    const int32_t offset_b = FloatToQuantizedUnclamped<T2>(0.0f, min_b, max_b);\n    const int32_t offset_c = 0;\n    const int32_t mult_c = 1;\n    const int32_t shift_c = 0;\n\n    // Check that the dimensions of the two matrices are valid.\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(a.shape()),\n                errors::InvalidArgument(\"In[0] is not a matrix\"));\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(b.shape()),\n                errors::InvalidArgument(\"In[1] is not a matrix\"));\n    Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\n    dim_pair[0].first = transpose_a_ ? 0 : 1;\n    dim_pair[0].second = transpose_b_ ? 1 : 0;\n\n    OP_REQUIRES(context,\n                a.dim_size(dim_pair[0].first) == b.dim_size(dim_pair[0].second),\n                errors::InvalidArgument(\"Matrix size-incompatible: In[0]: \",\n                                        a.shape().DebugString(),\n                                        \", In[1]: \", b.shape().DebugString()));\n\n    OP_REQUIRES(context, ((shift_c >= 0) && (shift_c <= 31)),\n                errors::InvalidArgument(\"shift_c must be between 0 and 31, \"\n                                        \"inclusive.\"));\n\n    int a_dim_remaining = 1 - dim_pair[0].first;\n    int b_dim_remaining = 1 - dim_pair[0].second;\n    TensorShape out_shape(\n        {a.dim_size(a_dim_remaining), b.dim_size(b_dim_remaining)});\n    Tensor* c = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &c));\n    CHECK(c);\n\n    const T1* a_data = a.flat<T1>().data();\n    const T2* b_data = b.flat<T2>().data();\n    Toutput* c_data = c->flat<Toutput>().data();\n\n    const bool transpose_c = false;\n    const size_t m = a.dim_size(a_dim_remaining);\n    const size_t n = b.dim_size(b_dim_remaining);\n    const size_t k = a.dim_size(dim_pair[0].first);\n    const size_t lda = a.dim_size(1);\n    const size_t ldb = b.dim_size(1);\n    const size_t ldc = n;\n\n    if (meta::IsSupportedAndEnabled() && std::is_same<T1, quint8>() &&\n        std::is_same<T2, quint8>() && std::is_same<Toutput, qint32>() &&\n        (offset_c == 0) && (mult_c == 1) && (shift_c == 0) &&\n        (transpose_c == false) && (k <= 2048)) {\n      // Gemmlowp/meta code path works on 32 & 64 bit Arm with NEON Simd and\n      // allows optimized quantized 8bit to 32bit gemm.\n      meta::QuantizedGemm(context, transpose_a_, transpose_b_, a_data, b_data,\n                          c_data, m, n, k, -offset_a, -offset_b, lda, ldb, ldc);\n    } else if (std::is_same<T1, quint8>() && std::is_same<T2, quint8>() &&\n               std::is_same<Toutput, qint32>() && (offset_c == 0) &&\n               (mult_c == 1) && (shift_c == 0) && (transpose_c == false)) {\n      // The gemmlowp optimized library only works for a particular set of data\n      // types, so check if we meet those requirements and fall back to a slower\n      // reference implementation if not.\n      if (transpose_a_) {\n        if (transpose_b_) {\n          GemmlowpMultiply<true, true, false>(context, a_data, b_data, c_data,\n                                              m, n, k, offset_a, offset_b, lda,\n                                              ldb, ldc);\n        } else {\n          GemmlowpMultiply<true, false, false>(context, a_data, b_data, c_data,\n                                               m, n, k, offset_a, offset_b, lda,\n                                               ldb, ldc);\n        }\n      } else {\n        if (transpose_b_) {\n          GemmlowpMultiply<false, true, false>(context, a_data, b_data, c_data,\n                                               m, n, k, offset_a, offset_b, lda,\n                                               ldb, ldc);\n        } else {\n          GemmlowpMultiply<false, false, false>(context, a_data, b_data, c_data,\n                                                m, n, k, offset_a, offset_b,\n                                                lda, ldb, ldc);\n        }\n      }\n    } else {\n      ReferenceGemm<T1, T2, Toutput>(\n          transpose_a_, transpose_b_, transpose_c, m, n, k, a_data, offset_a,\n          lda, b_data, offset_b, ldb, c_data, shift_c, offset_c, mult_c, ldc);\n    }\n\n    float min_c_value;\n    float max_c_value;\n    QuantizationRangeForMultiplication<T1, T2, Toutput>(\n        min_a, max_a, min_b, max_b, &min_c_value, &max_c_value);\n    Tensor* c_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &c_min));\n    c_min->flat<float>()(0) = min_c_value;\n\n    Tensor* c_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &c_max));\n    c_max->flat<float>()(0) = max_c_value;\n  }\n\n private:\n  bool transpose_a_;\n  bool transpose_b_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"QuantizedMatMul\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint8>(\"T1\")\n                            .TypeConstraint<quint8>(\"T2\")\n                            .TypeConstraint<qint32>(\"Toutput\"),\n                        QuantizedMatMulOp<quint8, quint8, qint32>);\n\n}  // namespace tensorflow\n", "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#include <functional>\n#include <memory>\n#include <vector>\n\n#include \"tensorflow/core/framework/allocator.h\"\n#include \"tensorflow/core/framework/fake_input.h\"\n#include \"tensorflow/core/framework/node_def_builder.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_testutil.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/framework/types.pb.h\"\n#include \"tensorflow/core/kernels/ops_testutil.h\"\n#include \"tensorflow/core/kernels/ops_util.h\"\n#include \"tensorflow/core/kernels/quantization_utils.h\"\n#include \"tensorflow/core/lib/core/status_test_util.h\"\n#include \"tensorflow/core/platform/test.h\"\n\nnamespace tensorflow {\n\nclass QuantizedMatMulTest : public OpsTestBase {\n protected:\n};\n\n// Runs two small matrices through the operator, and leaves all the parameters\n// at their default values.\nTEST_F(QuantizedMatMulTest, Small_NoParams) {\n  TF_ASSERT_OK(NodeDefBuilder(\"quantized_mat_mul_op\", \"QuantizedMatMul\")\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Attr(\"Toutput\", DataTypeToEnum<qint32>::v())\n                   .Finalize(node_def()));\n  TF_ASSERT_OK(InitOp());\n  // A matrix is:\n  // |  1 |  2 |  3 |\n  // |  4 |  5 |  6 |\n  AddInputFromArray<quint8>(TensorShape({2, 3}), {1, 2, 3, 4, 5, 6});\n  // B matrix is:\n  // |  7 |  8 |  9 | 10 |\n  // | 11 | 12 | 13 | 14 |\n  // | 15 | 16 | 17 | 18 |\n  AddInputFromArray<quint8>(TensorShape({3, 4}),\n                            {7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18});\n  AddInputFromArray<float>(TensorShape({}), {0});\n  AddInputFromArray<float>(TensorShape({}), {255.0f});\n  AddInputFromArray<float>(TensorShape({}), {0});\n  AddInputFromArray<float>(TensorShape({}), {255.0f});\n\n  TF_ASSERT_OK(RunOpKernel());\n  // Here are the results we expect, from hand calculations:\n  // (1 * 7) + (2 * 11) + (3 * 15) = 74\n  // (1 * 8) + (2 * 12) + (3 * 16) = 80\n  // (1 * 9) + (2 * 13) + (3 * 17) = 86\n  // (1 * 10) + (2 * 14) + (3 * 18) = 92\n  // (4 * 7) + (5 * 11) + (6 * 15) = 173\n  // (4 * 8) + (5 * 12) + (6 * 16) = 188\n  // (4 * 9) + (5 * 13) + (6 * 17) = 203\n  // (4 * 10) + (5 * 14) + (6 * 18) = 218\n  Tensor expected(allocator(), DT_QINT32, TensorShape({2, 4}));\n  test::FillValues<qint32>(&expected, {74, 80, 86, 92, 173, 188, 203, 218});\n  test::ExpectTensorEqual<qint32>(expected, *GetOutput(0));\n}\n\n// This test multiplies two 1x1 8bit matrices, and compares the\n// results with hand-calculated expectations.\nTEST_F(QuantizedMatMulTest, VerySmall_WithParams) {\n  // These parameters reflect a typical production usage of eight-bit matmuls\n  // in an Inception-style network.\n  const bool transpose_a = true;\n  const int a_rows = 1;\n  const int a_cols = 1;\n  const int b_rows = 1;\n  const int b_cols = 1;\n  const bool transpose_b = false;\n  TF_ASSERT_OK(NodeDefBuilder(\"quantized_mat_mul_op\", \"QuantizedMatMul\")\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Attr(\"Toutput\", DataTypeToEnum<qint32>::v())\n                   .Attr(\"transpose_a\", transpose_a)\n                   .Attr(\"transpose_b\", transpose_b)\n                   .Finalize(node_def()));\n  TF_ASSERT_OK(InitOp());\n  // The A matrix is:\n  // |  -1 |\n  // The input array only contains unsigned bytes, so we specify the actual\n  // values as n+a_offset, where a_offset is 12 above. For example that means -1\n  // is represented as -1 + 12, or 11.\n  // We have set the transpose_a flag to true, so the matrix is transposed, and\n  // for filling the values the in-memory storage order is effectively\n  // column major, rather than the default row-major.\n  AddInputFromArray<quint8>(TensorShape({a_rows, a_cols}), {11});\n\n  // The B matrix is:\n  // |   1 |\n  AddInputFromArray<quint8>(TensorShape({b_rows, b_cols}), {0});\n  AddInputFromArray<float>(TensorShape({}), {-12.0f});\n  AddInputFromArray<float>(TensorShape({}), {243.0f});\n  AddInputFromArray<float>(TensorShape({}), {1.0f});\n  AddInputFromArray<float>(TensorShape({}), {256.0f});\n  TF_ASSERT_OK(RunOpKernel());\n  // We're requesting C = A.transposed() * B,\n  // so we expect to get these results:\n  // 1*-1 = -1\n  // | -1 |\n  Tensor expected(allocator(), DT_QINT32, TensorShape({a_cols, b_cols}));\n  test::FillValues<qint32>(&expected, {-1});\n  test::ExpectTensorEqual<qint32>(expected, *GetOutput(0));\n}\n\n// This test multiplies two 1x1 8bit matrices, but sets an invalid quantization\n// range, so we expect to get an error\nTEST_F(QuantizedMatMulTest, VerySmall_BadRange) {\n  // These parameters reflect a typical production usage of eight-bit matmuls\n  // in an Inception-style network.\n  const bool transpose_a = true;\n  const int a_rows = 1;\n  const int a_cols = 1;\n  const int b_rows = 1;\n  const int b_cols = 1;\n  const bool transpose_b = false;\n  TF_ASSERT_OK(NodeDefBuilder(\"quantized_mat_mul_op\", \"QuantizedMatMul\")\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Attr(\"Toutput\", DataTypeToEnum<qint32>::v())\n                   .Attr(\"transpose_a\", transpose_a)\n                   .Attr(\"transpose_b\", transpose_b)\n                   .Finalize(node_def()));\n  TF_ASSERT_OK(InitOp());\n  // The A matrix is:\n  // |  -1 |\n  AddInputFromArray<quint8>(TensorShape({a_rows, a_cols}), {11});\n\n  // The B matrix is:\n  // |   1 |\n  AddInputFromArray<quint8>(TensorShape({b_rows, b_cols}), {0});\n  AddInputFromArray<float>(TensorShape({}), {-12.0f});\n  AddInputFromArray<float>(TensorShape({}), {243.0f});\n  // Here we set the range so that the min and max are equal, so we expect to\n  // see an error when we run.\n  AddInputFromArray<float>(TensorShape({}), {1.0f});\n  AddInputFromArray<float>(TensorShape({}), {1.0f});\n  EXPECT_EQ(::tensorflow::error::INVALID_ARGUMENT, RunOpKernel().code());\n}\n\n// This test multiplies two 1x1 8bit matrices, but sets invalid quantized min\n// and max values, so we expect to get an error\nTEST_F(QuantizedMatMulTest, VerySmall_BadMinMax) {\n  // These parameters reflect a typical production usage of eight-bit matmuls\n  // in an Inception-style network.\n  const bool transpose_a = true;\n  const int a_rows = 1;\n  const int a_cols = 1;\n  const int b_rows = 1;\n  const int b_cols = 1;\n  const bool transpose_b = false;\n  TF_ASSERT_OK(NodeDefBuilder(\"quantized_mat_mul_op\", \"QuantizedMatMul\")\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Attr(\"Toutput\", DataTypeToEnum<qint32>::v())\n                   .Attr(\"transpose_a\", transpose_a)\n                   .Attr(\"transpose_b\", transpose_b)\n                   .Finalize(node_def()));\n  TF_ASSERT_OK(InitOp());\n  // The A matrix is:\n  // |  -1 |\n  AddInputFromArray<quint8>(TensorShape({a_rows, a_cols}), {11});\n  // The B matrix is:\n  // |   1 |\n  AddInputFromArray<quint8>(TensorShape({b_rows, b_cols}), {0});\n  // Here we set the error of a non scalar min_a value, so we expect to see an\n  // error when we run.\n  AddInputFromArray<float>(TensorShape({1}), {2});\n  AddInputFromArray<float>(TensorShape({}), {243.0f});\n  AddInputFromArray<float>(TensorShape({}), {1.0f});\n  AddInputFromArray<float>(TensorShape({}), {256.0f});\n  EXPECT_EQ(::tensorflow::error::INVALID_ARGUMENT, RunOpKernel().code());\n}\n\n// This test multiplies a couple of small 8-bit matrices, and compares the\n// results with hand-calculated expectations. It uses shifts and offsets to\n// control the range of the outputs.\nTEST_F(QuantizedMatMulTest, Small_WithParams) {\n  // These parameters reflect a typical production usage of eight-bit matmuls\n  // in an Inception-style network.\n  const bool transpose_a = true;\n  const int a_rows = 3;\n  const int a_cols = 4;\n  const int b_rows = 3;\n  const int b_cols = 2;\n  const bool transpose_b = false;\n  TF_ASSERT_OK(NodeDefBuilder(\"quantized_mat_mul_op\", \"QuantizedMatMul\")\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Attr(\"Toutput\", DataTypeToEnum<qint32>::v())\n                   .Attr(\"transpose_a\", transpose_a)\n                   .Attr(\"transpose_b\", transpose_b)\n                   .Finalize(node_def()));\n  TF_ASSERT_OK(InitOp());\n  // The A matrix is:\n  // |  -1 |  -5 |  -9 |\n  // |  -2 |  -6 | -10 |\n  // |  -3 |  -7 | -11 |\n  // |  -4 |  -8 | -12 |\n  // The input array only contains unsigned bytes, so we specify the actual\n  // values as n+a_offset, where a_offset is 12 above. For example that means -1\n  // is represented as -1 + 12, or 11.\n  // We have set the transpose_a flag to true, so the matrix is transposed, and\n  // for filling the values the in-memory storage order is effectively\n  // column major, rather than the default row-major.\n  AddInputFromArray<quint8>(TensorShape({a_rows, a_cols}), {\n                                                               11,\n                                                               10,\n                                                               9,\n                                                               8,\n                                                               7,\n                                                               6,\n                                                               5,\n                                                               4,\n                                                               3,\n                                                               2,\n                                                               1,\n                                                               0,\n                                                           });\n\n  // The B matrix is:\n  // |   1 |   4|\n  // |   2 |   5|\n  // |   3 |   6|\n  AddInputFromArray<quint8>(TensorShape({b_rows, b_cols}), {\n                                                               1,\n                                                               4,\n                                                               2,\n                                                               5,\n                                                               3,\n                                                               6,\n                                                           });\n  AddInputFromArray<float>(TensorShape({}), {-12.0f});\n  AddInputFromArray<float>(TensorShape({}), {243.0f});\n  AddInputFromArray<float>(TensorShape({}), {0});\n  AddInputFromArray<float>(TensorShape({}), {255.0f});\n  TF_ASSERT_OK(RunOpKernel());\n  // We're requesting C = A.transposed() * B,\n  // so we expect to get these results:\n  // 1*-1 + 2*-5 + 3*-9 = -38\n  // 4*-1 + 5*-5 + 6*-9 = -83\n  // 1*-2 + 2*-6 + 3*-10 = -44\n  // 4*-2 + 5*-6 + 6*-10 = -98\n  // 1*-3 + 2*-7 + 3*-11 = -50\n  // 4*-3 + 5*-7 + 6*-11 = -113\n  // 1*-4 + 2*-8 + 3*-12 = -56\n  // 4*-4 + 5*-8 + 6*-12 = -128\n  // |  -38 |  -83 |\n  // |  -44 |  -98 |\n  // |  -50 | -113 |\n  // |  -56 | -128 |\n  Tensor expected(allocator(), DT_QINT32, TensorShape({a_cols, b_cols}));\n  test::FillValues<qint32>(&expected, {\n                                          -38,\n                                          -83,\n                                          -44,\n                                          -98,\n                                          -50,\n                                          -113,\n                                          -56,\n                                          -128,\n                                      });\n  test::ExpectTensorEqual<qint32>(expected, *GetOutput(0));\n}\n\n// This test multiplies a couple of medium-sized 8-bit matrices, and tests the\n// results against what we saw from running a float MatMul with equivalent\n// inputs.\nTEST_F(QuantizedMatMulTest, Medium_WithParams) {\n  const bool transpose_a = true;\n  const bool transpose_b = false;\n  TF_ASSERT_OK(NodeDefBuilder(\"quantized_mat_mul_op\", \"QuantizedMatMul\")\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Attr(\"Toutput\", DataTypeToEnum<qint32>::v())\n                   .Attr(\"transpose_a\", transpose_a)\n                   .Attr(\"transpose_b\", transpose_b)\n                   .Finalize(node_def()));\n  TF_ASSERT_OK(InitOp());\n\n  const int a_rows = 8;\n  const int a_cols = 8;\n  const float a_min = -2164.25f;\n  const float a_max = 2006.27f;\n  Tensor a_float(DT_FLOAT, {a_rows, a_cols});\n  test::FillValues<float>(\n      &a_float,\n      {-1014.12, -157.382, -810.17,  1435.28,  1016.37,  219.684,  -316.054,\n       -2164.25, 2006.27,  -547.444, 857.376,  404.376,  9.72115,  332.588,\n       194.385,  -286.57,  26.062,   23.1125,  110.436,  247.055,  -127.683,\n       -376.275, -124.81,  -846.826, -77.1507, 305.581,  -202.747, 12.9528,\n       9.64886,  872.686,  40.9069,  197.816,  44.16,    -306.768, -1457.52,\n       -368.939, -1049.42, -486.353, 1745.87,  95.7695,  395.773,  -254.333,\n       -404.27,  787.16,   -2.44114, 199.37,   -1024.08, 784.901,  235.055,\n       -42.7295, 241.498,  -245.365, 470.763,  186.159,  186.579,  -220.163,\n       1304.58,  386.272,  -358.853, -755.996, 360.109,  -866.007, 55.2828,\n       -508.801});\n  Tensor a_quantized = FloatTensorToQuantized<quint8>(a_float, a_min, a_max);\n\n  const int b_rows = 8;\n  const int b_cols = 8;\n  const float b_min = -0.739539f;\n  const float b_max = 0.641057f;\n  Tensor b_float(DT_FLOAT, {b_rows, b_cols});\n  test::FillValues<float>(\n      &b_float,\n      {-0.294619, -0.0670519, 0.261507,   -0.126274, 0.127229,   -0.176945,\n       -0.251223, 0.231086,   0.453694,   0.415666,  -0.288733,  0.508717,\n       0.211551,  0.0435907,  -0.582383,  -0.308779, 0.0696883,  -0.438122,\n       0.114,     0.433964,   0.109883,   0.284931,  -0.149661,  0.108657,\n       0.458333,  -0.130231,  -0.35805,   -0.123206, -0.437968,  0.0282411,\n       0.628818,  -0.0522173, -0.0233403, 0.124863,  0.217165,   0.262294,\n       -0.171005, -0.254693,  -0.200433,  -0.287354, 0.488166,   -0.0354688,\n       -0.118091, -0.590444,  0.491537,   -0.739539, 0.083117,   0.282482,\n       0.275269,  -0.36574,   0.107476,   0.0511428, -0.136887,  -0.0149852,\n       -0.259694, 0.641057,   0.264054,   -0.295126, -0.0218791, 0.361211,\n       0.012448,  0.0709718,  -0.392394,  -0.434215});\n  Tensor b_quantized = FloatTensorToQuantized<quint8>(b_float, b_min, b_max);\n\n  AddInputFromArray<quint8>(a_quantized.shape(), a_quantized.flat<quint8>());\n  AddInputFromArray<quint8>(b_quantized.shape(), b_quantized.flat<quint8>());\n  AddInputFromArray<float>(TensorShape({}), {a_min});\n  AddInputFromArray<float>(TensorShape({}), {a_max});\n  AddInputFromArray<float>(TensorShape({}), {b_min});\n  AddInputFromArray<float>(TensorShape({}), {b_max});\n  TF_ASSERT_OK(RunOpKernel());\n\n  Tensor expected_float(DT_FLOAT, {a_cols, b_cols});\n  test::FillValues<float>(\n      &expected_float,\n      {1776.82f,  421.058f,  -854.308f, 1430.65f,  503.105f,  57.2744f,\n       -1514.97f, -1163.66f, -87.0979f, -394.577f, -39.4983f, -79.1938f,\n       -329.029f, 313.475f,  446.929f,  -59.5855f, 350.837f,  238.655f,\n       -609.21f,  350.499f,  192.238f,  847.576f,  -103.177f, 185.886f,\n       -90.5335f, 200.787f,  99.1981f,  -717.076f, 763.815f,  -703.726f,\n       -125.164f, 732.325f,  -51.5303f, -418.826f, 60.0783f,  -299.658f,\n       231.41f,   72.0622f,  -289.244f, 663.776f,  391.177f,  294.415f,\n       -484.148f, -677.932f, -180.342f, -194.764f, 761.715f,  553.061f,\n       -283.355f, 321.109f,  351.269f,  1171.7f,   -857.497f, 343.804f,\n       -494.599f, -844.119f, 725.237f,  586.052f,  -735.013f, -897.723f,\n       -122.434f, -502.907f, 1264.6f,   -239.991f});\n\n  const Tensor& output_quantized = *GetOutput(0);\n  const float output_min = GetOutput(1)->flat<float>()(0);\n  const float output_max = GetOutput(2)->flat<float>()(0);\n  Tensor output_float =\n      QuantizedTensorToFloat<qint32>(output_quantized, output_min, output_max);\n  test::ExpectTensorNear<float>(expected_float, output_float, 15.0);\n}\n\n}  // namespace tensorflow\n"], "filenames": ["tensorflow/core/kernels/quantized_matmul_op.cc", "tensorflow/core/kernels/quantized_matmul_op_test.cc"], "buggy_code_start_loc": [22, 65], "buggy_code_end_loc": [80, 333], "fixing_code_start_loc": [23, 65], "fixing_code_end_loc": [96, 371], "type": "NVD-CWE-noinfo", "message": "TensorFlow is an open source platform for machine learning. If `QuantizedMatMul` is given nonscalar input for: `min_a`, `max_a`, `min_b`, or `max_b` It gives a segfault that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit aca766ac7693bf29ed0df55ad6bfcc78f35e7f48. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.", "other": {"cve": {"id": "CVE-2022-35973", "sourceIdentifier": "security-advisories@github.com", "published": "2022-09-16T21:15:09.490", "lastModified": "2022-09-20T19:12:46.990", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. If `QuantizedMatMul` is given nonscalar input for: `min_a`, `max_a`, `min_b`, or `max_b` It gives a segfault that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit aca766ac7693bf29ed0df55ad6bfcc78f35e7f48. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. Si \"QuantizedMatMul\" recibe una entrada no escalar para: \"min_a\", \"max_a\", \"min_b\", o \"max_b\" resulta en un segfault que puede ser usado para desencadenar un ataque de denegaci\u00f3n de servicio. Hemos parcheado el problema en el commit de GitHub aca766ac7693bf29ed0df55ad6bfcc78f35e7f48. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.10.0. Tambi\u00e9n seleccionaremos este compromiso en TensorFlow versi\u00f3n 2.9.1, TensorFlow versi\u00f3n 2.8.1, y TensorFlow versi\u00f3n 2.7.2, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido. No se presentan mitigaciones conocidas para este problema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.7.0", "versionEndExcluding": "2.7.2", "matchCriteriaId": "C4DFBF2D-5283-42F6-8800-D653BFA5CE82"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.8.0", "versionEndExcluding": "2.8.1", "matchCriteriaId": "0F9D273D-02DC-441E-AA91-EAC8DEAA4B44"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.9.0", "versionEndExcluding": "2.9.1", "matchCriteriaId": "FE4F8A81-6CC2-4F7F-9602-C170FDD926E7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc0:*:*:*:*:*:*", "matchCriteriaId": "1DBFBCE2-0A01-4575-BE45-6775ABFB8B28"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc1:*:*:*:*:*:*", "matchCriteriaId": "89806CF9-E423-4CA6-A01A-8175C260CB24"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc2:*:*:*:*:*:*", "matchCriteriaId": "F2B80690-A257-4E16-BD27-9AE045BC56ED"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc3:*:*:*:*:*:*", "matchCriteriaId": "F335F9A4-5AB8-4E53-BC18-E01F7C653E5E"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/aca766ac7693bf29ed0df55ad6bfcc78f35e7f48", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-689c-r7h2-fv9v", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/aca766ac7693bf29ed0df55ad6bfcc78f35e7f48"}}