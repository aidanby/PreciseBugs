{"buggy_code": ["/*\n *  arch/arm/include/asm/thread_info.h\n *\n *  Copyright (C) 2002 Russell King.\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#ifndef __ASM_ARM_THREAD_INFO_H\n#define __ASM_ARM_THREAD_INFO_H\n\n#ifdef __KERNEL__\n\n#include <linux/compiler.h>\n#include <asm/fpstate.h>\n\n#define THREAD_SIZE_ORDER\t1\n#define THREAD_SIZE\t\t8192\n#define THREAD_START_SP\t\t(THREAD_SIZE - 8)\n\n#ifndef __ASSEMBLY__\n\nstruct task_struct;\nstruct exec_domain;\n\n#include <asm/types.h>\n#include <asm/domain.h>\n\ntypedef unsigned long mm_segment_t;\n\nstruct cpu_context_save {\n\t__u32\tr4;\n\t__u32\tr5;\n\t__u32\tr6;\n\t__u32\tr7;\n\t__u32\tr8;\n\t__u32\tr9;\n\t__u32\tsl;\n\t__u32\tfp;\n\t__u32\tsp;\n\t__u32\tpc;\n\t__u32\textra[2];\t\t/* Xscale 'acc' register, etc */\n};\n\n/*\n * low level task data that entry.S needs immediate access to.\n * __switch_to() assumes cpu_context follows immediately after cpu_domain.\n */\nstruct thread_info {\n\tunsigned long\t\tflags;\t\t/* low level flags */\n\tint\t\t\tpreempt_count;\t/* 0 => preemptable, <0 => bug */\n\tmm_segment_t\t\taddr_limit;\t/* address limit */\n\tstruct task_struct\t*task;\t\t/* main task structure */\n\tstruct exec_domain\t*exec_domain;\t/* execution domain */\n\t__u32\t\t\tcpu;\t\t/* cpu */\n\t__u32\t\t\tcpu_domain;\t/* cpu domain */\n\tstruct cpu_context_save\tcpu_context;\t/* cpu context */\n\t__u32\t\t\tsyscall;\t/* syscall number */\n\t__u8\t\t\tused_cp[16];\t/* thread used copro */\n\tunsigned long\t\ttp_value;\n#ifdef CONFIG_CRUNCH\n\tstruct crunch_state\tcrunchstate;\n#endif\n\tunion fp_state\t\tfpstate __attribute__((aligned(8)));\n\tunion vfp_state\t\tvfpstate;\n#ifdef CONFIG_ARM_THUMBEE\n\tunsigned long\t\tthumbee_state;\t/* ThumbEE Handler Base register */\n#endif\n\tstruct restart_block\trestart_block;\n};\n\n#define INIT_THREAD_INFO(tsk)\t\t\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\t.task\t\t= &tsk,\t\t\t\t\t\t\\\n\t.exec_domain\t= &default_exec_domain,\t\t\t\t\\\n\t.flags\t\t= 0,\t\t\t\t\t\t\\\n\t.preempt_count\t= INIT_PREEMPT_COUNT,\t\t\t\t\\\n\t.addr_limit\t= KERNEL_DS,\t\t\t\t\t\\\n\t.cpu_domain\t= domain_val(DOMAIN_USER, DOMAIN_MANAGER) |\t\\\n\t\t\t  domain_val(DOMAIN_KERNEL, DOMAIN_MANAGER) |\t\\\n\t\t\t  domain_val(DOMAIN_IO, DOMAIN_CLIENT),\t\t\\\n\t.restart_block\t= {\t\t\t\t\t\t\\\n\t\t.fn\t= do_no_restart_syscall,\t\t\t\\\n\t},\t\t\t\t\t\t\t\t\\\n}\n\n#define init_thread_info\t(init_thread_union.thread_info)\n#define init_stack\t\t(init_thread_union.stack)\n\n/*\n * how to get the thread information struct from C\n */\nstatic inline struct thread_info *current_thread_info(void) __attribute_const__;\n\nstatic inline struct thread_info *current_thread_info(void)\n{\n\tregister unsigned long sp asm (\"sp\");\n\treturn (struct thread_info *)(sp & ~(THREAD_SIZE - 1));\n}\n\n#define thread_saved_pc(tsk)\t\\\n\t((unsigned long)(task_thread_info(tsk)->cpu_context.pc))\n#define thread_saved_sp(tsk)\t\\\n\t((unsigned long)(task_thread_info(tsk)->cpu_context.sp))\n#define thread_saved_fp(tsk)\t\\\n\t((unsigned long)(task_thread_info(tsk)->cpu_context.fp))\n\nextern void crunch_task_disable(struct thread_info *);\nextern void crunch_task_copy(struct thread_info *, void *);\nextern void crunch_task_restore(struct thread_info *, void *);\nextern void crunch_task_release(struct thread_info *);\n\nextern void iwmmxt_task_disable(struct thread_info *);\nextern void iwmmxt_task_copy(struct thread_info *, void *);\nextern void iwmmxt_task_restore(struct thread_info *, void *);\nextern void iwmmxt_task_release(struct thread_info *);\nextern void iwmmxt_task_switch(struct thread_info *);\n\nextern void vfp_sync_hwstate(struct thread_info *);\nextern void vfp_flush_hwstate(struct thread_info *);\n\nstruct user_vfp;\nstruct user_vfp_exc;\n\nextern int vfp_preserve_user_clear_hwstate(struct user_vfp __user *,\n\t\t\t\t\t   struct user_vfp_exc __user *);\nextern int vfp_restore_user_hwstate(struct user_vfp __user *,\n\t\t\t\t    struct user_vfp_exc __user *);\n#endif\n\n/*\n * We use bit 30 of the preempt_count to indicate that kernel\n * preemption is occurring.  See <asm/hardirq.h>.\n */\n#define PREEMPT_ACTIVE\t0x40000000\n\n/*\n * thread information flags:\n *  TIF_SYSCALL_TRACE\t- syscall trace active\n *  TIF_SYSCAL_AUDIT\t- syscall auditing active\n *  TIF_SIGPENDING\t- signal pending\n *  TIF_NEED_RESCHED\t- rescheduling necessary\n *  TIF_NOTIFY_RESUME\t- callback before returning to user\n *  TIF_USEDFPU\t\t- FPU was used by this task this quantum (SMP)\n *  TIF_POLLING_NRFLAG\t- true if poll_idle() is polling TIF_NEED_RESCHED\n */\n#define TIF_SIGPENDING\t\t0\n#define TIF_NEED_RESCHED\t1\n#define TIF_NOTIFY_RESUME\t2\t/* callback before returning to user */\n#define TIF_SYSCALL_TRACE\t8\n#define TIF_SYSCALL_AUDIT\t9\n#define TIF_SYSCALL_TRACEPOINT\t10\n#define TIF_SECCOMP\t\t11\t/* seccomp syscall filtering active */\n#define TIF_NOHZ\t\t12\t/* in adaptive nohz mode */\n#define TIF_USING_IWMMXT\t17\n#define TIF_MEMDIE\t\t18\t/* is terminating due to OOM killer */\n#define TIF_RESTORE_SIGMASK\t20\n#define TIF_SWITCH_MM\t\t22\t/* deferred switch_mm */\n\n#define _TIF_SIGPENDING\t\t(1 << TIF_SIGPENDING)\n#define _TIF_NEED_RESCHED\t(1 << TIF_NEED_RESCHED)\n#define _TIF_NOTIFY_RESUME\t(1 << TIF_NOTIFY_RESUME)\n#define _TIF_SYSCALL_TRACE\t(1 << TIF_SYSCALL_TRACE)\n#define _TIF_SYSCALL_AUDIT\t(1 << TIF_SYSCALL_AUDIT)\n#define _TIF_SYSCALL_TRACEPOINT\t(1 << TIF_SYSCALL_TRACEPOINT)\n#define _TIF_SECCOMP\t\t(1 << TIF_SECCOMP)\n#define _TIF_USING_IWMMXT\t(1 << TIF_USING_IWMMXT)\n\n/* Checks for any syscall work in entry-common.S */\n#define _TIF_SYSCALL_WORK (_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | \\\n\t\t\t   _TIF_SYSCALL_TRACEPOINT | _TIF_SECCOMP)\n\n/*\n * Change these and you break ASM code in entry-common.S\n */\n#define _TIF_WORK_MASK\t\t(_TIF_NEED_RESCHED | _TIF_SIGPENDING | _TIF_NOTIFY_RESUME)\n\n#endif /* __KERNEL__ */\n#endif /* __ASM_ARM_THREAD_INFO_H */\n", "#ifndef __ASMARM_TLS_H\n#define __ASMARM_TLS_H\n\n#ifdef __ASSEMBLY__\n\t.macro set_tls_none, tp, tmp1, tmp2\n\t.endm\n\n\t.macro set_tls_v6k, tp, tmp1, tmp2\n\tmcr\tp15, 0, \\tp, c13, c0, 3\t\t@ set TLS register\n\tmov\t\\tmp1, #0\n\tmcr\tp15, 0, \\tmp1, c13, c0, 2\t@ clear user r/w TLS register\n\t.endm\n\n\t.macro set_tls_v6, tp, tmp1, tmp2\n\tldr\t\\tmp1, =elf_hwcap\n\tldr\t\\tmp1, [\\tmp1, #0]\n\tmov\t\\tmp2, #0xffff0fff\n\ttst\t\\tmp1, #HWCAP_TLS\t\t@ hardware TLS available?\n\tmcrne\tp15, 0, \\tp, c13, c0, 3\t\t@ yes, set TLS register\n\tmovne\t\\tmp1, #0\n\tmcrne\tp15, 0, \\tmp1, c13, c0, 2\t@ clear user r/w TLS register\n\tstreq\t\\tp, [\\tmp2, #-15]\t\t@ set TLS value at 0xffff0ff0\n\t.endm\n\n\t.macro set_tls_software, tp, tmp1, tmp2\n\tmov\t\\tmp1, #0xffff0fff\n\tstr\t\\tp, [\\tmp1, #-15]\t\t@ set TLS value at 0xffff0ff0\n\t.endm\n#endif\n\n#ifdef CONFIG_TLS_REG_EMUL\n#define tls_emu\t\t1\n#define has_tls_reg\t\t1\n#define set_tls\t\tset_tls_none\n#elif defined(CONFIG_CPU_V6)\n#define tls_emu\t\t0\n#define has_tls_reg\t\t(elf_hwcap & HWCAP_TLS)\n#define set_tls\t\tset_tls_v6\n#elif defined(CONFIG_CPU_32v6K)\n#define tls_emu\t\t0\n#define has_tls_reg\t\t1\n#define set_tls\t\tset_tls_v6k\n#else\n#define tls_emu\t\t0\n#define has_tls_reg\t\t0\n#define set_tls\t\tset_tls_software\n#endif\n\n#endif\t/* __ASMARM_TLS_H */\n", "/*\n *  linux/arch/arm/kernel/entry-armv.S\n *\n *  Copyright (C) 1996,1997,1998 Russell King.\n *  ARM700 fix by Matthew Godbolt (linux-user@willothewisp.demon.co.uk)\n *  nommu support by Hyok S. Choi (hyok.choi@samsung.com)\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n *  Low-level vector interface routines\n *\n *  Note:  there is a StrongARM bug in the STMIA rn, {regs}^ instruction\n *  that causes it to save wrong values...  Be aware!\n */\n\n#include <asm/assembler.h>\n#include <asm/memory.h>\n#include <asm/glue-df.h>\n#include <asm/glue-pf.h>\n#include <asm/vfpmacros.h>\n#ifndef CONFIG_MULTI_IRQ_HANDLER\n#include <mach/entry-macro.S>\n#endif\n#include <asm/thread_notify.h>\n#include <asm/unwind.h>\n#include <asm/unistd.h>\n#include <asm/tls.h>\n#include <asm/system_info.h>\n\n#include \"entry-header.S\"\n#include <asm/entry-macro-multi.S>\n\n/*\n * Interrupt handling.\n */\n\t.macro\tirq_handler\n#ifdef CONFIG_MULTI_IRQ_HANDLER\n\tldr\tr1, =handle_arch_irq\n\tmov\tr0, sp\n\tadr\tlr, BSYM(9997f)\n\tldr\tpc, [r1]\n#else\n\tarch_irq_handler_default\n#endif\n9997:\n\t.endm\n\n\t.macro\tpabt_helper\n\t@ PABORT handler takes pt_regs in r2, fault address in r4 and psr in r5\n#ifdef MULTI_PABORT\n\tldr\tip, .LCprocfns\n\tmov\tlr, pc\n\tldr\tpc, [ip, #PROCESSOR_PABT_FUNC]\n#else\n\tbl\tCPU_PABORT_HANDLER\n#endif\n\t.endm\n\n\t.macro\tdabt_helper\n\n\t@\n\t@ Call the processor-specific abort handler:\n\t@\n\t@  r2 - pt_regs\n\t@  r4 - aborted context pc\n\t@  r5 - aborted context psr\n\t@\n\t@ The abort handler must return the aborted address in r0, and\n\t@ the fault status register in r1.  r9 must be preserved.\n\t@\n#ifdef MULTI_DABORT\n\tldr\tip, .LCprocfns\n\tmov\tlr, pc\n\tldr\tpc, [ip, #PROCESSOR_DABT_FUNC]\n#else\n\tbl\tCPU_DABORT_HANDLER\n#endif\n\t.endm\n\n#ifdef CONFIG_KPROBES\n\t.section\t.kprobes.text,\"ax\",%progbits\n#else\n\t.text\n#endif\n\n/*\n * Invalid mode handlers\n */\n\t.macro\tinv_entry, reason\n\tsub\tsp, sp, #S_FRAME_SIZE\n ARM(\tstmib\tsp, {r1 - lr}\t\t)\n THUMB(\tstmia\tsp, {r0 - r12}\t\t)\n THUMB(\tstr\tsp, [sp, #S_SP]\t\t)\n THUMB(\tstr\tlr, [sp, #S_LR]\t\t)\n\tmov\tr1, #\\reason\n\t.endm\n\n__pabt_invalid:\n\tinv_entry BAD_PREFETCH\n\tb\tcommon_invalid\nENDPROC(__pabt_invalid)\n\n__dabt_invalid:\n\tinv_entry BAD_DATA\n\tb\tcommon_invalid\nENDPROC(__dabt_invalid)\n\n__irq_invalid:\n\tinv_entry BAD_IRQ\n\tb\tcommon_invalid\nENDPROC(__irq_invalid)\n\n__und_invalid:\n\tinv_entry BAD_UNDEFINSTR\n\n\t@\n\t@ XXX fall through to common_invalid\n\t@\n\n@\n@ common_invalid - generic code for failed exception (re-entrant version of handlers)\n@\ncommon_invalid:\n\tzero_fp\n\n\tldmia\tr0, {r4 - r6}\n\tadd\tr0, sp, #S_PC\t\t@ here for interlock avoidance\n\tmov\tr7, #-1\t\t\t@  \"\"   \"\"    \"\"        \"\"\n\tstr\tr4, [sp]\t\t@ save preserved r0\n\tstmia\tr0, {r5 - r7}\t\t@ lr_<exception>,\n\t\t\t\t\t@ cpsr_<exception>, \"old_r0\"\n\n\tmov\tr0, sp\n\tb\tbad_mode\nENDPROC(__und_invalid)\n\n/*\n * SVC mode handlers\n */\n\n#if defined(CONFIG_AEABI) && (__LINUX_ARM_ARCH__ >= 5)\n#define SPFIX(code...) code\n#else\n#define SPFIX(code...)\n#endif\n\n\t.macro\tsvc_entry, stack_hole=0\n UNWIND(.fnstart\t\t)\n UNWIND(.save {r0 - pc}\t\t)\n\tsub\tsp, sp, #(S_FRAME_SIZE + \\stack_hole - 4)\n#ifdef CONFIG_THUMB2_KERNEL\n SPFIX(\tstr\tr0, [sp]\t)\t@ temporarily saved\n SPFIX(\tmov\tr0, sp\t\t)\n SPFIX(\ttst\tr0, #4\t\t)\t@ test original stack alignment\n SPFIX(\tldr\tr0, [sp]\t)\t@ restored\n#else\n SPFIX(\ttst\tsp, #4\t\t)\n#endif\n SPFIX(\tsubeq\tsp, sp, #4\t)\n\tstmia\tsp, {r1 - r12}\n\n\tldmia\tr0, {r3 - r5}\n\tadd\tr7, sp, #S_SP - 4\t@ here for interlock avoidance\n\tmov\tr6, #-1\t\t\t@  \"\"  \"\"      \"\"       \"\"\n\tadd\tr2, sp, #(S_FRAME_SIZE + \\stack_hole - 4)\n SPFIX(\taddeq\tr2, r2, #4\t)\n\tstr\tr3, [sp, #-4]!\t\t@ save the \"real\" r0 copied\n\t\t\t\t\t@ from the exception stack\n\n\tmov\tr3, lr\n\n\t@\n\t@ We are now ready to fill in the remaining blanks on the stack:\n\t@\n\t@  r2 - sp_svc\n\t@  r3 - lr_svc\n\t@  r4 - lr_<exception>, already fixed up for correct return/restart\n\t@  r5 - spsr_<exception>\n\t@  r6 - orig_r0 (see pt_regs definition in ptrace.h)\n\t@\n\tstmia\tr7, {r2 - r6}\n\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tbl\ttrace_hardirqs_off\n#endif\n\t.endm\n\n\t.align\t5\n__dabt_svc:\n\tsvc_entry\n\tmov\tr2, sp\n\tdabt_helper\n\tsvc_exit r5\t\t\t\t@ return from exception\n UNWIND(.fnend\t\t)\nENDPROC(__dabt_svc)\n\n\t.align\t5\n__irq_svc:\n\tsvc_entry\n\tirq_handler\n\n#ifdef CONFIG_PREEMPT\n\tget_thread_info tsk\n\tldr\tr8, [tsk, #TI_PREEMPT]\t\t@ get preempt count\n\tldr\tr0, [tsk, #TI_FLAGS]\t\t@ get flags\n\tteq\tr8, #0\t\t\t\t@ if preempt count != 0\n\tmovne\tr0, #0\t\t\t\t@ force flags to 0\n\ttst\tr0, #_TIF_NEED_RESCHED\n\tblne\tsvc_preempt\n#endif\n\n\tsvc_exit r5, irq = 1\t\t\t@ return from exception\n UNWIND(.fnend\t\t)\nENDPROC(__irq_svc)\n\n\t.ltorg\n\n#ifdef CONFIG_PREEMPT\nsvc_preempt:\n\tmov\tr8, lr\n1:\tbl\tpreempt_schedule_irq\t\t@ irq en/disable is done inside\n\tldr\tr0, [tsk, #TI_FLAGS]\t\t@ get new tasks TI_FLAGS\n\ttst\tr0, #_TIF_NEED_RESCHED\n\tmoveq\tpc, r8\t\t\t\t@ go again\n\tb\t1b\n#endif\n\n__und_fault:\n\t@ Correct the PC such that it is pointing at the instruction\n\t@ which caused the fault.  If the faulting instruction was ARM\n\t@ the PC will be pointing at the next instruction, and have to\n\t@ subtract 4.  Otherwise, it is Thumb, and the PC will be\n\t@ pointing at the second half of the Thumb instruction.  We\n\t@ have to subtract 2.\n\tldr\tr2, [r0, #S_PC]\n\tsub\tr2, r2, r1\n\tstr\tr2, [r0, #S_PC]\n\tb\tdo_undefinstr\nENDPROC(__und_fault)\n\n\t.align\t5\n__und_svc:\n#ifdef CONFIG_KPROBES\n\t@ If a kprobe is about to simulate a \"stmdb sp...\" instruction,\n\t@ it obviously needs free stack space which then will belong to\n\t@ the saved context.\n\tsvc_entry 64\n#else\n\tsvc_entry\n#endif\n\t@\n\t@ call emulation code, which returns using r9 if it has emulated\n\t@ the instruction, or the more conventional lr if we are to treat\n\t@ this as a real undefined instruction\n\t@\n\t@  r0 - instruction\n\t@\n#ifndef CONFIG_THUMB2_KERNEL\n\tldr\tr0, [r4, #-4]\n#else\n\tmov\tr1, #2\n\tldrh\tr0, [r4, #-2]\t\t\t@ Thumb instruction at LR - 2\n\tcmp\tr0, #0xe800\t\t\t@ 32-bit instruction if xx >= 0\n\tblo\t__und_svc_fault\n\tldrh\tr9, [r4]\t\t\t@ bottom 16 bits\n\tadd\tr4, r4, #2\n\tstr\tr4, [sp, #S_PC]\n\torr\tr0, r9, r0, lsl #16\n#endif\n\tadr\tr9, BSYM(__und_svc_finish)\n\tmov\tr2, r4\n\tbl\tcall_fpe\n\n\tmov\tr1, #4\t\t\t\t@ PC correction to apply\n__und_svc_fault:\n\tmov\tr0, sp\t\t\t\t@ struct pt_regs *regs\n\tbl\t__und_fault\n\n__und_svc_finish:\n\tldr\tr5, [sp, #S_PSR]\t\t@ Get SVC cpsr\n\tsvc_exit r5\t\t\t\t@ return from exception\n UNWIND(.fnend\t\t)\nENDPROC(__und_svc)\n\n\t.align\t5\n__pabt_svc:\n\tsvc_entry\n\tmov\tr2, sp\t\t\t\t@ regs\n\tpabt_helper\n\tsvc_exit r5\t\t\t\t@ return from exception\n UNWIND(.fnend\t\t)\nENDPROC(__pabt_svc)\n\n\t.align\t5\n.LCcralign:\n\t.word\tcr_alignment\n#ifdef MULTI_DABORT\n.LCprocfns:\n\t.word\tprocessor\n#endif\n.LCfp:\n\t.word\tfp_enter\n\n/*\n * User mode handlers\n *\n * EABI note: sp_svc is always 64-bit aligned here, so should S_FRAME_SIZE\n */\n\n#if defined(CONFIG_AEABI) && (__LINUX_ARM_ARCH__ >= 5) && (S_FRAME_SIZE & 7)\n#error \"sizeof(struct pt_regs) must be a multiple of 8\"\n#endif\n\n\t.macro\tusr_entry\n UNWIND(.fnstart\t)\n UNWIND(.cantunwind\t)\t@ don't unwind the user space\n\tsub\tsp, sp, #S_FRAME_SIZE\n ARM(\tstmib\tsp, {r1 - r12}\t)\n THUMB(\tstmia\tsp, {r0 - r12}\t)\n\n\tldmia\tr0, {r3 - r5}\n\tadd\tr0, sp, #S_PC\t\t@ here for interlock avoidance\n\tmov\tr6, #-1\t\t\t@  \"\"  \"\"     \"\"        \"\"\n\n\tstr\tr3, [sp]\t\t@ save the \"real\" r0 copied\n\t\t\t\t\t@ from the exception stack\n\n\t@\n\t@ We are now ready to fill in the remaining blanks on the stack:\n\t@\n\t@  r4 - lr_<exception>, already fixed up for correct return/restart\n\t@  r5 - spsr_<exception>\n\t@  r6 - orig_r0 (see pt_regs definition in ptrace.h)\n\t@\n\t@ Also, separately save sp_usr and lr_usr\n\t@\n\tstmia\tr0, {r4 - r6}\n ARM(\tstmdb\tr0, {sp, lr}^\t\t\t)\n THUMB(\tstore_user_sp_lr r0, r1, S_SP - S_PC\t)\n\n\t@\n\t@ Enable the alignment trap while in kernel mode\n\t@\n\talignment_trap r0\n\n\t@\n\t@ Clear FP to mark the first stack frame\n\t@\n\tzero_fp\n\n#ifdef CONFIG_IRQSOFF_TRACER\n\tbl\ttrace_hardirqs_off\n#endif\n\tct_user_exit save = 0\n\t.endm\n\n\t.macro\tkuser_cmpxchg_check\n#if !defined(CONFIG_CPU_32v6K) && !defined(CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG)\n#ifndef CONFIG_MMU\n#warning \"NPTL on non MMU needs fixing\"\n#else\n\t@ Make sure our user space atomic helper is restarted\n\t@ if it was interrupted in a critical region.  Here we\n\t@ perform a quick test inline since it should be false\n\t@ 99.9999% of the time.  The rest is done out of line.\n\tcmp\tr4, #TASK_SIZE\n\tblhs\tkuser_cmpxchg64_fixup\n#endif\n#endif\n\t.endm\n\n\t.align\t5\n__dabt_usr:\n\tusr_entry\n\tkuser_cmpxchg_check\n\tmov\tr2, sp\n\tdabt_helper\n\tb\tret_from_exception\n UNWIND(.fnend\t\t)\nENDPROC(__dabt_usr)\n\n\t.align\t5\n__irq_usr:\n\tusr_entry\n\tkuser_cmpxchg_check\n\tirq_handler\n\tget_thread_info tsk\n\tmov\twhy, #0\n\tb\tret_to_user_from_irq\n UNWIND(.fnend\t\t)\nENDPROC(__irq_usr)\n\n\t.ltorg\n\n\t.align\t5\n__und_usr:\n\tusr_entry\n\n\tmov\tr2, r4\n\tmov\tr3, r5\n\n\t@ r2 = regs->ARM_pc, which is either 2 or 4 bytes ahead of the\n\t@      faulting instruction depending on Thumb mode.\n\t@ r3 = regs->ARM_cpsr\n\t@\n\t@ The emulation code returns using r9 if it has emulated the\n\t@ instruction, or the more conventional lr if we are to treat\n\t@ this as a real undefined instruction\n\t@\n\tadr\tr9, BSYM(ret_from_exception)\n\n\ttst\tr3, #PSR_T_BIT\t\t\t@ Thumb mode?\n\tbne\t__und_usr_thumb\n\tsub\tr4, r2, #4\t\t\t@ ARM instr at LR - 4\n1:\tldrt\tr0, [r4]\n#ifdef CONFIG_CPU_ENDIAN_BE8\n\trev\tr0, r0\t\t\t\t@ little endian instruction\n#endif\n\t@ r0 = 32-bit ARM instruction which caused the exception\n\t@ r2 = PC value for the following instruction (:= regs->ARM_pc)\n\t@ r4 = PC value for the faulting instruction\n\t@ lr = 32-bit undefined instruction function\n\tadr\tlr, BSYM(__und_usr_fault_32)\n\tb\tcall_fpe\n\n__und_usr_thumb:\n\t@ Thumb instruction\n\tsub\tr4, r2, #2\t\t\t@ First half of thumb instr at LR - 2\n#if CONFIG_ARM_THUMB && __LINUX_ARM_ARCH__ >= 6 && CONFIG_CPU_V7\n/*\n * Thumb-2 instruction handling.  Note that because pre-v6 and >= v6 platforms\n * can never be supported in a single kernel, this code is not applicable at\n * all when __LINUX_ARM_ARCH__ < 6.  This allows simplifying assumptions to be\n * made about .arch directives.\n */\n#if __LINUX_ARM_ARCH__ < 7\n/* If the target CPU may not be Thumb-2-capable, a run-time check is needed: */\n#define NEED_CPU_ARCHITECTURE\n\tldr\tr5, .LCcpu_architecture\n\tldr\tr5, [r5]\n\tcmp\tr5, #CPU_ARCH_ARMv7\n\tblo\t__und_usr_fault_16\t\t@ 16bit undefined instruction\n/*\n * The following code won't get run unless the running CPU really is v7, so\n * coding round the lack of ldrht on older arches is pointless.  Temporarily\n * override the assembler target arch with the minimum required instead:\n */\n\t.arch\tarmv6t2\n#endif\n2:\tldrht\tr5, [r4]\n\tcmp\tr5, #0xe800\t\t\t@ 32bit instruction if xx != 0\n\tblo\t__und_usr_fault_16\t\t@ 16bit undefined instruction\n3:\tldrht\tr0, [r2]\n\tadd\tr2, r2, #2\t\t\t@ r2 is PC + 2, make it PC + 4\n\tstr\tr2, [sp, #S_PC]\t\t\t@ it's a 2x16bit instr, update\n\torr\tr0, r0, r5, lsl #16\n\tadr\tlr, BSYM(__und_usr_fault_32)\n\t@ r0 = the two 16-bit Thumb instructions which caused the exception\n\t@ r2 = PC value for the following Thumb instruction (:= regs->ARM_pc)\n\t@ r4 = PC value for the first 16-bit Thumb instruction\n\t@ lr = 32bit undefined instruction function\n\n#if __LINUX_ARM_ARCH__ < 7\n/* If the target arch was overridden, change it back: */\n#ifdef CONFIG_CPU_32v6K\n\t.arch\tarmv6k\n#else\n\t.arch\tarmv6\n#endif\n#endif /* __LINUX_ARM_ARCH__ < 7 */\n#else /* !(CONFIG_ARM_THUMB && __LINUX_ARM_ARCH__ >= 6 && CONFIG_CPU_V7) */\n\tb\t__und_usr_fault_16\n#endif\n UNWIND(.fnend)\nENDPROC(__und_usr)\n\n/*\n * The out of line fixup for the ldrt instructions above.\n */\n\t.pushsection .fixup, \"ax\"\n\t.align\t2\n4:\tmov\tpc, r9\n\t.popsection\n\t.pushsection __ex_table,\"a\"\n\t.long\t1b, 4b\n#if CONFIG_ARM_THUMB && __LINUX_ARM_ARCH__ >= 6 && CONFIG_CPU_V7\n\t.long\t2b, 4b\n\t.long\t3b, 4b\n#endif\n\t.popsection\n\n/*\n * Check whether the instruction is a co-processor instruction.\n * If yes, we need to call the relevant co-processor handler.\n *\n * Note that we don't do a full check here for the co-processor\n * instructions; all instructions with bit 27 set are well\n * defined.  The only instructions that should fault are the\n * co-processor instructions.  However, we have to watch out\n * for the ARM6/ARM7 SWI bug.\n *\n * NEON is a special case that has to be handled here. Not all\n * NEON instructions are co-processor instructions, so we have\n * to make a special case of checking for them. Plus, there's\n * five groups of them, so we have a table of mask/opcode pairs\n * to check against, and if any match then we branch off into the\n * NEON handler code.\n *\n * Emulators may wish to make use of the following registers:\n *  r0  = instruction opcode (32-bit ARM or two 16-bit Thumb)\n *  r2  = PC value to resume execution after successful emulation\n *  r9  = normal \"successful\" return address\n *  r10 = this threads thread_info structure\n *  lr  = unrecognised instruction return address\n * IRQs disabled, FIQs enabled.\n */\n\t@\n\t@ Fall-through from Thumb-2 __und_usr\n\t@\n#ifdef CONFIG_NEON\n\tget_thread_info r10\t\t\t@ get current thread\n\tadr\tr6, .LCneon_thumb_opcodes\n\tb\t2f\n#endif\ncall_fpe:\n\tget_thread_info r10\t\t\t@ get current thread\n#ifdef CONFIG_NEON\n\tadr\tr6, .LCneon_arm_opcodes\n2:\tldr\tr5, [r6], #4\t\t\t@ mask value\n\tldr\tr7, [r6], #4\t\t\t@ opcode bits matching in mask\n\tcmp\tr5, #0\t\t\t\t@ end mask?\n\tbeq\t1f\n\tand\tr8, r0, r5\n\tcmp\tr8, r7\t\t\t\t@ NEON instruction?\n\tbne\t2b\n\tmov\tr7, #1\n\tstrb\tr7, [r10, #TI_USED_CP + 10]\t@ mark CP#10 as used\n\tstrb\tr7, [r10, #TI_USED_CP + 11]\t@ mark CP#11 as used\n\tb\tdo_vfp\t\t\t\t@ let VFP handler handle this\n1:\n#endif\n\ttst\tr0, #0x08000000\t\t\t@ only CDP/CPRT/LDC/STC have bit 27\n\ttstne\tr0, #0x04000000\t\t\t@ bit 26 set on both ARM and Thumb-2\n\tmoveq\tpc, lr\n\tand\tr8, r0, #0x00000f00\t\t@ mask out CP number\n THUMB(\tlsr\tr8, r8, #8\t\t)\n\tmov\tr7, #1\n\tadd\tr6, r10, #TI_USED_CP\n ARM(\tstrb\tr7, [r6, r8, lsr #8]\t)\t@ set appropriate used_cp[]\n THUMB(\tstrb\tr7, [r6, r8]\t\t)\t@ set appropriate used_cp[]\n#ifdef CONFIG_IWMMXT\n\t@ Test if we need to give access to iWMMXt coprocessors\n\tldr\tr5, [r10, #TI_FLAGS]\n\trsbs\tr7, r8, #(1 << 8)\t\t@ CP 0 or 1 only\n\tmovcss\tr7, r5, lsr #(TIF_USING_IWMMXT + 1)\n\tbcs\tiwmmxt_task_enable\n#endif\n ARM(\tadd\tpc, pc, r8, lsr #6\t)\n THUMB(\tlsl\tr8, r8, #2\t\t)\n THUMB(\tadd\tpc, r8\t\t\t)\n\tnop\n\n\tmovw_pc\tlr\t\t\t\t@ CP#0\n\tW(b)\tdo_fpe\t\t\t\t@ CP#1 (FPE)\n\tW(b)\tdo_fpe\t\t\t\t@ CP#2 (FPE)\n\tmovw_pc\tlr\t\t\t\t@ CP#3\n#ifdef CONFIG_CRUNCH\n\tb\tcrunch_task_enable\t\t@ CP#4 (MaverickCrunch)\n\tb\tcrunch_task_enable\t\t@ CP#5 (MaverickCrunch)\n\tb\tcrunch_task_enable\t\t@ CP#6 (MaverickCrunch)\n#else\n\tmovw_pc\tlr\t\t\t\t@ CP#4\n\tmovw_pc\tlr\t\t\t\t@ CP#5\n\tmovw_pc\tlr\t\t\t\t@ CP#6\n#endif\n\tmovw_pc\tlr\t\t\t\t@ CP#7\n\tmovw_pc\tlr\t\t\t\t@ CP#8\n\tmovw_pc\tlr\t\t\t\t@ CP#9\n#ifdef CONFIG_VFP\n\tW(b)\tdo_vfp\t\t\t\t@ CP#10 (VFP)\n\tW(b)\tdo_vfp\t\t\t\t@ CP#11 (VFP)\n#else\n\tmovw_pc\tlr\t\t\t\t@ CP#10 (VFP)\n\tmovw_pc\tlr\t\t\t\t@ CP#11 (VFP)\n#endif\n\tmovw_pc\tlr\t\t\t\t@ CP#12\n\tmovw_pc\tlr\t\t\t\t@ CP#13\n\tmovw_pc\tlr\t\t\t\t@ CP#14 (Debug)\n\tmovw_pc\tlr\t\t\t\t@ CP#15 (Control)\n\n#ifdef NEED_CPU_ARCHITECTURE\n\t.align\t2\n.LCcpu_architecture:\n\t.word\t__cpu_architecture\n#endif\n\n#ifdef CONFIG_NEON\n\t.align\t6\n\n.LCneon_arm_opcodes:\n\t.word\t0xfe000000\t\t\t@ mask\n\t.word\t0xf2000000\t\t\t@ opcode\n\n\t.word\t0xff100000\t\t\t@ mask\n\t.word\t0xf4000000\t\t\t@ opcode\n\n\t.word\t0x00000000\t\t\t@ mask\n\t.word\t0x00000000\t\t\t@ opcode\n\n.LCneon_thumb_opcodes:\n\t.word\t0xef000000\t\t\t@ mask\n\t.word\t0xef000000\t\t\t@ opcode\n\n\t.word\t0xff100000\t\t\t@ mask\n\t.word\t0xf9000000\t\t\t@ opcode\n\n\t.word\t0x00000000\t\t\t@ mask\n\t.word\t0x00000000\t\t\t@ opcode\n#endif\n\ndo_fpe:\n\tenable_irq\n\tldr\tr4, .LCfp\n\tadd\tr10, r10, #TI_FPSTATE\t\t@ r10 = workspace\n\tldr\tpc, [r4]\t\t\t@ Call FP module USR entry point\n\n/*\n * The FP module is called with these registers set:\n *  r0  = instruction\n *  r2  = PC+4\n *  r9  = normal \"successful\" return address\n *  r10 = FP workspace\n *  lr  = unrecognised FP instruction return address\n */\n\n\t.pushsection .data\nENTRY(fp_enter)\n\t.word\tno_fp\n\t.popsection\n\nENTRY(no_fp)\n\tmov\tpc, lr\nENDPROC(no_fp)\n\n__und_usr_fault_32:\n\tmov\tr1, #4\n\tb\t1f\n__und_usr_fault_16:\n\tmov\tr1, #2\n1:\tenable_irq\n\tmov\tr0, sp\n\tadr\tlr, BSYM(ret_from_exception)\n\tb\t__und_fault\nENDPROC(__und_usr_fault_32)\nENDPROC(__und_usr_fault_16)\n\n\t.align\t5\n__pabt_usr:\n\tusr_entry\n\tmov\tr2, sp\t\t\t\t@ regs\n\tpabt_helper\n UNWIND(.fnend\t\t)\n\t/* fall through */\n/*\n * This is the return code to user mode for abort handlers\n */\nENTRY(ret_from_exception)\n UNWIND(.fnstart\t)\n UNWIND(.cantunwind\t)\n\tget_thread_info tsk\n\tmov\twhy, #0\n\tb\tret_to_user\n UNWIND(.fnend\t\t)\nENDPROC(__pabt_usr)\nENDPROC(ret_from_exception)\n\n/*\n * Register switch for ARMv3 and ARMv4 processors\n * r0 = previous task_struct, r1 = previous thread_info, r2 = next thread_info\n * previous and next are guaranteed not to be the same.\n */\nENTRY(__switch_to)\n UNWIND(.fnstart\t)\n UNWIND(.cantunwind\t)\n\tadd\tip, r1, #TI_CPU_SAVE\n\tldr\tr3, [r2, #TI_TP_VALUE]\n ARM(\tstmia\tip!, {r4 - sl, fp, sp, lr} )\t@ Store most regs on stack\n THUMB(\tstmia\tip!, {r4 - sl, fp}\t   )\t@ Store most regs on stack\n THUMB(\tstr\tsp, [ip], #4\t\t   )\n THUMB(\tstr\tlr, [ip], #4\t\t   )\n#ifdef CONFIG_CPU_USE_DOMAINS\n\tldr\tr6, [r2, #TI_CPU_DOMAIN]\n#endif\n\tset_tls\tr3, r4, r5\n#if defined(CONFIG_CC_STACKPROTECTOR) && !defined(CONFIG_SMP)\n\tldr\tr7, [r2, #TI_TASK]\n\tldr\tr8, =__stack_chk_guard\n\tldr\tr7, [r7, #TSK_STACK_CANARY]\n#endif\n#ifdef CONFIG_CPU_USE_DOMAINS\n\tmcr\tp15, 0, r6, c3, c0, 0\t\t@ Set domain register\n#endif\n\tmov\tr5, r0\n\tadd\tr4, r2, #TI_CPU_SAVE\n\tldr\tr0, =thread_notify_head\n\tmov\tr1, #THREAD_NOTIFY_SWITCH\n\tbl\tatomic_notifier_call_chain\n#if defined(CONFIG_CC_STACKPROTECTOR) && !defined(CONFIG_SMP)\n\tstr\tr7, [r8]\n#endif\n THUMB(\tmov\tip, r4\t\t\t   )\n\tmov\tr0, r5\n ARM(\tldmia\tr4, {r4 - sl, fp, sp, pc}  )\t@ Load all regs saved previously\n THUMB(\tldmia\tip!, {r4 - sl, fp}\t   )\t@ Load all regs saved previously\n THUMB(\tldr\tsp, [ip], #4\t\t   )\n THUMB(\tldr\tpc, [ip]\t\t   )\n UNWIND(.fnend\t\t)\nENDPROC(__switch_to)\n\n\t__INIT\n\n/*\n * User helpers.\n *\n * Each segment is 32-byte aligned and will be moved to the top of the high\n * vector page.  New segments (if ever needed) must be added in front of\n * existing ones.  This mechanism should be used only for things that are\n * really small and justified, and not be abused freely.\n *\n * See Documentation/arm/kernel_user_helpers.txt for formal definitions.\n */\n THUMB(\t.arm\t)\n\n\t.macro\tusr_ret, reg\n#ifdef CONFIG_ARM_THUMB\n\tbx\t\\reg\n#else\n\tmov\tpc, \\reg\n#endif\n\t.endm\n\n\t.align\t5\n\t.globl\t__kuser_helper_start\n__kuser_helper_start:\n\n/*\n * Due to the length of some sequences, __kuser_cmpxchg64 spans 2 regular\n * kuser \"slots\", therefore 0xffff0f80 is not used as a valid entry point.\n */\n\n__kuser_cmpxchg64:\t\t\t\t@ 0xffff0f60\n\n#if defined(CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG)\n\n\t/*\n\t * Poor you.  No fast solution possible...\n\t * The kernel itself must perform the operation.\n\t * A special ghost syscall is used for that (see traps.c).\n\t */\n\tstmfd\tsp!, {r7, lr}\n\tldr\tr7, 1f\t\t\t@ it's 20 bits\n\tswi\t__ARM_NR_cmpxchg64\n\tldmfd\tsp!, {r7, pc}\n1:\t.word\t__ARM_NR_cmpxchg64\n\n#elif defined(CONFIG_CPU_32v6K)\n\n\tstmfd\tsp!, {r4, r5, r6, r7}\n\tldrd\tr4, r5, [r0]\t\t\t@ load old val\n\tldrd\tr6, r7, [r1]\t\t\t@ load new val\n\tsmp_dmb\tarm\n1:\tldrexd\tr0, r1, [r2]\t\t\t@ load current val\n\teors\tr3, r0, r4\t\t\t@ compare with oldval (1)\n\teoreqs\tr3, r1, r5\t\t\t@ compare with oldval (2)\n\tstrexdeq r3, r6, r7, [r2]\t\t@ store newval if eq\n\tteqeq\tr3, #1\t\t\t\t@ success?\n\tbeq\t1b\t\t\t\t@ if no then retry\n\tsmp_dmb\tarm\n\trsbs\tr0, r3, #0\t\t\t@ set returned val and C flag\n\tldmfd\tsp!, {r4, r5, r6, r7}\n\tusr_ret\tlr\n\n#elif !defined(CONFIG_SMP)\n\n#ifdef CONFIG_MMU\n\n\t/*\n\t * The only thing that can break atomicity in this cmpxchg64\n\t * implementation is either an IRQ or a data abort exception\n\t * causing another process/thread to be scheduled in the middle of\n\t * the critical sequence.  The same strategy as for cmpxchg is used.\n\t */\n\tstmfd\tsp!, {r4, r5, r6, lr}\n\tldmia\tr0, {r4, r5}\t\t\t@ load old val\n\tldmia\tr1, {r6, lr}\t\t\t@ load new val\n1:\tldmia\tr2, {r0, r1}\t\t\t@ load current val\n\teors\tr3, r0, r4\t\t\t@ compare with oldval (1)\n\teoreqs\tr3, r1, r5\t\t\t@ compare with oldval (2)\n2:\tstmeqia\tr2, {r6, lr}\t\t\t@ store newval if eq\n\trsbs\tr0, r3, #0\t\t\t@ set return val and C flag\n\tldmfd\tsp!, {r4, r5, r6, pc}\n\n\t.text\nkuser_cmpxchg64_fixup:\n\t@ Called from kuser_cmpxchg_fixup.\n\t@ r4 = address of interrupted insn (must be preserved).\n\t@ sp = saved regs. r7 and r8 are clobbered.\n\t@ 1b = first critical insn, 2b = last critical insn.\n\t@ If r4 >= 1b and r4 <= 2b then saved pc_usr is set to 1b.\n\tmov\tr7, #0xffff0fff\n\tsub\tr7, r7, #(0xffff0fff - (0xffff0f60 + (1b - __kuser_cmpxchg64)))\n\tsubs\tr8, r4, r7\n\trsbcss\tr8, r8, #(2b - 1b)\n\tstrcs\tr7, [sp, #S_PC]\n#if __LINUX_ARM_ARCH__ < 6\n\tbcc\tkuser_cmpxchg32_fixup\n#endif\n\tmov\tpc, lr\n\t.previous\n\n#else\n#warning \"NPTL on non MMU needs fixing\"\n\tmov\tr0, #-1\n\tadds\tr0, r0, #0\n\tusr_ret\tlr\n#endif\n\n#else\n#error \"incoherent kernel configuration\"\n#endif\n\n\t/* pad to next slot */\n\t.rept\t(16 - (. - __kuser_cmpxchg64)/4)\n\t.word\t0\n\t.endr\n\n\t.align\t5\n\n__kuser_memory_barrier:\t\t\t\t@ 0xffff0fa0\n\tsmp_dmb\tarm\n\tusr_ret\tlr\n\n\t.align\t5\n\n__kuser_cmpxchg:\t\t\t\t@ 0xffff0fc0\n\n#if defined(CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG)\n\n\t/*\n\t * Poor you.  No fast solution possible...\n\t * The kernel itself must perform the operation.\n\t * A special ghost syscall is used for that (see traps.c).\n\t */\n\tstmfd\tsp!, {r7, lr}\n\tldr\tr7, 1f\t\t\t@ it's 20 bits\n\tswi\t__ARM_NR_cmpxchg\n\tldmfd\tsp!, {r7, pc}\n1:\t.word\t__ARM_NR_cmpxchg\n\n#elif __LINUX_ARM_ARCH__ < 6\n\n#ifdef CONFIG_MMU\n\n\t/*\n\t * The only thing that can break atomicity in this cmpxchg\n\t * implementation is either an IRQ or a data abort exception\n\t * causing another process/thread to be scheduled in the middle\n\t * of the critical sequence.  To prevent this, code is added to\n\t * the IRQ and data abort exception handlers to set the pc back\n\t * to the beginning of the critical section if it is found to be\n\t * within that critical section (see kuser_cmpxchg_fixup).\n\t */\n1:\tldr\tr3, [r2]\t\t\t@ load current val\n\tsubs\tr3, r3, r0\t\t\t@ compare with oldval\n2:\tstreq\tr1, [r2]\t\t\t@ store newval if eq\n\trsbs\tr0, r3, #0\t\t\t@ set return val and C flag\n\tusr_ret\tlr\n\n\t.text\nkuser_cmpxchg32_fixup:\n\t@ Called from kuser_cmpxchg_check macro.\n\t@ r4 = address of interrupted insn (must be preserved).\n\t@ sp = saved regs. r7 and r8 are clobbered.\n\t@ 1b = first critical insn, 2b = last critical insn.\n\t@ If r4 >= 1b and r4 <= 2b then saved pc_usr is set to 1b.\n\tmov\tr7, #0xffff0fff\n\tsub\tr7, r7, #(0xffff0fff - (0xffff0fc0 + (1b - __kuser_cmpxchg)))\n\tsubs\tr8, r4, r7\n\trsbcss\tr8, r8, #(2b - 1b)\n\tstrcs\tr7, [sp, #S_PC]\n\tmov\tpc, lr\n\t.previous\n\n#else\n#warning \"NPTL on non MMU needs fixing\"\n\tmov\tr0, #-1\n\tadds\tr0, r0, #0\n\tusr_ret\tlr\n#endif\n\n#else\n\n\tsmp_dmb\tarm\n1:\tldrex\tr3, [r2]\n\tsubs\tr3, r3, r0\n\tstrexeq\tr3, r1, [r2]\n\tteqeq\tr3, #1\n\tbeq\t1b\n\trsbs\tr0, r3, #0\n\t/* beware -- each __kuser slot must be 8 instructions max */\n\tALT_SMP(b\t__kuser_memory_barrier)\n\tALT_UP(usr_ret\tlr)\n\n#endif\n\n\t.align\t5\n\n__kuser_get_tls:\t\t\t\t@ 0xffff0fe0\n\tldr\tr0, [pc, #(16 - 8)]\t@ read TLS, set in kuser_get_tls_init\n\tusr_ret\tlr\n\tmrc\tp15, 0, r0, c13, c0, 3\t@ 0xffff0fe8 hardware TLS code\n\t.rep\t4\n\t.word\t0\t\t\t@ 0xffff0ff0 software TLS value, then\n\t.endr\t\t\t\t@ pad up to __kuser_helper_version\n\n__kuser_helper_version:\t\t\t\t@ 0xffff0ffc\n\t.word\t((__kuser_helper_end - __kuser_helper_start) >> 5)\n\n\t.globl\t__kuser_helper_end\n__kuser_helper_end:\n\n THUMB(\t.thumb\t)\n\n/*\n * Vector stubs.\n *\n * This code is copied to 0xffff0200 so we can use branches in the\n * vectors, rather than ldr's.  Note that this code must not\n * exceed 0x300 bytes.\n *\n * Common stub entry macro:\n *   Enter in IRQ mode, spsr = SVC/USR CPSR, lr = SVC/USR PC\n *\n * SP points to a minimal amount of processor-private memory, the address\n * of which is copied into r0 for the mode specific abort handler.\n */\n\t.macro\tvector_stub, name, mode, correction=0\n\t.align\t5\n\nvector_\\name:\n\t.if \\correction\n\tsub\tlr, lr, #\\correction\n\t.endif\n\n\t@\n\t@ Save r0, lr_<exception> (parent PC) and spsr_<exception>\n\t@ (parent CPSR)\n\t@\n\tstmia\tsp, {r0, lr}\t\t@ save r0, lr\n\tmrs\tlr, spsr\n\tstr\tlr, [sp, #8]\t\t@ save spsr\n\n\t@\n\t@ Prepare for SVC32 mode.  IRQs remain disabled.\n\t@\n\tmrs\tr0, cpsr\n\teor\tr0, r0, #(\\mode ^ SVC_MODE | PSR_ISETSTATE)\n\tmsr\tspsr_cxsf, r0\n\n\t@\n\t@ the branch table must immediately follow this code\n\t@\n\tand\tlr, lr, #0x0f\n THUMB(\tadr\tr0, 1f\t\t\t)\n THUMB(\tldr\tlr, [r0, lr, lsl #2]\t)\n\tmov\tr0, sp\n ARM(\tldr\tlr, [pc, lr, lsl #2]\t)\n\tmovs\tpc, lr\t\t\t@ branch to handler in SVC mode\nENDPROC(vector_\\name)\n\n\t.align\t2\n\t@ handler addresses follow this label\n1:\n\t.endm\n\n\t.globl\t__stubs_start\n__stubs_start:\n/*\n * Interrupt dispatcher\n */\n\tvector_stub\tirq, IRQ_MODE, 4\n\n\t.long\t__irq_usr\t\t\t@  0  (USR_26 / USR_32)\n\t.long\t__irq_invalid\t\t\t@  1  (FIQ_26 / FIQ_32)\n\t.long\t__irq_invalid\t\t\t@  2  (IRQ_26 / IRQ_32)\n\t.long\t__irq_svc\t\t\t@  3  (SVC_26 / SVC_32)\n\t.long\t__irq_invalid\t\t\t@  4\n\t.long\t__irq_invalid\t\t\t@  5\n\t.long\t__irq_invalid\t\t\t@  6\n\t.long\t__irq_invalid\t\t\t@  7\n\t.long\t__irq_invalid\t\t\t@  8\n\t.long\t__irq_invalid\t\t\t@  9\n\t.long\t__irq_invalid\t\t\t@  a\n\t.long\t__irq_invalid\t\t\t@  b\n\t.long\t__irq_invalid\t\t\t@  c\n\t.long\t__irq_invalid\t\t\t@  d\n\t.long\t__irq_invalid\t\t\t@  e\n\t.long\t__irq_invalid\t\t\t@  f\n\n/*\n * Data abort dispatcher\n * Enter in ABT mode, spsr = USR CPSR, lr = USR PC\n */\n\tvector_stub\tdabt, ABT_MODE, 8\n\n\t.long\t__dabt_usr\t\t\t@  0  (USR_26 / USR_32)\n\t.long\t__dabt_invalid\t\t\t@  1  (FIQ_26 / FIQ_32)\n\t.long\t__dabt_invalid\t\t\t@  2  (IRQ_26 / IRQ_32)\n\t.long\t__dabt_svc\t\t\t@  3  (SVC_26 / SVC_32)\n\t.long\t__dabt_invalid\t\t\t@  4\n\t.long\t__dabt_invalid\t\t\t@  5\n\t.long\t__dabt_invalid\t\t\t@  6\n\t.long\t__dabt_invalid\t\t\t@  7\n\t.long\t__dabt_invalid\t\t\t@  8\n\t.long\t__dabt_invalid\t\t\t@  9\n\t.long\t__dabt_invalid\t\t\t@  a\n\t.long\t__dabt_invalid\t\t\t@  b\n\t.long\t__dabt_invalid\t\t\t@  c\n\t.long\t__dabt_invalid\t\t\t@  d\n\t.long\t__dabt_invalid\t\t\t@  e\n\t.long\t__dabt_invalid\t\t\t@  f\n\n/*\n * Prefetch abort dispatcher\n * Enter in ABT mode, spsr = USR CPSR, lr = USR PC\n */\n\tvector_stub\tpabt, ABT_MODE, 4\n\n\t.long\t__pabt_usr\t\t\t@  0 (USR_26 / USR_32)\n\t.long\t__pabt_invalid\t\t\t@  1 (FIQ_26 / FIQ_32)\n\t.long\t__pabt_invalid\t\t\t@  2 (IRQ_26 / IRQ_32)\n\t.long\t__pabt_svc\t\t\t@  3 (SVC_26 / SVC_32)\n\t.long\t__pabt_invalid\t\t\t@  4\n\t.long\t__pabt_invalid\t\t\t@  5\n\t.long\t__pabt_invalid\t\t\t@  6\n\t.long\t__pabt_invalid\t\t\t@  7\n\t.long\t__pabt_invalid\t\t\t@  8\n\t.long\t__pabt_invalid\t\t\t@  9\n\t.long\t__pabt_invalid\t\t\t@  a\n\t.long\t__pabt_invalid\t\t\t@  b\n\t.long\t__pabt_invalid\t\t\t@  c\n\t.long\t__pabt_invalid\t\t\t@  d\n\t.long\t__pabt_invalid\t\t\t@  e\n\t.long\t__pabt_invalid\t\t\t@  f\n\n/*\n * Undef instr entry dispatcher\n * Enter in UND mode, spsr = SVC/USR CPSR, lr = SVC/USR PC\n */\n\tvector_stub\tund, UND_MODE\n\n\t.long\t__und_usr\t\t\t@  0 (USR_26 / USR_32)\n\t.long\t__und_invalid\t\t\t@  1 (FIQ_26 / FIQ_32)\n\t.long\t__und_invalid\t\t\t@  2 (IRQ_26 / IRQ_32)\n\t.long\t__und_svc\t\t\t@  3 (SVC_26 / SVC_32)\n\t.long\t__und_invalid\t\t\t@  4\n\t.long\t__und_invalid\t\t\t@  5\n\t.long\t__und_invalid\t\t\t@  6\n\t.long\t__und_invalid\t\t\t@  7\n\t.long\t__und_invalid\t\t\t@  8\n\t.long\t__und_invalid\t\t\t@  9\n\t.long\t__und_invalid\t\t\t@  a\n\t.long\t__und_invalid\t\t\t@  b\n\t.long\t__und_invalid\t\t\t@  c\n\t.long\t__und_invalid\t\t\t@  d\n\t.long\t__und_invalid\t\t\t@  e\n\t.long\t__und_invalid\t\t\t@  f\n\n\t.align\t5\n\n/*=============================================================================\n * Undefined FIQs\n *-----------------------------------------------------------------------------\n * Enter in FIQ mode, spsr = ANY CPSR, lr = ANY PC\n * MUST PRESERVE SVC SPSR, but need to switch to SVC mode to show our msg.\n * Basically to switch modes, we *HAVE* to clobber one register...  brain\n * damage alert!  I don't think that we can execute any code in here in any\n * other mode than FIQ...  Ok you can switch to another mode, but you can't\n * get out of that mode without clobbering one register.\n */\nvector_fiq:\n\tsubs\tpc, lr, #4\n\n/*=============================================================================\n * Address exception handler\n *-----------------------------------------------------------------------------\n * These aren't too critical.\n * (they're not supposed to happen, and won't happen in 32-bit data mode).\n */\n\nvector_addrexcptn:\n\tb\tvector_addrexcptn\n\n/*\n * We group all the following data together to optimise\n * for CPUs with separate I & D caches.\n */\n\t.align\t5\n\n.LCvswi:\n\t.word\tvector_swi\n\n\t.globl\t__stubs_end\n__stubs_end:\n\n\t.equ\tstubs_offset, __vectors_start + 0x200 - __stubs_start\n\n\t.globl\t__vectors_start\n__vectors_start:\n ARM(\tswi\tSYS_ERROR0\t)\n THUMB(\tsvc\t#0\t\t)\n THUMB(\tnop\t\t\t)\n\tW(b)\tvector_und + stubs_offset\n\tW(ldr)\tpc, .LCvswi + stubs_offset\n\tW(b)\tvector_pabt + stubs_offset\n\tW(b)\tvector_dabt + stubs_offset\n\tW(b)\tvector_addrexcptn + stubs_offset\n\tW(b)\tvector_irq + stubs_offset\n\tW(b)\tvector_fiq + stubs_offset\n\n\t.globl\t__vectors_end\n__vectors_end:\n\n\t.data\n\n\t.globl\tcr_alignment\n\t.globl\tcr_no_alignment\ncr_alignment:\n\t.space\t4\ncr_no_alignment:\n\t.space\t4\n\n#ifdef CONFIG_MULTI_IRQ_HANDLER\n\t.globl\thandle_arch_irq\nhandle_arch_irq:\n\t.space\t4\n#endif\n", "/*\n *  linux/arch/arm/kernel/process.c\n *\n *  Copyright (C) 1996-2000 Russell King - Converted to ARM.\n *  Original Copyright (C) 1995  Linus Torvalds\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#include <stdarg.h>\n\n#include <linux/export.h>\n#include <linux/sched.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/stddef.h>\n#include <linux/unistd.h>\n#include <linux/user.h>\n#include <linux/delay.h>\n#include <linux/reboot.h>\n#include <linux/interrupt.h>\n#include <linux/kallsyms.h>\n#include <linux/init.h>\n#include <linux/cpu.h>\n#include <linux/elfcore.h>\n#include <linux/pm.h>\n#include <linux/tick.h>\n#include <linux/utsname.h>\n#include <linux/uaccess.h>\n#include <linux/random.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/cpuidle.h>\n#include <linux/leds.h>\n\n#include <asm/cacheflush.h>\n#include <asm/idmap.h>\n#include <asm/processor.h>\n#include <asm/thread_notify.h>\n#include <asm/stacktrace.h>\n#include <asm/mach/time.h>\n\n#ifdef CONFIG_CC_STACKPROTECTOR\n#include <linux/stackprotector.h>\nunsigned long __stack_chk_guard __read_mostly;\nEXPORT_SYMBOL(__stack_chk_guard);\n#endif\n\nstatic const char *processor_modes[] = {\n  \"USER_26\", \"FIQ_26\" , \"IRQ_26\" , \"SVC_26\" , \"UK4_26\" , \"UK5_26\" , \"UK6_26\" , \"UK7_26\" ,\n  \"UK8_26\" , \"UK9_26\" , \"UK10_26\", \"UK11_26\", \"UK12_26\", \"UK13_26\", \"UK14_26\", \"UK15_26\",\n  \"USER_32\", \"FIQ_32\" , \"IRQ_32\" , \"SVC_32\" , \"UK4_32\" , \"UK5_32\" , \"UK6_32\" , \"ABT_32\" ,\n  \"UK8_32\" , \"UK9_32\" , \"UK10_32\", \"UND_32\" , \"UK12_32\", \"UK13_32\", \"UK14_32\", \"SYS_32\"\n};\n\nstatic const char *isa_modes[] = {\n  \"ARM\" , \"Thumb\" , \"Jazelle\", \"ThumbEE\"\n};\n\nextern void call_with_stack(void (*fn)(void *), void *arg, void *sp);\ntypedef void (*phys_reset_t)(unsigned long);\n\n/*\n * A temporary stack to use for CPU reset. This is static so that we\n * don't clobber it with the identity mapping. When running with this\n * stack, any references to the current task *will not work* so you\n * should really do as little as possible before jumping to your reset\n * code.\n */\nstatic u64 soft_restart_stack[16];\n\nstatic void __soft_restart(void *addr)\n{\n\tphys_reset_t phys_reset;\n\n\t/* Take out a flat memory mapping. */\n\tsetup_mm_for_reboot();\n\n\t/* Clean and invalidate caches */\n\tflush_cache_all();\n\n\t/* Turn off caching */\n\tcpu_proc_fin();\n\n\t/* Push out any further dirty data, and ensure cache is empty */\n\tflush_cache_all();\n\n\t/* Switch to the identity mapping. */\n\tphys_reset = (phys_reset_t)(unsigned long)virt_to_phys(cpu_reset);\n\tphys_reset((unsigned long)addr);\n\n\t/* Should never get here. */\n\tBUG();\n}\n\nvoid soft_restart(unsigned long addr)\n{\n\tu64 *stack = soft_restart_stack + ARRAY_SIZE(soft_restart_stack);\n\n\t/* Disable interrupts first */\n\tlocal_irq_disable();\n\tlocal_fiq_disable();\n\n\t/* Disable the L2 if we're the last man standing. */\n\tif (num_online_cpus() == 1)\n\t\touter_disable();\n\n\t/* Change to the new stack and continue with the reset. */\n\tcall_with_stack(__soft_restart, (void *)addr, (void *)stack);\n\n\t/* Should never get here. */\n\tBUG();\n}\n\nstatic void null_restart(char mode, const char *cmd)\n{\n}\n\n/*\n * Function pointers to optional machine specific functions\n */\nvoid (*pm_power_off)(void);\nEXPORT_SYMBOL(pm_power_off);\n\nvoid (*arm_pm_restart)(char str, const char *cmd) = null_restart;\nEXPORT_SYMBOL_GPL(arm_pm_restart);\n\n/*\n * This is our default idle handler.\n */\n\nvoid (*arm_pm_idle)(void);\n\nstatic void default_idle(void)\n{\n\tif (arm_pm_idle)\n\t\tarm_pm_idle();\n\telse\n\t\tcpu_do_idle();\n\tlocal_irq_enable();\n}\n\nvoid arch_cpu_idle_prepare(void)\n{\n\tlocal_fiq_enable();\n}\n\nvoid arch_cpu_idle_enter(void)\n{\n\tledtrig_cpu(CPU_LED_IDLE_START);\n#ifdef CONFIG_PL310_ERRATA_769419\n\twmb();\n#endif\n}\n\nvoid arch_cpu_idle_exit(void)\n{\n\tledtrig_cpu(CPU_LED_IDLE_END);\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\nvoid arch_cpu_idle_dead(void)\n{\n\tcpu_die();\n}\n#endif\n\n/*\n * Called from the core idle loop.\n */\nvoid arch_cpu_idle(void)\n{\n\tif (cpuidle_idle_call())\n\t\tdefault_idle();\n}\n\nstatic char reboot_mode = 'h';\n\nint __init reboot_setup(char *str)\n{\n\treboot_mode = str[0];\n\treturn 1;\n}\n\n__setup(\"reboot=\", reboot_setup);\n\nvoid machine_shutdown(void)\n{\n#ifdef CONFIG_SMP\n\tsmp_send_stop();\n#endif\n}\n\nvoid machine_halt(void)\n{\n\tmachine_shutdown();\n\tlocal_irq_disable();\n\twhile (1);\n}\n\nvoid machine_power_off(void)\n{\n\tmachine_shutdown();\n\tif (pm_power_off)\n\t\tpm_power_off();\n}\n\nvoid machine_restart(char *cmd)\n{\n\tmachine_shutdown();\n\n\tarm_pm_restart(reboot_mode, cmd);\n\n\t/* Give a grace period for failure to restart of 1s */\n\tmdelay(1000);\n\n\t/* Whoops - the platform was unable to reboot. Tell the user! */\n\tprintk(\"Reboot failed -- System halted\\n\");\n\tlocal_irq_disable();\n\twhile (1);\n}\n\nvoid __show_regs(struct pt_regs *regs)\n{\n\tunsigned long flags;\n\tchar buf[64];\n\n\tshow_regs_print_info(KERN_DEFAULT);\n\n\tprint_symbol(\"PC is at %s\\n\", instruction_pointer(regs));\n\tprint_symbol(\"LR is at %s\\n\", regs->ARM_lr);\n\tprintk(\"pc : [<%08lx>]    lr : [<%08lx>]    psr: %08lx\\n\"\n\t       \"sp : %08lx  ip : %08lx  fp : %08lx\\n\",\n\t\tregs->ARM_pc, regs->ARM_lr, regs->ARM_cpsr,\n\t\tregs->ARM_sp, regs->ARM_ip, regs->ARM_fp);\n\tprintk(\"r10: %08lx  r9 : %08lx  r8 : %08lx\\n\",\n\t\tregs->ARM_r10, regs->ARM_r9,\n\t\tregs->ARM_r8);\n\tprintk(\"r7 : %08lx  r6 : %08lx  r5 : %08lx  r4 : %08lx\\n\",\n\t\tregs->ARM_r7, regs->ARM_r6,\n\t\tregs->ARM_r5, regs->ARM_r4);\n\tprintk(\"r3 : %08lx  r2 : %08lx  r1 : %08lx  r0 : %08lx\\n\",\n\t\tregs->ARM_r3, regs->ARM_r2,\n\t\tregs->ARM_r1, regs->ARM_r0);\n\n\tflags = regs->ARM_cpsr;\n\tbuf[0] = flags & PSR_N_BIT ? 'N' : 'n';\n\tbuf[1] = flags & PSR_Z_BIT ? 'Z' : 'z';\n\tbuf[2] = flags & PSR_C_BIT ? 'C' : 'c';\n\tbuf[3] = flags & PSR_V_BIT ? 'V' : 'v';\n\tbuf[4] = '\\0';\n\n\tprintk(\"Flags: %s  IRQs o%s  FIQs o%s  Mode %s  ISA %s  Segment %s\\n\",\n\t\tbuf, interrupts_enabled(regs) ? \"n\" : \"ff\",\n\t\tfast_interrupts_enabled(regs) ? \"n\" : \"ff\",\n\t\tprocessor_modes[processor_mode(regs)],\n\t\tisa_modes[isa_mode(regs)],\n\t\tget_fs() == get_ds() ? \"kernel\" : \"user\");\n#ifdef CONFIG_CPU_CP15\n\t{\n\t\tunsigned int ctrl;\n\n\t\tbuf[0] = '\\0';\n#ifdef CONFIG_CPU_CP15_MMU\n\t\t{\n\t\t\tunsigned int transbase, dac;\n\t\t\tasm(\"mrc p15, 0, %0, c2, c0\\n\\t\"\n\t\t\t    \"mrc p15, 0, %1, c3, c0\\n\"\n\t\t\t    : \"=r\" (transbase), \"=r\" (dac));\n\t\t\tsnprintf(buf, sizeof(buf), \"  Table: %08x  DAC: %08x\",\n\t\t\t  \ttransbase, dac);\n\t\t}\n#endif\n\t\tasm(\"mrc p15, 0, %0, c1, c0\\n\" : \"=r\" (ctrl));\n\n\t\tprintk(\"Control: %08x%s\\n\", ctrl, buf);\n\t}\n#endif\n}\n\nvoid show_regs(struct pt_regs * regs)\n{\n\tprintk(\"\\n\");\n\t__show_regs(regs);\n\tdump_stack();\n}\n\nATOMIC_NOTIFIER_HEAD(thread_notify_head);\n\nEXPORT_SYMBOL_GPL(thread_notify_head);\n\n/*\n * Free current thread data structures etc..\n */\nvoid exit_thread(void)\n{\n\tthread_notify(THREAD_NOTIFY_EXIT, current_thread_info());\n}\n\nvoid flush_thread(void)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tstruct task_struct *tsk = current;\n\n\tflush_ptrace_hw_breakpoint(tsk);\n\n\tmemset(thread->used_cp, 0, sizeof(thread->used_cp));\n\tmemset(&tsk->thread.debug, 0, sizeof(struct debug_info));\n\tmemset(&thread->fpstate, 0, sizeof(union fp_state));\n\n\tthread_notify(THREAD_NOTIFY_FLUSH, thread);\n}\n\nvoid release_thread(struct task_struct *dead_task)\n{\n}\n\nasmlinkage void ret_from_fork(void) __asm__(\"ret_from_fork\");\n\nint\ncopy_thread(unsigned long clone_flags, unsigned long stack_start,\n\t    unsigned long stk_sz, struct task_struct *p)\n{\n\tstruct thread_info *thread = task_thread_info(p);\n\tstruct pt_regs *childregs = task_pt_regs(p);\n\n\tmemset(&thread->cpu_context, 0, sizeof(struct cpu_context_save));\n\n\tif (likely(!(p->flags & PF_KTHREAD))) {\n\t\t*childregs = *current_pt_regs();\n\t\tchildregs->ARM_r0 = 0;\n\t\tif (stack_start)\n\t\t\tchildregs->ARM_sp = stack_start;\n\t} else {\n\t\tmemset(childregs, 0, sizeof(struct pt_regs));\n\t\tthread->cpu_context.r4 = stk_sz;\n\t\tthread->cpu_context.r5 = stack_start;\n\t\tchildregs->ARM_cpsr = SVC_MODE;\n\t}\n\tthread->cpu_context.pc = (unsigned long)ret_from_fork;\n\tthread->cpu_context.sp = (unsigned long)childregs;\n\n\tclear_ptrace_hw_breakpoint(p);\n\n\tif (clone_flags & CLONE_SETTLS)\n\t\tthread->tp_value = childregs->ARM_r3;\n\n\tthread_notify(THREAD_NOTIFY_COPY, thread);\n\n\treturn 0;\n}\n\n/*\n * Fill in the task's elfregs structure for a core dump.\n */\nint dump_task_regs(struct task_struct *t, elf_gregset_t *elfregs)\n{\n\telf_core_copy_regs(elfregs, task_pt_regs(t));\n\treturn 1;\n}\n\n/*\n * fill in the fpe structure for a core dump...\n */\nint dump_fpu (struct pt_regs *regs, struct user_fp *fp)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tint used_math = thread->used_cp[1] | thread->used_cp[2];\n\n\tif (used_math)\n\t\tmemcpy(fp, &thread->fpstate.soft, sizeof (*fp));\n\n\treturn used_math != 0;\n}\nEXPORT_SYMBOL(dump_fpu);\n\nunsigned long get_wchan(struct task_struct *p)\n{\n\tstruct stackframe frame;\n\tint count = 0;\n\tif (!p || p == current || p->state == TASK_RUNNING)\n\t\treturn 0;\n\n\tframe.fp = thread_saved_fp(p);\n\tframe.sp = thread_saved_sp(p);\n\tframe.lr = 0;\t\t\t/* recovered from the stack */\n\tframe.pc = thread_saved_pc(p);\n\tdo {\n\t\tint ret = unwind_frame(&frame);\n\t\tif (ret < 0)\n\t\t\treturn 0;\n\t\tif (!in_sched_functions(frame.pc))\n\t\t\treturn frame.pc;\n\t} while (count ++ < 16);\n\treturn 0;\n}\n\nunsigned long arch_randomize_brk(struct mm_struct *mm)\n{\n\tunsigned long range_end = mm->brk + 0x02000000;\n\treturn randomize_range(mm->brk, range_end, 0) ? : mm->brk;\n}\n\n#ifdef CONFIG_MMU\n/*\n * The vectors page is always readable from user space for the\n * atomic helpers and the signal restart code. Insert it into the\n * gate_vma so that it is visible through ptrace and /proc/<pid>/mem.\n */\nstatic struct vm_area_struct gate_vma = {\n\t.vm_start\t= 0xffff0000,\n\t.vm_end\t\t= 0xffff0000 + PAGE_SIZE,\n\t.vm_flags\t= VM_READ | VM_EXEC | VM_MAYREAD | VM_MAYEXEC,\n\t.vm_mm\t\t= &init_mm,\n};\n\nstatic int __init gate_vma_init(void)\n{\n\tgate_vma.vm_page_prot = PAGE_READONLY_EXEC;\n\treturn 0;\n}\narch_initcall(gate_vma_init);\n\nstruct vm_area_struct *get_gate_vma(struct mm_struct *mm)\n{\n\treturn &gate_vma;\n}\n\nint in_gate_area(struct mm_struct *mm, unsigned long addr)\n{\n\treturn (addr >= gate_vma.vm_start) && (addr < gate_vma.vm_end);\n}\n\nint in_gate_area_no_mm(unsigned long addr)\n{\n\treturn in_gate_area(NULL, addr);\n}\n\nconst char *arch_vma_name(struct vm_area_struct *vma)\n{\n\treturn (vma == &gate_vma) ? \"[vectors]\" : NULL;\n}\n#endif\n", "/*\n *  linux/arch/arm/kernel/ptrace.c\n *\n *  By Ross Biro 1/23/92\n * edited by Linus Torvalds\n * ARM modifications Copyright (C) 2000 Russell King\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/elf.h>\n#include <linux/smp.h>\n#include <linux/ptrace.h>\n#include <linux/user.h>\n#include <linux/security.h>\n#include <linux/init.h>\n#include <linux/signal.h>\n#include <linux/uaccess.h>\n#include <linux/perf_event.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/regset.h>\n#include <linux/audit.h>\n#include <linux/tracehook.h>\n#include <linux/unistd.h>\n\n#include <asm/pgtable.h>\n#include <asm/traps.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/syscalls.h>\n\n#define REG_PC\t15\n#define REG_PSR\t16\n/*\n * does not yet catch signals sent when the child dies.\n * in exit.c or in signal.c.\n */\n\n#if 0\n/*\n * Breakpoint SWI instruction: SWI &9F0001\n */\n#define BREAKINST_ARM\t0xef9f0001\n#define BREAKINST_THUMB\t0xdf00\t\t/* fill this in later */\n#else\n/*\n * New breakpoints - use an undefined instruction.  The ARM architecture\n * reference manual guarantees that the following instruction space\n * will produce an undefined instruction exception on all CPUs:\n *\n *  ARM:   xxxx 0111 1111 xxxx xxxx xxxx 1111 xxxx\n *  Thumb: 1101 1110 xxxx xxxx\n */\n#define BREAKINST_ARM\t0xe7f001f0\n#define BREAKINST_THUMB\t0xde01\n#endif\n\nstruct pt_regs_offset {\n\tconst char *name;\n\tint offset;\n};\n\n#define REG_OFFSET_NAME(r) \\\n\t{.name = #r, .offset = offsetof(struct pt_regs, ARM_##r)}\n#define REG_OFFSET_END {.name = NULL, .offset = 0}\n\nstatic const struct pt_regs_offset regoffset_table[] = {\n\tREG_OFFSET_NAME(r0),\n\tREG_OFFSET_NAME(r1),\n\tREG_OFFSET_NAME(r2),\n\tREG_OFFSET_NAME(r3),\n\tREG_OFFSET_NAME(r4),\n\tREG_OFFSET_NAME(r5),\n\tREG_OFFSET_NAME(r6),\n\tREG_OFFSET_NAME(r7),\n\tREG_OFFSET_NAME(r8),\n\tREG_OFFSET_NAME(r9),\n\tREG_OFFSET_NAME(r10),\n\tREG_OFFSET_NAME(fp),\n\tREG_OFFSET_NAME(ip),\n\tREG_OFFSET_NAME(sp),\n\tREG_OFFSET_NAME(lr),\n\tREG_OFFSET_NAME(pc),\n\tREG_OFFSET_NAME(cpsr),\n\tREG_OFFSET_NAME(ORIG_r0),\n\tREG_OFFSET_END,\n};\n\n/**\n * regs_query_register_offset() - query register offset from its name\n * @name:\tthe name of a register\n *\n * regs_query_register_offset() returns the offset of a register in struct\n * pt_regs from its name. If the name is invalid, this returns -EINVAL;\n */\nint regs_query_register_offset(const char *name)\n{\n\tconst struct pt_regs_offset *roff;\n\tfor (roff = regoffset_table; roff->name != NULL; roff++)\n\t\tif (!strcmp(roff->name, name))\n\t\t\treturn roff->offset;\n\treturn -EINVAL;\n}\n\n/**\n * regs_query_register_name() - query register name from its offset\n * @offset:\tthe offset of a register in struct pt_regs.\n *\n * regs_query_register_name() returns the name of a register from its\n * offset in struct pt_regs. If the @offset is invalid, this returns NULL;\n */\nconst char *regs_query_register_name(unsigned int offset)\n{\n\tconst struct pt_regs_offset *roff;\n\tfor (roff = regoffset_table; roff->name != NULL; roff++)\n\t\tif (roff->offset == offset)\n\t\t\treturn roff->name;\n\treturn NULL;\n}\n\n/**\n * regs_within_kernel_stack() - check the address in the stack\n * @regs:      pt_regs which contains kernel stack pointer.\n * @addr:      address which is checked.\n *\n * regs_within_kernel_stack() checks @addr is within the kernel stack page(s).\n * If @addr is within the kernel stack, it returns true. If not, returns false.\n */\nbool regs_within_kernel_stack(struct pt_regs *regs, unsigned long addr)\n{\n\treturn ((addr & ~(THREAD_SIZE - 1))  ==\n\t\t(kernel_stack_pointer(regs) & ~(THREAD_SIZE - 1)));\n}\n\n/**\n * regs_get_kernel_stack_nth() - get Nth entry of the stack\n * @regs:\tpt_regs which contains kernel stack pointer.\n * @n:\t\tstack entry number.\n *\n * regs_get_kernel_stack_nth() returns @n th entry of the kernel stack which\n * is specified by @regs. If the @n th entry is NOT in the kernel stack,\n * this returns 0.\n */\nunsigned long regs_get_kernel_stack_nth(struct pt_regs *regs, unsigned int n)\n{\n\tunsigned long *addr = (unsigned long *)kernel_stack_pointer(regs);\n\taddr += n;\n\tif (regs_within_kernel_stack(regs, (unsigned long)addr))\n\t\treturn *addr;\n\telse\n\t\treturn 0;\n}\n\n/*\n * this routine will get a word off of the processes privileged stack.\n * the offset is how far from the base addr as stored in the THREAD.\n * this routine assumes that all the privileged stacks are in our\n * data space.\n */\nstatic inline long get_user_reg(struct task_struct *task, int offset)\n{\n\treturn task_pt_regs(task)->uregs[offset];\n}\n\n/*\n * this routine will put a word on the processes privileged stack.\n * the offset is how far from the base addr as stored in the THREAD.\n * this routine assumes that all the privileged stacks are in our\n * data space.\n */\nstatic inline int\nput_user_reg(struct task_struct *task, int offset, long data)\n{\n\tstruct pt_regs newregs, *regs = task_pt_regs(task);\n\tint ret = -EINVAL;\n\n\tnewregs = *regs;\n\tnewregs.uregs[offset] = data;\n\n\tif (valid_user_regs(&newregs)) {\n\t\tregs->uregs[offset] = data;\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\n/*\n * Called by kernel/ptrace.c when detaching..\n */\nvoid ptrace_disable(struct task_struct *child)\n{\n\t/* Nothing to do. */\n}\n\n/*\n * Handle hitting a breakpoint.\n */\nvoid ptrace_break(struct task_struct *tsk, struct pt_regs *regs)\n{\n\tsiginfo_t info;\n\n\tinfo.si_signo = SIGTRAP;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = TRAP_BRKPT;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs);\n\n\tforce_sig_info(SIGTRAP, &info, tsk);\n}\n\nstatic int break_trap(struct pt_regs *regs, unsigned int instr)\n{\n\tptrace_break(current, regs);\n\treturn 0;\n}\n\nstatic struct undef_hook arm_break_hook = {\n\t.instr_mask\t= 0x0fffffff,\n\t.instr_val\t= 0x07f001f0,\n\t.cpsr_mask\t= PSR_T_BIT,\n\t.cpsr_val\t= 0,\n\t.fn\t\t= break_trap,\n};\n\nstatic struct undef_hook thumb_break_hook = {\n\t.instr_mask\t= 0xffff,\n\t.instr_val\t= 0xde01,\n\t.cpsr_mask\t= PSR_T_BIT,\n\t.cpsr_val\t= PSR_T_BIT,\n\t.fn\t\t= break_trap,\n};\n\nstatic struct undef_hook thumb2_break_hook = {\n\t.instr_mask\t= 0xffffffff,\n\t.instr_val\t= 0xf7f0a000,\n\t.cpsr_mask\t= PSR_T_BIT,\n\t.cpsr_val\t= PSR_T_BIT,\n\t.fn\t\t= break_trap,\n};\n\nstatic int __init ptrace_break_init(void)\n{\n\tregister_undef_hook(&arm_break_hook);\n\tregister_undef_hook(&thumb_break_hook);\n\tregister_undef_hook(&thumb2_break_hook);\n\treturn 0;\n}\n\ncore_initcall(ptrace_break_init);\n\n/*\n * Read the word at offset \"off\" into the \"struct user\".  We\n * actually access the pt_regs stored on the kernel stack.\n */\nstatic int ptrace_read_user(struct task_struct *tsk, unsigned long off,\n\t\t\t    unsigned long __user *ret)\n{\n\tunsigned long tmp;\n\n\tif (off & 3)\n\t\treturn -EIO;\n\n\ttmp = 0;\n\tif (off == PT_TEXT_ADDR)\n\t\ttmp = tsk->mm->start_code;\n\telse if (off == PT_DATA_ADDR)\n\t\ttmp = tsk->mm->start_data;\n\telse if (off == PT_TEXT_END_ADDR)\n\t\ttmp = tsk->mm->end_code;\n\telse if (off < sizeof(struct pt_regs))\n\t\ttmp = get_user_reg(tsk, off >> 2);\n\telse if (off >= sizeof(struct user))\n\t\treturn -EIO;\n\n\treturn put_user(tmp, ret);\n}\n\n/*\n * Write the word at offset \"off\" into \"struct user\".  We\n * actually access the pt_regs stored on the kernel stack.\n */\nstatic int ptrace_write_user(struct task_struct *tsk, unsigned long off,\n\t\t\t     unsigned long val)\n{\n\tif (off & 3 || off >= sizeof(struct user))\n\t\treturn -EIO;\n\n\tif (off >= sizeof(struct pt_regs))\n\t\treturn 0;\n\n\treturn put_user_reg(tsk, off >> 2, val);\n}\n\n#ifdef CONFIG_IWMMXT\n\n/*\n * Get the child iWMMXt state.\n */\nstatic int ptrace_getwmmxregs(struct task_struct *tsk, void __user *ufp)\n{\n\tstruct thread_info *thread = task_thread_info(tsk);\n\n\tif (!test_ti_thread_flag(thread, TIF_USING_IWMMXT))\n\t\treturn -ENODATA;\n\tiwmmxt_task_disable(thread);  /* force it to ram */\n\treturn copy_to_user(ufp, &thread->fpstate.iwmmxt, IWMMXT_SIZE)\n\t\t? -EFAULT : 0;\n}\n\n/*\n * Set the child iWMMXt state.\n */\nstatic int ptrace_setwmmxregs(struct task_struct *tsk, void __user *ufp)\n{\n\tstruct thread_info *thread = task_thread_info(tsk);\n\n\tif (!test_ti_thread_flag(thread, TIF_USING_IWMMXT))\n\t\treturn -EACCES;\n\tiwmmxt_task_release(thread);  /* force a reload */\n\treturn copy_from_user(&thread->fpstate.iwmmxt, ufp, IWMMXT_SIZE)\n\t\t? -EFAULT : 0;\n}\n\n#endif\n\n#ifdef CONFIG_CRUNCH\n/*\n * Get the child Crunch state.\n */\nstatic int ptrace_getcrunchregs(struct task_struct *tsk, void __user *ufp)\n{\n\tstruct thread_info *thread = task_thread_info(tsk);\n\n\tcrunch_task_disable(thread);  /* force it to ram */\n\treturn copy_to_user(ufp, &thread->crunchstate, CRUNCH_SIZE)\n\t\t? -EFAULT : 0;\n}\n\n/*\n * Set the child Crunch state.\n */\nstatic int ptrace_setcrunchregs(struct task_struct *tsk, void __user *ufp)\n{\n\tstruct thread_info *thread = task_thread_info(tsk);\n\n\tcrunch_task_release(thread);  /* force a reload */\n\treturn copy_from_user(&thread->crunchstate, ufp, CRUNCH_SIZE)\n\t\t? -EFAULT : 0;\n}\n#endif\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n/*\n * Convert a virtual register number into an index for a thread_info\n * breakpoint array. Breakpoints are identified using positive numbers\n * whilst watchpoints are negative. The registers are laid out as pairs\n * of (address, control), each pair mapping to a unique hw_breakpoint struct.\n * Register 0 is reserved for describing resource information.\n */\nstatic int ptrace_hbp_num_to_idx(long num)\n{\n\tif (num < 0)\n\t\tnum = (ARM_MAX_BRP << 1) - num;\n\treturn (num - 1) >> 1;\n}\n\n/*\n * Returns the virtual register number for the address of the\n * breakpoint at index idx.\n */\nstatic long ptrace_hbp_idx_to_num(int idx)\n{\n\tlong mid = ARM_MAX_BRP << 1;\n\tlong num = (idx << 1) + 1;\n\treturn num > mid ? mid - num : num;\n}\n\n/*\n * Handle hitting a HW-breakpoint.\n */\nstatic void ptrace_hbptriggered(struct perf_event *bp,\n\t\t\t\t     struct perf_sample_data *data,\n\t\t\t\t     struct pt_regs *regs)\n{\n\tstruct arch_hw_breakpoint *bkpt = counter_arch_bp(bp);\n\tlong num;\n\tint i;\n\tsiginfo_t info;\n\n\tfor (i = 0; i < ARM_MAX_HBP_SLOTS; ++i)\n\t\tif (current->thread.debug.hbp[i] == bp)\n\t\t\tbreak;\n\n\tnum = (i == ARM_MAX_HBP_SLOTS) ? 0 : ptrace_hbp_idx_to_num(i);\n\n\tinfo.si_signo\t= SIGTRAP;\n\tinfo.si_errno\t= (int)num;\n\tinfo.si_code\t= TRAP_HWBKPT;\n\tinfo.si_addr\t= (void __user *)(bkpt->trigger);\n\n\tforce_sig_info(SIGTRAP, &info, current);\n}\n\n/*\n * Set ptrace breakpoint pointers to zero for this task.\n * This is required in order to prevent child processes from unregistering\n * breakpoints held by their parent.\n */\nvoid clear_ptrace_hw_breakpoint(struct task_struct *tsk)\n{\n\tmemset(tsk->thread.debug.hbp, 0, sizeof(tsk->thread.debug.hbp));\n}\n\n/*\n * Unregister breakpoints from this task and reset the pointers in\n * the thread_struct.\n */\nvoid flush_ptrace_hw_breakpoint(struct task_struct *tsk)\n{\n\tint i;\n\tstruct thread_struct *t = &tsk->thread;\n\n\tfor (i = 0; i < ARM_MAX_HBP_SLOTS; i++) {\n\t\tif (t->debug.hbp[i]) {\n\t\t\tunregister_hw_breakpoint(t->debug.hbp[i]);\n\t\t\tt->debug.hbp[i] = NULL;\n\t\t}\n\t}\n}\n\nstatic u32 ptrace_get_hbp_resource_info(void)\n{\n\tu8 num_brps, num_wrps, debug_arch, wp_len;\n\tu32 reg = 0;\n\n\tnum_brps\t= hw_breakpoint_slots(TYPE_INST);\n\tnum_wrps\t= hw_breakpoint_slots(TYPE_DATA);\n\tdebug_arch\t= arch_get_debug_arch();\n\twp_len\t\t= arch_get_max_wp_len();\n\n\treg\t\t|= debug_arch;\n\treg\t\t<<= 8;\n\treg\t\t|= wp_len;\n\treg\t\t<<= 8;\n\treg\t\t|= num_wrps;\n\treg\t\t<<= 8;\n\treg\t\t|= num_brps;\n\n\treturn reg;\n}\n\nstatic struct perf_event *ptrace_hbp_create(struct task_struct *tsk, int type)\n{\n\tstruct perf_event_attr attr;\n\n\tptrace_breakpoint_init(&attr);\n\n\t/* Initialise fields to sane defaults. */\n\tattr.bp_addr\t= 0;\n\tattr.bp_len\t= HW_BREAKPOINT_LEN_4;\n\tattr.bp_type\t= type;\n\tattr.disabled\t= 1;\n\n\treturn register_user_hw_breakpoint(&attr, ptrace_hbptriggered, NULL,\n\t\t\t\t\t   tsk);\n}\n\nstatic int ptrace_gethbpregs(struct task_struct *tsk, long num,\n\t\t\t     unsigned long  __user *data)\n{\n\tu32 reg;\n\tint idx, ret = 0;\n\tstruct perf_event *bp;\n\tstruct arch_hw_breakpoint_ctrl arch_ctrl;\n\n\tif (num == 0) {\n\t\treg = ptrace_get_hbp_resource_info();\n\t} else {\n\t\tidx = ptrace_hbp_num_to_idx(num);\n\t\tif (idx < 0 || idx >= ARM_MAX_HBP_SLOTS) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tbp = tsk->thread.debug.hbp[idx];\n\t\tif (!bp) {\n\t\t\treg = 0;\n\t\t\tgoto put;\n\t\t}\n\n\t\tarch_ctrl = counter_arch_bp(bp)->ctrl;\n\n\t\t/*\n\t\t * Fix up the len because we may have adjusted it\n\t\t * to compensate for an unaligned address.\n\t\t */\n\t\twhile (!(arch_ctrl.len & 0x1))\n\t\t\tarch_ctrl.len >>= 1;\n\n\t\tif (num & 0x1)\n\t\t\treg = bp->attr.bp_addr;\n\t\telse\n\t\t\treg = encode_ctrl_reg(arch_ctrl);\n\t}\n\nput:\n\tif (put_user(reg, data))\n\t\tret = -EFAULT;\n\nout:\n\treturn ret;\n}\n\nstatic int ptrace_sethbpregs(struct task_struct *tsk, long num,\n\t\t\t     unsigned long __user *data)\n{\n\tint idx, gen_len, gen_type, implied_type, ret = 0;\n\tu32 user_val;\n\tstruct perf_event *bp;\n\tstruct arch_hw_breakpoint_ctrl ctrl;\n\tstruct perf_event_attr attr;\n\n\tif (num == 0)\n\t\tgoto out;\n\telse if (num < 0)\n\t\timplied_type = HW_BREAKPOINT_RW;\n\telse\n\t\timplied_type = HW_BREAKPOINT_X;\n\n\tidx = ptrace_hbp_num_to_idx(num);\n\tif (idx < 0 || idx >= ARM_MAX_HBP_SLOTS) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (get_user(user_val, data)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tbp = tsk->thread.debug.hbp[idx];\n\tif (!bp) {\n\t\tbp = ptrace_hbp_create(tsk, implied_type);\n\t\tif (IS_ERR(bp)) {\n\t\t\tret = PTR_ERR(bp);\n\t\t\tgoto out;\n\t\t}\n\t\ttsk->thread.debug.hbp[idx] = bp;\n\t}\n\n\tattr = bp->attr;\n\n\tif (num & 0x1) {\n\t\t/* Address */\n\t\tattr.bp_addr\t= user_val;\n\t} else {\n\t\t/* Control */\n\t\tdecode_ctrl_reg(user_val, &ctrl);\n\t\tret = arch_bp_generic_fields(ctrl, &gen_len, &gen_type);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tif ((gen_type & implied_type) != gen_type) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tattr.bp_len\t= gen_len;\n\t\tattr.bp_type\t= gen_type;\n\t\tattr.disabled\t= !ctrl.enabled;\n\t}\n\n\tret = modify_user_hw_breakpoint(bp, &attr);\nout:\n\treturn ret;\n}\n#endif\n\n/* regset get/set implementations */\n\nstatic int gpr_get(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n\tstruct pt_regs *regs = task_pt_regs(target);\n\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t   regs,\n\t\t\t\t   0, sizeof(*regs));\n}\n\nstatic int gpr_set(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   const void *kbuf, const void __user *ubuf)\n{\n\tint ret;\n\tstruct pt_regs newregs;\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t &newregs,\n\t\t\t\t 0, sizeof(newregs));\n\tif (ret)\n\t\treturn ret;\n\n\tif (!valid_user_regs(&newregs))\n\t\treturn -EINVAL;\n\n\t*task_pt_regs(target) = newregs;\n\treturn 0;\n}\n\nstatic int fpa_get(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t   &task_thread_info(target)->fpstate,\n\t\t\t\t   0, sizeof(struct user_fp));\n}\n\nstatic int fpa_set(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   const void *kbuf, const void __user *ubuf)\n{\n\tstruct thread_info *thread = task_thread_info(target);\n\n\tthread->used_cp[1] = thread->used_cp[2] = 1;\n\n\treturn user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t&thread->fpstate,\n\t\t0, sizeof(struct user_fp));\n}\n\n#ifdef CONFIG_VFP\n/*\n * VFP register get/set implementations.\n *\n * With respect to the kernel, struct user_fp is divided into three chunks:\n * 16 or 32 real VFP registers (d0-d15 or d0-31)\n *\tThese are transferred to/from the real registers in the task's\n *\tvfp_hard_struct.  The number of registers depends on the kernel\n *\tconfiguration.\n *\n * 16 or 0 fake VFP registers (d16-d31 or empty)\n *\ti.e., the user_vfp structure has space for 32 registers even if\n *\tthe kernel doesn't have them all.\n *\n *\tvfp_get() reads this chunk as zero where applicable\n *\tvfp_set() ignores this chunk\n *\n * 1 word for the FPSCR\n *\n * The bounds-checking logic built into user_regset_copyout and friends\n * means that we can make a simple sequence of calls to map the relevant data\n * to/from the specified slice of the user regset structure.\n */\nstatic int vfp_get(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n\tint ret;\n\tstruct thread_info *thread = task_thread_info(target);\n\tstruct vfp_hard_struct const *vfp = &thread->vfpstate.hard;\n\tconst size_t user_fpregs_offset = offsetof(struct user_vfp, fpregs);\n\tconst size_t user_fpscr_offset = offsetof(struct user_vfp, fpscr);\n\n\tvfp_sync_hwstate(thread);\n\n\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  &vfp->fpregs,\n\t\t\t\t  user_fpregs_offset,\n\t\t\t\t  user_fpregs_offset + sizeof(vfp->fpregs));\n\tif (ret)\n\t\treturn ret;\n\n\tret = user_regset_copyout_zero(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t       user_fpregs_offset + sizeof(vfp->fpregs),\n\t\t\t\t       user_fpscr_offset);\n\tif (ret)\n\t\treturn ret;\n\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t   &vfp->fpscr,\n\t\t\t\t   user_fpscr_offset,\n\t\t\t\t   user_fpscr_offset + sizeof(vfp->fpscr));\n}\n\n/*\n * For vfp_set() a read-modify-write is done on the VFP registers,\n * in order to avoid writing back a half-modified set of registers on\n * failure.\n */\nstatic int vfp_set(struct task_struct *target,\n\t\t\t  const struct user_regset *regset,\n\t\t\t  unsigned int pos, unsigned int count,\n\t\t\t  const void *kbuf, const void __user *ubuf)\n{\n\tint ret;\n\tstruct thread_info *thread = task_thread_info(target);\n\tstruct vfp_hard_struct new_vfp;\n\tconst size_t user_fpregs_offset = offsetof(struct user_vfp, fpregs);\n\tconst size_t user_fpscr_offset = offsetof(struct user_vfp, fpscr);\n\n\tvfp_sync_hwstate(thread);\n\tnew_vfp = thread->vfpstate.hard;\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  &new_vfp.fpregs,\n\t\t\t\t  user_fpregs_offset,\n\t\t\t\t  user_fpregs_offset + sizeof(new_vfp.fpregs));\n\tif (ret)\n\t\treturn ret;\n\n\tret = user_regset_copyin_ignore(&pos, &count, &kbuf, &ubuf,\n\t\t\t\tuser_fpregs_offset + sizeof(new_vfp.fpregs),\n\t\t\t\tuser_fpscr_offset);\n\tif (ret)\n\t\treturn ret;\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t &new_vfp.fpscr,\n\t\t\t\t user_fpscr_offset,\n\t\t\t\t user_fpscr_offset + sizeof(new_vfp.fpscr));\n\tif (ret)\n\t\treturn ret;\n\n\tvfp_flush_hwstate(thread);\n\tthread->vfpstate.hard = new_vfp;\n\n\treturn 0;\n}\n#endif /* CONFIG_VFP */\n\nenum arm_regset {\n\tREGSET_GPR,\n\tREGSET_FPR,\n#ifdef CONFIG_VFP\n\tREGSET_VFP,\n#endif\n};\n\nstatic const struct user_regset arm_regsets[] = {\n\t[REGSET_GPR] = {\n\t\t.core_note_type = NT_PRSTATUS,\n\t\t.n = ELF_NGREG,\n\t\t.size = sizeof(u32),\n\t\t.align = sizeof(u32),\n\t\t.get = gpr_get,\n\t\t.set = gpr_set\n\t},\n\t[REGSET_FPR] = {\n\t\t/*\n\t\t * For the FPA regs in fpstate, the real fields are a mixture\n\t\t * of sizes, so pretend that the registers are word-sized:\n\t\t */\n\t\t.core_note_type = NT_PRFPREG,\n\t\t.n = sizeof(struct user_fp) / sizeof(u32),\n\t\t.size = sizeof(u32),\n\t\t.align = sizeof(u32),\n\t\t.get = fpa_get,\n\t\t.set = fpa_set\n\t},\n#ifdef CONFIG_VFP\n\t[REGSET_VFP] = {\n\t\t/*\n\t\t * Pretend that the VFP regs are word-sized, since the FPSCR is\n\t\t * a single word dangling at the end of struct user_vfp:\n\t\t */\n\t\t.core_note_type = NT_ARM_VFP,\n\t\t.n = ARM_VFPREGS_SIZE / sizeof(u32),\n\t\t.size = sizeof(u32),\n\t\t.align = sizeof(u32),\n\t\t.get = vfp_get,\n\t\t.set = vfp_set\n\t},\n#endif /* CONFIG_VFP */\n};\n\nstatic const struct user_regset_view user_arm_view = {\n\t.name = \"arm\", .e_machine = ELF_ARCH, .ei_osabi = ELF_OSABI,\n\t.regsets = arm_regsets, .n = ARRAY_SIZE(arm_regsets)\n};\n\nconst struct user_regset_view *task_user_regset_view(struct task_struct *task)\n{\n\treturn &user_arm_view;\n}\n\nlong arch_ptrace(struct task_struct *child, long request,\n\t\t unsigned long addr, unsigned long data)\n{\n\tint ret;\n\tunsigned long __user *datap = (unsigned long __user *) data;\n\n\tswitch (request) {\n\t\tcase PTRACE_PEEKUSR:\n\t\t\tret = ptrace_read_user(child, addr, datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_POKEUSR:\n\t\t\tret = ptrace_write_user(child, addr, data);\n\t\t\tbreak;\n\n\t\tcase PTRACE_GETREGS:\n\t\t\tret = copy_regset_to_user(child,\n\t\t\t\t\t\t  &user_arm_view, REGSET_GPR,\n\t\t\t\t\t\t  0, sizeof(struct pt_regs),\n\t\t\t\t\t\t  datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETREGS:\n\t\t\tret = copy_regset_from_user(child,\n\t\t\t\t\t\t    &user_arm_view, REGSET_GPR,\n\t\t\t\t\t\t    0, sizeof(struct pt_regs),\n\t\t\t\t\t\t    datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_GETFPREGS:\n\t\t\tret = copy_regset_to_user(child,\n\t\t\t\t\t\t  &user_arm_view, REGSET_FPR,\n\t\t\t\t\t\t  0, sizeof(union fp_state),\n\t\t\t\t\t\t  datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETFPREGS:\n\t\t\tret = copy_regset_from_user(child,\n\t\t\t\t\t\t    &user_arm_view, REGSET_FPR,\n\t\t\t\t\t\t    0, sizeof(union fp_state),\n\t\t\t\t\t\t    datap);\n\t\t\tbreak;\n\n#ifdef CONFIG_IWMMXT\n\t\tcase PTRACE_GETWMMXREGS:\n\t\t\tret = ptrace_getwmmxregs(child, datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETWMMXREGS:\n\t\t\tret = ptrace_setwmmxregs(child, datap);\n\t\t\tbreak;\n#endif\n\n\t\tcase PTRACE_GET_THREAD_AREA:\n\t\t\tret = put_user(task_thread_info(child)->tp_value,\n\t\t\t\t       datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SET_SYSCALL:\n\t\t\ttask_thread_info(child)->syscall = data;\n\t\t\tret = 0;\n\t\t\tbreak;\n\n#ifdef CONFIG_CRUNCH\n\t\tcase PTRACE_GETCRUNCHREGS:\n\t\t\tret = ptrace_getcrunchregs(child, datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETCRUNCHREGS:\n\t\t\tret = ptrace_setcrunchregs(child, datap);\n\t\t\tbreak;\n#endif\n\n#ifdef CONFIG_VFP\n\t\tcase PTRACE_GETVFPREGS:\n\t\t\tret = copy_regset_to_user(child,\n\t\t\t\t\t\t  &user_arm_view, REGSET_VFP,\n\t\t\t\t\t\t  0, ARM_VFPREGS_SIZE,\n\t\t\t\t\t\t  datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETVFPREGS:\n\t\t\tret = copy_regset_from_user(child,\n\t\t\t\t\t\t    &user_arm_view, REGSET_VFP,\n\t\t\t\t\t\t    0, ARM_VFPREGS_SIZE,\n\t\t\t\t\t\t    datap);\n\t\t\tbreak;\n#endif\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\t\tcase PTRACE_GETHBPREGS:\n\t\t\tif (ptrace_get_breakpoints(child) < 0)\n\t\t\t\treturn -ESRCH;\n\n\t\t\tret = ptrace_gethbpregs(child, addr,\n\t\t\t\t\t\t(unsigned long __user *)data);\n\t\t\tptrace_put_breakpoints(child);\n\t\t\tbreak;\n\t\tcase PTRACE_SETHBPREGS:\n\t\t\tif (ptrace_get_breakpoints(child) < 0)\n\t\t\t\treturn -ESRCH;\n\n\t\t\tret = ptrace_sethbpregs(child, addr,\n\t\t\t\t\t\t(unsigned long __user *)data);\n\t\t\tptrace_put_breakpoints(child);\n\t\t\tbreak;\n#endif\n\n\t\tdefault:\n\t\t\tret = ptrace_request(child, request, addr, data);\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nenum ptrace_syscall_dir {\n\tPTRACE_SYSCALL_ENTER = 0,\n\tPTRACE_SYSCALL_EXIT,\n};\n\nstatic int tracehook_report_syscall(struct pt_regs *regs,\n\t\t\t\t    enum ptrace_syscall_dir dir)\n{\n\tunsigned long ip;\n\n\t/*\n\t * IP is used to denote syscall entry/exit:\n\t * IP = 0 -> entry, =1 -> exit\n\t */\n\tip = regs->ARM_ip;\n\tregs->ARM_ip = dir;\n\n\tif (dir == PTRACE_SYSCALL_EXIT)\n\t\ttracehook_report_syscall_exit(regs, 0);\n\telse if (tracehook_report_syscall_entry(regs))\n\t\tcurrent_thread_info()->syscall = -1;\n\n\tregs->ARM_ip = ip;\n\treturn current_thread_info()->syscall;\n}\n\nasmlinkage int syscall_trace_enter(struct pt_regs *regs, int scno)\n{\n\tcurrent_thread_info()->syscall = scno;\n\n\t/* Do the secure computing check first; failures should be fast. */\n\tif (secure_computing(scno) == -1)\n\t\treturn -1;\n\n\tif (test_thread_flag(TIF_SYSCALL_TRACE))\n\t\tscno = tracehook_report_syscall(regs, PTRACE_SYSCALL_ENTER);\n\n\tif (test_thread_flag(TIF_SYSCALL_TRACEPOINT))\n\t\ttrace_sys_enter(regs, scno);\n\n\taudit_syscall_entry(AUDIT_ARCH_ARM, scno, regs->ARM_r0, regs->ARM_r1,\n\t\t\t    regs->ARM_r2, regs->ARM_r3);\n\n\treturn scno;\n}\n\nasmlinkage void syscall_trace_exit(struct pt_regs *regs)\n{\n\t/*\n\t * Audit the syscall before anything else, as a debugger may\n\t * come in and change the current registers.\n\t */\n\taudit_syscall_exit(regs);\n\n\t/*\n\t * Note that we haven't updated the ->syscall field for the\n\t * current thread. This isn't a problem because it will have\n\t * been set on syscall entry and there hasn't been an opportunity\n\t * for a PTRACE_SET_SYSCALL since then.\n\t */\n\tif (test_thread_flag(TIF_SYSCALL_TRACEPOINT))\n\t\ttrace_sys_exit(regs, regs_return_value(regs));\n\n\tif (test_thread_flag(TIF_SYSCALL_TRACE))\n\t\ttracehook_report_syscall(regs, PTRACE_SYSCALL_EXIT);\n}\n", "/*\n *  linux/arch/arm/kernel/traps.c\n *\n *  Copyright (C) 1995-2009 Russell King\n *  Fragments that appear the same as linux/arch/i386/kernel/traps.c (C) Linus Torvalds\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n *  'traps.c' handles hardware exceptions after we have saved some state in\n *  'linux/arch/arm/lib/traps.S'.  Mostly a debugging aid, but will probably\n *  kill the offending process.\n */\n#include <linux/signal.h>\n#include <linux/personality.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/uaccess.h>\n#include <linux/hardirq.h>\n#include <linux/kdebug.h>\n#include <linux/module.h>\n#include <linux/kexec.h>\n#include <linux/bug.h>\n#include <linux/delay.h>\n#include <linux/init.h>\n#include <linux/sched.h>\n\n#include <linux/atomic.h>\n#include <asm/cacheflush.h>\n#include <asm/exception.h>\n#include <asm/unistd.h>\n#include <asm/traps.h>\n#include <asm/unwind.h>\n#include <asm/tls.h>\n#include <asm/system_misc.h>\n\n#include \"signal.h\"\n\nstatic const char *handler[]= { \"prefetch abort\", \"data abort\", \"address exception\", \"interrupt\" };\n\nvoid *vectors_page;\n\n#ifdef CONFIG_DEBUG_USER\nunsigned int user_debug;\n\nstatic int __init user_debug_setup(char *str)\n{\n\tget_option(&str, &user_debug);\n\treturn 1;\n}\n__setup(\"user_debug=\", user_debug_setup);\n#endif\n\nstatic void dump_mem(const char *, const char *, unsigned long, unsigned long);\n\nvoid dump_backtrace_entry(unsigned long where, unsigned long from, unsigned long frame)\n{\n#ifdef CONFIG_KALLSYMS\n\tprintk(\"[<%08lx>] (%pS) from [<%08lx>] (%pS)\\n\", where, (void *)where, from, (void *)from);\n#else\n\tprintk(\"Function entered at [<%08lx>] from [<%08lx>]\\n\", where, from);\n#endif\n\n\tif (in_exception_text(where))\n\t\tdump_mem(\"\", \"Exception stack\", frame + 4, frame + 4 + sizeof(struct pt_regs));\n}\n\n#ifndef CONFIG_ARM_UNWIND\n/*\n * Stack pointers should always be within the kernels view of\n * physical memory.  If it is not there, then we can't dump\n * out any information relating to the stack.\n */\nstatic int verify_stack(unsigned long sp)\n{\n\tif (sp < PAGE_OFFSET ||\n\t    (sp > (unsigned long)high_memory && high_memory != NULL))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n#endif\n\n/*\n * Dump out the contents of some memory nicely...\n */\nstatic void dump_mem(const char *lvl, const char *str, unsigned long bottom,\n\t\t     unsigned long top)\n{\n\tunsigned long first;\n\tmm_segment_t fs;\n\tint i;\n\n\t/*\n\t * We need to switch to kernel mode so that we can use __get_user\n\t * to safely read from kernel space.  Note that we now dump the\n\t * code first, just in case the backtrace kills us.\n\t */\n\tfs = get_fs();\n\tset_fs(KERNEL_DS);\n\n\tprintk(\"%s%s(0x%08lx to 0x%08lx)\\n\", lvl, str, bottom, top);\n\n\tfor (first = bottom & ~31; first < top; first += 32) {\n\t\tunsigned long p;\n\t\tchar str[sizeof(\" 12345678\") * 8 + 1];\n\n\t\tmemset(str, ' ', sizeof(str));\n\t\tstr[sizeof(str) - 1] = '\\0';\n\n\t\tfor (p = first, i = 0; i < 8 && p < top; i++, p += 4) {\n\t\t\tif (p >= bottom && p < top) {\n\t\t\t\tunsigned long val;\n\t\t\t\tif (__get_user(val, (unsigned long *)p) == 0)\n\t\t\t\t\tsprintf(str + i * 9, \" %08lx\", val);\n\t\t\t\telse\n\t\t\t\t\tsprintf(str + i * 9, \" ????????\");\n\t\t\t}\n\t\t}\n\t\tprintk(\"%s%04lx:%s\\n\", lvl, first & 0xffff, str);\n\t}\n\n\tset_fs(fs);\n}\n\nstatic void dump_instr(const char *lvl, struct pt_regs *regs)\n{\n\tunsigned long addr = instruction_pointer(regs);\n\tconst int thumb = thumb_mode(regs);\n\tconst int width = thumb ? 4 : 8;\n\tmm_segment_t fs;\n\tchar str[sizeof(\"00000000 \") * 5 + 2 + 1], *p = str;\n\tint i;\n\n\t/*\n\t * We need to switch to kernel mode so that we can use __get_user\n\t * to safely read from kernel space.  Note that we now dump the\n\t * code first, just in case the backtrace kills us.\n\t */\n\tfs = get_fs();\n\tset_fs(KERNEL_DS);\n\n\tfor (i = -4; i < 1 + !!thumb; i++) {\n\t\tunsigned int val, bad;\n\n\t\tif (thumb)\n\t\t\tbad = __get_user(val, &((u16 *)addr)[i]);\n\t\telse\n\t\t\tbad = __get_user(val, &((u32 *)addr)[i]);\n\n\t\tif (!bad)\n\t\t\tp += sprintf(p, i == 0 ? \"(%0*x) \" : \"%0*x \",\n\t\t\t\t\twidth, val);\n\t\telse {\n\t\t\tp += sprintf(p, \"bad PC value\");\n\t\t\tbreak;\n\t\t}\n\t}\n\tprintk(\"%sCode: %s\\n\", lvl, str);\n\n\tset_fs(fs);\n}\n\n#ifdef CONFIG_ARM_UNWIND\nstatic inline void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)\n{\n\tunwind_backtrace(regs, tsk);\n}\n#else\nstatic void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)\n{\n\tunsigned int fp, mode;\n\tint ok = 1;\n\n\tprintk(\"Backtrace: \");\n\n\tif (!tsk)\n\t\ttsk = current;\n\n\tif (regs) {\n\t\tfp = regs->ARM_fp;\n\t\tmode = processor_mode(regs);\n\t} else if (tsk != current) {\n\t\tfp = thread_saved_fp(tsk);\n\t\tmode = 0x10;\n\t} else {\n\t\tasm(\"mov %0, fp\" : \"=r\" (fp) : : \"cc\");\n\t\tmode = 0x10;\n\t}\n\n\tif (!fp) {\n\t\tprintk(\"no frame pointer\");\n\t\tok = 0;\n\t} else if (verify_stack(fp)) {\n\t\tprintk(\"invalid frame pointer 0x%08x\", fp);\n\t\tok = 0;\n\t} else if (fp < (unsigned long)end_of_stack(tsk))\n\t\tprintk(\"frame pointer underflow\");\n\tprintk(\"\\n\");\n\n\tif (ok)\n\t\tc_backtrace(fp, mode);\n}\n#endif\n\nvoid show_stack(struct task_struct *tsk, unsigned long *sp)\n{\n\tdump_backtrace(NULL, tsk);\n\tbarrier();\n}\n\n#ifdef CONFIG_PREEMPT\n#define S_PREEMPT \" PREEMPT\"\n#else\n#define S_PREEMPT \"\"\n#endif\n#ifdef CONFIG_SMP\n#define S_SMP \" SMP\"\n#else\n#define S_SMP \"\"\n#endif\n#ifdef CONFIG_THUMB2_KERNEL\n#define S_ISA \" THUMB2\"\n#else\n#define S_ISA \" ARM\"\n#endif\n\nstatic int __die(const char *str, int err, struct pt_regs *regs)\n{\n\tstruct task_struct *tsk = current;\n\tstatic int die_counter;\n\tint ret;\n\n\tprintk(KERN_EMERG \"Internal error: %s: %x [#%d]\" S_PREEMPT S_SMP\n\t       S_ISA \"\\n\", str, err, ++die_counter);\n\n\t/* trap and error numbers are mostly meaningless on ARM */\n\tret = notify_die(DIE_OOPS, str, regs, err, tsk->thread.trap_no, SIGSEGV);\n\tif (ret == NOTIFY_STOP)\n\t\treturn 1;\n\n\tprint_modules();\n\t__show_regs(regs);\n\tprintk(KERN_EMERG \"Process %.*s (pid: %d, stack limit = 0x%p)\\n\",\n\t\tTASK_COMM_LEN, tsk->comm, task_pid_nr(tsk), end_of_stack(tsk));\n\n\tif (!user_mode(regs) || in_interrupt()) {\n\t\tdump_mem(KERN_EMERG, \"Stack: \", regs->ARM_sp,\n\t\t\t THREAD_SIZE + (unsigned long)task_stack_page(tsk));\n\t\tdump_backtrace(regs, tsk);\n\t\tdump_instr(KERN_EMERG, regs);\n\t}\n\n\treturn 0;\n}\n\nstatic arch_spinlock_t die_lock = __ARCH_SPIN_LOCK_UNLOCKED;\nstatic int die_owner = -1;\nstatic unsigned int die_nest_count;\n\nstatic unsigned long oops_begin(void)\n{\n\tint cpu;\n\tunsigned long flags;\n\n\toops_enter();\n\n\t/* racy, but better than risking deadlock. */\n\traw_local_irq_save(flags);\n\tcpu = smp_processor_id();\n\tif (!arch_spin_trylock(&die_lock)) {\n\t\tif (cpu == die_owner)\n\t\t\t/* nested oops. should stop eventually */;\n\t\telse\n\t\t\tarch_spin_lock(&die_lock);\n\t}\n\tdie_nest_count++;\n\tdie_owner = cpu;\n\tconsole_verbose();\n\tbust_spinlocks(1);\n\treturn flags;\n}\n\nstatic void oops_end(unsigned long flags, struct pt_regs *regs, int signr)\n{\n\tif (regs && kexec_should_crash(current))\n\t\tcrash_kexec(regs);\n\n\tbust_spinlocks(0);\n\tdie_owner = -1;\n\tadd_taint(TAINT_DIE, LOCKDEP_NOW_UNRELIABLE);\n\tdie_nest_count--;\n\tif (!die_nest_count)\n\t\t/* Nest count reaches zero, release the lock. */\n\t\tarch_spin_unlock(&die_lock);\n\traw_local_irq_restore(flags);\n\toops_exit();\n\n\tif (in_interrupt())\n\t\tpanic(\"Fatal exception in interrupt\");\n\tif (panic_on_oops)\n\t\tpanic(\"Fatal exception\");\n\tif (signr)\n\t\tdo_exit(signr);\n}\n\n/*\n * This function is protected against re-entrancy.\n */\nvoid die(const char *str, struct pt_regs *regs, int err)\n{\n\tenum bug_trap_type bug_type = BUG_TRAP_TYPE_NONE;\n\tunsigned long flags = oops_begin();\n\tint sig = SIGSEGV;\n\n\tif (!user_mode(regs))\n\t\tbug_type = report_bug(regs->ARM_pc, regs);\n\tif (bug_type != BUG_TRAP_TYPE_NONE)\n\t\tstr = \"Oops - BUG\";\n\n\tif (__die(str, err, regs))\n\t\tsig = 0;\n\n\toops_end(flags, regs, sig);\n}\n\nvoid arm_notify_die(const char *str, struct pt_regs *regs,\n\t\tstruct siginfo *info, unsigned long err, unsigned long trap)\n{\n\tif (user_mode(regs)) {\n\t\tcurrent->thread.error_code = err;\n\t\tcurrent->thread.trap_no = trap;\n\n\t\tforce_sig_info(info->si_signo, info, current);\n\t} else {\n\t\tdie(str, regs, err);\n\t}\n}\n\n#ifdef CONFIG_GENERIC_BUG\n\nint is_valid_bugaddr(unsigned long pc)\n{\n#ifdef CONFIG_THUMB2_KERNEL\n\tunsigned short bkpt;\n#else\n\tunsigned long bkpt;\n#endif\n\n\tif (probe_kernel_address((unsigned *)pc, bkpt))\n\t\treturn 0;\n\n\treturn bkpt == BUG_INSTR_VALUE;\n}\n\n#endif\n\nstatic LIST_HEAD(undef_hook);\nstatic DEFINE_RAW_SPINLOCK(undef_lock);\n\nvoid register_undef_hook(struct undef_hook *hook)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&undef_lock, flags);\n\tlist_add(&hook->node, &undef_hook);\n\traw_spin_unlock_irqrestore(&undef_lock, flags);\n}\n\nvoid unregister_undef_hook(struct undef_hook *hook)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&undef_lock, flags);\n\tlist_del(&hook->node);\n\traw_spin_unlock_irqrestore(&undef_lock, flags);\n}\n\nstatic int call_undef_hook(struct pt_regs *regs, unsigned int instr)\n{\n\tstruct undef_hook *hook;\n\tunsigned long flags;\n\tint (*fn)(struct pt_regs *regs, unsigned int instr) = NULL;\n\n\traw_spin_lock_irqsave(&undef_lock, flags);\n\tlist_for_each_entry(hook, &undef_hook, node)\n\t\tif ((instr & hook->instr_mask) == hook->instr_val &&\n\t\t    (regs->ARM_cpsr & hook->cpsr_mask) == hook->cpsr_val)\n\t\t\tfn = hook->fn;\n\traw_spin_unlock_irqrestore(&undef_lock, flags);\n\n\treturn fn ? fn(regs, instr) : 1;\n}\n\nasmlinkage void __exception do_undefinstr(struct pt_regs *regs)\n{\n\tunsigned int instr;\n\tsiginfo_t info;\n\tvoid __user *pc;\n\n\tpc = (void __user *)instruction_pointer(regs);\n\n\tif (processor_mode(regs) == SVC_MODE) {\n#ifdef CONFIG_THUMB2_KERNEL\n\t\tif (thumb_mode(regs)) {\n\t\t\tinstr = ((u16 *)pc)[0];\n\t\t\tif (is_wide_instruction(instr)) {\n\t\t\t\tinstr <<= 16;\n\t\t\t\tinstr |= ((u16 *)pc)[1];\n\t\t\t}\n\t\t} else\n#endif\n\t\t\tinstr = *(u32 *) pc;\n\t} else if (thumb_mode(regs)) {\n\t\tif (get_user(instr, (u16 __user *)pc))\n\t\t\tgoto die_sig;\n\t\tif (is_wide_instruction(instr)) {\n\t\t\tunsigned int instr2;\n\t\t\tif (get_user(instr2, (u16 __user *)pc+1))\n\t\t\t\tgoto die_sig;\n\t\t\tinstr <<= 16;\n\t\t\tinstr |= instr2;\n\t\t}\n\t} else if (get_user(instr, (u32 __user *)pc)) {\n\t\tgoto die_sig;\n\t}\n\n\tif (call_undef_hook(regs, instr) == 0)\n\t\treturn;\n\ndie_sig:\n#ifdef CONFIG_DEBUG_USER\n\tif (user_debug & UDBG_UNDEFINED) {\n\t\tprintk(KERN_INFO \"%s (%d): undefined instruction: pc=%p\\n\",\n\t\t\tcurrent->comm, task_pid_nr(current), pc);\n\t\tdump_instr(KERN_INFO, regs);\n\t}\n#endif\n\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLOPC;\n\tinfo.si_addr  = pc;\n\n\tarm_notify_die(\"Oops - undefined instruction\", regs, &info, 0, 6);\n}\n\nasmlinkage void do_unexp_fiq (struct pt_regs *regs)\n{\n\tprintk(\"Hmm.  Unexpected FIQ received, but trying to continue\\n\");\n\tprintk(\"You may have a hardware problem...\\n\");\n}\n\n/*\n * bad_mode handles the impossible case in the vectors.  If you see one of\n * these, then it's extremely serious, and could mean you have buggy hardware.\n * It never returns, and never tries to sync.  We hope that we can at least\n * dump out some state information...\n */\nasmlinkage void bad_mode(struct pt_regs *regs, int reason)\n{\n\tconsole_verbose();\n\n\tprintk(KERN_CRIT \"Bad mode in %s handler detected\\n\", handler[reason]);\n\n\tdie(\"Oops - bad mode\", regs, 0);\n\tlocal_irq_disable();\n\tpanic(\"bad mode\");\n}\n\nstatic int bad_syscall(int n, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((current->personality & PER_MASK) != PER_LINUX &&\n\t    thread->exec_domain->handler) {\n\t\tthread->exec_domain->handler(n, regs);\n\t\treturn regs->ARM_r0;\n\t}\n\n#ifdef CONFIG_DEBUG_USER\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(KERN_ERR \"[%d] %s: obsolete system call %08x.\\n\",\n\t\t\ttask_pid_nr(current), current->comm, n);\n\t\tdump_instr(KERN_ERR, regs);\n\t}\n#endif\n\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall\", regs, &info, n, 0);\n\n\treturn regs->ARM_r0;\n}\n\nstatic inline int\ndo_cache_op(unsigned long start, unsigned long end, int flags)\n{\n\tstruct mm_struct *mm = current->active_mm;\n\tstruct vm_area_struct *vma;\n\n\tif (end < start || flags)\n\t\treturn -EINVAL;\n\n\tdown_read(&mm->mmap_sem);\n\tvma = find_vma(mm, start);\n\tif (vma && vma->vm_start < end) {\n\t\tif (start < vma->vm_start)\n\t\t\tstart = vma->vm_start;\n\t\tif (end > vma->vm_end)\n\t\t\tend = vma->vm_end;\n\n\t\tup_read(&mm->mmap_sem);\n\t\treturn flush_cache_user_range(start, end);\n\t}\n\tup_read(&mm->mmap_sem);\n\treturn -EINVAL;\n}\n\n/*\n * Handle all unrecognised system calls.\n *  0x9f0000 - 0x9fffff are some more esoteric system calls\n */\n#define NR(x) ((__ARM_NR_##x) - __ARM_NR_BASE)\nasmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}\n\n#ifdef CONFIG_TLS_REG_EMUL\n\n/*\n * We might be running on an ARMv6+ processor which should have the TLS\n * register but for some reason we can't use it, or maybe an SMP system\n * using a pre-ARMv6 processor (there are apparently a few prototypes like\n * that in existence) and therefore access to that register must be\n * emulated.\n */\n\nstatic int get_tp_trap(struct pt_regs *regs, unsigned int instr)\n{\n\tint reg = (instr >> 12) & 15;\n\tif (reg == 15)\n\t\treturn 1;\n\tregs->uregs[reg] = current_thread_info()->tp_value;\n\tregs->ARM_pc += 4;\n\treturn 0;\n}\n\nstatic struct undef_hook arm_mrc_hook = {\n\t.instr_mask\t= 0x0fff0fff,\n\t.instr_val\t= 0x0e1d0f70,\n\t.cpsr_mask\t= PSR_T_BIT,\n\t.cpsr_val\t= 0,\n\t.fn\t\t= get_tp_trap,\n};\n\nstatic int __init arm_mrc_hook_init(void)\n{\n\tregister_undef_hook(&arm_mrc_hook);\n\treturn 0;\n}\n\nlate_initcall(arm_mrc_hook_init);\n\n#endif\n\nvoid __bad_xchg(volatile void *ptr, int size)\n{\n\tprintk(\"xchg: bad data size: pc 0x%p, ptr 0x%p, size %d\\n\",\n\t\t__builtin_return_address(0), ptr, size);\n\tBUG();\n}\nEXPORT_SYMBOL(__bad_xchg);\n\n/*\n * A data abort trap was taken, but we did not handle the instruction.\n * Try to abort the user program, or panic if it was the kernel.\n */\nasmlinkage void\nbaddataabort(int code, unsigned long instr, struct pt_regs *regs)\n{\n\tunsigned long addr = instruction_pointer(regs);\n\tsiginfo_t info;\n\n#ifdef CONFIG_DEBUG_USER\n\tif (user_debug & UDBG_BADABORT) {\n\t\tprintk(KERN_ERR \"[%d] %s: bad data abort: code %d instr 0x%08lx\\n\",\n\t\t\ttask_pid_nr(current), current->comm, code, instr);\n\t\tdump_instr(KERN_ERR, regs);\n\t\tshow_pte(current->mm, addr);\n\t}\n#endif\n\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLOPC;\n\tinfo.si_addr  = (void __user *)addr;\n\n\tarm_notify_die(\"unknown data abort code\", regs, &info, instr, 0);\n}\n\nvoid __readwrite_bug(const char *fn)\n{\n\tprintk(\"%s called, but not implemented\\n\", fn);\n\tBUG();\n}\nEXPORT_SYMBOL(__readwrite_bug);\n\nvoid __pte_error(const char *file, int line, pte_t pte)\n{\n\tprintk(\"%s:%d: bad pte %08llx.\\n\", file, line, (long long)pte_val(pte));\n}\n\nvoid __pmd_error(const char *file, int line, pmd_t pmd)\n{\n\tprintk(\"%s:%d: bad pmd %08llx.\\n\", file, line, (long long)pmd_val(pmd));\n}\n\nvoid __pgd_error(const char *file, int line, pgd_t pgd)\n{\n\tprintk(\"%s:%d: bad pgd %08llx.\\n\", file, line, (long long)pgd_val(pgd));\n}\n\nasmlinkage void __div0(void)\n{\n\tprintk(\"Division by zero in kernel.\\n\");\n\tdump_stack();\n}\nEXPORT_SYMBOL(__div0);\n\nvoid abort(void)\n{\n\tBUG();\n\n\t/* if that doesn't kill us, halt */\n\tpanic(\"Oops failed to kill thread\");\n}\nEXPORT_SYMBOL(abort);\n\nvoid __init trap_init(void)\n{\n\treturn;\n}\n\nstatic void __init kuser_get_tls_init(unsigned long vectors)\n{\n\t/*\n\t * vectors + 0xfe0 = __kuser_get_tls\n\t * vectors + 0xfe8 = hardware TLS instruction at 0xffff0fe8\n\t */\n\tif (tls_emu || has_tls_reg)\n\t\tmemcpy((void *)vectors + 0xfe0, (void *)vectors + 0xfe8, 4);\n}\n\nvoid __init early_trap_init(void *vectors_base)\n{\n\tunsigned long vectors = (unsigned long)vectors_base;\n\textern char __stubs_start[], __stubs_end[];\n\textern char __vectors_start[], __vectors_end[];\n\textern char __kuser_helper_start[], __kuser_helper_end[];\n\tint kuser_sz = __kuser_helper_end - __kuser_helper_start;\n\n\tvectors_page = vectors_base;\n\n\t/*\n\t * Copy the vectors, stubs and kuser helpers (in entry-armv.S)\n\t * into the vector page, mapped at 0xffff0000, and ensure these\n\t * are visible to the instruction stream.\n\t */\n\tmemcpy((void *)vectors, __vectors_start, __vectors_end - __vectors_start);\n\tmemcpy((void *)vectors + 0x200, __stubs_start, __stubs_end - __stubs_start);\n\tmemcpy((void *)vectors + 0x1000 - kuser_sz, __kuser_helper_start, kuser_sz);\n\n\t/*\n\t * Do processor specific fixups for the kuser helpers\n\t */\n\tkuser_get_tls_init(vectors);\n\n\t/*\n\t * Copy signal return handlers into the vector page, and\n\t * set sigreturn to be a pointer to these.\n\t */\n\tmemcpy((void *)(vectors + KERN_SIGRETURN_CODE - CONFIG_VECTORS_BASE),\n\t       sigreturn_codes, sizeof(sigreturn_codes));\n\n\tflush_icache_range(vectors, vectors + PAGE_SIZE);\n\tmodify_domain(DOMAIN_USER, DOMAIN_CLIENT);\n}\n"], "fixing_code": ["/*\n *  arch/arm/include/asm/thread_info.h\n *\n *  Copyright (C) 2002 Russell King.\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#ifndef __ASM_ARM_THREAD_INFO_H\n#define __ASM_ARM_THREAD_INFO_H\n\n#ifdef __KERNEL__\n\n#include <linux/compiler.h>\n#include <asm/fpstate.h>\n\n#define THREAD_SIZE_ORDER\t1\n#define THREAD_SIZE\t\t8192\n#define THREAD_START_SP\t\t(THREAD_SIZE - 8)\n\n#ifndef __ASSEMBLY__\n\nstruct task_struct;\nstruct exec_domain;\n\n#include <asm/types.h>\n#include <asm/domain.h>\n\ntypedef unsigned long mm_segment_t;\n\nstruct cpu_context_save {\n\t__u32\tr4;\n\t__u32\tr5;\n\t__u32\tr6;\n\t__u32\tr7;\n\t__u32\tr8;\n\t__u32\tr9;\n\t__u32\tsl;\n\t__u32\tfp;\n\t__u32\tsp;\n\t__u32\tpc;\n\t__u32\textra[2];\t\t/* Xscale 'acc' register, etc */\n};\n\n/*\n * low level task data that entry.S needs immediate access to.\n * __switch_to() assumes cpu_context follows immediately after cpu_domain.\n */\nstruct thread_info {\n\tunsigned long\t\tflags;\t\t/* low level flags */\n\tint\t\t\tpreempt_count;\t/* 0 => preemptable, <0 => bug */\n\tmm_segment_t\t\taddr_limit;\t/* address limit */\n\tstruct task_struct\t*task;\t\t/* main task structure */\n\tstruct exec_domain\t*exec_domain;\t/* execution domain */\n\t__u32\t\t\tcpu;\t\t/* cpu */\n\t__u32\t\t\tcpu_domain;\t/* cpu domain */\n\tstruct cpu_context_save\tcpu_context;\t/* cpu context */\n\t__u32\t\t\tsyscall;\t/* syscall number */\n\t__u8\t\t\tused_cp[16];\t/* thread used copro */\n\tunsigned long\t\ttp_value[2];\t/* TLS registers */\n#ifdef CONFIG_CRUNCH\n\tstruct crunch_state\tcrunchstate;\n#endif\n\tunion fp_state\t\tfpstate __attribute__((aligned(8)));\n\tunion vfp_state\t\tvfpstate;\n#ifdef CONFIG_ARM_THUMBEE\n\tunsigned long\t\tthumbee_state;\t/* ThumbEE Handler Base register */\n#endif\n\tstruct restart_block\trestart_block;\n};\n\n#define INIT_THREAD_INFO(tsk)\t\t\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\t.task\t\t= &tsk,\t\t\t\t\t\t\\\n\t.exec_domain\t= &default_exec_domain,\t\t\t\t\\\n\t.flags\t\t= 0,\t\t\t\t\t\t\\\n\t.preempt_count\t= INIT_PREEMPT_COUNT,\t\t\t\t\\\n\t.addr_limit\t= KERNEL_DS,\t\t\t\t\t\\\n\t.cpu_domain\t= domain_val(DOMAIN_USER, DOMAIN_MANAGER) |\t\\\n\t\t\t  domain_val(DOMAIN_KERNEL, DOMAIN_MANAGER) |\t\\\n\t\t\t  domain_val(DOMAIN_IO, DOMAIN_CLIENT),\t\t\\\n\t.restart_block\t= {\t\t\t\t\t\t\\\n\t\t.fn\t= do_no_restart_syscall,\t\t\t\\\n\t},\t\t\t\t\t\t\t\t\\\n}\n\n#define init_thread_info\t(init_thread_union.thread_info)\n#define init_stack\t\t(init_thread_union.stack)\n\n/*\n * how to get the thread information struct from C\n */\nstatic inline struct thread_info *current_thread_info(void) __attribute_const__;\n\nstatic inline struct thread_info *current_thread_info(void)\n{\n\tregister unsigned long sp asm (\"sp\");\n\treturn (struct thread_info *)(sp & ~(THREAD_SIZE - 1));\n}\n\n#define thread_saved_pc(tsk)\t\\\n\t((unsigned long)(task_thread_info(tsk)->cpu_context.pc))\n#define thread_saved_sp(tsk)\t\\\n\t((unsigned long)(task_thread_info(tsk)->cpu_context.sp))\n#define thread_saved_fp(tsk)\t\\\n\t((unsigned long)(task_thread_info(tsk)->cpu_context.fp))\n\nextern void crunch_task_disable(struct thread_info *);\nextern void crunch_task_copy(struct thread_info *, void *);\nextern void crunch_task_restore(struct thread_info *, void *);\nextern void crunch_task_release(struct thread_info *);\n\nextern void iwmmxt_task_disable(struct thread_info *);\nextern void iwmmxt_task_copy(struct thread_info *, void *);\nextern void iwmmxt_task_restore(struct thread_info *, void *);\nextern void iwmmxt_task_release(struct thread_info *);\nextern void iwmmxt_task_switch(struct thread_info *);\n\nextern void vfp_sync_hwstate(struct thread_info *);\nextern void vfp_flush_hwstate(struct thread_info *);\n\nstruct user_vfp;\nstruct user_vfp_exc;\n\nextern int vfp_preserve_user_clear_hwstate(struct user_vfp __user *,\n\t\t\t\t\t   struct user_vfp_exc __user *);\nextern int vfp_restore_user_hwstate(struct user_vfp __user *,\n\t\t\t\t    struct user_vfp_exc __user *);\n#endif\n\n/*\n * We use bit 30 of the preempt_count to indicate that kernel\n * preemption is occurring.  See <asm/hardirq.h>.\n */\n#define PREEMPT_ACTIVE\t0x40000000\n\n/*\n * thread information flags:\n *  TIF_SYSCALL_TRACE\t- syscall trace active\n *  TIF_SYSCAL_AUDIT\t- syscall auditing active\n *  TIF_SIGPENDING\t- signal pending\n *  TIF_NEED_RESCHED\t- rescheduling necessary\n *  TIF_NOTIFY_RESUME\t- callback before returning to user\n *  TIF_USEDFPU\t\t- FPU was used by this task this quantum (SMP)\n *  TIF_POLLING_NRFLAG\t- true if poll_idle() is polling TIF_NEED_RESCHED\n */\n#define TIF_SIGPENDING\t\t0\n#define TIF_NEED_RESCHED\t1\n#define TIF_NOTIFY_RESUME\t2\t/* callback before returning to user */\n#define TIF_SYSCALL_TRACE\t8\n#define TIF_SYSCALL_AUDIT\t9\n#define TIF_SYSCALL_TRACEPOINT\t10\n#define TIF_SECCOMP\t\t11\t/* seccomp syscall filtering active */\n#define TIF_NOHZ\t\t12\t/* in adaptive nohz mode */\n#define TIF_USING_IWMMXT\t17\n#define TIF_MEMDIE\t\t18\t/* is terminating due to OOM killer */\n#define TIF_RESTORE_SIGMASK\t20\n#define TIF_SWITCH_MM\t\t22\t/* deferred switch_mm */\n\n#define _TIF_SIGPENDING\t\t(1 << TIF_SIGPENDING)\n#define _TIF_NEED_RESCHED\t(1 << TIF_NEED_RESCHED)\n#define _TIF_NOTIFY_RESUME\t(1 << TIF_NOTIFY_RESUME)\n#define _TIF_SYSCALL_TRACE\t(1 << TIF_SYSCALL_TRACE)\n#define _TIF_SYSCALL_AUDIT\t(1 << TIF_SYSCALL_AUDIT)\n#define _TIF_SYSCALL_TRACEPOINT\t(1 << TIF_SYSCALL_TRACEPOINT)\n#define _TIF_SECCOMP\t\t(1 << TIF_SECCOMP)\n#define _TIF_USING_IWMMXT\t(1 << TIF_USING_IWMMXT)\n\n/* Checks for any syscall work in entry-common.S */\n#define _TIF_SYSCALL_WORK (_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | \\\n\t\t\t   _TIF_SYSCALL_TRACEPOINT | _TIF_SECCOMP)\n\n/*\n * Change these and you break ASM code in entry-common.S\n */\n#define _TIF_WORK_MASK\t\t(_TIF_NEED_RESCHED | _TIF_SIGPENDING | _TIF_NOTIFY_RESUME)\n\n#endif /* __KERNEL__ */\n#endif /* __ASM_ARM_THREAD_INFO_H */\n", "#ifndef __ASMARM_TLS_H\n#define __ASMARM_TLS_H\n\n#ifdef __ASSEMBLY__\n#include <asm/asm-offsets.h>\n\t.macro switch_tls_none, base, tp, tpuser, tmp1, tmp2\n\t.endm\n\n\t.macro switch_tls_v6k, base, tp, tpuser, tmp1, tmp2\n\tmrc\tp15, 0, \\tmp2, c13, c0, 2\t@ get the user r/w register\n\tmcr\tp15, 0, \\tp, c13, c0, 3\t\t@ set TLS register\n\tmcr\tp15, 0, \\tpuser, c13, c0, 2\t@ and the user r/w register\n\tstr\t\\tmp2, [\\base, #TI_TP_VALUE + 4] @ save it\n\t.endm\n\n\t.macro switch_tls_v6, base, tp, tpuser, tmp1, tmp2\n\tldr\t\\tmp1, =elf_hwcap\n\tldr\t\\tmp1, [\\tmp1, #0]\n\tmov\t\\tmp2, #0xffff0fff\n\ttst\t\\tmp1, #HWCAP_TLS\t\t@ hardware TLS available?\n\tstreq\t\\tp, [\\tmp2, #-15]\t\t@ set TLS value at 0xffff0ff0\n\tmrcne\tp15, 0, \\tmp2, c13, c0, 2\t@ get the user r/w register\n\tmcrne\tp15, 0, \\tp, c13, c0, 3\t\t@ yes, set TLS register\n\tmcrne\tp15, 0, \\tpuser, c13, c0, 2\t@ set user r/w register\n\tstrne\t\\tmp2, [\\base, #TI_TP_VALUE + 4] @ save it\n\t.endm\n\n\t.macro switch_tls_software, base, tp, tpuser, tmp1, tmp2\n\tmov\t\\tmp1, #0xffff0fff\n\tstr\t\\tp, [\\tmp1, #-15]\t\t@ set TLS value at 0xffff0ff0\n\t.endm\n#endif\n\n#ifdef CONFIG_TLS_REG_EMUL\n#define tls_emu\t\t1\n#define has_tls_reg\t\t1\n#define switch_tls\tswitch_tls_none\n#elif defined(CONFIG_CPU_V6)\n#define tls_emu\t\t0\n#define has_tls_reg\t\t(elf_hwcap & HWCAP_TLS)\n#define switch_tls\tswitch_tls_v6\n#elif defined(CONFIG_CPU_32v6K)\n#define tls_emu\t\t0\n#define has_tls_reg\t\t1\n#define switch_tls\tswitch_tls_v6k\n#else\n#define tls_emu\t\t0\n#define has_tls_reg\t\t0\n#define switch_tls\tswitch_tls_software\n#endif\n\n#ifndef __ASSEMBLY__\nstatic inline unsigned long get_tpuser(void)\n{\n\tunsigned long reg = 0;\n\n\tif (has_tls_reg && !tls_emu)\n\t\t__asm__(\"mrc p15, 0, %0, c13, c0, 2\" : \"=r\" (reg));\n\n\treturn reg;\n}\n#endif\n#endif\t/* __ASMARM_TLS_H */\n", "/*\n *  linux/arch/arm/kernel/entry-armv.S\n *\n *  Copyright (C) 1996,1997,1998 Russell King.\n *  ARM700 fix by Matthew Godbolt (linux-user@willothewisp.demon.co.uk)\n *  nommu support by Hyok S. Choi (hyok.choi@samsung.com)\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n *  Low-level vector interface routines\n *\n *  Note:  there is a StrongARM bug in the STMIA rn, {regs}^ instruction\n *  that causes it to save wrong values...  Be aware!\n */\n\n#include <asm/assembler.h>\n#include <asm/memory.h>\n#include <asm/glue-df.h>\n#include <asm/glue-pf.h>\n#include <asm/vfpmacros.h>\n#ifndef CONFIG_MULTI_IRQ_HANDLER\n#include <mach/entry-macro.S>\n#endif\n#include <asm/thread_notify.h>\n#include <asm/unwind.h>\n#include <asm/unistd.h>\n#include <asm/tls.h>\n#include <asm/system_info.h>\n\n#include \"entry-header.S\"\n#include <asm/entry-macro-multi.S>\n\n/*\n * Interrupt handling.\n */\n\t.macro\tirq_handler\n#ifdef CONFIG_MULTI_IRQ_HANDLER\n\tldr\tr1, =handle_arch_irq\n\tmov\tr0, sp\n\tadr\tlr, BSYM(9997f)\n\tldr\tpc, [r1]\n#else\n\tarch_irq_handler_default\n#endif\n9997:\n\t.endm\n\n\t.macro\tpabt_helper\n\t@ PABORT handler takes pt_regs in r2, fault address in r4 and psr in r5\n#ifdef MULTI_PABORT\n\tldr\tip, .LCprocfns\n\tmov\tlr, pc\n\tldr\tpc, [ip, #PROCESSOR_PABT_FUNC]\n#else\n\tbl\tCPU_PABORT_HANDLER\n#endif\n\t.endm\n\n\t.macro\tdabt_helper\n\n\t@\n\t@ Call the processor-specific abort handler:\n\t@\n\t@  r2 - pt_regs\n\t@  r4 - aborted context pc\n\t@  r5 - aborted context psr\n\t@\n\t@ The abort handler must return the aborted address in r0, and\n\t@ the fault status register in r1.  r9 must be preserved.\n\t@\n#ifdef MULTI_DABORT\n\tldr\tip, .LCprocfns\n\tmov\tlr, pc\n\tldr\tpc, [ip, #PROCESSOR_DABT_FUNC]\n#else\n\tbl\tCPU_DABORT_HANDLER\n#endif\n\t.endm\n\n#ifdef CONFIG_KPROBES\n\t.section\t.kprobes.text,\"ax\",%progbits\n#else\n\t.text\n#endif\n\n/*\n * Invalid mode handlers\n */\n\t.macro\tinv_entry, reason\n\tsub\tsp, sp, #S_FRAME_SIZE\n ARM(\tstmib\tsp, {r1 - lr}\t\t)\n THUMB(\tstmia\tsp, {r0 - r12}\t\t)\n THUMB(\tstr\tsp, [sp, #S_SP]\t\t)\n THUMB(\tstr\tlr, [sp, #S_LR]\t\t)\n\tmov\tr1, #\\reason\n\t.endm\n\n__pabt_invalid:\n\tinv_entry BAD_PREFETCH\n\tb\tcommon_invalid\nENDPROC(__pabt_invalid)\n\n__dabt_invalid:\n\tinv_entry BAD_DATA\n\tb\tcommon_invalid\nENDPROC(__dabt_invalid)\n\n__irq_invalid:\n\tinv_entry BAD_IRQ\n\tb\tcommon_invalid\nENDPROC(__irq_invalid)\n\n__und_invalid:\n\tinv_entry BAD_UNDEFINSTR\n\n\t@\n\t@ XXX fall through to common_invalid\n\t@\n\n@\n@ common_invalid - generic code for failed exception (re-entrant version of handlers)\n@\ncommon_invalid:\n\tzero_fp\n\n\tldmia\tr0, {r4 - r6}\n\tadd\tr0, sp, #S_PC\t\t@ here for interlock avoidance\n\tmov\tr7, #-1\t\t\t@  \"\"   \"\"    \"\"        \"\"\n\tstr\tr4, [sp]\t\t@ save preserved r0\n\tstmia\tr0, {r5 - r7}\t\t@ lr_<exception>,\n\t\t\t\t\t@ cpsr_<exception>, \"old_r0\"\n\n\tmov\tr0, sp\n\tb\tbad_mode\nENDPROC(__und_invalid)\n\n/*\n * SVC mode handlers\n */\n\n#if defined(CONFIG_AEABI) && (__LINUX_ARM_ARCH__ >= 5)\n#define SPFIX(code...) code\n#else\n#define SPFIX(code...)\n#endif\n\n\t.macro\tsvc_entry, stack_hole=0\n UNWIND(.fnstart\t\t)\n UNWIND(.save {r0 - pc}\t\t)\n\tsub\tsp, sp, #(S_FRAME_SIZE + \\stack_hole - 4)\n#ifdef CONFIG_THUMB2_KERNEL\n SPFIX(\tstr\tr0, [sp]\t)\t@ temporarily saved\n SPFIX(\tmov\tr0, sp\t\t)\n SPFIX(\ttst\tr0, #4\t\t)\t@ test original stack alignment\n SPFIX(\tldr\tr0, [sp]\t)\t@ restored\n#else\n SPFIX(\ttst\tsp, #4\t\t)\n#endif\n SPFIX(\tsubeq\tsp, sp, #4\t)\n\tstmia\tsp, {r1 - r12}\n\n\tldmia\tr0, {r3 - r5}\n\tadd\tr7, sp, #S_SP - 4\t@ here for interlock avoidance\n\tmov\tr6, #-1\t\t\t@  \"\"  \"\"      \"\"       \"\"\n\tadd\tr2, sp, #(S_FRAME_SIZE + \\stack_hole - 4)\n SPFIX(\taddeq\tr2, r2, #4\t)\n\tstr\tr3, [sp, #-4]!\t\t@ save the \"real\" r0 copied\n\t\t\t\t\t@ from the exception stack\n\n\tmov\tr3, lr\n\n\t@\n\t@ We are now ready to fill in the remaining blanks on the stack:\n\t@\n\t@  r2 - sp_svc\n\t@  r3 - lr_svc\n\t@  r4 - lr_<exception>, already fixed up for correct return/restart\n\t@  r5 - spsr_<exception>\n\t@  r6 - orig_r0 (see pt_regs definition in ptrace.h)\n\t@\n\tstmia\tr7, {r2 - r6}\n\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tbl\ttrace_hardirqs_off\n#endif\n\t.endm\n\n\t.align\t5\n__dabt_svc:\n\tsvc_entry\n\tmov\tr2, sp\n\tdabt_helper\n\tsvc_exit r5\t\t\t\t@ return from exception\n UNWIND(.fnend\t\t)\nENDPROC(__dabt_svc)\n\n\t.align\t5\n__irq_svc:\n\tsvc_entry\n\tirq_handler\n\n#ifdef CONFIG_PREEMPT\n\tget_thread_info tsk\n\tldr\tr8, [tsk, #TI_PREEMPT]\t\t@ get preempt count\n\tldr\tr0, [tsk, #TI_FLAGS]\t\t@ get flags\n\tteq\tr8, #0\t\t\t\t@ if preempt count != 0\n\tmovne\tr0, #0\t\t\t\t@ force flags to 0\n\ttst\tr0, #_TIF_NEED_RESCHED\n\tblne\tsvc_preempt\n#endif\n\n\tsvc_exit r5, irq = 1\t\t\t@ return from exception\n UNWIND(.fnend\t\t)\nENDPROC(__irq_svc)\n\n\t.ltorg\n\n#ifdef CONFIG_PREEMPT\nsvc_preempt:\n\tmov\tr8, lr\n1:\tbl\tpreempt_schedule_irq\t\t@ irq en/disable is done inside\n\tldr\tr0, [tsk, #TI_FLAGS]\t\t@ get new tasks TI_FLAGS\n\ttst\tr0, #_TIF_NEED_RESCHED\n\tmoveq\tpc, r8\t\t\t\t@ go again\n\tb\t1b\n#endif\n\n__und_fault:\n\t@ Correct the PC such that it is pointing at the instruction\n\t@ which caused the fault.  If the faulting instruction was ARM\n\t@ the PC will be pointing at the next instruction, and have to\n\t@ subtract 4.  Otherwise, it is Thumb, and the PC will be\n\t@ pointing at the second half of the Thumb instruction.  We\n\t@ have to subtract 2.\n\tldr\tr2, [r0, #S_PC]\n\tsub\tr2, r2, r1\n\tstr\tr2, [r0, #S_PC]\n\tb\tdo_undefinstr\nENDPROC(__und_fault)\n\n\t.align\t5\n__und_svc:\n#ifdef CONFIG_KPROBES\n\t@ If a kprobe is about to simulate a \"stmdb sp...\" instruction,\n\t@ it obviously needs free stack space which then will belong to\n\t@ the saved context.\n\tsvc_entry 64\n#else\n\tsvc_entry\n#endif\n\t@\n\t@ call emulation code, which returns using r9 if it has emulated\n\t@ the instruction, or the more conventional lr if we are to treat\n\t@ this as a real undefined instruction\n\t@\n\t@  r0 - instruction\n\t@\n#ifndef CONFIG_THUMB2_KERNEL\n\tldr\tr0, [r4, #-4]\n#else\n\tmov\tr1, #2\n\tldrh\tr0, [r4, #-2]\t\t\t@ Thumb instruction at LR - 2\n\tcmp\tr0, #0xe800\t\t\t@ 32-bit instruction if xx >= 0\n\tblo\t__und_svc_fault\n\tldrh\tr9, [r4]\t\t\t@ bottom 16 bits\n\tadd\tr4, r4, #2\n\tstr\tr4, [sp, #S_PC]\n\torr\tr0, r9, r0, lsl #16\n#endif\n\tadr\tr9, BSYM(__und_svc_finish)\n\tmov\tr2, r4\n\tbl\tcall_fpe\n\n\tmov\tr1, #4\t\t\t\t@ PC correction to apply\n__und_svc_fault:\n\tmov\tr0, sp\t\t\t\t@ struct pt_regs *regs\n\tbl\t__und_fault\n\n__und_svc_finish:\n\tldr\tr5, [sp, #S_PSR]\t\t@ Get SVC cpsr\n\tsvc_exit r5\t\t\t\t@ return from exception\n UNWIND(.fnend\t\t)\nENDPROC(__und_svc)\n\n\t.align\t5\n__pabt_svc:\n\tsvc_entry\n\tmov\tr2, sp\t\t\t\t@ regs\n\tpabt_helper\n\tsvc_exit r5\t\t\t\t@ return from exception\n UNWIND(.fnend\t\t)\nENDPROC(__pabt_svc)\n\n\t.align\t5\n.LCcralign:\n\t.word\tcr_alignment\n#ifdef MULTI_DABORT\n.LCprocfns:\n\t.word\tprocessor\n#endif\n.LCfp:\n\t.word\tfp_enter\n\n/*\n * User mode handlers\n *\n * EABI note: sp_svc is always 64-bit aligned here, so should S_FRAME_SIZE\n */\n\n#if defined(CONFIG_AEABI) && (__LINUX_ARM_ARCH__ >= 5) && (S_FRAME_SIZE & 7)\n#error \"sizeof(struct pt_regs) must be a multiple of 8\"\n#endif\n\n\t.macro\tusr_entry\n UNWIND(.fnstart\t)\n UNWIND(.cantunwind\t)\t@ don't unwind the user space\n\tsub\tsp, sp, #S_FRAME_SIZE\n ARM(\tstmib\tsp, {r1 - r12}\t)\n THUMB(\tstmia\tsp, {r0 - r12}\t)\n\n\tldmia\tr0, {r3 - r5}\n\tadd\tr0, sp, #S_PC\t\t@ here for interlock avoidance\n\tmov\tr6, #-1\t\t\t@  \"\"  \"\"     \"\"        \"\"\n\n\tstr\tr3, [sp]\t\t@ save the \"real\" r0 copied\n\t\t\t\t\t@ from the exception stack\n\n\t@\n\t@ We are now ready to fill in the remaining blanks on the stack:\n\t@\n\t@  r4 - lr_<exception>, already fixed up for correct return/restart\n\t@  r5 - spsr_<exception>\n\t@  r6 - orig_r0 (see pt_regs definition in ptrace.h)\n\t@\n\t@ Also, separately save sp_usr and lr_usr\n\t@\n\tstmia\tr0, {r4 - r6}\n ARM(\tstmdb\tr0, {sp, lr}^\t\t\t)\n THUMB(\tstore_user_sp_lr r0, r1, S_SP - S_PC\t)\n\n\t@\n\t@ Enable the alignment trap while in kernel mode\n\t@\n\talignment_trap r0\n\n\t@\n\t@ Clear FP to mark the first stack frame\n\t@\n\tzero_fp\n\n#ifdef CONFIG_IRQSOFF_TRACER\n\tbl\ttrace_hardirqs_off\n#endif\n\tct_user_exit save = 0\n\t.endm\n\n\t.macro\tkuser_cmpxchg_check\n#if !defined(CONFIG_CPU_32v6K) && !defined(CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG)\n#ifndef CONFIG_MMU\n#warning \"NPTL on non MMU needs fixing\"\n#else\n\t@ Make sure our user space atomic helper is restarted\n\t@ if it was interrupted in a critical region.  Here we\n\t@ perform a quick test inline since it should be false\n\t@ 99.9999% of the time.  The rest is done out of line.\n\tcmp\tr4, #TASK_SIZE\n\tblhs\tkuser_cmpxchg64_fixup\n#endif\n#endif\n\t.endm\n\n\t.align\t5\n__dabt_usr:\n\tusr_entry\n\tkuser_cmpxchg_check\n\tmov\tr2, sp\n\tdabt_helper\n\tb\tret_from_exception\n UNWIND(.fnend\t\t)\nENDPROC(__dabt_usr)\n\n\t.align\t5\n__irq_usr:\n\tusr_entry\n\tkuser_cmpxchg_check\n\tirq_handler\n\tget_thread_info tsk\n\tmov\twhy, #0\n\tb\tret_to_user_from_irq\n UNWIND(.fnend\t\t)\nENDPROC(__irq_usr)\n\n\t.ltorg\n\n\t.align\t5\n__und_usr:\n\tusr_entry\n\n\tmov\tr2, r4\n\tmov\tr3, r5\n\n\t@ r2 = regs->ARM_pc, which is either 2 or 4 bytes ahead of the\n\t@      faulting instruction depending on Thumb mode.\n\t@ r3 = regs->ARM_cpsr\n\t@\n\t@ The emulation code returns using r9 if it has emulated the\n\t@ instruction, or the more conventional lr if we are to treat\n\t@ this as a real undefined instruction\n\t@\n\tadr\tr9, BSYM(ret_from_exception)\n\n\ttst\tr3, #PSR_T_BIT\t\t\t@ Thumb mode?\n\tbne\t__und_usr_thumb\n\tsub\tr4, r2, #4\t\t\t@ ARM instr at LR - 4\n1:\tldrt\tr0, [r4]\n#ifdef CONFIG_CPU_ENDIAN_BE8\n\trev\tr0, r0\t\t\t\t@ little endian instruction\n#endif\n\t@ r0 = 32-bit ARM instruction which caused the exception\n\t@ r2 = PC value for the following instruction (:= regs->ARM_pc)\n\t@ r4 = PC value for the faulting instruction\n\t@ lr = 32-bit undefined instruction function\n\tadr\tlr, BSYM(__und_usr_fault_32)\n\tb\tcall_fpe\n\n__und_usr_thumb:\n\t@ Thumb instruction\n\tsub\tr4, r2, #2\t\t\t@ First half of thumb instr at LR - 2\n#if CONFIG_ARM_THUMB && __LINUX_ARM_ARCH__ >= 6 && CONFIG_CPU_V7\n/*\n * Thumb-2 instruction handling.  Note that because pre-v6 and >= v6 platforms\n * can never be supported in a single kernel, this code is not applicable at\n * all when __LINUX_ARM_ARCH__ < 6.  This allows simplifying assumptions to be\n * made about .arch directives.\n */\n#if __LINUX_ARM_ARCH__ < 7\n/* If the target CPU may not be Thumb-2-capable, a run-time check is needed: */\n#define NEED_CPU_ARCHITECTURE\n\tldr\tr5, .LCcpu_architecture\n\tldr\tr5, [r5]\n\tcmp\tr5, #CPU_ARCH_ARMv7\n\tblo\t__und_usr_fault_16\t\t@ 16bit undefined instruction\n/*\n * The following code won't get run unless the running CPU really is v7, so\n * coding round the lack of ldrht on older arches is pointless.  Temporarily\n * override the assembler target arch with the minimum required instead:\n */\n\t.arch\tarmv6t2\n#endif\n2:\tldrht\tr5, [r4]\n\tcmp\tr5, #0xe800\t\t\t@ 32bit instruction if xx != 0\n\tblo\t__und_usr_fault_16\t\t@ 16bit undefined instruction\n3:\tldrht\tr0, [r2]\n\tadd\tr2, r2, #2\t\t\t@ r2 is PC + 2, make it PC + 4\n\tstr\tr2, [sp, #S_PC]\t\t\t@ it's a 2x16bit instr, update\n\torr\tr0, r0, r5, lsl #16\n\tadr\tlr, BSYM(__und_usr_fault_32)\n\t@ r0 = the two 16-bit Thumb instructions which caused the exception\n\t@ r2 = PC value for the following Thumb instruction (:= regs->ARM_pc)\n\t@ r4 = PC value for the first 16-bit Thumb instruction\n\t@ lr = 32bit undefined instruction function\n\n#if __LINUX_ARM_ARCH__ < 7\n/* If the target arch was overridden, change it back: */\n#ifdef CONFIG_CPU_32v6K\n\t.arch\tarmv6k\n#else\n\t.arch\tarmv6\n#endif\n#endif /* __LINUX_ARM_ARCH__ < 7 */\n#else /* !(CONFIG_ARM_THUMB && __LINUX_ARM_ARCH__ >= 6 && CONFIG_CPU_V7) */\n\tb\t__und_usr_fault_16\n#endif\n UNWIND(.fnend)\nENDPROC(__und_usr)\n\n/*\n * The out of line fixup for the ldrt instructions above.\n */\n\t.pushsection .fixup, \"ax\"\n\t.align\t2\n4:\tmov\tpc, r9\n\t.popsection\n\t.pushsection __ex_table,\"a\"\n\t.long\t1b, 4b\n#if CONFIG_ARM_THUMB && __LINUX_ARM_ARCH__ >= 6 && CONFIG_CPU_V7\n\t.long\t2b, 4b\n\t.long\t3b, 4b\n#endif\n\t.popsection\n\n/*\n * Check whether the instruction is a co-processor instruction.\n * If yes, we need to call the relevant co-processor handler.\n *\n * Note that we don't do a full check here for the co-processor\n * instructions; all instructions with bit 27 set are well\n * defined.  The only instructions that should fault are the\n * co-processor instructions.  However, we have to watch out\n * for the ARM6/ARM7 SWI bug.\n *\n * NEON is a special case that has to be handled here. Not all\n * NEON instructions are co-processor instructions, so we have\n * to make a special case of checking for them. Plus, there's\n * five groups of them, so we have a table of mask/opcode pairs\n * to check against, and if any match then we branch off into the\n * NEON handler code.\n *\n * Emulators may wish to make use of the following registers:\n *  r0  = instruction opcode (32-bit ARM or two 16-bit Thumb)\n *  r2  = PC value to resume execution after successful emulation\n *  r9  = normal \"successful\" return address\n *  r10 = this threads thread_info structure\n *  lr  = unrecognised instruction return address\n * IRQs disabled, FIQs enabled.\n */\n\t@\n\t@ Fall-through from Thumb-2 __und_usr\n\t@\n#ifdef CONFIG_NEON\n\tget_thread_info r10\t\t\t@ get current thread\n\tadr\tr6, .LCneon_thumb_opcodes\n\tb\t2f\n#endif\ncall_fpe:\n\tget_thread_info r10\t\t\t@ get current thread\n#ifdef CONFIG_NEON\n\tadr\tr6, .LCneon_arm_opcodes\n2:\tldr\tr5, [r6], #4\t\t\t@ mask value\n\tldr\tr7, [r6], #4\t\t\t@ opcode bits matching in mask\n\tcmp\tr5, #0\t\t\t\t@ end mask?\n\tbeq\t1f\n\tand\tr8, r0, r5\n\tcmp\tr8, r7\t\t\t\t@ NEON instruction?\n\tbne\t2b\n\tmov\tr7, #1\n\tstrb\tr7, [r10, #TI_USED_CP + 10]\t@ mark CP#10 as used\n\tstrb\tr7, [r10, #TI_USED_CP + 11]\t@ mark CP#11 as used\n\tb\tdo_vfp\t\t\t\t@ let VFP handler handle this\n1:\n#endif\n\ttst\tr0, #0x08000000\t\t\t@ only CDP/CPRT/LDC/STC have bit 27\n\ttstne\tr0, #0x04000000\t\t\t@ bit 26 set on both ARM and Thumb-2\n\tmoveq\tpc, lr\n\tand\tr8, r0, #0x00000f00\t\t@ mask out CP number\n THUMB(\tlsr\tr8, r8, #8\t\t)\n\tmov\tr7, #1\n\tadd\tr6, r10, #TI_USED_CP\n ARM(\tstrb\tr7, [r6, r8, lsr #8]\t)\t@ set appropriate used_cp[]\n THUMB(\tstrb\tr7, [r6, r8]\t\t)\t@ set appropriate used_cp[]\n#ifdef CONFIG_IWMMXT\n\t@ Test if we need to give access to iWMMXt coprocessors\n\tldr\tr5, [r10, #TI_FLAGS]\n\trsbs\tr7, r8, #(1 << 8)\t\t@ CP 0 or 1 only\n\tmovcss\tr7, r5, lsr #(TIF_USING_IWMMXT + 1)\n\tbcs\tiwmmxt_task_enable\n#endif\n ARM(\tadd\tpc, pc, r8, lsr #6\t)\n THUMB(\tlsl\tr8, r8, #2\t\t)\n THUMB(\tadd\tpc, r8\t\t\t)\n\tnop\n\n\tmovw_pc\tlr\t\t\t\t@ CP#0\n\tW(b)\tdo_fpe\t\t\t\t@ CP#1 (FPE)\n\tW(b)\tdo_fpe\t\t\t\t@ CP#2 (FPE)\n\tmovw_pc\tlr\t\t\t\t@ CP#3\n#ifdef CONFIG_CRUNCH\n\tb\tcrunch_task_enable\t\t@ CP#4 (MaverickCrunch)\n\tb\tcrunch_task_enable\t\t@ CP#5 (MaverickCrunch)\n\tb\tcrunch_task_enable\t\t@ CP#6 (MaverickCrunch)\n#else\n\tmovw_pc\tlr\t\t\t\t@ CP#4\n\tmovw_pc\tlr\t\t\t\t@ CP#5\n\tmovw_pc\tlr\t\t\t\t@ CP#6\n#endif\n\tmovw_pc\tlr\t\t\t\t@ CP#7\n\tmovw_pc\tlr\t\t\t\t@ CP#8\n\tmovw_pc\tlr\t\t\t\t@ CP#9\n#ifdef CONFIG_VFP\n\tW(b)\tdo_vfp\t\t\t\t@ CP#10 (VFP)\n\tW(b)\tdo_vfp\t\t\t\t@ CP#11 (VFP)\n#else\n\tmovw_pc\tlr\t\t\t\t@ CP#10 (VFP)\n\tmovw_pc\tlr\t\t\t\t@ CP#11 (VFP)\n#endif\n\tmovw_pc\tlr\t\t\t\t@ CP#12\n\tmovw_pc\tlr\t\t\t\t@ CP#13\n\tmovw_pc\tlr\t\t\t\t@ CP#14 (Debug)\n\tmovw_pc\tlr\t\t\t\t@ CP#15 (Control)\n\n#ifdef NEED_CPU_ARCHITECTURE\n\t.align\t2\n.LCcpu_architecture:\n\t.word\t__cpu_architecture\n#endif\n\n#ifdef CONFIG_NEON\n\t.align\t6\n\n.LCneon_arm_opcodes:\n\t.word\t0xfe000000\t\t\t@ mask\n\t.word\t0xf2000000\t\t\t@ opcode\n\n\t.word\t0xff100000\t\t\t@ mask\n\t.word\t0xf4000000\t\t\t@ opcode\n\n\t.word\t0x00000000\t\t\t@ mask\n\t.word\t0x00000000\t\t\t@ opcode\n\n.LCneon_thumb_opcodes:\n\t.word\t0xef000000\t\t\t@ mask\n\t.word\t0xef000000\t\t\t@ opcode\n\n\t.word\t0xff100000\t\t\t@ mask\n\t.word\t0xf9000000\t\t\t@ opcode\n\n\t.word\t0x00000000\t\t\t@ mask\n\t.word\t0x00000000\t\t\t@ opcode\n#endif\n\ndo_fpe:\n\tenable_irq\n\tldr\tr4, .LCfp\n\tadd\tr10, r10, #TI_FPSTATE\t\t@ r10 = workspace\n\tldr\tpc, [r4]\t\t\t@ Call FP module USR entry point\n\n/*\n * The FP module is called with these registers set:\n *  r0  = instruction\n *  r2  = PC+4\n *  r9  = normal \"successful\" return address\n *  r10 = FP workspace\n *  lr  = unrecognised FP instruction return address\n */\n\n\t.pushsection .data\nENTRY(fp_enter)\n\t.word\tno_fp\n\t.popsection\n\nENTRY(no_fp)\n\tmov\tpc, lr\nENDPROC(no_fp)\n\n__und_usr_fault_32:\n\tmov\tr1, #4\n\tb\t1f\n__und_usr_fault_16:\n\tmov\tr1, #2\n1:\tenable_irq\n\tmov\tr0, sp\n\tadr\tlr, BSYM(ret_from_exception)\n\tb\t__und_fault\nENDPROC(__und_usr_fault_32)\nENDPROC(__und_usr_fault_16)\n\n\t.align\t5\n__pabt_usr:\n\tusr_entry\n\tmov\tr2, sp\t\t\t\t@ regs\n\tpabt_helper\n UNWIND(.fnend\t\t)\n\t/* fall through */\n/*\n * This is the return code to user mode for abort handlers\n */\nENTRY(ret_from_exception)\n UNWIND(.fnstart\t)\n UNWIND(.cantunwind\t)\n\tget_thread_info tsk\n\tmov\twhy, #0\n\tb\tret_to_user\n UNWIND(.fnend\t\t)\nENDPROC(__pabt_usr)\nENDPROC(ret_from_exception)\n\n/*\n * Register switch for ARMv3 and ARMv4 processors\n * r0 = previous task_struct, r1 = previous thread_info, r2 = next thread_info\n * previous and next are guaranteed not to be the same.\n */\nENTRY(__switch_to)\n UNWIND(.fnstart\t)\n UNWIND(.cantunwind\t)\n\tadd\tip, r1, #TI_CPU_SAVE\n ARM(\tstmia\tip!, {r4 - sl, fp, sp, lr} )\t@ Store most regs on stack\n THUMB(\tstmia\tip!, {r4 - sl, fp}\t   )\t@ Store most regs on stack\n THUMB(\tstr\tsp, [ip], #4\t\t   )\n THUMB(\tstr\tlr, [ip], #4\t\t   )\n\tldr\tr4, [r2, #TI_TP_VALUE]\n\tldr\tr5, [r2, #TI_TP_VALUE + 4]\n#ifdef CONFIG_CPU_USE_DOMAINS\n\tldr\tr6, [r2, #TI_CPU_DOMAIN]\n#endif\n\tswitch_tls r1, r4, r5, r3, r7\n#if defined(CONFIG_CC_STACKPROTECTOR) && !defined(CONFIG_SMP)\n\tldr\tr7, [r2, #TI_TASK]\n\tldr\tr8, =__stack_chk_guard\n\tldr\tr7, [r7, #TSK_STACK_CANARY]\n#endif\n#ifdef CONFIG_CPU_USE_DOMAINS\n\tmcr\tp15, 0, r6, c3, c0, 0\t\t@ Set domain register\n#endif\n\tmov\tr5, r0\n\tadd\tr4, r2, #TI_CPU_SAVE\n\tldr\tr0, =thread_notify_head\n\tmov\tr1, #THREAD_NOTIFY_SWITCH\n\tbl\tatomic_notifier_call_chain\n#if defined(CONFIG_CC_STACKPROTECTOR) && !defined(CONFIG_SMP)\n\tstr\tr7, [r8]\n#endif\n THUMB(\tmov\tip, r4\t\t\t   )\n\tmov\tr0, r5\n ARM(\tldmia\tr4, {r4 - sl, fp, sp, pc}  )\t@ Load all regs saved previously\n THUMB(\tldmia\tip!, {r4 - sl, fp}\t   )\t@ Load all regs saved previously\n THUMB(\tldr\tsp, [ip], #4\t\t   )\n THUMB(\tldr\tpc, [ip]\t\t   )\n UNWIND(.fnend\t\t)\nENDPROC(__switch_to)\n\n\t__INIT\n\n/*\n * User helpers.\n *\n * Each segment is 32-byte aligned and will be moved to the top of the high\n * vector page.  New segments (if ever needed) must be added in front of\n * existing ones.  This mechanism should be used only for things that are\n * really small and justified, and not be abused freely.\n *\n * See Documentation/arm/kernel_user_helpers.txt for formal definitions.\n */\n THUMB(\t.arm\t)\n\n\t.macro\tusr_ret, reg\n#ifdef CONFIG_ARM_THUMB\n\tbx\t\\reg\n#else\n\tmov\tpc, \\reg\n#endif\n\t.endm\n\n\t.align\t5\n\t.globl\t__kuser_helper_start\n__kuser_helper_start:\n\n/*\n * Due to the length of some sequences, __kuser_cmpxchg64 spans 2 regular\n * kuser \"slots\", therefore 0xffff0f80 is not used as a valid entry point.\n */\n\n__kuser_cmpxchg64:\t\t\t\t@ 0xffff0f60\n\n#if defined(CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG)\n\n\t/*\n\t * Poor you.  No fast solution possible...\n\t * The kernel itself must perform the operation.\n\t * A special ghost syscall is used for that (see traps.c).\n\t */\n\tstmfd\tsp!, {r7, lr}\n\tldr\tr7, 1f\t\t\t@ it's 20 bits\n\tswi\t__ARM_NR_cmpxchg64\n\tldmfd\tsp!, {r7, pc}\n1:\t.word\t__ARM_NR_cmpxchg64\n\n#elif defined(CONFIG_CPU_32v6K)\n\n\tstmfd\tsp!, {r4, r5, r6, r7}\n\tldrd\tr4, r5, [r0]\t\t\t@ load old val\n\tldrd\tr6, r7, [r1]\t\t\t@ load new val\n\tsmp_dmb\tarm\n1:\tldrexd\tr0, r1, [r2]\t\t\t@ load current val\n\teors\tr3, r0, r4\t\t\t@ compare with oldval (1)\n\teoreqs\tr3, r1, r5\t\t\t@ compare with oldval (2)\n\tstrexdeq r3, r6, r7, [r2]\t\t@ store newval if eq\n\tteqeq\tr3, #1\t\t\t\t@ success?\n\tbeq\t1b\t\t\t\t@ if no then retry\n\tsmp_dmb\tarm\n\trsbs\tr0, r3, #0\t\t\t@ set returned val and C flag\n\tldmfd\tsp!, {r4, r5, r6, r7}\n\tusr_ret\tlr\n\n#elif !defined(CONFIG_SMP)\n\n#ifdef CONFIG_MMU\n\n\t/*\n\t * The only thing that can break atomicity in this cmpxchg64\n\t * implementation is either an IRQ or a data abort exception\n\t * causing another process/thread to be scheduled in the middle of\n\t * the critical sequence.  The same strategy as for cmpxchg is used.\n\t */\n\tstmfd\tsp!, {r4, r5, r6, lr}\n\tldmia\tr0, {r4, r5}\t\t\t@ load old val\n\tldmia\tr1, {r6, lr}\t\t\t@ load new val\n1:\tldmia\tr2, {r0, r1}\t\t\t@ load current val\n\teors\tr3, r0, r4\t\t\t@ compare with oldval (1)\n\teoreqs\tr3, r1, r5\t\t\t@ compare with oldval (2)\n2:\tstmeqia\tr2, {r6, lr}\t\t\t@ store newval if eq\n\trsbs\tr0, r3, #0\t\t\t@ set return val and C flag\n\tldmfd\tsp!, {r4, r5, r6, pc}\n\n\t.text\nkuser_cmpxchg64_fixup:\n\t@ Called from kuser_cmpxchg_fixup.\n\t@ r4 = address of interrupted insn (must be preserved).\n\t@ sp = saved regs. r7 and r8 are clobbered.\n\t@ 1b = first critical insn, 2b = last critical insn.\n\t@ If r4 >= 1b and r4 <= 2b then saved pc_usr is set to 1b.\n\tmov\tr7, #0xffff0fff\n\tsub\tr7, r7, #(0xffff0fff - (0xffff0f60 + (1b - __kuser_cmpxchg64)))\n\tsubs\tr8, r4, r7\n\trsbcss\tr8, r8, #(2b - 1b)\n\tstrcs\tr7, [sp, #S_PC]\n#if __LINUX_ARM_ARCH__ < 6\n\tbcc\tkuser_cmpxchg32_fixup\n#endif\n\tmov\tpc, lr\n\t.previous\n\n#else\n#warning \"NPTL on non MMU needs fixing\"\n\tmov\tr0, #-1\n\tadds\tr0, r0, #0\n\tusr_ret\tlr\n#endif\n\n#else\n#error \"incoherent kernel configuration\"\n#endif\n\n\t/* pad to next slot */\n\t.rept\t(16 - (. - __kuser_cmpxchg64)/4)\n\t.word\t0\n\t.endr\n\n\t.align\t5\n\n__kuser_memory_barrier:\t\t\t\t@ 0xffff0fa0\n\tsmp_dmb\tarm\n\tusr_ret\tlr\n\n\t.align\t5\n\n__kuser_cmpxchg:\t\t\t\t@ 0xffff0fc0\n\n#if defined(CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG)\n\n\t/*\n\t * Poor you.  No fast solution possible...\n\t * The kernel itself must perform the operation.\n\t * A special ghost syscall is used for that (see traps.c).\n\t */\n\tstmfd\tsp!, {r7, lr}\n\tldr\tr7, 1f\t\t\t@ it's 20 bits\n\tswi\t__ARM_NR_cmpxchg\n\tldmfd\tsp!, {r7, pc}\n1:\t.word\t__ARM_NR_cmpxchg\n\n#elif __LINUX_ARM_ARCH__ < 6\n\n#ifdef CONFIG_MMU\n\n\t/*\n\t * The only thing that can break atomicity in this cmpxchg\n\t * implementation is either an IRQ or a data abort exception\n\t * causing another process/thread to be scheduled in the middle\n\t * of the critical sequence.  To prevent this, code is added to\n\t * the IRQ and data abort exception handlers to set the pc back\n\t * to the beginning of the critical section if it is found to be\n\t * within that critical section (see kuser_cmpxchg_fixup).\n\t */\n1:\tldr\tr3, [r2]\t\t\t@ load current val\n\tsubs\tr3, r3, r0\t\t\t@ compare with oldval\n2:\tstreq\tr1, [r2]\t\t\t@ store newval if eq\n\trsbs\tr0, r3, #0\t\t\t@ set return val and C flag\n\tusr_ret\tlr\n\n\t.text\nkuser_cmpxchg32_fixup:\n\t@ Called from kuser_cmpxchg_check macro.\n\t@ r4 = address of interrupted insn (must be preserved).\n\t@ sp = saved regs. r7 and r8 are clobbered.\n\t@ 1b = first critical insn, 2b = last critical insn.\n\t@ If r4 >= 1b and r4 <= 2b then saved pc_usr is set to 1b.\n\tmov\tr7, #0xffff0fff\n\tsub\tr7, r7, #(0xffff0fff - (0xffff0fc0 + (1b - __kuser_cmpxchg)))\n\tsubs\tr8, r4, r7\n\trsbcss\tr8, r8, #(2b - 1b)\n\tstrcs\tr7, [sp, #S_PC]\n\tmov\tpc, lr\n\t.previous\n\n#else\n#warning \"NPTL on non MMU needs fixing\"\n\tmov\tr0, #-1\n\tadds\tr0, r0, #0\n\tusr_ret\tlr\n#endif\n\n#else\n\n\tsmp_dmb\tarm\n1:\tldrex\tr3, [r2]\n\tsubs\tr3, r3, r0\n\tstrexeq\tr3, r1, [r2]\n\tteqeq\tr3, #1\n\tbeq\t1b\n\trsbs\tr0, r3, #0\n\t/* beware -- each __kuser slot must be 8 instructions max */\n\tALT_SMP(b\t__kuser_memory_barrier)\n\tALT_UP(usr_ret\tlr)\n\n#endif\n\n\t.align\t5\n\n__kuser_get_tls:\t\t\t\t@ 0xffff0fe0\n\tldr\tr0, [pc, #(16 - 8)]\t@ read TLS, set in kuser_get_tls_init\n\tusr_ret\tlr\n\tmrc\tp15, 0, r0, c13, c0, 3\t@ 0xffff0fe8 hardware TLS code\n\t.rep\t4\n\t.word\t0\t\t\t@ 0xffff0ff0 software TLS value, then\n\t.endr\t\t\t\t@ pad up to __kuser_helper_version\n\n__kuser_helper_version:\t\t\t\t@ 0xffff0ffc\n\t.word\t((__kuser_helper_end - __kuser_helper_start) >> 5)\n\n\t.globl\t__kuser_helper_end\n__kuser_helper_end:\n\n THUMB(\t.thumb\t)\n\n/*\n * Vector stubs.\n *\n * This code is copied to 0xffff0200 so we can use branches in the\n * vectors, rather than ldr's.  Note that this code must not\n * exceed 0x300 bytes.\n *\n * Common stub entry macro:\n *   Enter in IRQ mode, spsr = SVC/USR CPSR, lr = SVC/USR PC\n *\n * SP points to a minimal amount of processor-private memory, the address\n * of which is copied into r0 for the mode specific abort handler.\n */\n\t.macro\tvector_stub, name, mode, correction=0\n\t.align\t5\n\nvector_\\name:\n\t.if \\correction\n\tsub\tlr, lr, #\\correction\n\t.endif\n\n\t@\n\t@ Save r0, lr_<exception> (parent PC) and spsr_<exception>\n\t@ (parent CPSR)\n\t@\n\tstmia\tsp, {r0, lr}\t\t@ save r0, lr\n\tmrs\tlr, spsr\n\tstr\tlr, [sp, #8]\t\t@ save spsr\n\n\t@\n\t@ Prepare for SVC32 mode.  IRQs remain disabled.\n\t@\n\tmrs\tr0, cpsr\n\teor\tr0, r0, #(\\mode ^ SVC_MODE | PSR_ISETSTATE)\n\tmsr\tspsr_cxsf, r0\n\n\t@\n\t@ the branch table must immediately follow this code\n\t@\n\tand\tlr, lr, #0x0f\n THUMB(\tadr\tr0, 1f\t\t\t)\n THUMB(\tldr\tlr, [r0, lr, lsl #2]\t)\n\tmov\tr0, sp\n ARM(\tldr\tlr, [pc, lr, lsl #2]\t)\n\tmovs\tpc, lr\t\t\t@ branch to handler in SVC mode\nENDPROC(vector_\\name)\n\n\t.align\t2\n\t@ handler addresses follow this label\n1:\n\t.endm\n\n\t.globl\t__stubs_start\n__stubs_start:\n/*\n * Interrupt dispatcher\n */\n\tvector_stub\tirq, IRQ_MODE, 4\n\n\t.long\t__irq_usr\t\t\t@  0  (USR_26 / USR_32)\n\t.long\t__irq_invalid\t\t\t@  1  (FIQ_26 / FIQ_32)\n\t.long\t__irq_invalid\t\t\t@  2  (IRQ_26 / IRQ_32)\n\t.long\t__irq_svc\t\t\t@  3  (SVC_26 / SVC_32)\n\t.long\t__irq_invalid\t\t\t@  4\n\t.long\t__irq_invalid\t\t\t@  5\n\t.long\t__irq_invalid\t\t\t@  6\n\t.long\t__irq_invalid\t\t\t@  7\n\t.long\t__irq_invalid\t\t\t@  8\n\t.long\t__irq_invalid\t\t\t@  9\n\t.long\t__irq_invalid\t\t\t@  a\n\t.long\t__irq_invalid\t\t\t@  b\n\t.long\t__irq_invalid\t\t\t@  c\n\t.long\t__irq_invalid\t\t\t@  d\n\t.long\t__irq_invalid\t\t\t@  e\n\t.long\t__irq_invalid\t\t\t@  f\n\n/*\n * Data abort dispatcher\n * Enter in ABT mode, spsr = USR CPSR, lr = USR PC\n */\n\tvector_stub\tdabt, ABT_MODE, 8\n\n\t.long\t__dabt_usr\t\t\t@  0  (USR_26 / USR_32)\n\t.long\t__dabt_invalid\t\t\t@  1  (FIQ_26 / FIQ_32)\n\t.long\t__dabt_invalid\t\t\t@  2  (IRQ_26 / IRQ_32)\n\t.long\t__dabt_svc\t\t\t@  3  (SVC_26 / SVC_32)\n\t.long\t__dabt_invalid\t\t\t@  4\n\t.long\t__dabt_invalid\t\t\t@  5\n\t.long\t__dabt_invalid\t\t\t@  6\n\t.long\t__dabt_invalid\t\t\t@  7\n\t.long\t__dabt_invalid\t\t\t@  8\n\t.long\t__dabt_invalid\t\t\t@  9\n\t.long\t__dabt_invalid\t\t\t@  a\n\t.long\t__dabt_invalid\t\t\t@  b\n\t.long\t__dabt_invalid\t\t\t@  c\n\t.long\t__dabt_invalid\t\t\t@  d\n\t.long\t__dabt_invalid\t\t\t@  e\n\t.long\t__dabt_invalid\t\t\t@  f\n\n/*\n * Prefetch abort dispatcher\n * Enter in ABT mode, spsr = USR CPSR, lr = USR PC\n */\n\tvector_stub\tpabt, ABT_MODE, 4\n\n\t.long\t__pabt_usr\t\t\t@  0 (USR_26 / USR_32)\n\t.long\t__pabt_invalid\t\t\t@  1 (FIQ_26 / FIQ_32)\n\t.long\t__pabt_invalid\t\t\t@  2 (IRQ_26 / IRQ_32)\n\t.long\t__pabt_svc\t\t\t@  3 (SVC_26 / SVC_32)\n\t.long\t__pabt_invalid\t\t\t@  4\n\t.long\t__pabt_invalid\t\t\t@  5\n\t.long\t__pabt_invalid\t\t\t@  6\n\t.long\t__pabt_invalid\t\t\t@  7\n\t.long\t__pabt_invalid\t\t\t@  8\n\t.long\t__pabt_invalid\t\t\t@  9\n\t.long\t__pabt_invalid\t\t\t@  a\n\t.long\t__pabt_invalid\t\t\t@  b\n\t.long\t__pabt_invalid\t\t\t@  c\n\t.long\t__pabt_invalid\t\t\t@  d\n\t.long\t__pabt_invalid\t\t\t@  e\n\t.long\t__pabt_invalid\t\t\t@  f\n\n/*\n * Undef instr entry dispatcher\n * Enter in UND mode, spsr = SVC/USR CPSR, lr = SVC/USR PC\n */\n\tvector_stub\tund, UND_MODE\n\n\t.long\t__und_usr\t\t\t@  0 (USR_26 / USR_32)\n\t.long\t__und_invalid\t\t\t@  1 (FIQ_26 / FIQ_32)\n\t.long\t__und_invalid\t\t\t@  2 (IRQ_26 / IRQ_32)\n\t.long\t__und_svc\t\t\t@  3 (SVC_26 / SVC_32)\n\t.long\t__und_invalid\t\t\t@  4\n\t.long\t__und_invalid\t\t\t@  5\n\t.long\t__und_invalid\t\t\t@  6\n\t.long\t__und_invalid\t\t\t@  7\n\t.long\t__und_invalid\t\t\t@  8\n\t.long\t__und_invalid\t\t\t@  9\n\t.long\t__und_invalid\t\t\t@  a\n\t.long\t__und_invalid\t\t\t@  b\n\t.long\t__und_invalid\t\t\t@  c\n\t.long\t__und_invalid\t\t\t@  d\n\t.long\t__und_invalid\t\t\t@  e\n\t.long\t__und_invalid\t\t\t@  f\n\n\t.align\t5\n\n/*=============================================================================\n * Undefined FIQs\n *-----------------------------------------------------------------------------\n * Enter in FIQ mode, spsr = ANY CPSR, lr = ANY PC\n * MUST PRESERVE SVC SPSR, but need to switch to SVC mode to show our msg.\n * Basically to switch modes, we *HAVE* to clobber one register...  brain\n * damage alert!  I don't think that we can execute any code in here in any\n * other mode than FIQ...  Ok you can switch to another mode, but you can't\n * get out of that mode without clobbering one register.\n */\nvector_fiq:\n\tsubs\tpc, lr, #4\n\n/*=============================================================================\n * Address exception handler\n *-----------------------------------------------------------------------------\n * These aren't too critical.\n * (they're not supposed to happen, and won't happen in 32-bit data mode).\n */\n\nvector_addrexcptn:\n\tb\tvector_addrexcptn\n\n/*\n * We group all the following data together to optimise\n * for CPUs with separate I & D caches.\n */\n\t.align\t5\n\n.LCvswi:\n\t.word\tvector_swi\n\n\t.globl\t__stubs_end\n__stubs_end:\n\n\t.equ\tstubs_offset, __vectors_start + 0x200 - __stubs_start\n\n\t.globl\t__vectors_start\n__vectors_start:\n ARM(\tswi\tSYS_ERROR0\t)\n THUMB(\tsvc\t#0\t\t)\n THUMB(\tnop\t\t\t)\n\tW(b)\tvector_und + stubs_offset\n\tW(ldr)\tpc, .LCvswi + stubs_offset\n\tW(b)\tvector_pabt + stubs_offset\n\tW(b)\tvector_dabt + stubs_offset\n\tW(b)\tvector_addrexcptn + stubs_offset\n\tW(b)\tvector_irq + stubs_offset\n\tW(b)\tvector_fiq + stubs_offset\n\n\t.globl\t__vectors_end\n__vectors_end:\n\n\t.data\n\n\t.globl\tcr_alignment\n\t.globl\tcr_no_alignment\ncr_alignment:\n\t.space\t4\ncr_no_alignment:\n\t.space\t4\n\n#ifdef CONFIG_MULTI_IRQ_HANDLER\n\t.globl\thandle_arch_irq\nhandle_arch_irq:\n\t.space\t4\n#endif\n", "/*\n *  linux/arch/arm/kernel/process.c\n *\n *  Copyright (C) 1996-2000 Russell King - Converted to ARM.\n *  Original Copyright (C) 1995  Linus Torvalds\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#include <stdarg.h>\n\n#include <linux/export.h>\n#include <linux/sched.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/stddef.h>\n#include <linux/unistd.h>\n#include <linux/user.h>\n#include <linux/delay.h>\n#include <linux/reboot.h>\n#include <linux/interrupt.h>\n#include <linux/kallsyms.h>\n#include <linux/init.h>\n#include <linux/cpu.h>\n#include <linux/elfcore.h>\n#include <linux/pm.h>\n#include <linux/tick.h>\n#include <linux/utsname.h>\n#include <linux/uaccess.h>\n#include <linux/random.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/cpuidle.h>\n#include <linux/leds.h>\n\n#include <asm/cacheflush.h>\n#include <asm/idmap.h>\n#include <asm/processor.h>\n#include <asm/thread_notify.h>\n#include <asm/stacktrace.h>\n#include <asm/mach/time.h>\n#include <asm/tls.h>\n\n#ifdef CONFIG_CC_STACKPROTECTOR\n#include <linux/stackprotector.h>\nunsigned long __stack_chk_guard __read_mostly;\nEXPORT_SYMBOL(__stack_chk_guard);\n#endif\n\nstatic const char *processor_modes[] = {\n  \"USER_26\", \"FIQ_26\" , \"IRQ_26\" , \"SVC_26\" , \"UK4_26\" , \"UK5_26\" , \"UK6_26\" , \"UK7_26\" ,\n  \"UK8_26\" , \"UK9_26\" , \"UK10_26\", \"UK11_26\", \"UK12_26\", \"UK13_26\", \"UK14_26\", \"UK15_26\",\n  \"USER_32\", \"FIQ_32\" , \"IRQ_32\" , \"SVC_32\" , \"UK4_32\" , \"UK5_32\" , \"UK6_32\" , \"ABT_32\" ,\n  \"UK8_32\" , \"UK9_32\" , \"UK10_32\", \"UND_32\" , \"UK12_32\", \"UK13_32\", \"UK14_32\", \"SYS_32\"\n};\n\nstatic const char *isa_modes[] = {\n  \"ARM\" , \"Thumb\" , \"Jazelle\", \"ThumbEE\"\n};\n\nextern void call_with_stack(void (*fn)(void *), void *arg, void *sp);\ntypedef void (*phys_reset_t)(unsigned long);\n\n/*\n * A temporary stack to use for CPU reset. This is static so that we\n * don't clobber it with the identity mapping. When running with this\n * stack, any references to the current task *will not work* so you\n * should really do as little as possible before jumping to your reset\n * code.\n */\nstatic u64 soft_restart_stack[16];\n\nstatic void __soft_restart(void *addr)\n{\n\tphys_reset_t phys_reset;\n\n\t/* Take out a flat memory mapping. */\n\tsetup_mm_for_reboot();\n\n\t/* Clean and invalidate caches */\n\tflush_cache_all();\n\n\t/* Turn off caching */\n\tcpu_proc_fin();\n\n\t/* Push out any further dirty data, and ensure cache is empty */\n\tflush_cache_all();\n\n\t/* Switch to the identity mapping. */\n\tphys_reset = (phys_reset_t)(unsigned long)virt_to_phys(cpu_reset);\n\tphys_reset((unsigned long)addr);\n\n\t/* Should never get here. */\n\tBUG();\n}\n\nvoid soft_restart(unsigned long addr)\n{\n\tu64 *stack = soft_restart_stack + ARRAY_SIZE(soft_restart_stack);\n\n\t/* Disable interrupts first */\n\tlocal_irq_disable();\n\tlocal_fiq_disable();\n\n\t/* Disable the L2 if we're the last man standing. */\n\tif (num_online_cpus() == 1)\n\t\touter_disable();\n\n\t/* Change to the new stack and continue with the reset. */\n\tcall_with_stack(__soft_restart, (void *)addr, (void *)stack);\n\n\t/* Should never get here. */\n\tBUG();\n}\n\nstatic void null_restart(char mode, const char *cmd)\n{\n}\n\n/*\n * Function pointers to optional machine specific functions\n */\nvoid (*pm_power_off)(void);\nEXPORT_SYMBOL(pm_power_off);\n\nvoid (*arm_pm_restart)(char str, const char *cmd) = null_restart;\nEXPORT_SYMBOL_GPL(arm_pm_restart);\n\n/*\n * This is our default idle handler.\n */\n\nvoid (*arm_pm_idle)(void);\n\nstatic void default_idle(void)\n{\n\tif (arm_pm_idle)\n\t\tarm_pm_idle();\n\telse\n\t\tcpu_do_idle();\n\tlocal_irq_enable();\n}\n\nvoid arch_cpu_idle_prepare(void)\n{\n\tlocal_fiq_enable();\n}\n\nvoid arch_cpu_idle_enter(void)\n{\n\tledtrig_cpu(CPU_LED_IDLE_START);\n#ifdef CONFIG_PL310_ERRATA_769419\n\twmb();\n#endif\n}\n\nvoid arch_cpu_idle_exit(void)\n{\n\tledtrig_cpu(CPU_LED_IDLE_END);\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\nvoid arch_cpu_idle_dead(void)\n{\n\tcpu_die();\n}\n#endif\n\n/*\n * Called from the core idle loop.\n */\nvoid arch_cpu_idle(void)\n{\n\tif (cpuidle_idle_call())\n\t\tdefault_idle();\n}\n\nstatic char reboot_mode = 'h';\n\nint __init reboot_setup(char *str)\n{\n\treboot_mode = str[0];\n\treturn 1;\n}\n\n__setup(\"reboot=\", reboot_setup);\n\nvoid machine_shutdown(void)\n{\n#ifdef CONFIG_SMP\n\tsmp_send_stop();\n#endif\n}\n\nvoid machine_halt(void)\n{\n\tmachine_shutdown();\n\tlocal_irq_disable();\n\twhile (1);\n}\n\nvoid machine_power_off(void)\n{\n\tmachine_shutdown();\n\tif (pm_power_off)\n\t\tpm_power_off();\n}\n\nvoid machine_restart(char *cmd)\n{\n\tmachine_shutdown();\n\n\tarm_pm_restart(reboot_mode, cmd);\n\n\t/* Give a grace period for failure to restart of 1s */\n\tmdelay(1000);\n\n\t/* Whoops - the platform was unable to reboot. Tell the user! */\n\tprintk(\"Reboot failed -- System halted\\n\");\n\tlocal_irq_disable();\n\twhile (1);\n}\n\nvoid __show_regs(struct pt_regs *regs)\n{\n\tunsigned long flags;\n\tchar buf[64];\n\n\tshow_regs_print_info(KERN_DEFAULT);\n\n\tprint_symbol(\"PC is at %s\\n\", instruction_pointer(regs));\n\tprint_symbol(\"LR is at %s\\n\", regs->ARM_lr);\n\tprintk(\"pc : [<%08lx>]    lr : [<%08lx>]    psr: %08lx\\n\"\n\t       \"sp : %08lx  ip : %08lx  fp : %08lx\\n\",\n\t\tregs->ARM_pc, regs->ARM_lr, regs->ARM_cpsr,\n\t\tregs->ARM_sp, regs->ARM_ip, regs->ARM_fp);\n\tprintk(\"r10: %08lx  r9 : %08lx  r8 : %08lx\\n\",\n\t\tregs->ARM_r10, regs->ARM_r9,\n\t\tregs->ARM_r8);\n\tprintk(\"r7 : %08lx  r6 : %08lx  r5 : %08lx  r4 : %08lx\\n\",\n\t\tregs->ARM_r7, regs->ARM_r6,\n\t\tregs->ARM_r5, regs->ARM_r4);\n\tprintk(\"r3 : %08lx  r2 : %08lx  r1 : %08lx  r0 : %08lx\\n\",\n\t\tregs->ARM_r3, regs->ARM_r2,\n\t\tregs->ARM_r1, regs->ARM_r0);\n\n\tflags = regs->ARM_cpsr;\n\tbuf[0] = flags & PSR_N_BIT ? 'N' : 'n';\n\tbuf[1] = flags & PSR_Z_BIT ? 'Z' : 'z';\n\tbuf[2] = flags & PSR_C_BIT ? 'C' : 'c';\n\tbuf[3] = flags & PSR_V_BIT ? 'V' : 'v';\n\tbuf[4] = '\\0';\n\n\tprintk(\"Flags: %s  IRQs o%s  FIQs o%s  Mode %s  ISA %s  Segment %s\\n\",\n\t\tbuf, interrupts_enabled(regs) ? \"n\" : \"ff\",\n\t\tfast_interrupts_enabled(regs) ? \"n\" : \"ff\",\n\t\tprocessor_modes[processor_mode(regs)],\n\t\tisa_modes[isa_mode(regs)],\n\t\tget_fs() == get_ds() ? \"kernel\" : \"user\");\n#ifdef CONFIG_CPU_CP15\n\t{\n\t\tunsigned int ctrl;\n\n\t\tbuf[0] = '\\0';\n#ifdef CONFIG_CPU_CP15_MMU\n\t\t{\n\t\t\tunsigned int transbase, dac;\n\t\t\tasm(\"mrc p15, 0, %0, c2, c0\\n\\t\"\n\t\t\t    \"mrc p15, 0, %1, c3, c0\\n\"\n\t\t\t    : \"=r\" (transbase), \"=r\" (dac));\n\t\t\tsnprintf(buf, sizeof(buf), \"  Table: %08x  DAC: %08x\",\n\t\t\t  \ttransbase, dac);\n\t\t}\n#endif\n\t\tasm(\"mrc p15, 0, %0, c1, c0\\n\" : \"=r\" (ctrl));\n\n\t\tprintk(\"Control: %08x%s\\n\", ctrl, buf);\n\t}\n#endif\n}\n\nvoid show_regs(struct pt_regs * regs)\n{\n\tprintk(\"\\n\");\n\t__show_regs(regs);\n\tdump_stack();\n}\n\nATOMIC_NOTIFIER_HEAD(thread_notify_head);\n\nEXPORT_SYMBOL_GPL(thread_notify_head);\n\n/*\n * Free current thread data structures etc..\n */\nvoid exit_thread(void)\n{\n\tthread_notify(THREAD_NOTIFY_EXIT, current_thread_info());\n}\n\nvoid flush_thread(void)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tstruct task_struct *tsk = current;\n\n\tflush_ptrace_hw_breakpoint(tsk);\n\n\tmemset(thread->used_cp, 0, sizeof(thread->used_cp));\n\tmemset(&tsk->thread.debug, 0, sizeof(struct debug_info));\n\tmemset(&thread->fpstate, 0, sizeof(union fp_state));\n\n\tthread_notify(THREAD_NOTIFY_FLUSH, thread);\n}\n\nvoid release_thread(struct task_struct *dead_task)\n{\n}\n\nasmlinkage void ret_from_fork(void) __asm__(\"ret_from_fork\");\n\nint\ncopy_thread(unsigned long clone_flags, unsigned long stack_start,\n\t    unsigned long stk_sz, struct task_struct *p)\n{\n\tstruct thread_info *thread = task_thread_info(p);\n\tstruct pt_regs *childregs = task_pt_regs(p);\n\n\tmemset(&thread->cpu_context, 0, sizeof(struct cpu_context_save));\n\n\tif (likely(!(p->flags & PF_KTHREAD))) {\n\t\t*childregs = *current_pt_regs();\n\t\tchildregs->ARM_r0 = 0;\n\t\tif (stack_start)\n\t\t\tchildregs->ARM_sp = stack_start;\n\t} else {\n\t\tmemset(childregs, 0, sizeof(struct pt_regs));\n\t\tthread->cpu_context.r4 = stk_sz;\n\t\tthread->cpu_context.r5 = stack_start;\n\t\tchildregs->ARM_cpsr = SVC_MODE;\n\t}\n\tthread->cpu_context.pc = (unsigned long)ret_from_fork;\n\tthread->cpu_context.sp = (unsigned long)childregs;\n\n\tclear_ptrace_hw_breakpoint(p);\n\n\tif (clone_flags & CLONE_SETTLS)\n\t\tthread->tp_value[0] = childregs->ARM_r3;\n\tthread->tp_value[1] = get_tpuser();\n\n\tthread_notify(THREAD_NOTIFY_COPY, thread);\n\n\treturn 0;\n}\n\n/*\n * Fill in the task's elfregs structure for a core dump.\n */\nint dump_task_regs(struct task_struct *t, elf_gregset_t *elfregs)\n{\n\telf_core_copy_regs(elfregs, task_pt_regs(t));\n\treturn 1;\n}\n\n/*\n * fill in the fpe structure for a core dump...\n */\nint dump_fpu (struct pt_regs *regs, struct user_fp *fp)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tint used_math = thread->used_cp[1] | thread->used_cp[2];\n\n\tif (used_math)\n\t\tmemcpy(fp, &thread->fpstate.soft, sizeof (*fp));\n\n\treturn used_math != 0;\n}\nEXPORT_SYMBOL(dump_fpu);\n\nunsigned long get_wchan(struct task_struct *p)\n{\n\tstruct stackframe frame;\n\tint count = 0;\n\tif (!p || p == current || p->state == TASK_RUNNING)\n\t\treturn 0;\n\n\tframe.fp = thread_saved_fp(p);\n\tframe.sp = thread_saved_sp(p);\n\tframe.lr = 0;\t\t\t/* recovered from the stack */\n\tframe.pc = thread_saved_pc(p);\n\tdo {\n\t\tint ret = unwind_frame(&frame);\n\t\tif (ret < 0)\n\t\t\treturn 0;\n\t\tif (!in_sched_functions(frame.pc))\n\t\t\treturn frame.pc;\n\t} while (count ++ < 16);\n\treturn 0;\n}\n\nunsigned long arch_randomize_brk(struct mm_struct *mm)\n{\n\tunsigned long range_end = mm->brk + 0x02000000;\n\treturn randomize_range(mm->brk, range_end, 0) ? : mm->brk;\n}\n\n#ifdef CONFIG_MMU\n/*\n * The vectors page is always readable from user space for the\n * atomic helpers and the signal restart code. Insert it into the\n * gate_vma so that it is visible through ptrace and /proc/<pid>/mem.\n */\nstatic struct vm_area_struct gate_vma = {\n\t.vm_start\t= 0xffff0000,\n\t.vm_end\t\t= 0xffff0000 + PAGE_SIZE,\n\t.vm_flags\t= VM_READ | VM_EXEC | VM_MAYREAD | VM_MAYEXEC,\n\t.vm_mm\t\t= &init_mm,\n};\n\nstatic int __init gate_vma_init(void)\n{\n\tgate_vma.vm_page_prot = PAGE_READONLY_EXEC;\n\treturn 0;\n}\narch_initcall(gate_vma_init);\n\nstruct vm_area_struct *get_gate_vma(struct mm_struct *mm)\n{\n\treturn &gate_vma;\n}\n\nint in_gate_area(struct mm_struct *mm, unsigned long addr)\n{\n\treturn (addr >= gate_vma.vm_start) && (addr < gate_vma.vm_end);\n}\n\nint in_gate_area_no_mm(unsigned long addr)\n{\n\treturn in_gate_area(NULL, addr);\n}\n\nconst char *arch_vma_name(struct vm_area_struct *vma)\n{\n\treturn (vma == &gate_vma) ? \"[vectors]\" : NULL;\n}\n#endif\n", "/*\n *  linux/arch/arm/kernel/ptrace.c\n *\n *  By Ross Biro 1/23/92\n * edited by Linus Torvalds\n * ARM modifications Copyright (C) 2000 Russell King\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/elf.h>\n#include <linux/smp.h>\n#include <linux/ptrace.h>\n#include <linux/user.h>\n#include <linux/security.h>\n#include <linux/init.h>\n#include <linux/signal.h>\n#include <linux/uaccess.h>\n#include <linux/perf_event.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/regset.h>\n#include <linux/audit.h>\n#include <linux/tracehook.h>\n#include <linux/unistd.h>\n\n#include <asm/pgtable.h>\n#include <asm/traps.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/syscalls.h>\n\n#define REG_PC\t15\n#define REG_PSR\t16\n/*\n * does not yet catch signals sent when the child dies.\n * in exit.c or in signal.c.\n */\n\n#if 0\n/*\n * Breakpoint SWI instruction: SWI &9F0001\n */\n#define BREAKINST_ARM\t0xef9f0001\n#define BREAKINST_THUMB\t0xdf00\t\t/* fill this in later */\n#else\n/*\n * New breakpoints - use an undefined instruction.  The ARM architecture\n * reference manual guarantees that the following instruction space\n * will produce an undefined instruction exception on all CPUs:\n *\n *  ARM:   xxxx 0111 1111 xxxx xxxx xxxx 1111 xxxx\n *  Thumb: 1101 1110 xxxx xxxx\n */\n#define BREAKINST_ARM\t0xe7f001f0\n#define BREAKINST_THUMB\t0xde01\n#endif\n\nstruct pt_regs_offset {\n\tconst char *name;\n\tint offset;\n};\n\n#define REG_OFFSET_NAME(r) \\\n\t{.name = #r, .offset = offsetof(struct pt_regs, ARM_##r)}\n#define REG_OFFSET_END {.name = NULL, .offset = 0}\n\nstatic const struct pt_regs_offset regoffset_table[] = {\n\tREG_OFFSET_NAME(r0),\n\tREG_OFFSET_NAME(r1),\n\tREG_OFFSET_NAME(r2),\n\tREG_OFFSET_NAME(r3),\n\tREG_OFFSET_NAME(r4),\n\tREG_OFFSET_NAME(r5),\n\tREG_OFFSET_NAME(r6),\n\tREG_OFFSET_NAME(r7),\n\tREG_OFFSET_NAME(r8),\n\tREG_OFFSET_NAME(r9),\n\tREG_OFFSET_NAME(r10),\n\tREG_OFFSET_NAME(fp),\n\tREG_OFFSET_NAME(ip),\n\tREG_OFFSET_NAME(sp),\n\tREG_OFFSET_NAME(lr),\n\tREG_OFFSET_NAME(pc),\n\tREG_OFFSET_NAME(cpsr),\n\tREG_OFFSET_NAME(ORIG_r0),\n\tREG_OFFSET_END,\n};\n\n/**\n * regs_query_register_offset() - query register offset from its name\n * @name:\tthe name of a register\n *\n * regs_query_register_offset() returns the offset of a register in struct\n * pt_regs from its name. If the name is invalid, this returns -EINVAL;\n */\nint regs_query_register_offset(const char *name)\n{\n\tconst struct pt_regs_offset *roff;\n\tfor (roff = regoffset_table; roff->name != NULL; roff++)\n\t\tif (!strcmp(roff->name, name))\n\t\t\treturn roff->offset;\n\treturn -EINVAL;\n}\n\n/**\n * regs_query_register_name() - query register name from its offset\n * @offset:\tthe offset of a register in struct pt_regs.\n *\n * regs_query_register_name() returns the name of a register from its\n * offset in struct pt_regs. If the @offset is invalid, this returns NULL;\n */\nconst char *regs_query_register_name(unsigned int offset)\n{\n\tconst struct pt_regs_offset *roff;\n\tfor (roff = regoffset_table; roff->name != NULL; roff++)\n\t\tif (roff->offset == offset)\n\t\t\treturn roff->name;\n\treturn NULL;\n}\n\n/**\n * regs_within_kernel_stack() - check the address in the stack\n * @regs:      pt_regs which contains kernel stack pointer.\n * @addr:      address which is checked.\n *\n * regs_within_kernel_stack() checks @addr is within the kernel stack page(s).\n * If @addr is within the kernel stack, it returns true. If not, returns false.\n */\nbool regs_within_kernel_stack(struct pt_regs *regs, unsigned long addr)\n{\n\treturn ((addr & ~(THREAD_SIZE - 1))  ==\n\t\t(kernel_stack_pointer(regs) & ~(THREAD_SIZE - 1)));\n}\n\n/**\n * regs_get_kernel_stack_nth() - get Nth entry of the stack\n * @regs:\tpt_regs which contains kernel stack pointer.\n * @n:\t\tstack entry number.\n *\n * regs_get_kernel_stack_nth() returns @n th entry of the kernel stack which\n * is specified by @regs. If the @n th entry is NOT in the kernel stack,\n * this returns 0.\n */\nunsigned long regs_get_kernel_stack_nth(struct pt_regs *regs, unsigned int n)\n{\n\tunsigned long *addr = (unsigned long *)kernel_stack_pointer(regs);\n\taddr += n;\n\tif (regs_within_kernel_stack(regs, (unsigned long)addr))\n\t\treturn *addr;\n\telse\n\t\treturn 0;\n}\n\n/*\n * this routine will get a word off of the processes privileged stack.\n * the offset is how far from the base addr as stored in the THREAD.\n * this routine assumes that all the privileged stacks are in our\n * data space.\n */\nstatic inline long get_user_reg(struct task_struct *task, int offset)\n{\n\treturn task_pt_regs(task)->uregs[offset];\n}\n\n/*\n * this routine will put a word on the processes privileged stack.\n * the offset is how far from the base addr as stored in the THREAD.\n * this routine assumes that all the privileged stacks are in our\n * data space.\n */\nstatic inline int\nput_user_reg(struct task_struct *task, int offset, long data)\n{\n\tstruct pt_regs newregs, *regs = task_pt_regs(task);\n\tint ret = -EINVAL;\n\n\tnewregs = *regs;\n\tnewregs.uregs[offset] = data;\n\n\tif (valid_user_regs(&newregs)) {\n\t\tregs->uregs[offset] = data;\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\n/*\n * Called by kernel/ptrace.c when detaching..\n */\nvoid ptrace_disable(struct task_struct *child)\n{\n\t/* Nothing to do. */\n}\n\n/*\n * Handle hitting a breakpoint.\n */\nvoid ptrace_break(struct task_struct *tsk, struct pt_regs *regs)\n{\n\tsiginfo_t info;\n\n\tinfo.si_signo = SIGTRAP;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = TRAP_BRKPT;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs);\n\n\tforce_sig_info(SIGTRAP, &info, tsk);\n}\n\nstatic int break_trap(struct pt_regs *regs, unsigned int instr)\n{\n\tptrace_break(current, regs);\n\treturn 0;\n}\n\nstatic struct undef_hook arm_break_hook = {\n\t.instr_mask\t= 0x0fffffff,\n\t.instr_val\t= 0x07f001f0,\n\t.cpsr_mask\t= PSR_T_BIT,\n\t.cpsr_val\t= 0,\n\t.fn\t\t= break_trap,\n};\n\nstatic struct undef_hook thumb_break_hook = {\n\t.instr_mask\t= 0xffff,\n\t.instr_val\t= 0xde01,\n\t.cpsr_mask\t= PSR_T_BIT,\n\t.cpsr_val\t= PSR_T_BIT,\n\t.fn\t\t= break_trap,\n};\n\nstatic struct undef_hook thumb2_break_hook = {\n\t.instr_mask\t= 0xffffffff,\n\t.instr_val\t= 0xf7f0a000,\n\t.cpsr_mask\t= PSR_T_BIT,\n\t.cpsr_val\t= PSR_T_BIT,\n\t.fn\t\t= break_trap,\n};\n\nstatic int __init ptrace_break_init(void)\n{\n\tregister_undef_hook(&arm_break_hook);\n\tregister_undef_hook(&thumb_break_hook);\n\tregister_undef_hook(&thumb2_break_hook);\n\treturn 0;\n}\n\ncore_initcall(ptrace_break_init);\n\n/*\n * Read the word at offset \"off\" into the \"struct user\".  We\n * actually access the pt_regs stored on the kernel stack.\n */\nstatic int ptrace_read_user(struct task_struct *tsk, unsigned long off,\n\t\t\t    unsigned long __user *ret)\n{\n\tunsigned long tmp;\n\n\tif (off & 3)\n\t\treturn -EIO;\n\n\ttmp = 0;\n\tif (off == PT_TEXT_ADDR)\n\t\ttmp = tsk->mm->start_code;\n\telse if (off == PT_DATA_ADDR)\n\t\ttmp = tsk->mm->start_data;\n\telse if (off == PT_TEXT_END_ADDR)\n\t\ttmp = tsk->mm->end_code;\n\telse if (off < sizeof(struct pt_regs))\n\t\ttmp = get_user_reg(tsk, off >> 2);\n\telse if (off >= sizeof(struct user))\n\t\treturn -EIO;\n\n\treturn put_user(tmp, ret);\n}\n\n/*\n * Write the word at offset \"off\" into \"struct user\".  We\n * actually access the pt_regs stored on the kernel stack.\n */\nstatic int ptrace_write_user(struct task_struct *tsk, unsigned long off,\n\t\t\t     unsigned long val)\n{\n\tif (off & 3 || off >= sizeof(struct user))\n\t\treturn -EIO;\n\n\tif (off >= sizeof(struct pt_regs))\n\t\treturn 0;\n\n\treturn put_user_reg(tsk, off >> 2, val);\n}\n\n#ifdef CONFIG_IWMMXT\n\n/*\n * Get the child iWMMXt state.\n */\nstatic int ptrace_getwmmxregs(struct task_struct *tsk, void __user *ufp)\n{\n\tstruct thread_info *thread = task_thread_info(tsk);\n\n\tif (!test_ti_thread_flag(thread, TIF_USING_IWMMXT))\n\t\treturn -ENODATA;\n\tiwmmxt_task_disable(thread);  /* force it to ram */\n\treturn copy_to_user(ufp, &thread->fpstate.iwmmxt, IWMMXT_SIZE)\n\t\t? -EFAULT : 0;\n}\n\n/*\n * Set the child iWMMXt state.\n */\nstatic int ptrace_setwmmxregs(struct task_struct *tsk, void __user *ufp)\n{\n\tstruct thread_info *thread = task_thread_info(tsk);\n\n\tif (!test_ti_thread_flag(thread, TIF_USING_IWMMXT))\n\t\treturn -EACCES;\n\tiwmmxt_task_release(thread);  /* force a reload */\n\treturn copy_from_user(&thread->fpstate.iwmmxt, ufp, IWMMXT_SIZE)\n\t\t? -EFAULT : 0;\n}\n\n#endif\n\n#ifdef CONFIG_CRUNCH\n/*\n * Get the child Crunch state.\n */\nstatic int ptrace_getcrunchregs(struct task_struct *tsk, void __user *ufp)\n{\n\tstruct thread_info *thread = task_thread_info(tsk);\n\n\tcrunch_task_disable(thread);  /* force it to ram */\n\treturn copy_to_user(ufp, &thread->crunchstate, CRUNCH_SIZE)\n\t\t? -EFAULT : 0;\n}\n\n/*\n * Set the child Crunch state.\n */\nstatic int ptrace_setcrunchregs(struct task_struct *tsk, void __user *ufp)\n{\n\tstruct thread_info *thread = task_thread_info(tsk);\n\n\tcrunch_task_release(thread);  /* force a reload */\n\treturn copy_from_user(&thread->crunchstate, ufp, CRUNCH_SIZE)\n\t\t? -EFAULT : 0;\n}\n#endif\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n/*\n * Convert a virtual register number into an index for a thread_info\n * breakpoint array. Breakpoints are identified using positive numbers\n * whilst watchpoints are negative. The registers are laid out as pairs\n * of (address, control), each pair mapping to a unique hw_breakpoint struct.\n * Register 0 is reserved for describing resource information.\n */\nstatic int ptrace_hbp_num_to_idx(long num)\n{\n\tif (num < 0)\n\t\tnum = (ARM_MAX_BRP << 1) - num;\n\treturn (num - 1) >> 1;\n}\n\n/*\n * Returns the virtual register number for the address of the\n * breakpoint at index idx.\n */\nstatic long ptrace_hbp_idx_to_num(int idx)\n{\n\tlong mid = ARM_MAX_BRP << 1;\n\tlong num = (idx << 1) + 1;\n\treturn num > mid ? mid - num : num;\n}\n\n/*\n * Handle hitting a HW-breakpoint.\n */\nstatic void ptrace_hbptriggered(struct perf_event *bp,\n\t\t\t\t     struct perf_sample_data *data,\n\t\t\t\t     struct pt_regs *regs)\n{\n\tstruct arch_hw_breakpoint *bkpt = counter_arch_bp(bp);\n\tlong num;\n\tint i;\n\tsiginfo_t info;\n\n\tfor (i = 0; i < ARM_MAX_HBP_SLOTS; ++i)\n\t\tif (current->thread.debug.hbp[i] == bp)\n\t\t\tbreak;\n\n\tnum = (i == ARM_MAX_HBP_SLOTS) ? 0 : ptrace_hbp_idx_to_num(i);\n\n\tinfo.si_signo\t= SIGTRAP;\n\tinfo.si_errno\t= (int)num;\n\tinfo.si_code\t= TRAP_HWBKPT;\n\tinfo.si_addr\t= (void __user *)(bkpt->trigger);\n\n\tforce_sig_info(SIGTRAP, &info, current);\n}\n\n/*\n * Set ptrace breakpoint pointers to zero for this task.\n * This is required in order to prevent child processes from unregistering\n * breakpoints held by their parent.\n */\nvoid clear_ptrace_hw_breakpoint(struct task_struct *tsk)\n{\n\tmemset(tsk->thread.debug.hbp, 0, sizeof(tsk->thread.debug.hbp));\n}\n\n/*\n * Unregister breakpoints from this task and reset the pointers in\n * the thread_struct.\n */\nvoid flush_ptrace_hw_breakpoint(struct task_struct *tsk)\n{\n\tint i;\n\tstruct thread_struct *t = &tsk->thread;\n\n\tfor (i = 0; i < ARM_MAX_HBP_SLOTS; i++) {\n\t\tif (t->debug.hbp[i]) {\n\t\t\tunregister_hw_breakpoint(t->debug.hbp[i]);\n\t\t\tt->debug.hbp[i] = NULL;\n\t\t}\n\t}\n}\n\nstatic u32 ptrace_get_hbp_resource_info(void)\n{\n\tu8 num_brps, num_wrps, debug_arch, wp_len;\n\tu32 reg = 0;\n\n\tnum_brps\t= hw_breakpoint_slots(TYPE_INST);\n\tnum_wrps\t= hw_breakpoint_slots(TYPE_DATA);\n\tdebug_arch\t= arch_get_debug_arch();\n\twp_len\t\t= arch_get_max_wp_len();\n\n\treg\t\t|= debug_arch;\n\treg\t\t<<= 8;\n\treg\t\t|= wp_len;\n\treg\t\t<<= 8;\n\treg\t\t|= num_wrps;\n\treg\t\t<<= 8;\n\treg\t\t|= num_brps;\n\n\treturn reg;\n}\n\nstatic struct perf_event *ptrace_hbp_create(struct task_struct *tsk, int type)\n{\n\tstruct perf_event_attr attr;\n\n\tptrace_breakpoint_init(&attr);\n\n\t/* Initialise fields to sane defaults. */\n\tattr.bp_addr\t= 0;\n\tattr.bp_len\t= HW_BREAKPOINT_LEN_4;\n\tattr.bp_type\t= type;\n\tattr.disabled\t= 1;\n\n\treturn register_user_hw_breakpoint(&attr, ptrace_hbptriggered, NULL,\n\t\t\t\t\t   tsk);\n}\n\nstatic int ptrace_gethbpregs(struct task_struct *tsk, long num,\n\t\t\t     unsigned long  __user *data)\n{\n\tu32 reg;\n\tint idx, ret = 0;\n\tstruct perf_event *bp;\n\tstruct arch_hw_breakpoint_ctrl arch_ctrl;\n\n\tif (num == 0) {\n\t\treg = ptrace_get_hbp_resource_info();\n\t} else {\n\t\tidx = ptrace_hbp_num_to_idx(num);\n\t\tif (idx < 0 || idx >= ARM_MAX_HBP_SLOTS) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tbp = tsk->thread.debug.hbp[idx];\n\t\tif (!bp) {\n\t\t\treg = 0;\n\t\t\tgoto put;\n\t\t}\n\n\t\tarch_ctrl = counter_arch_bp(bp)->ctrl;\n\n\t\t/*\n\t\t * Fix up the len because we may have adjusted it\n\t\t * to compensate for an unaligned address.\n\t\t */\n\t\twhile (!(arch_ctrl.len & 0x1))\n\t\t\tarch_ctrl.len >>= 1;\n\n\t\tif (num & 0x1)\n\t\t\treg = bp->attr.bp_addr;\n\t\telse\n\t\t\treg = encode_ctrl_reg(arch_ctrl);\n\t}\n\nput:\n\tif (put_user(reg, data))\n\t\tret = -EFAULT;\n\nout:\n\treturn ret;\n}\n\nstatic int ptrace_sethbpregs(struct task_struct *tsk, long num,\n\t\t\t     unsigned long __user *data)\n{\n\tint idx, gen_len, gen_type, implied_type, ret = 0;\n\tu32 user_val;\n\tstruct perf_event *bp;\n\tstruct arch_hw_breakpoint_ctrl ctrl;\n\tstruct perf_event_attr attr;\n\n\tif (num == 0)\n\t\tgoto out;\n\telse if (num < 0)\n\t\timplied_type = HW_BREAKPOINT_RW;\n\telse\n\t\timplied_type = HW_BREAKPOINT_X;\n\n\tidx = ptrace_hbp_num_to_idx(num);\n\tif (idx < 0 || idx >= ARM_MAX_HBP_SLOTS) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (get_user(user_val, data)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tbp = tsk->thread.debug.hbp[idx];\n\tif (!bp) {\n\t\tbp = ptrace_hbp_create(tsk, implied_type);\n\t\tif (IS_ERR(bp)) {\n\t\t\tret = PTR_ERR(bp);\n\t\t\tgoto out;\n\t\t}\n\t\ttsk->thread.debug.hbp[idx] = bp;\n\t}\n\n\tattr = bp->attr;\n\n\tif (num & 0x1) {\n\t\t/* Address */\n\t\tattr.bp_addr\t= user_val;\n\t} else {\n\t\t/* Control */\n\t\tdecode_ctrl_reg(user_val, &ctrl);\n\t\tret = arch_bp_generic_fields(ctrl, &gen_len, &gen_type);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tif ((gen_type & implied_type) != gen_type) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tattr.bp_len\t= gen_len;\n\t\tattr.bp_type\t= gen_type;\n\t\tattr.disabled\t= !ctrl.enabled;\n\t}\n\n\tret = modify_user_hw_breakpoint(bp, &attr);\nout:\n\treturn ret;\n}\n#endif\n\n/* regset get/set implementations */\n\nstatic int gpr_get(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n\tstruct pt_regs *regs = task_pt_regs(target);\n\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t   regs,\n\t\t\t\t   0, sizeof(*regs));\n}\n\nstatic int gpr_set(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   const void *kbuf, const void __user *ubuf)\n{\n\tint ret;\n\tstruct pt_regs newregs;\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t &newregs,\n\t\t\t\t 0, sizeof(newregs));\n\tif (ret)\n\t\treturn ret;\n\n\tif (!valid_user_regs(&newregs))\n\t\treturn -EINVAL;\n\n\t*task_pt_regs(target) = newregs;\n\treturn 0;\n}\n\nstatic int fpa_get(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t   &task_thread_info(target)->fpstate,\n\t\t\t\t   0, sizeof(struct user_fp));\n}\n\nstatic int fpa_set(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   const void *kbuf, const void __user *ubuf)\n{\n\tstruct thread_info *thread = task_thread_info(target);\n\n\tthread->used_cp[1] = thread->used_cp[2] = 1;\n\n\treturn user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t&thread->fpstate,\n\t\t0, sizeof(struct user_fp));\n}\n\n#ifdef CONFIG_VFP\n/*\n * VFP register get/set implementations.\n *\n * With respect to the kernel, struct user_fp is divided into three chunks:\n * 16 or 32 real VFP registers (d0-d15 or d0-31)\n *\tThese are transferred to/from the real registers in the task's\n *\tvfp_hard_struct.  The number of registers depends on the kernel\n *\tconfiguration.\n *\n * 16 or 0 fake VFP registers (d16-d31 or empty)\n *\ti.e., the user_vfp structure has space for 32 registers even if\n *\tthe kernel doesn't have them all.\n *\n *\tvfp_get() reads this chunk as zero where applicable\n *\tvfp_set() ignores this chunk\n *\n * 1 word for the FPSCR\n *\n * The bounds-checking logic built into user_regset_copyout and friends\n * means that we can make a simple sequence of calls to map the relevant data\n * to/from the specified slice of the user regset structure.\n */\nstatic int vfp_get(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n\tint ret;\n\tstruct thread_info *thread = task_thread_info(target);\n\tstruct vfp_hard_struct const *vfp = &thread->vfpstate.hard;\n\tconst size_t user_fpregs_offset = offsetof(struct user_vfp, fpregs);\n\tconst size_t user_fpscr_offset = offsetof(struct user_vfp, fpscr);\n\n\tvfp_sync_hwstate(thread);\n\n\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  &vfp->fpregs,\n\t\t\t\t  user_fpregs_offset,\n\t\t\t\t  user_fpregs_offset + sizeof(vfp->fpregs));\n\tif (ret)\n\t\treturn ret;\n\n\tret = user_regset_copyout_zero(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t       user_fpregs_offset + sizeof(vfp->fpregs),\n\t\t\t\t       user_fpscr_offset);\n\tif (ret)\n\t\treturn ret;\n\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t   &vfp->fpscr,\n\t\t\t\t   user_fpscr_offset,\n\t\t\t\t   user_fpscr_offset + sizeof(vfp->fpscr));\n}\n\n/*\n * For vfp_set() a read-modify-write is done on the VFP registers,\n * in order to avoid writing back a half-modified set of registers on\n * failure.\n */\nstatic int vfp_set(struct task_struct *target,\n\t\t\t  const struct user_regset *regset,\n\t\t\t  unsigned int pos, unsigned int count,\n\t\t\t  const void *kbuf, const void __user *ubuf)\n{\n\tint ret;\n\tstruct thread_info *thread = task_thread_info(target);\n\tstruct vfp_hard_struct new_vfp;\n\tconst size_t user_fpregs_offset = offsetof(struct user_vfp, fpregs);\n\tconst size_t user_fpscr_offset = offsetof(struct user_vfp, fpscr);\n\n\tvfp_sync_hwstate(thread);\n\tnew_vfp = thread->vfpstate.hard;\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  &new_vfp.fpregs,\n\t\t\t\t  user_fpregs_offset,\n\t\t\t\t  user_fpregs_offset + sizeof(new_vfp.fpregs));\n\tif (ret)\n\t\treturn ret;\n\n\tret = user_regset_copyin_ignore(&pos, &count, &kbuf, &ubuf,\n\t\t\t\tuser_fpregs_offset + sizeof(new_vfp.fpregs),\n\t\t\t\tuser_fpscr_offset);\n\tif (ret)\n\t\treturn ret;\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t &new_vfp.fpscr,\n\t\t\t\t user_fpscr_offset,\n\t\t\t\t user_fpscr_offset + sizeof(new_vfp.fpscr));\n\tif (ret)\n\t\treturn ret;\n\n\tvfp_flush_hwstate(thread);\n\tthread->vfpstate.hard = new_vfp;\n\n\treturn 0;\n}\n#endif /* CONFIG_VFP */\n\nenum arm_regset {\n\tREGSET_GPR,\n\tREGSET_FPR,\n#ifdef CONFIG_VFP\n\tREGSET_VFP,\n#endif\n};\n\nstatic const struct user_regset arm_regsets[] = {\n\t[REGSET_GPR] = {\n\t\t.core_note_type = NT_PRSTATUS,\n\t\t.n = ELF_NGREG,\n\t\t.size = sizeof(u32),\n\t\t.align = sizeof(u32),\n\t\t.get = gpr_get,\n\t\t.set = gpr_set\n\t},\n\t[REGSET_FPR] = {\n\t\t/*\n\t\t * For the FPA regs in fpstate, the real fields are a mixture\n\t\t * of sizes, so pretend that the registers are word-sized:\n\t\t */\n\t\t.core_note_type = NT_PRFPREG,\n\t\t.n = sizeof(struct user_fp) / sizeof(u32),\n\t\t.size = sizeof(u32),\n\t\t.align = sizeof(u32),\n\t\t.get = fpa_get,\n\t\t.set = fpa_set\n\t},\n#ifdef CONFIG_VFP\n\t[REGSET_VFP] = {\n\t\t/*\n\t\t * Pretend that the VFP regs are word-sized, since the FPSCR is\n\t\t * a single word dangling at the end of struct user_vfp:\n\t\t */\n\t\t.core_note_type = NT_ARM_VFP,\n\t\t.n = ARM_VFPREGS_SIZE / sizeof(u32),\n\t\t.size = sizeof(u32),\n\t\t.align = sizeof(u32),\n\t\t.get = vfp_get,\n\t\t.set = vfp_set\n\t},\n#endif /* CONFIG_VFP */\n};\n\nstatic const struct user_regset_view user_arm_view = {\n\t.name = \"arm\", .e_machine = ELF_ARCH, .ei_osabi = ELF_OSABI,\n\t.regsets = arm_regsets, .n = ARRAY_SIZE(arm_regsets)\n};\n\nconst struct user_regset_view *task_user_regset_view(struct task_struct *task)\n{\n\treturn &user_arm_view;\n}\n\nlong arch_ptrace(struct task_struct *child, long request,\n\t\t unsigned long addr, unsigned long data)\n{\n\tint ret;\n\tunsigned long __user *datap = (unsigned long __user *) data;\n\n\tswitch (request) {\n\t\tcase PTRACE_PEEKUSR:\n\t\t\tret = ptrace_read_user(child, addr, datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_POKEUSR:\n\t\t\tret = ptrace_write_user(child, addr, data);\n\t\t\tbreak;\n\n\t\tcase PTRACE_GETREGS:\n\t\t\tret = copy_regset_to_user(child,\n\t\t\t\t\t\t  &user_arm_view, REGSET_GPR,\n\t\t\t\t\t\t  0, sizeof(struct pt_regs),\n\t\t\t\t\t\t  datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETREGS:\n\t\t\tret = copy_regset_from_user(child,\n\t\t\t\t\t\t    &user_arm_view, REGSET_GPR,\n\t\t\t\t\t\t    0, sizeof(struct pt_regs),\n\t\t\t\t\t\t    datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_GETFPREGS:\n\t\t\tret = copy_regset_to_user(child,\n\t\t\t\t\t\t  &user_arm_view, REGSET_FPR,\n\t\t\t\t\t\t  0, sizeof(union fp_state),\n\t\t\t\t\t\t  datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETFPREGS:\n\t\t\tret = copy_regset_from_user(child,\n\t\t\t\t\t\t    &user_arm_view, REGSET_FPR,\n\t\t\t\t\t\t    0, sizeof(union fp_state),\n\t\t\t\t\t\t    datap);\n\t\t\tbreak;\n\n#ifdef CONFIG_IWMMXT\n\t\tcase PTRACE_GETWMMXREGS:\n\t\t\tret = ptrace_getwmmxregs(child, datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETWMMXREGS:\n\t\t\tret = ptrace_setwmmxregs(child, datap);\n\t\t\tbreak;\n#endif\n\n\t\tcase PTRACE_GET_THREAD_AREA:\n\t\t\tret = put_user(task_thread_info(child)->tp_value[0],\n\t\t\t\t       datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SET_SYSCALL:\n\t\t\ttask_thread_info(child)->syscall = data;\n\t\t\tret = 0;\n\t\t\tbreak;\n\n#ifdef CONFIG_CRUNCH\n\t\tcase PTRACE_GETCRUNCHREGS:\n\t\t\tret = ptrace_getcrunchregs(child, datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETCRUNCHREGS:\n\t\t\tret = ptrace_setcrunchregs(child, datap);\n\t\t\tbreak;\n#endif\n\n#ifdef CONFIG_VFP\n\t\tcase PTRACE_GETVFPREGS:\n\t\t\tret = copy_regset_to_user(child,\n\t\t\t\t\t\t  &user_arm_view, REGSET_VFP,\n\t\t\t\t\t\t  0, ARM_VFPREGS_SIZE,\n\t\t\t\t\t\t  datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETVFPREGS:\n\t\t\tret = copy_regset_from_user(child,\n\t\t\t\t\t\t    &user_arm_view, REGSET_VFP,\n\t\t\t\t\t\t    0, ARM_VFPREGS_SIZE,\n\t\t\t\t\t\t    datap);\n\t\t\tbreak;\n#endif\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\t\tcase PTRACE_GETHBPREGS:\n\t\t\tif (ptrace_get_breakpoints(child) < 0)\n\t\t\t\treturn -ESRCH;\n\n\t\t\tret = ptrace_gethbpregs(child, addr,\n\t\t\t\t\t\t(unsigned long __user *)data);\n\t\t\tptrace_put_breakpoints(child);\n\t\t\tbreak;\n\t\tcase PTRACE_SETHBPREGS:\n\t\t\tif (ptrace_get_breakpoints(child) < 0)\n\t\t\t\treturn -ESRCH;\n\n\t\t\tret = ptrace_sethbpregs(child, addr,\n\t\t\t\t\t\t(unsigned long __user *)data);\n\t\t\tptrace_put_breakpoints(child);\n\t\t\tbreak;\n#endif\n\n\t\tdefault:\n\t\t\tret = ptrace_request(child, request, addr, data);\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nenum ptrace_syscall_dir {\n\tPTRACE_SYSCALL_ENTER = 0,\n\tPTRACE_SYSCALL_EXIT,\n};\n\nstatic int tracehook_report_syscall(struct pt_regs *regs,\n\t\t\t\t    enum ptrace_syscall_dir dir)\n{\n\tunsigned long ip;\n\n\t/*\n\t * IP is used to denote syscall entry/exit:\n\t * IP = 0 -> entry, =1 -> exit\n\t */\n\tip = regs->ARM_ip;\n\tregs->ARM_ip = dir;\n\n\tif (dir == PTRACE_SYSCALL_EXIT)\n\t\ttracehook_report_syscall_exit(regs, 0);\n\telse if (tracehook_report_syscall_entry(regs))\n\t\tcurrent_thread_info()->syscall = -1;\n\n\tregs->ARM_ip = ip;\n\treturn current_thread_info()->syscall;\n}\n\nasmlinkage int syscall_trace_enter(struct pt_regs *regs, int scno)\n{\n\tcurrent_thread_info()->syscall = scno;\n\n\t/* Do the secure computing check first; failures should be fast. */\n\tif (secure_computing(scno) == -1)\n\t\treturn -1;\n\n\tif (test_thread_flag(TIF_SYSCALL_TRACE))\n\t\tscno = tracehook_report_syscall(regs, PTRACE_SYSCALL_ENTER);\n\n\tif (test_thread_flag(TIF_SYSCALL_TRACEPOINT))\n\t\ttrace_sys_enter(regs, scno);\n\n\taudit_syscall_entry(AUDIT_ARCH_ARM, scno, regs->ARM_r0, regs->ARM_r1,\n\t\t\t    regs->ARM_r2, regs->ARM_r3);\n\n\treturn scno;\n}\n\nasmlinkage void syscall_trace_exit(struct pt_regs *regs)\n{\n\t/*\n\t * Audit the syscall before anything else, as a debugger may\n\t * come in and change the current registers.\n\t */\n\taudit_syscall_exit(regs);\n\n\t/*\n\t * Note that we haven't updated the ->syscall field for the\n\t * current thread. This isn't a problem because it will have\n\t * been set on syscall entry and there hasn't been an opportunity\n\t * for a PTRACE_SET_SYSCALL since then.\n\t */\n\tif (test_thread_flag(TIF_SYSCALL_TRACEPOINT))\n\t\ttrace_sys_exit(regs, regs_return_value(regs));\n\n\tif (test_thread_flag(TIF_SYSCALL_TRACE))\n\t\ttracehook_report_syscall(regs, PTRACE_SYSCALL_EXIT);\n}\n", "/*\n *  linux/arch/arm/kernel/traps.c\n *\n *  Copyright (C) 1995-2009 Russell King\n *  Fragments that appear the same as linux/arch/i386/kernel/traps.c (C) Linus Torvalds\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n *  'traps.c' handles hardware exceptions after we have saved some state in\n *  'linux/arch/arm/lib/traps.S'.  Mostly a debugging aid, but will probably\n *  kill the offending process.\n */\n#include <linux/signal.h>\n#include <linux/personality.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/uaccess.h>\n#include <linux/hardirq.h>\n#include <linux/kdebug.h>\n#include <linux/module.h>\n#include <linux/kexec.h>\n#include <linux/bug.h>\n#include <linux/delay.h>\n#include <linux/init.h>\n#include <linux/sched.h>\n\n#include <linux/atomic.h>\n#include <asm/cacheflush.h>\n#include <asm/exception.h>\n#include <asm/unistd.h>\n#include <asm/traps.h>\n#include <asm/unwind.h>\n#include <asm/tls.h>\n#include <asm/system_misc.h>\n\n#include \"signal.h\"\n\nstatic const char *handler[]= { \"prefetch abort\", \"data abort\", \"address exception\", \"interrupt\" };\n\nvoid *vectors_page;\n\n#ifdef CONFIG_DEBUG_USER\nunsigned int user_debug;\n\nstatic int __init user_debug_setup(char *str)\n{\n\tget_option(&str, &user_debug);\n\treturn 1;\n}\n__setup(\"user_debug=\", user_debug_setup);\n#endif\n\nstatic void dump_mem(const char *, const char *, unsigned long, unsigned long);\n\nvoid dump_backtrace_entry(unsigned long where, unsigned long from, unsigned long frame)\n{\n#ifdef CONFIG_KALLSYMS\n\tprintk(\"[<%08lx>] (%pS) from [<%08lx>] (%pS)\\n\", where, (void *)where, from, (void *)from);\n#else\n\tprintk(\"Function entered at [<%08lx>] from [<%08lx>]\\n\", where, from);\n#endif\n\n\tif (in_exception_text(where))\n\t\tdump_mem(\"\", \"Exception stack\", frame + 4, frame + 4 + sizeof(struct pt_regs));\n}\n\n#ifndef CONFIG_ARM_UNWIND\n/*\n * Stack pointers should always be within the kernels view of\n * physical memory.  If it is not there, then we can't dump\n * out any information relating to the stack.\n */\nstatic int verify_stack(unsigned long sp)\n{\n\tif (sp < PAGE_OFFSET ||\n\t    (sp > (unsigned long)high_memory && high_memory != NULL))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n#endif\n\n/*\n * Dump out the contents of some memory nicely...\n */\nstatic void dump_mem(const char *lvl, const char *str, unsigned long bottom,\n\t\t     unsigned long top)\n{\n\tunsigned long first;\n\tmm_segment_t fs;\n\tint i;\n\n\t/*\n\t * We need to switch to kernel mode so that we can use __get_user\n\t * to safely read from kernel space.  Note that we now dump the\n\t * code first, just in case the backtrace kills us.\n\t */\n\tfs = get_fs();\n\tset_fs(KERNEL_DS);\n\n\tprintk(\"%s%s(0x%08lx to 0x%08lx)\\n\", lvl, str, bottom, top);\n\n\tfor (first = bottom & ~31; first < top; first += 32) {\n\t\tunsigned long p;\n\t\tchar str[sizeof(\" 12345678\") * 8 + 1];\n\n\t\tmemset(str, ' ', sizeof(str));\n\t\tstr[sizeof(str) - 1] = '\\0';\n\n\t\tfor (p = first, i = 0; i < 8 && p < top; i++, p += 4) {\n\t\t\tif (p >= bottom && p < top) {\n\t\t\t\tunsigned long val;\n\t\t\t\tif (__get_user(val, (unsigned long *)p) == 0)\n\t\t\t\t\tsprintf(str + i * 9, \" %08lx\", val);\n\t\t\t\telse\n\t\t\t\t\tsprintf(str + i * 9, \" ????????\");\n\t\t\t}\n\t\t}\n\t\tprintk(\"%s%04lx:%s\\n\", lvl, first & 0xffff, str);\n\t}\n\n\tset_fs(fs);\n}\n\nstatic void dump_instr(const char *lvl, struct pt_regs *regs)\n{\n\tunsigned long addr = instruction_pointer(regs);\n\tconst int thumb = thumb_mode(regs);\n\tconst int width = thumb ? 4 : 8;\n\tmm_segment_t fs;\n\tchar str[sizeof(\"00000000 \") * 5 + 2 + 1], *p = str;\n\tint i;\n\n\t/*\n\t * We need to switch to kernel mode so that we can use __get_user\n\t * to safely read from kernel space.  Note that we now dump the\n\t * code first, just in case the backtrace kills us.\n\t */\n\tfs = get_fs();\n\tset_fs(KERNEL_DS);\n\n\tfor (i = -4; i < 1 + !!thumb; i++) {\n\t\tunsigned int val, bad;\n\n\t\tif (thumb)\n\t\t\tbad = __get_user(val, &((u16 *)addr)[i]);\n\t\telse\n\t\t\tbad = __get_user(val, &((u32 *)addr)[i]);\n\n\t\tif (!bad)\n\t\t\tp += sprintf(p, i == 0 ? \"(%0*x) \" : \"%0*x \",\n\t\t\t\t\twidth, val);\n\t\telse {\n\t\t\tp += sprintf(p, \"bad PC value\");\n\t\t\tbreak;\n\t\t}\n\t}\n\tprintk(\"%sCode: %s\\n\", lvl, str);\n\n\tset_fs(fs);\n}\n\n#ifdef CONFIG_ARM_UNWIND\nstatic inline void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)\n{\n\tunwind_backtrace(regs, tsk);\n}\n#else\nstatic void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)\n{\n\tunsigned int fp, mode;\n\tint ok = 1;\n\n\tprintk(\"Backtrace: \");\n\n\tif (!tsk)\n\t\ttsk = current;\n\n\tif (regs) {\n\t\tfp = regs->ARM_fp;\n\t\tmode = processor_mode(regs);\n\t} else if (tsk != current) {\n\t\tfp = thread_saved_fp(tsk);\n\t\tmode = 0x10;\n\t} else {\n\t\tasm(\"mov %0, fp\" : \"=r\" (fp) : : \"cc\");\n\t\tmode = 0x10;\n\t}\n\n\tif (!fp) {\n\t\tprintk(\"no frame pointer\");\n\t\tok = 0;\n\t} else if (verify_stack(fp)) {\n\t\tprintk(\"invalid frame pointer 0x%08x\", fp);\n\t\tok = 0;\n\t} else if (fp < (unsigned long)end_of_stack(tsk))\n\t\tprintk(\"frame pointer underflow\");\n\tprintk(\"\\n\");\n\n\tif (ok)\n\t\tc_backtrace(fp, mode);\n}\n#endif\n\nvoid show_stack(struct task_struct *tsk, unsigned long *sp)\n{\n\tdump_backtrace(NULL, tsk);\n\tbarrier();\n}\n\n#ifdef CONFIG_PREEMPT\n#define S_PREEMPT \" PREEMPT\"\n#else\n#define S_PREEMPT \"\"\n#endif\n#ifdef CONFIG_SMP\n#define S_SMP \" SMP\"\n#else\n#define S_SMP \"\"\n#endif\n#ifdef CONFIG_THUMB2_KERNEL\n#define S_ISA \" THUMB2\"\n#else\n#define S_ISA \" ARM\"\n#endif\n\nstatic int __die(const char *str, int err, struct pt_regs *regs)\n{\n\tstruct task_struct *tsk = current;\n\tstatic int die_counter;\n\tint ret;\n\n\tprintk(KERN_EMERG \"Internal error: %s: %x [#%d]\" S_PREEMPT S_SMP\n\t       S_ISA \"\\n\", str, err, ++die_counter);\n\n\t/* trap and error numbers are mostly meaningless on ARM */\n\tret = notify_die(DIE_OOPS, str, regs, err, tsk->thread.trap_no, SIGSEGV);\n\tif (ret == NOTIFY_STOP)\n\t\treturn 1;\n\n\tprint_modules();\n\t__show_regs(regs);\n\tprintk(KERN_EMERG \"Process %.*s (pid: %d, stack limit = 0x%p)\\n\",\n\t\tTASK_COMM_LEN, tsk->comm, task_pid_nr(tsk), end_of_stack(tsk));\n\n\tif (!user_mode(regs) || in_interrupt()) {\n\t\tdump_mem(KERN_EMERG, \"Stack: \", regs->ARM_sp,\n\t\t\t THREAD_SIZE + (unsigned long)task_stack_page(tsk));\n\t\tdump_backtrace(regs, tsk);\n\t\tdump_instr(KERN_EMERG, regs);\n\t}\n\n\treturn 0;\n}\n\nstatic arch_spinlock_t die_lock = __ARCH_SPIN_LOCK_UNLOCKED;\nstatic int die_owner = -1;\nstatic unsigned int die_nest_count;\n\nstatic unsigned long oops_begin(void)\n{\n\tint cpu;\n\tunsigned long flags;\n\n\toops_enter();\n\n\t/* racy, but better than risking deadlock. */\n\traw_local_irq_save(flags);\n\tcpu = smp_processor_id();\n\tif (!arch_spin_trylock(&die_lock)) {\n\t\tif (cpu == die_owner)\n\t\t\t/* nested oops. should stop eventually */;\n\t\telse\n\t\t\tarch_spin_lock(&die_lock);\n\t}\n\tdie_nest_count++;\n\tdie_owner = cpu;\n\tconsole_verbose();\n\tbust_spinlocks(1);\n\treturn flags;\n}\n\nstatic void oops_end(unsigned long flags, struct pt_regs *regs, int signr)\n{\n\tif (regs && kexec_should_crash(current))\n\t\tcrash_kexec(regs);\n\n\tbust_spinlocks(0);\n\tdie_owner = -1;\n\tadd_taint(TAINT_DIE, LOCKDEP_NOW_UNRELIABLE);\n\tdie_nest_count--;\n\tif (!die_nest_count)\n\t\t/* Nest count reaches zero, release the lock. */\n\t\tarch_spin_unlock(&die_lock);\n\traw_local_irq_restore(flags);\n\toops_exit();\n\n\tif (in_interrupt())\n\t\tpanic(\"Fatal exception in interrupt\");\n\tif (panic_on_oops)\n\t\tpanic(\"Fatal exception\");\n\tif (signr)\n\t\tdo_exit(signr);\n}\n\n/*\n * This function is protected against re-entrancy.\n */\nvoid die(const char *str, struct pt_regs *regs, int err)\n{\n\tenum bug_trap_type bug_type = BUG_TRAP_TYPE_NONE;\n\tunsigned long flags = oops_begin();\n\tint sig = SIGSEGV;\n\n\tif (!user_mode(regs))\n\t\tbug_type = report_bug(regs->ARM_pc, regs);\n\tif (bug_type != BUG_TRAP_TYPE_NONE)\n\t\tstr = \"Oops - BUG\";\n\n\tif (__die(str, err, regs))\n\t\tsig = 0;\n\n\toops_end(flags, regs, sig);\n}\n\nvoid arm_notify_die(const char *str, struct pt_regs *regs,\n\t\tstruct siginfo *info, unsigned long err, unsigned long trap)\n{\n\tif (user_mode(regs)) {\n\t\tcurrent->thread.error_code = err;\n\t\tcurrent->thread.trap_no = trap;\n\n\t\tforce_sig_info(info->si_signo, info, current);\n\t} else {\n\t\tdie(str, regs, err);\n\t}\n}\n\n#ifdef CONFIG_GENERIC_BUG\n\nint is_valid_bugaddr(unsigned long pc)\n{\n#ifdef CONFIG_THUMB2_KERNEL\n\tunsigned short bkpt;\n#else\n\tunsigned long bkpt;\n#endif\n\n\tif (probe_kernel_address((unsigned *)pc, bkpt))\n\t\treturn 0;\n\n\treturn bkpt == BUG_INSTR_VALUE;\n}\n\n#endif\n\nstatic LIST_HEAD(undef_hook);\nstatic DEFINE_RAW_SPINLOCK(undef_lock);\n\nvoid register_undef_hook(struct undef_hook *hook)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&undef_lock, flags);\n\tlist_add(&hook->node, &undef_hook);\n\traw_spin_unlock_irqrestore(&undef_lock, flags);\n}\n\nvoid unregister_undef_hook(struct undef_hook *hook)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&undef_lock, flags);\n\tlist_del(&hook->node);\n\traw_spin_unlock_irqrestore(&undef_lock, flags);\n}\n\nstatic int call_undef_hook(struct pt_regs *regs, unsigned int instr)\n{\n\tstruct undef_hook *hook;\n\tunsigned long flags;\n\tint (*fn)(struct pt_regs *regs, unsigned int instr) = NULL;\n\n\traw_spin_lock_irqsave(&undef_lock, flags);\n\tlist_for_each_entry(hook, &undef_hook, node)\n\t\tif ((instr & hook->instr_mask) == hook->instr_val &&\n\t\t    (regs->ARM_cpsr & hook->cpsr_mask) == hook->cpsr_val)\n\t\t\tfn = hook->fn;\n\traw_spin_unlock_irqrestore(&undef_lock, flags);\n\n\treturn fn ? fn(regs, instr) : 1;\n}\n\nasmlinkage void __exception do_undefinstr(struct pt_regs *regs)\n{\n\tunsigned int instr;\n\tsiginfo_t info;\n\tvoid __user *pc;\n\n\tpc = (void __user *)instruction_pointer(regs);\n\n\tif (processor_mode(regs) == SVC_MODE) {\n#ifdef CONFIG_THUMB2_KERNEL\n\t\tif (thumb_mode(regs)) {\n\t\t\tinstr = ((u16 *)pc)[0];\n\t\t\tif (is_wide_instruction(instr)) {\n\t\t\t\tinstr <<= 16;\n\t\t\t\tinstr |= ((u16 *)pc)[1];\n\t\t\t}\n\t\t} else\n#endif\n\t\t\tinstr = *(u32 *) pc;\n\t} else if (thumb_mode(regs)) {\n\t\tif (get_user(instr, (u16 __user *)pc))\n\t\t\tgoto die_sig;\n\t\tif (is_wide_instruction(instr)) {\n\t\t\tunsigned int instr2;\n\t\t\tif (get_user(instr2, (u16 __user *)pc+1))\n\t\t\t\tgoto die_sig;\n\t\t\tinstr <<= 16;\n\t\t\tinstr |= instr2;\n\t\t}\n\t} else if (get_user(instr, (u32 __user *)pc)) {\n\t\tgoto die_sig;\n\t}\n\n\tif (call_undef_hook(regs, instr) == 0)\n\t\treturn;\n\ndie_sig:\n#ifdef CONFIG_DEBUG_USER\n\tif (user_debug & UDBG_UNDEFINED) {\n\t\tprintk(KERN_INFO \"%s (%d): undefined instruction: pc=%p\\n\",\n\t\t\tcurrent->comm, task_pid_nr(current), pc);\n\t\tdump_instr(KERN_INFO, regs);\n\t}\n#endif\n\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLOPC;\n\tinfo.si_addr  = pc;\n\n\tarm_notify_die(\"Oops - undefined instruction\", regs, &info, 0, 6);\n}\n\nasmlinkage void do_unexp_fiq (struct pt_regs *regs)\n{\n\tprintk(\"Hmm.  Unexpected FIQ received, but trying to continue\\n\");\n\tprintk(\"You may have a hardware problem...\\n\");\n}\n\n/*\n * bad_mode handles the impossible case in the vectors.  If you see one of\n * these, then it's extremely serious, and could mean you have buggy hardware.\n * It never returns, and never tries to sync.  We hope that we can at least\n * dump out some state information...\n */\nasmlinkage void bad_mode(struct pt_regs *regs, int reason)\n{\n\tconsole_verbose();\n\n\tprintk(KERN_CRIT \"Bad mode in %s handler detected\\n\", handler[reason]);\n\n\tdie(\"Oops - bad mode\", regs, 0);\n\tlocal_irq_disable();\n\tpanic(\"bad mode\");\n}\n\nstatic int bad_syscall(int n, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((current->personality & PER_MASK) != PER_LINUX &&\n\t    thread->exec_domain->handler) {\n\t\tthread->exec_domain->handler(n, regs);\n\t\treturn regs->ARM_r0;\n\t}\n\n#ifdef CONFIG_DEBUG_USER\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(KERN_ERR \"[%d] %s: obsolete system call %08x.\\n\",\n\t\t\ttask_pid_nr(current), current->comm, n);\n\t\tdump_instr(KERN_ERR, regs);\n\t}\n#endif\n\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall\", regs, &info, n, 0);\n\n\treturn regs->ARM_r0;\n}\n\nstatic inline int\ndo_cache_op(unsigned long start, unsigned long end, int flags)\n{\n\tstruct mm_struct *mm = current->active_mm;\n\tstruct vm_area_struct *vma;\n\n\tif (end < start || flags)\n\t\treturn -EINVAL;\n\n\tdown_read(&mm->mmap_sem);\n\tvma = find_vma(mm, start);\n\tif (vma && vma->vm_start < end) {\n\t\tif (start < vma->vm_start)\n\t\t\tstart = vma->vm_start;\n\t\tif (end > vma->vm_end)\n\t\t\tend = vma->vm_end;\n\n\t\tup_read(&mm->mmap_sem);\n\t\treturn flush_cache_user_range(start, end);\n\t}\n\tup_read(&mm->mmap_sem);\n\treturn -EINVAL;\n}\n\n/*\n * Handle all unrecognised system calls.\n *  0x9f0000 - 0x9fffff are some more esoteric system calls\n */\n#define NR(x) ((__ARM_NR_##x) - __ARM_NR_BASE)\nasmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value[0] = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}\n\n#ifdef CONFIG_TLS_REG_EMUL\n\n/*\n * We might be running on an ARMv6+ processor which should have the TLS\n * register but for some reason we can't use it, or maybe an SMP system\n * using a pre-ARMv6 processor (there are apparently a few prototypes like\n * that in existence) and therefore access to that register must be\n * emulated.\n */\n\nstatic int get_tp_trap(struct pt_regs *regs, unsigned int instr)\n{\n\tint reg = (instr >> 12) & 15;\n\tif (reg == 15)\n\t\treturn 1;\n\tregs->uregs[reg] = current_thread_info()->tp_value[0];\n\tregs->ARM_pc += 4;\n\treturn 0;\n}\n\nstatic struct undef_hook arm_mrc_hook = {\n\t.instr_mask\t= 0x0fff0fff,\n\t.instr_val\t= 0x0e1d0f70,\n\t.cpsr_mask\t= PSR_T_BIT,\n\t.cpsr_val\t= 0,\n\t.fn\t\t= get_tp_trap,\n};\n\nstatic int __init arm_mrc_hook_init(void)\n{\n\tregister_undef_hook(&arm_mrc_hook);\n\treturn 0;\n}\n\nlate_initcall(arm_mrc_hook_init);\n\n#endif\n\nvoid __bad_xchg(volatile void *ptr, int size)\n{\n\tprintk(\"xchg: bad data size: pc 0x%p, ptr 0x%p, size %d\\n\",\n\t\t__builtin_return_address(0), ptr, size);\n\tBUG();\n}\nEXPORT_SYMBOL(__bad_xchg);\n\n/*\n * A data abort trap was taken, but we did not handle the instruction.\n * Try to abort the user program, or panic if it was the kernel.\n */\nasmlinkage void\nbaddataabort(int code, unsigned long instr, struct pt_regs *regs)\n{\n\tunsigned long addr = instruction_pointer(regs);\n\tsiginfo_t info;\n\n#ifdef CONFIG_DEBUG_USER\n\tif (user_debug & UDBG_BADABORT) {\n\t\tprintk(KERN_ERR \"[%d] %s: bad data abort: code %d instr 0x%08lx\\n\",\n\t\t\ttask_pid_nr(current), current->comm, code, instr);\n\t\tdump_instr(KERN_ERR, regs);\n\t\tshow_pte(current->mm, addr);\n\t}\n#endif\n\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLOPC;\n\tinfo.si_addr  = (void __user *)addr;\n\n\tarm_notify_die(\"unknown data abort code\", regs, &info, instr, 0);\n}\n\nvoid __readwrite_bug(const char *fn)\n{\n\tprintk(\"%s called, but not implemented\\n\", fn);\n\tBUG();\n}\nEXPORT_SYMBOL(__readwrite_bug);\n\nvoid __pte_error(const char *file, int line, pte_t pte)\n{\n\tprintk(\"%s:%d: bad pte %08llx.\\n\", file, line, (long long)pte_val(pte));\n}\n\nvoid __pmd_error(const char *file, int line, pmd_t pmd)\n{\n\tprintk(\"%s:%d: bad pmd %08llx.\\n\", file, line, (long long)pmd_val(pmd));\n}\n\nvoid __pgd_error(const char *file, int line, pgd_t pgd)\n{\n\tprintk(\"%s:%d: bad pgd %08llx.\\n\", file, line, (long long)pgd_val(pgd));\n}\n\nasmlinkage void __div0(void)\n{\n\tprintk(\"Division by zero in kernel.\\n\");\n\tdump_stack();\n}\nEXPORT_SYMBOL(__div0);\n\nvoid abort(void)\n{\n\tBUG();\n\n\t/* if that doesn't kill us, halt */\n\tpanic(\"Oops failed to kill thread\");\n}\nEXPORT_SYMBOL(abort);\n\nvoid __init trap_init(void)\n{\n\treturn;\n}\n\nstatic void __init kuser_get_tls_init(unsigned long vectors)\n{\n\t/*\n\t * vectors + 0xfe0 = __kuser_get_tls\n\t * vectors + 0xfe8 = hardware TLS instruction at 0xffff0fe8\n\t */\n\tif (tls_emu || has_tls_reg)\n\t\tmemcpy((void *)vectors + 0xfe0, (void *)vectors + 0xfe8, 4);\n}\n\nvoid __init early_trap_init(void *vectors_base)\n{\n\tunsigned long vectors = (unsigned long)vectors_base;\n\textern char __stubs_start[], __stubs_end[];\n\textern char __vectors_start[], __vectors_end[];\n\textern char __kuser_helper_start[], __kuser_helper_end[];\n\tint kuser_sz = __kuser_helper_end - __kuser_helper_start;\n\n\tvectors_page = vectors_base;\n\n\t/*\n\t * Copy the vectors, stubs and kuser helpers (in entry-armv.S)\n\t * into the vector page, mapped at 0xffff0000, and ensure these\n\t * are visible to the instruction stream.\n\t */\n\tmemcpy((void *)vectors, __vectors_start, __vectors_end - __vectors_start);\n\tmemcpy((void *)vectors + 0x200, __stubs_start, __stubs_end - __stubs_start);\n\tmemcpy((void *)vectors + 0x1000 - kuser_sz, __kuser_helper_start, kuser_sz);\n\n\t/*\n\t * Do processor specific fixups for the kuser helpers\n\t */\n\tkuser_get_tls_init(vectors);\n\n\t/*\n\t * Copy signal return handlers into the vector page, and\n\t * set sigreturn to be a pointer to these.\n\t */\n\tmemcpy((void *)(vectors + KERN_SIGRETURN_CODE - CONFIG_VECTORS_BASE),\n\t       sigreturn_codes, sizeof(sigreturn_codes));\n\n\tflush_icache_range(vectors, vectors + PAGE_SIZE);\n\tmodify_domain(DOMAIN_USER, DOMAIN_CLIENT);\n}\n"], "filenames": ["arch/arm/include/asm/thread_info.h", "arch/arm/include/asm/tls.h", "arch/arm/kernel/entry-armv.S", "arch/arm/kernel/process.c", "arch/arm/kernel/ptrace.c", "arch/arm/kernel/traps.c"], "buggy_code_start_loc": [61, 5, 688, 41, 852, 584], "buggy_code_end_loc": [62, 48, 697, 347, 853, 703], "fixing_code_start_loc": [61, 5, 687, 42, 852, 584], "fixing_code_end_loc": [62, 63, 698, 349, 853, 703], "type": "CWE-264", "message": "The Linux kernel before 3.11 on ARM platforms, as used in Android before 2016-08-05 on Nexus 5 and 7 (2013) devices, does not properly consider user-space access to the TPIDRURW register, which allows local users to gain privileges via a crafted application, aka Android internal bug 28749743 and Qualcomm internal bug CR561044.", "other": {"cve": {"id": "CVE-2014-9870", "sourceIdentifier": "security@android.com", "published": "2016-08-06T10:59:10.417", "lastModified": "2016-11-28T19:15:12.653", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The Linux kernel before 3.11 on ARM platforms, as used in Android before 2016-08-05 on Nexus 5 and 7 (2013) devices, does not properly consider user-space access to the TPIDRURW register, which allows local users to gain privileges via a crafted application, aka Android internal bug 28749743 and Qualcomm internal bug CR561044."}, {"lang": "es", "value": "El kernel de Linux en versiones anteriores a 3.11 en plataformas ARM, tal como se utiliza en Android en versiones anteriores a 2016-08-05 en dispositivos Nexus 5 y 7 (2013), no restringe adecuadamente el acceso de espacio de usuario al registro TPIDRURW, lo que permite a los usuarios locales obtener privilegios a trav\u00e9s de una aplicaci\u00f3n manipulada, tambi\u00e9n conocido como error interno de Android 28749743 y error interno de Qualcomm CR561044."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:C/I:C/A:C", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 9.3}, "baseSeverity": "HIGH", "exploitabilityScore": 8.6, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": true}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-264"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:google:android:*:*:*:*:*:*:*:*", "versionEndIncluding": "6.0.1", "matchCriteriaId": "2567A6D5-BBA1-47B2-B1C3-EFABE9408FA9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.10.101", "matchCriteriaId": "DB4872CF-FBF1-49F6-910C-F1C17B204030"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=a4780adeefd042482f624f5e0d577bf9cdcbb760", "source": "security@android.com", "tags": ["Issue Tracking", "Patch"]}, {"url": "http://source.android.com/security/bulletin/2016-08-01.html", "source": "security@android.com", "tags": ["Vendor Advisory"]}, {"url": "http://www.securityfocus.com/bid/92219", "source": "security@android.com"}, {"url": "https://github.com/torvalds/linux/commit/a4780adeefd042482f624f5e0d577bf9cdcbb760", "source": "security@android.com", "tags": ["Issue Tracking", "Patch"]}, {"url": "https://source.codeaurora.org/quic/la/kernel/msm/commit/?id=4f57652fcd2dce7741f1ac6dc0417e2f265cd1de", "source": "security@android.com", "tags": ["Issue Tracking", "Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/a4780adeefd042482f624f5e0d577bf9cdcbb760"}}