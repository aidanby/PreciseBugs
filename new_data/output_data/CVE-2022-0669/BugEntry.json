{"buggy_code": ["/* SPDX-License-Identifier: BSD-3-Clause\n * Copyright(c) 2010-2018 Intel Corporation\n */\n\n/* Security model\n * --------------\n * The vhost-user protocol connection is an external interface, so it must be\n * robust against invalid inputs.\n *\n * This is important because the vhost-user master is only one step removed\n * from the guest.  Malicious guests that have escaped will then launch further\n * attacks from the vhost-user master.\n *\n * Even in deployments where guests are trusted, a bug in the vhost-user master\n * can still cause invalid messages to be sent.  Such messages must not\n * compromise the stability of the DPDK application by causing crashes, memory\n * corruption, or other problematic behavior.\n *\n * Do not assume received VhostUserMsg fields contain sensible values!\n */\n\n#include <stdint.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <unistd.h>\n#include <fcntl.h>\n#include <sys/ioctl.h>\n#include <sys/mman.h>\n#include <sys/stat.h>\n#include <sys/syscall.h>\n#ifdef RTE_LIBRTE_VHOST_NUMA\n#include <numaif.h>\n#endif\n#ifdef RTE_LIBRTE_VHOST_POSTCOPY\n#include <linux/userfaultfd.h>\n#endif\n#ifdef F_ADD_SEALS /* if file sealing is supported, so is memfd */\n#include <linux/memfd.h>\n#define MEMFD_SUPPORTED\n#endif\n\n#include <rte_common.h>\n#include <rte_malloc.h>\n#include <rte_log.h>\n#include <rte_vfio.h>\n#include <rte_errno.h>\n\n#include \"iotlb.h\"\n#include \"vhost.h\"\n#include \"vhost_user.h\"\n\n#define VIRTIO_MIN_MTU 68\n#define VIRTIO_MAX_MTU 65535\n\n#define INFLIGHT_ALIGNMENT\t64\n#define INFLIGHT_VERSION\t0x1\n\nstatic const char *vhost_message_str[VHOST_USER_MAX] = {\n\t[VHOST_USER_NONE] = \"VHOST_USER_NONE\",\n\t[VHOST_USER_GET_FEATURES] = \"VHOST_USER_GET_FEATURES\",\n\t[VHOST_USER_SET_FEATURES] = \"VHOST_USER_SET_FEATURES\",\n\t[VHOST_USER_SET_OWNER] = \"VHOST_USER_SET_OWNER\",\n\t[VHOST_USER_RESET_OWNER] = \"VHOST_USER_RESET_OWNER\",\n\t[VHOST_USER_SET_MEM_TABLE] = \"VHOST_USER_SET_MEM_TABLE\",\n\t[VHOST_USER_SET_LOG_BASE] = \"VHOST_USER_SET_LOG_BASE\",\n\t[VHOST_USER_SET_LOG_FD] = \"VHOST_USER_SET_LOG_FD\",\n\t[VHOST_USER_SET_VRING_NUM] = \"VHOST_USER_SET_VRING_NUM\",\n\t[VHOST_USER_SET_VRING_ADDR] = \"VHOST_USER_SET_VRING_ADDR\",\n\t[VHOST_USER_SET_VRING_BASE] = \"VHOST_USER_SET_VRING_BASE\",\n\t[VHOST_USER_GET_VRING_BASE] = \"VHOST_USER_GET_VRING_BASE\",\n\t[VHOST_USER_SET_VRING_KICK] = \"VHOST_USER_SET_VRING_KICK\",\n\t[VHOST_USER_SET_VRING_CALL] = \"VHOST_USER_SET_VRING_CALL\",\n\t[VHOST_USER_SET_VRING_ERR]  = \"VHOST_USER_SET_VRING_ERR\",\n\t[VHOST_USER_GET_PROTOCOL_FEATURES]  = \"VHOST_USER_GET_PROTOCOL_FEATURES\",\n\t[VHOST_USER_SET_PROTOCOL_FEATURES]  = \"VHOST_USER_SET_PROTOCOL_FEATURES\",\n\t[VHOST_USER_GET_QUEUE_NUM]  = \"VHOST_USER_GET_QUEUE_NUM\",\n\t[VHOST_USER_SET_VRING_ENABLE]  = \"VHOST_USER_SET_VRING_ENABLE\",\n\t[VHOST_USER_SEND_RARP]  = \"VHOST_USER_SEND_RARP\",\n\t[VHOST_USER_NET_SET_MTU]  = \"VHOST_USER_NET_SET_MTU\",\n\t[VHOST_USER_SET_SLAVE_REQ_FD]  = \"VHOST_USER_SET_SLAVE_REQ_FD\",\n\t[VHOST_USER_IOTLB_MSG]  = \"VHOST_USER_IOTLB_MSG\",\n\t[VHOST_USER_CRYPTO_CREATE_SESS] = \"VHOST_USER_CRYPTO_CREATE_SESS\",\n\t[VHOST_USER_CRYPTO_CLOSE_SESS] = \"VHOST_USER_CRYPTO_CLOSE_SESS\",\n\t[VHOST_USER_POSTCOPY_ADVISE]  = \"VHOST_USER_POSTCOPY_ADVISE\",\n\t[VHOST_USER_POSTCOPY_LISTEN]  = \"VHOST_USER_POSTCOPY_LISTEN\",\n\t[VHOST_USER_POSTCOPY_END]  = \"VHOST_USER_POSTCOPY_END\",\n\t[VHOST_USER_GET_INFLIGHT_FD] = \"VHOST_USER_GET_INFLIGHT_FD\",\n\t[VHOST_USER_SET_INFLIGHT_FD] = \"VHOST_USER_SET_INFLIGHT_FD\",\n\t[VHOST_USER_SET_STATUS] = \"VHOST_USER_SET_STATUS\",\n\t[VHOST_USER_GET_STATUS] = \"VHOST_USER_GET_STATUS\",\n};\n\nstatic int send_vhost_reply(struct virtio_net *dev, int sockfd, struct vhu_msg_context *ctx);\nstatic int read_vhost_message(struct virtio_net *dev, int sockfd, struct vhu_msg_context *ctx);\n\nstatic void\nclose_msg_fds(struct vhu_msg_context *ctx)\n{\n\tint i;\n\n\tfor (i = 0; i < ctx->fd_num; i++) {\n\t\tint fd = ctx->fds[i];\n\n\t\tif (fd == -1)\n\t\t\tcontinue;\n\n\t\tctx->fds[i] = -1;\n\t\tclose(fd);\n\t}\n}\n\n/*\n * Ensure the expected number of FDs is received,\n * close all FDs and return an error if this is not the case.\n */\nstatic int\nvalidate_msg_fds(struct virtio_net *dev, struct vhu_msg_context *ctx, int expected_fds)\n{\n\tif (ctx->fd_num == expected_fds)\n\t\treturn 0;\n\n\tVHOST_LOG_CONFIG(ERR, \"(%s) expect %d FDs for request %s, received %d\\n\",\n\t\tdev->ifname, expected_fds,\n\t\tvhost_message_str[ctx->msg.request.master],\n\t\tctx->fd_num);\n\n\tclose_msg_fds(ctx);\n\n\treturn -1;\n}\n\nstatic uint64_t\nget_blk_size(int fd)\n{\n\tstruct stat stat;\n\tint ret;\n\n\tret = fstat(fd, &stat);\n\treturn ret == -1 ? (uint64_t)-1 : (uint64_t)stat.st_blksize;\n}\n\nstatic void\nasync_dma_map(struct virtio_net *dev, bool do_map)\n{\n\tint ret = 0;\n\tuint32_t i;\n\tstruct guest_page *page;\n\n\tif (do_map) {\n\t\tfor (i = 0; i < dev->nr_guest_pages; i++) {\n\t\t\tpage = &dev->guest_pages[i];\n\t\t\tret = rte_vfio_container_dma_map(RTE_VFIO_DEFAULT_CONTAINER_FD,\n\t\t\t\t\t\t\t page->host_user_addr,\n\t\t\t\t\t\t\t page->host_iova,\n\t\t\t\t\t\t\t page->size);\n\t\t\tif (ret) {\n\t\t\t\t/*\n\t\t\t\t * DMA device may bind with kernel driver, in this case,\n\t\t\t\t * we don't need to program IOMMU manually. However, if no\n\t\t\t\t * device is bound with vfio/uio in DPDK, and vfio kernel\n\t\t\t\t * module is loaded, the API will still be called and return\n\t\t\t\t * with ENODEV.\n\t\t\t\t *\n\t\t\t\t * DPDK vfio only returns ENODEV in very similar situations\n\t\t\t\t * (vfio either unsupported, or supported but no devices found).\n\t\t\t\t * Either way, no mappings could be performed. We treat it as\n\t\t\t\t * normal case in async path. This is a workaround.\n\t\t\t\t */\n\t\t\t\tif (rte_errno == ENODEV)\n\t\t\t\t\treturn;\n\n\t\t\t\t/* DMA mapping errors won't stop VHOST_USER_SET_MEM_TABLE. */\n\t\t\t\tVHOST_LOG_CONFIG(ERR, \"DMA engine map failed\\n\");\n\t\t\t}\n\t\t}\n\n\t} else {\n\t\tfor (i = 0; i < dev->nr_guest_pages; i++) {\n\t\t\tpage = &dev->guest_pages[i];\n\t\t\tret = rte_vfio_container_dma_unmap(RTE_VFIO_DEFAULT_CONTAINER_FD,\n\t\t\t\t\t\t\t   page->host_user_addr,\n\t\t\t\t\t\t\t   page->host_iova,\n\t\t\t\t\t\t\t   page->size);\n\t\t\tif (ret) {\n\t\t\t\t/* like DMA map, ignore the kernel driver case when unmap. */\n\t\t\t\tif (rte_errno == EINVAL)\n\t\t\t\t\treturn;\n\n\t\t\t\tVHOST_LOG_CONFIG(ERR, \"DMA engine unmap failed\\n\");\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void\nfree_mem_region(struct virtio_net *dev)\n{\n\tuint32_t i;\n\tstruct rte_vhost_mem_region *reg;\n\n\tif (!dev || !dev->mem)\n\t\treturn;\n\n\tif (dev->async_copy && rte_vfio_is_enabled(\"vfio\"))\n\t\tasync_dma_map(dev, false);\n\n\tfor (i = 0; i < dev->mem->nregions; i++) {\n\t\treg = &dev->mem->regions[i];\n\t\tif (reg->host_user_addr) {\n\t\t\tmunmap(reg->mmap_addr, reg->mmap_size);\n\t\t\tclose(reg->fd);\n\t\t}\n\t}\n}\n\nvoid\nvhost_backend_cleanup(struct virtio_net *dev)\n{\n\tstruct rte_vdpa_device *vdpa_dev;\n\n\tvdpa_dev = dev->vdpa_dev;\n\tif (vdpa_dev && vdpa_dev->ops->dev_cleanup != NULL)\n\t\tvdpa_dev->ops->dev_cleanup(dev->vid);\n\n\tif (dev->mem) {\n\t\tfree_mem_region(dev);\n\t\trte_free(dev->mem);\n\t\tdev->mem = NULL;\n\t}\n\n\trte_free(dev->guest_pages);\n\tdev->guest_pages = NULL;\n\n\tif (dev->log_addr) {\n\t\tmunmap((void *)(uintptr_t)dev->log_addr, dev->log_size);\n\t\tdev->log_addr = 0;\n\t}\n\n\tif (dev->inflight_info) {\n\t\tif (dev->inflight_info->addr) {\n\t\t\tmunmap(dev->inflight_info->addr,\n\t\t\t       dev->inflight_info->size);\n\t\t\tdev->inflight_info->addr = NULL;\n\t\t}\n\n\t\tif (dev->inflight_info->fd >= 0) {\n\t\t\tclose(dev->inflight_info->fd);\n\t\t\tdev->inflight_info->fd = -1;\n\t\t}\n\n\t\trte_free(dev->inflight_info);\n\t\tdev->inflight_info = NULL;\n\t}\n\n\tif (dev->slave_req_fd >= 0) {\n\t\tclose(dev->slave_req_fd);\n\t\tdev->slave_req_fd = -1;\n\t}\n\n\tif (dev->postcopy_ufd >= 0) {\n\t\tclose(dev->postcopy_ufd);\n\t\tdev->postcopy_ufd = -1;\n\t}\n\n\tdev->postcopy_listening = 0;\n}\n\nstatic void\nvhost_user_notify_queue_state(struct virtio_net *dev, uint16_t index,\n\t\t\t      int enable)\n{\n\tstruct rte_vdpa_device *vdpa_dev = dev->vdpa_dev;\n\tstruct vhost_virtqueue *vq = dev->virtqueue[index];\n\n\t/* Configure guest notifications on enable */\n\tif (enable && vq->notif_enable != VIRTIO_UNINITIALIZED_NOTIF)\n\t\tvhost_enable_guest_notification(dev, vq, vq->notif_enable);\n\n\tif (vdpa_dev && vdpa_dev->ops->set_vring_state)\n\t\tvdpa_dev->ops->set_vring_state(dev->vid, index, enable);\n\n\tif (dev->notify_ops->vring_state_changed)\n\t\tdev->notify_ops->vring_state_changed(dev->vid,\n\t\t\t\tindex, enable);\n}\n\n/*\n * This function just returns success at the moment unless\n * the device hasn't been initialised.\n */\nstatic int\nvhost_user_set_owner(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_user_reset_owner(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tvhost_destroy_device_notify(dev);\n\n\tcleanup_device(dev, 0);\n\treset_device(dev);\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\n/*\n * The features that we support are requested.\n */\nstatic int\nvhost_user_get_features(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tuint64_t features = 0;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\trte_vhost_driver_get_features(dev->ifname, &features);\n\n\tctx->msg.payload.u64 = features;\n\tctx->msg.size = sizeof(ctx->msg.payload.u64);\n\tctx->fd_num = 0;\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n}\n\n/*\n * The queue number that we support are requested.\n */\nstatic int\nvhost_user_get_queue_num(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tuint32_t queue_num = 0;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\trte_vhost_driver_get_queue_num(dev->ifname, &queue_num);\n\n\tctx->msg.payload.u64 = (uint64_t)queue_num;\n\tctx->msg.size = sizeof(ctx->msg.payload.u64);\n\tctx->fd_num = 0;\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n}\n\n/*\n * We receive the negotiated features supported by us and the virtio device.\n */\nstatic int\nvhost_user_set_features(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tuint64_t features = ctx->msg.payload.u64;\n\tuint64_t vhost_features = 0;\n\tstruct rte_vdpa_device *vdpa_dev;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\trte_vhost_driver_get_features(dev->ifname, &vhost_features);\n\tif (features & ~vhost_features) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) received invalid negotiated features.\\n\",\n\t\t\tdev->ifname);\n\t\tdev->flags |= VIRTIO_DEV_FEATURES_FAILED;\n\t\tdev->status &= ~VIRTIO_DEVICE_STATUS_FEATURES_OK;\n\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tif (dev->flags & VIRTIO_DEV_RUNNING) {\n\t\tif (dev->features == features)\n\t\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\n\t\t/*\n\t\t * Error out if master tries to change features while device is\n\t\t * in running state. The exception being VHOST_F_LOG_ALL, which\n\t\t * is enabled when the live-migration starts.\n\t\t */\n\t\tif ((dev->features ^ features) & ~(1ULL << VHOST_F_LOG_ALL)) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) features changed while device is running.\\n\",\n\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\n\t\tif (dev->notify_ops->features_changed)\n\t\t\tdev->notify_ops->features_changed(dev->vid, features);\n\t}\n\n\tdev->features = features;\n\tif (dev->features &\n\t\t((1ULL << VIRTIO_NET_F_MRG_RXBUF) |\n\t\t (1ULL << VIRTIO_F_VERSION_1) |\n\t\t (1ULL << VIRTIO_F_RING_PACKED))) {\n\t\tdev->vhost_hlen = sizeof(struct virtio_net_hdr_mrg_rxbuf);\n\t} else {\n\t\tdev->vhost_hlen = sizeof(struct virtio_net_hdr);\n\t}\n\tVHOST_LOG_CONFIG(INFO, \"(%s) negotiated Virtio features: 0x%\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, dev->features);\n\tVHOST_LOG_CONFIG(DEBUG, \"(%s) mergeable RX buffers %s, virtio 1 %s\\n\",\n\t\tdev->ifname,\n\t\t(dev->features & (1 << VIRTIO_NET_F_MRG_RXBUF)) ? \"on\" : \"off\",\n\t\t(dev->features & (1ULL << VIRTIO_F_VERSION_1)) ? \"on\" : \"off\");\n\n\tif ((dev->flags & VIRTIO_DEV_BUILTIN_VIRTIO_NET) &&\n\t    !(dev->features & (1ULL << VIRTIO_NET_F_MQ))) {\n\t\t/*\n\t\t * Remove all but first queue pair if MQ hasn't been\n\t\t * negotiated. This is safe because the device is not\n\t\t * running at this stage.\n\t\t */\n\t\twhile (dev->nr_vring > 2) {\n\t\t\tstruct vhost_virtqueue *vq;\n\n\t\t\tvq = dev->virtqueue[--dev->nr_vring];\n\t\t\tif (!vq)\n\t\t\t\tcontinue;\n\n\t\t\tdev->virtqueue[dev->nr_vring] = NULL;\n\t\t\tcleanup_vq(vq, 1);\n\t\t\tcleanup_vq_inflight(dev, vq);\n\t\t\tfree_vq(dev, vq);\n\t\t}\n\t}\n\n\tvdpa_dev = dev->vdpa_dev;\n\tif (vdpa_dev)\n\t\tvdpa_dev->ops->set_features(dev->vid);\n\n\tdev->flags &= ~VIRTIO_DEV_FEATURES_FAILED;\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\n/*\n * The virtio device sends us the size of the descriptor ring.\n */\nstatic int\nvhost_user_set_vring_num(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tstruct vhost_virtqueue *vq = dev->virtqueue[ctx->msg.payload.state.index];\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (ctx->msg.payload.state.num > 32768) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid virtqueue size %u\\n\",\n\t\t\t\tdev->ifname, ctx->msg.payload.state.num);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tvq->size = ctx->msg.payload.state.num;\n\n\t/* VIRTIO 1.0, 2.4 Virtqueues says:\n\t *\n\t *   Queue Size value is always a power of 2. The maximum Queue Size\n\t *   value is 32768.\n\t *\n\t * VIRTIO 1.1 2.7 Virtqueues says:\n\t *\n\t *   Packed virtqueues support up to 2^15 entries each.\n\t */\n\tif (!vq_is_packed(dev)) {\n\t\tif (vq->size & (vq->size - 1)) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid virtqueue size %u\\n\",\n\t\t\t\t\tdev->ifname, vq->size);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\t}\n\n\tif (vq_is_packed(dev)) {\n\t\trte_free(vq->shadow_used_packed);\n\t\tvq->shadow_used_packed = rte_malloc_socket(NULL,\n\t\t\t\tvq->size *\n\t\t\t\tsizeof(struct vring_used_elem_packed),\n\t\t\t\tRTE_CACHE_LINE_SIZE, vq->numa_node);\n\t\tif (!vq->shadow_used_packed) {\n\t\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\"(%s) failed to allocate memory for shadow used ring.\\n\",\n\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\n\t} else {\n\t\trte_free(vq->shadow_used_split);\n\n\t\tvq->shadow_used_split = rte_malloc_socket(NULL,\n\t\t\t\tvq->size * sizeof(struct vring_used_elem),\n\t\t\t\tRTE_CACHE_LINE_SIZE, vq->numa_node);\n\n\t\tif (!vq->shadow_used_split) {\n\t\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\"(%s) failed to allocate memory for vq internal data.\\n\",\n\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\t}\n\n\trte_free(vq->batch_copy_elems);\n\tvq->batch_copy_elems = rte_malloc_socket(NULL,\n\t\t\t\tvq->size * sizeof(struct batch_copy_elem),\n\t\t\t\tRTE_CACHE_LINE_SIZE, vq->numa_node);\n\tif (!vq->batch_copy_elems) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to allocate memory for batching copy.\\n\",\n\t\t\tdev->ifname);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\n/*\n * Reallocate virtio_dev, vhost_virtqueue and related data structures to\n * make them on the same numa node as the memory of vring descriptor.\n */\n#ifdef RTE_LIBRTE_VHOST_NUMA\nstatic struct virtio_net*\nnuma_realloc(struct virtio_net *dev, int index)\n{\n\tint node, dev_node;\n\tstruct virtio_net *old_dev;\n\tstruct vhost_virtqueue *vq;\n\tstruct batch_copy_elem *bce;\n\tstruct guest_page *gp;\n\tstruct rte_vhost_memory *mem;\n\tsize_t mem_size;\n\tint ret;\n\n\told_dev = dev;\n\tvq = dev->virtqueue[index];\n\n\t/*\n\t * If VQ is ready, it is too late to reallocate, it certainly already\n\t * happened anyway on VHOST_USER_SET_VRING_ADRR.\n\t */\n\tif (vq->ready)\n\t\treturn dev;\n\n\tret = get_mempolicy(&node, NULL, 0, vq->desc, MPOL_F_NODE | MPOL_F_ADDR);\n\tif (ret) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) unable to get virtqueue %d numa information.\\n\",\n\t\t\t\tdev->ifname, index);\n\t\treturn dev;\n\t}\n\n\tif (node == vq->numa_node)\n\t\tgoto out_dev_realloc;\n\n\tvq = rte_realloc_socket(vq, sizeof(*vq), 0, node);\n\tif (!vq) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc virtqueue %d on node %d\\n\",\n\t\t\t\tdev->ifname, index, node);\n\t\treturn dev;\n\t}\n\n\tif (vq != dev->virtqueue[index]) {\n\t\tVHOST_LOG_CONFIG(INFO, \"(%s) reallocated virtqueue on node %d\\n\",\n\t\t\t\tdev->ifname, node);\n\t\tdev->virtqueue[index] = vq;\n\t\tvhost_user_iotlb_init(dev, index);\n\t}\n\n\tif (vq_is_packed(dev)) {\n\t\tstruct vring_used_elem_packed *sup;\n\n\t\tsup = rte_realloc_socket(vq->shadow_used_packed, vq->size * sizeof(*sup),\n\t\t\t\tRTE_CACHE_LINE_SIZE, node);\n\t\tif (!sup) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc shadow packed on node %d\\n\",\n\t\t\t\t\tdev->ifname, node);\n\t\t\treturn dev;\n\t\t}\n\t\tvq->shadow_used_packed = sup;\n\t} else {\n\t\tstruct vring_used_elem *sus;\n\n\t\tsus = rte_realloc_socket(vq->shadow_used_split, vq->size * sizeof(*sus),\n\t\t\t\tRTE_CACHE_LINE_SIZE, node);\n\t\tif (!sus) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc shadow split on node %d\\n\",\n\t\t\t\t\tdev->ifname, node);\n\t\t\treturn dev;\n\t\t}\n\t\tvq->shadow_used_split = sus;\n\t}\n\n\tbce = rte_realloc_socket(vq->batch_copy_elems, vq->size * sizeof(*bce),\n\t\t\tRTE_CACHE_LINE_SIZE, node);\n\tif (!bce) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc batch copy elem on node %d\\n\",\n\t\t\t\tdev->ifname, node);\n\t\treturn dev;\n\t}\n\tvq->batch_copy_elems = bce;\n\n\tif (vq->log_cache) {\n\t\tstruct log_cache_entry *lc;\n\n\t\tlc = rte_realloc_socket(vq->log_cache, sizeof(*lc) * VHOST_LOG_CACHE_NR, 0, node);\n\t\tif (!lc) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc log cache on node %d\\n\",\n\t\t\t\t\tdev->ifname, node);\n\t\t\treturn dev;\n\t\t}\n\t\tvq->log_cache = lc;\n\t}\n\n\tif (vq->resubmit_inflight) {\n\t\tstruct rte_vhost_resubmit_info *ri;\n\n\t\tri = rte_realloc_socket(vq->resubmit_inflight, sizeof(*ri), 0, node);\n\t\tif (!ri) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc resubmit inflight on node %d\\n\",\n\t\t\t\t\tdev->ifname, node);\n\t\t\treturn dev;\n\t\t}\n\t\tvq->resubmit_inflight = ri;\n\n\t\tif (ri->resubmit_list) {\n\t\t\tstruct rte_vhost_resubmit_desc *rd;\n\n\t\t\trd = rte_realloc_socket(ri->resubmit_list, sizeof(*rd) * ri->resubmit_num,\n\t\t\t\t\t0, node);\n\t\t\tif (!rd) {\n\t\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc resubmit list on node %d\\n\",\n\t\t\t\t\t\tdev->ifname, node);\n\t\t\t\treturn dev;\n\t\t\t}\n\t\t\tri->resubmit_list = rd;\n\t\t}\n\t}\n\n\tvq->numa_node = node;\n\nout_dev_realloc:\n\n\tif (dev->flags & VIRTIO_DEV_RUNNING)\n\t\treturn dev;\n\n\tret = get_mempolicy(&dev_node, NULL, 0, dev, MPOL_F_NODE | MPOL_F_ADDR);\n\tif (ret) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) unable to get numa information.\\n\", dev->ifname);\n\t\treturn dev;\n\t}\n\n\tif (dev_node == node)\n\t\treturn dev;\n\n\tdev = rte_realloc_socket(old_dev, sizeof(*dev), 0, node);\n\tif (!dev) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc dev on node %d\\n\",\n\t\t\t\told_dev->ifname, node);\n\t\treturn old_dev;\n\t}\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) reallocated device on node %d\\n\", dev->ifname, node);\n\tvhost_devices[dev->vid] = dev;\n\n\tmem_size = sizeof(struct rte_vhost_memory) +\n\t\tsizeof(struct rte_vhost_mem_region) * dev->mem->nregions;\n\tmem = rte_realloc_socket(dev->mem, mem_size, 0, node);\n\tif (!mem) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc mem table on node %d\\n\",\n\t\t\t\tdev->ifname, node);\n\t\treturn dev;\n\t}\n\tdev->mem = mem;\n\n\tgp = rte_realloc_socket(dev->guest_pages, dev->max_guest_pages * sizeof(*gp),\n\t\t\tRTE_CACHE_LINE_SIZE, node);\n\tif (!gp) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc guest pages on node %d\\n\",\n\t\t\t\tdev->ifname, node);\n\t\treturn dev;\n\t}\n\tdev->guest_pages = gp;\n\n\treturn dev;\n}\n#else\nstatic struct virtio_net*\nnuma_realloc(struct virtio_net *dev, int index __rte_unused)\n{\n\treturn dev;\n}\n#endif\n\n/* Converts QEMU virtual address to Vhost virtual address. */\nstatic uint64_t\nqva_to_vva(struct virtio_net *dev, uint64_t qva, uint64_t *len)\n{\n\tstruct rte_vhost_mem_region *r;\n\tuint32_t i;\n\n\tif (unlikely(!dev || !dev->mem))\n\t\tgoto out_error;\n\n\t/* Find the region where the address lives. */\n\tfor (i = 0; i < dev->mem->nregions; i++) {\n\t\tr = &dev->mem->regions[i];\n\n\t\tif (qva >= r->guest_user_addr &&\n\t\t    qva <  r->guest_user_addr + r->size) {\n\n\t\t\tif (unlikely(*len > r->guest_user_addr + r->size - qva))\n\t\t\t\t*len = r->guest_user_addr + r->size - qva;\n\n\t\t\treturn qva - r->guest_user_addr +\n\t\t\t       r->host_user_addr;\n\t\t}\n\t}\nout_error:\n\t*len = 0;\n\n\treturn 0;\n}\n\n\n/*\n * Converts ring address to Vhost virtual address.\n * If IOMMU is enabled, the ring address is a guest IO virtual address,\n * else it is a QEMU virtual address.\n */\nstatic uint64_t\nring_addr_to_vva(struct virtio_net *dev, struct vhost_virtqueue *vq,\n\t\tuint64_t ra, uint64_t *size)\n{\n\tif (dev->features & (1ULL << VIRTIO_F_IOMMU_PLATFORM)) {\n\t\tuint64_t vva;\n\n\t\tvhost_user_iotlb_rd_lock(vq);\n\t\tvva = vhost_iova_to_vva(dev, vq, ra,\n\t\t\t\t\tsize, VHOST_ACCESS_RW);\n\t\tvhost_user_iotlb_rd_unlock(vq);\n\n\t\treturn vva;\n\t}\n\n\treturn qva_to_vva(dev, ra, size);\n}\n\nstatic uint64_t\nlog_addr_to_gpa(struct virtio_net *dev, struct vhost_virtqueue *vq)\n{\n\tuint64_t log_gpa;\n\n\tvhost_user_iotlb_rd_lock(vq);\n\tlog_gpa = translate_log_addr(dev, vq, vq->ring_addrs.log_guest_addr);\n\tvhost_user_iotlb_rd_unlock(vq);\n\n\treturn log_gpa;\n}\n\nstatic struct virtio_net *\ntranslate_ring_addresses(struct virtio_net *dev, int vq_index)\n{\n\tstruct vhost_virtqueue *vq = dev->virtqueue[vq_index];\n\tstruct vhost_vring_addr *addr = &vq->ring_addrs;\n\tuint64_t len, expected_len;\n\n\tif (addr->flags & (1 << VHOST_VRING_F_LOG)) {\n\t\tvq->log_guest_addr =\n\t\t\tlog_addr_to_gpa(dev, vq);\n\t\tif (vq->log_guest_addr == 0) {\n\t\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) failed to map log_guest_addr.\\n\",\n\t\t\t\tdev->ifname);\n\t\t\treturn dev;\n\t\t}\n\t}\n\n\tif (vq_is_packed(dev)) {\n\t\tlen = sizeof(struct vring_packed_desc) * vq->size;\n\t\tvq->desc_packed = (struct vring_packed_desc *)(uintptr_t)\n\t\t\tring_addr_to_vva(dev, vq, addr->desc_user_addr, &len);\n\t\tif (vq->desc_packed == NULL ||\n\t\t\t\tlen != sizeof(struct vring_packed_desc) *\n\t\t\t\tvq->size) {\n\t\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) failed to map desc_packed ring.\\n\",\n\t\t\t\tdev->ifname);\n\t\t\treturn dev;\n\t\t}\n\n\t\tdev = numa_realloc(dev, vq_index);\n\t\tvq = dev->virtqueue[vq_index];\n\t\taddr = &vq->ring_addrs;\n\n\t\tlen = sizeof(struct vring_packed_desc_event);\n\t\tvq->driver_event = (struct vring_packed_desc_event *)\n\t\t\t\t\t(uintptr_t)ring_addr_to_vva(dev,\n\t\t\t\t\tvq, addr->avail_user_addr, &len);\n\t\tif (vq->driver_event == NULL ||\n\t\t\t\tlen != sizeof(struct vring_packed_desc_event)) {\n\t\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) failed to find driver area address.\\n\",\n\t\t\t\tdev->ifname);\n\t\t\treturn dev;\n\t\t}\n\n\t\tlen = sizeof(struct vring_packed_desc_event);\n\t\tvq->device_event = (struct vring_packed_desc_event *)\n\t\t\t\t\t(uintptr_t)ring_addr_to_vva(dev,\n\t\t\t\t\tvq, addr->used_user_addr, &len);\n\t\tif (vq->device_event == NULL ||\n\t\t\t\tlen != sizeof(struct vring_packed_desc_event)) {\n\t\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) failed to find device area address.\\n\",\n\t\t\t\tdev->ifname);\n\t\t\treturn dev;\n\t\t}\n\n\t\tvq->access_ok = true;\n\t\treturn dev;\n\t}\n\n\t/* The addresses are converted from QEMU virtual to Vhost virtual. */\n\tif (vq->desc && vq->avail && vq->used)\n\t\treturn dev;\n\n\tlen = sizeof(struct vring_desc) * vq->size;\n\tvq->desc = (struct vring_desc *)(uintptr_t)ring_addr_to_vva(dev,\n\t\t\tvq, addr->desc_user_addr, &len);\n\tif (vq->desc == 0 || len != sizeof(struct vring_desc) * vq->size) {\n\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) failed to map desc ring.\\n\", dev->ifname);\n\t\treturn dev;\n\t}\n\n\tdev = numa_realloc(dev, vq_index);\n\tvq = dev->virtqueue[vq_index];\n\taddr = &vq->ring_addrs;\n\n\tlen = sizeof(struct vring_avail) + sizeof(uint16_t) * vq->size;\n\tif (dev->features & (1ULL << VIRTIO_RING_F_EVENT_IDX))\n\t\tlen += sizeof(uint16_t);\n\texpected_len = len;\n\tvq->avail = (struct vring_avail *)(uintptr_t)ring_addr_to_vva(dev,\n\t\t\tvq, addr->avail_user_addr, &len);\n\tif (vq->avail == 0 || len != expected_len) {\n\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) failed to map avail ring.\\n\", dev->ifname);\n\t\treturn dev;\n\t}\n\n\tlen = sizeof(struct vring_used) +\n\t\tsizeof(struct vring_used_elem) * vq->size;\n\tif (dev->features & (1ULL << VIRTIO_RING_F_EVENT_IDX))\n\t\tlen += sizeof(uint16_t);\n\texpected_len = len;\n\tvq->used = (struct vring_used *)(uintptr_t)ring_addr_to_vva(dev,\n\t\t\tvq, addr->used_user_addr, &len);\n\tif (vq->used == 0 || len != expected_len) {\n\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) failed to map used ring.\\n\", dev->ifname);\n\t\treturn dev;\n\t}\n\n\tif (vq->last_used_idx != vq->used->idx) {\n\t\tVHOST_LOG_CONFIG(WARNING, \"(%s) last_used_idx (%u) and vq->used->idx (%u) mismatches;\\n\",\n\t\t\tdev->ifname,\n\t\t\tvq->last_used_idx, vq->used->idx);\n\t\tvq->last_used_idx  = vq->used->idx;\n\t\tvq->last_avail_idx = vq->used->idx;\n\t\tVHOST_LOG_CONFIG(WARNING, \"(%s) some packets maybe resent for Tx and dropped for Rx\\n\",\n\t\t\tdev->ifname);\n\t}\n\n\tvq->access_ok = true;\n\n\tVHOST_LOG_CONFIG(DEBUG, \"(%s) mapped address desc: %p\\n\", dev->ifname, vq->desc);\n\tVHOST_LOG_CONFIG(DEBUG, \"(%s) mapped address avail: %p\\n\", dev->ifname, vq->avail);\n\tVHOST_LOG_CONFIG(DEBUG, \"(%s) mapped address used: %p\\n\", dev->ifname, vq->used);\n\tVHOST_LOG_CONFIG(DEBUG, \"(%s) log_guest_addr: %\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, vq->log_guest_addr);\n\n\treturn dev;\n}\n\n/*\n * The virtio device sends us the desc, used and avail ring addresses.\n * This function then converts these to our address space.\n */\nstatic int\nvhost_user_set_vring_addr(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tstruct vhost_virtqueue *vq;\n\tstruct vhost_vring_addr *addr = &ctx->msg.payload.addr;\n\tbool access_ok;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (dev->mem == NULL)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\t/* addr->index refers to the queue index. The txq 1, rxq is 0. */\n\tvq = dev->virtqueue[ctx->msg.payload.addr.index];\n\n\taccess_ok = vq->access_ok;\n\n\t/*\n\t * Rings addresses should not be interpreted as long as the ring is not\n\t * started and enabled\n\t */\n\tmemcpy(&vq->ring_addrs, addr, sizeof(*addr));\n\n\tvring_invalidate(dev, vq);\n\n\tif ((vq->enabled && (dev->features &\n\t\t\t\t(1ULL << VHOST_USER_F_PROTOCOL_FEATURES))) ||\n\t\t\taccess_ok) {\n\t\tdev = translate_ring_addresses(dev, ctx->msg.payload.addr.index);\n\t\tif (!dev)\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\t\t*pdev = dev;\n\t}\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\n/*\n * The virtio device sends us the available ring last used index.\n */\nstatic int\nvhost_user_set_vring_base(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tstruct vhost_virtqueue *vq = dev->virtqueue[ctx->msg.payload.state.index];\n\tuint64_t val = ctx->msg.payload.state.num;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (vq_is_packed(dev)) {\n\t\t/*\n\t\t * Bit[0:14]: avail index\n\t\t * Bit[15]: avail wrap counter\n\t\t */\n\t\tvq->last_avail_idx = val & 0x7fff;\n\t\tvq->avail_wrap_counter = !!(val & (0x1 << 15));\n\t\t/*\n\t\t * Set used index to same value as available one, as\n\t\t * their values should be the same since ring processing\n\t\t * was stopped at get time.\n\t\t */\n\t\tvq->last_used_idx = vq->last_avail_idx;\n\t\tvq->used_wrap_counter = vq->avail_wrap_counter;\n\t} else {\n\t\tvq->last_used_idx = ctx->msg.payload.state.num;\n\t\tvq->last_avail_idx = ctx->msg.payload.state.num;\n\t}\n\n\tVHOST_LOG_CONFIG(INFO,\n\t\t\"(%s) vring base idx:%u last_used_idx:%u last_avail_idx:%u.\\n\",\n\t\tdev->ifname, ctx->msg.payload.state.index, vq->last_used_idx,\n\t\tvq->last_avail_idx);\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nadd_one_guest_page(struct virtio_net *dev, uint64_t guest_phys_addr,\n\t\t   uint64_t host_iova, uint64_t host_user_addr, uint64_t size)\n{\n\tstruct guest_page *page, *last_page;\n\tstruct guest_page *old_pages;\n\n\tif (dev->nr_guest_pages == dev->max_guest_pages) {\n\t\tdev->max_guest_pages *= 2;\n\t\told_pages = dev->guest_pages;\n\t\tdev->guest_pages = rte_realloc(dev->guest_pages,\n\t\t\t\t\tdev->max_guest_pages * sizeof(*page),\n\t\t\t\t\tRTE_CACHE_LINE_SIZE);\n\t\tif (dev->guest_pages == NULL) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"cannot realloc guest_pages\\n\");\n\t\t\trte_free(old_pages);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tif (dev->nr_guest_pages > 0) {\n\t\tlast_page = &dev->guest_pages[dev->nr_guest_pages - 1];\n\t\t/* merge if the two pages are continuous */\n\t\tif (host_iova == last_page->host_iova + last_page->size &&\n\t\t    guest_phys_addr == last_page->guest_phys_addr + last_page->size &&\n\t\t    host_user_addr == last_page->host_user_addr + last_page->size) {\n\t\t\tlast_page->size += size;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tpage = &dev->guest_pages[dev->nr_guest_pages++];\n\tpage->guest_phys_addr = guest_phys_addr;\n\tpage->host_iova  = host_iova;\n\tpage->host_user_addr = host_user_addr;\n\tpage->size = size;\n\n\treturn 0;\n}\n\nstatic int\nadd_guest_pages(struct virtio_net *dev, struct rte_vhost_mem_region *reg,\n\t\tuint64_t page_size)\n{\n\tuint64_t reg_size = reg->size;\n\tuint64_t host_user_addr  = reg->host_user_addr;\n\tuint64_t guest_phys_addr = reg->guest_phys_addr;\n\tuint64_t host_iova;\n\tuint64_t size;\n\n\thost_iova = rte_mem_virt2iova((void *)(uintptr_t)host_user_addr);\n\tsize = page_size - (guest_phys_addr & (page_size - 1));\n\tsize = RTE_MIN(size, reg_size);\n\n\tif (add_one_guest_page(dev, guest_phys_addr, host_iova,\n\t\t\t       host_user_addr, size) < 0)\n\t\treturn -1;\n\n\thost_user_addr  += size;\n\tguest_phys_addr += size;\n\treg_size -= size;\n\n\twhile (reg_size > 0) {\n\t\tsize = RTE_MIN(reg_size, page_size);\n\t\thost_iova = rte_mem_virt2iova((void *)(uintptr_t)\n\t\t\t\t\t\t  host_user_addr);\n\t\tif (add_one_guest_page(dev, guest_phys_addr, host_iova,\n\t\t\t\t       host_user_addr, size) < 0)\n\t\t\treturn -1;\n\n\t\thost_user_addr  += size;\n\t\tguest_phys_addr += size;\n\t\treg_size -= size;\n\t}\n\n\t/* sort guest page array if over binary search threshold */\n\tif (dev->nr_guest_pages >= VHOST_BINARY_SEARCH_THRESH) {\n\t\tqsort((void *)dev->guest_pages, dev->nr_guest_pages,\n\t\t\tsizeof(struct guest_page), guest_page_addrcmp);\n\t}\n\n\treturn 0;\n}\n\n#ifdef RTE_LIBRTE_VHOST_DEBUG\n/* TODO: enable it only in debug mode? */\nstatic void\ndump_guest_pages(struct virtio_net *dev)\n{\n\tuint32_t i;\n\tstruct guest_page *page;\n\n\tfor (i = 0; i < dev->nr_guest_pages; i++) {\n\t\tpage = &dev->guest_pages[i];\n\n\t\tVHOST_LOG_CONFIG(INFO, \"(%s) guest physical page region %u\\n\",\n\t\t\t\tdev->ifname, i);\n\t\tVHOST_LOG_CONFIG(INFO, \"(%s)\\tguest_phys_addr: %\" PRIx64 \"\\n\",\n\t\t\t\tdev->ifname, page->guest_phys_addr);\n\t\tVHOST_LOG_CONFIG(INFO, \"(%s)\\thost_iova : %\" PRIx64 \"\\n\",\n\t\t\t\tdev->ifname, page->host_iova);\n\t\tVHOST_LOG_CONFIG(INFO, \"(%s)\\tsize           : %\" PRIx64 \"\\n\",\n\t\t\t\tdev->ifname, page->size);\n\t}\n}\n#else\n#define dump_guest_pages(dev)\n#endif\n\nstatic bool\nvhost_memory_changed(struct VhostUserMemory *new,\n\t\t     struct rte_vhost_memory *old)\n{\n\tuint32_t i;\n\n\tif (new->nregions != old->nregions)\n\t\treturn true;\n\n\tfor (i = 0; i < new->nregions; ++i) {\n\t\tVhostUserMemoryRegion *new_r = &new->regions[i];\n\t\tstruct rte_vhost_mem_region *old_r = &old->regions[i];\n\n\t\tif (new_r->guest_phys_addr != old_r->guest_phys_addr)\n\t\t\treturn true;\n\t\tif (new_r->memory_size != old_r->size)\n\t\t\treturn true;\n\t\tif (new_r->userspace_addr != old_r->guest_user_addr)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n#ifdef RTE_LIBRTE_VHOST_POSTCOPY\nstatic int\nvhost_user_postcopy_region_register(struct virtio_net *dev,\n\t\tstruct rte_vhost_mem_region *reg)\n{\n\tstruct uffdio_register reg_struct;\n\n\t/*\n\t * Let's register all the mmapped area to ensure\n\t * alignment on page boundary.\n\t */\n\treg_struct.range.start = (uint64_t)(uintptr_t)reg->mmap_addr;\n\treg_struct.range.len = reg->mmap_size;\n\treg_struct.mode = UFFDIO_REGISTER_MODE_MISSING;\n\n\tif (ioctl(dev->postcopy_ufd, UFFDIO_REGISTER,\n\t\t\t\t&reg_struct)) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to register ufd for region \"\n\t\t\t\t\"%\" PRIx64 \" - %\" PRIx64 \" (ufd = %d) %s\\n\",\n\t\t\t\tdev->ifname,\n\t\t\t\t(uint64_t)reg_struct.range.start,\n\t\t\t\t(uint64_t)reg_struct.range.start +\n\t\t\t\t(uint64_t)reg_struct.range.len - 1,\n\t\t\t\tdev->postcopy_ufd,\n\t\t\t\tstrerror(errno));\n\t\treturn -1;\n\t}\n\n\tVHOST_LOG_CONFIG(INFO,\n\t\t\t\"(%s)\\t userfaultfd registered for range : %\" PRIx64 \" - %\" PRIx64 \"\\n\",\n\t\t\tdev->ifname,\n\t\t\t(uint64_t)reg_struct.range.start,\n\t\t\t(uint64_t)reg_struct.range.start +\n\t\t\t(uint64_t)reg_struct.range.len - 1);\n\n\treturn 0;\n}\n#else\nstatic int\nvhost_user_postcopy_region_register(struct virtio_net *dev __rte_unused,\n\t\tstruct rte_vhost_mem_region *reg __rte_unused)\n{\n\treturn -1;\n}\n#endif\n\nstatic int\nvhost_user_postcopy_register(struct virtio_net *dev, int main_fd,\n\t\tstruct vhu_msg_context *ctx)\n{\n\tstruct VhostUserMemory *memory;\n\tstruct rte_vhost_mem_region *reg;\n\tstruct vhu_msg_context ack_ctx;\n\tuint32_t i;\n\n\tif (!dev->postcopy_listening)\n\t\treturn 0;\n\n\t/*\n\t * We haven't a better way right now than sharing\n\t * DPDK's virtual address with Qemu, so that Qemu can\n\t * retrieve the region offset when handling userfaults.\n\t */\n\tmemory = &ctx->msg.payload.memory;\n\tfor (i = 0; i < memory->nregions; i++) {\n\t\treg = &dev->mem->regions[i];\n\t\tmemory->regions[i].userspace_addr = reg->host_user_addr;\n\t}\n\n\t/* Send the addresses back to qemu */\n\tctx->fd_num = 0;\n\tsend_vhost_reply(dev, main_fd, ctx);\n\n\t/* Wait for qemu to acknowledge it got the addresses\n\t * we've got to wait before we're allowed to generate faults.\n\t */\n\tif (read_vhost_message(dev, main_fd, &ack_ctx) <= 0) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to read qemu ack on postcopy set-mem-table\\n\",\n\t\t\t\tdev->ifname);\n\t\treturn -1;\n\t}\n\n\tif (validate_msg_fds(dev, &ack_ctx, 0) != 0)\n\t\treturn -1;\n\n\tif (ack_ctx.msg.request.master != VHOST_USER_SET_MEM_TABLE) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) bad qemu ack on postcopy set-mem-table (%d)\\n\",\n\t\t\t\tdev->ifname, ack_ctx.msg.request.master);\n\t\treturn -1;\n\t}\n\n\t/* Now userfault register and we can use the memory */\n\tfor (i = 0; i < memory->nregions; i++) {\n\t\treg = &dev->mem->regions[i];\n\t\tif (vhost_user_postcopy_region_register(dev, reg) < 0)\n\t\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nvhost_user_mmap_region(struct virtio_net *dev,\n\t\tstruct rte_vhost_mem_region *region,\n\t\tuint64_t mmap_offset)\n{\n\tvoid *mmap_addr;\n\tuint64_t mmap_size;\n\tuint64_t alignment;\n\tint populate;\n\n\t/* Check for memory_size + mmap_offset overflow */\n\tif (mmap_offset >= -region->size) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) mmap_offset (%#\"PRIx64\") and memory_size (%#\"PRIx64\") overflow\\n\",\n\t\t\t\tdev->ifname, mmap_offset, region->size);\n\t\treturn -1;\n\t}\n\n\tmmap_size = region->size + mmap_offset;\n\n\t/* mmap() without flag of MAP_ANONYMOUS, should be called with length\n\t * argument aligned with hugepagesz at older longterm version Linux,\n\t * like 2.6.32 and 3.2.72, or mmap() will fail with EINVAL.\n\t *\n\t * To avoid failure, make sure in caller to keep length aligned.\n\t */\n\talignment = get_blk_size(region->fd);\n\tif (alignment == (uint64_t)-1) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) couldn't get hugepage size through fstat\\n\",\n\t\t\t\tdev->ifname);\n\t\treturn -1;\n\t}\n\tmmap_size = RTE_ALIGN_CEIL(mmap_size, alignment);\n\tif (mmap_size == 0) {\n\t\t/*\n\t\t * It could happen if initial mmap_size + alignment overflows\n\t\t * the sizeof uint64, which could happen if either mmap_size or\n\t\t * alignment value is wrong.\n\t\t *\n\t\t * mmap() kernel implementation would return an error, but\n\t\t * better catch it before and provide useful info in the logs.\n\t\t */\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) mmap size (0x%\" PRIx64 \") or alignment (0x%\" PRIx64 \") is invalid\\n\",\n\t\t\t\tdev->ifname, region->size + mmap_offset, alignment);\n\t\treturn -1;\n\t}\n\n\tpopulate = dev->async_copy ? MAP_POPULATE : 0;\n\tmmap_addr = mmap(NULL, mmap_size, PROT_READ | PROT_WRITE,\n\t\t\tMAP_SHARED | populate, region->fd, 0);\n\n\tif (mmap_addr == MAP_FAILED) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) mmap failed (%s).\\n\", dev->ifname, strerror(errno));\n\t\treturn -1;\n\t}\n\n\tregion->mmap_addr = mmap_addr;\n\tregion->mmap_size = mmap_size;\n\tregion->host_user_addr = (uint64_t)(uintptr_t)mmap_addr + mmap_offset;\n\n\tif (dev->async_copy) {\n\t\tif (add_guest_pages(dev, region, alignment) < 0) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) adding guest pages to region failed.\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) guest memory region size: 0x%\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, region->size);\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t guest physical addr: 0x%\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, region->guest_phys_addr);\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t guest virtual  addr: 0x%\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, region->guest_user_addr);\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t host  virtual  addr: 0x%\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, region->host_user_addr);\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t mmap addr : 0x%\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, (uint64_t)(uintptr_t)mmap_addr);\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t mmap size : 0x%\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, mmap_size);\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t mmap align: 0x%\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, alignment);\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t mmap off  : 0x%\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, mmap_offset);\n\n\treturn 0;\n}\n\nstatic int\nvhost_user_set_mem_table(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd)\n{\n\tstruct virtio_net *dev = *pdev;\n\tstruct VhostUserMemory *memory = &ctx->msg.payload.memory;\n\tstruct rte_vhost_mem_region *reg;\n\tint numa_node = SOCKET_ID_ANY;\n\tuint64_t mmap_offset;\n\tuint32_t i;\n\tbool async_notify = false;\n\n\tif (validate_msg_fds(dev, ctx, memory->nregions) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (memory->nregions > VHOST_MEMORY_MAX_NREGIONS) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) too many memory regions (%u)\\n\",\n\t\t\t\tdev->ifname, memory->nregions);\n\t\tgoto close_msg_fds;\n\t}\n\n\tif (dev->mem && !vhost_memory_changed(memory, dev->mem)) {\n\t\tVHOST_LOG_CONFIG(INFO, \"(%s) memory regions not changed\\n\", dev->ifname);\n\n\t\tclose_msg_fds(ctx);\n\n\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\t}\n\n\tif (dev->mem) {\n\t\tif (dev->flags & VIRTIO_DEV_VDPA_CONFIGURED) {\n\t\t\tstruct rte_vdpa_device *vdpa_dev = dev->vdpa_dev;\n\n\t\t\tif (vdpa_dev && vdpa_dev->ops->dev_close)\n\t\t\t\tvdpa_dev->ops->dev_close(dev->vid);\n\t\t\tdev->flags &= ~VIRTIO_DEV_VDPA_CONFIGURED;\n\t\t}\n\n\t\t/* notify the vhost application to stop DMA transfers */\n\t\tif (dev->async_copy && dev->notify_ops->vring_state_changed) {\n\t\t\tfor (i = 0; i < dev->nr_vring; i++) {\n\t\t\t\tdev->notify_ops->vring_state_changed(dev->vid,\n\t\t\t\t\t\ti, 0);\n\t\t\t}\n\t\t\tasync_notify = true;\n\t\t}\n\n\t\tfree_mem_region(dev);\n\t\trte_free(dev->mem);\n\t\tdev->mem = NULL;\n\t}\n\n\t/* Flush IOTLB cache as previous HVAs are now invalid */\n\tif (dev->features & (1ULL << VIRTIO_F_IOMMU_PLATFORM))\n\t\tfor (i = 0; i < dev->nr_vring; i++)\n\t\t\tvhost_user_iotlb_flush_all(dev->virtqueue[i]);\n\n\t/*\n\t * If VQ 0 has already been allocated, try to allocate on the same\n\t * NUMA node. It can be reallocated later in numa_realloc().\n\t */\n\tif (dev->nr_vring > 0)\n\t\tnuma_node = dev->virtqueue[0]->numa_node;\n\n\tdev->nr_guest_pages = 0;\n\tif (dev->guest_pages == NULL) {\n\t\tdev->max_guest_pages = 8;\n\t\tdev->guest_pages = rte_zmalloc_socket(NULL,\n\t\t\t\t\tdev->max_guest_pages *\n\t\t\t\t\tsizeof(struct guest_page),\n\t\t\t\t\tRTE_CACHE_LINE_SIZE,\n\t\t\t\t\tnuma_node);\n\t\tif (dev->guest_pages == NULL) {\n\t\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\"(%s) failed to allocate memory for dev->guest_pages\\n\",\n\t\t\t\tdev->ifname);\n\t\t\tgoto close_msg_fds;\n\t\t}\n\t}\n\n\tdev->mem = rte_zmalloc_socket(\"vhost-mem-table\", sizeof(struct rte_vhost_memory) +\n\t\tsizeof(struct rte_vhost_mem_region) * memory->nregions, 0, numa_node);\n\tif (dev->mem == NULL) {\n\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\"(%s) failed to allocate memory for dev->mem\\n\",\n\t\t\tdev->ifname);\n\t\tgoto free_guest_pages;\n\t}\n\n\tfor (i = 0; i < memory->nregions; i++) {\n\t\treg = &dev->mem->regions[i];\n\n\t\treg->guest_phys_addr = memory->regions[i].guest_phys_addr;\n\t\treg->guest_user_addr = memory->regions[i].userspace_addr;\n\t\treg->size            = memory->regions[i].memory_size;\n\t\treg->fd              = ctx->fds[i];\n\n\t\t/*\n\t\t * Assign invalid file descriptor value to avoid double\n\t\t * closing on error path.\n\t\t */\n\t\tctx->fds[i] = -1;\n\n\t\tmmap_offset = memory->regions[i].mmap_offset;\n\n\t\tif (vhost_user_mmap_region(dev, reg, mmap_offset) < 0) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to mmap region %u\\n\", dev->ifname, i);\n\t\t\tgoto free_mem_table;\n\t\t}\n\n\t\tdev->mem->nregions++;\n\t}\n\n\tif (dev->async_copy && rte_vfio_is_enabled(\"vfio\"))\n\t\tasync_dma_map(dev, true);\n\n\tif (vhost_user_postcopy_register(dev, main_fd, ctx) < 0)\n\t\tgoto free_mem_table;\n\n\tfor (i = 0; i < dev->nr_vring; i++) {\n\t\tstruct vhost_virtqueue *vq = dev->virtqueue[i];\n\n\t\tif (!vq)\n\t\t\tcontinue;\n\n\t\tif (vq->desc || vq->avail || vq->used) {\n\t\t\t/*\n\t\t\t * If the memory table got updated, the ring addresses\n\t\t\t * need to be translated again as virtual addresses have\n\t\t\t * changed.\n\t\t\t */\n\t\t\tvring_invalidate(dev, vq);\n\n\t\t\tdev = translate_ring_addresses(dev, i);\n\t\t\tif (!dev) {\n\t\t\t\tdev = *pdev;\n\t\t\t\tgoto free_mem_table;\n\t\t\t}\n\n\t\t\t*pdev = dev;\n\t\t}\n\t}\n\n\tdump_guest_pages(dev);\n\n\tif (async_notify) {\n\t\tfor (i = 0; i < dev->nr_vring; i++)\n\t\t\tdev->notify_ops->vring_state_changed(dev->vid, i, 1);\n\t}\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n\nfree_mem_table:\n\tfree_mem_region(dev);\n\trte_free(dev->mem);\n\tdev->mem = NULL;\n\nfree_guest_pages:\n\trte_free(dev->guest_pages);\n\tdev->guest_pages = NULL;\nclose_msg_fds:\n\tclose_msg_fds(ctx);\n\treturn RTE_VHOST_MSG_RESULT_ERR;\n}\n\nstatic bool\nvq_is_ready(struct virtio_net *dev, struct vhost_virtqueue *vq)\n{\n\tbool rings_ok;\n\n\tif (!vq)\n\t\treturn false;\n\n\tif (vq_is_packed(dev))\n\t\trings_ok = vq->desc_packed && vq->driver_event &&\n\t\t\tvq->device_event;\n\telse\n\t\trings_ok = vq->desc && vq->avail && vq->used;\n\n\treturn rings_ok &&\n\t       vq->kickfd != VIRTIO_UNINITIALIZED_EVENTFD &&\n\t       vq->callfd != VIRTIO_UNINITIALIZED_EVENTFD &&\n\t       vq->enabled;\n}\n\n#define VIRTIO_BUILTIN_NUM_VQS_TO_BE_READY 2u\n\nstatic int\nvirtio_is_ready(struct virtio_net *dev)\n{\n\tstruct vhost_virtqueue *vq;\n\tuint32_t i, nr_vring = dev->nr_vring;\n\n\tif (dev->flags & VIRTIO_DEV_READY)\n\t\treturn 1;\n\n\tif (!dev->nr_vring)\n\t\treturn 0;\n\n\tif (dev->flags & VIRTIO_DEV_BUILTIN_VIRTIO_NET) {\n\t\tnr_vring = VIRTIO_BUILTIN_NUM_VQS_TO_BE_READY;\n\n\t\tif (dev->nr_vring < nr_vring)\n\t\t\treturn 0;\n\t}\n\n\tfor (i = 0; i < nr_vring; i++) {\n\t\tvq = dev->virtqueue[i];\n\n\t\tif (!vq_is_ready(dev, vq))\n\t\t\treturn 0;\n\t}\n\n\t/* If supported, ensure the frontend is really done with config */\n\tif (dev->protocol_features & (1ULL << VHOST_USER_PROTOCOL_F_STATUS))\n\t\tif (!(dev->status & VIRTIO_DEVICE_STATUS_DRIVER_OK))\n\t\t\treturn 0;\n\n\tdev->flags |= VIRTIO_DEV_READY;\n\n\tif (!(dev->flags & VIRTIO_DEV_RUNNING))\n\t\tVHOST_LOG_CONFIG(INFO, \"(%s) virtio is now ready for processing.\\n\", dev->ifname);\n\treturn 1;\n}\n\nstatic void *\ninflight_mem_alloc(struct virtio_net *dev, const char *name, size_t size, int *fd)\n{\n\tvoid *ptr;\n\tint mfd = -1;\n\tchar fname[20] = \"/tmp/memfd-XXXXXX\";\n\n\t*fd = -1;\n#ifdef MEMFD_SUPPORTED\n\tmfd = memfd_create(name, MFD_CLOEXEC);\n#else\n\tRTE_SET_USED(name);\n#endif\n\tif (mfd == -1) {\n\t\tmfd = mkstemp(fname);\n\t\tif (mfd == -1) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to get inflight buffer fd\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tunlink(fname);\n\t}\n\n\tif (ftruncate(mfd, size) == -1) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to alloc inflight buffer\\n\", dev->ifname);\n\t\tclose(mfd);\n\t\treturn NULL;\n\t}\n\n\tptr = mmap(0, size, PROT_READ | PROT_WRITE, MAP_SHARED, mfd, 0);\n\tif (ptr == MAP_FAILED) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to mmap inflight buffer\\n\", dev->ifname);\n\t\tclose(mfd);\n\t\treturn NULL;\n\t}\n\n\t*fd = mfd;\n\treturn ptr;\n}\n\nstatic uint32_t\nget_pervq_shm_size_split(uint16_t queue_size)\n{\n\treturn RTE_ALIGN_MUL_CEIL(sizeof(struct rte_vhost_inflight_desc_split) *\n\t\t\t\t  queue_size + sizeof(uint64_t) +\n\t\t\t\t  sizeof(uint16_t) * 4, INFLIGHT_ALIGNMENT);\n}\n\nstatic uint32_t\nget_pervq_shm_size_packed(uint16_t queue_size)\n{\n\treturn RTE_ALIGN_MUL_CEIL(sizeof(struct rte_vhost_inflight_desc_packed)\n\t\t\t\t  * queue_size + sizeof(uint64_t) +\n\t\t\t\t  sizeof(uint16_t) * 6 + sizeof(uint8_t) * 9,\n\t\t\t\t  INFLIGHT_ALIGNMENT);\n}\n\nstatic int\nvhost_user_get_inflight_fd(struct virtio_net **pdev,\n\t\t\t   struct vhu_msg_context *ctx,\n\t\t\t   int main_fd __rte_unused)\n{\n\tstruct rte_vhost_inflight_info_packed *inflight_packed;\n\tuint64_t pervq_inflight_size, mmap_size;\n\tuint16_t num_queues, queue_size;\n\tstruct virtio_net *dev = *pdev;\n\tint fd, i, j;\n\tint numa_node = SOCKET_ID_ANY;\n\tvoid *addr;\n\n\tif (ctx->msg.size != sizeof(ctx->msg.payload.inflight)) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid get_inflight_fd message size is %d\\n\",\n\t\t\tdev->ifname, ctx->msg.size);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\t/*\n\t * If VQ 0 has already been allocated, try to allocate on the same\n\t * NUMA node. It can be reallocated later in numa_realloc().\n\t */\n\tif (dev->nr_vring > 0)\n\t\tnuma_node = dev->virtqueue[0]->numa_node;\n\n\tif (dev->inflight_info == NULL) {\n\t\tdev->inflight_info = rte_zmalloc_socket(\"inflight_info\",\n\t\t\t\tsizeof(struct inflight_mem_info), 0, numa_node);\n\t\tif (!dev->inflight_info) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to alloc dev inflight area\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\t\tdev->inflight_info->fd = -1;\n\t}\n\n\tnum_queues = ctx->msg.payload.inflight.num_queues;\n\tqueue_size = ctx->msg.payload.inflight.queue_size;\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) get_inflight_fd num_queues: %u\\n\",\n\t\tdev->ifname, ctx->msg.payload.inflight.num_queues);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) get_inflight_fd queue_size: %u\\n\",\n\t\tdev->ifname, ctx->msg.payload.inflight.queue_size);\n\n\tif (vq_is_packed(dev))\n\t\tpervq_inflight_size = get_pervq_shm_size_packed(queue_size);\n\telse\n\t\tpervq_inflight_size = get_pervq_shm_size_split(queue_size);\n\n\tmmap_size = num_queues * pervq_inflight_size;\n\taddr = inflight_mem_alloc(dev, \"vhost-inflight\", mmap_size, &fd);\n\tif (!addr) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to alloc vhost inflight area\\n\", dev->ifname);\n\t\t\tctx->msg.payload.inflight.mmap_size = 0;\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\tmemset(addr, 0, mmap_size);\n\n\tif (dev->inflight_info->addr) {\n\t\tmunmap(dev->inflight_info->addr, dev->inflight_info->size);\n\t\tdev->inflight_info->addr = NULL;\n\t}\n\n\tif (dev->inflight_info->fd >= 0) {\n\t\tclose(dev->inflight_info->fd);\n\t\tdev->inflight_info->fd = -1;\n\t}\n\n\tdev->inflight_info->addr = addr;\n\tdev->inflight_info->size = ctx->msg.payload.inflight.mmap_size = mmap_size;\n\tdev->inflight_info->fd = ctx->fds[0] = fd;\n\tctx->msg.payload.inflight.mmap_offset = 0;\n\tctx->fd_num = 1;\n\n\tif (vq_is_packed(dev)) {\n\t\tfor (i = 0; i < num_queues; i++) {\n\t\t\tinflight_packed =\n\t\t\t\t(struct rte_vhost_inflight_info_packed *)addr;\n\t\t\tinflight_packed->used_wrap_counter = 1;\n\t\t\tinflight_packed->old_used_wrap_counter = 1;\n\t\t\tfor (j = 0; j < queue_size; j++)\n\t\t\t\tinflight_packed->desc[j].next = j + 1;\n\t\t\taddr = (void *)((char *)addr + pervq_inflight_size);\n\t\t}\n\t}\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) send inflight mmap_size: %\"PRIu64\"\\n\",\n\t\t\tdev->ifname, ctx->msg.payload.inflight.mmap_size);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) send inflight mmap_offset: %\"PRIu64\"\\n\",\n\t\t\tdev->ifname, ctx->msg.payload.inflight.mmap_offset);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) send inflight fd: %d\\n\", dev->ifname, ctx->fds[0]);\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n}\n\nstatic int\nvhost_user_set_inflight_fd(struct virtio_net **pdev,\n\t\t\t   struct vhu_msg_context *ctx,\n\t\t\t   int main_fd __rte_unused)\n{\n\tuint64_t mmap_size, mmap_offset;\n\tuint16_t num_queues, queue_size;\n\tstruct virtio_net *dev = *pdev;\n\tuint32_t pervq_inflight_size;\n\tstruct vhost_virtqueue *vq;\n\tvoid *addr;\n\tint fd, i;\n\tint numa_node = SOCKET_ID_ANY;\n\n\tfd = ctx->fds[0];\n\tif (ctx->msg.size != sizeof(ctx->msg.payload.inflight) || fd < 0) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid set_inflight_fd message size is %d,fd is %d\\n\",\n\t\t\tdev->ifname, ctx->msg.size, fd);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tmmap_size = ctx->msg.payload.inflight.mmap_size;\n\tmmap_offset = ctx->msg.payload.inflight.mmap_offset;\n\tnum_queues = ctx->msg.payload.inflight.num_queues;\n\tqueue_size = ctx->msg.payload.inflight.queue_size;\n\n\tif (vq_is_packed(dev))\n\t\tpervq_inflight_size = get_pervq_shm_size_packed(queue_size);\n\telse\n\t\tpervq_inflight_size = get_pervq_shm_size_split(queue_size);\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd mmap_size: %\"PRIu64\"\\n\",\n\t\t\tdev->ifname, mmap_size);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd mmap_offset: %\"PRIu64\"\\n\",\n\t\t\tdev->ifname, mmap_offset);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd num_queues: %u\\n\", dev->ifname, num_queues);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd queue_size: %u\\n\", dev->ifname, queue_size);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd fd: %d\\n\", dev->ifname, fd);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd pervq_inflight_size: %d\\n\",\n\t\t\tdev->ifname, pervq_inflight_size);\n\n\t/*\n\t * If VQ 0 has already been allocated, try to allocate on the same\n\t * NUMA node. It can be reallocated later in numa_realloc().\n\t */\n\tif (dev->nr_vring > 0)\n\t\tnuma_node = dev->virtqueue[0]->numa_node;\n\n\tif (!dev->inflight_info) {\n\t\tdev->inflight_info = rte_zmalloc_socket(\"inflight_info\",\n\t\t\t\tsizeof(struct inflight_mem_info), 0, numa_node);\n\t\tif (dev->inflight_info == NULL) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to alloc dev inflight area\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\t\tdev->inflight_info->fd = -1;\n\t}\n\n\tif (dev->inflight_info->addr) {\n\t\tmunmap(dev->inflight_info->addr, dev->inflight_info->size);\n\t\tdev->inflight_info->addr = NULL;\n\t}\n\n\taddr = mmap(0, mmap_size, PROT_READ | PROT_WRITE, MAP_SHARED,\n\t\t    fd, mmap_offset);\n\tif (addr == MAP_FAILED) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to mmap share memory.\\n\", dev->ifname);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tif (dev->inflight_info->fd >= 0) {\n\t\tclose(dev->inflight_info->fd);\n\t\tdev->inflight_info->fd = -1;\n\t}\n\n\tdev->inflight_info->fd = fd;\n\tdev->inflight_info->addr = addr;\n\tdev->inflight_info->size = mmap_size;\n\n\tfor (i = 0; i < num_queues; i++) {\n\t\tvq = dev->virtqueue[i];\n\t\tif (!vq)\n\t\t\tcontinue;\n\n\t\tif (vq_is_packed(dev)) {\n\t\t\tvq->inflight_packed = addr;\n\t\t\tvq->inflight_packed->desc_num = queue_size;\n\t\t} else {\n\t\t\tvq->inflight_split = addr;\n\t\t\tvq->inflight_split->desc_num = queue_size;\n\t\t}\n\t\taddr = (void *)((char *)addr + pervq_inflight_size);\n\t}\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_user_set_vring_call(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tstruct vhost_vring_file file;\n\tstruct vhost_virtqueue *vq;\n\tint expected_fds;\n\n\texpected_fds = (ctx->msg.payload.u64 & VHOST_USER_VRING_NOFD_MASK) ? 0 : 1;\n\tif (validate_msg_fds(dev, ctx, expected_fds) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tfile.index = ctx->msg.payload.u64 & VHOST_USER_VRING_IDX_MASK;\n\tif (ctx->msg.payload.u64 & VHOST_USER_VRING_NOFD_MASK)\n\t\tfile.fd = VIRTIO_INVALID_EVENTFD;\n\telse\n\t\tfile.fd = ctx->fds[0];\n\tVHOST_LOG_CONFIG(INFO, \"(%s) vring call idx:%d file:%d\\n\",\n\t\t\tdev->ifname, file.index, file.fd);\n\n\tvq = dev->virtqueue[file.index];\n\n\tif (vq->ready) {\n\t\tvq->ready = false;\n\t\tvhost_user_notify_queue_state(dev, file.index, 0);\n\t}\n\n\tif (vq->callfd >= 0)\n\t\tclose(vq->callfd);\n\n\tvq->callfd = file.fd;\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int vhost_user_set_vring_err(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tint expected_fds;\n\n\texpected_fds = (ctx->msg.payload.u64 & VHOST_USER_VRING_NOFD_MASK) ? 0 : 1;\n\tif (validate_msg_fds(dev, ctx, expected_fds) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (!(ctx->msg.payload.u64 & VHOST_USER_VRING_NOFD_MASK))\n\t\tclose(ctx->fds[0]);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) not implemented\\n\", dev->ifname);\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nresubmit_desc_compare(const void *a, const void *b)\n{\n\tconst struct rte_vhost_resubmit_desc *desc0 = a;\n\tconst struct rte_vhost_resubmit_desc *desc1 = b;\n\n\tif (desc1->counter > desc0->counter)\n\t\treturn 1;\n\n\treturn -1;\n}\n\nstatic int\nvhost_check_queue_inflights_split(struct virtio_net *dev,\n\t\t\t\t  struct vhost_virtqueue *vq)\n{\n\tuint16_t i;\n\tuint16_t resubmit_num = 0, last_io, num;\n\tstruct vring_used *used = vq->used;\n\tstruct rte_vhost_resubmit_info *resubmit;\n\tstruct rte_vhost_inflight_info_split *inflight_split;\n\n\tif (!(dev->protocol_features &\n\t    (1ULL << VHOST_USER_PROTOCOL_F_INFLIGHT_SHMFD)))\n\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\n\t/* The frontend may still not support the inflight feature\n\t * although we negotiate the protocol feature.\n\t */\n\tif ((!vq->inflight_split))\n\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\n\tif (!vq->inflight_split->version) {\n\t\tvq->inflight_split->version = INFLIGHT_VERSION;\n\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\t}\n\n\tif (vq->resubmit_inflight)\n\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\n\tinflight_split = vq->inflight_split;\n\tvq->global_counter = 0;\n\tlast_io = inflight_split->last_inflight_io;\n\n\tif (inflight_split->used_idx != used->idx) {\n\t\tinflight_split->desc[last_io].inflight = 0;\n\t\trte_atomic_thread_fence(__ATOMIC_SEQ_CST);\n\t\tinflight_split->used_idx = used->idx;\n\t}\n\n\tfor (i = 0; i < inflight_split->desc_num; i++) {\n\t\tif (inflight_split->desc[i].inflight == 1)\n\t\t\tresubmit_num++;\n\t}\n\n\tvq->last_avail_idx += resubmit_num;\n\n\tif (resubmit_num) {\n\t\tresubmit = rte_zmalloc_socket(\"resubmit\", sizeof(struct rte_vhost_resubmit_info),\n\t\t\t\t0, vq->numa_node);\n\t\tif (!resubmit) {\n\t\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\t\"(%s) failed to allocate memory for resubmit info.\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\n\t\tresubmit->resubmit_list = rte_zmalloc_socket(\"resubmit_list\",\n\t\t\t\tresubmit_num * sizeof(struct rte_vhost_resubmit_desc),\n\t\t\t\t0, vq->numa_node);\n\t\tif (!resubmit->resubmit_list) {\n\t\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\t\"(%s) failed to allocate memory for inflight desc.\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\trte_free(resubmit);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\n\t\tnum = 0;\n\t\tfor (i = 0; i < vq->inflight_split->desc_num; i++) {\n\t\t\tif (vq->inflight_split->desc[i].inflight == 1) {\n\t\t\t\tresubmit->resubmit_list[num].index = i;\n\t\t\t\tresubmit->resubmit_list[num].counter =\n\t\t\t\t\tinflight_split->desc[i].counter;\n\t\t\t\tnum++;\n\t\t\t}\n\t\t}\n\t\tresubmit->resubmit_num = num;\n\n\t\tif (resubmit->resubmit_num > 1)\n\t\t\tqsort(resubmit->resubmit_list, resubmit->resubmit_num,\n\t\t\t      sizeof(struct rte_vhost_resubmit_desc),\n\t\t\t      resubmit_desc_compare);\n\n\t\tvq->global_counter = resubmit->resubmit_list[0].counter + 1;\n\t\tvq->resubmit_inflight = resubmit;\n\t}\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_check_queue_inflights_packed(struct virtio_net *dev,\n\t\t\t\t   struct vhost_virtqueue *vq)\n{\n\tuint16_t i;\n\tuint16_t resubmit_num = 0, old_used_idx, num;\n\tstruct rte_vhost_resubmit_info *resubmit;\n\tstruct rte_vhost_inflight_info_packed *inflight_packed;\n\n\tif (!(dev->protocol_features &\n\t    (1ULL << VHOST_USER_PROTOCOL_F_INFLIGHT_SHMFD)))\n\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\n\t/* The frontend may still not support the inflight feature\n\t * although we negotiate the protocol feature.\n\t */\n\tif ((!vq->inflight_packed))\n\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\n\tif (!vq->inflight_packed->version) {\n\t\tvq->inflight_packed->version = INFLIGHT_VERSION;\n\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\t}\n\n\tif (vq->resubmit_inflight)\n\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\n\tinflight_packed = vq->inflight_packed;\n\tvq->global_counter = 0;\n\told_used_idx = inflight_packed->old_used_idx;\n\n\tif (inflight_packed->used_idx != old_used_idx) {\n\t\tif (inflight_packed->desc[old_used_idx].inflight == 0) {\n\t\t\tinflight_packed->old_used_idx =\n\t\t\t\tinflight_packed->used_idx;\n\t\t\tinflight_packed->old_used_wrap_counter =\n\t\t\t\tinflight_packed->used_wrap_counter;\n\t\t\tinflight_packed->old_free_head =\n\t\t\t\tinflight_packed->free_head;\n\t\t} else {\n\t\t\tinflight_packed->used_idx =\n\t\t\t\tinflight_packed->old_used_idx;\n\t\t\tinflight_packed->used_wrap_counter =\n\t\t\t\tinflight_packed->old_used_wrap_counter;\n\t\t\tinflight_packed->free_head =\n\t\t\t\tinflight_packed->old_free_head;\n\t\t}\n\t}\n\n\tfor (i = 0; i < inflight_packed->desc_num; i++) {\n\t\tif (inflight_packed->desc[i].inflight == 1)\n\t\t\tresubmit_num++;\n\t}\n\n\tif (resubmit_num) {\n\t\tresubmit = rte_zmalloc_socket(\"resubmit\", sizeof(struct rte_vhost_resubmit_info),\n\t\t\t\t0, vq->numa_node);\n\t\tif (resubmit == NULL) {\n\t\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\t\"(%s) failed to allocate memory for resubmit info.\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\n\t\tresubmit->resubmit_list = rte_zmalloc_socket(\"resubmit_list\",\n\t\t\t\tresubmit_num * sizeof(struct rte_vhost_resubmit_desc),\n\t\t\t\t0, vq->numa_node);\n\t\tif (resubmit->resubmit_list == NULL) {\n\t\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\t\"(%s) failed to allocate memory for resubmit desc.\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\trte_free(resubmit);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\n\t\tnum = 0;\n\t\tfor (i = 0; i < inflight_packed->desc_num; i++) {\n\t\t\tif (vq->inflight_packed->desc[i].inflight == 1) {\n\t\t\t\tresubmit->resubmit_list[num].index = i;\n\t\t\t\tresubmit->resubmit_list[num].counter =\n\t\t\t\t\tinflight_packed->desc[i].counter;\n\t\t\t\tnum++;\n\t\t\t}\n\t\t}\n\t\tresubmit->resubmit_num = num;\n\n\t\tif (resubmit->resubmit_num > 1)\n\t\t\tqsort(resubmit->resubmit_list, resubmit->resubmit_num,\n\t\t\t      sizeof(struct rte_vhost_resubmit_desc),\n\t\t\t      resubmit_desc_compare);\n\n\t\tvq->global_counter = resubmit->resubmit_list[0].counter + 1;\n\t\tvq->resubmit_inflight = resubmit;\n\t}\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_user_set_vring_kick(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tstruct vhost_vring_file file;\n\tstruct vhost_virtqueue *vq;\n\tint expected_fds;\n\n\texpected_fds = (ctx->msg.payload.u64 & VHOST_USER_VRING_NOFD_MASK) ? 0 : 1;\n\tif (validate_msg_fds(dev, ctx, expected_fds) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tfile.index = ctx->msg.payload.u64 & VHOST_USER_VRING_IDX_MASK;\n\tif (ctx->msg.payload.u64 & VHOST_USER_VRING_NOFD_MASK)\n\t\tfile.fd = VIRTIO_INVALID_EVENTFD;\n\telse\n\t\tfile.fd = ctx->fds[0];\n\tVHOST_LOG_CONFIG(INFO, \"(%s) vring kick idx:%d file:%d\\n\",\n\t\t\tdev->ifname, file.index, file.fd);\n\n\t/* Interpret ring addresses only when ring is started. */\n\tdev = translate_ring_addresses(dev, file.index);\n\tif (!dev) {\n\t\tif (file.fd != VIRTIO_INVALID_EVENTFD)\n\t\t\tclose(file.fd);\n\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\t*pdev = dev;\n\n\tvq = dev->virtqueue[file.index];\n\n\t/*\n\t * When VHOST_USER_F_PROTOCOL_FEATURES is not negotiated,\n\t * the ring starts already enabled. Otherwise, it is enabled via\n\t * the SET_VRING_ENABLE message.\n\t */\n\tif (!(dev->features & (1ULL << VHOST_USER_F_PROTOCOL_FEATURES))) {\n\t\tvq->enabled = true;\n\t}\n\n\tif (vq->ready) {\n\t\tvq->ready = false;\n\t\tvhost_user_notify_queue_state(dev, file.index, 0);\n\t}\n\n\tif (vq->kickfd >= 0)\n\t\tclose(vq->kickfd);\n\tvq->kickfd = file.fd;\n\n\tif (vq_is_packed(dev)) {\n\t\tif (vhost_check_queue_inflights_packed(dev, vq)) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to inflights for vq: %d\\n\",\n\t\t\t\t\tdev->ifname, file.index);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\t} else {\n\t\tif (vhost_check_queue_inflights_split(dev, vq)) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to inflights for vq: %d\\n\",\n\t\t\t\t\tdev->ifname, file.index);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\t}\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\n/*\n * when virtio is stopped, qemu will send us the GET_VRING_BASE message.\n */\nstatic int\nvhost_user_get_vring_base(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tstruct vhost_virtqueue *vq = dev->virtqueue[ctx->msg.payload.state.index];\n\tuint64_t val;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\t/* We have to stop the queue (virtio) if it is running. */\n\tvhost_destroy_device_notify(dev);\n\n\tdev->flags &= ~VIRTIO_DEV_READY;\n\tdev->flags &= ~VIRTIO_DEV_VDPA_CONFIGURED;\n\n\t/* Here we are safe to get the indexes */\n\tif (vq_is_packed(dev)) {\n\t\t/*\n\t\t * Bit[0:14]: avail index\n\t\t * Bit[15]: avail wrap counter\n\t\t */\n\t\tval = vq->last_avail_idx & 0x7fff;\n\t\tval |= vq->avail_wrap_counter << 15;\n\t\tctx->msg.payload.state.num = val;\n\t} else {\n\t\tctx->msg.payload.state.num = vq->last_avail_idx;\n\t}\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) vring base idx:%d file:%d\\n\",\n\t\t\tdev->ifname, ctx->msg.payload.state.index,\n\t\t\tctx->msg.payload.state.num);\n\t/*\n\t * Based on current qemu vhost-user implementation, this message is\n\t * sent and only sent in vhost_vring_stop.\n\t * TODO: cleanup the vring, it isn't usable since here.\n\t */\n\tif (vq->kickfd >= 0)\n\t\tclose(vq->kickfd);\n\n\tvq->kickfd = VIRTIO_UNINITIALIZED_EVENTFD;\n\n\tif (vq->callfd >= 0)\n\t\tclose(vq->callfd);\n\n\tvq->callfd = VIRTIO_UNINITIALIZED_EVENTFD;\n\n\tvq->signalled_used_valid = false;\n\n\tif (vq_is_packed(dev)) {\n\t\trte_free(vq->shadow_used_packed);\n\t\tvq->shadow_used_packed = NULL;\n\t} else {\n\t\trte_free(vq->shadow_used_split);\n\t\tvq->shadow_used_split = NULL;\n\t}\n\n\trte_free(vq->batch_copy_elems);\n\tvq->batch_copy_elems = NULL;\n\n\trte_free(vq->log_cache);\n\tvq->log_cache = NULL;\n\n\tctx->msg.size = sizeof(ctx->msg.payload.state);\n\tctx->fd_num = 0;\n\n\tvhost_user_iotlb_flush_all(vq);\n\n\tvring_invalidate(dev, vq);\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n}\n\n/*\n * when virtio queues are ready to work, qemu will send us to\n * enable the virtio queue pair.\n */\nstatic int\nvhost_user_set_vring_enable(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tbool enable = !!ctx->msg.payload.state.num;\n\tint index = (int)ctx->msg.payload.state.index;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set queue enable: %d to qp idx: %d\\n\",\n\t\t\tdev->ifname, enable, index);\n\n\tif (enable && dev->virtqueue[index]->async) {\n\t\tif (dev->virtqueue[index]->async->pkts_inflight_n) {\n\t\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\"(%s) failed to enable vring. Inflight packets must be completed first\\n\",\n\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\t}\n\n\tdev->virtqueue[index]->enabled = enable;\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_user_get_protocol_features(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tuint64_t features, protocol_features;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\trte_vhost_driver_get_features(dev->ifname, &features);\n\trte_vhost_driver_get_protocol_features(dev->ifname, &protocol_features);\n\n\tctx->msg.payload.u64 = protocol_features;\n\tctx->msg.size = sizeof(ctx->msg.payload.u64);\n\tctx->fd_num = 0;\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n}\n\nstatic int\nvhost_user_set_protocol_features(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tuint64_t protocol_features = ctx->msg.payload.u64;\n\tuint64_t slave_protocol_features = 0;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\trte_vhost_driver_get_protocol_features(dev->ifname,\n\t\t\t&slave_protocol_features);\n\tif (protocol_features & ~slave_protocol_features) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) received invalid protocol features.\\n\", dev->ifname);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tdev->protocol_features = protocol_features;\n\tVHOST_LOG_CONFIG(INFO, \"(%s) negotiated Vhost-user protocol features: 0x%\" PRIx64 \"\\n\",\n\t\tdev->ifname, dev->protocol_features);\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_user_set_log_base(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tint fd = ctx->fds[0];\n\tuint64_t size, off;\n\tvoid *addr;\n\tuint32_t i;\n\n\tif (validate_msg_fds(dev, ctx, 1) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (fd < 0) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid log fd: %d\\n\", dev->ifname, fd);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tif (ctx->msg.size != sizeof(VhostUserLog)) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid log base msg size: %\"PRId32\" != %d\\n\",\n\t\t\tdev->ifname, ctx->msg.size, (int)sizeof(VhostUserLog));\n\t\tgoto close_msg_fds;\n\t}\n\n\tsize = ctx->msg.payload.log.mmap_size;\n\toff  = ctx->msg.payload.log.mmap_offset;\n\n\t/* Check for mmap size and offset overflow. */\n\tif (off >= -size) {\n\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\"(%s) log offset %#\"PRIx64\" and log size %#\"PRIx64\" overflow\\n\",\n\t\t\t\tdev->ifname, off, size);\n\t\tgoto close_msg_fds;\n\t}\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) log mmap size: %\"PRId64\", offset: %\"PRId64\"\\n\",\n\t\t\tdev->ifname, size, off);\n\n\t/*\n\t * mmap from 0 to workaround a hugepage mmap bug: mmap will\n\t * fail when offset is not page size aligned.\n\t */\n\taddr = mmap(0, size + off, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);\n\tclose(fd);\n\tif (addr == MAP_FAILED) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) mmap log base failed!\\n\", dev->ifname);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\t/*\n\t * Free previously mapped log memory on occasionally\n\t * multiple VHOST_USER_SET_LOG_BASE.\n\t */\n\tif (dev->log_addr) {\n\t\tmunmap((void *)(uintptr_t)dev->log_addr, dev->log_size);\n\t}\n\tdev->log_addr = (uint64_t)(uintptr_t)addr;\n\tdev->log_base = dev->log_addr + off;\n\tdev->log_size = size;\n\n\tfor (i = 0; i < dev->nr_vring; i++) {\n\t\tstruct vhost_virtqueue *vq = dev->virtqueue[i];\n\n\t\trte_free(vq->log_cache);\n\t\tvq->log_cache = NULL;\n\t\tvq->log_cache_nb_elem = 0;\n\t\tvq->log_cache = rte_malloc_socket(\"vq log cache\",\n\t\t\t\tsizeof(struct log_cache_entry) * VHOST_LOG_CACHE_NR,\n\t\t\t\t0, vq->numa_node);\n\t\t/*\n\t\t * If log cache alloc fail, don't fail migration, but no\n\t\t * caching will be done, which will impact performance\n\t\t */\n\t\tif (!vq->log_cache)\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to allocate VQ logging cache\\n\",\n\t\t\t\t\tdev->ifname);\n\t}\n\n\t/*\n\t * The spec is not clear about it (yet), but QEMU doesn't expect\n\t * any payload in the reply.\n\t */\n\tctx->msg.size = 0;\n\tctx->fd_num = 0;\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n\nclose_msg_fds:\n\tclose_msg_fds(ctx);\n\treturn RTE_VHOST_MSG_RESULT_ERR;\n}\n\nstatic int vhost_user_set_log_fd(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\n\tif (validate_msg_fds(dev, ctx, 1) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tclose(ctx->fds[0]);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) not implemented.\\n\", dev->ifname);\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\n/*\n * An rarp packet is constructed and broadcasted to notify switches about\n * the new location of the migrated VM, so that packets from outside will\n * not be lost after migration.\n *\n * However, we don't actually \"send\" a rarp packet here, instead, we set\n * a flag 'broadcast_rarp' to let rte_vhost_dequeue_burst() inject it.\n */\nstatic int\nvhost_user_send_rarp(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tuint8_t *mac = (uint8_t *)&ctx->msg.payload.u64;\n\tstruct rte_vdpa_device *vdpa_dev;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tVHOST_LOG_CONFIG(DEBUG, \"(%s) MAC: \" RTE_ETHER_ADDR_PRT_FMT \"\\n\",\n\t\tdev->ifname, mac[0], mac[1], mac[2], mac[3], mac[4], mac[5]);\n\tmemcpy(dev->mac.addr_bytes, mac, 6);\n\n\t/*\n\t * Set the flag to inject a RARP broadcast packet at\n\t * rte_vhost_dequeue_burst().\n\t *\n\t * __ATOMIC_RELEASE ordering is for making sure the mac is\n\t * copied before the flag is set.\n\t */\n\t__atomic_store_n(&dev->broadcast_rarp, 1, __ATOMIC_RELEASE);\n\tvdpa_dev = dev->vdpa_dev;\n\tif (vdpa_dev && vdpa_dev->ops->migration_done)\n\t\tvdpa_dev->ops->migration_done(dev->vid);\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_user_net_set_mtu(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (ctx->msg.payload.u64 < VIRTIO_MIN_MTU ||\n\t\t\tctx->msg.payload.u64 > VIRTIO_MAX_MTU) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid MTU size (%\"PRIu64\")\\n\",\n\t\t\t\tdev->ifname, ctx->msg.payload.u64);\n\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tdev->mtu = ctx->msg.payload.u64;\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_user_set_req_fd(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tint fd = ctx->fds[0];\n\n\tif (validate_msg_fds(dev, ctx, 1) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (fd < 0) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid file descriptor for slave channel (%d)\\n\",\n\t\t\t\tdev->ifname, fd);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tif (dev->slave_req_fd >= 0)\n\t\tclose(dev->slave_req_fd);\n\n\tdev->slave_req_fd = fd;\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nis_vring_iotlb_split(struct vhost_virtqueue *vq, struct vhost_iotlb_msg *imsg)\n{\n\tstruct vhost_vring_addr *ra;\n\tuint64_t start, end, len;\n\n\tstart = imsg->iova;\n\tend = start + imsg->size;\n\n\tra = &vq->ring_addrs;\n\tlen = sizeof(struct vring_desc) * vq->size;\n\tif (ra->desc_user_addr < end && (ra->desc_user_addr + len) > start)\n\t\treturn 1;\n\n\tlen = sizeof(struct vring_avail) + sizeof(uint16_t) * vq->size;\n\tif (ra->avail_user_addr < end && (ra->avail_user_addr + len) > start)\n\t\treturn 1;\n\n\tlen = sizeof(struct vring_used) +\n\t       sizeof(struct vring_used_elem) * vq->size;\n\tif (ra->used_user_addr < end && (ra->used_user_addr + len) > start)\n\t\treturn 1;\n\n\tif (ra->flags & (1 << VHOST_VRING_F_LOG)) {\n\t\tlen = sizeof(uint64_t);\n\t\tif (ra->log_guest_addr < end &&\n\t\t    (ra->log_guest_addr + len) > start)\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nis_vring_iotlb_packed(struct vhost_virtqueue *vq, struct vhost_iotlb_msg *imsg)\n{\n\tstruct vhost_vring_addr *ra;\n\tuint64_t start, end, len;\n\n\tstart = imsg->iova;\n\tend = start + imsg->size;\n\n\tra = &vq->ring_addrs;\n\tlen = sizeof(struct vring_packed_desc) * vq->size;\n\tif (ra->desc_user_addr < end && (ra->desc_user_addr + len) > start)\n\t\treturn 1;\n\n\tlen = sizeof(struct vring_packed_desc_event);\n\tif (ra->avail_user_addr < end && (ra->avail_user_addr + len) > start)\n\t\treturn 1;\n\n\tlen = sizeof(struct vring_packed_desc_event);\n\tif (ra->used_user_addr < end && (ra->used_user_addr + len) > start)\n\t\treturn 1;\n\n\tif (ra->flags & (1 << VHOST_VRING_F_LOG)) {\n\t\tlen = sizeof(uint64_t);\n\t\tif (ra->log_guest_addr < end &&\n\t\t    (ra->log_guest_addr + len) > start)\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int is_vring_iotlb(struct virtio_net *dev,\n\t\t\t  struct vhost_virtqueue *vq,\n\t\t\t  struct vhost_iotlb_msg *imsg)\n{\n\tif (vq_is_packed(dev))\n\t\treturn is_vring_iotlb_packed(vq, imsg);\n\telse\n\t\treturn is_vring_iotlb_split(vq, imsg);\n}\n\nstatic int\nvhost_user_iotlb_msg(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tstruct vhost_iotlb_msg *imsg = &ctx->msg.payload.iotlb;\n\tuint16_t i;\n\tuint64_t vva, len;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tswitch (imsg->type) {\n\tcase VHOST_IOTLB_UPDATE:\n\t\tlen = imsg->size;\n\t\tvva = qva_to_vva(dev, imsg->uaddr, &len);\n\t\tif (!vva)\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\t\tfor (i = 0; i < dev->nr_vring; i++) {\n\t\t\tstruct vhost_virtqueue *vq = dev->virtqueue[i];\n\n\t\t\tif (!vq)\n\t\t\t\tcontinue;\n\n\t\t\tvhost_user_iotlb_cache_insert(dev, vq, imsg->iova, vva,\n\t\t\t\t\tlen, imsg->perm);\n\n\t\t\tif (is_vring_iotlb(dev, vq, imsg)) {\n\t\t\t\trte_spinlock_lock(&vq->access_lock);\n\t\t\t\t*pdev = dev = translate_ring_addresses(dev, i);\n\t\t\t\trte_spinlock_unlock(&vq->access_lock);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase VHOST_IOTLB_INVALIDATE:\n\t\tfor (i = 0; i < dev->nr_vring; i++) {\n\t\t\tstruct vhost_virtqueue *vq = dev->virtqueue[i];\n\n\t\t\tif (!vq)\n\t\t\t\tcontinue;\n\n\t\t\tvhost_user_iotlb_cache_remove(vq, imsg->iova,\n\t\t\t\t\timsg->size);\n\n\t\t\tif (is_vring_iotlb(dev, vq, imsg)) {\n\t\t\t\trte_spinlock_lock(&vq->access_lock);\n\t\t\t\tvring_invalidate(dev, vq);\n\t\t\t\trte_spinlock_unlock(&vq->access_lock);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid IOTLB message type (%d)\\n\",\n\t\t\t\tdev->ifname, imsg->type);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_user_set_postcopy_advise(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n#ifdef RTE_LIBRTE_VHOST_POSTCOPY\n\tstruct uffdio_api api_struct;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tdev->postcopy_ufd = syscall(__NR_userfaultfd, O_CLOEXEC | O_NONBLOCK);\n\n\tif (dev->postcopy_ufd == -1) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) userfaultfd not available: %s\\n\",\n\t\t\tdev->ifname, strerror(errno));\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\tapi_struct.api = UFFD_API;\n\tapi_struct.features = 0;\n\tif (ioctl(dev->postcopy_ufd, UFFDIO_API, &api_struct)) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) UFFDIO_API ioctl failure: %s\\n\",\n\t\t\tdev->ifname, strerror(errno));\n\t\tclose(dev->postcopy_ufd);\n\t\tdev->postcopy_ufd = -1;\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\tctx->fds[0] = dev->postcopy_ufd;\n\tctx->fd_num = 1;\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n#else\n\tdev->postcopy_ufd = -1;\n\tctx->fd_num = 0;\n\n\treturn RTE_VHOST_MSG_RESULT_ERR;\n#endif\n}\n\nstatic int\nvhost_user_set_postcopy_listen(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx __rte_unused,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (dev->mem && dev->mem->nregions) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) regions already registered at postcopy-listen\\n\",\n\t\t\t\tdev->ifname);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\tdev->postcopy_listening = 1;\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_user_postcopy_end(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tdev->postcopy_listening = 0;\n\tif (dev->postcopy_ufd >= 0) {\n\t\tclose(dev->postcopy_ufd);\n\t\tdev->postcopy_ufd = -1;\n\t}\n\n\tctx->msg.payload.u64 = 0;\n\tctx->msg.size = sizeof(ctx->msg.payload.u64);\n\tctx->fd_num = 0;\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n}\n\nstatic int\nvhost_user_get_status(struct virtio_net **pdev,\n\t\t      struct vhu_msg_context *ctx,\n\t\t      int main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tctx->msg.payload.u64 = dev->status;\n\tctx->msg.size = sizeof(ctx->msg.payload.u64);\n\tctx->fd_num = 0;\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n}\n\nstatic int\nvhost_user_set_status(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\t/* As per Virtio specification, the device status is 8bits long */\n\tif (ctx->msg.payload.u64 > UINT8_MAX) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid VHOST_USER_SET_STATUS payload 0x%\" PRIx64 \"\\n\",\n\t\t\t\tdev->ifname, ctx->msg.payload.u64);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tdev->status = ctx->msg.payload.u64;\n\n\tif ((dev->status & VIRTIO_DEVICE_STATUS_FEATURES_OK) &&\n\t    (dev->flags & VIRTIO_DEV_FEATURES_FAILED)) {\n\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\"(%s) FEATURES_OK bit is set but feature negotiation failed\\n\",\n\t\t\t\tdev->ifname);\n\t\t/*\n\t\t * Clear the bit to let the driver know about the feature\n\t\t * negotiation failure\n\t\t */\n\t\tdev->status &= ~VIRTIO_DEVICE_STATUS_FEATURES_OK;\n\t}\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) new device status(0x%08x):\\n\", dev->ifname,\n\t\t\tdev->status);\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t-RESET: %u\\n\", dev->ifname,\n\t\t\t(dev->status == VIRTIO_DEVICE_STATUS_RESET));\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t-ACKNOWLEDGE: %u\\n\", dev->ifname,\n\t\t\t!!(dev->status & VIRTIO_DEVICE_STATUS_ACK));\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t-DRIVER: %u\\n\", dev->ifname,\n\t\t\t!!(dev->status & VIRTIO_DEVICE_STATUS_DRIVER));\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t-FEATURES_OK: %u\\n\", dev->ifname,\n\t\t\t!!(dev->status & VIRTIO_DEVICE_STATUS_FEATURES_OK));\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t-DRIVER_OK: %u\\n\", dev->ifname,\n\t\t\t!!(dev->status & VIRTIO_DEVICE_STATUS_DRIVER_OK));\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t-DEVICE_NEED_RESET: %u\\n\", dev->ifname,\n\t\t\t!!(dev->status & VIRTIO_DEVICE_STATUS_DEV_NEED_RESET));\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t-FAILED: %u\\n\", dev->ifname,\n\t\t\t!!(dev->status & VIRTIO_DEVICE_STATUS_FAILED));\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\ntypedef int (*vhost_message_handler_t)(struct virtio_net **pdev,\n\t\t\t\t\tstruct vhu_msg_context *ctx,\n\t\t\t\t\tint main_fd);\n\nstatic vhost_message_handler_t vhost_message_handlers[VHOST_USER_MAX] = {\n\t[VHOST_USER_NONE] = NULL,\n\t[VHOST_USER_GET_FEATURES] = vhost_user_get_features,\n\t[VHOST_USER_SET_FEATURES] = vhost_user_set_features,\n\t[VHOST_USER_SET_OWNER] = vhost_user_set_owner,\n\t[VHOST_USER_RESET_OWNER] = vhost_user_reset_owner,\n\t[VHOST_USER_SET_MEM_TABLE] = vhost_user_set_mem_table,\n\t[VHOST_USER_SET_LOG_BASE] = vhost_user_set_log_base,\n\t[VHOST_USER_SET_LOG_FD] = vhost_user_set_log_fd,\n\t[VHOST_USER_SET_VRING_NUM] = vhost_user_set_vring_num,\n\t[VHOST_USER_SET_VRING_ADDR] = vhost_user_set_vring_addr,\n\t[VHOST_USER_SET_VRING_BASE] = vhost_user_set_vring_base,\n\t[VHOST_USER_GET_VRING_BASE] = vhost_user_get_vring_base,\n\t[VHOST_USER_SET_VRING_KICK] = vhost_user_set_vring_kick,\n\t[VHOST_USER_SET_VRING_CALL] = vhost_user_set_vring_call,\n\t[VHOST_USER_SET_VRING_ERR] = vhost_user_set_vring_err,\n\t[VHOST_USER_GET_PROTOCOL_FEATURES] = vhost_user_get_protocol_features,\n\t[VHOST_USER_SET_PROTOCOL_FEATURES] = vhost_user_set_protocol_features,\n\t[VHOST_USER_GET_QUEUE_NUM] = vhost_user_get_queue_num,\n\t[VHOST_USER_SET_VRING_ENABLE] = vhost_user_set_vring_enable,\n\t[VHOST_USER_SEND_RARP] = vhost_user_send_rarp,\n\t[VHOST_USER_NET_SET_MTU] = vhost_user_net_set_mtu,\n\t[VHOST_USER_SET_SLAVE_REQ_FD] = vhost_user_set_req_fd,\n\t[VHOST_USER_IOTLB_MSG] = vhost_user_iotlb_msg,\n\t[VHOST_USER_POSTCOPY_ADVISE] = vhost_user_set_postcopy_advise,\n\t[VHOST_USER_POSTCOPY_LISTEN] = vhost_user_set_postcopy_listen,\n\t[VHOST_USER_POSTCOPY_END] = vhost_user_postcopy_end,\n\t[VHOST_USER_GET_INFLIGHT_FD] = vhost_user_get_inflight_fd,\n\t[VHOST_USER_SET_INFLIGHT_FD] = vhost_user_set_inflight_fd,\n\t[VHOST_USER_SET_STATUS] = vhost_user_set_status,\n\t[VHOST_USER_GET_STATUS] = vhost_user_get_status,\n};\n\n/* return bytes# of read on success or negative val on failure. */\nstatic int\nread_vhost_message(struct virtio_net *dev, int sockfd, struct  vhu_msg_context *ctx)\n{\n\tint ret;\n\n\tret = read_fd_message(dev->ifname, sockfd, (char *)&ctx->msg, VHOST_USER_HDR_SIZE,\n\t\tctx->fds, VHOST_MEMORY_MAX_NREGIONS, &ctx->fd_num);\n\tif (ret <= 0) {\n\t\treturn ret;\n\t} else if (ret != VHOST_USER_HDR_SIZE) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) Unexpected header size read\\n\", dev->ifname);\n\t\tclose_msg_fds(ctx);\n\t\treturn -1;\n\t}\n\n\tif (ctx->msg.size) {\n\t\tif (ctx->msg.size > sizeof(ctx->msg.payload)) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid msg size: %d\\n\",\n\t\t\t\t\tdev->ifname, ctx->msg.size);\n\t\t\treturn -1;\n\t\t}\n\t\tret = read(sockfd, &ctx->msg.payload, ctx->msg.size);\n\t\tif (ret <= 0)\n\t\t\treturn ret;\n\t\tif (ret != (int)ctx->msg.size) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) read control message failed\\n\", dev->ifname);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic int\nsend_vhost_message(struct virtio_net *dev, int sockfd, struct vhu_msg_context *ctx)\n{\n\tif (!ctx)\n\t\treturn 0;\n\n\treturn send_fd_message(dev->ifname, sockfd, (char *)&ctx->msg,\n\t\tVHOST_USER_HDR_SIZE + ctx->msg.size, ctx->fds, ctx->fd_num);\n}\n\nstatic int\nsend_vhost_reply(struct virtio_net *dev, int sockfd, struct vhu_msg_context *ctx)\n{\n\tif (!ctx)\n\t\treturn 0;\n\n\tctx->msg.flags &= ~VHOST_USER_VERSION_MASK;\n\tctx->msg.flags &= ~VHOST_USER_NEED_REPLY;\n\tctx->msg.flags |= VHOST_USER_VERSION;\n\tctx->msg.flags |= VHOST_USER_REPLY_MASK;\n\n\treturn send_vhost_message(dev, sockfd, ctx);\n}\n\nstatic int\nsend_vhost_slave_message(struct virtio_net *dev,\n\t\tstruct vhu_msg_context *ctx)\n{\n\tint ret;\n\n\tif (ctx->msg.flags & VHOST_USER_NEED_REPLY)\n\t\trte_spinlock_lock(&dev->slave_req_lock);\n\n\tret = send_vhost_message(dev, dev->slave_req_fd, ctx);\n\tif (ret < 0 && (ctx->msg.flags & VHOST_USER_NEED_REPLY))\n\t\trte_spinlock_unlock(&dev->slave_req_lock);\n\n\treturn ret;\n}\n\n/*\n * Allocate a queue pair if it hasn't been allocated yet\n */\nstatic int\nvhost_user_check_and_alloc_queue_pair(struct virtio_net *dev,\n\t\t\tstruct vhu_msg_context *ctx)\n{\n\tuint32_t vring_idx;\n\n\tswitch (ctx->msg.request.master) {\n\tcase VHOST_USER_SET_VRING_KICK:\n\tcase VHOST_USER_SET_VRING_CALL:\n\tcase VHOST_USER_SET_VRING_ERR:\n\t\tvring_idx = ctx->msg.payload.u64 & VHOST_USER_VRING_IDX_MASK;\n\t\tbreak;\n\tcase VHOST_USER_SET_VRING_NUM:\n\tcase VHOST_USER_SET_VRING_BASE:\n\tcase VHOST_USER_GET_VRING_BASE:\n\tcase VHOST_USER_SET_VRING_ENABLE:\n\t\tvring_idx = ctx->msg.payload.state.index;\n\t\tbreak;\n\tcase VHOST_USER_SET_VRING_ADDR:\n\t\tvring_idx = ctx->msg.payload.addr.index;\n\t\tbreak;\n\tcase VHOST_USER_SET_INFLIGHT_FD:\n\t\tvring_idx = ctx->msg.payload.inflight.num_queues - 1;\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (vring_idx >= VHOST_MAX_VRING) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid vring index: %u\\n\", dev->ifname, vring_idx);\n\t\treturn -1;\n\t}\n\n\tif (dev->virtqueue[vring_idx])\n\t\treturn 0;\n\n\treturn alloc_vring_queue(dev, vring_idx);\n}\n\nstatic void\nvhost_user_lock_all_queue_pairs(struct virtio_net *dev)\n{\n\tunsigned int i = 0;\n\tunsigned int vq_num = 0;\n\n\twhile (vq_num < dev->nr_vring) {\n\t\tstruct vhost_virtqueue *vq = dev->virtqueue[i];\n\n\t\tif (vq) {\n\t\t\trte_spinlock_lock(&vq->access_lock);\n\t\t\tvq_num++;\n\t\t}\n\t\ti++;\n\t}\n}\n\nstatic void\nvhost_user_unlock_all_queue_pairs(struct virtio_net *dev)\n{\n\tunsigned int i = 0;\n\tunsigned int vq_num = 0;\n\n\twhile (vq_num < dev->nr_vring) {\n\t\tstruct vhost_virtqueue *vq = dev->virtqueue[i];\n\n\t\tif (vq) {\n\t\t\trte_spinlock_unlock(&vq->access_lock);\n\t\t\tvq_num++;\n\t\t}\n\t\ti++;\n\t}\n}\n\nint\nvhost_user_msg_handler(int vid, int fd)\n{\n\tstruct virtio_net *dev;\n\tstruct vhu_msg_context ctx;\n\tstruct rte_vdpa_device *vdpa_dev;\n\tint ret;\n\tint unlock_required = 0;\n\tbool handled;\n\tint request;\n\tuint32_t i;\n\n\tdev = get_device(vid);\n\tif (dev == NULL)\n\t\treturn -1;\n\n\tif (!dev->notify_ops) {\n\t\tdev->notify_ops = vhost_driver_callback_get(dev->ifname);\n\t\tif (!dev->notify_ops) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to get callback ops for driver\\n\",\n\t\t\t\tdev->ifname);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tret = read_vhost_message(dev, fd, &ctx);\n\tif (ret <= 0) {\n\t\tif (ret < 0)\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) vhost read message failed\\n\", dev->ifname);\n\t\telse\n\t\t\tVHOST_LOG_CONFIG(INFO, \"(%s) vhost peer closed\\n\", dev->ifname);\n\n\t\treturn -1;\n\t}\n\n\tret = 0;\n\trequest = ctx.msg.request.master;\n\tif (request > VHOST_USER_NONE && request < VHOST_USER_MAX &&\n\t\t\tvhost_message_str[request]) {\n\t\tif (request != VHOST_USER_IOTLB_MSG)\n\t\t\tVHOST_LOG_CONFIG(INFO, \"(%s) read message %s\\n\",\n\t\t\t\tdev->ifname, vhost_message_str[request]);\n\t\telse\n\t\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) read message %s\\n\",\n\t\t\t\tdev->ifname, vhost_message_str[request]);\n\t} else {\n\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) external request %d\\n\", dev->ifname, request);\n\t}\n\n\tret = vhost_user_check_and_alloc_queue_pair(dev, &ctx);\n\tif (ret < 0) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to alloc queue\\n\", dev->ifname);\n\t\treturn -1;\n\t}\n\n\t/*\n\t * Note: we don't lock all queues on VHOST_USER_GET_VRING_BASE\n\t * and VHOST_USER_RESET_OWNER, since it is sent when virtio stops\n\t * and device is destroyed. destroy_device waits for queues to be\n\t * inactive, so it is safe. Otherwise taking the access_lock\n\t * would cause a dead lock.\n\t */\n\tswitch (request) {\n\tcase VHOST_USER_SET_FEATURES:\n\tcase VHOST_USER_SET_PROTOCOL_FEATURES:\n\tcase VHOST_USER_SET_OWNER:\n\tcase VHOST_USER_SET_MEM_TABLE:\n\tcase VHOST_USER_SET_LOG_BASE:\n\tcase VHOST_USER_SET_LOG_FD:\n\tcase VHOST_USER_SET_VRING_NUM:\n\tcase VHOST_USER_SET_VRING_ADDR:\n\tcase VHOST_USER_SET_VRING_BASE:\n\tcase VHOST_USER_SET_VRING_KICK:\n\tcase VHOST_USER_SET_VRING_CALL:\n\tcase VHOST_USER_SET_VRING_ERR:\n\tcase VHOST_USER_SET_VRING_ENABLE:\n\tcase VHOST_USER_SEND_RARP:\n\tcase VHOST_USER_NET_SET_MTU:\n\tcase VHOST_USER_SET_SLAVE_REQ_FD:\n\t\tif (!(dev->flags & VIRTIO_DEV_VDPA_CONFIGURED)) {\n\t\t\tvhost_user_lock_all_queue_pairs(dev);\n\t\t\tunlock_required = 1;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\n\t}\n\n\thandled = false;\n\tif (dev->extern_ops.pre_msg_handle) {\n\t\tRTE_BUILD_BUG_ON(offsetof(struct vhu_msg_context, msg) != 0);\n\t\tret = (*dev->extern_ops.pre_msg_handle)(dev->vid, &ctx);\n\t\tswitch (ret) {\n\t\tcase RTE_VHOST_MSG_RESULT_REPLY:\n\t\t\tsend_vhost_reply(dev, fd, &ctx);\n\t\t\t/* Fall-through */\n\t\tcase RTE_VHOST_MSG_RESULT_ERR:\n\t\tcase RTE_VHOST_MSG_RESULT_OK:\n\t\t\thandled = true;\n\t\t\tgoto skip_to_post_handle;\n\t\tcase RTE_VHOST_MSG_RESULT_NOT_HANDLED:\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (request > VHOST_USER_NONE && request < VHOST_USER_MAX) {\n\t\tif (!vhost_message_handlers[request])\n\t\t\tgoto skip_to_post_handle;\n\t\tret = vhost_message_handlers[request](&dev, &ctx, fd);\n\n\t\tswitch (ret) {\n\t\tcase RTE_VHOST_MSG_RESULT_ERR:\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) processing %s failed.\\n\",\n\t\t\t\t\tdev->ifname, vhost_message_str[request]);\n\t\t\thandled = true;\n\t\t\tbreak;\n\t\tcase RTE_VHOST_MSG_RESULT_OK:\n\t\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) processing %s succeeded.\\n\",\n\t\t\t\t\tdev->ifname, vhost_message_str[request]);\n\t\t\thandled = true;\n\t\t\tbreak;\n\t\tcase RTE_VHOST_MSG_RESULT_REPLY:\n\t\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) processing %s succeeded and needs reply.\\n\",\n\t\t\t\t\tdev->ifname, vhost_message_str[request]);\n\t\t\tsend_vhost_reply(dev, fd, &ctx);\n\t\t\thandled = true;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\nskip_to_post_handle:\n\tif (ret != RTE_VHOST_MSG_RESULT_ERR &&\n\t\t\tdev->extern_ops.post_msg_handle) {\n\t\tRTE_BUILD_BUG_ON(offsetof(struct vhu_msg_context, msg) != 0);\n\t\tret = (*dev->extern_ops.post_msg_handle)(dev->vid, &ctx);\n\t\tswitch (ret) {\n\t\tcase RTE_VHOST_MSG_RESULT_REPLY:\n\t\t\tsend_vhost_reply(dev, fd, &ctx);\n\t\t\t/* Fall-through */\n\t\tcase RTE_VHOST_MSG_RESULT_ERR:\n\t\tcase RTE_VHOST_MSG_RESULT_OK:\n\t\t\thandled = true;\n\t\tcase RTE_VHOST_MSG_RESULT_NOT_HANDLED:\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* If message was not handled at this stage, treat it as an error */\n\tif (!handled) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) vhost message (req: %d) was not handled.\\n\",\n\t\t\t\tdev->ifname, request);\n\t\tclose_msg_fds(&ctx);\n\t\tret = RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\t/*\n\t * If the request required a reply that was already sent,\n\t * this optional reply-ack won't be sent as the\n\t * VHOST_USER_NEED_REPLY was cleared in send_vhost_reply().\n\t */\n\tif (ctx.msg.flags & VHOST_USER_NEED_REPLY) {\n\t\tctx.msg.payload.u64 = ret == RTE_VHOST_MSG_RESULT_ERR;\n\t\tctx.msg.size = sizeof(ctx.msg.payload.u64);\n\t\tctx.fd_num = 0;\n\t\tsend_vhost_reply(dev, fd, &ctx);\n\t} else if (ret == RTE_VHOST_MSG_RESULT_ERR) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) vhost message handling failed.\\n\", dev->ifname);\n\t\treturn -1;\n\t}\n\n\tfor (i = 0; i < dev->nr_vring; i++) {\n\t\tstruct vhost_virtqueue *vq = dev->virtqueue[i];\n\t\tbool cur_ready = vq_is_ready(dev, vq);\n\n\t\tif (cur_ready != (vq && vq->ready)) {\n\t\t\tvq->ready = cur_ready;\n\t\t\tvhost_user_notify_queue_state(dev, i, cur_ready);\n\t\t}\n\t}\n\n\tif (unlock_required)\n\t\tvhost_user_unlock_all_queue_pairs(dev);\n\n\tif (!virtio_is_ready(dev))\n\t\tgoto out;\n\n\t/*\n\t * Virtio is now ready. If not done already, it is time\n\t * to notify the application it can process the rings and\n\t * configure the vDPA device if present.\n\t */\n\n\tif (!(dev->flags & VIRTIO_DEV_RUNNING)) {\n\t\tif (dev->notify_ops->new_device(dev->vid) == 0)\n\t\t\tdev->flags |= VIRTIO_DEV_RUNNING;\n\t}\n\n\tvdpa_dev = dev->vdpa_dev;\n\tif (!vdpa_dev)\n\t\tgoto out;\n\n\tif (!(dev->flags & VIRTIO_DEV_VDPA_CONFIGURED)) {\n\t\tif (vdpa_dev->ops->dev_conf(dev->vid))\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to configure vDPA device\\n\",\n\t\t\t\t\tdev->ifname);\n\t\telse\n\t\t\tdev->flags |= VIRTIO_DEV_VDPA_CONFIGURED;\n\t}\n\nout:\n\treturn 0;\n}\n\nstatic int process_slave_message_reply(struct virtio_net *dev,\n\t\t\t\t       const struct vhu_msg_context *ctx)\n{\n\tstruct vhu_msg_context msg_reply;\n\tint ret;\n\n\tif ((ctx->msg.flags & VHOST_USER_NEED_REPLY) == 0)\n\t\treturn 0;\n\n\tret = read_vhost_message(dev, dev->slave_req_fd, &msg_reply);\n\tif (ret <= 0) {\n\t\tif (ret < 0)\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) vhost read slave message reply failed\\n\",\n\t\t\t\t\tdev->ifname);\n\t\telse\n\t\t\tVHOST_LOG_CONFIG(INFO, \"(%s) vhost peer closed\\n\", dev->ifname);\n\t\tret = -1;\n\t\tgoto out;\n\t}\n\n\tret = 0;\n\tif (msg_reply.msg.request.slave != ctx->msg.request.slave) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) received unexpected msg type (%u), expected %u\\n\",\n\t\t\t\tdev->ifname, msg_reply.msg.request.slave, ctx->msg.request.slave);\n\t\tret = -1;\n\t\tgoto out;\n\t}\n\n\tret = msg_reply.msg.payload.u64 ? -1 : 0;\n\nout:\n\trte_spinlock_unlock(&dev->slave_req_lock);\n\treturn ret;\n}\n\nint\nvhost_user_iotlb_miss(struct virtio_net *dev, uint64_t iova, uint8_t perm)\n{\n\tint ret;\n\tstruct vhu_msg_context ctx = {\n\t\t.msg = {\n\t\t\t.request.slave = VHOST_USER_SLAVE_IOTLB_MSG,\n\t\t\t.flags = VHOST_USER_VERSION,\n\t\t\t.size = sizeof(ctx.msg.payload.iotlb),\n\t\t\t.payload.iotlb = {\n\t\t\t\t.iova = iova,\n\t\t\t\t.perm = perm,\n\t\t\t\t.type = VHOST_IOTLB_MISS,\n\t\t\t},\n\t\t},\n\t};\n\n\tret = send_vhost_message(dev, dev->slave_req_fd, &ctx);\n\tif (ret < 0) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to send IOTLB miss message (%d)\\n\",\n\t\t\t\tdev->ifname, ret);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nvhost_user_slave_config_change(struct virtio_net *dev, bool need_reply)\n{\n\tint ret;\n\tstruct vhu_msg_context ctx = {\n\t\t.msg = {\n\t\t\t.request.slave = VHOST_USER_SLAVE_CONFIG_CHANGE_MSG,\n\t\t\t.flags = VHOST_USER_VERSION,\n\t\t\t.size = 0,\n\t\t}\n\t};\n\n\tif (need_reply)\n\t\tctx.msg.flags |= VHOST_USER_NEED_REPLY;\n\n\tret = send_vhost_slave_message(dev, &ctx);\n\tif (ret < 0) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to send config change (%d)\\n\",\n\t\t\t\tdev->ifname, ret);\n\t\treturn ret;\n\t}\n\n\treturn process_slave_message_reply(dev, &ctx);\n}\n\nint\nrte_vhost_slave_config_change(int vid, bool need_reply)\n{\n\tstruct virtio_net *dev;\n\n\tdev = get_device(vid);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\treturn vhost_user_slave_config_change(dev, need_reply);\n}\n\nstatic int vhost_user_slave_set_vring_host_notifier(struct virtio_net *dev,\n\t\t\t\t\t\t    int index, int fd,\n\t\t\t\t\t\t    uint64_t offset,\n\t\t\t\t\t\t    uint64_t size)\n{\n\tint ret;\n\tstruct vhu_msg_context ctx = {\n\t\t.msg = {\n\t\t\t.request.slave = VHOST_USER_SLAVE_VRING_HOST_NOTIFIER_MSG,\n\t\t\t.flags = VHOST_USER_VERSION | VHOST_USER_NEED_REPLY,\n\t\t\t.size = sizeof(ctx.msg.payload.area),\n\t\t\t.payload.area = {\n\t\t\t\t.u64 = index & VHOST_USER_VRING_IDX_MASK,\n\t\t\t\t.size = size,\n\t\t\t\t.offset = offset,\n\t\t\t},\n\t\t},\n\t};\n\n\tif (fd < 0)\n\t\tctx.msg.payload.area.u64 |= VHOST_USER_VRING_NOFD_MASK;\n\telse {\n\t\tctx.fds[0] = fd;\n\t\tctx.fd_num = 1;\n\t}\n\n\tret = send_vhost_slave_message(dev, &ctx);\n\tif (ret < 0) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to set host notifier (%d)\\n\",\n\t\t\t\tdev->ifname, ret);\n\t\treturn ret;\n\t}\n\n\treturn process_slave_message_reply(dev, &ctx);\n}\n\nint rte_vhost_host_notifier_ctrl(int vid, uint16_t qid, bool enable)\n{\n\tstruct virtio_net *dev;\n\tstruct rte_vdpa_device *vdpa_dev;\n\tint vfio_device_fd, ret = 0;\n\tuint64_t offset, size;\n\tunsigned int i, q_start, q_last;\n\n\tdev = get_device(vid);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tvdpa_dev = dev->vdpa_dev;\n\tif (vdpa_dev == NULL)\n\t\treturn -ENODEV;\n\n\tif (!(dev->features & (1ULL << VIRTIO_F_VERSION_1)) ||\n\t    !(dev->features & (1ULL << VHOST_USER_F_PROTOCOL_FEATURES)) ||\n\t    !(dev->protocol_features &\n\t\t\t(1ULL << VHOST_USER_PROTOCOL_F_SLAVE_REQ)) ||\n\t    !(dev->protocol_features &\n\t\t\t(1ULL << VHOST_USER_PROTOCOL_F_SLAVE_SEND_FD)) ||\n\t    !(dev->protocol_features &\n\t\t\t(1ULL << VHOST_USER_PROTOCOL_F_HOST_NOTIFIER)))\n\t\treturn -ENOTSUP;\n\n\tif (qid == RTE_VHOST_QUEUE_ALL) {\n\t\tq_start = 0;\n\t\tq_last = dev->nr_vring - 1;\n\t} else {\n\t\tif (qid >= dev->nr_vring)\n\t\t\treturn -EINVAL;\n\t\tq_start = qid;\n\t\tq_last = qid;\n\t}\n\n\tRTE_FUNC_PTR_OR_ERR_RET(vdpa_dev->ops->get_vfio_device_fd, -ENOTSUP);\n\tRTE_FUNC_PTR_OR_ERR_RET(vdpa_dev->ops->get_notify_area, -ENOTSUP);\n\n\tvfio_device_fd = vdpa_dev->ops->get_vfio_device_fd(vid);\n\tif (vfio_device_fd < 0)\n\t\treturn -ENOTSUP;\n\n\tif (enable) {\n\t\tfor (i = q_start; i <= q_last; i++) {\n\t\t\tif (vdpa_dev->ops->get_notify_area(vid, i, &offset,\n\t\t\t\t\t&size) < 0) {\n\t\t\t\tret = -ENOTSUP;\n\t\t\t\tgoto disable;\n\t\t\t}\n\n\t\t\tif (vhost_user_slave_set_vring_host_notifier(dev, i,\n\t\t\t\t\tvfio_device_fd, offset, size) < 0) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto disable;\n\t\t\t}\n\t\t}\n\t} else {\ndisable:\n\t\tfor (i = q_start; i <= q_last; i++) {\n\t\t\tvhost_user_slave_set_vring_host_notifier(dev, i, -1,\n\t\t\t\t\t0, 0);\n\t\t}\n\t}\n\n\treturn ret;\n}\n"], "fixing_code": ["/* SPDX-License-Identifier: BSD-3-Clause\n * Copyright(c) 2010-2018 Intel Corporation\n */\n\n/* Security model\n * --------------\n * The vhost-user protocol connection is an external interface, so it must be\n * robust against invalid inputs.\n *\n * This is important because the vhost-user master is only one step removed\n * from the guest.  Malicious guests that have escaped will then launch further\n * attacks from the vhost-user master.\n *\n * Even in deployments where guests are trusted, a bug in the vhost-user master\n * can still cause invalid messages to be sent.  Such messages must not\n * compromise the stability of the DPDK application by causing crashes, memory\n * corruption, or other problematic behavior.\n *\n * Do not assume received VhostUserMsg fields contain sensible values!\n */\n\n#include <stdint.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <unistd.h>\n#include <fcntl.h>\n#include <sys/ioctl.h>\n#include <sys/mman.h>\n#include <sys/stat.h>\n#include <sys/syscall.h>\n#ifdef RTE_LIBRTE_VHOST_NUMA\n#include <numaif.h>\n#endif\n#ifdef RTE_LIBRTE_VHOST_POSTCOPY\n#include <linux/userfaultfd.h>\n#endif\n#ifdef F_ADD_SEALS /* if file sealing is supported, so is memfd */\n#include <linux/memfd.h>\n#define MEMFD_SUPPORTED\n#endif\n\n#include <rte_common.h>\n#include <rte_malloc.h>\n#include <rte_log.h>\n#include <rte_vfio.h>\n#include <rte_errno.h>\n\n#include \"iotlb.h\"\n#include \"vhost.h\"\n#include \"vhost_user.h\"\n\n#define VIRTIO_MIN_MTU 68\n#define VIRTIO_MAX_MTU 65535\n\n#define INFLIGHT_ALIGNMENT\t64\n#define INFLIGHT_VERSION\t0x1\n\nstatic const char *vhost_message_str[VHOST_USER_MAX] = {\n\t[VHOST_USER_NONE] = \"VHOST_USER_NONE\",\n\t[VHOST_USER_GET_FEATURES] = \"VHOST_USER_GET_FEATURES\",\n\t[VHOST_USER_SET_FEATURES] = \"VHOST_USER_SET_FEATURES\",\n\t[VHOST_USER_SET_OWNER] = \"VHOST_USER_SET_OWNER\",\n\t[VHOST_USER_RESET_OWNER] = \"VHOST_USER_RESET_OWNER\",\n\t[VHOST_USER_SET_MEM_TABLE] = \"VHOST_USER_SET_MEM_TABLE\",\n\t[VHOST_USER_SET_LOG_BASE] = \"VHOST_USER_SET_LOG_BASE\",\n\t[VHOST_USER_SET_LOG_FD] = \"VHOST_USER_SET_LOG_FD\",\n\t[VHOST_USER_SET_VRING_NUM] = \"VHOST_USER_SET_VRING_NUM\",\n\t[VHOST_USER_SET_VRING_ADDR] = \"VHOST_USER_SET_VRING_ADDR\",\n\t[VHOST_USER_SET_VRING_BASE] = \"VHOST_USER_SET_VRING_BASE\",\n\t[VHOST_USER_GET_VRING_BASE] = \"VHOST_USER_GET_VRING_BASE\",\n\t[VHOST_USER_SET_VRING_KICK] = \"VHOST_USER_SET_VRING_KICK\",\n\t[VHOST_USER_SET_VRING_CALL] = \"VHOST_USER_SET_VRING_CALL\",\n\t[VHOST_USER_SET_VRING_ERR]  = \"VHOST_USER_SET_VRING_ERR\",\n\t[VHOST_USER_GET_PROTOCOL_FEATURES]  = \"VHOST_USER_GET_PROTOCOL_FEATURES\",\n\t[VHOST_USER_SET_PROTOCOL_FEATURES]  = \"VHOST_USER_SET_PROTOCOL_FEATURES\",\n\t[VHOST_USER_GET_QUEUE_NUM]  = \"VHOST_USER_GET_QUEUE_NUM\",\n\t[VHOST_USER_SET_VRING_ENABLE]  = \"VHOST_USER_SET_VRING_ENABLE\",\n\t[VHOST_USER_SEND_RARP]  = \"VHOST_USER_SEND_RARP\",\n\t[VHOST_USER_NET_SET_MTU]  = \"VHOST_USER_NET_SET_MTU\",\n\t[VHOST_USER_SET_SLAVE_REQ_FD]  = \"VHOST_USER_SET_SLAVE_REQ_FD\",\n\t[VHOST_USER_IOTLB_MSG]  = \"VHOST_USER_IOTLB_MSG\",\n\t[VHOST_USER_CRYPTO_CREATE_SESS] = \"VHOST_USER_CRYPTO_CREATE_SESS\",\n\t[VHOST_USER_CRYPTO_CLOSE_SESS] = \"VHOST_USER_CRYPTO_CLOSE_SESS\",\n\t[VHOST_USER_POSTCOPY_ADVISE]  = \"VHOST_USER_POSTCOPY_ADVISE\",\n\t[VHOST_USER_POSTCOPY_LISTEN]  = \"VHOST_USER_POSTCOPY_LISTEN\",\n\t[VHOST_USER_POSTCOPY_END]  = \"VHOST_USER_POSTCOPY_END\",\n\t[VHOST_USER_GET_INFLIGHT_FD] = \"VHOST_USER_GET_INFLIGHT_FD\",\n\t[VHOST_USER_SET_INFLIGHT_FD] = \"VHOST_USER_SET_INFLIGHT_FD\",\n\t[VHOST_USER_SET_STATUS] = \"VHOST_USER_SET_STATUS\",\n\t[VHOST_USER_GET_STATUS] = \"VHOST_USER_GET_STATUS\",\n};\n\nstatic int send_vhost_reply(struct virtio_net *dev, int sockfd, struct vhu_msg_context *ctx);\nstatic int read_vhost_message(struct virtio_net *dev, int sockfd, struct vhu_msg_context *ctx);\n\nstatic void\nclose_msg_fds(struct vhu_msg_context *ctx)\n{\n\tint i;\n\n\tfor (i = 0; i < ctx->fd_num; i++) {\n\t\tint fd = ctx->fds[i];\n\n\t\tif (fd == -1)\n\t\t\tcontinue;\n\n\t\tctx->fds[i] = -1;\n\t\tclose(fd);\n\t}\n}\n\n/*\n * Ensure the expected number of FDs is received,\n * close all FDs and return an error if this is not the case.\n */\nstatic int\nvalidate_msg_fds(struct virtio_net *dev, struct vhu_msg_context *ctx, int expected_fds)\n{\n\tif (ctx->fd_num == expected_fds)\n\t\treturn 0;\n\n\tVHOST_LOG_CONFIG(ERR, \"(%s) expect %d FDs for request %s, received %d\\n\",\n\t\tdev->ifname, expected_fds,\n\t\tvhost_message_str[ctx->msg.request.master],\n\t\tctx->fd_num);\n\n\tclose_msg_fds(ctx);\n\n\treturn -1;\n}\n\nstatic uint64_t\nget_blk_size(int fd)\n{\n\tstruct stat stat;\n\tint ret;\n\n\tret = fstat(fd, &stat);\n\treturn ret == -1 ? (uint64_t)-1 : (uint64_t)stat.st_blksize;\n}\n\nstatic void\nasync_dma_map(struct virtio_net *dev, bool do_map)\n{\n\tint ret = 0;\n\tuint32_t i;\n\tstruct guest_page *page;\n\n\tif (do_map) {\n\t\tfor (i = 0; i < dev->nr_guest_pages; i++) {\n\t\t\tpage = &dev->guest_pages[i];\n\t\t\tret = rte_vfio_container_dma_map(RTE_VFIO_DEFAULT_CONTAINER_FD,\n\t\t\t\t\t\t\t page->host_user_addr,\n\t\t\t\t\t\t\t page->host_iova,\n\t\t\t\t\t\t\t page->size);\n\t\t\tif (ret) {\n\t\t\t\t/*\n\t\t\t\t * DMA device may bind with kernel driver, in this case,\n\t\t\t\t * we don't need to program IOMMU manually. However, if no\n\t\t\t\t * device is bound with vfio/uio in DPDK, and vfio kernel\n\t\t\t\t * module is loaded, the API will still be called and return\n\t\t\t\t * with ENODEV.\n\t\t\t\t *\n\t\t\t\t * DPDK vfio only returns ENODEV in very similar situations\n\t\t\t\t * (vfio either unsupported, or supported but no devices found).\n\t\t\t\t * Either way, no mappings could be performed. We treat it as\n\t\t\t\t * normal case in async path. This is a workaround.\n\t\t\t\t */\n\t\t\t\tif (rte_errno == ENODEV)\n\t\t\t\t\treturn;\n\n\t\t\t\t/* DMA mapping errors won't stop VHOST_USER_SET_MEM_TABLE. */\n\t\t\t\tVHOST_LOG_CONFIG(ERR, \"DMA engine map failed\\n\");\n\t\t\t}\n\t\t}\n\n\t} else {\n\t\tfor (i = 0; i < dev->nr_guest_pages; i++) {\n\t\t\tpage = &dev->guest_pages[i];\n\t\t\tret = rte_vfio_container_dma_unmap(RTE_VFIO_DEFAULT_CONTAINER_FD,\n\t\t\t\t\t\t\t   page->host_user_addr,\n\t\t\t\t\t\t\t   page->host_iova,\n\t\t\t\t\t\t\t   page->size);\n\t\t\tif (ret) {\n\t\t\t\t/* like DMA map, ignore the kernel driver case when unmap. */\n\t\t\t\tif (rte_errno == EINVAL)\n\t\t\t\t\treturn;\n\n\t\t\t\tVHOST_LOG_CONFIG(ERR, \"DMA engine unmap failed\\n\");\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void\nfree_mem_region(struct virtio_net *dev)\n{\n\tuint32_t i;\n\tstruct rte_vhost_mem_region *reg;\n\n\tif (!dev || !dev->mem)\n\t\treturn;\n\n\tif (dev->async_copy && rte_vfio_is_enabled(\"vfio\"))\n\t\tasync_dma_map(dev, false);\n\n\tfor (i = 0; i < dev->mem->nregions; i++) {\n\t\treg = &dev->mem->regions[i];\n\t\tif (reg->host_user_addr) {\n\t\t\tmunmap(reg->mmap_addr, reg->mmap_size);\n\t\t\tclose(reg->fd);\n\t\t}\n\t}\n}\n\nvoid\nvhost_backend_cleanup(struct virtio_net *dev)\n{\n\tstruct rte_vdpa_device *vdpa_dev;\n\n\tvdpa_dev = dev->vdpa_dev;\n\tif (vdpa_dev && vdpa_dev->ops->dev_cleanup != NULL)\n\t\tvdpa_dev->ops->dev_cleanup(dev->vid);\n\n\tif (dev->mem) {\n\t\tfree_mem_region(dev);\n\t\trte_free(dev->mem);\n\t\tdev->mem = NULL;\n\t}\n\n\trte_free(dev->guest_pages);\n\tdev->guest_pages = NULL;\n\n\tif (dev->log_addr) {\n\t\tmunmap((void *)(uintptr_t)dev->log_addr, dev->log_size);\n\t\tdev->log_addr = 0;\n\t}\n\n\tif (dev->inflight_info) {\n\t\tif (dev->inflight_info->addr) {\n\t\t\tmunmap(dev->inflight_info->addr,\n\t\t\t       dev->inflight_info->size);\n\t\t\tdev->inflight_info->addr = NULL;\n\t\t}\n\n\t\tif (dev->inflight_info->fd >= 0) {\n\t\t\tclose(dev->inflight_info->fd);\n\t\t\tdev->inflight_info->fd = -1;\n\t\t}\n\n\t\trte_free(dev->inflight_info);\n\t\tdev->inflight_info = NULL;\n\t}\n\n\tif (dev->slave_req_fd >= 0) {\n\t\tclose(dev->slave_req_fd);\n\t\tdev->slave_req_fd = -1;\n\t}\n\n\tif (dev->postcopy_ufd >= 0) {\n\t\tclose(dev->postcopy_ufd);\n\t\tdev->postcopy_ufd = -1;\n\t}\n\n\tdev->postcopy_listening = 0;\n}\n\nstatic void\nvhost_user_notify_queue_state(struct virtio_net *dev, uint16_t index,\n\t\t\t      int enable)\n{\n\tstruct rte_vdpa_device *vdpa_dev = dev->vdpa_dev;\n\tstruct vhost_virtqueue *vq = dev->virtqueue[index];\n\n\t/* Configure guest notifications on enable */\n\tif (enable && vq->notif_enable != VIRTIO_UNINITIALIZED_NOTIF)\n\t\tvhost_enable_guest_notification(dev, vq, vq->notif_enable);\n\n\tif (vdpa_dev && vdpa_dev->ops->set_vring_state)\n\t\tvdpa_dev->ops->set_vring_state(dev->vid, index, enable);\n\n\tif (dev->notify_ops->vring_state_changed)\n\t\tdev->notify_ops->vring_state_changed(dev->vid,\n\t\t\t\tindex, enable);\n}\n\n/*\n * This function just returns success at the moment unless\n * the device hasn't been initialised.\n */\nstatic int\nvhost_user_set_owner(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_user_reset_owner(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tvhost_destroy_device_notify(dev);\n\n\tcleanup_device(dev, 0);\n\treset_device(dev);\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\n/*\n * The features that we support are requested.\n */\nstatic int\nvhost_user_get_features(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tuint64_t features = 0;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\trte_vhost_driver_get_features(dev->ifname, &features);\n\n\tctx->msg.payload.u64 = features;\n\tctx->msg.size = sizeof(ctx->msg.payload.u64);\n\tctx->fd_num = 0;\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n}\n\n/*\n * The queue number that we support are requested.\n */\nstatic int\nvhost_user_get_queue_num(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tuint32_t queue_num = 0;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\trte_vhost_driver_get_queue_num(dev->ifname, &queue_num);\n\n\tctx->msg.payload.u64 = (uint64_t)queue_num;\n\tctx->msg.size = sizeof(ctx->msg.payload.u64);\n\tctx->fd_num = 0;\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n}\n\n/*\n * We receive the negotiated features supported by us and the virtio device.\n */\nstatic int\nvhost_user_set_features(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tuint64_t features = ctx->msg.payload.u64;\n\tuint64_t vhost_features = 0;\n\tstruct rte_vdpa_device *vdpa_dev;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\trte_vhost_driver_get_features(dev->ifname, &vhost_features);\n\tif (features & ~vhost_features) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) received invalid negotiated features.\\n\",\n\t\t\tdev->ifname);\n\t\tdev->flags |= VIRTIO_DEV_FEATURES_FAILED;\n\t\tdev->status &= ~VIRTIO_DEVICE_STATUS_FEATURES_OK;\n\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tif (dev->flags & VIRTIO_DEV_RUNNING) {\n\t\tif (dev->features == features)\n\t\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\n\t\t/*\n\t\t * Error out if master tries to change features while device is\n\t\t * in running state. The exception being VHOST_F_LOG_ALL, which\n\t\t * is enabled when the live-migration starts.\n\t\t */\n\t\tif ((dev->features ^ features) & ~(1ULL << VHOST_F_LOG_ALL)) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) features changed while device is running.\\n\",\n\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\n\t\tif (dev->notify_ops->features_changed)\n\t\t\tdev->notify_ops->features_changed(dev->vid, features);\n\t}\n\n\tdev->features = features;\n\tif (dev->features &\n\t\t((1ULL << VIRTIO_NET_F_MRG_RXBUF) |\n\t\t (1ULL << VIRTIO_F_VERSION_1) |\n\t\t (1ULL << VIRTIO_F_RING_PACKED))) {\n\t\tdev->vhost_hlen = sizeof(struct virtio_net_hdr_mrg_rxbuf);\n\t} else {\n\t\tdev->vhost_hlen = sizeof(struct virtio_net_hdr);\n\t}\n\tVHOST_LOG_CONFIG(INFO, \"(%s) negotiated Virtio features: 0x%\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, dev->features);\n\tVHOST_LOG_CONFIG(DEBUG, \"(%s) mergeable RX buffers %s, virtio 1 %s\\n\",\n\t\tdev->ifname,\n\t\t(dev->features & (1 << VIRTIO_NET_F_MRG_RXBUF)) ? \"on\" : \"off\",\n\t\t(dev->features & (1ULL << VIRTIO_F_VERSION_1)) ? \"on\" : \"off\");\n\n\tif ((dev->flags & VIRTIO_DEV_BUILTIN_VIRTIO_NET) &&\n\t    !(dev->features & (1ULL << VIRTIO_NET_F_MQ))) {\n\t\t/*\n\t\t * Remove all but first queue pair if MQ hasn't been\n\t\t * negotiated. This is safe because the device is not\n\t\t * running at this stage.\n\t\t */\n\t\twhile (dev->nr_vring > 2) {\n\t\t\tstruct vhost_virtqueue *vq;\n\n\t\t\tvq = dev->virtqueue[--dev->nr_vring];\n\t\t\tif (!vq)\n\t\t\t\tcontinue;\n\n\t\t\tdev->virtqueue[dev->nr_vring] = NULL;\n\t\t\tcleanup_vq(vq, 1);\n\t\t\tcleanup_vq_inflight(dev, vq);\n\t\t\tfree_vq(dev, vq);\n\t\t}\n\t}\n\n\tvdpa_dev = dev->vdpa_dev;\n\tif (vdpa_dev)\n\t\tvdpa_dev->ops->set_features(dev->vid);\n\n\tdev->flags &= ~VIRTIO_DEV_FEATURES_FAILED;\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\n/*\n * The virtio device sends us the size of the descriptor ring.\n */\nstatic int\nvhost_user_set_vring_num(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tstruct vhost_virtqueue *vq = dev->virtqueue[ctx->msg.payload.state.index];\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (ctx->msg.payload.state.num > 32768) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid virtqueue size %u\\n\",\n\t\t\t\tdev->ifname, ctx->msg.payload.state.num);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tvq->size = ctx->msg.payload.state.num;\n\n\t/* VIRTIO 1.0, 2.4 Virtqueues says:\n\t *\n\t *   Queue Size value is always a power of 2. The maximum Queue Size\n\t *   value is 32768.\n\t *\n\t * VIRTIO 1.1 2.7 Virtqueues says:\n\t *\n\t *   Packed virtqueues support up to 2^15 entries each.\n\t */\n\tif (!vq_is_packed(dev)) {\n\t\tif (vq->size & (vq->size - 1)) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid virtqueue size %u\\n\",\n\t\t\t\t\tdev->ifname, vq->size);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\t}\n\n\tif (vq_is_packed(dev)) {\n\t\trte_free(vq->shadow_used_packed);\n\t\tvq->shadow_used_packed = rte_malloc_socket(NULL,\n\t\t\t\tvq->size *\n\t\t\t\tsizeof(struct vring_used_elem_packed),\n\t\t\t\tRTE_CACHE_LINE_SIZE, vq->numa_node);\n\t\tif (!vq->shadow_used_packed) {\n\t\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\"(%s) failed to allocate memory for shadow used ring.\\n\",\n\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\n\t} else {\n\t\trte_free(vq->shadow_used_split);\n\n\t\tvq->shadow_used_split = rte_malloc_socket(NULL,\n\t\t\t\tvq->size * sizeof(struct vring_used_elem),\n\t\t\t\tRTE_CACHE_LINE_SIZE, vq->numa_node);\n\n\t\tif (!vq->shadow_used_split) {\n\t\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\"(%s) failed to allocate memory for vq internal data.\\n\",\n\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\t}\n\n\trte_free(vq->batch_copy_elems);\n\tvq->batch_copy_elems = rte_malloc_socket(NULL,\n\t\t\t\tvq->size * sizeof(struct batch_copy_elem),\n\t\t\t\tRTE_CACHE_LINE_SIZE, vq->numa_node);\n\tif (!vq->batch_copy_elems) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to allocate memory for batching copy.\\n\",\n\t\t\tdev->ifname);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\n/*\n * Reallocate virtio_dev, vhost_virtqueue and related data structures to\n * make them on the same numa node as the memory of vring descriptor.\n */\n#ifdef RTE_LIBRTE_VHOST_NUMA\nstatic struct virtio_net*\nnuma_realloc(struct virtio_net *dev, int index)\n{\n\tint node, dev_node;\n\tstruct virtio_net *old_dev;\n\tstruct vhost_virtqueue *vq;\n\tstruct batch_copy_elem *bce;\n\tstruct guest_page *gp;\n\tstruct rte_vhost_memory *mem;\n\tsize_t mem_size;\n\tint ret;\n\n\told_dev = dev;\n\tvq = dev->virtqueue[index];\n\n\t/*\n\t * If VQ is ready, it is too late to reallocate, it certainly already\n\t * happened anyway on VHOST_USER_SET_VRING_ADRR.\n\t */\n\tif (vq->ready)\n\t\treturn dev;\n\n\tret = get_mempolicy(&node, NULL, 0, vq->desc, MPOL_F_NODE | MPOL_F_ADDR);\n\tif (ret) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) unable to get virtqueue %d numa information.\\n\",\n\t\t\t\tdev->ifname, index);\n\t\treturn dev;\n\t}\n\n\tif (node == vq->numa_node)\n\t\tgoto out_dev_realloc;\n\n\tvq = rte_realloc_socket(vq, sizeof(*vq), 0, node);\n\tif (!vq) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc virtqueue %d on node %d\\n\",\n\t\t\t\tdev->ifname, index, node);\n\t\treturn dev;\n\t}\n\n\tif (vq != dev->virtqueue[index]) {\n\t\tVHOST_LOG_CONFIG(INFO, \"(%s) reallocated virtqueue on node %d\\n\",\n\t\t\t\tdev->ifname, node);\n\t\tdev->virtqueue[index] = vq;\n\t\tvhost_user_iotlb_init(dev, index);\n\t}\n\n\tif (vq_is_packed(dev)) {\n\t\tstruct vring_used_elem_packed *sup;\n\n\t\tsup = rte_realloc_socket(vq->shadow_used_packed, vq->size * sizeof(*sup),\n\t\t\t\tRTE_CACHE_LINE_SIZE, node);\n\t\tif (!sup) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc shadow packed on node %d\\n\",\n\t\t\t\t\tdev->ifname, node);\n\t\t\treturn dev;\n\t\t}\n\t\tvq->shadow_used_packed = sup;\n\t} else {\n\t\tstruct vring_used_elem *sus;\n\n\t\tsus = rte_realloc_socket(vq->shadow_used_split, vq->size * sizeof(*sus),\n\t\t\t\tRTE_CACHE_LINE_SIZE, node);\n\t\tif (!sus) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc shadow split on node %d\\n\",\n\t\t\t\t\tdev->ifname, node);\n\t\t\treturn dev;\n\t\t}\n\t\tvq->shadow_used_split = sus;\n\t}\n\n\tbce = rte_realloc_socket(vq->batch_copy_elems, vq->size * sizeof(*bce),\n\t\t\tRTE_CACHE_LINE_SIZE, node);\n\tif (!bce) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc batch copy elem on node %d\\n\",\n\t\t\t\tdev->ifname, node);\n\t\treturn dev;\n\t}\n\tvq->batch_copy_elems = bce;\n\n\tif (vq->log_cache) {\n\t\tstruct log_cache_entry *lc;\n\n\t\tlc = rte_realloc_socket(vq->log_cache, sizeof(*lc) * VHOST_LOG_CACHE_NR, 0, node);\n\t\tif (!lc) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc log cache on node %d\\n\",\n\t\t\t\t\tdev->ifname, node);\n\t\t\treturn dev;\n\t\t}\n\t\tvq->log_cache = lc;\n\t}\n\n\tif (vq->resubmit_inflight) {\n\t\tstruct rte_vhost_resubmit_info *ri;\n\n\t\tri = rte_realloc_socket(vq->resubmit_inflight, sizeof(*ri), 0, node);\n\t\tif (!ri) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc resubmit inflight on node %d\\n\",\n\t\t\t\t\tdev->ifname, node);\n\t\t\treturn dev;\n\t\t}\n\t\tvq->resubmit_inflight = ri;\n\n\t\tif (ri->resubmit_list) {\n\t\t\tstruct rte_vhost_resubmit_desc *rd;\n\n\t\t\trd = rte_realloc_socket(ri->resubmit_list, sizeof(*rd) * ri->resubmit_num,\n\t\t\t\t\t0, node);\n\t\t\tif (!rd) {\n\t\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc resubmit list on node %d\\n\",\n\t\t\t\t\t\tdev->ifname, node);\n\t\t\t\treturn dev;\n\t\t\t}\n\t\t\tri->resubmit_list = rd;\n\t\t}\n\t}\n\n\tvq->numa_node = node;\n\nout_dev_realloc:\n\n\tif (dev->flags & VIRTIO_DEV_RUNNING)\n\t\treturn dev;\n\n\tret = get_mempolicy(&dev_node, NULL, 0, dev, MPOL_F_NODE | MPOL_F_ADDR);\n\tif (ret) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) unable to get numa information.\\n\", dev->ifname);\n\t\treturn dev;\n\t}\n\n\tif (dev_node == node)\n\t\treturn dev;\n\n\tdev = rte_realloc_socket(old_dev, sizeof(*dev), 0, node);\n\tif (!dev) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc dev on node %d\\n\",\n\t\t\t\told_dev->ifname, node);\n\t\treturn old_dev;\n\t}\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) reallocated device on node %d\\n\", dev->ifname, node);\n\tvhost_devices[dev->vid] = dev;\n\n\tmem_size = sizeof(struct rte_vhost_memory) +\n\t\tsizeof(struct rte_vhost_mem_region) * dev->mem->nregions;\n\tmem = rte_realloc_socket(dev->mem, mem_size, 0, node);\n\tif (!mem) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc mem table on node %d\\n\",\n\t\t\t\tdev->ifname, node);\n\t\treturn dev;\n\t}\n\tdev->mem = mem;\n\n\tgp = rte_realloc_socket(dev->guest_pages, dev->max_guest_pages * sizeof(*gp),\n\t\t\tRTE_CACHE_LINE_SIZE, node);\n\tif (!gp) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to realloc guest pages on node %d\\n\",\n\t\t\t\tdev->ifname, node);\n\t\treturn dev;\n\t}\n\tdev->guest_pages = gp;\n\n\treturn dev;\n}\n#else\nstatic struct virtio_net*\nnuma_realloc(struct virtio_net *dev, int index __rte_unused)\n{\n\treturn dev;\n}\n#endif\n\n/* Converts QEMU virtual address to Vhost virtual address. */\nstatic uint64_t\nqva_to_vva(struct virtio_net *dev, uint64_t qva, uint64_t *len)\n{\n\tstruct rte_vhost_mem_region *r;\n\tuint32_t i;\n\n\tif (unlikely(!dev || !dev->mem))\n\t\tgoto out_error;\n\n\t/* Find the region where the address lives. */\n\tfor (i = 0; i < dev->mem->nregions; i++) {\n\t\tr = &dev->mem->regions[i];\n\n\t\tif (qva >= r->guest_user_addr &&\n\t\t    qva <  r->guest_user_addr + r->size) {\n\n\t\t\tif (unlikely(*len > r->guest_user_addr + r->size - qva))\n\t\t\t\t*len = r->guest_user_addr + r->size - qva;\n\n\t\t\treturn qva - r->guest_user_addr +\n\t\t\t       r->host_user_addr;\n\t\t}\n\t}\nout_error:\n\t*len = 0;\n\n\treturn 0;\n}\n\n\n/*\n * Converts ring address to Vhost virtual address.\n * If IOMMU is enabled, the ring address is a guest IO virtual address,\n * else it is a QEMU virtual address.\n */\nstatic uint64_t\nring_addr_to_vva(struct virtio_net *dev, struct vhost_virtqueue *vq,\n\t\tuint64_t ra, uint64_t *size)\n{\n\tif (dev->features & (1ULL << VIRTIO_F_IOMMU_PLATFORM)) {\n\t\tuint64_t vva;\n\n\t\tvhost_user_iotlb_rd_lock(vq);\n\t\tvva = vhost_iova_to_vva(dev, vq, ra,\n\t\t\t\t\tsize, VHOST_ACCESS_RW);\n\t\tvhost_user_iotlb_rd_unlock(vq);\n\n\t\treturn vva;\n\t}\n\n\treturn qva_to_vva(dev, ra, size);\n}\n\nstatic uint64_t\nlog_addr_to_gpa(struct virtio_net *dev, struct vhost_virtqueue *vq)\n{\n\tuint64_t log_gpa;\n\n\tvhost_user_iotlb_rd_lock(vq);\n\tlog_gpa = translate_log_addr(dev, vq, vq->ring_addrs.log_guest_addr);\n\tvhost_user_iotlb_rd_unlock(vq);\n\n\treturn log_gpa;\n}\n\nstatic struct virtio_net *\ntranslate_ring_addresses(struct virtio_net *dev, int vq_index)\n{\n\tstruct vhost_virtqueue *vq = dev->virtqueue[vq_index];\n\tstruct vhost_vring_addr *addr = &vq->ring_addrs;\n\tuint64_t len, expected_len;\n\n\tif (addr->flags & (1 << VHOST_VRING_F_LOG)) {\n\t\tvq->log_guest_addr =\n\t\t\tlog_addr_to_gpa(dev, vq);\n\t\tif (vq->log_guest_addr == 0) {\n\t\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) failed to map log_guest_addr.\\n\",\n\t\t\t\tdev->ifname);\n\t\t\treturn dev;\n\t\t}\n\t}\n\n\tif (vq_is_packed(dev)) {\n\t\tlen = sizeof(struct vring_packed_desc) * vq->size;\n\t\tvq->desc_packed = (struct vring_packed_desc *)(uintptr_t)\n\t\t\tring_addr_to_vva(dev, vq, addr->desc_user_addr, &len);\n\t\tif (vq->desc_packed == NULL ||\n\t\t\t\tlen != sizeof(struct vring_packed_desc) *\n\t\t\t\tvq->size) {\n\t\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) failed to map desc_packed ring.\\n\",\n\t\t\t\tdev->ifname);\n\t\t\treturn dev;\n\t\t}\n\n\t\tdev = numa_realloc(dev, vq_index);\n\t\tvq = dev->virtqueue[vq_index];\n\t\taddr = &vq->ring_addrs;\n\n\t\tlen = sizeof(struct vring_packed_desc_event);\n\t\tvq->driver_event = (struct vring_packed_desc_event *)\n\t\t\t\t\t(uintptr_t)ring_addr_to_vva(dev,\n\t\t\t\t\tvq, addr->avail_user_addr, &len);\n\t\tif (vq->driver_event == NULL ||\n\t\t\t\tlen != sizeof(struct vring_packed_desc_event)) {\n\t\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) failed to find driver area address.\\n\",\n\t\t\t\tdev->ifname);\n\t\t\treturn dev;\n\t\t}\n\n\t\tlen = sizeof(struct vring_packed_desc_event);\n\t\tvq->device_event = (struct vring_packed_desc_event *)\n\t\t\t\t\t(uintptr_t)ring_addr_to_vva(dev,\n\t\t\t\t\tvq, addr->used_user_addr, &len);\n\t\tif (vq->device_event == NULL ||\n\t\t\t\tlen != sizeof(struct vring_packed_desc_event)) {\n\t\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) failed to find device area address.\\n\",\n\t\t\t\tdev->ifname);\n\t\t\treturn dev;\n\t\t}\n\n\t\tvq->access_ok = true;\n\t\treturn dev;\n\t}\n\n\t/* The addresses are converted from QEMU virtual to Vhost virtual. */\n\tif (vq->desc && vq->avail && vq->used)\n\t\treturn dev;\n\n\tlen = sizeof(struct vring_desc) * vq->size;\n\tvq->desc = (struct vring_desc *)(uintptr_t)ring_addr_to_vva(dev,\n\t\t\tvq, addr->desc_user_addr, &len);\n\tif (vq->desc == 0 || len != sizeof(struct vring_desc) * vq->size) {\n\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) failed to map desc ring.\\n\", dev->ifname);\n\t\treturn dev;\n\t}\n\n\tdev = numa_realloc(dev, vq_index);\n\tvq = dev->virtqueue[vq_index];\n\taddr = &vq->ring_addrs;\n\n\tlen = sizeof(struct vring_avail) + sizeof(uint16_t) * vq->size;\n\tif (dev->features & (1ULL << VIRTIO_RING_F_EVENT_IDX))\n\t\tlen += sizeof(uint16_t);\n\texpected_len = len;\n\tvq->avail = (struct vring_avail *)(uintptr_t)ring_addr_to_vva(dev,\n\t\t\tvq, addr->avail_user_addr, &len);\n\tif (vq->avail == 0 || len != expected_len) {\n\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) failed to map avail ring.\\n\", dev->ifname);\n\t\treturn dev;\n\t}\n\n\tlen = sizeof(struct vring_used) +\n\t\tsizeof(struct vring_used_elem) * vq->size;\n\tif (dev->features & (1ULL << VIRTIO_RING_F_EVENT_IDX))\n\t\tlen += sizeof(uint16_t);\n\texpected_len = len;\n\tvq->used = (struct vring_used *)(uintptr_t)ring_addr_to_vva(dev,\n\t\t\tvq, addr->used_user_addr, &len);\n\tif (vq->used == 0 || len != expected_len) {\n\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) failed to map used ring.\\n\", dev->ifname);\n\t\treturn dev;\n\t}\n\n\tif (vq->last_used_idx != vq->used->idx) {\n\t\tVHOST_LOG_CONFIG(WARNING, \"(%s) last_used_idx (%u) and vq->used->idx (%u) mismatches;\\n\",\n\t\t\tdev->ifname,\n\t\t\tvq->last_used_idx, vq->used->idx);\n\t\tvq->last_used_idx  = vq->used->idx;\n\t\tvq->last_avail_idx = vq->used->idx;\n\t\tVHOST_LOG_CONFIG(WARNING, \"(%s) some packets maybe resent for Tx and dropped for Rx\\n\",\n\t\t\tdev->ifname);\n\t}\n\n\tvq->access_ok = true;\n\n\tVHOST_LOG_CONFIG(DEBUG, \"(%s) mapped address desc: %p\\n\", dev->ifname, vq->desc);\n\tVHOST_LOG_CONFIG(DEBUG, \"(%s) mapped address avail: %p\\n\", dev->ifname, vq->avail);\n\tVHOST_LOG_CONFIG(DEBUG, \"(%s) mapped address used: %p\\n\", dev->ifname, vq->used);\n\tVHOST_LOG_CONFIG(DEBUG, \"(%s) log_guest_addr: %\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, vq->log_guest_addr);\n\n\treturn dev;\n}\n\n/*\n * The virtio device sends us the desc, used and avail ring addresses.\n * This function then converts these to our address space.\n */\nstatic int\nvhost_user_set_vring_addr(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tstruct vhost_virtqueue *vq;\n\tstruct vhost_vring_addr *addr = &ctx->msg.payload.addr;\n\tbool access_ok;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (dev->mem == NULL)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\t/* addr->index refers to the queue index. The txq 1, rxq is 0. */\n\tvq = dev->virtqueue[ctx->msg.payload.addr.index];\n\n\taccess_ok = vq->access_ok;\n\n\t/*\n\t * Rings addresses should not be interpreted as long as the ring is not\n\t * started and enabled\n\t */\n\tmemcpy(&vq->ring_addrs, addr, sizeof(*addr));\n\n\tvring_invalidate(dev, vq);\n\n\tif ((vq->enabled && (dev->features &\n\t\t\t\t(1ULL << VHOST_USER_F_PROTOCOL_FEATURES))) ||\n\t\t\taccess_ok) {\n\t\tdev = translate_ring_addresses(dev, ctx->msg.payload.addr.index);\n\t\tif (!dev)\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\t\t*pdev = dev;\n\t}\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\n/*\n * The virtio device sends us the available ring last used index.\n */\nstatic int\nvhost_user_set_vring_base(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tstruct vhost_virtqueue *vq = dev->virtqueue[ctx->msg.payload.state.index];\n\tuint64_t val = ctx->msg.payload.state.num;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (vq_is_packed(dev)) {\n\t\t/*\n\t\t * Bit[0:14]: avail index\n\t\t * Bit[15]: avail wrap counter\n\t\t */\n\t\tvq->last_avail_idx = val & 0x7fff;\n\t\tvq->avail_wrap_counter = !!(val & (0x1 << 15));\n\t\t/*\n\t\t * Set used index to same value as available one, as\n\t\t * their values should be the same since ring processing\n\t\t * was stopped at get time.\n\t\t */\n\t\tvq->last_used_idx = vq->last_avail_idx;\n\t\tvq->used_wrap_counter = vq->avail_wrap_counter;\n\t} else {\n\t\tvq->last_used_idx = ctx->msg.payload.state.num;\n\t\tvq->last_avail_idx = ctx->msg.payload.state.num;\n\t}\n\n\tVHOST_LOG_CONFIG(INFO,\n\t\t\"(%s) vring base idx:%u last_used_idx:%u last_avail_idx:%u.\\n\",\n\t\tdev->ifname, ctx->msg.payload.state.index, vq->last_used_idx,\n\t\tvq->last_avail_idx);\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nadd_one_guest_page(struct virtio_net *dev, uint64_t guest_phys_addr,\n\t\t   uint64_t host_iova, uint64_t host_user_addr, uint64_t size)\n{\n\tstruct guest_page *page, *last_page;\n\tstruct guest_page *old_pages;\n\n\tif (dev->nr_guest_pages == dev->max_guest_pages) {\n\t\tdev->max_guest_pages *= 2;\n\t\told_pages = dev->guest_pages;\n\t\tdev->guest_pages = rte_realloc(dev->guest_pages,\n\t\t\t\t\tdev->max_guest_pages * sizeof(*page),\n\t\t\t\t\tRTE_CACHE_LINE_SIZE);\n\t\tif (dev->guest_pages == NULL) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"cannot realloc guest_pages\\n\");\n\t\t\trte_free(old_pages);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tif (dev->nr_guest_pages > 0) {\n\t\tlast_page = &dev->guest_pages[dev->nr_guest_pages - 1];\n\t\t/* merge if the two pages are continuous */\n\t\tif (host_iova == last_page->host_iova + last_page->size &&\n\t\t    guest_phys_addr == last_page->guest_phys_addr + last_page->size &&\n\t\t    host_user_addr == last_page->host_user_addr + last_page->size) {\n\t\t\tlast_page->size += size;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tpage = &dev->guest_pages[dev->nr_guest_pages++];\n\tpage->guest_phys_addr = guest_phys_addr;\n\tpage->host_iova  = host_iova;\n\tpage->host_user_addr = host_user_addr;\n\tpage->size = size;\n\n\treturn 0;\n}\n\nstatic int\nadd_guest_pages(struct virtio_net *dev, struct rte_vhost_mem_region *reg,\n\t\tuint64_t page_size)\n{\n\tuint64_t reg_size = reg->size;\n\tuint64_t host_user_addr  = reg->host_user_addr;\n\tuint64_t guest_phys_addr = reg->guest_phys_addr;\n\tuint64_t host_iova;\n\tuint64_t size;\n\n\thost_iova = rte_mem_virt2iova((void *)(uintptr_t)host_user_addr);\n\tsize = page_size - (guest_phys_addr & (page_size - 1));\n\tsize = RTE_MIN(size, reg_size);\n\n\tif (add_one_guest_page(dev, guest_phys_addr, host_iova,\n\t\t\t       host_user_addr, size) < 0)\n\t\treturn -1;\n\n\thost_user_addr  += size;\n\tguest_phys_addr += size;\n\treg_size -= size;\n\n\twhile (reg_size > 0) {\n\t\tsize = RTE_MIN(reg_size, page_size);\n\t\thost_iova = rte_mem_virt2iova((void *)(uintptr_t)\n\t\t\t\t\t\t  host_user_addr);\n\t\tif (add_one_guest_page(dev, guest_phys_addr, host_iova,\n\t\t\t\t       host_user_addr, size) < 0)\n\t\t\treturn -1;\n\n\t\thost_user_addr  += size;\n\t\tguest_phys_addr += size;\n\t\treg_size -= size;\n\t}\n\n\t/* sort guest page array if over binary search threshold */\n\tif (dev->nr_guest_pages >= VHOST_BINARY_SEARCH_THRESH) {\n\t\tqsort((void *)dev->guest_pages, dev->nr_guest_pages,\n\t\t\tsizeof(struct guest_page), guest_page_addrcmp);\n\t}\n\n\treturn 0;\n}\n\n#ifdef RTE_LIBRTE_VHOST_DEBUG\n/* TODO: enable it only in debug mode? */\nstatic void\ndump_guest_pages(struct virtio_net *dev)\n{\n\tuint32_t i;\n\tstruct guest_page *page;\n\n\tfor (i = 0; i < dev->nr_guest_pages; i++) {\n\t\tpage = &dev->guest_pages[i];\n\n\t\tVHOST_LOG_CONFIG(INFO, \"(%s) guest physical page region %u\\n\",\n\t\t\t\tdev->ifname, i);\n\t\tVHOST_LOG_CONFIG(INFO, \"(%s)\\tguest_phys_addr: %\" PRIx64 \"\\n\",\n\t\t\t\tdev->ifname, page->guest_phys_addr);\n\t\tVHOST_LOG_CONFIG(INFO, \"(%s)\\thost_iova : %\" PRIx64 \"\\n\",\n\t\t\t\tdev->ifname, page->host_iova);\n\t\tVHOST_LOG_CONFIG(INFO, \"(%s)\\tsize           : %\" PRIx64 \"\\n\",\n\t\t\t\tdev->ifname, page->size);\n\t}\n}\n#else\n#define dump_guest_pages(dev)\n#endif\n\nstatic bool\nvhost_memory_changed(struct VhostUserMemory *new,\n\t\t     struct rte_vhost_memory *old)\n{\n\tuint32_t i;\n\n\tif (new->nregions != old->nregions)\n\t\treturn true;\n\n\tfor (i = 0; i < new->nregions; ++i) {\n\t\tVhostUserMemoryRegion *new_r = &new->regions[i];\n\t\tstruct rte_vhost_mem_region *old_r = &old->regions[i];\n\n\t\tif (new_r->guest_phys_addr != old_r->guest_phys_addr)\n\t\t\treturn true;\n\t\tif (new_r->memory_size != old_r->size)\n\t\t\treturn true;\n\t\tif (new_r->userspace_addr != old_r->guest_user_addr)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n#ifdef RTE_LIBRTE_VHOST_POSTCOPY\nstatic int\nvhost_user_postcopy_region_register(struct virtio_net *dev,\n\t\tstruct rte_vhost_mem_region *reg)\n{\n\tstruct uffdio_register reg_struct;\n\n\t/*\n\t * Let's register all the mmapped area to ensure\n\t * alignment on page boundary.\n\t */\n\treg_struct.range.start = (uint64_t)(uintptr_t)reg->mmap_addr;\n\treg_struct.range.len = reg->mmap_size;\n\treg_struct.mode = UFFDIO_REGISTER_MODE_MISSING;\n\n\tif (ioctl(dev->postcopy_ufd, UFFDIO_REGISTER,\n\t\t\t\t&reg_struct)) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to register ufd for region \"\n\t\t\t\t\"%\" PRIx64 \" - %\" PRIx64 \" (ufd = %d) %s\\n\",\n\t\t\t\tdev->ifname,\n\t\t\t\t(uint64_t)reg_struct.range.start,\n\t\t\t\t(uint64_t)reg_struct.range.start +\n\t\t\t\t(uint64_t)reg_struct.range.len - 1,\n\t\t\t\tdev->postcopy_ufd,\n\t\t\t\tstrerror(errno));\n\t\treturn -1;\n\t}\n\n\tVHOST_LOG_CONFIG(INFO,\n\t\t\t\"(%s)\\t userfaultfd registered for range : %\" PRIx64 \" - %\" PRIx64 \"\\n\",\n\t\t\tdev->ifname,\n\t\t\t(uint64_t)reg_struct.range.start,\n\t\t\t(uint64_t)reg_struct.range.start +\n\t\t\t(uint64_t)reg_struct.range.len - 1);\n\n\treturn 0;\n}\n#else\nstatic int\nvhost_user_postcopy_region_register(struct virtio_net *dev __rte_unused,\n\t\tstruct rte_vhost_mem_region *reg __rte_unused)\n{\n\treturn -1;\n}\n#endif\n\nstatic int\nvhost_user_postcopy_register(struct virtio_net *dev, int main_fd,\n\t\tstruct vhu_msg_context *ctx)\n{\n\tstruct VhostUserMemory *memory;\n\tstruct rte_vhost_mem_region *reg;\n\tstruct vhu_msg_context ack_ctx;\n\tuint32_t i;\n\n\tif (!dev->postcopy_listening)\n\t\treturn 0;\n\n\t/*\n\t * We haven't a better way right now than sharing\n\t * DPDK's virtual address with Qemu, so that Qemu can\n\t * retrieve the region offset when handling userfaults.\n\t */\n\tmemory = &ctx->msg.payload.memory;\n\tfor (i = 0; i < memory->nregions; i++) {\n\t\treg = &dev->mem->regions[i];\n\t\tmemory->regions[i].userspace_addr = reg->host_user_addr;\n\t}\n\n\t/* Send the addresses back to qemu */\n\tctx->fd_num = 0;\n\tsend_vhost_reply(dev, main_fd, ctx);\n\n\t/* Wait for qemu to acknowledge it got the addresses\n\t * we've got to wait before we're allowed to generate faults.\n\t */\n\tif (read_vhost_message(dev, main_fd, &ack_ctx) <= 0) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to read qemu ack on postcopy set-mem-table\\n\",\n\t\t\t\tdev->ifname);\n\t\treturn -1;\n\t}\n\n\tif (validate_msg_fds(dev, &ack_ctx, 0) != 0)\n\t\treturn -1;\n\n\tif (ack_ctx.msg.request.master != VHOST_USER_SET_MEM_TABLE) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) bad qemu ack on postcopy set-mem-table (%d)\\n\",\n\t\t\t\tdev->ifname, ack_ctx.msg.request.master);\n\t\treturn -1;\n\t}\n\n\t/* Now userfault register and we can use the memory */\n\tfor (i = 0; i < memory->nregions; i++) {\n\t\treg = &dev->mem->regions[i];\n\t\tif (vhost_user_postcopy_region_register(dev, reg) < 0)\n\t\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nvhost_user_mmap_region(struct virtio_net *dev,\n\t\tstruct rte_vhost_mem_region *region,\n\t\tuint64_t mmap_offset)\n{\n\tvoid *mmap_addr;\n\tuint64_t mmap_size;\n\tuint64_t alignment;\n\tint populate;\n\n\t/* Check for memory_size + mmap_offset overflow */\n\tif (mmap_offset >= -region->size) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) mmap_offset (%#\"PRIx64\") and memory_size (%#\"PRIx64\") overflow\\n\",\n\t\t\t\tdev->ifname, mmap_offset, region->size);\n\t\treturn -1;\n\t}\n\n\tmmap_size = region->size + mmap_offset;\n\n\t/* mmap() without flag of MAP_ANONYMOUS, should be called with length\n\t * argument aligned with hugepagesz at older longterm version Linux,\n\t * like 2.6.32 and 3.2.72, or mmap() will fail with EINVAL.\n\t *\n\t * To avoid failure, make sure in caller to keep length aligned.\n\t */\n\talignment = get_blk_size(region->fd);\n\tif (alignment == (uint64_t)-1) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) couldn't get hugepage size through fstat\\n\",\n\t\t\t\tdev->ifname);\n\t\treturn -1;\n\t}\n\tmmap_size = RTE_ALIGN_CEIL(mmap_size, alignment);\n\tif (mmap_size == 0) {\n\t\t/*\n\t\t * It could happen if initial mmap_size + alignment overflows\n\t\t * the sizeof uint64, which could happen if either mmap_size or\n\t\t * alignment value is wrong.\n\t\t *\n\t\t * mmap() kernel implementation would return an error, but\n\t\t * better catch it before and provide useful info in the logs.\n\t\t */\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) mmap size (0x%\" PRIx64 \") or alignment (0x%\" PRIx64 \") is invalid\\n\",\n\t\t\t\tdev->ifname, region->size + mmap_offset, alignment);\n\t\treturn -1;\n\t}\n\n\tpopulate = dev->async_copy ? MAP_POPULATE : 0;\n\tmmap_addr = mmap(NULL, mmap_size, PROT_READ | PROT_WRITE,\n\t\t\tMAP_SHARED | populate, region->fd, 0);\n\n\tif (mmap_addr == MAP_FAILED) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) mmap failed (%s).\\n\", dev->ifname, strerror(errno));\n\t\treturn -1;\n\t}\n\n\tregion->mmap_addr = mmap_addr;\n\tregion->mmap_size = mmap_size;\n\tregion->host_user_addr = (uint64_t)(uintptr_t)mmap_addr + mmap_offset;\n\n\tif (dev->async_copy) {\n\t\tif (add_guest_pages(dev, region, alignment) < 0) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) adding guest pages to region failed.\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) guest memory region size: 0x%\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, region->size);\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t guest physical addr: 0x%\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, region->guest_phys_addr);\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t guest virtual  addr: 0x%\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, region->guest_user_addr);\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t host  virtual  addr: 0x%\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, region->host_user_addr);\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t mmap addr : 0x%\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, (uint64_t)(uintptr_t)mmap_addr);\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t mmap size : 0x%\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, mmap_size);\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t mmap align: 0x%\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, alignment);\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t mmap off  : 0x%\" PRIx64 \"\\n\",\n\t\t\tdev->ifname, mmap_offset);\n\n\treturn 0;\n}\n\nstatic int\nvhost_user_set_mem_table(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd)\n{\n\tstruct virtio_net *dev = *pdev;\n\tstruct VhostUserMemory *memory = &ctx->msg.payload.memory;\n\tstruct rte_vhost_mem_region *reg;\n\tint numa_node = SOCKET_ID_ANY;\n\tuint64_t mmap_offset;\n\tuint32_t i;\n\tbool async_notify = false;\n\n\tif (validate_msg_fds(dev, ctx, memory->nregions) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (memory->nregions > VHOST_MEMORY_MAX_NREGIONS) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) too many memory regions (%u)\\n\",\n\t\t\t\tdev->ifname, memory->nregions);\n\t\tgoto close_msg_fds;\n\t}\n\n\tif (dev->mem && !vhost_memory_changed(memory, dev->mem)) {\n\t\tVHOST_LOG_CONFIG(INFO, \"(%s) memory regions not changed\\n\", dev->ifname);\n\n\t\tclose_msg_fds(ctx);\n\n\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\t}\n\n\tif (dev->mem) {\n\t\tif (dev->flags & VIRTIO_DEV_VDPA_CONFIGURED) {\n\t\t\tstruct rte_vdpa_device *vdpa_dev = dev->vdpa_dev;\n\n\t\t\tif (vdpa_dev && vdpa_dev->ops->dev_close)\n\t\t\t\tvdpa_dev->ops->dev_close(dev->vid);\n\t\t\tdev->flags &= ~VIRTIO_DEV_VDPA_CONFIGURED;\n\t\t}\n\n\t\t/* notify the vhost application to stop DMA transfers */\n\t\tif (dev->async_copy && dev->notify_ops->vring_state_changed) {\n\t\t\tfor (i = 0; i < dev->nr_vring; i++) {\n\t\t\t\tdev->notify_ops->vring_state_changed(dev->vid,\n\t\t\t\t\t\ti, 0);\n\t\t\t}\n\t\t\tasync_notify = true;\n\t\t}\n\n\t\tfree_mem_region(dev);\n\t\trte_free(dev->mem);\n\t\tdev->mem = NULL;\n\t}\n\n\t/* Flush IOTLB cache as previous HVAs are now invalid */\n\tif (dev->features & (1ULL << VIRTIO_F_IOMMU_PLATFORM))\n\t\tfor (i = 0; i < dev->nr_vring; i++)\n\t\t\tvhost_user_iotlb_flush_all(dev->virtqueue[i]);\n\n\t/*\n\t * If VQ 0 has already been allocated, try to allocate on the same\n\t * NUMA node. It can be reallocated later in numa_realloc().\n\t */\n\tif (dev->nr_vring > 0)\n\t\tnuma_node = dev->virtqueue[0]->numa_node;\n\n\tdev->nr_guest_pages = 0;\n\tif (dev->guest_pages == NULL) {\n\t\tdev->max_guest_pages = 8;\n\t\tdev->guest_pages = rte_zmalloc_socket(NULL,\n\t\t\t\t\tdev->max_guest_pages *\n\t\t\t\t\tsizeof(struct guest_page),\n\t\t\t\t\tRTE_CACHE_LINE_SIZE,\n\t\t\t\t\tnuma_node);\n\t\tif (dev->guest_pages == NULL) {\n\t\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\"(%s) failed to allocate memory for dev->guest_pages\\n\",\n\t\t\t\tdev->ifname);\n\t\t\tgoto close_msg_fds;\n\t\t}\n\t}\n\n\tdev->mem = rte_zmalloc_socket(\"vhost-mem-table\", sizeof(struct rte_vhost_memory) +\n\t\tsizeof(struct rte_vhost_mem_region) * memory->nregions, 0, numa_node);\n\tif (dev->mem == NULL) {\n\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\"(%s) failed to allocate memory for dev->mem\\n\",\n\t\t\tdev->ifname);\n\t\tgoto free_guest_pages;\n\t}\n\n\tfor (i = 0; i < memory->nregions; i++) {\n\t\treg = &dev->mem->regions[i];\n\n\t\treg->guest_phys_addr = memory->regions[i].guest_phys_addr;\n\t\treg->guest_user_addr = memory->regions[i].userspace_addr;\n\t\treg->size            = memory->regions[i].memory_size;\n\t\treg->fd              = ctx->fds[i];\n\n\t\t/*\n\t\t * Assign invalid file descriptor value to avoid double\n\t\t * closing on error path.\n\t\t */\n\t\tctx->fds[i] = -1;\n\n\t\tmmap_offset = memory->regions[i].mmap_offset;\n\n\t\tif (vhost_user_mmap_region(dev, reg, mmap_offset) < 0) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to mmap region %u\\n\", dev->ifname, i);\n\t\t\tgoto free_mem_table;\n\t\t}\n\n\t\tdev->mem->nregions++;\n\t}\n\n\tif (dev->async_copy && rte_vfio_is_enabled(\"vfio\"))\n\t\tasync_dma_map(dev, true);\n\n\tif (vhost_user_postcopy_register(dev, main_fd, ctx) < 0)\n\t\tgoto free_mem_table;\n\n\tfor (i = 0; i < dev->nr_vring; i++) {\n\t\tstruct vhost_virtqueue *vq = dev->virtqueue[i];\n\n\t\tif (!vq)\n\t\t\tcontinue;\n\n\t\tif (vq->desc || vq->avail || vq->used) {\n\t\t\t/*\n\t\t\t * If the memory table got updated, the ring addresses\n\t\t\t * need to be translated again as virtual addresses have\n\t\t\t * changed.\n\t\t\t */\n\t\t\tvring_invalidate(dev, vq);\n\n\t\t\tdev = translate_ring_addresses(dev, i);\n\t\t\tif (!dev) {\n\t\t\t\tdev = *pdev;\n\t\t\t\tgoto free_mem_table;\n\t\t\t}\n\n\t\t\t*pdev = dev;\n\t\t}\n\t}\n\n\tdump_guest_pages(dev);\n\n\tif (async_notify) {\n\t\tfor (i = 0; i < dev->nr_vring; i++)\n\t\t\tdev->notify_ops->vring_state_changed(dev->vid, i, 1);\n\t}\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n\nfree_mem_table:\n\tfree_mem_region(dev);\n\trte_free(dev->mem);\n\tdev->mem = NULL;\n\nfree_guest_pages:\n\trte_free(dev->guest_pages);\n\tdev->guest_pages = NULL;\nclose_msg_fds:\n\tclose_msg_fds(ctx);\n\treturn RTE_VHOST_MSG_RESULT_ERR;\n}\n\nstatic bool\nvq_is_ready(struct virtio_net *dev, struct vhost_virtqueue *vq)\n{\n\tbool rings_ok;\n\n\tif (!vq)\n\t\treturn false;\n\n\tif (vq_is_packed(dev))\n\t\trings_ok = vq->desc_packed && vq->driver_event &&\n\t\t\tvq->device_event;\n\telse\n\t\trings_ok = vq->desc && vq->avail && vq->used;\n\n\treturn rings_ok &&\n\t       vq->kickfd != VIRTIO_UNINITIALIZED_EVENTFD &&\n\t       vq->callfd != VIRTIO_UNINITIALIZED_EVENTFD &&\n\t       vq->enabled;\n}\n\n#define VIRTIO_BUILTIN_NUM_VQS_TO_BE_READY 2u\n\nstatic int\nvirtio_is_ready(struct virtio_net *dev)\n{\n\tstruct vhost_virtqueue *vq;\n\tuint32_t i, nr_vring = dev->nr_vring;\n\n\tif (dev->flags & VIRTIO_DEV_READY)\n\t\treturn 1;\n\n\tif (!dev->nr_vring)\n\t\treturn 0;\n\n\tif (dev->flags & VIRTIO_DEV_BUILTIN_VIRTIO_NET) {\n\t\tnr_vring = VIRTIO_BUILTIN_NUM_VQS_TO_BE_READY;\n\n\t\tif (dev->nr_vring < nr_vring)\n\t\t\treturn 0;\n\t}\n\n\tfor (i = 0; i < nr_vring; i++) {\n\t\tvq = dev->virtqueue[i];\n\n\t\tif (!vq_is_ready(dev, vq))\n\t\t\treturn 0;\n\t}\n\n\t/* If supported, ensure the frontend is really done with config */\n\tif (dev->protocol_features & (1ULL << VHOST_USER_PROTOCOL_F_STATUS))\n\t\tif (!(dev->status & VIRTIO_DEVICE_STATUS_DRIVER_OK))\n\t\t\treturn 0;\n\n\tdev->flags |= VIRTIO_DEV_READY;\n\n\tif (!(dev->flags & VIRTIO_DEV_RUNNING))\n\t\tVHOST_LOG_CONFIG(INFO, \"(%s) virtio is now ready for processing.\\n\", dev->ifname);\n\treturn 1;\n}\n\nstatic void *\ninflight_mem_alloc(struct virtio_net *dev, const char *name, size_t size, int *fd)\n{\n\tvoid *ptr;\n\tint mfd = -1;\n\tchar fname[20] = \"/tmp/memfd-XXXXXX\";\n\n\t*fd = -1;\n#ifdef MEMFD_SUPPORTED\n\tmfd = memfd_create(name, MFD_CLOEXEC);\n#else\n\tRTE_SET_USED(name);\n#endif\n\tif (mfd == -1) {\n\t\tmfd = mkstemp(fname);\n\t\tif (mfd == -1) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to get inflight buffer fd\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tunlink(fname);\n\t}\n\n\tif (ftruncate(mfd, size) == -1) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to alloc inflight buffer\\n\", dev->ifname);\n\t\tclose(mfd);\n\t\treturn NULL;\n\t}\n\n\tptr = mmap(0, size, PROT_READ | PROT_WRITE, MAP_SHARED, mfd, 0);\n\tif (ptr == MAP_FAILED) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to mmap inflight buffer\\n\", dev->ifname);\n\t\tclose(mfd);\n\t\treturn NULL;\n\t}\n\n\t*fd = mfd;\n\treturn ptr;\n}\n\nstatic uint32_t\nget_pervq_shm_size_split(uint16_t queue_size)\n{\n\treturn RTE_ALIGN_MUL_CEIL(sizeof(struct rte_vhost_inflight_desc_split) *\n\t\t\t\t  queue_size + sizeof(uint64_t) +\n\t\t\t\t  sizeof(uint16_t) * 4, INFLIGHT_ALIGNMENT);\n}\n\nstatic uint32_t\nget_pervq_shm_size_packed(uint16_t queue_size)\n{\n\treturn RTE_ALIGN_MUL_CEIL(sizeof(struct rte_vhost_inflight_desc_packed)\n\t\t\t\t  * queue_size + sizeof(uint64_t) +\n\t\t\t\t  sizeof(uint16_t) * 6 + sizeof(uint8_t) * 9,\n\t\t\t\t  INFLIGHT_ALIGNMENT);\n}\n\nstatic int\nvhost_user_get_inflight_fd(struct virtio_net **pdev,\n\t\t\t   struct vhu_msg_context *ctx,\n\t\t\t   int main_fd __rte_unused)\n{\n\tstruct rte_vhost_inflight_info_packed *inflight_packed;\n\tuint64_t pervq_inflight_size, mmap_size;\n\tuint16_t num_queues, queue_size;\n\tstruct virtio_net *dev = *pdev;\n\tint fd, i, j;\n\tint numa_node = SOCKET_ID_ANY;\n\tvoid *addr;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (ctx->msg.size != sizeof(ctx->msg.payload.inflight)) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid get_inflight_fd message size is %d\\n\",\n\t\t\tdev->ifname, ctx->msg.size);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\t/*\n\t * If VQ 0 has already been allocated, try to allocate on the same\n\t * NUMA node. It can be reallocated later in numa_realloc().\n\t */\n\tif (dev->nr_vring > 0)\n\t\tnuma_node = dev->virtqueue[0]->numa_node;\n\n\tif (dev->inflight_info == NULL) {\n\t\tdev->inflight_info = rte_zmalloc_socket(\"inflight_info\",\n\t\t\t\tsizeof(struct inflight_mem_info), 0, numa_node);\n\t\tif (!dev->inflight_info) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to alloc dev inflight area\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\t\tdev->inflight_info->fd = -1;\n\t}\n\n\tnum_queues = ctx->msg.payload.inflight.num_queues;\n\tqueue_size = ctx->msg.payload.inflight.queue_size;\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) get_inflight_fd num_queues: %u\\n\",\n\t\tdev->ifname, ctx->msg.payload.inflight.num_queues);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) get_inflight_fd queue_size: %u\\n\",\n\t\tdev->ifname, ctx->msg.payload.inflight.queue_size);\n\n\tif (vq_is_packed(dev))\n\t\tpervq_inflight_size = get_pervq_shm_size_packed(queue_size);\n\telse\n\t\tpervq_inflight_size = get_pervq_shm_size_split(queue_size);\n\n\tmmap_size = num_queues * pervq_inflight_size;\n\taddr = inflight_mem_alloc(dev, \"vhost-inflight\", mmap_size, &fd);\n\tif (!addr) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to alloc vhost inflight area\\n\", dev->ifname);\n\t\t\tctx->msg.payload.inflight.mmap_size = 0;\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\tmemset(addr, 0, mmap_size);\n\n\tif (dev->inflight_info->addr) {\n\t\tmunmap(dev->inflight_info->addr, dev->inflight_info->size);\n\t\tdev->inflight_info->addr = NULL;\n\t}\n\n\tif (dev->inflight_info->fd >= 0) {\n\t\tclose(dev->inflight_info->fd);\n\t\tdev->inflight_info->fd = -1;\n\t}\n\n\tdev->inflight_info->addr = addr;\n\tdev->inflight_info->size = ctx->msg.payload.inflight.mmap_size = mmap_size;\n\tdev->inflight_info->fd = ctx->fds[0] = fd;\n\tctx->msg.payload.inflight.mmap_offset = 0;\n\tctx->fd_num = 1;\n\n\tif (vq_is_packed(dev)) {\n\t\tfor (i = 0; i < num_queues; i++) {\n\t\t\tinflight_packed =\n\t\t\t\t(struct rte_vhost_inflight_info_packed *)addr;\n\t\t\tinflight_packed->used_wrap_counter = 1;\n\t\t\tinflight_packed->old_used_wrap_counter = 1;\n\t\t\tfor (j = 0; j < queue_size; j++)\n\t\t\t\tinflight_packed->desc[j].next = j + 1;\n\t\t\taddr = (void *)((char *)addr + pervq_inflight_size);\n\t\t}\n\t}\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) send inflight mmap_size: %\"PRIu64\"\\n\",\n\t\t\tdev->ifname, ctx->msg.payload.inflight.mmap_size);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) send inflight mmap_offset: %\"PRIu64\"\\n\",\n\t\t\tdev->ifname, ctx->msg.payload.inflight.mmap_offset);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) send inflight fd: %d\\n\", dev->ifname, ctx->fds[0]);\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n}\n\nstatic int\nvhost_user_set_inflight_fd(struct virtio_net **pdev,\n\t\t\t   struct vhu_msg_context *ctx,\n\t\t\t   int main_fd __rte_unused)\n{\n\tuint64_t mmap_size, mmap_offset;\n\tuint16_t num_queues, queue_size;\n\tstruct virtio_net *dev = *pdev;\n\tuint32_t pervq_inflight_size;\n\tstruct vhost_virtqueue *vq;\n\tvoid *addr;\n\tint fd, i;\n\tint numa_node = SOCKET_ID_ANY;\n\n\tif (validate_msg_fds(dev, ctx, 1) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tfd = ctx->fds[0];\n\tif (ctx->msg.size != sizeof(ctx->msg.payload.inflight) || fd < 0) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid set_inflight_fd message size is %d,fd is %d\\n\",\n\t\t\tdev->ifname, ctx->msg.size, fd);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tmmap_size = ctx->msg.payload.inflight.mmap_size;\n\tmmap_offset = ctx->msg.payload.inflight.mmap_offset;\n\tnum_queues = ctx->msg.payload.inflight.num_queues;\n\tqueue_size = ctx->msg.payload.inflight.queue_size;\n\n\tif (vq_is_packed(dev))\n\t\tpervq_inflight_size = get_pervq_shm_size_packed(queue_size);\n\telse\n\t\tpervq_inflight_size = get_pervq_shm_size_split(queue_size);\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd mmap_size: %\"PRIu64\"\\n\",\n\t\t\tdev->ifname, mmap_size);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd mmap_offset: %\"PRIu64\"\\n\",\n\t\t\tdev->ifname, mmap_offset);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd num_queues: %u\\n\", dev->ifname, num_queues);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd queue_size: %u\\n\", dev->ifname, queue_size);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd fd: %d\\n\", dev->ifname, fd);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd pervq_inflight_size: %d\\n\",\n\t\t\tdev->ifname, pervq_inflight_size);\n\n\t/*\n\t * If VQ 0 has already been allocated, try to allocate on the same\n\t * NUMA node. It can be reallocated later in numa_realloc().\n\t */\n\tif (dev->nr_vring > 0)\n\t\tnuma_node = dev->virtqueue[0]->numa_node;\n\n\tif (!dev->inflight_info) {\n\t\tdev->inflight_info = rte_zmalloc_socket(\"inflight_info\",\n\t\t\t\tsizeof(struct inflight_mem_info), 0, numa_node);\n\t\tif (dev->inflight_info == NULL) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to alloc dev inflight area\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\t\tdev->inflight_info->fd = -1;\n\t}\n\n\tif (dev->inflight_info->addr) {\n\t\tmunmap(dev->inflight_info->addr, dev->inflight_info->size);\n\t\tdev->inflight_info->addr = NULL;\n\t}\n\n\taddr = mmap(0, mmap_size, PROT_READ | PROT_WRITE, MAP_SHARED,\n\t\t    fd, mmap_offset);\n\tif (addr == MAP_FAILED) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to mmap share memory.\\n\", dev->ifname);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tif (dev->inflight_info->fd >= 0) {\n\t\tclose(dev->inflight_info->fd);\n\t\tdev->inflight_info->fd = -1;\n\t}\n\n\tdev->inflight_info->fd = fd;\n\tdev->inflight_info->addr = addr;\n\tdev->inflight_info->size = mmap_size;\n\n\tfor (i = 0; i < num_queues; i++) {\n\t\tvq = dev->virtqueue[i];\n\t\tif (!vq)\n\t\t\tcontinue;\n\n\t\tif (vq_is_packed(dev)) {\n\t\t\tvq->inflight_packed = addr;\n\t\t\tvq->inflight_packed->desc_num = queue_size;\n\t\t} else {\n\t\t\tvq->inflight_split = addr;\n\t\t\tvq->inflight_split->desc_num = queue_size;\n\t\t}\n\t\taddr = (void *)((char *)addr + pervq_inflight_size);\n\t}\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_user_set_vring_call(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tstruct vhost_vring_file file;\n\tstruct vhost_virtqueue *vq;\n\tint expected_fds;\n\n\texpected_fds = (ctx->msg.payload.u64 & VHOST_USER_VRING_NOFD_MASK) ? 0 : 1;\n\tif (validate_msg_fds(dev, ctx, expected_fds) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tfile.index = ctx->msg.payload.u64 & VHOST_USER_VRING_IDX_MASK;\n\tif (ctx->msg.payload.u64 & VHOST_USER_VRING_NOFD_MASK)\n\t\tfile.fd = VIRTIO_INVALID_EVENTFD;\n\telse\n\t\tfile.fd = ctx->fds[0];\n\tVHOST_LOG_CONFIG(INFO, \"(%s) vring call idx:%d file:%d\\n\",\n\t\t\tdev->ifname, file.index, file.fd);\n\n\tvq = dev->virtqueue[file.index];\n\n\tif (vq->ready) {\n\t\tvq->ready = false;\n\t\tvhost_user_notify_queue_state(dev, file.index, 0);\n\t}\n\n\tif (vq->callfd >= 0)\n\t\tclose(vq->callfd);\n\n\tvq->callfd = file.fd;\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int vhost_user_set_vring_err(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tint expected_fds;\n\n\texpected_fds = (ctx->msg.payload.u64 & VHOST_USER_VRING_NOFD_MASK) ? 0 : 1;\n\tif (validate_msg_fds(dev, ctx, expected_fds) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (!(ctx->msg.payload.u64 & VHOST_USER_VRING_NOFD_MASK))\n\t\tclose(ctx->fds[0]);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) not implemented\\n\", dev->ifname);\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nresubmit_desc_compare(const void *a, const void *b)\n{\n\tconst struct rte_vhost_resubmit_desc *desc0 = a;\n\tconst struct rte_vhost_resubmit_desc *desc1 = b;\n\n\tif (desc1->counter > desc0->counter)\n\t\treturn 1;\n\n\treturn -1;\n}\n\nstatic int\nvhost_check_queue_inflights_split(struct virtio_net *dev,\n\t\t\t\t  struct vhost_virtqueue *vq)\n{\n\tuint16_t i;\n\tuint16_t resubmit_num = 0, last_io, num;\n\tstruct vring_used *used = vq->used;\n\tstruct rte_vhost_resubmit_info *resubmit;\n\tstruct rte_vhost_inflight_info_split *inflight_split;\n\n\tif (!(dev->protocol_features &\n\t    (1ULL << VHOST_USER_PROTOCOL_F_INFLIGHT_SHMFD)))\n\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\n\t/* The frontend may still not support the inflight feature\n\t * although we negotiate the protocol feature.\n\t */\n\tif ((!vq->inflight_split))\n\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\n\tif (!vq->inflight_split->version) {\n\t\tvq->inflight_split->version = INFLIGHT_VERSION;\n\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\t}\n\n\tif (vq->resubmit_inflight)\n\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\n\tinflight_split = vq->inflight_split;\n\tvq->global_counter = 0;\n\tlast_io = inflight_split->last_inflight_io;\n\n\tif (inflight_split->used_idx != used->idx) {\n\t\tinflight_split->desc[last_io].inflight = 0;\n\t\trte_atomic_thread_fence(__ATOMIC_SEQ_CST);\n\t\tinflight_split->used_idx = used->idx;\n\t}\n\n\tfor (i = 0; i < inflight_split->desc_num; i++) {\n\t\tif (inflight_split->desc[i].inflight == 1)\n\t\t\tresubmit_num++;\n\t}\n\n\tvq->last_avail_idx += resubmit_num;\n\n\tif (resubmit_num) {\n\t\tresubmit = rte_zmalloc_socket(\"resubmit\", sizeof(struct rte_vhost_resubmit_info),\n\t\t\t\t0, vq->numa_node);\n\t\tif (!resubmit) {\n\t\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\t\"(%s) failed to allocate memory for resubmit info.\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\n\t\tresubmit->resubmit_list = rte_zmalloc_socket(\"resubmit_list\",\n\t\t\t\tresubmit_num * sizeof(struct rte_vhost_resubmit_desc),\n\t\t\t\t0, vq->numa_node);\n\t\tif (!resubmit->resubmit_list) {\n\t\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\t\"(%s) failed to allocate memory for inflight desc.\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\trte_free(resubmit);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\n\t\tnum = 0;\n\t\tfor (i = 0; i < vq->inflight_split->desc_num; i++) {\n\t\t\tif (vq->inflight_split->desc[i].inflight == 1) {\n\t\t\t\tresubmit->resubmit_list[num].index = i;\n\t\t\t\tresubmit->resubmit_list[num].counter =\n\t\t\t\t\tinflight_split->desc[i].counter;\n\t\t\t\tnum++;\n\t\t\t}\n\t\t}\n\t\tresubmit->resubmit_num = num;\n\n\t\tif (resubmit->resubmit_num > 1)\n\t\t\tqsort(resubmit->resubmit_list, resubmit->resubmit_num,\n\t\t\t      sizeof(struct rte_vhost_resubmit_desc),\n\t\t\t      resubmit_desc_compare);\n\n\t\tvq->global_counter = resubmit->resubmit_list[0].counter + 1;\n\t\tvq->resubmit_inflight = resubmit;\n\t}\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_check_queue_inflights_packed(struct virtio_net *dev,\n\t\t\t\t   struct vhost_virtqueue *vq)\n{\n\tuint16_t i;\n\tuint16_t resubmit_num = 0, old_used_idx, num;\n\tstruct rte_vhost_resubmit_info *resubmit;\n\tstruct rte_vhost_inflight_info_packed *inflight_packed;\n\n\tif (!(dev->protocol_features &\n\t    (1ULL << VHOST_USER_PROTOCOL_F_INFLIGHT_SHMFD)))\n\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\n\t/* The frontend may still not support the inflight feature\n\t * although we negotiate the protocol feature.\n\t */\n\tif ((!vq->inflight_packed))\n\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\n\tif (!vq->inflight_packed->version) {\n\t\tvq->inflight_packed->version = INFLIGHT_VERSION;\n\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\t}\n\n\tif (vq->resubmit_inflight)\n\t\treturn RTE_VHOST_MSG_RESULT_OK;\n\n\tinflight_packed = vq->inflight_packed;\n\tvq->global_counter = 0;\n\told_used_idx = inflight_packed->old_used_idx;\n\n\tif (inflight_packed->used_idx != old_used_idx) {\n\t\tif (inflight_packed->desc[old_used_idx].inflight == 0) {\n\t\t\tinflight_packed->old_used_idx =\n\t\t\t\tinflight_packed->used_idx;\n\t\t\tinflight_packed->old_used_wrap_counter =\n\t\t\t\tinflight_packed->used_wrap_counter;\n\t\t\tinflight_packed->old_free_head =\n\t\t\t\tinflight_packed->free_head;\n\t\t} else {\n\t\t\tinflight_packed->used_idx =\n\t\t\t\tinflight_packed->old_used_idx;\n\t\t\tinflight_packed->used_wrap_counter =\n\t\t\t\tinflight_packed->old_used_wrap_counter;\n\t\t\tinflight_packed->free_head =\n\t\t\t\tinflight_packed->old_free_head;\n\t\t}\n\t}\n\n\tfor (i = 0; i < inflight_packed->desc_num; i++) {\n\t\tif (inflight_packed->desc[i].inflight == 1)\n\t\t\tresubmit_num++;\n\t}\n\n\tif (resubmit_num) {\n\t\tresubmit = rte_zmalloc_socket(\"resubmit\", sizeof(struct rte_vhost_resubmit_info),\n\t\t\t\t0, vq->numa_node);\n\t\tif (resubmit == NULL) {\n\t\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\t\"(%s) failed to allocate memory for resubmit info.\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\n\t\tresubmit->resubmit_list = rte_zmalloc_socket(\"resubmit_list\",\n\t\t\t\tresubmit_num * sizeof(struct rte_vhost_resubmit_desc),\n\t\t\t\t0, vq->numa_node);\n\t\tif (resubmit->resubmit_list == NULL) {\n\t\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\t\"(%s) failed to allocate memory for resubmit desc.\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\trte_free(resubmit);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\n\t\tnum = 0;\n\t\tfor (i = 0; i < inflight_packed->desc_num; i++) {\n\t\t\tif (vq->inflight_packed->desc[i].inflight == 1) {\n\t\t\t\tresubmit->resubmit_list[num].index = i;\n\t\t\t\tresubmit->resubmit_list[num].counter =\n\t\t\t\t\tinflight_packed->desc[i].counter;\n\t\t\t\tnum++;\n\t\t\t}\n\t\t}\n\t\tresubmit->resubmit_num = num;\n\n\t\tif (resubmit->resubmit_num > 1)\n\t\t\tqsort(resubmit->resubmit_list, resubmit->resubmit_num,\n\t\t\t      sizeof(struct rte_vhost_resubmit_desc),\n\t\t\t      resubmit_desc_compare);\n\n\t\tvq->global_counter = resubmit->resubmit_list[0].counter + 1;\n\t\tvq->resubmit_inflight = resubmit;\n\t}\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_user_set_vring_kick(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tstruct vhost_vring_file file;\n\tstruct vhost_virtqueue *vq;\n\tint expected_fds;\n\n\texpected_fds = (ctx->msg.payload.u64 & VHOST_USER_VRING_NOFD_MASK) ? 0 : 1;\n\tif (validate_msg_fds(dev, ctx, expected_fds) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tfile.index = ctx->msg.payload.u64 & VHOST_USER_VRING_IDX_MASK;\n\tif (ctx->msg.payload.u64 & VHOST_USER_VRING_NOFD_MASK)\n\t\tfile.fd = VIRTIO_INVALID_EVENTFD;\n\telse\n\t\tfile.fd = ctx->fds[0];\n\tVHOST_LOG_CONFIG(INFO, \"(%s) vring kick idx:%d file:%d\\n\",\n\t\t\tdev->ifname, file.index, file.fd);\n\n\t/* Interpret ring addresses only when ring is started. */\n\tdev = translate_ring_addresses(dev, file.index);\n\tif (!dev) {\n\t\tif (file.fd != VIRTIO_INVALID_EVENTFD)\n\t\t\tclose(file.fd);\n\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\t*pdev = dev;\n\n\tvq = dev->virtqueue[file.index];\n\n\t/*\n\t * When VHOST_USER_F_PROTOCOL_FEATURES is not negotiated,\n\t * the ring starts already enabled. Otherwise, it is enabled via\n\t * the SET_VRING_ENABLE message.\n\t */\n\tif (!(dev->features & (1ULL << VHOST_USER_F_PROTOCOL_FEATURES))) {\n\t\tvq->enabled = true;\n\t}\n\n\tif (vq->ready) {\n\t\tvq->ready = false;\n\t\tvhost_user_notify_queue_state(dev, file.index, 0);\n\t}\n\n\tif (vq->kickfd >= 0)\n\t\tclose(vq->kickfd);\n\tvq->kickfd = file.fd;\n\n\tif (vq_is_packed(dev)) {\n\t\tif (vhost_check_queue_inflights_packed(dev, vq)) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to inflights for vq: %d\\n\",\n\t\t\t\t\tdev->ifname, file.index);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\t} else {\n\t\tif (vhost_check_queue_inflights_split(dev, vq)) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to inflights for vq: %d\\n\",\n\t\t\t\t\tdev->ifname, file.index);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\t}\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\n/*\n * when virtio is stopped, qemu will send us the GET_VRING_BASE message.\n */\nstatic int\nvhost_user_get_vring_base(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tstruct vhost_virtqueue *vq = dev->virtqueue[ctx->msg.payload.state.index];\n\tuint64_t val;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\t/* We have to stop the queue (virtio) if it is running. */\n\tvhost_destroy_device_notify(dev);\n\n\tdev->flags &= ~VIRTIO_DEV_READY;\n\tdev->flags &= ~VIRTIO_DEV_VDPA_CONFIGURED;\n\n\t/* Here we are safe to get the indexes */\n\tif (vq_is_packed(dev)) {\n\t\t/*\n\t\t * Bit[0:14]: avail index\n\t\t * Bit[15]: avail wrap counter\n\t\t */\n\t\tval = vq->last_avail_idx & 0x7fff;\n\t\tval |= vq->avail_wrap_counter << 15;\n\t\tctx->msg.payload.state.num = val;\n\t} else {\n\t\tctx->msg.payload.state.num = vq->last_avail_idx;\n\t}\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) vring base idx:%d file:%d\\n\",\n\t\t\tdev->ifname, ctx->msg.payload.state.index,\n\t\t\tctx->msg.payload.state.num);\n\t/*\n\t * Based on current qemu vhost-user implementation, this message is\n\t * sent and only sent in vhost_vring_stop.\n\t * TODO: cleanup the vring, it isn't usable since here.\n\t */\n\tif (vq->kickfd >= 0)\n\t\tclose(vq->kickfd);\n\n\tvq->kickfd = VIRTIO_UNINITIALIZED_EVENTFD;\n\n\tif (vq->callfd >= 0)\n\t\tclose(vq->callfd);\n\n\tvq->callfd = VIRTIO_UNINITIALIZED_EVENTFD;\n\n\tvq->signalled_used_valid = false;\n\n\tif (vq_is_packed(dev)) {\n\t\trte_free(vq->shadow_used_packed);\n\t\tvq->shadow_used_packed = NULL;\n\t} else {\n\t\trte_free(vq->shadow_used_split);\n\t\tvq->shadow_used_split = NULL;\n\t}\n\n\trte_free(vq->batch_copy_elems);\n\tvq->batch_copy_elems = NULL;\n\n\trte_free(vq->log_cache);\n\tvq->log_cache = NULL;\n\n\tctx->msg.size = sizeof(ctx->msg.payload.state);\n\tctx->fd_num = 0;\n\n\tvhost_user_iotlb_flush_all(vq);\n\n\tvring_invalidate(dev, vq);\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n}\n\n/*\n * when virtio queues are ready to work, qemu will send us to\n * enable the virtio queue pair.\n */\nstatic int\nvhost_user_set_vring_enable(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tbool enable = !!ctx->msg.payload.state.num;\n\tint index = (int)ctx->msg.payload.state.index;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set queue enable: %d to qp idx: %d\\n\",\n\t\t\tdev->ifname, enable, index);\n\n\tif (enable && dev->virtqueue[index]->async) {\n\t\tif (dev->virtqueue[index]->async->pkts_inflight_n) {\n\t\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\"(%s) failed to enable vring. Inflight packets must be completed first\\n\",\n\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\t}\n\n\tdev->virtqueue[index]->enabled = enable;\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_user_get_protocol_features(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tuint64_t features, protocol_features;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\trte_vhost_driver_get_features(dev->ifname, &features);\n\trte_vhost_driver_get_protocol_features(dev->ifname, &protocol_features);\n\n\tctx->msg.payload.u64 = protocol_features;\n\tctx->msg.size = sizeof(ctx->msg.payload.u64);\n\tctx->fd_num = 0;\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n}\n\nstatic int\nvhost_user_set_protocol_features(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tuint64_t protocol_features = ctx->msg.payload.u64;\n\tuint64_t slave_protocol_features = 0;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\trte_vhost_driver_get_protocol_features(dev->ifname,\n\t\t\t&slave_protocol_features);\n\tif (protocol_features & ~slave_protocol_features) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) received invalid protocol features.\\n\", dev->ifname);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tdev->protocol_features = protocol_features;\n\tVHOST_LOG_CONFIG(INFO, \"(%s) negotiated Vhost-user protocol features: 0x%\" PRIx64 \"\\n\",\n\t\tdev->ifname, dev->protocol_features);\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_user_set_log_base(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tint fd = ctx->fds[0];\n\tuint64_t size, off;\n\tvoid *addr;\n\tuint32_t i;\n\n\tif (validate_msg_fds(dev, ctx, 1) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (fd < 0) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid log fd: %d\\n\", dev->ifname, fd);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tif (ctx->msg.size != sizeof(VhostUserLog)) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid log base msg size: %\"PRId32\" != %d\\n\",\n\t\t\tdev->ifname, ctx->msg.size, (int)sizeof(VhostUserLog));\n\t\tgoto close_msg_fds;\n\t}\n\n\tsize = ctx->msg.payload.log.mmap_size;\n\toff  = ctx->msg.payload.log.mmap_offset;\n\n\t/* Check for mmap size and offset overflow. */\n\tif (off >= -size) {\n\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\"(%s) log offset %#\"PRIx64\" and log size %#\"PRIx64\" overflow\\n\",\n\t\t\t\tdev->ifname, off, size);\n\t\tgoto close_msg_fds;\n\t}\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) log mmap size: %\"PRId64\", offset: %\"PRId64\"\\n\",\n\t\t\tdev->ifname, size, off);\n\n\t/*\n\t * mmap from 0 to workaround a hugepage mmap bug: mmap will\n\t * fail when offset is not page size aligned.\n\t */\n\taddr = mmap(0, size + off, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);\n\tclose(fd);\n\tif (addr == MAP_FAILED) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) mmap log base failed!\\n\", dev->ifname);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\t/*\n\t * Free previously mapped log memory on occasionally\n\t * multiple VHOST_USER_SET_LOG_BASE.\n\t */\n\tif (dev->log_addr) {\n\t\tmunmap((void *)(uintptr_t)dev->log_addr, dev->log_size);\n\t}\n\tdev->log_addr = (uint64_t)(uintptr_t)addr;\n\tdev->log_base = dev->log_addr + off;\n\tdev->log_size = size;\n\n\tfor (i = 0; i < dev->nr_vring; i++) {\n\t\tstruct vhost_virtqueue *vq = dev->virtqueue[i];\n\n\t\trte_free(vq->log_cache);\n\t\tvq->log_cache = NULL;\n\t\tvq->log_cache_nb_elem = 0;\n\t\tvq->log_cache = rte_malloc_socket(\"vq log cache\",\n\t\t\t\tsizeof(struct log_cache_entry) * VHOST_LOG_CACHE_NR,\n\t\t\t\t0, vq->numa_node);\n\t\t/*\n\t\t * If log cache alloc fail, don't fail migration, but no\n\t\t * caching will be done, which will impact performance\n\t\t */\n\t\tif (!vq->log_cache)\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to allocate VQ logging cache\\n\",\n\t\t\t\t\tdev->ifname);\n\t}\n\n\t/*\n\t * The spec is not clear about it (yet), but QEMU doesn't expect\n\t * any payload in the reply.\n\t */\n\tctx->msg.size = 0;\n\tctx->fd_num = 0;\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n\nclose_msg_fds:\n\tclose_msg_fds(ctx);\n\treturn RTE_VHOST_MSG_RESULT_ERR;\n}\n\nstatic int vhost_user_set_log_fd(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\n\tif (validate_msg_fds(dev, ctx, 1) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tclose(ctx->fds[0]);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) not implemented.\\n\", dev->ifname);\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\n/*\n * An rarp packet is constructed and broadcasted to notify switches about\n * the new location of the migrated VM, so that packets from outside will\n * not be lost after migration.\n *\n * However, we don't actually \"send\" a rarp packet here, instead, we set\n * a flag 'broadcast_rarp' to let rte_vhost_dequeue_burst() inject it.\n */\nstatic int\nvhost_user_send_rarp(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tuint8_t *mac = (uint8_t *)&ctx->msg.payload.u64;\n\tstruct rte_vdpa_device *vdpa_dev;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tVHOST_LOG_CONFIG(DEBUG, \"(%s) MAC: \" RTE_ETHER_ADDR_PRT_FMT \"\\n\",\n\t\tdev->ifname, mac[0], mac[1], mac[2], mac[3], mac[4], mac[5]);\n\tmemcpy(dev->mac.addr_bytes, mac, 6);\n\n\t/*\n\t * Set the flag to inject a RARP broadcast packet at\n\t * rte_vhost_dequeue_burst().\n\t *\n\t * __ATOMIC_RELEASE ordering is for making sure the mac is\n\t * copied before the flag is set.\n\t */\n\t__atomic_store_n(&dev->broadcast_rarp, 1, __ATOMIC_RELEASE);\n\tvdpa_dev = dev->vdpa_dev;\n\tif (vdpa_dev && vdpa_dev->ops->migration_done)\n\t\tvdpa_dev->ops->migration_done(dev->vid);\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_user_net_set_mtu(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (ctx->msg.payload.u64 < VIRTIO_MIN_MTU ||\n\t\t\tctx->msg.payload.u64 > VIRTIO_MAX_MTU) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid MTU size (%\"PRIu64\")\\n\",\n\t\t\t\tdev->ifname, ctx->msg.payload.u64);\n\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tdev->mtu = ctx->msg.payload.u64;\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_user_set_req_fd(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tint fd = ctx->fds[0];\n\n\tif (validate_msg_fds(dev, ctx, 1) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (fd < 0) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid file descriptor for slave channel (%d)\\n\",\n\t\t\t\tdev->ifname, fd);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tif (dev->slave_req_fd >= 0)\n\t\tclose(dev->slave_req_fd);\n\n\tdev->slave_req_fd = fd;\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nis_vring_iotlb_split(struct vhost_virtqueue *vq, struct vhost_iotlb_msg *imsg)\n{\n\tstruct vhost_vring_addr *ra;\n\tuint64_t start, end, len;\n\n\tstart = imsg->iova;\n\tend = start + imsg->size;\n\n\tra = &vq->ring_addrs;\n\tlen = sizeof(struct vring_desc) * vq->size;\n\tif (ra->desc_user_addr < end && (ra->desc_user_addr + len) > start)\n\t\treturn 1;\n\n\tlen = sizeof(struct vring_avail) + sizeof(uint16_t) * vq->size;\n\tif (ra->avail_user_addr < end && (ra->avail_user_addr + len) > start)\n\t\treturn 1;\n\n\tlen = sizeof(struct vring_used) +\n\t       sizeof(struct vring_used_elem) * vq->size;\n\tif (ra->used_user_addr < end && (ra->used_user_addr + len) > start)\n\t\treturn 1;\n\n\tif (ra->flags & (1 << VHOST_VRING_F_LOG)) {\n\t\tlen = sizeof(uint64_t);\n\t\tif (ra->log_guest_addr < end &&\n\t\t    (ra->log_guest_addr + len) > start)\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nis_vring_iotlb_packed(struct vhost_virtqueue *vq, struct vhost_iotlb_msg *imsg)\n{\n\tstruct vhost_vring_addr *ra;\n\tuint64_t start, end, len;\n\n\tstart = imsg->iova;\n\tend = start + imsg->size;\n\n\tra = &vq->ring_addrs;\n\tlen = sizeof(struct vring_packed_desc) * vq->size;\n\tif (ra->desc_user_addr < end && (ra->desc_user_addr + len) > start)\n\t\treturn 1;\n\n\tlen = sizeof(struct vring_packed_desc_event);\n\tif (ra->avail_user_addr < end && (ra->avail_user_addr + len) > start)\n\t\treturn 1;\n\n\tlen = sizeof(struct vring_packed_desc_event);\n\tif (ra->used_user_addr < end && (ra->used_user_addr + len) > start)\n\t\treturn 1;\n\n\tif (ra->flags & (1 << VHOST_VRING_F_LOG)) {\n\t\tlen = sizeof(uint64_t);\n\t\tif (ra->log_guest_addr < end &&\n\t\t    (ra->log_guest_addr + len) > start)\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int is_vring_iotlb(struct virtio_net *dev,\n\t\t\t  struct vhost_virtqueue *vq,\n\t\t\t  struct vhost_iotlb_msg *imsg)\n{\n\tif (vq_is_packed(dev))\n\t\treturn is_vring_iotlb_packed(vq, imsg);\n\telse\n\t\treturn is_vring_iotlb_split(vq, imsg);\n}\n\nstatic int\nvhost_user_iotlb_msg(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\tstruct vhost_iotlb_msg *imsg = &ctx->msg.payload.iotlb;\n\tuint16_t i;\n\tuint64_t vva, len;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tswitch (imsg->type) {\n\tcase VHOST_IOTLB_UPDATE:\n\t\tlen = imsg->size;\n\t\tvva = qva_to_vva(dev, imsg->uaddr, &len);\n\t\tif (!vva)\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\t\tfor (i = 0; i < dev->nr_vring; i++) {\n\t\t\tstruct vhost_virtqueue *vq = dev->virtqueue[i];\n\n\t\t\tif (!vq)\n\t\t\t\tcontinue;\n\n\t\t\tvhost_user_iotlb_cache_insert(dev, vq, imsg->iova, vva,\n\t\t\t\t\tlen, imsg->perm);\n\n\t\t\tif (is_vring_iotlb(dev, vq, imsg)) {\n\t\t\t\trte_spinlock_lock(&vq->access_lock);\n\t\t\t\t*pdev = dev = translate_ring_addresses(dev, i);\n\t\t\t\trte_spinlock_unlock(&vq->access_lock);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase VHOST_IOTLB_INVALIDATE:\n\t\tfor (i = 0; i < dev->nr_vring; i++) {\n\t\t\tstruct vhost_virtqueue *vq = dev->virtqueue[i];\n\n\t\t\tif (!vq)\n\t\t\t\tcontinue;\n\n\t\t\tvhost_user_iotlb_cache_remove(vq, imsg->iova,\n\t\t\t\t\timsg->size);\n\n\t\t\tif (is_vring_iotlb(dev, vq, imsg)) {\n\t\t\t\trte_spinlock_lock(&vq->access_lock);\n\t\t\t\tvring_invalidate(dev, vq);\n\t\t\t\trte_spinlock_unlock(&vq->access_lock);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid IOTLB message type (%d)\\n\",\n\t\t\t\tdev->ifname, imsg->type);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_user_set_postcopy_advise(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n#ifdef RTE_LIBRTE_VHOST_POSTCOPY\n\tstruct uffdio_api api_struct;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tdev->postcopy_ufd = syscall(__NR_userfaultfd, O_CLOEXEC | O_NONBLOCK);\n\n\tif (dev->postcopy_ufd == -1) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) userfaultfd not available: %s\\n\",\n\t\t\tdev->ifname, strerror(errno));\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\tapi_struct.api = UFFD_API;\n\tapi_struct.features = 0;\n\tif (ioctl(dev->postcopy_ufd, UFFDIO_API, &api_struct)) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) UFFDIO_API ioctl failure: %s\\n\",\n\t\t\tdev->ifname, strerror(errno));\n\t\tclose(dev->postcopy_ufd);\n\t\tdev->postcopy_ufd = -1;\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\tctx->fds[0] = dev->postcopy_ufd;\n\tctx->fd_num = 1;\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n#else\n\tdev->postcopy_ufd = -1;\n\tctx->fd_num = 0;\n\n\treturn RTE_VHOST_MSG_RESULT_ERR;\n#endif\n}\n\nstatic int\nvhost_user_set_postcopy_listen(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx __rte_unused,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (dev->mem && dev->mem->nregions) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) regions already registered at postcopy-listen\\n\",\n\t\t\t\tdev->ifname);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\tdev->postcopy_listening = 1;\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\nstatic int\nvhost_user_postcopy_end(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tdev->postcopy_listening = 0;\n\tif (dev->postcopy_ufd >= 0) {\n\t\tclose(dev->postcopy_ufd);\n\t\tdev->postcopy_ufd = -1;\n\t}\n\n\tctx->msg.payload.u64 = 0;\n\tctx->msg.size = sizeof(ctx->msg.payload.u64);\n\tctx->fd_num = 0;\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n}\n\nstatic int\nvhost_user_get_status(struct virtio_net **pdev,\n\t\t      struct vhu_msg_context *ctx,\n\t\t      int main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tctx->msg.payload.u64 = dev->status;\n\tctx->msg.size = sizeof(ctx->msg.payload.u64);\n\tctx->fd_num = 0;\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n}\n\nstatic int\nvhost_user_set_status(struct virtio_net **pdev,\n\t\t\tstruct vhu_msg_context *ctx,\n\t\t\tint main_fd __rte_unused)\n{\n\tstruct virtio_net *dev = *pdev;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\t/* As per Virtio specification, the device status is 8bits long */\n\tif (ctx->msg.payload.u64 > UINT8_MAX) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid VHOST_USER_SET_STATUS payload 0x%\" PRIx64 \"\\n\",\n\t\t\t\tdev->ifname, ctx->msg.payload.u64);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tdev->status = ctx->msg.payload.u64;\n\n\tif ((dev->status & VIRTIO_DEVICE_STATUS_FEATURES_OK) &&\n\t    (dev->flags & VIRTIO_DEV_FEATURES_FAILED)) {\n\t\tVHOST_LOG_CONFIG(ERR,\n\t\t\t\t\"(%s) FEATURES_OK bit is set but feature negotiation failed\\n\",\n\t\t\t\tdev->ifname);\n\t\t/*\n\t\t * Clear the bit to let the driver know about the feature\n\t\t * negotiation failure\n\t\t */\n\t\tdev->status &= ~VIRTIO_DEVICE_STATUS_FEATURES_OK;\n\t}\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) new device status(0x%08x):\\n\", dev->ifname,\n\t\t\tdev->status);\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t-RESET: %u\\n\", dev->ifname,\n\t\t\t(dev->status == VIRTIO_DEVICE_STATUS_RESET));\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t-ACKNOWLEDGE: %u\\n\", dev->ifname,\n\t\t\t!!(dev->status & VIRTIO_DEVICE_STATUS_ACK));\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t-DRIVER: %u\\n\", dev->ifname,\n\t\t\t!!(dev->status & VIRTIO_DEVICE_STATUS_DRIVER));\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t-FEATURES_OK: %u\\n\", dev->ifname,\n\t\t\t!!(dev->status & VIRTIO_DEVICE_STATUS_FEATURES_OK));\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t-DRIVER_OK: %u\\n\", dev->ifname,\n\t\t\t!!(dev->status & VIRTIO_DEVICE_STATUS_DRIVER_OK));\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t-DEVICE_NEED_RESET: %u\\n\", dev->ifname,\n\t\t\t!!(dev->status & VIRTIO_DEVICE_STATUS_DEV_NEED_RESET));\n\tVHOST_LOG_CONFIG(INFO, \"(%s)\\t-FAILED: %u\\n\", dev->ifname,\n\t\t\t!!(dev->status & VIRTIO_DEVICE_STATUS_FAILED));\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}\n\ntypedef int (*vhost_message_handler_t)(struct virtio_net **pdev,\n\t\t\t\t\tstruct vhu_msg_context *ctx,\n\t\t\t\t\tint main_fd);\n\nstatic vhost_message_handler_t vhost_message_handlers[VHOST_USER_MAX] = {\n\t[VHOST_USER_NONE] = NULL,\n\t[VHOST_USER_GET_FEATURES] = vhost_user_get_features,\n\t[VHOST_USER_SET_FEATURES] = vhost_user_set_features,\n\t[VHOST_USER_SET_OWNER] = vhost_user_set_owner,\n\t[VHOST_USER_RESET_OWNER] = vhost_user_reset_owner,\n\t[VHOST_USER_SET_MEM_TABLE] = vhost_user_set_mem_table,\n\t[VHOST_USER_SET_LOG_BASE] = vhost_user_set_log_base,\n\t[VHOST_USER_SET_LOG_FD] = vhost_user_set_log_fd,\n\t[VHOST_USER_SET_VRING_NUM] = vhost_user_set_vring_num,\n\t[VHOST_USER_SET_VRING_ADDR] = vhost_user_set_vring_addr,\n\t[VHOST_USER_SET_VRING_BASE] = vhost_user_set_vring_base,\n\t[VHOST_USER_GET_VRING_BASE] = vhost_user_get_vring_base,\n\t[VHOST_USER_SET_VRING_KICK] = vhost_user_set_vring_kick,\n\t[VHOST_USER_SET_VRING_CALL] = vhost_user_set_vring_call,\n\t[VHOST_USER_SET_VRING_ERR] = vhost_user_set_vring_err,\n\t[VHOST_USER_GET_PROTOCOL_FEATURES] = vhost_user_get_protocol_features,\n\t[VHOST_USER_SET_PROTOCOL_FEATURES] = vhost_user_set_protocol_features,\n\t[VHOST_USER_GET_QUEUE_NUM] = vhost_user_get_queue_num,\n\t[VHOST_USER_SET_VRING_ENABLE] = vhost_user_set_vring_enable,\n\t[VHOST_USER_SEND_RARP] = vhost_user_send_rarp,\n\t[VHOST_USER_NET_SET_MTU] = vhost_user_net_set_mtu,\n\t[VHOST_USER_SET_SLAVE_REQ_FD] = vhost_user_set_req_fd,\n\t[VHOST_USER_IOTLB_MSG] = vhost_user_iotlb_msg,\n\t[VHOST_USER_POSTCOPY_ADVISE] = vhost_user_set_postcopy_advise,\n\t[VHOST_USER_POSTCOPY_LISTEN] = vhost_user_set_postcopy_listen,\n\t[VHOST_USER_POSTCOPY_END] = vhost_user_postcopy_end,\n\t[VHOST_USER_GET_INFLIGHT_FD] = vhost_user_get_inflight_fd,\n\t[VHOST_USER_SET_INFLIGHT_FD] = vhost_user_set_inflight_fd,\n\t[VHOST_USER_SET_STATUS] = vhost_user_set_status,\n\t[VHOST_USER_GET_STATUS] = vhost_user_get_status,\n};\n\n/* return bytes# of read on success or negative val on failure. */\nstatic int\nread_vhost_message(struct virtio_net *dev, int sockfd, struct  vhu_msg_context *ctx)\n{\n\tint ret;\n\n\tret = read_fd_message(dev->ifname, sockfd, (char *)&ctx->msg, VHOST_USER_HDR_SIZE,\n\t\tctx->fds, VHOST_MEMORY_MAX_NREGIONS, &ctx->fd_num);\n\tif (ret <= 0) {\n\t\treturn ret;\n\t} else if (ret != VHOST_USER_HDR_SIZE) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) Unexpected header size read\\n\", dev->ifname);\n\t\tclose_msg_fds(ctx);\n\t\treturn -1;\n\t}\n\n\tif (ctx->msg.size) {\n\t\tif (ctx->msg.size > sizeof(ctx->msg.payload)) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid msg size: %d\\n\",\n\t\t\t\t\tdev->ifname, ctx->msg.size);\n\t\t\treturn -1;\n\t\t}\n\t\tret = read(sockfd, &ctx->msg.payload, ctx->msg.size);\n\t\tif (ret <= 0)\n\t\t\treturn ret;\n\t\tif (ret != (int)ctx->msg.size) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) read control message failed\\n\", dev->ifname);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic int\nsend_vhost_message(struct virtio_net *dev, int sockfd, struct vhu_msg_context *ctx)\n{\n\tif (!ctx)\n\t\treturn 0;\n\n\treturn send_fd_message(dev->ifname, sockfd, (char *)&ctx->msg,\n\t\tVHOST_USER_HDR_SIZE + ctx->msg.size, ctx->fds, ctx->fd_num);\n}\n\nstatic int\nsend_vhost_reply(struct virtio_net *dev, int sockfd, struct vhu_msg_context *ctx)\n{\n\tif (!ctx)\n\t\treturn 0;\n\n\tctx->msg.flags &= ~VHOST_USER_VERSION_MASK;\n\tctx->msg.flags &= ~VHOST_USER_NEED_REPLY;\n\tctx->msg.flags |= VHOST_USER_VERSION;\n\tctx->msg.flags |= VHOST_USER_REPLY_MASK;\n\n\treturn send_vhost_message(dev, sockfd, ctx);\n}\n\nstatic int\nsend_vhost_slave_message(struct virtio_net *dev,\n\t\tstruct vhu_msg_context *ctx)\n{\n\tint ret;\n\n\tif (ctx->msg.flags & VHOST_USER_NEED_REPLY)\n\t\trte_spinlock_lock(&dev->slave_req_lock);\n\n\tret = send_vhost_message(dev, dev->slave_req_fd, ctx);\n\tif (ret < 0 && (ctx->msg.flags & VHOST_USER_NEED_REPLY))\n\t\trte_spinlock_unlock(&dev->slave_req_lock);\n\n\treturn ret;\n}\n\n/*\n * Allocate a queue pair if it hasn't been allocated yet\n */\nstatic int\nvhost_user_check_and_alloc_queue_pair(struct virtio_net *dev,\n\t\t\tstruct vhu_msg_context *ctx)\n{\n\tuint32_t vring_idx;\n\n\tswitch (ctx->msg.request.master) {\n\tcase VHOST_USER_SET_VRING_KICK:\n\tcase VHOST_USER_SET_VRING_CALL:\n\tcase VHOST_USER_SET_VRING_ERR:\n\t\tvring_idx = ctx->msg.payload.u64 & VHOST_USER_VRING_IDX_MASK;\n\t\tbreak;\n\tcase VHOST_USER_SET_VRING_NUM:\n\tcase VHOST_USER_SET_VRING_BASE:\n\tcase VHOST_USER_GET_VRING_BASE:\n\tcase VHOST_USER_SET_VRING_ENABLE:\n\t\tvring_idx = ctx->msg.payload.state.index;\n\t\tbreak;\n\tcase VHOST_USER_SET_VRING_ADDR:\n\t\tvring_idx = ctx->msg.payload.addr.index;\n\t\tbreak;\n\tcase VHOST_USER_SET_INFLIGHT_FD:\n\t\tvring_idx = ctx->msg.payload.inflight.num_queues - 1;\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (vring_idx >= VHOST_MAX_VRING) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid vring index: %u\\n\", dev->ifname, vring_idx);\n\t\treturn -1;\n\t}\n\n\tif (dev->virtqueue[vring_idx])\n\t\treturn 0;\n\n\treturn alloc_vring_queue(dev, vring_idx);\n}\n\nstatic void\nvhost_user_lock_all_queue_pairs(struct virtio_net *dev)\n{\n\tunsigned int i = 0;\n\tunsigned int vq_num = 0;\n\n\twhile (vq_num < dev->nr_vring) {\n\t\tstruct vhost_virtqueue *vq = dev->virtqueue[i];\n\n\t\tif (vq) {\n\t\t\trte_spinlock_lock(&vq->access_lock);\n\t\t\tvq_num++;\n\t\t}\n\t\ti++;\n\t}\n}\n\nstatic void\nvhost_user_unlock_all_queue_pairs(struct virtio_net *dev)\n{\n\tunsigned int i = 0;\n\tunsigned int vq_num = 0;\n\n\twhile (vq_num < dev->nr_vring) {\n\t\tstruct vhost_virtqueue *vq = dev->virtqueue[i];\n\n\t\tif (vq) {\n\t\t\trte_spinlock_unlock(&vq->access_lock);\n\t\t\tvq_num++;\n\t\t}\n\t\ti++;\n\t}\n}\n\nint\nvhost_user_msg_handler(int vid, int fd)\n{\n\tstruct virtio_net *dev;\n\tstruct vhu_msg_context ctx;\n\tstruct rte_vdpa_device *vdpa_dev;\n\tint ret;\n\tint unlock_required = 0;\n\tbool handled;\n\tint request;\n\tuint32_t i;\n\n\tdev = get_device(vid);\n\tif (dev == NULL)\n\t\treturn -1;\n\n\tif (!dev->notify_ops) {\n\t\tdev->notify_ops = vhost_driver_callback_get(dev->ifname);\n\t\tif (!dev->notify_ops) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to get callback ops for driver\\n\",\n\t\t\t\tdev->ifname);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tret = read_vhost_message(dev, fd, &ctx);\n\tif (ret <= 0) {\n\t\tif (ret < 0)\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) vhost read message failed\\n\", dev->ifname);\n\t\telse\n\t\t\tVHOST_LOG_CONFIG(INFO, \"(%s) vhost peer closed\\n\", dev->ifname);\n\n\t\treturn -1;\n\t}\n\n\tret = 0;\n\trequest = ctx.msg.request.master;\n\tif (request > VHOST_USER_NONE && request < VHOST_USER_MAX &&\n\t\t\tvhost_message_str[request]) {\n\t\tif (request != VHOST_USER_IOTLB_MSG)\n\t\t\tVHOST_LOG_CONFIG(INFO, \"(%s) read message %s\\n\",\n\t\t\t\tdev->ifname, vhost_message_str[request]);\n\t\telse\n\t\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) read message %s\\n\",\n\t\t\t\tdev->ifname, vhost_message_str[request]);\n\t} else {\n\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) external request %d\\n\", dev->ifname, request);\n\t}\n\n\tret = vhost_user_check_and_alloc_queue_pair(dev, &ctx);\n\tif (ret < 0) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to alloc queue\\n\", dev->ifname);\n\t\treturn -1;\n\t}\n\n\t/*\n\t * Note: we don't lock all queues on VHOST_USER_GET_VRING_BASE\n\t * and VHOST_USER_RESET_OWNER, since it is sent when virtio stops\n\t * and device is destroyed. destroy_device waits for queues to be\n\t * inactive, so it is safe. Otherwise taking the access_lock\n\t * would cause a dead lock.\n\t */\n\tswitch (request) {\n\tcase VHOST_USER_SET_FEATURES:\n\tcase VHOST_USER_SET_PROTOCOL_FEATURES:\n\tcase VHOST_USER_SET_OWNER:\n\tcase VHOST_USER_SET_MEM_TABLE:\n\tcase VHOST_USER_SET_LOG_BASE:\n\tcase VHOST_USER_SET_LOG_FD:\n\tcase VHOST_USER_SET_VRING_NUM:\n\tcase VHOST_USER_SET_VRING_ADDR:\n\tcase VHOST_USER_SET_VRING_BASE:\n\tcase VHOST_USER_SET_VRING_KICK:\n\tcase VHOST_USER_SET_VRING_CALL:\n\tcase VHOST_USER_SET_VRING_ERR:\n\tcase VHOST_USER_SET_VRING_ENABLE:\n\tcase VHOST_USER_SEND_RARP:\n\tcase VHOST_USER_NET_SET_MTU:\n\tcase VHOST_USER_SET_SLAVE_REQ_FD:\n\t\tif (!(dev->flags & VIRTIO_DEV_VDPA_CONFIGURED)) {\n\t\t\tvhost_user_lock_all_queue_pairs(dev);\n\t\t\tunlock_required = 1;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\n\t}\n\n\thandled = false;\n\tif (dev->extern_ops.pre_msg_handle) {\n\t\tRTE_BUILD_BUG_ON(offsetof(struct vhu_msg_context, msg) != 0);\n\t\tret = (*dev->extern_ops.pre_msg_handle)(dev->vid, &ctx);\n\t\tswitch (ret) {\n\t\tcase RTE_VHOST_MSG_RESULT_REPLY:\n\t\t\tsend_vhost_reply(dev, fd, &ctx);\n\t\t\t/* Fall-through */\n\t\tcase RTE_VHOST_MSG_RESULT_ERR:\n\t\tcase RTE_VHOST_MSG_RESULT_OK:\n\t\t\thandled = true;\n\t\t\tgoto skip_to_post_handle;\n\t\tcase RTE_VHOST_MSG_RESULT_NOT_HANDLED:\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (request > VHOST_USER_NONE && request < VHOST_USER_MAX) {\n\t\tif (!vhost_message_handlers[request])\n\t\t\tgoto skip_to_post_handle;\n\t\tret = vhost_message_handlers[request](&dev, &ctx, fd);\n\n\t\tswitch (ret) {\n\t\tcase RTE_VHOST_MSG_RESULT_ERR:\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) processing %s failed.\\n\",\n\t\t\t\t\tdev->ifname, vhost_message_str[request]);\n\t\t\thandled = true;\n\t\t\tbreak;\n\t\tcase RTE_VHOST_MSG_RESULT_OK:\n\t\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) processing %s succeeded.\\n\",\n\t\t\t\t\tdev->ifname, vhost_message_str[request]);\n\t\t\thandled = true;\n\t\t\tbreak;\n\t\tcase RTE_VHOST_MSG_RESULT_REPLY:\n\t\t\tVHOST_LOG_CONFIG(DEBUG, \"(%s) processing %s succeeded and needs reply.\\n\",\n\t\t\t\t\tdev->ifname, vhost_message_str[request]);\n\t\t\tsend_vhost_reply(dev, fd, &ctx);\n\t\t\thandled = true;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\nskip_to_post_handle:\n\tif (ret != RTE_VHOST_MSG_RESULT_ERR &&\n\t\t\tdev->extern_ops.post_msg_handle) {\n\t\tRTE_BUILD_BUG_ON(offsetof(struct vhu_msg_context, msg) != 0);\n\t\tret = (*dev->extern_ops.post_msg_handle)(dev->vid, &ctx);\n\t\tswitch (ret) {\n\t\tcase RTE_VHOST_MSG_RESULT_REPLY:\n\t\t\tsend_vhost_reply(dev, fd, &ctx);\n\t\t\t/* Fall-through */\n\t\tcase RTE_VHOST_MSG_RESULT_ERR:\n\t\tcase RTE_VHOST_MSG_RESULT_OK:\n\t\t\thandled = true;\n\t\tcase RTE_VHOST_MSG_RESULT_NOT_HANDLED:\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* If message was not handled at this stage, treat it as an error */\n\tif (!handled) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) vhost message (req: %d) was not handled.\\n\",\n\t\t\t\tdev->ifname, request);\n\t\tclose_msg_fds(&ctx);\n\t\tret = RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\t/*\n\t * If the request required a reply that was already sent,\n\t * this optional reply-ack won't be sent as the\n\t * VHOST_USER_NEED_REPLY was cleared in send_vhost_reply().\n\t */\n\tif (ctx.msg.flags & VHOST_USER_NEED_REPLY) {\n\t\tctx.msg.payload.u64 = ret == RTE_VHOST_MSG_RESULT_ERR;\n\t\tctx.msg.size = sizeof(ctx.msg.payload.u64);\n\t\tctx.fd_num = 0;\n\t\tsend_vhost_reply(dev, fd, &ctx);\n\t} else if (ret == RTE_VHOST_MSG_RESULT_ERR) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) vhost message handling failed.\\n\", dev->ifname);\n\t\treturn -1;\n\t}\n\n\tfor (i = 0; i < dev->nr_vring; i++) {\n\t\tstruct vhost_virtqueue *vq = dev->virtqueue[i];\n\t\tbool cur_ready = vq_is_ready(dev, vq);\n\n\t\tif (cur_ready != (vq && vq->ready)) {\n\t\t\tvq->ready = cur_ready;\n\t\t\tvhost_user_notify_queue_state(dev, i, cur_ready);\n\t\t}\n\t}\n\n\tif (unlock_required)\n\t\tvhost_user_unlock_all_queue_pairs(dev);\n\n\tif (!virtio_is_ready(dev))\n\t\tgoto out;\n\n\t/*\n\t * Virtio is now ready. If not done already, it is time\n\t * to notify the application it can process the rings and\n\t * configure the vDPA device if present.\n\t */\n\n\tif (!(dev->flags & VIRTIO_DEV_RUNNING)) {\n\t\tif (dev->notify_ops->new_device(dev->vid) == 0)\n\t\t\tdev->flags |= VIRTIO_DEV_RUNNING;\n\t}\n\n\tvdpa_dev = dev->vdpa_dev;\n\tif (!vdpa_dev)\n\t\tgoto out;\n\n\tif (!(dev->flags & VIRTIO_DEV_VDPA_CONFIGURED)) {\n\t\tif (vdpa_dev->ops->dev_conf(dev->vid))\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to configure vDPA device\\n\",\n\t\t\t\t\tdev->ifname);\n\t\telse\n\t\t\tdev->flags |= VIRTIO_DEV_VDPA_CONFIGURED;\n\t}\n\nout:\n\treturn 0;\n}\n\nstatic int process_slave_message_reply(struct virtio_net *dev,\n\t\t\t\t       const struct vhu_msg_context *ctx)\n{\n\tstruct vhu_msg_context msg_reply;\n\tint ret;\n\n\tif ((ctx->msg.flags & VHOST_USER_NEED_REPLY) == 0)\n\t\treturn 0;\n\n\tret = read_vhost_message(dev, dev->slave_req_fd, &msg_reply);\n\tif (ret <= 0) {\n\t\tif (ret < 0)\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) vhost read slave message reply failed\\n\",\n\t\t\t\t\tdev->ifname);\n\t\telse\n\t\t\tVHOST_LOG_CONFIG(INFO, \"(%s) vhost peer closed\\n\", dev->ifname);\n\t\tret = -1;\n\t\tgoto out;\n\t}\n\n\tret = 0;\n\tif (msg_reply.msg.request.slave != ctx->msg.request.slave) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) received unexpected msg type (%u), expected %u\\n\",\n\t\t\t\tdev->ifname, msg_reply.msg.request.slave, ctx->msg.request.slave);\n\t\tret = -1;\n\t\tgoto out;\n\t}\n\n\tret = msg_reply.msg.payload.u64 ? -1 : 0;\n\nout:\n\trte_spinlock_unlock(&dev->slave_req_lock);\n\treturn ret;\n}\n\nint\nvhost_user_iotlb_miss(struct virtio_net *dev, uint64_t iova, uint8_t perm)\n{\n\tint ret;\n\tstruct vhu_msg_context ctx = {\n\t\t.msg = {\n\t\t\t.request.slave = VHOST_USER_SLAVE_IOTLB_MSG,\n\t\t\t.flags = VHOST_USER_VERSION,\n\t\t\t.size = sizeof(ctx.msg.payload.iotlb),\n\t\t\t.payload.iotlb = {\n\t\t\t\t.iova = iova,\n\t\t\t\t.perm = perm,\n\t\t\t\t.type = VHOST_IOTLB_MISS,\n\t\t\t},\n\t\t},\n\t};\n\n\tret = send_vhost_message(dev, dev->slave_req_fd, &ctx);\n\tif (ret < 0) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to send IOTLB miss message (%d)\\n\",\n\t\t\t\tdev->ifname, ret);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nvhost_user_slave_config_change(struct virtio_net *dev, bool need_reply)\n{\n\tint ret;\n\tstruct vhu_msg_context ctx = {\n\t\t.msg = {\n\t\t\t.request.slave = VHOST_USER_SLAVE_CONFIG_CHANGE_MSG,\n\t\t\t.flags = VHOST_USER_VERSION,\n\t\t\t.size = 0,\n\t\t}\n\t};\n\n\tif (need_reply)\n\t\tctx.msg.flags |= VHOST_USER_NEED_REPLY;\n\n\tret = send_vhost_slave_message(dev, &ctx);\n\tif (ret < 0) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to send config change (%d)\\n\",\n\t\t\t\tdev->ifname, ret);\n\t\treturn ret;\n\t}\n\n\treturn process_slave_message_reply(dev, &ctx);\n}\n\nint\nrte_vhost_slave_config_change(int vid, bool need_reply)\n{\n\tstruct virtio_net *dev;\n\n\tdev = get_device(vid);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\treturn vhost_user_slave_config_change(dev, need_reply);\n}\n\nstatic int vhost_user_slave_set_vring_host_notifier(struct virtio_net *dev,\n\t\t\t\t\t\t    int index, int fd,\n\t\t\t\t\t\t    uint64_t offset,\n\t\t\t\t\t\t    uint64_t size)\n{\n\tint ret;\n\tstruct vhu_msg_context ctx = {\n\t\t.msg = {\n\t\t\t.request.slave = VHOST_USER_SLAVE_VRING_HOST_NOTIFIER_MSG,\n\t\t\t.flags = VHOST_USER_VERSION | VHOST_USER_NEED_REPLY,\n\t\t\t.size = sizeof(ctx.msg.payload.area),\n\t\t\t.payload.area = {\n\t\t\t\t.u64 = index & VHOST_USER_VRING_IDX_MASK,\n\t\t\t\t.size = size,\n\t\t\t\t.offset = offset,\n\t\t\t},\n\t\t},\n\t};\n\n\tif (fd < 0)\n\t\tctx.msg.payload.area.u64 |= VHOST_USER_VRING_NOFD_MASK;\n\telse {\n\t\tctx.fds[0] = fd;\n\t\tctx.fd_num = 1;\n\t}\n\n\tret = send_vhost_slave_message(dev, &ctx);\n\tif (ret < 0) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to set host notifier (%d)\\n\",\n\t\t\t\tdev->ifname, ret);\n\t\treturn ret;\n\t}\n\n\treturn process_slave_message_reply(dev, &ctx);\n}\n\nint rte_vhost_host_notifier_ctrl(int vid, uint16_t qid, bool enable)\n{\n\tstruct virtio_net *dev;\n\tstruct rte_vdpa_device *vdpa_dev;\n\tint vfio_device_fd, ret = 0;\n\tuint64_t offset, size;\n\tunsigned int i, q_start, q_last;\n\n\tdev = get_device(vid);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tvdpa_dev = dev->vdpa_dev;\n\tif (vdpa_dev == NULL)\n\t\treturn -ENODEV;\n\n\tif (!(dev->features & (1ULL << VIRTIO_F_VERSION_1)) ||\n\t    !(dev->features & (1ULL << VHOST_USER_F_PROTOCOL_FEATURES)) ||\n\t    !(dev->protocol_features &\n\t\t\t(1ULL << VHOST_USER_PROTOCOL_F_SLAVE_REQ)) ||\n\t    !(dev->protocol_features &\n\t\t\t(1ULL << VHOST_USER_PROTOCOL_F_SLAVE_SEND_FD)) ||\n\t    !(dev->protocol_features &\n\t\t\t(1ULL << VHOST_USER_PROTOCOL_F_HOST_NOTIFIER)))\n\t\treturn -ENOTSUP;\n\n\tif (qid == RTE_VHOST_QUEUE_ALL) {\n\t\tq_start = 0;\n\t\tq_last = dev->nr_vring - 1;\n\t} else {\n\t\tif (qid >= dev->nr_vring)\n\t\t\treturn -EINVAL;\n\t\tq_start = qid;\n\t\tq_last = qid;\n\t}\n\n\tRTE_FUNC_PTR_OR_ERR_RET(vdpa_dev->ops->get_vfio_device_fd, -ENOTSUP);\n\tRTE_FUNC_PTR_OR_ERR_RET(vdpa_dev->ops->get_notify_area, -ENOTSUP);\n\n\tvfio_device_fd = vdpa_dev->ops->get_vfio_device_fd(vid);\n\tif (vfio_device_fd < 0)\n\t\treturn -ENOTSUP;\n\n\tif (enable) {\n\t\tfor (i = q_start; i <= q_last; i++) {\n\t\t\tif (vdpa_dev->ops->get_notify_area(vid, i, &offset,\n\t\t\t\t\t&size) < 0) {\n\t\t\t\tret = -ENOTSUP;\n\t\t\t\tgoto disable;\n\t\t\t}\n\n\t\t\tif (vhost_user_slave_set_vring_host_notifier(dev, i,\n\t\t\t\t\tvfio_device_fd, offset, size) < 0) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto disable;\n\t\t\t}\n\t\t}\n\t} else {\ndisable:\n\t\tfor (i = q_start; i <= q_last; i++) {\n\t\t\tvhost_user_slave_set_vring_host_notifier(dev, i, -1,\n\t\t\t\t\t0, 0);\n\t\t}\n\t}\n\n\treturn ret;\n}\n"], "filenames": ["lib/vhost/vhost_user.c"], "buggy_code_start_loc": [1604], "buggy_code_end_loc": [1700], "fixing_code_start_loc": [1605], "fixing_code_end_loc": [1707], "type": "NVD-CWE-noinfo", "message": "A flaw was found in dpdk. This flaw allows a malicious vhost-user master to attach an unexpected number of fds as ancillary data to VHOST_USER_GET_INFLIGHT_FD / VHOST_USER_SET_INFLIGHT_FD messages that are not closed by the vhost-user slave. By sending such messages continuously, the vhost-user master exhausts available fd in the vhost-user slave process, leading to a denial of service.", "other": {"cve": {"id": "CVE-2022-0669", "sourceIdentifier": "secalert@redhat.com", "published": "2022-08-29T15:15:09.750", "lastModified": "2022-09-01T20:35:47.027", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "A flaw was found in dpdk. This flaw allows a malicious vhost-user master to attach an unexpected number of fds as ancillary data to VHOST_USER_GET_INFLIGHT_FD / VHOST_USER_SET_INFLIGHT_FD messages that are not closed by the vhost-user slave. By sending such messages continuously, the vhost-user master exhausts available fd in the vhost-user slave process, leading to a denial of service."}, {"lang": "es", "value": "Se ha encontrado un fallo en dpdk. Este fallo permite a un vhost-user master malicioso adjuntar un n\u00famero inesperado de fds como datos auxiliares a los mensajes VHOST_USER_GET_INFLIGHT_FD / VHOST_USER_SET_INFLIGHT_FD que no son cerrados por el vhost-user slave. Al enviar dichos mensajes continuamente, el maestro vhost-user agota los fd disponibles en el proceso esclavo vhost-user, conllevando a una denegaci\u00f3n de servicio"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.0, "impactScore": 4.0}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}, {"source": "secalert@redhat.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-400"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:dpdk:data_plane_development_kit:*:*:*:*:*:*:*:*", "versionStartIncluding": "20.02", "versionEndExcluding": "22.03", "matchCriteriaId": "00189B34-1D41-4AE8-988A-65013F529ABA"}, {"vulnerable": true, "criteria": "cpe:2.3:a:dpdk:data_plane_development_kit:19.11:*:*:*:*:*:*:*", "matchCriteriaId": "D54C8537-09ED-447B-A677-C1B31CD43BE0"}, {"vulnerable": true, "criteria": "cpe:2.3:a:dpdk:data_plane_development_kit:19.11:rc1:*:*:*:*:*:*", "matchCriteriaId": "7D64C9BE-1254-4E55-A4B9-BE0059E4AC88"}, {"vulnerable": true, "criteria": "cpe:2.3:a:dpdk:data_plane_development_kit:19.11:rc2:*:*:*:*:*:*", "matchCriteriaId": "98F76795-E087-4163-8803-2DFA0571F720"}, {"vulnerable": true, "criteria": "cpe:2.3:a:dpdk:data_plane_development_kit:19.11:rc3:*:*:*:*:*:*", "matchCriteriaId": "F057C6ED-3DC1-41AD-A982-3DBA9FFBDC83"}, {"vulnerable": true, "criteria": "cpe:2.3:a:dpdk:data_plane_development_kit:19.11:rc4:*:*:*:*:*:*", "matchCriteriaId": "F65AF826-2336-48B7-A364-BEAC013CA4BC"}, {"vulnerable": true, "criteria": "cpe:2.3:a:dpdk:data_plane_development_kit:22.03:rc1:*:*:*:*:*:*", "matchCriteriaId": "CB0AB20E-A20A-4087-B944-6A1B6E7E936B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:dpdk:data_plane_development_kit:22.03:rc2:*:*:*:*:*:*", "matchCriteriaId": "4A0E6108-A040-4749-85A6-C1DA127F482A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:dpdk:data_plane_development_kit:22.03:rc3:*:*:*:*:*:*", "matchCriteriaId": "844676DA-EA6F-4DA7-8248-6AD0139CC919"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:openvswitch:openvswitch:2.13.0:*:*:*:*:*:*:*", "matchCriteriaId": "905886CA-734C-4988-8882-664826DFFEC2"}, {"vulnerable": true, "criteria": "cpe:2.3:a:openvswitch:openvswitch:2.15.0:*:*:*:*:*:*:*", "matchCriteriaId": "8B5C7BBB-D091-4A58-9316-AECF82506865"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:redhat:openshift_container_platform:4.0:*:*:*:*:*:*:*", "matchCriteriaId": "932D137F-528B-4526-9A89-CD59FA1AB0FE"}]}]}], "references": [{"url": "https://access.redhat.com/security/cve/CVE-2022-0669", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://bugs.dpdk.org/show_bug.cgi?id=922", "source": "secalert@redhat.com", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=2055793", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/DPDK/dpdk/commit/af74f7db384ed149fe42b21dbd7975f8a54ef227", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://security-tracker.debian.org/tracker/CVE-2022-0669", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/DPDK/dpdk/commit/af74f7db384ed149fe42b21dbd7975f8a54ef227"}}