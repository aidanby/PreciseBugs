{"buggy_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tImplementation of the Transmission Control Protocol(TCP).\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\t\tCorey Minyard <wf-rch!minyard@relay.EU.net>\n *\t\tFlorian La Roche, <flla@stud.uni-sb.de>\n *\t\tCharles Hedrick, <hedrick@klinzhai.rutgers.edu>\n *\t\tLinus Torvalds, <torvalds@cs.helsinki.fi>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\t\tMatthew Dillon, <dillon@apollo.west.oic.com>\n *\t\tArnt Gulbrandsen, <agulbra@nvg.unit.no>\n *\t\tJorge Cwik, <jorge@laser.satlink.net>\n *\n * Fixes:\n *\t\tAlan Cox\t:\tNumerous verify_area() calls\n *\t\tAlan Cox\t:\tSet the ACK bit on a reset\n *\t\tAlan Cox\t:\tStopped it crashing if it closed while\n *\t\t\t\t\tsk->inuse=1 and was trying to connect\n *\t\t\t\t\t(tcp_err()).\n *\t\tAlan Cox\t:\tAll icmp error handling was broken\n *\t\t\t\t\tpointers passed where wrong and the\n *\t\t\t\t\tsocket was looked up backwards. Nobody\n *\t\t\t\t\ttested any icmp error code obviously.\n *\t\tAlan Cox\t:\ttcp_err() now handled properly. It\n *\t\t\t\t\twakes people on errors. poll\n *\t\t\t\t\tbehaves and the icmp error race\n *\t\t\t\t\thas gone by moving it into sock.c\n *\t\tAlan Cox\t:\ttcp_send_reset() fixed to work for\n *\t\t\t\t\teverything not just packets for\n *\t\t\t\t\tunknown sockets.\n *\t\tAlan Cox\t:\ttcp option processing.\n *\t\tAlan Cox\t:\tReset tweaked (still not 100%) [Had\n *\t\t\t\t\tsyn rule wrong]\n *\t\tHerp Rosmanith  :\tMore reset fixes\n *\t\tAlan Cox\t:\tNo longer acks invalid rst frames.\n *\t\t\t\t\tAcking any kind of RST is right out.\n *\t\tAlan Cox\t:\tSets an ignore me flag on an rst\n *\t\t\t\t\treceive otherwise odd bits of prattle\n *\t\t\t\t\tescape still\n *\t\tAlan Cox\t:\tFixed another acking RST frame bug.\n *\t\t\t\t\tShould stop LAN workplace lockups.\n *\t\tAlan Cox\t: \tSome tidyups using the new skb list\n *\t\t\t\t\tfacilities\n *\t\tAlan Cox\t:\tsk->keepopen now seems to work\n *\t\tAlan Cox\t:\tPulls options out correctly on accepts\n *\t\tAlan Cox\t:\tFixed assorted sk->rqueue->next errors\n *\t\tAlan Cox\t:\tPSH doesn't end a TCP read. Switched a\n *\t\t\t\t\tbit to skb ops.\n *\t\tAlan Cox\t:\tTidied tcp_data to avoid a potential\n *\t\t\t\t\tnasty.\n *\t\tAlan Cox\t:\tAdded some better commenting, as the\n *\t\t\t\t\ttcp is hard to follow\n *\t\tAlan Cox\t:\tRemoved incorrect check for 20 * psh\n *\tMichael O'Reilly\t:\tack < copied bug fix.\n *\tJohannes Stille\t\t:\tMisc tcp fixes (not all in yet).\n *\t\tAlan Cox\t:\tFIN with no memory -> CRASH\n *\t\tAlan Cox\t:\tAdded socket option proto entries.\n *\t\t\t\t\tAlso added awareness of them to accept.\n *\t\tAlan Cox\t:\tAdded TCP options (SOL_TCP)\n *\t\tAlan Cox\t:\tSwitched wakeup calls to callbacks,\n *\t\t\t\t\tso the kernel can layer network\n *\t\t\t\t\tsockets.\n *\t\tAlan Cox\t:\tUse ip_tos/ip_ttl settings.\n *\t\tAlan Cox\t:\tHandle FIN (more) properly (we hope).\n *\t\tAlan Cox\t:\tRST frames sent on unsynchronised\n *\t\t\t\t\tstate ack error.\n *\t\tAlan Cox\t:\tPut in missing check for SYN bit.\n *\t\tAlan Cox\t:\tAdded tcp_select_window() aka NET2E\n *\t\t\t\t\twindow non shrink trick.\n *\t\tAlan Cox\t:\tAdded a couple of small NET2E timer\n *\t\t\t\t\tfixes\n *\t\tCharles Hedrick :\tTCP fixes\n *\t\tToomas Tamm\t:\tTCP window fixes\n *\t\tAlan Cox\t:\tSmall URG fix to rlogin ^C ack fight\n *\t\tCharles Hedrick\t:\tRewrote most of it to actually work\n *\t\tLinus\t\t:\tRewrote tcp_read() and URG handling\n *\t\t\t\t\tcompletely\n *\t\tGerhard Koerting:\tFixed some missing timer handling\n *\t\tMatthew Dillon  :\tReworked TCP machine states as per RFC\n *\t\tGerhard Koerting:\tPC/TCP workarounds\n *\t\tAdam Caldwell\t:\tAssorted timer/timing errors\n *\t\tMatthew Dillon\t:\tFixed another RST bug\n *\t\tAlan Cox\t:\tMove to kernel side addressing changes.\n *\t\tAlan Cox\t:\tBeginning work on TCP fastpathing\n *\t\t\t\t\t(not yet usable)\n *\t\tArnt Gulbrandsen:\tTurbocharged tcp_check() routine.\n *\t\tAlan Cox\t:\tTCP fast path debugging\n *\t\tAlan Cox\t:\tWindow clamping\n *\t\tMichael Riepe\t:\tBug in tcp_check()\n *\t\tMatt Dillon\t:\tMore TCP improvements and RST bug fixes\n *\t\tMatt Dillon\t:\tYet more small nasties remove from the\n *\t\t\t\t\tTCP code (Be very nice to this man if\n *\t\t\t\t\ttcp finally works 100%) 8)\n *\t\tAlan Cox\t:\tBSD accept semantics.\n *\t\tAlan Cox\t:\tReset on closedown bug.\n *\tPeter De Schrijver\t:\tENOTCONN check missing in tcp_sendto().\n *\t\tMichael Pall\t:\tHandle poll() after URG properly in\n *\t\t\t\t\tall cases.\n *\t\tMichael Pall\t:\tUndo the last fix in tcp_read_urg()\n *\t\t\t\t\t(multi URG PUSH broke rlogin).\n *\t\tMichael Pall\t:\tFix the multi URG PUSH problem in\n *\t\t\t\t\ttcp_readable(), poll() after URG\n *\t\t\t\t\tworks now.\n *\t\tMichael Pall\t:\trecv(...,MSG_OOB) never blocks in the\n *\t\t\t\t\tBSD api.\n *\t\tAlan Cox\t:\tChanged the semantics of sk->socket to\n *\t\t\t\t\tfix a race and a signal problem with\n *\t\t\t\t\taccept() and async I/O.\n *\t\tAlan Cox\t:\tRelaxed the rules on tcp_sendto().\n *\t\tYury Shevchuk\t:\tReally fixed accept() blocking problem.\n *\t\tCraig I. Hagan  :\tAllow for BSD compatible TIME_WAIT for\n *\t\t\t\t\tclients/servers which listen in on\n *\t\t\t\t\tfixed ports.\n *\t\tAlan Cox\t:\tCleaned the above up and shrank it to\n *\t\t\t\t\ta sensible code size.\n *\t\tAlan Cox\t:\tSelf connect lockup fix.\n *\t\tAlan Cox\t:\tNo connect to multicast.\n *\t\tRoss Biro\t:\tClose unaccepted children on master\n *\t\t\t\t\tsocket close.\n *\t\tAlan Cox\t:\tReset tracing code.\n *\t\tAlan Cox\t:\tSpurious resets on shutdown.\n *\t\tAlan Cox\t:\tGiant 15 minute/60 second timer error\n *\t\tAlan Cox\t:\tSmall whoops in polling before an\n *\t\t\t\t\taccept.\n *\t\tAlan Cox\t:\tKept the state trace facility since\n *\t\t\t\t\tit's handy for debugging.\n *\t\tAlan Cox\t:\tMore reset handler fixes.\n *\t\tAlan Cox\t:\tStarted rewriting the code based on\n *\t\t\t\t\tthe RFC's for other useful protocol\n *\t\t\t\t\treferences see: Comer, KA9Q NOS, and\n *\t\t\t\t\tfor a reference on the difference\n *\t\t\t\t\tbetween specifications and how BSD\n *\t\t\t\t\tworks see the 4.4lite source.\n *\t\tA.N.Kuznetsov\t:\tDon't time wait on completion of tidy\n *\t\t\t\t\tclose.\n *\t\tLinus Torvalds\t:\tFin/Shutdown & copied_seq changes.\n *\t\tLinus Torvalds\t:\tFixed BSD port reuse to work first syn\n *\t\tAlan Cox\t:\tReimplemented timers as per the RFC\n *\t\t\t\t\tand using multiple timers for sanity.\n *\t\tAlan Cox\t:\tSmall bug fixes, and a lot of new\n *\t\t\t\t\tcomments.\n *\t\tAlan Cox\t:\tFixed dual reader crash by locking\n *\t\t\t\t\tthe buffers (much like datagram.c)\n *\t\tAlan Cox\t:\tFixed stuck sockets in probe. A probe\n *\t\t\t\t\tnow gets fed up of retrying without\n *\t\t\t\t\t(even a no space) answer.\n *\t\tAlan Cox\t:\tExtracted closing code better\n *\t\tAlan Cox\t:\tFixed the closing state machine to\n *\t\t\t\t\tresemble the RFC.\n *\t\tAlan Cox\t:\tMore 'per spec' fixes.\n *\t\tJorge Cwik\t:\tEven faster checksumming.\n *\t\tAlan Cox\t:\ttcp_data() doesn't ack illegal PSH\n *\t\t\t\t\tonly frames. At least one pc tcp stack\n *\t\t\t\t\tgenerates them.\n *\t\tAlan Cox\t:\tCache last socket.\n *\t\tAlan Cox\t:\tPer route irtt.\n *\t\tMatt Day\t:\tpoll()->select() match BSD precisely on error\n *\t\tAlan Cox\t:\tNew buffers\n *\t\tMarc Tamsky\t:\tVarious sk->prot->retransmits and\n *\t\t\t\t\tsk->retransmits misupdating fixed.\n *\t\t\t\t\tFixed tcp_write_timeout: stuck close,\n *\t\t\t\t\tand TCP syn retries gets used now.\n *\t\tMark Yarvis\t:\tIn tcp_read_wakeup(), don't send an\n *\t\t\t\t\tack if state is TCP_CLOSED.\n *\t\tAlan Cox\t:\tLook up device on a retransmit - routes may\n *\t\t\t\t\tchange. Doesn't yet cope with MSS shrink right\n *\t\t\t\t\tbut it's a start!\n *\t\tMarc Tamsky\t:\tClosing in closing fixes.\n *\t\tMike Shaver\t:\tRFC1122 verifications.\n *\t\tAlan Cox\t:\trcv_saddr errors.\n *\t\tAlan Cox\t:\tBlock double connect().\n *\t\tAlan Cox\t:\tSmall hooks for enSKIP.\n *\t\tAlexey Kuznetsov:\tPath MTU discovery.\n *\t\tAlan Cox\t:\tSupport soft errors.\n *\t\tAlan Cox\t:\tFix MTU discovery pathological case\n *\t\t\t\t\twhen the remote claims no mtu!\n *\t\tMarc Tamsky\t:\tTCP_CLOSE fix.\n *\t\tColin (G3TNE)\t:\tSend a reset on syn ack replies in\n *\t\t\t\t\twindow but wrong (fixes NT lpd problems)\n *\t\tPedro Roque\t:\tBetter TCP window handling, delayed ack.\n *\t\tJoerg Reuter\t:\tNo modification of locked buffers in\n *\t\t\t\t\ttcp_do_retransmit()\n *\t\tEric Schenk\t:\tChanged receiver side silly window\n *\t\t\t\t\tavoidance algorithm to BSD style\n *\t\t\t\t\talgorithm. This doubles throughput\n *\t\t\t\t\tagainst machines running Solaris,\n *\t\t\t\t\tand seems to result in general\n *\t\t\t\t\timprovement.\n *\tStefan Magdalinski\t:\tadjusted tcp_readable() to fix FIONREAD\n *\tWilly Konynenberg\t:\tTransparent proxying support.\n *\tMike McLagan\t\t:\tRouting by source\n *\t\tKeith Owens\t:\tDo proper merging with partial SKB's in\n *\t\t\t\t\ttcp_do_sendmsg to avoid burstiness.\n *\t\tEric Schenk\t:\tFix fast close down bug with\n *\t\t\t\t\tshutdown() followed by close().\n *\t\tAndi Kleen \t:\tMake poll agree with SIGIO\n *\tSalvatore Sanfilippo\t:\tSupport SO_LINGER with linger == 1 and\n *\t\t\t\t\tlingertime == 0 (RFC 793 ABORT Call)\n *\tHirokazu Takahashi\t:\tUse copy_from_user() instead of\n *\t\t\t\t\tcsum_and_copy_from_user() if possible.\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or(at your option) any later version.\n *\n * Description of States:\n *\n *\tTCP_SYN_SENT\t\tsent a connection request, waiting for ack\n *\n *\tTCP_SYN_RECV\t\treceived a connection request, sent ack,\n *\t\t\t\twaiting for final ack in three-way handshake.\n *\n *\tTCP_ESTABLISHED\t\tconnection established\n *\n *\tTCP_FIN_WAIT1\t\tour side has shutdown, waiting to complete\n *\t\t\t\ttransmission of remaining buffered data\n *\n *\tTCP_FIN_WAIT2\t\tall buffered data sent, waiting for remote\n *\t\t\t\tto shutdown\n *\n *\tTCP_CLOSING\t\tboth sides have shutdown but we still have\n *\t\t\t\tdata we have to finish sending\n *\n *\tTCP_TIME_WAIT\t\ttimeout to catch resent junk before entering\n *\t\t\t\tclosed, can only be entered from FIN_WAIT2\n *\t\t\t\tor CLOSING.  Required because the other end\n *\t\t\t\tmay not have gotten our last ACK causing it\n *\t\t\t\tto retransmit the data packet (which we ignore)\n *\n *\tTCP_CLOSE_WAIT\t\tremote side has shutdown and is waiting for\n *\t\t\t\tus to finish writing our data and to shutdown\n *\t\t\t\t(we have to close() to move on to LAST_ACK)\n *\n *\tTCP_LAST_ACK\t\tout side has shutdown after remote has\n *\t\t\t\tshutdown.  There may still be data in our\n *\t\t\t\tbuffer that we have to finish sending\n *\n *\tTCP_CLOSE\t\tsocket is finished\n */\n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/fcntl.h>\n#include <linux/poll.h>\n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/skbuff.h>\n#include <linux/scatterlist.h>\n#include <linux/splice.h>\n#include <linux/net.h>\n#include <linux/socket.h>\n#include <linux/random.h>\n#include <linux/bootmem.h>\n#include <linux/highmem.h>\n#include <linux/swap.h>\n#include <linux/cache.h>\n#include <linux/err.h>\n#include <linux/crypto.h>\n#include <linux/time.h>\n\n#include <net/icmp.h>\n#include <net/tcp.h>\n#include <net/xfrm.h>\n#include <net/ip.h>\n#include <net/netdma.h>\n#include <net/sock.h>\n\n#include <asm/uaccess.h>\n#include <asm/ioctls.h>\n\nint sysctl_tcp_fin_timeout __read_mostly = TCP_FIN_TIMEOUT;\n\nstruct percpu_counter tcp_orphan_count;\nEXPORT_SYMBOL_GPL(tcp_orphan_count);\n\nint sysctl_tcp_mem[3] __read_mostly;\nint sysctl_tcp_wmem[3] __read_mostly;\nint sysctl_tcp_rmem[3] __read_mostly;\n\nEXPORT_SYMBOL(sysctl_tcp_mem);\nEXPORT_SYMBOL(sysctl_tcp_rmem);\nEXPORT_SYMBOL(sysctl_tcp_wmem);\n\natomic_t tcp_memory_allocated;\t/* Current allocated memory. */\nEXPORT_SYMBOL(tcp_memory_allocated);\n\n/*\n * Current number of TCP sockets.\n */\nstruct percpu_counter tcp_sockets_allocated;\nEXPORT_SYMBOL(tcp_sockets_allocated);\n\n/*\n * TCP splice context\n */\nstruct tcp_splice_state {\n\tstruct pipe_inode_info *pipe;\n\tsize_t len;\n\tunsigned int flags;\n};\n\n/*\n * Pressure flag: try to collapse.\n * Technical note: it is used by multiple contexts non atomically.\n * All the __sk_mem_schedule() is of this nature: accounting\n * is strict, actions are advisory and have some latency.\n */\nint tcp_memory_pressure __read_mostly;\n\nEXPORT_SYMBOL(tcp_memory_pressure);\n\nvoid tcp_enter_memory_pressure(struct sock *sk)\n{\n\tif (!tcp_memory_pressure) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMEMORYPRESSURES);\n\t\ttcp_memory_pressure = 1;\n\t}\n}\n\nEXPORT_SYMBOL(tcp_enter_memory_pressure);\n\n/* Convert seconds to retransmits based on initial and max timeout */\nstatic u8 secs_to_retrans(int seconds, int timeout, int rto_max)\n{\n\tu8 res = 0;\n\n\tif (seconds > 0) {\n\t\tint period = timeout;\n\n\t\tres = 1;\n\t\twhile (seconds > period && res < 255) {\n\t\t\tres++;\n\t\t\ttimeout <<= 1;\n\t\t\tif (timeout > rto_max)\n\t\t\t\ttimeout = rto_max;\n\t\t\tperiod += timeout;\n\t\t}\n\t}\n\treturn res;\n}\n\n/* Convert retransmits to seconds based on initial and max timeout */\nstatic int retrans_to_secs(u8 retrans, int timeout, int rto_max)\n{\n\tint period = 0;\n\n\tif (retrans > 0) {\n\t\tperiod = timeout;\n\t\twhile (--retrans) {\n\t\t\ttimeout <<= 1;\n\t\t\tif (timeout > rto_max)\n\t\t\t\ttimeout = rto_max;\n\t\t\tperiod += timeout;\n\t\t}\n\t}\n\treturn period;\n}\n\n/*\n *\tWait for a TCP event.\n *\n *\tNote that we don't need to lock the socket, as the upper poll layers\n *\ttake care of normal races (between the test and the event) and we don't\n *\tgo look at any of the socket buffers directly.\n */\nunsigned int tcp_poll(struct file *file, struct socket *sock, poll_table *wait)\n{\n\tunsigned int mask;\n\tstruct sock *sk = sock->sk;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tsock_poll_wait(file, sk->sk_sleep, wait);\n\tif (sk->sk_state == TCP_LISTEN)\n\t\treturn inet_csk_listen_poll(sk);\n\n\t/* Socket is not locked. We are protected from async events\n\t * by poll logic and correct handling of state changes\n\t * made by other threads is impossible in any case.\n\t */\n\n\tmask = 0;\n\tif (sk->sk_err)\n\t\tmask = POLLERR;\n\n\t/*\n\t * POLLHUP is certainly not done right. But poll() doesn't\n\t * have a notion of HUP in just one direction, and for a\n\t * socket the read side is more interesting.\n\t *\n\t * Some poll() documentation says that POLLHUP is incompatible\n\t * with the POLLOUT/POLLWR flags, so somebody should check this\n\t * all. But careful, it tends to be safer to return too many\n\t * bits than too few, and you can easily break real applications\n\t * if you don't tell them that something has hung up!\n\t *\n\t * Check-me.\n\t *\n\t * Check number 1. POLLHUP is _UNMASKABLE_ event (see UNIX98 and\n\t * our fs/select.c). It means that after we received EOF,\n\t * poll always returns immediately, making impossible poll() on write()\n\t * in state CLOSE_WAIT. One solution is evident --- to set POLLHUP\n\t * if and only if shutdown has been made in both directions.\n\t * Actually, it is interesting to look how Solaris and DUX\n\t * solve this dilemma. I would prefer, if POLLHUP were maskable,\n\t * then we could set it on SND_SHUTDOWN. BTW examples given\n\t * in Stevens' books assume exactly this behaviour, it explains\n\t * why POLLHUP is incompatible with POLLOUT.\t--ANK\n\t *\n\t * NOTE. Check for TCP_CLOSE is added. The goal is to prevent\n\t * blocking on fresh not-connected or disconnected socket. --ANK\n\t */\n\tif (sk->sk_shutdown == SHUTDOWN_MASK || sk->sk_state == TCP_CLOSE)\n\t\tmask |= POLLHUP;\n\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\tmask |= POLLIN | POLLRDNORM | POLLRDHUP;\n\n\t/* Connected? */\n\tif ((1 << sk->sk_state) & ~(TCPF_SYN_SENT | TCPF_SYN_RECV)) {\n\t\tint target = sock_rcvlowat(sk, 0, INT_MAX);\n\n\t\tif (tp->urg_seq == tp->copied_seq &&\n\t\t    !sock_flag(sk, SOCK_URGINLINE) &&\n\t\t    tp->urg_data)\n\t\t\ttarget++;\n\n\t\t/* Potential race condition. If read of tp below will\n\t\t * escape above sk->sk_state, we can be illegally awaken\n\t\t * in SYN_* states. */\n\t\tif (tp->rcv_nxt - tp->copied_seq >= target)\n\t\t\tmask |= POLLIN | POLLRDNORM;\n\n\t\tif (!(sk->sk_shutdown & SEND_SHUTDOWN)) {\n\t\t\tif (sk_stream_wspace(sk) >= sk_stream_min_wspace(sk)) {\n\t\t\t\tmask |= POLLOUT | POLLWRNORM;\n\t\t\t} else {  /* send SIGIO later */\n\t\t\t\tset_bit(SOCK_ASYNC_NOSPACE,\n\t\t\t\t\t&sk->sk_socket->flags);\n\t\t\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\n\t\t\t\t/* Race breaker. If space is freed after\n\t\t\t\t * wspace test but before the flags are set,\n\t\t\t\t * IO signal will be lost.\n\t\t\t\t */\n\t\t\t\tif (sk_stream_wspace(sk) >= sk_stream_min_wspace(sk))\n\t\t\t\t\tmask |= POLLOUT | POLLWRNORM;\n\t\t\t}\n\t\t}\n\n\t\tif (tp->urg_data & TCP_URG_VALID)\n\t\t\tmask |= POLLPRI;\n\t}\n\treturn mask;\n}\n\nint tcp_ioctl(struct sock *sk, int cmd, unsigned long arg)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint answ;\n\n\tswitch (cmd) {\n\tcase SIOCINQ:\n\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\treturn -EINVAL;\n\n\t\tlock_sock(sk);\n\t\tif ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV))\n\t\t\tansw = 0;\n\t\telse if (sock_flag(sk, SOCK_URGINLINE) ||\n\t\t\t !tp->urg_data ||\n\t\t\t before(tp->urg_seq, tp->copied_seq) ||\n\t\t\t !before(tp->urg_seq, tp->rcv_nxt)) {\n\t\t\tstruct sk_buff *skb;\n\n\t\t\tansw = tp->rcv_nxt - tp->copied_seq;\n\n\t\t\t/* Subtract 1, if FIN is in queue. */\n\t\t\tskb = skb_peek_tail(&sk->sk_receive_queue);\n\t\t\tif (answ && skb)\n\t\t\t\tansw -= tcp_hdr(skb)->fin;\n\t\t} else\n\t\t\tansw = tp->urg_seq - tp->copied_seq;\n\t\trelease_sock(sk);\n\t\tbreak;\n\tcase SIOCATMARK:\n\t\tansw = tp->urg_data && tp->urg_seq == tp->copied_seq;\n\t\tbreak;\n\tcase SIOCOUTQ:\n\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\treturn -EINVAL;\n\n\t\tif ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV))\n\t\t\tansw = 0;\n\t\telse\n\t\t\tansw = tp->write_seq - tp->snd_una;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\n\treturn put_user(answ, (int __user *)arg);\n}\n\nstatic inline void tcp_mark_push(struct tcp_sock *tp, struct sk_buff *skb)\n{\n\tTCP_SKB_CB(skb)->flags |= TCPCB_FLAG_PSH;\n\ttp->pushed_seq = tp->write_seq;\n}\n\nstatic inline int forced_push(struct tcp_sock *tp)\n{\n\treturn after(tp->write_seq, tp->pushed_seq + (tp->max_window >> 1));\n}\n\nstatic inline void skb_entail(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\n\n\tskb->csum    = 0;\n\ttcb->seq     = tcb->end_seq = tp->write_seq;\n\ttcb->flags   = TCPCB_FLAG_ACK;\n\ttcb->sacked  = 0;\n\tskb_header_release(skb);\n\ttcp_add_write_queue_tail(sk, skb);\n\tsk->sk_wmem_queued += skb->truesize;\n\tsk_mem_charge(sk, skb->truesize);\n\tif (tp->nonagle & TCP_NAGLE_PUSH)\n\t\ttp->nonagle &= ~TCP_NAGLE_PUSH;\n}\n\nstatic inline void tcp_mark_urg(struct tcp_sock *tp, int flags)\n{\n\tif (flags & MSG_OOB)\n\t\ttp->snd_up = tp->write_seq;\n}\n\nstatic inline void tcp_push(struct sock *sk, int flags, int mss_now,\n\t\t\t    int nonagle)\n{\n\tif (tcp_send_head(sk)) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t\tif (!(flags & MSG_MORE) || forced_push(tp))\n\t\t\ttcp_mark_push(tp, tcp_write_queue_tail(sk));\n\n\t\ttcp_mark_urg(tp, flags);\n\t\t__tcp_push_pending_frames(sk, mss_now,\n\t\t\t\t\t  (flags & MSG_MORE) ? TCP_NAGLE_CORK : nonagle);\n\t}\n}\n\nstatic int tcp_splice_data_recv(read_descriptor_t *rd_desc, struct sk_buff *skb,\n\t\t\t\tunsigned int offset, size_t len)\n{\n\tstruct tcp_splice_state *tss = rd_desc->arg.data;\n\tint ret;\n\n\tret = skb_splice_bits(skb, offset, tss->pipe, min(rd_desc->count, len),\n\t\t\t      tss->flags);\n\tif (ret > 0)\n\t\trd_desc->count -= ret;\n\treturn ret;\n}\n\nstatic int __tcp_splice_read(struct sock *sk, struct tcp_splice_state *tss)\n{\n\t/* Store TCP splice context information in read_descriptor_t. */\n\tread_descriptor_t rd_desc = {\n\t\t.arg.data = tss,\n\t\t.count\t  = tss->len,\n\t};\n\n\treturn tcp_read_sock(sk, &rd_desc, tcp_splice_data_recv);\n}\n\n/**\n *  tcp_splice_read - splice data from TCP socket to a pipe\n * @sock:\tsocket to splice from\n * @ppos:\tposition (not valid)\n * @pipe:\tpipe to splice to\n * @len:\tnumber of bytes to splice\n * @flags:\tsplice modifier flags\n *\n * Description:\n *    Will read pages from given socket and fill them into a pipe.\n *\n **/\nssize_t tcp_splice_read(struct socket *sock, loff_t *ppos,\n\t\t\tstruct pipe_inode_info *pipe, size_t len,\n\t\t\tunsigned int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct tcp_splice_state tss = {\n\t\t.pipe = pipe,\n\t\t.len = len,\n\t\t.flags = flags,\n\t};\n\tlong timeo;\n\tssize_t spliced;\n\tint ret;\n\n\t/*\n\t * We can't seek on a socket input\n\t */\n\tif (unlikely(*ppos))\n\t\treturn -ESPIPE;\n\n\tret = spliced = 0;\n\n\tlock_sock(sk);\n\n\ttimeo = sock_rcvtimeo(sk, sock->file->f_flags & O_NONBLOCK);\n\twhile (tss.len) {\n\t\tret = __tcp_splice_read(sk, &tss);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\telse if (!ret) {\n\t\t\tif (spliced)\n\t\t\t\tbreak;\n\t\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\t\tbreak;\n\t\t\tif (sk->sk_err) {\n\t\t\t\tret = sock_error(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\t\t\tif (sk->sk_state == TCP_CLOSE) {\n\t\t\t\t/*\n\t\t\t\t * This occurs when user tries to read\n\t\t\t\t * from never connected socket.\n\t\t\t\t */\n\t\t\t\tif (!sock_flag(sk, SOCK_DONE))\n\t\t\t\t\tret = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!timeo) {\n\t\t\t\tret = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tsk_wait_data(sk, &timeo);\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tret = sock_intr_errno(timeo);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\ttss.len -= ret;\n\t\tspliced += ret;\n\n\t\tif (!timeo)\n\t\t\tbreak;\n\t\trelease_sock(sk);\n\t\tlock_sock(sk);\n\n\t\tif (sk->sk_err || sk->sk_state == TCP_CLOSE ||\n\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t    signal_pending(current))\n\t\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\n\tif (spliced)\n\t\treturn spliced;\n\n\treturn ret;\n}\n\nstruct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp)\n{\n\tstruct sk_buff *skb;\n\n\t/* The TCP header must be at least 32-bit aligned.  */\n\tsize = ALIGN(size, 4);\n\n\tskb = alloc_skb_fclone(size + sk->sk_prot->max_header, gfp);\n\tif (skb) {\n\t\tif (sk_wmem_schedule(sk, skb->truesize)) {\n\t\t\t/*\n\t\t\t * Make sure that we have exactly size bytes\n\t\t\t * available to the caller, no more, no less.\n\t\t\t */\n\t\t\tskb_reserve(skb, skb_tailroom(skb) - size);\n\t\t\treturn skb;\n\t\t}\n\t\t__kfree_skb(skb);\n\t} else {\n\t\tsk->sk_prot->enter_memory_pressure(sk);\n\t\tsk_stream_moderate_sndbuf(sk);\n\t}\n\treturn NULL;\n}\n\nstatic unsigned int tcp_xmit_size_goal(struct sock *sk, u32 mss_now,\n\t\t\t\t       int large_allowed)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 xmit_size_goal, old_size_goal;\n\n\txmit_size_goal = mss_now;\n\n\tif (large_allowed && sk_can_gso(sk)) {\n\t\txmit_size_goal = ((sk->sk_gso_max_size - 1) -\n\t\t\t\t  inet_csk(sk)->icsk_af_ops->net_header_len -\n\t\t\t\t  inet_csk(sk)->icsk_ext_hdr_len -\n\t\t\t\t  tp->tcp_header_len);\n\n\t\txmit_size_goal = tcp_bound_to_half_wnd(tp, xmit_size_goal);\n\n\t\t/* We try hard to avoid divides here */\n\t\told_size_goal = tp->xmit_size_goal_segs * mss_now;\n\n\t\tif (likely(old_size_goal <= xmit_size_goal &&\n\t\t\t   old_size_goal + mss_now > xmit_size_goal)) {\n\t\t\txmit_size_goal = old_size_goal;\n\t\t} else {\n\t\t\ttp->xmit_size_goal_segs = xmit_size_goal / mss_now;\n\t\t\txmit_size_goal = tp->xmit_size_goal_segs * mss_now;\n\t\t}\n\t}\n\n\treturn max(xmit_size_goal, mss_now);\n}\n\nstatic int tcp_send_mss(struct sock *sk, int *size_goal, int flags)\n{\n\tint mss_now;\n\n\tmss_now = tcp_current_mss(sk);\n\t*size_goal = tcp_xmit_size_goal(sk, mss_now, !(flags & MSG_OOB));\n\n\treturn mss_now;\n}\n\nstatic ssize_t do_tcp_sendpages(struct sock *sk, struct page **pages, int poffset,\n\t\t\t size_t psize, int flags)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint mss_now, size_goal;\n\tint err;\n\tssize_t copied;\n\tlong timeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);\n\n\t/* Wait for a connection to finish. */\n\tif ((1 << sk->sk_state) & ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT))\n\t\tif ((err = sk_stream_wait_connect(sk, &timeo)) != 0)\n\t\t\tgoto out_err;\n\n\tclear_bit(SOCK_ASYNC_NOSPACE, &sk->sk_socket->flags);\n\n\tmss_now = tcp_send_mss(sk, &size_goal, flags);\n\tcopied = 0;\n\n\terr = -EPIPE;\n\tif (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN))\n\t\tgoto out_err;\n\n\twhile (psize > 0) {\n\t\tstruct sk_buff *skb = tcp_write_queue_tail(sk);\n\t\tstruct page *page = pages[poffset / PAGE_SIZE];\n\t\tint copy, i, can_coalesce;\n\t\tint offset = poffset % PAGE_SIZE;\n\t\tint size = min_t(size_t, psize, PAGE_SIZE - offset);\n\n\t\tif (!tcp_send_head(sk) || (copy = size_goal - skb->len) <= 0) {\nnew_segment:\n\t\t\tif (!sk_stream_memory_free(sk))\n\t\t\t\tgoto wait_for_sndbuf;\n\n\t\t\tskb = sk_stream_alloc_skb(sk, 0, sk->sk_allocation);\n\t\t\tif (!skb)\n\t\t\t\tgoto wait_for_memory;\n\n\t\t\tskb_entail(sk, skb);\n\t\t\tcopy = size_goal;\n\t\t}\n\n\t\tif (copy > size)\n\t\t\tcopy = size;\n\n\t\ti = skb_shinfo(skb)->nr_frags;\n\t\tcan_coalesce = skb_can_coalesce(skb, i, page, offset);\n\t\tif (!can_coalesce && i >= MAX_SKB_FRAGS) {\n\t\t\ttcp_mark_push(tp, skb);\n\t\t\tgoto new_segment;\n\t\t}\n\t\tif (!sk_wmem_schedule(sk, copy))\n\t\t\tgoto wait_for_memory;\n\n\t\tif (can_coalesce) {\n\t\t\tskb_shinfo(skb)->frags[i - 1].size += copy;\n\t\t} else {\n\t\t\tget_page(page);\n\t\t\tskb_fill_page_desc(skb, i, page, offset, copy);\n\t\t}\n\n\t\tskb->len += copy;\n\t\tskb->data_len += copy;\n\t\tskb->truesize += copy;\n\t\tsk->sk_wmem_queued += copy;\n\t\tsk_mem_charge(sk, copy);\n\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\ttp->write_seq += copy;\n\t\tTCP_SKB_CB(skb)->end_seq += copy;\n\t\tskb_shinfo(skb)->gso_segs = 0;\n\n\t\tif (!copied)\n\t\t\tTCP_SKB_CB(skb)->flags &= ~TCPCB_FLAG_PSH;\n\n\t\tcopied += copy;\n\t\tpoffset += copy;\n\t\tif (!(psize -= copy))\n\t\t\tgoto out;\n\n\t\tif (skb->len < size_goal || (flags & MSG_OOB))\n\t\t\tcontinue;\n\n\t\tif (forced_push(tp)) {\n\t\t\ttcp_mark_push(tp, skb);\n\t\t\t__tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH);\n\t\t} else if (skb == tcp_send_head(sk))\n\t\t\ttcp_push_one(sk, mss_now);\n\t\tcontinue;\n\nwait_for_sndbuf:\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\nwait_for_memory:\n\t\tif (copied)\n\t\t\ttcp_push(sk, flags & ~MSG_MORE, mss_now, TCP_NAGLE_PUSH);\n\n\t\tif ((err = sk_stream_wait_memory(sk, &timeo)) != 0)\n\t\t\tgoto do_error;\n\n\t\tmss_now = tcp_send_mss(sk, &size_goal, flags);\n\t}\n\nout:\n\tif (copied)\n\t\ttcp_push(sk, flags, mss_now, tp->nonagle);\n\treturn copied;\n\ndo_error:\n\tif (copied)\n\t\tgoto out;\nout_err:\n\treturn sk_stream_error(sk, flags, err);\n}\n\nssize_t tcp_sendpage(struct socket *sock, struct page *page, int offset,\n\t\t     size_t size, int flags)\n{\n\tssize_t res;\n\tstruct sock *sk = sock->sk;\n\n\tif (!(sk->sk_route_caps & NETIF_F_SG) ||\n\t    !(sk->sk_route_caps & NETIF_F_ALL_CSUM))\n\t\treturn sock_no_sendpage(sock, page, offset, size, flags);\n\n\tlock_sock(sk);\n\tTCP_CHECK_TIMER(sk);\n\tres = do_tcp_sendpages(sk, &page, offset, size, flags);\n\tTCP_CHECK_TIMER(sk);\n\trelease_sock(sk);\n\treturn res;\n}\n\n#define TCP_PAGE(sk)\t(sk->sk_sndmsg_page)\n#define TCP_OFF(sk)\t(sk->sk_sndmsg_off)\n\nstatic inline int select_size(struct sock *sk, int sg)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint tmp = tp->mss_cache;\n\n\tif (sg) {\n\t\tif (sk_can_gso(sk))\n\t\t\ttmp = 0;\n\t\telse {\n\t\t\tint pgbreak = SKB_MAX_HEAD(MAX_TCP_HEADER);\n\n\t\t\tif (tmp >= pgbreak &&\n\t\t\t    tmp <= pgbreak + (MAX_SKB_FRAGS - 1) * PAGE_SIZE)\n\t\t\t\ttmp = pgbreak;\n\t\t}\n\t}\n\n\treturn tmp;\n}\n\nint tcp_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,\n\t\tsize_t size)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct iovec *iov;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tint iovlen, flags;\n\tint mss_now, size_goal;\n\tint sg, err, copied;\n\tlong timeo;\n\n\tlock_sock(sk);\n\tTCP_CHECK_TIMER(sk);\n\n\tflags = msg->msg_flags;\n\ttimeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);\n\n\t/* Wait for a connection to finish. */\n\tif ((1 << sk->sk_state) & ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT))\n\t\tif ((err = sk_stream_wait_connect(sk, &timeo)) != 0)\n\t\t\tgoto out_err;\n\n\t/* This should be in poll */\n\tclear_bit(SOCK_ASYNC_NOSPACE, &sk->sk_socket->flags);\n\n\tmss_now = tcp_send_mss(sk, &size_goal, flags);\n\n\t/* Ok commence sending. */\n\tiovlen = msg->msg_iovlen;\n\tiov = msg->msg_iov;\n\tcopied = 0;\n\n\terr = -EPIPE;\n\tif (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN))\n\t\tgoto out_err;\n\n\tsg = sk->sk_route_caps & NETIF_F_SG;\n\n\twhile (--iovlen >= 0) {\n\t\tint seglen = iov->iov_len;\n\t\tunsigned char __user *from = iov->iov_base;\n\n\t\tiov++;\n\n\t\twhile (seglen > 0) {\n\t\t\tint copy = 0;\n\t\t\tint max = size_goal;\n\n\t\t\tskb = tcp_write_queue_tail(sk);\n\t\t\tif (tcp_send_head(sk)) {\n\t\t\t\tif (skb->ip_summed == CHECKSUM_NONE)\n\t\t\t\t\tmax = mss_now;\n\t\t\t\tcopy = max - skb->len;\n\t\t\t}\n\n\t\t\tif (copy <= 0) {\nnew_segment:\n\t\t\t\t/* Allocate new segment. If the interface is SG,\n\t\t\t\t * allocate skb fitting to single page.\n\t\t\t\t */\n\t\t\t\tif (!sk_stream_memory_free(sk))\n\t\t\t\t\tgoto wait_for_sndbuf;\n\n\t\t\t\tskb = sk_stream_alloc_skb(sk,\n\t\t\t\t\t\t\t  select_size(sk, sg),\n\t\t\t\t\t\t\t  sk->sk_allocation);\n\t\t\t\tif (!skb)\n\t\t\t\t\tgoto wait_for_memory;\n\n\t\t\t\t/*\n\t\t\t\t * Check whether we can use HW checksum.\n\t\t\t\t */\n\t\t\t\tif (sk->sk_route_caps & NETIF_F_ALL_CSUM)\n\t\t\t\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\t\t\t\tskb_entail(sk, skb);\n\t\t\t\tcopy = size_goal;\n\t\t\t\tmax = size_goal;\n\t\t\t}\n\n\t\t\t/* Try to append data to the end of skb. */\n\t\t\tif (copy > seglen)\n\t\t\t\tcopy = seglen;\n\n\t\t\t/* Where to copy to? */\n\t\t\tif (skb_tailroom(skb) > 0) {\n\t\t\t\t/* We have some space in skb head. Superb! */\n\t\t\t\tif (copy > skb_tailroom(skb))\n\t\t\t\t\tcopy = skb_tailroom(skb);\n\t\t\t\tif ((err = skb_add_data(skb, from, copy)) != 0)\n\t\t\t\t\tgoto do_fault;\n\t\t\t} else {\n\t\t\t\tint merge = 0;\n\t\t\t\tint i = skb_shinfo(skb)->nr_frags;\n\t\t\t\tstruct page *page = TCP_PAGE(sk);\n\t\t\t\tint off = TCP_OFF(sk);\n\n\t\t\t\tif (skb_can_coalesce(skb, i, page, off) &&\n\t\t\t\t    off != PAGE_SIZE) {\n\t\t\t\t\t/* We can extend the last page\n\t\t\t\t\t * fragment. */\n\t\t\t\t\tmerge = 1;\n\t\t\t\t} else if (i == MAX_SKB_FRAGS || !sg) {\n\t\t\t\t\t/* Need to add new fragment and cannot\n\t\t\t\t\t * do this because interface is non-SG,\n\t\t\t\t\t * or because all the page slots are\n\t\t\t\t\t * busy. */\n\t\t\t\t\ttcp_mark_push(tp, skb);\n\t\t\t\t\tgoto new_segment;\n\t\t\t\t} else if (page) {\n\t\t\t\t\tif (off == PAGE_SIZE) {\n\t\t\t\t\t\tput_page(page);\n\t\t\t\t\t\tTCP_PAGE(sk) = page = NULL;\n\t\t\t\t\t\toff = 0;\n\t\t\t\t\t}\n\t\t\t\t} else\n\t\t\t\t\toff = 0;\n\n\t\t\t\tif (copy > PAGE_SIZE - off)\n\t\t\t\t\tcopy = PAGE_SIZE - off;\n\n\t\t\t\tif (!sk_wmem_schedule(sk, copy))\n\t\t\t\t\tgoto wait_for_memory;\n\n\t\t\t\tif (!page) {\n\t\t\t\t\t/* Allocate new cache page. */\n\t\t\t\t\tif (!(page = sk_stream_alloc_page(sk)))\n\t\t\t\t\t\tgoto wait_for_memory;\n\t\t\t\t}\n\n\t\t\t\t/* Time to copy data. We are close to\n\t\t\t\t * the end! */\n\t\t\t\terr = skb_copy_to_page(sk, from, skb, page,\n\t\t\t\t\t\t       off, copy);\n\t\t\t\tif (err) {\n\t\t\t\t\t/* If this page was new, give it to the\n\t\t\t\t\t * socket so it does not get leaked.\n\t\t\t\t\t */\n\t\t\t\t\tif (!TCP_PAGE(sk)) {\n\t\t\t\t\t\tTCP_PAGE(sk) = page;\n\t\t\t\t\t\tTCP_OFF(sk) = 0;\n\t\t\t\t\t}\n\t\t\t\t\tgoto do_error;\n\t\t\t\t}\n\n\t\t\t\t/* Update the skb. */\n\t\t\t\tif (merge) {\n\t\t\t\t\tskb_shinfo(skb)->frags[i - 1].size +=\n\t\t\t\t\t\t\t\t\tcopy;\n\t\t\t\t} else {\n\t\t\t\t\tskb_fill_page_desc(skb, i, page, off, copy);\n\t\t\t\t\tif (TCP_PAGE(sk)) {\n\t\t\t\t\t\tget_page(page);\n\t\t\t\t\t} else if (off + copy < PAGE_SIZE) {\n\t\t\t\t\t\tget_page(page);\n\t\t\t\t\t\tTCP_PAGE(sk) = page;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tTCP_OFF(sk) = off + copy;\n\t\t\t}\n\n\t\t\tif (!copied)\n\t\t\t\tTCP_SKB_CB(skb)->flags &= ~TCPCB_FLAG_PSH;\n\n\t\t\ttp->write_seq += copy;\n\t\t\tTCP_SKB_CB(skb)->end_seq += copy;\n\t\t\tskb_shinfo(skb)->gso_segs = 0;\n\n\t\t\tfrom += copy;\n\t\t\tcopied += copy;\n\t\t\tif ((seglen -= copy) == 0 && iovlen == 0)\n\t\t\t\tgoto out;\n\n\t\t\tif (skb->len < max || (flags & MSG_OOB))\n\t\t\t\tcontinue;\n\n\t\t\tif (forced_push(tp)) {\n\t\t\t\ttcp_mark_push(tp, skb);\n\t\t\t\t__tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH);\n\t\t\t} else if (skb == tcp_send_head(sk))\n\t\t\t\ttcp_push_one(sk, mss_now);\n\t\t\tcontinue;\n\nwait_for_sndbuf:\n\t\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\nwait_for_memory:\n\t\t\tif (copied)\n\t\t\t\ttcp_push(sk, flags & ~MSG_MORE, mss_now, TCP_NAGLE_PUSH);\n\n\t\t\tif ((err = sk_stream_wait_memory(sk, &timeo)) != 0)\n\t\t\t\tgoto do_error;\n\n\t\t\tmss_now = tcp_send_mss(sk, &size_goal, flags);\n\t\t}\n\t}\n\nout:\n\tif (copied)\n\t\ttcp_push(sk, flags, mss_now, tp->nonagle);\n\tTCP_CHECK_TIMER(sk);\n\trelease_sock(sk);\n\treturn copied;\n\ndo_fault:\n\tif (!skb->len) {\n\t\ttcp_unlink_write_queue(skb, sk);\n\t\t/* It is the one place in all of TCP, except connection\n\t\t * reset, where we can be unlinking the send_head.\n\t\t */\n\t\ttcp_check_send_head(sk, skb);\n\t\tsk_wmem_free_skb(sk, skb);\n\t}\n\ndo_error:\n\tif (copied)\n\t\tgoto out;\nout_err:\n\terr = sk_stream_error(sk, flags, err);\n\tTCP_CHECK_TIMER(sk);\n\trelease_sock(sk);\n\treturn err;\n}\n\n/*\n *\tHandle reading urgent data. BSD has very simple semantics for\n *\tthis, no blocking and very strange errors 8)\n */\n\nstatic int tcp_recv_urg(struct sock *sk, struct msghdr *msg, int len, int flags)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* No URG data to read. */\n\tif (sock_flag(sk, SOCK_URGINLINE) || !tp->urg_data ||\n\t    tp->urg_data == TCP_URG_READ)\n\t\treturn -EINVAL;\t/* Yes this is right ! */\n\n\tif (sk->sk_state == TCP_CLOSE && !sock_flag(sk, SOCK_DONE))\n\t\treturn -ENOTCONN;\n\n\tif (tp->urg_data & TCP_URG_VALID) {\n\t\tint err = 0;\n\t\tchar c = tp->urg_data;\n\n\t\tif (!(flags & MSG_PEEK))\n\t\t\ttp->urg_data = TCP_URG_READ;\n\n\t\t/* Read urgent data. */\n\t\tmsg->msg_flags |= MSG_OOB;\n\n\t\tif (len > 0) {\n\t\t\tif (!(flags & MSG_TRUNC))\n\t\t\t\terr = memcpy_toiovec(msg->msg_iov, &c, 1);\n\t\t\tlen = 1;\n\t\t} else\n\t\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\t\treturn err ? -EFAULT : len;\n\t}\n\n\tif (sk->sk_state == TCP_CLOSE || (sk->sk_shutdown & RCV_SHUTDOWN))\n\t\treturn 0;\n\n\t/* Fixed the recv(..., MSG_OOB) behaviour.  BSD docs and\n\t * the available implementations agree in this case:\n\t * this call should never block, independent of the\n\t * blocking state of the socket.\n\t * Mike <pall@rz.uni-karlsruhe.de>\n\t */\n\treturn -EAGAIN;\n}\n\n/* Clean up the receive buffer for full frames taken by the user,\n * then send an ACK if necessary.  COPIED is the number of bytes\n * tcp_recvmsg has given to the user so far, it speeds up the\n * calculation of whether or not we must ACK for the sake of\n * a window update.\n */\nvoid tcp_cleanup_rbuf(struct sock *sk, int copied)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint time_to_ack = 0;\n\n#if TCP_DEBUG\n\tstruct sk_buff *skb = skb_peek(&sk->sk_receive_queue);\n\n\tWARN(skb && !before(tp->copied_seq, TCP_SKB_CB(skb)->end_seq),\n\t     KERN_INFO \"cleanup rbuf bug: copied %X seq %X rcvnxt %X\\n\",\n\t     tp->copied_seq, TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt);\n#endif\n\n\tif (inet_csk_ack_scheduled(sk)) {\n\t\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\t\t   /* Delayed ACKs frequently hit locked sockets during bulk\n\t\t    * receive. */\n\t\tif (icsk->icsk_ack.blocked ||\n\t\t    /* Once-per-two-segments ACK was not sent by tcp_input.c */\n\t\t    tp->rcv_nxt - tp->rcv_wup > icsk->icsk_ack.rcv_mss ||\n\t\t    /*\n\t\t     * If this read emptied read buffer, we send ACK, if\n\t\t     * connection is not bidirectional, user drained\n\t\t     * receive buffer and there was a small segment\n\t\t     * in queue.\n\t\t     */\n\t\t    (copied > 0 &&\n\t\t     ((icsk->icsk_ack.pending & ICSK_ACK_PUSHED2) ||\n\t\t      ((icsk->icsk_ack.pending & ICSK_ACK_PUSHED) &&\n\t\t       !icsk->icsk_ack.pingpong)) &&\n\t\t      !atomic_read(&sk->sk_rmem_alloc)))\n\t\t\ttime_to_ack = 1;\n\t}\n\n\t/* We send an ACK if we can now advertise a non-zero window\n\t * which has been raised \"significantly\".\n\t *\n\t * Even if window raised up to infinity, do not send window open ACK\n\t * in states, where we will not receive more. It is useless.\n\t */\n\tif (copied > 0 && !time_to_ack && !(sk->sk_shutdown & RCV_SHUTDOWN)) {\n\t\t__u32 rcv_window_now = tcp_receive_window(tp);\n\n\t\t/* Optimize, __tcp_select_window() is not cheap. */\n\t\tif (2*rcv_window_now <= tp->window_clamp) {\n\t\t\t__u32 new_window = __tcp_select_window(sk);\n\n\t\t\t/* Send ACK now, if this read freed lots of space\n\t\t\t * in our buffer. Certainly, new_window is new window.\n\t\t\t * We can advertise it now, if it is not less than current one.\n\t\t\t * \"Lots\" means \"at least twice\" here.\n\t\t\t */\n\t\t\tif (new_window && new_window >= 2 * rcv_window_now)\n\t\t\t\ttime_to_ack = 1;\n\t\t}\n\t}\n\tif (time_to_ack)\n\t\ttcp_send_ack(sk);\n}\n\nstatic void tcp_prequeue_process(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tNET_INC_STATS_USER(sock_net(sk), LINUX_MIB_TCPPREQUEUED);\n\n\t/* RX process wants to run with disabled BHs, though it is not\n\t * necessary */\n\tlocal_bh_disable();\n\twhile ((skb = __skb_dequeue(&tp->ucopy.prequeue)) != NULL)\n\t\tsk_backlog_rcv(sk, skb);\n\tlocal_bh_enable();\n\n\t/* Clear memory counter. */\n\ttp->ucopy.memory = 0;\n}\n\n#ifdef CONFIG_NET_DMA\nstatic void tcp_service_net_dma(struct sock *sk, bool wait)\n{\n\tdma_cookie_t done, used;\n\tdma_cookie_t last_issued;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!tp->ucopy.dma_chan)\n\t\treturn;\n\n\tlast_issued = tp->ucopy.dma_cookie;\n\tdma_async_memcpy_issue_pending(tp->ucopy.dma_chan);\n\n\tdo {\n\t\tif (dma_async_memcpy_complete(tp->ucopy.dma_chan,\n\t\t\t\t\t      last_issued, &done,\n\t\t\t\t\t      &used) == DMA_SUCCESS) {\n\t\t\t/* Safe to free early-copied skbs now */\n\t\t\t__skb_queue_purge(&sk->sk_async_wait_queue);\n\t\t\tbreak;\n\t\t} else {\n\t\t\tstruct sk_buff *skb;\n\t\t\twhile ((skb = skb_peek(&sk->sk_async_wait_queue)) &&\n\t\t\t       (dma_async_is_complete(skb->dma_cookie, done,\n\t\t\t\t\t\t      used) == DMA_SUCCESS)) {\n\t\t\t\t__skb_dequeue(&sk->sk_async_wait_queue);\n\t\t\t\tkfree_skb(skb);\n\t\t\t}\n\t\t}\n\t} while (wait);\n}\n#endif\n\nstatic inline struct sk_buff *tcp_recv_skb(struct sock *sk, u32 seq, u32 *off)\n{\n\tstruct sk_buff *skb;\n\tu32 offset;\n\n\tskb_queue_walk(&sk->sk_receive_queue, skb) {\n\t\toffset = seq - TCP_SKB_CB(skb)->seq;\n\t\tif (tcp_hdr(skb)->syn)\n\t\t\toffset--;\n\t\tif (offset < skb->len || tcp_hdr(skb)->fin) {\n\t\t\t*off = offset;\n\t\t\treturn skb;\n\t\t}\n\t}\n\treturn NULL;\n}\n\n/*\n * This routine provides an alternative to tcp_recvmsg() for routines\n * that would like to handle copying from skbuffs directly in 'sendfile'\n * fashion.\n * Note:\n *\t- It is assumed that the socket was locked by the caller.\n *\t- The routine does not block.\n *\t- At present, there is no support for reading OOB data\n *\t  or for 'peeking' the socket using this routine\n *\t  (although both would be easy to implement).\n */\nint tcp_read_sock(struct sock *sk, read_descriptor_t *desc,\n\t\t  sk_read_actor_t recv_actor)\n{\n\tstruct sk_buff *skb;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 seq = tp->copied_seq;\n\tu32 offset;\n\tint copied = 0;\n\n\tif (sk->sk_state == TCP_LISTEN)\n\t\treturn -ENOTCONN;\n\twhile ((skb = tcp_recv_skb(sk, seq, &offset)) != NULL) {\n\t\tif (offset < skb->len) {\n\t\t\tint used;\n\t\t\tsize_t len;\n\n\t\t\tlen = skb->len - offset;\n\t\t\t/* Stop reading if we hit a patch of urgent data */\n\t\t\tif (tp->urg_data) {\n\t\t\t\tu32 urg_offset = tp->urg_seq - seq;\n\t\t\t\tif (urg_offset < len)\n\t\t\t\t\tlen = urg_offset;\n\t\t\t\tif (!len)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tused = recv_actor(desc, skb, offset, len);\n\t\t\tif (used < 0) {\n\t\t\t\tif (!copied)\n\t\t\t\t\tcopied = used;\n\t\t\t\tbreak;\n\t\t\t} else if (used <= len) {\n\t\t\t\tseq += used;\n\t\t\t\tcopied += used;\n\t\t\t\toffset += used;\n\t\t\t}\n\t\t\t/*\n\t\t\t * If recv_actor drops the lock (e.g. TCP splice\n\t\t\t * receive) the skb pointer might be invalid when\n\t\t\t * getting here: tcp_collapse might have deleted it\n\t\t\t * while aggregating skbs from the socket queue.\n\t\t\t */\n\t\t\tskb = tcp_recv_skb(sk, seq-1, &offset);\n\t\t\tif (!skb || (offset+1 != skb->len))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (tcp_hdr(skb)->fin) {\n\t\t\tsk_eat_skb(sk, skb, 0);\n\t\t\t++seq;\n\t\t\tbreak;\n\t\t}\n\t\tsk_eat_skb(sk, skb, 0);\n\t\tif (!desc->count)\n\t\t\tbreak;\n\t}\n\ttp->copied_seq = seq;\n\n\ttcp_rcv_space_adjust(sk);\n\n\t/* Clean up data we have read: This will do ACK frames. */\n\tif (copied > 0)\n\t\ttcp_cleanup_rbuf(sk, copied);\n\treturn copied;\n}\n\n/*\n *\tThis routine copies from a sock struct into the user buffer.\n *\n *\tTechnical note: in 2.3 we work on _locked_ socket, so that\n *\ttricks with *seq access order and skb->users are not required.\n *\tProbably, code can be easily improved even more.\n */\n\nint tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\tsize_t len, int nonblock, int flags, int *addr_len)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint copied = 0;\n\tu32 peek_seq;\n\tu32 *seq;\n\tunsigned long used;\n\tint err;\n\tint target;\t\t/* Read at least this many bytes */\n\tlong timeo;\n\tstruct task_struct *user_recv = NULL;\n\tint copied_early = 0;\n\tstruct sk_buff *skb;\n\tu32 urg_hole = 0;\n\n\tlock_sock(sk);\n\n\tTCP_CHECK_TIMER(sk);\n\n\terr = -ENOTCONN;\n\tif (sk->sk_state == TCP_LISTEN)\n\t\tgoto out;\n\n\ttimeo = sock_rcvtimeo(sk, nonblock);\n\n\t/* Urgent data needs to be handled specially. */\n\tif (flags & MSG_OOB)\n\t\tgoto recv_urg;\n\n\tseq = &tp->copied_seq;\n\tif (flags & MSG_PEEK) {\n\t\tpeek_seq = tp->copied_seq;\n\t\tseq = &peek_seq;\n\t}\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\n#ifdef CONFIG_NET_DMA\n\ttp->ucopy.dma_chan = NULL;\n\tpreempt_disable();\n\tskb = skb_peek_tail(&sk->sk_receive_queue);\n\t{\n\t\tint available = 0;\n\n\t\tif (skb)\n\t\t\tavailable = TCP_SKB_CB(skb)->seq + skb->len - (*seq);\n\t\tif ((available < target) &&\n\t\t    (len > sysctl_tcp_dma_copybreak) && !(flags & MSG_PEEK) &&\n\t\t    !sysctl_tcp_low_latency &&\n\t\t    dma_find_channel(DMA_MEMCPY)) {\n\t\t\tpreempt_enable_no_resched();\n\t\t\ttp->ucopy.pinned_list =\n\t\t\t\t\tdma_pin_iovec_pages(msg->msg_iov, len);\n\t\t} else {\n\t\t\tpreempt_enable_no_resched();\n\t\t}\n\t}\n#endif\n\n\tdo {\n\t\tu32 offset;\n\n\t\t/* Are we at urgent data? Stop if we have read anything or have SIGURG pending. */\n\t\tif (tp->urg_data && tp->urg_seq == *seq) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tcopied = timeo ? sock_intr_errno(timeo) : -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t/* Next get a buffer. */\n\n\t\tskb_queue_walk(&sk->sk_receive_queue, skb) {\n\t\t\t/* Now that we have two receive queues this\n\t\t\t * shouldn't happen.\n\t\t\t */\n\t\t\tif (WARN(before(*seq, TCP_SKB_CB(skb)->seq),\n\t\t\t     KERN_INFO \"recvmsg bug: copied %X \"\n\t\t\t\t       \"seq %X rcvnxt %X fl %X\\n\", *seq,\n\t\t\t\t       TCP_SKB_CB(skb)->seq, tp->rcv_nxt,\n\t\t\t\t       flags))\n\t\t\t\tbreak;\n\n\t\t\toffset = *seq - TCP_SKB_CB(skb)->seq;\n\t\t\tif (tcp_hdr(skb)->syn)\n\t\t\t\toffset--;\n\t\t\tif (offset < skb->len)\n\t\t\t\tgoto found_ok_skb;\n\t\t\tif (tcp_hdr(skb)->fin)\n\t\t\t\tgoto found_fin_ok;\n\t\t\tWARN(!(flags & MSG_PEEK), KERN_INFO \"recvmsg bug 2: \"\n\t\t\t\t\t\"copied %X seq %X rcvnxt %X fl %X\\n\",\n\t\t\t\t\t*seq, TCP_SKB_CB(skb)->seq,\n\t\t\t\t\ttp->rcv_nxt, flags);\n\t\t}\n\n\t\t/* Well, if we have backlog, try to process it now yet. */\n\n\t\tif (copied >= target && !sk->sk_backlog.tail)\n\t\t\tbreak;\n\n\t\tif (copied) {\n\t\t\tif (sk->sk_err ||\n\t\t\t    sk->sk_state == TCP_CLOSE ||\n\t\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t\t    !timeo ||\n\t\t\t    signal_pending(current))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_err) {\n\t\t\t\tcopied = sock_error(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state == TCP_CLOSE) {\n\t\t\t\tif (!sock_flag(sk, SOCK_DONE)) {\n\t\t\t\t\t/* This occurs when user tries to read\n\t\t\t\t\t * from never connected socket.\n\t\t\t\t\t */\n\t\t\t\t\tcopied = -ENOTCONN;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (!timeo) {\n\t\t\t\tcopied = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tcopied = sock_intr_errno(timeo);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\ttcp_cleanup_rbuf(sk, copied);\n\n\t\tif (!sysctl_tcp_low_latency && tp->ucopy.task == user_recv) {\n\t\t\t/* Install new reader */\n\t\t\tif (!user_recv && !(flags & (MSG_TRUNC | MSG_PEEK))) {\n\t\t\t\tuser_recv = current;\n\t\t\t\ttp->ucopy.task = user_recv;\n\t\t\t\ttp->ucopy.iov = msg->msg_iov;\n\t\t\t}\n\n\t\t\ttp->ucopy.len = len;\n\n\t\t\tWARN_ON(tp->copied_seq != tp->rcv_nxt &&\n\t\t\t\t!(flags & (MSG_PEEK | MSG_TRUNC)));\n\n\t\t\t/* Ugly... If prequeue is not empty, we have to\n\t\t\t * process it before releasing socket, otherwise\n\t\t\t * order will be broken at second iteration.\n\t\t\t * More elegant solution is required!!!\n\t\t\t *\n\t\t\t * Look: we have the following (pseudo)queues:\n\t\t\t *\n\t\t\t * 1. packets in flight\n\t\t\t * 2. backlog\n\t\t\t * 3. prequeue\n\t\t\t * 4. receive_queue\n\t\t\t *\n\t\t\t * Each queue can be processed only if the next ones\n\t\t\t * are empty. At this point we have empty receive_queue.\n\t\t\t * But prequeue _can_ be not empty after 2nd iteration,\n\t\t\t * when we jumped to start of loop because backlog\n\t\t\t * processing added something to receive_queue.\n\t\t\t * We cannot release_sock(), because backlog contains\n\t\t\t * packets arrived _after_ prequeued ones.\n\t\t\t *\n\t\t\t * Shortly, algorithm is clear --- to process all\n\t\t\t * the queues in order. We could make it more directly,\n\t\t\t * requeueing packets from backlog to prequeue, if\n\t\t\t * is not empty. It is more elegant, but eats cycles,\n\t\t\t * unfortunately.\n\t\t\t */\n\t\t\tif (!skb_queue_empty(&tp->ucopy.prequeue))\n\t\t\t\tgoto do_prequeue;\n\n\t\t\t/* __ Set realtime policy in scheduler __ */\n\t\t}\n\n#ifdef CONFIG_NET_DMA\n\t\tif (tp->ucopy.dma_chan)\n\t\t\tdma_async_memcpy_issue_pending(tp->ucopy.dma_chan);\n#endif\n\t\tif (copied >= target) {\n\t\t\t/* Do not sleep, just process backlog. */\n\t\t\trelease_sock(sk);\n\t\t\tlock_sock(sk);\n\t\t} else\n\t\t\tsk_wait_data(sk, &timeo);\n\n#ifdef CONFIG_NET_DMA\n\t\ttcp_service_net_dma(sk, false);  /* Don't block */\n\t\ttp->ucopy.wakeup = 0;\n#endif\n\n\t\tif (user_recv) {\n\t\t\tint chunk;\n\n\t\t\t/* __ Restore normal policy in scheduler __ */\n\n\t\t\tif ((chunk = len - tp->ucopy.len) != 0) {\n\t\t\t\tNET_ADD_STATS_USER(sock_net(sk), LINUX_MIB_TCPDIRECTCOPYFROMBACKLOG, chunk);\n\t\t\t\tlen -= chunk;\n\t\t\t\tcopied += chunk;\n\t\t\t}\n\n\t\t\tif (tp->rcv_nxt == tp->copied_seq &&\n\t\t\t    !skb_queue_empty(&tp->ucopy.prequeue)) {\ndo_prequeue:\n\t\t\t\ttcp_prequeue_process(sk);\n\n\t\t\t\tif ((chunk = len - tp->ucopy.len) != 0) {\n\t\t\t\t\tNET_ADD_STATS_USER(sock_net(sk), LINUX_MIB_TCPDIRECTCOPYFROMPREQUEUE, chunk);\n\t\t\t\t\tlen -= chunk;\n\t\t\t\t\tcopied += chunk;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif ((flags & MSG_PEEK) &&\n\t\t    (peek_seq - copied - urg_hole != tp->copied_seq)) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tprintk(KERN_DEBUG \"TCP(%s:%d): Application bug, race in MSG_PEEK.\\n\",\n\t\t\t\t       current->comm, task_pid_nr(current));\n\t\t\tpeek_seq = tp->copied_seq;\n\t\t}\n\t\tcontinue;\n\n\tfound_ok_skb:\n\t\t/* Ok so how much can we use? */\n\t\tused = skb->len - offset;\n\t\tif (len < used)\n\t\t\tused = len;\n\n\t\t/* Do we have urgent data here? */\n\t\tif (tp->urg_data) {\n\t\t\tu32 urg_offset = tp->urg_seq - *seq;\n\t\t\tif (urg_offset < used) {\n\t\t\t\tif (!urg_offset) {\n\t\t\t\t\tif (!sock_flag(sk, SOCK_URGINLINE)) {\n\t\t\t\t\t\t++*seq;\n\t\t\t\t\t\turg_hole++;\n\t\t\t\t\t\toffset++;\n\t\t\t\t\t\tused--;\n\t\t\t\t\t\tif (!used)\n\t\t\t\t\t\t\tgoto skip_copy;\n\t\t\t\t\t}\n\t\t\t\t} else\n\t\t\t\t\tused = urg_offset;\n\t\t\t}\n\t\t}\n\n\t\tif (!(flags & MSG_TRUNC)) {\n#ifdef CONFIG_NET_DMA\n\t\t\tif (!tp->ucopy.dma_chan && tp->ucopy.pinned_list)\n\t\t\t\ttp->ucopy.dma_chan = dma_find_channel(DMA_MEMCPY);\n\n\t\t\tif (tp->ucopy.dma_chan) {\n\t\t\t\ttp->ucopy.dma_cookie = dma_skb_copy_datagram_iovec(\n\t\t\t\t\ttp->ucopy.dma_chan, skb, offset,\n\t\t\t\t\tmsg->msg_iov, used,\n\t\t\t\t\ttp->ucopy.pinned_list);\n\n\t\t\t\tif (tp->ucopy.dma_cookie < 0) {\n\n\t\t\t\t\tprintk(KERN_ALERT \"dma_cookie < 0\\n\");\n\n\t\t\t\t\t/* Exception. Bailout! */\n\t\t\t\t\tif (!copied)\n\t\t\t\t\t\tcopied = -EFAULT;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tdma_async_memcpy_issue_pending(tp->ucopy.dma_chan);\n\n\t\t\t\tif ((offset + used) == skb->len)\n\t\t\t\t\tcopied_early = 1;\n\n\t\t\t} else\n#endif\n\t\t\t{\n\t\t\t\terr = skb_copy_datagram_iovec(skb, offset,\n\t\t\t\t\t\tmsg->msg_iov, used);\n\t\t\t\tif (err) {\n\t\t\t\t\t/* Exception. Bailout! */\n\t\t\t\t\tif (!copied)\n\t\t\t\t\t\tcopied = -EFAULT;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t*seq += used;\n\t\tcopied += used;\n\t\tlen -= used;\n\n\t\ttcp_rcv_space_adjust(sk);\n\nskip_copy:\n\t\tif (tp->urg_data && after(tp->copied_seq, tp->urg_seq)) {\n\t\t\ttp->urg_data = 0;\n\t\t\ttcp_fast_path_check(sk);\n\t\t}\n\t\tif (used + offset < skb->len)\n\t\t\tcontinue;\n\n\t\tif (tcp_hdr(skb)->fin)\n\t\t\tgoto found_fin_ok;\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tsk_eat_skb(sk, skb, copied_early);\n\t\t\tcopied_early = 0;\n\t\t}\n\t\tcontinue;\n\n\tfound_fin_ok:\n\t\t/* Process the FIN. */\n\t\t++*seq;\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tsk_eat_skb(sk, skb, copied_early);\n\t\t\tcopied_early = 0;\n\t\t}\n\t\tbreak;\n\t} while (len > 0);\n\n\tif (user_recv) {\n\t\tif (!skb_queue_empty(&tp->ucopy.prequeue)) {\n\t\t\tint chunk;\n\n\t\t\ttp->ucopy.len = copied > 0 ? len : 0;\n\n\t\t\ttcp_prequeue_process(sk);\n\n\t\t\tif (copied > 0 && (chunk = len - tp->ucopy.len) != 0) {\n\t\t\t\tNET_ADD_STATS_USER(sock_net(sk), LINUX_MIB_TCPDIRECTCOPYFROMPREQUEUE, chunk);\n\t\t\t\tlen -= chunk;\n\t\t\t\tcopied += chunk;\n\t\t\t}\n\t\t}\n\n\t\ttp->ucopy.task = NULL;\n\t\ttp->ucopy.len = 0;\n\t}\n\n#ifdef CONFIG_NET_DMA\n\ttcp_service_net_dma(sk, true);  /* Wait for queue to drain */\n\ttp->ucopy.dma_chan = NULL;\n\n\tif (tp->ucopy.pinned_list) {\n\t\tdma_unpin_iovec_pages(tp->ucopy.pinned_list);\n\t\ttp->ucopy.pinned_list = NULL;\n\t}\n#endif\n\n\t/* According to UNIX98, msg_name/msg_namelen are ignored\n\t * on connected socket. I was just happy when found this 8) --ANK\n\t */\n\n\t/* Clean up data we have read: This will do ACK frames. */\n\ttcp_cleanup_rbuf(sk, copied);\n\n\tTCP_CHECK_TIMER(sk);\n\trelease_sock(sk);\n\treturn copied;\n\nout:\n\tTCP_CHECK_TIMER(sk);\n\trelease_sock(sk);\n\treturn err;\n\nrecv_urg:\n\terr = tcp_recv_urg(sk, msg, len, flags);\n\tgoto out;\n}\n\nvoid tcp_set_state(struct sock *sk, int state)\n{\n\tint oldstate = sk->sk_state;\n\n\tswitch (state) {\n\tcase TCP_ESTABLISHED:\n\t\tif (oldstate != TCP_ESTABLISHED)\n\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_CURRESTAB);\n\t\tbreak;\n\n\tcase TCP_CLOSE:\n\t\tif (oldstate == TCP_CLOSE_WAIT || oldstate == TCP_ESTABLISHED)\n\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_ESTABRESETS);\n\n\t\tsk->sk_prot->unhash(sk);\n\t\tif (inet_csk(sk)->icsk_bind_hash &&\n\t\t    !(sk->sk_userlocks & SOCK_BINDPORT_LOCK))\n\t\t\tinet_put_port(sk);\n\t\t/* fall through */\n\tdefault:\n\t\tif (oldstate == TCP_ESTABLISHED)\n\t\t\tTCP_DEC_STATS(sock_net(sk), TCP_MIB_CURRESTAB);\n\t}\n\n\t/* Change state AFTER socket is unhashed to avoid closed\n\t * socket sitting in hash tables.\n\t */\n\tsk->sk_state = state;\n\n#ifdef STATE_TRACE\n\tSOCK_DEBUG(sk, \"TCP sk=%p, State %s -> %s\\n\", sk, statename[oldstate], statename[state]);\n#endif\n}\nEXPORT_SYMBOL_GPL(tcp_set_state);\n\n/*\n *\tState processing on a close. This implements the state shift for\n *\tsending our FIN frame. Note that we only send a FIN for some\n *\tstates. A shutdown() may have already sent the FIN, or we may be\n *\tclosed.\n */\n\nstatic const unsigned char new_state[16] = {\n  /* current state:        new state:      action:\t*/\n  /* (Invalid)\t\t*/ TCP_CLOSE,\n  /* TCP_ESTABLISHED\t*/ TCP_FIN_WAIT1 | TCP_ACTION_FIN,\n  /* TCP_SYN_SENT\t*/ TCP_CLOSE,\n  /* TCP_SYN_RECV\t*/ TCP_FIN_WAIT1 | TCP_ACTION_FIN,\n  /* TCP_FIN_WAIT1\t*/ TCP_FIN_WAIT1,\n  /* TCP_FIN_WAIT2\t*/ TCP_FIN_WAIT2,\n  /* TCP_TIME_WAIT\t*/ TCP_CLOSE,\n  /* TCP_CLOSE\t\t*/ TCP_CLOSE,\n  /* TCP_CLOSE_WAIT\t*/ TCP_LAST_ACK  | TCP_ACTION_FIN,\n  /* TCP_LAST_ACK\t*/ TCP_LAST_ACK,\n  /* TCP_LISTEN\t\t*/ TCP_CLOSE,\n  /* TCP_CLOSING\t*/ TCP_CLOSING,\n};\n\nstatic int tcp_close_state(struct sock *sk)\n{\n\tint next = (int)new_state[sk->sk_state];\n\tint ns = next & TCP_STATE_MASK;\n\n\ttcp_set_state(sk, ns);\n\n\treturn next & TCP_ACTION_FIN;\n}\n\n/*\n *\tShutdown the sending side of a connection. Much like close except\n *\tthat we don't receive shut down or sock_set_flag(sk, SOCK_DEAD).\n */\n\nvoid tcp_shutdown(struct sock *sk, int how)\n{\n\t/*\tWe need to grab some memory, and put together a FIN,\n\t *\tand then put it into the queue to be sent.\n\t *\t\tTim MacKenzie(tym@dibbler.cs.monash.edu.au) 4 Dec '92.\n\t */\n\tif (!(how & SEND_SHUTDOWN))\n\t\treturn;\n\n\t/* If we've already sent a FIN, or it's a closed state, skip this. */\n\tif ((1 << sk->sk_state) &\n\t    (TCPF_ESTABLISHED | TCPF_SYN_SENT |\n\t     TCPF_SYN_RECV | TCPF_CLOSE_WAIT)) {\n\t\t/* Clear out any half completed packets.  FIN if needed. */\n\t\tif (tcp_close_state(sk))\n\t\t\ttcp_send_fin(sk);\n\t}\n}\n\nvoid tcp_close(struct sock *sk, long timeout)\n{\n\tstruct sk_buff *skb;\n\tint data_was_unread = 0;\n\tint state;\n\n\tlock_sock(sk);\n\tsk->sk_shutdown = SHUTDOWN_MASK;\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\n\t\t/* Special case. */\n\t\tinet_csk_listen_stop(sk);\n\n\t\tgoto adjudge_to_death;\n\t}\n\n\t/*  We need to flush the recv. buffs.  We do this only on the\n\t *  descriptor close, not protocol-sourced closes, because the\n\t *  reader process may not have drained the data yet!\n\t */\n\twhile ((skb = __skb_dequeue(&sk->sk_receive_queue)) != NULL) {\n\t\tu32 len = TCP_SKB_CB(skb)->end_seq - TCP_SKB_CB(skb)->seq -\n\t\t\t  tcp_hdr(skb)->fin;\n\t\tdata_was_unread += len;\n\t\t__kfree_skb(skb);\n\t}\n\n\tsk_mem_reclaim(sk);\n\n\t/* As outlined in RFC 2525, section 2.17, we send a RST here because\n\t * data was lost. To witness the awful effects of the old behavior of\n\t * always doing a FIN, run an older 2.1.x kernel or 2.0.x, start a bulk\n\t * GET in an FTP client, suspend the process, wait for the client to\n\t * advertise a zero window, then kill -9 the FTP client, wheee...\n\t * Note: timeout is always zero in such a case.\n\t */\n\tif (data_was_unread) {\n\t\t/* Unread data was tossed, zap the connection. */\n\t\tNET_INC_STATS_USER(sock_net(sk), LINUX_MIB_TCPABORTONCLOSE);\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\ttcp_send_active_reset(sk, sk->sk_allocation);\n\t} else if (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime) {\n\t\t/* Check zero linger _after_ checking for unread data. */\n\t\tsk->sk_prot->disconnect(sk, 0);\n\t\tNET_INC_STATS_USER(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t} else if (tcp_close_state(sk)) {\n\t\t/* We FIN if the application ate all the data before\n\t\t * zapping the connection.\n\t\t */\n\n\t\t/* RED-PEN. Formally speaking, we have broken TCP state\n\t\t * machine. State transitions:\n\t\t *\n\t\t * TCP_ESTABLISHED -> TCP_FIN_WAIT1\n\t\t * TCP_SYN_RECV\t-> TCP_FIN_WAIT1 (forget it, it's impossible)\n\t\t * TCP_CLOSE_WAIT -> TCP_LAST_ACK\n\t\t *\n\t\t * are legal only when FIN has been sent (i.e. in window),\n\t\t * rather than queued out of window. Purists blame.\n\t\t *\n\t\t * F.e. \"RFC state\" is ESTABLISHED,\n\t\t * if Linux state is FIN-WAIT-1, but FIN is still not sent.\n\t\t *\n\t\t * The visible declinations are that sometimes\n\t\t * we enter time-wait state, when it is not required really\n\t\t * (harmless), do not send active resets, when they are\n\t\t * required by specs (TCP_ESTABLISHED, TCP_CLOSE_WAIT, when\n\t\t * they look as CLOSING or LAST_ACK for Linux)\n\t\t * Probably, I missed some more holelets.\n\t\t * \t\t\t\t\t\t--ANK\n\t\t */\n\t\ttcp_send_fin(sk);\n\t}\n\n\tsk_stream_wait_close(sk, timeout);\n\nadjudge_to_death:\n\tstate = sk->sk_state;\n\tsock_hold(sk);\n\tsock_orphan(sk);\n\n\t/* It is the last release_sock in its life. It will remove backlog. */\n\trelease_sock(sk);\n\n\n\t/* Now socket is owned by kernel and we acquire BH lock\n\t   to finish close. No need to check for user refs.\n\t */\n\tlocal_bh_disable();\n\tbh_lock_sock(sk);\n\tWARN_ON(sock_owned_by_user(sk));\n\n\tpercpu_counter_inc(sk->sk_prot->orphan_count);\n\n\t/* Have we already been destroyed by a softirq or backlog? */\n\tif (state != TCP_CLOSE && sk->sk_state == TCP_CLOSE)\n\t\tgoto out;\n\n\t/*\tThis is a (useful) BSD violating of the RFC. There is a\n\t *\tproblem with TCP as specified in that the other end could\n\t *\tkeep a socket open forever with no application left this end.\n\t *\tWe use a 3 minute timeout (about the same as BSD) then kill\n\t *\tour end. If they send after that then tough - BUT: long enough\n\t *\tthat we won't make the old 4*rto = almost no time - whoops\n\t *\treset mistake.\n\t *\n\t *\tNope, it was not mistake. It is really desired behaviour\n\t *\tf.e. on http servers, when such sockets are useless, but\n\t *\tconsume significant resources. Let's do it with special\n\t *\tlinger2\toption.\t\t\t\t\t--ANK\n\t */\n\n\tif (sk->sk_state == TCP_FIN_WAIT2) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tif (tp->linger2 < 0) {\n\t\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\n\t\t\tNET_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_TCPABORTONLINGER);\n\t\t} else {\n\t\t\tconst int tmo = tcp_fin_time(sk);\n\n\t\t\tif (tmo > TCP_TIMEWAIT_LEN) {\n\t\t\t\tinet_csk_reset_keepalive_timer(sk,\n\t\t\t\t\t\ttmo - TCP_TIMEWAIT_LEN);\n\t\t\t} else {\n\t\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\tif (sk->sk_state != TCP_CLOSE) {\n\t\tint orphan_count = percpu_counter_read_positive(\n\t\t\t\t\t\tsk->sk_prot->orphan_count);\n\n\t\tsk_mem_reclaim(sk);\n\t\tif (tcp_too_many_orphans(sk, orphan_count)) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tprintk(KERN_INFO \"TCP: too many of orphaned \"\n\t\t\t\t       \"sockets\\n\");\n\t\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\n\t\t\tNET_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_TCPABORTONMEMORY);\n\t\t}\n\t}\n\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tinet_csk_destroy_sock(sk);\n\t/* Otherwise, socket is reprieved until protocol close. */\n\nout:\n\tbh_unlock_sock(sk);\n\tlocal_bh_enable();\n\tsock_put(sk);\n}\n\n/* These states need RST on ABORT according to RFC793 */\n\nstatic inline int tcp_need_reset(int state)\n{\n\treturn (1 << state) &\n\t       (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT | TCPF_FIN_WAIT1 |\n\t\tTCPF_FIN_WAIT2 | TCPF_SYN_RECV);\n}\n\nint tcp_disconnect(struct sock *sk, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint err = 0;\n\tint old_state = sk->sk_state;\n\n\tif (old_state != TCP_CLOSE)\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\n\t/* ABORT function of RFC793 */\n\tif (old_state == TCP_LISTEN) {\n\t\tinet_csk_listen_stop(sk);\n\t} else if (tcp_need_reset(old_state) ||\n\t\t   (tp->snd_nxt != tp->write_seq &&\n\t\t    (1 << old_state) & (TCPF_CLOSING | TCPF_LAST_ACK))) {\n\t\t/* The last check adjusts for discrepancy of Linux wrt. RFC\n\t\t * states\n\t\t */\n\t\ttcp_send_active_reset(sk, gfp_any());\n\t\tsk->sk_err = ECONNRESET;\n\t} else if (old_state == TCP_SYN_SENT)\n\t\tsk->sk_err = ECONNRESET;\n\n\ttcp_clear_xmit_timers(sk);\n\t__skb_queue_purge(&sk->sk_receive_queue);\n\ttcp_write_queue_purge(sk);\n\t__skb_queue_purge(&tp->out_of_order_queue);\n#ifdef CONFIG_NET_DMA\n\t__skb_queue_purge(&sk->sk_async_wait_queue);\n#endif\n\n\tinet->inet_dport = 0;\n\n\tif (!(sk->sk_userlocks & SOCK_BINDADDR_LOCK))\n\t\tinet_reset_saddr(sk);\n\n\tsk->sk_shutdown = 0;\n\tsock_reset_flag(sk, SOCK_DONE);\n\ttp->srtt = 0;\n\tif ((tp->write_seq += tp->max_window + 2) == 0)\n\t\ttp->write_seq = 1;\n\ticsk->icsk_backoff = 0;\n\ttp->snd_cwnd = 2;\n\ticsk->icsk_probes_out = 0;\n\ttp->packets_out = 0;\n\ttp->snd_ssthresh = TCP_INFINITE_SSTHRESH;\n\ttp->snd_cwnd_cnt = 0;\n\ttp->bytes_acked = 0;\n\ttp->window_clamp = 0;\n\ttcp_set_ca_state(sk, TCP_CA_Open);\n\ttcp_clear_retrans(tp);\n\tinet_csk_delack_init(sk);\n\ttcp_init_send_head(sk);\n\tmemset(&tp->rx_opt, 0, sizeof(tp->rx_opt));\n\t__sk_dst_reset(sk);\n\n\tWARN_ON(inet->inet_num && !icsk->icsk_bind_hash);\n\n\tsk->sk_error_report(sk);\n\treturn err;\n}\n\n/*\n *\tSocket option code for TCP.\n */\nstatic int do_tcp_setsockopt(struct sock *sk, int level,\n\t\tint optname, char __user *optval, unsigned int optlen)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tint val;\n\tint err = 0;\n\n\t/* These are data/string values, all the others are ints */\n\tswitch (optname) {\n\tcase TCP_CONGESTION: {\n\t\tchar name[TCP_CA_NAME_MAX];\n\n\t\tif (optlen < 1)\n\t\t\treturn -EINVAL;\n\n\t\tval = strncpy_from_user(name, optval,\n\t\t\t\t\tmin_t(long, TCP_CA_NAME_MAX-1, optlen));\n\t\tif (val < 0)\n\t\t\treturn -EFAULT;\n\t\tname[val] = 0;\n\n\t\tlock_sock(sk);\n\t\terr = tcp_set_congestion_control(sk, name);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\tcase TCP_COOKIE_TRANSACTIONS: {\n\t\tstruct tcp_cookie_transactions ctd;\n\t\tstruct tcp_cookie_values *cvp = NULL;\n\n\t\tif (sizeof(ctd) > optlen)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&ctd, optval, sizeof(ctd)))\n\t\t\treturn -EFAULT;\n\n\t\tif (ctd.tcpct_used > sizeof(ctd.tcpct_value) ||\n\t\t    ctd.tcpct_s_data_desired > TCP_MSS_DESIRED)\n\t\t\treturn -EINVAL;\n\n\t\tif (ctd.tcpct_cookie_desired == 0) {\n\t\t\t/* default to global value */\n\t\t} else if ((0x1 & ctd.tcpct_cookie_desired) ||\n\t\t\t   ctd.tcpct_cookie_desired > TCP_COOKIE_MAX ||\n\t\t\t   ctd.tcpct_cookie_desired < TCP_COOKIE_MIN) {\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (TCP_COOKIE_OUT_NEVER & ctd.tcpct_flags) {\n\t\t\t/* Supercedes all other values */\n\t\t\tlock_sock(sk);\n\t\t\tif (tp->cookie_values != NULL) {\n\t\t\t\tkref_put(&tp->cookie_values->kref,\n\t\t\t\t\t tcp_cookie_values_release);\n\t\t\t\ttp->cookie_values = NULL;\n\t\t\t}\n\t\t\ttp->rx_opt.cookie_in_always = 0; /* false */\n\t\t\ttp->rx_opt.cookie_out_never = 1; /* true */\n\t\t\trelease_sock(sk);\n\t\t\treturn err;\n\t\t}\n\n\t\t/* Allocate ancillary memory before locking.\n\t\t */\n\t\tif (ctd.tcpct_used > 0 ||\n\t\t    (tp->cookie_values == NULL &&\n\t\t     (sysctl_tcp_cookie_size > 0 ||\n\t\t      ctd.tcpct_cookie_desired > 0 ||\n\t\t      ctd.tcpct_s_data_desired > 0))) {\n\t\t\tcvp = kzalloc(sizeof(*cvp) + ctd.tcpct_used,\n\t\t\t\t      GFP_KERNEL);\n\t\t\tif (cvp == NULL)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\t\tlock_sock(sk);\n\t\ttp->rx_opt.cookie_in_always =\n\t\t\t(TCP_COOKIE_IN_ALWAYS & ctd.tcpct_flags);\n\t\ttp->rx_opt.cookie_out_never = 0; /* false */\n\n\t\tif (tp->cookie_values != NULL) {\n\t\t\tif (cvp != NULL) {\n\t\t\t\t/* Changed values are recorded by a changed\n\t\t\t\t * pointer, ensuring the cookie will differ,\n\t\t\t\t * without separately hashing each value later.\n\t\t\t\t */\n\t\t\t\tkref_put(&tp->cookie_values->kref,\n\t\t\t\t\t tcp_cookie_values_release);\n\t\t\t\tkref_init(&cvp->kref);\n\t\t\t\ttp->cookie_values = cvp;\n\t\t\t} else {\n\t\t\t\tcvp = tp->cookie_values;\n\t\t\t}\n\t\t}\n\t\tif (cvp != NULL) {\n\t\t\tcvp->cookie_desired = ctd.tcpct_cookie_desired;\n\n\t\t\tif (ctd.tcpct_used > 0) {\n\t\t\t\tmemcpy(cvp->s_data_payload, ctd.tcpct_value,\n\t\t\t\t       ctd.tcpct_used);\n\t\t\t\tcvp->s_data_desired = ctd.tcpct_used;\n\t\t\t\tcvp->s_data_constant = 1; /* true */\n\t\t\t} else {\n\t\t\t\t/* No constant payload data. */\n\t\t\t\tcvp->s_data_desired = ctd.tcpct_s_data_desired;\n\t\t\t\tcvp->s_data_constant = 0; /* false */\n\t\t\t}\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\tdefault:\n\t\t/* fallthru */\n\t\tbreak;\n\t};\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase TCP_MAXSEG:\n\t\t/* Values greater than interface MTU won't take effect. However\n\t\t * at the point when this call is done we typically don't yet\n\t\t * know which interface is going to be used */\n\t\tif (val < 8 || val > MAX_TCP_WINDOW) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\ttp->rx_opt.user_mss = val;\n\t\tbreak;\n\n\tcase TCP_NODELAY:\n\t\tif (val) {\n\t\t\t/* TCP_NODELAY is weaker than TCP_CORK, so that\n\t\t\t * this option on corked socket is remembered, but\n\t\t\t * it is not activated until cork is cleared.\n\t\t\t *\n\t\t\t * However, when TCP_NODELAY is set we make\n\t\t\t * an explicit push, which overrides even TCP_CORK\n\t\t\t * for currently queued segments.\n\t\t\t */\n\t\t\ttp->nonagle |= TCP_NAGLE_OFF|TCP_NAGLE_PUSH;\n\t\t\ttcp_push_pending_frames(sk);\n\t\t} else {\n\t\t\ttp->nonagle &= ~TCP_NAGLE_OFF;\n\t\t}\n\t\tbreak;\n\n\tcase TCP_THIN_LINEAR_TIMEOUTS:\n\t\tif (val < 0 || val > 1)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->thin_lto = val;\n\t\tbreak;\n\n\tcase TCP_THIN_DUPACK:\n\t\tif (val < 0 || val > 1)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->thin_dupack = val;\n\t\tbreak;\n\n\tcase TCP_CORK:\n\t\t/* When set indicates to always queue non-full frames.\n\t\t * Later the user clears this option and we transmit\n\t\t * any pending partial frames in the queue.  This is\n\t\t * meant to be used alongside sendfile() to get properly\n\t\t * filled frames when the user (for example) must write\n\t\t * out headers with a write() call first and then use\n\t\t * sendfile to send out the data parts.\n\t\t *\n\t\t * TCP_CORK can be set together with TCP_NODELAY and it is\n\t\t * stronger than TCP_NODELAY.\n\t\t */\n\t\tif (val) {\n\t\t\ttp->nonagle |= TCP_NAGLE_CORK;\n\t\t} else {\n\t\t\ttp->nonagle &= ~TCP_NAGLE_CORK;\n\t\t\tif (tp->nonagle&TCP_NAGLE_OFF)\n\t\t\t\ttp->nonagle |= TCP_NAGLE_PUSH;\n\t\t\ttcp_push_pending_frames(sk);\n\t\t}\n\t\tbreak;\n\n\tcase TCP_KEEPIDLE:\n\t\tif (val < 1 || val > MAX_TCP_KEEPIDLE)\n\t\t\terr = -EINVAL;\n\t\telse {\n\t\t\ttp->keepalive_time = val * HZ;\n\t\t\tif (sock_flag(sk, SOCK_KEEPOPEN) &&\n\t\t\t    !((1 << sk->sk_state) &\n\t\t\t      (TCPF_CLOSE | TCPF_LISTEN))) {\n\t\t\t\t__u32 elapsed = tcp_time_stamp - tp->rcv_tstamp;\n\t\t\t\tif (tp->keepalive_time > elapsed)\n\t\t\t\t\telapsed = tp->keepalive_time - elapsed;\n\t\t\t\telse\n\t\t\t\t\telapsed = 0;\n\t\t\t\tinet_csk_reset_keepalive_timer(sk, elapsed);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase TCP_KEEPINTVL:\n\t\tif (val < 1 || val > MAX_TCP_KEEPINTVL)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->keepalive_intvl = val * HZ;\n\t\tbreak;\n\tcase TCP_KEEPCNT:\n\t\tif (val < 1 || val > MAX_TCP_KEEPCNT)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->keepalive_probes = val;\n\t\tbreak;\n\tcase TCP_SYNCNT:\n\t\tif (val < 1 || val > MAX_TCP_SYNCNT)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ticsk->icsk_syn_retries = val;\n\t\tbreak;\n\n\tcase TCP_LINGER2:\n\t\tif (val < 0)\n\t\t\ttp->linger2 = -1;\n\t\telse if (val > sysctl_tcp_fin_timeout / HZ)\n\t\t\ttp->linger2 = 0;\n\t\telse\n\t\t\ttp->linger2 = val * HZ;\n\t\tbreak;\n\n\tcase TCP_DEFER_ACCEPT:\n\t\t/* Translate value in seconds to number of retransmits */\n\t\ticsk->icsk_accept_queue.rskq_defer_accept =\n\t\t\tsecs_to_retrans(val, TCP_TIMEOUT_INIT / HZ,\n\t\t\t\t\tTCP_RTO_MAX / HZ);\n\t\tbreak;\n\n\tcase TCP_WINDOW_CLAMP:\n\t\tif (!val) {\n\t\t\tif (sk->sk_state != TCP_CLOSE) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttp->window_clamp = 0;\n\t\t} else\n\t\t\ttp->window_clamp = val < SOCK_MIN_RCVBUF / 2 ?\n\t\t\t\t\t\tSOCK_MIN_RCVBUF / 2 : val;\n\t\tbreak;\n\n\tcase TCP_QUICKACK:\n\t\tif (!val) {\n\t\t\ticsk->icsk_ack.pingpong = 1;\n\t\t} else {\n\t\t\ticsk->icsk_ack.pingpong = 0;\n\t\t\tif ((1 << sk->sk_state) &\n\t\t\t    (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT) &&\n\t\t\t    inet_csk_ack_scheduled(sk)) {\n\t\t\t\ticsk->icsk_ack.pending |= ICSK_ACK_PUSHED;\n\t\t\t\ttcp_cleanup_rbuf(sk, 1);\n\t\t\t\tif (!(val & 1))\n\t\t\t\t\ticsk->icsk_ack.pingpong = 1;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n#ifdef CONFIG_TCP_MD5SIG\n\tcase TCP_MD5SIG:\n\t\t/* Read the IP->Key mappings from userspace */\n\t\terr = tp->af_specific->md5_parse(sk, optval, optlen);\n\t\tbreak;\n#endif\n\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\treturn err;\n}\n\nint tcp_setsockopt(struct sock *sk, int level, int optname, char __user *optval,\n\t\t   unsigned int optlen)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\treturn icsk->icsk_af_ops->setsockopt(sk, level, optname,\n\t\t\t\t\t\t     optval, optlen);\n\treturn do_tcp_setsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_tcp_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, unsigned int optlen)\n{\n\tif (level != SOL_TCP)\n\t\treturn inet_csk_compat_setsockopt(sk, level, optname,\n\t\t\t\t\t\t  optval, optlen);\n\treturn do_tcp_setsockopt(sk, level, optname, optval, optlen);\n}\n\nEXPORT_SYMBOL(compat_tcp_setsockopt);\n#endif\n\n/* Return information about state of tcp endpoint in API format. */\nvoid tcp_get_info(struct sock *sk, struct tcp_info *info)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tu32 now = tcp_time_stamp;\n\n\tmemset(info, 0, sizeof(*info));\n\n\tinfo->tcpi_state = sk->sk_state;\n\tinfo->tcpi_ca_state = icsk->icsk_ca_state;\n\tinfo->tcpi_retransmits = icsk->icsk_retransmits;\n\tinfo->tcpi_probes = icsk->icsk_probes_out;\n\tinfo->tcpi_backoff = icsk->icsk_backoff;\n\n\tif (tp->rx_opt.tstamp_ok)\n\t\tinfo->tcpi_options |= TCPI_OPT_TIMESTAMPS;\n\tif (tcp_is_sack(tp))\n\t\tinfo->tcpi_options |= TCPI_OPT_SACK;\n\tif (tp->rx_opt.wscale_ok) {\n\t\tinfo->tcpi_options |= TCPI_OPT_WSCALE;\n\t\tinfo->tcpi_snd_wscale = tp->rx_opt.snd_wscale;\n\t\tinfo->tcpi_rcv_wscale = tp->rx_opt.rcv_wscale;\n\t}\n\n\tif (tp->ecn_flags&TCP_ECN_OK)\n\t\tinfo->tcpi_options |= TCPI_OPT_ECN;\n\n\tinfo->tcpi_rto = jiffies_to_usecs(icsk->icsk_rto);\n\tinfo->tcpi_ato = jiffies_to_usecs(icsk->icsk_ack.ato);\n\tinfo->tcpi_snd_mss = tp->mss_cache;\n\tinfo->tcpi_rcv_mss = icsk->icsk_ack.rcv_mss;\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\tinfo->tcpi_unacked = sk->sk_ack_backlog;\n\t\tinfo->tcpi_sacked = sk->sk_max_ack_backlog;\n\t} else {\n\t\tinfo->tcpi_unacked = tp->packets_out;\n\t\tinfo->tcpi_sacked = tp->sacked_out;\n\t}\n\tinfo->tcpi_lost = tp->lost_out;\n\tinfo->tcpi_retrans = tp->retrans_out;\n\tinfo->tcpi_fackets = tp->fackets_out;\n\n\tinfo->tcpi_last_data_sent = jiffies_to_msecs(now - tp->lsndtime);\n\tinfo->tcpi_last_data_recv = jiffies_to_msecs(now - icsk->icsk_ack.lrcvtime);\n\tinfo->tcpi_last_ack_recv = jiffies_to_msecs(now - tp->rcv_tstamp);\n\n\tinfo->tcpi_pmtu = icsk->icsk_pmtu_cookie;\n\tinfo->tcpi_rcv_ssthresh = tp->rcv_ssthresh;\n\tinfo->tcpi_rtt = jiffies_to_usecs(tp->srtt)>>3;\n\tinfo->tcpi_rttvar = jiffies_to_usecs(tp->mdev)>>2;\n\tinfo->tcpi_snd_ssthresh = tp->snd_ssthresh;\n\tinfo->tcpi_snd_cwnd = tp->snd_cwnd;\n\tinfo->tcpi_advmss = tp->advmss;\n\tinfo->tcpi_reordering = tp->reordering;\n\n\tinfo->tcpi_rcv_rtt = jiffies_to_usecs(tp->rcv_rtt_est.rtt)>>3;\n\tinfo->tcpi_rcv_space = tp->rcvq_space.space;\n\n\tinfo->tcpi_total_retrans = tp->total_retrans;\n}\n\nEXPORT_SYMBOL_GPL(tcp_get_info);\n\nstatic int do_tcp_getsockopt(struct sock *sk, int level,\n\t\tint optname, char __user *optval, int __user *optlen)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint val, len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tlen = min_t(unsigned int, len, sizeof(int));\n\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase TCP_MAXSEG:\n\t\tval = tp->mss_cache;\n\t\tif (!val && ((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_LISTEN)))\n\t\t\tval = tp->rx_opt.user_mss;\n\t\tbreak;\n\tcase TCP_NODELAY:\n\t\tval = !!(tp->nonagle&TCP_NAGLE_OFF);\n\t\tbreak;\n\tcase TCP_CORK:\n\t\tval = !!(tp->nonagle&TCP_NAGLE_CORK);\n\t\tbreak;\n\tcase TCP_KEEPIDLE:\n\t\tval = keepalive_time_when(tp) / HZ;\n\t\tbreak;\n\tcase TCP_KEEPINTVL:\n\t\tval = keepalive_intvl_when(tp) / HZ;\n\t\tbreak;\n\tcase TCP_KEEPCNT:\n\t\tval = keepalive_probes(tp);\n\t\tbreak;\n\tcase TCP_SYNCNT:\n\t\tval = icsk->icsk_syn_retries ? : sysctl_tcp_syn_retries;\n\t\tbreak;\n\tcase TCP_LINGER2:\n\t\tval = tp->linger2;\n\t\tif (val >= 0)\n\t\t\tval = (val ? : sysctl_tcp_fin_timeout) / HZ;\n\t\tbreak;\n\tcase TCP_DEFER_ACCEPT:\n\t\tval = retrans_to_secs(icsk->icsk_accept_queue.rskq_defer_accept,\n\t\t\t\t      TCP_TIMEOUT_INIT / HZ, TCP_RTO_MAX / HZ);\n\t\tbreak;\n\tcase TCP_WINDOW_CLAMP:\n\t\tval = tp->window_clamp;\n\t\tbreak;\n\tcase TCP_INFO: {\n\t\tstruct tcp_info info;\n\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\ttcp_get_info(sk, &info);\n\n\t\tlen = min_t(unsigned int, len, sizeof(info));\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &info, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase TCP_QUICKACK:\n\t\tval = !icsk->icsk_ack.pingpong;\n\t\tbreak;\n\n\tcase TCP_CONGESTION:\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tlen = min_t(unsigned int, len, TCP_CA_NAME_MAX);\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, icsk->icsk_ca_ops->name, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\n\tcase TCP_COOKIE_TRANSACTIONS: {\n\t\tstruct tcp_cookie_transactions ctd;\n\t\tstruct tcp_cookie_values *cvp = tp->cookie_values;\n\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (len < sizeof(ctd))\n\t\t\treturn -EINVAL;\n\n\t\tmemset(&ctd, 0, sizeof(ctd));\n\t\tctd.tcpct_flags = (tp->rx_opt.cookie_in_always ?\n\t\t\t\t   TCP_COOKIE_IN_ALWAYS : 0)\n\t\t\t\t| (tp->rx_opt.cookie_out_never ?\n\t\t\t\t   TCP_COOKIE_OUT_NEVER : 0);\n\n\t\tif (cvp != NULL) {\n\t\t\tctd.tcpct_flags |= (cvp->s_data_in ?\n\t\t\t\t\t    TCP_S_DATA_IN : 0)\n\t\t\t\t\t | (cvp->s_data_out ?\n\t\t\t\t\t    TCP_S_DATA_OUT : 0);\n\n\t\t\tctd.tcpct_cookie_desired = cvp->cookie_desired;\n\t\t\tctd.tcpct_s_data_desired = cvp->s_data_desired;\n\n\t\t\tmemcpy(&ctd.tcpct_value[0], &cvp->cookie_pair[0],\n\t\t\t       cvp->cookie_pair_size);\n\t\t\tctd.tcpct_used = cvp->cookie_pair_size;\n\t\t}\n\n\t\tif (put_user(sizeof(ctd), optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &ctd, sizeof(ctd)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nint tcp_getsockopt(struct sock *sk, int level, int optname, char __user *optval,\n\t\t   int __user *optlen)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\treturn icsk->icsk_af_ops->getsockopt(sk, level, optname,\n\t\t\t\t\t\t     optval, optlen);\n\treturn do_tcp_getsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_tcp_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, int __user *optlen)\n{\n\tif (level != SOL_TCP)\n\t\treturn inet_csk_compat_getsockopt(sk, level, optname,\n\t\t\t\t\t\t  optval, optlen);\n\treturn do_tcp_getsockopt(sk, level, optname, optval, optlen);\n}\n\nEXPORT_SYMBOL(compat_tcp_getsockopt);\n#endif\n\nstruct sk_buff *tcp_tso_segment(struct sk_buff *skb, int features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tstruct tcphdr *th;\n\tunsigned thlen;\n\tunsigned int seq;\n\t__be32 delta;\n\tunsigned int oldlen;\n\tunsigned int mss;\n\n\tif (!pskb_may_pull(skb, sizeof(*th)))\n\t\tgoto out;\n\n\tth = tcp_hdr(skb);\n\tthlen = th->doff * 4;\n\tif (thlen < sizeof(*th))\n\t\tgoto out;\n\n\tif (!pskb_may_pull(skb, thlen))\n\t\tgoto out;\n\n\toldlen = (u16)~skb->len;\n\t__skb_pull(skb, thlen);\n\n\tmss = skb_shinfo(skb)->gso_size;\n\tif (unlikely(skb->len <= mss))\n\t\tgoto out;\n\n\tif (skb_gso_ok(skb, features | NETIF_F_GSO_ROBUST)) {\n\t\t/* Packet is from an untrusted source, reset gso_segs. */\n\t\tint type = skb_shinfo(skb)->gso_type;\n\n\t\tif (unlikely(type &\n\t\t\t     ~(SKB_GSO_TCPV4 |\n\t\t\t       SKB_GSO_DODGY |\n\t\t\t       SKB_GSO_TCP_ECN |\n\t\t\t       SKB_GSO_TCPV6 |\n\t\t\t       0) ||\n\t\t\t     !(type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6))))\n\t\t\tgoto out;\n\n\t\tskb_shinfo(skb)->gso_segs = DIV_ROUND_UP(skb->len, mss);\n\n\t\tsegs = NULL;\n\t\tgoto out;\n\t}\n\n\tsegs = skb_segment(skb, features);\n\tif (IS_ERR(segs))\n\t\tgoto out;\n\n\tdelta = htonl(oldlen + (thlen + mss));\n\n\tskb = segs;\n\tth = tcp_hdr(skb);\n\tseq = ntohl(th->seq);\n\n\tdo {\n\t\tth->fin = th->psh = 0;\n\n\t\tth->check = ~csum_fold((__force __wsum)((__force u32)th->check +\n\t\t\t\t       (__force u32)delta));\n\t\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\t\tth->check =\n\t\t\t     csum_fold(csum_partial(skb_transport_header(skb),\n\t\t\t\t\t\t    thlen, skb->csum));\n\n\t\tseq += mss;\n\t\tskb = skb->next;\n\t\tth = tcp_hdr(skb);\n\n\t\tth->seq = htonl(seq);\n\t\tth->cwr = 0;\n\t} while (skb->next);\n\n\tdelta = htonl(oldlen + (skb->tail - skb->transport_header) +\n\t\t      skb->data_len);\n\tth->check = ~csum_fold((__force __wsum)((__force u32)th->check +\n\t\t\t\t(__force u32)delta));\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\tth->check = csum_fold(csum_partial(skb_transport_header(skb),\n\t\t\t\t\t\t   thlen, skb->csum));\n\nout:\n\treturn segs;\n}\nEXPORT_SYMBOL(tcp_tso_segment);\n\nstruct sk_buff **tcp_gro_receive(struct sk_buff **head, struct sk_buff *skb)\n{\n\tstruct sk_buff **pp = NULL;\n\tstruct sk_buff *p;\n\tstruct tcphdr *th;\n\tstruct tcphdr *th2;\n\tunsigned int len;\n\tunsigned int thlen;\n\tunsigned int flags;\n\tunsigned int mss = 1;\n\tunsigned int hlen;\n\tunsigned int off;\n\tint flush = 1;\n\tint i;\n\n\toff = skb_gro_offset(skb);\n\thlen = off + sizeof(*th);\n\tth = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\tth = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!th))\n\t\t\tgoto out;\n\t}\n\n\tthlen = th->doff * 4;\n\tif (thlen < sizeof(*th))\n\t\tgoto out;\n\n\thlen = off + thlen;\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\tth = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!th))\n\t\t\tgoto out;\n\t}\n\n\tskb_gro_pull(skb, thlen);\n\n\tlen = skb_gro_len(skb);\n\tflags = tcp_flag_word(th);\n\n\tfor (; (p = *head); head = &p->next) {\n\t\tif (!NAPI_GRO_CB(p)->same_flow)\n\t\t\tcontinue;\n\n\t\tth2 = tcp_hdr(p);\n\n\t\tif (*(u32 *)&th->source ^ *(u32 *)&th2->source) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tgoto found;\n\t}\n\n\tgoto out_check_final;\n\nfound:\n\tflush = NAPI_GRO_CB(p)->flush;\n\tflush |= flags & TCP_FLAG_CWR;\n\tflush |= (flags ^ tcp_flag_word(th2)) &\n\t\t  ~(TCP_FLAG_CWR | TCP_FLAG_FIN | TCP_FLAG_PSH);\n\tflush |= th->ack_seq ^ th2->ack_seq;\n\tfor (i = sizeof(*th); i < thlen; i += 4)\n\t\tflush |= *(u32 *)((u8 *)th + i) ^\n\t\t\t *(u32 *)((u8 *)th2 + i);\n\n\tmss = skb_shinfo(p)->gso_size;\n\n\tflush |= (len - 1) >= mss;\n\tflush |= (ntohl(th2->seq) + skb_gro_len(p)) ^ ntohl(th->seq);\n\n\tif (flush || skb_gro_receive(head, skb)) {\n\t\tmss = 1;\n\t\tgoto out_check_final;\n\t}\n\n\tp = *head;\n\tth2 = tcp_hdr(p);\n\ttcp_flag_word(th2) |= flags & (TCP_FLAG_FIN | TCP_FLAG_PSH);\n\nout_check_final:\n\tflush = len < mss;\n\tflush |= flags & (TCP_FLAG_URG | TCP_FLAG_PSH | TCP_FLAG_RST |\n\t\t\t  TCP_FLAG_SYN | TCP_FLAG_FIN);\n\n\tif (p && (!NAPI_GRO_CB(skb)->same_flow || flush))\n\t\tpp = head;\n\nout:\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\n\treturn pp;\n}\nEXPORT_SYMBOL(tcp_gro_receive);\n\nint tcp_gro_complete(struct sk_buff *skb)\n{\n\tstruct tcphdr *th = tcp_hdr(skb);\n\n\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\tskb->csum_offset = offsetof(struct tcphdr, check);\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\tskb_shinfo(skb)->gso_segs = NAPI_GRO_CB(skb)->count;\n\n\tif (th->cwr)\n\t\tskb_shinfo(skb)->gso_type |= SKB_GSO_TCP_ECN;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_gro_complete);\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic unsigned long tcp_md5sig_users;\nstatic struct tcp_md5sig_pool * __percpu *tcp_md5sig_pool;\nstatic DEFINE_SPINLOCK(tcp_md5sig_pool_lock);\n\nstatic void __tcp_free_md5sig_pool(struct tcp_md5sig_pool * __percpu *pool)\n{\n\tint cpu;\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct tcp_md5sig_pool *p = *per_cpu_ptr(pool, cpu);\n\t\tif (p) {\n\t\t\tif (p->md5_desc.tfm)\n\t\t\t\tcrypto_free_hash(p->md5_desc.tfm);\n\t\t\tkfree(p);\n\t\t\tp = NULL;\n\t\t}\n\t}\n\tfree_percpu(pool);\n}\n\nvoid tcp_free_md5sig_pool(void)\n{\n\tstruct tcp_md5sig_pool * __percpu *pool = NULL;\n\n\tspin_lock_bh(&tcp_md5sig_pool_lock);\n\tif (--tcp_md5sig_users == 0) {\n\t\tpool = tcp_md5sig_pool;\n\t\ttcp_md5sig_pool = NULL;\n\t}\n\tspin_unlock_bh(&tcp_md5sig_pool_lock);\n\tif (pool)\n\t\t__tcp_free_md5sig_pool(pool);\n}\n\nEXPORT_SYMBOL(tcp_free_md5sig_pool);\n\nstatic struct tcp_md5sig_pool * __percpu *\n__tcp_alloc_md5sig_pool(struct sock *sk)\n{\n\tint cpu;\n\tstruct tcp_md5sig_pool * __percpu *pool;\n\n\tpool = alloc_percpu(struct tcp_md5sig_pool *);\n\tif (!pool)\n\t\treturn NULL;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct tcp_md5sig_pool *p;\n\t\tstruct crypto_hash *hash;\n\n\t\tp = kzalloc(sizeof(*p), sk->sk_allocation);\n\t\tif (!p)\n\t\t\tgoto out_free;\n\t\t*per_cpu_ptr(pool, cpu) = p;\n\n\t\thash = crypto_alloc_hash(\"md5\", 0, CRYPTO_ALG_ASYNC);\n\t\tif (!hash || IS_ERR(hash))\n\t\t\tgoto out_free;\n\n\t\tp->md5_desc.tfm = hash;\n\t}\n\treturn pool;\nout_free:\n\t__tcp_free_md5sig_pool(pool);\n\treturn NULL;\n}\n\nstruct tcp_md5sig_pool * __percpu *tcp_alloc_md5sig_pool(struct sock *sk)\n{\n\tstruct tcp_md5sig_pool * __percpu *pool;\n\tint alloc = 0;\n\nretry:\n\tspin_lock_bh(&tcp_md5sig_pool_lock);\n\tpool = tcp_md5sig_pool;\n\tif (tcp_md5sig_users++ == 0) {\n\t\talloc = 1;\n\t\tspin_unlock_bh(&tcp_md5sig_pool_lock);\n\t} else if (!pool) {\n\t\ttcp_md5sig_users--;\n\t\tspin_unlock_bh(&tcp_md5sig_pool_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t} else\n\t\tspin_unlock_bh(&tcp_md5sig_pool_lock);\n\n\tif (alloc) {\n\t\t/* we cannot hold spinlock here because this may sleep. */\n\t\tstruct tcp_md5sig_pool * __percpu *p;\n\n\t\tp = __tcp_alloc_md5sig_pool(sk);\n\t\tspin_lock_bh(&tcp_md5sig_pool_lock);\n\t\tif (!p) {\n\t\t\ttcp_md5sig_users--;\n\t\t\tspin_unlock_bh(&tcp_md5sig_pool_lock);\n\t\t\treturn NULL;\n\t\t}\n\t\tpool = tcp_md5sig_pool;\n\t\tif (pool) {\n\t\t\t/* oops, it has already been assigned. */\n\t\t\tspin_unlock_bh(&tcp_md5sig_pool_lock);\n\t\t\t__tcp_free_md5sig_pool(p);\n\t\t} else {\n\t\t\ttcp_md5sig_pool = pool = p;\n\t\t\tspin_unlock_bh(&tcp_md5sig_pool_lock);\n\t\t}\n\t}\n\treturn pool;\n}\n\nEXPORT_SYMBOL(tcp_alloc_md5sig_pool);\n\nstruct tcp_md5sig_pool *__tcp_get_md5sig_pool(int cpu)\n{\n\tstruct tcp_md5sig_pool * __percpu *p;\n\tspin_lock_bh(&tcp_md5sig_pool_lock);\n\tp = tcp_md5sig_pool;\n\tif (p)\n\t\ttcp_md5sig_users++;\n\tspin_unlock_bh(&tcp_md5sig_pool_lock);\n\treturn (p ? *per_cpu_ptr(p, cpu) : NULL);\n}\n\nEXPORT_SYMBOL(__tcp_get_md5sig_pool);\n\nvoid __tcp_put_md5sig_pool(void)\n{\n\ttcp_free_md5sig_pool();\n}\n\nEXPORT_SYMBOL(__tcp_put_md5sig_pool);\n\nint tcp_md5_hash_header(struct tcp_md5sig_pool *hp,\n\t\t\tstruct tcphdr *th)\n{\n\tstruct scatterlist sg;\n\tint err;\n\n\t__sum16 old_checksum = th->check;\n\tth->check = 0;\n\t/* options aren't included in the hash */\n\tsg_init_one(&sg, th, sizeof(struct tcphdr));\n\terr = crypto_hash_update(&hp->md5_desc, &sg, sizeof(struct tcphdr));\n\tth->check = old_checksum;\n\treturn err;\n}\n\nEXPORT_SYMBOL(tcp_md5_hash_header);\n\nint tcp_md5_hash_skb_data(struct tcp_md5sig_pool *hp,\n\t\t\t  struct sk_buff *skb, unsigned header_len)\n{\n\tstruct scatterlist sg;\n\tconst struct tcphdr *tp = tcp_hdr(skb);\n\tstruct hash_desc *desc = &hp->md5_desc;\n\tunsigned i;\n\tconst unsigned head_data_len = skb_headlen(skb) > header_len ?\n\t\t\t\t       skb_headlen(skb) - header_len : 0;\n\tconst struct skb_shared_info *shi = skb_shinfo(skb);\n\n\tsg_init_table(&sg, 1);\n\n\tsg_set_buf(&sg, ((u8 *) tp) + header_len, head_data_len);\n\tif (crypto_hash_update(desc, &sg, head_data_len))\n\t\treturn 1;\n\n\tfor (i = 0; i < shi->nr_frags; ++i) {\n\t\tconst struct skb_frag_struct *f = &shi->frags[i];\n\t\tsg_set_page(&sg, f->page, f->size, f->page_offset);\n\t\tif (crypto_hash_update(desc, &sg, f->size))\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nEXPORT_SYMBOL(tcp_md5_hash_skb_data);\n\nint tcp_md5_hash_key(struct tcp_md5sig_pool *hp, struct tcp_md5sig_key *key)\n{\n\tstruct scatterlist sg;\n\n\tsg_init_one(&sg, key->key, key->keylen);\n\treturn crypto_hash_update(&hp->md5_desc, &sg, key->keylen);\n}\n\nEXPORT_SYMBOL(tcp_md5_hash_key);\n\n#endif\n\n/**\n * Each Responder maintains up to two secret values concurrently for\n * efficient secret rollover.  Each secret value has 4 states:\n *\n * Generating.  (tcp_secret_generating != tcp_secret_primary)\n *    Generates new Responder-Cookies, but not yet used for primary\n *    verification.  This is a short-term state, typically lasting only\n *    one round trip time (RTT).\n *\n * Primary.  (tcp_secret_generating == tcp_secret_primary)\n *    Used both for generation and primary verification.\n *\n * Retiring.  (tcp_secret_retiring != tcp_secret_secondary)\n *    Used for verification, until the first failure that can be\n *    verified by the newer Generating secret.  At that time, this\n *    cookie's state is changed to Secondary, and the Generating\n *    cookie's state is changed to Primary.  This is a short-term state,\n *    typically lasting only one round trip time (RTT).\n *\n * Secondary.  (tcp_secret_retiring == tcp_secret_secondary)\n *    Used for secondary verification, after primary verification\n *    failures.  This state lasts no more than twice the Maximum Segment\n *    Lifetime (2MSL).  Then, the secret is discarded.\n */\nstruct tcp_cookie_secret {\n\t/* The secret is divided into two parts.  The digest part is the\n\t * equivalent of previously hashing a secret and saving the state,\n\t * and serves as an initialization vector (IV).  The message part\n\t * serves as the trailing secret.\n\t */\n\tu32\t\t\t\tsecrets[COOKIE_WORKSPACE_WORDS];\n\tunsigned long\t\t\texpires;\n};\n\n#define TCP_SECRET_1MSL (HZ * TCP_PAWS_MSL)\n#define TCP_SECRET_2MSL (HZ * TCP_PAWS_MSL * 2)\n#define TCP_SECRET_LIFE (HZ * 600)\n\nstatic struct tcp_cookie_secret tcp_secret_one;\nstatic struct tcp_cookie_secret tcp_secret_two;\n\n/* Essentially a circular list, without dynamic allocation. */\nstatic struct tcp_cookie_secret *tcp_secret_generating;\nstatic struct tcp_cookie_secret *tcp_secret_primary;\nstatic struct tcp_cookie_secret *tcp_secret_retiring;\nstatic struct tcp_cookie_secret *tcp_secret_secondary;\n\nstatic DEFINE_SPINLOCK(tcp_secret_locker);\n\n/* Select a pseudo-random word in the cookie workspace.\n */\nstatic inline u32 tcp_cookie_work(const u32 *ws, const int n)\n{\n\treturn ws[COOKIE_DIGEST_WORDS + ((COOKIE_MESSAGE_WORDS-1) & ws[n])];\n}\n\n/* Fill bakery[COOKIE_WORKSPACE_WORDS] with generator, updating as needed.\n * Called in softirq context.\n * Returns: 0 for success.\n */\nint tcp_cookie_generator(u32 *bakery)\n{\n\tunsigned long jiffy = jiffies;\n\n\tif (unlikely(time_after_eq(jiffy, tcp_secret_generating->expires))) {\n\t\tspin_lock_bh(&tcp_secret_locker);\n\t\tif (!time_after_eq(jiffy, tcp_secret_generating->expires)) {\n\t\t\t/* refreshed by another */\n\t\t\tmemcpy(bakery,\n\t\t\t       &tcp_secret_generating->secrets[0],\n\t\t\t       COOKIE_WORKSPACE_WORDS);\n\t\t} else {\n\t\t\t/* still needs refreshing */\n\t\t\tget_random_bytes(bakery, COOKIE_WORKSPACE_WORDS);\n\n\t\t\t/* The first time, paranoia assumes that the\n\t\t\t * randomization function isn't as strong.  But,\n\t\t\t * this secret initialization is delayed until\n\t\t\t * the last possible moment (packet arrival).\n\t\t\t * Although that time is observable, it is\n\t\t\t * unpredictably variable.  Mash in the most\n\t\t\t * volatile clock bits available, and expire the\n\t\t\t * secret extra quickly.\n\t\t\t */\n\t\t\tif (unlikely(tcp_secret_primary->expires ==\n\t\t\t\t     tcp_secret_secondary->expires)) {\n\t\t\t\tstruct timespec tv;\n\n\t\t\t\tgetnstimeofday(&tv);\n\t\t\t\tbakery[COOKIE_DIGEST_WORDS+0] ^=\n\t\t\t\t\t(u32)tv.tv_nsec;\n\n\t\t\t\ttcp_secret_secondary->expires = jiffy\n\t\t\t\t\t+ TCP_SECRET_1MSL\n\t\t\t\t\t+ (0x0f & tcp_cookie_work(bakery, 0));\n\t\t\t} else {\n\t\t\t\ttcp_secret_secondary->expires = jiffy\n\t\t\t\t\t+ TCP_SECRET_LIFE\n\t\t\t\t\t+ (0xff & tcp_cookie_work(bakery, 1));\n\t\t\t\ttcp_secret_primary->expires = jiffy\n\t\t\t\t\t+ TCP_SECRET_2MSL\n\t\t\t\t\t+ (0x1f & tcp_cookie_work(bakery, 2));\n\t\t\t}\n\t\t\tmemcpy(&tcp_secret_secondary->secrets[0],\n\t\t\t       bakery, COOKIE_WORKSPACE_WORDS);\n\n\t\t\trcu_assign_pointer(tcp_secret_generating,\n\t\t\t\t\t   tcp_secret_secondary);\n\t\t\trcu_assign_pointer(tcp_secret_retiring,\n\t\t\t\t\t   tcp_secret_primary);\n\t\t\t/*\n\t\t\t * Neither call_rcu() nor synchronize_rcu() needed.\n\t\t\t * Retiring data is not freed.  It is replaced after\n\t\t\t * further (locked) pointer updates, and a quiet time\n\t\t\t * (minimum 1MSL, maximum LIFE - 2MSL).\n\t\t\t */\n\t\t}\n\t\tspin_unlock_bh(&tcp_secret_locker);\n\t} else {\n\t\trcu_read_lock_bh();\n\t\tmemcpy(bakery,\n\t\t       &rcu_dereference(tcp_secret_generating)->secrets[0],\n\t\t       COOKIE_WORKSPACE_WORDS);\n\t\trcu_read_unlock_bh();\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_cookie_generator);\n\nvoid tcp_done(struct sock *sk)\n{\n\tif (sk->sk_state == TCP_SYN_SENT || sk->sk_state == TCP_SYN_RECV)\n\t\tTCP_INC_STATS_BH(sock_net(sk), TCP_MIB_ATTEMPTFAILS);\n\n\ttcp_set_state(sk, TCP_CLOSE);\n\ttcp_clear_xmit_timers(sk);\n\n\tsk->sk_shutdown = SHUTDOWN_MASK;\n\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_state_change(sk);\n\telse\n\t\tinet_csk_destroy_sock(sk);\n}\nEXPORT_SYMBOL_GPL(tcp_done);\n\nextern struct tcp_congestion_ops tcp_reno;\n\nstatic __initdata unsigned long thash_entries;\nstatic int __init set_thash_entries(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\tthash_entries = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"thash_entries=\", set_thash_entries);\n\nvoid __init tcp_init(void)\n{\n\tstruct sk_buff *skb = NULL;\n\tunsigned long nr_pages, limit;\n\tint order, i, max_share;\n\tunsigned long jiffy = jiffies;\n\n\tBUILD_BUG_ON(sizeof(struct tcp_skb_cb) > sizeof(skb->cb));\n\n\tpercpu_counter_init(&tcp_sockets_allocated, 0);\n\tpercpu_counter_init(&tcp_orphan_count, 0);\n\ttcp_hashinfo.bind_bucket_cachep =\n\t\tkmem_cache_create(\"tcp_bind_bucket\",\n\t\t\t\t  sizeof(struct inet_bind_bucket), 0,\n\t\t\t\t  SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);\n\n\t/* Size and allocate the main established and bind bucket\n\t * hash tables.\n\t *\n\t * The methodology is similar to that of the buffer cache.\n\t */\n\ttcp_hashinfo.ehash =\n\t\talloc_large_system_hash(\"TCP established\",\n\t\t\t\t\tsizeof(struct inet_ehash_bucket),\n\t\t\t\t\tthash_entries,\n\t\t\t\t\t(totalram_pages >= 128 * 1024) ?\n\t\t\t\t\t13 : 15,\n\t\t\t\t\t0,\n\t\t\t\t\tNULL,\n\t\t\t\t\t&tcp_hashinfo.ehash_mask,\n\t\t\t\t\tthash_entries ? 0 : 512 * 1024);\n\tfor (i = 0; i <= tcp_hashinfo.ehash_mask; i++) {\n\t\tINIT_HLIST_NULLS_HEAD(&tcp_hashinfo.ehash[i].chain, i);\n\t\tINIT_HLIST_NULLS_HEAD(&tcp_hashinfo.ehash[i].twchain, i);\n\t}\n\tif (inet_ehash_locks_alloc(&tcp_hashinfo))\n\t\tpanic(\"TCP: failed to alloc ehash_locks\");\n\ttcp_hashinfo.bhash =\n\t\talloc_large_system_hash(\"TCP bind\",\n\t\t\t\t\tsizeof(struct inet_bind_hashbucket),\n\t\t\t\t\ttcp_hashinfo.ehash_mask + 1,\n\t\t\t\t\t(totalram_pages >= 128 * 1024) ?\n\t\t\t\t\t13 : 15,\n\t\t\t\t\t0,\n\t\t\t\t\t&tcp_hashinfo.bhash_size,\n\t\t\t\t\tNULL,\n\t\t\t\t\t64 * 1024);\n\ttcp_hashinfo.bhash_size = 1 << tcp_hashinfo.bhash_size;\n\tfor (i = 0; i < tcp_hashinfo.bhash_size; i++) {\n\t\tspin_lock_init(&tcp_hashinfo.bhash[i].lock);\n\t\tINIT_HLIST_HEAD(&tcp_hashinfo.bhash[i].chain);\n\t}\n\n\t/* Try to be a bit smarter and adjust defaults depending\n\t * on available memory.\n\t */\n\tfor (order = 0; ((1 << order) << PAGE_SHIFT) <\n\t\t\t(tcp_hashinfo.bhash_size * sizeof(struct inet_bind_hashbucket));\n\t\t\torder++)\n\t\t;\n\tif (order >= 4) {\n\t\ttcp_death_row.sysctl_max_tw_buckets = 180000;\n\t\tsysctl_tcp_max_orphans = 4096 << (order - 4);\n\t\tsysctl_max_syn_backlog = 1024;\n\t} else if (order < 3) {\n\t\ttcp_death_row.sysctl_max_tw_buckets >>= (3 - order);\n\t\tsysctl_tcp_max_orphans >>= (3 - order);\n\t\tsysctl_max_syn_backlog = 128;\n\t}\n\n\t/* Set the pressure threshold to be a fraction of global memory that\n\t * is up to 1/2 at 256 MB, decreasing toward zero with the amount of\n\t * memory, with a floor of 128 pages.\n\t */\n\tnr_pages = totalram_pages - totalhigh_pages;\n\tlimit = min(nr_pages, 1UL<<(28-PAGE_SHIFT)) >> (20-PAGE_SHIFT);\n\tlimit = (limit * (nr_pages >> (20-PAGE_SHIFT))) >> (PAGE_SHIFT-11);\n\tlimit = max(limit, 128UL);\n\tsysctl_tcp_mem[0] = limit / 4 * 3;\n\tsysctl_tcp_mem[1] = limit;\n\tsysctl_tcp_mem[2] = sysctl_tcp_mem[0] * 2;\n\n\t/* Set per-socket limits to no more than 1/128 the pressure threshold */\n\tlimit = ((unsigned long)sysctl_tcp_mem[1]) << (PAGE_SHIFT - 7);\n\tmax_share = min(4UL*1024*1024, limit);\n\n\tsysctl_tcp_wmem[0] = SK_MEM_QUANTUM;\n\tsysctl_tcp_wmem[1] = 16*1024;\n\tsysctl_tcp_wmem[2] = max(64*1024, max_share);\n\n\tsysctl_tcp_rmem[0] = SK_MEM_QUANTUM;\n\tsysctl_tcp_rmem[1] = 87380;\n\tsysctl_tcp_rmem[2] = max(87380, max_share);\n\n\tprintk(KERN_INFO \"TCP: Hash tables configured \"\n\t       \"(established %u bind %u)\\n\",\n\t       tcp_hashinfo.ehash_mask + 1, tcp_hashinfo.bhash_size);\n\n\ttcp_register_congestion_control(&tcp_reno);\n\n\tmemset(&tcp_secret_one.secrets[0], 0, sizeof(tcp_secret_one.secrets));\n\tmemset(&tcp_secret_two.secrets[0], 0, sizeof(tcp_secret_two.secrets));\n\ttcp_secret_one.expires = jiffy; /* past due */\n\ttcp_secret_two.expires = jiffy; /* past due */\n\ttcp_secret_generating = &tcp_secret_one;\n\ttcp_secret_primary = &tcp_secret_one;\n\ttcp_secret_retiring = &tcp_secret_two;\n\ttcp_secret_secondary = &tcp_secret_two;\n}\n\nEXPORT_SYMBOL(tcp_close);\nEXPORT_SYMBOL(tcp_disconnect);\nEXPORT_SYMBOL(tcp_getsockopt);\nEXPORT_SYMBOL(tcp_ioctl);\nEXPORT_SYMBOL(tcp_poll);\nEXPORT_SYMBOL(tcp_read_sock);\nEXPORT_SYMBOL(tcp_recvmsg);\nEXPORT_SYMBOL(tcp_sendmsg);\nEXPORT_SYMBOL(tcp_splice_read);\nEXPORT_SYMBOL(tcp_sendpage);\nEXPORT_SYMBOL(tcp_setsockopt);\nEXPORT_SYMBOL(tcp_shutdown);\n"], "fixing_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tImplementation of the Transmission Control Protocol(TCP).\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\t\tCorey Minyard <wf-rch!minyard@relay.EU.net>\n *\t\tFlorian La Roche, <flla@stud.uni-sb.de>\n *\t\tCharles Hedrick, <hedrick@klinzhai.rutgers.edu>\n *\t\tLinus Torvalds, <torvalds@cs.helsinki.fi>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\t\tMatthew Dillon, <dillon@apollo.west.oic.com>\n *\t\tArnt Gulbrandsen, <agulbra@nvg.unit.no>\n *\t\tJorge Cwik, <jorge@laser.satlink.net>\n *\n * Fixes:\n *\t\tAlan Cox\t:\tNumerous verify_area() calls\n *\t\tAlan Cox\t:\tSet the ACK bit on a reset\n *\t\tAlan Cox\t:\tStopped it crashing if it closed while\n *\t\t\t\t\tsk->inuse=1 and was trying to connect\n *\t\t\t\t\t(tcp_err()).\n *\t\tAlan Cox\t:\tAll icmp error handling was broken\n *\t\t\t\t\tpointers passed where wrong and the\n *\t\t\t\t\tsocket was looked up backwards. Nobody\n *\t\t\t\t\ttested any icmp error code obviously.\n *\t\tAlan Cox\t:\ttcp_err() now handled properly. It\n *\t\t\t\t\twakes people on errors. poll\n *\t\t\t\t\tbehaves and the icmp error race\n *\t\t\t\t\thas gone by moving it into sock.c\n *\t\tAlan Cox\t:\ttcp_send_reset() fixed to work for\n *\t\t\t\t\teverything not just packets for\n *\t\t\t\t\tunknown sockets.\n *\t\tAlan Cox\t:\ttcp option processing.\n *\t\tAlan Cox\t:\tReset tweaked (still not 100%) [Had\n *\t\t\t\t\tsyn rule wrong]\n *\t\tHerp Rosmanith  :\tMore reset fixes\n *\t\tAlan Cox\t:\tNo longer acks invalid rst frames.\n *\t\t\t\t\tAcking any kind of RST is right out.\n *\t\tAlan Cox\t:\tSets an ignore me flag on an rst\n *\t\t\t\t\treceive otherwise odd bits of prattle\n *\t\t\t\t\tescape still\n *\t\tAlan Cox\t:\tFixed another acking RST frame bug.\n *\t\t\t\t\tShould stop LAN workplace lockups.\n *\t\tAlan Cox\t: \tSome tidyups using the new skb list\n *\t\t\t\t\tfacilities\n *\t\tAlan Cox\t:\tsk->keepopen now seems to work\n *\t\tAlan Cox\t:\tPulls options out correctly on accepts\n *\t\tAlan Cox\t:\tFixed assorted sk->rqueue->next errors\n *\t\tAlan Cox\t:\tPSH doesn't end a TCP read. Switched a\n *\t\t\t\t\tbit to skb ops.\n *\t\tAlan Cox\t:\tTidied tcp_data to avoid a potential\n *\t\t\t\t\tnasty.\n *\t\tAlan Cox\t:\tAdded some better commenting, as the\n *\t\t\t\t\ttcp is hard to follow\n *\t\tAlan Cox\t:\tRemoved incorrect check for 20 * psh\n *\tMichael O'Reilly\t:\tack < copied bug fix.\n *\tJohannes Stille\t\t:\tMisc tcp fixes (not all in yet).\n *\t\tAlan Cox\t:\tFIN with no memory -> CRASH\n *\t\tAlan Cox\t:\tAdded socket option proto entries.\n *\t\t\t\t\tAlso added awareness of them to accept.\n *\t\tAlan Cox\t:\tAdded TCP options (SOL_TCP)\n *\t\tAlan Cox\t:\tSwitched wakeup calls to callbacks,\n *\t\t\t\t\tso the kernel can layer network\n *\t\t\t\t\tsockets.\n *\t\tAlan Cox\t:\tUse ip_tos/ip_ttl settings.\n *\t\tAlan Cox\t:\tHandle FIN (more) properly (we hope).\n *\t\tAlan Cox\t:\tRST frames sent on unsynchronised\n *\t\t\t\t\tstate ack error.\n *\t\tAlan Cox\t:\tPut in missing check for SYN bit.\n *\t\tAlan Cox\t:\tAdded tcp_select_window() aka NET2E\n *\t\t\t\t\twindow non shrink trick.\n *\t\tAlan Cox\t:\tAdded a couple of small NET2E timer\n *\t\t\t\t\tfixes\n *\t\tCharles Hedrick :\tTCP fixes\n *\t\tToomas Tamm\t:\tTCP window fixes\n *\t\tAlan Cox\t:\tSmall URG fix to rlogin ^C ack fight\n *\t\tCharles Hedrick\t:\tRewrote most of it to actually work\n *\t\tLinus\t\t:\tRewrote tcp_read() and URG handling\n *\t\t\t\t\tcompletely\n *\t\tGerhard Koerting:\tFixed some missing timer handling\n *\t\tMatthew Dillon  :\tReworked TCP machine states as per RFC\n *\t\tGerhard Koerting:\tPC/TCP workarounds\n *\t\tAdam Caldwell\t:\tAssorted timer/timing errors\n *\t\tMatthew Dillon\t:\tFixed another RST bug\n *\t\tAlan Cox\t:\tMove to kernel side addressing changes.\n *\t\tAlan Cox\t:\tBeginning work on TCP fastpathing\n *\t\t\t\t\t(not yet usable)\n *\t\tArnt Gulbrandsen:\tTurbocharged tcp_check() routine.\n *\t\tAlan Cox\t:\tTCP fast path debugging\n *\t\tAlan Cox\t:\tWindow clamping\n *\t\tMichael Riepe\t:\tBug in tcp_check()\n *\t\tMatt Dillon\t:\tMore TCP improvements and RST bug fixes\n *\t\tMatt Dillon\t:\tYet more small nasties remove from the\n *\t\t\t\t\tTCP code (Be very nice to this man if\n *\t\t\t\t\ttcp finally works 100%) 8)\n *\t\tAlan Cox\t:\tBSD accept semantics.\n *\t\tAlan Cox\t:\tReset on closedown bug.\n *\tPeter De Schrijver\t:\tENOTCONN check missing in tcp_sendto().\n *\t\tMichael Pall\t:\tHandle poll() after URG properly in\n *\t\t\t\t\tall cases.\n *\t\tMichael Pall\t:\tUndo the last fix in tcp_read_urg()\n *\t\t\t\t\t(multi URG PUSH broke rlogin).\n *\t\tMichael Pall\t:\tFix the multi URG PUSH problem in\n *\t\t\t\t\ttcp_readable(), poll() after URG\n *\t\t\t\t\tworks now.\n *\t\tMichael Pall\t:\trecv(...,MSG_OOB) never blocks in the\n *\t\t\t\t\tBSD api.\n *\t\tAlan Cox\t:\tChanged the semantics of sk->socket to\n *\t\t\t\t\tfix a race and a signal problem with\n *\t\t\t\t\taccept() and async I/O.\n *\t\tAlan Cox\t:\tRelaxed the rules on tcp_sendto().\n *\t\tYury Shevchuk\t:\tReally fixed accept() blocking problem.\n *\t\tCraig I. Hagan  :\tAllow for BSD compatible TIME_WAIT for\n *\t\t\t\t\tclients/servers which listen in on\n *\t\t\t\t\tfixed ports.\n *\t\tAlan Cox\t:\tCleaned the above up and shrank it to\n *\t\t\t\t\ta sensible code size.\n *\t\tAlan Cox\t:\tSelf connect lockup fix.\n *\t\tAlan Cox\t:\tNo connect to multicast.\n *\t\tRoss Biro\t:\tClose unaccepted children on master\n *\t\t\t\t\tsocket close.\n *\t\tAlan Cox\t:\tReset tracing code.\n *\t\tAlan Cox\t:\tSpurious resets on shutdown.\n *\t\tAlan Cox\t:\tGiant 15 minute/60 second timer error\n *\t\tAlan Cox\t:\tSmall whoops in polling before an\n *\t\t\t\t\taccept.\n *\t\tAlan Cox\t:\tKept the state trace facility since\n *\t\t\t\t\tit's handy for debugging.\n *\t\tAlan Cox\t:\tMore reset handler fixes.\n *\t\tAlan Cox\t:\tStarted rewriting the code based on\n *\t\t\t\t\tthe RFC's for other useful protocol\n *\t\t\t\t\treferences see: Comer, KA9Q NOS, and\n *\t\t\t\t\tfor a reference on the difference\n *\t\t\t\t\tbetween specifications and how BSD\n *\t\t\t\t\tworks see the 4.4lite source.\n *\t\tA.N.Kuznetsov\t:\tDon't time wait on completion of tidy\n *\t\t\t\t\tclose.\n *\t\tLinus Torvalds\t:\tFin/Shutdown & copied_seq changes.\n *\t\tLinus Torvalds\t:\tFixed BSD port reuse to work first syn\n *\t\tAlan Cox\t:\tReimplemented timers as per the RFC\n *\t\t\t\t\tand using multiple timers for sanity.\n *\t\tAlan Cox\t:\tSmall bug fixes, and a lot of new\n *\t\t\t\t\tcomments.\n *\t\tAlan Cox\t:\tFixed dual reader crash by locking\n *\t\t\t\t\tthe buffers (much like datagram.c)\n *\t\tAlan Cox\t:\tFixed stuck sockets in probe. A probe\n *\t\t\t\t\tnow gets fed up of retrying without\n *\t\t\t\t\t(even a no space) answer.\n *\t\tAlan Cox\t:\tExtracted closing code better\n *\t\tAlan Cox\t:\tFixed the closing state machine to\n *\t\t\t\t\tresemble the RFC.\n *\t\tAlan Cox\t:\tMore 'per spec' fixes.\n *\t\tJorge Cwik\t:\tEven faster checksumming.\n *\t\tAlan Cox\t:\ttcp_data() doesn't ack illegal PSH\n *\t\t\t\t\tonly frames. At least one pc tcp stack\n *\t\t\t\t\tgenerates them.\n *\t\tAlan Cox\t:\tCache last socket.\n *\t\tAlan Cox\t:\tPer route irtt.\n *\t\tMatt Day\t:\tpoll()->select() match BSD precisely on error\n *\t\tAlan Cox\t:\tNew buffers\n *\t\tMarc Tamsky\t:\tVarious sk->prot->retransmits and\n *\t\t\t\t\tsk->retransmits misupdating fixed.\n *\t\t\t\t\tFixed tcp_write_timeout: stuck close,\n *\t\t\t\t\tand TCP syn retries gets used now.\n *\t\tMark Yarvis\t:\tIn tcp_read_wakeup(), don't send an\n *\t\t\t\t\tack if state is TCP_CLOSED.\n *\t\tAlan Cox\t:\tLook up device on a retransmit - routes may\n *\t\t\t\t\tchange. Doesn't yet cope with MSS shrink right\n *\t\t\t\t\tbut it's a start!\n *\t\tMarc Tamsky\t:\tClosing in closing fixes.\n *\t\tMike Shaver\t:\tRFC1122 verifications.\n *\t\tAlan Cox\t:\trcv_saddr errors.\n *\t\tAlan Cox\t:\tBlock double connect().\n *\t\tAlan Cox\t:\tSmall hooks for enSKIP.\n *\t\tAlexey Kuznetsov:\tPath MTU discovery.\n *\t\tAlan Cox\t:\tSupport soft errors.\n *\t\tAlan Cox\t:\tFix MTU discovery pathological case\n *\t\t\t\t\twhen the remote claims no mtu!\n *\t\tMarc Tamsky\t:\tTCP_CLOSE fix.\n *\t\tColin (G3TNE)\t:\tSend a reset on syn ack replies in\n *\t\t\t\t\twindow but wrong (fixes NT lpd problems)\n *\t\tPedro Roque\t:\tBetter TCP window handling, delayed ack.\n *\t\tJoerg Reuter\t:\tNo modification of locked buffers in\n *\t\t\t\t\ttcp_do_retransmit()\n *\t\tEric Schenk\t:\tChanged receiver side silly window\n *\t\t\t\t\tavoidance algorithm to BSD style\n *\t\t\t\t\talgorithm. This doubles throughput\n *\t\t\t\t\tagainst machines running Solaris,\n *\t\t\t\t\tand seems to result in general\n *\t\t\t\t\timprovement.\n *\tStefan Magdalinski\t:\tadjusted tcp_readable() to fix FIONREAD\n *\tWilly Konynenberg\t:\tTransparent proxying support.\n *\tMike McLagan\t\t:\tRouting by source\n *\t\tKeith Owens\t:\tDo proper merging with partial SKB's in\n *\t\t\t\t\ttcp_do_sendmsg to avoid burstiness.\n *\t\tEric Schenk\t:\tFix fast close down bug with\n *\t\t\t\t\tshutdown() followed by close().\n *\t\tAndi Kleen \t:\tMake poll agree with SIGIO\n *\tSalvatore Sanfilippo\t:\tSupport SO_LINGER with linger == 1 and\n *\t\t\t\t\tlingertime == 0 (RFC 793 ABORT Call)\n *\tHirokazu Takahashi\t:\tUse copy_from_user() instead of\n *\t\t\t\t\tcsum_and_copy_from_user() if possible.\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or(at your option) any later version.\n *\n * Description of States:\n *\n *\tTCP_SYN_SENT\t\tsent a connection request, waiting for ack\n *\n *\tTCP_SYN_RECV\t\treceived a connection request, sent ack,\n *\t\t\t\twaiting for final ack in three-way handshake.\n *\n *\tTCP_ESTABLISHED\t\tconnection established\n *\n *\tTCP_FIN_WAIT1\t\tour side has shutdown, waiting to complete\n *\t\t\t\ttransmission of remaining buffered data\n *\n *\tTCP_FIN_WAIT2\t\tall buffered data sent, waiting for remote\n *\t\t\t\tto shutdown\n *\n *\tTCP_CLOSING\t\tboth sides have shutdown but we still have\n *\t\t\t\tdata we have to finish sending\n *\n *\tTCP_TIME_WAIT\t\ttimeout to catch resent junk before entering\n *\t\t\t\tclosed, can only be entered from FIN_WAIT2\n *\t\t\t\tor CLOSING.  Required because the other end\n *\t\t\t\tmay not have gotten our last ACK causing it\n *\t\t\t\tto retransmit the data packet (which we ignore)\n *\n *\tTCP_CLOSE_WAIT\t\tremote side has shutdown and is waiting for\n *\t\t\t\tus to finish writing our data and to shutdown\n *\t\t\t\t(we have to close() to move on to LAST_ACK)\n *\n *\tTCP_LAST_ACK\t\tout side has shutdown after remote has\n *\t\t\t\tshutdown.  There may still be data in our\n *\t\t\t\tbuffer that we have to finish sending\n *\n *\tTCP_CLOSE\t\tsocket is finished\n */\n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/fcntl.h>\n#include <linux/poll.h>\n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/skbuff.h>\n#include <linux/scatterlist.h>\n#include <linux/splice.h>\n#include <linux/net.h>\n#include <linux/socket.h>\n#include <linux/random.h>\n#include <linux/bootmem.h>\n#include <linux/highmem.h>\n#include <linux/swap.h>\n#include <linux/cache.h>\n#include <linux/err.h>\n#include <linux/crypto.h>\n#include <linux/time.h>\n\n#include <net/icmp.h>\n#include <net/tcp.h>\n#include <net/xfrm.h>\n#include <net/ip.h>\n#include <net/netdma.h>\n#include <net/sock.h>\n\n#include <asm/uaccess.h>\n#include <asm/ioctls.h>\n\nint sysctl_tcp_fin_timeout __read_mostly = TCP_FIN_TIMEOUT;\n\nstruct percpu_counter tcp_orphan_count;\nEXPORT_SYMBOL_GPL(tcp_orphan_count);\n\nint sysctl_tcp_mem[3] __read_mostly;\nint sysctl_tcp_wmem[3] __read_mostly;\nint sysctl_tcp_rmem[3] __read_mostly;\n\nEXPORT_SYMBOL(sysctl_tcp_mem);\nEXPORT_SYMBOL(sysctl_tcp_rmem);\nEXPORT_SYMBOL(sysctl_tcp_wmem);\n\natomic_t tcp_memory_allocated;\t/* Current allocated memory. */\nEXPORT_SYMBOL(tcp_memory_allocated);\n\n/*\n * Current number of TCP sockets.\n */\nstruct percpu_counter tcp_sockets_allocated;\nEXPORT_SYMBOL(tcp_sockets_allocated);\n\n/*\n * TCP splice context\n */\nstruct tcp_splice_state {\n\tstruct pipe_inode_info *pipe;\n\tsize_t len;\n\tunsigned int flags;\n};\n\n/*\n * Pressure flag: try to collapse.\n * Technical note: it is used by multiple contexts non atomically.\n * All the __sk_mem_schedule() is of this nature: accounting\n * is strict, actions are advisory and have some latency.\n */\nint tcp_memory_pressure __read_mostly;\n\nEXPORT_SYMBOL(tcp_memory_pressure);\n\nvoid tcp_enter_memory_pressure(struct sock *sk)\n{\n\tif (!tcp_memory_pressure) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMEMORYPRESSURES);\n\t\ttcp_memory_pressure = 1;\n\t}\n}\n\nEXPORT_SYMBOL(tcp_enter_memory_pressure);\n\n/* Convert seconds to retransmits based on initial and max timeout */\nstatic u8 secs_to_retrans(int seconds, int timeout, int rto_max)\n{\n\tu8 res = 0;\n\n\tif (seconds > 0) {\n\t\tint period = timeout;\n\n\t\tres = 1;\n\t\twhile (seconds > period && res < 255) {\n\t\t\tres++;\n\t\t\ttimeout <<= 1;\n\t\t\tif (timeout > rto_max)\n\t\t\t\ttimeout = rto_max;\n\t\t\tperiod += timeout;\n\t\t}\n\t}\n\treturn res;\n}\n\n/* Convert retransmits to seconds based on initial and max timeout */\nstatic int retrans_to_secs(u8 retrans, int timeout, int rto_max)\n{\n\tint period = 0;\n\n\tif (retrans > 0) {\n\t\tperiod = timeout;\n\t\twhile (--retrans) {\n\t\t\ttimeout <<= 1;\n\t\t\tif (timeout > rto_max)\n\t\t\t\ttimeout = rto_max;\n\t\t\tperiod += timeout;\n\t\t}\n\t}\n\treturn period;\n}\n\n/*\n *\tWait for a TCP event.\n *\n *\tNote that we don't need to lock the socket, as the upper poll layers\n *\ttake care of normal races (between the test and the event) and we don't\n *\tgo look at any of the socket buffers directly.\n */\nunsigned int tcp_poll(struct file *file, struct socket *sock, poll_table *wait)\n{\n\tunsigned int mask;\n\tstruct sock *sk = sock->sk;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tsock_poll_wait(file, sk->sk_sleep, wait);\n\tif (sk->sk_state == TCP_LISTEN)\n\t\treturn inet_csk_listen_poll(sk);\n\n\t/* Socket is not locked. We are protected from async events\n\t * by poll logic and correct handling of state changes\n\t * made by other threads is impossible in any case.\n\t */\n\n\tmask = 0;\n\tif (sk->sk_err)\n\t\tmask = POLLERR;\n\n\t/*\n\t * POLLHUP is certainly not done right. But poll() doesn't\n\t * have a notion of HUP in just one direction, and for a\n\t * socket the read side is more interesting.\n\t *\n\t * Some poll() documentation says that POLLHUP is incompatible\n\t * with the POLLOUT/POLLWR flags, so somebody should check this\n\t * all. But careful, it tends to be safer to return too many\n\t * bits than too few, and you can easily break real applications\n\t * if you don't tell them that something has hung up!\n\t *\n\t * Check-me.\n\t *\n\t * Check number 1. POLLHUP is _UNMASKABLE_ event (see UNIX98 and\n\t * our fs/select.c). It means that after we received EOF,\n\t * poll always returns immediately, making impossible poll() on write()\n\t * in state CLOSE_WAIT. One solution is evident --- to set POLLHUP\n\t * if and only if shutdown has been made in both directions.\n\t * Actually, it is interesting to look how Solaris and DUX\n\t * solve this dilemma. I would prefer, if POLLHUP were maskable,\n\t * then we could set it on SND_SHUTDOWN. BTW examples given\n\t * in Stevens' books assume exactly this behaviour, it explains\n\t * why POLLHUP is incompatible with POLLOUT.\t--ANK\n\t *\n\t * NOTE. Check for TCP_CLOSE is added. The goal is to prevent\n\t * blocking on fresh not-connected or disconnected socket. --ANK\n\t */\n\tif (sk->sk_shutdown == SHUTDOWN_MASK || sk->sk_state == TCP_CLOSE)\n\t\tmask |= POLLHUP;\n\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\tmask |= POLLIN | POLLRDNORM | POLLRDHUP;\n\n\t/* Connected? */\n\tif ((1 << sk->sk_state) & ~(TCPF_SYN_SENT | TCPF_SYN_RECV)) {\n\t\tint target = sock_rcvlowat(sk, 0, INT_MAX);\n\n\t\tif (tp->urg_seq == tp->copied_seq &&\n\t\t    !sock_flag(sk, SOCK_URGINLINE) &&\n\t\t    tp->urg_data)\n\t\t\ttarget++;\n\n\t\t/* Potential race condition. If read of tp below will\n\t\t * escape above sk->sk_state, we can be illegally awaken\n\t\t * in SYN_* states. */\n\t\tif (tp->rcv_nxt - tp->copied_seq >= target)\n\t\t\tmask |= POLLIN | POLLRDNORM;\n\n\t\tif (!(sk->sk_shutdown & SEND_SHUTDOWN)) {\n\t\t\tif (sk_stream_wspace(sk) >= sk_stream_min_wspace(sk)) {\n\t\t\t\tmask |= POLLOUT | POLLWRNORM;\n\t\t\t} else {  /* send SIGIO later */\n\t\t\t\tset_bit(SOCK_ASYNC_NOSPACE,\n\t\t\t\t\t&sk->sk_socket->flags);\n\t\t\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\n\t\t\t\t/* Race breaker. If space is freed after\n\t\t\t\t * wspace test but before the flags are set,\n\t\t\t\t * IO signal will be lost.\n\t\t\t\t */\n\t\t\t\tif (sk_stream_wspace(sk) >= sk_stream_min_wspace(sk))\n\t\t\t\t\tmask |= POLLOUT | POLLWRNORM;\n\t\t\t}\n\t\t}\n\n\t\tif (tp->urg_data & TCP_URG_VALID)\n\t\t\tmask |= POLLPRI;\n\t}\n\treturn mask;\n}\n\nint tcp_ioctl(struct sock *sk, int cmd, unsigned long arg)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint answ;\n\n\tswitch (cmd) {\n\tcase SIOCINQ:\n\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\treturn -EINVAL;\n\n\t\tlock_sock(sk);\n\t\tif ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV))\n\t\t\tansw = 0;\n\t\telse if (sock_flag(sk, SOCK_URGINLINE) ||\n\t\t\t !tp->urg_data ||\n\t\t\t before(tp->urg_seq, tp->copied_seq) ||\n\t\t\t !before(tp->urg_seq, tp->rcv_nxt)) {\n\t\t\tstruct sk_buff *skb;\n\n\t\t\tansw = tp->rcv_nxt - tp->copied_seq;\n\n\t\t\t/* Subtract 1, if FIN is in queue. */\n\t\t\tskb = skb_peek_tail(&sk->sk_receive_queue);\n\t\t\tif (answ && skb)\n\t\t\t\tansw -= tcp_hdr(skb)->fin;\n\t\t} else\n\t\t\tansw = tp->urg_seq - tp->copied_seq;\n\t\trelease_sock(sk);\n\t\tbreak;\n\tcase SIOCATMARK:\n\t\tansw = tp->urg_data && tp->urg_seq == tp->copied_seq;\n\t\tbreak;\n\tcase SIOCOUTQ:\n\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\treturn -EINVAL;\n\n\t\tif ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV))\n\t\t\tansw = 0;\n\t\telse\n\t\t\tansw = tp->write_seq - tp->snd_una;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\n\treturn put_user(answ, (int __user *)arg);\n}\n\nstatic inline void tcp_mark_push(struct tcp_sock *tp, struct sk_buff *skb)\n{\n\tTCP_SKB_CB(skb)->flags |= TCPCB_FLAG_PSH;\n\ttp->pushed_seq = tp->write_seq;\n}\n\nstatic inline int forced_push(struct tcp_sock *tp)\n{\n\treturn after(tp->write_seq, tp->pushed_seq + (tp->max_window >> 1));\n}\n\nstatic inline void skb_entail(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\n\n\tskb->csum    = 0;\n\ttcb->seq     = tcb->end_seq = tp->write_seq;\n\ttcb->flags   = TCPCB_FLAG_ACK;\n\ttcb->sacked  = 0;\n\tskb_header_release(skb);\n\ttcp_add_write_queue_tail(sk, skb);\n\tsk->sk_wmem_queued += skb->truesize;\n\tsk_mem_charge(sk, skb->truesize);\n\tif (tp->nonagle & TCP_NAGLE_PUSH)\n\t\ttp->nonagle &= ~TCP_NAGLE_PUSH;\n}\n\nstatic inline void tcp_mark_urg(struct tcp_sock *tp, int flags)\n{\n\tif (flags & MSG_OOB)\n\t\ttp->snd_up = tp->write_seq;\n}\n\nstatic inline void tcp_push(struct sock *sk, int flags, int mss_now,\n\t\t\t    int nonagle)\n{\n\tif (tcp_send_head(sk)) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t\tif (!(flags & MSG_MORE) || forced_push(tp))\n\t\t\ttcp_mark_push(tp, tcp_write_queue_tail(sk));\n\n\t\ttcp_mark_urg(tp, flags);\n\t\t__tcp_push_pending_frames(sk, mss_now,\n\t\t\t\t\t  (flags & MSG_MORE) ? TCP_NAGLE_CORK : nonagle);\n\t}\n}\n\nstatic int tcp_splice_data_recv(read_descriptor_t *rd_desc, struct sk_buff *skb,\n\t\t\t\tunsigned int offset, size_t len)\n{\n\tstruct tcp_splice_state *tss = rd_desc->arg.data;\n\tint ret;\n\n\tret = skb_splice_bits(skb, offset, tss->pipe, min(rd_desc->count, len),\n\t\t\t      tss->flags);\n\tif (ret > 0)\n\t\trd_desc->count -= ret;\n\treturn ret;\n}\n\nstatic int __tcp_splice_read(struct sock *sk, struct tcp_splice_state *tss)\n{\n\t/* Store TCP splice context information in read_descriptor_t. */\n\tread_descriptor_t rd_desc = {\n\t\t.arg.data = tss,\n\t\t.count\t  = tss->len,\n\t};\n\n\treturn tcp_read_sock(sk, &rd_desc, tcp_splice_data_recv);\n}\n\n/**\n *  tcp_splice_read - splice data from TCP socket to a pipe\n * @sock:\tsocket to splice from\n * @ppos:\tposition (not valid)\n * @pipe:\tpipe to splice to\n * @len:\tnumber of bytes to splice\n * @flags:\tsplice modifier flags\n *\n * Description:\n *    Will read pages from given socket and fill them into a pipe.\n *\n **/\nssize_t tcp_splice_read(struct socket *sock, loff_t *ppos,\n\t\t\tstruct pipe_inode_info *pipe, size_t len,\n\t\t\tunsigned int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct tcp_splice_state tss = {\n\t\t.pipe = pipe,\n\t\t.len = len,\n\t\t.flags = flags,\n\t};\n\tlong timeo;\n\tssize_t spliced;\n\tint ret;\n\n\t/*\n\t * We can't seek on a socket input\n\t */\n\tif (unlikely(*ppos))\n\t\treturn -ESPIPE;\n\n\tret = spliced = 0;\n\n\tlock_sock(sk);\n\n\ttimeo = sock_rcvtimeo(sk, sock->file->f_flags & O_NONBLOCK);\n\twhile (tss.len) {\n\t\tret = __tcp_splice_read(sk, &tss);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\telse if (!ret) {\n\t\t\tif (spliced)\n\t\t\t\tbreak;\n\t\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\t\tbreak;\n\t\t\tif (sk->sk_err) {\n\t\t\t\tret = sock_error(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\t\t\tif (sk->sk_state == TCP_CLOSE) {\n\t\t\t\t/*\n\t\t\t\t * This occurs when user tries to read\n\t\t\t\t * from never connected socket.\n\t\t\t\t */\n\t\t\t\tif (!sock_flag(sk, SOCK_DONE))\n\t\t\t\t\tret = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!timeo) {\n\t\t\t\tret = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tsk_wait_data(sk, &timeo);\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tret = sock_intr_errno(timeo);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\ttss.len -= ret;\n\t\tspliced += ret;\n\n\t\tif (!timeo)\n\t\t\tbreak;\n\t\trelease_sock(sk);\n\t\tlock_sock(sk);\n\n\t\tif (sk->sk_err || sk->sk_state == TCP_CLOSE ||\n\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t    signal_pending(current))\n\t\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\n\tif (spliced)\n\t\treturn spliced;\n\n\treturn ret;\n}\n\nstruct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp)\n{\n\tstruct sk_buff *skb;\n\n\t/* The TCP header must be at least 32-bit aligned.  */\n\tsize = ALIGN(size, 4);\n\n\tskb = alloc_skb_fclone(size + sk->sk_prot->max_header, gfp);\n\tif (skb) {\n\t\tif (sk_wmem_schedule(sk, skb->truesize)) {\n\t\t\t/*\n\t\t\t * Make sure that we have exactly size bytes\n\t\t\t * available to the caller, no more, no less.\n\t\t\t */\n\t\t\tskb_reserve(skb, skb_tailroom(skb) - size);\n\t\t\treturn skb;\n\t\t}\n\t\t__kfree_skb(skb);\n\t} else {\n\t\tsk->sk_prot->enter_memory_pressure(sk);\n\t\tsk_stream_moderate_sndbuf(sk);\n\t}\n\treturn NULL;\n}\n\nstatic unsigned int tcp_xmit_size_goal(struct sock *sk, u32 mss_now,\n\t\t\t\t       int large_allowed)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 xmit_size_goal, old_size_goal;\n\n\txmit_size_goal = mss_now;\n\n\tif (large_allowed && sk_can_gso(sk)) {\n\t\txmit_size_goal = ((sk->sk_gso_max_size - 1) -\n\t\t\t\t  inet_csk(sk)->icsk_af_ops->net_header_len -\n\t\t\t\t  inet_csk(sk)->icsk_ext_hdr_len -\n\t\t\t\t  tp->tcp_header_len);\n\n\t\txmit_size_goal = tcp_bound_to_half_wnd(tp, xmit_size_goal);\n\n\t\t/* We try hard to avoid divides here */\n\t\told_size_goal = tp->xmit_size_goal_segs * mss_now;\n\n\t\tif (likely(old_size_goal <= xmit_size_goal &&\n\t\t\t   old_size_goal + mss_now > xmit_size_goal)) {\n\t\t\txmit_size_goal = old_size_goal;\n\t\t} else {\n\t\t\ttp->xmit_size_goal_segs = xmit_size_goal / mss_now;\n\t\t\txmit_size_goal = tp->xmit_size_goal_segs * mss_now;\n\t\t}\n\t}\n\n\treturn max(xmit_size_goal, mss_now);\n}\n\nstatic int tcp_send_mss(struct sock *sk, int *size_goal, int flags)\n{\n\tint mss_now;\n\n\tmss_now = tcp_current_mss(sk);\n\t*size_goal = tcp_xmit_size_goal(sk, mss_now, !(flags & MSG_OOB));\n\n\treturn mss_now;\n}\n\nstatic ssize_t do_tcp_sendpages(struct sock *sk, struct page **pages, int poffset,\n\t\t\t size_t psize, int flags)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint mss_now, size_goal;\n\tint err;\n\tssize_t copied;\n\tlong timeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);\n\n\t/* Wait for a connection to finish. */\n\tif ((1 << sk->sk_state) & ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT))\n\t\tif ((err = sk_stream_wait_connect(sk, &timeo)) != 0)\n\t\t\tgoto out_err;\n\n\tclear_bit(SOCK_ASYNC_NOSPACE, &sk->sk_socket->flags);\n\n\tmss_now = tcp_send_mss(sk, &size_goal, flags);\n\tcopied = 0;\n\n\terr = -EPIPE;\n\tif (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN))\n\t\tgoto out_err;\n\n\twhile (psize > 0) {\n\t\tstruct sk_buff *skb = tcp_write_queue_tail(sk);\n\t\tstruct page *page = pages[poffset / PAGE_SIZE];\n\t\tint copy, i, can_coalesce;\n\t\tint offset = poffset % PAGE_SIZE;\n\t\tint size = min_t(size_t, psize, PAGE_SIZE - offset);\n\n\t\tif (!tcp_send_head(sk) || (copy = size_goal - skb->len) <= 0) {\nnew_segment:\n\t\t\tif (!sk_stream_memory_free(sk))\n\t\t\t\tgoto wait_for_sndbuf;\n\n\t\t\tskb = sk_stream_alloc_skb(sk, 0, sk->sk_allocation);\n\t\t\tif (!skb)\n\t\t\t\tgoto wait_for_memory;\n\n\t\t\tskb_entail(sk, skb);\n\t\t\tcopy = size_goal;\n\t\t}\n\n\t\tif (copy > size)\n\t\t\tcopy = size;\n\n\t\ti = skb_shinfo(skb)->nr_frags;\n\t\tcan_coalesce = skb_can_coalesce(skb, i, page, offset);\n\t\tif (!can_coalesce && i >= MAX_SKB_FRAGS) {\n\t\t\ttcp_mark_push(tp, skb);\n\t\t\tgoto new_segment;\n\t\t}\n\t\tif (!sk_wmem_schedule(sk, copy))\n\t\t\tgoto wait_for_memory;\n\n\t\tif (can_coalesce) {\n\t\t\tskb_shinfo(skb)->frags[i - 1].size += copy;\n\t\t} else {\n\t\t\tget_page(page);\n\t\t\tskb_fill_page_desc(skb, i, page, offset, copy);\n\t\t}\n\n\t\tskb->len += copy;\n\t\tskb->data_len += copy;\n\t\tskb->truesize += copy;\n\t\tsk->sk_wmem_queued += copy;\n\t\tsk_mem_charge(sk, copy);\n\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\ttp->write_seq += copy;\n\t\tTCP_SKB_CB(skb)->end_seq += copy;\n\t\tskb_shinfo(skb)->gso_segs = 0;\n\n\t\tif (!copied)\n\t\t\tTCP_SKB_CB(skb)->flags &= ~TCPCB_FLAG_PSH;\n\n\t\tcopied += copy;\n\t\tpoffset += copy;\n\t\tif (!(psize -= copy))\n\t\t\tgoto out;\n\n\t\tif (skb->len < size_goal || (flags & MSG_OOB))\n\t\t\tcontinue;\n\n\t\tif (forced_push(tp)) {\n\t\t\ttcp_mark_push(tp, skb);\n\t\t\t__tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH);\n\t\t} else if (skb == tcp_send_head(sk))\n\t\t\ttcp_push_one(sk, mss_now);\n\t\tcontinue;\n\nwait_for_sndbuf:\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\nwait_for_memory:\n\t\tif (copied)\n\t\t\ttcp_push(sk, flags & ~MSG_MORE, mss_now, TCP_NAGLE_PUSH);\n\n\t\tif ((err = sk_stream_wait_memory(sk, &timeo)) != 0)\n\t\t\tgoto do_error;\n\n\t\tmss_now = tcp_send_mss(sk, &size_goal, flags);\n\t}\n\nout:\n\tif (copied)\n\t\ttcp_push(sk, flags, mss_now, tp->nonagle);\n\treturn copied;\n\ndo_error:\n\tif (copied)\n\t\tgoto out;\nout_err:\n\treturn sk_stream_error(sk, flags, err);\n}\n\nssize_t tcp_sendpage(struct socket *sock, struct page *page, int offset,\n\t\t     size_t size, int flags)\n{\n\tssize_t res;\n\tstruct sock *sk = sock->sk;\n\n\tif (!(sk->sk_route_caps & NETIF_F_SG) ||\n\t    !(sk->sk_route_caps & NETIF_F_ALL_CSUM))\n\t\treturn sock_no_sendpage(sock, page, offset, size, flags);\n\n\tlock_sock(sk);\n\tTCP_CHECK_TIMER(sk);\n\tres = do_tcp_sendpages(sk, &page, offset, size, flags);\n\tTCP_CHECK_TIMER(sk);\n\trelease_sock(sk);\n\treturn res;\n}\n\n#define TCP_PAGE(sk)\t(sk->sk_sndmsg_page)\n#define TCP_OFF(sk)\t(sk->sk_sndmsg_off)\n\nstatic inline int select_size(struct sock *sk, int sg)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint tmp = tp->mss_cache;\n\n\tif (sg) {\n\t\tif (sk_can_gso(sk))\n\t\t\ttmp = 0;\n\t\telse {\n\t\t\tint pgbreak = SKB_MAX_HEAD(MAX_TCP_HEADER);\n\n\t\t\tif (tmp >= pgbreak &&\n\t\t\t    tmp <= pgbreak + (MAX_SKB_FRAGS - 1) * PAGE_SIZE)\n\t\t\t\ttmp = pgbreak;\n\t\t}\n\t}\n\n\treturn tmp;\n}\n\nint tcp_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,\n\t\tsize_t size)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct iovec *iov;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tint iovlen, flags;\n\tint mss_now, size_goal;\n\tint sg, err, copied;\n\tlong timeo;\n\n\tlock_sock(sk);\n\tTCP_CHECK_TIMER(sk);\n\n\tflags = msg->msg_flags;\n\ttimeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);\n\n\t/* Wait for a connection to finish. */\n\tif ((1 << sk->sk_state) & ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT))\n\t\tif ((err = sk_stream_wait_connect(sk, &timeo)) != 0)\n\t\t\tgoto out_err;\n\n\t/* This should be in poll */\n\tclear_bit(SOCK_ASYNC_NOSPACE, &sk->sk_socket->flags);\n\n\tmss_now = tcp_send_mss(sk, &size_goal, flags);\n\n\t/* Ok commence sending. */\n\tiovlen = msg->msg_iovlen;\n\tiov = msg->msg_iov;\n\tcopied = 0;\n\n\terr = -EPIPE;\n\tif (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN))\n\t\tgoto out_err;\n\n\tsg = sk->sk_route_caps & NETIF_F_SG;\n\n\twhile (--iovlen >= 0) {\n\t\tint seglen = iov->iov_len;\n\t\tunsigned char __user *from = iov->iov_base;\n\n\t\tiov++;\n\n\t\twhile (seglen > 0) {\n\t\t\tint copy = 0;\n\t\t\tint max = size_goal;\n\n\t\t\tskb = tcp_write_queue_tail(sk);\n\t\t\tif (tcp_send_head(sk)) {\n\t\t\t\tif (skb->ip_summed == CHECKSUM_NONE)\n\t\t\t\t\tmax = mss_now;\n\t\t\t\tcopy = max - skb->len;\n\t\t\t}\n\n\t\t\tif (copy <= 0) {\nnew_segment:\n\t\t\t\t/* Allocate new segment. If the interface is SG,\n\t\t\t\t * allocate skb fitting to single page.\n\t\t\t\t */\n\t\t\t\tif (!sk_stream_memory_free(sk))\n\t\t\t\t\tgoto wait_for_sndbuf;\n\n\t\t\t\tskb = sk_stream_alloc_skb(sk,\n\t\t\t\t\t\t\t  select_size(sk, sg),\n\t\t\t\t\t\t\t  sk->sk_allocation);\n\t\t\t\tif (!skb)\n\t\t\t\t\tgoto wait_for_memory;\n\n\t\t\t\t/*\n\t\t\t\t * Check whether we can use HW checksum.\n\t\t\t\t */\n\t\t\t\tif (sk->sk_route_caps & NETIF_F_ALL_CSUM)\n\t\t\t\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\t\t\t\tskb_entail(sk, skb);\n\t\t\t\tcopy = size_goal;\n\t\t\t\tmax = size_goal;\n\t\t\t}\n\n\t\t\t/* Try to append data to the end of skb. */\n\t\t\tif (copy > seglen)\n\t\t\t\tcopy = seglen;\n\n\t\t\t/* Where to copy to? */\n\t\t\tif (skb_tailroom(skb) > 0) {\n\t\t\t\t/* We have some space in skb head. Superb! */\n\t\t\t\tif (copy > skb_tailroom(skb))\n\t\t\t\t\tcopy = skb_tailroom(skb);\n\t\t\t\tif ((err = skb_add_data(skb, from, copy)) != 0)\n\t\t\t\t\tgoto do_fault;\n\t\t\t} else {\n\t\t\t\tint merge = 0;\n\t\t\t\tint i = skb_shinfo(skb)->nr_frags;\n\t\t\t\tstruct page *page = TCP_PAGE(sk);\n\t\t\t\tint off = TCP_OFF(sk);\n\n\t\t\t\tif (skb_can_coalesce(skb, i, page, off) &&\n\t\t\t\t    off != PAGE_SIZE) {\n\t\t\t\t\t/* We can extend the last page\n\t\t\t\t\t * fragment. */\n\t\t\t\t\tmerge = 1;\n\t\t\t\t} else if (i == MAX_SKB_FRAGS || !sg) {\n\t\t\t\t\t/* Need to add new fragment and cannot\n\t\t\t\t\t * do this because interface is non-SG,\n\t\t\t\t\t * or because all the page slots are\n\t\t\t\t\t * busy. */\n\t\t\t\t\ttcp_mark_push(tp, skb);\n\t\t\t\t\tgoto new_segment;\n\t\t\t\t} else if (page) {\n\t\t\t\t\tif (off == PAGE_SIZE) {\n\t\t\t\t\t\tput_page(page);\n\t\t\t\t\t\tTCP_PAGE(sk) = page = NULL;\n\t\t\t\t\t\toff = 0;\n\t\t\t\t\t}\n\t\t\t\t} else\n\t\t\t\t\toff = 0;\n\n\t\t\t\tif (copy > PAGE_SIZE - off)\n\t\t\t\t\tcopy = PAGE_SIZE - off;\n\n\t\t\t\tif (!sk_wmem_schedule(sk, copy))\n\t\t\t\t\tgoto wait_for_memory;\n\n\t\t\t\tif (!page) {\n\t\t\t\t\t/* Allocate new cache page. */\n\t\t\t\t\tif (!(page = sk_stream_alloc_page(sk)))\n\t\t\t\t\t\tgoto wait_for_memory;\n\t\t\t\t}\n\n\t\t\t\t/* Time to copy data. We are close to\n\t\t\t\t * the end! */\n\t\t\t\terr = skb_copy_to_page(sk, from, skb, page,\n\t\t\t\t\t\t       off, copy);\n\t\t\t\tif (err) {\n\t\t\t\t\t/* If this page was new, give it to the\n\t\t\t\t\t * socket so it does not get leaked.\n\t\t\t\t\t */\n\t\t\t\t\tif (!TCP_PAGE(sk)) {\n\t\t\t\t\t\tTCP_PAGE(sk) = page;\n\t\t\t\t\t\tTCP_OFF(sk) = 0;\n\t\t\t\t\t}\n\t\t\t\t\tgoto do_error;\n\t\t\t\t}\n\n\t\t\t\t/* Update the skb. */\n\t\t\t\tif (merge) {\n\t\t\t\t\tskb_shinfo(skb)->frags[i - 1].size +=\n\t\t\t\t\t\t\t\t\tcopy;\n\t\t\t\t} else {\n\t\t\t\t\tskb_fill_page_desc(skb, i, page, off, copy);\n\t\t\t\t\tif (TCP_PAGE(sk)) {\n\t\t\t\t\t\tget_page(page);\n\t\t\t\t\t} else if (off + copy < PAGE_SIZE) {\n\t\t\t\t\t\tget_page(page);\n\t\t\t\t\t\tTCP_PAGE(sk) = page;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tTCP_OFF(sk) = off + copy;\n\t\t\t}\n\n\t\t\tif (!copied)\n\t\t\t\tTCP_SKB_CB(skb)->flags &= ~TCPCB_FLAG_PSH;\n\n\t\t\ttp->write_seq += copy;\n\t\t\tTCP_SKB_CB(skb)->end_seq += copy;\n\t\t\tskb_shinfo(skb)->gso_segs = 0;\n\n\t\t\tfrom += copy;\n\t\t\tcopied += copy;\n\t\t\tif ((seglen -= copy) == 0 && iovlen == 0)\n\t\t\t\tgoto out;\n\n\t\t\tif (skb->len < max || (flags & MSG_OOB))\n\t\t\t\tcontinue;\n\n\t\t\tif (forced_push(tp)) {\n\t\t\t\ttcp_mark_push(tp, skb);\n\t\t\t\t__tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH);\n\t\t\t} else if (skb == tcp_send_head(sk))\n\t\t\t\ttcp_push_one(sk, mss_now);\n\t\t\tcontinue;\n\nwait_for_sndbuf:\n\t\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\nwait_for_memory:\n\t\t\tif (copied)\n\t\t\t\ttcp_push(sk, flags & ~MSG_MORE, mss_now, TCP_NAGLE_PUSH);\n\n\t\t\tif ((err = sk_stream_wait_memory(sk, &timeo)) != 0)\n\t\t\t\tgoto do_error;\n\n\t\t\tmss_now = tcp_send_mss(sk, &size_goal, flags);\n\t\t}\n\t}\n\nout:\n\tif (copied)\n\t\ttcp_push(sk, flags, mss_now, tp->nonagle);\n\tTCP_CHECK_TIMER(sk);\n\trelease_sock(sk);\n\treturn copied;\n\ndo_fault:\n\tif (!skb->len) {\n\t\ttcp_unlink_write_queue(skb, sk);\n\t\t/* It is the one place in all of TCP, except connection\n\t\t * reset, where we can be unlinking the send_head.\n\t\t */\n\t\ttcp_check_send_head(sk, skb);\n\t\tsk_wmem_free_skb(sk, skb);\n\t}\n\ndo_error:\n\tif (copied)\n\t\tgoto out;\nout_err:\n\terr = sk_stream_error(sk, flags, err);\n\tTCP_CHECK_TIMER(sk);\n\trelease_sock(sk);\n\treturn err;\n}\n\n/*\n *\tHandle reading urgent data. BSD has very simple semantics for\n *\tthis, no blocking and very strange errors 8)\n */\n\nstatic int tcp_recv_urg(struct sock *sk, struct msghdr *msg, int len, int flags)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* No URG data to read. */\n\tif (sock_flag(sk, SOCK_URGINLINE) || !tp->urg_data ||\n\t    tp->urg_data == TCP_URG_READ)\n\t\treturn -EINVAL;\t/* Yes this is right ! */\n\n\tif (sk->sk_state == TCP_CLOSE && !sock_flag(sk, SOCK_DONE))\n\t\treturn -ENOTCONN;\n\n\tif (tp->urg_data & TCP_URG_VALID) {\n\t\tint err = 0;\n\t\tchar c = tp->urg_data;\n\n\t\tif (!(flags & MSG_PEEK))\n\t\t\ttp->urg_data = TCP_URG_READ;\n\n\t\t/* Read urgent data. */\n\t\tmsg->msg_flags |= MSG_OOB;\n\n\t\tif (len > 0) {\n\t\t\tif (!(flags & MSG_TRUNC))\n\t\t\t\terr = memcpy_toiovec(msg->msg_iov, &c, 1);\n\t\t\tlen = 1;\n\t\t} else\n\t\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\t\treturn err ? -EFAULT : len;\n\t}\n\n\tif (sk->sk_state == TCP_CLOSE || (sk->sk_shutdown & RCV_SHUTDOWN))\n\t\treturn 0;\n\n\t/* Fixed the recv(..., MSG_OOB) behaviour.  BSD docs and\n\t * the available implementations agree in this case:\n\t * this call should never block, independent of the\n\t * blocking state of the socket.\n\t * Mike <pall@rz.uni-karlsruhe.de>\n\t */\n\treturn -EAGAIN;\n}\n\n/* Clean up the receive buffer for full frames taken by the user,\n * then send an ACK if necessary.  COPIED is the number of bytes\n * tcp_recvmsg has given to the user so far, it speeds up the\n * calculation of whether or not we must ACK for the sake of\n * a window update.\n */\nvoid tcp_cleanup_rbuf(struct sock *sk, int copied)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint time_to_ack = 0;\n\n#if TCP_DEBUG\n\tstruct sk_buff *skb = skb_peek(&sk->sk_receive_queue);\n\n\tWARN(skb && !before(tp->copied_seq, TCP_SKB_CB(skb)->end_seq),\n\t     KERN_INFO \"cleanup rbuf bug: copied %X seq %X rcvnxt %X\\n\",\n\t     tp->copied_seq, TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt);\n#endif\n\n\tif (inet_csk_ack_scheduled(sk)) {\n\t\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\t\t   /* Delayed ACKs frequently hit locked sockets during bulk\n\t\t    * receive. */\n\t\tif (icsk->icsk_ack.blocked ||\n\t\t    /* Once-per-two-segments ACK was not sent by tcp_input.c */\n\t\t    tp->rcv_nxt - tp->rcv_wup > icsk->icsk_ack.rcv_mss ||\n\t\t    /*\n\t\t     * If this read emptied read buffer, we send ACK, if\n\t\t     * connection is not bidirectional, user drained\n\t\t     * receive buffer and there was a small segment\n\t\t     * in queue.\n\t\t     */\n\t\t    (copied > 0 &&\n\t\t     ((icsk->icsk_ack.pending & ICSK_ACK_PUSHED2) ||\n\t\t      ((icsk->icsk_ack.pending & ICSK_ACK_PUSHED) &&\n\t\t       !icsk->icsk_ack.pingpong)) &&\n\t\t      !atomic_read(&sk->sk_rmem_alloc)))\n\t\t\ttime_to_ack = 1;\n\t}\n\n\t/* We send an ACK if we can now advertise a non-zero window\n\t * which has been raised \"significantly\".\n\t *\n\t * Even if window raised up to infinity, do not send window open ACK\n\t * in states, where we will not receive more. It is useless.\n\t */\n\tif (copied > 0 && !time_to_ack && !(sk->sk_shutdown & RCV_SHUTDOWN)) {\n\t\t__u32 rcv_window_now = tcp_receive_window(tp);\n\n\t\t/* Optimize, __tcp_select_window() is not cheap. */\n\t\tif (2*rcv_window_now <= tp->window_clamp) {\n\t\t\t__u32 new_window = __tcp_select_window(sk);\n\n\t\t\t/* Send ACK now, if this read freed lots of space\n\t\t\t * in our buffer. Certainly, new_window is new window.\n\t\t\t * We can advertise it now, if it is not less than current one.\n\t\t\t * \"Lots\" means \"at least twice\" here.\n\t\t\t */\n\t\t\tif (new_window && new_window >= 2 * rcv_window_now)\n\t\t\t\ttime_to_ack = 1;\n\t\t}\n\t}\n\tif (time_to_ack)\n\t\ttcp_send_ack(sk);\n}\n\nstatic void tcp_prequeue_process(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tNET_INC_STATS_USER(sock_net(sk), LINUX_MIB_TCPPREQUEUED);\n\n\t/* RX process wants to run with disabled BHs, though it is not\n\t * necessary */\n\tlocal_bh_disable();\n\twhile ((skb = __skb_dequeue(&tp->ucopy.prequeue)) != NULL)\n\t\tsk_backlog_rcv(sk, skb);\n\tlocal_bh_enable();\n\n\t/* Clear memory counter. */\n\ttp->ucopy.memory = 0;\n}\n\n#ifdef CONFIG_NET_DMA\nstatic void tcp_service_net_dma(struct sock *sk, bool wait)\n{\n\tdma_cookie_t done, used;\n\tdma_cookie_t last_issued;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!tp->ucopy.dma_chan)\n\t\treturn;\n\n\tlast_issued = tp->ucopy.dma_cookie;\n\tdma_async_memcpy_issue_pending(tp->ucopy.dma_chan);\n\n\tdo {\n\t\tif (dma_async_memcpy_complete(tp->ucopy.dma_chan,\n\t\t\t\t\t      last_issued, &done,\n\t\t\t\t\t      &used) == DMA_SUCCESS) {\n\t\t\t/* Safe to free early-copied skbs now */\n\t\t\t__skb_queue_purge(&sk->sk_async_wait_queue);\n\t\t\tbreak;\n\t\t} else {\n\t\t\tstruct sk_buff *skb;\n\t\t\twhile ((skb = skb_peek(&sk->sk_async_wait_queue)) &&\n\t\t\t       (dma_async_is_complete(skb->dma_cookie, done,\n\t\t\t\t\t\t      used) == DMA_SUCCESS)) {\n\t\t\t\t__skb_dequeue(&sk->sk_async_wait_queue);\n\t\t\t\tkfree_skb(skb);\n\t\t\t}\n\t\t}\n\t} while (wait);\n}\n#endif\n\nstatic inline struct sk_buff *tcp_recv_skb(struct sock *sk, u32 seq, u32 *off)\n{\n\tstruct sk_buff *skb;\n\tu32 offset;\n\n\tskb_queue_walk(&sk->sk_receive_queue, skb) {\n\t\toffset = seq - TCP_SKB_CB(skb)->seq;\n\t\tif (tcp_hdr(skb)->syn)\n\t\t\toffset--;\n\t\tif (offset < skb->len || tcp_hdr(skb)->fin) {\n\t\t\t*off = offset;\n\t\t\treturn skb;\n\t\t}\n\t}\n\treturn NULL;\n}\n\n/*\n * This routine provides an alternative to tcp_recvmsg() for routines\n * that would like to handle copying from skbuffs directly in 'sendfile'\n * fashion.\n * Note:\n *\t- It is assumed that the socket was locked by the caller.\n *\t- The routine does not block.\n *\t- At present, there is no support for reading OOB data\n *\t  or for 'peeking' the socket using this routine\n *\t  (although both would be easy to implement).\n */\nint tcp_read_sock(struct sock *sk, read_descriptor_t *desc,\n\t\t  sk_read_actor_t recv_actor)\n{\n\tstruct sk_buff *skb;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 seq = tp->copied_seq;\n\tu32 offset;\n\tint copied = 0;\n\n\tif (sk->sk_state == TCP_LISTEN)\n\t\treturn -ENOTCONN;\n\twhile ((skb = tcp_recv_skb(sk, seq, &offset)) != NULL) {\n\t\tif (offset < skb->len) {\n\t\t\tint used;\n\t\t\tsize_t len;\n\n\t\t\tlen = skb->len - offset;\n\t\t\t/* Stop reading if we hit a patch of urgent data */\n\t\t\tif (tp->urg_data) {\n\t\t\t\tu32 urg_offset = tp->urg_seq - seq;\n\t\t\t\tif (urg_offset < len)\n\t\t\t\t\tlen = urg_offset;\n\t\t\t\tif (!len)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tused = recv_actor(desc, skb, offset, len);\n\t\t\tif (used < 0) {\n\t\t\t\tif (!copied)\n\t\t\t\t\tcopied = used;\n\t\t\t\tbreak;\n\t\t\t} else if (used <= len) {\n\t\t\t\tseq += used;\n\t\t\t\tcopied += used;\n\t\t\t\toffset += used;\n\t\t\t}\n\t\t\t/*\n\t\t\t * If recv_actor drops the lock (e.g. TCP splice\n\t\t\t * receive) the skb pointer might be invalid when\n\t\t\t * getting here: tcp_collapse might have deleted it\n\t\t\t * while aggregating skbs from the socket queue.\n\t\t\t */\n\t\t\tskb = tcp_recv_skb(sk, seq-1, &offset);\n\t\t\tif (!skb || (offset+1 != skb->len))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (tcp_hdr(skb)->fin) {\n\t\t\tsk_eat_skb(sk, skb, 0);\n\t\t\t++seq;\n\t\t\tbreak;\n\t\t}\n\t\tsk_eat_skb(sk, skb, 0);\n\t\tif (!desc->count)\n\t\t\tbreak;\n\t\ttp->copied_seq = seq;\n\t}\n\ttp->copied_seq = seq;\n\n\ttcp_rcv_space_adjust(sk);\n\n\t/* Clean up data we have read: This will do ACK frames. */\n\tif (copied > 0)\n\t\ttcp_cleanup_rbuf(sk, copied);\n\treturn copied;\n}\n\n/*\n *\tThis routine copies from a sock struct into the user buffer.\n *\n *\tTechnical note: in 2.3 we work on _locked_ socket, so that\n *\ttricks with *seq access order and skb->users are not required.\n *\tProbably, code can be easily improved even more.\n */\n\nint tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\tsize_t len, int nonblock, int flags, int *addr_len)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint copied = 0;\n\tu32 peek_seq;\n\tu32 *seq;\n\tunsigned long used;\n\tint err;\n\tint target;\t\t/* Read at least this many bytes */\n\tlong timeo;\n\tstruct task_struct *user_recv = NULL;\n\tint copied_early = 0;\n\tstruct sk_buff *skb;\n\tu32 urg_hole = 0;\n\n\tlock_sock(sk);\n\n\tTCP_CHECK_TIMER(sk);\n\n\terr = -ENOTCONN;\n\tif (sk->sk_state == TCP_LISTEN)\n\t\tgoto out;\n\n\ttimeo = sock_rcvtimeo(sk, nonblock);\n\n\t/* Urgent data needs to be handled specially. */\n\tif (flags & MSG_OOB)\n\t\tgoto recv_urg;\n\n\tseq = &tp->copied_seq;\n\tif (flags & MSG_PEEK) {\n\t\tpeek_seq = tp->copied_seq;\n\t\tseq = &peek_seq;\n\t}\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\n#ifdef CONFIG_NET_DMA\n\ttp->ucopy.dma_chan = NULL;\n\tpreempt_disable();\n\tskb = skb_peek_tail(&sk->sk_receive_queue);\n\t{\n\t\tint available = 0;\n\n\t\tif (skb)\n\t\t\tavailable = TCP_SKB_CB(skb)->seq + skb->len - (*seq);\n\t\tif ((available < target) &&\n\t\t    (len > sysctl_tcp_dma_copybreak) && !(flags & MSG_PEEK) &&\n\t\t    !sysctl_tcp_low_latency &&\n\t\t    dma_find_channel(DMA_MEMCPY)) {\n\t\t\tpreempt_enable_no_resched();\n\t\t\ttp->ucopy.pinned_list =\n\t\t\t\t\tdma_pin_iovec_pages(msg->msg_iov, len);\n\t\t} else {\n\t\t\tpreempt_enable_no_resched();\n\t\t}\n\t}\n#endif\n\n\tdo {\n\t\tu32 offset;\n\n\t\t/* Are we at urgent data? Stop if we have read anything or have SIGURG pending. */\n\t\tif (tp->urg_data && tp->urg_seq == *seq) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tcopied = timeo ? sock_intr_errno(timeo) : -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t/* Next get a buffer. */\n\n\t\tskb_queue_walk(&sk->sk_receive_queue, skb) {\n\t\t\t/* Now that we have two receive queues this\n\t\t\t * shouldn't happen.\n\t\t\t */\n\t\t\tif (WARN(before(*seq, TCP_SKB_CB(skb)->seq),\n\t\t\t     KERN_INFO \"recvmsg bug: copied %X \"\n\t\t\t\t       \"seq %X rcvnxt %X fl %X\\n\", *seq,\n\t\t\t\t       TCP_SKB_CB(skb)->seq, tp->rcv_nxt,\n\t\t\t\t       flags))\n\t\t\t\tbreak;\n\n\t\t\toffset = *seq - TCP_SKB_CB(skb)->seq;\n\t\t\tif (tcp_hdr(skb)->syn)\n\t\t\t\toffset--;\n\t\t\tif (offset < skb->len)\n\t\t\t\tgoto found_ok_skb;\n\t\t\tif (tcp_hdr(skb)->fin)\n\t\t\t\tgoto found_fin_ok;\n\t\t\tWARN(!(flags & MSG_PEEK), KERN_INFO \"recvmsg bug 2: \"\n\t\t\t\t\t\"copied %X seq %X rcvnxt %X fl %X\\n\",\n\t\t\t\t\t*seq, TCP_SKB_CB(skb)->seq,\n\t\t\t\t\ttp->rcv_nxt, flags);\n\t\t}\n\n\t\t/* Well, if we have backlog, try to process it now yet. */\n\n\t\tif (copied >= target && !sk->sk_backlog.tail)\n\t\t\tbreak;\n\n\t\tif (copied) {\n\t\t\tif (sk->sk_err ||\n\t\t\t    sk->sk_state == TCP_CLOSE ||\n\t\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t\t    !timeo ||\n\t\t\t    signal_pending(current))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_err) {\n\t\t\t\tcopied = sock_error(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state == TCP_CLOSE) {\n\t\t\t\tif (!sock_flag(sk, SOCK_DONE)) {\n\t\t\t\t\t/* This occurs when user tries to read\n\t\t\t\t\t * from never connected socket.\n\t\t\t\t\t */\n\t\t\t\t\tcopied = -ENOTCONN;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (!timeo) {\n\t\t\t\tcopied = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tcopied = sock_intr_errno(timeo);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\ttcp_cleanup_rbuf(sk, copied);\n\n\t\tif (!sysctl_tcp_low_latency && tp->ucopy.task == user_recv) {\n\t\t\t/* Install new reader */\n\t\t\tif (!user_recv && !(flags & (MSG_TRUNC | MSG_PEEK))) {\n\t\t\t\tuser_recv = current;\n\t\t\t\ttp->ucopy.task = user_recv;\n\t\t\t\ttp->ucopy.iov = msg->msg_iov;\n\t\t\t}\n\n\t\t\ttp->ucopy.len = len;\n\n\t\t\tWARN_ON(tp->copied_seq != tp->rcv_nxt &&\n\t\t\t\t!(flags & (MSG_PEEK | MSG_TRUNC)));\n\n\t\t\t/* Ugly... If prequeue is not empty, we have to\n\t\t\t * process it before releasing socket, otherwise\n\t\t\t * order will be broken at second iteration.\n\t\t\t * More elegant solution is required!!!\n\t\t\t *\n\t\t\t * Look: we have the following (pseudo)queues:\n\t\t\t *\n\t\t\t * 1. packets in flight\n\t\t\t * 2. backlog\n\t\t\t * 3. prequeue\n\t\t\t * 4. receive_queue\n\t\t\t *\n\t\t\t * Each queue can be processed only if the next ones\n\t\t\t * are empty. At this point we have empty receive_queue.\n\t\t\t * But prequeue _can_ be not empty after 2nd iteration,\n\t\t\t * when we jumped to start of loop because backlog\n\t\t\t * processing added something to receive_queue.\n\t\t\t * We cannot release_sock(), because backlog contains\n\t\t\t * packets arrived _after_ prequeued ones.\n\t\t\t *\n\t\t\t * Shortly, algorithm is clear --- to process all\n\t\t\t * the queues in order. We could make it more directly,\n\t\t\t * requeueing packets from backlog to prequeue, if\n\t\t\t * is not empty. It is more elegant, but eats cycles,\n\t\t\t * unfortunately.\n\t\t\t */\n\t\t\tif (!skb_queue_empty(&tp->ucopy.prequeue))\n\t\t\t\tgoto do_prequeue;\n\n\t\t\t/* __ Set realtime policy in scheduler __ */\n\t\t}\n\n#ifdef CONFIG_NET_DMA\n\t\tif (tp->ucopy.dma_chan)\n\t\t\tdma_async_memcpy_issue_pending(tp->ucopy.dma_chan);\n#endif\n\t\tif (copied >= target) {\n\t\t\t/* Do not sleep, just process backlog. */\n\t\t\trelease_sock(sk);\n\t\t\tlock_sock(sk);\n\t\t} else\n\t\t\tsk_wait_data(sk, &timeo);\n\n#ifdef CONFIG_NET_DMA\n\t\ttcp_service_net_dma(sk, false);  /* Don't block */\n\t\ttp->ucopy.wakeup = 0;\n#endif\n\n\t\tif (user_recv) {\n\t\t\tint chunk;\n\n\t\t\t/* __ Restore normal policy in scheduler __ */\n\n\t\t\tif ((chunk = len - tp->ucopy.len) != 0) {\n\t\t\t\tNET_ADD_STATS_USER(sock_net(sk), LINUX_MIB_TCPDIRECTCOPYFROMBACKLOG, chunk);\n\t\t\t\tlen -= chunk;\n\t\t\t\tcopied += chunk;\n\t\t\t}\n\n\t\t\tif (tp->rcv_nxt == tp->copied_seq &&\n\t\t\t    !skb_queue_empty(&tp->ucopy.prequeue)) {\ndo_prequeue:\n\t\t\t\ttcp_prequeue_process(sk);\n\n\t\t\t\tif ((chunk = len - tp->ucopy.len) != 0) {\n\t\t\t\t\tNET_ADD_STATS_USER(sock_net(sk), LINUX_MIB_TCPDIRECTCOPYFROMPREQUEUE, chunk);\n\t\t\t\t\tlen -= chunk;\n\t\t\t\t\tcopied += chunk;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif ((flags & MSG_PEEK) &&\n\t\t    (peek_seq - copied - urg_hole != tp->copied_seq)) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tprintk(KERN_DEBUG \"TCP(%s:%d): Application bug, race in MSG_PEEK.\\n\",\n\t\t\t\t       current->comm, task_pid_nr(current));\n\t\t\tpeek_seq = tp->copied_seq;\n\t\t}\n\t\tcontinue;\n\n\tfound_ok_skb:\n\t\t/* Ok so how much can we use? */\n\t\tused = skb->len - offset;\n\t\tif (len < used)\n\t\t\tused = len;\n\n\t\t/* Do we have urgent data here? */\n\t\tif (tp->urg_data) {\n\t\t\tu32 urg_offset = tp->urg_seq - *seq;\n\t\t\tif (urg_offset < used) {\n\t\t\t\tif (!urg_offset) {\n\t\t\t\t\tif (!sock_flag(sk, SOCK_URGINLINE)) {\n\t\t\t\t\t\t++*seq;\n\t\t\t\t\t\turg_hole++;\n\t\t\t\t\t\toffset++;\n\t\t\t\t\t\tused--;\n\t\t\t\t\t\tif (!used)\n\t\t\t\t\t\t\tgoto skip_copy;\n\t\t\t\t\t}\n\t\t\t\t} else\n\t\t\t\t\tused = urg_offset;\n\t\t\t}\n\t\t}\n\n\t\tif (!(flags & MSG_TRUNC)) {\n#ifdef CONFIG_NET_DMA\n\t\t\tif (!tp->ucopy.dma_chan && tp->ucopy.pinned_list)\n\t\t\t\ttp->ucopy.dma_chan = dma_find_channel(DMA_MEMCPY);\n\n\t\t\tif (tp->ucopy.dma_chan) {\n\t\t\t\ttp->ucopy.dma_cookie = dma_skb_copy_datagram_iovec(\n\t\t\t\t\ttp->ucopy.dma_chan, skb, offset,\n\t\t\t\t\tmsg->msg_iov, used,\n\t\t\t\t\ttp->ucopy.pinned_list);\n\n\t\t\t\tif (tp->ucopy.dma_cookie < 0) {\n\n\t\t\t\t\tprintk(KERN_ALERT \"dma_cookie < 0\\n\");\n\n\t\t\t\t\t/* Exception. Bailout! */\n\t\t\t\t\tif (!copied)\n\t\t\t\t\t\tcopied = -EFAULT;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tdma_async_memcpy_issue_pending(tp->ucopy.dma_chan);\n\n\t\t\t\tif ((offset + used) == skb->len)\n\t\t\t\t\tcopied_early = 1;\n\n\t\t\t} else\n#endif\n\t\t\t{\n\t\t\t\terr = skb_copy_datagram_iovec(skb, offset,\n\t\t\t\t\t\tmsg->msg_iov, used);\n\t\t\t\tif (err) {\n\t\t\t\t\t/* Exception. Bailout! */\n\t\t\t\t\tif (!copied)\n\t\t\t\t\t\tcopied = -EFAULT;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t*seq += used;\n\t\tcopied += used;\n\t\tlen -= used;\n\n\t\ttcp_rcv_space_adjust(sk);\n\nskip_copy:\n\t\tif (tp->urg_data && after(tp->copied_seq, tp->urg_seq)) {\n\t\t\ttp->urg_data = 0;\n\t\t\ttcp_fast_path_check(sk);\n\t\t}\n\t\tif (used + offset < skb->len)\n\t\t\tcontinue;\n\n\t\tif (tcp_hdr(skb)->fin)\n\t\t\tgoto found_fin_ok;\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tsk_eat_skb(sk, skb, copied_early);\n\t\t\tcopied_early = 0;\n\t\t}\n\t\tcontinue;\n\n\tfound_fin_ok:\n\t\t/* Process the FIN. */\n\t\t++*seq;\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tsk_eat_skb(sk, skb, copied_early);\n\t\t\tcopied_early = 0;\n\t\t}\n\t\tbreak;\n\t} while (len > 0);\n\n\tif (user_recv) {\n\t\tif (!skb_queue_empty(&tp->ucopy.prequeue)) {\n\t\t\tint chunk;\n\n\t\t\ttp->ucopy.len = copied > 0 ? len : 0;\n\n\t\t\ttcp_prequeue_process(sk);\n\n\t\t\tif (copied > 0 && (chunk = len - tp->ucopy.len) != 0) {\n\t\t\t\tNET_ADD_STATS_USER(sock_net(sk), LINUX_MIB_TCPDIRECTCOPYFROMPREQUEUE, chunk);\n\t\t\t\tlen -= chunk;\n\t\t\t\tcopied += chunk;\n\t\t\t}\n\t\t}\n\n\t\ttp->ucopy.task = NULL;\n\t\ttp->ucopy.len = 0;\n\t}\n\n#ifdef CONFIG_NET_DMA\n\ttcp_service_net_dma(sk, true);  /* Wait for queue to drain */\n\ttp->ucopy.dma_chan = NULL;\n\n\tif (tp->ucopy.pinned_list) {\n\t\tdma_unpin_iovec_pages(tp->ucopy.pinned_list);\n\t\ttp->ucopy.pinned_list = NULL;\n\t}\n#endif\n\n\t/* According to UNIX98, msg_name/msg_namelen are ignored\n\t * on connected socket. I was just happy when found this 8) --ANK\n\t */\n\n\t/* Clean up data we have read: This will do ACK frames. */\n\ttcp_cleanup_rbuf(sk, copied);\n\n\tTCP_CHECK_TIMER(sk);\n\trelease_sock(sk);\n\treturn copied;\n\nout:\n\tTCP_CHECK_TIMER(sk);\n\trelease_sock(sk);\n\treturn err;\n\nrecv_urg:\n\terr = tcp_recv_urg(sk, msg, len, flags);\n\tgoto out;\n}\n\nvoid tcp_set_state(struct sock *sk, int state)\n{\n\tint oldstate = sk->sk_state;\n\n\tswitch (state) {\n\tcase TCP_ESTABLISHED:\n\t\tif (oldstate != TCP_ESTABLISHED)\n\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_CURRESTAB);\n\t\tbreak;\n\n\tcase TCP_CLOSE:\n\t\tif (oldstate == TCP_CLOSE_WAIT || oldstate == TCP_ESTABLISHED)\n\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_ESTABRESETS);\n\n\t\tsk->sk_prot->unhash(sk);\n\t\tif (inet_csk(sk)->icsk_bind_hash &&\n\t\t    !(sk->sk_userlocks & SOCK_BINDPORT_LOCK))\n\t\t\tinet_put_port(sk);\n\t\t/* fall through */\n\tdefault:\n\t\tif (oldstate == TCP_ESTABLISHED)\n\t\t\tTCP_DEC_STATS(sock_net(sk), TCP_MIB_CURRESTAB);\n\t}\n\n\t/* Change state AFTER socket is unhashed to avoid closed\n\t * socket sitting in hash tables.\n\t */\n\tsk->sk_state = state;\n\n#ifdef STATE_TRACE\n\tSOCK_DEBUG(sk, \"TCP sk=%p, State %s -> %s\\n\", sk, statename[oldstate], statename[state]);\n#endif\n}\nEXPORT_SYMBOL_GPL(tcp_set_state);\n\n/*\n *\tState processing on a close. This implements the state shift for\n *\tsending our FIN frame. Note that we only send a FIN for some\n *\tstates. A shutdown() may have already sent the FIN, or we may be\n *\tclosed.\n */\n\nstatic const unsigned char new_state[16] = {\n  /* current state:        new state:      action:\t*/\n  /* (Invalid)\t\t*/ TCP_CLOSE,\n  /* TCP_ESTABLISHED\t*/ TCP_FIN_WAIT1 | TCP_ACTION_FIN,\n  /* TCP_SYN_SENT\t*/ TCP_CLOSE,\n  /* TCP_SYN_RECV\t*/ TCP_FIN_WAIT1 | TCP_ACTION_FIN,\n  /* TCP_FIN_WAIT1\t*/ TCP_FIN_WAIT1,\n  /* TCP_FIN_WAIT2\t*/ TCP_FIN_WAIT2,\n  /* TCP_TIME_WAIT\t*/ TCP_CLOSE,\n  /* TCP_CLOSE\t\t*/ TCP_CLOSE,\n  /* TCP_CLOSE_WAIT\t*/ TCP_LAST_ACK  | TCP_ACTION_FIN,\n  /* TCP_LAST_ACK\t*/ TCP_LAST_ACK,\n  /* TCP_LISTEN\t\t*/ TCP_CLOSE,\n  /* TCP_CLOSING\t*/ TCP_CLOSING,\n};\n\nstatic int tcp_close_state(struct sock *sk)\n{\n\tint next = (int)new_state[sk->sk_state];\n\tint ns = next & TCP_STATE_MASK;\n\n\ttcp_set_state(sk, ns);\n\n\treturn next & TCP_ACTION_FIN;\n}\n\n/*\n *\tShutdown the sending side of a connection. Much like close except\n *\tthat we don't receive shut down or sock_set_flag(sk, SOCK_DEAD).\n */\n\nvoid tcp_shutdown(struct sock *sk, int how)\n{\n\t/*\tWe need to grab some memory, and put together a FIN,\n\t *\tand then put it into the queue to be sent.\n\t *\t\tTim MacKenzie(tym@dibbler.cs.monash.edu.au) 4 Dec '92.\n\t */\n\tif (!(how & SEND_SHUTDOWN))\n\t\treturn;\n\n\t/* If we've already sent a FIN, or it's a closed state, skip this. */\n\tif ((1 << sk->sk_state) &\n\t    (TCPF_ESTABLISHED | TCPF_SYN_SENT |\n\t     TCPF_SYN_RECV | TCPF_CLOSE_WAIT)) {\n\t\t/* Clear out any half completed packets.  FIN if needed. */\n\t\tif (tcp_close_state(sk))\n\t\t\ttcp_send_fin(sk);\n\t}\n}\n\nvoid tcp_close(struct sock *sk, long timeout)\n{\n\tstruct sk_buff *skb;\n\tint data_was_unread = 0;\n\tint state;\n\n\tlock_sock(sk);\n\tsk->sk_shutdown = SHUTDOWN_MASK;\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\n\t\t/* Special case. */\n\t\tinet_csk_listen_stop(sk);\n\n\t\tgoto adjudge_to_death;\n\t}\n\n\t/*  We need to flush the recv. buffs.  We do this only on the\n\t *  descriptor close, not protocol-sourced closes, because the\n\t *  reader process may not have drained the data yet!\n\t */\n\twhile ((skb = __skb_dequeue(&sk->sk_receive_queue)) != NULL) {\n\t\tu32 len = TCP_SKB_CB(skb)->end_seq - TCP_SKB_CB(skb)->seq -\n\t\t\t  tcp_hdr(skb)->fin;\n\t\tdata_was_unread += len;\n\t\t__kfree_skb(skb);\n\t}\n\n\tsk_mem_reclaim(sk);\n\n\t/* As outlined in RFC 2525, section 2.17, we send a RST here because\n\t * data was lost. To witness the awful effects of the old behavior of\n\t * always doing a FIN, run an older 2.1.x kernel or 2.0.x, start a bulk\n\t * GET in an FTP client, suspend the process, wait for the client to\n\t * advertise a zero window, then kill -9 the FTP client, wheee...\n\t * Note: timeout is always zero in such a case.\n\t */\n\tif (data_was_unread) {\n\t\t/* Unread data was tossed, zap the connection. */\n\t\tNET_INC_STATS_USER(sock_net(sk), LINUX_MIB_TCPABORTONCLOSE);\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\ttcp_send_active_reset(sk, sk->sk_allocation);\n\t} else if (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime) {\n\t\t/* Check zero linger _after_ checking for unread data. */\n\t\tsk->sk_prot->disconnect(sk, 0);\n\t\tNET_INC_STATS_USER(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t} else if (tcp_close_state(sk)) {\n\t\t/* We FIN if the application ate all the data before\n\t\t * zapping the connection.\n\t\t */\n\n\t\t/* RED-PEN. Formally speaking, we have broken TCP state\n\t\t * machine. State transitions:\n\t\t *\n\t\t * TCP_ESTABLISHED -> TCP_FIN_WAIT1\n\t\t * TCP_SYN_RECV\t-> TCP_FIN_WAIT1 (forget it, it's impossible)\n\t\t * TCP_CLOSE_WAIT -> TCP_LAST_ACK\n\t\t *\n\t\t * are legal only when FIN has been sent (i.e. in window),\n\t\t * rather than queued out of window. Purists blame.\n\t\t *\n\t\t * F.e. \"RFC state\" is ESTABLISHED,\n\t\t * if Linux state is FIN-WAIT-1, but FIN is still not sent.\n\t\t *\n\t\t * The visible declinations are that sometimes\n\t\t * we enter time-wait state, when it is not required really\n\t\t * (harmless), do not send active resets, when they are\n\t\t * required by specs (TCP_ESTABLISHED, TCP_CLOSE_WAIT, when\n\t\t * they look as CLOSING or LAST_ACK for Linux)\n\t\t * Probably, I missed some more holelets.\n\t\t * \t\t\t\t\t\t--ANK\n\t\t */\n\t\ttcp_send_fin(sk);\n\t}\n\n\tsk_stream_wait_close(sk, timeout);\n\nadjudge_to_death:\n\tstate = sk->sk_state;\n\tsock_hold(sk);\n\tsock_orphan(sk);\n\n\t/* It is the last release_sock in its life. It will remove backlog. */\n\trelease_sock(sk);\n\n\n\t/* Now socket is owned by kernel and we acquire BH lock\n\t   to finish close. No need to check for user refs.\n\t */\n\tlocal_bh_disable();\n\tbh_lock_sock(sk);\n\tWARN_ON(sock_owned_by_user(sk));\n\n\tpercpu_counter_inc(sk->sk_prot->orphan_count);\n\n\t/* Have we already been destroyed by a softirq or backlog? */\n\tif (state != TCP_CLOSE && sk->sk_state == TCP_CLOSE)\n\t\tgoto out;\n\n\t/*\tThis is a (useful) BSD violating of the RFC. There is a\n\t *\tproblem with TCP as specified in that the other end could\n\t *\tkeep a socket open forever with no application left this end.\n\t *\tWe use a 3 minute timeout (about the same as BSD) then kill\n\t *\tour end. If they send after that then tough - BUT: long enough\n\t *\tthat we won't make the old 4*rto = almost no time - whoops\n\t *\treset mistake.\n\t *\n\t *\tNope, it was not mistake. It is really desired behaviour\n\t *\tf.e. on http servers, when such sockets are useless, but\n\t *\tconsume significant resources. Let's do it with special\n\t *\tlinger2\toption.\t\t\t\t\t--ANK\n\t */\n\n\tif (sk->sk_state == TCP_FIN_WAIT2) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tif (tp->linger2 < 0) {\n\t\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\n\t\t\tNET_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_TCPABORTONLINGER);\n\t\t} else {\n\t\t\tconst int tmo = tcp_fin_time(sk);\n\n\t\t\tif (tmo > TCP_TIMEWAIT_LEN) {\n\t\t\t\tinet_csk_reset_keepalive_timer(sk,\n\t\t\t\t\t\ttmo - TCP_TIMEWAIT_LEN);\n\t\t\t} else {\n\t\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\tif (sk->sk_state != TCP_CLOSE) {\n\t\tint orphan_count = percpu_counter_read_positive(\n\t\t\t\t\t\tsk->sk_prot->orphan_count);\n\n\t\tsk_mem_reclaim(sk);\n\t\tif (tcp_too_many_orphans(sk, orphan_count)) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tprintk(KERN_INFO \"TCP: too many of orphaned \"\n\t\t\t\t       \"sockets\\n\");\n\t\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\n\t\t\tNET_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_TCPABORTONMEMORY);\n\t\t}\n\t}\n\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tinet_csk_destroy_sock(sk);\n\t/* Otherwise, socket is reprieved until protocol close. */\n\nout:\n\tbh_unlock_sock(sk);\n\tlocal_bh_enable();\n\tsock_put(sk);\n}\n\n/* These states need RST on ABORT according to RFC793 */\n\nstatic inline int tcp_need_reset(int state)\n{\n\treturn (1 << state) &\n\t       (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT | TCPF_FIN_WAIT1 |\n\t\tTCPF_FIN_WAIT2 | TCPF_SYN_RECV);\n}\n\nint tcp_disconnect(struct sock *sk, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint err = 0;\n\tint old_state = sk->sk_state;\n\n\tif (old_state != TCP_CLOSE)\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\n\t/* ABORT function of RFC793 */\n\tif (old_state == TCP_LISTEN) {\n\t\tinet_csk_listen_stop(sk);\n\t} else if (tcp_need_reset(old_state) ||\n\t\t   (tp->snd_nxt != tp->write_seq &&\n\t\t    (1 << old_state) & (TCPF_CLOSING | TCPF_LAST_ACK))) {\n\t\t/* The last check adjusts for discrepancy of Linux wrt. RFC\n\t\t * states\n\t\t */\n\t\ttcp_send_active_reset(sk, gfp_any());\n\t\tsk->sk_err = ECONNRESET;\n\t} else if (old_state == TCP_SYN_SENT)\n\t\tsk->sk_err = ECONNRESET;\n\n\ttcp_clear_xmit_timers(sk);\n\t__skb_queue_purge(&sk->sk_receive_queue);\n\ttcp_write_queue_purge(sk);\n\t__skb_queue_purge(&tp->out_of_order_queue);\n#ifdef CONFIG_NET_DMA\n\t__skb_queue_purge(&sk->sk_async_wait_queue);\n#endif\n\n\tinet->inet_dport = 0;\n\n\tif (!(sk->sk_userlocks & SOCK_BINDADDR_LOCK))\n\t\tinet_reset_saddr(sk);\n\n\tsk->sk_shutdown = 0;\n\tsock_reset_flag(sk, SOCK_DONE);\n\ttp->srtt = 0;\n\tif ((tp->write_seq += tp->max_window + 2) == 0)\n\t\ttp->write_seq = 1;\n\ticsk->icsk_backoff = 0;\n\ttp->snd_cwnd = 2;\n\ticsk->icsk_probes_out = 0;\n\ttp->packets_out = 0;\n\ttp->snd_ssthresh = TCP_INFINITE_SSTHRESH;\n\ttp->snd_cwnd_cnt = 0;\n\ttp->bytes_acked = 0;\n\ttp->window_clamp = 0;\n\ttcp_set_ca_state(sk, TCP_CA_Open);\n\ttcp_clear_retrans(tp);\n\tinet_csk_delack_init(sk);\n\ttcp_init_send_head(sk);\n\tmemset(&tp->rx_opt, 0, sizeof(tp->rx_opt));\n\t__sk_dst_reset(sk);\n\n\tWARN_ON(inet->inet_num && !icsk->icsk_bind_hash);\n\n\tsk->sk_error_report(sk);\n\treturn err;\n}\n\n/*\n *\tSocket option code for TCP.\n */\nstatic int do_tcp_setsockopt(struct sock *sk, int level,\n\t\tint optname, char __user *optval, unsigned int optlen)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tint val;\n\tint err = 0;\n\n\t/* These are data/string values, all the others are ints */\n\tswitch (optname) {\n\tcase TCP_CONGESTION: {\n\t\tchar name[TCP_CA_NAME_MAX];\n\n\t\tif (optlen < 1)\n\t\t\treturn -EINVAL;\n\n\t\tval = strncpy_from_user(name, optval,\n\t\t\t\t\tmin_t(long, TCP_CA_NAME_MAX-1, optlen));\n\t\tif (val < 0)\n\t\t\treturn -EFAULT;\n\t\tname[val] = 0;\n\n\t\tlock_sock(sk);\n\t\terr = tcp_set_congestion_control(sk, name);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\tcase TCP_COOKIE_TRANSACTIONS: {\n\t\tstruct tcp_cookie_transactions ctd;\n\t\tstruct tcp_cookie_values *cvp = NULL;\n\n\t\tif (sizeof(ctd) > optlen)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&ctd, optval, sizeof(ctd)))\n\t\t\treturn -EFAULT;\n\n\t\tif (ctd.tcpct_used > sizeof(ctd.tcpct_value) ||\n\t\t    ctd.tcpct_s_data_desired > TCP_MSS_DESIRED)\n\t\t\treturn -EINVAL;\n\n\t\tif (ctd.tcpct_cookie_desired == 0) {\n\t\t\t/* default to global value */\n\t\t} else if ((0x1 & ctd.tcpct_cookie_desired) ||\n\t\t\t   ctd.tcpct_cookie_desired > TCP_COOKIE_MAX ||\n\t\t\t   ctd.tcpct_cookie_desired < TCP_COOKIE_MIN) {\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (TCP_COOKIE_OUT_NEVER & ctd.tcpct_flags) {\n\t\t\t/* Supercedes all other values */\n\t\t\tlock_sock(sk);\n\t\t\tif (tp->cookie_values != NULL) {\n\t\t\t\tkref_put(&tp->cookie_values->kref,\n\t\t\t\t\t tcp_cookie_values_release);\n\t\t\t\ttp->cookie_values = NULL;\n\t\t\t}\n\t\t\ttp->rx_opt.cookie_in_always = 0; /* false */\n\t\t\ttp->rx_opt.cookie_out_never = 1; /* true */\n\t\t\trelease_sock(sk);\n\t\t\treturn err;\n\t\t}\n\n\t\t/* Allocate ancillary memory before locking.\n\t\t */\n\t\tif (ctd.tcpct_used > 0 ||\n\t\t    (tp->cookie_values == NULL &&\n\t\t     (sysctl_tcp_cookie_size > 0 ||\n\t\t      ctd.tcpct_cookie_desired > 0 ||\n\t\t      ctd.tcpct_s_data_desired > 0))) {\n\t\t\tcvp = kzalloc(sizeof(*cvp) + ctd.tcpct_used,\n\t\t\t\t      GFP_KERNEL);\n\t\t\tif (cvp == NULL)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\t\tlock_sock(sk);\n\t\ttp->rx_opt.cookie_in_always =\n\t\t\t(TCP_COOKIE_IN_ALWAYS & ctd.tcpct_flags);\n\t\ttp->rx_opt.cookie_out_never = 0; /* false */\n\n\t\tif (tp->cookie_values != NULL) {\n\t\t\tif (cvp != NULL) {\n\t\t\t\t/* Changed values are recorded by a changed\n\t\t\t\t * pointer, ensuring the cookie will differ,\n\t\t\t\t * without separately hashing each value later.\n\t\t\t\t */\n\t\t\t\tkref_put(&tp->cookie_values->kref,\n\t\t\t\t\t tcp_cookie_values_release);\n\t\t\t\tkref_init(&cvp->kref);\n\t\t\t\ttp->cookie_values = cvp;\n\t\t\t} else {\n\t\t\t\tcvp = tp->cookie_values;\n\t\t\t}\n\t\t}\n\t\tif (cvp != NULL) {\n\t\t\tcvp->cookie_desired = ctd.tcpct_cookie_desired;\n\n\t\t\tif (ctd.tcpct_used > 0) {\n\t\t\t\tmemcpy(cvp->s_data_payload, ctd.tcpct_value,\n\t\t\t\t       ctd.tcpct_used);\n\t\t\t\tcvp->s_data_desired = ctd.tcpct_used;\n\t\t\t\tcvp->s_data_constant = 1; /* true */\n\t\t\t} else {\n\t\t\t\t/* No constant payload data. */\n\t\t\t\tcvp->s_data_desired = ctd.tcpct_s_data_desired;\n\t\t\t\tcvp->s_data_constant = 0; /* false */\n\t\t\t}\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\tdefault:\n\t\t/* fallthru */\n\t\tbreak;\n\t};\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase TCP_MAXSEG:\n\t\t/* Values greater than interface MTU won't take effect. However\n\t\t * at the point when this call is done we typically don't yet\n\t\t * know which interface is going to be used */\n\t\tif (val < 8 || val > MAX_TCP_WINDOW) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\ttp->rx_opt.user_mss = val;\n\t\tbreak;\n\n\tcase TCP_NODELAY:\n\t\tif (val) {\n\t\t\t/* TCP_NODELAY is weaker than TCP_CORK, so that\n\t\t\t * this option on corked socket is remembered, but\n\t\t\t * it is not activated until cork is cleared.\n\t\t\t *\n\t\t\t * However, when TCP_NODELAY is set we make\n\t\t\t * an explicit push, which overrides even TCP_CORK\n\t\t\t * for currently queued segments.\n\t\t\t */\n\t\t\ttp->nonagle |= TCP_NAGLE_OFF|TCP_NAGLE_PUSH;\n\t\t\ttcp_push_pending_frames(sk);\n\t\t} else {\n\t\t\ttp->nonagle &= ~TCP_NAGLE_OFF;\n\t\t}\n\t\tbreak;\n\n\tcase TCP_THIN_LINEAR_TIMEOUTS:\n\t\tif (val < 0 || val > 1)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->thin_lto = val;\n\t\tbreak;\n\n\tcase TCP_THIN_DUPACK:\n\t\tif (val < 0 || val > 1)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->thin_dupack = val;\n\t\tbreak;\n\n\tcase TCP_CORK:\n\t\t/* When set indicates to always queue non-full frames.\n\t\t * Later the user clears this option and we transmit\n\t\t * any pending partial frames in the queue.  This is\n\t\t * meant to be used alongside sendfile() to get properly\n\t\t * filled frames when the user (for example) must write\n\t\t * out headers with a write() call first and then use\n\t\t * sendfile to send out the data parts.\n\t\t *\n\t\t * TCP_CORK can be set together with TCP_NODELAY and it is\n\t\t * stronger than TCP_NODELAY.\n\t\t */\n\t\tif (val) {\n\t\t\ttp->nonagle |= TCP_NAGLE_CORK;\n\t\t} else {\n\t\t\ttp->nonagle &= ~TCP_NAGLE_CORK;\n\t\t\tif (tp->nonagle&TCP_NAGLE_OFF)\n\t\t\t\ttp->nonagle |= TCP_NAGLE_PUSH;\n\t\t\ttcp_push_pending_frames(sk);\n\t\t}\n\t\tbreak;\n\n\tcase TCP_KEEPIDLE:\n\t\tif (val < 1 || val > MAX_TCP_KEEPIDLE)\n\t\t\terr = -EINVAL;\n\t\telse {\n\t\t\ttp->keepalive_time = val * HZ;\n\t\t\tif (sock_flag(sk, SOCK_KEEPOPEN) &&\n\t\t\t    !((1 << sk->sk_state) &\n\t\t\t      (TCPF_CLOSE | TCPF_LISTEN))) {\n\t\t\t\t__u32 elapsed = tcp_time_stamp - tp->rcv_tstamp;\n\t\t\t\tif (tp->keepalive_time > elapsed)\n\t\t\t\t\telapsed = tp->keepalive_time - elapsed;\n\t\t\t\telse\n\t\t\t\t\telapsed = 0;\n\t\t\t\tinet_csk_reset_keepalive_timer(sk, elapsed);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase TCP_KEEPINTVL:\n\t\tif (val < 1 || val > MAX_TCP_KEEPINTVL)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->keepalive_intvl = val * HZ;\n\t\tbreak;\n\tcase TCP_KEEPCNT:\n\t\tif (val < 1 || val > MAX_TCP_KEEPCNT)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->keepalive_probes = val;\n\t\tbreak;\n\tcase TCP_SYNCNT:\n\t\tif (val < 1 || val > MAX_TCP_SYNCNT)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ticsk->icsk_syn_retries = val;\n\t\tbreak;\n\n\tcase TCP_LINGER2:\n\t\tif (val < 0)\n\t\t\ttp->linger2 = -1;\n\t\telse if (val > sysctl_tcp_fin_timeout / HZ)\n\t\t\ttp->linger2 = 0;\n\t\telse\n\t\t\ttp->linger2 = val * HZ;\n\t\tbreak;\n\n\tcase TCP_DEFER_ACCEPT:\n\t\t/* Translate value in seconds to number of retransmits */\n\t\ticsk->icsk_accept_queue.rskq_defer_accept =\n\t\t\tsecs_to_retrans(val, TCP_TIMEOUT_INIT / HZ,\n\t\t\t\t\tTCP_RTO_MAX / HZ);\n\t\tbreak;\n\n\tcase TCP_WINDOW_CLAMP:\n\t\tif (!val) {\n\t\t\tif (sk->sk_state != TCP_CLOSE) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttp->window_clamp = 0;\n\t\t} else\n\t\t\ttp->window_clamp = val < SOCK_MIN_RCVBUF / 2 ?\n\t\t\t\t\t\tSOCK_MIN_RCVBUF / 2 : val;\n\t\tbreak;\n\n\tcase TCP_QUICKACK:\n\t\tif (!val) {\n\t\t\ticsk->icsk_ack.pingpong = 1;\n\t\t} else {\n\t\t\ticsk->icsk_ack.pingpong = 0;\n\t\t\tif ((1 << sk->sk_state) &\n\t\t\t    (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT) &&\n\t\t\t    inet_csk_ack_scheduled(sk)) {\n\t\t\t\ticsk->icsk_ack.pending |= ICSK_ACK_PUSHED;\n\t\t\t\ttcp_cleanup_rbuf(sk, 1);\n\t\t\t\tif (!(val & 1))\n\t\t\t\t\ticsk->icsk_ack.pingpong = 1;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n#ifdef CONFIG_TCP_MD5SIG\n\tcase TCP_MD5SIG:\n\t\t/* Read the IP->Key mappings from userspace */\n\t\terr = tp->af_specific->md5_parse(sk, optval, optlen);\n\t\tbreak;\n#endif\n\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\treturn err;\n}\n\nint tcp_setsockopt(struct sock *sk, int level, int optname, char __user *optval,\n\t\t   unsigned int optlen)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\treturn icsk->icsk_af_ops->setsockopt(sk, level, optname,\n\t\t\t\t\t\t     optval, optlen);\n\treturn do_tcp_setsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_tcp_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, unsigned int optlen)\n{\n\tif (level != SOL_TCP)\n\t\treturn inet_csk_compat_setsockopt(sk, level, optname,\n\t\t\t\t\t\t  optval, optlen);\n\treturn do_tcp_setsockopt(sk, level, optname, optval, optlen);\n}\n\nEXPORT_SYMBOL(compat_tcp_setsockopt);\n#endif\n\n/* Return information about state of tcp endpoint in API format. */\nvoid tcp_get_info(struct sock *sk, struct tcp_info *info)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tu32 now = tcp_time_stamp;\n\n\tmemset(info, 0, sizeof(*info));\n\n\tinfo->tcpi_state = sk->sk_state;\n\tinfo->tcpi_ca_state = icsk->icsk_ca_state;\n\tinfo->tcpi_retransmits = icsk->icsk_retransmits;\n\tinfo->tcpi_probes = icsk->icsk_probes_out;\n\tinfo->tcpi_backoff = icsk->icsk_backoff;\n\n\tif (tp->rx_opt.tstamp_ok)\n\t\tinfo->tcpi_options |= TCPI_OPT_TIMESTAMPS;\n\tif (tcp_is_sack(tp))\n\t\tinfo->tcpi_options |= TCPI_OPT_SACK;\n\tif (tp->rx_opt.wscale_ok) {\n\t\tinfo->tcpi_options |= TCPI_OPT_WSCALE;\n\t\tinfo->tcpi_snd_wscale = tp->rx_opt.snd_wscale;\n\t\tinfo->tcpi_rcv_wscale = tp->rx_opt.rcv_wscale;\n\t}\n\n\tif (tp->ecn_flags&TCP_ECN_OK)\n\t\tinfo->tcpi_options |= TCPI_OPT_ECN;\n\n\tinfo->tcpi_rto = jiffies_to_usecs(icsk->icsk_rto);\n\tinfo->tcpi_ato = jiffies_to_usecs(icsk->icsk_ack.ato);\n\tinfo->tcpi_snd_mss = tp->mss_cache;\n\tinfo->tcpi_rcv_mss = icsk->icsk_ack.rcv_mss;\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\tinfo->tcpi_unacked = sk->sk_ack_backlog;\n\t\tinfo->tcpi_sacked = sk->sk_max_ack_backlog;\n\t} else {\n\t\tinfo->tcpi_unacked = tp->packets_out;\n\t\tinfo->tcpi_sacked = tp->sacked_out;\n\t}\n\tinfo->tcpi_lost = tp->lost_out;\n\tinfo->tcpi_retrans = tp->retrans_out;\n\tinfo->tcpi_fackets = tp->fackets_out;\n\n\tinfo->tcpi_last_data_sent = jiffies_to_msecs(now - tp->lsndtime);\n\tinfo->tcpi_last_data_recv = jiffies_to_msecs(now - icsk->icsk_ack.lrcvtime);\n\tinfo->tcpi_last_ack_recv = jiffies_to_msecs(now - tp->rcv_tstamp);\n\n\tinfo->tcpi_pmtu = icsk->icsk_pmtu_cookie;\n\tinfo->tcpi_rcv_ssthresh = tp->rcv_ssthresh;\n\tinfo->tcpi_rtt = jiffies_to_usecs(tp->srtt)>>3;\n\tinfo->tcpi_rttvar = jiffies_to_usecs(tp->mdev)>>2;\n\tinfo->tcpi_snd_ssthresh = tp->snd_ssthresh;\n\tinfo->tcpi_snd_cwnd = tp->snd_cwnd;\n\tinfo->tcpi_advmss = tp->advmss;\n\tinfo->tcpi_reordering = tp->reordering;\n\n\tinfo->tcpi_rcv_rtt = jiffies_to_usecs(tp->rcv_rtt_est.rtt)>>3;\n\tinfo->tcpi_rcv_space = tp->rcvq_space.space;\n\n\tinfo->tcpi_total_retrans = tp->total_retrans;\n}\n\nEXPORT_SYMBOL_GPL(tcp_get_info);\n\nstatic int do_tcp_getsockopt(struct sock *sk, int level,\n\t\tint optname, char __user *optval, int __user *optlen)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint val, len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tlen = min_t(unsigned int, len, sizeof(int));\n\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase TCP_MAXSEG:\n\t\tval = tp->mss_cache;\n\t\tif (!val && ((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_LISTEN)))\n\t\t\tval = tp->rx_opt.user_mss;\n\t\tbreak;\n\tcase TCP_NODELAY:\n\t\tval = !!(tp->nonagle&TCP_NAGLE_OFF);\n\t\tbreak;\n\tcase TCP_CORK:\n\t\tval = !!(tp->nonagle&TCP_NAGLE_CORK);\n\t\tbreak;\n\tcase TCP_KEEPIDLE:\n\t\tval = keepalive_time_when(tp) / HZ;\n\t\tbreak;\n\tcase TCP_KEEPINTVL:\n\t\tval = keepalive_intvl_when(tp) / HZ;\n\t\tbreak;\n\tcase TCP_KEEPCNT:\n\t\tval = keepalive_probes(tp);\n\t\tbreak;\n\tcase TCP_SYNCNT:\n\t\tval = icsk->icsk_syn_retries ? : sysctl_tcp_syn_retries;\n\t\tbreak;\n\tcase TCP_LINGER2:\n\t\tval = tp->linger2;\n\t\tif (val >= 0)\n\t\t\tval = (val ? : sysctl_tcp_fin_timeout) / HZ;\n\t\tbreak;\n\tcase TCP_DEFER_ACCEPT:\n\t\tval = retrans_to_secs(icsk->icsk_accept_queue.rskq_defer_accept,\n\t\t\t\t      TCP_TIMEOUT_INIT / HZ, TCP_RTO_MAX / HZ);\n\t\tbreak;\n\tcase TCP_WINDOW_CLAMP:\n\t\tval = tp->window_clamp;\n\t\tbreak;\n\tcase TCP_INFO: {\n\t\tstruct tcp_info info;\n\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\ttcp_get_info(sk, &info);\n\n\t\tlen = min_t(unsigned int, len, sizeof(info));\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &info, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase TCP_QUICKACK:\n\t\tval = !icsk->icsk_ack.pingpong;\n\t\tbreak;\n\n\tcase TCP_CONGESTION:\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tlen = min_t(unsigned int, len, TCP_CA_NAME_MAX);\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, icsk->icsk_ca_ops->name, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\n\tcase TCP_COOKIE_TRANSACTIONS: {\n\t\tstruct tcp_cookie_transactions ctd;\n\t\tstruct tcp_cookie_values *cvp = tp->cookie_values;\n\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (len < sizeof(ctd))\n\t\t\treturn -EINVAL;\n\n\t\tmemset(&ctd, 0, sizeof(ctd));\n\t\tctd.tcpct_flags = (tp->rx_opt.cookie_in_always ?\n\t\t\t\t   TCP_COOKIE_IN_ALWAYS : 0)\n\t\t\t\t| (tp->rx_opt.cookie_out_never ?\n\t\t\t\t   TCP_COOKIE_OUT_NEVER : 0);\n\n\t\tif (cvp != NULL) {\n\t\t\tctd.tcpct_flags |= (cvp->s_data_in ?\n\t\t\t\t\t    TCP_S_DATA_IN : 0)\n\t\t\t\t\t | (cvp->s_data_out ?\n\t\t\t\t\t    TCP_S_DATA_OUT : 0);\n\n\t\t\tctd.tcpct_cookie_desired = cvp->cookie_desired;\n\t\t\tctd.tcpct_s_data_desired = cvp->s_data_desired;\n\n\t\t\tmemcpy(&ctd.tcpct_value[0], &cvp->cookie_pair[0],\n\t\t\t       cvp->cookie_pair_size);\n\t\t\tctd.tcpct_used = cvp->cookie_pair_size;\n\t\t}\n\n\t\tif (put_user(sizeof(ctd), optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &ctd, sizeof(ctd)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nint tcp_getsockopt(struct sock *sk, int level, int optname, char __user *optval,\n\t\t   int __user *optlen)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\treturn icsk->icsk_af_ops->getsockopt(sk, level, optname,\n\t\t\t\t\t\t     optval, optlen);\n\treturn do_tcp_getsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_tcp_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, int __user *optlen)\n{\n\tif (level != SOL_TCP)\n\t\treturn inet_csk_compat_getsockopt(sk, level, optname,\n\t\t\t\t\t\t  optval, optlen);\n\treturn do_tcp_getsockopt(sk, level, optname, optval, optlen);\n}\n\nEXPORT_SYMBOL(compat_tcp_getsockopt);\n#endif\n\nstruct sk_buff *tcp_tso_segment(struct sk_buff *skb, int features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tstruct tcphdr *th;\n\tunsigned thlen;\n\tunsigned int seq;\n\t__be32 delta;\n\tunsigned int oldlen;\n\tunsigned int mss;\n\n\tif (!pskb_may_pull(skb, sizeof(*th)))\n\t\tgoto out;\n\n\tth = tcp_hdr(skb);\n\tthlen = th->doff * 4;\n\tif (thlen < sizeof(*th))\n\t\tgoto out;\n\n\tif (!pskb_may_pull(skb, thlen))\n\t\tgoto out;\n\n\toldlen = (u16)~skb->len;\n\t__skb_pull(skb, thlen);\n\n\tmss = skb_shinfo(skb)->gso_size;\n\tif (unlikely(skb->len <= mss))\n\t\tgoto out;\n\n\tif (skb_gso_ok(skb, features | NETIF_F_GSO_ROBUST)) {\n\t\t/* Packet is from an untrusted source, reset gso_segs. */\n\t\tint type = skb_shinfo(skb)->gso_type;\n\n\t\tif (unlikely(type &\n\t\t\t     ~(SKB_GSO_TCPV4 |\n\t\t\t       SKB_GSO_DODGY |\n\t\t\t       SKB_GSO_TCP_ECN |\n\t\t\t       SKB_GSO_TCPV6 |\n\t\t\t       0) ||\n\t\t\t     !(type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6))))\n\t\t\tgoto out;\n\n\t\tskb_shinfo(skb)->gso_segs = DIV_ROUND_UP(skb->len, mss);\n\n\t\tsegs = NULL;\n\t\tgoto out;\n\t}\n\n\tsegs = skb_segment(skb, features);\n\tif (IS_ERR(segs))\n\t\tgoto out;\n\n\tdelta = htonl(oldlen + (thlen + mss));\n\n\tskb = segs;\n\tth = tcp_hdr(skb);\n\tseq = ntohl(th->seq);\n\n\tdo {\n\t\tth->fin = th->psh = 0;\n\n\t\tth->check = ~csum_fold((__force __wsum)((__force u32)th->check +\n\t\t\t\t       (__force u32)delta));\n\t\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\t\tth->check =\n\t\t\t     csum_fold(csum_partial(skb_transport_header(skb),\n\t\t\t\t\t\t    thlen, skb->csum));\n\n\t\tseq += mss;\n\t\tskb = skb->next;\n\t\tth = tcp_hdr(skb);\n\n\t\tth->seq = htonl(seq);\n\t\tth->cwr = 0;\n\t} while (skb->next);\n\n\tdelta = htonl(oldlen + (skb->tail - skb->transport_header) +\n\t\t      skb->data_len);\n\tth->check = ~csum_fold((__force __wsum)((__force u32)th->check +\n\t\t\t\t(__force u32)delta));\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\tth->check = csum_fold(csum_partial(skb_transport_header(skb),\n\t\t\t\t\t\t   thlen, skb->csum));\n\nout:\n\treturn segs;\n}\nEXPORT_SYMBOL(tcp_tso_segment);\n\nstruct sk_buff **tcp_gro_receive(struct sk_buff **head, struct sk_buff *skb)\n{\n\tstruct sk_buff **pp = NULL;\n\tstruct sk_buff *p;\n\tstruct tcphdr *th;\n\tstruct tcphdr *th2;\n\tunsigned int len;\n\tunsigned int thlen;\n\tunsigned int flags;\n\tunsigned int mss = 1;\n\tunsigned int hlen;\n\tunsigned int off;\n\tint flush = 1;\n\tint i;\n\n\toff = skb_gro_offset(skb);\n\thlen = off + sizeof(*th);\n\tth = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\tth = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!th))\n\t\t\tgoto out;\n\t}\n\n\tthlen = th->doff * 4;\n\tif (thlen < sizeof(*th))\n\t\tgoto out;\n\n\thlen = off + thlen;\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\tth = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!th))\n\t\t\tgoto out;\n\t}\n\n\tskb_gro_pull(skb, thlen);\n\n\tlen = skb_gro_len(skb);\n\tflags = tcp_flag_word(th);\n\n\tfor (; (p = *head); head = &p->next) {\n\t\tif (!NAPI_GRO_CB(p)->same_flow)\n\t\t\tcontinue;\n\n\t\tth2 = tcp_hdr(p);\n\n\t\tif (*(u32 *)&th->source ^ *(u32 *)&th2->source) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tgoto found;\n\t}\n\n\tgoto out_check_final;\n\nfound:\n\tflush = NAPI_GRO_CB(p)->flush;\n\tflush |= flags & TCP_FLAG_CWR;\n\tflush |= (flags ^ tcp_flag_word(th2)) &\n\t\t  ~(TCP_FLAG_CWR | TCP_FLAG_FIN | TCP_FLAG_PSH);\n\tflush |= th->ack_seq ^ th2->ack_seq;\n\tfor (i = sizeof(*th); i < thlen; i += 4)\n\t\tflush |= *(u32 *)((u8 *)th + i) ^\n\t\t\t *(u32 *)((u8 *)th2 + i);\n\n\tmss = skb_shinfo(p)->gso_size;\n\n\tflush |= (len - 1) >= mss;\n\tflush |= (ntohl(th2->seq) + skb_gro_len(p)) ^ ntohl(th->seq);\n\n\tif (flush || skb_gro_receive(head, skb)) {\n\t\tmss = 1;\n\t\tgoto out_check_final;\n\t}\n\n\tp = *head;\n\tth2 = tcp_hdr(p);\n\ttcp_flag_word(th2) |= flags & (TCP_FLAG_FIN | TCP_FLAG_PSH);\n\nout_check_final:\n\tflush = len < mss;\n\tflush |= flags & (TCP_FLAG_URG | TCP_FLAG_PSH | TCP_FLAG_RST |\n\t\t\t  TCP_FLAG_SYN | TCP_FLAG_FIN);\n\n\tif (p && (!NAPI_GRO_CB(skb)->same_flow || flush))\n\t\tpp = head;\n\nout:\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\n\treturn pp;\n}\nEXPORT_SYMBOL(tcp_gro_receive);\n\nint tcp_gro_complete(struct sk_buff *skb)\n{\n\tstruct tcphdr *th = tcp_hdr(skb);\n\n\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\tskb->csum_offset = offsetof(struct tcphdr, check);\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\tskb_shinfo(skb)->gso_segs = NAPI_GRO_CB(skb)->count;\n\n\tif (th->cwr)\n\t\tskb_shinfo(skb)->gso_type |= SKB_GSO_TCP_ECN;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_gro_complete);\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic unsigned long tcp_md5sig_users;\nstatic struct tcp_md5sig_pool * __percpu *tcp_md5sig_pool;\nstatic DEFINE_SPINLOCK(tcp_md5sig_pool_lock);\n\nstatic void __tcp_free_md5sig_pool(struct tcp_md5sig_pool * __percpu *pool)\n{\n\tint cpu;\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct tcp_md5sig_pool *p = *per_cpu_ptr(pool, cpu);\n\t\tif (p) {\n\t\t\tif (p->md5_desc.tfm)\n\t\t\t\tcrypto_free_hash(p->md5_desc.tfm);\n\t\t\tkfree(p);\n\t\t\tp = NULL;\n\t\t}\n\t}\n\tfree_percpu(pool);\n}\n\nvoid tcp_free_md5sig_pool(void)\n{\n\tstruct tcp_md5sig_pool * __percpu *pool = NULL;\n\n\tspin_lock_bh(&tcp_md5sig_pool_lock);\n\tif (--tcp_md5sig_users == 0) {\n\t\tpool = tcp_md5sig_pool;\n\t\ttcp_md5sig_pool = NULL;\n\t}\n\tspin_unlock_bh(&tcp_md5sig_pool_lock);\n\tif (pool)\n\t\t__tcp_free_md5sig_pool(pool);\n}\n\nEXPORT_SYMBOL(tcp_free_md5sig_pool);\n\nstatic struct tcp_md5sig_pool * __percpu *\n__tcp_alloc_md5sig_pool(struct sock *sk)\n{\n\tint cpu;\n\tstruct tcp_md5sig_pool * __percpu *pool;\n\n\tpool = alloc_percpu(struct tcp_md5sig_pool *);\n\tif (!pool)\n\t\treturn NULL;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct tcp_md5sig_pool *p;\n\t\tstruct crypto_hash *hash;\n\n\t\tp = kzalloc(sizeof(*p), sk->sk_allocation);\n\t\tif (!p)\n\t\t\tgoto out_free;\n\t\t*per_cpu_ptr(pool, cpu) = p;\n\n\t\thash = crypto_alloc_hash(\"md5\", 0, CRYPTO_ALG_ASYNC);\n\t\tif (!hash || IS_ERR(hash))\n\t\t\tgoto out_free;\n\n\t\tp->md5_desc.tfm = hash;\n\t}\n\treturn pool;\nout_free:\n\t__tcp_free_md5sig_pool(pool);\n\treturn NULL;\n}\n\nstruct tcp_md5sig_pool * __percpu *tcp_alloc_md5sig_pool(struct sock *sk)\n{\n\tstruct tcp_md5sig_pool * __percpu *pool;\n\tint alloc = 0;\n\nretry:\n\tspin_lock_bh(&tcp_md5sig_pool_lock);\n\tpool = tcp_md5sig_pool;\n\tif (tcp_md5sig_users++ == 0) {\n\t\talloc = 1;\n\t\tspin_unlock_bh(&tcp_md5sig_pool_lock);\n\t} else if (!pool) {\n\t\ttcp_md5sig_users--;\n\t\tspin_unlock_bh(&tcp_md5sig_pool_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t} else\n\t\tspin_unlock_bh(&tcp_md5sig_pool_lock);\n\n\tif (alloc) {\n\t\t/* we cannot hold spinlock here because this may sleep. */\n\t\tstruct tcp_md5sig_pool * __percpu *p;\n\n\t\tp = __tcp_alloc_md5sig_pool(sk);\n\t\tspin_lock_bh(&tcp_md5sig_pool_lock);\n\t\tif (!p) {\n\t\t\ttcp_md5sig_users--;\n\t\t\tspin_unlock_bh(&tcp_md5sig_pool_lock);\n\t\t\treturn NULL;\n\t\t}\n\t\tpool = tcp_md5sig_pool;\n\t\tif (pool) {\n\t\t\t/* oops, it has already been assigned. */\n\t\t\tspin_unlock_bh(&tcp_md5sig_pool_lock);\n\t\t\t__tcp_free_md5sig_pool(p);\n\t\t} else {\n\t\t\ttcp_md5sig_pool = pool = p;\n\t\t\tspin_unlock_bh(&tcp_md5sig_pool_lock);\n\t\t}\n\t}\n\treturn pool;\n}\n\nEXPORT_SYMBOL(tcp_alloc_md5sig_pool);\n\nstruct tcp_md5sig_pool *__tcp_get_md5sig_pool(int cpu)\n{\n\tstruct tcp_md5sig_pool * __percpu *p;\n\tspin_lock_bh(&tcp_md5sig_pool_lock);\n\tp = tcp_md5sig_pool;\n\tif (p)\n\t\ttcp_md5sig_users++;\n\tspin_unlock_bh(&tcp_md5sig_pool_lock);\n\treturn (p ? *per_cpu_ptr(p, cpu) : NULL);\n}\n\nEXPORT_SYMBOL(__tcp_get_md5sig_pool);\n\nvoid __tcp_put_md5sig_pool(void)\n{\n\ttcp_free_md5sig_pool();\n}\n\nEXPORT_SYMBOL(__tcp_put_md5sig_pool);\n\nint tcp_md5_hash_header(struct tcp_md5sig_pool *hp,\n\t\t\tstruct tcphdr *th)\n{\n\tstruct scatterlist sg;\n\tint err;\n\n\t__sum16 old_checksum = th->check;\n\tth->check = 0;\n\t/* options aren't included in the hash */\n\tsg_init_one(&sg, th, sizeof(struct tcphdr));\n\terr = crypto_hash_update(&hp->md5_desc, &sg, sizeof(struct tcphdr));\n\tth->check = old_checksum;\n\treturn err;\n}\n\nEXPORT_SYMBOL(tcp_md5_hash_header);\n\nint tcp_md5_hash_skb_data(struct tcp_md5sig_pool *hp,\n\t\t\t  struct sk_buff *skb, unsigned header_len)\n{\n\tstruct scatterlist sg;\n\tconst struct tcphdr *tp = tcp_hdr(skb);\n\tstruct hash_desc *desc = &hp->md5_desc;\n\tunsigned i;\n\tconst unsigned head_data_len = skb_headlen(skb) > header_len ?\n\t\t\t\t       skb_headlen(skb) - header_len : 0;\n\tconst struct skb_shared_info *shi = skb_shinfo(skb);\n\n\tsg_init_table(&sg, 1);\n\n\tsg_set_buf(&sg, ((u8 *) tp) + header_len, head_data_len);\n\tif (crypto_hash_update(desc, &sg, head_data_len))\n\t\treturn 1;\n\n\tfor (i = 0; i < shi->nr_frags; ++i) {\n\t\tconst struct skb_frag_struct *f = &shi->frags[i];\n\t\tsg_set_page(&sg, f->page, f->size, f->page_offset);\n\t\tif (crypto_hash_update(desc, &sg, f->size))\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nEXPORT_SYMBOL(tcp_md5_hash_skb_data);\n\nint tcp_md5_hash_key(struct tcp_md5sig_pool *hp, struct tcp_md5sig_key *key)\n{\n\tstruct scatterlist sg;\n\n\tsg_init_one(&sg, key->key, key->keylen);\n\treturn crypto_hash_update(&hp->md5_desc, &sg, key->keylen);\n}\n\nEXPORT_SYMBOL(tcp_md5_hash_key);\n\n#endif\n\n/**\n * Each Responder maintains up to two secret values concurrently for\n * efficient secret rollover.  Each secret value has 4 states:\n *\n * Generating.  (tcp_secret_generating != tcp_secret_primary)\n *    Generates new Responder-Cookies, but not yet used for primary\n *    verification.  This is a short-term state, typically lasting only\n *    one round trip time (RTT).\n *\n * Primary.  (tcp_secret_generating == tcp_secret_primary)\n *    Used both for generation and primary verification.\n *\n * Retiring.  (tcp_secret_retiring != tcp_secret_secondary)\n *    Used for verification, until the first failure that can be\n *    verified by the newer Generating secret.  At that time, this\n *    cookie's state is changed to Secondary, and the Generating\n *    cookie's state is changed to Primary.  This is a short-term state,\n *    typically lasting only one round trip time (RTT).\n *\n * Secondary.  (tcp_secret_retiring == tcp_secret_secondary)\n *    Used for secondary verification, after primary verification\n *    failures.  This state lasts no more than twice the Maximum Segment\n *    Lifetime (2MSL).  Then, the secret is discarded.\n */\nstruct tcp_cookie_secret {\n\t/* The secret is divided into two parts.  The digest part is the\n\t * equivalent of previously hashing a secret and saving the state,\n\t * and serves as an initialization vector (IV).  The message part\n\t * serves as the trailing secret.\n\t */\n\tu32\t\t\t\tsecrets[COOKIE_WORKSPACE_WORDS];\n\tunsigned long\t\t\texpires;\n};\n\n#define TCP_SECRET_1MSL (HZ * TCP_PAWS_MSL)\n#define TCP_SECRET_2MSL (HZ * TCP_PAWS_MSL * 2)\n#define TCP_SECRET_LIFE (HZ * 600)\n\nstatic struct tcp_cookie_secret tcp_secret_one;\nstatic struct tcp_cookie_secret tcp_secret_two;\n\n/* Essentially a circular list, without dynamic allocation. */\nstatic struct tcp_cookie_secret *tcp_secret_generating;\nstatic struct tcp_cookie_secret *tcp_secret_primary;\nstatic struct tcp_cookie_secret *tcp_secret_retiring;\nstatic struct tcp_cookie_secret *tcp_secret_secondary;\n\nstatic DEFINE_SPINLOCK(tcp_secret_locker);\n\n/* Select a pseudo-random word in the cookie workspace.\n */\nstatic inline u32 tcp_cookie_work(const u32 *ws, const int n)\n{\n\treturn ws[COOKIE_DIGEST_WORDS + ((COOKIE_MESSAGE_WORDS-1) & ws[n])];\n}\n\n/* Fill bakery[COOKIE_WORKSPACE_WORDS] with generator, updating as needed.\n * Called in softirq context.\n * Returns: 0 for success.\n */\nint tcp_cookie_generator(u32 *bakery)\n{\n\tunsigned long jiffy = jiffies;\n\n\tif (unlikely(time_after_eq(jiffy, tcp_secret_generating->expires))) {\n\t\tspin_lock_bh(&tcp_secret_locker);\n\t\tif (!time_after_eq(jiffy, tcp_secret_generating->expires)) {\n\t\t\t/* refreshed by another */\n\t\t\tmemcpy(bakery,\n\t\t\t       &tcp_secret_generating->secrets[0],\n\t\t\t       COOKIE_WORKSPACE_WORDS);\n\t\t} else {\n\t\t\t/* still needs refreshing */\n\t\t\tget_random_bytes(bakery, COOKIE_WORKSPACE_WORDS);\n\n\t\t\t/* The first time, paranoia assumes that the\n\t\t\t * randomization function isn't as strong.  But,\n\t\t\t * this secret initialization is delayed until\n\t\t\t * the last possible moment (packet arrival).\n\t\t\t * Although that time is observable, it is\n\t\t\t * unpredictably variable.  Mash in the most\n\t\t\t * volatile clock bits available, and expire the\n\t\t\t * secret extra quickly.\n\t\t\t */\n\t\t\tif (unlikely(tcp_secret_primary->expires ==\n\t\t\t\t     tcp_secret_secondary->expires)) {\n\t\t\t\tstruct timespec tv;\n\n\t\t\t\tgetnstimeofday(&tv);\n\t\t\t\tbakery[COOKIE_DIGEST_WORDS+0] ^=\n\t\t\t\t\t(u32)tv.tv_nsec;\n\n\t\t\t\ttcp_secret_secondary->expires = jiffy\n\t\t\t\t\t+ TCP_SECRET_1MSL\n\t\t\t\t\t+ (0x0f & tcp_cookie_work(bakery, 0));\n\t\t\t} else {\n\t\t\t\ttcp_secret_secondary->expires = jiffy\n\t\t\t\t\t+ TCP_SECRET_LIFE\n\t\t\t\t\t+ (0xff & tcp_cookie_work(bakery, 1));\n\t\t\t\ttcp_secret_primary->expires = jiffy\n\t\t\t\t\t+ TCP_SECRET_2MSL\n\t\t\t\t\t+ (0x1f & tcp_cookie_work(bakery, 2));\n\t\t\t}\n\t\t\tmemcpy(&tcp_secret_secondary->secrets[0],\n\t\t\t       bakery, COOKIE_WORKSPACE_WORDS);\n\n\t\t\trcu_assign_pointer(tcp_secret_generating,\n\t\t\t\t\t   tcp_secret_secondary);\n\t\t\trcu_assign_pointer(tcp_secret_retiring,\n\t\t\t\t\t   tcp_secret_primary);\n\t\t\t/*\n\t\t\t * Neither call_rcu() nor synchronize_rcu() needed.\n\t\t\t * Retiring data is not freed.  It is replaced after\n\t\t\t * further (locked) pointer updates, and a quiet time\n\t\t\t * (minimum 1MSL, maximum LIFE - 2MSL).\n\t\t\t */\n\t\t}\n\t\tspin_unlock_bh(&tcp_secret_locker);\n\t} else {\n\t\trcu_read_lock_bh();\n\t\tmemcpy(bakery,\n\t\t       &rcu_dereference(tcp_secret_generating)->secrets[0],\n\t\t       COOKIE_WORKSPACE_WORDS);\n\t\trcu_read_unlock_bh();\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_cookie_generator);\n\nvoid tcp_done(struct sock *sk)\n{\n\tif (sk->sk_state == TCP_SYN_SENT || sk->sk_state == TCP_SYN_RECV)\n\t\tTCP_INC_STATS_BH(sock_net(sk), TCP_MIB_ATTEMPTFAILS);\n\n\ttcp_set_state(sk, TCP_CLOSE);\n\ttcp_clear_xmit_timers(sk);\n\n\tsk->sk_shutdown = SHUTDOWN_MASK;\n\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_state_change(sk);\n\telse\n\t\tinet_csk_destroy_sock(sk);\n}\nEXPORT_SYMBOL_GPL(tcp_done);\n\nextern struct tcp_congestion_ops tcp_reno;\n\nstatic __initdata unsigned long thash_entries;\nstatic int __init set_thash_entries(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\tthash_entries = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"thash_entries=\", set_thash_entries);\n\nvoid __init tcp_init(void)\n{\n\tstruct sk_buff *skb = NULL;\n\tunsigned long nr_pages, limit;\n\tint order, i, max_share;\n\tunsigned long jiffy = jiffies;\n\n\tBUILD_BUG_ON(sizeof(struct tcp_skb_cb) > sizeof(skb->cb));\n\n\tpercpu_counter_init(&tcp_sockets_allocated, 0);\n\tpercpu_counter_init(&tcp_orphan_count, 0);\n\ttcp_hashinfo.bind_bucket_cachep =\n\t\tkmem_cache_create(\"tcp_bind_bucket\",\n\t\t\t\t  sizeof(struct inet_bind_bucket), 0,\n\t\t\t\t  SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);\n\n\t/* Size and allocate the main established and bind bucket\n\t * hash tables.\n\t *\n\t * The methodology is similar to that of the buffer cache.\n\t */\n\ttcp_hashinfo.ehash =\n\t\talloc_large_system_hash(\"TCP established\",\n\t\t\t\t\tsizeof(struct inet_ehash_bucket),\n\t\t\t\t\tthash_entries,\n\t\t\t\t\t(totalram_pages >= 128 * 1024) ?\n\t\t\t\t\t13 : 15,\n\t\t\t\t\t0,\n\t\t\t\t\tNULL,\n\t\t\t\t\t&tcp_hashinfo.ehash_mask,\n\t\t\t\t\tthash_entries ? 0 : 512 * 1024);\n\tfor (i = 0; i <= tcp_hashinfo.ehash_mask; i++) {\n\t\tINIT_HLIST_NULLS_HEAD(&tcp_hashinfo.ehash[i].chain, i);\n\t\tINIT_HLIST_NULLS_HEAD(&tcp_hashinfo.ehash[i].twchain, i);\n\t}\n\tif (inet_ehash_locks_alloc(&tcp_hashinfo))\n\t\tpanic(\"TCP: failed to alloc ehash_locks\");\n\ttcp_hashinfo.bhash =\n\t\talloc_large_system_hash(\"TCP bind\",\n\t\t\t\t\tsizeof(struct inet_bind_hashbucket),\n\t\t\t\t\ttcp_hashinfo.ehash_mask + 1,\n\t\t\t\t\t(totalram_pages >= 128 * 1024) ?\n\t\t\t\t\t13 : 15,\n\t\t\t\t\t0,\n\t\t\t\t\t&tcp_hashinfo.bhash_size,\n\t\t\t\t\tNULL,\n\t\t\t\t\t64 * 1024);\n\ttcp_hashinfo.bhash_size = 1 << tcp_hashinfo.bhash_size;\n\tfor (i = 0; i < tcp_hashinfo.bhash_size; i++) {\n\t\tspin_lock_init(&tcp_hashinfo.bhash[i].lock);\n\t\tINIT_HLIST_HEAD(&tcp_hashinfo.bhash[i].chain);\n\t}\n\n\t/* Try to be a bit smarter and adjust defaults depending\n\t * on available memory.\n\t */\n\tfor (order = 0; ((1 << order) << PAGE_SHIFT) <\n\t\t\t(tcp_hashinfo.bhash_size * sizeof(struct inet_bind_hashbucket));\n\t\t\torder++)\n\t\t;\n\tif (order >= 4) {\n\t\ttcp_death_row.sysctl_max_tw_buckets = 180000;\n\t\tsysctl_tcp_max_orphans = 4096 << (order - 4);\n\t\tsysctl_max_syn_backlog = 1024;\n\t} else if (order < 3) {\n\t\ttcp_death_row.sysctl_max_tw_buckets >>= (3 - order);\n\t\tsysctl_tcp_max_orphans >>= (3 - order);\n\t\tsysctl_max_syn_backlog = 128;\n\t}\n\n\t/* Set the pressure threshold to be a fraction of global memory that\n\t * is up to 1/2 at 256 MB, decreasing toward zero with the amount of\n\t * memory, with a floor of 128 pages.\n\t */\n\tnr_pages = totalram_pages - totalhigh_pages;\n\tlimit = min(nr_pages, 1UL<<(28-PAGE_SHIFT)) >> (20-PAGE_SHIFT);\n\tlimit = (limit * (nr_pages >> (20-PAGE_SHIFT))) >> (PAGE_SHIFT-11);\n\tlimit = max(limit, 128UL);\n\tsysctl_tcp_mem[0] = limit / 4 * 3;\n\tsysctl_tcp_mem[1] = limit;\n\tsysctl_tcp_mem[2] = sysctl_tcp_mem[0] * 2;\n\n\t/* Set per-socket limits to no more than 1/128 the pressure threshold */\n\tlimit = ((unsigned long)sysctl_tcp_mem[1]) << (PAGE_SHIFT - 7);\n\tmax_share = min(4UL*1024*1024, limit);\n\n\tsysctl_tcp_wmem[0] = SK_MEM_QUANTUM;\n\tsysctl_tcp_wmem[1] = 16*1024;\n\tsysctl_tcp_wmem[2] = max(64*1024, max_share);\n\n\tsysctl_tcp_rmem[0] = SK_MEM_QUANTUM;\n\tsysctl_tcp_rmem[1] = 87380;\n\tsysctl_tcp_rmem[2] = max(87380, max_share);\n\n\tprintk(KERN_INFO \"TCP: Hash tables configured \"\n\t       \"(established %u bind %u)\\n\",\n\t       tcp_hashinfo.ehash_mask + 1, tcp_hashinfo.bhash_size);\n\n\ttcp_register_congestion_control(&tcp_reno);\n\n\tmemset(&tcp_secret_one.secrets[0], 0, sizeof(tcp_secret_one.secrets));\n\tmemset(&tcp_secret_two.secrets[0], 0, sizeof(tcp_secret_two.secrets));\n\ttcp_secret_one.expires = jiffy; /* past due */\n\ttcp_secret_two.expires = jiffy; /* past due */\n\ttcp_secret_generating = &tcp_secret_one;\n\ttcp_secret_primary = &tcp_secret_one;\n\ttcp_secret_retiring = &tcp_secret_two;\n\ttcp_secret_secondary = &tcp_secret_two;\n}\n\nEXPORT_SYMBOL(tcp_close);\nEXPORT_SYMBOL(tcp_disconnect);\nEXPORT_SYMBOL(tcp_getsockopt);\nEXPORT_SYMBOL(tcp_ioctl);\nEXPORT_SYMBOL(tcp_poll);\nEXPORT_SYMBOL(tcp_read_sock);\nEXPORT_SYMBOL(tcp_recvmsg);\nEXPORT_SYMBOL(tcp_sendmsg);\nEXPORT_SYMBOL(tcp_splice_read);\nEXPORT_SYMBOL(tcp_sendpage);\nEXPORT_SYMBOL(tcp_setsockopt);\nEXPORT_SYMBOL(tcp_shutdown);\n"], "filenames": ["net/ipv4/tcp.c"], "buggy_code_start_loc": [1370], "buggy_code_end_loc": [1370], "fixing_code_start_loc": [1371], "fixing_code_end_loc": [1372], "type": "CWE-400", "message": "The tcp_read_sock function in net/ipv4/tcp.c in the Linux kernel before 2.6.34 does not properly manage skb consumption, which allows local users to cause a denial of service (system crash) via a crafted splice system call for a TCP socket.", "other": {"cve": {"id": "CVE-2013-2128", "sourceIdentifier": "secalert@redhat.com", "published": "2013-06-07T14:03:18.533", "lastModified": "2023-02-13T04:42:54.237", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The tcp_read_sock function in net/ipv4/tcp.c in the Linux kernel before 2.6.34 does not properly manage skb consumption, which allows local users to cause a denial of service (system crash) via a crafted splice system call for a TCP socket."}, {"lang": "es", "value": "La funci\u00f3n tcp_read_sock en net/ipv4/tcp.c del kernel de Linux antes de v2.6.34 no gestiona correctamente el consumo skb, lo que permite a usuarios locales causar una denegaci\u00f3n de servicios (ca\u00edda del sistema) a trav\u00e9s de la llamada al sistema manipulada splice para un socket TCP."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-400"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.6.34", "matchCriteriaId": "6BA10979-D59C-4A33-ACC8-A110A5ACFF74"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=baff42ab1494528907bf4d5870359e31711746ae", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2013-1051.html", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2013/05/29/11", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=968484", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/baff42ab1494528907bf4d5870359e31711746ae", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/baff42ab1494528907bf4d5870359e31711746ae"}}