{"buggy_code": ["/*\n * Software multibuffer async crypto daemon.\n *\n * Copyright (c) 2014 Tim Chen <tim.c.chen@linux.intel.com>\n *\n * Adapted from crypto daemon.\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/algapi.h>\n#include <crypto/internal/hash.h>\n#include <crypto/internal/aead.h>\n#include <crypto/mcryptd.h>\n#include <crypto/crypto_wq.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/hardirq.h>\n\n#define MCRYPTD_MAX_CPU_QLEN 100\n#define MCRYPTD_BATCH 9\n\nstatic void *mcryptd_alloc_instance(struct crypto_alg *alg, unsigned int head,\n\t\t\t\t   unsigned int tail);\n\nstruct mcryptd_flush_list {\n\tstruct list_head list;\n\tstruct mutex lock;\n};\n\nstatic struct mcryptd_flush_list __percpu *mcryptd_flist;\n\nstruct hashd_instance_ctx {\n\tstruct crypto_ahash_spawn spawn;\n\tstruct mcryptd_queue *queue;\n};\n\nstatic void mcryptd_queue_worker(struct work_struct *work);\n\nvoid mcryptd_arm_flusher(struct mcryptd_alg_cstate *cstate, unsigned long delay)\n{\n\tstruct mcryptd_flush_list *flist;\n\n\tif (!cstate->flusher_engaged) {\n\t\t/* put the flusher on the flush list */\n\t\tflist = per_cpu_ptr(mcryptd_flist, smp_processor_id());\n\t\tmutex_lock(&flist->lock);\n\t\tlist_add_tail(&cstate->flush_list, &flist->list);\n\t\tcstate->flusher_engaged = true;\n\t\tcstate->next_flush = jiffies + delay;\n\t\tqueue_delayed_work_on(smp_processor_id(), kcrypto_wq,\n\t\t\t&cstate->flush, delay);\n\t\tmutex_unlock(&flist->lock);\n\t}\n}\nEXPORT_SYMBOL(mcryptd_arm_flusher);\n\nstatic int mcryptd_init_queue(struct mcryptd_queue *queue,\n\t\t\t     unsigned int max_cpu_qlen)\n{\n\tint cpu;\n\tstruct mcryptd_cpu_queue *cpu_queue;\n\n\tqueue->cpu_queue = alloc_percpu(struct mcryptd_cpu_queue);\n\tpr_debug(\"mqueue:%p mcryptd_cpu_queue %p\\n\", queue, queue->cpu_queue);\n\tif (!queue->cpu_queue)\n\t\treturn -ENOMEM;\n\tfor_each_possible_cpu(cpu) {\n\t\tcpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);\n\t\tpr_debug(\"cpu_queue #%d %p\\n\", cpu, queue->cpu_queue);\n\t\tcrypto_init_queue(&cpu_queue->queue, max_cpu_qlen);\n\t\tINIT_WORK(&cpu_queue->work, mcryptd_queue_worker);\n\t}\n\treturn 0;\n}\n\nstatic void mcryptd_fini_queue(struct mcryptd_queue *queue)\n{\n\tint cpu;\n\tstruct mcryptd_cpu_queue *cpu_queue;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);\n\t\tBUG_ON(cpu_queue->queue.qlen);\n\t}\n\tfree_percpu(queue->cpu_queue);\n}\n\nstatic int mcryptd_enqueue_request(struct mcryptd_queue *queue,\n\t\t\t\t  struct crypto_async_request *request,\n\t\t\t\t  struct mcryptd_hash_request_ctx *rctx)\n{\n\tint cpu, err;\n\tstruct mcryptd_cpu_queue *cpu_queue;\n\n\tcpu = get_cpu();\n\tcpu_queue = this_cpu_ptr(queue->cpu_queue);\n\trctx->tag.cpu = cpu;\n\n\terr = crypto_enqueue_request(&cpu_queue->queue, request);\n\tpr_debug(\"enqueue request: cpu %d cpu_queue %p request %p\\n\",\n\t\t cpu, cpu_queue, request);\n\tqueue_work_on(cpu, kcrypto_wq, &cpu_queue->work);\n\tput_cpu();\n\n\treturn err;\n}\n\n/*\n * Try to opportunisticlly flush the partially completed jobs if\n * crypto daemon is the only task running.\n */\nstatic void mcryptd_opportunistic_flush(void)\n{\n\tstruct mcryptd_flush_list *flist;\n\tstruct mcryptd_alg_cstate *cstate;\n\n\tflist = per_cpu_ptr(mcryptd_flist, smp_processor_id());\n\twhile (single_task_running()) {\n\t\tmutex_lock(&flist->lock);\n\t\tcstate = list_first_entry_or_null(&flist->list,\n\t\t\t\tstruct mcryptd_alg_cstate, flush_list);\n\t\tif (!cstate || !cstate->flusher_engaged) {\n\t\t\tmutex_unlock(&flist->lock);\n\t\t\treturn;\n\t\t}\n\t\tlist_del(&cstate->flush_list);\n\t\tcstate->flusher_engaged = false;\n\t\tmutex_unlock(&flist->lock);\n\t\tcstate->alg_state->flusher(cstate);\n\t}\n}\n\n/*\n * Called in workqueue context, do one real cryption work (via\n * req->complete) and reschedule itself if there are more work to\n * do.\n */\nstatic void mcryptd_queue_worker(struct work_struct *work)\n{\n\tstruct mcryptd_cpu_queue *cpu_queue;\n\tstruct crypto_async_request *req, *backlog;\n\tint i;\n\n\t/*\n\t * Need to loop through more than once for multi-buffer to\n\t * be effective.\n\t */\n\n\tcpu_queue = container_of(work, struct mcryptd_cpu_queue, work);\n\ti = 0;\n\twhile (i < MCRYPTD_BATCH || single_task_running()) {\n\t\t/*\n\t\t * preempt_disable/enable is used to prevent\n\t\t * being preempted by mcryptd_enqueue_request()\n\t\t */\n\t\tlocal_bh_disable();\n\t\tpreempt_disable();\n\t\tbacklog = crypto_get_backlog(&cpu_queue->queue);\n\t\treq = crypto_dequeue_request(&cpu_queue->queue);\n\t\tpreempt_enable();\n\t\tlocal_bh_enable();\n\n\t\tif (!req) {\n\t\t\tmcryptd_opportunistic_flush();\n\t\t\treturn;\n\t\t}\n\n\t\tif (backlog)\n\t\t\tbacklog->complete(backlog, -EINPROGRESS);\n\t\treq->complete(req, 0);\n\t\tif (!cpu_queue->queue.qlen)\n\t\t\treturn;\n\t\t++i;\n\t}\n\tif (cpu_queue->queue.qlen)\n\t\tqueue_work(kcrypto_wq, &cpu_queue->work);\n}\n\nvoid mcryptd_flusher(struct work_struct *__work)\n{\n\tstruct\tmcryptd_alg_cstate\t*alg_cpu_state;\n\tstruct\tmcryptd_alg_state\t*alg_state;\n\tstruct\tmcryptd_flush_list\t*flist;\n\tint\tcpu;\n\n\tcpu = smp_processor_id();\n\talg_cpu_state = container_of(to_delayed_work(__work),\n\t\t\t\t     struct mcryptd_alg_cstate, flush);\n\talg_state = alg_cpu_state->alg_state;\n\tif (alg_cpu_state->cpu != cpu)\n\t\tpr_debug(\"mcryptd error: work on cpu %d, should be cpu %d\\n\",\n\t\t\t\tcpu, alg_cpu_state->cpu);\n\n\tif (alg_cpu_state->flusher_engaged) {\n\t\tflist = per_cpu_ptr(mcryptd_flist, cpu);\n\t\tmutex_lock(&flist->lock);\n\t\tlist_del(&alg_cpu_state->flush_list);\n\t\talg_cpu_state->flusher_engaged = false;\n\t\tmutex_unlock(&flist->lock);\n\t\talg_state->flusher(alg_cpu_state);\n\t}\n}\nEXPORT_SYMBOL_GPL(mcryptd_flusher);\n\nstatic inline struct mcryptd_queue *mcryptd_get_queue(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct mcryptd_instance_ctx *ictx = crypto_instance_ctx(inst);\n\n\treturn ictx->queue;\n}\n\nstatic void *mcryptd_alloc_instance(struct crypto_alg *alg, unsigned int head,\n\t\t\t\t   unsigned int tail)\n{\n\tchar *p;\n\tstruct crypto_instance *inst;\n\tint err;\n\n\tp = kzalloc(head + sizeof(*inst) + tail, GFP_KERNEL);\n\tif (!p)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tinst = (void *)(p + head);\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t    \"mcryptd(%s)\", alg->cra_driver_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto out_free_inst;\n\n\tmemcpy(inst->alg.cra_name, alg->cra_name, CRYPTO_MAX_ALG_NAME);\n\n\tinst->alg.cra_priority = alg->cra_priority + 50;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\nout:\n\treturn p;\n\nout_free_inst:\n\tkfree(p);\n\tp = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic inline void mcryptd_check_internal(struct rtattr **tb, u32 *type,\n\t\t\t\t\t  u32 *mask)\n{\n\tstruct crypto_attr_type *algt;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn;\n\tif ((algt->type & CRYPTO_ALG_INTERNAL))\n\t\t*type |= CRYPTO_ALG_INTERNAL;\n\tif ((algt->mask & CRYPTO_ALG_INTERNAL))\n\t\t*mask |= CRYPTO_ALG_INTERNAL;\n}\n\nstatic int mcryptd_hash_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct hashd_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_ahash_spawn *spawn = &ictx->spawn;\n\tstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_ahash *hash;\n\n\thash = crypto_spawn_ahash(spawn);\n\tif (IS_ERR(hash))\n\t\treturn PTR_ERR(hash);\n\n\tctx->child = hash;\n\tcrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\n\t\t\t\t sizeof(struct mcryptd_hash_request_ctx) +\n\t\t\t\t crypto_ahash_reqsize(hash));\n\treturn 0;\n}\n\nstatic void mcryptd_hash_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_ahash(ctx->child);\n}\n\nstatic int mcryptd_hash_setkey(struct crypto_ahash *parent,\n\t\t\t\t   const u8 *key, unsigned int keylen)\n{\n\tstruct mcryptd_hash_ctx *ctx   = crypto_ahash_ctx(parent);\n\tstruct crypto_ahash *child = ctx->child;\n\tint err;\n\n\tcrypto_ahash_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ahash_set_flags(child, crypto_ahash_get_flags(parent) &\n\t\t\t\t      CRYPTO_TFM_REQ_MASK);\n\terr = crypto_ahash_setkey(child, key, keylen);\n\tcrypto_ahash_set_flags(parent, crypto_ahash_get_flags(child) &\n\t\t\t\t       CRYPTO_TFM_RES_MASK);\n\treturn err;\n}\n\nstatic int mcryptd_hash_enqueue(struct ahash_request *req,\n\t\t\t\tcrypto_completion_t complete)\n{\n\tint ret;\n\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct mcryptd_queue *queue =\n\t\tmcryptd_get_queue(crypto_ahash_tfm(tfm));\n\n\trctx->complete = req->base.complete;\n\treq->base.complete = complete;\n\n\tret = mcryptd_enqueue_request(queue, &req->base, rctx);\n\n\treturn ret;\n}\n\nstatic void mcryptd_hash_init(struct crypto_async_request *req_async, int err)\n{\n\tstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(req_async->tfm);\n\tstruct crypto_ahash *child = ctx->child;\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct ahash_request *desc = &rctx->areq;\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\tahash_request_set_tfm(desc, child);\n\tahash_request_set_callback(desc, CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t\t\trctx->complete, req_async);\n\n\trctx->out = req->result;\n\terr = crypto_ahash_init(desc);\n\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_init_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_init);\n}\n\nstatic void mcryptd_hash_update(struct crypto_async_request *req_async, int err)\n{\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\trctx->out = req->result;\n\terr = ahash_mcryptd_update(&rctx->areq);\n\tif (err) {\n\t\treq->base.complete = rctx->complete;\n\t\tgoto out;\n\t}\n\n\treturn;\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_update_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_update);\n}\n\nstatic void mcryptd_hash_final(struct crypto_async_request *req_async, int err)\n{\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\trctx->out = req->result;\n\terr = ahash_mcryptd_final(&rctx->areq);\n\tif (err) {\n\t\treq->base.complete = rctx->complete;\n\t\tgoto out;\n\t}\n\n\treturn;\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_final_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_final);\n}\n\nstatic void mcryptd_hash_finup(struct crypto_async_request *req_async, int err)\n{\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\trctx->out = req->result;\n\terr = ahash_mcryptd_finup(&rctx->areq);\n\n\tif (err) {\n\t\treq->base.complete = rctx->complete;\n\t\tgoto out;\n\t}\n\n\treturn;\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_finup_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_finup);\n}\n\nstatic void mcryptd_hash_digest(struct crypto_async_request *req_async, int err)\n{\n\tstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(req_async->tfm);\n\tstruct crypto_ahash *child = ctx->child;\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct ahash_request *desc = &rctx->areq;\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\tahash_request_set_tfm(desc, child);\n\tahash_request_set_callback(desc, CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t\t\trctx->complete, req_async);\n\n\trctx->out = req->result;\n\terr = ahash_mcryptd_digest(desc);\n\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_digest_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_digest);\n}\n\nstatic int mcryptd_hash_export(struct ahash_request *req, void *out)\n{\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\treturn crypto_ahash_export(&rctx->areq, out);\n}\n\nstatic int mcryptd_hash_import(struct ahash_request *req, const void *in)\n{\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\treturn crypto_ahash_import(&rctx->areq, in);\n}\n\nstatic int mcryptd_create_hash(struct crypto_template *tmpl, struct rtattr **tb,\n\t\t\t      struct mcryptd_queue *queue)\n{\n\tstruct hashd_instance_ctx *ctx;\n\tstruct ahash_instance *inst;\n\tstruct hash_alg_common *halg;\n\tstruct crypto_alg *alg;\n\tu32 type = 0;\n\tu32 mask = 0;\n\tint err;\n\n\tmcryptd_check_internal(tb, &type, &mask);\n\n\thalg = ahash_attr_alg(tb[1], type, mask);\n\tif (IS_ERR(halg))\n\t\treturn PTR_ERR(halg);\n\n\talg = &halg->base;\n\tpr_debug(\"crypto: mcryptd hash alg: %s\\n\", alg->cra_name);\n\tinst = mcryptd_alloc_instance(alg, ahash_instance_headroom(),\n\t\t\t\t\tsizeof(*ctx));\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tctx = ahash_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_init_ahash_spawn(&ctx->spawn, halg,\n\t\t\t\t      ahash_crypto_instance(inst));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\ttype = CRYPTO_ALG_ASYNC;\n\tif (alg->cra_flags & CRYPTO_ALG_INTERNAL)\n\t\ttype |= CRYPTO_ALG_INTERNAL;\n\tinst->alg.halg.base.cra_flags = type;\n\n\tinst->alg.halg.digestsize = halg->digestsize;\n\tinst->alg.halg.statesize = halg->statesize;\n\tinst->alg.halg.base.cra_ctxsize = sizeof(struct mcryptd_hash_ctx);\n\n\tinst->alg.halg.base.cra_init = mcryptd_hash_init_tfm;\n\tinst->alg.halg.base.cra_exit = mcryptd_hash_exit_tfm;\n\n\tinst->alg.init   = mcryptd_hash_init_enqueue;\n\tinst->alg.update = mcryptd_hash_update_enqueue;\n\tinst->alg.final  = mcryptd_hash_final_enqueue;\n\tinst->alg.finup  = mcryptd_hash_finup_enqueue;\n\tinst->alg.export = mcryptd_hash_export;\n\tinst->alg.import = mcryptd_hash_import;\n\tinst->alg.setkey = mcryptd_hash_setkey;\n\tinst->alg.digest = mcryptd_hash_digest_enqueue;\n\n\terr = ahash_register_instance(tmpl, inst);\n\tif (err) {\n\t\tcrypto_drop_ahash(&ctx->spawn);\nout_free_inst:\n\t\tkfree(inst);\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}\n\nstatic struct mcryptd_queue mqueue;\n\nstatic int mcryptd_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn PTR_ERR(algt);\n\n\tswitch (algt->type & algt->mask & CRYPTO_ALG_TYPE_MASK) {\n\tcase CRYPTO_ALG_TYPE_DIGEST:\n\t\treturn mcryptd_create_hash(tmpl, tb, &mqueue);\n\tbreak;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic void mcryptd_free(struct crypto_instance *inst)\n{\n\tstruct mcryptd_instance_ctx *ctx = crypto_instance_ctx(inst);\n\tstruct hashd_instance_ctx *hctx = crypto_instance_ctx(inst);\n\n\tswitch (inst->alg.cra_flags & CRYPTO_ALG_TYPE_MASK) {\n\tcase CRYPTO_ALG_TYPE_AHASH:\n\t\tcrypto_drop_ahash(&hctx->spawn);\n\t\tkfree(ahash_instance(inst));\n\t\treturn;\n\tdefault:\n\t\tcrypto_drop_spawn(&ctx->spawn);\n\t\tkfree(inst);\n\t}\n}\n\nstatic struct crypto_template mcryptd_tmpl = {\n\t.name = \"mcryptd\",\n\t.create = mcryptd_create,\n\t.free = mcryptd_free,\n\t.module = THIS_MODULE,\n};\n\nstruct mcryptd_ahash *mcryptd_alloc_ahash(const char *alg_name,\n\t\t\t\t\tu32 type, u32 mask)\n{\n\tchar mcryptd_alg_name[CRYPTO_MAX_ALG_NAME];\n\tstruct crypto_ahash *tfm;\n\n\tif (snprintf(mcryptd_alg_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"mcryptd(%s)\", alg_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-EINVAL);\n\ttfm = crypto_alloc_ahash(mcryptd_alg_name, type, mask);\n\tif (IS_ERR(tfm))\n\t\treturn ERR_CAST(tfm);\n\tif (tfm->base.__crt_alg->cra_module != THIS_MODULE) {\n\t\tcrypto_free_ahash(tfm);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn __mcryptd_ahash_cast(tfm);\n}\nEXPORT_SYMBOL_GPL(mcryptd_alloc_ahash);\n\nint ahash_mcryptd_digest(struct ahash_request *desc)\n{\n\treturn crypto_ahash_init(desc) ?: ahash_mcryptd_finup(desc);\n}\n\nint ahash_mcryptd_update(struct ahash_request *desc)\n{\n\t/* alignment is to be done by multi-buffer crypto algorithm if needed */\n\n\treturn crypto_ahash_update(desc);\n}\n\nint ahash_mcryptd_finup(struct ahash_request *desc)\n{\n\t/* alignment is to be done by multi-buffer crypto algorithm if needed */\n\n\treturn crypto_ahash_finup(desc);\n}\n\nint ahash_mcryptd_final(struct ahash_request *desc)\n{\n\t/* alignment is to be done by multi-buffer crypto algorithm if needed */\n\n\treturn crypto_ahash_final(desc);\n}\n\nstruct crypto_ahash *mcryptd_ahash_child(struct mcryptd_ahash *tfm)\n{\n\tstruct mcryptd_hash_ctx *ctx = crypto_ahash_ctx(&tfm->base);\n\n\treturn ctx->child;\n}\nEXPORT_SYMBOL_GPL(mcryptd_ahash_child);\n\nstruct ahash_request *mcryptd_ahash_desc(struct ahash_request *req)\n{\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\treturn &rctx->areq;\n}\nEXPORT_SYMBOL_GPL(mcryptd_ahash_desc);\n\nvoid mcryptd_free_ahash(struct mcryptd_ahash *tfm)\n{\n\tcrypto_free_ahash(&tfm->base);\n}\nEXPORT_SYMBOL_GPL(mcryptd_free_ahash);\n\nstatic int __init mcryptd_init(void)\n{\n\tint err, cpu;\n\tstruct mcryptd_flush_list *flist;\n\n\tmcryptd_flist = alloc_percpu(struct mcryptd_flush_list);\n\tfor_each_possible_cpu(cpu) {\n\t\tflist = per_cpu_ptr(mcryptd_flist, cpu);\n\t\tINIT_LIST_HEAD(&flist->list);\n\t\tmutex_init(&flist->lock);\n\t}\n\n\terr = mcryptd_init_queue(&mqueue, MCRYPTD_MAX_CPU_QLEN);\n\tif (err) {\n\t\tfree_percpu(mcryptd_flist);\n\t\treturn err;\n\t}\n\n\terr = crypto_register_template(&mcryptd_tmpl);\n\tif (err) {\n\t\tmcryptd_fini_queue(&mqueue);\n\t\tfree_percpu(mcryptd_flist);\n\t}\n\n\treturn err;\n}\n\nstatic void __exit mcryptd_exit(void)\n{\n\tmcryptd_fini_queue(&mqueue);\n\tcrypto_unregister_template(&mcryptd_tmpl);\n\tfree_percpu(mcryptd_flist);\n}\n\nsubsys_initcall(mcryptd_init);\nmodule_exit(mcryptd_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Software async multibuffer crypto daemon\");\nMODULE_ALIAS_CRYPTO(\"mcryptd\");\n"], "fixing_code": ["/*\n * Software multibuffer async crypto daemon.\n *\n * Copyright (c) 2014 Tim Chen <tim.c.chen@linux.intel.com>\n *\n * Adapted from crypto daemon.\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/algapi.h>\n#include <crypto/internal/hash.h>\n#include <crypto/internal/aead.h>\n#include <crypto/mcryptd.h>\n#include <crypto/crypto_wq.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/hardirq.h>\n\n#define MCRYPTD_MAX_CPU_QLEN 100\n#define MCRYPTD_BATCH 9\n\nstatic void *mcryptd_alloc_instance(struct crypto_alg *alg, unsigned int head,\n\t\t\t\t   unsigned int tail);\n\nstruct mcryptd_flush_list {\n\tstruct list_head list;\n\tstruct mutex lock;\n};\n\nstatic struct mcryptd_flush_list __percpu *mcryptd_flist;\n\nstruct hashd_instance_ctx {\n\tstruct crypto_ahash_spawn spawn;\n\tstruct mcryptd_queue *queue;\n};\n\nstatic void mcryptd_queue_worker(struct work_struct *work);\n\nvoid mcryptd_arm_flusher(struct mcryptd_alg_cstate *cstate, unsigned long delay)\n{\n\tstruct mcryptd_flush_list *flist;\n\n\tif (!cstate->flusher_engaged) {\n\t\t/* put the flusher on the flush list */\n\t\tflist = per_cpu_ptr(mcryptd_flist, smp_processor_id());\n\t\tmutex_lock(&flist->lock);\n\t\tlist_add_tail(&cstate->flush_list, &flist->list);\n\t\tcstate->flusher_engaged = true;\n\t\tcstate->next_flush = jiffies + delay;\n\t\tqueue_delayed_work_on(smp_processor_id(), kcrypto_wq,\n\t\t\t&cstate->flush, delay);\n\t\tmutex_unlock(&flist->lock);\n\t}\n}\nEXPORT_SYMBOL(mcryptd_arm_flusher);\n\nstatic int mcryptd_init_queue(struct mcryptd_queue *queue,\n\t\t\t     unsigned int max_cpu_qlen)\n{\n\tint cpu;\n\tstruct mcryptd_cpu_queue *cpu_queue;\n\n\tqueue->cpu_queue = alloc_percpu(struct mcryptd_cpu_queue);\n\tpr_debug(\"mqueue:%p mcryptd_cpu_queue %p\\n\", queue, queue->cpu_queue);\n\tif (!queue->cpu_queue)\n\t\treturn -ENOMEM;\n\tfor_each_possible_cpu(cpu) {\n\t\tcpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);\n\t\tpr_debug(\"cpu_queue #%d %p\\n\", cpu, queue->cpu_queue);\n\t\tcrypto_init_queue(&cpu_queue->queue, max_cpu_qlen);\n\t\tINIT_WORK(&cpu_queue->work, mcryptd_queue_worker);\n\t}\n\treturn 0;\n}\n\nstatic void mcryptd_fini_queue(struct mcryptd_queue *queue)\n{\n\tint cpu;\n\tstruct mcryptd_cpu_queue *cpu_queue;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);\n\t\tBUG_ON(cpu_queue->queue.qlen);\n\t}\n\tfree_percpu(queue->cpu_queue);\n}\n\nstatic int mcryptd_enqueue_request(struct mcryptd_queue *queue,\n\t\t\t\t  struct crypto_async_request *request,\n\t\t\t\t  struct mcryptd_hash_request_ctx *rctx)\n{\n\tint cpu, err;\n\tstruct mcryptd_cpu_queue *cpu_queue;\n\n\tcpu = get_cpu();\n\tcpu_queue = this_cpu_ptr(queue->cpu_queue);\n\trctx->tag.cpu = cpu;\n\n\terr = crypto_enqueue_request(&cpu_queue->queue, request);\n\tpr_debug(\"enqueue request: cpu %d cpu_queue %p request %p\\n\",\n\t\t cpu, cpu_queue, request);\n\tqueue_work_on(cpu, kcrypto_wq, &cpu_queue->work);\n\tput_cpu();\n\n\treturn err;\n}\n\n/*\n * Try to opportunisticlly flush the partially completed jobs if\n * crypto daemon is the only task running.\n */\nstatic void mcryptd_opportunistic_flush(void)\n{\n\tstruct mcryptd_flush_list *flist;\n\tstruct mcryptd_alg_cstate *cstate;\n\n\tflist = per_cpu_ptr(mcryptd_flist, smp_processor_id());\n\twhile (single_task_running()) {\n\t\tmutex_lock(&flist->lock);\n\t\tcstate = list_first_entry_or_null(&flist->list,\n\t\t\t\tstruct mcryptd_alg_cstate, flush_list);\n\t\tif (!cstate || !cstate->flusher_engaged) {\n\t\t\tmutex_unlock(&flist->lock);\n\t\t\treturn;\n\t\t}\n\t\tlist_del(&cstate->flush_list);\n\t\tcstate->flusher_engaged = false;\n\t\tmutex_unlock(&flist->lock);\n\t\tcstate->alg_state->flusher(cstate);\n\t}\n}\n\n/*\n * Called in workqueue context, do one real cryption work (via\n * req->complete) and reschedule itself if there are more work to\n * do.\n */\nstatic void mcryptd_queue_worker(struct work_struct *work)\n{\n\tstruct mcryptd_cpu_queue *cpu_queue;\n\tstruct crypto_async_request *req, *backlog;\n\tint i;\n\n\t/*\n\t * Need to loop through more than once for multi-buffer to\n\t * be effective.\n\t */\n\n\tcpu_queue = container_of(work, struct mcryptd_cpu_queue, work);\n\ti = 0;\n\twhile (i < MCRYPTD_BATCH || single_task_running()) {\n\t\t/*\n\t\t * preempt_disable/enable is used to prevent\n\t\t * being preempted by mcryptd_enqueue_request()\n\t\t */\n\t\tlocal_bh_disable();\n\t\tpreempt_disable();\n\t\tbacklog = crypto_get_backlog(&cpu_queue->queue);\n\t\treq = crypto_dequeue_request(&cpu_queue->queue);\n\t\tpreempt_enable();\n\t\tlocal_bh_enable();\n\n\t\tif (!req) {\n\t\t\tmcryptd_opportunistic_flush();\n\t\t\treturn;\n\t\t}\n\n\t\tif (backlog)\n\t\t\tbacklog->complete(backlog, -EINPROGRESS);\n\t\treq->complete(req, 0);\n\t\tif (!cpu_queue->queue.qlen)\n\t\t\treturn;\n\t\t++i;\n\t}\n\tif (cpu_queue->queue.qlen)\n\t\tqueue_work(kcrypto_wq, &cpu_queue->work);\n}\n\nvoid mcryptd_flusher(struct work_struct *__work)\n{\n\tstruct\tmcryptd_alg_cstate\t*alg_cpu_state;\n\tstruct\tmcryptd_alg_state\t*alg_state;\n\tstruct\tmcryptd_flush_list\t*flist;\n\tint\tcpu;\n\n\tcpu = smp_processor_id();\n\talg_cpu_state = container_of(to_delayed_work(__work),\n\t\t\t\t     struct mcryptd_alg_cstate, flush);\n\talg_state = alg_cpu_state->alg_state;\n\tif (alg_cpu_state->cpu != cpu)\n\t\tpr_debug(\"mcryptd error: work on cpu %d, should be cpu %d\\n\",\n\t\t\t\tcpu, alg_cpu_state->cpu);\n\n\tif (alg_cpu_state->flusher_engaged) {\n\t\tflist = per_cpu_ptr(mcryptd_flist, cpu);\n\t\tmutex_lock(&flist->lock);\n\t\tlist_del(&alg_cpu_state->flush_list);\n\t\talg_cpu_state->flusher_engaged = false;\n\t\tmutex_unlock(&flist->lock);\n\t\talg_state->flusher(alg_cpu_state);\n\t}\n}\nEXPORT_SYMBOL_GPL(mcryptd_flusher);\n\nstatic inline struct mcryptd_queue *mcryptd_get_queue(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct mcryptd_instance_ctx *ictx = crypto_instance_ctx(inst);\n\n\treturn ictx->queue;\n}\n\nstatic void *mcryptd_alloc_instance(struct crypto_alg *alg, unsigned int head,\n\t\t\t\t   unsigned int tail)\n{\n\tchar *p;\n\tstruct crypto_instance *inst;\n\tint err;\n\n\tp = kzalloc(head + sizeof(*inst) + tail, GFP_KERNEL);\n\tif (!p)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tinst = (void *)(p + head);\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t    \"mcryptd(%s)\", alg->cra_driver_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto out_free_inst;\n\n\tmemcpy(inst->alg.cra_name, alg->cra_name, CRYPTO_MAX_ALG_NAME);\n\n\tinst->alg.cra_priority = alg->cra_priority + 50;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\nout:\n\treturn p;\n\nout_free_inst:\n\tkfree(p);\n\tp = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic inline bool mcryptd_check_internal(struct rtattr **tb, u32 *type,\n\t\t\t\t\t  u32 *mask)\n{\n\tstruct crypto_attr_type *algt;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn false;\n\n\t*type |= algt->type & CRYPTO_ALG_INTERNAL;\n\t*mask |= algt->mask & CRYPTO_ALG_INTERNAL;\n\n\tif (*type & *mask & CRYPTO_ALG_INTERNAL)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nstatic int mcryptd_hash_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct hashd_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_ahash_spawn *spawn = &ictx->spawn;\n\tstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_ahash *hash;\n\n\thash = crypto_spawn_ahash(spawn);\n\tif (IS_ERR(hash))\n\t\treturn PTR_ERR(hash);\n\n\tctx->child = hash;\n\tcrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\n\t\t\t\t sizeof(struct mcryptd_hash_request_ctx) +\n\t\t\t\t crypto_ahash_reqsize(hash));\n\treturn 0;\n}\n\nstatic void mcryptd_hash_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_ahash(ctx->child);\n}\n\nstatic int mcryptd_hash_setkey(struct crypto_ahash *parent,\n\t\t\t\t   const u8 *key, unsigned int keylen)\n{\n\tstruct mcryptd_hash_ctx *ctx   = crypto_ahash_ctx(parent);\n\tstruct crypto_ahash *child = ctx->child;\n\tint err;\n\n\tcrypto_ahash_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ahash_set_flags(child, crypto_ahash_get_flags(parent) &\n\t\t\t\t      CRYPTO_TFM_REQ_MASK);\n\terr = crypto_ahash_setkey(child, key, keylen);\n\tcrypto_ahash_set_flags(parent, crypto_ahash_get_flags(child) &\n\t\t\t\t       CRYPTO_TFM_RES_MASK);\n\treturn err;\n}\n\nstatic int mcryptd_hash_enqueue(struct ahash_request *req,\n\t\t\t\tcrypto_completion_t complete)\n{\n\tint ret;\n\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct mcryptd_queue *queue =\n\t\tmcryptd_get_queue(crypto_ahash_tfm(tfm));\n\n\trctx->complete = req->base.complete;\n\treq->base.complete = complete;\n\n\tret = mcryptd_enqueue_request(queue, &req->base, rctx);\n\n\treturn ret;\n}\n\nstatic void mcryptd_hash_init(struct crypto_async_request *req_async, int err)\n{\n\tstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(req_async->tfm);\n\tstruct crypto_ahash *child = ctx->child;\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct ahash_request *desc = &rctx->areq;\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\tahash_request_set_tfm(desc, child);\n\tahash_request_set_callback(desc, CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t\t\trctx->complete, req_async);\n\n\trctx->out = req->result;\n\terr = crypto_ahash_init(desc);\n\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_init_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_init);\n}\n\nstatic void mcryptd_hash_update(struct crypto_async_request *req_async, int err)\n{\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\trctx->out = req->result;\n\terr = ahash_mcryptd_update(&rctx->areq);\n\tif (err) {\n\t\treq->base.complete = rctx->complete;\n\t\tgoto out;\n\t}\n\n\treturn;\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_update_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_update);\n}\n\nstatic void mcryptd_hash_final(struct crypto_async_request *req_async, int err)\n{\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\trctx->out = req->result;\n\terr = ahash_mcryptd_final(&rctx->areq);\n\tif (err) {\n\t\treq->base.complete = rctx->complete;\n\t\tgoto out;\n\t}\n\n\treturn;\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_final_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_final);\n}\n\nstatic void mcryptd_hash_finup(struct crypto_async_request *req_async, int err)\n{\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\trctx->out = req->result;\n\terr = ahash_mcryptd_finup(&rctx->areq);\n\n\tif (err) {\n\t\treq->base.complete = rctx->complete;\n\t\tgoto out;\n\t}\n\n\treturn;\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_finup_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_finup);\n}\n\nstatic void mcryptd_hash_digest(struct crypto_async_request *req_async, int err)\n{\n\tstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(req_async->tfm);\n\tstruct crypto_ahash *child = ctx->child;\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct ahash_request *desc = &rctx->areq;\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\tahash_request_set_tfm(desc, child);\n\tahash_request_set_callback(desc, CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t\t\trctx->complete, req_async);\n\n\trctx->out = req->result;\n\terr = ahash_mcryptd_digest(desc);\n\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_digest_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_digest);\n}\n\nstatic int mcryptd_hash_export(struct ahash_request *req, void *out)\n{\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\treturn crypto_ahash_export(&rctx->areq, out);\n}\n\nstatic int mcryptd_hash_import(struct ahash_request *req, const void *in)\n{\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\treturn crypto_ahash_import(&rctx->areq, in);\n}\n\nstatic int mcryptd_create_hash(struct crypto_template *tmpl, struct rtattr **tb,\n\t\t\t      struct mcryptd_queue *queue)\n{\n\tstruct hashd_instance_ctx *ctx;\n\tstruct ahash_instance *inst;\n\tstruct hash_alg_common *halg;\n\tstruct crypto_alg *alg;\n\tu32 type = 0;\n\tu32 mask = 0;\n\tint err;\n\n\tif (!mcryptd_check_internal(tb, &type, &mask))\n\t\treturn -EINVAL;\n\n\thalg = ahash_attr_alg(tb[1], type, mask);\n\tif (IS_ERR(halg))\n\t\treturn PTR_ERR(halg);\n\n\talg = &halg->base;\n\tpr_debug(\"crypto: mcryptd hash alg: %s\\n\", alg->cra_name);\n\tinst = mcryptd_alloc_instance(alg, ahash_instance_headroom(),\n\t\t\t\t\tsizeof(*ctx));\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tctx = ahash_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_init_ahash_spawn(&ctx->spawn, halg,\n\t\t\t\t      ahash_crypto_instance(inst));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\ttype = CRYPTO_ALG_ASYNC;\n\tif (alg->cra_flags & CRYPTO_ALG_INTERNAL)\n\t\ttype |= CRYPTO_ALG_INTERNAL;\n\tinst->alg.halg.base.cra_flags = type;\n\n\tinst->alg.halg.digestsize = halg->digestsize;\n\tinst->alg.halg.statesize = halg->statesize;\n\tinst->alg.halg.base.cra_ctxsize = sizeof(struct mcryptd_hash_ctx);\n\n\tinst->alg.halg.base.cra_init = mcryptd_hash_init_tfm;\n\tinst->alg.halg.base.cra_exit = mcryptd_hash_exit_tfm;\n\n\tinst->alg.init   = mcryptd_hash_init_enqueue;\n\tinst->alg.update = mcryptd_hash_update_enqueue;\n\tinst->alg.final  = mcryptd_hash_final_enqueue;\n\tinst->alg.finup  = mcryptd_hash_finup_enqueue;\n\tinst->alg.export = mcryptd_hash_export;\n\tinst->alg.import = mcryptd_hash_import;\n\tinst->alg.setkey = mcryptd_hash_setkey;\n\tinst->alg.digest = mcryptd_hash_digest_enqueue;\n\n\terr = ahash_register_instance(tmpl, inst);\n\tif (err) {\n\t\tcrypto_drop_ahash(&ctx->spawn);\nout_free_inst:\n\t\tkfree(inst);\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}\n\nstatic struct mcryptd_queue mqueue;\n\nstatic int mcryptd_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn PTR_ERR(algt);\n\n\tswitch (algt->type & algt->mask & CRYPTO_ALG_TYPE_MASK) {\n\tcase CRYPTO_ALG_TYPE_DIGEST:\n\t\treturn mcryptd_create_hash(tmpl, tb, &mqueue);\n\tbreak;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic void mcryptd_free(struct crypto_instance *inst)\n{\n\tstruct mcryptd_instance_ctx *ctx = crypto_instance_ctx(inst);\n\tstruct hashd_instance_ctx *hctx = crypto_instance_ctx(inst);\n\n\tswitch (inst->alg.cra_flags & CRYPTO_ALG_TYPE_MASK) {\n\tcase CRYPTO_ALG_TYPE_AHASH:\n\t\tcrypto_drop_ahash(&hctx->spawn);\n\t\tkfree(ahash_instance(inst));\n\t\treturn;\n\tdefault:\n\t\tcrypto_drop_spawn(&ctx->spawn);\n\t\tkfree(inst);\n\t}\n}\n\nstatic struct crypto_template mcryptd_tmpl = {\n\t.name = \"mcryptd\",\n\t.create = mcryptd_create,\n\t.free = mcryptd_free,\n\t.module = THIS_MODULE,\n};\n\nstruct mcryptd_ahash *mcryptd_alloc_ahash(const char *alg_name,\n\t\t\t\t\tu32 type, u32 mask)\n{\n\tchar mcryptd_alg_name[CRYPTO_MAX_ALG_NAME];\n\tstruct crypto_ahash *tfm;\n\n\tif (snprintf(mcryptd_alg_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"mcryptd(%s)\", alg_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-EINVAL);\n\ttfm = crypto_alloc_ahash(mcryptd_alg_name, type, mask);\n\tif (IS_ERR(tfm))\n\t\treturn ERR_CAST(tfm);\n\tif (tfm->base.__crt_alg->cra_module != THIS_MODULE) {\n\t\tcrypto_free_ahash(tfm);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn __mcryptd_ahash_cast(tfm);\n}\nEXPORT_SYMBOL_GPL(mcryptd_alloc_ahash);\n\nint ahash_mcryptd_digest(struct ahash_request *desc)\n{\n\treturn crypto_ahash_init(desc) ?: ahash_mcryptd_finup(desc);\n}\n\nint ahash_mcryptd_update(struct ahash_request *desc)\n{\n\t/* alignment is to be done by multi-buffer crypto algorithm if needed */\n\n\treturn crypto_ahash_update(desc);\n}\n\nint ahash_mcryptd_finup(struct ahash_request *desc)\n{\n\t/* alignment is to be done by multi-buffer crypto algorithm if needed */\n\n\treturn crypto_ahash_finup(desc);\n}\n\nint ahash_mcryptd_final(struct ahash_request *desc)\n{\n\t/* alignment is to be done by multi-buffer crypto algorithm if needed */\n\n\treturn crypto_ahash_final(desc);\n}\n\nstruct crypto_ahash *mcryptd_ahash_child(struct mcryptd_ahash *tfm)\n{\n\tstruct mcryptd_hash_ctx *ctx = crypto_ahash_ctx(&tfm->base);\n\n\treturn ctx->child;\n}\nEXPORT_SYMBOL_GPL(mcryptd_ahash_child);\n\nstruct ahash_request *mcryptd_ahash_desc(struct ahash_request *req)\n{\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\treturn &rctx->areq;\n}\nEXPORT_SYMBOL_GPL(mcryptd_ahash_desc);\n\nvoid mcryptd_free_ahash(struct mcryptd_ahash *tfm)\n{\n\tcrypto_free_ahash(&tfm->base);\n}\nEXPORT_SYMBOL_GPL(mcryptd_free_ahash);\n\nstatic int __init mcryptd_init(void)\n{\n\tint err, cpu;\n\tstruct mcryptd_flush_list *flist;\n\n\tmcryptd_flist = alloc_percpu(struct mcryptd_flush_list);\n\tfor_each_possible_cpu(cpu) {\n\t\tflist = per_cpu_ptr(mcryptd_flist, cpu);\n\t\tINIT_LIST_HEAD(&flist->list);\n\t\tmutex_init(&flist->lock);\n\t}\n\n\terr = mcryptd_init_queue(&mqueue, MCRYPTD_MAX_CPU_QLEN);\n\tif (err) {\n\t\tfree_percpu(mcryptd_flist);\n\t\treturn err;\n\t}\n\n\terr = crypto_register_template(&mcryptd_tmpl);\n\tif (err) {\n\t\tmcryptd_fini_queue(&mqueue);\n\t\tfree_percpu(mcryptd_flist);\n\t}\n\n\treturn err;\n}\n\nstatic void __exit mcryptd_exit(void)\n{\n\tmcryptd_fini_queue(&mqueue);\n\tcrypto_unregister_template(&mcryptd_tmpl);\n\tfree_percpu(mcryptd_flist);\n}\n\nsubsys_initcall(mcryptd_init);\nmodule_exit(mcryptd_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Software async multibuffer crypto daemon\");\nMODULE_ALIAS_CRYPTO(\"mcryptd\");\n"], "filenames": ["crypto/mcryptd.c"], "buggy_code_start_loc": [257], "buggy_code_end_loc": [496], "fixing_code_start_loc": [257], "fixing_code_end_loc": [501], "type": "CWE-476", "message": "crypto/mcryptd.c in the Linux kernel before 4.8.15 allows local users to cause a denial of service (NULL pointer dereference and system crash) by using an AF_ALG socket with an incompatible algorithm, as demonstrated by mcryptd(md5).", "other": {"cve": {"id": "CVE-2016-10147", "sourceIdentifier": "secalert@redhat.com", "published": "2017-01-18T21:59:00.167", "lastModified": "2023-02-12T23:16:49.557", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "crypto/mcryptd.c in the Linux kernel before 4.8.15 allows local users to cause a denial of service (NULL pointer dereference and system crash) by using an AF_ALG socket with an incompatible algorithm, as demonstrated by mcryptd(md5)."}, {"lang": "es", "value": "crypto/mcryptd.c en el kernel de Linux en versiones anteriores a 4.8.15 permite a usuarios locales provocar una denegaci\u00f3n de servicio (referencia a puntero NULL y ca\u00edda del sistema) usando un socket AF_ALG con un algoritmo incompatible, seg\u00fan lo demostrado por mcryptd(md5)."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.8.14", "matchCriteriaId": "BED5892F-F01B-4B15-9D0E-00685567EE0C"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=48a992727d82cb7db076fa15d372178743b1f4cd", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "http://marc.info/?l=linux-crypto-vger&m=148063683310477&w=2", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.8.15", "source": "secalert@redhat.com", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2017/01/17/13", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/95677", "source": "secalert@redhat.com"}, {"url": "https://access.redhat.com/errata/RHSA-2017:1842", "source": "secalert@redhat.com"}, {"url": "https://access.redhat.com/errata/RHSA-2017:2077", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1404200", "source": "secalert@redhat.com", "tags": ["Issue Tracking"]}, {"url": "https://github.com/torvalds/linux/commit/48a992727d82cb7db076fa15d372178743b1f4cd", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/48a992727d82cb7db076fa15d372178743b1f4cd"}}