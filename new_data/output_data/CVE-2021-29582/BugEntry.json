{"buggy_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/math_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/type_traits.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/meta_support.h\"\n#include \"tensorflow/core/kernels/quantization_utils.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/bfloat16.h\"\n\nnamespace {\nenum {\n  QUANTIZE_MODE_MIN_COMBINED,\n  QUANTIZE_MODE_MIN_FIRST,\n  QUANTIZE_MODE_SCALED,\n};\n}  // namespace\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename T>\nT Cast(float v) {\n  return v;\n}\n\ntemplate <>\nbfloat16 Cast<bfloat16>(float v) {\n  return bfloat16(v);\n}\n\ntemplate <typename Device, typename T, typename S>\nclass DequantizeOp : public OpKernel {\n public:\n  explicit DequantizeOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    string mode_string;\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"mode\", &mode_string));\n    OP_REQUIRES(\n        ctx,\n        (ctx->output_type(0) == DT_FLOAT || ctx->output_type(0) == DT_BFLOAT16),\n        errors::InvalidArgument(\"Output type must be bfloat16 or float,\"\n                                \" is '\" +\n                                DataTypeString(ctx->output_type(0)) + \"'\"));\n\n    need_cast_ = true;\n    if (ctx->output_type(0) == DT_FLOAT) {\n      need_cast_ = false;\n      OP_REQUIRES(ctx,\n                  (mode_string == \"MIN_COMBINED\" ||\n                   mode_string == \"MIN_FIRST\" || mode_string == \"SCALED\"),\n                  errors::InvalidArgument(\"Mode string must be 'MIN_COMBINED',\"\n                                          \" 'MIN_FIRST', or 'SCALED', is '\" +\n                                          mode_string + \"'\"));\n    } else {\n      OP_REQUIRES(\n          ctx, (mode_string == \"MIN_COMBINED\"),\n          errors::InvalidArgument(\"When output type is bfloat16, Mode\"\n                                  \" string must be 'MIN_COMBINED', is '\" +\n                                  mode_string + \"'\"));\n    }\n\n    if (mode_string == \"MIN_COMBINED\") {\n      mode_ = QUANTIZE_MODE_MIN_COMBINED;\n    } else if (mode_string == \"MIN_FIRST\") {\n      mode_ = QUANTIZE_MODE_MIN_FIRST;\n    } else if (mode_string == \"SCALED\") {\n      mode_ = QUANTIZE_MODE_SCALED;\n    }\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"narrow_range\", &narrow_range_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"axis\", &axis_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    const Tensor& input_min_tensor = ctx->input(1);\n    const Tensor& input_max_tensor = ctx->input(2);\n\n    int num_slices = 1;\n    if (axis_ > -1) {\n      num_slices = input.dim_size(axis_);\n    }\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n    Tensor float_output =\n        need_cast_ ? tensorflow::Tensor(DT_FLOAT, input.shape()) : *output;\n    if (num_slices == 1) {\n      const float min_range = input_min_tensor.flat<float>()(0);\n      const float max_range = input_max_tensor.flat<float>()(0);\n      DequantizeTensor(ctx, input, min_range, max_range, &float_output);\n    } else {\n      OP_REQUIRES(ctx, mode_ != QUANTIZE_MODE_MIN_FIRST,\n                  errors::Unimplemented(\"MIN_FIRST mode is not implemented for \"\n                                        \"Dequantize with axis != -1.\"));\n\n      int64 pre_dim = 1, post_dim = 1;\n      for (int i = 0; i < axis_; ++i) {\n        pre_dim *= float_output.dim_size(i);\n      }\n      for (int i = axis_ + 1; i < float_output.dims(); ++i) {\n        post_dim *= float_output.dim_size(i);\n      }\n      auto input_tensor = input.template bit_casted_shaped<T, 3>(\n          {pre_dim, num_slices, post_dim});\n      auto output_tensor =\n          float_output.flat_inner_outer_dims<float, 3>(axis_ - 1);\n      auto min_ranges = input_min_tensor.vec<float>();\n      auto max_ranges = input_max_tensor.vec<float>();\n      for (int i = 0; i < num_slices; ++i) {\n        DequantizeSlice(ctx->eigen_device<Device>(), ctx,\n                        input_tensor.template chip<1>(i), min_ranges(i),\n                        max_ranges(i), output_tensor.template chip<1>(i));\n      }\n    }\n    if (need_cast_) {\n      S* out_ptr = output->flat<S>().data();\n      float* in_ptr = float_output.flat<float>().data();\n      for (int64 i = 0; i < float_output.NumElements(); ++i) {\n        out_ptr[i] = static_cast<S>(in_ptr[i]);\n      }\n    }\n  }\n\n  void DequantizeTensor(OpKernelContext* ctx, const Tensor& input,\n                        const float min_range, const float max_range,\n                        Tensor* output) {\n    const float half_range =\n        !std::is_signed<T>::value\n            ? 0.0f\n            : (static_cast<float>(std::numeric_limits<T>::max()) -\n               std::numeric_limits<T>::min() + 1) /\n                  2.0f;\n\n    if (mode_ == QUANTIZE_MODE_MIN_COMBINED) {\n      const float scale_factor =\n          (max_range - min_range) /\n          (static_cast<float>(std::numeric_limits<T>::max()) -\n           std::numeric_limits<T>::min());\n\n      const auto& input_tensor = input.flat<T>();\n      output->flat<float>() =\n          ((input_tensor.template cast<float>() + half_range) * scale_factor) +\n          min_range;\n\n    } else if (mode_ == QUANTIZE_MODE_MIN_FIRST) {\n      if (meta::IsSupportedAndEnabled() && std::is_same<T, quint8>()) {\n        auto input_ui8_array = input.flat<quint8>();\n        meta::Dequantize(ctx, input_ui8_array.data(), input_ui8_array.size(),\n                         min_range, max_range, output->flat<float>().data());\n      } else {\n        QuantizedTensorToFloatInPlaceUsingEigen<T>(\n            ctx->template eigen_device<Device>(), input, min_range, max_range,\n            output);\n      }\n    } else if (mode_ == QUANTIZE_MODE_SCALED) {\n      const int min_output_value =\n          std::numeric_limits<T>::min() + (narrow_range_ ? 1 : 0);\n      const float scale_factor =\n          std::numeric_limits<T>::min() == 0\n              ? (max_range / std::numeric_limits<T>::max())\n              : std::max(min_range / min_output_value,\n                         max_range / std::numeric_limits<T>::max());\n      const auto& input_tensor = input.flat<T>();\n      output->flat<float>() =\n          input_tensor.template cast<int>().template cast<float>() *\n          scale_factor;\n    }\n  }\n\n  template <typename ConstVec, typename Vec>\n  void DequantizeSlice(const Device& d, OpKernelContext* ctx,\n                       const ConstVec& input, float min_range, float max_range,\n                       Vec output) {\n    // TODO(pauldonnelly): Factor out the similar calculations in quantize,\n    //   dequantize and quantize_and_dequantize ops.\n    const float half_range =\n        !std::is_signed<T>::value\n            ? 0.0f\n            : (static_cast<float>(std::numeric_limits<T>::max()) -\n               std::numeric_limits<T>::min() + 1) /\n                  2.0f;\n\n    if (mode_ == QUANTIZE_MODE_MIN_COMBINED) {\n      const float scale_factor =\n          (max_range - min_range) /\n          (static_cast<float>(std::numeric_limits<T>::max()) -\n           std::numeric_limits<T>::min());\n\n      output.device(d) =\n          ((input.template cast<float>() + half_range) * scale_factor) +\n          min_range;\n    } else if (mode_ == QUANTIZE_MODE_SCALED) {\n      const int min_output_value =\n          std::numeric_limits<T>::min() + (narrow_range_ ? 1 : 0);\n      const float scale_factor =\n          std::numeric_limits<T>::min() == 0\n              ? (max_range / std::numeric_limits<T>::max())\n              : std::max(min_range / min_output_value,\n                         max_range / std::numeric_limits<T>::max());\n      output.device(d) = input.template cast<float>() * scale_factor;\n    }\n  }\n\n private:\n  int mode_;\n  int axis_;\n  bool narrow_range_;\n  bool need_cast_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint8>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint8, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint8>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint8, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint16>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint16, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint16>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint16, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint32>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint32, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint8>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint8, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint8>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint8, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint16>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint16, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint16>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint16, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint32>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint32, bfloat16>);\n}  // namespace tensorflow\n"], "fixing_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/math_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/type_traits.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/meta_support.h\"\n#include \"tensorflow/core/kernels/quantization_utils.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/bfloat16.h\"\n\nnamespace {\nenum {\n  QUANTIZE_MODE_MIN_COMBINED,\n  QUANTIZE_MODE_MIN_FIRST,\n  QUANTIZE_MODE_SCALED,\n};\n}  // namespace\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename T>\nT Cast(float v) {\n  return v;\n}\n\ntemplate <>\nbfloat16 Cast<bfloat16>(float v) {\n  return bfloat16(v);\n}\n\ntemplate <typename Device, typename T, typename S>\nclass DequantizeOp : public OpKernel {\n public:\n  explicit DequantizeOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    string mode_string;\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"mode\", &mode_string));\n    OP_REQUIRES(\n        ctx,\n        (ctx->output_type(0) == DT_FLOAT || ctx->output_type(0) == DT_BFLOAT16),\n        errors::InvalidArgument(\"Output type must be bfloat16 or float,\"\n                                \" is '\" +\n                                DataTypeString(ctx->output_type(0)) + \"'\"));\n\n    need_cast_ = true;\n    if (ctx->output_type(0) == DT_FLOAT) {\n      need_cast_ = false;\n      OP_REQUIRES(ctx,\n                  (mode_string == \"MIN_COMBINED\" ||\n                   mode_string == \"MIN_FIRST\" || mode_string == \"SCALED\"),\n                  errors::InvalidArgument(\"Mode string must be 'MIN_COMBINED',\"\n                                          \" 'MIN_FIRST', or 'SCALED', is '\" +\n                                          mode_string + \"'\"));\n    } else {\n      OP_REQUIRES(\n          ctx, (mode_string == \"MIN_COMBINED\"),\n          errors::InvalidArgument(\"When output type is bfloat16, Mode\"\n                                  \" string must be 'MIN_COMBINED', is '\" +\n                                  mode_string + \"'\"));\n    }\n\n    if (mode_string == \"MIN_COMBINED\") {\n      mode_ = QUANTIZE_MODE_MIN_COMBINED;\n    } else if (mode_string == \"MIN_FIRST\") {\n      mode_ = QUANTIZE_MODE_MIN_FIRST;\n    } else if (mode_string == \"SCALED\") {\n      mode_ = QUANTIZE_MODE_SCALED;\n    }\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"narrow_range\", &narrow_range_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"axis\", &axis_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    const Tensor& input_min_tensor = ctx->input(1);\n    const Tensor& input_max_tensor = ctx->input(2);\n\n    int num_slices = 1;\n    if (axis_ > -1) {\n      num_slices = input.dim_size(axis_);\n    }\n    OP_REQUIRES(ctx, input_min_tensor.NumElements() == num_slices,\n                errors::InvalidArgument(\n                    \"input_min_tensor must have as many elements as input on \"\n                    \"the dequantization axis (\",\n                    axis_, \"), got \", input_min_tensor.NumElements(),\n                    \", expected \", num_slices));\n    OP_REQUIRES(ctx, input_max_tensor.NumElements() == num_slices,\n                errors::InvalidArgument(\n                    \"input_max_tensor must have as many elements as input on \"\n                    \"the dequantization axis (\",\n                    axis_, \"), got \", input_max_tensor.NumElements(),\n                    \", expected \", num_slices));\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n    Tensor float_output =\n        need_cast_ ? tensorflow::Tensor(DT_FLOAT, input.shape()) : *output;\n    if (num_slices == 1) {\n      const float min_range = input_min_tensor.flat<float>()(0);\n      const float max_range = input_max_tensor.flat<float>()(0);\n      DequantizeTensor(ctx, input, min_range, max_range, &float_output);\n    } else {\n      OP_REQUIRES(ctx, mode_ != QUANTIZE_MODE_MIN_FIRST,\n                  errors::Unimplemented(\"MIN_FIRST mode is not implemented for \"\n                                        \"Dequantize with axis != -1.\"));\n\n      int64 pre_dim = 1, post_dim = 1;\n      for (int i = 0; i < axis_; ++i) {\n        pre_dim *= float_output.dim_size(i);\n      }\n      for (int i = axis_ + 1; i < float_output.dims(); ++i) {\n        post_dim *= float_output.dim_size(i);\n      }\n      auto input_tensor = input.template bit_casted_shaped<T, 3>(\n          {pre_dim, num_slices, post_dim});\n      auto output_tensor =\n          float_output.flat_inner_outer_dims<float, 3>(axis_ - 1);\n      auto min_ranges = input_min_tensor.vec<float>();\n      auto max_ranges = input_max_tensor.vec<float>();\n      for (int i = 0; i < num_slices; ++i) {\n        DequantizeSlice(ctx->eigen_device<Device>(), ctx,\n                        input_tensor.template chip<1>(i), min_ranges(i),\n                        max_ranges(i), output_tensor.template chip<1>(i));\n      }\n    }\n    if (need_cast_) {\n      S* out_ptr = output->flat<S>().data();\n      float* in_ptr = float_output.flat<float>().data();\n      for (int64 i = 0; i < float_output.NumElements(); ++i) {\n        out_ptr[i] = static_cast<S>(in_ptr[i]);\n      }\n    }\n  }\n\n  void DequantizeTensor(OpKernelContext* ctx, const Tensor& input,\n                        const float min_range, const float max_range,\n                        Tensor* output) {\n    const float half_range =\n        !std::is_signed<T>::value\n            ? 0.0f\n            : (static_cast<float>(std::numeric_limits<T>::max()) -\n               std::numeric_limits<T>::min() + 1) /\n                  2.0f;\n\n    if (mode_ == QUANTIZE_MODE_MIN_COMBINED) {\n      const float scale_factor =\n          (max_range - min_range) /\n          (static_cast<float>(std::numeric_limits<T>::max()) -\n           std::numeric_limits<T>::min());\n\n      const auto& input_tensor = input.flat<T>();\n      output->flat<float>() =\n          ((input_tensor.template cast<float>() + half_range) * scale_factor) +\n          min_range;\n\n    } else if (mode_ == QUANTIZE_MODE_MIN_FIRST) {\n      if (meta::IsSupportedAndEnabled() && std::is_same<T, quint8>()) {\n        auto input_ui8_array = input.flat<quint8>();\n        meta::Dequantize(ctx, input_ui8_array.data(), input_ui8_array.size(),\n                         min_range, max_range, output->flat<float>().data());\n      } else {\n        QuantizedTensorToFloatInPlaceUsingEigen<T>(\n            ctx->template eigen_device<Device>(), input, min_range, max_range,\n            output);\n      }\n    } else if (mode_ == QUANTIZE_MODE_SCALED) {\n      const int min_output_value =\n          std::numeric_limits<T>::min() + (narrow_range_ ? 1 : 0);\n      const float scale_factor =\n          std::numeric_limits<T>::min() == 0\n              ? (max_range / std::numeric_limits<T>::max())\n              : std::max(min_range / min_output_value,\n                         max_range / std::numeric_limits<T>::max());\n      const auto& input_tensor = input.flat<T>();\n      output->flat<float>() =\n          input_tensor.template cast<int>().template cast<float>() *\n          scale_factor;\n    }\n  }\n\n  template <typename ConstVec, typename Vec>\n  void DequantizeSlice(const Device& d, OpKernelContext* ctx,\n                       const ConstVec& input, float min_range, float max_range,\n                       Vec output) {\n    // TODO(pauldonnelly): Factor out the similar calculations in quantize,\n    //   dequantize and quantize_and_dequantize ops.\n    const float half_range =\n        !std::is_signed<T>::value\n            ? 0.0f\n            : (static_cast<float>(std::numeric_limits<T>::max()) -\n               std::numeric_limits<T>::min() + 1) /\n                  2.0f;\n\n    if (mode_ == QUANTIZE_MODE_MIN_COMBINED) {\n      const float scale_factor =\n          (max_range - min_range) /\n          (static_cast<float>(std::numeric_limits<T>::max()) -\n           std::numeric_limits<T>::min());\n\n      output.device(d) =\n          ((input.template cast<float>() + half_range) * scale_factor) +\n          min_range;\n    } else if (mode_ == QUANTIZE_MODE_SCALED) {\n      const int min_output_value =\n          std::numeric_limits<T>::min() + (narrow_range_ ? 1 : 0);\n      const float scale_factor =\n          std::numeric_limits<T>::min() == 0\n              ? (max_range / std::numeric_limits<T>::max())\n              : std::max(min_range / min_output_value,\n                         max_range / std::numeric_limits<T>::max());\n      output.device(d) = input.template cast<float>() * scale_factor;\n    }\n  }\n\n private:\n  int mode_;\n  int axis_;\n  bool narrow_range_;\n  bool need_cast_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint8>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint8, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint8>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint8, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint16>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint16, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint16>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint16, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint32>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint32, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint8>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint8, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint8>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint8, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint16>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint16, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint16>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint16, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint32>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint32, bfloat16>);\n}  // namespace tensorflow\n"], "filenames": ["tensorflow/core/kernels/dequantize_op.cc"], "buggy_code_start_loc": [100], "buggy_code_end_loc": [100], "fixing_code_start_loc": [101], "fixing_code_end_loc": [113], "type": "CWE-125", "message": "TensorFlow is an end-to-end open source platform for machine learning. Due to lack of validation in `tf.raw_ops.Dequantize`, an attacker can trigger a read from outside of bounds of heap allocated data. The implementation(https://github.com/tensorflow/tensorflow/blob/26003593aa94b1742f34dc22ce88a1e17776a67d/tensorflow/core/kernels/dequantize_op.cc#L106-L131) accesses the `min_range` and `max_range` tensors in parallel but fails to check that they have the same shape. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-29582", "sourceIdentifier": "security-advisories@github.com", "published": "2021-05-14T20:15:14.390", "lastModified": "2021-05-20T15:39:49.547", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an end-to-end open source platform for machine learning. Due to lack of validation in `tf.raw_ops.Dequantize`, an attacker can trigger a read from outside of bounds of heap allocated data. The implementation(https://github.com/tensorflow/tensorflow/blob/26003593aa94b1742f34dc22ce88a1e17776a67d/tensorflow/core/kernels/dequantize_op.cc#L106-L131) accesses the `min_range` and `max_range` tensors in parallel but fails to check that they have the same shape. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto de extremo a extremo para el aprendizaje autom\u00e1tico.&#xa0;Debido a una falta de comprobaci\u00f3n en la funci\u00f3n \"tf.raw_ops.Dequantize\", un atacante puede desencadenar una lectura desde fuera de l\u00edmites de los datos asignados a la pila.&#xa0;La implementaci\u00f3n (https://github.com/tensorflow/tensorflow/blob/26003593aa94b1742f34dc22ce88a1e17776a67d/tensorflow/core/kernels/dequantize_op.cc#L106-L131) accede a los tensores \"min_range\" y \"max_range\" en paralelo pero no puede comprobar eso presentan una misma forma.&#xa0;La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.5.0.&#xa0;Tambi\u00e9n seleccionaremos este compromiso en TensorFlow versi\u00f3n 2.4.2, TensorFlow versi\u00f3n 2.3.3, TensorFlow versi\u00f3n 2.2.3 y TensorFlow versi\u00f3n 2.1.4, ya que estos tambi\u00e9n est\u00e1n afectados y a\u00fan est\u00e1n en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.1, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.2}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:N/I:N/A:L", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "LOW", "baseScore": 2.5, "baseSeverity": "LOW"}, "exploitabilityScore": 1.0, "impactScore": 1.4}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 3.6}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 4.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-125"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.1.4", "matchCriteriaId": "323ABCCE-24EB-47CC-87F6-48C101477587"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.2.0", "versionEndExcluding": "2.2.3", "matchCriteriaId": "64ABA90C-0649-4BB0-89C9-83C14BBDCC0F"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.3", "matchCriteriaId": "0F83E0CF-CBF6-4C24-8683-3E7A5DC95BA9"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.2", "matchCriteriaId": "8259531B-A8AC-4F8B-B60F-B69DE4767C03"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/5899741d0421391ca878da47907b1452f06aaf1b", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-c45w-2wxr-pp53", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/5899741d0421391ca878da47907b1452f06aaf1b"}}