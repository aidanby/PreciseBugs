{"buggy_code": ["/*\n * Hardware performance events for the Alpha.\n *\n * We implement HW counts on the EV67 and subsequent CPUs only.\n *\n * (C) 2010 Michael J. Cree\n *\n * Somewhat based on the Sparc code, and to a lesser extent the PowerPC and\n * ARM code, which are copyright by their respective authors.\n */\n\n#include <linux/perf_event.h>\n#include <linux/kprobes.h>\n#include <linux/kernel.h>\n#include <linux/kdebug.h>\n#include <linux/mutex.h>\n#include <linux/init.h>\n\n#include <asm/hwrpb.h>\n#include <asm/atomic.h>\n#include <asm/irq.h>\n#include <asm/irq_regs.h>\n#include <asm/pal.h>\n#include <asm/wrperfmon.h>\n#include <asm/hw_irq.h>\n\n\n/* The maximum number of PMCs on any Alpha CPU whatsoever. */\n#define MAX_HWEVENTS 3\n#define PMC_NO_INDEX -1\n\n/* For tracking PMCs and the hw events they monitor on each CPU. */\nstruct cpu_hw_events {\n\tint\t\t\tenabled;\n\t/* Number of events scheduled; also number entries valid in arrays below. */\n\tint\t\t\tn_events;\n\t/* Number events added since last hw_perf_disable(). */\n\tint\t\t\tn_added;\n\t/* Events currently scheduled. */\n\tstruct perf_event\t*event[MAX_HWEVENTS];\n\t/* Event type of each scheduled event. */\n\tunsigned long\t\tevtype[MAX_HWEVENTS];\n\t/* Current index of each scheduled event; if not yet determined\n\t * contains PMC_NO_INDEX.\n\t */\n\tint\t\t\tcurrent_idx[MAX_HWEVENTS];\n\t/* The active PMCs' config for easy use with wrperfmon(). */\n\tunsigned long\t\tconfig;\n\t/* The active counters' indices for easy use with wrperfmon(). */\n\tunsigned long\t\tidx_mask;\n};\nDEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events);\n\n\n\n/*\n * A structure to hold the description of the PMCs available on a particular\n * type of Alpha CPU.\n */\nstruct alpha_pmu_t {\n\t/* Mapping of the perf system hw event types to indigenous event types */\n\tconst int *event_map;\n\t/* The number of entries in the event_map */\n\tint  max_events;\n\t/* The number of PMCs on this Alpha */\n\tint  num_pmcs;\n\t/*\n\t * All PMC counters reside in the IBOX register PCTR.  This is the\n\t * LSB of the counter.\n\t */\n\tint  pmc_count_shift[MAX_HWEVENTS];\n\t/*\n\t * The mask that isolates the PMC bits when the LSB of the counter\n\t * is shifted to bit 0.\n\t */\n\tunsigned long pmc_count_mask[MAX_HWEVENTS];\n\t/* The maximum period the PMC can count. */\n\tunsigned long pmc_max_period[MAX_HWEVENTS];\n\t/*\n\t * The maximum value that may be written to the counter due to\n\t * hardware restrictions is pmc_max_period - pmc_left.\n\t */\n\tlong pmc_left[3];\n\t /* Subroutine for allocation of PMCs.  Enforces constraints. */\n\tint (*check_constraints)(struct perf_event **, unsigned long *, int);\n};\n\n/*\n * The Alpha CPU PMU description currently in operation.  This is set during\n * the boot process to the specific CPU of the machine.\n */\nstatic const struct alpha_pmu_t *alpha_pmu;\n\n\n#define HW_OP_UNSUPPORTED -1\n\n/*\n * The hardware description of the EV67, EV68, EV69, EV7 and EV79 PMUs\n * follow. Since they are identical we refer to them collectively as the\n * EV67 henceforth.\n */\n\n/*\n * EV67 PMC event types\n *\n * There is no one-to-one mapping of the possible hw event types to the\n * actual codes that are used to program the PMCs hence we introduce our\n * own hw event type identifiers.\n */\nenum ev67_pmc_event_type {\n\tEV67_CYCLES = 1,\n\tEV67_INSTRUCTIONS,\n\tEV67_BCACHEMISS,\n\tEV67_MBOXREPLAY,\n\tEV67_LAST_ET\n};\n#define EV67_NUM_EVENT_TYPES (EV67_LAST_ET-EV67_CYCLES)\n\n\n/* Mapping of the hw event types to the perf tool interface */\nstatic const int ev67_perfmon_event_map[] = {\n\t[PERF_COUNT_HW_CPU_CYCLES]\t = EV67_CYCLES,\n\t[PERF_COUNT_HW_INSTRUCTIONS]\t = EV67_INSTRUCTIONS,\n\t[PERF_COUNT_HW_CACHE_REFERENCES] = HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_CACHE_MISSES]\t = EV67_BCACHEMISS,\n};\n\nstruct ev67_mapping_t {\n\tint config;\n\tint idx;\n};\n\n/*\n * The mapping used for one event only - these must be in same order as enum\n * ev67_pmc_event_type definition.\n */\nstatic const struct ev67_mapping_t ev67_mapping[] = {\n\t{EV67_PCTR_INSTR_CYCLES, 1},\t /* EV67_CYCLES, */\n\t{EV67_PCTR_INSTR_CYCLES, 0},\t /* EV67_INSTRUCTIONS */\n\t{EV67_PCTR_INSTR_BCACHEMISS, 1}, /* EV67_BCACHEMISS */\n\t{EV67_PCTR_CYCLES_MBOX, 1}\t /* EV67_MBOXREPLAY */\n};\n\n\n/*\n * Check that a group of events can be simultaneously scheduled on to the\n * EV67 PMU.  Also allocate counter indices and config.\n */\nstatic int ev67_check_constraints(struct perf_event **event,\n\t\t\t\tunsigned long *evtype, int n_ev)\n{\n\tint idx0;\n\tunsigned long config;\n\n\tidx0 = ev67_mapping[evtype[0]-1].idx;\n\tconfig = ev67_mapping[evtype[0]-1].config;\n\tif (n_ev == 1)\n\t\tgoto success;\n\n\tBUG_ON(n_ev != 2);\n\n\tif (evtype[0] == EV67_MBOXREPLAY || evtype[1] == EV67_MBOXREPLAY) {\n\t\t/* MBOX replay traps must be on PMC 1 */\n\t\tidx0 = (evtype[0] == EV67_MBOXREPLAY) ? 1 : 0;\n\t\t/* Only cycles can accompany MBOX replay traps */\n\t\tif (evtype[idx0] == EV67_CYCLES) {\n\t\t\tconfig = EV67_PCTR_CYCLES_MBOX;\n\t\t\tgoto success;\n\t\t}\n\t}\n\n\tif (evtype[0] == EV67_BCACHEMISS || evtype[1] == EV67_BCACHEMISS) {\n\t\t/* Bcache misses must be on PMC 1 */\n\t\tidx0 = (evtype[0] == EV67_BCACHEMISS) ? 1 : 0;\n\t\t/* Only instructions can accompany Bcache misses */\n\t\tif (evtype[idx0] == EV67_INSTRUCTIONS) {\n\t\t\tconfig = EV67_PCTR_INSTR_BCACHEMISS;\n\t\t\tgoto success;\n\t\t}\n\t}\n\n\tif (evtype[0] == EV67_INSTRUCTIONS || evtype[1] == EV67_INSTRUCTIONS) {\n\t\t/* Instructions must be on PMC 0 */\n\t\tidx0 = (evtype[0] == EV67_INSTRUCTIONS) ? 0 : 1;\n\t\t/* By this point only cycles can accompany instructions */\n\t\tif (evtype[idx0^1] == EV67_CYCLES) {\n\t\t\tconfig = EV67_PCTR_INSTR_CYCLES;\n\t\t\tgoto success;\n\t\t}\n\t}\n\n\t/* Otherwise, darn it, there is a conflict.  */\n\treturn -1;\n\nsuccess:\n\tevent[0]->hw.idx = idx0;\n\tevent[0]->hw.config_base = config;\n\tif (n_ev == 2) {\n\t\tevent[1]->hw.idx = idx0 ^ 1;\n\t\tevent[1]->hw.config_base = config;\n\t}\n\treturn 0;\n}\n\n\nstatic const struct alpha_pmu_t ev67_pmu = {\n\t.event_map = ev67_perfmon_event_map,\n\t.max_events = ARRAY_SIZE(ev67_perfmon_event_map),\n\t.num_pmcs = 2,\n\t.pmc_count_shift = {EV67_PCTR_0_COUNT_SHIFT, EV67_PCTR_1_COUNT_SHIFT, 0},\n\t.pmc_count_mask = {EV67_PCTR_0_COUNT_MASK,  EV67_PCTR_1_COUNT_MASK,  0},\n\t.pmc_max_period = {(1UL<<20) - 1, (1UL<<20) - 1, 0},\n\t.pmc_left = {16, 4, 0},\n\t.check_constraints = ev67_check_constraints\n};\n\n\n\n/*\n * Helper routines to ensure that we read/write only the correct PMC bits\n * when calling the wrperfmon PALcall.\n */\nstatic inline void alpha_write_pmc(int idx, unsigned long val)\n{\n\tval &= alpha_pmu->pmc_count_mask[idx];\n\tval <<= alpha_pmu->pmc_count_shift[idx];\n\tval |= (1<<idx);\n\twrperfmon(PERFMON_CMD_WRITE, val);\n}\n\nstatic inline unsigned long alpha_read_pmc(int idx)\n{\n\tunsigned long val;\n\n\tval = wrperfmon(PERFMON_CMD_READ, 0);\n\tval >>= alpha_pmu->pmc_count_shift[idx];\n\tval &= alpha_pmu->pmc_count_mask[idx];\n\treturn val;\n}\n\n/* Set a new period to sample over */\nstatic int alpha_perf_event_set_period(struct perf_event *event,\n\t\t\t\tstruct hw_perf_event *hwc, int idx)\n{\n\tlong left = local64_read(&hwc->period_left);\n\tlong period = hwc->sample_period;\n\tint ret = 0;\n\n\tif (unlikely(left <= -period)) {\n\t\tleft = period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\tif (unlikely(left <= 0)) {\n\t\tleft += period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\t/*\n\t * Hardware restrictions require that the counters must not be\n\t * written with values that are too close to the maximum period.\n\t */\n\tif (unlikely(left < alpha_pmu->pmc_left[idx]))\n\t\tleft = alpha_pmu->pmc_left[idx];\n\n\tif (left > (long)alpha_pmu->pmc_max_period[idx])\n\t\tleft = alpha_pmu->pmc_max_period[idx];\n\n\tlocal64_set(&hwc->prev_count, (unsigned long)(-left));\n\n\talpha_write_pmc(idx, (unsigned long)(-left));\n\n\tperf_event_update_userpage(event);\n\n\treturn ret;\n}\n\n\n/*\n * Calculates the count (the 'delta') since the last time the PMC was read.\n *\n * As the PMCs' full period can easily be exceeded within the perf system\n * sampling period we cannot use any high order bits as a guard bit in the\n * PMCs to detect overflow as is done by other architectures.  The code here\n * calculates the delta on the basis that there is no overflow when ovf is\n * zero.  The value passed via ovf by the interrupt handler corrects for\n * overflow.\n *\n * This can be racey on rare occasions -- a call to this routine can occur\n * with an overflowed counter just before the PMI service routine is called.\n * The check for delta negative hopefully always rectifies this situation.\n */\nstatic unsigned long alpha_perf_event_update(struct perf_event *event,\n\t\t\t\t\tstruct hw_perf_event *hwc, int idx, long ovf)\n{\n\tlong prev_raw_count, new_raw_count;\n\tlong delta;\n\nagain:\n\tprev_raw_count = local64_read(&hwc->prev_count);\n\tnew_raw_count = alpha_read_pmc(idx);\n\n\tif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\n\t\t\t     new_raw_count) != prev_raw_count)\n\t\tgoto again;\n\n\tdelta = (new_raw_count - (prev_raw_count & alpha_pmu->pmc_count_mask[idx])) + ovf;\n\n\t/* It is possible on very rare occasions that the PMC has overflowed\n\t * but the interrupt is yet to come.  Detect and fix this situation.\n\t */\n\tif (unlikely(delta < 0)) {\n\t\tdelta += alpha_pmu->pmc_max_period[idx] + 1;\n\t}\n\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &hwc->period_left);\n\n\treturn new_raw_count;\n}\n\n\n/*\n * Collect all HW events into the array event[].\n */\nstatic int collect_events(struct perf_event *group, int max_count,\n\t\t\t  struct perf_event *event[], unsigned long *evtype,\n\t\t\t  int *current_idx)\n{\n\tstruct perf_event *pe;\n\tint n = 0;\n\n\tif (!is_software_event(group)) {\n\t\tif (n >= max_count)\n\t\t\treturn -1;\n\t\tevent[n] = group;\n\t\tevtype[n] = group->hw.event_base;\n\t\tcurrent_idx[n++] = PMC_NO_INDEX;\n\t}\n\tlist_for_each_entry(pe, &group->sibling_list, group_entry) {\n\t\tif (!is_software_event(pe) && pe->state != PERF_EVENT_STATE_OFF) {\n\t\t\tif (n >= max_count)\n\t\t\t\treturn -1;\n\t\t\tevent[n] = pe;\n\t\t\tevtype[n] = pe->hw.event_base;\n\t\t\tcurrent_idx[n++] = PMC_NO_INDEX;\n\t\t}\n\t}\n\treturn n;\n}\n\n\n\n/*\n * Check that a group of events can be simultaneously scheduled on to the PMU.\n */\nstatic int alpha_check_constraints(struct perf_event **events,\n\t\t\t\t   unsigned long *evtypes, int n_ev)\n{\n\n\t/* No HW events is possible from hw_perf_group_sched_in(). */\n\tif (n_ev == 0)\n\t\treturn 0;\n\n\tif (n_ev > alpha_pmu->num_pmcs)\n\t\treturn -1;\n\n\treturn alpha_pmu->check_constraints(events, evtypes, n_ev);\n}\n\n\n/*\n * If new events have been scheduled then update cpuc with the new\n * configuration.  This may involve shifting cycle counts from one PMC to\n * another.\n */\nstatic void maybe_change_configuration(struct cpu_hw_events *cpuc)\n{\n\tint j;\n\n\tif (cpuc->n_added == 0)\n\t\treturn;\n\n\t/* Find counters that are moving to another PMC and update */\n\tfor (j = 0; j < cpuc->n_events; j++) {\n\t\tstruct perf_event *pe = cpuc->event[j];\n\n\t\tif (cpuc->current_idx[j] != PMC_NO_INDEX &&\n\t\t\tcpuc->current_idx[j] != pe->hw.idx) {\n\t\t\talpha_perf_event_update(pe, &pe->hw, cpuc->current_idx[j], 0);\n\t\t\tcpuc->current_idx[j] = PMC_NO_INDEX;\n\t\t}\n\t}\n\n\t/* Assign to counters all unassigned events. */\n\tcpuc->idx_mask = 0;\n\tfor (j = 0; j < cpuc->n_events; j++) {\n\t\tstruct perf_event *pe = cpuc->event[j];\n\t\tstruct hw_perf_event *hwc = &pe->hw;\n\t\tint idx = hwc->idx;\n\n\t\tif (cpuc->current_idx[j] == PMC_NO_INDEX) {\n\t\t\talpha_perf_event_set_period(pe, hwc, idx);\n\t\t\tcpuc->current_idx[j] = idx;\n\t\t}\n\n\t\tif (!(hwc->state & PERF_HES_STOPPED))\n\t\t\tcpuc->idx_mask |= (1<<cpuc->current_idx[j]);\n\t}\n\tcpuc->config = cpuc->event[0]->hw.config_base;\n}\n\n\n\n/* Schedule perf HW event on to PMU.\n *  - this function is called from outside this module via the pmu struct\n *    returned from perf event initialisation.\n */\nstatic int alpha_pmu_add(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint n0;\n\tint ret;\n\tunsigned long irq_flags;\n\n\t/*\n\t * The Sparc code has the IRQ disable first followed by the perf\n\t * disable, however this can lead to an overflowed counter with the\n\t * PMI disabled on rare occasions.  The alpha_perf_event_update()\n\t * routine should detect this situation by noting a negative delta,\n\t * nevertheless we disable the PMCs first to enable a potential\n\t * final PMI to occur before we disable interrupts.\n\t */\n\tperf_pmu_disable(event->pmu);\n\tlocal_irq_save(irq_flags);\n\n\t/* Default to error to be returned */\n\tret = -EAGAIN;\n\n\t/* Insert event on to PMU and if successful modify ret to valid return */\n\tn0 = cpuc->n_events;\n\tif (n0 < alpha_pmu->num_pmcs) {\n\t\tcpuc->event[n0] = event;\n\t\tcpuc->evtype[n0] = event->hw.event_base;\n\t\tcpuc->current_idx[n0] = PMC_NO_INDEX;\n\n\t\tif (!alpha_check_constraints(cpuc->event, cpuc->evtype, n0+1)) {\n\t\t\tcpuc->n_events++;\n\t\t\tcpuc->n_added++;\n\t\t\tret = 0;\n\t\t}\n\t}\n\n\thwc->state = PERF_HES_UPTODATE;\n\tif (!(flags & PERF_EF_START))\n\t\thwc->state |= PERF_HES_STOPPED;\n\n\tlocal_irq_restore(irq_flags);\n\tperf_pmu_enable(event->pmu);\n\n\treturn ret;\n}\n\n\n\n/* Disable performance monitoring unit\n *  - this function is called from outside this module via the pmu struct\n *    returned from perf event initialisation.\n */\nstatic void alpha_pmu_del(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tunsigned long irq_flags;\n\tint j;\n\n\tperf_pmu_disable(event->pmu);\n\tlocal_irq_save(irq_flags);\n\n\tfor (j = 0; j < cpuc->n_events; j++) {\n\t\tif (event == cpuc->event[j]) {\n\t\t\tint idx = cpuc->current_idx[j];\n\n\t\t\t/* Shift remaining entries down into the existing\n\t\t\t * slot.\n\t\t\t */\n\t\t\twhile (++j < cpuc->n_events) {\n\t\t\t\tcpuc->event[j - 1] = cpuc->event[j];\n\t\t\t\tcpuc->evtype[j - 1] = cpuc->evtype[j];\n\t\t\t\tcpuc->current_idx[j - 1] =\n\t\t\t\t\tcpuc->current_idx[j];\n\t\t\t}\n\n\t\t\t/* Absorb the final count and turn off the event. */\n\t\t\talpha_perf_event_update(event, hwc, idx, 0);\n\t\t\tperf_event_update_userpage(event);\n\n\t\t\tcpuc->idx_mask &= ~(1UL<<idx);\n\t\t\tcpuc->n_events--;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tlocal_irq_restore(irq_flags);\n\tperf_pmu_enable(event->pmu);\n}\n\n\nstatic void alpha_pmu_read(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\talpha_perf_event_update(event, hwc, hwc->idx, 0);\n}\n\n\nstatic void alpha_pmu_stop(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\tif (!(hwc->state & PERF_HES_STOPPED)) {\n\t\tcpuc->idx_mask &= ~(1UL<<hwc->idx);\n\t\thwc->state |= PERF_HES_STOPPED;\n\t}\n\n\tif ((flags & PERF_EF_UPDATE) && !(hwc->state & PERF_HES_UPTODATE)) {\n\t\talpha_perf_event_update(event, hwc, hwc->idx, 0);\n\t\thwc->state |= PERF_HES_UPTODATE;\n\t}\n\n\tif (cpuc->enabled)\n\t\twrperfmon(PERFMON_CMD_DISABLE, (1UL<<hwc->idx));\n}\n\n\nstatic void alpha_pmu_start(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\tif (WARN_ON_ONCE(!(hwc->state & PERF_HES_STOPPED)))\n\t\treturn;\n\n\tif (flags & PERF_EF_RELOAD) {\n\t\tWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\n\t\talpha_perf_event_set_period(event, hwc, hwc->idx);\n\t}\n\n\thwc->state = 0;\n\n\tcpuc->idx_mask |= 1UL<<hwc->idx;\n\tif (cpuc->enabled)\n\t\twrperfmon(PERFMON_CMD_ENABLE, (1UL<<hwc->idx));\n}\n\n\n/*\n * Check that CPU performance counters are supported.\n * - currently support EV67 and later CPUs.\n * - actually some later revisions of the EV6 have the same PMC model as the\n *     EV67 but we don't do suffiently deep CPU detection to detect them.\n *     Bad luck to the very few people who might have one, I guess.\n */\nstatic int supported_cpu(void)\n{\n\tstruct percpu_struct *cpu;\n\tunsigned long cputype;\n\n\t/* Get cpu type from HW */\n\tcpu = (struct percpu_struct *)((char *)hwrpb + hwrpb->processor_offset);\n\tcputype = cpu->type & 0xffffffff;\n\t/* Include all of EV67, EV68, EV7, EV79 and EV69 as supported. */\n\treturn (cputype >= EV67_CPU) && (cputype <= EV69_CPU);\n}\n\n\n\nstatic void hw_perf_event_destroy(struct perf_event *event)\n{\n\t/* Nothing to be done! */\n\treturn;\n}\n\n\n\nstatic int __hw_perf_event_init(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct perf_event *evts[MAX_HWEVENTS];\n\tunsigned long evtypes[MAX_HWEVENTS];\n\tint idx_rubbish_bin[MAX_HWEVENTS];\n\tint ev;\n\tint n;\n\n\t/* We only support a limited range of HARDWARE event types with one\n\t * only programmable via a RAW event type.\n\t */\n\tif (attr->type == PERF_TYPE_HARDWARE) {\n\t\tif (attr->config >= alpha_pmu->max_events)\n\t\t\treturn -EINVAL;\n\t\tev = alpha_pmu->event_map[attr->config];\n\t} else if (attr->type == PERF_TYPE_HW_CACHE) {\n\t\treturn -EOPNOTSUPP;\n\t} else if (attr->type == PERF_TYPE_RAW) {\n\t\tev = attr->config & 0xff;\n\t} else {\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (ev < 0) {\n\t\treturn ev;\n\t}\n\n\t/* The EV67 does not support mode exclusion */\n\tif (attr->exclude_kernel || attr->exclude_user\n\t\t\t|| attr->exclude_hv || attr->exclude_idle) {\n\t\treturn -EPERM;\n\t}\n\n\t/*\n\t * We place the event type in event_base here and leave calculation\n\t * of the codes to programme the PMU for alpha_pmu_enable() because\n\t * it is only then we will know what HW events are actually\n\t * scheduled on to the PMU.  At that point the code to programme the\n\t * PMU is put into config_base and the PMC to use is placed into\n\t * idx.  We initialise idx (below) to PMC_NO_INDEX to indicate that\n\t * it is yet to be determined.\n\t */\n\thwc->event_base = ev;\n\n\t/* Collect events in a group together suitable for calling\n\t * alpha_check_constraints() to verify that the group as a whole can\n\t * be scheduled on to the PMU.\n\t */\n\tn = 0;\n\tif (event->group_leader != event) {\n\t\tn = collect_events(event->group_leader,\n\t\t\t\talpha_pmu->num_pmcs - 1,\n\t\t\t\tevts, evtypes, idx_rubbish_bin);\n\t\tif (n < 0)\n\t\t\treturn -EINVAL;\n\t}\n\tevtypes[n] = hwc->event_base;\n\tevts[n] = event;\n\n\tif (alpha_check_constraints(evts, evtypes, n + 1))\n\t\treturn -EINVAL;\n\n\t/* Indicate that PMU config and idx are yet to be determined. */\n\thwc->config_base = 0;\n\thwc->idx = PMC_NO_INDEX;\n\n\tevent->destroy = hw_perf_event_destroy;\n\n\t/*\n\t * Most architectures reserve the PMU for their use at this point.\n\t * As there is no existing mechanism to arbitrate usage and there\n\t * appears to be no other user of the Alpha PMU we just assume\n\t * that we can just use it, hence a NO-OP here.\n\t *\n\t * Maybe an alpha_reserve_pmu() routine should be implemented but is\n\t * anything else ever going to use it?\n\t */\n\n\tif (!hwc->sample_period) {\n\t\thwc->sample_period = alpha_pmu->pmc_max_period[0];\n\t\thwc->last_period = hwc->sample_period;\n\t\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\t}\n\n\treturn 0;\n}\n\n/*\n * Main entry point to initialise a HW performance event.\n */\nstatic int alpha_pmu_event_init(struct perf_event *event)\n{\n\tint err;\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_RAW:\n\tcase PERF_TYPE_HARDWARE:\n\tcase PERF_TYPE_HW_CACHE:\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\tif (!alpha_pmu)\n\t\treturn -ENODEV;\n\n\t/* Do the real initialisation work. */\n\terr = __hw_perf_event_init(event);\n\n\treturn err;\n}\n\n/*\n * Main entry point - enable HW performance counters.\n */\nstatic void alpha_pmu_enable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\tif (cpuc->enabled)\n\t\treturn;\n\n\tcpuc->enabled = 1;\n\tbarrier();\n\n\tif (cpuc->n_events > 0) {\n\t\t/* Update cpuc with information from any new scheduled events. */\n\t\tmaybe_change_configuration(cpuc);\n\n\t\t/* Start counting the desired events. */\n\t\twrperfmon(PERFMON_CMD_LOGGING_OPTIONS, EV67_PCTR_MODE_AGGREGATE);\n\t\twrperfmon(PERFMON_CMD_DESIRED_EVENTS, cpuc->config);\n\t\twrperfmon(PERFMON_CMD_ENABLE, cpuc->idx_mask);\n\t}\n}\n\n\n/*\n * Main entry point - disable HW performance counters.\n */\n\nstatic void alpha_pmu_disable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\tif (!cpuc->enabled)\n\t\treturn;\n\n\tcpuc->enabled = 0;\n\tcpuc->n_added = 0;\n\n\twrperfmon(PERFMON_CMD_DISABLE, cpuc->idx_mask);\n}\n\nstatic struct pmu pmu = {\n\t.pmu_enable\t= alpha_pmu_enable,\n\t.pmu_disable\t= alpha_pmu_disable,\n\t.event_init\t= alpha_pmu_event_init,\n\t.add\t\t= alpha_pmu_add,\n\t.del\t\t= alpha_pmu_del,\n\t.start\t\t= alpha_pmu_start,\n\t.stop\t\t= alpha_pmu_stop,\n\t.read\t\t= alpha_pmu_read,\n};\n\n\n/*\n * Main entry point - don't know when this is called but it\n * obviously dumps debug info.\n */\nvoid perf_event_print_debug(void)\n{\n\tunsigned long flags;\n\tunsigned long pcr;\n\tint pcr0, pcr1;\n\tint cpu;\n\n\tif (!supported_cpu())\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tcpu = smp_processor_id();\n\n\tpcr = wrperfmon(PERFMON_CMD_READ, 0);\n\tpcr0 = (pcr >> alpha_pmu->pmc_count_shift[0]) & alpha_pmu->pmc_count_mask[0];\n\tpcr1 = (pcr >> alpha_pmu->pmc_count_shift[1]) & alpha_pmu->pmc_count_mask[1];\n\n\tpr_info(\"CPU#%d: PCTR0[%06x] PCTR1[%06x]\\n\", cpu, pcr0, pcr1);\n\n\tlocal_irq_restore(flags);\n}\n\n\n/*\n * Performance Monitoring Interrupt Service Routine called when a PMC\n * overflows.  The PMC that overflowed is passed in la_ptr.\n */\nstatic void alpha_perf_event_irq_handler(unsigned long la_ptr,\n\t\t\t\t\tstruct pt_regs *regs)\n{\n\tstruct cpu_hw_events *cpuc;\n\tstruct perf_sample_data data;\n\tstruct perf_event *event;\n\tstruct hw_perf_event *hwc;\n\tint idx, j;\n\n\t__get_cpu_var(irq_pmi_count)++;\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\n\t/* Completely counting through the PMC's period to trigger a new PMC\n\t * overflow interrupt while in this interrupt routine is utterly\n\t * disastrous!  The EV6 and EV67 counters are sufficiently large to\n\t * prevent this but to be really sure disable the PMCs.\n\t */\n\twrperfmon(PERFMON_CMD_DISABLE, cpuc->idx_mask);\n\n\t/* la_ptr is the counter that overflowed. */\n\tif (unlikely(la_ptr >= alpha_pmu->num_pmcs)) {\n\t\t/* This should never occur! */\n\t\tirq_err_count++;\n\t\tpr_warning(\"PMI: silly index %ld\\n\", la_ptr);\n\t\twrperfmon(PERFMON_CMD_ENABLE, cpuc->idx_mask);\n\t\treturn;\n\t}\n\n\tidx = la_ptr;\n\n\tperf_sample_data_init(&data, 0);\n\tfor (j = 0; j < cpuc->n_events; j++) {\n\t\tif (cpuc->current_idx[j] == idx)\n\t\t\tbreak;\n\t}\n\n\tif (unlikely(j == cpuc->n_events)) {\n\t\t/* This can occur if the event is disabled right on a PMC overflow. */\n\t\twrperfmon(PERFMON_CMD_ENABLE, cpuc->idx_mask);\n\t\treturn;\n\t}\n\n\tevent = cpuc->event[j];\n\n\tif (unlikely(!event)) {\n\t\t/* This should never occur! */\n\t\tirq_err_count++;\n\t\tpr_warning(\"PMI: No event at index %d!\\n\", idx);\n\t\twrperfmon(PERFMON_CMD_ENABLE, cpuc->idx_mask);\n\t\treturn;\n\t}\n\n\thwc = &event->hw;\n\talpha_perf_event_update(event, hwc, idx, alpha_pmu->pmc_max_period[idx]+1);\n\tdata.period = event->hw.last_period;\n\n\tif (alpha_perf_event_set_period(event, hwc, idx)) {\n\t\tif (perf_event_overflow(event, 1, &data, regs)) {\n\t\t\t/* Interrupts coming too quickly; \"throttle\" the\n\t\t\t * counter, i.e., disable it for a little while.\n\t\t\t */\n\t\t\talpha_pmu_stop(event, 0);\n\t\t}\n\t}\n\twrperfmon(PERFMON_CMD_ENABLE, cpuc->idx_mask);\n\n\treturn;\n}\n\n\n\n/*\n * Init call to initialise performance events at kernel startup.\n */\nint __init init_hw_perf_events(void)\n{\n\tpr_info(\"Performance events: \");\n\n\tif (!supported_cpu()) {\n\t\tpr_cont(\"No support for your CPU.\\n\");\n\t\treturn 0;\n\t}\n\n\tpr_cont(\"Supported CPU type!\\n\");\n\n\t/* Override performance counter IRQ vector */\n\n\tperf_irq = alpha_perf_event_irq_handler;\n\n\t/* And set up PMU specification */\n\talpha_pmu = &ev67_pmu;\n\n\tperf_pmu_register(&pmu, \"cpu\", PERF_TYPE_RAW);\n\n\treturn 0;\n}\nearly_initcall(init_hw_perf_events);\n", "/*\n * ARMv6 Performance counter handling code.\n *\n * Copyright (C) 2009 picoChip Designs, Ltd., Jamie Iles\n *\n * ARMv6 has 2 configurable performance counters and a single cycle counter.\n * They all share a single reset bit but can be written to zero so we can use\n * that for a reset.\n *\n * The counters can't be individually enabled or disabled so when we remove\n * one event and replace it with another we could get spurious counts from the\n * wrong event. However, we can take advantage of the fact that the\n * performance counters can export events to the event bus, and the event bus\n * itself can be monitored. This requires that we *don't* export the events to\n * the event bus. The procedure for disabling a configurable counter is:\n *\t- change the counter to count the ETMEXTOUT[0] signal (0x20). This\n *\t  effectively stops the counter from counting.\n *\t- disable the counter's interrupt generation (each counter has it's\n *\t  own interrupt enable bit).\n * Once stopped, the counter value can be written as 0 to reset.\n *\n * To enable a counter:\n *\t- enable the counter's interrupt generation.\n *\t- set the new event type.\n *\n * Note: the dedicated cycle counter only counts cycles and can't be\n * enabled/disabled independently of the others. When we want to disable the\n * cycle counter, we have to just disable the interrupt reporting and start\n * ignoring that counter. When re-enabling, we have to reset the value and\n * enable the interrupt.\n */\n\n#if defined(CONFIG_CPU_V6) || defined(CONFIG_CPU_V6K)\nenum armv6_perf_types {\n\tARMV6_PERFCTR_ICACHE_MISS\t    = 0x0,\n\tARMV6_PERFCTR_IBUF_STALL\t    = 0x1,\n\tARMV6_PERFCTR_DDEP_STALL\t    = 0x2,\n\tARMV6_PERFCTR_ITLB_MISS\t\t    = 0x3,\n\tARMV6_PERFCTR_DTLB_MISS\t\t    = 0x4,\n\tARMV6_PERFCTR_BR_EXEC\t\t    = 0x5,\n\tARMV6_PERFCTR_BR_MISPREDICT\t    = 0x6,\n\tARMV6_PERFCTR_INSTR_EXEC\t    = 0x7,\n\tARMV6_PERFCTR_DCACHE_HIT\t    = 0x9,\n\tARMV6_PERFCTR_DCACHE_ACCESS\t    = 0xA,\n\tARMV6_PERFCTR_DCACHE_MISS\t    = 0xB,\n\tARMV6_PERFCTR_DCACHE_WBACK\t    = 0xC,\n\tARMV6_PERFCTR_SW_PC_CHANGE\t    = 0xD,\n\tARMV6_PERFCTR_MAIN_TLB_MISS\t    = 0xF,\n\tARMV6_PERFCTR_EXPL_D_ACCESS\t    = 0x10,\n\tARMV6_PERFCTR_LSU_FULL_STALL\t    = 0x11,\n\tARMV6_PERFCTR_WBUF_DRAINED\t    = 0x12,\n\tARMV6_PERFCTR_CPU_CYCLES\t    = 0xFF,\n\tARMV6_PERFCTR_NOP\t\t    = 0x20,\n};\n\nenum armv6_counters {\n\tARMV6_CYCLE_COUNTER = 1,\n\tARMV6_COUNTER0,\n\tARMV6_COUNTER1,\n};\n\n/*\n * The hardware events that we support. We do support cache operations but\n * we have harvard caches and no way to combine instruction and data\n * accesses/misses in hardware.\n */\nstatic const unsigned armv6_perf_map[PERF_COUNT_HW_MAX] = {\n\t[PERF_COUNT_HW_CPU_CYCLES]\t    = ARMV6_PERFCTR_CPU_CYCLES,\n\t[PERF_COUNT_HW_INSTRUCTIONS]\t    = ARMV6_PERFCTR_INSTR_EXEC,\n\t[PERF_COUNT_HW_CACHE_REFERENCES]    = HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_CACHE_MISSES]\t    = HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_BRANCH_INSTRUCTIONS] = ARMV6_PERFCTR_BR_EXEC,\n\t[PERF_COUNT_HW_BRANCH_MISSES]\t    = ARMV6_PERFCTR_BR_MISPREDICT,\n\t[PERF_COUNT_HW_BUS_CYCLES]\t    = HW_OP_UNSUPPORTED,\n};\n\nstatic const unsigned armv6_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t  [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t  [PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\t[C(L1D)] = {\n\t\t/*\n\t\t * The performance counters don't differentiate between read\n\t\t * and write accesses/misses so this isn't strictly correct,\n\t\t * but it's the best we can do. Writes and reads get\n\t\t * combined.\n\t\t */\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV6_PERFCTR_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV6_PERFCTR_DCACHE_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV6_PERFCTR_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV6_PERFCTR_DCACHE_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(L1I)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV6_PERFCTR_ICACHE_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV6_PERFCTR_ICACHE_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(LL)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(DTLB)] = {\n\t\t/*\n\t\t * The ARM performance counters can count micro DTLB misses,\n\t\t * micro ITLB misses and main TLB misses. There isn't an event\n\t\t * for TLB misses, so use the micro misses here and if users\n\t\t * want the main TLB misses they can use a raw counter.\n\t\t */\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV6_PERFCTR_DTLB_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV6_PERFCTR_DTLB_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(ITLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV6_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV6_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(BPU)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n};\n\nenum armv6mpcore_perf_types {\n\tARMV6MPCORE_PERFCTR_ICACHE_MISS\t    = 0x0,\n\tARMV6MPCORE_PERFCTR_IBUF_STALL\t    = 0x1,\n\tARMV6MPCORE_PERFCTR_DDEP_STALL\t    = 0x2,\n\tARMV6MPCORE_PERFCTR_ITLB_MISS\t    = 0x3,\n\tARMV6MPCORE_PERFCTR_DTLB_MISS\t    = 0x4,\n\tARMV6MPCORE_PERFCTR_BR_EXEC\t    = 0x5,\n\tARMV6MPCORE_PERFCTR_BR_NOTPREDICT   = 0x6,\n\tARMV6MPCORE_PERFCTR_BR_MISPREDICT   = 0x7,\n\tARMV6MPCORE_PERFCTR_INSTR_EXEC\t    = 0x8,\n\tARMV6MPCORE_PERFCTR_DCACHE_RDACCESS = 0xA,\n\tARMV6MPCORE_PERFCTR_DCACHE_RDMISS   = 0xB,\n\tARMV6MPCORE_PERFCTR_DCACHE_WRACCESS = 0xC,\n\tARMV6MPCORE_PERFCTR_DCACHE_WRMISS   = 0xD,\n\tARMV6MPCORE_PERFCTR_DCACHE_EVICTION = 0xE,\n\tARMV6MPCORE_PERFCTR_SW_PC_CHANGE    = 0xF,\n\tARMV6MPCORE_PERFCTR_MAIN_TLB_MISS   = 0x10,\n\tARMV6MPCORE_PERFCTR_EXPL_MEM_ACCESS = 0x11,\n\tARMV6MPCORE_PERFCTR_LSU_FULL_STALL  = 0x12,\n\tARMV6MPCORE_PERFCTR_WBUF_DRAINED    = 0x13,\n\tARMV6MPCORE_PERFCTR_CPU_CYCLES\t    = 0xFF,\n};\n\n/*\n * The hardware events that we support. We do support cache operations but\n * we have harvard caches and no way to combine instruction and data\n * accesses/misses in hardware.\n */\nstatic const unsigned armv6mpcore_perf_map[PERF_COUNT_HW_MAX] = {\n\t[PERF_COUNT_HW_CPU_CYCLES]\t    = ARMV6MPCORE_PERFCTR_CPU_CYCLES,\n\t[PERF_COUNT_HW_INSTRUCTIONS]\t    = ARMV6MPCORE_PERFCTR_INSTR_EXEC,\n\t[PERF_COUNT_HW_CACHE_REFERENCES]    = HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_CACHE_MISSES]\t    = HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_BRANCH_INSTRUCTIONS] = ARMV6MPCORE_PERFCTR_BR_EXEC,\n\t[PERF_COUNT_HW_BRANCH_MISSES]\t    = ARMV6MPCORE_PERFCTR_BR_MISPREDICT,\n\t[PERF_COUNT_HW_BUS_CYCLES]\t    = HW_OP_UNSUPPORTED,\n};\n\nstatic const unsigned armv6mpcore_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\t[C(L1D)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]  =\n\t\t\t\tARMV6MPCORE_PERFCTR_DCACHE_RDACCESS,\n\t\t\t[C(RESULT_MISS)]    =\n\t\t\t\tARMV6MPCORE_PERFCTR_DCACHE_RDMISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]  =\n\t\t\t\tARMV6MPCORE_PERFCTR_DCACHE_WRACCESS,\n\t\t\t[C(RESULT_MISS)]    =\n\t\t\t\tARMV6MPCORE_PERFCTR_DCACHE_WRMISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(L1I)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = ARMV6MPCORE_PERFCTR_ICACHE_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = ARMV6MPCORE_PERFCTR_ICACHE_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(LL)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(DTLB)] = {\n\t\t/*\n\t\t * The ARM performance counters can count micro DTLB misses,\n\t\t * micro ITLB misses and main TLB misses. There isn't an event\n\t\t * for TLB misses, so use the micro misses here and if users\n\t\t * want the main TLB misses they can use a raw counter.\n\t\t */\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = ARMV6MPCORE_PERFCTR_DTLB_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = ARMV6MPCORE_PERFCTR_DTLB_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(ITLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = ARMV6MPCORE_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = ARMV6MPCORE_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(BPU)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n};\n\nstatic inline unsigned long\narmv6_pmcr_read(void)\n{\n\tu32 val;\n\tasm volatile(\"mrc   p15, 0, %0, c15, c12, 0\" : \"=r\"(val));\n\treturn val;\n}\n\nstatic inline void\narmv6_pmcr_write(unsigned long val)\n{\n\tasm volatile(\"mcr   p15, 0, %0, c15, c12, 0\" : : \"r\"(val));\n}\n\n#define ARMV6_PMCR_ENABLE\t\t(1 << 0)\n#define ARMV6_PMCR_CTR01_RESET\t\t(1 << 1)\n#define ARMV6_PMCR_CCOUNT_RESET\t\t(1 << 2)\n#define ARMV6_PMCR_CCOUNT_DIV\t\t(1 << 3)\n#define ARMV6_PMCR_COUNT0_IEN\t\t(1 << 4)\n#define ARMV6_PMCR_COUNT1_IEN\t\t(1 << 5)\n#define ARMV6_PMCR_CCOUNT_IEN\t\t(1 << 6)\n#define ARMV6_PMCR_COUNT0_OVERFLOW\t(1 << 8)\n#define ARMV6_PMCR_COUNT1_OVERFLOW\t(1 << 9)\n#define ARMV6_PMCR_CCOUNT_OVERFLOW\t(1 << 10)\n#define ARMV6_PMCR_EVT_COUNT0_SHIFT\t20\n#define ARMV6_PMCR_EVT_COUNT0_MASK\t(0xFF << ARMV6_PMCR_EVT_COUNT0_SHIFT)\n#define ARMV6_PMCR_EVT_COUNT1_SHIFT\t12\n#define ARMV6_PMCR_EVT_COUNT1_MASK\t(0xFF << ARMV6_PMCR_EVT_COUNT1_SHIFT)\n\n#define ARMV6_PMCR_OVERFLOWED_MASK \\\n\t(ARMV6_PMCR_COUNT0_OVERFLOW | ARMV6_PMCR_COUNT1_OVERFLOW | \\\n\t ARMV6_PMCR_CCOUNT_OVERFLOW)\n\nstatic inline int\narmv6_pmcr_has_overflowed(unsigned long pmcr)\n{\n\treturn pmcr & ARMV6_PMCR_OVERFLOWED_MASK;\n}\n\nstatic inline int\narmv6_pmcr_counter_has_overflowed(unsigned long pmcr,\n\t\t\t\t  enum armv6_counters counter)\n{\n\tint ret = 0;\n\n\tif (ARMV6_CYCLE_COUNTER == counter)\n\t\tret = pmcr & ARMV6_PMCR_CCOUNT_OVERFLOW;\n\telse if (ARMV6_COUNTER0 == counter)\n\t\tret = pmcr & ARMV6_PMCR_COUNT0_OVERFLOW;\n\telse if (ARMV6_COUNTER1 == counter)\n\t\tret = pmcr & ARMV6_PMCR_COUNT1_OVERFLOW;\n\telse\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", counter);\n\n\treturn ret;\n}\n\nstatic inline u32\narmv6pmu_read_counter(int counter)\n{\n\tunsigned long value = 0;\n\n\tif (ARMV6_CYCLE_COUNTER == counter)\n\t\tasm volatile(\"mrc   p15, 0, %0, c15, c12, 1\" : \"=r\"(value));\n\telse if (ARMV6_COUNTER0 == counter)\n\t\tasm volatile(\"mrc   p15, 0, %0, c15, c12, 2\" : \"=r\"(value));\n\telse if (ARMV6_COUNTER1 == counter)\n\t\tasm volatile(\"mrc   p15, 0, %0, c15, c12, 3\" : \"=r\"(value));\n\telse\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", counter);\n\n\treturn value;\n}\n\nstatic inline void\narmv6pmu_write_counter(int counter,\n\t\t       u32 value)\n{\n\tif (ARMV6_CYCLE_COUNTER == counter)\n\t\tasm volatile(\"mcr   p15, 0, %0, c15, c12, 1\" : : \"r\"(value));\n\telse if (ARMV6_COUNTER0 == counter)\n\t\tasm volatile(\"mcr   p15, 0, %0, c15, c12, 2\" : : \"r\"(value));\n\telse if (ARMV6_COUNTER1 == counter)\n\t\tasm volatile(\"mcr   p15, 0, %0, c15, c12, 3\" : : \"r\"(value));\n\telse\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", counter);\n}\n\nstatic void\narmv6pmu_enable_event(struct hw_perf_event *hwc,\n\t\t      int idx)\n{\n\tunsigned long val, mask, evt, flags;\n\n\tif (ARMV6_CYCLE_COUNTER == idx) {\n\t\tmask\t= 0;\n\t\tevt\t= ARMV6_PMCR_CCOUNT_IEN;\n\t} else if (ARMV6_COUNTER0 == idx) {\n\t\tmask\t= ARMV6_PMCR_EVT_COUNT0_MASK;\n\t\tevt\t= (hwc->config_base << ARMV6_PMCR_EVT_COUNT0_SHIFT) |\n\t\t\t  ARMV6_PMCR_COUNT0_IEN;\n\t} else if (ARMV6_COUNTER1 == idx) {\n\t\tmask\t= ARMV6_PMCR_EVT_COUNT1_MASK;\n\t\tevt\t= (hwc->config_base << ARMV6_PMCR_EVT_COUNT1_SHIFT) |\n\t\t\t  ARMV6_PMCR_COUNT1_IEN;\n\t} else {\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", idx);\n\t\treturn;\n\t}\n\n\t/*\n\t * Mask out the current event and set the counter to count the event\n\t * that we're interested in.\n\t */\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = armv6_pmcr_read();\n\tval &= ~mask;\n\tval |= evt;\n\tarmv6_pmcr_write(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic irqreturn_t\narmv6pmu_handle_irq(int irq_num,\n\t\t    void *dev)\n{\n\tunsigned long pmcr = armv6_pmcr_read();\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct pt_regs *regs;\n\tint idx;\n\n\tif (!armv6_pmcr_has_overflowed(pmcr))\n\t\treturn IRQ_NONE;\n\n\tregs = get_irq_regs();\n\n\t/*\n\t * The interrupts are cleared by writing the overflow flags back to\n\t * the control register. All of the other bits don't have any effect\n\t * if they are rewritten, so write the whole value back.\n\t */\n\tarmv6_pmcr_write(pmcr);\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\tfor (idx = 0; idx <= armpmu->num_events; ++idx) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tstruct hw_perf_event *hwc;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * We have a single interrupt for all counters. Check that\n\t\t * each counter has overflowed before we process it.\n\t\t */\n\t\tif (!armv6_pmcr_counter_has_overflowed(pmcr, idx))\n\t\t\tcontinue;\n\n\t\thwc = &event->hw;\n\t\tarmpmu_event_update(event, hwc, idx, 1);\n\t\tdata.period = event->hw.last_period;\n\t\tif (!armpmu_event_set_period(event, hwc, idx))\n\t\t\tcontinue;\n\n\t\tif (perf_event_overflow(event, 0, &data, regs))\n\t\t\tarmpmu->disable(hwc, idx);\n\t}\n\n\t/*\n\t * Handle the pending perf events.\n\t *\n\t * Note: this call *must* be run with interrupts disabled. For\n\t * platforms that can have the PMU interrupts raised as an NMI, this\n\t * will not work.\n\t */\n\tirq_work_run();\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void\narmv6pmu_start(void)\n{\n\tunsigned long flags, val;\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = armv6_pmcr_read();\n\tval |= ARMV6_PMCR_ENABLE;\n\tarmv6_pmcr_write(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic void\narmv6pmu_stop(void)\n{\n\tunsigned long flags, val;\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = armv6_pmcr_read();\n\tval &= ~ARMV6_PMCR_ENABLE;\n\tarmv6_pmcr_write(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic int\narmv6pmu_get_event_idx(struct cpu_hw_events *cpuc,\n\t\t       struct hw_perf_event *event)\n{\n\t/* Always place a cycle counter into the cycle counter. */\n\tif (ARMV6_PERFCTR_CPU_CYCLES == event->config_base) {\n\t\tif (test_and_set_bit(ARMV6_CYCLE_COUNTER, cpuc->used_mask))\n\t\t\treturn -EAGAIN;\n\n\t\treturn ARMV6_CYCLE_COUNTER;\n\t} else {\n\t\t/*\n\t\t * For anything other than a cycle counter, try and use\n\t\t * counter0 and counter1.\n\t\t */\n\t\tif (!test_and_set_bit(ARMV6_COUNTER1, cpuc->used_mask))\n\t\t\treturn ARMV6_COUNTER1;\n\n\t\tif (!test_and_set_bit(ARMV6_COUNTER0, cpuc->used_mask))\n\t\t\treturn ARMV6_COUNTER0;\n\n\t\t/* The counters are all in use. */\n\t\treturn -EAGAIN;\n\t}\n}\n\nstatic void\narmv6pmu_disable_event(struct hw_perf_event *hwc,\n\t\t       int idx)\n{\n\tunsigned long val, mask, evt, flags;\n\n\tif (ARMV6_CYCLE_COUNTER == idx) {\n\t\tmask\t= ARMV6_PMCR_CCOUNT_IEN;\n\t\tevt\t= 0;\n\t} else if (ARMV6_COUNTER0 == idx) {\n\t\tmask\t= ARMV6_PMCR_COUNT0_IEN | ARMV6_PMCR_EVT_COUNT0_MASK;\n\t\tevt\t= ARMV6_PERFCTR_NOP << ARMV6_PMCR_EVT_COUNT0_SHIFT;\n\t} else if (ARMV6_COUNTER1 == idx) {\n\t\tmask\t= ARMV6_PMCR_COUNT1_IEN | ARMV6_PMCR_EVT_COUNT1_MASK;\n\t\tevt\t= ARMV6_PERFCTR_NOP << ARMV6_PMCR_EVT_COUNT1_SHIFT;\n\t} else {\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", idx);\n\t\treturn;\n\t}\n\n\t/*\n\t * Mask out the current event and set the counter to count the number\n\t * of ETM bus signal assertion cycles. The external reporting should\n\t * be disabled and so this should never increment.\n\t */\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = armv6_pmcr_read();\n\tval &= ~mask;\n\tval |= evt;\n\tarmv6_pmcr_write(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic void\narmv6mpcore_pmu_disable_event(struct hw_perf_event *hwc,\n\t\t\t      int idx)\n{\n\tunsigned long val, mask, flags, evt = 0;\n\n\tif (ARMV6_CYCLE_COUNTER == idx) {\n\t\tmask\t= ARMV6_PMCR_CCOUNT_IEN;\n\t} else if (ARMV6_COUNTER0 == idx) {\n\t\tmask\t= ARMV6_PMCR_COUNT0_IEN;\n\t} else if (ARMV6_COUNTER1 == idx) {\n\t\tmask\t= ARMV6_PMCR_COUNT1_IEN;\n\t} else {\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", idx);\n\t\treturn;\n\t}\n\n\t/*\n\t * Unlike UP ARMv6, we don't have a way of stopping the counters. We\n\t * simply disable the interrupt reporting.\n\t */\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = armv6_pmcr_read();\n\tval &= ~mask;\n\tval |= evt;\n\tarmv6_pmcr_write(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic const struct arm_pmu armv6pmu = {\n\t.id\t\t\t= ARM_PERF_PMU_ID_V6,\n\t.name\t\t\t= \"v6\",\n\t.handle_irq\t\t= armv6pmu_handle_irq,\n\t.enable\t\t\t= armv6pmu_enable_event,\n\t.disable\t\t= armv6pmu_disable_event,\n\t.read_counter\t\t= armv6pmu_read_counter,\n\t.write_counter\t\t= armv6pmu_write_counter,\n\t.get_event_idx\t\t= armv6pmu_get_event_idx,\n\t.start\t\t\t= armv6pmu_start,\n\t.stop\t\t\t= armv6pmu_stop,\n\t.cache_map\t\t= &armv6_perf_cache_map,\n\t.event_map\t\t= &armv6_perf_map,\n\t.raw_event_mask\t\t= 0xFF,\n\t.num_events\t\t= 3,\n\t.max_period\t\t= (1LLU << 32) - 1,\n};\n\nstatic const struct arm_pmu *__init armv6pmu_init(void)\n{\n\treturn &armv6pmu;\n}\n\n/*\n * ARMv6mpcore is almost identical to single core ARMv6 with the exception\n * that some of the events have different enumerations and that there is no\n * *hack* to stop the programmable counters. To stop the counters we simply\n * disable the interrupt reporting and update the event. When unthrottling we\n * reset the period and enable the interrupt reporting.\n */\nstatic const struct arm_pmu armv6mpcore_pmu = {\n\t.id\t\t\t= ARM_PERF_PMU_ID_V6MP,\n\t.name\t\t\t= \"v6mpcore\",\n\t.handle_irq\t\t= armv6pmu_handle_irq,\n\t.enable\t\t\t= armv6pmu_enable_event,\n\t.disable\t\t= armv6mpcore_pmu_disable_event,\n\t.read_counter\t\t= armv6pmu_read_counter,\n\t.write_counter\t\t= armv6pmu_write_counter,\n\t.get_event_idx\t\t= armv6pmu_get_event_idx,\n\t.start\t\t\t= armv6pmu_start,\n\t.stop\t\t\t= armv6pmu_stop,\n\t.cache_map\t\t= &armv6mpcore_perf_cache_map,\n\t.event_map\t\t= &armv6mpcore_perf_map,\n\t.raw_event_mask\t\t= 0xFF,\n\t.num_events\t\t= 3,\n\t.max_period\t\t= (1LLU << 32) - 1,\n};\n\nstatic const struct arm_pmu *__init armv6mpcore_pmu_init(void)\n{\n\treturn &armv6mpcore_pmu;\n}\n#else\nstatic const struct arm_pmu *__init armv6pmu_init(void)\n{\n\treturn NULL;\n}\n\nstatic const struct arm_pmu *__init armv6mpcore_pmu_init(void)\n{\n\treturn NULL;\n}\n#endif\t/* CONFIG_CPU_V6 || CONFIG_CPU_V6K */\n", "/*\n * ARMv7 Cortex-A8 and Cortex-A9 Performance Events handling code.\n *\n * ARMv7 support: Jean Pihet <jpihet@mvista.com>\n * 2010 (c) MontaVista Software, LLC.\n *\n * Copied from ARMv6 code, with the low level code inspired\n *  by the ARMv7 Oprofile code.\n *\n * Cortex-A8 has up to 4 configurable performance counters and\n *  a single cycle counter.\n * Cortex-A9 has up to 31 configurable performance counters and\n *  a single cycle counter.\n *\n * All counters can be enabled/disabled and IRQ masked separately. The cycle\n *  counter and all 4 performance counters together can be reset separately.\n */\n\n#ifdef CONFIG_CPU_V7\n/* Common ARMv7 event types */\nenum armv7_perf_types {\n\tARMV7_PERFCTR_PMNC_SW_INCR\t\t= 0x00,\n\tARMV7_PERFCTR_IFETCH_MISS\t\t= 0x01,\n\tARMV7_PERFCTR_ITLB_MISS\t\t\t= 0x02,\n\tARMV7_PERFCTR_DCACHE_REFILL\t\t= 0x03,\n\tARMV7_PERFCTR_DCACHE_ACCESS\t\t= 0x04,\n\tARMV7_PERFCTR_DTLB_REFILL\t\t= 0x05,\n\tARMV7_PERFCTR_DREAD\t\t\t= 0x06,\n\tARMV7_PERFCTR_DWRITE\t\t\t= 0x07,\n\n\tARMV7_PERFCTR_EXC_TAKEN\t\t\t= 0x09,\n\tARMV7_PERFCTR_EXC_EXECUTED\t\t= 0x0A,\n\tARMV7_PERFCTR_CID_WRITE\t\t\t= 0x0B,\n\t/* ARMV7_PERFCTR_PC_WRITE is equivalent to HW_BRANCH_INSTRUCTIONS.\n\t * It counts:\n\t *  - all branch instructions,\n\t *  - instructions that explicitly write the PC,\n\t *  - exception generating instructions.\n\t */\n\tARMV7_PERFCTR_PC_WRITE\t\t\t= 0x0C,\n\tARMV7_PERFCTR_PC_IMM_BRANCH\t\t= 0x0D,\n\tARMV7_PERFCTR_UNALIGNED_ACCESS\t\t= 0x0F,\n\tARMV7_PERFCTR_PC_BRANCH_MIS_PRED\t= 0x10,\n\tARMV7_PERFCTR_CLOCK_CYCLES\t\t= 0x11,\n\n\tARMV7_PERFCTR_PC_BRANCH_MIS_USED\t= 0x12,\n\n\tARMV7_PERFCTR_CPU_CYCLES\t\t= 0xFF\n};\n\n/* ARMv7 Cortex-A8 specific event types */\nenum armv7_a8_perf_types {\n\tARMV7_PERFCTR_INSTR_EXECUTED\t\t= 0x08,\n\n\tARMV7_PERFCTR_PC_PROC_RETURN\t\t= 0x0E,\n\n\tARMV7_PERFCTR_WRITE_BUFFER_FULL\t\t= 0x40,\n\tARMV7_PERFCTR_L2_STORE_MERGED\t\t= 0x41,\n\tARMV7_PERFCTR_L2_STORE_BUFF\t\t= 0x42,\n\tARMV7_PERFCTR_L2_ACCESS\t\t\t= 0x43,\n\tARMV7_PERFCTR_L2_CACH_MISS\t\t= 0x44,\n\tARMV7_PERFCTR_AXI_READ_CYCLES\t\t= 0x45,\n\tARMV7_PERFCTR_AXI_WRITE_CYCLES\t\t= 0x46,\n\tARMV7_PERFCTR_MEMORY_REPLAY\t\t= 0x47,\n\tARMV7_PERFCTR_UNALIGNED_ACCESS_REPLAY\t= 0x48,\n\tARMV7_PERFCTR_L1_DATA_MISS\t\t= 0x49,\n\tARMV7_PERFCTR_L1_INST_MISS\t\t= 0x4A,\n\tARMV7_PERFCTR_L1_DATA_COLORING\t\t= 0x4B,\n\tARMV7_PERFCTR_L1_NEON_DATA\t\t= 0x4C,\n\tARMV7_PERFCTR_L1_NEON_CACH_DATA\t\t= 0x4D,\n\tARMV7_PERFCTR_L2_NEON\t\t\t= 0x4E,\n\tARMV7_PERFCTR_L2_NEON_HIT\t\t= 0x4F,\n\tARMV7_PERFCTR_L1_INST\t\t\t= 0x50,\n\tARMV7_PERFCTR_PC_RETURN_MIS_PRED\t= 0x51,\n\tARMV7_PERFCTR_PC_BRANCH_FAILED\t\t= 0x52,\n\tARMV7_PERFCTR_PC_BRANCH_TAKEN\t\t= 0x53,\n\tARMV7_PERFCTR_PC_BRANCH_EXECUTED\t= 0x54,\n\tARMV7_PERFCTR_OP_EXECUTED\t\t= 0x55,\n\tARMV7_PERFCTR_CYCLES_INST_STALL\t\t= 0x56,\n\tARMV7_PERFCTR_CYCLES_INST\t\t= 0x57,\n\tARMV7_PERFCTR_CYCLES_NEON_DATA_STALL\t= 0x58,\n\tARMV7_PERFCTR_CYCLES_NEON_INST_STALL\t= 0x59,\n\tARMV7_PERFCTR_NEON_CYCLES\t\t= 0x5A,\n\n\tARMV7_PERFCTR_PMU0_EVENTS\t\t= 0x70,\n\tARMV7_PERFCTR_PMU1_EVENTS\t\t= 0x71,\n\tARMV7_PERFCTR_PMU_EVENTS\t\t= 0x72,\n};\n\n/* ARMv7 Cortex-A9 specific event types */\nenum armv7_a9_perf_types {\n\tARMV7_PERFCTR_JAVA_HW_BYTECODE_EXEC\t= 0x40,\n\tARMV7_PERFCTR_JAVA_SW_BYTECODE_EXEC\t= 0x41,\n\tARMV7_PERFCTR_JAZELLE_BRANCH_EXEC\t= 0x42,\n\n\tARMV7_PERFCTR_COHERENT_LINE_MISS\t= 0x50,\n\tARMV7_PERFCTR_COHERENT_LINE_HIT\t\t= 0x51,\n\n\tARMV7_PERFCTR_ICACHE_DEP_STALL_CYCLES\t= 0x60,\n\tARMV7_PERFCTR_DCACHE_DEP_STALL_CYCLES\t= 0x61,\n\tARMV7_PERFCTR_TLB_MISS_DEP_STALL_CYCLES\t= 0x62,\n\tARMV7_PERFCTR_STREX_EXECUTED_PASSED\t= 0x63,\n\tARMV7_PERFCTR_STREX_EXECUTED_FAILED\t= 0x64,\n\tARMV7_PERFCTR_DATA_EVICTION\t\t= 0x65,\n\tARMV7_PERFCTR_ISSUE_STAGE_NO_INST\t= 0x66,\n\tARMV7_PERFCTR_ISSUE_STAGE_EMPTY\t\t= 0x67,\n\tARMV7_PERFCTR_INST_OUT_OF_RENAME_STAGE\t= 0x68,\n\n\tARMV7_PERFCTR_PREDICTABLE_FUNCT_RETURNS\t= 0x6E,\n\n\tARMV7_PERFCTR_MAIN_UNIT_EXECUTED_INST\t= 0x70,\n\tARMV7_PERFCTR_SECOND_UNIT_EXECUTED_INST\t= 0x71,\n\tARMV7_PERFCTR_LD_ST_UNIT_EXECUTED_INST\t= 0x72,\n\tARMV7_PERFCTR_FP_EXECUTED_INST\t\t= 0x73,\n\tARMV7_PERFCTR_NEON_EXECUTED_INST\t= 0x74,\n\n\tARMV7_PERFCTR_PLD_FULL_DEP_STALL_CYCLES\t= 0x80,\n\tARMV7_PERFCTR_DATA_WR_DEP_STALL_CYCLES\t= 0x81,\n\tARMV7_PERFCTR_ITLB_MISS_DEP_STALL_CYCLES\t= 0x82,\n\tARMV7_PERFCTR_DTLB_MISS_DEP_STALL_CYCLES\t= 0x83,\n\tARMV7_PERFCTR_MICRO_ITLB_MISS_DEP_STALL_CYCLES\t= 0x84,\n\tARMV7_PERFCTR_MICRO_DTLB_MISS_DEP_STALL_CYCLES\t= 0x85,\n\tARMV7_PERFCTR_DMB_DEP_STALL_CYCLES\t= 0x86,\n\n\tARMV7_PERFCTR_INTGR_CLK_ENABLED_CYCLES\t= 0x8A,\n\tARMV7_PERFCTR_DATA_ENGINE_CLK_EN_CYCLES\t= 0x8B,\n\n\tARMV7_PERFCTR_ISB_INST\t\t\t= 0x90,\n\tARMV7_PERFCTR_DSB_INST\t\t\t= 0x91,\n\tARMV7_PERFCTR_DMB_INST\t\t\t= 0x92,\n\tARMV7_PERFCTR_EXT_INTERRUPTS\t\t= 0x93,\n\n\tARMV7_PERFCTR_PLE_CACHE_LINE_RQST_COMPLETED\t= 0xA0,\n\tARMV7_PERFCTR_PLE_CACHE_LINE_RQST_SKIPPED\t= 0xA1,\n\tARMV7_PERFCTR_PLE_FIFO_FLUSH\t\t= 0xA2,\n\tARMV7_PERFCTR_PLE_RQST_COMPLETED\t= 0xA3,\n\tARMV7_PERFCTR_PLE_FIFO_OVERFLOW\t\t= 0xA4,\n\tARMV7_PERFCTR_PLE_RQST_PROG\t\t= 0xA5\n};\n\n/*\n * Cortex-A8 HW events mapping\n *\n * The hardware events that we support. We do support cache operations but\n * we have harvard caches and no way to combine instruction and data\n * accesses/misses in hardware.\n */\nstatic const unsigned armv7_a8_perf_map[PERF_COUNT_HW_MAX] = {\n\t[PERF_COUNT_HW_CPU_CYCLES]\t    = ARMV7_PERFCTR_CPU_CYCLES,\n\t[PERF_COUNT_HW_INSTRUCTIONS]\t    = ARMV7_PERFCTR_INSTR_EXECUTED,\n\t[PERF_COUNT_HW_CACHE_REFERENCES]    = HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_CACHE_MISSES]\t    = HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_BRANCH_INSTRUCTIONS] = ARMV7_PERFCTR_PC_WRITE,\n\t[PERF_COUNT_HW_BRANCH_MISSES]\t    = ARMV7_PERFCTR_PC_BRANCH_MIS_PRED,\n\t[PERF_COUNT_HW_BUS_CYCLES]\t    = ARMV7_PERFCTR_CLOCK_CYCLES,\n};\n\nstatic const unsigned armv7_a8_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t  [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t  [PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\t[C(L1D)] = {\n\t\t/*\n\t\t * The performance counters don't differentiate between read\n\t\t * and write accesses/misses so this isn't strictly correct,\n\t\t * but it's the best we can do. Writes and reads get\n\t\t * combined.\n\t\t */\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_DCACHE_REFILL,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_DCACHE_REFILL,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(L1I)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_L1_INST,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_L1_INST_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_L1_INST,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_L1_INST_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(LL)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_L2_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_L2_CACH_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_L2_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_L2_CACH_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(DTLB)] = {\n\t\t/*\n\t\t * Only ITLB misses and DTLB refills are supported.\n\t\t * If users want the DTLB refills misses a raw counter\n\t\t * must be used.\n\t\t */\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_DTLB_REFILL,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_DTLB_REFILL,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(ITLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(BPU)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_PC_WRITE,\n\t\t\t[C(RESULT_MISS)]\n\t\t\t\t\t= ARMV7_PERFCTR_PC_BRANCH_MIS_PRED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_PC_WRITE,\n\t\t\t[C(RESULT_MISS)]\n\t\t\t\t\t= ARMV7_PERFCTR_PC_BRANCH_MIS_PRED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n};\n\n/*\n * Cortex-A9 HW events mapping\n */\nstatic const unsigned armv7_a9_perf_map[PERF_COUNT_HW_MAX] = {\n\t[PERF_COUNT_HW_CPU_CYCLES]\t    = ARMV7_PERFCTR_CPU_CYCLES,\n\t[PERF_COUNT_HW_INSTRUCTIONS]\t    =\n\t\t\t\t\tARMV7_PERFCTR_INST_OUT_OF_RENAME_STAGE,\n\t[PERF_COUNT_HW_CACHE_REFERENCES]    = ARMV7_PERFCTR_COHERENT_LINE_HIT,\n\t[PERF_COUNT_HW_CACHE_MISSES]\t    = ARMV7_PERFCTR_COHERENT_LINE_MISS,\n\t[PERF_COUNT_HW_BRANCH_INSTRUCTIONS] = ARMV7_PERFCTR_PC_WRITE,\n\t[PERF_COUNT_HW_BRANCH_MISSES]\t    = ARMV7_PERFCTR_PC_BRANCH_MIS_PRED,\n\t[PERF_COUNT_HW_BUS_CYCLES]\t    = ARMV7_PERFCTR_CLOCK_CYCLES,\n};\n\nstatic const unsigned armv7_a9_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t  [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t  [PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\t[C(L1D)] = {\n\t\t/*\n\t\t * The performance counters don't differentiate between read\n\t\t * and write accesses/misses so this isn't strictly correct,\n\t\t * but it's the best we can do. Writes and reads get\n\t\t * combined.\n\t\t */\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_DCACHE_REFILL,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_DCACHE_REFILL,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(L1I)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_IFETCH_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_IFETCH_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(LL)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(DTLB)] = {\n\t\t/*\n\t\t * Only ITLB misses and DTLB refills are supported.\n\t\t * If users want the DTLB refills misses a raw counter\n\t\t * must be used.\n\t\t */\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_DTLB_REFILL,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_DTLB_REFILL,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(ITLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(BPU)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_PC_WRITE,\n\t\t\t[C(RESULT_MISS)]\n\t\t\t\t\t= ARMV7_PERFCTR_PC_BRANCH_MIS_PRED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_PC_WRITE,\n\t\t\t[C(RESULT_MISS)]\n\t\t\t\t\t= ARMV7_PERFCTR_PC_BRANCH_MIS_PRED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n};\n\n/*\n * Perf Events counters\n */\nenum armv7_counters {\n\tARMV7_CYCLE_COUNTER\t\t= 1,\t/* Cycle counter */\n\tARMV7_COUNTER0\t\t\t= 2,\t/* First event counter */\n};\n\n/*\n * The cycle counter is ARMV7_CYCLE_COUNTER.\n * The first event counter is ARMV7_COUNTER0.\n * The last event counter is (ARMV7_COUNTER0 + armpmu->num_events - 1).\n */\n#define\tARMV7_COUNTER_LAST\t(ARMV7_COUNTER0 + armpmu->num_events - 1)\n\n/*\n * ARMv7 low level PMNC access\n */\n\n/*\n * Per-CPU PMNC: config reg\n */\n#define ARMV7_PMNC_E\t\t(1 << 0) /* Enable all counters */\n#define ARMV7_PMNC_P\t\t(1 << 1) /* Reset all counters */\n#define ARMV7_PMNC_C\t\t(1 << 2) /* Cycle counter reset */\n#define ARMV7_PMNC_D\t\t(1 << 3) /* CCNT counts every 64th cpu cycle */\n#define ARMV7_PMNC_X\t\t(1 << 4) /* Export to ETM */\n#define ARMV7_PMNC_DP\t\t(1 << 5) /* Disable CCNT if non-invasive debug*/\n#define\tARMV7_PMNC_N_SHIFT\t11\t /* Number of counters supported */\n#define\tARMV7_PMNC_N_MASK\t0x1f\n#define\tARMV7_PMNC_MASK\t\t0x3f\t /* Mask for writable bits */\n\n/*\n * Available counters\n */\n#define ARMV7_CNT0\t\t0\t/* First event counter */\n#define ARMV7_CCNT\t\t31\t/* Cycle counter */\n\n/* Perf Event to low level counters mapping */\n#define ARMV7_EVENT_CNT_TO_CNTx\t(ARMV7_COUNTER0 - ARMV7_CNT0)\n\n/*\n * CNTENS: counters enable reg\n */\n#define ARMV7_CNTENS_P(idx)\t(1 << (idx - ARMV7_EVENT_CNT_TO_CNTx))\n#define ARMV7_CNTENS_C\t\t(1 << ARMV7_CCNT)\n\n/*\n * CNTENC: counters disable reg\n */\n#define ARMV7_CNTENC_P(idx)\t(1 << (idx - ARMV7_EVENT_CNT_TO_CNTx))\n#define ARMV7_CNTENC_C\t\t(1 << ARMV7_CCNT)\n\n/*\n * INTENS: counters overflow interrupt enable reg\n */\n#define ARMV7_INTENS_P(idx)\t(1 << (idx - ARMV7_EVENT_CNT_TO_CNTx))\n#define ARMV7_INTENS_C\t\t(1 << ARMV7_CCNT)\n\n/*\n * INTENC: counters overflow interrupt disable reg\n */\n#define ARMV7_INTENC_P(idx)\t(1 << (idx - ARMV7_EVENT_CNT_TO_CNTx))\n#define ARMV7_INTENC_C\t\t(1 << ARMV7_CCNT)\n\n/*\n * EVTSEL: Event selection reg\n */\n#define\tARMV7_EVTSEL_MASK\t0xff\t\t/* Mask for writable bits */\n\n/*\n * SELECT: Counter selection reg\n */\n#define\tARMV7_SELECT_MASK\t0x1f\t\t/* Mask for writable bits */\n\n/*\n * FLAG: counters overflow flag status reg\n */\n#define ARMV7_FLAG_P(idx)\t(1 << (idx - ARMV7_EVENT_CNT_TO_CNTx))\n#define ARMV7_FLAG_C\t\t(1 << ARMV7_CCNT)\n#define\tARMV7_FLAG_MASK\t\t0xffffffff\t/* Mask for writable bits */\n#define\tARMV7_OVERFLOWED_MASK\tARMV7_FLAG_MASK\n\nstatic inline unsigned long armv7_pmnc_read(void)\n{\n\tu32 val;\n\tasm volatile(\"mrc p15, 0, %0, c9, c12, 0\" : \"=r\"(val));\n\treturn val;\n}\n\nstatic inline void armv7_pmnc_write(unsigned long val)\n{\n\tval &= ARMV7_PMNC_MASK;\n\tisb();\n\tasm volatile(\"mcr p15, 0, %0, c9, c12, 0\" : : \"r\"(val));\n}\n\nstatic inline int armv7_pmnc_has_overflowed(unsigned long pmnc)\n{\n\treturn pmnc & ARMV7_OVERFLOWED_MASK;\n}\n\nstatic inline int armv7_pmnc_counter_has_overflowed(unsigned long pmnc,\n\t\t\t\t\tenum armv7_counters counter)\n{\n\tint ret = 0;\n\n\tif (counter == ARMV7_CYCLE_COUNTER)\n\t\tret = pmnc & ARMV7_FLAG_C;\n\telse if ((counter >= ARMV7_COUNTER0) && (counter <= ARMV7_COUNTER_LAST))\n\t\tret = pmnc & ARMV7_FLAG_P(counter);\n\telse\n\t\tpr_err(\"CPU%u checking wrong counter %d overflow status\\n\",\n\t\t\tsmp_processor_id(), counter);\n\n\treturn ret;\n}\n\nstatic inline int armv7_pmnc_select_counter(unsigned int idx)\n{\n\tu32 val;\n\n\tif ((idx < ARMV7_COUNTER0) || (idx > ARMV7_COUNTER_LAST)) {\n\t\tpr_err(\"CPU%u selecting wrong PMNC counter\"\n\t\t\t\" %d\\n\", smp_processor_id(), idx);\n\t\treturn -1;\n\t}\n\n\tval = (idx - ARMV7_EVENT_CNT_TO_CNTx) & ARMV7_SELECT_MASK;\n\tasm volatile(\"mcr p15, 0, %0, c9, c12, 5\" : : \"r\" (val));\n\tisb();\n\n\treturn idx;\n}\n\nstatic inline u32 armv7pmu_read_counter(int idx)\n{\n\tunsigned long value = 0;\n\n\tif (idx == ARMV7_CYCLE_COUNTER)\n\t\tasm volatile(\"mrc p15, 0, %0, c9, c13, 0\" : \"=r\" (value));\n\telse if ((idx >= ARMV7_COUNTER0) && (idx <= ARMV7_COUNTER_LAST)) {\n\t\tif (armv7_pmnc_select_counter(idx) == idx)\n\t\t\tasm volatile(\"mrc p15, 0, %0, c9, c13, 2\"\n\t\t\t\t     : \"=r\" (value));\n\t} else\n\t\tpr_err(\"CPU%u reading wrong counter %d\\n\",\n\t\t\tsmp_processor_id(), idx);\n\n\treturn value;\n}\n\nstatic inline void armv7pmu_write_counter(int idx, u32 value)\n{\n\tif (idx == ARMV7_CYCLE_COUNTER)\n\t\tasm volatile(\"mcr p15, 0, %0, c9, c13, 0\" : : \"r\" (value));\n\telse if ((idx >= ARMV7_COUNTER0) && (idx <= ARMV7_COUNTER_LAST)) {\n\t\tif (armv7_pmnc_select_counter(idx) == idx)\n\t\t\tasm volatile(\"mcr p15, 0, %0, c9, c13, 2\"\n\t\t\t\t     : : \"r\" (value));\n\t} else\n\t\tpr_err(\"CPU%u writing wrong counter %d\\n\",\n\t\t\tsmp_processor_id(), idx);\n}\n\nstatic inline void armv7_pmnc_write_evtsel(unsigned int idx, u32 val)\n{\n\tif (armv7_pmnc_select_counter(idx) == idx) {\n\t\tval &= ARMV7_EVTSEL_MASK;\n\t\tasm volatile(\"mcr p15, 0, %0, c9, c13, 1\" : : \"r\" (val));\n\t}\n}\n\nstatic inline u32 armv7_pmnc_enable_counter(unsigned int idx)\n{\n\tu32 val;\n\n\tif ((idx != ARMV7_CYCLE_COUNTER) &&\n\t    ((idx < ARMV7_COUNTER0) || (idx > ARMV7_COUNTER_LAST))) {\n\t\tpr_err(\"CPU%u enabling wrong PMNC counter\"\n\t\t\t\" %d\\n\", smp_processor_id(), idx);\n\t\treturn -1;\n\t}\n\n\tif (idx == ARMV7_CYCLE_COUNTER)\n\t\tval = ARMV7_CNTENS_C;\n\telse\n\t\tval = ARMV7_CNTENS_P(idx);\n\n\tasm volatile(\"mcr p15, 0, %0, c9, c12, 1\" : : \"r\" (val));\n\n\treturn idx;\n}\n\nstatic inline u32 armv7_pmnc_disable_counter(unsigned int idx)\n{\n\tu32 val;\n\n\n\tif ((idx != ARMV7_CYCLE_COUNTER) &&\n\t    ((idx < ARMV7_COUNTER0) || (idx > ARMV7_COUNTER_LAST))) {\n\t\tpr_err(\"CPU%u disabling wrong PMNC counter\"\n\t\t\t\" %d\\n\", smp_processor_id(), idx);\n\t\treturn -1;\n\t}\n\n\tif (idx == ARMV7_CYCLE_COUNTER)\n\t\tval = ARMV7_CNTENC_C;\n\telse\n\t\tval = ARMV7_CNTENC_P(idx);\n\n\tasm volatile(\"mcr p15, 0, %0, c9, c12, 2\" : : \"r\" (val));\n\n\treturn idx;\n}\n\nstatic inline u32 armv7_pmnc_enable_intens(unsigned int idx)\n{\n\tu32 val;\n\n\tif ((idx != ARMV7_CYCLE_COUNTER) &&\n\t    ((idx < ARMV7_COUNTER0) || (idx > ARMV7_COUNTER_LAST))) {\n\t\tpr_err(\"CPU%u enabling wrong PMNC counter\"\n\t\t\t\" interrupt enable %d\\n\", smp_processor_id(), idx);\n\t\treturn -1;\n\t}\n\n\tif (idx == ARMV7_CYCLE_COUNTER)\n\t\tval = ARMV7_INTENS_C;\n\telse\n\t\tval = ARMV7_INTENS_P(idx);\n\n\tasm volatile(\"mcr p15, 0, %0, c9, c14, 1\" : : \"r\" (val));\n\n\treturn idx;\n}\n\nstatic inline u32 armv7_pmnc_disable_intens(unsigned int idx)\n{\n\tu32 val;\n\n\tif ((idx != ARMV7_CYCLE_COUNTER) &&\n\t    ((idx < ARMV7_COUNTER0) || (idx > ARMV7_COUNTER_LAST))) {\n\t\tpr_err(\"CPU%u disabling wrong PMNC counter\"\n\t\t\t\" interrupt enable %d\\n\", smp_processor_id(), idx);\n\t\treturn -1;\n\t}\n\n\tif (idx == ARMV7_CYCLE_COUNTER)\n\t\tval = ARMV7_INTENC_C;\n\telse\n\t\tval = ARMV7_INTENC_P(idx);\n\n\tasm volatile(\"mcr p15, 0, %0, c9, c14, 2\" : : \"r\" (val));\n\n\treturn idx;\n}\n\nstatic inline u32 armv7_pmnc_getreset_flags(void)\n{\n\tu32 val;\n\n\t/* Read */\n\tasm volatile(\"mrc p15, 0, %0, c9, c12, 3\" : \"=r\" (val));\n\n\t/* Write to clear flags */\n\tval &= ARMV7_FLAG_MASK;\n\tasm volatile(\"mcr p15, 0, %0, c9, c12, 3\" : : \"r\" (val));\n\n\treturn val;\n}\n\n#ifdef DEBUG\nstatic void armv7_pmnc_dump_regs(void)\n{\n\tu32 val;\n\tunsigned int cnt;\n\n\tprintk(KERN_INFO \"PMNC registers dump:\\n\");\n\n\tasm volatile(\"mrc p15, 0, %0, c9, c12, 0\" : \"=r\" (val));\n\tprintk(KERN_INFO \"PMNC  =0x%08x\\n\", val);\n\n\tasm volatile(\"mrc p15, 0, %0, c9, c12, 1\" : \"=r\" (val));\n\tprintk(KERN_INFO \"CNTENS=0x%08x\\n\", val);\n\n\tasm volatile(\"mrc p15, 0, %0, c9, c14, 1\" : \"=r\" (val));\n\tprintk(KERN_INFO \"INTENS=0x%08x\\n\", val);\n\n\tasm volatile(\"mrc p15, 0, %0, c9, c12, 3\" : \"=r\" (val));\n\tprintk(KERN_INFO \"FLAGS =0x%08x\\n\", val);\n\n\tasm volatile(\"mrc p15, 0, %0, c9, c12, 5\" : \"=r\" (val));\n\tprintk(KERN_INFO \"SELECT=0x%08x\\n\", val);\n\n\tasm volatile(\"mrc p15, 0, %0, c9, c13, 0\" : \"=r\" (val));\n\tprintk(KERN_INFO \"CCNT  =0x%08x\\n\", val);\n\n\tfor (cnt = ARMV7_COUNTER0; cnt < ARMV7_COUNTER_LAST; cnt++) {\n\t\tarmv7_pmnc_select_counter(cnt);\n\t\tasm volatile(\"mrc p15, 0, %0, c9, c13, 2\" : \"=r\" (val));\n\t\tprintk(KERN_INFO \"CNT[%d] count =0x%08x\\n\",\n\t\t\tcnt-ARMV7_EVENT_CNT_TO_CNTx, val);\n\t\tasm volatile(\"mrc p15, 0, %0, c9, c13, 1\" : \"=r\" (val));\n\t\tprintk(KERN_INFO \"CNT[%d] evtsel=0x%08x\\n\",\n\t\t\tcnt-ARMV7_EVENT_CNT_TO_CNTx, val);\n\t}\n}\n#endif\n\nstatic void armv7pmu_enable_event(struct hw_perf_event *hwc, int idx)\n{\n\tunsigned long flags;\n\n\t/*\n\t * Enable counter and interrupt, and set the counter to count\n\t * the event that we're interested in.\n\t */\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\n\t/*\n\t * Disable counter\n\t */\n\tarmv7_pmnc_disable_counter(idx);\n\n\t/*\n\t * Set event (if destined for PMNx counters)\n\t * We don't need to set the event if it's a cycle count\n\t */\n\tif (idx != ARMV7_CYCLE_COUNTER)\n\t\tarmv7_pmnc_write_evtsel(idx, hwc->config_base);\n\n\t/*\n\t * Enable interrupt for this counter\n\t */\n\tarmv7_pmnc_enable_intens(idx);\n\n\t/*\n\t * Enable counter\n\t */\n\tarmv7_pmnc_enable_counter(idx);\n\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic void armv7pmu_disable_event(struct hw_perf_event *hwc, int idx)\n{\n\tunsigned long flags;\n\n\t/*\n\t * Disable counter and interrupt\n\t */\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\n\t/*\n\t * Disable counter\n\t */\n\tarmv7_pmnc_disable_counter(idx);\n\n\t/*\n\t * Disable interrupt for this counter\n\t */\n\tarmv7_pmnc_disable_intens(idx);\n\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic irqreturn_t armv7pmu_handle_irq(int irq_num, void *dev)\n{\n\tunsigned long pmnc;\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct pt_regs *regs;\n\tint idx;\n\n\t/*\n\t * Get and reset the IRQ flags\n\t */\n\tpmnc = armv7_pmnc_getreset_flags();\n\n\t/*\n\t * Did an overflow occur?\n\t */\n\tif (!armv7_pmnc_has_overflowed(pmnc))\n\t\treturn IRQ_NONE;\n\n\t/*\n\t * Handle the counter(s) overflow(s)\n\t */\n\tregs = get_irq_regs();\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\tfor (idx = 0; idx <= armpmu->num_events; ++idx) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tstruct hw_perf_event *hwc;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * We have a single interrupt for all counters. Check that\n\t\t * each counter has overflowed before we process it.\n\t\t */\n\t\tif (!armv7_pmnc_counter_has_overflowed(pmnc, idx))\n\t\t\tcontinue;\n\n\t\thwc = &event->hw;\n\t\tarmpmu_event_update(event, hwc, idx, 1);\n\t\tdata.period = event->hw.last_period;\n\t\tif (!armpmu_event_set_period(event, hwc, idx))\n\t\t\tcontinue;\n\n\t\tif (perf_event_overflow(event, 0, &data, regs))\n\t\t\tarmpmu->disable(hwc, idx);\n\t}\n\n\t/*\n\t * Handle the pending perf events.\n\t *\n\t * Note: this call *must* be run with interrupts disabled. For\n\t * platforms that can have the PMU interrupts raised as an NMI, this\n\t * will not work.\n\t */\n\tirq_work_run();\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void armv7pmu_start(void)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\t/* Enable all counters */\n\tarmv7_pmnc_write(armv7_pmnc_read() | ARMV7_PMNC_E);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic void armv7pmu_stop(void)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\t/* Disable all counters */\n\tarmv7_pmnc_write(armv7_pmnc_read() & ~ARMV7_PMNC_E);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic int armv7pmu_get_event_idx(struct cpu_hw_events *cpuc,\n\t\t\t\t  struct hw_perf_event *event)\n{\n\tint idx;\n\n\t/* Always place a cycle counter into the cycle counter. */\n\tif (event->config_base == ARMV7_PERFCTR_CPU_CYCLES) {\n\t\tif (test_and_set_bit(ARMV7_CYCLE_COUNTER, cpuc->used_mask))\n\t\t\treturn -EAGAIN;\n\n\t\treturn ARMV7_CYCLE_COUNTER;\n\t} else {\n\t\t/*\n\t\t * For anything other than a cycle counter, try and use\n\t\t * the events counters\n\t\t */\n\t\tfor (idx = ARMV7_COUNTER0; idx <= armpmu->num_events; ++idx) {\n\t\t\tif (!test_and_set_bit(idx, cpuc->used_mask))\n\t\t\t\treturn idx;\n\t\t}\n\n\t\t/* The counters are all in use. */\n\t\treturn -EAGAIN;\n\t}\n}\n\nstatic void armv7pmu_reset(void *info)\n{\n\tu32 idx, nb_cnt = armpmu->num_events;\n\n\t/* The counter and interrupt enable registers are unknown at reset. */\n\tfor (idx = 1; idx < nb_cnt; ++idx)\n\t\tarmv7pmu_disable_event(NULL, idx);\n\n\t/* Initialize & Reset PMNC: C and P bits */\n\tarmv7_pmnc_write(ARMV7_PMNC_P | ARMV7_PMNC_C);\n}\n\nstatic struct arm_pmu armv7pmu = {\n\t.handle_irq\t\t= armv7pmu_handle_irq,\n\t.enable\t\t\t= armv7pmu_enable_event,\n\t.disable\t\t= armv7pmu_disable_event,\n\t.read_counter\t\t= armv7pmu_read_counter,\n\t.write_counter\t\t= armv7pmu_write_counter,\n\t.get_event_idx\t\t= armv7pmu_get_event_idx,\n\t.start\t\t\t= armv7pmu_start,\n\t.stop\t\t\t= armv7pmu_stop,\n\t.reset\t\t\t= armv7pmu_reset,\n\t.raw_event_mask\t\t= 0xFF,\n\t.max_period\t\t= (1LLU << 32) - 1,\n};\n\nstatic u32 __init armv7_read_num_pmnc_events(void)\n{\n\tu32 nb_cnt;\n\n\t/* Read the nb of CNTx counters supported from PMNC */\n\tnb_cnt = (armv7_pmnc_read() >> ARMV7_PMNC_N_SHIFT) & ARMV7_PMNC_N_MASK;\n\n\t/* Add the CPU cycles counter and return */\n\treturn nb_cnt + 1;\n}\n\nstatic const struct arm_pmu *__init armv7_a8_pmu_init(void)\n{\n\tarmv7pmu.id\t\t= ARM_PERF_PMU_ID_CA8;\n\tarmv7pmu.name\t\t= \"ARMv7 Cortex-A8\";\n\tarmv7pmu.cache_map\t= &armv7_a8_perf_cache_map;\n\tarmv7pmu.event_map\t= &armv7_a8_perf_map;\n\tarmv7pmu.num_events\t= armv7_read_num_pmnc_events();\n\treturn &armv7pmu;\n}\n\nstatic const struct arm_pmu *__init armv7_a9_pmu_init(void)\n{\n\tarmv7pmu.id\t\t= ARM_PERF_PMU_ID_CA9;\n\tarmv7pmu.name\t\t= \"ARMv7 Cortex-A9\";\n\tarmv7pmu.cache_map\t= &armv7_a9_perf_cache_map;\n\tarmv7pmu.event_map\t= &armv7_a9_perf_map;\n\tarmv7pmu.num_events\t= armv7_read_num_pmnc_events();\n\treturn &armv7pmu;\n}\n#else\nstatic const struct arm_pmu *__init armv7_a8_pmu_init(void)\n{\n\treturn NULL;\n}\n\nstatic const struct arm_pmu *__init armv7_a9_pmu_init(void)\n{\n\treturn NULL;\n}\n#endif\t/* CONFIG_CPU_V7 */\n", "/*\n * ARMv5 [xscale] Performance counter handling code.\n *\n * Copyright (C) 2010, ARM Ltd., Will Deacon <will.deacon@arm.com>\n *\n * Based on the previous xscale OProfile code.\n *\n * There are two variants of the xscale PMU that we support:\n * \t- xscale1pmu: 2 event counters and a cycle counter\n * \t- xscale2pmu: 4 event counters and a cycle counter\n * The two variants share event definitions, but have different\n * PMU structures.\n */\n\n#ifdef CONFIG_CPU_XSCALE\nenum xscale_perf_types {\n\tXSCALE_PERFCTR_ICACHE_MISS\t\t= 0x00,\n\tXSCALE_PERFCTR_ICACHE_NO_DELIVER\t= 0x01,\n\tXSCALE_PERFCTR_DATA_STALL\t\t= 0x02,\n\tXSCALE_PERFCTR_ITLB_MISS\t\t= 0x03,\n\tXSCALE_PERFCTR_DTLB_MISS\t\t= 0x04,\n\tXSCALE_PERFCTR_BRANCH\t\t\t= 0x05,\n\tXSCALE_PERFCTR_BRANCH_MISS\t\t= 0x06,\n\tXSCALE_PERFCTR_INSTRUCTION\t\t= 0x07,\n\tXSCALE_PERFCTR_DCACHE_FULL_STALL\t= 0x08,\n\tXSCALE_PERFCTR_DCACHE_FULL_STALL_CONTIG\t= 0x09,\n\tXSCALE_PERFCTR_DCACHE_ACCESS\t\t= 0x0A,\n\tXSCALE_PERFCTR_DCACHE_MISS\t\t= 0x0B,\n\tXSCALE_PERFCTR_DCACHE_WRITE_BACK\t= 0x0C,\n\tXSCALE_PERFCTR_PC_CHANGED\t\t= 0x0D,\n\tXSCALE_PERFCTR_BCU_REQUEST\t\t= 0x10,\n\tXSCALE_PERFCTR_BCU_FULL\t\t\t= 0x11,\n\tXSCALE_PERFCTR_BCU_DRAIN\t\t= 0x12,\n\tXSCALE_PERFCTR_BCU_ECC_NO_ELOG\t\t= 0x14,\n\tXSCALE_PERFCTR_BCU_1_BIT_ERR\t\t= 0x15,\n\tXSCALE_PERFCTR_RMW\t\t\t= 0x16,\n\t/* XSCALE_PERFCTR_CCNT is not hardware defined */\n\tXSCALE_PERFCTR_CCNT\t\t\t= 0xFE,\n\tXSCALE_PERFCTR_UNUSED\t\t\t= 0xFF,\n};\n\nenum xscale_counters {\n\tXSCALE_CYCLE_COUNTER\t= 1,\n\tXSCALE_COUNTER0,\n\tXSCALE_COUNTER1,\n\tXSCALE_COUNTER2,\n\tXSCALE_COUNTER3,\n};\n\nstatic const unsigned xscale_perf_map[PERF_COUNT_HW_MAX] = {\n\t[PERF_COUNT_HW_CPU_CYCLES]\t    = XSCALE_PERFCTR_CCNT,\n\t[PERF_COUNT_HW_INSTRUCTIONS]\t    = XSCALE_PERFCTR_INSTRUCTION,\n\t[PERF_COUNT_HW_CACHE_REFERENCES]    = HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_CACHE_MISSES]\t    = HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_BRANCH_INSTRUCTIONS] = XSCALE_PERFCTR_BRANCH,\n\t[PERF_COUNT_HW_BRANCH_MISSES]\t    = XSCALE_PERFCTR_BRANCH_MISS,\n\t[PERF_COUNT_HW_BUS_CYCLES]\t    = HW_OP_UNSUPPORTED,\n};\n\nstatic const unsigned xscale_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t   [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t   [PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\t[C(L1D)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= XSCALE_PERFCTR_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= XSCALE_PERFCTR_DCACHE_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= XSCALE_PERFCTR_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= XSCALE_PERFCTR_DCACHE_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(L1I)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= XSCALE_PERFCTR_ICACHE_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= XSCALE_PERFCTR_ICACHE_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(LL)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(DTLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= XSCALE_PERFCTR_DTLB_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= XSCALE_PERFCTR_DTLB_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(ITLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= XSCALE_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= XSCALE_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(BPU)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n};\n\n#define\tXSCALE_PMU_ENABLE\t0x001\n#define XSCALE_PMN_RESET\t0x002\n#define\tXSCALE_CCNT_RESET\t0x004\n#define\tXSCALE_PMU_RESET\t(CCNT_RESET | PMN_RESET)\n#define XSCALE_PMU_CNT64\t0x008\n\n#define XSCALE1_OVERFLOWED_MASK\t0x700\n#define XSCALE1_CCOUNT_OVERFLOW\t0x400\n#define XSCALE1_COUNT0_OVERFLOW\t0x100\n#define XSCALE1_COUNT1_OVERFLOW\t0x200\n#define XSCALE1_CCOUNT_INT_EN\t0x040\n#define XSCALE1_COUNT0_INT_EN\t0x010\n#define XSCALE1_COUNT1_INT_EN\t0x020\n#define XSCALE1_COUNT0_EVT_SHFT\t12\n#define XSCALE1_COUNT0_EVT_MASK\t(0xff << XSCALE1_COUNT0_EVT_SHFT)\n#define XSCALE1_COUNT1_EVT_SHFT\t20\n#define XSCALE1_COUNT1_EVT_MASK\t(0xff << XSCALE1_COUNT1_EVT_SHFT)\n\nstatic inline u32\nxscale1pmu_read_pmnc(void)\n{\n\tu32 val;\n\tasm volatile(\"mrc p14, 0, %0, c0, c0, 0\" : \"=r\" (val));\n\treturn val;\n}\n\nstatic inline void\nxscale1pmu_write_pmnc(u32 val)\n{\n\t/* upper 4bits and 7, 11 are write-as-0 */\n\tval &= 0xffff77f;\n\tasm volatile(\"mcr p14, 0, %0, c0, c0, 0\" : : \"r\" (val));\n}\n\nstatic inline int\nxscale1_pmnc_counter_has_overflowed(unsigned long pmnc,\n\t\t\t\t\tenum xscale_counters counter)\n{\n\tint ret = 0;\n\n\tswitch (counter) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tret = pmnc & XSCALE1_CCOUNT_OVERFLOW;\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tret = pmnc & XSCALE1_COUNT0_OVERFLOW;\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tret = pmnc & XSCALE1_COUNT1_OVERFLOW;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", counter);\n\t}\n\n\treturn ret;\n}\n\nstatic irqreturn_t\nxscale1pmu_handle_irq(int irq_num, void *dev)\n{\n\tunsigned long pmnc;\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct pt_regs *regs;\n\tint idx;\n\n\t/*\n\t * NOTE: there's an A stepping erratum that states if an overflow\n\t *       bit already exists and another occurs, the previous\n\t *       Overflow bit gets cleared. There's no workaround.\n\t *\t Fixed in B stepping or later.\n\t */\n\tpmnc = xscale1pmu_read_pmnc();\n\n\t/*\n\t * Write the value back to clear the overflow flags. Overflow\n\t * flags remain in pmnc for use below. We also disable the PMU\n\t * while we process the interrupt.\n\t */\n\txscale1pmu_write_pmnc(pmnc & ~XSCALE_PMU_ENABLE);\n\n\tif (!(pmnc & XSCALE1_OVERFLOWED_MASK))\n\t\treturn IRQ_NONE;\n\n\tregs = get_irq_regs();\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\tfor (idx = 0; idx <= armpmu->num_events; ++idx) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tstruct hw_perf_event *hwc;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\tif (!xscale1_pmnc_counter_has_overflowed(pmnc, idx))\n\t\t\tcontinue;\n\n\t\thwc = &event->hw;\n\t\tarmpmu_event_update(event, hwc, idx, 1);\n\t\tdata.period = event->hw.last_period;\n\t\tif (!armpmu_event_set_period(event, hwc, idx))\n\t\t\tcontinue;\n\n\t\tif (perf_event_overflow(event, 0, &data, regs))\n\t\t\tarmpmu->disable(hwc, idx);\n\t}\n\n\tirq_work_run();\n\n\t/*\n\t * Re-enable the PMU.\n\t */\n\tpmnc = xscale1pmu_read_pmnc() | XSCALE_PMU_ENABLE;\n\txscale1pmu_write_pmnc(pmnc);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void\nxscale1pmu_enable_event(struct hw_perf_event *hwc, int idx)\n{\n\tunsigned long val, mask, evt, flags;\n\n\tswitch (idx) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tmask = 0;\n\t\tevt = XSCALE1_CCOUNT_INT_EN;\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tmask = XSCALE1_COUNT0_EVT_MASK;\n\t\tevt = (hwc->config_base << XSCALE1_COUNT0_EVT_SHFT) |\n\t\t\tXSCALE1_COUNT0_INT_EN;\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tmask = XSCALE1_COUNT1_EVT_MASK;\n\t\tevt = (hwc->config_base << XSCALE1_COUNT1_EVT_SHFT) |\n\t\t\tXSCALE1_COUNT1_INT_EN;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", idx);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = xscale1pmu_read_pmnc();\n\tval &= ~mask;\n\tval |= evt;\n\txscale1pmu_write_pmnc(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic void\nxscale1pmu_disable_event(struct hw_perf_event *hwc, int idx)\n{\n\tunsigned long val, mask, evt, flags;\n\n\tswitch (idx) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tmask = XSCALE1_CCOUNT_INT_EN;\n\t\tevt = 0;\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tmask = XSCALE1_COUNT0_INT_EN | XSCALE1_COUNT0_EVT_MASK;\n\t\tevt = XSCALE_PERFCTR_UNUSED << XSCALE1_COUNT0_EVT_SHFT;\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tmask = XSCALE1_COUNT1_INT_EN | XSCALE1_COUNT1_EVT_MASK;\n\t\tevt = XSCALE_PERFCTR_UNUSED << XSCALE1_COUNT1_EVT_SHFT;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", idx);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = xscale1pmu_read_pmnc();\n\tval &= ~mask;\n\tval |= evt;\n\txscale1pmu_write_pmnc(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic int\nxscale1pmu_get_event_idx(struct cpu_hw_events *cpuc,\n\t\t\tstruct hw_perf_event *event)\n{\n\tif (XSCALE_PERFCTR_CCNT == event->config_base) {\n\t\tif (test_and_set_bit(XSCALE_CYCLE_COUNTER, cpuc->used_mask))\n\t\t\treturn -EAGAIN;\n\n\t\treturn XSCALE_CYCLE_COUNTER;\n\t} else {\n\t\tif (!test_and_set_bit(XSCALE_COUNTER1, cpuc->used_mask))\n\t\t\treturn XSCALE_COUNTER1;\n\n\t\tif (!test_and_set_bit(XSCALE_COUNTER0, cpuc->used_mask))\n\t\t\treturn XSCALE_COUNTER0;\n\n\t\treturn -EAGAIN;\n\t}\n}\n\nstatic void\nxscale1pmu_start(void)\n{\n\tunsigned long flags, val;\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = xscale1pmu_read_pmnc();\n\tval |= XSCALE_PMU_ENABLE;\n\txscale1pmu_write_pmnc(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic void\nxscale1pmu_stop(void)\n{\n\tunsigned long flags, val;\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = xscale1pmu_read_pmnc();\n\tval &= ~XSCALE_PMU_ENABLE;\n\txscale1pmu_write_pmnc(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic inline u32\nxscale1pmu_read_counter(int counter)\n{\n\tu32 val = 0;\n\n\tswitch (counter) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tasm volatile(\"mrc p14, 0, %0, c1, c0, 0\" : \"=r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tasm volatile(\"mrc p14, 0, %0, c2, c0, 0\" : \"=r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tasm volatile(\"mrc p14, 0, %0, c3, c0, 0\" : \"=r\" (val));\n\t\tbreak;\n\t}\n\n\treturn val;\n}\n\nstatic inline void\nxscale1pmu_write_counter(int counter, u32 val)\n{\n\tswitch (counter) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tasm volatile(\"mcr p14, 0, %0, c1, c0, 0\" : : \"r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tasm volatile(\"mcr p14, 0, %0, c2, c0, 0\" : : \"r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tasm volatile(\"mcr p14, 0, %0, c3, c0, 0\" : : \"r\" (val));\n\t\tbreak;\n\t}\n}\n\nstatic const struct arm_pmu xscale1pmu = {\n\t.id\t\t= ARM_PERF_PMU_ID_XSCALE1,\n\t.name\t\t= \"xscale1\",\n\t.handle_irq\t= xscale1pmu_handle_irq,\n\t.enable\t\t= xscale1pmu_enable_event,\n\t.disable\t= xscale1pmu_disable_event,\n\t.read_counter\t= xscale1pmu_read_counter,\n\t.write_counter\t= xscale1pmu_write_counter,\n\t.get_event_idx\t= xscale1pmu_get_event_idx,\n\t.start\t\t= xscale1pmu_start,\n\t.stop\t\t= xscale1pmu_stop,\n\t.cache_map\t= &xscale_perf_cache_map,\n\t.event_map\t= &xscale_perf_map,\n\t.raw_event_mask\t= 0xFF,\n\t.num_events\t= 3,\n\t.max_period\t= (1LLU << 32) - 1,\n};\n\nstatic const struct arm_pmu *__init xscale1pmu_init(void)\n{\n\treturn &xscale1pmu;\n}\n\n#define XSCALE2_OVERFLOWED_MASK\t0x01f\n#define XSCALE2_CCOUNT_OVERFLOW\t0x001\n#define XSCALE2_COUNT0_OVERFLOW\t0x002\n#define XSCALE2_COUNT1_OVERFLOW\t0x004\n#define XSCALE2_COUNT2_OVERFLOW\t0x008\n#define XSCALE2_COUNT3_OVERFLOW\t0x010\n#define XSCALE2_CCOUNT_INT_EN\t0x001\n#define XSCALE2_COUNT0_INT_EN\t0x002\n#define XSCALE2_COUNT1_INT_EN\t0x004\n#define XSCALE2_COUNT2_INT_EN\t0x008\n#define XSCALE2_COUNT3_INT_EN\t0x010\n#define XSCALE2_COUNT0_EVT_SHFT\t0\n#define XSCALE2_COUNT0_EVT_MASK\t(0xff << XSCALE2_COUNT0_EVT_SHFT)\n#define XSCALE2_COUNT1_EVT_SHFT\t8\n#define XSCALE2_COUNT1_EVT_MASK\t(0xff << XSCALE2_COUNT1_EVT_SHFT)\n#define XSCALE2_COUNT2_EVT_SHFT\t16\n#define XSCALE2_COUNT2_EVT_MASK\t(0xff << XSCALE2_COUNT2_EVT_SHFT)\n#define XSCALE2_COUNT3_EVT_SHFT\t24\n#define XSCALE2_COUNT3_EVT_MASK\t(0xff << XSCALE2_COUNT3_EVT_SHFT)\n\nstatic inline u32\nxscale2pmu_read_pmnc(void)\n{\n\tu32 val;\n\tasm volatile(\"mrc p14, 0, %0, c0, c1, 0\" : \"=r\" (val));\n\t/* bits 1-2 and 4-23 are read-unpredictable */\n\treturn val & 0xff000009;\n}\n\nstatic inline void\nxscale2pmu_write_pmnc(u32 val)\n{\n\t/* bits 4-23 are write-as-0, 24-31 are write ignored */\n\tval &= 0xf;\n\tasm volatile(\"mcr p14, 0, %0, c0, c1, 0\" : : \"r\" (val));\n}\n\nstatic inline u32\nxscale2pmu_read_overflow_flags(void)\n{\n\tu32 val;\n\tasm volatile(\"mrc p14, 0, %0, c5, c1, 0\" : \"=r\" (val));\n\treturn val;\n}\n\nstatic inline void\nxscale2pmu_write_overflow_flags(u32 val)\n{\n\tasm volatile(\"mcr p14, 0, %0, c5, c1, 0\" : : \"r\" (val));\n}\n\nstatic inline u32\nxscale2pmu_read_event_select(void)\n{\n\tu32 val;\n\tasm volatile(\"mrc p14, 0, %0, c8, c1, 0\" : \"=r\" (val));\n\treturn val;\n}\n\nstatic inline void\nxscale2pmu_write_event_select(u32 val)\n{\n\tasm volatile(\"mcr p14, 0, %0, c8, c1, 0\" : : \"r\"(val));\n}\n\nstatic inline u32\nxscale2pmu_read_int_enable(void)\n{\n\tu32 val;\n\tasm volatile(\"mrc p14, 0, %0, c4, c1, 0\" : \"=r\" (val));\n\treturn val;\n}\n\nstatic void\nxscale2pmu_write_int_enable(u32 val)\n{\n\tasm volatile(\"mcr p14, 0, %0, c4, c1, 0\" : : \"r\" (val));\n}\n\nstatic inline int\nxscale2_pmnc_counter_has_overflowed(unsigned long of_flags,\n\t\t\t\t\tenum xscale_counters counter)\n{\n\tint ret = 0;\n\n\tswitch (counter) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tret = of_flags & XSCALE2_CCOUNT_OVERFLOW;\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tret = of_flags & XSCALE2_COUNT0_OVERFLOW;\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tret = of_flags & XSCALE2_COUNT1_OVERFLOW;\n\t\tbreak;\n\tcase XSCALE_COUNTER2:\n\t\tret = of_flags & XSCALE2_COUNT2_OVERFLOW;\n\t\tbreak;\n\tcase XSCALE_COUNTER3:\n\t\tret = of_flags & XSCALE2_COUNT3_OVERFLOW;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", counter);\n\t}\n\n\treturn ret;\n}\n\nstatic irqreturn_t\nxscale2pmu_handle_irq(int irq_num, void *dev)\n{\n\tunsigned long pmnc, of_flags;\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct pt_regs *regs;\n\tint idx;\n\n\t/* Disable the PMU. */\n\tpmnc = xscale2pmu_read_pmnc();\n\txscale2pmu_write_pmnc(pmnc & ~XSCALE_PMU_ENABLE);\n\n\t/* Check the overflow flag register. */\n\tof_flags = xscale2pmu_read_overflow_flags();\n\tif (!(of_flags & XSCALE2_OVERFLOWED_MASK))\n\t\treturn IRQ_NONE;\n\n\t/* Clear the overflow bits. */\n\txscale2pmu_write_overflow_flags(of_flags);\n\n\tregs = get_irq_regs();\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\tfor (idx = 0; idx <= armpmu->num_events; ++idx) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tstruct hw_perf_event *hwc;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\tif (!xscale2_pmnc_counter_has_overflowed(pmnc, idx))\n\t\t\tcontinue;\n\n\t\thwc = &event->hw;\n\t\tarmpmu_event_update(event, hwc, idx, 1);\n\t\tdata.period = event->hw.last_period;\n\t\tif (!armpmu_event_set_period(event, hwc, idx))\n\t\t\tcontinue;\n\n\t\tif (perf_event_overflow(event, 0, &data, regs))\n\t\t\tarmpmu->disable(hwc, idx);\n\t}\n\n\tirq_work_run();\n\n\t/*\n\t * Re-enable the PMU.\n\t */\n\tpmnc = xscale2pmu_read_pmnc() | XSCALE_PMU_ENABLE;\n\txscale2pmu_write_pmnc(pmnc);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void\nxscale2pmu_enable_event(struct hw_perf_event *hwc, int idx)\n{\n\tunsigned long flags, ien, evtsel;\n\n\tien = xscale2pmu_read_int_enable();\n\tevtsel = xscale2pmu_read_event_select();\n\n\tswitch (idx) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tien |= XSCALE2_CCOUNT_INT_EN;\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tien |= XSCALE2_COUNT0_INT_EN;\n\t\tevtsel &= ~XSCALE2_COUNT0_EVT_MASK;\n\t\tevtsel |= hwc->config_base << XSCALE2_COUNT0_EVT_SHFT;\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tien |= XSCALE2_COUNT1_INT_EN;\n\t\tevtsel &= ~XSCALE2_COUNT1_EVT_MASK;\n\t\tevtsel |= hwc->config_base << XSCALE2_COUNT1_EVT_SHFT;\n\t\tbreak;\n\tcase XSCALE_COUNTER2:\n\t\tien |= XSCALE2_COUNT2_INT_EN;\n\t\tevtsel &= ~XSCALE2_COUNT2_EVT_MASK;\n\t\tevtsel |= hwc->config_base << XSCALE2_COUNT2_EVT_SHFT;\n\t\tbreak;\n\tcase XSCALE_COUNTER3:\n\t\tien |= XSCALE2_COUNT3_INT_EN;\n\t\tevtsel &= ~XSCALE2_COUNT3_EVT_MASK;\n\t\tevtsel |= hwc->config_base << XSCALE2_COUNT3_EVT_SHFT;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", idx);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\txscale2pmu_write_event_select(evtsel);\n\txscale2pmu_write_int_enable(ien);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic void\nxscale2pmu_disable_event(struct hw_perf_event *hwc, int idx)\n{\n\tunsigned long flags, ien, evtsel;\n\n\tien = xscale2pmu_read_int_enable();\n\tevtsel = xscale2pmu_read_event_select();\n\n\tswitch (idx) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tien &= ~XSCALE2_CCOUNT_INT_EN;\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tien &= ~XSCALE2_COUNT0_INT_EN;\n\t\tevtsel &= ~XSCALE2_COUNT0_EVT_MASK;\n\t\tevtsel |= XSCALE_PERFCTR_UNUSED << XSCALE2_COUNT0_EVT_SHFT;\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tien &= ~XSCALE2_COUNT1_INT_EN;\n\t\tevtsel &= ~XSCALE2_COUNT1_EVT_MASK;\n\t\tevtsel |= XSCALE_PERFCTR_UNUSED << XSCALE2_COUNT1_EVT_SHFT;\n\t\tbreak;\n\tcase XSCALE_COUNTER2:\n\t\tien &= ~XSCALE2_COUNT2_INT_EN;\n\t\tevtsel &= ~XSCALE2_COUNT2_EVT_MASK;\n\t\tevtsel |= XSCALE_PERFCTR_UNUSED << XSCALE2_COUNT2_EVT_SHFT;\n\t\tbreak;\n\tcase XSCALE_COUNTER3:\n\t\tien &= ~XSCALE2_COUNT3_INT_EN;\n\t\tevtsel &= ~XSCALE2_COUNT3_EVT_MASK;\n\t\tevtsel |= XSCALE_PERFCTR_UNUSED << XSCALE2_COUNT3_EVT_SHFT;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", idx);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\txscale2pmu_write_event_select(evtsel);\n\txscale2pmu_write_int_enable(ien);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic int\nxscale2pmu_get_event_idx(struct cpu_hw_events *cpuc,\n\t\t\tstruct hw_perf_event *event)\n{\n\tint idx = xscale1pmu_get_event_idx(cpuc, event);\n\tif (idx >= 0)\n\t\tgoto out;\n\n\tif (!test_and_set_bit(XSCALE_COUNTER3, cpuc->used_mask))\n\t\tidx = XSCALE_COUNTER3;\n\telse if (!test_and_set_bit(XSCALE_COUNTER2, cpuc->used_mask))\n\t\tidx = XSCALE_COUNTER2;\nout:\n\treturn idx;\n}\n\nstatic void\nxscale2pmu_start(void)\n{\n\tunsigned long flags, val;\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = xscale2pmu_read_pmnc() & ~XSCALE_PMU_CNT64;\n\tval |= XSCALE_PMU_ENABLE;\n\txscale2pmu_write_pmnc(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic void\nxscale2pmu_stop(void)\n{\n\tunsigned long flags, val;\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = xscale2pmu_read_pmnc();\n\tval &= ~XSCALE_PMU_ENABLE;\n\txscale2pmu_write_pmnc(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic inline u32\nxscale2pmu_read_counter(int counter)\n{\n\tu32 val = 0;\n\n\tswitch (counter) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tasm volatile(\"mrc p14, 0, %0, c1, c1, 0\" : \"=r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tasm volatile(\"mrc p14, 0, %0, c0, c2, 0\" : \"=r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tasm volatile(\"mrc p14, 0, %0, c1, c2, 0\" : \"=r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER2:\n\t\tasm volatile(\"mrc p14, 0, %0, c2, c2, 0\" : \"=r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER3:\n\t\tasm volatile(\"mrc p14, 0, %0, c3, c2, 0\" : \"=r\" (val));\n\t\tbreak;\n\t}\n\n\treturn val;\n}\n\nstatic inline void\nxscale2pmu_write_counter(int counter, u32 val)\n{\n\tswitch (counter) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tasm volatile(\"mcr p14, 0, %0, c1, c1, 0\" : : \"r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tasm volatile(\"mcr p14, 0, %0, c0, c2, 0\" : : \"r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tasm volatile(\"mcr p14, 0, %0, c1, c2, 0\" : : \"r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER2:\n\t\tasm volatile(\"mcr p14, 0, %0, c2, c2, 0\" : : \"r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER3:\n\t\tasm volatile(\"mcr p14, 0, %0, c3, c2, 0\" : : \"r\" (val));\n\t\tbreak;\n\t}\n}\n\nstatic const struct arm_pmu xscale2pmu = {\n\t.id\t\t= ARM_PERF_PMU_ID_XSCALE2,\n\t.name\t\t= \"xscale2\",\n\t.handle_irq\t= xscale2pmu_handle_irq,\n\t.enable\t\t= xscale2pmu_enable_event,\n\t.disable\t= xscale2pmu_disable_event,\n\t.read_counter\t= xscale2pmu_read_counter,\n\t.write_counter\t= xscale2pmu_write_counter,\n\t.get_event_idx\t= xscale2pmu_get_event_idx,\n\t.start\t\t= xscale2pmu_start,\n\t.stop\t\t= xscale2pmu_stop,\n\t.cache_map\t= &xscale_perf_cache_map,\n\t.event_map\t= &xscale_perf_map,\n\t.raw_event_mask\t= 0xFF,\n\t.num_events\t= 5,\n\t.max_period\t= (1LLU << 32) - 1,\n};\n\nstatic const struct arm_pmu *__init xscale2pmu_init(void)\n{\n\treturn &xscale2pmu;\n}\n#else\nstatic const struct arm_pmu *__init xscale1pmu_init(void)\n{\n\treturn NULL;\n}\n\nstatic const struct arm_pmu *__init xscale2pmu_init(void)\n{\n\treturn NULL;\n}\n#endif\t/* CONFIG_CPU_XSCALE */\n", "/*\n *  linux/arch/arm/kernel/ptrace.c\n *\n *  By Ross Biro 1/23/92\n * edited by Linus Torvalds\n * ARM modifications Copyright (C) 2000 Russell King\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/ptrace.h>\n#include <linux/user.h>\n#include <linux/security.h>\n#include <linux/init.h>\n#include <linux/signal.h>\n#include <linux/uaccess.h>\n#include <linux/perf_event.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/regset.h>\n\n#include <asm/pgtable.h>\n#include <asm/system.h>\n#include <asm/traps.h>\n\n#define REG_PC\t15\n#define REG_PSR\t16\n/*\n * does not yet catch signals sent when the child dies.\n * in exit.c or in signal.c.\n */\n\n#if 0\n/*\n * Breakpoint SWI instruction: SWI &9F0001\n */\n#define BREAKINST_ARM\t0xef9f0001\n#define BREAKINST_THUMB\t0xdf00\t\t/* fill this in later */\n#else\n/*\n * New breakpoints - use an undefined instruction.  The ARM architecture\n * reference manual guarantees that the following instruction space\n * will produce an undefined instruction exception on all CPUs:\n *\n *  ARM:   xxxx 0111 1111 xxxx xxxx xxxx 1111 xxxx\n *  Thumb: 1101 1110 xxxx xxxx\n */\n#define BREAKINST_ARM\t0xe7f001f0\n#define BREAKINST_THUMB\t0xde01\n#endif\n\nstruct pt_regs_offset {\n\tconst char *name;\n\tint offset;\n};\n\n#define REG_OFFSET_NAME(r) \\\n\t{.name = #r, .offset = offsetof(struct pt_regs, ARM_##r)}\n#define REG_OFFSET_END {.name = NULL, .offset = 0}\n\nstatic const struct pt_regs_offset regoffset_table[] = {\n\tREG_OFFSET_NAME(r0),\n\tREG_OFFSET_NAME(r1),\n\tREG_OFFSET_NAME(r2),\n\tREG_OFFSET_NAME(r3),\n\tREG_OFFSET_NAME(r4),\n\tREG_OFFSET_NAME(r5),\n\tREG_OFFSET_NAME(r6),\n\tREG_OFFSET_NAME(r7),\n\tREG_OFFSET_NAME(r8),\n\tREG_OFFSET_NAME(r9),\n\tREG_OFFSET_NAME(r10),\n\tREG_OFFSET_NAME(fp),\n\tREG_OFFSET_NAME(ip),\n\tREG_OFFSET_NAME(sp),\n\tREG_OFFSET_NAME(lr),\n\tREG_OFFSET_NAME(pc),\n\tREG_OFFSET_NAME(cpsr),\n\tREG_OFFSET_NAME(ORIG_r0),\n\tREG_OFFSET_END,\n};\n\n/**\n * regs_query_register_offset() - query register offset from its name\n * @name:\tthe name of a register\n *\n * regs_query_register_offset() returns the offset of a register in struct\n * pt_regs from its name. If the name is invalid, this returns -EINVAL;\n */\nint regs_query_register_offset(const char *name)\n{\n\tconst struct pt_regs_offset *roff;\n\tfor (roff = regoffset_table; roff->name != NULL; roff++)\n\t\tif (!strcmp(roff->name, name))\n\t\t\treturn roff->offset;\n\treturn -EINVAL;\n}\n\n/**\n * regs_query_register_name() - query register name from its offset\n * @offset:\tthe offset of a register in struct pt_regs.\n *\n * regs_query_register_name() returns the name of a register from its\n * offset in struct pt_regs. If the @offset is invalid, this returns NULL;\n */\nconst char *regs_query_register_name(unsigned int offset)\n{\n\tconst struct pt_regs_offset *roff;\n\tfor (roff = regoffset_table; roff->name != NULL; roff++)\n\t\tif (roff->offset == offset)\n\t\t\treturn roff->name;\n\treturn NULL;\n}\n\n/**\n * regs_within_kernel_stack() - check the address in the stack\n * @regs:      pt_regs which contains kernel stack pointer.\n * @addr:      address which is checked.\n *\n * regs_within_kernel_stack() checks @addr is within the kernel stack page(s).\n * If @addr is within the kernel stack, it returns true. If not, returns false.\n */\nbool regs_within_kernel_stack(struct pt_regs *regs, unsigned long addr)\n{\n\treturn ((addr & ~(THREAD_SIZE - 1))  ==\n\t\t(kernel_stack_pointer(regs) & ~(THREAD_SIZE - 1)));\n}\n\n/**\n * regs_get_kernel_stack_nth() - get Nth entry of the stack\n * @regs:\tpt_regs which contains kernel stack pointer.\n * @n:\t\tstack entry number.\n *\n * regs_get_kernel_stack_nth() returns @n th entry of the kernel stack which\n * is specified by @regs. If the @n th entry is NOT in the kernel stack,\n * this returns 0.\n */\nunsigned long regs_get_kernel_stack_nth(struct pt_regs *regs, unsigned int n)\n{\n\tunsigned long *addr = (unsigned long *)kernel_stack_pointer(regs);\n\taddr += n;\n\tif (regs_within_kernel_stack(regs, (unsigned long)addr))\n\t\treturn *addr;\n\telse\n\t\treturn 0;\n}\n\n/*\n * this routine will get a word off of the processes privileged stack.\n * the offset is how far from the base addr as stored in the THREAD.\n * this routine assumes that all the privileged stacks are in our\n * data space.\n */\nstatic inline long get_user_reg(struct task_struct *task, int offset)\n{\n\treturn task_pt_regs(task)->uregs[offset];\n}\n\n/*\n * this routine will put a word on the processes privileged stack.\n * the offset is how far from the base addr as stored in the THREAD.\n * this routine assumes that all the privileged stacks are in our\n * data space.\n */\nstatic inline int\nput_user_reg(struct task_struct *task, int offset, long data)\n{\n\tstruct pt_regs newregs, *regs = task_pt_regs(task);\n\tint ret = -EINVAL;\n\n\tnewregs = *regs;\n\tnewregs.uregs[offset] = data;\n\n\tif (valid_user_regs(&newregs)) {\n\t\tregs->uregs[offset] = data;\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\n/*\n * Called by kernel/ptrace.c when detaching..\n */\nvoid ptrace_disable(struct task_struct *child)\n{\n\t/* Nothing to do. */\n}\n\n/*\n * Handle hitting a breakpoint.\n */\nvoid ptrace_break(struct task_struct *tsk, struct pt_regs *regs)\n{\n\tsiginfo_t info;\n\n\tinfo.si_signo = SIGTRAP;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = TRAP_BRKPT;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs);\n\n\tforce_sig_info(SIGTRAP, &info, tsk);\n}\n\nstatic int break_trap(struct pt_regs *regs, unsigned int instr)\n{\n\tptrace_break(current, regs);\n\treturn 0;\n}\n\nstatic struct undef_hook arm_break_hook = {\n\t.instr_mask\t= 0x0fffffff,\n\t.instr_val\t= 0x07f001f0,\n\t.cpsr_mask\t= PSR_T_BIT,\n\t.cpsr_val\t= 0,\n\t.fn\t\t= break_trap,\n};\n\nstatic struct undef_hook thumb_break_hook = {\n\t.instr_mask\t= 0xffff,\n\t.instr_val\t= 0xde01,\n\t.cpsr_mask\t= PSR_T_BIT,\n\t.cpsr_val\t= PSR_T_BIT,\n\t.fn\t\t= break_trap,\n};\n\nstatic int thumb2_break_trap(struct pt_regs *regs, unsigned int instr)\n{\n\tunsigned int instr2;\n\tvoid __user *pc;\n\n\t/* Check the second half of the instruction.  */\n\tpc = (void __user *)(instruction_pointer(regs) + 2);\n\n\tif (processor_mode(regs) == SVC_MODE) {\n\t\tinstr2 = *(u16 *) pc;\n\t} else {\n\t\tget_user(instr2, (u16 __user *)pc);\n\t}\n\n\tif (instr2 == 0xa000) {\n\t\tptrace_break(current, regs);\n\t\treturn 0;\n\t} else {\n\t\treturn 1;\n\t}\n}\n\nstatic struct undef_hook thumb2_break_hook = {\n\t.instr_mask\t= 0xffff,\n\t.instr_val\t= 0xf7f0,\n\t.cpsr_mask\t= PSR_T_BIT,\n\t.cpsr_val\t= PSR_T_BIT,\n\t.fn\t\t= thumb2_break_trap,\n};\n\nstatic int __init ptrace_break_init(void)\n{\n\tregister_undef_hook(&arm_break_hook);\n\tregister_undef_hook(&thumb_break_hook);\n\tregister_undef_hook(&thumb2_break_hook);\n\treturn 0;\n}\n\ncore_initcall(ptrace_break_init);\n\n/*\n * Read the word at offset \"off\" into the \"struct user\".  We\n * actually access the pt_regs stored on the kernel stack.\n */\nstatic int ptrace_read_user(struct task_struct *tsk, unsigned long off,\n\t\t\t    unsigned long __user *ret)\n{\n\tunsigned long tmp;\n\n\tif (off & 3 || off >= sizeof(struct user))\n\t\treturn -EIO;\n\n\ttmp = 0;\n\tif (off == PT_TEXT_ADDR)\n\t\ttmp = tsk->mm->start_code;\n\telse if (off == PT_DATA_ADDR)\n\t\ttmp = tsk->mm->start_data;\n\telse if (off == PT_TEXT_END_ADDR)\n\t\ttmp = tsk->mm->end_code;\n\telse if (off < sizeof(struct pt_regs))\n\t\ttmp = get_user_reg(tsk, off >> 2);\n\n\treturn put_user(tmp, ret);\n}\n\n/*\n * Write the word at offset \"off\" into \"struct user\".  We\n * actually access the pt_regs stored on the kernel stack.\n */\nstatic int ptrace_write_user(struct task_struct *tsk, unsigned long off,\n\t\t\t     unsigned long val)\n{\n\tif (off & 3 || off >= sizeof(struct user))\n\t\treturn -EIO;\n\n\tif (off >= sizeof(struct pt_regs))\n\t\treturn 0;\n\n\treturn put_user_reg(tsk, off >> 2, val);\n}\n\n#ifdef CONFIG_IWMMXT\n\n/*\n * Get the child iWMMXt state.\n */\nstatic int ptrace_getwmmxregs(struct task_struct *tsk, void __user *ufp)\n{\n\tstruct thread_info *thread = task_thread_info(tsk);\n\n\tif (!test_ti_thread_flag(thread, TIF_USING_IWMMXT))\n\t\treturn -ENODATA;\n\tiwmmxt_task_disable(thread);  /* force it to ram */\n\treturn copy_to_user(ufp, &thread->fpstate.iwmmxt, IWMMXT_SIZE)\n\t\t? -EFAULT : 0;\n}\n\n/*\n * Set the child iWMMXt state.\n */\nstatic int ptrace_setwmmxregs(struct task_struct *tsk, void __user *ufp)\n{\n\tstruct thread_info *thread = task_thread_info(tsk);\n\n\tif (!test_ti_thread_flag(thread, TIF_USING_IWMMXT))\n\t\treturn -EACCES;\n\tiwmmxt_task_release(thread);  /* force a reload */\n\treturn copy_from_user(&thread->fpstate.iwmmxt, ufp, IWMMXT_SIZE)\n\t\t? -EFAULT : 0;\n}\n\n#endif\n\n#ifdef CONFIG_CRUNCH\n/*\n * Get the child Crunch state.\n */\nstatic int ptrace_getcrunchregs(struct task_struct *tsk, void __user *ufp)\n{\n\tstruct thread_info *thread = task_thread_info(tsk);\n\n\tcrunch_task_disable(thread);  /* force it to ram */\n\treturn copy_to_user(ufp, &thread->crunchstate, CRUNCH_SIZE)\n\t\t? -EFAULT : 0;\n}\n\n/*\n * Set the child Crunch state.\n */\nstatic int ptrace_setcrunchregs(struct task_struct *tsk, void __user *ufp)\n{\n\tstruct thread_info *thread = task_thread_info(tsk);\n\n\tcrunch_task_release(thread);  /* force a reload */\n\treturn copy_from_user(&thread->crunchstate, ufp, CRUNCH_SIZE)\n\t\t? -EFAULT : 0;\n}\n#endif\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n/*\n * Convert a virtual register number into an index for a thread_info\n * breakpoint array. Breakpoints are identified using positive numbers\n * whilst watchpoints are negative. The registers are laid out as pairs\n * of (address, control), each pair mapping to a unique hw_breakpoint struct.\n * Register 0 is reserved for describing resource information.\n */\nstatic int ptrace_hbp_num_to_idx(long num)\n{\n\tif (num < 0)\n\t\tnum = (ARM_MAX_BRP << 1) - num;\n\treturn (num - 1) >> 1;\n}\n\n/*\n * Returns the virtual register number for the address of the\n * breakpoint at index idx.\n */\nstatic long ptrace_hbp_idx_to_num(int idx)\n{\n\tlong mid = ARM_MAX_BRP << 1;\n\tlong num = (idx << 1) + 1;\n\treturn num > mid ? mid - num : num;\n}\n\n/*\n * Handle hitting a HW-breakpoint.\n */\nstatic void ptrace_hbptriggered(struct perf_event *bp, int unused,\n\t\t\t\t     struct perf_sample_data *data,\n\t\t\t\t     struct pt_regs *regs)\n{\n\tstruct arch_hw_breakpoint *bkpt = counter_arch_bp(bp);\n\tlong num;\n\tint i;\n\tsiginfo_t info;\n\n\tfor (i = 0; i < ARM_MAX_HBP_SLOTS; ++i)\n\t\tif (current->thread.debug.hbp[i] == bp)\n\t\t\tbreak;\n\n\tnum = (i == ARM_MAX_HBP_SLOTS) ? 0 : ptrace_hbp_idx_to_num(i);\n\n\tinfo.si_signo\t= SIGTRAP;\n\tinfo.si_errno\t= (int)num;\n\tinfo.si_code\t= TRAP_HWBKPT;\n\tinfo.si_addr\t= (void __user *)(bkpt->trigger);\n\n\tforce_sig_info(SIGTRAP, &info, current);\n}\n\n/*\n * Set ptrace breakpoint pointers to zero for this task.\n * This is required in order to prevent child processes from unregistering\n * breakpoints held by their parent.\n */\nvoid clear_ptrace_hw_breakpoint(struct task_struct *tsk)\n{\n\tmemset(tsk->thread.debug.hbp, 0, sizeof(tsk->thread.debug.hbp));\n}\n\n/*\n * Unregister breakpoints from this task and reset the pointers in\n * the thread_struct.\n */\nvoid flush_ptrace_hw_breakpoint(struct task_struct *tsk)\n{\n\tint i;\n\tstruct thread_struct *t = &tsk->thread;\n\n\tfor (i = 0; i < ARM_MAX_HBP_SLOTS; i++) {\n\t\tif (t->debug.hbp[i]) {\n\t\t\tunregister_hw_breakpoint(t->debug.hbp[i]);\n\t\t\tt->debug.hbp[i] = NULL;\n\t\t}\n\t}\n}\n\nstatic u32 ptrace_get_hbp_resource_info(void)\n{\n\tu8 num_brps, num_wrps, debug_arch, wp_len;\n\tu32 reg = 0;\n\n\tnum_brps\t= hw_breakpoint_slots(TYPE_INST);\n\tnum_wrps\t= hw_breakpoint_slots(TYPE_DATA);\n\tdebug_arch\t= arch_get_debug_arch();\n\twp_len\t\t= arch_get_max_wp_len();\n\n\treg\t\t|= debug_arch;\n\treg\t\t<<= 8;\n\treg\t\t|= wp_len;\n\treg\t\t<<= 8;\n\treg\t\t|= num_wrps;\n\treg\t\t<<= 8;\n\treg\t\t|= num_brps;\n\n\treturn reg;\n}\n\nstatic struct perf_event *ptrace_hbp_create(struct task_struct *tsk, int type)\n{\n\tstruct perf_event_attr attr;\n\n\tptrace_breakpoint_init(&attr);\n\n\t/* Initialise fields to sane defaults. */\n\tattr.bp_addr\t= 0;\n\tattr.bp_len\t= HW_BREAKPOINT_LEN_4;\n\tattr.bp_type\t= type;\n\tattr.disabled\t= 1;\n\n\treturn register_user_hw_breakpoint(&attr, ptrace_hbptriggered, tsk);\n}\n\nstatic int ptrace_gethbpregs(struct task_struct *tsk, long num,\n\t\t\t     unsigned long  __user *data)\n{\n\tu32 reg;\n\tint idx, ret = 0;\n\tstruct perf_event *bp;\n\tstruct arch_hw_breakpoint_ctrl arch_ctrl;\n\n\tif (num == 0) {\n\t\treg = ptrace_get_hbp_resource_info();\n\t} else {\n\t\tidx = ptrace_hbp_num_to_idx(num);\n\t\tif (idx < 0 || idx >= ARM_MAX_HBP_SLOTS) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tbp = tsk->thread.debug.hbp[idx];\n\t\tif (!bp) {\n\t\t\treg = 0;\n\t\t\tgoto put;\n\t\t}\n\n\t\tarch_ctrl = counter_arch_bp(bp)->ctrl;\n\n\t\t/*\n\t\t * Fix up the len because we may have adjusted it\n\t\t * to compensate for an unaligned address.\n\t\t */\n\t\twhile (!(arch_ctrl.len & 0x1))\n\t\t\tarch_ctrl.len >>= 1;\n\n\t\tif (num & 0x1)\n\t\t\treg = bp->attr.bp_addr;\n\t\telse\n\t\t\treg = encode_ctrl_reg(arch_ctrl);\n\t}\n\nput:\n\tif (put_user(reg, data))\n\t\tret = -EFAULT;\n\nout:\n\treturn ret;\n}\n\nstatic int ptrace_sethbpregs(struct task_struct *tsk, long num,\n\t\t\t     unsigned long __user *data)\n{\n\tint idx, gen_len, gen_type, implied_type, ret = 0;\n\tu32 user_val;\n\tstruct perf_event *bp;\n\tstruct arch_hw_breakpoint_ctrl ctrl;\n\tstruct perf_event_attr attr;\n\n\tif (num == 0)\n\t\tgoto out;\n\telse if (num < 0)\n\t\timplied_type = HW_BREAKPOINT_RW;\n\telse\n\t\timplied_type = HW_BREAKPOINT_X;\n\n\tidx = ptrace_hbp_num_to_idx(num);\n\tif (idx < 0 || idx >= ARM_MAX_HBP_SLOTS) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (get_user(user_val, data)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tbp = tsk->thread.debug.hbp[idx];\n\tif (!bp) {\n\t\tbp = ptrace_hbp_create(tsk, implied_type);\n\t\tif (IS_ERR(bp)) {\n\t\t\tret = PTR_ERR(bp);\n\t\t\tgoto out;\n\t\t}\n\t\ttsk->thread.debug.hbp[idx] = bp;\n\t}\n\n\tattr = bp->attr;\n\n\tif (num & 0x1) {\n\t\t/* Address */\n\t\tattr.bp_addr\t= user_val;\n\t} else {\n\t\t/* Control */\n\t\tdecode_ctrl_reg(user_val, &ctrl);\n\t\tret = arch_bp_generic_fields(ctrl, &gen_len, &gen_type);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tif ((gen_type & implied_type) != gen_type) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tattr.bp_len\t= gen_len;\n\t\tattr.bp_type\t= gen_type;\n\t\tattr.disabled\t= !ctrl.enabled;\n\t}\n\n\tret = modify_user_hw_breakpoint(bp, &attr);\nout:\n\treturn ret;\n}\n#endif\n\n/* regset get/set implementations */\n\nstatic int gpr_get(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n\tstruct pt_regs *regs = task_pt_regs(target);\n\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t   regs,\n\t\t\t\t   0, sizeof(*regs));\n}\n\nstatic int gpr_set(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   const void *kbuf, const void __user *ubuf)\n{\n\tint ret;\n\tstruct pt_regs newregs;\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t &newregs,\n\t\t\t\t 0, sizeof(newregs));\n\tif (ret)\n\t\treturn ret;\n\n\tif (!valid_user_regs(&newregs))\n\t\treturn -EINVAL;\n\n\t*task_pt_regs(target) = newregs;\n\treturn 0;\n}\n\nstatic int fpa_get(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t   &task_thread_info(target)->fpstate,\n\t\t\t\t   0, sizeof(struct user_fp));\n}\n\nstatic int fpa_set(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   const void *kbuf, const void __user *ubuf)\n{\n\tstruct thread_info *thread = task_thread_info(target);\n\n\tthread->used_cp[1] = thread->used_cp[2] = 1;\n\n\treturn user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t&thread->fpstate,\n\t\t0, sizeof(struct user_fp));\n}\n\n#ifdef CONFIG_VFP\n/*\n * VFP register get/set implementations.\n *\n * With respect to the kernel, struct user_fp is divided into three chunks:\n * 16 or 32 real VFP registers (d0-d15 or d0-31)\n *\tThese are transferred to/from the real registers in the task's\n *\tvfp_hard_struct.  The number of registers depends on the kernel\n *\tconfiguration.\n *\n * 16 or 0 fake VFP registers (d16-d31 or empty)\n *\ti.e., the user_vfp structure has space for 32 registers even if\n *\tthe kernel doesn't have them all.\n *\n *\tvfp_get() reads this chunk as zero where applicable\n *\tvfp_set() ignores this chunk\n *\n * 1 word for the FPSCR\n *\n * The bounds-checking logic built into user_regset_copyout and friends\n * means that we can make a simple sequence of calls to map the relevant data\n * to/from the specified slice of the user regset structure.\n */\nstatic int vfp_get(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n\tint ret;\n\tstruct thread_info *thread = task_thread_info(target);\n\tstruct vfp_hard_struct const *vfp = &thread->vfpstate.hard;\n\tconst size_t user_fpregs_offset = offsetof(struct user_vfp, fpregs);\n\tconst size_t user_fpscr_offset = offsetof(struct user_vfp, fpscr);\n\n\tvfp_sync_hwstate(thread);\n\n\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  &vfp->fpregs,\n\t\t\t\t  user_fpregs_offset,\n\t\t\t\t  user_fpregs_offset + sizeof(vfp->fpregs));\n\tif (ret)\n\t\treturn ret;\n\n\tret = user_regset_copyout_zero(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t       user_fpregs_offset + sizeof(vfp->fpregs),\n\t\t\t\t       user_fpscr_offset);\n\tif (ret)\n\t\treturn ret;\n\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t   &vfp->fpscr,\n\t\t\t\t   user_fpscr_offset,\n\t\t\t\t   user_fpscr_offset + sizeof(vfp->fpscr));\n}\n\n/*\n * For vfp_set() a read-modify-write is done on the VFP registers,\n * in order to avoid writing back a half-modified set of registers on\n * failure.\n */\nstatic int vfp_set(struct task_struct *target,\n\t\t\t  const struct user_regset *regset,\n\t\t\t  unsigned int pos, unsigned int count,\n\t\t\t  const void *kbuf, const void __user *ubuf)\n{\n\tint ret;\n\tstruct thread_info *thread = task_thread_info(target);\n\tstruct vfp_hard_struct new_vfp = thread->vfpstate.hard;\n\tconst size_t user_fpregs_offset = offsetof(struct user_vfp, fpregs);\n\tconst size_t user_fpscr_offset = offsetof(struct user_vfp, fpscr);\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  &new_vfp.fpregs,\n\t\t\t\t  user_fpregs_offset,\n\t\t\t\t  user_fpregs_offset + sizeof(new_vfp.fpregs));\n\tif (ret)\n\t\treturn ret;\n\n\tret = user_regset_copyin_ignore(&pos, &count, &kbuf, &ubuf,\n\t\t\t\tuser_fpregs_offset + sizeof(new_vfp.fpregs),\n\t\t\t\tuser_fpscr_offset);\n\tif (ret)\n\t\treturn ret;\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t &new_vfp.fpscr,\n\t\t\t\t user_fpscr_offset,\n\t\t\t\t user_fpscr_offset + sizeof(new_vfp.fpscr));\n\tif (ret)\n\t\treturn ret;\n\n\tvfp_sync_hwstate(thread);\n\tthread->vfpstate.hard = new_vfp;\n\tvfp_flush_hwstate(thread);\n\n\treturn 0;\n}\n#endif /* CONFIG_VFP */\n\nenum arm_regset {\n\tREGSET_GPR,\n\tREGSET_FPR,\n#ifdef CONFIG_VFP\n\tREGSET_VFP,\n#endif\n};\n\nstatic const struct user_regset arm_regsets[] = {\n\t[REGSET_GPR] = {\n\t\t.core_note_type = NT_PRSTATUS,\n\t\t.n = ELF_NGREG,\n\t\t.size = sizeof(u32),\n\t\t.align = sizeof(u32),\n\t\t.get = gpr_get,\n\t\t.set = gpr_set\n\t},\n\t[REGSET_FPR] = {\n\t\t/*\n\t\t * For the FPA regs in fpstate, the real fields are a mixture\n\t\t * of sizes, so pretend that the registers are word-sized:\n\t\t */\n\t\t.core_note_type = NT_PRFPREG,\n\t\t.n = sizeof(struct user_fp) / sizeof(u32),\n\t\t.size = sizeof(u32),\n\t\t.align = sizeof(u32),\n\t\t.get = fpa_get,\n\t\t.set = fpa_set\n\t},\n#ifdef CONFIG_VFP\n\t[REGSET_VFP] = {\n\t\t/*\n\t\t * Pretend that the VFP regs are word-sized, since the FPSCR is\n\t\t * a single word dangling at the end of struct user_vfp:\n\t\t */\n\t\t.core_note_type = NT_ARM_VFP,\n\t\t.n = ARM_VFPREGS_SIZE / sizeof(u32),\n\t\t.size = sizeof(u32),\n\t\t.align = sizeof(u32),\n\t\t.get = vfp_get,\n\t\t.set = vfp_set\n\t},\n#endif /* CONFIG_VFP */\n};\n\nstatic const struct user_regset_view user_arm_view = {\n\t.name = \"arm\", .e_machine = ELF_ARCH, .ei_osabi = ELF_OSABI,\n\t.regsets = arm_regsets, .n = ARRAY_SIZE(arm_regsets)\n};\n\nconst struct user_regset_view *task_user_regset_view(struct task_struct *task)\n{\n\treturn &user_arm_view;\n}\n\nlong arch_ptrace(struct task_struct *child, long request,\n\t\t unsigned long addr, unsigned long data)\n{\n\tint ret;\n\tunsigned long __user *datap = (unsigned long __user *) data;\n\n\tswitch (request) {\n\t\tcase PTRACE_PEEKUSR:\n\t\t\tret = ptrace_read_user(child, addr, datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_POKEUSR:\n\t\t\tret = ptrace_write_user(child, addr, data);\n\t\t\tbreak;\n\n\t\tcase PTRACE_GETREGS:\n\t\t\tret = copy_regset_to_user(child,\n\t\t\t\t\t\t  &user_arm_view, REGSET_GPR,\n\t\t\t\t\t\t  0, sizeof(struct pt_regs),\n\t\t\t\t\t\t  datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETREGS:\n\t\t\tret = copy_regset_from_user(child,\n\t\t\t\t\t\t    &user_arm_view, REGSET_GPR,\n\t\t\t\t\t\t    0, sizeof(struct pt_regs),\n\t\t\t\t\t\t    datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_GETFPREGS:\n\t\t\tret = copy_regset_to_user(child,\n\t\t\t\t\t\t  &user_arm_view, REGSET_FPR,\n\t\t\t\t\t\t  0, sizeof(union fp_state),\n\t\t\t\t\t\t  datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETFPREGS:\n\t\t\tret = copy_regset_from_user(child,\n\t\t\t\t\t\t    &user_arm_view, REGSET_FPR,\n\t\t\t\t\t\t    0, sizeof(union fp_state),\n\t\t\t\t\t\t    datap);\n\t\t\tbreak;\n\n#ifdef CONFIG_IWMMXT\n\t\tcase PTRACE_GETWMMXREGS:\n\t\t\tret = ptrace_getwmmxregs(child, datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETWMMXREGS:\n\t\t\tret = ptrace_setwmmxregs(child, datap);\n\t\t\tbreak;\n#endif\n\n\t\tcase PTRACE_GET_THREAD_AREA:\n\t\t\tret = put_user(task_thread_info(child)->tp_value,\n\t\t\t\t       datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SET_SYSCALL:\n\t\t\ttask_thread_info(child)->syscall = data;\n\t\t\tret = 0;\n\t\t\tbreak;\n\n#ifdef CONFIG_CRUNCH\n\t\tcase PTRACE_GETCRUNCHREGS:\n\t\t\tret = ptrace_getcrunchregs(child, datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETCRUNCHREGS:\n\t\t\tret = ptrace_setcrunchregs(child, datap);\n\t\t\tbreak;\n#endif\n\n#ifdef CONFIG_VFP\n\t\tcase PTRACE_GETVFPREGS:\n\t\t\tret = copy_regset_to_user(child,\n\t\t\t\t\t\t  &user_arm_view, REGSET_VFP,\n\t\t\t\t\t\t  0, ARM_VFPREGS_SIZE,\n\t\t\t\t\t\t  datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETVFPREGS:\n\t\t\tret = copy_regset_from_user(child,\n\t\t\t\t\t\t    &user_arm_view, REGSET_VFP,\n\t\t\t\t\t\t    0, ARM_VFPREGS_SIZE,\n\t\t\t\t\t\t    datap);\n\t\t\tbreak;\n#endif\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\t\tcase PTRACE_GETHBPREGS:\n\t\t\tif (ptrace_get_breakpoints(child) < 0)\n\t\t\t\treturn -ESRCH;\n\n\t\t\tret = ptrace_gethbpregs(child, addr,\n\t\t\t\t\t\t(unsigned long __user *)data);\n\t\t\tptrace_put_breakpoints(child);\n\t\t\tbreak;\n\t\tcase PTRACE_SETHBPREGS:\n\t\t\tif (ptrace_get_breakpoints(child) < 0)\n\t\t\t\treturn -ESRCH;\n\n\t\t\tret = ptrace_sethbpregs(child, addr,\n\t\t\t\t\t\t(unsigned long __user *)data);\n\t\t\tptrace_put_breakpoints(child);\n\t\t\tbreak;\n#endif\n\n\t\tdefault:\n\t\t\tret = ptrace_request(child, request, addr, data);\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nasmlinkage int syscall_trace(int why, struct pt_regs *regs, int scno)\n{\n\tunsigned long ip;\n\n\tif (!test_thread_flag(TIF_SYSCALL_TRACE))\n\t\treturn scno;\n\tif (!(current->ptrace & PT_PTRACED))\n\t\treturn scno;\n\n\t/*\n\t * Save IP.  IP is used to denote syscall entry/exit:\n\t *  IP = 0 -> entry, = 1 -> exit\n\t */\n\tip = regs->ARM_ip;\n\tregs->ARM_ip = why;\n\n\tcurrent_thread_info()->syscall = scno;\n\n\t/* the 0x80 provides a way for the tracing parent to distinguish\n\t   between a syscall stop and SIGTRAP delivery */\n\tptrace_notify(SIGTRAP | ((current->ptrace & PT_TRACESYSGOOD)\n\t\t\t\t ? 0x80 : 0));\n\t/*\n\t * this isn't the same as continuing with a signal, but it will do\n\t * for normal use.  strace only continues with a signal if the\n\t * stopping signal is not SIGTRAP.  -brl\n\t */\n\tif (current->exit_code) {\n\t\tsend_sig(current->exit_code, current, 1);\n\t\tcurrent->exit_code = 0;\n\t}\n\tregs->ARM_ip = ip;\n\n\treturn current_thread_info()->syscall;\n}\n", "/*\n *  linux/arch/arm/kernel/swp_emulate.c\n *\n *  Copyright (C) 2009 ARM Limited\n *  __user_* functions adapted from include/asm/uaccess.h\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n *  Implements emulation of the SWP/SWPB instructions using load-exclusive and\n *  store-exclusive for processors that have them disabled (or future ones that\n *  might not implement them).\n *\n *  Syntax of SWP{B} instruction: SWP{B}<c> <Rt>, <Rt2>, [<Rn>]\n *  Where: Rt  = destination\n *\t   Rt2 = source\n *\t   Rn  = address\n */\n\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/proc_fs.h>\n#include <linux/sched.h>\n#include <linux/syscalls.h>\n#include <linux/perf_event.h>\n\n#include <asm/traps.h>\n#include <asm/uaccess.h>\n\n/*\n * Error-checking SWP macros implemented using ldrex{b}/strex{b}\n */\n#define __user_swpX_asm(data, addr, res, temp, B)\t\t\\\n\t__asm__ __volatile__(\t\t\t\t\t\\\n\t\"\tmov\t\t%2, %1\\n\"\t\t\t\\\n\t\"0:\tldrex\"B\"\t%1, [%3]\\n\"\t\t\t\\\n\t\"1:\tstrex\"B\"\t%0, %2, [%3]\\n\"\t\t\t\\\n\t\"\tcmp\t\t%0, #0\\n\"\t\t\t\\\n\t\"\tmovne\t\t%0, %4\\n\"\t\t\t\\\n\t\"2:\\n\"\t\t\t\t\t\t\t\\\n\t\"\t.section\t .fixup,\\\"ax\\\"\\n\"\t\t\\\n\t\"\t.align\t\t2\\n\"\t\t\t\t\\\n\t\"3:\tmov\t\t%0, %5\\n\"\t\t\t\\\n\t\"\tb\t\t2b\\n\"\t\t\t\t\\\n\t\"\t.previous\\n\"\t\t\t\t\t\\\n\t\"\t.section\t __ex_table,\\\"a\\\"\\n\"\t\t\\\n\t\"\t.align\t\t3\\n\"\t\t\t\t\\\n\t\"\t.long\t\t0b, 3b\\n\"\t\t\t\\\n\t\"\t.long\t\t1b, 3b\\n\"\t\t\t\\\n\t\"\t.previous\"\t\t\t\t\t\\\n\t: \"=&r\" (res), \"+r\" (data), \"=&r\" (temp)\t\t\\\n\t: \"r\" (addr), \"i\" (-EAGAIN), \"i\" (-EFAULT)\t\t\\\n\t: \"cc\", \"memory\")\n\n#define __user_swp_asm(data, addr, res, temp) \\\n\t__user_swpX_asm(data, addr, res, temp, \"\")\n#define __user_swpb_asm(data, addr, res, temp) \\\n\t__user_swpX_asm(data, addr, res, temp, \"b\")\n\n/*\n * Macros/defines for extracting register numbers from instruction.\n */\n#define EXTRACT_REG_NUM(instruction, offset) \\\n\t(((instruction) & (0xf << (offset))) >> (offset))\n#define RN_OFFSET  16\n#define RT_OFFSET  12\n#define RT2_OFFSET  0\n/*\n * Bit 22 of the instruction encoding distinguishes between\n * the SWP and SWPB variants (bit set means SWPB).\n */\n#define TYPE_SWPB (1 << 22)\n\nstatic unsigned long swpcounter;\nstatic unsigned long swpbcounter;\nstatic unsigned long abtcounter;\nstatic pid_t         previous_pid;\n\n#ifdef CONFIG_PROC_FS\nstatic int proc_read_status(char *page, char **start, off_t off, int count,\n\t\t\t    int *eof, void *data)\n{\n\tchar *p = page;\n\tint len;\n\n\tp += sprintf(p, \"Emulated SWP:\\t\\t%lu\\n\", swpcounter);\n\tp += sprintf(p, \"Emulated SWPB:\\t\\t%lu\\n\", swpbcounter);\n\tp += sprintf(p, \"Aborted SWP{B}:\\t\\t%lu\\n\", abtcounter);\n\tif (previous_pid != 0)\n\t\tp += sprintf(p, \"Last process:\\t\\t%d\\n\", previous_pid);\n\n\tlen = (p - page) - off;\n\tif (len < 0)\n\t\tlen = 0;\n\n\t*eof = (len <= count) ? 1 : 0;\n\t*start = page + off;\n\n\treturn len;\n}\n#endif\n\n/*\n * Set up process info to signal segmentation fault - called on access error.\n */\nstatic void set_segfault(struct pt_regs *regs, unsigned long addr)\n{\n\tsiginfo_t info;\n\n\tif (find_vma(current->mm, addr) == NULL)\n\t\tinfo.si_code = SEGV_MAPERR;\n\telse\n\t\tinfo.si_code = SEGV_ACCERR;\n\n\tinfo.si_signo = SIGSEGV;\n\tinfo.si_errno = 0;\n\tinfo.si_addr  = (void *) instruction_pointer(regs);\n\n\tpr_debug(\"SWP{B} emulation: access caused memory abort!\\n\");\n\tarm_notify_die(\"Illegal memory access\", regs, &info, 0, 0);\n\n\tabtcounter++;\n}\n\nstatic int emulate_swpX(unsigned int address, unsigned int *data,\n\t\t\tunsigned int type)\n{\n\tunsigned int res = 0;\n\n\tif ((type != TYPE_SWPB) && (address & 0x3)) {\n\t\t/* SWP to unaligned address not permitted */\n\t\tpr_debug(\"SWP instruction on unaligned pointer!\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\twhile (1) {\n\t\tunsigned long temp;\n\n\t\t/*\n\t\t * Barrier required between accessing protected resource and\n\t\t * releasing a lock for it. Legacy code might not have done\n\t\t * this, and we cannot determine that this is not the case\n\t\t * being emulated, so insert always.\n\t\t */\n\t\tsmp_mb();\n\n\t\tif (type == TYPE_SWPB)\n\t\t\t__user_swpb_asm(*data, address, res, temp);\n\t\telse\n\t\t\t__user_swp_asm(*data, address, res, temp);\n\n\t\tif (likely(res != -EAGAIN) || signal_pending(current))\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n\n\tif (res == 0) {\n\t\t/*\n\t\t * Barrier also required between acquiring a lock for a\n\t\t * protected resource and accessing the resource. Inserted for\n\t\t * same reason as above.\n\t\t */\n\t\tsmp_mb();\n\n\t\tif (type == TYPE_SWPB)\n\t\t\tswpbcounter++;\n\t\telse\n\t\t\tswpcounter++;\n\t}\n\n\treturn res;\n}\n\n/*\n * swp_handler logs the id of calling process, dissects the instruction, sanity\n * checks the memory location, calls emulate_swpX for the actual operation and\n * deals with fixup/error handling before returning\n */\nstatic int swp_handler(struct pt_regs *regs, unsigned int instr)\n{\n\tunsigned int address, destreg, data, type;\n\tunsigned int res = 0;\n\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, regs->ARM_pc);\n\n\tif (current->pid != previous_pid) {\n\t\tpr_debug(\"\\\"%s\\\" (%ld) uses deprecated SWP{B} instruction\\n\",\n\t\t\t current->comm, (unsigned long)current->pid);\n\t\tprevious_pid = current->pid;\n\t}\n\n\taddress = regs->uregs[EXTRACT_REG_NUM(instr, RN_OFFSET)];\n\tdata\t= regs->uregs[EXTRACT_REG_NUM(instr, RT2_OFFSET)];\n\tdestreg = EXTRACT_REG_NUM(instr, RT_OFFSET);\n\n\ttype = instr & TYPE_SWPB;\n\n\tpr_debug(\"addr in r%d->0x%08x, dest is r%d, source in r%d->0x%08x)\\n\",\n\t\t EXTRACT_REG_NUM(instr, RN_OFFSET), address,\n\t\t destreg, EXTRACT_REG_NUM(instr, RT2_OFFSET), data);\n\n\t/* Check access in reasonable access range for both SWP and SWPB */\n\tif (!access_ok(VERIFY_WRITE, (address & ~3), 4)) {\n\t\tpr_debug(\"SWP{B} emulation: access to %p not allowed!\\n\",\n\t\t\t (void *)address);\n\t\tres = -EFAULT;\n\t} else {\n\t\tres = emulate_swpX(address, &data, type);\n\t}\n\n\tif (res == 0) {\n\t\t/*\n\t\t * On successful emulation, revert the adjustment to the PC\n\t\t * made in kernel/traps.c in order to resume execution at the\n\t\t * instruction following the SWP{B}.\n\t\t */\n\t\tregs->ARM_pc += 4;\n\t\tregs->uregs[destreg] = data;\n\t} else if (res == -EFAULT) {\n\t\t/*\n\t\t * Memory errors do not mean emulation failed.\n\t\t * Set up signal info to return SEGV, then return OK\n\t\t */\n\t\tset_segfault(regs, address);\n\t}\n\n\treturn 0;\n}\n\n/*\n * Only emulate SWP/SWPB executed in ARM state/User mode.\n * The kernel must be SWP free and SWP{B} does not exist in Thumb/ThumbEE.\n */\nstatic struct undef_hook swp_hook = {\n\t.instr_mask = 0x0fb00ff0,\n\t.instr_val  = 0x01000090,\n\t.cpsr_mask  = MODE_MASK | PSR_T_BIT | PSR_J_BIT,\n\t.cpsr_val   = USR_MODE,\n\t.fn\t    = swp_handler\n};\n\n/*\n * Register handler and create status file in /proc/cpu\n * Invoked as late_initcall, since not needed before init spawned.\n */\nstatic int __init swp_emulation_init(void)\n{\n#ifdef CONFIG_PROC_FS\n\tstruct proc_dir_entry *res;\n\n\tres = create_proc_entry(\"cpu/swp_emulation\", S_IRUGO, NULL);\n\n\tif (!res)\n\t\treturn -ENOMEM;\n\n\tres->read_proc = proc_read_status;\n#endif /* CONFIG_PROC_FS */\n\n\tprintk(KERN_NOTICE \"Registering SWP/SWPB emulation handler\\n\");\n\tregister_undef_hook(&swp_hook);\n\n\treturn 0;\n}\n\nlate_initcall(swp_emulation_init);\n", "/*\n *  linux/arch/arm/mm/fault.c\n *\n *  Copyright (C) 1995  Linus Torvalds\n *  Modifications for ARM processor (c) 1995-2004 Russell King\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#include <linux/module.h>\n#include <linux/signal.h>\n#include <linux/mm.h>\n#include <linux/hardirq.h>\n#include <linux/init.h>\n#include <linux/kprobes.h>\n#include <linux/uaccess.h>\n#include <linux/page-flags.h>\n#include <linux/sched.h>\n#include <linux/highmem.h>\n#include <linux/perf_event.h>\n\n#include <asm/system.h>\n#include <asm/pgtable.h>\n#include <asm/tlbflush.h>\n\n#include \"fault.h\"\n\n/*\n * Fault status register encodings.  We steal bit 31 for our own purposes.\n */\n#define FSR_LNX_PF\t\t(1 << 31)\n#define FSR_WRITE\t\t(1 << 11)\n#define FSR_FS4\t\t\t(1 << 10)\n#define FSR_FS3_0\t\t(15)\n\nstatic inline int fsr_fs(unsigned int fsr)\n{\n\treturn (fsr & FSR_FS3_0) | (fsr & FSR_FS4) >> 6;\n}\n\n#ifdef CONFIG_MMU\n\n#ifdef CONFIG_KPROBES\nstatic inline int notify_page_fault(struct pt_regs *regs, unsigned int fsr)\n{\n\tint ret = 0;\n\n\tif (!user_mode(regs)) {\n\t\t/* kprobe_running() needs smp_processor_id() */\n\t\tpreempt_disable();\n\t\tif (kprobe_running() && kprobe_fault_handler(regs, fsr))\n\t\t\tret = 1;\n\t\tpreempt_enable();\n\t}\n\n\treturn ret;\n}\n#else\nstatic inline int notify_page_fault(struct pt_regs *regs, unsigned int fsr)\n{\n\treturn 0;\n}\n#endif\n\n/*\n * This is useful to dump out the page tables associated with\n * 'addr' in mm 'mm'.\n */\nvoid show_pte(struct mm_struct *mm, unsigned long addr)\n{\n\tpgd_t *pgd;\n\n\tif (!mm)\n\t\tmm = &init_mm;\n\n\tprintk(KERN_ALERT \"pgd = %p\\n\", mm->pgd);\n\tpgd = pgd_offset(mm, addr);\n\tprintk(KERN_ALERT \"[%08lx] *pgd=%08llx\",\n\t\t\taddr, (long long)pgd_val(*pgd));\n\n\tdo {\n\t\tpud_t *pud;\n\t\tpmd_t *pmd;\n\t\tpte_t *pte;\n\n\t\tif (pgd_none(*pgd))\n\t\t\tbreak;\n\n\t\tif (pgd_bad(*pgd)) {\n\t\t\tprintk(\"(bad)\");\n\t\t\tbreak;\n\t\t}\n\n\t\tpud = pud_offset(pgd, addr);\n\t\tif (PTRS_PER_PUD != 1)\n\t\t\tprintk(\", *pud=%08lx\", pud_val(*pud));\n\n\t\tif (pud_none(*pud))\n\t\t\tbreak;\n\n\t\tif (pud_bad(*pud)) {\n\t\t\tprintk(\"(bad)\");\n\t\t\tbreak;\n\t\t}\n\n\t\tpmd = pmd_offset(pud, addr);\n\t\tif (PTRS_PER_PMD != 1)\n\t\t\tprintk(\", *pmd=%08llx\", (long long)pmd_val(*pmd));\n\n\t\tif (pmd_none(*pmd))\n\t\t\tbreak;\n\n\t\tif (pmd_bad(*pmd)) {\n\t\t\tprintk(\"(bad)\");\n\t\t\tbreak;\n\t\t}\n\n\t\t/* We must not map this if we have highmem enabled */\n\t\tif (PageHighMem(pfn_to_page(pmd_val(*pmd) >> PAGE_SHIFT)))\n\t\t\tbreak;\n\n\t\tpte = pte_offset_map(pmd, addr);\n\t\tprintk(\", *pte=%08llx\", (long long)pte_val(*pte));\n\t\tprintk(\", *ppte=%08llx\",\n\t\t       (long long)pte_val(pte[PTE_HWTABLE_PTRS]));\n\t\tpte_unmap(pte);\n\t} while(0);\n\n\tprintk(\"\\n\");\n}\n#else\t\t\t\t\t/* CONFIG_MMU */\nvoid show_pte(struct mm_struct *mm, unsigned long addr)\n{ }\n#endif\t\t\t\t\t/* CONFIG_MMU */\n\n/*\n * Oops.  The kernel tried to access some page that wasn't present.\n */\nstatic void\n__do_kernel_fault(struct mm_struct *mm, unsigned long addr, unsigned int fsr,\n\t\t  struct pt_regs *regs)\n{\n\t/*\n\t * Are we prepared to handle this kernel fault?\n\t */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\t/*\n\t * No handler, we'll have to terminate things with extreme prejudice.\n\t */\n\tbust_spinlocks(1);\n\tprintk(KERN_ALERT\n\t\t\"Unable to handle kernel %s at virtual address %08lx\\n\",\n\t\t(addr < PAGE_SIZE) ? \"NULL pointer dereference\" :\n\t\t\"paging request\", addr);\n\n\tshow_pte(mm, addr);\n\tdie(\"Oops\", regs, fsr);\n\tbust_spinlocks(0);\n\tdo_exit(SIGKILL);\n}\n\n/*\n * Something tried to access memory that isn't in our memory map..\n * User mode accesses just cause a SIGSEGV\n */\nstatic void\n__do_user_fault(struct task_struct *tsk, unsigned long addr,\n\t\tunsigned int fsr, unsigned int sig, int code,\n\t\tstruct pt_regs *regs)\n{\n\tstruct siginfo si;\n\n#ifdef CONFIG_DEBUG_USER\n\tif (user_debug & UDBG_SEGV) {\n\t\tprintk(KERN_DEBUG \"%s: unhandled page fault (%d) at 0x%08lx, code 0x%03x\\n\",\n\t\t       tsk->comm, sig, addr, fsr);\n\t\tshow_pte(tsk->mm, addr);\n\t\tshow_regs(regs);\n\t}\n#endif\n\n\ttsk->thread.address = addr;\n\ttsk->thread.error_code = fsr;\n\ttsk->thread.trap_no = 14;\n\tsi.si_signo = sig;\n\tsi.si_errno = 0;\n\tsi.si_code = code;\n\tsi.si_addr = (void __user *)addr;\n\tforce_sig_info(sig, &si, tsk);\n}\n\nvoid do_bad_area(unsigned long addr, unsigned int fsr, struct pt_regs *regs)\n{\n\tstruct task_struct *tsk = current;\n\tstruct mm_struct *mm = tsk->active_mm;\n\n\t/*\n\t * If we are in kernel mode at this point, we\n\t * have no context to handle this fault with.\n\t */\n\tif (user_mode(regs))\n\t\t__do_user_fault(tsk, addr, fsr, SIGSEGV, SEGV_MAPERR, regs);\n\telse\n\t\t__do_kernel_fault(mm, addr, fsr, regs);\n}\n\n#ifdef CONFIG_MMU\n#define VM_FAULT_BADMAP\t\t0x010000\n#define VM_FAULT_BADACCESS\t0x020000\n\n/*\n * Check that the permissions on the VMA allow for the fault which occurred.\n * If we encountered a write fault, we must have write permission, otherwise\n * we allow any permission.\n */\nstatic inline bool access_error(unsigned int fsr, struct vm_area_struct *vma)\n{\n\tunsigned int mask = VM_READ | VM_WRITE | VM_EXEC;\n\n\tif (fsr & FSR_WRITE)\n\t\tmask = VM_WRITE;\n\tif (fsr & FSR_LNX_PF)\n\t\tmask = VM_EXEC;\n\n\treturn vma->vm_flags & mask ? false : true;\n}\n\nstatic int __kprobes\n__do_page_fault(struct mm_struct *mm, unsigned long addr, unsigned int fsr,\n\t\tstruct task_struct *tsk)\n{\n\tstruct vm_area_struct *vma;\n\tint fault;\n\n\tvma = find_vma(mm, addr);\n\tfault = VM_FAULT_BADMAP;\n\tif (unlikely(!vma))\n\t\tgoto out;\n\tif (unlikely(vma->vm_start > addr))\n\t\tgoto check_stack;\n\n\t/*\n\t * Ok, we have a good vm_area for this\n\t * memory access, so we can handle it.\n\t */\ngood_area:\n\tif (access_error(fsr, vma)) {\n\t\tfault = VM_FAULT_BADACCESS;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault, make\n\t * sure we exit gracefully rather than endlessly redo the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, addr & PAGE_MASK, (fsr & FSR_WRITE) ? FAULT_FLAG_WRITE : 0);\n\tif (unlikely(fault & VM_FAULT_ERROR))\n\t\treturn fault;\n\tif (fault & VM_FAULT_MAJOR)\n\t\ttsk->maj_flt++;\n\telse\n\t\ttsk->min_flt++;\n\treturn fault;\n\ncheck_stack:\n\tif (vma->vm_flags & VM_GROWSDOWN && !expand_stack(vma, addr))\n\t\tgoto good_area;\nout:\n\treturn fault;\n}\n\nstatic int __kprobes\ndo_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tint fault, sig, code;\n\n\tif (notify_page_fault(regs, fsr))\n\t\treturn 0;\n\n\ttsk = current;\n\tmm  = tsk->mm;\n\n\t/*\n\t * If we're in an interrupt or have no user\n\t * context, we must not take the fault..\n\t */\n\tif (in_atomic() || !mm)\n\t\tgoto no_context;\n\n\t/*\n\t * As per x86, we may deadlock here.  However, since the kernel only\n\t * validly references user space from well defined areas of the code,\n\t * we can bug out early if this is from code which shouldn't.\n\t */\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tif (!user_mode(regs) && !search_exception_tables(regs->ARM_pc))\n\t\t\tgoto no_context;\n\t\tdown_read(&mm->mmap_sem);\n\t} else {\n\t\t/*\n\t\t * The above down_read_trylock() might have succeeded in\n\t\t * which case, we'll have missed the might_sleep() from\n\t\t * down_read()\n\t\t */\n\t\tmight_sleep();\n#ifdef CONFIG_DEBUG_VM\n\t\tif (!user_mode(regs) &&\n\t\t    !search_exception_tables(regs->ARM_pc))\n\t\t\tgoto no_context;\n#endif\n\t}\n\n\tfault = __do_page_fault(mm, addr, fsr, tsk);\n\tup_read(&mm->mmap_sem);\n\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, addr);\n\tif (fault & VM_FAULT_MAJOR)\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0, regs, addr);\n\telse if (fault & VM_FAULT_MINOR)\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0, regs, addr);\n\n\t/*\n\t * Handle the \"normal\" case first - VM_FAULT_MAJOR / VM_FAULT_MINOR\n\t */\n\tif (likely(!(fault & (VM_FAULT_ERROR | VM_FAULT_BADMAP | VM_FAULT_BADACCESS))))\n\t\treturn 0;\n\n\tif (fault & VM_FAULT_OOM) {\n\t\t/*\n\t\t * We ran out of memory, call the OOM killer, and return to\n\t\t * userspace (which will retry the fault, or kill us if we\n\t\t * got oom-killed)\n\t\t */\n\t\tpagefault_out_of_memory();\n\t\treturn 0;\n\t}\n\n\t/*\n\t * If we are in kernel mode at this point, we\n\t * have no context to handle this fault with.\n\t */\n\tif (!user_mode(regs))\n\t\tgoto no_context;\n\n\tif (fault & VM_FAULT_SIGBUS) {\n\t\t/*\n\t\t * We had some memory, but were unable to\n\t\t * successfully fix up this page fault.\n\t\t */\n\t\tsig = SIGBUS;\n\t\tcode = BUS_ADRERR;\n\t} else {\n\t\t/*\n\t\t * Something tried to access memory that\n\t\t * isn't in our memory map..\n\t\t */\n\t\tsig = SIGSEGV;\n\t\tcode = fault == VM_FAULT_BADACCESS ?\n\t\t\tSEGV_ACCERR : SEGV_MAPERR;\n\t}\n\n\t__do_user_fault(tsk, addr, fsr, sig, code, regs);\n\treturn 0;\n\nno_context:\n\t__do_kernel_fault(mm, addr, fsr, regs);\n\treturn 0;\n}\n#else\t\t\t\t\t/* CONFIG_MMU */\nstatic int\ndo_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)\n{\n\treturn 0;\n}\n#endif\t\t\t\t\t/* CONFIG_MMU */\n\n/*\n * First Level Translation Fault Handler\n *\n * We enter here because the first level page table doesn't contain\n * a valid entry for the address.\n *\n * If the address is in kernel space (>= TASK_SIZE), then we are\n * probably faulting in the vmalloc() area.\n *\n * If the init_task's first level page tables contains the relevant\n * entry, we copy the it to this task.  If not, we send the process\n * a signal, fixup the exception, or oops the kernel.\n *\n * NOTE! We MUST NOT take any locks for this case. We may be in an\n * interrupt or a critical region, and should only copy the information\n * from the master page table, nothing more.\n */\n#ifdef CONFIG_MMU\nstatic int __kprobes\ndo_translation_fault(unsigned long addr, unsigned int fsr,\n\t\t     struct pt_regs *regs)\n{\n\tunsigned int index;\n\tpgd_t *pgd, *pgd_k;\n\tpud_t *pud, *pud_k;\n\tpmd_t *pmd, *pmd_k;\n\n\tif (addr < TASK_SIZE)\n\t\treturn do_page_fault(addr, fsr, regs);\n\n\tif (user_mode(regs))\n\t\tgoto bad_area;\n\n\tindex = pgd_index(addr);\n\n\t/*\n\t * FIXME: CP15 C1 is write only on ARMv3 architectures.\n\t */\n\tpgd = cpu_get_pgd() + index;\n\tpgd_k = init_mm.pgd + index;\n\n\tif (pgd_none(*pgd_k))\n\t\tgoto bad_area;\n\tif (!pgd_present(*pgd))\n\t\tset_pgd(pgd, *pgd_k);\n\n\tpud = pud_offset(pgd, addr);\n\tpud_k = pud_offset(pgd_k, addr);\n\n\tif (pud_none(*pud_k))\n\t\tgoto bad_area;\n\tif (!pud_present(*pud))\n\t\tset_pud(pud, *pud_k);\n\n\tpmd = pmd_offset(pud, addr);\n\tpmd_k = pmd_offset(pud_k, addr);\n\n\t/*\n\t * On ARM one Linux PGD entry contains two hardware entries (see page\n\t * tables layout in pgtable.h). We normally guarantee that we always\n\t * fill both L1 entries. But create_mapping() doesn't follow the rule.\n\t * It can create inidividual L1 entries, so here we have to call\n\t * pmd_none() check for the entry really corresponded to address, not\n\t * for the first of pair.\n\t */\n\tindex = (addr >> SECTION_SHIFT) & 1;\n\tif (pmd_none(pmd_k[index]))\n\t\tgoto bad_area;\n\n\tcopy_pmd(pmd, pmd_k);\n\treturn 0;\n\nbad_area:\n\tdo_bad_area(addr, fsr, regs);\n\treturn 0;\n}\n#else\t\t\t\t\t/* CONFIG_MMU */\nstatic int\ndo_translation_fault(unsigned long addr, unsigned int fsr,\n\t\t     struct pt_regs *regs)\n{\n\treturn 0;\n}\n#endif\t\t\t\t\t/* CONFIG_MMU */\n\n/*\n * Some section permission faults need to be handled gracefully.\n * They can happen due to a __{get,put}_user during an oops.\n */\nstatic int\ndo_sect_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)\n{\n\tdo_bad_area(addr, fsr, regs);\n\treturn 0;\n}\n\n/*\n * This abort handler always returns \"fault\".\n */\nstatic int\ndo_bad(unsigned long addr, unsigned int fsr, struct pt_regs *regs)\n{\n\treturn 1;\n}\n\nstatic struct fsr_info {\n\tint\t(*fn)(unsigned long addr, unsigned int fsr, struct pt_regs *regs);\n\tint\tsig;\n\tint\tcode;\n\tconst char *name;\n} fsr_info[] = {\n\t/*\n\t * The following are the standard ARMv3 and ARMv4 aborts.  ARMv5\n\t * defines these to be \"precise\" aborts.\n\t */\n\t{ do_bad,\t\tSIGSEGV, 0,\t\t\"vector exception\"\t\t   },\n\t{ do_bad,\t\tSIGBUS,\t BUS_ADRALN,\t\"alignment exception\"\t\t   },\n\t{ do_bad,\t\tSIGKILL, 0,\t\t\"terminal exception\"\t\t   },\n\t{ do_bad,\t\tSIGBUS,\t BUS_ADRALN,\t\"alignment exception\"\t\t   },\n\t{ do_bad,\t\tSIGBUS,\t 0,\t\t\"external abort on linefetch\"\t   },\n\t{ do_translation_fault,\tSIGSEGV, SEGV_MAPERR,\t\"section translation fault\"\t   },\n\t{ do_bad,\t\tSIGBUS,\t 0,\t\t\"external abort on linefetch\"\t   },\n\t{ do_page_fault,\tSIGSEGV, SEGV_MAPERR,\t\"page translation fault\"\t   },\n\t{ do_bad,\t\tSIGBUS,\t 0,\t\t\"external abort on non-linefetch\"  },\n\t{ do_bad,\t\tSIGSEGV, SEGV_ACCERR,\t\"section domain fault\"\t\t   },\n\t{ do_bad,\t\tSIGBUS,\t 0,\t\t\"external abort on non-linefetch\"  },\n\t{ do_bad,\t\tSIGSEGV, SEGV_ACCERR,\t\"page domain fault\"\t\t   },\n\t{ do_bad,\t\tSIGBUS,\t 0,\t\t\"external abort on translation\"\t   },\n\t{ do_sect_fault,\tSIGSEGV, SEGV_ACCERR,\t\"section permission fault\"\t   },\n\t{ do_bad,\t\tSIGBUS,\t 0,\t\t\"external abort on translation\"\t   },\n\t{ do_page_fault,\tSIGSEGV, SEGV_ACCERR,\t\"page permission fault\"\t\t   },\n\t/*\n\t * The following are \"imprecise\" aborts, which are signalled by bit\n\t * 10 of the FSR, and may not be recoverable.  These are only\n\t * supported if the CPU abort handler supports bit 10.\n\t */\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 16\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 17\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 18\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 19\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"lock abort\"\t\t\t   }, /* xscale */\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 21\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  BUS_OBJERR,\t\"imprecise external abort\"\t   }, /* xscale */\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 23\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"dcache parity error\"\t\t   }, /* xscale */\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 25\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 26\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 27\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 28\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 29\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 30\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 31\"\t\t\t   }\n};\n\nvoid __init\nhook_fault_code(int nr, int (*fn)(unsigned long, unsigned int, struct pt_regs *),\n\t\tint sig, int code, const char *name)\n{\n\tif (nr < 0 || nr >= ARRAY_SIZE(fsr_info))\n\t\tBUG();\n\n\tfsr_info[nr].fn   = fn;\n\tfsr_info[nr].sig  = sig;\n\tfsr_info[nr].code = code;\n\tfsr_info[nr].name = name;\n}\n\n/*\n * Dispatch a data abort to the relevant handler.\n */\nasmlinkage void __exception\ndo_DataAbort(unsigned long addr, unsigned int fsr, struct pt_regs *regs)\n{\n\tconst struct fsr_info *inf = fsr_info + fsr_fs(fsr);\n\tstruct siginfo info;\n\n\tif (!inf->fn(addr, fsr & ~FSR_LNX_PF, regs))\n\t\treturn;\n\n\tprintk(KERN_ALERT \"Unhandled fault: %s (0x%03x) at 0x%08lx\\n\",\n\t\tinf->name, fsr, addr);\n\n\tinfo.si_signo = inf->sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = inf->code;\n\tinfo.si_addr  = (void __user *)addr;\n\tarm_notify_die(\"\", regs, &info, fsr, 0);\n}\n\n\nstatic struct fsr_info ifsr_info[] = {\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 0\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 1\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"debug event\"\t\t\t   },\n\t{ do_bad,\t\tSIGSEGV, SEGV_ACCERR,\t\"section access flag fault\"\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 4\"\t\t\t   },\n\t{ do_translation_fault,\tSIGSEGV, SEGV_MAPERR,\t\"section translation fault\"\t   },\n\t{ do_bad,\t\tSIGSEGV, SEGV_ACCERR,\t\"page access flag fault\"\t   },\n\t{ do_page_fault,\tSIGSEGV, SEGV_MAPERR,\t\"page translation fault\"\t   },\n\t{ do_bad,\t\tSIGBUS,\t 0,\t\t\"external abort on non-linefetch\"  },\n\t{ do_bad,\t\tSIGSEGV, SEGV_ACCERR,\t\"section domain fault\"\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 10\"\t\t\t   },\n\t{ do_bad,\t\tSIGSEGV, SEGV_ACCERR,\t\"page domain fault\"\t\t   },\n\t{ do_bad,\t\tSIGBUS,\t 0,\t\t\"external abort on translation\"\t   },\n\t{ do_sect_fault,\tSIGSEGV, SEGV_ACCERR,\t\"section permission fault\"\t   },\n\t{ do_bad,\t\tSIGBUS,\t 0,\t\t\"external abort on translation\"\t   },\n\t{ do_page_fault,\tSIGSEGV, SEGV_ACCERR,\t\"page permission fault\"\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 16\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 17\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 18\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 19\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 20\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 21\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 22\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 23\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 24\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 25\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 26\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 27\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 28\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 29\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 30\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 31\"\t\t\t   },\n};\n\nvoid __init\nhook_ifault_code(int nr, int (*fn)(unsigned long, unsigned int, struct pt_regs *),\n\t\t int sig, int code, const char *name)\n{\n\tif (nr < 0 || nr >= ARRAY_SIZE(ifsr_info))\n\t\tBUG();\n\n\tifsr_info[nr].fn   = fn;\n\tifsr_info[nr].sig  = sig;\n\tifsr_info[nr].code = code;\n\tifsr_info[nr].name = name;\n}\n\nasmlinkage void __exception\ndo_PrefetchAbort(unsigned long addr, unsigned int ifsr, struct pt_regs *regs)\n{\n\tconst struct fsr_info *inf = ifsr_info + fsr_fs(ifsr);\n\tstruct siginfo info;\n\n\tif (!inf->fn(addr, ifsr | FSR_LNX_PF, regs))\n\t\treturn;\n\n\tprintk(KERN_ALERT \"Unhandled prefetch abort: %s (0x%03x) at 0x%08lx\\n\",\n\t\tinf->name, ifsr, addr);\n\n\tinfo.si_signo = inf->sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = inf->code;\n\tinfo.si_addr  = (void __user *)addr;\n\tarm_notify_die(\"\", regs, &info, ifsr, 0);\n}\n\nstatic int __init exceptions_init(void)\n{\n\tif (cpu_architecture() >= CPU_ARCH_ARMv6) {\n\t\thook_fault_code(4, do_translation_fault, SIGSEGV, SEGV_MAPERR,\n\t\t\t\t\"I-cache maintenance fault\");\n\t}\n\n\tif (cpu_architecture() >= CPU_ARCH_ARMv7) {\n\t\t/*\n\t\t * TODO: Access flag faults introduced in ARMv6K.\n\t\t * Runtime check for 'K' extension is needed\n\t\t */\n\t\thook_fault_code(3, do_bad, SIGSEGV, SEGV_MAPERR,\n\t\t\t\t\"section access flag fault\");\n\t\thook_fault_code(6, do_bad, SIGSEGV, SEGV_MAPERR,\n\t\t\t\t\"section access flag fault\");\n\t}\n\n\treturn 0;\n}\n\narch_initcall(exceptions_init);\n", "/*\n * Linux performance counter support for MIPS.\n *\n * Copyright (C) 2010 MIPS Technologies, Inc.\n * Author: Deng-Cheng Zhu\n *\n * This code is based on the implementation for ARM, which is in turn\n * based on the sparc64 perf event code and the x86 code. Performance\n * counter access is based on the MIPS Oprofile code. And the callchain\n * support references the code of MIPS stacktrace.c.\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n\n#include <linux/cpumask.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/kernel.h>\n#include <linux/perf_event.h>\n#include <linux/uaccess.h>\n\n#include <asm/irq.h>\n#include <asm/irq_regs.h>\n#include <asm/stacktrace.h>\n#include <asm/time.h> /* For perf_irq */\n\n/* These are for 32bit counters. For 64bit ones, define them accordingly. */\n#define MAX_PERIOD\t((1ULL << 32) - 1)\n#define VALID_COUNT\t0x7fffffff\n#define TOTAL_BITS\t32\n#define HIGHEST_BIT\t31\n\n#define MIPS_MAX_HWEVENTS 4\n\nstruct cpu_hw_events {\n\t/* Array of events on this cpu. */\n\tstruct perf_event\t*events[MIPS_MAX_HWEVENTS];\n\n\t/*\n\t * Set the bit (indexed by the counter number) when the counter\n\t * is used for an event.\n\t */\n\tunsigned long\t\tused_mask[BITS_TO_LONGS(MIPS_MAX_HWEVENTS)];\n\n\t/*\n\t * The borrowed MSB for the performance counter. A MIPS performance\n\t * counter uses its bit 31 (for 32bit counters) or bit 63 (for 64bit\n\t * counters) as a factor of determining whether a counter overflow\n\t * should be signaled. So here we use a separate MSB for each\n\t * counter to make things easy.\n\t */\n\tunsigned long\t\tmsbs[BITS_TO_LONGS(MIPS_MAX_HWEVENTS)];\n\n\t/*\n\t * Software copy of the control register for each performance counter.\n\t * MIPS CPUs vary in performance counters. They use this differently,\n\t * and even may not use it.\n\t */\n\tunsigned int\t\tsaved_ctrl[MIPS_MAX_HWEVENTS];\n};\nDEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events) = {\n\t.saved_ctrl = {0},\n};\n\n/* The description of MIPS performance events. */\nstruct mips_perf_event {\n\tunsigned int event_id;\n\t/*\n\t * MIPS performance counters are indexed starting from 0.\n\t * CNTR_EVEN indicates the indexes of the counters to be used are\n\t * even numbers.\n\t */\n\tunsigned int cntr_mask;\n\t#define CNTR_EVEN\t0x55555555\n\t#define CNTR_ODD\t0xaaaaaaaa\n#ifdef CONFIG_MIPS_MT_SMP\n\tenum {\n\t\tT  = 0,\n\t\tV  = 1,\n\t\tP  = 2,\n\t} range;\n#else\n\t#define T\n\t#define V\n\t#define P\n#endif\n};\n\nstatic struct mips_perf_event raw_event;\nstatic DEFINE_MUTEX(raw_event_mutex);\n\n#define UNSUPPORTED_PERF_EVENT_ID 0xffffffff\n#define C(x) PERF_COUNT_HW_CACHE_##x\n\nstruct mips_pmu {\n\tconst char\t*name;\n\tint\t\tirq;\n\tirqreturn_t\t(*handle_irq)(int irq, void *dev);\n\tint\t\t(*handle_shared_irq)(void);\n\tvoid\t\t(*start)(void);\n\tvoid\t\t(*stop)(void);\n\tint\t\t(*alloc_counter)(struct cpu_hw_events *cpuc,\n\t\t\t\t\tstruct hw_perf_event *hwc);\n\tu64\t\t(*read_counter)(unsigned int idx);\n\tvoid\t\t(*write_counter)(unsigned int idx, u64 val);\n\tvoid\t\t(*enable_event)(struct hw_perf_event *evt, int idx);\n\tvoid\t\t(*disable_event)(int idx);\n\tconst struct mips_perf_event *(*map_raw_event)(u64 config);\n\tconst struct mips_perf_event (*general_event_map)[PERF_COUNT_HW_MAX];\n\tconst struct mips_perf_event (*cache_event_map)\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX];\n\tunsigned int\tnum_counters;\n};\n\nstatic const struct mips_pmu *mipspmu;\n\nstatic int\nmipspmu_event_set_period(struct perf_event *event,\n\t\t\tstruct hw_perf_event *hwc,\n\t\t\tint idx)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\ts64 left = local64_read(&hwc->period_left);\n\ts64 period = hwc->sample_period;\n\tint ret = 0;\n\tu64 uleft;\n\tunsigned long flags;\n\n\tif (unlikely(left <= -period)) {\n\t\tleft = period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\tif (unlikely(left <= 0)) {\n\t\tleft += period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\tif (left > (s64)MAX_PERIOD)\n\t\tleft = MAX_PERIOD;\n\n\tlocal64_set(&hwc->prev_count, (u64)-left);\n\n\tlocal_irq_save(flags);\n\tuleft = (u64)(-left) & MAX_PERIOD;\n\tuleft > VALID_COUNT ?\n\t\tset_bit(idx, cpuc->msbs) : clear_bit(idx, cpuc->msbs);\n\tmipspmu->write_counter(idx, (u64)(-left) & VALID_COUNT);\n\tlocal_irq_restore(flags);\n\n\tperf_event_update_userpage(event);\n\n\treturn ret;\n}\n\nstatic void mipspmu_event_update(struct perf_event *event,\n\t\t\tstruct hw_perf_event *hwc,\n\t\t\tint idx)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tunsigned long flags;\n\tint shift = 64 - TOTAL_BITS;\n\ts64 prev_raw_count, new_raw_count;\n\tu64 delta;\n\nagain:\n\tprev_raw_count = local64_read(&hwc->prev_count);\n\tlocal_irq_save(flags);\n\t/* Make the counter value be a \"real\" one. */\n\tnew_raw_count = mipspmu->read_counter(idx);\n\tif (new_raw_count & (test_bit(idx, cpuc->msbs) << HIGHEST_BIT)) {\n\t\tnew_raw_count &= VALID_COUNT;\n\t\tclear_bit(idx, cpuc->msbs);\n\t} else\n\t\tnew_raw_count |= (test_bit(idx, cpuc->msbs) << HIGHEST_BIT);\n\tlocal_irq_restore(flags);\n\n\tif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\n\t\t\t\tnew_raw_count) != prev_raw_count)\n\t\tgoto again;\n\n\tdelta = (new_raw_count << shift) - (prev_raw_count << shift);\n\tdelta >>= shift;\n\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &hwc->period_left);\n\n\treturn;\n}\n\nstatic void mipspmu_start(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (!mipspmu)\n\t\treturn;\n\n\tif (flags & PERF_EF_RELOAD)\n\t\tWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\n\n\thwc->state = 0;\n\n\t/* Set the period for the event. */\n\tmipspmu_event_set_period(event, hwc, hwc->idx);\n\n\t/* Enable the event. */\n\tmipspmu->enable_event(hwc, hwc->idx);\n}\n\nstatic void mipspmu_stop(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (!mipspmu)\n\t\treturn;\n\n\tif (!(hwc->state & PERF_HES_STOPPED)) {\n\t\t/* We are working on a local event. */\n\t\tmipspmu->disable_event(hwc->idx);\n\t\tbarrier();\n\t\tmipspmu_event_update(event, hwc, hwc->idx);\n\t\thwc->state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\t}\n}\n\nstatic int mipspmu_add(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx;\n\tint err = 0;\n\n\tperf_pmu_disable(event->pmu);\n\n\t/* To look for a free counter for this event. */\n\tidx = mipspmu->alloc_counter(cpuc, hwc);\n\tif (idx < 0) {\n\t\terr = idx;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If there is an event in the counter we are going to use then\n\t * make sure it is disabled.\n\t */\n\tevent->hw.idx = idx;\n\tmipspmu->disable_event(idx);\n\tcpuc->events[idx] = event;\n\n\thwc->state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\tif (flags & PERF_EF_START)\n\t\tmipspmu_start(event, PERF_EF_RELOAD);\n\n\t/* Propagate our changes to the userspace mapping. */\n\tperf_event_update_userpage(event);\n\nout:\n\tperf_pmu_enable(event->pmu);\n\treturn err;\n}\n\nstatic void mipspmu_del(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\n\tWARN_ON(idx < 0 || idx >= mipspmu->num_counters);\n\n\tmipspmu_stop(event, PERF_EF_UPDATE);\n\tcpuc->events[idx] = NULL;\n\tclear_bit(idx, cpuc->used_mask);\n\n\tperf_event_update_userpage(event);\n}\n\nstatic void mipspmu_read(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t/* Don't read disabled counters! */\n\tif (hwc->idx < 0)\n\t\treturn;\n\n\tmipspmu_event_update(event, hwc, hwc->idx);\n}\n\nstatic void mipspmu_enable(struct pmu *pmu)\n{\n\tif (mipspmu)\n\t\tmipspmu->start();\n}\n\nstatic void mipspmu_disable(struct pmu *pmu)\n{\n\tif (mipspmu)\n\t\tmipspmu->stop();\n}\n\nstatic atomic_t active_events = ATOMIC_INIT(0);\nstatic DEFINE_MUTEX(pmu_reserve_mutex);\nstatic int (*save_perf_irq)(void);\n\nstatic int mipspmu_get_irq(void)\n{\n\tint err;\n\n\tif (mipspmu->irq >= 0) {\n\t\t/* Request my own irq handler. */\n\t\terr = request_irq(mipspmu->irq, mipspmu->handle_irq,\n\t\t\tIRQF_DISABLED | IRQF_NOBALANCING,\n\t\t\t\"mips_perf_pmu\", NULL);\n\t\tif (err) {\n\t\t\tpr_warning(\"Unable to request IRQ%d for MIPS \"\n\t\t\t   \"performance counters!\\n\", mipspmu->irq);\n\t\t}\n\t} else if (cp0_perfcount_irq < 0) {\n\t\t/*\n\t\t * We are sharing the irq number with the timer interrupt.\n\t\t */\n\t\tsave_perf_irq = perf_irq;\n\t\tperf_irq = mipspmu->handle_shared_irq;\n\t\terr = 0;\n\t} else {\n\t\tpr_warning(\"The platform hasn't properly defined its \"\n\t\t\t\"interrupt controller.\\n\");\n\t\terr = -ENOENT;\n\t}\n\n\treturn err;\n}\n\nstatic void mipspmu_free_irq(void)\n{\n\tif (mipspmu->irq >= 0)\n\t\tfree_irq(mipspmu->irq, NULL);\n\telse if (cp0_perfcount_irq < 0)\n\t\tperf_irq = save_perf_irq;\n}\n\n/*\n * mipsxx/rm9000/loongson2 have different performance counters, they have\n * specific low-level init routines.\n */\nstatic void reset_counters(void *arg);\nstatic int __hw_perf_event_init(struct perf_event *event);\n\nstatic void hw_perf_event_destroy(struct perf_event *event)\n{\n\tif (atomic_dec_and_mutex_lock(&active_events,\n\t\t\t\t&pmu_reserve_mutex)) {\n\t\t/*\n\t\t * We must not call the destroy function with interrupts\n\t\t * disabled.\n\t\t */\n\t\ton_each_cpu(reset_counters,\n\t\t\t(void *)(long)mipspmu->num_counters, 1);\n\t\tmipspmu_free_irq();\n\t\tmutex_unlock(&pmu_reserve_mutex);\n\t}\n}\n\nstatic int mipspmu_event_init(struct perf_event *event)\n{\n\tint err = 0;\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_RAW:\n\tcase PERF_TYPE_HARDWARE:\n\tcase PERF_TYPE_HW_CACHE:\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\tif (!mipspmu || event->cpu >= nr_cpumask_bits ||\n\t\t(event->cpu >= 0 && !cpu_online(event->cpu)))\n\t\treturn -ENODEV;\n\n\tif (!atomic_inc_not_zero(&active_events)) {\n\t\tif (atomic_read(&active_events) > MIPS_MAX_HWEVENTS) {\n\t\t\tatomic_dec(&active_events);\n\t\t\treturn -ENOSPC;\n\t\t}\n\n\t\tmutex_lock(&pmu_reserve_mutex);\n\t\tif (atomic_read(&active_events) == 0)\n\t\t\terr = mipspmu_get_irq();\n\n\t\tif (!err)\n\t\t\tatomic_inc(&active_events);\n\t\tmutex_unlock(&pmu_reserve_mutex);\n\t}\n\n\tif (err)\n\t\treturn err;\n\n\terr = __hw_perf_event_init(event);\n\tif (err)\n\t\thw_perf_event_destroy(event);\n\n\treturn err;\n}\n\nstatic struct pmu pmu = {\n\t.pmu_enable\t= mipspmu_enable,\n\t.pmu_disable\t= mipspmu_disable,\n\t.event_init\t= mipspmu_event_init,\n\t.add\t\t= mipspmu_add,\n\t.del\t\t= mipspmu_del,\n\t.start\t\t= mipspmu_start,\n\t.stop\t\t= mipspmu_stop,\n\t.read\t\t= mipspmu_read,\n};\n\nstatic inline unsigned int\nmipspmu_perf_event_encode(const struct mips_perf_event *pev)\n{\n/*\n * Top 8 bits for range, next 16 bits for cntr_mask, lowest 8 bits for\n * event_id.\n */\n#ifdef CONFIG_MIPS_MT_SMP\n\treturn ((unsigned int)pev->range << 24) |\n\t\t(pev->cntr_mask & 0xffff00) |\n\t\t(pev->event_id & 0xff);\n#else\n\treturn (pev->cntr_mask & 0xffff00) |\n\t\t(pev->event_id & 0xff);\n#endif\n}\n\nstatic const struct mips_perf_event *\nmipspmu_map_general_event(int idx)\n{\n\tconst struct mips_perf_event *pev;\n\n\tpev = ((*mipspmu->general_event_map)[idx].event_id ==\n\t\tUNSUPPORTED_PERF_EVENT_ID ? ERR_PTR(-EOPNOTSUPP) :\n\t\t&(*mipspmu->general_event_map)[idx]);\n\n\treturn pev;\n}\n\nstatic const struct mips_perf_event *\nmipspmu_map_cache_event(u64 config)\n{\n\tunsigned int cache_type, cache_op, cache_result;\n\tconst struct mips_perf_event *pev;\n\n\tcache_type = (config >> 0) & 0xff;\n\tif (cache_type >= PERF_COUNT_HW_CACHE_MAX)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tcache_op = (config >> 8) & 0xff;\n\tif (cache_op >= PERF_COUNT_HW_CACHE_OP_MAX)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tcache_result = (config >> 16) & 0xff;\n\tif (cache_result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tpev = &((*mipspmu->cache_event_map)\n\t\t\t\t\t[cache_type]\n\t\t\t\t\t[cache_op]\n\t\t\t\t\t[cache_result]);\n\n\tif (pev->event_id == UNSUPPORTED_PERF_EVENT_ID)\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\treturn pev;\n\n}\n\nstatic int validate_event(struct cpu_hw_events *cpuc,\n\t       struct perf_event *event)\n{\n\tstruct hw_perf_event fake_hwc = event->hw;\n\n\t/* Allow mixed event group. So return 1 to pass validation. */\n\tif (event->pmu != &pmu || event->state <= PERF_EVENT_STATE_OFF)\n\t\treturn 1;\n\n\treturn mipspmu->alloc_counter(cpuc, &fake_hwc) >= 0;\n}\n\nstatic int validate_group(struct perf_event *event)\n{\n\tstruct perf_event *sibling, *leader = event->group_leader;\n\tstruct cpu_hw_events fake_cpuc;\n\n\tmemset(&fake_cpuc, 0, sizeof(fake_cpuc));\n\n\tif (!validate_event(&fake_cpuc, leader))\n\t\treturn -ENOSPC;\n\n\tlist_for_each_entry(sibling, &leader->sibling_list, group_entry) {\n\t\tif (!validate_event(&fake_cpuc, sibling))\n\t\t\treturn -ENOSPC;\n\t}\n\n\tif (!validate_event(&fake_cpuc, event))\n\t\treturn -ENOSPC;\n\n\treturn 0;\n}\n\n/* This is needed by specific irq handlers in perf_event_*.c */\nstatic void\nhandle_associated_event(struct cpu_hw_events *cpuc,\n\tint idx, struct perf_sample_data *data, struct pt_regs *regs)\n{\n\tstruct perf_event *event = cpuc->events[idx];\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tmipspmu_event_update(event, hwc, idx);\n\tdata->period = event->hw.last_period;\n\tif (!mipspmu_event_set_period(event, hwc, idx))\n\t\treturn;\n\n\tif (perf_event_overflow(event, 0, data, regs))\n\t\tmipspmu->disable_event(idx);\n}\n\n#include \"perf_event_mipsxx.c\"\n\n/* Callchain handling code. */\n\n/*\n * Leave userspace callchain empty for now. When we find a way to trace\n * the user stack callchains, we add here.\n */\nvoid perf_callchain_user(struct perf_callchain_entry *entry,\n\t\t    struct pt_regs *regs)\n{\n}\n\nstatic void save_raw_perf_callchain(struct perf_callchain_entry *entry,\n\tunsigned long reg29)\n{\n\tunsigned long *sp = (unsigned long *)reg29;\n\tunsigned long addr;\n\n\twhile (!kstack_end(sp)) {\n\t\taddr = *sp++;\n\t\tif (__kernel_text_address(addr)) {\n\t\t\tperf_callchain_store(entry, addr);\n\t\t\tif (entry->nr >= PERF_MAX_STACK_DEPTH)\n\t\t\t\tbreak;\n\t\t}\n\t}\n}\n\nvoid perf_callchain_kernel(struct perf_callchain_entry *entry,\n\t\t      struct pt_regs *regs)\n{\n\tunsigned long sp = regs->regs[29];\n#ifdef CONFIG_KALLSYMS\n\tunsigned long ra = regs->regs[31];\n\tunsigned long pc = regs->cp0_epc;\n\n\tif (raw_show_trace || !__kernel_text_address(pc)) {\n\t\tunsigned long stack_page =\n\t\t\t(unsigned long)task_stack_page(current);\n\t\tif (stack_page && sp >= stack_page &&\n\t\t    sp <= stack_page + THREAD_SIZE - 32)\n\t\t\tsave_raw_perf_callchain(entry, sp);\n\t\treturn;\n\t}\n\tdo {\n\t\tperf_callchain_store(entry, pc);\n\t\tif (entry->nr >= PERF_MAX_STACK_DEPTH)\n\t\t\tbreak;\n\t\tpc = unwind_stack(current, &sp, pc, &ra);\n\t} while (pc);\n#else\n\tsave_raw_perf_callchain(entry, sp);\n#endif\n}\n", "/*\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n *\n * Copyright (C) 1994 - 1999, 2000, 01, 06 Ralf Baechle\n * Copyright (C) 1995, 1996 Paul M. Antoine\n * Copyright (C) 1998 Ulf Carlsson\n * Copyright (C) 1999 Silicon Graphics, Inc.\n * Kevin D. Kissell, kevink@mips.com and Carsten Langgaard, carstenl@mips.com\n * Copyright (C) 2000, 01 MIPS Technologies, Inc.\n * Copyright (C) 2002, 2003, 2004, 2005, 2007  Maciej W. Rozycki\n */\n#include <linux/bug.h>\n#include <linux/compiler.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/sched.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/kallsyms.h>\n#include <linux/bootmem.h>\n#include <linux/interrupt.h>\n#include <linux/ptrace.h>\n#include <linux/kgdb.h>\n#include <linux/kdebug.h>\n#include <linux/kprobes.h>\n#include <linux/notifier.h>\n#include <linux/kdb.h>\n#include <linux/irq.h>\n#include <linux/perf_event.h>\n\n#include <asm/bootinfo.h>\n#include <asm/branch.h>\n#include <asm/break.h>\n#include <asm/cop2.h>\n#include <asm/cpu.h>\n#include <asm/dsp.h>\n#include <asm/fpu.h>\n#include <asm/fpu_emulator.h>\n#include <asm/mipsregs.h>\n#include <asm/mipsmtregs.h>\n#include <asm/module.h>\n#include <asm/pgtable.h>\n#include <asm/ptrace.h>\n#include <asm/sections.h>\n#include <asm/system.h>\n#include <asm/tlbdebug.h>\n#include <asm/traps.h>\n#include <asm/uaccess.h>\n#include <asm/watch.h>\n#include <asm/mmu_context.h>\n#include <asm/types.h>\n#include <asm/stacktrace.h>\n#include <asm/uasm.h>\n\nextern void check_wait(void);\nextern asmlinkage void r4k_wait(void);\nextern asmlinkage void rollback_handle_int(void);\nextern asmlinkage void handle_int(void);\nextern asmlinkage void handle_tlbm(void);\nextern asmlinkage void handle_tlbl(void);\nextern asmlinkage void handle_tlbs(void);\nextern asmlinkage void handle_adel(void);\nextern asmlinkage void handle_ades(void);\nextern asmlinkage void handle_ibe(void);\nextern asmlinkage void handle_dbe(void);\nextern asmlinkage void handle_sys(void);\nextern asmlinkage void handle_bp(void);\nextern asmlinkage void handle_ri(void);\nextern asmlinkage void handle_ri_rdhwr_vivt(void);\nextern asmlinkage void handle_ri_rdhwr(void);\nextern asmlinkage void handle_cpu(void);\nextern asmlinkage void handle_ov(void);\nextern asmlinkage void handle_tr(void);\nextern asmlinkage void handle_fpe(void);\nextern asmlinkage void handle_mdmx(void);\nextern asmlinkage void handle_watch(void);\nextern asmlinkage void handle_mt(void);\nextern asmlinkage void handle_dsp(void);\nextern asmlinkage void handle_mcheck(void);\nextern asmlinkage void handle_reserved(void);\n\nextern int fpu_emulator_cop1Handler(struct pt_regs *xcp,\n\t\t\t\t    struct mips_fpu_struct *ctx, int has_fpu,\n\t\t\t\t    void *__user *fault_addr);\n\nvoid (*board_be_init)(void);\nint (*board_be_handler)(struct pt_regs *regs, int is_fixup);\nvoid (*board_nmi_handler_setup)(void);\nvoid (*board_ejtag_handler_setup)(void);\nvoid (*board_bind_eic_interrupt)(int irq, int regset);\n\n\nstatic void show_raw_backtrace(unsigned long reg29)\n{\n\tunsigned long *sp = (unsigned long *)(reg29 & ~3);\n\tunsigned long addr;\n\n\tprintk(\"Call Trace:\");\n#ifdef CONFIG_KALLSYMS\n\tprintk(\"\\n\");\n#endif\n\twhile (!kstack_end(sp)) {\n\t\tunsigned long __user *p =\n\t\t\t(unsigned long __user *)(unsigned long)sp++;\n\t\tif (__get_user(addr, p)) {\n\t\t\tprintk(\" (Bad stack address)\");\n\t\t\tbreak;\n\t\t}\n\t\tif (__kernel_text_address(addr))\n\t\t\tprint_ip_sym(addr);\n\t}\n\tprintk(\"\\n\");\n}\n\n#ifdef CONFIG_KALLSYMS\nint raw_show_trace;\nstatic int __init set_raw_show_trace(char *str)\n{\n\traw_show_trace = 1;\n\treturn 1;\n}\n__setup(\"raw_show_trace\", set_raw_show_trace);\n#endif\n\nstatic void show_backtrace(struct task_struct *task, const struct pt_regs *regs)\n{\n\tunsigned long sp = regs->regs[29];\n\tunsigned long ra = regs->regs[31];\n\tunsigned long pc = regs->cp0_epc;\n\n\tif (raw_show_trace || !__kernel_text_address(pc)) {\n\t\tshow_raw_backtrace(sp);\n\t\treturn;\n\t}\n\tprintk(\"Call Trace:\\n\");\n\tdo {\n\t\tprint_ip_sym(pc);\n\t\tpc = unwind_stack(task, &sp, pc, &ra);\n\t} while (pc);\n\tprintk(\"\\n\");\n}\n\n/*\n * This routine abuses get_user()/put_user() to reference pointers\n * with at least a bit of error checking ...\n */\nstatic void show_stacktrace(struct task_struct *task,\n\tconst struct pt_regs *regs)\n{\n\tconst int field = 2 * sizeof(unsigned long);\n\tlong stackdata;\n\tint i;\n\tunsigned long __user *sp = (unsigned long __user *)regs->regs[29];\n\n\tprintk(\"Stack :\");\n\ti = 0;\n\twhile ((unsigned long) sp & (PAGE_SIZE - 1)) {\n\t\tif (i && ((i % (64 / field)) == 0))\n\t\t\tprintk(\"\\n       \");\n\t\tif (i > 39) {\n\t\t\tprintk(\" ...\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (__get_user(stackdata, sp++)) {\n\t\t\tprintk(\" (Bad stack address)\");\n\t\t\tbreak;\n\t\t}\n\n\t\tprintk(\" %0*lx\", field, stackdata);\n\t\ti++;\n\t}\n\tprintk(\"\\n\");\n\tshow_backtrace(task, regs);\n}\n\nvoid show_stack(struct task_struct *task, unsigned long *sp)\n{\n\tstruct pt_regs regs;\n\tif (sp) {\n\t\tregs.regs[29] = (unsigned long)sp;\n\t\tregs.regs[31] = 0;\n\t\tregs.cp0_epc = 0;\n\t} else {\n\t\tif (task && task != current) {\n\t\t\tregs.regs[29] = task->thread.reg29;\n\t\t\tregs.regs[31] = 0;\n\t\t\tregs.cp0_epc = task->thread.reg31;\n#ifdef CONFIG_KGDB_KDB\n\t\t} else if (atomic_read(&kgdb_active) != -1 &&\n\t\t\t   kdb_current_regs) {\n\t\t\tmemcpy(&regs, kdb_current_regs, sizeof(regs));\n#endif /* CONFIG_KGDB_KDB */\n\t\t} else {\n\t\t\tprepare_frametrace(&regs);\n\t\t}\n\t}\n\tshow_stacktrace(task, &regs);\n}\n\n/*\n * The architecture-independent dump_stack generator\n */\nvoid dump_stack(void)\n{\n\tstruct pt_regs regs;\n\n\tprepare_frametrace(&regs);\n\tshow_backtrace(current, &regs);\n}\n\nEXPORT_SYMBOL(dump_stack);\n\nstatic void show_code(unsigned int __user *pc)\n{\n\tlong i;\n\tunsigned short __user *pc16 = NULL;\n\n\tprintk(\"\\nCode:\");\n\n\tif ((unsigned long)pc & 1)\n\t\tpc16 = (unsigned short __user *)((unsigned long)pc & ~1);\n\tfor(i = -3 ; i < 6 ; i++) {\n\t\tunsigned int insn;\n\t\tif (pc16 ? __get_user(insn, pc16 + i) : __get_user(insn, pc + i)) {\n\t\t\tprintk(\" (Bad address in epc)\\n\");\n\t\t\tbreak;\n\t\t}\n\t\tprintk(\"%c%0*x%c\", (i?' ':'<'), pc16 ? 4 : 8, insn, (i?' ':'>'));\n\t}\n}\n\nstatic void __show_regs(const struct pt_regs *regs)\n{\n\tconst int field = 2 * sizeof(unsigned long);\n\tunsigned int cause = regs->cp0_cause;\n\tint i;\n\n\tprintk(\"Cpu %d\\n\", smp_processor_id());\n\n\t/*\n\t * Saved main processor registers\n\t */\n\tfor (i = 0; i < 32; ) {\n\t\tif ((i % 4) == 0)\n\t\t\tprintk(\"$%2d   :\", i);\n\t\tif (i == 0)\n\t\t\tprintk(\" %0*lx\", field, 0UL);\n\t\telse if (i == 26 || i == 27)\n\t\t\tprintk(\" %*s\", field, \"\");\n\t\telse\n\t\t\tprintk(\" %0*lx\", field, regs->regs[i]);\n\n\t\ti++;\n\t\tif ((i % 4) == 0)\n\t\t\tprintk(\"\\n\");\n\t}\n\n#ifdef CONFIG_CPU_HAS_SMARTMIPS\n\tprintk(\"Acx    : %0*lx\\n\", field, regs->acx);\n#endif\n\tprintk(\"Hi    : %0*lx\\n\", field, regs->hi);\n\tprintk(\"Lo    : %0*lx\\n\", field, regs->lo);\n\n\t/*\n\t * Saved cp0 registers\n\t */\n\tprintk(\"epc   : %0*lx %pS\\n\", field, regs->cp0_epc,\n\t       (void *) regs->cp0_epc);\n\tprintk(\"    %s\\n\", print_tainted());\n\tprintk(\"ra    : %0*lx %pS\\n\", field, regs->regs[31],\n\t       (void *) regs->regs[31]);\n\n\tprintk(\"Status: %08x    \", (uint32_t) regs->cp0_status);\n\n\tif (current_cpu_data.isa_level == MIPS_CPU_ISA_I) {\n\t\tif (regs->cp0_status & ST0_KUO)\n\t\t\tprintk(\"KUo \");\n\t\tif (regs->cp0_status & ST0_IEO)\n\t\t\tprintk(\"IEo \");\n\t\tif (regs->cp0_status & ST0_KUP)\n\t\t\tprintk(\"KUp \");\n\t\tif (regs->cp0_status & ST0_IEP)\n\t\t\tprintk(\"IEp \");\n\t\tif (regs->cp0_status & ST0_KUC)\n\t\t\tprintk(\"KUc \");\n\t\tif (regs->cp0_status & ST0_IEC)\n\t\t\tprintk(\"IEc \");\n\t} else {\n\t\tif (regs->cp0_status & ST0_KX)\n\t\t\tprintk(\"KX \");\n\t\tif (regs->cp0_status & ST0_SX)\n\t\t\tprintk(\"SX \");\n\t\tif (regs->cp0_status & ST0_UX)\n\t\t\tprintk(\"UX \");\n\t\tswitch (regs->cp0_status & ST0_KSU) {\n\t\tcase KSU_USER:\n\t\t\tprintk(\"USER \");\n\t\t\tbreak;\n\t\tcase KSU_SUPERVISOR:\n\t\t\tprintk(\"SUPERVISOR \");\n\t\t\tbreak;\n\t\tcase KSU_KERNEL:\n\t\t\tprintk(\"KERNEL \");\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tprintk(\"BAD_MODE \");\n\t\t\tbreak;\n\t\t}\n\t\tif (regs->cp0_status & ST0_ERL)\n\t\t\tprintk(\"ERL \");\n\t\tif (regs->cp0_status & ST0_EXL)\n\t\t\tprintk(\"EXL \");\n\t\tif (regs->cp0_status & ST0_IE)\n\t\t\tprintk(\"IE \");\n\t}\n\tprintk(\"\\n\");\n\n\tprintk(\"Cause : %08x\\n\", cause);\n\n\tcause = (cause & CAUSEF_EXCCODE) >> CAUSEB_EXCCODE;\n\tif (1 <= cause && cause <= 5)\n\t\tprintk(\"BadVA : %0*lx\\n\", field, regs->cp0_badvaddr);\n\n\tprintk(\"PrId  : %08x (%s)\\n\", read_c0_prid(),\n\t       cpu_name_string());\n}\n\n/*\n * FIXME: really the generic show_regs should take a const pointer argument.\n */\nvoid show_regs(struct pt_regs *regs)\n{\n\t__show_regs((struct pt_regs *)regs);\n}\n\nvoid show_registers(struct pt_regs *regs)\n{\n\tconst int field = 2 * sizeof(unsigned long);\n\n\t__show_regs(regs);\n\tprint_modules();\n\tprintk(\"Process %s (pid: %d, threadinfo=%p, task=%p, tls=%0*lx)\\n\",\n\t       current->comm, current->pid, current_thread_info(), current,\n\t      field, current_thread_info()->tp_value);\n\tif (cpu_has_userlocal) {\n\t\tunsigned long tls;\n\n\t\ttls = read_c0_userlocal();\n\t\tif (tls != current_thread_info()->tp_value)\n\t\t\tprintk(\"*HwTLS: %0*lx\\n\", field, tls);\n\t}\n\n\tshow_stacktrace(current, regs);\n\tshow_code((unsigned int __user *) regs->cp0_epc);\n\tprintk(\"\\n\");\n}\n\nstatic int regs_to_trapnr(struct pt_regs *regs)\n{\n\treturn (regs->cp0_cause >> 2) & 0x1f;\n}\n\nstatic DEFINE_SPINLOCK(die_lock);\n\nvoid __noreturn die(const char *str, struct pt_regs *regs)\n{\n\tstatic int die_counter;\n\tint sig = SIGSEGV;\n#ifdef CONFIG_MIPS_MT_SMTC\n\tunsigned long dvpret = dvpe();\n#endif /* CONFIG_MIPS_MT_SMTC */\n\n\tif (notify_die(DIE_OOPS, str, regs, 0, regs_to_trapnr(regs), SIGSEGV) == NOTIFY_STOP)\n\t\tsig = 0;\n\n\tconsole_verbose();\n\tspin_lock_irq(&die_lock);\n\tbust_spinlocks(1);\n#ifdef CONFIG_MIPS_MT_SMTC\n\tmips_mt_regdump(dvpret);\n#endif /* CONFIG_MIPS_MT_SMTC */\n\n\tprintk(\"%s[#%d]:\\n\", str, ++die_counter);\n\tshow_registers(regs);\n\tadd_taint(TAINT_DIE);\n\tspin_unlock_irq(&die_lock);\n\n\tif (in_interrupt())\n\t\tpanic(\"Fatal exception in interrupt\");\n\n\tif (panic_on_oops) {\n\t\tprintk(KERN_EMERG \"Fatal exception: panic in 5 seconds\\n\");\n\t\tssleep(5);\n\t\tpanic(\"Fatal exception\");\n\t}\n\n\tdo_exit(sig);\n}\n\nextern struct exception_table_entry __start___dbe_table[];\nextern struct exception_table_entry __stop___dbe_table[];\n\n__asm__(\n\"\t.section\t__dbe_table, \\\"a\\\"\\n\"\n\"\t.previous\t\t\t\\n\");\n\n/* Given an address, look for it in the exception tables. */\nstatic const struct exception_table_entry *search_dbe_tables(unsigned long addr)\n{\n\tconst struct exception_table_entry *e;\n\n\te = search_extable(__start___dbe_table, __stop___dbe_table - 1, addr);\n\tif (!e)\n\t\te = search_module_dbetables(addr);\n\treturn e;\n}\n\nasmlinkage void do_be(struct pt_regs *regs)\n{\n\tconst int field = 2 * sizeof(unsigned long);\n\tconst struct exception_table_entry *fixup = NULL;\n\tint data = regs->cp0_cause & 4;\n\tint action = MIPS_BE_FATAL;\n\n\t/* XXX For now.  Fixme, this searches the wrong table ...  */\n\tif (data && !user_mode(regs))\n\t\tfixup = search_dbe_tables(exception_epc(regs));\n\n\tif (fixup)\n\t\taction = MIPS_BE_FIXUP;\n\n\tif (board_be_handler)\n\t\taction = board_be_handler(regs, fixup != NULL);\n\n\tswitch (action) {\n\tcase MIPS_BE_DISCARD:\n\t\treturn;\n\tcase MIPS_BE_FIXUP:\n\t\tif (fixup) {\n\t\t\tregs->cp0_epc = fixup->nextinsn;\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/*\n\t * Assume it would be too dangerous to continue ...\n\t */\n\tprintk(KERN_ALERT \"%s bus error, epc == %0*lx, ra == %0*lx\\n\",\n\t       data ? \"Data\" : \"Instruction\",\n\t       field, regs->cp0_epc, field, regs->regs[31]);\n\tif (notify_die(DIE_OOPS, \"bus error\", regs, 0, regs_to_trapnr(regs), SIGBUS)\n\t    == NOTIFY_STOP)\n\t\treturn;\n\n\tdie_if_kernel(\"Oops\", regs);\n\tforce_sig(SIGBUS, current);\n}\n\n/*\n * ll/sc, rdhwr, sync emulation\n */\n\n#define OPCODE 0xfc000000\n#define BASE   0x03e00000\n#define RT     0x001f0000\n#define OFFSET 0x0000ffff\n#define LL     0xc0000000\n#define SC     0xe0000000\n#define SPEC0  0x00000000\n#define SPEC3  0x7c000000\n#define RD     0x0000f800\n#define FUNC   0x0000003f\n#define SYNC   0x0000000f\n#define RDHWR  0x0000003b\n\n/*\n * The ll_bit is cleared by r*_switch.S\n */\n\nunsigned int ll_bit;\nstruct task_struct *ll_task;\n\nstatic inline int simulate_ll(struct pt_regs *regs, unsigned int opcode)\n{\n\tunsigned long value, __user *vaddr;\n\tlong offset;\n\n\t/*\n\t * analyse the ll instruction that just caused a ri exception\n\t * and put the referenced address to addr.\n\t */\n\n\t/* sign extend offset */\n\toffset = opcode & OFFSET;\n\toffset <<= 16;\n\toffset >>= 16;\n\n\tvaddr = (unsigned long __user *)\n\t        ((unsigned long)(regs->regs[(opcode & BASE) >> 21]) + offset);\n\n\tif ((unsigned long)vaddr & 3)\n\t\treturn SIGBUS;\n\tif (get_user(value, vaddr))\n\t\treturn SIGSEGV;\n\n\tpreempt_disable();\n\n\tif (ll_task == NULL || ll_task == current) {\n\t\tll_bit = 1;\n\t} else {\n\t\tll_bit = 0;\n\t}\n\tll_task = current;\n\n\tpreempt_enable();\n\n\tregs->regs[(opcode & RT) >> 16] = value;\n\n\treturn 0;\n}\n\nstatic inline int simulate_sc(struct pt_regs *regs, unsigned int opcode)\n{\n\tunsigned long __user *vaddr;\n\tunsigned long reg;\n\tlong offset;\n\n\t/*\n\t * analyse the sc instruction that just caused a ri exception\n\t * and put the referenced address to addr.\n\t */\n\n\t/* sign extend offset */\n\toffset = opcode & OFFSET;\n\toffset <<= 16;\n\toffset >>= 16;\n\n\tvaddr = (unsigned long __user *)\n\t        ((unsigned long)(regs->regs[(opcode & BASE) >> 21]) + offset);\n\treg = (opcode & RT) >> 16;\n\n\tif ((unsigned long)vaddr & 3)\n\t\treturn SIGBUS;\n\n\tpreempt_disable();\n\n\tif (ll_bit == 0 || ll_task != current) {\n\t\tregs->regs[reg] = 0;\n\t\tpreempt_enable();\n\t\treturn 0;\n\t}\n\n\tpreempt_enable();\n\n\tif (put_user(regs->regs[reg], vaddr))\n\t\treturn SIGSEGV;\n\n\tregs->regs[reg] = 1;\n\n\treturn 0;\n}\n\n/*\n * ll uses the opcode of lwc0 and sc uses the opcode of swc0.  That is both\n * opcodes are supposed to result in coprocessor unusable exceptions if\n * executed on ll/sc-less processors.  That's the theory.  In practice a\n * few processors such as NEC's VR4100 throw reserved instruction exceptions\n * instead, so we're doing the emulation thing in both exception handlers.\n */\nstatic int simulate_llsc(struct pt_regs *regs, unsigned int opcode)\n{\n\tif ((opcode & OPCODE) == LL) {\n\t\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,\n\t\t\t\t1, 0, regs, 0);\n\t\treturn simulate_ll(regs, opcode);\n\t}\n\tif ((opcode & OPCODE) == SC) {\n\t\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,\n\t\t\t\t1, 0, regs, 0);\n\t\treturn simulate_sc(regs, opcode);\n\t}\n\n\treturn -1;\t\t\t/* Must be something else ... */\n}\n\n/*\n * Simulate trapping 'rdhwr' instructions to provide user accessible\n * registers not implemented in hardware.\n */\nstatic int simulate_rdhwr(struct pt_regs *regs, unsigned int opcode)\n{\n\tstruct thread_info *ti = task_thread_info(current);\n\n\tif ((opcode & OPCODE) == SPEC3 && (opcode & FUNC) == RDHWR) {\n\t\tint rd = (opcode & RD) >> 11;\n\t\tint rt = (opcode & RT) >> 16;\n\t\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,\n\t\t\t\t1, 0, regs, 0);\n\t\tswitch (rd) {\n\t\tcase 0:\t\t/* CPU number */\n\t\t\tregs->regs[rt] = smp_processor_id();\n\t\t\treturn 0;\n\t\tcase 1:\t\t/* SYNCI length */\n\t\t\tregs->regs[rt] = min(current_cpu_data.dcache.linesz,\n\t\t\t\t\t     current_cpu_data.icache.linesz);\n\t\t\treturn 0;\n\t\tcase 2:\t\t/* Read count register */\n\t\t\tregs->regs[rt] = read_c0_count();\n\t\t\treturn 0;\n\t\tcase 3:\t\t/* Count register resolution */\n\t\t\tswitch (current_cpu_data.cputype) {\n\t\t\tcase CPU_20KC:\n\t\t\tcase CPU_25KF:\n\t\t\t\tregs->regs[rt] = 1;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tregs->regs[rt] = 2;\n\t\t\t}\n\t\t\treturn 0;\n\t\tcase 29:\n\t\t\tregs->regs[rt] = ti->tp_value;\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\t/* Not ours.  */\n\treturn -1;\n}\n\nstatic int simulate_sync(struct pt_regs *regs, unsigned int opcode)\n{\n\tif ((opcode & OPCODE) == SPEC0 && (opcode & FUNC) == SYNC) {\n\t\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,\n\t\t\t\t1, 0, regs, 0);\n\t\treturn 0;\n\t}\n\n\treturn -1;\t\t\t/* Must be something else ... */\n}\n\nasmlinkage void do_ov(struct pt_regs *regs)\n{\n\tsiginfo_t info;\n\n\tdie_if_kernel(\"Integer overflow\", regs);\n\n\tinfo.si_code = FPE_INTOVF;\n\tinfo.si_signo = SIGFPE;\n\tinfo.si_errno = 0;\n\tinfo.si_addr = (void __user *) regs->cp0_epc;\n\tforce_sig_info(SIGFPE, &info, current);\n}\n\nstatic int process_fpemu_return(int sig, void __user *fault_addr)\n{\n\tif (sig == SIGSEGV || sig == SIGBUS) {\n\t\tstruct siginfo si = {0};\n\t\tsi.si_addr = fault_addr;\n\t\tsi.si_signo = sig;\n\t\tif (sig == SIGSEGV) {\n\t\t\tif (find_vma(current->mm, (unsigned long)fault_addr))\n\t\t\t\tsi.si_code = SEGV_ACCERR;\n\t\t\telse\n\t\t\t\tsi.si_code = SEGV_MAPERR;\n\t\t} else {\n\t\t\tsi.si_code = BUS_ADRERR;\n\t\t}\n\t\tforce_sig_info(sig, &si, current);\n\t\treturn 1;\n\t} else if (sig) {\n\t\tforce_sig(sig, current);\n\t\treturn 1;\n\t} else {\n\t\treturn 0;\n\t}\n}\n\n/*\n * XXX Delayed fp exceptions when doing a lazy ctx switch XXX\n */\nasmlinkage void do_fpe(struct pt_regs *regs, unsigned long fcr31)\n{\n\tsiginfo_t info = {0};\n\n\tif (notify_die(DIE_FP, \"FP exception\", regs, 0, regs_to_trapnr(regs), SIGFPE)\n\t    == NOTIFY_STOP)\n\t\treturn;\n\tdie_if_kernel(\"FP exception in kernel code\", regs);\n\n\tif (fcr31 & FPU_CSR_UNI_X) {\n\t\tint sig;\n\t\tvoid __user *fault_addr = NULL;\n\n\t\t/*\n\t\t * Unimplemented operation exception.  If we've got the full\n\t\t * software emulator on-board, let's use it...\n\t\t *\n\t\t * Force FPU to dump state into task/thread context.  We're\n\t\t * moving a lot of data here for what is probably a single\n\t\t * instruction, but the alternative is to pre-decode the FP\n\t\t * register operands before invoking the emulator, which seems\n\t\t * a bit extreme for what should be an infrequent event.\n\t\t */\n\t\t/* Ensure 'resume' not overwrite saved fp context again. */\n\t\tlose_fpu(1);\n\n\t\t/* Run the emulator */\n\t\tsig = fpu_emulator_cop1Handler(regs, &current->thread.fpu, 1,\n\t\t\t\t\t       &fault_addr);\n\n\t\t/*\n\t\t * We can't allow the emulated instruction to leave any of\n\t\t * the cause bit set in $fcr31.\n\t\t */\n\t\tcurrent->thread.fpu.fcr31 &= ~FPU_CSR_ALL_X;\n\n\t\t/* Restore the hardware register state */\n\t\town_fpu(1);\t/* Using the FPU again.  */\n\n\t\t/* If something went wrong, signal */\n\t\tprocess_fpemu_return(sig, fault_addr);\n\n\t\treturn;\n\t} else if (fcr31 & FPU_CSR_INV_X)\n\t\tinfo.si_code = FPE_FLTINV;\n\telse if (fcr31 & FPU_CSR_DIV_X)\n\t\tinfo.si_code = FPE_FLTDIV;\n\telse if (fcr31 & FPU_CSR_OVF_X)\n\t\tinfo.si_code = FPE_FLTOVF;\n\telse if (fcr31 & FPU_CSR_UDF_X)\n\t\tinfo.si_code = FPE_FLTUND;\n\telse if (fcr31 & FPU_CSR_INE_X)\n\t\tinfo.si_code = FPE_FLTRES;\n\telse\n\t\tinfo.si_code = __SI_FAULT;\n\tinfo.si_signo = SIGFPE;\n\tinfo.si_errno = 0;\n\tinfo.si_addr = (void __user *) regs->cp0_epc;\n\tforce_sig_info(SIGFPE, &info, current);\n}\n\nstatic void do_trap_or_bp(struct pt_regs *regs, unsigned int code,\n\tconst char *str)\n{\n\tsiginfo_t info;\n\tchar b[40];\n\n#ifdef CONFIG_KGDB_LOW_LEVEL_TRAP\n\tif (kgdb_ll_trap(DIE_TRAP, str, regs, code, regs_to_trapnr(regs), SIGTRAP) == NOTIFY_STOP)\n\t\treturn;\n#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */\n\n\tif (notify_die(DIE_TRAP, str, regs, code, regs_to_trapnr(regs), SIGTRAP) == NOTIFY_STOP)\n\t\treturn;\n\n\t/*\n\t * A short test says that IRIX 5.3 sends SIGTRAP for all trap\n\t * insns, even for trap and break codes that indicate arithmetic\n\t * failures.  Weird ...\n\t * But should we continue the brokenness???  --macro\n\t */\n\tswitch (code) {\n\tcase BRK_OVERFLOW:\n\tcase BRK_DIVZERO:\n\t\tscnprintf(b, sizeof(b), \"%s instruction in kernel code\", str);\n\t\tdie_if_kernel(b, regs);\n\t\tif (code == BRK_DIVZERO)\n\t\t\tinfo.si_code = FPE_INTDIV;\n\t\telse\n\t\t\tinfo.si_code = FPE_INTOVF;\n\t\tinfo.si_signo = SIGFPE;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_addr = (void __user *) regs->cp0_epc;\n\t\tforce_sig_info(SIGFPE, &info, current);\n\t\tbreak;\n\tcase BRK_BUG:\n\t\tdie_if_kernel(\"Kernel bug detected\", regs);\n\t\tforce_sig(SIGTRAP, current);\n\t\tbreak;\n\tcase BRK_MEMU:\n\t\t/*\n\t\t * Address errors may be deliberately induced by the FPU\n\t\t * emulator to retake control of the CPU after executing the\n\t\t * instruction in the delay slot of an emulated branch.\n\t\t *\n\t\t * Terminate if exception was recognized as a delay slot return\n\t\t * otherwise handle as normal.\n\t\t */\n\t\tif (do_dsemulret(regs))\n\t\t\treturn;\n\n\t\tdie_if_kernel(\"Math emu break/trap\", regs);\n\t\tforce_sig(SIGTRAP, current);\n\t\tbreak;\n\tdefault:\n\t\tscnprintf(b, sizeof(b), \"%s instruction in kernel code\", str);\n\t\tdie_if_kernel(b, regs);\n\t\tforce_sig(SIGTRAP, current);\n\t}\n}\n\nasmlinkage void do_bp(struct pt_regs *regs)\n{\n\tunsigned int opcode, bcode;\n\n\tif (__get_user(opcode, (unsigned int __user *) exception_epc(regs)))\n\t\tgoto out_sigsegv;\n\n\t/*\n\t * There is the ancient bug in the MIPS assemblers that the break\n\t * code starts left to bit 16 instead to bit 6 in the opcode.\n\t * Gas is bug-compatible, but not always, grrr...\n\t * We handle both cases with a simple heuristics.  --macro\n\t */\n\tbcode = ((opcode >> 6) & ((1 << 20) - 1));\n\tif (bcode >= (1 << 10))\n\t\tbcode >>= 10;\n\n\t/*\n\t * notify the kprobe handlers, if instruction is likely to\n\t * pertain to them.\n\t */\n\tswitch (bcode) {\n\tcase BRK_KPROBE_BP:\n\t\tif (notify_die(DIE_BREAK, \"debug\", regs, bcode, regs_to_trapnr(regs), SIGTRAP) == NOTIFY_STOP)\n\t\t\treturn;\n\t\telse\n\t\t\tbreak;\n\tcase BRK_KPROBE_SSTEPBP:\n\t\tif (notify_die(DIE_SSTEPBP, \"single_step\", regs, bcode, regs_to_trapnr(regs), SIGTRAP) == NOTIFY_STOP)\n\t\t\treturn;\n\t\telse\n\t\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tdo_trap_or_bp(regs, bcode, \"Break\");\n\treturn;\n\nout_sigsegv:\n\tforce_sig(SIGSEGV, current);\n}\n\nasmlinkage void do_tr(struct pt_regs *regs)\n{\n\tunsigned int opcode, tcode = 0;\n\n\tif (__get_user(opcode, (unsigned int __user *) exception_epc(regs)))\n\t\tgoto out_sigsegv;\n\n\t/* Immediate versions don't provide a code.  */\n\tif (!(opcode & OPCODE))\n\t\ttcode = ((opcode >> 6) & ((1 << 10) - 1));\n\n\tdo_trap_or_bp(regs, tcode, \"Trap\");\n\treturn;\n\nout_sigsegv:\n\tforce_sig(SIGSEGV, current);\n}\n\nasmlinkage void do_ri(struct pt_regs *regs)\n{\n\tunsigned int __user *epc = (unsigned int __user *)exception_epc(regs);\n\tunsigned long old_epc = regs->cp0_epc;\n\tunsigned int opcode = 0;\n\tint status = -1;\n\n\tif (notify_die(DIE_RI, \"RI Fault\", regs, 0, regs_to_trapnr(regs), SIGILL)\n\t    == NOTIFY_STOP)\n\t\treturn;\n\n\tdie_if_kernel(\"Reserved instruction in kernel code\", regs);\n\n\tif (unlikely(compute_return_epc(regs) < 0))\n\t\treturn;\n\n\tif (unlikely(get_user(opcode, epc) < 0))\n\t\tstatus = SIGSEGV;\n\n\tif (!cpu_has_llsc && status < 0)\n\t\tstatus = simulate_llsc(regs, opcode);\n\n\tif (status < 0)\n\t\tstatus = simulate_rdhwr(regs, opcode);\n\n\tif (status < 0)\n\t\tstatus = simulate_sync(regs, opcode);\n\n\tif (status < 0)\n\t\tstatus = SIGILL;\n\n\tif (unlikely(status > 0)) {\n\t\tregs->cp0_epc = old_epc;\t\t/* Undo skip-over.  */\n\t\tforce_sig(status, current);\n\t}\n}\n\n/*\n * MIPS MT processors may have fewer FPU contexts than CPU threads. If we've\n * emulated more than some threshold number of instructions, force migration to\n * a \"CPU\" that has FP support.\n */\nstatic void mt_ase_fp_affinity(void)\n{\n#ifdef CONFIG_MIPS_MT_FPAFF\n\tif (mt_fpemul_threshold > 0 &&\n\t     ((current->thread.emulated_fp++ > mt_fpemul_threshold))) {\n\t\t/*\n\t\t * If there's no FPU present, or if the application has already\n\t\t * restricted the allowed set to exclude any CPUs with FPUs,\n\t\t * we'll skip the procedure.\n\t\t */\n\t\tif (cpus_intersects(current->cpus_allowed, mt_fpu_cpumask)) {\n\t\t\tcpumask_t tmask;\n\n\t\t\tcurrent->thread.user_cpus_allowed\n\t\t\t\t= current->cpus_allowed;\n\t\t\tcpus_and(tmask, current->cpus_allowed,\n\t\t\t\tmt_fpu_cpumask);\n\t\t\tset_cpus_allowed_ptr(current, &tmask);\n\t\t\tset_thread_flag(TIF_FPUBOUND);\n\t\t}\n\t}\n#endif /* CONFIG_MIPS_MT_FPAFF */\n}\n\n/*\n * No lock; only written during early bootup by CPU 0.\n */\nstatic RAW_NOTIFIER_HEAD(cu2_chain);\n\nint __ref register_cu2_notifier(struct notifier_block *nb)\n{\n\treturn raw_notifier_chain_register(&cu2_chain, nb);\n}\n\nint cu2_notifier_call_chain(unsigned long val, void *v)\n{\n\treturn raw_notifier_call_chain(&cu2_chain, val, v);\n}\n\nstatic int default_cu2_call(struct notifier_block *nfb, unsigned long action,\n        void *data)\n{\n\tstruct pt_regs *regs = data;\n\n\tswitch (action) {\n\tdefault:\n\t\tdie_if_kernel(\"Unhandled kernel unaligned access or invalid \"\n\t\t\t      \"instruction\", regs);\n\t\t/* Fall through  */\n\n\tcase CU2_EXCEPTION:\n\t\tforce_sig(SIGILL, current);\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nasmlinkage void do_cpu(struct pt_regs *regs)\n{\n\tunsigned int __user *epc;\n\tunsigned long old_epc;\n\tunsigned int opcode;\n\tunsigned int cpid;\n\tint status;\n\tunsigned long __maybe_unused flags;\n\n\tdie_if_kernel(\"do_cpu invoked from kernel context!\", regs);\n\n\tcpid = (regs->cp0_cause >> CAUSEB_CE) & 3;\n\n\tswitch (cpid) {\n\tcase 0:\n\t\tepc = (unsigned int __user *)exception_epc(regs);\n\t\told_epc = regs->cp0_epc;\n\t\topcode = 0;\n\t\tstatus = -1;\n\n\t\tif (unlikely(compute_return_epc(regs) < 0))\n\t\t\treturn;\n\n\t\tif (unlikely(get_user(opcode, epc) < 0))\n\t\t\tstatus = SIGSEGV;\n\n\t\tif (!cpu_has_llsc && status < 0)\n\t\t\tstatus = simulate_llsc(regs, opcode);\n\n\t\tif (status < 0)\n\t\t\tstatus = simulate_rdhwr(regs, opcode);\n\n\t\tif (status < 0)\n\t\t\tstatus = SIGILL;\n\n\t\tif (unlikely(status > 0)) {\n\t\t\tregs->cp0_epc = old_epc;\t/* Undo skip-over.  */\n\t\t\tforce_sig(status, current);\n\t\t}\n\n\t\treturn;\n\n\tcase 1:\n\t\tif (used_math())\t/* Using the FPU again.  */\n\t\t\town_fpu(1);\n\t\telse {\t\t\t/* First time FPU user.  */\n\t\t\tinit_fpu();\n\t\t\tset_used_math();\n\t\t}\n\n\t\tif (!raw_cpu_has_fpu) {\n\t\t\tint sig;\n\t\t\tvoid __user *fault_addr = NULL;\n\t\t\tsig = fpu_emulator_cop1Handler(regs,\n\t\t\t\t\t\t       &current->thread.fpu,\n\t\t\t\t\t\t       0, &fault_addr);\n\t\t\tif (!process_fpemu_return(sig, fault_addr))\n\t\t\t\tmt_ase_fp_affinity();\n\t\t}\n\n\t\treturn;\n\n\tcase 2:\n\t\traw_notifier_call_chain(&cu2_chain, CU2_EXCEPTION, regs);\n\t\treturn;\n\n\tcase 3:\n\t\tbreak;\n\t}\n\n\tforce_sig(SIGILL, current);\n}\n\nasmlinkage void do_mdmx(struct pt_regs *regs)\n{\n\tforce_sig(SIGILL, current);\n}\n\n/*\n * Called with interrupts disabled.\n */\nasmlinkage void do_watch(struct pt_regs *regs)\n{\n\tu32 cause;\n\n\t/*\n\t * Clear WP (bit 22) bit of cause register so we don't loop\n\t * forever.\n\t */\n\tcause = read_c0_cause();\n\tcause &= ~(1 << 22);\n\twrite_c0_cause(cause);\n\n\t/*\n\t * If the current thread has the watch registers loaded, save\n\t * their values and send SIGTRAP.  Otherwise another thread\n\t * left the registers set, clear them and continue.\n\t */\n\tif (test_tsk_thread_flag(current, TIF_LOAD_WATCH)) {\n\t\tmips_read_watch_registers();\n\t\tlocal_irq_enable();\n\t\tforce_sig(SIGTRAP, current);\n\t} else {\n\t\tmips_clear_watch_registers();\n\t\tlocal_irq_enable();\n\t}\n}\n\nasmlinkage void do_mcheck(struct pt_regs *regs)\n{\n\tconst int field = 2 * sizeof(unsigned long);\n\tint multi_match = regs->cp0_status & ST0_TS;\n\n\tshow_regs(regs);\n\n\tif (multi_match) {\n\t\tprintk(\"Index   : %0x\\n\", read_c0_index());\n\t\tprintk(\"Pagemask: %0x\\n\", read_c0_pagemask());\n\t\tprintk(\"EntryHi : %0*lx\\n\", field, read_c0_entryhi());\n\t\tprintk(\"EntryLo0: %0*lx\\n\", field, read_c0_entrylo0());\n\t\tprintk(\"EntryLo1: %0*lx\\n\", field, read_c0_entrylo1());\n\t\tprintk(\"\\n\");\n\t\tdump_tlb_all();\n\t}\n\n\tshow_code((unsigned int __user *) regs->cp0_epc);\n\n\t/*\n\t * Some chips may have other causes of machine check (e.g. SB1\n\t * graduation timer)\n\t */\n\tpanic(\"Caught Machine Check exception - %scaused by multiple \"\n\t      \"matching entries in the TLB.\",\n\t      (multi_match) ? \"\" : \"not \");\n}\n\nasmlinkage void do_mt(struct pt_regs *regs)\n{\n\tint subcode;\n\n\tsubcode = (read_vpe_c0_vpecontrol() & VPECONTROL_EXCPT)\n\t\t\t>> VPECONTROL_EXCPT_SHIFT;\n\tswitch (subcode) {\n\tcase 0:\n\t\tprintk(KERN_DEBUG \"Thread Underflow\\n\");\n\t\tbreak;\n\tcase 1:\n\t\tprintk(KERN_DEBUG \"Thread Overflow\\n\");\n\t\tbreak;\n\tcase 2:\n\t\tprintk(KERN_DEBUG \"Invalid YIELD Qualifier\\n\");\n\t\tbreak;\n\tcase 3:\n\t\tprintk(KERN_DEBUG \"Gating Storage Exception\\n\");\n\t\tbreak;\n\tcase 4:\n\t\tprintk(KERN_DEBUG \"YIELD Scheduler Exception\\n\");\n\t\tbreak;\n\tcase 5:\n\t\tprintk(KERN_DEBUG \"Gating Storage Schedulier Exception\\n\");\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_DEBUG \"*** UNKNOWN THREAD EXCEPTION %d ***\\n\",\n\t\t\tsubcode);\n\t\tbreak;\n\t}\n\tdie_if_kernel(\"MIPS MT Thread exception in kernel\", regs);\n\n\tforce_sig(SIGILL, current);\n}\n\n\nasmlinkage void do_dsp(struct pt_regs *regs)\n{\n\tif (cpu_has_dsp)\n\t\tpanic(\"Unexpected DSP exception\\n\");\n\n\tforce_sig(SIGILL, current);\n}\n\nasmlinkage void do_reserved(struct pt_regs *regs)\n{\n\t/*\n\t * Game over - no way to handle this if it ever occurs.  Most probably\n\t * caused by a new unknown cpu type or after another deadly\n\t * hard/software error.\n\t */\n\tshow_regs(regs);\n\tpanic(\"Caught reserved exception %ld - should not happen.\",\n\t      (regs->cp0_cause & 0x7f) >> 2);\n}\n\nstatic int __initdata l1parity = 1;\nstatic int __init nol1parity(char *s)\n{\n\tl1parity = 0;\n\treturn 1;\n}\n__setup(\"nol1par\", nol1parity);\nstatic int __initdata l2parity = 1;\nstatic int __init nol2parity(char *s)\n{\n\tl2parity = 0;\n\treturn 1;\n}\n__setup(\"nol2par\", nol2parity);\n\n/*\n * Some MIPS CPUs can enable/disable for cache parity detection, but do\n * it different ways.\n */\nstatic inline void parity_protection_init(void)\n{\n\tswitch (current_cpu_type()) {\n\tcase CPU_24K:\n\tcase CPU_34K:\n\tcase CPU_74K:\n\tcase CPU_1004K:\n\t\t{\n#define ERRCTL_PE\t0x80000000\n#define ERRCTL_L2P\t0x00800000\n\t\t\tunsigned long errctl;\n\t\t\tunsigned int l1parity_present, l2parity_present;\n\n\t\t\terrctl = read_c0_ecc();\n\t\t\terrctl &= ~(ERRCTL_PE|ERRCTL_L2P);\n\n\t\t\t/* probe L1 parity support */\n\t\t\twrite_c0_ecc(errctl | ERRCTL_PE);\n\t\t\tback_to_back_c0_hazard();\n\t\t\tl1parity_present = (read_c0_ecc() & ERRCTL_PE);\n\n\t\t\t/* probe L2 parity support */\n\t\t\twrite_c0_ecc(errctl|ERRCTL_L2P);\n\t\t\tback_to_back_c0_hazard();\n\t\t\tl2parity_present = (read_c0_ecc() & ERRCTL_L2P);\n\n\t\t\tif (l1parity_present && l2parity_present) {\n\t\t\t\tif (l1parity)\n\t\t\t\t\terrctl |= ERRCTL_PE;\n\t\t\t\tif (l1parity ^ l2parity)\n\t\t\t\t\terrctl |= ERRCTL_L2P;\n\t\t\t} else if (l1parity_present) {\n\t\t\t\tif (l1parity)\n\t\t\t\t\terrctl |= ERRCTL_PE;\n\t\t\t} else if (l2parity_present) {\n\t\t\t\tif (l2parity)\n\t\t\t\t\terrctl |= ERRCTL_L2P;\n\t\t\t} else {\n\t\t\t\t/* No parity available */\n\t\t\t}\n\n\t\t\tprintk(KERN_INFO \"Writing ErrCtl register=%08lx\\n\", errctl);\n\n\t\t\twrite_c0_ecc(errctl);\n\t\t\tback_to_back_c0_hazard();\n\t\t\terrctl = read_c0_ecc();\n\t\t\tprintk(KERN_INFO \"Readback ErrCtl register=%08lx\\n\", errctl);\n\n\t\t\tif (l1parity_present)\n\t\t\t\tprintk(KERN_INFO \"Cache parity protection %sabled\\n\",\n\t\t\t\t       (errctl & ERRCTL_PE) ? \"en\" : \"dis\");\n\n\t\t\tif (l2parity_present) {\n\t\t\t\tif (l1parity_present && l1parity)\n\t\t\t\t\terrctl ^= ERRCTL_L2P;\n\t\t\t\tprintk(KERN_INFO \"L2 cache parity protection %sabled\\n\",\n\t\t\t\t       (errctl & ERRCTL_L2P) ? \"en\" : \"dis\");\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\tcase CPU_5KC:\n\t\twrite_c0_ecc(0x80000000);\n\t\tback_to_back_c0_hazard();\n\t\t/* Set the PE bit (bit 31) in the c0_errctl register. */\n\t\tprintk(KERN_INFO \"Cache parity protection %sabled\\n\",\n\t\t       (read_c0_ecc() & 0x80000000) ? \"en\" : \"dis\");\n\t\tbreak;\n\tcase CPU_20KC:\n\tcase CPU_25KF:\n\t\t/* Clear the DE bit (bit 16) in the c0_status register. */\n\t\tprintk(KERN_INFO \"Enable cache parity protection for \"\n\t\t       \"MIPS 20KC/25KF CPUs.\\n\");\n\t\tclear_c0_status(ST0_DE);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nasmlinkage void cache_parity_error(void)\n{\n\tconst int field = 2 * sizeof(unsigned long);\n\tunsigned int reg_val;\n\n\t/* For the moment, report the problem and hang. */\n\tprintk(\"Cache error exception:\\n\");\n\tprintk(\"cp0_errorepc == %0*lx\\n\", field, read_c0_errorepc());\n\treg_val = read_c0_cacheerr();\n\tprintk(\"c0_cacheerr == %08x\\n\", reg_val);\n\n\tprintk(\"Decoded c0_cacheerr: %s cache fault in %s reference.\\n\",\n\t       reg_val & (1<<30) ? \"secondary\" : \"primary\",\n\t       reg_val & (1<<31) ? \"data\" : \"insn\");\n\tprintk(\"Error bits: %s%s%s%s%s%s%s\\n\",\n\t       reg_val & (1<<29) ? \"ED \" : \"\",\n\t       reg_val & (1<<28) ? \"ET \" : \"\",\n\t       reg_val & (1<<26) ? \"EE \" : \"\",\n\t       reg_val & (1<<25) ? \"EB \" : \"\",\n\t       reg_val & (1<<24) ? \"EI \" : \"\",\n\t       reg_val & (1<<23) ? \"E1 \" : \"\",\n\t       reg_val & (1<<22) ? \"E0 \" : \"\");\n\tprintk(\"IDX: 0x%08x\\n\", reg_val & ((1<<22)-1));\n\n#if defined(CONFIG_CPU_MIPS32) || defined(CONFIG_CPU_MIPS64)\n\tif (reg_val & (1<<22))\n\t\tprintk(\"DErrAddr0: 0x%0*lx\\n\", field, read_c0_derraddr0());\n\n\tif (reg_val & (1<<23))\n\t\tprintk(\"DErrAddr1: 0x%0*lx\\n\", field, read_c0_derraddr1());\n#endif\n\n\tpanic(\"Can't handle the cache error!\");\n}\n\n/*\n * SDBBP EJTAG debug exception handler.\n * We skip the instruction and return to the next instruction.\n */\nvoid ejtag_exception_handler(struct pt_regs *regs)\n{\n\tconst int field = 2 * sizeof(unsigned long);\n\tunsigned long depc, old_epc;\n\tunsigned int debug;\n\n\tprintk(KERN_DEBUG \"SDBBP EJTAG debug exception - not handled yet, just ignored!\\n\");\n\tdepc = read_c0_depc();\n\tdebug = read_c0_debug();\n\tprintk(KERN_DEBUG \"c0_depc = %0*lx, DEBUG = %08x\\n\", field, depc, debug);\n\tif (debug & 0x80000000) {\n\t\t/*\n\t\t * In branch delay slot.\n\t\t * We cheat a little bit here and use EPC to calculate the\n\t\t * debug return address (DEPC). EPC is restored after the\n\t\t * calculation.\n\t\t */\n\t\told_epc = regs->cp0_epc;\n\t\tregs->cp0_epc = depc;\n\t\t__compute_return_epc(regs);\n\t\tdepc = regs->cp0_epc;\n\t\tregs->cp0_epc = old_epc;\n\t} else\n\t\tdepc += 4;\n\twrite_c0_depc(depc);\n\n#if 0\n\tprintk(KERN_DEBUG \"\\n\\n----- Enable EJTAG single stepping ----\\n\\n\");\n\twrite_c0_debug(debug | 0x100);\n#endif\n}\n\n/*\n * NMI exception handler.\n */\nNORET_TYPE void ATTRIB_NORET nmi_exception_handler(struct pt_regs *regs)\n{\n\tbust_spinlocks(1);\n\tprintk(\"NMI taken!!!!\\n\");\n\tdie(\"NMI\", regs);\n}\n\n#define VECTORSPACING 0x100\t/* for EI/VI mode */\n\nunsigned long ebase;\nunsigned long exception_handlers[32];\nunsigned long vi_handlers[64];\n\nvoid __init *set_except_vector(int n, void *addr)\n{\n\tunsigned long handler = (unsigned long) addr;\n\tunsigned long old_handler = exception_handlers[n];\n\n\texception_handlers[n] = handler;\n\tif (n == 0 && cpu_has_divec) {\n\t\tunsigned long jump_mask = ~((1 << 28) - 1);\n\t\tu32 *buf = (u32 *)(ebase + 0x200);\n\t\tunsigned int k0 = 26;\n\t\tif ((handler & jump_mask) == ((ebase + 0x200) & jump_mask)) {\n\t\t\tuasm_i_j(&buf, handler & ~jump_mask);\n\t\t\tuasm_i_nop(&buf);\n\t\t} else {\n\t\t\tUASM_i_LA(&buf, k0, handler);\n\t\t\tuasm_i_jr(&buf, k0);\n\t\t\tuasm_i_nop(&buf);\n\t\t}\n\t\tlocal_flush_icache_range(ebase + 0x200, (unsigned long)buf);\n\t}\n\treturn (void *)old_handler;\n}\n\nstatic asmlinkage void do_default_vi(void)\n{\n\tshow_regs(get_irq_regs());\n\tpanic(\"Caught unexpected vectored interrupt.\");\n}\n\nstatic void *set_vi_srs_handler(int n, vi_handler_t addr, int srs)\n{\n\tunsigned long handler;\n\tunsigned long old_handler = vi_handlers[n];\n\tint srssets = current_cpu_data.srsets;\n\tu32 *w;\n\tunsigned char *b;\n\n\tBUG_ON(!cpu_has_veic && !cpu_has_vint);\n\n\tif (addr == NULL) {\n\t\thandler = (unsigned long) do_default_vi;\n\t\tsrs = 0;\n\t} else\n\t\thandler = (unsigned long) addr;\n\tvi_handlers[n] = (unsigned long) addr;\n\n\tb = (unsigned char *)(ebase + 0x200 + n*VECTORSPACING);\n\n\tif (srs >= srssets)\n\t\tpanic(\"Shadow register set %d not supported\", srs);\n\n\tif (cpu_has_veic) {\n\t\tif (board_bind_eic_interrupt)\n\t\t\tboard_bind_eic_interrupt(n, srs);\n\t} else if (cpu_has_vint) {\n\t\t/* SRSMap is only defined if shadow sets are implemented */\n\t\tif (srssets > 1)\n\t\t\tchange_c0_srsmap(0xf << n*4, srs << n*4);\n\t}\n\n\tif (srs == 0) {\n\t\t/*\n\t\t * If no shadow set is selected then use the default handler\n\t\t * that does normal register saving and a standard interrupt exit\n\t\t */\n\n\t\textern char except_vec_vi, except_vec_vi_lui;\n\t\textern char except_vec_vi_ori, except_vec_vi_end;\n\t\textern char rollback_except_vec_vi;\n\t\tchar *vec_start = (cpu_wait == r4k_wait) ?\n\t\t\t&rollback_except_vec_vi : &except_vec_vi;\n#ifdef CONFIG_MIPS_MT_SMTC\n\t\t/*\n\t\t * We need to provide the SMTC vectored interrupt handler\n\t\t * not only with the address of the handler, but with the\n\t\t * Status.IM bit to be masked before going there.\n\t\t */\n\t\textern char except_vec_vi_mori;\n\t\tconst int mori_offset = &except_vec_vi_mori - vec_start;\n#endif /* CONFIG_MIPS_MT_SMTC */\n\t\tconst int handler_len = &except_vec_vi_end - vec_start;\n\t\tconst int lui_offset = &except_vec_vi_lui - vec_start;\n\t\tconst int ori_offset = &except_vec_vi_ori - vec_start;\n\n\t\tif (handler_len > VECTORSPACING) {\n\t\t\t/*\n\t\t\t * Sigh... panicing won't help as the console\n\t\t\t * is probably not configured :(\n\t\t\t */\n\t\t\tpanic(\"VECTORSPACING too small\");\n\t\t}\n\n\t\tmemcpy(b, vec_start, handler_len);\n#ifdef CONFIG_MIPS_MT_SMTC\n\t\tBUG_ON(n > 7);\t/* Vector index %d exceeds SMTC maximum. */\n\n\t\tw = (u32 *)(b + mori_offset);\n\t\t*w = (*w & 0xffff0000) | (0x100 << n);\n#endif /* CONFIG_MIPS_MT_SMTC */\n\t\tw = (u32 *)(b + lui_offset);\n\t\t*w = (*w & 0xffff0000) | (((u32)handler >> 16) & 0xffff);\n\t\tw = (u32 *)(b + ori_offset);\n\t\t*w = (*w & 0xffff0000) | ((u32)handler & 0xffff);\n\t\tlocal_flush_icache_range((unsigned long)b,\n\t\t\t\t\t (unsigned long)(b+handler_len));\n\t}\n\telse {\n\t\t/*\n\t\t * In other cases jump directly to the interrupt handler\n\t\t *\n\t\t * It is the handlers responsibility to save registers if required\n\t\t * (eg hi/lo) and return from the exception using \"eret\"\n\t\t */\n\t\tw = (u32 *)b;\n\t\t*w++ = 0x08000000 | (((u32)handler >> 2) & 0x03fffff); /* j handler */\n\t\t*w = 0;\n\t\tlocal_flush_icache_range((unsigned long)b,\n\t\t\t\t\t (unsigned long)(b+8));\n\t}\n\n\treturn (void *)old_handler;\n}\n\nvoid *set_vi_handler(int n, vi_handler_t addr)\n{\n\treturn set_vi_srs_handler(n, addr, 0);\n}\n\nextern void cpu_cache_init(void);\nextern void tlb_init(void);\nextern void flush_tlb_handlers(void);\n\n/*\n * Timer interrupt\n */\nint cp0_compare_irq;\nint cp0_compare_irq_shift;\n\n/*\n * Performance counter IRQ or -1 if shared with timer\n */\nint cp0_perfcount_irq;\nEXPORT_SYMBOL_GPL(cp0_perfcount_irq);\n\nstatic int __cpuinitdata noulri;\n\nstatic int __init ulri_disable(char *s)\n{\n\tpr_info(\"Disabling ulri\\n\");\n\tnoulri = 1;\n\n\treturn 1;\n}\n__setup(\"noulri\", ulri_disable);\n\nvoid __cpuinit per_cpu_trap_init(void)\n{\n\tunsigned int cpu = smp_processor_id();\n\tunsigned int status_set = ST0_CU0;\n\tunsigned int hwrena = cpu_hwrena_impl_bits;\n#ifdef CONFIG_MIPS_MT_SMTC\n\tint secondaryTC = 0;\n\tint bootTC = (cpu == 0);\n\n\t/*\n\t * Only do per_cpu_trap_init() for first TC of Each VPE.\n\t * Note that this hack assumes that the SMTC init code\n\t * assigns TCs consecutively and in ascending order.\n\t */\n\n\tif (((read_c0_tcbind() & TCBIND_CURTC) != 0) &&\n\t    ((read_c0_tcbind() & TCBIND_CURVPE) == cpu_data[cpu - 1].vpe_id))\n\t\tsecondaryTC = 1;\n#endif /* CONFIG_MIPS_MT_SMTC */\n\n\t/*\n\t * Disable coprocessors and select 32-bit or 64-bit addressing\n\t * and the 16/32 or 32/32 FPR register model.  Reset the BEV\n\t * flag that some firmware may have left set and the TS bit (for\n\t * IP27).  Set XX for ISA IV code to work.\n\t */\n#ifdef CONFIG_64BIT\n\tstatus_set |= ST0_FR|ST0_KX|ST0_SX|ST0_UX;\n#endif\n\tif (current_cpu_data.isa_level == MIPS_CPU_ISA_IV)\n\t\tstatus_set |= ST0_XX;\n\tif (cpu_has_dsp)\n\t\tstatus_set |= ST0_MX;\n\n\tchange_c0_status(ST0_CU|ST0_MX|ST0_RE|ST0_FR|ST0_BEV|ST0_TS|ST0_KX|ST0_SX|ST0_UX,\n\t\t\t status_set);\n\n\tif (cpu_has_mips_r2)\n\t\thwrena |= 0x0000000f;\n\n\tif (!noulri && cpu_has_userlocal)\n\t\thwrena |= (1 << 29);\n\n\tif (hwrena)\n\t\twrite_c0_hwrena(hwrena);\n\n#ifdef CONFIG_MIPS_MT_SMTC\n\tif (!secondaryTC) {\n#endif /* CONFIG_MIPS_MT_SMTC */\n\n\tif (cpu_has_veic || cpu_has_vint) {\n\t\tunsigned long sr = set_c0_status(ST0_BEV);\n\t\twrite_c0_ebase(ebase);\n\t\twrite_c0_status(sr);\n\t\t/* Setting vector spacing enables EI/VI mode  */\n\t\tchange_c0_intctl(0x3e0, VECTORSPACING);\n\t}\n\tif (cpu_has_divec) {\n\t\tif (cpu_has_mipsmt) {\n\t\t\tunsigned int vpflags = dvpe();\n\t\t\tset_c0_cause(CAUSEF_IV);\n\t\t\tevpe(vpflags);\n\t\t} else\n\t\t\tset_c0_cause(CAUSEF_IV);\n\t}\n\n\t/*\n\t * Before R2 both interrupt numbers were fixed to 7, so on R2 only:\n\t *\n\t *  o read IntCtl.IPTI to determine the timer interrupt\n\t *  o read IntCtl.IPPCI to determine the performance counter interrupt\n\t */\n\tif (cpu_has_mips_r2) {\n\t\tcp0_compare_irq_shift = CAUSEB_TI - CAUSEB_IP;\n\t\tcp0_compare_irq = (read_c0_intctl() >> INTCTLB_IPTI) & 7;\n\t\tcp0_perfcount_irq = (read_c0_intctl() >> INTCTLB_IPPCI) & 7;\n\t\tif (cp0_perfcount_irq == cp0_compare_irq)\n\t\t\tcp0_perfcount_irq = -1;\n\t} else {\n\t\tcp0_compare_irq = CP0_LEGACY_COMPARE_IRQ;\n\t\tcp0_compare_irq_shift = cp0_compare_irq;\n\t\tcp0_perfcount_irq = -1;\n\t}\n\n#ifdef CONFIG_MIPS_MT_SMTC\n\t}\n#endif /* CONFIG_MIPS_MT_SMTC */\n\n\tcpu_data[cpu].asid_cache = ASID_FIRST_VERSION;\n\n\tatomic_inc(&init_mm.mm_count);\n\tcurrent->active_mm = &init_mm;\n\tBUG_ON(current->mm);\n\tenter_lazy_tlb(&init_mm, current);\n\n#ifdef CONFIG_MIPS_MT_SMTC\n\tif (bootTC) {\n#endif /* CONFIG_MIPS_MT_SMTC */\n\t\tcpu_cache_init();\n\t\ttlb_init();\n#ifdef CONFIG_MIPS_MT_SMTC\n\t} else if (!secondaryTC) {\n\t\t/*\n\t\t * First TC in non-boot VPE must do subset of tlb_init()\n\t\t * for MMU countrol registers.\n\t\t */\n\t\twrite_c0_pagemask(PM_DEFAULT_MASK);\n\t\twrite_c0_wired(0);\n\t}\n#endif /* CONFIG_MIPS_MT_SMTC */\n\tTLBMISS_HANDLER_SETUP();\n}\n\n/* Install CPU exception handler */\nvoid __init set_handler(unsigned long offset, void *addr, unsigned long size)\n{\n\tmemcpy((void *)(ebase + offset), addr, size);\n\tlocal_flush_icache_range(ebase + offset, ebase + offset + size);\n}\n\nstatic char panic_null_cerr[] __cpuinitdata =\n\t\"Trying to set NULL cache error exception handler\";\n\n/*\n * Install uncached CPU exception handler.\n * This is suitable only for the cache error exception which is the only\n * exception handler that is being run uncached.\n */\nvoid __cpuinit set_uncached_handler(unsigned long offset, void *addr,\n\tunsigned long size)\n{\n\tunsigned long uncached_ebase = CKSEG1ADDR(ebase);\n\n\tif (!addr)\n\t\tpanic(panic_null_cerr);\n\n\tmemcpy((void *)(uncached_ebase + offset), addr, size);\n}\n\nstatic int __initdata rdhwr_noopt;\nstatic int __init set_rdhwr_noopt(char *str)\n{\n\trdhwr_noopt = 1;\n\treturn 1;\n}\n\n__setup(\"rdhwr_noopt\", set_rdhwr_noopt);\n\nvoid __init trap_init(void)\n{\n\textern char except_vec3_generic, except_vec3_r4000;\n\textern char except_vec4;\n\tunsigned long i;\n\tint rollback;\n\n\tcheck_wait();\n\trollback = (cpu_wait == r4k_wait);\n\n#if defined(CONFIG_KGDB)\n\tif (kgdb_early_setup)\n\t\treturn;\t/* Already done */\n#endif\n\n\tif (cpu_has_veic || cpu_has_vint) {\n\t\tunsigned long size = 0x200 + VECTORSPACING*64;\n\t\tebase = (unsigned long)\n\t\t\t__alloc_bootmem(size, 1 << fls(size), 0);\n\t} else {\n\t\tebase = CKSEG0;\n\t\tif (cpu_has_mips_r2)\n\t\t\tebase += (read_c0_ebase() & 0x3ffff000);\n\t}\n\n\tper_cpu_trap_init();\n\n\t/*\n\t * Copy the generic exception handlers to their final destination.\n\t * This will be overriden later as suitable for a particular\n\t * configuration.\n\t */\n\tset_handler(0x180, &except_vec3_generic, 0x80);\n\n\t/*\n\t * Setup default vectors\n\t */\n\tfor (i = 0; i <= 31; i++)\n\t\tset_except_vector(i, handle_reserved);\n\n\t/*\n\t * Copy the EJTAG debug exception vector handler code to it's final\n\t * destination.\n\t */\n\tif (cpu_has_ejtag && board_ejtag_handler_setup)\n\t\tboard_ejtag_handler_setup();\n\n\t/*\n\t * Only some CPUs have the watch exceptions.\n\t */\n\tif (cpu_has_watch)\n\t\tset_except_vector(23, handle_watch);\n\n\t/*\n\t * Initialise interrupt handlers\n\t */\n\tif (cpu_has_veic || cpu_has_vint) {\n\t\tint nvec = cpu_has_veic ? 64 : 8;\n\t\tfor (i = 0; i < nvec; i++)\n\t\t\tset_vi_handler(i, NULL);\n\t}\n\telse if (cpu_has_divec)\n\t\tset_handler(0x200, &except_vec4, 0x8);\n\n\t/*\n\t * Some CPUs can enable/disable for cache parity detection, but does\n\t * it different ways.\n\t */\n\tparity_protection_init();\n\n\t/*\n\t * The Data Bus Errors / Instruction Bus Errors are signaled\n\t * by external hardware.  Therefore these two exceptions\n\t * may have board specific handlers.\n\t */\n\tif (board_be_init)\n\t\tboard_be_init();\n\n\tset_except_vector(0, rollback ? rollback_handle_int : handle_int);\n\tset_except_vector(1, handle_tlbm);\n\tset_except_vector(2, handle_tlbl);\n\tset_except_vector(3, handle_tlbs);\n\n\tset_except_vector(4, handle_adel);\n\tset_except_vector(5, handle_ades);\n\n\tset_except_vector(6, handle_ibe);\n\tset_except_vector(7, handle_dbe);\n\n\tset_except_vector(8, handle_sys);\n\tset_except_vector(9, handle_bp);\n\tset_except_vector(10, rdhwr_noopt ? handle_ri :\n\t\t\t  (cpu_has_vtag_icache ?\n\t\t\t   handle_ri_rdhwr_vivt : handle_ri_rdhwr));\n\tset_except_vector(11, handle_cpu);\n\tset_except_vector(12, handle_ov);\n\tset_except_vector(13, handle_tr);\n\n\tif (current_cpu_type() == CPU_R6000 ||\n\t    current_cpu_type() == CPU_R6000A) {\n\t\t/*\n\t\t * The R6000 is the only R-series CPU that features a machine\n\t\t * check exception (similar to the R4000 cache error) and\n\t\t * unaligned ldc1/sdc1 exception.  The handlers have not been\n\t\t * written yet.  Well, anyway there is no R6000 machine on the\n\t\t * current list of targets for Linux/MIPS.\n\t\t * (Duh, crap, there is someone with a triple R6k machine)\n\t\t */\n\t\t//set_except_vector(14, handle_mc);\n\t\t//set_except_vector(15, handle_ndc);\n\t}\n\n\n\tif (board_nmi_handler_setup)\n\t\tboard_nmi_handler_setup();\n\n\tif (cpu_has_fpu && !cpu_has_nofpuex)\n\t\tset_except_vector(15, handle_fpe);\n\n\tset_except_vector(22, handle_mdmx);\n\n\tif (cpu_has_mcheck)\n\t\tset_except_vector(24, handle_mcheck);\n\n\tif (cpu_has_mipsmt)\n\t\tset_except_vector(25, handle_mt);\n\n\tset_except_vector(26, handle_dsp);\n\n\tif (cpu_has_vce)\n\t\t/* Special exception: R4[04]00 uses also the divec space. */\n\t\tmemcpy((void *)(ebase + 0x180), &except_vec3_r4000, 0x100);\n\telse if (cpu_has_4kex)\n\t\tmemcpy((void *)(ebase + 0x180), &except_vec3_generic, 0x80);\n\telse\n\t\tmemcpy((void *)(ebase + 0x080), &except_vec3_generic, 0x80);\n\n\tlocal_flush_icache_range(ebase, ebase + 0x400);\n\tflush_tlb_handlers();\n\n\tsort_extable(__start___dbe_table, __stop___dbe_table);\n\n\tcu2_notifier(default_cu2_call, 0x80000000);\t/* Run last  */\n}\n", "/*\n * Handle unaligned accesses by emulation.\n *\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n *\n * Copyright (C) 1996, 1998, 1999, 2002 by Ralf Baechle\n * Copyright (C) 1999 Silicon Graphics, Inc.\n *\n * This file contains exception handler for address error exception with the\n * special capability to execute faulting instructions in software.  The\n * handler does not try to handle the case when the program counter points\n * to an address not aligned to a word boundary.\n *\n * Putting data to unaligned addresses is a bad practice even on Intel where\n * only the performance is affected.  Much worse is that such code is non-\n * portable.  Due to several programs that die on MIPS due to alignment\n * problems I decided to implement this handler anyway though I originally\n * didn't intend to do this at all for user code.\n *\n * For now I enable fixing of address errors by default to make life easier.\n * I however intend to disable this somewhen in the future when the alignment\n * problems with user programs have been fixed.  For programmers this is the\n * right way to go.\n *\n * Fixing address errors is a per process option.  The option is inherited\n * across fork(2) and execve(2) calls.  If you really want to use the\n * option in your user programs - I discourage the use of the software\n * emulation strongly - use the following code in your userland stuff:\n *\n * #include <sys/sysmips.h>\n *\n * ...\n * sysmips(MIPS_FIXADE, x);\n * ...\n *\n * The argument x is 0 for disabling software emulation, enabled otherwise.\n *\n * Below a little program to play around with this feature.\n *\n * #include <stdio.h>\n * #include <sys/sysmips.h>\n *\n * struct foo {\n *         unsigned char bar[8];\n * };\n *\n * main(int argc, char *argv[])\n * {\n *         struct foo x = {0, 1, 2, 3, 4, 5, 6, 7};\n *         unsigned int *p = (unsigned int *) (x.bar + 3);\n *         int i;\n *\n *         if (argc > 1)\n *                 sysmips(MIPS_FIXADE, atoi(argv[1]));\n *\n *         printf(\"*p = %08lx\\n\", *p);\n *\n *         *p = 0xdeadface;\n *\n *         for(i = 0; i <= 7; i++)\n *         printf(\"%02x \", x.bar[i]);\n *         printf(\"\\n\");\n * }\n *\n * Coprocessor loads are not supported; I think this case is unimportant\n * in the practice.\n *\n * TODO: Handle ndc (attempted store to doubleword in uncached memory)\n *       exception for the R6000.\n *       A store crossing a page boundary might be executed only partially.\n *       Undo the partial store in this case.\n */\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/signal.h>\n#include <linux/smp.h>\n#include <linux/sched.h>\n#include <linux/debugfs.h>\n#include <linux/perf_event.h>\n\n#include <asm/asm.h>\n#include <asm/branch.h>\n#include <asm/byteorder.h>\n#include <asm/cop2.h>\n#include <asm/inst.h>\n#include <asm/uaccess.h>\n#include <asm/system.h>\n\n#define STR(x)  __STR(x)\n#define __STR(x)  #x\n\nenum {\n\tUNALIGNED_ACTION_QUIET,\n\tUNALIGNED_ACTION_SIGNAL,\n\tUNALIGNED_ACTION_SHOW,\n};\n#ifdef CONFIG_DEBUG_FS\nstatic u32 unaligned_instructions;\nstatic u32 unaligned_action;\n#else\n#define unaligned_action UNALIGNED_ACTION_QUIET\n#endif\nextern void show_registers(struct pt_regs *regs);\n\nstatic void emulate_load_store_insn(struct pt_regs *regs,\n\tvoid __user *addr, unsigned int __user *pc)\n{\n\tunion mips_instruction insn;\n\tunsigned long value;\n\tunsigned int res;\n\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,\n\t\t      1, 0, regs, 0);\n\n\t/*\n\t * This load never faults.\n\t */\n\t__get_user(insn.word, pc);\n\n\tswitch (insn.i_format.opcode) {\n\t/*\n\t * These are instructions that a compiler doesn't generate.  We\n\t * can assume therefore that the code is MIPS-aware and\n\t * really buggy.  Emulating these instructions would break the\n\t * semantics anyway.\n\t */\n\tcase ll_op:\n\tcase lld_op:\n\tcase sc_op:\n\tcase scd_op:\n\n\t/*\n\t * For these instructions the only way to create an address\n\t * error is an attempted access to kernel/supervisor address\n\t * space.\n\t */\n\tcase ldl_op:\n\tcase ldr_op:\n\tcase lwl_op:\n\tcase lwr_op:\n\tcase sdl_op:\n\tcase sdr_op:\n\tcase swl_op:\n\tcase swr_op:\n\tcase lb_op:\n\tcase lbu_op:\n\tcase sb_op:\n\t\tgoto sigbus;\n\n\t/*\n\t * The remaining opcodes are the ones that are really of interest.\n\t */\n\tcase lh_op:\n\t\tif (!access_ok(VERIFY_READ, addr, 2))\n\t\t\tgoto sigbus;\n\n\t\t__asm__ __volatile__ (\".set\\tnoat\\n\"\n#ifdef __BIG_ENDIAN\n\t\t\t\"1:\\tlb\\t%0, 0(%2)\\n\"\n\t\t\t\"2:\\tlbu\\t$1, 1(%2)\\n\\t\"\n#endif\n#ifdef __LITTLE_ENDIAN\n\t\t\t\"1:\\tlb\\t%0, 1(%2)\\n\"\n\t\t\t\"2:\\tlbu\\t$1, 0(%2)\\n\\t\"\n#endif\n\t\t\t\"sll\\t%0, 0x8\\n\\t\"\n\t\t\t\"or\\t%0, $1\\n\\t\"\n\t\t\t\"li\\t%1, 0\\n\"\n\t\t\t\"3:\\t.set\\tat\\n\\t\"\n\t\t\t\".section\\t.fixup,\\\"ax\\\"\\n\\t\"\n\t\t\t\"4:\\tli\\t%1, %3\\n\\t\"\n\t\t\t\"j\\t3b\\n\\t\"\n\t\t\t\".previous\\n\\t\"\n\t\t\t\".section\\t__ex_table,\\\"a\\\"\\n\\t\"\n\t\t\tSTR(PTR)\"\\t1b, 4b\\n\\t\"\n\t\t\tSTR(PTR)\"\\t2b, 4b\\n\\t\"\n\t\t\t\".previous\"\n\t\t\t: \"=&r\" (value), \"=r\" (res)\n\t\t\t: \"r\" (addr), \"i\" (-EFAULT));\n\t\tif (res)\n\t\t\tgoto fault;\n\t\tcompute_return_epc(regs);\n\t\tregs->regs[insn.i_format.rt] = value;\n\t\tbreak;\n\n\tcase lw_op:\n\t\tif (!access_ok(VERIFY_READ, addr, 4))\n\t\t\tgoto sigbus;\n\n\t\t__asm__ __volatile__ (\n#ifdef __BIG_ENDIAN\n\t\t\t\"1:\\tlwl\\t%0, (%2)\\n\"\n\t\t\t\"2:\\tlwr\\t%0, 3(%2)\\n\\t\"\n#endif\n#ifdef __LITTLE_ENDIAN\n\t\t\t\"1:\\tlwl\\t%0, 3(%2)\\n\"\n\t\t\t\"2:\\tlwr\\t%0, (%2)\\n\\t\"\n#endif\n\t\t\t\"li\\t%1, 0\\n\"\n\t\t\t\"3:\\t.section\\t.fixup,\\\"ax\\\"\\n\\t\"\n\t\t\t\"4:\\tli\\t%1, %3\\n\\t\"\n\t\t\t\"j\\t3b\\n\\t\"\n\t\t\t\".previous\\n\\t\"\n\t\t\t\".section\\t__ex_table,\\\"a\\\"\\n\\t\"\n\t\t\tSTR(PTR)\"\\t1b, 4b\\n\\t\"\n\t\t\tSTR(PTR)\"\\t2b, 4b\\n\\t\"\n\t\t\t\".previous\"\n\t\t\t: \"=&r\" (value), \"=r\" (res)\n\t\t\t: \"r\" (addr), \"i\" (-EFAULT));\n\t\tif (res)\n\t\t\tgoto fault;\n\t\tcompute_return_epc(regs);\n\t\tregs->regs[insn.i_format.rt] = value;\n\t\tbreak;\n\n\tcase lhu_op:\n\t\tif (!access_ok(VERIFY_READ, addr, 2))\n\t\t\tgoto sigbus;\n\n\t\t__asm__ __volatile__ (\n\t\t\t\".set\\tnoat\\n\"\n#ifdef __BIG_ENDIAN\n\t\t\t\"1:\\tlbu\\t%0, 0(%2)\\n\"\n\t\t\t\"2:\\tlbu\\t$1, 1(%2)\\n\\t\"\n#endif\n#ifdef __LITTLE_ENDIAN\n\t\t\t\"1:\\tlbu\\t%0, 1(%2)\\n\"\n\t\t\t\"2:\\tlbu\\t$1, 0(%2)\\n\\t\"\n#endif\n\t\t\t\"sll\\t%0, 0x8\\n\\t\"\n\t\t\t\"or\\t%0, $1\\n\\t\"\n\t\t\t\"li\\t%1, 0\\n\"\n\t\t\t\"3:\\t.set\\tat\\n\\t\"\n\t\t\t\".section\\t.fixup,\\\"ax\\\"\\n\\t\"\n\t\t\t\"4:\\tli\\t%1, %3\\n\\t\"\n\t\t\t\"j\\t3b\\n\\t\"\n\t\t\t\".previous\\n\\t\"\n\t\t\t\".section\\t__ex_table,\\\"a\\\"\\n\\t\"\n\t\t\tSTR(PTR)\"\\t1b, 4b\\n\\t\"\n\t\t\tSTR(PTR)\"\\t2b, 4b\\n\\t\"\n\t\t\t\".previous\"\n\t\t\t: \"=&r\" (value), \"=r\" (res)\n\t\t\t: \"r\" (addr), \"i\" (-EFAULT));\n\t\tif (res)\n\t\t\tgoto fault;\n\t\tcompute_return_epc(regs);\n\t\tregs->regs[insn.i_format.rt] = value;\n\t\tbreak;\n\n\tcase lwu_op:\n#ifdef CONFIG_64BIT\n\t\t/*\n\t\t * A 32-bit kernel might be running on a 64-bit processor.  But\n\t\t * if we're on a 32-bit processor and an i-cache incoherency\n\t\t * or race makes us see a 64-bit instruction here the sdl/sdr\n\t\t * would blow up, so for now we don't handle unaligned 64-bit\n\t\t * instructions on 32-bit kernels.\n\t\t */\n\t\tif (!access_ok(VERIFY_READ, addr, 4))\n\t\t\tgoto sigbus;\n\n\t\t__asm__ __volatile__ (\n#ifdef __BIG_ENDIAN\n\t\t\t\"1:\\tlwl\\t%0, (%2)\\n\"\n\t\t\t\"2:\\tlwr\\t%0, 3(%2)\\n\\t\"\n#endif\n#ifdef __LITTLE_ENDIAN\n\t\t\t\"1:\\tlwl\\t%0, 3(%2)\\n\"\n\t\t\t\"2:\\tlwr\\t%0, (%2)\\n\\t\"\n#endif\n\t\t\t\"dsll\\t%0, %0, 32\\n\\t\"\n\t\t\t\"dsrl\\t%0, %0, 32\\n\\t\"\n\t\t\t\"li\\t%1, 0\\n\"\n\t\t\t\"3:\\t.section\\t.fixup,\\\"ax\\\"\\n\\t\"\n\t\t\t\"4:\\tli\\t%1, %3\\n\\t\"\n\t\t\t\"j\\t3b\\n\\t\"\n\t\t\t\".previous\\n\\t\"\n\t\t\t\".section\\t__ex_table,\\\"a\\\"\\n\\t\"\n\t\t\tSTR(PTR)\"\\t1b, 4b\\n\\t\"\n\t\t\tSTR(PTR)\"\\t2b, 4b\\n\\t\"\n\t\t\t\".previous\"\n\t\t\t: \"=&r\" (value), \"=r\" (res)\n\t\t\t: \"r\" (addr), \"i\" (-EFAULT));\n\t\tif (res)\n\t\t\tgoto fault;\n\t\tcompute_return_epc(regs);\n\t\tregs->regs[insn.i_format.rt] = value;\n\t\tbreak;\n#endif /* CONFIG_64BIT */\n\n\t\t/* Cannot handle 64-bit instructions in 32-bit kernel */\n\t\tgoto sigill;\n\n\tcase ld_op:\n#ifdef CONFIG_64BIT\n\t\t/*\n\t\t * A 32-bit kernel might be running on a 64-bit processor.  But\n\t\t * if we're on a 32-bit processor and an i-cache incoherency\n\t\t * or race makes us see a 64-bit instruction here the sdl/sdr\n\t\t * would blow up, so for now we don't handle unaligned 64-bit\n\t\t * instructions on 32-bit kernels.\n\t\t */\n\t\tif (!access_ok(VERIFY_READ, addr, 8))\n\t\t\tgoto sigbus;\n\n\t\t__asm__ __volatile__ (\n#ifdef __BIG_ENDIAN\n\t\t\t\"1:\\tldl\\t%0, (%2)\\n\"\n\t\t\t\"2:\\tldr\\t%0, 7(%2)\\n\\t\"\n#endif\n#ifdef __LITTLE_ENDIAN\n\t\t\t\"1:\\tldl\\t%0, 7(%2)\\n\"\n\t\t\t\"2:\\tldr\\t%0, (%2)\\n\\t\"\n#endif\n\t\t\t\"li\\t%1, 0\\n\"\n\t\t\t\"3:\\t.section\\t.fixup,\\\"ax\\\"\\n\\t\"\n\t\t\t\"4:\\tli\\t%1, %3\\n\\t\"\n\t\t\t\"j\\t3b\\n\\t\"\n\t\t\t\".previous\\n\\t\"\n\t\t\t\".section\\t__ex_table,\\\"a\\\"\\n\\t\"\n\t\t\tSTR(PTR)\"\\t1b, 4b\\n\\t\"\n\t\t\tSTR(PTR)\"\\t2b, 4b\\n\\t\"\n\t\t\t\".previous\"\n\t\t\t: \"=&r\" (value), \"=r\" (res)\n\t\t\t: \"r\" (addr), \"i\" (-EFAULT));\n\t\tif (res)\n\t\t\tgoto fault;\n\t\tcompute_return_epc(regs);\n\t\tregs->regs[insn.i_format.rt] = value;\n\t\tbreak;\n#endif /* CONFIG_64BIT */\n\n\t\t/* Cannot handle 64-bit instructions in 32-bit kernel */\n\t\tgoto sigill;\n\n\tcase sh_op:\n\t\tif (!access_ok(VERIFY_WRITE, addr, 2))\n\t\t\tgoto sigbus;\n\n\t\tvalue = regs->regs[insn.i_format.rt];\n\t\t__asm__ __volatile__ (\n#ifdef __BIG_ENDIAN\n\t\t\t\".set\\tnoat\\n\"\n\t\t\t\"1:\\tsb\\t%1, 1(%2)\\n\\t\"\n\t\t\t\"srl\\t$1, %1, 0x8\\n\"\n\t\t\t\"2:\\tsb\\t$1, 0(%2)\\n\\t\"\n\t\t\t\".set\\tat\\n\\t\"\n#endif\n#ifdef __LITTLE_ENDIAN\n\t\t\t\".set\\tnoat\\n\"\n\t\t\t\"1:\\tsb\\t%1, 0(%2)\\n\\t\"\n\t\t\t\"srl\\t$1,%1, 0x8\\n\"\n\t\t\t\"2:\\tsb\\t$1, 1(%2)\\n\\t\"\n\t\t\t\".set\\tat\\n\\t\"\n#endif\n\t\t\t\"li\\t%0, 0\\n\"\n\t\t\t\"3:\\n\\t\"\n\t\t\t\".section\\t.fixup,\\\"ax\\\"\\n\\t\"\n\t\t\t\"4:\\tli\\t%0, %3\\n\\t\"\n\t\t\t\"j\\t3b\\n\\t\"\n\t\t\t\".previous\\n\\t\"\n\t\t\t\".section\\t__ex_table,\\\"a\\\"\\n\\t\"\n\t\t\tSTR(PTR)\"\\t1b, 4b\\n\\t\"\n\t\t\tSTR(PTR)\"\\t2b, 4b\\n\\t\"\n\t\t\t\".previous\"\n\t\t\t: \"=r\" (res)\n\t\t\t: \"r\" (value), \"r\" (addr), \"i\" (-EFAULT));\n\t\tif (res)\n\t\t\tgoto fault;\n\t\tcompute_return_epc(regs);\n\t\tbreak;\n\n\tcase sw_op:\n\t\tif (!access_ok(VERIFY_WRITE, addr, 4))\n\t\t\tgoto sigbus;\n\n\t\tvalue = regs->regs[insn.i_format.rt];\n\t\t__asm__ __volatile__ (\n#ifdef __BIG_ENDIAN\n\t\t\t\"1:\\tswl\\t%1,(%2)\\n\"\n\t\t\t\"2:\\tswr\\t%1, 3(%2)\\n\\t\"\n#endif\n#ifdef __LITTLE_ENDIAN\n\t\t\t\"1:\\tswl\\t%1, 3(%2)\\n\"\n\t\t\t\"2:\\tswr\\t%1, (%2)\\n\\t\"\n#endif\n\t\t\t\"li\\t%0, 0\\n\"\n\t\t\t\"3:\\n\\t\"\n\t\t\t\".section\\t.fixup,\\\"ax\\\"\\n\\t\"\n\t\t\t\"4:\\tli\\t%0, %3\\n\\t\"\n\t\t\t\"j\\t3b\\n\\t\"\n\t\t\t\".previous\\n\\t\"\n\t\t\t\".section\\t__ex_table,\\\"a\\\"\\n\\t\"\n\t\t\tSTR(PTR)\"\\t1b, 4b\\n\\t\"\n\t\t\tSTR(PTR)\"\\t2b, 4b\\n\\t\"\n\t\t\t\".previous\"\n\t\t: \"=r\" (res)\n\t\t: \"r\" (value), \"r\" (addr), \"i\" (-EFAULT));\n\t\tif (res)\n\t\t\tgoto fault;\n\t\tcompute_return_epc(regs);\n\t\tbreak;\n\n\tcase sd_op:\n#ifdef CONFIG_64BIT\n\t\t/*\n\t\t * A 32-bit kernel might be running on a 64-bit processor.  But\n\t\t * if we're on a 32-bit processor and an i-cache incoherency\n\t\t * or race makes us see a 64-bit instruction here the sdl/sdr\n\t\t * would blow up, so for now we don't handle unaligned 64-bit\n\t\t * instructions on 32-bit kernels.\n\t\t */\n\t\tif (!access_ok(VERIFY_WRITE, addr, 8))\n\t\t\tgoto sigbus;\n\n\t\tvalue = regs->regs[insn.i_format.rt];\n\t\t__asm__ __volatile__ (\n#ifdef __BIG_ENDIAN\n\t\t\t\"1:\\tsdl\\t%1,(%2)\\n\"\n\t\t\t\"2:\\tsdr\\t%1, 7(%2)\\n\\t\"\n#endif\n#ifdef __LITTLE_ENDIAN\n\t\t\t\"1:\\tsdl\\t%1, 7(%2)\\n\"\n\t\t\t\"2:\\tsdr\\t%1, (%2)\\n\\t\"\n#endif\n\t\t\t\"li\\t%0, 0\\n\"\n\t\t\t\"3:\\n\\t\"\n\t\t\t\".section\\t.fixup,\\\"ax\\\"\\n\\t\"\n\t\t\t\"4:\\tli\\t%0, %3\\n\\t\"\n\t\t\t\"j\\t3b\\n\\t\"\n\t\t\t\".previous\\n\\t\"\n\t\t\t\".section\\t__ex_table,\\\"a\\\"\\n\\t\"\n\t\t\tSTR(PTR)\"\\t1b, 4b\\n\\t\"\n\t\t\tSTR(PTR)\"\\t2b, 4b\\n\\t\"\n\t\t\t\".previous\"\n\t\t: \"=r\" (res)\n\t\t: \"r\" (value), \"r\" (addr), \"i\" (-EFAULT));\n\t\tif (res)\n\t\t\tgoto fault;\n\t\tcompute_return_epc(regs);\n\t\tbreak;\n#endif /* CONFIG_64BIT */\n\n\t\t/* Cannot handle 64-bit instructions in 32-bit kernel */\n\t\tgoto sigill;\n\n\tcase lwc1_op:\n\tcase ldc1_op:\n\tcase swc1_op:\n\tcase sdc1_op:\n\t\t/*\n\t\t * I herewith declare: this does not happen.  So send SIGBUS.\n\t\t */\n\t\tgoto sigbus;\n\n\t/*\n\t * COP2 is available to implementor for application specific use.\n\t * It's up to applications to register a notifier chain and do\n\t * whatever they have to do, including possible sending of signals.\n\t */\n\tcase lwc2_op:\n\t\tcu2_notifier_call_chain(CU2_LWC2_OP, regs);\n\t\tbreak;\n\n\tcase ldc2_op:\n\t\tcu2_notifier_call_chain(CU2_LDC2_OP, regs);\n\t\tbreak;\n\n\tcase swc2_op:\n\t\tcu2_notifier_call_chain(CU2_SWC2_OP, regs);\n\t\tbreak;\n\n\tcase sdc2_op:\n\t\tcu2_notifier_call_chain(CU2_SDC2_OP, regs);\n\t\tbreak;\n\n\tdefault:\n\t\t/*\n\t\t * Pheeee...  We encountered an yet unknown instruction or\n\t\t * cache coherence problem.  Die sucker, die ...\n\t\t */\n\t\tgoto sigill;\n\t}\n\n#ifdef CONFIG_DEBUG_FS\n\tunaligned_instructions++;\n#endif\n\n\treturn;\n\nfault:\n\t/* Did we have an exception handler installed? */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\tdie_if_kernel(\"Unhandled kernel unaligned access\", regs);\n\tforce_sig(SIGSEGV, current);\n\n\treturn;\n\nsigbus:\n\tdie_if_kernel(\"Unhandled kernel unaligned access\", regs);\n\tforce_sig(SIGBUS, current);\n\n\treturn;\n\nsigill:\n\tdie_if_kernel(\"Unhandled kernel unaligned access or invalid instruction\", regs);\n\tforce_sig(SIGILL, current);\n}\n\nasmlinkage void do_ade(struct pt_regs *regs)\n{\n\tunsigned int __user *pc;\n\tmm_segment_t seg;\n\n\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS,\n\t\t\t1, 0, regs, regs->cp0_badvaddr);\n\t/*\n\t * Did we catch a fault trying to load an instruction?\n\t * Or are we running in MIPS16 mode?\n\t */\n\tif ((regs->cp0_badvaddr == regs->cp0_epc) || (regs->cp0_epc & 0x1))\n\t\tgoto sigbus;\n\n\tpc = (unsigned int __user *) exception_epc(regs);\n\tif (user_mode(regs) && !test_thread_flag(TIF_FIXADE))\n\t\tgoto sigbus;\n\tif (unaligned_action == UNALIGNED_ACTION_SIGNAL)\n\t\tgoto sigbus;\n\telse if (unaligned_action == UNALIGNED_ACTION_SHOW)\n\t\tshow_registers(regs);\n\n\t/*\n\t * Do branch emulation only if we didn't forward the exception.\n\t * This is all so but ugly ...\n\t */\n\tseg = get_fs();\n\tif (!user_mode(regs))\n\t\tset_fs(KERNEL_DS);\n\temulate_load_store_insn(regs, (void __user *)regs->cp0_badvaddr, pc);\n\tset_fs(seg);\n\n\treturn;\n\nsigbus:\n\tdie_if_kernel(\"Kernel unaligned instruction access\", regs);\n\tforce_sig(SIGBUS, current);\n\n\t/*\n\t * XXX On return from the signal handler we should advance the epc\n\t */\n}\n\n#ifdef CONFIG_DEBUG_FS\nextern struct dentry *mips_debugfs_dir;\nstatic int __init debugfs_unaligned(void)\n{\n\tstruct dentry *d;\n\n\tif (!mips_debugfs_dir)\n\t\treturn -ENODEV;\n\td = debugfs_create_u32(\"unaligned_instructions\", S_IRUGO,\n\t\t\t       mips_debugfs_dir, &unaligned_instructions);\n\tif (!d)\n\t\treturn -ENOMEM;\n\td = debugfs_create_u32(\"unaligned_action\", S_IRUGO | S_IWUSR,\n\t\t\t       mips_debugfs_dir, &unaligned_action);\n\tif (!d)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n__initcall(debugfs_unaligned);\n#endif\n", "/*\n * cp1emu.c: a MIPS coprocessor 1 (fpu) instruction emulator\n *\n * MIPS floating point support\n * Copyright (C) 1994-2000 Algorithmics Ltd.\n *\n * Kevin D. Kissell, kevink@mips.com and Carsten Langgaard, carstenl@mips.com\n * Copyright (C) 2000  MIPS Technologies, Inc.\n *\n *  This program is free software; you can distribute it and/or modify it\n *  under the terms of the GNU General Public License (Version 2) as\n *  published by the Free Software Foundation.\n *\n *  This program is distributed in the hope it will be useful, but WITHOUT\n *  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n *  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n *  for more details.\n *\n *  You should have received a copy of the GNU General Public License along\n *  with this program; if not, write to the Free Software Foundation, Inc.,\n *  59 Temple Place - Suite 330, Boston MA 02111-1307, USA.\n *\n * A complete emulator for MIPS coprocessor 1 instructions.  This is\n * required for #float(switch) or #float(trap), where it catches all\n * COP1 instructions via the \"CoProcessor Unusable\" exception.\n *\n * More surprisingly it is also required for #float(ieee), to help out\n * the hardware fpu at the boundaries of the IEEE-754 representation\n * (denormalised values, infinities, underflow, etc).  It is made\n * quite nasty because emulation of some non-COP1 instructions is\n * required, e.g. in branch delay slots.\n *\n * Note if you know that you won't have an fpu, then you'll get much\n * better performance by compiling with -msoft-float!\n */\n#include <linux/sched.h>\n#include <linux/module.h>\n#include <linux/debugfs.h>\n#include <linux/perf_event.h>\n\n#include <asm/inst.h>\n#include <asm/bootinfo.h>\n#include <asm/processor.h>\n#include <asm/ptrace.h>\n#include <asm/signal.h>\n#include <asm/mipsregs.h>\n#include <asm/fpu_emulator.h>\n#include <asm/uaccess.h>\n#include <asm/branch.h>\n\n#include \"ieee754.h\"\n\n/* Strap kernel emulator for full MIPS IV emulation */\n\n#ifdef __mips\n#undef __mips\n#endif\n#define __mips 4\n\n/* Function which emulates a floating point instruction. */\n\nstatic int fpu_emu(struct pt_regs *, struct mips_fpu_struct *,\n\tmips_instruction);\n\n#if __mips >= 4 && __mips != 32\nstatic int fpux_emu(struct pt_regs *,\n\tstruct mips_fpu_struct *, mips_instruction, void *__user *);\n#endif\n\n/* Further private data for which no space exists in mips_fpu_struct */\n\n#ifdef CONFIG_DEBUG_FS\nDEFINE_PER_CPU(struct mips_fpu_emulator_stats, fpuemustats);\n#endif\n\n/* Control registers */\n\n#define FPCREG_RID\t0\t/* $0  = revision id */\n#define FPCREG_CSR\t31\t/* $31 = csr */\n\n/* Determine rounding mode from the RM bits of the FCSR */\n#define modeindex(v) ((v) & FPU_CSR_RM)\n\n/* Convert Mips rounding mode (0..3) to IEEE library modes. */\nstatic const unsigned char ieee_rm[4] = {\n\t[FPU_CSR_RN] = IEEE754_RN,\n\t[FPU_CSR_RZ] = IEEE754_RZ,\n\t[FPU_CSR_RU] = IEEE754_RU,\n\t[FPU_CSR_RD] = IEEE754_RD,\n};\n/* Convert IEEE library modes to Mips rounding mode (0..3). */\nstatic const unsigned char mips_rm[4] = {\n\t[IEEE754_RN] = FPU_CSR_RN,\n\t[IEEE754_RZ] = FPU_CSR_RZ,\n\t[IEEE754_RD] = FPU_CSR_RD,\n\t[IEEE754_RU] = FPU_CSR_RU,\n};\n\n#if __mips >= 4\n/* convert condition code register number to csr bit */\nstatic const unsigned int fpucondbit[8] = {\n\tFPU_CSR_COND0,\n\tFPU_CSR_COND1,\n\tFPU_CSR_COND2,\n\tFPU_CSR_COND3,\n\tFPU_CSR_COND4,\n\tFPU_CSR_COND5,\n\tFPU_CSR_COND6,\n\tFPU_CSR_COND7\n};\n#endif\n\n\n/*\n * Redundant with logic already in kernel/branch.c,\n * embedded in compute_return_epc.  At some point,\n * a single subroutine should be used across both\n * modules.\n */\nstatic int isBranchInstr(mips_instruction * i)\n{\n\tswitch (MIPSInst_OPCODE(*i)) {\n\tcase spec_op:\n\t\tswitch (MIPSInst_FUNC(*i)) {\n\t\tcase jalr_op:\n\t\tcase jr_op:\n\t\t\treturn 1;\n\t\t}\n\t\tbreak;\n\n\tcase bcond_op:\n\t\tswitch (MIPSInst_RT(*i)) {\n\t\tcase bltz_op:\n\t\tcase bgez_op:\n\t\tcase bltzl_op:\n\t\tcase bgezl_op:\n\t\tcase bltzal_op:\n\t\tcase bgezal_op:\n\t\tcase bltzall_op:\n\t\tcase bgezall_op:\n\t\t\treturn 1;\n\t\t}\n\t\tbreak;\n\n\tcase j_op:\n\tcase jal_op:\n\tcase jalx_op:\n\tcase beq_op:\n\tcase bne_op:\n\tcase blez_op:\n\tcase bgtz_op:\n\tcase beql_op:\n\tcase bnel_op:\n\tcase blezl_op:\n\tcase bgtzl_op:\n\t\treturn 1;\n\n\tcase cop0_op:\n\tcase cop1_op:\n\tcase cop2_op:\n\tcase cop1x_op:\n\t\tif (MIPSInst_RS(*i) == bc_op)\n\t\t\treturn 1;\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\n/*\n * In the Linux kernel, we support selection of FPR format on the\n * basis of the Status.FR bit.  If an FPU is not present, the FR bit\n * is hardwired to zero, which would imply a 32-bit FPU even for\n * 64-bit CPUs.  For 64-bit kernels with no FPU we use TIF_32BIT_REGS\n * as a proxy for the FR bit so that a 64-bit FPU is emulated.  In any\n * case, for a 32-bit kernel which uses the O32 MIPS ABI, only the\n * even FPRs are used (Status.FR = 0).\n */\nstatic inline int cop1_64bit(struct pt_regs *xcp)\n{\n\tif (cpu_has_fpu)\n\t\treturn xcp->cp0_status & ST0_FR;\n#ifdef CONFIG_64BIT\n\treturn !test_thread_flag(TIF_32BIT_REGS);\n#else\n\treturn 0;\n#endif\n}\n\n#define SIFROMREG(si, x) ((si) = cop1_64bit(xcp) || !(x & 1) ? \\\n\t\t\t(int)ctx->fpr[x] : (int)(ctx->fpr[x & ~1] >> 32))\n\n#define SITOREG(si, x)\t(ctx->fpr[x & ~(cop1_64bit(xcp) == 0)] = \\\n\t\t\tcop1_64bit(xcp) || !(x & 1) ? \\\n\t\t\tctx->fpr[x & ~1] >> 32 << 32 | (u32)(si) : \\\n\t\t\tctx->fpr[x & ~1] << 32 >> 32 | (u64)(si) << 32)\n\n#define DIFROMREG(di, x) ((di) = ctx->fpr[x & ~(cop1_64bit(xcp) == 0)])\n#define DITOREG(di, x)\t(ctx->fpr[x & ~(cop1_64bit(xcp) == 0)] = (di))\n\n#define SPFROMREG(sp, x) SIFROMREG((sp).bits, x)\n#define SPTOREG(sp, x)\tSITOREG((sp).bits, x)\n#define DPFROMREG(dp, x)\tDIFROMREG((dp).bits, x)\n#define DPTOREG(dp, x)\tDITOREG((dp).bits, x)\n\n/*\n * Emulate the single floating point instruction pointed at by EPC.\n * Two instructions if the instruction is in a branch delay slot.\n */\n\nstatic int cop1Emulate(struct pt_regs *xcp, struct mips_fpu_struct *ctx,\n\t\t       void *__user *fault_addr)\n{\n\tmips_instruction ir;\n\tunsigned long emulpc, contpc;\n\tunsigned int cond;\n\n\tif (!access_ok(VERIFY_READ, xcp->cp0_epc, sizeof(mips_instruction))) {\n\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t*fault_addr = (mips_instruction __user *)xcp->cp0_epc;\n\t\treturn SIGBUS;\n\t}\n\tif (__get_user(ir, (mips_instruction __user *) xcp->cp0_epc)) {\n\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t*fault_addr = (mips_instruction __user *)xcp->cp0_epc;\n\t\treturn SIGSEGV;\n\t}\n\n\t/* XXX NEC Vr54xx bug workaround */\n\tif ((xcp->cp0_cause & CAUSEF_BD) && !isBranchInstr(&ir))\n\t\txcp->cp0_cause &= ~CAUSEF_BD;\n\n\tif (xcp->cp0_cause & CAUSEF_BD) {\n\t\t/*\n\t\t * The instruction to be emulated is in a branch delay slot\n\t\t * which means that we have to  emulate the branch instruction\n\t\t * BEFORE we do the cop1 instruction.\n\t\t *\n\t\t * This branch could be a COP1 branch, but in that case we\n\t\t * would have had a trap for that instruction, and would not\n\t\t * come through this route.\n\t\t *\n\t\t * Linux MIPS branch emulator operates on context, updating the\n\t\t * cp0_epc.\n\t\t */\n\t\temulpc = xcp->cp0_epc + 4;\t/* Snapshot emulation target */\n\n\t\tif (__compute_return_epc(xcp)) {\n#ifdef CP1DBG\n\t\t\tprintk(\"failed to emulate branch at %p\\n\",\n\t\t\t\t(void *) (xcp->cp0_epc));\n#endif\n\t\t\treturn SIGILL;\n\t\t}\n\t\tif (!access_ok(VERIFY_READ, emulpc, sizeof(mips_instruction))) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = (mips_instruction __user *)emulpc;\n\t\t\treturn SIGBUS;\n\t\t}\n\t\tif (__get_user(ir, (mips_instruction __user *) emulpc)) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = (mips_instruction __user *)emulpc;\n\t\t\treturn SIGSEGV;\n\t\t}\n\t\t/* __compute_return_epc() will have updated cp0_epc */\n\t\tcontpc = xcp->cp0_epc;\n\t\t/* In order not to confuse ptrace() et al, tweak context */\n\t\txcp->cp0_epc = emulpc - 4;\n\t} else {\n\t\temulpc = xcp->cp0_epc;\n\t\tcontpc = xcp->cp0_epc + 4;\n\t}\n\n      emul:\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,\n\t\t\t1, 0, xcp, 0);\n\tMIPS_FPU_EMU_INC_STATS(emulated);\n\tswitch (MIPSInst_OPCODE(ir)) {\n\tcase ldc1_op:{\n\t\tu64 __user *va = (u64 __user *) (xcp->regs[MIPSInst_RS(ir)] +\n\t\t\tMIPSInst_SIMM(ir));\n\t\tu64 val;\n\n\t\tMIPS_FPU_EMU_INC_STATS(loads);\n\n\t\tif (!access_ok(VERIFY_READ, va, sizeof(u64))) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = va;\n\t\t\treturn SIGBUS;\n\t\t}\n\t\tif (__get_user(val, va)) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = va;\n\t\t\treturn SIGSEGV;\n\t\t}\n\t\tDITOREG(val, MIPSInst_RT(ir));\n\t\tbreak;\n\t}\n\n\tcase sdc1_op:{\n\t\tu64 __user *va = (u64 __user *) (xcp->regs[MIPSInst_RS(ir)] +\n\t\t\tMIPSInst_SIMM(ir));\n\t\tu64 val;\n\n\t\tMIPS_FPU_EMU_INC_STATS(stores);\n\t\tDIFROMREG(val, MIPSInst_RT(ir));\n\t\tif (!access_ok(VERIFY_WRITE, va, sizeof(u64))) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = va;\n\t\t\treturn SIGBUS;\n\t\t}\n\t\tif (__put_user(val, va)) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = va;\n\t\t\treturn SIGSEGV;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase lwc1_op:{\n\t\tu32 __user *va = (u32 __user *) (xcp->regs[MIPSInst_RS(ir)] +\n\t\t\tMIPSInst_SIMM(ir));\n\t\tu32 val;\n\n\t\tMIPS_FPU_EMU_INC_STATS(loads);\n\t\tif (!access_ok(VERIFY_READ, va, sizeof(u32))) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = va;\n\t\t\treturn SIGBUS;\n\t\t}\n\t\tif (__get_user(val, va)) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = va;\n\t\t\treturn SIGSEGV;\n\t\t}\n\t\tSITOREG(val, MIPSInst_RT(ir));\n\t\tbreak;\n\t}\n\n\tcase swc1_op:{\n\t\tu32 __user *va = (u32 __user *) (xcp->regs[MIPSInst_RS(ir)] +\n\t\t\tMIPSInst_SIMM(ir));\n\t\tu32 val;\n\n\t\tMIPS_FPU_EMU_INC_STATS(stores);\n\t\tSIFROMREG(val, MIPSInst_RT(ir));\n\t\tif (!access_ok(VERIFY_WRITE, va, sizeof(u32))) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = va;\n\t\t\treturn SIGBUS;\n\t\t}\n\t\tif (__put_user(val, va)) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = va;\n\t\t\treturn SIGSEGV;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase cop1_op:\n\t\tswitch (MIPSInst_RS(ir)) {\n\n#if defined(__mips64)\n\t\tcase dmfc_op:\n\t\t\t/* copregister fs -> gpr[rt] */\n\t\t\tif (MIPSInst_RT(ir) != 0) {\n\t\t\t\tDIFROMREG(xcp->regs[MIPSInst_RT(ir)],\n\t\t\t\t\tMIPSInst_RD(ir));\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase dmtc_op:\n\t\t\t/* copregister fs <- rt */\n\t\t\tDITOREG(xcp->regs[MIPSInst_RT(ir)], MIPSInst_RD(ir));\n\t\t\tbreak;\n#endif\n\n\t\tcase mfc_op:\n\t\t\t/* copregister rd -> gpr[rt] */\n\t\t\tif (MIPSInst_RT(ir) != 0) {\n\t\t\t\tSIFROMREG(xcp->regs[MIPSInst_RT(ir)],\n\t\t\t\t\tMIPSInst_RD(ir));\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase mtc_op:\n\t\t\t/* copregister rd <- rt */\n\t\t\tSITOREG(xcp->regs[MIPSInst_RT(ir)], MIPSInst_RD(ir));\n\t\t\tbreak;\n\n\t\tcase cfc_op:{\n\t\t\t/* cop control register rd -> gpr[rt] */\n\t\t\tu32 value;\n\n\t\t\tif (MIPSInst_RD(ir) == FPCREG_CSR) {\n\t\t\t\tvalue = ctx->fcr31;\n\t\t\t\tvalue = (value & ~FPU_CSR_RM) |\n\t\t\t\t\tmips_rm[modeindex(value)];\n#ifdef CSRTRACE\n\t\t\t\tprintk(\"%p gpr[%d]<-csr=%08x\\n\",\n\t\t\t\t\t(void *) (xcp->cp0_epc),\n\t\t\t\t\tMIPSInst_RT(ir), value);\n#endif\n\t\t\t}\n\t\t\telse if (MIPSInst_RD(ir) == FPCREG_RID)\n\t\t\t\tvalue = 0;\n\t\t\telse\n\t\t\t\tvalue = 0;\n\t\t\tif (MIPSInst_RT(ir))\n\t\t\t\txcp->regs[MIPSInst_RT(ir)] = value;\n\t\t\tbreak;\n\t\t}\n\n\t\tcase ctc_op:{\n\t\t\t/* copregister rd <- rt */\n\t\t\tu32 value;\n\n\t\t\tif (MIPSInst_RT(ir) == 0)\n\t\t\t\tvalue = 0;\n\t\t\telse\n\t\t\t\tvalue = xcp->regs[MIPSInst_RT(ir)];\n\n\t\t\t/* we only have one writable control reg\n\t\t\t */\n\t\t\tif (MIPSInst_RD(ir) == FPCREG_CSR) {\n#ifdef CSRTRACE\n\t\t\t\tprintk(\"%p gpr[%d]->csr=%08x\\n\",\n\t\t\t\t\t(void *) (xcp->cp0_epc),\n\t\t\t\t\tMIPSInst_RT(ir), value);\n#endif\n\n\t\t\t\t/*\n\t\t\t\t * Don't write reserved bits,\n\t\t\t\t * and convert to ieee library modes\n\t\t\t\t */\n\t\t\t\tctx->fcr31 = (value &\n\t\t\t\t\t\t~(FPU_CSR_RSVD | FPU_CSR_RM)) |\n\t\t\t\t\t\tieee_rm[modeindex(value)];\n\t\t\t}\n\t\t\tif ((ctx->fcr31 >> 5) & ctx->fcr31 & FPU_CSR_ALL_E) {\n\t\t\t\treturn SIGFPE;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tcase bc_op:{\n\t\t\tint likely = 0;\n\n\t\t\tif (xcp->cp0_cause & CAUSEF_BD)\n\t\t\t\treturn SIGILL;\n\n#if __mips >= 4\n\t\t\tcond = ctx->fcr31 & fpucondbit[MIPSInst_RT(ir) >> 2];\n#else\n\t\t\tcond = ctx->fcr31 & FPU_CSR_COND;\n#endif\n\t\t\tswitch (MIPSInst_RT(ir) & 3) {\n\t\t\tcase bcfl_op:\n\t\t\t\tlikely = 1;\n\t\t\tcase bcf_op:\n\t\t\t\tcond = !cond;\n\t\t\t\tbreak;\n\t\t\tcase bctl_op:\n\t\t\t\tlikely = 1;\n\t\t\tcase bct_op:\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\t/* thats an illegal instruction */\n\t\t\t\treturn SIGILL;\n\t\t\t}\n\n\t\t\txcp->cp0_cause |= CAUSEF_BD;\n\t\t\tif (cond) {\n\t\t\t\t/* branch taken: emulate dslot\n\t\t\t\t * instruction\n\t\t\t\t */\n\t\t\t\txcp->cp0_epc += 4;\n\t\t\t\tcontpc = (xcp->cp0_epc +\n\t\t\t\t\t(MIPSInst_SIMM(ir) << 2));\n\n\t\t\t\tif (!access_ok(VERIFY_READ, xcp->cp0_epc,\n\t\t\t\t\t       sizeof(mips_instruction))) {\n\t\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t\t*fault_addr = (mips_instruction __user *)xcp->cp0_epc;\n\t\t\t\t\treturn SIGBUS;\n\t\t\t\t}\n\t\t\t\tif (__get_user(ir,\n\t\t\t\t    (mips_instruction __user *) xcp->cp0_epc)) {\n\t\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t\t*fault_addr = (mips_instruction __user *)xcp->cp0_epc;\n\t\t\t\t\treturn SIGSEGV;\n\t\t\t\t}\n\n\t\t\t\tswitch (MIPSInst_OPCODE(ir)) {\n\t\t\t\tcase lwc1_op:\n\t\t\t\tcase swc1_op:\n#if (__mips >= 2 || defined(__mips64))\n\t\t\t\tcase ldc1_op:\n\t\t\t\tcase sdc1_op:\n#endif\n\t\t\t\tcase cop1_op:\n#if __mips >= 4 && __mips != 32\n\t\t\t\tcase cop1x_op:\n#endif\n\t\t\t\t\t/* its one of ours */\n\t\t\t\t\tgoto emul;\n#if __mips >= 4\n\t\t\t\tcase spec_op:\n\t\t\t\t\tif (MIPSInst_FUNC(ir) == movc_op)\n\t\t\t\t\t\tgoto emul;\n\t\t\t\t\tbreak;\n#endif\n\t\t\t\t}\n\n\t\t\t\t/*\n\t\t\t\t * Single step the non-cp1\n\t\t\t\t * instruction in the dslot\n\t\t\t\t */\n\t\t\t\treturn mips_dsemul(xcp, ir, contpc);\n\t\t\t}\n\t\t\telse {\n\t\t\t\t/* branch not taken */\n\t\t\t\tif (likely) {\n\t\t\t\t\t/*\n\t\t\t\t\t * branch likely nullifies\n\t\t\t\t\t * dslot if not taken\n\t\t\t\t\t */\n\t\t\t\t\txcp->cp0_epc += 4;\n\t\t\t\t\tcontpc += 4;\n\t\t\t\t\t/*\n\t\t\t\t\t * else continue & execute\n\t\t\t\t\t * dslot as normal insn\n\t\t\t\t\t */\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tdefault:\n\t\t\tif (!(MIPSInst_RS(ir) & 0x10))\n\t\t\t\treturn SIGILL;\n\t\t\t{\n\t\t\t\tint sig;\n\n\t\t\t\t/* a real fpu computation instruction */\n\t\t\t\tif ((sig = fpu_emu(xcp, ctx, ir)))\n\t\t\t\t\treturn sig;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n#if __mips >= 4 && __mips != 32\n\tcase cop1x_op:{\n\t\tint sig = fpux_emu(xcp, ctx, ir, fault_addr);\n\t\tif (sig)\n\t\t\treturn sig;\n\t\tbreak;\n\t}\n#endif\n\n#if __mips >= 4\n\tcase spec_op:\n\t\tif (MIPSInst_FUNC(ir) != movc_op)\n\t\t\treturn SIGILL;\n\t\tcond = fpucondbit[MIPSInst_RT(ir) >> 2];\n\t\tif (((ctx->fcr31 & cond) != 0) == ((MIPSInst_RT(ir) & 1) != 0))\n\t\t\txcp->regs[MIPSInst_RD(ir)] =\n\t\t\t\txcp->regs[MIPSInst_RS(ir)];\n\t\tbreak;\n#endif\n\n\tdefault:\n\t\treturn SIGILL;\n\t}\n\n\t/* we did it !! */\n\txcp->cp0_epc = contpc;\n\txcp->cp0_cause &= ~CAUSEF_BD;\n\n\treturn 0;\n}\n\n/*\n * Conversion table from MIPS compare ops 48-63\n * cond = ieee754dp_cmp(x,y,IEEE754_UN,sig);\n */\nstatic const unsigned char cmptab[8] = {\n\t0,\t\t\t/* cmp_0 (sig) cmp_sf */\n\tIEEE754_CUN,\t\t/* cmp_un (sig) cmp_ngle */\n\tIEEE754_CEQ,\t\t/* cmp_eq (sig) cmp_seq */\n\tIEEE754_CEQ | IEEE754_CUN,\t/* cmp_ueq (sig) cmp_ngl  */\n\tIEEE754_CLT,\t\t/* cmp_olt (sig) cmp_lt */\n\tIEEE754_CLT | IEEE754_CUN,\t/* cmp_ult (sig) cmp_nge */\n\tIEEE754_CLT | IEEE754_CEQ,\t/* cmp_ole (sig) cmp_le */\n\tIEEE754_CLT | IEEE754_CEQ | IEEE754_CUN,\t/* cmp_ule (sig) cmp_ngt */\n};\n\n\n#if __mips >= 4 && __mips != 32\n\n/*\n * Additional MIPS4 instructions\n */\n\n#define DEF3OP(name, p, f1, f2, f3) \\\nstatic ieee754##p fpemu_##p##_##name(ieee754##p r, ieee754##p s, \\\n    ieee754##p t) \\\n{ \\\n\tstruct _ieee754_csr ieee754_csr_save; \\\n\ts = f1(s, t); \\\n\tieee754_csr_save = ieee754_csr; \\\n\ts = f2(s, r); \\\n\tieee754_csr_save.cx |= ieee754_csr.cx; \\\n\tieee754_csr_save.sx |= ieee754_csr.sx; \\\n\ts = f3(s); \\\n\tieee754_csr.cx |= ieee754_csr_save.cx; \\\n\tieee754_csr.sx |= ieee754_csr_save.sx; \\\n\treturn s; \\\n}\n\nstatic ieee754dp fpemu_dp_recip(ieee754dp d)\n{\n\treturn ieee754dp_div(ieee754dp_one(0), d);\n}\n\nstatic ieee754dp fpemu_dp_rsqrt(ieee754dp d)\n{\n\treturn ieee754dp_div(ieee754dp_one(0), ieee754dp_sqrt(d));\n}\n\nstatic ieee754sp fpemu_sp_recip(ieee754sp s)\n{\n\treturn ieee754sp_div(ieee754sp_one(0), s);\n}\n\nstatic ieee754sp fpemu_sp_rsqrt(ieee754sp s)\n{\n\treturn ieee754sp_div(ieee754sp_one(0), ieee754sp_sqrt(s));\n}\n\nDEF3OP(madd, sp, ieee754sp_mul, ieee754sp_add, );\nDEF3OP(msub, sp, ieee754sp_mul, ieee754sp_sub, );\nDEF3OP(nmadd, sp, ieee754sp_mul, ieee754sp_add, ieee754sp_neg);\nDEF3OP(nmsub, sp, ieee754sp_mul, ieee754sp_sub, ieee754sp_neg);\nDEF3OP(madd, dp, ieee754dp_mul, ieee754dp_add, );\nDEF3OP(msub, dp, ieee754dp_mul, ieee754dp_sub, );\nDEF3OP(nmadd, dp, ieee754dp_mul, ieee754dp_add, ieee754dp_neg);\nDEF3OP(nmsub, dp, ieee754dp_mul, ieee754dp_sub, ieee754dp_neg);\n\nstatic int fpux_emu(struct pt_regs *xcp, struct mips_fpu_struct *ctx,\n\tmips_instruction ir, void *__user *fault_addr)\n{\n\tunsigned rcsr = 0;\t/* resulting csr */\n\n\tMIPS_FPU_EMU_INC_STATS(cp1xops);\n\n\tswitch (MIPSInst_FMA_FFMT(ir)) {\n\tcase s_fmt:{\t\t/* 0 */\n\n\t\tieee754sp(*handler) (ieee754sp, ieee754sp, ieee754sp);\n\t\tieee754sp fd, fr, fs, ft;\n\t\tu32 __user *va;\n\t\tu32 val;\n\n\t\tswitch (MIPSInst_FUNC(ir)) {\n\t\tcase lwxc1_op:\n\t\t\tva = (void __user *) (xcp->regs[MIPSInst_FR(ir)] +\n\t\t\t\txcp->regs[MIPSInst_FT(ir)]);\n\n\t\t\tMIPS_FPU_EMU_INC_STATS(loads);\n\t\t\tif (!access_ok(VERIFY_READ, va, sizeof(u32))) {\n\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t*fault_addr = va;\n\t\t\t\treturn SIGBUS;\n\t\t\t}\n\t\t\tif (__get_user(val, va)) {\n\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t*fault_addr = va;\n\t\t\t\treturn SIGSEGV;\n\t\t\t}\n\t\t\tSITOREG(val, MIPSInst_FD(ir));\n\t\t\tbreak;\n\n\t\tcase swxc1_op:\n\t\t\tva = (void __user *) (xcp->regs[MIPSInst_FR(ir)] +\n\t\t\t\txcp->regs[MIPSInst_FT(ir)]);\n\n\t\t\tMIPS_FPU_EMU_INC_STATS(stores);\n\n\t\t\tSIFROMREG(val, MIPSInst_FS(ir));\n\t\t\tif (!access_ok(VERIFY_WRITE, va, sizeof(u32))) {\n\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t*fault_addr = va;\n\t\t\t\treturn SIGBUS;\n\t\t\t}\n\t\t\tif (put_user(val, va)) {\n\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t*fault_addr = va;\n\t\t\t\treturn SIGSEGV;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase madd_s_op:\n\t\t\thandler = fpemu_sp_madd;\n\t\t\tgoto scoptop;\n\t\tcase msub_s_op:\n\t\t\thandler = fpemu_sp_msub;\n\t\t\tgoto scoptop;\n\t\tcase nmadd_s_op:\n\t\t\thandler = fpemu_sp_nmadd;\n\t\t\tgoto scoptop;\n\t\tcase nmsub_s_op:\n\t\t\thandler = fpemu_sp_nmsub;\n\t\t\tgoto scoptop;\n\n\t\t      scoptop:\n\t\t\tSPFROMREG(fr, MIPSInst_FR(ir));\n\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\tSPFROMREG(ft, MIPSInst_FT(ir));\n\t\t\tfd = (*handler) (fr, fs, ft);\n\t\t\tSPTOREG(fd, MIPSInst_FD(ir));\n\n\t\t      copcsr:\n\t\t\tif (ieee754_cxtest(IEEE754_INEXACT))\n\t\t\t\trcsr |= FPU_CSR_INE_X | FPU_CSR_INE_S;\n\t\t\tif (ieee754_cxtest(IEEE754_UNDERFLOW))\n\t\t\t\trcsr |= FPU_CSR_UDF_X | FPU_CSR_UDF_S;\n\t\t\tif (ieee754_cxtest(IEEE754_OVERFLOW))\n\t\t\t\trcsr |= FPU_CSR_OVF_X | FPU_CSR_OVF_S;\n\t\t\tif (ieee754_cxtest(IEEE754_INVALID_OPERATION))\n\t\t\t\trcsr |= FPU_CSR_INV_X | FPU_CSR_INV_S;\n\n\t\t\tctx->fcr31 = (ctx->fcr31 & ~FPU_CSR_ALL_X) | rcsr;\n\t\t\tif ((ctx->fcr31 >> 5) & ctx->fcr31 & FPU_CSR_ALL_E) {\n\t\t\t\t/*printk (\"SIGFPE: fpu csr = %08x\\n\",\n\t\t\t\t   ctx->fcr31); */\n\t\t\t\treturn SIGFPE;\n\t\t\t}\n\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\treturn SIGILL;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase d_fmt:{\t\t/* 1 */\n\t\tieee754dp(*handler) (ieee754dp, ieee754dp, ieee754dp);\n\t\tieee754dp fd, fr, fs, ft;\n\t\tu64 __user *va;\n\t\tu64 val;\n\n\t\tswitch (MIPSInst_FUNC(ir)) {\n\t\tcase ldxc1_op:\n\t\t\tva = (void __user *) (xcp->regs[MIPSInst_FR(ir)] +\n\t\t\t\txcp->regs[MIPSInst_FT(ir)]);\n\n\t\t\tMIPS_FPU_EMU_INC_STATS(loads);\n\t\t\tif (!access_ok(VERIFY_READ, va, sizeof(u64))) {\n\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t*fault_addr = va;\n\t\t\t\treturn SIGBUS;\n\t\t\t}\n\t\t\tif (__get_user(val, va)) {\n\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t*fault_addr = va;\n\t\t\t\treturn SIGSEGV;\n\t\t\t}\n\t\t\tDITOREG(val, MIPSInst_FD(ir));\n\t\t\tbreak;\n\n\t\tcase sdxc1_op:\n\t\t\tva = (void __user *) (xcp->regs[MIPSInst_FR(ir)] +\n\t\t\t\txcp->regs[MIPSInst_FT(ir)]);\n\n\t\t\tMIPS_FPU_EMU_INC_STATS(stores);\n\t\t\tDIFROMREG(val, MIPSInst_FS(ir));\n\t\t\tif (!access_ok(VERIFY_WRITE, va, sizeof(u64))) {\n\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t*fault_addr = va;\n\t\t\t\treturn SIGBUS;\n\t\t\t}\n\t\t\tif (__put_user(val, va)) {\n\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t*fault_addr = va;\n\t\t\t\treturn SIGSEGV;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase madd_d_op:\n\t\t\thandler = fpemu_dp_madd;\n\t\t\tgoto dcoptop;\n\t\tcase msub_d_op:\n\t\t\thandler = fpemu_dp_msub;\n\t\t\tgoto dcoptop;\n\t\tcase nmadd_d_op:\n\t\t\thandler = fpemu_dp_nmadd;\n\t\t\tgoto dcoptop;\n\t\tcase nmsub_d_op:\n\t\t\thandler = fpemu_dp_nmsub;\n\t\t\tgoto dcoptop;\n\n\t\t      dcoptop:\n\t\t\tDPFROMREG(fr, MIPSInst_FR(ir));\n\t\t\tDPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\tDPFROMREG(ft, MIPSInst_FT(ir));\n\t\t\tfd = (*handler) (fr, fs, ft);\n\t\t\tDPTOREG(fd, MIPSInst_FD(ir));\n\t\t\tgoto copcsr;\n\n\t\tdefault:\n\t\t\treturn SIGILL;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase 0x7:\t\t/* 7 */\n\t\tif (MIPSInst_FUNC(ir) != pfetch_op) {\n\t\t\treturn SIGILL;\n\t\t}\n\t\t/* ignore prefx operation */\n\t\tbreak;\n\n\tdefault:\n\t\treturn SIGILL;\n\t}\n\n\treturn 0;\n}\n#endif\n\n\n\n/*\n * Emulate a single COP1 arithmetic instruction.\n */\nstatic int fpu_emu(struct pt_regs *xcp, struct mips_fpu_struct *ctx,\n\tmips_instruction ir)\n{\n\tint rfmt;\t\t/* resulting format */\n\tunsigned rcsr = 0;\t/* resulting csr */\n\tunsigned cond;\n\tunion {\n\t\tieee754dp d;\n\t\tieee754sp s;\n\t\tint w;\n#ifdef __mips64\n\t\ts64 l;\n#endif\n\t} rv;\t\t\t/* resulting value */\n\n\tMIPS_FPU_EMU_INC_STATS(cp1ops);\n\tswitch (rfmt = (MIPSInst_FFMT(ir) & 0xf)) {\n\tcase s_fmt:{\t\t/* 0 */\n\t\tunion {\n\t\t\tieee754sp(*b) (ieee754sp, ieee754sp);\n\t\t\tieee754sp(*u) (ieee754sp);\n\t\t} handler;\n\n\t\tswitch (MIPSInst_FUNC(ir)) {\n\t\t\t/* binary ops */\n\t\tcase fadd_op:\n\t\t\thandler.b = ieee754sp_add;\n\t\t\tgoto scopbop;\n\t\tcase fsub_op:\n\t\t\thandler.b = ieee754sp_sub;\n\t\t\tgoto scopbop;\n\t\tcase fmul_op:\n\t\t\thandler.b = ieee754sp_mul;\n\t\t\tgoto scopbop;\n\t\tcase fdiv_op:\n\t\t\thandler.b = ieee754sp_div;\n\t\t\tgoto scopbop;\n\n\t\t\t/* unary  ops */\n#if __mips >= 2 || defined(__mips64)\n\t\tcase fsqrt_op:\n\t\t\thandler.u = ieee754sp_sqrt;\n\t\t\tgoto scopuop;\n#endif\n#if __mips >= 4 && __mips != 32\n\t\tcase frsqrt_op:\n\t\t\thandler.u = fpemu_sp_rsqrt;\n\t\t\tgoto scopuop;\n\t\tcase frecip_op:\n\t\t\thandler.u = fpemu_sp_recip;\n\t\t\tgoto scopuop;\n#endif\n#if __mips >= 4\n\t\tcase fmovc_op:\n\t\t\tcond = fpucondbit[MIPSInst_FT(ir) >> 2];\n\t\t\tif (((ctx->fcr31 & cond) != 0) !=\n\t\t\t\t((MIPSInst_FT(ir) & 1) != 0))\n\t\t\t\treturn 0;\n\t\t\tSPFROMREG(rv.s, MIPSInst_FS(ir));\n\t\t\tbreak;\n\t\tcase fmovz_op:\n\t\t\tif (xcp->regs[MIPSInst_FT(ir)] != 0)\n\t\t\t\treturn 0;\n\t\t\tSPFROMREG(rv.s, MIPSInst_FS(ir));\n\t\t\tbreak;\n\t\tcase fmovn_op:\n\t\t\tif (xcp->regs[MIPSInst_FT(ir)] == 0)\n\t\t\t\treturn 0;\n\t\t\tSPFROMREG(rv.s, MIPSInst_FS(ir));\n\t\t\tbreak;\n#endif\n\t\tcase fabs_op:\n\t\t\thandler.u = ieee754sp_abs;\n\t\t\tgoto scopuop;\n\t\tcase fneg_op:\n\t\t\thandler.u = ieee754sp_neg;\n\t\t\tgoto scopuop;\n\t\tcase fmov_op:\n\t\t\t/* an easy one */\n\t\t\tSPFROMREG(rv.s, MIPSInst_FS(ir));\n\t\t\tgoto copcsr;\n\n\t\t\t/* binary op on handler */\n\t\t      scopbop:\n\t\t\t{\n\t\t\t\tieee754sp fs, ft;\n\n\t\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\t\tSPFROMREG(ft, MIPSInst_FT(ir));\n\n\t\t\t\trv.s = (*handler.b) (fs, ft);\n\t\t\t\tgoto copcsr;\n\t\t\t}\n\t\t      scopuop:\n\t\t\t{\n\t\t\t\tieee754sp fs;\n\n\t\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\t\trv.s = (*handler.u) (fs);\n\t\t\t\tgoto copcsr;\n\t\t\t}\n\t\t      copcsr:\n\t\t\tif (ieee754_cxtest(IEEE754_INEXACT))\n\t\t\t\trcsr |= FPU_CSR_INE_X | FPU_CSR_INE_S;\n\t\t\tif (ieee754_cxtest(IEEE754_UNDERFLOW))\n\t\t\t\trcsr |= FPU_CSR_UDF_X | FPU_CSR_UDF_S;\n\t\t\tif (ieee754_cxtest(IEEE754_OVERFLOW))\n\t\t\t\trcsr |= FPU_CSR_OVF_X | FPU_CSR_OVF_S;\n\t\t\tif (ieee754_cxtest(IEEE754_ZERO_DIVIDE))\n\t\t\t\trcsr |= FPU_CSR_DIV_X | FPU_CSR_DIV_S;\n\t\t\tif (ieee754_cxtest(IEEE754_INVALID_OPERATION))\n\t\t\t\trcsr |= FPU_CSR_INV_X | FPU_CSR_INV_S;\n\t\t\tbreak;\n\n\t\t\t/* unary conv ops */\n\t\tcase fcvts_op:\n\t\t\treturn SIGILL;\t/* not defined */\n\t\tcase fcvtd_op:{\n\t\t\tieee754sp fs;\n\n\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\trv.d = ieee754dp_fsp(fs);\n\t\t\trfmt = d_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n\t\tcase fcvtw_op:{\n\t\t\tieee754sp fs;\n\n\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\trv.w = ieee754sp_tint(fs);\n\t\t\trfmt = w_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n\n#if __mips >= 2 || defined(__mips64)\n\t\tcase fround_op:\n\t\tcase ftrunc_op:\n\t\tcase fceil_op:\n\t\tcase ffloor_op:{\n\t\t\tunsigned int oldrm = ieee754_csr.rm;\n\t\t\tieee754sp fs;\n\n\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\tieee754_csr.rm = ieee_rm[modeindex(MIPSInst_FUNC(ir))];\n\t\t\trv.w = ieee754sp_tint(fs);\n\t\t\tieee754_csr.rm = oldrm;\n\t\t\trfmt = w_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n#endif /* __mips >= 2 */\n\n#if defined(__mips64)\n\t\tcase fcvtl_op:{\n\t\t\tieee754sp fs;\n\n\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\trv.l = ieee754sp_tlong(fs);\n\t\t\trfmt = l_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n\n\t\tcase froundl_op:\n\t\tcase ftruncl_op:\n\t\tcase fceill_op:\n\t\tcase ffloorl_op:{\n\t\t\tunsigned int oldrm = ieee754_csr.rm;\n\t\t\tieee754sp fs;\n\n\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\tieee754_csr.rm = ieee_rm[modeindex(MIPSInst_FUNC(ir))];\n\t\t\trv.l = ieee754sp_tlong(fs);\n\t\t\tieee754_csr.rm = oldrm;\n\t\t\trfmt = l_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n#endif /* defined(__mips64) */\n\n\t\tdefault:\n\t\t\tif (MIPSInst_FUNC(ir) >= fcmp_op) {\n\t\t\t\tunsigned cmpop = MIPSInst_FUNC(ir) - fcmp_op;\n\t\t\t\tieee754sp fs, ft;\n\n\t\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\t\tSPFROMREG(ft, MIPSInst_FT(ir));\n\t\t\t\trv.w = ieee754sp_cmp(fs, ft,\n\t\t\t\t\tcmptab[cmpop & 0x7], cmpop & 0x8);\n\t\t\t\trfmt = -1;\n\t\t\t\tif ((cmpop & 0x8) && ieee754_cxtest\n\t\t\t\t\t(IEEE754_INVALID_OPERATION))\n\t\t\t\t\trcsr = FPU_CSR_INV_X | FPU_CSR_INV_S;\n\t\t\t\telse\n\t\t\t\t\tgoto copcsr;\n\n\t\t\t}\n\t\t\telse {\n\t\t\t\treturn SIGILL;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase d_fmt:{\n\t\tunion {\n\t\t\tieee754dp(*b) (ieee754dp, ieee754dp);\n\t\t\tieee754dp(*u) (ieee754dp);\n\t\t} handler;\n\n\t\tswitch (MIPSInst_FUNC(ir)) {\n\t\t\t/* binary ops */\n\t\tcase fadd_op:\n\t\t\thandler.b = ieee754dp_add;\n\t\t\tgoto dcopbop;\n\t\tcase fsub_op:\n\t\t\thandler.b = ieee754dp_sub;\n\t\t\tgoto dcopbop;\n\t\tcase fmul_op:\n\t\t\thandler.b = ieee754dp_mul;\n\t\t\tgoto dcopbop;\n\t\tcase fdiv_op:\n\t\t\thandler.b = ieee754dp_div;\n\t\t\tgoto dcopbop;\n\n\t\t\t/* unary  ops */\n#if __mips >= 2 || defined(__mips64)\n\t\tcase fsqrt_op:\n\t\t\thandler.u = ieee754dp_sqrt;\n\t\t\tgoto dcopuop;\n#endif\n#if __mips >= 4 && __mips != 32\n\t\tcase frsqrt_op:\n\t\t\thandler.u = fpemu_dp_rsqrt;\n\t\t\tgoto dcopuop;\n\t\tcase frecip_op:\n\t\t\thandler.u = fpemu_dp_recip;\n\t\t\tgoto dcopuop;\n#endif\n#if __mips >= 4\n\t\tcase fmovc_op:\n\t\t\tcond = fpucondbit[MIPSInst_FT(ir) >> 2];\n\t\t\tif (((ctx->fcr31 & cond) != 0) !=\n\t\t\t\t((MIPSInst_FT(ir) & 1) != 0))\n\t\t\t\treturn 0;\n\t\t\tDPFROMREG(rv.d, MIPSInst_FS(ir));\n\t\t\tbreak;\n\t\tcase fmovz_op:\n\t\t\tif (xcp->regs[MIPSInst_FT(ir)] != 0)\n\t\t\t\treturn 0;\n\t\t\tDPFROMREG(rv.d, MIPSInst_FS(ir));\n\t\t\tbreak;\n\t\tcase fmovn_op:\n\t\t\tif (xcp->regs[MIPSInst_FT(ir)] == 0)\n\t\t\t\treturn 0;\n\t\t\tDPFROMREG(rv.d, MIPSInst_FS(ir));\n\t\t\tbreak;\n#endif\n\t\tcase fabs_op:\n\t\t\thandler.u = ieee754dp_abs;\n\t\t\tgoto dcopuop;\n\n\t\tcase fneg_op:\n\t\t\thandler.u = ieee754dp_neg;\n\t\t\tgoto dcopuop;\n\n\t\tcase fmov_op:\n\t\t\t/* an easy one */\n\t\t\tDPFROMREG(rv.d, MIPSInst_FS(ir));\n\t\t\tgoto copcsr;\n\n\t\t\t/* binary op on handler */\n\t\t      dcopbop:{\n\t\t\t\tieee754dp fs, ft;\n\n\t\t\t\tDPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\t\tDPFROMREG(ft, MIPSInst_FT(ir));\n\n\t\t\t\trv.d = (*handler.b) (fs, ft);\n\t\t\t\tgoto copcsr;\n\t\t\t}\n\t\t      dcopuop:{\n\t\t\t\tieee754dp fs;\n\n\t\t\t\tDPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\t\trv.d = (*handler.u) (fs);\n\t\t\t\tgoto copcsr;\n\t\t\t}\n\n\t\t\t/* unary conv ops */\n\t\tcase fcvts_op:{\n\t\t\tieee754dp fs;\n\n\t\t\tDPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\trv.s = ieee754sp_fdp(fs);\n\t\t\trfmt = s_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n\t\tcase fcvtd_op:\n\t\t\treturn SIGILL;\t/* not defined */\n\n\t\tcase fcvtw_op:{\n\t\t\tieee754dp fs;\n\n\t\t\tDPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\trv.w = ieee754dp_tint(fs);\t/* wrong */\n\t\t\trfmt = w_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n\n#if __mips >= 2 || defined(__mips64)\n\t\tcase fround_op:\n\t\tcase ftrunc_op:\n\t\tcase fceil_op:\n\t\tcase ffloor_op:{\n\t\t\tunsigned int oldrm = ieee754_csr.rm;\n\t\t\tieee754dp fs;\n\n\t\t\tDPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\tieee754_csr.rm = ieee_rm[modeindex(MIPSInst_FUNC(ir))];\n\t\t\trv.w = ieee754dp_tint(fs);\n\t\t\tieee754_csr.rm = oldrm;\n\t\t\trfmt = w_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n#endif\n\n#if defined(__mips64)\n\t\tcase fcvtl_op:{\n\t\t\tieee754dp fs;\n\n\t\t\tDPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\trv.l = ieee754dp_tlong(fs);\n\t\t\trfmt = l_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n\n\t\tcase froundl_op:\n\t\tcase ftruncl_op:\n\t\tcase fceill_op:\n\t\tcase ffloorl_op:{\n\t\t\tunsigned int oldrm = ieee754_csr.rm;\n\t\t\tieee754dp fs;\n\n\t\t\tDPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\tieee754_csr.rm = ieee_rm[modeindex(MIPSInst_FUNC(ir))];\n\t\t\trv.l = ieee754dp_tlong(fs);\n\t\t\tieee754_csr.rm = oldrm;\n\t\t\trfmt = l_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n#endif /* __mips >= 3 */\n\n\t\tdefault:\n\t\t\tif (MIPSInst_FUNC(ir) >= fcmp_op) {\n\t\t\t\tunsigned cmpop = MIPSInst_FUNC(ir) - fcmp_op;\n\t\t\t\tieee754dp fs, ft;\n\n\t\t\t\tDPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\t\tDPFROMREG(ft, MIPSInst_FT(ir));\n\t\t\t\trv.w = ieee754dp_cmp(fs, ft,\n\t\t\t\t\tcmptab[cmpop & 0x7], cmpop & 0x8);\n\t\t\t\trfmt = -1;\n\t\t\t\tif ((cmpop & 0x8)\n\t\t\t\t\t&&\n\t\t\t\t\tieee754_cxtest\n\t\t\t\t\t(IEEE754_INVALID_OPERATION))\n\t\t\t\t\trcsr = FPU_CSR_INV_X | FPU_CSR_INV_S;\n\t\t\t\telse\n\t\t\t\t\tgoto copcsr;\n\n\t\t\t}\n\t\t\telse {\n\t\t\t\treturn SIGILL;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase w_fmt:{\n\t\tieee754sp fs;\n\n\t\tswitch (MIPSInst_FUNC(ir)) {\n\t\tcase fcvts_op:\n\t\t\t/* convert word to single precision real */\n\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\trv.s = ieee754sp_fint(fs.bits);\n\t\t\trfmt = s_fmt;\n\t\t\tgoto copcsr;\n\t\tcase fcvtd_op:\n\t\t\t/* convert word to double precision real */\n\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\trv.d = ieee754dp_fint(fs.bits);\n\t\t\trfmt = d_fmt;\n\t\t\tgoto copcsr;\n\t\tdefault:\n\t\t\treturn SIGILL;\n\t\t}\n\t\tbreak;\n\t}\n\n#if defined(__mips64)\n\tcase l_fmt:{\n\t\tswitch (MIPSInst_FUNC(ir)) {\n\t\tcase fcvts_op:\n\t\t\t/* convert long to single precision real */\n\t\t\trv.s = ieee754sp_flong(ctx->fpr[MIPSInst_FS(ir)]);\n\t\t\trfmt = s_fmt;\n\t\t\tgoto copcsr;\n\t\tcase fcvtd_op:\n\t\t\t/* convert long to double precision real */\n\t\t\trv.d = ieee754dp_flong(ctx->fpr[MIPSInst_FS(ir)]);\n\t\t\trfmt = d_fmt;\n\t\t\tgoto copcsr;\n\t\tdefault:\n\t\t\treturn SIGILL;\n\t\t}\n\t\tbreak;\n\t}\n#endif\n\n\tdefault:\n\t\treturn SIGILL;\n\t}\n\n\t/*\n\t * Update the fpu CSR register for this operation.\n\t * If an exception is required, generate a tidy SIGFPE exception,\n\t * without updating the result register.\n\t * Note: cause exception bits do not accumulate, they are rewritten\n\t * for each op; only the flag/sticky bits accumulate.\n\t */\n\tctx->fcr31 = (ctx->fcr31 & ~FPU_CSR_ALL_X) | rcsr;\n\tif ((ctx->fcr31 >> 5) & ctx->fcr31 & FPU_CSR_ALL_E) {\n\t\t/*printk (\"SIGFPE: fpu csr = %08x\\n\",ctx->fcr31); */\n\t\treturn SIGFPE;\n\t}\n\n\t/*\n\t * Now we can safely write the result back to the register file.\n\t */\n\tswitch (rfmt) {\n\tcase -1:{\n#if __mips >= 4\n\t\tcond = fpucondbit[MIPSInst_FD(ir) >> 2];\n#else\n\t\tcond = FPU_CSR_COND;\n#endif\n\t\tif (rv.w)\n\t\t\tctx->fcr31 |= cond;\n\t\telse\n\t\t\tctx->fcr31 &= ~cond;\n\t\tbreak;\n\t}\n\tcase d_fmt:\n\t\tDPTOREG(rv.d, MIPSInst_FD(ir));\n\t\tbreak;\n\tcase s_fmt:\n\t\tSPTOREG(rv.s, MIPSInst_FD(ir));\n\t\tbreak;\n\tcase w_fmt:\n\t\tSITOREG(rv.w, MIPSInst_FD(ir));\n\t\tbreak;\n#if defined(__mips64)\n\tcase l_fmt:\n\t\tDITOREG(rv.l, MIPSInst_FD(ir));\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn SIGILL;\n\t}\n\n\treturn 0;\n}\n\nint fpu_emulator_cop1Handler(struct pt_regs *xcp, struct mips_fpu_struct *ctx,\n\tint has_fpu, void *__user *fault_addr)\n{\n\tunsigned long oldepc, prevepc;\n\tmips_instruction insn;\n\tint sig = 0;\n\n\toldepc = xcp->cp0_epc;\n\tdo {\n\t\tprevepc = xcp->cp0_epc;\n\n\t\tif (!access_ok(VERIFY_READ, xcp->cp0_epc, sizeof(mips_instruction))) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = (mips_instruction __user *)xcp->cp0_epc;\n\t\t\treturn SIGBUS;\n\t\t}\n\t\tif (__get_user(insn, (mips_instruction __user *) xcp->cp0_epc)) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = (mips_instruction __user *)xcp->cp0_epc;\n\t\t\treturn SIGSEGV;\n\t\t}\n\t\tif (insn == 0)\n\t\t\txcp->cp0_epc += 4;\t/* skip nops */\n\t\telse {\n\t\t\t/*\n\t\t\t * The 'ieee754_csr' is an alias of\n\t\t\t * ctx->fcr31.  No need to copy ctx->fcr31 to\n\t\t\t * ieee754_csr.  But ieee754_csr.rm is ieee\n\t\t\t * library modes. (not mips rounding mode)\n\t\t\t */\n\t\t\t/* convert to ieee library modes */\n\t\t\tieee754_csr.rm = ieee_rm[ieee754_csr.rm];\n\t\t\tsig = cop1Emulate(xcp, ctx, fault_addr);\n\t\t\t/* revert to mips rounding mode */\n\t\t\tieee754_csr.rm = mips_rm[ieee754_csr.rm];\n\t\t}\n\n\t\tif (has_fpu)\n\t\t\tbreak;\n\t\tif (sig)\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t} while (xcp->cp0_epc > prevepc);\n\n\t/* SIGILL indicates a non-fpu instruction */\n\tif (sig == SIGILL && xcp->cp0_epc != oldepc)\n\t\t/* but if epc has advanced, then ignore it */\n\t\tsig = 0;\n\n\treturn sig;\n}\n\n#ifdef CONFIG_DEBUG_FS\n\nstatic int fpuemu_stat_get(void *data, u64 *val)\n{\n\tint cpu;\n\tunsigned long sum = 0;\n\tfor_each_online_cpu(cpu) {\n\t\tstruct mips_fpu_emulator_stats *ps;\n\t\tlocal_t *pv;\n\t\tps = &per_cpu(fpuemustats, cpu);\n\t\tpv = (void *)ps + (unsigned long)data;\n\t\tsum += local_read(pv);\n\t}\n\t*val = sum;\n\treturn 0;\n}\nDEFINE_SIMPLE_ATTRIBUTE(fops_fpuemu_stat, fpuemu_stat_get, NULL, \"%llu\\n\");\n\nextern struct dentry *mips_debugfs_dir;\nstatic int __init debugfs_fpuemu(void)\n{\n\tstruct dentry *d, *dir;\n\n\tif (!mips_debugfs_dir)\n\t\treturn -ENODEV;\n\tdir = debugfs_create_dir(\"fpuemustats\", mips_debugfs_dir);\n\tif (!dir)\n\t\treturn -ENOMEM;\n\n#define FPU_STAT_CREATE(M)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\td = debugfs_create_file(#M , S_IRUGO, dir,\t\t\\\n\t\t\t(void *)offsetof(struct mips_fpu_emulator_stats, M), \\\n\t\t\t&fops_fpuemu_stat);\t\t\t\t\\\n\t\tif (!d)\t\t\t\t\t\t\t\\\n\t\t\treturn -ENOMEM;\t\t\t\t\t\\\n\t} while (0)\n\n\tFPU_STAT_CREATE(emulated);\n\tFPU_STAT_CREATE(loads);\n\tFPU_STAT_CREATE(stores);\n\tFPU_STAT_CREATE(cp1ops);\n\tFPU_STAT_CREATE(cp1xops);\n\tFPU_STAT_CREATE(errors);\n\n\treturn 0;\n}\n__initcall(debugfs_fpuemu);\n#endif\n", "/*\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n *\n * Copyright (C) 1995 - 2000 by Ralf Baechle\n */\n#include <linux/signal.h>\n#include <linux/sched.h>\n#include <linux/interrupt.h>\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/ptrace.h>\n#include <linux/mman.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/module.h>\n#include <linux/kprobes.h>\n#include <linux/perf_event.h>\n\n#include <asm/branch.h>\n#include <asm/mmu_context.h>\n#include <asm/system.h>\n#include <asm/uaccess.h>\n#include <asm/ptrace.h>\n#include <asm/highmem.h>\t\t/* For VMALLOC_END */\n#include <linux/kdebug.h>\n\n/*\n * This routine handles page faults.  It determines the address,\n * and the problem, and then passes it off to one of the appropriate\n * routines.\n */\nasmlinkage void __kprobes do_page_fault(struct pt_regs *regs, unsigned long write,\n\t\t\t      unsigned long address)\n{\n\tstruct vm_area_struct * vma = NULL;\n\tstruct task_struct *tsk = current;\n\tstruct mm_struct *mm = tsk->mm;\n\tconst int field = sizeof(unsigned long) * 2;\n\tsiginfo_t info;\n\tint fault;\n\n#if 0\n\tprintk(\"Cpu%d[%s:%d:%0*lx:%ld:%0*lx]\\n\", raw_smp_processor_id(),\n\t       current->comm, current->pid, field, address, write,\n\t       field, regs->cp0_epc);\n#endif\n\n#ifdef CONFIG_KPROBES\n\t/*\n\t * This is to notify the fault handler of the kprobes.  The\n\t * exception code is redundant as it is also carried in REGS,\n\t * but we pass it anyhow.\n\t */\n\tif (notify_die(DIE_PAGE_FAULT, \"page fault\", regs, -1,\n\t\t       (regs->cp0_cause >> 2) & 0x1f, SIGSEGV) == NOTIFY_STOP)\n\t\treturn;\n#endif\n\n\tinfo.si_code = SEGV_MAPERR;\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t */\n#ifdef CONFIG_64BIT\n# define VMALLOC_FAULT_TARGET no_context\n#else\n# define VMALLOC_FAULT_TARGET vmalloc_fault\n#endif\n\n\tif (unlikely(address >= VMALLOC_START && address <= VMALLOC_END))\n\t\tgoto VMALLOC_FAULT_TARGET;\n#ifdef MODULE_START\n\tif (unlikely(address >= MODULE_START && address < MODULE_END))\n\t\tgoto VMALLOC_FAULT_TARGET;\n#endif\n\n\t/*\n\t * If we're in an interrupt or have no user\n\t * context, we must not take the fault..\n\t */\n\tif (in_atomic() || !mm)\n\t\tgoto bad_area_nosemaphore;\n\n\tdown_read(&mm->mmap_sem);\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n/*\n * Ok, we have a good vm_area for this memory access, so\n * we can handle it..\n */\ngood_area:\n\tinfo.si_code = SEGV_ACCERR;\n\n\tif (write) {\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t} else {\n\t\tif (kernel_uses_smartmips_rixi) {\n\t\t\tif (address == regs->cp0_epc && !(vma->vm_flags & VM_EXEC)) {\n#if 0\n\t\t\t\tpr_notice(\"Cpu%d[%s:%d:%0*lx:%ld:%0*lx] XI violation\\n\",\n\t\t\t\t\t  raw_smp_processor_id(),\n\t\t\t\t\t  current->comm, current->pid,\n\t\t\t\t\t  field, address, write,\n\t\t\t\t\t  field, regs->cp0_epc);\n#endif\n\t\t\t\tgoto bad_area;\n\t\t\t}\n\t\t\tif (!(vma->vm_flags & VM_READ)) {\n#if 0\n\t\t\t\tpr_notice(\"Cpu%d[%s:%d:%0*lx:%ld:%0*lx] RI violation\\n\",\n\t\t\t\t\t  raw_smp_processor_id(),\n\t\t\t\t\t  current->comm, current->pid,\n\t\t\t\t\t  field, address, write,\n\t\t\t\t\t  field, regs->cp0_epc);\n#endif\n\t\t\t\tgoto bad_area;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!(vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC)))\n\t\t\t\tgoto bad_area;\n\t\t}\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, write ? FAULT_FLAG_WRITE : 0);\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR) {\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ,\n\t\t\t\t1, 0, regs, address);\n\t\ttsk->maj_flt++;\n\t} else {\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN,\n\t\t\t\t1, 0, regs, address);\n\t\ttsk->min_flt++;\n\t}\n\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n/*\n * Something tried to access memory that isn't in our memory map..\n * Fix it, but check if it's kernel or user first..\n */\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (user_mode(regs)) {\n\t\ttsk->thread.cp0_badvaddr = address;\n\t\ttsk->thread.error_code = write;\n#if 0\n\t\tprintk(\"do_page_fault() #2: sending SIGSEGV to %s for \"\n\t\t       \"invalid %s\\n%0*lx (epc == %0*lx, ra == %0*lx)\\n\",\n\t\t       tsk->comm,\n\t\t       write ? \"write access to\" : \"read access from\",\n\t\t       field, address,\n\t\t       field, (unsigned long) regs->cp0_epc,\n\t\t       field, (unsigned long) regs->regs[31]);\n#endif\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\t/* info.si_code has been set above */\n\t\tinfo.si_addr = (void __user *) address;\n\t\tforce_sig_info(SIGSEGV, &info, tsk);\n\t\treturn;\n\t}\n\nno_context:\n\t/* Are we prepared to handle this kernel fault?  */\n\tif (fixup_exception(regs)) {\n\t\tcurrent->thread.cp0_baduaddr = address;\n\t\treturn;\n\t}\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice.\n\t */\n\tbust_spinlocks(1);\n\n\tprintk(KERN_ALERT \"CPU %d Unable to handle kernel paging request at \"\n\t       \"virtual address %0*lx, epc == %0*lx, ra == %0*lx\\n\",\n\t       raw_smp_processor_id(), field, address, field, regs->cp0_epc,\n\t       field,  regs->regs[31]);\n\tdie(\"Oops\", regs);\n\nout_of_memory:\n\t/*\n\t * We ran out of memory, call the OOM killer, and return the userspace\n\t * (which will retry the fault, or kill us if we got oom-killed).\n\t */\n\tup_read(&mm->mmap_sem);\n\tpagefault_out_of_memory();\n\treturn;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (!user_mode(regs))\n\t\tgoto no_context;\n\telse\n\t/*\n\t * Send a sigbus, regardless of whether we were in kernel\n\t * or user mode.\n\t */\n#if 0\n\t\tprintk(\"do_page_fault() #3: sending SIGBUS to %s for \"\n\t\t       \"invalid %s\\n%0*lx (epc == %0*lx, ra == %0*lx)\\n\",\n\t\t       tsk->comm,\n\t\t       write ? \"write access to\" : \"read access from\",\n\t\t       field, address,\n\t\t       field, (unsigned long) regs->cp0_epc,\n\t\t       field, (unsigned long) regs->regs[31]);\n#endif\n\ttsk->thread.cp0_badvaddr = address;\n\tinfo.si_signo = SIGBUS;\n\tinfo.si_errno = 0;\n\tinfo.si_code = BUS_ADRERR;\n\tinfo.si_addr = (void __user *) address;\n\tforce_sig_info(SIGBUS, &info, tsk);\n\n\treturn;\n#ifndef CONFIG_64BIT\nvmalloc_fault:\n\t{\n\t\t/*\n\t\t * Synchronize this task's top level page-table\n\t\t * with the 'reference' page table.\n\t\t *\n\t\t * Do _not_ use \"tsk\" here. We might be inside\n\t\t * an interrupt in the middle of a task switch..\n\t\t */\n\t\tint offset = __pgd_offset(address);\n\t\tpgd_t *pgd, *pgd_k;\n\t\tpud_t *pud, *pud_k;\n\t\tpmd_t *pmd, *pmd_k;\n\t\tpte_t *pte_k;\n\n\t\tpgd = (pgd_t *) pgd_current[raw_smp_processor_id()] + offset;\n\t\tpgd_k = init_mm.pgd + offset;\n\n\t\tif (!pgd_present(*pgd_k))\n\t\t\tgoto no_context;\n\t\tset_pgd(pgd, *pgd_k);\n\n\t\tpud = pud_offset(pgd, address);\n\t\tpud_k = pud_offset(pgd_k, address);\n\t\tif (!pud_present(*pud_k))\n\t\t\tgoto no_context;\n\n\t\tpmd = pmd_offset(pud, address);\n\t\tpmd_k = pmd_offset(pud_k, address);\n\t\tif (!pmd_present(*pmd_k))\n\t\t\tgoto no_context;\n\t\tset_pmd(pmd, *pmd_k);\n\n\t\tpte_k = pte_offset_kernel(pmd_k, address);\n\t\tif (!pte_present(*pte_k))\n\t\t\tgoto no_context;\n\t\treturn;\n\t}\n#endif\n}\n", "/*\n *  Copyright 2007 Sony Corporation\n *\n *  This program is free software; you can redistribute it and/or modify\n *  it under the terms of the GNU General Public License as published by\n *  the Free Software Foundation; version 2 of the License.\n *\n *  This program is distributed in the hope that it will be useful,\n *  but WITHOUT ANY WARRANTY; without even the implied warranty of\n *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n *  GNU General Public License for more details.\n *\n *  You should have received a copy of the GNU General Public License\n *  along with this program.\n *  If not, see <http://www.gnu.org/licenses/>.\n */\n\n#ifndef _ASM_POWERPC_EMULATED_OPS_H\n#define _ASM_POWERPC_EMULATED_OPS_H\n\n#include <asm/atomic.h>\n#include <linux/perf_event.h>\n\n\n#ifdef CONFIG_PPC_EMULATED_STATS\n\nstruct ppc_emulated_entry {\n\tconst char *name;\n\tatomic_t val;\n};\n\nextern struct ppc_emulated {\n#ifdef CONFIG_ALTIVEC\n\tstruct ppc_emulated_entry altivec;\n#endif\n\tstruct ppc_emulated_entry dcba;\n\tstruct ppc_emulated_entry dcbz;\n\tstruct ppc_emulated_entry fp_pair;\n\tstruct ppc_emulated_entry isel;\n\tstruct ppc_emulated_entry mcrxr;\n\tstruct ppc_emulated_entry mfpvr;\n\tstruct ppc_emulated_entry multiple;\n\tstruct ppc_emulated_entry popcntb;\n\tstruct ppc_emulated_entry spe;\n\tstruct ppc_emulated_entry string;\n\tstruct ppc_emulated_entry unaligned;\n#ifdef CONFIG_MATH_EMULATION\n\tstruct ppc_emulated_entry math;\n#elif defined(CONFIG_8XX_MINIMAL_FPEMU)\n\tstruct ppc_emulated_entry 8xx;\n#endif\n#ifdef CONFIG_VSX\n\tstruct ppc_emulated_entry vsx;\n#endif\n#ifdef CONFIG_PPC64\n\tstruct ppc_emulated_entry mfdscr;\n\tstruct ppc_emulated_entry mtdscr;\n#endif\n} ppc_emulated;\n\nextern u32 ppc_warn_emulated;\n\nextern void ppc_warn_emulated_print(const char *type);\n\n#define __PPC_WARN_EMULATED(type)\t\t\t\t\t \\\n\tdo {\t\t\t\t\t\t\t\t \\\n\t\tatomic_inc(&ppc_emulated.type.val);\t\t\t \\\n\t\tif (ppc_warn_emulated)\t\t\t\t\t \\\n\t\t\tppc_warn_emulated_print(ppc_emulated.type.name); \\\n\t} while (0)\n\n#else /* !CONFIG_PPC_EMULATED_STATS */\n\n#define __PPC_WARN_EMULATED(type)\tdo { } while (0)\n\n#endif /* !CONFIG_PPC_EMULATED_STATS */\n\n#define PPC_WARN_EMULATED(type, regs)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,\t\t\\\n\t\t\t1, 0, regs, 0);\t\t\t\t\t\\\n\t\t__PPC_WARN_EMULATED(type);\t\t\t\t\\\n\t} while (0)\n\n#define PPC_WARN_ALIGNMENT(type, regs)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS,\t\t\\\n\t\t\t1, 0, regs, regs->dar);\t\t\t\t\\\n\t\t__PPC_WARN_EMULATED(type);\t\t\t\t\\\n\t} while (0)\n\n#endif /* _ASM_POWERPC_EMULATED_OPS_H */\n", "/*\n * Performance event support - powerpc architecture code\n *\n * Copyright 2008-2009 Paul Mackerras, IBM Corporation.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public License\n * as published by the Free Software Foundation; either version\n * 2 of the License, or (at your option) any later version.\n */\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/perf_event.h>\n#include <linux/percpu.h>\n#include <linux/hardirq.h>\n#include <asm/reg.h>\n#include <asm/pmc.h>\n#include <asm/machdep.h>\n#include <asm/firmware.h>\n#include <asm/ptrace.h>\n\nstruct cpu_hw_events {\n\tint n_events;\n\tint n_percpu;\n\tint disabled;\n\tint n_added;\n\tint n_limited;\n\tu8  pmcs_enabled;\n\tstruct perf_event *event[MAX_HWEVENTS];\n\tu64 events[MAX_HWEVENTS];\n\tunsigned int flags[MAX_HWEVENTS];\n\tunsigned long mmcr[3];\n\tstruct perf_event *limited_counter[MAX_LIMITED_HWCOUNTERS];\n\tu8  limited_hwidx[MAX_LIMITED_HWCOUNTERS];\n\tu64 alternatives[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];\n\tunsigned long amasks[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];\n\tunsigned long avalues[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];\n\n\tunsigned int group_flag;\n\tint n_txn_start;\n};\nDEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events);\n\nstruct power_pmu *ppmu;\n\n/*\n * Normally, to ignore kernel events we set the FCS (freeze counters\n * in supervisor mode) bit in MMCR0, but if the kernel runs with the\n * hypervisor bit set in the MSR, or if we are running on a processor\n * where the hypervisor bit is forced to 1 (as on Apple G5 processors),\n * then we need to use the FCHV bit to ignore kernel events.\n */\nstatic unsigned int freeze_events_kernel = MMCR0_FCS;\n\n/*\n * 32-bit doesn't have MMCRA but does have an MMCR2,\n * and a few other names are different.\n */\n#ifdef CONFIG_PPC32\n\n#define MMCR0_FCHV\t\t0\n#define MMCR0_PMCjCE\t\tMMCR0_PMCnCE\n\n#define SPRN_MMCRA\t\tSPRN_MMCR2\n#define MMCRA_SAMPLE_ENABLE\t0\n\nstatic inline unsigned long perf_ip_adjust(struct pt_regs *regs)\n{\n\treturn 0;\n}\nstatic inline void perf_get_data_addr(struct pt_regs *regs, u64 *addrp) { }\nstatic inline u32 perf_get_misc_flags(struct pt_regs *regs)\n{\n\treturn 0;\n}\nstatic inline void perf_read_regs(struct pt_regs *regs) { }\nstatic inline int perf_intr_is_nmi(struct pt_regs *regs)\n{\n\treturn 0;\n}\n\n#endif /* CONFIG_PPC32 */\n\n/*\n * Things that are specific to 64-bit implementations.\n */\n#ifdef CONFIG_PPC64\n\nstatic inline unsigned long perf_ip_adjust(struct pt_regs *regs)\n{\n\tunsigned long mmcra = regs->dsisr;\n\n\tif ((mmcra & MMCRA_SAMPLE_ENABLE) && !(ppmu->flags & PPMU_ALT_SIPR)) {\n\t\tunsigned long slot = (mmcra & MMCRA_SLOT) >> MMCRA_SLOT_SHIFT;\n\t\tif (slot > 1)\n\t\t\treturn 4 * (slot - 1);\n\t}\n\treturn 0;\n}\n\n/*\n * The user wants a data address recorded.\n * If we're not doing instruction sampling, give them the SDAR\n * (sampled data address).  If we are doing instruction sampling, then\n * only give them the SDAR if it corresponds to the instruction\n * pointed to by SIAR; this is indicated by the [POWER6_]MMCRA_SDSYNC\n * bit in MMCRA.\n */\nstatic inline void perf_get_data_addr(struct pt_regs *regs, u64 *addrp)\n{\n\tunsigned long mmcra = regs->dsisr;\n\tunsigned long sdsync = (ppmu->flags & PPMU_ALT_SIPR) ?\n\t\tPOWER6_MMCRA_SDSYNC : MMCRA_SDSYNC;\n\n\tif (!(mmcra & MMCRA_SAMPLE_ENABLE) || (mmcra & sdsync))\n\t\t*addrp = mfspr(SPRN_SDAR);\n}\n\nstatic inline u32 perf_get_misc_flags(struct pt_regs *regs)\n{\n\tunsigned long mmcra = regs->dsisr;\n\tunsigned long sihv = MMCRA_SIHV;\n\tunsigned long sipr = MMCRA_SIPR;\n\n\tif (TRAP(regs) != 0xf00)\n\t\treturn 0;\t/* not a PMU interrupt */\n\n\tif (ppmu->flags & PPMU_ALT_SIPR) {\n\t\tsihv = POWER6_MMCRA_SIHV;\n\t\tsipr = POWER6_MMCRA_SIPR;\n\t}\n\n\t/* PR has priority over HV, so order below is important */\n\tif (mmcra & sipr)\n\t\treturn PERF_RECORD_MISC_USER;\n\tif ((mmcra & sihv) && (freeze_events_kernel != MMCR0_FCHV))\n\t\treturn PERF_RECORD_MISC_HYPERVISOR;\n\treturn PERF_RECORD_MISC_KERNEL;\n}\n\n/*\n * Overload regs->dsisr to store MMCRA so we only need to read it once\n * on each interrupt.\n */\nstatic inline void perf_read_regs(struct pt_regs *regs)\n{\n\tregs->dsisr = mfspr(SPRN_MMCRA);\n}\n\n/*\n * If interrupts were soft-disabled when a PMU interrupt occurs, treat\n * it as an NMI.\n */\nstatic inline int perf_intr_is_nmi(struct pt_regs *regs)\n{\n\treturn !regs->softe;\n}\n\n#endif /* CONFIG_PPC64 */\n\nstatic void perf_event_interrupt(struct pt_regs *regs);\n\nvoid perf_event_print_debug(void)\n{\n}\n\n/*\n * Read one performance monitor counter (PMC).\n */\nstatic unsigned long read_pmc(int idx)\n{\n\tunsigned long val;\n\n\tswitch (idx) {\n\tcase 1:\n\t\tval = mfspr(SPRN_PMC1);\n\t\tbreak;\n\tcase 2:\n\t\tval = mfspr(SPRN_PMC2);\n\t\tbreak;\n\tcase 3:\n\t\tval = mfspr(SPRN_PMC3);\n\t\tbreak;\n\tcase 4:\n\t\tval = mfspr(SPRN_PMC4);\n\t\tbreak;\n\tcase 5:\n\t\tval = mfspr(SPRN_PMC5);\n\t\tbreak;\n\tcase 6:\n\t\tval = mfspr(SPRN_PMC6);\n\t\tbreak;\n#ifdef CONFIG_PPC64\n\tcase 7:\n\t\tval = mfspr(SPRN_PMC7);\n\t\tbreak;\n\tcase 8:\n\t\tval = mfspr(SPRN_PMC8);\n\t\tbreak;\n#endif /* CONFIG_PPC64 */\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to read PMC%d\\n\", idx);\n\t\tval = 0;\n\t}\n\treturn val;\n}\n\n/*\n * Write one PMC.\n */\nstatic void write_pmc(int idx, unsigned long val)\n{\n\tswitch (idx) {\n\tcase 1:\n\t\tmtspr(SPRN_PMC1, val);\n\t\tbreak;\n\tcase 2:\n\t\tmtspr(SPRN_PMC2, val);\n\t\tbreak;\n\tcase 3:\n\t\tmtspr(SPRN_PMC3, val);\n\t\tbreak;\n\tcase 4:\n\t\tmtspr(SPRN_PMC4, val);\n\t\tbreak;\n\tcase 5:\n\t\tmtspr(SPRN_PMC5, val);\n\t\tbreak;\n\tcase 6:\n\t\tmtspr(SPRN_PMC6, val);\n\t\tbreak;\n#ifdef CONFIG_PPC64\n\tcase 7:\n\t\tmtspr(SPRN_PMC7, val);\n\t\tbreak;\n\tcase 8:\n\t\tmtspr(SPRN_PMC8, val);\n\t\tbreak;\n#endif /* CONFIG_PPC64 */\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to write PMC%d\\n\", idx);\n\t}\n}\n\n/*\n * Check if a set of events can all go on the PMU at once.\n * If they can't, this will look at alternative codes for the events\n * and see if any combination of alternative codes is feasible.\n * The feasible set is returned in event_id[].\n */\nstatic int power_check_constraints(struct cpu_hw_events *cpuhw,\n\t\t\t\t   u64 event_id[], unsigned int cflags[],\n\t\t\t\t   int n_ev)\n{\n\tunsigned long mask, value, nv;\n\tunsigned long smasks[MAX_HWEVENTS], svalues[MAX_HWEVENTS];\n\tint n_alt[MAX_HWEVENTS], choice[MAX_HWEVENTS];\n\tint i, j;\n\tunsigned long addf = ppmu->add_fields;\n\tunsigned long tadd = ppmu->test_adder;\n\n\tif (n_ev > ppmu->n_counter)\n\t\treturn -1;\n\n\t/* First see if the events will go on as-is */\n\tfor (i = 0; i < n_ev; ++i) {\n\t\tif ((cflags[i] & PPMU_LIMITED_PMC_REQD)\n\t\t    && !ppmu->limited_pmc_event(event_id[i])) {\n\t\t\tppmu->get_alternatives(event_id[i], cflags[i],\n\t\t\t\t\t       cpuhw->alternatives[i]);\n\t\t\tevent_id[i] = cpuhw->alternatives[i][0];\n\t\t}\n\t\tif (ppmu->get_constraint(event_id[i], &cpuhw->amasks[i][0],\n\t\t\t\t\t &cpuhw->avalues[i][0]))\n\t\t\treturn -1;\n\t}\n\tvalue = mask = 0;\n\tfor (i = 0; i < n_ev; ++i) {\n\t\tnv = (value | cpuhw->avalues[i][0]) +\n\t\t\t(value & cpuhw->avalues[i][0] & addf);\n\t\tif ((((nv + tadd) ^ value) & mask) != 0 ||\n\t\t    (((nv + tadd) ^ cpuhw->avalues[i][0]) &\n\t\t     cpuhw->amasks[i][0]) != 0)\n\t\t\tbreak;\n\t\tvalue = nv;\n\t\tmask |= cpuhw->amasks[i][0];\n\t}\n\tif (i == n_ev)\n\t\treturn 0;\t/* all OK */\n\n\t/* doesn't work, gather alternatives... */\n\tif (!ppmu->get_alternatives)\n\t\treturn -1;\n\tfor (i = 0; i < n_ev; ++i) {\n\t\tchoice[i] = 0;\n\t\tn_alt[i] = ppmu->get_alternatives(event_id[i], cflags[i],\n\t\t\t\t\t\t  cpuhw->alternatives[i]);\n\t\tfor (j = 1; j < n_alt[i]; ++j)\n\t\t\tppmu->get_constraint(cpuhw->alternatives[i][j],\n\t\t\t\t\t     &cpuhw->amasks[i][j],\n\t\t\t\t\t     &cpuhw->avalues[i][j]);\n\t}\n\n\t/* enumerate all possibilities and see if any will work */\n\ti = 0;\n\tj = -1;\n\tvalue = mask = nv = 0;\n\twhile (i < n_ev) {\n\t\tif (j >= 0) {\n\t\t\t/* we're backtracking, restore context */\n\t\t\tvalue = svalues[i];\n\t\t\tmask = smasks[i];\n\t\t\tj = choice[i];\n\t\t}\n\t\t/*\n\t\t * See if any alternative k for event_id i,\n\t\t * where k > j, will satisfy the constraints.\n\t\t */\n\t\twhile (++j < n_alt[i]) {\n\t\t\tnv = (value | cpuhw->avalues[i][j]) +\n\t\t\t\t(value & cpuhw->avalues[i][j] & addf);\n\t\t\tif ((((nv + tadd) ^ value) & mask) == 0 &&\n\t\t\t    (((nv + tadd) ^ cpuhw->avalues[i][j])\n\t\t\t     & cpuhw->amasks[i][j]) == 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (j >= n_alt[i]) {\n\t\t\t/*\n\t\t\t * No feasible alternative, backtrack\n\t\t\t * to event_id i-1 and continue enumerating its\n\t\t\t * alternatives from where we got up to.\n\t\t\t */\n\t\t\tif (--i < 0)\n\t\t\t\treturn -1;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Found a feasible alternative for event_id i,\n\t\t\t * remember where we got up to with this event_id,\n\t\t\t * go on to the next event_id, and start with\n\t\t\t * the first alternative for it.\n\t\t\t */\n\t\t\tchoice[i] = j;\n\t\t\tsvalues[i] = value;\n\t\t\tsmasks[i] = mask;\n\t\t\tvalue = nv;\n\t\t\tmask |= cpuhw->amasks[i][j];\n\t\t\t++i;\n\t\t\tj = -1;\n\t\t}\n\t}\n\n\t/* OK, we have a feasible combination, tell the caller the solution */\n\tfor (i = 0; i < n_ev; ++i)\n\t\tevent_id[i] = cpuhw->alternatives[i][choice[i]];\n\treturn 0;\n}\n\n/*\n * Check if newly-added events have consistent settings for\n * exclude_{user,kernel,hv} with each other and any previously\n * added events.\n */\nstatic int check_excludes(struct perf_event **ctrs, unsigned int cflags[],\n\t\t\t  int n_prev, int n_new)\n{\n\tint eu = 0, ek = 0, eh = 0;\n\tint i, n, first;\n\tstruct perf_event *event;\n\n\tn = n_prev + n_new;\n\tif (n <= 1)\n\t\treturn 0;\n\n\tfirst = 1;\n\tfor (i = 0; i < n; ++i) {\n\t\tif (cflags[i] & PPMU_LIMITED_PMC_OK) {\n\t\t\tcflags[i] &= ~PPMU_LIMITED_PMC_REQD;\n\t\t\tcontinue;\n\t\t}\n\t\tevent = ctrs[i];\n\t\tif (first) {\n\t\t\teu = event->attr.exclude_user;\n\t\t\tek = event->attr.exclude_kernel;\n\t\t\teh = event->attr.exclude_hv;\n\t\t\tfirst = 0;\n\t\t} else if (event->attr.exclude_user != eu ||\n\t\t\t   event->attr.exclude_kernel != ek ||\n\t\t\t   event->attr.exclude_hv != eh) {\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\tif (eu || ek || eh)\n\t\tfor (i = 0; i < n; ++i)\n\t\t\tif (cflags[i] & PPMU_LIMITED_PMC_OK)\n\t\t\t\tcflags[i] |= PPMU_LIMITED_PMC_REQD;\n\n\treturn 0;\n}\n\nstatic u64 check_and_compute_delta(u64 prev, u64 val)\n{\n\tu64 delta = (val - prev) & 0xfffffffful;\n\n\t/*\n\t * POWER7 can roll back counter values, if the new value is smaller\n\t * than the previous value it will cause the delta and the counter to\n\t * have bogus values unless we rolled a counter over.  If a coutner is\n\t * rolled back, it will be smaller, but within 256, which is the maximum\n\t * number of events to rollback at once.  If we dectect a rollback\n\t * return 0.  This can lead to a small lack of precision in the\n\t * counters.\n\t */\n\tif (prev > val && (prev - val) < 256)\n\t\tdelta = 0;\n\n\treturn delta;\n}\n\nstatic void power_pmu_read(struct perf_event *event)\n{\n\ts64 val, delta, prev;\n\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn;\n\n\tif (!event->hw.idx)\n\t\treturn;\n\t/*\n\t * Performance monitor interrupts come even when interrupts\n\t * are soft-disabled, as long as interrupts are hard-enabled.\n\t * Therefore we treat them like NMIs.\n\t */\n\tdo {\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tbarrier();\n\t\tval = read_pmc(event->hw.idx);\n\t\tdelta = check_and_compute_delta(prev, val);\n\t\tif (!delta)\n\t\t\treturn;\n\t} while (local64_cmpxchg(&event->hw.prev_count, prev, val) != prev);\n\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &event->hw.period_left);\n}\n\n/*\n * On some machines, PMC5 and PMC6 can't be written, don't respect\n * the freeze conditions, and don't generate interrupts.  This tells\n * us if `event' is using such a PMC.\n */\nstatic int is_limited_pmc(int pmcnum)\n{\n\treturn (ppmu->flags & PPMU_LIMITED_PMC5_6)\n\t\t&& (pmcnum == 5 || pmcnum == 6);\n}\n\nstatic void freeze_limited_counters(struct cpu_hw_events *cpuhw,\n\t\t\t\t    unsigned long pmc5, unsigned long pmc6)\n{\n\tstruct perf_event *event;\n\tu64 val, prev, delta;\n\tint i;\n\n\tfor (i = 0; i < cpuhw->n_limited; ++i) {\n\t\tevent = cpuhw->limited_counter[i];\n\t\tif (!event->hw.idx)\n\t\t\tcontinue;\n\t\tval = (event->hw.idx == 5) ? pmc5 : pmc6;\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tevent->hw.idx = 0;\n\t\tdelta = check_and_compute_delta(prev, val);\n\t\tif (delta)\n\t\t\tlocal64_add(delta, &event->count);\n\t}\n}\n\nstatic void thaw_limited_counters(struct cpu_hw_events *cpuhw,\n\t\t\t\t  unsigned long pmc5, unsigned long pmc6)\n{\n\tstruct perf_event *event;\n\tu64 val, prev;\n\tint i;\n\n\tfor (i = 0; i < cpuhw->n_limited; ++i) {\n\t\tevent = cpuhw->limited_counter[i];\n\t\tevent->hw.idx = cpuhw->limited_hwidx[i];\n\t\tval = (event->hw.idx == 5) ? pmc5 : pmc6;\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tif (check_and_compute_delta(prev, val))\n\t\t\tlocal64_set(&event->hw.prev_count, val);\n\t\tperf_event_update_userpage(event);\n\t}\n}\n\n/*\n * Since limited events don't respect the freeze conditions, we\n * have to read them immediately after freezing or unfreezing the\n * other events.  We try to keep the values from the limited\n * events as consistent as possible by keeping the delay (in\n * cycles and instructions) between freezing/unfreezing and reading\n * the limited events as small and consistent as possible.\n * Therefore, if any limited events are in use, we read them\n * both, and always in the same order, to minimize variability,\n * and do it inside the same asm that writes MMCR0.\n */\nstatic void write_mmcr0(struct cpu_hw_events *cpuhw, unsigned long mmcr0)\n{\n\tunsigned long pmc5, pmc6;\n\n\tif (!cpuhw->n_limited) {\n\t\tmtspr(SPRN_MMCR0, mmcr0);\n\t\treturn;\n\t}\n\n\t/*\n\t * Write MMCR0, then read PMC5 and PMC6 immediately.\n\t * To ensure we don't get a performance monitor interrupt\n\t * between writing MMCR0 and freezing/thawing the limited\n\t * events, we first write MMCR0 with the event overflow\n\t * interrupt enable bits turned off.\n\t */\n\tasm volatile(\"mtspr %3,%2; mfspr %0,%4; mfspr %1,%5\"\n\t\t     : \"=&r\" (pmc5), \"=&r\" (pmc6)\n\t\t     : \"r\" (mmcr0 & ~(MMCR0_PMC1CE | MMCR0_PMCjCE)),\n\t\t       \"i\" (SPRN_MMCR0),\n\t\t       \"i\" (SPRN_PMC5), \"i\" (SPRN_PMC6));\n\n\tif (mmcr0 & MMCR0_FC)\n\t\tfreeze_limited_counters(cpuhw, pmc5, pmc6);\n\telse\n\t\tthaw_limited_counters(cpuhw, pmc5, pmc6);\n\n\t/*\n\t * Write the full MMCR0 including the event overflow interrupt\n\t * enable bits, if necessary.\n\t */\n\tif (mmcr0 & (MMCR0_PMC1CE | MMCR0_PMCjCE))\n\t\tmtspr(SPRN_MMCR0, mmcr0);\n}\n\n/*\n * Disable all events to prevent PMU interrupts and to allow\n * events to be added or removed.\n */\nstatic void power_pmu_disable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\n\tif (!ppmu)\n\t\treturn;\n\tlocal_irq_save(flags);\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\n\tif (!cpuhw->disabled) {\n\t\tcpuhw->disabled = 1;\n\t\tcpuhw->n_added = 0;\n\n\t\t/*\n\t\t * Check if we ever enabled the PMU on this cpu.\n\t\t */\n\t\tif (!cpuhw->pmcs_enabled) {\n\t\t\tppc_enable_pmcs();\n\t\t\tcpuhw->pmcs_enabled = 1;\n\t\t}\n\n\t\t/*\n\t\t * Disable instruction sampling if it was enabled\n\t\t */\n\t\tif (cpuhw->mmcr[2] & MMCRA_SAMPLE_ENABLE) {\n\t\t\tmtspr(SPRN_MMCRA,\n\t\t\t      cpuhw->mmcr[2] & ~MMCRA_SAMPLE_ENABLE);\n\t\t\tmb();\n\t\t}\n\n\t\t/*\n\t\t * Set the 'freeze counters' bit.\n\t\t * The barrier is to make sure the mtspr has been\n\t\t * executed and the PMU has frozen the events\n\t\t * before we return.\n\t\t */\n\t\twrite_mmcr0(cpuhw, mfspr(SPRN_MMCR0) | MMCR0_FC);\n\t\tmb();\n\t}\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Re-enable all events if disable == 0.\n * If we were previously disabled and events were added, then\n * put the new config on the PMU.\n */\nstatic void power_pmu_enable(struct pmu *pmu)\n{\n\tstruct perf_event *event;\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\tlong i;\n\tunsigned long val;\n\ts64 left;\n\tunsigned int hwc_index[MAX_HWEVENTS];\n\tint n_lim;\n\tint idx;\n\n\tif (!ppmu)\n\t\treturn;\n\tlocal_irq_save(flags);\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\tif (!cpuhw->disabled) {\n\t\tlocal_irq_restore(flags);\n\t\treturn;\n\t}\n\tcpuhw->disabled = 0;\n\n\t/*\n\t * If we didn't change anything, or only removed events,\n\t * no need to recalculate MMCR* settings and reset the PMCs.\n\t * Just reenable the PMU with the current MMCR* settings\n\t * (possibly updated for removal of events).\n\t */\n\tif (!cpuhw->n_added) {\n\t\tmtspr(SPRN_MMCRA, cpuhw->mmcr[2] & ~MMCRA_SAMPLE_ENABLE);\n\t\tmtspr(SPRN_MMCR1, cpuhw->mmcr[1]);\n\t\tif (cpuhw->n_events == 0)\n\t\t\tppc_set_pmu_inuse(0);\n\t\tgoto out_enable;\n\t}\n\n\t/*\n\t * Compute MMCR* values for the new set of events\n\t */\n\tif (ppmu->compute_mmcr(cpuhw->events, cpuhw->n_events, hwc_index,\n\t\t\t       cpuhw->mmcr)) {\n\t\t/* shouldn't ever get here */\n\t\tprintk(KERN_ERR \"oops compute_mmcr failed\\n\");\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Add in MMCR0 freeze bits corresponding to the\n\t * attr.exclude_* bits for the first event.\n\t * We have already checked that all events have the\n\t * same values for these bits as the first event.\n\t */\n\tevent = cpuhw->event[0];\n\tif (event->attr.exclude_user)\n\t\tcpuhw->mmcr[0] |= MMCR0_FCP;\n\tif (event->attr.exclude_kernel)\n\t\tcpuhw->mmcr[0] |= freeze_events_kernel;\n\tif (event->attr.exclude_hv)\n\t\tcpuhw->mmcr[0] |= MMCR0_FCHV;\n\n\t/*\n\t * Write the new configuration to MMCR* with the freeze\n\t * bit set and set the hardware events to their initial values.\n\t * Then unfreeze the events.\n\t */\n\tppc_set_pmu_inuse(1);\n\tmtspr(SPRN_MMCRA, cpuhw->mmcr[2] & ~MMCRA_SAMPLE_ENABLE);\n\tmtspr(SPRN_MMCR1, cpuhw->mmcr[1]);\n\tmtspr(SPRN_MMCR0, (cpuhw->mmcr[0] & ~(MMCR0_PMC1CE | MMCR0_PMCjCE))\n\t\t\t\t| MMCR0_FC);\n\n\t/*\n\t * Read off any pre-existing events that need to move\n\t * to another PMC.\n\t */\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tevent = cpuhw->event[i];\n\t\tif (event->hw.idx && event->hw.idx != hwc_index[i] + 1) {\n\t\t\tpower_pmu_read(event);\n\t\t\twrite_pmc(event->hw.idx, 0);\n\t\t\tevent->hw.idx = 0;\n\t\t}\n\t}\n\n\t/*\n\t * Initialize the PMCs for all the new and moved events.\n\t */\n\tcpuhw->n_limited = n_lim = 0;\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tevent = cpuhw->event[i];\n\t\tif (event->hw.idx)\n\t\t\tcontinue;\n\t\tidx = hwc_index[i] + 1;\n\t\tif (is_limited_pmc(idx)) {\n\t\t\tcpuhw->limited_counter[n_lim] = event;\n\t\t\tcpuhw->limited_hwidx[n_lim] = idx;\n\t\t\t++n_lim;\n\t\t\tcontinue;\n\t\t}\n\t\tval = 0;\n\t\tif (event->hw.sample_period) {\n\t\t\tleft = local64_read(&event->hw.period_left);\n\t\t\tif (left < 0x80000000L)\n\t\t\t\tval = 0x80000000L - left;\n\t\t}\n\t\tlocal64_set(&event->hw.prev_count, val);\n\t\tevent->hw.idx = idx;\n\t\tif (event->hw.state & PERF_HES_STOPPED)\n\t\t\tval = 0;\n\t\twrite_pmc(idx, val);\n\t\tperf_event_update_userpage(event);\n\t}\n\tcpuhw->n_limited = n_lim;\n\tcpuhw->mmcr[0] |= MMCR0_PMXE | MMCR0_FCECE;\n\n out_enable:\n\tmb();\n\twrite_mmcr0(cpuhw, cpuhw->mmcr[0]);\n\n\t/*\n\t * Enable instruction sampling if necessary\n\t */\n\tif (cpuhw->mmcr[2] & MMCRA_SAMPLE_ENABLE) {\n\t\tmb();\n\t\tmtspr(SPRN_MMCRA, cpuhw->mmcr[2]);\n\t}\n\n out:\n\tlocal_irq_restore(flags);\n}\n\nstatic int collect_events(struct perf_event *group, int max_count,\n\t\t\t  struct perf_event *ctrs[], u64 *events,\n\t\t\t  unsigned int *flags)\n{\n\tint n = 0;\n\tstruct perf_event *event;\n\n\tif (!is_software_event(group)) {\n\t\tif (n >= max_count)\n\t\t\treturn -1;\n\t\tctrs[n] = group;\n\t\tflags[n] = group->hw.event_base;\n\t\tevents[n++] = group->hw.config;\n\t}\n\tlist_for_each_entry(event, &group->sibling_list, group_entry) {\n\t\tif (!is_software_event(event) &&\n\t\t    event->state != PERF_EVENT_STATE_OFF) {\n\t\t\tif (n >= max_count)\n\t\t\t\treturn -1;\n\t\t\tctrs[n] = event;\n\t\t\tflags[n] = event->hw.event_base;\n\t\t\tevents[n++] = event->hw.config;\n\t\t}\n\t}\n\treturn n;\n}\n\n/*\n * Add a event to the PMU.\n * If all events are not already frozen, then we disable and\n * re-enable the PMU in order to get hw_perf_enable to do the\n * actual work of reconfiguring the PMU.\n */\nstatic int power_pmu_add(struct perf_event *event, int ef_flags)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\tint n0;\n\tint ret = -EAGAIN;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\t/*\n\t * Add the event to the list (if there is room)\n\t * and check whether the total set is still feasible.\n\t */\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\tn0 = cpuhw->n_events;\n\tif (n0 >= ppmu->n_counter)\n\t\tgoto out;\n\tcpuhw->event[n0] = event;\n\tcpuhw->events[n0] = event->hw.config;\n\tcpuhw->flags[n0] = event->hw.event_base;\n\n\tif (!(ef_flags & PERF_EF_START))\n\t\tevent->hw.state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\n\t/*\n\t * If group events scheduling transaction was started,\n\t * skip the schedulability test here, it will be performed\n\t * at commit time(->commit_txn) as a whole\n\t */\n\tif (cpuhw->group_flag & PERF_EVENT_TXN)\n\t\tgoto nocheck;\n\n\tif (check_excludes(cpuhw->event, cpuhw->flags, n0, 1))\n\t\tgoto out;\n\tif (power_check_constraints(cpuhw, cpuhw->events, cpuhw->flags, n0 + 1))\n\t\tgoto out;\n\tevent->hw.config = cpuhw->events[n0];\n\nnocheck:\n\t++cpuhw->n_events;\n\t++cpuhw->n_added;\n\n\tret = 0;\n out:\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\n\n/*\n * Remove a event from the PMU.\n */\nstatic void power_pmu_del(struct perf_event *event, int ef_flags)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tlong i;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tpower_pmu_read(event);\n\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tif (event == cpuhw->event[i]) {\n\t\t\twhile (++i < cpuhw->n_events) {\n\t\t\t\tcpuhw->event[i-1] = cpuhw->event[i];\n\t\t\t\tcpuhw->events[i-1] = cpuhw->events[i];\n\t\t\t\tcpuhw->flags[i-1] = cpuhw->flags[i];\n\t\t\t}\n\t\t\t--cpuhw->n_events;\n\t\t\tppmu->disable_pmc(event->hw.idx - 1, cpuhw->mmcr);\n\t\t\tif (event->hw.idx) {\n\t\t\t\twrite_pmc(event->hw.idx, 0);\n\t\t\t\tevent->hw.idx = 0;\n\t\t\t}\n\t\t\tperf_event_update_userpage(event);\n\t\t\tbreak;\n\t\t}\n\t}\n\tfor (i = 0; i < cpuhw->n_limited; ++i)\n\t\tif (event == cpuhw->limited_counter[i])\n\t\t\tbreak;\n\tif (i < cpuhw->n_limited) {\n\t\twhile (++i < cpuhw->n_limited) {\n\t\t\tcpuhw->limited_counter[i-1] = cpuhw->limited_counter[i];\n\t\t\tcpuhw->limited_hwidx[i-1] = cpuhw->limited_hwidx[i];\n\t\t}\n\t\t--cpuhw->n_limited;\n\t}\n\tif (cpuhw->n_events == 0) {\n\t\t/* disable exceptions if no events are running */\n\t\tcpuhw->mmcr[0] &= ~(MMCR0_PMXE | MMCR0_FCECE);\n\t}\n\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\n/*\n * POWER-PMU does not support disabling individual counters, hence\n * program their cycle counter to their max value and ignore the interrupts.\n */\n\nstatic void power_pmu_start(struct perf_event *event, int ef_flags)\n{\n\tunsigned long flags;\n\ts64 left;\n\n\tif (!event->hw.idx || !event->hw.sample_period)\n\t\treturn;\n\n\tif (!(event->hw.state & PERF_HES_STOPPED))\n\t\treturn;\n\n\tif (ef_flags & PERF_EF_RELOAD)\n\t\tWARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tevent->hw.state = 0;\n\tleft = local64_read(&event->hw.period_left);\n\twrite_pmc(event->hw.idx, left);\n\n\tperf_event_update_userpage(event);\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\nstatic void power_pmu_stop(struct perf_event *event, int ef_flags)\n{\n\tunsigned long flags;\n\n\tif (!event->hw.idx || !event->hw.sample_period)\n\t\treturn;\n\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tpower_pmu_read(event);\n\tevent->hw.state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\twrite_pmc(event->hw.idx, 0);\n\n\tperf_event_update_userpage(event);\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Start group events scheduling transaction\n * Set the flag to make pmu::enable() not perform the\n * schedulability test, it will be performed at commit time\n */\nvoid power_pmu_start_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\n\n\tperf_pmu_disable(pmu);\n\tcpuhw->group_flag |= PERF_EVENT_TXN;\n\tcpuhw->n_txn_start = cpuhw->n_events;\n}\n\n/*\n * Stop group events scheduling transaction\n * Clear the flag and pmu::enable() will perform the\n * schedulability test.\n */\nvoid power_pmu_cancel_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\n\n\tcpuhw->group_flag &= ~PERF_EVENT_TXN;\n\tperf_pmu_enable(pmu);\n}\n\n/*\n * Commit group events scheduling transaction\n * Perform the group schedulability test as a whole\n * Return 0 if success\n */\nint power_pmu_commit_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tlong i, n;\n\n\tif (!ppmu)\n\t\treturn -EAGAIN;\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\tn = cpuhw->n_events;\n\tif (check_excludes(cpuhw->event, cpuhw->flags, 0, n))\n\t\treturn -EAGAIN;\n\ti = power_check_constraints(cpuhw, cpuhw->events, cpuhw->flags, n);\n\tif (i < 0)\n\t\treturn -EAGAIN;\n\n\tfor (i = cpuhw->n_txn_start; i < n; ++i)\n\t\tcpuhw->event[i]->hw.config = cpuhw->events[i];\n\n\tcpuhw->group_flag &= ~PERF_EVENT_TXN;\n\tperf_pmu_enable(pmu);\n\treturn 0;\n}\n\n/*\n * Return 1 if we might be able to put event on a limited PMC,\n * or 0 if not.\n * A event can only go on a limited PMC if it counts something\n * that a limited PMC can count, doesn't require interrupts, and\n * doesn't exclude any processor mode.\n */\nstatic int can_go_on_limited_pmc(struct perf_event *event, u64 ev,\n\t\t\t\t unsigned int flags)\n{\n\tint n;\n\tu64 alt[MAX_EVENT_ALTERNATIVES];\n\n\tif (event->attr.exclude_user\n\t    || event->attr.exclude_kernel\n\t    || event->attr.exclude_hv\n\t    || event->attr.sample_period)\n\t\treturn 0;\n\n\tif (ppmu->limited_pmc_event(ev))\n\t\treturn 1;\n\n\t/*\n\t * The requested event_id isn't on a limited PMC already;\n\t * see if any alternative code goes on a limited PMC.\n\t */\n\tif (!ppmu->get_alternatives)\n\t\treturn 0;\n\n\tflags |= PPMU_LIMITED_PMC_OK | PPMU_LIMITED_PMC_REQD;\n\tn = ppmu->get_alternatives(ev, flags, alt);\n\n\treturn n > 0;\n}\n\n/*\n * Find an alternative event_id that goes on a normal PMC, if possible,\n * and return the event_id code, or 0 if there is no such alternative.\n * (Note: event_id code 0 is \"don't count\" on all machines.)\n */\nstatic u64 normal_pmc_alternative(u64 ev, unsigned long flags)\n{\n\tu64 alt[MAX_EVENT_ALTERNATIVES];\n\tint n;\n\n\tflags &= ~(PPMU_LIMITED_PMC_OK | PPMU_LIMITED_PMC_REQD);\n\tn = ppmu->get_alternatives(ev, flags, alt);\n\tif (!n)\n\t\treturn 0;\n\treturn alt[0];\n}\n\n/* Number of perf_events counting hardware events */\nstatic atomic_t num_events;\n/* Used to avoid races in calling reserve/release_pmc_hardware */\nstatic DEFINE_MUTEX(pmc_reserve_mutex);\n\n/*\n * Release the PMU if this is the last perf_event.\n */\nstatic void hw_perf_event_destroy(struct perf_event *event)\n{\n\tif (!atomic_add_unless(&num_events, -1, 1)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_dec_return(&num_events) == 0)\n\t\t\trelease_pmc_hardware();\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n}\n\n/*\n * Translate a generic cache event_id config to a raw event_id code.\n */\nstatic int hw_perf_cache_event(u64 config, u64 *eventp)\n{\n\tunsigned long type, op, result;\n\tint ev;\n\n\tif (!ppmu->cache_events)\n\t\treturn -EINVAL;\n\n\t/* unpack config */\n\ttype = config & 0xff;\n\top = (config >> 8) & 0xff;\n\tresult = (config >> 16) & 0xff;\n\n\tif (type >= PERF_COUNT_HW_CACHE_MAX ||\n\t    op >= PERF_COUNT_HW_CACHE_OP_MAX ||\n\t    result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn -EINVAL;\n\n\tev = (*ppmu->cache_events)[type][op][result];\n\tif (ev == 0)\n\t\treturn -EOPNOTSUPP;\n\tif (ev == -1)\n\t\treturn -EINVAL;\n\t*eventp = ev;\n\treturn 0;\n}\n\nstatic int power_pmu_event_init(struct perf_event *event)\n{\n\tu64 ev;\n\tunsigned long flags;\n\tstruct perf_event *ctrs[MAX_HWEVENTS];\n\tu64 events[MAX_HWEVENTS];\n\tunsigned int cflags[MAX_HWEVENTS];\n\tint n;\n\tint err;\n\tstruct cpu_hw_events *cpuhw;\n\n\tif (!ppmu)\n\t\treturn -ENOENT;\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_HARDWARE:\n\t\tev = event->attr.config;\n\t\tif (ev >= ppmu->n_generic || ppmu->generic_events[ev] == 0)\n\t\t\treturn -EOPNOTSUPP;\n\t\tev = ppmu->generic_events[ev];\n\t\tbreak;\n\tcase PERF_TYPE_HW_CACHE:\n\t\terr = hw_perf_cache_event(event->attr.config, &ev);\n\t\tif (err)\n\t\t\treturn err;\n\t\tbreak;\n\tcase PERF_TYPE_RAW:\n\t\tev = event->attr.config;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\tevent->hw.config_base = ev;\n\tevent->hw.idx = 0;\n\n\t/*\n\t * If we are not running on a hypervisor, force the\n\t * exclude_hv bit to 0 so that we don't care what\n\t * the user set it to.\n\t */\n\tif (!firmware_has_feature(FW_FEATURE_LPAR))\n\t\tevent->attr.exclude_hv = 0;\n\n\t/*\n\t * If this is a per-task event, then we can use\n\t * PM_RUN_* events interchangeably with their non RUN_*\n\t * equivalents, e.g. PM_RUN_CYC instead of PM_CYC.\n\t * XXX we should check if the task is an idle task.\n\t */\n\tflags = 0;\n\tif (event->attach_state & PERF_ATTACH_TASK)\n\t\tflags |= PPMU_ONLY_COUNT_RUN;\n\n\t/*\n\t * If this machine has limited events, check whether this\n\t * event_id could go on a limited event.\n\t */\n\tif (ppmu->flags & PPMU_LIMITED_PMC5_6) {\n\t\tif (can_go_on_limited_pmc(event, ev, flags)) {\n\t\t\tflags |= PPMU_LIMITED_PMC_OK;\n\t\t} else if (ppmu->limited_pmc_event(ev)) {\n\t\t\t/*\n\t\t\t * The requested event_id is on a limited PMC,\n\t\t\t * but we can't use a limited PMC; see if any\n\t\t\t * alternative goes on a normal PMC.\n\t\t\t */\n\t\t\tev = normal_pmc_alternative(ev, flags);\n\t\t\tif (!ev)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/*\n\t * If this is in a group, check if it can go on with all the\n\t * other hardware events in the group.  We assume the event\n\t * hasn't been linked into its leader's sibling list at this point.\n\t */\n\tn = 0;\n\tif (event->group_leader != event) {\n\t\tn = collect_events(event->group_leader, ppmu->n_counter - 1,\n\t\t\t\t   ctrs, events, cflags);\n\t\tif (n < 0)\n\t\t\treturn -EINVAL;\n\t}\n\tevents[n] = ev;\n\tctrs[n] = event;\n\tcflags[n] = flags;\n\tif (check_excludes(ctrs, cflags, n, 1))\n\t\treturn -EINVAL;\n\n\tcpuhw = &get_cpu_var(cpu_hw_events);\n\terr = power_check_constraints(cpuhw, events, cflags, n + 1);\n\tput_cpu_var(cpu_hw_events);\n\tif (err)\n\t\treturn -EINVAL;\n\n\tevent->hw.config = events[n];\n\tevent->hw.event_base = cflags[n];\n\tevent->hw.last_period = event->hw.sample_period;\n\tlocal64_set(&event->hw.period_left, event->hw.last_period);\n\n\t/*\n\t * See if we need to reserve the PMU.\n\t * If no events are currently in use, then we have to take a\n\t * mutex to ensure that we don't race with another task doing\n\t * reserve_pmc_hardware or release_pmc_hardware.\n\t */\n\terr = 0;\n\tif (!atomic_inc_not_zero(&num_events)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_read(&num_events) == 0 &&\n\t\t    reserve_pmc_hardware(perf_event_interrupt))\n\t\t\terr = -EBUSY;\n\t\telse\n\t\t\tatomic_inc(&num_events);\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n\tevent->destroy = hw_perf_event_destroy;\n\n\treturn err;\n}\n\nstruct pmu power_pmu = {\n\t.pmu_enable\t= power_pmu_enable,\n\t.pmu_disable\t= power_pmu_disable,\n\t.event_init\t= power_pmu_event_init,\n\t.add\t\t= power_pmu_add,\n\t.del\t\t= power_pmu_del,\n\t.start\t\t= power_pmu_start,\n\t.stop\t\t= power_pmu_stop,\n\t.read\t\t= power_pmu_read,\n\t.start_txn\t= power_pmu_start_txn,\n\t.cancel_txn\t= power_pmu_cancel_txn,\n\t.commit_txn\t= power_pmu_commit_txn,\n};\n\n/*\n * A counter has overflowed; update its count and record\n * things if requested.  Note that interrupts are hard-disabled\n * here so there is no possibility of being interrupted.\n */\nstatic void record_and_restart(struct perf_event *event, unsigned long val,\n\t\t\t       struct pt_regs *regs, int nmi)\n{\n\tu64 period = event->hw.sample_period;\n\ts64 prev, delta, left;\n\tint record = 0;\n\n\tif (event->hw.state & PERF_HES_STOPPED) {\n\t\twrite_pmc(event->hw.idx, 0);\n\t\treturn;\n\t}\n\n\t/* we don't have to worry about interrupts here */\n\tprev = local64_read(&event->hw.prev_count);\n\tdelta = check_and_compute_delta(prev, val);\n\tlocal64_add(delta, &event->count);\n\n\t/*\n\t * See if the total period for this event has expired,\n\t * and update for the next period.\n\t */\n\tval = 0;\n\tleft = local64_read(&event->hw.period_left) - delta;\n\tif (period) {\n\t\tif (left <= 0) {\n\t\t\tleft += period;\n\t\t\tif (left <= 0)\n\t\t\t\tleft = period;\n\t\t\trecord = 1;\n\t\t\tevent->hw.last_period = event->hw.sample_period;\n\t\t}\n\t\tif (left < 0x80000000LL)\n\t\t\tval = 0x80000000LL - left;\n\t}\n\n\twrite_pmc(event->hw.idx, val);\n\tlocal64_set(&event->hw.prev_count, val);\n\tlocal64_set(&event->hw.period_left, left);\n\tperf_event_update_userpage(event);\n\n\t/*\n\t * Finally record data if requested.\n\t */\n\tif (record) {\n\t\tstruct perf_sample_data data;\n\n\t\tperf_sample_data_init(&data, ~0ULL);\n\t\tdata.period = event->hw.last_period;\n\n\t\tif (event->attr.sample_type & PERF_SAMPLE_ADDR)\n\t\t\tperf_get_data_addr(regs, &data.addr);\n\n\t\tif (perf_event_overflow(event, nmi, &data, regs))\n\t\t\tpower_pmu_stop(event, 0);\n\t}\n}\n\n/*\n * Called from generic code to get the misc flags (i.e. processor mode)\n * for an event_id.\n */\nunsigned long perf_misc_flags(struct pt_regs *regs)\n{\n\tu32 flags = perf_get_misc_flags(regs);\n\n\tif (flags)\n\t\treturn flags;\n\treturn user_mode(regs) ? PERF_RECORD_MISC_USER :\n\t\tPERF_RECORD_MISC_KERNEL;\n}\n\n/*\n * Called from generic code to get the instruction pointer\n * for an event_id.\n */\nunsigned long perf_instruction_pointer(struct pt_regs *regs)\n{\n\tunsigned long ip;\n\n\tif (TRAP(regs) != 0xf00)\n\t\treturn regs->nip;\t/* not a PMU interrupt */\n\n\tip = mfspr(SPRN_SIAR) + perf_ip_adjust(regs);\n\treturn ip;\n}\n\nstatic bool pmc_overflow(unsigned long val)\n{\n\tif ((int)val < 0)\n\t\treturn true;\n\n\t/*\n\t * Events on POWER7 can roll back if a speculative event doesn't\n\t * eventually complete. Unfortunately in some rare cases they will\n\t * raise a performance monitor exception. We need to catch this to\n\t * ensure we reset the PMC. In all cases the PMC will be 256 or less\n\t * cycles from overflow.\n\t *\n\t * We only do this if the first pass fails to find any overflowing\n\t * PMCs because a user might set a period of less than 256 and we\n\t * don't want to mistakenly reset them.\n\t */\n\tif (__is_processor(PV_POWER7) && ((0x80000000 - val) <= 256))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * Performance monitor interrupt stuff\n */\nstatic void perf_event_interrupt(struct pt_regs *regs)\n{\n\tint i;\n\tstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\n\tstruct perf_event *event;\n\tunsigned long val;\n\tint found = 0;\n\tint nmi;\n\n\tif (cpuhw->n_limited)\n\t\tfreeze_limited_counters(cpuhw, mfspr(SPRN_PMC5),\n\t\t\t\t\tmfspr(SPRN_PMC6));\n\n\tperf_read_regs(regs);\n\n\tnmi = perf_intr_is_nmi(regs);\n\tif (nmi)\n\t\tnmi_enter();\n\telse\n\t\tirq_enter();\n\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tevent = cpuhw->event[i];\n\t\tif (!event->hw.idx || is_limited_pmc(event->hw.idx))\n\t\t\tcontinue;\n\t\tval = read_pmc(event->hw.idx);\n\t\tif ((int)val < 0) {\n\t\t\t/* event has overflowed */\n\t\t\tfound = 1;\n\t\t\trecord_and_restart(event, val, regs, nmi);\n\t\t}\n\t}\n\n\t/*\n\t * In case we didn't find and reset the event that caused\n\t * the interrupt, scan all events and reset any that are\n\t * negative, to avoid getting continual interrupts.\n\t * Any that we processed in the previous loop will not be negative.\n\t */\n\tif (!found) {\n\t\tfor (i = 0; i < ppmu->n_counter; ++i) {\n\t\t\tif (is_limited_pmc(i + 1))\n\t\t\t\tcontinue;\n\t\t\tval = read_pmc(i + 1);\n\t\t\tif (pmc_overflow(val))\n\t\t\t\twrite_pmc(i + 1, 0);\n\t\t}\n\t}\n\n\t/*\n\t * Reset MMCR0 to its normal value.  This will set PMXE and\n\t * clear FC (freeze counters) and PMAO (perf mon alert occurred)\n\t * and thus allow interrupts to occur again.\n\t * XXX might want to use MSR.PM to keep the events frozen until\n\t * we get back out of this interrupt.\n\t */\n\twrite_mmcr0(cpuhw, cpuhw->mmcr[0]);\n\n\tif (nmi)\n\t\tnmi_exit();\n\telse\n\t\tirq_exit();\n}\n\nstatic void power_pmu_setup(int cpu)\n{\n\tstruct cpu_hw_events *cpuhw = &per_cpu(cpu_hw_events, cpu);\n\n\tif (!ppmu)\n\t\treturn;\n\tmemset(cpuhw, 0, sizeof(*cpuhw));\n\tcpuhw->mmcr[0] = MMCR0_FC;\n}\n\nstatic int __cpuinit\npower_pmu_notifier(struct notifier_block *self, unsigned long action, void *hcpu)\n{\n\tunsigned int cpu = (long)hcpu;\n\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_UP_PREPARE:\n\t\tpower_pmu_setup(cpu);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nint register_power_pmu(struct power_pmu *pmu)\n{\n\tif (ppmu)\n\t\treturn -EBUSY;\t\t/* something's already registered */\n\n\tppmu = pmu;\n\tpr_info(\"%s performance monitor hardware support registered\\n\",\n\t\tpmu->name);\n\n#ifdef MSR_HV\n\t/*\n\t * Use FCHV to ignore kernel events if MSR.HV is set.\n\t */\n\tif (mfmsr() & MSR_HV)\n\t\tfreeze_events_kernel = MMCR0_FCHV;\n#endif /* CONFIG_PPC64 */\n\n\tperf_pmu_register(&power_pmu, \"cpu\", PERF_TYPE_RAW);\n\tperf_cpu_notifier(power_pmu_notifier);\n\n\treturn 0;\n}\n", "/*\n * Performance event support - Freescale Embedded Performance Monitor\n *\n * Copyright 2008-2009 Paul Mackerras, IBM Corporation.\n * Copyright 2010 Freescale Semiconductor, Inc.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public License\n * as published by the Free Software Foundation; either version\n * 2 of the License, or (at your option) any later version.\n */\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/perf_event.h>\n#include <linux/percpu.h>\n#include <linux/hardirq.h>\n#include <asm/reg_fsl_emb.h>\n#include <asm/pmc.h>\n#include <asm/machdep.h>\n#include <asm/firmware.h>\n#include <asm/ptrace.h>\n\nstruct cpu_hw_events {\n\tint n_events;\n\tint disabled;\n\tu8  pmcs_enabled;\n\tstruct perf_event *event[MAX_HWEVENTS];\n};\nstatic DEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events);\n\nstatic struct fsl_emb_pmu *ppmu;\n\n/* Number of perf_events counting hardware events */\nstatic atomic_t num_events;\n/* Used to avoid races in calling reserve/release_pmc_hardware */\nstatic DEFINE_MUTEX(pmc_reserve_mutex);\n\n/*\n * If interrupts were soft-disabled when a PMU interrupt occurs, treat\n * it as an NMI.\n */\nstatic inline int perf_intr_is_nmi(struct pt_regs *regs)\n{\n#ifdef __powerpc64__\n\treturn !regs->softe;\n#else\n\treturn 0;\n#endif\n}\n\nstatic void perf_event_interrupt(struct pt_regs *regs);\n\n/*\n * Read one performance monitor counter (PMC).\n */\nstatic unsigned long read_pmc(int idx)\n{\n\tunsigned long val;\n\n\tswitch (idx) {\n\tcase 0:\n\t\tval = mfpmr(PMRN_PMC0);\n\t\tbreak;\n\tcase 1:\n\t\tval = mfpmr(PMRN_PMC1);\n\t\tbreak;\n\tcase 2:\n\t\tval = mfpmr(PMRN_PMC2);\n\t\tbreak;\n\tcase 3:\n\t\tval = mfpmr(PMRN_PMC3);\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to read PMC%d\\n\", idx);\n\t\tval = 0;\n\t}\n\treturn val;\n}\n\n/*\n * Write one PMC.\n */\nstatic void write_pmc(int idx, unsigned long val)\n{\n\tswitch (idx) {\n\tcase 0:\n\t\tmtpmr(PMRN_PMC0, val);\n\t\tbreak;\n\tcase 1:\n\t\tmtpmr(PMRN_PMC1, val);\n\t\tbreak;\n\tcase 2:\n\t\tmtpmr(PMRN_PMC2, val);\n\t\tbreak;\n\tcase 3:\n\t\tmtpmr(PMRN_PMC3, val);\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to write PMC%d\\n\", idx);\n\t}\n\n\tisync();\n}\n\n/*\n * Write one local control A register\n */\nstatic void write_pmlca(int idx, unsigned long val)\n{\n\tswitch (idx) {\n\tcase 0:\n\t\tmtpmr(PMRN_PMLCA0, val);\n\t\tbreak;\n\tcase 1:\n\t\tmtpmr(PMRN_PMLCA1, val);\n\t\tbreak;\n\tcase 2:\n\t\tmtpmr(PMRN_PMLCA2, val);\n\t\tbreak;\n\tcase 3:\n\t\tmtpmr(PMRN_PMLCA3, val);\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to write PMLCA%d\\n\", idx);\n\t}\n\n\tisync();\n}\n\n/*\n * Write one local control B register\n */\nstatic void write_pmlcb(int idx, unsigned long val)\n{\n\tswitch (idx) {\n\tcase 0:\n\t\tmtpmr(PMRN_PMLCB0, val);\n\t\tbreak;\n\tcase 1:\n\t\tmtpmr(PMRN_PMLCB1, val);\n\t\tbreak;\n\tcase 2:\n\t\tmtpmr(PMRN_PMLCB2, val);\n\t\tbreak;\n\tcase 3:\n\t\tmtpmr(PMRN_PMLCB3, val);\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to write PMLCB%d\\n\", idx);\n\t}\n\n\tisync();\n}\n\nstatic void fsl_emb_pmu_read(struct perf_event *event)\n{\n\ts64 val, delta, prev;\n\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn;\n\n\t/*\n\t * Performance monitor interrupts come even when interrupts\n\t * are soft-disabled, as long as interrupts are hard-enabled.\n\t * Therefore we treat them like NMIs.\n\t */\n\tdo {\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tbarrier();\n\t\tval = read_pmc(event->hw.idx);\n\t} while (local64_cmpxchg(&event->hw.prev_count, prev, val) != prev);\n\n\t/* The counters are only 32 bits wide */\n\tdelta = (val - prev) & 0xfffffffful;\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &event->hw.period_left);\n}\n\n/*\n * Disable all events to prevent PMU interrupts and to allow\n * events to be added or removed.\n */\nstatic void fsl_emb_pmu_disable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\n\tif (!cpuhw->disabled) {\n\t\tcpuhw->disabled = 1;\n\n\t\t/*\n\t\t * Check if we ever enabled the PMU on this cpu.\n\t\t */\n\t\tif (!cpuhw->pmcs_enabled) {\n\t\t\tppc_enable_pmcs();\n\t\t\tcpuhw->pmcs_enabled = 1;\n\t\t}\n\n\t\tif (atomic_read(&num_events)) {\n\t\t\t/*\n\t\t\t * Set the 'freeze all counters' bit, and disable\n\t\t\t * interrupts.  The barrier is to make sure the\n\t\t\t * mtpmr has been executed and the PMU has frozen\n\t\t\t * the events before we return.\n\t\t\t */\n\n\t\t\tmtpmr(PMRN_PMGC0, PMGC0_FAC);\n\t\t\tisync();\n\t\t}\n\t}\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Re-enable all events if disable == 0.\n * If we were previously disabled and events were added, then\n * put the new config on the PMU.\n */\nstatic void fsl_emb_pmu_enable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\tif (!cpuhw->disabled)\n\t\tgoto out;\n\n\tcpuhw->disabled = 0;\n\tppc_set_pmu_inuse(cpuhw->n_events != 0);\n\n\tif (cpuhw->n_events > 0) {\n\t\tmtpmr(PMRN_PMGC0, PMGC0_PMIE | PMGC0_FCECE);\n\t\tisync();\n\t}\n\n out:\n\tlocal_irq_restore(flags);\n}\n\nstatic int collect_events(struct perf_event *group, int max_count,\n\t\t\t  struct perf_event *ctrs[])\n{\n\tint n = 0;\n\tstruct perf_event *event;\n\n\tif (!is_software_event(group)) {\n\t\tif (n >= max_count)\n\t\t\treturn -1;\n\t\tctrs[n] = group;\n\t\tn++;\n\t}\n\tlist_for_each_entry(event, &group->sibling_list, group_entry) {\n\t\tif (!is_software_event(event) &&\n\t\t    event->state != PERF_EVENT_STATE_OFF) {\n\t\t\tif (n >= max_count)\n\t\t\t\treturn -1;\n\t\t\tctrs[n] = event;\n\t\t\tn++;\n\t\t}\n\t}\n\treturn n;\n}\n\n/* context locked on entry */\nstatic int fsl_emb_pmu_add(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tint ret = -EAGAIN;\n\tint num_counters = ppmu->n_counter;\n\tu64 val;\n\tint i;\n\n\tperf_pmu_disable(event->pmu);\n\tcpuhw = &get_cpu_var(cpu_hw_events);\n\n\tif (event->hw.config & FSL_EMB_EVENT_RESTRICTED)\n\t\tnum_counters = ppmu->n_restricted;\n\n\t/*\n\t * Allocate counters from top-down, so that restricted-capable\n\t * counters are kept free as long as possible.\n\t */\n\tfor (i = num_counters - 1; i >= 0; i--) {\n\t\tif (cpuhw->event[i])\n\t\t\tcontinue;\n\n\t\tbreak;\n\t}\n\n\tif (i < 0)\n\t\tgoto out;\n\n\tevent->hw.idx = i;\n\tcpuhw->event[i] = event;\n\t++cpuhw->n_events;\n\n\tval = 0;\n\tif (event->hw.sample_period) {\n\t\ts64 left = local64_read(&event->hw.period_left);\n\t\tif (left < 0x80000000L)\n\t\t\tval = 0x80000000L - left;\n\t}\n\tlocal64_set(&event->hw.prev_count, val);\n\n\tif (!(flags & PERF_EF_START)) {\n\t\tevent->hw.state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\t\tval = 0;\n\t}\n\n\twrite_pmc(i, val);\n\tperf_event_update_userpage(event);\n\n\twrite_pmlcb(i, event->hw.config >> 32);\n\twrite_pmlca(i, event->hw.config_base);\n\n\tret = 0;\n out:\n\tput_cpu_var(cpu_hw_events);\n\tperf_pmu_enable(event->pmu);\n\treturn ret;\n}\n\n/* context locked on entry */\nstatic void fsl_emb_pmu_del(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tint i = event->hw.idx;\n\n\tperf_pmu_disable(event->pmu);\n\tif (i < 0)\n\t\tgoto out;\n\n\tfsl_emb_pmu_read(event);\n\n\tcpuhw = &get_cpu_var(cpu_hw_events);\n\n\tWARN_ON(event != cpuhw->event[event->hw.idx]);\n\n\twrite_pmlca(i, 0);\n\twrite_pmlcb(i, 0);\n\twrite_pmc(i, 0);\n\n\tcpuhw->event[i] = NULL;\n\tevent->hw.idx = -1;\n\n\t/*\n\t * TODO: if at least one restricted event exists, and we\n\t * just freed up a non-restricted-capable counter, and\n\t * there is a restricted-capable counter occupied by\n\t * a non-restricted event, migrate that event to the\n\t * vacated counter.\n\t */\n\n\tcpuhw->n_events--;\n\n out:\n\tperf_pmu_enable(event->pmu);\n\tput_cpu_var(cpu_hw_events);\n}\n\nstatic void fsl_emb_pmu_start(struct perf_event *event, int ef_flags)\n{\n\tunsigned long flags;\n\ts64 left;\n\n\tif (event->hw.idx < 0 || !event->hw.sample_period)\n\t\treturn;\n\n\tif (!(event->hw.state & PERF_HES_STOPPED))\n\t\treturn;\n\n\tif (ef_flags & PERF_EF_RELOAD)\n\t\tWARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tevent->hw.state = 0;\n\tleft = local64_read(&event->hw.period_left);\n\twrite_pmc(event->hw.idx, left);\n\n\tperf_event_update_userpage(event);\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\nstatic void fsl_emb_pmu_stop(struct perf_event *event, int ef_flags)\n{\n\tunsigned long flags;\n\n\tif (event->hw.idx < 0 || !event->hw.sample_period)\n\t\treturn;\n\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tfsl_emb_pmu_read(event);\n\tevent->hw.state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\twrite_pmc(event->hw.idx, 0);\n\n\tperf_event_update_userpage(event);\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Release the PMU if this is the last perf_event.\n */\nstatic void hw_perf_event_destroy(struct perf_event *event)\n{\n\tif (!atomic_add_unless(&num_events, -1, 1)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_dec_return(&num_events) == 0)\n\t\t\trelease_pmc_hardware();\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n}\n\n/*\n * Translate a generic cache event_id config to a raw event_id code.\n */\nstatic int hw_perf_cache_event(u64 config, u64 *eventp)\n{\n\tunsigned long type, op, result;\n\tint ev;\n\n\tif (!ppmu->cache_events)\n\t\treturn -EINVAL;\n\n\t/* unpack config */\n\ttype = config & 0xff;\n\top = (config >> 8) & 0xff;\n\tresult = (config >> 16) & 0xff;\n\n\tif (type >= PERF_COUNT_HW_CACHE_MAX ||\n\t    op >= PERF_COUNT_HW_CACHE_OP_MAX ||\n\t    result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn -EINVAL;\n\n\tev = (*ppmu->cache_events)[type][op][result];\n\tif (ev == 0)\n\t\treturn -EOPNOTSUPP;\n\tif (ev == -1)\n\t\treturn -EINVAL;\n\t*eventp = ev;\n\treturn 0;\n}\n\nstatic int fsl_emb_pmu_event_init(struct perf_event *event)\n{\n\tu64 ev;\n\tstruct perf_event *events[MAX_HWEVENTS];\n\tint n;\n\tint err;\n\tint num_restricted;\n\tint i;\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_HARDWARE:\n\t\tev = event->attr.config;\n\t\tif (ev >= ppmu->n_generic || ppmu->generic_events[ev] == 0)\n\t\t\treturn -EOPNOTSUPP;\n\t\tev = ppmu->generic_events[ev];\n\t\tbreak;\n\n\tcase PERF_TYPE_HW_CACHE:\n\t\terr = hw_perf_cache_event(event->attr.config, &ev);\n\t\tif (err)\n\t\t\treturn err;\n\t\tbreak;\n\n\tcase PERF_TYPE_RAW:\n\t\tev = event->attr.config;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\tevent->hw.config = ppmu->xlate_event(ev);\n\tif (!(event->hw.config & FSL_EMB_EVENT_VALID))\n\t\treturn -EINVAL;\n\n\t/*\n\t * If this is in a group, check if it can go on with all the\n\t * other hardware events in the group.  We assume the event\n\t * hasn't been linked into its leader's sibling list at this point.\n\t */\n\tn = 0;\n\tif (event->group_leader != event) {\n\t\tn = collect_events(event->group_leader,\n\t\t                   ppmu->n_counter - 1, events);\n\t\tif (n < 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (event->hw.config & FSL_EMB_EVENT_RESTRICTED) {\n\t\tnum_restricted = 0;\n\t\tfor (i = 0; i < n; i++) {\n\t\t\tif (events[i]->hw.config & FSL_EMB_EVENT_RESTRICTED)\n\t\t\t\tnum_restricted++;\n\t\t}\n\n\t\tif (num_restricted >= ppmu->n_restricted)\n\t\t\treturn -EINVAL;\n\t}\n\n\tevent->hw.idx = -1;\n\n\tevent->hw.config_base = PMLCA_CE | PMLCA_FCM1 |\n\t                        (u32)((ev << 16) & PMLCA_EVENT_MASK);\n\n\tif (event->attr.exclude_user)\n\t\tevent->hw.config_base |= PMLCA_FCU;\n\tif (event->attr.exclude_kernel)\n\t\tevent->hw.config_base |= PMLCA_FCS;\n\tif (event->attr.exclude_idle)\n\t\treturn -ENOTSUPP;\n\n\tevent->hw.last_period = event->hw.sample_period;\n\tlocal64_set(&event->hw.period_left, event->hw.last_period);\n\n\t/*\n\t * See if we need to reserve the PMU.\n\t * If no events are currently in use, then we have to take a\n\t * mutex to ensure that we don't race with another task doing\n\t * reserve_pmc_hardware or release_pmc_hardware.\n\t */\n\terr = 0;\n\tif (!atomic_inc_not_zero(&num_events)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_read(&num_events) == 0 &&\n\t\t    reserve_pmc_hardware(perf_event_interrupt))\n\t\t\terr = -EBUSY;\n\t\telse\n\t\t\tatomic_inc(&num_events);\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\n\t\tmtpmr(PMRN_PMGC0, PMGC0_FAC);\n\t\tisync();\n\t}\n\tevent->destroy = hw_perf_event_destroy;\n\n\treturn err;\n}\n\nstatic struct pmu fsl_emb_pmu = {\n\t.pmu_enable\t= fsl_emb_pmu_enable,\n\t.pmu_disable\t= fsl_emb_pmu_disable,\n\t.event_init\t= fsl_emb_pmu_event_init,\n\t.add\t\t= fsl_emb_pmu_add,\n\t.del\t\t= fsl_emb_pmu_del,\n\t.start\t\t= fsl_emb_pmu_start,\n\t.stop\t\t= fsl_emb_pmu_stop,\n\t.read\t\t= fsl_emb_pmu_read,\n};\n\n/*\n * A counter has overflowed; update its count and record\n * things if requested.  Note that interrupts are hard-disabled\n * here so there is no possibility of being interrupted.\n */\nstatic void record_and_restart(struct perf_event *event, unsigned long val,\n\t\t\t       struct pt_regs *regs, int nmi)\n{\n\tu64 period = event->hw.sample_period;\n\ts64 prev, delta, left;\n\tint record = 0;\n\n\tif (event->hw.state & PERF_HES_STOPPED) {\n\t\twrite_pmc(event->hw.idx, 0);\n\t\treturn;\n\t}\n\n\t/* we don't have to worry about interrupts here */\n\tprev = local64_read(&event->hw.prev_count);\n\tdelta = (val - prev) & 0xfffffffful;\n\tlocal64_add(delta, &event->count);\n\n\t/*\n\t * See if the total period for this event has expired,\n\t * and update for the next period.\n\t */\n\tval = 0;\n\tleft = local64_read(&event->hw.period_left) - delta;\n\tif (period) {\n\t\tif (left <= 0) {\n\t\t\tleft += period;\n\t\t\tif (left <= 0)\n\t\t\t\tleft = period;\n\t\t\trecord = 1;\n\t\t\tevent->hw.last_period = event->hw.sample_period;\n\t\t}\n\t\tif (left < 0x80000000LL)\n\t\t\tval = 0x80000000LL - left;\n\t}\n\n\twrite_pmc(event->hw.idx, val);\n\tlocal64_set(&event->hw.prev_count, val);\n\tlocal64_set(&event->hw.period_left, left);\n\tperf_event_update_userpage(event);\n\n\t/*\n\t * Finally record data if requested.\n\t */\n\tif (record) {\n\t\tstruct perf_sample_data data;\n\n\t\tperf_sample_data_init(&data, 0);\n\t\tdata.period = event->hw.last_period;\n\n\t\tif (perf_event_overflow(event, nmi, &data, regs))\n\t\t\tfsl_emb_pmu_stop(event, 0);\n\t}\n}\n\nstatic void perf_event_interrupt(struct pt_regs *regs)\n{\n\tint i;\n\tstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\n\tstruct perf_event *event;\n\tunsigned long val;\n\tint found = 0;\n\tint nmi;\n\n\tnmi = perf_intr_is_nmi(regs);\n\tif (nmi)\n\t\tnmi_enter();\n\telse\n\t\tirq_enter();\n\n\tfor (i = 0; i < ppmu->n_counter; ++i) {\n\t\tevent = cpuhw->event[i];\n\n\t\tval = read_pmc(i);\n\t\tif ((int)val < 0) {\n\t\t\tif (event) {\n\t\t\t\t/* event has overflowed */\n\t\t\t\tfound = 1;\n\t\t\t\trecord_and_restart(event, val, regs, nmi);\n\t\t\t} else {\n\t\t\t\t/*\n\t\t\t\t * Disabled counter is negative,\n\t\t\t\t * reset it just in case.\n\t\t\t\t */\n\t\t\t\twrite_pmc(i, 0);\n\t\t\t}\n\t\t}\n\t}\n\n\t/* PMM will keep counters frozen until we return from the interrupt. */\n\tmtmsr(mfmsr() | MSR_PMM);\n\tmtpmr(PMRN_PMGC0, PMGC0_PMIE | PMGC0_FCECE);\n\tisync();\n\n\tif (nmi)\n\t\tnmi_exit();\n\telse\n\t\tirq_exit();\n}\n\nvoid hw_perf_event_setup(int cpu)\n{\n\tstruct cpu_hw_events *cpuhw = &per_cpu(cpu_hw_events, cpu);\n\n\tmemset(cpuhw, 0, sizeof(*cpuhw));\n}\n\nint register_fsl_emb_pmu(struct fsl_emb_pmu *pmu)\n{\n\tif (ppmu)\n\t\treturn -EBUSY;\t\t/* something's already registered */\n\n\tppmu = pmu;\n\tpr_info(\"%s performance monitor hardware support registered\\n\",\n\t\tpmu->name);\n\n\tperf_pmu_register(&fsl_emb_pmu, \"cpu\", PERF_TYPE_RAW);\n\n\treturn 0;\n}\n", "/*\n *  PowerPC version\n *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)\n *\n *  Derived from \"arch/m68k/kernel/ptrace.c\"\n *  Copyright (C) 1994 by Hamish Macdonald\n *  Taken from linux/kernel/ptrace.c and modified for M680x0.\n *  linux/kernel/ptrace.c is by Ross Biro 1/23/92, edited by Linus Torvalds\n *\n * Modified by Cort Dougan (cort@hq.fsmlabs.com)\n * and Paul Mackerras (paulus@samba.org).\n *\n * This file is subject to the terms and conditions of the GNU General\n * Public License.  See the file README.legal in the main directory of\n * this archive for more details.\n */\n\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/errno.h>\n#include <linux/ptrace.h>\n#include <linux/regset.h>\n#include <linux/tracehook.h>\n#include <linux/elf.h>\n#include <linux/user.h>\n#include <linux/security.h>\n#include <linux/signal.h>\n#include <linux/seccomp.h>\n#include <linux/audit.h>\n#include <trace/syscall.h>\n#ifdef CONFIG_PPC32\n#include <linux/module.h>\n#endif\n#include <linux/hw_breakpoint.h>\n#include <linux/perf_event.h>\n\n#include <asm/uaccess.h>\n#include <asm/page.h>\n#include <asm/pgtable.h>\n#include <asm/system.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/syscalls.h>\n\n/*\n * The parameter save area on the stack is used to store arguments being passed\n * to callee function and is located at fixed offset from stack pointer.\n */\n#ifdef CONFIG_PPC32\n#define PARAMETER_SAVE_AREA_OFFSET\t24  /* bytes */\n#else /* CONFIG_PPC32 */\n#define PARAMETER_SAVE_AREA_OFFSET\t48  /* bytes */\n#endif\n\nstruct pt_regs_offset {\n\tconst char *name;\n\tint offset;\n};\n\n#define STR(s)\t#s\t\t\t/* convert to string */\n#define REG_OFFSET_NAME(r) {.name = #r, .offset = offsetof(struct pt_regs, r)}\n#define GPR_OFFSET_NAME(num)\t\\\n\t{.name = STR(gpr##num), .offset = offsetof(struct pt_regs, gpr[num])}\n#define REG_OFFSET_END {.name = NULL, .offset = 0}\n\nstatic const struct pt_regs_offset regoffset_table[] = {\n\tGPR_OFFSET_NAME(0),\n\tGPR_OFFSET_NAME(1),\n\tGPR_OFFSET_NAME(2),\n\tGPR_OFFSET_NAME(3),\n\tGPR_OFFSET_NAME(4),\n\tGPR_OFFSET_NAME(5),\n\tGPR_OFFSET_NAME(6),\n\tGPR_OFFSET_NAME(7),\n\tGPR_OFFSET_NAME(8),\n\tGPR_OFFSET_NAME(9),\n\tGPR_OFFSET_NAME(10),\n\tGPR_OFFSET_NAME(11),\n\tGPR_OFFSET_NAME(12),\n\tGPR_OFFSET_NAME(13),\n\tGPR_OFFSET_NAME(14),\n\tGPR_OFFSET_NAME(15),\n\tGPR_OFFSET_NAME(16),\n\tGPR_OFFSET_NAME(17),\n\tGPR_OFFSET_NAME(18),\n\tGPR_OFFSET_NAME(19),\n\tGPR_OFFSET_NAME(20),\n\tGPR_OFFSET_NAME(21),\n\tGPR_OFFSET_NAME(22),\n\tGPR_OFFSET_NAME(23),\n\tGPR_OFFSET_NAME(24),\n\tGPR_OFFSET_NAME(25),\n\tGPR_OFFSET_NAME(26),\n\tGPR_OFFSET_NAME(27),\n\tGPR_OFFSET_NAME(28),\n\tGPR_OFFSET_NAME(29),\n\tGPR_OFFSET_NAME(30),\n\tGPR_OFFSET_NAME(31),\n\tREG_OFFSET_NAME(nip),\n\tREG_OFFSET_NAME(msr),\n\tREG_OFFSET_NAME(ctr),\n\tREG_OFFSET_NAME(link),\n\tREG_OFFSET_NAME(xer),\n\tREG_OFFSET_NAME(ccr),\n#ifdef CONFIG_PPC64\n\tREG_OFFSET_NAME(softe),\n#else\n\tREG_OFFSET_NAME(mq),\n#endif\n\tREG_OFFSET_NAME(trap),\n\tREG_OFFSET_NAME(dar),\n\tREG_OFFSET_NAME(dsisr),\n\tREG_OFFSET_END,\n};\n\n/**\n * regs_query_register_offset() - query register offset from its name\n * @name:\tthe name of a register\n *\n * regs_query_register_offset() returns the offset of a register in struct\n * pt_regs from its name. If the name is invalid, this returns -EINVAL;\n */\nint regs_query_register_offset(const char *name)\n{\n\tconst struct pt_regs_offset *roff;\n\tfor (roff = regoffset_table; roff->name != NULL; roff++)\n\t\tif (!strcmp(roff->name, name))\n\t\t\treturn roff->offset;\n\treturn -EINVAL;\n}\n\n/**\n * regs_query_register_name() - query register name from its offset\n * @offset:\tthe offset of a register in struct pt_regs.\n *\n * regs_query_register_name() returns the name of a register from its\n * offset in struct pt_regs. If the @offset is invalid, this returns NULL;\n */\nconst char *regs_query_register_name(unsigned int offset)\n{\n\tconst struct pt_regs_offset *roff;\n\tfor (roff = regoffset_table; roff->name != NULL; roff++)\n\t\tif (roff->offset == offset)\n\t\t\treturn roff->name;\n\treturn NULL;\n}\n\n/*\n * does not yet catch signals sent when the child dies.\n * in exit.c or in signal.c.\n */\n\n/*\n * Set of msr bits that gdb can change on behalf of a process.\n */\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n#define MSR_DEBUGCHANGE\t0\n#else\n#define MSR_DEBUGCHANGE\t(MSR_SE | MSR_BE)\n#endif\n\n/*\n * Max register writeable via put_reg\n */\n#ifdef CONFIG_PPC32\n#define PT_MAX_PUT_REG\tPT_MQ\n#else\n#define PT_MAX_PUT_REG\tPT_CCR\n#endif\n\nstatic unsigned long get_user_msr(struct task_struct *task)\n{\n\treturn task->thread.regs->msr | task->thread.fpexc_mode;\n}\n\nstatic int set_user_msr(struct task_struct *task, unsigned long msr)\n{\n\ttask->thread.regs->msr &= ~MSR_DEBUGCHANGE;\n\ttask->thread.regs->msr |= msr & MSR_DEBUGCHANGE;\n\treturn 0;\n}\n\n/*\n * We prevent mucking around with the reserved area of trap\n * which are used internally by the kernel.\n */\nstatic int set_user_trap(struct task_struct *task, unsigned long trap)\n{\n\ttask->thread.regs->trap = trap & 0xfff0;\n\treturn 0;\n}\n\n/*\n * Get contents of register REGNO in task TASK.\n */\nunsigned long ptrace_get_reg(struct task_struct *task, int regno)\n{\n\tif (task->thread.regs == NULL)\n\t\treturn -EIO;\n\n\tif (regno == PT_MSR)\n\t\treturn get_user_msr(task);\n\n\tif (regno < (sizeof(struct pt_regs) / sizeof(unsigned long)))\n\t\treturn ((unsigned long *)task->thread.regs)[regno];\n\n\treturn -EIO;\n}\n\n/*\n * Write contents of register REGNO in task TASK.\n */\nint ptrace_put_reg(struct task_struct *task, int regno, unsigned long data)\n{\n\tif (task->thread.regs == NULL)\n\t\treturn -EIO;\n\n\tif (regno == PT_MSR)\n\t\treturn set_user_msr(task, data);\n\tif (regno == PT_TRAP)\n\t\treturn set_user_trap(task, data);\n\n\tif (regno <= PT_MAX_PUT_REG) {\n\t\t((unsigned long *)task->thread.regs)[regno] = data;\n\t\treturn 0;\n\t}\n\treturn -EIO;\n}\n\nstatic int gpr_get(struct task_struct *target, const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n\tint i, ret;\n\n\tif (target->thread.regs == NULL)\n\t\treturn -EIO;\n\n\tif (!FULL_REGS(target->thread.regs)) {\n\t\t/* We have a partial register set.  Fill 14-31 with bogus values */\n\t\tfor (i = 14; i < 32; i++)\n\t\t\ttarget->thread.regs->gpr[i] = NV_REG_POISON;\n\t}\n\n\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  target->thread.regs,\n\t\t\t\t  0, offsetof(struct pt_regs, msr));\n\tif (!ret) {\n\t\tunsigned long msr = get_user_msr(target);\n\t\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf, &msr,\n\t\t\t\t\t  offsetof(struct pt_regs, msr),\n\t\t\t\t\t  offsetof(struct pt_regs, msr) +\n\t\t\t\t\t  sizeof(msr));\n\t}\n\n\tBUILD_BUG_ON(offsetof(struct pt_regs, orig_gpr3) !=\n\t\t     offsetof(struct pt_regs, msr) + sizeof(long));\n\n\tif (!ret)\n\t\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t  &target->thread.regs->orig_gpr3,\n\t\t\t\t\t  offsetof(struct pt_regs, orig_gpr3),\n\t\t\t\t\t  sizeof(struct pt_regs));\n\tif (!ret)\n\t\tret = user_regset_copyout_zero(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t       sizeof(struct pt_regs), -1);\n\n\treturn ret;\n}\n\nstatic int gpr_set(struct task_struct *target, const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   const void *kbuf, const void __user *ubuf)\n{\n\tunsigned long reg;\n\tint ret;\n\n\tif (target->thread.regs == NULL)\n\t\treturn -EIO;\n\n\tCHECK_FULL_REGS(target->thread.regs);\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t target->thread.regs,\n\t\t\t\t 0, PT_MSR * sizeof(reg));\n\n\tif (!ret && count > 0) {\n\t\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &reg,\n\t\t\t\t\t PT_MSR * sizeof(reg),\n\t\t\t\t\t (PT_MSR + 1) * sizeof(reg));\n\t\tif (!ret)\n\t\t\tret = set_user_msr(target, reg);\n\t}\n\n\tBUILD_BUG_ON(offsetof(struct pt_regs, orig_gpr3) !=\n\t\t     offsetof(struct pt_regs, msr) + sizeof(long));\n\n\tif (!ret)\n\t\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t &target->thread.regs->orig_gpr3,\n\t\t\t\t\t PT_ORIG_R3 * sizeof(reg),\n\t\t\t\t\t (PT_MAX_PUT_REG + 1) * sizeof(reg));\n\n\tif (PT_MAX_PUT_REG + 1 < PT_TRAP && !ret)\n\t\tret = user_regset_copyin_ignore(\n\t\t\t&pos, &count, &kbuf, &ubuf,\n\t\t\t(PT_MAX_PUT_REG + 1) * sizeof(reg),\n\t\t\tPT_TRAP * sizeof(reg));\n\n\tif (!ret && count > 0) {\n\t\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &reg,\n\t\t\t\t\t PT_TRAP * sizeof(reg),\n\t\t\t\t\t (PT_TRAP + 1) * sizeof(reg));\n\t\tif (!ret)\n\t\t\tret = set_user_trap(target, reg);\n\t}\n\n\tif (!ret)\n\t\tret = user_regset_copyin_ignore(\n\t\t\t&pos, &count, &kbuf, &ubuf,\n\t\t\t(PT_TRAP + 1) * sizeof(reg), -1);\n\n\treturn ret;\n}\n\nstatic int fpr_get(struct task_struct *target, const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n#ifdef CONFIG_VSX\n\tdouble buf[33];\n\tint i;\n#endif\n\tflush_fp_to_thread(target);\n\n#ifdef CONFIG_VSX\n\t/* copy to local buffer then write that out */\n\tfor (i = 0; i < 32 ; i++)\n\t\tbuf[i] = target->thread.TS_FPR(i);\n\tmemcpy(&buf[32], &target->thread.fpscr, sizeof(double));\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf, buf, 0, -1);\n\n#else\n\tBUILD_BUG_ON(offsetof(struct thread_struct, fpscr) !=\n\t\t     offsetof(struct thread_struct, TS_FPR(32)));\n\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t   &target->thread.fpr, 0, -1);\n#endif\n}\n\nstatic int fpr_set(struct task_struct *target, const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   const void *kbuf, const void __user *ubuf)\n{\n#ifdef CONFIG_VSX\n\tdouble buf[33];\n\tint i;\n#endif\n\tflush_fp_to_thread(target);\n\n#ifdef CONFIG_VSX\n\t/* copy to local buffer then write that out */\n\ti = user_regset_copyin(&pos, &count, &kbuf, &ubuf, buf, 0, -1);\n\tif (i)\n\t\treturn i;\n\tfor (i = 0; i < 32 ; i++)\n\t\ttarget->thread.TS_FPR(i) = buf[i];\n\tmemcpy(&target->thread.fpscr, &buf[32], sizeof(double));\n\treturn 0;\n#else\n\tBUILD_BUG_ON(offsetof(struct thread_struct, fpscr) !=\n\t\t     offsetof(struct thread_struct, TS_FPR(32)));\n\n\treturn user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  &target->thread.fpr, 0, -1);\n#endif\n}\n\n#ifdef CONFIG_ALTIVEC\n/*\n * Get/set all the altivec registers vr0..vr31, vscr, vrsave, in one go.\n * The transfer totals 34 quadword.  Quadwords 0-31 contain the\n * corresponding vector registers.  Quadword 32 contains the vscr as the\n * last word (offset 12) within that quadword.  Quadword 33 contains the\n * vrsave as the first word (offset 0) within the quadword.\n *\n * This definition of the VMX state is compatible with the current PPC32\n * ptrace interface.  This allows signal handling and ptrace to use the\n * same structures.  This also simplifies the implementation of a bi-arch\n * (combined (32- and 64-bit) gdb.\n */\n\nstatic int vr_active(struct task_struct *target,\n\t\t     const struct user_regset *regset)\n{\n\tflush_altivec_to_thread(target);\n\treturn target->thread.used_vr ? regset->n : 0;\n}\n\nstatic int vr_get(struct task_struct *target, const struct user_regset *regset,\n\t\t  unsigned int pos, unsigned int count,\n\t\t  void *kbuf, void __user *ubuf)\n{\n\tint ret;\n\n\tflush_altivec_to_thread(target);\n\n\tBUILD_BUG_ON(offsetof(struct thread_struct, vscr) !=\n\t\t     offsetof(struct thread_struct, vr[32]));\n\n\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  &target->thread.vr, 0,\n\t\t\t\t  33 * sizeof(vector128));\n\tif (!ret) {\n\t\t/*\n\t\t * Copy out only the low-order word of vrsave.\n\t\t */\n\t\tunion {\n\t\t\telf_vrreg_t reg;\n\t\t\tu32 word;\n\t\t} vrsave;\n\t\tmemset(&vrsave, 0, sizeof(vrsave));\n\t\tvrsave.word = target->thread.vrsave;\n\t\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf, &vrsave,\n\t\t\t\t\t  33 * sizeof(vector128), -1);\n\t}\n\n\treturn ret;\n}\n\nstatic int vr_set(struct task_struct *target, const struct user_regset *regset,\n\t\t  unsigned int pos, unsigned int count,\n\t\t  const void *kbuf, const void __user *ubuf)\n{\n\tint ret;\n\n\tflush_altivec_to_thread(target);\n\n\tBUILD_BUG_ON(offsetof(struct thread_struct, vscr) !=\n\t\t     offsetof(struct thread_struct, vr[32]));\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t &target->thread.vr, 0, 33 * sizeof(vector128));\n\tif (!ret && count > 0) {\n\t\t/*\n\t\t * We use only the first word of vrsave.\n\t\t */\n\t\tunion {\n\t\t\telf_vrreg_t reg;\n\t\t\tu32 word;\n\t\t} vrsave;\n\t\tmemset(&vrsave, 0, sizeof(vrsave));\n\t\tvrsave.word = target->thread.vrsave;\n\t\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &vrsave,\n\t\t\t\t\t 33 * sizeof(vector128), -1);\n\t\tif (!ret)\n\t\t\ttarget->thread.vrsave = vrsave.word;\n\t}\n\n\treturn ret;\n}\n#endif /* CONFIG_ALTIVEC */\n\n#ifdef CONFIG_VSX\n/*\n * Currently to set and and get all the vsx state, you need to call\n * the fp and VMX calls as well.  This only get/sets the lower 32\n * 128bit VSX registers.\n */\n\nstatic int vsr_active(struct task_struct *target,\n\t\t      const struct user_regset *regset)\n{\n\tflush_vsx_to_thread(target);\n\treturn target->thread.used_vsr ? regset->n : 0;\n}\n\nstatic int vsr_get(struct task_struct *target, const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n\tdouble buf[32];\n\tint ret, i;\n\n\tflush_vsx_to_thread(target);\n\n\tfor (i = 0; i < 32 ; i++)\n\t\tbuf[i] = target->thread.fpr[i][TS_VSRLOWOFFSET];\n\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  buf, 0, 32 * sizeof(double));\n\n\treturn ret;\n}\n\nstatic int vsr_set(struct task_struct *target, const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   const void *kbuf, const void __user *ubuf)\n{\n\tdouble buf[32];\n\tint ret,i;\n\n\tflush_vsx_to_thread(target);\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t buf, 0, 32 * sizeof(double));\n\tfor (i = 0; i < 32 ; i++)\n\t\ttarget->thread.fpr[i][TS_VSRLOWOFFSET] = buf[i];\n\n\n\treturn ret;\n}\n#endif /* CONFIG_VSX */\n\n#ifdef CONFIG_SPE\n\n/*\n * For get_evrregs/set_evrregs functions 'data' has the following layout:\n *\n * struct {\n *   u32 evr[32];\n *   u64 acc;\n *   u32 spefscr;\n * }\n */\n\nstatic int evr_active(struct task_struct *target,\n\t\t      const struct user_regset *regset)\n{\n\tflush_spe_to_thread(target);\n\treturn target->thread.used_spe ? regset->n : 0;\n}\n\nstatic int evr_get(struct task_struct *target, const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n\tint ret;\n\n\tflush_spe_to_thread(target);\n\n\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  &target->thread.evr,\n\t\t\t\t  0, sizeof(target->thread.evr));\n\n\tBUILD_BUG_ON(offsetof(struct thread_struct, acc) + sizeof(u64) !=\n\t\t     offsetof(struct thread_struct, spefscr));\n\n\tif (!ret)\n\t\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t  &target->thread.acc,\n\t\t\t\t\t  sizeof(target->thread.evr), -1);\n\n\treturn ret;\n}\n\nstatic int evr_set(struct task_struct *target, const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   const void *kbuf, const void __user *ubuf)\n{\n\tint ret;\n\n\tflush_spe_to_thread(target);\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t &target->thread.evr,\n\t\t\t\t 0, sizeof(target->thread.evr));\n\n\tBUILD_BUG_ON(offsetof(struct thread_struct, acc) + sizeof(u64) !=\n\t\t     offsetof(struct thread_struct, spefscr));\n\n\tif (!ret)\n\t\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t &target->thread.acc,\n\t\t\t\t\t sizeof(target->thread.evr), -1);\n\n\treturn ret;\n}\n#endif /* CONFIG_SPE */\n\n\n/*\n * These are our native regset flavors.\n */\nenum powerpc_regset {\n\tREGSET_GPR,\n\tREGSET_FPR,\n#ifdef CONFIG_ALTIVEC\n\tREGSET_VMX,\n#endif\n#ifdef CONFIG_VSX\n\tREGSET_VSX,\n#endif\n#ifdef CONFIG_SPE\n\tREGSET_SPE,\n#endif\n};\n\nstatic const struct user_regset native_regsets[] = {\n\t[REGSET_GPR] = {\n\t\t.core_note_type = NT_PRSTATUS, .n = ELF_NGREG,\n\t\t.size = sizeof(long), .align = sizeof(long),\n\t\t.get = gpr_get, .set = gpr_set\n\t},\n\t[REGSET_FPR] = {\n\t\t.core_note_type = NT_PRFPREG, .n = ELF_NFPREG,\n\t\t.size = sizeof(double), .align = sizeof(double),\n\t\t.get = fpr_get, .set = fpr_set\n\t},\n#ifdef CONFIG_ALTIVEC\n\t[REGSET_VMX] = {\n\t\t.core_note_type = NT_PPC_VMX, .n = 34,\n\t\t.size = sizeof(vector128), .align = sizeof(vector128),\n\t\t.active = vr_active, .get = vr_get, .set = vr_set\n\t},\n#endif\n#ifdef CONFIG_VSX\n\t[REGSET_VSX] = {\n\t\t.core_note_type = NT_PPC_VSX, .n = 32,\n\t\t.size = sizeof(double), .align = sizeof(double),\n\t\t.active = vsr_active, .get = vsr_get, .set = vsr_set\n\t},\n#endif\n#ifdef CONFIG_SPE\n\t[REGSET_SPE] = {\n\t\t.n = 35,\n\t\t.size = sizeof(u32), .align = sizeof(u32),\n\t\t.active = evr_active, .get = evr_get, .set = evr_set\n\t},\n#endif\n};\n\nstatic const struct user_regset_view user_ppc_native_view = {\n\t.name = UTS_MACHINE, .e_machine = ELF_ARCH, .ei_osabi = ELF_OSABI,\n\t.regsets = native_regsets, .n = ARRAY_SIZE(native_regsets)\n};\n\n#ifdef CONFIG_PPC64\n#include <linux/compat.h>\n\nstatic int gpr32_get(struct task_struct *target,\n\t\t     const struct user_regset *regset,\n\t\t     unsigned int pos, unsigned int count,\n\t\t     void *kbuf, void __user *ubuf)\n{\n\tconst unsigned long *regs = &target->thread.regs->gpr[0];\n\tcompat_ulong_t *k = kbuf;\n\tcompat_ulong_t __user *u = ubuf;\n\tcompat_ulong_t reg;\n\tint i;\n\n\tif (target->thread.regs == NULL)\n\t\treturn -EIO;\n\n\tif (!FULL_REGS(target->thread.regs)) {\n\t\t/* We have a partial register set.  Fill 14-31 with bogus values */\n\t\tfor (i = 14; i < 32; i++)\n\t\t\ttarget->thread.regs->gpr[i] = NV_REG_POISON; \n\t}\n\n\tpos /= sizeof(reg);\n\tcount /= sizeof(reg);\n\n\tif (kbuf)\n\t\tfor (; count > 0 && pos < PT_MSR; --count)\n\t\t\t*k++ = regs[pos++];\n\telse\n\t\tfor (; count > 0 && pos < PT_MSR; --count)\n\t\t\tif (__put_user((compat_ulong_t) regs[pos++], u++))\n\t\t\t\treturn -EFAULT;\n\n\tif (count > 0 && pos == PT_MSR) {\n\t\treg = get_user_msr(target);\n\t\tif (kbuf)\n\t\t\t*k++ = reg;\n\t\telse if (__put_user(reg, u++))\n\t\t\treturn -EFAULT;\n\t\t++pos;\n\t\t--count;\n\t}\n\n\tif (kbuf)\n\t\tfor (; count > 0 && pos < PT_REGS_COUNT; --count)\n\t\t\t*k++ = regs[pos++];\n\telse\n\t\tfor (; count > 0 && pos < PT_REGS_COUNT; --count)\n\t\t\tif (__put_user((compat_ulong_t) regs[pos++], u++))\n\t\t\t\treturn -EFAULT;\n\n\tkbuf = k;\n\tubuf = u;\n\tpos *= sizeof(reg);\n\tcount *= sizeof(reg);\n\treturn user_regset_copyout_zero(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\tPT_REGS_COUNT * sizeof(reg), -1);\n}\n\nstatic int gpr32_set(struct task_struct *target,\n\t\t     const struct user_regset *regset,\n\t\t     unsigned int pos, unsigned int count,\n\t\t     const void *kbuf, const void __user *ubuf)\n{\n\tunsigned long *regs = &target->thread.regs->gpr[0];\n\tconst compat_ulong_t *k = kbuf;\n\tconst compat_ulong_t __user *u = ubuf;\n\tcompat_ulong_t reg;\n\n\tif (target->thread.regs == NULL)\n\t\treturn -EIO;\n\n\tCHECK_FULL_REGS(target->thread.regs);\n\n\tpos /= sizeof(reg);\n\tcount /= sizeof(reg);\n\n\tif (kbuf)\n\t\tfor (; count > 0 && pos < PT_MSR; --count)\n\t\t\tregs[pos++] = *k++;\n\telse\n\t\tfor (; count > 0 && pos < PT_MSR; --count) {\n\t\t\tif (__get_user(reg, u++))\n\t\t\t\treturn -EFAULT;\n\t\t\tregs[pos++] = reg;\n\t\t}\n\n\n\tif (count > 0 && pos == PT_MSR) {\n\t\tif (kbuf)\n\t\t\treg = *k++;\n\t\telse if (__get_user(reg, u++))\n\t\t\treturn -EFAULT;\n\t\tset_user_msr(target, reg);\n\t\t++pos;\n\t\t--count;\n\t}\n\n\tif (kbuf) {\n\t\tfor (; count > 0 && pos <= PT_MAX_PUT_REG; --count)\n\t\t\tregs[pos++] = *k++;\n\t\tfor (; count > 0 && pos < PT_TRAP; --count, ++pos)\n\t\t\t++k;\n\t} else {\n\t\tfor (; count > 0 && pos <= PT_MAX_PUT_REG; --count) {\n\t\t\tif (__get_user(reg, u++))\n\t\t\t\treturn -EFAULT;\n\t\t\tregs[pos++] = reg;\n\t\t}\n\t\tfor (; count > 0 && pos < PT_TRAP; --count, ++pos)\n\t\t\tif (__get_user(reg, u++))\n\t\t\t\treturn -EFAULT;\n\t}\n\n\tif (count > 0 && pos == PT_TRAP) {\n\t\tif (kbuf)\n\t\t\treg = *k++;\n\t\telse if (__get_user(reg, u++))\n\t\t\treturn -EFAULT;\n\t\tset_user_trap(target, reg);\n\t\t++pos;\n\t\t--count;\n\t}\n\n\tkbuf = k;\n\tubuf = u;\n\tpos *= sizeof(reg);\n\tcount *= sizeof(reg);\n\treturn user_regset_copyin_ignore(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t (PT_TRAP + 1) * sizeof(reg), -1);\n}\n\n/*\n * These are the regset flavors matching the CONFIG_PPC32 native set.\n */\nstatic const struct user_regset compat_regsets[] = {\n\t[REGSET_GPR] = {\n\t\t.core_note_type = NT_PRSTATUS, .n = ELF_NGREG,\n\t\t.size = sizeof(compat_long_t), .align = sizeof(compat_long_t),\n\t\t.get = gpr32_get, .set = gpr32_set\n\t},\n\t[REGSET_FPR] = {\n\t\t.core_note_type = NT_PRFPREG, .n = ELF_NFPREG,\n\t\t.size = sizeof(double), .align = sizeof(double),\n\t\t.get = fpr_get, .set = fpr_set\n\t},\n#ifdef CONFIG_ALTIVEC\n\t[REGSET_VMX] = {\n\t\t.core_note_type = NT_PPC_VMX, .n = 34,\n\t\t.size = sizeof(vector128), .align = sizeof(vector128),\n\t\t.active = vr_active, .get = vr_get, .set = vr_set\n\t},\n#endif\n#ifdef CONFIG_SPE\n\t[REGSET_SPE] = {\n\t\t.core_note_type = NT_PPC_SPE, .n = 35,\n\t\t.size = sizeof(u32), .align = sizeof(u32),\n\t\t.active = evr_active, .get = evr_get, .set = evr_set\n\t},\n#endif\n};\n\nstatic const struct user_regset_view user_ppc_compat_view = {\n\t.name = \"ppc\", .e_machine = EM_PPC, .ei_osabi = ELF_OSABI,\n\t.regsets = compat_regsets, .n = ARRAY_SIZE(compat_regsets)\n};\n#endif\t/* CONFIG_PPC64 */\n\nconst struct user_regset_view *task_user_regset_view(struct task_struct *task)\n{\n#ifdef CONFIG_PPC64\n\tif (test_tsk_thread_flag(task, TIF_32BIT))\n\t\treturn &user_ppc_compat_view;\n#endif\n\treturn &user_ppc_native_view;\n}\n\n\nvoid user_enable_single_step(struct task_struct *task)\n{\n\tstruct pt_regs *regs = task->thread.regs;\n\n\tif (regs != NULL) {\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n\t\ttask->thread.dbcr0 &= ~DBCR0_BT;\n\t\ttask->thread.dbcr0 |= DBCR0_IDM | DBCR0_IC;\n\t\tregs->msr |= MSR_DE;\n#else\n\t\tregs->msr &= ~MSR_BE;\n\t\tregs->msr |= MSR_SE;\n#endif\n\t}\n\tset_tsk_thread_flag(task, TIF_SINGLESTEP);\n}\n\nvoid user_enable_block_step(struct task_struct *task)\n{\n\tstruct pt_regs *regs = task->thread.regs;\n\n\tif (regs != NULL) {\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n\t\ttask->thread.dbcr0 &= ~DBCR0_IC;\n\t\ttask->thread.dbcr0 = DBCR0_IDM | DBCR0_BT;\n\t\tregs->msr |= MSR_DE;\n#else\n\t\tregs->msr &= ~MSR_SE;\n\t\tregs->msr |= MSR_BE;\n#endif\n\t}\n\tset_tsk_thread_flag(task, TIF_SINGLESTEP);\n}\n\nvoid user_disable_single_step(struct task_struct *task)\n{\n\tstruct pt_regs *regs = task->thread.regs;\n\n\tif (regs != NULL) {\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n\t\t/*\n\t\t * The logic to disable single stepping should be as\n\t\t * simple as turning off the Instruction Complete flag.\n\t\t * And, after doing so, if all debug flags are off, turn\n\t\t * off DBCR0(IDM) and MSR(DE) .... Torez\n\t\t */\n\t\ttask->thread.dbcr0 &= ~DBCR0_IC;\n\t\t/*\n\t\t * Test to see if any of the DBCR_ACTIVE_EVENTS bits are set.\n\t\t */\n\t\tif (!DBCR_ACTIVE_EVENTS(task->thread.dbcr0,\n\t\t\t\t\ttask->thread.dbcr1)) {\n\t\t\t/*\n\t\t\t * All debug events were off.....\n\t\t\t */\n\t\t\ttask->thread.dbcr0 &= ~DBCR0_IDM;\n\t\t\tregs->msr &= ~MSR_DE;\n\t\t}\n#else\n\t\tregs->msr &= ~(MSR_SE | MSR_BE);\n#endif\n\t}\n\tclear_tsk_thread_flag(task, TIF_SINGLESTEP);\n}\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\nvoid ptrace_triggered(struct perf_event *bp, int nmi,\n\t\t      struct perf_sample_data *data, struct pt_regs *regs)\n{\n\tstruct perf_event_attr attr;\n\n\t/*\n\t * Disable the breakpoint request here since ptrace has defined a\n\t * one-shot behaviour for breakpoint exceptions in PPC64.\n\t * The SIGTRAP signal is generated automatically for us in do_dabr().\n\t * We don't have to do anything about that here\n\t */\n\tattr = bp->attr;\n\tattr.disabled = true;\n\tmodify_user_hw_breakpoint(bp, &attr);\n}\n#endif /* CONFIG_HAVE_HW_BREAKPOINT */\n\nint ptrace_set_debugreg(struct task_struct *task, unsigned long addr,\n\t\t\t       unsigned long data)\n{\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\tint ret;\n\tstruct thread_struct *thread = &(task->thread);\n\tstruct perf_event *bp;\n\tstruct perf_event_attr attr;\n#endif /* CONFIG_HAVE_HW_BREAKPOINT */\n\n\t/* For ppc64 we support one DABR and no IABR's at the moment (ppc64).\n\t *  For embedded processors we support one DAC and no IAC's at the\n\t *  moment.\n\t */\n\tif (addr > 0)\n\t\treturn -EINVAL;\n\n\t/* The bottom 3 bits in dabr are flags */\n\tif ((data & ~0x7UL) >= TASK_SIZE)\n\t\treturn -EIO;\n\n#ifndef CONFIG_PPC_ADV_DEBUG_REGS\n\t/* For processors using DABR (i.e. 970), the bottom 3 bits are flags.\n\t *  It was assumed, on previous implementations, that 3 bits were\n\t *  passed together with the data address, fitting the design of the\n\t *  DABR register, as follows:\n\t *\n\t *  bit 0: Read flag\n\t *  bit 1: Write flag\n\t *  bit 2: Breakpoint translation\n\t *\n\t *  Thus, we use them here as so.\n\t */\n\n\t/* Ensure breakpoint translation bit is set */\n\tif (data && !(data & DABR_TRANSLATION))\n\t\treturn -EIO;\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\tif (ptrace_get_breakpoints(task) < 0)\n\t\treturn -ESRCH;\n\n\tbp = thread->ptrace_bps[0];\n\tif ((!data) || !(data & (DABR_DATA_WRITE | DABR_DATA_READ))) {\n\t\tif (bp) {\n\t\t\tunregister_hw_breakpoint(bp);\n\t\t\tthread->ptrace_bps[0] = NULL;\n\t\t}\n\t\tptrace_put_breakpoints(task);\n\t\treturn 0;\n\t}\n\tif (bp) {\n\t\tattr = bp->attr;\n\t\tattr.bp_addr = data & ~HW_BREAKPOINT_ALIGN;\n\t\tarch_bp_generic_fields(data &\n\t\t\t\t\t(DABR_DATA_WRITE | DABR_DATA_READ),\n\t\t\t\t\t\t\t&attr.bp_type);\n\t\tret =  modify_user_hw_breakpoint(bp, &attr);\n\t\tif (ret) {\n\t\t\tptrace_put_breakpoints(task);\n\t\t\treturn ret;\n\t\t}\n\t\tthread->ptrace_bps[0] = bp;\n\t\tptrace_put_breakpoints(task);\n\t\tthread->dabr = data;\n\t\treturn 0;\n\t}\n\n\t/* Create a new breakpoint request if one doesn't exist already */\n\thw_breakpoint_init(&attr);\n\tattr.bp_addr = data & ~HW_BREAKPOINT_ALIGN;\n\tarch_bp_generic_fields(data & (DABR_DATA_WRITE | DABR_DATA_READ),\n\t\t\t\t\t\t\t\t&attr.bp_type);\n\n\tthread->ptrace_bps[0] = bp = register_user_hw_breakpoint(&attr,\n\t\t\t\t\t\t\tptrace_triggered, task);\n\tif (IS_ERR(bp)) {\n\t\tthread->ptrace_bps[0] = NULL;\n\t\tptrace_put_breakpoints(task);\n\t\treturn PTR_ERR(bp);\n\t}\n\n\tptrace_put_breakpoints(task);\n\n#endif /* CONFIG_HAVE_HW_BREAKPOINT */\n\n\t/* Move contents to the DABR register */\n\ttask->thread.dabr = data;\n#else /* CONFIG_PPC_ADV_DEBUG_REGS */\n\t/* As described above, it was assumed 3 bits were passed with the data\n\t *  address, but we will assume only the mode bits will be passed\n\t *  as to not cause alignment restrictions for DAC-based processors.\n\t */\n\n\t/* DAC's hold the whole address without any mode flags */\n\ttask->thread.dac1 = data & ~0x3UL;\n\n\tif (task->thread.dac1 == 0) {\n\t\tdbcr_dac(task) &= ~(DBCR_DAC1R | DBCR_DAC1W);\n\t\tif (!DBCR_ACTIVE_EVENTS(task->thread.dbcr0,\n\t\t\t\t\ttask->thread.dbcr1)) {\n\t\t\ttask->thread.regs->msr &= ~MSR_DE;\n\t\t\ttask->thread.dbcr0 &= ~DBCR0_IDM;\n\t\t}\n\t\treturn 0;\n\t}\n\n\t/* Read or Write bits must be set */\n\n\tif (!(data & 0x3UL))\n\t\treturn -EINVAL;\n\n\t/* Set the Internal Debugging flag (IDM bit 1) for the DBCR0\n\t   register */\n\ttask->thread.dbcr0 |= DBCR0_IDM;\n\n\t/* Check for write and read flags and set DBCR0\n\t   accordingly */\n\tdbcr_dac(task) &= ~(DBCR_DAC1R|DBCR_DAC1W);\n\tif (data & 0x1UL)\n\t\tdbcr_dac(task) |= DBCR_DAC1R;\n\tif (data & 0x2UL)\n\t\tdbcr_dac(task) |= DBCR_DAC1W;\n\ttask->thread.regs->msr |= MSR_DE;\n#endif /* CONFIG_PPC_ADV_DEBUG_REGS */\n\treturn 0;\n}\n\n/*\n * Called by kernel/ptrace.c when detaching..\n *\n * Make sure single step bits etc are not set.\n */\nvoid ptrace_disable(struct task_struct *child)\n{\n\t/* make sure the single step bit is not set. */\n\tuser_disable_single_step(child);\n}\n\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\nstatic long set_intruction_bp(struct task_struct *child,\n\t\t\t      struct ppc_hw_breakpoint *bp_info)\n{\n\tint slot;\n\tint slot1_in_use = ((child->thread.dbcr0 & DBCR0_IAC1) != 0);\n\tint slot2_in_use = ((child->thread.dbcr0 & DBCR0_IAC2) != 0);\n\tint slot3_in_use = ((child->thread.dbcr0 & DBCR0_IAC3) != 0);\n\tint slot4_in_use = ((child->thread.dbcr0 & DBCR0_IAC4) != 0);\n\n\tif (dbcr_iac_range(child) & DBCR_IAC12MODE)\n\t\tslot2_in_use = 1;\n\tif (dbcr_iac_range(child) & DBCR_IAC34MODE)\n\t\tslot4_in_use = 1;\n\n\tif (bp_info->addr >= TASK_SIZE)\n\t\treturn -EIO;\n\n\tif (bp_info->addr_mode != PPC_BREAKPOINT_MODE_EXACT) {\n\n\t\t/* Make sure range is valid. */\n\t\tif (bp_info->addr2 >= TASK_SIZE)\n\t\t\treturn -EIO;\n\n\t\t/* We need a pair of IAC regsisters */\n\t\tif ((!slot1_in_use) && (!slot2_in_use)) {\n\t\t\tslot = 1;\n\t\t\tchild->thread.iac1 = bp_info->addr;\n\t\t\tchild->thread.iac2 = bp_info->addr2;\n\t\t\tchild->thread.dbcr0 |= DBCR0_IAC1;\n\t\t\tif (bp_info->addr_mode ==\n\t\t\t\t\tPPC_BREAKPOINT_MODE_RANGE_EXCLUSIVE)\n\t\t\t\tdbcr_iac_range(child) |= DBCR_IAC12X;\n\t\t\telse\n\t\t\t\tdbcr_iac_range(child) |= DBCR_IAC12I;\n#if CONFIG_PPC_ADV_DEBUG_IACS > 2\n\t\t} else if ((!slot3_in_use) && (!slot4_in_use)) {\n\t\t\tslot = 3;\n\t\t\tchild->thread.iac3 = bp_info->addr;\n\t\t\tchild->thread.iac4 = bp_info->addr2;\n\t\t\tchild->thread.dbcr0 |= DBCR0_IAC3;\n\t\t\tif (bp_info->addr_mode ==\n\t\t\t\t\tPPC_BREAKPOINT_MODE_RANGE_EXCLUSIVE)\n\t\t\t\tdbcr_iac_range(child) |= DBCR_IAC34X;\n\t\t\telse\n\t\t\t\tdbcr_iac_range(child) |= DBCR_IAC34I;\n#endif\n\t\t} else\n\t\t\treturn -ENOSPC;\n\t} else {\n\t\t/* We only need one.  If possible leave a pair free in\n\t\t * case a range is needed later\n\t\t */\n\t\tif (!slot1_in_use) {\n\t\t\t/*\n\t\t\t * Don't use iac1 if iac1-iac2 are free and either\n\t\t\t * iac3 or iac4 (but not both) are free\n\t\t\t */\n\t\t\tif (slot2_in_use || (slot3_in_use == slot4_in_use)) {\n\t\t\t\tslot = 1;\n\t\t\t\tchild->thread.iac1 = bp_info->addr;\n\t\t\t\tchild->thread.dbcr0 |= DBCR0_IAC1;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tif (!slot2_in_use) {\n\t\t\tslot = 2;\n\t\t\tchild->thread.iac2 = bp_info->addr;\n\t\t\tchild->thread.dbcr0 |= DBCR0_IAC2;\n#if CONFIG_PPC_ADV_DEBUG_IACS > 2\n\t\t} else if (!slot3_in_use) {\n\t\t\tslot = 3;\n\t\t\tchild->thread.iac3 = bp_info->addr;\n\t\t\tchild->thread.dbcr0 |= DBCR0_IAC3;\n\t\t} else if (!slot4_in_use) {\n\t\t\tslot = 4;\n\t\t\tchild->thread.iac4 = bp_info->addr;\n\t\t\tchild->thread.dbcr0 |= DBCR0_IAC4;\n#endif\n\t\t} else\n\t\t\treturn -ENOSPC;\n\t}\nout:\n\tchild->thread.dbcr0 |= DBCR0_IDM;\n\tchild->thread.regs->msr |= MSR_DE;\n\n\treturn slot;\n}\n\nstatic int del_instruction_bp(struct task_struct *child, int slot)\n{\n\tswitch (slot) {\n\tcase 1:\n\t\tif ((child->thread.dbcr0 & DBCR0_IAC1) == 0)\n\t\t\treturn -ENOENT;\n\n\t\tif (dbcr_iac_range(child) & DBCR_IAC12MODE) {\n\t\t\t/* address range - clear slots 1 & 2 */\n\t\t\tchild->thread.iac2 = 0;\n\t\t\tdbcr_iac_range(child) &= ~DBCR_IAC12MODE;\n\t\t}\n\t\tchild->thread.iac1 = 0;\n\t\tchild->thread.dbcr0 &= ~DBCR0_IAC1;\n\t\tbreak;\n\tcase 2:\n\t\tif ((child->thread.dbcr0 & DBCR0_IAC2) == 0)\n\t\t\treturn -ENOENT;\n\n\t\tif (dbcr_iac_range(child) & DBCR_IAC12MODE)\n\t\t\t/* used in a range */\n\t\t\treturn -EINVAL;\n\t\tchild->thread.iac2 = 0;\n\t\tchild->thread.dbcr0 &= ~DBCR0_IAC2;\n\t\tbreak;\n#if CONFIG_PPC_ADV_DEBUG_IACS > 2\n\tcase 3:\n\t\tif ((child->thread.dbcr0 & DBCR0_IAC3) == 0)\n\t\t\treturn -ENOENT;\n\n\t\tif (dbcr_iac_range(child) & DBCR_IAC34MODE) {\n\t\t\t/* address range - clear slots 3 & 4 */\n\t\t\tchild->thread.iac4 = 0;\n\t\t\tdbcr_iac_range(child) &= ~DBCR_IAC34MODE;\n\t\t}\n\t\tchild->thread.iac3 = 0;\n\t\tchild->thread.dbcr0 &= ~DBCR0_IAC3;\n\t\tbreak;\n\tcase 4:\n\t\tif ((child->thread.dbcr0 & DBCR0_IAC4) == 0)\n\t\t\treturn -ENOENT;\n\n\t\tif (dbcr_iac_range(child) & DBCR_IAC34MODE)\n\t\t\t/* Used in a range */\n\t\t\treturn -EINVAL;\n\t\tchild->thread.iac4 = 0;\n\t\tchild->thread.dbcr0 &= ~DBCR0_IAC4;\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic int set_dac(struct task_struct *child, struct ppc_hw_breakpoint *bp_info)\n{\n\tint byte_enable =\n\t\t(bp_info->condition_mode >> PPC_BREAKPOINT_CONDITION_BE_SHIFT)\n\t\t& 0xf;\n\tint condition_mode =\n\t\tbp_info->condition_mode & PPC_BREAKPOINT_CONDITION_MODE;\n\tint slot;\n\n\tif (byte_enable && (condition_mode == 0))\n\t\treturn -EINVAL;\n\n\tif (bp_info->addr >= TASK_SIZE)\n\t\treturn -EIO;\n\n\tif ((dbcr_dac(child) & (DBCR_DAC1R | DBCR_DAC1W)) == 0) {\n\t\tslot = 1;\n\t\tif (bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_READ)\n\t\t\tdbcr_dac(child) |= DBCR_DAC1R;\n\t\tif (bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_WRITE)\n\t\t\tdbcr_dac(child) |= DBCR_DAC1W;\n\t\tchild->thread.dac1 = (unsigned long)bp_info->addr;\n#if CONFIG_PPC_ADV_DEBUG_DVCS > 0\n\t\tif (byte_enable) {\n\t\t\tchild->thread.dvc1 =\n\t\t\t\t(unsigned long)bp_info->condition_value;\n\t\t\tchild->thread.dbcr2 |=\n\t\t\t\t((byte_enable << DBCR2_DVC1BE_SHIFT) |\n\t\t\t\t (condition_mode << DBCR2_DVC1M_SHIFT));\n\t\t}\n#endif\n#ifdef CONFIG_PPC_ADV_DEBUG_DAC_RANGE\n\t} else if (child->thread.dbcr2 & DBCR2_DAC12MODE) {\n\t\t/* Both dac1 and dac2 are part of a range */\n\t\treturn -ENOSPC;\n#endif\n\t} else if ((dbcr_dac(child) & (DBCR_DAC2R | DBCR_DAC2W)) == 0) {\n\t\tslot = 2;\n\t\tif (bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_READ)\n\t\t\tdbcr_dac(child) |= DBCR_DAC2R;\n\t\tif (bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_WRITE)\n\t\t\tdbcr_dac(child) |= DBCR_DAC2W;\n\t\tchild->thread.dac2 = (unsigned long)bp_info->addr;\n#if CONFIG_PPC_ADV_DEBUG_DVCS > 0\n\t\tif (byte_enable) {\n\t\t\tchild->thread.dvc2 =\n\t\t\t\t(unsigned long)bp_info->condition_value;\n\t\t\tchild->thread.dbcr2 |=\n\t\t\t\t((byte_enable << DBCR2_DVC2BE_SHIFT) |\n\t\t\t\t (condition_mode << DBCR2_DVC2M_SHIFT));\n\t\t}\n#endif\n\t} else\n\t\treturn -ENOSPC;\n\tchild->thread.dbcr0 |= DBCR0_IDM;\n\tchild->thread.regs->msr |= MSR_DE;\n\n\treturn slot + 4;\n}\n\nstatic int del_dac(struct task_struct *child, int slot)\n{\n\tif (slot == 1) {\n\t\tif ((dbcr_dac(child) & (DBCR_DAC1R | DBCR_DAC1W)) == 0)\n\t\t\treturn -ENOENT;\n\n\t\tchild->thread.dac1 = 0;\n\t\tdbcr_dac(child) &= ~(DBCR_DAC1R | DBCR_DAC1W);\n#ifdef CONFIG_PPC_ADV_DEBUG_DAC_RANGE\n\t\tif (child->thread.dbcr2 & DBCR2_DAC12MODE) {\n\t\t\tchild->thread.dac2 = 0;\n\t\t\tchild->thread.dbcr2 &= ~DBCR2_DAC12MODE;\n\t\t}\n\t\tchild->thread.dbcr2 &= ~(DBCR2_DVC1M | DBCR2_DVC1BE);\n#endif\n#if CONFIG_PPC_ADV_DEBUG_DVCS > 0\n\t\tchild->thread.dvc1 = 0;\n#endif\n\t} else if (slot == 2) {\n\t\tif ((dbcr_dac(child) & (DBCR_DAC2R | DBCR_DAC2W)) == 0)\n\t\t\treturn -ENOENT;\n\n#ifdef CONFIG_PPC_ADV_DEBUG_DAC_RANGE\n\t\tif (child->thread.dbcr2 & DBCR2_DAC12MODE)\n\t\t\t/* Part of a range */\n\t\t\treturn -EINVAL;\n\t\tchild->thread.dbcr2 &= ~(DBCR2_DVC2M | DBCR2_DVC2BE);\n#endif\n#if CONFIG_PPC_ADV_DEBUG_DVCS > 0\n\t\tchild->thread.dvc2 = 0;\n#endif\n\t\tchild->thread.dac2 = 0;\n\t\tdbcr_dac(child) &= ~(DBCR_DAC2R | DBCR_DAC2W);\n\t} else\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n#endif /* CONFIG_PPC_ADV_DEBUG_REGS */\n\n#ifdef CONFIG_PPC_ADV_DEBUG_DAC_RANGE\nstatic int set_dac_range(struct task_struct *child,\n\t\t\t struct ppc_hw_breakpoint *bp_info)\n{\n\tint mode = bp_info->addr_mode & PPC_BREAKPOINT_MODE_MASK;\n\n\t/* We don't allow range watchpoints to be used with DVC */\n\tif (bp_info->condition_mode)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Best effort to verify the address range.  The user/supervisor bits\n\t * prevent trapping in kernel space, but let's fail on an obvious bad\n\t * range.  The simple test on the mask is not fool-proof, and any\n\t * exclusive range will spill over into kernel space.\n\t */\n\tif (bp_info->addr >= TASK_SIZE)\n\t\treturn -EIO;\n\tif (mode == PPC_BREAKPOINT_MODE_MASK) {\n\t\t/*\n\t\t * dac2 is a bitmask.  Don't allow a mask that makes a\n\t\t * kernel space address from a valid dac1 value\n\t\t */\n\t\tif (~((unsigned long)bp_info->addr2) >= TASK_SIZE)\n\t\t\treturn -EIO;\n\t} else {\n\t\t/*\n\t\t * For range breakpoints, addr2 must also be a valid address\n\t\t */\n\t\tif (bp_info->addr2 >= TASK_SIZE)\n\t\t\treturn -EIO;\n\t}\n\n\tif (child->thread.dbcr0 &\n\t    (DBCR0_DAC1R | DBCR0_DAC1W | DBCR0_DAC2R | DBCR0_DAC2W))\n\t\treturn -ENOSPC;\n\n\tif (bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_READ)\n\t\tchild->thread.dbcr0 |= (DBCR0_DAC1R | DBCR0_IDM);\n\tif (bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_WRITE)\n\t\tchild->thread.dbcr0 |= (DBCR0_DAC1W | DBCR0_IDM);\n\tchild->thread.dac1 = bp_info->addr;\n\tchild->thread.dac2 = bp_info->addr2;\n\tif (mode == PPC_BREAKPOINT_MODE_RANGE_INCLUSIVE)\n\t\tchild->thread.dbcr2  |= DBCR2_DAC12M;\n\telse if (mode == PPC_BREAKPOINT_MODE_RANGE_EXCLUSIVE)\n\t\tchild->thread.dbcr2  |= DBCR2_DAC12MX;\n\telse\t/* PPC_BREAKPOINT_MODE_MASK */\n\t\tchild->thread.dbcr2  |= DBCR2_DAC12MM;\n\tchild->thread.regs->msr |= MSR_DE;\n\n\treturn 5;\n}\n#endif /* CONFIG_PPC_ADV_DEBUG_DAC_RANGE */\n\nstatic long ppc_set_hwdebug(struct task_struct *child,\n\t\t     struct ppc_hw_breakpoint *bp_info)\n{\n#ifndef CONFIG_PPC_ADV_DEBUG_REGS\n\tunsigned long dabr;\n#endif\n\n\tif (bp_info->version != 1)\n\t\treturn -ENOTSUPP;\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n\t/*\n\t * Check for invalid flags and combinations\n\t */\n\tif ((bp_info->trigger_type == 0) ||\n\t    (bp_info->trigger_type & ~(PPC_BREAKPOINT_TRIGGER_EXECUTE |\n\t\t\t\t       PPC_BREAKPOINT_TRIGGER_RW)) ||\n\t    (bp_info->addr_mode & ~PPC_BREAKPOINT_MODE_MASK) ||\n\t    (bp_info->condition_mode &\n\t     ~(PPC_BREAKPOINT_CONDITION_MODE |\n\t       PPC_BREAKPOINT_CONDITION_BE_ALL)))\n\t\treturn -EINVAL;\n#if CONFIG_PPC_ADV_DEBUG_DVCS == 0\n\tif (bp_info->condition_mode != PPC_BREAKPOINT_CONDITION_NONE)\n\t\treturn -EINVAL;\n#endif\n\n\tif (bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_EXECUTE) {\n\t\tif ((bp_info->trigger_type != PPC_BREAKPOINT_TRIGGER_EXECUTE) ||\n\t\t    (bp_info->condition_mode != PPC_BREAKPOINT_CONDITION_NONE))\n\t\t\treturn -EINVAL;\n\t\treturn set_intruction_bp(child, bp_info);\n\t}\n\tif (bp_info->addr_mode == PPC_BREAKPOINT_MODE_EXACT)\n\t\treturn set_dac(child, bp_info);\n\n#ifdef CONFIG_PPC_ADV_DEBUG_DAC_RANGE\n\treturn set_dac_range(child, bp_info);\n#else\n\treturn -EINVAL;\n#endif\n#else /* !CONFIG_PPC_ADV_DEBUG_DVCS */\n\t/*\n\t * We only support one data breakpoint\n\t */\n\tif ((bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_RW) == 0 ||\n\t    (bp_info->trigger_type & ~PPC_BREAKPOINT_TRIGGER_RW) != 0 ||\n\t    bp_info->addr_mode != PPC_BREAKPOINT_MODE_EXACT ||\n\t    bp_info->condition_mode != PPC_BREAKPOINT_CONDITION_NONE)\n\t\treturn -EINVAL;\n\n\tif (child->thread.dabr)\n\t\treturn -ENOSPC;\n\n\tif ((unsigned long)bp_info->addr >= TASK_SIZE)\n\t\treturn -EIO;\n\n\tdabr = (unsigned long)bp_info->addr & ~7UL;\n\tdabr |= DABR_TRANSLATION;\n\tif (bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_READ)\n\t\tdabr |= DABR_DATA_READ;\n\tif (bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_WRITE)\n\t\tdabr |= DABR_DATA_WRITE;\n\n\tchild->thread.dabr = dabr;\n\n\treturn 1;\n#endif /* !CONFIG_PPC_ADV_DEBUG_DVCS */\n}\n\nstatic long ppc_del_hwdebug(struct task_struct *child, long addr, long data)\n{\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n\tint rc;\n\n\tif (data <= 4)\n\t\trc = del_instruction_bp(child, (int)data);\n\telse\n\t\trc = del_dac(child, (int)data - 4);\n\n\tif (!rc) {\n\t\tif (!DBCR_ACTIVE_EVENTS(child->thread.dbcr0,\n\t\t\t\t\tchild->thread.dbcr1)) {\n\t\t\tchild->thread.dbcr0 &= ~DBCR0_IDM;\n\t\t\tchild->thread.regs->msr &= ~MSR_DE;\n\t\t}\n\t}\n\treturn rc;\n#else\n\tif (data != 1)\n\t\treturn -EINVAL;\n\tif (child->thread.dabr == 0)\n\t\treturn -ENOENT;\n\n\tchild->thread.dabr = 0;\n\n\treturn 0;\n#endif\n}\n\n/*\n * Here are the old \"legacy\" powerpc specific getregs/setregs ptrace calls,\n * we mark them as obsolete now, they will be removed in a future version\n */\nstatic long arch_ptrace_old(struct task_struct *child, long request,\n\t\t\t    unsigned long addr, unsigned long data)\n{\n\tvoid __user *datavp = (void __user *) data;\n\n\tswitch (request) {\n\tcase PPC_PTRACE_GETREGS:\t/* Get GPRs 0 - 31. */\n\t\treturn copy_regset_to_user(child, &user_ppc_native_view,\n\t\t\t\t\t   REGSET_GPR, 0, 32 * sizeof(long),\n\t\t\t\t\t   datavp);\n\n\tcase PPC_PTRACE_SETREGS:\t/* Set GPRs 0 - 31. */\n\t\treturn copy_regset_from_user(child, &user_ppc_native_view,\n\t\t\t\t\t     REGSET_GPR, 0, 32 * sizeof(long),\n\t\t\t\t\t     datavp);\n\n\tcase PPC_PTRACE_GETFPREGS:\t/* Get FPRs 0 - 31. */\n\t\treturn copy_regset_to_user(child, &user_ppc_native_view,\n\t\t\t\t\t   REGSET_FPR, 0, 32 * sizeof(double),\n\t\t\t\t\t   datavp);\n\n\tcase PPC_PTRACE_SETFPREGS:\t/* Set FPRs 0 - 31. */\n\t\treturn copy_regset_from_user(child, &user_ppc_native_view,\n\t\t\t\t\t     REGSET_FPR, 0, 32 * sizeof(double),\n\t\t\t\t\t     datavp);\n\t}\n\n\treturn -EPERM;\n}\n\nlong arch_ptrace(struct task_struct *child, long request,\n\t\t unsigned long addr, unsigned long data)\n{\n\tint ret = -EPERM;\n\tvoid __user *datavp = (void __user *) data;\n\tunsigned long __user *datalp = datavp;\n\n\tswitch (request) {\n\t/* read the word at location addr in the USER area. */\n\tcase PTRACE_PEEKUSR: {\n\t\tunsigned long index, tmp;\n\n\t\tret = -EIO;\n\t\t/* convert to index and check */\n#ifdef CONFIG_PPC32\n\t\tindex = addr >> 2;\n\t\tif ((addr & 3) || (index > PT_FPSCR)\n\t\t    || (child->thread.regs == NULL))\n#else\n\t\tindex = addr >> 3;\n\t\tif ((addr & 7) || (index > PT_FPSCR))\n#endif\n\t\t\tbreak;\n\n\t\tCHECK_FULL_REGS(child->thread.regs);\n\t\tif (index < PT_FPR0) {\n\t\t\ttmp = ptrace_get_reg(child, (int) index);\n\t\t} else {\n\t\t\tflush_fp_to_thread(child);\n\t\t\ttmp = ((unsigned long *)child->thread.fpr)\n\t\t\t\t[TS_FPRWIDTH * (index - PT_FPR0)];\n\t\t}\n\t\tret = put_user(tmp, datalp);\n\t\tbreak;\n\t}\n\n\t/* write the word at location addr in the USER area */\n\tcase PTRACE_POKEUSR: {\n\t\tunsigned long index;\n\n\t\tret = -EIO;\n\t\t/* convert to index and check */\n#ifdef CONFIG_PPC32\n\t\tindex = addr >> 2;\n\t\tif ((addr & 3) || (index > PT_FPSCR)\n\t\t    || (child->thread.regs == NULL))\n#else\n\t\tindex = addr >> 3;\n\t\tif ((addr & 7) || (index > PT_FPSCR))\n#endif\n\t\t\tbreak;\n\n\t\tCHECK_FULL_REGS(child->thread.regs);\n\t\tif (index < PT_FPR0) {\n\t\t\tret = ptrace_put_reg(child, index, data);\n\t\t} else {\n\t\t\tflush_fp_to_thread(child);\n\t\t\t((unsigned long *)child->thread.fpr)\n\t\t\t\t[TS_FPRWIDTH * (index - PT_FPR0)] = data;\n\t\t\tret = 0;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase PPC_PTRACE_GETHWDBGINFO: {\n\t\tstruct ppc_debug_info dbginfo;\n\n\t\tdbginfo.version = 1;\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n\t\tdbginfo.num_instruction_bps = CONFIG_PPC_ADV_DEBUG_IACS;\n\t\tdbginfo.num_data_bps = CONFIG_PPC_ADV_DEBUG_DACS;\n\t\tdbginfo.num_condition_regs = CONFIG_PPC_ADV_DEBUG_DVCS;\n\t\tdbginfo.data_bp_alignment = 4;\n\t\tdbginfo.sizeof_condition = 4;\n\t\tdbginfo.features = PPC_DEBUG_FEATURE_INSN_BP_RANGE |\n\t\t\t\t   PPC_DEBUG_FEATURE_INSN_BP_MASK;\n#ifdef CONFIG_PPC_ADV_DEBUG_DAC_RANGE\n\t\tdbginfo.features |=\n\t\t\t\t   PPC_DEBUG_FEATURE_DATA_BP_RANGE |\n\t\t\t\t   PPC_DEBUG_FEATURE_DATA_BP_MASK;\n#endif\n#else /* !CONFIG_PPC_ADV_DEBUG_REGS */\n\t\tdbginfo.num_instruction_bps = 0;\n\t\tdbginfo.num_data_bps = 1;\n\t\tdbginfo.num_condition_regs = 0;\n#ifdef CONFIG_PPC64\n\t\tdbginfo.data_bp_alignment = 8;\n#else\n\t\tdbginfo.data_bp_alignment = 4;\n#endif\n\t\tdbginfo.sizeof_condition = 0;\n\t\tdbginfo.features = 0;\n#endif /* CONFIG_PPC_ADV_DEBUG_REGS */\n\n\t\tif (!access_ok(VERIFY_WRITE, datavp,\n\t\t\t       sizeof(struct ppc_debug_info)))\n\t\t\treturn -EFAULT;\n\t\tret = __copy_to_user(datavp, &dbginfo,\n\t\t\t\t     sizeof(struct ppc_debug_info)) ?\n\t\t      -EFAULT : 0;\n\t\tbreak;\n\t}\n\n\tcase PPC_PTRACE_SETHWDEBUG: {\n\t\tstruct ppc_hw_breakpoint bp_info;\n\n\t\tif (!access_ok(VERIFY_READ, datavp,\n\t\t\t       sizeof(struct ppc_hw_breakpoint)))\n\t\t\treturn -EFAULT;\n\t\tret = __copy_from_user(&bp_info, datavp,\n\t\t\t\t       sizeof(struct ppc_hw_breakpoint)) ?\n\t\t      -EFAULT : 0;\n\t\tif (!ret)\n\t\t\tret = ppc_set_hwdebug(child, &bp_info);\n\t\tbreak;\n\t}\n\n\tcase PPC_PTRACE_DELHWDEBUG: {\n\t\tret = ppc_del_hwdebug(child, addr, data);\n\t\tbreak;\n\t}\n\n\tcase PTRACE_GET_DEBUGREG: {\n\t\tret = -EINVAL;\n\t\t/* We only support one DABR and no IABRS at the moment */\n\t\tif (addr > 0)\n\t\t\tbreak;\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n\t\tret = put_user(child->thread.dac1, datalp);\n#else\n\t\tret = put_user(child->thread.dabr, datalp);\n#endif\n\t\tbreak;\n\t}\n\n\tcase PTRACE_SET_DEBUGREG:\n\t\tret = ptrace_set_debugreg(child, addr, data);\n\t\tbreak;\n\n#ifdef CONFIG_PPC64\n\tcase PTRACE_GETREGS64:\n#endif\n\tcase PTRACE_GETREGS:\t/* Get all pt_regs from the child. */\n\t\treturn copy_regset_to_user(child, &user_ppc_native_view,\n\t\t\t\t\t   REGSET_GPR,\n\t\t\t\t\t   0, sizeof(struct pt_regs),\n\t\t\t\t\t   datavp);\n\n#ifdef CONFIG_PPC64\n\tcase PTRACE_SETREGS64:\n#endif\n\tcase PTRACE_SETREGS:\t/* Set all gp regs in the child. */\n\t\treturn copy_regset_from_user(child, &user_ppc_native_view,\n\t\t\t\t\t     REGSET_GPR,\n\t\t\t\t\t     0, sizeof(struct pt_regs),\n\t\t\t\t\t     datavp);\n\n\tcase PTRACE_GETFPREGS: /* Get the child FPU state (FPR0...31 + FPSCR) */\n\t\treturn copy_regset_to_user(child, &user_ppc_native_view,\n\t\t\t\t\t   REGSET_FPR,\n\t\t\t\t\t   0, sizeof(elf_fpregset_t),\n\t\t\t\t\t   datavp);\n\n\tcase PTRACE_SETFPREGS: /* Set the child FPU state (FPR0...31 + FPSCR) */\n\t\treturn copy_regset_from_user(child, &user_ppc_native_view,\n\t\t\t\t\t     REGSET_FPR,\n\t\t\t\t\t     0, sizeof(elf_fpregset_t),\n\t\t\t\t\t     datavp);\n\n#ifdef CONFIG_ALTIVEC\n\tcase PTRACE_GETVRREGS:\n\t\treturn copy_regset_to_user(child, &user_ppc_native_view,\n\t\t\t\t\t   REGSET_VMX,\n\t\t\t\t\t   0, (33 * sizeof(vector128) +\n\t\t\t\t\t       sizeof(u32)),\n\t\t\t\t\t   datavp);\n\n\tcase PTRACE_SETVRREGS:\n\t\treturn copy_regset_from_user(child, &user_ppc_native_view,\n\t\t\t\t\t     REGSET_VMX,\n\t\t\t\t\t     0, (33 * sizeof(vector128) +\n\t\t\t\t\t\t sizeof(u32)),\n\t\t\t\t\t     datavp);\n#endif\n#ifdef CONFIG_VSX\n\tcase PTRACE_GETVSRREGS:\n\t\treturn copy_regset_to_user(child, &user_ppc_native_view,\n\t\t\t\t\t   REGSET_VSX,\n\t\t\t\t\t   0, 32 * sizeof(double),\n\t\t\t\t\t   datavp);\n\n\tcase PTRACE_SETVSRREGS:\n\t\treturn copy_regset_from_user(child, &user_ppc_native_view,\n\t\t\t\t\t     REGSET_VSX,\n\t\t\t\t\t     0, 32 * sizeof(double),\n\t\t\t\t\t     datavp);\n#endif\n#ifdef CONFIG_SPE\n\tcase PTRACE_GETEVRREGS:\n\t\t/* Get the child spe register state. */\n\t\treturn copy_regset_to_user(child, &user_ppc_native_view,\n\t\t\t\t\t   REGSET_SPE, 0, 35 * sizeof(u32),\n\t\t\t\t\t   datavp);\n\n\tcase PTRACE_SETEVRREGS:\n\t\t/* Set the child spe register state. */\n\t\treturn copy_regset_from_user(child, &user_ppc_native_view,\n\t\t\t\t\t     REGSET_SPE, 0, 35 * sizeof(u32),\n\t\t\t\t\t     datavp);\n#endif\n\n\t/* Old reverse args ptrace callss */\n\tcase PPC_PTRACE_GETREGS: /* Get GPRs 0 - 31. */\n\tcase PPC_PTRACE_SETREGS: /* Set GPRs 0 - 31. */\n\tcase PPC_PTRACE_GETFPREGS: /* Get FPRs 0 - 31. */\n\tcase PPC_PTRACE_SETFPREGS: /* Get FPRs 0 - 31. */\n\t\tret = arch_ptrace_old(child, request, addr, data);\n\t\tbreak;\n\n\tdefault:\n\t\tret = ptrace_request(child, request, addr, data);\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\n/*\n * We must return the syscall number to actually look up in the table.\n * This can be -1L to skip running any syscall at all.\n */\nlong do_syscall_trace_enter(struct pt_regs *regs)\n{\n\tlong ret = 0;\n\n\tsecure_computing(regs->gpr[0]);\n\n\tif (test_thread_flag(TIF_SYSCALL_TRACE) &&\n\t    tracehook_report_syscall_entry(regs))\n\t\t/*\n\t\t * Tracing decided this syscall should not happen.\n\t\t * We'll return a bogus call number to get an ENOSYS\n\t\t * error, but leave the original number in regs->gpr[0].\n\t\t */\n\t\tret = -1L;\n\n\tif (unlikely(test_thread_flag(TIF_SYSCALL_TRACEPOINT)))\n\t\ttrace_sys_enter(regs, regs->gpr[0]);\n\n\tif (unlikely(current->audit_context)) {\n#ifdef CONFIG_PPC64\n\t\tif (!is_32bit_task())\n\t\t\taudit_syscall_entry(AUDIT_ARCH_PPC64,\n\t\t\t\t\t    regs->gpr[0],\n\t\t\t\t\t    regs->gpr[3], regs->gpr[4],\n\t\t\t\t\t    regs->gpr[5], regs->gpr[6]);\n\t\telse\n#endif\n\t\t\taudit_syscall_entry(AUDIT_ARCH_PPC,\n\t\t\t\t\t    regs->gpr[0],\n\t\t\t\t\t    regs->gpr[3] & 0xffffffff,\n\t\t\t\t\t    regs->gpr[4] & 0xffffffff,\n\t\t\t\t\t    regs->gpr[5] & 0xffffffff,\n\t\t\t\t\t    regs->gpr[6] & 0xffffffff);\n\t}\n\n\treturn ret ?: regs->gpr[0];\n}\n\nvoid do_syscall_trace_leave(struct pt_regs *regs)\n{\n\tint step;\n\n\tif (unlikely(current->audit_context))\n\t\taudit_syscall_exit((regs->ccr&0x10000000)?AUDITSC_FAILURE:AUDITSC_SUCCESS,\n\t\t\t\t   regs->result);\n\n\tif (unlikely(test_thread_flag(TIF_SYSCALL_TRACEPOINT)))\n\t\ttrace_sys_exit(regs, regs->result);\n\n\tstep = test_thread_flag(TIF_SINGLESTEP);\n\tif (step || test_thread_flag(TIF_SYSCALL_TRACE))\n\t\ttracehook_report_syscall_exit(regs, step);\n}\n", "/*\n *  PowerPC version\n *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)\n *\n *  Derived from \"arch/i386/mm/fault.c\"\n *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds\n *\n *  Modified by Cort Dougan and Paul Mackerras.\n *\n *  Modified for PPC64 by Dave Engebretsen (engebret@ibm.com)\n *\n *  This program is free software; you can redistribute it and/or\n *  modify it under the terms of the GNU General Public License\n *  as published by the Free Software Foundation; either version\n *  2 of the License, or (at your option) any later version.\n */\n\n#include <linux/signal.h>\n#include <linux/sched.h>\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/ptrace.h>\n#include <linux/mman.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/highmem.h>\n#include <linux/module.h>\n#include <linux/kprobes.h>\n#include <linux/kdebug.h>\n#include <linux/perf_event.h>\n#include <linux/magic.h>\n\n#include <asm/firmware.h>\n#include <asm/page.h>\n#include <asm/pgtable.h>\n#include <asm/mmu.h>\n#include <asm/mmu_context.h>\n#include <asm/system.h>\n#include <asm/uaccess.h>\n#include <asm/tlbflush.h>\n#include <asm/siginfo.h>\n#include <mm/mmu_decl.h>\n\n#ifdef CONFIG_KPROBES\nstatic inline int notify_page_fault(struct pt_regs *regs)\n{\n\tint ret = 0;\n\n\t/* kprobe_running() needs smp_processor_id() */\n\tif (!user_mode(regs)) {\n\t\tpreempt_disable();\n\t\tif (kprobe_running() && kprobe_fault_handler(regs, 11))\n\t\t\tret = 1;\n\t\tpreempt_enable();\n\t}\n\n\treturn ret;\n}\n#else\nstatic inline int notify_page_fault(struct pt_regs *regs)\n{\n\treturn 0;\n}\n#endif\n\n/*\n * Check whether the instruction at regs->nip is a store using\n * an update addressing form which will update r1.\n */\nstatic int store_updates_sp(struct pt_regs *regs)\n{\n\tunsigned int inst;\n\n\tif (get_user(inst, (unsigned int __user *)regs->nip))\n\t\treturn 0;\n\t/* check for 1 in the rA field */\n\tif (((inst >> 16) & 0x1f) != 1)\n\t\treturn 0;\n\t/* check major opcode */\n\tswitch (inst >> 26) {\n\tcase 37:\t/* stwu */\n\tcase 39:\t/* stbu */\n\tcase 45:\t/* sthu */\n\tcase 53:\t/* stfsu */\n\tcase 55:\t/* stfdu */\n\t\treturn 1;\n\tcase 62:\t/* std or stdu */\n\t\treturn (inst & 3) == 1;\n\tcase 31:\n\t\t/* check minor opcode */\n\t\tswitch ((inst >> 1) & 0x3ff) {\n\t\tcase 181:\t/* stdux */\n\t\tcase 183:\t/* stwux */\n\t\tcase 247:\t/* stbux */\n\t\tcase 439:\t/* sthux */\n\t\tcase 695:\t/* stfsux */\n\t\tcase 759:\t/* stfdux */\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/*\n * For 600- and 800-family processors, the error_code parameter is DSISR\n * for a data fault, SRR1 for an instruction fault. For 400-family processors\n * the error_code parameter is ESR for a data fault, 0 for an instruction\n * fault.\n * For 64-bit processors, the error_code parameter is\n *  - DSISR for a non-SLB data access fault,\n *  - SRR1 & 0x08000000 for a non-SLB instruction access fault\n *  - 0 any SLB fault.\n *\n * The return value is 0 if the fault was handled, or the signal\n * number if this is a kernel fault that can't be handled here.\n */\nint __kprobes do_page_fault(struct pt_regs *regs, unsigned long address,\n\t\t\t    unsigned long error_code)\n{\n\tstruct vm_area_struct * vma;\n\tstruct mm_struct *mm = current->mm;\n\tsiginfo_t info;\n\tint code = SEGV_MAPERR;\n\tint is_write = 0, ret;\n\tint trap = TRAP(regs);\n \tint is_exec = trap == 0x400;\n\n#if !(defined(CONFIG_4xx) || defined(CONFIG_BOOKE))\n\t/*\n\t * Fortunately the bit assignments in SRR1 for an instruction\n\t * fault and DSISR for a data fault are mostly the same for the\n\t * bits we are interested in.  But there are some bits which\n\t * indicate errors in DSISR but can validly be set in SRR1.\n\t */\n\tif (trap == 0x400)\n\t\terror_code &= 0x48200000;\n\telse\n\t\tis_write = error_code & DSISR_ISSTORE;\n#else\n\tis_write = error_code & ESR_DST;\n#endif /* CONFIG_4xx || CONFIG_BOOKE */\n\n\tif (notify_page_fault(regs))\n\t\treturn 0;\n\n\tif (unlikely(debugger_fault_handler(regs)))\n\t\treturn 0;\n\n\t/* On a kernel SLB miss we can only check for a valid exception entry */\n\tif (!user_mode(regs) && (address >= TASK_SIZE))\n\t\treturn SIGSEGV;\n\n#if !(defined(CONFIG_4xx) || defined(CONFIG_BOOKE) || \\\n\t\t\t     defined(CONFIG_PPC_BOOK3S_64))\n  \tif (error_code & DSISR_DABRMATCH) {\n\t\t/* DABR match */\n\t\tdo_dabr(regs, address, error_code);\n\t\treturn 0;\n\t}\n#endif\n\n\tif (in_atomic() || mm == NULL) {\n\t\tif (!user_mode(regs))\n\t\t\treturn SIGSEGV;\n\t\t/* in_atomic() in user mode is really bad,\n\t\t   as is current->mm == NULL. */\n\t\tprintk(KERN_EMERG \"Page fault in user mode with \"\n\t\t       \"in_atomic() = %d mm = %p\\n\", in_atomic(), mm);\n\t\tprintk(KERN_EMERG \"NIP = %lx  MSR = %lx\\n\",\n\t\t       regs->nip, regs->msr);\n\t\tdie(\"Weird page fault\", regs, SIGSEGV);\n\t}\n\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);\n\n\t/* When running in the kernel we expect faults to occur only to\n\t * addresses in user space.  All other faults represent errors in the\n\t * kernel and should generate an OOPS.  Unfortunately, in the case of an\n\t * erroneous fault occurring in a code path which already holds mmap_sem\n\t * we will deadlock attempting to validate the fault against the\n\t * address space.  Luckily the kernel only validly references user\n\t * space from well defined areas of code, which are listed in the\n\t * exceptions table.\n\t *\n\t * As the vast majority of faults will be valid we will only perform\n\t * the source reference check when there is a possibility of a deadlock.\n\t * Attempt to lock the address space, if we cannot we then validate the\n\t * source.  If this is invalid we can skip the address space check,\n\t * thus avoiding the deadlock.\n\t */\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tif (!user_mode(regs) && !search_exception_tables(regs->nip))\n\t\t\tgoto bad_area_nosemaphore;\n\n\t\tdown_read(&mm->mmap_sem);\n\t}\n\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\n\t/*\n\t * N.B. The POWER/Open ABI allows programs to access up to\n\t * 288 bytes below the stack pointer.\n\t * The kernel signal delivery code writes up to about 1.5kB\n\t * below the stack pointer (r1) before decrementing it.\n\t * The exec code can write slightly over 640kB to the stack\n\t * before setting the user r1.  Thus we allow the stack to\n\t * expand to 1MB without further checks.\n\t */\n\tif (address + 0x100000 < vma->vm_end) {\n\t\t/* get user regs even if this fault is in kernel mode */\n\t\tstruct pt_regs *uregs = current->thread.regs;\n\t\tif (uregs == NULL)\n\t\t\tgoto bad_area;\n\n\t\t/*\n\t\t * A user-mode access to an address a long way below\n\t\t * the stack pointer is only valid if the instruction\n\t\t * is one which would update the stack pointer to the\n\t\t * address accessed if the instruction completed,\n\t\t * i.e. either stwu rs,n(r1) or stwux rs,r1,rb\n\t\t * (or the byte, halfword, float or double forms).\n\t\t *\n\t\t * If we don't check this then any write to the area\n\t\t * between the last mapped region and the stack will\n\t\t * expand the stack rather than segfaulting.\n\t\t */\n\t\tif (address + 2048 < uregs->gpr[1]\n\t\t    && (!user_mode(regs) || !store_updates_sp(regs)))\n\t\t\tgoto bad_area;\n\t}\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n\ngood_area:\n\tcode = SEGV_ACCERR;\n#if defined(CONFIG_6xx)\n\tif (error_code & 0x95700000)\n\t\t/* an error such as lwarx to I/O controller space,\n\t\t   address matching DABR, eciwx, etc. */\n\t\tgoto bad_area;\n#endif /* CONFIG_6xx */\n#if defined(CONFIG_8xx)\n\t/* 8xx sometimes need to load a invalid/non-present TLBs.\n\t * These must be invalidated separately as linux mm don't.\n\t */\n\tif (error_code & 0x40000000) /* no translation? */\n\t\t_tlbil_va(address, 0, 0, 0);\n\n        /* The MPC8xx seems to always set 0x80000000, which is\n         * \"undefined\".  Of those that can be set, this is the only\n         * one which seems bad.\n         */\n\tif (error_code & 0x10000000)\n                /* Guarded storage error. */\n\t\tgoto bad_area;\n#endif /* CONFIG_8xx */\n\n\tif (is_exec) {\n#ifdef CONFIG_PPC_STD_MMU\n\t\t/* Protection fault on exec go straight to failure on\n\t\t * Hash based MMUs as they either don't support per-page\n\t\t * execute permission, or if they do, it's handled already\n\t\t * at the hash level. This test would probably have to\n\t\t * be removed if we change the way this works to make hash\n\t\t * processors use the same I/D cache coherency mechanism\n\t\t * as embedded.\n\t\t */\n\t\tif (error_code & DSISR_PROTFAULT)\n\t\t\tgoto bad_area;\n#endif /* CONFIG_PPC_STD_MMU */\n\n\t\t/*\n\t\t * Allow execution from readable areas if the MMU does not\n\t\t * provide separate controls over reading and executing.\n\t\t *\n\t\t * Note: That code used to not be enabled for 4xx/BookE.\n\t\t * It is now as I/D cache coherency for these is done at\n\t\t * set_pte_at() time and I see no reason why the test\n\t\t * below wouldn't be valid on those processors. This -may-\n\t\t * break programs compiled with a really old ABI though.\n\t\t */\n\t\tif (!(vma->vm_flags & VM_EXEC) &&\n\t\t    (cpu_has_feature(CPU_FTR_NOEXECUTE) ||\n\t\t     !(vma->vm_flags & (VM_READ | VM_WRITE))))\n\t\t\tgoto bad_area;\n\t/* a write */\n\t} else if (is_write) {\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t/* a read */\n\t} else {\n\t\t/* protection fault */\n\t\tif (error_code & 0x08000000)\n\t\t\tgoto bad_area;\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tgoto bad_area;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tret = handle_mm_fault(mm, vma, address, is_write ? FAULT_FLAG_WRITE : 0);\n\tif (unlikely(ret & VM_FAULT_ERROR)) {\n\t\tif (ret & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (ret & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (ret & VM_FAULT_MAJOR) {\n\t\tcurrent->maj_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,\n\t\t\t\t     regs, address);\n#ifdef CONFIG_PPC_SMLPAR\n\t\tif (firmware_has_feature(FW_FEATURE_CMO)) {\n\t\t\tpreempt_disable();\n\t\t\tget_lppaca()->page_ins += (1 << PAGE_FACTOR);\n\t\t\tpreempt_enable();\n\t\t}\n#endif\n\t} else {\n\t\tcurrent->min_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,\n\t\t\t\t     regs, address);\n\t}\n\tup_read(&mm->mmap_sem);\n\treturn 0;\n\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\t/* User mode accesses cause a SIGSEGV */\n\tif (user_mode(regs)) {\n\t\t_exception(SIGSEGV, regs, code, address);\n\t\treturn 0;\n\t}\n\n\tif (is_exec && (error_code & DSISR_PROTFAULT)\n\t    && printk_ratelimit())\n\t\tprintk(KERN_CRIT \"kernel tried to execute NX-protected\"\n\t\t       \" page (%lx) - exploit attempt? (uid: %d)\\n\",\n\t\t       address, current_uid());\n\n\treturn SIGSEGV;\n\n/*\n * We ran out of memory, or some other thing happened to us that made\n * us unable to handle the page fault gracefully.\n */\nout_of_memory:\n\tup_read(&mm->mmap_sem);\n\tif (!user_mode(regs))\n\t\treturn SIGKILL;\n\tpagefault_out_of_memory();\n\treturn 0;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\tif (user_mode(regs)) {\n\t\tinfo.si_signo = SIGBUS;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code = BUS_ADRERR;\n\t\tinfo.si_addr = (void __user *)address;\n\t\tforce_sig_info(SIGBUS, &info, current);\n\t\treturn 0;\n\t}\n\treturn SIGBUS;\n}\n\n/*\n * bad_page_fault is called when we have a bad access from the kernel.\n * It is called from the DSI and ISI handlers in head.S and from some\n * of the procedures in traps.c.\n */\nvoid bad_page_fault(struct pt_regs *regs, unsigned long address, int sig)\n{\n\tconst struct exception_table_entry *entry;\n\tunsigned long *stackend;\n\n\t/* Are we prepared to handle this fault?  */\n\tif ((entry = search_exception_tables(regs->nip)) != NULL) {\n\t\tregs->nip = entry->fixup;\n\t\treturn;\n\t}\n\n\t/* kernel has accessed a bad area */\n\n\tswitch (regs->trap) {\n\tcase 0x300:\n\tcase 0x380:\n\t\tprintk(KERN_ALERT \"Unable to handle kernel paging request for \"\n\t\t\t\"data at address 0x%08lx\\n\", regs->dar);\n\t\tbreak;\n\tcase 0x400:\n\tcase 0x480:\n\t\tprintk(KERN_ALERT \"Unable to handle kernel paging request for \"\n\t\t\t\"instruction fetch\\n\");\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ALERT \"Unable to handle kernel paging request for \"\n\t\t\t\"unknown fault\\n\");\n\t\tbreak;\n\t}\n\tprintk(KERN_ALERT \"Faulting instruction address: 0x%08lx\\n\",\n\t\tregs->nip);\n\n\tstackend = end_of_stack(current);\n\tif (current != &init_task && *stackend != STACK_END_MAGIC)\n\t\tprintk(KERN_ALERT \"Thread overran stack, or stack corrupted\\n\");\n\n\tdie(\"Kernel access of bad area\", regs, sig);\n}\n", "/*\n *  arch/s390/mm/fault.c\n *\n *  S390 version\n *    Copyright (C) 1999 IBM Deutschland Entwicklung GmbH, IBM Corporation\n *    Author(s): Hartmut Penner (hp@de.ibm.com)\n *               Ulrich Weigand (uweigand@de.ibm.com)\n *\n *  Derived from \"arch/i386/mm/fault.c\"\n *    Copyright (C) 1995  Linus Torvalds\n */\n\n#include <linux/kernel_stat.h>\n#include <linux/perf_event.h>\n#include <linux/signal.h>\n#include <linux/sched.h>\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/ptrace.h>\n#include <linux/mman.h>\n#include <linux/mm.h>\n#include <linux/compat.h>\n#include <linux/smp.h>\n#include <linux/kdebug.h>\n#include <linux/init.h>\n#include <linux/console.h>\n#include <linux/module.h>\n#include <linux/hardirq.h>\n#include <linux/kprobes.h>\n#include <linux/uaccess.h>\n#include <linux/hugetlb.h>\n#include <asm/asm-offsets.h>\n#include <asm/system.h>\n#include <asm/pgtable.h>\n#include <asm/irq.h>\n#include <asm/mmu_context.h>\n#include <asm/compat.h>\n#include \"../kernel/entry.h\"\n\n#ifndef CONFIG_64BIT\n#define __FAIL_ADDR_MASK 0x7ffff000\n#define __SUBCODE_MASK 0x0200\n#define __PF_RES_FIELD 0ULL\n#else /* CONFIG_64BIT */\n#define __FAIL_ADDR_MASK -4096L\n#define __SUBCODE_MASK 0x0600\n#define __PF_RES_FIELD 0x8000000000000000ULL\n#endif /* CONFIG_64BIT */\n\n#define VM_FAULT_BADCONTEXT\t0x010000\n#define VM_FAULT_BADMAP\t\t0x020000\n#define VM_FAULT_BADACCESS\t0x040000\n\nstatic unsigned long store_indication;\n\nvoid fault_init(void)\n{\n\tif (test_facility(2) && test_facility(75))\n\t\tstore_indication = 0xc00;\n}\n\nstatic inline int notify_page_fault(struct pt_regs *regs)\n{\n\tint ret = 0;\n\n\t/* kprobe_running() needs smp_processor_id() */\n\tif (kprobes_built_in() && !user_mode(regs)) {\n\t\tpreempt_disable();\n\t\tif (kprobe_running() && kprobe_fault_handler(regs, 14))\n\t\t\tret = 1;\n\t\tpreempt_enable();\n\t}\n\treturn ret;\n}\n\n\n/*\n * Unlock any spinlocks which will prevent us from getting the\n * message out.\n */\nvoid bust_spinlocks(int yes)\n{\n\tif (yes) {\n\t\toops_in_progress = 1;\n\t} else {\n\t\tint loglevel_save = console_loglevel;\n\t\tconsole_unblank();\n\t\toops_in_progress = 0;\n\t\t/*\n\t\t * OK, the message is on the console.  Now we call printk()\n\t\t * without oops_in_progress set so that printk will give klogd\n\t\t * a poke.  Hold onto your hats...\n\t\t */\n\t\tconsole_loglevel = 15;\n\t\tprintk(\" \");\n\t\tconsole_loglevel = loglevel_save;\n\t}\n}\n\n/*\n * Returns the address space associated with the fault.\n * Returns 0 for kernel space and 1 for user space.\n */\nstatic inline int user_space_fault(unsigned long trans_exc_code)\n{\n\t/*\n\t * The lowest two bits of the translation exception\n\t * identification indicate which paging table was used.\n\t */\n\ttrans_exc_code &= 3;\n\tif (trans_exc_code == 2)\n\t\t/* Access via secondary space, set_fs setting decides */\n\t\treturn current->thread.mm_segment.ar4;\n\tif (user_mode == HOME_SPACE_MODE)\n\t\t/* User space if the access has been done via home space. */\n\t\treturn trans_exc_code == 3;\n\t/*\n\t * If the user space is not the home space the kernel runs in home\n\t * space. Access via secondary space has already been covered,\n\t * access via primary space or access register is from user space\n\t * and access via home space is from the kernel.\n\t */\n\treturn trans_exc_code != 3;\n}\n\nstatic inline void report_user_fault(struct pt_regs *regs, long int_code,\n\t\t\t\t     int signr, unsigned long address)\n{\n\tif ((task_pid_nr(current) > 1) && !show_unhandled_signals)\n\t\treturn;\n\tif (!unhandled_signal(current, signr))\n\t\treturn;\n\tif (!printk_ratelimit())\n\t\treturn;\n\tprintk(\"User process fault: interruption code 0x%lX \", int_code);\n\tprint_vma_addr(KERN_CONT \"in \", regs->psw.addr & PSW_ADDR_INSN);\n\tprintk(\"\\n\");\n\tprintk(\"failing address: %lX\\n\", address);\n\tshow_regs(regs);\n}\n\n/*\n * Send SIGSEGV to task.  This is an external routine\n * to keep the stack usage of do_page_fault small.\n */\nstatic noinline void do_sigsegv(struct pt_regs *regs, long int_code,\n\t\t\t\tint si_code, unsigned long trans_exc_code)\n{\n\tstruct siginfo si;\n\tunsigned long address;\n\n\taddress = trans_exc_code & __FAIL_ADDR_MASK;\n\tcurrent->thread.prot_addr = address;\n\tcurrent->thread.trap_no = int_code;\n\treport_user_fault(regs, int_code, SIGSEGV, address);\n\tsi.si_signo = SIGSEGV;\n\tsi.si_code = si_code;\n\tsi.si_addr = (void __user *) address;\n\tforce_sig_info(SIGSEGV, &si, current);\n}\n\nstatic noinline void do_no_context(struct pt_regs *regs, long int_code,\n\t\t\t\t   unsigned long trans_exc_code)\n{\n\tconst struct exception_table_entry *fixup;\n\tunsigned long address;\n\n\t/* Are we prepared to handle this kernel fault?  */\n\tfixup = search_exception_tables(regs->psw.addr & PSW_ADDR_INSN);\n\tif (fixup) {\n\t\tregs->psw.addr = fixup->fixup | PSW_ADDR_AMODE;\n\t\treturn;\n\t}\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice.\n\t */\n\taddress = trans_exc_code & __FAIL_ADDR_MASK;\n\tif (!user_space_fault(trans_exc_code))\n\t\tprintk(KERN_ALERT \"Unable to handle kernel pointer dereference\"\n\t\t       \" at virtual kernel address %p\\n\", (void *)address);\n\telse\n\t\tprintk(KERN_ALERT \"Unable to handle kernel paging request\"\n\t\t       \" at virtual user address %p\\n\", (void *)address);\n\n\tdie(\"Oops\", regs, int_code);\n\tdo_exit(SIGKILL);\n}\n\nstatic noinline void do_low_address(struct pt_regs *regs, long int_code,\n\t\t\t\t    unsigned long trans_exc_code)\n{\n\t/* Low-address protection hit in kernel mode means\n\t   NULL pointer write access in kernel mode.  */\n\tif (regs->psw.mask & PSW_MASK_PSTATE) {\n\t\t/* Low-address protection hit in user mode 'cannot happen'. */\n\t\tdie (\"Low-address protection\", regs, int_code);\n\t\tdo_exit(SIGKILL);\n\t}\n\n\tdo_no_context(regs, int_code, trans_exc_code);\n}\n\nstatic noinline void do_sigbus(struct pt_regs *regs, long int_code,\n\t\t\t       unsigned long trans_exc_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long address;\n\tstruct siginfo si;\n\n\t/*\n\t * Send a sigbus, regardless of whether we were in kernel\n\t * or user mode.\n\t */\n\taddress = trans_exc_code & __FAIL_ADDR_MASK;\n\ttsk->thread.prot_addr = address;\n\ttsk->thread.trap_no = int_code;\n\tsi.si_signo = SIGBUS;\n\tsi.si_errno = 0;\n\tsi.si_code = BUS_ADRERR;\n\tsi.si_addr = (void __user *) address;\n\tforce_sig_info(SIGBUS, &si, tsk);\n}\n\nstatic noinline void do_fault_error(struct pt_regs *regs, long int_code,\n\t\t\t\t    unsigned long trans_exc_code, int fault)\n{\n\tint si_code;\n\n\tswitch (fault) {\n\tcase VM_FAULT_BADACCESS:\n\tcase VM_FAULT_BADMAP:\n\t\t/* Bad memory access. Check if it is kernel or user space. */\n\t\tif (regs->psw.mask & PSW_MASK_PSTATE) {\n\t\t\t/* User mode accesses just cause a SIGSEGV */\n\t\t\tsi_code = (fault == VM_FAULT_BADMAP) ?\n\t\t\t\tSEGV_MAPERR : SEGV_ACCERR;\n\t\t\tdo_sigsegv(regs, int_code, si_code, trans_exc_code);\n\t\t\treturn;\n\t\t}\n\tcase VM_FAULT_BADCONTEXT:\n\t\tdo_no_context(regs, int_code, trans_exc_code);\n\t\tbreak;\n\tdefault: /* fault & VM_FAULT_ERROR */\n\t\tif (fault & VM_FAULT_OOM) {\n\t\t\tif (!(regs->psw.mask & PSW_MASK_PSTATE))\n\t\t\t\tdo_no_context(regs, int_code, trans_exc_code);\n\t\t\telse\n\t\t\t\tpagefault_out_of_memory();\n\t\t} else if (fault & VM_FAULT_SIGBUS) {\n\t\t\t/* Kernel mode? Handle exceptions or die */\n\t\t\tif (!(regs->psw.mask & PSW_MASK_PSTATE))\n\t\t\t\tdo_no_context(regs, int_code, trans_exc_code);\n\t\t\telse\n\t\t\t\tdo_sigbus(regs, int_code, trans_exc_code);\n\t\t} else\n\t\t\tBUG();\n\t\tbreak;\n\t}\n}\n\n/*\n * This routine handles page faults.  It determines the address,\n * and the problem, and then passes it off to one of the appropriate\n * routines.\n *\n * interruption code (int_code):\n *   04       Protection           ->  Write-Protection  (suprression)\n *   10       Segment translation  ->  Not present       (nullification)\n *   11       Page translation     ->  Not present       (nullification)\n *   3b       Region third trans.  ->  Not present       (nullification)\n */\nstatic inline int do_exception(struct pt_regs *regs, int access,\n\t\t\t       unsigned long trans_exc_code)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long address;\n\tunsigned int flags;\n\tint fault;\n\n\tif (notify_page_fault(regs))\n\t\treturn 0;\n\n\ttsk = current;\n\tmm = tsk->mm;\n\n\t/*\n\t * Verify that the fault happened in user space, that\n\t * we are not in an interrupt and that there is a \n\t * user context.\n\t */\n\tfault = VM_FAULT_BADCONTEXT;\n\tif (unlikely(!user_space_fault(trans_exc_code) || in_atomic() || !mm))\n\t\tgoto out;\n\n\taddress = trans_exc_code & __FAIL_ADDR_MASK;\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);\n\tflags = FAULT_FLAG_ALLOW_RETRY;\n\tif (access == VM_WRITE || (trans_exc_code & store_indication) == 0x400)\n\t\tflags |= FAULT_FLAG_WRITE;\nretry:\n\tdown_read(&mm->mmap_sem);\n\n\tfault = VM_FAULT_BADMAP;\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto out_up;\n\n\tif (unlikely(vma->vm_start > address)) {\n\t\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\t\tgoto out_up;\n\t\tif (expand_stack(vma, address))\n\t\t\tgoto out_up;\n\t}\n\n\t/*\n\t * Ok, we have a good vm_area for this memory access, so\n\t * we can handle it..\n\t */\n\tfault = VM_FAULT_BADACCESS;\n\tif (unlikely(!(vma->vm_flags & access)))\n\t\tgoto out_up;\n\n\tif (is_vm_hugetlb_page(vma))\n\t\taddress &= HPAGE_MASK;\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, flags);\n\tif (unlikely(fault & VM_FAULT_ERROR))\n\t\tgoto out_up;\n\n\t/*\n\t * Major/minor page fault accounting is only done on the\n\t * initial attempt. If we go through a retry, it is extremely\n\t * likely that the page will be found in page cache at that point.\n\t */\n\tif (flags & FAULT_FLAG_ALLOW_RETRY) {\n\t\tif (fault & VM_FAULT_MAJOR) {\n\t\t\ttsk->maj_flt++;\n\t\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,\n\t\t\t\t      regs, address);\n\t\t} else {\n\t\t\ttsk->min_flt++;\n\t\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,\n\t\t\t\t      regs, address);\n\t\t}\n\t\tif (fault & VM_FAULT_RETRY) {\n\t\t\t/* Clear FAULT_FLAG_ALLOW_RETRY to avoid any risk\n\t\t\t * of starvation. */\n\t\t\tflags &= ~FAULT_FLAG_ALLOW_RETRY;\n\t\t\tgoto retry;\n\t\t}\n\t}\n\t/*\n\t * The instruction that caused the program check will\n\t * be repeated. Don't signal single step via SIGTRAP.\n\t */\n\tclear_tsk_thread_flag(tsk, TIF_PER_TRAP);\n\tfault = 0;\nout_up:\n\tup_read(&mm->mmap_sem);\nout:\n\treturn fault;\n}\n\nvoid __kprobes do_protection_exception(struct pt_regs *regs, long pgm_int_code,\n\t\t\t\t       unsigned long trans_exc_code)\n{\n\tint fault;\n\n\t/* Protection exception is suppressing, decrement psw address. */\n\tregs->psw.addr -= (pgm_int_code >> 16);\n\t/*\n\t * Check for low-address protection.  This needs to be treated\n\t * as a special case because the translation exception code\n\t * field is not guaranteed to contain valid data in this case.\n\t */\n\tif (unlikely(!(trans_exc_code & 4))) {\n\t\tdo_low_address(regs, pgm_int_code, trans_exc_code);\n\t\treturn;\n\t}\n\tfault = do_exception(regs, VM_WRITE, trans_exc_code);\n\tif (unlikely(fault))\n\t\tdo_fault_error(regs, 4, trans_exc_code, fault);\n}\n\nvoid __kprobes do_dat_exception(struct pt_regs *regs, long pgm_int_code,\n\t\t\t\tunsigned long trans_exc_code)\n{\n\tint access, fault;\n\n\taccess = VM_READ | VM_EXEC | VM_WRITE;\n\tfault = do_exception(regs, access, trans_exc_code);\n\tif (unlikely(fault))\n\t\tdo_fault_error(regs, pgm_int_code & 255, trans_exc_code, fault);\n}\n\n#ifdef CONFIG_64BIT\nvoid __kprobes do_asce_exception(struct pt_regs *regs, long pgm_int_code,\n\t\t\t\t unsigned long trans_exc_code)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\n\tif (unlikely(!user_space_fault(trans_exc_code) || in_atomic() || !mm))\n\t\tgoto no_context;\n\n\tdown_read(&mm->mmap_sem);\n\tvma = find_vma(mm, trans_exc_code & __FAIL_ADDR_MASK);\n\tup_read(&mm->mmap_sem);\n\n\tif (vma) {\n\t\tupdate_mm(mm, current);\n\t\treturn;\n\t}\n\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (regs->psw.mask & PSW_MASK_PSTATE) {\n\t\tdo_sigsegv(regs, pgm_int_code, SEGV_MAPERR, trans_exc_code);\n\t\treturn;\n\t}\n\nno_context:\n\tdo_no_context(regs, pgm_int_code, trans_exc_code);\n}\n#endif\n\nint __handle_fault(unsigned long uaddr, unsigned long pgm_int_code, int write)\n{\n\tstruct pt_regs regs;\n\tint access, fault;\n\n\tregs.psw.mask = psw_kernel_bits;\n\tif (!irqs_disabled())\n\t\tregs.psw.mask |= PSW_MASK_IO | PSW_MASK_EXT;\n\tregs.psw.addr = (unsigned long) __builtin_return_address(0);\n\tregs.psw.addr |= PSW_ADDR_AMODE;\n\tuaddr &= PAGE_MASK;\n\taccess = write ? VM_WRITE : VM_READ;\n\tfault = do_exception(&regs, access, uaddr | 2);\n\tif (unlikely(fault)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\treturn -EFAULT;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tdo_sigbus(&regs, pgm_int_code, uaddr);\n\t}\n\treturn fault ? -EFAULT : 0;\n}\n\n#ifdef CONFIG_PFAULT \n/*\n * 'pfault' pseudo page faults routines.\n */\nstatic int pfault_disable;\n\nstatic int __init nopfault(char *str)\n{\n\tpfault_disable = 1;\n\treturn 1;\n}\n\n__setup(\"nopfault\", nopfault);\n\nstruct pfault_refbk {\n\tu16 refdiagc;\n\tu16 reffcode;\n\tu16 refdwlen;\n\tu16 refversn;\n\tu64 refgaddr;\n\tu64 refselmk;\n\tu64 refcmpmk;\n\tu64 reserved;\n} __attribute__ ((packed, aligned(8)));\n\nint pfault_init(void)\n{\n\tstruct pfault_refbk refbk = {\n\t\t.refdiagc = 0x258,\n\t\t.reffcode = 0,\n\t\t.refdwlen = 5,\n\t\t.refversn = 2,\n\t\t.refgaddr = __LC_CURRENT_PID,\n\t\t.refselmk = 1ULL << 48,\n\t\t.refcmpmk = 1ULL << 48,\n\t\t.reserved = __PF_RES_FIELD };\n        int rc;\n\n\tif (!MACHINE_IS_VM || pfault_disable)\n\t\treturn -1;\n\tasm volatile(\n\t\t\"\tdiag\t%1,%0,0x258\\n\"\n\t\t\"0:\tj\t2f\\n\"\n\t\t\"1:\tla\t%0,8\\n\"\n\t\t\"2:\\n\"\n\t\tEX_TABLE(0b,1b)\n\t\t: \"=d\" (rc) : \"a\" (&refbk), \"m\" (refbk) : \"cc\");\n        return rc;\n}\n\nvoid pfault_fini(void)\n{\n\tstruct pfault_refbk refbk = {\n\t\t.refdiagc = 0x258,\n\t\t.reffcode = 1,\n\t\t.refdwlen = 5,\n\t\t.refversn = 2,\n\t};\n\n\tif (!MACHINE_IS_VM || pfault_disable)\n\t\treturn;\n\tasm volatile(\n\t\t\"\tdiag\t%0,0,0x258\\n\"\n\t\t\"0:\\n\"\n\t\tEX_TABLE(0b,0b)\n\t\t: : \"a\" (&refbk), \"m\" (refbk) : \"cc\");\n}\n\nstatic DEFINE_SPINLOCK(pfault_lock);\nstatic LIST_HEAD(pfault_list);\n\nstatic void pfault_interrupt(unsigned int ext_int_code,\n\t\t\t     unsigned int param32, unsigned long param64)\n{\n\tstruct task_struct *tsk;\n\t__u16 subcode;\n\tpid_t pid;\n\n\t/*\n\t * Get the external interruption subcode & pfault\n\t * initial/completion signal bit. VM stores this \n\t * in the 'cpu address' field associated with the\n         * external interrupt. \n\t */\n\tsubcode = ext_int_code >> 16;\n\tif ((subcode & 0xff00) != __SUBCODE_MASK)\n\t\treturn;\n\tkstat_cpu(smp_processor_id()).irqs[EXTINT_PFL]++;\n\tif (subcode & 0x0080) {\n\t\t/* Get the token (= pid of the affected task). */\n\t\tpid = sizeof(void *) == 4 ? param32 : param64;\n\t\trcu_read_lock();\n\t\ttsk = find_task_by_pid_ns(pid, &init_pid_ns);\n\t\tif (tsk)\n\t\t\tget_task_struct(tsk);\n\t\trcu_read_unlock();\n\t\tif (!tsk)\n\t\t\treturn;\n\t} else {\n\t\ttsk = current;\n\t}\n\tspin_lock(&pfault_lock);\n\tif (subcode & 0x0080) {\n\t\t/* signal bit is set -> a page has been swapped in by VM */\n\t\tif (tsk->thread.pfault_wait == 1) {\n\t\t\t/* Initial interrupt was faster than the completion\n\t\t\t * interrupt. pfault_wait is valid. Set pfault_wait\n\t\t\t * back to zero and wake up the process. This can\n\t\t\t * safely be done because the task is still sleeping\n\t\t\t * and can't produce new pfaults. */\n\t\t\ttsk->thread.pfault_wait = 0;\n\t\t\tlist_del(&tsk->thread.list);\n\t\t\twake_up_process(tsk);\n\t\t} else {\n\t\t\t/* Completion interrupt was faster than initial\n\t\t\t * interrupt. Set pfault_wait to -1 so the initial\n\t\t\t * interrupt doesn't put the task to sleep. */\n\t\t\ttsk->thread.pfault_wait = -1;\n\t\t}\n\t\tput_task_struct(tsk);\n\t} else {\n\t\t/* signal bit not set -> a real page is missing. */\n\t\tif (tsk->thread.pfault_wait == -1) {\n\t\t\t/* Completion interrupt was faster than the initial\n\t\t\t * interrupt (pfault_wait == -1). Set pfault_wait\n\t\t\t * back to zero and exit. */\n\t\t\ttsk->thread.pfault_wait = 0;\n\t\t} else {\n\t\t\t/* Initial interrupt arrived before completion\n\t\t\t * interrupt. Let the task sleep. */\n\t\t\ttsk->thread.pfault_wait = 1;\n\t\t\tlist_add(&tsk->thread.list, &pfault_list);\n\t\t\tset_task_state(tsk, TASK_UNINTERRUPTIBLE);\n\t\t\tset_tsk_need_resched(tsk);\n\t\t}\n\t}\n\tspin_unlock(&pfault_lock);\n}\n\nstatic int __cpuinit pfault_cpu_notify(struct notifier_block *self,\n\t\t\t\t       unsigned long action, void *hcpu)\n{\n\tstruct thread_struct *thread, *next;\n\tstruct task_struct *tsk;\n\n\tswitch (action) {\n\tcase CPU_DEAD:\n\tcase CPU_DEAD_FROZEN:\n\t\tspin_lock_irq(&pfault_lock);\n\t\tlist_for_each_entry_safe(thread, next, &pfault_list, list) {\n\t\t\tthread->pfault_wait = 0;\n\t\t\tlist_del(&thread->list);\n\t\t\ttsk = container_of(thread, struct task_struct, thread);\n\t\t\twake_up_process(tsk);\n\t\t}\n\t\tspin_unlock_irq(&pfault_lock);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n\nstatic int __init pfault_irq_init(void)\n{\n\tint rc;\n\n\tif (!MACHINE_IS_VM)\n\t\treturn 0;\n\trc = register_external_interrupt(0x2603, pfault_interrupt);\n\tif (rc)\n\t\tgoto out_extint;\n\trc = pfault_init() == 0 ? 0 : -EOPNOTSUPP;\n\tif (rc)\n\t\tgoto out_pfault;\n\tservice_subclass_irq_register();\n\thotcpu_notifier(pfault_cpu_notify, 0);\n\treturn 0;\n\nout_pfault:\n\tunregister_external_interrupt(0x2603, pfault_interrupt);\nout_extint:\n\tpfault_disable = 1;\n\treturn rc;\n}\nearly_initcall(pfault_irq_init);\n\n#endif /* CONFIG_PFAULT */\n", "/*\n * SuperH process tracing\n *\n * Copyright (C) 1999, 2000  Kaz Kojima & Niibe Yutaka\n * Copyright (C) 2002 - 2009  Paul Mundt\n *\n * Audit support by Yuichi Nakamura <ynakam@hitachisoft.jp>\n *\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n */\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/errno.h>\n#include <linux/ptrace.h>\n#include <linux/user.h>\n#include <linux/security.h>\n#include <linux/signal.h>\n#include <linux/io.h>\n#include <linux/audit.h>\n#include <linux/seccomp.h>\n#include <linux/tracehook.h>\n#include <linux/elf.h>\n#include <linux/regset.h>\n#include <linux/hw_breakpoint.h>\n#include <asm/uaccess.h>\n#include <asm/pgtable.h>\n#include <asm/system.h>\n#include <asm/processor.h>\n#include <asm/mmu_context.h>\n#include <asm/syscalls.h>\n#include <asm/fpu.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/syscalls.h>\n\n/*\n * This routine will get a word off of the process kernel stack.\n */\nstatic inline int get_stack_long(struct task_struct *task, int offset)\n{\n\tunsigned char *stack;\n\n\tstack = (unsigned char *)task_pt_regs(task);\n\tstack += offset;\n\treturn (*((int *)stack));\n}\n\n/*\n * This routine will put a word on the process kernel stack.\n */\nstatic inline int put_stack_long(struct task_struct *task, int offset,\n\t\t\t\t unsigned long data)\n{\n\tunsigned char *stack;\n\n\tstack = (unsigned char *)task_pt_regs(task);\n\tstack += offset;\n\t*(unsigned long *) stack = data;\n\treturn 0;\n}\n\nvoid ptrace_triggered(struct perf_event *bp, int nmi,\n\t\t      struct perf_sample_data *data, struct pt_regs *regs)\n{\n\tstruct perf_event_attr attr;\n\n\t/*\n\t * Disable the breakpoint request here since ptrace has defined a\n\t * one-shot behaviour for breakpoint exceptions.\n\t */\n\tattr = bp->attr;\n\tattr.disabled = true;\n\tmodify_user_hw_breakpoint(bp, &attr);\n}\n\nstatic int set_single_step(struct task_struct *tsk, unsigned long addr)\n{\n\tstruct thread_struct *thread = &tsk->thread;\n\tstruct perf_event *bp;\n\tstruct perf_event_attr attr;\n\n\tbp = thread->ptrace_bps[0];\n\tif (!bp) {\n\t\tptrace_breakpoint_init(&attr);\n\n\t\tattr.bp_addr = addr;\n\t\tattr.bp_len = HW_BREAKPOINT_LEN_2;\n\t\tattr.bp_type = HW_BREAKPOINT_R;\n\n\t\tbp = register_user_hw_breakpoint(&attr, ptrace_triggered, tsk);\n\t\tif (IS_ERR(bp))\n\t\t\treturn PTR_ERR(bp);\n\n\t\tthread->ptrace_bps[0] = bp;\n\t} else {\n\t\tint err;\n\n\t\tattr = bp->attr;\n\t\tattr.bp_addr = addr;\n\t\t/* reenable breakpoint */\n\t\tattr.disabled = false;\n\t\terr = modify_user_hw_breakpoint(bp, &attr);\n\t\tif (unlikely(err))\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nvoid user_enable_single_step(struct task_struct *child)\n{\n\tunsigned long pc = get_stack_long(child, offsetof(struct pt_regs, pc));\n\n\tset_tsk_thread_flag(child, TIF_SINGLESTEP);\n\n\tif (ptrace_get_breakpoints(child) < 0)\n\t\treturn;\n\n\tset_single_step(child, pc);\n\tptrace_put_breakpoints(child);\n}\n\nvoid user_disable_single_step(struct task_struct *child)\n{\n\tclear_tsk_thread_flag(child, TIF_SINGLESTEP);\n}\n\n/*\n * Called by kernel/ptrace.c when detaching..\n *\n * Make sure single step bits etc are not set.\n */\nvoid ptrace_disable(struct task_struct *child)\n{\n\tuser_disable_single_step(child);\n}\n\nstatic int genregs_get(struct task_struct *target,\n\t\t       const struct user_regset *regset,\n\t\t       unsigned int pos, unsigned int count,\n\t\t       void *kbuf, void __user *ubuf)\n{\n\tconst struct pt_regs *regs = task_pt_regs(target);\n\tint ret;\n\n\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  regs->regs,\n\t\t\t\t  0, 16 * sizeof(unsigned long));\n\tif (!ret)\n\t\t/* PC, PR, SR, GBR, MACH, MACL, TRA */\n\t\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t  &regs->pc,\n\t\t\t\t\t  offsetof(struct pt_regs, pc),\n\t\t\t\t\t  sizeof(struct pt_regs));\n\tif (!ret)\n\t\tret = user_regset_copyout_zero(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t       sizeof(struct pt_regs), -1);\n\n\treturn ret;\n}\n\nstatic int genregs_set(struct task_struct *target,\n\t\t       const struct user_regset *regset,\n\t\t       unsigned int pos, unsigned int count,\n\t\t       const void *kbuf, const void __user *ubuf)\n{\n\tstruct pt_regs *regs = task_pt_regs(target);\n\tint ret;\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t regs->regs,\n\t\t\t\t 0, 16 * sizeof(unsigned long));\n\tif (!ret && count > 0)\n\t\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t &regs->pc,\n\t\t\t\t\t offsetof(struct pt_regs, pc),\n\t\t\t\t\t sizeof(struct pt_regs));\n\tif (!ret)\n\t\tret = user_regset_copyin_ignore(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t\tsizeof(struct pt_regs), -1);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_SH_FPU\nint fpregs_get(struct task_struct *target,\n\t       const struct user_regset *regset,\n\t       unsigned int pos, unsigned int count,\n\t       void *kbuf, void __user *ubuf)\n{\n\tint ret;\n\n\tret = init_fpu(target);\n\tif (ret)\n\t\treturn ret;\n\n\tif ((boot_cpu_data.flags & CPU_HAS_FPU))\n\t\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t   &target->thread.xstate->hardfpu, 0, -1);\n\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t   &target->thread.xstate->softfpu, 0, -1);\n}\n\nstatic int fpregs_set(struct task_struct *target,\n\t\t       const struct user_regset *regset,\n\t\t       unsigned int pos, unsigned int count,\n\t\t       const void *kbuf, const void __user *ubuf)\n{\n\tint ret;\n\n\tret = init_fpu(target);\n\tif (ret)\n\t\treturn ret;\n\n\tset_stopped_child_used_math(target);\n\n\tif ((boot_cpu_data.flags & CPU_HAS_FPU))\n\t\treturn user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t  &target->thread.xstate->hardfpu, 0, -1);\n\n\treturn user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  &target->thread.xstate->softfpu, 0, -1);\n}\n\nstatic int fpregs_active(struct task_struct *target,\n\t\t\t const struct user_regset *regset)\n{\n\treturn tsk_used_math(target) ? regset->n : 0;\n}\n#endif\n\n#ifdef CONFIG_SH_DSP\nstatic int dspregs_get(struct task_struct *target,\n\t\t       const struct user_regset *regset,\n\t\t       unsigned int pos, unsigned int count,\n\t\t       void *kbuf, void __user *ubuf)\n{\n\tconst struct pt_dspregs *regs =\n\t\t(struct pt_dspregs *)&target->thread.dsp_status.dsp_regs;\n\tint ret;\n\n\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf, regs,\n\t\t\t\t  0, sizeof(struct pt_dspregs));\n\tif (!ret)\n\t\tret = user_regset_copyout_zero(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t       sizeof(struct pt_dspregs), -1);\n\n\treturn ret;\n}\n\nstatic int dspregs_set(struct task_struct *target,\n\t\t       const struct user_regset *regset,\n\t\t       unsigned int pos, unsigned int count,\n\t\t       const void *kbuf, const void __user *ubuf)\n{\n\tstruct pt_dspregs *regs =\n\t\t(struct pt_dspregs *)&target->thread.dsp_status.dsp_regs;\n\tint ret;\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, regs,\n\t\t\t\t 0, sizeof(struct pt_dspregs));\n\tif (!ret)\n\t\tret = user_regset_copyin_ignore(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t\tsizeof(struct pt_dspregs), -1);\n\n\treturn ret;\n}\n\nstatic int dspregs_active(struct task_struct *target,\n\t\t\t  const struct user_regset *regset)\n{\n\tstruct pt_regs *regs = task_pt_regs(target);\n\n\treturn regs->sr & SR_DSP ? regset->n : 0;\n}\n#endif\n\nconst struct pt_regs_offset regoffset_table[] = {\n\tREGS_OFFSET_NAME(0),\n\tREGS_OFFSET_NAME(1),\n\tREGS_OFFSET_NAME(2),\n\tREGS_OFFSET_NAME(3),\n\tREGS_OFFSET_NAME(4),\n\tREGS_OFFSET_NAME(5),\n\tREGS_OFFSET_NAME(6),\n\tREGS_OFFSET_NAME(7),\n\tREGS_OFFSET_NAME(8),\n\tREGS_OFFSET_NAME(9),\n\tREGS_OFFSET_NAME(10),\n\tREGS_OFFSET_NAME(11),\n\tREGS_OFFSET_NAME(12),\n\tREGS_OFFSET_NAME(13),\n\tREGS_OFFSET_NAME(14),\n\tREGS_OFFSET_NAME(15),\n\tREG_OFFSET_NAME(pc),\n\tREG_OFFSET_NAME(pr),\n\tREG_OFFSET_NAME(sr),\n\tREG_OFFSET_NAME(gbr),\n\tREG_OFFSET_NAME(mach),\n\tREG_OFFSET_NAME(macl),\n\tREG_OFFSET_NAME(tra),\n\tREG_OFFSET_END,\n};\n\n/*\n * These are our native regset flavours.\n */\nenum sh_regset {\n\tREGSET_GENERAL,\n#ifdef CONFIG_SH_FPU\n\tREGSET_FPU,\n#endif\n#ifdef CONFIG_SH_DSP\n\tREGSET_DSP,\n#endif\n};\n\nstatic const struct user_regset sh_regsets[] = {\n\t/*\n\t * Format is:\n\t *\tR0 --> R15\n\t *\tPC, PR, SR, GBR, MACH, MACL, TRA\n\t */\n\t[REGSET_GENERAL] = {\n\t\t.core_note_type\t= NT_PRSTATUS,\n\t\t.n\t\t= ELF_NGREG,\n\t\t.size\t\t= sizeof(long),\n\t\t.align\t\t= sizeof(long),\n\t\t.get\t\t= genregs_get,\n\t\t.set\t\t= genregs_set,\n\t},\n\n#ifdef CONFIG_SH_FPU\n\t[REGSET_FPU] = {\n\t\t.core_note_type\t= NT_PRFPREG,\n\t\t.n\t\t= sizeof(struct user_fpu_struct) / sizeof(long),\n\t\t.size\t\t= sizeof(long),\n\t\t.align\t\t= sizeof(long),\n\t\t.get\t\t= fpregs_get,\n\t\t.set\t\t= fpregs_set,\n\t\t.active\t\t= fpregs_active,\n\t},\n#endif\n\n#ifdef CONFIG_SH_DSP\n\t[REGSET_DSP] = {\n\t\t.n\t\t= sizeof(struct pt_dspregs) / sizeof(long),\n\t\t.size\t\t= sizeof(long),\n\t\t.align\t\t= sizeof(long),\n\t\t.get\t\t= dspregs_get,\n\t\t.set\t\t= dspregs_set,\n\t\t.active\t\t= dspregs_active,\n\t},\n#endif\n};\n\nstatic const struct user_regset_view user_sh_native_view = {\n\t.name\t\t= \"sh\",\n\t.e_machine\t= EM_SH,\n\t.regsets\t= sh_regsets,\n\t.n\t\t= ARRAY_SIZE(sh_regsets),\n};\n\nconst struct user_regset_view *task_user_regset_view(struct task_struct *task)\n{\n\treturn &user_sh_native_view;\n}\n\nlong arch_ptrace(struct task_struct *child, long request,\n\t\t unsigned long addr, unsigned long data)\n{\n\tunsigned long __user *datap = (unsigned long __user *)data;\n\tint ret;\n\n\tswitch (request) {\n\t/* read the word at location addr in the USER area. */\n\tcase PTRACE_PEEKUSR: {\n\t\tunsigned long tmp;\n\n\t\tret = -EIO;\n\t\tif ((addr & 3) || addr < 0 ||\n\t\t    addr > sizeof(struct user) - 3)\n\t\t\tbreak;\n\n\t\tif (addr < sizeof(struct pt_regs))\n\t\t\ttmp = get_stack_long(child, addr);\n\t\telse if (addr >= offsetof(struct user, fpu) &&\n\t\t\t addr < offsetof(struct user, u_fpvalid)) {\n\t\t\tif (!tsk_used_math(child)) {\n\t\t\t\tif (addr == offsetof(struct user, fpu.fpscr))\n\t\t\t\t\ttmp = FPSCR_INIT;\n\t\t\t\telse\n\t\t\t\t\ttmp = 0;\n\t\t\t} else {\n\t\t\t\tunsigned long index;\n\t\t\t\tret = init_fpu(child);\n\t\t\t\tif (ret)\n\t\t\t\t\tbreak;\n\t\t\t\tindex = addr - offsetof(struct user, fpu);\n\t\t\t\ttmp = ((unsigned long *)child->thread.xstate)\n\t\t\t\t\t[index >> 2];\n\t\t\t}\n\t\t} else if (addr == offsetof(struct user, u_fpvalid))\n\t\t\ttmp = !!tsk_used_math(child);\n\t\telse if (addr == PT_TEXT_ADDR)\n\t\t\ttmp = child->mm->start_code;\n\t\telse if (addr == PT_DATA_ADDR)\n\t\t\ttmp = child->mm->start_data;\n\t\telse if (addr == PT_TEXT_END_ADDR)\n\t\t\ttmp = child->mm->end_code;\n\t\telse if (addr == PT_TEXT_LEN)\n\t\t\ttmp = child->mm->end_code - child->mm->start_code;\n\t\telse\n\t\t\ttmp = 0;\n\t\tret = put_user(tmp, datap);\n\t\tbreak;\n\t}\n\n\tcase PTRACE_POKEUSR: /* write the word at location addr in the USER area */\n\t\tret = -EIO;\n\t\tif ((addr & 3) || addr < 0 ||\n\t\t    addr > sizeof(struct user) - 3)\n\t\t\tbreak;\n\n\t\tif (addr < sizeof(struct pt_regs))\n\t\t\tret = put_stack_long(child, addr, data);\n\t\telse if (addr >= offsetof(struct user, fpu) &&\n\t\t\t addr < offsetof(struct user, u_fpvalid)) {\n\t\t\tunsigned long index;\n\t\t\tret = init_fpu(child);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tindex = addr - offsetof(struct user, fpu);\n\t\t\tset_stopped_child_used_math(child);\n\t\t\t((unsigned long *)child->thread.xstate)\n\t\t\t\t[index >> 2] = data;\n\t\t\tret = 0;\n\t\t} else if (addr == offsetof(struct user, u_fpvalid)) {\n\t\t\tconditional_stopped_child_used_math(data, child);\n\t\t\tret = 0;\n\t\t}\n\t\tbreak;\n\n\tcase PTRACE_GETREGS:\n\t\treturn copy_regset_to_user(child, &user_sh_native_view,\n\t\t\t\t\t   REGSET_GENERAL,\n\t\t\t\t\t   0, sizeof(struct pt_regs),\n\t\t\t\t\t   datap);\n\tcase PTRACE_SETREGS:\n\t\treturn copy_regset_from_user(child, &user_sh_native_view,\n\t\t\t\t\t     REGSET_GENERAL,\n\t\t\t\t\t     0, sizeof(struct pt_regs),\n\t\t\t\t\t     datap);\n#ifdef CONFIG_SH_FPU\n\tcase PTRACE_GETFPREGS:\n\t\treturn copy_regset_to_user(child, &user_sh_native_view,\n\t\t\t\t\t   REGSET_FPU,\n\t\t\t\t\t   0, sizeof(struct user_fpu_struct),\n\t\t\t\t\t   datap);\n\tcase PTRACE_SETFPREGS:\n\t\treturn copy_regset_from_user(child, &user_sh_native_view,\n\t\t\t\t\t     REGSET_FPU,\n\t\t\t\t\t     0, sizeof(struct user_fpu_struct),\n\t\t\t\t\t     datap);\n#endif\n#ifdef CONFIG_SH_DSP\n\tcase PTRACE_GETDSPREGS:\n\t\treturn copy_regset_to_user(child, &user_sh_native_view,\n\t\t\t\t\t   REGSET_DSP,\n\t\t\t\t\t   0, sizeof(struct pt_dspregs),\n\t\t\t\t\t   datap);\n\tcase PTRACE_SETDSPREGS:\n\t\treturn copy_regset_from_user(child, &user_sh_native_view,\n\t\t\t\t\t     REGSET_DSP,\n\t\t\t\t\t     0, sizeof(struct pt_dspregs),\n\t\t\t\t\t     datap);\n#endif\n\tdefault:\n\t\tret = ptrace_request(child, request, addr, data);\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic inline int audit_arch(void)\n{\n\tint arch = EM_SH;\n\n#ifdef CONFIG_CPU_LITTLE_ENDIAN\n\tarch |= __AUDIT_ARCH_LE;\n#endif\n\n\treturn arch;\n}\n\nasmlinkage long do_syscall_trace_enter(struct pt_regs *regs)\n{\n\tlong ret = 0;\n\n\tsecure_computing(regs->regs[0]);\n\n\tif (test_thread_flag(TIF_SYSCALL_TRACE) &&\n\t    tracehook_report_syscall_entry(regs))\n\t\t/*\n\t\t * Tracing decided this syscall should not happen.\n\t\t * We'll return a bogus call number to get an ENOSYS\n\t\t * error, but leave the original number in regs->regs[0].\n\t\t */\n\t\tret = -1L;\n\n\tif (unlikely(test_thread_flag(TIF_SYSCALL_TRACEPOINT)))\n\t\ttrace_sys_enter(regs, regs->regs[0]);\n\n\tif (unlikely(current->audit_context))\n\t\taudit_syscall_entry(audit_arch(), regs->regs[3],\n\t\t\t\t    regs->regs[4], regs->regs[5],\n\t\t\t\t    regs->regs[6], regs->regs[7]);\n\n\treturn ret ?: regs->regs[0];\n}\n\nasmlinkage void do_syscall_trace_leave(struct pt_regs *regs)\n{\n\tint step;\n\n\tif (unlikely(current->audit_context))\n\t\taudit_syscall_exit(AUDITSC_RESULT(regs->regs[0]),\n\t\t\t\t   regs->regs[0]);\n\n\tif (unlikely(test_thread_flag(TIF_SYSCALL_TRACEPOINT)))\n\t\ttrace_sys_exit(regs, regs->regs[0]);\n\n\tstep = test_thread_flag(TIF_SINGLESTEP);\n\tif (step || test_thread_flag(TIF_SYSCALL_TRACE))\n\t\ttracehook_report_syscall_exit(regs, step);\n}\n", "/*\n * 'traps.c' handles hardware traps and faults after we have saved some\n * state in 'entry.S'.\n *\n *  SuperH version: Copyright (C) 1999 Niibe Yutaka\n *                  Copyright (C) 2000 Philipp Rumpf\n *                  Copyright (C) 2000 David Howells\n *                  Copyright (C) 2002 - 2010 Paul Mundt\n *\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n */\n#include <linux/kernel.h>\n#include <linux/ptrace.h>\n#include <linux/hardirq.h>\n#include <linux/init.h>\n#include <linux/spinlock.h>\n#include <linux/module.h>\n#include <linux/kallsyms.h>\n#include <linux/io.h>\n#include <linux/bug.h>\n#include <linux/debug_locks.h>\n#include <linux/kdebug.h>\n#include <linux/kexec.h>\n#include <linux/limits.h>\n#include <linux/sysfs.h>\n#include <linux/uaccess.h>\n#include <linux/perf_event.h>\n#include <asm/system.h>\n#include <asm/alignment.h>\n#include <asm/fpu.h>\n#include <asm/kprobes.h>\n\n#ifdef CONFIG_CPU_SH2\n# define TRAP_RESERVED_INST\t4\n# define TRAP_ILLEGAL_SLOT_INST\t6\n# define TRAP_ADDRESS_ERROR\t9\n# ifdef CONFIG_CPU_SH2A\n#  define TRAP_UBC\t\t12\n#  define TRAP_FPU_ERROR\t13\n#  define TRAP_DIVZERO_ERROR\t17\n#  define TRAP_DIVOVF_ERROR\t18\n# endif\n#else\n#define TRAP_RESERVED_INST\t12\n#define TRAP_ILLEGAL_SLOT_INST\t13\n#endif\n\nstatic void dump_mem(const char *str, unsigned long bottom, unsigned long top)\n{\n\tunsigned long p;\n\tint i;\n\n\tprintk(\"%s(0x%08lx to 0x%08lx)\\n\", str, bottom, top);\n\n\tfor (p = bottom & ~31; p < top; ) {\n\t\tprintk(\"%04lx: \", p & 0xffff);\n\n\t\tfor (i = 0; i < 8; i++, p += 4) {\n\t\t\tunsigned int val;\n\n\t\t\tif (p < bottom || p >= top)\n\t\t\t\tprintk(\"         \");\n\t\t\telse {\n\t\t\t\tif (__get_user(val, (unsigned int __user *)p)) {\n\t\t\t\t\tprintk(\"\\n\");\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tprintk(\"%08x \", val);\n\t\t\t}\n\t\t}\n\t\tprintk(\"\\n\");\n\t}\n}\n\nstatic DEFINE_SPINLOCK(die_lock);\n\nvoid die(const char * str, struct pt_regs * regs, long err)\n{\n\tstatic int die_counter;\n\n\toops_enter();\n\n\tspin_lock_irq(&die_lock);\n\tconsole_verbose();\n\tbust_spinlocks(1);\n\n\tprintk(\"%s: %04lx [#%d]\\n\", str, err & 0xffff, ++die_counter);\n\tprint_modules();\n\tshow_regs(regs);\n\n\tprintk(\"Process: %s (pid: %d, stack limit = %p)\\n\", current->comm,\n\t\t\ttask_pid_nr(current), task_stack_page(current) + 1);\n\n\tif (!user_mode(regs) || in_interrupt())\n\t\tdump_mem(\"Stack: \", regs->regs[15], THREAD_SIZE +\n\t\t\t (unsigned long)task_stack_page(current));\n\n\tnotify_die(DIE_OOPS, str, regs, err, 255, SIGSEGV);\n\n\tbust_spinlocks(0);\n\tadd_taint(TAINT_DIE);\n\tspin_unlock_irq(&die_lock);\n\toops_exit();\n\n\tif (kexec_should_crash(current))\n\t\tcrash_kexec(regs);\n\n\tif (in_interrupt())\n\t\tpanic(\"Fatal exception in interrupt\");\n\n\tif (panic_on_oops)\n\t\tpanic(\"Fatal exception\");\n\n\tdo_exit(SIGSEGV);\n}\n\nstatic inline void die_if_kernel(const char *str, struct pt_regs *regs,\n\t\t\t\t long err)\n{\n\tif (!user_mode(regs))\n\t\tdie(str, regs, err);\n}\n\n/*\n * try and fix up kernelspace address errors\n * - userspace errors just cause EFAULT to be returned, resulting in SEGV\n * - kernel/userspace interfaces cause a jump to an appropriate handler\n * - other kernel errors are bad\n */\nstatic void die_if_no_fixup(const char * str, struct pt_regs * regs, long err)\n{\n\tif (!user_mode(regs)) {\n\t\tconst struct exception_table_entry *fixup;\n\t\tfixup = search_exception_tables(regs->pc);\n\t\tif (fixup) {\n\t\t\tregs->pc = fixup->fixup;\n\t\t\treturn;\n\t\t}\n\n\t\tdie(str, regs, err);\n\t}\n}\n\nstatic inline void sign_extend(unsigned int count, unsigned char *dst)\n{\n#ifdef __LITTLE_ENDIAN__\n\tif ((count == 1) && dst[0] & 0x80) {\n\t\tdst[1] = 0xff;\n\t\tdst[2] = 0xff;\n\t\tdst[3] = 0xff;\n\t}\n\tif ((count == 2) && dst[1] & 0x80) {\n\t\tdst[2] = 0xff;\n\t\tdst[3] = 0xff;\n\t}\n#else\n\tif ((count == 1) && dst[3] & 0x80) {\n\t\tdst[2] = 0xff;\n\t\tdst[1] = 0xff;\n\t\tdst[0] = 0xff;\n\t}\n\tif ((count == 2) && dst[2] & 0x80) {\n\t\tdst[1] = 0xff;\n\t\tdst[0] = 0xff;\n\t}\n#endif\n}\n\nstatic struct mem_access user_mem_access = {\n\tcopy_from_user,\n\tcopy_to_user,\n};\n\n/*\n * handle an instruction that does an unaligned memory access by emulating the\n * desired behaviour\n * - note that PC _may not_ point to the faulting instruction\n *   (if that instruction is in a branch delay slot)\n * - return 0 if emulation okay, -EFAULT on existential error\n */\nstatic int handle_unaligned_ins(insn_size_t instruction, struct pt_regs *regs,\n\t\t\t\tstruct mem_access *ma)\n{\n\tint ret, index, count;\n\tunsigned long *rm, *rn;\n\tunsigned char *src, *dst;\n\tunsigned char __user *srcu, *dstu;\n\n\tindex = (instruction>>8)&15;\t/* 0x0F00 */\n\trn = &regs->regs[index];\n\n\tindex = (instruction>>4)&15;\t/* 0x00F0 */\n\trm = &regs->regs[index];\n\n\tcount = 1<<(instruction&3);\n\n\tswitch (count) {\n\tcase 1: inc_unaligned_byte_access(); break;\n\tcase 2: inc_unaligned_word_access(); break;\n\tcase 4: inc_unaligned_dword_access(); break;\n\tcase 8: inc_unaligned_multi_access(); break;\n\t}\n\n\tret = -EFAULT;\n\tswitch (instruction>>12) {\n\tcase 0: /* mov.[bwl] to/from memory via r0+rn */\n\t\tif (instruction & 8) {\n\t\t\t/* from memory */\n\t\t\tsrcu = (unsigned char __user *)*rm;\n\t\t\tsrcu += regs->regs[0];\n\t\t\tdst = (unsigned char *)rn;\n\t\t\t*(unsigned long *)dst = 0;\n\n#if !defined(__LITTLE_ENDIAN__)\n\t\t\tdst += 4-count;\n#endif\n\t\t\tif (ma->from(dst, srcu, count))\n\t\t\t\tgoto fetch_fault;\n\n\t\t\tsign_extend(count, dst);\n\t\t} else {\n\t\t\t/* to memory */\n\t\t\tsrc = (unsigned char *)rm;\n#if !defined(__LITTLE_ENDIAN__)\n\t\t\tsrc += 4-count;\n#endif\n\t\t\tdstu = (unsigned char __user *)*rn;\n\t\t\tdstu += regs->regs[0];\n\n\t\t\tif (ma->to(dstu, src, count))\n\t\t\t\tgoto fetch_fault;\n\t\t}\n\t\tret = 0;\n\t\tbreak;\n\n\tcase 1: /* mov.l Rm,@(disp,Rn) */\n\t\tsrc = (unsigned char*) rm;\n\t\tdstu = (unsigned char __user *)*rn;\n\t\tdstu += (instruction&0x000F)<<2;\n\n\t\tif (ma->to(dstu, src, 4))\n\t\t\tgoto fetch_fault;\n\t\tret = 0;\n\t\tbreak;\n\n\tcase 2: /* mov.[bwl] to memory, possibly with pre-decrement */\n\t\tif (instruction & 4)\n\t\t\t*rn -= count;\n\t\tsrc = (unsigned char*) rm;\n\t\tdstu = (unsigned char __user *)*rn;\n#if !defined(__LITTLE_ENDIAN__)\n\t\tsrc += 4-count;\n#endif\n\t\tif (ma->to(dstu, src, count))\n\t\t\tgoto fetch_fault;\n\t\tret = 0;\n\t\tbreak;\n\n\tcase 5: /* mov.l @(disp,Rm),Rn */\n\t\tsrcu = (unsigned char __user *)*rm;\n\t\tsrcu += (instruction & 0x000F) << 2;\n\t\tdst = (unsigned char *)rn;\n\t\t*(unsigned long *)dst = 0;\n\n\t\tif (ma->from(dst, srcu, 4))\n\t\t\tgoto fetch_fault;\n\t\tret = 0;\n\t\tbreak;\n\n\tcase 6:\t/* mov.[bwl] from memory, possibly with post-increment */\n\t\tsrcu = (unsigned char __user *)*rm;\n\t\tif (instruction & 4)\n\t\t\t*rm += count;\n\t\tdst = (unsigned char*) rn;\n\t\t*(unsigned long*)dst = 0;\n\n#if !defined(__LITTLE_ENDIAN__)\n\t\tdst += 4-count;\n#endif\n\t\tif (ma->from(dst, srcu, count))\n\t\t\tgoto fetch_fault;\n\t\tsign_extend(count, dst);\n\t\tret = 0;\n\t\tbreak;\n\n\tcase 8:\n\t\tswitch ((instruction&0xFF00)>>8) {\n\t\tcase 0x81: /* mov.w R0,@(disp,Rn) */\n\t\t\tsrc = (unsigned char *) &regs->regs[0];\n#if !defined(__LITTLE_ENDIAN__)\n\t\t\tsrc += 2;\n#endif\n\t\t\tdstu = (unsigned char __user *)*rm; /* called Rn in the spec */\n\t\t\tdstu += (instruction & 0x000F) << 1;\n\n\t\t\tif (ma->to(dstu, src, 2))\n\t\t\t\tgoto fetch_fault;\n\t\t\tret = 0;\n\t\t\tbreak;\n\n\t\tcase 0x85: /* mov.w @(disp,Rm),R0 */\n\t\t\tsrcu = (unsigned char __user *)*rm;\n\t\t\tsrcu += (instruction & 0x000F) << 1;\n\t\t\tdst = (unsigned char *) &regs->regs[0];\n\t\t\t*(unsigned long *)dst = 0;\n\n#if !defined(__LITTLE_ENDIAN__)\n\t\t\tdst += 2;\n#endif\n\t\t\tif (ma->from(dst, srcu, 2))\n\t\t\t\tgoto fetch_fault;\n\t\t\tsign_extend(2, dst);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\treturn ret;\n\n fetch_fault:\n\t/* Argh. Address not only misaligned but also non-existent.\n\t * Raise an EFAULT and see if it's trapped\n\t */\n\tdie_if_no_fixup(\"Fault in unaligned fixup\", regs, 0);\n\treturn -EFAULT;\n}\n\n/*\n * emulate the instruction in the delay slot\n * - fetches the instruction from PC+2\n */\nstatic inline int handle_delayslot(struct pt_regs *regs,\n\t\t\t\t   insn_size_t old_instruction,\n\t\t\t\t   struct mem_access *ma)\n{\n\tinsn_size_t instruction;\n\tvoid __user *addr = (void __user *)(regs->pc +\n\t\tinstruction_size(old_instruction));\n\n\tif (copy_from_user(&instruction, addr, sizeof(instruction))) {\n\t\t/* the instruction-fetch faulted */\n\t\tif (user_mode(regs))\n\t\t\treturn -EFAULT;\n\n\t\t/* kernel */\n\t\tdie(\"delay-slot-insn faulting in handle_unaligned_delayslot\",\n\t\t    regs, 0);\n\t}\n\n\treturn handle_unaligned_ins(instruction, regs, ma);\n}\n\n/*\n * handle an instruction that does an unaligned memory access\n * - have to be careful of branch delay-slot instructions that fault\n *  SH3:\n *   - if the branch would be taken PC points to the branch\n *   - if the branch would not be taken, PC points to delay-slot\n *  SH4:\n *   - PC always points to delayed branch\n * - return 0 if handled, -EFAULT if failed (may not return if in kernel)\n */\n\n/* Macros to determine offset from current PC for branch instructions */\n/* Explicit type coercion is used to force sign extension where needed */\n#define SH_PC_8BIT_OFFSET(instr) ((((signed char)(instr))*2) + 4)\n#define SH_PC_12BIT_OFFSET(instr) ((((signed short)(instr<<4))>>3) + 4)\n\nint handle_unaligned_access(insn_size_t instruction, struct pt_regs *regs,\n\t\t\t    struct mem_access *ma, int expected,\n\t\t\t    unsigned long address)\n{\n\tu_int rm;\n\tint ret, index;\n\n\t/*\n\t * XXX: We can't handle mixed 16/32-bit instructions yet\n\t */\n\tif (instruction_size(instruction) != 2)\n\t\treturn -EINVAL;\n\n\tindex = (instruction>>8)&15;\t/* 0x0F00 */\n\trm = regs->regs[index];\n\n\t/*\n\t * Log the unexpected fixups, and then pass them on to perf.\n\t *\n\t * We intentionally don't report the expected cases to perf as\n\t * otherwise the trapped I/O case will skew the results too much\n\t * to be useful.\n\t */\n\tif (!expected) {\n\t\tunaligned_fixups_notify(current, instruction, regs);\n\t\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0,\n\t\t\t      regs, address);\n\t}\n\n\tret = -EFAULT;\n\tswitch (instruction&0xF000) {\n\tcase 0x0000:\n\t\tif (instruction==0x000B) {\n\t\t\t/* rts */\n\t\t\tret = handle_delayslot(regs, instruction, ma);\n\t\t\tif (ret==0)\n\t\t\t\tregs->pc = regs->pr;\n\t\t}\n\t\telse if ((instruction&0x00FF)==0x0023) {\n\t\t\t/* braf @Rm */\n\t\t\tret = handle_delayslot(regs, instruction, ma);\n\t\t\tif (ret==0)\n\t\t\t\tregs->pc += rm + 4;\n\t\t}\n\t\telse if ((instruction&0x00FF)==0x0003) {\n\t\t\t/* bsrf @Rm */\n\t\t\tret = handle_delayslot(regs, instruction, ma);\n\t\t\tif (ret==0) {\n\t\t\t\tregs->pr = regs->pc + 4;\n\t\t\t\tregs->pc += rm + 4;\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t/* mov.[bwl] to/from memory via r0+rn */\n\t\t\tgoto simple;\n\t\t}\n\t\tbreak;\n\n\tcase 0x1000: /* mov.l Rm,@(disp,Rn) */\n\t\tgoto simple;\n\n\tcase 0x2000: /* mov.[bwl] to memory, possibly with pre-decrement */\n\t\tgoto simple;\n\n\tcase 0x4000:\n\t\tif ((instruction&0x00FF)==0x002B) {\n\t\t\t/* jmp @Rm */\n\t\t\tret = handle_delayslot(regs, instruction, ma);\n\t\t\tif (ret==0)\n\t\t\t\tregs->pc = rm;\n\t\t}\n\t\telse if ((instruction&0x00FF)==0x000B) {\n\t\t\t/* jsr @Rm */\n\t\t\tret = handle_delayslot(regs, instruction, ma);\n\t\t\tif (ret==0) {\n\t\t\t\tregs->pr = regs->pc + 4;\n\t\t\t\tregs->pc = rm;\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t/* mov.[bwl] to/from memory via r0+rn */\n\t\t\tgoto simple;\n\t\t}\n\t\tbreak;\n\n\tcase 0x5000: /* mov.l @(disp,Rm),Rn */\n\t\tgoto simple;\n\n\tcase 0x6000: /* mov.[bwl] from memory, possibly with post-increment */\n\t\tgoto simple;\n\n\tcase 0x8000: /* bf lab, bf/s lab, bt lab, bt/s lab */\n\t\tswitch (instruction&0x0F00) {\n\t\tcase 0x0100: /* mov.w R0,@(disp,Rm) */\n\t\t\tgoto simple;\n\t\tcase 0x0500: /* mov.w @(disp,Rm),R0 */\n\t\t\tgoto simple;\n\t\tcase 0x0B00: /* bf   lab - no delayslot*/\n\t\t\tbreak;\n\t\tcase 0x0F00: /* bf/s lab */\n\t\t\tret = handle_delayslot(regs, instruction, ma);\n\t\t\tif (ret==0) {\n#if defined(CONFIG_CPU_SH4) || defined(CONFIG_SH7705_CACHE_32KB)\n\t\t\t\tif ((regs->sr & 0x00000001) != 0)\n\t\t\t\t\tregs->pc += 4; /* next after slot */\n\t\t\t\telse\n#endif\n\t\t\t\t\tregs->pc += SH_PC_8BIT_OFFSET(instruction);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 0x0900: /* bt   lab - no delayslot */\n\t\t\tbreak;\n\t\tcase 0x0D00: /* bt/s lab */\n\t\t\tret = handle_delayslot(regs, instruction, ma);\n\t\t\tif (ret==0) {\n#if defined(CONFIG_CPU_SH4) || defined(CONFIG_SH7705_CACHE_32KB)\n\t\t\t\tif ((regs->sr & 0x00000001) == 0)\n\t\t\t\t\tregs->pc += 4; /* next after slot */\n\t\t\t\telse\n#endif\n\t\t\t\t\tregs->pc += SH_PC_8BIT_OFFSET(instruction);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase 0xA000: /* bra label */\n\t\tret = handle_delayslot(regs, instruction, ma);\n\t\tif (ret==0)\n\t\t\tregs->pc += SH_PC_12BIT_OFFSET(instruction);\n\t\tbreak;\n\n\tcase 0xB000: /* bsr label */\n\t\tret = handle_delayslot(regs, instruction, ma);\n\t\tif (ret==0) {\n\t\t\tregs->pr = regs->pc + 4;\n\t\t\tregs->pc += SH_PC_12BIT_OFFSET(instruction);\n\t\t}\n\t\tbreak;\n\t}\n\treturn ret;\n\n\t/* handle non-delay-slot instruction */\n simple:\n\tret = handle_unaligned_ins(instruction, regs, ma);\n\tif (ret==0)\n\t\tregs->pc += instruction_size(instruction);\n\treturn ret;\n}\n\n/*\n * Handle various address error exceptions:\n *  - instruction address error:\n *       misaligned PC\n *       PC >= 0x80000000 in user mode\n *  - data address error (read and write)\n *       misaligned data access\n *       access to >= 0x80000000 is user mode\n * Unfortuntaly we can't distinguish between instruction address error\n * and data address errors caused by read accesses.\n */\nasmlinkage void do_address_error(struct pt_regs *regs,\n\t\t\t\t unsigned long writeaccess,\n\t\t\t\t unsigned long address)\n{\n\tunsigned long error_code = 0;\n\tmm_segment_t oldfs;\n\tsiginfo_t info;\n\tinsn_size_t instruction;\n\tint tmp;\n\n\t/* Intentional ifdef */\n#ifdef CONFIG_CPU_HAS_SR_RB\n\terror_code = lookup_exception_vector();\n#endif\n\n\toldfs = get_fs();\n\n\tif (user_mode(regs)) {\n\t\tint si_code = BUS_ADRERR;\n\t\tunsigned int user_action;\n\n\t\tlocal_irq_enable();\n\t\tinc_unaligned_user_access();\n\n\t\tset_fs(USER_DS);\n\t\tif (copy_from_user(&instruction, (insn_size_t *)(regs->pc & ~1),\n\t\t\t\t   sizeof(instruction))) {\n\t\t\tset_fs(oldfs);\n\t\t\tgoto uspace_segv;\n\t\t}\n\t\tset_fs(oldfs);\n\n\t\t/* shout about userspace fixups */\n\t\tunaligned_fixups_notify(current, instruction, regs);\n\n\t\tuser_action = unaligned_user_action();\n\t\tif (user_action & UM_FIXUP)\n\t\t\tgoto fixup;\n\t\tif (user_action & UM_SIGNAL)\n\t\t\tgoto uspace_segv;\n\t\telse {\n\t\t\t/* ignore */\n\t\t\tregs->pc += instruction_size(instruction);\n\t\t\treturn;\n\t\t}\n\nfixup:\n\t\t/* bad PC is not something we can fix */\n\t\tif (regs->pc & 1) {\n\t\t\tsi_code = BUS_ADRALN;\n\t\t\tgoto uspace_segv;\n\t\t}\n\n\t\tset_fs(USER_DS);\n\t\ttmp = handle_unaligned_access(instruction, regs,\n\t\t\t\t\t      &user_mem_access, 0,\n\t\t\t\t\t      address);\n\t\tset_fs(oldfs);\n\n\t\tif (tmp == 0)\n\t\t\treturn; /* sorted */\nuspace_segv:\n\t\tprintk(KERN_NOTICE \"Sending SIGBUS to \\\"%s\\\" due to unaligned \"\n\t\t       \"access (PC %lx PR %lx)\\n\", current->comm, regs->pc,\n\t\t       regs->pr);\n\n\t\tinfo.si_signo = SIGBUS;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code = si_code;\n\t\tinfo.si_addr = (void __user *)address;\n\t\tforce_sig_info(SIGBUS, &info, current);\n\t} else {\n\t\tinc_unaligned_kernel_access();\n\n\t\tif (regs->pc & 1)\n\t\t\tdie(\"unaligned program counter\", regs, error_code);\n\n\t\tset_fs(KERNEL_DS);\n\t\tif (copy_from_user(&instruction, (void __user *)(regs->pc),\n\t\t\t\t   sizeof(instruction))) {\n\t\t\t/* Argh. Fault on the instruction itself.\n\t\t\t   This should never happen non-SMP\n\t\t\t*/\n\t\t\tset_fs(oldfs);\n\t\t\tdie(\"insn faulting in do_address_error\", regs, 0);\n\t\t}\n\n\t\tunaligned_fixups_notify(current, instruction, regs);\n\n\t\thandle_unaligned_access(instruction, regs, &user_mem_access,\n\t\t\t\t\t0, address);\n\t\tset_fs(oldfs);\n\t}\n}\n\n#ifdef CONFIG_SH_DSP\n/*\n *\tSH-DSP support gerg@snapgear.com.\n */\nint is_dsp_inst(struct pt_regs *regs)\n{\n\tunsigned short inst = 0;\n\n\t/*\n\t * Safe guard if DSP mode is already enabled or we're lacking\n\t * the DSP altogether.\n\t */\n\tif (!(current_cpu_data.flags & CPU_HAS_DSP) || (regs->sr & SR_DSP))\n\t\treturn 0;\n\n\tget_user(inst, ((unsigned short *) regs->pc));\n\n\tinst &= 0xf000;\n\n\t/* Check for any type of DSP or support instruction */\n\tif ((inst == 0xf000) || (inst == 0x4000))\n\t\treturn 1;\n\n\treturn 0;\n}\n#else\n#define is_dsp_inst(regs)\t(0)\n#endif /* CONFIG_SH_DSP */\n\n#ifdef CONFIG_CPU_SH2A\nasmlinkage void do_divide_error(unsigned long r4, unsigned long r5,\n\t\t\t\tunsigned long r6, unsigned long r7,\n\t\t\t\tstruct pt_regs __regs)\n{\n\tsiginfo_t info;\n\n\tswitch (r4) {\n\tcase TRAP_DIVZERO_ERROR:\n\t\tinfo.si_code = FPE_INTDIV;\n\t\tbreak;\n\tcase TRAP_DIVOVF_ERROR:\n\t\tinfo.si_code = FPE_INTOVF;\n\t\tbreak;\n\t}\n\n\tforce_sig_info(SIGFPE, &info, current);\n}\n#endif\n\nasmlinkage void do_reserved_inst(unsigned long r4, unsigned long r5,\n\t\t\t\tunsigned long r6, unsigned long r7,\n\t\t\t\tstruct pt_regs __regs)\n{\n\tstruct pt_regs *regs = RELOC_HIDE(&__regs, 0);\n\tunsigned long error_code;\n\tstruct task_struct *tsk = current;\n\n#ifdef CONFIG_SH_FPU_EMU\n\tunsigned short inst = 0;\n\tint err;\n\n\tget_user(inst, (unsigned short*)regs->pc);\n\n\terr = do_fpu_inst(inst, regs);\n\tif (!err) {\n\t\tregs->pc += instruction_size(inst);\n\t\treturn;\n\t}\n\t/* not a FPU inst. */\n#endif\n\n#ifdef CONFIG_SH_DSP\n\t/* Check if it's a DSP instruction */\n\tif (is_dsp_inst(regs)) {\n\t\t/* Enable DSP mode, and restart instruction. */\n\t\tregs->sr |= SR_DSP;\n\t\t/* Save DSP mode */\n\t\ttsk->thread.dsp_status.status |= SR_DSP;\n\t\treturn;\n\t}\n#endif\n\n\terror_code = lookup_exception_vector();\n\n\tlocal_irq_enable();\n\tforce_sig(SIGILL, tsk);\n\tdie_if_no_fixup(\"reserved instruction\", regs, error_code);\n}\n\n#ifdef CONFIG_SH_FPU_EMU\nstatic int emulate_branch(unsigned short inst, struct pt_regs *regs)\n{\n\t/*\n\t * bfs: 8fxx: PC+=d*2+4;\n\t * bts: 8dxx: PC+=d*2+4;\n\t * bra: axxx: PC+=D*2+4;\n\t * bsr: bxxx: PC+=D*2+4  after PR=PC+4;\n\t * braf:0x23: PC+=Rn*2+4;\n\t * bsrf:0x03: PC+=Rn*2+4 after PR=PC+4;\n\t * jmp: 4x2b: PC=Rn;\n\t * jsr: 4x0b: PC=Rn      after PR=PC+4;\n\t * rts: 000b: PC=PR;\n\t */\n\tif (((inst & 0xf000) == 0xb000)  ||\t/* bsr */\n\t    ((inst & 0xf0ff) == 0x0003)  ||\t/* bsrf */\n\t    ((inst & 0xf0ff) == 0x400b))\t/* jsr */\n\t\tregs->pr = regs->pc + 4;\n\n\tif ((inst & 0xfd00) == 0x8d00) {\t/* bfs, bts */\n\t\tregs->pc += SH_PC_8BIT_OFFSET(inst);\n\t\treturn 0;\n\t}\n\n\tif ((inst & 0xe000) == 0xa000) {\t/* bra, bsr */\n\t\tregs->pc += SH_PC_12BIT_OFFSET(inst);\n\t\treturn 0;\n\t}\n\n\tif ((inst & 0xf0df) == 0x0003) {\t/* braf, bsrf */\n\t\tregs->pc += regs->regs[(inst & 0x0f00) >> 8] + 4;\n\t\treturn 0;\n\t}\n\n\tif ((inst & 0xf0df) == 0x400b) {\t/* jmp, jsr */\n\t\tregs->pc = regs->regs[(inst & 0x0f00) >> 8];\n\t\treturn 0;\n\t}\n\n\tif ((inst & 0xffff) == 0x000b) {\t/* rts */\n\t\tregs->pc = regs->pr;\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n#endif\n\nasmlinkage void do_illegal_slot_inst(unsigned long r4, unsigned long r5,\n\t\t\t\tunsigned long r6, unsigned long r7,\n\t\t\t\tstruct pt_regs __regs)\n{\n\tstruct pt_regs *regs = RELOC_HIDE(&__regs, 0);\n\tunsigned long inst;\n\tstruct task_struct *tsk = current;\n\n\tif (kprobe_handle_illslot(regs->pc) == 0)\n\t\treturn;\n\n#ifdef CONFIG_SH_FPU_EMU\n\tget_user(inst, (unsigned short *)regs->pc + 1);\n\tif (!do_fpu_inst(inst, regs)) {\n\t\tget_user(inst, (unsigned short *)regs->pc);\n\t\tif (!emulate_branch(inst, regs))\n\t\t\treturn;\n\t\t/* fault in branch.*/\n\t}\n\t/* not a FPU inst. */\n#endif\n\n\tinst = lookup_exception_vector();\n\n\tlocal_irq_enable();\n\tforce_sig(SIGILL, tsk);\n\tdie_if_no_fixup(\"illegal slot instruction\", regs, inst);\n}\n\nasmlinkage void do_exception_error(unsigned long r4, unsigned long r5,\n\t\t\t\t   unsigned long r6, unsigned long r7,\n\t\t\t\t   struct pt_regs __regs)\n{\n\tstruct pt_regs *regs = RELOC_HIDE(&__regs, 0);\n\tlong ex;\n\n\tex = lookup_exception_vector();\n\tdie_if_kernel(\"exception\", regs, ex);\n}\n\nvoid __cpuinit per_cpu_trap_init(void)\n{\n\textern void *vbr_base;\n\n\t/* NOTE: The VBR value should be at P1\n\t   (or P2, virtural \"fixed\" address space).\n\t   It's definitely should not in physical address.  */\n\n\tasm volatile(\"ldc\t%0, vbr\"\n\t\t     : /* no output */\n\t\t     : \"r\" (&vbr_base)\n\t\t     : \"memory\");\n\n\t/* disable exception blocking now when the vbr has been setup */\n\tclear_bl_bit();\n}\n\nvoid *set_exception_table_vec(unsigned int vec, void *handler)\n{\n\textern void *exception_handling_table[];\n\tvoid *old_handler;\n\n\told_handler = exception_handling_table[vec];\n\texception_handling_table[vec] = handler;\n\treturn old_handler;\n}\n\nvoid __init trap_init(void)\n{\n\tset_exception_table_vec(TRAP_RESERVED_INST, do_reserved_inst);\n\tset_exception_table_vec(TRAP_ILLEGAL_SLOT_INST, do_illegal_slot_inst);\n\n#if defined(CONFIG_CPU_SH4) && !defined(CONFIG_SH_FPU) || \\\n    defined(CONFIG_SH_FPU_EMU)\n\t/*\n\t * For SH-4 lacking an FPU, treat floating point instructions as\n\t * reserved. They'll be handled in the math-emu case, or faulted on\n\t * otherwise.\n\t */\n\tset_exception_table_evt(0x800, do_reserved_inst);\n\tset_exception_table_evt(0x820, do_illegal_slot_inst);\n#elif defined(CONFIG_SH_FPU)\n\tset_exception_table_evt(0x800, fpu_state_restore_trap_handler);\n\tset_exception_table_evt(0x820, fpu_state_restore_trap_handler);\n#endif\n\n#ifdef CONFIG_CPU_SH2\n\tset_exception_table_vec(TRAP_ADDRESS_ERROR, address_error_trap_handler);\n#endif\n#ifdef CONFIG_CPU_SH2A\n\tset_exception_table_vec(TRAP_DIVZERO_ERROR, do_divide_error);\n\tset_exception_table_vec(TRAP_DIVOVF_ERROR, do_divide_error);\n#ifdef CONFIG_SH_FPU\n\tset_exception_table_vec(TRAP_FPU_ERROR, fpu_error_trap_handler);\n#endif\n#endif\n\n#ifdef TRAP_UBC\n\tset_exception_table_vec(TRAP_UBC, breakpoint_trap_handler);\n#endif\n}\n\nvoid show_stack(struct task_struct *tsk, unsigned long *sp)\n{\n\tunsigned long stack;\n\n\tif (!tsk)\n\t\ttsk = current;\n\tif (tsk == current)\n\t\tsp = (unsigned long *)current_stack_pointer;\n\telse\n\t\tsp = (unsigned long *)tsk->thread.sp;\n\n\tstack = (unsigned long)sp;\n\tdump_mem(\"Stack: \", stack, THREAD_SIZE +\n\t\t (unsigned long)task_stack_page(tsk));\n\tshow_trace(tsk, sp, NULL);\n}\n\nvoid dump_stack(void)\n{\n\tshow_stack(NULL, NULL);\n}\nEXPORT_SYMBOL(dump_stack);\n", "/*\n * arch/sh/kernel/traps_64.c\n *\n * Copyright (C) 2000, 2001  Paolo Alberelli\n * Copyright (C) 2003, 2004  Paul Mundt\n * Copyright (C) 2003, 2004  Richard Curnow\n *\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n */\n#include <linux/sched.h>\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/errno.h>\n#include <linux/ptrace.h>\n#include <linux/timer.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/init.h>\n#include <linux/delay.h>\n#include <linux/spinlock.h>\n#include <linux/kallsyms.h>\n#include <linux/interrupt.h>\n#include <linux/sysctl.h>\n#include <linux/module.h>\n#include <linux/perf_event.h>\n#include <asm/system.h>\n#include <asm/uaccess.h>\n#include <asm/io.h>\n#include <asm/atomic.h>\n#include <asm/processor.h>\n#include <asm/pgtable.h>\n#include <asm/fpu.h>\n\n#undef DEBUG_EXCEPTION\n#ifdef DEBUG_EXCEPTION\n/* implemented in ../lib/dbg.c */\nextern void show_excp_regs(char *fname, int trapnr, int signr,\n\t\t\t   struct pt_regs *regs);\n#else\n#define show_excp_regs(a, b, c, d)\n#endif\n\nstatic void do_unhandled_exception(int trapnr, int signr, char *str, char *fn_name,\n\t\tunsigned long error_code, struct pt_regs *regs, struct task_struct *tsk);\n\n#define DO_ERROR(trapnr, signr, str, name, tsk) \\\nasmlinkage void do_##name(unsigned long error_code, struct pt_regs *regs) \\\n{ \\\n\tdo_unhandled_exception(trapnr, signr, str, __stringify(name), error_code, regs, current); \\\n}\n\nstatic DEFINE_SPINLOCK(die_lock);\n\nvoid die(const char * str, struct pt_regs * regs, long err)\n{\n\tconsole_verbose();\n\tspin_lock_irq(&die_lock);\n\tprintk(\"%s: %lx\\n\", str, (err & 0xffffff));\n\tshow_regs(regs);\n\tspin_unlock_irq(&die_lock);\n\tdo_exit(SIGSEGV);\n}\n\nstatic inline void die_if_kernel(const char * str, struct pt_regs * regs, long err)\n{\n\tif (!user_mode(regs))\n\t\tdie(str, regs, err);\n}\n\nstatic void die_if_no_fixup(const char * str, struct pt_regs * regs, long err)\n{\n\tif (!user_mode(regs)) {\n\t\tconst struct exception_table_entry *fixup;\n\t\tfixup = search_exception_tables(regs->pc);\n\t\tif (fixup) {\n\t\t\tregs->pc = fixup->fixup;\n\t\t\treturn;\n\t\t}\n\t\tdie(str, regs, err);\n\t}\n}\n\nDO_ERROR(13, SIGILL,  \"illegal slot instruction\", illegal_slot_inst, current)\nDO_ERROR(87, SIGSEGV, \"address error (exec)\", address_error_exec, current)\n\n\n/* Implement misaligned load/store handling for kernel (and optionally for user\n   mode too).  Limitation : only SHmedia mode code is handled - there is no\n   handling at all for misaligned accesses occurring in SHcompact code yet. */\n\nstatic int misaligned_fixup(struct pt_regs *regs);\n\nasmlinkage void do_address_error_load(unsigned long error_code, struct pt_regs *regs)\n{\n\tif (misaligned_fixup(regs) < 0) {\n\t\tdo_unhandled_exception(7, SIGSEGV, \"address error(load)\",\n\t\t\t\t\"do_address_error_load\",\n\t\t\t\terror_code, regs, current);\n\t}\n\treturn;\n}\n\nasmlinkage void do_address_error_store(unsigned long error_code, struct pt_regs *regs)\n{\n\tif (misaligned_fixup(regs) < 0) {\n\t\tdo_unhandled_exception(8, SIGSEGV, \"address error(store)\",\n\t\t\t\t\"do_address_error_store\",\n\t\t\t\terror_code, regs, current);\n\t}\n\treturn;\n}\n\n#if defined(CONFIG_SH64_ID2815_WORKAROUND)\n\n#define OPCODE_INVALID      0\n#define OPCODE_USER_VALID   1\n#define OPCODE_PRIV_VALID   2\n\n/* getcon/putcon - requires checking which control register is referenced. */\n#define OPCODE_CTRL_REG     3\n\n/* Table of valid opcodes for SHmedia mode.\n   Form a 10-bit value by concatenating the major/minor opcodes i.e.\n   opcode[31:26,20:16].  The 6 MSBs of this value index into the following\n   array.  The 4 LSBs select the bit-pair in the entry (bits 1:0 correspond to\n   LSBs==4'b0000 etc). */\nstatic unsigned long shmedia_opcode_table[64] = {\n\t0x55554044,0x54445055,0x15141514,0x14541414,0x00000000,0x10001000,0x01110055,0x04050015,\n\t0x00000444,0xc0000000,0x44545515,0x40405555,0x55550015,0x10005555,0x55555505,0x04050000,\n\t0x00000555,0x00000404,0x00040445,0x15151414,0x00000000,0x00000000,0x00000000,0x00000000,\n\t0x00000055,0x40404444,0x00000404,0xc0009495,0x00000000,0x00000000,0x00000000,0x00000000,\n\t0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,\n\t0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,\n\t0x80005050,0x04005055,0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,\n\t0x81055554,0x00000404,0x55555555,0x55555555,0x00000000,0x00000000,0x00000000,0x00000000\n};\n\nvoid do_reserved_inst(unsigned long error_code, struct pt_regs *regs)\n{\n\t/* Workaround SH5-101 cut2 silicon defect #2815 :\n\t   in some situations, inter-mode branches from SHcompact -> SHmedia\n\t   which should take ITLBMISS or EXECPROT exceptions at the target\n\t   falsely take RESINST at the target instead. */\n\n\tunsigned long opcode = 0x6ff4fff0; /* guaranteed reserved opcode */\n\tunsigned long pc, aligned_pc;\n\tint get_user_error;\n\tint trapnr = 12;\n\tint signr = SIGILL;\n\tchar *exception_name = \"reserved_instruction\";\n\n\tpc = regs->pc;\n\tif ((pc & 3) == 1) {\n\t\t/* SHmedia : check for defect.  This requires executable vmas\n\t\t   to be readable too. */\n\t\taligned_pc = pc & ~3;\n\t\tif (!access_ok(VERIFY_READ, aligned_pc, sizeof(unsigned long))) {\n\t\t\tget_user_error = -EFAULT;\n\t\t} else {\n\t\t\tget_user_error = __get_user(opcode, (unsigned long *)aligned_pc);\n\t\t}\n\t\tif (get_user_error >= 0) {\n\t\t\tunsigned long index, shift;\n\t\t\tunsigned long major, minor, combined;\n\t\t\tunsigned long reserved_field;\n\t\t\treserved_field = opcode & 0xf; /* These bits are currently reserved as zero in all valid opcodes */\n\t\t\tmajor = (opcode >> 26) & 0x3f;\n\t\t\tminor = (opcode >> 16) & 0xf;\n\t\t\tcombined = (major << 4) | minor;\n\t\t\tindex = major;\n\t\t\tshift = minor << 1;\n\t\t\tif (reserved_field == 0) {\n\t\t\t\tint opcode_state = (shmedia_opcode_table[index] >> shift) & 0x3;\n\t\t\t\tswitch (opcode_state) {\n\t\t\t\t\tcase OPCODE_INVALID:\n\t\t\t\t\t\t/* Trap. */\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase OPCODE_USER_VALID:\n\t\t\t\t\t\t/* Restart the instruction : the branch to the instruction will now be from an RTE\n\t\t\t\t\t\t   not from SHcompact so the silicon defect won't be triggered. */\n\t\t\t\t\t\treturn;\n\t\t\t\t\tcase OPCODE_PRIV_VALID:\n\t\t\t\t\t\tif (!user_mode(regs)) {\n\t\t\t\t\t\t\t/* Should only ever get here if a module has\n\t\t\t\t\t\t\t   SHcompact code inside it.  If so, the same fix up is needed. */\n\t\t\t\t\t\t\treturn; /* same reason */\n\t\t\t\t\t\t}\n\t\t\t\t\t\t/* Otherwise, user mode trying to execute a privileged instruction -\n\t\t\t\t\t\t   fall through to trap. */\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase OPCODE_CTRL_REG:\n\t\t\t\t\t\t/* If in privileged mode, return as above. */\n\t\t\t\t\t\tif (!user_mode(regs)) return;\n\t\t\t\t\t\t/* In user mode ... */\n\t\t\t\t\t\tif (combined == 0x9f) { /* GETCON */\n\t\t\t\t\t\t\tunsigned long regno = (opcode >> 20) & 0x3f;\n\t\t\t\t\t\t\tif (regno >= 62) {\n\t\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t/* Otherwise, reserved or privileged control register, => trap */\n\t\t\t\t\t\t} else if (combined == 0x1bf) { /* PUTCON */\n\t\t\t\t\t\t\tunsigned long regno = (opcode >> 4) & 0x3f;\n\t\t\t\t\t\t\tif (regno >= 62) {\n\t\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t/* Otherwise, reserved or privileged control register, => trap */\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t/* Trap */\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tdefault:\n\t\t\t\t\t\t/* Fall through to trap. */\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\t/* fall through to normal resinst processing */\n\t\t} else {\n\t\t\t/* Error trying to read opcode.  This typically means a\n\t\t\t   real fault, not a RESINST any more.  So change the\n\t\t\t   codes. */\n\t\t\ttrapnr = 87;\n\t\t\texception_name = \"address error (exec)\";\n\t\t\tsignr = SIGSEGV;\n\t\t}\n\t}\n\n\tdo_unhandled_exception(trapnr, signr, exception_name, \"do_reserved_inst\", error_code, regs, current);\n}\n\n#else /* CONFIG_SH64_ID2815_WORKAROUND */\n\n/* If the workaround isn't needed, this is just a straightforward reserved\n   instruction */\nDO_ERROR(12, SIGILL,  \"reserved instruction\", reserved_inst, current)\n\n#endif /* CONFIG_SH64_ID2815_WORKAROUND */\n\n/* Called with interrupts disabled */\nasmlinkage void do_exception_error(unsigned long ex, struct pt_regs *regs)\n{\n\tshow_excp_regs(__func__, -1, -1, regs);\n\tdie_if_kernel(\"exception\", regs, ex);\n}\n\nint do_unknown_trapa(unsigned long scId, struct pt_regs *regs)\n{\n\t/* Syscall debug */\n        printk(\"System call ID error: [0x1#args:8 #syscall:16  0x%lx]\\n\", scId);\n\n\tdie_if_kernel(\"unknown trapa\", regs, scId);\n\n\treturn -ENOSYS;\n}\n\nvoid show_stack(struct task_struct *tsk, unsigned long *sp)\n{\n#ifdef CONFIG_KALLSYMS\n\textern void sh64_unwind(struct pt_regs *regs);\n\tstruct pt_regs *regs;\n\n\tregs = tsk ? tsk->thread.kregs : NULL;\n\n\tsh64_unwind(regs);\n#else\n\tprintk(KERN_ERR \"Can't backtrace on sh64 without CONFIG_KALLSYMS\\n\");\n#endif\n}\n\nvoid show_task(unsigned long *sp)\n{\n\tshow_stack(NULL, sp);\n}\n\nvoid dump_stack(void)\n{\n\tshow_task(NULL);\n}\n/* Needed by any user of WARN_ON in view of the defn in include/asm-sh/bug.h */\nEXPORT_SYMBOL(dump_stack);\n\nstatic void do_unhandled_exception(int trapnr, int signr, char *str, char *fn_name,\n\t\tunsigned long error_code, struct pt_regs *regs, struct task_struct *tsk)\n{\n\tshow_excp_regs(fn_name, trapnr, signr, regs);\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_no = trapnr;\n\n\tif (user_mode(regs))\n\t\tforce_sig(signr, tsk);\n\n\tdie_if_no_fixup(str, regs, error_code);\n}\n\nstatic int read_opcode(unsigned long long pc, unsigned long *result_opcode, int from_user_mode)\n{\n\tint get_user_error;\n\tunsigned long aligned_pc;\n\tunsigned long opcode;\n\n\tif ((pc & 3) == 1) {\n\t\t/* SHmedia */\n\t\taligned_pc = pc & ~3;\n\t\tif (from_user_mode) {\n\t\t\tif (!access_ok(VERIFY_READ, aligned_pc, sizeof(unsigned long))) {\n\t\t\t\tget_user_error = -EFAULT;\n\t\t\t} else {\n\t\t\t\tget_user_error = __get_user(opcode, (unsigned long *)aligned_pc);\n\t\t\t\t*result_opcode = opcode;\n\t\t\t}\n\t\t\treturn get_user_error;\n\t\t} else {\n\t\t\t/* If the fault was in the kernel, we can either read\n\t\t\t * this directly, or if not, we fault.\n\t\t\t*/\n\t\t\t*result_opcode = *(unsigned long *) aligned_pc;\n\t\t\treturn 0;\n\t\t}\n\t} else if ((pc & 1) == 0) {\n\t\t/* SHcompact */\n\t\t/* TODO : provide handling for this.  We don't really support\n\t\t   user-mode SHcompact yet, and for a kernel fault, this would\n\t\t   have to come from a module built for SHcompact.  */\n\t\treturn -EFAULT;\n\t} else {\n\t\t/* misaligned */\n\t\treturn -EFAULT;\n\t}\n}\n\nstatic int address_is_sign_extended(__u64 a)\n{\n\t__u64 b;\n#if (NEFF == 32)\n\tb = (__u64)(__s64)(__s32)(a & 0xffffffffUL);\n\treturn (b == a) ? 1 : 0;\n#else\n#error \"Sign extend check only works for NEFF==32\"\n#endif\n}\n\nstatic int generate_and_check_address(struct pt_regs *regs,\n\t\t\t\t      __u32 opcode,\n\t\t\t\t      int displacement_not_indexed,\n\t\t\t\t      int width_shift,\n\t\t\t\t      __u64 *address)\n{\n\t/* return -1 for fault, 0 for OK */\n\n\t__u64 base_address, addr;\n\tint basereg;\n\n\tbasereg = (opcode >> 20) & 0x3f;\n\tbase_address = regs->regs[basereg];\n\tif (displacement_not_indexed) {\n\t\t__s64 displacement;\n\t\tdisplacement = (opcode >> 10) & 0x3ff;\n\t\tdisplacement = ((displacement << 54) >> 54); /* sign extend */\n\t\taddr = (__u64)((__s64)base_address + (displacement << width_shift));\n\t} else {\n\t\t__u64 offset;\n\t\tint offsetreg;\n\t\toffsetreg = (opcode >> 10) & 0x3f;\n\t\toffset = regs->regs[offsetreg];\n\t\taddr = base_address + offset;\n\t}\n\n\t/* Check sign extended */\n\tif (!address_is_sign_extended(addr)) {\n\t\treturn -1;\n\t}\n\n\t/* Check accessible.  For misaligned access in the kernel, assume the\n\t   address is always accessible (and if not, just fault when the\n\t   load/store gets done.) */\n\tif (user_mode(regs)) {\n\t\tif (addr >= TASK_SIZE) {\n\t\t\treturn -1;\n\t\t}\n\t\t/* Do access_ok check later - it depends on whether it's a load or a store. */\n\t}\n\n\t*address = addr;\n\treturn 0;\n}\n\nstatic int user_mode_unaligned_fixup_count = 10;\nstatic int user_mode_unaligned_fixup_enable = 1;\nstatic int kernel_mode_unaligned_fixup_count = 32;\n\nstatic void misaligned_kernel_word_load(__u64 address, int do_sign_extend, __u64 *result)\n{\n\tunsigned short x;\n\tunsigned char *p, *q;\n\tp = (unsigned char *) (int) address;\n\tq = (unsigned char *) &x;\n\tq[0] = p[0];\n\tq[1] = p[1];\n\n\tif (do_sign_extend) {\n\t\t*result = (__u64)(__s64) *(short *) &x;\n\t} else {\n\t\t*result = (__u64) x;\n\t}\n}\n\nstatic void misaligned_kernel_word_store(__u64 address, __u64 value)\n{\n\tunsigned short x;\n\tunsigned char *p, *q;\n\tp = (unsigned char *) (int) address;\n\tq = (unsigned char *) &x;\n\n\tx = (__u16) value;\n\tp[0] = q[0];\n\tp[1] = q[1];\n}\n\nstatic int misaligned_load(struct pt_regs *regs,\n\t\t\t   __u32 opcode,\n\t\t\t   int displacement_not_indexed,\n\t\t\t   int width_shift,\n\t\t\t   int do_sign_extend)\n{\n\t/* Return -1 for a fault, 0 for OK */\n\tint error;\n\tint destreg;\n\t__u64 address;\n\n\terror = generate_and_check_address(regs, opcode,\n\t\t\tdisplacement_not_indexed, width_shift, &address);\n\tif (error < 0) {\n\t\treturn error;\n\t}\n\n\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, address);\n\n\tdestreg = (opcode >> 4) & 0x3f;\n\tif (user_mode(regs)) {\n\t\t__u64 buffer;\n\n\t\tif (!access_ok(VERIFY_READ, (unsigned long) address, 1UL<<width_shift)) {\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (__copy_user(&buffer, (const void *)(int)address, (1 << width_shift)) > 0) {\n\t\t\treturn -1; /* fault */\n\t\t}\n\t\tswitch (width_shift) {\n\t\tcase 1:\n\t\t\tif (do_sign_extend) {\n\t\t\t\tregs->regs[destreg] = (__u64)(__s64) *(__s16 *) &buffer;\n\t\t\t} else {\n\t\t\t\tregs->regs[destreg] = (__u64) *(__u16 *) &buffer;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tregs->regs[destreg] = (__u64)(__s64) *(__s32 *) &buffer;\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tregs->regs[destreg] = buffer;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tprintk(\"Unexpected width_shift %d in misaligned_load, PC=%08lx\\n\",\n\t\t\t\twidth_shift, (unsigned long) regs->pc);\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\t/* kernel mode - we can take short cuts since if we fault, it's a genuine bug */\n\t\t__u64 lo, hi;\n\n\t\tswitch (width_shift) {\n\t\tcase 1:\n\t\t\tmisaligned_kernel_word_load(address, do_sign_extend, &regs->regs[destreg]);\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tasm (\"ldlo.l %1, 0, %0\" : \"=r\" (lo) : \"r\" (address));\n\t\t\tasm (\"ldhi.l %1, 3, %0\" : \"=r\" (hi) : \"r\" (address));\n\t\t\tregs->regs[destreg] = lo | hi;\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tasm (\"ldlo.q %1, 0, %0\" : \"=r\" (lo) : \"r\" (address));\n\t\t\tasm (\"ldhi.q %1, 7, %0\" : \"=r\" (hi) : \"r\" (address));\n\t\t\tregs->regs[destreg] = lo | hi;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tprintk(\"Unexpected width_shift %d in misaligned_load, PC=%08lx\\n\",\n\t\t\t\twidth_shift, (unsigned long) regs->pc);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n\n}\n\nstatic int misaligned_store(struct pt_regs *regs,\n\t\t\t    __u32 opcode,\n\t\t\t    int displacement_not_indexed,\n\t\t\t    int width_shift)\n{\n\t/* Return -1 for a fault, 0 for OK */\n\tint error;\n\tint srcreg;\n\t__u64 address;\n\n\terror = generate_and_check_address(regs, opcode,\n\t\t\tdisplacement_not_indexed, width_shift, &address);\n\tif (error < 0) {\n\t\treturn error;\n\t}\n\n\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, address);\n\n\tsrcreg = (opcode >> 4) & 0x3f;\n\tif (user_mode(regs)) {\n\t\t__u64 buffer;\n\n\t\tif (!access_ok(VERIFY_WRITE, (unsigned long) address, 1UL<<width_shift)) {\n\t\t\treturn -1;\n\t\t}\n\n\t\tswitch (width_shift) {\n\t\tcase 1:\n\t\t\t*(__u16 *) &buffer = (__u16) regs->regs[srcreg];\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\t*(__u32 *) &buffer = (__u32) regs->regs[srcreg];\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tbuffer = regs->regs[srcreg];\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tprintk(\"Unexpected width_shift %d in misaligned_store, PC=%08lx\\n\",\n\t\t\t\twidth_shift, (unsigned long) regs->pc);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (__copy_user((void *)(int)address, &buffer, (1 << width_shift)) > 0) {\n\t\t\treturn -1; /* fault */\n\t\t}\n\t} else {\n\t\t/* kernel mode - we can take short cuts since if we fault, it's a genuine bug */\n\t\t__u64 val = regs->regs[srcreg];\n\n\t\tswitch (width_shift) {\n\t\tcase 1:\n\t\t\tmisaligned_kernel_word_store(address, val);\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tasm (\"stlo.l %1, 0, %0\" : : \"r\" (val), \"r\" (address));\n\t\t\tasm (\"sthi.l %1, 3, %0\" : : \"r\" (val), \"r\" (address));\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tasm (\"stlo.q %1, 0, %0\" : : \"r\" (val), \"r\" (address));\n\t\t\tasm (\"sthi.q %1, 7, %0\" : : \"r\" (val), \"r\" (address));\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tprintk(\"Unexpected width_shift %d in misaligned_store, PC=%08lx\\n\",\n\t\t\t\twidth_shift, (unsigned long) regs->pc);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n\n}\n\n/* Never need to fix up misaligned FPU accesses within the kernel since that's a real\n   error. */\nstatic int misaligned_fpu_load(struct pt_regs *regs,\n\t\t\t   __u32 opcode,\n\t\t\t   int displacement_not_indexed,\n\t\t\t   int width_shift,\n\t\t\t   int do_paired_load)\n{\n\t/* Return -1 for a fault, 0 for OK */\n\tint error;\n\tint destreg;\n\t__u64 address;\n\n\terror = generate_and_check_address(regs, opcode,\n\t\t\tdisplacement_not_indexed, width_shift, &address);\n\tif (error < 0) {\n\t\treturn error;\n\t}\n\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, address);\n\n\tdestreg = (opcode >> 4) & 0x3f;\n\tif (user_mode(regs)) {\n\t\t__u64 buffer;\n\t\t__u32 buflo, bufhi;\n\n\t\tif (!access_ok(VERIFY_READ, (unsigned long) address, 1UL<<width_shift)) {\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (__copy_user(&buffer, (const void *)(int)address, (1 << width_shift)) > 0) {\n\t\t\treturn -1; /* fault */\n\t\t}\n\t\t/* 'current' may be the current owner of the FPU state, so\n\t\t   context switch the registers into memory so they can be\n\t\t   indexed by register number. */\n\t\tif (last_task_used_math == current) {\n\t\t\tenable_fpu();\n\t\t\tsave_fpu(current);\n\t\t\tdisable_fpu();\n\t\t\tlast_task_used_math = NULL;\n\t\t\tregs->sr |= SR_FD;\n\t\t}\n\n\t\tbuflo = *(__u32*) &buffer;\n\t\tbufhi = *(1 + (__u32*) &buffer);\n\n\t\tswitch (width_shift) {\n\t\tcase 2:\n\t\t\tcurrent->thread.xstate->hardfpu.fp_regs[destreg] = buflo;\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tif (do_paired_load) {\n\t\t\t\tcurrent->thread.xstate->hardfpu.fp_regs[destreg] = buflo;\n\t\t\t\tcurrent->thread.xstate->hardfpu.fp_regs[destreg+1] = bufhi;\n\t\t\t} else {\n#if defined(CONFIG_CPU_LITTLE_ENDIAN)\n\t\t\t\tcurrent->thread.xstate->hardfpu.fp_regs[destreg] = bufhi;\n\t\t\t\tcurrent->thread.xstate->hardfpu.fp_regs[destreg+1] = buflo;\n#else\n\t\t\t\tcurrent->thread.xstate->hardfpu.fp_regs[destreg] = buflo;\n\t\t\t\tcurrent->thread.xstate->hardfpu.fp_regs[destreg+1] = bufhi;\n#endif\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tprintk(\"Unexpected width_shift %d in misaligned_fpu_load, PC=%08lx\\n\",\n\t\t\t\twidth_shift, (unsigned long) regs->pc);\n\t\t\tbreak;\n\t\t}\n\t\treturn 0;\n\t} else {\n\t\tdie (\"Misaligned FPU load inside kernel\", regs, 0);\n\t\treturn -1;\n\t}\n\n\n}\n\nstatic int misaligned_fpu_store(struct pt_regs *regs,\n\t\t\t   __u32 opcode,\n\t\t\t   int displacement_not_indexed,\n\t\t\t   int width_shift,\n\t\t\t   int do_paired_load)\n{\n\t/* Return -1 for a fault, 0 for OK */\n\tint error;\n\tint srcreg;\n\t__u64 address;\n\n\terror = generate_and_check_address(regs, opcode,\n\t\t\tdisplacement_not_indexed, width_shift, &address);\n\tif (error < 0) {\n\t\treturn error;\n\t}\n\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, address);\n\n\tsrcreg = (opcode >> 4) & 0x3f;\n\tif (user_mode(regs)) {\n\t\t__u64 buffer;\n\t\t/* Initialise these to NaNs. */\n\t\t__u32 buflo=0xffffffffUL, bufhi=0xffffffffUL;\n\n\t\tif (!access_ok(VERIFY_WRITE, (unsigned long) address, 1UL<<width_shift)) {\n\t\t\treturn -1;\n\t\t}\n\n\t\t/* 'current' may be the current owner of the FPU state, so\n\t\t   context switch the registers into memory so they can be\n\t\t   indexed by register number. */\n\t\tif (last_task_used_math == current) {\n\t\t\tenable_fpu();\n\t\t\tsave_fpu(current);\n\t\t\tdisable_fpu();\n\t\t\tlast_task_used_math = NULL;\n\t\t\tregs->sr |= SR_FD;\n\t\t}\n\n\t\tswitch (width_shift) {\n\t\tcase 2:\n\t\t\tbuflo = current->thread.xstate->hardfpu.fp_regs[srcreg];\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tif (do_paired_load) {\n\t\t\t\tbuflo = current->thread.xstate->hardfpu.fp_regs[srcreg];\n\t\t\t\tbufhi = current->thread.xstate->hardfpu.fp_regs[srcreg+1];\n\t\t\t} else {\n#if defined(CONFIG_CPU_LITTLE_ENDIAN)\n\t\t\t\tbufhi = current->thread.xstate->hardfpu.fp_regs[srcreg];\n\t\t\t\tbuflo = current->thread.xstate->hardfpu.fp_regs[srcreg+1];\n#else\n\t\t\t\tbuflo = current->thread.xstate->hardfpu.fp_regs[srcreg];\n\t\t\t\tbufhi = current->thread.xstate->hardfpu.fp_regs[srcreg+1];\n#endif\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tprintk(\"Unexpected width_shift %d in misaligned_fpu_store, PC=%08lx\\n\",\n\t\t\t\twidth_shift, (unsigned long) regs->pc);\n\t\t\tbreak;\n\t\t}\n\n\t\t*(__u32*) &buffer = buflo;\n\t\t*(1 + (__u32*) &buffer) = bufhi;\n\t\tif (__copy_user((void *)(int)address, &buffer, (1 << width_shift)) > 0) {\n\t\t\treturn -1; /* fault */\n\t\t}\n\t\treturn 0;\n\t} else {\n\t\tdie (\"Misaligned FPU load inside kernel\", regs, 0);\n\t\treturn -1;\n\t}\n}\n\nstatic int misaligned_fixup(struct pt_regs *regs)\n{\n\tunsigned long opcode;\n\tint error;\n\tint major, minor;\n\n\tif (!user_mode_unaligned_fixup_enable)\n\t\treturn -1;\n\n\terror = read_opcode(regs->pc, &opcode, user_mode(regs));\n\tif (error < 0) {\n\t\treturn error;\n\t}\n\tmajor = (opcode >> 26) & 0x3f;\n\tminor = (opcode >> 16) & 0xf;\n\n\tif (user_mode(regs) && (user_mode_unaligned_fixup_count > 0)) {\n\t\t--user_mode_unaligned_fixup_count;\n\t\t/* Only do 'count' worth of these reports, to remove a potential DoS against syslog */\n\t\tprintk(\"Fixing up unaligned userspace access in \\\"%s\\\" pid=%d pc=0x%08x ins=0x%08lx\\n\",\n\t\t       current->comm, task_pid_nr(current), (__u32)regs->pc, opcode);\n\t} else if (!user_mode(regs) && (kernel_mode_unaligned_fixup_count > 0)) {\n\t\t--kernel_mode_unaligned_fixup_count;\n\t\tif (in_interrupt()) {\n\t\t\tprintk(\"Fixing up unaligned kernelspace access in interrupt pc=0x%08x ins=0x%08lx\\n\",\n\t\t\t       (__u32)regs->pc, opcode);\n\t\t} else {\n\t\t\tprintk(\"Fixing up unaligned kernelspace access in \\\"%s\\\" pid=%d pc=0x%08x ins=0x%08lx\\n\",\n\t\t\t       current->comm, task_pid_nr(current), (__u32)regs->pc, opcode);\n\t\t}\n\t}\n\n\n\tswitch (major) {\n\t\tcase (0x84>>2): /* LD.W */\n\t\t\terror = misaligned_load(regs, opcode, 1, 1, 1);\n\t\t\tbreak;\n\t\tcase (0xb0>>2): /* LD.UW */\n\t\t\terror = misaligned_load(regs, opcode, 1, 1, 0);\n\t\t\tbreak;\n\t\tcase (0x88>>2): /* LD.L */\n\t\t\terror = misaligned_load(regs, opcode, 1, 2, 1);\n\t\t\tbreak;\n\t\tcase (0x8c>>2): /* LD.Q */\n\t\t\terror = misaligned_load(regs, opcode, 1, 3, 0);\n\t\t\tbreak;\n\n\t\tcase (0xa4>>2): /* ST.W */\n\t\t\terror = misaligned_store(regs, opcode, 1, 1);\n\t\t\tbreak;\n\t\tcase (0xa8>>2): /* ST.L */\n\t\t\terror = misaligned_store(regs, opcode, 1, 2);\n\t\t\tbreak;\n\t\tcase (0xac>>2): /* ST.Q */\n\t\t\terror = misaligned_store(regs, opcode, 1, 3);\n\t\t\tbreak;\n\n\t\tcase (0x40>>2): /* indexed loads */\n\t\t\tswitch (minor) {\n\t\t\t\tcase 0x1: /* LDX.W */\n\t\t\t\t\terror = misaligned_load(regs, opcode, 0, 1, 1);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 0x5: /* LDX.UW */\n\t\t\t\t\terror = misaligned_load(regs, opcode, 0, 1, 0);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 0x2: /* LDX.L */\n\t\t\t\t\terror = misaligned_load(regs, opcode, 0, 2, 1);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 0x3: /* LDX.Q */\n\t\t\t\t\terror = misaligned_load(regs, opcode, 0, 3, 0);\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\terror = -1;\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase (0x60>>2): /* indexed stores */\n\t\t\tswitch (minor) {\n\t\t\t\tcase 0x1: /* STX.W */\n\t\t\t\t\terror = misaligned_store(regs, opcode, 0, 1);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 0x2: /* STX.L */\n\t\t\t\t\terror = misaligned_store(regs, opcode, 0, 2);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 0x3: /* STX.Q */\n\t\t\t\t\terror = misaligned_store(regs, opcode, 0, 3);\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\terror = -1;\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase (0x94>>2): /* FLD.S */\n\t\t\terror = misaligned_fpu_load(regs, opcode, 1, 2, 0);\n\t\t\tbreak;\n\t\tcase (0x98>>2): /* FLD.P */\n\t\t\terror = misaligned_fpu_load(regs, opcode, 1, 3, 1);\n\t\t\tbreak;\n\t\tcase (0x9c>>2): /* FLD.D */\n\t\t\terror = misaligned_fpu_load(regs, opcode, 1, 3, 0);\n\t\t\tbreak;\n\t\tcase (0x1c>>2): /* floating indexed loads */\n\t\t\tswitch (minor) {\n\t\t\tcase 0x8: /* FLDX.S */\n\t\t\t\terror = misaligned_fpu_load(regs, opcode, 0, 2, 0);\n\t\t\t\tbreak;\n\t\t\tcase 0xd: /* FLDX.P */\n\t\t\t\terror = misaligned_fpu_load(regs, opcode, 0, 3, 1);\n\t\t\t\tbreak;\n\t\t\tcase 0x9: /* FLDX.D */\n\t\t\t\terror = misaligned_fpu_load(regs, opcode, 0, 3, 0);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\terror = -1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase (0xb4>>2): /* FLD.S */\n\t\t\terror = misaligned_fpu_store(regs, opcode, 1, 2, 0);\n\t\t\tbreak;\n\t\tcase (0xb8>>2): /* FLD.P */\n\t\t\terror = misaligned_fpu_store(regs, opcode, 1, 3, 1);\n\t\t\tbreak;\n\t\tcase (0xbc>>2): /* FLD.D */\n\t\t\terror = misaligned_fpu_store(regs, opcode, 1, 3, 0);\n\t\t\tbreak;\n\t\tcase (0x3c>>2): /* floating indexed stores */\n\t\t\tswitch (minor) {\n\t\t\tcase 0x8: /* FSTX.S */\n\t\t\t\terror = misaligned_fpu_store(regs, opcode, 0, 2, 0);\n\t\t\t\tbreak;\n\t\t\tcase 0xd: /* FSTX.P */\n\t\t\t\terror = misaligned_fpu_store(regs, opcode, 0, 3, 1);\n\t\t\t\tbreak;\n\t\t\tcase 0x9: /* FSTX.D */\n\t\t\t\terror = misaligned_fpu_store(regs, opcode, 0, 3, 0);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\terror = -1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\t/* Fault */\n\t\t\terror = -1;\n\t\t\tbreak;\n\t}\n\n\tif (error < 0) {\n\t\treturn error;\n\t} else {\n\t\tregs->pc += 4; /* Skip the instruction that's just been emulated */\n\t\treturn 0;\n\t}\n\n}\n\nstatic ctl_table unaligned_table[] = {\n\t{\n\t\t.procname\t= \"kernel_reports\",\n\t\t.data\t\t= &kernel_mode_unaligned_fixup_count,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec\n\t},\n\t{\n\t\t.procname\t= \"user_reports\",\n\t\t.data\t\t= &user_mode_unaligned_fixup_count,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec\n\t},\n\t{\n\t\t.procname\t= \"user_enable\",\n\t\t.data\t\t= &user_mode_unaligned_fixup_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec},\n\t{}\n};\n\nstatic ctl_table unaligned_root[] = {\n\t{\n\t\t.procname\t= \"unaligned_fixup\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= unaligned_table\n\t},\n\t{}\n};\n\nstatic ctl_table sh64_root[] = {\n\t{\n\t\t.procname\t= \"sh64\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= unaligned_root\n\t},\n\t{}\n};\nstatic struct ctl_table_header *sysctl_header;\nstatic int __init init_sysctl(void)\n{\n\tsysctl_header = register_sysctl_table(sh64_root);\n\treturn 0;\n}\n\n__initcall(init_sysctl);\n\n\nasmlinkage void do_debug_interrupt(unsigned long code, struct pt_regs *regs)\n{\n\tu64 peek_real_address_q(u64 addr);\n\tu64 poke_real_address_q(u64 addr, u64 val);\n\tunsigned long long DM_EXP_CAUSE_PHY = 0x0c100010;\n\tunsigned long long exp_cause;\n\t/* It's not worth ioremapping the debug module registers for the amount\n\t   of access we make to them - just go direct to their physical\n\t   addresses. */\n\texp_cause = peek_real_address_q(DM_EXP_CAUSE_PHY);\n\tif (exp_cause & ~4) {\n\t\tprintk(\"DM.EXP_CAUSE had unexpected bits set (=%08lx)\\n\",\n\t\t\t(unsigned long)(exp_cause & 0xffffffff));\n\t}\n\tshow_state();\n\t/* Clear all DEBUGINT causes */\n\tpoke_real_address_q(DM_EXP_CAUSE_PHY, 0x0);\n}\n\nvoid __cpuinit per_cpu_trap_init(void)\n{\n\t/* Nothing to do for now, VBR initialization later. */\n}\n", "/*\n * arch/sh/math-emu/math.c\n *\n * Copyright (C) 2006 Takashi YOSHII <takasi-y@ops.dti.ne.jp>\n *\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n */\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/sched.h>\n#include <linux/signal.h>\n#include <linux/perf_event.h>\n\n#include <asm/system.h>\n#include <asm/uaccess.h>\n#include <asm/processor.h>\n#include <asm/io.h>\n\n#include \"sfp-util.h\"\n#include <math-emu/soft-fp.h>\n#include <math-emu/single.h>\n#include <math-emu/double.h>\n\n#define\tFPUL\t\t(fregs->fpul)\n#define FPSCR\t\t(fregs->fpscr)\n#define FPSCR_RM\t(FPSCR&3)\n#define FPSCR_DN\t((FPSCR>>18)&1)\n#define FPSCR_PR\t((FPSCR>>19)&1)\n#define FPSCR_SZ\t((FPSCR>>20)&1)\n#define FPSCR_FR\t((FPSCR>>21)&1)\n#define FPSCR_MASK\t0x003fffffUL\n\n#define BANK(n)\t(n^(FPSCR_FR?16:0))\n#define FR\t((unsigned long*)(fregs->fp_regs))\n#define FR0\t(FR[BANK(0)])\n#define FRn\t(FR[BANK(n)])\n#define FRm\t(FR[BANK(m)])\n#define DR\t((unsigned long long*)(fregs->fp_regs))\n#define DRn\t(DR[BANK(n)/2])\n#define DRm\t(DR[BANK(m)/2])\n\n#define XREG(n)\t(n^16)\n#define XFn\t(FR[BANK(XREG(n))])\n#define XFm\t(FR[BANK(XREG(m))])\n#define XDn\t(DR[BANK(XREG(n))/2])\n#define XDm\t(DR[BANK(XREG(m))/2])\n\n#define R0\t(regs->regs[0])\n#define Rn\t(regs->regs[n])\n#define Rm\t(regs->regs[m])\n\n#define WRITE(d,a)\t({if(put_user(d, (typeof (d)*)a)) return -EFAULT;})\n#define READ(d,a)\t({if(get_user(d, (typeof (d)*)a)) return -EFAULT;})\n\n#define PACK_S(r,f)\tFP_PACK_SP(&r,f)\n#define UNPACK_S(f,r)\tFP_UNPACK_SP(f,&r)\n#define PACK_D(r,f) \\\n\t{u32 t[2]; FP_PACK_DP(t,f); ((u32*)&r)[0]=t[1]; ((u32*)&r)[1]=t[0];}\n#define UNPACK_D(f,r) \\\n\t{u32 t[2]; t[0]=((u32*)&r)[1]; t[1]=((u32*)&r)[0]; FP_UNPACK_DP(f,t);}\n\n// 2 args instructions.\n#define BOTH_PRmn(op,x) \\\n\tFP_DECL_EX; if(FPSCR_PR) op(D,x,DRm,DRn); else op(S,x,FRm,FRn);\n\n#define CMP_X(SZ,R,M,N) do{ \\\n\tFP_DECL_##SZ(Fm); FP_DECL_##SZ(Fn); \\\n\tUNPACK_##SZ(Fm, M); UNPACK_##SZ(Fn, N); \\\n\tFP_CMP_##SZ(R, Fn, Fm, 2); }while(0)\n#define EQ_X(SZ,R,M,N) do{ \\\n\tFP_DECL_##SZ(Fm); FP_DECL_##SZ(Fn); \\\n\tUNPACK_##SZ(Fm, M); UNPACK_##SZ(Fn, N); \\\n\tFP_CMP_EQ_##SZ(R, Fn, Fm); }while(0)\n#define CMP(OP) ({ int r; BOTH_PRmn(OP##_X,r); r; })\n\nstatic int\nfcmp_gt(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m, int n)\n{\n\tif (CMP(CMP) > 0)\n\t\tregs->sr |= 1;\n\telse\n\t\tregs->sr &= ~1;\n\n\treturn 0;\n}\n\nstatic int\nfcmp_eq(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m, int n)\n{\n\tif (CMP(CMP /*EQ*/) == 0)\n\t\tregs->sr |= 1;\n\telse\n\t\tregs->sr &= ~1;\n\treturn 0;\n}\n\n#define ARITH_X(SZ,OP,M,N) do{ \\\n\tFP_DECL_##SZ(Fm); FP_DECL_##SZ(Fn); FP_DECL_##SZ(Fr); \\\n\tUNPACK_##SZ(Fm, M); UNPACK_##SZ(Fn, N); \\\n\tFP_##OP##_##SZ(Fr, Fn, Fm); \\\n\tPACK_##SZ(N, Fr); }while(0)\n\nstatic int\nfadd(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m, int n)\n{\n\tBOTH_PRmn(ARITH_X, ADD);\n\treturn 0;\n}\n\nstatic int\nfsub(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m, int n)\n{\n\tBOTH_PRmn(ARITH_X, SUB);\n\treturn 0;\n}\n\nstatic int\nfmul(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m, int n)\n{\n\tBOTH_PRmn(ARITH_X, MUL);\n\treturn 0;\n}\n\nstatic int\nfdiv(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m, int n)\n{\n\tBOTH_PRmn(ARITH_X, DIV);\n\treturn 0;\n}\n\nstatic int\nfmac(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m, int n)\n{\n\tFP_DECL_EX;\n\tFP_DECL_S(Fr);\n\tFP_DECL_S(Ft);\n\tFP_DECL_S(F0);\n\tFP_DECL_S(Fm);\n\tFP_DECL_S(Fn);\n\tUNPACK_S(F0, FR0);\n\tUNPACK_S(Fm, FRm);\n\tUNPACK_S(Fn, FRn);\n\tFP_MUL_S(Ft, Fm, F0);\n\tFP_ADD_S(Fr, Fn, Ft);\n\tPACK_S(FRn, Fr);\n\treturn 0;\n}\n\n// to process fmov's extension (odd n for DR access XD).\n#define FMOV_EXT(x) if(x&1) x+=16-1\n\nstatic int\nfmov_idx_reg(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m,\n\t     int n)\n{\n\tif (FPSCR_SZ) {\n\t\tFMOV_EXT(n);\n\t\tREAD(FRn, Rm + R0 + 4);\n\t\tn++;\n\t\tREAD(FRn, Rm + R0);\n\t} else {\n\t\tREAD(FRn, Rm + R0);\n\t}\n\n\treturn 0;\n}\n\nstatic int\nfmov_mem_reg(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m,\n\t     int n)\n{\n\tif (FPSCR_SZ) {\n\t\tFMOV_EXT(n);\n\t\tREAD(FRn, Rm + 4);\n\t\tn++;\n\t\tREAD(FRn, Rm);\n\t} else {\n\t\tREAD(FRn, Rm);\n\t}\n\n\treturn 0;\n}\n\nstatic int\nfmov_inc_reg(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m,\n\t     int n)\n{\n\tif (FPSCR_SZ) {\n\t\tFMOV_EXT(n);\n\t\tREAD(FRn, Rm + 4);\n\t\tn++;\n\t\tREAD(FRn, Rm);\n\t\tRm += 8;\n\t} else {\n\t\tREAD(FRn, Rm);\n\t\tRm += 4;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nfmov_reg_idx(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m,\n\t     int n)\n{\n\tif (FPSCR_SZ) {\n\t\tFMOV_EXT(m);\n\t\tWRITE(FRm, Rn + R0 + 4);\n\t\tm++;\n\t\tWRITE(FRm, Rn + R0);\n\t} else {\n\t\tWRITE(FRm, Rn + R0);\n\t}\n\n\treturn 0;\n}\n\nstatic int\nfmov_reg_mem(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m,\n\t     int n)\n{\n\tif (FPSCR_SZ) {\n\t\tFMOV_EXT(m);\n\t\tWRITE(FRm, Rn + 4);\n\t\tm++;\n\t\tWRITE(FRm, Rn);\n\t} else {\n\t\tWRITE(FRm, Rn);\n\t}\n\n\treturn 0;\n}\n\nstatic int\nfmov_reg_dec(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m,\n\t     int n)\n{\n\tif (FPSCR_SZ) {\n\t\tFMOV_EXT(m);\n\t\tRn -= 8;\n\t\tWRITE(FRm, Rn + 4);\n\t\tm++;\n\t\tWRITE(FRm, Rn);\n\t} else {\n\t\tRn -= 4;\n\t\tWRITE(FRm, Rn);\n\t}\n\n\treturn 0;\n}\n\nstatic int\nfmov_reg_reg(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m,\n\t     int n)\n{\n\tif (FPSCR_SZ) {\n\t\tFMOV_EXT(m);\n\t\tFMOV_EXT(n);\n\t\tDRn = DRm;\n\t} else {\n\t\tFRn = FRm;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nfnop_mn(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m, int n)\n{\n\treturn -EINVAL;\n}\n\n// 1 arg instructions.\n#define NOTYETn(i) static int i(struct sh_fpu_soft_struct *fregs, int n) \\\n\t{ printk( #i \" not yet done.\\n\"); return 0; }\n\nNOTYETn(ftrv)\nNOTYETn(fsqrt)\nNOTYETn(fipr)\nNOTYETn(fsca)\nNOTYETn(fsrra)\n\n#define EMU_FLOAT_X(SZ,N) do { \\\n\tFP_DECL_##SZ(Fn); \\\n\tFP_FROM_INT_##SZ(Fn, FPUL, 32, int); \\\n\tPACK_##SZ(N, Fn); }while(0)\nstatic int ffloat(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFP_DECL_EX;\n\n\tif (FPSCR_PR)\n\t\tEMU_FLOAT_X(D, DRn);\n\telse\n\t\tEMU_FLOAT_X(S, FRn);\n\n\treturn 0;\n}\n\n#define EMU_FTRC_X(SZ,N) do { \\\n\tFP_DECL_##SZ(Fn); \\\n\tUNPACK_##SZ(Fn, N); \\\n\tFP_TO_INT_##SZ(FPUL, Fn, 32, 1); }while(0)\nstatic int ftrc(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFP_DECL_EX;\n\n\tif (FPSCR_PR)\n\t\tEMU_FTRC_X(D, DRn);\n\telse\n\t\tEMU_FTRC_X(S, FRn);\n\n\treturn 0;\n}\n\nstatic int fcnvsd(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFP_DECL_EX;\n\tFP_DECL_S(Fn);\n\tFP_DECL_D(Fr);\n\tUNPACK_S(Fn, FPUL);\n\tFP_CONV(D, S, 2, 1, Fr, Fn);\n\tPACK_D(DRn, Fr);\n\treturn 0;\n}\n\nstatic int fcnvds(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFP_DECL_EX;\n\tFP_DECL_D(Fn);\n\tFP_DECL_S(Fr);\n\tUNPACK_D(Fn, DRn);\n\tFP_CONV(S, D, 1, 2, Fr, Fn);\n\tPACK_S(FPUL, Fr);\n\treturn 0;\n}\n\nstatic int fxchg(struct sh_fpu_soft_struct *fregs, int flag)\n{\n\tFPSCR ^= flag;\n\treturn 0;\n}\n\nstatic int fsts(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFRn = FPUL;\n\treturn 0;\n}\n\nstatic int flds(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFPUL = FRn;\n\treturn 0;\n}\n\nstatic int fneg(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFRn ^= (1 << (_FP_W_TYPE_SIZE - 1));\n\treturn 0;\n}\n\nstatic int fabs(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFRn &= ~(1 << (_FP_W_TYPE_SIZE - 1));\n\treturn 0;\n}\n\nstatic int fld0(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFRn = 0;\n\treturn 0;\n}\n\nstatic int fld1(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFRn = (_FP_EXPBIAS_S << (_FP_FRACBITS_S - 1));\n\treturn 0;\n}\n\nstatic int fnop_n(struct sh_fpu_soft_struct *fregs, int n)\n{\n\treturn -EINVAL;\n}\n\n/// Instruction decoders.\n\nstatic int id_fxfd(struct sh_fpu_soft_struct *, int);\nstatic int id_fnxd(struct sh_fpu_soft_struct *, struct pt_regs *, int, int);\n\nstatic int (*fnxd[])(struct sh_fpu_soft_struct *, int) = {\n\tfsts, flds, ffloat, ftrc, fneg, fabs, fsqrt, fsrra,\n\tfld0, fld1, fcnvsd, fcnvds, fnop_n, fnop_n, fipr, id_fxfd\n};\n\nstatic int (*fnmx[])(struct sh_fpu_soft_struct *, struct pt_regs *, int, int) = {\n\tfadd, fsub, fmul, fdiv, fcmp_eq, fcmp_gt, fmov_idx_reg, fmov_reg_idx,\n\tfmov_mem_reg, fmov_inc_reg, fmov_reg_mem, fmov_reg_dec,\n\tfmov_reg_reg, id_fnxd, fmac, fnop_mn};\n\nstatic int id_fxfd(struct sh_fpu_soft_struct *fregs, int x)\n{\n\tconst int flag[] = { FPSCR_SZ, FPSCR_PR, FPSCR_FR, 0 };\n\tswitch (x & 3) {\n\tcase 3:\n\t\tfxchg(fregs, flag[x >> 2]);\n\t\tbreak;\n\tcase 1:\n\t\tftrv(fregs, x - 1);\n\t\tbreak;\n\tdefault:\n\t\tfsca(fregs, x);\n\t}\n\treturn 0;\n}\n\nstatic int\nid_fnxd(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int x, int n)\n{\n\treturn (fnxd[x])(fregs, n);\n}\n\nstatic int\nid_fnmx(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, u16 code)\n{\n\tint n = (code >> 8) & 0xf, m = (code >> 4) & 0xf, x = code & 0xf;\n\treturn (fnmx[x])(fregs, regs, m, n);\n}\n\nstatic int\nid_sys(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, u16 code)\n{\n\tint n = ((code >> 8) & 0xf);\n\tunsigned long *reg = (code & 0x0010) ? &FPUL : &FPSCR;\n\n\tswitch (code & 0xf0ff) {\n\tcase 0x005a:\n\tcase 0x006a:\n\t\tRn = *reg;\n\t\tbreak;\n\tcase 0x405a:\n\tcase 0x406a:\n\t\t*reg = Rn;\n\t\tbreak;\n\tcase 0x4052:\n\tcase 0x4062:\n\t\tRn -= 4;\n\t\tWRITE(*reg, Rn);\n\t\tbreak;\n\tcase 0x4056:\n\tcase 0x4066:\n\t\tREAD(*reg, Rn);\n\t\tRn += 4;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int fpu_emulate(u16 code, struct sh_fpu_soft_struct *fregs, struct pt_regs *regs)\n{\n\tif ((code & 0xf000) == 0xf000)\n\t\treturn id_fnmx(fregs, regs, code);\n\telse\n\t\treturn id_sys(fregs, regs, code);\n}\n\n/**\n *\tdenormal_to_double - Given denormalized float number,\n *\t                     store double float\n *\n *\t@fpu: Pointer to sh_fpu_soft structure\n *\t@n: Index to FP register\n */\nstatic void denormal_to_double(struct sh_fpu_soft_struct *fpu, int n)\n{\n\tunsigned long du, dl;\n\tunsigned long x = fpu->fpul;\n\tint exp = 1023 - 126;\n\n\tif (x != 0 && (x & 0x7f800000) == 0) {\n\t\tdu = (x & 0x80000000);\n\t\twhile ((x & 0x00800000) == 0) {\n\t\t\tx <<= 1;\n\t\t\texp--;\n\t\t}\n\t\tx &= 0x007fffff;\n\t\tdu |= (exp << 20) | (x >> 3);\n\t\tdl = x << 29;\n\n\t\tfpu->fp_regs[n] = du;\n\t\tfpu->fp_regs[n+1] = dl;\n\t}\n}\n\n/**\n *\tieee_fpe_handler - Handle denormalized number exception\n *\n *\t@regs: Pointer to register structure\n *\n *\tReturns 1 when it's handled (should not cause exception).\n */\nstatic int ieee_fpe_handler(struct pt_regs *regs)\n{\n\tunsigned short insn = *(unsigned short *)regs->pc;\n\tunsigned short finsn;\n\tunsigned long nextpc;\n\tsiginfo_t info;\n\tint nib[4] = {\n\t\t(insn >> 12) & 0xf,\n\t\t(insn >> 8) & 0xf,\n\t\t(insn >> 4) & 0xf,\n\t\tinsn & 0xf};\n\n\tif (nib[0] == 0xb ||\n\t    (nib[0] == 0x4 && nib[2] == 0x0 && nib[3] == 0xb)) /* bsr & jsr */\n\t\tregs->pr = regs->pc + 4;\n\n\tif (nib[0] == 0xa || nib[0] == 0xb) { /* bra & bsr */\n\t\tnextpc = regs->pc + 4 + ((short) ((insn & 0xfff) << 4) >> 3);\n\t\tfinsn = *(unsigned short *) (regs->pc + 2);\n\t} else if (nib[0] == 0x8 && nib[1] == 0xd) { /* bt/s */\n\t\tif (regs->sr & 1)\n\t\t\tnextpc = regs->pc + 4 + ((char) (insn & 0xff) << 1);\n\t\telse\n\t\t\tnextpc = regs->pc + 4;\n\t\tfinsn = *(unsigned short *) (regs->pc + 2);\n\t} else if (nib[0] == 0x8 && nib[1] == 0xf) { /* bf/s */\n\t\tif (regs->sr & 1)\n\t\t\tnextpc = regs->pc + 4;\n\t\telse\n\t\t\tnextpc = regs->pc + 4 + ((char) (insn & 0xff) << 1);\n\t\tfinsn = *(unsigned short *) (regs->pc + 2);\n\t} else if (nib[0] == 0x4 && nib[3] == 0xb &&\n\t\t (nib[2] == 0x0 || nib[2] == 0x2)) { /* jmp & jsr */\n\t\tnextpc = regs->regs[nib[1]];\n\t\tfinsn = *(unsigned short *) (regs->pc + 2);\n\t} else if (nib[0] == 0x0 && nib[3] == 0x3 &&\n\t\t (nib[2] == 0x0 || nib[2] == 0x2)) { /* braf & bsrf */\n\t\tnextpc = regs->pc + 4 + regs->regs[nib[1]];\n\t\tfinsn = *(unsigned short *) (regs->pc + 2);\n\t} else if (insn == 0x000b) { /* rts */\n\t\tnextpc = regs->pr;\n\t\tfinsn = *(unsigned short *) (regs->pc + 2);\n\t} else {\n\t\tnextpc = regs->pc + 2;\n\t\tfinsn = insn;\n\t}\n\n\tif ((finsn & 0xf1ff) == 0xf0ad) { /* fcnvsd */\n\t\tstruct task_struct *tsk = current;\n\n\t\tif ((tsk->thread.xstate->softfpu.fpscr & (1 << 17))) {\n\t\t\t/* FPU error */\n\t\t\tdenormal_to_double (&tsk->thread.xstate->softfpu,\n\t\t\t\t\t    (finsn >> 8) & 0xf);\n\t\t\ttsk->thread.xstate->softfpu.fpscr &=\n\t\t\t\t~(FPSCR_CAUSE_MASK | FPSCR_FLAG_MASK);\n\t\t\ttask_thread_info(tsk)->status |= TS_USEDFPU;\n\t\t} else {\n\t\t\tinfo.si_signo = SIGFPE;\n\t\t\tinfo.si_errno = 0;\n\t\t\tinfo.si_code = FPE_FLTINV;\n\t\t\tinfo.si_addr = (void __user *)regs->pc;\n\t\t\tforce_sig_info(SIGFPE, &info, tsk);\n\t\t}\n\n\t\tregs->pc = nextpc;\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nasmlinkage void do_fpu_error(unsigned long r4, unsigned long r5,\n\t\t\t     unsigned long r6, unsigned long r7,\n\t\t\t     struct pt_regs regs)\n{\n\tstruct task_struct *tsk = current;\n\tsiginfo_t info;\n\n\tif (ieee_fpe_handler (&regs))\n\t\treturn;\n\n\tregs.pc += 2;\n\tinfo.si_signo = SIGFPE;\n\tinfo.si_errno = 0;\n\tinfo.si_code = FPE_FLTINV;\n\tinfo.si_addr = (void __user *)regs.pc;\n\tforce_sig_info(SIGFPE, &info, tsk);\n}\n\n/**\n * fpu_init - Initialize FPU registers\n * @fpu: Pointer to software emulated FPU registers.\n */\nstatic void fpu_init(struct sh_fpu_soft_struct *fpu)\n{\n\tint i;\n\n\tfpu->fpscr = FPSCR_INIT;\n\tfpu->fpul = 0;\n\n\tfor (i = 0; i < 16; i++) {\n\t\tfpu->fp_regs[i] = 0;\n\t\tfpu->xfp_regs[i]= 0;\n\t}\n}\n\n/**\n * do_fpu_inst - Handle reserved instructions for FPU emulation\n * @inst: instruction code.\n * @regs: registers on stack.\n */\nint do_fpu_inst(unsigned short inst, struct pt_regs *regs)\n{\n\tstruct task_struct *tsk = current;\n\tstruct sh_fpu_soft_struct *fpu = &(tsk->thread.xstate->softfpu);\n\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);\n\n\tif (!(task_thread_info(tsk)->status & TS_USEDFPU)) {\n\t\t/* initialize once. */\n\t\tfpu_init(fpu);\n\t\ttask_thread_info(tsk)->status |= TS_USEDFPU;\n\t}\n\n\treturn fpu_emulate(inst, fpu, regs);\n}\n", "/*\n * Page fault handler for SH with an MMU.\n *\n *  Copyright (C) 1999  Niibe Yutaka\n *  Copyright (C) 2003 - 2009  Paul Mundt\n *\n *  Based on linux/arch/i386/mm/fault.c:\n *   Copyright (C) 1995  Linus Torvalds\n *\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n */\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/hardirq.h>\n#include <linux/kprobes.h>\n#include <linux/perf_event.h>\n#include <asm/io_trapped.h>\n#include <asm/system.h>\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n\nstatic inline int notify_page_fault(struct pt_regs *regs, int trap)\n{\n\tint ret = 0;\n\n\tif (kprobes_built_in() && !user_mode(regs)) {\n\t\tpreempt_disable();\n\t\tif (kprobe_running() && kprobe_fault_handler(regs, trap))\n\t\t\tret = 1;\n\t\tpreempt_enable();\n\t}\n\n\treturn ret;\n}\n\nstatic inline pmd_t *vmalloc_sync_one(pgd_t *pgd, unsigned long address)\n{\n\tunsigned index = pgd_index(address);\n\tpgd_t *pgd_k;\n\tpud_t *pud, *pud_k;\n\tpmd_t *pmd, *pmd_k;\n\n\tpgd += index;\n\tpgd_k = init_mm.pgd + index;\n\n\tif (!pgd_present(*pgd_k))\n\t\treturn NULL;\n\n\tpud = pud_offset(pgd, address);\n\tpud_k = pud_offset(pgd_k, address);\n\tif (!pud_present(*pud_k))\n\t\treturn NULL;\n\n\tif (!pud_present(*pud))\n\t    set_pud(pud, *pud_k);\n\n\tpmd = pmd_offset(pud, address);\n\tpmd_k = pmd_offset(pud_k, address);\n\tif (!pmd_present(*pmd_k))\n\t\treturn NULL;\n\n\tif (!pmd_present(*pmd))\n\t\tset_pmd(pmd, *pmd_k);\n\telse {\n\t\t/*\n\t\t * The page tables are fully synchronised so there must\n\t\t * be another reason for the fault. Return NULL here to\n\t\t * signal that we have not taken care of the fault.\n\t\t */\n\t\tBUG_ON(pmd_page(*pmd) != pmd_page(*pmd_k));\n\t\treturn NULL;\n\t}\n\n\treturn pmd_k;\n}\n\n/*\n * Handle a fault on the vmalloc or module mapping area\n */\nstatic noinline int vmalloc_fault(unsigned long address)\n{\n\tpgd_t *pgd_k;\n\tpmd_t *pmd_k;\n\tpte_t *pte_k;\n\n\t/* Make sure we are in vmalloc/module/P3 area: */\n\tif (!(address >= VMALLOC_START && address < P3_ADDR_MAX))\n\t\treturn -1;\n\n\t/*\n\t * Synchronize this task's top level page-table\n\t * with the 'reference' page table.\n\t *\n\t * Do _not_ use \"current\" here. We might be inside\n\t * an interrupt in the middle of a task switch..\n\t */\n\tpgd_k = get_TTB();\n\tpmd_k = vmalloc_sync_one(pgd_k, address);\n\tif (!pmd_k)\n\t\treturn -1;\n\n\tpte_k = pte_offset_kernel(pmd_k, address);\n\tif (!pte_present(*pte_k))\n\t\treturn -1;\n\n\treturn 0;\n}\n\nstatic int fault_in_kernel_space(unsigned long address)\n{\n\treturn address >= TASK_SIZE;\n}\n\n/*\n * This routine handles page faults.  It determines the address,\n * and the problem, and then passes it off to one of the appropriate\n * routines.\n */\nasmlinkage void __kprobes do_page_fault(struct pt_regs *regs,\n\t\t\t\t\tunsigned long writeaccess,\n\t\t\t\t\tunsigned long address)\n{\n\tunsigned long vec;\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct * vma;\n\tint si_code;\n\tint fault;\n\tsiginfo_t info;\n\n\ttsk = current;\n\tmm = tsk->mm;\n\tsi_code = SEGV_MAPERR;\n\tvec = lookup_exception_vector();\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t */\n\tif (unlikely(fault_in_kernel_space(address))) {\n\t\tif (vmalloc_fault(address) >= 0)\n\t\t\treturn;\n\t\tif (notify_page_fault(regs, vec))\n\t\t\treturn;\n\n\t\tgoto bad_area_nosemaphore;\n\t}\n\n\tif (unlikely(notify_page_fault(regs, vec)))\n\t\treturn;\n\n\t/* Only enable interrupts if they were on before the fault */\n\tif ((regs->sr & SR_IMASK) != SR_IMASK)\n\t\tlocal_irq_enable();\n\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);\n\n\t/*\n\t * If we're in an interrupt, have no user context or are running\n\t * in an atomic region then we must not take the fault:\n\t */\n\tif (in_atomic() || !mm)\n\t\tgoto no_context;\n\n\tdown_read(&mm->mmap_sem);\n\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n\n\t/*\n\t * Ok, we have a good vm_area for this memory access, so\n\t * we can handle it..\n\t */\ngood_area:\n\tsi_code = SEGV_ACCERR;\n\tif (writeaccess) {\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t} else {\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tgoto bad_area;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, writeaccess ? FAULT_FLAG_WRITE : 0);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR) {\n\t\ttsk->maj_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,\n\t\t\t\t     regs, address);\n\t} else {\n\t\ttsk->min_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,\n\t\t\t\t     regs, address);\n\t}\n\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n\t/*\n\t * Something tried to access memory that isn't in our memory map..\n\t * Fix it, but check if it's kernel or user first..\n\t */\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\tif (user_mode(regs)) {\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code = si_code;\n\t\tinfo.si_addr = (void *) address;\n\t\tforce_sig_info(SIGSEGV, &info, tsk);\n\t\treturn;\n\t}\n\nno_context:\n\t/* Are we prepared to handle this kernel fault?  */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\tif (handle_trapped_io(regs, address))\n\t\treturn;\n/*\n * Oops. The kernel tried to access some bad page. We'll have to\n * terminate things with extreme prejudice.\n *\n */\n\n\tbust_spinlocks(1);\n\n\tif (oops_may_print()) {\n\t\tunsigned long page;\n\n\t\tif (address < PAGE_SIZE)\n\t\t\tprintk(KERN_ALERT \"Unable to handle kernel NULL \"\n\t\t\t\t\t  \"pointer dereference\");\n\t\telse\n\t\t\tprintk(KERN_ALERT \"Unable to handle kernel paging \"\n\t\t\t\t\t  \"request\");\n\t\tprintk(\" at virtual address %08lx\\n\", address);\n\t\tprintk(KERN_ALERT \"pc = %08lx\\n\", regs->pc);\n\t\tpage = (unsigned long)get_TTB();\n\t\tif (page) {\n\t\t\tpage = ((__typeof__(page) *)page)[address >> PGDIR_SHIFT];\n\t\t\tprintk(KERN_ALERT \"*pde = %08lx\\n\", page);\n\t\t\tif (page & _PAGE_PRESENT) {\n\t\t\t\tpage &= PAGE_MASK;\n\t\t\t\taddress &= 0x003ff000;\n\t\t\t\tpage = ((__typeof__(page) *)\n\t\t\t\t\t\t__va(page))[address >>\n\t\t\t\t\t\t\t    PAGE_SHIFT];\n\t\t\t\tprintk(KERN_ALERT \"*pte = %08lx\\n\", page);\n\t\t\t}\n\t\t}\n\t}\n\n\tdie(\"Oops\", regs, writeaccess);\n\tbust_spinlocks(0);\n\tdo_exit(SIGKILL);\n\n/*\n * We ran out of memory, or some other thing happened to us that made\n * us unable to handle the page fault gracefully.\n */\nout_of_memory:\n\tup_read(&mm->mmap_sem);\n\tif (!user_mode(regs))\n\t\tgoto no_context;\n\tpagefault_out_of_memory();\n\treturn;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\n\t/*\n\t * Send a sigbus, regardless of whether we were in kernel\n\t * or user mode.\n\t */\n\tinfo.si_signo = SIGBUS;\n\tinfo.si_errno = 0;\n\tinfo.si_code = BUS_ADRERR;\n\tinfo.si_addr = (void *)address;\n\tforce_sig_info(SIGBUS, &info, tsk);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (!user_mode(regs))\n\t\tgoto no_context;\n}\n\n/*\n * Called with interrupts disabled.\n */\nasmlinkage int __kprobes\nhandle_tlbmiss(struct pt_regs *regs, unsigned long writeaccess,\n\t       unsigned long address)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tpte_t entry;\n\n\t/*\n\t * We don't take page faults for P1, P2, and parts of P4, these\n\t * are always mapped, whether it be due to legacy behaviour in\n\t * 29-bit mode, or due to PMB configuration in 32-bit mode.\n\t */\n\tif (address >= P3SEG && address < P3_ADDR_MAX) {\n\t\tpgd = pgd_offset_k(address);\n\t} else {\n\t\tif (unlikely(address >= TASK_SIZE || !current->mm))\n\t\t\treturn 1;\n\n\t\tpgd = pgd_offset(current->mm, address);\n\t}\n\n\tpud = pud_offset(pgd, address);\n\tif (pud_none_or_clear_bad(pud))\n\t\treturn 1;\n\tpmd = pmd_offset(pud, address);\n\tif (pmd_none_or_clear_bad(pmd))\n\t\treturn 1;\n\tpte = pte_offset_kernel(pmd, address);\n\tentry = *pte;\n\tif (unlikely(pte_none(entry) || pte_not_present(entry)))\n\t\treturn 1;\n\tif (unlikely(writeaccess && !pte_write(entry)))\n\t\treturn 1;\n\n\tif (writeaccess)\n\t\tentry = pte_mkdirty(entry);\n\tentry = pte_mkyoung(entry);\n\n\tset_pte(pte, entry);\n\n#if defined(CONFIG_CPU_SH4) && !defined(CONFIG_SMP)\n\t/*\n\t * SH-4 does not set MMUCR.RC to the corresponding TLB entry in\n\t * the case of an initial page write exception, so we need to\n\t * flush it in order to avoid potential TLB entry duplication.\n\t */\n\tif (writeaccess == 2)\n\t\tlocal_flush_tlb_one(get_asid(), address & PAGE_MASK);\n#endif\n\n\tupdate_mmu_cache(NULL, address, pte);\n\n\treturn 0;\n}\n", "/*\n * arch/sh/mm/tlb-flush_64.c\n *\n * Copyright (C) 2000, 2001  Paolo Alberelli\n * Copyright (C) 2003  Richard Curnow (/proc/tlb, bug fixes)\n * Copyright (C) 2003 - 2009 Paul Mundt\n *\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n */\n#include <linux/signal.h>\n#include <linux/rwsem.h>\n#include <linux/sched.h>\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/ptrace.h>\n#include <linux/mman.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/perf_event.h>\n#include <linux/interrupt.h>\n#include <asm/system.h>\n#include <asm/io.h>\n#include <asm/tlb.h>\n#include <asm/uaccess.h>\n#include <asm/pgalloc.h>\n#include <asm/mmu_context.h>\n\nextern void die(const char *,struct pt_regs *,long);\n\n#define PFLAG(val,flag)   (( (val) & (flag) ) ? #flag : \"\" )\n#define PPROT(flag) PFLAG(pgprot_val(prot),flag)\n\nstatic inline void print_prots(pgprot_t prot)\n{\n\tprintk(\"prot is 0x%016llx\\n\",pgprot_val(prot));\n\n\tprintk(\"%s %s %s %s %s\\n\",PPROT(_PAGE_SHARED),PPROT(_PAGE_READ),\n\t       PPROT(_PAGE_EXECUTE),PPROT(_PAGE_WRITE),PPROT(_PAGE_USER));\n}\n\nstatic inline void print_vma(struct vm_area_struct *vma)\n{\n\tprintk(\"vma start 0x%08lx\\n\", vma->vm_start);\n\tprintk(\"vma end   0x%08lx\\n\", vma->vm_end);\n\n\tprint_prots(vma->vm_page_prot);\n\tprintk(\"vm_flags 0x%08lx\\n\", vma->vm_flags);\n}\n\nstatic inline void print_task(struct task_struct *tsk)\n{\n\tprintk(\"Task pid %d\\n\", task_pid_nr(tsk));\n}\n\nstatic pte_t *lookup_pte(struct mm_struct *mm, unsigned long address)\n{\n\tpgd_t *dir;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tpte_t entry;\n\n\tdir = pgd_offset(mm, address);\n\tif (pgd_none(*dir))\n\t\treturn NULL;\n\n\tpud = pud_offset(dir, address);\n\tif (pud_none(*pud))\n\t\treturn NULL;\n\n\tpmd = pmd_offset(pud, address);\n\tif (pmd_none(*pmd))\n\t\treturn NULL;\n\n\tpte = pte_offset_kernel(pmd, address);\n\tentry = *pte;\n\tif (pte_none(entry) || !pte_present(entry))\n\t\treturn NULL;\n\n\treturn pte;\n}\n\n/*\n * This routine handles page faults.  It determines the address,\n * and the problem, and then passes it off to one of the appropriate\n * routines.\n */\nasmlinkage void do_page_fault(struct pt_regs *regs, unsigned long writeaccess,\n\t\t\t      unsigned long textaccess, unsigned long address)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct * vma;\n\tconst struct exception_table_entry *fixup;\n\tpte_t *pte;\n\tint fault;\n\n\t/* SIM\n\t * Note this is now called with interrupts still disabled\n\t * This is to cope with being called for a missing IO port\n\t * address with interrupts disabled. This should be fixed as\n\t * soon as we have a better 'fast path' miss handler.\n\t *\n\t * Plus take care how you try and debug this stuff.\n\t * For example, writing debug data to a port which you\n\t * have just faulted on is not going to work.\n\t */\n\n\ttsk = current;\n\tmm = tsk->mm;\n\n\t/* Not an IO address, so reenable interrupts */\n\tlocal_irq_enable();\n\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);\n\n\t/*\n\t * If we're in an interrupt or have no user\n\t * context, we must not take the fault..\n\t */\n\tif (in_atomic() || !mm)\n\t\tgoto no_context;\n\n\t/* TLB misses upon some cache flushes get done under cli() */\n\tdown_read(&mm->mmap_sem);\n\n\tvma = find_vma(mm, address);\n\n\tif (!vma) {\n#ifdef DEBUG_FAULT\n\t\tprint_task(tsk);\n\t\tprintk(\"%s:%d fault, address is 0x%08x PC %016Lx textaccess %d writeaccess %d\\n\",\n\t\t       __func__, __LINE__,\n\t\t       address,regs->pc,textaccess,writeaccess);\n\t\tshow_regs(regs);\n#endif\n\t\tgoto bad_area;\n\t}\n\tif (vma->vm_start <= address) {\n\t\tgoto good_area;\n\t}\n\n\tif (!(vma->vm_flags & VM_GROWSDOWN)) {\n#ifdef DEBUG_FAULT\n\t\tprint_task(tsk);\n\t\tprintk(\"%s:%d fault, address is 0x%08x PC %016Lx textaccess %d writeaccess %d\\n\",\n\t\t       __func__, __LINE__,\n\t\t       address,regs->pc,textaccess,writeaccess);\n\t\tshow_regs(regs);\n\n\t\tprint_vma(vma);\n#endif\n\t\tgoto bad_area;\n\t}\n\tif (expand_stack(vma, address)) {\n#ifdef DEBUG_FAULT\n\t\tprint_task(tsk);\n\t\tprintk(\"%s:%d fault, address is 0x%08x PC %016Lx textaccess %d writeaccess %d\\n\",\n\t\t       __func__, __LINE__,\n\t\t       address,regs->pc,textaccess,writeaccess);\n\t\tshow_regs(regs);\n#endif\n\t\tgoto bad_area;\n\t}\n/*\n * Ok, we have a good vm_area for this memory access, so\n * we can handle it..\n */\ngood_area:\n\tif (textaccess) {\n\t\tif (!(vma->vm_flags & VM_EXEC))\n\t\t\tgoto bad_area;\n\t} else {\n\t\tif (writeaccess) {\n\t\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\t\tgoto bad_area;\n\t\t} else {\n\t\t\tif (!(vma->vm_flags & VM_READ))\n\t\t\t\tgoto bad_area;\n\t\t}\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, writeaccess ? FAULT_FLAG_WRITE : 0);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\n\tif (fault & VM_FAULT_MAJOR) {\n\t\ttsk->maj_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,\n\t\t\t\t     regs, address);\n\t} else {\n\t\ttsk->min_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,\n\t\t\t\t     regs, address);\n\t}\n\n\t/* If we get here, the page fault has been handled.  Do the TLB refill\n\t   now from the newly-setup PTE, to avoid having to fault again right\n\t   away on the same instruction. */\n\tpte = lookup_pte (mm, address);\n\tif (!pte) {\n\t\t/* From empirical evidence, we can get here, due to\n\t\t   !pte_present(pte).  (e.g. if a swap-in occurs, and the page\n\t\t   is swapped back out again before the process that wanted it\n\t\t   gets rescheduled?) */\n\t\tgoto no_pte;\n\t}\n\n\t__do_tlb_refill(address, textaccess, pte);\n\nno_pte:\n\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n/*\n * Something tried to access memory that isn't in our memory map..\n * Fix it, but check if it's kernel or user first..\n */\nbad_area:\n#ifdef DEBUG_FAULT\n\tprintk(\"fault:bad area\\n\");\n#endif\n\tup_read(&mm->mmap_sem);\n\n\tif (user_mode(regs)) {\n\t\tstatic int count=0;\n\t\tsiginfo_t info;\n\t\tif (count < 4) {\n\t\t\t/* This is really to help debug faults when starting\n\t\t\t * usermode, so only need a few */\n\t\t\tcount++;\n\t\t\tprintk(\"user mode bad_area address=%08lx pid=%d (%s) pc=%08lx\\n\",\n\t\t\t\taddress, task_pid_nr(current), current->comm,\n\t\t\t\t(unsigned long) regs->pc);\n#if 0\n\t\t\tshow_regs(regs);\n#endif\n\t\t}\n\t\tif (is_global_init(tsk)) {\n\t\t\tpanic(\"INIT had user mode bad_area\\n\");\n\t\t}\n\t\ttsk->thread.address = address;\n\t\ttsk->thread.error_code = writeaccess;\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_addr = (void *) address;\n\t\tforce_sig_info(SIGSEGV, &info, tsk);\n\t\treturn;\n\t}\n\nno_context:\n#ifdef DEBUG_FAULT\n\tprintk(\"fault:No context\\n\");\n#endif\n\t/* Are we prepared to handle this kernel fault?  */\n\tfixup = search_exception_tables(regs->pc);\n\tif (fixup) {\n\t\tregs->pc = fixup->fixup;\n\t\treturn;\n\t}\n\n/*\n * Oops. The kernel tried to access some bad page. We'll have to\n * terminate things with extreme prejudice.\n *\n */\n\tif (address < PAGE_SIZE)\n\t\tprintk(KERN_ALERT \"Unable to handle kernel NULL pointer dereference\");\n\telse\n\t\tprintk(KERN_ALERT \"Unable to handle kernel paging request\");\n\tprintk(\" at virtual address %08lx\\n\", address);\n\tprintk(KERN_ALERT \"pc = %08Lx%08Lx\\n\", regs->pc >> 32, regs->pc & 0xffffffff);\n\tdie(\"Oops\", regs, writeaccess);\n\tdo_exit(SIGKILL);\n\n/*\n * We ran out of memory, or some other thing happened to us that made\n * us unable to handle the page fault gracefully.\n */\nout_of_memory:\n\tup_read(&mm->mmap_sem);\n\tif (!user_mode(regs))\n\t\tgoto no_context;\n\tpagefault_out_of_memory();\n\treturn;\n\ndo_sigbus:\n\tprintk(\"fault:Do sigbus\\n\");\n\tup_read(&mm->mmap_sem);\n\n\t/*\n\t * Send a sigbus, regardless of whether we were in kernel\n\t * or user mode.\n\t */\n\ttsk->thread.address = address;\n\ttsk->thread.error_code = writeaccess;\n\ttsk->thread.trap_no = 14;\n\tforce_sig(SIGBUS, tsk);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (!user_mode(regs))\n\t\tgoto no_context;\n}\n\nvoid local_flush_tlb_one(unsigned long asid, unsigned long page)\n{\n\tunsigned long long match, pteh=0, lpage;\n\tunsigned long tlb;\n\n\t/*\n\t * Sign-extend based on neff.\n\t */\n\tlpage = neff_sign_extend(page);\n\tmatch = (asid << PTEH_ASID_SHIFT) | PTEH_VALID;\n\tmatch |= lpage;\n\n\tfor_each_itlb_entry(tlb) {\n\t\tasm volatile (\"getcfg\t%1, 0, %0\"\n\t\t\t      : \"=r\" (pteh)\n\t\t\t      : \"r\" (tlb) );\n\n\t\tif (pteh == match) {\n\t\t\t__flush_tlb_slot(tlb);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfor_each_dtlb_entry(tlb) {\n\t\tasm volatile (\"getcfg\t%1, 0, %0\"\n\t\t\t      : \"=r\" (pteh)\n\t\t\t      : \"r\" (tlb) );\n\n\t\tif (pteh == match) {\n\t\t\t__flush_tlb_slot(tlb);\n\t\t\tbreak;\n\t\t}\n\n\t}\n}\n\nvoid local_flush_tlb_page(struct vm_area_struct *vma, unsigned long page)\n{\n\tunsigned long flags;\n\n\tif (vma->vm_mm) {\n\t\tpage &= PAGE_MASK;\n\t\tlocal_irq_save(flags);\n\t\tlocal_flush_tlb_one(get_asid(), page);\n\t\tlocal_irq_restore(flags);\n\t}\n}\n\nvoid local_flush_tlb_range(struct vm_area_struct *vma, unsigned long start,\n\t\t\t   unsigned long end)\n{\n\tunsigned long flags;\n\tunsigned long long match, pteh=0, pteh_epn, pteh_low;\n\tunsigned long tlb;\n\tunsigned int cpu = smp_processor_id();\n\tstruct mm_struct *mm;\n\n\tmm = vma->vm_mm;\n\tif (cpu_context(cpu, mm) == NO_CONTEXT)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tstart &= PAGE_MASK;\n\tend &= PAGE_MASK;\n\n\tmatch = (cpu_asid(cpu, mm) << PTEH_ASID_SHIFT) | PTEH_VALID;\n\n\t/* Flush ITLB */\n\tfor_each_itlb_entry(tlb) {\n\t\tasm volatile (\"getcfg\t%1, 0, %0\"\n\t\t\t      : \"=r\" (pteh)\n\t\t\t      : \"r\" (tlb) );\n\n\t\tpteh_epn = pteh & PAGE_MASK;\n\t\tpteh_low = pteh & ~PAGE_MASK;\n\n\t\tif (pteh_low == match && pteh_epn >= start && pteh_epn <= end)\n\t\t\t__flush_tlb_slot(tlb);\n\t}\n\n\t/* Flush DTLB */\n\tfor_each_dtlb_entry(tlb) {\n\t\tasm volatile (\"getcfg\t%1, 0, %0\"\n\t\t\t      : \"=r\" (pteh)\n\t\t\t      : \"r\" (tlb) );\n\n\t\tpteh_epn = pteh & PAGE_MASK;\n\t\tpteh_low = pteh & ~PAGE_MASK;\n\n\t\tif (pteh_low == match && pteh_epn >= start && pteh_epn <= end)\n\t\t\t__flush_tlb_slot(tlb);\n\t}\n\n\tlocal_irq_restore(flags);\n}\n\nvoid local_flush_tlb_mm(struct mm_struct *mm)\n{\n\tunsigned long flags;\n\tunsigned int cpu = smp_processor_id();\n\n\tif (cpu_context(cpu, mm) == NO_CONTEXT)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tcpu_context(cpu, mm) = NO_CONTEXT;\n\tif (mm == current->mm)\n\t\tactivate_context(mm, cpu);\n\n\tlocal_irq_restore(flags);\n}\n\nvoid local_flush_tlb_all(void)\n{\n\t/* Invalidate all, including shared pages, excluding fixed TLBs */\n\tunsigned long flags, tlb;\n\n\tlocal_irq_save(flags);\n\n\t/* Flush each ITLB entry */\n\tfor_each_itlb_entry(tlb)\n\t\t__flush_tlb_slot(tlb);\n\n\t/* Flush each DTLB entry */\n\tfor_each_dtlb_entry(tlb)\n\t\t__flush_tlb_slot(tlb);\n\n\tlocal_irq_restore(flags);\n}\n\nvoid local_flush_tlb_kernel_range(unsigned long start, unsigned long end)\n{\n        /* FIXME: Optimize this later.. */\n        flush_tlb_all();\n}\n\nvoid __flush_tlb_global(void)\n{\n\tflush_tlb_all();\n}\n\nvoid __update_tlb(struct vm_area_struct *vma, unsigned long address, pte_t pte)\n{\n}\n", "/* Performance event support for sparc64.\n *\n * Copyright (C) 2009, 2010 David S. Miller <davem@davemloft.net>\n *\n * This code is based almost entirely upon the x86 perf event\n * code, which is:\n *\n *  Copyright (C) 2008 Thomas Gleixner <tglx@linutronix.de>\n *  Copyright (C) 2008-2009 Red Hat, Inc., Ingo Molnar\n *  Copyright (C) 2009 Jaswinder Singh Rajput\n *  Copyright (C) 2009 Advanced Micro Devices, Inc., Robert Richter\n *  Copyright (C) 2008-2009 Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>\n */\n\n#include <linux/perf_event.h>\n#include <linux/kprobes.h>\n#include <linux/ftrace.h>\n#include <linux/kernel.h>\n#include <linux/kdebug.h>\n#include <linux/mutex.h>\n\n#include <asm/stacktrace.h>\n#include <asm/cpudata.h>\n#include <asm/uaccess.h>\n#include <asm/atomic.h>\n#include <asm/nmi.h>\n#include <asm/pcr.h>\n\n#include \"kernel.h\"\n#include \"kstack.h\"\n\n/* Sparc64 chips have two performance counters, 32-bits each, with\n * overflow interrupts generated on transition from 0xffffffff to 0.\n * The counters are accessed in one go using a 64-bit register.\n *\n * Both counters are controlled using a single control register.  The\n * only way to stop all sampling is to clear all of the context (user,\n * supervisor, hypervisor) sampling enable bits.  But these bits apply\n * to both counters, thus the two counters can't be enabled/disabled\n * individually.\n *\n * The control register has two event fields, one for each of the two\n * counters.  It's thus nearly impossible to have one counter going\n * while keeping the other one stopped.  Therefore it is possible to\n * get overflow interrupts for counters not currently \"in use\" and\n * that condition must be checked in the overflow interrupt handler.\n *\n * So we use a hack, in that we program inactive counters with the\n * \"sw_count0\" and \"sw_count1\" events.  These count how many times\n * the instruction \"sethi %hi(0xfc000), %g0\" is executed.  It's an\n * unusual way to encode a NOP and therefore will not trigger in\n * normal code.\n */\n\n#define MAX_HWEVENTS\t\t\t2\n#define MAX_PERIOD\t\t\t((1UL << 32) - 1)\n\n#define PIC_UPPER_INDEX\t\t\t0\n#define PIC_LOWER_INDEX\t\t\t1\n#define PIC_NO_INDEX\t\t\t-1\n\nstruct cpu_hw_events {\n\t/* Number of events currently scheduled onto this cpu.\n\t * This tells how many entries in the arrays below\n\t * are valid.\n\t */\n\tint\t\t\tn_events;\n\n\t/* Number of new events added since the last hw_perf_disable().\n\t * This works because the perf event layer always adds new\n\t * events inside of a perf_{disable,enable}() sequence.\n\t */\n\tint\t\t\tn_added;\n\n\t/* Array of events current scheduled on this cpu.  */\n\tstruct perf_event\t*event[MAX_HWEVENTS];\n\n\t/* Array of encoded longs, specifying the %pcr register\n\t * encoding and the mask of PIC counters this even can\n\t * be scheduled on.  See perf_event_encode() et al.\n\t */\n\tunsigned long\t\tevents[MAX_HWEVENTS];\n\n\t/* The current counter index assigned to an event.  When the\n\t * event hasn't been programmed into the cpu yet, this will\n\t * hold PIC_NO_INDEX.  The event->hw.idx value tells us where\n\t * we ought to schedule the event.\n\t */\n\tint\t\t\tcurrent_idx[MAX_HWEVENTS];\n\n\t/* Software copy of %pcr register on this cpu.  */\n\tu64\t\t\tpcr;\n\n\t/* Enabled/disable state.  */\n\tint\t\t\tenabled;\n\n\tunsigned int\t\tgroup_flag;\n};\nDEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events) = { .enabled = 1, };\n\n/* An event map describes the characteristics of a performance\n * counter event.  In particular it gives the encoding as well as\n * a mask telling which counters the event can be measured on.\n */\nstruct perf_event_map {\n\tu16\tencoding;\n\tu8\tpic_mask;\n#define PIC_NONE\t0x00\n#define PIC_UPPER\t0x01\n#define PIC_LOWER\t0x02\n};\n\n/* Encode a perf_event_map entry into a long.  */\nstatic unsigned long perf_event_encode(const struct perf_event_map *pmap)\n{\n\treturn ((unsigned long) pmap->encoding << 16) | pmap->pic_mask;\n}\n\nstatic u8 perf_event_get_msk(unsigned long val)\n{\n\treturn val & 0xff;\n}\n\nstatic u64 perf_event_get_enc(unsigned long val)\n{\n\treturn val >> 16;\n}\n\n#define C(x) PERF_COUNT_HW_CACHE_##x\n\n#define CACHE_OP_UNSUPPORTED\t0xfffe\n#define CACHE_OP_NONSENSE\t0xffff\n\ntypedef struct perf_event_map cache_map_t\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX];\n\nstruct sparc_pmu {\n\tconst struct perf_event_map\t*(*event_map)(int);\n\tconst cache_map_t\t\t*cache_map;\n\tint\t\t\t\tmax_events;\n\tint\t\t\t\tupper_shift;\n\tint\t\t\t\tlower_shift;\n\tint\t\t\t\tevent_mask;\n\tint\t\t\t\thv_bit;\n\tint\t\t\t\tirq_bit;\n\tint\t\t\t\tupper_nop;\n\tint\t\t\t\tlower_nop;\n};\n\nstatic const struct perf_event_map ultra3_perfmon_event_map[] = {\n\t[PERF_COUNT_HW_CPU_CYCLES] = { 0x0000, PIC_UPPER | PIC_LOWER },\n\t[PERF_COUNT_HW_INSTRUCTIONS] = { 0x0001, PIC_UPPER | PIC_LOWER },\n\t[PERF_COUNT_HW_CACHE_REFERENCES] = { 0x0009, PIC_LOWER },\n\t[PERF_COUNT_HW_CACHE_MISSES] = { 0x0009, PIC_UPPER },\n};\n\nstatic const struct perf_event_map *ultra3_event_map(int event_id)\n{\n\treturn &ultra3_perfmon_event_map[event_id];\n}\n\nstatic const cache_map_t ultra3_cache_map = {\n[C(L1D)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x09, PIC_LOWER, },\n\t\t[C(RESULT_MISS)] = { 0x09, PIC_UPPER, },\n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x0a, PIC_LOWER },\n\t\t[C(RESULT_MISS)] = { 0x0a, PIC_UPPER },\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(L1I)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x09, PIC_LOWER, },\n\t\t[C(RESULT_MISS)] = { 0x09, PIC_UPPER, },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_NONSENSE },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_NONSENSE },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(LL)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x0c, PIC_LOWER, },\n\t\t[C(RESULT_MISS)] = { 0x0c, PIC_UPPER, },\n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x0c, PIC_LOWER },\n\t\t[C(RESULT_MISS)] = { 0x0c, PIC_UPPER },\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(DTLB)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0x12, PIC_UPPER, },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(ITLB)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0x11, PIC_UPPER, },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(BPU)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n};\n\nstatic const struct sparc_pmu ultra3_pmu = {\n\t.event_map\t= ultra3_event_map,\n\t.cache_map\t= &ultra3_cache_map,\n\t.max_events\t= ARRAY_SIZE(ultra3_perfmon_event_map),\n\t.upper_shift\t= 11,\n\t.lower_shift\t= 4,\n\t.event_mask\t= 0x3f,\n\t.upper_nop\t= 0x1c,\n\t.lower_nop\t= 0x14,\n};\n\n/* Niagara1 is very limited.  The upper PIC is hard-locked to count\n * only instructions, so it is free running which creates all kinds of\n * problems.  Some hardware designs make one wonder if the creator\n * even looked at how this stuff gets used by software.\n */\nstatic const struct perf_event_map niagara1_perfmon_event_map[] = {\n\t[PERF_COUNT_HW_CPU_CYCLES] = { 0x00, PIC_UPPER },\n\t[PERF_COUNT_HW_INSTRUCTIONS] = { 0x00, PIC_UPPER },\n\t[PERF_COUNT_HW_CACHE_REFERENCES] = { 0, PIC_NONE },\n\t[PERF_COUNT_HW_CACHE_MISSES] = { 0x03, PIC_LOWER },\n};\n\nstatic const struct perf_event_map *niagara1_event_map(int event_id)\n{\n\treturn &niagara1_perfmon_event_map[event_id];\n}\n\nstatic const cache_map_t niagara1_cache_map = {\n[C(L1D)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0x03, PIC_LOWER, },\n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0x03, PIC_LOWER, },\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(L1I)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x00, PIC_UPPER },\n\t\t[C(RESULT_MISS)] = { 0x02, PIC_LOWER, },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_NONSENSE },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_NONSENSE },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(LL)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0x07, PIC_LOWER, },\n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0x07, PIC_LOWER, },\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(DTLB)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0x05, PIC_LOWER, },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(ITLB)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0x04, PIC_LOWER, },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(BPU)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n};\n\nstatic const struct sparc_pmu niagara1_pmu = {\n\t.event_map\t= niagara1_event_map,\n\t.cache_map\t= &niagara1_cache_map,\n\t.max_events\t= ARRAY_SIZE(niagara1_perfmon_event_map),\n\t.upper_shift\t= 0,\n\t.lower_shift\t= 4,\n\t.event_mask\t= 0x7,\n\t.upper_nop\t= 0x0,\n\t.lower_nop\t= 0x0,\n};\n\nstatic const struct perf_event_map niagara2_perfmon_event_map[] = {\n\t[PERF_COUNT_HW_CPU_CYCLES] = { 0x02ff, PIC_UPPER | PIC_LOWER },\n\t[PERF_COUNT_HW_INSTRUCTIONS] = { 0x02ff, PIC_UPPER | PIC_LOWER },\n\t[PERF_COUNT_HW_CACHE_REFERENCES] = { 0x0208, PIC_UPPER | PIC_LOWER },\n\t[PERF_COUNT_HW_CACHE_MISSES] = { 0x0302, PIC_UPPER | PIC_LOWER },\n\t[PERF_COUNT_HW_BRANCH_INSTRUCTIONS] = { 0x0201, PIC_UPPER | PIC_LOWER },\n\t[PERF_COUNT_HW_BRANCH_MISSES] = { 0x0202, PIC_UPPER | PIC_LOWER },\n};\n\nstatic const struct perf_event_map *niagara2_event_map(int event_id)\n{\n\treturn &niagara2_perfmon_event_map[event_id];\n}\n\nstatic const cache_map_t niagara2_cache_map = {\n[C(L1D)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x0208, PIC_UPPER | PIC_LOWER, },\n\t\t[C(RESULT_MISS)] = { 0x0302, PIC_UPPER | PIC_LOWER, },\n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x0210, PIC_UPPER | PIC_LOWER, },\n\t\t[C(RESULT_MISS)] = { 0x0302, PIC_UPPER | PIC_LOWER, },\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(L1I)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x02ff, PIC_UPPER | PIC_LOWER, },\n\t\t[C(RESULT_MISS)] = { 0x0301, PIC_UPPER | PIC_LOWER, },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_NONSENSE },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_NONSENSE },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(LL)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x0208, PIC_UPPER | PIC_LOWER, },\n\t\t[C(RESULT_MISS)] = { 0x0330, PIC_UPPER | PIC_LOWER, },\n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x0210, PIC_UPPER | PIC_LOWER, },\n\t\t[C(RESULT_MISS)] = { 0x0320, PIC_UPPER | PIC_LOWER, },\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(DTLB)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0x0b08, PIC_UPPER | PIC_LOWER, },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(ITLB)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0xb04, PIC_UPPER | PIC_LOWER, },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(BPU)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n};\n\nstatic const struct sparc_pmu niagara2_pmu = {\n\t.event_map\t= niagara2_event_map,\n\t.cache_map\t= &niagara2_cache_map,\n\t.max_events\t= ARRAY_SIZE(niagara2_perfmon_event_map),\n\t.upper_shift\t= 19,\n\t.lower_shift\t= 6,\n\t.event_mask\t= 0xfff,\n\t.hv_bit\t\t= 0x8,\n\t.irq_bit\t= 0x30,\n\t.upper_nop\t= 0x220,\n\t.lower_nop\t= 0x220,\n};\n\nstatic const struct sparc_pmu *sparc_pmu __read_mostly;\n\nstatic u64 event_encoding(u64 event_id, int idx)\n{\n\tif (idx == PIC_UPPER_INDEX)\n\t\tevent_id <<= sparc_pmu->upper_shift;\n\telse\n\t\tevent_id <<= sparc_pmu->lower_shift;\n\treturn event_id;\n}\n\nstatic u64 mask_for_index(int idx)\n{\n\treturn event_encoding(sparc_pmu->event_mask, idx);\n}\n\nstatic u64 nop_for_index(int idx)\n{\n\treturn event_encoding(idx == PIC_UPPER_INDEX ?\n\t\t\t      sparc_pmu->upper_nop :\n\t\t\t      sparc_pmu->lower_nop, idx);\n}\n\nstatic inline void sparc_pmu_enable_event(struct cpu_hw_events *cpuc, struct hw_perf_event *hwc, int idx)\n{\n\tu64 val, mask = mask_for_index(idx);\n\n\tval = cpuc->pcr;\n\tval &= ~mask;\n\tval |= hwc->config;\n\tcpuc->pcr = val;\n\n\tpcr_ops->write(cpuc->pcr);\n}\n\nstatic inline void sparc_pmu_disable_event(struct cpu_hw_events *cpuc, struct hw_perf_event *hwc, int idx)\n{\n\tu64 mask = mask_for_index(idx);\n\tu64 nop = nop_for_index(idx);\n\tu64 val;\n\n\tval = cpuc->pcr;\n\tval &= ~mask;\n\tval |= nop;\n\tcpuc->pcr = val;\n\n\tpcr_ops->write(cpuc->pcr);\n}\n\nstatic u32 read_pmc(int idx)\n{\n\tu64 val;\n\n\tread_pic(val);\n\tif (idx == PIC_UPPER_INDEX)\n\t\tval >>= 32;\n\n\treturn val & 0xffffffff;\n}\n\nstatic void write_pmc(int idx, u64 val)\n{\n\tu64 shift, mask, pic;\n\n\tshift = 0;\n\tif (idx == PIC_UPPER_INDEX)\n\t\tshift = 32;\n\n\tmask = ((u64) 0xffffffff) << shift;\n\tval <<= shift;\n\n\tread_pic(pic);\n\tpic &= ~mask;\n\tpic |= val;\n\twrite_pic(pic);\n}\n\nstatic u64 sparc_perf_event_update(struct perf_event *event,\n\t\t\t\t   struct hw_perf_event *hwc, int idx)\n{\n\tint shift = 64 - 32;\n\tu64 prev_raw_count, new_raw_count;\n\ts64 delta;\n\nagain:\n\tprev_raw_count = local64_read(&hwc->prev_count);\n\tnew_raw_count = read_pmc(idx);\n\n\tif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\n\t\t\t     new_raw_count) != prev_raw_count)\n\t\tgoto again;\n\n\tdelta = (new_raw_count << shift) - (prev_raw_count << shift);\n\tdelta >>= shift;\n\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &hwc->period_left);\n\n\treturn new_raw_count;\n}\n\nstatic int sparc_perf_event_set_period(struct perf_event *event,\n\t\t\t\t       struct hw_perf_event *hwc, int idx)\n{\n\ts64 left = local64_read(&hwc->period_left);\n\ts64 period = hwc->sample_period;\n\tint ret = 0;\n\n\tif (unlikely(left <= -period)) {\n\t\tleft = period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\tif (unlikely(left <= 0)) {\n\t\tleft += period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\tif (left > MAX_PERIOD)\n\t\tleft = MAX_PERIOD;\n\n\tlocal64_set(&hwc->prev_count, (u64)-left);\n\n\twrite_pmc(idx, (u64)(-left) & 0xffffffff);\n\n\tperf_event_update_userpage(event);\n\n\treturn ret;\n}\n\n/* If performance event entries have been added, move existing\n * events around (if necessary) and then assign new entries to\n * counters.\n */\nstatic u64 maybe_change_configuration(struct cpu_hw_events *cpuc, u64 pcr)\n{\n\tint i;\n\n\tif (!cpuc->n_added)\n\t\tgoto out;\n\n\t/* Read in the counters which are moving.  */\n\tfor (i = 0; i < cpuc->n_events; i++) {\n\t\tstruct perf_event *cp = cpuc->event[i];\n\n\t\tif (cpuc->current_idx[i] != PIC_NO_INDEX &&\n\t\t    cpuc->current_idx[i] != cp->hw.idx) {\n\t\t\tsparc_perf_event_update(cp, &cp->hw,\n\t\t\t\t\t\tcpuc->current_idx[i]);\n\t\t\tcpuc->current_idx[i] = PIC_NO_INDEX;\n\t\t}\n\t}\n\n\t/* Assign to counters all unassigned events.  */\n\tfor (i = 0; i < cpuc->n_events; i++) {\n\t\tstruct perf_event *cp = cpuc->event[i];\n\t\tstruct hw_perf_event *hwc = &cp->hw;\n\t\tint idx = hwc->idx;\n\t\tu64 enc;\n\n\t\tif (cpuc->current_idx[i] != PIC_NO_INDEX)\n\t\t\tcontinue;\n\n\t\tsparc_perf_event_set_period(cp, hwc, idx);\n\t\tcpuc->current_idx[i] = idx;\n\n\t\tenc = perf_event_get_enc(cpuc->events[i]);\n\t\tpcr &= ~mask_for_index(idx);\n\t\tif (hwc->state & PERF_HES_STOPPED)\n\t\t\tpcr |= nop_for_index(idx);\n\t\telse\n\t\t\tpcr |= event_encoding(enc, idx);\n\t}\nout:\n\treturn pcr;\n}\n\nstatic void sparc_pmu_enable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tu64 pcr;\n\n\tif (cpuc->enabled)\n\t\treturn;\n\n\tcpuc->enabled = 1;\n\tbarrier();\n\n\tpcr = cpuc->pcr;\n\tif (!cpuc->n_events) {\n\t\tpcr = 0;\n\t} else {\n\t\tpcr = maybe_change_configuration(cpuc, pcr);\n\n\t\t/* We require that all of the events have the same\n\t\t * configuration, so just fetch the settings from the\n\t\t * first entry.\n\t\t */\n\t\tcpuc->pcr = pcr | cpuc->event[0]->hw.config_base;\n\t}\n\n\tpcr_ops->write(cpuc->pcr);\n}\n\nstatic void sparc_pmu_disable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tu64 val;\n\n\tif (!cpuc->enabled)\n\t\treturn;\n\n\tcpuc->enabled = 0;\n\tcpuc->n_added = 0;\n\n\tval = cpuc->pcr;\n\tval &= ~(PCR_UTRACE | PCR_STRACE |\n\t\t sparc_pmu->hv_bit | sparc_pmu->irq_bit);\n\tcpuc->pcr = val;\n\n\tpcr_ops->write(cpuc->pcr);\n}\n\nstatic int active_event_index(struct cpu_hw_events *cpuc,\n\t\t\t      struct perf_event *event)\n{\n\tint i;\n\n\tfor (i = 0; i < cpuc->n_events; i++) {\n\t\tif (cpuc->event[i] == event)\n\t\t\tbreak;\n\t}\n\tBUG_ON(i == cpuc->n_events);\n\treturn cpuc->current_idx[i];\n}\n\nstatic void sparc_pmu_start(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint idx = active_event_index(cpuc, event);\n\n\tif (flags & PERF_EF_RELOAD) {\n\t\tWARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));\n\t\tsparc_perf_event_set_period(event, &event->hw, idx);\n\t}\n\n\tevent->hw.state = 0;\n\n\tsparc_pmu_enable_event(cpuc, &event->hw, idx);\n}\n\nstatic void sparc_pmu_stop(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint idx = active_event_index(cpuc, event);\n\n\tif (!(event->hw.state & PERF_HES_STOPPED)) {\n\t\tsparc_pmu_disable_event(cpuc, &event->hw, idx);\n\t\tevent->hw.state |= PERF_HES_STOPPED;\n\t}\n\n\tif (!(event->hw.state & PERF_HES_UPTODATE) && (flags & PERF_EF_UPDATE)) {\n\t\tsparc_perf_event_update(event, &event->hw, idx);\n\t\tevent->hw.state |= PERF_HES_UPTODATE;\n\t}\n}\n\nstatic void sparc_pmu_del(struct perf_event *event, int _flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tunsigned long flags;\n\tint i;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tfor (i = 0; i < cpuc->n_events; i++) {\n\t\tif (event == cpuc->event[i]) {\n\t\t\t/* Absorb the final count and turn off the\n\t\t\t * event.\n\t\t\t */\n\t\t\tsparc_pmu_stop(event, PERF_EF_UPDATE);\n\n\t\t\t/* Shift remaining entries down into\n\t\t\t * the existing slot.\n\t\t\t */\n\t\t\twhile (++i < cpuc->n_events) {\n\t\t\t\tcpuc->event[i - 1] = cpuc->event[i];\n\t\t\t\tcpuc->events[i - 1] = cpuc->events[i];\n\t\t\t\tcpuc->current_idx[i - 1] =\n\t\t\t\t\tcpuc->current_idx[i];\n\t\t\t}\n\n\t\t\tperf_event_update_userpage(event);\n\n\t\t\tcpuc->n_events--;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\nstatic void sparc_pmu_read(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint idx = active_event_index(cpuc, event);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tsparc_perf_event_update(event, hwc, idx);\n}\n\nstatic atomic_t active_events = ATOMIC_INIT(0);\nstatic DEFINE_MUTEX(pmc_grab_mutex);\n\nstatic void perf_stop_nmi_watchdog(void *unused)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\tstop_nmi_watchdog(NULL);\n\tcpuc->pcr = pcr_ops->read();\n}\n\nvoid perf_event_grab_pmc(void)\n{\n\tif (atomic_inc_not_zero(&active_events))\n\t\treturn;\n\n\tmutex_lock(&pmc_grab_mutex);\n\tif (atomic_read(&active_events) == 0) {\n\t\tif (atomic_read(&nmi_active) > 0) {\n\t\t\ton_each_cpu(perf_stop_nmi_watchdog, NULL, 1);\n\t\t\tBUG_ON(atomic_read(&nmi_active) != 0);\n\t\t}\n\t\tatomic_inc(&active_events);\n\t}\n\tmutex_unlock(&pmc_grab_mutex);\n}\n\nvoid perf_event_release_pmc(void)\n{\n\tif (atomic_dec_and_mutex_lock(&active_events, &pmc_grab_mutex)) {\n\t\tif (atomic_read(&nmi_active) == 0)\n\t\t\ton_each_cpu(start_nmi_watchdog, NULL, 1);\n\t\tmutex_unlock(&pmc_grab_mutex);\n\t}\n}\n\nstatic const struct perf_event_map *sparc_map_cache_event(u64 config)\n{\n\tunsigned int cache_type, cache_op, cache_result;\n\tconst struct perf_event_map *pmap;\n\n\tif (!sparc_pmu->cache_map)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tcache_type = (config >>  0) & 0xff;\n\tif (cache_type >= PERF_COUNT_HW_CACHE_MAX)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tcache_op = (config >>  8) & 0xff;\n\tif (cache_op >= PERF_COUNT_HW_CACHE_OP_MAX)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tcache_result = (config >> 16) & 0xff;\n\tif (cache_result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tpmap = &((*sparc_pmu->cache_map)[cache_type][cache_op][cache_result]);\n\n\tif (pmap->encoding == CACHE_OP_UNSUPPORTED)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tif (pmap->encoding == CACHE_OP_NONSENSE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn pmap;\n}\n\nstatic void hw_perf_event_destroy(struct perf_event *event)\n{\n\tperf_event_release_pmc();\n}\n\n/* Make sure all events can be scheduled into the hardware at\n * the same time.  This is simplified by the fact that we only\n * need to support 2 simultaneous HW events.\n *\n * As a side effect, the evts[]->hw.idx values will be assigned\n * on success.  These are pending indexes.  When the events are\n * actually programmed into the chip, these values will propagate\n * to the per-cpu cpuc->current_idx[] slots, see the code in\n * maybe_change_configuration() for details.\n */\nstatic int sparc_check_constraints(struct perf_event **evts,\n\t\t\t\t   unsigned long *events, int n_ev)\n{\n\tu8 msk0 = 0, msk1 = 0;\n\tint idx0 = 0;\n\n\t/* This case is possible when we are invoked from\n\t * hw_perf_group_sched_in().\n\t */\n\tif (!n_ev)\n\t\treturn 0;\n\n\tif (n_ev > MAX_HWEVENTS)\n\t\treturn -1;\n\n\tmsk0 = perf_event_get_msk(events[0]);\n\tif (n_ev == 1) {\n\t\tif (msk0 & PIC_LOWER)\n\t\t\tidx0 = 1;\n\t\tgoto success;\n\t}\n\tBUG_ON(n_ev != 2);\n\tmsk1 = perf_event_get_msk(events[1]);\n\n\t/* If both events can go on any counter, OK.  */\n\tif (msk0 == (PIC_UPPER | PIC_LOWER) &&\n\t    msk1 == (PIC_UPPER | PIC_LOWER))\n\t\tgoto success;\n\n\t/* If one event is limited to a specific counter,\n\t * and the other can go on both, OK.\n\t */\n\tif ((msk0 == PIC_UPPER || msk0 == PIC_LOWER) &&\n\t    msk1 == (PIC_UPPER | PIC_LOWER)) {\n\t\tif (msk0 & PIC_LOWER)\n\t\t\tidx0 = 1;\n\t\tgoto success;\n\t}\n\n\tif ((msk1 == PIC_UPPER || msk1 == PIC_LOWER) &&\n\t    msk0 == (PIC_UPPER | PIC_LOWER)) {\n\t\tif (msk1 & PIC_UPPER)\n\t\t\tidx0 = 1;\n\t\tgoto success;\n\t}\n\n\t/* If the events are fixed to different counters, OK.  */\n\tif ((msk0 == PIC_UPPER && msk1 == PIC_LOWER) ||\n\t    (msk0 == PIC_LOWER && msk1 == PIC_UPPER)) {\n\t\tif (msk0 & PIC_LOWER)\n\t\t\tidx0 = 1;\n\t\tgoto success;\n\t}\n\n\t/* Otherwise, there is a conflict.  */\n\treturn -1;\n\nsuccess:\n\tevts[0]->hw.idx = idx0;\n\tif (n_ev == 2)\n\t\tevts[1]->hw.idx = idx0 ^ 1;\n\treturn 0;\n}\n\nstatic int check_excludes(struct perf_event **evts, int n_prev, int n_new)\n{\n\tint eu = 0, ek = 0, eh = 0;\n\tstruct perf_event *event;\n\tint i, n, first;\n\n\tn = n_prev + n_new;\n\tif (n <= 1)\n\t\treturn 0;\n\n\tfirst = 1;\n\tfor (i = 0; i < n; i++) {\n\t\tevent = evts[i];\n\t\tif (first) {\n\t\t\teu = event->attr.exclude_user;\n\t\t\tek = event->attr.exclude_kernel;\n\t\t\teh = event->attr.exclude_hv;\n\t\t\tfirst = 0;\n\t\t} else if (event->attr.exclude_user != eu ||\n\t\t\t   event->attr.exclude_kernel != ek ||\n\t\t\t   event->attr.exclude_hv != eh) {\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int collect_events(struct perf_event *group, int max_count,\n\t\t\t  struct perf_event *evts[], unsigned long *events,\n\t\t\t  int *current_idx)\n{\n\tstruct perf_event *event;\n\tint n = 0;\n\n\tif (!is_software_event(group)) {\n\t\tif (n >= max_count)\n\t\t\treturn -1;\n\t\tevts[n] = group;\n\t\tevents[n] = group->hw.event_base;\n\t\tcurrent_idx[n++] = PIC_NO_INDEX;\n\t}\n\tlist_for_each_entry(event, &group->sibling_list, group_entry) {\n\t\tif (!is_software_event(event) &&\n\t\t    event->state != PERF_EVENT_STATE_OFF) {\n\t\t\tif (n >= max_count)\n\t\t\t\treturn -1;\n\t\t\tevts[n] = event;\n\t\t\tevents[n] = event->hw.event_base;\n\t\t\tcurrent_idx[n++] = PIC_NO_INDEX;\n\t\t}\n\t}\n\treturn n;\n}\n\nstatic int sparc_pmu_add(struct perf_event *event, int ef_flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint n0, ret = -EAGAIN;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tn0 = cpuc->n_events;\n\tif (n0 >= MAX_HWEVENTS)\n\t\tgoto out;\n\n\tcpuc->event[n0] = event;\n\tcpuc->events[n0] = event->hw.event_base;\n\tcpuc->current_idx[n0] = PIC_NO_INDEX;\n\n\tevent->hw.state = PERF_HES_UPTODATE;\n\tif (!(ef_flags & PERF_EF_START))\n\t\tevent->hw.state |= PERF_HES_STOPPED;\n\n\t/*\n\t * If group events scheduling transaction was started,\n\t * skip the schedulability test here, it will be performed\n\t * at commit time(->commit_txn) as a whole\n\t */\n\tif (cpuc->group_flag & PERF_EVENT_TXN)\n\t\tgoto nocheck;\n\n\tif (check_excludes(cpuc->event, n0, 1))\n\t\tgoto out;\n\tif (sparc_check_constraints(cpuc->event, cpuc->events, n0 + 1))\n\t\tgoto out;\n\nnocheck:\n\tcpuc->n_events++;\n\tcpuc->n_added++;\n\n\tret = 0;\nout:\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\n\nstatic int sparc_pmu_event_init(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\tstruct perf_event *evts[MAX_HWEVENTS];\n\tstruct hw_perf_event *hwc = &event->hw;\n\tunsigned long events[MAX_HWEVENTS];\n\tint current_idx_dmy[MAX_HWEVENTS];\n\tconst struct perf_event_map *pmap;\n\tint n;\n\n\tif (atomic_read(&nmi_active) < 0)\n\t\treturn -ENODEV;\n\n\tswitch (attr->type) {\n\tcase PERF_TYPE_HARDWARE:\n\t\tif (attr->config >= sparc_pmu->max_events)\n\t\t\treturn -EINVAL;\n\t\tpmap = sparc_pmu->event_map(attr->config);\n\t\tbreak;\n\n\tcase PERF_TYPE_HW_CACHE:\n\t\tpmap = sparc_map_cache_event(attr->config);\n\t\tif (IS_ERR(pmap))\n\t\t\treturn PTR_ERR(pmap);\n\t\tbreak;\n\n\tcase PERF_TYPE_RAW:\n\t\tpmap = NULL;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOENT;\n\n\t}\n\n\tif (pmap) {\n\t\thwc->event_base = perf_event_encode(pmap);\n\t} else {\n\t\t/*\n\t\t * User gives us \"(encoding << 16) | pic_mask\" for\n\t\t * PERF_TYPE_RAW events.\n\t\t */\n\t\thwc->event_base = attr->config;\n\t}\n\n\t/* We save the enable bits in the config_base.  */\n\thwc->config_base = sparc_pmu->irq_bit;\n\tif (!attr->exclude_user)\n\t\thwc->config_base |= PCR_UTRACE;\n\tif (!attr->exclude_kernel)\n\t\thwc->config_base |= PCR_STRACE;\n\tif (!attr->exclude_hv)\n\t\thwc->config_base |= sparc_pmu->hv_bit;\n\n\tn = 0;\n\tif (event->group_leader != event) {\n\t\tn = collect_events(event->group_leader,\n\t\t\t\t   MAX_HWEVENTS - 1,\n\t\t\t\t   evts, events, current_idx_dmy);\n\t\tif (n < 0)\n\t\t\treturn -EINVAL;\n\t}\n\tevents[n] = hwc->event_base;\n\tevts[n] = event;\n\n\tif (check_excludes(evts, n, 1))\n\t\treturn -EINVAL;\n\n\tif (sparc_check_constraints(evts, events, n + 1))\n\t\treturn -EINVAL;\n\n\thwc->idx = PIC_NO_INDEX;\n\n\t/* Try to do all error checking before this point, as unwinding\n\t * state after grabbing the PMC is difficult.\n\t */\n\tperf_event_grab_pmc();\n\tevent->destroy = hw_perf_event_destroy;\n\n\tif (!hwc->sample_period) {\n\t\thwc->sample_period = MAX_PERIOD;\n\t\thwc->last_period = hwc->sample_period;\n\t\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\t}\n\n\treturn 0;\n}\n\n/*\n * Start group events scheduling transaction\n * Set the flag to make pmu::enable() not perform the\n * schedulability test, it will be performed at commit time\n */\nstatic void sparc_pmu_start_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\n\n\tperf_pmu_disable(pmu);\n\tcpuhw->group_flag |= PERF_EVENT_TXN;\n}\n\n/*\n * Stop group events scheduling transaction\n * Clear the flag and pmu::enable() will perform the\n * schedulability test.\n */\nstatic void sparc_pmu_cancel_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\n\n\tcpuhw->group_flag &= ~PERF_EVENT_TXN;\n\tperf_pmu_enable(pmu);\n}\n\n/*\n * Commit group events scheduling transaction\n * Perform the group schedulability test as a whole\n * Return 0 if success\n */\nstatic int sparc_pmu_commit_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint n;\n\n\tif (!sparc_pmu)\n\t\treturn -EINVAL;\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\tn = cpuc->n_events;\n\tif (check_excludes(cpuc->event, 0, n))\n\t\treturn -EINVAL;\n\tif (sparc_check_constraints(cpuc->event, cpuc->events, n))\n\t\treturn -EAGAIN;\n\n\tcpuc->group_flag &= ~PERF_EVENT_TXN;\n\tperf_pmu_enable(pmu);\n\treturn 0;\n}\n\nstatic struct pmu pmu = {\n\t.pmu_enable\t= sparc_pmu_enable,\n\t.pmu_disable\t= sparc_pmu_disable,\n\t.event_init\t= sparc_pmu_event_init,\n\t.add\t\t= sparc_pmu_add,\n\t.del\t\t= sparc_pmu_del,\n\t.start\t\t= sparc_pmu_start,\n\t.stop\t\t= sparc_pmu_stop,\n\t.read\t\t= sparc_pmu_read,\n\t.start_txn\t= sparc_pmu_start_txn,\n\t.cancel_txn\t= sparc_pmu_cancel_txn,\n\t.commit_txn\t= sparc_pmu_commit_txn,\n};\n\nvoid perf_event_print_debug(void)\n{\n\tunsigned long flags;\n\tu64 pcr, pic;\n\tint cpu;\n\n\tif (!sparc_pmu)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tcpu = smp_processor_id();\n\n\tpcr = pcr_ops->read();\n\tread_pic(pic);\n\n\tpr_info(\"\\n\");\n\tpr_info(\"CPU#%d: PCR[%016llx] PIC[%016llx]\\n\",\n\t\tcpu, pcr, pic);\n\n\tlocal_irq_restore(flags);\n}\n\nstatic int __kprobes perf_event_nmi_handler(struct notifier_block *self,\n\t\t\t\t\t    unsigned long cmd, void *__args)\n{\n\tstruct die_args *args = __args;\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct pt_regs *regs;\n\tint i;\n\n\tif (!atomic_read(&active_events))\n\t\treturn NOTIFY_DONE;\n\n\tswitch (cmd) {\n\tcase DIE_NMI:\n\t\tbreak;\n\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n\n\tregs = args->regs;\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\n\t/* If the PMU has the TOE IRQ enable bits, we need to do a\n\t * dummy write to the %pcr to clear the overflow bits and thus\n\t * the interrupt.\n\t *\n\t * Do this before we peek at the counters to determine\n\t * overflow so we don't lose any events.\n\t */\n\tif (sparc_pmu->irq_bit)\n\t\tpcr_ops->write(cpuc->pcr);\n\n\tfor (i = 0; i < cpuc->n_events; i++) {\n\t\tstruct perf_event *event = cpuc->event[i];\n\t\tint idx = cpuc->current_idx[i];\n\t\tstruct hw_perf_event *hwc;\n\t\tu64 val;\n\n\t\thwc = &event->hw;\n\t\tval = sparc_perf_event_update(event, hwc, idx);\n\t\tif (val & (1ULL << 31))\n\t\t\tcontinue;\n\n\t\tdata.period = event->hw.last_period;\n\t\tif (!sparc_perf_event_set_period(event, hwc, idx))\n\t\t\tcontinue;\n\n\t\tif (perf_event_overflow(event, 1, &data, regs))\n\t\t\tsparc_pmu_stop(event, 0);\n\t}\n\n\treturn NOTIFY_STOP;\n}\n\nstatic __read_mostly struct notifier_block perf_event_nmi_notifier = {\n\t.notifier_call\t\t= perf_event_nmi_handler,\n};\n\nstatic bool __init supported_pmu(void)\n{\n\tif (!strcmp(sparc_pmu_type, \"ultra3\") ||\n\t    !strcmp(sparc_pmu_type, \"ultra3+\") ||\n\t    !strcmp(sparc_pmu_type, \"ultra3i\") ||\n\t    !strcmp(sparc_pmu_type, \"ultra4+\")) {\n\t\tsparc_pmu = &ultra3_pmu;\n\t\treturn true;\n\t}\n\tif (!strcmp(sparc_pmu_type, \"niagara\")) {\n\t\tsparc_pmu = &niagara1_pmu;\n\t\treturn true;\n\t}\n\tif (!strcmp(sparc_pmu_type, \"niagara2\")) {\n\t\tsparc_pmu = &niagara2_pmu;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nint __init init_hw_perf_events(void)\n{\n\tpr_info(\"Performance events: \");\n\n\tif (!supported_pmu()) {\n\t\tpr_cont(\"No support for PMU type '%s'\\n\", sparc_pmu_type);\n\t\treturn 0;\n\t}\n\n\tpr_cont(\"Supported PMU type is '%s'\\n\", sparc_pmu_type);\n\n\tperf_pmu_register(&pmu, \"cpu\", PERF_TYPE_RAW);\n\tregister_die_notifier(&perf_event_nmi_notifier);\n\n\treturn 0;\n}\nearly_initcall(init_hw_perf_events);\n\nvoid perf_callchain_kernel(struct perf_callchain_entry *entry,\n\t\t\t   struct pt_regs *regs)\n{\n\tunsigned long ksp, fp;\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\tint graph = 0;\n#endif\n\n\tstack_trace_flush();\n\n\tperf_callchain_store(entry, regs->tpc);\n\n\tksp = regs->u_regs[UREG_I6];\n\tfp = ksp + STACK_BIAS;\n\tdo {\n\t\tstruct sparc_stackf *sf;\n\t\tstruct pt_regs *regs;\n\t\tunsigned long pc;\n\n\t\tif (!kstack_valid(current_thread_info(), fp))\n\t\t\tbreak;\n\n\t\tsf = (struct sparc_stackf *) fp;\n\t\tregs = (struct pt_regs *) (sf + 1);\n\n\t\tif (kstack_is_trap_frame(current_thread_info(), regs)) {\n\t\t\tif (user_mode(regs))\n\t\t\t\tbreak;\n\t\t\tpc = regs->tpc;\n\t\t\tfp = regs->u_regs[UREG_I6] + STACK_BIAS;\n\t\t} else {\n\t\t\tpc = sf->callers_pc;\n\t\t\tfp = (unsigned long)sf->fp + STACK_BIAS;\n\t\t}\n\t\tperf_callchain_store(entry, pc);\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\t\tif ((pc + 8UL) == (unsigned long) &return_to_handler) {\n\t\t\tint index = current->curr_ret_stack;\n\t\t\tif (current->ret_stack && index >= graph) {\n\t\t\t\tpc = current->ret_stack[index - graph].ret;\n\t\t\t\tperf_callchain_store(entry, pc);\n\t\t\t\tgraph++;\n\t\t\t}\n\t\t}\n#endif\n\t} while (entry->nr < PERF_MAX_STACK_DEPTH);\n}\n\nstatic void perf_callchain_user_64(struct perf_callchain_entry *entry,\n\t\t\t\t   struct pt_regs *regs)\n{\n\tunsigned long ufp;\n\n\tperf_callchain_store(entry, regs->tpc);\n\n\tufp = regs->u_regs[UREG_I6] + STACK_BIAS;\n\tdo {\n\t\tstruct sparc_stackf *usf, sf;\n\t\tunsigned long pc;\n\n\t\tusf = (struct sparc_stackf *) ufp;\n\t\tif (__copy_from_user_inatomic(&sf, usf, sizeof(sf)))\n\t\t\tbreak;\n\n\t\tpc = sf.callers_pc;\n\t\tufp = (unsigned long)sf.fp + STACK_BIAS;\n\t\tperf_callchain_store(entry, pc);\n\t} while (entry->nr < PERF_MAX_STACK_DEPTH);\n}\n\nstatic void perf_callchain_user_32(struct perf_callchain_entry *entry,\n\t\t\t\t   struct pt_regs *regs)\n{\n\tunsigned long ufp;\n\n\tperf_callchain_store(entry, regs->tpc);\n\n\tufp = regs->u_regs[UREG_I6] & 0xffffffffUL;\n\tdo {\n\t\tstruct sparc_stackf32 *usf, sf;\n\t\tunsigned long pc;\n\n\t\tusf = (struct sparc_stackf32 *) ufp;\n\t\tif (__copy_from_user_inatomic(&sf, usf, sizeof(sf)))\n\t\t\tbreak;\n\n\t\tpc = sf.callers_pc;\n\t\tufp = (unsigned long)sf.fp;\n\t\tperf_callchain_store(entry, pc);\n\t} while (entry->nr < PERF_MAX_STACK_DEPTH);\n}\n\nvoid\nperf_callchain_user(struct perf_callchain_entry *entry, struct pt_regs *regs)\n{\n\tflushw_user();\n\tif (test_thread_flag(TIF_32BIT))\n\t\tperf_callchain_user_32(entry, regs);\n\telse\n\t\tperf_callchain_user_64(entry, regs);\n}\n", "/*\n * unaligned.c: Unaligned load/store trap handling with special\n *              cases for the kernel to do them more quickly.\n *\n * Copyright (C) 1996 David S. Miller (davem@caip.rutgers.edu)\n * Copyright (C) 1996 Jakub Jelinek (jj@sunsite.mff.cuni.cz)\n */\n\n\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <asm/ptrace.h>\n#include <asm/processor.h>\n#include <asm/system.h>\n#include <asm/uaccess.h>\n#include <linux/smp.h>\n#include <linux/perf_event.h>\n\nenum direction {\n\tload,    /* ld, ldd, ldh, ldsh */\n\tstore,   /* st, std, sth, stsh */\n\tboth,    /* Swap, ldstub, etc. */\n\tfpload,\n\tfpstore,\n\tinvalid,\n};\n\nstatic inline enum direction decode_direction(unsigned int insn)\n{\n\tunsigned long tmp = (insn >> 21) & 1;\n\n\tif(!tmp)\n\t\treturn load;\n\telse {\n\t\tif(((insn>>19)&0x3f) == 15)\n\t\t\treturn both;\n\t\telse\n\t\t\treturn store;\n\t}\n}\n\n/* 8 = double-word, 4 = word, 2 = half-word */\nstatic inline int decode_access_size(unsigned int insn)\n{\n\tinsn = (insn >> 19) & 3;\n\n\tif(!insn)\n\t\treturn 4;\n\telse if(insn == 3)\n\t\treturn 8;\n\telse if(insn == 2)\n\t\treturn 2;\n\telse {\n\t\tprintk(\"Impossible unaligned trap. insn=%08x\\n\", insn);\n\t\tdie_if_kernel(\"Byte sized unaligned access?!?!\", current->thread.kregs);\n\t\treturn 4; /* just to keep gcc happy. */\n\t}\n}\n\n/* 0x400000 = signed, 0 = unsigned */\nstatic inline int decode_signedness(unsigned int insn)\n{\n\treturn (insn & 0x400000);\n}\n\nstatic inline void maybe_flush_windows(unsigned int rs1, unsigned int rs2,\n\t\t\t\t       unsigned int rd)\n{\n\tif(rs2 >= 16 || rs1 >= 16 || rd >= 16) {\n\t\t/* Wheee... */\n\t\t__asm__ __volatile__(\"save %sp, -0x40, %sp\\n\\t\"\n\t\t\t\t     \"save %sp, -0x40, %sp\\n\\t\"\n\t\t\t\t     \"save %sp, -0x40, %sp\\n\\t\"\n\t\t\t\t     \"save %sp, -0x40, %sp\\n\\t\"\n\t\t\t\t     \"save %sp, -0x40, %sp\\n\\t\"\n\t\t\t\t     \"save %sp, -0x40, %sp\\n\\t\"\n\t\t\t\t     \"save %sp, -0x40, %sp\\n\\t\"\n\t\t\t\t     \"restore; restore; restore; restore;\\n\\t\"\n\t\t\t\t     \"restore; restore; restore;\\n\\t\");\n\t}\n}\n\nstatic inline int sign_extend_imm13(int imm)\n{\n\treturn imm << 19 >> 19;\n}\n\nstatic inline unsigned long fetch_reg(unsigned int reg, struct pt_regs *regs)\n{\n\tstruct reg_window32 *win;\n\n\tif(reg < 16)\n\t\treturn (!reg ? 0 : regs->u_regs[reg]);\n\n\t/* Ho hum, the slightly complicated case. */\n\twin = (struct reg_window32 *) regs->u_regs[UREG_FP];\n\treturn win->locals[reg - 16]; /* yes, I know what this does... */\n}\n\nstatic inline unsigned long safe_fetch_reg(unsigned int reg, struct pt_regs *regs)\n{\n\tstruct reg_window32 __user *win;\n\tunsigned long ret;\n\n\tif (reg < 16)\n\t\treturn (!reg ? 0 : regs->u_regs[reg]);\n\n\t/* Ho hum, the slightly complicated case. */\n\twin = (struct reg_window32 __user *) regs->u_regs[UREG_FP];\n\n\tif ((unsigned long)win & 3)\n\t\treturn -1;\n\n\tif (get_user(ret, &win->locals[reg - 16]))\n\t\treturn -1;\n\n\treturn ret;\n}\n\nstatic inline unsigned long *fetch_reg_addr(unsigned int reg, struct pt_regs *regs)\n{\n\tstruct reg_window32 *win;\n\n\tif(reg < 16)\n\t\treturn &regs->u_regs[reg];\n\twin = (struct reg_window32 *) regs->u_regs[UREG_FP];\n\treturn &win->locals[reg - 16];\n}\n\nstatic unsigned long compute_effective_address(struct pt_regs *regs,\n\t\t\t\t\t       unsigned int insn)\n{\n\tunsigned int rs1 = (insn >> 14) & 0x1f;\n\tunsigned int rs2 = insn & 0x1f;\n\tunsigned int rd = (insn >> 25) & 0x1f;\n\n\tif(insn & 0x2000) {\n\t\tmaybe_flush_windows(rs1, 0, rd);\n\t\treturn (fetch_reg(rs1, regs) + sign_extend_imm13(insn));\n\t} else {\n\t\tmaybe_flush_windows(rs1, rs2, rd);\n\t\treturn (fetch_reg(rs1, regs) + fetch_reg(rs2, regs));\n\t}\n}\n\nunsigned long safe_compute_effective_address(struct pt_regs *regs,\n\t\t\t\t\t     unsigned int insn)\n{\n\tunsigned int rs1 = (insn >> 14) & 0x1f;\n\tunsigned int rs2 = insn & 0x1f;\n\tunsigned int rd = (insn >> 25) & 0x1f;\n\n\tif(insn & 0x2000) {\n\t\tmaybe_flush_windows(rs1, 0, rd);\n\t\treturn (safe_fetch_reg(rs1, regs) + sign_extend_imm13(insn));\n\t} else {\n\t\tmaybe_flush_windows(rs1, rs2, rd);\n\t\treturn (safe_fetch_reg(rs1, regs) + safe_fetch_reg(rs2, regs));\n\t}\n}\n\n/* This is just to make gcc think panic does return... */\nstatic void unaligned_panic(char *str)\n{\n\tpanic(str);\n}\n\n/* una_asm.S */\nextern int do_int_load(unsigned long *dest_reg, int size,\n\t\t       unsigned long *saddr, int is_signed);\nextern int __do_int_store(unsigned long *dst_addr, int size,\n\t\t\t  unsigned long *src_val);\n\nstatic int do_int_store(int reg_num, int size, unsigned long *dst_addr,\n\t\t\tstruct pt_regs *regs)\n{\n\tunsigned long zero[2] = { 0, 0 };\n\tunsigned long *src_val;\n\n\tif (reg_num)\n\t\tsrc_val = fetch_reg_addr(reg_num, regs);\n\telse {\n\t\tsrc_val = &zero[0];\n\t\tif (size == 8)\n\t\t\tzero[1] = fetch_reg(1, regs);\n\t}\n\treturn __do_int_store(dst_addr, size, src_val);\n}\n\nextern void smp_capture(void);\nextern void smp_release(void);\n\nstatic inline void advance(struct pt_regs *regs)\n{\n\tregs->pc   = regs->npc;\n\tregs->npc += 4;\n}\n\nstatic inline int floating_point_load_or_store_p(unsigned int insn)\n{\n\treturn (insn >> 24) & 1;\n}\n\nstatic inline int ok_for_kernel(unsigned int insn)\n{\n\treturn !floating_point_load_or_store_p(insn);\n}\n\nstatic void kernel_mna_trap_fault(struct pt_regs *regs, unsigned int insn)\n{\n\tunsigned long g2 = regs->u_regs [UREG_G2];\n\tunsigned long fixup = search_extables_range(regs->pc, &g2);\n\n\tif (!fixup) {\n\t\tunsigned long address = compute_effective_address(regs, insn);\n        \tif(address < PAGE_SIZE) {\n                \tprintk(KERN_ALERT \"Unable to handle kernel NULL pointer dereference in mna handler\");\n        \t} else\n                \tprintk(KERN_ALERT \"Unable to handle kernel paging request in mna handler\");\n\t        printk(KERN_ALERT \" at virtual address %08lx\\n\",address);\n\t\tprintk(KERN_ALERT \"current->{mm,active_mm}->context = %08lx\\n\",\n\t\t\t(current->mm ? current->mm->context :\n\t\t\tcurrent->active_mm->context));\n\t\tprintk(KERN_ALERT \"current->{mm,active_mm}->pgd = %08lx\\n\",\n\t\t\t(current->mm ? (unsigned long) current->mm->pgd :\n\t\t\t(unsigned long) current->active_mm->pgd));\n\t        die_if_kernel(\"Oops\", regs);\n\t\t/* Not reached */\n\t}\n\tregs->pc = fixup;\n\tregs->npc = regs->pc + 4;\n\tregs->u_regs [UREG_G2] = g2;\n}\n\nasmlinkage void kernel_unaligned_trap(struct pt_regs *regs, unsigned int insn)\n{\n\tenum direction dir = decode_direction(insn);\n\tint size = decode_access_size(insn);\n\n\tif(!ok_for_kernel(insn) || dir == both) {\n\t\tprintk(\"Unsupported unaligned load/store trap for kernel at <%08lx>.\\n\",\n\t\t       regs->pc);\n\t\tunaligned_panic(\"Wheee. Kernel does fpu/atomic unaligned load/store.\");\n\t} else {\n\t\tunsigned long addr = compute_effective_address(regs, insn);\n\t\tint err;\n\n\t\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, addr);\n\t\tswitch (dir) {\n\t\tcase load:\n\t\t\terr = do_int_load(fetch_reg_addr(((insn>>25)&0x1f),\n\t\t\t\t\t\t\t regs),\n\t\t\t\t\t  size, (unsigned long *) addr,\n\t\t\t\t\t  decode_signedness(insn));\n\t\t\tbreak;\n\n\t\tcase store:\n\t\t\terr = do_int_store(((insn>>25)&0x1f), size,\n\t\t\t\t\t   (unsigned long *) addr, regs);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpanic(\"Impossible kernel unaligned trap.\");\n\t\t\t/* Not reached... */\n\t\t}\n\t\tif (err)\n\t\t\tkernel_mna_trap_fault(regs, insn);\n\t\telse\n\t\t\tadvance(regs);\n\t}\n}\n\nstatic inline int ok_for_user(struct pt_regs *regs, unsigned int insn,\n\t\t\t      enum direction dir)\n{\n\tunsigned int reg;\n\tint check = (dir == load) ? VERIFY_READ : VERIFY_WRITE;\n\tint size = ((insn >> 19) & 3) == 3 ? 8 : 4;\n\n\tif ((regs->pc | regs->npc) & 3)\n\t\treturn 0;\n\n\t/* Must access_ok() in all the necessary places. */\n#define WINREG_ADDR(regnum) \\\n\t((void __user *)(((unsigned long *)regs->u_regs[UREG_FP])+(regnum)))\n\n\treg = (insn >> 25) & 0x1f;\n\tif (reg >= 16) {\n\t\tif (!access_ok(check, WINREG_ADDR(reg - 16), size))\n\t\t\treturn -EFAULT;\n\t}\n\treg = (insn >> 14) & 0x1f;\n\tif (reg >= 16) {\n\t\tif (!access_ok(check, WINREG_ADDR(reg - 16), size))\n\t\t\treturn -EFAULT;\n\t}\n\tif (!(insn & 0x2000)) {\n\t\treg = (insn & 0x1f);\n\t\tif (reg >= 16) {\n\t\t\tif (!access_ok(check, WINREG_ADDR(reg - 16), size))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t}\n#undef WINREG_ADDR\n\treturn 0;\n}\n\nstatic void user_mna_trap_fault(struct pt_regs *regs, unsigned int insn)\n{\n\tsiginfo_t info;\n\n\tinfo.si_signo = SIGBUS;\n\tinfo.si_errno = 0;\n\tinfo.si_code = BUS_ADRALN;\n\tinfo.si_addr = (void __user *)safe_compute_effective_address(regs, insn);\n\tinfo.si_trapno = 0;\n\tsend_sig_info(SIGBUS, &info, current);\n}\n\nasmlinkage void user_unaligned_trap(struct pt_regs *regs, unsigned int insn)\n{\n\tenum direction dir;\n\n\tif(!(current->thread.flags & SPARC_FLAG_UNALIGNED) ||\n\t   (((insn >> 30) & 3) != 3))\n\t\tgoto kill_user;\n\tdir = decode_direction(insn);\n\tif(!ok_for_user(regs, insn, dir)) {\n\t\tgoto kill_user;\n\t} else {\n\t\tint err, size = decode_access_size(insn);\n\t\tunsigned long addr;\n\n\t\tif(floating_point_load_or_store_p(insn)) {\n\t\t\tprintk(\"User FPU load/store unaligned unsupported.\\n\");\n\t\t\tgoto kill_user;\n\t\t}\n\n\t\taddr = compute_effective_address(regs, insn);\n\t\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, addr);\n\t\tswitch(dir) {\n\t\tcase load:\n\t\t\terr = do_int_load(fetch_reg_addr(((insn>>25)&0x1f),\n\t\t\t\t\t\t\t regs),\n\t\t\t\t\t  size, (unsigned long *) addr,\n\t\t\t\t\t  decode_signedness(insn));\n\t\t\tbreak;\n\n\t\tcase store:\n\t\t\terr = do_int_store(((insn>>25)&0x1f), size,\n\t\t\t\t\t   (unsigned long *) addr, regs);\n\t\t\tbreak;\n\n\t\tcase both:\n\t\t\t/*\n\t\t\t * This was supported in 2.4. However, we question\n\t\t\t * the value of SWAP instruction across word boundaries.\n\t\t\t */\n\t\t\tprintk(\"Unaligned SWAP unsupported.\\n\");\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tunaligned_panic(\"Impossible user unaligned trap.\");\n\t\t\tgoto out;\n\t\t}\n\t\tif (err)\n\t\t\tgoto kill_user;\n\t\telse\n\t\t\tadvance(regs);\n\t\tgoto out;\n\t}\n\nkill_user:\n\tuser_mna_trap_fault(regs, insn);\nout:\n\t;\n}\n", "/*\n * unaligned.c: Unaligned load/store trap handling with special\n *              cases for the kernel to do them more quickly.\n *\n * Copyright (C) 1996,2008 David S. Miller (davem@davemloft.net)\n * Copyright (C) 1996,1997 Jakub Jelinek (jj@sunsite.mff.cuni.cz)\n */\n\n\n#include <linux/jiffies.h>\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <asm/asi.h>\n#include <asm/ptrace.h>\n#include <asm/pstate.h>\n#include <asm/processor.h>\n#include <asm/system.h>\n#include <asm/uaccess.h>\n#include <linux/smp.h>\n#include <linux/bitops.h>\n#include <linux/perf_event.h>\n#include <linux/ratelimit.h>\n#include <asm/fpumacro.h>\n\nenum direction {\n\tload,    /* ld, ldd, ldh, ldsh */\n\tstore,   /* st, std, sth, stsh */\n\tboth,    /* Swap, ldstub, cas, ... */\n\tfpld,\n\tfpst,\n\tinvalid,\n};\n\nstatic inline enum direction decode_direction(unsigned int insn)\n{\n\tunsigned long tmp = (insn >> 21) & 1;\n\n\tif (!tmp)\n\t\treturn load;\n\telse {\n\t\tswitch ((insn>>19)&0xf) {\n\t\tcase 15: /* swap* */\n\t\t\treturn both;\n\t\tdefault:\n\t\t\treturn store;\n\t\t}\n\t}\n}\n\n/* 16 = double-word, 8 = extra-word, 4 = word, 2 = half-word */\nstatic inline int decode_access_size(struct pt_regs *regs, unsigned int insn)\n{\n\tunsigned int tmp;\n\n\ttmp = ((insn >> 19) & 0xf);\n\tif (tmp == 11 || tmp == 14) /* ldx/stx */\n\t\treturn 8;\n\ttmp &= 3;\n\tif (!tmp)\n\t\treturn 4;\n\telse if (tmp == 3)\n\t\treturn 16;\t/* ldd/std - Although it is actually 8 */\n\telse if (tmp == 2)\n\t\treturn 2;\n\telse {\n\t\tprintk(\"Impossible unaligned trap. insn=%08x\\n\", insn);\n\t\tdie_if_kernel(\"Byte sized unaligned access?!?!\", regs);\n\n\t\t/* GCC should never warn that control reaches the end\n\t\t * of this function without returning a value because\n\t\t * die_if_kernel() is marked with attribute 'noreturn'.\n\t\t * Alas, some versions do...\n\t\t */\n\n\t\treturn 0;\n\t}\n}\n\nstatic inline int decode_asi(unsigned int insn, struct pt_regs *regs)\n{\n\tif (insn & 0x800000) {\n\t\tif (insn & 0x2000)\n\t\t\treturn (unsigned char)(regs->tstate >> 24);\t/* %asi */\n\t\telse\n\t\t\treturn (unsigned char)(insn >> 5);\t\t/* imm_asi */\n\t} else\n\t\treturn ASI_P;\n}\n\n/* 0x400000 = signed, 0 = unsigned */\nstatic inline int decode_signedness(unsigned int insn)\n{\n\treturn (insn & 0x400000);\n}\n\nstatic inline void maybe_flush_windows(unsigned int rs1, unsigned int rs2,\n\t\t\t\t       unsigned int rd, int from_kernel)\n{\n\tif (rs2 >= 16 || rs1 >= 16 || rd >= 16) {\n\t\tif (from_kernel != 0)\n\t\t\t__asm__ __volatile__(\"flushw\");\n\t\telse\n\t\t\tflushw_user();\n\t}\n}\n\nstatic inline long sign_extend_imm13(long imm)\n{\n\treturn imm << 51 >> 51;\n}\n\nstatic unsigned long fetch_reg(unsigned int reg, struct pt_regs *regs)\n{\n\tunsigned long value;\n\t\n\tif (reg < 16)\n\t\treturn (!reg ? 0 : regs->u_regs[reg]);\n\tif (regs->tstate & TSTATE_PRIV) {\n\t\tstruct reg_window *win;\n\t\twin = (struct reg_window *)(regs->u_regs[UREG_FP] + STACK_BIAS);\n\t\tvalue = win->locals[reg - 16];\n\t} else if (test_thread_flag(TIF_32BIT)) {\n\t\tstruct reg_window32 __user *win32;\n\t\twin32 = (struct reg_window32 __user *)((unsigned long)((u32)regs->u_regs[UREG_FP]));\n\t\tget_user(value, &win32->locals[reg - 16]);\n\t} else {\n\t\tstruct reg_window __user *win;\n\t\twin = (struct reg_window __user *)(regs->u_regs[UREG_FP] + STACK_BIAS);\n\t\tget_user(value, &win->locals[reg - 16]);\n\t}\n\treturn value;\n}\n\nstatic unsigned long *fetch_reg_addr(unsigned int reg, struct pt_regs *regs)\n{\n\tif (reg < 16)\n\t\treturn &regs->u_regs[reg];\n\tif (regs->tstate & TSTATE_PRIV) {\n\t\tstruct reg_window *win;\n\t\twin = (struct reg_window *)(regs->u_regs[UREG_FP] + STACK_BIAS);\n\t\treturn &win->locals[reg - 16];\n\t} else if (test_thread_flag(TIF_32BIT)) {\n\t\tstruct reg_window32 *win32;\n\t\twin32 = (struct reg_window32 *)((unsigned long)((u32)regs->u_regs[UREG_FP]));\n\t\treturn (unsigned long *)&win32->locals[reg - 16];\n\t} else {\n\t\tstruct reg_window *win;\n\t\twin = (struct reg_window *)(regs->u_regs[UREG_FP] + STACK_BIAS);\n\t\treturn &win->locals[reg - 16];\n\t}\n}\n\nunsigned long compute_effective_address(struct pt_regs *regs,\n\t\t\t\t\tunsigned int insn, unsigned int rd)\n{\n\tunsigned int rs1 = (insn >> 14) & 0x1f;\n\tunsigned int rs2 = insn & 0x1f;\n\tint from_kernel = (regs->tstate & TSTATE_PRIV) != 0;\n\n\tif (insn & 0x2000) {\n\t\tmaybe_flush_windows(rs1, 0, rd, from_kernel);\n\t\treturn (fetch_reg(rs1, regs) + sign_extend_imm13(insn));\n\t} else {\n\t\tmaybe_flush_windows(rs1, rs2, rd, from_kernel);\n\t\treturn (fetch_reg(rs1, regs) + fetch_reg(rs2, regs));\n\t}\n}\n\n/* This is just to make gcc think die_if_kernel does return... */\nstatic void __used unaligned_panic(char *str, struct pt_regs *regs)\n{\n\tdie_if_kernel(str, regs);\n}\n\nextern int do_int_load(unsigned long *dest_reg, int size,\n\t\t       unsigned long *saddr, int is_signed, int asi);\n\t\nextern int __do_int_store(unsigned long *dst_addr, int size,\n\t\t\t  unsigned long src_val, int asi);\n\nstatic inline int do_int_store(int reg_num, int size, unsigned long *dst_addr,\n\t\t\t       struct pt_regs *regs, int asi, int orig_asi)\n{\n\tunsigned long zero = 0;\n\tunsigned long *src_val_p = &zero;\n\tunsigned long src_val;\n\n\tif (size == 16) {\n\t\tsize = 8;\n\t\tzero = (((long)(reg_num ?\n\t\t        (unsigned)fetch_reg(reg_num, regs) : 0)) << 32) |\n\t\t\t(unsigned)fetch_reg(reg_num + 1, regs);\n\t} else if (reg_num) {\n\t\tsrc_val_p = fetch_reg_addr(reg_num, regs);\n\t}\n\tsrc_val = *src_val_p;\n\tif (unlikely(asi != orig_asi)) {\n\t\tswitch (size) {\n\t\tcase 2:\n\t\t\tsrc_val = swab16(src_val);\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\tsrc_val = swab32(src_val);\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tsrc_val = swab64(src_val);\n\t\t\tbreak;\n\t\tcase 16:\n\t\tdefault:\n\t\t\tBUG();\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn __do_int_store(dst_addr, size, src_val, asi);\n}\n\nstatic inline void advance(struct pt_regs *regs)\n{\n\tregs->tpc   = regs->tnpc;\n\tregs->tnpc += 4;\n\tif (test_thread_flag(TIF_32BIT)) {\n\t\tregs->tpc &= 0xffffffff;\n\t\tregs->tnpc &= 0xffffffff;\n\t}\n}\n\nstatic inline int floating_point_load_or_store_p(unsigned int insn)\n{\n\treturn (insn >> 24) & 1;\n}\n\nstatic inline int ok_for_kernel(unsigned int insn)\n{\n\treturn !floating_point_load_or_store_p(insn);\n}\n\nstatic void kernel_mna_trap_fault(int fixup_tstate_asi)\n{\n\tstruct pt_regs *regs = current_thread_info()->kern_una_regs;\n\tunsigned int insn = current_thread_info()->kern_una_insn;\n\tconst struct exception_table_entry *entry;\n\n\tentry = search_exception_tables(regs->tpc);\n\tif (!entry) {\n\t\tunsigned long address;\n\n\t\taddress = compute_effective_address(regs, insn,\n\t\t\t\t\t\t    ((insn >> 25) & 0x1f));\n        \tif (address < PAGE_SIZE) {\n                \tprintk(KERN_ALERT \"Unable to handle kernel NULL \"\n\t\t\t       \"pointer dereference in mna handler\");\n        \t} else\n                \tprintk(KERN_ALERT \"Unable to handle kernel paging \"\n\t\t\t       \"request in mna handler\");\n\t        printk(KERN_ALERT \" at virtual address %016lx\\n\",address);\n\t\tprintk(KERN_ALERT \"current->{active_,}mm->context = %016lx\\n\",\n\t\t\t(current->mm ? CTX_HWBITS(current->mm->context) :\n\t\t\tCTX_HWBITS(current->active_mm->context)));\n\t\tprintk(KERN_ALERT \"current->{active_,}mm->pgd = %016lx\\n\",\n\t\t\t(current->mm ? (unsigned long) current->mm->pgd :\n\t\t\t(unsigned long) current->active_mm->pgd));\n\t        die_if_kernel(\"Oops\", regs);\n\t\t/* Not reached */\n\t}\n\tregs->tpc = entry->fixup;\n\tregs->tnpc = regs->tpc + 4;\n\n\tif (fixup_tstate_asi) {\n\t\tregs->tstate &= ~TSTATE_ASI;\n\t\tregs->tstate |= (ASI_AIUS << 24UL);\n\t}\n}\n\nstatic void log_unaligned(struct pt_regs *regs)\n{\n\tstatic DEFINE_RATELIMIT_STATE(ratelimit, 5 * HZ, 5);\n\n\tif (__ratelimit(&ratelimit)) {\n\t\tprintk(\"Kernel unaligned access at TPC[%lx] %pS\\n\",\n\t\t       regs->tpc, (void *) regs->tpc);\n\t}\n}\n\nasmlinkage void kernel_unaligned_trap(struct pt_regs *regs, unsigned int insn)\n{\n\tenum direction dir = decode_direction(insn);\n\tint size = decode_access_size(regs, insn);\n\tint orig_asi, asi;\n\n\tcurrent_thread_info()->kern_una_regs = regs;\n\tcurrent_thread_info()->kern_una_insn = insn;\n\n\torig_asi = asi = decode_asi(insn, regs);\n\n\t/* If this is a {get,put}_user() on an unaligned userspace pointer,\n\t * just signal a fault and do not log the event.\n\t */\n\tif (asi == ASI_AIUS) {\n\t\tkernel_mna_trap_fault(0);\n\t\treturn;\n\t}\n\n\tlog_unaligned(regs);\n\n\tif (!ok_for_kernel(insn) || dir == both) {\n\t\tprintk(\"Unsupported unaligned load/store trap for kernel \"\n\t\t       \"at <%016lx>.\\n\", regs->tpc);\n\t\tunaligned_panic(\"Kernel does fpu/atomic \"\n\t\t\t\t\"unaligned load/store.\", regs);\n\n\t\tkernel_mna_trap_fault(0);\n\t} else {\n\t\tunsigned long addr, *reg_addr;\n\t\tint err;\n\n\t\taddr = compute_effective_address(regs, insn,\n\t\t\t\t\t\t ((insn >> 25) & 0x1f));\n\t\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, addr);\n\t\tswitch (asi) {\n\t\tcase ASI_NL:\n\t\tcase ASI_AIUPL:\n\t\tcase ASI_AIUSL:\n\t\tcase ASI_PL:\n\t\tcase ASI_SL:\n\t\tcase ASI_PNFL:\n\t\tcase ASI_SNFL:\n\t\t\tasi &= ~0x08;\n\t\t\tbreak;\n\t\t}\n\t\tswitch (dir) {\n\t\tcase load:\n\t\t\treg_addr = fetch_reg_addr(((insn>>25)&0x1f), regs);\n\t\t\terr = do_int_load(reg_addr, size,\n\t\t\t\t\t  (unsigned long *) addr,\n\t\t\t\t\t  decode_signedness(insn), asi);\n\t\t\tif (likely(!err) && unlikely(asi != orig_asi)) {\n\t\t\t\tunsigned long val_in = *reg_addr;\n\t\t\t\tswitch (size) {\n\t\t\t\tcase 2:\n\t\t\t\t\tval_in = swab16(val_in);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 4:\n\t\t\t\t\tval_in = swab32(val_in);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 8:\n\t\t\t\t\tval_in = swab64(val_in);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 16:\n\t\t\t\tdefault:\n\t\t\t\t\tBUG();\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\t*reg_addr = val_in;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase store:\n\t\t\terr = do_int_store(((insn>>25)&0x1f), size,\n\t\t\t\t\t   (unsigned long *) addr, regs,\n\t\t\t\t\t   asi, orig_asi);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tpanic(\"Impossible kernel unaligned trap.\");\n\t\t\t/* Not reached... */\n\t\t}\n\t\tif (unlikely(err))\n\t\t\tkernel_mna_trap_fault(1);\n\t\telse\n\t\t\tadvance(regs);\n\t}\n}\n\nstatic char popc_helper[] = {\n0, 1, 1, 2, 1, 2, 2, 3,\n1, 2, 2, 3, 2, 3, 3, 4, \n};\n\nint handle_popc(u32 insn, struct pt_regs *regs)\n{\n\tu64 value;\n\tint ret, i, rd = ((insn >> 25) & 0x1f);\n\tint from_kernel = (regs->tstate & TSTATE_PRIV) != 0;\n\t                        \n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);\n\tif (insn & 0x2000) {\n\t\tmaybe_flush_windows(0, 0, rd, from_kernel);\n\t\tvalue = sign_extend_imm13(insn);\n\t} else {\n\t\tmaybe_flush_windows(0, insn & 0x1f, rd, from_kernel);\n\t\tvalue = fetch_reg(insn & 0x1f, regs);\n\t}\n\tfor (ret = 0, i = 0; i < 16; i++) {\n\t\tret += popc_helper[value & 0xf];\n\t\tvalue >>= 4;\n\t}\n\tif (rd < 16) {\n\t\tif (rd)\n\t\t\tregs->u_regs[rd] = ret;\n\t} else {\n\t\tif (test_thread_flag(TIF_32BIT)) {\n\t\t\tstruct reg_window32 __user *win32;\n\t\t\twin32 = (struct reg_window32 __user *)((unsigned long)((u32)regs->u_regs[UREG_FP]));\n\t\t\tput_user(ret, &win32->locals[rd - 16]);\n\t\t} else {\n\t\t\tstruct reg_window __user *win;\n\t\t\twin = (struct reg_window __user *)(regs->u_regs[UREG_FP] + STACK_BIAS);\n\t\t\tput_user(ret, &win->locals[rd - 16]);\n\t\t}\n\t}\n\tadvance(regs);\n\treturn 1;\n}\n\nextern void do_fpother(struct pt_regs *regs);\nextern void do_privact(struct pt_regs *regs);\nextern void spitfire_data_access_exception(struct pt_regs *regs,\n\t\t\t\t\t   unsigned long sfsr,\n\t\t\t\t\t   unsigned long sfar);\nextern void sun4v_data_access_exception(struct pt_regs *regs,\n\t\t\t\t\tunsigned long addr,\n\t\t\t\t\tunsigned long type_ctx);\n\nint handle_ldf_stq(u32 insn, struct pt_regs *regs)\n{\n\tunsigned long addr = compute_effective_address(regs, insn, 0);\n\tint freg = ((insn >> 25) & 0x1e) | ((insn >> 20) & 0x20);\n\tstruct fpustate *f = FPUSTATE;\n\tint asi = decode_asi(insn, regs);\n\tint flag = (freg < 32) ? FPRS_DL : FPRS_DU;\n\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);\n\n\tsave_and_clear_fpu();\n\tcurrent_thread_info()->xfsr[0] &= ~0x1c000;\n\tif (freg & 3) {\n\t\tcurrent_thread_info()->xfsr[0] |= (6 << 14) /* invalid_fp_register */;\n\t\tdo_fpother(regs);\n\t\treturn 0;\n\t}\n\tif (insn & 0x200000) {\n\t\t/* STQ */\n\t\tu64 first = 0, second = 0;\n\t\t\n\t\tif (current_thread_info()->fpsaved[0] & flag) {\n\t\t\tfirst = *(u64 *)&f->regs[freg];\n\t\t\tsecond = *(u64 *)&f->regs[freg+2];\n\t\t}\n\t\tif (asi < 0x80) {\n\t\t\tdo_privact(regs);\n\t\t\treturn 1;\n\t\t}\n\t\tswitch (asi) {\n\t\tcase ASI_P:\n\t\tcase ASI_S: break;\n\t\tcase ASI_PL:\n\t\tcase ASI_SL: \n\t\t\t{\n\t\t\t\t/* Need to convert endians */\n\t\t\t\tu64 tmp = __swab64p(&first);\n\t\t\t\t\n\t\t\t\tfirst = __swab64p(&second);\n\t\t\t\tsecond = tmp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tdefault:\n\t\t\tif (tlb_type == hypervisor)\n\t\t\t\tsun4v_data_access_exception(regs, addr, 0);\n\t\t\telse\n\t\t\t\tspitfire_data_access_exception(regs, 0, addr);\n\t\t\treturn 1;\n\t\t}\n\t\tif (put_user (first >> 32, (u32 __user *)addr) ||\n\t\t    __put_user ((u32)first, (u32 __user *)(addr + 4)) ||\n\t\t    __put_user (second >> 32, (u32 __user *)(addr + 8)) ||\n\t\t    __put_user ((u32)second, (u32 __user *)(addr + 12))) {\n\t\t\tif (tlb_type == hypervisor)\n\t\t\t\tsun4v_data_access_exception(regs, addr, 0);\n\t\t\telse\n\t\t\t\tspitfire_data_access_exception(regs, 0, addr);\n\t\t    \treturn 1;\n\t\t}\n\t} else {\n\t\t/* LDF, LDDF, LDQF */\n\t\tu32 data[4] __attribute__ ((aligned(8)));\n\t\tint size, i;\n\t\tint err;\n\n\t\tif (asi < 0x80) {\n\t\t\tdo_privact(regs);\n\t\t\treturn 1;\n\t\t} else if (asi > ASI_SNFL) {\n\t\t\tif (tlb_type == hypervisor)\n\t\t\t\tsun4v_data_access_exception(regs, addr, 0);\n\t\t\telse\n\t\t\t\tspitfire_data_access_exception(regs, 0, addr);\n\t\t\treturn 1;\n\t\t}\n\t\tswitch (insn & 0x180000) {\n\t\tcase 0x000000: size = 1; break;\n\t\tcase 0x100000: size = 4; break;\n\t\tdefault: size = 2; break;\n\t\t}\n\t\tfor (i = 0; i < size; i++)\n\t\t\tdata[i] = 0;\n\t\t\n\t\terr = get_user (data[0], (u32 __user *) addr);\n\t\tif (!err) {\n\t\t\tfor (i = 1; i < size; i++)\n\t\t\t\terr |= __get_user (data[i], (u32 __user *)(addr + 4*i));\n\t\t}\n\t\tif (err && !(asi & 0x2 /* NF */)) {\n\t\t\tif (tlb_type == hypervisor)\n\t\t\t\tsun4v_data_access_exception(regs, addr, 0);\n\t\t\telse\n\t\t\t\tspitfire_data_access_exception(regs, 0, addr);\n\t\t\treturn 1;\n\t\t}\n\t\tif (asi & 0x8) /* Little */ {\n\t\t\tu64 tmp;\n\n\t\t\tswitch (size) {\n\t\t\tcase 1: data[0] = le32_to_cpup(data + 0); break;\n\t\t\tdefault:*(u64 *)(data + 0) = le64_to_cpup((u64 *)(data + 0));\n\t\t\t\tbreak;\n\t\t\tcase 4: tmp = le64_to_cpup((u64 *)(data + 0));\n\t\t\t\t*(u64 *)(data + 0) = le64_to_cpup((u64 *)(data + 2));\n\t\t\t\t*(u64 *)(data + 2) = tmp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!(current_thread_info()->fpsaved[0] & FPRS_FEF)) {\n\t\t\tcurrent_thread_info()->fpsaved[0] = FPRS_FEF;\n\t\t\tcurrent_thread_info()->gsr[0] = 0;\n\t\t}\n\t\tif (!(current_thread_info()->fpsaved[0] & flag)) {\n\t\t\tif (freg < 32)\n\t\t\t\tmemset(f->regs, 0, 32*sizeof(u32));\n\t\t\telse\n\t\t\t\tmemset(f->regs+32, 0, 32*sizeof(u32));\n\t\t}\n\t\tmemcpy(f->regs + freg, data, size * 4);\n\t\tcurrent_thread_info()->fpsaved[0] |= flag;\n\t}\n\tadvance(regs);\n\treturn 1;\n}\n\nvoid handle_ld_nf(u32 insn, struct pt_regs *regs)\n{\n\tint rd = ((insn >> 25) & 0x1f);\n\tint from_kernel = (regs->tstate & TSTATE_PRIV) != 0;\n\tunsigned long *reg;\n\t                        \n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);\n\n\tmaybe_flush_windows(0, 0, rd, from_kernel);\n\treg = fetch_reg_addr(rd, regs);\n\tif (from_kernel || rd < 16) {\n\t\treg[0] = 0;\n\t\tif ((insn & 0x780000) == 0x180000)\n\t\t\treg[1] = 0;\n\t} else if (test_thread_flag(TIF_32BIT)) {\n\t\tput_user(0, (int __user *) reg);\n\t\tif ((insn & 0x780000) == 0x180000)\n\t\t\tput_user(0, ((int __user *) reg) + 1);\n\t} else {\n\t\tput_user(0, (unsigned long __user *) reg);\n\t\tif ((insn & 0x780000) == 0x180000)\n\t\t\tput_user(0, (unsigned long __user *) reg + 1);\n\t}\n\tadvance(regs);\n}\n\nvoid handle_lddfmna(struct pt_regs *regs, unsigned long sfar, unsigned long sfsr)\n{\n\tunsigned long pc = regs->tpc;\n\tunsigned long tstate = regs->tstate;\n\tu32 insn;\n\tu64 value;\n\tu8 freg;\n\tint flag;\n\tstruct fpustate *f = FPUSTATE;\n\n\tif (tstate & TSTATE_PRIV)\n\t\tdie_if_kernel(\"lddfmna from kernel\", regs);\n\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, sfar);\n\tif (test_thread_flag(TIF_32BIT))\n\t\tpc = (u32)pc;\n\tif (get_user(insn, (u32 __user *) pc) != -EFAULT) {\n\t\tint asi = decode_asi(insn, regs);\n\t\tu32 first, second;\n\t\tint err;\n\n\t\tif ((asi > ASI_SNFL) ||\n\t\t    (asi < ASI_P))\n\t\t\tgoto daex;\n\t\tfirst = second = 0;\n\t\terr = get_user(first, (u32 __user *)sfar);\n\t\tif (!err)\n\t\t\terr = get_user(second, (u32 __user *)(sfar + 4));\n\t\tif (err) {\n\t\t\tif (!(asi & 0x2))\n\t\t\t\tgoto daex;\n\t\t\tfirst = second = 0;\n\t\t}\n\t\tsave_and_clear_fpu();\n\t\tfreg = ((insn >> 25) & 0x1e) | ((insn >> 20) & 0x20);\n\t\tvalue = (((u64)first) << 32) | second;\n\t\tif (asi & 0x8) /* Little */\n\t\t\tvalue = __swab64p(&value);\n\t\tflag = (freg < 32) ? FPRS_DL : FPRS_DU;\n\t\tif (!(current_thread_info()->fpsaved[0] & FPRS_FEF)) {\n\t\t\tcurrent_thread_info()->fpsaved[0] = FPRS_FEF;\n\t\t\tcurrent_thread_info()->gsr[0] = 0;\n\t\t}\n\t\tif (!(current_thread_info()->fpsaved[0] & flag)) {\n\t\t\tif (freg < 32)\n\t\t\t\tmemset(f->regs, 0, 32*sizeof(u32));\n\t\t\telse\n\t\t\t\tmemset(f->regs+32, 0, 32*sizeof(u32));\n\t\t}\n\t\t*(u64 *)(f->regs + freg) = value;\n\t\tcurrent_thread_info()->fpsaved[0] |= flag;\n\t} else {\ndaex:\n\t\tif (tlb_type == hypervisor)\n\t\t\tsun4v_data_access_exception(regs, sfar, sfsr);\n\t\telse\n\t\t\tspitfire_data_access_exception(regs, sfsr, sfar);\n\t\treturn;\n\t}\n\tadvance(regs);\n}\n\nvoid handle_stdfmna(struct pt_regs *regs, unsigned long sfar, unsigned long sfsr)\n{\n\tunsigned long pc = regs->tpc;\n\tunsigned long tstate = regs->tstate;\n\tu32 insn;\n\tu64 value;\n\tu8 freg;\n\tint flag;\n\tstruct fpustate *f = FPUSTATE;\n\n\tif (tstate & TSTATE_PRIV)\n\t\tdie_if_kernel(\"stdfmna from kernel\", regs);\n\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, sfar);\n\tif (test_thread_flag(TIF_32BIT))\n\t\tpc = (u32)pc;\n\tif (get_user(insn, (u32 __user *) pc) != -EFAULT) {\n\t\tint asi = decode_asi(insn, regs);\n\t\tfreg = ((insn >> 25) & 0x1e) | ((insn >> 20) & 0x20);\n\t\tvalue = 0;\n\t\tflag = (freg < 32) ? FPRS_DL : FPRS_DU;\n\t\tif ((asi > ASI_SNFL) ||\n\t\t    (asi < ASI_P))\n\t\t\tgoto daex;\n\t\tsave_and_clear_fpu();\n\t\tif (current_thread_info()->fpsaved[0] & flag)\n\t\t\tvalue = *(u64 *)&f->regs[freg];\n\t\tswitch (asi) {\n\t\tcase ASI_P:\n\t\tcase ASI_S: break;\n\t\tcase ASI_PL:\n\t\tcase ASI_SL: \n\t\t\tvalue = __swab64p(&value); break;\n\t\tdefault: goto daex;\n\t\t}\n\t\tif (put_user (value >> 32, (u32 __user *) sfar) ||\n\t\t    __put_user ((u32)value, (u32 __user *)(sfar + 4)))\n\t\t\tgoto daex;\n\t} else {\ndaex:\n\t\tif (tlb_type == hypervisor)\n\t\t\tsun4v_data_access_exception(regs, sfar, sfsr);\n\t\telse\n\t\t\tspitfire_data_access_exception(regs, sfsr, sfar);\n\t\treturn;\n\t}\n\tadvance(regs);\n}\n", "/* visemul.c: Emulation of VIS instructions.\n *\n * Copyright (C) 2006 David S. Miller (davem@davemloft.net)\n */\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/thread_info.h>\n#include <linux/perf_event.h>\n\n#include <asm/ptrace.h>\n#include <asm/pstate.h>\n#include <asm/system.h>\n#include <asm/fpumacro.h>\n#include <asm/uaccess.h>\n\n/* OPF field of various VIS instructions.  */\n\n/* 000111011 - four 16-bit packs  */\n#define FPACK16_OPF\t0x03b\n\n/* 000111010 - two 32-bit packs  */\n#define FPACK32_OPF\t0x03a\n\n/* 000111101 - four 16-bit packs  */\n#define FPACKFIX_OPF\t0x03d\n\n/* 001001101 - four 16-bit expands  */\n#define FEXPAND_OPF\t0x04d\n\n/* 001001011 - two 32-bit merges */\n#define FPMERGE_OPF\t0x04b\n\n/* 000110001 - 8-by-16-bit partitoned product  */\n#define FMUL8x16_OPF\t0x031\n\n/* 000110011 - 8-by-16-bit upper alpha partitioned product  */\n#define FMUL8x16AU_OPF\t0x033\n\n/* 000110101 - 8-by-16-bit lower alpha partitioned product  */\n#define FMUL8x16AL_OPF\t0x035\n\n/* 000110110 - upper 8-by-16-bit partitioned product  */\n#define FMUL8SUx16_OPF\t0x036\n\n/* 000110111 - lower 8-by-16-bit partitioned product  */\n#define FMUL8ULx16_OPF\t0x037\n\n/* 000111000 - upper 8-by-16-bit partitioned product  */\n#define FMULD8SUx16_OPF\t0x038\n\n/* 000111001 - lower unsigned 8-by-16-bit partitioned product  */\n#define FMULD8ULx16_OPF\t0x039\n\n/* 000101000 - four 16-bit compare; set rd if src1 > src2  */\n#define FCMPGT16_OPF\t0x028\n\n/* 000101100 - two 32-bit compare; set rd if src1 > src2  */\n#define FCMPGT32_OPF\t0x02c\n\n/* 000100000 - four 16-bit compare; set rd if src1 <= src2  */\n#define FCMPLE16_OPF\t0x020\n\n/* 000100100 - two 32-bit compare; set rd if src1 <= src2  */\n#define FCMPLE32_OPF\t0x024\n\n/* 000100010 - four 16-bit compare; set rd if src1 != src2  */\n#define FCMPNE16_OPF\t0x022\n\n/* 000100110 - two 32-bit compare; set rd if src1 != src2  */\n#define FCMPNE32_OPF\t0x026\n\n/* 000101010 - four 16-bit compare; set rd if src1 == src2  */\n#define FCMPEQ16_OPF\t0x02a\n\n/* 000101110 - two 32-bit compare; set rd if src1 == src2  */\n#define FCMPEQ32_OPF\t0x02e\n\n/* 000000000 - Eight 8-bit edge boundary processing  */\n#define EDGE8_OPF\t0x000\n\n/* 000000001 - Eight 8-bit edge boundary processing, no CC */\n#define EDGE8N_OPF\t0x001\n\n/* 000000010 - Eight 8-bit edge boundary processing, little-endian  */\n#define EDGE8L_OPF\t0x002\n\n/* 000000011 - Eight 8-bit edge boundary processing, little-endian, no CC  */\n#define EDGE8LN_OPF\t0x003\n\n/* 000000100 - Four 16-bit edge boundary processing  */\n#define EDGE16_OPF\t0x004\n\n/* 000000101 - Four 16-bit edge boundary processing, no CC  */\n#define EDGE16N_OPF\t0x005\n\n/* 000000110 - Four 16-bit edge boundary processing, little-endian  */\n#define EDGE16L_OPF\t0x006\n\n/* 000000111 - Four 16-bit edge boundary processing, little-endian, no CC  */\n#define EDGE16LN_OPF\t0x007\n\n/* 000001000 - Two 32-bit edge boundary processing  */\n#define EDGE32_OPF\t0x008\n\n/* 000001001 - Two 32-bit edge boundary processing, no CC  */\n#define EDGE32N_OPF\t0x009\n\n/* 000001010 - Two 32-bit edge boundary processing, little-endian  */\n#define EDGE32L_OPF\t0x00a\n\n/* 000001011 - Two 32-bit edge boundary processing, little-endian, no CC  */\n#define EDGE32LN_OPF\t0x00b\n\n/* 000111110 - distance between 8 8-bit components  */\n#define PDIST_OPF\t0x03e\n\n/* 000010000 - convert 8-bit 3-D address to blocked byte address  */\n#define ARRAY8_OPF\t0x010\n\n/* 000010010 - convert 16-bit 3-D address to blocked byte address  */\n#define ARRAY16_OPF\t0x012\n\n/* 000010100 - convert 32-bit 3-D address to blocked byte address  */\n#define ARRAY32_OPF\t0x014\n\n/* 000011001 - Set the GSR.MASK field in preparation for a BSHUFFLE  */\n#define BMASK_OPF\t0x019\n\n/* 001001100 - Permute bytes as specified by GSR.MASK  */\n#define BSHUFFLE_OPF\t0x04c\n\n#define VIS_OPF_SHIFT\t5\n#define VIS_OPF_MASK\t(0x1ff << VIS_OPF_SHIFT)\n\n#define RS1(INSN)\t(((INSN) >> 14) & 0x1f)\n#define RS2(INSN)\t(((INSN) >>  0) & 0x1f)\n#define RD(INSN)\t(((INSN) >> 25) & 0x1f)\n\nstatic inline void maybe_flush_windows(unsigned int rs1, unsigned int rs2,\n\t\t\t\t       unsigned int rd, int from_kernel)\n{\n\tif (rs2 >= 16 || rs1 >= 16 || rd >= 16) {\n\t\tif (from_kernel != 0)\n\t\t\t__asm__ __volatile__(\"flushw\");\n\t\telse\n\t\t\tflushw_user();\n\t}\n}\n\nstatic unsigned long fetch_reg(unsigned int reg, struct pt_regs *regs)\n{\n\tunsigned long value;\n\t\n\tif (reg < 16)\n\t\treturn (!reg ? 0 : regs->u_regs[reg]);\n\tif (regs->tstate & TSTATE_PRIV) {\n\t\tstruct reg_window *win;\n\t\twin = (struct reg_window *)(regs->u_regs[UREG_FP] + STACK_BIAS);\n\t\tvalue = win->locals[reg - 16];\n\t} else if (test_thread_flag(TIF_32BIT)) {\n\t\tstruct reg_window32 __user *win32;\n\t\twin32 = (struct reg_window32 __user *)((unsigned long)((u32)regs->u_regs[UREG_FP]));\n\t\tget_user(value, &win32->locals[reg - 16]);\n\t} else {\n\t\tstruct reg_window __user *win;\n\t\twin = (struct reg_window __user *)(regs->u_regs[UREG_FP] + STACK_BIAS);\n\t\tget_user(value, &win->locals[reg - 16]);\n\t}\n\treturn value;\n}\n\nstatic inline unsigned long __user *__fetch_reg_addr_user(unsigned int reg,\n\t\t\t\t\t\t\t  struct pt_regs *regs)\n{\n\tBUG_ON(reg < 16);\n\tBUG_ON(regs->tstate & TSTATE_PRIV);\n\n\tif (test_thread_flag(TIF_32BIT)) {\n\t\tstruct reg_window32 __user *win32;\n\t\twin32 = (struct reg_window32 __user *)((unsigned long)((u32)regs->u_regs[UREG_FP]));\n\t\treturn (unsigned long __user *)&win32->locals[reg - 16];\n\t} else {\n\t\tstruct reg_window __user *win;\n\t\twin = (struct reg_window __user *)(regs->u_regs[UREG_FP] + STACK_BIAS);\n\t\treturn &win->locals[reg - 16];\n\t}\n}\n\nstatic inline unsigned long *__fetch_reg_addr_kern(unsigned int reg,\n\t\t\t\t\t\t   struct pt_regs *regs)\n{\n\tBUG_ON(reg >= 16);\n\tBUG_ON(regs->tstate & TSTATE_PRIV);\n\n\treturn &regs->u_regs[reg];\n}\n\nstatic void store_reg(struct pt_regs *regs, unsigned long val, unsigned long rd)\n{\n\tif (rd < 16) {\n\t\tunsigned long *rd_kern = __fetch_reg_addr_kern(rd, regs);\n\n\t\t*rd_kern = val;\n\t} else {\n\t\tunsigned long __user *rd_user = __fetch_reg_addr_user(rd, regs);\n\n\t\tif (test_thread_flag(TIF_32BIT))\n\t\t\t__put_user((u32)val, (u32 __user *)rd_user);\n\t\telse\n\t\t\t__put_user(val, rd_user);\n\t}\n}\n\nstatic inline unsigned long fpd_regval(struct fpustate *f,\n\t\t\t\t       unsigned int insn_regnum)\n{\n\tinsn_regnum = (((insn_regnum & 1) << 5) |\n\t\t       (insn_regnum & 0x1e));\n\n\treturn *(unsigned long *) &f->regs[insn_regnum];\n}\n\nstatic inline unsigned long *fpd_regaddr(struct fpustate *f,\n\t\t\t\t\t unsigned int insn_regnum)\n{\n\tinsn_regnum = (((insn_regnum & 1) << 5) |\n\t\t       (insn_regnum & 0x1e));\n\n\treturn (unsigned long *) &f->regs[insn_regnum];\n}\n\nstatic inline unsigned int fps_regval(struct fpustate *f,\n\t\t\t\t      unsigned int insn_regnum)\n{\n\treturn f->regs[insn_regnum];\n}\n\nstatic inline unsigned int *fps_regaddr(struct fpustate *f,\n\t\t\t\t\tunsigned int insn_regnum)\n{\n\treturn &f->regs[insn_regnum];\n}\n\nstruct edge_tab {\n\tu16 left, right;\n};\nstatic struct edge_tab edge8_tab[8] = {\n\t{ 0xff, 0x80 },\n\t{ 0x7f, 0xc0 },\n\t{ 0x3f, 0xe0 },\n\t{ 0x1f, 0xf0 },\n\t{ 0x0f, 0xf8 },\n\t{ 0x07, 0xfc },\n\t{ 0x03, 0xfe },\n\t{ 0x01, 0xff },\n};\nstatic struct edge_tab edge8_tab_l[8] = {\n\t{ 0xff, 0x01 },\n\t{ 0xfe, 0x03 },\n\t{ 0xfc, 0x07 },\n\t{ 0xf8, 0x0f },\n\t{ 0xf0, 0x1f },\n\t{ 0xe0, 0x3f },\n\t{ 0xc0, 0x7f },\n\t{ 0x80, 0xff },\n};\nstatic struct edge_tab edge16_tab[4] = {\n\t{ 0xf, 0x8 },\n\t{ 0x7, 0xc },\n\t{ 0x3, 0xe },\n\t{ 0x1, 0xf },\n};\nstatic struct edge_tab edge16_tab_l[4] = {\n\t{ 0xf, 0x1 },\n\t{ 0xe, 0x3 },\n\t{ 0xc, 0x7 },\n\t{ 0x8, 0xf },\n};\nstatic struct edge_tab edge32_tab[2] = {\n\t{ 0x3, 0x2 },\n\t{ 0x1, 0x3 },\n};\nstatic struct edge_tab edge32_tab_l[2] = {\n\t{ 0x3, 0x1 },\n\t{ 0x2, 0x3 },\n};\n\nstatic void edge(struct pt_regs *regs, unsigned int insn, unsigned int opf)\n{\n\tunsigned long orig_rs1, rs1, orig_rs2, rs2, rd_val;\n\tu16 left, right;\n\n\tmaybe_flush_windows(RS1(insn), RS2(insn), RD(insn), 0);\n\torig_rs1 = rs1 = fetch_reg(RS1(insn), regs);\n\torig_rs2 = rs2 = fetch_reg(RS2(insn), regs);\n\n\tif (test_thread_flag(TIF_32BIT)) {\n\t\trs1 = rs1 & 0xffffffff;\n\t\trs2 = rs2 & 0xffffffff;\n\t}\n\tswitch (opf) {\n\tdefault:\n\tcase EDGE8_OPF:\n\tcase EDGE8N_OPF:\n\t\tleft = edge8_tab[rs1 & 0x7].left;\n\t\tright = edge8_tab[rs2 & 0x7].right;\n\t\tbreak;\n\tcase EDGE8L_OPF:\n\tcase EDGE8LN_OPF:\n\t\tleft = edge8_tab_l[rs1 & 0x7].left;\n\t\tright = edge8_tab_l[rs2 & 0x7].right;\n\t\tbreak;\n\n\tcase EDGE16_OPF:\n\tcase EDGE16N_OPF:\n\t\tleft = edge16_tab[(rs1 >> 1) & 0x3].left;\n\t\tright = edge16_tab[(rs2 >> 1) & 0x3].right;\n\t\tbreak;\n\n\tcase EDGE16L_OPF:\n\tcase EDGE16LN_OPF:\n\t\tleft = edge16_tab_l[(rs1 >> 1) & 0x3].left;\n\t\tright = edge16_tab_l[(rs2 >> 1) & 0x3].right;\n\t\tbreak;\n\n\tcase EDGE32_OPF:\n\tcase EDGE32N_OPF:\n\t\tleft = edge32_tab[(rs1 >> 2) & 0x1].left;\n\t\tright = edge32_tab[(rs2 >> 2) & 0x1].right;\n\t\tbreak;\n\n\tcase EDGE32L_OPF:\n\tcase EDGE32LN_OPF:\n\t\tleft = edge32_tab_l[(rs1 >> 2) & 0x1].left;\n\t\tright = edge32_tab_l[(rs2 >> 2) & 0x1].right;\n\t\tbreak;\n\t}\n\n\tif ((rs1 & ~0x7UL) == (rs2 & ~0x7UL))\n\t\trd_val = right & left;\n\telse\n\t\trd_val = left;\n\n\tstore_reg(regs, rd_val, RD(insn));\n\n\tswitch (opf) {\n\tcase EDGE8_OPF:\n\tcase EDGE8L_OPF:\n\tcase EDGE16_OPF:\n\tcase EDGE16L_OPF:\n\tcase EDGE32_OPF:\n\tcase EDGE32L_OPF: {\n\t\tunsigned long ccr, tstate;\n\n\t\t__asm__ __volatile__(\"subcc\t%1, %2, %%g0\\n\\t\"\n\t\t\t\t     \"rd\t%%ccr, %0\"\n\t\t\t\t     : \"=r\" (ccr)\n\t\t\t\t     : \"r\" (orig_rs1), \"r\" (orig_rs2)\n\t\t\t\t     : \"cc\");\n\t\ttstate = regs->tstate & ~(TSTATE_XCC | TSTATE_ICC);\n\t\tregs->tstate = tstate | (ccr << 32UL);\n\t}\n\t}\n}\n\nstatic void array(struct pt_regs *regs, unsigned int insn, unsigned int opf)\n{\n\tunsigned long rs1, rs2, rd_val;\n\tunsigned int bits, bits_mask;\n\n\tmaybe_flush_windows(RS1(insn), RS2(insn), RD(insn), 0);\n\trs1 = fetch_reg(RS1(insn), regs);\n\trs2 = fetch_reg(RS2(insn), regs);\n\n\tbits = (rs2 > 5 ? 5 : rs2);\n\tbits_mask = (1UL << bits) - 1UL;\n\n\trd_val = ((((rs1 >> 11) & 0x3) <<  0) |\n\t\t  (((rs1 >> 33) & 0x3) <<  2) |\n\t\t  (((rs1 >> 55) & 0x1) <<  4) |\n\t\t  (((rs1 >> 13) & 0xf) <<  5) |\n\t\t  (((rs1 >> 35) & 0xf) <<  9) |\n\t\t  (((rs1 >> 56) & 0xf) << 13) |\n\t\t  (((rs1 >> 17) & bits_mask) << 17) |\n\t\t  (((rs1 >> 39) & bits_mask) << (17 + bits)) |\n\t\t  (((rs1 >> 60) & 0xf)       << (17 + (2*bits))));\n\n\tswitch (opf) {\n\tcase ARRAY16_OPF:\n\t\trd_val <<= 1;\n\t\tbreak;\n\n\tcase ARRAY32_OPF:\n\t\trd_val <<= 2;\n\t}\n\n\tstore_reg(regs, rd_val, RD(insn));\n}\n\nstatic void bmask(struct pt_regs *regs, unsigned int insn)\n{\n\tunsigned long rs1, rs2, rd_val, gsr;\n\n\tmaybe_flush_windows(RS1(insn), RS2(insn), RD(insn), 0);\n\trs1 = fetch_reg(RS1(insn), regs);\n\trs2 = fetch_reg(RS2(insn), regs);\n\trd_val = rs1 + rs2;\n\n\tstore_reg(regs, rd_val, RD(insn));\n\n\tgsr = current_thread_info()->gsr[0] & 0xffffffff;\n\tgsr |= rd_val << 32UL;\n\tcurrent_thread_info()->gsr[0] = gsr;\n}\n\nstatic void bshuffle(struct pt_regs *regs, unsigned int insn)\n{\n\tstruct fpustate *f = FPUSTATE;\n\tunsigned long rs1, rs2, rd_val;\n\tunsigned long bmask, i;\n\n\tbmask = current_thread_info()->gsr[0] >> 32UL;\n\n\trs1 = fpd_regval(f, RS1(insn));\n\trs2 = fpd_regval(f, RS2(insn));\n\n\trd_val = 0UL;\n\tfor (i = 0; i < 8; i++) {\n\t\tunsigned long which = (bmask >> (i * 4)) & 0xf;\n\t\tunsigned long byte;\n\n\t\tif (which < 8)\n\t\t\tbyte = (rs1 >> (which * 8)) & 0xff;\n\t\telse\n\t\t\tbyte = (rs2 >> ((which-8)*8)) & 0xff;\n\t\trd_val |= (byte << (i * 8));\n\t}\n\n\t*fpd_regaddr(f, RD(insn)) = rd_val;\n}\n\nstatic void pdist(struct pt_regs *regs, unsigned int insn)\n{\n\tstruct fpustate *f = FPUSTATE;\n\tunsigned long rs1, rs2, *rd, rd_val;\n\tunsigned long i;\n\n\trs1 = fpd_regval(f, RS1(insn));\n\trs2 = fpd_regval(f, RS2(insn));\n\trd = fpd_regaddr(f, RD(insn));\n\n\trd_val = *rd;\n\n\tfor (i = 0; i < 8; i++) {\n\t\ts16 s1, s2;\n\n\t\ts1 = (rs1 >> (56 - (i * 8))) & 0xff;\n\t\ts2 = (rs2 >> (56 - (i * 8))) & 0xff;\n\n\t\t/* Absolute value of difference. */\n\t\ts1 -= s2;\n\t\tif (s1 < 0)\n\t\t\ts1 = ~s1 + 1;\n\n\t\trd_val += s1;\n\t}\n\n\t*rd = rd_val;\n}\n\nstatic void pformat(struct pt_regs *regs, unsigned int insn, unsigned int opf)\n{\n\tstruct fpustate *f = FPUSTATE;\n\tunsigned long rs1, rs2, gsr, scale, rd_val;\n\n\tgsr = current_thread_info()->gsr[0];\n\tscale = (gsr >> 3) & (opf == FPACK16_OPF ? 0xf : 0x1f);\n\tswitch (opf) {\n\tcase FPACK16_OPF: {\n\t\tunsigned long byte;\n\n\t\trs2 = fpd_regval(f, RS2(insn));\n\t\trd_val = 0;\n\t\tfor (byte = 0; byte < 4; byte++) {\n\t\t\tunsigned int val;\n\t\t\ts16 src = (rs2 >> (byte * 16UL)) & 0xffffUL;\n\t\t\tint scaled = src << scale;\n\t\t\tint from_fixed = scaled >> 7;\n\n\t\t\tval = ((from_fixed < 0) ?\n\t\t\t       0 :\n\t\t\t       (from_fixed > 255) ?\n\t\t\t       255 : from_fixed);\n\n\t\t\trd_val |= (val << (8 * byte));\n\t\t}\n\t\t*fps_regaddr(f, RD(insn)) = rd_val;\n\t\tbreak;\n\t}\n\n\tcase FPACK32_OPF: {\n\t\tunsigned long word;\n\n\t\trs1 = fpd_regval(f, RS1(insn));\n\t\trs2 = fpd_regval(f, RS2(insn));\n\t\trd_val = (rs1 << 8) & ~(0x000000ff000000ffUL);\n\t\tfor (word = 0; word < 2; word++) {\n\t\t\tunsigned long val;\n\t\t\ts32 src = (rs2 >> (word * 32UL));\n\t\t\ts64 scaled = src << scale;\n\t\t\ts64 from_fixed = scaled >> 23;\n\n\t\t\tval = ((from_fixed < 0) ?\n\t\t\t       0 :\n\t\t\t       (from_fixed > 255) ?\n\t\t\t       255 : from_fixed);\n\n\t\t\trd_val |= (val << (32 * word));\n\t\t}\n\t\t*fpd_regaddr(f, RD(insn)) = rd_val;\n\t\tbreak;\n\t}\n\n\tcase FPACKFIX_OPF: {\n\t\tunsigned long word;\n\n\t\trs2 = fpd_regval(f, RS2(insn));\n\n\t\trd_val = 0;\n\t\tfor (word = 0; word < 2; word++) {\n\t\t\tlong val;\n\t\t\ts32 src = (rs2 >> (word * 32UL));\n\t\t\ts64 scaled = src << scale;\n\t\t\ts64 from_fixed = scaled >> 16;\n\n\t\t\tval = ((from_fixed < -32768) ?\n\t\t\t       -32768 :\n\t\t\t       (from_fixed > 32767) ?\n\t\t\t       32767 : from_fixed);\n\n\t\t\trd_val |= ((val & 0xffff) << (word * 16));\n\t\t}\n\t\t*fps_regaddr(f, RD(insn)) = rd_val;\n\t\tbreak;\n\t}\n\n\tcase FEXPAND_OPF: {\n\t\tunsigned long byte;\n\n\t\trs2 = fps_regval(f, RS2(insn));\n\n\t\trd_val = 0;\n\t\tfor (byte = 0; byte < 4; byte++) {\n\t\t\tunsigned long val;\n\t\t\tu8 src = (rs2 >> (byte * 8)) & 0xff;\n\n\t\t\tval = src << 4;\n\n\t\t\trd_val |= (val << (byte * 16));\n\t\t}\n\t\t*fpd_regaddr(f, RD(insn)) = rd_val;\n\t\tbreak;\n\t}\n\n\tcase FPMERGE_OPF: {\n\t\trs1 = fps_regval(f, RS1(insn));\n\t\trs2 = fps_regval(f, RS2(insn));\n\n\t\trd_val = (((rs2 & 0x000000ff) <<  0) |\n\t\t\t  ((rs1 & 0x000000ff) <<  8) |\n\t\t\t  ((rs2 & 0x0000ff00) <<  8) |\n\t\t\t  ((rs1 & 0x0000ff00) << 16) |\n\t\t\t  ((rs2 & 0x00ff0000) << 16) |\n\t\t\t  ((rs1 & 0x00ff0000) << 24) |\n\t\t\t  ((rs2 & 0xff000000) << 24) |\n\t\t\t  ((rs1 & 0xff000000) << 32));\n\t\t*fpd_regaddr(f, RD(insn)) = rd_val;\n\t\tbreak;\n\t}\n\t}\n}\n\nstatic void pmul(struct pt_regs *regs, unsigned int insn, unsigned int opf)\n{\n\tstruct fpustate *f = FPUSTATE;\n\tunsigned long rs1, rs2, rd_val;\n\n\tswitch (opf) {\n\tcase FMUL8x16_OPF: {\n\t\tunsigned long byte;\n\n\t\trs1 = fps_regval(f, RS1(insn));\n\t\trs2 = fpd_regval(f, RS2(insn));\n\n\t\trd_val = 0;\n\t\tfor (byte = 0; byte < 4; byte++) {\n\t\t\tu16 src1 = (rs1 >> (byte *  8)) & 0x00ff;\n\t\t\ts16 src2 = (rs2 >> (byte * 16)) & 0xffff;\n\t\t\tu32 prod = src1 * src2;\n\t\t\tu16 scaled = ((prod & 0x00ffff00) >> 8);\n\n\t\t\t/* Round up.  */\n\t\t\tif (prod & 0x80)\n\t\t\t\tscaled++;\n\t\t\trd_val |= ((scaled & 0xffffUL) << (byte * 16UL));\n\t\t}\n\n\t\t*fpd_regaddr(f, RD(insn)) = rd_val;\n\t\tbreak;\n\t}\n\n\tcase FMUL8x16AU_OPF:\n\tcase FMUL8x16AL_OPF: {\n\t\tunsigned long byte;\n\t\ts16 src2;\n\n\t\trs1 = fps_regval(f, RS1(insn));\n\t\trs2 = fps_regval(f, RS2(insn));\n\n\t\trd_val = 0;\n\t\tsrc2 = rs2 >> (opf == FMUL8x16AU_OPF ? 16 : 0);\n\t\tfor (byte = 0; byte < 4; byte++) {\n\t\t\tu16 src1 = (rs1 >> (byte * 8)) & 0x00ff;\n\t\t\tu32 prod = src1 * src2;\n\t\t\tu16 scaled = ((prod & 0x00ffff00) >> 8);\n\n\t\t\t/* Round up.  */\n\t\t\tif (prod & 0x80)\n\t\t\t\tscaled++;\n\t\t\trd_val |= ((scaled & 0xffffUL) << (byte * 16UL));\n\t\t}\n\n\t\t*fpd_regaddr(f, RD(insn)) = rd_val;\n\t\tbreak;\n\t}\n\n\tcase FMUL8SUx16_OPF:\n\tcase FMUL8ULx16_OPF: {\n\t\tunsigned long byte, ushift;\n\n\t\trs1 = fpd_regval(f, RS1(insn));\n\t\trs2 = fpd_regval(f, RS2(insn));\n\n\t\trd_val = 0;\n\t\tushift = (opf == FMUL8SUx16_OPF) ? 8 : 0;\n\t\tfor (byte = 0; byte < 4; byte++) {\n\t\t\tu16 src1;\n\t\t\ts16 src2;\n\t\t\tu32 prod;\n\t\t\tu16 scaled;\n\n\t\t\tsrc1 = ((rs1 >> ((16 * byte) + ushift)) & 0x00ff);\n\t\t\tsrc2 = ((rs2 >> (16 * byte)) & 0xffff);\n\t\t\tprod = src1 * src2;\n\t\t\tscaled = ((prod & 0x00ffff00) >> 8);\n\n\t\t\t/* Round up.  */\n\t\t\tif (prod & 0x80)\n\t\t\t\tscaled++;\n\t\t\trd_val |= ((scaled & 0xffffUL) << (byte * 16UL));\n\t\t}\n\n\t\t*fpd_regaddr(f, RD(insn)) = rd_val;\n\t\tbreak;\n\t}\n\n\tcase FMULD8SUx16_OPF:\n\tcase FMULD8ULx16_OPF: {\n\t\tunsigned long byte, ushift;\n\n\t\trs1 = fps_regval(f, RS1(insn));\n\t\trs2 = fps_regval(f, RS2(insn));\n\n\t\trd_val = 0;\n\t\tushift = (opf == FMULD8SUx16_OPF) ? 8 : 0;\n\t\tfor (byte = 0; byte < 2; byte++) {\n\t\t\tu16 src1;\n\t\t\ts16 src2;\n\t\t\tu32 prod;\n\t\t\tu16 scaled;\n\n\t\t\tsrc1 = ((rs1 >> ((16 * byte) + ushift)) & 0x00ff);\n\t\t\tsrc2 = ((rs2 >> (16 * byte)) & 0xffff);\n\t\t\tprod = src1 * src2;\n\t\t\tscaled = ((prod & 0x00ffff00) >> 8);\n\n\t\t\t/* Round up.  */\n\t\t\tif (prod & 0x80)\n\t\t\t\tscaled++;\n\t\t\trd_val |= ((scaled & 0xffffUL) <<\n\t\t\t\t   ((byte * 32UL) + 7UL));\n\t\t}\n\t\t*fpd_regaddr(f, RD(insn)) = rd_val;\n\t\tbreak;\n\t}\n\t}\n}\n\nstatic void pcmp(struct pt_regs *regs, unsigned int insn, unsigned int opf)\n{\n\tstruct fpustate *f = FPUSTATE;\n\tunsigned long rs1, rs2, rd_val, i;\n\n\trs1 = fpd_regval(f, RS1(insn));\n\trs2 = fpd_regval(f, RS2(insn));\n\n\trd_val = 0;\n\n\tswitch (opf) {\n\tcase FCMPGT16_OPF:\n\t\tfor (i = 0; i < 4; i++) {\n\t\t\ts16 a = (rs1 >> (i * 16)) & 0xffff;\n\t\t\ts16 b = (rs2 >> (i * 16)) & 0xffff;\n\n\t\t\tif (a > b)\n\t\t\t\trd_val |= 1 << i;\n\t\t}\n\t\tbreak;\n\n\tcase FCMPGT32_OPF:\n\t\tfor (i = 0; i < 2; i++) {\n\t\t\ts32 a = (rs1 >> (i * 32)) & 0xffff;\n\t\t\ts32 b = (rs2 >> (i * 32)) & 0xffff;\n\n\t\t\tif (a > b)\n\t\t\t\trd_val |= 1 << i;\n\t\t}\n\t\tbreak;\n\n\tcase FCMPLE16_OPF:\n\t\tfor (i = 0; i < 4; i++) {\n\t\t\ts16 a = (rs1 >> (i * 16)) & 0xffff;\n\t\t\ts16 b = (rs2 >> (i * 16)) & 0xffff;\n\n\t\t\tif (a <= b)\n\t\t\t\trd_val |= 1 << i;\n\t\t}\n\t\tbreak;\n\n\tcase FCMPLE32_OPF:\n\t\tfor (i = 0; i < 2; i++) {\n\t\t\ts32 a = (rs1 >> (i * 32)) & 0xffff;\n\t\t\ts32 b = (rs2 >> (i * 32)) & 0xffff;\n\n\t\t\tif (a <= b)\n\t\t\t\trd_val |= 1 << i;\n\t\t}\n\t\tbreak;\n\n\tcase FCMPNE16_OPF:\n\t\tfor (i = 0; i < 4; i++) {\n\t\t\ts16 a = (rs1 >> (i * 16)) & 0xffff;\n\t\t\ts16 b = (rs2 >> (i * 16)) & 0xffff;\n\n\t\t\tif (a != b)\n\t\t\t\trd_val |= 1 << i;\n\t\t}\n\t\tbreak;\n\n\tcase FCMPNE32_OPF:\n\t\tfor (i = 0; i < 2; i++) {\n\t\t\ts32 a = (rs1 >> (i * 32)) & 0xffff;\n\t\t\ts32 b = (rs2 >> (i * 32)) & 0xffff;\n\n\t\t\tif (a != b)\n\t\t\t\trd_val |= 1 << i;\n\t\t}\n\t\tbreak;\n\n\tcase FCMPEQ16_OPF:\n\t\tfor (i = 0; i < 4; i++) {\n\t\t\ts16 a = (rs1 >> (i * 16)) & 0xffff;\n\t\t\ts16 b = (rs2 >> (i * 16)) & 0xffff;\n\n\t\t\tif (a == b)\n\t\t\t\trd_val |= 1 << i;\n\t\t}\n\t\tbreak;\n\n\tcase FCMPEQ32_OPF:\n\t\tfor (i = 0; i < 2; i++) {\n\t\t\ts32 a = (rs1 >> (i * 32)) & 0xffff;\n\t\t\ts32 b = (rs2 >> (i * 32)) & 0xffff;\n\n\t\t\tif (a == b)\n\t\t\t\trd_val |= 1 << i;\n\t\t}\n\t\tbreak;\n\t}\n\n\tmaybe_flush_windows(0, 0, RD(insn), 0);\n\tstore_reg(regs, rd_val, RD(insn));\n}\n\n/* Emulate the VIS instructions which are not implemented in\n * hardware on Niagara.\n */\nint vis_emul(struct pt_regs *regs, unsigned int insn)\n{\n\tunsigned long pc = regs->tpc;\n\tunsigned int opf;\n\n\tBUG_ON(regs->tstate & TSTATE_PRIV);\n\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);\n\n\tif (test_thread_flag(TIF_32BIT))\n\t\tpc = (u32)pc;\n\n\tif (get_user(insn, (u32 __user *) pc))\n\t\treturn -EFAULT;\n\n\tsave_and_clear_fpu();\n\n\topf = (insn & VIS_OPF_MASK) >> VIS_OPF_SHIFT;\n\tswitch (opf) {\n\tdefault:\n\t\treturn -EINVAL;\n\n\t/* Pixel Formatting Instructions.  */\n\tcase FPACK16_OPF:\n\tcase FPACK32_OPF:\n\tcase FPACKFIX_OPF:\n\tcase FEXPAND_OPF:\n\tcase FPMERGE_OPF:\n\t\tpformat(regs, insn, opf);\n\t\tbreak;\n\n\t/* Partitioned Multiply Instructions  */\n\tcase FMUL8x16_OPF:\n\tcase FMUL8x16AU_OPF:\n\tcase FMUL8x16AL_OPF:\n\tcase FMUL8SUx16_OPF:\n\tcase FMUL8ULx16_OPF:\n\tcase FMULD8SUx16_OPF:\n\tcase FMULD8ULx16_OPF:\n\t\tpmul(regs, insn, opf);\n\t\tbreak;\n\n\t/* Pixel Compare Instructions  */\n\tcase FCMPGT16_OPF:\n\tcase FCMPGT32_OPF:\n\tcase FCMPLE16_OPF:\n\tcase FCMPLE32_OPF:\n\tcase FCMPNE16_OPF:\n\tcase FCMPNE32_OPF:\n\tcase FCMPEQ16_OPF:\n\tcase FCMPEQ32_OPF:\n\t\tpcmp(regs, insn, opf);\n\t\tbreak;\n\n\t/* Edge Handling Instructions  */\n\tcase EDGE8_OPF:\n\tcase EDGE8N_OPF:\n\tcase EDGE8L_OPF:\n\tcase EDGE8LN_OPF:\n\tcase EDGE16_OPF:\n\tcase EDGE16N_OPF:\n\tcase EDGE16L_OPF:\n\tcase EDGE16LN_OPF:\n\tcase EDGE32_OPF:\n\tcase EDGE32N_OPF:\n\tcase EDGE32L_OPF:\n\tcase EDGE32LN_OPF:\n\t\tedge(regs, insn, opf);\n\t\tbreak;\n\n\t/* Pixel Component Distance  */\n\tcase PDIST_OPF:\n\t\tpdist(regs, insn);\n\t\tbreak;\n\n\t/* Three-Dimensional Array Addressing Instructions  */\n\tcase ARRAY8_OPF:\n\tcase ARRAY16_OPF:\n\tcase ARRAY32_OPF:\n\t\tarray(regs, insn, opf);\n\t\tbreak;\n\n\t/* Byte Mask and Shuffle Instructions  */\n\tcase BMASK_OPF:\n\t\tbmask(regs, insn);\n\t\tbreak;\n\n\tcase BSHUFFLE_OPF:\n\t\tbshuffle(regs, insn);\n\t\tbreak;\n\t}\n\n\tregs->tpc = regs->tnpc;\n\tregs->tnpc += 4;\n\treturn 0;\n}\n", "/*\n * arch/sparc/math-emu/math.c\n *\n * Copyright (C) 1998 Peter Maydell (pmaydell@chiark.greenend.org.uk)\n * Copyright (C) 1997, 1999 Jakub Jelinek (jj@ultra.linux.cz)\n * Copyright (C) 1999 David S. Miller (davem@redhat.com)\n *\n * This is a good place to start if you're trying to understand the\n * emulation code, because it's pretty simple. What we do is\n * essentially analyse the instruction to work out what the operation\n * is and which registers are involved. We then execute the appropriate\n * FXXXX function. [The floating point queue introduces a minor wrinkle;\n * see below...]\n * The fxxxxx.c files each emulate a single insn. They look relatively\n * simple because the complexity is hidden away in an unholy tangle\n * of preprocessor macros.\n *\n * The first layer of macros is single.h, double.h, quad.h. Generally\n * these files define macros for working with floating point numbers\n * of the three IEEE formats. FP_ADD_D(R,A,B) is for adding doubles,\n * for instance. These macros are usually defined as calls to more\n * generic macros (in this case _FP_ADD(D,2,R,X,Y) where the number\n * of machine words required to store the given IEEE format is passed\n * as a parameter. [double.h and co check the number of bits in a word\n * and define FP_ADD_D & co appropriately].\n * The generic macros are defined in op-common.h. This is where all\n * the grotty stuff like handling NaNs is coded. To handle the possible\n * word sizes macros in op-common.h use macros like _FP_FRAC_SLL_##wc()\n * where wc is the 'number of machine words' parameter (here 2).\n * These are defined in the third layer of macros: op-1.h, op-2.h\n * and op-4.h. These handle operations on floating point numbers composed\n * of 1,2 and 4 machine words respectively. [For example, on sparc64\n * doubles are one machine word so macros in double.h eventually use\n * constructs in op-1.h, but on sparc32 they use op-2.h definitions.]\n * soft-fp.h is on the same level as op-common.h, and defines some\n * macros which are independent of both word size and FP format.\n * Finally, sfp-machine.h is the machine dependent part of the\n * code: it defines the word size and what type a word is. It also\n * defines how _FP_MUL_MEAT_t() maps to _FP_MUL_MEAT_n_* : op-n.h\n * provide several possible flavours of multiply algorithm, most\n * of which require that you supply some form of asm or C primitive to\n * do the actual multiply. (such asm primitives should be defined\n * in sfp-machine.h too). udivmodti4.c is the same sort of thing.\n *\n * There may be some errors here because I'm working from a\n * SPARC architecture manual V9, and what I really want is V8...\n * Also, the insns which can generate exceptions seem to be a\n * greater subset of the FPops than for V9 (for example, FCMPED\n * has to be emulated on V8). So I think I'm going to have\n * to emulate them all just to be on the safe side...\n *\n * Emulation routines originate from soft-fp package, which is\n * part of glibc and has appropriate copyrights in it (allegedly).\n *\n * NB: on sparc int == long == 4 bytes, long long == 8 bytes.\n * Most bits of the kernel seem to go for long rather than int,\n * so we follow that practice...\n */\n\n/* TODO:\n * fpsave() saves the FP queue but fpload() doesn't reload it.\n * Therefore when we context switch or change FPU ownership\n * we have to check to see if the queue had anything in it and\n * emulate it if it did. This is going to be a pain.\n */\n\n#include <linux/types.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/perf_event.h>\n#include <asm/uaccess.h>\n\n#include \"sfp-util_32.h\"\n#include <math-emu/soft-fp.h>\n#include <math-emu/single.h>\n#include <math-emu/double.h>\n#include <math-emu/quad.h>\n\n#define FLOATFUNC(x) extern int x(void *,void *,void *)\n\n/* The Vn labels indicate what version of the SPARC architecture gas thinks\n * each insn is. This is from the binutils source :->\n */\n/* quadword instructions */\n#define FSQRTQ\t0x02b\t\t/* v8 */\n#define FADDQ\t0x043\t\t/* v8 */\n#define FSUBQ\t0x047\t\t/* v8 */\n#define FMULQ\t0x04b\t\t/* v8 */\n#define FDIVQ\t0x04f\t\t/* v8 */\n#define FDMULQ\t0x06e\t\t/* v8 */\n#define FQTOS\t0x0c7\t\t/* v8 */\n#define FQTOD\t0x0cb\t\t/* v8 */\n#define FITOQ\t0x0cc\t\t/* v8 */\n#define FSTOQ\t0x0cd\t\t/* v8 */\n#define FDTOQ\t0x0ce\t\t/* v8 */\n#define FQTOI\t0x0d3\t\t/* v8 */\n#define FCMPQ\t0x053\t\t/* v8 */\n#define FCMPEQ\t0x057\t\t/* v8 */\n/* single/double instructions (subnormal): should all work */\n#define FSQRTS\t0x029\t\t/* v7 */\n#define FSQRTD\t0x02a\t\t/* v7 */\n#define FADDS\t0x041\t\t/* v6 */\n#define FADDD\t0x042\t\t/* v6 */\n#define FSUBS\t0x045\t\t/* v6 */\n#define FSUBD\t0x046\t\t/* v6 */\n#define FMULS\t0x049\t\t/* v6 */\n#define FMULD\t0x04a\t\t/* v6 */\n#define FDIVS\t0x04d\t\t/* v6 */\n#define FDIVD\t0x04e\t\t/* v6 */\n#define FSMULD\t0x069\t\t/* v6 */\n#define FDTOS\t0x0c6\t\t/* v6 */\n#define FSTOD\t0x0c9\t\t/* v6 */\n#define FSTOI\t0x0d1\t\t/* v6 */\n#define FDTOI\t0x0d2\t\t/* v6 */\n#define FABSS\t0x009\t\t/* v6 */\n#define FCMPS\t0x051\t\t/* v6 */\n#define FCMPES\t0x055\t\t/* v6 */\n#define FCMPD\t0x052\t\t/* v6 */\n#define FCMPED\t0x056\t\t/* v6 */\n#define FMOVS\t0x001\t\t/* v6 */\n#define FNEGS\t0x005\t\t/* v6 */\n#define FITOS\t0x0c4\t\t/* v6 */\n#define FITOD\t0x0c8\t\t/* v6 */\n\n#define FSR_TEM_SHIFT\t23UL\n#define FSR_TEM_MASK\t(0x1fUL << FSR_TEM_SHIFT)\n#define FSR_AEXC_SHIFT\t5UL\n#define FSR_AEXC_MASK\t(0x1fUL << FSR_AEXC_SHIFT)\n#define FSR_CEXC_SHIFT\t0UL\n#define FSR_CEXC_MASK\t(0x1fUL << FSR_CEXC_SHIFT)\n\nstatic int do_one_mathemu(u32 insn, unsigned long *fsr, unsigned long *fregs);\n\n/* Unlike the Sparc64 version (which has a struct fpustate), we\n * pass the taskstruct corresponding to the task which currently owns the\n * FPU. This is partly because we don't have the fpustate struct and\n * partly because the task owning the FPU isn't always current (as is\n * the case for the Sparc64 port). This is probably SMP-related...\n * This function returns 1 if all queued insns were emulated successfully.\n * The test for unimplemented FPop in kernel mode has been moved into\n * kernel/traps.c for simplicity.\n */\nint do_mathemu(struct pt_regs *regs, struct task_struct *fpt)\n{\n\t/* regs->pc isn't necessarily the PC at which the offending insn is sitting.\n\t * The FPU maintains a queue of FPops which cause traps.\n\t * When it hits an instruction that requires that the trapped op succeeded\n\t * (usually because it reads a reg. that the trapped op wrote) then it\n\t * causes this exception. We need to emulate all the insns on the queue\n\t * and then allow the op to proceed.\n\t * This code should also handle the case where the trap was precise,\n\t * in which case the queue length is zero and regs->pc points at the\n\t * single FPop to be emulated. (this case is untested, though :->)\n\t * You'll need this case if you want to be able to emulate all FPops\n\t * because the FPU either doesn't exist or has been software-disabled.\n\t * [The UltraSPARC makes FP a precise trap; this isn't as stupid as it\n\t * might sound because the Ultra does funky things with a superscalar\n\t * architecture.]\n\t */\n\n\t/* You wouldn't believe how often I typed 'ftp' when I meant 'fpt' :-> */\n\n\tint i;\n\tint retcode = 0;                               /* assume all succeed */\n\tunsigned long insn;\n\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);\n\n#ifdef DEBUG_MATHEMU\n\tprintk(\"In do_mathemu()... pc is %08lx\\n\", regs->pc);\n\tprintk(\"fpqdepth is %ld\\n\", fpt->thread.fpqdepth);\n\tfor (i = 0; i < fpt->thread.fpqdepth; i++)\n\t\tprintk(\"%d: %08lx at %08lx\\n\", i, fpt->thread.fpqueue[i].insn,\n\t\t       (unsigned long)fpt->thread.fpqueue[i].insn_addr);\n#endif\n\n\tif (fpt->thread.fpqdepth == 0) {                   /* no queue, guilty insn is at regs->pc */\n#ifdef DEBUG_MATHEMU\n\t\tprintk(\"precise trap at %08lx\\n\", regs->pc);\n#endif\n\t\tif (!get_user(insn, (u32 __user *) regs->pc)) {\n\t\t\tretcode = do_one_mathemu(insn, &fpt->thread.fsr, fpt->thread.float_regs);\n\t\t\tif (retcode) {\n\t\t\t\t/* in this case we need to fix up PC & nPC */\n\t\t\t\tregs->pc = regs->npc;\n\t\t\t\tregs->npc += 4;\n\t\t\t}\n\t\t}\n\t\treturn retcode;\n\t}\n\n\t/* Normal case: need to empty the queue... */\n\tfor (i = 0; i < fpt->thread.fpqdepth; i++) {\n\t\tretcode = do_one_mathemu(fpt->thread.fpqueue[i].insn, &(fpt->thread.fsr), fpt->thread.float_regs);\n\t\tif (!retcode)                               /* insn failed, no point doing any more */\n\t\t\tbreak;\n\t}\n\t/* Now empty the queue and clear the queue_not_empty flag */\n\tif (retcode)\n\t\tfpt->thread.fsr &= ~(0x3000 | FSR_CEXC_MASK);\n\telse\n\t\tfpt->thread.fsr &= ~0x3000;\n\tfpt->thread.fpqdepth = 0;\n\n\treturn retcode;\n}\n\n/* All routines returning an exception to raise should detect\n * such exceptions _before_ rounding to be consistent with\n * the behavior of the hardware in the implemented cases\n * (and thus with the recommendations in the V9 architecture\n * manual).\n *\n * We return 0 if a SIGFPE should be sent, 1 otherwise.\n */\nstatic inline int record_exception(unsigned long *pfsr, int eflag)\n{\n\tunsigned long fsr = *pfsr;\n\tint would_trap;\n\n\t/* Determine if this exception would have generated a trap. */\n\twould_trap = (fsr & ((long)eflag << FSR_TEM_SHIFT)) != 0UL;\n\n\t/* If trapping, we only want to signal one bit. */\n\tif (would_trap != 0) {\n\t\teflag &= ((fsr & FSR_TEM_MASK) >> FSR_TEM_SHIFT);\n\t\tif ((eflag & (eflag - 1)) != 0) {\n\t\t\tif (eflag & FP_EX_INVALID)\n\t\t\t\teflag = FP_EX_INVALID;\n\t\t\telse if (eflag & FP_EX_OVERFLOW)\n\t\t\t\teflag = FP_EX_OVERFLOW;\n\t\t\telse if (eflag & FP_EX_UNDERFLOW)\n\t\t\t\teflag = FP_EX_UNDERFLOW;\n\t\t\telse if (eflag & FP_EX_DIVZERO)\n\t\t\t\teflag = FP_EX_DIVZERO;\n\t\t\telse if (eflag & FP_EX_INEXACT)\n\t\t\t\teflag = FP_EX_INEXACT;\n\t\t}\n\t}\n\n\t/* Set CEXC, here is the rule:\n\t *\n\t *    In general all FPU ops will set one and only one\n\t *    bit in the CEXC field, this is always the case\n\t *    when the IEEE exception trap is enabled in TEM.\n\t */\n\tfsr &= ~(FSR_CEXC_MASK);\n\tfsr |= ((long)eflag << FSR_CEXC_SHIFT);\n\n\t/* Set the AEXC field, rule is:\n\t *\n\t *    If a trap would not be generated, the\n\t *    CEXC just generated is OR'd into the\n\t *    existing value of AEXC.\n\t */\n\tif (would_trap == 0)\n\t\tfsr |= ((long)eflag << FSR_AEXC_SHIFT);\n\n\t/* If trapping, indicate fault trap type IEEE. */\n\tif (would_trap != 0)\n\t\tfsr |= (1UL << 14);\n\n\t*pfsr = fsr;\n\n\treturn (would_trap ? 0 : 1);\n}\n\ntypedef union {\n\tu32 s;\n\tu64 d;\n\tu64 q[2];\n} *argp;\n\nstatic int do_one_mathemu(u32 insn, unsigned long *pfsr, unsigned long *fregs)\n{\n\t/* Emulate the given insn, updating fsr and fregs appropriately. */\n\tint type = 0;\n\t/* r is rd, b is rs2 and a is rs1. The *u arg tells\n\t   whether the argument should be packed/unpacked (0 - do not unpack/pack, 1 - unpack/pack)\n\t   non-u args tells the size of the argument (0 - no argument, 1 - single, 2 - double, 3 - quad */\n#define TYPE(dummy, r, ru, b, bu, a, au) type = (au << 2) | (a << 0) | (bu << 5) | (b << 3) | (ru << 8) | (r << 6)\n\tint freg;\n\targp rs1 = NULL, rs2 = NULL, rd = NULL;\n\tFP_DECL_EX;\n\tFP_DECL_S(SA); FP_DECL_S(SB); FP_DECL_S(SR);\n\tFP_DECL_D(DA); FP_DECL_D(DB); FP_DECL_D(DR);\n\tFP_DECL_Q(QA); FP_DECL_Q(QB); FP_DECL_Q(QR);\n\tint IR;\n\tlong fsr;\n\n#ifdef DEBUG_MATHEMU\n\tprintk(\"In do_mathemu(), emulating %08lx\\n\", insn);\n#endif\n\n\tif ((insn & 0xc1f80000) == 0x81a00000)\t/* FPOP1 */ {\n\t\tswitch ((insn >> 5) & 0x1ff) {\n\t\tcase FSQRTQ: TYPE(3,3,1,3,1,0,0); break;\n\t\tcase FADDQ:\n\t\tcase FSUBQ:\n\t\tcase FMULQ:\n\t\tcase FDIVQ: TYPE(3,3,1,3,1,3,1); break;\n\t\tcase FDMULQ: TYPE(3,3,1,2,1,2,1); break;\n\t\tcase FQTOS: TYPE(3,1,1,3,1,0,0); break;\n\t\tcase FQTOD: TYPE(3,2,1,3,1,0,0); break;\n\t\tcase FITOQ: TYPE(3,3,1,1,0,0,0); break;\n\t\tcase FSTOQ: TYPE(3,3,1,1,1,0,0); break;\n\t\tcase FDTOQ: TYPE(3,3,1,2,1,0,0); break;\n\t\tcase FQTOI: TYPE(3,1,0,3,1,0,0); break;\n\t\tcase FSQRTS: TYPE(2,1,1,1,1,0,0); break;\n\t\tcase FSQRTD: TYPE(2,2,1,2,1,0,0); break;\n\t\tcase FADDD:\n\t\tcase FSUBD:\n\t\tcase FMULD:\n\t\tcase FDIVD: TYPE(2,2,1,2,1,2,1); break;\n\t\tcase FADDS:\n\t\tcase FSUBS:\n\t\tcase FMULS:\n\t\tcase FDIVS: TYPE(2,1,1,1,1,1,1); break;\n\t\tcase FSMULD: TYPE(2,2,1,1,1,1,1); break;\n\t\tcase FDTOS: TYPE(2,1,1,2,1,0,0); break;\n\t\tcase FSTOD: TYPE(2,2,1,1,1,0,0); break;\n\t\tcase FSTOI: TYPE(2,1,0,1,1,0,0); break;\n\t\tcase FDTOI: TYPE(2,1,0,2,1,0,0); break;\n\t\tcase FITOS: TYPE(2,1,1,1,0,0,0); break;\n\t\tcase FITOD: TYPE(2,2,1,1,0,0,0); break;\n\t\tcase FMOVS:\n\t\tcase FABSS:\n\t\tcase FNEGS: TYPE(2,1,0,1,0,0,0); break;\n\t\t}\n\t} else if ((insn & 0xc1f80000) == 0x81a80000)\t/* FPOP2 */ {\n\t\tswitch ((insn >> 5) & 0x1ff) {\n\t\tcase FCMPS: TYPE(3,0,0,1,1,1,1); break;\n\t\tcase FCMPES: TYPE(3,0,0,1,1,1,1); break;\n\t\tcase FCMPD: TYPE(3,0,0,2,1,2,1); break;\n\t\tcase FCMPED: TYPE(3,0,0,2,1,2,1); break;\n\t\tcase FCMPQ: TYPE(3,0,0,3,1,3,1); break;\n\t\tcase FCMPEQ: TYPE(3,0,0,3,1,3,1); break;\n\t\t}\n\t}\n\n\tif (!type) {\t/* oops, didn't recognise that FPop */\n#ifdef DEBUG_MATHEMU\n\t\tprintk(\"attempt to emulate unrecognised FPop!\\n\");\n#endif\n\t\treturn 0;\n\t}\n\n\t/* Decode the registers to be used */\n\tfreg = (*pfsr >> 14) & 0xf;\n\n\t*pfsr &= ~0x1c000;\t\t\t\t/* clear the traptype bits */\n\t\n\tfreg = ((insn >> 14) & 0x1f);\n\tswitch (type & 0x3) {\t\t\t\t/* is rs1 single, double or quad? */\n\tcase 3:\n\t\tif (freg & 3) {\t\t\t\t/* quadwords must have bits 4&5 of the */\n\t\t\t\t\t\t\t/* encoded reg. number set to zero. */\n\t\t\t*pfsr |= (6 << 14);\n\t\t\treturn 0;\t\t\t/* simulate invalid_fp_register exception */\n\t\t}\n\t/* fall through */\n\tcase 2:\n\t\tif (freg & 1) {\t\t\t\t/* doublewords must have bit 5 zeroed */\n\t\t\t*pfsr |= (6 << 14);\n\t\t\treturn 0;\n\t\t}\n\t}\n\trs1 = (argp)&fregs[freg];\n\tswitch (type & 0x7) {\n\tcase 7: FP_UNPACK_QP (QA, rs1); break;\n\tcase 6: FP_UNPACK_DP (DA, rs1); break;\n\tcase 5: FP_UNPACK_SP (SA, rs1); break;\n\t}\n\tfreg = (insn & 0x1f);\n\tswitch ((type >> 3) & 0x3) {\t\t\t/* same again for rs2 */\n\tcase 3:\n\t\tif (freg & 3) {\t\t\t\t/* quadwords must have bits 4&5 of the */\n\t\t\t\t\t\t\t/* encoded reg. number set to zero. */\n\t\t\t*pfsr |= (6 << 14);\n\t\t\treturn 0;\t\t\t/* simulate invalid_fp_register exception */\n\t\t}\n\t/* fall through */\n\tcase 2:\n\t\tif (freg & 1) {\t\t\t\t/* doublewords must have bit 5 zeroed */\n\t\t\t*pfsr |= (6 << 14);\n\t\t\treturn 0;\n\t\t}\n\t}\n\trs2 = (argp)&fregs[freg];\n\tswitch ((type >> 3) & 0x7) {\n\tcase 7: FP_UNPACK_QP (QB, rs2); break;\n\tcase 6: FP_UNPACK_DP (DB, rs2); break;\n\tcase 5: FP_UNPACK_SP (SB, rs2); break;\n\t}\n\tfreg = ((insn >> 25) & 0x1f);\n\tswitch ((type >> 6) & 0x3) {\t\t\t/* and finally rd. This one's a bit different */\n\tcase 0:\t\t\t\t\t\t/* dest is fcc. (this must be FCMPQ or FCMPEQ) */\n\t\tif (freg) {\t\t\t\t/* V8 has only one set of condition codes, so */\n\t\t\t\t\t\t\t/* anything but 0 in the rd field is an error */\n\t\t\t*pfsr |= (6 << 14);\t\t/* (should probably flag as invalid opcode */\n\t\t\treturn 0;\t\t\t/* but SIGFPE will do :-> ) */\n\t\t}\n\t\tbreak;\n\tcase 3:\n\t\tif (freg & 3) {\t\t\t\t/* quadwords must have bits 4&5 of the */\n\t\t\t\t\t\t\t/* encoded reg. number set to zero. */\n\t\t\t*pfsr |= (6 << 14);\n\t\t\treturn 0;\t\t\t/* simulate invalid_fp_register exception */\n\t\t}\n\t/* fall through */\n\tcase 2:\n\t\tif (freg & 1) {\t\t\t\t/* doublewords must have bit 5 zeroed */\n\t\t\t*pfsr |= (6 << 14);\n\t\t\treturn 0;\n\t\t}\n\t/* fall through */\n\tcase 1:\n\t\trd = (void *)&fregs[freg];\n\t\tbreak;\n\t}\n#ifdef DEBUG_MATHEMU\n\tprintk(\"executing insn...\\n\");\n#endif\n\t/* do the Right Thing */\n\tswitch ((insn >> 5) & 0x1ff) {\n\t/* + */\n\tcase FADDS: FP_ADD_S (SR, SA, SB); break;\n\tcase FADDD: FP_ADD_D (DR, DA, DB); break;\n\tcase FADDQ: FP_ADD_Q (QR, QA, QB); break;\n\t/* - */\n\tcase FSUBS: FP_SUB_S (SR, SA, SB); break;\n\tcase FSUBD: FP_SUB_D (DR, DA, DB); break;\n\tcase FSUBQ: FP_SUB_Q (QR, QA, QB); break;\n\t/* * */\n\tcase FMULS: FP_MUL_S (SR, SA, SB); break;\n\tcase FSMULD: FP_CONV (D, S, 2, 1, DA, SA);\n\t\t     FP_CONV (D, S, 2, 1, DB, SB);\n\tcase FMULD: FP_MUL_D (DR, DA, DB); break;\n\tcase FDMULQ: FP_CONV (Q, D, 4, 2, QA, DA);\n\t\t     FP_CONV (Q, D, 4, 2, QB, DB);\n\tcase FMULQ: FP_MUL_Q (QR, QA, QB); break;\n\t/* / */\n\tcase FDIVS: FP_DIV_S (SR, SA, SB); break;\n\tcase FDIVD: FP_DIV_D (DR, DA, DB); break;\n\tcase FDIVQ: FP_DIV_Q (QR, QA, QB); break;\n\t/* sqrt */\n\tcase FSQRTS: FP_SQRT_S (SR, SB); break;\n\tcase FSQRTD: FP_SQRT_D (DR, DB); break;\n\tcase FSQRTQ: FP_SQRT_Q (QR, QB); break;\n\t/* mov */\n\tcase FMOVS: rd->s = rs2->s; break;\n\tcase FABSS: rd->s = rs2->s & 0x7fffffff; break;\n\tcase FNEGS: rd->s = rs2->s ^ 0x80000000; break;\n\t/* float to int */\n\tcase FSTOI: FP_TO_INT_S (IR, SB, 32, 1); break;\n\tcase FDTOI: FP_TO_INT_D (IR, DB, 32, 1); break;\n\tcase FQTOI: FP_TO_INT_Q (IR, QB, 32, 1); break;\n\t/* int to float */\n\tcase FITOS: IR = rs2->s; FP_FROM_INT_S (SR, IR, 32, int); break;\n\tcase FITOD: IR = rs2->s; FP_FROM_INT_D (DR, IR, 32, int); break;\n\tcase FITOQ: IR = rs2->s; FP_FROM_INT_Q (QR, IR, 32, int); break;\n\t/* float to float */\n\tcase FSTOD: FP_CONV (D, S, 2, 1, DR, SB); break;\n\tcase FSTOQ: FP_CONV (Q, S, 4, 1, QR, SB); break;\n\tcase FDTOQ: FP_CONV (Q, D, 4, 2, QR, DB); break;\n\tcase FDTOS: FP_CONV (S, D, 1, 2, SR, DB); break;\n\tcase FQTOS: FP_CONV (S, Q, 1, 4, SR, QB); break;\n\tcase FQTOD: FP_CONV (D, Q, 2, 4, DR, QB); break;\n\t/* comparison */\n\tcase FCMPS:\n\tcase FCMPES:\n\t\tFP_CMP_S(IR, SB, SA, 3);\n\t\tif (IR == 3 &&\n\t\t    (((insn >> 5) & 0x1ff) == FCMPES ||\n\t\t     FP_ISSIGNAN_S(SA) ||\n\t\t     FP_ISSIGNAN_S(SB)))\n\t\t\tFP_SET_EXCEPTION (FP_EX_INVALID);\n\t\tbreak;\n\tcase FCMPD:\n\tcase FCMPED:\n\t\tFP_CMP_D(IR, DB, DA, 3);\n\t\tif (IR == 3 &&\n\t\t    (((insn >> 5) & 0x1ff) == FCMPED ||\n\t\t     FP_ISSIGNAN_D(DA) ||\n\t\t     FP_ISSIGNAN_D(DB)))\n\t\t\tFP_SET_EXCEPTION (FP_EX_INVALID);\n\t\tbreak;\n\tcase FCMPQ:\n\tcase FCMPEQ:\n\t\tFP_CMP_Q(IR, QB, QA, 3);\n\t\tif (IR == 3 &&\n\t\t    (((insn >> 5) & 0x1ff) == FCMPEQ ||\n\t\t     FP_ISSIGNAN_Q(QA) ||\n\t\t     FP_ISSIGNAN_Q(QB)))\n\t\t\tFP_SET_EXCEPTION (FP_EX_INVALID);\n\t}\n\tif (!FP_INHIBIT_RESULTS) {\n\t\tswitch ((type >> 6) & 0x7) {\n\t\tcase 0: fsr = *pfsr;\n\t\t\tif (IR == -1) IR = 2;\n\t\t\t/* fcc is always fcc0 */\n\t\t\tfsr &= ~0xc00; fsr |= (IR << 10); break;\n\t\t\t*pfsr = fsr;\n\t\t\tbreak;\n\t\tcase 1: rd->s = IR; break;\n\t\tcase 5: FP_PACK_SP (rd, SR); break;\n\t\tcase 6: FP_PACK_DP (rd, DR); break;\n\t\tcase 7: FP_PACK_QP (rd, QR); break;\n\t\t}\n\t}\n\tif (_fex == 0)\n\t\treturn 1;\t\t\t\t/* success! */\n\treturn record_exception(pfsr, _fex);\n}\n", "/*\n * arch/sparc64/math-emu/math.c\n *\n * Copyright (C) 1997,1999 Jakub Jelinek (jj@ultra.linux.cz)\n * Copyright (C) 1999 David S. Miller (davem@redhat.com)\n *\n * Emulation routines originate from soft-fp package, which is part\n * of glibc and has appropriate copyrights in it.\n */\n\n#include <linux/types.h>\n#include <linux/sched.h>\n#include <linux/errno.h>\n#include <linux/perf_event.h>\n\n#include <asm/fpumacro.h>\n#include <asm/ptrace.h>\n#include <asm/uaccess.h>\n\n#include \"sfp-util_64.h\"\n#include <math-emu/soft-fp.h>\n#include <math-emu/single.h>\n#include <math-emu/double.h>\n#include <math-emu/quad.h>\n\n/* QUAD - ftt == 3 */\n#define FMOVQ\t0x003\n#define FNEGQ\t0x007\n#define FABSQ\t0x00b\n#define FSQRTQ\t0x02b\n#define FADDQ\t0x043\n#define FSUBQ\t0x047\n#define FMULQ\t0x04b\n#define FDIVQ\t0x04f\n#define FDMULQ\t0x06e\n#define FQTOX\t0x083\n#define FXTOQ\t0x08c\n#define FQTOS\t0x0c7\n#define FQTOD\t0x0cb\n#define FITOQ\t0x0cc\n#define FSTOQ\t0x0cd\n#define FDTOQ\t0x0ce\n#define FQTOI\t0x0d3\n/* SUBNORMAL - ftt == 2 */\n#define FSQRTS\t0x029\n#define FSQRTD\t0x02a\n#define FADDS\t0x041\n#define FADDD\t0x042\n#define FSUBS\t0x045\n#define FSUBD\t0x046\n#define FMULS\t0x049\n#define FMULD\t0x04a\n#define FDIVS\t0x04d\n#define FDIVD\t0x04e\n#define FSMULD\t0x069\n#define FSTOX\t0x081\n#define FDTOX\t0x082\n#define FDTOS\t0x0c6\n#define FSTOD\t0x0c9\n#define FSTOI\t0x0d1\n#define FDTOI\t0x0d2\n#define FXTOS\t0x084 /* Only Ultra-III generates this. */\n#define FXTOD\t0x088 /* Only Ultra-III generates this. */\n#if 0\t/* Optimized inline in sparc64/kernel/entry.S */\n#define FITOS\t0x0c4 /* Only Ultra-III generates this. */\n#endif\n#define FITOD\t0x0c8 /* Only Ultra-III generates this. */\n/* FPOP2 */\n#define FCMPQ\t0x053\n#define FCMPEQ\t0x057\n#define FMOVQ0\t0x003\n#define FMOVQ1\t0x043\n#define FMOVQ2\t0x083\n#define FMOVQ3\t0x0c3\n#define FMOVQI\t0x103\n#define FMOVQX\t0x183\n#define FMOVQZ\t0x027\n#define FMOVQLE\t0x047\n#define FMOVQLZ 0x067\n#define FMOVQNZ\t0x0a7\n#define FMOVQGZ\t0x0c7\n#define FMOVQGE 0x0e7\n\n#define FSR_TEM_SHIFT\t23UL\n#define FSR_TEM_MASK\t(0x1fUL << FSR_TEM_SHIFT)\n#define FSR_AEXC_SHIFT\t5UL\n#define FSR_AEXC_MASK\t(0x1fUL << FSR_AEXC_SHIFT)\n#define FSR_CEXC_SHIFT\t0UL\n#define FSR_CEXC_MASK\t(0x1fUL << FSR_CEXC_SHIFT)\n\n/* All routines returning an exception to raise should detect\n * such exceptions _before_ rounding to be consistent with\n * the behavior of the hardware in the implemented cases\n * (and thus with the recommendations in the V9 architecture\n * manual).\n *\n * We return 0 if a SIGFPE should be sent, 1 otherwise.\n */\nstatic inline int record_exception(struct pt_regs *regs, int eflag)\n{\n\tu64 fsr = current_thread_info()->xfsr[0];\n\tint would_trap;\n\n\t/* Determine if this exception would have generated a trap. */\n\twould_trap = (fsr & ((long)eflag << FSR_TEM_SHIFT)) != 0UL;\n\n\t/* If trapping, we only want to signal one bit. */\n\tif(would_trap != 0) {\n\t\teflag &= ((fsr & FSR_TEM_MASK) >> FSR_TEM_SHIFT);\n\t\tif((eflag & (eflag - 1)) != 0) {\n\t\t\tif(eflag & FP_EX_INVALID)\n\t\t\t\teflag = FP_EX_INVALID;\n\t\t\telse if(eflag & FP_EX_OVERFLOW)\n\t\t\t\teflag = FP_EX_OVERFLOW;\n\t\t\telse if(eflag & FP_EX_UNDERFLOW)\n\t\t\t\teflag = FP_EX_UNDERFLOW;\n\t\t\telse if(eflag & FP_EX_DIVZERO)\n\t\t\t\teflag = FP_EX_DIVZERO;\n\t\t\telse if(eflag & FP_EX_INEXACT)\n\t\t\t\teflag = FP_EX_INEXACT;\n\t\t}\n\t}\n\n\t/* Set CEXC, here is the rule:\n\t *\n\t *    In general all FPU ops will set one and only one\n\t *    bit in the CEXC field, this is always the case\n\t *    when the IEEE exception trap is enabled in TEM.\n\t */\n\tfsr &= ~(FSR_CEXC_MASK);\n\tfsr |= ((long)eflag << FSR_CEXC_SHIFT);\n\n\t/* Set the AEXC field, rule is:\n\t *\n\t *    If a trap would not be generated, the\n\t *    CEXC just generated is OR'd into the\n\t *    existing value of AEXC.\n\t */\n\tif(would_trap == 0)\n\t\tfsr |= ((long)eflag << FSR_AEXC_SHIFT);\n\n\t/* If trapping, indicate fault trap type IEEE. */\n\tif(would_trap != 0)\n\t\tfsr |= (1UL << 14);\n\n\tcurrent_thread_info()->xfsr[0] = fsr;\n\n\t/* If we will not trap, advance the program counter over\n\t * the instruction being handled.\n\t */\n\tif(would_trap == 0) {\n\t\tregs->tpc = regs->tnpc;\n\t\tregs->tnpc += 4;\n\t}\n\n\treturn (would_trap ? 0 : 1);\n}\n\ntypedef union {\n\tu32 s;\n\tu64 d;\n\tu64 q[2];\n} *argp;\n\nint do_mathemu(struct pt_regs *regs, struct fpustate *f)\n{\n\tunsigned long pc = regs->tpc;\n\tunsigned long tstate = regs->tstate;\n\tu32 insn = 0;\n\tint type = 0;\n\t/* ftt tells which ftt it may happen in, r is rd, b is rs2 and a is rs1. The *u arg tells\n\t   whether the argument should be packed/unpacked (0 - do not unpack/pack, 1 - unpack/pack)\n\t   non-u args tells the size of the argument (0 - no argument, 1 - single, 2 - double, 3 - quad */\n#define TYPE(ftt, r, ru, b, bu, a, au) type = (au << 2) | (a << 0) | (bu << 5) | (b << 3) | (ru << 8) | (r << 6) | (ftt << 9)\n\tint freg;\n\tstatic u64 zero[2] = { 0L, 0L };\n\tint flags;\n\tFP_DECL_EX;\n\tFP_DECL_S(SA); FP_DECL_S(SB); FP_DECL_S(SR);\n\tFP_DECL_D(DA); FP_DECL_D(DB); FP_DECL_D(DR);\n\tFP_DECL_Q(QA); FP_DECL_Q(QB); FP_DECL_Q(QR);\n\tint IR;\n\tlong XR, xfsr;\n\n\tif (tstate & TSTATE_PRIV)\n\t\tdie_if_kernel(\"unfinished/unimplemented FPop from kernel\", regs);\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);\n\tif (test_thread_flag(TIF_32BIT))\n\t\tpc = (u32)pc;\n\tif (get_user(insn, (u32 __user *) pc) != -EFAULT) {\n\t\tif ((insn & 0xc1f80000) == 0x81a00000) /* FPOP1 */ {\n\t\t\tswitch ((insn >> 5) & 0x1ff) {\n\t\t\t/* QUAD - ftt == 3 */\n\t\t\tcase FMOVQ:\n\t\t\tcase FNEGQ:\n\t\t\tcase FABSQ: TYPE(3,3,0,3,0,0,0); break;\n\t\t\tcase FSQRTQ: TYPE(3,3,1,3,1,0,0); break;\n\t\t\tcase FADDQ:\n\t\t\tcase FSUBQ:\n\t\t\tcase FMULQ:\n\t\t\tcase FDIVQ: TYPE(3,3,1,3,1,3,1); break;\n\t\t\tcase FDMULQ: TYPE(3,3,1,2,1,2,1); break;\n\t\t\tcase FQTOX: TYPE(3,2,0,3,1,0,0); break;\n\t\t\tcase FXTOQ: TYPE(3,3,1,2,0,0,0); break;\n\t\t\tcase FQTOS: TYPE(3,1,1,3,1,0,0); break;\n\t\t\tcase FQTOD: TYPE(3,2,1,3,1,0,0); break;\n\t\t\tcase FITOQ: TYPE(3,3,1,1,0,0,0); break;\n\t\t\tcase FSTOQ: TYPE(3,3,1,1,1,0,0); break;\n\t\t\tcase FDTOQ: TYPE(3,3,1,2,1,0,0); break;\n\t\t\tcase FQTOI: TYPE(3,1,0,3,1,0,0); break;\n\n\t\t\t/* We can get either unimplemented or unfinished\n\t\t\t * for these cases.  Pre-Niagara systems generate\n\t\t\t * unfinished fpop for SUBNORMAL cases, and Niagara\n\t\t\t * always gives unimplemented fpop for fsqrt{s,d}.\n\t\t\t */\n\t\t\tcase FSQRTS: {\n\t\t\t\tunsigned long x = current_thread_info()->xfsr[0];\n\n\t\t\t\tx = (x >> 14) & 0xf;\n\t\t\t\tTYPE(x,1,1,1,1,0,0);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tcase FSQRTD: {\n\t\t\t\tunsigned long x = current_thread_info()->xfsr[0];\n\n\t\t\t\tx = (x >> 14) & 0xf;\n\t\t\t\tTYPE(x,2,1,2,1,0,0);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* SUBNORMAL - ftt == 2 */\n\t\t\tcase FADDD:\n\t\t\tcase FSUBD:\n\t\t\tcase FMULD:\n\t\t\tcase FDIVD: TYPE(2,2,1,2,1,2,1); break;\n\t\t\tcase FADDS:\n\t\t\tcase FSUBS:\n\t\t\tcase FMULS:\n\t\t\tcase FDIVS: TYPE(2,1,1,1,1,1,1); break;\n\t\t\tcase FSMULD: TYPE(2,2,1,1,1,1,1); break;\n\t\t\tcase FSTOX: TYPE(2,2,0,1,1,0,0); break;\n\t\t\tcase FDTOX: TYPE(2,2,0,2,1,0,0); break;\n\t\t\tcase FDTOS: TYPE(2,1,1,2,1,0,0); break;\n\t\t\tcase FSTOD: TYPE(2,2,1,1,1,0,0); break;\n\t\t\tcase FSTOI: TYPE(2,1,0,1,1,0,0); break;\n\t\t\tcase FDTOI: TYPE(2,1,0,2,1,0,0); break;\n\n\t\t\t/* Only Ultra-III generates these */\n\t\t\tcase FXTOS: TYPE(2,1,1,2,0,0,0); break;\n\t\t\tcase FXTOD: TYPE(2,2,1,2,0,0,0); break;\n#if 0\t\t\t/* Optimized inline in sparc64/kernel/entry.S */\n\t\t\tcase FITOS: TYPE(2,1,1,1,0,0,0); break;\n#endif\n\t\t\tcase FITOD: TYPE(2,2,1,1,0,0,0); break;\n\t\t\t}\n\t\t}\n\t\telse if ((insn & 0xc1f80000) == 0x81a80000) /* FPOP2 */ {\n\t\t\tIR = 2;\n\t\t\tswitch ((insn >> 5) & 0x1ff) {\n\t\t\tcase FCMPQ: TYPE(3,0,0,3,1,3,1); break;\n\t\t\tcase FCMPEQ: TYPE(3,0,0,3,1,3,1); break;\n\t\t\t/* Now the conditional fmovq support */\n\t\t\tcase FMOVQ0:\n\t\t\tcase FMOVQ1:\n\t\t\tcase FMOVQ2:\n\t\t\tcase FMOVQ3:\n\t\t\t\t/* fmovq %fccX, %fY, %fZ */\n\t\t\t\tif (!((insn >> 11) & 3))\n\t\t\t\t\tXR = current_thread_info()->xfsr[0] >> 10;\n\t\t\t\telse\n\t\t\t\t\tXR = current_thread_info()->xfsr[0] >> (30 + ((insn >> 10) & 0x6));\n\t\t\t\tXR &= 3;\n\t\t\t\tIR = 0;\n\t\t\t\tswitch ((insn >> 14) & 0x7) {\n\t\t\t\t/* case 0: IR = 0; break; */\t\t\t/* Never */\n\t\t\t\tcase 1: if (XR) IR = 1; break;\t\t\t/* Not Equal */\n\t\t\t\tcase 2: if (XR == 1 || XR == 2) IR = 1; break;\t/* Less or Greater */\n\t\t\t\tcase 3: if (XR & 1) IR = 1; break;\t\t/* Unordered or Less */\n\t\t\t\tcase 4: if (XR == 1) IR = 1; break;\t\t/* Less */\n\t\t\t\tcase 5: if (XR & 2) IR = 1; break;\t\t/* Unordered or Greater */\n\t\t\t\tcase 6: if (XR == 2) IR = 1; break;\t\t/* Greater */\n\t\t\t\tcase 7: if (XR == 3) IR = 1; break;\t\t/* Unordered */\n\t\t\t\t}\n\t\t\t\tif ((insn >> 14) & 8)\n\t\t\t\t\tIR ^= 1;\n\t\t\t\tbreak;\n\t\t\tcase FMOVQI:\n\t\t\tcase FMOVQX:\n\t\t\t\t/* fmovq %[ix]cc, %fY, %fZ */\n\t\t\t\tXR = regs->tstate >> 32;\n\t\t\t\tif ((insn >> 5) & 0x80)\n\t\t\t\t\tXR >>= 4;\n\t\t\t\tXR &= 0xf;\n\t\t\t\tIR = 0;\n\t\t\t\tfreg = ((XR >> 2) ^ XR) & 2;\n\t\t\t\tswitch ((insn >> 14) & 0x7) {\n\t\t\t\t/* case 0: IR = 0; break; */\t\t\t/* Never */\n\t\t\t\tcase 1: if (XR & 4) IR = 1; break;\t\t/* Equal */\n\t\t\t\tcase 2: if ((XR & 4) || freg) IR = 1; break;\t/* Less or Equal */\n\t\t\t\tcase 3: if (freg) IR = 1; break;\t\t/* Less */\n\t\t\t\tcase 4: if (XR & 5) IR = 1; break;\t\t/* Less or Equal Unsigned */\n\t\t\t\tcase 5: if (XR & 1) IR = 1; break;\t\t/* Carry Set */\n\t\t\t\tcase 6: if (XR & 8) IR = 1; break;\t\t/* Negative */\n\t\t\t\tcase 7: if (XR & 2) IR = 1; break;\t\t/* Overflow Set */\n\t\t\t\t}\n\t\t\t\tif ((insn >> 14) & 8)\n\t\t\t\t\tIR ^= 1;\n\t\t\t\tbreak;\n\t\t\tcase FMOVQZ:\n\t\t\tcase FMOVQLE:\n\t\t\tcase FMOVQLZ:\n\t\t\tcase FMOVQNZ:\n\t\t\tcase FMOVQGZ:\n\t\t\tcase FMOVQGE:\n\t\t\t\tfreg = (insn >> 14) & 0x1f;\n\t\t\t\tif (!freg)\n\t\t\t\t\tXR = 0;\n\t\t\t\telse if (freg < 16)\n\t\t\t\t\tXR = regs->u_regs[freg];\n\t\t\t\telse if (test_thread_flag(TIF_32BIT)) {\n\t\t\t\t\tstruct reg_window32 __user *win32;\n\t\t\t\t\tflushw_user ();\n\t\t\t\t\twin32 = (struct reg_window32 __user *)((unsigned long)((u32)regs->u_regs[UREG_FP]));\n\t\t\t\t\tget_user(XR, &win32->locals[freg - 16]);\n\t\t\t\t} else {\n\t\t\t\t\tstruct reg_window __user *win;\n\t\t\t\t\tflushw_user ();\n\t\t\t\t\twin = (struct reg_window __user *)(regs->u_regs[UREG_FP] + STACK_BIAS);\n\t\t\t\t\tget_user(XR, &win->locals[freg - 16]);\n\t\t\t\t}\n\t\t\t\tIR = 0;\n\t\t\t\tswitch ((insn >> 10) & 3) {\n\t\t\t\tcase 1: if (!XR) IR = 1; break;\t\t\t/* Register Zero */\n\t\t\t\tcase 2: if (XR <= 0) IR = 1; break;\t\t/* Register Less Than or Equal to Zero */\n\t\t\t\tcase 3: if (XR < 0) IR = 1; break;\t\t/* Register Less Than Zero */\n\t\t\t\t}\n\t\t\t\tif ((insn >> 10) & 4)\n\t\t\t\t\tIR ^= 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (IR == 0) {\n\t\t\t\t/* The fmov test was false. Do a nop instead */\n\t\t\t\tcurrent_thread_info()->xfsr[0] &= ~(FSR_CEXC_MASK);\n\t\t\t\tregs->tpc = regs->tnpc;\n\t\t\t\tregs->tnpc += 4;\n\t\t\t\treturn 1;\n\t\t\t} else if (IR == 1) {\n\t\t\t\t/* Change the instruction into plain fmovq */\n\t\t\t\tinsn = (insn & 0x3e00001f) | 0x81a00060;\n\t\t\t\tTYPE(3,3,0,3,0,0,0); \n\t\t\t}\n\t\t}\n\t}\n\tif (type) {\n\t\targp rs1 = NULL, rs2 = NULL, rd = NULL;\n\t\t\n\t\tfreg = (current_thread_info()->xfsr[0] >> 14) & 0xf;\n\t\tif (freg != (type >> 9))\n\t\t\tgoto err;\n\t\tcurrent_thread_info()->xfsr[0] &= ~0x1c000;\n\t\tfreg = ((insn >> 14) & 0x1f);\n\t\tswitch (type & 0x3) {\n\t\tcase 3: if (freg & 2) {\n\t\t\t\tcurrent_thread_info()->xfsr[0] |= (6 << 14) /* invalid_fp_register */;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\tcase 2: freg = ((freg & 1) << 5) | (freg & 0x1e);\n\t\tcase 1: rs1 = (argp)&f->regs[freg];\n\t\t\tflags = (freg < 32) ? FPRS_DL : FPRS_DU; \n\t\t\tif (!(current_thread_info()->fpsaved[0] & flags))\n\t\t\t\trs1 = (argp)&zero;\n\t\t\tbreak;\n\t\t}\n\t\tswitch (type & 0x7) {\n\t\tcase 7: FP_UNPACK_QP (QA, rs1); break;\n\t\tcase 6: FP_UNPACK_DP (DA, rs1); break;\n\t\tcase 5: FP_UNPACK_SP (SA, rs1); break;\n\t\t}\n\t\tfreg = (insn & 0x1f);\n\t\tswitch ((type >> 3) & 0x3) {\n\t\tcase 3: if (freg & 2) {\n\t\t\t\tcurrent_thread_info()->xfsr[0] |= (6 << 14) /* invalid_fp_register */;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\tcase 2: freg = ((freg & 1) << 5) | (freg & 0x1e);\n\t\tcase 1: rs2 = (argp)&f->regs[freg];\n\t\t\tflags = (freg < 32) ? FPRS_DL : FPRS_DU; \n\t\t\tif (!(current_thread_info()->fpsaved[0] & flags))\n\t\t\t\trs2 = (argp)&zero;\n\t\t\tbreak;\n\t\t}\n\t\tswitch ((type >> 3) & 0x7) {\n\t\tcase 7: FP_UNPACK_QP (QB, rs2); break;\n\t\tcase 6: FP_UNPACK_DP (DB, rs2); break;\n\t\tcase 5: FP_UNPACK_SP (SB, rs2); break;\n\t\t}\n\t\tfreg = ((insn >> 25) & 0x1f);\n\t\tswitch ((type >> 6) & 0x3) {\n\t\tcase 3: if (freg & 2) {\n\t\t\t\tcurrent_thread_info()->xfsr[0] |= (6 << 14) /* invalid_fp_register */;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\tcase 2: freg = ((freg & 1) << 5) | (freg & 0x1e);\n\t\tcase 1: rd = (argp)&f->regs[freg];\n\t\t\tflags = (freg < 32) ? FPRS_DL : FPRS_DU; \n\t\t\tif (!(current_thread_info()->fpsaved[0] & FPRS_FEF)) {\n\t\t\t\tcurrent_thread_info()->fpsaved[0] = FPRS_FEF;\n\t\t\t\tcurrent_thread_info()->gsr[0] = 0;\n\t\t\t}\n\t\t\tif (!(current_thread_info()->fpsaved[0] & flags)) {\n\t\t\t\tif (freg < 32)\n\t\t\t\t\tmemset(f->regs, 0, 32*sizeof(u32));\n\t\t\t\telse\n\t\t\t\t\tmemset(f->regs+32, 0, 32*sizeof(u32));\n\t\t\t}\n\t\t\tcurrent_thread_info()->fpsaved[0] |= flags;\n\t\t\tbreak;\n\t\t}\n\t\tswitch ((insn >> 5) & 0x1ff) {\n\t\t/* + */\n\t\tcase FADDS: FP_ADD_S (SR, SA, SB); break;\n\t\tcase FADDD: FP_ADD_D (DR, DA, DB); break;\n\t\tcase FADDQ: FP_ADD_Q (QR, QA, QB); break;\n\t\t/* - */\n\t\tcase FSUBS: FP_SUB_S (SR, SA, SB); break;\n\t\tcase FSUBD: FP_SUB_D (DR, DA, DB); break;\n\t\tcase FSUBQ: FP_SUB_Q (QR, QA, QB); break;\n\t\t/* * */\n\t\tcase FMULS: FP_MUL_S (SR, SA, SB); break;\n\t\tcase FSMULD: FP_CONV (D, S, 1, 1, DA, SA);\n\t\t\t     FP_CONV (D, S, 1, 1, DB, SB);\n\t\tcase FMULD: FP_MUL_D (DR, DA, DB); break;\n\t\tcase FDMULQ: FP_CONV (Q, D, 2, 1, QA, DA);\n\t\t\t     FP_CONV (Q, D, 2, 1, QB, DB);\n\t\tcase FMULQ: FP_MUL_Q (QR, QA, QB); break;\n\t\t/* / */\n\t\tcase FDIVS: FP_DIV_S (SR, SA, SB); break;\n\t\tcase FDIVD: FP_DIV_D (DR, DA, DB); break;\n\t\tcase FDIVQ: FP_DIV_Q (QR, QA, QB); break;\n\t\t/* sqrt */\n\t\tcase FSQRTS: FP_SQRT_S (SR, SB); break;\n\t\tcase FSQRTD: FP_SQRT_D (DR, DB); break;\n\t\tcase FSQRTQ: FP_SQRT_Q (QR, QB); break;\n\t\t/* mov */\n\t\tcase FMOVQ: rd->q[0] = rs2->q[0]; rd->q[1] = rs2->q[1]; break;\n\t\tcase FABSQ: rd->q[0] = rs2->q[0] & 0x7fffffffffffffffUL; rd->q[1] = rs2->q[1]; break;\n\t\tcase FNEGQ: rd->q[0] = rs2->q[0] ^ 0x8000000000000000UL; rd->q[1] = rs2->q[1]; break;\n\t\t/* float to int */\n\t\tcase FSTOI: FP_TO_INT_S (IR, SB, 32, 1); break;\n\t\tcase FDTOI: FP_TO_INT_D (IR, DB, 32, 1); break;\n\t\tcase FQTOI: FP_TO_INT_Q (IR, QB, 32, 1); break;\n\t\tcase FSTOX: FP_TO_INT_S (XR, SB, 64, 1); break;\n\t\tcase FDTOX: FP_TO_INT_D (XR, DB, 64, 1); break;\n\t\tcase FQTOX: FP_TO_INT_Q (XR, QB, 64, 1); break;\n\t\t/* int to float */\n\t\tcase FITOQ: IR = rs2->s; FP_FROM_INT_Q (QR, IR, 32, int); break;\n\t\tcase FXTOQ: XR = rs2->d; FP_FROM_INT_Q (QR, XR, 64, long); break;\n\t\t/* Only Ultra-III generates these */\n\t\tcase FXTOS: XR = rs2->d; FP_FROM_INT_S (SR, XR, 64, long); break;\n\t\tcase FXTOD: XR = rs2->d; FP_FROM_INT_D (DR, XR, 64, long); break;\n#if 0\t\t/* Optimized inline in sparc64/kernel/entry.S */\n\t\tcase FITOS: IR = rs2->s; FP_FROM_INT_S (SR, IR, 32, int); break;\n#endif\n\t\tcase FITOD: IR = rs2->s; FP_FROM_INT_D (DR, IR, 32, int); break;\n\t\t/* float to float */\n\t\tcase FSTOD: FP_CONV (D, S, 1, 1, DR, SB); break;\n\t\tcase FSTOQ: FP_CONV (Q, S, 2, 1, QR, SB); break;\n\t\tcase FDTOQ: FP_CONV (Q, D, 2, 1, QR, DB); break;\n\t\tcase FDTOS: FP_CONV (S, D, 1, 1, SR, DB); break;\n\t\tcase FQTOS: FP_CONV (S, Q, 1, 2, SR, QB); break;\n\t\tcase FQTOD: FP_CONV (D, Q, 1, 2, DR, QB); break;\n\t\t/* comparison */\n\t\tcase FCMPQ:\n\t\tcase FCMPEQ:\n\t\t\tFP_CMP_Q(XR, QB, QA, 3);\n\t\t\tif (XR == 3 &&\n\t\t\t    (((insn >> 5) & 0x1ff) == FCMPEQ ||\n\t\t\t     FP_ISSIGNAN_Q(QA) ||\n\t\t\t     FP_ISSIGNAN_Q(QB)))\n\t\t\t\tFP_SET_EXCEPTION (FP_EX_INVALID);\n\t\t}\n\t\tif (!FP_INHIBIT_RESULTS) {\n\t\t\tswitch ((type >> 6) & 0x7) {\n\t\t\tcase 0: xfsr = current_thread_info()->xfsr[0];\n\t\t\t\tif (XR == -1) XR = 2;\n\t\t\t\tswitch (freg & 3) {\n\t\t\t\t/* fcc0, 1, 2, 3 */\n\t\t\t\tcase 0: xfsr &= ~0xc00; xfsr |= (XR << 10); break;\n\t\t\t\tcase 1: xfsr &= ~0x300000000UL; xfsr |= (XR << 32); break;\n\t\t\t\tcase 2: xfsr &= ~0xc00000000UL; xfsr |= (XR << 34); break;\n\t\t\t\tcase 3: xfsr &= ~0x3000000000UL; xfsr |= (XR << 36); break;\n\t\t\t\t}\n\t\t\t\tcurrent_thread_info()->xfsr[0] = xfsr;\n\t\t\t\tbreak;\n\t\t\tcase 1: rd->s = IR; break;\n\t\t\tcase 2: rd->d = XR; break;\n\t\t\tcase 5: FP_PACK_SP (rd, SR); break;\n\t\t\tcase 6: FP_PACK_DP (rd, DR); break;\n\t\t\tcase 7: FP_PACK_QP (rd, QR); break;\n\t\t\t}\n\t\t}\n\n\t\tif(_fex != 0)\n\t\t\treturn record_exception(regs, _fex);\n\n\t\t/* Success and no exceptions detected. */\n\t\tcurrent_thread_info()->xfsr[0] &= ~(FSR_CEXC_MASK);\n\t\tregs->tpc = regs->tnpc;\n\t\tregs->tnpc += 4;\n\t\treturn 1;\n\t}\nerr:\treturn 0;\n}\n", "/*\n * fault.c:  Page fault handlers for the Sparc.\n *\n * Copyright (C) 1995 David S. Miller (davem@caip.rutgers.edu)\n * Copyright (C) 1996 Eddie C. Dost (ecd@skynet.be)\n * Copyright (C) 1997 Jakub Jelinek (jj@sunsite.mff.cuni.cz)\n */\n\n#include <asm/head.h>\n\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/sched.h>\n#include <linux/ptrace.h>\n#include <linux/mman.h>\n#include <linux/threads.h>\n#include <linux/kernel.h>\n#include <linux/signal.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/perf_event.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/kdebug.h>\n\n#include <asm/system.h>\n#include <asm/page.h>\n#include <asm/pgtable.h>\n#include <asm/memreg.h>\n#include <asm/openprom.h>\n#include <asm/oplib.h>\n#include <asm/smp.h>\n#include <asm/traps.h>\n#include <asm/uaccess.h>\n\nextern int prom_node_root;\n\nint show_unhandled_signals = 1;\n\n/* At boot time we determine these two values necessary for setting\n * up the segment maps and page table entries (pte's).\n */\n\nint num_segmaps, num_contexts;\nint invalid_segment;\n\n/* various Virtual Address Cache parameters we find at boot time... */\n\nint vac_size, vac_linesize, vac_do_hw_vac_flushes;\nint vac_entries_per_context, vac_entries_per_segment;\nint vac_entries_per_page;\n\n/* Return how much physical memory we have.  */\nunsigned long probe_memory(void)\n{\n\tunsigned long total = 0;\n\tint i;\n\n\tfor (i = 0; sp_banks[i].num_bytes; i++)\n\t\ttotal += sp_banks[i].num_bytes;\n\n\treturn total;\n}\n\nextern void sun4c_complete_all_stores(void);\n\n/* Whee, a level 15 NMI interrupt memory error.  Let's have fun... */\nasmlinkage void sparc_lvl15_nmi(struct pt_regs *regs, unsigned long serr,\n\t\t\t\tunsigned long svaddr, unsigned long aerr,\n\t\t\t\tunsigned long avaddr)\n{\n\tsun4c_complete_all_stores();\n\tprintk(\"FAULT: NMI received\\n\");\n\tprintk(\"SREGS: Synchronous Error %08lx\\n\", serr);\n\tprintk(\"       Synchronous Vaddr %08lx\\n\", svaddr);\n\tprintk(\"      Asynchronous Error %08lx\\n\", aerr);\n\tprintk(\"      Asynchronous Vaddr %08lx\\n\", avaddr);\n\tif (sun4c_memerr_reg)\n\t\tprintk(\"     Memory Parity Error %08lx\\n\", *sun4c_memerr_reg);\n\tprintk(\"REGISTER DUMP:\\n\");\n\tshow_regs(regs);\n\tprom_halt();\n}\n\nstatic void unhandled_fault(unsigned long, struct task_struct *,\n\t\tstruct pt_regs *) __attribute__ ((noreturn));\n\nstatic void unhandled_fault(unsigned long address, struct task_struct *tsk,\n                     struct pt_regs *regs)\n{\n\tif((unsigned long) address < PAGE_SIZE) {\n\t\tprintk(KERN_ALERT\n\t\t    \"Unable to handle kernel NULL pointer dereference\\n\");\n\t} else {\n\t\tprintk(KERN_ALERT \"Unable to handle kernel paging request \"\n\t\t       \"at virtual address %08lx\\n\", address);\n\t}\n\tprintk(KERN_ALERT \"tsk->{mm,active_mm}->context = %08lx\\n\",\n\t\t(tsk->mm ? tsk->mm->context : tsk->active_mm->context));\n\tprintk(KERN_ALERT \"tsk->{mm,active_mm}->pgd = %08lx\\n\",\n\t\t(tsk->mm ? (unsigned long) tsk->mm->pgd :\n\t\t \t(unsigned long) tsk->active_mm->pgd));\n\tdie_if_kernel(\"Oops\", regs);\n}\n\nasmlinkage int lookup_fault(unsigned long pc, unsigned long ret_pc, \n\t\t\t    unsigned long address)\n{\n\tstruct pt_regs regs;\n\tunsigned long g2;\n\tunsigned int insn;\n\tint i;\n\t\n\ti = search_extables_range(ret_pc, &g2);\n\tswitch (i) {\n\tcase 3:\n\t\t/* load & store will be handled by fixup */\n\t\treturn 3;\n\n\tcase 1:\n\t\t/* store will be handled by fixup, load will bump out */\n\t\t/* for _to_ macros */\n\t\tinsn = *((unsigned int *) pc);\n\t\tif ((insn >> 21) & 1)\n\t\t\treturn 1;\n\t\tbreak;\n\n\tcase 2:\n\t\t/* load will be handled by fixup, store will bump out */\n\t\t/* for _from_ macros */\n\t\tinsn = *((unsigned int *) pc);\n\t\tif (!((insn >> 21) & 1) || ((insn>>19)&0x3f) == 15)\n\t\t\treturn 2; \n\t\tbreak; \n\n\tdefault:\n\t\tbreak;\n\t}\n\n\tmemset(&regs, 0, sizeof (regs));\n\tregs.pc = pc;\n\tregs.npc = pc + 4;\n\t__asm__ __volatile__(\n\t\t\"rd %%psr, %0\\n\\t\"\n\t\t\"nop\\n\\t\"\n\t\t\"nop\\n\\t\"\n\t\t\"nop\\n\" : \"=r\" (regs.psr));\n\tunhandled_fault(address, current, &regs);\n\n\t/* Not reached */\n\treturn 0;\n}\n\nstatic inline void\nshow_signal_msg(struct pt_regs *regs, int sig, int code,\n\t\tunsigned long address, struct task_struct *tsk)\n{\n\tif (!unhandled_signal(tsk, sig))\n\t\treturn;\n\n\tif (!printk_ratelimit())\n\t\treturn;\n\n\tprintk(\"%s%s[%d]: segfault at %lx ip %p (rpc %p) sp %p error %x\",\n\t       task_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,\n\t       tsk->comm, task_pid_nr(tsk), address,\n\t       (void *)regs->pc, (void *)regs->u_regs[UREG_I7],\n\t       (void *)regs->u_regs[UREG_FP], code);\n\n\tprint_vma_addr(KERN_CONT \" in \", regs->pc);\n\n\tprintk(KERN_CONT \"\\n\");\n}\n\nstatic void __do_fault_siginfo(int code, int sig, struct pt_regs *regs,\n\t\t\t       unsigned long addr)\n{\n\tsiginfo_t info;\n\n\tinfo.si_signo = sig;\n\tinfo.si_code = code;\n\tinfo.si_errno = 0;\n\tinfo.si_addr = (void __user *) addr;\n\tinfo.si_trapno = 0;\n\n\tif (unlikely(show_unhandled_signals))\n\t\tshow_signal_msg(regs, sig, info.si_code,\n\t\t\t\taddr, current);\n\n\tforce_sig_info (sig, &info, current);\n}\n\nextern unsigned long safe_compute_effective_address(struct pt_regs *,\n\t\t\t\t\t\t    unsigned int);\n\nstatic unsigned long compute_si_addr(struct pt_regs *regs, int text_fault)\n{\n\tunsigned int insn;\n\n\tif (text_fault)\n\t\treturn regs->pc;\n\n\tif (regs->psr & PSR_PS) {\n\t\tinsn = *(unsigned int *) regs->pc;\n\t} else {\n\t\t__get_user(insn, (unsigned int *) regs->pc);\n\t}\n\n\treturn safe_compute_effective_address(regs, insn);\n}\n\nstatic noinline void do_fault_siginfo(int code, int sig, struct pt_regs *regs,\n\t\t\t\t      int text_fault)\n{\n\tunsigned long addr = compute_si_addr(regs, text_fault);\n\n\t__do_fault_siginfo(code, sig, regs, addr);\n}\n\nasmlinkage void do_sparc_fault(struct pt_regs *regs, int text_fault, int write,\n\t\t\t       unsigned long address)\n{\n\tstruct vm_area_struct *vma;\n\tstruct task_struct *tsk = current;\n\tstruct mm_struct *mm = tsk->mm;\n\tunsigned int fixup;\n\tunsigned long g2;\n\tint from_user = !(regs->psr & PSR_PS);\n\tint fault, code;\n\n\tif(text_fault)\n\t\taddress = regs->pc;\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t */\n\tcode = SEGV_MAPERR;\n\tif (!ARCH_SUN4C && address >= TASK_SIZE)\n\t\tgoto vmalloc_fault;\n\n\t/*\n\t * If we're in an interrupt or have no user\n\t * context, we must not take the fault..\n\t */\n        if (in_atomic() || !mm)\n                goto no_context;\n\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);\n\n\tdown_read(&mm->mmap_sem);\n\n\t/*\n\t * The kernel referencing a bad kernel pointer can lock up\n\t * a sun4c machine completely, so we must attempt recovery.\n\t */\n\tif(!from_user && address >= PAGE_OFFSET)\n\t\tgoto bad_area;\n\n\tvma = find_vma(mm, address);\n\tif(!vma)\n\t\tgoto bad_area;\n\tif(vma->vm_start <= address)\n\t\tgoto good_area;\n\tif(!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif(expand_stack(vma, address))\n\t\tgoto bad_area;\n\t/*\n\t * Ok, we have a good vm_area for this memory access, so\n\t * we can handle it..\n\t */\ngood_area:\n\tcode = SEGV_ACCERR;\n\tif(write) {\n\t\tif(!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t} else {\n\t\t/* Allow reads even for write-only mappings */\n\t\tif(!(vma->vm_flags & (VM_READ | VM_EXEC)))\n\t\t\tgoto bad_area;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, write ? FAULT_FLAG_WRITE : 0);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR) {\n\t\tcurrent->maj_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,\n\t\t\t      regs, address);\n\t} else {\n\t\tcurrent->min_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,\n\t\t\t      regs, address);\n\t}\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n\t/*\n\t * Something tried to access memory that isn't in our memory map..\n\t * Fix it, but check if it's kernel or user first..\n\t */\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (from_user) {\n\t\tdo_fault_siginfo(code, SIGSEGV, regs, text_fault);\n\t\treturn;\n\t}\n\n\t/* Is this in ex_table? */\nno_context:\n\tg2 = regs->u_regs[UREG_G2];\n\tif (!from_user) {\n\t\tfixup = search_extables_range(regs->pc, &g2);\n\t\tif (fixup > 10) { /* Values below are reserved for other things */\n\t\t\textern const unsigned __memset_start[];\n\t\t\textern const unsigned __memset_end[];\n\t\t\textern const unsigned __csum_partial_copy_start[];\n\t\t\textern const unsigned __csum_partial_copy_end[];\n\n#ifdef DEBUG_EXCEPTIONS\n\t\t\tprintk(\"Exception: PC<%08lx> faddr<%08lx>\\n\", regs->pc, address);\n\t\t\tprintk(\"EX_TABLE: insn<%08lx> fixup<%08x> g2<%08lx>\\n\",\n\t\t\t\tregs->pc, fixup, g2);\n#endif\n\t\t\tif ((regs->pc >= (unsigned long)__memset_start &&\n\t\t\t     regs->pc < (unsigned long)__memset_end) ||\n\t\t\t    (regs->pc >= (unsigned long)__csum_partial_copy_start &&\n\t\t\t     regs->pc < (unsigned long)__csum_partial_copy_end)) {\n\t\t\t        regs->u_regs[UREG_I4] = address;\n\t\t\t\tregs->u_regs[UREG_I5] = regs->pc;\n\t\t\t}\n\t\t\tregs->u_regs[UREG_G2] = g2;\n\t\t\tregs->pc = fixup;\n\t\t\tregs->npc = regs->pc + 4;\n\t\t\treturn;\n\t\t}\n\t}\n\t\n\tunhandled_fault (address, tsk, regs);\n\tdo_exit(SIGKILL);\n\n/*\n * We ran out of memory, or some other thing happened to us that made\n * us unable to handle the page fault gracefully.\n */\nout_of_memory:\n\tup_read(&mm->mmap_sem);\n\tif (from_user) {\n\t\tpagefault_out_of_memory();\n\t\treturn;\n\t}\n\tgoto no_context;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\tdo_fault_siginfo(BUS_ADRERR, SIGBUS, regs, text_fault);\n\tif (!from_user)\n\t\tgoto no_context;\n\nvmalloc_fault:\n\t{\n\t\t/*\n\t\t * Synchronize this task's top level page-table\n\t\t * with the 'reference' page table.\n\t\t */\n\t\tint offset = pgd_index(address);\n\t\tpgd_t *pgd, *pgd_k;\n\t\tpmd_t *pmd, *pmd_k;\n\n\t\tpgd = tsk->active_mm->pgd + offset;\n\t\tpgd_k = init_mm.pgd + offset;\n\n\t\tif (!pgd_present(*pgd)) {\n\t\t\tif (!pgd_present(*pgd_k))\n\t\t\t\tgoto bad_area_nosemaphore;\n\t\t\tpgd_val(*pgd) = pgd_val(*pgd_k);\n\t\t\treturn;\n\t\t}\n\n\t\tpmd = pmd_offset(pgd, address);\n\t\tpmd_k = pmd_offset(pgd_k, address);\n\n\t\tif (pmd_present(*pmd) || !pmd_present(*pmd_k))\n\t\t\tgoto bad_area_nosemaphore;\n\t\t*pmd = *pmd_k;\n\t\treturn;\n\t}\n}\n\nasmlinkage void do_sun4c_fault(struct pt_regs *regs, int text_fault, int write,\n\t\t\t       unsigned long address)\n{\n\textern void sun4c_update_mmu_cache(struct vm_area_struct *,\n\t\t\t\t\t   unsigned long,pte_t *);\n\textern pte_t *sun4c_pte_offset_kernel(pmd_t *,unsigned long);\n\tstruct task_struct *tsk = current;\n\tstruct mm_struct *mm = tsk->mm;\n\tpgd_t *pgdp;\n\tpte_t *ptep;\n\n\tif (text_fault) {\n\t\taddress = regs->pc;\n\t} else if (!write &&\n\t\t   !(regs->psr & PSR_PS)) {\n\t\tunsigned int insn, __user *ip;\n\n\t\tip = (unsigned int __user *)regs->pc;\n\t\tif (!get_user(insn, ip)) {\n\t\t\tif ((insn & 0xc1680000) == 0xc0680000)\n\t\t\t\twrite = 1;\n\t\t}\n\t}\n\n\tif (!mm) {\n\t\t/* We are oopsing. */\n\t\tdo_sparc_fault(regs, text_fault, write, address);\n\t\tBUG();\t/* P3 Oops already, you bitch */\n\t}\n\n\tpgdp = pgd_offset(mm, address);\n\tptep = sun4c_pte_offset_kernel((pmd_t *) pgdp, address);\n\n\tif (pgd_val(*pgdp)) {\n\t    if (write) {\n\t\tif ((pte_val(*ptep) & (_SUN4C_PAGE_WRITE|_SUN4C_PAGE_PRESENT))\n\t\t\t\t   == (_SUN4C_PAGE_WRITE|_SUN4C_PAGE_PRESENT)) {\n\t\t\tunsigned long flags;\n\n\t\t\t*ptep = __pte(pte_val(*ptep) | _SUN4C_PAGE_ACCESSED |\n\t\t\t\t      _SUN4C_PAGE_MODIFIED |\n\t\t\t\t      _SUN4C_PAGE_VALID |\n\t\t\t\t      _SUN4C_PAGE_DIRTY);\n\n\t\t\tlocal_irq_save(flags);\n\t\t\tif (sun4c_get_segmap(address) != invalid_segment) {\n\t\t\t\tsun4c_put_pte(address, pte_val(*ptep));\n\t\t\t\tlocal_irq_restore(flags);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tlocal_irq_restore(flags);\n\t\t}\n\t    } else {\n\t\tif ((pte_val(*ptep) & (_SUN4C_PAGE_READ|_SUN4C_PAGE_PRESENT))\n\t\t\t\t   == (_SUN4C_PAGE_READ|_SUN4C_PAGE_PRESENT)) {\n\t\t\tunsigned long flags;\n\n\t\t\t*ptep = __pte(pte_val(*ptep) | _SUN4C_PAGE_ACCESSED |\n\t\t\t\t      _SUN4C_PAGE_VALID);\n\n\t\t\tlocal_irq_save(flags);\n\t\t\tif (sun4c_get_segmap(address) != invalid_segment) {\n\t\t\t\tsun4c_put_pte(address, pte_val(*ptep));\n\t\t\t\tlocal_irq_restore(flags);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tlocal_irq_restore(flags);\n\t\t}\n\t    }\n\t}\n\n\t/* This conditional is 'interesting'. */\n\tif (pgd_val(*pgdp) && !(write && !(pte_val(*ptep) & _SUN4C_PAGE_WRITE))\n\t    && (pte_val(*ptep) & _SUN4C_PAGE_VALID))\n\t\t/* Note: It is safe to not grab the MMAP semaphore here because\n\t\t *       we know that update_mmu_cache() will not sleep for\n\t\t *       any reason (at least not in the current implementation)\n\t\t *       and therefore there is no danger of another thread getting\n\t\t *       on the CPU and doing a shrink_mmap() on this vma.\n\t\t */\n\t\tsun4c_update_mmu_cache (find_vma(current->mm, address), address,\n\t\t\t\t\tptep);\n\telse\n\t\tdo_sparc_fault(regs, text_fault, write, address);\n}\n\n/* This always deals with user addresses. */\nstatic void force_user_fault(unsigned long address, int write)\n{\n\tstruct vm_area_struct *vma;\n\tstruct task_struct *tsk = current;\n\tstruct mm_struct *mm = tsk->mm;\n\tint code;\n\n\tcode = SEGV_MAPERR;\n\n\tdown_read(&mm->mmap_sem);\n\tvma = find_vma(mm, address);\n\tif(!vma)\n\t\tgoto bad_area;\n\tif(vma->vm_start <= address)\n\t\tgoto good_area;\n\tif(!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif(expand_stack(vma, address))\n\t\tgoto bad_area;\ngood_area:\n\tcode = SEGV_ACCERR;\n\tif(write) {\n\t\tif(!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t} else {\n\t\tif(!(vma->vm_flags & (VM_READ | VM_EXEC)))\n\t\t\tgoto bad_area;\n\t}\n\tswitch (handle_mm_fault(mm, vma, address, write ? FAULT_FLAG_WRITE : 0)) {\n\tcase VM_FAULT_SIGBUS:\n\tcase VM_FAULT_OOM:\n\t\tgoto do_sigbus;\n\t}\n\tup_read(&mm->mmap_sem);\n\treturn;\nbad_area:\n\tup_read(&mm->mmap_sem);\n\t__do_fault_siginfo(code, SIGSEGV, tsk->thread.kregs, address);\n\treturn;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\t__do_fault_siginfo(BUS_ADRERR, SIGBUS, tsk->thread.kregs, address);\n}\n\nstatic void check_stack_aligned(unsigned long sp)\n{\n\tif (sp & 0x7UL)\n\t\tforce_sig(SIGILL, current);\n}\n\nvoid window_overflow_fault(void)\n{\n\tunsigned long sp;\n\n\tsp = current_thread_info()->rwbuf_stkptrs[0];\n\tif(((sp + 0x38) & PAGE_MASK) != (sp & PAGE_MASK))\n\t\tforce_user_fault(sp + 0x38, 1);\n\tforce_user_fault(sp, 1);\n\n\tcheck_stack_aligned(sp);\n}\n\nvoid window_underflow_fault(unsigned long sp)\n{\n\tif(((sp + 0x38) & PAGE_MASK) != (sp & PAGE_MASK))\n\t\tforce_user_fault(sp + 0x38, 0);\n\tforce_user_fault(sp, 0);\n\n\tcheck_stack_aligned(sp);\n}\n\nvoid window_ret_fault(struct pt_regs *regs)\n{\n\tunsigned long sp;\n\n\tsp = regs->u_regs[UREG_FP];\n\tif(((sp + 0x38) & PAGE_MASK) != (sp & PAGE_MASK))\n\t\tforce_user_fault(sp + 0x38, 0);\n\tforce_user_fault(sp, 0);\n\n\tcheck_stack_aligned(sp);\n}\n", "/*\n * arch/sparc64/mm/fault.c: Page fault handlers for the 64-bit Sparc.\n *\n * Copyright (C) 1996, 2008 David S. Miller (davem@davemloft.net)\n * Copyright (C) 1997, 1999 Jakub Jelinek (jj@ultra.linux.cz)\n */\n\n#include <asm/head.h>\n\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/sched.h>\n#include <linux/ptrace.h>\n#include <linux/mman.h>\n#include <linux/signal.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/perf_event.h>\n#include <linux/interrupt.h>\n#include <linux/kprobes.h>\n#include <linux/kdebug.h>\n#include <linux/percpu.h>\n\n#include <asm/page.h>\n#include <asm/pgtable.h>\n#include <asm/openprom.h>\n#include <asm/oplib.h>\n#include <asm/uaccess.h>\n#include <asm/asi.h>\n#include <asm/lsu.h>\n#include <asm/sections.h>\n#include <asm/mmu_context.h>\n\nint show_unhandled_signals = 1;\n\nstatic inline __kprobes int notify_page_fault(struct pt_regs *regs)\n{\n\tint ret = 0;\n\n\t/* kprobe_running() needs smp_processor_id() */\n\tif (kprobes_built_in() && !user_mode(regs)) {\n\t\tpreempt_disable();\n\t\tif (kprobe_running() && kprobe_fault_handler(regs, 0))\n\t\t\tret = 1;\n\t\tpreempt_enable();\n\t}\n\treturn ret;\n}\n\nstatic void __kprobes unhandled_fault(unsigned long address,\n\t\t\t\t      struct task_struct *tsk,\n\t\t\t\t      struct pt_regs *regs)\n{\n\tif ((unsigned long) address < PAGE_SIZE) {\n\t\tprintk(KERN_ALERT \"Unable to handle kernel NULL \"\n\t\t       \"pointer dereference\\n\");\n\t} else {\n\t\tprintk(KERN_ALERT \"Unable to handle kernel paging request \"\n\t\t       \"at virtual address %016lx\\n\", (unsigned long)address);\n\t}\n\tprintk(KERN_ALERT \"tsk->{mm,active_mm}->context = %016lx\\n\",\n\t       (tsk->mm ?\n\t\tCTX_HWBITS(tsk->mm->context) :\n\t\tCTX_HWBITS(tsk->active_mm->context)));\n\tprintk(KERN_ALERT \"tsk->{mm,active_mm}->pgd = %016lx\\n\",\n\t       (tsk->mm ? (unsigned long) tsk->mm->pgd :\n\t\t          (unsigned long) tsk->active_mm->pgd));\n\tdie_if_kernel(\"Oops\", regs);\n}\n\nstatic void __kprobes bad_kernel_pc(struct pt_regs *regs, unsigned long vaddr)\n{\n\tprintk(KERN_CRIT \"OOPS: Bogus kernel PC [%016lx] in fault handler\\n\",\n\t       regs->tpc);\n\tprintk(KERN_CRIT \"OOPS: RPC [%016lx]\\n\", regs->u_regs[15]);\n\tprintk(\"OOPS: RPC <%pS>\\n\", (void *) regs->u_regs[15]);\n\tprintk(KERN_CRIT \"OOPS: Fault was to vaddr[%lx]\\n\", vaddr);\n\tdump_stack();\n\tunhandled_fault(regs->tpc, current, regs);\n}\n\n/*\n * We now make sure that mmap_sem is held in all paths that call \n * this. Additionally, to prevent kswapd from ripping ptes from\n * under us, raise interrupts around the time that we look at the\n * pte, kswapd will have to wait to get his smp ipi response from\n * us. vmtruncate likewise. This saves us having to get pte lock.\n */\nstatic unsigned int get_user_insn(unsigned long tpc)\n{\n\tpgd_t *pgdp = pgd_offset(current->mm, tpc);\n\tpud_t *pudp;\n\tpmd_t *pmdp;\n\tpte_t *ptep, pte;\n\tunsigned long pa;\n\tu32 insn = 0;\n\tunsigned long pstate;\n\n\tif (pgd_none(*pgdp))\n\t\tgoto outret;\n\tpudp = pud_offset(pgdp, tpc);\n\tif (pud_none(*pudp))\n\t\tgoto outret;\n\tpmdp = pmd_offset(pudp, tpc);\n\tif (pmd_none(*pmdp))\n\t\tgoto outret;\n\n\t/* This disables preemption for us as well. */\n\t__asm__ __volatile__(\"rdpr %%pstate, %0\" : \"=r\" (pstate));\n\t__asm__ __volatile__(\"wrpr %0, %1, %%pstate\"\n\t\t\t\t: : \"r\" (pstate), \"i\" (PSTATE_IE));\n\tptep = pte_offset_map(pmdp, tpc);\n\tpte = *ptep;\n\tif (!pte_present(pte))\n\t\tgoto out;\n\n\tpa  = (pte_pfn(pte) << PAGE_SHIFT);\n\tpa += (tpc & ~PAGE_MASK);\n\n\t/* Use phys bypass so we don't pollute dtlb/dcache. */\n\t__asm__ __volatile__(\"lduwa [%1] %2, %0\"\n\t\t\t     : \"=r\" (insn)\n\t\t\t     : \"r\" (pa), \"i\" (ASI_PHYS_USE_EC));\n\nout:\n\tpte_unmap(ptep);\n\t__asm__ __volatile__(\"wrpr %0, 0x0, %%pstate\" : : \"r\" (pstate));\noutret:\n\treturn insn;\n}\n\nstatic inline void\nshow_signal_msg(struct pt_regs *regs, int sig, int code,\n\t\tunsigned long address, struct task_struct *tsk)\n{\n\tif (!unhandled_signal(tsk, sig))\n\t\treturn;\n\n\tif (!printk_ratelimit())\n\t\treturn;\n\n\tprintk(\"%s%s[%d]: segfault at %lx ip %p (rpc %p) sp %p error %x\",\n\t       task_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,\n\t       tsk->comm, task_pid_nr(tsk), address,\n\t       (void *)regs->tpc, (void *)regs->u_regs[UREG_I7],\n\t       (void *)regs->u_regs[UREG_FP], code);\n\n\tprint_vma_addr(KERN_CONT \" in \", regs->tpc);\n\n\tprintk(KERN_CONT \"\\n\");\n}\n\nextern unsigned long compute_effective_address(struct pt_regs *, unsigned int, unsigned int);\n\nstatic void do_fault_siginfo(int code, int sig, struct pt_regs *regs,\n\t\t\t     unsigned int insn, int fault_code)\n{\n\tunsigned long addr;\n\tsiginfo_t info;\n\n\tinfo.si_code = code;\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\tif (fault_code & FAULT_CODE_ITLB)\n\t\taddr = regs->tpc;\n\telse\n\t\taddr = compute_effective_address(regs, insn, 0);\n\tinfo.si_addr = (void __user *) addr;\n\tinfo.si_trapno = 0;\n\n\tif (unlikely(show_unhandled_signals))\n\t\tshow_signal_msg(regs, sig, code, addr, current);\n\n\tforce_sig_info(sig, &info, current);\n}\n\nextern int handle_ldf_stq(u32, struct pt_regs *);\nextern int handle_ld_nf(u32, struct pt_regs *);\n\nstatic unsigned int get_fault_insn(struct pt_regs *regs, unsigned int insn)\n{\n\tif (!insn) {\n\t\tif (!regs->tpc || (regs->tpc & 0x3))\n\t\t\treturn 0;\n\t\tif (regs->tstate & TSTATE_PRIV) {\n\t\t\tinsn = *(unsigned int *) regs->tpc;\n\t\t} else {\n\t\t\tinsn = get_user_insn(regs->tpc);\n\t\t}\n\t}\n\treturn insn;\n}\n\nstatic void __kprobes do_kernel_fault(struct pt_regs *regs, int si_code,\n\t\t\t\t      int fault_code, unsigned int insn,\n\t\t\t\t      unsigned long address)\n{\n\tunsigned char asi = ASI_P;\n \n\tif ((!insn) && (regs->tstate & TSTATE_PRIV))\n\t\tgoto cannot_handle;\n\n\t/* If user insn could be read (thus insn is zero), that\n\t * is fine.  We will just gun down the process with a signal\n\t * in that case.\n\t */\n\n\tif (!(fault_code & (FAULT_CODE_WRITE|FAULT_CODE_ITLB)) &&\n\t    (insn & 0xc0800000) == 0xc0800000) {\n\t\tif (insn & 0x2000)\n\t\t\tasi = (regs->tstate >> 24);\n\t\telse\n\t\t\tasi = (insn >> 5);\n\t\tif ((asi & 0xf2) == 0x82) {\n\t\t\tif (insn & 0x1000000) {\n\t\t\t\thandle_ldf_stq(insn, regs);\n\t\t\t} else {\n\t\t\t\t/* This was a non-faulting load. Just clear the\n\t\t\t\t * destination register(s) and continue with the next\n\t\t\t\t * instruction. -jj\n\t\t\t\t */\n\t\t\t\thandle_ld_nf(insn, regs);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t}\n\t\t\n\t/* Is this in ex_table? */\n\tif (regs->tstate & TSTATE_PRIV) {\n\t\tconst struct exception_table_entry *entry;\n\n\t\tentry = search_exception_tables(regs->tpc);\n\t\tif (entry) {\n\t\t\tregs->tpc = entry->fixup;\n\t\t\tregs->tnpc = regs->tpc + 4;\n\t\t\treturn;\n\t\t}\n\t} else {\n\t\t/* The si_code was set to make clear whether\n\t\t * this was a SEGV_MAPERR or SEGV_ACCERR fault.\n\t\t */\n\t\tdo_fault_siginfo(si_code, SIGSEGV, regs, insn, fault_code);\n\t\treturn;\n\t}\n\ncannot_handle:\n\tunhandled_fault (address, current, regs);\n}\n\nstatic void noinline __kprobes bogus_32bit_fault_tpc(struct pt_regs *regs)\n{\n\tstatic int times;\n\n\tif (times++ < 10)\n\t\tprintk(KERN_ERR \"FAULT[%s:%d]: 32-bit process reports \"\n\t\t       \"64-bit TPC [%lx]\\n\",\n\t\t       current->comm, current->pid,\n\t\t       regs->tpc);\n\tshow_regs(regs);\n}\n\nstatic void noinline __kprobes bogus_32bit_fault_address(struct pt_regs *regs,\n\t\t\t\t\t\t\t unsigned long addr)\n{\n\tstatic int times;\n\n\tif (times++ < 10)\n\t\tprintk(KERN_ERR \"FAULT[%s:%d]: 32-bit process \"\n\t\t       \"reports 64-bit fault address [%lx]\\n\",\n\t\t       current->comm, current->pid, addr);\n\tshow_regs(regs);\n}\n\nasmlinkage void __kprobes do_sparc64_fault(struct pt_regs *regs)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned int insn = 0;\n\tint si_code, fault_code, fault;\n\tunsigned long address, mm_rss;\n\n\tfault_code = get_thread_fault_code();\n\n\tif (notify_page_fault(regs))\n\t\treturn;\n\n\tsi_code = SEGV_MAPERR;\n\taddress = current_thread_info()->fault_address;\n\n\tif ((fault_code & FAULT_CODE_ITLB) &&\n\t    (fault_code & FAULT_CODE_DTLB))\n\t\tBUG();\n\n\tif (test_thread_flag(TIF_32BIT)) {\n\t\tif (!(regs->tstate & TSTATE_PRIV)) {\n\t\t\tif (unlikely((regs->tpc >> 32) != 0)) {\n\t\t\t\tbogus_32bit_fault_tpc(regs);\n\t\t\t\tgoto intr_or_no_mm;\n\t\t\t}\n\t\t}\n\t\tif (unlikely((address >> 32) != 0)) {\n\t\t\tbogus_32bit_fault_address(regs, address);\n\t\t\tgoto intr_or_no_mm;\n\t\t}\n\t}\n\n\tif (regs->tstate & TSTATE_PRIV) {\n\t\tunsigned long tpc = regs->tpc;\n\n\t\t/* Sanity check the PC. */\n\t\tif ((tpc >= KERNBASE && tpc < (unsigned long) __init_end) ||\n\t\t    (tpc >= MODULES_VADDR && tpc < MODULES_END)) {\n\t\t\t/* Valid, no problems... */\n\t\t} else {\n\t\t\tbad_kernel_pc(regs, address);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/*\n\t * If we're in an interrupt or have no user\n\t * context, we must not take the fault..\n\t */\n\tif (in_atomic() || !mm)\n\t\tgoto intr_or_no_mm;\n\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);\n\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tif ((regs->tstate & TSTATE_PRIV) &&\n\t\t    !search_exception_tables(regs->tpc)) {\n\t\t\tinsn = get_fault_insn(regs, insn);\n\t\t\tgoto handle_kernel_fault;\n\t\t}\n\t\tdown_read(&mm->mmap_sem);\n\t}\n\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\n\t/* Pure DTLB misses do not tell us whether the fault causing\n\t * load/store/atomic was a write or not, it only says that there\n\t * was no match.  So in such a case we (carefully) read the\n\t * instruction to try and figure this out.  It's an optimization\n\t * so it's ok if we can't do this.\n\t *\n\t * Special hack, window spill/fill knows the exact fault type.\n\t */\n\tif (((fault_code &\n\t      (FAULT_CODE_DTLB | FAULT_CODE_WRITE | FAULT_CODE_WINFIXUP)) == FAULT_CODE_DTLB) &&\n\t    (vma->vm_flags & VM_WRITE) != 0) {\n\t\tinsn = get_fault_insn(regs, 0);\n\t\tif (!insn)\n\t\t\tgoto continue_fault;\n\t\t/* All loads, stores and atomics have bits 30 and 31 both set\n\t\t * in the instruction.  Bit 21 is set in all stores, but we\n\t\t * have to avoid prefetches which also have bit 21 set.\n\t\t */\n\t\tif ((insn & 0xc0200000) == 0xc0200000 &&\n\t\t    (insn & 0x01780000) != 0x01680000) {\n\t\t\t/* Don't bother updating thread struct value,\n\t\t\t * because update_mmu_cache only cares which tlb\n\t\t\t * the access came from.\n\t\t\t */\n\t\t\tfault_code |= FAULT_CODE_WRITE;\n\t\t}\n\t}\ncontinue_fault:\n\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif (!(fault_code & FAULT_CODE_WRITE)) {\n\t\t/* Non-faulting loads shouldn't expand stack. */\n\t\tinsn = get_fault_insn(regs, insn);\n\t\tif ((insn & 0xc0800000) == 0xc0800000) {\n\t\t\tunsigned char asi;\n\n\t\t\tif (insn & 0x2000)\n\t\t\t\tasi = (regs->tstate >> 24);\n\t\t\telse\n\t\t\t\tasi = (insn >> 5);\n\t\t\tif ((asi & 0xf2) == 0x82)\n\t\t\t\tgoto bad_area;\n\t\t}\n\t}\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n\t/*\n\t * Ok, we have a good vm_area for this memory access, so\n\t * we can handle it..\n\t */\ngood_area:\n\tsi_code = SEGV_ACCERR;\n\n\t/* If we took a ITLB miss on a non-executable page, catch\n\t * that here.\n\t */\n\tif ((fault_code & FAULT_CODE_ITLB) && !(vma->vm_flags & VM_EXEC)) {\n\t\tBUG_ON(address != regs->tpc);\n\t\tBUG_ON(regs->tstate & TSTATE_PRIV);\n\t\tgoto bad_area;\n\t}\n\n\tif (fault_code & FAULT_CODE_WRITE) {\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\n\t\t/* Spitfire has an icache which does not snoop\n\t\t * processor stores.  Later processors do...\n\t\t */\n\t\tif (tlb_type == spitfire &&\n\t\t    (vma->vm_flags & VM_EXEC) != 0 &&\n\t\t    vma->vm_file != NULL)\n\t\t\tset_thread_fault_code(fault_code |\n\t\t\t\t\t      FAULT_CODE_BLKCOMMIT);\n\t} else {\n\t\t/* Allow reads even for write-only mappings */\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC)))\n\t\t\tgoto bad_area;\n\t}\n\n\tfault = handle_mm_fault(mm, vma, address, (fault_code & FAULT_CODE_WRITE) ? FAULT_FLAG_WRITE : 0);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR) {\n\t\tcurrent->maj_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,\n\t\t\t      regs, address);\n\t} else {\n\t\tcurrent->min_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,\n\t\t\t      regs, address);\n\t}\n\tup_read(&mm->mmap_sem);\n\n\tmm_rss = get_mm_rss(mm);\n#ifdef CONFIG_HUGETLB_PAGE\n\tmm_rss -= (mm->context.huge_pte_count * (HPAGE_SIZE / PAGE_SIZE));\n#endif\n\tif (unlikely(mm_rss >\n\t\t     mm->context.tsb_block[MM_TSB_BASE].tsb_rss_limit))\n\t\ttsb_grow(mm, MM_TSB_BASE, mm_rss);\n#ifdef CONFIG_HUGETLB_PAGE\n\tmm_rss = mm->context.huge_pte_count;\n\tif (unlikely(mm_rss >\n\t\t     mm->context.tsb_block[MM_TSB_HUGE].tsb_rss_limit))\n\t\ttsb_grow(mm, MM_TSB_HUGE, mm_rss);\n#endif\n\treturn;\n\n\t/*\n\t * Something tried to access memory that isn't in our memory map..\n\t * Fix it, but check if it's kernel or user first..\n\t */\nbad_area:\n\tinsn = get_fault_insn(regs, insn);\n\tup_read(&mm->mmap_sem);\n\nhandle_kernel_fault:\n\tdo_kernel_fault(regs, si_code, fault_code, insn, address);\n\treturn;\n\n/*\n * We ran out of memory, or some other thing happened to us that made\n * us unable to handle the page fault gracefully.\n */\nout_of_memory:\n\tinsn = get_fault_insn(regs, insn);\n\tup_read(&mm->mmap_sem);\n\tif (!(regs->tstate & TSTATE_PRIV)) {\n\t\tpagefault_out_of_memory();\n\t\treturn;\n\t}\n\tgoto handle_kernel_fault;\n\nintr_or_no_mm:\n\tinsn = get_fault_insn(regs, 0);\n\tgoto handle_kernel_fault;\n\ndo_sigbus:\n\tinsn = get_fault_insn(regs, insn);\n\tup_read(&mm->mmap_sem);\n\n\t/*\n\t * Send a sigbus, regardless of whether we were in kernel\n\t * or user mode.\n\t */\n\tdo_fault_siginfo(BUS_ADRERR, SIGBUS, regs, insn, fault_code);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (regs->tstate & TSTATE_PRIV)\n\t\tgoto handle_kernel_fault;\n}\n", "/*\n * Performance events x86 architecture code\n *\n *  Copyright (C) 2008 Thomas Gleixner <tglx@linutronix.de>\n *  Copyright (C) 2008-2009 Red Hat, Inc., Ingo Molnar\n *  Copyright (C) 2009 Jaswinder Singh Rajput\n *  Copyright (C) 2009 Advanced Micro Devices, Inc., Robert Richter\n *  Copyright (C) 2008-2009 Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>\n *  Copyright (C) 2009 Intel Corporation, <markus.t.metzger@intel.com>\n *  Copyright (C) 2009 Google, Inc., Stephane Eranian\n *\n *  For licencing details see kernel-base/COPYING\n */\n\n#include <linux/perf_event.h>\n#include <linux/capability.h>\n#include <linux/notifier.h>\n#include <linux/hardirq.h>\n#include <linux/kprobes.h>\n#include <linux/module.h>\n#include <linux/kdebug.h>\n#include <linux/sched.h>\n#include <linux/uaccess.h>\n#include <linux/slab.h>\n#include <linux/highmem.h>\n#include <linux/cpu.h>\n#include <linux/bitops.h>\n\n#include <asm/apic.h>\n#include <asm/stacktrace.h>\n#include <asm/nmi.h>\n#include <asm/compat.h>\n#include <asm/smp.h>\n#include <asm/alternative.h>\n\n#if 0\n#undef wrmsrl\n#define wrmsrl(msr, val) \t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\ttrace_printk(\"wrmsrl(%lx, %lx)\\n\", (unsigned long)(msr),\\\n\t\t\t(unsigned long)(val));\t\t\t\\\n\tnative_write_msr((msr), (u32)((u64)(val)), \t\t\\\n\t\t\t(u32)((u64)(val) >> 32));\t\t\\\n} while (0)\n#endif\n\n/*\n * best effort, GUP based copy_from_user() that assumes IRQ or NMI context\n */\nstatic unsigned long\ncopy_from_user_nmi(void *to, const void __user *from, unsigned long n)\n{\n\tunsigned long offset, addr = (unsigned long)from;\n\tunsigned long size, len = 0;\n\tstruct page *page;\n\tvoid *map;\n\tint ret;\n\n\tdo {\n\t\tret = __get_user_pages_fast(addr, 1, 0, &page);\n\t\tif (!ret)\n\t\t\tbreak;\n\n\t\toffset = addr & (PAGE_SIZE - 1);\n\t\tsize = min(PAGE_SIZE - offset, n - len);\n\n\t\tmap = kmap_atomic(page);\n\t\tmemcpy(to, map+offset, size);\n\t\tkunmap_atomic(map);\n\t\tput_page(page);\n\n\t\tlen  += size;\n\t\tto   += size;\n\t\taddr += size;\n\n\t} while (len < n);\n\n\treturn len;\n}\n\nstruct event_constraint {\n\tunion {\n\t\tunsigned long\tidxmsk[BITS_TO_LONGS(X86_PMC_IDX_MAX)];\n\t\tu64\t\tidxmsk64;\n\t};\n\tu64\tcode;\n\tu64\tcmask;\n\tint\tweight;\n};\n\nstruct amd_nb {\n\tint nb_id;  /* NorthBridge id */\n\tint refcnt; /* reference count */\n\tstruct perf_event *owners[X86_PMC_IDX_MAX];\n\tstruct event_constraint event_constraints[X86_PMC_IDX_MAX];\n};\n\nstruct intel_percore;\n\n#define MAX_LBR_ENTRIES\t\t16\n\nstruct cpu_hw_events {\n\t/*\n\t * Generic x86 PMC bits\n\t */\n\tstruct perf_event\t*events[X86_PMC_IDX_MAX]; /* in counter order */\n\tunsigned long\t\tactive_mask[BITS_TO_LONGS(X86_PMC_IDX_MAX)];\n\tunsigned long\t\trunning[BITS_TO_LONGS(X86_PMC_IDX_MAX)];\n\tint\t\t\tenabled;\n\n\tint\t\t\tn_events;\n\tint\t\t\tn_added;\n\tint\t\t\tn_txn;\n\tint\t\t\tassign[X86_PMC_IDX_MAX]; /* event to counter assignment */\n\tu64\t\t\ttags[X86_PMC_IDX_MAX];\n\tstruct perf_event\t*event_list[X86_PMC_IDX_MAX]; /* in enabled order */\n\n\tunsigned int\t\tgroup_flag;\n\n\t/*\n\t * Intel DebugStore bits\n\t */\n\tstruct debug_store\t*ds;\n\tu64\t\t\tpebs_enabled;\n\n\t/*\n\t * Intel LBR bits\n\t */\n\tint\t\t\t\tlbr_users;\n\tvoid\t\t\t\t*lbr_context;\n\tstruct perf_branch_stack\tlbr_stack;\n\tstruct perf_branch_entry\tlbr_entries[MAX_LBR_ENTRIES];\n\n\t/*\n\t * Intel percore register state.\n\t * Coordinate shared resources between HT threads.\n\t */\n\tint\t\t\t\tpercore_used; /* Used by this CPU? */\n\tstruct intel_percore\t\t*per_core;\n\n\t/*\n\t * AMD specific bits\n\t */\n\tstruct amd_nb\t\t*amd_nb;\n};\n\n#define __EVENT_CONSTRAINT(c, n, m, w) {\\\n\t{ .idxmsk64 = (n) },\t\t\\\n\t.code = (c),\t\t\t\\\n\t.cmask = (m),\t\t\t\\\n\t.weight = (w),\t\t\t\\\n}\n\n#define EVENT_CONSTRAINT(c, n, m)\t\\\n\t__EVENT_CONSTRAINT(c, n, m, HWEIGHT(n))\n\n/*\n * Constraint on the Event code.\n */\n#define INTEL_EVENT_CONSTRAINT(c, n)\t\\\n\tEVENT_CONSTRAINT(c, n, ARCH_PERFMON_EVENTSEL_EVENT)\n\n/*\n * Constraint on the Event code + UMask + fixed-mask\n *\n * filter mask to validate fixed counter events.\n * the following filters disqualify for fixed counters:\n *  - inv\n *  - edge\n *  - cnt-mask\n *  The other filters are supported by fixed counters.\n *  The any-thread option is supported starting with v3.\n */\n#define FIXED_EVENT_CONSTRAINT(c, n)\t\\\n\tEVENT_CONSTRAINT(c, (1ULL << (32+n)), X86_RAW_EVENT_MASK)\n\n/*\n * Constraint on the Event code + UMask\n */\n#define INTEL_UEVENT_CONSTRAINT(c, n)\t\\\n\tEVENT_CONSTRAINT(c, n, INTEL_ARCH_EVENT_MASK)\n\n#define EVENT_CONSTRAINT_END\t\t\\\n\tEVENT_CONSTRAINT(0, 0, 0)\n\n#define for_each_event_constraint(e, c)\t\\\n\tfor ((e) = (c); (e)->weight; (e)++)\n\n/*\n * Extra registers for specific events.\n * Some events need large masks and require external MSRs.\n * Define a mapping to these extra registers.\n */\nstruct extra_reg {\n\tunsigned int\t\tevent;\n\tunsigned int\t\tmsr;\n\tu64\t\t\tconfig_mask;\n\tu64\t\t\tvalid_mask;\n};\n\n#define EVENT_EXTRA_REG(e, ms, m, vm) {\t\\\n\t.event = (e),\t\t\\\n\t.msr = (ms),\t\t\\\n\t.config_mask = (m),\t\\\n\t.valid_mask = (vm),\t\\\n\t}\n#define INTEL_EVENT_EXTRA_REG(event, msr, vm)\t\\\n\tEVENT_EXTRA_REG(event, msr, ARCH_PERFMON_EVENTSEL_EVENT, vm)\n#define EVENT_EXTRA_END EVENT_EXTRA_REG(0, 0, 0, 0)\n\nunion perf_capabilities {\n\tstruct {\n\t\tu64\tlbr_format    : 6;\n\t\tu64\tpebs_trap     : 1;\n\t\tu64\tpebs_arch_reg : 1;\n\t\tu64\tpebs_format   : 4;\n\t\tu64\tsmm_freeze    : 1;\n\t};\n\tu64\tcapabilities;\n};\n\n/*\n * struct x86_pmu - generic x86 pmu\n */\nstruct x86_pmu {\n\t/*\n\t * Generic x86 PMC bits\n\t */\n\tconst char\t*name;\n\tint\t\tversion;\n\tint\t\t(*handle_irq)(struct pt_regs *);\n\tvoid\t\t(*disable_all)(void);\n\tvoid\t\t(*enable_all)(int added);\n\tvoid\t\t(*enable)(struct perf_event *);\n\tvoid\t\t(*disable)(struct perf_event *);\n\tvoid\t\t(*hw_watchdog_set_attr)(struct perf_event_attr *attr);\n\tint\t\t(*hw_config)(struct perf_event *event);\n\tint\t\t(*schedule_events)(struct cpu_hw_events *cpuc, int n, int *assign);\n\tunsigned\teventsel;\n\tunsigned\tperfctr;\n\tu64\t\t(*event_map)(int);\n\tint\t\tmax_events;\n\tint\t\tnum_counters;\n\tint\t\tnum_counters_fixed;\n\tint\t\tcntval_bits;\n\tu64\t\tcntval_mask;\n\tint\t\tapic;\n\tu64\t\tmax_period;\n\tstruct event_constraint *\n\t\t\t(*get_event_constraints)(struct cpu_hw_events *cpuc,\n\t\t\t\t\t\t struct perf_event *event);\n\n\tvoid\t\t(*put_event_constraints)(struct cpu_hw_events *cpuc,\n\t\t\t\t\t\t struct perf_event *event);\n\tstruct event_constraint *event_constraints;\n\tstruct event_constraint *percore_constraints;\n\tvoid\t\t(*quirks)(void);\n\tint\t\tperfctr_second_write;\n\n\tint\t\t(*cpu_prepare)(int cpu);\n\tvoid\t\t(*cpu_starting)(int cpu);\n\tvoid\t\t(*cpu_dying)(int cpu);\n\tvoid\t\t(*cpu_dead)(int cpu);\n\n\t/*\n\t * Intel Arch Perfmon v2+\n\t */\n\tu64\t\t\tintel_ctrl;\n\tunion perf_capabilities intel_cap;\n\n\t/*\n\t * Intel DebugStore bits\n\t */\n\tint\t\tbts, pebs;\n\tint\t\tbts_active, pebs_active;\n\tint\t\tpebs_record_size;\n\tvoid\t\t(*drain_pebs)(struct pt_regs *regs);\n\tstruct event_constraint *pebs_constraints;\n\n\t/*\n\t * Intel LBR\n\t */\n\tunsigned long\tlbr_tos, lbr_from, lbr_to; /* MSR base regs       */\n\tint\t\tlbr_nr;\t\t\t   /* hardware stack size */\n\n\t/*\n\t * Extra registers for events\n\t */\n\tstruct extra_reg *extra_regs;\n};\n\nstatic struct x86_pmu x86_pmu __read_mostly;\n\nstatic DEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events) = {\n\t.enabled = 1,\n};\n\nstatic int x86_perf_event_set_period(struct perf_event *event);\n\n/*\n * Generalized hw caching related hw_event table, filled\n * in on a per model basis. A value of 0 means\n * 'not supported', -1 means 'hw_event makes no sense on\n * this CPU', any other value means the raw hw_event\n * ID.\n */\n\n#define C(x) PERF_COUNT_HW_CACHE_##x\n\nstatic u64 __read_mostly hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX];\nstatic u64 __read_mostly hw_cache_extra_regs\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX];\n\nvoid hw_nmi_watchdog_set_attr(struct perf_event_attr *wd_attr)\n{\n\tif (x86_pmu.hw_watchdog_set_attr)\n\t\tx86_pmu.hw_watchdog_set_attr(wd_attr);\n}\n\n/*\n * Propagate event elapsed time into the generic event.\n * Can only be executed on the CPU where the event is active.\n * Returns the delta events processed.\n */\nstatic u64\nx86_perf_event_update(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint shift = 64 - x86_pmu.cntval_bits;\n\tu64 prev_raw_count, new_raw_count;\n\tint idx = hwc->idx;\n\ts64 delta;\n\n\tif (idx == X86_PMC_IDX_FIXED_BTS)\n\t\treturn 0;\n\n\t/*\n\t * Careful: an NMI might modify the previous event value.\n\t *\n\t * Our tactic to handle this is to first atomically read and\n\t * exchange a new raw count - then add that new-prev delta\n\t * count to the generic event atomically:\n\t */\nagain:\n\tprev_raw_count = local64_read(&hwc->prev_count);\n\trdmsrl(hwc->event_base, new_raw_count);\n\n\tif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\n\t\t\t\t\tnew_raw_count) != prev_raw_count)\n\t\tgoto again;\n\n\t/*\n\t * Now we have the new raw value and have updated the prev\n\t * timestamp already. We can now calculate the elapsed delta\n\t * (event-)time and add that to the generic event.\n\t *\n\t * Careful, not all hw sign-extends above the physical width\n\t * of the count.\n\t */\n\tdelta = (new_raw_count << shift) - (prev_raw_count << shift);\n\tdelta >>= shift;\n\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &hwc->period_left);\n\n\treturn new_raw_count;\n}\n\nstatic inline int x86_pmu_addr_offset(int index)\n{\n\tint offset;\n\n\t/* offset = X86_FEATURE_PERFCTR_CORE ? index << 1 : index */\n\talternative_io(ASM_NOP2,\n\t\t       \"shll $1, %%eax\",\n\t\t       X86_FEATURE_PERFCTR_CORE,\n\t\t       \"=a\" (offset),\n\t\t       \"a\"  (index));\n\n\treturn offset;\n}\n\nstatic inline unsigned int x86_pmu_config_addr(int index)\n{\n\treturn x86_pmu.eventsel + x86_pmu_addr_offset(index);\n}\n\nstatic inline unsigned int x86_pmu_event_addr(int index)\n{\n\treturn x86_pmu.perfctr + x86_pmu_addr_offset(index);\n}\n\n/*\n * Find and validate any extra registers to set up.\n */\nstatic int x86_pmu_extra_regs(u64 config, struct perf_event *event)\n{\n\tstruct extra_reg *er;\n\n\tevent->hw.extra_reg = 0;\n\tevent->hw.extra_config = 0;\n\n\tif (!x86_pmu.extra_regs)\n\t\treturn 0;\n\n\tfor (er = x86_pmu.extra_regs; er->msr; er++) {\n\t\tif (er->event != (config & er->config_mask))\n\t\t\tcontinue;\n\t\tif (event->attr.config1 & ~er->valid_mask)\n\t\t\treturn -EINVAL;\n\t\tevent->hw.extra_reg = er->msr;\n\t\tevent->hw.extra_config = event->attr.config1;\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic atomic_t active_events;\nstatic DEFINE_MUTEX(pmc_reserve_mutex);\n\n#ifdef CONFIG_X86_LOCAL_APIC\n\nstatic bool reserve_pmc_hardware(void)\n{\n\tint i;\n\n\tfor (i = 0; i < x86_pmu.num_counters; i++) {\n\t\tif (!reserve_perfctr_nmi(x86_pmu_event_addr(i)))\n\t\t\tgoto perfctr_fail;\n\t}\n\n\tfor (i = 0; i < x86_pmu.num_counters; i++) {\n\t\tif (!reserve_evntsel_nmi(x86_pmu_config_addr(i)))\n\t\t\tgoto eventsel_fail;\n\t}\n\n\treturn true;\n\neventsel_fail:\n\tfor (i--; i >= 0; i--)\n\t\trelease_evntsel_nmi(x86_pmu_config_addr(i));\n\n\ti = x86_pmu.num_counters;\n\nperfctr_fail:\n\tfor (i--; i >= 0; i--)\n\t\trelease_perfctr_nmi(x86_pmu_event_addr(i));\n\n\treturn false;\n}\n\nstatic void release_pmc_hardware(void)\n{\n\tint i;\n\n\tfor (i = 0; i < x86_pmu.num_counters; i++) {\n\t\trelease_perfctr_nmi(x86_pmu_event_addr(i));\n\t\trelease_evntsel_nmi(x86_pmu_config_addr(i));\n\t}\n}\n\n#else\n\nstatic bool reserve_pmc_hardware(void) { return true; }\nstatic void release_pmc_hardware(void) {}\n\n#endif\n\nstatic bool check_hw_exists(void)\n{\n\tu64 val, val_new = 0;\n\tint i, reg, ret = 0;\n\n\t/*\n\t * Check to see if the BIOS enabled any of the counters, if so\n\t * complain and bail.\n\t */\n\tfor (i = 0; i < x86_pmu.num_counters; i++) {\n\t\treg = x86_pmu_config_addr(i);\n\t\tret = rdmsrl_safe(reg, &val);\n\t\tif (ret)\n\t\t\tgoto msr_fail;\n\t\tif (val & ARCH_PERFMON_EVENTSEL_ENABLE)\n\t\t\tgoto bios_fail;\n\t}\n\n\tif (x86_pmu.num_counters_fixed) {\n\t\treg = MSR_ARCH_PERFMON_FIXED_CTR_CTRL;\n\t\tret = rdmsrl_safe(reg, &val);\n\t\tif (ret)\n\t\t\tgoto msr_fail;\n\t\tfor (i = 0; i < x86_pmu.num_counters_fixed; i++) {\n\t\t\tif (val & (0x03 << i*4))\n\t\t\t\tgoto bios_fail;\n\t\t}\n\t}\n\n\t/*\n\t * Now write a value and read it back to see if it matches,\n\t * this is needed to detect certain hardware emulators (qemu/kvm)\n\t * that don't trap on the MSR access and always return 0s.\n\t */\n\tval = 0xabcdUL;\n\tret = checking_wrmsrl(x86_pmu_event_addr(0), val);\n\tret |= rdmsrl_safe(x86_pmu_event_addr(0), &val_new);\n\tif (ret || val != val_new)\n\t\tgoto msr_fail;\n\n\treturn true;\n\nbios_fail:\n\t/*\n\t * We still allow the PMU driver to operate:\n\t */\n\tprintk(KERN_CONT \"Broken BIOS detected, complain to your hardware vendor.\\n\");\n\tprintk(KERN_ERR FW_BUG \"the BIOS has corrupted hw-PMU resources (MSR %x is %Lx)\\n\", reg, val);\n\n\treturn true;\n\nmsr_fail:\n\tprintk(KERN_CONT \"Broken PMU hardware detected, using software events only.\\n\");\n\n\treturn false;\n}\n\nstatic void reserve_ds_buffers(void);\nstatic void release_ds_buffers(void);\n\nstatic void hw_perf_event_destroy(struct perf_event *event)\n{\n\tif (atomic_dec_and_mutex_lock(&active_events, &pmc_reserve_mutex)) {\n\t\trelease_pmc_hardware();\n\t\trelease_ds_buffers();\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n}\n\nstatic inline int x86_pmu_initialized(void)\n{\n\treturn x86_pmu.handle_irq != NULL;\n}\n\nstatic inline int\nset_ext_hw_attr(struct hw_perf_event *hwc, struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\tunsigned int cache_type, cache_op, cache_result;\n\tu64 config, val;\n\n\tconfig = attr->config;\n\n\tcache_type = (config >>  0) & 0xff;\n\tif (cache_type >= PERF_COUNT_HW_CACHE_MAX)\n\t\treturn -EINVAL;\n\n\tcache_op = (config >>  8) & 0xff;\n\tif (cache_op >= PERF_COUNT_HW_CACHE_OP_MAX)\n\t\treturn -EINVAL;\n\n\tcache_result = (config >> 16) & 0xff;\n\tif (cache_result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn -EINVAL;\n\n\tval = hw_cache_event_ids[cache_type][cache_op][cache_result];\n\n\tif (val == 0)\n\t\treturn -ENOENT;\n\n\tif (val == -1)\n\t\treturn -EINVAL;\n\n\thwc->config |= val;\n\tattr->config1 = hw_cache_extra_regs[cache_type][cache_op][cache_result];\n\treturn x86_pmu_extra_regs(val, event);\n}\n\nstatic int x86_setup_perfctr(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 config;\n\n\tif (!is_sampling_event(event)) {\n\t\thwc->sample_period = x86_pmu.max_period;\n\t\thwc->last_period = hwc->sample_period;\n\t\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\t} else {\n\t\t/*\n\t\t * If we have a PMU initialized but no APIC\n\t\t * interrupts, we cannot sample hardware\n\t\t * events (user-space has to fall back and\n\t\t * sample via a hrtimer based software event):\n\t\t */\n\t\tif (!x86_pmu.apic)\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\t/*\n\t * Do not allow config1 (extended registers) to propagate,\n\t * there's no sane user-space generalization yet:\n\t */\n\tif (attr->type == PERF_TYPE_RAW)\n\t\treturn 0;\n\n\tif (attr->type == PERF_TYPE_HW_CACHE)\n\t\treturn set_ext_hw_attr(hwc, event);\n\n\tif (attr->config >= x86_pmu.max_events)\n\t\treturn -EINVAL;\n\n\t/*\n\t * The generic map:\n\t */\n\tconfig = x86_pmu.event_map(attr->config);\n\n\tif (config == 0)\n\t\treturn -ENOENT;\n\n\tif (config == -1LL)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Branch tracing:\n\t */\n\tif (attr->config == PERF_COUNT_HW_BRANCH_INSTRUCTIONS &&\n\t    !attr->freq && hwc->sample_period == 1) {\n\t\t/* BTS is not supported by this architecture. */\n\t\tif (!x86_pmu.bts_active)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\t/* BTS is currently only allowed for user-mode. */\n\t\tif (!attr->exclude_kernel)\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\thwc->config |= config;\n\n\treturn 0;\n}\n\nstatic int x86_pmu_hw_config(struct perf_event *event)\n{\n\tif (event->attr.precise_ip) {\n\t\tint precise = 0;\n\n\t\t/* Support for constant skid */\n\t\tif (x86_pmu.pebs_active) {\n\t\t\tprecise++;\n\n\t\t\t/* Support for IP fixup */\n\t\t\tif (x86_pmu.lbr_nr)\n\t\t\t\tprecise++;\n\t\t}\n\n\t\tif (event->attr.precise_ip > precise)\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\t/*\n\t * Generate PMC IRQs:\n\t * (keep 'enabled' bit clear for now)\n\t */\n\tevent->hw.config = ARCH_PERFMON_EVENTSEL_INT;\n\n\t/*\n\t * Count user and OS events unless requested not to\n\t */\n\tif (!event->attr.exclude_user)\n\t\tevent->hw.config |= ARCH_PERFMON_EVENTSEL_USR;\n\tif (!event->attr.exclude_kernel)\n\t\tevent->hw.config |= ARCH_PERFMON_EVENTSEL_OS;\n\n\tif (event->attr.type == PERF_TYPE_RAW)\n\t\tevent->hw.config |= event->attr.config & X86_RAW_EVENT_MASK;\n\n\treturn x86_setup_perfctr(event);\n}\n\n/*\n * Setup the hardware configuration for a given attr_type\n */\nstatic int __x86_pmu_event_init(struct perf_event *event)\n{\n\tint err;\n\n\tif (!x86_pmu_initialized())\n\t\treturn -ENODEV;\n\n\terr = 0;\n\tif (!atomic_inc_not_zero(&active_events)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_read(&active_events) == 0) {\n\t\t\tif (!reserve_pmc_hardware())\n\t\t\t\terr = -EBUSY;\n\t\t\telse\n\t\t\t\treserve_ds_buffers();\n\t\t}\n\t\tif (!err)\n\t\t\tatomic_inc(&active_events);\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n\tif (err)\n\t\treturn err;\n\n\tevent->destroy = hw_perf_event_destroy;\n\n\tevent->hw.idx = -1;\n\tevent->hw.last_cpu = -1;\n\tevent->hw.last_tag = ~0ULL;\n\n\treturn x86_pmu.hw_config(event);\n}\n\nstatic void x86_pmu_disable_all(void)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint idx;\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tu64 val;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\t\trdmsrl(x86_pmu_config_addr(idx), val);\n\t\tif (!(val & ARCH_PERFMON_EVENTSEL_ENABLE))\n\t\t\tcontinue;\n\t\tval &= ~ARCH_PERFMON_EVENTSEL_ENABLE;\n\t\twrmsrl(x86_pmu_config_addr(idx), val);\n\t}\n}\n\nstatic void x86_pmu_disable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\tif (!x86_pmu_initialized())\n\t\treturn;\n\n\tif (!cpuc->enabled)\n\t\treturn;\n\n\tcpuc->n_added = 0;\n\tcpuc->enabled = 0;\n\tbarrier();\n\n\tx86_pmu.disable_all();\n}\n\nstatic inline void __x86_pmu_enable_event(struct hw_perf_event *hwc,\n\t\t\t\t\t  u64 enable_mask)\n{\n\tif (hwc->extra_reg)\n\t\twrmsrl(hwc->extra_reg, hwc->extra_config);\n\twrmsrl(hwc->config_base, hwc->config | enable_mask);\n}\n\nstatic void x86_pmu_enable_all(int added)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint idx;\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tstruct hw_perf_event *hwc = &cpuc->events[idx]->hw;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\t__x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);\n\t}\n}\n\nstatic struct pmu pmu;\n\nstatic inline int is_x86_event(struct perf_event *event)\n{\n\treturn event->pmu == &pmu;\n}\n\nstatic int x86_schedule_events(struct cpu_hw_events *cpuc, int n, int *assign)\n{\n\tstruct event_constraint *c, *constraints[X86_PMC_IDX_MAX];\n\tunsigned long used_mask[BITS_TO_LONGS(X86_PMC_IDX_MAX)];\n\tint i, j, w, wmax, num = 0;\n\tstruct hw_perf_event *hwc;\n\n\tbitmap_zero(used_mask, X86_PMC_IDX_MAX);\n\n\tfor (i = 0; i < n; i++) {\n\t\tc = x86_pmu.get_event_constraints(cpuc, cpuc->event_list[i]);\n\t\tconstraints[i] = c;\n\t}\n\n\t/*\n\t * fastpath, try to reuse previous register\n\t */\n\tfor (i = 0; i < n; i++) {\n\t\thwc = &cpuc->event_list[i]->hw;\n\t\tc = constraints[i];\n\n\t\t/* never assigned */\n\t\tif (hwc->idx == -1)\n\t\t\tbreak;\n\n\t\t/* constraint still honored */\n\t\tif (!test_bit(hwc->idx, c->idxmsk))\n\t\t\tbreak;\n\n\t\t/* not already used */\n\t\tif (test_bit(hwc->idx, used_mask))\n\t\t\tbreak;\n\n\t\t__set_bit(hwc->idx, used_mask);\n\t\tif (assign)\n\t\t\tassign[i] = hwc->idx;\n\t}\n\tif (i == n)\n\t\tgoto done;\n\n\t/*\n\t * begin slow path\n\t */\n\n\tbitmap_zero(used_mask, X86_PMC_IDX_MAX);\n\n\t/*\n\t * weight = number of possible counters\n\t *\n\t * 1    = most constrained, only works on one counter\n\t * wmax = least constrained, works on any counter\n\t *\n\t * assign events to counters starting with most\n\t * constrained events.\n\t */\n\twmax = x86_pmu.num_counters;\n\n\t/*\n\t * when fixed event counters are present,\n\t * wmax is incremented by 1 to account\n\t * for one more choice\n\t */\n\tif (x86_pmu.num_counters_fixed)\n\t\twmax++;\n\n\tfor (w = 1, num = n; num && w <= wmax; w++) {\n\t\t/* for each event */\n\t\tfor (i = 0; num && i < n; i++) {\n\t\t\tc = constraints[i];\n\t\t\thwc = &cpuc->event_list[i]->hw;\n\n\t\t\tif (c->weight != w)\n\t\t\t\tcontinue;\n\n\t\t\tfor_each_set_bit(j, c->idxmsk, X86_PMC_IDX_MAX) {\n\t\t\t\tif (!test_bit(j, used_mask))\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (j == X86_PMC_IDX_MAX)\n\t\t\t\tbreak;\n\n\t\t\t__set_bit(j, used_mask);\n\n\t\t\tif (assign)\n\t\t\t\tassign[i] = j;\n\t\t\tnum--;\n\t\t}\n\t}\ndone:\n\t/*\n\t * scheduling failed or is just a simulation,\n\t * free resources if necessary\n\t */\n\tif (!assign || num) {\n\t\tfor (i = 0; i < n; i++) {\n\t\t\tif (x86_pmu.put_event_constraints)\n\t\t\t\tx86_pmu.put_event_constraints(cpuc, cpuc->event_list[i]);\n\t\t}\n\t}\n\treturn num ? -ENOSPC : 0;\n}\n\n/*\n * dogrp: true if must collect siblings events (group)\n * returns total number of events and error code\n */\nstatic int collect_events(struct cpu_hw_events *cpuc, struct perf_event *leader, bool dogrp)\n{\n\tstruct perf_event *event;\n\tint n, max_count;\n\n\tmax_count = x86_pmu.num_counters + x86_pmu.num_counters_fixed;\n\n\t/* current number of events already accepted */\n\tn = cpuc->n_events;\n\n\tif (is_x86_event(leader)) {\n\t\tif (n >= max_count)\n\t\t\treturn -ENOSPC;\n\t\tcpuc->event_list[n] = leader;\n\t\tn++;\n\t}\n\tif (!dogrp)\n\t\treturn n;\n\n\tlist_for_each_entry(event, &leader->sibling_list, group_entry) {\n\t\tif (!is_x86_event(event) ||\n\t\t    event->state <= PERF_EVENT_STATE_OFF)\n\t\t\tcontinue;\n\n\t\tif (n >= max_count)\n\t\t\treturn -ENOSPC;\n\n\t\tcpuc->event_list[n] = event;\n\t\tn++;\n\t}\n\treturn n;\n}\n\nstatic inline void x86_assign_hw_event(struct perf_event *event,\n\t\t\t\tstruct cpu_hw_events *cpuc, int i)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\thwc->idx = cpuc->assign[i];\n\thwc->last_cpu = smp_processor_id();\n\thwc->last_tag = ++cpuc->tags[i];\n\n\tif (hwc->idx == X86_PMC_IDX_FIXED_BTS) {\n\t\thwc->config_base = 0;\n\t\thwc->event_base\t= 0;\n\t} else if (hwc->idx >= X86_PMC_IDX_FIXED) {\n\t\thwc->config_base = MSR_ARCH_PERFMON_FIXED_CTR_CTRL;\n\t\thwc->event_base = MSR_ARCH_PERFMON_FIXED_CTR0 + (hwc->idx - X86_PMC_IDX_FIXED);\n\t} else {\n\t\thwc->config_base = x86_pmu_config_addr(hwc->idx);\n\t\thwc->event_base  = x86_pmu_event_addr(hwc->idx);\n\t}\n}\n\nstatic inline int match_prev_assignment(struct hw_perf_event *hwc,\n\t\t\t\t\tstruct cpu_hw_events *cpuc,\n\t\t\t\t\tint i)\n{\n\treturn hwc->idx == cpuc->assign[i] &&\n\t\thwc->last_cpu == smp_processor_id() &&\n\t\thwc->last_tag == cpuc->tags[i];\n}\n\nstatic void x86_pmu_start(struct perf_event *event, int flags);\nstatic void x86_pmu_stop(struct perf_event *event, int flags);\n\nstatic void x86_pmu_enable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct perf_event *event;\n\tstruct hw_perf_event *hwc;\n\tint i, added = cpuc->n_added;\n\n\tif (!x86_pmu_initialized())\n\t\treturn;\n\n\tif (cpuc->enabled)\n\t\treturn;\n\n\tif (cpuc->n_added) {\n\t\tint n_running = cpuc->n_events - cpuc->n_added;\n\t\t/*\n\t\t * apply assignment obtained either from\n\t\t * hw_perf_group_sched_in() or x86_pmu_enable()\n\t\t *\n\t\t * step1: save events moving to new counters\n\t\t * step2: reprogram moved events into new counters\n\t\t */\n\t\tfor (i = 0; i < n_running; i++) {\n\t\t\tevent = cpuc->event_list[i];\n\t\t\thwc = &event->hw;\n\n\t\t\t/*\n\t\t\t * we can avoid reprogramming counter if:\n\t\t\t * - assigned same counter as last time\n\t\t\t * - running on same CPU as last time\n\t\t\t * - no other event has used the counter since\n\t\t\t */\n\t\t\tif (hwc->idx == -1 ||\n\t\t\t    match_prev_assignment(hwc, cpuc, i))\n\t\t\t\tcontinue;\n\n\t\t\t/*\n\t\t\t * Ensure we don't accidentally enable a stopped\n\t\t\t * counter simply because we rescheduled.\n\t\t\t */\n\t\t\tif (hwc->state & PERF_HES_STOPPED)\n\t\t\t\thwc->state |= PERF_HES_ARCH;\n\n\t\t\tx86_pmu_stop(event, PERF_EF_UPDATE);\n\t\t}\n\n\t\tfor (i = 0; i < cpuc->n_events; i++) {\n\t\t\tevent = cpuc->event_list[i];\n\t\t\thwc = &event->hw;\n\n\t\t\tif (!match_prev_assignment(hwc, cpuc, i))\n\t\t\t\tx86_assign_hw_event(event, cpuc, i);\n\t\t\telse if (i < n_running)\n\t\t\t\tcontinue;\n\n\t\t\tif (hwc->state & PERF_HES_ARCH)\n\t\t\t\tcontinue;\n\n\t\t\tx86_pmu_start(event, PERF_EF_RELOAD);\n\t\t}\n\t\tcpuc->n_added = 0;\n\t\tperf_events_lapic_init();\n\t}\n\n\tcpuc->enabled = 1;\n\tbarrier();\n\n\tx86_pmu.enable_all(added);\n}\n\nstatic inline void x86_pmu_disable_event(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\twrmsrl(hwc->config_base, hwc->config);\n}\n\nstatic DEFINE_PER_CPU(u64 [X86_PMC_IDX_MAX], pmc_prev_left);\n\n/*\n * Set the next IRQ period, based on the hwc->period_left value.\n * To be called with the event disabled in hw:\n */\nstatic int\nx86_perf_event_set_period(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\ts64 left = local64_read(&hwc->period_left);\n\ts64 period = hwc->sample_period;\n\tint ret = 0, idx = hwc->idx;\n\n\tif (idx == X86_PMC_IDX_FIXED_BTS)\n\t\treturn 0;\n\n\t/*\n\t * If we are way outside a reasonable range then just skip forward:\n\t */\n\tif (unlikely(left <= -period)) {\n\t\tleft = period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\tif (unlikely(left <= 0)) {\n\t\tleft += period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\t/*\n\t * Quirk: certain CPUs dont like it if just 1 hw_event is left:\n\t */\n\tif (unlikely(left < 2))\n\t\tleft = 2;\n\n\tif (left > x86_pmu.max_period)\n\t\tleft = x86_pmu.max_period;\n\n\tper_cpu(pmc_prev_left[idx], smp_processor_id()) = left;\n\n\t/*\n\t * The hw event starts counting from this event offset,\n\t * mark it to be able to extra future deltas:\n\t */\n\tlocal64_set(&hwc->prev_count, (u64)-left);\n\n\twrmsrl(hwc->event_base, (u64)(-left) & x86_pmu.cntval_mask);\n\n\t/*\n\t * Due to erratum on certan cpu we need\n\t * a second write to be sure the register\n\t * is updated properly\n\t */\n\tif (x86_pmu.perfctr_second_write) {\n\t\twrmsrl(hwc->event_base,\n\t\t\t(u64)(-left) & x86_pmu.cntval_mask);\n\t}\n\n\tperf_event_update_userpage(event);\n\n\treturn ret;\n}\n\nstatic void x86_pmu_enable_event(struct perf_event *event)\n{\n\tif (__this_cpu_read(cpu_hw_events.enabled))\n\t\t__x86_pmu_enable_event(&event->hw,\n\t\t\t\t       ARCH_PERFMON_EVENTSEL_ENABLE);\n}\n\n/*\n * Add a single event to the PMU.\n *\n * The event is added to the group of enabled events\n * but only if it can be scehduled with existing events.\n */\nstatic int x86_pmu_add(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct hw_perf_event *hwc;\n\tint assign[X86_PMC_IDX_MAX];\n\tint n, n0, ret;\n\n\thwc = &event->hw;\n\n\tperf_pmu_disable(event->pmu);\n\tn0 = cpuc->n_events;\n\tret = n = collect_events(cpuc, event, false);\n\tif (ret < 0)\n\t\tgoto out;\n\n\thwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;\n\tif (!(flags & PERF_EF_START))\n\t\thwc->state |= PERF_HES_ARCH;\n\n\t/*\n\t * If group events scheduling transaction was started,\n\t * skip the schedulability test here, it will be performed\n\t * at commit time (->commit_txn) as a whole\n\t */\n\tif (cpuc->group_flag & PERF_EVENT_TXN)\n\t\tgoto done_collect;\n\n\tret = x86_pmu.schedule_events(cpuc, n, assign);\n\tif (ret)\n\t\tgoto out;\n\t/*\n\t * copy new assignment, now we know it is possible\n\t * will be used by hw_perf_enable()\n\t */\n\tmemcpy(cpuc->assign, assign, n*sizeof(int));\n\ndone_collect:\n\tcpuc->n_events = n;\n\tcpuc->n_added += n - n0;\n\tcpuc->n_txn += n - n0;\n\n\tret = 0;\nout:\n\tperf_pmu_enable(event->pmu);\n\treturn ret;\n}\n\nstatic void x86_pmu_start(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint idx = event->hw.idx;\n\n\tif (WARN_ON_ONCE(!(event->hw.state & PERF_HES_STOPPED)))\n\t\treturn;\n\n\tif (WARN_ON_ONCE(idx == -1))\n\t\treturn;\n\n\tif (flags & PERF_EF_RELOAD) {\n\t\tWARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));\n\t\tx86_perf_event_set_period(event);\n\t}\n\n\tevent->hw.state = 0;\n\n\tcpuc->events[idx] = event;\n\t__set_bit(idx, cpuc->active_mask);\n\t__set_bit(idx, cpuc->running);\n\tx86_pmu.enable(event);\n\tperf_event_update_userpage(event);\n}\n\nvoid perf_event_print_debug(void)\n{\n\tu64 ctrl, status, overflow, pmc_ctrl, pmc_count, prev_left, fixed;\n\tu64 pebs;\n\tstruct cpu_hw_events *cpuc;\n\tunsigned long flags;\n\tint cpu, idx;\n\n\tif (!x86_pmu.num_counters)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tcpu = smp_processor_id();\n\tcpuc = &per_cpu(cpu_hw_events, cpu);\n\n\tif (x86_pmu.version >= 2) {\n\t\trdmsrl(MSR_CORE_PERF_GLOBAL_CTRL, ctrl);\n\t\trdmsrl(MSR_CORE_PERF_GLOBAL_STATUS, status);\n\t\trdmsrl(MSR_CORE_PERF_GLOBAL_OVF_CTRL, overflow);\n\t\trdmsrl(MSR_ARCH_PERFMON_FIXED_CTR_CTRL, fixed);\n\t\trdmsrl(MSR_IA32_PEBS_ENABLE, pebs);\n\n\t\tpr_info(\"\\n\");\n\t\tpr_info(\"CPU#%d: ctrl:       %016llx\\n\", cpu, ctrl);\n\t\tpr_info(\"CPU#%d: status:     %016llx\\n\", cpu, status);\n\t\tpr_info(\"CPU#%d: overflow:   %016llx\\n\", cpu, overflow);\n\t\tpr_info(\"CPU#%d: fixed:      %016llx\\n\", cpu, fixed);\n\t\tpr_info(\"CPU#%d: pebs:       %016llx\\n\", cpu, pebs);\n\t}\n\tpr_info(\"CPU#%d: active:     %016llx\\n\", cpu, *(u64 *)cpuc->active_mask);\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\trdmsrl(x86_pmu_config_addr(idx), pmc_ctrl);\n\t\trdmsrl(x86_pmu_event_addr(idx), pmc_count);\n\n\t\tprev_left = per_cpu(pmc_prev_left[idx], cpu);\n\n\t\tpr_info(\"CPU#%d:   gen-PMC%d ctrl:  %016llx\\n\",\n\t\t\tcpu, idx, pmc_ctrl);\n\t\tpr_info(\"CPU#%d:   gen-PMC%d count: %016llx\\n\",\n\t\t\tcpu, idx, pmc_count);\n\t\tpr_info(\"CPU#%d:   gen-PMC%d left:  %016llx\\n\",\n\t\t\tcpu, idx, prev_left);\n\t}\n\tfor (idx = 0; idx < x86_pmu.num_counters_fixed; idx++) {\n\t\trdmsrl(MSR_ARCH_PERFMON_FIXED_CTR0 + idx, pmc_count);\n\n\t\tpr_info(\"CPU#%d: fixed-PMC%d count: %016llx\\n\",\n\t\t\tcpu, idx, pmc_count);\n\t}\n\tlocal_irq_restore(flags);\n}\n\nstatic void x86_pmu_stop(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (__test_and_clear_bit(hwc->idx, cpuc->active_mask)) {\n\t\tx86_pmu.disable(event);\n\t\tcpuc->events[hwc->idx] = NULL;\n\t\tWARN_ON_ONCE(hwc->state & PERF_HES_STOPPED);\n\t\thwc->state |= PERF_HES_STOPPED;\n\t}\n\n\tif ((flags & PERF_EF_UPDATE) && !(hwc->state & PERF_HES_UPTODATE)) {\n\t\t/*\n\t\t * Drain the remaining delta count out of a event\n\t\t * that we are disabling:\n\t\t */\n\t\tx86_perf_event_update(event);\n\t\thwc->state |= PERF_HES_UPTODATE;\n\t}\n}\n\nstatic void x86_pmu_del(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint i;\n\n\t/*\n\t * If we're called during a txn, we don't need to do anything.\n\t * The events never got scheduled and ->cancel_txn will truncate\n\t * the event_list.\n\t */\n\tif (cpuc->group_flag & PERF_EVENT_TXN)\n\t\treturn;\n\n\tx86_pmu_stop(event, PERF_EF_UPDATE);\n\n\tfor (i = 0; i < cpuc->n_events; i++) {\n\t\tif (event == cpuc->event_list[i]) {\n\n\t\t\tif (x86_pmu.put_event_constraints)\n\t\t\t\tx86_pmu.put_event_constraints(cpuc, event);\n\n\t\t\twhile (++i < cpuc->n_events)\n\t\t\t\tcpuc->event_list[i-1] = cpuc->event_list[i];\n\n\t\t\t--cpuc->n_events;\n\t\t\tbreak;\n\t\t}\n\t}\n\tperf_event_update_userpage(event);\n}\n\nstatic int x86_pmu_handle_irq(struct pt_regs *regs)\n{\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct perf_event *event;\n\tint idx, handled = 0;\n\tu64 val;\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\n\t/*\n\t * Some chipsets need to unmask the LVTPC in a particular spot\n\t * inside the nmi handler.  As a result, the unmasking was pushed\n\t * into all the nmi handlers.\n\t *\n\t * This generic handler doesn't seem to have any issues where the\n\t * unmasking occurs so it was left at the top.\n\t */\n\tapic_write(APIC_LVTPC, APIC_DM_NMI);\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tif (!test_bit(idx, cpuc->active_mask)) {\n\t\t\t/*\n\t\t\t * Though we deactivated the counter some cpus\n\t\t\t * might still deliver spurious interrupts still\n\t\t\t * in flight. Catch them:\n\t\t\t */\n\t\t\tif (__test_and_clear_bit(idx, cpuc->running))\n\t\t\t\thandled++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tevent = cpuc->events[idx];\n\n\t\tval = x86_perf_event_update(event);\n\t\tif (val & (1ULL << (x86_pmu.cntval_bits - 1)))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * event overflow\n\t\t */\n\t\thandled++;\n\t\tdata.period\t= event->hw.last_period;\n\n\t\tif (!x86_perf_event_set_period(event))\n\t\t\tcontinue;\n\n\t\tif (perf_event_overflow(event, 1, &data, regs))\n\t\t\tx86_pmu_stop(event, 0);\n\t}\n\n\tif (handled)\n\t\tinc_irq_stat(apic_perf_irqs);\n\n\treturn handled;\n}\n\nvoid perf_events_lapic_init(void)\n{\n\tif (!x86_pmu.apic || !x86_pmu_initialized())\n\t\treturn;\n\n\t/*\n\t * Always use NMI for PMU\n\t */\n\tapic_write(APIC_LVTPC, APIC_DM_NMI);\n}\n\nstruct pmu_nmi_state {\n\tunsigned int\tmarked;\n\tint\t\thandled;\n};\n\nstatic DEFINE_PER_CPU(struct pmu_nmi_state, pmu_nmi);\n\nstatic int __kprobes\nperf_event_nmi_handler(struct notifier_block *self,\n\t\t\t unsigned long cmd, void *__args)\n{\n\tstruct die_args *args = __args;\n\tunsigned int this_nmi;\n\tint handled;\n\n\tif (!atomic_read(&active_events))\n\t\treturn NOTIFY_DONE;\n\n\tswitch (cmd) {\n\tcase DIE_NMI:\n\t\tbreak;\n\tcase DIE_NMIUNKNOWN:\n\t\tthis_nmi = percpu_read(irq_stat.__nmi_count);\n\t\tif (this_nmi != __this_cpu_read(pmu_nmi.marked))\n\t\t\t/* let the kernel handle the unknown nmi */\n\t\t\treturn NOTIFY_DONE;\n\t\t/*\n\t\t * This one is a PMU back-to-back nmi. Two events\n\t\t * trigger 'simultaneously' raising two back-to-back\n\t\t * NMIs. If the first NMI handles both, the latter\n\t\t * will be empty and daze the CPU. So, we drop it to\n\t\t * avoid false-positive 'unknown nmi' messages.\n\t\t */\n\t\treturn NOTIFY_STOP;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n\n\thandled = x86_pmu.handle_irq(args->regs);\n\tif (!handled)\n\t\treturn NOTIFY_DONE;\n\n\tthis_nmi = percpu_read(irq_stat.__nmi_count);\n\tif ((handled > 1) ||\n\t\t/* the next nmi could be a back-to-back nmi */\n\t    ((__this_cpu_read(pmu_nmi.marked) == this_nmi) &&\n\t     (__this_cpu_read(pmu_nmi.handled) > 1))) {\n\t\t/*\n\t\t * We could have two subsequent back-to-back nmis: The\n\t\t * first handles more than one counter, the 2nd\n\t\t * handles only one counter and the 3rd handles no\n\t\t * counter.\n\t\t *\n\t\t * This is the 2nd nmi because the previous was\n\t\t * handling more than one counter. We will mark the\n\t\t * next (3rd) and then drop it if unhandled.\n\t\t */\n\t\t__this_cpu_write(pmu_nmi.marked, this_nmi + 1);\n\t\t__this_cpu_write(pmu_nmi.handled, handled);\n\t}\n\n\treturn NOTIFY_STOP;\n}\n\nstatic __read_mostly struct notifier_block perf_event_nmi_notifier = {\n\t.notifier_call\t\t= perf_event_nmi_handler,\n\t.next\t\t\t= NULL,\n\t.priority\t\t= NMI_LOCAL_LOW_PRIOR,\n};\n\nstatic struct event_constraint unconstrained;\nstatic struct event_constraint emptyconstraint;\n\nstatic struct event_constraint *\nx86_get_event_constraints(struct cpu_hw_events *cpuc, struct perf_event *event)\n{\n\tstruct event_constraint *c;\n\n\tif (x86_pmu.event_constraints) {\n\t\tfor_each_event_constraint(c, x86_pmu.event_constraints) {\n\t\t\tif ((event->hw.config & c->cmask) == c->code)\n\t\t\t\treturn c;\n\t\t}\n\t}\n\n\treturn &unconstrained;\n}\n\n#include \"perf_event_amd.c\"\n#include \"perf_event_p6.c\"\n#include \"perf_event_p4.c\"\n#include \"perf_event_intel_lbr.c\"\n#include \"perf_event_intel_ds.c\"\n#include \"perf_event_intel.c\"\n\nstatic int __cpuinit\nx86_pmu_notifier(struct notifier_block *self, unsigned long action, void *hcpu)\n{\n\tunsigned int cpu = (long)hcpu;\n\tint ret = NOTIFY_OK;\n\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_UP_PREPARE:\n\t\tif (x86_pmu.cpu_prepare)\n\t\t\tret = x86_pmu.cpu_prepare(cpu);\n\t\tbreak;\n\n\tcase CPU_STARTING:\n\t\tif (x86_pmu.cpu_starting)\n\t\t\tx86_pmu.cpu_starting(cpu);\n\t\tbreak;\n\n\tcase CPU_DYING:\n\t\tif (x86_pmu.cpu_dying)\n\t\t\tx86_pmu.cpu_dying(cpu);\n\t\tbreak;\n\n\tcase CPU_UP_CANCELED:\n\tcase CPU_DEAD:\n\t\tif (x86_pmu.cpu_dead)\n\t\t\tx86_pmu.cpu_dead(cpu);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic void __init pmu_check_apic(void)\n{\n\tif (cpu_has_apic)\n\t\treturn;\n\n\tx86_pmu.apic = 0;\n\tpr_info(\"no APIC, boot with the \\\"lapic\\\" boot parameter to force-enable it.\\n\");\n\tpr_info(\"no hardware sampling interrupt available.\\n\");\n}\n\nstatic int __init init_hw_perf_events(void)\n{\n\tstruct event_constraint *c;\n\tint err;\n\n\tpr_info(\"Performance Events: \");\n\n\tswitch (boot_cpu_data.x86_vendor) {\n\tcase X86_VENDOR_INTEL:\n\t\terr = intel_pmu_init();\n\t\tbreak;\n\tcase X86_VENDOR_AMD:\n\t\terr = amd_pmu_init();\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\tif (err != 0) {\n\t\tpr_cont(\"no PMU driver, software events only.\\n\");\n\t\treturn 0;\n\t}\n\n\tpmu_check_apic();\n\n\t/* sanity check that the hardware exists or is emulated */\n\tif (!check_hw_exists())\n\t\treturn 0;\n\n\tpr_cont(\"%s PMU driver.\\n\", x86_pmu.name);\n\n\tif (x86_pmu.quirks)\n\t\tx86_pmu.quirks();\n\n\tif (x86_pmu.num_counters > X86_PMC_MAX_GENERIC) {\n\t\tWARN(1, KERN_ERR \"hw perf events %d > max(%d), clipping!\",\n\t\t     x86_pmu.num_counters, X86_PMC_MAX_GENERIC);\n\t\tx86_pmu.num_counters = X86_PMC_MAX_GENERIC;\n\t}\n\tx86_pmu.intel_ctrl = (1 << x86_pmu.num_counters) - 1;\n\n\tif (x86_pmu.num_counters_fixed > X86_PMC_MAX_FIXED) {\n\t\tWARN(1, KERN_ERR \"hw perf events fixed %d > max(%d), clipping!\",\n\t\t     x86_pmu.num_counters_fixed, X86_PMC_MAX_FIXED);\n\t\tx86_pmu.num_counters_fixed = X86_PMC_MAX_FIXED;\n\t}\n\n\tx86_pmu.intel_ctrl |=\n\t\t((1LL << x86_pmu.num_counters_fixed)-1) << X86_PMC_IDX_FIXED;\n\n\tperf_events_lapic_init();\n\tregister_die_notifier(&perf_event_nmi_notifier);\n\n\tunconstrained = (struct event_constraint)\n\t\t__EVENT_CONSTRAINT(0, (1ULL << x86_pmu.num_counters) - 1,\n\t\t\t\t   0, x86_pmu.num_counters);\n\n\tif (x86_pmu.event_constraints) {\n\t\tfor_each_event_constraint(c, x86_pmu.event_constraints) {\n\t\t\tif (c->cmask != X86_RAW_EVENT_MASK)\n\t\t\t\tcontinue;\n\n\t\t\tc->idxmsk64 |= (1ULL << x86_pmu.num_counters) - 1;\n\t\t\tc->weight += x86_pmu.num_counters;\n\t\t}\n\t}\n\n\tpr_info(\"... version:                %d\\n\",     x86_pmu.version);\n\tpr_info(\"... bit width:              %d\\n\",     x86_pmu.cntval_bits);\n\tpr_info(\"... generic registers:      %d\\n\",     x86_pmu.num_counters);\n\tpr_info(\"... value mask:             %016Lx\\n\", x86_pmu.cntval_mask);\n\tpr_info(\"... max period:             %016Lx\\n\", x86_pmu.max_period);\n\tpr_info(\"... fixed-purpose events:   %d\\n\",     x86_pmu.num_counters_fixed);\n\tpr_info(\"... event mask:             %016Lx\\n\", x86_pmu.intel_ctrl);\n\n\tperf_pmu_register(&pmu, \"cpu\", PERF_TYPE_RAW);\n\tperf_cpu_notifier(x86_pmu_notifier);\n\n\treturn 0;\n}\nearly_initcall(init_hw_perf_events);\n\nstatic inline void x86_pmu_read(struct perf_event *event)\n{\n\tx86_perf_event_update(event);\n}\n\n/*\n * Start group events scheduling transaction\n * Set the flag to make pmu::enable() not perform the\n * schedulability test, it will be performed at commit time\n */\nstatic void x86_pmu_start_txn(struct pmu *pmu)\n{\n\tperf_pmu_disable(pmu);\n\t__this_cpu_or(cpu_hw_events.group_flag, PERF_EVENT_TXN);\n\t__this_cpu_write(cpu_hw_events.n_txn, 0);\n}\n\n/*\n * Stop group events scheduling transaction\n * Clear the flag and pmu::enable() will perform the\n * schedulability test.\n */\nstatic void x86_pmu_cancel_txn(struct pmu *pmu)\n{\n\t__this_cpu_and(cpu_hw_events.group_flag, ~PERF_EVENT_TXN);\n\t/*\n\t * Truncate the collected events.\n\t */\n\t__this_cpu_sub(cpu_hw_events.n_added, __this_cpu_read(cpu_hw_events.n_txn));\n\t__this_cpu_sub(cpu_hw_events.n_events, __this_cpu_read(cpu_hw_events.n_txn));\n\tperf_pmu_enable(pmu);\n}\n\n/*\n * Commit group events scheduling transaction\n * Perform the group schedulability test as a whole\n * Return 0 if success\n */\nstatic int x86_pmu_commit_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint assign[X86_PMC_IDX_MAX];\n\tint n, ret;\n\n\tn = cpuc->n_events;\n\n\tif (!x86_pmu_initialized())\n\t\treturn -EAGAIN;\n\n\tret = x86_pmu.schedule_events(cpuc, n, assign);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * copy new assignment, now we know it is possible\n\t * will be used by hw_perf_enable()\n\t */\n\tmemcpy(cpuc->assign, assign, n*sizeof(int));\n\n\tcpuc->group_flag &= ~PERF_EVENT_TXN;\n\tperf_pmu_enable(pmu);\n\treturn 0;\n}\n\n/*\n * validate that we can schedule this event\n */\nstatic int validate_event(struct perf_event *event)\n{\n\tstruct cpu_hw_events *fake_cpuc;\n\tstruct event_constraint *c;\n\tint ret = 0;\n\n\tfake_cpuc = kmalloc(sizeof(*fake_cpuc), GFP_KERNEL | __GFP_ZERO);\n\tif (!fake_cpuc)\n\t\treturn -ENOMEM;\n\n\tc = x86_pmu.get_event_constraints(fake_cpuc, event);\n\n\tif (!c || !c->weight)\n\t\tret = -ENOSPC;\n\n\tif (x86_pmu.put_event_constraints)\n\t\tx86_pmu.put_event_constraints(fake_cpuc, event);\n\n\tkfree(fake_cpuc);\n\n\treturn ret;\n}\n\n/*\n * validate a single event group\n *\n * validation include:\n *\t- check events are compatible which each other\n *\t- events do not compete for the same counter\n *\t- number of events <= number of counters\n *\n * validation ensures the group can be loaded onto the\n * PMU if it was the only group available.\n */\nstatic int validate_group(struct perf_event *event)\n{\n\tstruct perf_event *leader = event->group_leader;\n\tstruct cpu_hw_events *fake_cpuc;\n\tint ret, n;\n\n\tret = -ENOMEM;\n\tfake_cpuc = kmalloc(sizeof(*fake_cpuc), GFP_KERNEL | __GFP_ZERO);\n\tif (!fake_cpuc)\n\t\tgoto out;\n\n\t/*\n\t * the event is not yet connected with its\n\t * siblings therefore we must first collect\n\t * existing siblings, then add the new event\n\t * before we can simulate the scheduling\n\t */\n\tret = -ENOSPC;\n\tn = collect_events(fake_cpuc, leader, true);\n\tif (n < 0)\n\t\tgoto out_free;\n\n\tfake_cpuc->n_events = n;\n\tn = collect_events(fake_cpuc, event, false);\n\tif (n < 0)\n\t\tgoto out_free;\n\n\tfake_cpuc->n_events = n;\n\n\tret = x86_pmu.schedule_events(fake_cpuc, n, NULL);\n\nout_free:\n\tkfree(fake_cpuc);\nout:\n\treturn ret;\n}\n\nstatic int x86_pmu_event_init(struct perf_event *event)\n{\n\tstruct pmu *tmp;\n\tint err;\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_RAW:\n\tcase PERF_TYPE_HARDWARE:\n\tcase PERF_TYPE_HW_CACHE:\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\terr = __x86_pmu_event_init(event);\n\tif (!err) {\n\t\t/*\n\t\t * we temporarily connect event to its pmu\n\t\t * such that validate_group() can classify\n\t\t * it as an x86 event using is_x86_event()\n\t\t */\n\t\ttmp = event->pmu;\n\t\tevent->pmu = &pmu;\n\n\t\tif (event->group_leader != event)\n\t\t\terr = validate_group(event);\n\t\telse\n\t\t\terr = validate_event(event);\n\n\t\tevent->pmu = tmp;\n\t}\n\tif (err) {\n\t\tif (event->destroy)\n\t\t\tevent->destroy(event);\n\t}\n\n\treturn err;\n}\n\nstatic struct pmu pmu = {\n\t.pmu_enable\t= x86_pmu_enable,\n\t.pmu_disable\t= x86_pmu_disable,\n\n\t.event_init\t= x86_pmu_event_init,\n\n\t.add\t\t= x86_pmu_add,\n\t.del\t\t= x86_pmu_del,\n\t.start\t\t= x86_pmu_start,\n\t.stop\t\t= x86_pmu_stop,\n\t.read\t\t= x86_pmu_read,\n\n\t.start_txn\t= x86_pmu_start_txn,\n\t.cancel_txn\t= x86_pmu_cancel_txn,\n\t.commit_txn\t= x86_pmu_commit_txn,\n};\n\n/*\n * callchain support\n */\n\nstatic int backtrace_stack(void *data, char *name)\n{\n\treturn 0;\n}\n\nstatic void backtrace_address(void *data, unsigned long addr, int reliable)\n{\n\tstruct perf_callchain_entry *entry = data;\n\n\tperf_callchain_store(entry, addr);\n}\n\nstatic const struct stacktrace_ops backtrace_ops = {\n\t.stack\t\t\t= backtrace_stack,\n\t.address\t\t= backtrace_address,\n\t.walk_stack\t\t= print_context_stack_bp,\n};\n\nvoid\nperf_callchain_kernel(struct perf_callchain_entry *entry, struct pt_regs *regs)\n{\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest()) {\n\t\t/* TODO: We don't support guest os callchain now */\n\t\treturn;\n\t}\n\n\tperf_callchain_store(entry, regs->ip);\n\n\tdump_trace(NULL, regs, NULL, 0, &backtrace_ops, entry);\n}\n\n#ifdef CONFIG_COMPAT\nstatic inline int\nperf_callchain_user32(struct pt_regs *regs, struct perf_callchain_entry *entry)\n{\n\t/* 32-bit process in 64-bit kernel. */\n\tstruct stack_frame_ia32 frame;\n\tconst void __user *fp;\n\n\tif (!test_thread_flag(TIF_IA32))\n\t\treturn 0;\n\n\tfp = compat_ptr(regs->bp);\n\twhile (entry->nr < PERF_MAX_STACK_DEPTH) {\n\t\tunsigned long bytes;\n\t\tframe.next_frame     = 0;\n\t\tframe.return_address = 0;\n\n\t\tbytes = copy_from_user_nmi(&frame, fp, sizeof(frame));\n\t\tif (bytes != sizeof(frame))\n\t\t\tbreak;\n\n\t\tif (fp < compat_ptr(regs->sp))\n\t\t\tbreak;\n\n\t\tperf_callchain_store(entry, frame.return_address);\n\t\tfp = compat_ptr(frame.next_frame);\n\t}\n\treturn 1;\n}\n#else\nstatic inline int\nperf_callchain_user32(struct pt_regs *regs, struct perf_callchain_entry *entry)\n{\n    return 0;\n}\n#endif\n\nvoid\nperf_callchain_user(struct perf_callchain_entry *entry, struct pt_regs *regs)\n{\n\tstruct stack_frame frame;\n\tconst void __user *fp;\n\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest()) {\n\t\t/* TODO: We don't support guest os callchain now */\n\t\treturn;\n\t}\n\n\tfp = (void __user *)regs->bp;\n\n\tperf_callchain_store(entry, regs->ip);\n\n\tif (perf_callchain_user32(regs, entry))\n\t\treturn;\n\n\twhile (entry->nr < PERF_MAX_STACK_DEPTH) {\n\t\tunsigned long bytes;\n\t\tframe.next_frame\t     = NULL;\n\t\tframe.return_address = 0;\n\n\t\tbytes = copy_from_user_nmi(&frame, fp, sizeof(frame));\n\t\tif (bytes != sizeof(frame))\n\t\t\tbreak;\n\n\t\tif ((unsigned long)fp < regs->sp)\n\t\t\tbreak;\n\n\t\tperf_callchain_store(entry, frame.return_address);\n\t\tfp = frame.next_frame;\n\t}\n}\n\nunsigned long perf_instruction_pointer(struct pt_regs *regs)\n{\n\tunsigned long ip;\n\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest())\n\t\tip = perf_guest_cbs->get_guest_ip();\n\telse\n\t\tip = instruction_pointer(regs);\n\n\treturn ip;\n}\n\nunsigned long perf_misc_flags(struct pt_regs *regs)\n{\n\tint misc = 0;\n\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest()) {\n\t\tif (perf_guest_cbs->is_user_mode())\n\t\t\tmisc |= PERF_RECORD_MISC_GUEST_USER;\n\t\telse\n\t\t\tmisc |= PERF_RECORD_MISC_GUEST_KERNEL;\n\t} else {\n\t\tif (user_mode(regs))\n\t\t\tmisc |= PERF_RECORD_MISC_USER;\n\t\telse\n\t\t\tmisc |= PERF_RECORD_MISC_KERNEL;\n\t}\n\n\tif (regs->flags & PERF_EFLAGS_EXACT)\n\t\tmisc |= PERF_RECORD_MISC_EXACT_IP;\n\n\treturn misc;\n}\n", "#ifdef CONFIG_CPU_SUP_INTEL\n\n#define MAX_EXTRA_REGS 2\n\n/*\n * Per register state.\n */\nstruct er_account {\n\tint\t\t\tref;\t\t/* reference count */\n\tunsigned int\t\textra_reg;\t/* extra MSR number */\n\tu64\t\t\textra_config;\t/* extra MSR config */\n};\n\n/*\n * Per core state\n * This used to coordinate shared registers for HT threads.\n */\nstruct intel_percore {\n\traw_spinlock_t\t\tlock;\t\t/* protect structure */\n\tstruct er_account\tregs[MAX_EXTRA_REGS];\n\tint\t\t\trefcnt;\t\t/* number of threads */\n\tunsigned\t\tcore_id;\n};\n\n/*\n * Intel PerfMon, used on Core and later.\n */\nstatic u64 intel_perfmon_event_map[PERF_COUNT_HW_MAX] __read_mostly =\n{\n  [PERF_COUNT_HW_CPU_CYCLES]\t\t= 0x003c,\n  [PERF_COUNT_HW_INSTRUCTIONS]\t\t= 0x00c0,\n  [PERF_COUNT_HW_CACHE_REFERENCES]\t= 0x4f2e,\n  [PERF_COUNT_HW_CACHE_MISSES]\t\t= 0x412e,\n  [PERF_COUNT_HW_BRANCH_INSTRUCTIONS]\t= 0x00c4,\n  [PERF_COUNT_HW_BRANCH_MISSES]\t\t= 0x00c5,\n  [PERF_COUNT_HW_BUS_CYCLES]\t\t= 0x013c,\n};\n\nstatic struct event_constraint intel_core_event_constraints[] __read_mostly =\n{\n\tINTEL_EVENT_CONSTRAINT(0x11, 0x2), /* FP_ASSIST */\n\tINTEL_EVENT_CONSTRAINT(0x12, 0x2), /* MUL */\n\tINTEL_EVENT_CONSTRAINT(0x13, 0x2), /* DIV */\n\tINTEL_EVENT_CONSTRAINT(0x14, 0x1), /* CYCLES_DIV_BUSY */\n\tINTEL_EVENT_CONSTRAINT(0x19, 0x2), /* DELAYED_BYPASS */\n\tINTEL_EVENT_CONSTRAINT(0xc1, 0x1), /* FP_COMP_INSTR_RET */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_core2_event_constraints[] __read_mostly =\n{\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0), /* INST_RETIRED.ANY */\n\tFIXED_EVENT_CONSTRAINT(0x003c, 1), /* CPU_CLK_UNHALTED.CORE */\n\t/*\n\t * Core2 has Fixed Counter 2 listed as CPU_CLK_UNHALTED.REF and event\n\t * 0x013c as CPU_CLK_UNHALTED.BUS and specifies there is a fixed\n\t * ratio between these counters.\n\t */\n\t/* FIXED_EVENT_CONSTRAINT(0x013c, 2),  CPU_CLK_UNHALTED.REF */\n\tINTEL_EVENT_CONSTRAINT(0x10, 0x1), /* FP_COMP_OPS_EXE */\n\tINTEL_EVENT_CONSTRAINT(0x11, 0x2), /* FP_ASSIST */\n\tINTEL_EVENT_CONSTRAINT(0x12, 0x2), /* MUL */\n\tINTEL_EVENT_CONSTRAINT(0x13, 0x2), /* DIV */\n\tINTEL_EVENT_CONSTRAINT(0x14, 0x1), /* CYCLES_DIV_BUSY */\n\tINTEL_EVENT_CONSTRAINT(0x18, 0x1), /* IDLE_DURING_DIV */\n\tINTEL_EVENT_CONSTRAINT(0x19, 0x2), /* DELAYED_BYPASS */\n\tINTEL_EVENT_CONSTRAINT(0xa1, 0x1), /* RS_UOPS_DISPATCH_CYCLES */\n\tINTEL_EVENT_CONSTRAINT(0xc9, 0x1), /* ITLB_MISS_RETIRED (T30-9) */\n\tINTEL_EVENT_CONSTRAINT(0xcb, 0x1), /* MEM_LOAD_RETIRED */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_nehalem_event_constraints[] __read_mostly =\n{\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0), /* INST_RETIRED.ANY */\n\tFIXED_EVENT_CONSTRAINT(0x003c, 1), /* CPU_CLK_UNHALTED.CORE */\n\t/* FIXED_EVENT_CONSTRAINT(0x013c, 2), CPU_CLK_UNHALTED.REF */\n\tINTEL_EVENT_CONSTRAINT(0x40, 0x3), /* L1D_CACHE_LD */\n\tINTEL_EVENT_CONSTRAINT(0x41, 0x3), /* L1D_CACHE_ST */\n\tINTEL_EVENT_CONSTRAINT(0x42, 0x3), /* L1D_CACHE_LOCK */\n\tINTEL_EVENT_CONSTRAINT(0x43, 0x3), /* L1D_ALL_REF */\n\tINTEL_EVENT_CONSTRAINT(0x48, 0x3), /* L1D_PEND_MISS */\n\tINTEL_EVENT_CONSTRAINT(0x4e, 0x3), /* L1D_PREFETCH */\n\tINTEL_EVENT_CONSTRAINT(0x51, 0x3), /* L1D */\n\tINTEL_EVENT_CONSTRAINT(0x63, 0x3), /* CACHE_LOCK_CYCLES */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct extra_reg intel_nehalem_extra_regs[] __read_mostly =\n{\n\tINTEL_EVENT_EXTRA_REG(0xb7, MSR_OFFCORE_RSP_0, 0xffff),\n\tEVENT_EXTRA_END\n};\n\nstatic struct event_constraint intel_nehalem_percore_constraints[] __read_mostly =\n{\n\tINTEL_EVENT_CONSTRAINT(0xb7, 0),\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_westmere_event_constraints[] __read_mostly =\n{\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0), /* INST_RETIRED.ANY */\n\tFIXED_EVENT_CONSTRAINT(0x003c, 1), /* CPU_CLK_UNHALTED.CORE */\n\t/* FIXED_EVENT_CONSTRAINT(0x013c, 2), CPU_CLK_UNHALTED.REF */\n\tINTEL_EVENT_CONSTRAINT(0x51, 0x3), /* L1D */\n\tINTEL_EVENT_CONSTRAINT(0x60, 0x1), /* OFFCORE_REQUESTS_OUTSTANDING */\n\tINTEL_EVENT_CONSTRAINT(0x63, 0x3), /* CACHE_LOCK_CYCLES */\n\tINTEL_EVENT_CONSTRAINT(0xb3, 0x1), /* SNOOPQ_REQUEST_OUTSTANDING */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_snb_event_constraints[] __read_mostly =\n{\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0), /* INST_RETIRED.ANY */\n\tFIXED_EVENT_CONSTRAINT(0x003c, 1), /* CPU_CLK_UNHALTED.CORE */\n\t/* FIXED_EVENT_CONSTRAINT(0x013c, 2), CPU_CLK_UNHALTED.REF */\n\tINTEL_EVENT_CONSTRAINT(0x48, 0x4), /* L1D_PEND_MISS.PENDING */\n\tINTEL_EVENT_CONSTRAINT(0xb7, 0x1), /* OFF_CORE_RESPONSE_0 */\n\tINTEL_EVENT_CONSTRAINT(0xbb, 0x8), /* OFF_CORE_RESPONSE_1 */\n\tINTEL_UEVENT_CONSTRAINT(0x01c0, 0x2), /* INST_RETIRED.PREC_DIST */\n\tINTEL_EVENT_CONSTRAINT(0xcd, 0x8), /* MEM_TRANS_RETIRED.LOAD_LATENCY */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct extra_reg intel_westmere_extra_regs[] __read_mostly =\n{\n\tINTEL_EVENT_EXTRA_REG(0xb7, MSR_OFFCORE_RSP_0, 0xffff),\n\tINTEL_EVENT_EXTRA_REG(0xbb, MSR_OFFCORE_RSP_1, 0xffff),\n\tEVENT_EXTRA_END\n};\n\nstatic struct event_constraint intel_westmere_percore_constraints[] __read_mostly =\n{\n\tINTEL_EVENT_CONSTRAINT(0xb7, 0),\n\tINTEL_EVENT_CONSTRAINT(0xbb, 0),\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_gen_event_constraints[] __read_mostly =\n{\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0), /* INST_RETIRED.ANY */\n\tFIXED_EVENT_CONSTRAINT(0x003c, 1), /* CPU_CLK_UNHALTED.CORE */\n\t/* FIXED_EVENT_CONSTRAINT(0x013c, 2), CPU_CLK_UNHALTED.REF */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic u64 intel_pmu_event_map(int hw_event)\n{\n\treturn intel_perfmon_event_map[hw_event];\n}\n\nstatic __initconst const u64 snb_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0xf1d0, /* MEM_UOP_RETIRED.LOADS        */\n\t\t[ C(RESULT_MISS)   ] = 0x0151, /* L1D.REPLACEMENT              */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0xf2d0, /* MEM_UOP_RETIRED.STORES       */\n\t\t[ C(RESULT_MISS)   ] = 0x0851, /* L1D.ALL_M_REPLACEMENT        */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x024e, /* HW_PRE_REQ.DL1_MISS          */\n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0280, /* ICACHE.MISSES */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t/* OFFCORE_RESPONSE.ANY_DATA.LOCAL_CACHE */\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t/* OFFCORE_RESPONSE.ANY_DATA.ANY_LLC_MISS */\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t/* OFFCORE_RESPONSE.ANY_RFO.LOCAL_CACHE */\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t/* OFFCORE_RESPONSE.ANY_RFO.ANY_LLC_MISS */\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t/* OFFCORE_RESPONSE.PREFETCH.LOCAL_CACHE */\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t/* OFFCORE_RESPONSE.PREFETCH.ANY_LLC_MISS */\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x81d0, /* MEM_UOP_RETIRED.ALL_LOADS */\n\t\t[ C(RESULT_MISS)   ] = 0x0108, /* DTLB_LOAD_MISSES.CAUSES_A_WALK */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x82d0, /* MEM_UOP_RETIRED.ALL_STORES */\n\t\t[ C(RESULT_MISS)   ] = 0x0149, /* DTLB_STORE_MISSES.MISS_CAUSES_A_WALK */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x1085, /* ITLB_MISSES.STLB_HIT         */\n\t\t[ C(RESULT_MISS)   ] = 0x0185, /* ITLB_MISSES.CAUSES_A_WALK    */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c4, /* BR_INST_RETIRED.ALL_BRANCHES */\n\t\t[ C(RESULT_MISS)   ] = 0x00c5, /* BR_MISP_RETIRED.ALL_BRANCHES */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n};\n\nstatic __initconst const u64 westmere_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x010b, /* MEM_INST_RETIRED.LOADS       */\n\t\t[ C(RESULT_MISS)   ] = 0x0151, /* L1D.REPL                     */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x020b, /* MEM_INST_RETURED.STORES      */\n\t\t[ C(RESULT_MISS)   ] = 0x0251, /* L1D.M_REPL                   */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x014e, /* L1D_PREFETCH.REQUESTS        */\n\t\t[ C(RESULT_MISS)   ] = 0x024e, /* L1D_PREFETCH.MISS            */\n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0380, /* L1I.READS                    */\n\t\t[ C(RESULT_MISS)   ] = 0x0280, /* L1I.MISSES                   */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t/* OFFCORE_RESPONSE.ANY_DATA.LOCAL_CACHE */\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t/* OFFCORE_RESPONSE.ANY_DATA.ANY_LLC_MISS */\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t/*\n\t * Use RFO, not WRITEBACK, because a write miss would typically occur\n\t * on RFO.\n\t */\n\t[ C(OP_WRITE) ] = {\n\t\t/* OFFCORE_RESPONSE.ANY_RFO.LOCAL_CACHE */\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t/* OFFCORE_RESPONSE.ANY_RFO.ANY_LLC_MISS */\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t/* OFFCORE_RESPONSE.PREFETCH.LOCAL_CACHE */\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t/* OFFCORE_RESPONSE.PREFETCH.ANY_LLC_MISS */\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x010b, /* MEM_INST_RETIRED.LOADS       */\n\t\t[ C(RESULT_MISS)   ] = 0x0108, /* DTLB_LOAD_MISSES.ANY         */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x020b, /* MEM_INST_RETURED.STORES      */\n\t\t[ C(RESULT_MISS)   ] = 0x010c, /* MEM_STORE_RETIRED.DTLB_MISS  */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x01c0, /* INST_RETIRED.ANY_P           */\n\t\t[ C(RESULT_MISS)   ] = 0x0185, /* ITLB_MISSES.ANY              */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c4, /* BR_INST_RETIRED.ALL_BRANCHES */\n\t\t[ C(RESULT_MISS)   ] = 0x03e8, /* BPU_CLEARS.ANY               */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n};\n\n/*\n * Nehalem/Westmere MSR_OFFCORE_RESPONSE bits;\n * See IA32 SDM Vol 3B 30.6.1.3\n */\n\n#define NHM_DMND_DATA_RD\t(1 << 0)\n#define NHM_DMND_RFO\t\t(1 << 1)\n#define NHM_DMND_IFETCH\t\t(1 << 2)\n#define NHM_DMND_WB\t\t(1 << 3)\n#define NHM_PF_DATA_RD\t\t(1 << 4)\n#define NHM_PF_DATA_RFO\t\t(1 << 5)\n#define NHM_PF_IFETCH\t\t(1 << 6)\n#define NHM_OFFCORE_OTHER\t(1 << 7)\n#define NHM_UNCORE_HIT\t\t(1 << 8)\n#define NHM_OTHER_CORE_HIT_SNP\t(1 << 9)\n#define NHM_OTHER_CORE_HITM\t(1 << 10)\n        \t\t\t/* reserved */\n#define NHM_REMOTE_CACHE_FWD\t(1 << 12)\n#define NHM_REMOTE_DRAM\t\t(1 << 13)\n#define NHM_LOCAL_DRAM\t\t(1 << 14)\n#define NHM_NON_DRAM\t\t(1 << 15)\n\n#define NHM_ALL_DRAM\t\t(NHM_REMOTE_DRAM|NHM_LOCAL_DRAM)\n\n#define NHM_DMND_READ\t\t(NHM_DMND_DATA_RD)\n#define NHM_DMND_WRITE\t\t(NHM_DMND_RFO|NHM_DMND_WB)\n#define NHM_DMND_PREFETCH\t(NHM_PF_DATA_RD|NHM_PF_DATA_RFO)\n\n#define NHM_L3_HIT\t(NHM_UNCORE_HIT|NHM_OTHER_CORE_HIT_SNP|NHM_OTHER_CORE_HITM)\n#define NHM_L3_MISS\t(NHM_NON_DRAM|NHM_ALL_DRAM|NHM_REMOTE_CACHE_FWD)\n#define NHM_L3_ACCESS\t(NHM_L3_HIT|NHM_L3_MISS)\n\nstatic __initconst const u64 nehalem_hw_cache_extra_regs\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = NHM_DMND_READ|NHM_L3_ACCESS,\n\t\t[ C(RESULT_MISS)   ] = NHM_DMND_READ|NHM_L3_MISS,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = NHM_DMND_WRITE|NHM_L3_ACCESS,\n\t\t[ C(RESULT_MISS)   ] = NHM_DMND_WRITE|NHM_L3_MISS,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = NHM_DMND_PREFETCH|NHM_L3_ACCESS,\n\t\t[ C(RESULT_MISS)   ] = NHM_DMND_PREFETCH|NHM_L3_MISS,\n\t},\n }\n};\n\nstatic __initconst const u64 nehalem_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x010b, /* MEM_INST_RETIRED.LOADS       */\n\t\t[ C(RESULT_MISS)   ] = 0x0151, /* L1D.REPL                     */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x020b, /* MEM_INST_RETURED.STORES      */\n\t\t[ C(RESULT_MISS)   ] = 0x0251, /* L1D.M_REPL                   */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x014e, /* L1D_PREFETCH.REQUESTS        */\n\t\t[ C(RESULT_MISS)   ] = 0x024e, /* L1D_PREFETCH.MISS            */\n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0380, /* L1I.READS                    */\n\t\t[ C(RESULT_MISS)   ] = 0x0280, /* L1I.MISSES                   */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t/* OFFCORE_RESPONSE.ANY_DATA.LOCAL_CACHE */\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t/* OFFCORE_RESPONSE.ANY_DATA.ANY_LLC_MISS */\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t/*\n\t * Use RFO, not WRITEBACK, because a write miss would typically occur\n\t * on RFO.\n\t */\n\t[ C(OP_WRITE) ] = {\n\t\t/* OFFCORE_RESPONSE.ANY_RFO.LOCAL_CACHE */\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t/* OFFCORE_RESPONSE.ANY_RFO.ANY_LLC_MISS */\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t/* OFFCORE_RESPONSE.PREFETCH.LOCAL_CACHE */\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t/* OFFCORE_RESPONSE.PREFETCH.ANY_LLC_MISS */\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0f40, /* L1D_CACHE_LD.MESI   (alias)  */\n\t\t[ C(RESULT_MISS)   ] = 0x0108, /* DTLB_LOAD_MISSES.ANY         */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0f41, /* L1D_CACHE_ST.MESI   (alias)  */\n\t\t[ C(RESULT_MISS)   ] = 0x010c, /* MEM_STORE_RETIRED.DTLB_MISS  */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x01c0, /* INST_RETIRED.ANY_P           */\n\t\t[ C(RESULT_MISS)   ] = 0x20c8, /* ITLB_MISS_RETIRED            */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c4, /* BR_INST_RETIRED.ALL_BRANCHES */\n\t\t[ C(RESULT_MISS)   ] = 0x03e8, /* BPU_CLEARS.ANY               */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n};\n\nstatic __initconst const u64 core2_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0f40, /* L1D_CACHE_LD.MESI          */\n\t\t[ C(RESULT_MISS)   ] = 0x0140, /* L1D_CACHE_LD.I_STATE       */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0f41, /* L1D_CACHE_ST.MESI          */\n\t\t[ C(RESULT_MISS)   ] = 0x0141, /* L1D_CACHE_ST.I_STATE       */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x104e, /* L1D_PREFETCH.REQUESTS      */\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0080, /* L1I.READS                  */\n\t\t[ C(RESULT_MISS)   ] = 0x0081, /* L1I.MISSES                 */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x4f29, /* L2_LD.MESI                 */\n\t\t[ C(RESULT_MISS)   ] = 0x4129, /* L2_LD.ISTATE               */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x4f2A, /* L2_ST.MESI                 */\n\t\t[ C(RESULT_MISS)   ] = 0x412A, /* L2_ST.ISTATE               */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0f40, /* L1D_CACHE_LD.MESI  (alias) */\n\t\t[ C(RESULT_MISS)   ] = 0x0208, /* DTLB_MISSES.MISS_LD        */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0f41, /* L1D_CACHE_ST.MESI  (alias) */\n\t\t[ C(RESULT_MISS)   ] = 0x0808, /* DTLB_MISSES.MISS_ST        */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c0, /* INST_RETIRED.ANY_P         */\n\t\t[ C(RESULT_MISS)   ] = 0x1282, /* ITLBMISSES                 */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c4, /* BR_INST_RETIRED.ANY        */\n\t\t[ C(RESULT_MISS)   ] = 0x00c5, /* BP_INST_RETIRED.MISPRED    */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n};\n\nstatic __initconst const u64 atom_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x2140, /* L1D_CACHE.LD               */\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x2240, /* L1D_CACHE.ST               */\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0380, /* L1I.READS                  */\n\t\t[ C(RESULT_MISS)   ] = 0x0280, /* L1I.MISSES                 */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x4f29, /* L2_LD.MESI                 */\n\t\t[ C(RESULT_MISS)   ] = 0x4129, /* L2_LD.ISTATE               */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x4f2A, /* L2_ST.MESI                 */\n\t\t[ C(RESULT_MISS)   ] = 0x412A, /* L2_ST.ISTATE               */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x2140, /* L1D_CACHE_LD.MESI  (alias) */\n\t\t[ C(RESULT_MISS)   ] = 0x0508, /* DTLB_MISSES.MISS_LD        */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x2240, /* L1D_CACHE_ST.MESI  (alias) */\n\t\t[ C(RESULT_MISS)   ] = 0x0608, /* DTLB_MISSES.MISS_ST        */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c0, /* INST_RETIRED.ANY_P         */\n\t\t[ C(RESULT_MISS)   ] = 0x0282, /* ITLB.MISSES                */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c4, /* BR_INST_RETIRED.ANY        */\n\t\t[ C(RESULT_MISS)   ] = 0x00c5, /* BP_INST_RETIRED.MISPRED    */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n};\n\nstatic void intel_pmu_disable_all(void)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\twrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, 0);\n\n\tif (test_bit(X86_PMC_IDX_FIXED_BTS, cpuc->active_mask))\n\t\tintel_pmu_disable_bts();\n\n\tintel_pmu_pebs_disable_all();\n\tintel_pmu_lbr_disable_all();\n}\n\nstatic void intel_pmu_enable_all(int added)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\tintel_pmu_pebs_enable_all();\n\tintel_pmu_lbr_enable_all();\n\twrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, x86_pmu.intel_ctrl);\n\n\tif (test_bit(X86_PMC_IDX_FIXED_BTS, cpuc->active_mask)) {\n\t\tstruct perf_event *event =\n\t\t\tcpuc->events[X86_PMC_IDX_FIXED_BTS];\n\n\t\tif (WARN_ON_ONCE(!event))\n\t\t\treturn;\n\n\t\tintel_pmu_enable_bts(event->hw.config);\n\t}\n}\n\n/*\n * Workaround for:\n *   Intel Errata AAK100 (model 26)\n *   Intel Errata AAP53  (model 30)\n *   Intel Errata BD53   (model 44)\n *\n * The official story:\n *   These chips need to be 'reset' when adding counters by programming the\n *   magic three (non-counting) events 0x4300B5, 0x4300D2, and 0x4300B1 either\n *   in sequence on the same PMC or on different PMCs.\n *\n * In practise it appears some of these events do in fact count, and\n * we need to programm all 4 events.\n */\nstatic void intel_pmu_nhm_workaround(void)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstatic const unsigned long nhm_magic[4] = {\n\t\t0x4300B5,\n\t\t0x4300D2,\n\t\t0x4300B1,\n\t\t0x4300B1\n\t};\n\tstruct perf_event *event;\n\tint i;\n\n\t/*\n\t * The Errata requires below steps:\n\t * 1) Clear MSR_IA32_PEBS_ENABLE and MSR_CORE_PERF_GLOBAL_CTRL;\n\t * 2) Configure 4 PERFEVTSELx with the magic events and clear\n\t *    the corresponding PMCx;\n\t * 3) set bit0~bit3 of MSR_CORE_PERF_GLOBAL_CTRL;\n\t * 4) Clear MSR_CORE_PERF_GLOBAL_CTRL;\n\t * 5) Clear 4 pairs of ERFEVTSELx and PMCx;\n\t */\n\n\t/*\n\t * The real steps we choose are a little different from above.\n\t * A) To reduce MSR operations, we don't run step 1) as they\n\t *    are already cleared before this function is called;\n\t * B) Call x86_perf_event_update to save PMCx before configuring\n\t *    PERFEVTSELx with magic number;\n\t * C) With step 5), we do clear only when the PERFEVTSELx is\n\t *    not used currently.\n\t * D) Call x86_perf_event_set_period to restore PMCx;\n\t */\n\n\t/* We always operate 4 pairs of PERF Counters */\n\tfor (i = 0; i < 4; i++) {\n\t\tevent = cpuc->events[i];\n\t\tif (event)\n\t\t\tx86_perf_event_update(event);\n\t}\n\n\tfor (i = 0; i < 4; i++) {\n\t\twrmsrl(MSR_ARCH_PERFMON_EVENTSEL0 + i, nhm_magic[i]);\n\t\twrmsrl(MSR_ARCH_PERFMON_PERFCTR0 + i, 0x0);\n\t}\n\n\twrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, 0xf);\n\twrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, 0x0);\n\n\tfor (i = 0; i < 4; i++) {\n\t\tevent = cpuc->events[i];\n\n\t\tif (event) {\n\t\t\tx86_perf_event_set_period(event);\n\t\t\t__x86_pmu_enable_event(&event->hw,\n\t\t\t\t\tARCH_PERFMON_EVENTSEL_ENABLE);\n\t\t} else\n\t\t\twrmsrl(MSR_ARCH_PERFMON_EVENTSEL0 + i, 0x0);\n\t}\n}\n\nstatic void intel_pmu_nhm_enable_all(int added)\n{\n\tif (added)\n\t\tintel_pmu_nhm_workaround();\n\tintel_pmu_enable_all(added);\n}\n\nstatic inline u64 intel_pmu_get_status(void)\n{\n\tu64 status;\n\n\trdmsrl(MSR_CORE_PERF_GLOBAL_STATUS, status);\n\n\treturn status;\n}\n\nstatic inline void intel_pmu_ack_status(u64 ack)\n{\n\twrmsrl(MSR_CORE_PERF_GLOBAL_OVF_CTRL, ack);\n}\n\nstatic void intel_pmu_disable_fixed(struct hw_perf_event *hwc)\n{\n\tint idx = hwc->idx - X86_PMC_IDX_FIXED;\n\tu64 ctrl_val, mask;\n\n\tmask = 0xfULL << (idx * 4);\n\n\trdmsrl(hwc->config_base, ctrl_val);\n\tctrl_val &= ~mask;\n\twrmsrl(hwc->config_base, ctrl_val);\n}\n\nstatic void intel_pmu_disable_event(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (unlikely(hwc->idx == X86_PMC_IDX_FIXED_BTS)) {\n\t\tintel_pmu_disable_bts();\n\t\tintel_pmu_drain_bts_buffer();\n\t\treturn;\n\t}\n\n\tif (unlikely(hwc->config_base == MSR_ARCH_PERFMON_FIXED_CTR_CTRL)) {\n\t\tintel_pmu_disable_fixed(hwc);\n\t\treturn;\n\t}\n\n\tx86_pmu_disable_event(event);\n\n\tif (unlikely(event->attr.precise_ip))\n\t\tintel_pmu_pebs_disable(event);\n}\n\nstatic void intel_pmu_enable_fixed(struct hw_perf_event *hwc)\n{\n\tint idx = hwc->idx - X86_PMC_IDX_FIXED;\n\tu64 ctrl_val, bits, mask;\n\n\t/*\n\t * Enable IRQ generation (0x8),\n\t * and enable ring-3 counting (0x2) and ring-0 counting (0x1)\n\t * if requested:\n\t */\n\tbits = 0x8ULL;\n\tif (hwc->config & ARCH_PERFMON_EVENTSEL_USR)\n\t\tbits |= 0x2;\n\tif (hwc->config & ARCH_PERFMON_EVENTSEL_OS)\n\t\tbits |= 0x1;\n\n\t/*\n\t * ANY bit is supported in v3 and up\n\t */\n\tif (x86_pmu.version > 2 && hwc->config & ARCH_PERFMON_EVENTSEL_ANY)\n\t\tbits |= 0x4;\n\n\tbits <<= (idx * 4);\n\tmask = 0xfULL << (idx * 4);\n\n\trdmsrl(hwc->config_base, ctrl_val);\n\tctrl_val &= ~mask;\n\tctrl_val |= bits;\n\twrmsrl(hwc->config_base, ctrl_val);\n}\n\nstatic void intel_pmu_enable_event(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (unlikely(hwc->idx == X86_PMC_IDX_FIXED_BTS)) {\n\t\tif (!__this_cpu_read(cpu_hw_events.enabled))\n\t\t\treturn;\n\n\t\tintel_pmu_enable_bts(hwc->config);\n\t\treturn;\n\t}\n\n\tif (unlikely(hwc->config_base == MSR_ARCH_PERFMON_FIXED_CTR_CTRL)) {\n\t\tintel_pmu_enable_fixed(hwc);\n\t\treturn;\n\t}\n\n\tif (unlikely(event->attr.precise_ip))\n\t\tintel_pmu_pebs_enable(event);\n\n\t__x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);\n}\n\n/*\n * Save and restart an expired event. Called by NMI contexts,\n * so it has to be careful about preempting normal event ops:\n */\nstatic int intel_pmu_save_and_restart(struct perf_event *event)\n{\n\tx86_perf_event_update(event);\n\treturn x86_perf_event_set_period(event);\n}\n\nstatic void intel_pmu_reset(void)\n{\n\tstruct debug_store *ds = __this_cpu_read(cpu_hw_events.ds);\n\tunsigned long flags;\n\tint idx;\n\n\tif (!x86_pmu.num_counters)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tprintk(\"clearing PMU state on CPU#%d\\n\", smp_processor_id());\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tchecking_wrmsrl(x86_pmu_config_addr(idx), 0ull);\n\t\tchecking_wrmsrl(x86_pmu_event_addr(idx),  0ull);\n\t}\n\tfor (idx = 0; idx < x86_pmu.num_counters_fixed; idx++)\n\t\tchecking_wrmsrl(MSR_ARCH_PERFMON_FIXED_CTR0 + idx, 0ull);\n\n\tif (ds)\n\t\tds->bts_index = ds->bts_buffer_base;\n\n\tlocal_irq_restore(flags);\n}\n\n/*\n * This handler is triggered by the local APIC, so the APIC IRQ handling\n * rules apply:\n */\nstatic int intel_pmu_handle_irq(struct pt_regs *regs)\n{\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tint bit, loops;\n\tu64 status;\n\tint handled;\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\n\t/*\n\t * Some chipsets need to unmask the LVTPC in a particular spot\n\t * inside the nmi handler.  As a result, the unmasking was pushed\n\t * into all the nmi handlers.\n\t *\n\t * This handler doesn't seem to have any issues with the unmasking\n\t * so it was left at the top.\n\t */\n\tapic_write(APIC_LVTPC, APIC_DM_NMI);\n\n\tintel_pmu_disable_all();\n\thandled = intel_pmu_drain_bts_buffer();\n\tstatus = intel_pmu_get_status();\n\tif (!status) {\n\t\tintel_pmu_enable_all(0);\n\t\treturn handled;\n\t}\n\n\tloops = 0;\nagain:\n\tintel_pmu_ack_status(status);\n\tif (++loops > 100) {\n\t\tWARN_ONCE(1, \"perfevents: irq loop stuck!\\n\");\n\t\tperf_event_print_debug();\n\t\tintel_pmu_reset();\n\t\tgoto done;\n\t}\n\n\tinc_irq_stat(apic_perf_irqs);\n\n\tintel_pmu_lbr_read();\n\n\t/*\n\t * PEBS overflow sets bit 62 in the global status register\n\t */\n\tif (__test_and_clear_bit(62, (unsigned long *)&status)) {\n\t\thandled++;\n\t\tx86_pmu.drain_pebs(regs);\n\t}\n\n\tfor_each_set_bit(bit, (unsigned long *)&status, X86_PMC_IDX_MAX) {\n\t\tstruct perf_event *event = cpuc->events[bit];\n\n\t\thandled++;\n\n\t\tif (!test_bit(bit, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\tif (!intel_pmu_save_and_restart(event))\n\t\t\tcontinue;\n\n\t\tdata.period = event->hw.last_period;\n\n\t\tif (perf_event_overflow(event, 1, &data, regs))\n\t\t\tx86_pmu_stop(event, 0);\n\t}\n\n\t/*\n\t * Repeat if there is more work to be done:\n\t */\n\tstatus = intel_pmu_get_status();\n\tif (status)\n\t\tgoto again;\n\ndone:\n\tintel_pmu_enable_all(0);\n\treturn handled;\n}\n\nstatic struct event_constraint *\nintel_bts_constraints(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tunsigned int hw_event, bts_event;\n\n\tif (event->attr.freq)\n\t\treturn NULL;\n\n\thw_event = hwc->config & INTEL_ARCH_EVENT_MASK;\n\tbts_event = x86_pmu.event_map(PERF_COUNT_HW_BRANCH_INSTRUCTIONS);\n\n\tif (unlikely(hw_event == bts_event && hwc->sample_period == 1))\n\t\treturn &bts_constraint;\n\n\treturn NULL;\n}\n\nstatic struct event_constraint *\nintel_percore_constraints(struct cpu_hw_events *cpuc, struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tunsigned int e = hwc->config & ARCH_PERFMON_EVENTSEL_EVENT;\n\tstruct event_constraint *c;\n\tstruct intel_percore *pc;\n\tstruct er_account *era;\n\tint i;\n\tint free_slot;\n\tint found;\n\n\tif (!x86_pmu.percore_constraints || hwc->extra_alloc)\n\t\treturn NULL;\n\n\tfor (c = x86_pmu.percore_constraints; c->cmask; c++) {\n\t\tif (e != c->code)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Allocate resource per core.\n\t\t */\n\t\tpc = cpuc->per_core;\n\t\tif (!pc)\n\t\t\tbreak;\n\t\tc = &emptyconstraint;\n\t\traw_spin_lock(&pc->lock);\n\t\tfree_slot = -1;\n\t\tfound = 0;\n\t\tfor (i = 0; i < MAX_EXTRA_REGS; i++) {\n\t\t\tera = &pc->regs[i];\n\t\t\tif (era->ref > 0 && hwc->extra_reg == era->extra_reg) {\n\t\t\t\t/* Allow sharing same config */\n\t\t\t\tif (hwc->extra_config == era->extra_config) {\n\t\t\t\t\tera->ref++;\n\t\t\t\t\tcpuc->percore_used = 1;\n\t\t\t\t\thwc->extra_alloc = 1;\n\t\t\t\t\tc = NULL;\n\t\t\t\t}\n\t\t\t\t/* else conflict */\n\t\t\t\tfound = 1;\n\t\t\t\tbreak;\n\t\t\t} else if (era->ref == 0 && free_slot == -1)\n\t\t\t\tfree_slot = i;\n\t\t}\n\t\tif (!found && free_slot != -1) {\n\t\t\tera = &pc->regs[free_slot];\n\t\t\tera->ref = 1;\n\t\t\tera->extra_reg = hwc->extra_reg;\n\t\t\tera->extra_config = hwc->extra_config;\n\t\t\tcpuc->percore_used = 1;\n\t\t\thwc->extra_alloc = 1;\n\t\t\tc = NULL;\n\t\t}\n\t\traw_spin_unlock(&pc->lock);\n\t\treturn c;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct event_constraint *\nintel_get_event_constraints(struct cpu_hw_events *cpuc, struct perf_event *event)\n{\n\tstruct event_constraint *c;\n\n\tc = intel_bts_constraints(event);\n\tif (c)\n\t\treturn c;\n\n\tc = intel_pebs_constraints(event);\n\tif (c)\n\t\treturn c;\n\n\tc = intel_percore_constraints(cpuc, event);\n\tif (c)\n\t\treturn c;\n\n\treturn x86_get_event_constraints(cpuc, event);\n}\n\nstatic void intel_put_event_constraints(struct cpu_hw_events *cpuc,\n\t\t\t\t\tstruct perf_event *event)\n{\n\tstruct extra_reg *er;\n\tstruct intel_percore *pc;\n\tstruct er_account *era;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint i, allref;\n\n\tif (!cpuc->percore_used)\n\t\treturn;\n\n\tfor (er = x86_pmu.extra_regs; er->msr; er++) {\n\t\tif (er->event != (hwc->config & er->config_mask))\n\t\t\tcontinue;\n\n\t\tpc = cpuc->per_core;\n\t\traw_spin_lock(&pc->lock);\n\t\tfor (i = 0; i < MAX_EXTRA_REGS; i++) {\n\t\t\tera = &pc->regs[i];\n\t\t\tif (era->ref > 0 &&\n\t\t\t    era->extra_config == hwc->extra_config &&\n\t\t\t    era->extra_reg == er->msr) {\n\t\t\t\tera->ref--;\n\t\t\t\thwc->extra_alloc = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tallref = 0;\n\t\tfor (i = 0; i < MAX_EXTRA_REGS; i++)\n\t\t\tallref += pc->regs[i].ref;\n\t\tif (allref == 0)\n\t\t\tcpuc->percore_used = 0;\n\t\traw_spin_unlock(&pc->lock);\n\t\tbreak;\n\t}\n}\n\nstatic int intel_pmu_hw_config(struct perf_event *event)\n{\n\tint ret = x86_pmu_hw_config(event);\n\n\tif (ret)\n\t\treturn ret;\n\n\tif (event->attr.precise_ip &&\n\t    (event->hw.config & X86_RAW_EVENT_MASK) == 0x003c) {\n\t\t/*\n\t\t * Use an alternative encoding for CPU_CLK_UNHALTED.THREAD_P\n\t\t * (0x003c) so that we can use it with PEBS.\n\t\t *\n\t\t * The regular CPU_CLK_UNHALTED.THREAD_P event (0x003c) isn't\n\t\t * PEBS capable. However we can use INST_RETIRED.ANY_P\n\t\t * (0x00c0), which is a PEBS capable event, to get the same\n\t\t * count.\n\t\t *\n\t\t * INST_RETIRED.ANY_P counts the number of cycles that retires\n\t\t * CNTMASK instructions. By setting CNTMASK to a value (16)\n\t\t * larger than the maximum number of instructions that can be\n\t\t * retired per cycle (4) and then inverting the condition, we\n\t\t * count all cycles that retire 16 or less instructions, which\n\t\t * is every cycle.\n\t\t *\n\t\t * Thereby we gain a PEBS capable cycle counter.\n\t\t */\n\t\tu64 alt_config = 0x108000c0; /* INST_RETIRED.TOTAL_CYCLES */\n\n\t\talt_config |= (event->hw.config & ~X86_RAW_EVENT_MASK);\n\t\tevent->hw.config = alt_config;\n\t}\n\n\tif (event->attr.type != PERF_TYPE_RAW)\n\t\treturn 0;\n\n\tif (!(event->attr.config & ARCH_PERFMON_EVENTSEL_ANY))\n\t\treturn 0;\n\n\tif (x86_pmu.version < 3)\n\t\treturn -EINVAL;\n\n\tif (perf_paranoid_cpu() && !capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\n\tevent->hw.config |= ARCH_PERFMON_EVENTSEL_ANY;\n\n\treturn 0;\n}\n\nstatic __initconst const struct x86_pmu core_pmu = {\n\t.name\t\t\t= \"core\",\n\t.handle_irq\t\t= x86_pmu_handle_irq,\n\t.disable_all\t\t= x86_pmu_disable_all,\n\t.enable_all\t\t= x86_pmu_enable_all,\n\t.enable\t\t\t= x86_pmu_enable_event,\n\t.disable\t\t= x86_pmu_disable_event,\n\t.hw_config\t\t= x86_pmu_hw_config,\n\t.schedule_events\t= x86_schedule_events,\n\t.eventsel\t\t= MSR_ARCH_PERFMON_EVENTSEL0,\n\t.perfctr\t\t= MSR_ARCH_PERFMON_PERFCTR0,\n\t.event_map\t\t= intel_pmu_event_map,\n\t.max_events\t\t= ARRAY_SIZE(intel_perfmon_event_map),\n\t.apic\t\t\t= 1,\n\t/*\n\t * Intel PMCs cannot be accessed sanely above 32 bit width,\n\t * so we install an artificial 1<<31 period regardless of\n\t * the generic event period:\n\t */\n\t.max_period\t\t= (1ULL << 31) - 1,\n\t.get_event_constraints\t= intel_get_event_constraints,\n\t.put_event_constraints\t= intel_put_event_constraints,\n\t.event_constraints\t= intel_core_event_constraints,\n};\n\nstatic int intel_pmu_cpu_prepare(int cpu)\n{\n\tstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\n\n\tif (!cpu_has_ht_siblings())\n\t\treturn NOTIFY_OK;\n\n\tcpuc->per_core = kzalloc_node(sizeof(struct intel_percore),\n\t\t\t\t      GFP_KERNEL, cpu_to_node(cpu));\n\tif (!cpuc->per_core)\n\t\treturn NOTIFY_BAD;\n\n\traw_spin_lock_init(&cpuc->per_core->lock);\n\tcpuc->per_core->core_id = -1;\n\treturn NOTIFY_OK;\n}\n\nstatic void intel_pmu_cpu_starting(int cpu)\n{\n\tstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\n\tint core_id = topology_core_id(cpu);\n\tint i;\n\n\tinit_debug_store_on_cpu(cpu);\n\t/*\n\t * Deal with CPUs that don't clear their LBRs on power-up.\n\t */\n\tintel_pmu_lbr_reset();\n\n\tif (!cpu_has_ht_siblings())\n\t\treturn;\n\n\tfor_each_cpu(i, topology_thread_cpumask(cpu)) {\n\t\tstruct intel_percore *pc = per_cpu(cpu_hw_events, i).per_core;\n\n\t\tif (pc && pc->core_id == core_id) {\n\t\t\tkfree(cpuc->per_core);\n\t\t\tcpuc->per_core = pc;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tcpuc->per_core->core_id = core_id;\n\tcpuc->per_core->refcnt++;\n}\n\nstatic void intel_pmu_cpu_dying(int cpu)\n{\n\tstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\n\tstruct intel_percore *pc = cpuc->per_core;\n\n\tif (pc) {\n\t\tif (pc->core_id == -1 || --pc->refcnt == 0)\n\t\t\tkfree(pc);\n\t\tcpuc->per_core = NULL;\n\t}\n\n\tfini_debug_store_on_cpu(cpu);\n}\n\nstatic __initconst const struct x86_pmu intel_pmu = {\n\t.name\t\t\t= \"Intel\",\n\t.handle_irq\t\t= intel_pmu_handle_irq,\n\t.disable_all\t\t= intel_pmu_disable_all,\n\t.enable_all\t\t= intel_pmu_enable_all,\n\t.enable\t\t\t= intel_pmu_enable_event,\n\t.disable\t\t= intel_pmu_disable_event,\n\t.hw_config\t\t= intel_pmu_hw_config,\n\t.schedule_events\t= x86_schedule_events,\n\t.eventsel\t\t= MSR_ARCH_PERFMON_EVENTSEL0,\n\t.perfctr\t\t= MSR_ARCH_PERFMON_PERFCTR0,\n\t.event_map\t\t= intel_pmu_event_map,\n\t.max_events\t\t= ARRAY_SIZE(intel_perfmon_event_map),\n\t.apic\t\t\t= 1,\n\t/*\n\t * Intel PMCs cannot be accessed sanely above 32 bit width,\n\t * so we install an artificial 1<<31 period regardless of\n\t * the generic event period:\n\t */\n\t.max_period\t\t= (1ULL << 31) - 1,\n\t.get_event_constraints\t= intel_get_event_constraints,\n\t.put_event_constraints\t= intel_put_event_constraints,\n\n\t.cpu_prepare\t\t= intel_pmu_cpu_prepare,\n\t.cpu_starting\t\t= intel_pmu_cpu_starting,\n\t.cpu_dying\t\t= intel_pmu_cpu_dying,\n};\n\nstatic void intel_clovertown_quirks(void)\n{\n\t/*\n\t * PEBS is unreliable due to:\n\t *\n\t *   AJ67  - PEBS may experience CPL leaks\n\t *   AJ68  - PEBS PMI may be delayed by one event\n\t *   AJ69  - GLOBAL_STATUS[62] will only be set when DEBUGCTL[12]\n\t *   AJ106 - FREEZE_LBRS_ON_PMI doesn't work in combination with PEBS\n\t *\n\t * AJ67 could be worked around by restricting the OS/USR flags.\n\t * AJ69 could be worked around by setting PMU_FREEZE_ON_PMI.\n\t *\n\t * AJ106 could possibly be worked around by not allowing LBR\n\t *       usage from PEBS, including the fixup.\n\t * AJ68  could possibly be worked around by always programming\n\t *\t a pebs_event_reset[0] value and coping with the lost events.\n\t *\n\t * But taken together it might just make sense to not enable PEBS on\n\t * these chips.\n\t */\n\tprintk(KERN_WARNING \"PEBS disabled due to CPU errata.\\n\");\n\tx86_pmu.pebs = 0;\n\tx86_pmu.pebs_constraints = NULL;\n}\n\nstatic __init int intel_pmu_init(void)\n{\n\tunion cpuid10_edx edx;\n\tunion cpuid10_eax eax;\n\tunsigned int unused;\n\tunsigned int ebx;\n\tint version;\n\n\tif (!cpu_has(&boot_cpu_data, X86_FEATURE_ARCH_PERFMON)) {\n\t\tswitch (boot_cpu_data.x86) {\n\t\tcase 0x6:\n\t\t\treturn p6_pmu_init();\n\t\tcase 0xf:\n\t\t\treturn p4_pmu_init();\n\t\t}\n\t\treturn -ENODEV;\n\t}\n\n\t/*\n\t * Check whether the Architectural PerfMon supports\n\t * Branch Misses Retired hw_event or not.\n\t */\n\tcpuid(10, &eax.full, &ebx, &unused, &edx.full);\n\tif (eax.split.mask_length <= ARCH_PERFMON_BRANCH_MISSES_RETIRED)\n\t\treturn -ENODEV;\n\n\tversion = eax.split.version_id;\n\tif (version < 2)\n\t\tx86_pmu = core_pmu;\n\telse\n\t\tx86_pmu = intel_pmu;\n\n\tx86_pmu.version\t\t\t= version;\n\tx86_pmu.num_counters\t\t= eax.split.num_counters;\n\tx86_pmu.cntval_bits\t\t= eax.split.bit_width;\n\tx86_pmu.cntval_mask\t\t= (1ULL << eax.split.bit_width) - 1;\n\n\t/*\n\t * Quirk: v2 perfmon does not report fixed-purpose events, so\n\t * assume at least 3 events:\n\t */\n\tif (version > 1)\n\t\tx86_pmu.num_counters_fixed = max((int)edx.split.num_counters_fixed, 3);\n\n\t/*\n\t * v2 and above have a perf capabilities MSR\n\t */\n\tif (version > 1) {\n\t\tu64 capabilities;\n\n\t\trdmsrl(MSR_IA32_PERF_CAPABILITIES, capabilities);\n\t\tx86_pmu.intel_cap.capabilities = capabilities;\n\t}\n\n\tintel_ds_init();\n\n\t/*\n\t * Install the hw-cache-events table:\n\t */\n\tswitch (boot_cpu_data.x86_model) {\n\tcase 14: /* 65 nm core solo/duo, \"Yonah\" */\n\t\tpr_cont(\"Core events, \");\n\t\tbreak;\n\n\tcase 15: /* original 65 nm celeron/pentium/core2/xeon, \"Merom\"/\"Conroe\" */\n\t\tx86_pmu.quirks = intel_clovertown_quirks;\n\tcase 22: /* single-core 65 nm celeron/core2solo \"Merom-L\"/\"Conroe-L\" */\n\tcase 23: /* current 45 nm celeron/core2/xeon \"Penryn\"/\"Wolfdale\" */\n\tcase 29: /* six-core 45 nm xeon \"Dunnington\" */\n\t\tmemcpy(hw_cache_event_ids, core2_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\n\t\tintel_pmu_lbr_init_core();\n\n\t\tx86_pmu.event_constraints = intel_core2_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_core2_pebs_event_constraints;\n\t\tpr_cont(\"Core2 events, \");\n\t\tbreak;\n\n\tcase 26: /* 45 nm nehalem, \"Bloomfield\" */\n\tcase 30: /* 45 nm nehalem, \"Lynnfield\" */\n\tcase 46: /* 45 nm nehalem-ex, \"Beckton\" */\n\t\tmemcpy(hw_cache_event_ids, nehalem_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, nehalem_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_nhm();\n\n\t\tx86_pmu.event_constraints = intel_nehalem_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_nehalem_pebs_event_constraints;\n\t\tx86_pmu.percore_constraints = intel_nehalem_percore_constraints;\n\t\tx86_pmu.enable_all = intel_pmu_nhm_enable_all;\n\t\tx86_pmu.extra_regs = intel_nehalem_extra_regs;\n\n\t\t/* UOPS_ISSUED.STALLED_CYCLES */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] = 0x180010e;\n\t\t/* UOPS_EXECUTED.CORE_ACTIVE_CYCLES,c=1,i=1 */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_BACKEND] = 0x1803fb1;\n\n\t\tif (ebx & 0x40) {\n\t\t\t/*\n\t\t\t * Erratum AAJ80 detected, we work it around by using\n\t\t\t * the BR_MISP_EXEC.ANY event. This will over-count\n\t\t\t * branch-misses, but it's still much better than the\n\t\t\t * architectural event which is often completely bogus:\n\t\t\t */\n\t\t\tintel_perfmon_event_map[PERF_COUNT_HW_BRANCH_MISSES] = 0x7f89;\n\n\t\t\tpr_cont(\"erratum AAJ80 worked around, \");\n\t\t}\n\t\tpr_cont(\"Nehalem events, \");\n\t\tbreak;\n\n\tcase 28: /* Atom */\n\t\tmemcpy(hw_cache_event_ids, atom_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\n\t\tintel_pmu_lbr_init_atom();\n\n\t\tx86_pmu.event_constraints = intel_gen_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_atom_pebs_event_constraints;\n\t\tpr_cont(\"Atom events, \");\n\t\tbreak;\n\n\tcase 37: /* 32 nm nehalem, \"Clarkdale\" */\n\tcase 44: /* 32 nm nehalem, \"Gulftown\" */\n\tcase 47: /* 32 nm Xeon E7 */\n\t\tmemcpy(hw_cache_event_ids, westmere_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, nehalem_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_nhm();\n\n\t\tx86_pmu.event_constraints = intel_westmere_event_constraints;\n\t\tx86_pmu.percore_constraints = intel_westmere_percore_constraints;\n\t\tx86_pmu.enable_all = intel_pmu_nhm_enable_all;\n\t\tx86_pmu.pebs_constraints = intel_westmere_pebs_event_constraints;\n\t\tx86_pmu.extra_regs = intel_westmere_extra_regs;\n\n\t\t/* UOPS_ISSUED.STALLED_CYCLES */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] = 0x180010e;\n\t\t/* UOPS_EXECUTED.CORE_ACTIVE_CYCLES,c=1,i=1 */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_BACKEND] = 0x1803fb1;\n\n\t\tpr_cont(\"Westmere events, \");\n\t\tbreak;\n\n\tcase 42: /* SandyBridge */\n\t\tmemcpy(hw_cache_event_ids, snb_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\n\t\tintel_pmu_lbr_init_nhm();\n\n\t\tx86_pmu.event_constraints = intel_snb_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_snb_pebs_events;\n\n\t\t/* UOPS_ISSUED.ANY,c=1,i=1 to count stall cycles */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] = 0x180010e;\n\t\t/* UOPS_DISPATCHED.THREAD,c=1,i=1 to count stall cycles*/\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_BACKEND] = 0x18001b1;\n\n\t\tpr_cont(\"SandyBridge events, \");\n\t\tbreak;\n\n\tdefault:\n\t\t/*\n\t\t * default constraints for v2 and up\n\t\t */\n\t\tx86_pmu.event_constraints = intel_gen_event_constraints;\n\t\tpr_cont(\"generic architected perfmon, \");\n\t}\n\treturn 0;\n}\n\n#else /* CONFIG_CPU_SUP_INTEL */\n\nstatic int intel_pmu_init(void)\n{\n\treturn 0;\n}\n\n#endif /* CONFIG_CPU_SUP_INTEL */\n", "#ifdef CONFIG_CPU_SUP_INTEL\n\n/* The maximal number of PEBS events: */\n#define MAX_PEBS_EVENTS\t\t4\n\n/* The size of a BTS record in bytes: */\n#define BTS_RECORD_SIZE\t\t24\n\n#define BTS_BUFFER_SIZE\t\t(PAGE_SIZE << 4)\n#define PEBS_BUFFER_SIZE\tPAGE_SIZE\n\n/*\n * pebs_record_32 for p4 and core not supported\n\nstruct pebs_record_32 {\n\tu32 flags, ip;\n\tu32 ax, bc, cx, dx;\n\tu32 si, di, bp, sp;\n};\n\n */\n\nstruct pebs_record_core {\n\tu64 flags, ip;\n\tu64 ax, bx, cx, dx;\n\tu64 si, di, bp, sp;\n\tu64 r8,  r9,  r10, r11;\n\tu64 r12, r13, r14, r15;\n};\n\nstruct pebs_record_nhm {\n\tu64 flags, ip;\n\tu64 ax, bx, cx, dx;\n\tu64 si, di, bp, sp;\n\tu64 r8,  r9,  r10, r11;\n\tu64 r12, r13, r14, r15;\n\tu64 status, dla, dse, lat;\n};\n\n/*\n * A debug store configuration.\n *\n * We only support architectures that use 64bit fields.\n */\nstruct debug_store {\n\tu64\tbts_buffer_base;\n\tu64\tbts_index;\n\tu64\tbts_absolute_maximum;\n\tu64\tbts_interrupt_threshold;\n\tu64\tpebs_buffer_base;\n\tu64\tpebs_index;\n\tu64\tpebs_absolute_maximum;\n\tu64\tpebs_interrupt_threshold;\n\tu64\tpebs_event_reset[MAX_PEBS_EVENTS];\n};\n\nstatic void init_debug_store_on_cpu(int cpu)\n{\n\tstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\n\n\tif (!ds)\n\t\treturn;\n\n\twrmsr_on_cpu(cpu, MSR_IA32_DS_AREA,\n\t\t     (u32)((u64)(unsigned long)ds),\n\t\t     (u32)((u64)(unsigned long)ds >> 32));\n}\n\nstatic void fini_debug_store_on_cpu(int cpu)\n{\n\tif (!per_cpu(cpu_hw_events, cpu).ds)\n\t\treturn;\n\n\twrmsr_on_cpu(cpu, MSR_IA32_DS_AREA, 0, 0);\n}\n\nstatic int alloc_pebs_buffer(int cpu)\n{\n\tstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\n\tint node = cpu_to_node(cpu);\n\tint max, thresh = 1; /* always use a single PEBS record */\n\tvoid *buffer;\n\n\tif (!x86_pmu.pebs)\n\t\treturn 0;\n\n\tbuffer = kmalloc_node(PEBS_BUFFER_SIZE, GFP_KERNEL | __GFP_ZERO, node);\n\tif (unlikely(!buffer))\n\t\treturn -ENOMEM;\n\n\tmax = PEBS_BUFFER_SIZE / x86_pmu.pebs_record_size;\n\n\tds->pebs_buffer_base = (u64)(unsigned long)buffer;\n\tds->pebs_index = ds->pebs_buffer_base;\n\tds->pebs_absolute_maximum = ds->pebs_buffer_base +\n\t\tmax * x86_pmu.pebs_record_size;\n\n\tds->pebs_interrupt_threshold = ds->pebs_buffer_base +\n\t\tthresh * x86_pmu.pebs_record_size;\n\n\treturn 0;\n}\n\nstatic void release_pebs_buffer(int cpu)\n{\n\tstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\n\n\tif (!ds || !x86_pmu.pebs)\n\t\treturn;\n\n\tkfree((void *)(unsigned long)ds->pebs_buffer_base);\n\tds->pebs_buffer_base = 0;\n}\n\nstatic int alloc_bts_buffer(int cpu)\n{\n\tstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\n\tint node = cpu_to_node(cpu);\n\tint max, thresh;\n\tvoid *buffer;\n\n\tif (!x86_pmu.bts)\n\t\treturn 0;\n\n\tbuffer = kmalloc_node(BTS_BUFFER_SIZE, GFP_KERNEL | __GFP_ZERO, node);\n\tif (unlikely(!buffer))\n\t\treturn -ENOMEM;\n\n\tmax = BTS_BUFFER_SIZE / BTS_RECORD_SIZE;\n\tthresh = max / 16;\n\n\tds->bts_buffer_base = (u64)(unsigned long)buffer;\n\tds->bts_index = ds->bts_buffer_base;\n\tds->bts_absolute_maximum = ds->bts_buffer_base +\n\t\tmax * BTS_RECORD_SIZE;\n\tds->bts_interrupt_threshold = ds->bts_absolute_maximum -\n\t\tthresh * BTS_RECORD_SIZE;\n\n\treturn 0;\n}\n\nstatic void release_bts_buffer(int cpu)\n{\n\tstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\n\n\tif (!ds || !x86_pmu.bts)\n\t\treturn;\n\n\tkfree((void *)(unsigned long)ds->bts_buffer_base);\n\tds->bts_buffer_base = 0;\n}\n\nstatic int alloc_ds_buffer(int cpu)\n{\n\tint node = cpu_to_node(cpu);\n\tstruct debug_store *ds;\n\n\tds = kmalloc_node(sizeof(*ds), GFP_KERNEL | __GFP_ZERO, node);\n\tif (unlikely(!ds))\n\t\treturn -ENOMEM;\n\n\tper_cpu(cpu_hw_events, cpu).ds = ds;\n\n\treturn 0;\n}\n\nstatic void release_ds_buffer(int cpu)\n{\n\tstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\n\n\tif (!ds)\n\t\treturn;\n\n\tper_cpu(cpu_hw_events, cpu).ds = NULL;\n\tkfree(ds);\n}\n\nstatic void release_ds_buffers(void)\n{\n\tint cpu;\n\n\tif (!x86_pmu.bts && !x86_pmu.pebs)\n\t\treturn;\n\n\tget_online_cpus();\n\tfor_each_online_cpu(cpu)\n\t\tfini_debug_store_on_cpu(cpu);\n\n\tfor_each_possible_cpu(cpu) {\n\t\trelease_pebs_buffer(cpu);\n\t\trelease_bts_buffer(cpu);\n\t\trelease_ds_buffer(cpu);\n\t}\n\tput_online_cpus();\n}\n\nstatic void reserve_ds_buffers(void)\n{\n\tint bts_err = 0, pebs_err = 0;\n\tint cpu;\n\n\tx86_pmu.bts_active = 0;\n\tx86_pmu.pebs_active = 0;\n\n\tif (!x86_pmu.bts && !x86_pmu.pebs)\n\t\treturn;\n\n\tif (!x86_pmu.bts)\n\t\tbts_err = 1;\n\n\tif (!x86_pmu.pebs)\n\t\tpebs_err = 1;\n\n\tget_online_cpus();\n\n\tfor_each_possible_cpu(cpu) {\n\t\tif (alloc_ds_buffer(cpu)) {\n\t\t\tbts_err = 1;\n\t\t\tpebs_err = 1;\n\t\t}\n\n\t\tif (!bts_err && alloc_bts_buffer(cpu))\n\t\t\tbts_err = 1;\n\n\t\tif (!pebs_err && alloc_pebs_buffer(cpu))\n\t\t\tpebs_err = 1;\n\n\t\tif (bts_err && pebs_err)\n\t\t\tbreak;\n\t}\n\n\tif (bts_err) {\n\t\tfor_each_possible_cpu(cpu)\n\t\t\trelease_bts_buffer(cpu);\n\t}\n\n\tif (pebs_err) {\n\t\tfor_each_possible_cpu(cpu)\n\t\t\trelease_pebs_buffer(cpu);\n\t}\n\n\tif (bts_err && pebs_err) {\n\t\tfor_each_possible_cpu(cpu)\n\t\t\trelease_ds_buffer(cpu);\n\t} else {\n\t\tif (x86_pmu.bts && !bts_err)\n\t\t\tx86_pmu.bts_active = 1;\n\n\t\tif (x86_pmu.pebs && !pebs_err)\n\t\t\tx86_pmu.pebs_active = 1;\n\n\t\tfor_each_online_cpu(cpu)\n\t\t\tinit_debug_store_on_cpu(cpu);\n\t}\n\n\tput_online_cpus();\n}\n\n/*\n * BTS\n */\n\nstatic struct event_constraint bts_constraint =\n\tEVENT_CONSTRAINT(0, 1ULL << X86_PMC_IDX_FIXED_BTS, 0);\n\nstatic void intel_pmu_enable_bts(u64 config)\n{\n\tunsigned long debugctlmsr;\n\n\tdebugctlmsr = get_debugctlmsr();\n\n\tdebugctlmsr |= DEBUGCTLMSR_TR;\n\tdebugctlmsr |= DEBUGCTLMSR_BTS;\n\tdebugctlmsr |= DEBUGCTLMSR_BTINT;\n\n\tif (!(config & ARCH_PERFMON_EVENTSEL_OS))\n\t\tdebugctlmsr |= DEBUGCTLMSR_BTS_OFF_OS;\n\n\tif (!(config & ARCH_PERFMON_EVENTSEL_USR))\n\t\tdebugctlmsr |= DEBUGCTLMSR_BTS_OFF_USR;\n\n\tupdate_debugctlmsr(debugctlmsr);\n}\n\nstatic void intel_pmu_disable_bts(void)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tunsigned long debugctlmsr;\n\n\tif (!cpuc->ds)\n\t\treturn;\n\n\tdebugctlmsr = get_debugctlmsr();\n\n\tdebugctlmsr &=\n\t\t~(DEBUGCTLMSR_TR | DEBUGCTLMSR_BTS | DEBUGCTLMSR_BTINT |\n\t\t  DEBUGCTLMSR_BTS_OFF_OS | DEBUGCTLMSR_BTS_OFF_USR);\n\n\tupdate_debugctlmsr(debugctlmsr);\n}\n\nstatic int intel_pmu_drain_bts_buffer(void)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct debug_store *ds = cpuc->ds;\n\tstruct bts_record {\n\t\tu64\tfrom;\n\t\tu64\tto;\n\t\tu64\tflags;\n\t};\n\tstruct perf_event *event = cpuc->events[X86_PMC_IDX_FIXED_BTS];\n\tstruct bts_record *at, *top;\n\tstruct perf_output_handle handle;\n\tstruct perf_event_header header;\n\tstruct perf_sample_data data;\n\tstruct pt_regs regs;\n\n\tif (!event)\n\t\treturn 0;\n\n\tif (!x86_pmu.bts_active)\n\t\treturn 0;\n\n\tat  = (struct bts_record *)(unsigned long)ds->bts_buffer_base;\n\ttop = (struct bts_record *)(unsigned long)ds->bts_index;\n\n\tif (top <= at)\n\t\treturn 0;\n\n\tds->bts_index = ds->bts_buffer_base;\n\n\tperf_sample_data_init(&data, 0);\n\tdata.period = event->hw.last_period;\n\tregs.ip     = 0;\n\n\t/*\n\t * Prepare a generic sample, i.e. fill in the invariant fields.\n\t * We will overwrite the from and to address before we output\n\t * the sample.\n\t */\n\tperf_prepare_sample(&header, &data, event, &regs);\n\n\tif (perf_output_begin(&handle, event, header.size * (top - at), 1, 1))\n\t\treturn 1;\n\n\tfor (; at < top; at++) {\n\t\tdata.ip\t\t= at->from;\n\t\tdata.addr\t= at->to;\n\n\t\tperf_output_sample(&handle, &header, &data, event);\n\t}\n\n\tperf_output_end(&handle);\n\n\t/* There's new data available. */\n\tevent->hw.interrupts++;\n\tevent->pending_kill = POLL_IN;\n\treturn 1;\n}\n\n/*\n * PEBS\n */\nstatic struct event_constraint intel_core2_pebs_event_constraints[] = {\n\tINTEL_UEVENT_CONSTRAINT(0x00c0, 0x1), /* INST_RETIRED.ANY */\n\tINTEL_UEVENT_CONSTRAINT(0xfec1, 0x1), /* X87_OPS_RETIRED.ANY */\n\tINTEL_UEVENT_CONSTRAINT(0x00c5, 0x1), /* BR_INST_RETIRED.MISPRED */\n\tINTEL_UEVENT_CONSTRAINT(0x1fc7, 0x1), /* SIMD_INST_RETURED.ANY */\n\tINTEL_EVENT_CONSTRAINT(0xcb, 0x1),    /* MEM_LOAD_RETIRED.* */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_atom_pebs_event_constraints[] = {\n\tINTEL_UEVENT_CONSTRAINT(0x00c0, 0x1), /* INST_RETIRED.ANY */\n\tINTEL_UEVENT_CONSTRAINT(0x00c5, 0x1), /* MISPREDICTED_BRANCH_RETIRED */\n\tINTEL_EVENT_CONSTRAINT(0xcb, 0x1),    /* MEM_LOAD_RETIRED.* */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_nehalem_pebs_event_constraints[] = {\n\tINTEL_EVENT_CONSTRAINT(0x0b, 0xf),    /* MEM_INST_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0x0f, 0xf),    /* MEM_UNCORE_RETIRED.* */\n\tINTEL_UEVENT_CONSTRAINT(0x010c, 0xf), /* MEM_STORE_RETIRED.DTLB_MISS */\n\tINTEL_EVENT_CONSTRAINT(0xc0, 0xf),    /* INST_RETIRED.ANY */\n\tINTEL_EVENT_CONSTRAINT(0xc2, 0xf),    /* UOPS_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xc4, 0xf),    /* BR_INST_RETIRED.* */\n\tINTEL_UEVENT_CONSTRAINT(0x02c5, 0xf), /* BR_MISP_RETIRED.NEAR_CALL */\n\tINTEL_EVENT_CONSTRAINT(0xc7, 0xf),    /* SSEX_UOPS_RETIRED.* */\n\tINTEL_UEVENT_CONSTRAINT(0x20c8, 0xf), /* ITLB_MISS_RETIRED */\n\tINTEL_EVENT_CONSTRAINT(0xcb, 0xf),    /* MEM_LOAD_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xf7, 0xf),    /* FP_ASSIST.* */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_westmere_pebs_event_constraints[] = {\n\tINTEL_EVENT_CONSTRAINT(0x0b, 0xf),    /* MEM_INST_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0x0f, 0xf),    /* MEM_UNCORE_RETIRED.* */\n\tINTEL_UEVENT_CONSTRAINT(0x010c, 0xf), /* MEM_STORE_RETIRED.DTLB_MISS */\n\tINTEL_EVENT_CONSTRAINT(0xc0, 0xf),    /* INSTR_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xc2, 0xf),    /* UOPS_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xc4, 0xf),    /* BR_INST_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xc5, 0xf),    /* BR_MISP_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xc7, 0xf),    /* SSEX_UOPS_RETIRED.* */\n\tINTEL_UEVENT_CONSTRAINT(0x20c8, 0xf), /* ITLB_MISS_RETIRED */\n\tINTEL_EVENT_CONSTRAINT(0xcb, 0xf),    /* MEM_LOAD_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xf7, 0xf),    /* FP_ASSIST.* */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_snb_pebs_events[] = {\n\tINTEL_UEVENT_CONSTRAINT(0x01c0, 0x2), /* INST_RETIRED.PRECDIST */\n\tINTEL_UEVENT_CONSTRAINT(0x01c2, 0xf), /* UOPS_RETIRED.ALL */\n\tINTEL_UEVENT_CONSTRAINT(0x02c2, 0xf), /* UOPS_RETIRED.RETIRE_SLOTS */\n\tINTEL_EVENT_CONSTRAINT(0xc4, 0xf),    /* BR_INST_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xc5, 0xf),    /* BR_MISP_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xcd, 0x8),    /* MEM_TRANS_RETIRED.* */\n\tINTEL_UEVENT_CONSTRAINT(0x11d0, 0xf), /* MEM_UOP_RETIRED.STLB_MISS_LOADS */\n\tINTEL_UEVENT_CONSTRAINT(0x12d0, 0xf), /* MEM_UOP_RETIRED.STLB_MISS_STORES */\n\tINTEL_UEVENT_CONSTRAINT(0x21d0, 0xf), /* MEM_UOP_RETIRED.LOCK_LOADS */\n\tINTEL_UEVENT_CONSTRAINT(0x22d0, 0xf), /* MEM_UOP_RETIRED.LOCK_STORES */\n\tINTEL_UEVENT_CONSTRAINT(0x41d0, 0xf), /* MEM_UOP_RETIRED.SPLIT_LOADS */\n\tINTEL_UEVENT_CONSTRAINT(0x42d0, 0xf), /* MEM_UOP_RETIRED.SPLIT_STORES */\n\tINTEL_UEVENT_CONSTRAINT(0x81d0, 0xf), /* MEM_UOP_RETIRED.ANY_LOADS */\n\tINTEL_UEVENT_CONSTRAINT(0x82d0, 0xf), /* MEM_UOP_RETIRED.ANY_STORES */\n\tINTEL_EVENT_CONSTRAINT(0xd1, 0xf),    /* MEM_LOAD_UOPS_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xd2, 0xf),    /* MEM_LOAD_UOPS_LLC_HIT_RETIRED.* */\n\tINTEL_UEVENT_CONSTRAINT(0x02d4, 0xf), /* MEM_LOAD_UOPS_MISC_RETIRED.LLC_MISS */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint *\nintel_pebs_constraints(struct perf_event *event)\n{\n\tstruct event_constraint *c;\n\n\tif (!event->attr.precise_ip)\n\t\treturn NULL;\n\n\tif (x86_pmu.pebs_constraints) {\n\t\tfor_each_event_constraint(c, x86_pmu.pebs_constraints) {\n\t\t\tif ((event->hw.config & c->cmask) == c->code)\n\t\t\t\treturn c;\n\t\t}\n\t}\n\n\treturn &emptyconstraint;\n}\n\nstatic void intel_pmu_pebs_enable(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\thwc->config &= ~ARCH_PERFMON_EVENTSEL_INT;\n\n\tcpuc->pebs_enabled |= 1ULL << hwc->idx;\n\tWARN_ON_ONCE(cpuc->enabled);\n\n\tif (x86_pmu.intel_cap.pebs_trap && event->attr.precise_ip > 1)\n\t\tintel_pmu_lbr_enable(event);\n}\n\nstatic void intel_pmu_pebs_disable(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tcpuc->pebs_enabled &= ~(1ULL << hwc->idx);\n\tif (cpuc->enabled)\n\t\twrmsrl(MSR_IA32_PEBS_ENABLE, cpuc->pebs_enabled);\n\n\thwc->config |= ARCH_PERFMON_EVENTSEL_INT;\n\n\tif (x86_pmu.intel_cap.pebs_trap && event->attr.precise_ip > 1)\n\t\tintel_pmu_lbr_disable(event);\n}\n\nstatic void intel_pmu_pebs_enable_all(void)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\tif (cpuc->pebs_enabled)\n\t\twrmsrl(MSR_IA32_PEBS_ENABLE, cpuc->pebs_enabled);\n}\n\nstatic void intel_pmu_pebs_disable_all(void)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\tif (cpuc->pebs_enabled)\n\t\twrmsrl(MSR_IA32_PEBS_ENABLE, 0);\n}\n\n#include <asm/insn.h>\n\nstatic inline bool kernel_ip(unsigned long ip)\n{\n#ifdef CONFIG_X86_32\n\treturn ip > PAGE_OFFSET;\n#else\n\treturn (long)ip < 0;\n#endif\n}\n\nstatic int intel_pmu_pebs_fixup_ip(struct pt_regs *regs)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tunsigned long from = cpuc->lbr_entries[0].from;\n\tunsigned long old_to, to = cpuc->lbr_entries[0].to;\n\tunsigned long ip = regs->ip;\n\n\t/*\n\t * We don't need to fixup if the PEBS assist is fault like\n\t */\n\tif (!x86_pmu.intel_cap.pebs_trap)\n\t\treturn 1;\n\n\t/*\n\t * No LBR entry, no basic block, no rewinding\n\t */\n\tif (!cpuc->lbr_stack.nr || !from || !to)\n\t\treturn 0;\n\n\t/*\n\t * Basic blocks should never cross user/kernel boundaries\n\t */\n\tif (kernel_ip(ip) != kernel_ip(to))\n\t\treturn 0;\n\n\t/*\n\t * unsigned math, either ip is before the start (impossible) or\n\t * the basic block is larger than 1 page (sanity)\n\t */\n\tif ((ip - to) > PAGE_SIZE)\n\t\treturn 0;\n\n\t/*\n\t * We sampled a branch insn, rewind using the LBR stack\n\t */\n\tif (ip == to) {\n\t\tregs->ip = from;\n\t\treturn 1;\n\t}\n\n\tdo {\n\t\tstruct insn insn;\n\t\tu8 buf[MAX_INSN_SIZE];\n\t\tvoid *kaddr;\n\n\t\told_to = to;\n\t\tif (!kernel_ip(ip)) {\n\t\t\tint bytes, size = MAX_INSN_SIZE;\n\n\t\t\tbytes = copy_from_user_nmi(buf, (void __user *)to, size);\n\t\t\tif (bytes != size)\n\t\t\t\treturn 0;\n\n\t\t\tkaddr = buf;\n\t\t} else\n\t\t\tkaddr = (void *)to;\n\n\t\tkernel_insn_init(&insn, kaddr);\n\t\tinsn_get_length(&insn);\n\t\tto += insn.length;\n\t} while (to < ip);\n\n\tif (to == ip) {\n\t\tregs->ip = old_to;\n\t\treturn 1;\n\t}\n\n\t/*\n\t * Even though we decoded the basic block, the instruction stream\n\t * never matched the given IP, either the TO or the IP got corrupted.\n\t */\n\treturn 0;\n}\n\nstatic int intel_pmu_save_and_restart(struct perf_event *event);\n\nstatic void __intel_pmu_pebs_event(struct perf_event *event,\n\t\t\t\t   struct pt_regs *iregs, void *__pebs)\n{\n\t/*\n\t * We cast to pebs_record_core since that is a subset of\n\t * both formats and we don't use the other fields in this\n\t * routine.\n\t */\n\tstruct pebs_record_core *pebs = __pebs;\n\tstruct perf_sample_data data;\n\tstruct pt_regs regs;\n\n\tif (!intel_pmu_save_and_restart(event))\n\t\treturn;\n\n\tperf_sample_data_init(&data, 0);\n\tdata.period = event->hw.last_period;\n\n\t/*\n\t * We use the interrupt regs as a base because the PEBS record\n\t * does not contain a full regs set, specifically it seems to\n\t * lack segment descriptors, which get used by things like\n\t * user_mode().\n\t *\n\t * In the simple case fix up only the IP and BP,SP regs, for\n\t * PERF_SAMPLE_IP and PERF_SAMPLE_CALLCHAIN to function properly.\n\t * A possible PERF_SAMPLE_REGS will have to transfer all regs.\n\t */\n\tregs = *iregs;\n\tregs.ip = pebs->ip;\n\tregs.bp = pebs->bp;\n\tregs.sp = pebs->sp;\n\n\tif (event->attr.precise_ip > 1 && intel_pmu_pebs_fixup_ip(&regs))\n\t\tregs.flags |= PERF_EFLAGS_EXACT;\n\telse\n\t\tregs.flags &= ~PERF_EFLAGS_EXACT;\n\n\tif (perf_event_overflow(event, 1, &data, &regs))\n\t\tx86_pmu_stop(event, 0);\n}\n\nstatic void intel_pmu_drain_pebs_core(struct pt_regs *iregs)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct debug_store *ds = cpuc->ds;\n\tstruct perf_event *event = cpuc->events[0]; /* PMC0 only */\n\tstruct pebs_record_core *at, *top;\n\tint n;\n\n\tif (!x86_pmu.pebs_active)\n\t\treturn;\n\n\tat  = (struct pebs_record_core *)(unsigned long)ds->pebs_buffer_base;\n\ttop = (struct pebs_record_core *)(unsigned long)ds->pebs_index;\n\n\t/*\n\t * Whatever else happens, drain the thing\n\t */\n\tds->pebs_index = ds->pebs_buffer_base;\n\n\tif (!test_bit(0, cpuc->active_mask))\n\t\treturn;\n\n\tWARN_ON_ONCE(!event);\n\n\tif (!event->attr.precise_ip)\n\t\treturn;\n\n\tn = top - at;\n\tif (n <= 0)\n\t\treturn;\n\n\t/*\n\t * Should not happen, we program the threshold at 1 and do not\n\t * set a reset value.\n\t */\n\tWARN_ON_ONCE(n > 1);\n\tat += n - 1;\n\n\t__intel_pmu_pebs_event(event, iregs, at);\n}\n\nstatic void intel_pmu_drain_pebs_nhm(struct pt_regs *iregs)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct debug_store *ds = cpuc->ds;\n\tstruct pebs_record_nhm *at, *top;\n\tstruct perf_event *event = NULL;\n\tu64 status = 0;\n\tint bit, n;\n\n\tif (!x86_pmu.pebs_active)\n\t\treturn;\n\n\tat  = (struct pebs_record_nhm *)(unsigned long)ds->pebs_buffer_base;\n\ttop = (struct pebs_record_nhm *)(unsigned long)ds->pebs_index;\n\n\tds->pebs_index = ds->pebs_buffer_base;\n\n\tn = top - at;\n\tif (n <= 0)\n\t\treturn;\n\n\t/*\n\t * Should not happen, we program the threshold at 1 and do not\n\t * set a reset value.\n\t */\n\tWARN_ON_ONCE(n > MAX_PEBS_EVENTS);\n\n\tfor ( ; at < top; at++) {\n\t\tfor_each_set_bit(bit, (unsigned long *)&at->status, MAX_PEBS_EVENTS) {\n\t\t\tevent = cpuc->events[bit];\n\t\t\tif (!test_bit(bit, cpuc->active_mask))\n\t\t\t\tcontinue;\n\n\t\t\tWARN_ON_ONCE(!event);\n\n\t\t\tif (!event->attr.precise_ip)\n\t\t\t\tcontinue;\n\n\t\t\tif (__test_and_set_bit(bit, (unsigned long *)&status))\n\t\t\t\tcontinue;\n\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!event || bit >= MAX_PEBS_EVENTS)\n\t\t\tcontinue;\n\n\t\t__intel_pmu_pebs_event(event, iregs, at);\n\t}\n}\n\n/*\n * BTS, PEBS probe and setup\n */\n\nstatic void intel_ds_init(void)\n{\n\t/*\n\t * No support for 32bit formats\n\t */\n\tif (!boot_cpu_has(X86_FEATURE_DTES64))\n\t\treturn;\n\n\tx86_pmu.bts  = boot_cpu_has(X86_FEATURE_BTS);\n\tx86_pmu.pebs = boot_cpu_has(X86_FEATURE_PEBS);\n\tif (x86_pmu.pebs) {\n\t\tchar pebs_type = x86_pmu.intel_cap.pebs_trap ?  '+' : '-';\n\t\tint format = x86_pmu.intel_cap.pebs_format;\n\n\t\tswitch (format) {\n\t\tcase 0:\n\t\t\tprintk(KERN_CONT \"PEBS fmt0%c, \", pebs_type);\n\t\t\tx86_pmu.pebs_record_size = sizeof(struct pebs_record_core);\n\t\t\tx86_pmu.drain_pebs = intel_pmu_drain_pebs_core;\n\t\t\tbreak;\n\n\t\tcase 1:\n\t\t\tprintk(KERN_CONT \"PEBS fmt1%c, \", pebs_type);\n\t\t\tx86_pmu.pebs_record_size = sizeof(struct pebs_record_nhm);\n\t\t\tx86_pmu.drain_pebs = intel_pmu_drain_pebs_nhm;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tprintk(KERN_CONT \"no PEBS fmt%d%c, \", format, pebs_type);\n\t\t\tx86_pmu.pebs = 0;\n\t\t}\n\t}\n}\n\n#else /* CONFIG_CPU_SUP_INTEL */\n\nstatic void reserve_ds_buffers(void)\n{\n}\n\nstatic void release_ds_buffers(void)\n{\n}\n\n#endif /* CONFIG_CPU_SUP_INTEL */\n", "/*\n * Netburst Performance Events (P4, old Xeon)\n *\n *  Copyright (C) 2010 Parallels, Inc., Cyrill Gorcunov <gorcunov@openvz.org>\n *  Copyright (C) 2010 Intel Corporation, Lin Ming <ming.m.lin@intel.com>\n *\n *  For licencing details see kernel-base/COPYING\n */\n\n#ifdef CONFIG_CPU_SUP_INTEL\n\n#include <asm/perf_event_p4.h>\n\n#define P4_CNTR_LIMIT 3\n/*\n * array indices: 0,1 - HT threads, used with HT enabled cpu\n */\nstruct p4_event_bind {\n\tunsigned int opcode;\t\t\t/* Event code and ESCR selector */\n\tunsigned int escr_msr[2];\t\t/* ESCR MSR for this event */\n\tunsigned int escr_emask;\t\t/* valid ESCR EventMask bits */\n\tunsigned int shared;\t\t\t/* event is shared across threads */\n\tchar cntr[2][P4_CNTR_LIMIT];\t\t/* counter index (offset), -1 on abscence */\n};\n\nstruct p4_pebs_bind {\n\tunsigned int metric_pebs;\n\tunsigned int metric_vert;\n};\n\n/* it sets P4_PEBS_ENABLE_UOP_TAG as well */\n#define P4_GEN_PEBS_BIND(name, pebs, vert)\t\t\t\\\n\t[P4_PEBS_METRIC__##name] = {\t\t\t\t\\\n\t\t.metric_pebs = pebs | P4_PEBS_ENABLE_UOP_TAG,\t\\\n\t\t.metric_vert = vert,\t\t\t\t\\\n\t}\n\n/*\n * note we have P4_PEBS_ENABLE_UOP_TAG always set here\n *\n * it's needed for mapping P4_PEBS_CONFIG_METRIC_MASK bits of\n * event configuration to find out which values are to be\n * written into MSR_IA32_PEBS_ENABLE and MSR_P4_PEBS_MATRIX_VERT\n * resgisters\n */\nstatic struct p4_pebs_bind p4_pebs_bind_map[] = {\n\tP4_GEN_PEBS_BIND(1stl_cache_load_miss_retired,\t0x0000001, 0x0000001),\n\tP4_GEN_PEBS_BIND(2ndl_cache_load_miss_retired,\t0x0000002, 0x0000001),\n\tP4_GEN_PEBS_BIND(dtlb_load_miss_retired,\t0x0000004, 0x0000001),\n\tP4_GEN_PEBS_BIND(dtlb_store_miss_retired,\t0x0000004, 0x0000002),\n\tP4_GEN_PEBS_BIND(dtlb_all_miss_retired,\t\t0x0000004, 0x0000003),\n\tP4_GEN_PEBS_BIND(tagged_mispred_branch,\t\t0x0018000, 0x0000010),\n\tP4_GEN_PEBS_BIND(mob_load_replay_retired,\t0x0000200, 0x0000001),\n\tP4_GEN_PEBS_BIND(split_load_retired,\t\t0x0000400, 0x0000001),\n\tP4_GEN_PEBS_BIND(split_store_retired,\t\t0x0000400, 0x0000002),\n};\n\n/*\n * Note that we don't use CCCR1 here, there is an\n * exception for P4_BSQ_ALLOCATION but we just have\n * no workaround\n *\n * consider this binding as resources which particular\n * event may borrow, it doesn't contain EventMask,\n * Tags and friends -- they are left to a caller\n */\nstatic struct p4_event_bind p4_event_bind_map[] = {\n\t[P4_EVENT_TC_DELIVER_MODE] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_TC_DELIVER_MODE),\n\t\t.escr_msr\t= { MSR_P4_TC_ESCR0, MSR_P4_TC_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_TC_DELIVER_MODE, DD)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_TC_DELIVER_MODE, DB)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_TC_DELIVER_MODE, DI)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_TC_DELIVER_MODE, BD)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_TC_DELIVER_MODE, BB)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_TC_DELIVER_MODE, BI)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_TC_DELIVER_MODE, ID),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {4, 5, -1}, {6, 7, -1} },\n\t},\n\t[P4_EVENT_BPU_FETCH_REQUEST] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_BPU_FETCH_REQUEST),\n\t\t.escr_msr\t= { MSR_P4_BPU_ESCR0, MSR_P4_BPU_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BPU_FETCH_REQUEST, TCMISS),\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_ITLB_REFERENCE] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_ITLB_REFERENCE),\n\t\t.escr_msr\t= { MSR_P4_ITLB_ESCR0, MSR_P4_ITLB_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_ITLB_REFERENCE, HIT)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_ITLB_REFERENCE, MISS)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_ITLB_REFERENCE, HIT_UK),\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_MEMORY_CANCEL] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_MEMORY_CANCEL),\n\t\t.escr_msr\t= { MSR_P4_DAC_ESCR0, MSR_P4_DAC_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MEMORY_CANCEL, ST_RB_FULL)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MEMORY_CANCEL, 64K_CONF),\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_MEMORY_COMPLETE] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_MEMORY_COMPLETE),\n\t\t.escr_msr\t= { MSR_P4_SAAT_ESCR0 , MSR_P4_SAAT_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MEMORY_COMPLETE, LSC)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MEMORY_COMPLETE, SSC),\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_LOAD_PORT_REPLAY] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_LOAD_PORT_REPLAY),\n\t\t.escr_msr\t= { MSR_P4_SAAT_ESCR0, MSR_P4_SAAT_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_LOAD_PORT_REPLAY, SPLIT_LD),\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_STORE_PORT_REPLAY] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_STORE_PORT_REPLAY),\n\t\t.escr_msr\t= { MSR_P4_SAAT_ESCR0 ,  MSR_P4_SAAT_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_STORE_PORT_REPLAY, SPLIT_ST),\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_MOB_LOAD_REPLAY] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_MOB_LOAD_REPLAY),\n\t\t.escr_msr\t= { MSR_P4_MOB_ESCR0, MSR_P4_MOB_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MOB_LOAD_REPLAY, NO_STA)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MOB_LOAD_REPLAY, NO_STD)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MOB_LOAD_REPLAY, PARTIAL_DATA)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MOB_LOAD_REPLAY, UNALGN_ADDR),\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_PAGE_WALK_TYPE] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_PAGE_WALK_TYPE),\n\t\t.escr_msr\t= { MSR_P4_PMH_ESCR0, MSR_P4_PMH_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_PAGE_WALK_TYPE, DTMISS)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_PAGE_WALK_TYPE, ITMISS),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_BSQ_CACHE_REFERENCE] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_BSQ_CACHE_REFERENCE),\n\t\t.escr_msr\t= { MSR_P4_BSU_ESCR0, MSR_P4_BSU_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_2ndL_HITS)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_2ndL_HITE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_2ndL_HITM)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_3rdL_HITS)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_3rdL_HITE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_3rdL_HITM)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_2ndL_MISS)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_3rdL_MISS)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, WR_2ndL_MISS),\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_IOQ_ALLOCATION] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_IOQ_ALLOCATION),\n\t\t.escr_msr\t= { MSR_P4_FSB_ESCR0, MSR_P4_FSB_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, DEFAULT)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, ALL_READ)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, ALL_WRITE)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, MEM_UC)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, MEM_WC)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, MEM_WT)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, MEM_WP)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, MEM_WB)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, OWN)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, OTHER)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, PREFETCH),\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_IOQ_ACTIVE_ENTRIES] = {\t/* shared ESCR */\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_IOQ_ACTIVE_ENTRIES),\n\t\t.escr_msr\t= { MSR_P4_FSB_ESCR1,  MSR_P4_FSB_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, DEFAULT)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, ALL_READ)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, ALL_WRITE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, MEM_UC)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, MEM_WC)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, MEM_WT)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, MEM_WP)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, MEM_WB)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, OWN)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, OTHER)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, PREFETCH),\n\t\t.cntr\t\t= { {2, -1, -1}, {3, -1, -1} },\n\t},\n\t[P4_EVENT_FSB_DATA_ACTIVITY] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_FSB_DATA_ACTIVITY),\n\t\t.escr_msr\t= { MSR_P4_FSB_ESCR0, MSR_P4_FSB_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FSB_DATA_ACTIVITY, DRDY_DRV)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FSB_DATA_ACTIVITY, DRDY_OWN)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FSB_DATA_ACTIVITY, DRDY_OTHER)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FSB_DATA_ACTIVITY, DBSY_DRV)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FSB_DATA_ACTIVITY, DBSY_OWN)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FSB_DATA_ACTIVITY, DBSY_OTHER),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_BSQ_ALLOCATION] = {\t\t/* shared ESCR, broken CCCR1 */\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_BSQ_ALLOCATION),\n\t\t.escr_msr\t= { MSR_P4_BSU_ESCR0, MSR_P4_BSU_ESCR0 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_TYPE0)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_TYPE1)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_LEN0)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_LEN1)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_IO_TYPE)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_LOCK_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_CACHE_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_SPLIT_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_DEM_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_ORD_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, MEM_TYPE0)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, MEM_TYPE1)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, MEM_TYPE2),\n\t\t.cntr\t\t= { {0, -1, -1}, {1, -1, -1} },\n\t},\n\t[P4_EVENT_BSQ_ACTIVE_ENTRIES] = {\t/* shared ESCR */\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_BSQ_ACTIVE_ENTRIES),\n\t\t.escr_msr\t= { MSR_P4_BSU_ESCR1 , MSR_P4_BSU_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_TYPE0)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_TYPE1)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_LEN0)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_LEN1)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_IO_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_LOCK_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_CACHE_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_SPLIT_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_DEM_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_ORD_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, MEM_TYPE0)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, MEM_TYPE1)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, MEM_TYPE2),\n\t\t.cntr\t\t= { {2, -1, -1}, {3, -1, -1} },\n\t},\n\t[P4_EVENT_SSE_INPUT_ASSIST] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_SSE_INPUT_ASSIST),\n\t\t.escr_msr\t= { MSR_P4_FIRM_ESCR0, MSR_P4_FIRM_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_SSE_INPUT_ASSIST, ALL),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_PACKED_SP_UOP] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_PACKED_SP_UOP),\n\t\t.escr_msr\t= { MSR_P4_FIRM_ESCR0, MSR_P4_FIRM_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_PACKED_SP_UOP, ALL),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_PACKED_DP_UOP] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_PACKED_DP_UOP),\n\t\t.escr_msr\t= { MSR_P4_FIRM_ESCR0, MSR_P4_FIRM_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_PACKED_DP_UOP, ALL),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_SCALAR_SP_UOP] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_SCALAR_SP_UOP),\n\t\t.escr_msr\t= { MSR_P4_FIRM_ESCR0, MSR_P4_FIRM_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_SCALAR_SP_UOP, ALL),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_SCALAR_DP_UOP] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_SCALAR_DP_UOP),\n\t\t.escr_msr\t= { MSR_P4_FIRM_ESCR0, MSR_P4_FIRM_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_SCALAR_DP_UOP, ALL),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_64BIT_MMX_UOP] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_64BIT_MMX_UOP),\n\t\t.escr_msr\t= { MSR_P4_FIRM_ESCR0, MSR_P4_FIRM_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_64BIT_MMX_UOP, ALL),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_128BIT_MMX_UOP] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_128BIT_MMX_UOP),\n\t\t.escr_msr\t= { MSR_P4_FIRM_ESCR0, MSR_P4_FIRM_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_128BIT_MMX_UOP, ALL),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_X87_FP_UOP] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_X87_FP_UOP),\n\t\t.escr_msr\t= { MSR_P4_FIRM_ESCR0, MSR_P4_FIRM_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_X87_FP_UOP, ALL),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_TC_MISC] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_TC_MISC),\n\t\t.escr_msr\t= { MSR_P4_TC_ESCR0, MSR_P4_TC_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_TC_MISC, FLUSH),\n\t\t.cntr\t\t= { {4, 5, -1}, {6, 7, -1} },\n\t},\n\t[P4_EVENT_GLOBAL_POWER_EVENTS] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_GLOBAL_POWER_EVENTS),\n\t\t.escr_msr\t= { MSR_P4_FSB_ESCR0, MSR_P4_FSB_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_GLOBAL_POWER_EVENTS, RUNNING),\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_TC_MS_XFER] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_TC_MS_XFER),\n\t\t.escr_msr\t= { MSR_P4_MS_ESCR0, MSR_P4_MS_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_TC_MS_XFER, CISC),\n\t\t.cntr\t\t= { {4, 5, -1}, {6, 7, -1} },\n\t},\n\t[P4_EVENT_UOP_QUEUE_WRITES] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_UOP_QUEUE_WRITES),\n\t\t.escr_msr\t= { MSR_P4_MS_ESCR0, MSR_P4_MS_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_UOP_QUEUE_WRITES, FROM_TC_BUILD)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_UOP_QUEUE_WRITES, FROM_TC_DELIVER)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_UOP_QUEUE_WRITES, FROM_ROM),\n\t\t.cntr\t\t= { {4, 5, -1}, {6, 7, -1} },\n\t},\n\t[P4_EVENT_RETIRED_MISPRED_BRANCH_TYPE] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_RETIRED_MISPRED_BRANCH_TYPE),\n\t\t.escr_msr\t= { MSR_P4_TBPU_ESCR0 , MSR_P4_TBPU_ESCR0 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_MISPRED_BRANCH_TYPE, CONDITIONAL)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_MISPRED_BRANCH_TYPE, CALL)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_MISPRED_BRANCH_TYPE, RETURN)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_MISPRED_BRANCH_TYPE, INDIRECT),\n\t\t.cntr\t\t= { {4, 5, -1}, {6, 7, -1} },\n\t},\n\t[P4_EVENT_RETIRED_BRANCH_TYPE] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_RETIRED_BRANCH_TYPE),\n\t\t.escr_msr\t= { MSR_P4_TBPU_ESCR0 , MSR_P4_TBPU_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_BRANCH_TYPE, CONDITIONAL)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_BRANCH_TYPE, CALL)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_BRANCH_TYPE, RETURN)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_BRANCH_TYPE, INDIRECT),\n\t\t.cntr\t\t= { {4, 5, -1}, {6, 7, -1} },\n\t},\n\t[P4_EVENT_RESOURCE_STALL] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_RESOURCE_STALL),\n\t\t.escr_msr\t= { MSR_P4_ALF_ESCR0, MSR_P4_ALF_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RESOURCE_STALL, SBFULL),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_WC_BUFFER] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_WC_BUFFER),\n\t\t.escr_msr\t= { MSR_P4_DAC_ESCR0, MSR_P4_DAC_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_WC_BUFFER, WCB_EVICTS)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_WC_BUFFER, WCB_FULL_EVICTS),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_B2B_CYCLES] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_B2B_CYCLES),\n\t\t.escr_msr\t= { MSR_P4_FSB_ESCR0, MSR_P4_FSB_ESCR1 },\n\t\t.escr_emask\t= 0,\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_BNR] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_BNR),\n\t\t.escr_msr\t= { MSR_P4_FSB_ESCR0, MSR_P4_FSB_ESCR1 },\n\t\t.escr_emask\t= 0,\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_SNOOP] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_SNOOP),\n\t\t.escr_msr\t= { MSR_P4_FSB_ESCR0, MSR_P4_FSB_ESCR1 },\n\t\t.escr_emask\t= 0,\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_RESPONSE] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_RESPONSE),\n\t\t.escr_msr\t= { MSR_P4_FSB_ESCR0, MSR_P4_FSB_ESCR1 },\n\t\t.escr_emask\t= 0,\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_FRONT_END_EVENT] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_FRONT_END_EVENT),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR2, MSR_P4_CRU_ESCR3 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FRONT_END_EVENT, NBOGUS)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FRONT_END_EVENT, BOGUS),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_EXECUTION_EVENT] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_EXECUTION_EVENT),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR2, MSR_P4_CRU_ESCR3 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, NBOGUS0)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, NBOGUS1)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, NBOGUS2)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, NBOGUS3)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, BOGUS0)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, BOGUS1)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, BOGUS2)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, BOGUS3),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_REPLAY_EVENT] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_REPLAY_EVENT),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR2, MSR_P4_CRU_ESCR3 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_REPLAY_EVENT, NBOGUS)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_REPLAY_EVENT, BOGUS),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_INSTR_RETIRED] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_INSTR_RETIRED),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR0, MSR_P4_CRU_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_INSTR_RETIRED, NBOGUSNTAG)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_INSTR_RETIRED, NBOGUSTAG)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_INSTR_RETIRED, BOGUSNTAG)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_INSTR_RETIRED, BOGUSTAG),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_UOPS_RETIRED] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_UOPS_RETIRED),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR0, MSR_P4_CRU_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_UOPS_RETIRED, NBOGUS)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_UOPS_RETIRED, BOGUS),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_UOP_TYPE] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_UOP_TYPE),\n\t\t.escr_msr\t= { MSR_P4_RAT_ESCR0, MSR_P4_RAT_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_UOP_TYPE, TAGLOADS)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_UOP_TYPE, TAGSTORES),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_BRANCH_RETIRED] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_BRANCH_RETIRED),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR2, MSR_P4_CRU_ESCR3 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BRANCH_RETIRED, MMNP)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BRANCH_RETIRED, MMNM)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BRANCH_RETIRED, MMTP)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BRANCH_RETIRED, MMTM),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_MISPRED_BRANCH_RETIRED] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_MISPRED_BRANCH_RETIRED),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR0, MSR_P4_CRU_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MISPRED_BRANCH_RETIRED, NBOGUS),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_X87_ASSIST] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_X87_ASSIST),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR2, MSR_P4_CRU_ESCR3 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_X87_ASSIST, FPSU)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_X87_ASSIST, FPSO)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_X87_ASSIST, POAO)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_X87_ASSIST, POAU)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_X87_ASSIST, PREA),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_MACHINE_CLEAR] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_MACHINE_CLEAR),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR2, MSR_P4_CRU_ESCR3 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MACHINE_CLEAR, CLEAR)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MACHINE_CLEAR, MOCLEAR)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MACHINE_CLEAR, SMCLEAR),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_INSTR_COMPLETED] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_INSTR_COMPLETED),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR0, MSR_P4_CRU_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_INSTR_COMPLETED, NBOGUS)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_INSTR_COMPLETED, BOGUS),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n};\n\n#define P4_GEN_CACHE_EVENT(event, bit, metric)\t\t\t\t  \\\n\tp4_config_pack_escr(P4_ESCR_EVENT(event)\t\t\t| \\\n\t\t\t    P4_ESCR_EMASK_BIT(event, bit))\t\t| \\\n\tp4_config_pack_cccr(metric\t\t\t\t\t| \\\n\t\t\t    P4_CCCR_ESEL(P4_OPCODE_ESEL(P4_OPCODE(event))))\n\nstatic __initconst const u64 p4_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = P4_GEN_CACHE_EVENT(P4_EVENT_REPLAY_EVENT, NBOGUS,\n\t\t\t\t\t\tP4_PEBS_METRIC__1stl_cache_load_miss_retired),\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = P4_GEN_CACHE_EVENT(P4_EVENT_REPLAY_EVENT, NBOGUS,\n\t\t\t\t\t\tP4_PEBS_METRIC__2ndl_cache_load_miss_retired),\n\t},\n},\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = P4_GEN_CACHE_EVENT(P4_EVENT_REPLAY_EVENT, NBOGUS,\n\t\t\t\t\t\tP4_PEBS_METRIC__dtlb_load_miss_retired),\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = P4_GEN_CACHE_EVENT(P4_EVENT_REPLAY_EVENT, NBOGUS,\n\t\t\t\t\t\tP4_PEBS_METRIC__dtlb_store_miss_retired),\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = P4_GEN_CACHE_EVENT(P4_EVENT_ITLB_REFERENCE, HIT,\n\t\t\t\t\t\tP4_PEBS_METRIC__none),\n\t\t[ C(RESULT_MISS)   ] = P4_GEN_CACHE_EVENT(P4_EVENT_ITLB_REFERENCE, MISS,\n\t\t\t\t\t\tP4_PEBS_METRIC__none),\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n};\n\nstatic u64 p4_general_events[PERF_COUNT_HW_MAX] = {\n  /* non-halted CPU clocks */\n  [PERF_COUNT_HW_CPU_CYCLES] =\n\tp4_config_pack_escr(P4_ESCR_EVENT(P4_EVENT_GLOBAL_POWER_EVENTS)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_GLOBAL_POWER_EVENTS, RUNNING)),\n\n  /*\n   * retired instructions\n   * in a sake of simplicity we don't use the FSB tagging\n   */\n  [PERF_COUNT_HW_INSTRUCTIONS] =\n\tp4_config_pack_escr(P4_ESCR_EVENT(P4_EVENT_INSTR_RETIRED)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_INSTR_RETIRED, NBOGUSNTAG)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_INSTR_RETIRED, BOGUSNTAG)),\n\n  /* cache hits */\n  [PERF_COUNT_HW_CACHE_REFERENCES] =\n\tp4_config_pack_escr(P4_ESCR_EVENT(P4_EVENT_BSQ_CACHE_REFERENCE)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_2ndL_HITS)\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_2ndL_HITE)\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_2ndL_HITM)\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_3rdL_HITS)\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_3rdL_HITE)\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_3rdL_HITM)),\n\n  /* cache misses */\n  [PERF_COUNT_HW_CACHE_MISSES] =\n\tp4_config_pack_escr(P4_ESCR_EVENT(P4_EVENT_BSQ_CACHE_REFERENCE)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_2ndL_MISS)\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_3rdL_MISS)\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, WR_2ndL_MISS)),\n\n  /* branch instructions retired */\n  [PERF_COUNT_HW_BRANCH_INSTRUCTIONS] =\n\tp4_config_pack_escr(P4_ESCR_EVENT(P4_EVENT_RETIRED_BRANCH_TYPE)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_BRANCH_TYPE, CONDITIONAL)\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_BRANCH_TYPE, CALL)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_BRANCH_TYPE, RETURN)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_BRANCH_TYPE, INDIRECT)),\n\n  /* mispredicted branches retired */\n  [PERF_COUNT_HW_BRANCH_MISSES]\t=\n\tp4_config_pack_escr(P4_ESCR_EVENT(P4_EVENT_MISPRED_BRANCH_RETIRED)\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MISPRED_BRANCH_RETIRED, NBOGUS)),\n\n  /* bus ready clocks (cpu is driving #DRDY_DRV\\#DRDY_OWN):  */\n  [PERF_COUNT_HW_BUS_CYCLES] =\n\tp4_config_pack_escr(P4_ESCR_EVENT(P4_EVENT_FSB_DATA_ACTIVITY)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FSB_DATA_ACTIVITY, DRDY_DRV)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FSB_DATA_ACTIVITY, DRDY_OWN))\t|\n\tp4_config_pack_cccr(P4_CCCR_EDGE | P4_CCCR_COMPARE),\n};\n\nstatic struct p4_event_bind *p4_config_get_bind(u64 config)\n{\n\tunsigned int evnt = p4_config_unpack_event(config);\n\tstruct p4_event_bind *bind = NULL;\n\n\tif (evnt < ARRAY_SIZE(p4_event_bind_map))\n\t\tbind = &p4_event_bind_map[evnt];\n\n\treturn bind;\n}\n\nstatic u64 p4_pmu_event_map(int hw_event)\n{\n\tstruct p4_event_bind *bind;\n\tunsigned int esel;\n\tu64 config;\n\n\tconfig = p4_general_events[hw_event];\n\tbind = p4_config_get_bind(config);\n\tesel = P4_OPCODE_ESEL(bind->opcode);\n\tconfig |= p4_config_pack_cccr(P4_CCCR_ESEL(esel));\n\n\treturn config;\n}\n\n/* check cpu model specifics */\nstatic bool p4_event_match_cpu_model(unsigned int event_idx)\n{\n\t/* INSTR_COMPLETED event only exist for model 3, 4, 6 (Prescott) */\n\tif (event_idx == P4_EVENT_INSTR_COMPLETED) {\n\t\tif (boot_cpu_data.x86_model != 3 &&\n\t\t\tboot_cpu_data.x86_model != 4 &&\n\t\t\tboot_cpu_data.x86_model != 6)\n\t\t\treturn false;\n\t}\n\n\t/*\n\t * For info\n\t * - IQ_ESCR0, IQ_ESCR1 only for models 1 and 2\n\t */\n\n\treturn true;\n}\n\nstatic int p4_validate_raw_event(struct perf_event *event)\n{\n\tunsigned int v, emask;\n\n\t/* User data may have out-of-bound event index */\n\tv = p4_config_unpack_event(event->attr.config);\n\tif (v >= ARRAY_SIZE(p4_event_bind_map))\n\t\treturn -EINVAL;\n\n\t/* It may be unsupported: */\n\tif (!p4_event_match_cpu_model(v))\n\t\treturn -EINVAL;\n\n\t/*\n\t * NOTE: P4_CCCR_THREAD_ANY has not the same meaning as\n\t * in Architectural Performance Monitoring, it means not\n\t * on _which_ logical cpu to count but rather _when_, ie it\n\t * depends on logical cpu state -- count event if one cpu active,\n\t * none, both or any, so we just allow user to pass any value\n\t * desired.\n\t *\n\t * In turn we always set Tx_OS/Tx_USR bits bound to logical\n\t * cpu without their propagation to another cpu\n\t */\n\n\t/*\n\t * if an event is shared across the logical threads\n\t * the user needs special permissions to be able to use it\n\t */\n\tif (p4_ht_active() && p4_event_bind_map[v].shared) {\n\t\tif (perf_paranoid_cpu() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\t/* ESCR EventMask bits may be invalid */\n\temask = p4_config_unpack_escr(event->attr.config) & P4_ESCR_EVENTMASK_MASK;\n\tif (emask & ~p4_event_bind_map[v].escr_emask)\n\t\treturn -EINVAL;\n\n\t/*\n\t * it may have some invalid PEBS bits\n\t */\n\tif (p4_config_pebs_has(event->attr.config, P4_PEBS_CONFIG_ENABLE))\n\t\treturn -EINVAL;\n\n\tv = p4_config_unpack_metric(event->attr.config);\n\tif (v >= ARRAY_SIZE(p4_pebs_bind_map))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void p4_hw_watchdog_set_attr(struct perf_event_attr *wd_attr)\n{\n\t/*\n\t * Watchdog ticks are special on Netburst, we use\n\t * that named \"non-sleeping\" ticks as recommended\n\t * by Intel SDM Vol3b.\n\t */\n\tWARN_ON_ONCE(wd_attr->type\t!= PERF_TYPE_HARDWARE ||\n\t\t     wd_attr->config\t!= PERF_COUNT_HW_CPU_CYCLES);\n\n\twd_attr->type\t= PERF_TYPE_RAW;\n\twd_attr->config\t=\n\t\tp4_config_pack_escr(P4_ESCR_EVENT(P4_EVENT_EXECUTION_EVENT)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, NBOGUS0)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, NBOGUS1)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, NBOGUS2)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, NBOGUS3)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, BOGUS0)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, BOGUS1)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, BOGUS2)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, BOGUS3))\t\t|\n\t\tp4_config_pack_cccr(P4_CCCR_THRESHOLD(15) | P4_CCCR_COMPLEMENT\t\t|\n\t\t\tP4_CCCR_COMPARE);\n}\n\nstatic int p4_hw_config(struct perf_event *event)\n{\n\tint cpu = get_cpu();\n\tint rc = 0;\n\tu32 escr, cccr;\n\n\t/*\n\t * the reason we use cpu that early is that: if we get scheduled\n\t * first time on the same cpu -- we will not need swap thread\n\t * specific flags in config (and will save some cpu cycles)\n\t */\n\n\tcccr = p4_default_cccr_conf(cpu);\n\tescr = p4_default_escr_conf(cpu, event->attr.exclude_kernel,\n\t\t\t\t\t event->attr.exclude_user);\n\tevent->hw.config = p4_config_pack_escr(escr) |\n\t\t\t   p4_config_pack_cccr(cccr);\n\n\tif (p4_ht_active() && p4_ht_thread(cpu))\n\t\tevent->hw.config = p4_set_ht_bit(event->hw.config);\n\n\tif (event->attr.type == PERF_TYPE_RAW) {\n\t\tstruct p4_event_bind *bind;\n\t\tunsigned int esel;\n\t\t/*\n\t\t * Clear bits we reserve to be managed by kernel itself\n\t\t * and never allowed from a user space\n\t\t */\n\t\t event->attr.config &= P4_CONFIG_MASK;\n\n\t\trc = p4_validate_raw_event(event);\n\t\tif (rc)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * Note that for RAW events we allow user to use P4_CCCR_RESERVED\n\t\t * bits since we keep additional info here (for cache events and etc)\n\t\t */\n\t\tevent->hw.config |= event->attr.config;\n\t\tbind = p4_config_get_bind(event->attr.config);\n\t\tif (!bind) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tesel = P4_OPCODE_ESEL(bind->opcode);\n\t\tevent->hw.config |= p4_config_pack_cccr(P4_CCCR_ESEL(esel));\n\t}\n\n\trc = x86_setup_perfctr(event);\nout:\n\tput_cpu();\n\treturn rc;\n}\n\nstatic inline int p4_pmu_clear_cccr_ovf(struct hw_perf_event *hwc)\n{\n\tu64 v;\n\n\t/* an official way for overflow indication */\n\trdmsrl(hwc->config_base, v);\n\tif (v & P4_CCCR_OVF) {\n\t\twrmsrl(hwc->config_base, v & ~P4_CCCR_OVF);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * In some circumstances the overflow might issue an NMI but did\n\t * not set P4_CCCR_OVF bit. Because a counter holds a negative value\n\t * we simply check for high bit being set, if it's cleared it means\n\t * the counter has reached zero value and continued counting before\n\t * real NMI signal was received:\n\t */\n\trdmsrl(hwc->event_base, v);\n\tif (!(v & ARCH_P4_UNFLAGGED_BIT))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void p4_pmu_disable_pebs(void)\n{\n\t/*\n\t * FIXME\n\t *\n\t * It's still allowed that two threads setup same cache\n\t * events so we can't simply clear metrics until we knew\n\t * no one is depending on us, so we need kind of counter\n\t * for \"ReplayEvent\" users.\n\t *\n\t * What is more complex -- RAW events, if user (for some\n\t * reason) will pass some cache event metric with improper\n\t * event opcode -- it's fine from hardware point of view\n\t * but completely nonsense from \"meaning\" of such action.\n\t *\n\t * So at moment let leave metrics turned on forever -- it's\n\t * ok for now but need to be revisited!\n\t *\n\t * (void)checking_wrmsrl(MSR_IA32_PEBS_ENABLE, (u64)0);\n\t * (void)checking_wrmsrl(MSR_P4_PEBS_MATRIX_VERT, (u64)0);\n\t */\n}\n\nstatic inline void p4_pmu_disable_event(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t/*\n\t * If event gets disabled while counter is in overflowed\n\t * state we need to clear P4_CCCR_OVF, otherwise interrupt get\n\t * asserted again and again\n\t */\n\t(void)checking_wrmsrl(hwc->config_base,\n\t\t(u64)(p4_config_unpack_cccr(hwc->config)) &\n\t\t\t~P4_CCCR_ENABLE & ~P4_CCCR_OVF & ~P4_CCCR_RESERVED);\n}\n\nstatic void p4_pmu_disable_all(void)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint idx;\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\t\tp4_pmu_disable_event(event);\n\t}\n\n\tp4_pmu_disable_pebs();\n}\n\n/* configuration must be valid */\nstatic void p4_pmu_enable_pebs(u64 config)\n{\n\tstruct p4_pebs_bind *bind;\n\tunsigned int idx;\n\n\tBUILD_BUG_ON(P4_PEBS_METRIC__max > P4_PEBS_CONFIG_METRIC_MASK);\n\n\tidx = p4_config_unpack_metric(config);\n\tif (idx == P4_PEBS_METRIC__none)\n\t\treturn;\n\n\tbind = &p4_pebs_bind_map[idx];\n\n\t(void)checking_wrmsrl(MSR_IA32_PEBS_ENABLE,\t(u64)bind->metric_pebs);\n\t(void)checking_wrmsrl(MSR_P4_PEBS_MATRIX_VERT,\t(u64)bind->metric_vert);\n}\n\nstatic void p4_pmu_enable_event(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint thread = p4_ht_config_thread(hwc->config);\n\tu64 escr_conf = p4_config_unpack_escr(p4_clear_ht_bit(hwc->config));\n\tunsigned int idx = p4_config_unpack_event(hwc->config);\n\tstruct p4_event_bind *bind;\n\tu64 escr_addr, cccr;\n\n\tbind = &p4_event_bind_map[idx];\n\tescr_addr = (u64)bind->escr_msr[thread];\n\n\t/*\n\t * - we dont support cascaded counters yet\n\t * - and counter 1 is broken (erratum)\n\t */\n\tWARN_ON_ONCE(p4_is_event_cascaded(hwc->config));\n\tWARN_ON_ONCE(hwc->idx == 1);\n\n\t/* we need a real Event value */\n\tescr_conf &= ~P4_ESCR_EVENT_MASK;\n\tescr_conf |= P4_ESCR_EVENT(P4_OPCODE_EVNT(bind->opcode));\n\n\tcccr = p4_config_unpack_cccr(hwc->config);\n\n\t/*\n\t * it could be Cache event so we need to write metrics\n\t * into additional MSRs\n\t */\n\tp4_pmu_enable_pebs(hwc->config);\n\n\t(void)checking_wrmsrl(escr_addr, escr_conf);\n\t(void)checking_wrmsrl(hwc->config_base,\n\t\t\t\t(cccr & ~P4_CCCR_RESERVED) | P4_CCCR_ENABLE);\n}\n\nstatic void p4_pmu_enable_all(int added)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint idx;\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\t\tp4_pmu_enable_event(event);\n\t}\n}\n\nstatic int p4_pmu_handle_irq(struct pt_regs *regs)\n{\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct perf_event *event;\n\tstruct hw_perf_event *hwc;\n\tint idx, handled = 0;\n\tu64 val;\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tint overflow;\n\n\t\tif (!test_bit(idx, cpuc->active_mask)) {\n\t\t\t/* catch in-flight IRQs */\n\t\t\tif (__test_and_clear_bit(idx, cpuc->running))\n\t\t\t\thandled++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tevent = cpuc->events[idx];\n\t\thwc = &event->hw;\n\n\t\tWARN_ON_ONCE(hwc->idx != idx);\n\n\t\t/* it might be unflagged overflow */\n\t\toverflow = p4_pmu_clear_cccr_ovf(hwc);\n\n\t\tval = x86_perf_event_update(event);\n\t\tif (!overflow && (val & (1ULL << (x86_pmu.cntval_bits - 1))))\n\t\t\tcontinue;\n\n\t\thandled += overflow;\n\n\t\t/* event overflow for sure */\n\t\tdata.period = event->hw.last_period;\n\n\t\tif (!x86_perf_event_set_period(event))\n\t\t\tcontinue;\n\t\tif (perf_event_overflow(event, 1, &data, regs))\n\t\t\tx86_pmu_stop(event, 0);\n\t}\n\n\tif (handled)\n\t\tinc_irq_stat(apic_perf_irqs);\n\n\t/*\n\t * When dealing with the unmasking of the LVTPC on P4 perf hw, it has\n\t * been observed that the OVF bit flag has to be cleared first _before_\n\t * the LVTPC can be unmasked.\n\t *\n\t * The reason is the NMI line will continue to be asserted while the OVF\n\t * bit is set.  This causes a second NMI to generate if the LVTPC is\n\t * unmasked before the OVF bit is cleared, leading to unknown NMI\n\t * messages.\n\t */\n\tapic_write(APIC_LVTPC, APIC_DM_NMI);\n\n\treturn handled;\n}\n\n/*\n * swap thread specific fields according to a thread\n * we are going to run on\n */\nstatic void p4_pmu_swap_config_ts(struct hw_perf_event *hwc, int cpu)\n{\n\tu32 escr, cccr;\n\n\t/*\n\t * we either lucky and continue on same cpu or no HT support\n\t */\n\tif (!p4_should_swap_ts(hwc->config, cpu))\n\t\treturn;\n\n\t/*\n\t * the event is migrated from an another logical\n\t * cpu, so we need to swap thread specific flags\n\t */\n\n\tescr = p4_config_unpack_escr(hwc->config);\n\tcccr = p4_config_unpack_cccr(hwc->config);\n\n\tif (p4_ht_thread(cpu)) {\n\t\tcccr &= ~P4_CCCR_OVF_PMI_T0;\n\t\tcccr |= P4_CCCR_OVF_PMI_T1;\n\t\tif (escr & P4_ESCR_T0_OS) {\n\t\t\tescr &= ~P4_ESCR_T0_OS;\n\t\t\tescr |= P4_ESCR_T1_OS;\n\t\t}\n\t\tif (escr & P4_ESCR_T0_USR) {\n\t\t\tescr &= ~P4_ESCR_T0_USR;\n\t\t\tescr |= P4_ESCR_T1_USR;\n\t\t}\n\t\thwc->config  = p4_config_pack_escr(escr);\n\t\thwc->config |= p4_config_pack_cccr(cccr);\n\t\thwc->config |= P4_CONFIG_HT;\n\t} else {\n\t\tcccr &= ~P4_CCCR_OVF_PMI_T1;\n\t\tcccr |= P4_CCCR_OVF_PMI_T0;\n\t\tif (escr & P4_ESCR_T1_OS) {\n\t\t\tescr &= ~P4_ESCR_T1_OS;\n\t\t\tescr |= P4_ESCR_T0_OS;\n\t\t}\n\t\tif (escr & P4_ESCR_T1_USR) {\n\t\t\tescr &= ~P4_ESCR_T1_USR;\n\t\t\tescr |= P4_ESCR_T0_USR;\n\t\t}\n\t\thwc->config  = p4_config_pack_escr(escr);\n\t\thwc->config |= p4_config_pack_cccr(cccr);\n\t\thwc->config &= ~P4_CONFIG_HT;\n\t}\n}\n\n/*\n * ESCR address hashing is tricky, ESCRs are not sequential\n * in memory but all starts from MSR_P4_BSU_ESCR0 (0x03a0) and\n * the metric between any ESCRs is laid in range [0xa0,0xe1]\n *\n * so we make ~70% filled hashtable\n */\n\n#define P4_ESCR_MSR_BASE\t\t0x000003a0\n#define P4_ESCR_MSR_MAX\t\t\t0x000003e1\n#define P4_ESCR_MSR_TABLE_SIZE\t\t(P4_ESCR_MSR_MAX - P4_ESCR_MSR_BASE + 1)\n#define P4_ESCR_MSR_IDX(msr)\t\t(msr - P4_ESCR_MSR_BASE)\n#define P4_ESCR_MSR_TABLE_ENTRY(msr)\t[P4_ESCR_MSR_IDX(msr)] = msr\n\nstatic const unsigned int p4_escr_table[P4_ESCR_MSR_TABLE_SIZE] = {\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_ALF_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_ALF_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_BPU_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_BPU_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_BSU_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_BSU_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_CRU_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_CRU_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_CRU_ESCR2),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_CRU_ESCR3),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_CRU_ESCR4),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_CRU_ESCR5),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_DAC_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_DAC_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_FIRM_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_FIRM_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_FLAME_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_FLAME_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_FSB_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_FSB_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_IQ_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_IQ_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_IS_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_IS_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_ITLB_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_ITLB_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_IX_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_IX_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_MOB_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_MOB_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_MS_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_MS_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_PMH_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_PMH_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_RAT_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_RAT_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_SAAT_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_SAAT_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_SSU_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_SSU_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_TBPU_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_TBPU_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_TC_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_TC_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_U2L_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_U2L_ESCR1),\n};\n\nstatic int p4_get_escr_idx(unsigned int addr)\n{\n\tunsigned int idx = P4_ESCR_MSR_IDX(addr);\n\n\tif (unlikely(idx >= P4_ESCR_MSR_TABLE_SIZE\t||\n\t\t\t!p4_escr_table[idx]\t\t||\n\t\t\tp4_escr_table[idx] != addr)) {\n\t\tWARN_ONCE(1, \"P4 PMU: Wrong address passed: %x\\n\", addr);\n\t\treturn -1;\n\t}\n\n\treturn idx;\n}\n\nstatic int p4_next_cntr(int thread, unsigned long *used_mask,\n\t\t\tstruct p4_event_bind *bind)\n{\n\tint i, j;\n\n\tfor (i = 0; i < P4_CNTR_LIMIT; i++) {\n\t\tj = bind->cntr[thread][i];\n\t\tif (j != -1 && !test_bit(j, used_mask))\n\t\t\treturn j;\n\t}\n\n\treturn -1;\n}\n\nstatic int p4_pmu_schedule_events(struct cpu_hw_events *cpuc, int n, int *assign)\n{\n\tunsigned long used_mask[BITS_TO_LONGS(X86_PMC_IDX_MAX)];\n\tunsigned long escr_mask[BITS_TO_LONGS(P4_ESCR_MSR_TABLE_SIZE)];\n\tint cpu = smp_processor_id();\n\tstruct hw_perf_event *hwc;\n\tstruct p4_event_bind *bind;\n\tunsigned int i, thread, num;\n\tint cntr_idx, escr_idx;\n\n\tbitmap_zero(used_mask, X86_PMC_IDX_MAX);\n\tbitmap_zero(escr_mask, P4_ESCR_MSR_TABLE_SIZE);\n\n\tfor (i = 0, num = n; i < n; i++, num--) {\n\n\t\thwc = &cpuc->event_list[i]->hw;\n\t\tthread = p4_ht_thread(cpu);\n\t\tbind = p4_config_get_bind(hwc->config);\n\t\tescr_idx = p4_get_escr_idx(bind->escr_msr[thread]);\n\t\tif (unlikely(escr_idx == -1))\n\t\t\tgoto done;\n\n\t\tif (hwc->idx != -1 && !p4_should_swap_ts(hwc->config, cpu)) {\n\t\t\tcntr_idx = hwc->idx;\n\t\t\tif (assign)\n\t\t\t\tassign[i] = hwc->idx;\n\t\t\tgoto reserve;\n\t\t}\n\n\t\tcntr_idx = p4_next_cntr(thread, used_mask, bind);\n\t\tif (cntr_idx == -1 || test_bit(escr_idx, escr_mask))\n\t\t\tgoto done;\n\n\t\tp4_pmu_swap_config_ts(hwc, cpu);\n\t\tif (assign)\n\t\t\tassign[i] = cntr_idx;\nreserve:\n\t\tset_bit(cntr_idx, used_mask);\n\t\tset_bit(escr_idx, escr_mask);\n\t}\n\ndone:\n\treturn num ? -ENOSPC : 0;\n}\n\nstatic __initconst const struct x86_pmu p4_pmu = {\n\t.name\t\t\t= \"Netburst P4/Xeon\",\n\t.handle_irq\t\t= p4_pmu_handle_irq,\n\t.disable_all\t\t= p4_pmu_disable_all,\n\t.enable_all\t\t= p4_pmu_enable_all,\n\t.enable\t\t\t= p4_pmu_enable_event,\n\t.disable\t\t= p4_pmu_disable_event,\n\t.eventsel\t\t= MSR_P4_BPU_CCCR0,\n\t.perfctr\t\t= MSR_P4_BPU_PERFCTR0,\n\t.event_map\t\t= p4_pmu_event_map,\n\t.max_events\t\t= ARRAY_SIZE(p4_general_events),\n\t.get_event_constraints\t= x86_get_event_constraints,\n\t/*\n\t * IF HT disabled we may need to use all\n\t * ARCH_P4_MAX_CCCR counters simulaneously\n\t * though leave it restricted at moment assuming\n\t * HT is on\n\t */\n\t.num_counters\t\t= ARCH_P4_MAX_CCCR,\n\t.apic\t\t\t= 1,\n\t.cntval_bits\t\t= ARCH_P4_CNTRVAL_BITS,\n\t.cntval_mask\t\t= ARCH_P4_CNTRVAL_MASK,\n\t.max_period\t\t= (1ULL << (ARCH_P4_CNTRVAL_BITS - 1)) - 1,\n\t.hw_watchdog_set_attr\t= p4_hw_watchdog_set_attr,\n\t.hw_config\t\t= p4_hw_config,\n\t.schedule_events\t= p4_pmu_schedule_events,\n\t/*\n\t * This handles erratum N15 in intel doc 249199-029,\n\t * the counter may not be updated correctly on write\n\t * so we need a second write operation to do the trick\n\t * (the official workaround didn't work)\n\t *\n\t * the former idea is taken from OProfile code\n\t */\n\t.perfctr_second_write\t= 1,\n};\n\nstatic __init int p4_pmu_init(void)\n{\n\tunsigned int low, high;\n\n\t/* If we get stripped -- indexing fails */\n\tBUILD_BUG_ON(ARCH_P4_MAX_CCCR > X86_PMC_MAX_GENERIC);\n\n\trdmsr(MSR_IA32_MISC_ENABLE, low, high);\n\tif (!(low & (1 << 7))) {\n\t\tpr_cont(\"unsupported Netburst CPU model %d \",\n\t\t\tboot_cpu_data.x86_model);\n\t\treturn -ENODEV;\n\t}\n\n\tmemcpy(hw_cache_event_ids, p4_hw_cache_event_ids,\n\t\tsizeof(hw_cache_event_ids));\n\n\tpr_cont(\"Netburst events, \");\n\n\tx86_pmu = p4_pmu;\n\n\treturn 0;\n}\n\n#endif /* CONFIG_CPU_SUP_INTEL */\n", "/*\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the\n * Free Software Foundation; either version 2, or (at your option) any\n * later version.\n *\n * This program is distributed in the hope that it will be useful, but\n * WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * General Public License for more details.\n *\n */\n\n/*\n * Copyright (C) 2004 Amit S. Kale <amitkale@linsyssoft.com>\n * Copyright (C) 2000-2001 VERITAS Software Corporation.\n * Copyright (C) 2002 Andi Kleen, SuSE Labs\n * Copyright (C) 2004 LinSysSoft Technologies Pvt. Ltd.\n * Copyright (C) 2007 MontaVista Software, Inc.\n * Copyright (C) 2007-2008 Jason Wessel, Wind River Systems, Inc.\n */\n/****************************************************************************\n *  Contributor:     Lake Stevens Instrument Division$\n *  Written by:      Glenn Engel $\n *  Updated by:\t     Amit Kale<akale@veritas.com>\n *  Updated by:\t     Tom Rini <trini@kernel.crashing.org>\n *  Updated by:\t     Jason Wessel <jason.wessel@windriver.com>\n *  Modified for 386 by Jim Kingdon, Cygnus Support.\n *  Origianl kgdb, compatibility with 2.1.xx kernel by\n *  David Grothe <dave@gcom.com>\n *  Integrated into 2.2.5 kernel by Tigran Aivazian <tigran@sco.com>\n *  X86_64 changes from Andi Kleen's patch merged by Jim Houston\n */\n#include <linux/spinlock.h>\n#include <linux/kdebug.h>\n#include <linux/string.h>\n#include <linux/kernel.h>\n#include <linux/ptrace.h>\n#include <linux/sched.h>\n#include <linux/delay.h>\n#include <linux/kgdb.h>\n#include <linux/init.h>\n#include <linux/smp.h>\n#include <linux/nmi.h>\n#include <linux/hw_breakpoint.h>\n\n#include <asm/debugreg.h>\n#include <asm/apicdef.h>\n#include <asm/system.h>\n#include <asm/apic.h>\n#include <asm/nmi.h>\n\nstruct dbg_reg_def_t dbg_reg_def[DBG_MAX_REG_NUM] =\n{\n#ifdef CONFIG_X86_32\n\t{ \"ax\", 4, offsetof(struct pt_regs, ax) },\n\t{ \"cx\", 4, offsetof(struct pt_regs, cx) },\n\t{ \"dx\", 4, offsetof(struct pt_regs, dx) },\n\t{ \"bx\", 4, offsetof(struct pt_regs, bx) },\n\t{ \"sp\", 4, offsetof(struct pt_regs, sp) },\n\t{ \"bp\", 4, offsetof(struct pt_regs, bp) },\n\t{ \"si\", 4, offsetof(struct pt_regs, si) },\n\t{ \"di\", 4, offsetof(struct pt_regs, di) },\n\t{ \"ip\", 4, offsetof(struct pt_regs, ip) },\n\t{ \"flags\", 4, offsetof(struct pt_regs, flags) },\n\t{ \"cs\", 4, offsetof(struct pt_regs, cs) },\n\t{ \"ss\", 4, offsetof(struct pt_regs, ss) },\n\t{ \"ds\", 4, offsetof(struct pt_regs, ds) },\n\t{ \"es\", 4, offsetof(struct pt_regs, es) },\n\t{ \"fs\", 4, -1 },\n\t{ \"gs\", 4, -1 },\n#else\n\t{ \"ax\", 8, offsetof(struct pt_regs, ax) },\n\t{ \"bx\", 8, offsetof(struct pt_regs, bx) },\n\t{ \"cx\", 8, offsetof(struct pt_regs, cx) },\n\t{ \"dx\", 8, offsetof(struct pt_regs, dx) },\n\t{ \"si\", 8, offsetof(struct pt_regs, dx) },\n\t{ \"di\", 8, offsetof(struct pt_regs, di) },\n\t{ \"bp\", 8, offsetof(struct pt_regs, bp) },\n\t{ \"sp\", 8, offsetof(struct pt_regs, sp) },\n\t{ \"r8\", 8, offsetof(struct pt_regs, r8) },\n\t{ \"r9\", 8, offsetof(struct pt_regs, r9) },\n\t{ \"r10\", 8, offsetof(struct pt_regs, r10) },\n\t{ \"r11\", 8, offsetof(struct pt_regs, r11) },\n\t{ \"r12\", 8, offsetof(struct pt_regs, r12) },\n\t{ \"r13\", 8, offsetof(struct pt_regs, r13) },\n\t{ \"r14\", 8, offsetof(struct pt_regs, r14) },\n\t{ \"r15\", 8, offsetof(struct pt_regs, r15) },\n\t{ \"ip\", 8, offsetof(struct pt_regs, ip) },\n\t{ \"flags\", 4, offsetof(struct pt_regs, flags) },\n\t{ \"cs\", 4, offsetof(struct pt_regs, cs) },\n\t{ \"ss\", 4, offsetof(struct pt_regs, ss) },\n#endif\n};\n\nint dbg_set_reg(int regno, void *mem, struct pt_regs *regs)\n{\n\tif (\n#ifdef CONFIG_X86_32\n\t    regno == GDB_SS || regno == GDB_FS || regno == GDB_GS ||\n#endif\n\t    regno == GDB_SP || regno == GDB_ORIG_AX)\n\t\treturn 0;\n\n\tif (dbg_reg_def[regno].offset != -1)\n\t\tmemcpy((void *)regs + dbg_reg_def[regno].offset, mem,\n\t\t       dbg_reg_def[regno].size);\n\treturn 0;\n}\n\nchar *dbg_get_reg(int regno, void *mem, struct pt_regs *regs)\n{\n\tif (regno == GDB_ORIG_AX) {\n\t\tmemcpy(mem, &regs->orig_ax, sizeof(regs->orig_ax));\n\t\treturn \"orig_ax\";\n\t}\n\tif (regno >= DBG_MAX_REG_NUM || regno < 0)\n\t\treturn NULL;\n\n\tif (dbg_reg_def[regno].offset != -1)\n\t\tmemcpy(mem, (void *)regs + dbg_reg_def[regno].offset,\n\t\t       dbg_reg_def[regno].size);\n\n#ifdef CONFIG_X86_32\n\tswitch (regno) {\n\tcase GDB_SS:\n\t\tif (!user_mode_vm(regs))\n\t\t\t*(unsigned long *)mem = __KERNEL_DS;\n\t\tbreak;\n\tcase GDB_SP:\n\t\tif (!user_mode_vm(regs))\n\t\t\t*(unsigned long *)mem = kernel_stack_pointer(regs);\n\t\tbreak;\n\tcase GDB_GS:\n\tcase GDB_FS:\n\t\t*(unsigned long *)mem = 0xFFFF;\n\t\tbreak;\n\t}\n#endif\n\treturn dbg_reg_def[regno].name;\n}\n\n/**\n *\tsleeping_thread_to_gdb_regs - Convert ptrace regs to GDB regs\n *\t@gdb_regs: A pointer to hold the registers in the order GDB wants.\n *\t@p: The &struct task_struct of the desired process.\n *\n *\tConvert the register values of the sleeping process in @p to\n *\tthe format that GDB expects.\n *\tThis function is called when kgdb does not have access to the\n *\t&struct pt_regs and therefore it should fill the gdb registers\n *\t@gdb_regs with what has\tbeen saved in &struct thread_struct\n *\tthread field during switch_to.\n */\nvoid sleeping_thread_to_gdb_regs(unsigned long *gdb_regs, struct task_struct *p)\n{\n#ifndef CONFIG_X86_32\n\tu32 *gdb_regs32 = (u32 *)gdb_regs;\n#endif\n\tgdb_regs[GDB_AX]\t= 0;\n\tgdb_regs[GDB_BX]\t= 0;\n\tgdb_regs[GDB_CX]\t= 0;\n\tgdb_regs[GDB_DX]\t= 0;\n\tgdb_regs[GDB_SI]\t= 0;\n\tgdb_regs[GDB_DI]\t= 0;\n\tgdb_regs[GDB_BP]\t= *(unsigned long *)p->thread.sp;\n#ifdef CONFIG_X86_32\n\tgdb_regs[GDB_DS]\t= __KERNEL_DS;\n\tgdb_regs[GDB_ES]\t= __KERNEL_DS;\n\tgdb_regs[GDB_PS]\t= 0;\n\tgdb_regs[GDB_CS]\t= __KERNEL_CS;\n\tgdb_regs[GDB_PC]\t= p->thread.ip;\n\tgdb_regs[GDB_SS]\t= __KERNEL_DS;\n\tgdb_regs[GDB_FS]\t= 0xFFFF;\n\tgdb_regs[GDB_GS]\t= 0xFFFF;\n#else\n\tgdb_regs32[GDB_PS]\t= *(unsigned long *)(p->thread.sp + 8);\n\tgdb_regs32[GDB_CS]\t= __KERNEL_CS;\n\tgdb_regs32[GDB_SS]\t= __KERNEL_DS;\n\tgdb_regs[GDB_PC]\t= 0;\n\tgdb_regs[GDB_R8]\t= 0;\n\tgdb_regs[GDB_R9]\t= 0;\n\tgdb_regs[GDB_R10]\t= 0;\n\tgdb_regs[GDB_R11]\t= 0;\n\tgdb_regs[GDB_R12]\t= 0;\n\tgdb_regs[GDB_R13]\t= 0;\n\tgdb_regs[GDB_R14]\t= 0;\n\tgdb_regs[GDB_R15]\t= 0;\n#endif\n\tgdb_regs[GDB_SP]\t= p->thread.sp;\n}\n\nstatic struct hw_breakpoint {\n\tunsigned\t\tenabled;\n\tunsigned long\t\taddr;\n\tint\t\t\tlen;\n\tint\t\t\ttype;\n\tstruct perf_event\t* __percpu *pev;\n} breakinfo[HBP_NUM];\n\nstatic unsigned long early_dr7;\n\nstatic void kgdb_correct_hw_break(void)\n{\n\tint breakno;\n\n\tfor (breakno = 0; breakno < HBP_NUM; breakno++) {\n\t\tstruct perf_event *bp;\n\t\tstruct arch_hw_breakpoint *info;\n\t\tint val;\n\t\tint cpu = raw_smp_processor_id();\n\t\tif (!breakinfo[breakno].enabled)\n\t\t\tcontinue;\n\t\tif (dbg_is_early) {\n\t\t\tset_debugreg(breakinfo[breakno].addr, breakno);\n\t\t\tearly_dr7 |= encode_dr7(breakno,\n\t\t\t\t\t\tbreakinfo[breakno].len,\n\t\t\t\t\t\tbreakinfo[breakno].type);\n\t\t\tset_debugreg(early_dr7, 7);\n\t\t\tcontinue;\n\t\t}\n\t\tbp = *per_cpu_ptr(breakinfo[breakno].pev, cpu);\n\t\tinfo = counter_arch_bp(bp);\n\t\tif (bp->attr.disabled != 1)\n\t\t\tcontinue;\n\t\tbp->attr.bp_addr = breakinfo[breakno].addr;\n\t\tbp->attr.bp_len = breakinfo[breakno].len;\n\t\tbp->attr.bp_type = breakinfo[breakno].type;\n\t\tinfo->address = breakinfo[breakno].addr;\n\t\tinfo->len = breakinfo[breakno].len;\n\t\tinfo->type = breakinfo[breakno].type;\n\t\tval = arch_install_hw_breakpoint(bp);\n\t\tif (!val)\n\t\t\tbp->attr.disabled = 0;\n\t}\n\tif (!dbg_is_early)\n\t\thw_breakpoint_restore();\n}\n\nstatic int hw_break_reserve_slot(int breakno)\n{\n\tint cpu;\n\tint cnt = 0;\n\tstruct perf_event **pevent;\n\n\tif (dbg_is_early)\n\t\treturn 0;\n\n\tfor_each_online_cpu(cpu) {\n\t\tcnt++;\n\t\tpevent = per_cpu_ptr(breakinfo[breakno].pev, cpu);\n\t\tif (dbg_reserve_bp_slot(*pevent))\n\t\t\tgoto fail;\n\t}\n\n\treturn 0;\n\nfail:\n\tfor_each_online_cpu(cpu) {\n\t\tcnt--;\n\t\tif (!cnt)\n\t\t\tbreak;\n\t\tpevent = per_cpu_ptr(breakinfo[breakno].pev, cpu);\n\t\tdbg_release_bp_slot(*pevent);\n\t}\n\treturn -1;\n}\n\nstatic int hw_break_release_slot(int breakno)\n{\n\tstruct perf_event **pevent;\n\tint cpu;\n\n\tif (dbg_is_early)\n\t\treturn 0;\n\n\tfor_each_online_cpu(cpu) {\n\t\tpevent = per_cpu_ptr(breakinfo[breakno].pev, cpu);\n\t\tif (dbg_release_bp_slot(*pevent))\n\t\t\t/*\n\t\t\t * The debugger is responsible for handing the retry on\n\t\t\t * remove failure.\n\t\t\t */\n\t\t\treturn -1;\n\t}\n\treturn 0;\n}\n\nstatic int\nkgdb_remove_hw_break(unsigned long addr, int len, enum kgdb_bptype bptype)\n{\n\tint i;\n\n\tfor (i = 0; i < HBP_NUM; i++)\n\t\tif (breakinfo[i].addr == addr && breakinfo[i].enabled)\n\t\t\tbreak;\n\tif (i == HBP_NUM)\n\t\treturn -1;\n\n\tif (hw_break_release_slot(i)) {\n\t\tprintk(KERN_ERR \"Cannot remove hw breakpoint at %lx\\n\", addr);\n\t\treturn -1;\n\t}\n\tbreakinfo[i].enabled = 0;\n\n\treturn 0;\n}\n\nstatic void kgdb_remove_all_hw_break(void)\n{\n\tint i;\n\tint cpu = raw_smp_processor_id();\n\tstruct perf_event *bp;\n\n\tfor (i = 0; i < HBP_NUM; i++) {\n\t\tif (!breakinfo[i].enabled)\n\t\t\tcontinue;\n\t\tbp = *per_cpu_ptr(breakinfo[i].pev, cpu);\n\t\tif (!bp->attr.disabled) {\n\t\t\tarch_uninstall_hw_breakpoint(bp);\n\t\t\tbp->attr.disabled = 1;\n\t\t\tcontinue;\n\t\t}\n\t\tif (dbg_is_early)\n\t\t\tearly_dr7 &= ~encode_dr7(i, breakinfo[i].len,\n\t\t\t\t\t\t breakinfo[i].type);\n\t\telse if (hw_break_release_slot(i))\n\t\t\tprintk(KERN_ERR \"KGDB: hw bpt remove failed %lx\\n\",\n\t\t\t       breakinfo[i].addr);\n\t\tbreakinfo[i].enabled = 0;\n\t}\n}\n\nstatic int\nkgdb_set_hw_break(unsigned long addr, int len, enum kgdb_bptype bptype)\n{\n\tint i;\n\n\tfor (i = 0; i < HBP_NUM; i++)\n\t\tif (!breakinfo[i].enabled)\n\t\t\tbreak;\n\tif (i == HBP_NUM)\n\t\treturn -1;\n\n\tswitch (bptype) {\n\tcase BP_HARDWARE_BREAKPOINT:\n\t\tlen = 1;\n\t\tbreakinfo[i].type = X86_BREAKPOINT_EXECUTE;\n\t\tbreak;\n\tcase BP_WRITE_WATCHPOINT:\n\t\tbreakinfo[i].type = X86_BREAKPOINT_WRITE;\n\t\tbreak;\n\tcase BP_ACCESS_WATCHPOINT:\n\t\tbreakinfo[i].type = X86_BREAKPOINT_RW;\n\t\tbreak;\n\tdefault:\n\t\treturn -1;\n\t}\n\tswitch (len) {\n\tcase 1:\n\t\tbreakinfo[i].len = X86_BREAKPOINT_LEN_1;\n\t\tbreak;\n\tcase 2:\n\t\tbreakinfo[i].len = X86_BREAKPOINT_LEN_2;\n\t\tbreak;\n\tcase 4:\n\t\tbreakinfo[i].len = X86_BREAKPOINT_LEN_4;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase 8:\n\t\tbreakinfo[i].len = X86_BREAKPOINT_LEN_8;\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn -1;\n\t}\n\tbreakinfo[i].addr = addr;\n\tif (hw_break_reserve_slot(i)) {\n\t\tbreakinfo[i].addr = 0;\n\t\treturn -1;\n\t}\n\tbreakinfo[i].enabled = 1;\n\n\treturn 0;\n}\n\n/**\n *\tkgdb_disable_hw_debug - Disable hardware debugging while we in kgdb.\n *\t@regs: Current &struct pt_regs.\n *\n *\tThis function will be called if the particular architecture must\n *\tdisable hardware debugging while it is processing gdb packets or\n *\thandling exception.\n */\nstatic void kgdb_disable_hw_debug(struct pt_regs *regs)\n{\n\tint i;\n\tint cpu = raw_smp_processor_id();\n\tstruct perf_event *bp;\n\n\t/* Disable hardware debugging while we are in kgdb: */\n\tset_debugreg(0UL, 7);\n\tfor (i = 0; i < HBP_NUM; i++) {\n\t\tif (!breakinfo[i].enabled)\n\t\t\tcontinue;\n\t\tif (dbg_is_early) {\n\t\t\tearly_dr7 &= ~encode_dr7(i, breakinfo[i].len,\n\t\t\t\t\t\t breakinfo[i].type);\n\t\t\tcontinue;\n\t\t}\n\t\tbp = *per_cpu_ptr(breakinfo[i].pev, cpu);\n\t\tif (bp->attr.disabled == 1)\n\t\t\tcontinue;\n\t\tarch_uninstall_hw_breakpoint(bp);\n\t\tbp->attr.disabled = 1;\n\t}\n}\n\n#ifdef CONFIG_SMP\n/**\n *\tkgdb_roundup_cpus - Get other CPUs into a holding pattern\n *\t@flags: Current IRQ state\n *\n *\tOn SMP systems, we need to get the attention of the other CPUs\n *\tand get them be in a known state.  This should do what is needed\n *\tto get the other CPUs to call kgdb_wait(). Note that on some arches,\n *\tthe NMI approach is not used for rounding up all the CPUs. For example,\n *\tin case of MIPS, smp_call_function() is used to roundup CPUs. In\n *\tthis case, we have to make sure that interrupts are enabled before\n *\tcalling smp_call_function(). The argument to this function is\n *\tthe flags that will be used when restoring the interrupts. There is\n *\tlocal_irq_save() call before kgdb_roundup_cpus().\n *\n *\tOn non-SMP systems, this is not called.\n */\nvoid kgdb_roundup_cpus(unsigned long flags)\n{\n\tapic->send_IPI_allbutself(APIC_DM_NMI);\n}\n#endif\n\n/**\n *\tkgdb_arch_handle_exception - Handle architecture specific GDB packets.\n *\t@vector: The error vector of the exception that happened.\n *\t@signo: The signal number of the exception that happened.\n *\t@err_code: The error code of the exception that happened.\n *\t@remcom_in_buffer: The buffer of the packet we have read.\n *\t@remcom_out_buffer: The buffer of %BUFMAX bytes to write a packet into.\n *\t@regs: The &struct pt_regs of the current process.\n *\n *\tThis function MUST handle the 'c' and 's' command packets,\n *\tas well packets to set / remove a hardware breakpoint, if used.\n *\tIf there are additional packets which the hardware needs to handle,\n *\tthey are handled here.  The code should return -1 if it wants to\n *\tprocess more packets, and a %0 or %1 if it wants to exit from the\n *\tkgdb callback.\n */\nint kgdb_arch_handle_exception(int e_vector, int signo, int err_code,\n\t\t\t       char *remcomInBuffer, char *remcomOutBuffer,\n\t\t\t       struct pt_regs *linux_regs)\n{\n\tunsigned long addr;\n\tchar *ptr;\n\n\tswitch (remcomInBuffer[0]) {\n\tcase 'c':\n\tcase 's':\n\t\t/* try to read optional parameter, pc unchanged if no parm */\n\t\tptr = &remcomInBuffer[1];\n\t\tif (kgdb_hex2long(&ptr, &addr))\n\t\t\tlinux_regs->ip = addr;\n\tcase 'D':\n\tcase 'k':\n\t\t/* clear the trace bit */\n\t\tlinux_regs->flags &= ~X86_EFLAGS_TF;\n\t\tatomic_set(&kgdb_cpu_doing_single_step, -1);\n\n\t\t/* set the trace bit if we're stepping */\n\t\tif (remcomInBuffer[0] == 's') {\n\t\t\tlinux_regs->flags |= X86_EFLAGS_TF;\n\t\t\tatomic_set(&kgdb_cpu_doing_single_step,\n\t\t\t\t   raw_smp_processor_id());\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\t/* this means that we do not want to exit from the handler: */\n\treturn -1;\n}\n\nstatic inline int\nsingle_step_cont(struct pt_regs *regs, struct die_args *args)\n{\n\t/*\n\t * Single step exception from kernel space to user space so\n\t * eat the exception and continue the process:\n\t */\n\tprintk(KERN_ERR \"KGDB: trap/step from kernel to user space, \"\n\t\t\t\"resuming...\\n\");\n\tkgdb_arch_handle_exception(args->trapnr, args->signr,\n\t\t\t\t   args->err, \"c\", \"\", regs);\n\t/*\n\t * Reset the BS bit in dr6 (pointed by args->err) to\n\t * denote completion of processing\n\t */\n\t(*(unsigned long *)ERR_PTR(args->err)) &= ~DR_STEP;\n\n\treturn NOTIFY_STOP;\n}\n\nstatic int was_in_debug_nmi[NR_CPUS];\n\nstatic int __kgdb_notify(struct die_args *args, unsigned long cmd)\n{\n\tstruct pt_regs *regs = args->regs;\n\n\tswitch (cmd) {\n\tcase DIE_NMI:\n\t\tif (atomic_read(&kgdb_active) != -1) {\n\t\t\t/* KGDB CPU roundup */\n\t\t\tkgdb_nmicallback(raw_smp_processor_id(), regs);\n\t\t\twas_in_debug_nmi[raw_smp_processor_id()] = 1;\n\t\t\ttouch_nmi_watchdog();\n\t\t\treturn NOTIFY_STOP;\n\t\t}\n\t\treturn NOTIFY_DONE;\n\n\tcase DIE_NMIUNKNOWN:\n\t\tif (was_in_debug_nmi[raw_smp_processor_id()]) {\n\t\t\twas_in_debug_nmi[raw_smp_processor_id()] = 0;\n\t\t\treturn NOTIFY_STOP;\n\t\t}\n\t\treturn NOTIFY_DONE;\n\n\tcase DIE_DEBUG:\n\t\tif (atomic_read(&kgdb_cpu_doing_single_step) != -1) {\n\t\t\tif (user_mode(regs))\n\t\t\t\treturn single_step_cont(regs, args);\n\t\t\tbreak;\n\t\t} else if (test_thread_flag(TIF_SINGLESTEP))\n\t\t\t/* This means a user thread is single stepping\n\t\t\t * a system call which should be ignored\n\t\t\t */\n\t\t\treturn NOTIFY_DONE;\n\t\t/* fall through */\n\tdefault:\n\t\tif (user_mode(regs))\n\t\t\treturn NOTIFY_DONE;\n\t}\n\n\tif (kgdb_handle_exception(args->trapnr, args->signr, cmd, regs))\n\t\treturn NOTIFY_DONE;\n\n\t/* Must touch watchdog before return to normal operation */\n\ttouch_nmi_watchdog();\n\treturn NOTIFY_STOP;\n}\n\nint kgdb_ll_trap(int cmd, const char *str,\n\t\t struct pt_regs *regs, long err, int trap, int sig)\n{\n\tstruct die_args args = {\n\t\t.regs\t= regs,\n\t\t.str\t= str,\n\t\t.err\t= err,\n\t\t.trapnr\t= trap,\n\t\t.signr\t= sig,\n\n\t};\n\n\tif (!kgdb_io_module_registered)\n\t\treturn NOTIFY_DONE;\n\n\treturn __kgdb_notify(&args, cmd);\n}\n\nstatic int\nkgdb_notify(struct notifier_block *self, unsigned long cmd, void *ptr)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tlocal_irq_save(flags);\n\tret = __kgdb_notify(ptr, cmd);\n\tlocal_irq_restore(flags);\n\n\treturn ret;\n}\n\nstatic struct notifier_block kgdb_notifier = {\n\t.notifier_call\t= kgdb_notify,\n\n\t/*\n\t * Lowest-prio notifier priority, we want to be notified last:\n\t */\n\t.priority\t= NMI_LOCAL_LOW_PRIOR,\n};\n\n/**\n *\tkgdb_arch_init - Perform any architecture specific initalization.\n *\n *\tThis function will handle the initalization of any architecture\n *\tspecific callbacks.\n */\nint kgdb_arch_init(void)\n{\n\treturn register_die_notifier(&kgdb_notifier);\n}\n\nstatic void kgdb_hw_overflow_handler(struct perf_event *event, int nmi,\n\t\tstruct perf_sample_data *data, struct pt_regs *regs)\n{\n\tstruct task_struct *tsk = current;\n\tint i;\n\n\tfor (i = 0; i < 4; i++)\n\t\tif (breakinfo[i].enabled)\n\t\t\ttsk->thread.debugreg6 |= (DR_TRAP0 << i);\n}\n\nvoid kgdb_arch_late(void)\n{\n\tint i, cpu;\n\tstruct perf_event_attr attr;\n\tstruct perf_event **pevent;\n\n\t/*\n\t * Pre-allocate the hw breakpoint structions in the non-atomic\n\t * portion of kgdb because this operation requires mutexs to\n\t * complete.\n\t */\n\thw_breakpoint_init(&attr);\n\tattr.bp_addr = (unsigned long)kgdb_arch_init;\n\tattr.bp_len = HW_BREAKPOINT_LEN_1;\n\tattr.bp_type = HW_BREAKPOINT_W;\n\tattr.disabled = 1;\n\tfor (i = 0; i < HBP_NUM; i++) {\n\t\tif (breakinfo[i].pev)\n\t\t\tcontinue;\n\t\tbreakinfo[i].pev = register_wide_hw_breakpoint(&attr, NULL);\n\t\tif (IS_ERR((void * __force)breakinfo[i].pev)) {\n\t\t\tprintk(KERN_ERR \"kgdb: Could not allocate hw\"\n\t\t\t       \"breakpoints\\nDisabling the kernel debugger\\n\");\n\t\t\tbreakinfo[i].pev = NULL;\n\t\t\tkgdb_arch_exit();\n\t\t\treturn;\n\t\t}\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tpevent = per_cpu_ptr(breakinfo[i].pev, cpu);\n\t\t\tpevent[0]->hw.sample_period = 1;\n\t\t\tpevent[0]->overflow_handler = kgdb_hw_overflow_handler;\n\t\t\tif (pevent[0]->destroy != NULL) {\n\t\t\t\tpevent[0]->destroy = NULL;\n\t\t\t\trelease_bp_slot(*pevent);\n\t\t\t}\n\t\t}\n\t}\n}\n\n/**\n *\tkgdb_arch_exit - Perform any architecture specific uninitalization.\n *\n *\tThis function will handle the uninitalization of any architecture\n *\tspecific callbacks, for dynamic registration and unregistration.\n */\nvoid kgdb_arch_exit(void)\n{\n\tint i;\n\tfor (i = 0; i < 4; i++) {\n\t\tif (breakinfo[i].pev) {\n\t\t\tunregister_wide_hw_breakpoint(breakinfo[i].pev);\n\t\t\tbreakinfo[i].pev = NULL;\n\t\t}\n\t}\n\tunregister_die_notifier(&kgdb_notifier);\n}\n\n/**\n *\n *\tkgdb_skipexception - Bail out of KGDB when we've been triggered.\n *\t@exception: Exception vector number\n *\t@regs: Current &struct pt_regs.\n *\n *\tOn some architectures we need to skip a breakpoint exception when\n *\tit occurs after a breakpoint has been removed.\n *\n * Skip an int3 exception when it occurs after a breakpoint has been\n * removed. Backtrack eip by 1 since the int3 would have caused it to\n * increment by 1.\n */\nint kgdb_skipexception(int exception, struct pt_regs *regs)\n{\n\tif (exception == 3 && kgdb_isremovedbreak(regs->ip - 1)) {\n\t\tregs->ip -= 1;\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nunsigned long kgdb_arch_pc(int exception, struct pt_regs *regs)\n{\n\tif (exception == 3)\n\t\treturn instruction_pointer(regs) - 1;\n\treturn instruction_pointer(regs);\n}\n\nvoid kgdb_arch_set_pc(struct pt_regs *regs, unsigned long ip)\n{\n\tregs->ip = ip;\n}\n\nstruct kgdb_arch arch_kgdb_ops = {\n\t/* Breakpoint instruction: */\n\t.gdb_bpt_instr\t\t= { 0xcc },\n\t.flags\t\t\t= KGDB_HW_BREAKPOINT,\n\t.set_hw_breakpoint\t= kgdb_set_hw_break,\n\t.remove_hw_breakpoint\t= kgdb_remove_hw_break,\n\t.disable_hw_break\t= kgdb_disable_hw_debug,\n\t.remove_all_hw_break\t= kgdb_remove_all_hw_break,\n\t.correct_hw_break\t= kgdb_correct_hw_break,\n};\n", "/* By Ross Biro 1/23/92 */\n/*\n * Pentium III FXSR, SSE support\n *\tGareth Hughes <gareth@valinux.com>, May 2000\n */\n\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/errno.h>\n#include <linux/slab.h>\n#include <linux/ptrace.h>\n#include <linux/regset.h>\n#include <linux/tracehook.h>\n#include <linux/user.h>\n#include <linux/elf.h>\n#include <linux/security.h>\n#include <linux/audit.h>\n#include <linux/seccomp.h>\n#include <linux/signal.h>\n#include <linux/perf_event.h>\n#include <linux/hw_breakpoint.h>\n\n#include <asm/uaccess.h>\n#include <asm/pgtable.h>\n#include <asm/system.h>\n#include <asm/processor.h>\n#include <asm/i387.h>\n#include <asm/debugreg.h>\n#include <asm/ldt.h>\n#include <asm/desc.h>\n#include <asm/prctl.h>\n#include <asm/proto.h>\n#include <asm/hw_breakpoint.h>\n\n#include \"tls.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/syscalls.h>\n\nenum x86_regset {\n\tREGSET_GENERAL,\n\tREGSET_FP,\n\tREGSET_XFP,\n\tREGSET_IOPERM64 = REGSET_XFP,\n\tREGSET_XSTATE,\n\tREGSET_TLS,\n\tREGSET_IOPERM32,\n};\n\nstruct pt_regs_offset {\n\tconst char *name;\n\tint offset;\n};\n\n#define REG_OFFSET_NAME(r) {.name = #r, .offset = offsetof(struct pt_regs, r)}\n#define REG_OFFSET_END {.name = NULL, .offset = 0}\n\nstatic const struct pt_regs_offset regoffset_table[] = {\n#ifdef CONFIG_X86_64\n\tREG_OFFSET_NAME(r15),\n\tREG_OFFSET_NAME(r14),\n\tREG_OFFSET_NAME(r13),\n\tREG_OFFSET_NAME(r12),\n\tREG_OFFSET_NAME(r11),\n\tREG_OFFSET_NAME(r10),\n\tREG_OFFSET_NAME(r9),\n\tREG_OFFSET_NAME(r8),\n#endif\n\tREG_OFFSET_NAME(bx),\n\tREG_OFFSET_NAME(cx),\n\tREG_OFFSET_NAME(dx),\n\tREG_OFFSET_NAME(si),\n\tREG_OFFSET_NAME(di),\n\tREG_OFFSET_NAME(bp),\n\tREG_OFFSET_NAME(ax),\n#ifdef CONFIG_X86_32\n\tREG_OFFSET_NAME(ds),\n\tREG_OFFSET_NAME(es),\n\tREG_OFFSET_NAME(fs),\n\tREG_OFFSET_NAME(gs),\n#endif\n\tREG_OFFSET_NAME(orig_ax),\n\tREG_OFFSET_NAME(ip),\n\tREG_OFFSET_NAME(cs),\n\tREG_OFFSET_NAME(flags),\n\tREG_OFFSET_NAME(sp),\n\tREG_OFFSET_NAME(ss),\n\tREG_OFFSET_END,\n};\n\n/**\n * regs_query_register_offset() - query register offset from its name\n * @name:\tthe name of a register\n *\n * regs_query_register_offset() returns the offset of a register in struct\n * pt_regs from its name. If the name is invalid, this returns -EINVAL;\n */\nint regs_query_register_offset(const char *name)\n{\n\tconst struct pt_regs_offset *roff;\n\tfor (roff = regoffset_table; roff->name != NULL; roff++)\n\t\tif (!strcmp(roff->name, name))\n\t\t\treturn roff->offset;\n\treturn -EINVAL;\n}\n\n/**\n * regs_query_register_name() - query register name from its offset\n * @offset:\tthe offset of a register in struct pt_regs.\n *\n * regs_query_register_name() returns the name of a register from its\n * offset in struct pt_regs. If the @offset is invalid, this returns NULL;\n */\nconst char *regs_query_register_name(unsigned int offset)\n{\n\tconst struct pt_regs_offset *roff;\n\tfor (roff = regoffset_table; roff->name != NULL; roff++)\n\t\tif (roff->offset == offset)\n\t\t\treturn roff->name;\n\treturn NULL;\n}\n\nstatic const int arg_offs_table[] = {\n#ifdef CONFIG_X86_32\n\t[0] = offsetof(struct pt_regs, ax),\n\t[1] = offsetof(struct pt_regs, dx),\n\t[2] = offsetof(struct pt_regs, cx)\n#else /* CONFIG_X86_64 */\n\t[0] = offsetof(struct pt_regs, di),\n\t[1] = offsetof(struct pt_regs, si),\n\t[2] = offsetof(struct pt_regs, dx),\n\t[3] = offsetof(struct pt_regs, cx),\n\t[4] = offsetof(struct pt_regs, r8),\n\t[5] = offsetof(struct pt_regs, r9)\n#endif\n};\n\n/*\n * does not yet catch signals sent when the child dies.\n * in exit.c or in signal.c.\n */\n\n/*\n * Determines which flags the user has access to [1 = access, 0 = no access].\n */\n#define FLAG_MASK_32\t\t((unsigned long)\t\t\t\\\n\t\t\t\t (X86_EFLAGS_CF | X86_EFLAGS_PF |\t\\\n\t\t\t\t  X86_EFLAGS_AF | X86_EFLAGS_ZF |\t\\\n\t\t\t\t  X86_EFLAGS_SF | X86_EFLAGS_TF |\t\\\n\t\t\t\t  X86_EFLAGS_DF | X86_EFLAGS_OF |\t\\\n\t\t\t\t  X86_EFLAGS_RF | X86_EFLAGS_AC))\n\n/*\n * Determines whether a value may be installed in a segment register.\n */\nstatic inline bool invalid_selector(u16 value)\n{\n\treturn unlikely(value != 0 && (value & SEGMENT_RPL_MASK) != USER_RPL);\n}\n\n#ifdef CONFIG_X86_32\n\n#define FLAG_MASK\t\tFLAG_MASK_32\n\nstatic unsigned long *pt_regs_access(struct pt_regs *regs, unsigned long regno)\n{\n\tBUILD_BUG_ON(offsetof(struct pt_regs, bx) != 0);\n\treturn &regs->bx + (regno >> 2);\n}\n\nstatic u16 get_segment_reg(struct task_struct *task, unsigned long offset)\n{\n\t/*\n\t * Returning the value truncates it to 16 bits.\n\t */\n\tunsigned int retval;\n\tif (offset != offsetof(struct user_regs_struct, gs))\n\t\tretval = *pt_regs_access(task_pt_regs(task), offset);\n\telse {\n\t\tif (task == current)\n\t\t\tretval = get_user_gs(task_pt_regs(task));\n\t\telse\n\t\t\tretval = task_user_gs(task);\n\t}\n\treturn retval;\n}\n\nstatic int set_segment_reg(struct task_struct *task,\n\t\t\t   unsigned long offset, u16 value)\n{\n\t/*\n\t * The value argument was already truncated to 16 bits.\n\t */\n\tif (invalid_selector(value))\n\t\treturn -EIO;\n\n\t/*\n\t * For %cs and %ss we cannot permit a null selector.\n\t * We can permit a bogus selector as long as it has USER_RPL.\n\t * Null selectors are fine for other segment registers, but\n\t * we will never get back to user mode with invalid %cs or %ss\n\t * and will take the trap in iret instead.  Much code relies\n\t * on user_mode() to distinguish a user trap frame (which can\n\t * safely use invalid selectors) from a kernel trap frame.\n\t */\n\tswitch (offset) {\n\tcase offsetof(struct user_regs_struct, cs):\n\tcase offsetof(struct user_regs_struct, ss):\n\t\tif (unlikely(value == 0))\n\t\t\treturn -EIO;\n\n\tdefault:\n\t\t*pt_regs_access(task_pt_regs(task), offset) = value;\n\t\tbreak;\n\n\tcase offsetof(struct user_regs_struct, gs):\n\t\tif (task == current)\n\t\t\tset_user_gs(task_pt_regs(task), value);\n\t\telse\n\t\t\ttask_user_gs(task) = value;\n\t}\n\n\treturn 0;\n}\n\n#else  /* CONFIG_X86_64 */\n\n#define FLAG_MASK\t\t(FLAG_MASK_32 | X86_EFLAGS_NT)\n\nstatic unsigned long *pt_regs_access(struct pt_regs *regs, unsigned long offset)\n{\n\tBUILD_BUG_ON(offsetof(struct pt_regs, r15) != 0);\n\treturn &regs->r15 + (offset / sizeof(regs->r15));\n}\n\nstatic u16 get_segment_reg(struct task_struct *task, unsigned long offset)\n{\n\t/*\n\t * Returning the value truncates it to 16 bits.\n\t */\n\tunsigned int seg;\n\n\tswitch (offset) {\n\tcase offsetof(struct user_regs_struct, fs):\n\t\tif (task == current) {\n\t\t\t/* Older gas can't assemble movq %?s,%r?? */\n\t\t\tasm(\"movl %%fs,%0\" : \"=r\" (seg));\n\t\t\treturn seg;\n\t\t}\n\t\treturn task->thread.fsindex;\n\tcase offsetof(struct user_regs_struct, gs):\n\t\tif (task == current) {\n\t\t\tasm(\"movl %%gs,%0\" : \"=r\" (seg));\n\t\t\treturn seg;\n\t\t}\n\t\treturn task->thread.gsindex;\n\tcase offsetof(struct user_regs_struct, ds):\n\t\tif (task == current) {\n\t\t\tasm(\"movl %%ds,%0\" : \"=r\" (seg));\n\t\t\treturn seg;\n\t\t}\n\t\treturn task->thread.ds;\n\tcase offsetof(struct user_regs_struct, es):\n\t\tif (task == current) {\n\t\t\tasm(\"movl %%es,%0\" : \"=r\" (seg));\n\t\t\treturn seg;\n\t\t}\n\t\treturn task->thread.es;\n\n\tcase offsetof(struct user_regs_struct, cs):\n\tcase offsetof(struct user_regs_struct, ss):\n\t\tbreak;\n\t}\n\treturn *pt_regs_access(task_pt_regs(task), offset);\n}\n\nstatic int set_segment_reg(struct task_struct *task,\n\t\t\t   unsigned long offset, u16 value)\n{\n\t/*\n\t * The value argument was already truncated to 16 bits.\n\t */\n\tif (invalid_selector(value))\n\t\treturn -EIO;\n\n\tswitch (offset) {\n\tcase offsetof(struct user_regs_struct,fs):\n\t\t/*\n\t\t * If this is setting fs as for normal 64-bit use but\n\t\t * setting fs_base has implicitly changed it, leave it.\n\t\t */\n\t\tif ((value == FS_TLS_SEL && task->thread.fsindex == 0 &&\n\t\t     task->thread.fs != 0) ||\n\t\t    (value == 0 && task->thread.fsindex == FS_TLS_SEL &&\n\t\t     task->thread.fs == 0))\n\t\t\tbreak;\n\t\ttask->thread.fsindex = value;\n\t\tif (task == current)\n\t\t\tloadsegment(fs, task->thread.fsindex);\n\t\tbreak;\n\tcase offsetof(struct user_regs_struct,gs):\n\t\t/*\n\t\t * If this is setting gs as for normal 64-bit use but\n\t\t * setting gs_base has implicitly changed it, leave it.\n\t\t */\n\t\tif ((value == GS_TLS_SEL && task->thread.gsindex == 0 &&\n\t\t     task->thread.gs != 0) ||\n\t\t    (value == 0 && task->thread.gsindex == GS_TLS_SEL &&\n\t\t     task->thread.gs == 0))\n\t\t\tbreak;\n\t\ttask->thread.gsindex = value;\n\t\tif (task == current)\n\t\t\tload_gs_index(task->thread.gsindex);\n\t\tbreak;\n\tcase offsetof(struct user_regs_struct,ds):\n\t\ttask->thread.ds = value;\n\t\tif (task == current)\n\t\t\tloadsegment(ds, task->thread.ds);\n\t\tbreak;\n\tcase offsetof(struct user_regs_struct,es):\n\t\ttask->thread.es = value;\n\t\tif (task == current)\n\t\t\tloadsegment(es, task->thread.es);\n\t\tbreak;\n\n\t\t/*\n\t\t * Can't actually change these in 64-bit mode.\n\t\t */\n\tcase offsetof(struct user_regs_struct,cs):\n\t\tif (unlikely(value == 0))\n\t\t\treturn -EIO;\n#ifdef CONFIG_IA32_EMULATION\n\t\tif (test_tsk_thread_flag(task, TIF_IA32))\n\t\t\ttask_pt_regs(task)->cs = value;\n#endif\n\t\tbreak;\n\tcase offsetof(struct user_regs_struct,ss):\n\t\tif (unlikely(value == 0))\n\t\t\treturn -EIO;\n#ifdef CONFIG_IA32_EMULATION\n\t\tif (test_tsk_thread_flag(task, TIF_IA32))\n\t\t\ttask_pt_regs(task)->ss = value;\n#endif\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\n#endif\t/* CONFIG_X86_32 */\n\nstatic unsigned long get_flags(struct task_struct *task)\n{\n\tunsigned long retval = task_pt_regs(task)->flags;\n\n\t/*\n\t * If the debugger set TF, hide it from the readout.\n\t */\n\tif (test_tsk_thread_flag(task, TIF_FORCED_TF))\n\t\tretval &= ~X86_EFLAGS_TF;\n\n\treturn retval;\n}\n\nstatic int set_flags(struct task_struct *task, unsigned long value)\n{\n\tstruct pt_regs *regs = task_pt_regs(task);\n\n\t/*\n\t * If the user value contains TF, mark that\n\t * it was not \"us\" (the debugger) that set it.\n\t * If not, make sure it stays set if we had.\n\t */\n\tif (value & X86_EFLAGS_TF)\n\t\tclear_tsk_thread_flag(task, TIF_FORCED_TF);\n\telse if (test_tsk_thread_flag(task, TIF_FORCED_TF))\n\t\tvalue |= X86_EFLAGS_TF;\n\n\tregs->flags = (regs->flags & ~FLAG_MASK) | (value & FLAG_MASK);\n\n\treturn 0;\n}\n\nstatic int putreg(struct task_struct *child,\n\t\t  unsigned long offset, unsigned long value)\n{\n\tswitch (offset) {\n\tcase offsetof(struct user_regs_struct, cs):\n\tcase offsetof(struct user_regs_struct, ds):\n\tcase offsetof(struct user_regs_struct, es):\n\tcase offsetof(struct user_regs_struct, fs):\n\tcase offsetof(struct user_regs_struct, gs):\n\tcase offsetof(struct user_regs_struct, ss):\n\t\treturn set_segment_reg(child, offset, value);\n\n\tcase offsetof(struct user_regs_struct, flags):\n\t\treturn set_flags(child, value);\n\n#ifdef CONFIG_X86_64\n\tcase offsetof(struct user_regs_struct,fs_base):\n\t\tif (value >= TASK_SIZE_OF(child))\n\t\t\treturn -EIO;\n\t\t/*\n\t\t * When changing the segment base, use do_arch_prctl\n\t\t * to set either thread.fs or thread.fsindex and the\n\t\t * corresponding GDT slot.\n\t\t */\n\t\tif (child->thread.fs != value)\n\t\t\treturn do_arch_prctl(child, ARCH_SET_FS, value);\n\t\treturn 0;\n\tcase offsetof(struct user_regs_struct,gs_base):\n\t\t/*\n\t\t * Exactly the same here as the %fs handling above.\n\t\t */\n\t\tif (value >= TASK_SIZE_OF(child))\n\t\t\treturn -EIO;\n\t\tif (child->thread.gs != value)\n\t\t\treturn do_arch_prctl(child, ARCH_SET_GS, value);\n\t\treturn 0;\n#endif\n\t}\n\n\t*pt_regs_access(task_pt_regs(child), offset) = value;\n\treturn 0;\n}\n\nstatic unsigned long getreg(struct task_struct *task, unsigned long offset)\n{\n\tswitch (offset) {\n\tcase offsetof(struct user_regs_struct, cs):\n\tcase offsetof(struct user_regs_struct, ds):\n\tcase offsetof(struct user_regs_struct, es):\n\tcase offsetof(struct user_regs_struct, fs):\n\tcase offsetof(struct user_regs_struct, gs):\n\tcase offsetof(struct user_regs_struct, ss):\n\t\treturn get_segment_reg(task, offset);\n\n\tcase offsetof(struct user_regs_struct, flags):\n\t\treturn get_flags(task);\n\n#ifdef CONFIG_X86_64\n\tcase offsetof(struct user_regs_struct, fs_base): {\n\t\t/*\n\t\t * do_arch_prctl may have used a GDT slot instead of\n\t\t * the MSR.  To userland, it appears the same either\n\t\t * way, except the %fs segment selector might not be 0.\n\t\t */\n\t\tunsigned int seg = task->thread.fsindex;\n\t\tif (task->thread.fs != 0)\n\t\t\treturn task->thread.fs;\n\t\tif (task == current)\n\t\t\tasm(\"movl %%fs,%0\" : \"=r\" (seg));\n\t\tif (seg != FS_TLS_SEL)\n\t\t\treturn 0;\n\t\treturn get_desc_base(&task->thread.tls_array[FS_TLS]);\n\t}\n\tcase offsetof(struct user_regs_struct, gs_base): {\n\t\t/*\n\t\t * Exactly the same here as the %fs handling above.\n\t\t */\n\t\tunsigned int seg = task->thread.gsindex;\n\t\tif (task->thread.gs != 0)\n\t\t\treturn task->thread.gs;\n\t\tif (task == current)\n\t\t\tasm(\"movl %%gs,%0\" : \"=r\" (seg));\n\t\tif (seg != GS_TLS_SEL)\n\t\t\treturn 0;\n\t\treturn get_desc_base(&task->thread.tls_array[GS_TLS]);\n\t}\n#endif\n\t}\n\n\treturn *pt_regs_access(task_pt_regs(task), offset);\n}\n\nstatic int genregs_get(struct task_struct *target,\n\t\t       const struct user_regset *regset,\n\t\t       unsigned int pos, unsigned int count,\n\t\t       void *kbuf, void __user *ubuf)\n{\n\tif (kbuf) {\n\t\tunsigned long *k = kbuf;\n\t\twhile (count >= sizeof(*k)) {\n\t\t\t*k++ = getreg(target, pos);\n\t\t\tcount -= sizeof(*k);\n\t\t\tpos += sizeof(*k);\n\t\t}\n\t} else {\n\t\tunsigned long __user *u = ubuf;\n\t\twhile (count >= sizeof(*u)) {\n\t\t\tif (__put_user(getreg(target, pos), u++))\n\t\t\t\treturn -EFAULT;\n\t\t\tcount -= sizeof(*u);\n\t\t\tpos += sizeof(*u);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int genregs_set(struct task_struct *target,\n\t\t       const struct user_regset *regset,\n\t\t       unsigned int pos, unsigned int count,\n\t\t       const void *kbuf, const void __user *ubuf)\n{\n\tint ret = 0;\n\tif (kbuf) {\n\t\tconst unsigned long *k = kbuf;\n\t\twhile (count >= sizeof(*k) && !ret) {\n\t\t\tret = putreg(target, pos, *k++);\n\t\t\tcount -= sizeof(*k);\n\t\t\tpos += sizeof(*k);\n\t\t}\n\t} else {\n\t\tconst unsigned long  __user *u = ubuf;\n\t\twhile (count >= sizeof(*u) && !ret) {\n\t\t\tunsigned long word;\n\t\t\tret = __get_user(word, u++);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tret = putreg(target, pos, word);\n\t\t\tcount -= sizeof(*u);\n\t\t\tpos += sizeof(*u);\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic void ptrace_triggered(struct perf_event *bp, int nmi,\n\t\t\t     struct perf_sample_data *data,\n\t\t\t     struct pt_regs *regs)\n{\n\tint i;\n\tstruct thread_struct *thread = &(current->thread);\n\n\t/*\n\t * Store in the virtual DR6 register the fact that the breakpoint\n\t * was hit so the thread's debugger will see it.\n\t */\n\tfor (i = 0; i < HBP_NUM; i++) {\n\t\tif (thread->ptrace_bps[i] == bp)\n\t\t\tbreak;\n\t}\n\n\tthread->debugreg6 |= (DR_TRAP0 << i);\n}\n\n/*\n * Walk through every ptrace breakpoints for this thread and\n * build the dr7 value on top of their attributes.\n *\n */\nstatic unsigned long ptrace_get_dr7(struct perf_event *bp[])\n{\n\tint i;\n\tint dr7 = 0;\n\tstruct arch_hw_breakpoint *info;\n\n\tfor (i = 0; i < HBP_NUM; i++) {\n\t\tif (bp[i] && !bp[i]->attr.disabled) {\n\t\t\tinfo = counter_arch_bp(bp[i]);\n\t\t\tdr7 |= encode_dr7(i, info->len, info->type);\n\t\t}\n\t}\n\n\treturn dr7;\n}\n\nstatic int\nptrace_modify_breakpoint(struct perf_event *bp, int len, int type,\n\t\t\t struct task_struct *tsk, int disabled)\n{\n\tint err;\n\tint gen_len, gen_type;\n\tstruct perf_event_attr attr;\n\n\t/*\n\t * We should have at least an inactive breakpoint at this\n\t * slot. It means the user is writing dr7 without having\n\t * written the address register first\n\t */\n\tif (!bp)\n\t\treturn -EINVAL;\n\n\terr = arch_bp_generic_fields(len, type, &gen_len, &gen_type);\n\tif (err)\n\t\treturn err;\n\n\tattr = bp->attr;\n\tattr.bp_len = gen_len;\n\tattr.bp_type = gen_type;\n\tattr.disabled = disabled;\n\n\treturn modify_user_hw_breakpoint(bp, &attr);\n}\n\n/*\n * Handle ptrace writes to debug register 7.\n */\nstatic int ptrace_write_dr7(struct task_struct *tsk, unsigned long data)\n{\n\tstruct thread_struct *thread = &(tsk->thread);\n\tunsigned long old_dr7;\n\tint i, orig_ret = 0, rc = 0;\n\tint enabled, second_pass = 0;\n\tunsigned len, type;\n\tstruct perf_event *bp;\n\n\tif (ptrace_get_breakpoints(tsk) < 0)\n\t\treturn -ESRCH;\n\n\tdata &= ~DR_CONTROL_RESERVED;\n\told_dr7 = ptrace_get_dr7(thread->ptrace_bps);\nrestore:\n\t/*\n\t * Loop through all the hardware breakpoints, making the\n\t * appropriate changes to each.\n\t */\n\tfor (i = 0; i < HBP_NUM; i++) {\n\t\tenabled = decode_dr7(data, i, &len, &type);\n\t\tbp = thread->ptrace_bps[i];\n\n\t\tif (!enabled) {\n\t\t\tif (bp) {\n\t\t\t\t/*\n\t\t\t\t * Don't unregister the breakpoints right-away,\n\t\t\t\t * unless all register_user_hw_breakpoint()\n\t\t\t\t * requests have succeeded. This prevents\n\t\t\t\t * any window of opportunity for debug\n\t\t\t\t * register grabbing by other users.\n\t\t\t\t */\n\t\t\t\tif (!second_pass)\n\t\t\t\t\tcontinue;\n\n\t\t\t\trc = ptrace_modify_breakpoint(bp, len, type,\n\t\t\t\t\t\t\t      tsk, 1);\n\t\t\t\tif (rc)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\trc = ptrace_modify_breakpoint(bp, len, type, tsk, 0);\n\t\tif (rc)\n\t\t\tbreak;\n\t}\n\t/*\n\t * Make a second pass to free the remaining unused breakpoints\n\t * or to restore the original breakpoints if an error occurred.\n\t */\n\tif (!second_pass) {\n\t\tsecond_pass = 1;\n\t\tif (rc < 0) {\n\t\t\torig_ret = rc;\n\t\t\tdata = old_dr7;\n\t\t}\n\t\tgoto restore;\n\t}\n\n\tptrace_put_breakpoints(tsk);\n\n\treturn ((orig_ret < 0) ? orig_ret : rc);\n}\n\n/*\n * Handle PTRACE_PEEKUSR calls for the debug register area.\n */\nstatic unsigned long ptrace_get_debugreg(struct task_struct *tsk, int n)\n{\n\tstruct thread_struct *thread = &(tsk->thread);\n\tunsigned long val = 0;\n\n\tif (n < HBP_NUM) {\n\t\tstruct perf_event *bp;\n\n\t\tif (ptrace_get_breakpoints(tsk) < 0)\n\t\t\treturn -ESRCH;\n\n\t\tbp = thread->ptrace_bps[n];\n\t\tif (!bp)\n\t\t\tval = 0;\n\t\telse\n\t\t\tval = bp->hw.info.address;\n\n\t\tptrace_put_breakpoints(tsk);\n\t} else if (n == 6) {\n\t\tval = thread->debugreg6;\n\t } else if (n == 7) {\n\t\tval = thread->ptrace_dr7;\n\t}\n\treturn val;\n}\n\nstatic int ptrace_set_breakpoint_addr(struct task_struct *tsk, int nr,\n\t\t\t\t      unsigned long addr)\n{\n\tstruct perf_event *bp;\n\tstruct thread_struct *t = &tsk->thread;\n\tstruct perf_event_attr attr;\n\tint err = 0;\n\n\tif (ptrace_get_breakpoints(tsk) < 0)\n\t\treturn -ESRCH;\n\n\tif (!t->ptrace_bps[nr]) {\n\t\tptrace_breakpoint_init(&attr);\n\t\t/*\n\t\t * Put stub len and type to register (reserve) an inactive but\n\t\t * correct bp\n\t\t */\n\t\tattr.bp_addr = addr;\n\t\tattr.bp_len = HW_BREAKPOINT_LEN_1;\n\t\tattr.bp_type = HW_BREAKPOINT_W;\n\t\tattr.disabled = 1;\n\n\t\tbp = register_user_hw_breakpoint(&attr, ptrace_triggered, tsk);\n\n\t\t/*\n\t\t * CHECKME: the previous code returned -EIO if the addr wasn't\n\t\t * a valid task virtual addr. The new one will return -EINVAL in\n\t\t *  this case.\n\t\t * -EINVAL may be what we want for in-kernel breakpoints users,\n\t\t * but -EIO looks better for ptrace, since we refuse a register\n\t\t * writing for the user. And anyway this is the previous\n\t\t * behaviour.\n\t\t */\n\t\tif (IS_ERR(bp)) {\n\t\t\terr = PTR_ERR(bp);\n\t\t\tgoto put;\n\t\t}\n\n\t\tt->ptrace_bps[nr] = bp;\n\t} else {\n\t\tbp = t->ptrace_bps[nr];\n\n\t\tattr = bp->attr;\n\t\tattr.bp_addr = addr;\n\t\terr = modify_user_hw_breakpoint(bp, &attr);\n\t}\n\nput:\n\tptrace_put_breakpoints(tsk);\n\treturn err;\n}\n\n/*\n * Handle PTRACE_POKEUSR calls for the debug register area.\n */\nint ptrace_set_debugreg(struct task_struct *tsk, int n, unsigned long val)\n{\n\tstruct thread_struct *thread = &(tsk->thread);\n\tint rc = 0;\n\n\t/* There are no DR4 or DR5 registers */\n\tif (n == 4 || n == 5)\n\t\treturn -EIO;\n\n\tif (n == 6) {\n\t\tthread->debugreg6 = val;\n\t\tgoto ret_path;\n\t}\n\tif (n < HBP_NUM) {\n\t\trc = ptrace_set_breakpoint_addr(tsk, n, val);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\t/* All that's left is DR7 */\n\tif (n == 7) {\n\t\trc = ptrace_write_dr7(tsk, val);\n\t\tif (!rc)\n\t\t\tthread->ptrace_dr7 = val;\n\t}\n\nret_path:\n\treturn rc;\n}\n\n/*\n * These access the current or another (stopped) task's io permission\n * bitmap for debugging or core dump.\n */\nstatic int ioperm_active(struct task_struct *target,\n\t\t\t const struct user_regset *regset)\n{\n\treturn target->thread.io_bitmap_max / regset->size;\n}\n\nstatic int ioperm_get(struct task_struct *target,\n\t\t      const struct user_regset *regset,\n\t\t      unsigned int pos, unsigned int count,\n\t\t      void *kbuf, void __user *ubuf)\n{\n\tif (!target->thread.io_bitmap_ptr)\n\t\treturn -ENXIO;\n\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t   target->thread.io_bitmap_ptr,\n\t\t\t\t   0, IO_BITMAP_BYTES);\n}\n\n/*\n * Called by kernel/ptrace.c when detaching..\n *\n * Make sure the single step bit is not set.\n */\nvoid ptrace_disable(struct task_struct *child)\n{\n\tuser_disable_single_step(child);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(child, TIF_SYSCALL_EMU);\n#endif\n}\n\n#if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION\nstatic const struct user_regset_view user_x86_32_view; /* Initialized below. */\n#endif\n\nlong arch_ptrace(struct task_struct *child, long request,\n\t\t unsigned long addr, unsigned long data)\n{\n\tint ret;\n\tunsigned long __user *datap = (unsigned long __user *)data;\n\n\tswitch (request) {\n\t/* read the word at location addr in the USER area. */\n\tcase PTRACE_PEEKUSR: {\n\t\tunsigned long tmp;\n\n\t\tret = -EIO;\n\t\tif ((addr & (sizeof(data) - 1)) || addr >= sizeof(struct user))\n\t\t\tbreak;\n\n\t\ttmp = 0;  /* Default return condition */\n\t\tif (addr < sizeof(struct user_regs_struct))\n\t\t\ttmp = getreg(child, addr);\n\t\telse if (addr >= offsetof(struct user, u_debugreg[0]) &&\n\t\t\t addr <= offsetof(struct user, u_debugreg[7])) {\n\t\t\taddr -= offsetof(struct user, u_debugreg[0]);\n\t\t\ttmp = ptrace_get_debugreg(child, addr / sizeof(data));\n\t\t}\n\t\tret = put_user(tmp, datap);\n\t\tbreak;\n\t}\n\n\tcase PTRACE_POKEUSR: /* write the word at location addr in the USER area */\n\t\tret = -EIO;\n\t\tif ((addr & (sizeof(data) - 1)) || addr >= sizeof(struct user))\n\t\t\tbreak;\n\n\t\tif (addr < sizeof(struct user_regs_struct))\n\t\t\tret = putreg(child, addr, data);\n\t\telse if (addr >= offsetof(struct user, u_debugreg[0]) &&\n\t\t\t addr <= offsetof(struct user, u_debugreg[7])) {\n\t\t\taddr -= offsetof(struct user, u_debugreg[0]);\n\t\t\tret = ptrace_set_debugreg(child,\n\t\t\t\t\t\t  addr / sizeof(data), data);\n\t\t}\n\t\tbreak;\n\n\tcase PTRACE_GETREGS:\t/* Get all gp regs from the child. */\n\t\treturn copy_regset_to_user(child,\n\t\t\t\t\t   task_user_regset_view(current),\n\t\t\t\t\t   REGSET_GENERAL,\n\t\t\t\t\t   0, sizeof(struct user_regs_struct),\n\t\t\t\t\t   datap);\n\n\tcase PTRACE_SETREGS:\t/* Set all gp regs in the child. */\n\t\treturn copy_regset_from_user(child,\n\t\t\t\t\t     task_user_regset_view(current),\n\t\t\t\t\t     REGSET_GENERAL,\n\t\t\t\t\t     0, sizeof(struct user_regs_struct),\n\t\t\t\t\t     datap);\n\n\tcase PTRACE_GETFPREGS:\t/* Get the child FPU state. */\n\t\treturn copy_regset_to_user(child,\n\t\t\t\t\t   task_user_regset_view(current),\n\t\t\t\t\t   REGSET_FP,\n\t\t\t\t\t   0, sizeof(struct user_i387_struct),\n\t\t\t\t\t   datap);\n\n\tcase PTRACE_SETFPREGS:\t/* Set the child FPU state. */\n\t\treturn copy_regset_from_user(child,\n\t\t\t\t\t     task_user_regset_view(current),\n\t\t\t\t\t     REGSET_FP,\n\t\t\t\t\t     0, sizeof(struct user_i387_struct),\n\t\t\t\t\t     datap);\n\n#ifdef CONFIG_X86_32\n\tcase PTRACE_GETFPXREGS:\t/* Get the child extended FPU state. */\n\t\treturn copy_regset_to_user(child, &user_x86_32_view,\n\t\t\t\t\t   REGSET_XFP,\n\t\t\t\t\t   0, sizeof(struct user_fxsr_struct),\n\t\t\t\t\t   datap) ? -EIO : 0;\n\n\tcase PTRACE_SETFPXREGS:\t/* Set the child extended FPU state. */\n\t\treturn copy_regset_from_user(child, &user_x86_32_view,\n\t\t\t\t\t     REGSET_XFP,\n\t\t\t\t\t     0, sizeof(struct user_fxsr_struct),\n\t\t\t\t\t     datap) ? -EIO : 0;\n#endif\n\n#if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION\n\tcase PTRACE_GET_THREAD_AREA:\n\t\tif ((int) addr < 0)\n\t\t\treturn -EIO;\n\t\tret = do_get_thread_area(child, addr,\n\t\t\t\t\t(struct user_desc __user *)data);\n\t\tbreak;\n\n\tcase PTRACE_SET_THREAD_AREA:\n\t\tif ((int) addr < 0)\n\t\t\treturn -EIO;\n\t\tret = do_set_thread_area(child, addr,\n\t\t\t\t\t(struct user_desc __user *)data, 0);\n\t\tbreak;\n#endif\n\n#ifdef CONFIG_X86_64\n\t\t/* normal 64bit interface to access TLS data.\n\t\t   Works just like arch_prctl, except that the arguments\n\t\t   are reversed. */\n\tcase PTRACE_ARCH_PRCTL:\n\t\tret = do_arch_prctl(child, data, addr);\n\t\tbreak;\n#endif\n\n\tdefault:\n\t\tret = ptrace_request(child, request, addr, data);\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n#ifdef CONFIG_IA32_EMULATION\n\n#include <linux/compat.h>\n#include <linux/syscalls.h>\n#include <asm/ia32.h>\n#include <asm/user32.h>\n\n#define R32(l,q)\t\t\t\t\t\t\t\\\n\tcase offsetof(struct user32, regs.l):\t\t\t\t\\\n\t\tregs->q = value; break\n\n#define SEG32(rs)\t\t\t\t\t\t\t\\\n\tcase offsetof(struct user32, regs.rs):\t\t\t\t\\\n\t\treturn set_segment_reg(child,\t\t\t\t\\\n\t\t\t\t       offsetof(struct user_regs_struct, rs), \\\n\t\t\t\t       value);\t\t\t\t\\\n\t\tbreak\n\nstatic int putreg32(struct task_struct *child, unsigned regno, u32 value)\n{\n\tstruct pt_regs *regs = task_pt_regs(child);\n\n\tswitch (regno) {\n\n\tSEG32(cs);\n\tSEG32(ds);\n\tSEG32(es);\n\tSEG32(fs);\n\tSEG32(gs);\n\tSEG32(ss);\n\n\tR32(ebx, bx);\n\tR32(ecx, cx);\n\tR32(edx, dx);\n\tR32(edi, di);\n\tR32(esi, si);\n\tR32(ebp, bp);\n\tR32(eax, ax);\n\tR32(eip, ip);\n\tR32(esp, sp);\n\n\tcase offsetof(struct user32, regs.orig_eax):\n\t\t/*\n\t\t * A 32-bit debugger setting orig_eax means to restore\n\t\t * the state of the task restarting a 32-bit syscall.\n\t\t * Make sure we interpret the -ERESTART* codes correctly\n\t\t * in case the task is not actually still sitting at the\n\t\t * exit from a 32-bit syscall with TS_COMPAT still set.\n\t\t */\n\t\tregs->orig_ax = value;\n\t\tif (syscall_get_nr(child, regs) >= 0)\n\t\t\ttask_thread_info(child)->status |= TS_COMPAT;\n\t\tbreak;\n\n\tcase offsetof(struct user32, regs.eflags):\n\t\treturn set_flags(child, value);\n\n\tcase offsetof(struct user32, u_debugreg[0]) ...\n\t\toffsetof(struct user32, u_debugreg[7]):\n\t\tregno -= offsetof(struct user32, u_debugreg[0]);\n\t\treturn ptrace_set_debugreg(child, regno / 4, value);\n\n\tdefault:\n\t\tif (regno > sizeof(struct user32) || (regno & 3))\n\t\t\treturn -EIO;\n\n\t\t/*\n\t\t * Other dummy fields in the virtual user structure\n\t\t * are ignored\n\t\t */\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\n#undef R32\n#undef SEG32\n\n#define R32(l,q)\t\t\t\t\t\t\t\\\n\tcase offsetof(struct user32, regs.l):\t\t\t\t\\\n\t\t*val = regs->q; break\n\n#define SEG32(rs)\t\t\t\t\t\t\t\\\n\tcase offsetof(struct user32, regs.rs):\t\t\t\t\\\n\t\t*val = get_segment_reg(child,\t\t\t\t\\\n\t\t\t\t       offsetof(struct user_regs_struct, rs)); \\\n\t\tbreak\n\nstatic int getreg32(struct task_struct *child, unsigned regno, u32 *val)\n{\n\tstruct pt_regs *regs = task_pt_regs(child);\n\n\tswitch (regno) {\n\n\tSEG32(ds);\n\tSEG32(es);\n\tSEG32(fs);\n\tSEG32(gs);\n\n\tR32(cs, cs);\n\tR32(ss, ss);\n\tR32(ebx, bx);\n\tR32(ecx, cx);\n\tR32(edx, dx);\n\tR32(edi, di);\n\tR32(esi, si);\n\tR32(ebp, bp);\n\tR32(eax, ax);\n\tR32(orig_eax, orig_ax);\n\tR32(eip, ip);\n\tR32(esp, sp);\n\n\tcase offsetof(struct user32, regs.eflags):\n\t\t*val = get_flags(child);\n\t\tbreak;\n\n\tcase offsetof(struct user32, u_debugreg[0]) ...\n\t\toffsetof(struct user32, u_debugreg[7]):\n\t\tregno -= offsetof(struct user32, u_debugreg[0]);\n\t\t*val = ptrace_get_debugreg(child, regno / 4);\n\t\tbreak;\n\n\tdefault:\n\t\tif (regno > sizeof(struct user32) || (regno & 3))\n\t\t\treturn -EIO;\n\n\t\t/*\n\t\t * Other dummy fields in the virtual user structure\n\t\t * are ignored\n\t\t */\n\t\t*val = 0;\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\n#undef R32\n#undef SEG32\n\nstatic int genregs32_get(struct task_struct *target,\n\t\t\t const struct user_regset *regset,\n\t\t\t unsigned int pos, unsigned int count,\n\t\t\t void *kbuf, void __user *ubuf)\n{\n\tif (kbuf) {\n\t\tcompat_ulong_t *k = kbuf;\n\t\twhile (count >= sizeof(*k)) {\n\t\t\tgetreg32(target, pos, k++);\n\t\t\tcount -= sizeof(*k);\n\t\t\tpos += sizeof(*k);\n\t\t}\n\t} else {\n\t\tcompat_ulong_t __user *u = ubuf;\n\t\twhile (count >= sizeof(*u)) {\n\t\t\tcompat_ulong_t word;\n\t\t\tgetreg32(target, pos, &word);\n\t\t\tif (__put_user(word, u++))\n\t\t\t\treturn -EFAULT;\n\t\t\tcount -= sizeof(*u);\n\t\t\tpos += sizeof(*u);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int genregs32_set(struct task_struct *target,\n\t\t\t const struct user_regset *regset,\n\t\t\t unsigned int pos, unsigned int count,\n\t\t\t const void *kbuf, const void __user *ubuf)\n{\n\tint ret = 0;\n\tif (kbuf) {\n\t\tconst compat_ulong_t *k = kbuf;\n\t\twhile (count >= sizeof(*k) && !ret) {\n\t\t\tret = putreg32(target, pos, *k++);\n\t\t\tcount -= sizeof(*k);\n\t\t\tpos += sizeof(*k);\n\t\t}\n\t} else {\n\t\tconst compat_ulong_t __user *u = ubuf;\n\t\twhile (count >= sizeof(*u) && !ret) {\n\t\t\tcompat_ulong_t word;\n\t\t\tret = __get_user(word, u++);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tret = putreg32(target, pos, word);\n\t\t\tcount -= sizeof(*u);\n\t\t\tpos += sizeof(*u);\n\t\t}\n\t}\n\treturn ret;\n}\n\nlong compat_arch_ptrace(struct task_struct *child, compat_long_t request,\n\t\t\tcompat_ulong_t caddr, compat_ulong_t cdata)\n{\n\tunsigned long addr = caddr;\n\tunsigned long data = cdata;\n\tvoid __user *datap = compat_ptr(data);\n\tint ret;\n\t__u32 val;\n\n\tswitch (request) {\n\tcase PTRACE_PEEKUSR:\n\t\tret = getreg32(child, addr, &val);\n\t\tif (ret == 0)\n\t\t\tret = put_user(val, (__u32 __user *)datap);\n\t\tbreak;\n\n\tcase PTRACE_POKEUSR:\n\t\tret = putreg32(child, addr, data);\n\t\tbreak;\n\n\tcase PTRACE_GETREGS:\t/* Get all gp regs from the child. */\n\t\treturn copy_regset_to_user(child, &user_x86_32_view,\n\t\t\t\t\t   REGSET_GENERAL,\n\t\t\t\t\t   0, sizeof(struct user_regs_struct32),\n\t\t\t\t\t   datap);\n\n\tcase PTRACE_SETREGS:\t/* Set all gp regs in the child. */\n\t\treturn copy_regset_from_user(child, &user_x86_32_view,\n\t\t\t\t\t     REGSET_GENERAL, 0,\n\t\t\t\t\t     sizeof(struct user_regs_struct32),\n\t\t\t\t\t     datap);\n\n\tcase PTRACE_GETFPREGS:\t/* Get the child FPU state. */\n\t\treturn copy_regset_to_user(child, &user_x86_32_view,\n\t\t\t\t\t   REGSET_FP, 0,\n\t\t\t\t\t   sizeof(struct user_i387_ia32_struct),\n\t\t\t\t\t   datap);\n\n\tcase PTRACE_SETFPREGS:\t/* Set the child FPU state. */\n\t\treturn copy_regset_from_user(\n\t\t\tchild, &user_x86_32_view, REGSET_FP,\n\t\t\t0, sizeof(struct user_i387_ia32_struct), datap);\n\n\tcase PTRACE_GETFPXREGS:\t/* Get the child extended FPU state. */\n\t\treturn copy_regset_to_user(child, &user_x86_32_view,\n\t\t\t\t\t   REGSET_XFP, 0,\n\t\t\t\t\t   sizeof(struct user32_fxsr_struct),\n\t\t\t\t\t   datap);\n\n\tcase PTRACE_SETFPXREGS:\t/* Set the child extended FPU state. */\n\t\treturn copy_regset_from_user(child, &user_x86_32_view,\n\t\t\t\t\t     REGSET_XFP, 0,\n\t\t\t\t\t     sizeof(struct user32_fxsr_struct),\n\t\t\t\t\t     datap);\n\n\tcase PTRACE_GET_THREAD_AREA:\n\tcase PTRACE_SET_THREAD_AREA:\n\t\treturn arch_ptrace(child, request, addr, data);\n\n\tdefault:\n\t\treturn compat_ptrace_request(child, request, addr, data);\n\t}\n\n\treturn ret;\n}\n\n#endif\t/* CONFIG_IA32_EMULATION */\n\n#ifdef CONFIG_X86_64\n\nstatic struct user_regset x86_64_regsets[] __read_mostly = {\n\t[REGSET_GENERAL] = {\n\t\t.core_note_type = NT_PRSTATUS,\n\t\t.n = sizeof(struct user_regs_struct) / sizeof(long),\n\t\t.size = sizeof(long), .align = sizeof(long),\n\t\t.get = genregs_get, .set = genregs_set\n\t},\n\t[REGSET_FP] = {\n\t\t.core_note_type = NT_PRFPREG,\n\t\t.n = sizeof(struct user_i387_struct) / sizeof(long),\n\t\t.size = sizeof(long), .align = sizeof(long),\n\t\t.active = xfpregs_active, .get = xfpregs_get, .set = xfpregs_set\n\t},\n\t[REGSET_XSTATE] = {\n\t\t.core_note_type = NT_X86_XSTATE,\n\t\t.size = sizeof(u64), .align = sizeof(u64),\n\t\t.active = xstateregs_active, .get = xstateregs_get,\n\t\t.set = xstateregs_set\n\t},\n\t[REGSET_IOPERM64] = {\n\t\t.core_note_type = NT_386_IOPERM,\n\t\t.n = IO_BITMAP_LONGS,\n\t\t.size = sizeof(long), .align = sizeof(long),\n\t\t.active = ioperm_active, .get = ioperm_get\n\t},\n};\n\nstatic const struct user_regset_view user_x86_64_view = {\n\t.name = \"x86_64\", .e_machine = EM_X86_64,\n\t.regsets = x86_64_regsets, .n = ARRAY_SIZE(x86_64_regsets)\n};\n\n#else  /* CONFIG_X86_32 */\n\n#define user_regs_struct32\tuser_regs_struct\n#define genregs32_get\t\tgenregs_get\n#define genregs32_set\t\tgenregs_set\n\n#define user_i387_ia32_struct\tuser_i387_struct\n#define user32_fxsr_struct\tuser_fxsr_struct\n\n#endif\t/* CONFIG_X86_64 */\n\n#if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION\nstatic struct user_regset x86_32_regsets[] __read_mostly = {\n\t[REGSET_GENERAL] = {\n\t\t.core_note_type = NT_PRSTATUS,\n\t\t.n = sizeof(struct user_regs_struct32) / sizeof(u32),\n\t\t.size = sizeof(u32), .align = sizeof(u32),\n\t\t.get = genregs32_get, .set = genregs32_set\n\t},\n\t[REGSET_FP] = {\n\t\t.core_note_type = NT_PRFPREG,\n\t\t.n = sizeof(struct user_i387_ia32_struct) / sizeof(u32),\n\t\t.size = sizeof(u32), .align = sizeof(u32),\n\t\t.active = fpregs_active, .get = fpregs_get, .set = fpregs_set\n\t},\n\t[REGSET_XFP] = {\n\t\t.core_note_type = NT_PRXFPREG,\n\t\t.n = sizeof(struct user32_fxsr_struct) / sizeof(u32),\n\t\t.size = sizeof(u32), .align = sizeof(u32),\n\t\t.active = xfpregs_active, .get = xfpregs_get, .set = xfpregs_set\n\t},\n\t[REGSET_XSTATE] = {\n\t\t.core_note_type = NT_X86_XSTATE,\n\t\t.size = sizeof(u64), .align = sizeof(u64),\n\t\t.active = xstateregs_active, .get = xstateregs_get,\n\t\t.set = xstateregs_set\n\t},\n\t[REGSET_TLS] = {\n\t\t.core_note_type = NT_386_TLS,\n\t\t.n = GDT_ENTRY_TLS_ENTRIES, .bias = GDT_ENTRY_TLS_MIN,\n\t\t.size = sizeof(struct user_desc),\n\t\t.align = sizeof(struct user_desc),\n\t\t.active = regset_tls_active,\n\t\t.get = regset_tls_get, .set = regset_tls_set\n\t},\n\t[REGSET_IOPERM32] = {\n\t\t.core_note_type = NT_386_IOPERM,\n\t\t.n = IO_BITMAP_BYTES / sizeof(u32),\n\t\t.size = sizeof(u32), .align = sizeof(u32),\n\t\t.active = ioperm_active, .get = ioperm_get\n\t},\n};\n\nstatic const struct user_regset_view user_x86_32_view = {\n\t.name = \"i386\", .e_machine = EM_386,\n\t.regsets = x86_32_regsets, .n = ARRAY_SIZE(x86_32_regsets)\n};\n#endif\n\n/*\n * This represents bytes 464..511 in the memory layout exported through\n * the REGSET_XSTATE interface.\n */\nu64 xstate_fx_sw_bytes[USER_XSTATE_FX_SW_WORDS];\n\nvoid update_regset_xstate_info(unsigned int size, u64 xstate_mask)\n{\n#ifdef CONFIG_X86_64\n\tx86_64_regsets[REGSET_XSTATE].n = size / sizeof(u64);\n#endif\n#if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION\n\tx86_32_regsets[REGSET_XSTATE].n = size / sizeof(u64);\n#endif\n\txstate_fx_sw_bytes[USER_XSTATE_XCR0_WORD] = xstate_mask;\n}\n\nconst struct user_regset_view *task_user_regset_view(struct task_struct *task)\n{\n#ifdef CONFIG_IA32_EMULATION\n\tif (test_tsk_thread_flag(task, TIF_IA32))\n#endif\n#if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION\n\t\treturn &user_x86_32_view;\n#endif\n#ifdef CONFIG_X86_64\n\treturn &user_x86_64_view;\n#endif\n}\n\nstatic void fill_sigtrap_info(struct task_struct *tsk,\n\t\t\t\tstruct pt_regs *regs,\n\t\t\t\tint error_code, int si_code,\n\t\t\t\tstruct siginfo *info)\n{\n\ttsk->thread.trap_no = 1;\n\ttsk->thread.error_code = error_code;\n\n\tmemset(info, 0, sizeof(*info));\n\tinfo->si_signo = SIGTRAP;\n\tinfo->si_code = si_code;\n\tinfo->si_addr = user_mode_vm(regs) ? (void __user *)regs->ip : NULL;\n}\n\nvoid user_single_step_siginfo(struct task_struct *tsk,\n\t\t\t\tstruct pt_regs *regs,\n\t\t\t\tstruct siginfo *info)\n{\n\tfill_sigtrap_info(tsk, regs, 0, TRAP_BRKPT, info);\n}\n\nvoid send_sigtrap(struct task_struct *tsk, struct pt_regs *regs,\n\t\t\t\t\t int error_code, int si_code)\n{\n\tstruct siginfo info;\n\n\tfill_sigtrap_info(tsk, regs, error_code, si_code, &info);\n\t/* Send us the fake SIGTRAP */\n\tforce_sig_info(SIGTRAP, &info, tsk);\n}\n\n\n#ifdef CONFIG_X86_32\n# define IS_IA32\t1\n#elif defined CONFIG_IA32_EMULATION\n# define IS_IA32\tis_compat_task()\n#else\n# define IS_IA32\t0\n#endif\n\n/*\n * We must return the syscall number to actually look up in the table.\n * This can be -1L to skip running any syscall at all.\n */\nlong syscall_trace_enter(struct pt_regs *regs)\n{\n\tlong ret = 0;\n\n\t/*\n\t * If we stepped into a sysenter/syscall insn, it trapped in\n\t * kernel mode; do_debug() cleared TF and set TIF_SINGLESTEP.\n\t * If user-mode had set TF itself, then it's still clear from\n\t * do_debug() and we need to set it again to restore the user\n\t * state.  If we entered on the slow path, TF was already set.\n\t */\n\tif (test_thread_flag(TIF_SINGLESTEP))\n\t\tregs->flags |= X86_EFLAGS_TF;\n\n\t/* do the secure computing check first */\n\tsecure_computing(regs->orig_ax);\n\n\tif (unlikely(test_thread_flag(TIF_SYSCALL_EMU)))\n\t\tret = -1L;\n\n\tif ((ret || test_thread_flag(TIF_SYSCALL_TRACE)) &&\n\t    tracehook_report_syscall_entry(regs))\n\t\tret = -1L;\n\n\tif (unlikely(test_thread_flag(TIF_SYSCALL_TRACEPOINT)))\n\t\ttrace_sys_enter(regs, regs->orig_ax);\n\n\tif (unlikely(current->audit_context)) {\n\t\tif (IS_IA32)\n\t\t\taudit_syscall_entry(AUDIT_ARCH_I386,\n\t\t\t\t\t    regs->orig_ax,\n\t\t\t\t\t    regs->bx, regs->cx,\n\t\t\t\t\t    regs->dx, regs->si);\n#ifdef CONFIG_X86_64\n\t\telse\n\t\t\taudit_syscall_entry(AUDIT_ARCH_X86_64,\n\t\t\t\t\t    regs->orig_ax,\n\t\t\t\t\t    regs->di, regs->si,\n\t\t\t\t\t    regs->dx, regs->r10);\n#endif\n\t}\n\n\treturn ret ?: regs->orig_ax;\n}\n\nvoid syscall_trace_leave(struct pt_regs *regs)\n{\n\tbool step;\n\n\tif (unlikely(current->audit_context))\n\t\taudit_syscall_exit(AUDITSC_RESULT(regs->ax), regs->ax);\n\n\tif (unlikely(test_thread_flag(TIF_SYSCALL_TRACEPOINT)))\n\t\ttrace_sys_exit(regs, regs->ax);\n\n\t/*\n\t * If TIF_SYSCALL_EMU is set, we only get here because of\n\t * TIF_SINGLESTEP (i.e. this is PTRACE_SYSEMU_SINGLESTEP).\n\t * We already reported this syscall instruction in\n\t * syscall_trace_enter().\n\t */\n\tstep = unlikely(test_thread_flag(TIF_SINGLESTEP)) &&\n\t\t\t!test_thread_flag(TIF_SYSCALL_EMU);\n\tif (step || test_thread_flag(TIF_SYSCALL_TRACE))\n\t\ttracehook_report_syscall_exit(regs, step);\n}\n", "/*\n *  Copyright (C) 1995  Linus Torvalds\n *  Copyright (C) 2001, 2002 Andi Kleen, SuSE Labs.\n *  Copyright (C) 2008-2009, Red Hat Inc., Ingo Molnar\n */\n#include <linux/magic.h>\t\t/* STACK_END_MAGIC\t\t*/\n#include <linux/sched.h>\t\t/* test_thread_flag(), ...\t*/\n#include <linux/kdebug.h>\t\t/* oops_begin/end, ...\t\t*/\n#include <linux/module.h>\t\t/* search_exception_table\t*/\n#include <linux/bootmem.h>\t\t/* max_low_pfn\t\t\t*/\n#include <linux/kprobes.h>\t\t/* __kprobes, ...\t\t*/\n#include <linux/mmiotrace.h>\t\t/* kmmio_handler, ...\t\t*/\n#include <linux/perf_event.h>\t\t/* perf_sw_event\t\t*/\n#include <linux/hugetlb.h>\t\t/* hstate_index_to_shift\t*/\n#include <linux/prefetch.h>\t\t/* prefetchw\t\t\t*/\n\n#include <asm/traps.h>\t\t\t/* dotraplinkage, ...\t\t*/\n#include <asm/pgalloc.h>\t\t/* pgd_*(), ...\t\t\t*/\n#include <asm/kmemcheck.h>\t\t/* kmemcheck_*(), ...\t\t*/\n\n/*\n * Page fault error code bits:\n *\n *   bit 0 ==\t 0: no page found\t1: protection fault\n *   bit 1 ==\t 0: read access\t\t1: write access\n *   bit 2 ==\t 0: kernel-mode access\t1: user-mode access\n *   bit 3 ==\t\t\t\t1: use of reserved bit detected\n *   bit 4 ==\t\t\t\t1: fault was an instruction fetch\n */\nenum x86_pf_error_code {\n\n\tPF_PROT\t\t=\t\t1 << 0,\n\tPF_WRITE\t=\t\t1 << 1,\n\tPF_USER\t\t=\t\t1 << 2,\n\tPF_RSVD\t\t=\t\t1 << 3,\n\tPF_INSTR\t=\t\t1 << 4,\n};\n\n/*\n * Returns 0 if mmiotrace is disabled, or if the fault is not\n * handled by mmiotrace:\n */\nstatic inline int __kprobes\nkmmio_fault(struct pt_regs *regs, unsigned long addr)\n{\n\tif (unlikely(is_kmmio_active()))\n\t\tif (kmmio_handler(regs, addr) == 1)\n\t\t\treturn -1;\n\treturn 0;\n}\n\nstatic inline int __kprobes notify_page_fault(struct pt_regs *regs)\n{\n\tint ret = 0;\n\n\t/* kprobe_running() needs smp_processor_id() */\n\tif (kprobes_built_in() && !user_mode_vm(regs)) {\n\t\tpreempt_disable();\n\t\tif (kprobe_running() && kprobe_fault_handler(regs, 14))\n\t\t\tret = 1;\n\t\tpreempt_enable();\n\t}\n\n\treturn ret;\n}\n\n/*\n * Prefetch quirks:\n *\n * 32-bit mode:\n *\n *   Sometimes AMD Athlon/Opteron CPUs report invalid exceptions on prefetch.\n *   Check that here and ignore it.\n *\n * 64-bit mode:\n *\n *   Sometimes the CPU reports invalid exceptions on prefetch.\n *   Check that here and ignore it.\n *\n * Opcode checker based on code by Richard Brunner.\n */\nstatic inline int\ncheck_prefetch_opcode(struct pt_regs *regs, unsigned char *instr,\n\t\t      unsigned char opcode, int *prefetch)\n{\n\tunsigned char instr_hi = opcode & 0xf0;\n\tunsigned char instr_lo = opcode & 0x0f;\n\n\tswitch (instr_hi) {\n\tcase 0x20:\n\tcase 0x30:\n\t\t/*\n\t\t * Values 0x26,0x2E,0x36,0x3E are valid x86 prefixes.\n\t\t * In X86_64 long mode, the CPU will signal invalid\n\t\t * opcode if some of these prefixes are present so\n\t\t * X86_64 will never get here anyway\n\t\t */\n\t\treturn ((instr_lo & 7) == 0x6);\n#ifdef CONFIG_X86_64\n\tcase 0x40:\n\t\t/*\n\t\t * In AMD64 long mode 0x40..0x4F are valid REX prefixes\n\t\t * Need to figure out under what instruction mode the\n\t\t * instruction was issued. Could check the LDT for lm,\n\t\t * but for now it's good enough to assume that long\n\t\t * mode only uses well known segments or kernel.\n\t\t */\n\t\treturn (!user_mode(regs)) || (regs->cs == __USER_CS);\n#endif\n\tcase 0x60:\n\t\t/* 0x64 thru 0x67 are valid prefixes in all modes. */\n\t\treturn (instr_lo & 0xC) == 0x4;\n\tcase 0xF0:\n\t\t/* 0xF0, 0xF2, 0xF3 are valid prefixes in all modes. */\n\t\treturn !instr_lo || (instr_lo>>1) == 1;\n\tcase 0x00:\n\t\t/* Prefetch instruction is 0x0F0D or 0x0F18 */\n\t\tif (probe_kernel_address(instr, opcode))\n\t\t\treturn 0;\n\n\t\t*prefetch = (instr_lo == 0xF) &&\n\t\t\t(opcode == 0x0D || opcode == 0x18);\n\t\treturn 0;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic int\nis_prefetch(struct pt_regs *regs, unsigned long error_code, unsigned long addr)\n{\n\tunsigned char *max_instr;\n\tunsigned char *instr;\n\tint prefetch = 0;\n\n\t/*\n\t * If it was a exec (instruction fetch) fault on NX page, then\n\t * do not ignore the fault:\n\t */\n\tif (error_code & PF_INSTR)\n\t\treturn 0;\n\n\tinstr = (void *)convert_ip_to_linear(current, regs);\n\tmax_instr = instr + 15;\n\n\tif (user_mode(regs) && instr >= (unsigned char *)TASK_SIZE)\n\t\treturn 0;\n\n\twhile (instr < max_instr) {\n\t\tunsigned char opcode;\n\n\t\tif (probe_kernel_address(instr, opcode))\n\t\t\tbreak;\n\n\t\tinstr++;\n\n\t\tif (!check_prefetch_opcode(regs, instr, opcode, &prefetch))\n\t\t\tbreak;\n\t}\n\treturn prefetch;\n}\n\nstatic void\nforce_sig_info_fault(int si_signo, int si_code, unsigned long address,\n\t\t     struct task_struct *tsk, int fault)\n{\n\tunsigned lsb = 0;\n\tsiginfo_t info;\n\n\tinfo.si_signo\t= si_signo;\n\tinfo.si_errno\t= 0;\n\tinfo.si_code\t= si_code;\n\tinfo.si_addr\t= (void __user *)address;\n\tif (fault & VM_FAULT_HWPOISON_LARGE)\n\t\tlsb = hstate_index_to_shift(VM_FAULT_GET_HINDEX(fault)); \n\tif (fault & VM_FAULT_HWPOISON)\n\t\tlsb = PAGE_SHIFT;\n\tinfo.si_addr_lsb = lsb;\n\n\tforce_sig_info(si_signo, &info, tsk);\n}\n\nDEFINE_SPINLOCK(pgd_lock);\nLIST_HEAD(pgd_list);\n\n#ifdef CONFIG_X86_32\nstatic inline pmd_t *vmalloc_sync_one(pgd_t *pgd, unsigned long address)\n{\n\tunsigned index = pgd_index(address);\n\tpgd_t *pgd_k;\n\tpud_t *pud, *pud_k;\n\tpmd_t *pmd, *pmd_k;\n\n\tpgd += index;\n\tpgd_k = init_mm.pgd + index;\n\n\tif (!pgd_present(*pgd_k))\n\t\treturn NULL;\n\n\t/*\n\t * set_pgd(pgd, *pgd_k); here would be useless on PAE\n\t * and redundant with the set_pmd() on non-PAE. As would\n\t * set_pud.\n\t */\n\tpud = pud_offset(pgd, address);\n\tpud_k = pud_offset(pgd_k, address);\n\tif (!pud_present(*pud_k))\n\t\treturn NULL;\n\n\tpmd = pmd_offset(pud, address);\n\tpmd_k = pmd_offset(pud_k, address);\n\tif (!pmd_present(*pmd_k))\n\t\treturn NULL;\n\n\tif (!pmd_present(*pmd))\n\t\tset_pmd(pmd, *pmd_k);\n\telse\n\t\tBUG_ON(pmd_page(*pmd) != pmd_page(*pmd_k));\n\n\treturn pmd_k;\n}\n\nvoid vmalloc_sync_all(void)\n{\n\tunsigned long address;\n\n\tif (SHARED_KERNEL_PMD)\n\t\treturn;\n\n\tfor (address = VMALLOC_START & PMD_MASK;\n\t     address >= TASK_SIZE && address < FIXADDR_TOP;\n\t     address += PMD_SIZE) {\n\t\tstruct page *page;\n\n\t\tspin_lock(&pgd_lock);\n\t\tlist_for_each_entry(page, &pgd_list, lru) {\n\t\t\tspinlock_t *pgt_lock;\n\t\t\tpmd_t *ret;\n\n\t\t\t/* the pgt_lock only for Xen */\n\t\t\tpgt_lock = &pgd_page_get_mm(page)->page_table_lock;\n\n\t\t\tspin_lock(pgt_lock);\n\t\t\tret = vmalloc_sync_one(page_address(page), address);\n\t\t\tspin_unlock(pgt_lock);\n\n\t\t\tif (!ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&pgd_lock);\n\t}\n}\n\n/*\n * 32-bit:\n *\n *   Handle a fault on the vmalloc or module mapping area\n */\nstatic noinline __kprobes int vmalloc_fault(unsigned long address)\n{\n\tunsigned long pgd_paddr;\n\tpmd_t *pmd_k;\n\tpte_t *pte_k;\n\n\t/* Make sure we are in vmalloc area: */\n\tif (!(address >= VMALLOC_START && address < VMALLOC_END))\n\t\treturn -1;\n\n\tWARN_ON_ONCE(in_nmi());\n\n\t/*\n\t * Synchronize this task's top level page-table\n\t * with the 'reference' page table.\n\t *\n\t * Do _not_ use \"current\" here. We might be inside\n\t * an interrupt in the middle of a task switch..\n\t */\n\tpgd_paddr = read_cr3();\n\tpmd_k = vmalloc_sync_one(__va(pgd_paddr), address);\n\tif (!pmd_k)\n\t\treturn -1;\n\n\tpte_k = pte_offset_kernel(pmd_k, address);\n\tif (!pte_present(*pte_k))\n\t\treturn -1;\n\n\treturn 0;\n}\n\n/*\n * Did it hit the DOS screen memory VA from vm86 mode?\n */\nstatic inline void\ncheck_v8086_mode(struct pt_regs *regs, unsigned long address,\n\t\t struct task_struct *tsk)\n{\n\tunsigned long bit;\n\n\tif (!v8086_mode(regs))\n\t\treturn;\n\n\tbit = (address - 0xA0000) >> PAGE_SHIFT;\n\tif (bit < 32)\n\t\ttsk->thread.screen_bitmap |= 1 << bit;\n}\n\nstatic bool low_pfn(unsigned long pfn)\n{\n\treturn pfn < max_low_pfn;\n}\n\nstatic void dump_pagetable(unsigned long address)\n{\n\tpgd_t *base = __va(read_cr3());\n\tpgd_t *pgd = &base[pgd_index(address)];\n\tpmd_t *pmd;\n\tpte_t *pte;\n\n#ifdef CONFIG_X86_PAE\n\tprintk(\"*pdpt = %016Lx \", pgd_val(*pgd));\n\tif (!low_pfn(pgd_val(*pgd) >> PAGE_SHIFT) || !pgd_present(*pgd))\n\t\tgoto out;\n#endif\n\tpmd = pmd_offset(pud_offset(pgd, address), address);\n\tprintk(KERN_CONT \"*pde = %0*Lx \", sizeof(*pmd) * 2, (u64)pmd_val(*pmd));\n\n\t/*\n\t * We must not directly access the pte in the highpte\n\t * case if the page table is located in highmem.\n\t * And let's rather not kmap-atomic the pte, just in case\n\t * it's allocated already:\n\t */\n\tif (!low_pfn(pmd_pfn(*pmd)) || !pmd_present(*pmd) || pmd_large(*pmd))\n\t\tgoto out;\n\n\tpte = pte_offset_kernel(pmd, address);\n\tprintk(\"*pte = %0*Lx \", sizeof(*pte) * 2, (u64)pte_val(*pte));\nout:\n\tprintk(\"\\n\");\n}\n\n#else /* CONFIG_X86_64: */\n\nvoid vmalloc_sync_all(void)\n{\n\tsync_global_pgds(VMALLOC_START & PGDIR_MASK, VMALLOC_END);\n}\n\n/*\n * 64-bit:\n *\n *   Handle a fault on the vmalloc area\n *\n * This assumes no large pages in there.\n */\nstatic noinline __kprobes int vmalloc_fault(unsigned long address)\n{\n\tpgd_t *pgd, *pgd_ref;\n\tpud_t *pud, *pud_ref;\n\tpmd_t *pmd, *pmd_ref;\n\tpte_t *pte, *pte_ref;\n\n\t/* Make sure we are in vmalloc area: */\n\tif (!(address >= VMALLOC_START && address < VMALLOC_END))\n\t\treturn -1;\n\n\tWARN_ON_ONCE(in_nmi());\n\n\t/*\n\t * Copy kernel mappings over when needed. This can also\n\t * happen within a race in page table update. In the later\n\t * case just flush:\n\t */\n\tpgd = pgd_offset(current->active_mm, address);\n\tpgd_ref = pgd_offset_k(address);\n\tif (pgd_none(*pgd_ref))\n\t\treturn -1;\n\n\tif (pgd_none(*pgd))\n\t\tset_pgd(pgd, *pgd_ref);\n\telse\n\t\tBUG_ON(pgd_page_vaddr(*pgd) != pgd_page_vaddr(*pgd_ref));\n\n\t/*\n\t * Below here mismatches are bugs because these lower tables\n\t * are shared:\n\t */\n\n\tpud = pud_offset(pgd, address);\n\tpud_ref = pud_offset(pgd_ref, address);\n\tif (pud_none(*pud_ref))\n\t\treturn -1;\n\n\tif (pud_none(*pud) || pud_page_vaddr(*pud) != pud_page_vaddr(*pud_ref))\n\t\tBUG();\n\n\tpmd = pmd_offset(pud, address);\n\tpmd_ref = pmd_offset(pud_ref, address);\n\tif (pmd_none(*pmd_ref))\n\t\treturn -1;\n\n\tif (pmd_none(*pmd) || pmd_page(*pmd) != pmd_page(*pmd_ref))\n\t\tBUG();\n\n\tpte_ref = pte_offset_kernel(pmd_ref, address);\n\tif (!pte_present(*pte_ref))\n\t\treturn -1;\n\n\tpte = pte_offset_kernel(pmd, address);\n\n\t/*\n\t * Don't use pte_page here, because the mappings can point\n\t * outside mem_map, and the NUMA hash lookup cannot handle\n\t * that:\n\t */\n\tif (!pte_present(*pte) || pte_pfn(*pte) != pte_pfn(*pte_ref))\n\t\tBUG();\n\n\treturn 0;\n}\n\nstatic const char errata93_warning[] =\nKERN_ERR \n\"******* Your BIOS seems to not contain a fix for K8 errata #93\\n\"\n\"******* Working around it, but it may cause SEGVs or burn power.\\n\"\n\"******* Please consider a BIOS update.\\n\"\n\"******* Disabling USB legacy in the BIOS may also help.\\n\";\n\n/*\n * No vm86 mode in 64-bit mode:\n */\nstatic inline void\ncheck_v8086_mode(struct pt_regs *regs, unsigned long address,\n\t\t struct task_struct *tsk)\n{\n}\n\nstatic int bad_address(void *p)\n{\n\tunsigned long dummy;\n\n\treturn probe_kernel_address((unsigned long *)p, dummy);\n}\n\nstatic void dump_pagetable(unsigned long address)\n{\n\tpgd_t *base = __va(read_cr3() & PHYSICAL_PAGE_MASK);\n\tpgd_t *pgd = base + pgd_index(address);\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\n\tif (bad_address(pgd))\n\t\tgoto bad;\n\n\tprintk(\"PGD %lx \", pgd_val(*pgd));\n\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\n\tpud = pud_offset(pgd, address);\n\tif (bad_address(pud))\n\t\tgoto bad;\n\n\tprintk(\"PUD %lx \", pud_val(*pud));\n\tif (!pud_present(*pud) || pud_large(*pud))\n\t\tgoto out;\n\n\tpmd = pmd_offset(pud, address);\n\tif (bad_address(pmd))\n\t\tgoto bad;\n\n\tprintk(\"PMD %lx \", pmd_val(*pmd));\n\tif (!pmd_present(*pmd) || pmd_large(*pmd))\n\t\tgoto out;\n\n\tpte = pte_offset_kernel(pmd, address);\n\tif (bad_address(pte))\n\t\tgoto bad;\n\n\tprintk(\"PTE %lx\", pte_val(*pte));\nout:\n\tprintk(\"\\n\");\n\treturn;\nbad:\n\tprintk(\"BAD\\n\");\n}\n\n#endif /* CONFIG_X86_64 */\n\n/*\n * Workaround for K8 erratum #93 & buggy BIOS.\n *\n * BIOS SMM functions are required to use a specific workaround\n * to avoid corruption of the 64bit RIP register on C stepping K8.\n *\n * A lot of BIOS that didn't get tested properly miss this.\n *\n * The OS sees this as a page fault with the upper 32bits of RIP cleared.\n * Try to work around it here.\n *\n * Note we only handle faults in kernel here.\n * Does nothing on 32-bit.\n */\nstatic int is_errata93(struct pt_regs *regs, unsigned long address)\n{\n#ifdef CONFIG_X86_64\n\tif (address != regs->ip)\n\t\treturn 0;\n\n\tif ((address >> 32) != 0)\n\t\treturn 0;\n\n\taddress |= 0xffffffffUL << 32;\n\tif ((address >= (u64)_stext && address <= (u64)_etext) ||\n\t    (address >= MODULES_VADDR && address <= MODULES_END)) {\n\t\tprintk_once(errata93_warning);\n\t\tregs->ip = address;\n\t\treturn 1;\n\t}\n#endif\n\treturn 0;\n}\n\n/*\n * Work around K8 erratum #100 K8 in compat mode occasionally jumps\n * to illegal addresses >4GB.\n *\n * We catch this in the page fault handler because these addresses\n * are not reachable. Just detect this case and return.  Any code\n * segment in LDT is compatibility mode.\n */\nstatic int is_errata100(struct pt_regs *regs, unsigned long address)\n{\n#ifdef CONFIG_X86_64\n\tif ((regs->cs == __USER32_CS || (regs->cs & (1<<2))) && (address >> 32))\n\t\treturn 1;\n#endif\n\treturn 0;\n}\n\nstatic int is_f00f_bug(struct pt_regs *regs, unsigned long address)\n{\n#ifdef CONFIG_X86_F00F_BUG\n\tunsigned long nr;\n\n\t/*\n\t * Pentium F0 0F C7 C8 bug workaround:\n\t */\n\tif (boot_cpu_data.f00f_bug) {\n\t\tnr = (address - idt_descr.address) >> 3;\n\n\t\tif (nr == 6) {\n\t\t\tdo_invalid_op(regs, 0);\n\t\t\treturn 1;\n\t\t}\n\t}\n#endif\n\treturn 0;\n}\n\nstatic const char nx_warning[] = KERN_CRIT\n\"kernel tried to execute NX-protected page - exploit attempt? (uid: %d)\\n\";\n\nstatic void\nshow_fault_oops(struct pt_regs *regs, unsigned long error_code,\n\t\tunsigned long address)\n{\n\tif (!oops_may_print())\n\t\treturn;\n\n\tif (error_code & PF_INSTR) {\n\t\tunsigned int level;\n\n\t\tpte_t *pte = lookup_address(address, &level);\n\n\t\tif (pte && pte_present(*pte) && !pte_exec(*pte))\n\t\t\tprintk(nx_warning, current_uid());\n\t}\n\n\tprintk(KERN_ALERT \"BUG: unable to handle kernel \");\n\tif (address < PAGE_SIZE)\n\t\tprintk(KERN_CONT \"NULL pointer dereference\");\n\telse\n\t\tprintk(KERN_CONT \"paging request\");\n\n\tprintk(KERN_CONT \" at %p\\n\", (void *) address);\n\tprintk(KERN_ALERT \"IP:\");\n\tprintk_address(regs->ip, 1);\n\n\tdump_pagetable(address);\n}\n\nstatic noinline void\npgtable_bad(struct pt_regs *regs, unsigned long error_code,\n\t    unsigned long address)\n{\n\tstruct task_struct *tsk;\n\tunsigned long flags;\n\tint sig;\n\n\tflags = oops_begin();\n\ttsk = current;\n\tsig = SIGKILL;\n\n\tprintk(KERN_ALERT \"%s: Corrupted page table at address %lx\\n\",\n\t       tsk->comm, address);\n\tdump_pagetable(address);\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_no\t= 14;\n\ttsk->thread.error_code\t= error_code;\n\n\tif (__die(\"Bad pagetable\", regs, error_code))\n\t\tsig = 0;\n\n\toops_end(flags, regs, sig);\n}\n\nstatic noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long *stackend;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tstackend = end_of_stack(tsk);\n\tif (tsk != &init_task && *stackend != STACK_END_MAGIC)\n\t\tprintk(KERN_ALERT \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_no\t= 14;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_EMERG \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}\n\n/*\n * Print out info about fatal segfaults, if the show_unhandled_signals\n * sysctl is set:\n */\nstatic inline void\nshow_signal_msg(struct pt_regs *regs, unsigned long error_code,\n\t\tunsigned long address, struct task_struct *tsk)\n{\n\tif (!unhandled_signal(tsk, SIGSEGV))\n\t\treturn;\n\n\tif (!printk_ratelimit())\n\t\treturn;\n\n\tprintk(\"%s%s[%d]: segfault at %lx ip %p sp %p error %lx\",\n\t\ttask_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,\n\t\ttsk->comm, task_pid_nr(tsk), address,\n\t\t(void *)regs->ip, (void *)regs->sp, error_code);\n\n\tprint_vma_addr(KERN_CONT \" in \", regs->ip);\n\n\tprintk(KERN_CONT \"\\n\");\n}\n\nstatic void\n__bad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,\n\t\t       unsigned long address, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * It's possible to have interrupts off here:\n\t\t */\n\t\tlocal_irq_enable();\n\n\t\t/*\n\t\t * Valid to do another page fault here because this one came\n\t\t * from user space:\n\t\t */\n\t\tif (is_prefetch(regs, error_code, address))\n\t\t\treturn;\n\n\t\tif (is_errata100(regs, address))\n\t\t\treturn;\n\n\t\tif (unlikely(show_unhandled_signals))\n\t\t\tshow_signal_msg(regs, error_code, address, tsk);\n\n\t\t/* Kernel addresses are always protection faults: */\n\t\ttsk->thread.cr2\t\t= address;\n\t\ttsk->thread.error_code\t= error_code | (address >= TASK_SIZE);\n\t\ttsk->thread.trap_no\t= 14;\n\n\t\tforce_sig_info_fault(SIGSEGV, si_code, address, tsk, 0);\n\n\t\treturn;\n\t}\n\n\tif (is_f00f_bug(regs, address))\n\t\treturn;\n\n\tno_context(regs, error_code, address);\n}\n\nstatic noinline void\nbad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,\n\t\t     unsigned long address)\n{\n\t__bad_area_nosemaphore(regs, error_code, address, SEGV_MAPERR);\n}\n\nstatic void\n__bad_area(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int si_code)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\t/*\n\t * Something tried to access memory that isn't in our memory map..\n\t * Fix it, but check if it's kernel or user first..\n\t */\n\tup_read(&mm->mmap_sem);\n\n\t__bad_area_nosemaphore(regs, error_code, address, si_code);\n}\n\nstatic noinline void\nbad_area(struct pt_regs *regs, unsigned long error_code, unsigned long address)\n{\n\t__bad_area(regs, error_code, address, SEGV_MAPERR);\n}\n\nstatic noinline void\nbad_area_access_error(struct pt_regs *regs, unsigned long error_code,\n\t\t      unsigned long address)\n{\n\t__bad_area(regs, error_code, address, SEGV_ACCERR);\n}\n\n/* TODO: fixup for \"mm-invoke-oom-killer-from-page-fault.patch\" */\nstatic void\nout_of_memory(struct pt_regs *regs, unsigned long error_code,\n\t      unsigned long address)\n{\n\t/*\n\t * We ran out of memory, call the OOM killer, and return the userspace\n\t * (which will retry the fault, or kill us if we got oom-killed):\n\t */\n\tup_read(&current->mm->mmap_sem);\n\n\tpagefault_out_of_memory();\n}\n\nstatic void\ndo_sigbus(struct pt_regs *regs, unsigned long error_code, unsigned long address,\n\t  unsigned int fault)\n{\n\tstruct task_struct *tsk = current;\n\tstruct mm_struct *mm = tsk->mm;\n\tint code = BUS_ADRERR;\n\n\tup_read(&mm->mmap_sem);\n\n\t/* Kernel mode? Handle exceptions or die: */\n\tif (!(error_code & PF_USER)) {\n\t\tno_context(regs, error_code, address);\n\t\treturn;\n\t}\n\n\t/* User-space => ok to do another page fault: */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.error_code\t= error_code;\n\ttsk->thread.trap_no\t= 14;\n\n#ifdef CONFIG_MEMORY_FAILURE\n\tif (fault & (VM_FAULT_HWPOISON|VM_FAULT_HWPOISON_LARGE)) {\n\t\tprintk(KERN_ERR\n\t\"MCE: Killing %s:%d due to hardware memory corruption fault at %lx\\n\",\n\t\t\ttsk->comm, tsk->pid, address);\n\t\tcode = BUS_MCEERR_AR;\n\t}\n#endif\n\tforce_sig_info_fault(SIGBUS, code, address, tsk, fault);\n}\n\nstatic noinline int\nmm_fault_error(struct pt_regs *regs, unsigned long error_code,\n\t       unsigned long address, unsigned int fault)\n{\n\t/*\n\t * Pagefault was interrupted by SIGKILL. We have no reason to\n\t * continue pagefault.\n\t */\n\tif (fatal_signal_pending(current)) {\n\t\tif (!(fault & VM_FAULT_RETRY))\n\t\t\tup_read(&current->mm->mmap_sem);\n\t\tif (!(error_code & PF_USER))\n\t\t\tno_context(regs, error_code, address);\n\t\treturn 1;\n\t}\n\tif (!(fault & VM_FAULT_ERROR))\n\t\treturn 0;\n\n\tif (fault & VM_FAULT_OOM) {\n\t\t/* Kernel mode? Handle exceptions or die: */\n\t\tif (!(error_code & PF_USER)) {\n\t\t\tup_read(&current->mm->mmap_sem);\n\t\t\tno_context(regs, error_code, address);\n\t\t\treturn 1;\n\t\t}\n\n\t\tout_of_memory(regs, error_code, address);\n\t} else {\n\t\tif (fault & (VM_FAULT_SIGBUS|VM_FAULT_HWPOISON|\n\t\t\t     VM_FAULT_HWPOISON_LARGE))\n\t\t\tdo_sigbus(regs, error_code, address, fault);\n\t\telse\n\t\t\tBUG();\n\t}\n\treturn 1;\n}\n\nstatic int spurious_fault_check(unsigned long error_code, pte_t *pte)\n{\n\tif ((error_code & PF_WRITE) && !pte_write(*pte))\n\t\treturn 0;\n\n\tif ((error_code & PF_INSTR) && !pte_exec(*pte))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n/*\n * Handle a spurious fault caused by a stale TLB entry.\n *\n * This allows us to lazily refresh the TLB when increasing the\n * permissions of a kernel page (RO -> RW or NX -> X).  Doing it\n * eagerly is very expensive since that implies doing a full\n * cross-processor TLB flush, even if no stale TLB entries exist\n * on other processors.\n *\n * There are no security implications to leaving a stale TLB when\n * increasing the permissions on a page.\n */\nstatic noinline __kprobes int\nspurious_fault(unsigned long error_code, unsigned long address)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tint ret;\n\n\t/* Reserved-bit violation or user access to kernel space? */\n\tif (error_code & (PF_USER | PF_RSVD))\n\t\treturn 0;\n\n\tpgd = init_mm.pgd + pgd_index(address);\n\tif (!pgd_present(*pgd))\n\t\treturn 0;\n\n\tpud = pud_offset(pgd, address);\n\tif (!pud_present(*pud))\n\t\treturn 0;\n\n\tif (pud_large(*pud))\n\t\treturn spurious_fault_check(error_code, (pte_t *) pud);\n\n\tpmd = pmd_offset(pud, address);\n\tif (!pmd_present(*pmd))\n\t\treturn 0;\n\n\tif (pmd_large(*pmd))\n\t\treturn spurious_fault_check(error_code, (pte_t *) pmd);\n\n\t/*\n\t * Note: don't use pte_present() here, since it returns true\n\t * if the _PAGE_PROTNONE bit is set.  However, this aliases the\n\t * _PAGE_GLOBAL bit, which for kernel pages give false positives\n\t * when CONFIG_DEBUG_PAGEALLOC is used.\n\t */\n\tpte = pte_offset_kernel(pmd, address);\n\tif (!(pte_flags(*pte) & _PAGE_PRESENT))\n\t\treturn 0;\n\n\tret = spurious_fault_check(error_code, pte);\n\tif (!ret)\n\t\treturn 0;\n\n\t/*\n\t * Make sure we have permissions in PMD.\n\t * If not, then there's a bug in the page tables:\n\t */\n\tret = spurious_fault_check(error_code, (pte_t *) pmd);\n\tWARN_ONCE(!ret, \"PMD has incorrect permission bits\\n\");\n\n\treturn ret;\n}\n\nint show_unhandled_signals = 1;\n\nstatic inline int\naccess_error(unsigned long error_code, struct vm_area_struct *vma)\n{\n\tif (error_code & PF_WRITE) {\n\t\t/* write, present and write, not present: */\n\t\tif (unlikely(!(vma->vm_flags & VM_WRITE)))\n\t\t\treturn 1;\n\t\treturn 0;\n\t}\n\n\t/* read, present: */\n\tif (unlikely(error_code & PF_PROT))\n\t\treturn 1;\n\n\t/* read, not present: */\n\tif (unlikely(!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE))))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int fault_in_kernel_space(unsigned long address)\n{\n\treturn address >= TASK_SIZE_MAX;\n}\n\n/*\n * This routine handles page faults.  It determines the address,\n * and the problem, and then passes it off to one of the appropriate\n * routines.\n */\ndotraplinkage void __kprobes\ndo_page_fault(struct pt_regs *regs, unsigned long error_code)\n{\n\tstruct vm_area_struct *vma;\n\tstruct task_struct *tsk;\n\tunsigned long address;\n\tstruct mm_struct *mm;\n\tint fault;\n\tint write = error_code & PF_WRITE;\n\tunsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE |\n\t\t\t\t\t(write ? FAULT_FLAG_WRITE : 0);\n\n\ttsk = current;\n\tmm = tsk->mm;\n\n\t/* Get the faulting address: */\n\taddress = read_cr2();\n\n\t/*\n\t * Detect and handle instructions that would cause a page fault for\n\t * both a tracked kernel page and a userspace page.\n\t */\n\tif (kmemcheck_active(regs))\n\t\tkmemcheck_hide(regs);\n\tprefetchw(&mm->mmap_sem);\n\n\tif (unlikely(kmmio_fault(regs, address)))\n\t\treturn;\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t *\n\t * This verifies that the fault happens in kernel space\n\t * (error_code & 4) == 0, and that the fault was not a\n\t * protection error (error_code & 9) == 0.\n\t */\n\tif (unlikely(fault_in_kernel_space(address))) {\n\t\tif (!(error_code & (PF_RSVD | PF_USER | PF_PROT))) {\n\t\t\tif (vmalloc_fault(address) >= 0)\n\t\t\t\treturn;\n\n\t\t\tif (kmemcheck_fault(regs, address, error_code))\n\t\t\t\treturn;\n\t\t}\n\n\t\t/* Can handle a stale RO->RW TLB: */\n\t\tif (spurious_fault(error_code, address))\n\t\t\treturn;\n\n\t\t/* kprobes don't want to hook the spurious faults: */\n\t\tif (notify_page_fault(regs))\n\t\t\treturn;\n\t\t/*\n\t\t * Don't take the mm semaphore here. If we fixup a prefetch\n\t\t * fault we could otherwise deadlock:\n\t\t */\n\t\tbad_area_nosemaphore(regs, error_code, address);\n\n\t\treturn;\n\t}\n\n\t/* kprobes don't want to hook the spurious faults: */\n\tif (unlikely(notify_page_fault(regs)))\n\t\treturn;\n\t/*\n\t * It's safe to allow irq's after cr2 has been saved and the\n\t * vmalloc fault has been handled.\n\t *\n\t * User-mode registers count as a user access even for any\n\t * potential system fault or CPU buglet:\n\t */\n\tif (user_mode_vm(regs)) {\n\t\tlocal_irq_enable();\n\t\terror_code |= PF_USER;\n\t} else {\n\t\tif (regs->flags & X86_EFLAGS_IF)\n\t\t\tlocal_irq_enable();\n\t}\n\n\tif (unlikely(error_code & PF_RSVD))\n\t\tpgtable_bad(regs, error_code, address);\n\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);\n\n\t/*\n\t * If we're in an interrupt, have no user context or are running\n\t * in an atomic region then we must not take the fault:\n\t */\n\tif (unlikely(in_atomic() || !mm)) {\n\t\tbad_area_nosemaphore(regs, error_code, address);\n\t\treturn;\n\t}\n\n\t/*\n\t * When running in the kernel we expect faults to occur only to\n\t * addresses in user space.  All other faults represent errors in\n\t * the kernel and should generate an OOPS.  Unfortunately, in the\n\t * case of an erroneous fault occurring in a code path which already\n\t * holds mmap_sem we will deadlock attempting to validate the fault\n\t * against the address space.  Luckily the kernel only validly\n\t * references user space from well defined areas of code, which are\n\t * listed in the exceptions table.\n\t *\n\t * As the vast majority of faults will be valid we will only perform\n\t * the source reference check when there is a possibility of a\n\t * deadlock. Attempt to lock the address space, if we cannot we then\n\t * validate the source. If this is invalid we can skip the address\n\t * space check, thus avoiding the deadlock:\n\t */\n\tif (unlikely(!down_read_trylock(&mm->mmap_sem))) {\n\t\tif ((error_code & PF_USER) == 0 &&\n\t\t    !search_exception_tables(regs->ip)) {\n\t\t\tbad_area_nosemaphore(regs, error_code, address);\n\t\t\treturn;\n\t\t}\nretry:\n\t\tdown_read(&mm->mmap_sem);\n\t} else {\n\t\t/*\n\t\t * The above down_read_trylock() might have succeeded in\n\t\t * which case we'll have missed the might_sleep() from\n\t\t * down_read():\n\t\t */\n\t\tmight_sleep();\n\t}\n\n\tvma = find_vma(mm, address);\n\tif (unlikely(!vma)) {\n\t\tbad_area(regs, error_code, address);\n\t\treturn;\n\t}\n\tif (likely(vma->vm_start <= address))\n\t\tgoto good_area;\n\tif (unlikely(!(vma->vm_flags & VM_GROWSDOWN))) {\n\t\tbad_area(regs, error_code, address);\n\t\treturn;\n\t}\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * Accessing the stack below %sp is always a bug.\n\t\t * The large cushion allows instructions like enter\n\t\t * and pusha to work. (\"enter $65535, $31\" pushes\n\t\t * 32 pointers and then decrements %sp by 65535.)\n\t\t */\n\t\tif (unlikely(address + 65536 + 32 * sizeof(unsigned long) < regs->sp)) {\n\t\t\tbad_area(regs, error_code, address);\n\t\t\treturn;\n\t\t}\n\t}\n\tif (unlikely(expand_stack(vma, address))) {\n\t\tbad_area(regs, error_code, address);\n\t\treturn;\n\t}\n\n\t/*\n\t * Ok, we have a good vm_area for this memory access, so\n\t * we can handle it..\n\t */\ngood_area:\n\tif (unlikely(access_error(error_code, vma))) {\n\t\tbad_area_access_error(regs, error_code, address);\n\t\treturn;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault:\n\t */\n\tfault = handle_mm_fault(mm, vma, address, flags);\n\n\tif (unlikely(fault & (VM_FAULT_RETRY|VM_FAULT_ERROR))) {\n\t\tif (mm_fault_error(regs, error_code, address, fault))\n\t\t\treturn;\n\t}\n\n\t/*\n\t * Major/minor page fault accounting is only done on the\n\t * initial attempt. If we go through a retry, it is extremely\n\t * likely that the page will be found in page cache at that point.\n\t */\n\tif (flags & FAULT_FLAG_ALLOW_RETRY) {\n\t\tif (fault & VM_FAULT_MAJOR) {\n\t\t\ttsk->maj_flt++;\n\t\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,\n\t\t\t\t      regs, address);\n\t\t} else {\n\t\t\ttsk->min_flt++;\n\t\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,\n\t\t\t\t      regs, address);\n\t\t}\n\t\tif (fault & VM_FAULT_RETRY) {\n\t\t\t/* Clear FAULT_FLAG_ALLOW_RETRY to avoid any risk\n\t\t\t * of starvation. */\n\t\t\tflags &= ~FAULT_FLAG_ALLOW_RETRY;\n\t\t\tgoto retry;\n\t\t}\n\t}\n\n\tcheck_v8086_mode(regs, address, tsk);\n\n\tup_read(&mm->mmap_sem);\n}\n", "/*\n * Performance events:\n *\n *    Copyright (C) 2008-2009, Thomas Gleixner <tglx@linutronix.de>\n *    Copyright (C) 2008-2011, Red Hat, Inc., Ingo Molnar\n *    Copyright (C) 2008-2011, Red Hat, Inc., Peter Zijlstra\n *\n * Data type definitions, declarations, prototypes.\n *\n *    Started by: Thomas Gleixner and Ingo Molnar\n *\n * For licencing details see kernel-base/COPYING\n */\n#ifndef _LINUX_PERF_EVENT_H\n#define _LINUX_PERF_EVENT_H\n\n#include <linux/types.h>\n#include <linux/ioctl.h>\n#include <asm/byteorder.h>\n\n/*\n * User-space ABI bits:\n */\n\n/*\n * attr.type\n */\nenum perf_type_id {\n\tPERF_TYPE_HARDWARE\t\t\t= 0,\n\tPERF_TYPE_SOFTWARE\t\t\t= 1,\n\tPERF_TYPE_TRACEPOINT\t\t\t= 2,\n\tPERF_TYPE_HW_CACHE\t\t\t= 3,\n\tPERF_TYPE_RAW\t\t\t\t= 4,\n\tPERF_TYPE_BREAKPOINT\t\t\t= 5,\n\n\tPERF_TYPE_MAX,\t\t\t\t/* non-ABI */\n};\n\n/*\n * Generalized performance event event_id types, used by the\n * attr.event_id parameter of the sys_perf_event_open()\n * syscall:\n */\nenum perf_hw_id {\n\t/*\n\t * Common hardware events, generalized by the kernel:\n\t */\n\tPERF_COUNT_HW_CPU_CYCLES\t\t= 0,\n\tPERF_COUNT_HW_INSTRUCTIONS\t\t= 1,\n\tPERF_COUNT_HW_CACHE_REFERENCES\t\t= 2,\n\tPERF_COUNT_HW_CACHE_MISSES\t\t= 3,\n\tPERF_COUNT_HW_BRANCH_INSTRUCTIONS\t= 4,\n\tPERF_COUNT_HW_BRANCH_MISSES\t\t= 5,\n\tPERF_COUNT_HW_BUS_CYCLES\t\t= 6,\n\tPERF_COUNT_HW_STALLED_CYCLES_FRONTEND\t= 7,\n\tPERF_COUNT_HW_STALLED_CYCLES_BACKEND\t= 8,\n\n\tPERF_COUNT_HW_MAX,\t\t\t/* non-ABI */\n};\n\n/*\n * Generalized hardware cache events:\n *\n *       { L1-D, L1-I, LLC, ITLB, DTLB, BPU } x\n *       { read, write, prefetch } x\n *       { accesses, misses }\n */\nenum perf_hw_cache_id {\n\tPERF_COUNT_HW_CACHE_L1D\t\t\t= 0,\n\tPERF_COUNT_HW_CACHE_L1I\t\t\t= 1,\n\tPERF_COUNT_HW_CACHE_LL\t\t\t= 2,\n\tPERF_COUNT_HW_CACHE_DTLB\t\t= 3,\n\tPERF_COUNT_HW_CACHE_ITLB\t\t= 4,\n\tPERF_COUNT_HW_CACHE_BPU\t\t\t= 5,\n\n\tPERF_COUNT_HW_CACHE_MAX,\t\t/* non-ABI */\n};\n\nenum perf_hw_cache_op_id {\n\tPERF_COUNT_HW_CACHE_OP_READ\t\t= 0,\n\tPERF_COUNT_HW_CACHE_OP_WRITE\t\t= 1,\n\tPERF_COUNT_HW_CACHE_OP_PREFETCH\t\t= 2,\n\n\tPERF_COUNT_HW_CACHE_OP_MAX,\t\t/* non-ABI */\n};\n\nenum perf_hw_cache_op_result_id {\n\tPERF_COUNT_HW_CACHE_RESULT_ACCESS\t= 0,\n\tPERF_COUNT_HW_CACHE_RESULT_MISS\t\t= 1,\n\n\tPERF_COUNT_HW_CACHE_RESULT_MAX,\t\t/* non-ABI */\n};\n\n/*\n * Special \"software\" events provided by the kernel, even if the hardware\n * does not support performance events. These events measure various\n * physical and sw events of the kernel (and allow the profiling of them as\n * well):\n */\nenum perf_sw_ids {\n\tPERF_COUNT_SW_CPU_CLOCK\t\t\t= 0,\n\tPERF_COUNT_SW_TASK_CLOCK\t\t= 1,\n\tPERF_COUNT_SW_PAGE_FAULTS\t\t= 2,\n\tPERF_COUNT_SW_CONTEXT_SWITCHES\t\t= 3,\n\tPERF_COUNT_SW_CPU_MIGRATIONS\t\t= 4,\n\tPERF_COUNT_SW_PAGE_FAULTS_MIN\t\t= 5,\n\tPERF_COUNT_SW_PAGE_FAULTS_MAJ\t\t= 6,\n\tPERF_COUNT_SW_ALIGNMENT_FAULTS\t\t= 7,\n\tPERF_COUNT_SW_EMULATION_FAULTS\t\t= 8,\n\n\tPERF_COUNT_SW_MAX,\t\t\t/* non-ABI */\n};\n\n/*\n * Bits that can be set in attr.sample_type to request information\n * in the overflow packets.\n */\nenum perf_event_sample_format {\n\tPERF_SAMPLE_IP\t\t\t\t= 1U << 0,\n\tPERF_SAMPLE_TID\t\t\t\t= 1U << 1,\n\tPERF_SAMPLE_TIME\t\t\t= 1U << 2,\n\tPERF_SAMPLE_ADDR\t\t\t= 1U << 3,\n\tPERF_SAMPLE_READ\t\t\t= 1U << 4,\n\tPERF_SAMPLE_CALLCHAIN\t\t\t= 1U << 5,\n\tPERF_SAMPLE_ID\t\t\t\t= 1U << 6,\n\tPERF_SAMPLE_CPU\t\t\t\t= 1U << 7,\n\tPERF_SAMPLE_PERIOD\t\t\t= 1U << 8,\n\tPERF_SAMPLE_STREAM_ID\t\t\t= 1U << 9,\n\tPERF_SAMPLE_RAW\t\t\t\t= 1U << 10,\n\n\tPERF_SAMPLE_MAX = 1U << 11,\t\t/* non-ABI */\n};\n\n/*\n * The format of the data returned by read() on a perf event fd,\n * as specified by attr.read_format:\n *\n * struct read_format {\n *\t{ u64\t\tvalue;\n *\t  { u64\t\ttime_enabled; } && PERF_FORMAT_TOTAL_TIME_ENABLED\n *\t  { u64\t\ttime_running; } && PERF_FORMAT_TOTAL_TIME_RUNNING\n *\t  { u64\t\tid;           } && PERF_FORMAT_ID\n *\t} && !PERF_FORMAT_GROUP\n *\n *\t{ u64\t\tnr;\n *\t  { u64\t\ttime_enabled; } && PERF_FORMAT_TOTAL_TIME_ENABLED\n *\t  { u64\t\ttime_running; } && PERF_FORMAT_TOTAL_TIME_RUNNING\n *\t  { u64\t\tvalue;\n *\t    { u64\tid;           } && PERF_FORMAT_ID\n *\t  }\t\tcntr[nr];\n *\t} && PERF_FORMAT_GROUP\n * };\n */\nenum perf_event_read_format {\n\tPERF_FORMAT_TOTAL_TIME_ENABLED\t\t= 1U << 0,\n\tPERF_FORMAT_TOTAL_TIME_RUNNING\t\t= 1U << 1,\n\tPERF_FORMAT_ID\t\t\t\t= 1U << 2,\n\tPERF_FORMAT_GROUP\t\t\t= 1U << 3,\n\n\tPERF_FORMAT_MAX = 1U << 4,\t\t/* non-ABI */\n};\n\n#define PERF_ATTR_SIZE_VER0\t64\t/* sizeof first published struct */\n\n/*\n * Hardware event_id to monitor via a performance monitoring event:\n */\nstruct perf_event_attr {\n\n\t/*\n\t * Major type: hardware/software/tracepoint/etc.\n\t */\n\t__u32\t\t\ttype;\n\n\t/*\n\t * Size of the attr structure, for fwd/bwd compat.\n\t */\n\t__u32\t\t\tsize;\n\n\t/*\n\t * Type specific configuration information.\n\t */\n\t__u64\t\t\tconfig;\n\n\tunion {\n\t\t__u64\t\tsample_period;\n\t\t__u64\t\tsample_freq;\n\t};\n\n\t__u64\t\t\tsample_type;\n\t__u64\t\t\tread_format;\n\n\t__u64\t\t\tdisabled       :  1, /* off by default        */\n\t\t\t\tinherit\t       :  1, /* children inherit it   */\n\t\t\t\tpinned\t       :  1, /* must always be on PMU */\n\t\t\t\texclusive      :  1, /* only group on PMU     */\n\t\t\t\texclude_user   :  1, /* don't count user      */\n\t\t\t\texclude_kernel :  1, /* ditto kernel          */\n\t\t\t\texclude_hv     :  1, /* ditto hypervisor      */\n\t\t\t\texclude_idle   :  1, /* don't count when idle */\n\t\t\t\tmmap           :  1, /* include mmap data     */\n\t\t\t\tcomm\t       :  1, /* include comm data     */\n\t\t\t\tfreq           :  1, /* use freq, not period  */\n\t\t\t\tinherit_stat   :  1, /* per task counts       */\n\t\t\t\tenable_on_exec :  1, /* next exec enables     */\n\t\t\t\ttask           :  1, /* trace fork/exit       */\n\t\t\t\twatermark      :  1, /* wakeup_watermark      */\n\t\t\t\t/*\n\t\t\t\t * precise_ip:\n\t\t\t\t *\n\t\t\t\t *  0 - SAMPLE_IP can have arbitrary skid\n\t\t\t\t *  1 - SAMPLE_IP must have constant skid\n\t\t\t\t *  2 - SAMPLE_IP requested to have 0 skid\n\t\t\t\t *  3 - SAMPLE_IP must have 0 skid\n\t\t\t\t *\n\t\t\t\t *  See also PERF_RECORD_MISC_EXACT_IP\n\t\t\t\t */\n\t\t\t\tprecise_ip     :  2, /* skid constraint       */\n\t\t\t\tmmap_data      :  1, /* non-exec mmap data    */\n\t\t\t\tsample_id_all  :  1, /* sample_type all events */\n\n\t\t\t\t__reserved_1   : 45;\n\n\tunion {\n\t\t__u32\t\twakeup_events;\t  /* wakeup every n events */\n\t\t__u32\t\twakeup_watermark; /* bytes before wakeup   */\n\t};\n\n\t__u32\t\t\tbp_type;\n\tunion {\n\t\t__u64\t\tbp_addr;\n\t\t__u64\t\tconfig1; /* extension of config */\n\t};\n\tunion {\n\t\t__u64\t\tbp_len;\n\t\t__u64\t\tconfig2; /* extension of config1 */\n\t};\n};\n\n/*\n * Ioctls that can be done on a perf event fd:\n */\n#define PERF_EVENT_IOC_ENABLE\t\t_IO ('$', 0)\n#define PERF_EVENT_IOC_DISABLE\t\t_IO ('$', 1)\n#define PERF_EVENT_IOC_REFRESH\t\t_IO ('$', 2)\n#define PERF_EVENT_IOC_RESET\t\t_IO ('$', 3)\n#define PERF_EVENT_IOC_PERIOD\t\t_IOW('$', 4, __u64)\n#define PERF_EVENT_IOC_SET_OUTPUT\t_IO ('$', 5)\n#define PERF_EVENT_IOC_SET_FILTER\t_IOW('$', 6, char *)\n\nenum perf_event_ioc_flags {\n\tPERF_IOC_FLAG_GROUP\t\t= 1U << 0,\n};\n\n/*\n * Structure of the page that can be mapped via mmap\n */\nstruct perf_event_mmap_page {\n\t__u32\tversion;\t\t/* version number of this structure */\n\t__u32\tcompat_version;\t\t/* lowest version this is compat with */\n\n\t/*\n\t * Bits needed to read the hw events in user-space.\n\t *\n\t *   u32 seq;\n\t *   s64 count;\n\t *\n\t *   do {\n\t *     seq = pc->lock;\n\t *\n\t *     barrier()\n\t *     if (pc->index) {\n\t *       count = pmc_read(pc->index - 1);\n\t *       count += pc->offset;\n\t *     } else\n\t *       goto regular_read;\n\t *\n\t *     barrier();\n\t *   } while (pc->lock != seq);\n\t *\n\t * NOTE: for obvious reason this only works on self-monitoring\n\t *       processes.\n\t */\n\t__u32\tlock;\t\t\t/* seqlock for synchronization */\n\t__u32\tindex;\t\t\t/* hardware event identifier */\n\t__s64\toffset;\t\t\t/* add to hardware event value */\n\t__u64\ttime_enabled;\t\t/* time event active */\n\t__u64\ttime_running;\t\t/* time event on cpu */\n\n\t\t/*\n\t\t * Hole for extension of the self monitor capabilities\n\t\t */\n\n\t__u64\t__reserved[123];\t/* align to 1k */\n\n\t/*\n\t * Control data for the mmap() data buffer.\n\t *\n\t * User-space reading the @data_head value should issue an rmb(), on\n\t * SMP capable platforms, after reading this value -- see\n\t * perf_event_wakeup().\n\t *\n\t * When the mapping is PROT_WRITE the @data_tail value should be\n\t * written by userspace to reflect the last read data. In this case\n\t * the kernel will not over-write unread data.\n\t */\n\t__u64   data_head;\t\t/* head in the data section */\n\t__u64\tdata_tail;\t\t/* user-space written tail */\n};\n\n#define PERF_RECORD_MISC_CPUMODE_MASK\t\t(7 << 0)\n#define PERF_RECORD_MISC_CPUMODE_UNKNOWN\t(0 << 0)\n#define PERF_RECORD_MISC_KERNEL\t\t\t(1 << 0)\n#define PERF_RECORD_MISC_USER\t\t\t(2 << 0)\n#define PERF_RECORD_MISC_HYPERVISOR\t\t(3 << 0)\n#define PERF_RECORD_MISC_GUEST_KERNEL\t\t(4 << 0)\n#define PERF_RECORD_MISC_GUEST_USER\t\t(5 << 0)\n\n/*\n * Indicates that the content of PERF_SAMPLE_IP points to\n * the actual instruction that triggered the event. See also\n * perf_event_attr::precise_ip.\n */\n#define PERF_RECORD_MISC_EXACT_IP\t\t(1 << 14)\n/*\n * Reserve the last bit to indicate some extended misc field\n */\n#define PERF_RECORD_MISC_EXT_RESERVED\t\t(1 << 15)\n\nstruct perf_event_header {\n\t__u32\ttype;\n\t__u16\tmisc;\n\t__u16\tsize;\n};\n\nenum perf_event_type {\n\n\t/*\n\t * If perf_event_attr.sample_id_all is set then all event types will\n\t * have the sample_type selected fields related to where/when\n\t * (identity) an event took place (TID, TIME, ID, CPU, STREAM_ID)\n\t * described in PERF_RECORD_SAMPLE below, it will be stashed just after\n\t * the perf_event_header and the fields already present for the existing\n\t * fields, i.e. at the end of the payload. That way a newer perf.data\n\t * file will be supported by older perf tools, with these new optional\n\t * fields being ignored.\n\t *\n\t * The MMAP events record the PROT_EXEC mappings so that we can\n\t * correlate userspace IPs to code. They have the following structure:\n\t *\n\t * struct {\n\t *\tstruct perf_event_header\theader;\n\t *\n\t *\tu32\t\t\t\tpid, tid;\n\t *\tu64\t\t\t\taddr;\n\t *\tu64\t\t\t\tlen;\n\t *\tu64\t\t\t\tpgoff;\n\t *\tchar\t\t\t\tfilename[];\n\t * };\n\t */\n\tPERF_RECORD_MMAP\t\t\t= 1,\n\n\t/*\n\t * struct {\n\t *\tstruct perf_event_header\theader;\n\t *\tu64\t\t\t\tid;\n\t *\tu64\t\t\t\tlost;\n\t * };\n\t */\n\tPERF_RECORD_LOST\t\t\t= 2,\n\n\t/*\n\t * struct {\n\t *\tstruct perf_event_header\theader;\n\t *\n\t *\tu32\t\t\t\tpid, tid;\n\t *\tchar\t\t\t\tcomm[];\n\t * };\n\t */\n\tPERF_RECORD_COMM\t\t\t= 3,\n\n\t/*\n\t * struct {\n\t *\tstruct perf_event_header\theader;\n\t *\tu32\t\t\t\tpid, ppid;\n\t *\tu32\t\t\t\ttid, ptid;\n\t *\tu64\t\t\t\ttime;\n\t * };\n\t */\n\tPERF_RECORD_EXIT\t\t\t= 4,\n\n\t/*\n\t * struct {\n\t *\tstruct perf_event_header\theader;\n\t *\tu64\t\t\t\ttime;\n\t *\tu64\t\t\t\tid;\n\t *\tu64\t\t\t\tstream_id;\n\t * };\n\t */\n\tPERF_RECORD_THROTTLE\t\t\t= 5,\n\tPERF_RECORD_UNTHROTTLE\t\t\t= 6,\n\n\t/*\n\t * struct {\n\t *\tstruct perf_event_header\theader;\n\t *\tu32\t\t\t\tpid, ppid;\n\t *\tu32\t\t\t\ttid, ptid;\n\t *\tu64\t\t\t\ttime;\n\t * };\n\t */\n\tPERF_RECORD_FORK\t\t\t= 7,\n\n\t/*\n\t * struct {\n\t *\tstruct perf_event_header\theader;\n\t *\tu32\t\t\t\tpid, tid;\n\t *\n\t *\tstruct read_format\t\tvalues;\n\t * };\n\t */\n\tPERF_RECORD_READ\t\t\t= 8,\n\n\t/*\n\t * struct {\n\t *\tstruct perf_event_header\theader;\n\t *\n\t *\t{ u64\t\t\tip;\t  } && PERF_SAMPLE_IP\n\t *\t{ u32\t\t\tpid, tid; } && PERF_SAMPLE_TID\n\t *\t{ u64\t\t\ttime;     } && PERF_SAMPLE_TIME\n\t *\t{ u64\t\t\taddr;     } && PERF_SAMPLE_ADDR\n\t *\t{ u64\t\t\tid;\t  } && PERF_SAMPLE_ID\n\t *\t{ u64\t\t\tstream_id;} && PERF_SAMPLE_STREAM_ID\n\t *\t{ u32\t\t\tcpu, res; } && PERF_SAMPLE_CPU\n\t *\t{ u64\t\t\tperiod;   } && PERF_SAMPLE_PERIOD\n\t *\n\t *\t{ struct read_format\tvalues;\t  } && PERF_SAMPLE_READ\n\t *\n\t *\t{ u64\t\t\tnr,\n\t *\t  u64\t\t\tips[nr];  } && PERF_SAMPLE_CALLCHAIN\n\t *\n\t *\t#\n\t *\t# The RAW record below is opaque data wrt the ABI\n\t *\t#\n\t *\t# That is, the ABI doesn't make any promises wrt to\n\t *\t# the stability of its content, it may vary depending\n\t *\t# on event, hardware, kernel version and phase of\n\t *\t# the moon.\n\t *\t#\n\t *\t# In other words, PERF_SAMPLE_RAW contents are not an ABI.\n\t *\t#\n\t *\n\t *\t{ u32\t\t\tsize;\n\t *\t  char                  data[size];}&& PERF_SAMPLE_RAW\n\t * };\n\t */\n\tPERF_RECORD_SAMPLE\t\t\t= 9,\n\n\tPERF_RECORD_MAX,\t\t\t/* non-ABI */\n};\n\nenum perf_callchain_context {\n\tPERF_CONTEXT_HV\t\t\t= (__u64)-32,\n\tPERF_CONTEXT_KERNEL\t\t= (__u64)-128,\n\tPERF_CONTEXT_USER\t\t= (__u64)-512,\n\n\tPERF_CONTEXT_GUEST\t\t= (__u64)-2048,\n\tPERF_CONTEXT_GUEST_KERNEL\t= (__u64)-2176,\n\tPERF_CONTEXT_GUEST_USER\t\t= (__u64)-2560,\n\n\tPERF_CONTEXT_MAX\t\t= (__u64)-4095,\n};\n\n#define PERF_FLAG_FD_NO_GROUP\t\t(1U << 0)\n#define PERF_FLAG_FD_OUTPUT\t\t(1U << 1)\n#define PERF_FLAG_PID_CGROUP\t\t(1U << 2) /* pid=cgroup id, per-cpu mode only */\n\n#ifdef __KERNEL__\n/*\n * Kernel-internal data types and definitions:\n */\n\n#ifdef CONFIG_PERF_EVENTS\n# include <linux/cgroup.h>\n# include <asm/perf_event.h>\n# include <asm/local64.h>\n#endif\n\nstruct perf_guest_info_callbacks {\n\tint\t\t\t\t(*is_in_guest)(void);\n\tint\t\t\t\t(*is_user_mode)(void);\n\tunsigned long\t\t\t(*get_guest_ip)(void);\n};\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n#include <asm/hw_breakpoint.h>\n#endif\n\n#include <linux/list.h>\n#include <linux/mutex.h>\n#include <linux/rculist.h>\n#include <linux/rcupdate.h>\n#include <linux/spinlock.h>\n#include <linux/hrtimer.h>\n#include <linux/fs.h>\n#include <linux/pid_namespace.h>\n#include <linux/workqueue.h>\n#include <linux/ftrace.h>\n#include <linux/cpu.h>\n#include <linux/irq_work.h>\n#include <linux/jump_label.h>\n#include <asm/atomic.h>\n#include <asm/local.h>\n\n#define PERF_MAX_STACK_DEPTH\t\t255\n\nstruct perf_callchain_entry {\n\t__u64\t\t\t\tnr;\n\t__u64\t\t\t\tip[PERF_MAX_STACK_DEPTH];\n};\n\nstruct perf_raw_record {\n\tu32\t\t\t\tsize;\n\tvoid\t\t\t\t*data;\n};\n\nstruct perf_branch_entry {\n\t__u64\t\t\t\tfrom;\n\t__u64\t\t\t\tto;\n\t__u64\t\t\t\tflags;\n};\n\nstruct perf_branch_stack {\n\t__u64\t\t\t\tnr;\n\tstruct perf_branch_entry\tentries[0];\n};\n\nstruct task_struct;\n\n/**\n * struct hw_perf_event - performance event hardware details:\n */\nstruct hw_perf_event {\n#ifdef CONFIG_PERF_EVENTS\n\tunion {\n\t\tstruct { /* hardware */\n\t\t\tu64\t\tconfig;\n\t\t\tu64\t\tlast_tag;\n\t\t\tunsigned long\tconfig_base;\n\t\t\tunsigned long\tevent_base;\n\t\t\tint\t\tidx;\n\t\t\tint\t\tlast_cpu;\n\t\t\tunsigned int\textra_reg;\n\t\t\tu64\t\textra_config;\n\t\t\tint\t\textra_alloc;\n\t\t};\n\t\tstruct { /* software */\n\t\t\tstruct hrtimer\thrtimer;\n\t\t};\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\t\tstruct { /* breakpoint */\n\t\t\tstruct arch_hw_breakpoint\tinfo;\n\t\t\tstruct list_head\t\tbp_list;\n\t\t\t/*\n\t\t\t * Crufty hack to avoid the chicken and egg\n\t\t\t * problem hw_breakpoint has with context\n\t\t\t * creation and event initalization.\n\t\t\t */\n\t\t\tstruct task_struct\t\t*bp_target;\n\t\t};\n#endif\n\t};\n\tint\t\t\t\tstate;\n\tlocal64_t\t\t\tprev_count;\n\tu64\t\t\t\tsample_period;\n\tu64\t\t\t\tlast_period;\n\tlocal64_t\t\t\tperiod_left;\n\tu64\t\t\t\tinterrupts;\n\n\tu64\t\t\t\tfreq_time_stamp;\n\tu64\t\t\t\tfreq_count_stamp;\n#endif\n};\n\n/*\n * hw_perf_event::state flags\n */\n#define PERF_HES_STOPPED\t0x01 /* the counter is stopped */\n#define PERF_HES_UPTODATE\t0x02 /* event->count up-to-date */\n#define PERF_HES_ARCH\t\t0x04\n\nstruct perf_event;\n\n/*\n * Common implementation detail of pmu::{start,commit,cancel}_txn\n */\n#define PERF_EVENT_TXN 0x1\n\n/**\n * struct pmu - generic performance monitoring unit\n */\nstruct pmu {\n\tstruct list_head\t\tentry;\n\n\tstruct device\t\t\t*dev;\n\tchar\t\t\t\t*name;\n\tint\t\t\t\ttype;\n\n\tint * __percpu\t\t\tpmu_disable_count;\n\tstruct perf_cpu_context * __percpu pmu_cpu_context;\n\tint\t\t\t\ttask_ctx_nr;\n\n\t/*\n\t * Fully disable/enable this PMU, can be used to protect from the PMI\n\t * as well as for lazy/batch writing of the MSRs.\n\t */\n\tvoid (*pmu_enable)\t\t(struct pmu *pmu); /* optional */\n\tvoid (*pmu_disable)\t\t(struct pmu *pmu); /* optional */\n\n\t/*\n\t * Try and initialize the event for this PMU.\n\t * Should return -ENOENT when the @event doesn't match this PMU.\n\t */\n\tint (*event_init)\t\t(struct perf_event *event);\n\n#define PERF_EF_START\t0x01\t\t/* start the counter when adding    */\n#define PERF_EF_RELOAD\t0x02\t\t/* reload the counter when starting */\n#define PERF_EF_UPDATE\t0x04\t\t/* update the counter when stopping */\n\n\t/*\n\t * Adds/Removes a counter to/from the PMU, can be done inside\n\t * a transaction, see the ->*_txn() methods.\n\t */\n\tint  (*add)\t\t\t(struct perf_event *event, int flags);\n\tvoid (*del)\t\t\t(struct perf_event *event, int flags);\n\n\t/*\n\t * Starts/Stops a counter present on the PMU. The PMI handler\n\t * should stop the counter when perf_event_overflow() returns\n\t * !0. ->start() will be used to continue.\n\t */\n\tvoid (*start)\t\t\t(struct perf_event *event, int flags);\n\tvoid (*stop)\t\t\t(struct perf_event *event, int flags);\n\n\t/*\n\t * Updates the counter value of the event.\n\t */\n\tvoid (*read)\t\t\t(struct perf_event *event);\n\n\t/*\n\t * Group events scheduling is treated as a transaction, add\n\t * group events as a whole and perform one schedulability test.\n\t * If the test fails, roll back the whole group\n\t *\n\t * Start the transaction, after this ->add() doesn't need to\n\t * do schedulability tests.\n\t */\n\tvoid (*start_txn)\t\t(struct pmu *pmu); /* optional */\n\t/*\n\t * If ->start_txn() disabled the ->add() schedulability test\n\t * then ->commit_txn() is required to perform one. On success\n\t * the transaction is closed. On error the transaction is kept\n\t * open until ->cancel_txn() is called.\n\t */\n\tint  (*commit_txn)\t\t(struct pmu *pmu); /* optional */\n\t/*\n\t * Will cancel the transaction, assumes ->del() is called\n\t * for each successful ->add() during the transaction.\n\t */\n\tvoid (*cancel_txn)\t\t(struct pmu *pmu); /* optional */\n};\n\n/**\n * enum perf_event_active_state - the states of a event\n */\nenum perf_event_active_state {\n\tPERF_EVENT_STATE_ERROR\t\t= -2,\n\tPERF_EVENT_STATE_OFF\t\t= -1,\n\tPERF_EVENT_STATE_INACTIVE\t=  0,\n\tPERF_EVENT_STATE_ACTIVE\t\t=  1,\n};\n\nstruct file;\nstruct perf_sample_data;\n\ntypedef void (*perf_overflow_handler_t)(struct perf_event *, int,\n\t\t\t\t\tstruct perf_sample_data *,\n\t\t\t\t\tstruct pt_regs *regs);\n\nenum perf_group_flag {\n\tPERF_GROUP_SOFTWARE\t\t= 0x1,\n};\n\n#define SWEVENT_HLIST_BITS\t\t8\n#define SWEVENT_HLIST_SIZE\t\t(1 << SWEVENT_HLIST_BITS)\n\nstruct swevent_hlist {\n\tstruct hlist_head\t\theads[SWEVENT_HLIST_SIZE];\n\tstruct rcu_head\t\t\trcu_head;\n};\n\n#define PERF_ATTACH_CONTEXT\t0x01\n#define PERF_ATTACH_GROUP\t0x02\n#define PERF_ATTACH_TASK\t0x04\n\n#ifdef CONFIG_CGROUP_PERF\n/*\n * perf_cgroup_info keeps track of time_enabled for a cgroup.\n * This is a per-cpu dynamically allocated data structure.\n */\nstruct perf_cgroup_info {\n\tu64\t\t\t\ttime;\n\tu64\t\t\t\ttimestamp;\n};\n\nstruct perf_cgroup {\n\tstruct\t\t\t\tcgroup_subsys_state css;\n\tstruct\t\t\t\tperf_cgroup_info *info;\t/* timing info, one per cpu */\n};\n#endif\n\nstruct ring_buffer;\n\n/**\n * struct perf_event - performance event kernel representation:\n */\nstruct perf_event {\n#ifdef CONFIG_PERF_EVENTS\n\tstruct list_head\t\tgroup_entry;\n\tstruct list_head\t\tevent_entry;\n\tstruct list_head\t\tsibling_list;\n\tstruct hlist_node\t\thlist_entry;\n\tint\t\t\t\tnr_siblings;\n\tint\t\t\t\tgroup_flags;\n\tstruct perf_event\t\t*group_leader;\n\tstruct pmu\t\t\t*pmu;\n\n\tenum perf_event_active_state\tstate;\n\tunsigned int\t\t\tattach_state;\n\tlocal64_t\t\t\tcount;\n\tatomic64_t\t\t\tchild_count;\n\n\t/*\n\t * These are the total time in nanoseconds that the event\n\t * has been enabled (i.e. eligible to run, and the task has\n\t * been scheduled in, if this is a per-task event)\n\t * and running (scheduled onto the CPU), respectively.\n\t *\n\t * They are computed from tstamp_enabled, tstamp_running and\n\t * tstamp_stopped when the event is in INACTIVE or ACTIVE state.\n\t */\n\tu64\t\t\t\ttotal_time_enabled;\n\tu64\t\t\t\ttotal_time_running;\n\n\t/*\n\t * These are timestamps used for computing total_time_enabled\n\t * and total_time_running when the event is in INACTIVE or\n\t * ACTIVE state, measured in nanoseconds from an arbitrary point\n\t * in time.\n\t * tstamp_enabled: the notional time when the event was enabled\n\t * tstamp_running: the notional time when the event was scheduled on\n\t * tstamp_stopped: in INACTIVE state, the notional time when the\n\t *\tevent was scheduled off.\n\t */\n\tu64\t\t\t\ttstamp_enabled;\n\tu64\t\t\t\ttstamp_running;\n\tu64\t\t\t\ttstamp_stopped;\n\n\t/*\n\t * timestamp shadows the actual context timing but it can\n\t * be safely used in NMI interrupt context. It reflects the\n\t * context time as it was when the event was last scheduled in.\n\t *\n\t * ctx_time already accounts for ctx->timestamp. Therefore to\n\t * compute ctx_time for a sample, simply add perf_clock().\n\t */\n\tu64\t\t\t\tshadow_ctx_time;\n\n\tstruct perf_event_attr\t\tattr;\n\tu16\t\t\t\theader_size;\n\tu16\t\t\t\tid_header_size;\n\tu16\t\t\t\tread_size;\n\tstruct hw_perf_event\t\thw;\n\n\tstruct perf_event_context\t*ctx;\n\tstruct file\t\t\t*filp;\n\n\t/*\n\t * These accumulate total time (in nanoseconds) that children\n\t * events have been enabled and running, respectively.\n\t */\n\tatomic64_t\t\t\tchild_total_time_enabled;\n\tatomic64_t\t\t\tchild_total_time_running;\n\n\t/*\n\t * Protect attach/detach and child_list:\n\t */\n\tstruct mutex\t\t\tchild_mutex;\n\tstruct list_head\t\tchild_list;\n\tstruct perf_event\t\t*parent;\n\n\tint\t\t\t\toncpu;\n\tint\t\t\t\tcpu;\n\n\tstruct list_head\t\towner_entry;\n\tstruct task_struct\t\t*owner;\n\n\t/* mmap bits */\n\tstruct mutex\t\t\tmmap_mutex;\n\tatomic_t\t\t\tmmap_count;\n\tint\t\t\t\tmmap_locked;\n\tstruct user_struct\t\t*mmap_user;\n\tstruct ring_buffer\t\t*rb;\n\n\t/* poll related */\n\twait_queue_head_t\t\twaitq;\n\tstruct fasync_struct\t\t*fasync;\n\n\t/* delayed work for NMIs and such */\n\tint\t\t\t\tpending_wakeup;\n\tint\t\t\t\tpending_kill;\n\tint\t\t\t\tpending_disable;\n\tstruct irq_work\t\t\tpending;\n\n\tatomic_t\t\t\tevent_limit;\n\n\tvoid (*destroy)(struct perf_event *);\n\tstruct rcu_head\t\t\trcu_head;\n\n\tstruct pid_namespace\t\t*ns;\n\tu64\t\t\t\tid;\n\n\tperf_overflow_handler_t\t\toverflow_handler;\n\n#ifdef CONFIG_EVENT_TRACING\n\tstruct ftrace_event_call\t*tp_event;\n\tstruct event_filter\t\t*filter;\n#endif\n\n#ifdef CONFIG_CGROUP_PERF\n\tstruct perf_cgroup\t\t*cgrp; /* cgroup event is attach to */\n\tint\t\t\t\tcgrp_defer_enabled;\n#endif\n\n#endif /* CONFIG_PERF_EVENTS */\n};\n\nenum perf_event_context_type {\n\ttask_context,\n\tcpu_context,\n};\n\n/**\n * struct perf_event_context - event context structure\n *\n * Used as a container for task events and CPU events as well:\n */\nstruct perf_event_context {\n\tstruct pmu\t\t\t*pmu;\n\tenum perf_event_context_type\ttype;\n\t/*\n\t * Protect the states of the events in the list,\n\t * nr_active, and the list:\n\t */\n\traw_spinlock_t\t\t\tlock;\n\t/*\n\t * Protect the list of events.  Locking either mutex or lock\n\t * is sufficient to ensure the list doesn't change; to change\n\t * the list you need to lock both the mutex and the spinlock.\n\t */\n\tstruct mutex\t\t\tmutex;\n\n\tstruct list_head\t\tpinned_groups;\n\tstruct list_head\t\tflexible_groups;\n\tstruct list_head\t\tevent_list;\n\tint\t\t\t\tnr_events;\n\tint\t\t\t\tnr_active;\n\tint\t\t\t\tis_active;\n\tint\t\t\t\tnr_stat;\n\tint\t\t\t\trotate_disable;\n\tatomic_t\t\t\trefcount;\n\tstruct task_struct\t\t*task;\n\n\t/*\n\t * Context clock, runs when context enabled.\n\t */\n\tu64\t\t\t\ttime;\n\tu64\t\t\t\ttimestamp;\n\n\t/*\n\t * These fields let us detect when two contexts have both\n\t * been cloned (inherited) from a common ancestor.\n\t */\n\tstruct perf_event_context\t*parent_ctx;\n\tu64\t\t\t\tparent_gen;\n\tu64\t\t\t\tgeneration;\n\tint\t\t\t\tpin_count;\n\tint\t\t\t\tnr_cgroups; /* cgroup events present */\n\tstruct rcu_head\t\t\trcu_head;\n};\n\n/*\n * Number of contexts where an event can trigger:\n *\ttask, softirq, hardirq, nmi.\n */\n#define PERF_NR_CONTEXTS\t4\n\n/**\n * struct perf_event_cpu_context - per cpu event context structure\n */\nstruct perf_cpu_context {\n\tstruct perf_event_context\tctx;\n\tstruct perf_event_context\t*task_ctx;\n\tint\t\t\t\tactive_oncpu;\n\tint\t\t\t\texclusive;\n\tstruct list_head\t\trotation_list;\n\tint\t\t\t\tjiffies_interval;\n\tstruct pmu\t\t\t*active_pmu;\n\tstruct perf_cgroup\t\t*cgrp;\n};\n\nstruct perf_output_handle {\n\tstruct perf_event\t\t*event;\n\tstruct ring_buffer\t\t*rb;\n\tunsigned long\t\t\twakeup;\n\tunsigned long\t\t\tsize;\n\tvoid\t\t\t\t*addr;\n\tint\t\t\t\tpage;\n\tint\t\t\t\tnmi;\n\tint\t\t\t\tsample;\n};\n\n#ifdef CONFIG_PERF_EVENTS\n\nextern int perf_pmu_register(struct pmu *pmu, char *name, int type);\nextern void perf_pmu_unregister(struct pmu *pmu);\n\nextern int perf_num_counters(void);\nextern const char *perf_pmu_name(void);\nextern void __perf_event_task_sched_in(struct task_struct *task);\nextern void __perf_event_task_sched_out(struct task_struct *task, struct task_struct *next);\nextern int perf_event_init_task(struct task_struct *child);\nextern void perf_event_exit_task(struct task_struct *child);\nextern void perf_event_free_task(struct task_struct *task);\nextern void perf_event_delayed_put(struct task_struct *task);\nextern void perf_event_print_debug(void);\nextern void perf_pmu_disable(struct pmu *pmu);\nextern void perf_pmu_enable(struct pmu *pmu);\nextern int perf_event_task_disable(void);\nextern int perf_event_task_enable(void);\nextern void perf_event_update_userpage(struct perf_event *event);\nextern int perf_event_release_kernel(struct perf_event *event);\nextern struct perf_event *\nperf_event_create_kernel_counter(struct perf_event_attr *attr,\n\t\t\t\tint cpu,\n\t\t\t\tstruct task_struct *task,\n\t\t\t\tperf_overflow_handler_t callback);\nextern u64 perf_event_read_value(struct perf_event *event,\n\t\t\t\t u64 *enabled, u64 *running);\n\nstruct perf_sample_data {\n\tu64\t\t\t\ttype;\n\n\tu64\t\t\t\tip;\n\tstruct {\n\t\tu32\tpid;\n\t\tu32\ttid;\n\t}\t\t\t\ttid_entry;\n\tu64\t\t\t\ttime;\n\tu64\t\t\t\taddr;\n\tu64\t\t\t\tid;\n\tu64\t\t\t\tstream_id;\n\tstruct {\n\t\tu32\tcpu;\n\t\tu32\treserved;\n\t}\t\t\t\tcpu_entry;\n\tu64\t\t\t\tperiod;\n\tstruct perf_callchain_entry\t*callchain;\n\tstruct perf_raw_record\t\t*raw;\n};\n\nstatic inline void perf_sample_data_init(struct perf_sample_data *data, u64 addr)\n{\n\tdata->addr = addr;\n\tdata->raw  = NULL;\n}\n\nextern void perf_output_sample(struct perf_output_handle *handle,\n\t\t\t       struct perf_event_header *header,\n\t\t\t       struct perf_sample_data *data,\n\t\t\t       struct perf_event *event);\nextern void perf_prepare_sample(struct perf_event_header *header,\n\t\t\t\tstruct perf_sample_data *data,\n\t\t\t\tstruct perf_event *event,\n\t\t\t\tstruct pt_regs *regs);\n\nextern int perf_event_overflow(struct perf_event *event, int nmi,\n\t\t\t\t struct perf_sample_data *data,\n\t\t\t\t struct pt_regs *regs);\n\nstatic inline bool is_sampling_event(struct perf_event *event)\n{\n\treturn event->attr.sample_period != 0;\n}\n\n/*\n * Return 1 for a software event, 0 for a hardware event\n */\nstatic inline int is_software_event(struct perf_event *event)\n{\n\treturn event->pmu->task_ctx_nr == perf_sw_context;\n}\n\nextern struct jump_label_key perf_swevent_enabled[PERF_COUNT_SW_MAX];\n\nextern void __perf_sw_event(u32, u64, int, struct pt_regs *, u64);\n\n#ifndef perf_arch_fetch_caller_regs\nstatic inline void perf_arch_fetch_caller_regs(struct pt_regs *regs, unsigned long ip) { }\n#endif\n\n/*\n * Take a snapshot of the regs. Skip ip and frame pointer to\n * the nth caller. We only need a few of the regs:\n * - ip for PERF_SAMPLE_IP\n * - cs for user_mode() tests\n * - bp for callchains\n * - eflags, for future purposes, just in case\n */\nstatic inline void perf_fetch_caller_regs(struct pt_regs *regs)\n{\n\tmemset(regs, 0, sizeof(*regs));\n\n\tperf_arch_fetch_caller_regs(regs, CALLER_ADDR0);\n}\n\nstatic __always_inline void\nperf_sw_event(u32 event_id, u64 nr, int nmi, struct pt_regs *regs, u64 addr)\n{\n\tstruct pt_regs hot_regs;\n\n\tif (static_branch(&perf_swevent_enabled[event_id])) {\n\t\tif (!regs) {\n\t\t\tperf_fetch_caller_regs(&hot_regs);\n\t\t\tregs = &hot_regs;\n\t\t}\n\t\t__perf_sw_event(event_id, nr, nmi, regs, addr);\n\t}\n}\n\nextern struct jump_label_key perf_sched_events;\n\nstatic inline void perf_event_task_sched_in(struct task_struct *task)\n{\n\tif (static_branch(&perf_sched_events))\n\t\t__perf_event_task_sched_in(task);\n}\n\nstatic inline void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next)\n{\n\tperf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, 1, NULL, 0);\n\n\t__perf_event_task_sched_out(task, next);\n}\n\nextern void perf_event_mmap(struct vm_area_struct *vma);\nextern struct perf_guest_info_callbacks *perf_guest_cbs;\nextern int perf_register_guest_info_callbacks(struct perf_guest_info_callbacks *callbacks);\nextern int perf_unregister_guest_info_callbacks(struct perf_guest_info_callbacks *callbacks);\n\nextern void perf_event_comm(struct task_struct *tsk);\nextern void perf_event_fork(struct task_struct *tsk);\n\n/* Callchains */\nDECLARE_PER_CPU(struct perf_callchain_entry, perf_callchain_entry);\n\nextern void perf_callchain_user(struct perf_callchain_entry *entry, struct pt_regs *regs);\nextern void perf_callchain_kernel(struct perf_callchain_entry *entry, struct pt_regs *regs);\n\nstatic inline void perf_callchain_store(struct perf_callchain_entry *entry, u64 ip)\n{\n\tif (entry->nr < PERF_MAX_STACK_DEPTH)\n\t\tentry->ip[entry->nr++] = ip;\n}\n\nextern int sysctl_perf_event_paranoid;\nextern int sysctl_perf_event_mlock;\nextern int sysctl_perf_event_sample_rate;\n\nextern int perf_proc_update_handler(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *lenp,\n\t\tloff_t *ppos);\n\nstatic inline bool perf_paranoid_tracepoint_raw(void)\n{\n\treturn sysctl_perf_event_paranoid > -1;\n}\n\nstatic inline bool perf_paranoid_cpu(void)\n{\n\treturn sysctl_perf_event_paranoid > 0;\n}\n\nstatic inline bool perf_paranoid_kernel(void)\n{\n\treturn sysctl_perf_event_paranoid > 1;\n}\n\nextern void perf_event_init(void);\nextern void perf_tp_event(u64 addr, u64 count, void *record,\n\t\t\t  int entry_size, struct pt_regs *regs,\n\t\t\t  struct hlist_head *head, int rctx);\nextern void perf_bp_event(struct perf_event *event, void *data);\n\n#ifndef perf_misc_flags\n# define perf_misc_flags(regs) \\\n\t\t(user_mode(regs) ? PERF_RECORD_MISC_USER : PERF_RECORD_MISC_KERNEL)\n# define perf_instruction_pointer(regs)\tinstruction_pointer(regs)\n#endif\n\nextern int perf_output_begin(struct perf_output_handle *handle,\n\t\t\t     struct perf_event *event, unsigned int size,\n\t\t\t     int nmi, int sample);\nextern void perf_output_end(struct perf_output_handle *handle);\nextern void perf_output_copy(struct perf_output_handle *handle,\n\t\t\t     const void *buf, unsigned int len);\nextern int perf_swevent_get_recursion_context(void);\nextern void perf_swevent_put_recursion_context(int rctx);\nextern void perf_event_enable(struct perf_event *event);\nextern void perf_event_disable(struct perf_event *event);\nextern void perf_event_task_tick(void);\n#else\nstatic inline void\nperf_event_task_sched_in(struct task_struct *task)\t\t\t{ }\nstatic inline void\nperf_event_task_sched_out(struct task_struct *task,\n\t\t\t    struct task_struct *next)\t\t\t{ }\nstatic inline int perf_event_init_task(struct task_struct *child)\t{ return 0; }\nstatic inline void perf_event_exit_task(struct task_struct *child)\t{ }\nstatic inline void perf_event_free_task(struct task_struct *task)\t{ }\nstatic inline void perf_event_delayed_put(struct task_struct *task)\t{ }\nstatic inline void perf_event_print_debug(void)\t\t\t\t{ }\nstatic inline int perf_event_task_disable(void)\t\t\t\t{ return -EINVAL; }\nstatic inline int perf_event_task_enable(void)\t\t\t\t{ return -EINVAL; }\n\nstatic inline void\nperf_sw_event(u32 event_id, u64 nr, int nmi,\n\t\t     struct pt_regs *regs, u64 addr)\t\t\t{ }\nstatic inline void\nperf_bp_event(struct perf_event *event, void *data)\t\t\t{ }\n\nstatic inline int perf_register_guest_info_callbacks\n(struct perf_guest_info_callbacks *callbacks)\t\t\t\t{ return 0; }\nstatic inline int perf_unregister_guest_info_callbacks\n(struct perf_guest_info_callbacks *callbacks)\t\t\t\t{ return 0; }\n\nstatic inline void perf_event_mmap(struct vm_area_struct *vma)\t\t{ }\nstatic inline void perf_event_comm(struct task_struct *tsk)\t\t{ }\nstatic inline void perf_event_fork(struct task_struct *tsk)\t\t{ }\nstatic inline void perf_event_init(void)\t\t\t\t{ }\nstatic inline int  perf_swevent_get_recursion_context(void)\t\t{ return -1; }\nstatic inline void perf_swevent_put_recursion_context(int rctx)\t\t{ }\nstatic inline void perf_event_enable(struct perf_event *event)\t\t{ }\nstatic inline void perf_event_disable(struct perf_event *event)\t\t{ }\nstatic inline void perf_event_task_tick(void)\t\t\t\t{ }\n#endif\n\n#define perf_output_put(handle, x) perf_output_copy((handle), &(x), sizeof(x))\n\n/*\n * This has to have a higher priority than migration_notifier in sched.c.\n */\n#define perf_cpu_notifier(fn)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic struct notifier_block fn##_nb __cpuinitdata =\t\t\\\n\t\t{ .notifier_call = fn, .priority = CPU_PRI_PERF };\t\\\n\tfn(&fn##_nb, (unsigned long)CPU_UP_PREPARE,\t\t\t\\\n\t\t(void *)(unsigned long)smp_processor_id());\t\t\\\n\tfn(&fn##_nb, (unsigned long)CPU_STARTING,\t\t\t\\\n\t\t(void *)(unsigned long)smp_processor_id());\t\t\\\n\tfn(&fn##_nb, (unsigned long)CPU_ONLINE,\t\t\t\t\\\n\t\t(void *)(unsigned long)smp_processor_id());\t\t\\\n\tregister_cpu_notifier(&fn##_nb);\t\t\t\t\\\n} while (0)\n\n#endif /* __KERNEL__ */\n#endif /* _LINUX_PERF_EVENT_H */\n", "/*\n * Performance events core code:\n *\n *  Copyright (C) 2008 Thomas Gleixner <tglx@linutronix.de>\n *  Copyright (C) 2008-2011 Red Hat, Inc., Ingo Molnar\n *  Copyright (C) 2008-2011 Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>\n *  Copyright  \ufffd  2009 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>\n *\n * For licensing details see kernel-base/COPYING\n */\n\n#include <linux/fs.h>\n#include <linux/mm.h>\n#include <linux/cpu.h>\n#include <linux/smp.h>\n#include <linux/idr.h>\n#include <linux/file.h>\n#include <linux/poll.h>\n#include <linux/slab.h>\n#include <linux/hash.h>\n#include <linux/sysfs.h>\n#include <linux/dcache.h>\n#include <linux/percpu.h>\n#include <linux/ptrace.h>\n#include <linux/reboot.h>\n#include <linux/vmstat.h>\n#include <linux/device.h>\n#include <linux/vmalloc.h>\n#include <linux/hardirq.h>\n#include <linux/rculist.h>\n#include <linux/uaccess.h>\n#include <linux/syscalls.h>\n#include <linux/anon_inodes.h>\n#include <linux/kernel_stat.h>\n#include <linux/perf_event.h>\n#include <linux/ftrace_event.h>\n#include <linux/hw_breakpoint.h>\n\n#include \"internal.h\"\n\n#include <asm/irq_regs.h>\n\nstruct remote_function_call {\n\tstruct task_struct\t*p;\n\tint\t\t\t(*func)(void *info);\n\tvoid\t\t\t*info;\n\tint\t\t\tret;\n};\n\nstatic void remote_function(void *data)\n{\n\tstruct remote_function_call *tfc = data;\n\tstruct task_struct *p = tfc->p;\n\n\tif (p) {\n\t\ttfc->ret = -EAGAIN;\n\t\tif (task_cpu(p) != smp_processor_id() || !task_curr(p))\n\t\t\treturn;\n\t}\n\n\ttfc->ret = tfc->func(tfc->info);\n}\n\n/**\n * task_function_call - call a function on the cpu on which a task runs\n * @p:\t\tthe task to evaluate\n * @func:\tthe function to be called\n * @info:\tthe function call argument\n *\n * Calls the function @func when the task is currently running. This might\n * be on the current CPU, which just calls the function directly\n *\n * returns: @func return value, or\n *\t    -ESRCH  - when the process isn't running\n *\t    -EAGAIN - when the process moved away\n */\nstatic int\ntask_function_call(struct task_struct *p, int (*func) (void *info), void *info)\n{\n\tstruct remote_function_call data = {\n\t\t.p\t= p,\n\t\t.func\t= func,\n\t\t.info\t= info,\n\t\t.ret\t= -ESRCH, /* No such (running) process */\n\t};\n\n\tif (task_curr(p))\n\t\tsmp_call_function_single(task_cpu(p), remote_function, &data, 1);\n\n\treturn data.ret;\n}\n\n/**\n * cpu_function_call - call a function on the cpu\n * @func:\tthe function to be called\n * @info:\tthe function call argument\n *\n * Calls the function @func on the remote cpu.\n *\n * returns: @func return value or -ENXIO when the cpu is offline\n */\nstatic int cpu_function_call(int cpu, int (*func) (void *info), void *info)\n{\n\tstruct remote_function_call data = {\n\t\t.p\t= NULL,\n\t\t.func\t= func,\n\t\t.info\t= info,\n\t\t.ret\t= -ENXIO, /* No such CPU */\n\t};\n\n\tsmp_call_function_single(cpu, remote_function, &data, 1);\n\n\treturn data.ret;\n}\n\n#define PERF_FLAG_ALL (PERF_FLAG_FD_NO_GROUP |\\\n\t\t       PERF_FLAG_FD_OUTPUT  |\\\n\t\t       PERF_FLAG_PID_CGROUP)\n\nenum event_type_t {\n\tEVENT_FLEXIBLE = 0x1,\n\tEVENT_PINNED = 0x2,\n\tEVENT_ALL = EVENT_FLEXIBLE | EVENT_PINNED,\n};\n\n/*\n * perf_sched_events : >0 events exist\n * perf_cgroup_events: >0 per-cpu cgroup events exist on this cpu\n */\nstruct jump_label_key perf_sched_events __read_mostly;\nstatic DEFINE_PER_CPU(atomic_t, perf_cgroup_events);\n\nstatic atomic_t nr_mmap_events __read_mostly;\nstatic atomic_t nr_comm_events __read_mostly;\nstatic atomic_t nr_task_events __read_mostly;\n\nstatic LIST_HEAD(pmus);\nstatic DEFINE_MUTEX(pmus_lock);\nstatic struct srcu_struct pmus_srcu;\n\n/*\n * perf event paranoia level:\n *  -1 - not paranoid at all\n *   0 - disallow raw tracepoint access for unpriv\n *   1 - disallow cpu events for unpriv\n *   2 - disallow kernel profiling for unpriv\n */\nint sysctl_perf_event_paranoid __read_mostly = 1;\n\n/* Minimum for 512 kiB + 1 user control page */\nint sysctl_perf_event_mlock __read_mostly = 512 + (PAGE_SIZE / 1024); /* 'free' kiB per user */\n\n/*\n * max perf event sample rate\n */\n#define DEFAULT_MAX_SAMPLE_RATE 100000\nint sysctl_perf_event_sample_rate __read_mostly = DEFAULT_MAX_SAMPLE_RATE;\nstatic int max_samples_per_tick __read_mostly =\n\tDIV_ROUND_UP(DEFAULT_MAX_SAMPLE_RATE, HZ);\n\nint perf_proc_update_handler(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *lenp,\n\t\tloff_t *ppos)\n{\n\tint ret = proc_dointvec(table, write, buffer, lenp, ppos);\n\n\tif (ret || !write)\n\t\treturn ret;\n\n\tmax_samples_per_tick = DIV_ROUND_UP(sysctl_perf_event_sample_rate, HZ);\n\n\treturn 0;\n}\n\nstatic atomic64_t perf_event_id;\n\nstatic void cpu_ctx_sched_out(struct perf_cpu_context *cpuctx,\n\t\t\t      enum event_type_t event_type);\n\nstatic void cpu_ctx_sched_in(struct perf_cpu_context *cpuctx,\n\t\t\t     enum event_type_t event_type,\n\t\t\t     struct task_struct *task);\n\nstatic void update_context_time(struct perf_event_context *ctx);\nstatic u64 perf_event_time(struct perf_event *event);\n\nvoid __weak perf_event_print_debug(void)\t{ }\n\nextern __weak const char *perf_pmu_name(void)\n{\n\treturn \"pmu\";\n}\n\nstatic inline u64 perf_clock(void)\n{\n\treturn local_clock();\n}\n\nstatic inline struct perf_cpu_context *\n__get_cpu_context(struct perf_event_context *ctx)\n{\n\treturn this_cpu_ptr(ctx->pmu->pmu_cpu_context);\n}\n\nstatic void perf_ctx_lock(struct perf_cpu_context *cpuctx,\n\t\t\t  struct perf_event_context *ctx)\n{\n\traw_spin_lock(&cpuctx->ctx.lock);\n\tif (ctx)\n\t\traw_spin_lock(&ctx->lock);\n}\n\nstatic void perf_ctx_unlock(struct perf_cpu_context *cpuctx,\n\t\t\t    struct perf_event_context *ctx)\n{\n\tif (ctx)\n\t\traw_spin_unlock(&ctx->lock);\n\traw_spin_unlock(&cpuctx->ctx.lock);\n}\n\n#ifdef CONFIG_CGROUP_PERF\n\n/*\n * Must ensure cgroup is pinned (css_get) before calling\n * this function. In other words, we cannot call this function\n * if there is no cgroup event for the current CPU context.\n */\nstatic inline struct perf_cgroup *\nperf_cgroup_from_task(struct task_struct *task)\n{\n\treturn container_of(task_subsys_state(task, perf_subsys_id),\n\t\t\tstruct perf_cgroup, css);\n}\n\nstatic inline bool\nperf_cgroup_match(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_cpu_context *cpuctx = __get_cpu_context(ctx);\n\n\treturn !event->cgrp || event->cgrp == cpuctx->cgrp;\n}\n\nstatic inline void perf_get_cgroup(struct perf_event *event)\n{\n\tcss_get(&event->cgrp->css);\n}\n\nstatic inline void perf_put_cgroup(struct perf_event *event)\n{\n\tcss_put(&event->cgrp->css);\n}\n\nstatic inline void perf_detach_cgroup(struct perf_event *event)\n{\n\tperf_put_cgroup(event);\n\tevent->cgrp = NULL;\n}\n\nstatic inline int is_cgroup_event(struct perf_event *event)\n{\n\treturn event->cgrp != NULL;\n}\n\nstatic inline u64 perf_cgroup_event_time(struct perf_event *event)\n{\n\tstruct perf_cgroup_info *t;\n\n\tt = per_cpu_ptr(event->cgrp->info, event->cpu);\n\treturn t->time;\n}\n\nstatic inline void __update_cgrp_time(struct perf_cgroup *cgrp)\n{\n\tstruct perf_cgroup_info *info;\n\tu64 now;\n\n\tnow = perf_clock();\n\n\tinfo = this_cpu_ptr(cgrp->info);\n\n\tinfo->time += now - info->timestamp;\n\tinfo->timestamp = now;\n}\n\nstatic inline void update_cgrp_time_from_cpuctx(struct perf_cpu_context *cpuctx)\n{\n\tstruct perf_cgroup *cgrp_out = cpuctx->cgrp;\n\tif (cgrp_out)\n\t\t__update_cgrp_time(cgrp_out);\n}\n\nstatic inline void update_cgrp_time_from_event(struct perf_event *event)\n{\n\tstruct perf_cgroup *cgrp;\n\n\t/*\n\t * ensure we access cgroup data only when needed and\n\t * when we know the cgroup is pinned (css_get)\n\t */\n\tif (!is_cgroup_event(event))\n\t\treturn;\n\n\tcgrp = perf_cgroup_from_task(current);\n\t/*\n\t * Do not update time when cgroup is not active\n\t */\n\tif (cgrp == event->cgrp)\n\t\t__update_cgrp_time(event->cgrp);\n}\n\nstatic inline void\nperf_cgroup_set_timestamp(struct task_struct *task,\n\t\t\t  struct perf_event_context *ctx)\n{\n\tstruct perf_cgroup *cgrp;\n\tstruct perf_cgroup_info *info;\n\n\t/*\n\t * ctx->lock held by caller\n\t * ensure we do not access cgroup data\n\t * unless we have the cgroup pinned (css_get)\n\t */\n\tif (!task || !ctx->nr_cgroups)\n\t\treturn;\n\n\tcgrp = perf_cgroup_from_task(task);\n\tinfo = this_cpu_ptr(cgrp->info);\n\tinfo->timestamp = ctx->timestamp;\n}\n\n#define PERF_CGROUP_SWOUT\t0x1 /* cgroup switch out every event */\n#define PERF_CGROUP_SWIN\t0x2 /* cgroup switch in events based on task */\n\n/*\n * reschedule events based on the cgroup constraint of task.\n *\n * mode SWOUT : schedule out everything\n * mode SWIN : schedule in based on cgroup for next\n */\nvoid perf_cgroup_switch(struct task_struct *task, int mode)\n{\n\tstruct perf_cpu_context *cpuctx;\n\tstruct pmu *pmu;\n\tunsigned long flags;\n\n\t/*\n\t * disable interrupts to avoid geting nr_cgroup\n\t * changes via __perf_event_disable(). Also\n\t * avoids preemption.\n\t */\n\tlocal_irq_save(flags);\n\n\t/*\n\t * we reschedule only in the presence of cgroup\n\t * constrained events.\n\t */\n\trcu_read_lock();\n\n\tlist_for_each_entry_rcu(pmu, &pmus, entry) {\n\t\tcpuctx = this_cpu_ptr(pmu->pmu_cpu_context);\n\n\t\t/*\n\t\t * perf_cgroup_events says at least one\n\t\t * context on this CPU has cgroup events.\n\t\t *\n\t\t * ctx->nr_cgroups reports the number of cgroup\n\t\t * events for a context.\n\t\t */\n\t\tif (cpuctx->ctx.nr_cgroups > 0) {\n\t\t\tperf_ctx_lock(cpuctx, cpuctx->task_ctx);\n\t\t\tperf_pmu_disable(cpuctx->ctx.pmu);\n\n\t\t\tif (mode & PERF_CGROUP_SWOUT) {\n\t\t\t\tcpu_ctx_sched_out(cpuctx, EVENT_ALL);\n\t\t\t\t/*\n\t\t\t\t * must not be done before ctxswout due\n\t\t\t\t * to event_filter_match() in event_sched_out()\n\t\t\t\t */\n\t\t\t\tcpuctx->cgrp = NULL;\n\t\t\t}\n\n\t\t\tif (mode & PERF_CGROUP_SWIN) {\n\t\t\t\tWARN_ON_ONCE(cpuctx->cgrp);\n\t\t\t\t/* set cgrp before ctxsw in to\n\t\t\t\t * allow event_filter_match() to not\n\t\t\t\t * have to pass task around\n\t\t\t\t */\n\t\t\t\tcpuctx->cgrp = perf_cgroup_from_task(task);\n\t\t\t\tcpu_ctx_sched_in(cpuctx, EVENT_ALL, task);\n\t\t\t}\n\t\t\tperf_pmu_enable(cpuctx->ctx.pmu);\n\t\t\tperf_ctx_unlock(cpuctx, cpuctx->task_ctx);\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\n\tlocal_irq_restore(flags);\n}\n\nstatic inline void perf_cgroup_sched_out(struct task_struct *task)\n{\n\tperf_cgroup_switch(task, PERF_CGROUP_SWOUT);\n}\n\nstatic inline void perf_cgroup_sched_in(struct task_struct *task)\n{\n\tperf_cgroup_switch(task, PERF_CGROUP_SWIN);\n}\n\nstatic inline int perf_cgroup_connect(int fd, struct perf_event *event,\n\t\t\t\t      struct perf_event_attr *attr,\n\t\t\t\t      struct perf_event *group_leader)\n{\n\tstruct perf_cgroup *cgrp;\n\tstruct cgroup_subsys_state *css;\n\tstruct file *file;\n\tint ret = 0, fput_needed;\n\n\tfile = fget_light(fd, &fput_needed);\n\tif (!file)\n\t\treturn -EBADF;\n\n\tcss = cgroup_css_from_dir(file, perf_subsys_id);\n\tif (IS_ERR(css)) {\n\t\tret = PTR_ERR(css);\n\t\tgoto out;\n\t}\n\n\tcgrp = container_of(css, struct perf_cgroup, css);\n\tevent->cgrp = cgrp;\n\n\t/* must be done before we fput() the file */\n\tperf_get_cgroup(event);\n\n\t/*\n\t * all events in a group must monitor\n\t * the same cgroup because a task belongs\n\t * to only one perf cgroup at a time\n\t */\n\tif (group_leader && group_leader->cgrp != cgrp) {\n\t\tperf_detach_cgroup(event);\n\t\tret = -EINVAL;\n\t}\nout:\n\tfput_light(file, fput_needed);\n\treturn ret;\n}\n\nstatic inline void\nperf_cgroup_set_shadow_time(struct perf_event *event, u64 now)\n{\n\tstruct perf_cgroup_info *t;\n\tt = per_cpu_ptr(event->cgrp->info, event->cpu);\n\tevent->shadow_ctx_time = now - t->timestamp;\n}\n\nstatic inline void\nperf_cgroup_defer_enabled(struct perf_event *event)\n{\n\t/*\n\t * when the current task's perf cgroup does not match\n\t * the event's, we need to remember to call the\n\t * perf_mark_enable() function the first time a task with\n\t * a matching perf cgroup is scheduled in.\n\t */\n\tif (is_cgroup_event(event) && !perf_cgroup_match(event))\n\t\tevent->cgrp_defer_enabled = 1;\n}\n\nstatic inline void\nperf_cgroup_mark_enabled(struct perf_event *event,\n\t\t\t struct perf_event_context *ctx)\n{\n\tstruct perf_event *sub;\n\tu64 tstamp = perf_event_time(event);\n\n\tif (!event->cgrp_defer_enabled)\n\t\treturn;\n\n\tevent->cgrp_defer_enabled = 0;\n\n\tevent->tstamp_enabled = tstamp - event->total_time_enabled;\n\tlist_for_each_entry(sub, &event->sibling_list, group_entry) {\n\t\tif (sub->state >= PERF_EVENT_STATE_INACTIVE) {\n\t\t\tsub->tstamp_enabled = tstamp - sub->total_time_enabled;\n\t\t\tsub->cgrp_defer_enabled = 0;\n\t\t}\n\t}\n}\n#else /* !CONFIG_CGROUP_PERF */\n\nstatic inline bool\nperf_cgroup_match(struct perf_event *event)\n{\n\treturn true;\n}\n\nstatic inline void perf_detach_cgroup(struct perf_event *event)\n{}\n\nstatic inline int is_cgroup_event(struct perf_event *event)\n{\n\treturn 0;\n}\n\nstatic inline u64 perf_cgroup_event_cgrp_time(struct perf_event *event)\n{\n\treturn 0;\n}\n\nstatic inline void update_cgrp_time_from_event(struct perf_event *event)\n{\n}\n\nstatic inline void update_cgrp_time_from_cpuctx(struct perf_cpu_context *cpuctx)\n{\n}\n\nstatic inline void perf_cgroup_sched_out(struct task_struct *task)\n{\n}\n\nstatic inline void perf_cgroup_sched_in(struct task_struct *task)\n{\n}\n\nstatic inline int perf_cgroup_connect(pid_t pid, struct perf_event *event,\n\t\t\t\t      struct perf_event_attr *attr,\n\t\t\t\t      struct perf_event *group_leader)\n{\n\treturn -EINVAL;\n}\n\nstatic inline void\nperf_cgroup_set_timestamp(struct task_struct *task,\n\t\t\t  struct perf_event_context *ctx)\n{\n}\n\nvoid\nperf_cgroup_switch(struct task_struct *task, struct task_struct *next)\n{\n}\n\nstatic inline void\nperf_cgroup_set_shadow_time(struct perf_event *event, u64 now)\n{\n}\n\nstatic inline u64 perf_cgroup_event_time(struct perf_event *event)\n{\n\treturn 0;\n}\n\nstatic inline void\nperf_cgroup_defer_enabled(struct perf_event *event)\n{\n}\n\nstatic inline void\nperf_cgroup_mark_enabled(struct perf_event *event,\n\t\t\t struct perf_event_context *ctx)\n{\n}\n#endif\n\nvoid perf_pmu_disable(struct pmu *pmu)\n{\n\tint *count = this_cpu_ptr(pmu->pmu_disable_count);\n\tif (!(*count)++)\n\t\tpmu->pmu_disable(pmu);\n}\n\nvoid perf_pmu_enable(struct pmu *pmu)\n{\n\tint *count = this_cpu_ptr(pmu->pmu_disable_count);\n\tif (!--(*count))\n\t\tpmu->pmu_enable(pmu);\n}\n\nstatic DEFINE_PER_CPU(struct list_head, rotation_list);\n\n/*\n * perf_pmu_rotate_start() and perf_rotate_context() are fully serialized\n * because they're strictly cpu affine and rotate_start is called with IRQs\n * disabled, while rotate_context is called from IRQ context.\n */\nstatic void perf_pmu_rotate_start(struct pmu *pmu)\n{\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(pmu->pmu_cpu_context);\n\tstruct list_head *head = &__get_cpu_var(rotation_list);\n\n\tWARN_ON(!irqs_disabled());\n\n\tif (list_empty(&cpuctx->rotation_list))\n\t\tlist_add(&cpuctx->rotation_list, head);\n}\n\nstatic void get_ctx(struct perf_event_context *ctx)\n{\n\tWARN_ON(!atomic_inc_not_zero(&ctx->refcount));\n}\n\nstatic void put_ctx(struct perf_event_context *ctx)\n{\n\tif (atomic_dec_and_test(&ctx->refcount)) {\n\t\tif (ctx->parent_ctx)\n\t\t\tput_ctx(ctx->parent_ctx);\n\t\tif (ctx->task)\n\t\t\tput_task_struct(ctx->task);\n\t\tkfree_rcu(ctx, rcu_head);\n\t}\n}\n\nstatic void unclone_ctx(struct perf_event_context *ctx)\n{\n\tif (ctx->parent_ctx) {\n\t\tput_ctx(ctx->parent_ctx);\n\t\tctx->parent_ctx = NULL;\n\t}\n}\n\nstatic u32 perf_event_pid(struct perf_event *event, struct task_struct *p)\n{\n\t/*\n\t * only top level events have the pid namespace they were created in\n\t */\n\tif (event->parent)\n\t\tevent = event->parent;\n\n\treturn task_tgid_nr_ns(p, event->ns);\n}\n\nstatic u32 perf_event_tid(struct perf_event *event, struct task_struct *p)\n{\n\t/*\n\t * only top level events have the pid namespace they were created in\n\t */\n\tif (event->parent)\n\t\tevent = event->parent;\n\n\treturn task_pid_nr_ns(p, event->ns);\n}\n\n/*\n * If we inherit events we want to return the parent event id\n * to userspace.\n */\nstatic u64 primary_event_id(struct perf_event *event)\n{\n\tu64 id = event->id;\n\n\tif (event->parent)\n\t\tid = event->parent->id;\n\n\treturn id;\n}\n\n/*\n * Get the perf_event_context for a task and lock it.\n * This has to cope with with the fact that until it is locked,\n * the context could get moved to another task.\n */\nstatic struct perf_event_context *\nperf_lock_task_context(struct task_struct *task, int ctxn, unsigned long *flags)\n{\n\tstruct perf_event_context *ctx;\n\n\trcu_read_lock();\nretry:\n\tctx = rcu_dereference(task->perf_event_ctxp[ctxn]);\n\tif (ctx) {\n\t\t/*\n\t\t * If this context is a clone of another, it might\n\t\t * get swapped for another underneath us by\n\t\t * perf_event_task_sched_out, though the\n\t\t * rcu_read_lock() protects us from any context\n\t\t * getting freed.  Lock the context and check if it\n\t\t * got swapped before we could get the lock, and retry\n\t\t * if so.  If we locked the right context, then it\n\t\t * can't get swapped on us any more.\n\t\t */\n\t\traw_spin_lock_irqsave(&ctx->lock, *flags);\n\t\tif (ctx != rcu_dereference(task->perf_event_ctxp[ctxn])) {\n\t\t\traw_spin_unlock_irqrestore(&ctx->lock, *flags);\n\t\t\tgoto retry;\n\t\t}\n\n\t\tif (!atomic_inc_not_zero(&ctx->refcount)) {\n\t\t\traw_spin_unlock_irqrestore(&ctx->lock, *flags);\n\t\t\tctx = NULL;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn ctx;\n}\n\n/*\n * Get the context for a task and increment its pin_count so it\n * can't get swapped to another task.  This also increments its\n * reference count so that the context can't get freed.\n */\nstatic struct perf_event_context *\nperf_pin_task_context(struct task_struct *task, int ctxn)\n{\n\tstruct perf_event_context *ctx;\n\tunsigned long flags;\n\n\tctx = perf_lock_task_context(task, ctxn, &flags);\n\tif (ctx) {\n\t\t++ctx->pin_count;\n\t\traw_spin_unlock_irqrestore(&ctx->lock, flags);\n\t}\n\treturn ctx;\n}\n\nstatic void perf_unpin_context(struct perf_event_context *ctx)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&ctx->lock, flags);\n\t--ctx->pin_count;\n\traw_spin_unlock_irqrestore(&ctx->lock, flags);\n}\n\n/*\n * Update the record of the current time in a context.\n */\nstatic void update_context_time(struct perf_event_context *ctx)\n{\n\tu64 now = perf_clock();\n\n\tctx->time += now - ctx->timestamp;\n\tctx->timestamp = now;\n}\n\nstatic u64 perf_event_time(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\n\tif (is_cgroup_event(event))\n\t\treturn perf_cgroup_event_time(event);\n\n\treturn ctx ? ctx->time : 0;\n}\n\n/*\n * Update the total_time_enabled and total_time_running fields for a event.\n * The caller of this function needs to hold the ctx->lock.\n */\nstatic void update_event_times(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tu64 run_end;\n\n\tif (event->state < PERF_EVENT_STATE_INACTIVE ||\n\t    event->group_leader->state < PERF_EVENT_STATE_INACTIVE)\n\t\treturn;\n\t/*\n\t * in cgroup mode, time_enabled represents\n\t * the time the event was enabled AND active\n\t * tasks were in the monitored cgroup. This is\n\t * independent of the activity of the context as\n\t * there may be a mix of cgroup and non-cgroup events.\n\t *\n\t * That is why we treat cgroup events differently\n\t * here.\n\t */\n\tif (is_cgroup_event(event))\n\t\trun_end = perf_event_time(event);\n\telse if (ctx->is_active)\n\t\trun_end = ctx->time;\n\telse\n\t\trun_end = event->tstamp_stopped;\n\n\tevent->total_time_enabled = run_end - event->tstamp_enabled;\n\n\tif (event->state == PERF_EVENT_STATE_INACTIVE)\n\t\trun_end = event->tstamp_stopped;\n\telse\n\t\trun_end = perf_event_time(event);\n\n\tevent->total_time_running = run_end - event->tstamp_running;\n\n}\n\n/*\n * Update total_time_enabled and total_time_running for all events in a group.\n */\nstatic void update_group_times(struct perf_event *leader)\n{\n\tstruct perf_event *event;\n\n\tupdate_event_times(leader);\n\tlist_for_each_entry(event, &leader->sibling_list, group_entry)\n\t\tupdate_event_times(event);\n}\n\nstatic struct list_head *\nctx_group_list(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tif (event->attr.pinned)\n\t\treturn &ctx->pinned_groups;\n\telse\n\t\treturn &ctx->flexible_groups;\n}\n\n/*\n * Add a event from the lists for its context.\n * Must be called with ctx->mutex and ctx->lock held.\n */\nstatic void\nlist_add_event(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tWARN_ON_ONCE(event->attach_state & PERF_ATTACH_CONTEXT);\n\tevent->attach_state |= PERF_ATTACH_CONTEXT;\n\n\t/*\n\t * If we're a stand alone event or group leader, we go to the context\n\t * list, group events are kept attached to the group so that\n\t * perf_group_detach can, at all times, locate all siblings.\n\t */\n\tif (event->group_leader == event) {\n\t\tstruct list_head *list;\n\n\t\tif (is_software_event(event))\n\t\t\tevent->group_flags |= PERF_GROUP_SOFTWARE;\n\n\t\tlist = ctx_group_list(event, ctx);\n\t\tlist_add_tail(&event->group_entry, list);\n\t}\n\n\tif (is_cgroup_event(event))\n\t\tctx->nr_cgroups++;\n\n\tlist_add_rcu(&event->event_entry, &ctx->event_list);\n\tif (!ctx->nr_events)\n\t\tperf_pmu_rotate_start(ctx->pmu);\n\tctx->nr_events++;\n\tif (event->attr.inherit_stat)\n\t\tctx->nr_stat++;\n}\n\n/*\n * Called at perf_event creation and when events are attached/detached from a\n * group.\n */\nstatic void perf_event__read_size(struct perf_event *event)\n{\n\tint entry = sizeof(u64); /* value */\n\tint size = 0;\n\tint nr = 1;\n\n\tif (event->attr.read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tsize += sizeof(u64);\n\n\tif (event->attr.read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tsize += sizeof(u64);\n\n\tif (event->attr.read_format & PERF_FORMAT_ID)\n\t\tentry += sizeof(u64);\n\n\tif (event->attr.read_format & PERF_FORMAT_GROUP) {\n\t\tnr += event->group_leader->nr_siblings;\n\t\tsize += sizeof(u64);\n\t}\n\n\tsize += entry * nr;\n\tevent->read_size = size;\n}\n\nstatic void perf_event__header_size(struct perf_event *event)\n{\n\tstruct perf_sample_data *data;\n\tu64 sample_type = event->attr.sample_type;\n\tu16 size = 0;\n\n\tperf_event__read_size(event);\n\n\tif (sample_type & PERF_SAMPLE_IP)\n\t\tsize += sizeof(data->ip);\n\n\tif (sample_type & PERF_SAMPLE_ADDR)\n\t\tsize += sizeof(data->addr);\n\n\tif (sample_type & PERF_SAMPLE_PERIOD)\n\t\tsize += sizeof(data->period);\n\n\tif (sample_type & PERF_SAMPLE_READ)\n\t\tsize += event->read_size;\n\n\tevent->header_size = size;\n}\n\nstatic void perf_event__id_header_size(struct perf_event *event)\n{\n\tstruct perf_sample_data *data;\n\tu64 sample_type = event->attr.sample_type;\n\tu16 size = 0;\n\n\tif (sample_type & PERF_SAMPLE_TID)\n\t\tsize += sizeof(data->tid_entry);\n\n\tif (sample_type & PERF_SAMPLE_TIME)\n\t\tsize += sizeof(data->time);\n\n\tif (sample_type & PERF_SAMPLE_ID)\n\t\tsize += sizeof(data->id);\n\n\tif (sample_type & PERF_SAMPLE_STREAM_ID)\n\t\tsize += sizeof(data->stream_id);\n\n\tif (sample_type & PERF_SAMPLE_CPU)\n\t\tsize += sizeof(data->cpu_entry);\n\n\tevent->id_header_size = size;\n}\n\nstatic void perf_group_attach(struct perf_event *event)\n{\n\tstruct perf_event *group_leader = event->group_leader, *pos;\n\n\t/*\n\t * We can have double attach due to group movement in perf_event_open.\n\t */\n\tif (event->attach_state & PERF_ATTACH_GROUP)\n\t\treturn;\n\n\tevent->attach_state |= PERF_ATTACH_GROUP;\n\n\tif (group_leader == event)\n\t\treturn;\n\n\tif (group_leader->group_flags & PERF_GROUP_SOFTWARE &&\n\t\t\t!is_software_event(event))\n\t\tgroup_leader->group_flags &= ~PERF_GROUP_SOFTWARE;\n\n\tlist_add_tail(&event->group_entry, &group_leader->sibling_list);\n\tgroup_leader->nr_siblings++;\n\n\tperf_event__header_size(group_leader);\n\n\tlist_for_each_entry(pos, &group_leader->sibling_list, group_entry)\n\t\tperf_event__header_size(pos);\n}\n\n/*\n * Remove a event from the lists for its context.\n * Must be called with ctx->mutex and ctx->lock held.\n */\nstatic void\nlist_del_event(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tstruct perf_cpu_context *cpuctx;\n\t/*\n\t * We can have double detach due to exit/hot-unplug + close.\n\t */\n\tif (!(event->attach_state & PERF_ATTACH_CONTEXT))\n\t\treturn;\n\n\tevent->attach_state &= ~PERF_ATTACH_CONTEXT;\n\n\tif (is_cgroup_event(event)) {\n\t\tctx->nr_cgroups--;\n\t\tcpuctx = __get_cpu_context(ctx);\n\t\t/*\n\t\t * if there are no more cgroup events\n\t\t * then cler cgrp to avoid stale pointer\n\t\t * in update_cgrp_time_from_cpuctx()\n\t\t */\n\t\tif (!ctx->nr_cgroups)\n\t\t\tcpuctx->cgrp = NULL;\n\t}\n\n\tctx->nr_events--;\n\tif (event->attr.inherit_stat)\n\t\tctx->nr_stat--;\n\n\tlist_del_rcu(&event->event_entry);\n\n\tif (event->group_leader == event)\n\t\tlist_del_init(&event->group_entry);\n\n\tupdate_group_times(event);\n\n\t/*\n\t * If event was in error state, then keep it\n\t * that way, otherwise bogus counts will be\n\t * returned on read(). The only way to get out\n\t * of error state is by explicit re-enabling\n\t * of the event\n\t */\n\tif (event->state > PERF_EVENT_STATE_OFF)\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n}\n\nstatic void perf_group_detach(struct perf_event *event)\n{\n\tstruct perf_event *sibling, *tmp;\n\tstruct list_head *list = NULL;\n\n\t/*\n\t * We can have double detach due to exit/hot-unplug + close.\n\t */\n\tif (!(event->attach_state & PERF_ATTACH_GROUP))\n\t\treturn;\n\n\tevent->attach_state &= ~PERF_ATTACH_GROUP;\n\n\t/*\n\t * If this is a sibling, remove it from its group.\n\t */\n\tif (event->group_leader != event) {\n\t\tlist_del_init(&event->group_entry);\n\t\tevent->group_leader->nr_siblings--;\n\t\tgoto out;\n\t}\n\n\tif (!list_empty(&event->group_entry))\n\t\tlist = &event->group_entry;\n\n\t/*\n\t * If this was a group event with sibling events then\n\t * upgrade the siblings to singleton events by adding them\n\t * to whatever list we are on.\n\t */\n\tlist_for_each_entry_safe(sibling, tmp, &event->sibling_list, group_entry) {\n\t\tif (list)\n\t\t\tlist_move_tail(&sibling->group_entry, list);\n\t\tsibling->group_leader = sibling;\n\n\t\t/* Inherit group flags from the previous leader */\n\t\tsibling->group_flags = event->group_flags;\n\t}\n\nout:\n\tperf_event__header_size(event->group_leader);\n\n\tlist_for_each_entry(tmp, &event->group_leader->sibling_list, group_entry)\n\t\tperf_event__header_size(tmp);\n}\n\nstatic inline int\nevent_filter_match(struct perf_event *event)\n{\n\treturn (event->cpu == -1 || event->cpu == smp_processor_id())\n\t    && perf_cgroup_match(event);\n}\n\nstatic void\nevent_sched_out(struct perf_event *event,\n\t\t  struct perf_cpu_context *cpuctx,\n\t\t  struct perf_event_context *ctx)\n{\n\tu64 tstamp = perf_event_time(event);\n\tu64 delta;\n\t/*\n\t * An event which could not be activated because of\n\t * filter mismatch still needs to have its timings\n\t * maintained, otherwise bogus information is return\n\t * via read() for time_enabled, time_running:\n\t */\n\tif (event->state == PERF_EVENT_STATE_INACTIVE\n\t    && !event_filter_match(event)) {\n\t\tdelta = tstamp - event->tstamp_stopped;\n\t\tevent->tstamp_running += delta;\n\t\tevent->tstamp_stopped = tstamp;\n\t}\n\n\tif (event->state != PERF_EVENT_STATE_ACTIVE)\n\t\treturn;\n\n\tevent->state = PERF_EVENT_STATE_INACTIVE;\n\tif (event->pending_disable) {\n\t\tevent->pending_disable = 0;\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\t}\n\tevent->tstamp_stopped = tstamp;\n\tevent->pmu->del(event, 0);\n\tevent->oncpu = -1;\n\n\tif (!is_software_event(event))\n\t\tcpuctx->active_oncpu--;\n\tctx->nr_active--;\n\tif (event->attr.exclusive || !cpuctx->active_oncpu)\n\t\tcpuctx->exclusive = 0;\n}\n\nstatic void\ngroup_sched_out(struct perf_event *group_event,\n\t\tstruct perf_cpu_context *cpuctx,\n\t\tstruct perf_event_context *ctx)\n{\n\tstruct perf_event *event;\n\tint state = group_event->state;\n\n\tevent_sched_out(group_event, cpuctx, ctx);\n\n\t/*\n\t * Schedule out siblings (if any):\n\t */\n\tlist_for_each_entry(event, &group_event->sibling_list, group_entry)\n\t\tevent_sched_out(event, cpuctx, ctx);\n\n\tif (state == PERF_EVENT_STATE_ACTIVE && group_event->attr.exclusive)\n\t\tcpuctx->exclusive = 0;\n}\n\n/*\n * Cross CPU call to remove a performance event\n *\n * We disable the event on the hardware level first. After that we\n * remove it from the context list.\n */\nstatic int __perf_remove_from_context(void *info)\n{\n\tstruct perf_event *event = info;\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_cpu_context *cpuctx = __get_cpu_context(ctx);\n\n\traw_spin_lock(&ctx->lock);\n\tevent_sched_out(event, cpuctx, ctx);\n\tlist_del_event(event, ctx);\n\tif (!ctx->nr_events && cpuctx->task_ctx == ctx) {\n\t\tctx->is_active = 0;\n\t\tcpuctx->task_ctx = NULL;\n\t}\n\traw_spin_unlock(&ctx->lock);\n\n\treturn 0;\n}\n\n\n/*\n * Remove the event from a task's (or a CPU's) list of events.\n *\n * CPU events are removed with a smp call. For task events we only\n * call when the task is on a CPU.\n *\n * If event->ctx is a cloned context, callers must make sure that\n * every task struct that event->ctx->task could possibly point to\n * remains valid.  This is OK when called from perf_release since\n * that only calls us on the top-level context, which can't be a clone.\n * When called from perf_event_exit_task, it's OK because the\n * context has been detached from its task.\n */\nstatic void perf_remove_from_context(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct task_struct *task = ctx->task;\n\n\tlockdep_assert_held(&ctx->mutex);\n\n\tif (!task) {\n\t\t/*\n\t\t * Per cpu events are removed via an smp call and\n\t\t * the removal is always successful.\n\t\t */\n\t\tcpu_function_call(event->cpu, __perf_remove_from_context, event);\n\t\treturn;\n\t}\n\nretry:\n\tif (!task_function_call(task, __perf_remove_from_context, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\t/*\n\t * If we failed to find a running task, but find the context active now\n\t * that we've acquired the ctx->lock, retry.\n\t */\n\tif (ctx->is_active) {\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * Since the task isn't running, its safe to remove the event, us\n\t * holding the ctx->lock ensures the task won't get scheduled in.\n\t */\n\tlist_del_event(event, ctx);\n\traw_spin_unlock_irq(&ctx->lock);\n}\n\n/*\n * Cross CPU call to disable a performance event\n */\nstatic int __perf_event_disable(void *info)\n{\n\tstruct perf_event *event = info;\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_cpu_context *cpuctx = __get_cpu_context(ctx);\n\n\t/*\n\t * If this is a per-task event, need to check whether this\n\t * event's task is the current task on this cpu.\n\t *\n\t * Can trigger due to concurrent perf_event_context_sched_out()\n\t * flipping contexts around.\n\t */\n\tif (ctx->task && cpuctx->task_ctx != ctx)\n\t\treturn -EINVAL;\n\n\traw_spin_lock(&ctx->lock);\n\n\t/*\n\t * If the event is on, turn it off.\n\t * If it is in error state, leave it in error state.\n\t */\n\tif (event->state >= PERF_EVENT_STATE_INACTIVE) {\n\t\tupdate_context_time(ctx);\n\t\tupdate_cgrp_time_from_event(event);\n\t\tupdate_group_times(event);\n\t\tif (event == event->group_leader)\n\t\t\tgroup_sched_out(event, cpuctx, ctx);\n\t\telse\n\t\t\tevent_sched_out(event, cpuctx, ctx);\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\t}\n\n\traw_spin_unlock(&ctx->lock);\n\n\treturn 0;\n}\n\n/*\n * Disable a event.\n *\n * If event->ctx is a cloned context, callers must make sure that\n * every task struct that event->ctx->task could possibly point to\n * remains valid.  This condition is satisifed when called through\n * perf_event_for_each_child or perf_event_for_each because they\n * hold the top-level event's child_mutex, so any descendant that\n * goes to exit will block in sync_child_event.\n * When called from perf_pending_event it's OK because event->ctx\n * is the current context on this CPU and preemption is disabled,\n * hence we can't get into perf_event_task_sched_out for this context.\n */\nvoid perf_event_disable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct task_struct *task = ctx->task;\n\n\tif (!task) {\n\t\t/*\n\t\t * Disable the event on the cpu that it's on\n\t\t */\n\t\tcpu_function_call(event->cpu, __perf_event_disable, event);\n\t\treturn;\n\t}\n\nretry:\n\tif (!task_function_call(task, __perf_event_disable, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\t/*\n\t * If the event is still active, we need to retry the cross-call.\n\t */\n\tif (event->state == PERF_EVENT_STATE_ACTIVE) {\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\t/*\n\t\t * Reload the task pointer, it might have been changed by\n\t\t * a concurrent perf_event_context_sched_out().\n\t\t */\n\t\ttask = ctx->task;\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * Since we have the lock this context can't be scheduled\n\t * in, so we can change the state safely.\n\t */\n\tif (event->state == PERF_EVENT_STATE_INACTIVE) {\n\t\tupdate_group_times(event);\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\t}\n\traw_spin_unlock_irq(&ctx->lock);\n}\n\nstatic void perf_set_shadow_time(struct perf_event *event,\n\t\t\t\t struct perf_event_context *ctx,\n\t\t\t\t u64 tstamp)\n{\n\t/*\n\t * use the correct time source for the time snapshot\n\t *\n\t * We could get by without this by leveraging the\n\t * fact that to get to this function, the caller\n\t * has most likely already called update_context_time()\n\t * and update_cgrp_time_xx() and thus both timestamp\n\t * are identical (or very close). Given that tstamp is,\n\t * already adjusted for cgroup, we could say that:\n\t *    tstamp - ctx->timestamp\n\t * is equivalent to\n\t *    tstamp - cgrp->timestamp.\n\t *\n\t * Then, in perf_output_read(), the calculation would\n\t * work with no changes because:\n\t * - event is guaranteed scheduled in\n\t * - no scheduled out in between\n\t * - thus the timestamp would be the same\n\t *\n\t * But this is a bit hairy.\n\t *\n\t * So instead, we have an explicit cgroup call to remain\n\t * within the time time source all along. We believe it\n\t * is cleaner and simpler to understand.\n\t */\n\tif (is_cgroup_event(event))\n\t\tperf_cgroup_set_shadow_time(event, tstamp);\n\telse\n\t\tevent->shadow_ctx_time = tstamp - ctx->timestamp;\n}\n\n#define MAX_INTERRUPTS (~0ULL)\n\nstatic void perf_log_throttle(struct perf_event *event, int enable);\n\nstatic int\nevent_sched_in(struct perf_event *event,\n\t\t struct perf_cpu_context *cpuctx,\n\t\t struct perf_event_context *ctx)\n{\n\tu64 tstamp = perf_event_time(event);\n\n\tif (event->state <= PERF_EVENT_STATE_OFF)\n\t\treturn 0;\n\n\tevent->state = PERF_EVENT_STATE_ACTIVE;\n\tevent->oncpu = smp_processor_id();\n\n\t/*\n\t * Unthrottle events, since we scheduled we might have missed several\n\t * ticks already, also for a heavily scheduling task there is little\n\t * guarantee it'll get a tick in a timely manner.\n\t */\n\tif (unlikely(event->hw.interrupts == MAX_INTERRUPTS)) {\n\t\tperf_log_throttle(event, 1);\n\t\tevent->hw.interrupts = 0;\n\t}\n\n\t/*\n\t * The new state must be visible before we turn it on in the hardware:\n\t */\n\tsmp_wmb();\n\n\tif (event->pmu->add(event, PERF_EF_START)) {\n\t\tevent->state = PERF_EVENT_STATE_INACTIVE;\n\t\tevent->oncpu = -1;\n\t\treturn -EAGAIN;\n\t}\n\n\tevent->tstamp_running += tstamp - event->tstamp_stopped;\n\n\tperf_set_shadow_time(event, ctx, tstamp);\n\n\tif (!is_software_event(event))\n\t\tcpuctx->active_oncpu++;\n\tctx->nr_active++;\n\n\tif (event->attr.exclusive)\n\t\tcpuctx->exclusive = 1;\n\n\treturn 0;\n}\n\nstatic int\ngroup_sched_in(struct perf_event *group_event,\n\t       struct perf_cpu_context *cpuctx,\n\t       struct perf_event_context *ctx)\n{\n\tstruct perf_event *event, *partial_group = NULL;\n\tstruct pmu *pmu = group_event->pmu;\n\tu64 now = ctx->time;\n\tbool simulate = false;\n\n\tif (group_event->state == PERF_EVENT_STATE_OFF)\n\t\treturn 0;\n\n\tpmu->start_txn(pmu);\n\n\tif (event_sched_in(group_event, cpuctx, ctx)) {\n\t\tpmu->cancel_txn(pmu);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * Schedule in siblings as one group (if any):\n\t */\n\tlist_for_each_entry(event, &group_event->sibling_list, group_entry) {\n\t\tif (event_sched_in(event, cpuctx, ctx)) {\n\t\t\tpartial_group = event;\n\t\t\tgoto group_error;\n\t\t}\n\t}\n\n\tif (!pmu->commit_txn(pmu))\n\t\treturn 0;\n\ngroup_error:\n\t/*\n\t * Groups can be scheduled in as one unit only, so undo any\n\t * partial group before returning:\n\t * The events up to the failed event are scheduled out normally,\n\t * tstamp_stopped will be updated.\n\t *\n\t * The failed events and the remaining siblings need to have\n\t * their timings updated as if they had gone thru event_sched_in()\n\t * and event_sched_out(). This is required to get consistent timings\n\t * across the group. This also takes care of the case where the group\n\t * could never be scheduled by ensuring tstamp_stopped is set to mark\n\t * the time the event was actually stopped, such that time delta\n\t * calculation in update_event_times() is correct.\n\t */\n\tlist_for_each_entry(event, &group_event->sibling_list, group_entry) {\n\t\tif (event == partial_group)\n\t\t\tsimulate = true;\n\n\t\tif (simulate) {\n\t\t\tevent->tstamp_running += now - event->tstamp_stopped;\n\t\t\tevent->tstamp_stopped = now;\n\t\t} else {\n\t\t\tevent_sched_out(event, cpuctx, ctx);\n\t\t}\n\t}\n\tevent_sched_out(group_event, cpuctx, ctx);\n\n\tpmu->cancel_txn(pmu);\n\n\treturn -EAGAIN;\n}\n\n/*\n * Work out whether we can put this event group on the CPU now.\n */\nstatic int group_can_go_on(struct perf_event *event,\n\t\t\t   struct perf_cpu_context *cpuctx,\n\t\t\t   int can_add_hw)\n{\n\t/*\n\t * Groups consisting entirely of software events can always go on.\n\t */\n\tif (event->group_flags & PERF_GROUP_SOFTWARE)\n\t\treturn 1;\n\t/*\n\t * If an exclusive group is already on, no other hardware\n\t * events can go on.\n\t */\n\tif (cpuctx->exclusive)\n\t\treturn 0;\n\t/*\n\t * If this group is exclusive and there are already\n\t * events on the CPU, it can't go on.\n\t */\n\tif (event->attr.exclusive && cpuctx->active_oncpu)\n\t\treturn 0;\n\t/*\n\t * Otherwise, try to add it if all previous groups were able\n\t * to go on.\n\t */\n\treturn can_add_hw;\n}\n\nstatic void add_event_to_ctx(struct perf_event *event,\n\t\t\t       struct perf_event_context *ctx)\n{\n\tu64 tstamp = perf_event_time(event);\n\n\tlist_add_event(event, ctx);\n\tperf_group_attach(event);\n\tevent->tstamp_enabled = tstamp;\n\tevent->tstamp_running = tstamp;\n\tevent->tstamp_stopped = tstamp;\n}\n\nstatic void task_ctx_sched_out(struct perf_event_context *ctx);\nstatic void\nctx_sched_in(struct perf_event_context *ctx,\n\t     struct perf_cpu_context *cpuctx,\n\t     enum event_type_t event_type,\n\t     struct task_struct *task);\n\nstatic void perf_event_sched_in(struct perf_cpu_context *cpuctx,\n\t\t\t\tstruct perf_event_context *ctx,\n\t\t\t\tstruct task_struct *task)\n{\n\tcpu_ctx_sched_in(cpuctx, EVENT_PINNED, task);\n\tif (ctx)\n\t\tctx_sched_in(ctx, cpuctx, EVENT_PINNED, task);\n\tcpu_ctx_sched_in(cpuctx, EVENT_FLEXIBLE, task);\n\tif (ctx)\n\t\tctx_sched_in(ctx, cpuctx, EVENT_FLEXIBLE, task);\n}\n\n/*\n * Cross CPU call to install and enable a performance event\n *\n * Must be called with ctx->mutex held\n */\nstatic int  __perf_install_in_context(void *info)\n{\n\tstruct perf_event *event = info;\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_cpu_context *cpuctx = __get_cpu_context(ctx);\n\tstruct perf_event_context *task_ctx = cpuctx->task_ctx;\n\tstruct task_struct *task = current;\n\n\tperf_ctx_lock(cpuctx, task_ctx);\n\tperf_pmu_disable(cpuctx->ctx.pmu);\n\n\t/*\n\t * If there was an active task_ctx schedule it out.\n\t */\n\tif (task_ctx)\n\t\ttask_ctx_sched_out(task_ctx);\n\n\t/*\n\t * If the context we're installing events in is not the\n\t * active task_ctx, flip them.\n\t */\n\tif (ctx->task && task_ctx != ctx) {\n\t\tif (task_ctx)\n\t\t\traw_spin_unlock(&task_ctx->lock);\n\t\traw_spin_lock(&ctx->lock);\n\t\ttask_ctx = ctx;\n\t}\n\n\tif (task_ctx) {\n\t\tcpuctx->task_ctx = task_ctx;\n\t\ttask = task_ctx->task;\n\t}\n\n\tcpu_ctx_sched_out(cpuctx, EVENT_ALL);\n\n\tupdate_context_time(ctx);\n\t/*\n\t * update cgrp time only if current cgrp\n\t * matches event->cgrp. Must be done before\n\t * calling add_event_to_ctx()\n\t */\n\tupdate_cgrp_time_from_event(event);\n\n\tadd_event_to_ctx(event, ctx);\n\n\t/*\n\t * Schedule everything back in\n\t */\n\tperf_event_sched_in(cpuctx, task_ctx, task);\n\n\tperf_pmu_enable(cpuctx->ctx.pmu);\n\tperf_ctx_unlock(cpuctx, task_ctx);\n\n\treturn 0;\n}\n\n/*\n * Attach a performance event to a context\n *\n * First we add the event to the list with the hardware enable bit\n * in event->hw_config cleared.\n *\n * If the event is attached to a task which is on a CPU we use a smp\n * call to enable it in the task context. The task might have been\n * scheduled away, but we check this in the smp call again.\n */\nstatic void\nperf_install_in_context(struct perf_event_context *ctx,\n\t\t\tstruct perf_event *event,\n\t\t\tint cpu)\n{\n\tstruct task_struct *task = ctx->task;\n\n\tlockdep_assert_held(&ctx->mutex);\n\n\tevent->ctx = ctx;\n\n\tif (!task) {\n\t\t/*\n\t\t * Per cpu events are installed via an smp call and\n\t\t * the install is always successful.\n\t\t */\n\t\tcpu_function_call(cpu, __perf_install_in_context, event);\n\t\treturn;\n\t}\n\nretry:\n\tif (!task_function_call(task, __perf_install_in_context, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\t/*\n\t * If we failed to find a running task, but find the context active now\n\t * that we've acquired the ctx->lock, retry.\n\t */\n\tif (ctx->is_active) {\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * Since the task isn't running, its safe to add the event, us holding\n\t * the ctx->lock ensures the task won't get scheduled in.\n\t */\n\tadd_event_to_ctx(event, ctx);\n\traw_spin_unlock_irq(&ctx->lock);\n}\n\n/*\n * Put a event into inactive state and update time fields.\n * Enabling the leader of a group effectively enables all\n * the group members that aren't explicitly disabled, so we\n * have to update their ->tstamp_enabled also.\n * Note: this works for group members as well as group leaders\n * since the non-leader members' sibling_lists will be empty.\n */\nstatic void __perf_event_mark_enabled(struct perf_event *event,\n\t\t\t\t\tstruct perf_event_context *ctx)\n{\n\tstruct perf_event *sub;\n\tu64 tstamp = perf_event_time(event);\n\n\tevent->state = PERF_EVENT_STATE_INACTIVE;\n\tevent->tstamp_enabled = tstamp - event->total_time_enabled;\n\tlist_for_each_entry(sub, &event->sibling_list, group_entry) {\n\t\tif (sub->state >= PERF_EVENT_STATE_INACTIVE)\n\t\t\tsub->tstamp_enabled = tstamp - sub->total_time_enabled;\n\t}\n}\n\n/*\n * Cross CPU call to enable a performance event\n */\nstatic int __perf_event_enable(void *info)\n{\n\tstruct perf_event *event = info;\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *leader = event->group_leader;\n\tstruct perf_cpu_context *cpuctx = __get_cpu_context(ctx);\n\tint err;\n\n\tif (WARN_ON_ONCE(!ctx->is_active))\n\t\treturn -EINVAL;\n\n\traw_spin_lock(&ctx->lock);\n\tupdate_context_time(ctx);\n\n\tif (event->state >= PERF_EVENT_STATE_INACTIVE)\n\t\tgoto unlock;\n\n\t/*\n\t * set current task's cgroup time reference point\n\t */\n\tperf_cgroup_set_timestamp(current, ctx);\n\n\t__perf_event_mark_enabled(event, ctx);\n\n\tif (!event_filter_match(event)) {\n\t\tif (is_cgroup_event(event))\n\t\t\tperf_cgroup_defer_enabled(event);\n\t\tgoto unlock;\n\t}\n\n\t/*\n\t * If the event is in a group and isn't the group leader,\n\t * then don't put it on unless the group is on.\n\t */\n\tif (leader != event && leader->state != PERF_EVENT_STATE_ACTIVE)\n\t\tgoto unlock;\n\n\tif (!group_can_go_on(event, cpuctx, 1)) {\n\t\terr = -EEXIST;\n\t} else {\n\t\tif (event == leader)\n\t\t\terr = group_sched_in(event, cpuctx, ctx);\n\t\telse\n\t\t\terr = event_sched_in(event, cpuctx, ctx);\n\t}\n\n\tif (err) {\n\t\t/*\n\t\t * If this event can't go on and it's part of a\n\t\t * group, then the whole group has to come off.\n\t\t */\n\t\tif (leader != event)\n\t\t\tgroup_sched_out(leader, cpuctx, ctx);\n\t\tif (leader->attr.pinned) {\n\t\t\tupdate_group_times(leader);\n\t\t\tleader->state = PERF_EVENT_STATE_ERROR;\n\t\t}\n\t}\n\nunlock:\n\traw_spin_unlock(&ctx->lock);\n\n\treturn 0;\n}\n\n/*\n * Enable a event.\n *\n * If event->ctx is a cloned context, callers must make sure that\n * every task struct that event->ctx->task could possibly point to\n * remains valid.  This condition is satisfied when called through\n * perf_event_for_each_child or perf_event_for_each as described\n * for perf_event_disable.\n */\nvoid perf_event_enable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct task_struct *task = ctx->task;\n\n\tif (!task) {\n\t\t/*\n\t\t * Enable the event on the cpu that it's on\n\t\t */\n\t\tcpu_function_call(event->cpu, __perf_event_enable, event);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irq(&ctx->lock);\n\tif (event->state >= PERF_EVENT_STATE_INACTIVE)\n\t\tgoto out;\n\n\t/*\n\t * If the event is in error state, clear that first.\n\t * That way, if we see the event in error state below, we\n\t * know that it has gone back into error state, as distinct\n\t * from the task having been scheduled away before the\n\t * cross-call arrived.\n\t */\n\tif (event->state == PERF_EVENT_STATE_ERROR)\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\nretry:\n\tif (!ctx->is_active) {\n\t\t__perf_event_mark_enabled(event, ctx);\n\t\tgoto out;\n\t}\n\n\traw_spin_unlock_irq(&ctx->lock);\n\n\tif (!task_function_call(task, __perf_event_enable, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\n\t/*\n\t * If the context is active and the event is still off,\n\t * we need to retry the cross-call.\n\t */\n\tif (ctx->is_active && event->state == PERF_EVENT_STATE_OFF) {\n\t\t/*\n\t\t * task could have been flipped by a concurrent\n\t\t * perf_event_context_sched_out()\n\t\t */\n\t\ttask = ctx->task;\n\t\tgoto retry;\n\t}\n\nout:\n\traw_spin_unlock_irq(&ctx->lock);\n}\n\nstatic int perf_event_refresh(struct perf_event *event, int refresh)\n{\n\t/*\n\t * not supported on inherited events\n\t */\n\tif (event->attr.inherit || !is_sampling_event(event))\n\t\treturn -EINVAL;\n\n\tatomic_add(refresh, &event->event_limit);\n\tperf_event_enable(event);\n\n\treturn 0;\n}\n\nstatic void ctx_sched_out(struct perf_event_context *ctx,\n\t\t\t  struct perf_cpu_context *cpuctx,\n\t\t\t  enum event_type_t event_type)\n{\n\tstruct perf_event *event;\n\tint is_active = ctx->is_active;\n\n\tctx->is_active &= ~event_type;\n\tif (likely(!ctx->nr_events))\n\t\treturn;\n\n\tupdate_context_time(ctx);\n\tupdate_cgrp_time_from_cpuctx(cpuctx);\n\tif (!ctx->nr_active)\n\t\treturn;\n\n\tperf_pmu_disable(ctx->pmu);\n\tif ((is_active & EVENT_PINNED) && (event_type & EVENT_PINNED)) {\n\t\tlist_for_each_entry(event, &ctx->pinned_groups, group_entry)\n\t\t\tgroup_sched_out(event, cpuctx, ctx);\n\t}\n\n\tif ((is_active & EVENT_FLEXIBLE) && (event_type & EVENT_FLEXIBLE)) {\n\t\tlist_for_each_entry(event, &ctx->flexible_groups, group_entry)\n\t\t\tgroup_sched_out(event, cpuctx, ctx);\n\t}\n\tperf_pmu_enable(ctx->pmu);\n}\n\n/*\n * Test whether two contexts are equivalent, i.e. whether they\n * have both been cloned from the same version of the same context\n * and they both have the same number of enabled events.\n * If the number of enabled events is the same, then the set\n * of enabled events should be the same, because these are both\n * inherited contexts, therefore we can't access individual events\n * in them directly with an fd; we can only enable/disable all\n * events via prctl, or enable/disable all events in a family\n * via ioctl, which will have the same effect on both contexts.\n */\nstatic int context_equiv(struct perf_event_context *ctx1,\n\t\t\t struct perf_event_context *ctx2)\n{\n\treturn ctx1->parent_ctx && ctx1->parent_ctx == ctx2->parent_ctx\n\t\t&& ctx1->parent_gen == ctx2->parent_gen\n\t\t&& !ctx1->pin_count && !ctx2->pin_count;\n}\n\nstatic void __perf_event_sync_stat(struct perf_event *event,\n\t\t\t\t     struct perf_event *next_event)\n{\n\tu64 value;\n\n\tif (!event->attr.inherit_stat)\n\t\treturn;\n\n\t/*\n\t * Update the event value, we cannot use perf_event_read()\n\t * because we're in the middle of a context switch and have IRQs\n\t * disabled, which upsets smp_call_function_single(), however\n\t * we know the event must be on the current CPU, therefore we\n\t * don't need to use it.\n\t */\n\tswitch (event->state) {\n\tcase PERF_EVENT_STATE_ACTIVE:\n\t\tevent->pmu->read(event);\n\t\t/* fall-through */\n\n\tcase PERF_EVENT_STATE_INACTIVE:\n\t\tupdate_event_times(event);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/*\n\t * In order to keep per-task stats reliable we need to flip the event\n\t * values when we flip the contexts.\n\t */\n\tvalue = local64_read(&next_event->count);\n\tvalue = local64_xchg(&event->count, value);\n\tlocal64_set(&next_event->count, value);\n\n\tswap(event->total_time_enabled, next_event->total_time_enabled);\n\tswap(event->total_time_running, next_event->total_time_running);\n\n\t/*\n\t * Since we swizzled the values, update the user visible data too.\n\t */\n\tperf_event_update_userpage(event);\n\tperf_event_update_userpage(next_event);\n}\n\n#define list_next_entry(pos, member) \\\n\tlist_entry(pos->member.next, typeof(*pos), member)\n\nstatic void perf_event_sync_stat(struct perf_event_context *ctx,\n\t\t\t\t   struct perf_event_context *next_ctx)\n{\n\tstruct perf_event *event, *next_event;\n\n\tif (!ctx->nr_stat)\n\t\treturn;\n\n\tupdate_context_time(ctx);\n\n\tevent = list_first_entry(&ctx->event_list,\n\t\t\t\t   struct perf_event, event_entry);\n\n\tnext_event = list_first_entry(&next_ctx->event_list,\n\t\t\t\t\tstruct perf_event, event_entry);\n\n\twhile (&event->event_entry != &ctx->event_list &&\n\t       &next_event->event_entry != &next_ctx->event_list) {\n\n\t\t__perf_event_sync_stat(event, next_event);\n\n\t\tevent = list_next_entry(event, event_entry);\n\t\tnext_event = list_next_entry(next_event, event_entry);\n\t}\n}\n\nstatic void perf_event_context_sched_out(struct task_struct *task, int ctxn,\n\t\t\t\t\t struct task_struct *next)\n{\n\tstruct perf_event_context *ctx = task->perf_event_ctxp[ctxn];\n\tstruct perf_event_context *next_ctx;\n\tstruct perf_event_context *parent;\n\tstruct perf_cpu_context *cpuctx;\n\tint do_switch = 1;\n\n\tif (likely(!ctx))\n\t\treturn;\n\n\tcpuctx = __get_cpu_context(ctx);\n\tif (!cpuctx->task_ctx)\n\t\treturn;\n\n\trcu_read_lock();\n\tparent = rcu_dereference(ctx->parent_ctx);\n\tnext_ctx = next->perf_event_ctxp[ctxn];\n\tif (parent && next_ctx &&\n\t    rcu_dereference(next_ctx->parent_ctx) == parent) {\n\t\t/*\n\t\t * Looks like the two contexts are clones, so we might be\n\t\t * able to optimize the context switch.  We lock both\n\t\t * contexts and check that they are clones under the\n\t\t * lock (including re-checking that neither has been\n\t\t * uncloned in the meantime).  It doesn't matter which\n\t\t * order we take the locks because no other cpu could\n\t\t * be trying to lock both of these tasks.\n\t\t */\n\t\traw_spin_lock(&ctx->lock);\n\t\traw_spin_lock_nested(&next_ctx->lock, SINGLE_DEPTH_NESTING);\n\t\tif (context_equiv(ctx, next_ctx)) {\n\t\t\t/*\n\t\t\t * XXX do we need a memory barrier of sorts\n\t\t\t * wrt to rcu_dereference() of perf_event_ctxp\n\t\t\t */\n\t\t\ttask->perf_event_ctxp[ctxn] = next_ctx;\n\t\t\tnext->perf_event_ctxp[ctxn] = ctx;\n\t\t\tctx->task = next;\n\t\t\tnext_ctx->task = task;\n\t\t\tdo_switch = 0;\n\n\t\t\tperf_event_sync_stat(ctx, next_ctx);\n\t\t}\n\t\traw_spin_unlock(&next_ctx->lock);\n\t\traw_spin_unlock(&ctx->lock);\n\t}\n\trcu_read_unlock();\n\n\tif (do_switch) {\n\t\traw_spin_lock(&ctx->lock);\n\t\tctx_sched_out(ctx, cpuctx, EVENT_ALL);\n\t\tcpuctx->task_ctx = NULL;\n\t\traw_spin_unlock(&ctx->lock);\n\t}\n}\n\n#define for_each_task_context_nr(ctxn)\t\t\t\t\t\\\n\tfor ((ctxn) = 0; (ctxn) < perf_nr_task_contexts; (ctxn)++)\n\n/*\n * Called from scheduler to remove the events of the current task,\n * with interrupts disabled.\n *\n * We stop each event and update the event value in event->count.\n *\n * This does not protect us against NMI, but disable()\n * sets the disabled bit in the control field of event _before_\n * accessing the event control register. If a NMI hits, then it will\n * not restart the event.\n */\nvoid __perf_event_task_sched_out(struct task_struct *task,\n\t\t\t\t struct task_struct *next)\n{\n\tint ctxn;\n\n\tfor_each_task_context_nr(ctxn)\n\t\tperf_event_context_sched_out(task, ctxn, next);\n\n\t/*\n\t * if cgroup events exist on this CPU, then we need\n\t * to check if we have to switch out PMU state.\n\t * cgroup event are system-wide mode only\n\t */\n\tif (atomic_read(&__get_cpu_var(perf_cgroup_events)))\n\t\tperf_cgroup_sched_out(task);\n}\n\nstatic void task_ctx_sched_out(struct perf_event_context *ctx)\n{\n\tstruct perf_cpu_context *cpuctx = __get_cpu_context(ctx);\n\n\tif (!cpuctx->task_ctx)\n\t\treturn;\n\n\tif (WARN_ON_ONCE(ctx != cpuctx->task_ctx))\n\t\treturn;\n\n\tctx_sched_out(ctx, cpuctx, EVENT_ALL);\n\tcpuctx->task_ctx = NULL;\n}\n\n/*\n * Called with IRQs disabled\n */\nstatic void cpu_ctx_sched_out(struct perf_cpu_context *cpuctx,\n\t\t\t      enum event_type_t event_type)\n{\n\tctx_sched_out(&cpuctx->ctx, cpuctx, event_type);\n}\n\nstatic void\nctx_pinned_sched_in(struct perf_event_context *ctx,\n\t\t    struct perf_cpu_context *cpuctx)\n{\n\tstruct perf_event *event;\n\n\tlist_for_each_entry(event, &ctx->pinned_groups, group_entry) {\n\t\tif (event->state <= PERF_EVENT_STATE_OFF)\n\t\t\tcontinue;\n\t\tif (!event_filter_match(event))\n\t\t\tcontinue;\n\n\t\t/* may need to reset tstamp_enabled */\n\t\tif (is_cgroup_event(event))\n\t\t\tperf_cgroup_mark_enabled(event, ctx);\n\n\t\tif (group_can_go_on(event, cpuctx, 1))\n\t\t\tgroup_sched_in(event, cpuctx, ctx);\n\n\t\t/*\n\t\t * If this pinned group hasn't been scheduled,\n\t\t * put it in error state.\n\t\t */\n\t\tif (event->state == PERF_EVENT_STATE_INACTIVE) {\n\t\t\tupdate_group_times(event);\n\t\t\tevent->state = PERF_EVENT_STATE_ERROR;\n\t\t}\n\t}\n}\n\nstatic void\nctx_flexible_sched_in(struct perf_event_context *ctx,\n\t\t      struct perf_cpu_context *cpuctx)\n{\n\tstruct perf_event *event;\n\tint can_add_hw = 1;\n\n\tlist_for_each_entry(event, &ctx->flexible_groups, group_entry) {\n\t\t/* Ignore events in OFF or ERROR state */\n\t\tif (event->state <= PERF_EVENT_STATE_OFF)\n\t\t\tcontinue;\n\t\t/*\n\t\t * Listen to the 'cpu' scheduling filter constraint\n\t\t * of events:\n\t\t */\n\t\tif (!event_filter_match(event))\n\t\t\tcontinue;\n\n\t\t/* may need to reset tstamp_enabled */\n\t\tif (is_cgroup_event(event))\n\t\t\tperf_cgroup_mark_enabled(event, ctx);\n\n\t\tif (group_can_go_on(event, cpuctx, can_add_hw)) {\n\t\t\tif (group_sched_in(event, cpuctx, ctx))\n\t\t\t\tcan_add_hw = 0;\n\t\t}\n\t}\n}\n\nstatic void\nctx_sched_in(struct perf_event_context *ctx,\n\t     struct perf_cpu_context *cpuctx,\n\t     enum event_type_t event_type,\n\t     struct task_struct *task)\n{\n\tu64 now;\n\tint is_active = ctx->is_active;\n\n\tctx->is_active |= event_type;\n\tif (likely(!ctx->nr_events))\n\t\treturn;\n\n\tnow = perf_clock();\n\tctx->timestamp = now;\n\tperf_cgroup_set_timestamp(task, ctx);\n\t/*\n\t * First go through the list and put on any pinned groups\n\t * in order to give them the best chance of going on.\n\t */\n\tif (!(is_active & EVENT_PINNED) && (event_type & EVENT_PINNED))\n\t\tctx_pinned_sched_in(ctx, cpuctx);\n\n\t/* Then walk through the lower prio flexible groups */\n\tif (!(is_active & EVENT_FLEXIBLE) && (event_type & EVENT_FLEXIBLE))\n\t\tctx_flexible_sched_in(ctx, cpuctx);\n}\n\nstatic void cpu_ctx_sched_in(struct perf_cpu_context *cpuctx,\n\t\t\t     enum event_type_t event_type,\n\t\t\t     struct task_struct *task)\n{\n\tstruct perf_event_context *ctx = &cpuctx->ctx;\n\n\tctx_sched_in(ctx, cpuctx, event_type, task);\n}\n\nstatic void perf_event_context_sched_in(struct perf_event_context *ctx,\n\t\t\t\t\tstruct task_struct *task)\n{\n\tstruct perf_cpu_context *cpuctx;\n\n\tcpuctx = __get_cpu_context(ctx);\n\tif (cpuctx->task_ctx == ctx)\n\t\treturn;\n\n\tperf_ctx_lock(cpuctx, ctx);\n\tperf_pmu_disable(ctx->pmu);\n\t/*\n\t * We want to keep the following priority order:\n\t * cpu pinned (that don't need to move), task pinned,\n\t * cpu flexible, task flexible.\n\t */\n\tcpu_ctx_sched_out(cpuctx, EVENT_FLEXIBLE);\n\n\tperf_event_sched_in(cpuctx, ctx, task);\n\n\tcpuctx->task_ctx = ctx;\n\n\tperf_pmu_enable(ctx->pmu);\n\tperf_ctx_unlock(cpuctx, ctx);\n\n\t/*\n\t * Since these rotations are per-cpu, we need to ensure the\n\t * cpu-context we got scheduled on is actually rotating.\n\t */\n\tperf_pmu_rotate_start(ctx->pmu);\n}\n\n/*\n * Called from scheduler to add the events of the current task\n * with interrupts disabled.\n *\n * We restore the event value and then enable it.\n *\n * This does not protect us against NMI, but enable()\n * sets the enabled bit in the control field of event _before_\n * accessing the event control register. If a NMI hits, then it will\n * keep the event running.\n */\nvoid __perf_event_task_sched_in(struct task_struct *task)\n{\n\tstruct perf_event_context *ctx;\n\tint ctxn;\n\n\tfor_each_task_context_nr(ctxn) {\n\t\tctx = task->perf_event_ctxp[ctxn];\n\t\tif (likely(!ctx))\n\t\t\tcontinue;\n\n\t\tperf_event_context_sched_in(ctx, task);\n\t}\n\t/*\n\t * if cgroup events exist on this CPU, then we need\n\t * to check if we have to switch in PMU state.\n\t * cgroup event are system-wide mode only\n\t */\n\tif (atomic_read(&__get_cpu_var(perf_cgroup_events)))\n\t\tperf_cgroup_sched_in(task);\n}\n\nstatic u64 perf_calculate_period(struct perf_event *event, u64 nsec, u64 count)\n{\n\tu64 frequency = event->attr.sample_freq;\n\tu64 sec = NSEC_PER_SEC;\n\tu64 divisor, dividend;\n\n\tint count_fls, nsec_fls, frequency_fls, sec_fls;\n\n\tcount_fls = fls64(count);\n\tnsec_fls = fls64(nsec);\n\tfrequency_fls = fls64(frequency);\n\tsec_fls = 30;\n\n\t/*\n\t * We got @count in @nsec, with a target of sample_freq HZ\n\t * the target period becomes:\n\t *\n\t *             @count * 10^9\n\t * period = -------------------\n\t *          @nsec * sample_freq\n\t *\n\t */\n\n\t/*\n\t * Reduce accuracy by one bit such that @a and @b converge\n\t * to a similar magnitude.\n\t */\n#define REDUCE_FLS(a, b)\t\t\\\ndo {\t\t\t\t\t\\\n\tif (a##_fls > b##_fls) {\t\\\n\t\ta >>= 1;\t\t\\\n\t\ta##_fls--;\t\t\\\n\t} else {\t\t\t\\\n\t\tb >>= 1;\t\t\\\n\t\tb##_fls--;\t\t\\\n\t}\t\t\t\t\\\n} while (0)\n\n\t/*\n\t * Reduce accuracy until either term fits in a u64, then proceed with\n\t * the other, so that finally we can do a u64/u64 division.\n\t */\n\twhile (count_fls + sec_fls > 64 && nsec_fls + frequency_fls > 64) {\n\t\tREDUCE_FLS(nsec, frequency);\n\t\tREDUCE_FLS(sec, count);\n\t}\n\n\tif (count_fls + sec_fls > 64) {\n\t\tdivisor = nsec * frequency;\n\n\t\twhile (count_fls + sec_fls > 64) {\n\t\t\tREDUCE_FLS(count, sec);\n\t\t\tdivisor >>= 1;\n\t\t}\n\n\t\tdividend = count * sec;\n\t} else {\n\t\tdividend = count * sec;\n\n\t\twhile (nsec_fls + frequency_fls > 64) {\n\t\t\tREDUCE_FLS(nsec, frequency);\n\t\t\tdividend >>= 1;\n\t\t}\n\n\t\tdivisor = nsec * frequency;\n\t}\n\n\tif (!divisor)\n\t\treturn dividend;\n\n\treturn div64_u64(dividend, divisor);\n}\n\nstatic void perf_adjust_period(struct perf_event *event, u64 nsec, u64 count)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\ts64 period, sample_period;\n\ts64 delta;\n\n\tperiod = perf_calculate_period(event, nsec, count);\n\n\tdelta = (s64)(period - hwc->sample_period);\n\tdelta = (delta + 7) / 8; /* low pass filter */\n\n\tsample_period = hwc->sample_period + delta;\n\n\tif (!sample_period)\n\t\tsample_period = 1;\n\n\thwc->sample_period = sample_period;\n\n\tif (local64_read(&hwc->period_left) > 8*sample_period) {\n\t\tevent->pmu->stop(event, PERF_EF_UPDATE);\n\t\tlocal64_set(&hwc->period_left, 0);\n\t\tevent->pmu->start(event, PERF_EF_RELOAD);\n\t}\n}\n\nstatic void perf_ctx_adjust_freq(struct perf_event_context *ctx, u64 period)\n{\n\tstruct perf_event *event;\n\tstruct hw_perf_event *hwc;\n\tu64 interrupts, now;\n\ts64 delta;\n\n\tlist_for_each_entry_rcu(event, &ctx->event_list, event_entry) {\n\t\tif (event->state != PERF_EVENT_STATE_ACTIVE)\n\t\t\tcontinue;\n\n\t\tif (!event_filter_match(event))\n\t\t\tcontinue;\n\n\t\thwc = &event->hw;\n\n\t\tinterrupts = hwc->interrupts;\n\t\thwc->interrupts = 0;\n\n\t\t/*\n\t\t * unthrottle events on the tick\n\t\t */\n\t\tif (interrupts == MAX_INTERRUPTS) {\n\t\t\tperf_log_throttle(event, 1);\n\t\t\tevent->pmu->start(event, 0);\n\t\t}\n\n\t\tif (!event->attr.freq || !event->attr.sample_freq)\n\t\t\tcontinue;\n\n\t\tevent->pmu->read(event);\n\t\tnow = local64_read(&event->count);\n\t\tdelta = now - hwc->freq_count_stamp;\n\t\thwc->freq_count_stamp = now;\n\n\t\tif (delta > 0)\n\t\t\tperf_adjust_period(event, period, delta);\n\t}\n}\n\n/*\n * Round-robin a context's events:\n */\nstatic void rotate_ctx(struct perf_event_context *ctx)\n{\n\t/*\n\t * Rotate the first entry last of non-pinned groups. Rotation might be\n\t * disabled by the inheritance code.\n\t */\n\tif (!ctx->rotate_disable)\n\t\tlist_rotate_left(&ctx->flexible_groups);\n}\n\n/*\n * perf_pmu_rotate_start() and perf_rotate_context() are fully serialized\n * because they're strictly cpu affine and rotate_start is called with IRQs\n * disabled, while rotate_context is called from IRQ context.\n */\nstatic void perf_rotate_context(struct perf_cpu_context *cpuctx)\n{\n\tu64 interval = (u64)cpuctx->jiffies_interval * TICK_NSEC;\n\tstruct perf_event_context *ctx = NULL;\n\tint rotate = 0, remove = 1;\n\n\tif (cpuctx->ctx.nr_events) {\n\t\tremove = 0;\n\t\tif (cpuctx->ctx.nr_events != cpuctx->ctx.nr_active)\n\t\t\trotate = 1;\n\t}\n\n\tctx = cpuctx->task_ctx;\n\tif (ctx && ctx->nr_events) {\n\t\tremove = 0;\n\t\tif (ctx->nr_events != ctx->nr_active)\n\t\t\trotate = 1;\n\t}\n\n\tperf_ctx_lock(cpuctx, cpuctx->task_ctx);\n\tperf_pmu_disable(cpuctx->ctx.pmu);\n\tperf_ctx_adjust_freq(&cpuctx->ctx, interval);\n\tif (ctx)\n\t\tperf_ctx_adjust_freq(ctx, interval);\n\n\tif (!rotate)\n\t\tgoto done;\n\n\tcpu_ctx_sched_out(cpuctx, EVENT_FLEXIBLE);\n\tif (ctx)\n\t\tctx_sched_out(ctx, cpuctx, EVENT_FLEXIBLE);\n\n\trotate_ctx(&cpuctx->ctx);\n\tif (ctx)\n\t\trotate_ctx(ctx);\n\n\tperf_event_sched_in(cpuctx, ctx, current);\n\ndone:\n\tif (remove)\n\t\tlist_del_init(&cpuctx->rotation_list);\n\n\tperf_pmu_enable(cpuctx->ctx.pmu);\n\tperf_ctx_unlock(cpuctx, cpuctx->task_ctx);\n}\n\nvoid perf_event_task_tick(void)\n{\n\tstruct list_head *head = &__get_cpu_var(rotation_list);\n\tstruct perf_cpu_context *cpuctx, *tmp;\n\n\tWARN_ON(!irqs_disabled());\n\n\tlist_for_each_entry_safe(cpuctx, tmp, head, rotation_list) {\n\t\tif (cpuctx->jiffies_interval == 1 ||\n\t\t\t\t!(jiffies % cpuctx->jiffies_interval))\n\t\t\tperf_rotate_context(cpuctx);\n\t}\n}\n\nstatic int event_enable_on_exec(struct perf_event *event,\n\t\t\t\tstruct perf_event_context *ctx)\n{\n\tif (!event->attr.enable_on_exec)\n\t\treturn 0;\n\n\tevent->attr.enable_on_exec = 0;\n\tif (event->state >= PERF_EVENT_STATE_INACTIVE)\n\t\treturn 0;\n\n\t__perf_event_mark_enabled(event, ctx);\n\n\treturn 1;\n}\n\n/*\n * Enable all of a task's events that have been marked enable-on-exec.\n * This expects task == current.\n */\nstatic void perf_event_enable_on_exec(struct perf_event_context *ctx)\n{\n\tstruct perf_event *event;\n\tunsigned long flags;\n\tint enabled = 0;\n\tint ret;\n\n\tlocal_irq_save(flags);\n\tif (!ctx || !ctx->nr_events)\n\t\tgoto out;\n\n\t/*\n\t * We must ctxsw out cgroup events to avoid conflict\n\t * when invoking perf_task_event_sched_in() later on\n\t * in this function. Otherwise we end up trying to\n\t * ctxswin cgroup events which are already scheduled\n\t * in.\n\t */\n\tperf_cgroup_sched_out(current);\n\n\traw_spin_lock(&ctx->lock);\n\ttask_ctx_sched_out(ctx);\n\n\tlist_for_each_entry(event, &ctx->pinned_groups, group_entry) {\n\t\tret = event_enable_on_exec(event, ctx);\n\t\tif (ret)\n\t\t\tenabled = 1;\n\t}\n\n\tlist_for_each_entry(event, &ctx->flexible_groups, group_entry) {\n\t\tret = event_enable_on_exec(event, ctx);\n\t\tif (ret)\n\t\t\tenabled = 1;\n\t}\n\n\t/*\n\t * Unclone this context if we enabled any event.\n\t */\n\tif (enabled)\n\t\tunclone_ctx(ctx);\n\n\traw_spin_unlock(&ctx->lock);\n\n\t/*\n\t * Also calls ctxswin for cgroup events, if any:\n\t */\n\tperf_event_context_sched_in(ctx, ctx->task);\nout:\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Cross CPU call to read the hardware event\n */\nstatic void __perf_event_read(void *info)\n{\n\tstruct perf_event *event = info;\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_cpu_context *cpuctx = __get_cpu_context(ctx);\n\n\t/*\n\t * If this is a task context, we need to check whether it is\n\t * the current task context of this cpu.  If not it has been\n\t * scheduled out before the smp call arrived.  In that case\n\t * event->count would have been updated to a recent sample\n\t * when the event was scheduled out.\n\t */\n\tif (ctx->task && cpuctx->task_ctx != ctx)\n\t\treturn;\n\n\traw_spin_lock(&ctx->lock);\n\tif (ctx->is_active) {\n\t\tupdate_context_time(ctx);\n\t\tupdate_cgrp_time_from_event(event);\n\t}\n\tupdate_event_times(event);\n\tif (event->state == PERF_EVENT_STATE_ACTIVE)\n\t\tevent->pmu->read(event);\n\traw_spin_unlock(&ctx->lock);\n}\n\nstatic inline u64 perf_event_count(struct perf_event *event)\n{\n\treturn local64_read(&event->count) + atomic64_read(&event->child_count);\n}\n\nstatic u64 perf_event_read(struct perf_event *event)\n{\n\t/*\n\t * If event is enabled and currently active on a CPU, update the\n\t * value in the event structure:\n\t */\n\tif (event->state == PERF_EVENT_STATE_ACTIVE) {\n\t\tsmp_call_function_single(event->oncpu,\n\t\t\t\t\t __perf_event_read, event, 1);\n\t} else if (event->state == PERF_EVENT_STATE_INACTIVE) {\n\t\tstruct perf_event_context *ctx = event->ctx;\n\t\tunsigned long flags;\n\n\t\traw_spin_lock_irqsave(&ctx->lock, flags);\n\t\t/*\n\t\t * may read while context is not active\n\t\t * (e.g., thread is blocked), in that case\n\t\t * we cannot update context time\n\t\t */\n\t\tif (ctx->is_active) {\n\t\t\tupdate_context_time(ctx);\n\t\t\tupdate_cgrp_time_from_event(event);\n\t\t}\n\t\tupdate_event_times(event);\n\t\traw_spin_unlock_irqrestore(&ctx->lock, flags);\n\t}\n\n\treturn perf_event_count(event);\n}\n\n/*\n * Callchain support\n */\n\nstruct callchain_cpus_entries {\n\tstruct rcu_head\t\t\trcu_head;\n\tstruct perf_callchain_entry\t*cpu_entries[0];\n};\n\nstatic DEFINE_PER_CPU(int, callchain_recursion[PERF_NR_CONTEXTS]);\nstatic atomic_t nr_callchain_events;\nstatic DEFINE_MUTEX(callchain_mutex);\nstruct callchain_cpus_entries *callchain_cpus_entries;\n\n\n__weak void perf_callchain_kernel(struct perf_callchain_entry *entry,\n\t\t\t\t  struct pt_regs *regs)\n{\n}\n\n__weak void perf_callchain_user(struct perf_callchain_entry *entry,\n\t\t\t\tstruct pt_regs *regs)\n{\n}\n\nstatic void release_callchain_buffers_rcu(struct rcu_head *head)\n{\n\tstruct callchain_cpus_entries *entries;\n\tint cpu;\n\n\tentries = container_of(head, struct callchain_cpus_entries, rcu_head);\n\n\tfor_each_possible_cpu(cpu)\n\t\tkfree(entries->cpu_entries[cpu]);\n\n\tkfree(entries);\n}\n\nstatic void release_callchain_buffers(void)\n{\n\tstruct callchain_cpus_entries *entries;\n\n\tentries = callchain_cpus_entries;\n\trcu_assign_pointer(callchain_cpus_entries, NULL);\n\tcall_rcu(&entries->rcu_head, release_callchain_buffers_rcu);\n}\n\nstatic int alloc_callchain_buffers(void)\n{\n\tint cpu;\n\tint size;\n\tstruct callchain_cpus_entries *entries;\n\n\t/*\n\t * We can't use the percpu allocation API for data that can be\n\t * accessed from NMI. Use a temporary manual per cpu allocation\n\t * until that gets sorted out.\n\t */\n\tsize = offsetof(struct callchain_cpus_entries, cpu_entries[nr_cpu_ids]);\n\n\tentries = kzalloc(size, GFP_KERNEL);\n\tif (!entries)\n\t\treturn -ENOMEM;\n\n\tsize = sizeof(struct perf_callchain_entry) * PERF_NR_CONTEXTS;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tentries->cpu_entries[cpu] = kmalloc_node(size, GFP_KERNEL,\n\t\t\t\t\t\t\t cpu_to_node(cpu));\n\t\tif (!entries->cpu_entries[cpu])\n\t\t\tgoto fail;\n\t}\n\n\trcu_assign_pointer(callchain_cpus_entries, entries);\n\n\treturn 0;\n\nfail:\n\tfor_each_possible_cpu(cpu)\n\t\tkfree(entries->cpu_entries[cpu]);\n\tkfree(entries);\n\n\treturn -ENOMEM;\n}\n\nstatic int get_callchain_buffers(void)\n{\n\tint err = 0;\n\tint count;\n\n\tmutex_lock(&callchain_mutex);\n\n\tcount = atomic_inc_return(&nr_callchain_events);\n\tif (WARN_ON_ONCE(count < 1)) {\n\t\terr = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\tif (count > 1) {\n\t\t/* If the allocation failed, give up */\n\t\tif (!callchain_cpus_entries)\n\t\t\terr = -ENOMEM;\n\t\tgoto exit;\n\t}\n\n\terr = alloc_callchain_buffers();\n\tif (err)\n\t\trelease_callchain_buffers();\nexit:\n\tmutex_unlock(&callchain_mutex);\n\n\treturn err;\n}\n\nstatic void put_callchain_buffers(void)\n{\n\tif (atomic_dec_and_mutex_lock(&nr_callchain_events, &callchain_mutex)) {\n\t\trelease_callchain_buffers();\n\t\tmutex_unlock(&callchain_mutex);\n\t}\n}\n\nstatic int get_recursion_context(int *recursion)\n{\n\tint rctx;\n\n\tif (in_nmi())\n\t\trctx = 3;\n\telse if (in_irq())\n\t\trctx = 2;\n\telse if (in_softirq())\n\t\trctx = 1;\n\telse\n\t\trctx = 0;\n\n\tif (recursion[rctx])\n\t\treturn -1;\n\n\trecursion[rctx]++;\n\tbarrier();\n\n\treturn rctx;\n}\n\nstatic inline void put_recursion_context(int *recursion, int rctx)\n{\n\tbarrier();\n\trecursion[rctx]--;\n}\n\nstatic struct perf_callchain_entry *get_callchain_entry(int *rctx)\n{\n\tint cpu;\n\tstruct callchain_cpus_entries *entries;\n\n\t*rctx = get_recursion_context(__get_cpu_var(callchain_recursion));\n\tif (*rctx == -1)\n\t\treturn NULL;\n\n\tentries = rcu_dereference(callchain_cpus_entries);\n\tif (!entries)\n\t\treturn NULL;\n\n\tcpu = smp_processor_id();\n\n\treturn &entries->cpu_entries[cpu][*rctx];\n}\n\nstatic void\nput_callchain_entry(int rctx)\n{\n\tput_recursion_context(__get_cpu_var(callchain_recursion), rctx);\n}\n\nstatic struct perf_callchain_entry *perf_callchain(struct pt_regs *regs)\n{\n\tint rctx;\n\tstruct perf_callchain_entry *entry;\n\n\n\tentry = get_callchain_entry(&rctx);\n\tif (rctx == -1)\n\t\treturn NULL;\n\n\tif (!entry)\n\t\tgoto exit_put;\n\n\tentry->nr = 0;\n\n\tif (!user_mode(regs)) {\n\t\tperf_callchain_store(entry, PERF_CONTEXT_KERNEL);\n\t\tperf_callchain_kernel(entry, regs);\n\t\tif (current->mm)\n\t\t\tregs = task_pt_regs(current);\n\t\telse\n\t\t\tregs = NULL;\n\t}\n\n\tif (regs) {\n\t\tperf_callchain_store(entry, PERF_CONTEXT_USER);\n\t\tperf_callchain_user(entry, regs);\n\t}\n\nexit_put:\n\tput_callchain_entry(rctx);\n\n\treturn entry;\n}\n\n/*\n * Initialize the perf_event context in a task_struct:\n */\nstatic void __perf_event_init_context(struct perf_event_context *ctx)\n{\n\traw_spin_lock_init(&ctx->lock);\n\tmutex_init(&ctx->mutex);\n\tINIT_LIST_HEAD(&ctx->pinned_groups);\n\tINIT_LIST_HEAD(&ctx->flexible_groups);\n\tINIT_LIST_HEAD(&ctx->event_list);\n\tatomic_set(&ctx->refcount, 1);\n}\n\nstatic struct perf_event_context *\nalloc_perf_context(struct pmu *pmu, struct task_struct *task)\n{\n\tstruct perf_event_context *ctx;\n\n\tctx = kzalloc(sizeof(struct perf_event_context), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn NULL;\n\n\t__perf_event_init_context(ctx);\n\tif (task) {\n\t\tctx->task = task;\n\t\tget_task_struct(task);\n\t}\n\tctx->pmu = pmu;\n\n\treturn ctx;\n}\n\nstatic struct task_struct *\nfind_lively_task_by_vpid(pid_t vpid)\n{\n\tstruct task_struct *task;\n\tint err;\n\n\trcu_read_lock();\n\tif (!vpid)\n\t\ttask = current;\n\telse\n\t\ttask = find_task_by_vpid(vpid);\n\tif (task)\n\t\tget_task_struct(task);\n\trcu_read_unlock();\n\n\tif (!task)\n\t\treturn ERR_PTR(-ESRCH);\n\n\t/* Reuse ptrace permission checks for now. */\n\terr = -EACCES;\n\tif (!ptrace_may_access(task, PTRACE_MODE_READ))\n\t\tgoto errout;\n\n\treturn task;\nerrout:\n\tput_task_struct(task);\n\treturn ERR_PTR(err);\n\n}\n\n/*\n * Returns a matching context with refcount and pincount.\n */\nstatic struct perf_event_context *\nfind_get_context(struct pmu *pmu, struct task_struct *task, int cpu)\n{\n\tstruct perf_event_context *ctx;\n\tstruct perf_cpu_context *cpuctx;\n\tunsigned long flags;\n\tint ctxn, err;\n\n\tif (!task) {\n\t\t/* Must be root to operate on a CPU event: */\n\t\tif (perf_paranoid_cpu() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn ERR_PTR(-EACCES);\n\n\t\t/*\n\t\t * We could be clever and allow to attach a event to an\n\t\t * offline CPU and activate it when the CPU comes up, but\n\t\t * that's for later.\n\t\t */\n\t\tif (!cpu_online(cpu))\n\t\t\treturn ERR_PTR(-ENODEV);\n\n\t\tcpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);\n\t\tctx = &cpuctx->ctx;\n\t\tget_ctx(ctx);\n\t\t++ctx->pin_count;\n\n\t\treturn ctx;\n\t}\n\n\terr = -EINVAL;\n\tctxn = pmu->task_ctx_nr;\n\tif (ctxn < 0)\n\t\tgoto errout;\n\nretry:\n\tctx = perf_lock_task_context(task, ctxn, &flags);\n\tif (ctx) {\n\t\tunclone_ctx(ctx);\n\t\t++ctx->pin_count;\n\t\traw_spin_unlock_irqrestore(&ctx->lock, flags);\n\t} else {\n\t\tctx = alloc_perf_context(pmu, task);\n\t\terr = -ENOMEM;\n\t\tif (!ctx)\n\t\t\tgoto errout;\n\n\t\terr = 0;\n\t\tmutex_lock(&task->perf_event_mutex);\n\t\t/*\n\t\t * If it has already passed perf_event_exit_task().\n\t\t * we must see PF_EXITING, it takes this mutex too.\n\t\t */\n\t\tif (task->flags & PF_EXITING)\n\t\t\terr = -ESRCH;\n\t\telse if (task->perf_event_ctxp[ctxn])\n\t\t\terr = -EAGAIN;\n\t\telse {\n\t\t\tget_ctx(ctx);\n\t\t\t++ctx->pin_count;\n\t\t\trcu_assign_pointer(task->perf_event_ctxp[ctxn], ctx);\n\t\t}\n\t\tmutex_unlock(&task->perf_event_mutex);\n\n\t\tif (unlikely(err)) {\n\t\t\tput_ctx(ctx);\n\n\t\t\tif (err == -EAGAIN)\n\t\t\t\tgoto retry;\n\t\t\tgoto errout;\n\t\t}\n\t}\n\n\treturn ctx;\n\nerrout:\n\treturn ERR_PTR(err);\n}\n\nstatic void perf_event_free_filter(struct perf_event *event);\n\nstatic void free_event_rcu(struct rcu_head *head)\n{\n\tstruct perf_event *event;\n\n\tevent = container_of(head, struct perf_event, rcu_head);\n\tif (event->ns)\n\t\tput_pid_ns(event->ns);\n\tperf_event_free_filter(event);\n\tkfree(event);\n}\n\nstatic void ring_buffer_put(struct ring_buffer *rb);\n\nstatic void free_event(struct perf_event *event)\n{\n\tirq_work_sync(&event->pending);\n\n\tif (!event->parent) {\n\t\tif (event->attach_state & PERF_ATTACH_TASK)\n\t\t\tjump_label_dec(&perf_sched_events);\n\t\tif (event->attr.mmap || event->attr.mmap_data)\n\t\t\tatomic_dec(&nr_mmap_events);\n\t\tif (event->attr.comm)\n\t\t\tatomic_dec(&nr_comm_events);\n\t\tif (event->attr.task)\n\t\t\tatomic_dec(&nr_task_events);\n\t\tif (event->attr.sample_type & PERF_SAMPLE_CALLCHAIN)\n\t\t\tput_callchain_buffers();\n\t\tif (is_cgroup_event(event)) {\n\t\t\tatomic_dec(&per_cpu(perf_cgroup_events, event->cpu));\n\t\t\tjump_label_dec(&perf_sched_events);\n\t\t}\n\t}\n\n\tif (event->rb) {\n\t\tring_buffer_put(event->rb);\n\t\tevent->rb = NULL;\n\t}\n\n\tif (is_cgroup_event(event))\n\t\tperf_detach_cgroup(event);\n\n\tif (event->destroy)\n\t\tevent->destroy(event);\n\n\tif (event->ctx)\n\t\tput_ctx(event->ctx);\n\n\tcall_rcu(&event->rcu_head, free_event_rcu);\n}\n\nint perf_event_release_kernel(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\t/*\n\t * There are two ways this annotation is useful:\n\t *\n\t *  1) there is a lock recursion from perf_event_exit_task\n\t *     see the comment there.\n\t *\n\t *  2) there is a lock-inversion with mmap_sem through\n\t *     perf_event_read_group(), which takes faults while\n\t *     holding ctx->mutex, however this is called after\n\t *     the last filedesc died, so there is no possibility\n\t *     to trigger the AB-BA case.\n\t */\n\tmutex_lock_nested(&ctx->mutex, SINGLE_DEPTH_NESTING);\n\traw_spin_lock_irq(&ctx->lock);\n\tperf_group_detach(event);\n\traw_spin_unlock_irq(&ctx->lock);\n\tperf_remove_from_context(event);\n\tmutex_unlock(&ctx->mutex);\n\n\tfree_event(event);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(perf_event_release_kernel);\n\n/*\n * Called when the last reference to the file is gone.\n */\nstatic int perf_release(struct inode *inode, struct file *file)\n{\n\tstruct perf_event *event = file->private_data;\n\tstruct task_struct *owner;\n\n\tfile->private_data = NULL;\n\n\trcu_read_lock();\n\towner = ACCESS_ONCE(event->owner);\n\t/*\n\t * Matches the smp_wmb() in perf_event_exit_task(). If we observe\n\t * !owner it means the list deletion is complete and we can indeed\n\t * free this event, otherwise we need to serialize on\n\t * owner->perf_event_mutex.\n\t */\n\tsmp_read_barrier_depends();\n\tif (owner) {\n\t\t/*\n\t\t * Since delayed_put_task_struct() also drops the last\n\t\t * task reference we can safely take a new reference\n\t\t * while holding the rcu_read_lock().\n\t\t */\n\t\tget_task_struct(owner);\n\t}\n\trcu_read_unlock();\n\n\tif (owner) {\n\t\tmutex_lock(&owner->perf_event_mutex);\n\t\t/*\n\t\t * We have to re-check the event->owner field, if it is cleared\n\t\t * we raced with perf_event_exit_task(), acquiring the mutex\n\t\t * ensured they're done, and we can proceed with freeing the\n\t\t * event.\n\t\t */\n\t\tif (event->owner)\n\t\t\tlist_del_init(&event->owner_entry);\n\t\tmutex_unlock(&owner->perf_event_mutex);\n\t\tput_task_struct(owner);\n\t}\n\n\treturn perf_event_release_kernel(event);\n}\n\nu64 perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *running)\n{\n\tstruct perf_event *child;\n\tu64 total = 0;\n\n\t*enabled = 0;\n\t*running = 0;\n\n\tmutex_lock(&event->child_mutex);\n\ttotal += perf_event_read(event);\n\t*enabled += event->total_time_enabled +\n\t\t\tatomic64_read(&event->child_total_time_enabled);\n\t*running += event->total_time_running +\n\t\t\tatomic64_read(&event->child_total_time_running);\n\n\tlist_for_each_entry(child, &event->child_list, child_list) {\n\t\ttotal += perf_event_read(child);\n\t\t*enabled += child->total_time_enabled;\n\t\t*running += child->total_time_running;\n\t}\n\tmutex_unlock(&event->child_mutex);\n\n\treturn total;\n}\nEXPORT_SYMBOL_GPL(perf_event_read_value);\n\nstatic int perf_event_read_group(struct perf_event *event,\n\t\t\t\t   u64 read_format, char __user *buf)\n{\n\tstruct perf_event *leader = event->group_leader, *sub;\n\tint n = 0, size = 0, ret = -EFAULT;\n\tstruct perf_event_context *ctx = leader->ctx;\n\tu64 values[5];\n\tu64 count, enabled, running;\n\n\tmutex_lock(&ctx->mutex);\n\tcount = perf_event_read_value(leader, &enabled, &running);\n\n\tvalues[n++] = 1 + leader->nr_siblings;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tvalues[n++] = enabled;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tvalues[n++] = running;\n\tvalues[n++] = count;\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(leader);\n\n\tsize = n * sizeof(u64);\n\n\tif (copy_to_user(buf, values, size))\n\t\tgoto unlock;\n\n\tret = size;\n\n\tlist_for_each_entry(sub, &leader->sibling_list, group_entry) {\n\t\tn = 0;\n\n\t\tvalues[n++] = perf_event_read_value(sub, &enabled, &running);\n\t\tif (read_format & PERF_FORMAT_ID)\n\t\t\tvalues[n++] = primary_event_id(sub);\n\n\t\tsize = n * sizeof(u64);\n\n\t\tif (copy_to_user(buf + ret, values, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tret += size;\n\t}\nunlock:\n\tmutex_unlock(&ctx->mutex);\n\n\treturn ret;\n}\n\nstatic int perf_event_read_one(struct perf_event *event,\n\t\t\t\t u64 read_format, char __user *buf)\n{\n\tu64 enabled, running;\n\tu64 values[4];\n\tint n = 0;\n\n\tvalues[n++] = perf_event_read_value(event, &enabled, &running);\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tvalues[n++] = enabled;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tvalues[n++] = running;\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(event);\n\n\tif (copy_to_user(buf, values, n * sizeof(u64)))\n\t\treturn -EFAULT;\n\n\treturn n * sizeof(u64);\n}\n\n/*\n * Read the performance event - simple non blocking version for now\n */\nstatic ssize_t\nperf_read_hw(struct perf_event *event, char __user *buf, size_t count)\n{\n\tu64 read_format = event->attr.read_format;\n\tint ret;\n\n\t/*\n\t * Return end-of-file for a read on a event that is in\n\t * error state (i.e. because it was pinned but it couldn't be\n\t * scheduled on to the CPU at some point).\n\t */\n\tif (event->state == PERF_EVENT_STATE_ERROR)\n\t\treturn 0;\n\n\tif (count < event->read_size)\n\t\treturn -ENOSPC;\n\n\tWARN_ON_ONCE(event->ctx->parent_ctx);\n\tif (read_format & PERF_FORMAT_GROUP)\n\t\tret = perf_event_read_group(event, read_format, buf);\n\telse\n\t\tret = perf_event_read_one(event, read_format, buf);\n\n\treturn ret;\n}\n\nstatic ssize_t\nperf_read(struct file *file, char __user *buf, size_t count, loff_t *ppos)\n{\n\tstruct perf_event *event = file->private_data;\n\n\treturn perf_read_hw(event, buf, count);\n}\n\nstatic unsigned int perf_poll(struct file *file, poll_table *wait)\n{\n\tstruct perf_event *event = file->private_data;\n\tstruct ring_buffer *rb;\n\tunsigned int events = POLL_HUP;\n\n\trcu_read_lock();\n\trb = rcu_dereference(event->rb);\n\tif (rb)\n\t\tevents = atomic_xchg(&rb->poll, 0);\n\trcu_read_unlock();\n\n\tpoll_wait(file, &event->waitq, wait);\n\n\treturn events;\n}\n\nstatic void perf_event_reset(struct perf_event *event)\n{\n\t(void)perf_event_read(event);\n\tlocal64_set(&event->count, 0);\n\tperf_event_update_userpage(event);\n}\n\n/*\n * Holding the top-level event's child_mutex means that any\n * descendant process that has inherited this event will block\n * in sync_child_event if it goes to exit, thus satisfying the\n * task existence requirements of perf_event_enable/disable.\n */\nstatic void perf_event_for_each_child(struct perf_event *event,\n\t\t\t\t\tvoid (*func)(struct perf_event *))\n{\n\tstruct perf_event *child;\n\n\tWARN_ON_ONCE(event->ctx->parent_ctx);\n\tmutex_lock(&event->child_mutex);\n\tfunc(event);\n\tlist_for_each_entry(child, &event->child_list, child_list)\n\t\tfunc(child);\n\tmutex_unlock(&event->child_mutex);\n}\n\nstatic void perf_event_for_each(struct perf_event *event,\n\t\t\t\t  void (*func)(struct perf_event *))\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *sibling;\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\tevent = event->group_leader;\n\n\tperf_event_for_each_child(event, func);\n\tfunc(event);\n\tlist_for_each_entry(sibling, &event->sibling_list, group_entry)\n\t\tperf_event_for_each_child(event, func);\n\tmutex_unlock(&ctx->mutex);\n}\n\nstatic int perf_event_period(struct perf_event *event, u64 __user *arg)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tint ret = 0;\n\tu64 value;\n\n\tif (!is_sampling_event(event))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&value, arg, sizeof(value)))\n\t\treturn -EFAULT;\n\n\tif (!value)\n\t\treturn -EINVAL;\n\n\traw_spin_lock_irq(&ctx->lock);\n\tif (event->attr.freq) {\n\t\tif (value > sysctl_perf_event_sample_rate) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tevent->attr.sample_freq = value;\n\t} else {\n\t\tevent->attr.sample_period = value;\n\t\tevent->hw.sample_period = value;\n\t}\nunlock:\n\traw_spin_unlock_irq(&ctx->lock);\n\n\treturn ret;\n}\n\nstatic const struct file_operations perf_fops;\n\nstatic struct perf_event *perf_fget_light(int fd, int *fput_needed)\n{\n\tstruct file *file;\n\n\tfile = fget_light(fd, fput_needed);\n\tif (!file)\n\t\treturn ERR_PTR(-EBADF);\n\n\tif (file->f_op != &perf_fops) {\n\t\tfput_light(file, *fput_needed);\n\t\t*fput_needed = 0;\n\t\treturn ERR_PTR(-EBADF);\n\t}\n\n\treturn file->private_data;\n}\n\nstatic int perf_event_set_output(struct perf_event *event,\n\t\t\t\t struct perf_event *output_event);\nstatic int perf_event_set_filter(struct perf_event *event, void __user *arg);\n\nstatic long perf_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct perf_event *event = file->private_data;\n\tvoid (*func)(struct perf_event *);\n\tu32 flags = arg;\n\n\tswitch (cmd) {\n\tcase PERF_EVENT_IOC_ENABLE:\n\t\tfunc = perf_event_enable;\n\t\tbreak;\n\tcase PERF_EVENT_IOC_DISABLE:\n\t\tfunc = perf_event_disable;\n\t\tbreak;\n\tcase PERF_EVENT_IOC_RESET:\n\t\tfunc = perf_event_reset;\n\t\tbreak;\n\n\tcase PERF_EVENT_IOC_REFRESH:\n\t\treturn perf_event_refresh(event, arg);\n\n\tcase PERF_EVENT_IOC_PERIOD:\n\t\treturn perf_event_period(event, (u64 __user *)arg);\n\n\tcase PERF_EVENT_IOC_SET_OUTPUT:\n\t{\n\t\tstruct perf_event *output_event = NULL;\n\t\tint fput_needed = 0;\n\t\tint ret;\n\n\t\tif (arg != -1) {\n\t\t\toutput_event = perf_fget_light(arg, &fput_needed);\n\t\t\tif (IS_ERR(output_event))\n\t\t\t\treturn PTR_ERR(output_event);\n\t\t}\n\n\t\tret = perf_event_set_output(event, output_event);\n\t\tif (output_event)\n\t\t\tfput_light(output_event->filp, fput_needed);\n\n\t\treturn ret;\n\t}\n\n\tcase PERF_EVENT_IOC_SET_FILTER:\n\t\treturn perf_event_set_filter(event, (void __user *)arg);\n\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (flags & PERF_IOC_FLAG_GROUP)\n\t\tperf_event_for_each(event, func);\n\telse\n\t\tperf_event_for_each_child(event, func);\n\n\treturn 0;\n}\n\nint perf_event_task_enable(void)\n{\n\tstruct perf_event *event;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_for_each_entry(event, &current->perf_event_list, owner_entry)\n\t\tperf_event_for_each_child(event, perf_event_enable);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\treturn 0;\n}\n\nint perf_event_task_disable(void)\n{\n\tstruct perf_event *event;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_for_each_entry(event, &current->perf_event_list, owner_entry)\n\t\tperf_event_for_each_child(event, perf_event_disable);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\treturn 0;\n}\n\n#ifndef PERF_EVENT_INDEX_OFFSET\n# define PERF_EVENT_INDEX_OFFSET 0\n#endif\n\nstatic int perf_event_index(struct perf_event *event)\n{\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn 0;\n\n\tif (event->state != PERF_EVENT_STATE_ACTIVE)\n\t\treturn 0;\n\n\treturn event->hw.idx + 1 - PERF_EVENT_INDEX_OFFSET;\n}\n\nstatic void calc_timer_values(struct perf_event *event,\n\t\t\t\tu64 *running,\n\t\t\t\tu64 *enabled)\n{\n\tu64 now, ctx_time;\n\n\tnow = perf_clock();\n\tctx_time = event->shadow_ctx_time + now;\n\t*enabled = ctx_time - event->tstamp_enabled;\n\t*running = ctx_time - event->tstamp_running;\n}\n\n/*\n * Callers need to ensure there can be no nesting of this function, otherwise\n * the seqlock logic goes bad. We can not serialize this because the arch\n * code calls this from NMI context.\n */\nvoid perf_event_update_userpage(struct perf_event *event)\n{\n\tstruct perf_event_mmap_page *userpg;\n\tstruct ring_buffer *rb;\n\tu64 enabled, running;\n\n\trcu_read_lock();\n\t/*\n\t * compute total_time_enabled, total_time_running\n\t * based on snapshot values taken when the event\n\t * was last scheduled in.\n\t *\n\t * we cannot simply called update_context_time()\n\t * because of locking issue as we can be called in\n\t * NMI context\n\t */\n\tcalc_timer_values(event, &enabled, &running);\n\trb = rcu_dereference(event->rb);\n\tif (!rb)\n\t\tgoto unlock;\n\n\tuserpg = rb->user_page;\n\n\t/*\n\t * Disable preemption so as to not let the corresponding user-space\n\t * spin too long if we get preempted.\n\t */\n\tpreempt_disable();\n\t++userpg->lock;\n\tbarrier();\n\tuserpg->index = perf_event_index(event);\n\tuserpg->offset = perf_event_count(event);\n\tif (event->state == PERF_EVENT_STATE_ACTIVE)\n\t\tuserpg->offset -= local64_read(&event->hw.prev_count);\n\n\tuserpg->time_enabled = enabled +\n\t\t\tatomic64_read(&event->child_total_time_enabled);\n\n\tuserpg->time_running = running +\n\t\t\tatomic64_read(&event->child_total_time_running);\n\n\tbarrier();\n\t++userpg->lock;\n\tpreempt_enable();\nunlock:\n\trcu_read_unlock();\n}\n\nstatic int perf_mmap_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct perf_event *event = vma->vm_file->private_data;\n\tstruct ring_buffer *rb;\n\tint ret = VM_FAULT_SIGBUS;\n\n\tif (vmf->flags & FAULT_FLAG_MKWRITE) {\n\t\tif (vmf->pgoff == 0)\n\t\t\tret = 0;\n\t\treturn ret;\n\t}\n\n\trcu_read_lock();\n\trb = rcu_dereference(event->rb);\n\tif (!rb)\n\t\tgoto unlock;\n\n\tif (vmf->pgoff && (vmf->flags & FAULT_FLAG_WRITE))\n\t\tgoto unlock;\n\n\tvmf->page = perf_mmap_to_page(rb, vmf->pgoff);\n\tif (!vmf->page)\n\t\tgoto unlock;\n\n\tget_page(vmf->page);\n\tvmf->page->mapping = vma->vm_file->f_mapping;\n\tvmf->page->index   = vmf->pgoff;\n\n\tret = 0;\nunlock:\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic void rb_free_rcu(struct rcu_head *rcu_head)\n{\n\tstruct ring_buffer *rb;\n\n\trb = container_of(rcu_head, struct ring_buffer, rcu_head);\n\trb_free(rb);\n}\n\nstatic struct ring_buffer *ring_buffer_get(struct perf_event *event)\n{\n\tstruct ring_buffer *rb;\n\n\trcu_read_lock();\n\trb = rcu_dereference(event->rb);\n\tif (rb) {\n\t\tif (!atomic_inc_not_zero(&rb->refcount))\n\t\t\trb = NULL;\n\t}\n\trcu_read_unlock();\n\n\treturn rb;\n}\n\nstatic void ring_buffer_put(struct ring_buffer *rb)\n{\n\tif (!atomic_dec_and_test(&rb->refcount))\n\t\treturn;\n\n\tcall_rcu(&rb->rcu_head, rb_free_rcu);\n}\n\nstatic void perf_mmap_open(struct vm_area_struct *vma)\n{\n\tstruct perf_event *event = vma->vm_file->private_data;\n\n\tatomic_inc(&event->mmap_count);\n}\n\nstatic void perf_mmap_close(struct vm_area_struct *vma)\n{\n\tstruct perf_event *event = vma->vm_file->private_data;\n\n\tif (atomic_dec_and_mutex_lock(&event->mmap_count, &event->mmap_mutex)) {\n\t\tunsigned long size = perf_data_size(event->rb);\n\t\tstruct user_struct *user = event->mmap_user;\n\t\tstruct ring_buffer *rb = event->rb;\n\n\t\tatomic_long_sub((size >> PAGE_SHIFT) + 1, &user->locked_vm);\n\t\tvma->vm_mm->locked_vm -= event->mmap_locked;\n\t\trcu_assign_pointer(event->rb, NULL);\n\t\tmutex_unlock(&event->mmap_mutex);\n\n\t\tring_buffer_put(rb);\n\t\tfree_uid(user);\n\t}\n}\n\nstatic const struct vm_operations_struct perf_mmap_vmops = {\n\t.open\t\t= perf_mmap_open,\n\t.close\t\t= perf_mmap_close,\n\t.fault\t\t= perf_mmap_fault,\n\t.page_mkwrite\t= perf_mmap_fault,\n};\n\nstatic int perf_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tstruct perf_event *event = file->private_data;\n\tunsigned long user_locked, user_lock_limit;\n\tstruct user_struct *user = current_user();\n\tunsigned long locked, lock_limit;\n\tstruct ring_buffer *rb;\n\tunsigned long vma_size;\n\tunsigned long nr_pages;\n\tlong user_extra, extra;\n\tint ret = 0, flags = 0;\n\n\t/*\n\t * Don't allow mmap() of inherited per-task counters. This would\n\t * create a performance issue due to all children writing to the\n\t * same rb.\n\t */\n\tif (event->cpu == -1 && event->attr.inherit)\n\t\treturn -EINVAL;\n\n\tif (!(vma->vm_flags & VM_SHARED))\n\t\treturn -EINVAL;\n\n\tvma_size = vma->vm_end - vma->vm_start;\n\tnr_pages = (vma_size / PAGE_SIZE) - 1;\n\n\t/*\n\t * If we have rb pages ensure they're a power-of-two number, so we\n\t * can do bitmasks instead of modulo.\n\t */\n\tif (nr_pages != 0 && !is_power_of_2(nr_pages))\n\t\treturn -EINVAL;\n\n\tif (vma_size != PAGE_SIZE * (1 + nr_pages))\n\t\treturn -EINVAL;\n\n\tif (vma->vm_pgoff != 0)\n\t\treturn -EINVAL;\n\n\tWARN_ON_ONCE(event->ctx->parent_ctx);\n\tmutex_lock(&event->mmap_mutex);\n\tif (event->rb) {\n\t\tif (event->rb->nr_pages == nr_pages)\n\t\t\tatomic_inc(&event->rb->refcount);\n\t\telse\n\t\t\tret = -EINVAL;\n\t\tgoto unlock;\n\t}\n\n\tuser_extra = nr_pages + 1;\n\tuser_lock_limit = sysctl_perf_event_mlock >> (PAGE_SHIFT - 10);\n\n\t/*\n\t * Increase the limit linearly with more CPUs:\n\t */\n\tuser_lock_limit *= num_online_cpus();\n\n\tuser_locked = atomic_long_read(&user->locked_vm) + user_extra;\n\n\textra = 0;\n\tif (user_locked > user_lock_limit)\n\t\textra = user_locked - user_lock_limit;\n\n\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\tlock_limit >>= PAGE_SHIFT;\n\tlocked = vma->vm_mm->locked_vm + extra;\n\n\tif ((locked > lock_limit) && perf_paranoid_tracepoint_raw() &&\n\t\t!capable(CAP_IPC_LOCK)) {\n\t\tret = -EPERM;\n\t\tgoto unlock;\n\t}\n\n\tWARN_ON(event->rb);\n\n\tif (vma->vm_flags & VM_WRITE)\n\t\tflags |= RING_BUFFER_WRITABLE;\n\n\trb = rb_alloc(nr_pages, \n\t\tevent->attr.watermark ? event->attr.wakeup_watermark : 0,\n\t\tevent->cpu, flags);\n\n\tif (!rb) {\n\t\tret = -ENOMEM;\n\t\tgoto unlock;\n\t}\n\trcu_assign_pointer(event->rb, rb);\n\n\tatomic_long_add(user_extra, &user->locked_vm);\n\tevent->mmap_locked = extra;\n\tevent->mmap_user = get_current_user();\n\tvma->vm_mm->locked_vm += event->mmap_locked;\n\nunlock:\n\tif (!ret)\n\t\tatomic_inc(&event->mmap_count);\n\tmutex_unlock(&event->mmap_mutex);\n\n\tvma->vm_flags |= VM_RESERVED;\n\tvma->vm_ops = &perf_mmap_vmops;\n\n\treturn ret;\n}\n\nstatic int perf_fasync(int fd, struct file *filp, int on)\n{\n\tstruct inode *inode = filp->f_path.dentry->d_inode;\n\tstruct perf_event *event = filp->private_data;\n\tint retval;\n\n\tmutex_lock(&inode->i_mutex);\n\tretval = fasync_helper(fd, filp, on, &event->fasync);\n\tmutex_unlock(&inode->i_mutex);\n\n\tif (retval < 0)\n\t\treturn retval;\n\n\treturn 0;\n}\n\nstatic const struct file_operations perf_fops = {\n\t.llseek\t\t\t= no_llseek,\n\t.release\t\t= perf_release,\n\t.read\t\t\t= perf_read,\n\t.poll\t\t\t= perf_poll,\n\t.unlocked_ioctl\t\t= perf_ioctl,\n\t.compat_ioctl\t\t= perf_ioctl,\n\t.mmap\t\t\t= perf_mmap,\n\t.fasync\t\t\t= perf_fasync,\n};\n\n/*\n * Perf event wakeup\n *\n * If there's data, ensure we set the poll() state and publish everything\n * to user-space before waking everybody up.\n */\n\nvoid perf_event_wakeup(struct perf_event *event)\n{\n\twake_up_all(&event->waitq);\n\n\tif (event->pending_kill) {\n\t\tkill_fasync(&event->fasync, SIGIO, event->pending_kill);\n\t\tevent->pending_kill = 0;\n\t}\n}\n\nstatic void perf_pending_event(struct irq_work *entry)\n{\n\tstruct perf_event *event = container_of(entry,\n\t\t\tstruct perf_event, pending);\n\n\tif (event->pending_disable) {\n\t\tevent->pending_disable = 0;\n\t\t__perf_event_disable(event);\n\t}\n\n\tif (event->pending_wakeup) {\n\t\tevent->pending_wakeup = 0;\n\t\tperf_event_wakeup(event);\n\t}\n}\n\n/*\n * We assume there is only KVM supporting the callbacks.\n * Later on, we might change it to a list if there is\n * another virtualization implementation supporting the callbacks.\n */\nstruct perf_guest_info_callbacks *perf_guest_cbs;\n\nint perf_register_guest_info_callbacks(struct perf_guest_info_callbacks *cbs)\n{\n\tperf_guest_cbs = cbs;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(perf_register_guest_info_callbacks);\n\nint perf_unregister_guest_info_callbacks(struct perf_guest_info_callbacks *cbs)\n{\n\tperf_guest_cbs = NULL;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(perf_unregister_guest_info_callbacks);\n\nstatic void __perf_event_header__init_id(struct perf_event_header *header,\n\t\t\t\t\t struct perf_sample_data *data,\n\t\t\t\t\t struct perf_event *event)\n{\n\tu64 sample_type = event->attr.sample_type;\n\n\tdata->type = sample_type;\n\theader->size += event->id_header_size;\n\n\tif (sample_type & PERF_SAMPLE_TID) {\n\t\t/* namespace issues */\n\t\tdata->tid_entry.pid = perf_event_pid(event, current);\n\t\tdata->tid_entry.tid = perf_event_tid(event, current);\n\t}\n\n\tif (sample_type & PERF_SAMPLE_TIME)\n\t\tdata->time = perf_clock();\n\n\tif (sample_type & PERF_SAMPLE_ID)\n\t\tdata->id = primary_event_id(event);\n\n\tif (sample_type & PERF_SAMPLE_STREAM_ID)\n\t\tdata->stream_id = event->id;\n\n\tif (sample_type & PERF_SAMPLE_CPU) {\n\t\tdata->cpu_entry.cpu\t = raw_smp_processor_id();\n\t\tdata->cpu_entry.reserved = 0;\n\t}\n}\n\nvoid perf_event_header__init_id(struct perf_event_header *header,\n\t\t\t\tstruct perf_sample_data *data,\n\t\t\t\tstruct perf_event *event)\n{\n\tif (event->attr.sample_id_all)\n\t\t__perf_event_header__init_id(header, data, event);\n}\n\nstatic void __perf_event__output_id_sample(struct perf_output_handle *handle,\n\t\t\t\t\t   struct perf_sample_data *data)\n{\n\tu64 sample_type = data->type;\n\n\tif (sample_type & PERF_SAMPLE_TID)\n\t\tperf_output_put(handle, data->tid_entry);\n\n\tif (sample_type & PERF_SAMPLE_TIME)\n\t\tperf_output_put(handle, data->time);\n\n\tif (sample_type & PERF_SAMPLE_ID)\n\t\tperf_output_put(handle, data->id);\n\n\tif (sample_type & PERF_SAMPLE_STREAM_ID)\n\t\tperf_output_put(handle, data->stream_id);\n\n\tif (sample_type & PERF_SAMPLE_CPU)\n\t\tperf_output_put(handle, data->cpu_entry);\n}\n\nvoid perf_event__output_id_sample(struct perf_event *event,\n\t\t\t\t  struct perf_output_handle *handle,\n\t\t\t\t  struct perf_sample_data *sample)\n{\n\tif (event->attr.sample_id_all)\n\t\t__perf_event__output_id_sample(handle, sample);\n}\n\nstatic void perf_output_read_one(struct perf_output_handle *handle,\n\t\t\t\t struct perf_event *event,\n\t\t\t\t u64 enabled, u64 running)\n{\n\tu64 read_format = event->attr.read_format;\n\tu64 values[4];\n\tint n = 0;\n\n\tvalues[n++] = perf_event_count(event);\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED) {\n\t\tvalues[n++] = enabled +\n\t\t\tatomic64_read(&event->child_total_time_enabled);\n\t}\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING) {\n\t\tvalues[n++] = running +\n\t\t\tatomic64_read(&event->child_total_time_running);\n\t}\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(event);\n\n\t__output_copy(handle, values, n * sizeof(u64));\n}\n\n/*\n * XXX PERF_FORMAT_GROUP vs inherited events seems difficult.\n */\nstatic void perf_output_read_group(struct perf_output_handle *handle,\n\t\t\t    struct perf_event *event,\n\t\t\t    u64 enabled, u64 running)\n{\n\tstruct perf_event *leader = event->group_leader, *sub;\n\tu64 read_format = event->attr.read_format;\n\tu64 values[5];\n\tint n = 0;\n\n\tvalues[n++] = 1 + leader->nr_siblings;\n\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tvalues[n++] = enabled;\n\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tvalues[n++] = running;\n\n\tif (leader != event)\n\t\tleader->pmu->read(leader);\n\n\tvalues[n++] = perf_event_count(leader);\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(leader);\n\n\t__output_copy(handle, values, n * sizeof(u64));\n\n\tlist_for_each_entry(sub, &leader->sibling_list, group_entry) {\n\t\tn = 0;\n\n\t\tif (sub != event)\n\t\t\tsub->pmu->read(sub);\n\n\t\tvalues[n++] = perf_event_count(sub);\n\t\tif (read_format & PERF_FORMAT_ID)\n\t\t\tvalues[n++] = primary_event_id(sub);\n\n\t\t__output_copy(handle, values, n * sizeof(u64));\n\t}\n}\n\n#define PERF_FORMAT_TOTAL_TIMES (PERF_FORMAT_TOTAL_TIME_ENABLED|\\\n\t\t\t\t PERF_FORMAT_TOTAL_TIME_RUNNING)\n\nstatic void perf_output_read(struct perf_output_handle *handle,\n\t\t\t     struct perf_event *event)\n{\n\tu64 enabled = 0, running = 0;\n\tu64 read_format = event->attr.read_format;\n\n\t/*\n\t * compute total_time_enabled, total_time_running\n\t * based on snapshot values taken when the event\n\t * was last scheduled in.\n\t *\n\t * we cannot simply called update_context_time()\n\t * because of locking issue as we are called in\n\t * NMI context\n\t */\n\tif (read_format & PERF_FORMAT_TOTAL_TIMES)\n\t\tcalc_timer_values(event, &enabled, &running);\n\n\tif (event->attr.read_format & PERF_FORMAT_GROUP)\n\t\tperf_output_read_group(handle, event, enabled, running);\n\telse\n\t\tperf_output_read_one(handle, event, enabled, running);\n}\n\nvoid perf_output_sample(struct perf_output_handle *handle,\n\t\t\tstruct perf_event_header *header,\n\t\t\tstruct perf_sample_data *data,\n\t\t\tstruct perf_event *event)\n{\n\tu64 sample_type = data->type;\n\n\tperf_output_put(handle, *header);\n\n\tif (sample_type & PERF_SAMPLE_IP)\n\t\tperf_output_put(handle, data->ip);\n\n\tif (sample_type & PERF_SAMPLE_TID)\n\t\tperf_output_put(handle, data->tid_entry);\n\n\tif (sample_type & PERF_SAMPLE_TIME)\n\t\tperf_output_put(handle, data->time);\n\n\tif (sample_type & PERF_SAMPLE_ADDR)\n\t\tperf_output_put(handle, data->addr);\n\n\tif (sample_type & PERF_SAMPLE_ID)\n\t\tperf_output_put(handle, data->id);\n\n\tif (sample_type & PERF_SAMPLE_STREAM_ID)\n\t\tperf_output_put(handle, data->stream_id);\n\n\tif (sample_type & PERF_SAMPLE_CPU)\n\t\tperf_output_put(handle, data->cpu_entry);\n\n\tif (sample_type & PERF_SAMPLE_PERIOD)\n\t\tperf_output_put(handle, data->period);\n\n\tif (sample_type & PERF_SAMPLE_READ)\n\t\tperf_output_read(handle, event);\n\n\tif (sample_type & PERF_SAMPLE_CALLCHAIN) {\n\t\tif (data->callchain) {\n\t\t\tint size = 1;\n\n\t\t\tif (data->callchain)\n\t\t\t\tsize += data->callchain->nr;\n\n\t\t\tsize *= sizeof(u64);\n\n\t\t\t__output_copy(handle, data->callchain, size);\n\t\t} else {\n\t\t\tu64 nr = 0;\n\t\t\tperf_output_put(handle, nr);\n\t\t}\n\t}\n\n\tif (sample_type & PERF_SAMPLE_RAW) {\n\t\tif (data->raw) {\n\t\t\tperf_output_put(handle, data->raw->size);\n\t\t\t__output_copy(handle, data->raw->data,\n\t\t\t\t\t   data->raw->size);\n\t\t} else {\n\t\t\tstruct {\n\t\t\t\tu32\tsize;\n\t\t\t\tu32\tdata;\n\t\t\t} raw = {\n\t\t\t\t.size = sizeof(u32),\n\t\t\t\t.data = 0,\n\t\t\t};\n\t\t\tperf_output_put(handle, raw);\n\t\t}\n\t}\n}\n\nvoid perf_prepare_sample(struct perf_event_header *header,\n\t\t\t struct perf_sample_data *data,\n\t\t\t struct perf_event *event,\n\t\t\t struct pt_regs *regs)\n{\n\tu64 sample_type = event->attr.sample_type;\n\n\theader->type = PERF_RECORD_SAMPLE;\n\theader->size = sizeof(*header) + event->header_size;\n\n\theader->misc = 0;\n\theader->misc |= perf_misc_flags(regs);\n\n\t__perf_event_header__init_id(header, data, event);\n\n\tif (sample_type & PERF_SAMPLE_IP)\n\t\tdata->ip = perf_instruction_pointer(regs);\n\n\tif (sample_type & PERF_SAMPLE_CALLCHAIN) {\n\t\tint size = 1;\n\n\t\tdata->callchain = perf_callchain(regs);\n\n\t\tif (data->callchain)\n\t\t\tsize += data->callchain->nr;\n\n\t\theader->size += size * sizeof(u64);\n\t}\n\n\tif (sample_type & PERF_SAMPLE_RAW) {\n\t\tint size = sizeof(u32);\n\n\t\tif (data->raw)\n\t\t\tsize += data->raw->size;\n\t\telse\n\t\t\tsize += sizeof(u32);\n\n\t\tWARN_ON_ONCE(size & (sizeof(u64)-1));\n\t\theader->size += size;\n\t}\n}\n\nstatic void perf_event_output(struct perf_event *event, int nmi,\n\t\t\t\tstruct perf_sample_data *data,\n\t\t\t\tstruct pt_regs *regs)\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_event_header header;\n\n\t/* protect the callchain buffers */\n\trcu_read_lock();\n\n\tperf_prepare_sample(&header, data, event, regs);\n\n\tif (perf_output_begin(&handle, event, header.size, nmi, 1))\n\t\tgoto exit;\n\n\tperf_output_sample(&handle, &header, data, event);\n\n\tperf_output_end(&handle);\n\nexit:\n\trcu_read_unlock();\n}\n\n/*\n * read event_id\n */\n\nstruct perf_read_event {\n\tstruct perf_event_header\theader;\n\n\tu32\t\t\t\tpid;\n\tu32\t\t\t\ttid;\n};\n\nstatic void\nperf_event_read_event(struct perf_event *event,\n\t\t\tstruct task_struct *task)\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tstruct perf_read_event read_event = {\n\t\t.header = {\n\t\t\t.type = PERF_RECORD_READ,\n\t\t\t.misc = 0,\n\t\t\t.size = sizeof(read_event) + event->read_size,\n\t\t},\n\t\t.pid = perf_event_pid(event, task),\n\t\t.tid = perf_event_tid(event, task),\n\t};\n\tint ret;\n\n\tperf_event_header__init_id(&read_event.header, &sample, event);\n\tret = perf_output_begin(&handle, event, read_event.header.size, 0, 0);\n\tif (ret)\n\t\treturn;\n\n\tperf_output_put(&handle, read_event);\n\tperf_output_read(&handle, event);\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\n}\n\n/*\n * task tracking -- fork/exit\n *\n * enabled by: attr.comm | attr.mmap | attr.mmap_data | attr.task\n */\n\nstruct perf_task_event {\n\tstruct task_struct\t\t*task;\n\tstruct perf_event_context\t*task_ctx;\n\n\tstruct {\n\t\tstruct perf_event_header\theader;\n\n\t\tu32\t\t\t\tpid;\n\t\tu32\t\t\t\tppid;\n\t\tu32\t\t\t\ttid;\n\t\tu32\t\t\t\tptid;\n\t\tu64\t\t\t\ttime;\n\t} event_id;\n};\n\nstatic void perf_event_task_output(struct perf_event *event,\n\t\t\t\t     struct perf_task_event *task_event)\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data\tsample;\n\tstruct task_struct *task = task_event->task;\n\tint ret, size = task_event->event_id.header.size;\n\n\tperf_event_header__init_id(&task_event->event_id.header, &sample, event);\n\n\tret = perf_output_begin(&handle, event,\n\t\t\t\ttask_event->event_id.header.size, 0, 0);\n\tif (ret)\n\t\tgoto out;\n\n\ttask_event->event_id.pid = perf_event_pid(event, task);\n\ttask_event->event_id.ppid = perf_event_pid(event, current);\n\n\ttask_event->event_id.tid = perf_event_tid(event, task);\n\ttask_event->event_id.ptid = perf_event_tid(event, current);\n\n\tperf_output_put(&handle, task_event->event_id);\n\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\nout:\n\ttask_event->event_id.header.size = size;\n}\n\nstatic int perf_event_task_match(struct perf_event *event)\n{\n\tif (event->state < PERF_EVENT_STATE_INACTIVE)\n\t\treturn 0;\n\n\tif (!event_filter_match(event))\n\t\treturn 0;\n\n\tif (event->attr.comm || event->attr.mmap ||\n\t    event->attr.mmap_data || event->attr.task)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void perf_event_task_ctx(struct perf_event_context *ctx,\n\t\t\t\t  struct perf_task_event *task_event)\n{\n\tstruct perf_event *event;\n\n\tlist_for_each_entry_rcu(event, &ctx->event_list, event_entry) {\n\t\tif (perf_event_task_match(event))\n\t\t\tperf_event_task_output(event, task_event);\n\t}\n}\n\nstatic void perf_event_task_event(struct perf_task_event *task_event)\n{\n\tstruct perf_cpu_context *cpuctx;\n\tstruct perf_event_context *ctx;\n\tstruct pmu *pmu;\n\tint ctxn;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(pmu, &pmus, entry) {\n\t\tcpuctx = get_cpu_ptr(pmu->pmu_cpu_context);\n\t\tif (cpuctx->active_pmu != pmu)\n\t\t\tgoto next;\n\t\tperf_event_task_ctx(&cpuctx->ctx, task_event);\n\n\t\tctx = task_event->task_ctx;\n\t\tif (!ctx) {\n\t\t\tctxn = pmu->task_ctx_nr;\n\t\t\tif (ctxn < 0)\n\t\t\t\tgoto next;\n\t\t\tctx = rcu_dereference(current->perf_event_ctxp[ctxn]);\n\t\t}\n\t\tif (ctx)\n\t\t\tperf_event_task_ctx(ctx, task_event);\nnext:\n\t\tput_cpu_ptr(pmu->pmu_cpu_context);\n\t}\n\trcu_read_unlock();\n}\n\nstatic void perf_event_task(struct task_struct *task,\n\t\t\t      struct perf_event_context *task_ctx,\n\t\t\t      int new)\n{\n\tstruct perf_task_event task_event;\n\n\tif (!atomic_read(&nr_comm_events) &&\n\t    !atomic_read(&nr_mmap_events) &&\n\t    !atomic_read(&nr_task_events))\n\t\treturn;\n\n\ttask_event = (struct perf_task_event){\n\t\t.task\t  = task,\n\t\t.task_ctx = task_ctx,\n\t\t.event_id    = {\n\t\t\t.header = {\n\t\t\t\t.type = new ? PERF_RECORD_FORK : PERF_RECORD_EXIT,\n\t\t\t\t.misc = 0,\n\t\t\t\t.size = sizeof(task_event.event_id),\n\t\t\t},\n\t\t\t/* .pid  */\n\t\t\t/* .ppid */\n\t\t\t/* .tid  */\n\t\t\t/* .ptid */\n\t\t\t.time = perf_clock(),\n\t\t},\n\t};\n\n\tperf_event_task_event(&task_event);\n}\n\nvoid perf_event_fork(struct task_struct *task)\n{\n\tperf_event_task(task, NULL, 1);\n}\n\n/*\n * comm tracking\n */\n\nstruct perf_comm_event {\n\tstruct task_struct\t*task;\n\tchar\t\t\t*comm;\n\tint\t\t\tcomm_size;\n\n\tstruct {\n\t\tstruct perf_event_header\theader;\n\n\t\tu32\t\t\t\tpid;\n\t\tu32\t\t\t\ttid;\n\t} event_id;\n};\n\nstatic void perf_event_comm_output(struct perf_event *event,\n\t\t\t\t     struct perf_comm_event *comm_event)\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tint size = comm_event->event_id.header.size;\n\tint ret;\n\n\tperf_event_header__init_id(&comm_event->event_id.header, &sample, event);\n\tret = perf_output_begin(&handle, event,\n\t\t\t\tcomm_event->event_id.header.size, 0, 0);\n\n\tif (ret)\n\t\tgoto out;\n\n\tcomm_event->event_id.pid = perf_event_pid(event, comm_event->task);\n\tcomm_event->event_id.tid = perf_event_tid(event, comm_event->task);\n\n\tperf_output_put(&handle, comm_event->event_id);\n\t__output_copy(&handle, comm_event->comm,\n\t\t\t\t   comm_event->comm_size);\n\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\nout:\n\tcomm_event->event_id.header.size = size;\n}\n\nstatic int perf_event_comm_match(struct perf_event *event)\n{\n\tif (event->state < PERF_EVENT_STATE_INACTIVE)\n\t\treturn 0;\n\n\tif (!event_filter_match(event))\n\t\treturn 0;\n\n\tif (event->attr.comm)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void perf_event_comm_ctx(struct perf_event_context *ctx,\n\t\t\t\t  struct perf_comm_event *comm_event)\n{\n\tstruct perf_event *event;\n\n\tlist_for_each_entry_rcu(event, &ctx->event_list, event_entry) {\n\t\tif (perf_event_comm_match(event))\n\t\t\tperf_event_comm_output(event, comm_event);\n\t}\n}\n\nstatic void perf_event_comm_event(struct perf_comm_event *comm_event)\n{\n\tstruct perf_cpu_context *cpuctx;\n\tstruct perf_event_context *ctx;\n\tchar comm[TASK_COMM_LEN];\n\tunsigned int size;\n\tstruct pmu *pmu;\n\tint ctxn;\n\n\tmemset(comm, 0, sizeof(comm));\n\tstrlcpy(comm, comm_event->task->comm, sizeof(comm));\n\tsize = ALIGN(strlen(comm)+1, sizeof(u64));\n\n\tcomm_event->comm = comm;\n\tcomm_event->comm_size = size;\n\n\tcomm_event->event_id.header.size = sizeof(comm_event->event_id) + size;\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(pmu, &pmus, entry) {\n\t\tcpuctx = get_cpu_ptr(pmu->pmu_cpu_context);\n\t\tif (cpuctx->active_pmu != pmu)\n\t\t\tgoto next;\n\t\tperf_event_comm_ctx(&cpuctx->ctx, comm_event);\n\n\t\tctxn = pmu->task_ctx_nr;\n\t\tif (ctxn < 0)\n\t\t\tgoto next;\n\n\t\tctx = rcu_dereference(current->perf_event_ctxp[ctxn]);\n\t\tif (ctx)\n\t\t\tperf_event_comm_ctx(ctx, comm_event);\nnext:\n\t\tput_cpu_ptr(pmu->pmu_cpu_context);\n\t}\n\trcu_read_unlock();\n}\n\nvoid perf_event_comm(struct task_struct *task)\n{\n\tstruct perf_comm_event comm_event;\n\tstruct perf_event_context *ctx;\n\tint ctxn;\n\n\tfor_each_task_context_nr(ctxn) {\n\t\tctx = task->perf_event_ctxp[ctxn];\n\t\tif (!ctx)\n\t\t\tcontinue;\n\n\t\tperf_event_enable_on_exec(ctx);\n\t}\n\n\tif (!atomic_read(&nr_comm_events))\n\t\treturn;\n\n\tcomm_event = (struct perf_comm_event){\n\t\t.task\t= task,\n\t\t/* .comm      */\n\t\t/* .comm_size */\n\t\t.event_id  = {\n\t\t\t.header = {\n\t\t\t\t.type = PERF_RECORD_COMM,\n\t\t\t\t.misc = 0,\n\t\t\t\t/* .size */\n\t\t\t},\n\t\t\t/* .pid */\n\t\t\t/* .tid */\n\t\t},\n\t};\n\n\tperf_event_comm_event(&comm_event);\n}\n\n/*\n * mmap tracking\n */\n\nstruct perf_mmap_event {\n\tstruct vm_area_struct\t*vma;\n\n\tconst char\t\t*file_name;\n\tint\t\t\tfile_size;\n\n\tstruct {\n\t\tstruct perf_event_header\theader;\n\n\t\tu32\t\t\t\tpid;\n\t\tu32\t\t\t\ttid;\n\t\tu64\t\t\t\tstart;\n\t\tu64\t\t\t\tlen;\n\t\tu64\t\t\t\tpgoff;\n\t} event_id;\n};\n\nstatic void perf_event_mmap_output(struct perf_event *event,\n\t\t\t\t     struct perf_mmap_event *mmap_event)\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tint size = mmap_event->event_id.header.size;\n\tint ret;\n\n\tperf_event_header__init_id(&mmap_event->event_id.header, &sample, event);\n\tret = perf_output_begin(&handle, event,\n\t\t\t\tmmap_event->event_id.header.size, 0, 0);\n\tif (ret)\n\t\tgoto out;\n\n\tmmap_event->event_id.pid = perf_event_pid(event, current);\n\tmmap_event->event_id.tid = perf_event_tid(event, current);\n\n\tperf_output_put(&handle, mmap_event->event_id);\n\t__output_copy(&handle, mmap_event->file_name,\n\t\t\t\t   mmap_event->file_size);\n\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\nout:\n\tmmap_event->event_id.header.size = size;\n}\n\nstatic int perf_event_mmap_match(struct perf_event *event,\n\t\t\t\t   struct perf_mmap_event *mmap_event,\n\t\t\t\t   int executable)\n{\n\tif (event->state < PERF_EVENT_STATE_INACTIVE)\n\t\treturn 0;\n\n\tif (!event_filter_match(event))\n\t\treturn 0;\n\n\tif ((!executable && event->attr.mmap_data) ||\n\t    (executable && event->attr.mmap))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void perf_event_mmap_ctx(struct perf_event_context *ctx,\n\t\t\t\t  struct perf_mmap_event *mmap_event,\n\t\t\t\t  int executable)\n{\n\tstruct perf_event *event;\n\n\tlist_for_each_entry_rcu(event, &ctx->event_list, event_entry) {\n\t\tif (perf_event_mmap_match(event, mmap_event, executable))\n\t\t\tperf_event_mmap_output(event, mmap_event);\n\t}\n}\n\nstatic void perf_event_mmap_event(struct perf_mmap_event *mmap_event)\n{\n\tstruct perf_cpu_context *cpuctx;\n\tstruct perf_event_context *ctx;\n\tstruct vm_area_struct *vma = mmap_event->vma;\n\tstruct file *file = vma->vm_file;\n\tunsigned int size;\n\tchar tmp[16];\n\tchar *buf = NULL;\n\tconst char *name;\n\tstruct pmu *pmu;\n\tint ctxn;\n\n\tmemset(tmp, 0, sizeof(tmp));\n\n\tif (file) {\n\t\t/*\n\t\t * d_path works from the end of the rb backwards, so we\n\t\t * need to add enough zero bytes after the string to handle\n\t\t * the 64bit alignment we do later.\n\t\t */\n\t\tbuf = kzalloc(PATH_MAX + sizeof(u64), GFP_KERNEL);\n\t\tif (!buf) {\n\t\t\tname = strncpy(tmp, \"//enomem\", sizeof(tmp));\n\t\t\tgoto got_name;\n\t\t}\n\t\tname = d_path(&file->f_path, buf, PATH_MAX);\n\t\tif (IS_ERR(name)) {\n\t\t\tname = strncpy(tmp, \"//toolong\", sizeof(tmp));\n\t\t\tgoto got_name;\n\t\t}\n\t} else {\n\t\tif (arch_vma_name(mmap_event->vma)) {\n\t\t\tname = strncpy(tmp, arch_vma_name(mmap_event->vma),\n\t\t\t\t       sizeof(tmp));\n\t\t\tgoto got_name;\n\t\t}\n\n\t\tif (!vma->vm_mm) {\n\t\t\tname = strncpy(tmp, \"[vdso]\", sizeof(tmp));\n\t\t\tgoto got_name;\n\t\t} else if (vma->vm_start <= vma->vm_mm->start_brk &&\n\t\t\t\tvma->vm_end >= vma->vm_mm->brk) {\n\t\t\tname = strncpy(tmp, \"[heap]\", sizeof(tmp));\n\t\t\tgoto got_name;\n\t\t} else if (vma->vm_start <= vma->vm_mm->start_stack &&\n\t\t\t\tvma->vm_end >= vma->vm_mm->start_stack) {\n\t\t\tname = strncpy(tmp, \"[stack]\", sizeof(tmp));\n\t\t\tgoto got_name;\n\t\t}\n\n\t\tname = strncpy(tmp, \"//anon\", sizeof(tmp));\n\t\tgoto got_name;\n\t}\n\ngot_name:\n\tsize = ALIGN(strlen(name)+1, sizeof(u64));\n\n\tmmap_event->file_name = name;\n\tmmap_event->file_size = size;\n\n\tmmap_event->event_id.header.size = sizeof(mmap_event->event_id) + size;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(pmu, &pmus, entry) {\n\t\tcpuctx = get_cpu_ptr(pmu->pmu_cpu_context);\n\t\tif (cpuctx->active_pmu != pmu)\n\t\t\tgoto next;\n\t\tperf_event_mmap_ctx(&cpuctx->ctx, mmap_event,\n\t\t\t\t\tvma->vm_flags & VM_EXEC);\n\n\t\tctxn = pmu->task_ctx_nr;\n\t\tif (ctxn < 0)\n\t\t\tgoto next;\n\n\t\tctx = rcu_dereference(current->perf_event_ctxp[ctxn]);\n\t\tif (ctx) {\n\t\t\tperf_event_mmap_ctx(ctx, mmap_event,\n\t\t\t\t\tvma->vm_flags & VM_EXEC);\n\t\t}\nnext:\n\t\tput_cpu_ptr(pmu->pmu_cpu_context);\n\t}\n\trcu_read_unlock();\n\n\tkfree(buf);\n}\n\nvoid perf_event_mmap(struct vm_area_struct *vma)\n{\n\tstruct perf_mmap_event mmap_event;\n\n\tif (!atomic_read(&nr_mmap_events))\n\t\treturn;\n\n\tmmap_event = (struct perf_mmap_event){\n\t\t.vma\t= vma,\n\t\t/* .file_name */\n\t\t/* .file_size */\n\t\t.event_id  = {\n\t\t\t.header = {\n\t\t\t\t.type = PERF_RECORD_MMAP,\n\t\t\t\t.misc = PERF_RECORD_MISC_USER,\n\t\t\t\t/* .size */\n\t\t\t},\n\t\t\t/* .pid */\n\t\t\t/* .tid */\n\t\t\t.start  = vma->vm_start,\n\t\t\t.len    = vma->vm_end - vma->vm_start,\n\t\t\t.pgoff  = (u64)vma->vm_pgoff << PAGE_SHIFT,\n\t\t},\n\t};\n\n\tperf_event_mmap_event(&mmap_event);\n}\n\n/*\n * IRQ throttle logging\n */\n\nstatic void perf_log_throttle(struct perf_event *event, int enable)\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tint ret;\n\n\tstruct {\n\t\tstruct perf_event_header\theader;\n\t\tu64\t\t\t\ttime;\n\t\tu64\t\t\t\tid;\n\t\tu64\t\t\t\tstream_id;\n\t} throttle_event = {\n\t\t.header = {\n\t\t\t.type = PERF_RECORD_THROTTLE,\n\t\t\t.misc = 0,\n\t\t\t.size = sizeof(throttle_event),\n\t\t},\n\t\t.time\t\t= perf_clock(),\n\t\t.id\t\t= primary_event_id(event),\n\t\t.stream_id\t= event->id,\n\t};\n\n\tif (enable)\n\t\tthrottle_event.header.type = PERF_RECORD_UNTHROTTLE;\n\n\tperf_event_header__init_id(&throttle_event.header, &sample, event);\n\n\tret = perf_output_begin(&handle, event,\n\t\t\t\tthrottle_event.header.size, 1, 0);\n\tif (ret)\n\t\treturn;\n\n\tperf_output_put(&handle, throttle_event);\n\tperf_event__output_id_sample(event, &handle, &sample);\n\tperf_output_end(&handle);\n}\n\n/*\n * Generic event overflow handling, sampling.\n */\n\nstatic int __perf_event_overflow(struct perf_event *event, int nmi,\n\t\t\t\t   int throttle, struct perf_sample_data *data,\n\t\t\t\t   struct pt_regs *regs)\n{\n\tint events = atomic_read(&event->event_limit);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint ret = 0;\n\n\t/*\n\t * Non-sampling counters might still use the PMI to fold short\n\t * hardware counters, ignore those.\n\t */\n\tif (unlikely(!is_sampling_event(event)))\n\t\treturn 0;\n\n\tif (unlikely(hwc->interrupts >= max_samples_per_tick)) {\n\t\tif (throttle) {\n\t\t\thwc->interrupts = MAX_INTERRUPTS;\n\t\t\tperf_log_throttle(event, 0);\n\t\t\tret = 1;\n\t\t}\n\t} else\n\t\thwc->interrupts++;\n\n\tif (event->attr.freq) {\n\t\tu64 now = perf_clock();\n\t\ts64 delta = now - hwc->freq_time_stamp;\n\n\t\thwc->freq_time_stamp = now;\n\n\t\tif (delta > 0 && delta < 2*TICK_NSEC)\n\t\t\tperf_adjust_period(event, delta, hwc->last_period);\n\t}\n\n\t/*\n\t * XXX event_limit might not quite work as expected on inherited\n\t * events\n\t */\n\n\tevent->pending_kill = POLL_IN;\n\tif (events && atomic_dec_and_test(&event->event_limit)) {\n\t\tret = 1;\n\t\tevent->pending_kill = POLL_HUP;\n\t\tif (nmi) {\n\t\t\tevent->pending_disable = 1;\n\t\t\tirq_work_queue(&event->pending);\n\t\t} else\n\t\t\tperf_event_disable(event);\n\t}\n\n\tif (event->overflow_handler)\n\t\tevent->overflow_handler(event, nmi, data, regs);\n\telse\n\t\tperf_event_output(event, nmi, data, regs);\n\n\tif (event->fasync && event->pending_kill) {\n\t\tif (nmi) {\n\t\t\tevent->pending_wakeup = 1;\n\t\t\tirq_work_queue(&event->pending);\n\t\t} else\n\t\t\tperf_event_wakeup(event);\n\t}\n\n\treturn ret;\n}\n\nint perf_event_overflow(struct perf_event *event, int nmi,\n\t\t\t  struct perf_sample_data *data,\n\t\t\t  struct pt_regs *regs)\n{\n\treturn __perf_event_overflow(event, nmi, 1, data, regs);\n}\n\n/*\n * Generic software event infrastructure\n */\n\nstruct swevent_htable {\n\tstruct swevent_hlist\t\t*swevent_hlist;\n\tstruct mutex\t\t\thlist_mutex;\n\tint\t\t\t\thlist_refcount;\n\n\t/* Recursion avoidance in each contexts */\n\tint\t\t\t\trecursion[PERF_NR_CONTEXTS];\n};\n\nstatic DEFINE_PER_CPU(struct swevent_htable, swevent_htable);\n\n/*\n * We directly increment event->count and keep a second value in\n * event->hw.period_left to count intervals. This period event\n * is kept in the range [-sample_period, 0] so that we can use the\n * sign as trigger.\n */\n\nstatic u64 perf_swevent_set_period(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 period = hwc->last_period;\n\tu64 nr, offset;\n\ts64 old, val;\n\n\thwc->last_period = hwc->sample_period;\n\nagain:\n\told = val = local64_read(&hwc->period_left);\n\tif (val < 0)\n\t\treturn 0;\n\n\tnr = div64_u64(period + val, period);\n\toffset = nr * period;\n\tval -= offset;\n\tif (local64_cmpxchg(&hwc->period_left, old, val) != old)\n\t\tgoto again;\n\n\treturn nr;\n}\n\nstatic void perf_swevent_overflow(struct perf_event *event, u64 overflow,\n\t\t\t\t    int nmi, struct perf_sample_data *data,\n\t\t\t\t    struct pt_regs *regs)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint throttle = 0;\n\n\tdata->period = event->hw.last_period;\n\tif (!overflow)\n\t\toverflow = perf_swevent_set_period(event);\n\n\tif (hwc->interrupts == MAX_INTERRUPTS)\n\t\treturn;\n\n\tfor (; overflow; overflow--) {\n\t\tif (__perf_event_overflow(event, nmi, throttle,\n\t\t\t\t\t    data, regs)) {\n\t\t\t/*\n\t\t\t * We inhibit the overflow from happening when\n\t\t\t * hwc->interrupts == MAX_INTERRUPTS.\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\t\tthrottle = 1;\n\t}\n}\n\nstatic void perf_swevent_event(struct perf_event *event, u64 nr,\n\t\t\t       int nmi, struct perf_sample_data *data,\n\t\t\t       struct pt_regs *regs)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tlocal64_add(nr, &event->count);\n\n\tif (!regs)\n\t\treturn;\n\n\tif (!is_sampling_event(event))\n\t\treturn;\n\n\tif (nr == 1 && hwc->sample_period == 1 && !event->attr.freq)\n\t\treturn perf_swevent_overflow(event, 1, nmi, data, regs);\n\n\tif (local64_add_negative(nr, &hwc->period_left))\n\t\treturn;\n\n\tperf_swevent_overflow(event, 0, nmi, data, regs);\n}\n\nstatic int perf_exclude_event(struct perf_event *event,\n\t\t\t      struct pt_regs *regs)\n{\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn 1;\n\n\tif (regs) {\n\t\tif (event->attr.exclude_user && user_mode(regs))\n\t\t\treturn 1;\n\n\t\tif (event->attr.exclude_kernel && !user_mode(regs))\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int perf_swevent_match(struct perf_event *event,\n\t\t\t\tenum perf_type_id type,\n\t\t\t\tu32 event_id,\n\t\t\t\tstruct perf_sample_data *data,\n\t\t\t\tstruct pt_regs *regs)\n{\n\tif (event->attr.type != type)\n\t\treturn 0;\n\n\tif (event->attr.config != event_id)\n\t\treturn 0;\n\n\tif (perf_exclude_event(event, regs))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic inline u64 swevent_hash(u64 type, u32 event_id)\n{\n\tu64 val = event_id | (type << 32);\n\n\treturn hash_64(val, SWEVENT_HLIST_BITS);\n}\n\nstatic inline struct hlist_head *\n__find_swevent_head(struct swevent_hlist *hlist, u64 type, u32 event_id)\n{\n\tu64 hash = swevent_hash(type, event_id);\n\n\treturn &hlist->heads[hash];\n}\n\n/* For the read side: events when they trigger */\nstatic inline struct hlist_head *\nfind_swevent_head_rcu(struct swevent_htable *swhash, u64 type, u32 event_id)\n{\n\tstruct swevent_hlist *hlist;\n\n\thlist = rcu_dereference(swhash->swevent_hlist);\n\tif (!hlist)\n\t\treturn NULL;\n\n\treturn __find_swevent_head(hlist, type, event_id);\n}\n\n/* For the event head insertion and removal in the hlist */\nstatic inline struct hlist_head *\nfind_swevent_head(struct swevent_htable *swhash, struct perf_event *event)\n{\n\tstruct swevent_hlist *hlist;\n\tu32 event_id = event->attr.config;\n\tu64 type = event->attr.type;\n\n\t/*\n\t * Event scheduling is always serialized against hlist allocation\n\t * and release. Which makes the protected version suitable here.\n\t * The context lock guarantees that.\n\t */\n\thlist = rcu_dereference_protected(swhash->swevent_hlist,\n\t\t\t\t\t  lockdep_is_held(&event->ctx->lock));\n\tif (!hlist)\n\t\treturn NULL;\n\n\treturn __find_swevent_head(hlist, type, event_id);\n}\n\nstatic void do_perf_sw_event(enum perf_type_id type, u32 event_id,\n\t\t\t\t    u64 nr, int nmi,\n\t\t\t\t    struct perf_sample_data *data,\n\t\t\t\t    struct pt_regs *regs)\n{\n\tstruct swevent_htable *swhash = &__get_cpu_var(swevent_htable);\n\tstruct perf_event *event;\n\tstruct hlist_node *node;\n\tstruct hlist_head *head;\n\n\trcu_read_lock();\n\thead = find_swevent_head_rcu(swhash, type, event_id);\n\tif (!head)\n\t\tgoto end;\n\n\thlist_for_each_entry_rcu(event, node, head, hlist_entry) {\n\t\tif (perf_swevent_match(event, type, event_id, data, regs))\n\t\t\tperf_swevent_event(event, nr, nmi, data, regs);\n\t}\nend:\n\trcu_read_unlock();\n}\n\nint perf_swevent_get_recursion_context(void)\n{\n\tstruct swevent_htable *swhash = &__get_cpu_var(swevent_htable);\n\n\treturn get_recursion_context(swhash->recursion);\n}\nEXPORT_SYMBOL_GPL(perf_swevent_get_recursion_context);\n\ninline void perf_swevent_put_recursion_context(int rctx)\n{\n\tstruct swevent_htable *swhash = &__get_cpu_var(swevent_htable);\n\n\tput_recursion_context(swhash->recursion, rctx);\n}\n\nvoid __perf_sw_event(u32 event_id, u64 nr, int nmi,\n\t\t\t    struct pt_regs *regs, u64 addr)\n{\n\tstruct perf_sample_data data;\n\tint rctx;\n\n\tpreempt_disable_notrace();\n\trctx = perf_swevent_get_recursion_context();\n\tif (rctx < 0)\n\t\treturn;\n\n\tperf_sample_data_init(&data, addr);\n\n\tdo_perf_sw_event(PERF_TYPE_SOFTWARE, event_id, nr, nmi, &data, regs);\n\n\tperf_swevent_put_recursion_context(rctx);\n\tpreempt_enable_notrace();\n}\n\nstatic void perf_swevent_read(struct perf_event *event)\n{\n}\n\nstatic int perf_swevent_add(struct perf_event *event, int flags)\n{\n\tstruct swevent_htable *swhash = &__get_cpu_var(swevent_htable);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct hlist_head *head;\n\n\tif (is_sampling_event(event)) {\n\t\thwc->last_period = hwc->sample_period;\n\t\tperf_swevent_set_period(event);\n\t}\n\n\thwc->state = !(flags & PERF_EF_START);\n\n\thead = find_swevent_head(swhash, event);\n\tif (WARN_ON_ONCE(!head))\n\t\treturn -EINVAL;\n\n\thlist_add_head_rcu(&event->hlist_entry, head);\n\n\treturn 0;\n}\n\nstatic void perf_swevent_del(struct perf_event *event, int flags)\n{\n\thlist_del_rcu(&event->hlist_entry);\n}\n\nstatic void perf_swevent_start(struct perf_event *event, int flags)\n{\n\tevent->hw.state = 0;\n}\n\nstatic void perf_swevent_stop(struct perf_event *event, int flags)\n{\n\tevent->hw.state = PERF_HES_STOPPED;\n}\n\n/* Deref the hlist from the update side */\nstatic inline struct swevent_hlist *\nswevent_hlist_deref(struct swevent_htable *swhash)\n{\n\treturn rcu_dereference_protected(swhash->swevent_hlist,\n\t\t\t\t\t lockdep_is_held(&swhash->hlist_mutex));\n}\n\nstatic void swevent_hlist_release(struct swevent_htable *swhash)\n{\n\tstruct swevent_hlist *hlist = swevent_hlist_deref(swhash);\n\n\tif (!hlist)\n\t\treturn;\n\n\trcu_assign_pointer(swhash->swevent_hlist, NULL);\n\tkfree_rcu(hlist, rcu_head);\n}\n\nstatic void swevent_hlist_put_cpu(struct perf_event *event, int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\n\tif (!--swhash->hlist_refcount)\n\t\tswevent_hlist_release(swhash);\n\n\tmutex_unlock(&swhash->hlist_mutex);\n}\n\nstatic void swevent_hlist_put(struct perf_event *event)\n{\n\tint cpu;\n\n\tif (event->cpu != -1) {\n\t\tswevent_hlist_put_cpu(event, event->cpu);\n\t\treturn;\n\t}\n\n\tfor_each_possible_cpu(cpu)\n\t\tswevent_hlist_put_cpu(event, cpu);\n}\n\nstatic int swevent_hlist_get_cpu(struct perf_event *event, int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\tint err = 0;\n\n\tmutex_lock(&swhash->hlist_mutex);\n\n\tif (!swevent_hlist_deref(swhash) && cpu_online(cpu)) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc(sizeof(*hlist), GFP_KERNEL);\n\t\tif (!hlist) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto exit;\n\t\t}\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tswhash->hlist_refcount++;\nexit:\n\tmutex_unlock(&swhash->hlist_mutex);\n\n\treturn err;\n}\n\nstatic int swevent_hlist_get(struct perf_event *event)\n{\n\tint err;\n\tint cpu, failed_cpu;\n\n\tif (event->cpu != -1)\n\t\treturn swevent_hlist_get_cpu(event, event->cpu);\n\n\tget_online_cpus();\n\tfor_each_possible_cpu(cpu) {\n\t\terr = swevent_hlist_get_cpu(event, cpu);\n\t\tif (err) {\n\t\t\tfailed_cpu = cpu;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\tput_online_cpus();\n\n\treturn 0;\nfail:\n\tfor_each_possible_cpu(cpu) {\n\t\tif (cpu == failed_cpu)\n\t\t\tbreak;\n\t\tswevent_hlist_put_cpu(event, cpu);\n\t}\n\n\tput_online_cpus();\n\treturn err;\n}\n\nstruct jump_label_key perf_swevent_enabled[PERF_COUNT_SW_MAX];\n\nstatic void sw_perf_event_destroy(struct perf_event *event)\n{\n\tu64 event_id = event->attr.config;\n\n\tWARN_ON(event->parent);\n\n\tjump_label_dec(&perf_swevent_enabled[event_id]);\n\tswevent_hlist_put(event);\n}\n\nstatic int perf_swevent_init(struct perf_event *event)\n{\n\tint event_id = event->attr.config;\n\n\tif (event->attr.type != PERF_TYPE_SOFTWARE)\n\t\treturn -ENOENT;\n\n\tswitch (event_id) {\n\tcase PERF_COUNT_SW_CPU_CLOCK:\n\tcase PERF_COUNT_SW_TASK_CLOCK:\n\t\treturn -ENOENT;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (event_id >= PERF_COUNT_SW_MAX)\n\t\treturn -ENOENT;\n\n\tif (!event->parent) {\n\t\tint err;\n\n\t\terr = swevent_hlist_get(event);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tjump_label_inc(&perf_swevent_enabled[event_id]);\n\t\tevent->destroy = sw_perf_event_destroy;\n\t}\n\n\treturn 0;\n}\n\nstatic struct pmu perf_swevent = {\n\t.task_ctx_nr\t= perf_sw_context,\n\n\t.event_init\t= perf_swevent_init,\n\t.add\t\t= perf_swevent_add,\n\t.del\t\t= perf_swevent_del,\n\t.start\t\t= perf_swevent_start,\n\t.stop\t\t= perf_swevent_stop,\n\t.read\t\t= perf_swevent_read,\n};\n\n#ifdef CONFIG_EVENT_TRACING\n\nstatic int perf_tp_filter_match(struct perf_event *event,\n\t\t\t\tstruct perf_sample_data *data)\n{\n\tvoid *record = data->raw->data;\n\n\tif (likely(!event->filter) || filter_match_preds(event->filter, record))\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic int perf_tp_event_match(struct perf_event *event,\n\t\t\t\tstruct perf_sample_data *data,\n\t\t\t\tstruct pt_regs *regs)\n{\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn 0;\n\t/*\n\t * All tracepoints are from kernel-space.\n\t */\n\tif (event->attr.exclude_kernel)\n\t\treturn 0;\n\n\tif (!perf_tp_filter_match(event, data))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nvoid perf_tp_event(u64 addr, u64 count, void *record, int entry_size,\n\t\t   struct pt_regs *regs, struct hlist_head *head, int rctx)\n{\n\tstruct perf_sample_data data;\n\tstruct perf_event *event;\n\tstruct hlist_node *node;\n\n\tstruct perf_raw_record raw = {\n\t\t.size = entry_size,\n\t\t.data = record,\n\t};\n\n\tperf_sample_data_init(&data, addr);\n\tdata.raw = &raw;\n\n\thlist_for_each_entry_rcu(event, node, head, hlist_entry) {\n\t\tif (perf_tp_event_match(event, &data, regs))\n\t\t\tperf_swevent_event(event, count, 1, &data, regs);\n\t}\n\n\tperf_swevent_put_recursion_context(rctx);\n}\nEXPORT_SYMBOL_GPL(perf_tp_event);\n\nstatic void tp_perf_event_destroy(struct perf_event *event)\n{\n\tperf_trace_destroy(event);\n}\n\nstatic int perf_tp_event_init(struct perf_event *event)\n{\n\tint err;\n\n\tif (event->attr.type != PERF_TYPE_TRACEPOINT)\n\t\treturn -ENOENT;\n\n\terr = perf_trace_init(event);\n\tif (err)\n\t\treturn err;\n\n\tevent->destroy = tp_perf_event_destroy;\n\n\treturn 0;\n}\n\nstatic struct pmu perf_tracepoint = {\n\t.task_ctx_nr\t= perf_sw_context,\n\n\t.event_init\t= perf_tp_event_init,\n\t.add\t\t= perf_trace_add,\n\t.del\t\t= perf_trace_del,\n\t.start\t\t= perf_swevent_start,\n\t.stop\t\t= perf_swevent_stop,\n\t.read\t\t= perf_swevent_read,\n};\n\nstatic inline void perf_tp_register(void)\n{\n\tperf_pmu_register(&perf_tracepoint, \"tracepoint\", PERF_TYPE_TRACEPOINT);\n}\n\nstatic int perf_event_set_filter(struct perf_event *event, void __user *arg)\n{\n\tchar *filter_str;\n\tint ret;\n\n\tif (event->attr.type != PERF_TYPE_TRACEPOINT)\n\t\treturn -EINVAL;\n\n\tfilter_str = strndup_user(arg, PAGE_SIZE);\n\tif (IS_ERR(filter_str))\n\t\treturn PTR_ERR(filter_str);\n\n\tret = ftrace_profile_set_filter(event, event->attr.config, filter_str);\n\n\tkfree(filter_str);\n\treturn ret;\n}\n\nstatic void perf_event_free_filter(struct perf_event *event)\n{\n\tftrace_profile_free_filter(event);\n}\n\n#else\n\nstatic inline void perf_tp_register(void)\n{\n}\n\nstatic int perf_event_set_filter(struct perf_event *event, void __user *arg)\n{\n\treturn -ENOENT;\n}\n\nstatic void perf_event_free_filter(struct perf_event *event)\n{\n}\n\n#endif /* CONFIG_EVENT_TRACING */\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\nvoid perf_bp_event(struct perf_event *bp, void *data)\n{\n\tstruct perf_sample_data sample;\n\tstruct pt_regs *regs = data;\n\n\tperf_sample_data_init(&sample, bp->attr.bp_addr);\n\n\tif (!bp->hw.state && !perf_exclude_event(bp, regs))\n\t\tperf_swevent_event(bp, 1, 1, &sample, regs);\n}\n#endif\n\n/*\n * hrtimer based swevent callback\n */\n\nstatic enum hrtimer_restart perf_swevent_hrtimer(struct hrtimer *hrtimer)\n{\n\tenum hrtimer_restart ret = HRTIMER_RESTART;\n\tstruct perf_sample_data data;\n\tstruct pt_regs *regs;\n\tstruct perf_event *event;\n\tu64 period;\n\n\tevent = container_of(hrtimer, struct perf_event, hw.hrtimer);\n\n\tif (event->state != PERF_EVENT_STATE_ACTIVE)\n\t\treturn HRTIMER_NORESTART;\n\n\tevent->pmu->read(event);\n\n\tperf_sample_data_init(&data, 0);\n\tdata.period = event->hw.last_period;\n\tregs = get_irq_regs();\n\n\tif (regs && !perf_exclude_event(event, regs)) {\n\t\tif (!(event->attr.exclude_idle && current->pid == 0))\n\t\t\tif (perf_event_overflow(event, 0, &data, regs))\n\t\t\t\tret = HRTIMER_NORESTART;\n\t}\n\n\tperiod = max_t(u64, 10000, event->hw.sample_period);\n\thrtimer_forward_now(hrtimer, ns_to_ktime(period));\n\n\treturn ret;\n}\n\nstatic void perf_swevent_start_hrtimer(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\ts64 period;\n\n\tif (!is_sampling_event(event))\n\t\treturn;\n\n\tperiod = local64_read(&hwc->period_left);\n\tif (period) {\n\t\tif (period < 0)\n\t\t\tperiod = 10000;\n\n\t\tlocal64_set(&hwc->period_left, 0);\n\t} else {\n\t\tperiod = max_t(u64, 10000, hwc->sample_period);\n\t}\n\t__hrtimer_start_range_ns(&hwc->hrtimer,\n\t\t\t\tns_to_ktime(period), 0,\n\t\t\t\tHRTIMER_MODE_REL_PINNED, 0);\n}\n\nstatic void perf_swevent_cancel_hrtimer(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (is_sampling_event(event)) {\n\t\tktime_t remaining = hrtimer_get_remaining(&hwc->hrtimer);\n\t\tlocal64_set(&hwc->period_left, ktime_to_ns(remaining));\n\n\t\thrtimer_cancel(&hwc->hrtimer);\n\t}\n}\n\nstatic void perf_swevent_init_hrtimer(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (!is_sampling_event(event))\n\t\treturn;\n\n\thrtimer_init(&hwc->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\thwc->hrtimer.function = perf_swevent_hrtimer;\n\n\t/*\n\t * Since hrtimers have a fixed rate, we can do a static freq->period\n\t * mapping and avoid the whole period adjust feedback stuff.\n\t */\n\tif (event->attr.freq) {\n\t\tlong freq = event->attr.sample_freq;\n\n\t\tevent->attr.sample_period = NSEC_PER_SEC / freq;\n\t\thwc->sample_period = event->attr.sample_period;\n\t\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\t\tevent->attr.freq = 0;\n\t}\n}\n\n/*\n * Software event: cpu wall time clock\n */\n\nstatic void cpu_clock_event_update(struct perf_event *event)\n{\n\ts64 prev;\n\tu64 now;\n\n\tnow = local_clock();\n\tprev = local64_xchg(&event->hw.prev_count, now);\n\tlocal64_add(now - prev, &event->count);\n}\n\nstatic void cpu_clock_event_start(struct perf_event *event, int flags)\n{\n\tlocal64_set(&event->hw.prev_count, local_clock());\n\tperf_swevent_start_hrtimer(event);\n}\n\nstatic void cpu_clock_event_stop(struct perf_event *event, int flags)\n{\n\tperf_swevent_cancel_hrtimer(event);\n\tcpu_clock_event_update(event);\n}\n\nstatic int cpu_clock_event_add(struct perf_event *event, int flags)\n{\n\tif (flags & PERF_EF_START)\n\t\tcpu_clock_event_start(event, flags);\n\n\treturn 0;\n}\n\nstatic void cpu_clock_event_del(struct perf_event *event, int flags)\n{\n\tcpu_clock_event_stop(event, flags);\n}\n\nstatic void cpu_clock_event_read(struct perf_event *event)\n{\n\tcpu_clock_event_update(event);\n}\n\nstatic int cpu_clock_event_init(struct perf_event *event)\n{\n\tif (event->attr.type != PERF_TYPE_SOFTWARE)\n\t\treturn -ENOENT;\n\n\tif (event->attr.config != PERF_COUNT_SW_CPU_CLOCK)\n\t\treturn -ENOENT;\n\n\tperf_swevent_init_hrtimer(event);\n\n\treturn 0;\n}\n\nstatic struct pmu perf_cpu_clock = {\n\t.task_ctx_nr\t= perf_sw_context,\n\n\t.event_init\t= cpu_clock_event_init,\n\t.add\t\t= cpu_clock_event_add,\n\t.del\t\t= cpu_clock_event_del,\n\t.start\t\t= cpu_clock_event_start,\n\t.stop\t\t= cpu_clock_event_stop,\n\t.read\t\t= cpu_clock_event_read,\n};\n\n/*\n * Software event: task time clock\n */\n\nstatic void task_clock_event_update(struct perf_event *event, u64 now)\n{\n\tu64 prev;\n\ts64 delta;\n\n\tprev = local64_xchg(&event->hw.prev_count, now);\n\tdelta = now - prev;\n\tlocal64_add(delta, &event->count);\n}\n\nstatic void task_clock_event_start(struct perf_event *event, int flags)\n{\n\tlocal64_set(&event->hw.prev_count, event->ctx->time);\n\tperf_swevent_start_hrtimer(event);\n}\n\nstatic void task_clock_event_stop(struct perf_event *event, int flags)\n{\n\tperf_swevent_cancel_hrtimer(event);\n\ttask_clock_event_update(event, event->ctx->time);\n}\n\nstatic int task_clock_event_add(struct perf_event *event, int flags)\n{\n\tif (flags & PERF_EF_START)\n\t\ttask_clock_event_start(event, flags);\n\n\treturn 0;\n}\n\nstatic void task_clock_event_del(struct perf_event *event, int flags)\n{\n\ttask_clock_event_stop(event, PERF_EF_UPDATE);\n}\n\nstatic void task_clock_event_read(struct perf_event *event)\n{\n\tu64 now = perf_clock();\n\tu64 delta = now - event->ctx->timestamp;\n\tu64 time = event->ctx->time + delta;\n\n\ttask_clock_event_update(event, time);\n}\n\nstatic int task_clock_event_init(struct perf_event *event)\n{\n\tif (event->attr.type != PERF_TYPE_SOFTWARE)\n\t\treturn -ENOENT;\n\n\tif (event->attr.config != PERF_COUNT_SW_TASK_CLOCK)\n\t\treturn -ENOENT;\n\n\tperf_swevent_init_hrtimer(event);\n\n\treturn 0;\n}\n\nstatic struct pmu perf_task_clock = {\n\t.task_ctx_nr\t= perf_sw_context,\n\n\t.event_init\t= task_clock_event_init,\n\t.add\t\t= task_clock_event_add,\n\t.del\t\t= task_clock_event_del,\n\t.start\t\t= task_clock_event_start,\n\t.stop\t\t= task_clock_event_stop,\n\t.read\t\t= task_clock_event_read,\n};\n\nstatic void perf_pmu_nop_void(struct pmu *pmu)\n{\n}\n\nstatic int perf_pmu_nop_int(struct pmu *pmu)\n{\n\treturn 0;\n}\n\nstatic void perf_pmu_start_txn(struct pmu *pmu)\n{\n\tperf_pmu_disable(pmu);\n}\n\nstatic int perf_pmu_commit_txn(struct pmu *pmu)\n{\n\tperf_pmu_enable(pmu);\n\treturn 0;\n}\n\nstatic void perf_pmu_cancel_txn(struct pmu *pmu)\n{\n\tperf_pmu_enable(pmu);\n}\n\n/*\n * Ensures all contexts with the same task_ctx_nr have the same\n * pmu_cpu_context too.\n */\nstatic void *find_pmu_context(int ctxn)\n{\n\tstruct pmu *pmu;\n\n\tif (ctxn < 0)\n\t\treturn NULL;\n\n\tlist_for_each_entry(pmu, &pmus, entry) {\n\t\tif (pmu->task_ctx_nr == ctxn)\n\t\t\treturn pmu->pmu_cpu_context;\n\t}\n\n\treturn NULL;\n}\n\nstatic void update_pmu_context(struct pmu *pmu, struct pmu *old_pmu)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct perf_cpu_context *cpuctx;\n\n\t\tcpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);\n\n\t\tif (cpuctx->active_pmu == old_pmu)\n\t\t\tcpuctx->active_pmu = pmu;\n\t}\n}\n\nstatic void free_pmu_context(struct pmu *pmu)\n{\n\tstruct pmu *i;\n\n\tmutex_lock(&pmus_lock);\n\t/*\n\t * Like a real lame refcount.\n\t */\n\tlist_for_each_entry(i, &pmus, entry) {\n\t\tif (i->pmu_cpu_context == pmu->pmu_cpu_context) {\n\t\t\tupdate_pmu_context(i, pmu);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tfree_percpu(pmu->pmu_cpu_context);\nout:\n\tmutex_unlock(&pmus_lock);\n}\nstatic struct idr pmu_idr;\n\nstatic ssize_t\ntype_show(struct device *dev, struct device_attribute *attr, char *page)\n{\n\tstruct pmu *pmu = dev_get_drvdata(dev);\n\n\treturn snprintf(page, PAGE_SIZE-1, \"%d\\n\", pmu->type);\n}\n\nstatic struct device_attribute pmu_dev_attrs[] = {\n       __ATTR_RO(type),\n       __ATTR_NULL,\n};\n\nstatic int pmu_bus_running;\nstatic struct bus_type pmu_bus = {\n\t.name\t\t= \"event_source\",\n\t.dev_attrs\t= pmu_dev_attrs,\n};\n\nstatic void pmu_dev_release(struct device *dev)\n{\n\tkfree(dev);\n}\n\nstatic int pmu_dev_alloc(struct pmu *pmu)\n{\n\tint ret = -ENOMEM;\n\n\tpmu->dev = kzalloc(sizeof(struct device), GFP_KERNEL);\n\tif (!pmu->dev)\n\t\tgoto out;\n\n\tdevice_initialize(pmu->dev);\n\tret = dev_set_name(pmu->dev, \"%s\", pmu->name);\n\tif (ret)\n\t\tgoto free_dev;\n\n\tdev_set_drvdata(pmu->dev, pmu);\n\tpmu->dev->bus = &pmu_bus;\n\tpmu->dev->release = pmu_dev_release;\n\tret = device_add(pmu->dev);\n\tif (ret)\n\t\tgoto free_dev;\n\nout:\n\treturn ret;\n\nfree_dev:\n\tput_device(pmu->dev);\n\tgoto out;\n}\n\nstatic struct lock_class_key cpuctx_mutex;\nstatic struct lock_class_key cpuctx_lock;\n\nint perf_pmu_register(struct pmu *pmu, char *name, int type)\n{\n\tint cpu, ret;\n\n\tmutex_lock(&pmus_lock);\n\tret = -ENOMEM;\n\tpmu->pmu_disable_count = alloc_percpu(int);\n\tif (!pmu->pmu_disable_count)\n\t\tgoto unlock;\n\n\tpmu->type = -1;\n\tif (!name)\n\t\tgoto skip_type;\n\tpmu->name = name;\n\n\tif (type < 0) {\n\t\tint err = idr_pre_get(&pmu_idr, GFP_KERNEL);\n\t\tif (!err)\n\t\t\tgoto free_pdc;\n\n\t\terr = idr_get_new_above(&pmu_idr, pmu, PERF_TYPE_MAX, &type);\n\t\tif (err) {\n\t\t\tret = err;\n\t\t\tgoto free_pdc;\n\t\t}\n\t}\n\tpmu->type = type;\n\n\tif (pmu_bus_running) {\n\t\tret = pmu_dev_alloc(pmu);\n\t\tif (ret)\n\t\t\tgoto free_idr;\n\t}\n\nskip_type:\n\tpmu->pmu_cpu_context = find_pmu_context(pmu->task_ctx_nr);\n\tif (pmu->pmu_cpu_context)\n\t\tgoto got_cpu_context;\n\n\tpmu->pmu_cpu_context = alloc_percpu(struct perf_cpu_context);\n\tif (!pmu->pmu_cpu_context)\n\t\tgoto free_dev;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct perf_cpu_context *cpuctx;\n\n\t\tcpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);\n\t\t__perf_event_init_context(&cpuctx->ctx);\n\t\tlockdep_set_class(&cpuctx->ctx.mutex, &cpuctx_mutex);\n\t\tlockdep_set_class(&cpuctx->ctx.lock, &cpuctx_lock);\n\t\tcpuctx->ctx.type = cpu_context;\n\t\tcpuctx->ctx.pmu = pmu;\n\t\tcpuctx->jiffies_interval = 1;\n\t\tINIT_LIST_HEAD(&cpuctx->rotation_list);\n\t\tcpuctx->active_pmu = pmu;\n\t}\n\ngot_cpu_context:\n\tif (!pmu->start_txn) {\n\t\tif (pmu->pmu_enable) {\n\t\t\t/*\n\t\t\t * If we have pmu_enable/pmu_disable calls, install\n\t\t\t * transaction stubs that use that to try and batch\n\t\t\t * hardware accesses.\n\t\t\t */\n\t\t\tpmu->start_txn  = perf_pmu_start_txn;\n\t\t\tpmu->commit_txn = perf_pmu_commit_txn;\n\t\t\tpmu->cancel_txn = perf_pmu_cancel_txn;\n\t\t} else {\n\t\t\tpmu->start_txn  = perf_pmu_nop_void;\n\t\t\tpmu->commit_txn = perf_pmu_nop_int;\n\t\t\tpmu->cancel_txn = perf_pmu_nop_void;\n\t\t}\n\t}\n\n\tif (!pmu->pmu_enable) {\n\t\tpmu->pmu_enable  = perf_pmu_nop_void;\n\t\tpmu->pmu_disable = perf_pmu_nop_void;\n\t}\n\n\tlist_add_rcu(&pmu->entry, &pmus);\n\tret = 0;\nunlock:\n\tmutex_unlock(&pmus_lock);\n\n\treturn ret;\n\nfree_dev:\n\tdevice_del(pmu->dev);\n\tput_device(pmu->dev);\n\nfree_idr:\n\tif (pmu->type >= PERF_TYPE_MAX)\n\t\tidr_remove(&pmu_idr, pmu->type);\n\nfree_pdc:\n\tfree_percpu(pmu->pmu_disable_count);\n\tgoto unlock;\n}\n\nvoid perf_pmu_unregister(struct pmu *pmu)\n{\n\tmutex_lock(&pmus_lock);\n\tlist_del_rcu(&pmu->entry);\n\tmutex_unlock(&pmus_lock);\n\n\t/*\n\t * We dereference the pmu list under both SRCU and regular RCU, so\n\t * synchronize against both of those.\n\t */\n\tsynchronize_srcu(&pmus_srcu);\n\tsynchronize_rcu();\n\n\tfree_percpu(pmu->pmu_disable_count);\n\tif (pmu->type >= PERF_TYPE_MAX)\n\t\tidr_remove(&pmu_idr, pmu->type);\n\tdevice_del(pmu->dev);\n\tput_device(pmu->dev);\n\tfree_pmu_context(pmu);\n}\n\nstruct pmu *perf_init_event(struct perf_event *event)\n{\n\tstruct pmu *pmu = NULL;\n\tint idx;\n\tint ret;\n\n\tidx = srcu_read_lock(&pmus_srcu);\n\n\trcu_read_lock();\n\tpmu = idr_find(&pmu_idr, event->attr.type);\n\trcu_read_unlock();\n\tif (pmu) {\n\t\tret = pmu->event_init(event);\n\t\tif (ret)\n\t\t\tpmu = ERR_PTR(ret);\n\t\tgoto unlock;\n\t}\n\n\tlist_for_each_entry_rcu(pmu, &pmus, entry) {\n\t\tret = pmu->event_init(event);\n\t\tif (!ret)\n\t\t\tgoto unlock;\n\n\t\tif (ret != -ENOENT) {\n\t\t\tpmu = ERR_PTR(ret);\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\tpmu = ERR_PTR(-ENOENT);\nunlock:\n\tsrcu_read_unlock(&pmus_srcu, idx);\n\n\treturn pmu;\n}\n\n/*\n * Allocate and initialize a event structure\n */\nstatic struct perf_event *\nperf_event_alloc(struct perf_event_attr *attr, int cpu,\n\t\t struct task_struct *task,\n\t\t struct perf_event *group_leader,\n\t\t struct perf_event *parent_event,\n\t\t perf_overflow_handler_t overflow_handler)\n{\n\tstruct pmu *pmu;\n\tstruct perf_event *event;\n\tstruct hw_perf_event *hwc;\n\tlong err;\n\n\tif ((unsigned)cpu >= nr_cpu_ids) {\n\t\tif (!task || cpu != -1)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tevent = kzalloc(sizeof(*event), GFP_KERNEL);\n\tif (!event)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/*\n\t * Single events are their own group leaders, with an\n\t * empty sibling list:\n\t */\n\tif (!group_leader)\n\t\tgroup_leader = event;\n\n\tmutex_init(&event->child_mutex);\n\tINIT_LIST_HEAD(&event->child_list);\n\n\tINIT_LIST_HEAD(&event->group_entry);\n\tINIT_LIST_HEAD(&event->event_entry);\n\tINIT_LIST_HEAD(&event->sibling_list);\n\tinit_waitqueue_head(&event->waitq);\n\tinit_irq_work(&event->pending, perf_pending_event);\n\n\tmutex_init(&event->mmap_mutex);\n\n\tevent->cpu\t\t= cpu;\n\tevent->attr\t\t= *attr;\n\tevent->group_leader\t= group_leader;\n\tevent->pmu\t\t= NULL;\n\tevent->oncpu\t\t= -1;\n\n\tevent->parent\t\t= parent_event;\n\n\tevent->ns\t\t= get_pid_ns(current->nsproxy->pid_ns);\n\tevent->id\t\t= atomic64_inc_return(&perf_event_id);\n\n\tevent->state\t\t= PERF_EVENT_STATE_INACTIVE;\n\n\tif (task) {\n\t\tevent->attach_state = PERF_ATTACH_TASK;\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\t\t/*\n\t\t * hw_breakpoint is a bit difficult here..\n\t\t */\n\t\tif (attr->type == PERF_TYPE_BREAKPOINT)\n\t\t\tevent->hw.bp_target = task;\n#endif\n\t}\n\n\tif (!overflow_handler && parent_event)\n\t\toverflow_handler = parent_event->overflow_handler;\n\n\tevent->overflow_handler\t= overflow_handler;\n\n\tif (attr->disabled)\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\n\tpmu = NULL;\n\n\thwc = &event->hw;\n\thwc->sample_period = attr->sample_period;\n\tif (attr->freq && attr->sample_freq)\n\t\thwc->sample_period = 1;\n\thwc->last_period = hwc->sample_period;\n\n\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\n\t/*\n\t * we currently do not support PERF_FORMAT_GROUP on inherited events\n\t */\n\tif (attr->inherit && (attr->read_format & PERF_FORMAT_GROUP))\n\t\tgoto done;\n\n\tpmu = perf_init_event(event);\n\ndone:\n\terr = 0;\n\tif (!pmu)\n\t\terr = -EINVAL;\n\telse if (IS_ERR(pmu))\n\t\terr = PTR_ERR(pmu);\n\n\tif (err) {\n\t\tif (event->ns)\n\t\t\tput_pid_ns(event->ns);\n\t\tkfree(event);\n\t\treturn ERR_PTR(err);\n\t}\n\n\tevent->pmu = pmu;\n\n\tif (!event->parent) {\n\t\tif (event->attach_state & PERF_ATTACH_TASK)\n\t\t\tjump_label_inc(&perf_sched_events);\n\t\tif (event->attr.mmap || event->attr.mmap_data)\n\t\t\tatomic_inc(&nr_mmap_events);\n\t\tif (event->attr.comm)\n\t\t\tatomic_inc(&nr_comm_events);\n\t\tif (event->attr.task)\n\t\t\tatomic_inc(&nr_task_events);\n\t\tif (event->attr.sample_type & PERF_SAMPLE_CALLCHAIN) {\n\t\t\terr = get_callchain_buffers();\n\t\t\tif (err) {\n\t\t\t\tfree_event(event);\n\t\t\t\treturn ERR_PTR(err);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn event;\n}\n\nstatic int perf_copy_attr(struct perf_event_attr __user *uattr,\n\t\t\t  struct perf_event_attr *attr)\n{\n\tu32 size;\n\tint ret;\n\n\tif (!access_ok(VERIFY_WRITE, uattr, PERF_ATTR_SIZE_VER0))\n\t\treturn -EFAULT;\n\n\t/*\n\t * zero the full structure, so that a short copy will be nice.\n\t */\n\tmemset(attr, 0, sizeof(*attr));\n\n\tret = get_user(size, &uattr->size);\n\tif (ret)\n\t\treturn ret;\n\n\tif (size > PAGE_SIZE)\t/* silly large */\n\t\tgoto err_size;\n\n\tif (!size)\t\t/* abi compat */\n\t\tsize = PERF_ATTR_SIZE_VER0;\n\n\tif (size < PERF_ATTR_SIZE_VER0)\n\t\tgoto err_size;\n\n\t/*\n\t * If we're handed a bigger struct than we know of,\n\t * ensure all the unknown bits are 0 - i.e. new\n\t * user-space does not rely on any kernel feature\n\t * extensions we dont know about yet.\n\t */\n\tif (size > sizeof(*attr)) {\n\t\tunsigned char __user *addr;\n\t\tunsigned char __user *end;\n\t\tunsigned char val;\n\n\t\taddr = (void __user *)uattr + sizeof(*attr);\n\t\tend  = (void __user *)uattr + size;\n\n\t\tfor (; addr < end; addr++) {\n\t\t\tret = get_user(val, addr);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tif (val)\n\t\t\t\tgoto err_size;\n\t\t}\n\t\tsize = sizeof(*attr);\n\t}\n\n\tret = copy_from_user(attr, uattr, size);\n\tif (ret)\n\t\treturn -EFAULT;\n\n\t/*\n\t * If the type exists, the corresponding creation will verify\n\t * the attr->config.\n\t */\n\tif (attr->type >= PERF_TYPE_MAX)\n\t\treturn -EINVAL;\n\n\tif (attr->__reserved_1)\n\t\treturn -EINVAL;\n\n\tif (attr->sample_type & ~(PERF_SAMPLE_MAX-1))\n\t\treturn -EINVAL;\n\n\tif (attr->read_format & ~(PERF_FORMAT_MAX-1))\n\t\treturn -EINVAL;\n\nout:\n\treturn ret;\n\nerr_size:\n\tput_user(sizeof(*attr), &uattr->size);\n\tret = -E2BIG;\n\tgoto out;\n}\n\nstatic int\nperf_event_set_output(struct perf_event *event, struct perf_event *output_event)\n{\n\tstruct ring_buffer *rb = NULL, *old_rb = NULL;\n\tint ret = -EINVAL;\n\n\tif (!output_event)\n\t\tgoto set;\n\n\t/* don't allow circular references */\n\tif (event == output_event)\n\t\tgoto out;\n\n\t/*\n\t * Don't allow cross-cpu buffers\n\t */\n\tif (output_event->cpu != event->cpu)\n\t\tgoto out;\n\n\t/*\n\t * If its not a per-cpu rb, it must be the same task.\n\t */\n\tif (output_event->cpu == -1 && output_event->ctx != event->ctx)\n\t\tgoto out;\n\nset:\n\tmutex_lock(&event->mmap_mutex);\n\t/* Can't redirect output if we've got an active mmap() */\n\tif (atomic_read(&event->mmap_count))\n\t\tgoto unlock;\n\n\tif (output_event) {\n\t\t/* get the rb we want to redirect to */\n\t\trb = ring_buffer_get(output_event);\n\t\tif (!rb)\n\t\t\tgoto unlock;\n\t}\n\n\told_rb = event->rb;\n\trcu_assign_pointer(event->rb, rb);\n\tret = 0;\nunlock:\n\tmutex_unlock(&event->mmap_mutex);\n\n\tif (old_rb)\n\t\tring_buffer_put(old_rb);\nout:\n\treturn ret;\n}\n\n/**\n * sys_perf_event_open - open a performance event, associate it to a task/cpu\n *\n * @attr_uptr:\tevent_id type attributes for monitoring/sampling\n * @pid:\t\ttarget pid\n * @cpu:\t\ttarget cpu\n * @group_fd:\t\tgroup leader event fd\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct file *group_file = NULL;\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint fput_needed = 0;\n\tint err;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tevent_fd = get_unused_fd_flags(O_RDWR);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\tgroup_leader = perf_fget_light(group_fd, &fput_needed);\n\t\tif (IS_ERR(group_leader)) {\n\t\t\terr = PTR_ERR(group_leader);\n\t\t\tgoto err_fd;\n\t\t}\n\t\tgroup_file = group_leader->filp;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_task;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err)\n\t\t\tgoto err_alloc;\n\t\t/*\n\t\t * one more event:\n\t\t * - that has cgroup constraint on event->cpu\n\t\t * - that may need work on context switch\n\t\t */\n\t\tatomic_inc(&per_cpu(perf_cgroup_events, event->cpu));\n\t\tjump_label_inc(&perf_sched_events);\n\t}\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\tif (group_leader->ctx->type != ctx->type)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event, O_RDWR);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tevent->filp = event_file;\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tperf_install_in_context(ctx, group_leader, cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, cpu);\n\t++ctx->generation;\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfput_light(group_file, fput_needed);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfput_light(group_file, fput_needed);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n\n/**\n * perf_event_create_kernel_counter\n *\n * @attr: attributes of the counter to create\n * @cpu: cpu in which the counter is bound\n * @task: task to profile (NULL for percpu)\n */\nstruct perf_event *\nperf_event_create_kernel_counter(struct perf_event_attr *attr, int cpu,\n\t\t\t\t struct task_struct *task,\n\t\t\t\t perf_overflow_handler_t overflow_handler)\n{\n\tstruct perf_event_context *ctx;\n\tstruct perf_event *event;\n\tint err;\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\n\tevent = perf_event_alloc(attr, cpu, task, NULL, NULL, overflow_handler);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err;\n\t}\n\n\tctx = find_get_context(event->pmu, task, cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_free;\n\t}\n\n\tevent->filp = NULL;\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\tperf_install_in_context(ctx, event, cpu);\n\t++ctx->generation;\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\treturn event;\n\nerr_free:\n\tfree_event(event);\nerr:\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(perf_event_create_kernel_counter);\n\nstatic void sync_child_event(struct perf_event *child_event,\n\t\t\t       struct task_struct *child)\n{\n\tstruct perf_event *parent_event = child_event->parent;\n\tu64 child_val;\n\n\tif (child_event->attr.inherit_stat)\n\t\tperf_event_read_event(child_event, child);\n\n\tchild_val = perf_event_count(child_event);\n\n\t/*\n\t * Add back the child's count to the parent's count:\n\t */\n\tatomic64_add(child_val, &parent_event->child_count);\n\tatomic64_add(child_event->total_time_enabled,\n\t\t     &parent_event->child_total_time_enabled);\n\tatomic64_add(child_event->total_time_running,\n\t\t     &parent_event->child_total_time_running);\n\n\t/*\n\t * Remove this event from the parent's list\n\t */\n\tWARN_ON_ONCE(parent_event->ctx->parent_ctx);\n\tmutex_lock(&parent_event->child_mutex);\n\tlist_del_init(&child_event->child_list);\n\tmutex_unlock(&parent_event->child_mutex);\n\n\t/*\n\t * Release the parent event, if this was the last\n\t * reference to it.\n\t */\n\tfput(parent_event->filp);\n}\n\nstatic void\n__perf_event_exit_task(struct perf_event *child_event,\n\t\t\t struct perf_event_context *child_ctx,\n\t\t\t struct task_struct *child)\n{\n\tif (child_event->parent) {\n\t\traw_spin_lock_irq(&child_ctx->lock);\n\t\tperf_group_detach(child_event);\n\t\traw_spin_unlock_irq(&child_ctx->lock);\n\t}\n\n\tperf_remove_from_context(child_event);\n\n\t/*\n\t * It can happen that the parent exits first, and has events\n\t * that are still around due to the child reference. These\n\t * events need to be zapped.\n\t */\n\tif (child_event->parent) {\n\t\tsync_child_event(child_event, child);\n\t\tfree_event(child_event);\n\t}\n}\n\nstatic void perf_event_exit_task_context(struct task_struct *child, int ctxn)\n{\n\tstruct perf_event *child_event, *tmp;\n\tstruct perf_event_context *child_ctx;\n\tunsigned long flags;\n\n\tif (likely(!child->perf_event_ctxp[ctxn])) {\n\t\tperf_event_task(child, NULL, 0);\n\t\treturn;\n\t}\n\n\tlocal_irq_save(flags);\n\t/*\n\t * We can't reschedule here because interrupts are disabled,\n\t * and either child is current or it is a task that can't be\n\t * scheduled, so we are now safe from rescheduling changing\n\t * our context.\n\t */\n\tchild_ctx = rcu_dereference_raw(child->perf_event_ctxp[ctxn]);\n\n\t/*\n\t * Take the context lock here so that if find_get_context is\n\t * reading child->perf_event_ctxp, we wait until it has\n\t * incremented the context's refcount before we do put_ctx below.\n\t */\n\traw_spin_lock(&child_ctx->lock);\n\ttask_ctx_sched_out(child_ctx);\n\tchild->perf_event_ctxp[ctxn] = NULL;\n\t/*\n\t * If this context is a clone; unclone it so it can't get\n\t * swapped to another process while we're removing all\n\t * the events from it.\n\t */\n\tunclone_ctx(child_ctx);\n\tupdate_context_time(child_ctx);\n\traw_spin_unlock_irqrestore(&child_ctx->lock, flags);\n\n\t/*\n\t * Report the task dead after unscheduling the events so that we\n\t * won't get any samples after PERF_RECORD_EXIT. We can however still\n\t * get a few PERF_RECORD_READ events.\n\t */\n\tperf_event_task(child, child_ctx, 0);\n\n\t/*\n\t * We can recurse on the same lock type through:\n\t *\n\t *   __perf_event_exit_task()\n\t *     sync_child_event()\n\t *       fput(parent_event->filp)\n\t *         perf_release()\n\t *           mutex_lock(&ctx->mutex)\n\t *\n\t * But since its the parent context it won't be the same instance.\n\t */\n\tmutex_lock(&child_ctx->mutex);\n\nagain:\n\tlist_for_each_entry_safe(child_event, tmp, &child_ctx->pinned_groups,\n\t\t\t\t group_entry)\n\t\t__perf_event_exit_task(child_event, child_ctx, child);\n\n\tlist_for_each_entry_safe(child_event, tmp, &child_ctx->flexible_groups,\n\t\t\t\t group_entry)\n\t\t__perf_event_exit_task(child_event, child_ctx, child);\n\n\t/*\n\t * If the last event was a group event, it will have appended all\n\t * its siblings to the list, but we obtained 'tmp' before that which\n\t * will still point to the list head terminating the iteration.\n\t */\n\tif (!list_empty(&child_ctx->pinned_groups) ||\n\t    !list_empty(&child_ctx->flexible_groups))\n\t\tgoto again;\n\n\tmutex_unlock(&child_ctx->mutex);\n\n\tput_ctx(child_ctx);\n}\n\n/*\n * When a child task exits, feed back event values to parent events.\n */\nvoid perf_event_exit_task(struct task_struct *child)\n{\n\tstruct perf_event *event, *tmp;\n\tint ctxn;\n\n\tmutex_lock(&child->perf_event_mutex);\n\tlist_for_each_entry_safe(event, tmp, &child->perf_event_list,\n\t\t\t\t owner_entry) {\n\t\tlist_del_init(&event->owner_entry);\n\n\t\t/*\n\t\t * Ensure the list deletion is visible before we clear\n\t\t * the owner, closes a race against perf_release() where\n\t\t * we need to serialize on the owner->perf_event_mutex.\n\t\t */\n\t\tsmp_wmb();\n\t\tevent->owner = NULL;\n\t}\n\tmutex_unlock(&child->perf_event_mutex);\n\n\tfor_each_task_context_nr(ctxn)\n\t\tperf_event_exit_task_context(child, ctxn);\n}\n\nstatic void perf_free_event(struct perf_event *event,\n\t\t\t    struct perf_event_context *ctx)\n{\n\tstruct perf_event *parent = event->parent;\n\n\tif (WARN_ON_ONCE(!parent))\n\t\treturn;\n\n\tmutex_lock(&parent->child_mutex);\n\tlist_del_init(&event->child_list);\n\tmutex_unlock(&parent->child_mutex);\n\n\tfput(parent->filp);\n\n\tperf_group_detach(event);\n\tlist_del_event(event, ctx);\n\tfree_event(event);\n}\n\n/*\n * free an unexposed, unused context as created by inheritance by\n * perf_event_init_task below, used by fork() in case of fail.\n */\nvoid perf_event_free_task(struct task_struct *task)\n{\n\tstruct perf_event_context *ctx;\n\tstruct perf_event *event, *tmp;\n\tint ctxn;\n\n\tfor_each_task_context_nr(ctxn) {\n\t\tctx = task->perf_event_ctxp[ctxn];\n\t\tif (!ctx)\n\t\t\tcontinue;\n\n\t\tmutex_lock(&ctx->mutex);\nagain:\n\t\tlist_for_each_entry_safe(event, tmp, &ctx->pinned_groups,\n\t\t\t\tgroup_entry)\n\t\t\tperf_free_event(event, ctx);\n\n\t\tlist_for_each_entry_safe(event, tmp, &ctx->flexible_groups,\n\t\t\t\tgroup_entry)\n\t\t\tperf_free_event(event, ctx);\n\n\t\tif (!list_empty(&ctx->pinned_groups) ||\n\t\t\t\t!list_empty(&ctx->flexible_groups))\n\t\t\tgoto again;\n\n\t\tmutex_unlock(&ctx->mutex);\n\n\t\tput_ctx(ctx);\n\t}\n}\n\nvoid perf_event_delayed_put(struct task_struct *task)\n{\n\tint ctxn;\n\n\tfor_each_task_context_nr(ctxn)\n\t\tWARN_ON_ONCE(task->perf_event_ctxp[ctxn]);\n}\n\n/*\n * inherit a event from parent task to child task:\n */\nstatic struct perf_event *\ninherit_event(struct perf_event *parent_event,\n\t      struct task_struct *parent,\n\t      struct perf_event_context *parent_ctx,\n\t      struct task_struct *child,\n\t      struct perf_event *group_leader,\n\t      struct perf_event_context *child_ctx)\n{\n\tstruct perf_event *child_event;\n\tunsigned long flags;\n\n\t/*\n\t * Instead of creating recursive hierarchies of events,\n\t * we link inherited events back to the original parent,\n\t * which has a filp for sure, which we use as the reference\n\t * count:\n\t */\n\tif (parent_event->parent)\n\t\tparent_event = parent_event->parent;\n\n\tchild_event = perf_event_alloc(&parent_event->attr,\n\t\t\t\t\t   parent_event->cpu,\n\t\t\t\t\t   child,\n\t\t\t\t\t   group_leader, parent_event,\n\t\t\t\t\t   NULL);\n\tif (IS_ERR(child_event))\n\t\treturn child_event;\n\tget_ctx(child_ctx);\n\n\t/*\n\t * Make the child state follow the state of the parent event,\n\t * not its attr.disabled bit.  We hold the parent's mutex,\n\t * so we won't race with perf_event_{en, dis}able_family.\n\t */\n\tif (parent_event->state >= PERF_EVENT_STATE_INACTIVE)\n\t\tchild_event->state = PERF_EVENT_STATE_INACTIVE;\n\telse\n\t\tchild_event->state = PERF_EVENT_STATE_OFF;\n\n\tif (parent_event->attr.freq) {\n\t\tu64 sample_period = parent_event->hw.sample_period;\n\t\tstruct hw_perf_event *hwc = &child_event->hw;\n\n\t\thwc->sample_period = sample_period;\n\t\thwc->last_period   = sample_period;\n\n\t\tlocal64_set(&hwc->period_left, sample_period);\n\t}\n\n\tchild_event->ctx = child_ctx;\n\tchild_event->overflow_handler = parent_event->overflow_handler;\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(child_event);\n\tperf_event__id_header_size(child_event);\n\n\t/*\n\t * Link it up in the child's context:\n\t */\n\traw_spin_lock_irqsave(&child_ctx->lock, flags);\n\tadd_event_to_ctx(child_event, child_ctx);\n\traw_spin_unlock_irqrestore(&child_ctx->lock, flags);\n\n\t/*\n\t * Get a reference to the parent filp - we will fput it\n\t * when the child event exits. This is safe to do because\n\t * we are in the parent and we know that the filp still\n\t * exists and has a nonzero count:\n\t */\n\tatomic_long_inc(&parent_event->filp->f_count);\n\n\t/*\n\t * Link this into the parent event's child list\n\t */\n\tWARN_ON_ONCE(parent_event->ctx->parent_ctx);\n\tmutex_lock(&parent_event->child_mutex);\n\tlist_add_tail(&child_event->child_list, &parent_event->child_list);\n\tmutex_unlock(&parent_event->child_mutex);\n\n\treturn child_event;\n}\n\nstatic int inherit_group(struct perf_event *parent_event,\n\t      struct task_struct *parent,\n\t      struct perf_event_context *parent_ctx,\n\t      struct task_struct *child,\n\t      struct perf_event_context *child_ctx)\n{\n\tstruct perf_event *leader;\n\tstruct perf_event *sub;\n\tstruct perf_event *child_ctr;\n\n\tleader = inherit_event(parent_event, parent, parent_ctx,\n\t\t\t\t child, NULL, child_ctx);\n\tif (IS_ERR(leader))\n\t\treturn PTR_ERR(leader);\n\tlist_for_each_entry(sub, &parent_event->sibling_list, group_entry) {\n\t\tchild_ctr = inherit_event(sub, parent, parent_ctx,\n\t\t\t\t\t    child, leader, child_ctx);\n\t\tif (IS_ERR(child_ctr))\n\t\t\treturn PTR_ERR(child_ctr);\n\t}\n\treturn 0;\n}\n\nstatic int\ninherit_task_group(struct perf_event *event, struct task_struct *parent,\n\t\t   struct perf_event_context *parent_ctx,\n\t\t   struct task_struct *child, int ctxn,\n\t\t   int *inherited_all)\n{\n\tint ret;\n\tstruct perf_event_context *child_ctx;\n\n\tif (!event->attr.inherit) {\n\t\t*inherited_all = 0;\n\t\treturn 0;\n\t}\n\n\tchild_ctx = child->perf_event_ctxp[ctxn];\n\tif (!child_ctx) {\n\t\t/*\n\t\t * This is executed from the parent task context, so\n\t\t * inherit events that have been marked for cloning.\n\t\t * First allocate and initialize a context for the\n\t\t * child.\n\t\t */\n\n\t\tchild_ctx = alloc_perf_context(event->pmu, child);\n\t\tif (!child_ctx)\n\t\t\treturn -ENOMEM;\n\n\t\tchild->perf_event_ctxp[ctxn] = child_ctx;\n\t}\n\n\tret = inherit_group(event, parent, parent_ctx,\n\t\t\t    child, child_ctx);\n\n\tif (ret)\n\t\t*inherited_all = 0;\n\n\treturn ret;\n}\n\n/*\n * Initialize the perf_event context in task_struct\n */\nint perf_event_init_context(struct task_struct *child, int ctxn)\n{\n\tstruct perf_event_context *child_ctx, *parent_ctx;\n\tstruct perf_event_context *cloned_ctx;\n\tstruct perf_event *event;\n\tstruct task_struct *parent = current;\n\tint inherited_all = 1;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tif (likely(!parent->perf_event_ctxp[ctxn]))\n\t\treturn 0;\n\n\t/*\n\t * If the parent's context is a clone, pin it so it won't get\n\t * swapped under us.\n\t */\n\tparent_ctx = perf_pin_task_context(parent, ctxn);\n\n\t/*\n\t * No need to check if parent_ctx != NULL here; since we saw\n\t * it non-NULL earlier, the only reason for it to become NULL\n\t * is if we exit, and since we're currently in the middle of\n\t * a fork we can't be exiting at the same time.\n\t */\n\n\t/*\n\t * Lock the parent list. No need to lock the child - not PID\n\t * hashed yet and not running, so nobody can access it.\n\t */\n\tmutex_lock(&parent_ctx->mutex);\n\n\t/*\n\t * We dont have to disable NMIs - we are only looking at\n\t * the list, not manipulating it:\n\t */\n\tlist_for_each_entry(event, &parent_ctx->pinned_groups, group_entry) {\n\t\tret = inherit_task_group(event, parent, parent_ctx,\n\t\t\t\t\t child, ctxn, &inherited_all);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\t/*\n\t * We can't hold ctx->lock when iterating the ->flexible_group list due\n\t * to allocations, but we need to prevent rotation because\n\t * rotate_ctx() will change the list from interrupt context.\n\t */\n\traw_spin_lock_irqsave(&parent_ctx->lock, flags);\n\tparent_ctx->rotate_disable = 1;\n\traw_spin_unlock_irqrestore(&parent_ctx->lock, flags);\n\n\tlist_for_each_entry(event, &parent_ctx->flexible_groups, group_entry) {\n\t\tret = inherit_task_group(event, parent, parent_ctx,\n\t\t\t\t\t child, ctxn, &inherited_all);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\traw_spin_lock_irqsave(&parent_ctx->lock, flags);\n\tparent_ctx->rotate_disable = 0;\n\n\tchild_ctx = child->perf_event_ctxp[ctxn];\n\n\tif (child_ctx && inherited_all) {\n\t\t/*\n\t\t * Mark the child context as a clone of the parent\n\t\t * context, or of whatever the parent is a clone of.\n\t\t *\n\t\t * Note that if the parent is a clone, the holding of\n\t\t * parent_ctx->lock avoids it from being uncloned.\n\t\t */\n\t\tcloned_ctx = parent_ctx->parent_ctx;\n\t\tif (cloned_ctx) {\n\t\t\tchild_ctx->parent_ctx = cloned_ctx;\n\t\t\tchild_ctx->parent_gen = parent_ctx->parent_gen;\n\t\t} else {\n\t\t\tchild_ctx->parent_ctx = parent_ctx;\n\t\t\tchild_ctx->parent_gen = parent_ctx->generation;\n\t\t}\n\t\tget_ctx(child_ctx->parent_ctx);\n\t}\n\n\traw_spin_unlock_irqrestore(&parent_ctx->lock, flags);\n\tmutex_unlock(&parent_ctx->mutex);\n\n\tperf_unpin_context(parent_ctx);\n\tput_ctx(parent_ctx);\n\n\treturn ret;\n}\n\n/*\n * Initialize the perf_event context in task_struct\n */\nint perf_event_init_task(struct task_struct *child)\n{\n\tint ctxn, ret;\n\n\tmemset(child->perf_event_ctxp, 0, sizeof(child->perf_event_ctxp));\n\tmutex_init(&child->perf_event_mutex);\n\tINIT_LIST_HEAD(&child->perf_event_list);\n\n\tfor_each_task_context_nr(ctxn) {\n\t\tret = perf_event_init_context(child, ctxn);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void __init perf_event_init_all_cpus(void)\n{\n\tstruct swevent_htable *swhash;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tswhash = &per_cpu(swevent_htable, cpu);\n\t\tmutex_init(&swhash->hlist_mutex);\n\t\tINIT_LIST_HEAD(&per_cpu(rotation_list, cpu));\n\t}\n}\n\nstatic void __cpuinit perf_event_init_cpu(int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tif (swhash->hlist_refcount > 0) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc_node(sizeof(*hlist), GFP_KERNEL, cpu_to_node(cpu));\n\t\tWARN_ON(!hlist);\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tmutex_unlock(&swhash->hlist_mutex);\n}\n\n#if defined CONFIG_HOTPLUG_CPU || defined CONFIG_KEXEC\nstatic void perf_pmu_rotate_stop(struct pmu *pmu)\n{\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(pmu->pmu_cpu_context);\n\n\tWARN_ON(!irqs_disabled());\n\n\tlist_del_init(&cpuctx->rotation_list);\n}\n\nstatic void __perf_event_exit_context(void *__info)\n{\n\tstruct perf_event_context *ctx = __info;\n\tstruct perf_event *event, *tmp;\n\n\tperf_pmu_rotate_stop(ctx->pmu);\n\n\tlist_for_each_entry_safe(event, tmp, &ctx->pinned_groups, group_entry)\n\t\t__perf_remove_from_context(event);\n\tlist_for_each_entry_safe(event, tmp, &ctx->flexible_groups, group_entry)\n\t\t__perf_remove_from_context(event);\n}\n\nstatic void perf_event_exit_cpu_context(int cpu)\n{\n\tstruct perf_event_context *ctx;\n\tstruct pmu *pmu;\n\tint idx;\n\n\tidx = srcu_read_lock(&pmus_srcu);\n\tlist_for_each_entry_rcu(pmu, &pmus, entry) {\n\t\tctx = &per_cpu_ptr(pmu->pmu_cpu_context, cpu)->ctx;\n\n\t\tmutex_lock(&ctx->mutex);\n\t\tsmp_call_function_single(cpu, __perf_event_exit_context, ctx, 1);\n\t\tmutex_unlock(&ctx->mutex);\n\t}\n\tsrcu_read_unlock(&pmus_srcu, idx);\n}\n\nstatic void perf_event_exit_cpu(int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tswevent_hlist_release(swhash);\n\tmutex_unlock(&swhash->hlist_mutex);\n\n\tperf_event_exit_cpu_context(cpu);\n}\n#else\nstatic inline void perf_event_exit_cpu(int cpu) { }\n#endif\n\nstatic int\nperf_reboot(struct notifier_block *notifier, unsigned long val, void *v)\n{\n\tint cpu;\n\n\tfor_each_online_cpu(cpu)\n\t\tperf_event_exit_cpu(cpu);\n\n\treturn NOTIFY_OK;\n}\n\n/*\n * Run the perf reboot notifier at the very last possible moment so that\n * the generic watchdog code runs as long as possible.\n */\nstatic struct notifier_block perf_reboot_notifier = {\n\t.notifier_call = perf_reboot,\n\t.priority = INT_MIN,\n};\n\nstatic int __cpuinit\nperf_cpu_notify(struct notifier_block *self, unsigned long action, void *hcpu)\n{\n\tunsigned int cpu = (long)hcpu;\n\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\n\tcase CPU_UP_PREPARE:\n\tcase CPU_DOWN_FAILED:\n\t\tperf_event_init_cpu(cpu);\n\t\tbreak;\n\n\tcase CPU_UP_CANCELED:\n\tcase CPU_DOWN_PREPARE:\n\t\tperf_event_exit_cpu(cpu);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nvoid __init perf_event_init(void)\n{\n\tint ret;\n\n\tidr_init(&pmu_idr);\n\n\tperf_event_init_all_cpus();\n\tinit_srcu_struct(&pmus_srcu);\n\tperf_pmu_register(&perf_swevent, \"software\", PERF_TYPE_SOFTWARE);\n\tperf_pmu_register(&perf_cpu_clock, NULL, -1);\n\tperf_pmu_register(&perf_task_clock, NULL, -1);\n\tperf_tp_register();\n\tperf_cpu_notifier(perf_cpu_notify);\n\tregister_reboot_notifier(&perf_reboot_notifier);\n\n\tret = init_hw_breakpoint();\n\tWARN(ret, \"hw_breakpoint initialization failed with: %d\", ret);\n}\n\nstatic int __init perf_event_sysfs_init(void)\n{\n\tstruct pmu *pmu;\n\tint ret;\n\n\tmutex_lock(&pmus_lock);\n\n\tret = bus_register(&pmu_bus);\n\tif (ret)\n\t\tgoto unlock;\n\n\tlist_for_each_entry(pmu, &pmus, entry) {\n\t\tif (!pmu->name || pmu->type < 0)\n\t\t\tcontinue;\n\n\t\tret = pmu_dev_alloc(pmu);\n\t\tWARN(ret, \"Failed to register pmu: %s, reason %d\\n\", pmu->name, ret);\n\t}\n\tpmu_bus_running = 1;\n\tret = 0;\n\nunlock:\n\tmutex_unlock(&pmus_lock);\n\n\treturn ret;\n}\ndevice_initcall(perf_event_sysfs_init);\n\n#ifdef CONFIG_CGROUP_PERF\nstatic struct cgroup_subsys_state *perf_cgroup_create(\n\tstruct cgroup_subsys *ss, struct cgroup *cont)\n{\n\tstruct perf_cgroup *jc;\n\n\tjc = kzalloc(sizeof(*jc), GFP_KERNEL);\n\tif (!jc)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tjc->info = alloc_percpu(struct perf_cgroup_info);\n\tif (!jc->info) {\n\t\tkfree(jc);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\treturn &jc->css;\n}\n\nstatic void perf_cgroup_destroy(struct cgroup_subsys *ss,\n\t\t\t\tstruct cgroup *cont)\n{\n\tstruct perf_cgroup *jc;\n\tjc = container_of(cgroup_subsys_state(cont, perf_subsys_id),\n\t\t\t  struct perf_cgroup, css);\n\tfree_percpu(jc->info);\n\tkfree(jc);\n}\n\nstatic int __perf_cgroup_move(void *info)\n{\n\tstruct task_struct *task = info;\n\tperf_cgroup_switch(task, PERF_CGROUP_SWOUT | PERF_CGROUP_SWIN);\n\treturn 0;\n}\n\nstatic void\nperf_cgroup_attach_task(struct cgroup *cgrp, struct task_struct *task)\n{\n\ttask_function_call(task, __perf_cgroup_move, task);\n}\n\nstatic void perf_cgroup_exit(struct cgroup_subsys *ss, struct cgroup *cgrp,\n\t\tstruct cgroup *old_cgrp, struct task_struct *task)\n{\n\t/*\n\t * cgroup_exit() is called in the copy_process() failure path.\n\t * Ignore this case since the task hasn't ran yet, this avoids\n\t * trying to poke a half freed task state from generic code.\n\t */\n\tif (!(task->flags & PF_EXITING))\n\t\treturn;\n\n\tperf_cgroup_attach_task(cgrp, task);\n}\n\nstruct cgroup_subsys perf_subsys = {\n\t.name\t\t= \"perf_event\",\n\t.subsys_id\t= perf_subsys_id,\n\t.create\t\t= perf_cgroup_create,\n\t.destroy\t= perf_cgroup_destroy,\n\t.exit\t\t= perf_cgroup_exit,\n\t.attach_task\t= perf_cgroup_attach_task,\n};\n#endif /* CONFIG_CGROUP_PERF */\n", "#ifndef _KERNEL_EVENTS_INTERNAL_H\n#define _KERNEL_EVENTS_INTERNAL_H\n\n#define RING_BUFFER_WRITABLE\t\t0x01\n\nstruct ring_buffer {\n\tatomic_t\t\t\trefcount;\n\tstruct rcu_head\t\t\trcu_head;\n#ifdef CONFIG_PERF_USE_VMALLOC\n\tstruct work_struct\t\twork;\n\tint\t\t\t\tpage_order;\t/* allocation order  */\n#endif\n\tint\t\t\t\tnr_pages;\t/* nr of data pages  */\n\tint\t\t\t\twritable;\t/* are we writable   */\n\n\tatomic_t\t\t\tpoll;\t\t/* POLL_ for wakeups */\n\n\tlocal_t\t\t\t\thead;\t\t/* write position    */\n\tlocal_t\t\t\t\tnest;\t\t/* nested writers    */\n\tlocal_t\t\t\t\tevents;\t\t/* event limit       */\n\tlocal_t\t\t\t\twakeup;\t\t/* wakeup stamp      */\n\tlocal_t\t\t\t\tlost;\t\t/* nr records lost   */\n\n\tlong\t\t\t\twatermark;\t/* wakeup watermark  */\n\n\tstruct perf_event_mmap_page\t*user_page;\n\tvoid\t\t\t\t*data_pages[0];\n};\n\n\nextern void rb_free(struct ring_buffer *rb);\nextern struct ring_buffer *\nrb_alloc(int nr_pages, long watermark, int cpu, int flags);\nextern void perf_event_wakeup(struct perf_event *event);\n\nextern void\nperf_event_header__init_id(struct perf_event_header *header,\n\t\t\t   struct perf_sample_data *data,\n\t\t\t   struct perf_event *event);\nextern void\nperf_event__output_id_sample(struct perf_event *event,\n\t\t\t     struct perf_output_handle *handle,\n\t\t\t     struct perf_sample_data *sample);\n\nextern struct page *\nperf_mmap_to_page(struct ring_buffer *rb, unsigned long pgoff);\n\n#ifdef CONFIG_PERF_USE_VMALLOC\n/*\n * Back perf_mmap() with vmalloc memory.\n *\n * Required for architectures that have d-cache aliasing issues.\n */\n\nstatic inline int page_order(struct ring_buffer *rb)\n{\n\treturn rb->page_order;\n}\n\n#else\n\nstatic inline int page_order(struct ring_buffer *rb)\n{\n\treturn 0;\n}\n#endif\n\nstatic unsigned long perf_data_size(struct ring_buffer *rb)\n{\n\treturn rb->nr_pages << (PAGE_SHIFT + page_order(rb));\n}\n\nstatic inline void\n__output_copy(struct perf_output_handle *handle,\n\t\t   const void *buf, unsigned int len)\n{\n\tdo {\n\t\tunsigned long size = min_t(unsigned long, handle->size, len);\n\n\t\tmemcpy(handle->addr, buf, size);\n\n\t\tlen -= size;\n\t\thandle->addr += size;\n\t\tbuf += size;\n\t\thandle->size -= size;\n\t\tif (!handle->size) {\n\t\t\tstruct ring_buffer *rb = handle->rb;\n\n\t\t\thandle->page++;\n\t\t\thandle->page &= rb->nr_pages - 1;\n\t\t\thandle->addr = rb->data_pages[handle->page];\n\t\t\thandle->size = PAGE_SIZE << page_order(rb);\n\t\t}\n\t} while (len);\n}\n\n#endif /* _KERNEL_EVENTS_INTERNAL_H */\n", "/*\n * Performance events ring-buffer code:\n *\n *  Copyright (C) 2008 Thomas Gleixner <tglx@linutronix.de>\n *  Copyright (C) 2008-2011 Red Hat, Inc., Ingo Molnar\n *  Copyright (C) 2008-2011 Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>\n *  Copyright  \ufffd  2009 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>\n *\n * For licensing details see kernel-base/COPYING\n */\n\n#include <linux/perf_event.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n\n#include \"internal.h\"\n\nstatic bool perf_output_space(struct ring_buffer *rb, unsigned long tail,\n\t\t\t      unsigned long offset, unsigned long head)\n{\n\tunsigned long mask;\n\n\tif (!rb->writable)\n\t\treturn true;\n\n\tmask = perf_data_size(rb) - 1;\n\n\toffset = (offset - tail) & mask;\n\thead   = (head   - tail) & mask;\n\n\tif ((int)(head - offset) < 0)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void perf_output_wakeup(struct perf_output_handle *handle)\n{\n\tatomic_set(&handle->rb->poll, POLL_IN);\n\n\tif (handle->nmi) {\n\t\thandle->event->pending_wakeup = 1;\n\t\tirq_work_queue(&handle->event->pending);\n\t} else\n\t\tperf_event_wakeup(handle->event);\n}\n\n/*\n * We need to ensure a later event_id doesn't publish a head when a former\n * event isn't done writing. However since we need to deal with NMIs we\n * cannot fully serialize things.\n *\n * We only publish the head (and generate a wakeup) when the outer-most\n * event completes.\n */\nstatic void perf_output_get_handle(struct perf_output_handle *handle)\n{\n\tstruct ring_buffer *rb = handle->rb;\n\n\tpreempt_disable();\n\tlocal_inc(&rb->nest);\n\thandle->wakeup = local_read(&rb->wakeup);\n}\n\nstatic void perf_output_put_handle(struct perf_output_handle *handle)\n{\n\tstruct ring_buffer *rb = handle->rb;\n\tunsigned long head;\n\nagain:\n\thead = local_read(&rb->head);\n\n\t/*\n\t * IRQ/NMI can happen here, which means we can miss a head update.\n\t */\n\n\tif (!local_dec_and_test(&rb->nest))\n\t\tgoto out;\n\n\t/*\n\t * Publish the known good head. Rely on the full barrier implied\n\t * by atomic_dec_and_test() order the rb->head read and this\n\t * write.\n\t */\n\trb->user_page->data_head = head;\n\n\t/*\n\t * Now check if we missed an update, rely on the (compiler)\n\t * barrier in atomic_dec_and_test() to re-read rb->head.\n\t */\n\tif (unlikely(head != local_read(&rb->head))) {\n\t\tlocal_inc(&rb->nest);\n\t\tgoto again;\n\t}\n\n\tif (handle->wakeup != local_read(&rb->wakeup))\n\t\tperf_output_wakeup(handle);\n\nout:\n\tpreempt_enable();\n}\n\nint perf_output_begin(struct perf_output_handle *handle,\n\t\t      struct perf_event *event, unsigned int size,\n\t\t      int nmi, int sample)\n{\n\tstruct ring_buffer *rb;\n\tunsigned long tail, offset, head;\n\tint have_lost;\n\tstruct perf_sample_data sample_data;\n\tstruct {\n\t\tstruct perf_event_header header;\n\t\tu64\t\t\t id;\n\t\tu64\t\t\t lost;\n\t} lost_event;\n\n\trcu_read_lock();\n\t/*\n\t * For inherited events we send all the output towards the parent.\n\t */\n\tif (event->parent)\n\t\tevent = event->parent;\n\n\trb = rcu_dereference(event->rb);\n\tif (!rb)\n\t\tgoto out;\n\n\thandle->rb\t= rb;\n\thandle->event\t= event;\n\thandle->nmi\t= nmi;\n\thandle->sample\t= sample;\n\n\tif (!rb->nr_pages)\n\t\tgoto out;\n\n\thave_lost = local_read(&rb->lost);\n\tif (have_lost) {\n\t\tlost_event.header.size = sizeof(lost_event);\n\t\tperf_event_header__init_id(&lost_event.header, &sample_data,\n\t\t\t\t\t   event);\n\t\tsize += lost_event.header.size;\n\t}\n\n\tperf_output_get_handle(handle);\n\n\tdo {\n\t\t/*\n\t\t * Userspace could choose to issue a mb() before updating the\n\t\t * tail pointer. So that all reads will be completed before the\n\t\t * write is issued.\n\t\t */\n\t\ttail = ACCESS_ONCE(rb->user_page->data_tail);\n\t\tsmp_rmb();\n\t\toffset = head = local_read(&rb->head);\n\t\thead += size;\n\t\tif (unlikely(!perf_output_space(rb, tail, offset, head)))\n\t\t\tgoto fail;\n\t} while (local_cmpxchg(&rb->head, offset, head) != offset);\n\n\tif (head - local_read(&rb->wakeup) > rb->watermark)\n\t\tlocal_add(rb->watermark, &rb->wakeup);\n\n\thandle->page = offset >> (PAGE_SHIFT + page_order(rb));\n\thandle->page &= rb->nr_pages - 1;\n\thandle->size = offset & ((PAGE_SIZE << page_order(rb)) - 1);\n\thandle->addr = rb->data_pages[handle->page];\n\thandle->addr += handle->size;\n\thandle->size = (PAGE_SIZE << page_order(rb)) - handle->size;\n\n\tif (have_lost) {\n\t\tlost_event.header.type = PERF_RECORD_LOST;\n\t\tlost_event.header.misc = 0;\n\t\tlost_event.id          = event->id;\n\t\tlost_event.lost        = local_xchg(&rb->lost, 0);\n\n\t\tperf_output_put(handle, lost_event);\n\t\tperf_event__output_id_sample(event, handle, &sample_data);\n\t}\n\n\treturn 0;\n\nfail:\n\tlocal_inc(&rb->lost);\n\tperf_output_put_handle(handle);\nout:\n\trcu_read_unlock();\n\n\treturn -ENOSPC;\n}\n\nvoid perf_output_copy(struct perf_output_handle *handle,\n\t\t      const void *buf, unsigned int len)\n{\n\t__output_copy(handle, buf, len);\n}\n\nvoid perf_output_end(struct perf_output_handle *handle)\n{\n\tstruct perf_event *event = handle->event;\n\tstruct ring_buffer *rb = handle->rb;\n\n\tif (handle->sample && !event->attr.watermark) {\n\t\tint wakeup_events = event->attr.wakeup_events;\n\n\t\tif (wakeup_events) {\n\t\t\tint events = local_inc_return(&rb->events);\n\t\t\tif (events >= wakeup_events) {\n\t\t\t\tlocal_sub(wakeup_events, &rb->events);\n\t\t\t\tlocal_inc(&rb->wakeup);\n\t\t\t}\n\t\t}\n\t}\n\n\tperf_output_put_handle(handle);\n\trcu_read_unlock();\n}\n\nstatic void\nring_buffer_init(struct ring_buffer *rb, long watermark, int flags)\n{\n\tlong max_size = perf_data_size(rb);\n\n\tif (watermark)\n\t\trb->watermark = min(max_size, watermark);\n\n\tif (!rb->watermark)\n\t\trb->watermark = max_size / 2;\n\n\tif (flags & RING_BUFFER_WRITABLE)\n\t\trb->writable = 1;\n\n\tatomic_set(&rb->refcount, 1);\n}\n\n#ifndef CONFIG_PERF_USE_VMALLOC\n\n/*\n * Back perf_mmap() with regular GFP_KERNEL-0 pages.\n */\n\nstruct page *\nperf_mmap_to_page(struct ring_buffer *rb, unsigned long pgoff)\n{\n\tif (pgoff > rb->nr_pages)\n\t\treturn NULL;\n\n\tif (pgoff == 0)\n\t\treturn virt_to_page(rb->user_page);\n\n\treturn virt_to_page(rb->data_pages[pgoff - 1]);\n}\n\nstatic void *perf_mmap_alloc_page(int cpu)\n{\n\tstruct page *page;\n\tint node;\n\n\tnode = (cpu == -1) ? cpu : cpu_to_node(cpu);\n\tpage = alloc_pages_node(node, GFP_KERNEL | __GFP_ZERO, 0);\n\tif (!page)\n\t\treturn NULL;\n\n\treturn page_address(page);\n}\n\nstruct ring_buffer *rb_alloc(int nr_pages, long watermark, int cpu, int flags)\n{\n\tstruct ring_buffer *rb;\n\tunsigned long size;\n\tint i;\n\n\tsize = sizeof(struct ring_buffer);\n\tsize += nr_pages * sizeof(void *);\n\n\trb = kzalloc(size, GFP_KERNEL);\n\tif (!rb)\n\t\tgoto fail;\n\n\trb->user_page = perf_mmap_alloc_page(cpu);\n\tif (!rb->user_page)\n\t\tgoto fail_user_page;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\trb->data_pages[i] = perf_mmap_alloc_page(cpu);\n\t\tif (!rb->data_pages[i])\n\t\t\tgoto fail_data_pages;\n\t}\n\n\trb->nr_pages = nr_pages;\n\n\tring_buffer_init(rb, watermark, flags);\n\n\treturn rb;\n\nfail_data_pages:\n\tfor (i--; i >= 0; i--)\n\t\tfree_page((unsigned long)rb->data_pages[i]);\n\n\tfree_page((unsigned long)rb->user_page);\n\nfail_user_page:\n\tkfree(rb);\n\nfail:\n\treturn NULL;\n}\n\nstatic void perf_mmap_free_page(unsigned long addr)\n{\n\tstruct page *page = virt_to_page((void *)addr);\n\n\tpage->mapping = NULL;\n\t__free_page(page);\n}\n\nvoid rb_free(struct ring_buffer *rb)\n{\n\tint i;\n\n\tperf_mmap_free_page((unsigned long)rb->user_page);\n\tfor (i = 0; i < rb->nr_pages; i++)\n\t\tperf_mmap_free_page((unsigned long)rb->data_pages[i]);\n\tkfree(rb);\n}\n\n#else\n\nstruct page *\nperf_mmap_to_page(struct ring_buffer *rb, unsigned long pgoff)\n{\n\tif (pgoff > (1UL << page_order(rb)))\n\t\treturn NULL;\n\n\treturn vmalloc_to_page((void *)rb->user_page + pgoff * PAGE_SIZE);\n}\n\nstatic void perf_mmap_unmark_page(void *addr)\n{\n\tstruct page *page = vmalloc_to_page(addr);\n\n\tpage->mapping = NULL;\n}\n\nstatic void rb_free_work(struct work_struct *work)\n{\n\tstruct ring_buffer *rb;\n\tvoid *base;\n\tint i, nr;\n\n\trb = container_of(work, struct ring_buffer, work);\n\tnr = 1 << page_order(rb);\n\n\tbase = rb->user_page;\n\tfor (i = 0; i < nr + 1; i++)\n\t\tperf_mmap_unmark_page(base + (i * PAGE_SIZE));\n\n\tvfree(base);\n\tkfree(rb);\n}\n\nvoid rb_free(struct ring_buffer *rb)\n{\n\tschedule_work(&rb->work);\n}\n\nstruct ring_buffer *rb_alloc(int nr_pages, long watermark, int cpu, int flags)\n{\n\tstruct ring_buffer *rb;\n\tunsigned long size;\n\tvoid *all_buf;\n\n\tsize = sizeof(struct ring_buffer);\n\tsize += sizeof(void *);\n\n\trb = kzalloc(size, GFP_KERNEL);\n\tif (!rb)\n\t\tgoto fail;\n\n\tINIT_WORK(&rb->work, rb_free_work);\n\n\tall_buf = vmalloc_user((nr_pages + 1) * PAGE_SIZE);\n\tif (!all_buf)\n\t\tgoto fail_all_buf;\n\n\trb->user_page = all_buf;\n\trb->data_pages[0] = all_buf + PAGE_SIZE;\n\trb->page_order = ilog2(nr_pages);\n\trb->nr_pages = 1;\n\n\tring_buffer_init(rb, watermark, flags);\n\n\treturn rb;\n\nfail_all_buf:\n\tkfree(rb);\n\nfail:\n\treturn NULL;\n}\n\n#endif\n", "/*\n *  kernel/sched.c\n *\n *  Kernel scheduler and related syscalls\n *\n *  Copyright (C) 1991-2002  Linus Torvalds\n *\n *  1996-12-23  Modified by Dave Grothe to fix bugs in semaphores and\n *\t\tmake semaphores SMP safe\n *  1998-11-19\tImplemented schedule_timeout() and related stuff\n *\t\tby Andrea Arcangeli\n *  2002-01-04\tNew ultra-scalable O(1) scheduler by Ingo Molnar:\n *\t\thybrid priority-list and round-robin design with\n *\t\tan array-switch method of distributing timeslices\n *\t\tand per-CPU runqueues.  Cleanups and useful suggestions\n *\t\tby Davide Libenzi, preemptible kernel bits by Robert Love.\n *  2003-09-03\tInteractivity tuning by Con Kolivas.\n *  2004-04-02\tScheduler domains code by Nick Piggin\n *  2007-04-15  Work begun on replacing all interactivity tuning with a\n *              fair scheduling design by Con Kolivas.\n *  2007-05-05  Load balancing (smp-nice) and other improvements\n *              by Peter Williams\n *  2007-05-06  Interactivity improvements to CFS by Mike Galbraith\n *  2007-07-01  Group scheduling enhancements by Srivatsa Vaddagiri\n *  2007-11-29  RT balancing improvements by Steven Rostedt, Gregory Haskins,\n *              Thomas Gleixner, Mike Kravetz\n */\n\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/nmi.h>\n#include <linux/init.h>\n#include <linux/uaccess.h>\n#include <linux/highmem.h>\n#include <asm/mmu_context.h>\n#include <linux/interrupt.h>\n#include <linux/capability.h>\n#include <linux/completion.h>\n#include <linux/kernel_stat.h>\n#include <linux/debug_locks.h>\n#include <linux/perf_event.h>\n#include <linux/security.h>\n#include <linux/notifier.h>\n#include <linux/profile.h>\n#include <linux/freezer.h>\n#include <linux/vmalloc.h>\n#include <linux/blkdev.h>\n#include <linux/delay.h>\n#include <linux/pid_namespace.h>\n#include <linux/smp.h>\n#include <linux/threads.h>\n#include <linux/timer.h>\n#include <linux/rcupdate.h>\n#include <linux/cpu.h>\n#include <linux/cpuset.h>\n#include <linux/percpu.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/stop_machine.h>\n#include <linux/sysctl.h>\n#include <linux/syscalls.h>\n#include <linux/times.h>\n#include <linux/tsacct_kern.h>\n#include <linux/kprobes.h>\n#include <linux/delayacct.h>\n#include <linux/unistd.h>\n#include <linux/pagemap.h>\n#include <linux/hrtimer.h>\n#include <linux/tick.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/ftrace.h>\n#include <linux/slab.h>\n\n#include <asm/tlb.h>\n#include <asm/irq_regs.h>\n#include <asm/mutex.h>\n\n#include \"sched_cpupri.h\"\n#include \"workqueue_sched.h\"\n#include \"sched_autogroup.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/sched.h>\n\n/*\n * Convert user-nice values [ -20 ... 0 ... 19 ]\n * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],\n * and back.\n */\n#define NICE_TO_PRIO(nice)\t(MAX_RT_PRIO + (nice) + 20)\n#define PRIO_TO_NICE(prio)\t((prio) - MAX_RT_PRIO - 20)\n#define TASK_NICE(p)\t\tPRIO_TO_NICE((p)->static_prio)\n\n/*\n * 'User priority' is the nice value converted to something we\n * can work with better when scaling various scheduler parameters,\n * it's a [ 0 ... 39 ] range.\n */\n#define USER_PRIO(p)\t\t((p)-MAX_RT_PRIO)\n#define TASK_USER_PRIO(p)\tUSER_PRIO((p)->static_prio)\n#define MAX_USER_PRIO\t\t(USER_PRIO(MAX_PRIO))\n\n/*\n * Helpers for converting nanosecond timing to jiffy resolution\n */\n#define NS_TO_JIFFIES(TIME)\t((unsigned long)(TIME) / (NSEC_PER_SEC / HZ))\n\n#define NICE_0_LOAD\t\tSCHED_LOAD_SCALE\n#define NICE_0_SHIFT\t\tSCHED_LOAD_SHIFT\n\n/*\n * These are the 'tuning knobs' of the scheduler:\n *\n * default timeslice is 100 msecs (used only for SCHED_RR tasks).\n * Timeslices get refilled after they expire.\n */\n#define DEF_TIMESLICE\t\t(100 * HZ / 1000)\n\n/*\n * single value that denotes runtime == period, ie unlimited time.\n */\n#define RUNTIME_INF\t((u64)~0ULL)\n\nstatic inline int rt_policy(int policy)\n{\n\tif (unlikely(policy == SCHED_FIFO || policy == SCHED_RR))\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic inline int task_has_rt_policy(struct task_struct *p)\n{\n\treturn rt_policy(p->policy);\n}\n\n/*\n * This is the priority-queue data structure of the RT scheduling class:\n */\nstruct rt_prio_array {\n\tDECLARE_BITMAP(bitmap, MAX_RT_PRIO+1); /* include 1 bit for delimiter */\n\tstruct list_head queue[MAX_RT_PRIO];\n};\n\nstruct rt_bandwidth {\n\t/* nests inside the rq lock: */\n\traw_spinlock_t\t\trt_runtime_lock;\n\tktime_t\t\t\trt_period;\n\tu64\t\t\trt_runtime;\n\tstruct hrtimer\t\trt_period_timer;\n};\n\nstatic struct rt_bandwidth def_rt_bandwidth;\n\nstatic int do_sched_rt_period_timer(struct rt_bandwidth *rt_b, int overrun);\n\nstatic enum hrtimer_restart sched_rt_period_timer(struct hrtimer *timer)\n{\n\tstruct rt_bandwidth *rt_b =\n\t\tcontainer_of(timer, struct rt_bandwidth, rt_period_timer);\n\tktime_t now;\n\tint overrun;\n\tint idle = 0;\n\n\tfor (;;) {\n\t\tnow = hrtimer_cb_get_time(timer);\n\t\toverrun = hrtimer_forward(timer, now, rt_b->rt_period);\n\n\t\tif (!overrun)\n\t\t\tbreak;\n\n\t\tidle = do_sched_rt_period_timer(rt_b, overrun);\n\t}\n\n\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;\n}\n\nstatic\nvoid init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)\n{\n\trt_b->rt_period = ns_to_ktime(period);\n\trt_b->rt_runtime = runtime;\n\n\traw_spin_lock_init(&rt_b->rt_runtime_lock);\n\n\thrtimer_init(&rt_b->rt_period_timer,\n\t\t\tCLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\trt_b->rt_period_timer.function = sched_rt_period_timer;\n}\n\nstatic inline int rt_bandwidth_enabled(void)\n{\n\treturn sysctl_sched_rt_runtime >= 0;\n}\n\nstatic void start_rt_bandwidth(struct rt_bandwidth *rt_b)\n{\n\tktime_t now;\n\n\tif (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF)\n\t\treturn;\n\n\tif (hrtimer_active(&rt_b->rt_period_timer))\n\t\treturn;\n\n\traw_spin_lock(&rt_b->rt_runtime_lock);\n\tfor (;;) {\n\t\tunsigned long delta;\n\t\tktime_t soft, hard;\n\n\t\tif (hrtimer_active(&rt_b->rt_period_timer))\n\t\t\tbreak;\n\n\t\tnow = hrtimer_cb_get_time(&rt_b->rt_period_timer);\n\t\thrtimer_forward(&rt_b->rt_period_timer, now, rt_b->rt_period);\n\n\t\tsoft = hrtimer_get_softexpires(&rt_b->rt_period_timer);\n\t\thard = hrtimer_get_expires(&rt_b->rt_period_timer);\n\t\tdelta = ktime_to_ns(ktime_sub(hard, soft));\n\t\t__hrtimer_start_range_ns(&rt_b->rt_period_timer, soft, delta,\n\t\t\t\tHRTIMER_MODE_ABS_PINNED, 0);\n\t}\n\traw_spin_unlock(&rt_b->rt_runtime_lock);\n}\n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic void destroy_rt_bandwidth(struct rt_bandwidth *rt_b)\n{\n\thrtimer_cancel(&rt_b->rt_period_timer);\n}\n#endif\n\n/*\n * sched_domains_mutex serializes calls to init_sched_domains,\n * detach_destroy_domains and partition_sched_domains.\n */\nstatic DEFINE_MUTEX(sched_domains_mutex);\n\n#ifdef CONFIG_CGROUP_SCHED\n\n#include <linux/cgroup.h>\n\nstruct cfs_rq;\n\nstatic LIST_HEAD(task_groups);\n\n/* task group related information */\nstruct task_group {\n\tstruct cgroup_subsys_state css;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t/* schedulable entities of this group on each cpu */\n\tstruct sched_entity **se;\n\t/* runqueue \"owned\" by this group on each cpu */\n\tstruct cfs_rq **cfs_rq;\n\tunsigned long shares;\n\n\tatomic_t load_weight;\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tstruct sched_rt_entity **rt_se;\n\tstruct rt_rq **rt_rq;\n\n\tstruct rt_bandwidth rt_bandwidth;\n#endif\n\n\tstruct rcu_head rcu;\n\tstruct list_head list;\n\n\tstruct task_group *parent;\n\tstruct list_head siblings;\n\tstruct list_head children;\n\n#ifdef CONFIG_SCHED_AUTOGROUP\n\tstruct autogroup *autogroup;\n#endif\n};\n\n/* task_group_lock serializes the addition/removal of task groups */\nstatic DEFINE_SPINLOCK(task_group_lock);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\n# define ROOT_TASK_GROUP_LOAD\tNICE_0_LOAD\n\n/*\n * A weight of 0 or 1 can cause arithmetics problems.\n * A weight of a cfs_rq is the sum of weights of which entities\n * are queued on this cfs_rq, so a weight of a entity should not be\n * too large, so as the shares value of a task group.\n * (The default weight is 1024 - so there's no practical\n *  limitation from this.)\n */\n#define MIN_SHARES\t2\n#define MAX_SHARES\t(1UL << (18 + SCHED_LOAD_RESOLUTION))\n\nstatic int root_task_group_load = ROOT_TASK_GROUP_LOAD;\n#endif\n\n/* Default task group.\n *\tEvery task in system belong to this group at bootup.\n */\nstruct task_group root_task_group;\n\n#endif\t/* CONFIG_CGROUP_SCHED */\n\n/* CFS-related fields in a runqueue */\nstruct cfs_rq {\n\tstruct load_weight load;\n\tunsigned long nr_running;\n\n\tu64 exec_clock;\n\tu64 min_vruntime;\n#ifndef CONFIG_64BIT\n\tu64 min_vruntime_copy;\n#endif\n\n\tstruct rb_root tasks_timeline;\n\tstruct rb_node *rb_leftmost;\n\n\tstruct list_head tasks;\n\tstruct list_head *balance_iterator;\n\n\t/*\n\t * 'curr' points to currently running entity on this cfs_rq.\n\t * It is set to NULL otherwise (i.e when none are currently running).\n\t */\n\tstruct sched_entity *curr, *next, *last, *skip;\n\n#ifdef\tCONFIG_SCHED_DEBUG\n\tunsigned int nr_spread_over;\n#endif\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tstruct rq *rq;\t/* cpu runqueue to which this cfs_rq is attached */\n\n\t/*\n\t * leaf cfs_rqs are those that hold tasks (lowest schedulable entity in\n\t * a hierarchy). Non-leaf lrqs hold other higher schedulable entities\n\t * (like users, containers etc.)\n\t *\n\t * leaf_cfs_rq_list ties together list of leaf cfs_rq's in a cpu. This\n\t * list is used during load balance.\n\t */\n\tint on_list;\n\tstruct list_head leaf_cfs_rq_list;\n\tstruct task_group *tg;\t/* group that \"owns\" this runqueue */\n\n#ifdef CONFIG_SMP\n\t/*\n\t * the part of load.weight contributed by tasks\n\t */\n\tunsigned long task_weight;\n\n\t/*\n\t *   h_load = weight * f(tg)\n\t *\n\t * Where f(tg) is the recursive weight fraction assigned to\n\t * this group.\n\t */\n\tunsigned long h_load;\n\n\t/*\n\t * Maintaining per-cpu shares distribution for group scheduling\n\t *\n\t * load_stamp is the last time we updated the load average\n\t * load_last is the last time we updated the load average and saw load\n\t * load_unacc_exec_time is currently unaccounted execution time\n\t */\n\tu64 load_avg;\n\tu64 load_period;\n\tu64 load_stamp, load_last, load_unacc_exec_time;\n\n\tunsigned long load_contribution;\n#endif\n#endif\n};\n\n/* Real-Time classes' related field in a runqueue: */\nstruct rt_rq {\n\tstruct rt_prio_array active;\n\tunsigned long rt_nr_running;\n#if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED\n\tstruct {\n\t\tint curr; /* highest queued rt task prio */\n#ifdef CONFIG_SMP\n\t\tint next; /* next highest */\n#endif\n\t} highest_prio;\n#endif\n#ifdef CONFIG_SMP\n\tunsigned long rt_nr_migratory;\n\tunsigned long rt_nr_total;\n\tint overloaded;\n\tstruct plist_head pushable_tasks;\n#endif\n\tint rt_throttled;\n\tu64 rt_time;\n\tu64 rt_runtime;\n\t/* Nests inside the rq lock: */\n\traw_spinlock_t rt_runtime_lock;\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tunsigned long rt_nr_boosted;\n\n\tstruct rq *rq;\n\tstruct list_head leaf_rt_rq_list;\n\tstruct task_group *tg;\n#endif\n};\n\n#ifdef CONFIG_SMP\n\n/*\n * We add the notion of a root-domain which will be used to define per-domain\n * variables. Each exclusive cpuset essentially defines an island domain by\n * fully partitioning the member cpus from any other cpuset. Whenever a new\n * exclusive cpuset is created, we also create and attach a new root-domain\n * object.\n *\n */\nstruct root_domain {\n\tatomic_t refcount;\n\tstruct rcu_head rcu;\n\tcpumask_var_t span;\n\tcpumask_var_t online;\n\n\t/*\n\t * The \"RT overload\" flag: it gets set if a CPU has more than\n\t * one runnable RT task.\n\t */\n\tcpumask_var_t rto_mask;\n\tatomic_t rto_count;\n\tstruct cpupri cpupri;\n};\n\n/*\n * By default the system creates a single root-domain with all cpus as\n * members (mimicking the global state we have today).\n */\nstatic struct root_domain def_root_domain;\n\n#endif /* CONFIG_SMP */\n\n/*\n * This is the main, per-CPU runqueue data structure.\n *\n * Locking rule: those places that want to lock multiple runqueues\n * (such as the load balancing or the thread migration code), lock\n * acquire operations must be ordered by ascending &runqueue.\n */\nstruct rq {\n\t/* runqueue lock: */\n\traw_spinlock_t lock;\n\n\t/*\n\t * nr_running and cpu_load should be in the same cacheline because\n\t * remote CPUs use both these fields when doing load calculation.\n\t */\n\tunsigned long nr_running;\n\t#define CPU_LOAD_IDX_MAX 5\n\tunsigned long cpu_load[CPU_LOAD_IDX_MAX];\n\tunsigned long last_load_update_tick;\n#ifdef CONFIG_NO_HZ\n\tu64 nohz_stamp;\n\tunsigned char nohz_balance_kick;\n#endif\n\tint skip_clock_update;\n\n\t/* capture load from *all* tasks on this cpu: */\n\tstruct load_weight load;\n\tunsigned long nr_load_updates;\n\tu64 nr_switches;\n\n\tstruct cfs_rq cfs;\n\tstruct rt_rq rt;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t/* list of leaf cfs_rq on this cpu: */\n\tstruct list_head leaf_cfs_rq_list;\n#endif\n#ifdef CONFIG_RT_GROUP_SCHED\n\tstruct list_head leaf_rt_rq_list;\n#endif\n\n\t/*\n\t * This is part of a global counter where only the total sum\n\t * over all CPUs matters. A task can increase this counter on\n\t * one CPU and if it got migrated afterwards it may decrease\n\t * it on another CPU. Always updated under the runqueue lock:\n\t */\n\tunsigned long nr_uninterruptible;\n\n\tstruct task_struct *curr, *idle, *stop;\n\tunsigned long next_balance;\n\tstruct mm_struct *prev_mm;\n\n\tu64 clock;\n\tu64 clock_task;\n\n\tatomic_t nr_iowait;\n\n#ifdef CONFIG_SMP\n\tstruct root_domain *rd;\n\tstruct sched_domain *sd;\n\n\tunsigned long cpu_power;\n\n\tunsigned char idle_at_tick;\n\t/* For active balancing */\n\tint post_schedule;\n\tint active_balance;\n\tint push_cpu;\n\tstruct cpu_stop_work active_balance_work;\n\t/* cpu of this runqueue: */\n\tint cpu;\n\tint online;\n\n\tunsigned long avg_load_per_task;\n\n\tu64 rt_avg;\n\tu64 age_stamp;\n\tu64 idle_stamp;\n\tu64 avg_idle;\n#endif\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n\tu64 prev_irq_time;\n#endif\n\n\t/* calc_load related fields */\n\tunsigned long calc_load_update;\n\tlong calc_load_active;\n\n#ifdef CONFIG_SCHED_HRTICK\n#ifdef CONFIG_SMP\n\tint hrtick_csd_pending;\n\tstruct call_single_data hrtick_csd;\n#endif\n\tstruct hrtimer hrtick_timer;\n#endif\n\n#ifdef CONFIG_SCHEDSTATS\n\t/* latency stats */\n\tstruct sched_info rq_sched_info;\n\tunsigned long long rq_cpu_time;\n\t/* could above be rq->cfs_rq.exec_clock + rq->rt_rq.rt_runtime ? */\n\n\t/* sys_sched_yield() stats */\n\tunsigned int yld_count;\n\n\t/* schedule() stats */\n\tunsigned int sched_switch;\n\tunsigned int sched_count;\n\tunsigned int sched_goidle;\n\n\t/* try_to_wake_up() stats */\n\tunsigned int ttwu_count;\n\tunsigned int ttwu_local;\n#endif\n\n#ifdef CONFIG_SMP\n\tstruct task_struct *wake_list;\n#endif\n};\n\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);\n\n\nstatic void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}\n\n#define rcu_dereference_check_sched_domain(p) \\\n\trcu_dereference_check((p), \\\n\t\t\t      rcu_read_lock_held() || \\\n\t\t\t      lockdep_is_held(&sched_domains_mutex))\n\n/*\n * The domain tree (rq->sd) is protected by RCU's quiescent state transition.\n * See detach_destroy_domains: synchronize_sched for details.\n *\n * The domain tree of any CPU may only be accessed from within\n * preempt-disabled sections.\n */\n#define for_each_domain(cpu, __sd) \\\n\tfor (__sd = rcu_dereference_check_sched_domain(cpu_rq(cpu)->sd); __sd; __sd = __sd->parent)\n\n#define cpu_rq(cpu)\t\t(&per_cpu(runqueues, (cpu)))\n#define this_rq()\t\t(&__get_cpu_var(runqueues))\n#define task_rq(p)\t\tcpu_rq(task_cpu(p))\n#define cpu_curr(cpu)\t\t(cpu_rq(cpu)->curr)\n#define raw_rq()\t\t(&__raw_get_cpu_var(runqueues))\n\n#ifdef CONFIG_CGROUP_SCHED\n\n/*\n * Return the group to which this tasks belongs.\n *\n * We use task_subsys_state_check() and extend the RCU verification with\n * pi->lock and rq->lock because cpu_cgroup_attach() holds those locks for each\n * task it moves into the cgroup. Therefore by holding either of those locks,\n * we pin the task to the current cgroup.\n */\nstatic inline struct task_group *task_group(struct task_struct *p)\n{\n\tstruct task_group *tg;\n\tstruct cgroup_subsys_state *css;\n\n\tcss = task_subsys_state_check(p, cpu_cgroup_subsys_id,\n\t\t\tlockdep_is_held(&p->pi_lock) ||\n\t\t\tlockdep_is_held(&task_rq(p)->lock));\n\ttg = container_of(css, struct task_group, css);\n\n\treturn autogroup_task_group(p, tg);\n}\n\n/* Change a task's cfs_rq and parent entity if it moves across CPUs/groups */\nstatic inline void set_task_rq(struct task_struct *p, unsigned int cpu)\n{\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tp->se.cfs_rq = task_group(p)->cfs_rq[cpu];\n\tp->se.parent = task_group(p)->se[cpu];\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tp->rt.rt_rq  = task_group(p)->rt_rq[cpu];\n\tp->rt.parent = task_group(p)->rt_se[cpu];\n#endif\n}\n\n#else /* CONFIG_CGROUP_SCHED */\n\nstatic inline void set_task_rq(struct task_struct *p, unsigned int cpu) { }\nstatic inline struct task_group *task_group(struct task_struct *p)\n{\n\treturn NULL;\n}\n\n#endif /* CONFIG_CGROUP_SCHED */\n\nstatic void update_rq_clock_task(struct rq *rq, s64 delta);\n\nstatic void update_rq_clock(struct rq *rq)\n{\n\ts64 delta;\n\n\tif (rq->skip_clock_update > 0)\n\t\treturn;\n\n\tdelta = sched_clock_cpu(cpu_of(rq)) - rq->clock;\n\trq->clock += delta;\n\tupdate_rq_clock_task(rq, delta);\n}\n\n/*\n * Tunables that become constants when CONFIG_SCHED_DEBUG is off:\n */\n#ifdef CONFIG_SCHED_DEBUG\n# define const_debug __read_mostly\n#else\n# define const_debug static const\n#endif\n\n/**\n * runqueue_is_locked - Returns true if the current cpu runqueue is locked\n * @cpu: the processor in question.\n *\n * This interface allows printk to be called with the runqueue lock\n * held and know whether or not it is OK to wake up the klogd.\n */\nint runqueue_is_locked(int cpu)\n{\n\treturn raw_spin_is_locked(&cpu_rq(cpu)->lock);\n}\n\n/*\n * Debugging: various feature bits\n */\n\n#define SCHED_FEAT(name, enabled)\t\\\n\t__SCHED_FEAT_##name ,\n\nenum {\n#include \"sched_features.h\"\n};\n\n#undef SCHED_FEAT\n\n#define SCHED_FEAT(name, enabled)\t\\\n\t(1UL << __SCHED_FEAT_##name) * enabled |\n\nconst_debug unsigned int sysctl_sched_features =\n#include \"sched_features.h\"\n\t0;\n\n#undef SCHED_FEAT\n\n#ifdef CONFIG_SCHED_DEBUG\n#define SCHED_FEAT(name, enabled)\t\\\n\t#name ,\n\nstatic __read_mostly char *sched_feat_names[] = {\n#include \"sched_features.h\"\n\tNULL\n};\n\n#undef SCHED_FEAT\n\nstatic int sched_feat_show(struct seq_file *m, void *v)\n{\n\tint i;\n\n\tfor (i = 0; sched_feat_names[i]; i++) {\n\t\tif (!(sysctl_sched_features & (1UL << i)))\n\t\t\tseq_puts(m, \"NO_\");\n\t\tseq_printf(m, \"%s \", sched_feat_names[i]);\n\t}\n\tseq_puts(m, \"\\n\");\n\n\treturn 0;\n}\n\nstatic ssize_t\nsched_feat_write(struct file *filp, const char __user *ubuf,\n\t\tsize_t cnt, loff_t *ppos)\n{\n\tchar buf[64];\n\tchar *cmp;\n\tint neg = 0;\n\tint i;\n\n\tif (cnt > 63)\n\t\tcnt = 63;\n\n\tif (copy_from_user(&buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\tcmp = strstrip(buf);\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\tfor (i = 0; sched_feat_names[i]; i++) {\n\t\tif (strcmp(cmp, sched_feat_names[i]) == 0) {\n\t\t\tif (neg)\n\t\t\t\tsysctl_sched_features &= ~(1UL << i);\n\t\t\telse\n\t\t\t\tsysctl_sched_features |= (1UL << i);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!sched_feat_names[i])\n\t\treturn -EINVAL;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic int sched_feat_open(struct inode *inode, struct file *filp)\n{\n\treturn single_open(filp, sched_feat_show, NULL);\n}\n\nstatic const struct file_operations sched_feat_fops = {\n\t.open\t\t= sched_feat_open,\n\t.write\t\t= sched_feat_write,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= single_release,\n};\n\nstatic __init int sched_init_debug(void)\n{\n\tdebugfs_create_file(\"sched_features\", 0644, NULL, NULL,\n\t\t\t&sched_feat_fops);\n\n\treturn 0;\n}\nlate_initcall(sched_init_debug);\n\n#endif\n\n#define sched_feat(x) (sysctl_sched_features & (1UL << __SCHED_FEAT_##x))\n\n/*\n * Number of tasks to iterate in a single balance run.\n * Limited because this is done with IRQs disabled.\n */\nconst_debug unsigned int sysctl_sched_nr_migrate = 32;\n\n/*\n * period over which we average the RT time consumption, measured\n * in ms.\n *\n * default: 1s\n */\nconst_debug unsigned int sysctl_sched_time_avg = MSEC_PER_SEC;\n\n/*\n * period over which we measure -rt task cpu usage in us.\n * default: 1s\n */\nunsigned int sysctl_sched_rt_period = 1000000;\n\nstatic __read_mostly int scheduler_running;\n\n/*\n * part of the period that we allow rt tasks to run in us.\n * default: 0.95s\n */\nint sysctl_sched_rt_runtime = 950000;\n\nstatic inline u64 global_rt_period(void)\n{\n\treturn (u64)sysctl_sched_rt_period * NSEC_PER_USEC;\n}\n\nstatic inline u64 global_rt_runtime(void)\n{\n\tif (sysctl_sched_rt_runtime < 0)\n\t\treturn RUNTIME_INF;\n\n\treturn (u64)sysctl_sched_rt_runtime * NSEC_PER_USEC;\n}\n\n#ifndef prepare_arch_switch\n# define prepare_arch_switch(next)\tdo { } while (0)\n#endif\n#ifndef finish_arch_switch\n# define finish_arch_switch(prev)\tdo { } while (0)\n#endif\n\nstatic inline int task_current(struct rq *rq, struct task_struct *p)\n{\n\treturn rq->curr == p;\n}\n\nstatic inline int task_running(struct rq *rq, struct task_struct *p)\n{\n#ifdef CONFIG_SMP\n\treturn p->on_cpu;\n#else\n\treturn task_current(rq, p);\n#endif\n}\n\n#ifndef __ARCH_WANT_UNLOCKED_CTXSW\nstatic inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)\n{\n#ifdef CONFIG_SMP\n\t/*\n\t * We can optimise this out completely for !SMP, because the\n\t * SMP rebalancing from interrupt is the only thing that cares\n\t * here.\n\t */\n\tnext->on_cpu = 1;\n#endif\n}\n\nstatic inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)\n{\n#ifdef CONFIG_SMP\n\t/*\n\t * After ->on_cpu is cleared, the task can be moved to a different CPU.\n\t * We must ensure this doesn't happen until the switch is completely\n\t * finished.\n\t */\n\tsmp_wmb();\n\tprev->on_cpu = 0;\n#endif\n#ifdef CONFIG_DEBUG_SPINLOCK\n\t/* this is a valid case when another task releases the spinlock */\n\trq->lock.owner = current;\n#endif\n\t/*\n\t * If we are tracking spinlock dependencies then we have to\n\t * fix up the runqueue lock - which gets 'carried over' from\n\t * prev into current:\n\t */\n\tspin_acquire(&rq->lock.dep_map, 0, 0, _THIS_IP_);\n\n\traw_spin_unlock_irq(&rq->lock);\n}\n\n#else /* __ARCH_WANT_UNLOCKED_CTXSW */\nstatic inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)\n{\n#ifdef CONFIG_SMP\n\t/*\n\t * We can optimise this out completely for !SMP, because the\n\t * SMP rebalancing from interrupt is the only thing that cares\n\t * here.\n\t */\n\tnext->on_cpu = 1;\n#endif\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\traw_spin_unlock_irq(&rq->lock);\n#else\n\traw_spin_unlock(&rq->lock);\n#endif\n}\n\nstatic inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)\n{\n#ifdef CONFIG_SMP\n\t/*\n\t * After ->on_cpu is cleared, the task can be moved to a different CPU.\n\t * We must ensure this doesn't happen until the switch is completely\n\t * finished.\n\t */\n\tsmp_wmb();\n\tprev->on_cpu = 0;\n#endif\n#ifndef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\tlocal_irq_enable();\n#endif\n}\n#endif /* __ARCH_WANT_UNLOCKED_CTXSW */\n\n/*\n * __task_rq_lock - lock the rq @p resides on.\n */\nstatic inline struct rq *__task_rq_lock(struct task_struct *p)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tfor (;;) {\n\t\trq = task_rq(p);\n\t\traw_spin_lock(&rq->lock);\n\t\tif (likely(rq == task_rq(p)))\n\t\t\treturn rq;\n\t\traw_spin_unlock(&rq->lock);\n\t}\n}\n\n/*\n * task_rq_lock - lock p->pi_lock and lock the rq @p resides on.\n */\nstatic struct rq *task_rq_lock(struct task_struct *p, unsigned long *flags)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tfor (;;) {\n\t\traw_spin_lock_irqsave(&p->pi_lock, *flags);\n\t\trq = task_rq(p);\n\t\traw_spin_lock(&rq->lock);\n\t\tif (likely(rq == task_rq(p)))\n\t\t\treturn rq;\n\t\traw_spin_unlock(&rq->lock);\n\t\traw_spin_unlock_irqrestore(&p->pi_lock, *flags);\n\t}\n}\n\nstatic void __task_rq_unlock(struct rq *rq)\n\t__releases(rq->lock)\n{\n\traw_spin_unlock(&rq->lock);\n}\n\nstatic inline void\ntask_rq_unlock(struct rq *rq, struct task_struct *p, unsigned long *flags)\n\t__releases(rq->lock)\n\t__releases(p->pi_lock)\n{\n\traw_spin_unlock(&rq->lock);\n\traw_spin_unlock_irqrestore(&p->pi_lock, *flags);\n}\n\n/*\n * this_rq_lock - lock this runqueue and disable interrupts.\n */\nstatic struct rq *this_rq_lock(void)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tlocal_irq_disable();\n\trq = this_rq();\n\traw_spin_lock(&rq->lock);\n\n\treturn rq;\n}\n\n#ifdef CONFIG_SCHED_HRTICK\n/*\n * Use HR-timers to deliver accurate preemption points.\n *\n * Its all a bit involved since we cannot program an hrt while holding the\n * rq->lock. So what we do is store a state in in rq->hrtick_* and ask for a\n * reschedule event.\n *\n * When we get rescheduled we reprogram the hrtick_timer outside of the\n * rq->lock.\n */\n\n/*\n * Use hrtick when:\n *  - enabled by features\n *  - hrtimer is actually high res\n */\nstatic inline int hrtick_enabled(struct rq *rq)\n{\n\tif (!sched_feat(HRTICK))\n\t\treturn 0;\n\tif (!cpu_active(cpu_of(rq)))\n\t\treturn 0;\n\treturn hrtimer_is_hres_active(&rq->hrtick_timer);\n}\n\nstatic void hrtick_clear(struct rq *rq)\n{\n\tif (hrtimer_active(&rq->hrtick_timer))\n\t\thrtimer_cancel(&rq->hrtick_timer);\n}\n\n/*\n * High-resolution timer tick.\n * Runs from hardirq context with interrupts disabled.\n */\nstatic enum hrtimer_restart hrtick(struct hrtimer *timer)\n{\n\tstruct rq *rq = container_of(timer, struct rq, hrtick_timer);\n\n\tWARN_ON_ONCE(cpu_of(rq) != smp_processor_id());\n\n\traw_spin_lock(&rq->lock);\n\tupdate_rq_clock(rq);\n\trq->curr->sched_class->task_tick(rq, rq->curr, 1);\n\traw_spin_unlock(&rq->lock);\n\n\treturn HRTIMER_NORESTART;\n}\n\n#ifdef CONFIG_SMP\n/*\n * called from hardirq (IPI) context\n */\nstatic void __hrtick_start(void *arg)\n{\n\tstruct rq *rq = arg;\n\n\traw_spin_lock(&rq->lock);\n\thrtimer_restart(&rq->hrtick_timer);\n\trq->hrtick_csd_pending = 0;\n\traw_spin_unlock(&rq->lock);\n}\n\n/*\n * Called to set the hrtick timer state.\n *\n * called with rq->lock held and irqs disabled\n */\nstatic void hrtick_start(struct rq *rq, u64 delay)\n{\n\tstruct hrtimer *timer = &rq->hrtick_timer;\n\tktime_t time = ktime_add_ns(timer->base->get_time(), delay);\n\n\thrtimer_set_expires(timer, time);\n\n\tif (rq == this_rq()) {\n\t\thrtimer_restart(timer);\n\t} else if (!rq->hrtick_csd_pending) {\n\t\t__smp_call_function_single(cpu_of(rq), &rq->hrtick_csd, 0);\n\t\trq->hrtick_csd_pending = 1;\n\t}\n}\n\nstatic int\nhotplug_hrtick(struct notifier_block *nfb, unsigned long action, void *hcpu)\n{\n\tint cpu = (int)(long)hcpu;\n\n\tswitch (action) {\n\tcase CPU_UP_CANCELED:\n\tcase CPU_UP_CANCELED_FROZEN:\n\tcase CPU_DOWN_PREPARE:\n\tcase CPU_DOWN_PREPARE_FROZEN:\n\tcase CPU_DEAD:\n\tcase CPU_DEAD_FROZEN:\n\t\thrtick_clear(cpu_rq(cpu));\n\t\treturn NOTIFY_OK;\n\t}\n\n\treturn NOTIFY_DONE;\n}\n\nstatic __init void init_hrtick(void)\n{\n\thotcpu_notifier(hotplug_hrtick, 0);\n}\n#else\n/*\n * Called to set the hrtick timer state.\n *\n * called with rq->lock held and irqs disabled\n */\nstatic void hrtick_start(struct rq *rq, u64 delay)\n{\n\t__hrtimer_start_range_ns(&rq->hrtick_timer, ns_to_ktime(delay), 0,\n\t\t\tHRTIMER_MODE_REL_PINNED, 0);\n}\n\nstatic inline void init_hrtick(void)\n{\n}\n#endif /* CONFIG_SMP */\n\nstatic void init_rq_hrtick(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\trq->hrtick_csd_pending = 0;\n\n\trq->hrtick_csd.flags = 0;\n\trq->hrtick_csd.func = __hrtick_start;\n\trq->hrtick_csd.info = rq;\n#endif\n\n\thrtimer_init(&rq->hrtick_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\trq->hrtick_timer.function = hrtick;\n}\n#else\t/* CONFIG_SCHED_HRTICK */\nstatic inline void hrtick_clear(struct rq *rq)\n{\n}\n\nstatic inline void init_rq_hrtick(struct rq *rq)\n{\n}\n\nstatic inline void init_hrtick(void)\n{\n}\n#endif\t/* CONFIG_SCHED_HRTICK */\n\n/*\n * resched_task - mark a task 'to be rescheduled now'.\n *\n * On UP this means the setting of the need_resched flag, on SMP it\n * might also involve a cross-CPU call to trigger the scheduler on\n * the target CPU.\n */\n#ifdef CONFIG_SMP\n\n#ifndef tsk_is_polling\n#define tsk_is_polling(t) test_tsk_thread_flag(t, TIF_POLLING_NRFLAG)\n#endif\n\nstatic void resched_task(struct task_struct *p)\n{\n\tint cpu;\n\n\tassert_raw_spin_locked(&task_rq(p)->lock);\n\n\tif (test_tsk_need_resched(p))\n\t\treturn;\n\n\tset_tsk_need_resched(p);\n\n\tcpu = task_cpu(p);\n\tif (cpu == smp_processor_id())\n\t\treturn;\n\n\t/* NEED_RESCHED must be visible before we test polling */\n\tsmp_mb();\n\tif (!tsk_is_polling(p))\n\t\tsmp_send_reschedule(cpu);\n}\n\nstatic void resched_cpu(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long flags;\n\n\tif (!raw_spin_trylock_irqsave(&rq->lock, flags))\n\t\treturn;\n\tresched_task(cpu_curr(cpu));\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n}\n\n#ifdef CONFIG_NO_HZ\n/*\n * In the semi idle case, use the nearest busy cpu for migrating timers\n * from an idle cpu.  This is good for power-savings.\n *\n * We don't do similar optimization for completely idle system, as\n * selecting an idle cpu will add more delays to the timers than intended\n * (as that cpu's timer base may not be uptodate wrt jiffies etc).\n */\nint get_nohz_timer_target(void)\n{\n\tint cpu = smp_processor_id();\n\tint i;\n\tstruct sched_domain *sd;\n\n\trcu_read_lock();\n\tfor_each_domain(cpu, sd) {\n\t\tfor_each_cpu(i, sched_domain_span(sd)) {\n\t\t\tif (!idle_cpu(i)) {\n\t\t\t\tcpu = i;\n\t\t\t\tgoto unlock;\n\t\t\t}\n\t\t}\n\t}\nunlock:\n\trcu_read_unlock();\n\treturn cpu;\n}\n/*\n * When add_timer_on() enqueues a timer into the timer wheel of an\n * idle CPU then this timer might expire before the next timer event\n * which is scheduled to wake up that CPU. In case of a completely\n * idle system the next event might even be infinite time into the\n * future. wake_up_idle_cpu() ensures that the CPU is woken up and\n * leaves the inner idle loop so the newly added timer is taken into\n * account when the CPU goes back to idle and evaluates the timer\n * wheel for the next timer event.\n */\nvoid wake_up_idle_cpu(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tif (cpu == smp_processor_id())\n\t\treturn;\n\n\t/*\n\t * This is safe, as this function is called with the timer\n\t * wheel base lock of (cpu) held. When the CPU is on the way\n\t * to idle and has not yet set rq->curr to idle then it will\n\t * be serialized on the timer wheel base lock and take the new\n\t * timer into account automatically.\n\t */\n\tif (rq->curr != rq->idle)\n\t\treturn;\n\n\t/*\n\t * We can set TIF_RESCHED on the idle task of the other CPU\n\t * lockless. The worst case is that the other CPU runs the\n\t * idle task through an additional NOOP schedule()\n\t */\n\tset_tsk_need_resched(rq->idle);\n\n\t/* NEED_RESCHED must be visible before we test polling */\n\tsmp_mb();\n\tif (!tsk_is_polling(rq->idle))\n\t\tsmp_send_reschedule(cpu);\n}\n\n#endif /* CONFIG_NO_HZ */\n\nstatic u64 sched_avg_period(void)\n{\n\treturn (u64)sysctl_sched_time_avg * NSEC_PER_MSEC / 2;\n}\n\nstatic void sched_avg_update(struct rq *rq)\n{\n\ts64 period = sched_avg_period();\n\n\twhile ((s64)(rq->clock - rq->age_stamp) > period) {\n\t\t/*\n\t\t * Inline assembly required to prevent the compiler\n\t\t * optimising this loop into a divmod call.\n\t\t * See __iter_div_u64_rem() for another example of this.\n\t\t */\n\t\tasm(\"\" : \"+rm\" (rq->age_stamp));\n\t\trq->age_stamp += period;\n\t\trq->rt_avg /= 2;\n\t}\n}\n\nstatic void sched_rt_avg_update(struct rq *rq, u64 rt_delta)\n{\n\trq->rt_avg += rt_delta;\n\tsched_avg_update(rq);\n}\n\n#else /* !CONFIG_SMP */\nstatic void resched_task(struct task_struct *p)\n{\n\tassert_raw_spin_locked(&task_rq(p)->lock);\n\tset_tsk_need_resched(p);\n}\n\nstatic void sched_rt_avg_update(struct rq *rq, u64 rt_delta)\n{\n}\n\nstatic void sched_avg_update(struct rq *rq)\n{\n}\n#endif /* CONFIG_SMP */\n\n#if BITS_PER_LONG == 32\n# define WMULT_CONST\t(~0UL)\n#else\n# define WMULT_CONST\t(1UL << 32)\n#endif\n\n#define WMULT_SHIFT\t32\n\n/*\n * Shift right and round:\n */\n#define SRR(x, y) (((x) + (1UL << ((y) - 1))) >> (y))\n\n/*\n * delta *= weight / lw\n */\nstatic unsigned long\ncalc_delta_mine(unsigned long delta_exec, unsigned long weight,\n\t\tstruct load_weight *lw)\n{\n\tu64 tmp;\n\n\t/*\n\t * weight can be less than 2^SCHED_LOAD_RESOLUTION for task group sched\n\t * entities since MIN_SHARES = 2. Treat weight as 1 if less than\n\t * 2^SCHED_LOAD_RESOLUTION.\n\t */\n\tif (likely(weight > (1UL << SCHED_LOAD_RESOLUTION)))\n\t\ttmp = (u64)delta_exec * scale_load_down(weight);\n\telse\n\t\ttmp = (u64)delta_exec;\n\n\tif (!lw->inv_weight) {\n\t\tunsigned long w = scale_load_down(lw->weight);\n\n\t\tif (BITS_PER_LONG > 32 && unlikely(w >= WMULT_CONST))\n\t\t\tlw->inv_weight = 1;\n\t\telse if (unlikely(!w))\n\t\t\tlw->inv_weight = WMULT_CONST;\n\t\telse\n\t\t\tlw->inv_weight = WMULT_CONST / w;\n\t}\n\n\t/*\n\t * Check whether we'd overflow the 64-bit multiplication:\n\t */\n\tif (unlikely(tmp > WMULT_CONST))\n\t\ttmp = SRR(SRR(tmp, WMULT_SHIFT/2) * lw->inv_weight,\n\t\t\tWMULT_SHIFT/2);\n\telse\n\t\ttmp = SRR(tmp * lw->inv_weight, WMULT_SHIFT);\n\n\treturn (unsigned long)min(tmp, (u64)(unsigned long)LONG_MAX);\n}\n\nstatic inline void update_load_add(struct load_weight *lw, unsigned long inc)\n{\n\tlw->weight += inc;\n\tlw->inv_weight = 0;\n}\n\nstatic inline void update_load_sub(struct load_weight *lw, unsigned long dec)\n{\n\tlw->weight -= dec;\n\tlw->inv_weight = 0;\n}\n\nstatic inline void update_load_set(struct load_weight *lw, unsigned long w)\n{\n\tlw->weight = w;\n\tlw->inv_weight = 0;\n}\n\n/*\n * To aid in avoiding the subversion of \"niceness\" due to uneven distribution\n * of tasks with abnormal \"nice\" values across CPUs the contribution that\n * each task makes to its run queue's load is weighted according to its\n * scheduling class and \"nice\" value. For SCHED_NORMAL tasks this is just a\n * scaled version of the new time slice allocation that they receive on time\n * slice expiry etc.\n */\n\n#define WEIGHT_IDLEPRIO                3\n#define WMULT_IDLEPRIO         1431655765\n\n/*\n * Nice levels are multiplicative, with a gentle 10% change for every\n * nice level changed. I.e. when a CPU-bound task goes from nice 0 to\n * nice 1, it will get ~10% less CPU time than another CPU-bound task\n * that remained on nice 0.\n *\n * The \"10% effect\" is relative and cumulative: from _any_ nice level,\n * if you go up 1 level, it's -10% CPU usage, if you go down 1 level\n * it's +10% CPU usage. (to achieve that we use a multiplier of 1.25.\n * If a task goes up by ~10% and another task goes down by ~10% then\n * the relative distance between them is ~25%.)\n */\nstatic const int prio_to_weight[40] = {\n /* -20 */     88761,     71755,     56483,     46273,     36291,\n /* -15 */     29154,     23254,     18705,     14949,     11916,\n /* -10 */      9548,      7620,      6100,      4904,      3906,\n /*  -5 */      3121,      2501,      1991,      1586,      1277,\n /*   0 */      1024,       820,       655,       526,       423,\n /*   5 */       335,       272,       215,       172,       137,\n /*  10 */       110,        87,        70,        56,        45,\n /*  15 */        36,        29,        23,        18,        15,\n};\n\n/*\n * Inverse (2^32/x) values of the prio_to_weight[] array, precalculated.\n *\n * In cases where the weight does not change often, we can use the\n * precalculated inverse to speed up arithmetics by turning divisions\n * into multiplications:\n */\nstatic const u32 prio_to_wmult[40] = {\n /* -20 */     48388,     59856,     76040,     92818,    118348,\n /* -15 */    147320,    184698,    229616,    287308,    360437,\n /* -10 */    449829,    563644,    704093,    875809,   1099582,\n /*  -5 */   1376151,   1717300,   2157191,   2708050,   3363326,\n /*   0 */   4194304,   5237765,   6557202,   8165337,  10153587,\n /*   5 */  12820798,  15790321,  19976592,  24970740,  31350126,\n /*  10 */  39045157,  49367440,  61356676,  76695844,  95443717,\n /*  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,\n};\n\n/* Time spent by the tasks of the cpu accounting group executing in ... */\nenum cpuacct_stat_index {\n\tCPUACCT_STAT_USER,\t/* ... user mode */\n\tCPUACCT_STAT_SYSTEM,\t/* ... kernel mode */\n\n\tCPUACCT_STAT_NSTATS,\n};\n\n#ifdef CONFIG_CGROUP_CPUACCT\nstatic void cpuacct_charge(struct task_struct *tsk, u64 cputime);\nstatic void cpuacct_update_stats(struct task_struct *tsk,\n\t\tenum cpuacct_stat_index idx, cputime_t val);\n#else\nstatic inline void cpuacct_charge(struct task_struct *tsk, u64 cputime) {}\nstatic inline void cpuacct_update_stats(struct task_struct *tsk,\n\t\tenum cpuacct_stat_index idx, cputime_t val) {}\n#endif\n\nstatic inline void inc_cpu_load(struct rq *rq, unsigned long load)\n{\n\tupdate_load_add(&rq->load, load);\n}\n\nstatic inline void dec_cpu_load(struct rq *rq, unsigned long load)\n{\n\tupdate_load_sub(&rq->load, load);\n}\n\n#if (defined(CONFIG_SMP) && defined(CONFIG_FAIR_GROUP_SCHED)) || defined(CONFIG_RT_GROUP_SCHED)\ntypedef int (*tg_visitor)(struct task_group *, void *);\n\n/*\n * Iterate the full tree, calling @down when first entering a node and @up when\n * leaving it for the final time.\n */\nstatic int walk_tg_tree(tg_visitor down, tg_visitor up, void *data)\n{\n\tstruct task_group *parent, *child;\n\tint ret;\n\n\trcu_read_lock();\n\tparent = &root_task_group;\ndown:\n\tret = (*down)(parent, data);\n\tif (ret)\n\t\tgoto out_unlock;\n\tlist_for_each_entry_rcu(child, &parent->children, siblings) {\n\t\tparent = child;\n\t\tgoto down;\n\nup:\n\t\tcontinue;\n\t}\n\tret = (*up)(parent, data);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tchild = parent;\n\tparent = parent->parent;\n\tif (parent)\n\t\tgoto up;\nout_unlock:\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic int tg_nop(struct task_group *tg, void *data)\n{\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_SMP\n/* Used instead of source_load when we know the type == 0 */\nstatic unsigned long weighted_cpuload(const int cpu)\n{\n\treturn cpu_rq(cpu)->load.weight;\n}\n\n/*\n * Return a low guess at the load of a migration-source cpu weighted\n * according to the scheduling class and \"nice\" value.\n *\n * We want to under-estimate the load of migration sources, to\n * balance conservatively.\n */\nstatic unsigned long source_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(cpu);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn min(rq->cpu_load[type-1], total);\n}\n\n/*\n * Return a high guess at the load of a migration-target cpu weighted\n * according to the scheduling class and \"nice\" value.\n */\nstatic unsigned long target_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(cpu);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn max(rq->cpu_load[type-1], total);\n}\n\nstatic unsigned long power_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_power;\n}\n\nstatic int task_hot(struct task_struct *p, u64 now, struct sched_domain *sd);\n\nstatic unsigned long cpu_avg_load_per_task(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long nr_running = ACCESS_ONCE(rq->nr_running);\n\n\tif (nr_running)\n\t\trq->avg_load_per_task = rq->load.weight / nr_running;\n\telse\n\t\trq->avg_load_per_task = 0;\n\n\treturn rq->avg_load_per_task;\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\n/*\n * Compute the cpu's hierarchical load factor for each task group.\n * This needs to be done in a top-down fashion because the load of a child\n * group is a fraction of its parents load.\n */\nstatic int tg_load_down(struct task_group *tg, void *data)\n{\n\tunsigned long load;\n\tlong cpu = (long)data;\n\n\tif (!tg->parent) {\n\t\tload = cpu_rq(cpu)->load.weight;\n\t} else {\n\t\tload = tg->parent->cfs_rq[cpu]->h_load;\n\t\tload *= tg->se[cpu]->load.weight;\n\t\tload /= tg->parent->cfs_rq[cpu]->load.weight + 1;\n\t}\n\n\ttg->cfs_rq[cpu]->h_load = load;\n\n\treturn 0;\n}\n\nstatic void update_h_load(long cpu)\n{\n\twalk_tg_tree(tg_load_down, tg_nop, (void *)cpu);\n}\n\n#endif\n\n#ifdef CONFIG_PREEMPT\n\nstatic void double_rq_lock(struct rq *rq1, struct rq *rq2);\n\n/*\n * fair double_lock_balance: Safely acquires both rq->locks in a fair\n * way at the expense of forcing extra atomic operations in all\n * invocations.  This assures that the double_lock is acquired using the\n * same underlying policy as the spinlock_t on this architecture, which\n * reduces latency compared to the unfair variant below.  However, it\n * also adds more overhead and therefore may reduce throughput.\n */\nstatic inline int _double_lock_balance(struct rq *this_rq, struct rq *busiest)\n\t__releases(this_rq->lock)\n\t__acquires(busiest->lock)\n\t__acquires(this_rq->lock)\n{\n\traw_spin_unlock(&this_rq->lock);\n\tdouble_rq_lock(this_rq, busiest);\n\n\treturn 1;\n}\n\n#else\n/*\n * Unfair double_lock_balance: Optimizes throughput at the expense of\n * latency by eliminating extra atomic operations when the locks are\n * already in proper order on entry.  This favors lower cpu-ids and will\n * grant the double lock to lower cpus over higher ids under contention,\n * regardless of entry order into the function.\n */\nstatic int _double_lock_balance(struct rq *this_rq, struct rq *busiest)\n\t__releases(this_rq->lock)\n\t__acquires(busiest->lock)\n\t__acquires(this_rq->lock)\n{\n\tint ret = 0;\n\n\tif (unlikely(!raw_spin_trylock(&busiest->lock))) {\n\t\tif (busiest < this_rq) {\n\t\t\traw_spin_unlock(&this_rq->lock);\n\t\t\traw_spin_lock(&busiest->lock);\n\t\t\traw_spin_lock_nested(&this_rq->lock,\n\t\t\t\t\t      SINGLE_DEPTH_NESTING);\n\t\t\tret = 1;\n\t\t} else\n\t\t\traw_spin_lock_nested(&busiest->lock,\n\t\t\t\t\t      SINGLE_DEPTH_NESTING);\n\t}\n\treturn ret;\n}\n\n#endif /* CONFIG_PREEMPT */\n\n/*\n * double_lock_balance - lock the busiest runqueue, this_rq is locked already.\n */\nstatic int double_lock_balance(struct rq *this_rq, struct rq *busiest)\n{\n\tif (unlikely(!irqs_disabled())) {\n\t\t/* printk() doesn't work good under rq->lock */\n\t\traw_spin_unlock(&this_rq->lock);\n\t\tBUG_ON(1);\n\t}\n\n\treturn _double_lock_balance(this_rq, busiest);\n}\n\nstatic inline void double_unlock_balance(struct rq *this_rq, struct rq *busiest)\n\t__releases(busiest->lock)\n{\n\traw_spin_unlock(&busiest->lock);\n\tlock_set_subclass(&this_rq->lock.dep_map, 0, _RET_IP_);\n}\n\n/*\n * double_rq_lock - safely lock two runqueues\n *\n * Note this does not disable interrupts like task_rq_lock,\n * you need to do so manually before calling.\n */\nstatic void double_rq_lock(struct rq *rq1, struct rq *rq2)\n\t__acquires(rq1->lock)\n\t__acquires(rq2->lock)\n{\n\tBUG_ON(!irqs_disabled());\n\tif (rq1 == rq2) {\n\t\traw_spin_lock(&rq1->lock);\n\t\t__acquire(rq2->lock);\t/* Fake it out ;) */\n\t} else {\n\t\tif (rq1 < rq2) {\n\t\t\traw_spin_lock(&rq1->lock);\n\t\t\traw_spin_lock_nested(&rq2->lock, SINGLE_DEPTH_NESTING);\n\t\t} else {\n\t\t\traw_spin_lock(&rq2->lock);\n\t\t\traw_spin_lock_nested(&rq1->lock, SINGLE_DEPTH_NESTING);\n\t\t}\n\t}\n}\n\n/*\n * double_rq_unlock - safely unlock two runqueues\n *\n * Note this does not restore interrupts like task_rq_unlock,\n * you need to do so manually after calling.\n */\nstatic void double_rq_unlock(struct rq *rq1, struct rq *rq2)\n\t__releases(rq1->lock)\n\t__releases(rq2->lock)\n{\n\traw_spin_unlock(&rq1->lock);\n\tif (rq1 != rq2)\n\t\traw_spin_unlock(&rq2->lock);\n\telse\n\t\t__release(rq2->lock);\n}\n\n#else /* CONFIG_SMP */\n\n/*\n * double_rq_lock - safely lock two runqueues\n *\n * Note this does not disable interrupts like task_rq_lock,\n * you need to do so manually before calling.\n */\nstatic void double_rq_lock(struct rq *rq1, struct rq *rq2)\n\t__acquires(rq1->lock)\n\t__acquires(rq2->lock)\n{\n\tBUG_ON(!irqs_disabled());\n\tBUG_ON(rq1 != rq2);\n\traw_spin_lock(&rq1->lock);\n\t__acquire(rq2->lock);\t/* Fake it out ;) */\n}\n\n/*\n * double_rq_unlock - safely unlock two runqueues\n *\n * Note this does not restore interrupts like task_rq_unlock,\n * you need to do so manually after calling.\n */\nstatic void double_rq_unlock(struct rq *rq1, struct rq *rq2)\n\t__releases(rq1->lock)\n\t__releases(rq2->lock)\n{\n\tBUG_ON(rq1 != rq2);\n\traw_spin_unlock(&rq1->lock);\n\t__release(rq2->lock);\n}\n\n#endif\n\nstatic void calc_load_account_idle(struct rq *this_rq);\nstatic void update_sysctl(void);\nstatic int get_update_sysctl_factor(void);\nstatic void update_cpu_load(struct rq *this_rq);\n\nstatic inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)\n{\n\tset_task_rq(p, cpu);\n#ifdef CONFIG_SMP\n\t/*\n\t * After ->cpu is set up to a new value, task_rq_lock(p, ...) can be\n\t * successfuly executed on another CPU. We must ensure that updates of\n\t * per-task data have been completed by this moment.\n\t */\n\tsmp_wmb();\n\ttask_thread_info(p)->cpu = cpu;\n#endif\n}\n\nstatic const struct sched_class rt_sched_class;\n\n#define sched_class_highest (&stop_sched_class)\n#define for_each_class(class) \\\n   for (class = sched_class_highest; class; class = class->next)\n\n#include \"sched_stats.h\"\n\nstatic void inc_nr_running(struct rq *rq)\n{\n\trq->nr_running++;\n}\n\nstatic void dec_nr_running(struct rq *rq)\n{\n\trq->nr_running--;\n}\n\nstatic void set_load_weight(struct task_struct *p)\n{\n\tint prio = p->static_prio - MAX_RT_PRIO;\n\tstruct load_weight *load = &p->se.load;\n\n\t/*\n\t * SCHED_IDLE tasks get minimal weight:\n\t */\n\tif (p->policy == SCHED_IDLE) {\n\t\tload->weight = scale_load(WEIGHT_IDLEPRIO);\n\t\tload->inv_weight = WMULT_IDLEPRIO;\n\t\treturn;\n\t}\n\n\tload->weight = scale_load(prio_to_weight[prio]);\n\tload->inv_weight = prio_to_wmult[prio];\n}\n\nstatic void enqueue_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tupdate_rq_clock(rq);\n\tsched_info_queued(p);\n\tp->sched_class->enqueue_task(rq, p, flags);\n}\n\nstatic void dequeue_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tupdate_rq_clock(rq);\n\tsched_info_dequeued(p);\n\tp->sched_class->dequeue_task(rq, p, flags);\n}\n\n/*\n * activate_task - move a task to the runqueue.\n */\nstatic void activate_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tif (task_contributes_to_load(p))\n\t\trq->nr_uninterruptible--;\n\n\tenqueue_task(rq, p, flags);\n\tinc_nr_running(rq);\n}\n\n/*\n * deactivate_task - remove a task from the runqueue.\n */\nstatic void deactivate_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tif (task_contributes_to_load(p))\n\t\trq->nr_uninterruptible++;\n\n\tdequeue_task(rq, p, flags);\n\tdec_nr_running(rq);\n}\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n\n/*\n * There are no locks covering percpu hardirq/softirq time.\n * They are only modified in account_system_vtime, on corresponding CPU\n * with interrupts disabled. So, writes are safe.\n * They are read and saved off onto struct rq in update_rq_clock().\n * This may result in other CPU reading this CPU's irq time and can\n * race with irq/account_system_vtime on this CPU. We would either get old\n * or new value with a side effect of accounting a slice of irq time to wrong\n * task when irq is in progress while we read rq->clock. That is a worthy\n * compromise in place of having locks on each irq in account_system_time.\n */\nstatic DEFINE_PER_CPU(u64, cpu_hardirq_time);\nstatic DEFINE_PER_CPU(u64, cpu_softirq_time);\n\nstatic DEFINE_PER_CPU(u64, irq_start_time);\nstatic int sched_clock_irqtime;\n\nvoid enable_sched_clock_irqtime(void)\n{\n\tsched_clock_irqtime = 1;\n}\n\nvoid disable_sched_clock_irqtime(void)\n{\n\tsched_clock_irqtime = 0;\n}\n\n#ifndef CONFIG_64BIT\nstatic DEFINE_PER_CPU(seqcount_t, irq_time_seq);\n\nstatic inline void irq_time_write_begin(void)\n{\n\t__this_cpu_inc(irq_time_seq.sequence);\n\tsmp_wmb();\n}\n\nstatic inline void irq_time_write_end(void)\n{\n\tsmp_wmb();\n\t__this_cpu_inc(irq_time_seq.sequence);\n}\n\nstatic inline u64 irq_time_read(int cpu)\n{\n\tu64 irq_time;\n\tunsigned seq;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&per_cpu(irq_time_seq, cpu));\n\t\tirq_time = per_cpu(cpu_softirq_time, cpu) +\n\t\t\t   per_cpu(cpu_hardirq_time, cpu);\n\t} while (read_seqcount_retry(&per_cpu(irq_time_seq, cpu), seq));\n\n\treturn irq_time;\n}\n#else /* CONFIG_64BIT */\nstatic inline void irq_time_write_begin(void)\n{\n}\n\nstatic inline void irq_time_write_end(void)\n{\n}\n\nstatic inline u64 irq_time_read(int cpu)\n{\n\treturn per_cpu(cpu_softirq_time, cpu) + per_cpu(cpu_hardirq_time, cpu);\n}\n#endif /* CONFIG_64BIT */\n\n/*\n * Called before incrementing preempt_count on {soft,}irq_enter\n * and before decrementing preempt_count on {soft,}irq_exit.\n */\nvoid account_system_vtime(struct task_struct *curr)\n{\n\tunsigned long flags;\n\ts64 delta;\n\tint cpu;\n\n\tif (!sched_clock_irqtime)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tcpu = smp_processor_id();\n\tdelta = sched_clock_cpu(cpu) - __this_cpu_read(irq_start_time);\n\t__this_cpu_add(irq_start_time, delta);\n\n\tirq_time_write_begin();\n\t/*\n\t * We do not account for softirq time from ksoftirqd here.\n\t * We want to continue accounting softirq time to ksoftirqd thread\n\t * in that case, so as not to confuse scheduler with a special task\n\t * that do not consume any time, but still wants to run.\n\t */\n\tif (hardirq_count())\n\t\t__this_cpu_add(cpu_hardirq_time, delta);\n\telse if (in_serving_softirq() && curr != this_cpu_ksoftirqd())\n\t\t__this_cpu_add(cpu_softirq_time, delta);\n\n\tirq_time_write_end();\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(account_system_vtime);\n\nstatic void update_rq_clock_task(struct rq *rq, s64 delta)\n{\n\ts64 irq_delta;\n\n\tirq_delta = irq_time_read(cpu_of(rq)) - rq->prev_irq_time;\n\n\t/*\n\t * Since irq_time is only updated on {soft,}irq_exit, we might run into\n\t * this case when a previous update_rq_clock() happened inside a\n\t * {soft,}irq region.\n\t *\n\t * When this happens, we stop ->clock_task and only update the\n\t * prev_irq_time stamp to account for the part that fit, so that a next\n\t * update will consume the rest. This ensures ->clock_task is\n\t * monotonic.\n\t *\n\t * It does however cause some slight miss-attribution of {soft,}irq\n\t * time, a more accurate solution would be to update the irq_time using\n\t * the current rq->clock timestamp, except that would require using\n\t * atomic ops.\n\t */\n\tif (irq_delta > delta)\n\t\tirq_delta = delta;\n\n\trq->prev_irq_time += irq_delta;\n\tdelta -= irq_delta;\n\trq->clock_task += delta;\n\n\tif (irq_delta && sched_feat(NONIRQ_POWER))\n\t\tsched_rt_avg_update(rq, irq_delta);\n}\n\nstatic int irqtime_account_hi_update(void)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tunsigned long flags;\n\tu64 latest_ns;\n\tint ret = 0;\n\n\tlocal_irq_save(flags);\n\tlatest_ns = this_cpu_read(cpu_hardirq_time);\n\tif (cputime64_gt(nsecs_to_cputime64(latest_ns), cpustat->irq))\n\t\tret = 1;\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\n\nstatic int irqtime_account_si_update(void)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tunsigned long flags;\n\tu64 latest_ns;\n\tint ret = 0;\n\n\tlocal_irq_save(flags);\n\tlatest_ns = this_cpu_read(cpu_softirq_time);\n\tif (cputime64_gt(nsecs_to_cputime64(latest_ns), cpustat->softirq))\n\t\tret = 1;\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\n\n#else /* CONFIG_IRQ_TIME_ACCOUNTING */\n\n#define sched_clock_irqtime\t(0)\n\nstatic void update_rq_clock_task(struct rq *rq, s64 delta)\n{\n\trq->clock_task += delta;\n}\n\n#endif /* CONFIG_IRQ_TIME_ACCOUNTING */\n\n#include \"sched_idletask.c\"\n#include \"sched_fair.c\"\n#include \"sched_rt.c\"\n#include \"sched_autogroup.c\"\n#include \"sched_stoptask.c\"\n#ifdef CONFIG_SCHED_DEBUG\n# include \"sched_debug.c\"\n#endif\n\nvoid sched_set_stop_task(int cpu, struct task_struct *stop)\n{\n\tstruct sched_param param = { .sched_priority = MAX_RT_PRIO - 1 };\n\tstruct task_struct *old_stop = cpu_rq(cpu)->stop;\n\n\tif (stop) {\n\t\t/*\n\t\t * Make it appear like a SCHED_FIFO task, its something\n\t\t * userspace knows about and won't get confused about.\n\t\t *\n\t\t * Also, it will make PI more or less work without too\n\t\t * much confusion -- but then, stop work should not\n\t\t * rely on PI working anyway.\n\t\t */\n\t\tsched_setscheduler_nocheck(stop, SCHED_FIFO, &param);\n\n\t\tstop->sched_class = &stop_sched_class;\n\t}\n\n\tcpu_rq(cpu)->stop = stop;\n\n\tif (old_stop) {\n\t\t/*\n\t\t * Reset it back to a normal scheduling class so that\n\t\t * it can die in pieces.\n\t\t */\n\t\told_stop->sched_class = &rt_sched_class;\n\t}\n}\n\n/*\n * __normal_prio - return the priority that is based on the static prio\n */\nstatic inline int __normal_prio(struct task_struct *p)\n{\n\treturn p->static_prio;\n}\n\n/*\n * Calculate the expected normal priority: i.e. priority\n * without taking RT-inheritance into account. Might be\n * boosted by interactivity modifiers. Changes upon fork,\n * setprio syscalls, and whenever the interactivity\n * estimator recalculates.\n */\nstatic inline int normal_prio(struct task_struct *p)\n{\n\tint prio;\n\n\tif (task_has_rt_policy(p))\n\t\tprio = MAX_RT_PRIO-1 - p->rt_priority;\n\telse\n\t\tprio = __normal_prio(p);\n\treturn prio;\n}\n\n/*\n * Calculate the current priority, i.e. the priority\n * taken into account by the scheduler. This value might\n * be boosted by RT tasks, or might be boosted by\n * interactivity modifiers. Will be RT if the task got\n * RT-boosted. If not then it returns p->normal_prio.\n */\nstatic int effective_prio(struct task_struct *p)\n{\n\tp->normal_prio = normal_prio(p);\n\t/*\n\t * If we are RT tasks or we were boosted to RT priority,\n\t * keep the priority unchanged. Otherwise, update priority\n\t * to the normal priority:\n\t */\n\tif (!rt_prio(p->prio))\n\t\treturn p->normal_prio;\n\treturn p->prio;\n}\n\n/**\n * task_curr - is this task currently executing on a CPU?\n * @p: the task in question.\n */\ninline int task_curr(const struct task_struct *p)\n{\n\treturn cpu_curr(task_cpu(p)) == p;\n}\n\nstatic inline void check_class_changed(struct rq *rq, struct task_struct *p,\n\t\t\t\t       const struct sched_class *prev_class,\n\t\t\t\t       int oldprio)\n{\n\tif (prev_class != p->sched_class) {\n\t\tif (prev_class->switched_from)\n\t\t\tprev_class->switched_from(rq, p);\n\t\tp->sched_class->switched_to(rq, p);\n\t} else if (oldprio != p->prio)\n\t\tp->sched_class->prio_changed(rq, p, oldprio);\n}\n\nstatic void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags)\n{\n\tconst struct sched_class *class;\n\n\tif (p->sched_class == rq->curr->sched_class) {\n\t\trq->curr->sched_class->check_preempt_curr(rq, p, flags);\n\t} else {\n\t\tfor_each_class(class) {\n\t\t\tif (class == rq->curr->sched_class)\n\t\t\t\tbreak;\n\t\t\tif (class == p->sched_class) {\n\t\t\t\tresched_task(rq->curr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * A queue event has occurred, and we're going to schedule.  In\n\t * this case, we can save a useless back to back clock update.\n\t */\n\tif (rq->curr->on_rq && test_tsk_need_resched(rq->curr))\n\t\trq->skip_clock_update = 1;\n}\n\n#ifdef CONFIG_SMP\n/*\n * Is this task likely cache-hot:\n */\nstatic int\ntask_hot(struct task_struct *p, u64 now, struct sched_domain *sd)\n{\n\ts64 delta;\n\n\tif (p->sched_class != &fair_sched_class)\n\t\treturn 0;\n\n\tif (unlikely(p->policy == SCHED_IDLE))\n\t\treturn 0;\n\n\t/*\n\t * Buddy candidates are cache hot:\n\t */\n\tif (sched_feat(CACHE_HOT_BUDDY) && this_rq()->nr_running &&\n\t\t\t(&p->se == cfs_rq_of(&p->se)->next ||\n\t\t\t &p->se == cfs_rq_of(&p->se)->last))\n\t\treturn 1;\n\n\tif (sysctl_sched_migration_cost == -1)\n\t\treturn 1;\n\tif (sysctl_sched_migration_cost == 0)\n\t\treturn 0;\n\n\tdelta = now - p->se.exec_start;\n\n\treturn delta < (s64)sysctl_sched_migration_cost;\n}\n\nvoid set_task_cpu(struct task_struct *p, unsigned int new_cpu)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\t/*\n\t * We should never call set_task_cpu() on a blocked task,\n\t * ttwu() will sort out the placement.\n\t */\n\tWARN_ON_ONCE(p->state != TASK_RUNNING && p->state != TASK_WAKING &&\n\t\t\t!(task_thread_info(p)->preempt_count & PREEMPT_ACTIVE));\n\n#ifdef CONFIG_LOCKDEP\n\t/*\n\t * The caller should hold either p->pi_lock or rq->lock, when changing\n\t * a task's CPU. ->pi_lock for waking tasks, rq->lock for runnable tasks.\n\t *\n\t * sched_move_task() holds both and thus holding either pins the cgroup,\n\t * see set_task_rq().\n\t *\n\t * Furthermore, all task_rq users should acquire both locks, see\n\t * task_rq_lock().\n\t */\n\tWARN_ON_ONCE(debug_locks && !(lockdep_is_held(&p->pi_lock) ||\n\t\t\t\t      lockdep_is_held(&task_rq(p)->lock)));\n#endif\n#endif\n\n\ttrace_sched_migrate_task(p, new_cpu);\n\n\tif (task_cpu(p) != new_cpu) {\n\t\tp->se.nr_migrations++;\n\t\tperf_sw_event(PERF_COUNT_SW_CPU_MIGRATIONS, 1, 1, NULL, 0);\n\t}\n\n\t__set_task_cpu(p, new_cpu);\n}\n\nstruct migration_arg {\n\tstruct task_struct *task;\n\tint dest_cpu;\n};\n\nstatic int migration_cpu_stop(void *data);\n\n/*\n * wait_task_inactive - wait for a thread to unschedule.\n *\n * If @match_state is nonzero, it's the @p->state value just checked and\n * not expected to change.  If it changes, i.e. @p might have woken up,\n * then return zero.  When we succeed in waiting for @p to be off its CPU,\n * we return a positive number (its total switch count).  If a second call\n * a short while later returns the same number, the caller can be sure that\n * @p has remained unscheduled the whole time.\n *\n * The caller must ensure that the task *will* unschedule sometime soon,\n * else this function might spin for a *long* time. This function can't\n * be called with interrupts off, or it may introduce deadlock with\n * smp_call_function() if an IPI is sent by the same process we are\n * waiting to become inactive.\n */\nunsigned long wait_task_inactive(struct task_struct *p, long match_state)\n{\n\tunsigned long flags;\n\tint running, on_rq;\n\tunsigned long ncsw;\n\tstruct rq *rq;\n\n\tfor (;;) {\n\t\t/*\n\t\t * We do the initial early heuristics without holding\n\t\t * any task-queue locks at all. We'll only try to get\n\t\t * the runqueue lock when things look like they will\n\t\t * work out!\n\t\t */\n\t\trq = task_rq(p);\n\n\t\t/*\n\t\t * If the task is actively running on another CPU\n\t\t * still, just relax and busy-wait without holding\n\t\t * any locks.\n\t\t *\n\t\t * NOTE! Since we don't hold any locks, it's not\n\t\t * even sure that \"rq\" stays as the right runqueue!\n\t\t * But we don't care, since \"task_running()\" will\n\t\t * return false if the runqueue has changed and p\n\t\t * is actually now running somewhere else!\n\t\t */\n\t\twhile (task_running(rq, p)) {\n\t\t\tif (match_state && unlikely(p->state != match_state))\n\t\t\t\treturn 0;\n\t\t\tcpu_relax();\n\t\t}\n\n\t\t/*\n\t\t * Ok, time to look more closely! We need the rq\n\t\t * lock now, to be *sure*. If we're wrong, we'll\n\t\t * just go back and repeat.\n\t\t */\n\t\trq = task_rq_lock(p, &flags);\n\t\ttrace_sched_wait_task(p);\n\t\trunning = task_running(rq, p);\n\t\ton_rq = p->on_rq;\n\t\tncsw = 0;\n\t\tif (!match_state || p->state == match_state)\n\t\t\tncsw = p->nvcsw | LONG_MIN; /* sets MSB */\n\t\ttask_rq_unlock(rq, p, &flags);\n\n\t\t/*\n\t\t * If it changed from the expected state, bail out now.\n\t\t */\n\t\tif (unlikely(!ncsw))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Was it really running after all now that we\n\t\t * checked with the proper locks actually held?\n\t\t *\n\t\t * Oops. Go back and try again..\n\t\t */\n\t\tif (unlikely(running)) {\n\t\t\tcpu_relax();\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * It's not enough that it's not actively running,\n\t\t * it must be off the runqueue _entirely_, and not\n\t\t * preempted!\n\t\t *\n\t\t * So if it was still runnable (but just not actively\n\t\t * running right now), it's preempted, and we should\n\t\t * yield - it could be a while.\n\t\t */\n\t\tif (unlikely(on_rq)) {\n\t\t\tktime_t to = ktime_set(0, NSEC_PER_SEC/HZ);\n\n\t\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\t\tschedule_hrtimeout(&to, HRTIMER_MODE_REL);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Ahh, all good. It wasn't running, and it wasn't\n\t\t * runnable, which means that it will never become\n\t\t * running in the future either. We're all done!\n\t\t */\n\t\tbreak;\n\t}\n\n\treturn ncsw;\n}\n\n/***\n * kick_process - kick a running thread to enter/exit the kernel\n * @p: the to-be-kicked thread\n *\n * Cause a process which is running on another CPU to enter\n * kernel-mode, without any delay. (to get signals handled.)\n *\n * NOTE: this function doesn't have to take the runqueue lock,\n * because all it wants to ensure is that the remote task enters\n * the kernel. If the IPI races and the task has been migrated\n * to another CPU then no harm is done and the purpose has been\n * achieved as well.\n */\nvoid kick_process(struct task_struct *p)\n{\n\tint cpu;\n\n\tpreempt_disable();\n\tcpu = task_cpu(p);\n\tif ((cpu != smp_processor_id()) && task_curr(p))\n\t\tsmp_send_reschedule(cpu);\n\tpreempt_enable();\n}\nEXPORT_SYMBOL_GPL(kick_process);\n#endif /* CONFIG_SMP */\n\n#ifdef CONFIG_SMP\n/*\n * ->cpus_allowed is protected by both rq->lock and p->pi_lock\n */\nstatic int select_fallback_rq(int cpu, struct task_struct *p)\n{\n\tint dest_cpu;\n\tconst struct cpumask *nodemask = cpumask_of_node(cpu_to_node(cpu));\n\n\t/* Look for allowed, online CPU in same node. */\n\tfor_each_cpu_and(dest_cpu, nodemask, cpu_active_mask)\n\t\tif (cpumask_test_cpu(dest_cpu, &p->cpus_allowed))\n\t\t\treturn dest_cpu;\n\n\t/* Any allowed, online CPU? */\n\tdest_cpu = cpumask_any_and(&p->cpus_allowed, cpu_active_mask);\n\tif (dest_cpu < nr_cpu_ids)\n\t\treturn dest_cpu;\n\n\t/* No more Mr. Nice Guy. */\n\tdest_cpu = cpuset_cpus_allowed_fallback(p);\n\t/*\n\t * Don't tell them about moving exiting tasks or\n\t * kernel threads (both mm NULL), since they never\n\t * leave kernel.\n\t */\n\tif (p->mm && printk_ratelimit()) {\n\t\tprintk(KERN_INFO \"process %d (%s) no longer affine to cpu%d\\n\",\n\t\t\t\ttask_pid_nr(p), p->comm, cpu);\n\t}\n\n\treturn dest_cpu;\n}\n\n/*\n * The caller (fork, wakeup) owns p->pi_lock, ->cpus_allowed is stable.\n */\nstatic inline\nint select_task_rq(struct task_struct *p, int sd_flags, int wake_flags)\n{\n\tint cpu = p->sched_class->select_task_rq(p, sd_flags, wake_flags);\n\n\t/*\n\t * In order not to call set_task_cpu() on a blocking task we need\n\t * to rely on ttwu() to place the task on a valid ->cpus_allowed\n\t * cpu.\n\t *\n\t * Since this is common to all placement strategies, this lives here.\n\t *\n\t * [ this allows ->select_task() to simply return task_cpu(p) and\n\t *   not worry about this generic constraint ]\n\t */\n\tif (unlikely(!cpumask_test_cpu(cpu, &p->cpus_allowed) ||\n\t\t     !cpu_online(cpu)))\n\t\tcpu = select_fallback_rq(task_cpu(p), p);\n\n\treturn cpu;\n}\n\nstatic void update_avg(u64 *avg, u64 sample)\n{\n\ts64 diff = sample - *avg;\n\t*avg += diff >> 3;\n}\n#endif\n\nstatic void\nttwu_stat(struct task_struct *p, int cpu, int wake_flags)\n{\n#ifdef CONFIG_SCHEDSTATS\n\tstruct rq *rq = this_rq();\n\n#ifdef CONFIG_SMP\n\tint this_cpu = smp_processor_id();\n\n\tif (cpu == this_cpu) {\n\t\tschedstat_inc(rq, ttwu_local);\n\t\tschedstat_inc(p, se.statistics.nr_wakeups_local);\n\t} else {\n\t\tstruct sched_domain *sd;\n\n\t\tschedstat_inc(p, se.statistics.nr_wakeups_remote);\n\t\trcu_read_lock();\n\t\tfor_each_domain(this_cpu, sd) {\n\t\t\tif (cpumask_test_cpu(cpu, sched_domain_span(sd))) {\n\t\t\t\tschedstat_inc(sd, ttwu_wake_remote);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tif (wake_flags & WF_MIGRATED)\n\t\tschedstat_inc(p, se.statistics.nr_wakeups_migrate);\n\n#endif /* CONFIG_SMP */\n\n\tschedstat_inc(rq, ttwu_count);\n\tschedstat_inc(p, se.statistics.nr_wakeups);\n\n\tif (wake_flags & WF_SYNC)\n\t\tschedstat_inc(p, se.statistics.nr_wakeups_sync);\n\n#endif /* CONFIG_SCHEDSTATS */\n}\n\nstatic void ttwu_activate(struct rq *rq, struct task_struct *p, int en_flags)\n{\n\tactivate_task(rq, p, en_flags);\n\tp->on_rq = 1;\n\n\t/* if a worker is waking up, notify workqueue */\n\tif (p->flags & PF_WQ_WORKER)\n\t\twq_worker_waking_up(p, cpu_of(rq));\n}\n\n/*\n * Mark the task runnable and perform wakeup-preemption.\n */\nstatic void\nttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags)\n{\n\ttrace_sched_wakeup(p, true);\n\tcheck_preempt_curr(rq, p, wake_flags);\n\n\tp->state = TASK_RUNNING;\n#ifdef CONFIG_SMP\n\tif (p->sched_class->task_woken)\n\t\tp->sched_class->task_woken(rq, p);\n\n\tif (unlikely(rq->idle_stamp)) {\n\t\tu64 delta = rq->clock - rq->idle_stamp;\n\t\tu64 max = 2*sysctl_sched_migration_cost;\n\n\t\tif (delta > max)\n\t\t\trq->avg_idle = max;\n\t\telse\n\t\t\tupdate_avg(&rq->avg_idle, delta);\n\t\trq->idle_stamp = 0;\n\t}\n#endif\n}\n\nstatic void\nttwu_do_activate(struct rq *rq, struct task_struct *p, int wake_flags)\n{\n#ifdef CONFIG_SMP\n\tif (p->sched_contributes_to_load)\n\t\trq->nr_uninterruptible--;\n#endif\n\n\tttwu_activate(rq, p, ENQUEUE_WAKEUP | ENQUEUE_WAKING);\n\tttwu_do_wakeup(rq, p, wake_flags);\n}\n\n/*\n * Called in case the task @p isn't fully descheduled from its runqueue,\n * in this case we must do a remote wakeup. Its a 'light' wakeup though,\n * since all we need to do is flip p->state to TASK_RUNNING, since\n * the task is still ->on_rq.\n */\nstatic int ttwu_remote(struct task_struct *p, int wake_flags)\n{\n\tstruct rq *rq;\n\tint ret = 0;\n\n\trq = __task_rq_lock(p);\n\tif (p->on_rq) {\n\t\tttwu_do_wakeup(rq, p, wake_flags);\n\t\tret = 1;\n\t}\n\t__task_rq_unlock(rq);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_SMP\nstatic void sched_ttwu_pending(void)\n{\n\tstruct rq *rq = this_rq();\n\tstruct task_struct *list = xchg(&rq->wake_list, NULL);\n\n\tif (!list)\n\t\treturn;\n\n\traw_spin_lock(&rq->lock);\n\n\twhile (list) {\n\t\tstruct task_struct *p = list;\n\t\tlist = list->wake_entry;\n\t\tttwu_do_activate(rq, p, 0);\n\t}\n\n\traw_spin_unlock(&rq->lock);\n}\n\nvoid scheduler_ipi(void)\n{\n\tsched_ttwu_pending();\n}\n\nstatic void ttwu_queue_remote(struct task_struct *p, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct task_struct *next = rq->wake_list;\n\n\tfor (;;) {\n\t\tstruct task_struct *old = next;\n\n\t\tp->wake_entry = next;\n\t\tnext = cmpxchg(&rq->wake_list, old, p);\n\t\tif (next == old)\n\t\t\tbreak;\n\t}\n\n\tif (!next)\n\t\tsmp_send_reschedule(cpu);\n}\n\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\nstatic int ttwu_activate_remote(struct task_struct *p, int wake_flags)\n{\n\tstruct rq *rq;\n\tint ret = 0;\n\n\trq = __task_rq_lock(p);\n\tif (p->on_cpu) {\n\t\tttwu_activate(rq, p, ENQUEUE_WAKEUP);\n\t\tttwu_do_wakeup(rq, p, wake_flags);\n\t\tret = 1;\n\t}\n\t__task_rq_unlock(rq);\n\n\treturn ret;\n\n}\n#endif /* __ARCH_WANT_INTERRUPTS_ON_CTXSW */\n#endif /* CONFIG_SMP */\n\nstatic void ttwu_queue(struct task_struct *p, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n#if defined(CONFIG_SMP)\n\tif (sched_feat(TTWU_QUEUE) && cpu != smp_processor_id()) {\n\t\tsched_clock_cpu(cpu); /* sync clocks x-cpu */\n\t\tttwu_queue_remote(p, cpu);\n\t\treturn;\n\t}\n#endif\n\n\traw_spin_lock(&rq->lock);\n\tttwu_do_activate(rq, p, 0);\n\traw_spin_unlock(&rq->lock);\n}\n\n/**\n * try_to_wake_up - wake up a thread\n * @p: the thread to be awakened\n * @state: the mask of task states that can be woken\n * @wake_flags: wake modifier flags (WF_*)\n *\n * Put it on the run-queue if it's not already there. The \"current\"\n * thread is always on the run-queue (except when the actual\n * re-schedule is in progress), and as such you're allowed to do\n * the simpler \"current->state = TASK_RUNNING\" to mark yourself\n * runnable without the overhead of this.\n *\n * Returns %true if @p was woken up, %false if it was already running\n * or @state didn't match @p's state.\n */\nstatic int\ntry_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)\n{\n\tunsigned long flags;\n\tint cpu, success = 0;\n\n\tsmp_wmb();\n\traw_spin_lock_irqsave(&p->pi_lock, flags);\n\tif (!(p->state & state))\n\t\tgoto out;\n\n\tsuccess = 1; /* we're going to change ->state */\n\tcpu = task_cpu(p);\n\n\tif (p->on_rq && ttwu_remote(p, wake_flags))\n\t\tgoto stat;\n\n#ifdef CONFIG_SMP\n\t/*\n\t * If the owning (remote) cpu is still in the middle of schedule() with\n\t * this task as prev, wait until its done referencing the task.\n\t */\n\twhile (p->on_cpu) {\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\t\t/*\n\t\t * In case the architecture enables interrupts in\n\t\t * context_switch(), we cannot busy wait, since that\n\t\t * would lead to deadlocks when an interrupt hits and\n\t\t * tries to wake up @prev. So bail and do a complete\n\t\t * remote wakeup.\n\t\t */\n\t\tif (ttwu_activate_remote(p, wake_flags))\n\t\t\tgoto stat;\n#else\n\t\tcpu_relax();\n#endif\n\t}\n\t/*\n\t * Pairs with the smp_wmb() in finish_lock_switch().\n\t */\n\tsmp_rmb();\n\n\tp->sched_contributes_to_load = !!task_contributes_to_load(p);\n\tp->state = TASK_WAKING;\n\n\tif (p->sched_class->task_waking)\n\t\tp->sched_class->task_waking(p);\n\n\tcpu = select_task_rq(p, SD_BALANCE_WAKE, wake_flags);\n\tif (task_cpu(p) != cpu) {\n\t\twake_flags |= WF_MIGRATED;\n\t\tset_task_cpu(p, cpu);\n\t}\n#endif /* CONFIG_SMP */\n\n\tttwu_queue(p, cpu);\nstat:\n\tttwu_stat(p, cpu, wake_flags);\nout:\n\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\n\treturn success;\n}\n\n/**\n * try_to_wake_up_local - try to wake up a local task with rq lock held\n * @p: the thread to be awakened\n *\n * Put @p on the run-queue if it's not already there. The caller must\n * ensure that this_rq() is locked, @p is bound to this_rq() and not\n * the current task.\n */\nstatic void try_to_wake_up_local(struct task_struct *p)\n{\n\tstruct rq *rq = task_rq(p);\n\n\tBUG_ON(rq != this_rq());\n\tBUG_ON(p == current);\n\tlockdep_assert_held(&rq->lock);\n\n\tif (!raw_spin_trylock(&p->pi_lock)) {\n\t\traw_spin_unlock(&rq->lock);\n\t\traw_spin_lock(&p->pi_lock);\n\t\traw_spin_lock(&rq->lock);\n\t}\n\n\tif (!(p->state & TASK_NORMAL))\n\t\tgoto out;\n\n\tif (!p->on_rq)\n\t\tttwu_activate(rq, p, ENQUEUE_WAKEUP);\n\n\tttwu_do_wakeup(rq, p, 0);\n\tttwu_stat(p, smp_processor_id(), 0);\nout:\n\traw_spin_unlock(&p->pi_lock);\n}\n\n/**\n * wake_up_process - Wake up a specific process\n * @p: The process to be woken up.\n *\n * Attempt to wake up the nominated process and move it to the set of runnable\n * processes.  Returns 1 if the process was woken up, 0 if it was already\n * running.\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nint wake_up_process(struct task_struct *p)\n{\n\treturn try_to_wake_up(p, TASK_ALL, 0);\n}\nEXPORT_SYMBOL(wake_up_process);\n\nint wake_up_state(struct task_struct *p, unsigned int state)\n{\n\treturn try_to_wake_up(p, state, 0);\n}\n\n/*\n * Perform scheduler related setup for a newly forked process p.\n * p is forked by current.\n *\n * __sched_fork() is basic setup used by init_idle() too:\n */\nstatic void __sched_fork(struct task_struct *p)\n{\n\tp->on_rq\t\t\t= 0;\n\n\tp->se.on_rq\t\t\t= 0;\n\tp->se.exec_start\t\t= 0;\n\tp->se.sum_exec_runtime\t\t= 0;\n\tp->se.prev_sum_exec_runtime\t= 0;\n\tp->se.nr_migrations\t\t= 0;\n\tp->se.vruntime\t\t\t= 0;\n\tINIT_LIST_HEAD(&p->se.group_node);\n\n#ifdef CONFIG_SCHEDSTATS\n\tmemset(&p->se.statistics, 0, sizeof(p->se.statistics));\n#endif\n\n\tINIT_LIST_HEAD(&p->rt.run_list);\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\tINIT_HLIST_HEAD(&p->preempt_notifiers);\n#endif\n}\n\n/*\n * fork()/clone()-time setup:\n */\nvoid sched_fork(struct task_struct *p)\n{\n\tunsigned long flags;\n\tint cpu = get_cpu();\n\n\t__sched_fork(p);\n\t/*\n\t * We mark the process as running here. This guarantees that\n\t * nobody will actually run it, and a signal or other external\n\t * event cannot wake it up and insert it on the runqueue either.\n\t */\n\tp->state = TASK_RUNNING;\n\n\t/*\n\t * Revert to default priority/policy on fork if requested.\n\t */\n\tif (unlikely(p->sched_reset_on_fork)) {\n\t\tif (p->policy == SCHED_FIFO || p->policy == SCHED_RR) {\n\t\t\tp->policy = SCHED_NORMAL;\n\t\t\tp->normal_prio = p->static_prio;\n\t\t}\n\n\t\tif (PRIO_TO_NICE(p->static_prio) < 0) {\n\t\t\tp->static_prio = NICE_TO_PRIO(0);\n\t\t\tp->normal_prio = p->static_prio;\n\t\t\tset_load_weight(p);\n\t\t}\n\n\t\t/*\n\t\t * We don't need the reset flag anymore after the fork. It has\n\t\t * fulfilled its duty:\n\t\t */\n\t\tp->sched_reset_on_fork = 0;\n\t}\n\n\t/*\n\t * Make sure we do not leak PI boosting priority to the child.\n\t */\n\tp->prio = current->normal_prio;\n\n\tif (!rt_prio(p->prio))\n\t\tp->sched_class = &fair_sched_class;\n\n\tif (p->sched_class->task_fork)\n\t\tp->sched_class->task_fork(p);\n\n\t/*\n\t * The child is not yet in the pid-hash so no cgroup attach races,\n\t * and the cgroup is pinned to this child due to cgroup_fork()\n\t * is ran before sched_fork().\n\t *\n\t * Silence PROVE_RCU.\n\t */\n\traw_spin_lock_irqsave(&p->pi_lock, flags);\n\tset_task_cpu(p, cpu);\n\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\n#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)\n\tif (likely(sched_info_on()))\n\t\tmemset(&p->sched_info, 0, sizeof(p->sched_info));\n#endif\n#if defined(CONFIG_SMP)\n\tp->on_cpu = 0;\n#endif\n#ifdef CONFIG_PREEMPT\n\t/* Want to start with kernel preemption disabled. */\n\ttask_thread_info(p)->preempt_count = 1;\n#endif\n#ifdef CONFIG_SMP\n\tplist_node_init(&p->pushable_tasks, MAX_PRIO);\n#endif\n\n\tput_cpu();\n}\n\n/*\n * wake_up_new_task - wake up a newly created task for the first time.\n *\n * This function will do some initial scheduler statistics housekeeping\n * that must be done for every newly created context, then puts the task\n * on the runqueue and wakes it.\n */\nvoid wake_up_new_task(struct task_struct *p)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\n\traw_spin_lock_irqsave(&p->pi_lock, flags);\n#ifdef CONFIG_SMP\n\t/*\n\t * Fork balancing, do it here and not earlier because:\n\t *  - cpus_allowed can change in the fork path\n\t *  - any previously selected cpu might disappear through hotplug\n\t */\n\tset_task_cpu(p, select_task_rq(p, SD_BALANCE_FORK, 0));\n#endif\n\n\trq = __task_rq_lock(p);\n\tactivate_task(rq, p, 0);\n\tp->on_rq = 1;\n\ttrace_sched_wakeup_new(p, true);\n\tcheck_preempt_curr(rq, p, WF_FORK);\n#ifdef CONFIG_SMP\n\tif (p->sched_class->task_woken)\n\t\tp->sched_class->task_woken(rq, p);\n#endif\n\ttask_rq_unlock(rq, p, &flags);\n}\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\n/**\n * preempt_notifier_register - tell me when current is being preempted & rescheduled\n * @notifier: notifier struct to register\n */\nvoid preempt_notifier_register(struct preempt_notifier *notifier)\n{\n\thlist_add_head(&notifier->link, &current->preempt_notifiers);\n}\nEXPORT_SYMBOL_GPL(preempt_notifier_register);\n\n/**\n * preempt_notifier_unregister - no longer interested in preemption notifications\n * @notifier: notifier struct to unregister\n *\n * This is safe to call from within a preemption notifier.\n */\nvoid preempt_notifier_unregister(struct preempt_notifier *notifier)\n{\n\thlist_del(&notifier->link);\n}\nEXPORT_SYMBOL_GPL(preempt_notifier_unregister);\n\nstatic void fire_sched_in_preempt_notifiers(struct task_struct *curr)\n{\n\tstruct preempt_notifier *notifier;\n\tstruct hlist_node *node;\n\n\thlist_for_each_entry(notifier, node, &curr->preempt_notifiers, link)\n\t\tnotifier->ops->sched_in(notifier, raw_smp_processor_id());\n}\n\nstatic void\nfire_sched_out_preempt_notifiers(struct task_struct *curr,\n\t\t\t\t struct task_struct *next)\n{\n\tstruct preempt_notifier *notifier;\n\tstruct hlist_node *node;\n\n\thlist_for_each_entry(notifier, node, &curr->preempt_notifiers, link)\n\t\tnotifier->ops->sched_out(notifier, next);\n}\n\n#else /* !CONFIG_PREEMPT_NOTIFIERS */\n\nstatic void fire_sched_in_preempt_notifiers(struct task_struct *curr)\n{\n}\n\nstatic void\nfire_sched_out_preempt_notifiers(struct task_struct *curr,\n\t\t\t\t struct task_struct *next)\n{\n}\n\n#endif /* CONFIG_PREEMPT_NOTIFIERS */\n\n/**\n * prepare_task_switch - prepare to switch tasks\n * @rq: the runqueue preparing to switch\n * @prev: the current task that is being switched out\n * @next: the task we are going to switch to.\n *\n * This is called with the rq lock held and interrupts off. It must\n * be paired with a subsequent finish_task_switch after the context\n * switch.\n *\n * prepare_task_switch sets up locking and calls architecture specific\n * hooks.\n */\nstatic inline void\nprepare_task_switch(struct rq *rq, struct task_struct *prev,\n\t\t    struct task_struct *next)\n{\n\tsched_info_switch(prev, next);\n\tperf_event_task_sched_out(prev, next);\n\tfire_sched_out_preempt_notifiers(prev, next);\n\tprepare_lock_switch(rq, next);\n\tprepare_arch_switch(next);\n\ttrace_sched_switch(prev, next);\n}\n\n/**\n * finish_task_switch - clean up after a task-switch\n * @rq: runqueue associated with task-switch\n * @prev: the thread we just switched away from.\n *\n * finish_task_switch must be called after the context switch, paired\n * with a prepare_task_switch call before the context switch.\n * finish_task_switch will reconcile locking set up by prepare_task_switch,\n * and do any other architecture-specific cleanup actions.\n *\n * Note that we may have delayed dropping an mm in context_switch(). If\n * so, we finish that here outside of the runqueue lock. (Doing it\n * with the lock held can cause deadlocks; see schedule() for\n * details.)\n */\nstatic void finish_task_switch(struct rq *rq, struct task_struct *prev)\n\t__releases(rq->lock)\n{\n\tstruct mm_struct *mm = rq->prev_mm;\n\tlong prev_state;\n\n\trq->prev_mm = NULL;\n\n\t/*\n\t * A task struct has one reference for the use as \"current\".\n\t * If a task dies, then it sets TASK_DEAD in tsk->state and calls\n\t * schedule one last time. The schedule call will never return, and\n\t * the scheduled task must drop that reference.\n\t * The test for TASK_DEAD must occur while the runqueue locks are\n\t * still held, otherwise prev could be scheduled on another cpu, die\n\t * there before we look at prev->state, and then the reference would\n\t * be dropped twice.\n\t *\t\tManfred Spraul <manfred@colorfullife.com>\n\t */\n\tprev_state = prev->state;\n\tfinish_arch_switch(prev);\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\tlocal_irq_disable();\n#endif /* __ARCH_WANT_INTERRUPTS_ON_CTXSW */\n\tperf_event_task_sched_in(current);\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\tlocal_irq_enable();\n#endif /* __ARCH_WANT_INTERRUPTS_ON_CTXSW */\n\tfinish_lock_switch(rq, prev);\n\n\tfire_sched_in_preempt_notifiers(current);\n\tif (mm)\n\t\tmmdrop(mm);\n\tif (unlikely(prev_state == TASK_DEAD)) {\n\t\t/*\n\t\t * Remove function-return probe instances associated with this\n\t\t * task and put them back on the free list.\n\t\t */\n\t\tkprobe_flush_task(prev);\n\t\tput_task_struct(prev);\n\t}\n}\n\n#ifdef CONFIG_SMP\n\n/* assumes rq->lock is held */\nstatic inline void pre_schedule(struct rq *rq, struct task_struct *prev)\n{\n\tif (prev->sched_class->pre_schedule)\n\t\tprev->sched_class->pre_schedule(rq, prev);\n}\n\n/* rq->lock is NOT held, but preemption is disabled */\nstatic inline void post_schedule(struct rq *rq)\n{\n\tif (rq->post_schedule) {\n\t\tunsigned long flags;\n\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tif (rq->curr->sched_class->post_schedule)\n\t\t\trq->curr->sched_class->post_schedule(rq);\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\n\t\trq->post_schedule = 0;\n\t}\n}\n\n#else\n\nstatic inline void pre_schedule(struct rq *rq, struct task_struct *p)\n{\n}\n\nstatic inline void post_schedule(struct rq *rq)\n{\n}\n\n#endif\n\n/**\n * schedule_tail - first thing a freshly forked thread must call.\n * @prev: the thread we just switched away from.\n */\nasmlinkage void schedule_tail(struct task_struct *prev)\n\t__releases(rq->lock)\n{\n\tstruct rq *rq = this_rq();\n\n\tfinish_task_switch(rq, prev);\n\n\t/*\n\t * FIXME: do we need to worry about rq being invalidated by the\n\t * task_switch?\n\t */\n\tpost_schedule(rq);\n\n#ifdef __ARCH_WANT_UNLOCKED_CTXSW\n\t/* In this case, finish_task_switch does not reenable preemption */\n\tpreempt_enable();\n#endif\n\tif (current->set_child_tid)\n\t\tput_user(task_pid_vnr(current), current->set_child_tid);\n}\n\n/*\n * context_switch - switch to the new MM and the new\n * thread's register state.\n */\nstatic inline void\ncontext_switch(struct rq *rq, struct task_struct *prev,\n\t       struct task_struct *next)\n{\n\tstruct mm_struct *mm, *oldmm;\n\n\tprepare_task_switch(rq, prev, next);\n\n\tmm = next->mm;\n\toldmm = prev->active_mm;\n\t/*\n\t * For paravirt, this is coupled with an exit in switch_to to\n\t * combine the page table reload and the switch backend into\n\t * one hypercall.\n\t */\n\tarch_start_context_switch(prev);\n\n\tif (!mm) {\n\t\tnext->active_mm = oldmm;\n\t\tatomic_inc(&oldmm->mm_count);\n\t\tenter_lazy_tlb(oldmm, next);\n\t} else\n\t\tswitch_mm(oldmm, mm, next);\n\n\tif (!prev->mm) {\n\t\tprev->active_mm = NULL;\n\t\trq->prev_mm = oldmm;\n\t}\n\t/*\n\t * Since the runqueue lock will be released by the next\n\t * task (which is an invalid locking op but in the case\n\t * of the scheduler it's an obvious special-case), so we\n\t * do an early lockdep release here:\n\t */\n#ifndef __ARCH_WANT_UNLOCKED_CTXSW\n\tspin_release(&rq->lock.dep_map, 1, _THIS_IP_);\n#endif\n\n\t/* Here we just switch the register state and the stack. */\n\tswitch_to(prev, next, prev);\n\n\tbarrier();\n\t/*\n\t * this_rq must be evaluated again because prev may have moved\n\t * CPUs since it called schedule(), thus the 'rq' on its stack\n\t * frame will be invalid.\n\t */\n\tfinish_task_switch(this_rq(), prev);\n}\n\n/*\n * nr_running, nr_uninterruptible and nr_context_switches:\n *\n * externally visible scheduler statistics: current number of runnable\n * threads, current number of uninterruptible-sleeping threads, total\n * number of context switches performed since bootup.\n */\nunsigned long nr_running(void)\n{\n\tunsigned long i, sum = 0;\n\n\tfor_each_online_cpu(i)\n\t\tsum += cpu_rq(i)->nr_running;\n\n\treturn sum;\n}\n\nunsigned long nr_uninterruptible(void)\n{\n\tunsigned long i, sum = 0;\n\n\tfor_each_possible_cpu(i)\n\t\tsum += cpu_rq(i)->nr_uninterruptible;\n\n\t/*\n\t * Since we read the counters lockless, it might be slightly\n\t * inaccurate. Do not allow it to go below zero though:\n\t */\n\tif (unlikely((long)sum < 0))\n\t\tsum = 0;\n\n\treturn sum;\n}\n\nunsigned long long nr_context_switches(void)\n{\n\tint i;\n\tunsigned long long sum = 0;\n\n\tfor_each_possible_cpu(i)\n\t\tsum += cpu_rq(i)->nr_switches;\n\n\treturn sum;\n}\n\nunsigned long nr_iowait(void)\n{\n\tunsigned long i, sum = 0;\n\n\tfor_each_possible_cpu(i)\n\t\tsum += atomic_read(&cpu_rq(i)->nr_iowait);\n\n\treturn sum;\n}\n\nunsigned long nr_iowait_cpu(int cpu)\n{\n\tstruct rq *this = cpu_rq(cpu);\n\treturn atomic_read(&this->nr_iowait);\n}\n\nunsigned long this_cpu_load(void)\n{\n\tstruct rq *this = this_rq();\n\treturn this->cpu_load[0];\n}\n\n\n/* Variables and functions for calc_load */\nstatic atomic_long_t calc_load_tasks;\nstatic unsigned long calc_load_update;\nunsigned long avenrun[3];\nEXPORT_SYMBOL(avenrun);\n\nstatic long calc_load_fold_active(struct rq *this_rq)\n{\n\tlong nr_active, delta = 0;\n\n\tnr_active = this_rq->nr_running;\n\tnr_active += (long) this_rq->nr_uninterruptible;\n\n\tif (nr_active != this_rq->calc_load_active) {\n\t\tdelta = nr_active - this_rq->calc_load_active;\n\t\tthis_rq->calc_load_active = nr_active;\n\t}\n\n\treturn delta;\n}\n\nstatic unsigned long\ncalc_load(unsigned long load, unsigned long exp, unsigned long active)\n{\n\tload *= exp;\n\tload += active * (FIXED_1 - exp);\n\tload += 1UL << (FSHIFT - 1);\n\treturn load >> FSHIFT;\n}\n\n#ifdef CONFIG_NO_HZ\n/*\n * For NO_HZ we delay the active fold to the next LOAD_FREQ update.\n *\n * When making the ILB scale, we should try to pull this in as well.\n */\nstatic atomic_long_t calc_load_tasks_idle;\n\nstatic void calc_load_account_idle(struct rq *this_rq)\n{\n\tlong delta;\n\n\tdelta = calc_load_fold_active(this_rq);\n\tif (delta)\n\t\tatomic_long_add(delta, &calc_load_tasks_idle);\n}\n\nstatic long calc_load_fold_idle(void)\n{\n\tlong delta = 0;\n\n\t/*\n\t * Its got a race, we don't care...\n\t */\n\tif (atomic_long_read(&calc_load_tasks_idle))\n\t\tdelta = atomic_long_xchg(&calc_load_tasks_idle, 0);\n\n\treturn delta;\n}\n\n/**\n * fixed_power_int - compute: x^n, in O(log n) time\n *\n * @x:         base of the power\n * @frac_bits: fractional bits of @x\n * @n:         power to raise @x to.\n *\n * By exploiting the relation between the definition of the natural power\n * function: x^n := x*x*...*x (x multiplied by itself for n times), and\n * the binary encoding of numbers used by computers: n := \\Sum n_i * 2^i,\n * (where: n_i \\elem {0, 1}, the binary vector representing n),\n * we find: x^n := x^(\\Sum n_i * 2^i) := \\Prod x^(n_i * 2^i), which is\n * of course trivially computable in O(log_2 n), the length of our binary\n * vector.\n */\nstatic unsigned long\nfixed_power_int(unsigned long x, unsigned int frac_bits, unsigned int n)\n{\n\tunsigned long result = 1UL << frac_bits;\n\n\tif (n) for (;;) {\n\t\tif (n & 1) {\n\t\t\tresult *= x;\n\t\t\tresult += 1UL << (frac_bits - 1);\n\t\t\tresult >>= frac_bits;\n\t\t}\n\t\tn >>= 1;\n\t\tif (!n)\n\t\t\tbreak;\n\t\tx *= x;\n\t\tx += 1UL << (frac_bits - 1);\n\t\tx >>= frac_bits;\n\t}\n\n\treturn result;\n}\n\n/*\n * a1 = a0 * e + a * (1 - e)\n *\n * a2 = a1 * e + a * (1 - e)\n *    = (a0 * e + a * (1 - e)) * e + a * (1 - e)\n *    = a0 * e^2 + a * (1 - e) * (1 + e)\n *\n * a3 = a2 * e + a * (1 - e)\n *    = (a0 * e^2 + a * (1 - e) * (1 + e)) * e + a * (1 - e)\n *    = a0 * e^3 + a * (1 - e) * (1 + e + e^2)\n *\n *  ...\n *\n * an = a0 * e^n + a * (1 - e) * (1 + e + ... + e^n-1) [1]\n *    = a0 * e^n + a * (1 - e) * (1 - e^n)/(1 - e)\n *    = a0 * e^n + a * (1 - e^n)\n *\n * [1] application of the geometric series:\n *\n *              n         1 - x^(n+1)\n *     S_n := \\Sum x^i = -------------\n *             i=0          1 - x\n */\nstatic unsigned long\ncalc_load_n(unsigned long load, unsigned long exp,\n\t    unsigned long active, unsigned int n)\n{\n\n\treturn calc_load(load, fixed_power_int(exp, FSHIFT, n), active);\n}\n\n/*\n * NO_HZ can leave us missing all per-cpu ticks calling\n * calc_load_account_active(), but since an idle CPU folds its delta into\n * calc_load_tasks_idle per calc_load_account_idle(), all we need to do is fold\n * in the pending idle delta if our idle period crossed a load cycle boundary.\n *\n * Once we've updated the global active value, we need to apply the exponential\n * weights adjusted to the number of cycles missed.\n */\nstatic void calc_global_nohz(unsigned long ticks)\n{\n\tlong delta, active, n;\n\n\tif (time_before(jiffies, calc_load_update))\n\t\treturn;\n\n\t/*\n\t * If we crossed a calc_load_update boundary, make sure to fold\n\t * any pending idle changes, the respective CPUs might have\n\t * missed the tick driven calc_load_account_active() update\n\t * due to NO_HZ.\n\t */\n\tdelta = calc_load_fold_idle();\n\tif (delta)\n\t\tatomic_long_add(delta, &calc_load_tasks);\n\n\t/*\n\t * If we were idle for multiple load cycles, apply them.\n\t */\n\tif (ticks >= LOAD_FREQ) {\n\t\tn = ticks / LOAD_FREQ;\n\n\t\tactive = atomic_long_read(&calc_load_tasks);\n\t\tactive = active > 0 ? active * FIXED_1 : 0;\n\n\t\tavenrun[0] = calc_load_n(avenrun[0], EXP_1, active, n);\n\t\tavenrun[1] = calc_load_n(avenrun[1], EXP_5, active, n);\n\t\tavenrun[2] = calc_load_n(avenrun[2], EXP_15, active, n);\n\n\t\tcalc_load_update += n * LOAD_FREQ;\n\t}\n\n\t/*\n\t * Its possible the remainder of the above division also crosses\n\t * a LOAD_FREQ period, the regular check in calc_global_load()\n\t * which comes after this will take care of that.\n\t *\n\t * Consider us being 11 ticks before a cycle completion, and us\n\t * sleeping for 4*LOAD_FREQ + 22 ticks, then the above code will\n\t * age us 4 cycles, and the test in calc_global_load() will\n\t * pick up the final one.\n\t */\n}\n#else\nstatic void calc_load_account_idle(struct rq *this_rq)\n{\n}\n\nstatic inline long calc_load_fold_idle(void)\n{\n\treturn 0;\n}\n\nstatic void calc_global_nohz(unsigned long ticks)\n{\n}\n#endif\n\n/**\n * get_avenrun - get the load average array\n * @loads:\tpointer to dest load array\n * @offset:\toffset to add\n * @shift:\tshift count to shift the result left\n *\n * These values are estimates at best, so no need for locking.\n */\nvoid get_avenrun(unsigned long *loads, unsigned long offset, int shift)\n{\n\tloads[0] = (avenrun[0] + offset) << shift;\n\tloads[1] = (avenrun[1] + offset) << shift;\n\tloads[2] = (avenrun[2] + offset) << shift;\n}\n\n/*\n * calc_load - update the avenrun load estimates 10 ticks after the\n * CPUs have updated calc_load_tasks.\n */\nvoid calc_global_load(unsigned long ticks)\n{\n\tlong active;\n\n\tcalc_global_nohz(ticks);\n\n\tif (time_before(jiffies, calc_load_update + 10))\n\t\treturn;\n\n\tactive = atomic_long_read(&calc_load_tasks);\n\tactive = active > 0 ? active * FIXED_1 : 0;\n\n\tavenrun[0] = calc_load(avenrun[0], EXP_1, active);\n\tavenrun[1] = calc_load(avenrun[1], EXP_5, active);\n\tavenrun[2] = calc_load(avenrun[2], EXP_15, active);\n\n\tcalc_load_update += LOAD_FREQ;\n}\n\n/*\n * Called from update_cpu_load() to periodically update this CPU's\n * active count.\n */\nstatic void calc_load_account_active(struct rq *this_rq)\n{\n\tlong delta;\n\n\tif (time_before(jiffies, this_rq->calc_load_update))\n\t\treturn;\n\n\tdelta  = calc_load_fold_active(this_rq);\n\tdelta += calc_load_fold_idle();\n\tif (delta)\n\t\tatomic_long_add(delta, &calc_load_tasks);\n\n\tthis_rq->calc_load_update += LOAD_FREQ;\n}\n\n/*\n * The exact cpuload at various idx values, calculated at every tick would be\n * load = (2^idx - 1) / 2^idx * load + 1 / 2^idx * cur_load\n *\n * If a cpu misses updates for n-1 ticks (as it was idle) and update gets called\n * on nth tick when cpu may be busy, then we have:\n * load = ((2^idx - 1) / 2^idx)^(n-1) * load\n * load = (2^idx - 1) / 2^idx) * load + 1 / 2^idx * cur_load\n *\n * decay_load_missed() below does efficient calculation of\n * load = ((2^idx - 1) / 2^idx)^(n-1) * load\n * avoiding 0..n-1 loop doing load = ((2^idx - 1) / 2^idx) * load\n *\n * The calculation is approximated on a 128 point scale.\n * degrade_zero_ticks is the number of ticks after which load at any\n * particular idx is approximated to be zero.\n * degrade_factor is a precomputed table, a row for each load idx.\n * Each column corresponds to degradation factor for a power of two ticks,\n * based on 128 point scale.\n * Example:\n * row 2, col 3 (=12) says that the degradation at load idx 2 after\n * 8 ticks is 12/128 (which is an approximation of exact factor 3^8/4^8).\n *\n * With this power of 2 load factors, we can degrade the load n times\n * by looking at 1 bits in n and doing as many mult/shift instead of\n * n mult/shifts needed by the exact degradation.\n */\n#define DEGRADE_SHIFT\t\t7\nstatic const unsigned char\n\t\tdegrade_zero_ticks[CPU_LOAD_IDX_MAX] = {0, 8, 32, 64, 128};\nstatic const unsigned char\n\t\tdegrade_factor[CPU_LOAD_IDX_MAX][DEGRADE_SHIFT + 1] = {\n\t\t\t\t\t{0, 0, 0, 0, 0, 0, 0, 0},\n\t\t\t\t\t{64, 32, 8, 0, 0, 0, 0, 0},\n\t\t\t\t\t{96, 72, 40, 12, 1, 0, 0},\n\t\t\t\t\t{112, 98, 75, 43, 15, 1, 0},\n\t\t\t\t\t{120, 112, 98, 76, 45, 16, 2} };\n\n/*\n * Update cpu_load for any missed ticks, due to tickless idle. The backlog\n * would be when CPU is idle and so we just decay the old load without\n * adding any new load.\n */\nstatic unsigned long\ndecay_load_missed(unsigned long load, unsigned long missed_updates, int idx)\n{\n\tint j = 0;\n\n\tif (!missed_updates)\n\t\treturn load;\n\n\tif (missed_updates >= degrade_zero_ticks[idx])\n\t\treturn 0;\n\n\tif (idx == 1)\n\t\treturn load >> missed_updates;\n\n\twhile (missed_updates) {\n\t\tif (missed_updates % 2)\n\t\t\tload = (load * degrade_factor[idx][j]) >> DEGRADE_SHIFT;\n\n\t\tmissed_updates >>= 1;\n\t\tj++;\n\t}\n\treturn load;\n}\n\n/*\n * Update rq->cpu_load[] statistics. This function is usually called every\n * scheduler tick (TICK_NSEC). With tickless idle this will not be called\n * every tick. We fix it up based on jiffies.\n */\nstatic void update_cpu_load(struct rq *this_rq)\n{\n\tunsigned long this_load = this_rq->load.weight;\n\tunsigned long curr_jiffies = jiffies;\n\tunsigned long pending_updates;\n\tint i, scale;\n\n\tthis_rq->nr_load_updates++;\n\n\t/* Avoid repeated calls on same jiffy, when moving in and out of idle */\n\tif (curr_jiffies == this_rq->last_load_update_tick)\n\t\treturn;\n\n\tpending_updates = curr_jiffies - this_rq->last_load_update_tick;\n\tthis_rq->last_load_update_tick = curr_jiffies;\n\n\t/* Update our load: */\n\tthis_rq->cpu_load[0] = this_load; /* Fasttrack for idx 0 */\n\tfor (i = 1, scale = 2; i < CPU_LOAD_IDX_MAX; i++, scale += scale) {\n\t\tunsigned long old_load, new_load;\n\n\t\t/* scale is effectively 1 << i now, and >> i divides by scale */\n\n\t\told_load = this_rq->cpu_load[i];\n\t\told_load = decay_load_missed(old_load, pending_updates - 1, i);\n\t\tnew_load = this_load;\n\t\t/*\n\t\t * Round up the averaging division if load is increasing. This\n\t\t * prevents us from getting stuck on 9 if the load is 10, for\n\t\t * example.\n\t\t */\n\t\tif (new_load > old_load)\n\t\t\tnew_load += scale - 1;\n\n\t\tthis_rq->cpu_load[i] = (old_load * (scale - 1) + new_load) >> i;\n\t}\n\n\tsched_avg_update(this_rq);\n}\n\nstatic void update_cpu_load_active(struct rq *this_rq)\n{\n\tupdate_cpu_load(this_rq);\n\n\tcalc_load_account_active(this_rq);\n}\n\n#ifdef CONFIG_SMP\n\n/*\n * sched_exec - execve() is a valuable balancing opportunity, because at\n * this point the task has the smallest effective memory and cache footprint.\n */\nvoid sched_exec(void)\n{\n\tstruct task_struct *p = current;\n\tunsigned long flags;\n\tint dest_cpu;\n\n\traw_spin_lock_irqsave(&p->pi_lock, flags);\n\tdest_cpu = p->sched_class->select_task_rq(p, SD_BALANCE_EXEC, 0);\n\tif (dest_cpu == smp_processor_id())\n\t\tgoto unlock;\n\n\tif (likely(cpu_active(dest_cpu))) {\n\t\tstruct migration_arg arg = { p, dest_cpu };\n\n\t\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\t\tstop_one_cpu(task_cpu(p), migration_cpu_stop, &arg);\n\t\treturn;\n\t}\nunlock:\n\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n}\n\n#endif\n\nDEFINE_PER_CPU(struct kernel_stat, kstat);\n\nEXPORT_PER_CPU_SYMBOL(kstat);\n\n/*\n * Return any ns on the sched_clock that have not yet been accounted in\n * @p in case that task is currently running.\n *\n * Called with task_rq_lock() held on @rq.\n */\nstatic u64 do_task_delta_exec(struct task_struct *p, struct rq *rq)\n{\n\tu64 ns = 0;\n\n\tif (task_current(rq, p)) {\n\t\tupdate_rq_clock(rq);\n\t\tns = rq->clock_task - p->se.exec_start;\n\t\tif ((s64)ns < 0)\n\t\t\tns = 0;\n\t}\n\n\treturn ns;\n}\n\nunsigned long long task_delta_exec(struct task_struct *p)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tu64 ns = 0;\n\n\trq = task_rq_lock(p, &flags);\n\tns = do_task_delta_exec(p, rq);\n\ttask_rq_unlock(rq, p, &flags);\n\n\treturn ns;\n}\n\n/*\n * Return accounted runtime for the task.\n * In case the task is currently running, return the runtime plus current's\n * pending runtime that have not been accounted yet.\n */\nunsigned long long task_sched_runtime(struct task_struct *p)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tu64 ns = 0;\n\n\trq = task_rq_lock(p, &flags);\n\tns = p->se.sum_exec_runtime + do_task_delta_exec(p, rq);\n\ttask_rq_unlock(rq, p, &flags);\n\n\treturn ns;\n}\n\n/*\n * Return sum_exec_runtime for the thread group.\n * In case the task is currently running, return the sum plus current's\n * pending runtime that have not been accounted yet.\n *\n * Note that the thread group might have other running tasks as well,\n * so the return value not includes other pending runtime that other\n * running tasks might have.\n */\nunsigned long long thread_group_sched_runtime(struct task_struct *p)\n{\n\tstruct task_cputime totals;\n\tunsigned long flags;\n\tstruct rq *rq;\n\tu64 ns;\n\n\trq = task_rq_lock(p, &flags);\n\tthread_group_cputime(p, &totals);\n\tns = totals.sum_exec_runtime + do_task_delta_exec(p, rq);\n\ttask_rq_unlock(rq, p, &flags);\n\n\treturn ns;\n}\n\n/*\n * Account user cpu time to a process.\n * @p: the process that the cpu time gets accounted to\n * @cputime: the cpu time spent in user space since the last update\n * @cputime_scaled: cputime scaled by cpu frequency\n */\nvoid account_user_time(struct task_struct *p, cputime_t cputime,\n\t\t       cputime_t cputime_scaled)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tcputime64_t tmp;\n\n\t/* Add user time to process. */\n\tp->utime = cputime_add(p->utime, cputime);\n\tp->utimescaled = cputime_add(p->utimescaled, cputime_scaled);\n\taccount_group_user_time(p, cputime);\n\n\t/* Add user time to cpustat. */\n\ttmp = cputime_to_cputime64(cputime);\n\tif (TASK_NICE(p) > 0)\n\t\tcpustat->nice = cputime64_add(cpustat->nice, tmp);\n\telse\n\t\tcpustat->user = cputime64_add(cpustat->user, tmp);\n\n\tcpuacct_update_stats(p, CPUACCT_STAT_USER, cputime);\n\t/* Account for user time used */\n\tacct_update_integrals(p);\n}\n\n/*\n * Account guest cpu time to a process.\n * @p: the process that the cpu time gets accounted to\n * @cputime: the cpu time spent in virtual machine since the last update\n * @cputime_scaled: cputime scaled by cpu frequency\n */\nstatic void account_guest_time(struct task_struct *p, cputime_t cputime,\n\t\t\t       cputime_t cputime_scaled)\n{\n\tcputime64_t tmp;\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\n\ttmp = cputime_to_cputime64(cputime);\n\n\t/* Add guest time to process. */\n\tp->utime = cputime_add(p->utime, cputime);\n\tp->utimescaled = cputime_add(p->utimescaled, cputime_scaled);\n\taccount_group_user_time(p, cputime);\n\tp->gtime = cputime_add(p->gtime, cputime);\n\n\t/* Add guest time to cpustat. */\n\tif (TASK_NICE(p) > 0) {\n\t\tcpustat->nice = cputime64_add(cpustat->nice, tmp);\n\t\tcpustat->guest_nice = cputime64_add(cpustat->guest_nice, tmp);\n\t} else {\n\t\tcpustat->user = cputime64_add(cpustat->user, tmp);\n\t\tcpustat->guest = cputime64_add(cpustat->guest, tmp);\n\t}\n}\n\n/*\n * Account system cpu time to a process and desired cpustat field\n * @p: the process that the cpu time gets accounted to\n * @cputime: the cpu time spent in kernel space since the last update\n * @cputime_scaled: cputime scaled by cpu frequency\n * @target_cputime64: pointer to cpustat field that has to be updated\n */\nstatic inline\nvoid __account_system_time(struct task_struct *p, cputime_t cputime,\n\t\t\tcputime_t cputime_scaled, cputime64_t *target_cputime64)\n{\n\tcputime64_t tmp = cputime_to_cputime64(cputime);\n\n\t/* Add system time to process. */\n\tp->stime = cputime_add(p->stime, cputime);\n\tp->stimescaled = cputime_add(p->stimescaled, cputime_scaled);\n\taccount_group_system_time(p, cputime);\n\n\t/* Add system time to cpustat. */\n\t*target_cputime64 = cputime64_add(*target_cputime64, tmp);\n\tcpuacct_update_stats(p, CPUACCT_STAT_SYSTEM, cputime);\n\n\t/* Account for system time used */\n\tacct_update_integrals(p);\n}\n\n/*\n * Account system cpu time to a process.\n * @p: the process that the cpu time gets accounted to\n * @hardirq_offset: the offset to subtract from hardirq_count()\n * @cputime: the cpu time spent in kernel space since the last update\n * @cputime_scaled: cputime scaled by cpu frequency\n */\nvoid account_system_time(struct task_struct *p, int hardirq_offset,\n\t\t\t cputime_t cputime, cputime_t cputime_scaled)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tcputime64_t *target_cputime64;\n\n\tif ((p->flags & PF_VCPU) && (irq_count() - hardirq_offset == 0)) {\n\t\taccount_guest_time(p, cputime, cputime_scaled);\n\t\treturn;\n\t}\n\n\tif (hardirq_count() - hardirq_offset)\n\t\ttarget_cputime64 = &cpustat->irq;\n\telse if (in_serving_softirq())\n\t\ttarget_cputime64 = &cpustat->softirq;\n\telse\n\t\ttarget_cputime64 = &cpustat->system;\n\n\t__account_system_time(p, cputime, cputime_scaled, target_cputime64);\n}\n\n/*\n * Account for involuntary wait time.\n * @cputime: the cpu time spent in involuntary wait\n */\nvoid account_steal_time(cputime_t cputime)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tcputime64_t cputime64 = cputime_to_cputime64(cputime);\n\n\tcpustat->steal = cputime64_add(cpustat->steal, cputime64);\n}\n\n/*\n * Account for idle time.\n * @cputime: the cpu time spent in idle wait\n */\nvoid account_idle_time(cputime_t cputime)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tcputime64_t cputime64 = cputime_to_cputime64(cputime);\n\tstruct rq *rq = this_rq();\n\n\tif (atomic_read(&rq->nr_iowait) > 0)\n\t\tcpustat->iowait = cputime64_add(cpustat->iowait, cputime64);\n\telse\n\t\tcpustat->idle = cputime64_add(cpustat->idle, cputime64);\n}\n\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n/*\n * Account a tick to a process and cpustat\n * @p: the process that the cpu time gets accounted to\n * @user_tick: is the tick from userspace\n * @rq: the pointer to rq\n *\n * Tick demultiplexing follows the order\n * - pending hardirq update\n * - pending softirq update\n * - user_time\n * - idle_time\n * - system time\n *   - check for guest_time\n *   - else account as system_time\n *\n * Check for hardirq is done both for system and user time as there is\n * no timer going off while we are on hardirq and hence we may never get an\n * opportunity to update it solely in system time.\n * p->stime and friends are only updated on system time and not on irq\n * softirq as those do not count in task exec_runtime any more.\n */\nstatic void irqtime_account_process_tick(struct task_struct *p, int user_tick,\n\t\t\t\t\t\tstruct rq *rq)\n{\n\tcputime_t one_jiffy_scaled = cputime_to_scaled(cputime_one_jiffy);\n\tcputime64_t tmp = cputime_to_cputime64(cputime_one_jiffy);\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\n\tif (irqtime_account_hi_update()) {\n\t\tcpustat->irq = cputime64_add(cpustat->irq, tmp);\n\t} else if (irqtime_account_si_update()) {\n\t\tcpustat->softirq = cputime64_add(cpustat->softirq, tmp);\n\t} else if (this_cpu_ksoftirqd() == p) {\n\t\t/*\n\t\t * ksoftirqd time do not get accounted in cpu_softirq_time.\n\t\t * So, we have to handle it separately here.\n\t\t * Also, p->stime needs to be updated for ksoftirqd.\n\t\t */\n\t\t__account_system_time(p, cputime_one_jiffy, one_jiffy_scaled,\n\t\t\t\t\t&cpustat->softirq);\n\t} else if (user_tick) {\n\t\taccount_user_time(p, cputime_one_jiffy, one_jiffy_scaled);\n\t} else if (p == rq->idle) {\n\t\taccount_idle_time(cputime_one_jiffy);\n\t} else if (p->flags & PF_VCPU) { /* System time or guest time */\n\t\taccount_guest_time(p, cputime_one_jiffy, one_jiffy_scaled);\n\t} else {\n\t\t__account_system_time(p, cputime_one_jiffy, one_jiffy_scaled,\n\t\t\t\t\t&cpustat->system);\n\t}\n}\n\nstatic void irqtime_account_idle_ticks(int ticks)\n{\n\tint i;\n\tstruct rq *rq = this_rq();\n\n\tfor (i = 0; i < ticks; i++)\n\t\tirqtime_account_process_tick(current, 0, rq);\n}\n#else /* CONFIG_IRQ_TIME_ACCOUNTING */\nstatic void irqtime_account_idle_ticks(int ticks) {}\nstatic void irqtime_account_process_tick(struct task_struct *p, int user_tick,\n\t\t\t\t\t\tstruct rq *rq) {}\n#endif /* CONFIG_IRQ_TIME_ACCOUNTING */\n\n/*\n * Account a single tick of cpu time.\n * @p: the process that the cpu time gets accounted to\n * @user_tick: indicates if the tick is a user or a system tick\n */\nvoid account_process_tick(struct task_struct *p, int user_tick)\n{\n\tcputime_t one_jiffy_scaled = cputime_to_scaled(cputime_one_jiffy);\n\tstruct rq *rq = this_rq();\n\n\tif (sched_clock_irqtime) {\n\t\tirqtime_account_process_tick(p, user_tick, rq);\n\t\treturn;\n\t}\n\n\tif (user_tick)\n\t\taccount_user_time(p, cputime_one_jiffy, one_jiffy_scaled);\n\telse if ((p != rq->idle) || (irq_count() != HARDIRQ_OFFSET))\n\t\taccount_system_time(p, HARDIRQ_OFFSET, cputime_one_jiffy,\n\t\t\t\t    one_jiffy_scaled);\n\telse\n\t\taccount_idle_time(cputime_one_jiffy);\n}\n\n/*\n * Account multiple ticks of steal time.\n * @p: the process from which the cpu time has been stolen\n * @ticks: number of stolen ticks\n */\nvoid account_steal_ticks(unsigned long ticks)\n{\n\taccount_steal_time(jiffies_to_cputime(ticks));\n}\n\n/*\n * Account multiple ticks of idle time.\n * @ticks: number of stolen ticks\n */\nvoid account_idle_ticks(unsigned long ticks)\n{\n\n\tif (sched_clock_irqtime) {\n\t\tirqtime_account_idle_ticks(ticks);\n\t\treturn;\n\t}\n\n\taccount_idle_time(jiffies_to_cputime(ticks));\n}\n\n#endif\n\n/*\n * Use precise platform statistics if available:\n */\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING\nvoid task_times(struct task_struct *p, cputime_t *ut, cputime_t *st)\n{\n\t*ut = p->utime;\n\t*st = p->stime;\n}\n\nvoid thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *st)\n{\n\tstruct task_cputime cputime;\n\n\tthread_group_cputime(p, &cputime);\n\n\t*ut = cputime.utime;\n\t*st = cputime.stime;\n}\n#else\n\n#ifndef nsecs_to_cputime\n# define nsecs_to_cputime(__nsecs)\tnsecs_to_jiffies(__nsecs)\n#endif\n\nvoid task_times(struct task_struct *p, cputime_t *ut, cputime_t *st)\n{\n\tcputime_t rtime, utime = p->utime, total = cputime_add(utime, p->stime);\n\n\t/*\n\t * Use CFS's precise accounting:\n\t */\n\trtime = nsecs_to_cputime(p->se.sum_exec_runtime);\n\n\tif (total) {\n\t\tu64 temp = rtime;\n\n\t\ttemp *= utime;\n\t\tdo_div(temp, total);\n\t\tutime = (cputime_t)temp;\n\t} else\n\t\tutime = rtime;\n\n\t/*\n\t * Compare with previous values, to keep monotonicity:\n\t */\n\tp->prev_utime = max(p->prev_utime, utime);\n\tp->prev_stime = max(p->prev_stime, cputime_sub(rtime, p->prev_utime));\n\n\t*ut = p->prev_utime;\n\t*st = p->prev_stime;\n}\n\n/*\n * Must be called with siglock held.\n */\nvoid thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *st)\n{\n\tstruct signal_struct *sig = p->signal;\n\tstruct task_cputime cputime;\n\tcputime_t rtime, utime, total;\n\n\tthread_group_cputime(p, &cputime);\n\n\ttotal = cputime_add(cputime.utime, cputime.stime);\n\trtime = nsecs_to_cputime(cputime.sum_exec_runtime);\n\n\tif (total) {\n\t\tu64 temp = rtime;\n\n\t\ttemp *= cputime.utime;\n\t\tdo_div(temp, total);\n\t\tutime = (cputime_t)temp;\n\t} else\n\t\tutime = rtime;\n\n\tsig->prev_utime = max(sig->prev_utime, utime);\n\tsig->prev_stime = max(sig->prev_stime,\n\t\t\t      cputime_sub(rtime, sig->prev_utime));\n\n\t*ut = sig->prev_utime;\n\t*st = sig->prev_stime;\n}\n#endif\n\n/*\n * This function gets called by the timer code, with HZ frequency.\n * We call it with interrupts disabled.\n */\nvoid scheduler_tick(void)\n{\n\tint cpu = smp_processor_id();\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct task_struct *curr = rq->curr;\n\n\tsched_clock_tick();\n\n\traw_spin_lock(&rq->lock);\n\tupdate_rq_clock(rq);\n\tupdate_cpu_load_active(rq);\n\tcurr->sched_class->task_tick(rq, curr, 0);\n\traw_spin_unlock(&rq->lock);\n\n\tperf_event_task_tick();\n\n#ifdef CONFIG_SMP\n\trq->idle_at_tick = idle_cpu(cpu);\n\ttrigger_load_balance(rq, cpu);\n#endif\n}\n\nnotrace unsigned long get_parent_ip(unsigned long addr)\n{\n\tif (in_lock_functions(addr)) {\n\t\taddr = CALLER_ADDR2;\n\t\tif (in_lock_functions(addr))\n\t\t\taddr = CALLER_ADDR3;\n\t}\n\treturn addr;\n}\n\n#if defined(CONFIG_PREEMPT) && (defined(CONFIG_DEBUG_PREEMPT) || \\\n\t\t\t\tdefined(CONFIG_PREEMPT_TRACER))\n\nvoid __kprobes add_preempt_count(int val)\n{\n#ifdef CONFIG_DEBUG_PREEMPT\n\t/*\n\t * Underflow?\n\t */\n\tif (DEBUG_LOCKS_WARN_ON((preempt_count() < 0)))\n\t\treturn;\n#endif\n\tpreempt_count() += val;\n#ifdef CONFIG_DEBUG_PREEMPT\n\t/*\n\t * Spinlock count overflowing soon?\n\t */\n\tDEBUG_LOCKS_WARN_ON((preempt_count() & PREEMPT_MASK) >=\n\t\t\t\tPREEMPT_MASK - 10);\n#endif\n\tif (preempt_count() == val)\n\t\ttrace_preempt_off(CALLER_ADDR0, get_parent_ip(CALLER_ADDR1));\n}\nEXPORT_SYMBOL(add_preempt_count);\n\nvoid __kprobes sub_preempt_count(int val)\n{\n#ifdef CONFIG_DEBUG_PREEMPT\n\t/*\n\t * Underflow?\n\t */\n\tif (DEBUG_LOCKS_WARN_ON(val > preempt_count()))\n\t\treturn;\n\t/*\n\t * Is the spinlock portion underflowing?\n\t */\n\tif (DEBUG_LOCKS_WARN_ON((val < PREEMPT_MASK) &&\n\t\t\t!(preempt_count() & PREEMPT_MASK)))\n\t\treturn;\n#endif\n\n\tif (preempt_count() == val)\n\t\ttrace_preempt_on(CALLER_ADDR0, get_parent_ip(CALLER_ADDR1));\n\tpreempt_count() -= val;\n}\nEXPORT_SYMBOL(sub_preempt_count);\n\n#endif\n\n/*\n * Print scheduling while atomic bug:\n */\nstatic noinline void __schedule_bug(struct task_struct *prev)\n{\n\tstruct pt_regs *regs = get_irq_regs();\n\n\tprintk(KERN_ERR \"BUG: scheduling while atomic: %s/%d/0x%08x\\n\",\n\t\tprev->comm, prev->pid, preempt_count());\n\n\tdebug_show_held_locks(prev);\n\tprint_modules();\n\tif (irqs_disabled())\n\t\tprint_irqtrace_events(prev);\n\n\tif (regs)\n\t\tshow_regs(regs);\n\telse\n\t\tdump_stack();\n}\n\n/*\n * Various schedule()-time debugging checks and statistics:\n */\nstatic inline void schedule_debug(struct task_struct *prev)\n{\n\t/*\n\t * Test if we are atomic. Since do_exit() needs to call into\n\t * schedule() atomically, we ignore that path for now.\n\t * Otherwise, whine if we are scheduling when we should not be.\n\t */\n\tif (unlikely(in_atomic_preempt_off() && !prev->exit_state))\n\t\t__schedule_bug(prev);\n\n\tprofile_hit(SCHED_PROFILING, __builtin_return_address(0));\n\n\tschedstat_inc(this_rq(), sched_count);\n}\n\nstatic void put_prev_task(struct rq *rq, struct task_struct *prev)\n{\n\tif (prev->on_rq || rq->skip_clock_update < 0)\n\t\tupdate_rq_clock(rq);\n\tprev->sched_class->put_prev_task(rq, prev);\n}\n\n/*\n * Pick up the highest-prio task:\n */\nstatic inline struct task_struct *\npick_next_task(struct rq *rq)\n{\n\tconst struct sched_class *class;\n\tstruct task_struct *p;\n\n\t/*\n\t * Optimization: we know that if all tasks are in\n\t * the fair class we can call that function directly:\n\t */\n\tif (likely(rq->nr_running == rq->cfs.nr_running)) {\n\t\tp = fair_sched_class.pick_next_task(rq);\n\t\tif (likely(p))\n\t\t\treturn p;\n\t}\n\n\tfor_each_class(class) {\n\t\tp = class->pick_next_task(rq);\n\t\tif (p)\n\t\t\treturn p;\n\t}\n\n\tBUG(); /* the idle class will always have a runnable task */\n}\n\n/*\n * schedule() is the main scheduler function.\n */\nasmlinkage void __sched schedule(void)\n{\n\tstruct task_struct *prev, *next;\n\tunsigned long *switch_count;\n\tstruct rq *rq;\n\tint cpu;\n\nneed_resched:\n\tpreempt_disable();\n\tcpu = smp_processor_id();\n\trq = cpu_rq(cpu);\n\trcu_note_context_switch(cpu);\n\tprev = rq->curr;\n\n\tschedule_debug(prev);\n\n\tif (sched_feat(HRTICK))\n\t\thrtick_clear(rq);\n\n\traw_spin_lock_irq(&rq->lock);\n\n\tswitch_count = &prev->nivcsw;\n\tif (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) {\n\t\tif (unlikely(signal_pending_state(prev->state, prev))) {\n\t\t\tprev->state = TASK_RUNNING;\n\t\t} else {\n\t\t\tdeactivate_task(rq, prev, DEQUEUE_SLEEP);\n\t\t\tprev->on_rq = 0;\n\n\t\t\t/*\n\t\t\t * If a worker went to sleep, notify and ask workqueue\n\t\t\t * whether it wants to wake up a task to maintain\n\t\t\t * concurrency.\n\t\t\t */\n\t\t\tif (prev->flags & PF_WQ_WORKER) {\n\t\t\t\tstruct task_struct *to_wakeup;\n\n\t\t\t\tto_wakeup = wq_worker_sleeping(prev, cpu);\n\t\t\t\tif (to_wakeup)\n\t\t\t\t\ttry_to_wake_up_local(to_wakeup);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * If we are going to sleep and we have plugged IO\n\t\t\t * queued, make sure to submit it to avoid deadlocks.\n\t\t\t */\n\t\t\tif (blk_needs_flush_plug(prev)) {\n\t\t\t\traw_spin_unlock(&rq->lock);\n\t\t\t\tblk_schedule_flush_plug(prev);\n\t\t\t\traw_spin_lock(&rq->lock);\n\t\t\t}\n\t\t}\n\t\tswitch_count = &prev->nvcsw;\n\t}\n\n\tpre_schedule(rq, prev);\n\n\tif (unlikely(!rq->nr_running))\n\t\tidle_balance(cpu, rq);\n\n\tput_prev_task(rq, prev);\n\tnext = pick_next_task(rq);\n\tclear_tsk_need_resched(prev);\n\trq->skip_clock_update = 0;\n\n\tif (likely(prev != next)) {\n\t\trq->nr_switches++;\n\t\trq->curr = next;\n\t\t++*switch_count;\n\n\t\tcontext_switch(rq, prev, next); /* unlocks the rq */\n\t\t/*\n\t\t * The context switch have flipped the stack from under us\n\t\t * and restored the local variables which were saved when\n\t\t * this task called schedule() in the past. prev == current\n\t\t * is still correct, but it can be moved to another cpu/rq.\n\t\t */\n\t\tcpu = smp_processor_id();\n\t\trq = cpu_rq(cpu);\n\t} else\n\t\traw_spin_unlock_irq(&rq->lock);\n\n\tpost_schedule(rq);\n\n\tpreempt_enable_no_resched();\n\tif (need_resched())\n\t\tgoto need_resched;\n}\nEXPORT_SYMBOL(schedule);\n\n#ifdef CONFIG_MUTEX_SPIN_ON_OWNER\n\nstatic inline bool owner_running(struct mutex *lock, struct task_struct *owner)\n{\n\tbool ret = false;\n\n\trcu_read_lock();\n\tif (lock->owner != owner)\n\t\tgoto fail;\n\n\t/*\n\t * Ensure we emit the owner->on_cpu, dereference _after_ checking\n\t * lock->owner still matches owner, if that fails, owner might\n\t * point to free()d memory, if it still matches, the rcu_read_lock()\n\t * ensures the memory stays valid.\n\t */\n\tbarrier();\n\n\tret = owner->on_cpu;\nfail:\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n/*\n * Look out! \"owner\" is an entirely speculative pointer\n * access and not reliable.\n */\nint mutex_spin_on_owner(struct mutex *lock, struct task_struct *owner)\n{\n\tif (!sched_feat(OWNER_SPIN))\n\t\treturn 0;\n\n\twhile (owner_running(lock, owner)) {\n\t\tif (need_resched())\n\t\t\treturn 0;\n\n\t\tarch_mutex_cpu_relax();\n\t}\n\n\t/*\n\t * If the owner changed to another task there is likely\n\t * heavy contention, stop spinning.\n\t */\n\tif (lock->owner)\n\t\treturn 0;\n\n\treturn 1;\n}\n#endif\n\n#ifdef CONFIG_PREEMPT\n/*\n * this is the entry point to schedule() from in-kernel preemption\n * off of preempt_enable. Kernel preemptions off return from interrupt\n * occur there and call schedule directly.\n */\nasmlinkage void __sched notrace preempt_schedule(void)\n{\n\tstruct thread_info *ti = current_thread_info();\n\n\t/*\n\t * If there is a non-zero preempt_count or interrupts are disabled,\n\t * we do not want to preempt the current task. Just return..\n\t */\n\tif (likely(ti->preempt_count || irqs_disabled()))\n\t\treturn;\n\n\tdo {\n\t\tadd_preempt_count_notrace(PREEMPT_ACTIVE);\n\t\tschedule();\n\t\tsub_preempt_count_notrace(PREEMPT_ACTIVE);\n\n\t\t/*\n\t\t * Check again in case we missed a preemption opportunity\n\t\t * between schedule and now.\n\t\t */\n\t\tbarrier();\n\t} while (need_resched());\n}\nEXPORT_SYMBOL(preempt_schedule);\n\n/*\n * this is the entry point to schedule() from kernel preemption\n * off of irq context.\n * Note, that this is called and return with irqs disabled. This will\n * protect us against recursive calling from irq.\n */\nasmlinkage void __sched preempt_schedule_irq(void)\n{\n\tstruct thread_info *ti = current_thread_info();\n\n\t/* Catch callers which need to be fixed */\n\tBUG_ON(ti->preempt_count || !irqs_disabled());\n\n\tdo {\n\t\tadd_preempt_count(PREEMPT_ACTIVE);\n\t\tlocal_irq_enable();\n\t\tschedule();\n\t\tlocal_irq_disable();\n\t\tsub_preempt_count(PREEMPT_ACTIVE);\n\n\t\t/*\n\t\t * Check again in case we missed a preemption opportunity\n\t\t * between schedule and now.\n\t\t */\n\t\tbarrier();\n\t} while (need_resched());\n}\n\n#endif /* CONFIG_PREEMPT */\n\nint default_wake_function(wait_queue_t *curr, unsigned mode, int wake_flags,\n\t\t\t  void *key)\n{\n\treturn try_to_wake_up(curr->private, mode, wake_flags);\n}\nEXPORT_SYMBOL(default_wake_function);\n\n/*\n * The core wakeup function. Non-exclusive wakeups (nr_exclusive == 0) just\n * wake everything up. If it's an exclusive wakeup (nr_exclusive == small +ve\n * number) then we wake all the non-exclusive tasks and one exclusive task.\n *\n * There are circumstances in which we can try to wake a task which has already\n * started to run but is not in state TASK_RUNNING. try_to_wake_up() returns\n * zero in this (rare) case, and we handle it by continuing to scan the queue.\n */\nstatic void __wake_up_common(wait_queue_head_t *q, unsigned int mode,\n\t\t\tint nr_exclusive, int wake_flags, void *key)\n{\n\twait_queue_t *curr, *next;\n\n\tlist_for_each_entry_safe(curr, next, &q->task_list, task_list) {\n\t\tunsigned flags = curr->flags;\n\n\t\tif (curr->func(curr, mode, wake_flags, key) &&\n\t\t\t\t(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)\n\t\t\tbreak;\n\t}\n}\n\n/**\n * __wake_up - wake up threads blocked on a waitqueue.\n * @q: the waitqueue\n * @mode: which threads\n * @nr_exclusive: how many wake-one or wake-many threads to wake up\n * @key: is directly passed to the wakeup function\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nvoid __wake_up(wait_queue_head_t *q, unsigned int mode,\n\t\t\tint nr_exclusive, void *key)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&q->lock, flags);\n\t__wake_up_common(q, mode, nr_exclusive, 0, key);\n\tspin_unlock_irqrestore(&q->lock, flags);\n}\nEXPORT_SYMBOL(__wake_up);\n\n/*\n * Same as __wake_up but called with the spinlock in wait_queue_head_t held.\n */\nvoid __wake_up_locked(wait_queue_head_t *q, unsigned int mode)\n{\n\t__wake_up_common(q, mode, 1, 0, NULL);\n}\nEXPORT_SYMBOL_GPL(__wake_up_locked);\n\nvoid __wake_up_locked_key(wait_queue_head_t *q, unsigned int mode, void *key)\n{\n\t__wake_up_common(q, mode, 1, 0, key);\n}\nEXPORT_SYMBOL_GPL(__wake_up_locked_key);\n\n/**\n * __wake_up_sync_key - wake up threads blocked on a waitqueue.\n * @q: the waitqueue\n * @mode: which threads\n * @nr_exclusive: how many wake-one or wake-many threads to wake up\n * @key: opaque value to be passed to wakeup targets\n *\n * The sync wakeup differs that the waker knows that it will schedule\n * away soon, so while the target thread will be woken up, it will not\n * be migrated to another CPU - ie. the two threads are 'synchronized'\n * with each other. This can prevent needless bouncing between CPUs.\n *\n * On UP it can prevent extra preemption.\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nvoid __wake_up_sync_key(wait_queue_head_t *q, unsigned int mode,\n\t\t\tint nr_exclusive, void *key)\n{\n\tunsigned long flags;\n\tint wake_flags = WF_SYNC;\n\n\tif (unlikely(!q))\n\t\treturn;\n\n\tif (unlikely(!nr_exclusive))\n\t\twake_flags = 0;\n\n\tspin_lock_irqsave(&q->lock, flags);\n\t__wake_up_common(q, mode, nr_exclusive, wake_flags, key);\n\tspin_unlock_irqrestore(&q->lock, flags);\n}\nEXPORT_SYMBOL_GPL(__wake_up_sync_key);\n\n/*\n * __wake_up_sync - see __wake_up_sync_key()\n */\nvoid __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)\n{\n\t__wake_up_sync_key(q, mode, nr_exclusive, NULL);\n}\nEXPORT_SYMBOL_GPL(__wake_up_sync);\t/* For internal use only */\n\n/**\n * complete: - signals a single thread waiting on this completion\n * @x:  holds the state of this particular completion\n *\n * This will wake up a single thread waiting on this completion. Threads will be\n * awakened in the same order in which they were queued.\n *\n * See also complete_all(), wait_for_completion() and related routines.\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nvoid complete(struct completion *x)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&x->wait.lock, flags);\n\tx->done++;\n\t__wake_up_common(&x->wait, TASK_NORMAL, 1, 0, NULL);\n\tspin_unlock_irqrestore(&x->wait.lock, flags);\n}\nEXPORT_SYMBOL(complete);\n\n/**\n * complete_all: - signals all threads waiting on this completion\n * @x:  holds the state of this particular completion\n *\n * This will wake up all threads waiting on this particular completion event.\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nvoid complete_all(struct completion *x)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&x->wait.lock, flags);\n\tx->done += UINT_MAX/2;\n\t__wake_up_common(&x->wait, TASK_NORMAL, 0, 0, NULL);\n\tspin_unlock_irqrestore(&x->wait.lock, flags);\n}\nEXPORT_SYMBOL(complete_all);\n\nstatic inline long __sched\ndo_wait_for_common(struct completion *x, long timeout, int state)\n{\n\tif (!x->done) {\n\t\tDECLARE_WAITQUEUE(wait, current);\n\n\t\t__add_wait_queue_tail_exclusive(&x->wait, &wait);\n\t\tdo {\n\t\t\tif (signal_pending_state(state, current)) {\n\t\t\t\ttimeout = -ERESTARTSYS;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t__set_current_state(state);\n\t\t\tspin_unlock_irq(&x->wait.lock);\n\t\t\ttimeout = schedule_timeout(timeout);\n\t\t\tspin_lock_irq(&x->wait.lock);\n\t\t} while (!x->done && timeout);\n\t\t__remove_wait_queue(&x->wait, &wait);\n\t\tif (!x->done)\n\t\t\treturn timeout;\n\t}\n\tx->done--;\n\treturn timeout ?: 1;\n}\n\nstatic long __sched\nwait_for_common(struct completion *x, long timeout, int state)\n{\n\tmight_sleep();\n\n\tspin_lock_irq(&x->wait.lock);\n\ttimeout = do_wait_for_common(x, timeout, state);\n\tspin_unlock_irq(&x->wait.lock);\n\treturn timeout;\n}\n\n/**\n * wait_for_completion: - waits for completion of a task\n * @x:  holds the state of this particular completion\n *\n * This waits to be signaled for completion of a specific task. It is NOT\n * interruptible and there is no timeout.\n *\n * See also similar routines (i.e. wait_for_completion_timeout()) with timeout\n * and interrupt capability. Also see complete().\n */\nvoid __sched wait_for_completion(struct completion *x)\n{\n\twait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE);\n}\nEXPORT_SYMBOL(wait_for_completion);\n\n/**\n * wait_for_completion_timeout: - waits for completion of a task (w/timeout)\n * @x:  holds the state of this particular completion\n * @timeout:  timeout value in jiffies\n *\n * This waits for either a completion of a specific task to be signaled or for a\n * specified timeout to expire. The timeout is in jiffies. It is not\n * interruptible.\n */\nunsigned long __sched\nwait_for_completion_timeout(struct completion *x, unsigned long timeout)\n{\n\treturn wait_for_common(x, timeout, TASK_UNINTERRUPTIBLE);\n}\nEXPORT_SYMBOL(wait_for_completion_timeout);\n\n/**\n * wait_for_completion_interruptible: - waits for completion of a task (w/intr)\n * @x:  holds the state of this particular completion\n *\n * This waits for completion of a specific task to be signaled. It is\n * interruptible.\n */\nint __sched wait_for_completion_interruptible(struct completion *x)\n{\n\tlong t = wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_INTERRUPTIBLE);\n\tif (t == -ERESTARTSYS)\n\t\treturn t;\n\treturn 0;\n}\nEXPORT_SYMBOL(wait_for_completion_interruptible);\n\n/**\n * wait_for_completion_interruptible_timeout: - waits for completion (w/(to,intr))\n * @x:  holds the state of this particular completion\n * @timeout:  timeout value in jiffies\n *\n * This waits for either a completion of a specific task to be signaled or for a\n * specified timeout to expire. It is interruptible. The timeout is in jiffies.\n */\nlong __sched\nwait_for_completion_interruptible_timeout(struct completion *x,\n\t\t\t\t\t  unsigned long timeout)\n{\n\treturn wait_for_common(x, timeout, TASK_INTERRUPTIBLE);\n}\nEXPORT_SYMBOL(wait_for_completion_interruptible_timeout);\n\n/**\n * wait_for_completion_killable: - waits for completion of a task (killable)\n * @x:  holds the state of this particular completion\n *\n * This waits to be signaled for completion of a specific task. It can be\n * interrupted by a kill signal.\n */\nint __sched wait_for_completion_killable(struct completion *x)\n{\n\tlong t = wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_KILLABLE);\n\tif (t == -ERESTARTSYS)\n\t\treturn t;\n\treturn 0;\n}\nEXPORT_SYMBOL(wait_for_completion_killable);\n\n/**\n * wait_for_completion_killable_timeout: - waits for completion of a task (w/(to,killable))\n * @x:  holds the state of this particular completion\n * @timeout:  timeout value in jiffies\n *\n * This waits for either a completion of a specific task to be\n * signaled or for a specified timeout to expire. It can be\n * interrupted by a kill signal. The timeout is in jiffies.\n */\nlong __sched\nwait_for_completion_killable_timeout(struct completion *x,\n\t\t\t\t     unsigned long timeout)\n{\n\treturn wait_for_common(x, timeout, TASK_KILLABLE);\n}\nEXPORT_SYMBOL(wait_for_completion_killable_timeout);\n\n/**\n *\ttry_wait_for_completion - try to decrement a completion without blocking\n *\t@x:\tcompletion structure\n *\n *\tReturns: 0 if a decrement cannot be done without blocking\n *\t\t 1 if a decrement succeeded.\n *\n *\tIf a completion is being used as a counting completion,\n *\tattempt to decrement the counter without blocking. This\n *\tenables us to avoid waiting if the resource the completion\n *\tis protecting is not available.\n */\nbool try_wait_for_completion(struct completion *x)\n{\n\tunsigned long flags;\n\tint ret = 1;\n\n\tspin_lock_irqsave(&x->wait.lock, flags);\n\tif (!x->done)\n\t\tret = 0;\n\telse\n\t\tx->done--;\n\tspin_unlock_irqrestore(&x->wait.lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(try_wait_for_completion);\n\n/**\n *\tcompletion_done - Test to see if a completion has any waiters\n *\t@x:\tcompletion structure\n *\n *\tReturns: 0 if there are waiters (wait_for_completion() in progress)\n *\t\t 1 if there are no waiters.\n *\n */\nbool completion_done(struct completion *x)\n{\n\tunsigned long flags;\n\tint ret = 1;\n\n\tspin_lock_irqsave(&x->wait.lock, flags);\n\tif (!x->done)\n\t\tret = 0;\n\tspin_unlock_irqrestore(&x->wait.lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(completion_done);\n\nstatic long __sched\nsleep_on_common(wait_queue_head_t *q, int state, long timeout)\n{\n\tunsigned long flags;\n\twait_queue_t wait;\n\n\tinit_waitqueue_entry(&wait, current);\n\n\t__set_current_state(state);\n\n\tspin_lock_irqsave(&q->lock, flags);\n\t__add_wait_queue(q, &wait);\n\tspin_unlock(&q->lock);\n\ttimeout = schedule_timeout(timeout);\n\tspin_lock_irq(&q->lock);\n\t__remove_wait_queue(q, &wait);\n\tspin_unlock_irqrestore(&q->lock, flags);\n\n\treturn timeout;\n}\n\nvoid __sched interruptible_sleep_on(wait_queue_head_t *q)\n{\n\tsleep_on_common(q, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);\n}\nEXPORT_SYMBOL(interruptible_sleep_on);\n\nlong __sched\ninterruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)\n{\n\treturn sleep_on_common(q, TASK_INTERRUPTIBLE, timeout);\n}\nEXPORT_SYMBOL(interruptible_sleep_on_timeout);\n\nvoid __sched sleep_on(wait_queue_head_t *q)\n{\n\tsleep_on_common(q, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);\n}\nEXPORT_SYMBOL(sleep_on);\n\nlong __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)\n{\n\treturn sleep_on_common(q, TASK_UNINTERRUPTIBLE, timeout);\n}\nEXPORT_SYMBOL(sleep_on_timeout);\n\n#ifdef CONFIG_RT_MUTEXES\n\n/*\n * rt_mutex_setprio - set the current priority of a task\n * @p: task\n * @prio: prio value (kernel-internal form)\n *\n * This function changes the 'effective' priority of a task. It does\n * not touch ->normal_prio like __setscheduler().\n *\n * Used by the rt_mutex code to implement priority inheritance logic.\n */\nvoid rt_mutex_setprio(struct task_struct *p, int prio)\n{\n\tint oldprio, on_rq, running;\n\tstruct rq *rq;\n\tconst struct sched_class *prev_class;\n\n\tBUG_ON(prio < 0 || prio > MAX_PRIO);\n\n\trq = __task_rq_lock(p);\n\n\ttrace_sched_pi_setprio(p, prio);\n\toldprio = p->prio;\n\tprev_class = p->sched_class;\n\ton_rq = p->on_rq;\n\trunning = task_current(rq, p);\n\tif (on_rq)\n\t\tdequeue_task(rq, p, 0);\n\tif (running)\n\t\tp->sched_class->put_prev_task(rq, p);\n\n\tif (rt_prio(prio))\n\t\tp->sched_class = &rt_sched_class;\n\telse\n\t\tp->sched_class = &fair_sched_class;\n\n\tp->prio = prio;\n\n\tif (running)\n\t\tp->sched_class->set_curr_task(rq);\n\tif (on_rq)\n\t\tenqueue_task(rq, p, oldprio < prio ? ENQUEUE_HEAD : 0);\n\n\tcheck_class_changed(rq, p, prev_class, oldprio);\n\t__task_rq_unlock(rq);\n}\n\n#endif\n\nvoid set_user_nice(struct task_struct *p, long nice)\n{\n\tint old_prio, delta, on_rq;\n\tunsigned long flags;\n\tstruct rq *rq;\n\n\tif (TASK_NICE(p) == nice || nice < -20 || nice > 19)\n\t\treturn;\n\t/*\n\t * We have to be careful, if called from sys_setpriority(),\n\t * the task might be in the middle of scheduling on another CPU.\n\t */\n\trq = task_rq_lock(p, &flags);\n\t/*\n\t * The RT priorities are set via sched_setscheduler(), but we still\n\t * allow the 'normal' nice value to be set - but as expected\n\t * it wont have any effect on scheduling until the task is\n\t * SCHED_FIFO/SCHED_RR:\n\t */\n\tif (task_has_rt_policy(p)) {\n\t\tp->static_prio = NICE_TO_PRIO(nice);\n\t\tgoto out_unlock;\n\t}\n\ton_rq = p->on_rq;\n\tif (on_rq)\n\t\tdequeue_task(rq, p, 0);\n\n\tp->static_prio = NICE_TO_PRIO(nice);\n\tset_load_weight(p);\n\told_prio = p->prio;\n\tp->prio = effective_prio(p);\n\tdelta = p->prio - old_prio;\n\n\tif (on_rq) {\n\t\tenqueue_task(rq, p, 0);\n\t\t/*\n\t\t * If the task increased its priority or is running and\n\t\t * lowered its priority, then reschedule its CPU:\n\t\t */\n\t\tif (delta < 0 || (delta > 0 && task_running(rq, p)))\n\t\t\tresched_task(rq->curr);\n\t}\nout_unlock:\n\ttask_rq_unlock(rq, p, &flags);\n}\nEXPORT_SYMBOL(set_user_nice);\n\n/*\n * can_nice - check if a task can reduce its nice value\n * @p: task\n * @nice: nice value\n */\nint can_nice(const struct task_struct *p, const int nice)\n{\n\t/* convert nice value [19,-20] to rlimit style value [1,40] */\n\tint nice_rlim = 20 - nice;\n\n\treturn (nice_rlim <= task_rlimit(p, RLIMIT_NICE) ||\n\t\tcapable(CAP_SYS_NICE));\n}\n\n#ifdef __ARCH_WANT_SYS_NICE\n\n/*\n * sys_nice - change the priority of the current process.\n * @increment: priority increment\n *\n * sys_setpriority is a more generic, but much slower function that\n * does similar things.\n */\nSYSCALL_DEFINE1(nice, int, increment)\n{\n\tlong nice, retval;\n\n\t/*\n\t * Setpriority might change our priority at the same moment.\n\t * We don't have to worry. Conceptually one call occurs first\n\t * and we have a single winner.\n\t */\n\tif (increment < -40)\n\t\tincrement = -40;\n\tif (increment > 40)\n\t\tincrement = 40;\n\n\tnice = TASK_NICE(current) + increment;\n\tif (nice < -20)\n\t\tnice = -20;\n\tif (nice > 19)\n\t\tnice = 19;\n\n\tif (increment < 0 && !can_nice(current, nice))\n\t\treturn -EPERM;\n\n\tretval = security_task_setnice(current, nice);\n\tif (retval)\n\t\treturn retval;\n\n\tset_user_nice(current, nice);\n\treturn 0;\n}\n\n#endif\n\n/**\n * task_prio - return the priority value of a given task.\n * @p: the task in question.\n *\n * This is the priority value as seen by users in /proc.\n * RT tasks are offset by -200. Normal tasks are centered\n * around 0, value goes from -16 to +15.\n */\nint task_prio(const struct task_struct *p)\n{\n\treturn p->prio - MAX_RT_PRIO;\n}\n\n/**\n * task_nice - return the nice value of a given task.\n * @p: the task in question.\n */\nint task_nice(const struct task_struct *p)\n{\n\treturn TASK_NICE(p);\n}\nEXPORT_SYMBOL(task_nice);\n\n/**\n * idle_cpu - is a given cpu idle currently?\n * @cpu: the processor in question.\n */\nint idle_cpu(int cpu)\n{\n\treturn cpu_curr(cpu) == cpu_rq(cpu)->idle;\n}\n\n/**\n * idle_task - return the idle task for a given cpu.\n * @cpu: the processor in question.\n */\nstruct task_struct *idle_task(int cpu)\n{\n\treturn cpu_rq(cpu)->idle;\n}\n\n/**\n * find_process_by_pid - find a process with a matching PID value.\n * @pid: the pid in question.\n */\nstatic struct task_struct *find_process_by_pid(pid_t pid)\n{\n\treturn pid ? find_task_by_vpid(pid) : current;\n}\n\n/* Actually do priority change: must hold rq lock. */\nstatic void\n__setscheduler(struct rq *rq, struct task_struct *p, int policy, int prio)\n{\n\tp->policy = policy;\n\tp->rt_priority = prio;\n\tp->normal_prio = normal_prio(p);\n\t/* we are holding p->pi_lock already */\n\tp->prio = rt_mutex_getprio(p);\n\tif (rt_prio(p->prio))\n\t\tp->sched_class = &rt_sched_class;\n\telse\n\t\tp->sched_class = &fair_sched_class;\n\tset_load_weight(p);\n}\n\n/*\n * check the target process has a UID that matches the current process's\n */\nstatic bool check_same_owner(struct task_struct *p)\n{\n\tconst struct cred *cred = current_cred(), *pcred;\n\tbool match;\n\n\trcu_read_lock();\n\tpcred = __task_cred(p);\n\tif (cred->user->user_ns == pcred->user->user_ns)\n\t\tmatch = (cred->euid == pcred->euid ||\n\t\t\t cred->euid == pcred->uid);\n\telse\n\t\tmatch = false;\n\trcu_read_unlock();\n\treturn match;\n}\n\nstatic int __sched_setscheduler(struct task_struct *p, int policy,\n\t\t\t\tconst struct sched_param *param, bool user)\n{\n\tint retval, oldprio, oldpolicy = -1, on_rq, running;\n\tunsigned long flags;\n\tconst struct sched_class *prev_class;\n\tstruct rq *rq;\n\tint reset_on_fork;\n\n\t/* may grab non-irq protected spin_locks */\n\tBUG_ON(in_interrupt());\nrecheck:\n\t/* double check policy once rq lock held */\n\tif (policy < 0) {\n\t\treset_on_fork = p->sched_reset_on_fork;\n\t\tpolicy = oldpolicy = p->policy;\n\t} else {\n\t\treset_on_fork = !!(policy & SCHED_RESET_ON_FORK);\n\t\tpolicy &= ~SCHED_RESET_ON_FORK;\n\n\t\tif (policy != SCHED_FIFO && policy != SCHED_RR &&\n\t\t\t\tpolicy != SCHED_NORMAL && policy != SCHED_BATCH &&\n\t\t\t\tpolicy != SCHED_IDLE)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Valid priorities for SCHED_FIFO and SCHED_RR are\n\t * 1..MAX_USER_RT_PRIO-1, valid priority for SCHED_NORMAL,\n\t * SCHED_BATCH and SCHED_IDLE is 0.\n\t */\n\tif (param->sched_priority < 0 ||\n\t    (p->mm && param->sched_priority > MAX_USER_RT_PRIO-1) ||\n\t    (!p->mm && param->sched_priority > MAX_RT_PRIO-1))\n\t\treturn -EINVAL;\n\tif (rt_policy(policy) != (param->sched_priority != 0))\n\t\treturn -EINVAL;\n\n\t/*\n\t * Allow unprivileged RT tasks to decrease priority:\n\t */\n\tif (user && !capable(CAP_SYS_NICE)) {\n\t\tif (rt_policy(policy)) {\n\t\t\tunsigned long rlim_rtprio =\n\t\t\t\t\ttask_rlimit(p, RLIMIT_RTPRIO);\n\n\t\t\t/* can't set/change the rt policy */\n\t\t\tif (policy != p->policy && !rlim_rtprio)\n\t\t\t\treturn -EPERM;\n\n\t\t\t/* can't increase priority */\n\t\t\tif (param->sched_priority > p->rt_priority &&\n\t\t\t    param->sched_priority > rlim_rtprio)\n\t\t\t\treturn -EPERM;\n\t\t}\n\n\t\t/*\n\t\t * Treat SCHED_IDLE as nice 20. Only allow a switch to\n\t\t * SCHED_NORMAL if the RLIMIT_NICE would normally permit it.\n\t\t */\n\t\tif (p->policy == SCHED_IDLE && policy != SCHED_IDLE) {\n\t\t\tif (!can_nice(p, TASK_NICE(p)))\n\t\t\t\treturn -EPERM;\n\t\t}\n\n\t\t/* can't change other user's priorities */\n\t\tif (!check_same_owner(p))\n\t\t\treturn -EPERM;\n\n\t\t/* Normal users shall not reset the sched_reset_on_fork flag */\n\t\tif (p->sched_reset_on_fork && !reset_on_fork)\n\t\t\treturn -EPERM;\n\t}\n\n\tif (user) {\n\t\tretval = security_task_setscheduler(p);\n\t\tif (retval)\n\t\t\treturn retval;\n\t}\n\n\t/*\n\t * make sure no PI-waiters arrive (or leave) while we are\n\t * changing the priority of the task:\n\t *\n\t * To be able to change p->policy safely, the appropriate\n\t * runqueue lock must be held.\n\t */\n\trq = task_rq_lock(p, &flags);\n\n\t/*\n\t * Changing the policy of the stop threads its a very bad idea\n\t */\n\tif (p == rq->stop) {\n\t\ttask_rq_unlock(rq, p, &flags);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * If not changing anything there's no need to proceed further:\n\t */\n\tif (unlikely(policy == p->policy && (!rt_policy(policy) ||\n\t\t\tparam->sched_priority == p->rt_priority))) {\n\n\t\t__task_rq_unlock(rq);\n\t\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\t\treturn 0;\n\t}\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tif (user) {\n\t\t/*\n\t\t * Do not allow realtime tasks into groups that have no runtime\n\t\t * assigned.\n\t\t */\n\t\tif (rt_bandwidth_enabled() && rt_policy(policy) &&\n\t\t\t\ttask_group(p)->rt_bandwidth.rt_runtime == 0 &&\n\t\t\t\t!task_group_is_autogroup(task_group(p))) {\n\t\t\ttask_rq_unlock(rq, p, &flags);\n\t\t\treturn -EPERM;\n\t\t}\n\t}\n#endif\n\n\t/* recheck policy now with rq lock held */\n\tif (unlikely(oldpolicy != -1 && oldpolicy != p->policy)) {\n\t\tpolicy = oldpolicy = -1;\n\t\ttask_rq_unlock(rq, p, &flags);\n\t\tgoto recheck;\n\t}\n\ton_rq = p->on_rq;\n\trunning = task_current(rq, p);\n\tif (on_rq)\n\t\tdeactivate_task(rq, p, 0);\n\tif (running)\n\t\tp->sched_class->put_prev_task(rq, p);\n\n\tp->sched_reset_on_fork = reset_on_fork;\n\n\toldprio = p->prio;\n\tprev_class = p->sched_class;\n\t__setscheduler(rq, p, policy, param->sched_priority);\n\n\tif (running)\n\t\tp->sched_class->set_curr_task(rq);\n\tif (on_rq)\n\t\tactivate_task(rq, p, 0);\n\n\tcheck_class_changed(rq, p, prev_class, oldprio);\n\ttask_rq_unlock(rq, p, &flags);\n\n\trt_mutex_adjust_pi(p);\n\n\treturn 0;\n}\n\n/**\n * sched_setscheduler - change the scheduling policy and/or RT priority of a thread.\n * @p: the task in question.\n * @policy: new policy.\n * @param: structure containing the new RT priority.\n *\n * NOTE that the task may be already dead.\n */\nint sched_setscheduler(struct task_struct *p, int policy,\n\t\t       const struct sched_param *param)\n{\n\treturn __sched_setscheduler(p, policy, param, true);\n}\nEXPORT_SYMBOL_GPL(sched_setscheduler);\n\n/**\n * sched_setscheduler_nocheck - change the scheduling policy and/or RT priority of a thread from kernelspace.\n * @p: the task in question.\n * @policy: new policy.\n * @param: structure containing the new RT priority.\n *\n * Just like sched_setscheduler, only don't bother checking if the\n * current context has permission.  For example, this is needed in\n * stop_machine(): we create temporary high priority worker threads,\n * but our caller might not have that capability.\n */\nint sched_setscheduler_nocheck(struct task_struct *p, int policy,\n\t\t\t       const struct sched_param *param)\n{\n\treturn __sched_setscheduler(p, policy, param, false);\n}\n\nstatic int\ndo_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param)\n{\n\tstruct sched_param lparam;\n\tstruct task_struct *p;\n\tint retval;\n\n\tif (!param || pid < 0)\n\t\treturn -EINVAL;\n\tif (copy_from_user(&lparam, param, sizeof(struct sched_param)))\n\t\treturn -EFAULT;\n\n\trcu_read_lock();\n\tretval = -ESRCH;\n\tp = find_process_by_pid(pid);\n\tif (p != NULL)\n\t\tretval = sched_setscheduler(p, policy, &lparam);\n\trcu_read_unlock();\n\n\treturn retval;\n}\n\n/**\n * sys_sched_setscheduler - set/change the scheduler policy and RT priority\n * @pid: the pid in question.\n * @policy: new policy.\n * @param: structure containing the new RT priority.\n */\nSYSCALL_DEFINE3(sched_setscheduler, pid_t, pid, int, policy,\n\t\tstruct sched_param __user *, param)\n{\n\t/* negative values for policy are not valid */\n\tif (policy < 0)\n\t\treturn -EINVAL;\n\n\treturn do_sched_setscheduler(pid, policy, param);\n}\n\n/**\n * sys_sched_setparam - set/change the RT priority of a thread\n * @pid: the pid in question.\n * @param: structure containing the new RT priority.\n */\nSYSCALL_DEFINE2(sched_setparam, pid_t, pid, struct sched_param __user *, param)\n{\n\treturn do_sched_setscheduler(pid, -1, param);\n}\n\n/**\n * sys_sched_getscheduler - get the policy (scheduling class) of a thread\n * @pid: the pid in question.\n */\nSYSCALL_DEFINE1(sched_getscheduler, pid_t, pid)\n{\n\tstruct task_struct *p;\n\tint retval;\n\n\tif (pid < 0)\n\t\treturn -EINVAL;\n\n\tretval = -ESRCH;\n\trcu_read_lock();\n\tp = find_process_by_pid(pid);\n\tif (p) {\n\t\tretval = security_task_getscheduler(p);\n\t\tif (!retval)\n\t\t\tretval = p->policy\n\t\t\t\t| (p->sched_reset_on_fork ? SCHED_RESET_ON_FORK : 0);\n\t}\n\trcu_read_unlock();\n\treturn retval;\n}\n\n/**\n * sys_sched_getparam - get the RT priority of a thread\n * @pid: the pid in question.\n * @param: structure containing the RT priority.\n */\nSYSCALL_DEFINE2(sched_getparam, pid_t, pid, struct sched_param __user *, param)\n{\n\tstruct sched_param lp;\n\tstruct task_struct *p;\n\tint retval;\n\n\tif (!param || pid < 0)\n\t\treturn -EINVAL;\n\n\trcu_read_lock();\n\tp = find_process_by_pid(pid);\n\tretval = -ESRCH;\n\tif (!p)\n\t\tgoto out_unlock;\n\n\tretval = security_task_getscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\tlp.sched_priority = p->rt_priority;\n\trcu_read_unlock();\n\n\t/*\n\t * This one might sleep, we cannot do it with a spinlock held ...\n\t */\n\tretval = copy_to_user(param, &lp, sizeof(*param)) ? -EFAULT : 0;\n\n\treturn retval;\n\nout_unlock:\n\trcu_read_unlock();\n\treturn retval;\n}\n\nlong sched_setaffinity(pid_t pid, const struct cpumask *in_mask)\n{\n\tcpumask_var_t cpus_allowed, new_mask;\n\tstruct task_struct *p;\n\tint retval;\n\n\tget_online_cpus();\n\trcu_read_lock();\n\n\tp = find_process_by_pid(pid);\n\tif (!p) {\n\t\trcu_read_unlock();\n\t\tput_online_cpus();\n\t\treturn -ESRCH;\n\t}\n\n\t/* Prevent p going away */\n\tget_task_struct(p);\n\trcu_read_unlock();\n\n\tif (!alloc_cpumask_var(&cpus_allowed, GFP_KERNEL)) {\n\t\tretval = -ENOMEM;\n\t\tgoto out_put_task;\n\t}\n\tif (!alloc_cpumask_var(&new_mask, GFP_KERNEL)) {\n\t\tretval = -ENOMEM;\n\t\tgoto out_free_cpus_allowed;\n\t}\n\tretval = -EPERM;\n\tif (!check_same_owner(p) && !task_ns_capable(p, CAP_SYS_NICE))\n\t\tgoto out_unlock;\n\n\tretval = security_task_setscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\tcpuset_cpus_allowed(p, cpus_allowed);\n\tcpumask_and(new_mask, in_mask, cpus_allowed);\nagain:\n\tretval = set_cpus_allowed_ptr(p, new_mask);\n\n\tif (!retval) {\n\t\tcpuset_cpus_allowed(p, cpus_allowed);\n\t\tif (!cpumask_subset(new_mask, cpus_allowed)) {\n\t\t\t/*\n\t\t\t * We must have raced with a concurrent cpuset\n\t\t\t * update. Just reset the cpus_allowed to the\n\t\t\t * cpuset's cpus_allowed\n\t\t\t */\n\t\t\tcpumask_copy(new_mask, cpus_allowed);\n\t\t\tgoto again;\n\t\t}\n\t}\nout_unlock:\n\tfree_cpumask_var(new_mask);\nout_free_cpus_allowed:\n\tfree_cpumask_var(cpus_allowed);\nout_put_task:\n\tput_task_struct(p);\n\tput_online_cpus();\n\treturn retval;\n}\n\nstatic int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len,\n\t\t\t     struct cpumask *new_mask)\n{\n\tif (len < cpumask_size())\n\t\tcpumask_clear(new_mask);\n\telse if (len > cpumask_size())\n\t\tlen = cpumask_size();\n\n\treturn copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0;\n}\n\n/**\n * sys_sched_setaffinity - set the cpu affinity of a process\n * @pid: pid of the process\n * @len: length in bytes of the bitmask pointed to by user_mask_ptr\n * @user_mask_ptr: user-space pointer to the new cpu mask\n */\nSYSCALL_DEFINE3(sched_setaffinity, pid_t, pid, unsigned int, len,\n\t\tunsigned long __user *, user_mask_ptr)\n{\n\tcpumask_var_t new_mask;\n\tint retval;\n\n\tif (!alloc_cpumask_var(&new_mask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tretval = get_user_cpu_mask(user_mask_ptr, len, new_mask);\n\tif (retval == 0)\n\t\tretval = sched_setaffinity(pid, new_mask);\n\tfree_cpumask_var(new_mask);\n\treturn retval;\n}\n\nlong sched_getaffinity(pid_t pid, struct cpumask *mask)\n{\n\tstruct task_struct *p;\n\tunsigned long flags;\n\tint retval;\n\n\tget_online_cpus();\n\trcu_read_lock();\n\n\tretval = -ESRCH;\n\tp = find_process_by_pid(pid);\n\tif (!p)\n\t\tgoto out_unlock;\n\n\tretval = security_task_getscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\traw_spin_lock_irqsave(&p->pi_lock, flags);\n\tcpumask_and(mask, &p->cpus_allowed, cpu_online_mask);\n\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\nout_unlock:\n\trcu_read_unlock();\n\tput_online_cpus();\n\n\treturn retval;\n}\n\n/**\n * sys_sched_getaffinity - get the cpu affinity of a process\n * @pid: pid of the process\n * @len: length in bytes of the bitmask pointed to by user_mask_ptr\n * @user_mask_ptr: user-space pointer to hold the current cpu mask\n */\nSYSCALL_DEFINE3(sched_getaffinity, pid_t, pid, unsigned int, len,\n\t\tunsigned long __user *, user_mask_ptr)\n{\n\tint ret;\n\tcpumask_var_t mask;\n\n\tif ((len * BITS_PER_BYTE) < nr_cpu_ids)\n\t\treturn -EINVAL;\n\tif (len & (sizeof(unsigned long)-1))\n\t\treturn -EINVAL;\n\n\tif (!alloc_cpumask_var(&mask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tret = sched_getaffinity(pid, mask);\n\tif (ret == 0) {\n\t\tsize_t retlen = min_t(size_t, len, cpumask_size());\n\n\t\tif (copy_to_user(user_mask_ptr, mask, retlen))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = retlen;\n\t}\n\tfree_cpumask_var(mask);\n\n\treturn ret;\n}\n\n/**\n * sys_sched_yield - yield the current processor to other threads.\n *\n * This function yields the current CPU to other tasks. If there are no\n * other threads running on this CPU then this function will return.\n */\nSYSCALL_DEFINE0(sched_yield)\n{\n\tstruct rq *rq = this_rq_lock();\n\n\tschedstat_inc(rq, yld_count);\n\tcurrent->sched_class->yield_task(rq);\n\n\t/*\n\t * Since we are going to call schedule() anyway, there's\n\t * no need to preempt or enable interrupts:\n\t */\n\t__release(rq->lock);\n\tspin_release(&rq->lock.dep_map, 1, _THIS_IP_);\n\tdo_raw_spin_unlock(&rq->lock);\n\tpreempt_enable_no_resched();\n\n\tschedule();\n\n\treturn 0;\n}\n\nstatic inline int should_resched(void)\n{\n\treturn need_resched() && !(preempt_count() & PREEMPT_ACTIVE);\n}\n\nstatic void __cond_resched(void)\n{\n\tadd_preempt_count(PREEMPT_ACTIVE);\n\tschedule();\n\tsub_preempt_count(PREEMPT_ACTIVE);\n}\n\nint __sched _cond_resched(void)\n{\n\tif (should_resched()) {\n\t\t__cond_resched();\n\t\treturn 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(_cond_resched);\n\n/*\n * __cond_resched_lock() - if a reschedule is pending, drop the given lock,\n * call schedule, and on return reacquire the lock.\n *\n * This works OK both with and without CONFIG_PREEMPT. We do strange low-level\n * operations here to prevent schedule() from being called twice (once via\n * spin_unlock(), once by hand).\n */\nint __cond_resched_lock(spinlock_t *lock)\n{\n\tint resched = should_resched();\n\tint ret = 0;\n\n\tlockdep_assert_held(lock);\n\n\tif (spin_needbreak(lock) || resched) {\n\t\tspin_unlock(lock);\n\t\tif (resched)\n\t\t\t__cond_resched();\n\t\telse\n\t\t\tcpu_relax();\n\t\tret = 1;\n\t\tspin_lock(lock);\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__cond_resched_lock);\n\nint __sched __cond_resched_softirq(void)\n{\n\tBUG_ON(!in_softirq());\n\n\tif (should_resched()) {\n\t\tlocal_bh_enable();\n\t\t__cond_resched();\n\t\tlocal_bh_disable();\n\t\treturn 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(__cond_resched_softirq);\n\n/**\n * yield - yield the current processor to other threads.\n *\n * This is a shortcut for kernel-space yielding - it marks the\n * thread runnable and calls sys_sched_yield().\n */\nvoid __sched yield(void)\n{\n\tset_current_state(TASK_RUNNING);\n\tsys_sched_yield();\n}\nEXPORT_SYMBOL(yield);\n\n/**\n * yield_to - yield the current processor to another thread in\n * your thread group, or accelerate that thread toward the\n * processor it's on.\n * @p: target task\n * @preempt: whether task preemption is allowed or not\n *\n * It's the caller's job to ensure that the target task struct\n * can't go away on us before we can do any checks.\n *\n * Returns true if we indeed boosted the target task.\n */\nbool __sched yield_to(struct task_struct *p, bool preempt)\n{\n\tstruct task_struct *curr = current;\n\tstruct rq *rq, *p_rq;\n\tunsigned long flags;\n\tbool yielded = 0;\n\n\tlocal_irq_save(flags);\n\trq = this_rq();\n\nagain:\n\tp_rq = task_rq(p);\n\tdouble_rq_lock(rq, p_rq);\n\twhile (task_rq(p) != p_rq) {\n\t\tdouble_rq_unlock(rq, p_rq);\n\t\tgoto again;\n\t}\n\n\tif (!curr->sched_class->yield_to_task)\n\t\tgoto out;\n\n\tif (curr->sched_class != p->sched_class)\n\t\tgoto out;\n\n\tif (task_running(p_rq, p) || p->state)\n\t\tgoto out;\n\n\tyielded = curr->sched_class->yield_to_task(rq, p, preempt);\n\tif (yielded) {\n\t\tschedstat_inc(rq, yld_count);\n\t\t/*\n\t\t * Make p's CPU reschedule; pick_next_entity takes care of\n\t\t * fairness.\n\t\t */\n\t\tif (preempt && rq != p_rq)\n\t\t\tresched_task(p_rq->curr);\n\t}\n\nout:\n\tdouble_rq_unlock(rq, p_rq);\n\tlocal_irq_restore(flags);\n\n\tif (yielded)\n\t\tschedule();\n\n\treturn yielded;\n}\nEXPORT_SYMBOL_GPL(yield_to);\n\n/*\n * This task is about to go to sleep on IO. Increment rq->nr_iowait so\n * that process accounting knows that this is a task in IO wait state.\n */\nvoid __sched io_schedule(void)\n{\n\tstruct rq *rq = raw_rq();\n\n\tdelayacct_blkio_start();\n\tatomic_inc(&rq->nr_iowait);\n\tblk_flush_plug(current);\n\tcurrent->in_iowait = 1;\n\tschedule();\n\tcurrent->in_iowait = 0;\n\tatomic_dec(&rq->nr_iowait);\n\tdelayacct_blkio_end();\n}\nEXPORT_SYMBOL(io_schedule);\n\nlong __sched io_schedule_timeout(long timeout)\n{\n\tstruct rq *rq = raw_rq();\n\tlong ret;\n\n\tdelayacct_blkio_start();\n\tatomic_inc(&rq->nr_iowait);\n\tblk_flush_plug(current);\n\tcurrent->in_iowait = 1;\n\tret = schedule_timeout(timeout);\n\tcurrent->in_iowait = 0;\n\tatomic_dec(&rq->nr_iowait);\n\tdelayacct_blkio_end();\n\treturn ret;\n}\n\n/**\n * sys_sched_get_priority_max - return maximum RT priority.\n * @policy: scheduling class.\n *\n * this syscall returns the maximum rt_priority that can be used\n * by a given scheduling class.\n */\nSYSCALL_DEFINE1(sched_get_priority_max, int, policy)\n{\n\tint ret = -EINVAL;\n\n\tswitch (policy) {\n\tcase SCHED_FIFO:\n\tcase SCHED_RR:\n\t\tret = MAX_USER_RT_PRIO-1;\n\t\tbreak;\n\tcase SCHED_NORMAL:\n\tcase SCHED_BATCH:\n\tcase SCHED_IDLE:\n\t\tret = 0;\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\n/**\n * sys_sched_get_priority_min - return minimum RT priority.\n * @policy: scheduling class.\n *\n * this syscall returns the minimum rt_priority that can be used\n * by a given scheduling class.\n */\nSYSCALL_DEFINE1(sched_get_priority_min, int, policy)\n{\n\tint ret = -EINVAL;\n\n\tswitch (policy) {\n\tcase SCHED_FIFO:\n\tcase SCHED_RR:\n\t\tret = 1;\n\t\tbreak;\n\tcase SCHED_NORMAL:\n\tcase SCHED_BATCH:\n\tcase SCHED_IDLE:\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\n/**\n * sys_sched_rr_get_interval - return the default timeslice of a process.\n * @pid: pid of the process.\n * @interval: userspace pointer to the timeslice value.\n *\n * this syscall writes the default timeslice value of a given process\n * into the user-space timespec buffer. A value of '0' means infinity.\n */\nSYSCALL_DEFINE2(sched_rr_get_interval, pid_t, pid,\n\t\tstruct timespec __user *, interval)\n{\n\tstruct task_struct *p;\n\tunsigned int time_slice;\n\tunsigned long flags;\n\tstruct rq *rq;\n\tint retval;\n\tstruct timespec t;\n\n\tif (pid < 0)\n\t\treturn -EINVAL;\n\n\tretval = -ESRCH;\n\trcu_read_lock();\n\tp = find_process_by_pid(pid);\n\tif (!p)\n\t\tgoto out_unlock;\n\n\tretval = security_task_getscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\trq = task_rq_lock(p, &flags);\n\ttime_slice = p->sched_class->get_rr_interval(rq, p);\n\ttask_rq_unlock(rq, p, &flags);\n\n\trcu_read_unlock();\n\tjiffies_to_timespec(time_slice, &t);\n\tretval = copy_to_user(interval, &t, sizeof(t)) ? -EFAULT : 0;\n\treturn retval;\n\nout_unlock:\n\trcu_read_unlock();\n\treturn retval;\n}\n\nstatic const char stat_nam[] = TASK_STATE_TO_CHAR_STR;\n\nvoid sched_show_task(struct task_struct *p)\n{\n\tunsigned long free = 0;\n\tunsigned state;\n\n\tstate = p->state ? __ffs(p->state) + 1 : 0;\n\tprintk(KERN_INFO \"%-15.15s %c\", p->comm,\n\t\tstate < sizeof(stat_nam) - 1 ? stat_nam[state] : '?');\n#if BITS_PER_LONG == 32\n\tif (state == TASK_RUNNING)\n\t\tprintk(KERN_CONT \" running  \");\n\telse\n\t\tprintk(KERN_CONT \" %08lx \", thread_saved_pc(p));\n#else\n\tif (state == TASK_RUNNING)\n\t\tprintk(KERN_CONT \"  running task    \");\n\telse\n\t\tprintk(KERN_CONT \" %016lx \", thread_saved_pc(p));\n#endif\n#ifdef CONFIG_DEBUG_STACK_USAGE\n\tfree = stack_not_used(p);\n#endif\n\tprintk(KERN_CONT \"%5lu %5d %6d 0x%08lx\\n\", free,\n\t\ttask_pid_nr(p), task_pid_nr(p->real_parent),\n\t\t(unsigned long)task_thread_info(p)->flags);\n\n\tshow_stack(p, NULL);\n}\n\nvoid show_state_filter(unsigned long state_filter)\n{\n\tstruct task_struct *g, *p;\n\n#if BITS_PER_LONG == 32\n\tprintk(KERN_INFO\n\t\t\"  task                PC stack   pid father\\n\");\n#else\n\tprintk(KERN_INFO\n\t\t\"  task                        PC stack   pid father\\n\");\n#endif\n\tread_lock(&tasklist_lock);\n\tdo_each_thread(g, p) {\n\t\t/*\n\t\t * reset the NMI-timeout, listing all files on a slow\n\t\t * console might take a lot of time:\n\t\t */\n\t\ttouch_nmi_watchdog();\n\t\tif (!state_filter || (p->state & state_filter))\n\t\t\tsched_show_task(p);\n\t} while_each_thread(g, p);\n\n\ttouch_all_softlockup_watchdogs();\n\n#ifdef CONFIG_SCHED_DEBUG\n\tsysrq_sched_debug_show();\n#endif\n\tread_unlock(&tasklist_lock);\n\t/*\n\t * Only show locks if all tasks are dumped:\n\t */\n\tif (!state_filter)\n\t\tdebug_show_all_locks();\n}\n\nvoid __cpuinit init_idle_bootup_task(struct task_struct *idle)\n{\n\tidle->sched_class = &idle_sched_class;\n}\n\n/**\n * init_idle - set up an idle thread for a given CPU\n * @idle: task in question\n * @cpu: cpu the idle task belongs to\n *\n * NOTE: this function does not set the idle thread's NEED_RESCHED\n * flag, to make booting more robust.\n */\nvoid __cpuinit init_idle(struct task_struct *idle, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&rq->lock, flags);\n\n\t__sched_fork(idle);\n\tidle->state = TASK_RUNNING;\n\tidle->se.exec_start = sched_clock();\n\n\tdo_set_cpus_allowed(idle, cpumask_of(cpu));\n\t/*\n\t * We're having a chicken and egg problem, even though we are\n\t * holding rq->lock, the cpu isn't yet set to this cpu so the\n\t * lockdep check in task_group() will fail.\n\t *\n\t * Similar case to sched_fork(). / Alternatively we could\n\t * use task_rq_lock() here and obtain the other rq->lock.\n\t *\n\t * Silence PROVE_RCU\n\t */\n\trcu_read_lock();\n\t__set_task_cpu(idle, cpu);\n\trcu_read_unlock();\n\n\trq->curr = rq->idle = idle;\n#if defined(CONFIG_SMP)\n\tidle->on_cpu = 1;\n#endif\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\n\t/* Set the preempt count _outside_ the spinlocks! */\n\ttask_thread_info(idle)->preempt_count = 0;\n\n\t/*\n\t * The idle tasks have their own, simple scheduling class:\n\t */\n\tidle->sched_class = &idle_sched_class;\n\tftrace_graph_init_idle_task(idle, cpu);\n}\n\n/*\n * In a system that switches off the HZ timer nohz_cpu_mask\n * indicates which cpus entered this state. This is used\n * in the rcu update to wait only for active cpus. For system\n * which do not switch off the HZ timer nohz_cpu_mask should\n * always be CPU_BITS_NONE.\n */\ncpumask_var_t nohz_cpu_mask;\n\n/*\n * Increase the granularity value when there are more CPUs,\n * because with more CPUs the 'effective latency' as visible\n * to users decreases. But the relationship is not linear,\n * so pick a second-best guess by going with the log2 of the\n * number of CPUs.\n *\n * This idea comes from the SD scheduler of Con Kolivas:\n */\nstatic int get_update_sysctl_factor(void)\n{\n\tunsigned int cpus = min_t(int, num_online_cpus(), 8);\n\tunsigned int factor;\n\n\tswitch (sysctl_sched_tunable_scaling) {\n\tcase SCHED_TUNABLESCALING_NONE:\n\t\tfactor = 1;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LINEAR:\n\t\tfactor = cpus;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LOG:\n\tdefault:\n\t\tfactor = 1 + ilog2(cpus);\n\t\tbreak;\n\t}\n\n\treturn factor;\n}\n\nstatic void update_sysctl(void)\n{\n\tunsigned int factor = get_update_sysctl_factor();\n\n#define SET_SYSCTL(name) \\\n\t(sysctl_##name = (factor) * normalized_sysctl_##name)\n\tSET_SYSCTL(sched_min_granularity);\n\tSET_SYSCTL(sched_latency);\n\tSET_SYSCTL(sched_wakeup_granularity);\n#undef SET_SYSCTL\n}\n\nstatic inline void sched_init_granularity(void)\n{\n\tupdate_sysctl();\n}\n\n#ifdef CONFIG_SMP\nvoid do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)\n{\n\tif (p->sched_class && p->sched_class->set_cpus_allowed)\n\t\tp->sched_class->set_cpus_allowed(p, new_mask);\n\telse {\n\t\tcpumask_copy(&p->cpus_allowed, new_mask);\n\t\tp->rt.nr_cpus_allowed = cpumask_weight(new_mask);\n\t}\n}\n\n/*\n * This is how migration works:\n *\n * 1) we invoke migration_cpu_stop() on the target CPU using\n *    stop_one_cpu().\n * 2) stopper starts to run (implicitly forcing the migrated thread\n *    off the CPU)\n * 3) it checks whether the migrated task is still in the wrong runqueue.\n * 4) if it's in the wrong runqueue then the migration thread removes\n *    it and puts it into the right queue.\n * 5) stopper completes and stop_one_cpu() returns and the migration\n *    is done.\n */\n\n/*\n * Change a given task's CPU affinity. Migrate the thread to a\n * proper CPU and schedule it away if the CPU it's executing on\n * is removed from the allowed bitmask.\n *\n * NOTE: the caller must have a valid reference to the task, the\n * task must not exit() & deallocate itself prematurely. The\n * call is not atomic; no spinlocks may be held.\n */\nint set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tunsigned int dest_cpu;\n\tint ret = 0;\n\n\trq = task_rq_lock(p, &flags);\n\n\tif (cpumask_equal(&p->cpus_allowed, new_mask))\n\t\tgoto out;\n\n\tif (!cpumask_intersects(new_mask, cpu_active_mask)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (unlikely((p->flags & PF_THREAD_BOUND) && p != current)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tdo_set_cpus_allowed(p, new_mask);\n\n\t/* Can the task run on the task's current CPU? If so, we're done */\n\tif (cpumask_test_cpu(task_cpu(p), new_mask))\n\t\tgoto out;\n\n\tdest_cpu = cpumask_any_and(cpu_active_mask, new_mask);\n\tif (p->on_rq) {\n\t\tstruct migration_arg arg = { p, dest_cpu };\n\t\t/* Need help from migration thread: drop lock and wait. */\n\t\ttask_rq_unlock(rq, p, &flags);\n\t\tstop_one_cpu(cpu_of(rq), migration_cpu_stop, &arg);\n\t\ttlb_migrate_finish(p->mm);\n\t\treturn 0;\n\t}\nout:\n\ttask_rq_unlock(rq, p, &flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(set_cpus_allowed_ptr);\n\n/*\n * Move (not current) task off this cpu, onto dest cpu. We're doing\n * this because either it can't run here any more (set_cpus_allowed()\n * away from this CPU, or CPU going down), or because we're\n * attempting to rebalance this task on exec (sched_exec).\n *\n * So we race with normal scheduler movements, but that's OK, as long\n * as the task is no longer on this CPU.\n *\n * Returns non-zero if task was successfully migrated.\n */\nstatic int __migrate_task(struct task_struct *p, int src_cpu, int dest_cpu)\n{\n\tstruct rq *rq_dest, *rq_src;\n\tint ret = 0;\n\n\tif (unlikely(!cpu_active(dest_cpu)))\n\t\treturn ret;\n\n\trq_src = cpu_rq(src_cpu);\n\trq_dest = cpu_rq(dest_cpu);\n\n\traw_spin_lock(&p->pi_lock);\n\tdouble_rq_lock(rq_src, rq_dest);\n\t/* Already moved. */\n\tif (task_cpu(p) != src_cpu)\n\t\tgoto done;\n\t/* Affinity changed (again). */\n\tif (!cpumask_test_cpu(dest_cpu, &p->cpus_allowed))\n\t\tgoto fail;\n\n\t/*\n\t * If we're not on a rq, the next wake-up will ensure we're\n\t * placed properly.\n\t */\n\tif (p->on_rq) {\n\t\tdeactivate_task(rq_src, p, 0);\n\t\tset_task_cpu(p, dest_cpu);\n\t\tactivate_task(rq_dest, p, 0);\n\t\tcheck_preempt_curr(rq_dest, p, 0);\n\t}\ndone:\n\tret = 1;\nfail:\n\tdouble_rq_unlock(rq_src, rq_dest);\n\traw_spin_unlock(&p->pi_lock);\n\treturn ret;\n}\n\n/*\n * migration_cpu_stop - this will be executed by a highprio stopper thread\n * and performs thread migration by bumping thread off CPU then\n * 'pushing' onto another runqueue.\n */\nstatic int migration_cpu_stop(void *data)\n{\n\tstruct migration_arg *arg = data;\n\n\t/*\n\t * The original target cpu might have gone down and we might\n\t * be on another cpu but it doesn't matter.\n\t */\n\tlocal_irq_disable();\n\t__migrate_task(arg->task, raw_smp_processor_id(), arg->dest_cpu);\n\tlocal_irq_enable();\n\treturn 0;\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\n\n/*\n * Ensures that the idle task is using init_mm right before its cpu goes\n * offline.\n */\nvoid idle_task_exit(void)\n{\n\tstruct mm_struct *mm = current->active_mm;\n\n\tBUG_ON(cpu_online(smp_processor_id()));\n\n\tif (mm != &init_mm)\n\t\tswitch_mm(mm, &init_mm, current);\n\tmmdrop(mm);\n}\n\n/*\n * While a dead CPU has no uninterruptible tasks queued at this point,\n * it might still have a nonzero ->nr_uninterruptible counter, because\n * for performance reasons the counter is not stricly tracking tasks to\n * their home CPUs. So we just add the counter to another CPU's counter,\n * to keep the global sum constant after CPU-down:\n */\nstatic void migrate_nr_uninterruptible(struct rq *rq_src)\n{\n\tstruct rq *rq_dest = cpu_rq(cpumask_any(cpu_active_mask));\n\n\trq_dest->nr_uninterruptible += rq_src->nr_uninterruptible;\n\trq_src->nr_uninterruptible = 0;\n}\n\n/*\n * remove the tasks which were accounted by rq from calc_load_tasks.\n */\nstatic void calc_global_load_remove(struct rq *rq)\n{\n\tatomic_long_sub(rq->calc_load_active, &calc_load_tasks);\n\trq->calc_load_active = 0;\n}\n\n/*\n * Migrate all tasks from the rq, sleeping tasks will be migrated by\n * try_to_wake_up()->select_task_rq().\n *\n * Called with rq->lock held even though we'er in stop_machine() and\n * there's no concurrency possible, we hold the required locks anyway\n * because of lock validation efforts.\n */\nstatic void migrate_tasks(unsigned int dead_cpu)\n{\n\tstruct rq *rq = cpu_rq(dead_cpu);\n\tstruct task_struct *next, *stop = rq->stop;\n\tint dest_cpu;\n\n\t/*\n\t * Fudge the rq selection such that the below task selection loop\n\t * doesn't get stuck on the currently eligible stop task.\n\t *\n\t * We're currently inside stop_machine() and the rq is either stuck\n\t * in the stop_machine_cpu_stop() loop, or we're executing this code,\n\t * either way we should never end up calling schedule() until we're\n\t * done here.\n\t */\n\trq->stop = NULL;\n\n\tfor ( ; ; ) {\n\t\t/*\n\t\t * There's this thread running, bail when that's the only\n\t\t * remaining thread.\n\t\t */\n\t\tif (rq->nr_running == 1)\n\t\t\tbreak;\n\n\t\tnext = pick_next_task(rq);\n\t\tBUG_ON(!next);\n\t\tnext->sched_class->put_prev_task(rq, next);\n\n\t\t/* Find suitable destination for @next, with force if needed. */\n\t\tdest_cpu = select_fallback_rq(dead_cpu, next);\n\t\traw_spin_unlock(&rq->lock);\n\n\t\t__migrate_task(next, dead_cpu, dest_cpu);\n\n\t\traw_spin_lock(&rq->lock);\n\t}\n\n\trq->stop = stop;\n}\n\n#endif /* CONFIG_HOTPLUG_CPU */\n\n#if defined(CONFIG_SCHED_DEBUG) && defined(CONFIG_SYSCTL)\n\nstatic struct ctl_table sd_ctl_dir[] = {\n\t{\n\t\t.procname\t= \"sched_domain\",\n\t\t.mode\t\t= 0555,\n\t},\n\t{}\n};\n\nstatic struct ctl_table sd_ctl_root[] = {\n\t{\n\t\t.procname\t= \"kernel\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= sd_ctl_dir,\n\t},\n\t{}\n};\n\nstatic struct ctl_table *sd_alloc_ctl_entry(int n)\n{\n\tstruct ctl_table *entry =\n\t\tkcalloc(n, sizeof(struct ctl_table), GFP_KERNEL);\n\n\treturn entry;\n}\n\nstatic void sd_free_ctl_entry(struct ctl_table **tablep)\n{\n\tstruct ctl_table *entry;\n\n\t/*\n\t * In the intermediate directories, both the child directory and\n\t * procname are dynamically allocated and could fail but the mode\n\t * will always be set. In the lowest directory the names are\n\t * static strings and all have proc handlers.\n\t */\n\tfor (entry = *tablep; entry->mode; entry++) {\n\t\tif (entry->child)\n\t\t\tsd_free_ctl_entry(&entry->child);\n\t\tif (entry->proc_handler == NULL)\n\t\t\tkfree(entry->procname);\n\t}\n\n\tkfree(*tablep);\n\t*tablep = NULL;\n}\n\nstatic void\nset_table_entry(struct ctl_table *entry,\n\t\tconst char *procname, void *data, int maxlen,\n\t\tmode_t mode, proc_handler *proc_handler)\n{\n\tentry->procname = procname;\n\tentry->data = data;\n\tentry->maxlen = maxlen;\n\tentry->mode = mode;\n\tentry->proc_handler = proc_handler;\n}\n\nstatic struct ctl_table *\nsd_alloc_ctl_domain_table(struct sched_domain *sd)\n{\n\tstruct ctl_table *table = sd_alloc_ctl_entry(13);\n\n\tif (table == NULL)\n\t\treturn NULL;\n\n\tset_table_entry(&table[0], \"min_interval\", &sd->min_interval,\n\t\tsizeof(long), 0644, proc_doulongvec_minmax);\n\tset_table_entry(&table[1], \"max_interval\", &sd->max_interval,\n\t\tsizeof(long), 0644, proc_doulongvec_minmax);\n\tset_table_entry(&table[2], \"busy_idx\", &sd->busy_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[3], \"idle_idx\", &sd->idle_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[4], \"newidle_idx\", &sd->newidle_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[5], \"wake_idx\", &sd->wake_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[6], \"forkexec_idx\", &sd->forkexec_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[7], \"busy_factor\", &sd->busy_factor,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[8], \"imbalance_pct\", &sd->imbalance_pct,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[9], \"cache_nice_tries\",\n\t\t&sd->cache_nice_tries,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[10], \"flags\", &sd->flags,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[11], \"name\", sd->name,\n\t\tCORENAME_MAX_SIZE, 0444, proc_dostring);\n\t/* &table[12] is terminator */\n\n\treturn table;\n}\n\nstatic ctl_table *sd_alloc_ctl_cpu_table(int cpu)\n{\n\tstruct ctl_table *entry, *table;\n\tstruct sched_domain *sd;\n\tint domain_num = 0, i;\n\tchar buf[32];\n\n\tfor_each_domain(cpu, sd)\n\t\tdomain_num++;\n\tentry = table = sd_alloc_ctl_entry(domain_num + 1);\n\tif (table == NULL)\n\t\treturn NULL;\n\n\ti = 0;\n\tfor_each_domain(cpu, sd) {\n\t\tsnprintf(buf, 32, \"domain%d\", i);\n\t\tentry->procname = kstrdup(buf, GFP_KERNEL);\n\t\tentry->mode = 0555;\n\t\tentry->child = sd_alloc_ctl_domain_table(sd);\n\t\tentry++;\n\t\ti++;\n\t}\n\treturn table;\n}\n\nstatic struct ctl_table_header *sd_sysctl_header;\nstatic void register_sched_domain_sysctl(void)\n{\n\tint i, cpu_num = num_possible_cpus();\n\tstruct ctl_table *entry = sd_alloc_ctl_entry(cpu_num + 1);\n\tchar buf[32];\n\n\tWARN_ON(sd_ctl_dir[0].child);\n\tsd_ctl_dir[0].child = entry;\n\n\tif (entry == NULL)\n\t\treturn;\n\n\tfor_each_possible_cpu(i) {\n\t\tsnprintf(buf, 32, \"cpu%d\", i);\n\t\tentry->procname = kstrdup(buf, GFP_KERNEL);\n\t\tentry->mode = 0555;\n\t\tentry->child = sd_alloc_ctl_cpu_table(i);\n\t\tentry++;\n\t}\n\n\tWARN_ON(sd_sysctl_header);\n\tsd_sysctl_header = register_sysctl_table(sd_ctl_root);\n}\n\n/* may be called multiple times per register */\nstatic void unregister_sched_domain_sysctl(void)\n{\n\tif (sd_sysctl_header)\n\t\tunregister_sysctl_table(sd_sysctl_header);\n\tsd_sysctl_header = NULL;\n\tif (sd_ctl_dir[0].child)\n\t\tsd_free_ctl_entry(&sd_ctl_dir[0].child);\n}\n#else\nstatic void register_sched_domain_sysctl(void)\n{\n}\nstatic void unregister_sched_domain_sysctl(void)\n{\n}\n#endif\n\nstatic void set_rq_online(struct rq *rq)\n{\n\tif (!rq->online) {\n\t\tconst struct sched_class *class;\n\n\t\tcpumask_set_cpu(rq->cpu, rq->rd->online);\n\t\trq->online = 1;\n\n\t\tfor_each_class(class) {\n\t\t\tif (class->rq_online)\n\t\t\t\tclass->rq_online(rq);\n\t\t}\n\t}\n}\n\nstatic void set_rq_offline(struct rq *rq)\n{\n\tif (rq->online) {\n\t\tconst struct sched_class *class;\n\n\t\tfor_each_class(class) {\n\t\t\tif (class->rq_offline)\n\t\t\t\tclass->rq_offline(rq);\n\t\t}\n\n\t\tcpumask_clear_cpu(rq->cpu, rq->rd->online);\n\t\trq->online = 0;\n\t}\n}\n\n/*\n * migration_call - callback that gets triggered when a CPU is added.\n * Here we can start up the necessary migration thread for the new CPU.\n */\nstatic int __cpuinit\nmigration_call(struct notifier_block *nfb, unsigned long action, void *hcpu)\n{\n\tint cpu = (long)hcpu;\n\tunsigned long flags;\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\n\tcase CPU_UP_PREPARE:\n\t\trq->calc_load_update = calc_load_update;\n\t\tbreak;\n\n\tcase CPU_ONLINE:\n\t\t/* Update our root-domain */\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tif (rq->rd) {\n\t\t\tBUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));\n\n\t\t\tset_rq_online(rq);\n\t\t}\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\t\tbreak;\n\n#ifdef CONFIG_HOTPLUG_CPU\n\tcase CPU_DYING:\n\t\tsched_ttwu_pending();\n\t\t/* Update our root-domain */\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tif (rq->rd) {\n\t\t\tBUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));\n\t\t\tset_rq_offline(rq);\n\t\t}\n\t\tmigrate_tasks(cpu);\n\t\tBUG_ON(rq->nr_running != 1); /* the migration thread */\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\n\t\tmigrate_nr_uninterruptible(rq);\n\t\tcalc_global_load_remove(rq);\n\t\tbreak;\n#endif\n\t}\n\n\tupdate_max_interval();\n\n\treturn NOTIFY_OK;\n}\n\n/*\n * Register at high priority so that task migration (migrate_all_tasks)\n * happens before everything else.  This has to be lower priority than\n * the notifier in the perf_event subsystem, though.\n */\nstatic struct notifier_block __cpuinitdata migration_notifier = {\n\t.notifier_call = migration_call,\n\t.priority = CPU_PRI_MIGRATION,\n};\n\nstatic int __cpuinit sched_cpu_active(struct notifier_block *nfb,\n\t\t\t\t      unsigned long action, void *hcpu)\n{\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_ONLINE:\n\tcase CPU_DOWN_FAILED:\n\t\tset_cpu_active((long)hcpu, true);\n\t\treturn NOTIFY_OK;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nstatic int __cpuinit sched_cpu_inactive(struct notifier_block *nfb,\n\t\t\t\t\tunsigned long action, void *hcpu)\n{\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_DOWN_PREPARE:\n\t\tset_cpu_active((long)hcpu, false);\n\t\treturn NOTIFY_OK;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nstatic int __init migration_init(void)\n{\n\tvoid *cpu = (void *)(long)smp_processor_id();\n\tint err;\n\n\t/* Initialize migration for the boot CPU */\n\terr = migration_call(&migration_notifier, CPU_UP_PREPARE, cpu);\n\tBUG_ON(err == NOTIFY_BAD);\n\tmigration_call(&migration_notifier, CPU_ONLINE, cpu);\n\tregister_cpu_notifier(&migration_notifier);\n\n\t/* Register cpu active notifiers */\n\tcpu_notifier(sched_cpu_active, CPU_PRI_SCHED_ACTIVE);\n\tcpu_notifier(sched_cpu_inactive, CPU_PRI_SCHED_INACTIVE);\n\n\treturn 0;\n}\nearly_initcall(migration_init);\n#endif\n\n#ifdef CONFIG_SMP\n\nstatic cpumask_var_t sched_domains_tmpmask; /* sched_domains_mutex */\n\n#ifdef CONFIG_SCHED_DEBUG\n\nstatic __read_mostly int sched_domain_debug_enabled;\n\nstatic int __init sched_domain_debug_setup(char *str)\n{\n\tsched_domain_debug_enabled = 1;\n\n\treturn 0;\n}\nearly_param(\"sched_debug\", sched_domain_debug_setup);\n\nstatic int sched_domain_debug_one(struct sched_domain *sd, int cpu, int level,\n\t\t\t\t  struct cpumask *groupmask)\n{\n\tstruct sched_group *group = sd->groups;\n\tchar str[256];\n\n\tcpulist_scnprintf(str, sizeof(str), sched_domain_span(sd));\n\tcpumask_clear(groupmask);\n\n\tprintk(KERN_DEBUG \"%*s domain %d: \", level, \"\", level);\n\n\tif (!(sd->flags & SD_LOAD_BALANCE)) {\n\t\tprintk(\"does not load-balance\\n\");\n\t\tif (sd->parent)\n\t\t\tprintk(KERN_ERR \"ERROR: !SD_LOAD_BALANCE domain\"\n\t\t\t\t\t\" has parent\");\n\t\treturn -1;\n\t}\n\n\tprintk(KERN_CONT \"span %s level %s\\n\", str, sd->name);\n\n\tif (!cpumask_test_cpu(cpu, sched_domain_span(sd))) {\n\t\tprintk(KERN_ERR \"ERROR: domain->span does not contain \"\n\t\t\t\t\"CPU%d\\n\", cpu);\n\t}\n\tif (!cpumask_test_cpu(cpu, sched_group_cpus(group))) {\n\t\tprintk(KERN_ERR \"ERROR: domain->groups does not contain\"\n\t\t\t\t\" CPU%d\\n\", cpu);\n\t}\n\n\tprintk(KERN_DEBUG \"%*s groups:\", level + 1, \"\");\n\tdo {\n\t\tif (!group) {\n\t\t\tprintk(\"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: group is NULL\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!group->cpu_power) {\n\t\t\tprintk(KERN_CONT \"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: domain->cpu_power not \"\n\t\t\t\t\t\"set\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!cpumask_weight(sched_group_cpus(group))) {\n\t\t\tprintk(KERN_CONT \"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: empty group\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (cpumask_intersects(groupmask, sched_group_cpus(group))) {\n\t\t\tprintk(KERN_CONT \"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: repeated CPUs\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tcpumask_or(groupmask, groupmask, sched_group_cpus(group));\n\n\t\tcpulist_scnprintf(str, sizeof(str), sched_group_cpus(group));\n\n\t\tprintk(KERN_CONT \" %s\", str);\n\t\tif (group->cpu_power != SCHED_POWER_SCALE) {\n\t\t\tprintk(KERN_CONT \" (cpu_power = %d)\",\n\t\t\t\tgroup->cpu_power);\n\t\t}\n\n\t\tgroup = group->next;\n\t} while (group != sd->groups);\n\tprintk(KERN_CONT \"\\n\");\n\n\tif (!cpumask_equal(sched_domain_span(sd), groupmask))\n\t\tprintk(KERN_ERR \"ERROR: groups don't span domain->span\\n\");\n\n\tif (sd->parent &&\n\t    !cpumask_subset(groupmask, sched_domain_span(sd->parent)))\n\t\tprintk(KERN_ERR \"ERROR: parent span is not a superset \"\n\t\t\t\"of domain->span\\n\");\n\treturn 0;\n}\n\nstatic void sched_domain_debug(struct sched_domain *sd, int cpu)\n{\n\tint level = 0;\n\n\tif (!sched_domain_debug_enabled)\n\t\treturn;\n\n\tif (!sd) {\n\t\tprintk(KERN_DEBUG \"CPU%d attaching NULL sched-domain.\\n\", cpu);\n\t\treturn;\n\t}\n\n\tprintk(KERN_DEBUG \"CPU%d attaching sched-domain:\\n\", cpu);\n\n\tfor (;;) {\n\t\tif (sched_domain_debug_one(sd, cpu, level, sched_domains_tmpmask))\n\t\t\tbreak;\n\t\tlevel++;\n\t\tsd = sd->parent;\n\t\tif (!sd)\n\t\t\tbreak;\n\t}\n}\n#else /* !CONFIG_SCHED_DEBUG */\n# define sched_domain_debug(sd, cpu) do { } while (0)\n#endif /* CONFIG_SCHED_DEBUG */\n\nstatic int sd_degenerate(struct sched_domain *sd)\n{\n\tif (cpumask_weight(sched_domain_span(sd)) == 1)\n\t\treturn 1;\n\n\t/* Following flags need at least 2 groups */\n\tif (sd->flags & (SD_LOAD_BALANCE |\n\t\t\t SD_BALANCE_NEWIDLE |\n\t\t\t SD_BALANCE_FORK |\n\t\t\t SD_BALANCE_EXEC |\n\t\t\t SD_SHARE_CPUPOWER |\n\t\t\t SD_SHARE_PKG_RESOURCES)) {\n\t\tif (sd->groups != sd->groups->next)\n\t\t\treturn 0;\n\t}\n\n\t/* Following flags don't use groups */\n\tif (sd->flags & (SD_WAKE_AFFINE))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int\nsd_parent_degenerate(struct sched_domain *sd, struct sched_domain *parent)\n{\n\tunsigned long cflags = sd->flags, pflags = parent->flags;\n\n\tif (sd_degenerate(parent))\n\t\treturn 1;\n\n\tif (!cpumask_equal(sched_domain_span(sd), sched_domain_span(parent)))\n\t\treturn 0;\n\n\t/* Flags needing groups don't count if only 1 group in parent */\n\tif (parent->groups == parent->groups->next) {\n\t\tpflags &= ~(SD_LOAD_BALANCE |\n\t\t\t\tSD_BALANCE_NEWIDLE |\n\t\t\t\tSD_BALANCE_FORK |\n\t\t\t\tSD_BALANCE_EXEC |\n\t\t\t\tSD_SHARE_CPUPOWER |\n\t\t\t\tSD_SHARE_PKG_RESOURCES);\n\t\tif (nr_node_ids == 1)\n\t\t\tpflags &= ~SD_SERIALIZE;\n\t}\n\tif (~cflags & pflags)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic void free_rootdomain(struct rcu_head *rcu)\n{\n\tstruct root_domain *rd = container_of(rcu, struct root_domain, rcu);\n\n\tcpupri_cleanup(&rd->cpupri);\n\tfree_cpumask_var(rd->rto_mask);\n\tfree_cpumask_var(rd->online);\n\tfree_cpumask_var(rd->span);\n\tkfree(rd);\n}\n\nstatic void rq_attach_root(struct rq *rq, struct root_domain *rd)\n{\n\tstruct root_domain *old_rd = NULL;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&rq->lock, flags);\n\n\tif (rq->rd) {\n\t\told_rd = rq->rd;\n\n\t\tif (cpumask_test_cpu(rq->cpu, old_rd->online))\n\t\t\tset_rq_offline(rq);\n\n\t\tcpumask_clear_cpu(rq->cpu, old_rd->span);\n\n\t\t/*\n\t\t * If we dont want to free the old_rt yet then\n\t\t * set old_rd to NULL to skip the freeing later\n\t\t * in this function:\n\t\t */\n\t\tif (!atomic_dec_and_test(&old_rd->refcount))\n\t\t\told_rd = NULL;\n\t}\n\n\tatomic_inc(&rd->refcount);\n\trq->rd = rd;\n\n\tcpumask_set_cpu(rq->cpu, rd->span);\n\tif (cpumask_test_cpu(rq->cpu, cpu_active_mask))\n\t\tset_rq_online(rq);\n\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\n\tif (old_rd)\n\t\tcall_rcu_sched(&old_rd->rcu, free_rootdomain);\n}\n\nstatic int init_rootdomain(struct root_domain *rd)\n{\n\tmemset(rd, 0, sizeof(*rd));\n\n\tif (!alloc_cpumask_var(&rd->span, GFP_KERNEL))\n\t\tgoto out;\n\tif (!alloc_cpumask_var(&rd->online, GFP_KERNEL))\n\t\tgoto free_span;\n\tif (!alloc_cpumask_var(&rd->rto_mask, GFP_KERNEL))\n\t\tgoto free_online;\n\n\tif (cpupri_init(&rd->cpupri) != 0)\n\t\tgoto free_rto_mask;\n\treturn 0;\n\nfree_rto_mask:\n\tfree_cpumask_var(rd->rto_mask);\nfree_online:\n\tfree_cpumask_var(rd->online);\nfree_span:\n\tfree_cpumask_var(rd->span);\nout:\n\treturn -ENOMEM;\n}\n\nstatic void init_defrootdomain(void)\n{\n\tinit_rootdomain(&def_root_domain);\n\n\tatomic_set(&def_root_domain.refcount, 1);\n}\n\nstatic struct root_domain *alloc_rootdomain(void)\n{\n\tstruct root_domain *rd;\n\n\trd = kmalloc(sizeof(*rd), GFP_KERNEL);\n\tif (!rd)\n\t\treturn NULL;\n\n\tif (init_rootdomain(rd) != 0) {\n\t\tkfree(rd);\n\t\treturn NULL;\n\t}\n\n\treturn rd;\n}\n\nstatic void free_sched_domain(struct rcu_head *rcu)\n{\n\tstruct sched_domain *sd = container_of(rcu, struct sched_domain, rcu);\n\tif (atomic_dec_and_test(&sd->groups->ref))\n\t\tkfree(sd->groups);\n\tkfree(sd);\n}\n\nstatic void destroy_sched_domain(struct sched_domain *sd, int cpu)\n{\n\tcall_rcu(&sd->rcu, free_sched_domain);\n}\n\nstatic void destroy_sched_domains(struct sched_domain *sd, int cpu)\n{\n\tfor (; sd; sd = sd->parent)\n\t\tdestroy_sched_domain(sd, cpu);\n}\n\n/*\n * Attach the domain 'sd' to 'cpu' as its base domain. Callers must\n * hold the hotplug lock.\n */\nstatic void\ncpu_attach_domain(struct sched_domain *sd, struct root_domain *rd, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct sched_domain *tmp;\n\n\t/* Remove the sched domains which do not contribute to scheduling. */\n\tfor (tmp = sd; tmp; ) {\n\t\tstruct sched_domain *parent = tmp->parent;\n\t\tif (!parent)\n\t\t\tbreak;\n\n\t\tif (sd_parent_degenerate(tmp, parent)) {\n\t\t\ttmp->parent = parent->parent;\n\t\t\tif (parent->parent)\n\t\t\t\tparent->parent->child = tmp;\n\t\t\tdestroy_sched_domain(parent, cpu);\n\t\t} else\n\t\t\ttmp = tmp->parent;\n\t}\n\n\tif (sd && sd_degenerate(sd)) {\n\t\ttmp = sd;\n\t\tsd = sd->parent;\n\t\tdestroy_sched_domain(tmp, cpu);\n\t\tif (sd)\n\t\t\tsd->child = NULL;\n\t}\n\n\tsched_domain_debug(sd, cpu);\n\n\trq_attach_root(rq, rd);\n\ttmp = rq->sd;\n\trcu_assign_pointer(rq->sd, sd);\n\tdestroy_sched_domains(tmp, cpu);\n}\n\n/* cpus with isolated domains */\nstatic cpumask_var_t cpu_isolated_map;\n\n/* Setup the mask of cpus configured for isolated domains */\nstatic int __init isolated_cpu_setup(char *str)\n{\n\talloc_bootmem_cpumask_var(&cpu_isolated_map);\n\tcpulist_parse(str, cpu_isolated_map);\n\treturn 1;\n}\n\n__setup(\"isolcpus=\", isolated_cpu_setup);\n\n#define SD_NODES_PER_DOMAIN 16\n\n#ifdef CONFIG_NUMA\n\n/**\n * find_next_best_node - find the next node to include in a sched_domain\n * @node: node whose sched_domain we're building\n * @used_nodes: nodes already in the sched_domain\n *\n * Find the next node to include in a given scheduling domain. Simply\n * finds the closest node not already in the @used_nodes map.\n *\n * Should use nodemask_t.\n */\nstatic int find_next_best_node(int node, nodemask_t *used_nodes)\n{\n\tint i, n, val, min_val, best_node = -1;\n\n\tmin_val = INT_MAX;\n\n\tfor (i = 0; i < nr_node_ids; i++) {\n\t\t/* Start at @node */\n\t\tn = (node + i) % nr_node_ids;\n\n\t\tif (!nr_cpus_node(n))\n\t\t\tcontinue;\n\n\t\t/* Skip already used nodes */\n\t\tif (node_isset(n, *used_nodes))\n\t\t\tcontinue;\n\n\t\t/* Simple min distance search */\n\t\tval = node_distance(node, n);\n\n\t\tif (val < min_val) {\n\t\t\tmin_val = val;\n\t\t\tbest_node = n;\n\t\t}\n\t}\n\n\tif (best_node != -1)\n\t\tnode_set(best_node, *used_nodes);\n\treturn best_node;\n}\n\n/**\n * sched_domain_node_span - get a cpumask for a node's sched_domain\n * @node: node whose cpumask we're constructing\n * @span: resulting cpumask\n *\n * Given a node, construct a good cpumask for its sched_domain to span. It\n * should be one that prevents unnecessary balancing, but also spreads tasks\n * out optimally.\n */\nstatic void sched_domain_node_span(int node, struct cpumask *span)\n{\n\tnodemask_t used_nodes;\n\tint i;\n\n\tcpumask_clear(span);\n\tnodes_clear(used_nodes);\n\n\tcpumask_or(span, span, cpumask_of_node(node));\n\tnode_set(node, used_nodes);\n\n\tfor (i = 1; i < SD_NODES_PER_DOMAIN; i++) {\n\t\tint next_node = find_next_best_node(node, &used_nodes);\n\t\tif (next_node < 0)\n\t\t\tbreak;\n\t\tcpumask_or(span, span, cpumask_of_node(next_node));\n\t}\n}\n\nstatic const struct cpumask *cpu_node_mask(int cpu)\n{\n\tlockdep_assert_held(&sched_domains_mutex);\n\n\tsched_domain_node_span(cpu_to_node(cpu), sched_domains_tmpmask);\n\n\treturn sched_domains_tmpmask;\n}\n\nstatic const struct cpumask *cpu_allnodes_mask(int cpu)\n{\n\treturn cpu_possible_mask;\n}\n#endif /* CONFIG_NUMA */\n\nstatic const struct cpumask *cpu_cpu_mask(int cpu)\n{\n\treturn cpumask_of_node(cpu_to_node(cpu));\n}\n\nint sched_smt_power_savings = 0, sched_mc_power_savings = 0;\n\nstruct sd_data {\n\tstruct sched_domain **__percpu sd;\n\tstruct sched_group **__percpu sg;\n};\n\nstruct s_data {\n\tstruct sched_domain ** __percpu sd;\n\tstruct root_domain\t*rd;\n};\n\nenum s_alloc {\n\tsa_rootdomain,\n\tsa_sd,\n\tsa_sd_storage,\n\tsa_none,\n};\n\nstruct sched_domain_topology_level;\n\ntypedef struct sched_domain *(*sched_domain_init_f)(struct sched_domain_topology_level *tl, int cpu);\ntypedef const struct cpumask *(*sched_domain_mask_f)(int cpu);\n\nstruct sched_domain_topology_level {\n\tsched_domain_init_f init;\n\tsched_domain_mask_f mask;\n\tstruct sd_data      data;\n};\n\n/*\n * Assumes the sched_domain tree is fully constructed\n */\nstatic int get_group(int cpu, struct sd_data *sdd, struct sched_group **sg)\n{\n\tstruct sched_domain *sd = *per_cpu_ptr(sdd->sd, cpu);\n\tstruct sched_domain *child = sd->child;\n\n\tif (child)\n\t\tcpu = cpumask_first(sched_domain_span(child));\n\n\tif (sg)\n\t\t*sg = *per_cpu_ptr(sdd->sg, cpu);\n\n\treturn cpu;\n}\n\n/*\n * build_sched_groups takes the cpumask we wish to span, and a pointer\n * to a function which identifies what group(along with sched group) a CPU\n * belongs to. The return value of group_fn must be a >= 0 and < nr_cpu_ids\n * (due to the fact that we keep track of groups covered with a struct cpumask).\n *\n * build_sched_groups will build a circular linked list of the groups\n * covered by the given span, and will set each group's ->cpumask correctly,\n * and ->cpu_power to 0.\n */\nstatic void\nbuild_sched_groups(struct sched_domain *sd)\n{\n\tstruct sched_group *first = NULL, *last = NULL;\n\tstruct sd_data *sdd = sd->private;\n\tconst struct cpumask *span = sched_domain_span(sd);\n\tstruct cpumask *covered;\n\tint i;\n\n\tlockdep_assert_held(&sched_domains_mutex);\n\tcovered = sched_domains_tmpmask;\n\n\tcpumask_clear(covered);\n\n\tfor_each_cpu(i, span) {\n\t\tstruct sched_group *sg;\n\t\tint group = get_group(i, sdd, &sg);\n\t\tint j;\n\n\t\tif (cpumask_test_cpu(i, covered))\n\t\t\tcontinue;\n\n\t\tcpumask_clear(sched_group_cpus(sg));\n\t\tsg->cpu_power = 0;\n\n\t\tfor_each_cpu(j, span) {\n\t\t\tif (get_group(j, sdd, NULL) != group)\n\t\t\t\tcontinue;\n\n\t\t\tcpumask_set_cpu(j, covered);\n\t\t\tcpumask_set_cpu(j, sched_group_cpus(sg));\n\t\t}\n\n\t\tif (!first)\n\t\t\tfirst = sg;\n\t\tif (last)\n\t\t\tlast->next = sg;\n\t\tlast = sg;\n\t}\n\tlast->next = first;\n}\n\n/*\n * Initialize sched groups cpu_power.\n *\n * cpu_power indicates the capacity of sched group, which is used while\n * distributing the load between different sched groups in a sched domain.\n * Typically cpu_power for all the groups in a sched domain will be same unless\n * there are asymmetries in the topology. If there are asymmetries, group\n * having more cpu_power will pickup more load compared to the group having\n * less cpu_power.\n */\nstatic void init_sched_groups_power(int cpu, struct sched_domain *sd)\n{\n\tWARN_ON(!sd || !sd->groups);\n\n\tif (cpu != group_first_cpu(sd->groups))\n\t\treturn;\n\n\tsd->groups->group_weight = cpumask_weight(sched_group_cpus(sd->groups));\n\n\tupdate_group_power(sd, cpu);\n}\n\n/*\n * Initializers for schedule domains\n * Non-inlined to reduce accumulated stack pressure in build_sched_domains()\n */\n\n#ifdef CONFIG_SCHED_DEBUG\n# define SD_INIT_NAME(sd, type)\t\tsd->name = #type\n#else\n# define SD_INIT_NAME(sd, type)\t\tdo { } while (0)\n#endif\n\n#define SD_INIT_FUNC(type)\t\t\t\t\t\t\\\nstatic noinline struct sched_domain *\t\t\t\t\t\\\nsd_init_##type(struct sched_domain_topology_level *tl, int cpu) \t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct sched_domain *sd = *per_cpu_ptr(tl->data.sd, cpu);\t\\\n\t*sd = SD_##type##_INIT;\t\t\t\t\t\t\\\n\tSD_INIT_NAME(sd, type);\t\t\t\t\t\t\\\n\tsd->private = &tl->data;\t\t\t\t\t\\\n\treturn sd;\t\t\t\t\t\t\t\\\n}\n\nSD_INIT_FUNC(CPU)\n#ifdef CONFIG_NUMA\n SD_INIT_FUNC(ALLNODES)\n SD_INIT_FUNC(NODE)\n#endif\n#ifdef CONFIG_SCHED_SMT\n SD_INIT_FUNC(SIBLING)\n#endif\n#ifdef CONFIG_SCHED_MC\n SD_INIT_FUNC(MC)\n#endif\n#ifdef CONFIG_SCHED_BOOK\n SD_INIT_FUNC(BOOK)\n#endif\n\nstatic int default_relax_domain_level = -1;\nint sched_domain_level_max;\n\nstatic int __init setup_relax_domain_level(char *str)\n{\n\tunsigned long val;\n\n\tval = simple_strtoul(str, NULL, 0);\n\tif (val < sched_domain_level_max)\n\t\tdefault_relax_domain_level = val;\n\n\treturn 1;\n}\n__setup(\"relax_domain_level=\", setup_relax_domain_level);\n\nstatic void set_domain_attribute(struct sched_domain *sd,\n\t\t\t\t struct sched_domain_attr *attr)\n{\n\tint request;\n\n\tif (!attr || attr->relax_domain_level < 0) {\n\t\tif (default_relax_domain_level < 0)\n\t\t\treturn;\n\t\telse\n\t\t\trequest = default_relax_domain_level;\n\t} else\n\t\trequest = attr->relax_domain_level;\n\tif (request < sd->level) {\n\t\t/* turn off idle balance on this domain */\n\t\tsd->flags &= ~(SD_BALANCE_WAKE|SD_BALANCE_NEWIDLE);\n\t} else {\n\t\t/* turn on idle balance on this domain */\n\t\tsd->flags |= (SD_BALANCE_WAKE|SD_BALANCE_NEWIDLE);\n\t}\n}\n\nstatic void __sdt_free(const struct cpumask *cpu_map);\nstatic int __sdt_alloc(const struct cpumask *cpu_map);\n\nstatic void __free_domain_allocs(struct s_data *d, enum s_alloc what,\n\t\t\t\t const struct cpumask *cpu_map)\n{\n\tswitch (what) {\n\tcase sa_rootdomain:\n\t\tif (!atomic_read(&d->rd->refcount))\n\t\t\tfree_rootdomain(&d->rd->rcu); /* fall through */\n\tcase sa_sd:\n\t\tfree_percpu(d->sd); /* fall through */\n\tcase sa_sd_storage:\n\t\t__sdt_free(cpu_map); /* fall through */\n\tcase sa_none:\n\t\tbreak;\n\t}\n}\n\nstatic enum s_alloc __visit_domain_allocation_hell(struct s_data *d,\n\t\t\t\t\t\t   const struct cpumask *cpu_map)\n{\n\tmemset(d, 0, sizeof(*d));\n\n\tif (__sdt_alloc(cpu_map))\n\t\treturn sa_sd_storage;\n\td->sd = alloc_percpu(struct sched_domain *);\n\tif (!d->sd)\n\t\treturn sa_sd_storage;\n\td->rd = alloc_rootdomain();\n\tif (!d->rd)\n\t\treturn sa_sd;\n\treturn sa_rootdomain;\n}\n\n/*\n * NULL the sd_data elements we've used to build the sched_domain and\n * sched_group structure so that the subsequent __free_domain_allocs()\n * will not free the data we're using.\n */\nstatic void claim_allocations(int cpu, struct sched_domain *sd)\n{\n\tstruct sd_data *sdd = sd->private;\n\tstruct sched_group *sg = sd->groups;\n\n\tWARN_ON_ONCE(*per_cpu_ptr(sdd->sd, cpu) != sd);\n\t*per_cpu_ptr(sdd->sd, cpu) = NULL;\n\n\tif (cpu == cpumask_first(sched_group_cpus(sg))) {\n\t\tWARN_ON_ONCE(*per_cpu_ptr(sdd->sg, cpu) != sg);\n\t\t*per_cpu_ptr(sdd->sg, cpu) = NULL;\n\t}\n}\n\n#ifdef CONFIG_SCHED_SMT\nstatic const struct cpumask *cpu_smt_mask(int cpu)\n{\n\treturn topology_thread_cpumask(cpu);\n}\n#endif\n\n/*\n * Topology list, bottom-up.\n */\nstatic struct sched_domain_topology_level default_topology[] = {\n#ifdef CONFIG_SCHED_SMT\n\t{ sd_init_SIBLING, cpu_smt_mask, },\n#endif\n#ifdef CONFIG_SCHED_MC\n\t{ sd_init_MC, cpu_coregroup_mask, },\n#endif\n#ifdef CONFIG_SCHED_BOOK\n\t{ sd_init_BOOK, cpu_book_mask, },\n#endif\n\t{ sd_init_CPU, cpu_cpu_mask, },\n#ifdef CONFIG_NUMA\n\t{ sd_init_NODE, cpu_node_mask, },\n\t{ sd_init_ALLNODES, cpu_allnodes_mask, },\n#endif\n\t{ NULL, },\n};\n\nstatic struct sched_domain_topology_level *sched_domain_topology = default_topology;\n\nstatic int __sdt_alloc(const struct cpumask *cpu_map)\n{\n\tstruct sched_domain_topology_level *tl;\n\tint j;\n\n\tfor (tl = sched_domain_topology; tl->init; tl++) {\n\t\tstruct sd_data *sdd = &tl->data;\n\n\t\tsdd->sd = alloc_percpu(struct sched_domain *);\n\t\tif (!sdd->sd)\n\t\t\treturn -ENOMEM;\n\n\t\tsdd->sg = alloc_percpu(struct sched_group *);\n\t\tif (!sdd->sg)\n\t\t\treturn -ENOMEM;\n\n\t\tfor_each_cpu(j, cpu_map) {\n\t\t\tstruct sched_domain *sd;\n\t\t\tstruct sched_group *sg;\n\n\t\t       \tsd = kzalloc_node(sizeof(struct sched_domain) + cpumask_size(),\n\t\t\t\t\tGFP_KERNEL, cpu_to_node(j));\n\t\t\tif (!sd)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\t*per_cpu_ptr(sdd->sd, j) = sd;\n\n\t\t\tsg = kzalloc_node(sizeof(struct sched_group) + cpumask_size(),\n\t\t\t\t\tGFP_KERNEL, cpu_to_node(j));\n\t\t\tif (!sg)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\t*per_cpu_ptr(sdd->sg, j) = sg;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void __sdt_free(const struct cpumask *cpu_map)\n{\n\tstruct sched_domain_topology_level *tl;\n\tint j;\n\n\tfor (tl = sched_domain_topology; tl->init; tl++) {\n\t\tstruct sd_data *sdd = &tl->data;\n\n\t\tfor_each_cpu(j, cpu_map) {\n\t\t\tkfree(*per_cpu_ptr(sdd->sd, j));\n\t\t\tkfree(*per_cpu_ptr(sdd->sg, j));\n\t\t}\n\t\tfree_percpu(sdd->sd);\n\t\tfree_percpu(sdd->sg);\n\t}\n}\n\nstruct sched_domain *build_sched_domain(struct sched_domain_topology_level *tl,\n\t\tstruct s_data *d, const struct cpumask *cpu_map,\n\t\tstruct sched_domain_attr *attr, struct sched_domain *child,\n\t\tint cpu)\n{\n\tstruct sched_domain *sd = tl->init(tl, cpu);\n\tif (!sd)\n\t\treturn child;\n\n\tset_domain_attribute(sd, attr);\n\tcpumask_and(sched_domain_span(sd), cpu_map, tl->mask(cpu));\n\tif (child) {\n\t\tsd->level = child->level + 1;\n\t\tsched_domain_level_max = max(sched_domain_level_max, sd->level);\n\t\tchild->parent = sd;\n\t}\n\tsd->child = child;\n\n\treturn sd;\n}\n\n/*\n * Build sched domains for a given set of cpus and attach the sched domains\n * to the individual cpus\n */\nstatic int build_sched_domains(const struct cpumask *cpu_map,\n\t\t\t       struct sched_domain_attr *attr)\n{\n\tenum s_alloc alloc_state = sa_none;\n\tstruct sched_domain *sd;\n\tstruct s_data d;\n\tint i, ret = -ENOMEM;\n\n\talloc_state = __visit_domain_allocation_hell(&d, cpu_map);\n\tif (alloc_state != sa_rootdomain)\n\t\tgoto error;\n\n\t/* Set up domains for cpus specified by the cpu_map. */\n\tfor_each_cpu(i, cpu_map) {\n\t\tstruct sched_domain_topology_level *tl;\n\n\t\tsd = NULL;\n\t\tfor (tl = sched_domain_topology; tl->init; tl++)\n\t\t\tsd = build_sched_domain(tl, &d, cpu_map, attr, sd, i);\n\n\t\twhile (sd->child)\n\t\t\tsd = sd->child;\n\n\t\t*per_cpu_ptr(d.sd, i) = sd;\n\t}\n\n\t/* Build the groups for the domains */\n\tfor_each_cpu(i, cpu_map) {\n\t\tfor (sd = *per_cpu_ptr(d.sd, i); sd; sd = sd->parent) {\n\t\t\tsd->span_weight = cpumask_weight(sched_domain_span(sd));\n\t\t\tget_group(i, sd->private, &sd->groups);\n\t\t\tatomic_inc(&sd->groups->ref);\n\n\t\t\tif (i != cpumask_first(sched_domain_span(sd)))\n\t\t\t\tcontinue;\n\n\t\t\tbuild_sched_groups(sd);\n\t\t}\n\t}\n\n\t/* Calculate CPU power for physical packages and nodes */\n\tfor (i = nr_cpumask_bits-1; i >= 0; i--) {\n\t\tif (!cpumask_test_cpu(i, cpu_map))\n\t\t\tcontinue;\n\n\t\tfor (sd = *per_cpu_ptr(d.sd, i); sd; sd = sd->parent) {\n\t\t\tclaim_allocations(i, sd);\n\t\t\tinit_sched_groups_power(i, sd);\n\t\t}\n\t}\n\n\t/* Attach the domains */\n\trcu_read_lock();\n\tfor_each_cpu(i, cpu_map) {\n\t\tsd = *per_cpu_ptr(d.sd, i);\n\t\tcpu_attach_domain(sd, d.rd, i);\n\t}\n\trcu_read_unlock();\n\n\tret = 0;\nerror:\n\t__free_domain_allocs(&d, alloc_state, cpu_map);\n\treturn ret;\n}\n\nstatic cpumask_var_t *doms_cur;\t/* current sched domains */\nstatic int ndoms_cur;\t\t/* number of sched domains in 'doms_cur' */\nstatic struct sched_domain_attr *dattr_cur;\n\t\t\t\t/* attribues of custom domains in 'doms_cur' */\n\n/*\n * Special case: If a kmalloc of a doms_cur partition (array of\n * cpumask) fails, then fallback to a single sched domain,\n * as determined by the single cpumask fallback_doms.\n */\nstatic cpumask_var_t fallback_doms;\n\n/*\n * arch_update_cpu_topology lets virtualized architectures update the\n * cpu core maps. It is supposed to return 1 if the topology changed\n * or 0 if it stayed the same.\n */\nint __attribute__((weak)) arch_update_cpu_topology(void)\n{\n\treturn 0;\n}\n\ncpumask_var_t *alloc_sched_domains(unsigned int ndoms)\n{\n\tint i;\n\tcpumask_var_t *doms;\n\n\tdoms = kmalloc(sizeof(*doms) * ndoms, GFP_KERNEL);\n\tif (!doms)\n\t\treturn NULL;\n\tfor (i = 0; i < ndoms; i++) {\n\t\tif (!alloc_cpumask_var(&doms[i], GFP_KERNEL)) {\n\t\t\tfree_sched_domains(doms, i);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\treturn doms;\n}\n\nvoid free_sched_domains(cpumask_var_t doms[], unsigned int ndoms)\n{\n\tunsigned int i;\n\tfor (i = 0; i < ndoms; i++)\n\t\tfree_cpumask_var(doms[i]);\n\tkfree(doms);\n}\n\n/*\n * Set up scheduler domains and groups. Callers must hold the hotplug lock.\n * For now this just excludes isolated cpus, but could be used to\n * exclude other special cases in the future.\n */\nstatic int init_sched_domains(const struct cpumask *cpu_map)\n{\n\tint err;\n\n\tarch_update_cpu_topology();\n\tndoms_cur = 1;\n\tdoms_cur = alloc_sched_domains(ndoms_cur);\n\tif (!doms_cur)\n\t\tdoms_cur = &fallback_doms;\n\tcpumask_andnot(doms_cur[0], cpu_map, cpu_isolated_map);\n\tdattr_cur = NULL;\n\terr = build_sched_domains(doms_cur[0], NULL);\n\tregister_sched_domain_sysctl();\n\n\treturn err;\n}\n\n/*\n * Detach sched domains from a group of cpus specified in cpu_map\n * These cpus will now be attached to the NULL domain\n */\nstatic void detach_destroy_domains(const struct cpumask *cpu_map)\n{\n\tint i;\n\n\trcu_read_lock();\n\tfor_each_cpu(i, cpu_map)\n\t\tcpu_attach_domain(NULL, &def_root_domain, i);\n\trcu_read_unlock();\n}\n\n/* handle null as \"default\" */\nstatic int dattrs_equal(struct sched_domain_attr *cur, int idx_cur,\n\t\t\tstruct sched_domain_attr *new, int idx_new)\n{\n\tstruct sched_domain_attr tmp;\n\n\t/* fast path */\n\tif (!new && !cur)\n\t\treturn 1;\n\n\ttmp = SD_ATTR_INIT;\n\treturn !memcmp(cur ? (cur + idx_cur) : &tmp,\n\t\t\tnew ? (new + idx_new) : &tmp,\n\t\t\tsizeof(struct sched_domain_attr));\n}\n\n/*\n * Partition sched domains as specified by the 'ndoms_new'\n * cpumasks in the array doms_new[] of cpumasks. This compares\n * doms_new[] to the current sched domain partitioning, doms_cur[].\n * It destroys each deleted domain and builds each new domain.\n *\n * 'doms_new' is an array of cpumask_var_t's of length 'ndoms_new'.\n * The masks don't intersect (don't overlap.) We should setup one\n * sched domain for each mask. CPUs not in any of the cpumasks will\n * not be load balanced. If the same cpumask appears both in the\n * current 'doms_cur' domains and in the new 'doms_new', we can leave\n * it as it is.\n *\n * The passed in 'doms_new' should be allocated using\n * alloc_sched_domains.  This routine takes ownership of it and will\n * free_sched_domains it when done with it. If the caller failed the\n * alloc call, then it can pass in doms_new == NULL && ndoms_new == 1,\n * and partition_sched_domains() will fallback to the single partition\n * 'fallback_doms', it also forces the domains to be rebuilt.\n *\n * If doms_new == NULL it will be replaced with cpu_online_mask.\n * ndoms_new == 0 is a special case for destroying existing domains,\n * and it will not create the default domain.\n *\n * Call with hotplug lock held\n */\nvoid partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],\n\t\t\t     struct sched_domain_attr *dattr_new)\n{\n\tint i, j, n;\n\tint new_topology;\n\n\tmutex_lock(&sched_domains_mutex);\n\n\t/* always unregister in case we don't destroy any domains */\n\tunregister_sched_domain_sysctl();\n\n\t/* Let architecture update cpu core mappings. */\n\tnew_topology = arch_update_cpu_topology();\n\n\tn = doms_new ? ndoms_new : 0;\n\n\t/* Destroy deleted domains */\n\tfor (i = 0; i < ndoms_cur; i++) {\n\t\tfor (j = 0; j < n && !new_topology; j++) {\n\t\t\tif (cpumask_equal(doms_cur[i], doms_new[j])\n\t\t\t    && dattrs_equal(dattr_cur, i, dattr_new, j))\n\t\t\t\tgoto match1;\n\t\t}\n\t\t/* no match - a current sched domain not in new doms_new[] */\n\t\tdetach_destroy_domains(doms_cur[i]);\nmatch1:\n\t\t;\n\t}\n\n\tif (doms_new == NULL) {\n\t\tndoms_cur = 0;\n\t\tdoms_new = &fallback_doms;\n\t\tcpumask_andnot(doms_new[0], cpu_active_mask, cpu_isolated_map);\n\t\tWARN_ON_ONCE(dattr_new);\n\t}\n\n\t/* Build new domains */\n\tfor (i = 0; i < ndoms_new; i++) {\n\t\tfor (j = 0; j < ndoms_cur && !new_topology; j++) {\n\t\t\tif (cpumask_equal(doms_new[i], doms_cur[j])\n\t\t\t    && dattrs_equal(dattr_new, i, dattr_cur, j))\n\t\t\t\tgoto match2;\n\t\t}\n\t\t/* no match - add a new doms_new */\n\t\tbuild_sched_domains(doms_new[i], dattr_new ? dattr_new + i : NULL);\nmatch2:\n\t\t;\n\t}\n\n\t/* Remember the new sched domains */\n\tif (doms_cur != &fallback_doms)\n\t\tfree_sched_domains(doms_cur, ndoms_cur);\n\tkfree(dattr_cur);\t/* kfree(NULL) is safe */\n\tdoms_cur = doms_new;\n\tdattr_cur = dattr_new;\n\tndoms_cur = ndoms_new;\n\n\tregister_sched_domain_sysctl();\n\n\tmutex_unlock(&sched_domains_mutex);\n}\n\n#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)\nstatic void reinit_sched_domains(void)\n{\n\tget_online_cpus();\n\n\t/* Destroy domains first to force the rebuild */\n\tpartition_sched_domains(0, NULL, NULL);\n\n\trebuild_sched_domains();\n\tput_online_cpus();\n}\n\nstatic ssize_t sched_power_savings_store(const char *buf, size_t count, int smt)\n{\n\tunsigned int level = 0;\n\n\tif (sscanf(buf, \"%u\", &level) != 1)\n\t\treturn -EINVAL;\n\n\t/*\n\t * level is always be positive so don't check for\n\t * level < POWERSAVINGS_BALANCE_NONE which is 0\n\t * What happens on 0 or 1 byte write,\n\t * need to check for count as well?\n\t */\n\n\tif (level >= MAX_POWERSAVINGS_BALANCE_LEVELS)\n\t\treturn -EINVAL;\n\n\tif (smt)\n\t\tsched_smt_power_savings = level;\n\telse\n\t\tsched_mc_power_savings = level;\n\n\treinit_sched_domains();\n\n\treturn count;\n}\n\n#ifdef CONFIG_SCHED_MC\nstatic ssize_t sched_mc_power_savings_show(struct sysdev_class *class,\n\t\t\t\t\t   struct sysdev_class_attribute *attr,\n\t\t\t\t\t   char *page)\n{\n\treturn sprintf(page, \"%u\\n\", sched_mc_power_savings);\n}\nstatic ssize_t sched_mc_power_savings_store(struct sysdev_class *class,\n\t\t\t\t\t    struct sysdev_class_attribute *attr,\n\t\t\t\t\t    const char *buf, size_t count)\n{\n\treturn sched_power_savings_store(buf, count, 0);\n}\nstatic SYSDEV_CLASS_ATTR(sched_mc_power_savings, 0644,\n\t\t\t sched_mc_power_savings_show,\n\t\t\t sched_mc_power_savings_store);\n#endif\n\n#ifdef CONFIG_SCHED_SMT\nstatic ssize_t sched_smt_power_savings_show(struct sysdev_class *dev,\n\t\t\t\t\t    struct sysdev_class_attribute *attr,\n\t\t\t\t\t    char *page)\n{\n\treturn sprintf(page, \"%u\\n\", sched_smt_power_savings);\n}\nstatic ssize_t sched_smt_power_savings_store(struct sysdev_class *dev,\n\t\t\t\t\t     struct sysdev_class_attribute *attr,\n\t\t\t\t\t     const char *buf, size_t count)\n{\n\treturn sched_power_savings_store(buf, count, 1);\n}\nstatic SYSDEV_CLASS_ATTR(sched_smt_power_savings, 0644,\n\t\t   sched_smt_power_savings_show,\n\t\t   sched_smt_power_savings_store);\n#endif\n\nint __init sched_create_sysfs_power_savings_entries(struct sysdev_class *cls)\n{\n\tint err = 0;\n\n#ifdef CONFIG_SCHED_SMT\n\tif (smt_capable())\n\t\terr = sysfs_create_file(&cls->kset.kobj,\n\t\t\t\t\t&attr_sched_smt_power_savings.attr);\n#endif\n#ifdef CONFIG_SCHED_MC\n\tif (!err && mc_capable())\n\t\terr = sysfs_create_file(&cls->kset.kobj,\n\t\t\t\t\t&attr_sched_mc_power_savings.attr);\n#endif\n\treturn err;\n}\n#endif /* CONFIG_SCHED_MC || CONFIG_SCHED_SMT */\n\n/*\n * Update cpusets according to cpu_active mask.  If cpusets are\n * disabled, cpuset_update_active_cpus() becomes a simple wrapper\n * around partition_sched_domains().\n */\nstatic int cpuset_cpu_active(struct notifier_block *nfb, unsigned long action,\n\t\t\t     void *hcpu)\n{\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_ONLINE:\n\tcase CPU_DOWN_FAILED:\n\t\tcpuset_update_active_cpus();\n\t\treturn NOTIFY_OK;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nstatic int cpuset_cpu_inactive(struct notifier_block *nfb, unsigned long action,\n\t\t\t       void *hcpu)\n{\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_DOWN_PREPARE:\n\t\tcpuset_update_active_cpus();\n\t\treturn NOTIFY_OK;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nstatic int update_runtime(struct notifier_block *nfb,\n\t\t\t\tunsigned long action, void *hcpu)\n{\n\tint cpu = (int)(long)hcpu;\n\n\tswitch (action) {\n\tcase CPU_DOWN_PREPARE:\n\tcase CPU_DOWN_PREPARE_FROZEN:\n\t\tdisable_runtime(cpu_rq(cpu));\n\t\treturn NOTIFY_OK;\n\n\tcase CPU_DOWN_FAILED:\n\tcase CPU_DOWN_FAILED_FROZEN:\n\tcase CPU_ONLINE:\n\tcase CPU_ONLINE_FROZEN:\n\t\tenable_runtime(cpu_rq(cpu));\n\t\treturn NOTIFY_OK;\n\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nvoid __init sched_init_smp(void)\n{\n\tcpumask_var_t non_isolated_cpus;\n\n\talloc_cpumask_var(&non_isolated_cpus, GFP_KERNEL);\n\talloc_cpumask_var(&fallback_doms, GFP_KERNEL);\n\n\tget_online_cpus();\n\tmutex_lock(&sched_domains_mutex);\n\tinit_sched_domains(cpu_active_mask);\n\tcpumask_andnot(non_isolated_cpus, cpu_possible_mask, cpu_isolated_map);\n\tif (cpumask_empty(non_isolated_cpus))\n\t\tcpumask_set_cpu(smp_processor_id(), non_isolated_cpus);\n\tmutex_unlock(&sched_domains_mutex);\n\tput_online_cpus();\n\n\thotcpu_notifier(cpuset_cpu_active, CPU_PRI_CPUSET_ACTIVE);\n\thotcpu_notifier(cpuset_cpu_inactive, CPU_PRI_CPUSET_INACTIVE);\n\n\t/* RT runtime code needs to handle some hotplug events */\n\thotcpu_notifier(update_runtime, 0);\n\n\tinit_hrtick();\n\n\t/* Move init over to a non-isolated CPU */\n\tif (set_cpus_allowed_ptr(current, non_isolated_cpus) < 0)\n\t\tBUG();\n\tsched_init_granularity();\n\tfree_cpumask_var(non_isolated_cpus);\n\n\tinit_sched_rt_class();\n}\n#else\nvoid __init sched_init_smp(void)\n{\n\tsched_init_granularity();\n}\n#endif /* CONFIG_SMP */\n\nconst_debug unsigned int sysctl_timer_migration = 1;\n\nint in_sched_functions(unsigned long addr)\n{\n\treturn in_lock_functions(addr) ||\n\t\t(addr >= (unsigned long)__sched_text_start\n\t\t&& addr < (unsigned long)__sched_text_end);\n}\n\nstatic void init_cfs_rq(struct cfs_rq *cfs_rq, struct rq *rq)\n{\n\tcfs_rq->tasks_timeline = RB_ROOT;\n\tINIT_LIST_HEAD(&cfs_rq->tasks);\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tcfs_rq->rq = rq;\n\t/* allow initial update_cfs_load() to truncate */\n#ifdef CONFIG_SMP\n\tcfs_rq->load_stamp = 1;\n#endif\n#endif\n\tcfs_rq->min_vruntime = (u64)(-(1LL << 20));\n}\n\nstatic void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq)\n{\n\tstruct rt_prio_array *array;\n\tint i;\n\n\tarray = &rt_rq->active;\n\tfor (i = 0; i < MAX_RT_PRIO; i++) {\n\t\tINIT_LIST_HEAD(array->queue + i);\n\t\t__clear_bit(i, array->bitmap);\n\t}\n\t/* delimiter for bitsearch: */\n\t__set_bit(MAX_RT_PRIO, array->bitmap);\n\n#if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED\n\trt_rq->highest_prio.curr = MAX_RT_PRIO;\n#ifdef CONFIG_SMP\n\trt_rq->highest_prio.next = MAX_RT_PRIO;\n#endif\n#endif\n#ifdef CONFIG_SMP\n\trt_rq->rt_nr_migratory = 0;\n\trt_rq->overloaded = 0;\n\tplist_head_init_raw(&rt_rq->pushable_tasks, &rq->lock);\n#endif\n\n\trt_rq->rt_time = 0;\n\trt_rq->rt_throttled = 0;\n\trt_rq->rt_runtime = 0;\n\traw_spin_lock_init(&rt_rq->rt_runtime_lock);\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\trt_rq->rt_nr_boosted = 0;\n\trt_rq->rq = rq;\n#endif\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq,\n\t\t\t\tstruct sched_entity *se, int cpu,\n\t\t\t\tstruct sched_entity *parent)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\ttg->cfs_rq[cpu] = cfs_rq;\n\tinit_cfs_rq(cfs_rq, rq);\n\tcfs_rq->tg = tg;\n\n\ttg->se[cpu] = se;\n\t/* se could be NULL for root_task_group */\n\tif (!se)\n\t\treturn;\n\n\tif (!parent)\n\t\tse->cfs_rq = &rq->cfs;\n\telse\n\t\tse->cfs_rq = parent->my_q;\n\n\tse->my_q = cfs_rq;\n\tupdate_load_set(&se->load, 0);\n\tse->parent = parent;\n}\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic void init_tg_rt_entry(struct task_group *tg, struct rt_rq *rt_rq,\n\t\tstruct sched_rt_entity *rt_se, int cpu,\n\t\tstruct sched_rt_entity *parent)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\ttg->rt_rq[cpu] = rt_rq;\n\tinit_rt_rq(rt_rq, rq);\n\trt_rq->tg = tg;\n\trt_rq->rt_runtime = tg->rt_bandwidth.rt_runtime;\n\n\ttg->rt_se[cpu] = rt_se;\n\tif (!rt_se)\n\t\treturn;\n\n\tif (!parent)\n\t\trt_se->rt_rq = &rq->rt;\n\telse\n\t\trt_se->rt_rq = parent->my_q;\n\n\trt_se->my_q = rt_rq;\n\trt_se->parent = parent;\n\tINIT_LIST_HEAD(&rt_se->run_list);\n}\n#endif\n\nvoid __init sched_init(void)\n{\n\tint i, j;\n\tunsigned long alloc_size = 0, ptr;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\talloc_size += 2 * nr_cpu_ids * sizeof(void **);\n#endif\n#ifdef CONFIG_RT_GROUP_SCHED\n\talloc_size += 2 * nr_cpu_ids * sizeof(void **);\n#endif\n#ifdef CONFIG_CPUMASK_OFFSTACK\n\talloc_size += num_possible_cpus() * cpumask_size();\n#endif\n\tif (alloc_size) {\n\t\tptr = (unsigned long)kzalloc(alloc_size, GFP_NOWAIT);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t\troot_task_group.se = (struct sched_entity **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n\t\troot_task_group.cfs_rq = (struct cfs_rq **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n#ifdef CONFIG_RT_GROUP_SCHED\n\t\troot_task_group.rt_se = (struct sched_rt_entity **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n\t\troot_task_group.rt_rq = (struct rt_rq **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n#endif /* CONFIG_RT_GROUP_SCHED */\n#ifdef CONFIG_CPUMASK_OFFSTACK\n\t\tfor_each_possible_cpu(i) {\n\t\t\tper_cpu(load_balance_tmpmask, i) = (void *)ptr;\n\t\t\tptr += cpumask_size();\n\t\t}\n#endif /* CONFIG_CPUMASK_OFFSTACK */\n\t}\n\n#ifdef CONFIG_SMP\n\tinit_defrootdomain();\n#endif\n\n\tinit_rt_bandwidth(&def_rt_bandwidth,\n\t\t\tglobal_rt_period(), global_rt_runtime());\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tinit_rt_bandwidth(&root_task_group.rt_bandwidth,\n\t\t\tglobal_rt_period(), global_rt_runtime());\n#endif /* CONFIG_RT_GROUP_SCHED */\n\n#ifdef CONFIG_CGROUP_SCHED\n\tlist_add(&root_task_group.list, &task_groups);\n\tINIT_LIST_HEAD(&root_task_group.children);\n\tautogroup_init(&init_task);\n#endif /* CONFIG_CGROUP_SCHED */\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct rq *rq;\n\n\t\trq = cpu_rq(i);\n\t\traw_spin_lock_init(&rq->lock);\n\t\trq->nr_running = 0;\n\t\trq->calc_load_active = 0;\n\t\trq->calc_load_update = jiffies + LOAD_FREQ;\n\t\tinit_cfs_rq(&rq->cfs, rq);\n\t\tinit_rt_rq(&rq->rt, rq);\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t\troot_task_group.shares = root_task_group_load;\n\t\tINIT_LIST_HEAD(&rq->leaf_cfs_rq_list);\n\t\t/*\n\t\t * How much cpu bandwidth does root_task_group get?\n\t\t *\n\t\t * In case of task-groups formed thr' the cgroup filesystem, it\n\t\t * gets 100% of the cpu resources in the system. This overall\n\t\t * system cpu resource is divided among the tasks of\n\t\t * root_task_group and its child task-groups in a fair manner,\n\t\t * based on each entity's (task or task-group's) weight\n\t\t * (se->load.weight).\n\t\t *\n\t\t * In other words, if root_task_group has 10 tasks of weight\n\t\t * 1024) and two child groups A0 and A1 (of weight 1024 each),\n\t\t * then A0's share of the cpu resource is:\n\t\t *\n\t\t *\tA0's bandwidth = 1024 / (10*1024 + 1024 + 1024) = 8.33%\n\t\t *\n\t\t * We achieve this by letting root_task_group's tasks sit\n\t\t * directly in rq->cfs (i.e root_task_group->se[] = NULL).\n\t\t */\n\t\tinit_tg_cfs_entry(&root_task_group, &rq->cfs, NULL, i, NULL);\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n\t\trq->rt.rt_runtime = def_rt_bandwidth.rt_runtime;\n#ifdef CONFIG_RT_GROUP_SCHED\n\t\tINIT_LIST_HEAD(&rq->leaf_rt_rq_list);\n\t\tinit_tg_rt_entry(&root_task_group, &rq->rt, NULL, i, NULL);\n#endif\n\n\t\tfor (j = 0; j < CPU_LOAD_IDX_MAX; j++)\n\t\t\trq->cpu_load[j] = 0;\n\n\t\trq->last_load_update_tick = jiffies;\n\n#ifdef CONFIG_SMP\n\t\trq->sd = NULL;\n\t\trq->rd = NULL;\n\t\trq->cpu_power = SCHED_POWER_SCALE;\n\t\trq->post_schedule = 0;\n\t\trq->active_balance = 0;\n\t\trq->next_balance = jiffies;\n\t\trq->push_cpu = 0;\n\t\trq->cpu = i;\n\t\trq->online = 0;\n\t\trq->idle_stamp = 0;\n\t\trq->avg_idle = 2*sysctl_sched_migration_cost;\n\t\trq_attach_root(rq, &def_root_domain);\n#ifdef CONFIG_NO_HZ\n\t\trq->nohz_balance_kick = 0;\n\t\tinit_sched_softirq_csd(&per_cpu(remote_sched_softirq_cb, i));\n#endif\n#endif\n\t\tinit_rq_hrtick(rq);\n\t\tatomic_set(&rq->nr_iowait, 0);\n\t}\n\n\tset_load_weight(&init_task);\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\tINIT_HLIST_HEAD(&init_task.preempt_notifiers);\n#endif\n\n#ifdef CONFIG_SMP\n\topen_softirq(SCHED_SOFTIRQ, run_rebalance_domains);\n#endif\n\n#ifdef CONFIG_RT_MUTEXES\n\tplist_head_init_raw(&init_task.pi_waiters, &init_task.pi_lock);\n#endif\n\n\t/*\n\t * The boot idle thread does lazy MMU switching as well:\n\t */\n\tatomic_inc(&init_mm.mm_count);\n\tenter_lazy_tlb(&init_mm, current);\n\n\t/*\n\t * Make us the idle thread. Technically, schedule() should not be\n\t * called from this thread, however somewhere below it might be,\n\t * but because we are the idle thread, we just pick up running again\n\t * when this runqueue becomes \"idle\".\n\t */\n\tinit_idle(current, smp_processor_id());\n\n\tcalc_load_update = jiffies + LOAD_FREQ;\n\n\t/*\n\t * During early bootup we pretend to be a normal task:\n\t */\n\tcurrent->sched_class = &fair_sched_class;\n\n\t/* Allocate the nohz_cpu_mask if CONFIG_CPUMASK_OFFSTACK */\n\tzalloc_cpumask_var(&nohz_cpu_mask, GFP_NOWAIT);\n#ifdef CONFIG_SMP\n\tzalloc_cpumask_var(&sched_domains_tmpmask, GFP_NOWAIT);\n#ifdef CONFIG_NO_HZ\n\tzalloc_cpumask_var(&nohz.idle_cpus_mask, GFP_NOWAIT);\n\talloc_cpumask_var(&nohz.grp_idle_mask, GFP_NOWAIT);\n\tatomic_set(&nohz.load_balancer, nr_cpu_ids);\n\tatomic_set(&nohz.first_pick_cpu, nr_cpu_ids);\n\tatomic_set(&nohz.second_pick_cpu, nr_cpu_ids);\n#endif\n\t/* May be allocated at isolcpus cmdline parse time */\n\tif (cpu_isolated_map == NULL)\n\t\tzalloc_cpumask_var(&cpu_isolated_map, GFP_NOWAIT);\n#endif /* SMP */\n\n\tscheduler_running = 1;\n}\n\n#ifdef CONFIG_DEBUG_SPINLOCK_SLEEP\nstatic inline int preempt_count_equals(int preempt_offset)\n{\n\tint nested = (preempt_count() & ~PREEMPT_ACTIVE) + rcu_preempt_depth();\n\n\treturn (nested == preempt_offset);\n}\n\nvoid __might_sleep(const char *file, int line, int preempt_offset)\n{\n#ifdef in_atomic\n\tstatic unsigned long prev_jiffy;\t/* ratelimiting */\n\n\tif ((preempt_count_equals(preempt_offset) && !irqs_disabled()) ||\n\t    system_state != SYSTEM_RUNNING || oops_in_progress)\n\t\treturn;\n\tif (time_before(jiffies, prev_jiffy + HZ) && prev_jiffy)\n\t\treturn;\n\tprev_jiffy = jiffies;\n\n\tprintk(KERN_ERR\n\t\t\"BUG: sleeping function called from invalid context at %s:%d\\n\",\n\t\t\tfile, line);\n\tprintk(KERN_ERR\n\t\t\"in_atomic(): %d, irqs_disabled(): %d, pid: %d, name: %s\\n\",\n\t\t\tin_atomic(), irqs_disabled(),\n\t\t\tcurrent->pid, current->comm);\n\n\tdebug_show_held_locks(current);\n\tif (irqs_disabled())\n\t\tprint_irqtrace_events(current);\n\tdump_stack();\n#endif\n}\nEXPORT_SYMBOL(__might_sleep);\n#endif\n\n#ifdef CONFIG_MAGIC_SYSRQ\nstatic void normalize_task(struct rq *rq, struct task_struct *p)\n{\n\tconst struct sched_class *prev_class = p->sched_class;\n\tint old_prio = p->prio;\n\tint on_rq;\n\n\ton_rq = p->on_rq;\n\tif (on_rq)\n\t\tdeactivate_task(rq, p, 0);\n\t__setscheduler(rq, p, SCHED_NORMAL, 0);\n\tif (on_rq) {\n\t\tactivate_task(rq, p, 0);\n\t\tresched_task(rq->curr);\n\t}\n\n\tcheck_class_changed(rq, p, prev_class, old_prio);\n}\n\nvoid normalize_rt_tasks(void)\n{\n\tstruct task_struct *g, *p;\n\tunsigned long flags;\n\tstruct rq *rq;\n\n\tread_lock_irqsave(&tasklist_lock, flags);\n\tdo_each_thread(g, p) {\n\t\t/*\n\t\t * Only normalize user tasks:\n\t\t */\n\t\tif (!p->mm)\n\t\t\tcontinue;\n\n\t\tp->se.exec_start\t\t= 0;\n#ifdef CONFIG_SCHEDSTATS\n\t\tp->se.statistics.wait_start\t= 0;\n\t\tp->se.statistics.sleep_start\t= 0;\n\t\tp->se.statistics.block_start\t= 0;\n#endif\n\n\t\tif (!rt_task(p)) {\n\t\t\t/*\n\t\t\t * Renice negative nice level userspace\n\t\t\t * tasks back to 0:\n\t\t\t */\n\t\t\tif (TASK_NICE(p) < 0 && p->mm)\n\t\t\t\tset_user_nice(p, 0);\n\t\t\tcontinue;\n\t\t}\n\n\t\traw_spin_lock(&p->pi_lock);\n\t\trq = __task_rq_lock(p);\n\n\t\tnormalize_task(rq, p);\n\n\t\t__task_rq_unlock(rq);\n\t\traw_spin_unlock(&p->pi_lock);\n\t} while_each_thread(g, p);\n\n\tread_unlock_irqrestore(&tasklist_lock, flags);\n}\n\n#endif /* CONFIG_MAGIC_SYSRQ */\n\n#if defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB)\n/*\n * These functions are only useful for the IA64 MCA handling, or kdb.\n *\n * They can only be called when the whole system has been\n * stopped - every CPU needs to be quiescent, and no scheduling\n * activity can take place. Using them for anything else would\n * be a serious bug, and as a result, they aren't even visible\n * under any other configuration.\n */\n\n/**\n * curr_task - return the current task for a given cpu.\n * @cpu: the processor in question.\n *\n * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!\n */\nstruct task_struct *curr_task(int cpu)\n{\n\treturn cpu_curr(cpu);\n}\n\n#endif /* defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB) */\n\n#ifdef CONFIG_IA64\n/**\n * set_curr_task - set the current task for a given cpu.\n * @cpu: the processor in question.\n * @p: the task pointer to set.\n *\n * Description: This function must only be used when non-maskable interrupts\n * are serviced on a separate stack. It allows the architecture to switch the\n * notion of the current task on a cpu in a non-blocking manner. This function\n * must be called with all CPU's synchronized, and interrupts disabled, the\n * and caller must save the original value of the current task (see\n * curr_task() above) and restore that value before reenabling interrupts and\n * re-starting the system.\n *\n * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!\n */\nvoid set_curr_task(int cpu, struct task_struct *p)\n{\n\tcpu_curr(cpu) = p;\n}\n\n#endif\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void free_fair_sched_group(struct task_group *tg)\n{\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tif (tg->cfs_rq)\n\t\t\tkfree(tg->cfs_rq[i]);\n\t\tif (tg->se)\n\t\t\tkfree(tg->se[i]);\n\t}\n\n\tkfree(tg->cfs_rq);\n\tkfree(tg->se);\n}\n\nstatic\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se;\n\tint i;\n\n\ttg->cfs_rq = kzalloc(sizeof(cfs_rq) * nr_cpu_ids, GFP_KERNEL);\n\tif (!tg->cfs_rq)\n\t\tgoto err;\n\ttg->se = kzalloc(sizeof(se) * nr_cpu_ids, GFP_KERNEL);\n\tif (!tg->se)\n\t\tgoto err;\n\n\ttg->shares = NICE_0_LOAD;\n\n\tfor_each_possible_cpu(i) {\n\t\tcfs_rq = kzalloc_node(sizeof(struct cfs_rq),\n\t\t\t\t      GFP_KERNEL, cpu_to_node(i));\n\t\tif (!cfs_rq)\n\t\t\tgoto err;\n\n\t\tse = kzalloc_node(sizeof(struct sched_entity),\n\t\t\t\t  GFP_KERNEL, cpu_to_node(i));\n\t\tif (!se)\n\t\t\tgoto err_free_rq;\n\n\t\tinit_tg_cfs_entry(tg, cfs_rq, se, i, parent->se[i]);\n\t}\n\n\treturn 1;\n\nerr_free_rq:\n\tkfree(cfs_rq);\nerr:\n\treturn 0;\n}\n\nstatic inline void unregister_fair_sched_group(struct task_group *tg, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long flags;\n\n\t/*\n\t* Only empty task groups can be destroyed; so we can speculatively\n\t* check on_list without danger of it being re-added.\n\t*/\n\tif (!tg->cfs_rq[cpu]->on_list)\n\t\treturn;\n\n\traw_spin_lock_irqsave(&rq->lock, flags);\n\tlist_del_leaf_cfs_rq(tg->cfs_rq[cpu]);\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n}\n#else /* !CONFG_FAIR_GROUP_SCHED */\nstatic inline void free_fair_sched_group(struct task_group *tg)\n{\n}\n\nstatic inline\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\treturn 1;\n}\n\nstatic inline void unregister_fair_sched_group(struct task_group *tg, int cpu)\n{\n}\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic void free_rt_sched_group(struct task_group *tg)\n{\n\tint i;\n\n\tdestroy_rt_bandwidth(&tg->rt_bandwidth);\n\n\tfor_each_possible_cpu(i) {\n\t\tif (tg->rt_rq)\n\t\t\tkfree(tg->rt_rq[i]);\n\t\tif (tg->rt_se)\n\t\t\tkfree(tg->rt_se[i]);\n\t}\n\n\tkfree(tg->rt_rq);\n\tkfree(tg->rt_se);\n}\n\nstatic\nint alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\tstruct rt_rq *rt_rq;\n\tstruct sched_rt_entity *rt_se;\n\tint i;\n\n\ttg->rt_rq = kzalloc(sizeof(rt_rq) * nr_cpu_ids, GFP_KERNEL);\n\tif (!tg->rt_rq)\n\t\tgoto err;\n\ttg->rt_se = kzalloc(sizeof(rt_se) * nr_cpu_ids, GFP_KERNEL);\n\tif (!tg->rt_se)\n\t\tgoto err;\n\n\tinit_rt_bandwidth(&tg->rt_bandwidth,\n\t\t\tktime_to_ns(def_rt_bandwidth.rt_period), 0);\n\n\tfor_each_possible_cpu(i) {\n\t\trt_rq = kzalloc_node(sizeof(struct rt_rq),\n\t\t\t\t     GFP_KERNEL, cpu_to_node(i));\n\t\tif (!rt_rq)\n\t\t\tgoto err;\n\n\t\trt_se = kzalloc_node(sizeof(struct sched_rt_entity),\n\t\t\t\t     GFP_KERNEL, cpu_to_node(i));\n\t\tif (!rt_se)\n\t\t\tgoto err_free_rq;\n\n\t\tinit_tg_rt_entry(tg, rt_rq, rt_se, i, parent->rt_se[i]);\n\t}\n\n\treturn 1;\n\nerr_free_rq:\n\tkfree(rt_rq);\nerr:\n\treturn 0;\n}\n#else /* !CONFIG_RT_GROUP_SCHED */\nstatic inline void free_rt_sched_group(struct task_group *tg)\n{\n}\n\nstatic inline\nint alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\treturn 1;\n}\n#endif /* CONFIG_RT_GROUP_SCHED */\n\n#ifdef CONFIG_CGROUP_SCHED\nstatic void free_sched_group(struct task_group *tg)\n{\n\tfree_fair_sched_group(tg);\n\tfree_rt_sched_group(tg);\n\tautogroup_free(tg);\n\tkfree(tg);\n}\n\n/* allocate runqueue etc for a new task group */\nstruct task_group *sched_create_group(struct task_group *parent)\n{\n\tstruct task_group *tg;\n\tunsigned long flags;\n\n\ttg = kzalloc(sizeof(*tg), GFP_KERNEL);\n\tif (!tg)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (!alloc_fair_sched_group(tg, parent))\n\t\tgoto err;\n\n\tif (!alloc_rt_sched_group(tg, parent))\n\t\tgoto err;\n\n\tspin_lock_irqsave(&task_group_lock, flags);\n\tlist_add_rcu(&tg->list, &task_groups);\n\n\tWARN_ON(!parent); /* root should already exist */\n\n\ttg->parent = parent;\n\tINIT_LIST_HEAD(&tg->children);\n\tlist_add_rcu(&tg->siblings, &parent->children);\n\tspin_unlock_irqrestore(&task_group_lock, flags);\n\n\treturn tg;\n\nerr:\n\tfree_sched_group(tg);\n\treturn ERR_PTR(-ENOMEM);\n}\n\n/* rcu callback to free various structures associated with a task group */\nstatic void free_sched_group_rcu(struct rcu_head *rhp)\n{\n\t/* now it should be safe to free those cfs_rqs */\n\tfree_sched_group(container_of(rhp, struct task_group, rcu));\n}\n\n/* Destroy runqueue etc associated with a task group */\nvoid sched_destroy_group(struct task_group *tg)\n{\n\tunsigned long flags;\n\tint i;\n\n\t/* end participation in shares distribution */\n\tfor_each_possible_cpu(i)\n\t\tunregister_fair_sched_group(tg, i);\n\n\tspin_lock_irqsave(&task_group_lock, flags);\n\tlist_del_rcu(&tg->list);\n\tlist_del_rcu(&tg->siblings);\n\tspin_unlock_irqrestore(&task_group_lock, flags);\n\n\t/* wait for possible concurrent references to cfs_rqs complete */\n\tcall_rcu(&tg->rcu, free_sched_group_rcu);\n}\n\n/* change task's runqueue when it moves between groups.\n *\tThe caller of this function should have put the task in its new group\n *\tby now. This function just updates tsk->se.cfs_rq and tsk->se.parent to\n *\treflect its new group.\n */\nvoid sched_move_task(struct task_struct *tsk)\n{\n\tint on_rq, running;\n\tunsigned long flags;\n\tstruct rq *rq;\n\n\trq = task_rq_lock(tsk, &flags);\n\n\trunning = task_current(rq, tsk);\n\ton_rq = tsk->on_rq;\n\n\tif (on_rq)\n\t\tdequeue_task(rq, tsk, 0);\n\tif (unlikely(running))\n\t\ttsk->sched_class->put_prev_task(rq, tsk);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tif (tsk->sched_class->task_move_group)\n\t\ttsk->sched_class->task_move_group(tsk, on_rq);\n\telse\n#endif\n\t\tset_task_rq(tsk, task_cpu(tsk));\n\n\tif (unlikely(running))\n\t\ttsk->sched_class->set_curr_task(rq);\n\tif (on_rq)\n\t\tenqueue_task(rq, tsk, 0);\n\n\ttask_rq_unlock(rq, tsk, &flags);\n}\n#endif /* CONFIG_CGROUP_SCHED */\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic DEFINE_MUTEX(shares_mutex);\n\nint sched_group_set_shares(struct task_group *tg, unsigned long shares)\n{\n\tint i;\n\tunsigned long flags;\n\n\t/*\n\t * We can't change the weight of the root cgroup.\n\t */\n\tif (!tg->se[0])\n\t\treturn -EINVAL;\n\n\tif (shares < MIN_SHARES)\n\t\tshares = MIN_SHARES;\n\telse if (shares > MAX_SHARES)\n\t\tshares = MAX_SHARES;\n\n\tmutex_lock(&shares_mutex);\n\tif (tg->shares == shares)\n\t\tgoto done;\n\n\ttg->shares = shares;\n\tfor_each_possible_cpu(i) {\n\t\tstruct rq *rq = cpu_rq(i);\n\t\tstruct sched_entity *se;\n\n\t\tse = tg->se[i];\n\t\t/* Propagate contribution to hierarchy */\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tfor_each_sched_entity(se)\n\t\t\tupdate_cfs_shares(group_cfs_rq(se));\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\t}\n\ndone:\n\tmutex_unlock(&shares_mutex);\n\treturn 0;\n}\n\nunsigned long sched_group_shares(struct task_group *tg)\n{\n\treturn tg->shares;\n}\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\n/*\n * Ensure that the real time constraints are schedulable.\n */\nstatic DEFINE_MUTEX(rt_constraints_mutex);\n\nstatic unsigned long to_ratio(u64 period, u64 runtime)\n{\n\tif (runtime == RUNTIME_INF)\n\t\treturn 1ULL << 20;\n\n\treturn div64_u64(runtime << 20, period);\n}\n\n/* Must be called with tasklist_lock held */\nstatic inline int tg_has_rt_tasks(struct task_group *tg)\n{\n\tstruct task_struct *g, *p;\n\n\tdo_each_thread(g, p) {\n\t\tif (rt_task(p) && rt_rq_of_se(&p->rt)->tg == tg)\n\t\t\treturn 1;\n\t} while_each_thread(g, p);\n\n\treturn 0;\n}\n\nstruct rt_schedulable_data {\n\tstruct task_group *tg;\n\tu64 rt_period;\n\tu64 rt_runtime;\n};\n\nstatic int tg_schedulable(struct task_group *tg, void *data)\n{\n\tstruct rt_schedulable_data *d = data;\n\tstruct task_group *child;\n\tunsigned long total, sum = 0;\n\tu64 period, runtime;\n\n\tperiod = ktime_to_ns(tg->rt_bandwidth.rt_period);\n\truntime = tg->rt_bandwidth.rt_runtime;\n\n\tif (tg == d->tg) {\n\t\tperiod = d->rt_period;\n\t\truntime = d->rt_runtime;\n\t}\n\n\t/*\n\t * Cannot have more runtime than the period.\n\t */\n\tif (runtime > period && runtime != RUNTIME_INF)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Ensure we don't starve existing RT tasks.\n\t */\n\tif (rt_bandwidth_enabled() && !runtime && tg_has_rt_tasks(tg))\n\t\treturn -EBUSY;\n\n\ttotal = to_ratio(period, runtime);\n\n\t/*\n\t * Nobody can have more than the global setting allows.\n\t */\n\tif (total > to_ratio(global_rt_period(), global_rt_runtime()))\n\t\treturn -EINVAL;\n\n\t/*\n\t * The sum of our children's runtime should not exceed our own.\n\t */\n\tlist_for_each_entry_rcu(child, &tg->children, siblings) {\n\t\tperiod = ktime_to_ns(child->rt_bandwidth.rt_period);\n\t\truntime = child->rt_bandwidth.rt_runtime;\n\n\t\tif (child == d->tg) {\n\t\t\tperiod = d->rt_period;\n\t\t\truntime = d->rt_runtime;\n\t\t}\n\n\t\tsum += to_ratio(period, runtime);\n\t}\n\n\tif (sum > total)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int __rt_schedulable(struct task_group *tg, u64 period, u64 runtime)\n{\n\tstruct rt_schedulable_data data = {\n\t\t.tg = tg,\n\t\t.rt_period = period,\n\t\t.rt_runtime = runtime,\n\t};\n\n\treturn walk_tg_tree(tg_schedulable, tg_nop, &data);\n}\n\nstatic int tg_set_bandwidth(struct task_group *tg,\n\t\tu64 rt_period, u64 rt_runtime)\n{\n\tint i, err = 0;\n\n\tmutex_lock(&rt_constraints_mutex);\n\tread_lock(&tasklist_lock);\n\terr = __rt_schedulable(tg, rt_period, rt_runtime);\n\tif (err)\n\t\tgoto unlock;\n\n\traw_spin_lock_irq(&tg->rt_bandwidth.rt_runtime_lock);\n\ttg->rt_bandwidth.rt_period = ns_to_ktime(rt_period);\n\ttg->rt_bandwidth.rt_runtime = rt_runtime;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct rt_rq *rt_rq = tg->rt_rq[i];\n\n\t\traw_spin_lock(&rt_rq->rt_runtime_lock);\n\t\trt_rq->rt_runtime = rt_runtime;\n\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);\n\t}\n\traw_spin_unlock_irq(&tg->rt_bandwidth.rt_runtime_lock);\nunlock:\n\tread_unlock(&tasklist_lock);\n\tmutex_unlock(&rt_constraints_mutex);\n\n\treturn err;\n}\n\nint sched_group_set_rt_runtime(struct task_group *tg, long rt_runtime_us)\n{\n\tu64 rt_runtime, rt_period;\n\n\trt_period = ktime_to_ns(tg->rt_bandwidth.rt_period);\n\trt_runtime = (u64)rt_runtime_us * NSEC_PER_USEC;\n\tif (rt_runtime_us < 0)\n\t\trt_runtime = RUNTIME_INF;\n\n\treturn tg_set_bandwidth(tg, rt_period, rt_runtime);\n}\n\nlong sched_group_rt_runtime(struct task_group *tg)\n{\n\tu64 rt_runtime_us;\n\n\tif (tg->rt_bandwidth.rt_runtime == RUNTIME_INF)\n\t\treturn -1;\n\n\trt_runtime_us = tg->rt_bandwidth.rt_runtime;\n\tdo_div(rt_runtime_us, NSEC_PER_USEC);\n\treturn rt_runtime_us;\n}\n\nint sched_group_set_rt_period(struct task_group *tg, long rt_period_us)\n{\n\tu64 rt_runtime, rt_period;\n\n\trt_period = (u64)rt_period_us * NSEC_PER_USEC;\n\trt_runtime = tg->rt_bandwidth.rt_runtime;\n\n\tif (rt_period == 0)\n\t\treturn -EINVAL;\n\n\treturn tg_set_bandwidth(tg, rt_period, rt_runtime);\n}\n\nlong sched_group_rt_period(struct task_group *tg)\n{\n\tu64 rt_period_us;\n\n\trt_period_us = ktime_to_ns(tg->rt_bandwidth.rt_period);\n\tdo_div(rt_period_us, NSEC_PER_USEC);\n\treturn rt_period_us;\n}\n\nstatic int sched_rt_global_constraints(void)\n{\n\tu64 runtime, period;\n\tint ret = 0;\n\n\tif (sysctl_sched_rt_period <= 0)\n\t\treturn -EINVAL;\n\n\truntime = global_rt_runtime();\n\tperiod = global_rt_period();\n\n\t/*\n\t * Sanity check on the sysctl variables.\n\t */\n\tif (runtime > period && runtime != RUNTIME_INF)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&rt_constraints_mutex);\n\tread_lock(&tasklist_lock);\n\tret = __rt_schedulable(NULL, 0, 0);\n\tread_unlock(&tasklist_lock);\n\tmutex_unlock(&rt_constraints_mutex);\n\n\treturn ret;\n}\n\nint sched_rt_can_attach(struct task_group *tg, struct task_struct *tsk)\n{\n\t/* Don't accept realtime tasks when there is no way for them to run */\n\tif (rt_task(tsk) && tg->rt_bandwidth.rt_runtime == 0)\n\t\treturn 0;\n\n\treturn 1;\n}\n\n#else /* !CONFIG_RT_GROUP_SCHED */\nstatic int sched_rt_global_constraints(void)\n{\n\tunsigned long flags;\n\tint i;\n\n\tif (sysctl_sched_rt_period <= 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * There's always some RT tasks in the root group\n\t * -- migration, kstopmachine etc..\n\t */\n\tif (sysctl_sched_rt_runtime == 0)\n\t\treturn -EBUSY;\n\n\traw_spin_lock_irqsave(&def_rt_bandwidth.rt_runtime_lock, flags);\n\tfor_each_possible_cpu(i) {\n\t\tstruct rt_rq *rt_rq = &cpu_rq(i)->rt;\n\n\t\traw_spin_lock(&rt_rq->rt_runtime_lock);\n\t\trt_rq->rt_runtime = global_rt_runtime();\n\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);\n\t}\n\traw_spin_unlock_irqrestore(&def_rt_bandwidth.rt_runtime_lock, flags);\n\n\treturn 0;\n}\n#endif /* CONFIG_RT_GROUP_SCHED */\n\nint sched_rt_handler(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *lenp,\n\t\tloff_t *ppos)\n{\n\tint ret;\n\tint old_period, old_runtime;\n\tstatic DEFINE_MUTEX(mutex);\n\n\tmutex_lock(&mutex);\n\told_period = sysctl_sched_rt_period;\n\told_runtime = sysctl_sched_rt_runtime;\n\n\tret = proc_dointvec(table, write, buffer, lenp, ppos);\n\n\tif (!ret && write) {\n\t\tret = sched_rt_global_constraints();\n\t\tif (ret) {\n\t\t\tsysctl_sched_rt_period = old_period;\n\t\t\tsysctl_sched_rt_runtime = old_runtime;\n\t\t} else {\n\t\t\tdef_rt_bandwidth.rt_runtime = global_rt_runtime();\n\t\t\tdef_rt_bandwidth.rt_period =\n\t\t\t\tns_to_ktime(global_rt_period());\n\t\t}\n\t}\n\tmutex_unlock(&mutex);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_CGROUP_SCHED\n\n/* return corresponding task_group object of a cgroup */\nstatic inline struct task_group *cgroup_tg(struct cgroup *cgrp)\n{\n\treturn container_of(cgroup_subsys_state(cgrp, cpu_cgroup_subsys_id),\n\t\t\t    struct task_group, css);\n}\n\nstatic struct cgroup_subsys_state *\ncpu_cgroup_create(struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tstruct task_group *tg, *parent;\n\n\tif (!cgrp->parent) {\n\t\t/* This is early initialization for the top cgroup */\n\t\treturn &root_task_group.css;\n\t}\n\n\tparent = cgroup_tg(cgrp->parent);\n\ttg = sched_create_group(parent);\n\tif (IS_ERR(tg))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\treturn &tg->css;\n}\n\nstatic void\ncpu_cgroup_destroy(struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tstruct task_group *tg = cgroup_tg(cgrp);\n\n\tsched_destroy_group(tg);\n}\n\nstatic int\ncpu_cgroup_can_attach_task(struct cgroup *cgrp, struct task_struct *tsk)\n{\n#ifdef CONFIG_RT_GROUP_SCHED\n\tif (!sched_rt_can_attach(cgroup_tg(cgrp), tsk))\n\t\treturn -EINVAL;\n#else\n\t/* We don't support RT-tasks being in separate groups */\n\tif (tsk->sched_class != &fair_sched_class)\n\t\treturn -EINVAL;\n#endif\n\treturn 0;\n}\n\nstatic void\ncpu_cgroup_attach_task(struct cgroup *cgrp, struct task_struct *tsk)\n{\n\tsched_move_task(tsk);\n}\n\nstatic void\ncpu_cgroup_exit(struct cgroup_subsys *ss, struct cgroup *cgrp,\n\t\tstruct cgroup *old_cgrp, struct task_struct *task)\n{\n\t/*\n\t * cgroup_exit() is called in the copy_process() failure path.\n\t * Ignore this case since the task hasn't ran yet, this avoids\n\t * trying to poke a half freed task state from generic code.\n\t */\n\tif (!(task->flags & PF_EXITING))\n\t\treturn;\n\n\tsched_move_task(task);\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic int cpu_shares_write_u64(struct cgroup *cgrp, struct cftype *cftype,\n\t\t\t\tu64 shareval)\n{\n\treturn sched_group_set_shares(cgroup_tg(cgrp), scale_load(shareval));\n}\n\nstatic u64 cpu_shares_read_u64(struct cgroup *cgrp, struct cftype *cft)\n{\n\tstruct task_group *tg = cgroup_tg(cgrp);\n\n\treturn (u64) scale_load_down(tg->shares);\n}\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic int cpu_rt_runtime_write(struct cgroup *cgrp, struct cftype *cft,\n\t\t\t\ts64 val)\n{\n\treturn sched_group_set_rt_runtime(cgroup_tg(cgrp), val);\n}\n\nstatic s64 cpu_rt_runtime_read(struct cgroup *cgrp, struct cftype *cft)\n{\n\treturn sched_group_rt_runtime(cgroup_tg(cgrp));\n}\n\nstatic int cpu_rt_period_write_uint(struct cgroup *cgrp, struct cftype *cftype,\n\t\tu64 rt_period_us)\n{\n\treturn sched_group_set_rt_period(cgroup_tg(cgrp), rt_period_us);\n}\n\nstatic u64 cpu_rt_period_read_uint(struct cgroup *cgrp, struct cftype *cft)\n{\n\treturn sched_group_rt_period(cgroup_tg(cgrp));\n}\n#endif /* CONFIG_RT_GROUP_SCHED */\n\nstatic struct cftype cpu_files[] = {\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t{\n\t\t.name = \"shares\",\n\t\t.read_u64 = cpu_shares_read_u64,\n\t\t.write_u64 = cpu_shares_write_u64,\n\t},\n#endif\n#ifdef CONFIG_RT_GROUP_SCHED\n\t{\n\t\t.name = \"rt_runtime_us\",\n\t\t.read_s64 = cpu_rt_runtime_read,\n\t\t.write_s64 = cpu_rt_runtime_write,\n\t},\n\t{\n\t\t.name = \"rt_period_us\",\n\t\t.read_u64 = cpu_rt_period_read_uint,\n\t\t.write_u64 = cpu_rt_period_write_uint,\n\t},\n#endif\n};\n\nstatic int cpu_cgroup_populate(struct cgroup_subsys *ss, struct cgroup *cont)\n{\n\treturn cgroup_add_files(cont, ss, cpu_files, ARRAY_SIZE(cpu_files));\n}\n\nstruct cgroup_subsys cpu_cgroup_subsys = {\n\t.name\t\t= \"cpu\",\n\t.create\t\t= cpu_cgroup_create,\n\t.destroy\t= cpu_cgroup_destroy,\n\t.can_attach_task = cpu_cgroup_can_attach_task,\n\t.attach_task\t= cpu_cgroup_attach_task,\n\t.exit\t\t= cpu_cgroup_exit,\n\t.populate\t= cpu_cgroup_populate,\n\t.subsys_id\t= cpu_cgroup_subsys_id,\n\t.early_init\t= 1,\n};\n\n#endif\t/* CONFIG_CGROUP_SCHED */\n\n#ifdef CONFIG_CGROUP_CPUACCT\n\n/*\n * CPU accounting code for task groups.\n *\n * Based on the work by Paul Menage (menage@google.com) and Balbir Singh\n * (balbir@in.ibm.com).\n */\n\n/* track cpu usage of a group of tasks and its child groups */\nstruct cpuacct {\n\tstruct cgroup_subsys_state css;\n\t/* cpuusage holds pointer to a u64-type object on every cpu */\n\tu64 __percpu *cpuusage;\n\tstruct percpu_counter cpustat[CPUACCT_STAT_NSTATS];\n\tstruct cpuacct *parent;\n};\n\nstruct cgroup_subsys cpuacct_subsys;\n\n/* return cpu accounting group corresponding to this container */\nstatic inline struct cpuacct *cgroup_ca(struct cgroup *cgrp)\n{\n\treturn container_of(cgroup_subsys_state(cgrp, cpuacct_subsys_id),\n\t\t\t    struct cpuacct, css);\n}\n\n/* return cpu accounting group to which this task belongs */\nstatic inline struct cpuacct *task_ca(struct task_struct *tsk)\n{\n\treturn container_of(task_subsys_state(tsk, cpuacct_subsys_id),\n\t\t\t    struct cpuacct, css);\n}\n\n/* create a new cpu accounting group */\nstatic struct cgroup_subsys_state *cpuacct_create(\n\tstruct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tstruct cpuacct *ca = kzalloc(sizeof(*ca), GFP_KERNEL);\n\tint i;\n\n\tif (!ca)\n\t\tgoto out;\n\n\tca->cpuusage = alloc_percpu(u64);\n\tif (!ca->cpuusage)\n\t\tgoto out_free_ca;\n\n\tfor (i = 0; i < CPUACCT_STAT_NSTATS; i++)\n\t\tif (percpu_counter_init(&ca->cpustat[i], 0))\n\t\t\tgoto out_free_counters;\n\n\tif (cgrp->parent)\n\t\tca->parent = cgroup_ca(cgrp->parent);\n\n\treturn &ca->css;\n\nout_free_counters:\n\twhile (--i >= 0)\n\t\tpercpu_counter_destroy(&ca->cpustat[i]);\n\tfree_percpu(ca->cpuusage);\nout_free_ca:\n\tkfree(ca);\nout:\n\treturn ERR_PTR(-ENOMEM);\n}\n\n/* destroy an existing cpu accounting group */\nstatic void\ncpuacct_destroy(struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgrp);\n\tint i;\n\n\tfor (i = 0; i < CPUACCT_STAT_NSTATS; i++)\n\t\tpercpu_counter_destroy(&ca->cpustat[i]);\n\tfree_percpu(ca->cpuusage);\n\tkfree(ca);\n}\n\nstatic u64 cpuacct_cpuusage_read(struct cpuacct *ca, int cpu)\n{\n\tu64 *cpuusage = per_cpu_ptr(ca->cpuusage, cpu);\n\tu64 data;\n\n#ifndef CONFIG_64BIT\n\t/*\n\t * Take rq->lock to make 64-bit read safe on 32-bit platforms.\n\t */\n\traw_spin_lock_irq(&cpu_rq(cpu)->lock);\n\tdata = *cpuusage;\n\traw_spin_unlock_irq(&cpu_rq(cpu)->lock);\n#else\n\tdata = *cpuusage;\n#endif\n\n\treturn data;\n}\n\nstatic void cpuacct_cpuusage_write(struct cpuacct *ca, int cpu, u64 val)\n{\n\tu64 *cpuusage = per_cpu_ptr(ca->cpuusage, cpu);\n\n#ifndef CONFIG_64BIT\n\t/*\n\t * Take rq->lock to make 64-bit write safe on 32-bit platforms.\n\t */\n\traw_spin_lock_irq(&cpu_rq(cpu)->lock);\n\t*cpuusage = val;\n\traw_spin_unlock_irq(&cpu_rq(cpu)->lock);\n#else\n\t*cpuusage = val;\n#endif\n}\n\n/* return total cpu usage (in nanoseconds) of a group */\nstatic u64 cpuusage_read(struct cgroup *cgrp, struct cftype *cft)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgrp);\n\tu64 totalcpuusage = 0;\n\tint i;\n\n\tfor_each_present_cpu(i)\n\t\ttotalcpuusage += cpuacct_cpuusage_read(ca, i);\n\n\treturn totalcpuusage;\n}\n\nstatic int cpuusage_write(struct cgroup *cgrp, struct cftype *cftype,\n\t\t\t\t\t\t\t\tu64 reset)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgrp);\n\tint err = 0;\n\tint i;\n\n\tif (reset) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tfor_each_present_cpu(i)\n\t\tcpuacct_cpuusage_write(ca, i, 0);\n\nout:\n\treturn err;\n}\n\nstatic int cpuacct_percpu_seq_read(struct cgroup *cgroup, struct cftype *cft,\n\t\t\t\t   struct seq_file *m)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgroup);\n\tu64 percpu;\n\tint i;\n\n\tfor_each_present_cpu(i) {\n\t\tpercpu = cpuacct_cpuusage_read(ca, i);\n\t\tseq_printf(m, \"%llu \", (unsigned long long) percpu);\n\t}\n\tseq_printf(m, \"\\n\");\n\treturn 0;\n}\n\nstatic const char *cpuacct_stat_desc[] = {\n\t[CPUACCT_STAT_USER] = \"user\",\n\t[CPUACCT_STAT_SYSTEM] = \"system\",\n};\n\nstatic int cpuacct_stats_show(struct cgroup *cgrp, struct cftype *cft,\n\t\tstruct cgroup_map_cb *cb)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgrp);\n\tint i;\n\n\tfor (i = 0; i < CPUACCT_STAT_NSTATS; i++) {\n\t\ts64 val = percpu_counter_read(&ca->cpustat[i]);\n\t\tval = cputime64_to_clock_t(val);\n\t\tcb->fill(cb, cpuacct_stat_desc[i], val);\n\t}\n\treturn 0;\n}\n\nstatic struct cftype files[] = {\n\t{\n\t\t.name = \"usage\",\n\t\t.read_u64 = cpuusage_read,\n\t\t.write_u64 = cpuusage_write,\n\t},\n\t{\n\t\t.name = \"usage_percpu\",\n\t\t.read_seq_string = cpuacct_percpu_seq_read,\n\t},\n\t{\n\t\t.name = \"stat\",\n\t\t.read_map = cpuacct_stats_show,\n\t},\n};\n\nstatic int cpuacct_populate(struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\treturn cgroup_add_files(cgrp, ss, files, ARRAY_SIZE(files));\n}\n\n/*\n * charge this task's execution time to its accounting group.\n *\n * called with rq->lock held.\n */\nstatic void cpuacct_charge(struct task_struct *tsk, u64 cputime)\n{\n\tstruct cpuacct *ca;\n\tint cpu;\n\n\tif (unlikely(!cpuacct_subsys.active))\n\t\treturn;\n\n\tcpu = task_cpu(tsk);\n\n\trcu_read_lock();\n\n\tca = task_ca(tsk);\n\n\tfor (; ca; ca = ca->parent) {\n\t\tu64 *cpuusage = per_cpu_ptr(ca->cpuusage, cpu);\n\t\t*cpuusage += cputime;\n\t}\n\n\trcu_read_unlock();\n}\n\n/*\n * When CONFIG_VIRT_CPU_ACCOUNTING is enabled one jiffy can be very large\n * in cputime_t units. As a result, cpuacct_update_stats calls\n * percpu_counter_add with values large enough to always overflow the\n * per cpu batch limit causing bad SMP scalability.\n *\n * To fix this we scale percpu_counter_batch by cputime_one_jiffy so we\n * batch the same amount of time with CONFIG_VIRT_CPU_ACCOUNTING disabled\n * and enabled. We cap it at INT_MAX which is the largest allowed batch value.\n */\n#ifdef CONFIG_SMP\n#define CPUACCT_BATCH\t\\\n\tmin_t(long, percpu_counter_batch * cputime_one_jiffy, INT_MAX)\n#else\n#define CPUACCT_BATCH\t0\n#endif\n\n/*\n * Charge the system/user time to the task's accounting group.\n */\nstatic void cpuacct_update_stats(struct task_struct *tsk,\n\t\tenum cpuacct_stat_index idx, cputime_t val)\n{\n\tstruct cpuacct *ca;\n\tint batch = CPUACCT_BATCH;\n\n\tif (unlikely(!cpuacct_subsys.active))\n\t\treturn;\n\n\trcu_read_lock();\n\tca = task_ca(tsk);\n\n\tdo {\n\t\t__percpu_counter_add(&ca->cpustat[idx], val, batch);\n\t\tca = ca->parent;\n\t} while (ca);\n\trcu_read_unlock();\n}\n\nstruct cgroup_subsys cpuacct_subsys = {\n\t.name = \"cpuacct\",\n\t.create = cpuacct_create,\n\t.destroy = cpuacct_destroy,\n\t.populate = cpuacct_populate,\n\t.subsys_id = cpuacct_subsys_id,\n};\n#endif\t/* CONFIG_CGROUP_CPUACCT */\n\n", "/*\n * Detect hard and soft lockups on a system\n *\n * started by Don Zickus, Copyright (C) 2010 Red Hat, Inc.\n *\n * this code detects hard lockups: incidents in where on a CPU\n * the kernel does not respond to anything except NMI.\n *\n * Note: Most of this code is borrowed heavily from softlockup.c,\n * so thanks to Ingo for the initial implementation.\n * Some chunks also taken from arch/x86/kernel/apic/nmi.c, thanks\n * to those contributors as well.\n */\n\n#include <linux/mm.h>\n#include <linux/cpu.h>\n#include <linux/nmi.h>\n#include <linux/init.h>\n#include <linux/delay.h>\n#include <linux/freezer.h>\n#include <linux/kthread.h>\n#include <linux/lockdep.h>\n#include <linux/notifier.h>\n#include <linux/module.h>\n#include <linux/sysctl.h>\n\n#include <asm/irq_regs.h>\n#include <linux/perf_event.h>\n\nint watchdog_enabled = 1;\nint __read_mostly watchdog_thresh = 10;\n\nstatic DEFINE_PER_CPU(unsigned long, watchdog_touch_ts);\nstatic DEFINE_PER_CPU(struct task_struct *, softlockup_watchdog);\nstatic DEFINE_PER_CPU(struct hrtimer, watchdog_hrtimer);\nstatic DEFINE_PER_CPU(bool, softlockup_touch_sync);\nstatic DEFINE_PER_CPU(bool, soft_watchdog_warn);\n#ifdef CONFIG_HARDLOCKUP_DETECTOR\nstatic DEFINE_PER_CPU(bool, hard_watchdog_warn);\nstatic DEFINE_PER_CPU(bool, watchdog_nmi_touch);\nstatic DEFINE_PER_CPU(unsigned long, hrtimer_interrupts);\nstatic DEFINE_PER_CPU(unsigned long, hrtimer_interrupts_saved);\nstatic DEFINE_PER_CPU(struct perf_event *, watchdog_ev);\n#endif\n\n/* boot commands */\n/*\n * Should we panic when a soft-lockup or hard-lockup occurs:\n */\n#ifdef CONFIG_HARDLOCKUP_DETECTOR\nstatic int hardlockup_panic =\n\t\t\tCONFIG_BOOTPARAM_HARDLOCKUP_PANIC_VALUE;\n\nstatic int __init hardlockup_panic_setup(char *str)\n{\n\tif (!strncmp(str, \"panic\", 5))\n\t\thardlockup_panic = 1;\n\telse if (!strncmp(str, \"nopanic\", 7))\n\t\thardlockup_panic = 0;\n\telse if (!strncmp(str, \"0\", 1))\n\t\twatchdog_enabled = 0;\n\treturn 1;\n}\n__setup(\"nmi_watchdog=\", hardlockup_panic_setup);\n#endif\n\nunsigned int __read_mostly softlockup_panic =\n\t\t\tCONFIG_BOOTPARAM_SOFTLOCKUP_PANIC_VALUE;\n\nstatic int __init softlockup_panic_setup(char *str)\n{\n\tsoftlockup_panic = simple_strtoul(str, NULL, 0);\n\n\treturn 1;\n}\n__setup(\"softlockup_panic=\", softlockup_panic_setup);\n\nstatic int __init nowatchdog_setup(char *str)\n{\n\twatchdog_enabled = 0;\n\treturn 1;\n}\n__setup(\"nowatchdog\", nowatchdog_setup);\n\n/* deprecated */\nstatic int __init nosoftlockup_setup(char *str)\n{\n\twatchdog_enabled = 0;\n\treturn 1;\n}\n__setup(\"nosoftlockup\", nosoftlockup_setup);\n/*  */\n\n/*\n * Hard-lockup warnings should be triggered after just a few seconds. Soft-\n * lockups can have false positives under extreme conditions. So we generally\n * want a higher threshold for soft lockups than for hard lockups. So we couple\n * the thresholds with a factor: we make the soft threshold twice the amount of\n * time the hard threshold is.\n */\nstatic int get_softlockup_thresh(void)\n{\n\treturn watchdog_thresh * 2;\n}\n\n/*\n * Returns seconds, approximately.  We don't need nanosecond\n * resolution, and we don't need to waste time with a big divide when\n * 2^30ns == 1.074s.\n */\nstatic unsigned long get_timestamp(int this_cpu)\n{\n\treturn cpu_clock(this_cpu) >> 30LL;  /* 2^30 ~= 10^9 */\n}\n\nstatic unsigned long get_sample_period(void)\n{\n\t/*\n\t * convert watchdog_thresh from seconds to ns\n\t * the divide by 5 is to give hrtimer 5 chances to\n\t * increment before the hardlockup detector generates\n\t * a warning\n\t */\n\treturn get_softlockup_thresh() * (NSEC_PER_SEC / 5);\n}\n\n/* Commands for resetting the watchdog */\nstatic void __touch_watchdog(void)\n{\n\tint this_cpu = smp_processor_id();\n\n\t__this_cpu_write(watchdog_touch_ts, get_timestamp(this_cpu));\n}\n\nvoid touch_softlockup_watchdog(void)\n{\n\t__this_cpu_write(watchdog_touch_ts, 0);\n}\nEXPORT_SYMBOL(touch_softlockup_watchdog);\n\nvoid touch_all_softlockup_watchdogs(void)\n{\n\tint cpu;\n\n\t/*\n\t * this is done lockless\n\t * do we care if a 0 races with a timestamp?\n\t * all it means is the softlock check starts one cycle later\n\t */\n\tfor_each_online_cpu(cpu)\n\t\tper_cpu(watchdog_touch_ts, cpu) = 0;\n}\n\n#ifdef CONFIG_HARDLOCKUP_DETECTOR\nvoid touch_nmi_watchdog(void)\n{\n\tif (watchdog_enabled) {\n\t\tunsigned cpu;\n\n\t\tfor_each_present_cpu(cpu) {\n\t\t\tif (per_cpu(watchdog_nmi_touch, cpu) != true)\n\t\t\t\tper_cpu(watchdog_nmi_touch, cpu) = true;\n\t\t}\n\t}\n\ttouch_softlockup_watchdog();\n}\nEXPORT_SYMBOL(touch_nmi_watchdog);\n\n#endif\n\nvoid touch_softlockup_watchdog_sync(void)\n{\n\t__raw_get_cpu_var(softlockup_touch_sync) = true;\n\t__raw_get_cpu_var(watchdog_touch_ts) = 0;\n}\n\n#ifdef CONFIG_HARDLOCKUP_DETECTOR\n/* watchdog detector functions */\nstatic int is_hardlockup(void)\n{\n\tunsigned long hrint = __this_cpu_read(hrtimer_interrupts);\n\n\tif (__this_cpu_read(hrtimer_interrupts_saved) == hrint)\n\t\treturn 1;\n\n\t__this_cpu_write(hrtimer_interrupts_saved, hrint);\n\treturn 0;\n}\n#endif\n\nstatic int is_softlockup(unsigned long touch_ts)\n{\n\tunsigned long now = get_timestamp(smp_processor_id());\n\n\t/* Warn about unreasonable delays: */\n\tif (time_after(now, touch_ts + get_softlockup_thresh()))\n\t\treturn now - touch_ts;\n\n\treturn 0;\n}\n\n#ifdef CONFIG_HARDLOCKUP_DETECTOR\nvoid __weak hw_nmi_watchdog_set_attr(struct perf_event_attr *wd_attr) { }\n\nstatic struct perf_event_attr wd_hw_attr = {\n\t.type\t\t= PERF_TYPE_HARDWARE,\n\t.config\t\t= PERF_COUNT_HW_CPU_CYCLES,\n\t.size\t\t= sizeof(struct perf_event_attr),\n\t.pinned\t\t= 1,\n\t.disabled\t= 1,\n};\n\n/* Callback function for perf event subsystem */\nstatic void watchdog_overflow_callback(struct perf_event *event, int nmi,\n\t\t struct perf_sample_data *data,\n\t\t struct pt_regs *regs)\n{\n\t/* Ensure the watchdog never gets throttled */\n\tevent->hw.interrupts = 0;\n\n\tif (__this_cpu_read(watchdog_nmi_touch) == true) {\n\t\t__this_cpu_write(watchdog_nmi_touch, false);\n\t\treturn;\n\t}\n\n\t/* check for a hardlockup\n\t * This is done by making sure our timer interrupt\n\t * is incrementing.  The timer interrupt should have\n\t * fired multiple times before we overflow'd.  If it hasn't\n\t * then this is a good indication the cpu is stuck\n\t */\n\tif (is_hardlockup()) {\n\t\tint this_cpu = smp_processor_id();\n\n\t\t/* only print hardlockups once */\n\t\tif (__this_cpu_read(hard_watchdog_warn) == true)\n\t\t\treturn;\n\n\t\tif (hardlockup_panic)\n\t\t\tpanic(\"Watchdog detected hard LOCKUP on cpu %d\", this_cpu);\n\t\telse\n\t\t\tWARN(1, \"Watchdog detected hard LOCKUP on cpu %d\", this_cpu);\n\n\t\t__this_cpu_write(hard_watchdog_warn, true);\n\t\treturn;\n\t}\n\n\t__this_cpu_write(hard_watchdog_warn, false);\n\treturn;\n}\nstatic void watchdog_interrupt_count(void)\n{\n\t__this_cpu_inc(hrtimer_interrupts);\n}\n#else\nstatic inline void watchdog_interrupt_count(void) { return; }\n#endif /* CONFIG_HARDLOCKUP_DETECTOR */\n\n/* watchdog kicker functions */\nstatic enum hrtimer_restart watchdog_timer_fn(struct hrtimer *hrtimer)\n{\n\tunsigned long touch_ts = __this_cpu_read(watchdog_touch_ts);\n\tstruct pt_regs *regs = get_irq_regs();\n\tint duration;\n\n\t/* kick the hardlockup detector */\n\twatchdog_interrupt_count();\n\n\t/* kick the softlockup detector */\n\twake_up_process(__this_cpu_read(softlockup_watchdog));\n\n\t/* .. and repeat */\n\thrtimer_forward_now(hrtimer, ns_to_ktime(get_sample_period()));\n\n\tif (touch_ts == 0) {\n\t\tif (unlikely(__this_cpu_read(softlockup_touch_sync))) {\n\t\t\t/*\n\t\t\t * If the time stamp was touched atomically\n\t\t\t * make sure the scheduler tick is up to date.\n\t\t\t */\n\t\t\t__this_cpu_write(softlockup_touch_sync, false);\n\t\t\tsched_clock_tick();\n\t\t}\n\t\t__touch_watchdog();\n\t\treturn HRTIMER_RESTART;\n\t}\n\n\t/* check for a softlockup\n\t * This is done by making sure a high priority task is\n\t * being scheduled.  The task touches the watchdog to\n\t * indicate it is getting cpu time.  If it hasn't then\n\t * this is a good indication some task is hogging the cpu\n\t */\n\tduration = is_softlockup(touch_ts);\n\tif (unlikely(duration)) {\n\t\t/* only warn once */\n\t\tif (__this_cpu_read(soft_watchdog_warn) == true)\n\t\t\treturn HRTIMER_RESTART;\n\n\t\tprintk(KERN_ERR \"BUG: soft lockup - CPU#%d stuck for %us! [%s:%d]\\n\",\n\t\t\tsmp_processor_id(), duration,\n\t\t\tcurrent->comm, task_pid_nr(current));\n\t\tprint_modules();\n\t\tprint_irqtrace_events(current);\n\t\tif (regs)\n\t\t\tshow_regs(regs);\n\t\telse\n\t\t\tdump_stack();\n\n\t\tif (softlockup_panic)\n\t\t\tpanic(\"softlockup: hung tasks\");\n\t\t__this_cpu_write(soft_watchdog_warn, true);\n\t} else\n\t\t__this_cpu_write(soft_watchdog_warn, false);\n\n\treturn HRTIMER_RESTART;\n}\n\n\n/*\n * The watchdog thread - touches the timestamp.\n */\nstatic int watchdog(void *unused)\n{\n\tstatic struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };\n\tstruct hrtimer *hrtimer = &__raw_get_cpu_var(watchdog_hrtimer);\n\n\tsched_setscheduler(current, SCHED_FIFO, &param);\n\n\t/* initialize timestamp */\n\t__touch_watchdog();\n\n\t/* kick off the timer for the hardlockup detector */\n\t/* done here because hrtimer_start can only pin to smp_processor_id() */\n\thrtimer_start(hrtimer, ns_to_ktime(get_sample_period()),\n\t\t      HRTIMER_MODE_REL_PINNED);\n\n\tset_current_state(TASK_INTERRUPTIBLE);\n\t/*\n\t * Run briefly once per second to reset the softlockup timestamp.\n\t * If this gets delayed for more than 60 seconds then the\n\t * debug-printout triggers in watchdog_timer_fn().\n\t */\n\twhile (!kthread_should_stop()) {\n\t\t__touch_watchdog();\n\t\tschedule();\n\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\n\treturn 0;\n}\n\n\n#ifdef CONFIG_HARDLOCKUP_DETECTOR\nstatic int watchdog_nmi_enable(int cpu)\n{\n\tstruct perf_event_attr *wd_attr;\n\tstruct perf_event *event = per_cpu(watchdog_ev, cpu);\n\n\t/* is it already setup and enabled? */\n\tif (event && event->state > PERF_EVENT_STATE_OFF)\n\t\tgoto out;\n\n\t/* it is setup but not enabled */\n\tif (event != NULL)\n\t\tgoto out_enable;\n\n\twd_attr = &wd_hw_attr;\n\twd_attr->sample_period = hw_nmi_get_sample_period(watchdog_thresh);\n\thw_nmi_watchdog_set_attr(wd_attr);\n\n\t/* Try to register using hardware perf events */\n\tevent = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback);\n\tif (!IS_ERR(event)) {\n\t\tprintk(KERN_INFO \"NMI watchdog enabled, takes one hw-pmu counter.\\n\");\n\t\tgoto out_save;\n\t}\n\n\n\t/* vary the KERN level based on the returned errno */\n\tif (PTR_ERR(event) == -EOPNOTSUPP)\n\t\tprintk(KERN_INFO \"NMI watchdog disabled (cpu%i): not supported (no LAPIC?)\\n\", cpu);\n\telse if (PTR_ERR(event) == -ENOENT)\n\t\tprintk(KERN_WARNING \"NMI watchdog disabled (cpu%i): hardware events not enabled\\n\", cpu);\n\telse\n\t\tprintk(KERN_ERR \"NMI watchdog disabled (cpu%i): unable to create perf event: %ld\\n\", cpu, PTR_ERR(event));\n\treturn PTR_ERR(event);\n\n\t/* success path */\nout_save:\n\tper_cpu(watchdog_ev, cpu) = event;\nout_enable:\n\tperf_event_enable(per_cpu(watchdog_ev, cpu));\nout:\n\treturn 0;\n}\n\nstatic void watchdog_nmi_disable(int cpu)\n{\n\tstruct perf_event *event = per_cpu(watchdog_ev, cpu);\n\n\tif (event) {\n\t\tperf_event_disable(event);\n\t\tper_cpu(watchdog_ev, cpu) = NULL;\n\n\t\t/* should be in cleanup, but blocks oprofile */\n\t\tperf_event_release_kernel(event);\n\t}\n\treturn;\n}\n#else\nstatic int watchdog_nmi_enable(int cpu) { return 0; }\nstatic void watchdog_nmi_disable(int cpu) { return; }\n#endif /* CONFIG_HARDLOCKUP_DETECTOR */\n\n/* prepare/enable/disable routines */\nstatic void watchdog_prepare_cpu(int cpu)\n{\n\tstruct hrtimer *hrtimer = &per_cpu(watchdog_hrtimer, cpu);\n\n\tWARN_ON(per_cpu(softlockup_watchdog, cpu));\n\thrtimer_init(hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\thrtimer->function = watchdog_timer_fn;\n}\n\nstatic int watchdog_enable(int cpu)\n{\n\tstruct task_struct *p = per_cpu(softlockup_watchdog, cpu);\n\tint err = 0;\n\n\t/* enable the perf event */\n\terr = watchdog_nmi_enable(cpu);\n\n\t/* Regardless of err above, fall through and start softlockup */\n\n\t/* create the watchdog thread */\n\tif (!p) {\n\t\tp = kthread_create(watchdog, (void *)(unsigned long)cpu, \"watchdog/%d\", cpu);\n\t\tif (IS_ERR(p)) {\n\t\t\tprintk(KERN_ERR \"softlockup watchdog for %i failed\\n\", cpu);\n\t\t\tif (!err) {\n\t\t\t\t/* if hardlockup hasn't already set this */\n\t\t\t\terr = PTR_ERR(p);\n\t\t\t\t/* and disable the perf event */\n\t\t\t\twatchdog_nmi_disable(cpu);\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\t\tkthread_bind(p, cpu);\n\t\tper_cpu(watchdog_touch_ts, cpu) = 0;\n\t\tper_cpu(softlockup_watchdog, cpu) = p;\n\t\twake_up_process(p);\n\t}\n\nout:\n\treturn err;\n}\n\nstatic void watchdog_disable(int cpu)\n{\n\tstruct task_struct *p = per_cpu(softlockup_watchdog, cpu);\n\tstruct hrtimer *hrtimer = &per_cpu(watchdog_hrtimer, cpu);\n\n\t/*\n\t * cancel the timer first to stop incrementing the stats\n\t * and waking up the kthread\n\t */\n\thrtimer_cancel(hrtimer);\n\n\t/* disable the perf event */\n\twatchdog_nmi_disable(cpu);\n\n\t/* stop the watchdog thread */\n\tif (p) {\n\t\tper_cpu(softlockup_watchdog, cpu) = NULL;\n\t\tkthread_stop(p);\n\t}\n}\n\nstatic void watchdog_enable_all_cpus(void)\n{\n\tint cpu;\n\n\twatchdog_enabled = 0;\n\n\tfor_each_online_cpu(cpu)\n\t\tif (!watchdog_enable(cpu))\n\t\t\t/* if any cpu succeeds, watchdog is considered\n\t\t\t   enabled for the system */\n\t\t\twatchdog_enabled = 1;\n\n\tif (!watchdog_enabled)\n\t\tprintk(KERN_ERR \"watchdog: failed to be enabled on some cpus\\n\");\n\n}\n\nstatic void watchdog_disable_all_cpus(void)\n{\n\tint cpu;\n\n\tfor_each_online_cpu(cpu)\n\t\twatchdog_disable(cpu);\n\n\t/* if all watchdogs are disabled, then they are disabled for the system */\n\twatchdog_enabled = 0;\n}\n\n\n/* sysctl functions */\n#ifdef CONFIG_SYSCTL\n/*\n * proc handler for /proc/sys/kernel/nmi_watchdog,watchdog_thresh\n */\n\nint proc_dowatchdog(struct ctl_table *table, int write,\n\t\t    void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint ret;\n\n\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\tif (ret || !write)\n\t\tgoto out;\n\n\tif (watchdog_enabled && watchdog_thresh)\n\t\twatchdog_enable_all_cpus();\n\telse\n\t\twatchdog_disable_all_cpus();\n\nout:\n\treturn ret;\n}\n#endif /* CONFIG_SYSCTL */\n\n\n/*\n * Create/destroy watchdog threads as CPUs come and go:\n */\nstatic int __cpuinit\ncpu_callback(struct notifier_block *nfb, unsigned long action, void *hcpu)\n{\n\tint hotcpu = (unsigned long)hcpu;\n\n\tswitch (action) {\n\tcase CPU_UP_PREPARE:\n\tcase CPU_UP_PREPARE_FROZEN:\n\t\twatchdog_prepare_cpu(hotcpu);\n\t\tbreak;\n\tcase CPU_ONLINE:\n\tcase CPU_ONLINE_FROZEN:\n\t\tif (watchdog_enabled)\n\t\t\twatchdog_enable(hotcpu);\n\t\tbreak;\n#ifdef CONFIG_HOTPLUG_CPU\n\tcase CPU_UP_CANCELED:\n\tcase CPU_UP_CANCELED_FROZEN:\n\t\twatchdog_disable(hotcpu);\n\t\tbreak;\n\tcase CPU_DEAD:\n\tcase CPU_DEAD_FROZEN:\n\t\twatchdog_disable(hotcpu);\n\t\tbreak;\n#endif /* CONFIG_HOTPLUG_CPU */\n\t}\n\n\t/*\n\t * hardlockup and softlockup are not important enough\n\t * to block cpu bring up.  Just always succeed and\n\t * rely on printk output to flag problems.\n\t */\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block __cpuinitdata cpu_nfb = {\n\t.notifier_call = cpu_callback\n};\n\nvoid __init lockup_detector_init(void)\n{\n\tvoid *cpu = (void *)(long)smp_processor_id();\n\tint err;\n\n\terr = cpu_callback(&cpu_nfb, CPU_UP_PREPARE, cpu);\n\tWARN_ON(notifier_to_errno(err));\n\n\tcpu_callback(&cpu_nfb, CPU_ONLINE, cpu);\n\tregister_cpu_notifier(&cpu_nfb);\n\n\treturn;\n}\n", "/*\n * data_breakpoint.c - Sample HW Breakpoint file to watch kernel data address\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.\n *\n * usage: insmod data_breakpoint.ko ksym=<ksym_name>\n *\n * This file is a kernel module that places a breakpoint over ksym_name kernel\n * variable using Hardware Breakpoint register. The corresponding handler which\n * prints a backtrace is invoked every time a write operation is performed on\n * that variable.\n *\n * Copyright (C) IBM Corporation, 2009\n *\n * Author: K.Prasad <prasad@linux.vnet.ibm.com>\n */\n#include <linux/module.h>\t/* Needed by all modules */\n#include <linux/kernel.h>\t/* Needed for KERN_INFO */\n#include <linux/init.h>\t\t/* Needed for the macros */\n#include <linux/kallsyms.h>\n\n#include <linux/perf_event.h>\n#include <linux/hw_breakpoint.h>\n\nstruct perf_event * __percpu *sample_hbp;\n\nstatic char ksym_name[KSYM_NAME_LEN] = \"pid_max\";\nmodule_param_string(ksym, ksym_name, KSYM_NAME_LEN, S_IRUGO);\nMODULE_PARM_DESC(ksym, \"Kernel symbol to monitor; this module will report any\"\n\t\t\t\" write operations on the kernel symbol\");\n\nstatic void sample_hbp_handler(struct perf_event *bp, int nmi,\n\t\t\t       struct perf_sample_data *data,\n\t\t\t       struct pt_regs *regs)\n{\n\tprintk(KERN_INFO \"%s value is changed\\n\", ksym_name);\n\tdump_stack();\n\tprintk(KERN_INFO \"Dump stack from sample_hbp_handler\\n\");\n}\n\nstatic int __init hw_break_module_init(void)\n{\n\tint ret;\n\tstruct perf_event_attr attr;\n\n\thw_breakpoint_init(&attr);\n\tattr.bp_addr = kallsyms_lookup_name(ksym_name);\n\tattr.bp_len = HW_BREAKPOINT_LEN_4;\n\tattr.bp_type = HW_BREAKPOINT_W | HW_BREAKPOINT_R;\n\n\tsample_hbp = register_wide_hw_breakpoint(&attr, sample_hbp_handler);\n\tif (IS_ERR((void __force *)sample_hbp)) {\n\t\tret = PTR_ERR((void __force *)sample_hbp);\n\t\tgoto fail;\n\t}\n\n\tprintk(KERN_INFO \"HW Breakpoint for %s write installed\\n\", ksym_name);\n\n\treturn 0;\n\nfail:\n\tprintk(KERN_INFO \"Breakpoint registration failed\\n\");\n\n\treturn ret;\n}\n\nstatic void __exit hw_break_module_exit(void)\n{\n\tunregister_wide_hw_breakpoint(sample_hbp);\n\tprintk(KERN_INFO \"HW Breakpoint for %s write uninstalled\\n\", ksym_name);\n}\n\nmodule_init(hw_break_module_init);\nmodule_exit(hw_break_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"K.Prasad\");\nMODULE_DESCRIPTION(\"ksym breakpoint\");\n"], "fixing_code": ["/*\n * Hardware performance events for the Alpha.\n *\n * We implement HW counts on the EV67 and subsequent CPUs only.\n *\n * (C) 2010 Michael J. Cree\n *\n * Somewhat based on the Sparc code, and to a lesser extent the PowerPC and\n * ARM code, which are copyright by their respective authors.\n */\n\n#include <linux/perf_event.h>\n#include <linux/kprobes.h>\n#include <linux/kernel.h>\n#include <linux/kdebug.h>\n#include <linux/mutex.h>\n#include <linux/init.h>\n\n#include <asm/hwrpb.h>\n#include <asm/atomic.h>\n#include <asm/irq.h>\n#include <asm/irq_regs.h>\n#include <asm/pal.h>\n#include <asm/wrperfmon.h>\n#include <asm/hw_irq.h>\n\n\n/* The maximum number of PMCs on any Alpha CPU whatsoever. */\n#define MAX_HWEVENTS 3\n#define PMC_NO_INDEX -1\n\n/* For tracking PMCs and the hw events they monitor on each CPU. */\nstruct cpu_hw_events {\n\tint\t\t\tenabled;\n\t/* Number of events scheduled; also number entries valid in arrays below. */\n\tint\t\t\tn_events;\n\t/* Number events added since last hw_perf_disable(). */\n\tint\t\t\tn_added;\n\t/* Events currently scheduled. */\n\tstruct perf_event\t*event[MAX_HWEVENTS];\n\t/* Event type of each scheduled event. */\n\tunsigned long\t\tevtype[MAX_HWEVENTS];\n\t/* Current index of each scheduled event; if not yet determined\n\t * contains PMC_NO_INDEX.\n\t */\n\tint\t\t\tcurrent_idx[MAX_HWEVENTS];\n\t/* The active PMCs' config for easy use with wrperfmon(). */\n\tunsigned long\t\tconfig;\n\t/* The active counters' indices for easy use with wrperfmon(). */\n\tunsigned long\t\tidx_mask;\n};\nDEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events);\n\n\n\n/*\n * A structure to hold the description of the PMCs available on a particular\n * type of Alpha CPU.\n */\nstruct alpha_pmu_t {\n\t/* Mapping of the perf system hw event types to indigenous event types */\n\tconst int *event_map;\n\t/* The number of entries in the event_map */\n\tint  max_events;\n\t/* The number of PMCs on this Alpha */\n\tint  num_pmcs;\n\t/*\n\t * All PMC counters reside in the IBOX register PCTR.  This is the\n\t * LSB of the counter.\n\t */\n\tint  pmc_count_shift[MAX_HWEVENTS];\n\t/*\n\t * The mask that isolates the PMC bits when the LSB of the counter\n\t * is shifted to bit 0.\n\t */\n\tunsigned long pmc_count_mask[MAX_HWEVENTS];\n\t/* The maximum period the PMC can count. */\n\tunsigned long pmc_max_period[MAX_HWEVENTS];\n\t/*\n\t * The maximum value that may be written to the counter due to\n\t * hardware restrictions is pmc_max_period - pmc_left.\n\t */\n\tlong pmc_left[3];\n\t /* Subroutine for allocation of PMCs.  Enforces constraints. */\n\tint (*check_constraints)(struct perf_event **, unsigned long *, int);\n};\n\n/*\n * The Alpha CPU PMU description currently in operation.  This is set during\n * the boot process to the specific CPU of the machine.\n */\nstatic const struct alpha_pmu_t *alpha_pmu;\n\n\n#define HW_OP_UNSUPPORTED -1\n\n/*\n * The hardware description of the EV67, EV68, EV69, EV7 and EV79 PMUs\n * follow. Since they are identical we refer to them collectively as the\n * EV67 henceforth.\n */\n\n/*\n * EV67 PMC event types\n *\n * There is no one-to-one mapping of the possible hw event types to the\n * actual codes that are used to program the PMCs hence we introduce our\n * own hw event type identifiers.\n */\nenum ev67_pmc_event_type {\n\tEV67_CYCLES = 1,\n\tEV67_INSTRUCTIONS,\n\tEV67_BCACHEMISS,\n\tEV67_MBOXREPLAY,\n\tEV67_LAST_ET\n};\n#define EV67_NUM_EVENT_TYPES (EV67_LAST_ET-EV67_CYCLES)\n\n\n/* Mapping of the hw event types to the perf tool interface */\nstatic const int ev67_perfmon_event_map[] = {\n\t[PERF_COUNT_HW_CPU_CYCLES]\t = EV67_CYCLES,\n\t[PERF_COUNT_HW_INSTRUCTIONS]\t = EV67_INSTRUCTIONS,\n\t[PERF_COUNT_HW_CACHE_REFERENCES] = HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_CACHE_MISSES]\t = EV67_BCACHEMISS,\n};\n\nstruct ev67_mapping_t {\n\tint config;\n\tint idx;\n};\n\n/*\n * The mapping used for one event only - these must be in same order as enum\n * ev67_pmc_event_type definition.\n */\nstatic const struct ev67_mapping_t ev67_mapping[] = {\n\t{EV67_PCTR_INSTR_CYCLES, 1},\t /* EV67_CYCLES, */\n\t{EV67_PCTR_INSTR_CYCLES, 0},\t /* EV67_INSTRUCTIONS */\n\t{EV67_PCTR_INSTR_BCACHEMISS, 1}, /* EV67_BCACHEMISS */\n\t{EV67_PCTR_CYCLES_MBOX, 1}\t /* EV67_MBOXREPLAY */\n};\n\n\n/*\n * Check that a group of events can be simultaneously scheduled on to the\n * EV67 PMU.  Also allocate counter indices and config.\n */\nstatic int ev67_check_constraints(struct perf_event **event,\n\t\t\t\tunsigned long *evtype, int n_ev)\n{\n\tint idx0;\n\tunsigned long config;\n\n\tidx0 = ev67_mapping[evtype[0]-1].idx;\n\tconfig = ev67_mapping[evtype[0]-1].config;\n\tif (n_ev == 1)\n\t\tgoto success;\n\n\tBUG_ON(n_ev != 2);\n\n\tif (evtype[0] == EV67_MBOXREPLAY || evtype[1] == EV67_MBOXREPLAY) {\n\t\t/* MBOX replay traps must be on PMC 1 */\n\t\tidx0 = (evtype[0] == EV67_MBOXREPLAY) ? 1 : 0;\n\t\t/* Only cycles can accompany MBOX replay traps */\n\t\tif (evtype[idx0] == EV67_CYCLES) {\n\t\t\tconfig = EV67_PCTR_CYCLES_MBOX;\n\t\t\tgoto success;\n\t\t}\n\t}\n\n\tif (evtype[0] == EV67_BCACHEMISS || evtype[1] == EV67_BCACHEMISS) {\n\t\t/* Bcache misses must be on PMC 1 */\n\t\tidx0 = (evtype[0] == EV67_BCACHEMISS) ? 1 : 0;\n\t\t/* Only instructions can accompany Bcache misses */\n\t\tif (evtype[idx0] == EV67_INSTRUCTIONS) {\n\t\t\tconfig = EV67_PCTR_INSTR_BCACHEMISS;\n\t\t\tgoto success;\n\t\t}\n\t}\n\n\tif (evtype[0] == EV67_INSTRUCTIONS || evtype[1] == EV67_INSTRUCTIONS) {\n\t\t/* Instructions must be on PMC 0 */\n\t\tidx0 = (evtype[0] == EV67_INSTRUCTIONS) ? 0 : 1;\n\t\t/* By this point only cycles can accompany instructions */\n\t\tif (evtype[idx0^1] == EV67_CYCLES) {\n\t\t\tconfig = EV67_PCTR_INSTR_CYCLES;\n\t\t\tgoto success;\n\t\t}\n\t}\n\n\t/* Otherwise, darn it, there is a conflict.  */\n\treturn -1;\n\nsuccess:\n\tevent[0]->hw.idx = idx0;\n\tevent[0]->hw.config_base = config;\n\tif (n_ev == 2) {\n\t\tevent[1]->hw.idx = idx0 ^ 1;\n\t\tevent[1]->hw.config_base = config;\n\t}\n\treturn 0;\n}\n\n\nstatic const struct alpha_pmu_t ev67_pmu = {\n\t.event_map = ev67_perfmon_event_map,\n\t.max_events = ARRAY_SIZE(ev67_perfmon_event_map),\n\t.num_pmcs = 2,\n\t.pmc_count_shift = {EV67_PCTR_0_COUNT_SHIFT, EV67_PCTR_1_COUNT_SHIFT, 0},\n\t.pmc_count_mask = {EV67_PCTR_0_COUNT_MASK,  EV67_PCTR_1_COUNT_MASK,  0},\n\t.pmc_max_period = {(1UL<<20) - 1, (1UL<<20) - 1, 0},\n\t.pmc_left = {16, 4, 0},\n\t.check_constraints = ev67_check_constraints\n};\n\n\n\n/*\n * Helper routines to ensure that we read/write only the correct PMC bits\n * when calling the wrperfmon PALcall.\n */\nstatic inline void alpha_write_pmc(int idx, unsigned long val)\n{\n\tval &= alpha_pmu->pmc_count_mask[idx];\n\tval <<= alpha_pmu->pmc_count_shift[idx];\n\tval |= (1<<idx);\n\twrperfmon(PERFMON_CMD_WRITE, val);\n}\n\nstatic inline unsigned long alpha_read_pmc(int idx)\n{\n\tunsigned long val;\n\n\tval = wrperfmon(PERFMON_CMD_READ, 0);\n\tval >>= alpha_pmu->pmc_count_shift[idx];\n\tval &= alpha_pmu->pmc_count_mask[idx];\n\treturn val;\n}\n\n/* Set a new period to sample over */\nstatic int alpha_perf_event_set_period(struct perf_event *event,\n\t\t\t\tstruct hw_perf_event *hwc, int idx)\n{\n\tlong left = local64_read(&hwc->period_left);\n\tlong period = hwc->sample_period;\n\tint ret = 0;\n\n\tif (unlikely(left <= -period)) {\n\t\tleft = period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\tif (unlikely(left <= 0)) {\n\t\tleft += period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\t/*\n\t * Hardware restrictions require that the counters must not be\n\t * written with values that are too close to the maximum period.\n\t */\n\tif (unlikely(left < alpha_pmu->pmc_left[idx]))\n\t\tleft = alpha_pmu->pmc_left[idx];\n\n\tif (left > (long)alpha_pmu->pmc_max_period[idx])\n\t\tleft = alpha_pmu->pmc_max_period[idx];\n\n\tlocal64_set(&hwc->prev_count, (unsigned long)(-left));\n\n\talpha_write_pmc(idx, (unsigned long)(-left));\n\n\tperf_event_update_userpage(event);\n\n\treturn ret;\n}\n\n\n/*\n * Calculates the count (the 'delta') since the last time the PMC was read.\n *\n * As the PMCs' full period can easily be exceeded within the perf system\n * sampling period we cannot use any high order bits as a guard bit in the\n * PMCs to detect overflow as is done by other architectures.  The code here\n * calculates the delta on the basis that there is no overflow when ovf is\n * zero.  The value passed via ovf by the interrupt handler corrects for\n * overflow.\n *\n * This can be racey on rare occasions -- a call to this routine can occur\n * with an overflowed counter just before the PMI service routine is called.\n * The check for delta negative hopefully always rectifies this situation.\n */\nstatic unsigned long alpha_perf_event_update(struct perf_event *event,\n\t\t\t\t\tstruct hw_perf_event *hwc, int idx, long ovf)\n{\n\tlong prev_raw_count, new_raw_count;\n\tlong delta;\n\nagain:\n\tprev_raw_count = local64_read(&hwc->prev_count);\n\tnew_raw_count = alpha_read_pmc(idx);\n\n\tif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\n\t\t\t     new_raw_count) != prev_raw_count)\n\t\tgoto again;\n\n\tdelta = (new_raw_count - (prev_raw_count & alpha_pmu->pmc_count_mask[idx])) + ovf;\n\n\t/* It is possible on very rare occasions that the PMC has overflowed\n\t * but the interrupt is yet to come.  Detect and fix this situation.\n\t */\n\tif (unlikely(delta < 0)) {\n\t\tdelta += alpha_pmu->pmc_max_period[idx] + 1;\n\t}\n\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &hwc->period_left);\n\n\treturn new_raw_count;\n}\n\n\n/*\n * Collect all HW events into the array event[].\n */\nstatic int collect_events(struct perf_event *group, int max_count,\n\t\t\t  struct perf_event *event[], unsigned long *evtype,\n\t\t\t  int *current_idx)\n{\n\tstruct perf_event *pe;\n\tint n = 0;\n\n\tif (!is_software_event(group)) {\n\t\tif (n >= max_count)\n\t\t\treturn -1;\n\t\tevent[n] = group;\n\t\tevtype[n] = group->hw.event_base;\n\t\tcurrent_idx[n++] = PMC_NO_INDEX;\n\t}\n\tlist_for_each_entry(pe, &group->sibling_list, group_entry) {\n\t\tif (!is_software_event(pe) && pe->state != PERF_EVENT_STATE_OFF) {\n\t\t\tif (n >= max_count)\n\t\t\t\treturn -1;\n\t\t\tevent[n] = pe;\n\t\t\tevtype[n] = pe->hw.event_base;\n\t\t\tcurrent_idx[n++] = PMC_NO_INDEX;\n\t\t}\n\t}\n\treturn n;\n}\n\n\n\n/*\n * Check that a group of events can be simultaneously scheduled on to the PMU.\n */\nstatic int alpha_check_constraints(struct perf_event **events,\n\t\t\t\t   unsigned long *evtypes, int n_ev)\n{\n\n\t/* No HW events is possible from hw_perf_group_sched_in(). */\n\tif (n_ev == 0)\n\t\treturn 0;\n\n\tif (n_ev > alpha_pmu->num_pmcs)\n\t\treturn -1;\n\n\treturn alpha_pmu->check_constraints(events, evtypes, n_ev);\n}\n\n\n/*\n * If new events have been scheduled then update cpuc with the new\n * configuration.  This may involve shifting cycle counts from one PMC to\n * another.\n */\nstatic void maybe_change_configuration(struct cpu_hw_events *cpuc)\n{\n\tint j;\n\n\tif (cpuc->n_added == 0)\n\t\treturn;\n\n\t/* Find counters that are moving to another PMC and update */\n\tfor (j = 0; j < cpuc->n_events; j++) {\n\t\tstruct perf_event *pe = cpuc->event[j];\n\n\t\tif (cpuc->current_idx[j] != PMC_NO_INDEX &&\n\t\t\tcpuc->current_idx[j] != pe->hw.idx) {\n\t\t\talpha_perf_event_update(pe, &pe->hw, cpuc->current_idx[j], 0);\n\t\t\tcpuc->current_idx[j] = PMC_NO_INDEX;\n\t\t}\n\t}\n\n\t/* Assign to counters all unassigned events. */\n\tcpuc->idx_mask = 0;\n\tfor (j = 0; j < cpuc->n_events; j++) {\n\t\tstruct perf_event *pe = cpuc->event[j];\n\t\tstruct hw_perf_event *hwc = &pe->hw;\n\t\tint idx = hwc->idx;\n\n\t\tif (cpuc->current_idx[j] == PMC_NO_INDEX) {\n\t\t\talpha_perf_event_set_period(pe, hwc, idx);\n\t\t\tcpuc->current_idx[j] = idx;\n\t\t}\n\n\t\tif (!(hwc->state & PERF_HES_STOPPED))\n\t\t\tcpuc->idx_mask |= (1<<cpuc->current_idx[j]);\n\t}\n\tcpuc->config = cpuc->event[0]->hw.config_base;\n}\n\n\n\n/* Schedule perf HW event on to PMU.\n *  - this function is called from outside this module via the pmu struct\n *    returned from perf event initialisation.\n */\nstatic int alpha_pmu_add(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint n0;\n\tint ret;\n\tunsigned long irq_flags;\n\n\t/*\n\t * The Sparc code has the IRQ disable first followed by the perf\n\t * disable, however this can lead to an overflowed counter with the\n\t * PMI disabled on rare occasions.  The alpha_perf_event_update()\n\t * routine should detect this situation by noting a negative delta,\n\t * nevertheless we disable the PMCs first to enable a potential\n\t * final PMI to occur before we disable interrupts.\n\t */\n\tperf_pmu_disable(event->pmu);\n\tlocal_irq_save(irq_flags);\n\n\t/* Default to error to be returned */\n\tret = -EAGAIN;\n\n\t/* Insert event on to PMU and if successful modify ret to valid return */\n\tn0 = cpuc->n_events;\n\tif (n0 < alpha_pmu->num_pmcs) {\n\t\tcpuc->event[n0] = event;\n\t\tcpuc->evtype[n0] = event->hw.event_base;\n\t\tcpuc->current_idx[n0] = PMC_NO_INDEX;\n\n\t\tif (!alpha_check_constraints(cpuc->event, cpuc->evtype, n0+1)) {\n\t\t\tcpuc->n_events++;\n\t\t\tcpuc->n_added++;\n\t\t\tret = 0;\n\t\t}\n\t}\n\n\thwc->state = PERF_HES_UPTODATE;\n\tif (!(flags & PERF_EF_START))\n\t\thwc->state |= PERF_HES_STOPPED;\n\n\tlocal_irq_restore(irq_flags);\n\tperf_pmu_enable(event->pmu);\n\n\treturn ret;\n}\n\n\n\n/* Disable performance monitoring unit\n *  - this function is called from outside this module via the pmu struct\n *    returned from perf event initialisation.\n */\nstatic void alpha_pmu_del(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tunsigned long irq_flags;\n\tint j;\n\n\tperf_pmu_disable(event->pmu);\n\tlocal_irq_save(irq_flags);\n\n\tfor (j = 0; j < cpuc->n_events; j++) {\n\t\tif (event == cpuc->event[j]) {\n\t\t\tint idx = cpuc->current_idx[j];\n\n\t\t\t/* Shift remaining entries down into the existing\n\t\t\t * slot.\n\t\t\t */\n\t\t\twhile (++j < cpuc->n_events) {\n\t\t\t\tcpuc->event[j - 1] = cpuc->event[j];\n\t\t\t\tcpuc->evtype[j - 1] = cpuc->evtype[j];\n\t\t\t\tcpuc->current_idx[j - 1] =\n\t\t\t\t\tcpuc->current_idx[j];\n\t\t\t}\n\n\t\t\t/* Absorb the final count and turn off the event. */\n\t\t\talpha_perf_event_update(event, hwc, idx, 0);\n\t\t\tperf_event_update_userpage(event);\n\n\t\t\tcpuc->idx_mask &= ~(1UL<<idx);\n\t\t\tcpuc->n_events--;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tlocal_irq_restore(irq_flags);\n\tperf_pmu_enable(event->pmu);\n}\n\n\nstatic void alpha_pmu_read(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\talpha_perf_event_update(event, hwc, hwc->idx, 0);\n}\n\n\nstatic void alpha_pmu_stop(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\tif (!(hwc->state & PERF_HES_STOPPED)) {\n\t\tcpuc->idx_mask &= ~(1UL<<hwc->idx);\n\t\thwc->state |= PERF_HES_STOPPED;\n\t}\n\n\tif ((flags & PERF_EF_UPDATE) && !(hwc->state & PERF_HES_UPTODATE)) {\n\t\talpha_perf_event_update(event, hwc, hwc->idx, 0);\n\t\thwc->state |= PERF_HES_UPTODATE;\n\t}\n\n\tif (cpuc->enabled)\n\t\twrperfmon(PERFMON_CMD_DISABLE, (1UL<<hwc->idx));\n}\n\n\nstatic void alpha_pmu_start(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\tif (WARN_ON_ONCE(!(hwc->state & PERF_HES_STOPPED)))\n\t\treturn;\n\n\tif (flags & PERF_EF_RELOAD) {\n\t\tWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\n\t\talpha_perf_event_set_period(event, hwc, hwc->idx);\n\t}\n\n\thwc->state = 0;\n\n\tcpuc->idx_mask |= 1UL<<hwc->idx;\n\tif (cpuc->enabled)\n\t\twrperfmon(PERFMON_CMD_ENABLE, (1UL<<hwc->idx));\n}\n\n\n/*\n * Check that CPU performance counters are supported.\n * - currently support EV67 and later CPUs.\n * - actually some later revisions of the EV6 have the same PMC model as the\n *     EV67 but we don't do suffiently deep CPU detection to detect them.\n *     Bad luck to the very few people who might have one, I guess.\n */\nstatic int supported_cpu(void)\n{\n\tstruct percpu_struct *cpu;\n\tunsigned long cputype;\n\n\t/* Get cpu type from HW */\n\tcpu = (struct percpu_struct *)((char *)hwrpb + hwrpb->processor_offset);\n\tcputype = cpu->type & 0xffffffff;\n\t/* Include all of EV67, EV68, EV7, EV79 and EV69 as supported. */\n\treturn (cputype >= EV67_CPU) && (cputype <= EV69_CPU);\n}\n\n\n\nstatic void hw_perf_event_destroy(struct perf_event *event)\n{\n\t/* Nothing to be done! */\n\treturn;\n}\n\n\n\nstatic int __hw_perf_event_init(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct perf_event *evts[MAX_HWEVENTS];\n\tunsigned long evtypes[MAX_HWEVENTS];\n\tint idx_rubbish_bin[MAX_HWEVENTS];\n\tint ev;\n\tint n;\n\n\t/* We only support a limited range of HARDWARE event types with one\n\t * only programmable via a RAW event type.\n\t */\n\tif (attr->type == PERF_TYPE_HARDWARE) {\n\t\tif (attr->config >= alpha_pmu->max_events)\n\t\t\treturn -EINVAL;\n\t\tev = alpha_pmu->event_map[attr->config];\n\t} else if (attr->type == PERF_TYPE_HW_CACHE) {\n\t\treturn -EOPNOTSUPP;\n\t} else if (attr->type == PERF_TYPE_RAW) {\n\t\tev = attr->config & 0xff;\n\t} else {\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (ev < 0) {\n\t\treturn ev;\n\t}\n\n\t/* The EV67 does not support mode exclusion */\n\tif (attr->exclude_kernel || attr->exclude_user\n\t\t\t|| attr->exclude_hv || attr->exclude_idle) {\n\t\treturn -EPERM;\n\t}\n\n\t/*\n\t * We place the event type in event_base here and leave calculation\n\t * of the codes to programme the PMU for alpha_pmu_enable() because\n\t * it is only then we will know what HW events are actually\n\t * scheduled on to the PMU.  At that point the code to programme the\n\t * PMU is put into config_base and the PMC to use is placed into\n\t * idx.  We initialise idx (below) to PMC_NO_INDEX to indicate that\n\t * it is yet to be determined.\n\t */\n\thwc->event_base = ev;\n\n\t/* Collect events in a group together suitable for calling\n\t * alpha_check_constraints() to verify that the group as a whole can\n\t * be scheduled on to the PMU.\n\t */\n\tn = 0;\n\tif (event->group_leader != event) {\n\t\tn = collect_events(event->group_leader,\n\t\t\t\talpha_pmu->num_pmcs - 1,\n\t\t\t\tevts, evtypes, idx_rubbish_bin);\n\t\tif (n < 0)\n\t\t\treturn -EINVAL;\n\t}\n\tevtypes[n] = hwc->event_base;\n\tevts[n] = event;\n\n\tif (alpha_check_constraints(evts, evtypes, n + 1))\n\t\treturn -EINVAL;\n\n\t/* Indicate that PMU config and idx are yet to be determined. */\n\thwc->config_base = 0;\n\thwc->idx = PMC_NO_INDEX;\n\n\tevent->destroy = hw_perf_event_destroy;\n\n\t/*\n\t * Most architectures reserve the PMU for their use at this point.\n\t * As there is no existing mechanism to arbitrate usage and there\n\t * appears to be no other user of the Alpha PMU we just assume\n\t * that we can just use it, hence a NO-OP here.\n\t *\n\t * Maybe an alpha_reserve_pmu() routine should be implemented but is\n\t * anything else ever going to use it?\n\t */\n\n\tif (!hwc->sample_period) {\n\t\thwc->sample_period = alpha_pmu->pmc_max_period[0];\n\t\thwc->last_period = hwc->sample_period;\n\t\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\t}\n\n\treturn 0;\n}\n\n/*\n * Main entry point to initialise a HW performance event.\n */\nstatic int alpha_pmu_event_init(struct perf_event *event)\n{\n\tint err;\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_RAW:\n\tcase PERF_TYPE_HARDWARE:\n\tcase PERF_TYPE_HW_CACHE:\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\tif (!alpha_pmu)\n\t\treturn -ENODEV;\n\n\t/* Do the real initialisation work. */\n\terr = __hw_perf_event_init(event);\n\n\treturn err;\n}\n\n/*\n * Main entry point - enable HW performance counters.\n */\nstatic void alpha_pmu_enable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\tif (cpuc->enabled)\n\t\treturn;\n\n\tcpuc->enabled = 1;\n\tbarrier();\n\n\tif (cpuc->n_events > 0) {\n\t\t/* Update cpuc with information from any new scheduled events. */\n\t\tmaybe_change_configuration(cpuc);\n\n\t\t/* Start counting the desired events. */\n\t\twrperfmon(PERFMON_CMD_LOGGING_OPTIONS, EV67_PCTR_MODE_AGGREGATE);\n\t\twrperfmon(PERFMON_CMD_DESIRED_EVENTS, cpuc->config);\n\t\twrperfmon(PERFMON_CMD_ENABLE, cpuc->idx_mask);\n\t}\n}\n\n\n/*\n * Main entry point - disable HW performance counters.\n */\n\nstatic void alpha_pmu_disable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\tif (!cpuc->enabled)\n\t\treturn;\n\n\tcpuc->enabled = 0;\n\tcpuc->n_added = 0;\n\n\twrperfmon(PERFMON_CMD_DISABLE, cpuc->idx_mask);\n}\n\nstatic struct pmu pmu = {\n\t.pmu_enable\t= alpha_pmu_enable,\n\t.pmu_disable\t= alpha_pmu_disable,\n\t.event_init\t= alpha_pmu_event_init,\n\t.add\t\t= alpha_pmu_add,\n\t.del\t\t= alpha_pmu_del,\n\t.start\t\t= alpha_pmu_start,\n\t.stop\t\t= alpha_pmu_stop,\n\t.read\t\t= alpha_pmu_read,\n};\n\n\n/*\n * Main entry point - don't know when this is called but it\n * obviously dumps debug info.\n */\nvoid perf_event_print_debug(void)\n{\n\tunsigned long flags;\n\tunsigned long pcr;\n\tint pcr0, pcr1;\n\tint cpu;\n\n\tif (!supported_cpu())\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tcpu = smp_processor_id();\n\n\tpcr = wrperfmon(PERFMON_CMD_READ, 0);\n\tpcr0 = (pcr >> alpha_pmu->pmc_count_shift[0]) & alpha_pmu->pmc_count_mask[0];\n\tpcr1 = (pcr >> alpha_pmu->pmc_count_shift[1]) & alpha_pmu->pmc_count_mask[1];\n\n\tpr_info(\"CPU#%d: PCTR0[%06x] PCTR1[%06x]\\n\", cpu, pcr0, pcr1);\n\n\tlocal_irq_restore(flags);\n}\n\n\n/*\n * Performance Monitoring Interrupt Service Routine called when a PMC\n * overflows.  The PMC that overflowed is passed in la_ptr.\n */\nstatic void alpha_perf_event_irq_handler(unsigned long la_ptr,\n\t\t\t\t\tstruct pt_regs *regs)\n{\n\tstruct cpu_hw_events *cpuc;\n\tstruct perf_sample_data data;\n\tstruct perf_event *event;\n\tstruct hw_perf_event *hwc;\n\tint idx, j;\n\n\t__get_cpu_var(irq_pmi_count)++;\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\n\t/* Completely counting through the PMC's period to trigger a new PMC\n\t * overflow interrupt while in this interrupt routine is utterly\n\t * disastrous!  The EV6 and EV67 counters are sufficiently large to\n\t * prevent this but to be really sure disable the PMCs.\n\t */\n\twrperfmon(PERFMON_CMD_DISABLE, cpuc->idx_mask);\n\n\t/* la_ptr is the counter that overflowed. */\n\tif (unlikely(la_ptr >= alpha_pmu->num_pmcs)) {\n\t\t/* This should never occur! */\n\t\tirq_err_count++;\n\t\tpr_warning(\"PMI: silly index %ld\\n\", la_ptr);\n\t\twrperfmon(PERFMON_CMD_ENABLE, cpuc->idx_mask);\n\t\treturn;\n\t}\n\n\tidx = la_ptr;\n\n\tperf_sample_data_init(&data, 0);\n\tfor (j = 0; j < cpuc->n_events; j++) {\n\t\tif (cpuc->current_idx[j] == idx)\n\t\t\tbreak;\n\t}\n\n\tif (unlikely(j == cpuc->n_events)) {\n\t\t/* This can occur if the event is disabled right on a PMC overflow. */\n\t\twrperfmon(PERFMON_CMD_ENABLE, cpuc->idx_mask);\n\t\treturn;\n\t}\n\n\tevent = cpuc->event[j];\n\n\tif (unlikely(!event)) {\n\t\t/* This should never occur! */\n\t\tirq_err_count++;\n\t\tpr_warning(\"PMI: No event at index %d!\\n\", idx);\n\t\twrperfmon(PERFMON_CMD_ENABLE, cpuc->idx_mask);\n\t\treturn;\n\t}\n\n\thwc = &event->hw;\n\talpha_perf_event_update(event, hwc, idx, alpha_pmu->pmc_max_period[idx]+1);\n\tdata.period = event->hw.last_period;\n\n\tif (alpha_perf_event_set_period(event, hwc, idx)) {\n\t\tif (perf_event_overflow(event, &data, regs)) {\n\t\t\t/* Interrupts coming too quickly; \"throttle\" the\n\t\t\t * counter, i.e., disable it for a little while.\n\t\t\t */\n\t\t\talpha_pmu_stop(event, 0);\n\t\t}\n\t}\n\twrperfmon(PERFMON_CMD_ENABLE, cpuc->idx_mask);\n\n\treturn;\n}\n\n\n\n/*\n * Init call to initialise performance events at kernel startup.\n */\nint __init init_hw_perf_events(void)\n{\n\tpr_info(\"Performance events: \");\n\n\tif (!supported_cpu()) {\n\t\tpr_cont(\"No support for your CPU.\\n\");\n\t\treturn 0;\n\t}\n\n\tpr_cont(\"Supported CPU type!\\n\");\n\n\t/* Override performance counter IRQ vector */\n\n\tperf_irq = alpha_perf_event_irq_handler;\n\n\t/* And set up PMU specification */\n\talpha_pmu = &ev67_pmu;\n\n\tperf_pmu_register(&pmu, \"cpu\", PERF_TYPE_RAW);\n\n\treturn 0;\n}\nearly_initcall(init_hw_perf_events);\n", "/*\n * ARMv6 Performance counter handling code.\n *\n * Copyright (C) 2009 picoChip Designs, Ltd., Jamie Iles\n *\n * ARMv6 has 2 configurable performance counters and a single cycle counter.\n * They all share a single reset bit but can be written to zero so we can use\n * that for a reset.\n *\n * The counters can't be individually enabled or disabled so when we remove\n * one event and replace it with another we could get spurious counts from the\n * wrong event. However, we can take advantage of the fact that the\n * performance counters can export events to the event bus, and the event bus\n * itself can be monitored. This requires that we *don't* export the events to\n * the event bus. The procedure for disabling a configurable counter is:\n *\t- change the counter to count the ETMEXTOUT[0] signal (0x20). This\n *\t  effectively stops the counter from counting.\n *\t- disable the counter's interrupt generation (each counter has it's\n *\t  own interrupt enable bit).\n * Once stopped, the counter value can be written as 0 to reset.\n *\n * To enable a counter:\n *\t- enable the counter's interrupt generation.\n *\t- set the new event type.\n *\n * Note: the dedicated cycle counter only counts cycles and can't be\n * enabled/disabled independently of the others. When we want to disable the\n * cycle counter, we have to just disable the interrupt reporting and start\n * ignoring that counter. When re-enabling, we have to reset the value and\n * enable the interrupt.\n */\n\n#if defined(CONFIG_CPU_V6) || defined(CONFIG_CPU_V6K)\nenum armv6_perf_types {\n\tARMV6_PERFCTR_ICACHE_MISS\t    = 0x0,\n\tARMV6_PERFCTR_IBUF_STALL\t    = 0x1,\n\tARMV6_PERFCTR_DDEP_STALL\t    = 0x2,\n\tARMV6_PERFCTR_ITLB_MISS\t\t    = 0x3,\n\tARMV6_PERFCTR_DTLB_MISS\t\t    = 0x4,\n\tARMV6_PERFCTR_BR_EXEC\t\t    = 0x5,\n\tARMV6_PERFCTR_BR_MISPREDICT\t    = 0x6,\n\tARMV6_PERFCTR_INSTR_EXEC\t    = 0x7,\n\tARMV6_PERFCTR_DCACHE_HIT\t    = 0x9,\n\tARMV6_PERFCTR_DCACHE_ACCESS\t    = 0xA,\n\tARMV6_PERFCTR_DCACHE_MISS\t    = 0xB,\n\tARMV6_PERFCTR_DCACHE_WBACK\t    = 0xC,\n\tARMV6_PERFCTR_SW_PC_CHANGE\t    = 0xD,\n\tARMV6_PERFCTR_MAIN_TLB_MISS\t    = 0xF,\n\tARMV6_PERFCTR_EXPL_D_ACCESS\t    = 0x10,\n\tARMV6_PERFCTR_LSU_FULL_STALL\t    = 0x11,\n\tARMV6_PERFCTR_WBUF_DRAINED\t    = 0x12,\n\tARMV6_PERFCTR_CPU_CYCLES\t    = 0xFF,\n\tARMV6_PERFCTR_NOP\t\t    = 0x20,\n};\n\nenum armv6_counters {\n\tARMV6_CYCLE_COUNTER = 1,\n\tARMV6_COUNTER0,\n\tARMV6_COUNTER1,\n};\n\n/*\n * The hardware events that we support. We do support cache operations but\n * we have harvard caches and no way to combine instruction and data\n * accesses/misses in hardware.\n */\nstatic const unsigned armv6_perf_map[PERF_COUNT_HW_MAX] = {\n\t[PERF_COUNT_HW_CPU_CYCLES]\t    = ARMV6_PERFCTR_CPU_CYCLES,\n\t[PERF_COUNT_HW_INSTRUCTIONS]\t    = ARMV6_PERFCTR_INSTR_EXEC,\n\t[PERF_COUNT_HW_CACHE_REFERENCES]    = HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_CACHE_MISSES]\t    = HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_BRANCH_INSTRUCTIONS] = ARMV6_PERFCTR_BR_EXEC,\n\t[PERF_COUNT_HW_BRANCH_MISSES]\t    = ARMV6_PERFCTR_BR_MISPREDICT,\n\t[PERF_COUNT_HW_BUS_CYCLES]\t    = HW_OP_UNSUPPORTED,\n};\n\nstatic const unsigned armv6_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t  [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t  [PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\t[C(L1D)] = {\n\t\t/*\n\t\t * The performance counters don't differentiate between read\n\t\t * and write accesses/misses so this isn't strictly correct,\n\t\t * but it's the best we can do. Writes and reads get\n\t\t * combined.\n\t\t */\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV6_PERFCTR_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV6_PERFCTR_DCACHE_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV6_PERFCTR_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV6_PERFCTR_DCACHE_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(L1I)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV6_PERFCTR_ICACHE_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV6_PERFCTR_ICACHE_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(LL)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(DTLB)] = {\n\t\t/*\n\t\t * The ARM performance counters can count micro DTLB misses,\n\t\t * micro ITLB misses and main TLB misses. There isn't an event\n\t\t * for TLB misses, so use the micro misses here and if users\n\t\t * want the main TLB misses they can use a raw counter.\n\t\t */\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV6_PERFCTR_DTLB_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV6_PERFCTR_DTLB_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(ITLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV6_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV6_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(BPU)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n};\n\nenum armv6mpcore_perf_types {\n\tARMV6MPCORE_PERFCTR_ICACHE_MISS\t    = 0x0,\n\tARMV6MPCORE_PERFCTR_IBUF_STALL\t    = 0x1,\n\tARMV6MPCORE_PERFCTR_DDEP_STALL\t    = 0x2,\n\tARMV6MPCORE_PERFCTR_ITLB_MISS\t    = 0x3,\n\tARMV6MPCORE_PERFCTR_DTLB_MISS\t    = 0x4,\n\tARMV6MPCORE_PERFCTR_BR_EXEC\t    = 0x5,\n\tARMV6MPCORE_PERFCTR_BR_NOTPREDICT   = 0x6,\n\tARMV6MPCORE_PERFCTR_BR_MISPREDICT   = 0x7,\n\tARMV6MPCORE_PERFCTR_INSTR_EXEC\t    = 0x8,\n\tARMV6MPCORE_PERFCTR_DCACHE_RDACCESS = 0xA,\n\tARMV6MPCORE_PERFCTR_DCACHE_RDMISS   = 0xB,\n\tARMV6MPCORE_PERFCTR_DCACHE_WRACCESS = 0xC,\n\tARMV6MPCORE_PERFCTR_DCACHE_WRMISS   = 0xD,\n\tARMV6MPCORE_PERFCTR_DCACHE_EVICTION = 0xE,\n\tARMV6MPCORE_PERFCTR_SW_PC_CHANGE    = 0xF,\n\tARMV6MPCORE_PERFCTR_MAIN_TLB_MISS   = 0x10,\n\tARMV6MPCORE_PERFCTR_EXPL_MEM_ACCESS = 0x11,\n\tARMV6MPCORE_PERFCTR_LSU_FULL_STALL  = 0x12,\n\tARMV6MPCORE_PERFCTR_WBUF_DRAINED    = 0x13,\n\tARMV6MPCORE_PERFCTR_CPU_CYCLES\t    = 0xFF,\n};\n\n/*\n * The hardware events that we support. We do support cache operations but\n * we have harvard caches and no way to combine instruction and data\n * accesses/misses in hardware.\n */\nstatic const unsigned armv6mpcore_perf_map[PERF_COUNT_HW_MAX] = {\n\t[PERF_COUNT_HW_CPU_CYCLES]\t    = ARMV6MPCORE_PERFCTR_CPU_CYCLES,\n\t[PERF_COUNT_HW_INSTRUCTIONS]\t    = ARMV6MPCORE_PERFCTR_INSTR_EXEC,\n\t[PERF_COUNT_HW_CACHE_REFERENCES]    = HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_CACHE_MISSES]\t    = HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_BRANCH_INSTRUCTIONS] = ARMV6MPCORE_PERFCTR_BR_EXEC,\n\t[PERF_COUNT_HW_BRANCH_MISSES]\t    = ARMV6MPCORE_PERFCTR_BR_MISPREDICT,\n\t[PERF_COUNT_HW_BUS_CYCLES]\t    = HW_OP_UNSUPPORTED,\n};\n\nstatic const unsigned armv6mpcore_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\t[C(L1D)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]  =\n\t\t\t\tARMV6MPCORE_PERFCTR_DCACHE_RDACCESS,\n\t\t\t[C(RESULT_MISS)]    =\n\t\t\t\tARMV6MPCORE_PERFCTR_DCACHE_RDMISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]  =\n\t\t\t\tARMV6MPCORE_PERFCTR_DCACHE_WRACCESS,\n\t\t\t[C(RESULT_MISS)]    =\n\t\t\t\tARMV6MPCORE_PERFCTR_DCACHE_WRMISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(L1I)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = ARMV6MPCORE_PERFCTR_ICACHE_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = ARMV6MPCORE_PERFCTR_ICACHE_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(LL)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(DTLB)] = {\n\t\t/*\n\t\t * The ARM performance counters can count micro DTLB misses,\n\t\t * micro ITLB misses and main TLB misses. There isn't an event\n\t\t * for TLB misses, so use the micro misses here and if users\n\t\t * want the main TLB misses they can use a raw counter.\n\t\t */\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = ARMV6MPCORE_PERFCTR_DTLB_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = ARMV6MPCORE_PERFCTR_DTLB_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(ITLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = ARMV6MPCORE_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = ARMV6MPCORE_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(BPU)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n};\n\nstatic inline unsigned long\narmv6_pmcr_read(void)\n{\n\tu32 val;\n\tasm volatile(\"mrc   p15, 0, %0, c15, c12, 0\" : \"=r\"(val));\n\treturn val;\n}\n\nstatic inline void\narmv6_pmcr_write(unsigned long val)\n{\n\tasm volatile(\"mcr   p15, 0, %0, c15, c12, 0\" : : \"r\"(val));\n}\n\n#define ARMV6_PMCR_ENABLE\t\t(1 << 0)\n#define ARMV6_PMCR_CTR01_RESET\t\t(1 << 1)\n#define ARMV6_PMCR_CCOUNT_RESET\t\t(1 << 2)\n#define ARMV6_PMCR_CCOUNT_DIV\t\t(1 << 3)\n#define ARMV6_PMCR_COUNT0_IEN\t\t(1 << 4)\n#define ARMV6_PMCR_COUNT1_IEN\t\t(1 << 5)\n#define ARMV6_PMCR_CCOUNT_IEN\t\t(1 << 6)\n#define ARMV6_PMCR_COUNT0_OVERFLOW\t(1 << 8)\n#define ARMV6_PMCR_COUNT1_OVERFLOW\t(1 << 9)\n#define ARMV6_PMCR_CCOUNT_OVERFLOW\t(1 << 10)\n#define ARMV6_PMCR_EVT_COUNT0_SHIFT\t20\n#define ARMV6_PMCR_EVT_COUNT0_MASK\t(0xFF << ARMV6_PMCR_EVT_COUNT0_SHIFT)\n#define ARMV6_PMCR_EVT_COUNT1_SHIFT\t12\n#define ARMV6_PMCR_EVT_COUNT1_MASK\t(0xFF << ARMV6_PMCR_EVT_COUNT1_SHIFT)\n\n#define ARMV6_PMCR_OVERFLOWED_MASK \\\n\t(ARMV6_PMCR_COUNT0_OVERFLOW | ARMV6_PMCR_COUNT1_OVERFLOW | \\\n\t ARMV6_PMCR_CCOUNT_OVERFLOW)\n\nstatic inline int\narmv6_pmcr_has_overflowed(unsigned long pmcr)\n{\n\treturn pmcr & ARMV6_PMCR_OVERFLOWED_MASK;\n}\n\nstatic inline int\narmv6_pmcr_counter_has_overflowed(unsigned long pmcr,\n\t\t\t\t  enum armv6_counters counter)\n{\n\tint ret = 0;\n\n\tif (ARMV6_CYCLE_COUNTER == counter)\n\t\tret = pmcr & ARMV6_PMCR_CCOUNT_OVERFLOW;\n\telse if (ARMV6_COUNTER0 == counter)\n\t\tret = pmcr & ARMV6_PMCR_COUNT0_OVERFLOW;\n\telse if (ARMV6_COUNTER1 == counter)\n\t\tret = pmcr & ARMV6_PMCR_COUNT1_OVERFLOW;\n\telse\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", counter);\n\n\treturn ret;\n}\n\nstatic inline u32\narmv6pmu_read_counter(int counter)\n{\n\tunsigned long value = 0;\n\n\tif (ARMV6_CYCLE_COUNTER == counter)\n\t\tasm volatile(\"mrc   p15, 0, %0, c15, c12, 1\" : \"=r\"(value));\n\telse if (ARMV6_COUNTER0 == counter)\n\t\tasm volatile(\"mrc   p15, 0, %0, c15, c12, 2\" : \"=r\"(value));\n\telse if (ARMV6_COUNTER1 == counter)\n\t\tasm volatile(\"mrc   p15, 0, %0, c15, c12, 3\" : \"=r\"(value));\n\telse\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", counter);\n\n\treturn value;\n}\n\nstatic inline void\narmv6pmu_write_counter(int counter,\n\t\t       u32 value)\n{\n\tif (ARMV6_CYCLE_COUNTER == counter)\n\t\tasm volatile(\"mcr   p15, 0, %0, c15, c12, 1\" : : \"r\"(value));\n\telse if (ARMV6_COUNTER0 == counter)\n\t\tasm volatile(\"mcr   p15, 0, %0, c15, c12, 2\" : : \"r\"(value));\n\telse if (ARMV6_COUNTER1 == counter)\n\t\tasm volatile(\"mcr   p15, 0, %0, c15, c12, 3\" : : \"r\"(value));\n\telse\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", counter);\n}\n\nstatic void\narmv6pmu_enable_event(struct hw_perf_event *hwc,\n\t\t      int idx)\n{\n\tunsigned long val, mask, evt, flags;\n\n\tif (ARMV6_CYCLE_COUNTER == idx) {\n\t\tmask\t= 0;\n\t\tevt\t= ARMV6_PMCR_CCOUNT_IEN;\n\t} else if (ARMV6_COUNTER0 == idx) {\n\t\tmask\t= ARMV6_PMCR_EVT_COUNT0_MASK;\n\t\tevt\t= (hwc->config_base << ARMV6_PMCR_EVT_COUNT0_SHIFT) |\n\t\t\t  ARMV6_PMCR_COUNT0_IEN;\n\t} else if (ARMV6_COUNTER1 == idx) {\n\t\tmask\t= ARMV6_PMCR_EVT_COUNT1_MASK;\n\t\tevt\t= (hwc->config_base << ARMV6_PMCR_EVT_COUNT1_SHIFT) |\n\t\t\t  ARMV6_PMCR_COUNT1_IEN;\n\t} else {\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", idx);\n\t\treturn;\n\t}\n\n\t/*\n\t * Mask out the current event and set the counter to count the event\n\t * that we're interested in.\n\t */\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = armv6_pmcr_read();\n\tval &= ~mask;\n\tval |= evt;\n\tarmv6_pmcr_write(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic irqreturn_t\narmv6pmu_handle_irq(int irq_num,\n\t\t    void *dev)\n{\n\tunsigned long pmcr = armv6_pmcr_read();\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct pt_regs *regs;\n\tint idx;\n\n\tif (!armv6_pmcr_has_overflowed(pmcr))\n\t\treturn IRQ_NONE;\n\n\tregs = get_irq_regs();\n\n\t/*\n\t * The interrupts are cleared by writing the overflow flags back to\n\t * the control register. All of the other bits don't have any effect\n\t * if they are rewritten, so write the whole value back.\n\t */\n\tarmv6_pmcr_write(pmcr);\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\tfor (idx = 0; idx <= armpmu->num_events; ++idx) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tstruct hw_perf_event *hwc;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * We have a single interrupt for all counters. Check that\n\t\t * each counter has overflowed before we process it.\n\t\t */\n\t\tif (!armv6_pmcr_counter_has_overflowed(pmcr, idx))\n\t\t\tcontinue;\n\n\t\thwc = &event->hw;\n\t\tarmpmu_event_update(event, hwc, idx, 1);\n\t\tdata.period = event->hw.last_period;\n\t\tif (!armpmu_event_set_period(event, hwc, idx))\n\t\t\tcontinue;\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tarmpmu->disable(hwc, idx);\n\t}\n\n\t/*\n\t * Handle the pending perf events.\n\t *\n\t * Note: this call *must* be run with interrupts disabled. For\n\t * platforms that can have the PMU interrupts raised as an NMI, this\n\t * will not work.\n\t */\n\tirq_work_run();\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void\narmv6pmu_start(void)\n{\n\tunsigned long flags, val;\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = armv6_pmcr_read();\n\tval |= ARMV6_PMCR_ENABLE;\n\tarmv6_pmcr_write(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic void\narmv6pmu_stop(void)\n{\n\tunsigned long flags, val;\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = armv6_pmcr_read();\n\tval &= ~ARMV6_PMCR_ENABLE;\n\tarmv6_pmcr_write(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic int\narmv6pmu_get_event_idx(struct cpu_hw_events *cpuc,\n\t\t       struct hw_perf_event *event)\n{\n\t/* Always place a cycle counter into the cycle counter. */\n\tif (ARMV6_PERFCTR_CPU_CYCLES == event->config_base) {\n\t\tif (test_and_set_bit(ARMV6_CYCLE_COUNTER, cpuc->used_mask))\n\t\t\treturn -EAGAIN;\n\n\t\treturn ARMV6_CYCLE_COUNTER;\n\t} else {\n\t\t/*\n\t\t * For anything other than a cycle counter, try and use\n\t\t * counter0 and counter1.\n\t\t */\n\t\tif (!test_and_set_bit(ARMV6_COUNTER1, cpuc->used_mask))\n\t\t\treturn ARMV6_COUNTER1;\n\n\t\tif (!test_and_set_bit(ARMV6_COUNTER0, cpuc->used_mask))\n\t\t\treturn ARMV6_COUNTER0;\n\n\t\t/* The counters are all in use. */\n\t\treturn -EAGAIN;\n\t}\n}\n\nstatic void\narmv6pmu_disable_event(struct hw_perf_event *hwc,\n\t\t       int idx)\n{\n\tunsigned long val, mask, evt, flags;\n\n\tif (ARMV6_CYCLE_COUNTER == idx) {\n\t\tmask\t= ARMV6_PMCR_CCOUNT_IEN;\n\t\tevt\t= 0;\n\t} else if (ARMV6_COUNTER0 == idx) {\n\t\tmask\t= ARMV6_PMCR_COUNT0_IEN | ARMV6_PMCR_EVT_COUNT0_MASK;\n\t\tevt\t= ARMV6_PERFCTR_NOP << ARMV6_PMCR_EVT_COUNT0_SHIFT;\n\t} else if (ARMV6_COUNTER1 == idx) {\n\t\tmask\t= ARMV6_PMCR_COUNT1_IEN | ARMV6_PMCR_EVT_COUNT1_MASK;\n\t\tevt\t= ARMV6_PERFCTR_NOP << ARMV6_PMCR_EVT_COUNT1_SHIFT;\n\t} else {\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", idx);\n\t\treturn;\n\t}\n\n\t/*\n\t * Mask out the current event and set the counter to count the number\n\t * of ETM bus signal assertion cycles. The external reporting should\n\t * be disabled and so this should never increment.\n\t */\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = armv6_pmcr_read();\n\tval &= ~mask;\n\tval |= evt;\n\tarmv6_pmcr_write(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic void\narmv6mpcore_pmu_disable_event(struct hw_perf_event *hwc,\n\t\t\t      int idx)\n{\n\tunsigned long val, mask, flags, evt = 0;\n\n\tif (ARMV6_CYCLE_COUNTER == idx) {\n\t\tmask\t= ARMV6_PMCR_CCOUNT_IEN;\n\t} else if (ARMV6_COUNTER0 == idx) {\n\t\tmask\t= ARMV6_PMCR_COUNT0_IEN;\n\t} else if (ARMV6_COUNTER1 == idx) {\n\t\tmask\t= ARMV6_PMCR_COUNT1_IEN;\n\t} else {\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", idx);\n\t\treturn;\n\t}\n\n\t/*\n\t * Unlike UP ARMv6, we don't have a way of stopping the counters. We\n\t * simply disable the interrupt reporting.\n\t */\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = armv6_pmcr_read();\n\tval &= ~mask;\n\tval |= evt;\n\tarmv6_pmcr_write(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic const struct arm_pmu armv6pmu = {\n\t.id\t\t\t= ARM_PERF_PMU_ID_V6,\n\t.name\t\t\t= \"v6\",\n\t.handle_irq\t\t= armv6pmu_handle_irq,\n\t.enable\t\t\t= armv6pmu_enable_event,\n\t.disable\t\t= armv6pmu_disable_event,\n\t.read_counter\t\t= armv6pmu_read_counter,\n\t.write_counter\t\t= armv6pmu_write_counter,\n\t.get_event_idx\t\t= armv6pmu_get_event_idx,\n\t.start\t\t\t= armv6pmu_start,\n\t.stop\t\t\t= armv6pmu_stop,\n\t.cache_map\t\t= &armv6_perf_cache_map,\n\t.event_map\t\t= &armv6_perf_map,\n\t.raw_event_mask\t\t= 0xFF,\n\t.num_events\t\t= 3,\n\t.max_period\t\t= (1LLU << 32) - 1,\n};\n\nstatic const struct arm_pmu *__init armv6pmu_init(void)\n{\n\treturn &armv6pmu;\n}\n\n/*\n * ARMv6mpcore is almost identical to single core ARMv6 with the exception\n * that some of the events have different enumerations and that there is no\n * *hack* to stop the programmable counters. To stop the counters we simply\n * disable the interrupt reporting and update the event. When unthrottling we\n * reset the period and enable the interrupt reporting.\n */\nstatic const struct arm_pmu armv6mpcore_pmu = {\n\t.id\t\t\t= ARM_PERF_PMU_ID_V6MP,\n\t.name\t\t\t= \"v6mpcore\",\n\t.handle_irq\t\t= armv6pmu_handle_irq,\n\t.enable\t\t\t= armv6pmu_enable_event,\n\t.disable\t\t= armv6mpcore_pmu_disable_event,\n\t.read_counter\t\t= armv6pmu_read_counter,\n\t.write_counter\t\t= armv6pmu_write_counter,\n\t.get_event_idx\t\t= armv6pmu_get_event_idx,\n\t.start\t\t\t= armv6pmu_start,\n\t.stop\t\t\t= armv6pmu_stop,\n\t.cache_map\t\t= &armv6mpcore_perf_cache_map,\n\t.event_map\t\t= &armv6mpcore_perf_map,\n\t.raw_event_mask\t\t= 0xFF,\n\t.num_events\t\t= 3,\n\t.max_period\t\t= (1LLU << 32) - 1,\n};\n\nstatic const struct arm_pmu *__init armv6mpcore_pmu_init(void)\n{\n\treturn &armv6mpcore_pmu;\n}\n#else\nstatic const struct arm_pmu *__init armv6pmu_init(void)\n{\n\treturn NULL;\n}\n\nstatic const struct arm_pmu *__init armv6mpcore_pmu_init(void)\n{\n\treturn NULL;\n}\n#endif\t/* CONFIG_CPU_V6 || CONFIG_CPU_V6K */\n", "/*\n * ARMv7 Cortex-A8 and Cortex-A9 Performance Events handling code.\n *\n * ARMv7 support: Jean Pihet <jpihet@mvista.com>\n * 2010 (c) MontaVista Software, LLC.\n *\n * Copied from ARMv6 code, with the low level code inspired\n *  by the ARMv7 Oprofile code.\n *\n * Cortex-A8 has up to 4 configurable performance counters and\n *  a single cycle counter.\n * Cortex-A9 has up to 31 configurable performance counters and\n *  a single cycle counter.\n *\n * All counters can be enabled/disabled and IRQ masked separately. The cycle\n *  counter and all 4 performance counters together can be reset separately.\n */\n\n#ifdef CONFIG_CPU_V7\n/* Common ARMv7 event types */\nenum armv7_perf_types {\n\tARMV7_PERFCTR_PMNC_SW_INCR\t\t= 0x00,\n\tARMV7_PERFCTR_IFETCH_MISS\t\t= 0x01,\n\tARMV7_PERFCTR_ITLB_MISS\t\t\t= 0x02,\n\tARMV7_PERFCTR_DCACHE_REFILL\t\t= 0x03,\n\tARMV7_PERFCTR_DCACHE_ACCESS\t\t= 0x04,\n\tARMV7_PERFCTR_DTLB_REFILL\t\t= 0x05,\n\tARMV7_PERFCTR_DREAD\t\t\t= 0x06,\n\tARMV7_PERFCTR_DWRITE\t\t\t= 0x07,\n\n\tARMV7_PERFCTR_EXC_TAKEN\t\t\t= 0x09,\n\tARMV7_PERFCTR_EXC_EXECUTED\t\t= 0x0A,\n\tARMV7_PERFCTR_CID_WRITE\t\t\t= 0x0B,\n\t/* ARMV7_PERFCTR_PC_WRITE is equivalent to HW_BRANCH_INSTRUCTIONS.\n\t * It counts:\n\t *  - all branch instructions,\n\t *  - instructions that explicitly write the PC,\n\t *  - exception generating instructions.\n\t */\n\tARMV7_PERFCTR_PC_WRITE\t\t\t= 0x0C,\n\tARMV7_PERFCTR_PC_IMM_BRANCH\t\t= 0x0D,\n\tARMV7_PERFCTR_UNALIGNED_ACCESS\t\t= 0x0F,\n\tARMV7_PERFCTR_PC_BRANCH_MIS_PRED\t= 0x10,\n\tARMV7_PERFCTR_CLOCK_CYCLES\t\t= 0x11,\n\n\tARMV7_PERFCTR_PC_BRANCH_MIS_USED\t= 0x12,\n\n\tARMV7_PERFCTR_CPU_CYCLES\t\t= 0xFF\n};\n\n/* ARMv7 Cortex-A8 specific event types */\nenum armv7_a8_perf_types {\n\tARMV7_PERFCTR_INSTR_EXECUTED\t\t= 0x08,\n\n\tARMV7_PERFCTR_PC_PROC_RETURN\t\t= 0x0E,\n\n\tARMV7_PERFCTR_WRITE_BUFFER_FULL\t\t= 0x40,\n\tARMV7_PERFCTR_L2_STORE_MERGED\t\t= 0x41,\n\tARMV7_PERFCTR_L2_STORE_BUFF\t\t= 0x42,\n\tARMV7_PERFCTR_L2_ACCESS\t\t\t= 0x43,\n\tARMV7_PERFCTR_L2_CACH_MISS\t\t= 0x44,\n\tARMV7_PERFCTR_AXI_READ_CYCLES\t\t= 0x45,\n\tARMV7_PERFCTR_AXI_WRITE_CYCLES\t\t= 0x46,\n\tARMV7_PERFCTR_MEMORY_REPLAY\t\t= 0x47,\n\tARMV7_PERFCTR_UNALIGNED_ACCESS_REPLAY\t= 0x48,\n\tARMV7_PERFCTR_L1_DATA_MISS\t\t= 0x49,\n\tARMV7_PERFCTR_L1_INST_MISS\t\t= 0x4A,\n\tARMV7_PERFCTR_L1_DATA_COLORING\t\t= 0x4B,\n\tARMV7_PERFCTR_L1_NEON_DATA\t\t= 0x4C,\n\tARMV7_PERFCTR_L1_NEON_CACH_DATA\t\t= 0x4D,\n\tARMV7_PERFCTR_L2_NEON\t\t\t= 0x4E,\n\tARMV7_PERFCTR_L2_NEON_HIT\t\t= 0x4F,\n\tARMV7_PERFCTR_L1_INST\t\t\t= 0x50,\n\tARMV7_PERFCTR_PC_RETURN_MIS_PRED\t= 0x51,\n\tARMV7_PERFCTR_PC_BRANCH_FAILED\t\t= 0x52,\n\tARMV7_PERFCTR_PC_BRANCH_TAKEN\t\t= 0x53,\n\tARMV7_PERFCTR_PC_BRANCH_EXECUTED\t= 0x54,\n\tARMV7_PERFCTR_OP_EXECUTED\t\t= 0x55,\n\tARMV7_PERFCTR_CYCLES_INST_STALL\t\t= 0x56,\n\tARMV7_PERFCTR_CYCLES_INST\t\t= 0x57,\n\tARMV7_PERFCTR_CYCLES_NEON_DATA_STALL\t= 0x58,\n\tARMV7_PERFCTR_CYCLES_NEON_INST_STALL\t= 0x59,\n\tARMV7_PERFCTR_NEON_CYCLES\t\t= 0x5A,\n\n\tARMV7_PERFCTR_PMU0_EVENTS\t\t= 0x70,\n\tARMV7_PERFCTR_PMU1_EVENTS\t\t= 0x71,\n\tARMV7_PERFCTR_PMU_EVENTS\t\t= 0x72,\n};\n\n/* ARMv7 Cortex-A9 specific event types */\nenum armv7_a9_perf_types {\n\tARMV7_PERFCTR_JAVA_HW_BYTECODE_EXEC\t= 0x40,\n\tARMV7_PERFCTR_JAVA_SW_BYTECODE_EXEC\t= 0x41,\n\tARMV7_PERFCTR_JAZELLE_BRANCH_EXEC\t= 0x42,\n\n\tARMV7_PERFCTR_COHERENT_LINE_MISS\t= 0x50,\n\tARMV7_PERFCTR_COHERENT_LINE_HIT\t\t= 0x51,\n\n\tARMV7_PERFCTR_ICACHE_DEP_STALL_CYCLES\t= 0x60,\n\tARMV7_PERFCTR_DCACHE_DEP_STALL_CYCLES\t= 0x61,\n\tARMV7_PERFCTR_TLB_MISS_DEP_STALL_CYCLES\t= 0x62,\n\tARMV7_PERFCTR_STREX_EXECUTED_PASSED\t= 0x63,\n\tARMV7_PERFCTR_STREX_EXECUTED_FAILED\t= 0x64,\n\tARMV7_PERFCTR_DATA_EVICTION\t\t= 0x65,\n\tARMV7_PERFCTR_ISSUE_STAGE_NO_INST\t= 0x66,\n\tARMV7_PERFCTR_ISSUE_STAGE_EMPTY\t\t= 0x67,\n\tARMV7_PERFCTR_INST_OUT_OF_RENAME_STAGE\t= 0x68,\n\n\tARMV7_PERFCTR_PREDICTABLE_FUNCT_RETURNS\t= 0x6E,\n\n\tARMV7_PERFCTR_MAIN_UNIT_EXECUTED_INST\t= 0x70,\n\tARMV7_PERFCTR_SECOND_UNIT_EXECUTED_INST\t= 0x71,\n\tARMV7_PERFCTR_LD_ST_UNIT_EXECUTED_INST\t= 0x72,\n\tARMV7_PERFCTR_FP_EXECUTED_INST\t\t= 0x73,\n\tARMV7_PERFCTR_NEON_EXECUTED_INST\t= 0x74,\n\n\tARMV7_PERFCTR_PLD_FULL_DEP_STALL_CYCLES\t= 0x80,\n\tARMV7_PERFCTR_DATA_WR_DEP_STALL_CYCLES\t= 0x81,\n\tARMV7_PERFCTR_ITLB_MISS_DEP_STALL_CYCLES\t= 0x82,\n\tARMV7_PERFCTR_DTLB_MISS_DEP_STALL_CYCLES\t= 0x83,\n\tARMV7_PERFCTR_MICRO_ITLB_MISS_DEP_STALL_CYCLES\t= 0x84,\n\tARMV7_PERFCTR_MICRO_DTLB_MISS_DEP_STALL_CYCLES\t= 0x85,\n\tARMV7_PERFCTR_DMB_DEP_STALL_CYCLES\t= 0x86,\n\n\tARMV7_PERFCTR_INTGR_CLK_ENABLED_CYCLES\t= 0x8A,\n\tARMV7_PERFCTR_DATA_ENGINE_CLK_EN_CYCLES\t= 0x8B,\n\n\tARMV7_PERFCTR_ISB_INST\t\t\t= 0x90,\n\tARMV7_PERFCTR_DSB_INST\t\t\t= 0x91,\n\tARMV7_PERFCTR_DMB_INST\t\t\t= 0x92,\n\tARMV7_PERFCTR_EXT_INTERRUPTS\t\t= 0x93,\n\n\tARMV7_PERFCTR_PLE_CACHE_LINE_RQST_COMPLETED\t= 0xA0,\n\tARMV7_PERFCTR_PLE_CACHE_LINE_RQST_SKIPPED\t= 0xA1,\n\tARMV7_PERFCTR_PLE_FIFO_FLUSH\t\t= 0xA2,\n\tARMV7_PERFCTR_PLE_RQST_COMPLETED\t= 0xA3,\n\tARMV7_PERFCTR_PLE_FIFO_OVERFLOW\t\t= 0xA4,\n\tARMV7_PERFCTR_PLE_RQST_PROG\t\t= 0xA5\n};\n\n/*\n * Cortex-A8 HW events mapping\n *\n * The hardware events that we support. We do support cache operations but\n * we have harvard caches and no way to combine instruction and data\n * accesses/misses in hardware.\n */\nstatic const unsigned armv7_a8_perf_map[PERF_COUNT_HW_MAX] = {\n\t[PERF_COUNT_HW_CPU_CYCLES]\t    = ARMV7_PERFCTR_CPU_CYCLES,\n\t[PERF_COUNT_HW_INSTRUCTIONS]\t    = ARMV7_PERFCTR_INSTR_EXECUTED,\n\t[PERF_COUNT_HW_CACHE_REFERENCES]    = HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_CACHE_MISSES]\t    = HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_BRANCH_INSTRUCTIONS] = ARMV7_PERFCTR_PC_WRITE,\n\t[PERF_COUNT_HW_BRANCH_MISSES]\t    = ARMV7_PERFCTR_PC_BRANCH_MIS_PRED,\n\t[PERF_COUNT_HW_BUS_CYCLES]\t    = ARMV7_PERFCTR_CLOCK_CYCLES,\n};\n\nstatic const unsigned armv7_a8_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t  [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t  [PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\t[C(L1D)] = {\n\t\t/*\n\t\t * The performance counters don't differentiate between read\n\t\t * and write accesses/misses so this isn't strictly correct,\n\t\t * but it's the best we can do. Writes and reads get\n\t\t * combined.\n\t\t */\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_DCACHE_REFILL,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_DCACHE_REFILL,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(L1I)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_L1_INST,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_L1_INST_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_L1_INST,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_L1_INST_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(LL)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_L2_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_L2_CACH_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_L2_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_L2_CACH_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(DTLB)] = {\n\t\t/*\n\t\t * Only ITLB misses and DTLB refills are supported.\n\t\t * If users want the DTLB refills misses a raw counter\n\t\t * must be used.\n\t\t */\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_DTLB_REFILL,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_DTLB_REFILL,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(ITLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(BPU)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_PC_WRITE,\n\t\t\t[C(RESULT_MISS)]\n\t\t\t\t\t= ARMV7_PERFCTR_PC_BRANCH_MIS_PRED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_PC_WRITE,\n\t\t\t[C(RESULT_MISS)]\n\t\t\t\t\t= ARMV7_PERFCTR_PC_BRANCH_MIS_PRED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n};\n\n/*\n * Cortex-A9 HW events mapping\n */\nstatic const unsigned armv7_a9_perf_map[PERF_COUNT_HW_MAX] = {\n\t[PERF_COUNT_HW_CPU_CYCLES]\t    = ARMV7_PERFCTR_CPU_CYCLES,\n\t[PERF_COUNT_HW_INSTRUCTIONS]\t    =\n\t\t\t\t\tARMV7_PERFCTR_INST_OUT_OF_RENAME_STAGE,\n\t[PERF_COUNT_HW_CACHE_REFERENCES]    = ARMV7_PERFCTR_COHERENT_LINE_HIT,\n\t[PERF_COUNT_HW_CACHE_MISSES]\t    = ARMV7_PERFCTR_COHERENT_LINE_MISS,\n\t[PERF_COUNT_HW_BRANCH_INSTRUCTIONS] = ARMV7_PERFCTR_PC_WRITE,\n\t[PERF_COUNT_HW_BRANCH_MISSES]\t    = ARMV7_PERFCTR_PC_BRANCH_MIS_PRED,\n\t[PERF_COUNT_HW_BUS_CYCLES]\t    = ARMV7_PERFCTR_CLOCK_CYCLES,\n};\n\nstatic const unsigned armv7_a9_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t  [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t  [PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\t[C(L1D)] = {\n\t\t/*\n\t\t * The performance counters don't differentiate between read\n\t\t * and write accesses/misses so this isn't strictly correct,\n\t\t * but it's the best we can do. Writes and reads get\n\t\t * combined.\n\t\t */\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_DCACHE_REFILL,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_DCACHE_REFILL,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(L1I)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_IFETCH_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_IFETCH_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(LL)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(DTLB)] = {\n\t\t/*\n\t\t * Only ITLB misses and DTLB refills are supported.\n\t\t * If users want the DTLB refills misses a raw counter\n\t\t * must be used.\n\t\t */\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_DTLB_REFILL,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_DTLB_REFILL,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(ITLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV7_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(BPU)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_PC_WRITE,\n\t\t\t[C(RESULT_MISS)]\n\t\t\t\t\t= ARMV7_PERFCTR_PC_BRANCH_MIS_PRED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV7_PERFCTR_PC_WRITE,\n\t\t\t[C(RESULT_MISS)]\n\t\t\t\t\t= ARMV7_PERFCTR_PC_BRANCH_MIS_PRED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n};\n\n/*\n * Perf Events counters\n */\nenum armv7_counters {\n\tARMV7_CYCLE_COUNTER\t\t= 1,\t/* Cycle counter */\n\tARMV7_COUNTER0\t\t\t= 2,\t/* First event counter */\n};\n\n/*\n * The cycle counter is ARMV7_CYCLE_COUNTER.\n * The first event counter is ARMV7_COUNTER0.\n * The last event counter is (ARMV7_COUNTER0 + armpmu->num_events - 1).\n */\n#define\tARMV7_COUNTER_LAST\t(ARMV7_COUNTER0 + armpmu->num_events - 1)\n\n/*\n * ARMv7 low level PMNC access\n */\n\n/*\n * Per-CPU PMNC: config reg\n */\n#define ARMV7_PMNC_E\t\t(1 << 0) /* Enable all counters */\n#define ARMV7_PMNC_P\t\t(1 << 1) /* Reset all counters */\n#define ARMV7_PMNC_C\t\t(1 << 2) /* Cycle counter reset */\n#define ARMV7_PMNC_D\t\t(1 << 3) /* CCNT counts every 64th cpu cycle */\n#define ARMV7_PMNC_X\t\t(1 << 4) /* Export to ETM */\n#define ARMV7_PMNC_DP\t\t(1 << 5) /* Disable CCNT if non-invasive debug*/\n#define\tARMV7_PMNC_N_SHIFT\t11\t /* Number of counters supported */\n#define\tARMV7_PMNC_N_MASK\t0x1f\n#define\tARMV7_PMNC_MASK\t\t0x3f\t /* Mask for writable bits */\n\n/*\n * Available counters\n */\n#define ARMV7_CNT0\t\t0\t/* First event counter */\n#define ARMV7_CCNT\t\t31\t/* Cycle counter */\n\n/* Perf Event to low level counters mapping */\n#define ARMV7_EVENT_CNT_TO_CNTx\t(ARMV7_COUNTER0 - ARMV7_CNT0)\n\n/*\n * CNTENS: counters enable reg\n */\n#define ARMV7_CNTENS_P(idx)\t(1 << (idx - ARMV7_EVENT_CNT_TO_CNTx))\n#define ARMV7_CNTENS_C\t\t(1 << ARMV7_CCNT)\n\n/*\n * CNTENC: counters disable reg\n */\n#define ARMV7_CNTENC_P(idx)\t(1 << (idx - ARMV7_EVENT_CNT_TO_CNTx))\n#define ARMV7_CNTENC_C\t\t(1 << ARMV7_CCNT)\n\n/*\n * INTENS: counters overflow interrupt enable reg\n */\n#define ARMV7_INTENS_P(idx)\t(1 << (idx - ARMV7_EVENT_CNT_TO_CNTx))\n#define ARMV7_INTENS_C\t\t(1 << ARMV7_CCNT)\n\n/*\n * INTENC: counters overflow interrupt disable reg\n */\n#define ARMV7_INTENC_P(idx)\t(1 << (idx - ARMV7_EVENT_CNT_TO_CNTx))\n#define ARMV7_INTENC_C\t\t(1 << ARMV7_CCNT)\n\n/*\n * EVTSEL: Event selection reg\n */\n#define\tARMV7_EVTSEL_MASK\t0xff\t\t/* Mask for writable bits */\n\n/*\n * SELECT: Counter selection reg\n */\n#define\tARMV7_SELECT_MASK\t0x1f\t\t/* Mask for writable bits */\n\n/*\n * FLAG: counters overflow flag status reg\n */\n#define ARMV7_FLAG_P(idx)\t(1 << (idx - ARMV7_EVENT_CNT_TO_CNTx))\n#define ARMV7_FLAG_C\t\t(1 << ARMV7_CCNT)\n#define\tARMV7_FLAG_MASK\t\t0xffffffff\t/* Mask for writable bits */\n#define\tARMV7_OVERFLOWED_MASK\tARMV7_FLAG_MASK\n\nstatic inline unsigned long armv7_pmnc_read(void)\n{\n\tu32 val;\n\tasm volatile(\"mrc p15, 0, %0, c9, c12, 0\" : \"=r\"(val));\n\treturn val;\n}\n\nstatic inline void armv7_pmnc_write(unsigned long val)\n{\n\tval &= ARMV7_PMNC_MASK;\n\tisb();\n\tasm volatile(\"mcr p15, 0, %0, c9, c12, 0\" : : \"r\"(val));\n}\n\nstatic inline int armv7_pmnc_has_overflowed(unsigned long pmnc)\n{\n\treturn pmnc & ARMV7_OVERFLOWED_MASK;\n}\n\nstatic inline int armv7_pmnc_counter_has_overflowed(unsigned long pmnc,\n\t\t\t\t\tenum armv7_counters counter)\n{\n\tint ret = 0;\n\n\tif (counter == ARMV7_CYCLE_COUNTER)\n\t\tret = pmnc & ARMV7_FLAG_C;\n\telse if ((counter >= ARMV7_COUNTER0) && (counter <= ARMV7_COUNTER_LAST))\n\t\tret = pmnc & ARMV7_FLAG_P(counter);\n\telse\n\t\tpr_err(\"CPU%u checking wrong counter %d overflow status\\n\",\n\t\t\tsmp_processor_id(), counter);\n\n\treturn ret;\n}\n\nstatic inline int armv7_pmnc_select_counter(unsigned int idx)\n{\n\tu32 val;\n\n\tif ((idx < ARMV7_COUNTER0) || (idx > ARMV7_COUNTER_LAST)) {\n\t\tpr_err(\"CPU%u selecting wrong PMNC counter\"\n\t\t\t\" %d\\n\", smp_processor_id(), idx);\n\t\treturn -1;\n\t}\n\n\tval = (idx - ARMV7_EVENT_CNT_TO_CNTx) & ARMV7_SELECT_MASK;\n\tasm volatile(\"mcr p15, 0, %0, c9, c12, 5\" : : \"r\" (val));\n\tisb();\n\n\treturn idx;\n}\n\nstatic inline u32 armv7pmu_read_counter(int idx)\n{\n\tunsigned long value = 0;\n\n\tif (idx == ARMV7_CYCLE_COUNTER)\n\t\tasm volatile(\"mrc p15, 0, %0, c9, c13, 0\" : \"=r\" (value));\n\telse if ((idx >= ARMV7_COUNTER0) && (idx <= ARMV7_COUNTER_LAST)) {\n\t\tif (armv7_pmnc_select_counter(idx) == idx)\n\t\t\tasm volatile(\"mrc p15, 0, %0, c9, c13, 2\"\n\t\t\t\t     : \"=r\" (value));\n\t} else\n\t\tpr_err(\"CPU%u reading wrong counter %d\\n\",\n\t\t\tsmp_processor_id(), idx);\n\n\treturn value;\n}\n\nstatic inline void armv7pmu_write_counter(int idx, u32 value)\n{\n\tif (idx == ARMV7_CYCLE_COUNTER)\n\t\tasm volatile(\"mcr p15, 0, %0, c9, c13, 0\" : : \"r\" (value));\n\telse if ((idx >= ARMV7_COUNTER0) && (idx <= ARMV7_COUNTER_LAST)) {\n\t\tif (armv7_pmnc_select_counter(idx) == idx)\n\t\t\tasm volatile(\"mcr p15, 0, %0, c9, c13, 2\"\n\t\t\t\t     : : \"r\" (value));\n\t} else\n\t\tpr_err(\"CPU%u writing wrong counter %d\\n\",\n\t\t\tsmp_processor_id(), idx);\n}\n\nstatic inline void armv7_pmnc_write_evtsel(unsigned int idx, u32 val)\n{\n\tif (armv7_pmnc_select_counter(idx) == idx) {\n\t\tval &= ARMV7_EVTSEL_MASK;\n\t\tasm volatile(\"mcr p15, 0, %0, c9, c13, 1\" : : \"r\" (val));\n\t}\n}\n\nstatic inline u32 armv7_pmnc_enable_counter(unsigned int idx)\n{\n\tu32 val;\n\n\tif ((idx != ARMV7_CYCLE_COUNTER) &&\n\t    ((idx < ARMV7_COUNTER0) || (idx > ARMV7_COUNTER_LAST))) {\n\t\tpr_err(\"CPU%u enabling wrong PMNC counter\"\n\t\t\t\" %d\\n\", smp_processor_id(), idx);\n\t\treturn -1;\n\t}\n\n\tif (idx == ARMV7_CYCLE_COUNTER)\n\t\tval = ARMV7_CNTENS_C;\n\telse\n\t\tval = ARMV7_CNTENS_P(idx);\n\n\tasm volatile(\"mcr p15, 0, %0, c9, c12, 1\" : : \"r\" (val));\n\n\treturn idx;\n}\n\nstatic inline u32 armv7_pmnc_disable_counter(unsigned int idx)\n{\n\tu32 val;\n\n\n\tif ((idx != ARMV7_CYCLE_COUNTER) &&\n\t    ((idx < ARMV7_COUNTER0) || (idx > ARMV7_COUNTER_LAST))) {\n\t\tpr_err(\"CPU%u disabling wrong PMNC counter\"\n\t\t\t\" %d\\n\", smp_processor_id(), idx);\n\t\treturn -1;\n\t}\n\n\tif (idx == ARMV7_CYCLE_COUNTER)\n\t\tval = ARMV7_CNTENC_C;\n\telse\n\t\tval = ARMV7_CNTENC_P(idx);\n\n\tasm volatile(\"mcr p15, 0, %0, c9, c12, 2\" : : \"r\" (val));\n\n\treturn idx;\n}\n\nstatic inline u32 armv7_pmnc_enable_intens(unsigned int idx)\n{\n\tu32 val;\n\n\tif ((idx != ARMV7_CYCLE_COUNTER) &&\n\t    ((idx < ARMV7_COUNTER0) || (idx > ARMV7_COUNTER_LAST))) {\n\t\tpr_err(\"CPU%u enabling wrong PMNC counter\"\n\t\t\t\" interrupt enable %d\\n\", smp_processor_id(), idx);\n\t\treturn -1;\n\t}\n\n\tif (idx == ARMV7_CYCLE_COUNTER)\n\t\tval = ARMV7_INTENS_C;\n\telse\n\t\tval = ARMV7_INTENS_P(idx);\n\n\tasm volatile(\"mcr p15, 0, %0, c9, c14, 1\" : : \"r\" (val));\n\n\treturn idx;\n}\n\nstatic inline u32 armv7_pmnc_disable_intens(unsigned int idx)\n{\n\tu32 val;\n\n\tif ((idx != ARMV7_CYCLE_COUNTER) &&\n\t    ((idx < ARMV7_COUNTER0) || (idx > ARMV7_COUNTER_LAST))) {\n\t\tpr_err(\"CPU%u disabling wrong PMNC counter\"\n\t\t\t\" interrupt enable %d\\n\", smp_processor_id(), idx);\n\t\treturn -1;\n\t}\n\n\tif (idx == ARMV7_CYCLE_COUNTER)\n\t\tval = ARMV7_INTENC_C;\n\telse\n\t\tval = ARMV7_INTENC_P(idx);\n\n\tasm volatile(\"mcr p15, 0, %0, c9, c14, 2\" : : \"r\" (val));\n\n\treturn idx;\n}\n\nstatic inline u32 armv7_pmnc_getreset_flags(void)\n{\n\tu32 val;\n\n\t/* Read */\n\tasm volatile(\"mrc p15, 0, %0, c9, c12, 3\" : \"=r\" (val));\n\n\t/* Write to clear flags */\n\tval &= ARMV7_FLAG_MASK;\n\tasm volatile(\"mcr p15, 0, %0, c9, c12, 3\" : : \"r\" (val));\n\n\treturn val;\n}\n\n#ifdef DEBUG\nstatic void armv7_pmnc_dump_regs(void)\n{\n\tu32 val;\n\tunsigned int cnt;\n\n\tprintk(KERN_INFO \"PMNC registers dump:\\n\");\n\n\tasm volatile(\"mrc p15, 0, %0, c9, c12, 0\" : \"=r\" (val));\n\tprintk(KERN_INFO \"PMNC  =0x%08x\\n\", val);\n\n\tasm volatile(\"mrc p15, 0, %0, c9, c12, 1\" : \"=r\" (val));\n\tprintk(KERN_INFO \"CNTENS=0x%08x\\n\", val);\n\n\tasm volatile(\"mrc p15, 0, %0, c9, c14, 1\" : \"=r\" (val));\n\tprintk(KERN_INFO \"INTENS=0x%08x\\n\", val);\n\n\tasm volatile(\"mrc p15, 0, %0, c9, c12, 3\" : \"=r\" (val));\n\tprintk(KERN_INFO \"FLAGS =0x%08x\\n\", val);\n\n\tasm volatile(\"mrc p15, 0, %0, c9, c12, 5\" : \"=r\" (val));\n\tprintk(KERN_INFO \"SELECT=0x%08x\\n\", val);\n\n\tasm volatile(\"mrc p15, 0, %0, c9, c13, 0\" : \"=r\" (val));\n\tprintk(KERN_INFO \"CCNT  =0x%08x\\n\", val);\n\n\tfor (cnt = ARMV7_COUNTER0; cnt < ARMV7_COUNTER_LAST; cnt++) {\n\t\tarmv7_pmnc_select_counter(cnt);\n\t\tasm volatile(\"mrc p15, 0, %0, c9, c13, 2\" : \"=r\" (val));\n\t\tprintk(KERN_INFO \"CNT[%d] count =0x%08x\\n\",\n\t\t\tcnt-ARMV7_EVENT_CNT_TO_CNTx, val);\n\t\tasm volatile(\"mrc p15, 0, %0, c9, c13, 1\" : \"=r\" (val));\n\t\tprintk(KERN_INFO \"CNT[%d] evtsel=0x%08x\\n\",\n\t\t\tcnt-ARMV7_EVENT_CNT_TO_CNTx, val);\n\t}\n}\n#endif\n\nstatic void armv7pmu_enable_event(struct hw_perf_event *hwc, int idx)\n{\n\tunsigned long flags;\n\n\t/*\n\t * Enable counter and interrupt, and set the counter to count\n\t * the event that we're interested in.\n\t */\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\n\t/*\n\t * Disable counter\n\t */\n\tarmv7_pmnc_disable_counter(idx);\n\n\t/*\n\t * Set event (if destined for PMNx counters)\n\t * We don't need to set the event if it's a cycle count\n\t */\n\tif (idx != ARMV7_CYCLE_COUNTER)\n\t\tarmv7_pmnc_write_evtsel(idx, hwc->config_base);\n\n\t/*\n\t * Enable interrupt for this counter\n\t */\n\tarmv7_pmnc_enable_intens(idx);\n\n\t/*\n\t * Enable counter\n\t */\n\tarmv7_pmnc_enable_counter(idx);\n\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic void armv7pmu_disable_event(struct hw_perf_event *hwc, int idx)\n{\n\tunsigned long flags;\n\n\t/*\n\t * Disable counter and interrupt\n\t */\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\n\t/*\n\t * Disable counter\n\t */\n\tarmv7_pmnc_disable_counter(idx);\n\n\t/*\n\t * Disable interrupt for this counter\n\t */\n\tarmv7_pmnc_disable_intens(idx);\n\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic irqreturn_t armv7pmu_handle_irq(int irq_num, void *dev)\n{\n\tunsigned long pmnc;\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct pt_regs *regs;\n\tint idx;\n\n\t/*\n\t * Get and reset the IRQ flags\n\t */\n\tpmnc = armv7_pmnc_getreset_flags();\n\n\t/*\n\t * Did an overflow occur?\n\t */\n\tif (!armv7_pmnc_has_overflowed(pmnc))\n\t\treturn IRQ_NONE;\n\n\t/*\n\t * Handle the counter(s) overflow(s)\n\t */\n\tregs = get_irq_regs();\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\tfor (idx = 0; idx <= armpmu->num_events; ++idx) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tstruct hw_perf_event *hwc;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * We have a single interrupt for all counters. Check that\n\t\t * each counter has overflowed before we process it.\n\t\t */\n\t\tif (!armv7_pmnc_counter_has_overflowed(pmnc, idx))\n\t\t\tcontinue;\n\n\t\thwc = &event->hw;\n\t\tarmpmu_event_update(event, hwc, idx, 1);\n\t\tdata.period = event->hw.last_period;\n\t\tif (!armpmu_event_set_period(event, hwc, idx))\n\t\t\tcontinue;\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tarmpmu->disable(hwc, idx);\n\t}\n\n\t/*\n\t * Handle the pending perf events.\n\t *\n\t * Note: this call *must* be run with interrupts disabled. For\n\t * platforms that can have the PMU interrupts raised as an NMI, this\n\t * will not work.\n\t */\n\tirq_work_run();\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void armv7pmu_start(void)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\t/* Enable all counters */\n\tarmv7_pmnc_write(armv7_pmnc_read() | ARMV7_PMNC_E);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic void armv7pmu_stop(void)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\t/* Disable all counters */\n\tarmv7_pmnc_write(armv7_pmnc_read() & ~ARMV7_PMNC_E);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic int armv7pmu_get_event_idx(struct cpu_hw_events *cpuc,\n\t\t\t\t  struct hw_perf_event *event)\n{\n\tint idx;\n\n\t/* Always place a cycle counter into the cycle counter. */\n\tif (event->config_base == ARMV7_PERFCTR_CPU_CYCLES) {\n\t\tif (test_and_set_bit(ARMV7_CYCLE_COUNTER, cpuc->used_mask))\n\t\t\treturn -EAGAIN;\n\n\t\treturn ARMV7_CYCLE_COUNTER;\n\t} else {\n\t\t/*\n\t\t * For anything other than a cycle counter, try and use\n\t\t * the events counters\n\t\t */\n\t\tfor (idx = ARMV7_COUNTER0; idx <= armpmu->num_events; ++idx) {\n\t\t\tif (!test_and_set_bit(idx, cpuc->used_mask))\n\t\t\t\treturn idx;\n\t\t}\n\n\t\t/* The counters are all in use. */\n\t\treturn -EAGAIN;\n\t}\n}\n\nstatic void armv7pmu_reset(void *info)\n{\n\tu32 idx, nb_cnt = armpmu->num_events;\n\n\t/* The counter and interrupt enable registers are unknown at reset. */\n\tfor (idx = 1; idx < nb_cnt; ++idx)\n\t\tarmv7pmu_disable_event(NULL, idx);\n\n\t/* Initialize & Reset PMNC: C and P bits */\n\tarmv7_pmnc_write(ARMV7_PMNC_P | ARMV7_PMNC_C);\n}\n\nstatic struct arm_pmu armv7pmu = {\n\t.handle_irq\t\t= armv7pmu_handle_irq,\n\t.enable\t\t\t= armv7pmu_enable_event,\n\t.disable\t\t= armv7pmu_disable_event,\n\t.read_counter\t\t= armv7pmu_read_counter,\n\t.write_counter\t\t= armv7pmu_write_counter,\n\t.get_event_idx\t\t= armv7pmu_get_event_idx,\n\t.start\t\t\t= armv7pmu_start,\n\t.stop\t\t\t= armv7pmu_stop,\n\t.reset\t\t\t= armv7pmu_reset,\n\t.raw_event_mask\t\t= 0xFF,\n\t.max_period\t\t= (1LLU << 32) - 1,\n};\n\nstatic u32 __init armv7_read_num_pmnc_events(void)\n{\n\tu32 nb_cnt;\n\n\t/* Read the nb of CNTx counters supported from PMNC */\n\tnb_cnt = (armv7_pmnc_read() >> ARMV7_PMNC_N_SHIFT) & ARMV7_PMNC_N_MASK;\n\n\t/* Add the CPU cycles counter and return */\n\treturn nb_cnt + 1;\n}\n\nstatic const struct arm_pmu *__init armv7_a8_pmu_init(void)\n{\n\tarmv7pmu.id\t\t= ARM_PERF_PMU_ID_CA8;\n\tarmv7pmu.name\t\t= \"ARMv7 Cortex-A8\";\n\tarmv7pmu.cache_map\t= &armv7_a8_perf_cache_map;\n\tarmv7pmu.event_map\t= &armv7_a8_perf_map;\n\tarmv7pmu.num_events\t= armv7_read_num_pmnc_events();\n\treturn &armv7pmu;\n}\n\nstatic const struct arm_pmu *__init armv7_a9_pmu_init(void)\n{\n\tarmv7pmu.id\t\t= ARM_PERF_PMU_ID_CA9;\n\tarmv7pmu.name\t\t= \"ARMv7 Cortex-A9\";\n\tarmv7pmu.cache_map\t= &armv7_a9_perf_cache_map;\n\tarmv7pmu.event_map\t= &armv7_a9_perf_map;\n\tarmv7pmu.num_events\t= armv7_read_num_pmnc_events();\n\treturn &armv7pmu;\n}\n#else\nstatic const struct arm_pmu *__init armv7_a8_pmu_init(void)\n{\n\treturn NULL;\n}\n\nstatic const struct arm_pmu *__init armv7_a9_pmu_init(void)\n{\n\treturn NULL;\n}\n#endif\t/* CONFIG_CPU_V7 */\n", "/*\n * ARMv5 [xscale] Performance counter handling code.\n *\n * Copyright (C) 2010, ARM Ltd., Will Deacon <will.deacon@arm.com>\n *\n * Based on the previous xscale OProfile code.\n *\n * There are two variants of the xscale PMU that we support:\n * \t- xscale1pmu: 2 event counters and a cycle counter\n * \t- xscale2pmu: 4 event counters and a cycle counter\n * The two variants share event definitions, but have different\n * PMU structures.\n */\n\n#ifdef CONFIG_CPU_XSCALE\nenum xscale_perf_types {\n\tXSCALE_PERFCTR_ICACHE_MISS\t\t= 0x00,\n\tXSCALE_PERFCTR_ICACHE_NO_DELIVER\t= 0x01,\n\tXSCALE_PERFCTR_DATA_STALL\t\t= 0x02,\n\tXSCALE_PERFCTR_ITLB_MISS\t\t= 0x03,\n\tXSCALE_PERFCTR_DTLB_MISS\t\t= 0x04,\n\tXSCALE_PERFCTR_BRANCH\t\t\t= 0x05,\n\tXSCALE_PERFCTR_BRANCH_MISS\t\t= 0x06,\n\tXSCALE_PERFCTR_INSTRUCTION\t\t= 0x07,\n\tXSCALE_PERFCTR_DCACHE_FULL_STALL\t= 0x08,\n\tXSCALE_PERFCTR_DCACHE_FULL_STALL_CONTIG\t= 0x09,\n\tXSCALE_PERFCTR_DCACHE_ACCESS\t\t= 0x0A,\n\tXSCALE_PERFCTR_DCACHE_MISS\t\t= 0x0B,\n\tXSCALE_PERFCTR_DCACHE_WRITE_BACK\t= 0x0C,\n\tXSCALE_PERFCTR_PC_CHANGED\t\t= 0x0D,\n\tXSCALE_PERFCTR_BCU_REQUEST\t\t= 0x10,\n\tXSCALE_PERFCTR_BCU_FULL\t\t\t= 0x11,\n\tXSCALE_PERFCTR_BCU_DRAIN\t\t= 0x12,\n\tXSCALE_PERFCTR_BCU_ECC_NO_ELOG\t\t= 0x14,\n\tXSCALE_PERFCTR_BCU_1_BIT_ERR\t\t= 0x15,\n\tXSCALE_PERFCTR_RMW\t\t\t= 0x16,\n\t/* XSCALE_PERFCTR_CCNT is not hardware defined */\n\tXSCALE_PERFCTR_CCNT\t\t\t= 0xFE,\n\tXSCALE_PERFCTR_UNUSED\t\t\t= 0xFF,\n};\n\nenum xscale_counters {\n\tXSCALE_CYCLE_COUNTER\t= 1,\n\tXSCALE_COUNTER0,\n\tXSCALE_COUNTER1,\n\tXSCALE_COUNTER2,\n\tXSCALE_COUNTER3,\n};\n\nstatic const unsigned xscale_perf_map[PERF_COUNT_HW_MAX] = {\n\t[PERF_COUNT_HW_CPU_CYCLES]\t    = XSCALE_PERFCTR_CCNT,\n\t[PERF_COUNT_HW_INSTRUCTIONS]\t    = XSCALE_PERFCTR_INSTRUCTION,\n\t[PERF_COUNT_HW_CACHE_REFERENCES]    = HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_CACHE_MISSES]\t    = HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_BRANCH_INSTRUCTIONS] = XSCALE_PERFCTR_BRANCH,\n\t[PERF_COUNT_HW_BRANCH_MISSES]\t    = XSCALE_PERFCTR_BRANCH_MISS,\n\t[PERF_COUNT_HW_BUS_CYCLES]\t    = HW_OP_UNSUPPORTED,\n};\n\nstatic const unsigned xscale_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t   [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t   [PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\t[C(L1D)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= XSCALE_PERFCTR_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= XSCALE_PERFCTR_DCACHE_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= XSCALE_PERFCTR_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= XSCALE_PERFCTR_DCACHE_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(L1I)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= XSCALE_PERFCTR_ICACHE_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= XSCALE_PERFCTR_ICACHE_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(LL)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(DTLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= XSCALE_PERFCTR_DTLB_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= XSCALE_PERFCTR_DTLB_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(ITLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= XSCALE_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= XSCALE_PERFCTR_ITLB_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(BPU)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n};\n\n#define\tXSCALE_PMU_ENABLE\t0x001\n#define XSCALE_PMN_RESET\t0x002\n#define\tXSCALE_CCNT_RESET\t0x004\n#define\tXSCALE_PMU_RESET\t(CCNT_RESET | PMN_RESET)\n#define XSCALE_PMU_CNT64\t0x008\n\n#define XSCALE1_OVERFLOWED_MASK\t0x700\n#define XSCALE1_CCOUNT_OVERFLOW\t0x400\n#define XSCALE1_COUNT0_OVERFLOW\t0x100\n#define XSCALE1_COUNT1_OVERFLOW\t0x200\n#define XSCALE1_CCOUNT_INT_EN\t0x040\n#define XSCALE1_COUNT0_INT_EN\t0x010\n#define XSCALE1_COUNT1_INT_EN\t0x020\n#define XSCALE1_COUNT0_EVT_SHFT\t12\n#define XSCALE1_COUNT0_EVT_MASK\t(0xff << XSCALE1_COUNT0_EVT_SHFT)\n#define XSCALE1_COUNT1_EVT_SHFT\t20\n#define XSCALE1_COUNT1_EVT_MASK\t(0xff << XSCALE1_COUNT1_EVT_SHFT)\n\nstatic inline u32\nxscale1pmu_read_pmnc(void)\n{\n\tu32 val;\n\tasm volatile(\"mrc p14, 0, %0, c0, c0, 0\" : \"=r\" (val));\n\treturn val;\n}\n\nstatic inline void\nxscale1pmu_write_pmnc(u32 val)\n{\n\t/* upper 4bits and 7, 11 are write-as-0 */\n\tval &= 0xffff77f;\n\tasm volatile(\"mcr p14, 0, %0, c0, c0, 0\" : : \"r\" (val));\n}\n\nstatic inline int\nxscale1_pmnc_counter_has_overflowed(unsigned long pmnc,\n\t\t\t\t\tenum xscale_counters counter)\n{\n\tint ret = 0;\n\n\tswitch (counter) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tret = pmnc & XSCALE1_CCOUNT_OVERFLOW;\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tret = pmnc & XSCALE1_COUNT0_OVERFLOW;\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tret = pmnc & XSCALE1_COUNT1_OVERFLOW;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", counter);\n\t}\n\n\treturn ret;\n}\n\nstatic irqreturn_t\nxscale1pmu_handle_irq(int irq_num, void *dev)\n{\n\tunsigned long pmnc;\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct pt_regs *regs;\n\tint idx;\n\n\t/*\n\t * NOTE: there's an A stepping erratum that states if an overflow\n\t *       bit already exists and another occurs, the previous\n\t *       Overflow bit gets cleared. There's no workaround.\n\t *\t Fixed in B stepping or later.\n\t */\n\tpmnc = xscale1pmu_read_pmnc();\n\n\t/*\n\t * Write the value back to clear the overflow flags. Overflow\n\t * flags remain in pmnc for use below. We also disable the PMU\n\t * while we process the interrupt.\n\t */\n\txscale1pmu_write_pmnc(pmnc & ~XSCALE_PMU_ENABLE);\n\n\tif (!(pmnc & XSCALE1_OVERFLOWED_MASK))\n\t\treturn IRQ_NONE;\n\n\tregs = get_irq_regs();\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\tfor (idx = 0; idx <= armpmu->num_events; ++idx) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tstruct hw_perf_event *hwc;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\tif (!xscale1_pmnc_counter_has_overflowed(pmnc, idx))\n\t\t\tcontinue;\n\n\t\thwc = &event->hw;\n\t\tarmpmu_event_update(event, hwc, idx, 1);\n\t\tdata.period = event->hw.last_period;\n\t\tif (!armpmu_event_set_period(event, hwc, idx))\n\t\t\tcontinue;\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tarmpmu->disable(hwc, idx);\n\t}\n\n\tirq_work_run();\n\n\t/*\n\t * Re-enable the PMU.\n\t */\n\tpmnc = xscale1pmu_read_pmnc() | XSCALE_PMU_ENABLE;\n\txscale1pmu_write_pmnc(pmnc);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void\nxscale1pmu_enable_event(struct hw_perf_event *hwc, int idx)\n{\n\tunsigned long val, mask, evt, flags;\n\n\tswitch (idx) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tmask = 0;\n\t\tevt = XSCALE1_CCOUNT_INT_EN;\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tmask = XSCALE1_COUNT0_EVT_MASK;\n\t\tevt = (hwc->config_base << XSCALE1_COUNT0_EVT_SHFT) |\n\t\t\tXSCALE1_COUNT0_INT_EN;\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tmask = XSCALE1_COUNT1_EVT_MASK;\n\t\tevt = (hwc->config_base << XSCALE1_COUNT1_EVT_SHFT) |\n\t\t\tXSCALE1_COUNT1_INT_EN;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", idx);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = xscale1pmu_read_pmnc();\n\tval &= ~mask;\n\tval |= evt;\n\txscale1pmu_write_pmnc(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic void\nxscale1pmu_disable_event(struct hw_perf_event *hwc, int idx)\n{\n\tunsigned long val, mask, evt, flags;\n\n\tswitch (idx) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tmask = XSCALE1_CCOUNT_INT_EN;\n\t\tevt = 0;\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tmask = XSCALE1_COUNT0_INT_EN | XSCALE1_COUNT0_EVT_MASK;\n\t\tevt = XSCALE_PERFCTR_UNUSED << XSCALE1_COUNT0_EVT_SHFT;\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tmask = XSCALE1_COUNT1_INT_EN | XSCALE1_COUNT1_EVT_MASK;\n\t\tevt = XSCALE_PERFCTR_UNUSED << XSCALE1_COUNT1_EVT_SHFT;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", idx);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = xscale1pmu_read_pmnc();\n\tval &= ~mask;\n\tval |= evt;\n\txscale1pmu_write_pmnc(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic int\nxscale1pmu_get_event_idx(struct cpu_hw_events *cpuc,\n\t\t\tstruct hw_perf_event *event)\n{\n\tif (XSCALE_PERFCTR_CCNT == event->config_base) {\n\t\tif (test_and_set_bit(XSCALE_CYCLE_COUNTER, cpuc->used_mask))\n\t\t\treturn -EAGAIN;\n\n\t\treturn XSCALE_CYCLE_COUNTER;\n\t} else {\n\t\tif (!test_and_set_bit(XSCALE_COUNTER1, cpuc->used_mask))\n\t\t\treturn XSCALE_COUNTER1;\n\n\t\tif (!test_and_set_bit(XSCALE_COUNTER0, cpuc->used_mask))\n\t\t\treturn XSCALE_COUNTER0;\n\n\t\treturn -EAGAIN;\n\t}\n}\n\nstatic void\nxscale1pmu_start(void)\n{\n\tunsigned long flags, val;\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = xscale1pmu_read_pmnc();\n\tval |= XSCALE_PMU_ENABLE;\n\txscale1pmu_write_pmnc(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic void\nxscale1pmu_stop(void)\n{\n\tunsigned long flags, val;\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = xscale1pmu_read_pmnc();\n\tval &= ~XSCALE_PMU_ENABLE;\n\txscale1pmu_write_pmnc(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic inline u32\nxscale1pmu_read_counter(int counter)\n{\n\tu32 val = 0;\n\n\tswitch (counter) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tasm volatile(\"mrc p14, 0, %0, c1, c0, 0\" : \"=r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tasm volatile(\"mrc p14, 0, %0, c2, c0, 0\" : \"=r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tasm volatile(\"mrc p14, 0, %0, c3, c0, 0\" : \"=r\" (val));\n\t\tbreak;\n\t}\n\n\treturn val;\n}\n\nstatic inline void\nxscale1pmu_write_counter(int counter, u32 val)\n{\n\tswitch (counter) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tasm volatile(\"mcr p14, 0, %0, c1, c0, 0\" : : \"r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tasm volatile(\"mcr p14, 0, %0, c2, c0, 0\" : : \"r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tasm volatile(\"mcr p14, 0, %0, c3, c0, 0\" : : \"r\" (val));\n\t\tbreak;\n\t}\n}\n\nstatic const struct arm_pmu xscale1pmu = {\n\t.id\t\t= ARM_PERF_PMU_ID_XSCALE1,\n\t.name\t\t= \"xscale1\",\n\t.handle_irq\t= xscale1pmu_handle_irq,\n\t.enable\t\t= xscale1pmu_enable_event,\n\t.disable\t= xscale1pmu_disable_event,\n\t.read_counter\t= xscale1pmu_read_counter,\n\t.write_counter\t= xscale1pmu_write_counter,\n\t.get_event_idx\t= xscale1pmu_get_event_idx,\n\t.start\t\t= xscale1pmu_start,\n\t.stop\t\t= xscale1pmu_stop,\n\t.cache_map\t= &xscale_perf_cache_map,\n\t.event_map\t= &xscale_perf_map,\n\t.raw_event_mask\t= 0xFF,\n\t.num_events\t= 3,\n\t.max_period\t= (1LLU << 32) - 1,\n};\n\nstatic const struct arm_pmu *__init xscale1pmu_init(void)\n{\n\treturn &xscale1pmu;\n}\n\n#define XSCALE2_OVERFLOWED_MASK\t0x01f\n#define XSCALE2_CCOUNT_OVERFLOW\t0x001\n#define XSCALE2_COUNT0_OVERFLOW\t0x002\n#define XSCALE2_COUNT1_OVERFLOW\t0x004\n#define XSCALE2_COUNT2_OVERFLOW\t0x008\n#define XSCALE2_COUNT3_OVERFLOW\t0x010\n#define XSCALE2_CCOUNT_INT_EN\t0x001\n#define XSCALE2_COUNT0_INT_EN\t0x002\n#define XSCALE2_COUNT1_INT_EN\t0x004\n#define XSCALE2_COUNT2_INT_EN\t0x008\n#define XSCALE2_COUNT3_INT_EN\t0x010\n#define XSCALE2_COUNT0_EVT_SHFT\t0\n#define XSCALE2_COUNT0_EVT_MASK\t(0xff << XSCALE2_COUNT0_EVT_SHFT)\n#define XSCALE2_COUNT1_EVT_SHFT\t8\n#define XSCALE2_COUNT1_EVT_MASK\t(0xff << XSCALE2_COUNT1_EVT_SHFT)\n#define XSCALE2_COUNT2_EVT_SHFT\t16\n#define XSCALE2_COUNT2_EVT_MASK\t(0xff << XSCALE2_COUNT2_EVT_SHFT)\n#define XSCALE2_COUNT3_EVT_SHFT\t24\n#define XSCALE2_COUNT3_EVT_MASK\t(0xff << XSCALE2_COUNT3_EVT_SHFT)\n\nstatic inline u32\nxscale2pmu_read_pmnc(void)\n{\n\tu32 val;\n\tasm volatile(\"mrc p14, 0, %0, c0, c1, 0\" : \"=r\" (val));\n\t/* bits 1-2 and 4-23 are read-unpredictable */\n\treturn val & 0xff000009;\n}\n\nstatic inline void\nxscale2pmu_write_pmnc(u32 val)\n{\n\t/* bits 4-23 are write-as-0, 24-31 are write ignored */\n\tval &= 0xf;\n\tasm volatile(\"mcr p14, 0, %0, c0, c1, 0\" : : \"r\" (val));\n}\n\nstatic inline u32\nxscale2pmu_read_overflow_flags(void)\n{\n\tu32 val;\n\tasm volatile(\"mrc p14, 0, %0, c5, c1, 0\" : \"=r\" (val));\n\treturn val;\n}\n\nstatic inline void\nxscale2pmu_write_overflow_flags(u32 val)\n{\n\tasm volatile(\"mcr p14, 0, %0, c5, c1, 0\" : : \"r\" (val));\n}\n\nstatic inline u32\nxscale2pmu_read_event_select(void)\n{\n\tu32 val;\n\tasm volatile(\"mrc p14, 0, %0, c8, c1, 0\" : \"=r\" (val));\n\treturn val;\n}\n\nstatic inline void\nxscale2pmu_write_event_select(u32 val)\n{\n\tasm volatile(\"mcr p14, 0, %0, c8, c1, 0\" : : \"r\"(val));\n}\n\nstatic inline u32\nxscale2pmu_read_int_enable(void)\n{\n\tu32 val;\n\tasm volatile(\"mrc p14, 0, %0, c4, c1, 0\" : \"=r\" (val));\n\treturn val;\n}\n\nstatic void\nxscale2pmu_write_int_enable(u32 val)\n{\n\tasm volatile(\"mcr p14, 0, %0, c4, c1, 0\" : : \"r\" (val));\n}\n\nstatic inline int\nxscale2_pmnc_counter_has_overflowed(unsigned long of_flags,\n\t\t\t\t\tenum xscale_counters counter)\n{\n\tint ret = 0;\n\n\tswitch (counter) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tret = of_flags & XSCALE2_CCOUNT_OVERFLOW;\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tret = of_flags & XSCALE2_COUNT0_OVERFLOW;\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tret = of_flags & XSCALE2_COUNT1_OVERFLOW;\n\t\tbreak;\n\tcase XSCALE_COUNTER2:\n\t\tret = of_flags & XSCALE2_COUNT2_OVERFLOW;\n\t\tbreak;\n\tcase XSCALE_COUNTER3:\n\t\tret = of_flags & XSCALE2_COUNT3_OVERFLOW;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", counter);\n\t}\n\n\treturn ret;\n}\n\nstatic irqreturn_t\nxscale2pmu_handle_irq(int irq_num, void *dev)\n{\n\tunsigned long pmnc, of_flags;\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct pt_regs *regs;\n\tint idx;\n\n\t/* Disable the PMU. */\n\tpmnc = xscale2pmu_read_pmnc();\n\txscale2pmu_write_pmnc(pmnc & ~XSCALE_PMU_ENABLE);\n\n\t/* Check the overflow flag register. */\n\tof_flags = xscale2pmu_read_overflow_flags();\n\tif (!(of_flags & XSCALE2_OVERFLOWED_MASK))\n\t\treturn IRQ_NONE;\n\n\t/* Clear the overflow bits. */\n\txscale2pmu_write_overflow_flags(of_flags);\n\n\tregs = get_irq_regs();\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\tfor (idx = 0; idx <= armpmu->num_events; ++idx) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tstruct hw_perf_event *hwc;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\tif (!xscale2_pmnc_counter_has_overflowed(pmnc, idx))\n\t\t\tcontinue;\n\n\t\thwc = &event->hw;\n\t\tarmpmu_event_update(event, hwc, idx, 1);\n\t\tdata.period = event->hw.last_period;\n\t\tif (!armpmu_event_set_period(event, hwc, idx))\n\t\t\tcontinue;\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tarmpmu->disable(hwc, idx);\n\t}\n\n\tirq_work_run();\n\n\t/*\n\t * Re-enable the PMU.\n\t */\n\tpmnc = xscale2pmu_read_pmnc() | XSCALE_PMU_ENABLE;\n\txscale2pmu_write_pmnc(pmnc);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void\nxscale2pmu_enable_event(struct hw_perf_event *hwc, int idx)\n{\n\tunsigned long flags, ien, evtsel;\n\n\tien = xscale2pmu_read_int_enable();\n\tevtsel = xscale2pmu_read_event_select();\n\n\tswitch (idx) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tien |= XSCALE2_CCOUNT_INT_EN;\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tien |= XSCALE2_COUNT0_INT_EN;\n\t\tevtsel &= ~XSCALE2_COUNT0_EVT_MASK;\n\t\tevtsel |= hwc->config_base << XSCALE2_COUNT0_EVT_SHFT;\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tien |= XSCALE2_COUNT1_INT_EN;\n\t\tevtsel &= ~XSCALE2_COUNT1_EVT_MASK;\n\t\tevtsel |= hwc->config_base << XSCALE2_COUNT1_EVT_SHFT;\n\t\tbreak;\n\tcase XSCALE_COUNTER2:\n\t\tien |= XSCALE2_COUNT2_INT_EN;\n\t\tevtsel &= ~XSCALE2_COUNT2_EVT_MASK;\n\t\tevtsel |= hwc->config_base << XSCALE2_COUNT2_EVT_SHFT;\n\t\tbreak;\n\tcase XSCALE_COUNTER3:\n\t\tien |= XSCALE2_COUNT3_INT_EN;\n\t\tevtsel &= ~XSCALE2_COUNT3_EVT_MASK;\n\t\tevtsel |= hwc->config_base << XSCALE2_COUNT3_EVT_SHFT;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", idx);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\txscale2pmu_write_event_select(evtsel);\n\txscale2pmu_write_int_enable(ien);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic void\nxscale2pmu_disable_event(struct hw_perf_event *hwc, int idx)\n{\n\tunsigned long flags, ien, evtsel;\n\n\tien = xscale2pmu_read_int_enable();\n\tevtsel = xscale2pmu_read_event_select();\n\n\tswitch (idx) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tien &= ~XSCALE2_CCOUNT_INT_EN;\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tien &= ~XSCALE2_COUNT0_INT_EN;\n\t\tevtsel &= ~XSCALE2_COUNT0_EVT_MASK;\n\t\tevtsel |= XSCALE_PERFCTR_UNUSED << XSCALE2_COUNT0_EVT_SHFT;\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tien &= ~XSCALE2_COUNT1_INT_EN;\n\t\tevtsel &= ~XSCALE2_COUNT1_EVT_MASK;\n\t\tevtsel |= XSCALE_PERFCTR_UNUSED << XSCALE2_COUNT1_EVT_SHFT;\n\t\tbreak;\n\tcase XSCALE_COUNTER2:\n\t\tien &= ~XSCALE2_COUNT2_INT_EN;\n\t\tevtsel &= ~XSCALE2_COUNT2_EVT_MASK;\n\t\tevtsel |= XSCALE_PERFCTR_UNUSED << XSCALE2_COUNT2_EVT_SHFT;\n\t\tbreak;\n\tcase XSCALE_COUNTER3:\n\t\tien &= ~XSCALE2_COUNT3_INT_EN;\n\t\tevtsel &= ~XSCALE2_COUNT3_EVT_MASK;\n\t\tevtsel |= XSCALE_PERFCTR_UNUSED << XSCALE2_COUNT3_EVT_SHFT;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"invalid counter number (%d)\\n\", idx);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\txscale2pmu_write_event_select(evtsel);\n\txscale2pmu_write_int_enable(ien);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic int\nxscale2pmu_get_event_idx(struct cpu_hw_events *cpuc,\n\t\t\tstruct hw_perf_event *event)\n{\n\tint idx = xscale1pmu_get_event_idx(cpuc, event);\n\tif (idx >= 0)\n\t\tgoto out;\n\n\tif (!test_and_set_bit(XSCALE_COUNTER3, cpuc->used_mask))\n\t\tidx = XSCALE_COUNTER3;\n\telse if (!test_and_set_bit(XSCALE_COUNTER2, cpuc->used_mask))\n\t\tidx = XSCALE_COUNTER2;\nout:\n\treturn idx;\n}\n\nstatic void\nxscale2pmu_start(void)\n{\n\tunsigned long flags, val;\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = xscale2pmu_read_pmnc() & ~XSCALE_PMU_CNT64;\n\tval |= XSCALE_PMU_ENABLE;\n\txscale2pmu_write_pmnc(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic void\nxscale2pmu_stop(void)\n{\n\tunsigned long flags, val;\n\n\traw_spin_lock_irqsave(&pmu_lock, flags);\n\tval = xscale2pmu_read_pmnc();\n\tval &= ~XSCALE_PMU_ENABLE;\n\txscale2pmu_write_pmnc(val);\n\traw_spin_unlock_irqrestore(&pmu_lock, flags);\n}\n\nstatic inline u32\nxscale2pmu_read_counter(int counter)\n{\n\tu32 val = 0;\n\n\tswitch (counter) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tasm volatile(\"mrc p14, 0, %0, c1, c1, 0\" : \"=r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tasm volatile(\"mrc p14, 0, %0, c0, c2, 0\" : \"=r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tasm volatile(\"mrc p14, 0, %0, c1, c2, 0\" : \"=r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER2:\n\t\tasm volatile(\"mrc p14, 0, %0, c2, c2, 0\" : \"=r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER3:\n\t\tasm volatile(\"mrc p14, 0, %0, c3, c2, 0\" : \"=r\" (val));\n\t\tbreak;\n\t}\n\n\treturn val;\n}\n\nstatic inline void\nxscale2pmu_write_counter(int counter, u32 val)\n{\n\tswitch (counter) {\n\tcase XSCALE_CYCLE_COUNTER:\n\t\tasm volatile(\"mcr p14, 0, %0, c1, c1, 0\" : : \"r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER0:\n\t\tasm volatile(\"mcr p14, 0, %0, c0, c2, 0\" : : \"r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER1:\n\t\tasm volatile(\"mcr p14, 0, %0, c1, c2, 0\" : : \"r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER2:\n\t\tasm volatile(\"mcr p14, 0, %0, c2, c2, 0\" : : \"r\" (val));\n\t\tbreak;\n\tcase XSCALE_COUNTER3:\n\t\tasm volatile(\"mcr p14, 0, %0, c3, c2, 0\" : : \"r\" (val));\n\t\tbreak;\n\t}\n}\n\nstatic const struct arm_pmu xscale2pmu = {\n\t.id\t\t= ARM_PERF_PMU_ID_XSCALE2,\n\t.name\t\t= \"xscale2\",\n\t.handle_irq\t= xscale2pmu_handle_irq,\n\t.enable\t\t= xscale2pmu_enable_event,\n\t.disable\t= xscale2pmu_disable_event,\n\t.read_counter\t= xscale2pmu_read_counter,\n\t.write_counter\t= xscale2pmu_write_counter,\n\t.get_event_idx\t= xscale2pmu_get_event_idx,\n\t.start\t\t= xscale2pmu_start,\n\t.stop\t\t= xscale2pmu_stop,\n\t.cache_map\t= &xscale_perf_cache_map,\n\t.event_map\t= &xscale_perf_map,\n\t.raw_event_mask\t= 0xFF,\n\t.num_events\t= 5,\n\t.max_period\t= (1LLU << 32) - 1,\n};\n\nstatic const struct arm_pmu *__init xscale2pmu_init(void)\n{\n\treturn &xscale2pmu;\n}\n#else\nstatic const struct arm_pmu *__init xscale1pmu_init(void)\n{\n\treturn NULL;\n}\n\nstatic const struct arm_pmu *__init xscale2pmu_init(void)\n{\n\treturn NULL;\n}\n#endif\t/* CONFIG_CPU_XSCALE */\n", "/*\n *  linux/arch/arm/kernel/ptrace.c\n *\n *  By Ross Biro 1/23/92\n * edited by Linus Torvalds\n * ARM modifications Copyright (C) 2000 Russell King\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/ptrace.h>\n#include <linux/user.h>\n#include <linux/security.h>\n#include <linux/init.h>\n#include <linux/signal.h>\n#include <linux/uaccess.h>\n#include <linux/perf_event.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/regset.h>\n\n#include <asm/pgtable.h>\n#include <asm/system.h>\n#include <asm/traps.h>\n\n#define REG_PC\t15\n#define REG_PSR\t16\n/*\n * does not yet catch signals sent when the child dies.\n * in exit.c or in signal.c.\n */\n\n#if 0\n/*\n * Breakpoint SWI instruction: SWI &9F0001\n */\n#define BREAKINST_ARM\t0xef9f0001\n#define BREAKINST_THUMB\t0xdf00\t\t/* fill this in later */\n#else\n/*\n * New breakpoints - use an undefined instruction.  The ARM architecture\n * reference manual guarantees that the following instruction space\n * will produce an undefined instruction exception on all CPUs:\n *\n *  ARM:   xxxx 0111 1111 xxxx xxxx xxxx 1111 xxxx\n *  Thumb: 1101 1110 xxxx xxxx\n */\n#define BREAKINST_ARM\t0xe7f001f0\n#define BREAKINST_THUMB\t0xde01\n#endif\n\nstruct pt_regs_offset {\n\tconst char *name;\n\tint offset;\n};\n\n#define REG_OFFSET_NAME(r) \\\n\t{.name = #r, .offset = offsetof(struct pt_regs, ARM_##r)}\n#define REG_OFFSET_END {.name = NULL, .offset = 0}\n\nstatic const struct pt_regs_offset regoffset_table[] = {\n\tREG_OFFSET_NAME(r0),\n\tREG_OFFSET_NAME(r1),\n\tREG_OFFSET_NAME(r2),\n\tREG_OFFSET_NAME(r3),\n\tREG_OFFSET_NAME(r4),\n\tREG_OFFSET_NAME(r5),\n\tREG_OFFSET_NAME(r6),\n\tREG_OFFSET_NAME(r7),\n\tREG_OFFSET_NAME(r8),\n\tREG_OFFSET_NAME(r9),\n\tREG_OFFSET_NAME(r10),\n\tREG_OFFSET_NAME(fp),\n\tREG_OFFSET_NAME(ip),\n\tREG_OFFSET_NAME(sp),\n\tREG_OFFSET_NAME(lr),\n\tREG_OFFSET_NAME(pc),\n\tREG_OFFSET_NAME(cpsr),\n\tREG_OFFSET_NAME(ORIG_r0),\n\tREG_OFFSET_END,\n};\n\n/**\n * regs_query_register_offset() - query register offset from its name\n * @name:\tthe name of a register\n *\n * regs_query_register_offset() returns the offset of a register in struct\n * pt_regs from its name. If the name is invalid, this returns -EINVAL;\n */\nint regs_query_register_offset(const char *name)\n{\n\tconst struct pt_regs_offset *roff;\n\tfor (roff = regoffset_table; roff->name != NULL; roff++)\n\t\tif (!strcmp(roff->name, name))\n\t\t\treturn roff->offset;\n\treturn -EINVAL;\n}\n\n/**\n * regs_query_register_name() - query register name from its offset\n * @offset:\tthe offset of a register in struct pt_regs.\n *\n * regs_query_register_name() returns the name of a register from its\n * offset in struct pt_regs. If the @offset is invalid, this returns NULL;\n */\nconst char *regs_query_register_name(unsigned int offset)\n{\n\tconst struct pt_regs_offset *roff;\n\tfor (roff = regoffset_table; roff->name != NULL; roff++)\n\t\tif (roff->offset == offset)\n\t\t\treturn roff->name;\n\treturn NULL;\n}\n\n/**\n * regs_within_kernel_stack() - check the address in the stack\n * @regs:      pt_regs which contains kernel stack pointer.\n * @addr:      address which is checked.\n *\n * regs_within_kernel_stack() checks @addr is within the kernel stack page(s).\n * If @addr is within the kernel stack, it returns true. If not, returns false.\n */\nbool regs_within_kernel_stack(struct pt_regs *regs, unsigned long addr)\n{\n\treturn ((addr & ~(THREAD_SIZE - 1))  ==\n\t\t(kernel_stack_pointer(regs) & ~(THREAD_SIZE - 1)));\n}\n\n/**\n * regs_get_kernel_stack_nth() - get Nth entry of the stack\n * @regs:\tpt_regs which contains kernel stack pointer.\n * @n:\t\tstack entry number.\n *\n * regs_get_kernel_stack_nth() returns @n th entry of the kernel stack which\n * is specified by @regs. If the @n th entry is NOT in the kernel stack,\n * this returns 0.\n */\nunsigned long regs_get_kernel_stack_nth(struct pt_regs *regs, unsigned int n)\n{\n\tunsigned long *addr = (unsigned long *)kernel_stack_pointer(regs);\n\taddr += n;\n\tif (regs_within_kernel_stack(regs, (unsigned long)addr))\n\t\treturn *addr;\n\telse\n\t\treturn 0;\n}\n\n/*\n * this routine will get a word off of the processes privileged stack.\n * the offset is how far from the base addr as stored in the THREAD.\n * this routine assumes that all the privileged stacks are in our\n * data space.\n */\nstatic inline long get_user_reg(struct task_struct *task, int offset)\n{\n\treturn task_pt_regs(task)->uregs[offset];\n}\n\n/*\n * this routine will put a word on the processes privileged stack.\n * the offset is how far from the base addr as stored in the THREAD.\n * this routine assumes that all the privileged stacks are in our\n * data space.\n */\nstatic inline int\nput_user_reg(struct task_struct *task, int offset, long data)\n{\n\tstruct pt_regs newregs, *regs = task_pt_regs(task);\n\tint ret = -EINVAL;\n\n\tnewregs = *regs;\n\tnewregs.uregs[offset] = data;\n\n\tif (valid_user_regs(&newregs)) {\n\t\tregs->uregs[offset] = data;\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\n/*\n * Called by kernel/ptrace.c when detaching..\n */\nvoid ptrace_disable(struct task_struct *child)\n{\n\t/* Nothing to do. */\n}\n\n/*\n * Handle hitting a breakpoint.\n */\nvoid ptrace_break(struct task_struct *tsk, struct pt_regs *regs)\n{\n\tsiginfo_t info;\n\n\tinfo.si_signo = SIGTRAP;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = TRAP_BRKPT;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs);\n\n\tforce_sig_info(SIGTRAP, &info, tsk);\n}\n\nstatic int break_trap(struct pt_regs *regs, unsigned int instr)\n{\n\tptrace_break(current, regs);\n\treturn 0;\n}\n\nstatic struct undef_hook arm_break_hook = {\n\t.instr_mask\t= 0x0fffffff,\n\t.instr_val\t= 0x07f001f0,\n\t.cpsr_mask\t= PSR_T_BIT,\n\t.cpsr_val\t= 0,\n\t.fn\t\t= break_trap,\n};\n\nstatic struct undef_hook thumb_break_hook = {\n\t.instr_mask\t= 0xffff,\n\t.instr_val\t= 0xde01,\n\t.cpsr_mask\t= PSR_T_BIT,\n\t.cpsr_val\t= PSR_T_BIT,\n\t.fn\t\t= break_trap,\n};\n\nstatic int thumb2_break_trap(struct pt_regs *regs, unsigned int instr)\n{\n\tunsigned int instr2;\n\tvoid __user *pc;\n\n\t/* Check the second half of the instruction.  */\n\tpc = (void __user *)(instruction_pointer(regs) + 2);\n\n\tif (processor_mode(regs) == SVC_MODE) {\n\t\tinstr2 = *(u16 *) pc;\n\t} else {\n\t\tget_user(instr2, (u16 __user *)pc);\n\t}\n\n\tif (instr2 == 0xa000) {\n\t\tptrace_break(current, regs);\n\t\treturn 0;\n\t} else {\n\t\treturn 1;\n\t}\n}\n\nstatic struct undef_hook thumb2_break_hook = {\n\t.instr_mask\t= 0xffff,\n\t.instr_val\t= 0xf7f0,\n\t.cpsr_mask\t= PSR_T_BIT,\n\t.cpsr_val\t= PSR_T_BIT,\n\t.fn\t\t= thumb2_break_trap,\n};\n\nstatic int __init ptrace_break_init(void)\n{\n\tregister_undef_hook(&arm_break_hook);\n\tregister_undef_hook(&thumb_break_hook);\n\tregister_undef_hook(&thumb2_break_hook);\n\treturn 0;\n}\n\ncore_initcall(ptrace_break_init);\n\n/*\n * Read the word at offset \"off\" into the \"struct user\".  We\n * actually access the pt_regs stored on the kernel stack.\n */\nstatic int ptrace_read_user(struct task_struct *tsk, unsigned long off,\n\t\t\t    unsigned long __user *ret)\n{\n\tunsigned long tmp;\n\n\tif (off & 3 || off >= sizeof(struct user))\n\t\treturn -EIO;\n\n\ttmp = 0;\n\tif (off == PT_TEXT_ADDR)\n\t\ttmp = tsk->mm->start_code;\n\telse if (off == PT_DATA_ADDR)\n\t\ttmp = tsk->mm->start_data;\n\telse if (off == PT_TEXT_END_ADDR)\n\t\ttmp = tsk->mm->end_code;\n\telse if (off < sizeof(struct pt_regs))\n\t\ttmp = get_user_reg(tsk, off >> 2);\n\n\treturn put_user(tmp, ret);\n}\n\n/*\n * Write the word at offset \"off\" into \"struct user\".  We\n * actually access the pt_regs stored on the kernel stack.\n */\nstatic int ptrace_write_user(struct task_struct *tsk, unsigned long off,\n\t\t\t     unsigned long val)\n{\n\tif (off & 3 || off >= sizeof(struct user))\n\t\treturn -EIO;\n\n\tif (off >= sizeof(struct pt_regs))\n\t\treturn 0;\n\n\treturn put_user_reg(tsk, off >> 2, val);\n}\n\n#ifdef CONFIG_IWMMXT\n\n/*\n * Get the child iWMMXt state.\n */\nstatic int ptrace_getwmmxregs(struct task_struct *tsk, void __user *ufp)\n{\n\tstruct thread_info *thread = task_thread_info(tsk);\n\n\tif (!test_ti_thread_flag(thread, TIF_USING_IWMMXT))\n\t\treturn -ENODATA;\n\tiwmmxt_task_disable(thread);  /* force it to ram */\n\treturn copy_to_user(ufp, &thread->fpstate.iwmmxt, IWMMXT_SIZE)\n\t\t? -EFAULT : 0;\n}\n\n/*\n * Set the child iWMMXt state.\n */\nstatic int ptrace_setwmmxregs(struct task_struct *tsk, void __user *ufp)\n{\n\tstruct thread_info *thread = task_thread_info(tsk);\n\n\tif (!test_ti_thread_flag(thread, TIF_USING_IWMMXT))\n\t\treturn -EACCES;\n\tiwmmxt_task_release(thread);  /* force a reload */\n\treturn copy_from_user(&thread->fpstate.iwmmxt, ufp, IWMMXT_SIZE)\n\t\t? -EFAULT : 0;\n}\n\n#endif\n\n#ifdef CONFIG_CRUNCH\n/*\n * Get the child Crunch state.\n */\nstatic int ptrace_getcrunchregs(struct task_struct *tsk, void __user *ufp)\n{\n\tstruct thread_info *thread = task_thread_info(tsk);\n\n\tcrunch_task_disable(thread);  /* force it to ram */\n\treturn copy_to_user(ufp, &thread->crunchstate, CRUNCH_SIZE)\n\t\t? -EFAULT : 0;\n}\n\n/*\n * Set the child Crunch state.\n */\nstatic int ptrace_setcrunchregs(struct task_struct *tsk, void __user *ufp)\n{\n\tstruct thread_info *thread = task_thread_info(tsk);\n\n\tcrunch_task_release(thread);  /* force a reload */\n\treturn copy_from_user(&thread->crunchstate, ufp, CRUNCH_SIZE)\n\t\t? -EFAULT : 0;\n}\n#endif\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n/*\n * Convert a virtual register number into an index for a thread_info\n * breakpoint array. Breakpoints are identified using positive numbers\n * whilst watchpoints are negative. The registers are laid out as pairs\n * of (address, control), each pair mapping to a unique hw_breakpoint struct.\n * Register 0 is reserved for describing resource information.\n */\nstatic int ptrace_hbp_num_to_idx(long num)\n{\n\tif (num < 0)\n\t\tnum = (ARM_MAX_BRP << 1) - num;\n\treturn (num - 1) >> 1;\n}\n\n/*\n * Returns the virtual register number for the address of the\n * breakpoint at index idx.\n */\nstatic long ptrace_hbp_idx_to_num(int idx)\n{\n\tlong mid = ARM_MAX_BRP << 1;\n\tlong num = (idx << 1) + 1;\n\treturn num > mid ? mid - num : num;\n}\n\n/*\n * Handle hitting a HW-breakpoint.\n */\nstatic void ptrace_hbptriggered(struct perf_event *bp,\n\t\t\t\t     struct perf_sample_data *data,\n\t\t\t\t     struct pt_regs *regs)\n{\n\tstruct arch_hw_breakpoint *bkpt = counter_arch_bp(bp);\n\tlong num;\n\tint i;\n\tsiginfo_t info;\n\n\tfor (i = 0; i < ARM_MAX_HBP_SLOTS; ++i)\n\t\tif (current->thread.debug.hbp[i] == bp)\n\t\t\tbreak;\n\n\tnum = (i == ARM_MAX_HBP_SLOTS) ? 0 : ptrace_hbp_idx_to_num(i);\n\n\tinfo.si_signo\t= SIGTRAP;\n\tinfo.si_errno\t= (int)num;\n\tinfo.si_code\t= TRAP_HWBKPT;\n\tinfo.si_addr\t= (void __user *)(bkpt->trigger);\n\n\tforce_sig_info(SIGTRAP, &info, current);\n}\n\n/*\n * Set ptrace breakpoint pointers to zero for this task.\n * This is required in order to prevent child processes from unregistering\n * breakpoints held by their parent.\n */\nvoid clear_ptrace_hw_breakpoint(struct task_struct *tsk)\n{\n\tmemset(tsk->thread.debug.hbp, 0, sizeof(tsk->thread.debug.hbp));\n}\n\n/*\n * Unregister breakpoints from this task and reset the pointers in\n * the thread_struct.\n */\nvoid flush_ptrace_hw_breakpoint(struct task_struct *tsk)\n{\n\tint i;\n\tstruct thread_struct *t = &tsk->thread;\n\n\tfor (i = 0; i < ARM_MAX_HBP_SLOTS; i++) {\n\t\tif (t->debug.hbp[i]) {\n\t\t\tunregister_hw_breakpoint(t->debug.hbp[i]);\n\t\t\tt->debug.hbp[i] = NULL;\n\t\t}\n\t}\n}\n\nstatic u32 ptrace_get_hbp_resource_info(void)\n{\n\tu8 num_brps, num_wrps, debug_arch, wp_len;\n\tu32 reg = 0;\n\n\tnum_brps\t= hw_breakpoint_slots(TYPE_INST);\n\tnum_wrps\t= hw_breakpoint_slots(TYPE_DATA);\n\tdebug_arch\t= arch_get_debug_arch();\n\twp_len\t\t= arch_get_max_wp_len();\n\n\treg\t\t|= debug_arch;\n\treg\t\t<<= 8;\n\treg\t\t|= wp_len;\n\treg\t\t<<= 8;\n\treg\t\t|= num_wrps;\n\treg\t\t<<= 8;\n\treg\t\t|= num_brps;\n\n\treturn reg;\n}\n\nstatic struct perf_event *ptrace_hbp_create(struct task_struct *tsk, int type)\n{\n\tstruct perf_event_attr attr;\n\n\tptrace_breakpoint_init(&attr);\n\n\t/* Initialise fields to sane defaults. */\n\tattr.bp_addr\t= 0;\n\tattr.bp_len\t= HW_BREAKPOINT_LEN_4;\n\tattr.bp_type\t= type;\n\tattr.disabled\t= 1;\n\n\treturn register_user_hw_breakpoint(&attr, ptrace_hbptriggered, tsk);\n}\n\nstatic int ptrace_gethbpregs(struct task_struct *tsk, long num,\n\t\t\t     unsigned long  __user *data)\n{\n\tu32 reg;\n\tint idx, ret = 0;\n\tstruct perf_event *bp;\n\tstruct arch_hw_breakpoint_ctrl arch_ctrl;\n\n\tif (num == 0) {\n\t\treg = ptrace_get_hbp_resource_info();\n\t} else {\n\t\tidx = ptrace_hbp_num_to_idx(num);\n\t\tif (idx < 0 || idx >= ARM_MAX_HBP_SLOTS) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tbp = tsk->thread.debug.hbp[idx];\n\t\tif (!bp) {\n\t\t\treg = 0;\n\t\t\tgoto put;\n\t\t}\n\n\t\tarch_ctrl = counter_arch_bp(bp)->ctrl;\n\n\t\t/*\n\t\t * Fix up the len because we may have adjusted it\n\t\t * to compensate for an unaligned address.\n\t\t */\n\t\twhile (!(arch_ctrl.len & 0x1))\n\t\t\tarch_ctrl.len >>= 1;\n\n\t\tif (num & 0x1)\n\t\t\treg = bp->attr.bp_addr;\n\t\telse\n\t\t\treg = encode_ctrl_reg(arch_ctrl);\n\t}\n\nput:\n\tif (put_user(reg, data))\n\t\tret = -EFAULT;\n\nout:\n\treturn ret;\n}\n\nstatic int ptrace_sethbpregs(struct task_struct *tsk, long num,\n\t\t\t     unsigned long __user *data)\n{\n\tint idx, gen_len, gen_type, implied_type, ret = 0;\n\tu32 user_val;\n\tstruct perf_event *bp;\n\tstruct arch_hw_breakpoint_ctrl ctrl;\n\tstruct perf_event_attr attr;\n\n\tif (num == 0)\n\t\tgoto out;\n\telse if (num < 0)\n\t\timplied_type = HW_BREAKPOINT_RW;\n\telse\n\t\timplied_type = HW_BREAKPOINT_X;\n\n\tidx = ptrace_hbp_num_to_idx(num);\n\tif (idx < 0 || idx >= ARM_MAX_HBP_SLOTS) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (get_user(user_val, data)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tbp = tsk->thread.debug.hbp[idx];\n\tif (!bp) {\n\t\tbp = ptrace_hbp_create(tsk, implied_type);\n\t\tif (IS_ERR(bp)) {\n\t\t\tret = PTR_ERR(bp);\n\t\t\tgoto out;\n\t\t}\n\t\ttsk->thread.debug.hbp[idx] = bp;\n\t}\n\n\tattr = bp->attr;\n\n\tif (num & 0x1) {\n\t\t/* Address */\n\t\tattr.bp_addr\t= user_val;\n\t} else {\n\t\t/* Control */\n\t\tdecode_ctrl_reg(user_val, &ctrl);\n\t\tret = arch_bp_generic_fields(ctrl, &gen_len, &gen_type);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tif ((gen_type & implied_type) != gen_type) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tattr.bp_len\t= gen_len;\n\t\tattr.bp_type\t= gen_type;\n\t\tattr.disabled\t= !ctrl.enabled;\n\t}\n\n\tret = modify_user_hw_breakpoint(bp, &attr);\nout:\n\treturn ret;\n}\n#endif\n\n/* regset get/set implementations */\n\nstatic int gpr_get(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n\tstruct pt_regs *regs = task_pt_regs(target);\n\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t   regs,\n\t\t\t\t   0, sizeof(*regs));\n}\n\nstatic int gpr_set(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   const void *kbuf, const void __user *ubuf)\n{\n\tint ret;\n\tstruct pt_regs newregs;\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t &newregs,\n\t\t\t\t 0, sizeof(newregs));\n\tif (ret)\n\t\treturn ret;\n\n\tif (!valid_user_regs(&newregs))\n\t\treturn -EINVAL;\n\n\t*task_pt_regs(target) = newregs;\n\treturn 0;\n}\n\nstatic int fpa_get(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t   &task_thread_info(target)->fpstate,\n\t\t\t\t   0, sizeof(struct user_fp));\n}\n\nstatic int fpa_set(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   const void *kbuf, const void __user *ubuf)\n{\n\tstruct thread_info *thread = task_thread_info(target);\n\n\tthread->used_cp[1] = thread->used_cp[2] = 1;\n\n\treturn user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t&thread->fpstate,\n\t\t0, sizeof(struct user_fp));\n}\n\n#ifdef CONFIG_VFP\n/*\n * VFP register get/set implementations.\n *\n * With respect to the kernel, struct user_fp is divided into three chunks:\n * 16 or 32 real VFP registers (d0-d15 or d0-31)\n *\tThese are transferred to/from the real registers in the task's\n *\tvfp_hard_struct.  The number of registers depends on the kernel\n *\tconfiguration.\n *\n * 16 or 0 fake VFP registers (d16-d31 or empty)\n *\ti.e., the user_vfp structure has space for 32 registers even if\n *\tthe kernel doesn't have them all.\n *\n *\tvfp_get() reads this chunk as zero where applicable\n *\tvfp_set() ignores this chunk\n *\n * 1 word for the FPSCR\n *\n * The bounds-checking logic built into user_regset_copyout and friends\n * means that we can make a simple sequence of calls to map the relevant data\n * to/from the specified slice of the user regset structure.\n */\nstatic int vfp_get(struct task_struct *target,\n\t\t   const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n\tint ret;\n\tstruct thread_info *thread = task_thread_info(target);\n\tstruct vfp_hard_struct const *vfp = &thread->vfpstate.hard;\n\tconst size_t user_fpregs_offset = offsetof(struct user_vfp, fpregs);\n\tconst size_t user_fpscr_offset = offsetof(struct user_vfp, fpscr);\n\n\tvfp_sync_hwstate(thread);\n\n\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  &vfp->fpregs,\n\t\t\t\t  user_fpregs_offset,\n\t\t\t\t  user_fpregs_offset + sizeof(vfp->fpregs));\n\tif (ret)\n\t\treturn ret;\n\n\tret = user_regset_copyout_zero(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t       user_fpregs_offset + sizeof(vfp->fpregs),\n\t\t\t\t       user_fpscr_offset);\n\tif (ret)\n\t\treturn ret;\n\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t   &vfp->fpscr,\n\t\t\t\t   user_fpscr_offset,\n\t\t\t\t   user_fpscr_offset + sizeof(vfp->fpscr));\n}\n\n/*\n * For vfp_set() a read-modify-write is done on the VFP registers,\n * in order to avoid writing back a half-modified set of registers on\n * failure.\n */\nstatic int vfp_set(struct task_struct *target,\n\t\t\t  const struct user_regset *regset,\n\t\t\t  unsigned int pos, unsigned int count,\n\t\t\t  const void *kbuf, const void __user *ubuf)\n{\n\tint ret;\n\tstruct thread_info *thread = task_thread_info(target);\n\tstruct vfp_hard_struct new_vfp = thread->vfpstate.hard;\n\tconst size_t user_fpregs_offset = offsetof(struct user_vfp, fpregs);\n\tconst size_t user_fpscr_offset = offsetof(struct user_vfp, fpscr);\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  &new_vfp.fpregs,\n\t\t\t\t  user_fpregs_offset,\n\t\t\t\t  user_fpregs_offset + sizeof(new_vfp.fpregs));\n\tif (ret)\n\t\treturn ret;\n\n\tret = user_regset_copyin_ignore(&pos, &count, &kbuf, &ubuf,\n\t\t\t\tuser_fpregs_offset + sizeof(new_vfp.fpregs),\n\t\t\t\tuser_fpscr_offset);\n\tif (ret)\n\t\treturn ret;\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t &new_vfp.fpscr,\n\t\t\t\t user_fpscr_offset,\n\t\t\t\t user_fpscr_offset + sizeof(new_vfp.fpscr));\n\tif (ret)\n\t\treturn ret;\n\n\tvfp_sync_hwstate(thread);\n\tthread->vfpstate.hard = new_vfp;\n\tvfp_flush_hwstate(thread);\n\n\treturn 0;\n}\n#endif /* CONFIG_VFP */\n\nenum arm_regset {\n\tREGSET_GPR,\n\tREGSET_FPR,\n#ifdef CONFIG_VFP\n\tREGSET_VFP,\n#endif\n};\n\nstatic const struct user_regset arm_regsets[] = {\n\t[REGSET_GPR] = {\n\t\t.core_note_type = NT_PRSTATUS,\n\t\t.n = ELF_NGREG,\n\t\t.size = sizeof(u32),\n\t\t.align = sizeof(u32),\n\t\t.get = gpr_get,\n\t\t.set = gpr_set\n\t},\n\t[REGSET_FPR] = {\n\t\t/*\n\t\t * For the FPA regs in fpstate, the real fields are a mixture\n\t\t * of sizes, so pretend that the registers are word-sized:\n\t\t */\n\t\t.core_note_type = NT_PRFPREG,\n\t\t.n = sizeof(struct user_fp) / sizeof(u32),\n\t\t.size = sizeof(u32),\n\t\t.align = sizeof(u32),\n\t\t.get = fpa_get,\n\t\t.set = fpa_set\n\t},\n#ifdef CONFIG_VFP\n\t[REGSET_VFP] = {\n\t\t/*\n\t\t * Pretend that the VFP regs are word-sized, since the FPSCR is\n\t\t * a single word dangling at the end of struct user_vfp:\n\t\t */\n\t\t.core_note_type = NT_ARM_VFP,\n\t\t.n = ARM_VFPREGS_SIZE / sizeof(u32),\n\t\t.size = sizeof(u32),\n\t\t.align = sizeof(u32),\n\t\t.get = vfp_get,\n\t\t.set = vfp_set\n\t},\n#endif /* CONFIG_VFP */\n};\n\nstatic const struct user_regset_view user_arm_view = {\n\t.name = \"arm\", .e_machine = ELF_ARCH, .ei_osabi = ELF_OSABI,\n\t.regsets = arm_regsets, .n = ARRAY_SIZE(arm_regsets)\n};\n\nconst struct user_regset_view *task_user_regset_view(struct task_struct *task)\n{\n\treturn &user_arm_view;\n}\n\nlong arch_ptrace(struct task_struct *child, long request,\n\t\t unsigned long addr, unsigned long data)\n{\n\tint ret;\n\tunsigned long __user *datap = (unsigned long __user *) data;\n\n\tswitch (request) {\n\t\tcase PTRACE_PEEKUSR:\n\t\t\tret = ptrace_read_user(child, addr, datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_POKEUSR:\n\t\t\tret = ptrace_write_user(child, addr, data);\n\t\t\tbreak;\n\n\t\tcase PTRACE_GETREGS:\n\t\t\tret = copy_regset_to_user(child,\n\t\t\t\t\t\t  &user_arm_view, REGSET_GPR,\n\t\t\t\t\t\t  0, sizeof(struct pt_regs),\n\t\t\t\t\t\t  datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETREGS:\n\t\t\tret = copy_regset_from_user(child,\n\t\t\t\t\t\t    &user_arm_view, REGSET_GPR,\n\t\t\t\t\t\t    0, sizeof(struct pt_regs),\n\t\t\t\t\t\t    datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_GETFPREGS:\n\t\t\tret = copy_regset_to_user(child,\n\t\t\t\t\t\t  &user_arm_view, REGSET_FPR,\n\t\t\t\t\t\t  0, sizeof(union fp_state),\n\t\t\t\t\t\t  datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETFPREGS:\n\t\t\tret = copy_regset_from_user(child,\n\t\t\t\t\t\t    &user_arm_view, REGSET_FPR,\n\t\t\t\t\t\t    0, sizeof(union fp_state),\n\t\t\t\t\t\t    datap);\n\t\t\tbreak;\n\n#ifdef CONFIG_IWMMXT\n\t\tcase PTRACE_GETWMMXREGS:\n\t\t\tret = ptrace_getwmmxregs(child, datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETWMMXREGS:\n\t\t\tret = ptrace_setwmmxregs(child, datap);\n\t\t\tbreak;\n#endif\n\n\t\tcase PTRACE_GET_THREAD_AREA:\n\t\t\tret = put_user(task_thread_info(child)->tp_value,\n\t\t\t\t       datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SET_SYSCALL:\n\t\t\ttask_thread_info(child)->syscall = data;\n\t\t\tret = 0;\n\t\t\tbreak;\n\n#ifdef CONFIG_CRUNCH\n\t\tcase PTRACE_GETCRUNCHREGS:\n\t\t\tret = ptrace_getcrunchregs(child, datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETCRUNCHREGS:\n\t\t\tret = ptrace_setcrunchregs(child, datap);\n\t\t\tbreak;\n#endif\n\n#ifdef CONFIG_VFP\n\t\tcase PTRACE_GETVFPREGS:\n\t\t\tret = copy_regset_to_user(child,\n\t\t\t\t\t\t  &user_arm_view, REGSET_VFP,\n\t\t\t\t\t\t  0, ARM_VFPREGS_SIZE,\n\t\t\t\t\t\t  datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETVFPREGS:\n\t\t\tret = copy_regset_from_user(child,\n\t\t\t\t\t\t    &user_arm_view, REGSET_VFP,\n\t\t\t\t\t\t    0, ARM_VFPREGS_SIZE,\n\t\t\t\t\t\t    datap);\n\t\t\tbreak;\n#endif\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\t\tcase PTRACE_GETHBPREGS:\n\t\t\tif (ptrace_get_breakpoints(child) < 0)\n\t\t\t\treturn -ESRCH;\n\n\t\t\tret = ptrace_gethbpregs(child, addr,\n\t\t\t\t\t\t(unsigned long __user *)data);\n\t\t\tptrace_put_breakpoints(child);\n\t\t\tbreak;\n\t\tcase PTRACE_SETHBPREGS:\n\t\t\tif (ptrace_get_breakpoints(child) < 0)\n\t\t\t\treturn -ESRCH;\n\n\t\t\tret = ptrace_sethbpregs(child, addr,\n\t\t\t\t\t\t(unsigned long __user *)data);\n\t\t\tptrace_put_breakpoints(child);\n\t\t\tbreak;\n#endif\n\n\t\tdefault:\n\t\t\tret = ptrace_request(child, request, addr, data);\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nasmlinkage int syscall_trace(int why, struct pt_regs *regs, int scno)\n{\n\tunsigned long ip;\n\n\tif (!test_thread_flag(TIF_SYSCALL_TRACE))\n\t\treturn scno;\n\tif (!(current->ptrace & PT_PTRACED))\n\t\treturn scno;\n\n\t/*\n\t * Save IP.  IP is used to denote syscall entry/exit:\n\t *  IP = 0 -> entry, = 1 -> exit\n\t */\n\tip = regs->ARM_ip;\n\tregs->ARM_ip = why;\n\n\tcurrent_thread_info()->syscall = scno;\n\n\t/* the 0x80 provides a way for the tracing parent to distinguish\n\t   between a syscall stop and SIGTRAP delivery */\n\tptrace_notify(SIGTRAP | ((current->ptrace & PT_TRACESYSGOOD)\n\t\t\t\t ? 0x80 : 0));\n\t/*\n\t * this isn't the same as continuing with a signal, but it will do\n\t * for normal use.  strace only continues with a signal if the\n\t * stopping signal is not SIGTRAP.  -brl\n\t */\n\tif (current->exit_code) {\n\t\tsend_sig(current->exit_code, current, 1);\n\t\tcurrent->exit_code = 0;\n\t}\n\tregs->ARM_ip = ip;\n\n\treturn current_thread_info()->syscall;\n}\n", "/*\n *  linux/arch/arm/kernel/swp_emulate.c\n *\n *  Copyright (C) 2009 ARM Limited\n *  __user_* functions adapted from include/asm/uaccess.h\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n *  Implements emulation of the SWP/SWPB instructions using load-exclusive and\n *  store-exclusive for processors that have them disabled (or future ones that\n *  might not implement them).\n *\n *  Syntax of SWP{B} instruction: SWP{B}<c> <Rt>, <Rt2>, [<Rn>]\n *  Where: Rt  = destination\n *\t   Rt2 = source\n *\t   Rn  = address\n */\n\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/proc_fs.h>\n#include <linux/sched.h>\n#include <linux/syscalls.h>\n#include <linux/perf_event.h>\n\n#include <asm/traps.h>\n#include <asm/uaccess.h>\n\n/*\n * Error-checking SWP macros implemented using ldrex{b}/strex{b}\n */\n#define __user_swpX_asm(data, addr, res, temp, B)\t\t\\\n\t__asm__ __volatile__(\t\t\t\t\t\\\n\t\"\tmov\t\t%2, %1\\n\"\t\t\t\\\n\t\"0:\tldrex\"B\"\t%1, [%3]\\n\"\t\t\t\\\n\t\"1:\tstrex\"B\"\t%0, %2, [%3]\\n\"\t\t\t\\\n\t\"\tcmp\t\t%0, #0\\n\"\t\t\t\\\n\t\"\tmovne\t\t%0, %4\\n\"\t\t\t\\\n\t\"2:\\n\"\t\t\t\t\t\t\t\\\n\t\"\t.section\t .fixup,\\\"ax\\\"\\n\"\t\t\\\n\t\"\t.align\t\t2\\n\"\t\t\t\t\\\n\t\"3:\tmov\t\t%0, %5\\n\"\t\t\t\\\n\t\"\tb\t\t2b\\n\"\t\t\t\t\\\n\t\"\t.previous\\n\"\t\t\t\t\t\\\n\t\"\t.section\t __ex_table,\\\"a\\\"\\n\"\t\t\\\n\t\"\t.align\t\t3\\n\"\t\t\t\t\\\n\t\"\t.long\t\t0b, 3b\\n\"\t\t\t\\\n\t\"\t.long\t\t1b, 3b\\n\"\t\t\t\\\n\t\"\t.previous\"\t\t\t\t\t\\\n\t: \"=&r\" (res), \"+r\" (data), \"=&r\" (temp)\t\t\\\n\t: \"r\" (addr), \"i\" (-EAGAIN), \"i\" (-EFAULT)\t\t\\\n\t: \"cc\", \"memory\")\n\n#define __user_swp_asm(data, addr, res, temp) \\\n\t__user_swpX_asm(data, addr, res, temp, \"\")\n#define __user_swpb_asm(data, addr, res, temp) \\\n\t__user_swpX_asm(data, addr, res, temp, \"b\")\n\n/*\n * Macros/defines for extracting register numbers from instruction.\n */\n#define EXTRACT_REG_NUM(instruction, offset) \\\n\t(((instruction) & (0xf << (offset))) >> (offset))\n#define RN_OFFSET  16\n#define RT_OFFSET  12\n#define RT2_OFFSET  0\n/*\n * Bit 22 of the instruction encoding distinguishes between\n * the SWP and SWPB variants (bit set means SWPB).\n */\n#define TYPE_SWPB (1 << 22)\n\nstatic unsigned long swpcounter;\nstatic unsigned long swpbcounter;\nstatic unsigned long abtcounter;\nstatic pid_t         previous_pid;\n\n#ifdef CONFIG_PROC_FS\nstatic int proc_read_status(char *page, char **start, off_t off, int count,\n\t\t\t    int *eof, void *data)\n{\n\tchar *p = page;\n\tint len;\n\n\tp += sprintf(p, \"Emulated SWP:\\t\\t%lu\\n\", swpcounter);\n\tp += sprintf(p, \"Emulated SWPB:\\t\\t%lu\\n\", swpbcounter);\n\tp += sprintf(p, \"Aborted SWP{B}:\\t\\t%lu\\n\", abtcounter);\n\tif (previous_pid != 0)\n\t\tp += sprintf(p, \"Last process:\\t\\t%d\\n\", previous_pid);\n\n\tlen = (p - page) - off;\n\tif (len < 0)\n\t\tlen = 0;\n\n\t*eof = (len <= count) ? 1 : 0;\n\t*start = page + off;\n\n\treturn len;\n}\n#endif\n\n/*\n * Set up process info to signal segmentation fault - called on access error.\n */\nstatic void set_segfault(struct pt_regs *regs, unsigned long addr)\n{\n\tsiginfo_t info;\n\n\tif (find_vma(current->mm, addr) == NULL)\n\t\tinfo.si_code = SEGV_MAPERR;\n\telse\n\t\tinfo.si_code = SEGV_ACCERR;\n\n\tinfo.si_signo = SIGSEGV;\n\tinfo.si_errno = 0;\n\tinfo.si_addr  = (void *) instruction_pointer(regs);\n\n\tpr_debug(\"SWP{B} emulation: access caused memory abort!\\n\");\n\tarm_notify_die(\"Illegal memory access\", regs, &info, 0, 0);\n\n\tabtcounter++;\n}\n\nstatic int emulate_swpX(unsigned int address, unsigned int *data,\n\t\t\tunsigned int type)\n{\n\tunsigned int res = 0;\n\n\tif ((type != TYPE_SWPB) && (address & 0x3)) {\n\t\t/* SWP to unaligned address not permitted */\n\t\tpr_debug(\"SWP instruction on unaligned pointer!\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\twhile (1) {\n\t\tunsigned long temp;\n\n\t\t/*\n\t\t * Barrier required between accessing protected resource and\n\t\t * releasing a lock for it. Legacy code might not have done\n\t\t * this, and we cannot determine that this is not the case\n\t\t * being emulated, so insert always.\n\t\t */\n\t\tsmp_mb();\n\n\t\tif (type == TYPE_SWPB)\n\t\t\t__user_swpb_asm(*data, address, res, temp);\n\t\telse\n\t\t\t__user_swp_asm(*data, address, res, temp);\n\n\t\tif (likely(res != -EAGAIN) || signal_pending(current))\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n\n\tif (res == 0) {\n\t\t/*\n\t\t * Barrier also required between acquiring a lock for a\n\t\t * protected resource and accessing the resource. Inserted for\n\t\t * same reason as above.\n\t\t */\n\t\tsmp_mb();\n\n\t\tif (type == TYPE_SWPB)\n\t\t\tswpbcounter++;\n\t\telse\n\t\t\tswpcounter++;\n\t}\n\n\treturn res;\n}\n\n/*\n * swp_handler logs the id of calling process, dissects the instruction, sanity\n * checks the memory location, calls emulate_swpX for the actual operation and\n * deals with fixup/error handling before returning\n */\nstatic int swp_handler(struct pt_regs *regs, unsigned int instr)\n{\n\tunsigned int address, destreg, data, type;\n\tunsigned int res = 0;\n\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, regs->ARM_pc);\n\n\tif (current->pid != previous_pid) {\n\t\tpr_debug(\"\\\"%s\\\" (%ld) uses deprecated SWP{B} instruction\\n\",\n\t\t\t current->comm, (unsigned long)current->pid);\n\t\tprevious_pid = current->pid;\n\t}\n\n\taddress = regs->uregs[EXTRACT_REG_NUM(instr, RN_OFFSET)];\n\tdata\t= regs->uregs[EXTRACT_REG_NUM(instr, RT2_OFFSET)];\n\tdestreg = EXTRACT_REG_NUM(instr, RT_OFFSET);\n\n\ttype = instr & TYPE_SWPB;\n\n\tpr_debug(\"addr in r%d->0x%08x, dest is r%d, source in r%d->0x%08x)\\n\",\n\t\t EXTRACT_REG_NUM(instr, RN_OFFSET), address,\n\t\t destreg, EXTRACT_REG_NUM(instr, RT2_OFFSET), data);\n\n\t/* Check access in reasonable access range for both SWP and SWPB */\n\tif (!access_ok(VERIFY_WRITE, (address & ~3), 4)) {\n\t\tpr_debug(\"SWP{B} emulation: access to %p not allowed!\\n\",\n\t\t\t (void *)address);\n\t\tres = -EFAULT;\n\t} else {\n\t\tres = emulate_swpX(address, &data, type);\n\t}\n\n\tif (res == 0) {\n\t\t/*\n\t\t * On successful emulation, revert the adjustment to the PC\n\t\t * made in kernel/traps.c in order to resume execution at the\n\t\t * instruction following the SWP{B}.\n\t\t */\n\t\tregs->ARM_pc += 4;\n\t\tregs->uregs[destreg] = data;\n\t} else if (res == -EFAULT) {\n\t\t/*\n\t\t * Memory errors do not mean emulation failed.\n\t\t * Set up signal info to return SEGV, then return OK\n\t\t */\n\t\tset_segfault(regs, address);\n\t}\n\n\treturn 0;\n}\n\n/*\n * Only emulate SWP/SWPB executed in ARM state/User mode.\n * The kernel must be SWP free and SWP{B} does not exist in Thumb/ThumbEE.\n */\nstatic struct undef_hook swp_hook = {\n\t.instr_mask = 0x0fb00ff0,\n\t.instr_val  = 0x01000090,\n\t.cpsr_mask  = MODE_MASK | PSR_T_BIT | PSR_J_BIT,\n\t.cpsr_val   = USR_MODE,\n\t.fn\t    = swp_handler\n};\n\n/*\n * Register handler and create status file in /proc/cpu\n * Invoked as late_initcall, since not needed before init spawned.\n */\nstatic int __init swp_emulation_init(void)\n{\n#ifdef CONFIG_PROC_FS\n\tstruct proc_dir_entry *res;\n\n\tres = create_proc_entry(\"cpu/swp_emulation\", S_IRUGO, NULL);\n\n\tif (!res)\n\t\treturn -ENOMEM;\n\n\tres->read_proc = proc_read_status;\n#endif /* CONFIG_PROC_FS */\n\n\tprintk(KERN_NOTICE \"Registering SWP/SWPB emulation handler\\n\");\n\tregister_undef_hook(&swp_hook);\n\n\treturn 0;\n}\n\nlate_initcall(swp_emulation_init);\n", "/*\n *  linux/arch/arm/mm/fault.c\n *\n *  Copyright (C) 1995  Linus Torvalds\n *  Modifications for ARM processor (c) 1995-2004 Russell King\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n#include <linux/module.h>\n#include <linux/signal.h>\n#include <linux/mm.h>\n#include <linux/hardirq.h>\n#include <linux/init.h>\n#include <linux/kprobes.h>\n#include <linux/uaccess.h>\n#include <linux/page-flags.h>\n#include <linux/sched.h>\n#include <linux/highmem.h>\n#include <linux/perf_event.h>\n\n#include <asm/system.h>\n#include <asm/pgtable.h>\n#include <asm/tlbflush.h>\n\n#include \"fault.h\"\n\n/*\n * Fault status register encodings.  We steal bit 31 for our own purposes.\n */\n#define FSR_LNX_PF\t\t(1 << 31)\n#define FSR_WRITE\t\t(1 << 11)\n#define FSR_FS4\t\t\t(1 << 10)\n#define FSR_FS3_0\t\t(15)\n\nstatic inline int fsr_fs(unsigned int fsr)\n{\n\treturn (fsr & FSR_FS3_0) | (fsr & FSR_FS4) >> 6;\n}\n\n#ifdef CONFIG_MMU\n\n#ifdef CONFIG_KPROBES\nstatic inline int notify_page_fault(struct pt_regs *regs, unsigned int fsr)\n{\n\tint ret = 0;\n\n\tif (!user_mode(regs)) {\n\t\t/* kprobe_running() needs smp_processor_id() */\n\t\tpreempt_disable();\n\t\tif (kprobe_running() && kprobe_fault_handler(regs, fsr))\n\t\t\tret = 1;\n\t\tpreempt_enable();\n\t}\n\n\treturn ret;\n}\n#else\nstatic inline int notify_page_fault(struct pt_regs *regs, unsigned int fsr)\n{\n\treturn 0;\n}\n#endif\n\n/*\n * This is useful to dump out the page tables associated with\n * 'addr' in mm 'mm'.\n */\nvoid show_pte(struct mm_struct *mm, unsigned long addr)\n{\n\tpgd_t *pgd;\n\n\tif (!mm)\n\t\tmm = &init_mm;\n\n\tprintk(KERN_ALERT \"pgd = %p\\n\", mm->pgd);\n\tpgd = pgd_offset(mm, addr);\n\tprintk(KERN_ALERT \"[%08lx] *pgd=%08llx\",\n\t\t\taddr, (long long)pgd_val(*pgd));\n\n\tdo {\n\t\tpud_t *pud;\n\t\tpmd_t *pmd;\n\t\tpte_t *pte;\n\n\t\tif (pgd_none(*pgd))\n\t\t\tbreak;\n\n\t\tif (pgd_bad(*pgd)) {\n\t\t\tprintk(\"(bad)\");\n\t\t\tbreak;\n\t\t}\n\n\t\tpud = pud_offset(pgd, addr);\n\t\tif (PTRS_PER_PUD != 1)\n\t\t\tprintk(\", *pud=%08lx\", pud_val(*pud));\n\n\t\tif (pud_none(*pud))\n\t\t\tbreak;\n\n\t\tif (pud_bad(*pud)) {\n\t\t\tprintk(\"(bad)\");\n\t\t\tbreak;\n\t\t}\n\n\t\tpmd = pmd_offset(pud, addr);\n\t\tif (PTRS_PER_PMD != 1)\n\t\t\tprintk(\", *pmd=%08llx\", (long long)pmd_val(*pmd));\n\n\t\tif (pmd_none(*pmd))\n\t\t\tbreak;\n\n\t\tif (pmd_bad(*pmd)) {\n\t\t\tprintk(\"(bad)\");\n\t\t\tbreak;\n\t\t}\n\n\t\t/* We must not map this if we have highmem enabled */\n\t\tif (PageHighMem(pfn_to_page(pmd_val(*pmd) >> PAGE_SHIFT)))\n\t\t\tbreak;\n\n\t\tpte = pte_offset_map(pmd, addr);\n\t\tprintk(\", *pte=%08llx\", (long long)pte_val(*pte));\n\t\tprintk(\", *ppte=%08llx\",\n\t\t       (long long)pte_val(pte[PTE_HWTABLE_PTRS]));\n\t\tpte_unmap(pte);\n\t} while(0);\n\n\tprintk(\"\\n\");\n}\n#else\t\t\t\t\t/* CONFIG_MMU */\nvoid show_pte(struct mm_struct *mm, unsigned long addr)\n{ }\n#endif\t\t\t\t\t/* CONFIG_MMU */\n\n/*\n * Oops.  The kernel tried to access some page that wasn't present.\n */\nstatic void\n__do_kernel_fault(struct mm_struct *mm, unsigned long addr, unsigned int fsr,\n\t\t  struct pt_regs *regs)\n{\n\t/*\n\t * Are we prepared to handle this kernel fault?\n\t */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\t/*\n\t * No handler, we'll have to terminate things with extreme prejudice.\n\t */\n\tbust_spinlocks(1);\n\tprintk(KERN_ALERT\n\t\t\"Unable to handle kernel %s at virtual address %08lx\\n\",\n\t\t(addr < PAGE_SIZE) ? \"NULL pointer dereference\" :\n\t\t\"paging request\", addr);\n\n\tshow_pte(mm, addr);\n\tdie(\"Oops\", regs, fsr);\n\tbust_spinlocks(0);\n\tdo_exit(SIGKILL);\n}\n\n/*\n * Something tried to access memory that isn't in our memory map..\n * User mode accesses just cause a SIGSEGV\n */\nstatic void\n__do_user_fault(struct task_struct *tsk, unsigned long addr,\n\t\tunsigned int fsr, unsigned int sig, int code,\n\t\tstruct pt_regs *regs)\n{\n\tstruct siginfo si;\n\n#ifdef CONFIG_DEBUG_USER\n\tif (user_debug & UDBG_SEGV) {\n\t\tprintk(KERN_DEBUG \"%s: unhandled page fault (%d) at 0x%08lx, code 0x%03x\\n\",\n\t\t       tsk->comm, sig, addr, fsr);\n\t\tshow_pte(tsk->mm, addr);\n\t\tshow_regs(regs);\n\t}\n#endif\n\n\ttsk->thread.address = addr;\n\ttsk->thread.error_code = fsr;\n\ttsk->thread.trap_no = 14;\n\tsi.si_signo = sig;\n\tsi.si_errno = 0;\n\tsi.si_code = code;\n\tsi.si_addr = (void __user *)addr;\n\tforce_sig_info(sig, &si, tsk);\n}\n\nvoid do_bad_area(unsigned long addr, unsigned int fsr, struct pt_regs *regs)\n{\n\tstruct task_struct *tsk = current;\n\tstruct mm_struct *mm = tsk->active_mm;\n\n\t/*\n\t * If we are in kernel mode at this point, we\n\t * have no context to handle this fault with.\n\t */\n\tif (user_mode(regs))\n\t\t__do_user_fault(tsk, addr, fsr, SIGSEGV, SEGV_MAPERR, regs);\n\telse\n\t\t__do_kernel_fault(mm, addr, fsr, regs);\n}\n\n#ifdef CONFIG_MMU\n#define VM_FAULT_BADMAP\t\t0x010000\n#define VM_FAULT_BADACCESS\t0x020000\n\n/*\n * Check that the permissions on the VMA allow for the fault which occurred.\n * If we encountered a write fault, we must have write permission, otherwise\n * we allow any permission.\n */\nstatic inline bool access_error(unsigned int fsr, struct vm_area_struct *vma)\n{\n\tunsigned int mask = VM_READ | VM_WRITE | VM_EXEC;\n\n\tif (fsr & FSR_WRITE)\n\t\tmask = VM_WRITE;\n\tif (fsr & FSR_LNX_PF)\n\t\tmask = VM_EXEC;\n\n\treturn vma->vm_flags & mask ? false : true;\n}\n\nstatic int __kprobes\n__do_page_fault(struct mm_struct *mm, unsigned long addr, unsigned int fsr,\n\t\tstruct task_struct *tsk)\n{\n\tstruct vm_area_struct *vma;\n\tint fault;\n\n\tvma = find_vma(mm, addr);\n\tfault = VM_FAULT_BADMAP;\n\tif (unlikely(!vma))\n\t\tgoto out;\n\tif (unlikely(vma->vm_start > addr))\n\t\tgoto check_stack;\n\n\t/*\n\t * Ok, we have a good vm_area for this\n\t * memory access, so we can handle it.\n\t */\ngood_area:\n\tif (access_error(fsr, vma)) {\n\t\tfault = VM_FAULT_BADACCESS;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault, make\n\t * sure we exit gracefully rather than endlessly redo the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, addr & PAGE_MASK, (fsr & FSR_WRITE) ? FAULT_FLAG_WRITE : 0);\n\tif (unlikely(fault & VM_FAULT_ERROR))\n\t\treturn fault;\n\tif (fault & VM_FAULT_MAJOR)\n\t\ttsk->maj_flt++;\n\telse\n\t\ttsk->min_flt++;\n\treturn fault;\n\ncheck_stack:\n\tif (vma->vm_flags & VM_GROWSDOWN && !expand_stack(vma, addr))\n\t\tgoto good_area;\nout:\n\treturn fault;\n}\n\nstatic int __kprobes\ndo_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tint fault, sig, code;\n\n\tif (notify_page_fault(regs, fsr))\n\t\treturn 0;\n\n\ttsk = current;\n\tmm  = tsk->mm;\n\n\t/*\n\t * If we're in an interrupt or have no user\n\t * context, we must not take the fault..\n\t */\n\tif (in_atomic() || !mm)\n\t\tgoto no_context;\n\n\t/*\n\t * As per x86, we may deadlock here.  However, since the kernel only\n\t * validly references user space from well defined areas of the code,\n\t * we can bug out early if this is from code which shouldn't.\n\t */\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tif (!user_mode(regs) && !search_exception_tables(regs->ARM_pc))\n\t\t\tgoto no_context;\n\t\tdown_read(&mm->mmap_sem);\n\t} else {\n\t\t/*\n\t\t * The above down_read_trylock() might have succeeded in\n\t\t * which case, we'll have missed the might_sleep() from\n\t\t * down_read()\n\t\t */\n\t\tmight_sleep();\n#ifdef CONFIG_DEBUG_VM\n\t\tif (!user_mode(regs) &&\n\t\t    !search_exception_tables(regs->ARM_pc))\n\t\t\tgoto no_context;\n#endif\n\t}\n\n\tfault = __do_page_fault(mm, addr, fsr, tsk);\n\tup_read(&mm->mmap_sem);\n\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, addr);\n\tif (fault & VM_FAULT_MAJOR)\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs, addr);\n\telse if (fault & VM_FAULT_MINOR)\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs, addr);\n\n\t/*\n\t * Handle the \"normal\" case first - VM_FAULT_MAJOR / VM_FAULT_MINOR\n\t */\n\tif (likely(!(fault & (VM_FAULT_ERROR | VM_FAULT_BADMAP | VM_FAULT_BADACCESS))))\n\t\treturn 0;\n\n\tif (fault & VM_FAULT_OOM) {\n\t\t/*\n\t\t * We ran out of memory, call the OOM killer, and return to\n\t\t * userspace (which will retry the fault, or kill us if we\n\t\t * got oom-killed)\n\t\t */\n\t\tpagefault_out_of_memory();\n\t\treturn 0;\n\t}\n\n\t/*\n\t * If we are in kernel mode at this point, we\n\t * have no context to handle this fault with.\n\t */\n\tif (!user_mode(regs))\n\t\tgoto no_context;\n\n\tif (fault & VM_FAULT_SIGBUS) {\n\t\t/*\n\t\t * We had some memory, but were unable to\n\t\t * successfully fix up this page fault.\n\t\t */\n\t\tsig = SIGBUS;\n\t\tcode = BUS_ADRERR;\n\t} else {\n\t\t/*\n\t\t * Something tried to access memory that\n\t\t * isn't in our memory map..\n\t\t */\n\t\tsig = SIGSEGV;\n\t\tcode = fault == VM_FAULT_BADACCESS ?\n\t\t\tSEGV_ACCERR : SEGV_MAPERR;\n\t}\n\n\t__do_user_fault(tsk, addr, fsr, sig, code, regs);\n\treturn 0;\n\nno_context:\n\t__do_kernel_fault(mm, addr, fsr, regs);\n\treturn 0;\n}\n#else\t\t\t\t\t/* CONFIG_MMU */\nstatic int\ndo_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)\n{\n\treturn 0;\n}\n#endif\t\t\t\t\t/* CONFIG_MMU */\n\n/*\n * First Level Translation Fault Handler\n *\n * We enter here because the first level page table doesn't contain\n * a valid entry for the address.\n *\n * If the address is in kernel space (>= TASK_SIZE), then we are\n * probably faulting in the vmalloc() area.\n *\n * If the init_task's first level page tables contains the relevant\n * entry, we copy the it to this task.  If not, we send the process\n * a signal, fixup the exception, or oops the kernel.\n *\n * NOTE! We MUST NOT take any locks for this case. We may be in an\n * interrupt or a critical region, and should only copy the information\n * from the master page table, nothing more.\n */\n#ifdef CONFIG_MMU\nstatic int __kprobes\ndo_translation_fault(unsigned long addr, unsigned int fsr,\n\t\t     struct pt_regs *regs)\n{\n\tunsigned int index;\n\tpgd_t *pgd, *pgd_k;\n\tpud_t *pud, *pud_k;\n\tpmd_t *pmd, *pmd_k;\n\n\tif (addr < TASK_SIZE)\n\t\treturn do_page_fault(addr, fsr, regs);\n\n\tif (user_mode(regs))\n\t\tgoto bad_area;\n\n\tindex = pgd_index(addr);\n\n\t/*\n\t * FIXME: CP15 C1 is write only on ARMv3 architectures.\n\t */\n\tpgd = cpu_get_pgd() + index;\n\tpgd_k = init_mm.pgd + index;\n\n\tif (pgd_none(*pgd_k))\n\t\tgoto bad_area;\n\tif (!pgd_present(*pgd))\n\t\tset_pgd(pgd, *pgd_k);\n\n\tpud = pud_offset(pgd, addr);\n\tpud_k = pud_offset(pgd_k, addr);\n\n\tif (pud_none(*pud_k))\n\t\tgoto bad_area;\n\tif (!pud_present(*pud))\n\t\tset_pud(pud, *pud_k);\n\n\tpmd = pmd_offset(pud, addr);\n\tpmd_k = pmd_offset(pud_k, addr);\n\n\t/*\n\t * On ARM one Linux PGD entry contains two hardware entries (see page\n\t * tables layout in pgtable.h). We normally guarantee that we always\n\t * fill both L1 entries. But create_mapping() doesn't follow the rule.\n\t * It can create inidividual L1 entries, so here we have to call\n\t * pmd_none() check for the entry really corresponded to address, not\n\t * for the first of pair.\n\t */\n\tindex = (addr >> SECTION_SHIFT) & 1;\n\tif (pmd_none(pmd_k[index]))\n\t\tgoto bad_area;\n\n\tcopy_pmd(pmd, pmd_k);\n\treturn 0;\n\nbad_area:\n\tdo_bad_area(addr, fsr, regs);\n\treturn 0;\n}\n#else\t\t\t\t\t/* CONFIG_MMU */\nstatic int\ndo_translation_fault(unsigned long addr, unsigned int fsr,\n\t\t     struct pt_regs *regs)\n{\n\treturn 0;\n}\n#endif\t\t\t\t\t/* CONFIG_MMU */\n\n/*\n * Some section permission faults need to be handled gracefully.\n * They can happen due to a __{get,put}_user during an oops.\n */\nstatic int\ndo_sect_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)\n{\n\tdo_bad_area(addr, fsr, regs);\n\treturn 0;\n}\n\n/*\n * This abort handler always returns \"fault\".\n */\nstatic int\ndo_bad(unsigned long addr, unsigned int fsr, struct pt_regs *regs)\n{\n\treturn 1;\n}\n\nstatic struct fsr_info {\n\tint\t(*fn)(unsigned long addr, unsigned int fsr, struct pt_regs *regs);\n\tint\tsig;\n\tint\tcode;\n\tconst char *name;\n} fsr_info[] = {\n\t/*\n\t * The following are the standard ARMv3 and ARMv4 aborts.  ARMv5\n\t * defines these to be \"precise\" aborts.\n\t */\n\t{ do_bad,\t\tSIGSEGV, 0,\t\t\"vector exception\"\t\t   },\n\t{ do_bad,\t\tSIGBUS,\t BUS_ADRALN,\t\"alignment exception\"\t\t   },\n\t{ do_bad,\t\tSIGKILL, 0,\t\t\"terminal exception\"\t\t   },\n\t{ do_bad,\t\tSIGBUS,\t BUS_ADRALN,\t\"alignment exception\"\t\t   },\n\t{ do_bad,\t\tSIGBUS,\t 0,\t\t\"external abort on linefetch\"\t   },\n\t{ do_translation_fault,\tSIGSEGV, SEGV_MAPERR,\t\"section translation fault\"\t   },\n\t{ do_bad,\t\tSIGBUS,\t 0,\t\t\"external abort on linefetch\"\t   },\n\t{ do_page_fault,\tSIGSEGV, SEGV_MAPERR,\t\"page translation fault\"\t   },\n\t{ do_bad,\t\tSIGBUS,\t 0,\t\t\"external abort on non-linefetch\"  },\n\t{ do_bad,\t\tSIGSEGV, SEGV_ACCERR,\t\"section domain fault\"\t\t   },\n\t{ do_bad,\t\tSIGBUS,\t 0,\t\t\"external abort on non-linefetch\"  },\n\t{ do_bad,\t\tSIGSEGV, SEGV_ACCERR,\t\"page domain fault\"\t\t   },\n\t{ do_bad,\t\tSIGBUS,\t 0,\t\t\"external abort on translation\"\t   },\n\t{ do_sect_fault,\tSIGSEGV, SEGV_ACCERR,\t\"section permission fault\"\t   },\n\t{ do_bad,\t\tSIGBUS,\t 0,\t\t\"external abort on translation\"\t   },\n\t{ do_page_fault,\tSIGSEGV, SEGV_ACCERR,\t\"page permission fault\"\t\t   },\n\t/*\n\t * The following are \"imprecise\" aborts, which are signalled by bit\n\t * 10 of the FSR, and may not be recoverable.  These are only\n\t * supported if the CPU abort handler supports bit 10.\n\t */\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 16\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 17\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 18\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 19\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"lock abort\"\t\t\t   }, /* xscale */\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 21\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  BUS_OBJERR,\t\"imprecise external abort\"\t   }, /* xscale */\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 23\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"dcache parity error\"\t\t   }, /* xscale */\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 25\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 26\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 27\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 28\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 29\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 30\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 31\"\t\t\t   }\n};\n\nvoid __init\nhook_fault_code(int nr, int (*fn)(unsigned long, unsigned int, struct pt_regs *),\n\t\tint sig, int code, const char *name)\n{\n\tif (nr < 0 || nr >= ARRAY_SIZE(fsr_info))\n\t\tBUG();\n\n\tfsr_info[nr].fn   = fn;\n\tfsr_info[nr].sig  = sig;\n\tfsr_info[nr].code = code;\n\tfsr_info[nr].name = name;\n}\n\n/*\n * Dispatch a data abort to the relevant handler.\n */\nasmlinkage void __exception\ndo_DataAbort(unsigned long addr, unsigned int fsr, struct pt_regs *regs)\n{\n\tconst struct fsr_info *inf = fsr_info + fsr_fs(fsr);\n\tstruct siginfo info;\n\n\tif (!inf->fn(addr, fsr & ~FSR_LNX_PF, regs))\n\t\treturn;\n\n\tprintk(KERN_ALERT \"Unhandled fault: %s (0x%03x) at 0x%08lx\\n\",\n\t\tinf->name, fsr, addr);\n\n\tinfo.si_signo = inf->sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = inf->code;\n\tinfo.si_addr  = (void __user *)addr;\n\tarm_notify_die(\"\", regs, &info, fsr, 0);\n}\n\n\nstatic struct fsr_info ifsr_info[] = {\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 0\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 1\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"debug event\"\t\t\t   },\n\t{ do_bad,\t\tSIGSEGV, SEGV_ACCERR,\t\"section access flag fault\"\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 4\"\t\t\t   },\n\t{ do_translation_fault,\tSIGSEGV, SEGV_MAPERR,\t\"section translation fault\"\t   },\n\t{ do_bad,\t\tSIGSEGV, SEGV_ACCERR,\t\"page access flag fault\"\t   },\n\t{ do_page_fault,\tSIGSEGV, SEGV_MAPERR,\t\"page translation fault\"\t   },\n\t{ do_bad,\t\tSIGBUS,\t 0,\t\t\"external abort on non-linefetch\"  },\n\t{ do_bad,\t\tSIGSEGV, SEGV_ACCERR,\t\"section domain fault\"\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 10\"\t\t\t   },\n\t{ do_bad,\t\tSIGSEGV, SEGV_ACCERR,\t\"page domain fault\"\t\t   },\n\t{ do_bad,\t\tSIGBUS,\t 0,\t\t\"external abort on translation\"\t   },\n\t{ do_sect_fault,\tSIGSEGV, SEGV_ACCERR,\t\"section permission fault\"\t   },\n\t{ do_bad,\t\tSIGBUS,\t 0,\t\t\"external abort on translation\"\t   },\n\t{ do_page_fault,\tSIGSEGV, SEGV_ACCERR,\t\"page permission fault\"\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 16\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 17\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 18\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 19\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 20\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 21\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 22\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 23\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 24\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 25\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 26\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 27\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 28\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 29\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 30\"\t\t\t   },\n\t{ do_bad,\t\tSIGBUS,  0,\t\t\"unknown 31\"\t\t\t   },\n};\n\nvoid __init\nhook_ifault_code(int nr, int (*fn)(unsigned long, unsigned int, struct pt_regs *),\n\t\t int sig, int code, const char *name)\n{\n\tif (nr < 0 || nr >= ARRAY_SIZE(ifsr_info))\n\t\tBUG();\n\n\tifsr_info[nr].fn   = fn;\n\tifsr_info[nr].sig  = sig;\n\tifsr_info[nr].code = code;\n\tifsr_info[nr].name = name;\n}\n\nasmlinkage void __exception\ndo_PrefetchAbort(unsigned long addr, unsigned int ifsr, struct pt_regs *regs)\n{\n\tconst struct fsr_info *inf = ifsr_info + fsr_fs(ifsr);\n\tstruct siginfo info;\n\n\tif (!inf->fn(addr, ifsr | FSR_LNX_PF, regs))\n\t\treturn;\n\n\tprintk(KERN_ALERT \"Unhandled prefetch abort: %s (0x%03x) at 0x%08lx\\n\",\n\t\tinf->name, ifsr, addr);\n\n\tinfo.si_signo = inf->sig;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = inf->code;\n\tinfo.si_addr  = (void __user *)addr;\n\tarm_notify_die(\"\", regs, &info, ifsr, 0);\n}\n\nstatic int __init exceptions_init(void)\n{\n\tif (cpu_architecture() >= CPU_ARCH_ARMv6) {\n\t\thook_fault_code(4, do_translation_fault, SIGSEGV, SEGV_MAPERR,\n\t\t\t\t\"I-cache maintenance fault\");\n\t}\n\n\tif (cpu_architecture() >= CPU_ARCH_ARMv7) {\n\t\t/*\n\t\t * TODO: Access flag faults introduced in ARMv6K.\n\t\t * Runtime check for 'K' extension is needed\n\t\t */\n\t\thook_fault_code(3, do_bad, SIGSEGV, SEGV_MAPERR,\n\t\t\t\t\"section access flag fault\");\n\t\thook_fault_code(6, do_bad, SIGSEGV, SEGV_MAPERR,\n\t\t\t\t\"section access flag fault\");\n\t}\n\n\treturn 0;\n}\n\narch_initcall(exceptions_init);\n", "/*\n * Linux performance counter support for MIPS.\n *\n * Copyright (C) 2010 MIPS Technologies, Inc.\n * Author: Deng-Cheng Zhu\n *\n * This code is based on the implementation for ARM, which is in turn\n * based on the sparc64 perf event code and the x86 code. Performance\n * counter access is based on the MIPS Oprofile code. And the callchain\n * support references the code of MIPS stacktrace.c.\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n */\n\n#include <linux/cpumask.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/kernel.h>\n#include <linux/perf_event.h>\n#include <linux/uaccess.h>\n\n#include <asm/irq.h>\n#include <asm/irq_regs.h>\n#include <asm/stacktrace.h>\n#include <asm/time.h> /* For perf_irq */\n\n/* These are for 32bit counters. For 64bit ones, define them accordingly. */\n#define MAX_PERIOD\t((1ULL << 32) - 1)\n#define VALID_COUNT\t0x7fffffff\n#define TOTAL_BITS\t32\n#define HIGHEST_BIT\t31\n\n#define MIPS_MAX_HWEVENTS 4\n\nstruct cpu_hw_events {\n\t/* Array of events on this cpu. */\n\tstruct perf_event\t*events[MIPS_MAX_HWEVENTS];\n\n\t/*\n\t * Set the bit (indexed by the counter number) when the counter\n\t * is used for an event.\n\t */\n\tunsigned long\t\tused_mask[BITS_TO_LONGS(MIPS_MAX_HWEVENTS)];\n\n\t/*\n\t * The borrowed MSB for the performance counter. A MIPS performance\n\t * counter uses its bit 31 (for 32bit counters) or bit 63 (for 64bit\n\t * counters) as a factor of determining whether a counter overflow\n\t * should be signaled. So here we use a separate MSB for each\n\t * counter to make things easy.\n\t */\n\tunsigned long\t\tmsbs[BITS_TO_LONGS(MIPS_MAX_HWEVENTS)];\n\n\t/*\n\t * Software copy of the control register for each performance counter.\n\t * MIPS CPUs vary in performance counters. They use this differently,\n\t * and even may not use it.\n\t */\n\tunsigned int\t\tsaved_ctrl[MIPS_MAX_HWEVENTS];\n};\nDEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events) = {\n\t.saved_ctrl = {0},\n};\n\n/* The description of MIPS performance events. */\nstruct mips_perf_event {\n\tunsigned int event_id;\n\t/*\n\t * MIPS performance counters are indexed starting from 0.\n\t * CNTR_EVEN indicates the indexes of the counters to be used are\n\t * even numbers.\n\t */\n\tunsigned int cntr_mask;\n\t#define CNTR_EVEN\t0x55555555\n\t#define CNTR_ODD\t0xaaaaaaaa\n#ifdef CONFIG_MIPS_MT_SMP\n\tenum {\n\t\tT  = 0,\n\t\tV  = 1,\n\t\tP  = 2,\n\t} range;\n#else\n\t#define T\n\t#define V\n\t#define P\n#endif\n};\n\nstatic struct mips_perf_event raw_event;\nstatic DEFINE_MUTEX(raw_event_mutex);\n\n#define UNSUPPORTED_PERF_EVENT_ID 0xffffffff\n#define C(x) PERF_COUNT_HW_CACHE_##x\n\nstruct mips_pmu {\n\tconst char\t*name;\n\tint\t\tirq;\n\tirqreturn_t\t(*handle_irq)(int irq, void *dev);\n\tint\t\t(*handle_shared_irq)(void);\n\tvoid\t\t(*start)(void);\n\tvoid\t\t(*stop)(void);\n\tint\t\t(*alloc_counter)(struct cpu_hw_events *cpuc,\n\t\t\t\t\tstruct hw_perf_event *hwc);\n\tu64\t\t(*read_counter)(unsigned int idx);\n\tvoid\t\t(*write_counter)(unsigned int idx, u64 val);\n\tvoid\t\t(*enable_event)(struct hw_perf_event *evt, int idx);\n\tvoid\t\t(*disable_event)(int idx);\n\tconst struct mips_perf_event *(*map_raw_event)(u64 config);\n\tconst struct mips_perf_event (*general_event_map)[PERF_COUNT_HW_MAX];\n\tconst struct mips_perf_event (*cache_event_map)\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX];\n\tunsigned int\tnum_counters;\n};\n\nstatic const struct mips_pmu *mipspmu;\n\nstatic int\nmipspmu_event_set_period(struct perf_event *event,\n\t\t\tstruct hw_perf_event *hwc,\n\t\t\tint idx)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\ts64 left = local64_read(&hwc->period_left);\n\ts64 period = hwc->sample_period;\n\tint ret = 0;\n\tu64 uleft;\n\tunsigned long flags;\n\n\tif (unlikely(left <= -period)) {\n\t\tleft = period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\tif (unlikely(left <= 0)) {\n\t\tleft += period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\tif (left > (s64)MAX_PERIOD)\n\t\tleft = MAX_PERIOD;\n\n\tlocal64_set(&hwc->prev_count, (u64)-left);\n\n\tlocal_irq_save(flags);\n\tuleft = (u64)(-left) & MAX_PERIOD;\n\tuleft > VALID_COUNT ?\n\t\tset_bit(idx, cpuc->msbs) : clear_bit(idx, cpuc->msbs);\n\tmipspmu->write_counter(idx, (u64)(-left) & VALID_COUNT);\n\tlocal_irq_restore(flags);\n\n\tperf_event_update_userpage(event);\n\n\treturn ret;\n}\n\nstatic void mipspmu_event_update(struct perf_event *event,\n\t\t\tstruct hw_perf_event *hwc,\n\t\t\tint idx)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tunsigned long flags;\n\tint shift = 64 - TOTAL_BITS;\n\ts64 prev_raw_count, new_raw_count;\n\tu64 delta;\n\nagain:\n\tprev_raw_count = local64_read(&hwc->prev_count);\n\tlocal_irq_save(flags);\n\t/* Make the counter value be a \"real\" one. */\n\tnew_raw_count = mipspmu->read_counter(idx);\n\tif (new_raw_count & (test_bit(idx, cpuc->msbs) << HIGHEST_BIT)) {\n\t\tnew_raw_count &= VALID_COUNT;\n\t\tclear_bit(idx, cpuc->msbs);\n\t} else\n\t\tnew_raw_count |= (test_bit(idx, cpuc->msbs) << HIGHEST_BIT);\n\tlocal_irq_restore(flags);\n\n\tif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\n\t\t\t\tnew_raw_count) != prev_raw_count)\n\t\tgoto again;\n\n\tdelta = (new_raw_count << shift) - (prev_raw_count << shift);\n\tdelta >>= shift;\n\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &hwc->period_left);\n\n\treturn;\n}\n\nstatic void mipspmu_start(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (!mipspmu)\n\t\treturn;\n\n\tif (flags & PERF_EF_RELOAD)\n\t\tWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\n\n\thwc->state = 0;\n\n\t/* Set the period for the event. */\n\tmipspmu_event_set_period(event, hwc, hwc->idx);\n\n\t/* Enable the event. */\n\tmipspmu->enable_event(hwc, hwc->idx);\n}\n\nstatic void mipspmu_stop(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (!mipspmu)\n\t\treturn;\n\n\tif (!(hwc->state & PERF_HES_STOPPED)) {\n\t\t/* We are working on a local event. */\n\t\tmipspmu->disable_event(hwc->idx);\n\t\tbarrier();\n\t\tmipspmu_event_update(event, hwc, hwc->idx);\n\t\thwc->state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\t}\n}\n\nstatic int mipspmu_add(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx;\n\tint err = 0;\n\n\tperf_pmu_disable(event->pmu);\n\n\t/* To look for a free counter for this event. */\n\tidx = mipspmu->alloc_counter(cpuc, hwc);\n\tif (idx < 0) {\n\t\terr = idx;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If there is an event in the counter we are going to use then\n\t * make sure it is disabled.\n\t */\n\tevent->hw.idx = idx;\n\tmipspmu->disable_event(idx);\n\tcpuc->events[idx] = event;\n\n\thwc->state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\tif (flags & PERF_EF_START)\n\t\tmipspmu_start(event, PERF_EF_RELOAD);\n\n\t/* Propagate our changes to the userspace mapping. */\n\tperf_event_update_userpage(event);\n\nout:\n\tperf_pmu_enable(event->pmu);\n\treturn err;\n}\n\nstatic void mipspmu_del(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\n\tWARN_ON(idx < 0 || idx >= mipspmu->num_counters);\n\n\tmipspmu_stop(event, PERF_EF_UPDATE);\n\tcpuc->events[idx] = NULL;\n\tclear_bit(idx, cpuc->used_mask);\n\n\tperf_event_update_userpage(event);\n}\n\nstatic void mipspmu_read(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t/* Don't read disabled counters! */\n\tif (hwc->idx < 0)\n\t\treturn;\n\n\tmipspmu_event_update(event, hwc, hwc->idx);\n}\n\nstatic void mipspmu_enable(struct pmu *pmu)\n{\n\tif (mipspmu)\n\t\tmipspmu->start();\n}\n\nstatic void mipspmu_disable(struct pmu *pmu)\n{\n\tif (mipspmu)\n\t\tmipspmu->stop();\n}\n\nstatic atomic_t active_events = ATOMIC_INIT(0);\nstatic DEFINE_MUTEX(pmu_reserve_mutex);\nstatic int (*save_perf_irq)(void);\n\nstatic int mipspmu_get_irq(void)\n{\n\tint err;\n\n\tif (mipspmu->irq >= 0) {\n\t\t/* Request my own irq handler. */\n\t\terr = request_irq(mipspmu->irq, mipspmu->handle_irq,\n\t\t\tIRQF_DISABLED | IRQF_NOBALANCING,\n\t\t\t\"mips_perf_pmu\", NULL);\n\t\tif (err) {\n\t\t\tpr_warning(\"Unable to request IRQ%d for MIPS \"\n\t\t\t   \"performance counters!\\n\", mipspmu->irq);\n\t\t}\n\t} else if (cp0_perfcount_irq < 0) {\n\t\t/*\n\t\t * We are sharing the irq number with the timer interrupt.\n\t\t */\n\t\tsave_perf_irq = perf_irq;\n\t\tperf_irq = mipspmu->handle_shared_irq;\n\t\terr = 0;\n\t} else {\n\t\tpr_warning(\"The platform hasn't properly defined its \"\n\t\t\t\"interrupt controller.\\n\");\n\t\terr = -ENOENT;\n\t}\n\n\treturn err;\n}\n\nstatic void mipspmu_free_irq(void)\n{\n\tif (mipspmu->irq >= 0)\n\t\tfree_irq(mipspmu->irq, NULL);\n\telse if (cp0_perfcount_irq < 0)\n\t\tperf_irq = save_perf_irq;\n}\n\n/*\n * mipsxx/rm9000/loongson2 have different performance counters, they have\n * specific low-level init routines.\n */\nstatic void reset_counters(void *arg);\nstatic int __hw_perf_event_init(struct perf_event *event);\n\nstatic void hw_perf_event_destroy(struct perf_event *event)\n{\n\tif (atomic_dec_and_mutex_lock(&active_events,\n\t\t\t\t&pmu_reserve_mutex)) {\n\t\t/*\n\t\t * We must not call the destroy function with interrupts\n\t\t * disabled.\n\t\t */\n\t\ton_each_cpu(reset_counters,\n\t\t\t(void *)(long)mipspmu->num_counters, 1);\n\t\tmipspmu_free_irq();\n\t\tmutex_unlock(&pmu_reserve_mutex);\n\t}\n}\n\nstatic int mipspmu_event_init(struct perf_event *event)\n{\n\tint err = 0;\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_RAW:\n\tcase PERF_TYPE_HARDWARE:\n\tcase PERF_TYPE_HW_CACHE:\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\tif (!mipspmu || event->cpu >= nr_cpumask_bits ||\n\t\t(event->cpu >= 0 && !cpu_online(event->cpu)))\n\t\treturn -ENODEV;\n\n\tif (!atomic_inc_not_zero(&active_events)) {\n\t\tif (atomic_read(&active_events) > MIPS_MAX_HWEVENTS) {\n\t\t\tatomic_dec(&active_events);\n\t\t\treturn -ENOSPC;\n\t\t}\n\n\t\tmutex_lock(&pmu_reserve_mutex);\n\t\tif (atomic_read(&active_events) == 0)\n\t\t\terr = mipspmu_get_irq();\n\n\t\tif (!err)\n\t\t\tatomic_inc(&active_events);\n\t\tmutex_unlock(&pmu_reserve_mutex);\n\t}\n\n\tif (err)\n\t\treturn err;\n\n\terr = __hw_perf_event_init(event);\n\tif (err)\n\t\thw_perf_event_destroy(event);\n\n\treturn err;\n}\n\nstatic struct pmu pmu = {\n\t.pmu_enable\t= mipspmu_enable,\n\t.pmu_disable\t= mipspmu_disable,\n\t.event_init\t= mipspmu_event_init,\n\t.add\t\t= mipspmu_add,\n\t.del\t\t= mipspmu_del,\n\t.start\t\t= mipspmu_start,\n\t.stop\t\t= mipspmu_stop,\n\t.read\t\t= mipspmu_read,\n};\n\nstatic inline unsigned int\nmipspmu_perf_event_encode(const struct mips_perf_event *pev)\n{\n/*\n * Top 8 bits for range, next 16 bits for cntr_mask, lowest 8 bits for\n * event_id.\n */\n#ifdef CONFIG_MIPS_MT_SMP\n\treturn ((unsigned int)pev->range << 24) |\n\t\t(pev->cntr_mask & 0xffff00) |\n\t\t(pev->event_id & 0xff);\n#else\n\treturn (pev->cntr_mask & 0xffff00) |\n\t\t(pev->event_id & 0xff);\n#endif\n}\n\nstatic const struct mips_perf_event *\nmipspmu_map_general_event(int idx)\n{\n\tconst struct mips_perf_event *pev;\n\n\tpev = ((*mipspmu->general_event_map)[idx].event_id ==\n\t\tUNSUPPORTED_PERF_EVENT_ID ? ERR_PTR(-EOPNOTSUPP) :\n\t\t&(*mipspmu->general_event_map)[idx]);\n\n\treturn pev;\n}\n\nstatic const struct mips_perf_event *\nmipspmu_map_cache_event(u64 config)\n{\n\tunsigned int cache_type, cache_op, cache_result;\n\tconst struct mips_perf_event *pev;\n\n\tcache_type = (config >> 0) & 0xff;\n\tif (cache_type >= PERF_COUNT_HW_CACHE_MAX)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tcache_op = (config >> 8) & 0xff;\n\tif (cache_op >= PERF_COUNT_HW_CACHE_OP_MAX)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tcache_result = (config >> 16) & 0xff;\n\tif (cache_result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tpev = &((*mipspmu->cache_event_map)\n\t\t\t\t\t[cache_type]\n\t\t\t\t\t[cache_op]\n\t\t\t\t\t[cache_result]);\n\n\tif (pev->event_id == UNSUPPORTED_PERF_EVENT_ID)\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\treturn pev;\n\n}\n\nstatic int validate_event(struct cpu_hw_events *cpuc,\n\t       struct perf_event *event)\n{\n\tstruct hw_perf_event fake_hwc = event->hw;\n\n\t/* Allow mixed event group. So return 1 to pass validation. */\n\tif (event->pmu != &pmu || event->state <= PERF_EVENT_STATE_OFF)\n\t\treturn 1;\n\n\treturn mipspmu->alloc_counter(cpuc, &fake_hwc) >= 0;\n}\n\nstatic int validate_group(struct perf_event *event)\n{\n\tstruct perf_event *sibling, *leader = event->group_leader;\n\tstruct cpu_hw_events fake_cpuc;\n\n\tmemset(&fake_cpuc, 0, sizeof(fake_cpuc));\n\n\tif (!validate_event(&fake_cpuc, leader))\n\t\treturn -ENOSPC;\n\n\tlist_for_each_entry(sibling, &leader->sibling_list, group_entry) {\n\t\tif (!validate_event(&fake_cpuc, sibling))\n\t\t\treturn -ENOSPC;\n\t}\n\n\tif (!validate_event(&fake_cpuc, event))\n\t\treturn -ENOSPC;\n\n\treturn 0;\n}\n\n/* This is needed by specific irq handlers in perf_event_*.c */\nstatic void\nhandle_associated_event(struct cpu_hw_events *cpuc,\n\tint idx, struct perf_sample_data *data, struct pt_regs *regs)\n{\n\tstruct perf_event *event = cpuc->events[idx];\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tmipspmu_event_update(event, hwc, idx);\n\tdata->period = event->hw.last_period;\n\tif (!mipspmu_event_set_period(event, hwc, idx))\n\t\treturn;\n\n\tif (perf_event_overflow(event, data, regs))\n\t\tmipspmu->disable_event(idx);\n}\n\n#include \"perf_event_mipsxx.c\"\n\n/* Callchain handling code. */\n\n/*\n * Leave userspace callchain empty for now. When we find a way to trace\n * the user stack callchains, we add here.\n */\nvoid perf_callchain_user(struct perf_callchain_entry *entry,\n\t\t    struct pt_regs *regs)\n{\n}\n\nstatic void save_raw_perf_callchain(struct perf_callchain_entry *entry,\n\tunsigned long reg29)\n{\n\tunsigned long *sp = (unsigned long *)reg29;\n\tunsigned long addr;\n\n\twhile (!kstack_end(sp)) {\n\t\taddr = *sp++;\n\t\tif (__kernel_text_address(addr)) {\n\t\t\tperf_callchain_store(entry, addr);\n\t\t\tif (entry->nr >= PERF_MAX_STACK_DEPTH)\n\t\t\t\tbreak;\n\t\t}\n\t}\n}\n\nvoid perf_callchain_kernel(struct perf_callchain_entry *entry,\n\t\t      struct pt_regs *regs)\n{\n\tunsigned long sp = regs->regs[29];\n#ifdef CONFIG_KALLSYMS\n\tunsigned long ra = regs->regs[31];\n\tunsigned long pc = regs->cp0_epc;\n\n\tif (raw_show_trace || !__kernel_text_address(pc)) {\n\t\tunsigned long stack_page =\n\t\t\t(unsigned long)task_stack_page(current);\n\t\tif (stack_page && sp >= stack_page &&\n\t\t    sp <= stack_page + THREAD_SIZE - 32)\n\t\t\tsave_raw_perf_callchain(entry, sp);\n\t\treturn;\n\t}\n\tdo {\n\t\tperf_callchain_store(entry, pc);\n\t\tif (entry->nr >= PERF_MAX_STACK_DEPTH)\n\t\t\tbreak;\n\t\tpc = unwind_stack(current, &sp, pc, &ra);\n\t} while (pc);\n#else\n\tsave_raw_perf_callchain(entry, sp);\n#endif\n}\n", "/*\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n *\n * Copyright (C) 1994 - 1999, 2000, 01, 06 Ralf Baechle\n * Copyright (C) 1995, 1996 Paul M. Antoine\n * Copyright (C) 1998 Ulf Carlsson\n * Copyright (C) 1999 Silicon Graphics, Inc.\n * Kevin D. Kissell, kevink@mips.com and Carsten Langgaard, carstenl@mips.com\n * Copyright (C) 2000, 01 MIPS Technologies, Inc.\n * Copyright (C) 2002, 2003, 2004, 2005, 2007  Maciej W. Rozycki\n */\n#include <linux/bug.h>\n#include <linux/compiler.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/sched.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/kallsyms.h>\n#include <linux/bootmem.h>\n#include <linux/interrupt.h>\n#include <linux/ptrace.h>\n#include <linux/kgdb.h>\n#include <linux/kdebug.h>\n#include <linux/kprobes.h>\n#include <linux/notifier.h>\n#include <linux/kdb.h>\n#include <linux/irq.h>\n#include <linux/perf_event.h>\n\n#include <asm/bootinfo.h>\n#include <asm/branch.h>\n#include <asm/break.h>\n#include <asm/cop2.h>\n#include <asm/cpu.h>\n#include <asm/dsp.h>\n#include <asm/fpu.h>\n#include <asm/fpu_emulator.h>\n#include <asm/mipsregs.h>\n#include <asm/mipsmtregs.h>\n#include <asm/module.h>\n#include <asm/pgtable.h>\n#include <asm/ptrace.h>\n#include <asm/sections.h>\n#include <asm/system.h>\n#include <asm/tlbdebug.h>\n#include <asm/traps.h>\n#include <asm/uaccess.h>\n#include <asm/watch.h>\n#include <asm/mmu_context.h>\n#include <asm/types.h>\n#include <asm/stacktrace.h>\n#include <asm/uasm.h>\n\nextern void check_wait(void);\nextern asmlinkage void r4k_wait(void);\nextern asmlinkage void rollback_handle_int(void);\nextern asmlinkage void handle_int(void);\nextern asmlinkage void handle_tlbm(void);\nextern asmlinkage void handle_tlbl(void);\nextern asmlinkage void handle_tlbs(void);\nextern asmlinkage void handle_adel(void);\nextern asmlinkage void handle_ades(void);\nextern asmlinkage void handle_ibe(void);\nextern asmlinkage void handle_dbe(void);\nextern asmlinkage void handle_sys(void);\nextern asmlinkage void handle_bp(void);\nextern asmlinkage void handle_ri(void);\nextern asmlinkage void handle_ri_rdhwr_vivt(void);\nextern asmlinkage void handle_ri_rdhwr(void);\nextern asmlinkage void handle_cpu(void);\nextern asmlinkage void handle_ov(void);\nextern asmlinkage void handle_tr(void);\nextern asmlinkage void handle_fpe(void);\nextern asmlinkage void handle_mdmx(void);\nextern asmlinkage void handle_watch(void);\nextern asmlinkage void handle_mt(void);\nextern asmlinkage void handle_dsp(void);\nextern asmlinkage void handle_mcheck(void);\nextern asmlinkage void handle_reserved(void);\n\nextern int fpu_emulator_cop1Handler(struct pt_regs *xcp,\n\t\t\t\t    struct mips_fpu_struct *ctx, int has_fpu,\n\t\t\t\t    void *__user *fault_addr);\n\nvoid (*board_be_init)(void);\nint (*board_be_handler)(struct pt_regs *regs, int is_fixup);\nvoid (*board_nmi_handler_setup)(void);\nvoid (*board_ejtag_handler_setup)(void);\nvoid (*board_bind_eic_interrupt)(int irq, int regset);\n\n\nstatic void show_raw_backtrace(unsigned long reg29)\n{\n\tunsigned long *sp = (unsigned long *)(reg29 & ~3);\n\tunsigned long addr;\n\n\tprintk(\"Call Trace:\");\n#ifdef CONFIG_KALLSYMS\n\tprintk(\"\\n\");\n#endif\n\twhile (!kstack_end(sp)) {\n\t\tunsigned long __user *p =\n\t\t\t(unsigned long __user *)(unsigned long)sp++;\n\t\tif (__get_user(addr, p)) {\n\t\t\tprintk(\" (Bad stack address)\");\n\t\t\tbreak;\n\t\t}\n\t\tif (__kernel_text_address(addr))\n\t\t\tprint_ip_sym(addr);\n\t}\n\tprintk(\"\\n\");\n}\n\n#ifdef CONFIG_KALLSYMS\nint raw_show_trace;\nstatic int __init set_raw_show_trace(char *str)\n{\n\traw_show_trace = 1;\n\treturn 1;\n}\n__setup(\"raw_show_trace\", set_raw_show_trace);\n#endif\n\nstatic void show_backtrace(struct task_struct *task, const struct pt_regs *regs)\n{\n\tunsigned long sp = regs->regs[29];\n\tunsigned long ra = regs->regs[31];\n\tunsigned long pc = regs->cp0_epc;\n\n\tif (raw_show_trace || !__kernel_text_address(pc)) {\n\t\tshow_raw_backtrace(sp);\n\t\treturn;\n\t}\n\tprintk(\"Call Trace:\\n\");\n\tdo {\n\t\tprint_ip_sym(pc);\n\t\tpc = unwind_stack(task, &sp, pc, &ra);\n\t} while (pc);\n\tprintk(\"\\n\");\n}\n\n/*\n * This routine abuses get_user()/put_user() to reference pointers\n * with at least a bit of error checking ...\n */\nstatic void show_stacktrace(struct task_struct *task,\n\tconst struct pt_regs *regs)\n{\n\tconst int field = 2 * sizeof(unsigned long);\n\tlong stackdata;\n\tint i;\n\tunsigned long __user *sp = (unsigned long __user *)regs->regs[29];\n\n\tprintk(\"Stack :\");\n\ti = 0;\n\twhile ((unsigned long) sp & (PAGE_SIZE - 1)) {\n\t\tif (i && ((i % (64 / field)) == 0))\n\t\t\tprintk(\"\\n       \");\n\t\tif (i > 39) {\n\t\t\tprintk(\" ...\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (__get_user(stackdata, sp++)) {\n\t\t\tprintk(\" (Bad stack address)\");\n\t\t\tbreak;\n\t\t}\n\n\t\tprintk(\" %0*lx\", field, stackdata);\n\t\ti++;\n\t}\n\tprintk(\"\\n\");\n\tshow_backtrace(task, regs);\n}\n\nvoid show_stack(struct task_struct *task, unsigned long *sp)\n{\n\tstruct pt_regs regs;\n\tif (sp) {\n\t\tregs.regs[29] = (unsigned long)sp;\n\t\tregs.regs[31] = 0;\n\t\tregs.cp0_epc = 0;\n\t} else {\n\t\tif (task && task != current) {\n\t\t\tregs.regs[29] = task->thread.reg29;\n\t\t\tregs.regs[31] = 0;\n\t\t\tregs.cp0_epc = task->thread.reg31;\n#ifdef CONFIG_KGDB_KDB\n\t\t} else if (atomic_read(&kgdb_active) != -1 &&\n\t\t\t   kdb_current_regs) {\n\t\t\tmemcpy(&regs, kdb_current_regs, sizeof(regs));\n#endif /* CONFIG_KGDB_KDB */\n\t\t} else {\n\t\t\tprepare_frametrace(&regs);\n\t\t}\n\t}\n\tshow_stacktrace(task, &regs);\n}\n\n/*\n * The architecture-independent dump_stack generator\n */\nvoid dump_stack(void)\n{\n\tstruct pt_regs regs;\n\n\tprepare_frametrace(&regs);\n\tshow_backtrace(current, &regs);\n}\n\nEXPORT_SYMBOL(dump_stack);\n\nstatic void show_code(unsigned int __user *pc)\n{\n\tlong i;\n\tunsigned short __user *pc16 = NULL;\n\n\tprintk(\"\\nCode:\");\n\n\tif ((unsigned long)pc & 1)\n\t\tpc16 = (unsigned short __user *)((unsigned long)pc & ~1);\n\tfor(i = -3 ; i < 6 ; i++) {\n\t\tunsigned int insn;\n\t\tif (pc16 ? __get_user(insn, pc16 + i) : __get_user(insn, pc + i)) {\n\t\t\tprintk(\" (Bad address in epc)\\n\");\n\t\t\tbreak;\n\t\t}\n\t\tprintk(\"%c%0*x%c\", (i?' ':'<'), pc16 ? 4 : 8, insn, (i?' ':'>'));\n\t}\n}\n\nstatic void __show_regs(const struct pt_regs *regs)\n{\n\tconst int field = 2 * sizeof(unsigned long);\n\tunsigned int cause = regs->cp0_cause;\n\tint i;\n\n\tprintk(\"Cpu %d\\n\", smp_processor_id());\n\n\t/*\n\t * Saved main processor registers\n\t */\n\tfor (i = 0; i < 32; ) {\n\t\tif ((i % 4) == 0)\n\t\t\tprintk(\"$%2d   :\", i);\n\t\tif (i == 0)\n\t\t\tprintk(\" %0*lx\", field, 0UL);\n\t\telse if (i == 26 || i == 27)\n\t\t\tprintk(\" %*s\", field, \"\");\n\t\telse\n\t\t\tprintk(\" %0*lx\", field, regs->regs[i]);\n\n\t\ti++;\n\t\tif ((i % 4) == 0)\n\t\t\tprintk(\"\\n\");\n\t}\n\n#ifdef CONFIG_CPU_HAS_SMARTMIPS\n\tprintk(\"Acx    : %0*lx\\n\", field, regs->acx);\n#endif\n\tprintk(\"Hi    : %0*lx\\n\", field, regs->hi);\n\tprintk(\"Lo    : %0*lx\\n\", field, regs->lo);\n\n\t/*\n\t * Saved cp0 registers\n\t */\n\tprintk(\"epc   : %0*lx %pS\\n\", field, regs->cp0_epc,\n\t       (void *) regs->cp0_epc);\n\tprintk(\"    %s\\n\", print_tainted());\n\tprintk(\"ra    : %0*lx %pS\\n\", field, regs->regs[31],\n\t       (void *) regs->regs[31]);\n\n\tprintk(\"Status: %08x    \", (uint32_t) regs->cp0_status);\n\n\tif (current_cpu_data.isa_level == MIPS_CPU_ISA_I) {\n\t\tif (regs->cp0_status & ST0_KUO)\n\t\t\tprintk(\"KUo \");\n\t\tif (regs->cp0_status & ST0_IEO)\n\t\t\tprintk(\"IEo \");\n\t\tif (regs->cp0_status & ST0_KUP)\n\t\t\tprintk(\"KUp \");\n\t\tif (regs->cp0_status & ST0_IEP)\n\t\t\tprintk(\"IEp \");\n\t\tif (regs->cp0_status & ST0_KUC)\n\t\t\tprintk(\"KUc \");\n\t\tif (regs->cp0_status & ST0_IEC)\n\t\t\tprintk(\"IEc \");\n\t} else {\n\t\tif (regs->cp0_status & ST0_KX)\n\t\t\tprintk(\"KX \");\n\t\tif (regs->cp0_status & ST0_SX)\n\t\t\tprintk(\"SX \");\n\t\tif (regs->cp0_status & ST0_UX)\n\t\t\tprintk(\"UX \");\n\t\tswitch (regs->cp0_status & ST0_KSU) {\n\t\tcase KSU_USER:\n\t\t\tprintk(\"USER \");\n\t\t\tbreak;\n\t\tcase KSU_SUPERVISOR:\n\t\t\tprintk(\"SUPERVISOR \");\n\t\t\tbreak;\n\t\tcase KSU_KERNEL:\n\t\t\tprintk(\"KERNEL \");\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tprintk(\"BAD_MODE \");\n\t\t\tbreak;\n\t\t}\n\t\tif (regs->cp0_status & ST0_ERL)\n\t\t\tprintk(\"ERL \");\n\t\tif (regs->cp0_status & ST0_EXL)\n\t\t\tprintk(\"EXL \");\n\t\tif (regs->cp0_status & ST0_IE)\n\t\t\tprintk(\"IE \");\n\t}\n\tprintk(\"\\n\");\n\n\tprintk(\"Cause : %08x\\n\", cause);\n\n\tcause = (cause & CAUSEF_EXCCODE) >> CAUSEB_EXCCODE;\n\tif (1 <= cause && cause <= 5)\n\t\tprintk(\"BadVA : %0*lx\\n\", field, regs->cp0_badvaddr);\n\n\tprintk(\"PrId  : %08x (%s)\\n\", read_c0_prid(),\n\t       cpu_name_string());\n}\n\n/*\n * FIXME: really the generic show_regs should take a const pointer argument.\n */\nvoid show_regs(struct pt_regs *regs)\n{\n\t__show_regs((struct pt_regs *)regs);\n}\n\nvoid show_registers(struct pt_regs *regs)\n{\n\tconst int field = 2 * sizeof(unsigned long);\n\n\t__show_regs(regs);\n\tprint_modules();\n\tprintk(\"Process %s (pid: %d, threadinfo=%p, task=%p, tls=%0*lx)\\n\",\n\t       current->comm, current->pid, current_thread_info(), current,\n\t      field, current_thread_info()->tp_value);\n\tif (cpu_has_userlocal) {\n\t\tunsigned long tls;\n\n\t\ttls = read_c0_userlocal();\n\t\tif (tls != current_thread_info()->tp_value)\n\t\t\tprintk(\"*HwTLS: %0*lx\\n\", field, tls);\n\t}\n\n\tshow_stacktrace(current, regs);\n\tshow_code((unsigned int __user *) regs->cp0_epc);\n\tprintk(\"\\n\");\n}\n\nstatic int regs_to_trapnr(struct pt_regs *regs)\n{\n\treturn (regs->cp0_cause >> 2) & 0x1f;\n}\n\nstatic DEFINE_SPINLOCK(die_lock);\n\nvoid __noreturn die(const char *str, struct pt_regs *regs)\n{\n\tstatic int die_counter;\n\tint sig = SIGSEGV;\n#ifdef CONFIG_MIPS_MT_SMTC\n\tunsigned long dvpret = dvpe();\n#endif /* CONFIG_MIPS_MT_SMTC */\n\n\tif (notify_die(DIE_OOPS, str, regs, 0, regs_to_trapnr(regs), SIGSEGV) == NOTIFY_STOP)\n\t\tsig = 0;\n\n\tconsole_verbose();\n\tspin_lock_irq(&die_lock);\n\tbust_spinlocks(1);\n#ifdef CONFIG_MIPS_MT_SMTC\n\tmips_mt_regdump(dvpret);\n#endif /* CONFIG_MIPS_MT_SMTC */\n\n\tprintk(\"%s[#%d]:\\n\", str, ++die_counter);\n\tshow_registers(regs);\n\tadd_taint(TAINT_DIE);\n\tspin_unlock_irq(&die_lock);\n\n\tif (in_interrupt())\n\t\tpanic(\"Fatal exception in interrupt\");\n\n\tif (panic_on_oops) {\n\t\tprintk(KERN_EMERG \"Fatal exception: panic in 5 seconds\\n\");\n\t\tssleep(5);\n\t\tpanic(\"Fatal exception\");\n\t}\n\n\tdo_exit(sig);\n}\n\nextern struct exception_table_entry __start___dbe_table[];\nextern struct exception_table_entry __stop___dbe_table[];\n\n__asm__(\n\"\t.section\t__dbe_table, \\\"a\\\"\\n\"\n\"\t.previous\t\t\t\\n\");\n\n/* Given an address, look for it in the exception tables. */\nstatic const struct exception_table_entry *search_dbe_tables(unsigned long addr)\n{\n\tconst struct exception_table_entry *e;\n\n\te = search_extable(__start___dbe_table, __stop___dbe_table - 1, addr);\n\tif (!e)\n\t\te = search_module_dbetables(addr);\n\treturn e;\n}\n\nasmlinkage void do_be(struct pt_regs *regs)\n{\n\tconst int field = 2 * sizeof(unsigned long);\n\tconst struct exception_table_entry *fixup = NULL;\n\tint data = regs->cp0_cause & 4;\n\tint action = MIPS_BE_FATAL;\n\n\t/* XXX For now.  Fixme, this searches the wrong table ...  */\n\tif (data && !user_mode(regs))\n\t\tfixup = search_dbe_tables(exception_epc(regs));\n\n\tif (fixup)\n\t\taction = MIPS_BE_FIXUP;\n\n\tif (board_be_handler)\n\t\taction = board_be_handler(regs, fixup != NULL);\n\n\tswitch (action) {\n\tcase MIPS_BE_DISCARD:\n\t\treturn;\n\tcase MIPS_BE_FIXUP:\n\t\tif (fixup) {\n\t\t\tregs->cp0_epc = fixup->nextinsn;\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/*\n\t * Assume it would be too dangerous to continue ...\n\t */\n\tprintk(KERN_ALERT \"%s bus error, epc == %0*lx, ra == %0*lx\\n\",\n\t       data ? \"Data\" : \"Instruction\",\n\t       field, regs->cp0_epc, field, regs->regs[31]);\n\tif (notify_die(DIE_OOPS, \"bus error\", regs, 0, regs_to_trapnr(regs), SIGBUS)\n\t    == NOTIFY_STOP)\n\t\treturn;\n\n\tdie_if_kernel(\"Oops\", regs);\n\tforce_sig(SIGBUS, current);\n}\n\n/*\n * ll/sc, rdhwr, sync emulation\n */\n\n#define OPCODE 0xfc000000\n#define BASE   0x03e00000\n#define RT     0x001f0000\n#define OFFSET 0x0000ffff\n#define LL     0xc0000000\n#define SC     0xe0000000\n#define SPEC0  0x00000000\n#define SPEC3  0x7c000000\n#define RD     0x0000f800\n#define FUNC   0x0000003f\n#define SYNC   0x0000000f\n#define RDHWR  0x0000003b\n\n/*\n * The ll_bit is cleared by r*_switch.S\n */\n\nunsigned int ll_bit;\nstruct task_struct *ll_task;\n\nstatic inline int simulate_ll(struct pt_regs *regs, unsigned int opcode)\n{\n\tunsigned long value, __user *vaddr;\n\tlong offset;\n\n\t/*\n\t * analyse the ll instruction that just caused a ri exception\n\t * and put the referenced address to addr.\n\t */\n\n\t/* sign extend offset */\n\toffset = opcode & OFFSET;\n\toffset <<= 16;\n\toffset >>= 16;\n\n\tvaddr = (unsigned long __user *)\n\t        ((unsigned long)(regs->regs[(opcode & BASE) >> 21]) + offset);\n\n\tif ((unsigned long)vaddr & 3)\n\t\treturn SIGBUS;\n\tif (get_user(value, vaddr))\n\t\treturn SIGSEGV;\n\n\tpreempt_disable();\n\n\tif (ll_task == NULL || ll_task == current) {\n\t\tll_bit = 1;\n\t} else {\n\t\tll_bit = 0;\n\t}\n\tll_task = current;\n\n\tpreempt_enable();\n\n\tregs->regs[(opcode & RT) >> 16] = value;\n\n\treturn 0;\n}\n\nstatic inline int simulate_sc(struct pt_regs *regs, unsigned int opcode)\n{\n\tunsigned long __user *vaddr;\n\tunsigned long reg;\n\tlong offset;\n\n\t/*\n\t * analyse the sc instruction that just caused a ri exception\n\t * and put the referenced address to addr.\n\t */\n\n\t/* sign extend offset */\n\toffset = opcode & OFFSET;\n\toffset <<= 16;\n\toffset >>= 16;\n\n\tvaddr = (unsigned long __user *)\n\t        ((unsigned long)(regs->regs[(opcode & BASE) >> 21]) + offset);\n\treg = (opcode & RT) >> 16;\n\n\tif ((unsigned long)vaddr & 3)\n\t\treturn SIGBUS;\n\n\tpreempt_disable();\n\n\tif (ll_bit == 0 || ll_task != current) {\n\t\tregs->regs[reg] = 0;\n\t\tpreempt_enable();\n\t\treturn 0;\n\t}\n\n\tpreempt_enable();\n\n\tif (put_user(regs->regs[reg], vaddr))\n\t\treturn SIGSEGV;\n\n\tregs->regs[reg] = 1;\n\n\treturn 0;\n}\n\n/*\n * ll uses the opcode of lwc0 and sc uses the opcode of swc0.  That is both\n * opcodes are supposed to result in coprocessor unusable exceptions if\n * executed on ll/sc-less processors.  That's the theory.  In practice a\n * few processors such as NEC's VR4100 throw reserved instruction exceptions\n * instead, so we're doing the emulation thing in both exception handlers.\n */\nstatic int simulate_llsc(struct pt_regs *regs, unsigned int opcode)\n{\n\tif ((opcode & OPCODE) == LL) {\n\t\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,\n\t\t\t\t1, regs, 0);\n\t\treturn simulate_ll(regs, opcode);\n\t}\n\tif ((opcode & OPCODE) == SC) {\n\t\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,\n\t\t\t\t1, regs, 0);\n\t\treturn simulate_sc(regs, opcode);\n\t}\n\n\treturn -1;\t\t\t/* Must be something else ... */\n}\n\n/*\n * Simulate trapping 'rdhwr' instructions to provide user accessible\n * registers not implemented in hardware.\n */\nstatic int simulate_rdhwr(struct pt_regs *regs, unsigned int opcode)\n{\n\tstruct thread_info *ti = task_thread_info(current);\n\n\tif ((opcode & OPCODE) == SPEC3 && (opcode & FUNC) == RDHWR) {\n\t\tint rd = (opcode & RD) >> 11;\n\t\tint rt = (opcode & RT) >> 16;\n\t\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,\n\t\t\t\t1, regs, 0);\n\t\tswitch (rd) {\n\t\tcase 0:\t\t/* CPU number */\n\t\t\tregs->regs[rt] = smp_processor_id();\n\t\t\treturn 0;\n\t\tcase 1:\t\t/* SYNCI length */\n\t\t\tregs->regs[rt] = min(current_cpu_data.dcache.linesz,\n\t\t\t\t\t     current_cpu_data.icache.linesz);\n\t\t\treturn 0;\n\t\tcase 2:\t\t/* Read count register */\n\t\t\tregs->regs[rt] = read_c0_count();\n\t\t\treturn 0;\n\t\tcase 3:\t\t/* Count register resolution */\n\t\t\tswitch (current_cpu_data.cputype) {\n\t\t\tcase CPU_20KC:\n\t\t\tcase CPU_25KF:\n\t\t\t\tregs->regs[rt] = 1;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tregs->regs[rt] = 2;\n\t\t\t}\n\t\t\treturn 0;\n\t\tcase 29:\n\t\t\tregs->regs[rt] = ti->tp_value;\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\t/* Not ours.  */\n\treturn -1;\n}\n\nstatic int simulate_sync(struct pt_regs *regs, unsigned int opcode)\n{\n\tif ((opcode & OPCODE) == SPEC0 && (opcode & FUNC) == SYNC) {\n\t\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,\n\t\t\t\t1, regs, 0);\n\t\treturn 0;\n\t}\n\n\treturn -1;\t\t\t/* Must be something else ... */\n}\n\nasmlinkage void do_ov(struct pt_regs *regs)\n{\n\tsiginfo_t info;\n\n\tdie_if_kernel(\"Integer overflow\", regs);\n\n\tinfo.si_code = FPE_INTOVF;\n\tinfo.si_signo = SIGFPE;\n\tinfo.si_errno = 0;\n\tinfo.si_addr = (void __user *) regs->cp0_epc;\n\tforce_sig_info(SIGFPE, &info, current);\n}\n\nstatic int process_fpemu_return(int sig, void __user *fault_addr)\n{\n\tif (sig == SIGSEGV || sig == SIGBUS) {\n\t\tstruct siginfo si = {0};\n\t\tsi.si_addr = fault_addr;\n\t\tsi.si_signo = sig;\n\t\tif (sig == SIGSEGV) {\n\t\t\tif (find_vma(current->mm, (unsigned long)fault_addr))\n\t\t\t\tsi.si_code = SEGV_ACCERR;\n\t\t\telse\n\t\t\t\tsi.si_code = SEGV_MAPERR;\n\t\t} else {\n\t\t\tsi.si_code = BUS_ADRERR;\n\t\t}\n\t\tforce_sig_info(sig, &si, current);\n\t\treturn 1;\n\t} else if (sig) {\n\t\tforce_sig(sig, current);\n\t\treturn 1;\n\t} else {\n\t\treturn 0;\n\t}\n}\n\n/*\n * XXX Delayed fp exceptions when doing a lazy ctx switch XXX\n */\nasmlinkage void do_fpe(struct pt_regs *regs, unsigned long fcr31)\n{\n\tsiginfo_t info = {0};\n\n\tif (notify_die(DIE_FP, \"FP exception\", regs, 0, regs_to_trapnr(regs), SIGFPE)\n\t    == NOTIFY_STOP)\n\t\treturn;\n\tdie_if_kernel(\"FP exception in kernel code\", regs);\n\n\tif (fcr31 & FPU_CSR_UNI_X) {\n\t\tint sig;\n\t\tvoid __user *fault_addr = NULL;\n\n\t\t/*\n\t\t * Unimplemented operation exception.  If we've got the full\n\t\t * software emulator on-board, let's use it...\n\t\t *\n\t\t * Force FPU to dump state into task/thread context.  We're\n\t\t * moving a lot of data here for what is probably a single\n\t\t * instruction, but the alternative is to pre-decode the FP\n\t\t * register operands before invoking the emulator, which seems\n\t\t * a bit extreme for what should be an infrequent event.\n\t\t */\n\t\t/* Ensure 'resume' not overwrite saved fp context again. */\n\t\tlose_fpu(1);\n\n\t\t/* Run the emulator */\n\t\tsig = fpu_emulator_cop1Handler(regs, &current->thread.fpu, 1,\n\t\t\t\t\t       &fault_addr);\n\n\t\t/*\n\t\t * We can't allow the emulated instruction to leave any of\n\t\t * the cause bit set in $fcr31.\n\t\t */\n\t\tcurrent->thread.fpu.fcr31 &= ~FPU_CSR_ALL_X;\n\n\t\t/* Restore the hardware register state */\n\t\town_fpu(1);\t/* Using the FPU again.  */\n\n\t\t/* If something went wrong, signal */\n\t\tprocess_fpemu_return(sig, fault_addr);\n\n\t\treturn;\n\t} else if (fcr31 & FPU_CSR_INV_X)\n\t\tinfo.si_code = FPE_FLTINV;\n\telse if (fcr31 & FPU_CSR_DIV_X)\n\t\tinfo.si_code = FPE_FLTDIV;\n\telse if (fcr31 & FPU_CSR_OVF_X)\n\t\tinfo.si_code = FPE_FLTOVF;\n\telse if (fcr31 & FPU_CSR_UDF_X)\n\t\tinfo.si_code = FPE_FLTUND;\n\telse if (fcr31 & FPU_CSR_INE_X)\n\t\tinfo.si_code = FPE_FLTRES;\n\telse\n\t\tinfo.si_code = __SI_FAULT;\n\tinfo.si_signo = SIGFPE;\n\tinfo.si_errno = 0;\n\tinfo.si_addr = (void __user *) regs->cp0_epc;\n\tforce_sig_info(SIGFPE, &info, current);\n}\n\nstatic void do_trap_or_bp(struct pt_regs *regs, unsigned int code,\n\tconst char *str)\n{\n\tsiginfo_t info;\n\tchar b[40];\n\n#ifdef CONFIG_KGDB_LOW_LEVEL_TRAP\n\tif (kgdb_ll_trap(DIE_TRAP, str, regs, code, regs_to_trapnr(regs), SIGTRAP) == NOTIFY_STOP)\n\t\treturn;\n#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */\n\n\tif (notify_die(DIE_TRAP, str, regs, code, regs_to_trapnr(regs), SIGTRAP) == NOTIFY_STOP)\n\t\treturn;\n\n\t/*\n\t * A short test says that IRIX 5.3 sends SIGTRAP for all trap\n\t * insns, even for trap and break codes that indicate arithmetic\n\t * failures.  Weird ...\n\t * But should we continue the brokenness???  --macro\n\t */\n\tswitch (code) {\n\tcase BRK_OVERFLOW:\n\tcase BRK_DIVZERO:\n\t\tscnprintf(b, sizeof(b), \"%s instruction in kernel code\", str);\n\t\tdie_if_kernel(b, regs);\n\t\tif (code == BRK_DIVZERO)\n\t\t\tinfo.si_code = FPE_INTDIV;\n\t\telse\n\t\t\tinfo.si_code = FPE_INTOVF;\n\t\tinfo.si_signo = SIGFPE;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_addr = (void __user *) regs->cp0_epc;\n\t\tforce_sig_info(SIGFPE, &info, current);\n\t\tbreak;\n\tcase BRK_BUG:\n\t\tdie_if_kernel(\"Kernel bug detected\", regs);\n\t\tforce_sig(SIGTRAP, current);\n\t\tbreak;\n\tcase BRK_MEMU:\n\t\t/*\n\t\t * Address errors may be deliberately induced by the FPU\n\t\t * emulator to retake control of the CPU after executing the\n\t\t * instruction in the delay slot of an emulated branch.\n\t\t *\n\t\t * Terminate if exception was recognized as a delay slot return\n\t\t * otherwise handle as normal.\n\t\t */\n\t\tif (do_dsemulret(regs))\n\t\t\treturn;\n\n\t\tdie_if_kernel(\"Math emu break/trap\", regs);\n\t\tforce_sig(SIGTRAP, current);\n\t\tbreak;\n\tdefault:\n\t\tscnprintf(b, sizeof(b), \"%s instruction in kernel code\", str);\n\t\tdie_if_kernel(b, regs);\n\t\tforce_sig(SIGTRAP, current);\n\t}\n}\n\nasmlinkage void do_bp(struct pt_regs *regs)\n{\n\tunsigned int opcode, bcode;\n\n\tif (__get_user(opcode, (unsigned int __user *) exception_epc(regs)))\n\t\tgoto out_sigsegv;\n\n\t/*\n\t * There is the ancient bug in the MIPS assemblers that the break\n\t * code starts left to bit 16 instead to bit 6 in the opcode.\n\t * Gas is bug-compatible, but not always, grrr...\n\t * We handle both cases with a simple heuristics.  --macro\n\t */\n\tbcode = ((opcode >> 6) & ((1 << 20) - 1));\n\tif (bcode >= (1 << 10))\n\t\tbcode >>= 10;\n\n\t/*\n\t * notify the kprobe handlers, if instruction is likely to\n\t * pertain to them.\n\t */\n\tswitch (bcode) {\n\tcase BRK_KPROBE_BP:\n\t\tif (notify_die(DIE_BREAK, \"debug\", regs, bcode, regs_to_trapnr(regs), SIGTRAP) == NOTIFY_STOP)\n\t\t\treturn;\n\t\telse\n\t\t\tbreak;\n\tcase BRK_KPROBE_SSTEPBP:\n\t\tif (notify_die(DIE_SSTEPBP, \"single_step\", regs, bcode, regs_to_trapnr(regs), SIGTRAP) == NOTIFY_STOP)\n\t\t\treturn;\n\t\telse\n\t\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tdo_trap_or_bp(regs, bcode, \"Break\");\n\treturn;\n\nout_sigsegv:\n\tforce_sig(SIGSEGV, current);\n}\n\nasmlinkage void do_tr(struct pt_regs *regs)\n{\n\tunsigned int opcode, tcode = 0;\n\n\tif (__get_user(opcode, (unsigned int __user *) exception_epc(regs)))\n\t\tgoto out_sigsegv;\n\n\t/* Immediate versions don't provide a code.  */\n\tif (!(opcode & OPCODE))\n\t\ttcode = ((opcode >> 6) & ((1 << 10) - 1));\n\n\tdo_trap_or_bp(regs, tcode, \"Trap\");\n\treturn;\n\nout_sigsegv:\n\tforce_sig(SIGSEGV, current);\n}\n\nasmlinkage void do_ri(struct pt_regs *regs)\n{\n\tunsigned int __user *epc = (unsigned int __user *)exception_epc(regs);\n\tunsigned long old_epc = regs->cp0_epc;\n\tunsigned int opcode = 0;\n\tint status = -1;\n\n\tif (notify_die(DIE_RI, \"RI Fault\", regs, 0, regs_to_trapnr(regs), SIGILL)\n\t    == NOTIFY_STOP)\n\t\treturn;\n\n\tdie_if_kernel(\"Reserved instruction in kernel code\", regs);\n\n\tif (unlikely(compute_return_epc(regs) < 0))\n\t\treturn;\n\n\tif (unlikely(get_user(opcode, epc) < 0))\n\t\tstatus = SIGSEGV;\n\n\tif (!cpu_has_llsc && status < 0)\n\t\tstatus = simulate_llsc(regs, opcode);\n\n\tif (status < 0)\n\t\tstatus = simulate_rdhwr(regs, opcode);\n\n\tif (status < 0)\n\t\tstatus = simulate_sync(regs, opcode);\n\n\tif (status < 0)\n\t\tstatus = SIGILL;\n\n\tif (unlikely(status > 0)) {\n\t\tregs->cp0_epc = old_epc;\t\t/* Undo skip-over.  */\n\t\tforce_sig(status, current);\n\t}\n}\n\n/*\n * MIPS MT processors may have fewer FPU contexts than CPU threads. If we've\n * emulated more than some threshold number of instructions, force migration to\n * a \"CPU\" that has FP support.\n */\nstatic void mt_ase_fp_affinity(void)\n{\n#ifdef CONFIG_MIPS_MT_FPAFF\n\tif (mt_fpemul_threshold > 0 &&\n\t     ((current->thread.emulated_fp++ > mt_fpemul_threshold))) {\n\t\t/*\n\t\t * If there's no FPU present, or if the application has already\n\t\t * restricted the allowed set to exclude any CPUs with FPUs,\n\t\t * we'll skip the procedure.\n\t\t */\n\t\tif (cpus_intersects(current->cpus_allowed, mt_fpu_cpumask)) {\n\t\t\tcpumask_t tmask;\n\n\t\t\tcurrent->thread.user_cpus_allowed\n\t\t\t\t= current->cpus_allowed;\n\t\t\tcpus_and(tmask, current->cpus_allowed,\n\t\t\t\tmt_fpu_cpumask);\n\t\t\tset_cpus_allowed_ptr(current, &tmask);\n\t\t\tset_thread_flag(TIF_FPUBOUND);\n\t\t}\n\t}\n#endif /* CONFIG_MIPS_MT_FPAFF */\n}\n\n/*\n * No lock; only written during early bootup by CPU 0.\n */\nstatic RAW_NOTIFIER_HEAD(cu2_chain);\n\nint __ref register_cu2_notifier(struct notifier_block *nb)\n{\n\treturn raw_notifier_chain_register(&cu2_chain, nb);\n}\n\nint cu2_notifier_call_chain(unsigned long val, void *v)\n{\n\treturn raw_notifier_call_chain(&cu2_chain, val, v);\n}\n\nstatic int default_cu2_call(struct notifier_block *nfb, unsigned long action,\n        void *data)\n{\n\tstruct pt_regs *regs = data;\n\n\tswitch (action) {\n\tdefault:\n\t\tdie_if_kernel(\"Unhandled kernel unaligned access or invalid \"\n\t\t\t      \"instruction\", regs);\n\t\t/* Fall through  */\n\n\tcase CU2_EXCEPTION:\n\t\tforce_sig(SIGILL, current);\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nasmlinkage void do_cpu(struct pt_regs *regs)\n{\n\tunsigned int __user *epc;\n\tunsigned long old_epc;\n\tunsigned int opcode;\n\tunsigned int cpid;\n\tint status;\n\tunsigned long __maybe_unused flags;\n\n\tdie_if_kernel(\"do_cpu invoked from kernel context!\", regs);\n\n\tcpid = (regs->cp0_cause >> CAUSEB_CE) & 3;\n\n\tswitch (cpid) {\n\tcase 0:\n\t\tepc = (unsigned int __user *)exception_epc(regs);\n\t\told_epc = regs->cp0_epc;\n\t\topcode = 0;\n\t\tstatus = -1;\n\n\t\tif (unlikely(compute_return_epc(regs) < 0))\n\t\t\treturn;\n\n\t\tif (unlikely(get_user(opcode, epc) < 0))\n\t\t\tstatus = SIGSEGV;\n\n\t\tif (!cpu_has_llsc && status < 0)\n\t\t\tstatus = simulate_llsc(regs, opcode);\n\n\t\tif (status < 0)\n\t\t\tstatus = simulate_rdhwr(regs, opcode);\n\n\t\tif (status < 0)\n\t\t\tstatus = SIGILL;\n\n\t\tif (unlikely(status > 0)) {\n\t\t\tregs->cp0_epc = old_epc;\t/* Undo skip-over.  */\n\t\t\tforce_sig(status, current);\n\t\t}\n\n\t\treturn;\n\n\tcase 1:\n\t\tif (used_math())\t/* Using the FPU again.  */\n\t\t\town_fpu(1);\n\t\telse {\t\t\t/* First time FPU user.  */\n\t\t\tinit_fpu();\n\t\t\tset_used_math();\n\t\t}\n\n\t\tif (!raw_cpu_has_fpu) {\n\t\t\tint sig;\n\t\t\tvoid __user *fault_addr = NULL;\n\t\t\tsig = fpu_emulator_cop1Handler(regs,\n\t\t\t\t\t\t       &current->thread.fpu,\n\t\t\t\t\t\t       0, &fault_addr);\n\t\t\tif (!process_fpemu_return(sig, fault_addr))\n\t\t\t\tmt_ase_fp_affinity();\n\t\t}\n\n\t\treturn;\n\n\tcase 2:\n\t\traw_notifier_call_chain(&cu2_chain, CU2_EXCEPTION, regs);\n\t\treturn;\n\n\tcase 3:\n\t\tbreak;\n\t}\n\n\tforce_sig(SIGILL, current);\n}\n\nasmlinkage void do_mdmx(struct pt_regs *regs)\n{\n\tforce_sig(SIGILL, current);\n}\n\n/*\n * Called with interrupts disabled.\n */\nasmlinkage void do_watch(struct pt_regs *regs)\n{\n\tu32 cause;\n\n\t/*\n\t * Clear WP (bit 22) bit of cause register so we don't loop\n\t * forever.\n\t */\n\tcause = read_c0_cause();\n\tcause &= ~(1 << 22);\n\twrite_c0_cause(cause);\n\n\t/*\n\t * If the current thread has the watch registers loaded, save\n\t * their values and send SIGTRAP.  Otherwise another thread\n\t * left the registers set, clear them and continue.\n\t */\n\tif (test_tsk_thread_flag(current, TIF_LOAD_WATCH)) {\n\t\tmips_read_watch_registers();\n\t\tlocal_irq_enable();\n\t\tforce_sig(SIGTRAP, current);\n\t} else {\n\t\tmips_clear_watch_registers();\n\t\tlocal_irq_enable();\n\t}\n}\n\nasmlinkage void do_mcheck(struct pt_regs *regs)\n{\n\tconst int field = 2 * sizeof(unsigned long);\n\tint multi_match = regs->cp0_status & ST0_TS;\n\n\tshow_regs(regs);\n\n\tif (multi_match) {\n\t\tprintk(\"Index   : %0x\\n\", read_c0_index());\n\t\tprintk(\"Pagemask: %0x\\n\", read_c0_pagemask());\n\t\tprintk(\"EntryHi : %0*lx\\n\", field, read_c0_entryhi());\n\t\tprintk(\"EntryLo0: %0*lx\\n\", field, read_c0_entrylo0());\n\t\tprintk(\"EntryLo1: %0*lx\\n\", field, read_c0_entrylo1());\n\t\tprintk(\"\\n\");\n\t\tdump_tlb_all();\n\t}\n\n\tshow_code((unsigned int __user *) regs->cp0_epc);\n\n\t/*\n\t * Some chips may have other causes of machine check (e.g. SB1\n\t * graduation timer)\n\t */\n\tpanic(\"Caught Machine Check exception - %scaused by multiple \"\n\t      \"matching entries in the TLB.\",\n\t      (multi_match) ? \"\" : \"not \");\n}\n\nasmlinkage void do_mt(struct pt_regs *regs)\n{\n\tint subcode;\n\n\tsubcode = (read_vpe_c0_vpecontrol() & VPECONTROL_EXCPT)\n\t\t\t>> VPECONTROL_EXCPT_SHIFT;\n\tswitch (subcode) {\n\tcase 0:\n\t\tprintk(KERN_DEBUG \"Thread Underflow\\n\");\n\t\tbreak;\n\tcase 1:\n\t\tprintk(KERN_DEBUG \"Thread Overflow\\n\");\n\t\tbreak;\n\tcase 2:\n\t\tprintk(KERN_DEBUG \"Invalid YIELD Qualifier\\n\");\n\t\tbreak;\n\tcase 3:\n\t\tprintk(KERN_DEBUG \"Gating Storage Exception\\n\");\n\t\tbreak;\n\tcase 4:\n\t\tprintk(KERN_DEBUG \"YIELD Scheduler Exception\\n\");\n\t\tbreak;\n\tcase 5:\n\t\tprintk(KERN_DEBUG \"Gating Storage Schedulier Exception\\n\");\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_DEBUG \"*** UNKNOWN THREAD EXCEPTION %d ***\\n\",\n\t\t\tsubcode);\n\t\tbreak;\n\t}\n\tdie_if_kernel(\"MIPS MT Thread exception in kernel\", regs);\n\n\tforce_sig(SIGILL, current);\n}\n\n\nasmlinkage void do_dsp(struct pt_regs *regs)\n{\n\tif (cpu_has_dsp)\n\t\tpanic(\"Unexpected DSP exception\\n\");\n\n\tforce_sig(SIGILL, current);\n}\n\nasmlinkage void do_reserved(struct pt_regs *regs)\n{\n\t/*\n\t * Game over - no way to handle this if it ever occurs.  Most probably\n\t * caused by a new unknown cpu type or after another deadly\n\t * hard/software error.\n\t */\n\tshow_regs(regs);\n\tpanic(\"Caught reserved exception %ld - should not happen.\",\n\t      (regs->cp0_cause & 0x7f) >> 2);\n}\n\nstatic int __initdata l1parity = 1;\nstatic int __init nol1parity(char *s)\n{\n\tl1parity = 0;\n\treturn 1;\n}\n__setup(\"nol1par\", nol1parity);\nstatic int __initdata l2parity = 1;\nstatic int __init nol2parity(char *s)\n{\n\tl2parity = 0;\n\treturn 1;\n}\n__setup(\"nol2par\", nol2parity);\n\n/*\n * Some MIPS CPUs can enable/disable for cache parity detection, but do\n * it different ways.\n */\nstatic inline void parity_protection_init(void)\n{\n\tswitch (current_cpu_type()) {\n\tcase CPU_24K:\n\tcase CPU_34K:\n\tcase CPU_74K:\n\tcase CPU_1004K:\n\t\t{\n#define ERRCTL_PE\t0x80000000\n#define ERRCTL_L2P\t0x00800000\n\t\t\tunsigned long errctl;\n\t\t\tunsigned int l1parity_present, l2parity_present;\n\n\t\t\terrctl = read_c0_ecc();\n\t\t\terrctl &= ~(ERRCTL_PE|ERRCTL_L2P);\n\n\t\t\t/* probe L1 parity support */\n\t\t\twrite_c0_ecc(errctl | ERRCTL_PE);\n\t\t\tback_to_back_c0_hazard();\n\t\t\tl1parity_present = (read_c0_ecc() & ERRCTL_PE);\n\n\t\t\t/* probe L2 parity support */\n\t\t\twrite_c0_ecc(errctl|ERRCTL_L2P);\n\t\t\tback_to_back_c0_hazard();\n\t\t\tl2parity_present = (read_c0_ecc() & ERRCTL_L2P);\n\n\t\t\tif (l1parity_present && l2parity_present) {\n\t\t\t\tif (l1parity)\n\t\t\t\t\terrctl |= ERRCTL_PE;\n\t\t\t\tif (l1parity ^ l2parity)\n\t\t\t\t\terrctl |= ERRCTL_L2P;\n\t\t\t} else if (l1parity_present) {\n\t\t\t\tif (l1parity)\n\t\t\t\t\terrctl |= ERRCTL_PE;\n\t\t\t} else if (l2parity_present) {\n\t\t\t\tif (l2parity)\n\t\t\t\t\terrctl |= ERRCTL_L2P;\n\t\t\t} else {\n\t\t\t\t/* No parity available */\n\t\t\t}\n\n\t\t\tprintk(KERN_INFO \"Writing ErrCtl register=%08lx\\n\", errctl);\n\n\t\t\twrite_c0_ecc(errctl);\n\t\t\tback_to_back_c0_hazard();\n\t\t\terrctl = read_c0_ecc();\n\t\t\tprintk(KERN_INFO \"Readback ErrCtl register=%08lx\\n\", errctl);\n\n\t\t\tif (l1parity_present)\n\t\t\t\tprintk(KERN_INFO \"Cache parity protection %sabled\\n\",\n\t\t\t\t       (errctl & ERRCTL_PE) ? \"en\" : \"dis\");\n\n\t\t\tif (l2parity_present) {\n\t\t\t\tif (l1parity_present && l1parity)\n\t\t\t\t\terrctl ^= ERRCTL_L2P;\n\t\t\t\tprintk(KERN_INFO \"L2 cache parity protection %sabled\\n\",\n\t\t\t\t       (errctl & ERRCTL_L2P) ? \"en\" : \"dis\");\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\tcase CPU_5KC:\n\t\twrite_c0_ecc(0x80000000);\n\t\tback_to_back_c0_hazard();\n\t\t/* Set the PE bit (bit 31) in the c0_errctl register. */\n\t\tprintk(KERN_INFO \"Cache parity protection %sabled\\n\",\n\t\t       (read_c0_ecc() & 0x80000000) ? \"en\" : \"dis\");\n\t\tbreak;\n\tcase CPU_20KC:\n\tcase CPU_25KF:\n\t\t/* Clear the DE bit (bit 16) in the c0_status register. */\n\t\tprintk(KERN_INFO \"Enable cache parity protection for \"\n\t\t       \"MIPS 20KC/25KF CPUs.\\n\");\n\t\tclear_c0_status(ST0_DE);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nasmlinkage void cache_parity_error(void)\n{\n\tconst int field = 2 * sizeof(unsigned long);\n\tunsigned int reg_val;\n\n\t/* For the moment, report the problem and hang. */\n\tprintk(\"Cache error exception:\\n\");\n\tprintk(\"cp0_errorepc == %0*lx\\n\", field, read_c0_errorepc());\n\treg_val = read_c0_cacheerr();\n\tprintk(\"c0_cacheerr == %08x\\n\", reg_val);\n\n\tprintk(\"Decoded c0_cacheerr: %s cache fault in %s reference.\\n\",\n\t       reg_val & (1<<30) ? \"secondary\" : \"primary\",\n\t       reg_val & (1<<31) ? \"data\" : \"insn\");\n\tprintk(\"Error bits: %s%s%s%s%s%s%s\\n\",\n\t       reg_val & (1<<29) ? \"ED \" : \"\",\n\t       reg_val & (1<<28) ? \"ET \" : \"\",\n\t       reg_val & (1<<26) ? \"EE \" : \"\",\n\t       reg_val & (1<<25) ? \"EB \" : \"\",\n\t       reg_val & (1<<24) ? \"EI \" : \"\",\n\t       reg_val & (1<<23) ? \"E1 \" : \"\",\n\t       reg_val & (1<<22) ? \"E0 \" : \"\");\n\tprintk(\"IDX: 0x%08x\\n\", reg_val & ((1<<22)-1));\n\n#if defined(CONFIG_CPU_MIPS32) || defined(CONFIG_CPU_MIPS64)\n\tif (reg_val & (1<<22))\n\t\tprintk(\"DErrAddr0: 0x%0*lx\\n\", field, read_c0_derraddr0());\n\n\tif (reg_val & (1<<23))\n\t\tprintk(\"DErrAddr1: 0x%0*lx\\n\", field, read_c0_derraddr1());\n#endif\n\n\tpanic(\"Can't handle the cache error!\");\n}\n\n/*\n * SDBBP EJTAG debug exception handler.\n * We skip the instruction and return to the next instruction.\n */\nvoid ejtag_exception_handler(struct pt_regs *regs)\n{\n\tconst int field = 2 * sizeof(unsigned long);\n\tunsigned long depc, old_epc;\n\tunsigned int debug;\n\n\tprintk(KERN_DEBUG \"SDBBP EJTAG debug exception - not handled yet, just ignored!\\n\");\n\tdepc = read_c0_depc();\n\tdebug = read_c0_debug();\n\tprintk(KERN_DEBUG \"c0_depc = %0*lx, DEBUG = %08x\\n\", field, depc, debug);\n\tif (debug & 0x80000000) {\n\t\t/*\n\t\t * In branch delay slot.\n\t\t * We cheat a little bit here and use EPC to calculate the\n\t\t * debug return address (DEPC). EPC is restored after the\n\t\t * calculation.\n\t\t */\n\t\told_epc = regs->cp0_epc;\n\t\tregs->cp0_epc = depc;\n\t\t__compute_return_epc(regs);\n\t\tdepc = regs->cp0_epc;\n\t\tregs->cp0_epc = old_epc;\n\t} else\n\t\tdepc += 4;\n\twrite_c0_depc(depc);\n\n#if 0\n\tprintk(KERN_DEBUG \"\\n\\n----- Enable EJTAG single stepping ----\\n\\n\");\n\twrite_c0_debug(debug | 0x100);\n#endif\n}\n\n/*\n * NMI exception handler.\n */\nNORET_TYPE void ATTRIB_NORET nmi_exception_handler(struct pt_regs *regs)\n{\n\tbust_spinlocks(1);\n\tprintk(\"NMI taken!!!!\\n\");\n\tdie(\"NMI\", regs);\n}\n\n#define VECTORSPACING 0x100\t/* for EI/VI mode */\n\nunsigned long ebase;\nunsigned long exception_handlers[32];\nunsigned long vi_handlers[64];\n\nvoid __init *set_except_vector(int n, void *addr)\n{\n\tunsigned long handler = (unsigned long) addr;\n\tunsigned long old_handler = exception_handlers[n];\n\n\texception_handlers[n] = handler;\n\tif (n == 0 && cpu_has_divec) {\n\t\tunsigned long jump_mask = ~((1 << 28) - 1);\n\t\tu32 *buf = (u32 *)(ebase + 0x200);\n\t\tunsigned int k0 = 26;\n\t\tif ((handler & jump_mask) == ((ebase + 0x200) & jump_mask)) {\n\t\t\tuasm_i_j(&buf, handler & ~jump_mask);\n\t\t\tuasm_i_nop(&buf);\n\t\t} else {\n\t\t\tUASM_i_LA(&buf, k0, handler);\n\t\t\tuasm_i_jr(&buf, k0);\n\t\t\tuasm_i_nop(&buf);\n\t\t}\n\t\tlocal_flush_icache_range(ebase + 0x200, (unsigned long)buf);\n\t}\n\treturn (void *)old_handler;\n}\n\nstatic asmlinkage void do_default_vi(void)\n{\n\tshow_regs(get_irq_regs());\n\tpanic(\"Caught unexpected vectored interrupt.\");\n}\n\nstatic void *set_vi_srs_handler(int n, vi_handler_t addr, int srs)\n{\n\tunsigned long handler;\n\tunsigned long old_handler = vi_handlers[n];\n\tint srssets = current_cpu_data.srsets;\n\tu32 *w;\n\tunsigned char *b;\n\n\tBUG_ON(!cpu_has_veic && !cpu_has_vint);\n\n\tif (addr == NULL) {\n\t\thandler = (unsigned long) do_default_vi;\n\t\tsrs = 0;\n\t} else\n\t\thandler = (unsigned long) addr;\n\tvi_handlers[n] = (unsigned long) addr;\n\n\tb = (unsigned char *)(ebase + 0x200 + n*VECTORSPACING);\n\n\tif (srs >= srssets)\n\t\tpanic(\"Shadow register set %d not supported\", srs);\n\n\tif (cpu_has_veic) {\n\t\tif (board_bind_eic_interrupt)\n\t\t\tboard_bind_eic_interrupt(n, srs);\n\t} else if (cpu_has_vint) {\n\t\t/* SRSMap is only defined if shadow sets are implemented */\n\t\tif (srssets > 1)\n\t\t\tchange_c0_srsmap(0xf << n*4, srs << n*4);\n\t}\n\n\tif (srs == 0) {\n\t\t/*\n\t\t * If no shadow set is selected then use the default handler\n\t\t * that does normal register saving and a standard interrupt exit\n\t\t */\n\n\t\textern char except_vec_vi, except_vec_vi_lui;\n\t\textern char except_vec_vi_ori, except_vec_vi_end;\n\t\textern char rollback_except_vec_vi;\n\t\tchar *vec_start = (cpu_wait == r4k_wait) ?\n\t\t\t&rollback_except_vec_vi : &except_vec_vi;\n#ifdef CONFIG_MIPS_MT_SMTC\n\t\t/*\n\t\t * We need to provide the SMTC vectored interrupt handler\n\t\t * not only with the address of the handler, but with the\n\t\t * Status.IM bit to be masked before going there.\n\t\t */\n\t\textern char except_vec_vi_mori;\n\t\tconst int mori_offset = &except_vec_vi_mori - vec_start;\n#endif /* CONFIG_MIPS_MT_SMTC */\n\t\tconst int handler_len = &except_vec_vi_end - vec_start;\n\t\tconst int lui_offset = &except_vec_vi_lui - vec_start;\n\t\tconst int ori_offset = &except_vec_vi_ori - vec_start;\n\n\t\tif (handler_len > VECTORSPACING) {\n\t\t\t/*\n\t\t\t * Sigh... panicing won't help as the console\n\t\t\t * is probably not configured :(\n\t\t\t */\n\t\t\tpanic(\"VECTORSPACING too small\");\n\t\t}\n\n\t\tmemcpy(b, vec_start, handler_len);\n#ifdef CONFIG_MIPS_MT_SMTC\n\t\tBUG_ON(n > 7);\t/* Vector index %d exceeds SMTC maximum. */\n\n\t\tw = (u32 *)(b + mori_offset);\n\t\t*w = (*w & 0xffff0000) | (0x100 << n);\n#endif /* CONFIG_MIPS_MT_SMTC */\n\t\tw = (u32 *)(b + lui_offset);\n\t\t*w = (*w & 0xffff0000) | (((u32)handler >> 16) & 0xffff);\n\t\tw = (u32 *)(b + ori_offset);\n\t\t*w = (*w & 0xffff0000) | ((u32)handler & 0xffff);\n\t\tlocal_flush_icache_range((unsigned long)b,\n\t\t\t\t\t (unsigned long)(b+handler_len));\n\t}\n\telse {\n\t\t/*\n\t\t * In other cases jump directly to the interrupt handler\n\t\t *\n\t\t * It is the handlers responsibility to save registers if required\n\t\t * (eg hi/lo) and return from the exception using \"eret\"\n\t\t */\n\t\tw = (u32 *)b;\n\t\t*w++ = 0x08000000 | (((u32)handler >> 2) & 0x03fffff); /* j handler */\n\t\t*w = 0;\n\t\tlocal_flush_icache_range((unsigned long)b,\n\t\t\t\t\t (unsigned long)(b+8));\n\t}\n\n\treturn (void *)old_handler;\n}\n\nvoid *set_vi_handler(int n, vi_handler_t addr)\n{\n\treturn set_vi_srs_handler(n, addr, 0);\n}\n\nextern void cpu_cache_init(void);\nextern void tlb_init(void);\nextern void flush_tlb_handlers(void);\n\n/*\n * Timer interrupt\n */\nint cp0_compare_irq;\nint cp0_compare_irq_shift;\n\n/*\n * Performance counter IRQ or -1 if shared with timer\n */\nint cp0_perfcount_irq;\nEXPORT_SYMBOL_GPL(cp0_perfcount_irq);\n\nstatic int __cpuinitdata noulri;\n\nstatic int __init ulri_disable(char *s)\n{\n\tpr_info(\"Disabling ulri\\n\");\n\tnoulri = 1;\n\n\treturn 1;\n}\n__setup(\"noulri\", ulri_disable);\n\nvoid __cpuinit per_cpu_trap_init(void)\n{\n\tunsigned int cpu = smp_processor_id();\n\tunsigned int status_set = ST0_CU0;\n\tunsigned int hwrena = cpu_hwrena_impl_bits;\n#ifdef CONFIG_MIPS_MT_SMTC\n\tint secondaryTC = 0;\n\tint bootTC = (cpu == 0);\n\n\t/*\n\t * Only do per_cpu_trap_init() for first TC of Each VPE.\n\t * Note that this hack assumes that the SMTC init code\n\t * assigns TCs consecutively and in ascending order.\n\t */\n\n\tif (((read_c0_tcbind() & TCBIND_CURTC) != 0) &&\n\t    ((read_c0_tcbind() & TCBIND_CURVPE) == cpu_data[cpu - 1].vpe_id))\n\t\tsecondaryTC = 1;\n#endif /* CONFIG_MIPS_MT_SMTC */\n\n\t/*\n\t * Disable coprocessors and select 32-bit or 64-bit addressing\n\t * and the 16/32 or 32/32 FPR register model.  Reset the BEV\n\t * flag that some firmware may have left set and the TS bit (for\n\t * IP27).  Set XX for ISA IV code to work.\n\t */\n#ifdef CONFIG_64BIT\n\tstatus_set |= ST0_FR|ST0_KX|ST0_SX|ST0_UX;\n#endif\n\tif (current_cpu_data.isa_level == MIPS_CPU_ISA_IV)\n\t\tstatus_set |= ST0_XX;\n\tif (cpu_has_dsp)\n\t\tstatus_set |= ST0_MX;\n\n\tchange_c0_status(ST0_CU|ST0_MX|ST0_RE|ST0_FR|ST0_BEV|ST0_TS|ST0_KX|ST0_SX|ST0_UX,\n\t\t\t status_set);\n\n\tif (cpu_has_mips_r2)\n\t\thwrena |= 0x0000000f;\n\n\tif (!noulri && cpu_has_userlocal)\n\t\thwrena |= (1 << 29);\n\n\tif (hwrena)\n\t\twrite_c0_hwrena(hwrena);\n\n#ifdef CONFIG_MIPS_MT_SMTC\n\tif (!secondaryTC) {\n#endif /* CONFIG_MIPS_MT_SMTC */\n\n\tif (cpu_has_veic || cpu_has_vint) {\n\t\tunsigned long sr = set_c0_status(ST0_BEV);\n\t\twrite_c0_ebase(ebase);\n\t\twrite_c0_status(sr);\n\t\t/* Setting vector spacing enables EI/VI mode  */\n\t\tchange_c0_intctl(0x3e0, VECTORSPACING);\n\t}\n\tif (cpu_has_divec) {\n\t\tif (cpu_has_mipsmt) {\n\t\t\tunsigned int vpflags = dvpe();\n\t\t\tset_c0_cause(CAUSEF_IV);\n\t\t\tevpe(vpflags);\n\t\t} else\n\t\t\tset_c0_cause(CAUSEF_IV);\n\t}\n\n\t/*\n\t * Before R2 both interrupt numbers were fixed to 7, so on R2 only:\n\t *\n\t *  o read IntCtl.IPTI to determine the timer interrupt\n\t *  o read IntCtl.IPPCI to determine the performance counter interrupt\n\t */\n\tif (cpu_has_mips_r2) {\n\t\tcp0_compare_irq_shift = CAUSEB_TI - CAUSEB_IP;\n\t\tcp0_compare_irq = (read_c0_intctl() >> INTCTLB_IPTI) & 7;\n\t\tcp0_perfcount_irq = (read_c0_intctl() >> INTCTLB_IPPCI) & 7;\n\t\tif (cp0_perfcount_irq == cp0_compare_irq)\n\t\t\tcp0_perfcount_irq = -1;\n\t} else {\n\t\tcp0_compare_irq = CP0_LEGACY_COMPARE_IRQ;\n\t\tcp0_compare_irq_shift = cp0_compare_irq;\n\t\tcp0_perfcount_irq = -1;\n\t}\n\n#ifdef CONFIG_MIPS_MT_SMTC\n\t}\n#endif /* CONFIG_MIPS_MT_SMTC */\n\n\tcpu_data[cpu].asid_cache = ASID_FIRST_VERSION;\n\n\tatomic_inc(&init_mm.mm_count);\n\tcurrent->active_mm = &init_mm;\n\tBUG_ON(current->mm);\n\tenter_lazy_tlb(&init_mm, current);\n\n#ifdef CONFIG_MIPS_MT_SMTC\n\tif (bootTC) {\n#endif /* CONFIG_MIPS_MT_SMTC */\n\t\tcpu_cache_init();\n\t\ttlb_init();\n#ifdef CONFIG_MIPS_MT_SMTC\n\t} else if (!secondaryTC) {\n\t\t/*\n\t\t * First TC in non-boot VPE must do subset of tlb_init()\n\t\t * for MMU countrol registers.\n\t\t */\n\t\twrite_c0_pagemask(PM_DEFAULT_MASK);\n\t\twrite_c0_wired(0);\n\t}\n#endif /* CONFIG_MIPS_MT_SMTC */\n\tTLBMISS_HANDLER_SETUP();\n}\n\n/* Install CPU exception handler */\nvoid __init set_handler(unsigned long offset, void *addr, unsigned long size)\n{\n\tmemcpy((void *)(ebase + offset), addr, size);\n\tlocal_flush_icache_range(ebase + offset, ebase + offset + size);\n}\n\nstatic char panic_null_cerr[] __cpuinitdata =\n\t\"Trying to set NULL cache error exception handler\";\n\n/*\n * Install uncached CPU exception handler.\n * This is suitable only for the cache error exception which is the only\n * exception handler that is being run uncached.\n */\nvoid __cpuinit set_uncached_handler(unsigned long offset, void *addr,\n\tunsigned long size)\n{\n\tunsigned long uncached_ebase = CKSEG1ADDR(ebase);\n\n\tif (!addr)\n\t\tpanic(panic_null_cerr);\n\n\tmemcpy((void *)(uncached_ebase + offset), addr, size);\n}\n\nstatic int __initdata rdhwr_noopt;\nstatic int __init set_rdhwr_noopt(char *str)\n{\n\trdhwr_noopt = 1;\n\treturn 1;\n}\n\n__setup(\"rdhwr_noopt\", set_rdhwr_noopt);\n\nvoid __init trap_init(void)\n{\n\textern char except_vec3_generic, except_vec3_r4000;\n\textern char except_vec4;\n\tunsigned long i;\n\tint rollback;\n\n\tcheck_wait();\n\trollback = (cpu_wait == r4k_wait);\n\n#if defined(CONFIG_KGDB)\n\tif (kgdb_early_setup)\n\t\treturn;\t/* Already done */\n#endif\n\n\tif (cpu_has_veic || cpu_has_vint) {\n\t\tunsigned long size = 0x200 + VECTORSPACING*64;\n\t\tebase = (unsigned long)\n\t\t\t__alloc_bootmem(size, 1 << fls(size), 0);\n\t} else {\n\t\tebase = CKSEG0;\n\t\tif (cpu_has_mips_r2)\n\t\t\tebase += (read_c0_ebase() & 0x3ffff000);\n\t}\n\n\tper_cpu_trap_init();\n\n\t/*\n\t * Copy the generic exception handlers to their final destination.\n\t * This will be overriden later as suitable for a particular\n\t * configuration.\n\t */\n\tset_handler(0x180, &except_vec3_generic, 0x80);\n\n\t/*\n\t * Setup default vectors\n\t */\n\tfor (i = 0; i <= 31; i++)\n\t\tset_except_vector(i, handle_reserved);\n\n\t/*\n\t * Copy the EJTAG debug exception vector handler code to it's final\n\t * destination.\n\t */\n\tif (cpu_has_ejtag && board_ejtag_handler_setup)\n\t\tboard_ejtag_handler_setup();\n\n\t/*\n\t * Only some CPUs have the watch exceptions.\n\t */\n\tif (cpu_has_watch)\n\t\tset_except_vector(23, handle_watch);\n\n\t/*\n\t * Initialise interrupt handlers\n\t */\n\tif (cpu_has_veic || cpu_has_vint) {\n\t\tint nvec = cpu_has_veic ? 64 : 8;\n\t\tfor (i = 0; i < nvec; i++)\n\t\t\tset_vi_handler(i, NULL);\n\t}\n\telse if (cpu_has_divec)\n\t\tset_handler(0x200, &except_vec4, 0x8);\n\n\t/*\n\t * Some CPUs can enable/disable for cache parity detection, but does\n\t * it different ways.\n\t */\n\tparity_protection_init();\n\n\t/*\n\t * The Data Bus Errors / Instruction Bus Errors are signaled\n\t * by external hardware.  Therefore these two exceptions\n\t * may have board specific handlers.\n\t */\n\tif (board_be_init)\n\t\tboard_be_init();\n\n\tset_except_vector(0, rollback ? rollback_handle_int : handle_int);\n\tset_except_vector(1, handle_tlbm);\n\tset_except_vector(2, handle_tlbl);\n\tset_except_vector(3, handle_tlbs);\n\n\tset_except_vector(4, handle_adel);\n\tset_except_vector(5, handle_ades);\n\n\tset_except_vector(6, handle_ibe);\n\tset_except_vector(7, handle_dbe);\n\n\tset_except_vector(8, handle_sys);\n\tset_except_vector(9, handle_bp);\n\tset_except_vector(10, rdhwr_noopt ? handle_ri :\n\t\t\t  (cpu_has_vtag_icache ?\n\t\t\t   handle_ri_rdhwr_vivt : handle_ri_rdhwr));\n\tset_except_vector(11, handle_cpu);\n\tset_except_vector(12, handle_ov);\n\tset_except_vector(13, handle_tr);\n\n\tif (current_cpu_type() == CPU_R6000 ||\n\t    current_cpu_type() == CPU_R6000A) {\n\t\t/*\n\t\t * The R6000 is the only R-series CPU that features a machine\n\t\t * check exception (similar to the R4000 cache error) and\n\t\t * unaligned ldc1/sdc1 exception.  The handlers have not been\n\t\t * written yet.  Well, anyway there is no R6000 machine on the\n\t\t * current list of targets for Linux/MIPS.\n\t\t * (Duh, crap, there is someone with a triple R6k machine)\n\t\t */\n\t\t//set_except_vector(14, handle_mc);\n\t\t//set_except_vector(15, handle_ndc);\n\t}\n\n\n\tif (board_nmi_handler_setup)\n\t\tboard_nmi_handler_setup();\n\n\tif (cpu_has_fpu && !cpu_has_nofpuex)\n\t\tset_except_vector(15, handle_fpe);\n\n\tset_except_vector(22, handle_mdmx);\n\n\tif (cpu_has_mcheck)\n\t\tset_except_vector(24, handle_mcheck);\n\n\tif (cpu_has_mipsmt)\n\t\tset_except_vector(25, handle_mt);\n\n\tset_except_vector(26, handle_dsp);\n\n\tif (cpu_has_vce)\n\t\t/* Special exception: R4[04]00 uses also the divec space. */\n\t\tmemcpy((void *)(ebase + 0x180), &except_vec3_r4000, 0x100);\n\telse if (cpu_has_4kex)\n\t\tmemcpy((void *)(ebase + 0x180), &except_vec3_generic, 0x80);\n\telse\n\t\tmemcpy((void *)(ebase + 0x080), &except_vec3_generic, 0x80);\n\n\tlocal_flush_icache_range(ebase, ebase + 0x400);\n\tflush_tlb_handlers();\n\n\tsort_extable(__start___dbe_table, __stop___dbe_table);\n\n\tcu2_notifier(default_cu2_call, 0x80000000);\t/* Run last  */\n}\n", "/*\n * Handle unaligned accesses by emulation.\n *\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n *\n * Copyright (C) 1996, 1998, 1999, 2002 by Ralf Baechle\n * Copyright (C) 1999 Silicon Graphics, Inc.\n *\n * This file contains exception handler for address error exception with the\n * special capability to execute faulting instructions in software.  The\n * handler does not try to handle the case when the program counter points\n * to an address not aligned to a word boundary.\n *\n * Putting data to unaligned addresses is a bad practice even on Intel where\n * only the performance is affected.  Much worse is that such code is non-\n * portable.  Due to several programs that die on MIPS due to alignment\n * problems I decided to implement this handler anyway though I originally\n * didn't intend to do this at all for user code.\n *\n * For now I enable fixing of address errors by default to make life easier.\n * I however intend to disable this somewhen in the future when the alignment\n * problems with user programs have been fixed.  For programmers this is the\n * right way to go.\n *\n * Fixing address errors is a per process option.  The option is inherited\n * across fork(2) and execve(2) calls.  If you really want to use the\n * option in your user programs - I discourage the use of the software\n * emulation strongly - use the following code in your userland stuff:\n *\n * #include <sys/sysmips.h>\n *\n * ...\n * sysmips(MIPS_FIXADE, x);\n * ...\n *\n * The argument x is 0 for disabling software emulation, enabled otherwise.\n *\n * Below a little program to play around with this feature.\n *\n * #include <stdio.h>\n * #include <sys/sysmips.h>\n *\n * struct foo {\n *         unsigned char bar[8];\n * };\n *\n * main(int argc, char *argv[])\n * {\n *         struct foo x = {0, 1, 2, 3, 4, 5, 6, 7};\n *         unsigned int *p = (unsigned int *) (x.bar + 3);\n *         int i;\n *\n *         if (argc > 1)\n *                 sysmips(MIPS_FIXADE, atoi(argv[1]));\n *\n *         printf(\"*p = %08lx\\n\", *p);\n *\n *         *p = 0xdeadface;\n *\n *         for(i = 0; i <= 7; i++)\n *         printf(\"%02x \", x.bar[i]);\n *         printf(\"\\n\");\n * }\n *\n * Coprocessor loads are not supported; I think this case is unimportant\n * in the practice.\n *\n * TODO: Handle ndc (attempted store to doubleword in uncached memory)\n *       exception for the R6000.\n *       A store crossing a page boundary might be executed only partially.\n *       Undo the partial store in this case.\n */\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/signal.h>\n#include <linux/smp.h>\n#include <linux/sched.h>\n#include <linux/debugfs.h>\n#include <linux/perf_event.h>\n\n#include <asm/asm.h>\n#include <asm/branch.h>\n#include <asm/byteorder.h>\n#include <asm/cop2.h>\n#include <asm/inst.h>\n#include <asm/uaccess.h>\n#include <asm/system.h>\n\n#define STR(x)  __STR(x)\n#define __STR(x)  #x\n\nenum {\n\tUNALIGNED_ACTION_QUIET,\n\tUNALIGNED_ACTION_SIGNAL,\n\tUNALIGNED_ACTION_SHOW,\n};\n#ifdef CONFIG_DEBUG_FS\nstatic u32 unaligned_instructions;\nstatic u32 unaligned_action;\n#else\n#define unaligned_action UNALIGNED_ACTION_QUIET\n#endif\nextern void show_registers(struct pt_regs *regs);\n\nstatic void emulate_load_store_insn(struct pt_regs *regs,\n\tvoid __user *addr, unsigned int __user *pc)\n{\n\tunion mips_instruction insn;\n\tunsigned long value;\n\tunsigned int res;\n\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);\n\n\t/*\n\t * This load never faults.\n\t */\n\t__get_user(insn.word, pc);\n\n\tswitch (insn.i_format.opcode) {\n\t/*\n\t * These are instructions that a compiler doesn't generate.  We\n\t * can assume therefore that the code is MIPS-aware and\n\t * really buggy.  Emulating these instructions would break the\n\t * semantics anyway.\n\t */\n\tcase ll_op:\n\tcase lld_op:\n\tcase sc_op:\n\tcase scd_op:\n\n\t/*\n\t * For these instructions the only way to create an address\n\t * error is an attempted access to kernel/supervisor address\n\t * space.\n\t */\n\tcase ldl_op:\n\tcase ldr_op:\n\tcase lwl_op:\n\tcase lwr_op:\n\tcase sdl_op:\n\tcase sdr_op:\n\tcase swl_op:\n\tcase swr_op:\n\tcase lb_op:\n\tcase lbu_op:\n\tcase sb_op:\n\t\tgoto sigbus;\n\n\t/*\n\t * The remaining opcodes are the ones that are really of interest.\n\t */\n\tcase lh_op:\n\t\tif (!access_ok(VERIFY_READ, addr, 2))\n\t\t\tgoto sigbus;\n\n\t\t__asm__ __volatile__ (\".set\\tnoat\\n\"\n#ifdef __BIG_ENDIAN\n\t\t\t\"1:\\tlb\\t%0, 0(%2)\\n\"\n\t\t\t\"2:\\tlbu\\t$1, 1(%2)\\n\\t\"\n#endif\n#ifdef __LITTLE_ENDIAN\n\t\t\t\"1:\\tlb\\t%0, 1(%2)\\n\"\n\t\t\t\"2:\\tlbu\\t$1, 0(%2)\\n\\t\"\n#endif\n\t\t\t\"sll\\t%0, 0x8\\n\\t\"\n\t\t\t\"or\\t%0, $1\\n\\t\"\n\t\t\t\"li\\t%1, 0\\n\"\n\t\t\t\"3:\\t.set\\tat\\n\\t\"\n\t\t\t\".section\\t.fixup,\\\"ax\\\"\\n\\t\"\n\t\t\t\"4:\\tli\\t%1, %3\\n\\t\"\n\t\t\t\"j\\t3b\\n\\t\"\n\t\t\t\".previous\\n\\t\"\n\t\t\t\".section\\t__ex_table,\\\"a\\\"\\n\\t\"\n\t\t\tSTR(PTR)\"\\t1b, 4b\\n\\t\"\n\t\t\tSTR(PTR)\"\\t2b, 4b\\n\\t\"\n\t\t\t\".previous\"\n\t\t\t: \"=&r\" (value), \"=r\" (res)\n\t\t\t: \"r\" (addr), \"i\" (-EFAULT));\n\t\tif (res)\n\t\t\tgoto fault;\n\t\tcompute_return_epc(regs);\n\t\tregs->regs[insn.i_format.rt] = value;\n\t\tbreak;\n\n\tcase lw_op:\n\t\tif (!access_ok(VERIFY_READ, addr, 4))\n\t\t\tgoto sigbus;\n\n\t\t__asm__ __volatile__ (\n#ifdef __BIG_ENDIAN\n\t\t\t\"1:\\tlwl\\t%0, (%2)\\n\"\n\t\t\t\"2:\\tlwr\\t%0, 3(%2)\\n\\t\"\n#endif\n#ifdef __LITTLE_ENDIAN\n\t\t\t\"1:\\tlwl\\t%0, 3(%2)\\n\"\n\t\t\t\"2:\\tlwr\\t%0, (%2)\\n\\t\"\n#endif\n\t\t\t\"li\\t%1, 0\\n\"\n\t\t\t\"3:\\t.section\\t.fixup,\\\"ax\\\"\\n\\t\"\n\t\t\t\"4:\\tli\\t%1, %3\\n\\t\"\n\t\t\t\"j\\t3b\\n\\t\"\n\t\t\t\".previous\\n\\t\"\n\t\t\t\".section\\t__ex_table,\\\"a\\\"\\n\\t\"\n\t\t\tSTR(PTR)\"\\t1b, 4b\\n\\t\"\n\t\t\tSTR(PTR)\"\\t2b, 4b\\n\\t\"\n\t\t\t\".previous\"\n\t\t\t: \"=&r\" (value), \"=r\" (res)\n\t\t\t: \"r\" (addr), \"i\" (-EFAULT));\n\t\tif (res)\n\t\t\tgoto fault;\n\t\tcompute_return_epc(regs);\n\t\tregs->regs[insn.i_format.rt] = value;\n\t\tbreak;\n\n\tcase lhu_op:\n\t\tif (!access_ok(VERIFY_READ, addr, 2))\n\t\t\tgoto sigbus;\n\n\t\t__asm__ __volatile__ (\n\t\t\t\".set\\tnoat\\n\"\n#ifdef __BIG_ENDIAN\n\t\t\t\"1:\\tlbu\\t%0, 0(%2)\\n\"\n\t\t\t\"2:\\tlbu\\t$1, 1(%2)\\n\\t\"\n#endif\n#ifdef __LITTLE_ENDIAN\n\t\t\t\"1:\\tlbu\\t%0, 1(%2)\\n\"\n\t\t\t\"2:\\tlbu\\t$1, 0(%2)\\n\\t\"\n#endif\n\t\t\t\"sll\\t%0, 0x8\\n\\t\"\n\t\t\t\"or\\t%0, $1\\n\\t\"\n\t\t\t\"li\\t%1, 0\\n\"\n\t\t\t\"3:\\t.set\\tat\\n\\t\"\n\t\t\t\".section\\t.fixup,\\\"ax\\\"\\n\\t\"\n\t\t\t\"4:\\tli\\t%1, %3\\n\\t\"\n\t\t\t\"j\\t3b\\n\\t\"\n\t\t\t\".previous\\n\\t\"\n\t\t\t\".section\\t__ex_table,\\\"a\\\"\\n\\t\"\n\t\t\tSTR(PTR)\"\\t1b, 4b\\n\\t\"\n\t\t\tSTR(PTR)\"\\t2b, 4b\\n\\t\"\n\t\t\t\".previous\"\n\t\t\t: \"=&r\" (value), \"=r\" (res)\n\t\t\t: \"r\" (addr), \"i\" (-EFAULT));\n\t\tif (res)\n\t\t\tgoto fault;\n\t\tcompute_return_epc(regs);\n\t\tregs->regs[insn.i_format.rt] = value;\n\t\tbreak;\n\n\tcase lwu_op:\n#ifdef CONFIG_64BIT\n\t\t/*\n\t\t * A 32-bit kernel might be running on a 64-bit processor.  But\n\t\t * if we're on a 32-bit processor and an i-cache incoherency\n\t\t * or race makes us see a 64-bit instruction here the sdl/sdr\n\t\t * would blow up, so for now we don't handle unaligned 64-bit\n\t\t * instructions on 32-bit kernels.\n\t\t */\n\t\tif (!access_ok(VERIFY_READ, addr, 4))\n\t\t\tgoto sigbus;\n\n\t\t__asm__ __volatile__ (\n#ifdef __BIG_ENDIAN\n\t\t\t\"1:\\tlwl\\t%0, (%2)\\n\"\n\t\t\t\"2:\\tlwr\\t%0, 3(%2)\\n\\t\"\n#endif\n#ifdef __LITTLE_ENDIAN\n\t\t\t\"1:\\tlwl\\t%0, 3(%2)\\n\"\n\t\t\t\"2:\\tlwr\\t%0, (%2)\\n\\t\"\n#endif\n\t\t\t\"dsll\\t%0, %0, 32\\n\\t\"\n\t\t\t\"dsrl\\t%0, %0, 32\\n\\t\"\n\t\t\t\"li\\t%1, 0\\n\"\n\t\t\t\"3:\\t.section\\t.fixup,\\\"ax\\\"\\n\\t\"\n\t\t\t\"4:\\tli\\t%1, %3\\n\\t\"\n\t\t\t\"j\\t3b\\n\\t\"\n\t\t\t\".previous\\n\\t\"\n\t\t\t\".section\\t__ex_table,\\\"a\\\"\\n\\t\"\n\t\t\tSTR(PTR)\"\\t1b, 4b\\n\\t\"\n\t\t\tSTR(PTR)\"\\t2b, 4b\\n\\t\"\n\t\t\t\".previous\"\n\t\t\t: \"=&r\" (value), \"=r\" (res)\n\t\t\t: \"r\" (addr), \"i\" (-EFAULT));\n\t\tif (res)\n\t\t\tgoto fault;\n\t\tcompute_return_epc(regs);\n\t\tregs->regs[insn.i_format.rt] = value;\n\t\tbreak;\n#endif /* CONFIG_64BIT */\n\n\t\t/* Cannot handle 64-bit instructions in 32-bit kernel */\n\t\tgoto sigill;\n\n\tcase ld_op:\n#ifdef CONFIG_64BIT\n\t\t/*\n\t\t * A 32-bit kernel might be running on a 64-bit processor.  But\n\t\t * if we're on a 32-bit processor and an i-cache incoherency\n\t\t * or race makes us see a 64-bit instruction here the sdl/sdr\n\t\t * would blow up, so for now we don't handle unaligned 64-bit\n\t\t * instructions on 32-bit kernels.\n\t\t */\n\t\tif (!access_ok(VERIFY_READ, addr, 8))\n\t\t\tgoto sigbus;\n\n\t\t__asm__ __volatile__ (\n#ifdef __BIG_ENDIAN\n\t\t\t\"1:\\tldl\\t%0, (%2)\\n\"\n\t\t\t\"2:\\tldr\\t%0, 7(%2)\\n\\t\"\n#endif\n#ifdef __LITTLE_ENDIAN\n\t\t\t\"1:\\tldl\\t%0, 7(%2)\\n\"\n\t\t\t\"2:\\tldr\\t%0, (%2)\\n\\t\"\n#endif\n\t\t\t\"li\\t%1, 0\\n\"\n\t\t\t\"3:\\t.section\\t.fixup,\\\"ax\\\"\\n\\t\"\n\t\t\t\"4:\\tli\\t%1, %3\\n\\t\"\n\t\t\t\"j\\t3b\\n\\t\"\n\t\t\t\".previous\\n\\t\"\n\t\t\t\".section\\t__ex_table,\\\"a\\\"\\n\\t\"\n\t\t\tSTR(PTR)\"\\t1b, 4b\\n\\t\"\n\t\t\tSTR(PTR)\"\\t2b, 4b\\n\\t\"\n\t\t\t\".previous\"\n\t\t\t: \"=&r\" (value), \"=r\" (res)\n\t\t\t: \"r\" (addr), \"i\" (-EFAULT));\n\t\tif (res)\n\t\t\tgoto fault;\n\t\tcompute_return_epc(regs);\n\t\tregs->regs[insn.i_format.rt] = value;\n\t\tbreak;\n#endif /* CONFIG_64BIT */\n\n\t\t/* Cannot handle 64-bit instructions in 32-bit kernel */\n\t\tgoto sigill;\n\n\tcase sh_op:\n\t\tif (!access_ok(VERIFY_WRITE, addr, 2))\n\t\t\tgoto sigbus;\n\n\t\tvalue = regs->regs[insn.i_format.rt];\n\t\t__asm__ __volatile__ (\n#ifdef __BIG_ENDIAN\n\t\t\t\".set\\tnoat\\n\"\n\t\t\t\"1:\\tsb\\t%1, 1(%2)\\n\\t\"\n\t\t\t\"srl\\t$1, %1, 0x8\\n\"\n\t\t\t\"2:\\tsb\\t$1, 0(%2)\\n\\t\"\n\t\t\t\".set\\tat\\n\\t\"\n#endif\n#ifdef __LITTLE_ENDIAN\n\t\t\t\".set\\tnoat\\n\"\n\t\t\t\"1:\\tsb\\t%1, 0(%2)\\n\\t\"\n\t\t\t\"srl\\t$1,%1, 0x8\\n\"\n\t\t\t\"2:\\tsb\\t$1, 1(%2)\\n\\t\"\n\t\t\t\".set\\tat\\n\\t\"\n#endif\n\t\t\t\"li\\t%0, 0\\n\"\n\t\t\t\"3:\\n\\t\"\n\t\t\t\".section\\t.fixup,\\\"ax\\\"\\n\\t\"\n\t\t\t\"4:\\tli\\t%0, %3\\n\\t\"\n\t\t\t\"j\\t3b\\n\\t\"\n\t\t\t\".previous\\n\\t\"\n\t\t\t\".section\\t__ex_table,\\\"a\\\"\\n\\t\"\n\t\t\tSTR(PTR)\"\\t1b, 4b\\n\\t\"\n\t\t\tSTR(PTR)\"\\t2b, 4b\\n\\t\"\n\t\t\t\".previous\"\n\t\t\t: \"=r\" (res)\n\t\t\t: \"r\" (value), \"r\" (addr), \"i\" (-EFAULT));\n\t\tif (res)\n\t\t\tgoto fault;\n\t\tcompute_return_epc(regs);\n\t\tbreak;\n\n\tcase sw_op:\n\t\tif (!access_ok(VERIFY_WRITE, addr, 4))\n\t\t\tgoto sigbus;\n\n\t\tvalue = regs->regs[insn.i_format.rt];\n\t\t__asm__ __volatile__ (\n#ifdef __BIG_ENDIAN\n\t\t\t\"1:\\tswl\\t%1,(%2)\\n\"\n\t\t\t\"2:\\tswr\\t%1, 3(%2)\\n\\t\"\n#endif\n#ifdef __LITTLE_ENDIAN\n\t\t\t\"1:\\tswl\\t%1, 3(%2)\\n\"\n\t\t\t\"2:\\tswr\\t%1, (%2)\\n\\t\"\n#endif\n\t\t\t\"li\\t%0, 0\\n\"\n\t\t\t\"3:\\n\\t\"\n\t\t\t\".section\\t.fixup,\\\"ax\\\"\\n\\t\"\n\t\t\t\"4:\\tli\\t%0, %3\\n\\t\"\n\t\t\t\"j\\t3b\\n\\t\"\n\t\t\t\".previous\\n\\t\"\n\t\t\t\".section\\t__ex_table,\\\"a\\\"\\n\\t\"\n\t\t\tSTR(PTR)\"\\t1b, 4b\\n\\t\"\n\t\t\tSTR(PTR)\"\\t2b, 4b\\n\\t\"\n\t\t\t\".previous\"\n\t\t: \"=r\" (res)\n\t\t: \"r\" (value), \"r\" (addr), \"i\" (-EFAULT));\n\t\tif (res)\n\t\t\tgoto fault;\n\t\tcompute_return_epc(regs);\n\t\tbreak;\n\n\tcase sd_op:\n#ifdef CONFIG_64BIT\n\t\t/*\n\t\t * A 32-bit kernel might be running on a 64-bit processor.  But\n\t\t * if we're on a 32-bit processor and an i-cache incoherency\n\t\t * or race makes us see a 64-bit instruction here the sdl/sdr\n\t\t * would blow up, so for now we don't handle unaligned 64-bit\n\t\t * instructions on 32-bit kernels.\n\t\t */\n\t\tif (!access_ok(VERIFY_WRITE, addr, 8))\n\t\t\tgoto sigbus;\n\n\t\tvalue = regs->regs[insn.i_format.rt];\n\t\t__asm__ __volatile__ (\n#ifdef __BIG_ENDIAN\n\t\t\t\"1:\\tsdl\\t%1,(%2)\\n\"\n\t\t\t\"2:\\tsdr\\t%1, 7(%2)\\n\\t\"\n#endif\n#ifdef __LITTLE_ENDIAN\n\t\t\t\"1:\\tsdl\\t%1, 7(%2)\\n\"\n\t\t\t\"2:\\tsdr\\t%1, (%2)\\n\\t\"\n#endif\n\t\t\t\"li\\t%0, 0\\n\"\n\t\t\t\"3:\\n\\t\"\n\t\t\t\".section\\t.fixup,\\\"ax\\\"\\n\\t\"\n\t\t\t\"4:\\tli\\t%0, %3\\n\\t\"\n\t\t\t\"j\\t3b\\n\\t\"\n\t\t\t\".previous\\n\\t\"\n\t\t\t\".section\\t__ex_table,\\\"a\\\"\\n\\t\"\n\t\t\tSTR(PTR)\"\\t1b, 4b\\n\\t\"\n\t\t\tSTR(PTR)\"\\t2b, 4b\\n\\t\"\n\t\t\t\".previous\"\n\t\t: \"=r\" (res)\n\t\t: \"r\" (value), \"r\" (addr), \"i\" (-EFAULT));\n\t\tif (res)\n\t\t\tgoto fault;\n\t\tcompute_return_epc(regs);\n\t\tbreak;\n#endif /* CONFIG_64BIT */\n\n\t\t/* Cannot handle 64-bit instructions in 32-bit kernel */\n\t\tgoto sigill;\n\n\tcase lwc1_op:\n\tcase ldc1_op:\n\tcase swc1_op:\n\tcase sdc1_op:\n\t\t/*\n\t\t * I herewith declare: this does not happen.  So send SIGBUS.\n\t\t */\n\t\tgoto sigbus;\n\n\t/*\n\t * COP2 is available to implementor for application specific use.\n\t * It's up to applications to register a notifier chain and do\n\t * whatever they have to do, including possible sending of signals.\n\t */\n\tcase lwc2_op:\n\t\tcu2_notifier_call_chain(CU2_LWC2_OP, regs);\n\t\tbreak;\n\n\tcase ldc2_op:\n\t\tcu2_notifier_call_chain(CU2_LDC2_OP, regs);\n\t\tbreak;\n\n\tcase swc2_op:\n\t\tcu2_notifier_call_chain(CU2_SWC2_OP, regs);\n\t\tbreak;\n\n\tcase sdc2_op:\n\t\tcu2_notifier_call_chain(CU2_SDC2_OP, regs);\n\t\tbreak;\n\n\tdefault:\n\t\t/*\n\t\t * Pheeee...  We encountered an yet unknown instruction or\n\t\t * cache coherence problem.  Die sucker, die ...\n\t\t */\n\t\tgoto sigill;\n\t}\n\n#ifdef CONFIG_DEBUG_FS\n\tunaligned_instructions++;\n#endif\n\n\treturn;\n\nfault:\n\t/* Did we have an exception handler installed? */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\tdie_if_kernel(\"Unhandled kernel unaligned access\", regs);\n\tforce_sig(SIGSEGV, current);\n\n\treturn;\n\nsigbus:\n\tdie_if_kernel(\"Unhandled kernel unaligned access\", regs);\n\tforce_sig(SIGBUS, current);\n\n\treturn;\n\nsigill:\n\tdie_if_kernel(\"Unhandled kernel unaligned access or invalid instruction\", regs);\n\tforce_sig(SIGILL, current);\n}\n\nasmlinkage void do_ade(struct pt_regs *regs)\n{\n\tunsigned int __user *pc;\n\tmm_segment_t seg;\n\n\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS,\n\t\t\t1, regs, regs->cp0_badvaddr);\n\t/*\n\t * Did we catch a fault trying to load an instruction?\n\t * Or are we running in MIPS16 mode?\n\t */\n\tif ((regs->cp0_badvaddr == regs->cp0_epc) || (regs->cp0_epc & 0x1))\n\t\tgoto sigbus;\n\n\tpc = (unsigned int __user *) exception_epc(regs);\n\tif (user_mode(regs) && !test_thread_flag(TIF_FIXADE))\n\t\tgoto sigbus;\n\tif (unaligned_action == UNALIGNED_ACTION_SIGNAL)\n\t\tgoto sigbus;\n\telse if (unaligned_action == UNALIGNED_ACTION_SHOW)\n\t\tshow_registers(regs);\n\n\t/*\n\t * Do branch emulation only if we didn't forward the exception.\n\t * This is all so but ugly ...\n\t */\n\tseg = get_fs();\n\tif (!user_mode(regs))\n\t\tset_fs(KERNEL_DS);\n\temulate_load_store_insn(regs, (void __user *)regs->cp0_badvaddr, pc);\n\tset_fs(seg);\n\n\treturn;\n\nsigbus:\n\tdie_if_kernel(\"Kernel unaligned instruction access\", regs);\n\tforce_sig(SIGBUS, current);\n\n\t/*\n\t * XXX On return from the signal handler we should advance the epc\n\t */\n}\n\n#ifdef CONFIG_DEBUG_FS\nextern struct dentry *mips_debugfs_dir;\nstatic int __init debugfs_unaligned(void)\n{\n\tstruct dentry *d;\n\n\tif (!mips_debugfs_dir)\n\t\treturn -ENODEV;\n\td = debugfs_create_u32(\"unaligned_instructions\", S_IRUGO,\n\t\t\t       mips_debugfs_dir, &unaligned_instructions);\n\tif (!d)\n\t\treturn -ENOMEM;\n\td = debugfs_create_u32(\"unaligned_action\", S_IRUGO | S_IWUSR,\n\t\t\t       mips_debugfs_dir, &unaligned_action);\n\tif (!d)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n__initcall(debugfs_unaligned);\n#endif\n", "/*\n * cp1emu.c: a MIPS coprocessor 1 (fpu) instruction emulator\n *\n * MIPS floating point support\n * Copyright (C) 1994-2000 Algorithmics Ltd.\n *\n * Kevin D. Kissell, kevink@mips.com and Carsten Langgaard, carstenl@mips.com\n * Copyright (C) 2000  MIPS Technologies, Inc.\n *\n *  This program is free software; you can distribute it and/or modify it\n *  under the terms of the GNU General Public License (Version 2) as\n *  published by the Free Software Foundation.\n *\n *  This program is distributed in the hope it will be useful, but WITHOUT\n *  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n *  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n *  for more details.\n *\n *  You should have received a copy of the GNU General Public License along\n *  with this program; if not, write to the Free Software Foundation, Inc.,\n *  59 Temple Place - Suite 330, Boston MA 02111-1307, USA.\n *\n * A complete emulator for MIPS coprocessor 1 instructions.  This is\n * required for #float(switch) or #float(trap), where it catches all\n * COP1 instructions via the \"CoProcessor Unusable\" exception.\n *\n * More surprisingly it is also required for #float(ieee), to help out\n * the hardware fpu at the boundaries of the IEEE-754 representation\n * (denormalised values, infinities, underflow, etc).  It is made\n * quite nasty because emulation of some non-COP1 instructions is\n * required, e.g. in branch delay slots.\n *\n * Note if you know that you won't have an fpu, then you'll get much\n * better performance by compiling with -msoft-float!\n */\n#include <linux/sched.h>\n#include <linux/module.h>\n#include <linux/debugfs.h>\n#include <linux/perf_event.h>\n\n#include <asm/inst.h>\n#include <asm/bootinfo.h>\n#include <asm/processor.h>\n#include <asm/ptrace.h>\n#include <asm/signal.h>\n#include <asm/mipsregs.h>\n#include <asm/fpu_emulator.h>\n#include <asm/uaccess.h>\n#include <asm/branch.h>\n\n#include \"ieee754.h\"\n\n/* Strap kernel emulator for full MIPS IV emulation */\n\n#ifdef __mips\n#undef __mips\n#endif\n#define __mips 4\n\n/* Function which emulates a floating point instruction. */\n\nstatic int fpu_emu(struct pt_regs *, struct mips_fpu_struct *,\n\tmips_instruction);\n\n#if __mips >= 4 && __mips != 32\nstatic int fpux_emu(struct pt_regs *,\n\tstruct mips_fpu_struct *, mips_instruction, void *__user *);\n#endif\n\n/* Further private data for which no space exists in mips_fpu_struct */\n\n#ifdef CONFIG_DEBUG_FS\nDEFINE_PER_CPU(struct mips_fpu_emulator_stats, fpuemustats);\n#endif\n\n/* Control registers */\n\n#define FPCREG_RID\t0\t/* $0  = revision id */\n#define FPCREG_CSR\t31\t/* $31 = csr */\n\n/* Determine rounding mode from the RM bits of the FCSR */\n#define modeindex(v) ((v) & FPU_CSR_RM)\n\n/* Convert Mips rounding mode (0..3) to IEEE library modes. */\nstatic const unsigned char ieee_rm[4] = {\n\t[FPU_CSR_RN] = IEEE754_RN,\n\t[FPU_CSR_RZ] = IEEE754_RZ,\n\t[FPU_CSR_RU] = IEEE754_RU,\n\t[FPU_CSR_RD] = IEEE754_RD,\n};\n/* Convert IEEE library modes to Mips rounding mode (0..3). */\nstatic const unsigned char mips_rm[4] = {\n\t[IEEE754_RN] = FPU_CSR_RN,\n\t[IEEE754_RZ] = FPU_CSR_RZ,\n\t[IEEE754_RD] = FPU_CSR_RD,\n\t[IEEE754_RU] = FPU_CSR_RU,\n};\n\n#if __mips >= 4\n/* convert condition code register number to csr bit */\nstatic const unsigned int fpucondbit[8] = {\n\tFPU_CSR_COND0,\n\tFPU_CSR_COND1,\n\tFPU_CSR_COND2,\n\tFPU_CSR_COND3,\n\tFPU_CSR_COND4,\n\tFPU_CSR_COND5,\n\tFPU_CSR_COND6,\n\tFPU_CSR_COND7\n};\n#endif\n\n\n/*\n * Redundant with logic already in kernel/branch.c,\n * embedded in compute_return_epc.  At some point,\n * a single subroutine should be used across both\n * modules.\n */\nstatic int isBranchInstr(mips_instruction * i)\n{\n\tswitch (MIPSInst_OPCODE(*i)) {\n\tcase spec_op:\n\t\tswitch (MIPSInst_FUNC(*i)) {\n\t\tcase jalr_op:\n\t\tcase jr_op:\n\t\t\treturn 1;\n\t\t}\n\t\tbreak;\n\n\tcase bcond_op:\n\t\tswitch (MIPSInst_RT(*i)) {\n\t\tcase bltz_op:\n\t\tcase bgez_op:\n\t\tcase bltzl_op:\n\t\tcase bgezl_op:\n\t\tcase bltzal_op:\n\t\tcase bgezal_op:\n\t\tcase bltzall_op:\n\t\tcase bgezall_op:\n\t\t\treturn 1;\n\t\t}\n\t\tbreak;\n\n\tcase j_op:\n\tcase jal_op:\n\tcase jalx_op:\n\tcase beq_op:\n\tcase bne_op:\n\tcase blez_op:\n\tcase bgtz_op:\n\tcase beql_op:\n\tcase bnel_op:\n\tcase blezl_op:\n\tcase bgtzl_op:\n\t\treturn 1;\n\n\tcase cop0_op:\n\tcase cop1_op:\n\tcase cop2_op:\n\tcase cop1x_op:\n\t\tif (MIPSInst_RS(*i) == bc_op)\n\t\t\treturn 1;\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\n/*\n * In the Linux kernel, we support selection of FPR format on the\n * basis of the Status.FR bit.  If an FPU is not present, the FR bit\n * is hardwired to zero, which would imply a 32-bit FPU even for\n * 64-bit CPUs.  For 64-bit kernels with no FPU we use TIF_32BIT_REGS\n * as a proxy for the FR bit so that a 64-bit FPU is emulated.  In any\n * case, for a 32-bit kernel which uses the O32 MIPS ABI, only the\n * even FPRs are used (Status.FR = 0).\n */\nstatic inline int cop1_64bit(struct pt_regs *xcp)\n{\n\tif (cpu_has_fpu)\n\t\treturn xcp->cp0_status & ST0_FR;\n#ifdef CONFIG_64BIT\n\treturn !test_thread_flag(TIF_32BIT_REGS);\n#else\n\treturn 0;\n#endif\n}\n\n#define SIFROMREG(si, x) ((si) = cop1_64bit(xcp) || !(x & 1) ? \\\n\t\t\t(int)ctx->fpr[x] : (int)(ctx->fpr[x & ~1] >> 32))\n\n#define SITOREG(si, x)\t(ctx->fpr[x & ~(cop1_64bit(xcp) == 0)] = \\\n\t\t\tcop1_64bit(xcp) || !(x & 1) ? \\\n\t\t\tctx->fpr[x & ~1] >> 32 << 32 | (u32)(si) : \\\n\t\t\tctx->fpr[x & ~1] << 32 >> 32 | (u64)(si) << 32)\n\n#define DIFROMREG(di, x) ((di) = ctx->fpr[x & ~(cop1_64bit(xcp) == 0)])\n#define DITOREG(di, x)\t(ctx->fpr[x & ~(cop1_64bit(xcp) == 0)] = (di))\n\n#define SPFROMREG(sp, x) SIFROMREG((sp).bits, x)\n#define SPTOREG(sp, x)\tSITOREG((sp).bits, x)\n#define DPFROMREG(dp, x)\tDIFROMREG((dp).bits, x)\n#define DPTOREG(dp, x)\tDITOREG((dp).bits, x)\n\n/*\n * Emulate the single floating point instruction pointed at by EPC.\n * Two instructions if the instruction is in a branch delay slot.\n */\n\nstatic int cop1Emulate(struct pt_regs *xcp, struct mips_fpu_struct *ctx,\n\t\t       void *__user *fault_addr)\n{\n\tmips_instruction ir;\n\tunsigned long emulpc, contpc;\n\tunsigned int cond;\n\n\tif (!access_ok(VERIFY_READ, xcp->cp0_epc, sizeof(mips_instruction))) {\n\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t*fault_addr = (mips_instruction __user *)xcp->cp0_epc;\n\t\treturn SIGBUS;\n\t}\n\tif (__get_user(ir, (mips_instruction __user *) xcp->cp0_epc)) {\n\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t*fault_addr = (mips_instruction __user *)xcp->cp0_epc;\n\t\treturn SIGSEGV;\n\t}\n\n\t/* XXX NEC Vr54xx bug workaround */\n\tif ((xcp->cp0_cause & CAUSEF_BD) && !isBranchInstr(&ir))\n\t\txcp->cp0_cause &= ~CAUSEF_BD;\n\n\tif (xcp->cp0_cause & CAUSEF_BD) {\n\t\t/*\n\t\t * The instruction to be emulated is in a branch delay slot\n\t\t * which means that we have to  emulate the branch instruction\n\t\t * BEFORE we do the cop1 instruction.\n\t\t *\n\t\t * This branch could be a COP1 branch, but in that case we\n\t\t * would have had a trap for that instruction, and would not\n\t\t * come through this route.\n\t\t *\n\t\t * Linux MIPS branch emulator operates on context, updating the\n\t\t * cp0_epc.\n\t\t */\n\t\temulpc = xcp->cp0_epc + 4;\t/* Snapshot emulation target */\n\n\t\tif (__compute_return_epc(xcp)) {\n#ifdef CP1DBG\n\t\t\tprintk(\"failed to emulate branch at %p\\n\",\n\t\t\t\t(void *) (xcp->cp0_epc));\n#endif\n\t\t\treturn SIGILL;\n\t\t}\n\t\tif (!access_ok(VERIFY_READ, emulpc, sizeof(mips_instruction))) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = (mips_instruction __user *)emulpc;\n\t\t\treturn SIGBUS;\n\t\t}\n\t\tif (__get_user(ir, (mips_instruction __user *) emulpc)) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = (mips_instruction __user *)emulpc;\n\t\t\treturn SIGSEGV;\n\t\t}\n\t\t/* __compute_return_epc() will have updated cp0_epc */\n\t\tcontpc = xcp->cp0_epc;\n\t\t/* In order not to confuse ptrace() et al, tweak context */\n\t\txcp->cp0_epc = emulpc - 4;\n\t} else {\n\t\temulpc = xcp->cp0_epc;\n\t\tcontpc = xcp->cp0_epc + 4;\n\t}\n\n      emul:\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, xcp, 0);\n\tMIPS_FPU_EMU_INC_STATS(emulated);\n\tswitch (MIPSInst_OPCODE(ir)) {\n\tcase ldc1_op:{\n\t\tu64 __user *va = (u64 __user *) (xcp->regs[MIPSInst_RS(ir)] +\n\t\t\tMIPSInst_SIMM(ir));\n\t\tu64 val;\n\n\t\tMIPS_FPU_EMU_INC_STATS(loads);\n\n\t\tif (!access_ok(VERIFY_READ, va, sizeof(u64))) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = va;\n\t\t\treturn SIGBUS;\n\t\t}\n\t\tif (__get_user(val, va)) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = va;\n\t\t\treturn SIGSEGV;\n\t\t}\n\t\tDITOREG(val, MIPSInst_RT(ir));\n\t\tbreak;\n\t}\n\n\tcase sdc1_op:{\n\t\tu64 __user *va = (u64 __user *) (xcp->regs[MIPSInst_RS(ir)] +\n\t\t\tMIPSInst_SIMM(ir));\n\t\tu64 val;\n\n\t\tMIPS_FPU_EMU_INC_STATS(stores);\n\t\tDIFROMREG(val, MIPSInst_RT(ir));\n\t\tif (!access_ok(VERIFY_WRITE, va, sizeof(u64))) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = va;\n\t\t\treturn SIGBUS;\n\t\t}\n\t\tif (__put_user(val, va)) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = va;\n\t\t\treturn SIGSEGV;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase lwc1_op:{\n\t\tu32 __user *va = (u32 __user *) (xcp->regs[MIPSInst_RS(ir)] +\n\t\t\tMIPSInst_SIMM(ir));\n\t\tu32 val;\n\n\t\tMIPS_FPU_EMU_INC_STATS(loads);\n\t\tif (!access_ok(VERIFY_READ, va, sizeof(u32))) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = va;\n\t\t\treturn SIGBUS;\n\t\t}\n\t\tif (__get_user(val, va)) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = va;\n\t\t\treturn SIGSEGV;\n\t\t}\n\t\tSITOREG(val, MIPSInst_RT(ir));\n\t\tbreak;\n\t}\n\n\tcase swc1_op:{\n\t\tu32 __user *va = (u32 __user *) (xcp->regs[MIPSInst_RS(ir)] +\n\t\t\tMIPSInst_SIMM(ir));\n\t\tu32 val;\n\n\t\tMIPS_FPU_EMU_INC_STATS(stores);\n\t\tSIFROMREG(val, MIPSInst_RT(ir));\n\t\tif (!access_ok(VERIFY_WRITE, va, sizeof(u32))) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = va;\n\t\t\treturn SIGBUS;\n\t\t}\n\t\tif (__put_user(val, va)) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = va;\n\t\t\treturn SIGSEGV;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase cop1_op:\n\t\tswitch (MIPSInst_RS(ir)) {\n\n#if defined(__mips64)\n\t\tcase dmfc_op:\n\t\t\t/* copregister fs -> gpr[rt] */\n\t\t\tif (MIPSInst_RT(ir) != 0) {\n\t\t\t\tDIFROMREG(xcp->regs[MIPSInst_RT(ir)],\n\t\t\t\t\tMIPSInst_RD(ir));\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase dmtc_op:\n\t\t\t/* copregister fs <- rt */\n\t\t\tDITOREG(xcp->regs[MIPSInst_RT(ir)], MIPSInst_RD(ir));\n\t\t\tbreak;\n#endif\n\n\t\tcase mfc_op:\n\t\t\t/* copregister rd -> gpr[rt] */\n\t\t\tif (MIPSInst_RT(ir) != 0) {\n\t\t\t\tSIFROMREG(xcp->regs[MIPSInst_RT(ir)],\n\t\t\t\t\tMIPSInst_RD(ir));\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase mtc_op:\n\t\t\t/* copregister rd <- rt */\n\t\t\tSITOREG(xcp->regs[MIPSInst_RT(ir)], MIPSInst_RD(ir));\n\t\t\tbreak;\n\n\t\tcase cfc_op:{\n\t\t\t/* cop control register rd -> gpr[rt] */\n\t\t\tu32 value;\n\n\t\t\tif (MIPSInst_RD(ir) == FPCREG_CSR) {\n\t\t\t\tvalue = ctx->fcr31;\n\t\t\t\tvalue = (value & ~FPU_CSR_RM) |\n\t\t\t\t\tmips_rm[modeindex(value)];\n#ifdef CSRTRACE\n\t\t\t\tprintk(\"%p gpr[%d]<-csr=%08x\\n\",\n\t\t\t\t\t(void *) (xcp->cp0_epc),\n\t\t\t\t\tMIPSInst_RT(ir), value);\n#endif\n\t\t\t}\n\t\t\telse if (MIPSInst_RD(ir) == FPCREG_RID)\n\t\t\t\tvalue = 0;\n\t\t\telse\n\t\t\t\tvalue = 0;\n\t\t\tif (MIPSInst_RT(ir))\n\t\t\t\txcp->regs[MIPSInst_RT(ir)] = value;\n\t\t\tbreak;\n\t\t}\n\n\t\tcase ctc_op:{\n\t\t\t/* copregister rd <- rt */\n\t\t\tu32 value;\n\n\t\t\tif (MIPSInst_RT(ir) == 0)\n\t\t\t\tvalue = 0;\n\t\t\telse\n\t\t\t\tvalue = xcp->regs[MIPSInst_RT(ir)];\n\n\t\t\t/* we only have one writable control reg\n\t\t\t */\n\t\t\tif (MIPSInst_RD(ir) == FPCREG_CSR) {\n#ifdef CSRTRACE\n\t\t\t\tprintk(\"%p gpr[%d]->csr=%08x\\n\",\n\t\t\t\t\t(void *) (xcp->cp0_epc),\n\t\t\t\t\tMIPSInst_RT(ir), value);\n#endif\n\n\t\t\t\t/*\n\t\t\t\t * Don't write reserved bits,\n\t\t\t\t * and convert to ieee library modes\n\t\t\t\t */\n\t\t\t\tctx->fcr31 = (value &\n\t\t\t\t\t\t~(FPU_CSR_RSVD | FPU_CSR_RM)) |\n\t\t\t\t\t\tieee_rm[modeindex(value)];\n\t\t\t}\n\t\t\tif ((ctx->fcr31 >> 5) & ctx->fcr31 & FPU_CSR_ALL_E) {\n\t\t\t\treturn SIGFPE;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tcase bc_op:{\n\t\t\tint likely = 0;\n\n\t\t\tif (xcp->cp0_cause & CAUSEF_BD)\n\t\t\t\treturn SIGILL;\n\n#if __mips >= 4\n\t\t\tcond = ctx->fcr31 & fpucondbit[MIPSInst_RT(ir) >> 2];\n#else\n\t\t\tcond = ctx->fcr31 & FPU_CSR_COND;\n#endif\n\t\t\tswitch (MIPSInst_RT(ir) & 3) {\n\t\t\tcase bcfl_op:\n\t\t\t\tlikely = 1;\n\t\t\tcase bcf_op:\n\t\t\t\tcond = !cond;\n\t\t\t\tbreak;\n\t\t\tcase bctl_op:\n\t\t\t\tlikely = 1;\n\t\t\tcase bct_op:\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\t/* thats an illegal instruction */\n\t\t\t\treturn SIGILL;\n\t\t\t}\n\n\t\t\txcp->cp0_cause |= CAUSEF_BD;\n\t\t\tif (cond) {\n\t\t\t\t/* branch taken: emulate dslot\n\t\t\t\t * instruction\n\t\t\t\t */\n\t\t\t\txcp->cp0_epc += 4;\n\t\t\t\tcontpc = (xcp->cp0_epc +\n\t\t\t\t\t(MIPSInst_SIMM(ir) << 2));\n\n\t\t\t\tif (!access_ok(VERIFY_READ, xcp->cp0_epc,\n\t\t\t\t\t       sizeof(mips_instruction))) {\n\t\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t\t*fault_addr = (mips_instruction __user *)xcp->cp0_epc;\n\t\t\t\t\treturn SIGBUS;\n\t\t\t\t}\n\t\t\t\tif (__get_user(ir,\n\t\t\t\t    (mips_instruction __user *) xcp->cp0_epc)) {\n\t\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t\t*fault_addr = (mips_instruction __user *)xcp->cp0_epc;\n\t\t\t\t\treturn SIGSEGV;\n\t\t\t\t}\n\n\t\t\t\tswitch (MIPSInst_OPCODE(ir)) {\n\t\t\t\tcase lwc1_op:\n\t\t\t\tcase swc1_op:\n#if (__mips >= 2 || defined(__mips64))\n\t\t\t\tcase ldc1_op:\n\t\t\t\tcase sdc1_op:\n#endif\n\t\t\t\tcase cop1_op:\n#if __mips >= 4 && __mips != 32\n\t\t\t\tcase cop1x_op:\n#endif\n\t\t\t\t\t/* its one of ours */\n\t\t\t\t\tgoto emul;\n#if __mips >= 4\n\t\t\t\tcase spec_op:\n\t\t\t\t\tif (MIPSInst_FUNC(ir) == movc_op)\n\t\t\t\t\t\tgoto emul;\n\t\t\t\t\tbreak;\n#endif\n\t\t\t\t}\n\n\t\t\t\t/*\n\t\t\t\t * Single step the non-cp1\n\t\t\t\t * instruction in the dslot\n\t\t\t\t */\n\t\t\t\treturn mips_dsemul(xcp, ir, contpc);\n\t\t\t}\n\t\t\telse {\n\t\t\t\t/* branch not taken */\n\t\t\t\tif (likely) {\n\t\t\t\t\t/*\n\t\t\t\t\t * branch likely nullifies\n\t\t\t\t\t * dslot if not taken\n\t\t\t\t\t */\n\t\t\t\t\txcp->cp0_epc += 4;\n\t\t\t\t\tcontpc += 4;\n\t\t\t\t\t/*\n\t\t\t\t\t * else continue & execute\n\t\t\t\t\t * dslot as normal insn\n\t\t\t\t\t */\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tdefault:\n\t\t\tif (!(MIPSInst_RS(ir) & 0x10))\n\t\t\t\treturn SIGILL;\n\t\t\t{\n\t\t\t\tint sig;\n\n\t\t\t\t/* a real fpu computation instruction */\n\t\t\t\tif ((sig = fpu_emu(xcp, ctx, ir)))\n\t\t\t\t\treturn sig;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n#if __mips >= 4 && __mips != 32\n\tcase cop1x_op:{\n\t\tint sig = fpux_emu(xcp, ctx, ir, fault_addr);\n\t\tif (sig)\n\t\t\treturn sig;\n\t\tbreak;\n\t}\n#endif\n\n#if __mips >= 4\n\tcase spec_op:\n\t\tif (MIPSInst_FUNC(ir) != movc_op)\n\t\t\treturn SIGILL;\n\t\tcond = fpucondbit[MIPSInst_RT(ir) >> 2];\n\t\tif (((ctx->fcr31 & cond) != 0) == ((MIPSInst_RT(ir) & 1) != 0))\n\t\t\txcp->regs[MIPSInst_RD(ir)] =\n\t\t\t\txcp->regs[MIPSInst_RS(ir)];\n\t\tbreak;\n#endif\n\n\tdefault:\n\t\treturn SIGILL;\n\t}\n\n\t/* we did it !! */\n\txcp->cp0_epc = contpc;\n\txcp->cp0_cause &= ~CAUSEF_BD;\n\n\treturn 0;\n}\n\n/*\n * Conversion table from MIPS compare ops 48-63\n * cond = ieee754dp_cmp(x,y,IEEE754_UN,sig);\n */\nstatic const unsigned char cmptab[8] = {\n\t0,\t\t\t/* cmp_0 (sig) cmp_sf */\n\tIEEE754_CUN,\t\t/* cmp_un (sig) cmp_ngle */\n\tIEEE754_CEQ,\t\t/* cmp_eq (sig) cmp_seq */\n\tIEEE754_CEQ | IEEE754_CUN,\t/* cmp_ueq (sig) cmp_ngl  */\n\tIEEE754_CLT,\t\t/* cmp_olt (sig) cmp_lt */\n\tIEEE754_CLT | IEEE754_CUN,\t/* cmp_ult (sig) cmp_nge */\n\tIEEE754_CLT | IEEE754_CEQ,\t/* cmp_ole (sig) cmp_le */\n\tIEEE754_CLT | IEEE754_CEQ | IEEE754_CUN,\t/* cmp_ule (sig) cmp_ngt */\n};\n\n\n#if __mips >= 4 && __mips != 32\n\n/*\n * Additional MIPS4 instructions\n */\n\n#define DEF3OP(name, p, f1, f2, f3) \\\nstatic ieee754##p fpemu_##p##_##name(ieee754##p r, ieee754##p s, \\\n    ieee754##p t) \\\n{ \\\n\tstruct _ieee754_csr ieee754_csr_save; \\\n\ts = f1(s, t); \\\n\tieee754_csr_save = ieee754_csr; \\\n\ts = f2(s, r); \\\n\tieee754_csr_save.cx |= ieee754_csr.cx; \\\n\tieee754_csr_save.sx |= ieee754_csr.sx; \\\n\ts = f3(s); \\\n\tieee754_csr.cx |= ieee754_csr_save.cx; \\\n\tieee754_csr.sx |= ieee754_csr_save.sx; \\\n\treturn s; \\\n}\n\nstatic ieee754dp fpemu_dp_recip(ieee754dp d)\n{\n\treturn ieee754dp_div(ieee754dp_one(0), d);\n}\n\nstatic ieee754dp fpemu_dp_rsqrt(ieee754dp d)\n{\n\treturn ieee754dp_div(ieee754dp_one(0), ieee754dp_sqrt(d));\n}\n\nstatic ieee754sp fpemu_sp_recip(ieee754sp s)\n{\n\treturn ieee754sp_div(ieee754sp_one(0), s);\n}\n\nstatic ieee754sp fpemu_sp_rsqrt(ieee754sp s)\n{\n\treturn ieee754sp_div(ieee754sp_one(0), ieee754sp_sqrt(s));\n}\n\nDEF3OP(madd, sp, ieee754sp_mul, ieee754sp_add, );\nDEF3OP(msub, sp, ieee754sp_mul, ieee754sp_sub, );\nDEF3OP(nmadd, sp, ieee754sp_mul, ieee754sp_add, ieee754sp_neg);\nDEF3OP(nmsub, sp, ieee754sp_mul, ieee754sp_sub, ieee754sp_neg);\nDEF3OP(madd, dp, ieee754dp_mul, ieee754dp_add, );\nDEF3OP(msub, dp, ieee754dp_mul, ieee754dp_sub, );\nDEF3OP(nmadd, dp, ieee754dp_mul, ieee754dp_add, ieee754dp_neg);\nDEF3OP(nmsub, dp, ieee754dp_mul, ieee754dp_sub, ieee754dp_neg);\n\nstatic int fpux_emu(struct pt_regs *xcp, struct mips_fpu_struct *ctx,\n\tmips_instruction ir, void *__user *fault_addr)\n{\n\tunsigned rcsr = 0;\t/* resulting csr */\n\n\tMIPS_FPU_EMU_INC_STATS(cp1xops);\n\n\tswitch (MIPSInst_FMA_FFMT(ir)) {\n\tcase s_fmt:{\t\t/* 0 */\n\n\t\tieee754sp(*handler) (ieee754sp, ieee754sp, ieee754sp);\n\t\tieee754sp fd, fr, fs, ft;\n\t\tu32 __user *va;\n\t\tu32 val;\n\n\t\tswitch (MIPSInst_FUNC(ir)) {\n\t\tcase lwxc1_op:\n\t\t\tva = (void __user *) (xcp->regs[MIPSInst_FR(ir)] +\n\t\t\t\txcp->regs[MIPSInst_FT(ir)]);\n\n\t\t\tMIPS_FPU_EMU_INC_STATS(loads);\n\t\t\tif (!access_ok(VERIFY_READ, va, sizeof(u32))) {\n\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t*fault_addr = va;\n\t\t\t\treturn SIGBUS;\n\t\t\t}\n\t\t\tif (__get_user(val, va)) {\n\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t*fault_addr = va;\n\t\t\t\treturn SIGSEGV;\n\t\t\t}\n\t\t\tSITOREG(val, MIPSInst_FD(ir));\n\t\t\tbreak;\n\n\t\tcase swxc1_op:\n\t\t\tva = (void __user *) (xcp->regs[MIPSInst_FR(ir)] +\n\t\t\t\txcp->regs[MIPSInst_FT(ir)]);\n\n\t\t\tMIPS_FPU_EMU_INC_STATS(stores);\n\n\t\t\tSIFROMREG(val, MIPSInst_FS(ir));\n\t\t\tif (!access_ok(VERIFY_WRITE, va, sizeof(u32))) {\n\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t*fault_addr = va;\n\t\t\t\treturn SIGBUS;\n\t\t\t}\n\t\t\tif (put_user(val, va)) {\n\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t*fault_addr = va;\n\t\t\t\treturn SIGSEGV;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase madd_s_op:\n\t\t\thandler = fpemu_sp_madd;\n\t\t\tgoto scoptop;\n\t\tcase msub_s_op:\n\t\t\thandler = fpemu_sp_msub;\n\t\t\tgoto scoptop;\n\t\tcase nmadd_s_op:\n\t\t\thandler = fpemu_sp_nmadd;\n\t\t\tgoto scoptop;\n\t\tcase nmsub_s_op:\n\t\t\thandler = fpemu_sp_nmsub;\n\t\t\tgoto scoptop;\n\n\t\t      scoptop:\n\t\t\tSPFROMREG(fr, MIPSInst_FR(ir));\n\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\tSPFROMREG(ft, MIPSInst_FT(ir));\n\t\t\tfd = (*handler) (fr, fs, ft);\n\t\t\tSPTOREG(fd, MIPSInst_FD(ir));\n\n\t\t      copcsr:\n\t\t\tif (ieee754_cxtest(IEEE754_INEXACT))\n\t\t\t\trcsr |= FPU_CSR_INE_X | FPU_CSR_INE_S;\n\t\t\tif (ieee754_cxtest(IEEE754_UNDERFLOW))\n\t\t\t\trcsr |= FPU_CSR_UDF_X | FPU_CSR_UDF_S;\n\t\t\tif (ieee754_cxtest(IEEE754_OVERFLOW))\n\t\t\t\trcsr |= FPU_CSR_OVF_X | FPU_CSR_OVF_S;\n\t\t\tif (ieee754_cxtest(IEEE754_INVALID_OPERATION))\n\t\t\t\trcsr |= FPU_CSR_INV_X | FPU_CSR_INV_S;\n\n\t\t\tctx->fcr31 = (ctx->fcr31 & ~FPU_CSR_ALL_X) | rcsr;\n\t\t\tif ((ctx->fcr31 >> 5) & ctx->fcr31 & FPU_CSR_ALL_E) {\n\t\t\t\t/*printk (\"SIGFPE: fpu csr = %08x\\n\",\n\t\t\t\t   ctx->fcr31); */\n\t\t\t\treturn SIGFPE;\n\t\t\t}\n\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\treturn SIGILL;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase d_fmt:{\t\t/* 1 */\n\t\tieee754dp(*handler) (ieee754dp, ieee754dp, ieee754dp);\n\t\tieee754dp fd, fr, fs, ft;\n\t\tu64 __user *va;\n\t\tu64 val;\n\n\t\tswitch (MIPSInst_FUNC(ir)) {\n\t\tcase ldxc1_op:\n\t\t\tva = (void __user *) (xcp->regs[MIPSInst_FR(ir)] +\n\t\t\t\txcp->regs[MIPSInst_FT(ir)]);\n\n\t\t\tMIPS_FPU_EMU_INC_STATS(loads);\n\t\t\tif (!access_ok(VERIFY_READ, va, sizeof(u64))) {\n\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t*fault_addr = va;\n\t\t\t\treturn SIGBUS;\n\t\t\t}\n\t\t\tif (__get_user(val, va)) {\n\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t*fault_addr = va;\n\t\t\t\treturn SIGSEGV;\n\t\t\t}\n\t\t\tDITOREG(val, MIPSInst_FD(ir));\n\t\t\tbreak;\n\n\t\tcase sdxc1_op:\n\t\t\tva = (void __user *) (xcp->regs[MIPSInst_FR(ir)] +\n\t\t\t\txcp->regs[MIPSInst_FT(ir)]);\n\n\t\t\tMIPS_FPU_EMU_INC_STATS(stores);\n\t\t\tDIFROMREG(val, MIPSInst_FS(ir));\n\t\t\tif (!access_ok(VERIFY_WRITE, va, sizeof(u64))) {\n\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t*fault_addr = va;\n\t\t\t\treturn SIGBUS;\n\t\t\t}\n\t\t\tif (__put_user(val, va)) {\n\t\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t\t*fault_addr = va;\n\t\t\t\treturn SIGSEGV;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase madd_d_op:\n\t\t\thandler = fpemu_dp_madd;\n\t\t\tgoto dcoptop;\n\t\tcase msub_d_op:\n\t\t\thandler = fpemu_dp_msub;\n\t\t\tgoto dcoptop;\n\t\tcase nmadd_d_op:\n\t\t\thandler = fpemu_dp_nmadd;\n\t\t\tgoto dcoptop;\n\t\tcase nmsub_d_op:\n\t\t\thandler = fpemu_dp_nmsub;\n\t\t\tgoto dcoptop;\n\n\t\t      dcoptop:\n\t\t\tDPFROMREG(fr, MIPSInst_FR(ir));\n\t\t\tDPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\tDPFROMREG(ft, MIPSInst_FT(ir));\n\t\t\tfd = (*handler) (fr, fs, ft);\n\t\t\tDPTOREG(fd, MIPSInst_FD(ir));\n\t\t\tgoto copcsr;\n\n\t\tdefault:\n\t\t\treturn SIGILL;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase 0x7:\t\t/* 7 */\n\t\tif (MIPSInst_FUNC(ir) != pfetch_op) {\n\t\t\treturn SIGILL;\n\t\t}\n\t\t/* ignore prefx operation */\n\t\tbreak;\n\n\tdefault:\n\t\treturn SIGILL;\n\t}\n\n\treturn 0;\n}\n#endif\n\n\n\n/*\n * Emulate a single COP1 arithmetic instruction.\n */\nstatic int fpu_emu(struct pt_regs *xcp, struct mips_fpu_struct *ctx,\n\tmips_instruction ir)\n{\n\tint rfmt;\t\t/* resulting format */\n\tunsigned rcsr = 0;\t/* resulting csr */\n\tunsigned cond;\n\tunion {\n\t\tieee754dp d;\n\t\tieee754sp s;\n\t\tint w;\n#ifdef __mips64\n\t\ts64 l;\n#endif\n\t} rv;\t\t\t/* resulting value */\n\n\tMIPS_FPU_EMU_INC_STATS(cp1ops);\n\tswitch (rfmt = (MIPSInst_FFMT(ir) & 0xf)) {\n\tcase s_fmt:{\t\t/* 0 */\n\t\tunion {\n\t\t\tieee754sp(*b) (ieee754sp, ieee754sp);\n\t\t\tieee754sp(*u) (ieee754sp);\n\t\t} handler;\n\n\t\tswitch (MIPSInst_FUNC(ir)) {\n\t\t\t/* binary ops */\n\t\tcase fadd_op:\n\t\t\thandler.b = ieee754sp_add;\n\t\t\tgoto scopbop;\n\t\tcase fsub_op:\n\t\t\thandler.b = ieee754sp_sub;\n\t\t\tgoto scopbop;\n\t\tcase fmul_op:\n\t\t\thandler.b = ieee754sp_mul;\n\t\t\tgoto scopbop;\n\t\tcase fdiv_op:\n\t\t\thandler.b = ieee754sp_div;\n\t\t\tgoto scopbop;\n\n\t\t\t/* unary  ops */\n#if __mips >= 2 || defined(__mips64)\n\t\tcase fsqrt_op:\n\t\t\thandler.u = ieee754sp_sqrt;\n\t\t\tgoto scopuop;\n#endif\n#if __mips >= 4 && __mips != 32\n\t\tcase frsqrt_op:\n\t\t\thandler.u = fpemu_sp_rsqrt;\n\t\t\tgoto scopuop;\n\t\tcase frecip_op:\n\t\t\thandler.u = fpemu_sp_recip;\n\t\t\tgoto scopuop;\n#endif\n#if __mips >= 4\n\t\tcase fmovc_op:\n\t\t\tcond = fpucondbit[MIPSInst_FT(ir) >> 2];\n\t\t\tif (((ctx->fcr31 & cond) != 0) !=\n\t\t\t\t((MIPSInst_FT(ir) & 1) != 0))\n\t\t\t\treturn 0;\n\t\t\tSPFROMREG(rv.s, MIPSInst_FS(ir));\n\t\t\tbreak;\n\t\tcase fmovz_op:\n\t\t\tif (xcp->regs[MIPSInst_FT(ir)] != 0)\n\t\t\t\treturn 0;\n\t\t\tSPFROMREG(rv.s, MIPSInst_FS(ir));\n\t\t\tbreak;\n\t\tcase fmovn_op:\n\t\t\tif (xcp->regs[MIPSInst_FT(ir)] == 0)\n\t\t\t\treturn 0;\n\t\t\tSPFROMREG(rv.s, MIPSInst_FS(ir));\n\t\t\tbreak;\n#endif\n\t\tcase fabs_op:\n\t\t\thandler.u = ieee754sp_abs;\n\t\t\tgoto scopuop;\n\t\tcase fneg_op:\n\t\t\thandler.u = ieee754sp_neg;\n\t\t\tgoto scopuop;\n\t\tcase fmov_op:\n\t\t\t/* an easy one */\n\t\t\tSPFROMREG(rv.s, MIPSInst_FS(ir));\n\t\t\tgoto copcsr;\n\n\t\t\t/* binary op on handler */\n\t\t      scopbop:\n\t\t\t{\n\t\t\t\tieee754sp fs, ft;\n\n\t\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\t\tSPFROMREG(ft, MIPSInst_FT(ir));\n\n\t\t\t\trv.s = (*handler.b) (fs, ft);\n\t\t\t\tgoto copcsr;\n\t\t\t}\n\t\t      scopuop:\n\t\t\t{\n\t\t\t\tieee754sp fs;\n\n\t\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\t\trv.s = (*handler.u) (fs);\n\t\t\t\tgoto copcsr;\n\t\t\t}\n\t\t      copcsr:\n\t\t\tif (ieee754_cxtest(IEEE754_INEXACT))\n\t\t\t\trcsr |= FPU_CSR_INE_X | FPU_CSR_INE_S;\n\t\t\tif (ieee754_cxtest(IEEE754_UNDERFLOW))\n\t\t\t\trcsr |= FPU_CSR_UDF_X | FPU_CSR_UDF_S;\n\t\t\tif (ieee754_cxtest(IEEE754_OVERFLOW))\n\t\t\t\trcsr |= FPU_CSR_OVF_X | FPU_CSR_OVF_S;\n\t\t\tif (ieee754_cxtest(IEEE754_ZERO_DIVIDE))\n\t\t\t\trcsr |= FPU_CSR_DIV_X | FPU_CSR_DIV_S;\n\t\t\tif (ieee754_cxtest(IEEE754_INVALID_OPERATION))\n\t\t\t\trcsr |= FPU_CSR_INV_X | FPU_CSR_INV_S;\n\t\t\tbreak;\n\n\t\t\t/* unary conv ops */\n\t\tcase fcvts_op:\n\t\t\treturn SIGILL;\t/* not defined */\n\t\tcase fcvtd_op:{\n\t\t\tieee754sp fs;\n\n\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\trv.d = ieee754dp_fsp(fs);\n\t\t\trfmt = d_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n\t\tcase fcvtw_op:{\n\t\t\tieee754sp fs;\n\n\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\trv.w = ieee754sp_tint(fs);\n\t\t\trfmt = w_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n\n#if __mips >= 2 || defined(__mips64)\n\t\tcase fround_op:\n\t\tcase ftrunc_op:\n\t\tcase fceil_op:\n\t\tcase ffloor_op:{\n\t\t\tunsigned int oldrm = ieee754_csr.rm;\n\t\t\tieee754sp fs;\n\n\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\tieee754_csr.rm = ieee_rm[modeindex(MIPSInst_FUNC(ir))];\n\t\t\trv.w = ieee754sp_tint(fs);\n\t\t\tieee754_csr.rm = oldrm;\n\t\t\trfmt = w_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n#endif /* __mips >= 2 */\n\n#if defined(__mips64)\n\t\tcase fcvtl_op:{\n\t\t\tieee754sp fs;\n\n\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\trv.l = ieee754sp_tlong(fs);\n\t\t\trfmt = l_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n\n\t\tcase froundl_op:\n\t\tcase ftruncl_op:\n\t\tcase fceill_op:\n\t\tcase ffloorl_op:{\n\t\t\tunsigned int oldrm = ieee754_csr.rm;\n\t\t\tieee754sp fs;\n\n\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\tieee754_csr.rm = ieee_rm[modeindex(MIPSInst_FUNC(ir))];\n\t\t\trv.l = ieee754sp_tlong(fs);\n\t\t\tieee754_csr.rm = oldrm;\n\t\t\trfmt = l_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n#endif /* defined(__mips64) */\n\n\t\tdefault:\n\t\t\tif (MIPSInst_FUNC(ir) >= fcmp_op) {\n\t\t\t\tunsigned cmpop = MIPSInst_FUNC(ir) - fcmp_op;\n\t\t\t\tieee754sp fs, ft;\n\n\t\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\t\tSPFROMREG(ft, MIPSInst_FT(ir));\n\t\t\t\trv.w = ieee754sp_cmp(fs, ft,\n\t\t\t\t\tcmptab[cmpop & 0x7], cmpop & 0x8);\n\t\t\t\trfmt = -1;\n\t\t\t\tif ((cmpop & 0x8) && ieee754_cxtest\n\t\t\t\t\t(IEEE754_INVALID_OPERATION))\n\t\t\t\t\trcsr = FPU_CSR_INV_X | FPU_CSR_INV_S;\n\t\t\t\telse\n\t\t\t\t\tgoto copcsr;\n\n\t\t\t}\n\t\t\telse {\n\t\t\t\treturn SIGILL;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase d_fmt:{\n\t\tunion {\n\t\t\tieee754dp(*b) (ieee754dp, ieee754dp);\n\t\t\tieee754dp(*u) (ieee754dp);\n\t\t} handler;\n\n\t\tswitch (MIPSInst_FUNC(ir)) {\n\t\t\t/* binary ops */\n\t\tcase fadd_op:\n\t\t\thandler.b = ieee754dp_add;\n\t\t\tgoto dcopbop;\n\t\tcase fsub_op:\n\t\t\thandler.b = ieee754dp_sub;\n\t\t\tgoto dcopbop;\n\t\tcase fmul_op:\n\t\t\thandler.b = ieee754dp_mul;\n\t\t\tgoto dcopbop;\n\t\tcase fdiv_op:\n\t\t\thandler.b = ieee754dp_div;\n\t\t\tgoto dcopbop;\n\n\t\t\t/* unary  ops */\n#if __mips >= 2 || defined(__mips64)\n\t\tcase fsqrt_op:\n\t\t\thandler.u = ieee754dp_sqrt;\n\t\t\tgoto dcopuop;\n#endif\n#if __mips >= 4 && __mips != 32\n\t\tcase frsqrt_op:\n\t\t\thandler.u = fpemu_dp_rsqrt;\n\t\t\tgoto dcopuop;\n\t\tcase frecip_op:\n\t\t\thandler.u = fpemu_dp_recip;\n\t\t\tgoto dcopuop;\n#endif\n#if __mips >= 4\n\t\tcase fmovc_op:\n\t\t\tcond = fpucondbit[MIPSInst_FT(ir) >> 2];\n\t\t\tif (((ctx->fcr31 & cond) != 0) !=\n\t\t\t\t((MIPSInst_FT(ir) & 1) != 0))\n\t\t\t\treturn 0;\n\t\t\tDPFROMREG(rv.d, MIPSInst_FS(ir));\n\t\t\tbreak;\n\t\tcase fmovz_op:\n\t\t\tif (xcp->regs[MIPSInst_FT(ir)] != 0)\n\t\t\t\treturn 0;\n\t\t\tDPFROMREG(rv.d, MIPSInst_FS(ir));\n\t\t\tbreak;\n\t\tcase fmovn_op:\n\t\t\tif (xcp->regs[MIPSInst_FT(ir)] == 0)\n\t\t\t\treturn 0;\n\t\t\tDPFROMREG(rv.d, MIPSInst_FS(ir));\n\t\t\tbreak;\n#endif\n\t\tcase fabs_op:\n\t\t\thandler.u = ieee754dp_abs;\n\t\t\tgoto dcopuop;\n\n\t\tcase fneg_op:\n\t\t\thandler.u = ieee754dp_neg;\n\t\t\tgoto dcopuop;\n\n\t\tcase fmov_op:\n\t\t\t/* an easy one */\n\t\t\tDPFROMREG(rv.d, MIPSInst_FS(ir));\n\t\t\tgoto copcsr;\n\n\t\t\t/* binary op on handler */\n\t\t      dcopbop:{\n\t\t\t\tieee754dp fs, ft;\n\n\t\t\t\tDPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\t\tDPFROMREG(ft, MIPSInst_FT(ir));\n\n\t\t\t\trv.d = (*handler.b) (fs, ft);\n\t\t\t\tgoto copcsr;\n\t\t\t}\n\t\t      dcopuop:{\n\t\t\t\tieee754dp fs;\n\n\t\t\t\tDPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\t\trv.d = (*handler.u) (fs);\n\t\t\t\tgoto copcsr;\n\t\t\t}\n\n\t\t\t/* unary conv ops */\n\t\tcase fcvts_op:{\n\t\t\tieee754dp fs;\n\n\t\t\tDPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\trv.s = ieee754sp_fdp(fs);\n\t\t\trfmt = s_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n\t\tcase fcvtd_op:\n\t\t\treturn SIGILL;\t/* not defined */\n\n\t\tcase fcvtw_op:{\n\t\t\tieee754dp fs;\n\n\t\t\tDPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\trv.w = ieee754dp_tint(fs);\t/* wrong */\n\t\t\trfmt = w_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n\n#if __mips >= 2 || defined(__mips64)\n\t\tcase fround_op:\n\t\tcase ftrunc_op:\n\t\tcase fceil_op:\n\t\tcase ffloor_op:{\n\t\t\tunsigned int oldrm = ieee754_csr.rm;\n\t\t\tieee754dp fs;\n\n\t\t\tDPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\tieee754_csr.rm = ieee_rm[modeindex(MIPSInst_FUNC(ir))];\n\t\t\trv.w = ieee754dp_tint(fs);\n\t\t\tieee754_csr.rm = oldrm;\n\t\t\trfmt = w_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n#endif\n\n#if defined(__mips64)\n\t\tcase fcvtl_op:{\n\t\t\tieee754dp fs;\n\n\t\t\tDPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\trv.l = ieee754dp_tlong(fs);\n\t\t\trfmt = l_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n\n\t\tcase froundl_op:\n\t\tcase ftruncl_op:\n\t\tcase fceill_op:\n\t\tcase ffloorl_op:{\n\t\t\tunsigned int oldrm = ieee754_csr.rm;\n\t\t\tieee754dp fs;\n\n\t\t\tDPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\tieee754_csr.rm = ieee_rm[modeindex(MIPSInst_FUNC(ir))];\n\t\t\trv.l = ieee754dp_tlong(fs);\n\t\t\tieee754_csr.rm = oldrm;\n\t\t\trfmt = l_fmt;\n\t\t\tgoto copcsr;\n\t\t}\n#endif /* __mips >= 3 */\n\n\t\tdefault:\n\t\t\tif (MIPSInst_FUNC(ir) >= fcmp_op) {\n\t\t\t\tunsigned cmpop = MIPSInst_FUNC(ir) - fcmp_op;\n\t\t\t\tieee754dp fs, ft;\n\n\t\t\t\tDPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\t\tDPFROMREG(ft, MIPSInst_FT(ir));\n\t\t\t\trv.w = ieee754dp_cmp(fs, ft,\n\t\t\t\t\tcmptab[cmpop & 0x7], cmpop & 0x8);\n\t\t\t\trfmt = -1;\n\t\t\t\tif ((cmpop & 0x8)\n\t\t\t\t\t&&\n\t\t\t\t\tieee754_cxtest\n\t\t\t\t\t(IEEE754_INVALID_OPERATION))\n\t\t\t\t\trcsr = FPU_CSR_INV_X | FPU_CSR_INV_S;\n\t\t\t\telse\n\t\t\t\t\tgoto copcsr;\n\n\t\t\t}\n\t\t\telse {\n\t\t\t\treturn SIGILL;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase w_fmt:{\n\t\tieee754sp fs;\n\n\t\tswitch (MIPSInst_FUNC(ir)) {\n\t\tcase fcvts_op:\n\t\t\t/* convert word to single precision real */\n\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\trv.s = ieee754sp_fint(fs.bits);\n\t\t\trfmt = s_fmt;\n\t\t\tgoto copcsr;\n\t\tcase fcvtd_op:\n\t\t\t/* convert word to double precision real */\n\t\t\tSPFROMREG(fs, MIPSInst_FS(ir));\n\t\t\trv.d = ieee754dp_fint(fs.bits);\n\t\t\trfmt = d_fmt;\n\t\t\tgoto copcsr;\n\t\tdefault:\n\t\t\treturn SIGILL;\n\t\t}\n\t\tbreak;\n\t}\n\n#if defined(__mips64)\n\tcase l_fmt:{\n\t\tswitch (MIPSInst_FUNC(ir)) {\n\t\tcase fcvts_op:\n\t\t\t/* convert long to single precision real */\n\t\t\trv.s = ieee754sp_flong(ctx->fpr[MIPSInst_FS(ir)]);\n\t\t\trfmt = s_fmt;\n\t\t\tgoto copcsr;\n\t\tcase fcvtd_op:\n\t\t\t/* convert long to double precision real */\n\t\t\trv.d = ieee754dp_flong(ctx->fpr[MIPSInst_FS(ir)]);\n\t\t\trfmt = d_fmt;\n\t\t\tgoto copcsr;\n\t\tdefault:\n\t\t\treturn SIGILL;\n\t\t}\n\t\tbreak;\n\t}\n#endif\n\n\tdefault:\n\t\treturn SIGILL;\n\t}\n\n\t/*\n\t * Update the fpu CSR register for this operation.\n\t * If an exception is required, generate a tidy SIGFPE exception,\n\t * without updating the result register.\n\t * Note: cause exception bits do not accumulate, they are rewritten\n\t * for each op; only the flag/sticky bits accumulate.\n\t */\n\tctx->fcr31 = (ctx->fcr31 & ~FPU_CSR_ALL_X) | rcsr;\n\tif ((ctx->fcr31 >> 5) & ctx->fcr31 & FPU_CSR_ALL_E) {\n\t\t/*printk (\"SIGFPE: fpu csr = %08x\\n\",ctx->fcr31); */\n\t\treturn SIGFPE;\n\t}\n\n\t/*\n\t * Now we can safely write the result back to the register file.\n\t */\n\tswitch (rfmt) {\n\tcase -1:{\n#if __mips >= 4\n\t\tcond = fpucondbit[MIPSInst_FD(ir) >> 2];\n#else\n\t\tcond = FPU_CSR_COND;\n#endif\n\t\tif (rv.w)\n\t\t\tctx->fcr31 |= cond;\n\t\telse\n\t\t\tctx->fcr31 &= ~cond;\n\t\tbreak;\n\t}\n\tcase d_fmt:\n\t\tDPTOREG(rv.d, MIPSInst_FD(ir));\n\t\tbreak;\n\tcase s_fmt:\n\t\tSPTOREG(rv.s, MIPSInst_FD(ir));\n\t\tbreak;\n\tcase w_fmt:\n\t\tSITOREG(rv.w, MIPSInst_FD(ir));\n\t\tbreak;\n#if defined(__mips64)\n\tcase l_fmt:\n\t\tDITOREG(rv.l, MIPSInst_FD(ir));\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn SIGILL;\n\t}\n\n\treturn 0;\n}\n\nint fpu_emulator_cop1Handler(struct pt_regs *xcp, struct mips_fpu_struct *ctx,\n\tint has_fpu, void *__user *fault_addr)\n{\n\tunsigned long oldepc, prevepc;\n\tmips_instruction insn;\n\tint sig = 0;\n\n\toldepc = xcp->cp0_epc;\n\tdo {\n\t\tprevepc = xcp->cp0_epc;\n\n\t\tif (!access_ok(VERIFY_READ, xcp->cp0_epc, sizeof(mips_instruction))) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = (mips_instruction __user *)xcp->cp0_epc;\n\t\t\treturn SIGBUS;\n\t\t}\n\t\tif (__get_user(insn, (mips_instruction __user *) xcp->cp0_epc)) {\n\t\t\tMIPS_FPU_EMU_INC_STATS(errors);\n\t\t\t*fault_addr = (mips_instruction __user *)xcp->cp0_epc;\n\t\t\treturn SIGSEGV;\n\t\t}\n\t\tif (insn == 0)\n\t\t\txcp->cp0_epc += 4;\t/* skip nops */\n\t\telse {\n\t\t\t/*\n\t\t\t * The 'ieee754_csr' is an alias of\n\t\t\t * ctx->fcr31.  No need to copy ctx->fcr31 to\n\t\t\t * ieee754_csr.  But ieee754_csr.rm is ieee\n\t\t\t * library modes. (not mips rounding mode)\n\t\t\t */\n\t\t\t/* convert to ieee library modes */\n\t\t\tieee754_csr.rm = ieee_rm[ieee754_csr.rm];\n\t\t\tsig = cop1Emulate(xcp, ctx, fault_addr);\n\t\t\t/* revert to mips rounding mode */\n\t\t\tieee754_csr.rm = mips_rm[ieee754_csr.rm];\n\t\t}\n\n\t\tif (has_fpu)\n\t\t\tbreak;\n\t\tif (sig)\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t} while (xcp->cp0_epc > prevepc);\n\n\t/* SIGILL indicates a non-fpu instruction */\n\tif (sig == SIGILL && xcp->cp0_epc != oldepc)\n\t\t/* but if epc has advanced, then ignore it */\n\t\tsig = 0;\n\n\treturn sig;\n}\n\n#ifdef CONFIG_DEBUG_FS\n\nstatic int fpuemu_stat_get(void *data, u64 *val)\n{\n\tint cpu;\n\tunsigned long sum = 0;\n\tfor_each_online_cpu(cpu) {\n\t\tstruct mips_fpu_emulator_stats *ps;\n\t\tlocal_t *pv;\n\t\tps = &per_cpu(fpuemustats, cpu);\n\t\tpv = (void *)ps + (unsigned long)data;\n\t\tsum += local_read(pv);\n\t}\n\t*val = sum;\n\treturn 0;\n}\nDEFINE_SIMPLE_ATTRIBUTE(fops_fpuemu_stat, fpuemu_stat_get, NULL, \"%llu\\n\");\n\nextern struct dentry *mips_debugfs_dir;\nstatic int __init debugfs_fpuemu(void)\n{\n\tstruct dentry *d, *dir;\n\n\tif (!mips_debugfs_dir)\n\t\treturn -ENODEV;\n\tdir = debugfs_create_dir(\"fpuemustats\", mips_debugfs_dir);\n\tif (!dir)\n\t\treturn -ENOMEM;\n\n#define FPU_STAT_CREATE(M)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\td = debugfs_create_file(#M , S_IRUGO, dir,\t\t\\\n\t\t\t(void *)offsetof(struct mips_fpu_emulator_stats, M), \\\n\t\t\t&fops_fpuemu_stat);\t\t\t\t\\\n\t\tif (!d)\t\t\t\t\t\t\t\\\n\t\t\treturn -ENOMEM;\t\t\t\t\t\\\n\t} while (0)\n\n\tFPU_STAT_CREATE(emulated);\n\tFPU_STAT_CREATE(loads);\n\tFPU_STAT_CREATE(stores);\n\tFPU_STAT_CREATE(cp1ops);\n\tFPU_STAT_CREATE(cp1xops);\n\tFPU_STAT_CREATE(errors);\n\n\treturn 0;\n}\n__initcall(debugfs_fpuemu);\n#endif\n", "/*\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n *\n * Copyright (C) 1995 - 2000 by Ralf Baechle\n */\n#include <linux/signal.h>\n#include <linux/sched.h>\n#include <linux/interrupt.h>\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/ptrace.h>\n#include <linux/mman.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/module.h>\n#include <linux/kprobes.h>\n#include <linux/perf_event.h>\n\n#include <asm/branch.h>\n#include <asm/mmu_context.h>\n#include <asm/system.h>\n#include <asm/uaccess.h>\n#include <asm/ptrace.h>\n#include <asm/highmem.h>\t\t/* For VMALLOC_END */\n#include <linux/kdebug.h>\n\n/*\n * This routine handles page faults.  It determines the address,\n * and the problem, and then passes it off to one of the appropriate\n * routines.\n */\nasmlinkage void __kprobes do_page_fault(struct pt_regs *regs, unsigned long write,\n\t\t\t      unsigned long address)\n{\n\tstruct vm_area_struct * vma = NULL;\n\tstruct task_struct *tsk = current;\n\tstruct mm_struct *mm = tsk->mm;\n\tconst int field = sizeof(unsigned long) * 2;\n\tsiginfo_t info;\n\tint fault;\n\n#if 0\n\tprintk(\"Cpu%d[%s:%d:%0*lx:%ld:%0*lx]\\n\", raw_smp_processor_id(),\n\t       current->comm, current->pid, field, address, write,\n\t       field, regs->cp0_epc);\n#endif\n\n#ifdef CONFIG_KPROBES\n\t/*\n\t * This is to notify the fault handler of the kprobes.  The\n\t * exception code is redundant as it is also carried in REGS,\n\t * but we pass it anyhow.\n\t */\n\tif (notify_die(DIE_PAGE_FAULT, \"page fault\", regs, -1,\n\t\t       (regs->cp0_cause >> 2) & 0x1f, SIGSEGV) == NOTIFY_STOP)\n\t\treturn;\n#endif\n\n\tinfo.si_code = SEGV_MAPERR;\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t */\n#ifdef CONFIG_64BIT\n# define VMALLOC_FAULT_TARGET no_context\n#else\n# define VMALLOC_FAULT_TARGET vmalloc_fault\n#endif\n\n\tif (unlikely(address >= VMALLOC_START && address <= VMALLOC_END))\n\t\tgoto VMALLOC_FAULT_TARGET;\n#ifdef MODULE_START\n\tif (unlikely(address >= MODULE_START && address < MODULE_END))\n\t\tgoto VMALLOC_FAULT_TARGET;\n#endif\n\n\t/*\n\t * If we're in an interrupt or have no user\n\t * context, we must not take the fault..\n\t */\n\tif (in_atomic() || !mm)\n\t\tgoto bad_area_nosemaphore;\n\n\tdown_read(&mm->mmap_sem);\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n/*\n * Ok, we have a good vm_area for this memory access, so\n * we can handle it..\n */\ngood_area:\n\tinfo.si_code = SEGV_ACCERR;\n\n\tif (write) {\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t} else {\n\t\tif (kernel_uses_smartmips_rixi) {\n\t\t\tif (address == regs->cp0_epc && !(vma->vm_flags & VM_EXEC)) {\n#if 0\n\t\t\t\tpr_notice(\"Cpu%d[%s:%d:%0*lx:%ld:%0*lx] XI violation\\n\",\n\t\t\t\t\t  raw_smp_processor_id(),\n\t\t\t\t\t  current->comm, current->pid,\n\t\t\t\t\t  field, address, write,\n\t\t\t\t\t  field, regs->cp0_epc);\n#endif\n\t\t\t\tgoto bad_area;\n\t\t\t}\n\t\t\tif (!(vma->vm_flags & VM_READ)) {\n#if 0\n\t\t\t\tpr_notice(\"Cpu%d[%s:%d:%0*lx:%ld:%0*lx] RI violation\\n\",\n\t\t\t\t\t  raw_smp_processor_id(),\n\t\t\t\t\t  current->comm, current->pid,\n\t\t\t\t\t  field, address, write,\n\t\t\t\t\t  field, regs->cp0_epc);\n#endif\n\t\t\t\tgoto bad_area;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!(vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC)))\n\t\t\t\tgoto bad_area;\n\t\t}\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, write ? FAULT_FLAG_WRITE : 0);\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR) {\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs, address);\n\t\ttsk->maj_flt++;\n\t} else {\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs, address);\n\t\ttsk->min_flt++;\n\t}\n\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n/*\n * Something tried to access memory that isn't in our memory map..\n * Fix it, but check if it's kernel or user first..\n */\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (user_mode(regs)) {\n\t\ttsk->thread.cp0_badvaddr = address;\n\t\ttsk->thread.error_code = write;\n#if 0\n\t\tprintk(\"do_page_fault() #2: sending SIGSEGV to %s for \"\n\t\t       \"invalid %s\\n%0*lx (epc == %0*lx, ra == %0*lx)\\n\",\n\t\t       tsk->comm,\n\t\t       write ? \"write access to\" : \"read access from\",\n\t\t       field, address,\n\t\t       field, (unsigned long) regs->cp0_epc,\n\t\t       field, (unsigned long) regs->regs[31]);\n#endif\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\t/* info.si_code has been set above */\n\t\tinfo.si_addr = (void __user *) address;\n\t\tforce_sig_info(SIGSEGV, &info, tsk);\n\t\treturn;\n\t}\n\nno_context:\n\t/* Are we prepared to handle this kernel fault?  */\n\tif (fixup_exception(regs)) {\n\t\tcurrent->thread.cp0_baduaddr = address;\n\t\treturn;\n\t}\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice.\n\t */\n\tbust_spinlocks(1);\n\n\tprintk(KERN_ALERT \"CPU %d Unable to handle kernel paging request at \"\n\t       \"virtual address %0*lx, epc == %0*lx, ra == %0*lx\\n\",\n\t       raw_smp_processor_id(), field, address, field, regs->cp0_epc,\n\t       field,  regs->regs[31]);\n\tdie(\"Oops\", regs);\n\nout_of_memory:\n\t/*\n\t * We ran out of memory, call the OOM killer, and return the userspace\n\t * (which will retry the fault, or kill us if we got oom-killed).\n\t */\n\tup_read(&mm->mmap_sem);\n\tpagefault_out_of_memory();\n\treturn;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (!user_mode(regs))\n\t\tgoto no_context;\n\telse\n\t/*\n\t * Send a sigbus, regardless of whether we were in kernel\n\t * or user mode.\n\t */\n#if 0\n\t\tprintk(\"do_page_fault() #3: sending SIGBUS to %s for \"\n\t\t       \"invalid %s\\n%0*lx (epc == %0*lx, ra == %0*lx)\\n\",\n\t\t       tsk->comm,\n\t\t       write ? \"write access to\" : \"read access from\",\n\t\t       field, address,\n\t\t       field, (unsigned long) regs->cp0_epc,\n\t\t       field, (unsigned long) regs->regs[31]);\n#endif\n\ttsk->thread.cp0_badvaddr = address;\n\tinfo.si_signo = SIGBUS;\n\tinfo.si_errno = 0;\n\tinfo.si_code = BUS_ADRERR;\n\tinfo.si_addr = (void __user *) address;\n\tforce_sig_info(SIGBUS, &info, tsk);\n\n\treturn;\n#ifndef CONFIG_64BIT\nvmalloc_fault:\n\t{\n\t\t/*\n\t\t * Synchronize this task's top level page-table\n\t\t * with the 'reference' page table.\n\t\t *\n\t\t * Do _not_ use \"tsk\" here. We might be inside\n\t\t * an interrupt in the middle of a task switch..\n\t\t */\n\t\tint offset = __pgd_offset(address);\n\t\tpgd_t *pgd, *pgd_k;\n\t\tpud_t *pud, *pud_k;\n\t\tpmd_t *pmd, *pmd_k;\n\t\tpte_t *pte_k;\n\n\t\tpgd = (pgd_t *) pgd_current[raw_smp_processor_id()] + offset;\n\t\tpgd_k = init_mm.pgd + offset;\n\n\t\tif (!pgd_present(*pgd_k))\n\t\t\tgoto no_context;\n\t\tset_pgd(pgd, *pgd_k);\n\n\t\tpud = pud_offset(pgd, address);\n\t\tpud_k = pud_offset(pgd_k, address);\n\t\tif (!pud_present(*pud_k))\n\t\t\tgoto no_context;\n\n\t\tpmd = pmd_offset(pud, address);\n\t\tpmd_k = pmd_offset(pud_k, address);\n\t\tif (!pmd_present(*pmd_k))\n\t\t\tgoto no_context;\n\t\tset_pmd(pmd, *pmd_k);\n\n\t\tpte_k = pte_offset_kernel(pmd_k, address);\n\t\tif (!pte_present(*pte_k))\n\t\t\tgoto no_context;\n\t\treturn;\n\t}\n#endif\n}\n", "/*\n *  Copyright 2007 Sony Corporation\n *\n *  This program is free software; you can redistribute it and/or modify\n *  it under the terms of the GNU General Public License as published by\n *  the Free Software Foundation; version 2 of the License.\n *\n *  This program is distributed in the hope that it will be useful,\n *  but WITHOUT ANY WARRANTY; without even the implied warranty of\n *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n *  GNU General Public License for more details.\n *\n *  You should have received a copy of the GNU General Public License\n *  along with this program.\n *  If not, see <http://www.gnu.org/licenses/>.\n */\n\n#ifndef _ASM_POWERPC_EMULATED_OPS_H\n#define _ASM_POWERPC_EMULATED_OPS_H\n\n#include <asm/atomic.h>\n#include <linux/perf_event.h>\n\n\n#ifdef CONFIG_PPC_EMULATED_STATS\n\nstruct ppc_emulated_entry {\n\tconst char *name;\n\tatomic_t val;\n};\n\nextern struct ppc_emulated {\n#ifdef CONFIG_ALTIVEC\n\tstruct ppc_emulated_entry altivec;\n#endif\n\tstruct ppc_emulated_entry dcba;\n\tstruct ppc_emulated_entry dcbz;\n\tstruct ppc_emulated_entry fp_pair;\n\tstruct ppc_emulated_entry isel;\n\tstruct ppc_emulated_entry mcrxr;\n\tstruct ppc_emulated_entry mfpvr;\n\tstruct ppc_emulated_entry multiple;\n\tstruct ppc_emulated_entry popcntb;\n\tstruct ppc_emulated_entry spe;\n\tstruct ppc_emulated_entry string;\n\tstruct ppc_emulated_entry unaligned;\n#ifdef CONFIG_MATH_EMULATION\n\tstruct ppc_emulated_entry math;\n#elif defined(CONFIG_8XX_MINIMAL_FPEMU)\n\tstruct ppc_emulated_entry 8xx;\n#endif\n#ifdef CONFIG_VSX\n\tstruct ppc_emulated_entry vsx;\n#endif\n#ifdef CONFIG_PPC64\n\tstruct ppc_emulated_entry mfdscr;\n\tstruct ppc_emulated_entry mtdscr;\n#endif\n} ppc_emulated;\n\nextern u32 ppc_warn_emulated;\n\nextern void ppc_warn_emulated_print(const char *type);\n\n#define __PPC_WARN_EMULATED(type)\t\t\t\t\t \\\n\tdo {\t\t\t\t\t\t\t\t \\\n\t\tatomic_inc(&ppc_emulated.type.val);\t\t\t \\\n\t\tif (ppc_warn_emulated)\t\t\t\t\t \\\n\t\t\tppc_warn_emulated_print(ppc_emulated.type.name); \\\n\t} while (0)\n\n#else /* !CONFIG_PPC_EMULATED_STATS */\n\n#define __PPC_WARN_EMULATED(type)\tdo { } while (0)\n\n#endif /* !CONFIG_PPC_EMULATED_STATS */\n\n#define PPC_WARN_EMULATED(type, regs)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,\t\t\\\n\t\t\t1, regs, 0);\t\t\t\t\t\\\n\t\t__PPC_WARN_EMULATED(type);\t\t\t\t\\\n\t} while (0)\n\n#define PPC_WARN_ALIGNMENT(type, regs)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS,\t\t\\\n\t\t\t1, regs, regs->dar);\t\t\t\t\\\n\t\t__PPC_WARN_EMULATED(type);\t\t\t\t\\\n\t} while (0)\n\n#endif /* _ASM_POWERPC_EMULATED_OPS_H */\n", "/*\n * Performance event support - powerpc architecture code\n *\n * Copyright 2008-2009 Paul Mackerras, IBM Corporation.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public License\n * as published by the Free Software Foundation; either version\n * 2 of the License, or (at your option) any later version.\n */\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/perf_event.h>\n#include <linux/percpu.h>\n#include <linux/hardirq.h>\n#include <asm/reg.h>\n#include <asm/pmc.h>\n#include <asm/machdep.h>\n#include <asm/firmware.h>\n#include <asm/ptrace.h>\n\nstruct cpu_hw_events {\n\tint n_events;\n\tint n_percpu;\n\tint disabled;\n\tint n_added;\n\tint n_limited;\n\tu8  pmcs_enabled;\n\tstruct perf_event *event[MAX_HWEVENTS];\n\tu64 events[MAX_HWEVENTS];\n\tunsigned int flags[MAX_HWEVENTS];\n\tunsigned long mmcr[3];\n\tstruct perf_event *limited_counter[MAX_LIMITED_HWCOUNTERS];\n\tu8  limited_hwidx[MAX_LIMITED_HWCOUNTERS];\n\tu64 alternatives[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];\n\tunsigned long amasks[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];\n\tunsigned long avalues[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];\n\n\tunsigned int group_flag;\n\tint n_txn_start;\n};\nDEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events);\n\nstruct power_pmu *ppmu;\n\n/*\n * Normally, to ignore kernel events we set the FCS (freeze counters\n * in supervisor mode) bit in MMCR0, but if the kernel runs with the\n * hypervisor bit set in the MSR, or if we are running on a processor\n * where the hypervisor bit is forced to 1 (as on Apple G5 processors),\n * then we need to use the FCHV bit to ignore kernel events.\n */\nstatic unsigned int freeze_events_kernel = MMCR0_FCS;\n\n/*\n * 32-bit doesn't have MMCRA but does have an MMCR2,\n * and a few other names are different.\n */\n#ifdef CONFIG_PPC32\n\n#define MMCR0_FCHV\t\t0\n#define MMCR0_PMCjCE\t\tMMCR0_PMCnCE\n\n#define SPRN_MMCRA\t\tSPRN_MMCR2\n#define MMCRA_SAMPLE_ENABLE\t0\n\nstatic inline unsigned long perf_ip_adjust(struct pt_regs *regs)\n{\n\treturn 0;\n}\nstatic inline void perf_get_data_addr(struct pt_regs *regs, u64 *addrp) { }\nstatic inline u32 perf_get_misc_flags(struct pt_regs *regs)\n{\n\treturn 0;\n}\nstatic inline void perf_read_regs(struct pt_regs *regs) { }\nstatic inline int perf_intr_is_nmi(struct pt_regs *regs)\n{\n\treturn 0;\n}\n\n#endif /* CONFIG_PPC32 */\n\n/*\n * Things that are specific to 64-bit implementations.\n */\n#ifdef CONFIG_PPC64\n\nstatic inline unsigned long perf_ip_adjust(struct pt_regs *regs)\n{\n\tunsigned long mmcra = regs->dsisr;\n\n\tif ((mmcra & MMCRA_SAMPLE_ENABLE) && !(ppmu->flags & PPMU_ALT_SIPR)) {\n\t\tunsigned long slot = (mmcra & MMCRA_SLOT) >> MMCRA_SLOT_SHIFT;\n\t\tif (slot > 1)\n\t\t\treturn 4 * (slot - 1);\n\t}\n\treturn 0;\n}\n\n/*\n * The user wants a data address recorded.\n * If we're not doing instruction sampling, give them the SDAR\n * (sampled data address).  If we are doing instruction sampling, then\n * only give them the SDAR if it corresponds to the instruction\n * pointed to by SIAR; this is indicated by the [POWER6_]MMCRA_SDSYNC\n * bit in MMCRA.\n */\nstatic inline void perf_get_data_addr(struct pt_regs *regs, u64 *addrp)\n{\n\tunsigned long mmcra = regs->dsisr;\n\tunsigned long sdsync = (ppmu->flags & PPMU_ALT_SIPR) ?\n\t\tPOWER6_MMCRA_SDSYNC : MMCRA_SDSYNC;\n\n\tif (!(mmcra & MMCRA_SAMPLE_ENABLE) || (mmcra & sdsync))\n\t\t*addrp = mfspr(SPRN_SDAR);\n}\n\nstatic inline u32 perf_get_misc_flags(struct pt_regs *regs)\n{\n\tunsigned long mmcra = regs->dsisr;\n\tunsigned long sihv = MMCRA_SIHV;\n\tunsigned long sipr = MMCRA_SIPR;\n\n\tif (TRAP(regs) != 0xf00)\n\t\treturn 0;\t/* not a PMU interrupt */\n\n\tif (ppmu->flags & PPMU_ALT_SIPR) {\n\t\tsihv = POWER6_MMCRA_SIHV;\n\t\tsipr = POWER6_MMCRA_SIPR;\n\t}\n\n\t/* PR has priority over HV, so order below is important */\n\tif (mmcra & sipr)\n\t\treturn PERF_RECORD_MISC_USER;\n\tif ((mmcra & sihv) && (freeze_events_kernel != MMCR0_FCHV))\n\t\treturn PERF_RECORD_MISC_HYPERVISOR;\n\treturn PERF_RECORD_MISC_KERNEL;\n}\n\n/*\n * Overload regs->dsisr to store MMCRA so we only need to read it once\n * on each interrupt.\n */\nstatic inline void perf_read_regs(struct pt_regs *regs)\n{\n\tregs->dsisr = mfspr(SPRN_MMCRA);\n}\n\n/*\n * If interrupts were soft-disabled when a PMU interrupt occurs, treat\n * it as an NMI.\n */\nstatic inline int perf_intr_is_nmi(struct pt_regs *regs)\n{\n\treturn !regs->softe;\n}\n\n#endif /* CONFIG_PPC64 */\n\nstatic void perf_event_interrupt(struct pt_regs *regs);\n\nvoid perf_event_print_debug(void)\n{\n}\n\n/*\n * Read one performance monitor counter (PMC).\n */\nstatic unsigned long read_pmc(int idx)\n{\n\tunsigned long val;\n\n\tswitch (idx) {\n\tcase 1:\n\t\tval = mfspr(SPRN_PMC1);\n\t\tbreak;\n\tcase 2:\n\t\tval = mfspr(SPRN_PMC2);\n\t\tbreak;\n\tcase 3:\n\t\tval = mfspr(SPRN_PMC3);\n\t\tbreak;\n\tcase 4:\n\t\tval = mfspr(SPRN_PMC4);\n\t\tbreak;\n\tcase 5:\n\t\tval = mfspr(SPRN_PMC5);\n\t\tbreak;\n\tcase 6:\n\t\tval = mfspr(SPRN_PMC6);\n\t\tbreak;\n#ifdef CONFIG_PPC64\n\tcase 7:\n\t\tval = mfspr(SPRN_PMC7);\n\t\tbreak;\n\tcase 8:\n\t\tval = mfspr(SPRN_PMC8);\n\t\tbreak;\n#endif /* CONFIG_PPC64 */\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to read PMC%d\\n\", idx);\n\t\tval = 0;\n\t}\n\treturn val;\n}\n\n/*\n * Write one PMC.\n */\nstatic void write_pmc(int idx, unsigned long val)\n{\n\tswitch (idx) {\n\tcase 1:\n\t\tmtspr(SPRN_PMC1, val);\n\t\tbreak;\n\tcase 2:\n\t\tmtspr(SPRN_PMC2, val);\n\t\tbreak;\n\tcase 3:\n\t\tmtspr(SPRN_PMC3, val);\n\t\tbreak;\n\tcase 4:\n\t\tmtspr(SPRN_PMC4, val);\n\t\tbreak;\n\tcase 5:\n\t\tmtspr(SPRN_PMC5, val);\n\t\tbreak;\n\tcase 6:\n\t\tmtspr(SPRN_PMC6, val);\n\t\tbreak;\n#ifdef CONFIG_PPC64\n\tcase 7:\n\t\tmtspr(SPRN_PMC7, val);\n\t\tbreak;\n\tcase 8:\n\t\tmtspr(SPRN_PMC8, val);\n\t\tbreak;\n#endif /* CONFIG_PPC64 */\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to write PMC%d\\n\", idx);\n\t}\n}\n\n/*\n * Check if a set of events can all go on the PMU at once.\n * If they can't, this will look at alternative codes for the events\n * and see if any combination of alternative codes is feasible.\n * The feasible set is returned in event_id[].\n */\nstatic int power_check_constraints(struct cpu_hw_events *cpuhw,\n\t\t\t\t   u64 event_id[], unsigned int cflags[],\n\t\t\t\t   int n_ev)\n{\n\tunsigned long mask, value, nv;\n\tunsigned long smasks[MAX_HWEVENTS], svalues[MAX_HWEVENTS];\n\tint n_alt[MAX_HWEVENTS], choice[MAX_HWEVENTS];\n\tint i, j;\n\tunsigned long addf = ppmu->add_fields;\n\tunsigned long tadd = ppmu->test_adder;\n\n\tif (n_ev > ppmu->n_counter)\n\t\treturn -1;\n\n\t/* First see if the events will go on as-is */\n\tfor (i = 0; i < n_ev; ++i) {\n\t\tif ((cflags[i] & PPMU_LIMITED_PMC_REQD)\n\t\t    && !ppmu->limited_pmc_event(event_id[i])) {\n\t\t\tppmu->get_alternatives(event_id[i], cflags[i],\n\t\t\t\t\t       cpuhw->alternatives[i]);\n\t\t\tevent_id[i] = cpuhw->alternatives[i][0];\n\t\t}\n\t\tif (ppmu->get_constraint(event_id[i], &cpuhw->amasks[i][0],\n\t\t\t\t\t &cpuhw->avalues[i][0]))\n\t\t\treturn -1;\n\t}\n\tvalue = mask = 0;\n\tfor (i = 0; i < n_ev; ++i) {\n\t\tnv = (value | cpuhw->avalues[i][0]) +\n\t\t\t(value & cpuhw->avalues[i][0] & addf);\n\t\tif ((((nv + tadd) ^ value) & mask) != 0 ||\n\t\t    (((nv + tadd) ^ cpuhw->avalues[i][0]) &\n\t\t     cpuhw->amasks[i][0]) != 0)\n\t\t\tbreak;\n\t\tvalue = nv;\n\t\tmask |= cpuhw->amasks[i][0];\n\t}\n\tif (i == n_ev)\n\t\treturn 0;\t/* all OK */\n\n\t/* doesn't work, gather alternatives... */\n\tif (!ppmu->get_alternatives)\n\t\treturn -1;\n\tfor (i = 0; i < n_ev; ++i) {\n\t\tchoice[i] = 0;\n\t\tn_alt[i] = ppmu->get_alternatives(event_id[i], cflags[i],\n\t\t\t\t\t\t  cpuhw->alternatives[i]);\n\t\tfor (j = 1; j < n_alt[i]; ++j)\n\t\t\tppmu->get_constraint(cpuhw->alternatives[i][j],\n\t\t\t\t\t     &cpuhw->amasks[i][j],\n\t\t\t\t\t     &cpuhw->avalues[i][j]);\n\t}\n\n\t/* enumerate all possibilities and see if any will work */\n\ti = 0;\n\tj = -1;\n\tvalue = mask = nv = 0;\n\twhile (i < n_ev) {\n\t\tif (j >= 0) {\n\t\t\t/* we're backtracking, restore context */\n\t\t\tvalue = svalues[i];\n\t\t\tmask = smasks[i];\n\t\t\tj = choice[i];\n\t\t}\n\t\t/*\n\t\t * See if any alternative k for event_id i,\n\t\t * where k > j, will satisfy the constraints.\n\t\t */\n\t\twhile (++j < n_alt[i]) {\n\t\t\tnv = (value | cpuhw->avalues[i][j]) +\n\t\t\t\t(value & cpuhw->avalues[i][j] & addf);\n\t\t\tif ((((nv + tadd) ^ value) & mask) == 0 &&\n\t\t\t    (((nv + tadd) ^ cpuhw->avalues[i][j])\n\t\t\t     & cpuhw->amasks[i][j]) == 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (j >= n_alt[i]) {\n\t\t\t/*\n\t\t\t * No feasible alternative, backtrack\n\t\t\t * to event_id i-1 and continue enumerating its\n\t\t\t * alternatives from where we got up to.\n\t\t\t */\n\t\t\tif (--i < 0)\n\t\t\t\treturn -1;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Found a feasible alternative for event_id i,\n\t\t\t * remember where we got up to with this event_id,\n\t\t\t * go on to the next event_id, and start with\n\t\t\t * the first alternative for it.\n\t\t\t */\n\t\t\tchoice[i] = j;\n\t\t\tsvalues[i] = value;\n\t\t\tsmasks[i] = mask;\n\t\t\tvalue = nv;\n\t\t\tmask |= cpuhw->amasks[i][j];\n\t\t\t++i;\n\t\t\tj = -1;\n\t\t}\n\t}\n\n\t/* OK, we have a feasible combination, tell the caller the solution */\n\tfor (i = 0; i < n_ev; ++i)\n\t\tevent_id[i] = cpuhw->alternatives[i][choice[i]];\n\treturn 0;\n}\n\n/*\n * Check if newly-added events have consistent settings for\n * exclude_{user,kernel,hv} with each other and any previously\n * added events.\n */\nstatic int check_excludes(struct perf_event **ctrs, unsigned int cflags[],\n\t\t\t  int n_prev, int n_new)\n{\n\tint eu = 0, ek = 0, eh = 0;\n\tint i, n, first;\n\tstruct perf_event *event;\n\n\tn = n_prev + n_new;\n\tif (n <= 1)\n\t\treturn 0;\n\n\tfirst = 1;\n\tfor (i = 0; i < n; ++i) {\n\t\tif (cflags[i] & PPMU_LIMITED_PMC_OK) {\n\t\t\tcflags[i] &= ~PPMU_LIMITED_PMC_REQD;\n\t\t\tcontinue;\n\t\t}\n\t\tevent = ctrs[i];\n\t\tif (first) {\n\t\t\teu = event->attr.exclude_user;\n\t\t\tek = event->attr.exclude_kernel;\n\t\t\teh = event->attr.exclude_hv;\n\t\t\tfirst = 0;\n\t\t} else if (event->attr.exclude_user != eu ||\n\t\t\t   event->attr.exclude_kernel != ek ||\n\t\t\t   event->attr.exclude_hv != eh) {\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\tif (eu || ek || eh)\n\t\tfor (i = 0; i < n; ++i)\n\t\t\tif (cflags[i] & PPMU_LIMITED_PMC_OK)\n\t\t\t\tcflags[i] |= PPMU_LIMITED_PMC_REQD;\n\n\treturn 0;\n}\n\nstatic u64 check_and_compute_delta(u64 prev, u64 val)\n{\n\tu64 delta = (val - prev) & 0xfffffffful;\n\n\t/*\n\t * POWER7 can roll back counter values, if the new value is smaller\n\t * than the previous value it will cause the delta and the counter to\n\t * have bogus values unless we rolled a counter over.  If a coutner is\n\t * rolled back, it will be smaller, but within 256, which is the maximum\n\t * number of events to rollback at once.  If we dectect a rollback\n\t * return 0.  This can lead to a small lack of precision in the\n\t * counters.\n\t */\n\tif (prev > val && (prev - val) < 256)\n\t\tdelta = 0;\n\n\treturn delta;\n}\n\nstatic void power_pmu_read(struct perf_event *event)\n{\n\ts64 val, delta, prev;\n\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn;\n\n\tif (!event->hw.idx)\n\t\treturn;\n\t/*\n\t * Performance monitor interrupts come even when interrupts\n\t * are soft-disabled, as long as interrupts are hard-enabled.\n\t * Therefore we treat them like NMIs.\n\t */\n\tdo {\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tbarrier();\n\t\tval = read_pmc(event->hw.idx);\n\t\tdelta = check_and_compute_delta(prev, val);\n\t\tif (!delta)\n\t\t\treturn;\n\t} while (local64_cmpxchg(&event->hw.prev_count, prev, val) != prev);\n\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &event->hw.period_left);\n}\n\n/*\n * On some machines, PMC5 and PMC6 can't be written, don't respect\n * the freeze conditions, and don't generate interrupts.  This tells\n * us if `event' is using such a PMC.\n */\nstatic int is_limited_pmc(int pmcnum)\n{\n\treturn (ppmu->flags & PPMU_LIMITED_PMC5_6)\n\t\t&& (pmcnum == 5 || pmcnum == 6);\n}\n\nstatic void freeze_limited_counters(struct cpu_hw_events *cpuhw,\n\t\t\t\t    unsigned long pmc5, unsigned long pmc6)\n{\n\tstruct perf_event *event;\n\tu64 val, prev, delta;\n\tint i;\n\n\tfor (i = 0; i < cpuhw->n_limited; ++i) {\n\t\tevent = cpuhw->limited_counter[i];\n\t\tif (!event->hw.idx)\n\t\t\tcontinue;\n\t\tval = (event->hw.idx == 5) ? pmc5 : pmc6;\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tevent->hw.idx = 0;\n\t\tdelta = check_and_compute_delta(prev, val);\n\t\tif (delta)\n\t\t\tlocal64_add(delta, &event->count);\n\t}\n}\n\nstatic void thaw_limited_counters(struct cpu_hw_events *cpuhw,\n\t\t\t\t  unsigned long pmc5, unsigned long pmc6)\n{\n\tstruct perf_event *event;\n\tu64 val, prev;\n\tint i;\n\n\tfor (i = 0; i < cpuhw->n_limited; ++i) {\n\t\tevent = cpuhw->limited_counter[i];\n\t\tevent->hw.idx = cpuhw->limited_hwidx[i];\n\t\tval = (event->hw.idx == 5) ? pmc5 : pmc6;\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tif (check_and_compute_delta(prev, val))\n\t\t\tlocal64_set(&event->hw.prev_count, val);\n\t\tperf_event_update_userpage(event);\n\t}\n}\n\n/*\n * Since limited events don't respect the freeze conditions, we\n * have to read them immediately after freezing or unfreezing the\n * other events.  We try to keep the values from the limited\n * events as consistent as possible by keeping the delay (in\n * cycles and instructions) between freezing/unfreezing and reading\n * the limited events as small and consistent as possible.\n * Therefore, if any limited events are in use, we read them\n * both, and always in the same order, to minimize variability,\n * and do it inside the same asm that writes MMCR0.\n */\nstatic void write_mmcr0(struct cpu_hw_events *cpuhw, unsigned long mmcr0)\n{\n\tunsigned long pmc5, pmc6;\n\n\tif (!cpuhw->n_limited) {\n\t\tmtspr(SPRN_MMCR0, mmcr0);\n\t\treturn;\n\t}\n\n\t/*\n\t * Write MMCR0, then read PMC5 and PMC6 immediately.\n\t * To ensure we don't get a performance monitor interrupt\n\t * between writing MMCR0 and freezing/thawing the limited\n\t * events, we first write MMCR0 with the event overflow\n\t * interrupt enable bits turned off.\n\t */\n\tasm volatile(\"mtspr %3,%2; mfspr %0,%4; mfspr %1,%5\"\n\t\t     : \"=&r\" (pmc5), \"=&r\" (pmc6)\n\t\t     : \"r\" (mmcr0 & ~(MMCR0_PMC1CE | MMCR0_PMCjCE)),\n\t\t       \"i\" (SPRN_MMCR0),\n\t\t       \"i\" (SPRN_PMC5), \"i\" (SPRN_PMC6));\n\n\tif (mmcr0 & MMCR0_FC)\n\t\tfreeze_limited_counters(cpuhw, pmc5, pmc6);\n\telse\n\t\tthaw_limited_counters(cpuhw, pmc5, pmc6);\n\n\t/*\n\t * Write the full MMCR0 including the event overflow interrupt\n\t * enable bits, if necessary.\n\t */\n\tif (mmcr0 & (MMCR0_PMC1CE | MMCR0_PMCjCE))\n\t\tmtspr(SPRN_MMCR0, mmcr0);\n}\n\n/*\n * Disable all events to prevent PMU interrupts and to allow\n * events to be added or removed.\n */\nstatic void power_pmu_disable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\n\tif (!ppmu)\n\t\treturn;\n\tlocal_irq_save(flags);\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\n\tif (!cpuhw->disabled) {\n\t\tcpuhw->disabled = 1;\n\t\tcpuhw->n_added = 0;\n\n\t\t/*\n\t\t * Check if we ever enabled the PMU on this cpu.\n\t\t */\n\t\tif (!cpuhw->pmcs_enabled) {\n\t\t\tppc_enable_pmcs();\n\t\t\tcpuhw->pmcs_enabled = 1;\n\t\t}\n\n\t\t/*\n\t\t * Disable instruction sampling if it was enabled\n\t\t */\n\t\tif (cpuhw->mmcr[2] & MMCRA_SAMPLE_ENABLE) {\n\t\t\tmtspr(SPRN_MMCRA,\n\t\t\t      cpuhw->mmcr[2] & ~MMCRA_SAMPLE_ENABLE);\n\t\t\tmb();\n\t\t}\n\n\t\t/*\n\t\t * Set the 'freeze counters' bit.\n\t\t * The barrier is to make sure the mtspr has been\n\t\t * executed and the PMU has frozen the events\n\t\t * before we return.\n\t\t */\n\t\twrite_mmcr0(cpuhw, mfspr(SPRN_MMCR0) | MMCR0_FC);\n\t\tmb();\n\t}\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Re-enable all events if disable == 0.\n * If we were previously disabled and events were added, then\n * put the new config on the PMU.\n */\nstatic void power_pmu_enable(struct pmu *pmu)\n{\n\tstruct perf_event *event;\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\tlong i;\n\tunsigned long val;\n\ts64 left;\n\tunsigned int hwc_index[MAX_HWEVENTS];\n\tint n_lim;\n\tint idx;\n\n\tif (!ppmu)\n\t\treturn;\n\tlocal_irq_save(flags);\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\tif (!cpuhw->disabled) {\n\t\tlocal_irq_restore(flags);\n\t\treturn;\n\t}\n\tcpuhw->disabled = 0;\n\n\t/*\n\t * If we didn't change anything, or only removed events,\n\t * no need to recalculate MMCR* settings and reset the PMCs.\n\t * Just reenable the PMU with the current MMCR* settings\n\t * (possibly updated for removal of events).\n\t */\n\tif (!cpuhw->n_added) {\n\t\tmtspr(SPRN_MMCRA, cpuhw->mmcr[2] & ~MMCRA_SAMPLE_ENABLE);\n\t\tmtspr(SPRN_MMCR1, cpuhw->mmcr[1]);\n\t\tif (cpuhw->n_events == 0)\n\t\t\tppc_set_pmu_inuse(0);\n\t\tgoto out_enable;\n\t}\n\n\t/*\n\t * Compute MMCR* values for the new set of events\n\t */\n\tif (ppmu->compute_mmcr(cpuhw->events, cpuhw->n_events, hwc_index,\n\t\t\t       cpuhw->mmcr)) {\n\t\t/* shouldn't ever get here */\n\t\tprintk(KERN_ERR \"oops compute_mmcr failed\\n\");\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Add in MMCR0 freeze bits corresponding to the\n\t * attr.exclude_* bits for the first event.\n\t * We have already checked that all events have the\n\t * same values for these bits as the first event.\n\t */\n\tevent = cpuhw->event[0];\n\tif (event->attr.exclude_user)\n\t\tcpuhw->mmcr[0] |= MMCR0_FCP;\n\tif (event->attr.exclude_kernel)\n\t\tcpuhw->mmcr[0] |= freeze_events_kernel;\n\tif (event->attr.exclude_hv)\n\t\tcpuhw->mmcr[0] |= MMCR0_FCHV;\n\n\t/*\n\t * Write the new configuration to MMCR* with the freeze\n\t * bit set and set the hardware events to their initial values.\n\t * Then unfreeze the events.\n\t */\n\tppc_set_pmu_inuse(1);\n\tmtspr(SPRN_MMCRA, cpuhw->mmcr[2] & ~MMCRA_SAMPLE_ENABLE);\n\tmtspr(SPRN_MMCR1, cpuhw->mmcr[1]);\n\tmtspr(SPRN_MMCR0, (cpuhw->mmcr[0] & ~(MMCR0_PMC1CE | MMCR0_PMCjCE))\n\t\t\t\t| MMCR0_FC);\n\n\t/*\n\t * Read off any pre-existing events that need to move\n\t * to another PMC.\n\t */\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tevent = cpuhw->event[i];\n\t\tif (event->hw.idx && event->hw.idx != hwc_index[i] + 1) {\n\t\t\tpower_pmu_read(event);\n\t\t\twrite_pmc(event->hw.idx, 0);\n\t\t\tevent->hw.idx = 0;\n\t\t}\n\t}\n\n\t/*\n\t * Initialize the PMCs for all the new and moved events.\n\t */\n\tcpuhw->n_limited = n_lim = 0;\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tevent = cpuhw->event[i];\n\t\tif (event->hw.idx)\n\t\t\tcontinue;\n\t\tidx = hwc_index[i] + 1;\n\t\tif (is_limited_pmc(idx)) {\n\t\t\tcpuhw->limited_counter[n_lim] = event;\n\t\t\tcpuhw->limited_hwidx[n_lim] = idx;\n\t\t\t++n_lim;\n\t\t\tcontinue;\n\t\t}\n\t\tval = 0;\n\t\tif (event->hw.sample_period) {\n\t\t\tleft = local64_read(&event->hw.period_left);\n\t\t\tif (left < 0x80000000L)\n\t\t\t\tval = 0x80000000L - left;\n\t\t}\n\t\tlocal64_set(&event->hw.prev_count, val);\n\t\tevent->hw.idx = idx;\n\t\tif (event->hw.state & PERF_HES_STOPPED)\n\t\t\tval = 0;\n\t\twrite_pmc(idx, val);\n\t\tperf_event_update_userpage(event);\n\t}\n\tcpuhw->n_limited = n_lim;\n\tcpuhw->mmcr[0] |= MMCR0_PMXE | MMCR0_FCECE;\n\n out_enable:\n\tmb();\n\twrite_mmcr0(cpuhw, cpuhw->mmcr[0]);\n\n\t/*\n\t * Enable instruction sampling if necessary\n\t */\n\tif (cpuhw->mmcr[2] & MMCRA_SAMPLE_ENABLE) {\n\t\tmb();\n\t\tmtspr(SPRN_MMCRA, cpuhw->mmcr[2]);\n\t}\n\n out:\n\tlocal_irq_restore(flags);\n}\n\nstatic int collect_events(struct perf_event *group, int max_count,\n\t\t\t  struct perf_event *ctrs[], u64 *events,\n\t\t\t  unsigned int *flags)\n{\n\tint n = 0;\n\tstruct perf_event *event;\n\n\tif (!is_software_event(group)) {\n\t\tif (n >= max_count)\n\t\t\treturn -1;\n\t\tctrs[n] = group;\n\t\tflags[n] = group->hw.event_base;\n\t\tevents[n++] = group->hw.config;\n\t}\n\tlist_for_each_entry(event, &group->sibling_list, group_entry) {\n\t\tif (!is_software_event(event) &&\n\t\t    event->state != PERF_EVENT_STATE_OFF) {\n\t\t\tif (n >= max_count)\n\t\t\t\treturn -1;\n\t\t\tctrs[n] = event;\n\t\t\tflags[n] = event->hw.event_base;\n\t\t\tevents[n++] = event->hw.config;\n\t\t}\n\t}\n\treturn n;\n}\n\n/*\n * Add a event to the PMU.\n * If all events are not already frozen, then we disable and\n * re-enable the PMU in order to get hw_perf_enable to do the\n * actual work of reconfiguring the PMU.\n */\nstatic int power_pmu_add(struct perf_event *event, int ef_flags)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\tint n0;\n\tint ret = -EAGAIN;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\t/*\n\t * Add the event to the list (if there is room)\n\t * and check whether the total set is still feasible.\n\t */\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\tn0 = cpuhw->n_events;\n\tif (n0 >= ppmu->n_counter)\n\t\tgoto out;\n\tcpuhw->event[n0] = event;\n\tcpuhw->events[n0] = event->hw.config;\n\tcpuhw->flags[n0] = event->hw.event_base;\n\n\tif (!(ef_flags & PERF_EF_START))\n\t\tevent->hw.state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\n\t/*\n\t * If group events scheduling transaction was started,\n\t * skip the schedulability test here, it will be performed\n\t * at commit time(->commit_txn) as a whole\n\t */\n\tif (cpuhw->group_flag & PERF_EVENT_TXN)\n\t\tgoto nocheck;\n\n\tif (check_excludes(cpuhw->event, cpuhw->flags, n0, 1))\n\t\tgoto out;\n\tif (power_check_constraints(cpuhw, cpuhw->events, cpuhw->flags, n0 + 1))\n\t\tgoto out;\n\tevent->hw.config = cpuhw->events[n0];\n\nnocheck:\n\t++cpuhw->n_events;\n\t++cpuhw->n_added;\n\n\tret = 0;\n out:\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\n\n/*\n * Remove a event from the PMU.\n */\nstatic void power_pmu_del(struct perf_event *event, int ef_flags)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tlong i;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tpower_pmu_read(event);\n\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tif (event == cpuhw->event[i]) {\n\t\t\twhile (++i < cpuhw->n_events) {\n\t\t\t\tcpuhw->event[i-1] = cpuhw->event[i];\n\t\t\t\tcpuhw->events[i-1] = cpuhw->events[i];\n\t\t\t\tcpuhw->flags[i-1] = cpuhw->flags[i];\n\t\t\t}\n\t\t\t--cpuhw->n_events;\n\t\t\tppmu->disable_pmc(event->hw.idx - 1, cpuhw->mmcr);\n\t\t\tif (event->hw.idx) {\n\t\t\t\twrite_pmc(event->hw.idx, 0);\n\t\t\t\tevent->hw.idx = 0;\n\t\t\t}\n\t\t\tperf_event_update_userpage(event);\n\t\t\tbreak;\n\t\t}\n\t}\n\tfor (i = 0; i < cpuhw->n_limited; ++i)\n\t\tif (event == cpuhw->limited_counter[i])\n\t\t\tbreak;\n\tif (i < cpuhw->n_limited) {\n\t\twhile (++i < cpuhw->n_limited) {\n\t\t\tcpuhw->limited_counter[i-1] = cpuhw->limited_counter[i];\n\t\t\tcpuhw->limited_hwidx[i-1] = cpuhw->limited_hwidx[i];\n\t\t}\n\t\t--cpuhw->n_limited;\n\t}\n\tif (cpuhw->n_events == 0) {\n\t\t/* disable exceptions if no events are running */\n\t\tcpuhw->mmcr[0] &= ~(MMCR0_PMXE | MMCR0_FCECE);\n\t}\n\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\n/*\n * POWER-PMU does not support disabling individual counters, hence\n * program their cycle counter to their max value and ignore the interrupts.\n */\n\nstatic void power_pmu_start(struct perf_event *event, int ef_flags)\n{\n\tunsigned long flags;\n\ts64 left;\n\n\tif (!event->hw.idx || !event->hw.sample_period)\n\t\treturn;\n\n\tif (!(event->hw.state & PERF_HES_STOPPED))\n\t\treturn;\n\n\tif (ef_flags & PERF_EF_RELOAD)\n\t\tWARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tevent->hw.state = 0;\n\tleft = local64_read(&event->hw.period_left);\n\twrite_pmc(event->hw.idx, left);\n\n\tperf_event_update_userpage(event);\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\nstatic void power_pmu_stop(struct perf_event *event, int ef_flags)\n{\n\tunsigned long flags;\n\n\tif (!event->hw.idx || !event->hw.sample_period)\n\t\treturn;\n\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tpower_pmu_read(event);\n\tevent->hw.state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\twrite_pmc(event->hw.idx, 0);\n\n\tperf_event_update_userpage(event);\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Start group events scheduling transaction\n * Set the flag to make pmu::enable() not perform the\n * schedulability test, it will be performed at commit time\n */\nvoid power_pmu_start_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\n\n\tperf_pmu_disable(pmu);\n\tcpuhw->group_flag |= PERF_EVENT_TXN;\n\tcpuhw->n_txn_start = cpuhw->n_events;\n}\n\n/*\n * Stop group events scheduling transaction\n * Clear the flag and pmu::enable() will perform the\n * schedulability test.\n */\nvoid power_pmu_cancel_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\n\n\tcpuhw->group_flag &= ~PERF_EVENT_TXN;\n\tperf_pmu_enable(pmu);\n}\n\n/*\n * Commit group events scheduling transaction\n * Perform the group schedulability test as a whole\n * Return 0 if success\n */\nint power_pmu_commit_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tlong i, n;\n\n\tif (!ppmu)\n\t\treturn -EAGAIN;\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\tn = cpuhw->n_events;\n\tif (check_excludes(cpuhw->event, cpuhw->flags, 0, n))\n\t\treturn -EAGAIN;\n\ti = power_check_constraints(cpuhw, cpuhw->events, cpuhw->flags, n);\n\tif (i < 0)\n\t\treturn -EAGAIN;\n\n\tfor (i = cpuhw->n_txn_start; i < n; ++i)\n\t\tcpuhw->event[i]->hw.config = cpuhw->events[i];\n\n\tcpuhw->group_flag &= ~PERF_EVENT_TXN;\n\tperf_pmu_enable(pmu);\n\treturn 0;\n}\n\n/*\n * Return 1 if we might be able to put event on a limited PMC,\n * or 0 if not.\n * A event can only go on a limited PMC if it counts something\n * that a limited PMC can count, doesn't require interrupts, and\n * doesn't exclude any processor mode.\n */\nstatic int can_go_on_limited_pmc(struct perf_event *event, u64 ev,\n\t\t\t\t unsigned int flags)\n{\n\tint n;\n\tu64 alt[MAX_EVENT_ALTERNATIVES];\n\n\tif (event->attr.exclude_user\n\t    || event->attr.exclude_kernel\n\t    || event->attr.exclude_hv\n\t    || event->attr.sample_period)\n\t\treturn 0;\n\n\tif (ppmu->limited_pmc_event(ev))\n\t\treturn 1;\n\n\t/*\n\t * The requested event_id isn't on a limited PMC already;\n\t * see if any alternative code goes on a limited PMC.\n\t */\n\tif (!ppmu->get_alternatives)\n\t\treturn 0;\n\n\tflags |= PPMU_LIMITED_PMC_OK | PPMU_LIMITED_PMC_REQD;\n\tn = ppmu->get_alternatives(ev, flags, alt);\n\n\treturn n > 0;\n}\n\n/*\n * Find an alternative event_id that goes on a normal PMC, if possible,\n * and return the event_id code, or 0 if there is no such alternative.\n * (Note: event_id code 0 is \"don't count\" on all machines.)\n */\nstatic u64 normal_pmc_alternative(u64 ev, unsigned long flags)\n{\n\tu64 alt[MAX_EVENT_ALTERNATIVES];\n\tint n;\n\n\tflags &= ~(PPMU_LIMITED_PMC_OK | PPMU_LIMITED_PMC_REQD);\n\tn = ppmu->get_alternatives(ev, flags, alt);\n\tif (!n)\n\t\treturn 0;\n\treturn alt[0];\n}\n\n/* Number of perf_events counting hardware events */\nstatic atomic_t num_events;\n/* Used to avoid races in calling reserve/release_pmc_hardware */\nstatic DEFINE_MUTEX(pmc_reserve_mutex);\n\n/*\n * Release the PMU if this is the last perf_event.\n */\nstatic void hw_perf_event_destroy(struct perf_event *event)\n{\n\tif (!atomic_add_unless(&num_events, -1, 1)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_dec_return(&num_events) == 0)\n\t\t\trelease_pmc_hardware();\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n}\n\n/*\n * Translate a generic cache event_id config to a raw event_id code.\n */\nstatic int hw_perf_cache_event(u64 config, u64 *eventp)\n{\n\tunsigned long type, op, result;\n\tint ev;\n\n\tif (!ppmu->cache_events)\n\t\treturn -EINVAL;\n\n\t/* unpack config */\n\ttype = config & 0xff;\n\top = (config >> 8) & 0xff;\n\tresult = (config >> 16) & 0xff;\n\n\tif (type >= PERF_COUNT_HW_CACHE_MAX ||\n\t    op >= PERF_COUNT_HW_CACHE_OP_MAX ||\n\t    result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn -EINVAL;\n\n\tev = (*ppmu->cache_events)[type][op][result];\n\tif (ev == 0)\n\t\treturn -EOPNOTSUPP;\n\tif (ev == -1)\n\t\treturn -EINVAL;\n\t*eventp = ev;\n\treturn 0;\n}\n\nstatic int power_pmu_event_init(struct perf_event *event)\n{\n\tu64 ev;\n\tunsigned long flags;\n\tstruct perf_event *ctrs[MAX_HWEVENTS];\n\tu64 events[MAX_HWEVENTS];\n\tunsigned int cflags[MAX_HWEVENTS];\n\tint n;\n\tint err;\n\tstruct cpu_hw_events *cpuhw;\n\n\tif (!ppmu)\n\t\treturn -ENOENT;\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_HARDWARE:\n\t\tev = event->attr.config;\n\t\tif (ev >= ppmu->n_generic || ppmu->generic_events[ev] == 0)\n\t\t\treturn -EOPNOTSUPP;\n\t\tev = ppmu->generic_events[ev];\n\t\tbreak;\n\tcase PERF_TYPE_HW_CACHE:\n\t\terr = hw_perf_cache_event(event->attr.config, &ev);\n\t\tif (err)\n\t\t\treturn err;\n\t\tbreak;\n\tcase PERF_TYPE_RAW:\n\t\tev = event->attr.config;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\tevent->hw.config_base = ev;\n\tevent->hw.idx = 0;\n\n\t/*\n\t * If we are not running on a hypervisor, force the\n\t * exclude_hv bit to 0 so that we don't care what\n\t * the user set it to.\n\t */\n\tif (!firmware_has_feature(FW_FEATURE_LPAR))\n\t\tevent->attr.exclude_hv = 0;\n\n\t/*\n\t * If this is a per-task event, then we can use\n\t * PM_RUN_* events interchangeably with their non RUN_*\n\t * equivalents, e.g. PM_RUN_CYC instead of PM_CYC.\n\t * XXX we should check if the task is an idle task.\n\t */\n\tflags = 0;\n\tif (event->attach_state & PERF_ATTACH_TASK)\n\t\tflags |= PPMU_ONLY_COUNT_RUN;\n\n\t/*\n\t * If this machine has limited events, check whether this\n\t * event_id could go on a limited event.\n\t */\n\tif (ppmu->flags & PPMU_LIMITED_PMC5_6) {\n\t\tif (can_go_on_limited_pmc(event, ev, flags)) {\n\t\t\tflags |= PPMU_LIMITED_PMC_OK;\n\t\t} else if (ppmu->limited_pmc_event(ev)) {\n\t\t\t/*\n\t\t\t * The requested event_id is on a limited PMC,\n\t\t\t * but we can't use a limited PMC; see if any\n\t\t\t * alternative goes on a normal PMC.\n\t\t\t */\n\t\t\tev = normal_pmc_alternative(ev, flags);\n\t\t\tif (!ev)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/*\n\t * If this is in a group, check if it can go on with all the\n\t * other hardware events in the group.  We assume the event\n\t * hasn't been linked into its leader's sibling list at this point.\n\t */\n\tn = 0;\n\tif (event->group_leader != event) {\n\t\tn = collect_events(event->group_leader, ppmu->n_counter - 1,\n\t\t\t\t   ctrs, events, cflags);\n\t\tif (n < 0)\n\t\t\treturn -EINVAL;\n\t}\n\tevents[n] = ev;\n\tctrs[n] = event;\n\tcflags[n] = flags;\n\tif (check_excludes(ctrs, cflags, n, 1))\n\t\treturn -EINVAL;\n\n\tcpuhw = &get_cpu_var(cpu_hw_events);\n\terr = power_check_constraints(cpuhw, events, cflags, n + 1);\n\tput_cpu_var(cpu_hw_events);\n\tif (err)\n\t\treturn -EINVAL;\n\n\tevent->hw.config = events[n];\n\tevent->hw.event_base = cflags[n];\n\tevent->hw.last_period = event->hw.sample_period;\n\tlocal64_set(&event->hw.period_left, event->hw.last_period);\n\n\t/*\n\t * See if we need to reserve the PMU.\n\t * If no events are currently in use, then we have to take a\n\t * mutex to ensure that we don't race with another task doing\n\t * reserve_pmc_hardware or release_pmc_hardware.\n\t */\n\terr = 0;\n\tif (!atomic_inc_not_zero(&num_events)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_read(&num_events) == 0 &&\n\t\t    reserve_pmc_hardware(perf_event_interrupt))\n\t\t\terr = -EBUSY;\n\t\telse\n\t\t\tatomic_inc(&num_events);\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n\tevent->destroy = hw_perf_event_destroy;\n\n\treturn err;\n}\n\nstruct pmu power_pmu = {\n\t.pmu_enable\t= power_pmu_enable,\n\t.pmu_disable\t= power_pmu_disable,\n\t.event_init\t= power_pmu_event_init,\n\t.add\t\t= power_pmu_add,\n\t.del\t\t= power_pmu_del,\n\t.start\t\t= power_pmu_start,\n\t.stop\t\t= power_pmu_stop,\n\t.read\t\t= power_pmu_read,\n\t.start_txn\t= power_pmu_start_txn,\n\t.cancel_txn\t= power_pmu_cancel_txn,\n\t.commit_txn\t= power_pmu_commit_txn,\n};\n\n/*\n * A counter has overflowed; update its count and record\n * things if requested.  Note that interrupts are hard-disabled\n * here so there is no possibility of being interrupted.\n */\nstatic void record_and_restart(struct perf_event *event, unsigned long val,\n\t\t\t       struct pt_regs *regs)\n{\n\tu64 period = event->hw.sample_period;\n\ts64 prev, delta, left;\n\tint record = 0;\n\n\tif (event->hw.state & PERF_HES_STOPPED) {\n\t\twrite_pmc(event->hw.idx, 0);\n\t\treturn;\n\t}\n\n\t/* we don't have to worry about interrupts here */\n\tprev = local64_read(&event->hw.prev_count);\n\tdelta = check_and_compute_delta(prev, val);\n\tlocal64_add(delta, &event->count);\n\n\t/*\n\t * See if the total period for this event has expired,\n\t * and update for the next period.\n\t */\n\tval = 0;\n\tleft = local64_read(&event->hw.period_left) - delta;\n\tif (period) {\n\t\tif (left <= 0) {\n\t\t\tleft += period;\n\t\t\tif (left <= 0)\n\t\t\t\tleft = period;\n\t\t\trecord = 1;\n\t\t\tevent->hw.last_period = event->hw.sample_period;\n\t\t}\n\t\tif (left < 0x80000000LL)\n\t\t\tval = 0x80000000LL - left;\n\t}\n\n\twrite_pmc(event->hw.idx, val);\n\tlocal64_set(&event->hw.prev_count, val);\n\tlocal64_set(&event->hw.period_left, left);\n\tperf_event_update_userpage(event);\n\n\t/*\n\t * Finally record data if requested.\n\t */\n\tif (record) {\n\t\tstruct perf_sample_data data;\n\n\t\tperf_sample_data_init(&data, ~0ULL);\n\t\tdata.period = event->hw.last_period;\n\n\t\tif (event->attr.sample_type & PERF_SAMPLE_ADDR)\n\t\t\tperf_get_data_addr(regs, &data.addr);\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tpower_pmu_stop(event, 0);\n\t}\n}\n\n/*\n * Called from generic code to get the misc flags (i.e. processor mode)\n * for an event_id.\n */\nunsigned long perf_misc_flags(struct pt_regs *regs)\n{\n\tu32 flags = perf_get_misc_flags(regs);\n\n\tif (flags)\n\t\treturn flags;\n\treturn user_mode(regs) ? PERF_RECORD_MISC_USER :\n\t\tPERF_RECORD_MISC_KERNEL;\n}\n\n/*\n * Called from generic code to get the instruction pointer\n * for an event_id.\n */\nunsigned long perf_instruction_pointer(struct pt_regs *regs)\n{\n\tunsigned long ip;\n\n\tif (TRAP(regs) != 0xf00)\n\t\treturn regs->nip;\t/* not a PMU interrupt */\n\n\tip = mfspr(SPRN_SIAR) + perf_ip_adjust(regs);\n\treturn ip;\n}\n\nstatic bool pmc_overflow(unsigned long val)\n{\n\tif ((int)val < 0)\n\t\treturn true;\n\n\t/*\n\t * Events on POWER7 can roll back if a speculative event doesn't\n\t * eventually complete. Unfortunately in some rare cases they will\n\t * raise a performance monitor exception. We need to catch this to\n\t * ensure we reset the PMC. In all cases the PMC will be 256 or less\n\t * cycles from overflow.\n\t *\n\t * We only do this if the first pass fails to find any overflowing\n\t * PMCs because a user might set a period of less than 256 and we\n\t * don't want to mistakenly reset them.\n\t */\n\tif (__is_processor(PV_POWER7) && ((0x80000000 - val) <= 256))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * Performance monitor interrupt stuff\n */\nstatic void perf_event_interrupt(struct pt_regs *regs)\n{\n\tint i;\n\tstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\n\tstruct perf_event *event;\n\tunsigned long val;\n\tint found = 0;\n\tint nmi;\n\n\tif (cpuhw->n_limited)\n\t\tfreeze_limited_counters(cpuhw, mfspr(SPRN_PMC5),\n\t\t\t\t\tmfspr(SPRN_PMC6));\n\n\tperf_read_regs(regs);\n\n\tnmi = perf_intr_is_nmi(regs);\n\tif (nmi)\n\t\tnmi_enter();\n\telse\n\t\tirq_enter();\n\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tevent = cpuhw->event[i];\n\t\tif (!event->hw.idx || is_limited_pmc(event->hw.idx))\n\t\t\tcontinue;\n\t\tval = read_pmc(event->hw.idx);\n\t\tif ((int)val < 0) {\n\t\t\t/* event has overflowed */\n\t\t\tfound = 1;\n\t\t\trecord_and_restart(event, val, regs);\n\t\t}\n\t}\n\n\t/*\n\t * In case we didn't find and reset the event that caused\n\t * the interrupt, scan all events and reset any that are\n\t * negative, to avoid getting continual interrupts.\n\t * Any that we processed in the previous loop will not be negative.\n\t */\n\tif (!found) {\n\t\tfor (i = 0; i < ppmu->n_counter; ++i) {\n\t\t\tif (is_limited_pmc(i + 1))\n\t\t\t\tcontinue;\n\t\t\tval = read_pmc(i + 1);\n\t\t\tif (pmc_overflow(val))\n\t\t\t\twrite_pmc(i + 1, 0);\n\t\t}\n\t}\n\n\t/*\n\t * Reset MMCR0 to its normal value.  This will set PMXE and\n\t * clear FC (freeze counters) and PMAO (perf mon alert occurred)\n\t * and thus allow interrupts to occur again.\n\t * XXX might want to use MSR.PM to keep the events frozen until\n\t * we get back out of this interrupt.\n\t */\n\twrite_mmcr0(cpuhw, cpuhw->mmcr[0]);\n\n\tif (nmi)\n\t\tnmi_exit();\n\telse\n\t\tirq_exit();\n}\n\nstatic void power_pmu_setup(int cpu)\n{\n\tstruct cpu_hw_events *cpuhw = &per_cpu(cpu_hw_events, cpu);\n\n\tif (!ppmu)\n\t\treturn;\n\tmemset(cpuhw, 0, sizeof(*cpuhw));\n\tcpuhw->mmcr[0] = MMCR0_FC;\n}\n\nstatic int __cpuinit\npower_pmu_notifier(struct notifier_block *self, unsigned long action, void *hcpu)\n{\n\tunsigned int cpu = (long)hcpu;\n\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_UP_PREPARE:\n\t\tpower_pmu_setup(cpu);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nint register_power_pmu(struct power_pmu *pmu)\n{\n\tif (ppmu)\n\t\treturn -EBUSY;\t\t/* something's already registered */\n\n\tppmu = pmu;\n\tpr_info(\"%s performance monitor hardware support registered\\n\",\n\t\tpmu->name);\n\n#ifdef MSR_HV\n\t/*\n\t * Use FCHV to ignore kernel events if MSR.HV is set.\n\t */\n\tif (mfmsr() & MSR_HV)\n\t\tfreeze_events_kernel = MMCR0_FCHV;\n#endif /* CONFIG_PPC64 */\n\n\tperf_pmu_register(&power_pmu, \"cpu\", PERF_TYPE_RAW);\n\tperf_cpu_notifier(power_pmu_notifier);\n\n\treturn 0;\n}\n", "/*\n * Performance event support - Freescale Embedded Performance Monitor\n *\n * Copyright 2008-2009 Paul Mackerras, IBM Corporation.\n * Copyright 2010 Freescale Semiconductor, Inc.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public License\n * as published by the Free Software Foundation; either version\n * 2 of the License, or (at your option) any later version.\n */\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/perf_event.h>\n#include <linux/percpu.h>\n#include <linux/hardirq.h>\n#include <asm/reg_fsl_emb.h>\n#include <asm/pmc.h>\n#include <asm/machdep.h>\n#include <asm/firmware.h>\n#include <asm/ptrace.h>\n\nstruct cpu_hw_events {\n\tint n_events;\n\tint disabled;\n\tu8  pmcs_enabled;\n\tstruct perf_event *event[MAX_HWEVENTS];\n};\nstatic DEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events);\n\nstatic struct fsl_emb_pmu *ppmu;\n\n/* Number of perf_events counting hardware events */\nstatic atomic_t num_events;\n/* Used to avoid races in calling reserve/release_pmc_hardware */\nstatic DEFINE_MUTEX(pmc_reserve_mutex);\n\n/*\n * If interrupts were soft-disabled when a PMU interrupt occurs, treat\n * it as an NMI.\n */\nstatic inline int perf_intr_is_nmi(struct pt_regs *regs)\n{\n#ifdef __powerpc64__\n\treturn !regs->softe;\n#else\n\treturn 0;\n#endif\n}\n\nstatic void perf_event_interrupt(struct pt_regs *regs);\n\n/*\n * Read one performance monitor counter (PMC).\n */\nstatic unsigned long read_pmc(int idx)\n{\n\tunsigned long val;\n\n\tswitch (idx) {\n\tcase 0:\n\t\tval = mfpmr(PMRN_PMC0);\n\t\tbreak;\n\tcase 1:\n\t\tval = mfpmr(PMRN_PMC1);\n\t\tbreak;\n\tcase 2:\n\t\tval = mfpmr(PMRN_PMC2);\n\t\tbreak;\n\tcase 3:\n\t\tval = mfpmr(PMRN_PMC3);\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to read PMC%d\\n\", idx);\n\t\tval = 0;\n\t}\n\treturn val;\n}\n\n/*\n * Write one PMC.\n */\nstatic void write_pmc(int idx, unsigned long val)\n{\n\tswitch (idx) {\n\tcase 0:\n\t\tmtpmr(PMRN_PMC0, val);\n\t\tbreak;\n\tcase 1:\n\t\tmtpmr(PMRN_PMC1, val);\n\t\tbreak;\n\tcase 2:\n\t\tmtpmr(PMRN_PMC2, val);\n\t\tbreak;\n\tcase 3:\n\t\tmtpmr(PMRN_PMC3, val);\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to write PMC%d\\n\", idx);\n\t}\n\n\tisync();\n}\n\n/*\n * Write one local control A register\n */\nstatic void write_pmlca(int idx, unsigned long val)\n{\n\tswitch (idx) {\n\tcase 0:\n\t\tmtpmr(PMRN_PMLCA0, val);\n\t\tbreak;\n\tcase 1:\n\t\tmtpmr(PMRN_PMLCA1, val);\n\t\tbreak;\n\tcase 2:\n\t\tmtpmr(PMRN_PMLCA2, val);\n\t\tbreak;\n\tcase 3:\n\t\tmtpmr(PMRN_PMLCA3, val);\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to write PMLCA%d\\n\", idx);\n\t}\n\n\tisync();\n}\n\n/*\n * Write one local control B register\n */\nstatic void write_pmlcb(int idx, unsigned long val)\n{\n\tswitch (idx) {\n\tcase 0:\n\t\tmtpmr(PMRN_PMLCB0, val);\n\t\tbreak;\n\tcase 1:\n\t\tmtpmr(PMRN_PMLCB1, val);\n\t\tbreak;\n\tcase 2:\n\t\tmtpmr(PMRN_PMLCB2, val);\n\t\tbreak;\n\tcase 3:\n\t\tmtpmr(PMRN_PMLCB3, val);\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to write PMLCB%d\\n\", idx);\n\t}\n\n\tisync();\n}\n\nstatic void fsl_emb_pmu_read(struct perf_event *event)\n{\n\ts64 val, delta, prev;\n\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn;\n\n\t/*\n\t * Performance monitor interrupts come even when interrupts\n\t * are soft-disabled, as long as interrupts are hard-enabled.\n\t * Therefore we treat them like NMIs.\n\t */\n\tdo {\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tbarrier();\n\t\tval = read_pmc(event->hw.idx);\n\t} while (local64_cmpxchg(&event->hw.prev_count, prev, val) != prev);\n\n\t/* The counters are only 32 bits wide */\n\tdelta = (val - prev) & 0xfffffffful;\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &event->hw.period_left);\n}\n\n/*\n * Disable all events to prevent PMU interrupts and to allow\n * events to be added or removed.\n */\nstatic void fsl_emb_pmu_disable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\n\tif (!cpuhw->disabled) {\n\t\tcpuhw->disabled = 1;\n\n\t\t/*\n\t\t * Check if we ever enabled the PMU on this cpu.\n\t\t */\n\t\tif (!cpuhw->pmcs_enabled) {\n\t\t\tppc_enable_pmcs();\n\t\t\tcpuhw->pmcs_enabled = 1;\n\t\t}\n\n\t\tif (atomic_read(&num_events)) {\n\t\t\t/*\n\t\t\t * Set the 'freeze all counters' bit, and disable\n\t\t\t * interrupts.  The barrier is to make sure the\n\t\t\t * mtpmr has been executed and the PMU has frozen\n\t\t\t * the events before we return.\n\t\t\t */\n\n\t\t\tmtpmr(PMRN_PMGC0, PMGC0_FAC);\n\t\t\tisync();\n\t\t}\n\t}\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Re-enable all events if disable == 0.\n * If we were previously disabled and events were added, then\n * put the new config on the PMU.\n */\nstatic void fsl_emb_pmu_enable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tcpuhw = &__get_cpu_var(cpu_hw_events);\n\tif (!cpuhw->disabled)\n\t\tgoto out;\n\n\tcpuhw->disabled = 0;\n\tppc_set_pmu_inuse(cpuhw->n_events != 0);\n\n\tif (cpuhw->n_events > 0) {\n\t\tmtpmr(PMRN_PMGC0, PMGC0_PMIE | PMGC0_FCECE);\n\t\tisync();\n\t}\n\n out:\n\tlocal_irq_restore(flags);\n}\n\nstatic int collect_events(struct perf_event *group, int max_count,\n\t\t\t  struct perf_event *ctrs[])\n{\n\tint n = 0;\n\tstruct perf_event *event;\n\n\tif (!is_software_event(group)) {\n\t\tif (n >= max_count)\n\t\t\treturn -1;\n\t\tctrs[n] = group;\n\t\tn++;\n\t}\n\tlist_for_each_entry(event, &group->sibling_list, group_entry) {\n\t\tif (!is_software_event(event) &&\n\t\t    event->state != PERF_EVENT_STATE_OFF) {\n\t\t\tif (n >= max_count)\n\t\t\t\treturn -1;\n\t\t\tctrs[n] = event;\n\t\t\tn++;\n\t\t}\n\t}\n\treturn n;\n}\n\n/* context locked on entry */\nstatic int fsl_emb_pmu_add(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tint ret = -EAGAIN;\n\tint num_counters = ppmu->n_counter;\n\tu64 val;\n\tint i;\n\n\tperf_pmu_disable(event->pmu);\n\tcpuhw = &get_cpu_var(cpu_hw_events);\n\n\tif (event->hw.config & FSL_EMB_EVENT_RESTRICTED)\n\t\tnum_counters = ppmu->n_restricted;\n\n\t/*\n\t * Allocate counters from top-down, so that restricted-capable\n\t * counters are kept free as long as possible.\n\t */\n\tfor (i = num_counters - 1; i >= 0; i--) {\n\t\tif (cpuhw->event[i])\n\t\t\tcontinue;\n\n\t\tbreak;\n\t}\n\n\tif (i < 0)\n\t\tgoto out;\n\n\tevent->hw.idx = i;\n\tcpuhw->event[i] = event;\n\t++cpuhw->n_events;\n\n\tval = 0;\n\tif (event->hw.sample_period) {\n\t\ts64 left = local64_read(&event->hw.period_left);\n\t\tif (left < 0x80000000L)\n\t\t\tval = 0x80000000L - left;\n\t}\n\tlocal64_set(&event->hw.prev_count, val);\n\n\tif (!(flags & PERF_EF_START)) {\n\t\tevent->hw.state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\t\tval = 0;\n\t}\n\n\twrite_pmc(i, val);\n\tperf_event_update_userpage(event);\n\n\twrite_pmlcb(i, event->hw.config >> 32);\n\twrite_pmlca(i, event->hw.config_base);\n\n\tret = 0;\n out:\n\tput_cpu_var(cpu_hw_events);\n\tperf_pmu_enable(event->pmu);\n\treturn ret;\n}\n\n/* context locked on entry */\nstatic void fsl_emb_pmu_del(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tint i = event->hw.idx;\n\n\tperf_pmu_disable(event->pmu);\n\tif (i < 0)\n\t\tgoto out;\n\n\tfsl_emb_pmu_read(event);\n\n\tcpuhw = &get_cpu_var(cpu_hw_events);\n\n\tWARN_ON(event != cpuhw->event[event->hw.idx]);\n\n\twrite_pmlca(i, 0);\n\twrite_pmlcb(i, 0);\n\twrite_pmc(i, 0);\n\n\tcpuhw->event[i] = NULL;\n\tevent->hw.idx = -1;\n\n\t/*\n\t * TODO: if at least one restricted event exists, and we\n\t * just freed up a non-restricted-capable counter, and\n\t * there is a restricted-capable counter occupied by\n\t * a non-restricted event, migrate that event to the\n\t * vacated counter.\n\t */\n\n\tcpuhw->n_events--;\n\n out:\n\tperf_pmu_enable(event->pmu);\n\tput_cpu_var(cpu_hw_events);\n}\n\nstatic void fsl_emb_pmu_start(struct perf_event *event, int ef_flags)\n{\n\tunsigned long flags;\n\ts64 left;\n\n\tif (event->hw.idx < 0 || !event->hw.sample_period)\n\t\treturn;\n\n\tif (!(event->hw.state & PERF_HES_STOPPED))\n\t\treturn;\n\n\tif (ef_flags & PERF_EF_RELOAD)\n\t\tWARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tevent->hw.state = 0;\n\tleft = local64_read(&event->hw.period_left);\n\twrite_pmc(event->hw.idx, left);\n\n\tperf_event_update_userpage(event);\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\nstatic void fsl_emb_pmu_stop(struct perf_event *event, int ef_flags)\n{\n\tunsigned long flags;\n\n\tif (event->hw.idx < 0 || !event->hw.sample_period)\n\t\treturn;\n\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tfsl_emb_pmu_read(event);\n\tevent->hw.state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\twrite_pmc(event->hw.idx, 0);\n\n\tperf_event_update_userpage(event);\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Release the PMU if this is the last perf_event.\n */\nstatic void hw_perf_event_destroy(struct perf_event *event)\n{\n\tif (!atomic_add_unless(&num_events, -1, 1)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_dec_return(&num_events) == 0)\n\t\t\trelease_pmc_hardware();\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n}\n\n/*\n * Translate a generic cache event_id config to a raw event_id code.\n */\nstatic int hw_perf_cache_event(u64 config, u64 *eventp)\n{\n\tunsigned long type, op, result;\n\tint ev;\n\n\tif (!ppmu->cache_events)\n\t\treturn -EINVAL;\n\n\t/* unpack config */\n\ttype = config & 0xff;\n\top = (config >> 8) & 0xff;\n\tresult = (config >> 16) & 0xff;\n\n\tif (type >= PERF_COUNT_HW_CACHE_MAX ||\n\t    op >= PERF_COUNT_HW_CACHE_OP_MAX ||\n\t    result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn -EINVAL;\n\n\tev = (*ppmu->cache_events)[type][op][result];\n\tif (ev == 0)\n\t\treturn -EOPNOTSUPP;\n\tif (ev == -1)\n\t\treturn -EINVAL;\n\t*eventp = ev;\n\treturn 0;\n}\n\nstatic int fsl_emb_pmu_event_init(struct perf_event *event)\n{\n\tu64 ev;\n\tstruct perf_event *events[MAX_HWEVENTS];\n\tint n;\n\tint err;\n\tint num_restricted;\n\tint i;\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_HARDWARE:\n\t\tev = event->attr.config;\n\t\tif (ev >= ppmu->n_generic || ppmu->generic_events[ev] == 0)\n\t\t\treturn -EOPNOTSUPP;\n\t\tev = ppmu->generic_events[ev];\n\t\tbreak;\n\n\tcase PERF_TYPE_HW_CACHE:\n\t\terr = hw_perf_cache_event(event->attr.config, &ev);\n\t\tif (err)\n\t\t\treturn err;\n\t\tbreak;\n\n\tcase PERF_TYPE_RAW:\n\t\tev = event->attr.config;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\tevent->hw.config = ppmu->xlate_event(ev);\n\tif (!(event->hw.config & FSL_EMB_EVENT_VALID))\n\t\treturn -EINVAL;\n\n\t/*\n\t * If this is in a group, check if it can go on with all the\n\t * other hardware events in the group.  We assume the event\n\t * hasn't been linked into its leader's sibling list at this point.\n\t */\n\tn = 0;\n\tif (event->group_leader != event) {\n\t\tn = collect_events(event->group_leader,\n\t\t                   ppmu->n_counter - 1, events);\n\t\tif (n < 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (event->hw.config & FSL_EMB_EVENT_RESTRICTED) {\n\t\tnum_restricted = 0;\n\t\tfor (i = 0; i < n; i++) {\n\t\t\tif (events[i]->hw.config & FSL_EMB_EVENT_RESTRICTED)\n\t\t\t\tnum_restricted++;\n\t\t}\n\n\t\tif (num_restricted >= ppmu->n_restricted)\n\t\t\treturn -EINVAL;\n\t}\n\n\tevent->hw.idx = -1;\n\n\tevent->hw.config_base = PMLCA_CE | PMLCA_FCM1 |\n\t                        (u32)((ev << 16) & PMLCA_EVENT_MASK);\n\n\tif (event->attr.exclude_user)\n\t\tevent->hw.config_base |= PMLCA_FCU;\n\tif (event->attr.exclude_kernel)\n\t\tevent->hw.config_base |= PMLCA_FCS;\n\tif (event->attr.exclude_idle)\n\t\treturn -ENOTSUPP;\n\n\tevent->hw.last_period = event->hw.sample_period;\n\tlocal64_set(&event->hw.period_left, event->hw.last_period);\n\n\t/*\n\t * See if we need to reserve the PMU.\n\t * If no events are currently in use, then we have to take a\n\t * mutex to ensure that we don't race with another task doing\n\t * reserve_pmc_hardware or release_pmc_hardware.\n\t */\n\terr = 0;\n\tif (!atomic_inc_not_zero(&num_events)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_read(&num_events) == 0 &&\n\t\t    reserve_pmc_hardware(perf_event_interrupt))\n\t\t\terr = -EBUSY;\n\t\telse\n\t\t\tatomic_inc(&num_events);\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\n\t\tmtpmr(PMRN_PMGC0, PMGC0_FAC);\n\t\tisync();\n\t}\n\tevent->destroy = hw_perf_event_destroy;\n\n\treturn err;\n}\n\nstatic struct pmu fsl_emb_pmu = {\n\t.pmu_enable\t= fsl_emb_pmu_enable,\n\t.pmu_disable\t= fsl_emb_pmu_disable,\n\t.event_init\t= fsl_emb_pmu_event_init,\n\t.add\t\t= fsl_emb_pmu_add,\n\t.del\t\t= fsl_emb_pmu_del,\n\t.start\t\t= fsl_emb_pmu_start,\n\t.stop\t\t= fsl_emb_pmu_stop,\n\t.read\t\t= fsl_emb_pmu_read,\n};\n\n/*\n * A counter has overflowed; update its count and record\n * things if requested.  Note that interrupts are hard-disabled\n * here so there is no possibility of being interrupted.\n */\nstatic void record_and_restart(struct perf_event *event, unsigned long val,\n\t\t\t       struct pt_regs *regs)\n{\n\tu64 period = event->hw.sample_period;\n\ts64 prev, delta, left;\n\tint record = 0;\n\n\tif (event->hw.state & PERF_HES_STOPPED) {\n\t\twrite_pmc(event->hw.idx, 0);\n\t\treturn;\n\t}\n\n\t/* we don't have to worry about interrupts here */\n\tprev = local64_read(&event->hw.prev_count);\n\tdelta = (val - prev) & 0xfffffffful;\n\tlocal64_add(delta, &event->count);\n\n\t/*\n\t * See if the total period for this event has expired,\n\t * and update for the next period.\n\t */\n\tval = 0;\n\tleft = local64_read(&event->hw.period_left) - delta;\n\tif (period) {\n\t\tif (left <= 0) {\n\t\t\tleft += period;\n\t\t\tif (left <= 0)\n\t\t\t\tleft = period;\n\t\t\trecord = 1;\n\t\t\tevent->hw.last_period = event->hw.sample_period;\n\t\t}\n\t\tif (left < 0x80000000LL)\n\t\t\tval = 0x80000000LL - left;\n\t}\n\n\twrite_pmc(event->hw.idx, val);\n\tlocal64_set(&event->hw.prev_count, val);\n\tlocal64_set(&event->hw.period_left, left);\n\tperf_event_update_userpage(event);\n\n\t/*\n\t * Finally record data if requested.\n\t */\n\tif (record) {\n\t\tstruct perf_sample_data data;\n\n\t\tperf_sample_data_init(&data, 0);\n\t\tdata.period = event->hw.last_period;\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tfsl_emb_pmu_stop(event, 0);\n\t}\n}\n\nstatic void perf_event_interrupt(struct pt_regs *regs)\n{\n\tint i;\n\tstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\n\tstruct perf_event *event;\n\tunsigned long val;\n\tint found = 0;\n\tint nmi;\n\n\tnmi = perf_intr_is_nmi(regs);\n\tif (nmi)\n\t\tnmi_enter();\n\telse\n\t\tirq_enter();\n\n\tfor (i = 0; i < ppmu->n_counter; ++i) {\n\t\tevent = cpuhw->event[i];\n\n\t\tval = read_pmc(i);\n\t\tif ((int)val < 0) {\n\t\t\tif (event) {\n\t\t\t\t/* event has overflowed */\n\t\t\t\tfound = 1;\n\t\t\t\trecord_and_restart(event, val, regs);\n\t\t\t} else {\n\t\t\t\t/*\n\t\t\t\t * Disabled counter is negative,\n\t\t\t\t * reset it just in case.\n\t\t\t\t */\n\t\t\t\twrite_pmc(i, 0);\n\t\t\t}\n\t\t}\n\t}\n\n\t/* PMM will keep counters frozen until we return from the interrupt. */\n\tmtmsr(mfmsr() | MSR_PMM);\n\tmtpmr(PMRN_PMGC0, PMGC0_PMIE | PMGC0_FCECE);\n\tisync();\n\n\tif (nmi)\n\t\tnmi_exit();\n\telse\n\t\tirq_exit();\n}\n\nvoid hw_perf_event_setup(int cpu)\n{\n\tstruct cpu_hw_events *cpuhw = &per_cpu(cpu_hw_events, cpu);\n\n\tmemset(cpuhw, 0, sizeof(*cpuhw));\n}\n\nint register_fsl_emb_pmu(struct fsl_emb_pmu *pmu)\n{\n\tif (ppmu)\n\t\treturn -EBUSY;\t\t/* something's already registered */\n\n\tppmu = pmu;\n\tpr_info(\"%s performance monitor hardware support registered\\n\",\n\t\tpmu->name);\n\n\tperf_pmu_register(&fsl_emb_pmu, \"cpu\", PERF_TYPE_RAW);\n\n\treturn 0;\n}\n", "/*\n *  PowerPC version\n *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)\n *\n *  Derived from \"arch/m68k/kernel/ptrace.c\"\n *  Copyright (C) 1994 by Hamish Macdonald\n *  Taken from linux/kernel/ptrace.c and modified for M680x0.\n *  linux/kernel/ptrace.c is by Ross Biro 1/23/92, edited by Linus Torvalds\n *\n * Modified by Cort Dougan (cort@hq.fsmlabs.com)\n * and Paul Mackerras (paulus@samba.org).\n *\n * This file is subject to the terms and conditions of the GNU General\n * Public License.  See the file README.legal in the main directory of\n * this archive for more details.\n */\n\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/errno.h>\n#include <linux/ptrace.h>\n#include <linux/regset.h>\n#include <linux/tracehook.h>\n#include <linux/elf.h>\n#include <linux/user.h>\n#include <linux/security.h>\n#include <linux/signal.h>\n#include <linux/seccomp.h>\n#include <linux/audit.h>\n#include <trace/syscall.h>\n#ifdef CONFIG_PPC32\n#include <linux/module.h>\n#endif\n#include <linux/hw_breakpoint.h>\n#include <linux/perf_event.h>\n\n#include <asm/uaccess.h>\n#include <asm/page.h>\n#include <asm/pgtable.h>\n#include <asm/system.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/syscalls.h>\n\n/*\n * The parameter save area on the stack is used to store arguments being passed\n * to callee function and is located at fixed offset from stack pointer.\n */\n#ifdef CONFIG_PPC32\n#define PARAMETER_SAVE_AREA_OFFSET\t24  /* bytes */\n#else /* CONFIG_PPC32 */\n#define PARAMETER_SAVE_AREA_OFFSET\t48  /* bytes */\n#endif\n\nstruct pt_regs_offset {\n\tconst char *name;\n\tint offset;\n};\n\n#define STR(s)\t#s\t\t\t/* convert to string */\n#define REG_OFFSET_NAME(r) {.name = #r, .offset = offsetof(struct pt_regs, r)}\n#define GPR_OFFSET_NAME(num)\t\\\n\t{.name = STR(gpr##num), .offset = offsetof(struct pt_regs, gpr[num])}\n#define REG_OFFSET_END {.name = NULL, .offset = 0}\n\nstatic const struct pt_regs_offset regoffset_table[] = {\n\tGPR_OFFSET_NAME(0),\n\tGPR_OFFSET_NAME(1),\n\tGPR_OFFSET_NAME(2),\n\tGPR_OFFSET_NAME(3),\n\tGPR_OFFSET_NAME(4),\n\tGPR_OFFSET_NAME(5),\n\tGPR_OFFSET_NAME(6),\n\tGPR_OFFSET_NAME(7),\n\tGPR_OFFSET_NAME(8),\n\tGPR_OFFSET_NAME(9),\n\tGPR_OFFSET_NAME(10),\n\tGPR_OFFSET_NAME(11),\n\tGPR_OFFSET_NAME(12),\n\tGPR_OFFSET_NAME(13),\n\tGPR_OFFSET_NAME(14),\n\tGPR_OFFSET_NAME(15),\n\tGPR_OFFSET_NAME(16),\n\tGPR_OFFSET_NAME(17),\n\tGPR_OFFSET_NAME(18),\n\tGPR_OFFSET_NAME(19),\n\tGPR_OFFSET_NAME(20),\n\tGPR_OFFSET_NAME(21),\n\tGPR_OFFSET_NAME(22),\n\tGPR_OFFSET_NAME(23),\n\tGPR_OFFSET_NAME(24),\n\tGPR_OFFSET_NAME(25),\n\tGPR_OFFSET_NAME(26),\n\tGPR_OFFSET_NAME(27),\n\tGPR_OFFSET_NAME(28),\n\tGPR_OFFSET_NAME(29),\n\tGPR_OFFSET_NAME(30),\n\tGPR_OFFSET_NAME(31),\n\tREG_OFFSET_NAME(nip),\n\tREG_OFFSET_NAME(msr),\n\tREG_OFFSET_NAME(ctr),\n\tREG_OFFSET_NAME(link),\n\tREG_OFFSET_NAME(xer),\n\tREG_OFFSET_NAME(ccr),\n#ifdef CONFIG_PPC64\n\tREG_OFFSET_NAME(softe),\n#else\n\tREG_OFFSET_NAME(mq),\n#endif\n\tREG_OFFSET_NAME(trap),\n\tREG_OFFSET_NAME(dar),\n\tREG_OFFSET_NAME(dsisr),\n\tREG_OFFSET_END,\n};\n\n/**\n * regs_query_register_offset() - query register offset from its name\n * @name:\tthe name of a register\n *\n * regs_query_register_offset() returns the offset of a register in struct\n * pt_regs from its name. If the name is invalid, this returns -EINVAL;\n */\nint regs_query_register_offset(const char *name)\n{\n\tconst struct pt_regs_offset *roff;\n\tfor (roff = regoffset_table; roff->name != NULL; roff++)\n\t\tif (!strcmp(roff->name, name))\n\t\t\treturn roff->offset;\n\treturn -EINVAL;\n}\n\n/**\n * regs_query_register_name() - query register name from its offset\n * @offset:\tthe offset of a register in struct pt_regs.\n *\n * regs_query_register_name() returns the name of a register from its\n * offset in struct pt_regs. If the @offset is invalid, this returns NULL;\n */\nconst char *regs_query_register_name(unsigned int offset)\n{\n\tconst struct pt_regs_offset *roff;\n\tfor (roff = regoffset_table; roff->name != NULL; roff++)\n\t\tif (roff->offset == offset)\n\t\t\treturn roff->name;\n\treturn NULL;\n}\n\n/*\n * does not yet catch signals sent when the child dies.\n * in exit.c or in signal.c.\n */\n\n/*\n * Set of msr bits that gdb can change on behalf of a process.\n */\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n#define MSR_DEBUGCHANGE\t0\n#else\n#define MSR_DEBUGCHANGE\t(MSR_SE | MSR_BE)\n#endif\n\n/*\n * Max register writeable via put_reg\n */\n#ifdef CONFIG_PPC32\n#define PT_MAX_PUT_REG\tPT_MQ\n#else\n#define PT_MAX_PUT_REG\tPT_CCR\n#endif\n\nstatic unsigned long get_user_msr(struct task_struct *task)\n{\n\treturn task->thread.regs->msr | task->thread.fpexc_mode;\n}\n\nstatic int set_user_msr(struct task_struct *task, unsigned long msr)\n{\n\ttask->thread.regs->msr &= ~MSR_DEBUGCHANGE;\n\ttask->thread.regs->msr |= msr & MSR_DEBUGCHANGE;\n\treturn 0;\n}\n\n/*\n * We prevent mucking around with the reserved area of trap\n * which are used internally by the kernel.\n */\nstatic int set_user_trap(struct task_struct *task, unsigned long trap)\n{\n\ttask->thread.regs->trap = trap & 0xfff0;\n\treturn 0;\n}\n\n/*\n * Get contents of register REGNO in task TASK.\n */\nunsigned long ptrace_get_reg(struct task_struct *task, int regno)\n{\n\tif (task->thread.regs == NULL)\n\t\treturn -EIO;\n\n\tif (regno == PT_MSR)\n\t\treturn get_user_msr(task);\n\n\tif (regno < (sizeof(struct pt_regs) / sizeof(unsigned long)))\n\t\treturn ((unsigned long *)task->thread.regs)[regno];\n\n\treturn -EIO;\n}\n\n/*\n * Write contents of register REGNO in task TASK.\n */\nint ptrace_put_reg(struct task_struct *task, int regno, unsigned long data)\n{\n\tif (task->thread.regs == NULL)\n\t\treturn -EIO;\n\n\tif (regno == PT_MSR)\n\t\treturn set_user_msr(task, data);\n\tif (regno == PT_TRAP)\n\t\treturn set_user_trap(task, data);\n\n\tif (regno <= PT_MAX_PUT_REG) {\n\t\t((unsigned long *)task->thread.regs)[regno] = data;\n\t\treturn 0;\n\t}\n\treturn -EIO;\n}\n\nstatic int gpr_get(struct task_struct *target, const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n\tint i, ret;\n\n\tif (target->thread.regs == NULL)\n\t\treturn -EIO;\n\n\tif (!FULL_REGS(target->thread.regs)) {\n\t\t/* We have a partial register set.  Fill 14-31 with bogus values */\n\t\tfor (i = 14; i < 32; i++)\n\t\t\ttarget->thread.regs->gpr[i] = NV_REG_POISON;\n\t}\n\n\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  target->thread.regs,\n\t\t\t\t  0, offsetof(struct pt_regs, msr));\n\tif (!ret) {\n\t\tunsigned long msr = get_user_msr(target);\n\t\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf, &msr,\n\t\t\t\t\t  offsetof(struct pt_regs, msr),\n\t\t\t\t\t  offsetof(struct pt_regs, msr) +\n\t\t\t\t\t  sizeof(msr));\n\t}\n\n\tBUILD_BUG_ON(offsetof(struct pt_regs, orig_gpr3) !=\n\t\t     offsetof(struct pt_regs, msr) + sizeof(long));\n\n\tif (!ret)\n\t\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t  &target->thread.regs->orig_gpr3,\n\t\t\t\t\t  offsetof(struct pt_regs, orig_gpr3),\n\t\t\t\t\t  sizeof(struct pt_regs));\n\tif (!ret)\n\t\tret = user_regset_copyout_zero(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t       sizeof(struct pt_regs), -1);\n\n\treturn ret;\n}\n\nstatic int gpr_set(struct task_struct *target, const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   const void *kbuf, const void __user *ubuf)\n{\n\tunsigned long reg;\n\tint ret;\n\n\tif (target->thread.regs == NULL)\n\t\treturn -EIO;\n\n\tCHECK_FULL_REGS(target->thread.regs);\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t target->thread.regs,\n\t\t\t\t 0, PT_MSR * sizeof(reg));\n\n\tif (!ret && count > 0) {\n\t\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &reg,\n\t\t\t\t\t PT_MSR * sizeof(reg),\n\t\t\t\t\t (PT_MSR + 1) * sizeof(reg));\n\t\tif (!ret)\n\t\t\tret = set_user_msr(target, reg);\n\t}\n\n\tBUILD_BUG_ON(offsetof(struct pt_regs, orig_gpr3) !=\n\t\t     offsetof(struct pt_regs, msr) + sizeof(long));\n\n\tif (!ret)\n\t\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t &target->thread.regs->orig_gpr3,\n\t\t\t\t\t PT_ORIG_R3 * sizeof(reg),\n\t\t\t\t\t (PT_MAX_PUT_REG + 1) * sizeof(reg));\n\n\tif (PT_MAX_PUT_REG + 1 < PT_TRAP && !ret)\n\t\tret = user_regset_copyin_ignore(\n\t\t\t&pos, &count, &kbuf, &ubuf,\n\t\t\t(PT_MAX_PUT_REG + 1) * sizeof(reg),\n\t\t\tPT_TRAP * sizeof(reg));\n\n\tif (!ret && count > 0) {\n\t\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &reg,\n\t\t\t\t\t PT_TRAP * sizeof(reg),\n\t\t\t\t\t (PT_TRAP + 1) * sizeof(reg));\n\t\tif (!ret)\n\t\t\tret = set_user_trap(target, reg);\n\t}\n\n\tif (!ret)\n\t\tret = user_regset_copyin_ignore(\n\t\t\t&pos, &count, &kbuf, &ubuf,\n\t\t\t(PT_TRAP + 1) * sizeof(reg), -1);\n\n\treturn ret;\n}\n\nstatic int fpr_get(struct task_struct *target, const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n#ifdef CONFIG_VSX\n\tdouble buf[33];\n\tint i;\n#endif\n\tflush_fp_to_thread(target);\n\n#ifdef CONFIG_VSX\n\t/* copy to local buffer then write that out */\n\tfor (i = 0; i < 32 ; i++)\n\t\tbuf[i] = target->thread.TS_FPR(i);\n\tmemcpy(&buf[32], &target->thread.fpscr, sizeof(double));\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf, buf, 0, -1);\n\n#else\n\tBUILD_BUG_ON(offsetof(struct thread_struct, fpscr) !=\n\t\t     offsetof(struct thread_struct, TS_FPR(32)));\n\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t   &target->thread.fpr, 0, -1);\n#endif\n}\n\nstatic int fpr_set(struct task_struct *target, const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   const void *kbuf, const void __user *ubuf)\n{\n#ifdef CONFIG_VSX\n\tdouble buf[33];\n\tint i;\n#endif\n\tflush_fp_to_thread(target);\n\n#ifdef CONFIG_VSX\n\t/* copy to local buffer then write that out */\n\ti = user_regset_copyin(&pos, &count, &kbuf, &ubuf, buf, 0, -1);\n\tif (i)\n\t\treturn i;\n\tfor (i = 0; i < 32 ; i++)\n\t\ttarget->thread.TS_FPR(i) = buf[i];\n\tmemcpy(&target->thread.fpscr, &buf[32], sizeof(double));\n\treturn 0;\n#else\n\tBUILD_BUG_ON(offsetof(struct thread_struct, fpscr) !=\n\t\t     offsetof(struct thread_struct, TS_FPR(32)));\n\n\treturn user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  &target->thread.fpr, 0, -1);\n#endif\n}\n\n#ifdef CONFIG_ALTIVEC\n/*\n * Get/set all the altivec registers vr0..vr31, vscr, vrsave, in one go.\n * The transfer totals 34 quadword.  Quadwords 0-31 contain the\n * corresponding vector registers.  Quadword 32 contains the vscr as the\n * last word (offset 12) within that quadword.  Quadword 33 contains the\n * vrsave as the first word (offset 0) within the quadword.\n *\n * This definition of the VMX state is compatible with the current PPC32\n * ptrace interface.  This allows signal handling and ptrace to use the\n * same structures.  This also simplifies the implementation of a bi-arch\n * (combined (32- and 64-bit) gdb.\n */\n\nstatic int vr_active(struct task_struct *target,\n\t\t     const struct user_regset *regset)\n{\n\tflush_altivec_to_thread(target);\n\treturn target->thread.used_vr ? regset->n : 0;\n}\n\nstatic int vr_get(struct task_struct *target, const struct user_regset *regset,\n\t\t  unsigned int pos, unsigned int count,\n\t\t  void *kbuf, void __user *ubuf)\n{\n\tint ret;\n\n\tflush_altivec_to_thread(target);\n\n\tBUILD_BUG_ON(offsetof(struct thread_struct, vscr) !=\n\t\t     offsetof(struct thread_struct, vr[32]));\n\n\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  &target->thread.vr, 0,\n\t\t\t\t  33 * sizeof(vector128));\n\tif (!ret) {\n\t\t/*\n\t\t * Copy out only the low-order word of vrsave.\n\t\t */\n\t\tunion {\n\t\t\telf_vrreg_t reg;\n\t\t\tu32 word;\n\t\t} vrsave;\n\t\tmemset(&vrsave, 0, sizeof(vrsave));\n\t\tvrsave.word = target->thread.vrsave;\n\t\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf, &vrsave,\n\t\t\t\t\t  33 * sizeof(vector128), -1);\n\t}\n\n\treturn ret;\n}\n\nstatic int vr_set(struct task_struct *target, const struct user_regset *regset,\n\t\t  unsigned int pos, unsigned int count,\n\t\t  const void *kbuf, const void __user *ubuf)\n{\n\tint ret;\n\n\tflush_altivec_to_thread(target);\n\n\tBUILD_BUG_ON(offsetof(struct thread_struct, vscr) !=\n\t\t     offsetof(struct thread_struct, vr[32]));\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t &target->thread.vr, 0, 33 * sizeof(vector128));\n\tif (!ret && count > 0) {\n\t\t/*\n\t\t * We use only the first word of vrsave.\n\t\t */\n\t\tunion {\n\t\t\telf_vrreg_t reg;\n\t\t\tu32 word;\n\t\t} vrsave;\n\t\tmemset(&vrsave, 0, sizeof(vrsave));\n\t\tvrsave.word = target->thread.vrsave;\n\t\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &vrsave,\n\t\t\t\t\t 33 * sizeof(vector128), -1);\n\t\tif (!ret)\n\t\t\ttarget->thread.vrsave = vrsave.word;\n\t}\n\n\treturn ret;\n}\n#endif /* CONFIG_ALTIVEC */\n\n#ifdef CONFIG_VSX\n/*\n * Currently to set and and get all the vsx state, you need to call\n * the fp and VMX calls as well.  This only get/sets the lower 32\n * 128bit VSX registers.\n */\n\nstatic int vsr_active(struct task_struct *target,\n\t\t      const struct user_regset *regset)\n{\n\tflush_vsx_to_thread(target);\n\treturn target->thread.used_vsr ? regset->n : 0;\n}\n\nstatic int vsr_get(struct task_struct *target, const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n\tdouble buf[32];\n\tint ret, i;\n\n\tflush_vsx_to_thread(target);\n\n\tfor (i = 0; i < 32 ; i++)\n\t\tbuf[i] = target->thread.fpr[i][TS_VSRLOWOFFSET];\n\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  buf, 0, 32 * sizeof(double));\n\n\treturn ret;\n}\n\nstatic int vsr_set(struct task_struct *target, const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   const void *kbuf, const void __user *ubuf)\n{\n\tdouble buf[32];\n\tint ret,i;\n\n\tflush_vsx_to_thread(target);\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t buf, 0, 32 * sizeof(double));\n\tfor (i = 0; i < 32 ; i++)\n\t\ttarget->thread.fpr[i][TS_VSRLOWOFFSET] = buf[i];\n\n\n\treturn ret;\n}\n#endif /* CONFIG_VSX */\n\n#ifdef CONFIG_SPE\n\n/*\n * For get_evrregs/set_evrregs functions 'data' has the following layout:\n *\n * struct {\n *   u32 evr[32];\n *   u64 acc;\n *   u32 spefscr;\n * }\n */\n\nstatic int evr_active(struct task_struct *target,\n\t\t      const struct user_regset *regset)\n{\n\tflush_spe_to_thread(target);\n\treturn target->thread.used_spe ? regset->n : 0;\n}\n\nstatic int evr_get(struct task_struct *target, const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   void *kbuf, void __user *ubuf)\n{\n\tint ret;\n\n\tflush_spe_to_thread(target);\n\n\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  &target->thread.evr,\n\t\t\t\t  0, sizeof(target->thread.evr));\n\n\tBUILD_BUG_ON(offsetof(struct thread_struct, acc) + sizeof(u64) !=\n\t\t     offsetof(struct thread_struct, spefscr));\n\n\tif (!ret)\n\t\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t  &target->thread.acc,\n\t\t\t\t\t  sizeof(target->thread.evr), -1);\n\n\treturn ret;\n}\n\nstatic int evr_set(struct task_struct *target, const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   const void *kbuf, const void __user *ubuf)\n{\n\tint ret;\n\n\tflush_spe_to_thread(target);\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t &target->thread.evr,\n\t\t\t\t 0, sizeof(target->thread.evr));\n\n\tBUILD_BUG_ON(offsetof(struct thread_struct, acc) + sizeof(u64) !=\n\t\t     offsetof(struct thread_struct, spefscr));\n\n\tif (!ret)\n\t\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t &target->thread.acc,\n\t\t\t\t\t sizeof(target->thread.evr), -1);\n\n\treturn ret;\n}\n#endif /* CONFIG_SPE */\n\n\n/*\n * These are our native regset flavors.\n */\nenum powerpc_regset {\n\tREGSET_GPR,\n\tREGSET_FPR,\n#ifdef CONFIG_ALTIVEC\n\tREGSET_VMX,\n#endif\n#ifdef CONFIG_VSX\n\tREGSET_VSX,\n#endif\n#ifdef CONFIG_SPE\n\tREGSET_SPE,\n#endif\n};\n\nstatic const struct user_regset native_regsets[] = {\n\t[REGSET_GPR] = {\n\t\t.core_note_type = NT_PRSTATUS, .n = ELF_NGREG,\n\t\t.size = sizeof(long), .align = sizeof(long),\n\t\t.get = gpr_get, .set = gpr_set\n\t},\n\t[REGSET_FPR] = {\n\t\t.core_note_type = NT_PRFPREG, .n = ELF_NFPREG,\n\t\t.size = sizeof(double), .align = sizeof(double),\n\t\t.get = fpr_get, .set = fpr_set\n\t},\n#ifdef CONFIG_ALTIVEC\n\t[REGSET_VMX] = {\n\t\t.core_note_type = NT_PPC_VMX, .n = 34,\n\t\t.size = sizeof(vector128), .align = sizeof(vector128),\n\t\t.active = vr_active, .get = vr_get, .set = vr_set\n\t},\n#endif\n#ifdef CONFIG_VSX\n\t[REGSET_VSX] = {\n\t\t.core_note_type = NT_PPC_VSX, .n = 32,\n\t\t.size = sizeof(double), .align = sizeof(double),\n\t\t.active = vsr_active, .get = vsr_get, .set = vsr_set\n\t},\n#endif\n#ifdef CONFIG_SPE\n\t[REGSET_SPE] = {\n\t\t.n = 35,\n\t\t.size = sizeof(u32), .align = sizeof(u32),\n\t\t.active = evr_active, .get = evr_get, .set = evr_set\n\t},\n#endif\n};\n\nstatic const struct user_regset_view user_ppc_native_view = {\n\t.name = UTS_MACHINE, .e_machine = ELF_ARCH, .ei_osabi = ELF_OSABI,\n\t.regsets = native_regsets, .n = ARRAY_SIZE(native_regsets)\n};\n\n#ifdef CONFIG_PPC64\n#include <linux/compat.h>\n\nstatic int gpr32_get(struct task_struct *target,\n\t\t     const struct user_regset *regset,\n\t\t     unsigned int pos, unsigned int count,\n\t\t     void *kbuf, void __user *ubuf)\n{\n\tconst unsigned long *regs = &target->thread.regs->gpr[0];\n\tcompat_ulong_t *k = kbuf;\n\tcompat_ulong_t __user *u = ubuf;\n\tcompat_ulong_t reg;\n\tint i;\n\n\tif (target->thread.regs == NULL)\n\t\treturn -EIO;\n\n\tif (!FULL_REGS(target->thread.regs)) {\n\t\t/* We have a partial register set.  Fill 14-31 with bogus values */\n\t\tfor (i = 14; i < 32; i++)\n\t\t\ttarget->thread.regs->gpr[i] = NV_REG_POISON; \n\t}\n\n\tpos /= sizeof(reg);\n\tcount /= sizeof(reg);\n\n\tif (kbuf)\n\t\tfor (; count > 0 && pos < PT_MSR; --count)\n\t\t\t*k++ = regs[pos++];\n\telse\n\t\tfor (; count > 0 && pos < PT_MSR; --count)\n\t\t\tif (__put_user((compat_ulong_t) regs[pos++], u++))\n\t\t\t\treturn -EFAULT;\n\n\tif (count > 0 && pos == PT_MSR) {\n\t\treg = get_user_msr(target);\n\t\tif (kbuf)\n\t\t\t*k++ = reg;\n\t\telse if (__put_user(reg, u++))\n\t\t\treturn -EFAULT;\n\t\t++pos;\n\t\t--count;\n\t}\n\n\tif (kbuf)\n\t\tfor (; count > 0 && pos < PT_REGS_COUNT; --count)\n\t\t\t*k++ = regs[pos++];\n\telse\n\t\tfor (; count > 0 && pos < PT_REGS_COUNT; --count)\n\t\t\tif (__put_user((compat_ulong_t) regs[pos++], u++))\n\t\t\t\treturn -EFAULT;\n\n\tkbuf = k;\n\tubuf = u;\n\tpos *= sizeof(reg);\n\tcount *= sizeof(reg);\n\treturn user_regset_copyout_zero(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\tPT_REGS_COUNT * sizeof(reg), -1);\n}\n\nstatic int gpr32_set(struct task_struct *target,\n\t\t     const struct user_regset *regset,\n\t\t     unsigned int pos, unsigned int count,\n\t\t     const void *kbuf, const void __user *ubuf)\n{\n\tunsigned long *regs = &target->thread.regs->gpr[0];\n\tconst compat_ulong_t *k = kbuf;\n\tconst compat_ulong_t __user *u = ubuf;\n\tcompat_ulong_t reg;\n\n\tif (target->thread.regs == NULL)\n\t\treturn -EIO;\n\n\tCHECK_FULL_REGS(target->thread.regs);\n\n\tpos /= sizeof(reg);\n\tcount /= sizeof(reg);\n\n\tif (kbuf)\n\t\tfor (; count > 0 && pos < PT_MSR; --count)\n\t\t\tregs[pos++] = *k++;\n\telse\n\t\tfor (; count > 0 && pos < PT_MSR; --count) {\n\t\t\tif (__get_user(reg, u++))\n\t\t\t\treturn -EFAULT;\n\t\t\tregs[pos++] = reg;\n\t\t}\n\n\n\tif (count > 0 && pos == PT_MSR) {\n\t\tif (kbuf)\n\t\t\treg = *k++;\n\t\telse if (__get_user(reg, u++))\n\t\t\treturn -EFAULT;\n\t\tset_user_msr(target, reg);\n\t\t++pos;\n\t\t--count;\n\t}\n\n\tif (kbuf) {\n\t\tfor (; count > 0 && pos <= PT_MAX_PUT_REG; --count)\n\t\t\tregs[pos++] = *k++;\n\t\tfor (; count > 0 && pos < PT_TRAP; --count, ++pos)\n\t\t\t++k;\n\t} else {\n\t\tfor (; count > 0 && pos <= PT_MAX_PUT_REG; --count) {\n\t\t\tif (__get_user(reg, u++))\n\t\t\t\treturn -EFAULT;\n\t\t\tregs[pos++] = reg;\n\t\t}\n\t\tfor (; count > 0 && pos < PT_TRAP; --count, ++pos)\n\t\t\tif (__get_user(reg, u++))\n\t\t\t\treturn -EFAULT;\n\t}\n\n\tif (count > 0 && pos == PT_TRAP) {\n\t\tif (kbuf)\n\t\t\treg = *k++;\n\t\telse if (__get_user(reg, u++))\n\t\t\treturn -EFAULT;\n\t\tset_user_trap(target, reg);\n\t\t++pos;\n\t\t--count;\n\t}\n\n\tkbuf = k;\n\tubuf = u;\n\tpos *= sizeof(reg);\n\tcount *= sizeof(reg);\n\treturn user_regset_copyin_ignore(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t (PT_TRAP + 1) * sizeof(reg), -1);\n}\n\n/*\n * These are the regset flavors matching the CONFIG_PPC32 native set.\n */\nstatic const struct user_regset compat_regsets[] = {\n\t[REGSET_GPR] = {\n\t\t.core_note_type = NT_PRSTATUS, .n = ELF_NGREG,\n\t\t.size = sizeof(compat_long_t), .align = sizeof(compat_long_t),\n\t\t.get = gpr32_get, .set = gpr32_set\n\t},\n\t[REGSET_FPR] = {\n\t\t.core_note_type = NT_PRFPREG, .n = ELF_NFPREG,\n\t\t.size = sizeof(double), .align = sizeof(double),\n\t\t.get = fpr_get, .set = fpr_set\n\t},\n#ifdef CONFIG_ALTIVEC\n\t[REGSET_VMX] = {\n\t\t.core_note_type = NT_PPC_VMX, .n = 34,\n\t\t.size = sizeof(vector128), .align = sizeof(vector128),\n\t\t.active = vr_active, .get = vr_get, .set = vr_set\n\t},\n#endif\n#ifdef CONFIG_SPE\n\t[REGSET_SPE] = {\n\t\t.core_note_type = NT_PPC_SPE, .n = 35,\n\t\t.size = sizeof(u32), .align = sizeof(u32),\n\t\t.active = evr_active, .get = evr_get, .set = evr_set\n\t},\n#endif\n};\n\nstatic const struct user_regset_view user_ppc_compat_view = {\n\t.name = \"ppc\", .e_machine = EM_PPC, .ei_osabi = ELF_OSABI,\n\t.regsets = compat_regsets, .n = ARRAY_SIZE(compat_regsets)\n};\n#endif\t/* CONFIG_PPC64 */\n\nconst struct user_regset_view *task_user_regset_view(struct task_struct *task)\n{\n#ifdef CONFIG_PPC64\n\tif (test_tsk_thread_flag(task, TIF_32BIT))\n\t\treturn &user_ppc_compat_view;\n#endif\n\treturn &user_ppc_native_view;\n}\n\n\nvoid user_enable_single_step(struct task_struct *task)\n{\n\tstruct pt_regs *regs = task->thread.regs;\n\n\tif (regs != NULL) {\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n\t\ttask->thread.dbcr0 &= ~DBCR0_BT;\n\t\ttask->thread.dbcr0 |= DBCR0_IDM | DBCR0_IC;\n\t\tregs->msr |= MSR_DE;\n#else\n\t\tregs->msr &= ~MSR_BE;\n\t\tregs->msr |= MSR_SE;\n#endif\n\t}\n\tset_tsk_thread_flag(task, TIF_SINGLESTEP);\n}\n\nvoid user_enable_block_step(struct task_struct *task)\n{\n\tstruct pt_regs *regs = task->thread.regs;\n\n\tif (regs != NULL) {\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n\t\ttask->thread.dbcr0 &= ~DBCR0_IC;\n\t\ttask->thread.dbcr0 = DBCR0_IDM | DBCR0_BT;\n\t\tregs->msr |= MSR_DE;\n#else\n\t\tregs->msr &= ~MSR_SE;\n\t\tregs->msr |= MSR_BE;\n#endif\n\t}\n\tset_tsk_thread_flag(task, TIF_SINGLESTEP);\n}\n\nvoid user_disable_single_step(struct task_struct *task)\n{\n\tstruct pt_regs *regs = task->thread.regs;\n\n\tif (regs != NULL) {\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n\t\t/*\n\t\t * The logic to disable single stepping should be as\n\t\t * simple as turning off the Instruction Complete flag.\n\t\t * And, after doing so, if all debug flags are off, turn\n\t\t * off DBCR0(IDM) and MSR(DE) .... Torez\n\t\t */\n\t\ttask->thread.dbcr0 &= ~DBCR0_IC;\n\t\t/*\n\t\t * Test to see if any of the DBCR_ACTIVE_EVENTS bits are set.\n\t\t */\n\t\tif (!DBCR_ACTIVE_EVENTS(task->thread.dbcr0,\n\t\t\t\t\ttask->thread.dbcr1)) {\n\t\t\t/*\n\t\t\t * All debug events were off.....\n\t\t\t */\n\t\t\ttask->thread.dbcr0 &= ~DBCR0_IDM;\n\t\t\tregs->msr &= ~MSR_DE;\n\t\t}\n#else\n\t\tregs->msr &= ~(MSR_SE | MSR_BE);\n#endif\n\t}\n\tclear_tsk_thread_flag(task, TIF_SINGLESTEP);\n}\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\nvoid ptrace_triggered(struct perf_event *bp,\n\t\t      struct perf_sample_data *data, struct pt_regs *regs)\n{\n\tstruct perf_event_attr attr;\n\n\t/*\n\t * Disable the breakpoint request here since ptrace has defined a\n\t * one-shot behaviour for breakpoint exceptions in PPC64.\n\t * The SIGTRAP signal is generated automatically for us in do_dabr().\n\t * We don't have to do anything about that here\n\t */\n\tattr = bp->attr;\n\tattr.disabled = true;\n\tmodify_user_hw_breakpoint(bp, &attr);\n}\n#endif /* CONFIG_HAVE_HW_BREAKPOINT */\n\nint ptrace_set_debugreg(struct task_struct *task, unsigned long addr,\n\t\t\t       unsigned long data)\n{\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\tint ret;\n\tstruct thread_struct *thread = &(task->thread);\n\tstruct perf_event *bp;\n\tstruct perf_event_attr attr;\n#endif /* CONFIG_HAVE_HW_BREAKPOINT */\n\n\t/* For ppc64 we support one DABR and no IABR's at the moment (ppc64).\n\t *  For embedded processors we support one DAC and no IAC's at the\n\t *  moment.\n\t */\n\tif (addr > 0)\n\t\treturn -EINVAL;\n\n\t/* The bottom 3 bits in dabr are flags */\n\tif ((data & ~0x7UL) >= TASK_SIZE)\n\t\treturn -EIO;\n\n#ifndef CONFIG_PPC_ADV_DEBUG_REGS\n\t/* For processors using DABR (i.e. 970), the bottom 3 bits are flags.\n\t *  It was assumed, on previous implementations, that 3 bits were\n\t *  passed together with the data address, fitting the design of the\n\t *  DABR register, as follows:\n\t *\n\t *  bit 0: Read flag\n\t *  bit 1: Write flag\n\t *  bit 2: Breakpoint translation\n\t *\n\t *  Thus, we use them here as so.\n\t */\n\n\t/* Ensure breakpoint translation bit is set */\n\tif (data && !(data & DABR_TRANSLATION))\n\t\treturn -EIO;\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\tif (ptrace_get_breakpoints(task) < 0)\n\t\treturn -ESRCH;\n\n\tbp = thread->ptrace_bps[0];\n\tif ((!data) || !(data & (DABR_DATA_WRITE | DABR_DATA_READ))) {\n\t\tif (bp) {\n\t\t\tunregister_hw_breakpoint(bp);\n\t\t\tthread->ptrace_bps[0] = NULL;\n\t\t}\n\t\tptrace_put_breakpoints(task);\n\t\treturn 0;\n\t}\n\tif (bp) {\n\t\tattr = bp->attr;\n\t\tattr.bp_addr = data & ~HW_BREAKPOINT_ALIGN;\n\t\tarch_bp_generic_fields(data &\n\t\t\t\t\t(DABR_DATA_WRITE | DABR_DATA_READ),\n\t\t\t\t\t\t\t&attr.bp_type);\n\t\tret =  modify_user_hw_breakpoint(bp, &attr);\n\t\tif (ret) {\n\t\t\tptrace_put_breakpoints(task);\n\t\t\treturn ret;\n\t\t}\n\t\tthread->ptrace_bps[0] = bp;\n\t\tptrace_put_breakpoints(task);\n\t\tthread->dabr = data;\n\t\treturn 0;\n\t}\n\n\t/* Create a new breakpoint request if one doesn't exist already */\n\thw_breakpoint_init(&attr);\n\tattr.bp_addr = data & ~HW_BREAKPOINT_ALIGN;\n\tarch_bp_generic_fields(data & (DABR_DATA_WRITE | DABR_DATA_READ),\n\t\t\t\t\t\t\t\t&attr.bp_type);\n\n\tthread->ptrace_bps[0] = bp = register_user_hw_breakpoint(&attr,\n\t\t\t\t\t\t\tptrace_triggered, task);\n\tif (IS_ERR(bp)) {\n\t\tthread->ptrace_bps[0] = NULL;\n\t\tptrace_put_breakpoints(task);\n\t\treturn PTR_ERR(bp);\n\t}\n\n\tptrace_put_breakpoints(task);\n\n#endif /* CONFIG_HAVE_HW_BREAKPOINT */\n\n\t/* Move contents to the DABR register */\n\ttask->thread.dabr = data;\n#else /* CONFIG_PPC_ADV_DEBUG_REGS */\n\t/* As described above, it was assumed 3 bits were passed with the data\n\t *  address, but we will assume only the mode bits will be passed\n\t *  as to not cause alignment restrictions for DAC-based processors.\n\t */\n\n\t/* DAC's hold the whole address without any mode flags */\n\ttask->thread.dac1 = data & ~0x3UL;\n\n\tif (task->thread.dac1 == 0) {\n\t\tdbcr_dac(task) &= ~(DBCR_DAC1R | DBCR_DAC1W);\n\t\tif (!DBCR_ACTIVE_EVENTS(task->thread.dbcr0,\n\t\t\t\t\ttask->thread.dbcr1)) {\n\t\t\ttask->thread.regs->msr &= ~MSR_DE;\n\t\t\ttask->thread.dbcr0 &= ~DBCR0_IDM;\n\t\t}\n\t\treturn 0;\n\t}\n\n\t/* Read or Write bits must be set */\n\n\tif (!(data & 0x3UL))\n\t\treturn -EINVAL;\n\n\t/* Set the Internal Debugging flag (IDM bit 1) for the DBCR0\n\t   register */\n\ttask->thread.dbcr0 |= DBCR0_IDM;\n\n\t/* Check for write and read flags and set DBCR0\n\t   accordingly */\n\tdbcr_dac(task) &= ~(DBCR_DAC1R|DBCR_DAC1W);\n\tif (data & 0x1UL)\n\t\tdbcr_dac(task) |= DBCR_DAC1R;\n\tif (data & 0x2UL)\n\t\tdbcr_dac(task) |= DBCR_DAC1W;\n\ttask->thread.regs->msr |= MSR_DE;\n#endif /* CONFIG_PPC_ADV_DEBUG_REGS */\n\treturn 0;\n}\n\n/*\n * Called by kernel/ptrace.c when detaching..\n *\n * Make sure single step bits etc are not set.\n */\nvoid ptrace_disable(struct task_struct *child)\n{\n\t/* make sure the single step bit is not set. */\n\tuser_disable_single_step(child);\n}\n\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\nstatic long set_intruction_bp(struct task_struct *child,\n\t\t\t      struct ppc_hw_breakpoint *bp_info)\n{\n\tint slot;\n\tint slot1_in_use = ((child->thread.dbcr0 & DBCR0_IAC1) != 0);\n\tint slot2_in_use = ((child->thread.dbcr0 & DBCR0_IAC2) != 0);\n\tint slot3_in_use = ((child->thread.dbcr0 & DBCR0_IAC3) != 0);\n\tint slot4_in_use = ((child->thread.dbcr0 & DBCR0_IAC4) != 0);\n\n\tif (dbcr_iac_range(child) & DBCR_IAC12MODE)\n\t\tslot2_in_use = 1;\n\tif (dbcr_iac_range(child) & DBCR_IAC34MODE)\n\t\tslot4_in_use = 1;\n\n\tif (bp_info->addr >= TASK_SIZE)\n\t\treturn -EIO;\n\n\tif (bp_info->addr_mode != PPC_BREAKPOINT_MODE_EXACT) {\n\n\t\t/* Make sure range is valid. */\n\t\tif (bp_info->addr2 >= TASK_SIZE)\n\t\t\treturn -EIO;\n\n\t\t/* We need a pair of IAC regsisters */\n\t\tif ((!slot1_in_use) && (!slot2_in_use)) {\n\t\t\tslot = 1;\n\t\t\tchild->thread.iac1 = bp_info->addr;\n\t\t\tchild->thread.iac2 = bp_info->addr2;\n\t\t\tchild->thread.dbcr0 |= DBCR0_IAC1;\n\t\t\tif (bp_info->addr_mode ==\n\t\t\t\t\tPPC_BREAKPOINT_MODE_RANGE_EXCLUSIVE)\n\t\t\t\tdbcr_iac_range(child) |= DBCR_IAC12X;\n\t\t\telse\n\t\t\t\tdbcr_iac_range(child) |= DBCR_IAC12I;\n#if CONFIG_PPC_ADV_DEBUG_IACS > 2\n\t\t} else if ((!slot3_in_use) && (!slot4_in_use)) {\n\t\t\tslot = 3;\n\t\t\tchild->thread.iac3 = bp_info->addr;\n\t\t\tchild->thread.iac4 = bp_info->addr2;\n\t\t\tchild->thread.dbcr0 |= DBCR0_IAC3;\n\t\t\tif (bp_info->addr_mode ==\n\t\t\t\t\tPPC_BREAKPOINT_MODE_RANGE_EXCLUSIVE)\n\t\t\t\tdbcr_iac_range(child) |= DBCR_IAC34X;\n\t\t\telse\n\t\t\t\tdbcr_iac_range(child) |= DBCR_IAC34I;\n#endif\n\t\t} else\n\t\t\treturn -ENOSPC;\n\t} else {\n\t\t/* We only need one.  If possible leave a pair free in\n\t\t * case a range is needed later\n\t\t */\n\t\tif (!slot1_in_use) {\n\t\t\t/*\n\t\t\t * Don't use iac1 if iac1-iac2 are free and either\n\t\t\t * iac3 or iac4 (but not both) are free\n\t\t\t */\n\t\t\tif (slot2_in_use || (slot3_in_use == slot4_in_use)) {\n\t\t\t\tslot = 1;\n\t\t\t\tchild->thread.iac1 = bp_info->addr;\n\t\t\t\tchild->thread.dbcr0 |= DBCR0_IAC1;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tif (!slot2_in_use) {\n\t\t\tslot = 2;\n\t\t\tchild->thread.iac2 = bp_info->addr;\n\t\t\tchild->thread.dbcr0 |= DBCR0_IAC2;\n#if CONFIG_PPC_ADV_DEBUG_IACS > 2\n\t\t} else if (!slot3_in_use) {\n\t\t\tslot = 3;\n\t\t\tchild->thread.iac3 = bp_info->addr;\n\t\t\tchild->thread.dbcr0 |= DBCR0_IAC3;\n\t\t} else if (!slot4_in_use) {\n\t\t\tslot = 4;\n\t\t\tchild->thread.iac4 = bp_info->addr;\n\t\t\tchild->thread.dbcr0 |= DBCR0_IAC4;\n#endif\n\t\t} else\n\t\t\treturn -ENOSPC;\n\t}\nout:\n\tchild->thread.dbcr0 |= DBCR0_IDM;\n\tchild->thread.regs->msr |= MSR_DE;\n\n\treturn slot;\n}\n\nstatic int del_instruction_bp(struct task_struct *child, int slot)\n{\n\tswitch (slot) {\n\tcase 1:\n\t\tif ((child->thread.dbcr0 & DBCR0_IAC1) == 0)\n\t\t\treturn -ENOENT;\n\n\t\tif (dbcr_iac_range(child) & DBCR_IAC12MODE) {\n\t\t\t/* address range - clear slots 1 & 2 */\n\t\t\tchild->thread.iac2 = 0;\n\t\t\tdbcr_iac_range(child) &= ~DBCR_IAC12MODE;\n\t\t}\n\t\tchild->thread.iac1 = 0;\n\t\tchild->thread.dbcr0 &= ~DBCR0_IAC1;\n\t\tbreak;\n\tcase 2:\n\t\tif ((child->thread.dbcr0 & DBCR0_IAC2) == 0)\n\t\t\treturn -ENOENT;\n\n\t\tif (dbcr_iac_range(child) & DBCR_IAC12MODE)\n\t\t\t/* used in a range */\n\t\t\treturn -EINVAL;\n\t\tchild->thread.iac2 = 0;\n\t\tchild->thread.dbcr0 &= ~DBCR0_IAC2;\n\t\tbreak;\n#if CONFIG_PPC_ADV_DEBUG_IACS > 2\n\tcase 3:\n\t\tif ((child->thread.dbcr0 & DBCR0_IAC3) == 0)\n\t\t\treturn -ENOENT;\n\n\t\tif (dbcr_iac_range(child) & DBCR_IAC34MODE) {\n\t\t\t/* address range - clear slots 3 & 4 */\n\t\t\tchild->thread.iac4 = 0;\n\t\t\tdbcr_iac_range(child) &= ~DBCR_IAC34MODE;\n\t\t}\n\t\tchild->thread.iac3 = 0;\n\t\tchild->thread.dbcr0 &= ~DBCR0_IAC3;\n\t\tbreak;\n\tcase 4:\n\t\tif ((child->thread.dbcr0 & DBCR0_IAC4) == 0)\n\t\t\treturn -ENOENT;\n\n\t\tif (dbcr_iac_range(child) & DBCR_IAC34MODE)\n\t\t\t/* Used in a range */\n\t\t\treturn -EINVAL;\n\t\tchild->thread.iac4 = 0;\n\t\tchild->thread.dbcr0 &= ~DBCR0_IAC4;\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic int set_dac(struct task_struct *child, struct ppc_hw_breakpoint *bp_info)\n{\n\tint byte_enable =\n\t\t(bp_info->condition_mode >> PPC_BREAKPOINT_CONDITION_BE_SHIFT)\n\t\t& 0xf;\n\tint condition_mode =\n\t\tbp_info->condition_mode & PPC_BREAKPOINT_CONDITION_MODE;\n\tint slot;\n\n\tif (byte_enable && (condition_mode == 0))\n\t\treturn -EINVAL;\n\n\tif (bp_info->addr >= TASK_SIZE)\n\t\treturn -EIO;\n\n\tif ((dbcr_dac(child) & (DBCR_DAC1R | DBCR_DAC1W)) == 0) {\n\t\tslot = 1;\n\t\tif (bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_READ)\n\t\t\tdbcr_dac(child) |= DBCR_DAC1R;\n\t\tif (bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_WRITE)\n\t\t\tdbcr_dac(child) |= DBCR_DAC1W;\n\t\tchild->thread.dac1 = (unsigned long)bp_info->addr;\n#if CONFIG_PPC_ADV_DEBUG_DVCS > 0\n\t\tif (byte_enable) {\n\t\t\tchild->thread.dvc1 =\n\t\t\t\t(unsigned long)bp_info->condition_value;\n\t\t\tchild->thread.dbcr2 |=\n\t\t\t\t((byte_enable << DBCR2_DVC1BE_SHIFT) |\n\t\t\t\t (condition_mode << DBCR2_DVC1M_SHIFT));\n\t\t}\n#endif\n#ifdef CONFIG_PPC_ADV_DEBUG_DAC_RANGE\n\t} else if (child->thread.dbcr2 & DBCR2_DAC12MODE) {\n\t\t/* Both dac1 and dac2 are part of a range */\n\t\treturn -ENOSPC;\n#endif\n\t} else if ((dbcr_dac(child) & (DBCR_DAC2R | DBCR_DAC2W)) == 0) {\n\t\tslot = 2;\n\t\tif (bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_READ)\n\t\t\tdbcr_dac(child) |= DBCR_DAC2R;\n\t\tif (bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_WRITE)\n\t\t\tdbcr_dac(child) |= DBCR_DAC2W;\n\t\tchild->thread.dac2 = (unsigned long)bp_info->addr;\n#if CONFIG_PPC_ADV_DEBUG_DVCS > 0\n\t\tif (byte_enable) {\n\t\t\tchild->thread.dvc2 =\n\t\t\t\t(unsigned long)bp_info->condition_value;\n\t\t\tchild->thread.dbcr2 |=\n\t\t\t\t((byte_enable << DBCR2_DVC2BE_SHIFT) |\n\t\t\t\t (condition_mode << DBCR2_DVC2M_SHIFT));\n\t\t}\n#endif\n\t} else\n\t\treturn -ENOSPC;\n\tchild->thread.dbcr0 |= DBCR0_IDM;\n\tchild->thread.regs->msr |= MSR_DE;\n\n\treturn slot + 4;\n}\n\nstatic int del_dac(struct task_struct *child, int slot)\n{\n\tif (slot == 1) {\n\t\tif ((dbcr_dac(child) & (DBCR_DAC1R | DBCR_DAC1W)) == 0)\n\t\t\treturn -ENOENT;\n\n\t\tchild->thread.dac1 = 0;\n\t\tdbcr_dac(child) &= ~(DBCR_DAC1R | DBCR_DAC1W);\n#ifdef CONFIG_PPC_ADV_DEBUG_DAC_RANGE\n\t\tif (child->thread.dbcr2 & DBCR2_DAC12MODE) {\n\t\t\tchild->thread.dac2 = 0;\n\t\t\tchild->thread.dbcr2 &= ~DBCR2_DAC12MODE;\n\t\t}\n\t\tchild->thread.dbcr2 &= ~(DBCR2_DVC1M | DBCR2_DVC1BE);\n#endif\n#if CONFIG_PPC_ADV_DEBUG_DVCS > 0\n\t\tchild->thread.dvc1 = 0;\n#endif\n\t} else if (slot == 2) {\n\t\tif ((dbcr_dac(child) & (DBCR_DAC2R | DBCR_DAC2W)) == 0)\n\t\t\treturn -ENOENT;\n\n#ifdef CONFIG_PPC_ADV_DEBUG_DAC_RANGE\n\t\tif (child->thread.dbcr2 & DBCR2_DAC12MODE)\n\t\t\t/* Part of a range */\n\t\t\treturn -EINVAL;\n\t\tchild->thread.dbcr2 &= ~(DBCR2_DVC2M | DBCR2_DVC2BE);\n#endif\n#if CONFIG_PPC_ADV_DEBUG_DVCS > 0\n\t\tchild->thread.dvc2 = 0;\n#endif\n\t\tchild->thread.dac2 = 0;\n\t\tdbcr_dac(child) &= ~(DBCR_DAC2R | DBCR_DAC2W);\n\t} else\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n#endif /* CONFIG_PPC_ADV_DEBUG_REGS */\n\n#ifdef CONFIG_PPC_ADV_DEBUG_DAC_RANGE\nstatic int set_dac_range(struct task_struct *child,\n\t\t\t struct ppc_hw_breakpoint *bp_info)\n{\n\tint mode = bp_info->addr_mode & PPC_BREAKPOINT_MODE_MASK;\n\n\t/* We don't allow range watchpoints to be used with DVC */\n\tif (bp_info->condition_mode)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Best effort to verify the address range.  The user/supervisor bits\n\t * prevent trapping in kernel space, but let's fail on an obvious bad\n\t * range.  The simple test on the mask is not fool-proof, and any\n\t * exclusive range will spill over into kernel space.\n\t */\n\tif (bp_info->addr >= TASK_SIZE)\n\t\treturn -EIO;\n\tif (mode == PPC_BREAKPOINT_MODE_MASK) {\n\t\t/*\n\t\t * dac2 is a bitmask.  Don't allow a mask that makes a\n\t\t * kernel space address from a valid dac1 value\n\t\t */\n\t\tif (~((unsigned long)bp_info->addr2) >= TASK_SIZE)\n\t\t\treturn -EIO;\n\t} else {\n\t\t/*\n\t\t * For range breakpoints, addr2 must also be a valid address\n\t\t */\n\t\tif (bp_info->addr2 >= TASK_SIZE)\n\t\t\treturn -EIO;\n\t}\n\n\tif (child->thread.dbcr0 &\n\t    (DBCR0_DAC1R | DBCR0_DAC1W | DBCR0_DAC2R | DBCR0_DAC2W))\n\t\treturn -ENOSPC;\n\n\tif (bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_READ)\n\t\tchild->thread.dbcr0 |= (DBCR0_DAC1R | DBCR0_IDM);\n\tif (bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_WRITE)\n\t\tchild->thread.dbcr0 |= (DBCR0_DAC1W | DBCR0_IDM);\n\tchild->thread.dac1 = bp_info->addr;\n\tchild->thread.dac2 = bp_info->addr2;\n\tif (mode == PPC_BREAKPOINT_MODE_RANGE_INCLUSIVE)\n\t\tchild->thread.dbcr2  |= DBCR2_DAC12M;\n\telse if (mode == PPC_BREAKPOINT_MODE_RANGE_EXCLUSIVE)\n\t\tchild->thread.dbcr2  |= DBCR2_DAC12MX;\n\telse\t/* PPC_BREAKPOINT_MODE_MASK */\n\t\tchild->thread.dbcr2  |= DBCR2_DAC12MM;\n\tchild->thread.regs->msr |= MSR_DE;\n\n\treturn 5;\n}\n#endif /* CONFIG_PPC_ADV_DEBUG_DAC_RANGE */\n\nstatic long ppc_set_hwdebug(struct task_struct *child,\n\t\t     struct ppc_hw_breakpoint *bp_info)\n{\n#ifndef CONFIG_PPC_ADV_DEBUG_REGS\n\tunsigned long dabr;\n#endif\n\n\tif (bp_info->version != 1)\n\t\treturn -ENOTSUPP;\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n\t/*\n\t * Check for invalid flags and combinations\n\t */\n\tif ((bp_info->trigger_type == 0) ||\n\t    (bp_info->trigger_type & ~(PPC_BREAKPOINT_TRIGGER_EXECUTE |\n\t\t\t\t       PPC_BREAKPOINT_TRIGGER_RW)) ||\n\t    (bp_info->addr_mode & ~PPC_BREAKPOINT_MODE_MASK) ||\n\t    (bp_info->condition_mode &\n\t     ~(PPC_BREAKPOINT_CONDITION_MODE |\n\t       PPC_BREAKPOINT_CONDITION_BE_ALL)))\n\t\treturn -EINVAL;\n#if CONFIG_PPC_ADV_DEBUG_DVCS == 0\n\tif (bp_info->condition_mode != PPC_BREAKPOINT_CONDITION_NONE)\n\t\treturn -EINVAL;\n#endif\n\n\tif (bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_EXECUTE) {\n\t\tif ((bp_info->trigger_type != PPC_BREAKPOINT_TRIGGER_EXECUTE) ||\n\t\t    (bp_info->condition_mode != PPC_BREAKPOINT_CONDITION_NONE))\n\t\t\treturn -EINVAL;\n\t\treturn set_intruction_bp(child, bp_info);\n\t}\n\tif (bp_info->addr_mode == PPC_BREAKPOINT_MODE_EXACT)\n\t\treturn set_dac(child, bp_info);\n\n#ifdef CONFIG_PPC_ADV_DEBUG_DAC_RANGE\n\treturn set_dac_range(child, bp_info);\n#else\n\treturn -EINVAL;\n#endif\n#else /* !CONFIG_PPC_ADV_DEBUG_DVCS */\n\t/*\n\t * We only support one data breakpoint\n\t */\n\tif ((bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_RW) == 0 ||\n\t    (bp_info->trigger_type & ~PPC_BREAKPOINT_TRIGGER_RW) != 0 ||\n\t    bp_info->addr_mode != PPC_BREAKPOINT_MODE_EXACT ||\n\t    bp_info->condition_mode != PPC_BREAKPOINT_CONDITION_NONE)\n\t\treturn -EINVAL;\n\n\tif (child->thread.dabr)\n\t\treturn -ENOSPC;\n\n\tif ((unsigned long)bp_info->addr >= TASK_SIZE)\n\t\treturn -EIO;\n\n\tdabr = (unsigned long)bp_info->addr & ~7UL;\n\tdabr |= DABR_TRANSLATION;\n\tif (bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_READ)\n\t\tdabr |= DABR_DATA_READ;\n\tif (bp_info->trigger_type & PPC_BREAKPOINT_TRIGGER_WRITE)\n\t\tdabr |= DABR_DATA_WRITE;\n\n\tchild->thread.dabr = dabr;\n\n\treturn 1;\n#endif /* !CONFIG_PPC_ADV_DEBUG_DVCS */\n}\n\nstatic long ppc_del_hwdebug(struct task_struct *child, long addr, long data)\n{\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n\tint rc;\n\n\tif (data <= 4)\n\t\trc = del_instruction_bp(child, (int)data);\n\telse\n\t\trc = del_dac(child, (int)data - 4);\n\n\tif (!rc) {\n\t\tif (!DBCR_ACTIVE_EVENTS(child->thread.dbcr0,\n\t\t\t\t\tchild->thread.dbcr1)) {\n\t\t\tchild->thread.dbcr0 &= ~DBCR0_IDM;\n\t\t\tchild->thread.regs->msr &= ~MSR_DE;\n\t\t}\n\t}\n\treturn rc;\n#else\n\tif (data != 1)\n\t\treturn -EINVAL;\n\tif (child->thread.dabr == 0)\n\t\treturn -ENOENT;\n\n\tchild->thread.dabr = 0;\n\n\treturn 0;\n#endif\n}\n\n/*\n * Here are the old \"legacy\" powerpc specific getregs/setregs ptrace calls,\n * we mark them as obsolete now, they will be removed in a future version\n */\nstatic long arch_ptrace_old(struct task_struct *child, long request,\n\t\t\t    unsigned long addr, unsigned long data)\n{\n\tvoid __user *datavp = (void __user *) data;\n\n\tswitch (request) {\n\tcase PPC_PTRACE_GETREGS:\t/* Get GPRs 0 - 31. */\n\t\treturn copy_regset_to_user(child, &user_ppc_native_view,\n\t\t\t\t\t   REGSET_GPR, 0, 32 * sizeof(long),\n\t\t\t\t\t   datavp);\n\n\tcase PPC_PTRACE_SETREGS:\t/* Set GPRs 0 - 31. */\n\t\treturn copy_regset_from_user(child, &user_ppc_native_view,\n\t\t\t\t\t     REGSET_GPR, 0, 32 * sizeof(long),\n\t\t\t\t\t     datavp);\n\n\tcase PPC_PTRACE_GETFPREGS:\t/* Get FPRs 0 - 31. */\n\t\treturn copy_regset_to_user(child, &user_ppc_native_view,\n\t\t\t\t\t   REGSET_FPR, 0, 32 * sizeof(double),\n\t\t\t\t\t   datavp);\n\n\tcase PPC_PTRACE_SETFPREGS:\t/* Set FPRs 0 - 31. */\n\t\treturn copy_regset_from_user(child, &user_ppc_native_view,\n\t\t\t\t\t     REGSET_FPR, 0, 32 * sizeof(double),\n\t\t\t\t\t     datavp);\n\t}\n\n\treturn -EPERM;\n}\n\nlong arch_ptrace(struct task_struct *child, long request,\n\t\t unsigned long addr, unsigned long data)\n{\n\tint ret = -EPERM;\n\tvoid __user *datavp = (void __user *) data;\n\tunsigned long __user *datalp = datavp;\n\n\tswitch (request) {\n\t/* read the word at location addr in the USER area. */\n\tcase PTRACE_PEEKUSR: {\n\t\tunsigned long index, tmp;\n\n\t\tret = -EIO;\n\t\t/* convert to index and check */\n#ifdef CONFIG_PPC32\n\t\tindex = addr >> 2;\n\t\tif ((addr & 3) || (index > PT_FPSCR)\n\t\t    || (child->thread.regs == NULL))\n#else\n\t\tindex = addr >> 3;\n\t\tif ((addr & 7) || (index > PT_FPSCR))\n#endif\n\t\t\tbreak;\n\n\t\tCHECK_FULL_REGS(child->thread.regs);\n\t\tif (index < PT_FPR0) {\n\t\t\ttmp = ptrace_get_reg(child, (int) index);\n\t\t} else {\n\t\t\tflush_fp_to_thread(child);\n\t\t\ttmp = ((unsigned long *)child->thread.fpr)\n\t\t\t\t[TS_FPRWIDTH * (index - PT_FPR0)];\n\t\t}\n\t\tret = put_user(tmp, datalp);\n\t\tbreak;\n\t}\n\n\t/* write the word at location addr in the USER area */\n\tcase PTRACE_POKEUSR: {\n\t\tunsigned long index;\n\n\t\tret = -EIO;\n\t\t/* convert to index and check */\n#ifdef CONFIG_PPC32\n\t\tindex = addr >> 2;\n\t\tif ((addr & 3) || (index > PT_FPSCR)\n\t\t    || (child->thread.regs == NULL))\n#else\n\t\tindex = addr >> 3;\n\t\tif ((addr & 7) || (index > PT_FPSCR))\n#endif\n\t\t\tbreak;\n\n\t\tCHECK_FULL_REGS(child->thread.regs);\n\t\tif (index < PT_FPR0) {\n\t\t\tret = ptrace_put_reg(child, index, data);\n\t\t} else {\n\t\t\tflush_fp_to_thread(child);\n\t\t\t((unsigned long *)child->thread.fpr)\n\t\t\t\t[TS_FPRWIDTH * (index - PT_FPR0)] = data;\n\t\t\tret = 0;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase PPC_PTRACE_GETHWDBGINFO: {\n\t\tstruct ppc_debug_info dbginfo;\n\n\t\tdbginfo.version = 1;\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n\t\tdbginfo.num_instruction_bps = CONFIG_PPC_ADV_DEBUG_IACS;\n\t\tdbginfo.num_data_bps = CONFIG_PPC_ADV_DEBUG_DACS;\n\t\tdbginfo.num_condition_regs = CONFIG_PPC_ADV_DEBUG_DVCS;\n\t\tdbginfo.data_bp_alignment = 4;\n\t\tdbginfo.sizeof_condition = 4;\n\t\tdbginfo.features = PPC_DEBUG_FEATURE_INSN_BP_RANGE |\n\t\t\t\t   PPC_DEBUG_FEATURE_INSN_BP_MASK;\n#ifdef CONFIG_PPC_ADV_DEBUG_DAC_RANGE\n\t\tdbginfo.features |=\n\t\t\t\t   PPC_DEBUG_FEATURE_DATA_BP_RANGE |\n\t\t\t\t   PPC_DEBUG_FEATURE_DATA_BP_MASK;\n#endif\n#else /* !CONFIG_PPC_ADV_DEBUG_REGS */\n\t\tdbginfo.num_instruction_bps = 0;\n\t\tdbginfo.num_data_bps = 1;\n\t\tdbginfo.num_condition_regs = 0;\n#ifdef CONFIG_PPC64\n\t\tdbginfo.data_bp_alignment = 8;\n#else\n\t\tdbginfo.data_bp_alignment = 4;\n#endif\n\t\tdbginfo.sizeof_condition = 0;\n\t\tdbginfo.features = 0;\n#endif /* CONFIG_PPC_ADV_DEBUG_REGS */\n\n\t\tif (!access_ok(VERIFY_WRITE, datavp,\n\t\t\t       sizeof(struct ppc_debug_info)))\n\t\t\treturn -EFAULT;\n\t\tret = __copy_to_user(datavp, &dbginfo,\n\t\t\t\t     sizeof(struct ppc_debug_info)) ?\n\t\t      -EFAULT : 0;\n\t\tbreak;\n\t}\n\n\tcase PPC_PTRACE_SETHWDEBUG: {\n\t\tstruct ppc_hw_breakpoint bp_info;\n\n\t\tif (!access_ok(VERIFY_READ, datavp,\n\t\t\t       sizeof(struct ppc_hw_breakpoint)))\n\t\t\treturn -EFAULT;\n\t\tret = __copy_from_user(&bp_info, datavp,\n\t\t\t\t       sizeof(struct ppc_hw_breakpoint)) ?\n\t\t      -EFAULT : 0;\n\t\tif (!ret)\n\t\t\tret = ppc_set_hwdebug(child, &bp_info);\n\t\tbreak;\n\t}\n\n\tcase PPC_PTRACE_DELHWDEBUG: {\n\t\tret = ppc_del_hwdebug(child, addr, data);\n\t\tbreak;\n\t}\n\n\tcase PTRACE_GET_DEBUGREG: {\n\t\tret = -EINVAL;\n\t\t/* We only support one DABR and no IABRS at the moment */\n\t\tif (addr > 0)\n\t\t\tbreak;\n#ifdef CONFIG_PPC_ADV_DEBUG_REGS\n\t\tret = put_user(child->thread.dac1, datalp);\n#else\n\t\tret = put_user(child->thread.dabr, datalp);\n#endif\n\t\tbreak;\n\t}\n\n\tcase PTRACE_SET_DEBUGREG:\n\t\tret = ptrace_set_debugreg(child, addr, data);\n\t\tbreak;\n\n#ifdef CONFIG_PPC64\n\tcase PTRACE_GETREGS64:\n#endif\n\tcase PTRACE_GETREGS:\t/* Get all pt_regs from the child. */\n\t\treturn copy_regset_to_user(child, &user_ppc_native_view,\n\t\t\t\t\t   REGSET_GPR,\n\t\t\t\t\t   0, sizeof(struct pt_regs),\n\t\t\t\t\t   datavp);\n\n#ifdef CONFIG_PPC64\n\tcase PTRACE_SETREGS64:\n#endif\n\tcase PTRACE_SETREGS:\t/* Set all gp regs in the child. */\n\t\treturn copy_regset_from_user(child, &user_ppc_native_view,\n\t\t\t\t\t     REGSET_GPR,\n\t\t\t\t\t     0, sizeof(struct pt_regs),\n\t\t\t\t\t     datavp);\n\n\tcase PTRACE_GETFPREGS: /* Get the child FPU state (FPR0...31 + FPSCR) */\n\t\treturn copy_regset_to_user(child, &user_ppc_native_view,\n\t\t\t\t\t   REGSET_FPR,\n\t\t\t\t\t   0, sizeof(elf_fpregset_t),\n\t\t\t\t\t   datavp);\n\n\tcase PTRACE_SETFPREGS: /* Set the child FPU state (FPR0...31 + FPSCR) */\n\t\treturn copy_regset_from_user(child, &user_ppc_native_view,\n\t\t\t\t\t     REGSET_FPR,\n\t\t\t\t\t     0, sizeof(elf_fpregset_t),\n\t\t\t\t\t     datavp);\n\n#ifdef CONFIG_ALTIVEC\n\tcase PTRACE_GETVRREGS:\n\t\treturn copy_regset_to_user(child, &user_ppc_native_view,\n\t\t\t\t\t   REGSET_VMX,\n\t\t\t\t\t   0, (33 * sizeof(vector128) +\n\t\t\t\t\t       sizeof(u32)),\n\t\t\t\t\t   datavp);\n\n\tcase PTRACE_SETVRREGS:\n\t\treturn copy_regset_from_user(child, &user_ppc_native_view,\n\t\t\t\t\t     REGSET_VMX,\n\t\t\t\t\t     0, (33 * sizeof(vector128) +\n\t\t\t\t\t\t sizeof(u32)),\n\t\t\t\t\t     datavp);\n#endif\n#ifdef CONFIG_VSX\n\tcase PTRACE_GETVSRREGS:\n\t\treturn copy_regset_to_user(child, &user_ppc_native_view,\n\t\t\t\t\t   REGSET_VSX,\n\t\t\t\t\t   0, 32 * sizeof(double),\n\t\t\t\t\t   datavp);\n\n\tcase PTRACE_SETVSRREGS:\n\t\treturn copy_regset_from_user(child, &user_ppc_native_view,\n\t\t\t\t\t     REGSET_VSX,\n\t\t\t\t\t     0, 32 * sizeof(double),\n\t\t\t\t\t     datavp);\n#endif\n#ifdef CONFIG_SPE\n\tcase PTRACE_GETEVRREGS:\n\t\t/* Get the child spe register state. */\n\t\treturn copy_regset_to_user(child, &user_ppc_native_view,\n\t\t\t\t\t   REGSET_SPE, 0, 35 * sizeof(u32),\n\t\t\t\t\t   datavp);\n\n\tcase PTRACE_SETEVRREGS:\n\t\t/* Set the child spe register state. */\n\t\treturn copy_regset_from_user(child, &user_ppc_native_view,\n\t\t\t\t\t     REGSET_SPE, 0, 35 * sizeof(u32),\n\t\t\t\t\t     datavp);\n#endif\n\n\t/* Old reverse args ptrace callss */\n\tcase PPC_PTRACE_GETREGS: /* Get GPRs 0 - 31. */\n\tcase PPC_PTRACE_SETREGS: /* Set GPRs 0 - 31. */\n\tcase PPC_PTRACE_GETFPREGS: /* Get FPRs 0 - 31. */\n\tcase PPC_PTRACE_SETFPREGS: /* Get FPRs 0 - 31. */\n\t\tret = arch_ptrace_old(child, request, addr, data);\n\t\tbreak;\n\n\tdefault:\n\t\tret = ptrace_request(child, request, addr, data);\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\n/*\n * We must return the syscall number to actually look up in the table.\n * This can be -1L to skip running any syscall at all.\n */\nlong do_syscall_trace_enter(struct pt_regs *regs)\n{\n\tlong ret = 0;\n\n\tsecure_computing(regs->gpr[0]);\n\n\tif (test_thread_flag(TIF_SYSCALL_TRACE) &&\n\t    tracehook_report_syscall_entry(regs))\n\t\t/*\n\t\t * Tracing decided this syscall should not happen.\n\t\t * We'll return a bogus call number to get an ENOSYS\n\t\t * error, but leave the original number in regs->gpr[0].\n\t\t */\n\t\tret = -1L;\n\n\tif (unlikely(test_thread_flag(TIF_SYSCALL_TRACEPOINT)))\n\t\ttrace_sys_enter(regs, regs->gpr[0]);\n\n\tif (unlikely(current->audit_context)) {\n#ifdef CONFIG_PPC64\n\t\tif (!is_32bit_task())\n\t\t\taudit_syscall_entry(AUDIT_ARCH_PPC64,\n\t\t\t\t\t    regs->gpr[0],\n\t\t\t\t\t    regs->gpr[3], regs->gpr[4],\n\t\t\t\t\t    regs->gpr[5], regs->gpr[6]);\n\t\telse\n#endif\n\t\t\taudit_syscall_entry(AUDIT_ARCH_PPC,\n\t\t\t\t\t    regs->gpr[0],\n\t\t\t\t\t    regs->gpr[3] & 0xffffffff,\n\t\t\t\t\t    regs->gpr[4] & 0xffffffff,\n\t\t\t\t\t    regs->gpr[5] & 0xffffffff,\n\t\t\t\t\t    regs->gpr[6] & 0xffffffff);\n\t}\n\n\treturn ret ?: regs->gpr[0];\n}\n\nvoid do_syscall_trace_leave(struct pt_regs *regs)\n{\n\tint step;\n\n\tif (unlikely(current->audit_context))\n\t\taudit_syscall_exit((regs->ccr&0x10000000)?AUDITSC_FAILURE:AUDITSC_SUCCESS,\n\t\t\t\t   regs->result);\n\n\tif (unlikely(test_thread_flag(TIF_SYSCALL_TRACEPOINT)))\n\t\ttrace_sys_exit(regs, regs->result);\n\n\tstep = test_thread_flag(TIF_SINGLESTEP);\n\tif (step || test_thread_flag(TIF_SYSCALL_TRACE))\n\t\ttracehook_report_syscall_exit(regs, step);\n}\n", "/*\n *  PowerPC version\n *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)\n *\n *  Derived from \"arch/i386/mm/fault.c\"\n *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds\n *\n *  Modified by Cort Dougan and Paul Mackerras.\n *\n *  Modified for PPC64 by Dave Engebretsen (engebret@ibm.com)\n *\n *  This program is free software; you can redistribute it and/or\n *  modify it under the terms of the GNU General Public License\n *  as published by the Free Software Foundation; either version\n *  2 of the License, or (at your option) any later version.\n */\n\n#include <linux/signal.h>\n#include <linux/sched.h>\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/ptrace.h>\n#include <linux/mman.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/highmem.h>\n#include <linux/module.h>\n#include <linux/kprobes.h>\n#include <linux/kdebug.h>\n#include <linux/perf_event.h>\n#include <linux/magic.h>\n\n#include <asm/firmware.h>\n#include <asm/page.h>\n#include <asm/pgtable.h>\n#include <asm/mmu.h>\n#include <asm/mmu_context.h>\n#include <asm/system.h>\n#include <asm/uaccess.h>\n#include <asm/tlbflush.h>\n#include <asm/siginfo.h>\n#include <mm/mmu_decl.h>\n\n#ifdef CONFIG_KPROBES\nstatic inline int notify_page_fault(struct pt_regs *regs)\n{\n\tint ret = 0;\n\n\t/* kprobe_running() needs smp_processor_id() */\n\tif (!user_mode(regs)) {\n\t\tpreempt_disable();\n\t\tif (kprobe_running() && kprobe_fault_handler(regs, 11))\n\t\t\tret = 1;\n\t\tpreempt_enable();\n\t}\n\n\treturn ret;\n}\n#else\nstatic inline int notify_page_fault(struct pt_regs *regs)\n{\n\treturn 0;\n}\n#endif\n\n/*\n * Check whether the instruction at regs->nip is a store using\n * an update addressing form which will update r1.\n */\nstatic int store_updates_sp(struct pt_regs *regs)\n{\n\tunsigned int inst;\n\n\tif (get_user(inst, (unsigned int __user *)regs->nip))\n\t\treturn 0;\n\t/* check for 1 in the rA field */\n\tif (((inst >> 16) & 0x1f) != 1)\n\t\treturn 0;\n\t/* check major opcode */\n\tswitch (inst >> 26) {\n\tcase 37:\t/* stwu */\n\tcase 39:\t/* stbu */\n\tcase 45:\t/* sthu */\n\tcase 53:\t/* stfsu */\n\tcase 55:\t/* stfdu */\n\t\treturn 1;\n\tcase 62:\t/* std or stdu */\n\t\treturn (inst & 3) == 1;\n\tcase 31:\n\t\t/* check minor opcode */\n\t\tswitch ((inst >> 1) & 0x3ff) {\n\t\tcase 181:\t/* stdux */\n\t\tcase 183:\t/* stwux */\n\t\tcase 247:\t/* stbux */\n\t\tcase 439:\t/* sthux */\n\t\tcase 695:\t/* stfsux */\n\t\tcase 759:\t/* stfdux */\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/*\n * For 600- and 800-family processors, the error_code parameter is DSISR\n * for a data fault, SRR1 for an instruction fault. For 400-family processors\n * the error_code parameter is ESR for a data fault, 0 for an instruction\n * fault.\n * For 64-bit processors, the error_code parameter is\n *  - DSISR for a non-SLB data access fault,\n *  - SRR1 & 0x08000000 for a non-SLB instruction access fault\n *  - 0 any SLB fault.\n *\n * The return value is 0 if the fault was handled, or the signal\n * number if this is a kernel fault that can't be handled here.\n */\nint __kprobes do_page_fault(struct pt_regs *regs, unsigned long address,\n\t\t\t    unsigned long error_code)\n{\n\tstruct vm_area_struct * vma;\n\tstruct mm_struct *mm = current->mm;\n\tsiginfo_t info;\n\tint code = SEGV_MAPERR;\n\tint is_write = 0, ret;\n\tint trap = TRAP(regs);\n \tint is_exec = trap == 0x400;\n\n#if !(defined(CONFIG_4xx) || defined(CONFIG_BOOKE))\n\t/*\n\t * Fortunately the bit assignments in SRR1 for an instruction\n\t * fault and DSISR for a data fault are mostly the same for the\n\t * bits we are interested in.  But there are some bits which\n\t * indicate errors in DSISR but can validly be set in SRR1.\n\t */\n\tif (trap == 0x400)\n\t\terror_code &= 0x48200000;\n\telse\n\t\tis_write = error_code & DSISR_ISSTORE;\n#else\n\tis_write = error_code & ESR_DST;\n#endif /* CONFIG_4xx || CONFIG_BOOKE */\n\n\tif (notify_page_fault(regs))\n\t\treturn 0;\n\n\tif (unlikely(debugger_fault_handler(regs)))\n\t\treturn 0;\n\n\t/* On a kernel SLB miss we can only check for a valid exception entry */\n\tif (!user_mode(regs) && (address >= TASK_SIZE))\n\t\treturn SIGSEGV;\n\n#if !(defined(CONFIG_4xx) || defined(CONFIG_BOOKE) || \\\n\t\t\t     defined(CONFIG_PPC_BOOK3S_64))\n  \tif (error_code & DSISR_DABRMATCH) {\n\t\t/* DABR match */\n\t\tdo_dabr(regs, address, error_code);\n\t\treturn 0;\n\t}\n#endif\n\n\tif (in_atomic() || mm == NULL) {\n\t\tif (!user_mode(regs))\n\t\t\treturn SIGSEGV;\n\t\t/* in_atomic() in user mode is really bad,\n\t\t   as is current->mm == NULL. */\n\t\tprintk(KERN_EMERG \"Page fault in user mode with \"\n\t\t       \"in_atomic() = %d mm = %p\\n\", in_atomic(), mm);\n\t\tprintk(KERN_EMERG \"NIP = %lx  MSR = %lx\\n\",\n\t\t       regs->nip, regs->msr);\n\t\tdie(\"Weird page fault\", regs, SIGSEGV);\n\t}\n\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);\n\n\t/* When running in the kernel we expect faults to occur only to\n\t * addresses in user space.  All other faults represent errors in the\n\t * kernel and should generate an OOPS.  Unfortunately, in the case of an\n\t * erroneous fault occurring in a code path which already holds mmap_sem\n\t * we will deadlock attempting to validate the fault against the\n\t * address space.  Luckily the kernel only validly references user\n\t * space from well defined areas of code, which are listed in the\n\t * exceptions table.\n\t *\n\t * As the vast majority of faults will be valid we will only perform\n\t * the source reference check when there is a possibility of a deadlock.\n\t * Attempt to lock the address space, if we cannot we then validate the\n\t * source.  If this is invalid we can skip the address space check,\n\t * thus avoiding the deadlock.\n\t */\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tif (!user_mode(regs) && !search_exception_tables(regs->nip))\n\t\t\tgoto bad_area_nosemaphore;\n\n\t\tdown_read(&mm->mmap_sem);\n\t}\n\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\n\t/*\n\t * N.B. The POWER/Open ABI allows programs to access up to\n\t * 288 bytes below the stack pointer.\n\t * The kernel signal delivery code writes up to about 1.5kB\n\t * below the stack pointer (r1) before decrementing it.\n\t * The exec code can write slightly over 640kB to the stack\n\t * before setting the user r1.  Thus we allow the stack to\n\t * expand to 1MB without further checks.\n\t */\n\tif (address + 0x100000 < vma->vm_end) {\n\t\t/* get user regs even if this fault is in kernel mode */\n\t\tstruct pt_regs *uregs = current->thread.regs;\n\t\tif (uregs == NULL)\n\t\t\tgoto bad_area;\n\n\t\t/*\n\t\t * A user-mode access to an address a long way below\n\t\t * the stack pointer is only valid if the instruction\n\t\t * is one which would update the stack pointer to the\n\t\t * address accessed if the instruction completed,\n\t\t * i.e. either stwu rs,n(r1) or stwux rs,r1,rb\n\t\t * (or the byte, halfword, float or double forms).\n\t\t *\n\t\t * If we don't check this then any write to the area\n\t\t * between the last mapped region and the stack will\n\t\t * expand the stack rather than segfaulting.\n\t\t */\n\t\tif (address + 2048 < uregs->gpr[1]\n\t\t    && (!user_mode(regs) || !store_updates_sp(regs)))\n\t\t\tgoto bad_area;\n\t}\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n\ngood_area:\n\tcode = SEGV_ACCERR;\n#if defined(CONFIG_6xx)\n\tif (error_code & 0x95700000)\n\t\t/* an error such as lwarx to I/O controller space,\n\t\t   address matching DABR, eciwx, etc. */\n\t\tgoto bad_area;\n#endif /* CONFIG_6xx */\n#if defined(CONFIG_8xx)\n\t/* 8xx sometimes need to load a invalid/non-present TLBs.\n\t * These must be invalidated separately as linux mm don't.\n\t */\n\tif (error_code & 0x40000000) /* no translation? */\n\t\t_tlbil_va(address, 0, 0, 0);\n\n        /* The MPC8xx seems to always set 0x80000000, which is\n         * \"undefined\".  Of those that can be set, this is the only\n         * one which seems bad.\n         */\n\tif (error_code & 0x10000000)\n                /* Guarded storage error. */\n\t\tgoto bad_area;\n#endif /* CONFIG_8xx */\n\n\tif (is_exec) {\n#ifdef CONFIG_PPC_STD_MMU\n\t\t/* Protection fault on exec go straight to failure on\n\t\t * Hash based MMUs as they either don't support per-page\n\t\t * execute permission, or if they do, it's handled already\n\t\t * at the hash level. This test would probably have to\n\t\t * be removed if we change the way this works to make hash\n\t\t * processors use the same I/D cache coherency mechanism\n\t\t * as embedded.\n\t\t */\n\t\tif (error_code & DSISR_PROTFAULT)\n\t\t\tgoto bad_area;\n#endif /* CONFIG_PPC_STD_MMU */\n\n\t\t/*\n\t\t * Allow execution from readable areas if the MMU does not\n\t\t * provide separate controls over reading and executing.\n\t\t *\n\t\t * Note: That code used to not be enabled for 4xx/BookE.\n\t\t * It is now as I/D cache coherency for these is done at\n\t\t * set_pte_at() time and I see no reason why the test\n\t\t * below wouldn't be valid on those processors. This -may-\n\t\t * break programs compiled with a really old ABI though.\n\t\t */\n\t\tif (!(vma->vm_flags & VM_EXEC) &&\n\t\t    (cpu_has_feature(CPU_FTR_NOEXECUTE) ||\n\t\t     !(vma->vm_flags & (VM_READ | VM_WRITE))))\n\t\t\tgoto bad_area;\n\t/* a write */\n\t} else if (is_write) {\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t/* a read */\n\t} else {\n\t\t/* protection fault */\n\t\tif (error_code & 0x08000000)\n\t\t\tgoto bad_area;\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tgoto bad_area;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tret = handle_mm_fault(mm, vma, address, is_write ? FAULT_FLAG_WRITE : 0);\n\tif (unlikely(ret & VM_FAULT_ERROR)) {\n\t\tif (ret & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (ret & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (ret & VM_FAULT_MAJOR) {\n\t\tcurrent->maj_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1,\n\t\t\t\t     regs, address);\n#ifdef CONFIG_PPC_SMLPAR\n\t\tif (firmware_has_feature(FW_FEATURE_CMO)) {\n\t\t\tpreempt_disable();\n\t\t\tget_lppaca()->page_ins += (1 << PAGE_FACTOR);\n\t\t\tpreempt_enable();\n\t\t}\n#endif\n\t} else {\n\t\tcurrent->min_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1,\n\t\t\t\t     regs, address);\n\t}\n\tup_read(&mm->mmap_sem);\n\treturn 0;\n\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\t/* User mode accesses cause a SIGSEGV */\n\tif (user_mode(regs)) {\n\t\t_exception(SIGSEGV, regs, code, address);\n\t\treturn 0;\n\t}\n\n\tif (is_exec && (error_code & DSISR_PROTFAULT)\n\t    && printk_ratelimit())\n\t\tprintk(KERN_CRIT \"kernel tried to execute NX-protected\"\n\t\t       \" page (%lx) - exploit attempt? (uid: %d)\\n\",\n\t\t       address, current_uid());\n\n\treturn SIGSEGV;\n\n/*\n * We ran out of memory, or some other thing happened to us that made\n * us unable to handle the page fault gracefully.\n */\nout_of_memory:\n\tup_read(&mm->mmap_sem);\n\tif (!user_mode(regs))\n\t\treturn SIGKILL;\n\tpagefault_out_of_memory();\n\treturn 0;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\tif (user_mode(regs)) {\n\t\tinfo.si_signo = SIGBUS;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code = BUS_ADRERR;\n\t\tinfo.si_addr = (void __user *)address;\n\t\tforce_sig_info(SIGBUS, &info, current);\n\t\treturn 0;\n\t}\n\treturn SIGBUS;\n}\n\n/*\n * bad_page_fault is called when we have a bad access from the kernel.\n * It is called from the DSI and ISI handlers in head.S and from some\n * of the procedures in traps.c.\n */\nvoid bad_page_fault(struct pt_regs *regs, unsigned long address, int sig)\n{\n\tconst struct exception_table_entry *entry;\n\tunsigned long *stackend;\n\n\t/* Are we prepared to handle this fault?  */\n\tif ((entry = search_exception_tables(regs->nip)) != NULL) {\n\t\tregs->nip = entry->fixup;\n\t\treturn;\n\t}\n\n\t/* kernel has accessed a bad area */\n\n\tswitch (regs->trap) {\n\tcase 0x300:\n\tcase 0x380:\n\t\tprintk(KERN_ALERT \"Unable to handle kernel paging request for \"\n\t\t\t\"data at address 0x%08lx\\n\", regs->dar);\n\t\tbreak;\n\tcase 0x400:\n\tcase 0x480:\n\t\tprintk(KERN_ALERT \"Unable to handle kernel paging request for \"\n\t\t\t\"instruction fetch\\n\");\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ALERT \"Unable to handle kernel paging request for \"\n\t\t\t\"unknown fault\\n\");\n\t\tbreak;\n\t}\n\tprintk(KERN_ALERT \"Faulting instruction address: 0x%08lx\\n\",\n\t\tregs->nip);\n\n\tstackend = end_of_stack(current);\n\tif (current != &init_task && *stackend != STACK_END_MAGIC)\n\t\tprintk(KERN_ALERT \"Thread overran stack, or stack corrupted\\n\");\n\n\tdie(\"Kernel access of bad area\", regs, sig);\n}\n", "/*\n *  arch/s390/mm/fault.c\n *\n *  S390 version\n *    Copyright (C) 1999 IBM Deutschland Entwicklung GmbH, IBM Corporation\n *    Author(s): Hartmut Penner (hp@de.ibm.com)\n *               Ulrich Weigand (uweigand@de.ibm.com)\n *\n *  Derived from \"arch/i386/mm/fault.c\"\n *    Copyright (C) 1995  Linus Torvalds\n */\n\n#include <linux/kernel_stat.h>\n#include <linux/perf_event.h>\n#include <linux/signal.h>\n#include <linux/sched.h>\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/ptrace.h>\n#include <linux/mman.h>\n#include <linux/mm.h>\n#include <linux/compat.h>\n#include <linux/smp.h>\n#include <linux/kdebug.h>\n#include <linux/init.h>\n#include <linux/console.h>\n#include <linux/module.h>\n#include <linux/hardirq.h>\n#include <linux/kprobes.h>\n#include <linux/uaccess.h>\n#include <linux/hugetlb.h>\n#include <asm/asm-offsets.h>\n#include <asm/system.h>\n#include <asm/pgtable.h>\n#include <asm/irq.h>\n#include <asm/mmu_context.h>\n#include <asm/compat.h>\n#include \"../kernel/entry.h\"\n\n#ifndef CONFIG_64BIT\n#define __FAIL_ADDR_MASK 0x7ffff000\n#define __SUBCODE_MASK 0x0200\n#define __PF_RES_FIELD 0ULL\n#else /* CONFIG_64BIT */\n#define __FAIL_ADDR_MASK -4096L\n#define __SUBCODE_MASK 0x0600\n#define __PF_RES_FIELD 0x8000000000000000ULL\n#endif /* CONFIG_64BIT */\n\n#define VM_FAULT_BADCONTEXT\t0x010000\n#define VM_FAULT_BADMAP\t\t0x020000\n#define VM_FAULT_BADACCESS\t0x040000\n\nstatic unsigned long store_indication;\n\nvoid fault_init(void)\n{\n\tif (test_facility(2) && test_facility(75))\n\t\tstore_indication = 0xc00;\n}\n\nstatic inline int notify_page_fault(struct pt_regs *regs)\n{\n\tint ret = 0;\n\n\t/* kprobe_running() needs smp_processor_id() */\n\tif (kprobes_built_in() && !user_mode(regs)) {\n\t\tpreempt_disable();\n\t\tif (kprobe_running() && kprobe_fault_handler(regs, 14))\n\t\t\tret = 1;\n\t\tpreempt_enable();\n\t}\n\treturn ret;\n}\n\n\n/*\n * Unlock any spinlocks which will prevent us from getting the\n * message out.\n */\nvoid bust_spinlocks(int yes)\n{\n\tif (yes) {\n\t\toops_in_progress = 1;\n\t} else {\n\t\tint loglevel_save = console_loglevel;\n\t\tconsole_unblank();\n\t\toops_in_progress = 0;\n\t\t/*\n\t\t * OK, the message is on the console.  Now we call printk()\n\t\t * without oops_in_progress set so that printk will give klogd\n\t\t * a poke.  Hold onto your hats...\n\t\t */\n\t\tconsole_loglevel = 15;\n\t\tprintk(\" \");\n\t\tconsole_loglevel = loglevel_save;\n\t}\n}\n\n/*\n * Returns the address space associated with the fault.\n * Returns 0 for kernel space and 1 for user space.\n */\nstatic inline int user_space_fault(unsigned long trans_exc_code)\n{\n\t/*\n\t * The lowest two bits of the translation exception\n\t * identification indicate which paging table was used.\n\t */\n\ttrans_exc_code &= 3;\n\tif (trans_exc_code == 2)\n\t\t/* Access via secondary space, set_fs setting decides */\n\t\treturn current->thread.mm_segment.ar4;\n\tif (user_mode == HOME_SPACE_MODE)\n\t\t/* User space if the access has been done via home space. */\n\t\treturn trans_exc_code == 3;\n\t/*\n\t * If the user space is not the home space the kernel runs in home\n\t * space. Access via secondary space has already been covered,\n\t * access via primary space or access register is from user space\n\t * and access via home space is from the kernel.\n\t */\n\treturn trans_exc_code != 3;\n}\n\nstatic inline void report_user_fault(struct pt_regs *regs, long int_code,\n\t\t\t\t     int signr, unsigned long address)\n{\n\tif ((task_pid_nr(current) > 1) && !show_unhandled_signals)\n\t\treturn;\n\tif (!unhandled_signal(current, signr))\n\t\treturn;\n\tif (!printk_ratelimit())\n\t\treturn;\n\tprintk(\"User process fault: interruption code 0x%lX \", int_code);\n\tprint_vma_addr(KERN_CONT \"in \", regs->psw.addr & PSW_ADDR_INSN);\n\tprintk(\"\\n\");\n\tprintk(\"failing address: %lX\\n\", address);\n\tshow_regs(regs);\n}\n\n/*\n * Send SIGSEGV to task.  This is an external routine\n * to keep the stack usage of do_page_fault small.\n */\nstatic noinline void do_sigsegv(struct pt_regs *regs, long int_code,\n\t\t\t\tint si_code, unsigned long trans_exc_code)\n{\n\tstruct siginfo si;\n\tunsigned long address;\n\n\taddress = trans_exc_code & __FAIL_ADDR_MASK;\n\tcurrent->thread.prot_addr = address;\n\tcurrent->thread.trap_no = int_code;\n\treport_user_fault(regs, int_code, SIGSEGV, address);\n\tsi.si_signo = SIGSEGV;\n\tsi.si_code = si_code;\n\tsi.si_addr = (void __user *) address;\n\tforce_sig_info(SIGSEGV, &si, current);\n}\n\nstatic noinline void do_no_context(struct pt_regs *regs, long int_code,\n\t\t\t\t   unsigned long trans_exc_code)\n{\n\tconst struct exception_table_entry *fixup;\n\tunsigned long address;\n\n\t/* Are we prepared to handle this kernel fault?  */\n\tfixup = search_exception_tables(regs->psw.addr & PSW_ADDR_INSN);\n\tif (fixup) {\n\t\tregs->psw.addr = fixup->fixup | PSW_ADDR_AMODE;\n\t\treturn;\n\t}\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice.\n\t */\n\taddress = trans_exc_code & __FAIL_ADDR_MASK;\n\tif (!user_space_fault(trans_exc_code))\n\t\tprintk(KERN_ALERT \"Unable to handle kernel pointer dereference\"\n\t\t       \" at virtual kernel address %p\\n\", (void *)address);\n\telse\n\t\tprintk(KERN_ALERT \"Unable to handle kernel paging request\"\n\t\t       \" at virtual user address %p\\n\", (void *)address);\n\n\tdie(\"Oops\", regs, int_code);\n\tdo_exit(SIGKILL);\n}\n\nstatic noinline void do_low_address(struct pt_regs *regs, long int_code,\n\t\t\t\t    unsigned long trans_exc_code)\n{\n\t/* Low-address protection hit in kernel mode means\n\t   NULL pointer write access in kernel mode.  */\n\tif (regs->psw.mask & PSW_MASK_PSTATE) {\n\t\t/* Low-address protection hit in user mode 'cannot happen'. */\n\t\tdie (\"Low-address protection\", regs, int_code);\n\t\tdo_exit(SIGKILL);\n\t}\n\n\tdo_no_context(regs, int_code, trans_exc_code);\n}\n\nstatic noinline void do_sigbus(struct pt_regs *regs, long int_code,\n\t\t\t       unsigned long trans_exc_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long address;\n\tstruct siginfo si;\n\n\t/*\n\t * Send a sigbus, regardless of whether we were in kernel\n\t * or user mode.\n\t */\n\taddress = trans_exc_code & __FAIL_ADDR_MASK;\n\ttsk->thread.prot_addr = address;\n\ttsk->thread.trap_no = int_code;\n\tsi.si_signo = SIGBUS;\n\tsi.si_errno = 0;\n\tsi.si_code = BUS_ADRERR;\n\tsi.si_addr = (void __user *) address;\n\tforce_sig_info(SIGBUS, &si, tsk);\n}\n\nstatic noinline void do_fault_error(struct pt_regs *regs, long int_code,\n\t\t\t\t    unsigned long trans_exc_code, int fault)\n{\n\tint si_code;\n\n\tswitch (fault) {\n\tcase VM_FAULT_BADACCESS:\n\tcase VM_FAULT_BADMAP:\n\t\t/* Bad memory access. Check if it is kernel or user space. */\n\t\tif (regs->psw.mask & PSW_MASK_PSTATE) {\n\t\t\t/* User mode accesses just cause a SIGSEGV */\n\t\t\tsi_code = (fault == VM_FAULT_BADMAP) ?\n\t\t\t\tSEGV_MAPERR : SEGV_ACCERR;\n\t\t\tdo_sigsegv(regs, int_code, si_code, trans_exc_code);\n\t\t\treturn;\n\t\t}\n\tcase VM_FAULT_BADCONTEXT:\n\t\tdo_no_context(regs, int_code, trans_exc_code);\n\t\tbreak;\n\tdefault: /* fault & VM_FAULT_ERROR */\n\t\tif (fault & VM_FAULT_OOM) {\n\t\t\tif (!(regs->psw.mask & PSW_MASK_PSTATE))\n\t\t\t\tdo_no_context(regs, int_code, trans_exc_code);\n\t\t\telse\n\t\t\t\tpagefault_out_of_memory();\n\t\t} else if (fault & VM_FAULT_SIGBUS) {\n\t\t\t/* Kernel mode? Handle exceptions or die */\n\t\t\tif (!(regs->psw.mask & PSW_MASK_PSTATE))\n\t\t\t\tdo_no_context(regs, int_code, trans_exc_code);\n\t\t\telse\n\t\t\t\tdo_sigbus(regs, int_code, trans_exc_code);\n\t\t} else\n\t\t\tBUG();\n\t\tbreak;\n\t}\n}\n\n/*\n * This routine handles page faults.  It determines the address,\n * and the problem, and then passes it off to one of the appropriate\n * routines.\n *\n * interruption code (int_code):\n *   04       Protection           ->  Write-Protection  (suprression)\n *   10       Segment translation  ->  Not present       (nullification)\n *   11       Page translation     ->  Not present       (nullification)\n *   3b       Region third trans.  ->  Not present       (nullification)\n */\nstatic inline int do_exception(struct pt_regs *regs, int access,\n\t\t\t       unsigned long trans_exc_code)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long address;\n\tunsigned int flags;\n\tint fault;\n\n\tif (notify_page_fault(regs))\n\t\treturn 0;\n\n\ttsk = current;\n\tmm = tsk->mm;\n\n\t/*\n\t * Verify that the fault happened in user space, that\n\t * we are not in an interrupt and that there is a \n\t * user context.\n\t */\n\tfault = VM_FAULT_BADCONTEXT;\n\tif (unlikely(!user_space_fault(trans_exc_code) || in_atomic() || !mm))\n\t\tgoto out;\n\n\taddress = trans_exc_code & __FAIL_ADDR_MASK;\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);\n\tflags = FAULT_FLAG_ALLOW_RETRY;\n\tif (access == VM_WRITE || (trans_exc_code & store_indication) == 0x400)\n\t\tflags |= FAULT_FLAG_WRITE;\nretry:\n\tdown_read(&mm->mmap_sem);\n\n\tfault = VM_FAULT_BADMAP;\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto out_up;\n\n\tif (unlikely(vma->vm_start > address)) {\n\t\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\t\tgoto out_up;\n\t\tif (expand_stack(vma, address))\n\t\t\tgoto out_up;\n\t}\n\n\t/*\n\t * Ok, we have a good vm_area for this memory access, so\n\t * we can handle it..\n\t */\n\tfault = VM_FAULT_BADACCESS;\n\tif (unlikely(!(vma->vm_flags & access)))\n\t\tgoto out_up;\n\n\tif (is_vm_hugetlb_page(vma))\n\t\taddress &= HPAGE_MASK;\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, flags);\n\tif (unlikely(fault & VM_FAULT_ERROR))\n\t\tgoto out_up;\n\n\t/*\n\t * Major/minor page fault accounting is only done on the\n\t * initial attempt. If we go through a retry, it is extremely\n\t * likely that the page will be found in page cache at that point.\n\t */\n\tif (flags & FAULT_FLAG_ALLOW_RETRY) {\n\t\tif (fault & VM_FAULT_MAJOR) {\n\t\t\ttsk->maj_flt++;\n\t\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1,\n\t\t\t\t      regs, address);\n\t\t} else {\n\t\t\ttsk->min_flt++;\n\t\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1,\n\t\t\t\t      regs, address);\n\t\t}\n\t\tif (fault & VM_FAULT_RETRY) {\n\t\t\t/* Clear FAULT_FLAG_ALLOW_RETRY to avoid any risk\n\t\t\t * of starvation. */\n\t\t\tflags &= ~FAULT_FLAG_ALLOW_RETRY;\n\t\t\tgoto retry;\n\t\t}\n\t}\n\t/*\n\t * The instruction that caused the program check will\n\t * be repeated. Don't signal single step via SIGTRAP.\n\t */\n\tclear_tsk_thread_flag(tsk, TIF_PER_TRAP);\n\tfault = 0;\nout_up:\n\tup_read(&mm->mmap_sem);\nout:\n\treturn fault;\n}\n\nvoid __kprobes do_protection_exception(struct pt_regs *regs, long pgm_int_code,\n\t\t\t\t       unsigned long trans_exc_code)\n{\n\tint fault;\n\n\t/* Protection exception is suppressing, decrement psw address. */\n\tregs->psw.addr -= (pgm_int_code >> 16);\n\t/*\n\t * Check for low-address protection.  This needs to be treated\n\t * as a special case because the translation exception code\n\t * field is not guaranteed to contain valid data in this case.\n\t */\n\tif (unlikely(!(trans_exc_code & 4))) {\n\t\tdo_low_address(regs, pgm_int_code, trans_exc_code);\n\t\treturn;\n\t}\n\tfault = do_exception(regs, VM_WRITE, trans_exc_code);\n\tif (unlikely(fault))\n\t\tdo_fault_error(regs, 4, trans_exc_code, fault);\n}\n\nvoid __kprobes do_dat_exception(struct pt_regs *regs, long pgm_int_code,\n\t\t\t\tunsigned long trans_exc_code)\n{\n\tint access, fault;\n\n\taccess = VM_READ | VM_EXEC | VM_WRITE;\n\tfault = do_exception(regs, access, trans_exc_code);\n\tif (unlikely(fault))\n\t\tdo_fault_error(regs, pgm_int_code & 255, trans_exc_code, fault);\n}\n\n#ifdef CONFIG_64BIT\nvoid __kprobes do_asce_exception(struct pt_regs *regs, long pgm_int_code,\n\t\t\t\t unsigned long trans_exc_code)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\n\tif (unlikely(!user_space_fault(trans_exc_code) || in_atomic() || !mm))\n\t\tgoto no_context;\n\n\tdown_read(&mm->mmap_sem);\n\tvma = find_vma(mm, trans_exc_code & __FAIL_ADDR_MASK);\n\tup_read(&mm->mmap_sem);\n\n\tif (vma) {\n\t\tupdate_mm(mm, current);\n\t\treturn;\n\t}\n\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (regs->psw.mask & PSW_MASK_PSTATE) {\n\t\tdo_sigsegv(regs, pgm_int_code, SEGV_MAPERR, trans_exc_code);\n\t\treturn;\n\t}\n\nno_context:\n\tdo_no_context(regs, pgm_int_code, trans_exc_code);\n}\n#endif\n\nint __handle_fault(unsigned long uaddr, unsigned long pgm_int_code, int write)\n{\n\tstruct pt_regs regs;\n\tint access, fault;\n\n\tregs.psw.mask = psw_kernel_bits;\n\tif (!irqs_disabled())\n\t\tregs.psw.mask |= PSW_MASK_IO | PSW_MASK_EXT;\n\tregs.psw.addr = (unsigned long) __builtin_return_address(0);\n\tregs.psw.addr |= PSW_ADDR_AMODE;\n\tuaddr &= PAGE_MASK;\n\taccess = write ? VM_WRITE : VM_READ;\n\tfault = do_exception(&regs, access, uaddr | 2);\n\tif (unlikely(fault)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\treturn -EFAULT;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tdo_sigbus(&regs, pgm_int_code, uaddr);\n\t}\n\treturn fault ? -EFAULT : 0;\n}\n\n#ifdef CONFIG_PFAULT \n/*\n * 'pfault' pseudo page faults routines.\n */\nstatic int pfault_disable;\n\nstatic int __init nopfault(char *str)\n{\n\tpfault_disable = 1;\n\treturn 1;\n}\n\n__setup(\"nopfault\", nopfault);\n\nstruct pfault_refbk {\n\tu16 refdiagc;\n\tu16 reffcode;\n\tu16 refdwlen;\n\tu16 refversn;\n\tu64 refgaddr;\n\tu64 refselmk;\n\tu64 refcmpmk;\n\tu64 reserved;\n} __attribute__ ((packed, aligned(8)));\n\nint pfault_init(void)\n{\n\tstruct pfault_refbk refbk = {\n\t\t.refdiagc = 0x258,\n\t\t.reffcode = 0,\n\t\t.refdwlen = 5,\n\t\t.refversn = 2,\n\t\t.refgaddr = __LC_CURRENT_PID,\n\t\t.refselmk = 1ULL << 48,\n\t\t.refcmpmk = 1ULL << 48,\n\t\t.reserved = __PF_RES_FIELD };\n        int rc;\n\n\tif (!MACHINE_IS_VM || pfault_disable)\n\t\treturn -1;\n\tasm volatile(\n\t\t\"\tdiag\t%1,%0,0x258\\n\"\n\t\t\"0:\tj\t2f\\n\"\n\t\t\"1:\tla\t%0,8\\n\"\n\t\t\"2:\\n\"\n\t\tEX_TABLE(0b,1b)\n\t\t: \"=d\" (rc) : \"a\" (&refbk), \"m\" (refbk) : \"cc\");\n        return rc;\n}\n\nvoid pfault_fini(void)\n{\n\tstruct pfault_refbk refbk = {\n\t\t.refdiagc = 0x258,\n\t\t.reffcode = 1,\n\t\t.refdwlen = 5,\n\t\t.refversn = 2,\n\t};\n\n\tif (!MACHINE_IS_VM || pfault_disable)\n\t\treturn;\n\tasm volatile(\n\t\t\"\tdiag\t%0,0,0x258\\n\"\n\t\t\"0:\\n\"\n\t\tEX_TABLE(0b,0b)\n\t\t: : \"a\" (&refbk), \"m\" (refbk) : \"cc\");\n}\n\nstatic DEFINE_SPINLOCK(pfault_lock);\nstatic LIST_HEAD(pfault_list);\n\nstatic void pfault_interrupt(unsigned int ext_int_code,\n\t\t\t     unsigned int param32, unsigned long param64)\n{\n\tstruct task_struct *tsk;\n\t__u16 subcode;\n\tpid_t pid;\n\n\t/*\n\t * Get the external interruption subcode & pfault\n\t * initial/completion signal bit. VM stores this \n\t * in the 'cpu address' field associated with the\n         * external interrupt. \n\t */\n\tsubcode = ext_int_code >> 16;\n\tif ((subcode & 0xff00) != __SUBCODE_MASK)\n\t\treturn;\n\tkstat_cpu(smp_processor_id()).irqs[EXTINT_PFL]++;\n\tif (subcode & 0x0080) {\n\t\t/* Get the token (= pid of the affected task). */\n\t\tpid = sizeof(void *) == 4 ? param32 : param64;\n\t\trcu_read_lock();\n\t\ttsk = find_task_by_pid_ns(pid, &init_pid_ns);\n\t\tif (tsk)\n\t\t\tget_task_struct(tsk);\n\t\trcu_read_unlock();\n\t\tif (!tsk)\n\t\t\treturn;\n\t} else {\n\t\ttsk = current;\n\t}\n\tspin_lock(&pfault_lock);\n\tif (subcode & 0x0080) {\n\t\t/* signal bit is set -> a page has been swapped in by VM */\n\t\tif (tsk->thread.pfault_wait == 1) {\n\t\t\t/* Initial interrupt was faster than the completion\n\t\t\t * interrupt. pfault_wait is valid. Set pfault_wait\n\t\t\t * back to zero and wake up the process. This can\n\t\t\t * safely be done because the task is still sleeping\n\t\t\t * and can't produce new pfaults. */\n\t\t\ttsk->thread.pfault_wait = 0;\n\t\t\tlist_del(&tsk->thread.list);\n\t\t\twake_up_process(tsk);\n\t\t} else {\n\t\t\t/* Completion interrupt was faster than initial\n\t\t\t * interrupt. Set pfault_wait to -1 so the initial\n\t\t\t * interrupt doesn't put the task to sleep. */\n\t\t\ttsk->thread.pfault_wait = -1;\n\t\t}\n\t\tput_task_struct(tsk);\n\t} else {\n\t\t/* signal bit not set -> a real page is missing. */\n\t\tif (tsk->thread.pfault_wait == -1) {\n\t\t\t/* Completion interrupt was faster than the initial\n\t\t\t * interrupt (pfault_wait == -1). Set pfault_wait\n\t\t\t * back to zero and exit. */\n\t\t\ttsk->thread.pfault_wait = 0;\n\t\t} else {\n\t\t\t/* Initial interrupt arrived before completion\n\t\t\t * interrupt. Let the task sleep. */\n\t\t\ttsk->thread.pfault_wait = 1;\n\t\t\tlist_add(&tsk->thread.list, &pfault_list);\n\t\t\tset_task_state(tsk, TASK_UNINTERRUPTIBLE);\n\t\t\tset_tsk_need_resched(tsk);\n\t\t}\n\t}\n\tspin_unlock(&pfault_lock);\n}\n\nstatic int __cpuinit pfault_cpu_notify(struct notifier_block *self,\n\t\t\t\t       unsigned long action, void *hcpu)\n{\n\tstruct thread_struct *thread, *next;\n\tstruct task_struct *tsk;\n\n\tswitch (action) {\n\tcase CPU_DEAD:\n\tcase CPU_DEAD_FROZEN:\n\t\tspin_lock_irq(&pfault_lock);\n\t\tlist_for_each_entry_safe(thread, next, &pfault_list, list) {\n\t\t\tthread->pfault_wait = 0;\n\t\t\tlist_del(&thread->list);\n\t\t\ttsk = container_of(thread, struct task_struct, thread);\n\t\t\twake_up_process(tsk);\n\t\t}\n\t\tspin_unlock_irq(&pfault_lock);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n\nstatic int __init pfault_irq_init(void)\n{\n\tint rc;\n\n\tif (!MACHINE_IS_VM)\n\t\treturn 0;\n\trc = register_external_interrupt(0x2603, pfault_interrupt);\n\tif (rc)\n\t\tgoto out_extint;\n\trc = pfault_init() == 0 ? 0 : -EOPNOTSUPP;\n\tif (rc)\n\t\tgoto out_pfault;\n\tservice_subclass_irq_register();\n\thotcpu_notifier(pfault_cpu_notify, 0);\n\treturn 0;\n\nout_pfault:\n\tunregister_external_interrupt(0x2603, pfault_interrupt);\nout_extint:\n\tpfault_disable = 1;\n\treturn rc;\n}\nearly_initcall(pfault_irq_init);\n\n#endif /* CONFIG_PFAULT */\n", "/*\n * SuperH process tracing\n *\n * Copyright (C) 1999, 2000  Kaz Kojima & Niibe Yutaka\n * Copyright (C) 2002 - 2009  Paul Mundt\n *\n * Audit support by Yuichi Nakamura <ynakam@hitachisoft.jp>\n *\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n */\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/errno.h>\n#include <linux/ptrace.h>\n#include <linux/user.h>\n#include <linux/security.h>\n#include <linux/signal.h>\n#include <linux/io.h>\n#include <linux/audit.h>\n#include <linux/seccomp.h>\n#include <linux/tracehook.h>\n#include <linux/elf.h>\n#include <linux/regset.h>\n#include <linux/hw_breakpoint.h>\n#include <asm/uaccess.h>\n#include <asm/pgtable.h>\n#include <asm/system.h>\n#include <asm/processor.h>\n#include <asm/mmu_context.h>\n#include <asm/syscalls.h>\n#include <asm/fpu.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/syscalls.h>\n\n/*\n * This routine will get a word off of the process kernel stack.\n */\nstatic inline int get_stack_long(struct task_struct *task, int offset)\n{\n\tunsigned char *stack;\n\n\tstack = (unsigned char *)task_pt_regs(task);\n\tstack += offset;\n\treturn (*((int *)stack));\n}\n\n/*\n * This routine will put a word on the process kernel stack.\n */\nstatic inline int put_stack_long(struct task_struct *task, int offset,\n\t\t\t\t unsigned long data)\n{\n\tunsigned char *stack;\n\n\tstack = (unsigned char *)task_pt_regs(task);\n\tstack += offset;\n\t*(unsigned long *) stack = data;\n\treturn 0;\n}\n\nvoid ptrace_triggered(struct perf_event *bp,\n\t\t      struct perf_sample_data *data, struct pt_regs *regs)\n{\n\tstruct perf_event_attr attr;\n\n\t/*\n\t * Disable the breakpoint request here since ptrace has defined a\n\t * one-shot behaviour for breakpoint exceptions.\n\t */\n\tattr = bp->attr;\n\tattr.disabled = true;\n\tmodify_user_hw_breakpoint(bp, &attr);\n}\n\nstatic int set_single_step(struct task_struct *tsk, unsigned long addr)\n{\n\tstruct thread_struct *thread = &tsk->thread;\n\tstruct perf_event *bp;\n\tstruct perf_event_attr attr;\n\n\tbp = thread->ptrace_bps[0];\n\tif (!bp) {\n\t\tptrace_breakpoint_init(&attr);\n\n\t\tattr.bp_addr = addr;\n\t\tattr.bp_len = HW_BREAKPOINT_LEN_2;\n\t\tattr.bp_type = HW_BREAKPOINT_R;\n\n\t\tbp = register_user_hw_breakpoint(&attr, ptrace_triggered, tsk);\n\t\tif (IS_ERR(bp))\n\t\t\treturn PTR_ERR(bp);\n\n\t\tthread->ptrace_bps[0] = bp;\n\t} else {\n\t\tint err;\n\n\t\tattr = bp->attr;\n\t\tattr.bp_addr = addr;\n\t\t/* reenable breakpoint */\n\t\tattr.disabled = false;\n\t\terr = modify_user_hw_breakpoint(bp, &attr);\n\t\tif (unlikely(err))\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nvoid user_enable_single_step(struct task_struct *child)\n{\n\tunsigned long pc = get_stack_long(child, offsetof(struct pt_regs, pc));\n\n\tset_tsk_thread_flag(child, TIF_SINGLESTEP);\n\n\tif (ptrace_get_breakpoints(child) < 0)\n\t\treturn;\n\n\tset_single_step(child, pc);\n\tptrace_put_breakpoints(child);\n}\n\nvoid user_disable_single_step(struct task_struct *child)\n{\n\tclear_tsk_thread_flag(child, TIF_SINGLESTEP);\n}\n\n/*\n * Called by kernel/ptrace.c when detaching..\n *\n * Make sure single step bits etc are not set.\n */\nvoid ptrace_disable(struct task_struct *child)\n{\n\tuser_disable_single_step(child);\n}\n\nstatic int genregs_get(struct task_struct *target,\n\t\t       const struct user_regset *regset,\n\t\t       unsigned int pos, unsigned int count,\n\t\t       void *kbuf, void __user *ubuf)\n{\n\tconst struct pt_regs *regs = task_pt_regs(target);\n\tint ret;\n\n\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  regs->regs,\n\t\t\t\t  0, 16 * sizeof(unsigned long));\n\tif (!ret)\n\t\t/* PC, PR, SR, GBR, MACH, MACL, TRA */\n\t\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t  &regs->pc,\n\t\t\t\t\t  offsetof(struct pt_regs, pc),\n\t\t\t\t\t  sizeof(struct pt_regs));\n\tif (!ret)\n\t\tret = user_regset_copyout_zero(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t       sizeof(struct pt_regs), -1);\n\n\treturn ret;\n}\n\nstatic int genregs_set(struct task_struct *target,\n\t\t       const struct user_regset *regset,\n\t\t       unsigned int pos, unsigned int count,\n\t\t       const void *kbuf, const void __user *ubuf)\n{\n\tstruct pt_regs *regs = task_pt_regs(target);\n\tint ret;\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t regs->regs,\n\t\t\t\t 0, 16 * sizeof(unsigned long));\n\tif (!ret && count > 0)\n\t\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t &regs->pc,\n\t\t\t\t\t offsetof(struct pt_regs, pc),\n\t\t\t\t\t sizeof(struct pt_regs));\n\tif (!ret)\n\t\tret = user_regset_copyin_ignore(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t\tsizeof(struct pt_regs), -1);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_SH_FPU\nint fpregs_get(struct task_struct *target,\n\t       const struct user_regset *regset,\n\t       unsigned int pos, unsigned int count,\n\t       void *kbuf, void __user *ubuf)\n{\n\tint ret;\n\n\tret = init_fpu(target);\n\tif (ret)\n\t\treturn ret;\n\n\tif ((boot_cpu_data.flags & CPU_HAS_FPU))\n\t\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t   &target->thread.xstate->hardfpu, 0, -1);\n\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t   &target->thread.xstate->softfpu, 0, -1);\n}\n\nstatic int fpregs_set(struct task_struct *target,\n\t\t       const struct user_regset *regset,\n\t\t       unsigned int pos, unsigned int count,\n\t\t       const void *kbuf, const void __user *ubuf)\n{\n\tint ret;\n\n\tret = init_fpu(target);\n\tif (ret)\n\t\treturn ret;\n\n\tset_stopped_child_used_math(target);\n\n\tif ((boot_cpu_data.flags & CPU_HAS_FPU))\n\t\treturn user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t  &target->thread.xstate->hardfpu, 0, -1);\n\n\treturn user_regset_copyin(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t  &target->thread.xstate->softfpu, 0, -1);\n}\n\nstatic int fpregs_active(struct task_struct *target,\n\t\t\t const struct user_regset *regset)\n{\n\treturn tsk_used_math(target) ? regset->n : 0;\n}\n#endif\n\n#ifdef CONFIG_SH_DSP\nstatic int dspregs_get(struct task_struct *target,\n\t\t       const struct user_regset *regset,\n\t\t       unsigned int pos, unsigned int count,\n\t\t       void *kbuf, void __user *ubuf)\n{\n\tconst struct pt_dspregs *regs =\n\t\t(struct pt_dspregs *)&target->thread.dsp_status.dsp_regs;\n\tint ret;\n\n\tret = user_regset_copyout(&pos, &count, &kbuf, &ubuf, regs,\n\t\t\t\t  0, sizeof(struct pt_dspregs));\n\tif (!ret)\n\t\tret = user_regset_copyout_zero(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t       sizeof(struct pt_dspregs), -1);\n\n\treturn ret;\n}\n\nstatic int dspregs_set(struct task_struct *target,\n\t\t       const struct user_regset *regset,\n\t\t       unsigned int pos, unsigned int count,\n\t\t       const void *kbuf, const void __user *ubuf)\n{\n\tstruct pt_dspregs *regs =\n\t\t(struct pt_dspregs *)&target->thread.dsp_status.dsp_regs;\n\tint ret;\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, regs,\n\t\t\t\t 0, sizeof(struct pt_dspregs));\n\tif (!ret)\n\t\tret = user_regset_copyin_ignore(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t\t\tsizeof(struct pt_dspregs), -1);\n\n\treturn ret;\n}\n\nstatic int dspregs_active(struct task_struct *target,\n\t\t\t  const struct user_regset *regset)\n{\n\tstruct pt_regs *regs = task_pt_regs(target);\n\n\treturn regs->sr & SR_DSP ? regset->n : 0;\n}\n#endif\n\nconst struct pt_regs_offset regoffset_table[] = {\n\tREGS_OFFSET_NAME(0),\n\tREGS_OFFSET_NAME(1),\n\tREGS_OFFSET_NAME(2),\n\tREGS_OFFSET_NAME(3),\n\tREGS_OFFSET_NAME(4),\n\tREGS_OFFSET_NAME(5),\n\tREGS_OFFSET_NAME(6),\n\tREGS_OFFSET_NAME(7),\n\tREGS_OFFSET_NAME(8),\n\tREGS_OFFSET_NAME(9),\n\tREGS_OFFSET_NAME(10),\n\tREGS_OFFSET_NAME(11),\n\tREGS_OFFSET_NAME(12),\n\tREGS_OFFSET_NAME(13),\n\tREGS_OFFSET_NAME(14),\n\tREGS_OFFSET_NAME(15),\n\tREG_OFFSET_NAME(pc),\n\tREG_OFFSET_NAME(pr),\n\tREG_OFFSET_NAME(sr),\n\tREG_OFFSET_NAME(gbr),\n\tREG_OFFSET_NAME(mach),\n\tREG_OFFSET_NAME(macl),\n\tREG_OFFSET_NAME(tra),\n\tREG_OFFSET_END,\n};\n\n/*\n * These are our native regset flavours.\n */\nenum sh_regset {\n\tREGSET_GENERAL,\n#ifdef CONFIG_SH_FPU\n\tREGSET_FPU,\n#endif\n#ifdef CONFIG_SH_DSP\n\tREGSET_DSP,\n#endif\n};\n\nstatic const struct user_regset sh_regsets[] = {\n\t/*\n\t * Format is:\n\t *\tR0 --> R15\n\t *\tPC, PR, SR, GBR, MACH, MACL, TRA\n\t */\n\t[REGSET_GENERAL] = {\n\t\t.core_note_type\t= NT_PRSTATUS,\n\t\t.n\t\t= ELF_NGREG,\n\t\t.size\t\t= sizeof(long),\n\t\t.align\t\t= sizeof(long),\n\t\t.get\t\t= genregs_get,\n\t\t.set\t\t= genregs_set,\n\t},\n\n#ifdef CONFIG_SH_FPU\n\t[REGSET_FPU] = {\n\t\t.core_note_type\t= NT_PRFPREG,\n\t\t.n\t\t= sizeof(struct user_fpu_struct) / sizeof(long),\n\t\t.size\t\t= sizeof(long),\n\t\t.align\t\t= sizeof(long),\n\t\t.get\t\t= fpregs_get,\n\t\t.set\t\t= fpregs_set,\n\t\t.active\t\t= fpregs_active,\n\t},\n#endif\n\n#ifdef CONFIG_SH_DSP\n\t[REGSET_DSP] = {\n\t\t.n\t\t= sizeof(struct pt_dspregs) / sizeof(long),\n\t\t.size\t\t= sizeof(long),\n\t\t.align\t\t= sizeof(long),\n\t\t.get\t\t= dspregs_get,\n\t\t.set\t\t= dspregs_set,\n\t\t.active\t\t= dspregs_active,\n\t},\n#endif\n};\n\nstatic const struct user_regset_view user_sh_native_view = {\n\t.name\t\t= \"sh\",\n\t.e_machine\t= EM_SH,\n\t.regsets\t= sh_regsets,\n\t.n\t\t= ARRAY_SIZE(sh_regsets),\n};\n\nconst struct user_regset_view *task_user_regset_view(struct task_struct *task)\n{\n\treturn &user_sh_native_view;\n}\n\nlong arch_ptrace(struct task_struct *child, long request,\n\t\t unsigned long addr, unsigned long data)\n{\n\tunsigned long __user *datap = (unsigned long __user *)data;\n\tint ret;\n\n\tswitch (request) {\n\t/* read the word at location addr in the USER area. */\n\tcase PTRACE_PEEKUSR: {\n\t\tunsigned long tmp;\n\n\t\tret = -EIO;\n\t\tif ((addr & 3) || addr < 0 ||\n\t\t    addr > sizeof(struct user) - 3)\n\t\t\tbreak;\n\n\t\tif (addr < sizeof(struct pt_regs))\n\t\t\ttmp = get_stack_long(child, addr);\n\t\telse if (addr >= offsetof(struct user, fpu) &&\n\t\t\t addr < offsetof(struct user, u_fpvalid)) {\n\t\t\tif (!tsk_used_math(child)) {\n\t\t\t\tif (addr == offsetof(struct user, fpu.fpscr))\n\t\t\t\t\ttmp = FPSCR_INIT;\n\t\t\t\telse\n\t\t\t\t\ttmp = 0;\n\t\t\t} else {\n\t\t\t\tunsigned long index;\n\t\t\t\tret = init_fpu(child);\n\t\t\t\tif (ret)\n\t\t\t\t\tbreak;\n\t\t\t\tindex = addr - offsetof(struct user, fpu);\n\t\t\t\ttmp = ((unsigned long *)child->thread.xstate)\n\t\t\t\t\t[index >> 2];\n\t\t\t}\n\t\t} else if (addr == offsetof(struct user, u_fpvalid))\n\t\t\ttmp = !!tsk_used_math(child);\n\t\telse if (addr == PT_TEXT_ADDR)\n\t\t\ttmp = child->mm->start_code;\n\t\telse if (addr == PT_DATA_ADDR)\n\t\t\ttmp = child->mm->start_data;\n\t\telse if (addr == PT_TEXT_END_ADDR)\n\t\t\ttmp = child->mm->end_code;\n\t\telse if (addr == PT_TEXT_LEN)\n\t\t\ttmp = child->mm->end_code - child->mm->start_code;\n\t\telse\n\t\t\ttmp = 0;\n\t\tret = put_user(tmp, datap);\n\t\tbreak;\n\t}\n\n\tcase PTRACE_POKEUSR: /* write the word at location addr in the USER area */\n\t\tret = -EIO;\n\t\tif ((addr & 3) || addr < 0 ||\n\t\t    addr > sizeof(struct user) - 3)\n\t\t\tbreak;\n\n\t\tif (addr < sizeof(struct pt_regs))\n\t\t\tret = put_stack_long(child, addr, data);\n\t\telse if (addr >= offsetof(struct user, fpu) &&\n\t\t\t addr < offsetof(struct user, u_fpvalid)) {\n\t\t\tunsigned long index;\n\t\t\tret = init_fpu(child);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tindex = addr - offsetof(struct user, fpu);\n\t\t\tset_stopped_child_used_math(child);\n\t\t\t((unsigned long *)child->thread.xstate)\n\t\t\t\t[index >> 2] = data;\n\t\t\tret = 0;\n\t\t} else if (addr == offsetof(struct user, u_fpvalid)) {\n\t\t\tconditional_stopped_child_used_math(data, child);\n\t\t\tret = 0;\n\t\t}\n\t\tbreak;\n\n\tcase PTRACE_GETREGS:\n\t\treturn copy_regset_to_user(child, &user_sh_native_view,\n\t\t\t\t\t   REGSET_GENERAL,\n\t\t\t\t\t   0, sizeof(struct pt_regs),\n\t\t\t\t\t   datap);\n\tcase PTRACE_SETREGS:\n\t\treturn copy_regset_from_user(child, &user_sh_native_view,\n\t\t\t\t\t     REGSET_GENERAL,\n\t\t\t\t\t     0, sizeof(struct pt_regs),\n\t\t\t\t\t     datap);\n#ifdef CONFIG_SH_FPU\n\tcase PTRACE_GETFPREGS:\n\t\treturn copy_regset_to_user(child, &user_sh_native_view,\n\t\t\t\t\t   REGSET_FPU,\n\t\t\t\t\t   0, sizeof(struct user_fpu_struct),\n\t\t\t\t\t   datap);\n\tcase PTRACE_SETFPREGS:\n\t\treturn copy_regset_from_user(child, &user_sh_native_view,\n\t\t\t\t\t     REGSET_FPU,\n\t\t\t\t\t     0, sizeof(struct user_fpu_struct),\n\t\t\t\t\t     datap);\n#endif\n#ifdef CONFIG_SH_DSP\n\tcase PTRACE_GETDSPREGS:\n\t\treturn copy_regset_to_user(child, &user_sh_native_view,\n\t\t\t\t\t   REGSET_DSP,\n\t\t\t\t\t   0, sizeof(struct pt_dspregs),\n\t\t\t\t\t   datap);\n\tcase PTRACE_SETDSPREGS:\n\t\treturn copy_regset_from_user(child, &user_sh_native_view,\n\t\t\t\t\t     REGSET_DSP,\n\t\t\t\t\t     0, sizeof(struct pt_dspregs),\n\t\t\t\t\t     datap);\n#endif\n\tdefault:\n\t\tret = ptrace_request(child, request, addr, data);\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic inline int audit_arch(void)\n{\n\tint arch = EM_SH;\n\n#ifdef CONFIG_CPU_LITTLE_ENDIAN\n\tarch |= __AUDIT_ARCH_LE;\n#endif\n\n\treturn arch;\n}\n\nasmlinkage long do_syscall_trace_enter(struct pt_regs *regs)\n{\n\tlong ret = 0;\n\n\tsecure_computing(regs->regs[0]);\n\n\tif (test_thread_flag(TIF_SYSCALL_TRACE) &&\n\t    tracehook_report_syscall_entry(regs))\n\t\t/*\n\t\t * Tracing decided this syscall should not happen.\n\t\t * We'll return a bogus call number to get an ENOSYS\n\t\t * error, but leave the original number in regs->regs[0].\n\t\t */\n\t\tret = -1L;\n\n\tif (unlikely(test_thread_flag(TIF_SYSCALL_TRACEPOINT)))\n\t\ttrace_sys_enter(regs, regs->regs[0]);\n\n\tif (unlikely(current->audit_context))\n\t\taudit_syscall_entry(audit_arch(), regs->regs[3],\n\t\t\t\t    regs->regs[4], regs->regs[5],\n\t\t\t\t    regs->regs[6], regs->regs[7]);\n\n\treturn ret ?: regs->regs[0];\n}\n\nasmlinkage void do_syscall_trace_leave(struct pt_regs *regs)\n{\n\tint step;\n\n\tif (unlikely(current->audit_context))\n\t\taudit_syscall_exit(AUDITSC_RESULT(regs->regs[0]),\n\t\t\t\t   regs->regs[0]);\n\n\tif (unlikely(test_thread_flag(TIF_SYSCALL_TRACEPOINT)))\n\t\ttrace_sys_exit(regs, regs->regs[0]);\n\n\tstep = test_thread_flag(TIF_SINGLESTEP);\n\tif (step || test_thread_flag(TIF_SYSCALL_TRACE))\n\t\ttracehook_report_syscall_exit(regs, step);\n}\n", "/*\n * 'traps.c' handles hardware traps and faults after we have saved some\n * state in 'entry.S'.\n *\n *  SuperH version: Copyright (C) 1999 Niibe Yutaka\n *                  Copyright (C) 2000 Philipp Rumpf\n *                  Copyright (C) 2000 David Howells\n *                  Copyright (C) 2002 - 2010 Paul Mundt\n *\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n */\n#include <linux/kernel.h>\n#include <linux/ptrace.h>\n#include <linux/hardirq.h>\n#include <linux/init.h>\n#include <linux/spinlock.h>\n#include <linux/module.h>\n#include <linux/kallsyms.h>\n#include <linux/io.h>\n#include <linux/bug.h>\n#include <linux/debug_locks.h>\n#include <linux/kdebug.h>\n#include <linux/kexec.h>\n#include <linux/limits.h>\n#include <linux/sysfs.h>\n#include <linux/uaccess.h>\n#include <linux/perf_event.h>\n#include <asm/system.h>\n#include <asm/alignment.h>\n#include <asm/fpu.h>\n#include <asm/kprobes.h>\n\n#ifdef CONFIG_CPU_SH2\n# define TRAP_RESERVED_INST\t4\n# define TRAP_ILLEGAL_SLOT_INST\t6\n# define TRAP_ADDRESS_ERROR\t9\n# ifdef CONFIG_CPU_SH2A\n#  define TRAP_UBC\t\t12\n#  define TRAP_FPU_ERROR\t13\n#  define TRAP_DIVZERO_ERROR\t17\n#  define TRAP_DIVOVF_ERROR\t18\n# endif\n#else\n#define TRAP_RESERVED_INST\t12\n#define TRAP_ILLEGAL_SLOT_INST\t13\n#endif\n\nstatic void dump_mem(const char *str, unsigned long bottom, unsigned long top)\n{\n\tunsigned long p;\n\tint i;\n\n\tprintk(\"%s(0x%08lx to 0x%08lx)\\n\", str, bottom, top);\n\n\tfor (p = bottom & ~31; p < top; ) {\n\t\tprintk(\"%04lx: \", p & 0xffff);\n\n\t\tfor (i = 0; i < 8; i++, p += 4) {\n\t\t\tunsigned int val;\n\n\t\t\tif (p < bottom || p >= top)\n\t\t\t\tprintk(\"         \");\n\t\t\telse {\n\t\t\t\tif (__get_user(val, (unsigned int __user *)p)) {\n\t\t\t\t\tprintk(\"\\n\");\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tprintk(\"%08x \", val);\n\t\t\t}\n\t\t}\n\t\tprintk(\"\\n\");\n\t}\n}\n\nstatic DEFINE_SPINLOCK(die_lock);\n\nvoid die(const char * str, struct pt_regs * regs, long err)\n{\n\tstatic int die_counter;\n\n\toops_enter();\n\n\tspin_lock_irq(&die_lock);\n\tconsole_verbose();\n\tbust_spinlocks(1);\n\n\tprintk(\"%s: %04lx [#%d]\\n\", str, err & 0xffff, ++die_counter);\n\tprint_modules();\n\tshow_regs(regs);\n\n\tprintk(\"Process: %s (pid: %d, stack limit = %p)\\n\", current->comm,\n\t\t\ttask_pid_nr(current), task_stack_page(current) + 1);\n\n\tif (!user_mode(regs) || in_interrupt())\n\t\tdump_mem(\"Stack: \", regs->regs[15], THREAD_SIZE +\n\t\t\t (unsigned long)task_stack_page(current));\n\n\tnotify_die(DIE_OOPS, str, regs, err, 255, SIGSEGV);\n\n\tbust_spinlocks(0);\n\tadd_taint(TAINT_DIE);\n\tspin_unlock_irq(&die_lock);\n\toops_exit();\n\n\tif (kexec_should_crash(current))\n\t\tcrash_kexec(regs);\n\n\tif (in_interrupt())\n\t\tpanic(\"Fatal exception in interrupt\");\n\n\tif (panic_on_oops)\n\t\tpanic(\"Fatal exception\");\n\n\tdo_exit(SIGSEGV);\n}\n\nstatic inline void die_if_kernel(const char *str, struct pt_regs *regs,\n\t\t\t\t long err)\n{\n\tif (!user_mode(regs))\n\t\tdie(str, regs, err);\n}\n\n/*\n * try and fix up kernelspace address errors\n * - userspace errors just cause EFAULT to be returned, resulting in SEGV\n * - kernel/userspace interfaces cause a jump to an appropriate handler\n * - other kernel errors are bad\n */\nstatic void die_if_no_fixup(const char * str, struct pt_regs * regs, long err)\n{\n\tif (!user_mode(regs)) {\n\t\tconst struct exception_table_entry *fixup;\n\t\tfixup = search_exception_tables(regs->pc);\n\t\tif (fixup) {\n\t\t\tregs->pc = fixup->fixup;\n\t\t\treturn;\n\t\t}\n\n\t\tdie(str, regs, err);\n\t}\n}\n\nstatic inline void sign_extend(unsigned int count, unsigned char *dst)\n{\n#ifdef __LITTLE_ENDIAN__\n\tif ((count == 1) && dst[0] & 0x80) {\n\t\tdst[1] = 0xff;\n\t\tdst[2] = 0xff;\n\t\tdst[3] = 0xff;\n\t}\n\tif ((count == 2) && dst[1] & 0x80) {\n\t\tdst[2] = 0xff;\n\t\tdst[3] = 0xff;\n\t}\n#else\n\tif ((count == 1) && dst[3] & 0x80) {\n\t\tdst[2] = 0xff;\n\t\tdst[1] = 0xff;\n\t\tdst[0] = 0xff;\n\t}\n\tif ((count == 2) && dst[2] & 0x80) {\n\t\tdst[1] = 0xff;\n\t\tdst[0] = 0xff;\n\t}\n#endif\n}\n\nstatic struct mem_access user_mem_access = {\n\tcopy_from_user,\n\tcopy_to_user,\n};\n\n/*\n * handle an instruction that does an unaligned memory access by emulating the\n * desired behaviour\n * - note that PC _may not_ point to the faulting instruction\n *   (if that instruction is in a branch delay slot)\n * - return 0 if emulation okay, -EFAULT on existential error\n */\nstatic int handle_unaligned_ins(insn_size_t instruction, struct pt_regs *regs,\n\t\t\t\tstruct mem_access *ma)\n{\n\tint ret, index, count;\n\tunsigned long *rm, *rn;\n\tunsigned char *src, *dst;\n\tunsigned char __user *srcu, *dstu;\n\n\tindex = (instruction>>8)&15;\t/* 0x0F00 */\n\trn = &regs->regs[index];\n\n\tindex = (instruction>>4)&15;\t/* 0x00F0 */\n\trm = &regs->regs[index];\n\n\tcount = 1<<(instruction&3);\n\n\tswitch (count) {\n\tcase 1: inc_unaligned_byte_access(); break;\n\tcase 2: inc_unaligned_word_access(); break;\n\tcase 4: inc_unaligned_dword_access(); break;\n\tcase 8: inc_unaligned_multi_access(); break;\n\t}\n\n\tret = -EFAULT;\n\tswitch (instruction>>12) {\n\tcase 0: /* mov.[bwl] to/from memory via r0+rn */\n\t\tif (instruction & 8) {\n\t\t\t/* from memory */\n\t\t\tsrcu = (unsigned char __user *)*rm;\n\t\t\tsrcu += regs->regs[0];\n\t\t\tdst = (unsigned char *)rn;\n\t\t\t*(unsigned long *)dst = 0;\n\n#if !defined(__LITTLE_ENDIAN__)\n\t\t\tdst += 4-count;\n#endif\n\t\t\tif (ma->from(dst, srcu, count))\n\t\t\t\tgoto fetch_fault;\n\n\t\t\tsign_extend(count, dst);\n\t\t} else {\n\t\t\t/* to memory */\n\t\t\tsrc = (unsigned char *)rm;\n#if !defined(__LITTLE_ENDIAN__)\n\t\t\tsrc += 4-count;\n#endif\n\t\t\tdstu = (unsigned char __user *)*rn;\n\t\t\tdstu += regs->regs[0];\n\n\t\t\tif (ma->to(dstu, src, count))\n\t\t\t\tgoto fetch_fault;\n\t\t}\n\t\tret = 0;\n\t\tbreak;\n\n\tcase 1: /* mov.l Rm,@(disp,Rn) */\n\t\tsrc = (unsigned char*) rm;\n\t\tdstu = (unsigned char __user *)*rn;\n\t\tdstu += (instruction&0x000F)<<2;\n\n\t\tif (ma->to(dstu, src, 4))\n\t\t\tgoto fetch_fault;\n\t\tret = 0;\n\t\tbreak;\n\n\tcase 2: /* mov.[bwl] to memory, possibly with pre-decrement */\n\t\tif (instruction & 4)\n\t\t\t*rn -= count;\n\t\tsrc = (unsigned char*) rm;\n\t\tdstu = (unsigned char __user *)*rn;\n#if !defined(__LITTLE_ENDIAN__)\n\t\tsrc += 4-count;\n#endif\n\t\tif (ma->to(dstu, src, count))\n\t\t\tgoto fetch_fault;\n\t\tret = 0;\n\t\tbreak;\n\n\tcase 5: /* mov.l @(disp,Rm),Rn */\n\t\tsrcu = (unsigned char __user *)*rm;\n\t\tsrcu += (instruction & 0x000F) << 2;\n\t\tdst = (unsigned char *)rn;\n\t\t*(unsigned long *)dst = 0;\n\n\t\tif (ma->from(dst, srcu, 4))\n\t\t\tgoto fetch_fault;\n\t\tret = 0;\n\t\tbreak;\n\n\tcase 6:\t/* mov.[bwl] from memory, possibly with post-increment */\n\t\tsrcu = (unsigned char __user *)*rm;\n\t\tif (instruction & 4)\n\t\t\t*rm += count;\n\t\tdst = (unsigned char*) rn;\n\t\t*(unsigned long*)dst = 0;\n\n#if !defined(__LITTLE_ENDIAN__)\n\t\tdst += 4-count;\n#endif\n\t\tif (ma->from(dst, srcu, count))\n\t\t\tgoto fetch_fault;\n\t\tsign_extend(count, dst);\n\t\tret = 0;\n\t\tbreak;\n\n\tcase 8:\n\t\tswitch ((instruction&0xFF00)>>8) {\n\t\tcase 0x81: /* mov.w R0,@(disp,Rn) */\n\t\t\tsrc = (unsigned char *) &regs->regs[0];\n#if !defined(__LITTLE_ENDIAN__)\n\t\t\tsrc += 2;\n#endif\n\t\t\tdstu = (unsigned char __user *)*rm; /* called Rn in the spec */\n\t\t\tdstu += (instruction & 0x000F) << 1;\n\n\t\t\tif (ma->to(dstu, src, 2))\n\t\t\t\tgoto fetch_fault;\n\t\t\tret = 0;\n\t\t\tbreak;\n\n\t\tcase 0x85: /* mov.w @(disp,Rm),R0 */\n\t\t\tsrcu = (unsigned char __user *)*rm;\n\t\t\tsrcu += (instruction & 0x000F) << 1;\n\t\t\tdst = (unsigned char *) &regs->regs[0];\n\t\t\t*(unsigned long *)dst = 0;\n\n#if !defined(__LITTLE_ENDIAN__)\n\t\t\tdst += 2;\n#endif\n\t\t\tif (ma->from(dst, srcu, 2))\n\t\t\t\tgoto fetch_fault;\n\t\t\tsign_extend(2, dst);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\treturn ret;\n\n fetch_fault:\n\t/* Argh. Address not only misaligned but also non-existent.\n\t * Raise an EFAULT and see if it's trapped\n\t */\n\tdie_if_no_fixup(\"Fault in unaligned fixup\", regs, 0);\n\treturn -EFAULT;\n}\n\n/*\n * emulate the instruction in the delay slot\n * - fetches the instruction from PC+2\n */\nstatic inline int handle_delayslot(struct pt_regs *regs,\n\t\t\t\t   insn_size_t old_instruction,\n\t\t\t\t   struct mem_access *ma)\n{\n\tinsn_size_t instruction;\n\tvoid __user *addr = (void __user *)(regs->pc +\n\t\tinstruction_size(old_instruction));\n\n\tif (copy_from_user(&instruction, addr, sizeof(instruction))) {\n\t\t/* the instruction-fetch faulted */\n\t\tif (user_mode(regs))\n\t\t\treturn -EFAULT;\n\n\t\t/* kernel */\n\t\tdie(\"delay-slot-insn faulting in handle_unaligned_delayslot\",\n\t\t    regs, 0);\n\t}\n\n\treturn handle_unaligned_ins(instruction, regs, ma);\n}\n\n/*\n * handle an instruction that does an unaligned memory access\n * - have to be careful of branch delay-slot instructions that fault\n *  SH3:\n *   - if the branch would be taken PC points to the branch\n *   - if the branch would not be taken, PC points to delay-slot\n *  SH4:\n *   - PC always points to delayed branch\n * - return 0 if handled, -EFAULT if failed (may not return if in kernel)\n */\n\n/* Macros to determine offset from current PC for branch instructions */\n/* Explicit type coercion is used to force sign extension where needed */\n#define SH_PC_8BIT_OFFSET(instr) ((((signed char)(instr))*2) + 4)\n#define SH_PC_12BIT_OFFSET(instr) ((((signed short)(instr<<4))>>3) + 4)\n\nint handle_unaligned_access(insn_size_t instruction, struct pt_regs *regs,\n\t\t\t    struct mem_access *ma, int expected,\n\t\t\t    unsigned long address)\n{\n\tu_int rm;\n\tint ret, index;\n\n\t/*\n\t * XXX: We can't handle mixed 16/32-bit instructions yet\n\t */\n\tif (instruction_size(instruction) != 2)\n\t\treturn -EINVAL;\n\n\tindex = (instruction>>8)&15;\t/* 0x0F00 */\n\trm = regs->regs[index];\n\n\t/*\n\t * Log the unexpected fixups, and then pass them on to perf.\n\t *\n\t * We intentionally don't report the expected cases to perf as\n\t * otherwise the trapped I/O case will skew the results too much\n\t * to be useful.\n\t */\n\tif (!expected) {\n\t\tunaligned_fixups_notify(current, instruction, regs);\n\t\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1,\n\t\t\t      regs, address);\n\t}\n\n\tret = -EFAULT;\n\tswitch (instruction&0xF000) {\n\tcase 0x0000:\n\t\tif (instruction==0x000B) {\n\t\t\t/* rts */\n\t\t\tret = handle_delayslot(regs, instruction, ma);\n\t\t\tif (ret==0)\n\t\t\t\tregs->pc = regs->pr;\n\t\t}\n\t\telse if ((instruction&0x00FF)==0x0023) {\n\t\t\t/* braf @Rm */\n\t\t\tret = handle_delayslot(regs, instruction, ma);\n\t\t\tif (ret==0)\n\t\t\t\tregs->pc += rm + 4;\n\t\t}\n\t\telse if ((instruction&0x00FF)==0x0003) {\n\t\t\t/* bsrf @Rm */\n\t\t\tret = handle_delayslot(regs, instruction, ma);\n\t\t\tif (ret==0) {\n\t\t\t\tregs->pr = regs->pc + 4;\n\t\t\t\tregs->pc += rm + 4;\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t/* mov.[bwl] to/from memory via r0+rn */\n\t\t\tgoto simple;\n\t\t}\n\t\tbreak;\n\n\tcase 0x1000: /* mov.l Rm,@(disp,Rn) */\n\t\tgoto simple;\n\n\tcase 0x2000: /* mov.[bwl] to memory, possibly with pre-decrement */\n\t\tgoto simple;\n\n\tcase 0x4000:\n\t\tif ((instruction&0x00FF)==0x002B) {\n\t\t\t/* jmp @Rm */\n\t\t\tret = handle_delayslot(regs, instruction, ma);\n\t\t\tif (ret==0)\n\t\t\t\tregs->pc = rm;\n\t\t}\n\t\telse if ((instruction&0x00FF)==0x000B) {\n\t\t\t/* jsr @Rm */\n\t\t\tret = handle_delayslot(regs, instruction, ma);\n\t\t\tif (ret==0) {\n\t\t\t\tregs->pr = regs->pc + 4;\n\t\t\t\tregs->pc = rm;\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t/* mov.[bwl] to/from memory via r0+rn */\n\t\t\tgoto simple;\n\t\t}\n\t\tbreak;\n\n\tcase 0x5000: /* mov.l @(disp,Rm),Rn */\n\t\tgoto simple;\n\n\tcase 0x6000: /* mov.[bwl] from memory, possibly with post-increment */\n\t\tgoto simple;\n\n\tcase 0x8000: /* bf lab, bf/s lab, bt lab, bt/s lab */\n\t\tswitch (instruction&0x0F00) {\n\t\tcase 0x0100: /* mov.w R0,@(disp,Rm) */\n\t\t\tgoto simple;\n\t\tcase 0x0500: /* mov.w @(disp,Rm),R0 */\n\t\t\tgoto simple;\n\t\tcase 0x0B00: /* bf   lab - no delayslot*/\n\t\t\tbreak;\n\t\tcase 0x0F00: /* bf/s lab */\n\t\t\tret = handle_delayslot(regs, instruction, ma);\n\t\t\tif (ret==0) {\n#if defined(CONFIG_CPU_SH4) || defined(CONFIG_SH7705_CACHE_32KB)\n\t\t\t\tif ((regs->sr & 0x00000001) != 0)\n\t\t\t\t\tregs->pc += 4; /* next after slot */\n\t\t\t\telse\n#endif\n\t\t\t\t\tregs->pc += SH_PC_8BIT_OFFSET(instruction);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 0x0900: /* bt   lab - no delayslot */\n\t\t\tbreak;\n\t\tcase 0x0D00: /* bt/s lab */\n\t\t\tret = handle_delayslot(regs, instruction, ma);\n\t\t\tif (ret==0) {\n#if defined(CONFIG_CPU_SH4) || defined(CONFIG_SH7705_CACHE_32KB)\n\t\t\t\tif ((regs->sr & 0x00000001) == 0)\n\t\t\t\t\tregs->pc += 4; /* next after slot */\n\t\t\t\telse\n#endif\n\t\t\t\t\tregs->pc += SH_PC_8BIT_OFFSET(instruction);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase 0xA000: /* bra label */\n\t\tret = handle_delayslot(regs, instruction, ma);\n\t\tif (ret==0)\n\t\t\tregs->pc += SH_PC_12BIT_OFFSET(instruction);\n\t\tbreak;\n\n\tcase 0xB000: /* bsr label */\n\t\tret = handle_delayslot(regs, instruction, ma);\n\t\tif (ret==0) {\n\t\t\tregs->pr = regs->pc + 4;\n\t\t\tregs->pc += SH_PC_12BIT_OFFSET(instruction);\n\t\t}\n\t\tbreak;\n\t}\n\treturn ret;\n\n\t/* handle non-delay-slot instruction */\n simple:\n\tret = handle_unaligned_ins(instruction, regs, ma);\n\tif (ret==0)\n\t\tregs->pc += instruction_size(instruction);\n\treturn ret;\n}\n\n/*\n * Handle various address error exceptions:\n *  - instruction address error:\n *       misaligned PC\n *       PC >= 0x80000000 in user mode\n *  - data address error (read and write)\n *       misaligned data access\n *       access to >= 0x80000000 is user mode\n * Unfortuntaly we can't distinguish between instruction address error\n * and data address errors caused by read accesses.\n */\nasmlinkage void do_address_error(struct pt_regs *regs,\n\t\t\t\t unsigned long writeaccess,\n\t\t\t\t unsigned long address)\n{\n\tunsigned long error_code = 0;\n\tmm_segment_t oldfs;\n\tsiginfo_t info;\n\tinsn_size_t instruction;\n\tint tmp;\n\n\t/* Intentional ifdef */\n#ifdef CONFIG_CPU_HAS_SR_RB\n\terror_code = lookup_exception_vector();\n#endif\n\n\toldfs = get_fs();\n\n\tif (user_mode(regs)) {\n\t\tint si_code = BUS_ADRERR;\n\t\tunsigned int user_action;\n\n\t\tlocal_irq_enable();\n\t\tinc_unaligned_user_access();\n\n\t\tset_fs(USER_DS);\n\t\tif (copy_from_user(&instruction, (insn_size_t *)(regs->pc & ~1),\n\t\t\t\t   sizeof(instruction))) {\n\t\t\tset_fs(oldfs);\n\t\t\tgoto uspace_segv;\n\t\t}\n\t\tset_fs(oldfs);\n\n\t\t/* shout about userspace fixups */\n\t\tunaligned_fixups_notify(current, instruction, regs);\n\n\t\tuser_action = unaligned_user_action();\n\t\tif (user_action & UM_FIXUP)\n\t\t\tgoto fixup;\n\t\tif (user_action & UM_SIGNAL)\n\t\t\tgoto uspace_segv;\n\t\telse {\n\t\t\t/* ignore */\n\t\t\tregs->pc += instruction_size(instruction);\n\t\t\treturn;\n\t\t}\n\nfixup:\n\t\t/* bad PC is not something we can fix */\n\t\tif (regs->pc & 1) {\n\t\t\tsi_code = BUS_ADRALN;\n\t\t\tgoto uspace_segv;\n\t\t}\n\n\t\tset_fs(USER_DS);\n\t\ttmp = handle_unaligned_access(instruction, regs,\n\t\t\t\t\t      &user_mem_access, 0,\n\t\t\t\t\t      address);\n\t\tset_fs(oldfs);\n\n\t\tif (tmp == 0)\n\t\t\treturn; /* sorted */\nuspace_segv:\n\t\tprintk(KERN_NOTICE \"Sending SIGBUS to \\\"%s\\\" due to unaligned \"\n\t\t       \"access (PC %lx PR %lx)\\n\", current->comm, regs->pc,\n\t\t       regs->pr);\n\n\t\tinfo.si_signo = SIGBUS;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code = si_code;\n\t\tinfo.si_addr = (void __user *)address;\n\t\tforce_sig_info(SIGBUS, &info, current);\n\t} else {\n\t\tinc_unaligned_kernel_access();\n\n\t\tif (regs->pc & 1)\n\t\t\tdie(\"unaligned program counter\", regs, error_code);\n\n\t\tset_fs(KERNEL_DS);\n\t\tif (copy_from_user(&instruction, (void __user *)(regs->pc),\n\t\t\t\t   sizeof(instruction))) {\n\t\t\t/* Argh. Fault on the instruction itself.\n\t\t\t   This should never happen non-SMP\n\t\t\t*/\n\t\t\tset_fs(oldfs);\n\t\t\tdie(\"insn faulting in do_address_error\", regs, 0);\n\t\t}\n\n\t\tunaligned_fixups_notify(current, instruction, regs);\n\n\t\thandle_unaligned_access(instruction, regs, &user_mem_access,\n\t\t\t\t\t0, address);\n\t\tset_fs(oldfs);\n\t}\n}\n\n#ifdef CONFIG_SH_DSP\n/*\n *\tSH-DSP support gerg@snapgear.com.\n */\nint is_dsp_inst(struct pt_regs *regs)\n{\n\tunsigned short inst = 0;\n\n\t/*\n\t * Safe guard if DSP mode is already enabled or we're lacking\n\t * the DSP altogether.\n\t */\n\tif (!(current_cpu_data.flags & CPU_HAS_DSP) || (regs->sr & SR_DSP))\n\t\treturn 0;\n\n\tget_user(inst, ((unsigned short *) regs->pc));\n\n\tinst &= 0xf000;\n\n\t/* Check for any type of DSP or support instruction */\n\tif ((inst == 0xf000) || (inst == 0x4000))\n\t\treturn 1;\n\n\treturn 0;\n}\n#else\n#define is_dsp_inst(regs)\t(0)\n#endif /* CONFIG_SH_DSP */\n\n#ifdef CONFIG_CPU_SH2A\nasmlinkage void do_divide_error(unsigned long r4, unsigned long r5,\n\t\t\t\tunsigned long r6, unsigned long r7,\n\t\t\t\tstruct pt_regs __regs)\n{\n\tsiginfo_t info;\n\n\tswitch (r4) {\n\tcase TRAP_DIVZERO_ERROR:\n\t\tinfo.si_code = FPE_INTDIV;\n\t\tbreak;\n\tcase TRAP_DIVOVF_ERROR:\n\t\tinfo.si_code = FPE_INTOVF;\n\t\tbreak;\n\t}\n\n\tforce_sig_info(SIGFPE, &info, current);\n}\n#endif\n\nasmlinkage void do_reserved_inst(unsigned long r4, unsigned long r5,\n\t\t\t\tunsigned long r6, unsigned long r7,\n\t\t\t\tstruct pt_regs __regs)\n{\n\tstruct pt_regs *regs = RELOC_HIDE(&__regs, 0);\n\tunsigned long error_code;\n\tstruct task_struct *tsk = current;\n\n#ifdef CONFIG_SH_FPU_EMU\n\tunsigned short inst = 0;\n\tint err;\n\n\tget_user(inst, (unsigned short*)regs->pc);\n\n\terr = do_fpu_inst(inst, regs);\n\tif (!err) {\n\t\tregs->pc += instruction_size(inst);\n\t\treturn;\n\t}\n\t/* not a FPU inst. */\n#endif\n\n#ifdef CONFIG_SH_DSP\n\t/* Check if it's a DSP instruction */\n\tif (is_dsp_inst(regs)) {\n\t\t/* Enable DSP mode, and restart instruction. */\n\t\tregs->sr |= SR_DSP;\n\t\t/* Save DSP mode */\n\t\ttsk->thread.dsp_status.status |= SR_DSP;\n\t\treturn;\n\t}\n#endif\n\n\terror_code = lookup_exception_vector();\n\n\tlocal_irq_enable();\n\tforce_sig(SIGILL, tsk);\n\tdie_if_no_fixup(\"reserved instruction\", regs, error_code);\n}\n\n#ifdef CONFIG_SH_FPU_EMU\nstatic int emulate_branch(unsigned short inst, struct pt_regs *regs)\n{\n\t/*\n\t * bfs: 8fxx: PC+=d*2+4;\n\t * bts: 8dxx: PC+=d*2+4;\n\t * bra: axxx: PC+=D*2+4;\n\t * bsr: bxxx: PC+=D*2+4  after PR=PC+4;\n\t * braf:0x23: PC+=Rn*2+4;\n\t * bsrf:0x03: PC+=Rn*2+4 after PR=PC+4;\n\t * jmp: 4x2b: PC=Rn;\n\t * jsr: 4x0b: PC=Rn      after PR=PC+4;\n\t * rts: 000b: PC=PR;\n\t */\n\tif (((inst & 0xf000) == 0xb000)  ||\t/* bsr */\n\t    ((inst & 0xf0ff) == 0x0003)  ||\t/* bsrf */\n\t    ((inst & 0xf0ff) == 0x400b))\t/* jsr */\n\t\tregs->pr = regs->pc + 4;\n\n\tif ((inst & 0xfd00) == 0x8d00) {\t/* bfs, bts */\n\t\tregs->pc += SH_PC_8BIT_OFFSET(inst);\n\t\treturn 0;\n\t}\n\n\tif ((inst & 0xe000) == 0xa000) {\t/* bra, bsr */\n\t\tregs->pc += SH_PC_12BIT_OFFSET(inst);\n\t\treturn 0;\n\t}\n\n\tif ((inst & 0xf0df) == 0x0003) {\t/* braf, bsrf */\n\t\tregs->pc += regs->regs[(inst & 0x0f00) >> 8] + 4;\n\t\treturn 0;\n\t}\n\n\tif ((inst & 0xf0df) == 0x400b) {\t/* jmp, jsr */\n\t\tregs->pc = regs->regs[(inst & 0x0f00) >> 8];\n\t\treturn 0;\n\t}\n\n\tif ((inst & 0xffff) == 0x000b) {\t/* rts */\n\t\tregs->pc = regs->pr;\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n#endif\n\nasmlinkage void do_illegal_slot_inst(unsigned long r4, unsigned long r5,\n\t\t\t\tunsigned long r6, unsigned long r7,\n\t\t\t\tstruct pt_regs __regs)\n{\n\tstruct pt_regs *regs = RELOC_HIDE(&__regs, 0);\n\tunsigned long inst;\n\tstruct task_struct *tsk = current;\n\n\tif (kprobe_handle_illslot(regs->pc) == 0)\n\t\treturn;\n\n#ifdef CONFIG_SH_FPU_EMU\n\tget_user(inst, (unsigned short *)regs->pc + 1);\n\tif (!do_fpu_inst(inst, regs)) {\n\t\tget_user(inst, (unsigned short *)regs->pc);\n\t\tif (!emulate_branch(inst, regs))\n\t\t\treturn;\n\t\t/* fault in branch.*/\n\t}\n\t/* not a FPU inst. */\n#endif\n\n\tinst = lookup_exception_vector();\n\n\tlocal_irq_enable();\n\tforce_sig(SIGILL, tsk);\n\tdie_if_no_fixup(\"illegal slot instruction\", regs, inst);\n}\n\nasmlinkage void do_exception_error(unsigned long r4, unsigned long r5,\n\t\t\t\t   unsigned long r6, unsigned long r7,\n\t\t\t\t   struct pt_regs __regs)\n{\n\tstruct pt_regs *regs = RELOC_HIDE(&__regs, 0);\n\tlong ex;\n\n\tex = lookup_exception_vector();\n\tdie_if_kernel(\"exception\", regs, ex);\n}\n\nvoid __cpuinit per_cpu_trap_init(void)\n{\n\textern void *vbr_base;\n\n\t/* NOTE: The VBR value should be at P1\n\t   (or P2, virtural \"fixed\" address space).\n\t   It's definitely should not in physical address.  */\n\n\tasm volatile(\"ldc\t%0, vbr\"\n\t\t     : /* no output */\n\t\t     : \"r\" (&vbr_base)\n\t\t     : \"memory\");\n\n\t/* disable exception blocking now when the vbr has been setup */\n\tclear_bl_bit();\n}\n\nvoid *set_exception_table_vec(unsigned int vec, void *handler)\n{\n\textern void *exception_handling_table[];\n\tvoid *old_handler;\n\n\told_handler = exception_handling_table[vec];\n\texception_handling_table[vec] = handler;\n\treturn old_handler;\n}\n\nvoid __init trap_init(void)\n{\n\tset_exception_table_vec(TRAP_RESERVED_INST, do_reserved_inst);\n\tset_exception_table_vec(TRAP_ILLEGAL_SLOT_INST, do_illegal_slot_inst);\n\n#if defined(CONFIG_CPU_SH4) && !defined(CONFIG_SH_FPU) || \\\n    defined(CONFIG_SH_FPU_EMU)\n\t/*\n\t * For SH-4 lacking an FPU, treat floating point instructions as\n\t * reserved. They'll be handled in the math-emu case, or faulted on\n\t * otherwise.\n\t */\n\tset_exception_table_evt(0x800, do_reserved_inst);\n\tset_exception_table_evt(0x820, do_illegal_slot_inst);\n#elif defined(CONFIG_SH_FPU)\n\tset_exception_table_evt(0x800, fpu_state_restore_trap_handler);\n\tset_exception_table_evt(0x820, fpu_state_restore_trap_handler);\n#endif\n\n#ifdef CONFIG_CPU_SH2\n\tset_exception_table_vec(TRAP_ADDRESS_ERROR, address_error_trap_handler);\n#endif\n#ifdef CONFIG_CPU_SH2A\n\tset_exception_table_vec(TRAP_DIVZERO_ERROR, do_divide_error);\n\tset_exception_table_vec(TRAP_DIVOVF_ERROR, do_divide_error);\n#ifdef CONFIG_SH_FPU\n\tset_exception_table_vec(TRAP_FPU_ERROR, fpu_error_trap_handler);\n#endif\n#endif\n\n#ifdef TRAP_UBC\n\tset_exception_table_vec(TRAP_UBC, breakpoint_trap_handler);\n#endif\n}\n\nvoid show_stack(struct task_struct *tsk, unsigned long *sp)\n{\n\tunsigned long stack;\n\n\tif (!tsk)\n\t\ttsk = current;\n\tif (tsk == current)\n\t\tsp = (unsigned long *)current_stack_pointer;\n\telse\n\t\tsp = (unsigned long *)tsk->thread.sp;\n\n\tstack = (unsigned long)sp;\n\tdump_mem(\"Stack: \", stack, THREAD_SIZE +\n\t\t (unsigned long)task_stack_page(tsk));\n\tshow_trace(tsk, sp, NULL);\n}\n\nvoid dump_stack(void)\n{\n\tshow_stack(NULL, NULL);\n}\nEXPORT_SYMBOL(dump_stack);\n", "/*\n * arch/sh/kernel/traps_64.c\n *\n * Copyright (C) 2000, 2001  Paolo Alberelli\n * Copyright (C) 2003, 2004  Paul Mundt\n * Copyright (C) 2003, 2004  Richard Curnow\n *\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n */\n#include <linux/sched.h>\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/errno.h>\n#include <linux/ptrace.h>\n#include <linux/timer.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/init.h>\n#include <linux/delay.h>\n#include <linux/spinlock.h>\n#include <linux/kallsyms.h>\n#include <linux/interrupt.h>\n#include <linux/sysctl.h>\n#include <linux/module.h>\n#include <linux/perf_event.h>\n#include <asm/system.h>\n#include <asm/uaccess.h>\n#include <asm/io.h>\n#include <asm/atomic.h>\n#include <asm/processor.h>\n#include <asm/pgtable.h>\n#include <asm/fpu.h>\n\n#undef DEBUG_EXCEPTION\n#ifdef DEBUG_EXCEPTION\n/* implemented in ../lib/dbg.c */\nextern void show_excp_regs(char *fname, int trapnr, int signr,\n\t\t\t   struct pt_regs *regs);\n#else\n#define show_excp_regs(a, b, c, d)\n#endif\n\nstatic void do_unhandled_exception(int trapnr, int signr, char *str, char *fn_name,\n\t\tunsigned long error_code, struct pt_regs *regs, struct task_struct *tsk);\n\n#define DO_ERROR(trapnr, signr, str, name, tsk) \\\nasmlinkage void do_##name(unsigned long error_code, struct pt_regs *regs) \\\n{ \\\n\tdo_unhandled_exception(trapnr, signr, str, __stringify(name), error_code, regs, current); \\\n}\n\nstatic DEFINE_SPINLOCK(die_lock);\n\nvoid die(const char * str, struct pt_regs * regs, long err)\n{\n\tconsole_verbose();\n\tspin_lock_irq(&die_lock);\n\tprintk(\"%s: %lx\\n\", str, (err & 0xffffff));\n\tshow_regs(regs);\n\tspin_unlock_irq(&die_lock);\n\tdo_exit(SIGSEGV);\n}\n\nstatic inline void die_if_kernel(const char * str, struct pt_regs * regs, long err)\n{\n\tif (!user_mode(regs))\n\t\tdie(str, regs, err);\n}\n\nstatic void die_if_no_fixup(const char * str, struct pt_regs * regs, long err)\n{\n\tif (!user_mode(regs)) {\n\t\tconst struct exception_table_entry *fixup;\n\t\tfixup = search_exception_tables(regs->pc);\n\t\tif (fixup) {\n\t\t\tregs->pc = fixup->fixup;\n\t\t\treturn;\n\t\t}\n\t\tdie(str, regs, err);\n\t}\n}\n\nDO_ERROR(13, SIGILL,  \"illegal slot instruction\", illegal_slot_inst, current)\nDO_ERROR(87, SIGSEGV, \"address error (exec)\", address_error_exec, current)\n\n\n/* Implement misaligned load/store handling for kernel (and optionally for user\n   mode too).  Limitation : only SHmedia mode code is handled - there is no\n   handling at all for misaligned accesses occurring in SHcompact code yet. */\n\nstatic int misaligned_fixup(struct pt_regs *regs);\n\nasmlinkage void do_address_error_load(unsigned long error_code, struct pt_regs *regs)\n{\n\tif (misaligned_fixup(regs) < 0) {\n\t\tdo_unhandled_exception(7, SIGSEGV, \"address error(load)\",\n\t\t\t\t\"do_address_error_load\",\n\t\t\t\terror_code, regs, current);\n\t}\n\treturn;\n}\n\nasmlinkage void do_address_error_store(unsigned long error_code, struct pt_regs *regs)\n{\n\tif (misaligned_fixup(regs) < 0) {\n\t\tdo_unhandled_exception(8, SIGSEGV, \"address error(store)\",\n\t\t\t\t\"do_address_error_store\",\n\t\t\t\terror_code, regs, current);\n\t}\n\treturn;\n}\n\n#if defined(CONFIG_SH64_ID2815_WORKAROUND)\n\n#define OPCODE_INVALID      0\n#define OPCODE_USER_VALID   1\n#define OPCODE_PRIV_VALID   2\n\n/* getcon/putcon - requires checking which control register is referenced. */\n#define OPCODE_CTRL_REG     3\n\n/* Table of valid opcodes for SHmedia mode.\n   Form a 10-bit value by concatenating the major/minor opcodes i.e.\n   opcode[31:26,20:16].  The 6 MSBs of this value index into the following\n   array.  The 4 LSBs select the bit-pair in the entry (bits 1:0 correspond to\n   LSBs==4'b0000 etc). */\nstatic unsigned long shmedia_opcode_table[64] = {\n\t0x55554044,0x54445055,0x15141514,0x14541414,0x00000000,0x10001000,0x01110055,0x04050015,\n\t0x00000444,0xc0000000,0x44545515,0x40405555,0x55550015,0x10005555,0x55555505,0x04050000,\n\t0x00000555,0x00000404,0x00040445,0x15151414,0x00000000,0x00000000,0x00000000,0x00000000,\n\t0x00000055,0x40404444,0x00000404,0xc0009495,0x00000000,0x00000000,0x00000000,0x00000000,\n\t0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,\n\t0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,\n\t0x80005050,0x04005055,0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,0x55555555,\n\t0x81055554,0x00000404,0x55555555,0x55555555,0x00000000,0x00000000,0x00000000,0x00000000\n};\n\nvoid do_reserved_inst(unsigned long error_code, struct pt_regs *regs)\n{\n\t/* Workaround SH5-101 cut2 silicon defect #2815 :\n\t   in some situations, inter-mode branches from SHcompact -> SHmedia\n\t   which should take ITLBMISS or EXECPROT exceptions at the target\n\t   falsely take RESINST at the target instead. */\n\n\tunsigned long opcode = 0x6ff4fff0; /* guaranteed reserved opcode */\n\tunsigned long pc, aligned_pc;\n\tint get_user_error;\n\tint trapnr = 12;\n\tint signr = SIGILL;\n\tchar *exception_name = \"reserved_instruction\";\n\n\tpc = regs->pc;\n\tif ((pc & 3) == 1) {\n\t\t/* SHmedia : check for defect.  This requires executable vmas\n\t\t   to be readable too. */\n\t\taligned_pc = pc & ~3;\n\t\tif (!access_ok(VERIFY_READ, aligned_pc, sizeof(unsigned long))) {\n\t\t\tget_user_error = -EFAULT;\n\t\t} else {\n\t\t\tget_user_error = __get_user(opcode, (unsigned long *)aligned_pc);\n\t\t}\n\t\tif (get_user_error >= 0) {\n\t\t\tunsigned long index, shift;\n\t\t\tunsigned long major, minor, combined;\n\t\t\tunsigned long reserved_field;\n\t\t\treserved_field = opcode & 0xf; /* These bits are currently reserved as zero in all valid opcodes */\n\t\t\tmajor = (opcode >> 26) & 0x3f;\n\t\t\tminor = (opcode >> 16) & 0xf;\n\t\t\tcombined = (major << 4) | minor;\n\t\t\tindex = major;\n\t\t\tshift = minor << 1;\n\t\t\tif (reserved_field == 0) {\n\t\t\t\tint opcode_state = (shmedia_opcode_table[index] >> shift) & 0x3;\n\t\t\t\tswitch (opcode_state) {\n\t\t\t\t\tcase OPCODE_INVALID:\n\t\t\t\t\t\t/* Trap. */\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase OPCODE_USER_VALID:\n\t\t\t\t\t\t/* Restart the instruction : the branch to the instruction will now be from an RTE\n\t\t\t\t\t\t   not from SHcompact so the silicon defect won't be triggered. */\n\t\t\t\t\t\treturn;\n\t\t\t\t\tcase OPCODE_PRIV_VALID:\n\t\t\t\t\t\tif (!user_mode(regs)) {\n\t\t\t\t\t\t\t/* Should only ever get here if a module has\n\t\t\t\t\t\t\t   SHcompact code inside it.  If so, the same fix up is needed. */\n\t\t\t\t\t\t\treturn; /* same reason */\n\t\t\t\t\t\t}\n\t\t\t\t\t\t/* Otherwise, user mode trying to execute a privileged instruction -\n\t\t\t\t\t\t   fall through to trap. */\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase OPCODE_CTRL_REG:\n\t\t\t\t\t\t/* If in privileged mode, return as above. */\n\t\t\t\t\t\tif (!user_mode(regs)) return;\n\t\t\t\t\t\t/* In user mode ... */\n\t\t\t\t\t\tif (combined == 0x9f) { /* GETCON */\n\t\t\t\t\t\t\tunsigned long regno = (opcode >> 20) & 0x3f;\n\t\t\t\t\t\t\tif (regno >= 62) {\n\t\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t/* Otherwise, reserved or privileged control register, => trap */\n\t\t\t\t\t\t} else if (combined == 0x1bf) { /* PUTCON */\n\t\t\t\t\t\t\tunsigned long regno = (opcode >> 4) & 0x3f;\n\t\t\t\t\t\t\tif (regno >= 62) {\n\t\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t/* Otherwise, reserved or privileged control register, => trap */\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t/* Trap */\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tdefault:\n\t\t\t\t\t\t/* Fall through to trap. */\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\t/* fall through to normal resinst processing */\n\t\t} else {\n\t\t\t/* Error trying to read opcode.  This typically means a\n\t\t\t   real fault, not a RESINST any more.  So change the\n\t\t\t   codes. */\n\t\t\ttrapnr = 87;\n\t\t\texception_name = \"address error (exec)\";\n\t\t\tsignr = SIGSEGV;\n\t\t}\n\t}\n\n\tdo_unhandled_exception(trapnr, signr, exception_name, \"do_reserved_inst\", error_code, regs, current);\n}\n\n#else /* CONFIG_SH64_ID2815_WORKAROUND */\n\n/* If the workaround isn't needed, this is just a straightforward reserved\n   instruction */\nDO_ERROR(12, SIGILL,  \"reserved instruction\", reserved_inst, current)\n\n#endif /* CONFIG_SH64_ID2815_WORKAROUND */\n\n/* Called with interrupts disabled */\nasmlinkage void do_exception_error(unsigned long ex, struct pt_regs *regs)\n{\n\tshow_excp_regs(__func__, -1, -1, regs);\n\tdie_if_kernel(\"exception\", regs, ex);\n}\n\nint do_unknown_trapa(unsigned long scId, struct pt_regs *regs)\n{\n\t/* Syscall debug */\n        printk(\"System call ID error: [0x1#args:8 #syscall:16  0x%lx]\\n\", scId);\n\n\tdie_if_kernel(\"unknown trapa\", regs, scId);\n\n\treturn -ENOSYS;\n}\n\nvoid show_stack(struct task_struct *tsk, unsigned long *sp)\n{\n#ifdef CONFIG_KALLSYMS\n\textern void sh64_unwind(struct pt_regs *regs);\n\tstruct pt_regs *regs;\n\n\tregs = tsk ? tsk->thread.kregs : NULL;\n\n\tsh64_unwind(regs);\n#else\n\tprintk(KERN_ERR \"Can't backtrace on sh64 without CONFIG_KALLSYMS\\n\");\n#endif\n}\n\nvoid show_task(unsigned long *sp)\n{\n\tshow_stack(NULL, sp);\n}\n\nvoid dump_stack(void)\n{\n\tshow_task(NULL);\n}\n/* Needed by any user of WARN_ON in view of the defn in include/asm-sh/bug.h */\nEXPORT_SYMBOL(dump_stack);\n\nstatic void do_unhandled_exception(int trapnr, int signr, char *str, char *fn_name,\n\t\tunsigned long error_code, struct pt_regs *regs, struct task_struct *tsk)\n{\n\tshow_excp_regs(fn_name, trapnr, signr, regs);\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_no = trapnr;\n\n\tif (user_mode(regs))\n\t\tforce_sig(signr, tsk);\n\n\tdie_if_no_fixup(str, regs, error_code);\n}\n\nstatic int read_opcode(unsigned long long pc, unsigned long *result_opcode, int from_user_mode)\n{\n\tint get_user_error;\n\tunsigned long aligned_pc;\n\tunsigned long opcode;\n\n\tif ((pc & 3) == 1) {\n\t\t/* SHmedia */\n\t\taligned_pc = pc & ~3;\n\t\tif (from_user_mode) {\n\t\t\tif (!access_ok(VERIFY_READ, aligned_pc, sizeof(unsigned long))) {\n\t\t\t\tget_user_error = -EFAULT;\n\t\t\t} else {\n\t\t\t\tget_user_error = __get_user(opcode, (unsigned long *)aligned_pc);\n\t\t\t\t*result_opcode = opcode;\n\t\t\t}\n\t\t\treturn get_user_error;\n\t\t} else {\n\t\t\t/* If the fault was in the kernel, we can either read\n\t\t\t * this directly, or if not, we fault.\n\t\t\t*/\n\t\t\t*result_opcode = *(unsigned long *) aligned_pc;\n\t\t\treturn 0;\n\t\t}\n\t} else if ((pc & 1) == 0) {\n\t\t/* SHcompact */\n\t\t/* TODO : provide handling for this.  We don't really support\n\t\t   user-mode SHcompact yet, and for a kernel fault, this would\n\t\t   have to come from a module built for SHcompact.  */\n\t\treturn -EFAULT;\n\t} else {\n\t\t/* misaligned */\n\t\treturn -EFAULT;\n\t}\n}\n\nstatic int address_is_sign_extended(__u64 a)\n{\n\t__u64 b;\n#if (NEFF == 32)\n\tb = (__u64)(__s64)(__s32)(a & 0xffffffffUL);\n\treturn (b == a) ? 1 : 0;\n#else\n#error \"Sign extend check only works for NEFF==32\"\n#endif\n}\n\nstatic int generate_and_check_address(struct pt_regs *regs,\n\t\t\t\t      __u32 opcode,\n\t\t\t\t      int displacement_not_indexed,\n\t\t\t\t      int width_shift,\n\t\t\t\t      __u64 *address)\n{\n\t/* return -1 for fault, 0 for OK */\n\n\t__u64 base_address, addr;\n\tint basereg;\n\n\tbasereg = (opcode >> 20) & 0x3f;\n\tbase_address = regs->regs[basereg];\n\tif (displacement_not_indexed) {\n\t\t__s64 displacement;\n\t\tdisplacement = (opcode >> 10) & 0x3ff;\n\t\tdisplacement = ((displacement << 54) >> 54); /* sign extend */\n\t\taddr = (__u64)((__s64)base_address + (displacement << width_shift));\n\t} else {\n\t\t__u64 offset;\n\t\tint offsetreg;\n\t\toffsetreg = (opcode >> 10) & 0x3f;\n\t\toffset = regs->regs[offsetreg];\n\t\taddr = base_address + offset;\n\t}\n\n\t/* Check sign extended */\n\tif (!address_is_sign_extended(addr)) {\n\t\treturn -1;\n\t}\n\n\t/* Check accessible.  For misaligned access in the kernel, assume the\n\t   address is always accessible (and if not, just fault when the\n\t   load/store gets done.) */\n\tif (user_mode(regs)) {\n\t\tif (addr >= TASK_SIZE) {\n\t\t\treturn -1;\n\t\t}\n\t\t/* Do access_ok check later - it depends on whether it's a load or a store. */\n\t}\n\n\t*address = addr;\n\treturn 0;\n}\n\nstatic int user_mode_unaligned_fixup_count = 10;\nstatic int user_mode_unaligned_fixup_enable = 1;\nstatic int kernel_mode_unaligned_fixup_count = 32;\n\nstatic void misaligned_kernel_word_load(__u64 address, int do_sign_extend, __u64 *result)\n{\n\tunsigned short x;\n\tunsigned char *p, *q;\n\tp = (unsigned char *) (int) address;\n\tq = (unsigned char *) &x;\n\tq[0] = p[0];\n\tq[1] = p[1];\n\n\tif (do_sign_extend) {\n\t\t*result = (__u64)(__s64) *(short *) &x;\n\t} else {\n\t\t*result = (__u64) x;\n\t}\n}\n\nstatic void misaligned_kernel_word_store(__u64 address, __u64 value)\n{\n\tunsigned short x;\n\tunsigned char *p, *q;\n\tp = (unsigned char *) (int) address;\n\tq = (unsigned char *) &x;\n\n\tx = (__u16) value;\n\tp[0] = q[0];\n\tp[1] = q[1];\n}\n\nstatic int misaligned_load(struct pt_regs *regs,\n\t\t\t   __u32 opcode,\n\t\t\t   int displacement_not_indexed,\n\t\t\t   int width_shift,\n\t\t\t   int do_sign_extend)\n{\n\t/* Return -1 for a fault, 0 for OK */\n\tint error;\n\tint destreg;\n\t__u64 address;\n\n\terror = generate_and_check_address(regs, opcode,\n\t\t\tdisplacement_not_indexed, width_shift, &address);\n\tif (error < 0) {\n\t\treturn error;\n\t}\n\n\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, address);\n\n\tdestreg = (opcode >> 4) & 0x3f;\n\tif (user_mode(regs)) {\n\t\t__u64 buffer;\n\n\t\tif (!access_ok(VERIFY_READ, (unsigned long) address, 1UL<<width_shift)) {\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (__copy_user(&buffer, (const void *)(int)address, (1 << width_shift)) > 0) {\n\t\t\treturn -1; /* fault */\n\t\t}\n\t\tswitch (width_shift) {\n\t\tcase 1:\n\t\t\tif (do_sign_extend) {\n\t\t\t\tregs->regs[destreg] = (__u64)(__s64) *(__s16 *) &buffer;\n\t\t\t} else {\n\t\t\t\tregs->regs[destreg] = (__u64) *(__u16 *) &buffer;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tregs->regs[destreg] = (__u64)(__s64) *(__s32 *) &buffer;\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tregs->regs[destreg] = buffer;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tprintk(\"Unexpected width_shift %d in misaligned_load, PC=%08lx\\n\",\n\t\t\t\twidth_shift, (unsigned long) regs->pc);\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\t/* kernel mode - we can take short cuts since if we fault, it's a genuine bug */\n\t\t__u64 lo, hi;\n\n\t\tswitch (width_shift) {\n\t\tcase 1:\n\t\t\tmisaligned_kernel_word_load(address, do_sign_extend, &regs->regs[destreg]);\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tasm (\"ldlo.l %1, 0, %0\" : \"=r\" (lo) : \"r\" (address));\n\t\t\tasm (\"ldhi.l %1, 3, %0\" : \"=r\" (hi) : \"r\" (address));\n\t\t\tregs->regs[destreg] = lo | hi;\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tasm (\"ldlo.q %1, 0, %0\" : \"=r\" (lo) : \"r\" (address));\n\t\t\tasm (\"ldhi.q %1, 7, %0\" : \"=r\" (hi) : \"r\" (address));\n\t\t\tregs->regs[destreg] = lo | hi;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tprintk(\"Unexpected width_shift %d in misaligned_load, PC=%08lx\\n\",\n\t\t\t\twidth_shift, (unsigned long) regs->pc);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n\n}\n\nstatic int misaligned_store(struct pt_regs *regs,\n\t\t\t    __u32 opcode,\n\t\t\t    int displacement_not_indexed,\n\t\t\t    int width_shift)\n{\n\t/* Return -1 for a fault, 0 for OK */\n\tint error;\n\tint srcreg;\n\t__u64 address;\n\n\terror = generate_and_check_address(regs, opcode,\n\t\t\tdisplacement_not_indexed, width_shift, &address);\n\tif (error < 0) {\n\t\treturn error;\n\t}\n\n\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, address);\n\n\tsrcreg = (opcode >> 4) & 0x3f;\n\tif (user_mode(regs)) {\n\t\t__u64 buffer;\n\n\t\tif (!access_ok(VERIFY_WRITE, (unsigned long) address, 1UL<<width_shift)) {\n\t\t\treturn -1;\n\t\t}\n\n\t\tswitch (width_shift) {\n\t\tcase 1:\n\t\t\t*(__u16 *) &buffer = (__u16) regs->regs[srcreg];\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\t*(__u32 *) &buffer = (__u32) regs->regs[srcreg];\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tbuffer = regs->regs[srcreg];\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tprintk(\"Unexpected width_shift %d in misaligned_store, PC=%08lx\\n\",\n\t\t\t\twidth_shift, (unsigned long) regs->pc);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (__copy_user((void *)(int)address, &buffer, (1 << width_shift)) > 0) {\n\t\t\treturn -1; /* fault */\n\t\t}\n\t} else {\n\t\t/* kernel mode - we can take short cuts since if we fault, it's a genuine bug */\n\t\t__u64 val = regs->regs[srcreg];\n\n\t\tswitch (width_shift) {\n\t\tcase 1:\n\t\t\tmisaligned_kernel_word_store(address, val);\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tasm (\"stlo.l %1, 0, %0\" : : \"r\" (val), \"r\" (address));\n\t\t\tasm (\"sthi.l %1, 3, %0\" : : \"r\" (val), \"r\" (address));\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tasm (\"stlo.q %1, 0, %0\" : : \"r\" (val), \"r\" (address));\n\t\t\tasm (\"sthi.q %1, 7, %0\" : : \"r\" (val), \"r\" (address));\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tprintk(\"Unexpected width_shift %d in misaligned_store, PC=%08lx\\n\",\n\t\t\t\twidth_shift, (unsigned long) regs->pc);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n\n}\n\n/* Never need to fix up misaligned FPU accesses within the kernel since that's a real\n   error. */\nstatic int misaligned_fpu_load(struct pt_regs *regs,\n\t\t\t   __u32 opcode,\n\t\t\t   int displacement_not_indexed,\n\t\t\t   int width_shift,\n\t\t\t   int do_paired_load)\n{\n\t/* Return -1 for a fault, 0 for OK */\n\tint error;\n\tint destreg;\n\t__u64 address;\n\n\terror = generate_and_check_address(regs, opcode,\n\t\t\tdisplacement_not_indexed, width_shift, &address);\n\tif (error < 0) {\n\t\treturn error;\n\t}\n\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, address);\n\n\tdestreg = (opcode >> 4) & 0x3f;\n\tif (user_mode(regs)) {\n\t\t__u64 buffer;\n\t\t__u32 buflo, bufhi;\n\n\t\tif (!access_ok(VERIFY_READ, (unsigned long) address, 1UL<<width_shift)) {\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (__copy_user(&buffer, (const void *)(int)address, (1 << width_shift)) > 0) {\n\t\t\treturn -1; /* fault */\n\t\t}\n\t\t/* 'current' may be the current owner of the FPU state, so\n\t\t   context switch the registers into memory so they can be\n\t\t   indexed by register number. */\n\t\tif (last_task_used_math == current) {\n\t\t\tenable_fpu();\n\t\t\tsave_fpu(current);\n\t\t\tdisable_fpu();\n\t\t\tlast_task_used_math = NULL;\n\t\t\tregs->sr |= SR_FD;\n\t\t}\n\n\t\tbuflo = *(__u32*) &buffer;\n\t\tbufhi = *(1 + (__u32*) &buffer);\n\n\t\tswitch (width_shift) {\n\t\tcase 2:\n\t\t\tcurrent->thread.xstate->hardfpu.fp_regs[destreg] = buflo;\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tif (do_paired_load) {\n\t\t\t\tcurrent->thread.xstate->hardfpu.fp_regs[destreg] = buflo;\n\t\t\t\tcurrent->thread.xstate->hardfpu.fp_regs[destreg+1] = bufhi;\n\t\t\t} else {\n#if defined(CONFIG_CPU_LITTLE_ENDIAN)\n\t\t\t\tcurrent->thread.xstate->hardfpu.fp_regs[destreg] = bufhi;\n\t\t\t\tcurrent->thread.xstate->hardfpu.fp_regs[destreg+1] = buflo;\n#else\n\t\t\t\tcurrent->thread.xstate->hardfpu.fp_regs[destreg] = buflo;\n\t\t\t\tcurrent->thread.xstate->hardfpu.fp_regs[destreg+1] = bufhi;\n#endif\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tprintk(\"Unexpected width_shift %d in misaligned_fpu_load, PC=%08lx\\n\",\n\t\t\t\twidth_shift, (unsigned long) regs->pc);\n\t\t\tbreak;\n\t\t}\n\t\treturn 0;\n\t} else {\n\t\tdie (\"Misaligned FPU load inside kernel\", regs, 0);\n\t\treturn -1;\n\t}\n\n\n}\n\nstatic int misaligned_fpu_store(struct pt_regs *regs,\n\t\t\t   __u32 opcode,\n\t\t\t   int displacement_not_indexed,\n\t\t\t   int width_shift,\n\t\t\t   int do_paired_load)\n{\n\t/* Return -1 for a fault, 0 for OK */\n\tint error;\n\tint srcreg;\n\t__u64 address;\n\n\terror = generate_and_check_address(regs, opcode,\n\t\t\tdisplacement_not_indexed, width_shift, &address);\n\tif (error < 0) {\n\t\treturn error;\n\t}\n\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, address);\n\n\tsrcreg = (opcode >> 4) & 0x3f;\n\tif (user_mode(regs)) {\n\t\t__u64 buffer;\n\t\t/* Initialise these to NaNs. */\n\t\t__u32 buflo=0xffffffffUL, bufhi=0xffffffffUL;\n\n\t\tif (!access_ok(VERIFY_WRITE, (unsigned long) address, 1UL<<width_shift)) {\n\t\t\treturn -1;\n\t\t}\n\n\t\t/* 'current' may be the current owner of the FPU state, so\n\t\t   context switch the registers into memory so they can be\n\t\t   indexed by register number. */\n\t\tif (last_task_used_math == current) {\n\t\t\tenable_fpu();\n\t\t\tsave_fpu(current);\n\t\t\tdisable_fpu();\n\t\t\tlast_task_used_math = NULL;\n\t\t\tregs->sr |= SR_FD;\n\t\t}\n\n\t\tswitch (width_shift) {\n\t\tcase 2:\n\t\t\tbuflo = current->thread.xstate->hardfpu.fp_regs[srcreg];\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tif (do_paired_load) {\n\t\t\t\tbuflo = current->thread.xstate->hardfpu.fp_regs[srcreg];\n\t\t\t\tbufhi = current->thread.xstate->hardfpu.fp_regs[srcreg+1];\n\t\t\t} else {\n#if defined(CONFIG_CPU_LITTLE_ENDIAN)\n\t\t\t\tbufhi = current->thread.xstate->hardfpu.fp_regs[srcreg];\n\t\t\t\tbuflo = current->thread.xstate->hardfpu.fp_regs[srcreg+1];\n#else\n\t\t\t\tbuflo = current->thread.xstate->hardfpu.fp_regs[srcreg];\n\t\t\t\tbufhi = current->thread.xstate->hardfpu.fp_regs[srcreg+1];\n#endif\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tprintk(\"Unexpected width_shift %d in misaligned_fpu_store, PC=%08lx\\n\",\n\t\t\t\twidth_shift, (unsigned long) regs->pc);\n\t\t\tbreak;\n\t\t}\n\n\t\t*(__u32*) &buffer = buflo;\n\t\t*(1 + (__u32*) &buffer) = bufhi;\n\t\tif (__copy_user((void *)(int)address, &buffer, (1 << width_shift)) > 0) {\n\t\t\treturn -1; /* fault */\n\t\t}\n\t\treturn 0;\n\t} else {\n\t\tdie (\"Misaligned FPU load inside kernel\", regs, 0);\n\t\treturn -1;\n\t}\n}\n\nstatic int misaligned_fixup(struct pt_regs *regs)\n{\n\tunsigned long opcode;\n\tint error;\n\tint major, minor;\n\n\tif (!user_mode_unaligned_fixup_enable)\n\t\treturn -1;\n\n\terror = read_opcode(regs->pc, &opcode, user_mode(regs));\n\tif (error < 0) {\n\t\treturn error;\n\t}\n\tmajor = (opcode >> 26) & 0x3f;\n\tminor = (opcode >> 16) & 0xf;\n\n\tif (user_mode(regs) && (user_mode_unaligned_fixup_count > 0)) {\n\t\t--user_mode_unaligned_fixup_count;\n\t\t/* Only do 'count' worth of these reports, to remove a potential DoS against syslog */\n\t\tprintk(\"Fixing up unaligned userspace access in \\\"%s\\\" pid=%d pc=0x%08x ins=0x%08lx\\n\",\n\t\t       current->comm, task_pid_nr(current), (__u32)regs->pc, opcode);\n\t} else if (!user_mode(regs) && (kernel_mode_unaligned_fixup_count > 0)) {\n\t\t--kernel_mode_unaligned_fixup_count;\n\t\tif (in_interrupt()) {\n\t\t\tprintk(\"Fixing up unaligned kernelspace access in interrupt pc=0x%08x ins=0x%08lx\\n\",\n\t\t\t       (__u32)regs->pc, opcode);\n\t\t} else {\n\t\t\tprintk(\"Fixing up unaligned kernelspace access in \\\"%s\\\" pid=%d pc=0x%08x ins=0x%08lx\\n\",\n\t\t\t       current->comm, task_pid_nr(current), (__u32)regs->pc, opcode);\n\t\t}\n\t}\n\n\n\tswitch (major) {\n\t\tcase (0x84>>2): /* LD.W */\n\t\t\terror = misaligned_load(regs, opcode, 1, 1, 1);\n\t\t\tbreak;\n\t\tcase (0xb0>>2): /* LD.UW */\n\t\t\terror = misaligned_load(regs, opcode, 1, 1, 0);\n\t\t\tbreak;\n\t\tcase (0x88>>2): /* LD.L */\n\t\t\terror = misaligned_load(regs, opcode, 1, 2, 1);\n\t\t\tbreak;\n\t\tcase (0x8c>>2): /* LD.Q */\n\t\t\terror = misaligned_load(regs, opcode, 1, 3, 0);\n\t\t\tbreak;\n\n\t\tcase (0xa4>>2): /* ST.W */\n\t\t\terror = misaligned_store(regs, opcode, 1, 1);\n\t\t\tbreak;\n\t\tcase (0xa8>>2): /* ST.L */\n\t\t\terror = misaligned_store(regs, opcode, 1, 2);\n\t\t\tbreak;\n\t\tcase (0xac>>2): /* ST.Q */\n\t\t\terror = misaligned_store(regs, opcode, 1, 3);\n\t\t\tbreak;\n\n\t\tcase (0x40>>2): /* indexed loads */\n\t\t\tswitch (minor) {\n\t\t\t\tcase 0x1: /* LDX.W */\n\t\t\t\t\terror = misaligned_load(regs, opcode, 0, 1, 1);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 0x5: /* LDX.UW */\n\t\t\t\t\terror = misaligned_load(regs, opcode, 0, 1, 0);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 0x2: /* LDX.L */\n\t\t\t\t\terror = misaligned_load(regs, opcode, 0, 2, 1);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 0x3: /* LDX.Q */\n\t\t\t\t\terror = misaligned_load(regs, opcode, 0, 3, 0);\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\terror = -1;\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase (0x60>>2): /* indexed stores */\n\t\t\tswitch (minor) {\n\t\t\t\tcase 0x1: /* STX.W */\n\t\t\t\t\terror = misaligned_store(regs, opcode, 0, 1);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 0x2: /* STX.L */\n\t\t\t\t\terror = misaligned_store(regs, opcode, 0, 2);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 0x3: /* STX.Q */\n\t\t\t\t\terror = misaligned_store(regs, opcode, 0, 3);\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\terror = -1;\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase (0x94>>2): /* FLD.S */\n\t\t\terror = misaligned_fpu_load(regs, opcode, 1, 2, 0);\n\t\t\tbreak;\n\t\tcase (0x98>>2): /* FLD.P */\n\t\t\terror = misaligned_fpu_load(regs, opcode, 1, 3, 1);\n\t\t\tbreak;\n\t\tcase (0x9c>>2): /* FLD.D */\n\t\t\terror = misaligned_fpu_load(regs, opcode, 1, 3, 0);\n\t\t\tbreak;\n\t\tcase (0x1c>>2): /* floating indexed loads */\n\t\t\tswitch (minor) {\n\t\t\tcase 0x8: /* FLDX.S */\n\t\t\t\terror = misaligned_fpu_load(regs, opcode, 0, 2, 0);\n\t\t\t\tbreak;\n\t\t\tcase 0xd: /* FLDX.P */\n\t\t\t\terror = misaligned_fpu_load(regs, opcode, 0, 3, 1);\n\t\t\t\tbreak;\n\t\t\tcase 0x9: /* FLDX.D */\n\t\t\t\terror = misaligned_fpu_load(regs, opcode, 0, 3, 0);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\terror = -1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase (0xb4>>2): /* FLD.S */\n\t\t\terror = misaligned_fpu_store(regs, opcode, 1, 2, 0);\n\t\t\tbreak;\n\t\tcase (0xb8>>2): /* FLD.P */\n\t\t\terror = misaligned_fpu_store(regs, opcode, 1, 3, 1);\n\t\t\tbreak;\n\t\tcase (0xbc>>2): /* FLD.D */\n\t\t\terror = misaligned_fpu_store(regs, opcode, 1, 3, 0);\n\t\t\tbreak;\n\t\tcase (0x3c>>2): /* floating indexed stores */\n\t\t\tswitch (minor) {\n\t\t\tcase 0x8: /* FSTX.S */\n\t\t\t\terror = misaligned_fpu_store(regs, opcode, 0, 2, 0);\n\t\t\t\tbreak;\n\t\t\tcase 0xd: /* FSTX.P */\n\t\t\t\terror = misaligned_fpu_store(regs, opcode, 0, 3, 1);\n\t\t\t\tbreak;\n\t\t\tcase 0x9: /* FSTX.D */\n\t\t\t\terror = misaligned_fpu_store(regs, opcode, 0, 3, 0);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\terror = -1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\t/* Fault */\n\t\t\terror = -1;\n\t\t\tbreak;\n\t}\n\n\tif (error < 0) {\n\t\treturn error;\n\t} else {\n\t\tregs->pc += 4; /* Skip the instruction that's just been emulated */\n\t\treturn 0;\n\t}\n\n}\n\nstatic ctl_table unaligned_table[] = {\n\t{\n\t\t.procname\t= \"kernel_reports\",\n\t\t.data\t\t= &kernel_mode_unaligned_fixup_count,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec\n\t},\n\t{\n\t\t.procname\t= \"user_reports\",\n\t\t.data\t\t= &user_mode_unaligned_fixup_count,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec\n\t},\n\t{\n\t\t.procname\t= \"user_enable\",\n\t\t.data\t\t= &user_mode_unaligned_fixup_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec},\n\t{}\n};\n\nstatic ctl_table unaligned_root[] = {\n\t{\n\t\t.procname\t= \"unaligned_fixup\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= unaligned_table\n\t},\n\t{}\n};\n\nstatic ctl_table sh64_root[] = {\n\t{\n\t\t.procname\t= \"sh64\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= unaligned_root\n\t},\n\t{}\n};\nstatic struct ctl_table_header *sysctl_header;\nstatic int __init init_sysctl(void)\n{\n\tsysctl_header = register_sysctl_table(sh64_root);\n\treturn 0;\n}\n\n__initcall(init_sysctl);\n\n\nasmlinkage void do_debug_interrupt(unsigned long code, struct pt_regs *regs)\n{\n\tu64 peek_real_address_q(u64 addr);\n\tu64 poke_real_address_q(u64 addr, u64 val);\n\tunsigned long long DM_EXP_CAUSE_PHY = 0x0c100010;\n\tunsigned long long exp_cause;\n\t/* It's not worth ioremapping the debug module registers for the amount\n\t   of access we make to them - just go direct to their physical\n\t   addresses. */\n\texp_cause = peek_real_address_q(DM_EXP_CAUSE_PHY);\n\tif (exp_cause & ~4) {\n\t\tprintk(\"DM.EXP_CAUSE had unexpected bits set (=%08lx)\\n\",\n\t\t\t(unsigned long)(exp_cause & 0xffffffff));\n\t}\n\tshow_state();\n\t/* Clear all DEBUGINT causes */\n\tpoke_real_address_q(DM_EXP_CAUSE_PHY, 0x0);\n}\n\nvoid __cpuinit per_cpu_trap_init(void)\n{\n\t/* Nothing to do for now, VBR initialization later. */\n}\n", "/*\n * arch/sh/math-emu/math.c\n *\n * Copyright (C) 2006 Takashi YOSHII <takasi-y@ops.dti.ne.jp>\n *\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n */\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/sched.h>\n#include <linux/signal.h>\n#include <linux/perf_event.h>\n\n#include <asm/system.h>\n#include <asm/uaccess.h>\n#include <asm/processor.h>\n#include <asm/io.h>\n\n#include \"sfp-util.h\"\n#include <math-emu/soft-fp.h>\n#include <math-emu/single.h>\n#include <math-emu/double.h>\n\n#define\tFPUL\t\t(fregs->fpul)\n#define FPSCR\t\t(fregs->fpscr)\n#define FPSCR_RM\t(FPSCR&3)\n#define FPSCR_DN\t((FPSCR>>18)&1)\n#define FPSCR_PR\t((FPSCR>>19)&1)\n#define FPSCR_SZ\t((FPSCR>>20)&1)\n#define FPSCR_FR\t((FPSCR>>21)&1)\n#define FPSCR_MASK\t0x003fffffUL\n\n#define BANK(n)\t(n^(FPSCR_FR?16:0))\n#define FR\t((unsigned long*)(fregs->fp_regs))\n#define FR0\t(FR[BANK(0)])\n#define FRn\t(FR[BANK(n)])\n#define FRm\t(FR[BANK(m)])\n#define DR\t((unsigned long long*)(fregs->fp_regs))\n#define DRn\t(DR[BANK(n)/2])\n#define DRm\t(DR[BANK(m)/2])\n\n#define XREG(n)\t(n^16)\n#define XFn\t(FR[BANK(XREG(n))])\n#define XFm\t(FR[BANK(XREG(m))])\n#define XDn\t(DR[BANK(XREG(n))/2])\n#define XDm\t(DR[BANK(XREG(m))/2])\n\n#define R0\t(regs->regs[0])\n#define Rn\t(regs->regs[n])\n#define Rm\t(regs->regs[m])\n\n#define WRITE(d,a)\t({if(put_user(d, (typeof (d)*)a)) return -EFAULT;})\n#define READ(d,a)\t({if(get_user(d, (typeof (d)*)a)) return -EFAULT;})\n\n#define PACK_S(r,f)\tFP_PACK_SP(&r,f)\n#define UNPACK_S(f,r)\tFP_UNPACK_SP(f,&r)\n#define PACK_D(r,f) \\\n\t{u32 t[2]; FP_PACK_DP(t,f); ((u32*)&r)[0]=t[1]; ((u32*)&r)[1]=t[0];}\n#define UNPACK_D(f,r) \\\n\t{u32 t[2]; t[0]=((u32*)&r)[1]; t[1]=((u32*)&r)[0]; FP_UNPACK_DP(f,t);}\n\n// 2 args instructions.\n#define BOTH_PRmn(op,x) \\\n\tFP_DECL_EX; if(FPSCR_PR) op(D,x,DRm,DRn); else op(S,x,FRm,FRn);\n\n#define CMP_X(SZ,R,M,N) do{ \\\n\tFP_DECL_##SZ(Fm); FP_DECL_##SZ(Fn); \\\n\tUNPACK_##SZ(Fm, M); UNPACK_##SZ(Fn, N); \\\n\tFP_CMP_##SZ(R, Fn, Fm, 2); }while(0)\n#define EQ_X(SZ,R,M,N) do{ \\\n\tFP_DECL_##SZ(Fm); FP_DECL_##SZ(Fn); \\\n\tUNPACK_##SZ(Fm, M); UNPACK_##SZ(Fn, N); \\\n\tFP_CMP_EQ_##SZ(R, Fn, Fm); }while(0)\n#define CMP(OP) ({ int r; BOTH_PRmn(OP##_X,r); r; })\n\nstatic int\nfcmp_gt(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m, int n)\n{\n\tif (CMP(CMP) > 0)\n\t\tregs->sr |= 1;\n\telse\n\t\tregs->sr &= ~1;\n\n\treturn 0;\n}\n\nstatic int\nfcmp_eq(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m, int n)\n{\n\tif (CMP(CMP /*EQ*/) == 0)\n\t\tregs->sr |= 1;\n\telse\n\t\tregs->sr &= ~1;\n\treturn 0;\n}\n\n#define ARITH_X(SZ,OP,M,N) do{ \\\n\tFP_DECL_##SZ(Fm); FP_DECL_##SZ(Fn); FP_DECL_##SZ(Fr); \\\n\tUNPACK_##SZ(Fm, M); UNPACK_##SZ(Fn, N); \\\n\tFP_##OP##_##SZ(Fr, Fn, Fm); \\\n\tPACK_##SZ(N, Fr); }while(0)\n\nstatic int\nfadd(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m, int n)\n{\n\tBOTH_PRmn(ARITH_X, ADD);\n\treturn 0;\n}\n\nstatic int\nfsub(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m, int n)\n{\n\tBOTH_PRmn(ARITH_X, SUB);\n\treturn 0;\n}\n\nstatic int\nfmul(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m, int n)\n{\n\tBOTH_PRmn(ARITH_X, MUL);\n\treturn 0;\n}\n\nstatic int\nfdiv(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m, int n)\n{\n\tBOTH_PRmn(ARITH_X, DIV);\n\treturn 0;\n}\n\nstatic int\nfmac(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m, int n)\n{\n\tFP_DECL_EX;\n\tFP_DECL_S(Fr);\n\tFP_DECL_S(Ft);\n\tFP_DECL_S(F0);\n\tFP_DECL_S(Fm);\n\tFP_DECL_S(Fn);\n\tUNPACK_S(F0, FR0);\n\tUNPACK_S(Fm, FRm);\n\tUNPACK_S(Fn, FRn);\n\tFP_MUL_S(Ft, Fm, F0);\n\tFP_ADD_S(Fr, Fn, Ft);\n\tPACK_S(FRn, Fr);\n\treturn 0;\n}\n\n// to process fmov's extension (odd n for DR access XD).\n#define FMOV_EXT(x) if(x&1) x+=16-1\n\nstatic int\nfmov_idx_reg(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m,\n\t     int n)\n{\n\tif (FPSCR_SZ) {\n\t\tFMOV_EXT(n);\n\t\tREAD(FRn, Rm + R0 + 4);\n\t\tn++;\n\t\tREAD(FRn, Rm + R0);\n\t} else {\n\t\tREAD(FRn, Rm + R0);\n\t}\n\n\treturn 0;\n}\n\nstatic int\nfmov_mem_reg(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m,\n\t     int n)\n{\n\tif (FPSCR_SZ) {\n\t\tFMOV_EXT(n);\n\t\tREAD(FRn, Rm + 4);\n\t\tn++;\n\t\tREAD(FRn, Rm);\n\t} else {\n\t\tREAD(FRn, Rm);\n\t}\n\n\treturn 0;\n}\n\nstatic int\nfmov_inc_reg(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m,\n\t     int n)\n{\n\tif (FPSCR_SZ) {\n\t\tFMOV_EXT(n);\n\t\tREAD(FRn, Rm + 4);\n\t\tn++;\n\t\tREAD(FRn, Rm);\n\t\tRm += 8;\n\t} else {\n\t\tREAD(FRn, Rm);\n\t\tRm += 4;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nfmov_reg_idx(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m,\n\t     int n)\n{\n\tif (FPSCR_SZ) {\n\t\tFMOV_EXT(m);\n\t\tWRITE(FRm, Rn + R0 + 4);\n\t\tm++;\n\t\tWRITE(FRm, Rn + R0);\n\t} else {\n\t\tWRITE(FRm, Rn + R0);\n\t}\n\n\treturn 0;\n}\n\nstatic int\nfmov_reg_mem(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m,\n\t     int n)\n{\n\tif (FPSCR_SZ) {\n\t\tFMOV_EXT(m);\n\t\tWRITE(FRm, Rn + 4);\n\t\tm++;\n\t\tWRITE(FRm, Rn);\n\t} else {\n\t\tWRITE(FRm, Rn);\n\t}\n\n\treturn 0;\n}\n\nstatic int\nfmov_reg_dec(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m,\n\t     int n)\n{\n\tif (FPSCR_SZ) {\n\t\tFMOV_EXT(m);\n\t\tRn -= 8;\n\t\tWRITE(FRm, Rn + 4);\n\t\tm++;\n\t\tWRITE(FRm, Rn);\n\t} else {\n\t\tRn -= 4;\n\t\tWRITE(FRm, Rn);\n\t}\n\n\treturn 0;\n}\n\nstatic int\nfmov_reg_reg(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m,\n\t     int n)\n{\n\tif (FPSCR_SZ) {\n\t\tFMOV_EXT(m);\n\t\tFMOV_EXT(n);\n\t\tDRn = DRm;\n\t} else {\n\t\tFRn = FRm;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nfnop_mn(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int m, int n)\n{\n\treturn -EINVAL;\n}\n\n// 1 arg instructions.\n#define NOTYETn(i) static int i(struct sh_fpu_soft_struct *fregs, int n) \\\n\t{ printk( #i \" not yet done.\\n\"); return 0; }\n\nNOTYETn(ftrv)\nNOTYETn(fsqrt)\nNOTYETn(fipr)\nNOTYETn(fsca)\nNOTYETn(fsrra)\n\n#define EMU_FLOAT_X(SZ,N) do { \\\n\tFP_DECL_##SZ(Fn); \\\n\tFP_FROM_INT_##SZ(Fn, FPUL, 32, int); \\\n\tPACK_##SZ(N, Fn); }while(0)\nstatic int ffloat(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFP_DECL_EX;\n\n\tif (FPSCR_PR)\n\t\tEMU_FLOAT_X(D, DRn);\n\telse\n\t\tEMU_FLOAT_X(S, FRn);\n\n\treturn 0;\n}\n\n#define EMU_FTRC_X(SZ,N) do { \\\n\tFP_DECL_##SZ(Fn); \\\n\tUNPACK_##SZ(Fn, N); \\\n\tFP_TO_INT_##SZ(FPUL, Fn, 32, 1); }while(0)\nstatic int ftrc(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFP_DECL_EX;\n\n\tif (FPSCR_PR)\n\t\tEMU_FTRC_X(D, DRn);\n\telse\n\t\tEMU_FTRC_X(S, FRn);\n\n\treturn 0;\n}\n\nstatic int fcnvsd(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFP_DECL_EX;\n\tFP_DECL_S(Fn);\n\tFP_DECL_D(Fr);\n\tUNPACK_S(Fn, FPUL);\n\tFP_CONV(D, S, 2, 1, Fr, Fn);\n\tPACK_D(DRn, Fr);\n\treturn 0;\n}\n\nstatic int fcnvds(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFP_DECL_EX;\n\tFP_DECL_D(Fn);\n\tFP_DECL_S(Fr);\n\tUNPACK_D(Fn, DRn);\n\tFP_CONV(S, D, 1, 2, Fr, Fn);\n\tPACK_S(FPUL, Fr);\n\treturn 0;\n}\n\nstatic int fxchg(struct sh_fpu_soft_struct *fregs, int flag)\n{\n\tFPSCR ^= flag;\n\treturn 0;\n}\n\nstatic int fsts(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFRn = FPUL;\n\treturn 0;\n}\n\nstatic int flds(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFPUL = FRn;\n\treturn 0;\n}\n\nstatic int fneg(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFRn ^= (1 << (_FP_W_TYPE_SIZE - 1));\n\treturn 0;\n}\n\nstatic int fabs(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFRn &= ~(1 << (_FP_W_TYPE_SIZE - 1));\n\treturn 0;\n}\n\nstatic int fld0(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFRn = 0;\n\treturn 0;\n}\n\nstatic int fld1(struct sh_fpu_soft_struct *fregs, int n)\n{\n\tFRn = (_FP_EXPBIAS_S << (_FP_FRACBITS_S - 1));\n\treturn 0;\n}\n\nstatic int fnop_n(struct sh_fpu_soft_struct *fregs, int n)\n{\n\treturn -EINVAL;\n}\n\n/// Instruction decoders.\n\nstatic int id_fxfd(struct sh_fpu_soft_struct *, int);\nstatic int id_fnxd(struct sh_fpu_soft_struct *, struct pt_regs *, int, int);\n\nstatic int (*fnxd[])(struct sh_fpu_soft_struct *, int) = {\n\tfsts, flds, ffloat, ftrc, fneg, fabs, fsqrt, fsrra,\n\tfld0, fld1, fcnvsd, fcnvds, fnop_n, fnop_n, fipr, id_fxfd\n};\n\nstatic int (*fnmx[])(struct sh_fpu_soft_struct *, struct pt_regs *, int, int) = {\n\tfadd, fsub, fmul, fdiv, fcmp_eq, fcmp_gt, fmov_idx_reg, fmov_reg_idx,\n\tfmov_mem_reg, fmov_inc_reg, fmov_reg_mem, fmov_reg_dec,\n\tfmov_reg_reg, id_fnxd, fmac, fnop_mn};\n\nstatic int id_fxfd(struct sh_fpu_soft_struct *fregs, int x)\n{\n\tconst int flag[] = { FPSCR_SZ, FPSCR_PR, FPSCR_FR, 0 };\n\tswitch (x & 3) {\n\tcase 3:\n\t\tfxchg(fregs, flag[x >> 2]);\n\t\tbreak;\n\tcase 1:\n\t\tftrv(fregs, x - 1);\n\t\tbreak;\n\tdefault:\n\t\tfsca(fregs, x);\n\t}\n\treturn 0;\n}\n\nstatic int\nid_fnxd(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, int x, int n)\n{\n\treturn (fnxd[x])(fregs, n);\n}\n\nstatic int\nid_fnmx(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, u16 code)\n{\n\tint n = (code >> 8) & 0xf, m = (code >> 4) & 0xf, x = code & 0xf;\n\treturn (fnmx[x])(fregs, regs, m, n);\n}\n\nstatic int\nid_sys(struct sh_fpu_soft_struct *fregs, struct pt_regs *regs, u16 code)\n{\n\tint n = ((code >> 8) & 0xf);\n\tunsigned long *reg = (code & 0x0010) ? &FPUL : &FPSCR;\n\n\tswitch (code & 0xf0ff) {\n\tcase 0x005a:\n\tcase 0x006a:\n\t\tRn = *reg;\n\t\tbreak;\n\tcase 0x405a:\n\tcase 0x406a:\n\t\t*reg = Rn;\n\t\tbreak;\n\tcase 0x4052:\n\tcase 0x4062:\n\t\tRn -= 4;\n\t\tWRITE(*reg, Rn);\n\t\tbreak;\n\tcase 0x4056:\n\tcase 0x4066:\n\t\tREAD(*reg, Rn);\n\t\tRn += 4;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int fpu_emulate(u16 code, struct sh_fpu_soft_struct *fregs, struct pt_regs *regs)\n{\n\tif ((code & 0xf000) == 0xf000)\n\t\treturn id_fnmx(fregs, regs, code);\n\telse\n\t\treturn id_sys(fregs, regs, code);\n}\n\n/**\n *\tdenormal_to_double - Given denormalized float number,\n *\t                     store double float\n *\n *\t@fpu: Pointer to sh_fpu_soft structure\n *\t@n: Index to FP register\n */\nstatic void denormal_to_double(struct sh_fpu_soft_struct *fpu, int n)\n{\n\tunsigned long du, dl;\n\tunsigned long x = fpu->fpul;\n\tint exp = 1023 - 126;\n\n\tif (x != 0 && (x & 0x7f800000) == 0) {\n\t\tdu = (x & 0x80000000);\n\t\twhile ((x & 0x00800000) == 0) {\n\t\t\tx <<= 1;\n\t\t\texp--;\n\t\t}\n\t\tx &= 0x007fffff;\n\t\tdu |= (exp << 20) | (x >> 3);\n\t\tdl = x << 29;\n\n\t\tfpu->fp_regs[n] = du;\n\t\tfpu->fp_regs[n+1] = dl;\n\t}\n}\n\n/**\n *\tieee_fpe_handler - Handle denormalized number exception\n *\n *\t@regs: Pointer to register structure\n *\n *\tReturns 1 when it's handled (should not cause exception).\n */\nstatic int ieee_fpe_handler(struct pt_regs *regs)\n{\n\tunsigned short insn = *(unsigned short *)regs->pc;\n\tunsigned short finsn;\n\tunsigned long nextpc;\n\tsiginfo_t info;\n\tint nib[4] = {\n\t\t(insn >> 12) & 0xf,\n\t\t(insn >> 8) & 0xf,\n\t\t(insn >> 4) & 0xf,\n\t\tinsn & 0xf};\n\n\tif (nib[0] == 0xb ||\n\t    (nib[0] == 0x4 && nib[2] == 0x0 && nib[3] == 0xb)) /* bsr & jsr */\n\t\tregs->pr = regs->pc + 4;\n\n\tif (nib[0] == 0xa || nib[0] == 0xb) { /* bra & bsr */\n\t\tnextpc = regs->pc + 4 + ((short) ((insn & 0xfff) << 4) >> 3);\n\t\tfinsn = *(unsigned short *) (regs->pc + 2);\n\t} else if (nib[0] == 0x8 && nib[1] == 0xd) { /* bt/s */\n\t\tif (regs->sr & 1)\n\t\t\tnextpc = regs->pc + 4 + ((char) (insn & 0xff) << 1);\n\t\telse\n\t\t\tnextpc = regs->pc + 4;\n\t\tfinsn = *(unsigned short *) (regs->pc + 2);\n\t} else if (nib[0] == 0x8 && nib[1] == 0xf) { /* bf/s */\n\t\tif (regs->sr & 1)\n\t\t\tnextpc = regs->pc + 4;\n\t\telse\n\t\t\tnextpc = regs->pc + 4 + ((char) (insn & 0xff) << 1);\n\t\tfinsn = *(unsigned short *) (regs->pc + 2);\n\t} else if (nib[0] == 0x4 && nib[3] == 0xb &&\n\t\t (nib[2] == 0x0 || nib[2] == 0x2)) { /* jmp & jsr */\n\t\tnextpc = regs->regs[nib[1]];\n\t\tfinsn = *(unsigned short *) (regs->pc + 2);\n\t} else if (nib[0] == 0x0 && nib[3] == 0x3 &&\n\t\t (nib[2] == 0x0 || nib[2] == 0x2)) { /* braf & bsrf */\n\t\tnextpc = regs->pc + 4 + regs->regs[nib[1]];\n\t\tfinsn = *(unsigned short *) (regs->pc + 2);\n\t} else if (insn == 0x000b) { /* rts */\n\t\tnextpc = regs->pr;\n\t\tfinsn = *(unsigned short *) (regs->pc + 2);\n\t} else {\n\t\tnextpc = regs->pc + 2;\n\t\tfinsn = insn;\n\t}\n\n\tif ((finsn & 0xf1ff) == 0xf0ad) { /* fcnvsd */\n\t\tstruct task_struct *tsk = current;\n\n\t\tif ((tsk->thread.xstate->softfpu.fpscr & (1 << 17))) {\n\t\t\t/* FPU error */\n\t\t\tdenormal_to_double (&tsk->thread.xstate->softfpu,\n\t\t\t\t\t    (finsn >> 8) & 0xf);\n\t\t\ttsk->thread.xstate->softfpu.fpscr &=\n\t\t\t\t~(FPSCR_CAUSE_MASK | FPSCR_FLAG_MASK);\n\t\t\ttask_thread_info(tsk)->status |= TS_USEDFPU;\n\t\t} else {\n\t\t\tinfo.si_signo = SIGFPE;\n\t\t\tinfo.si_errno = 0;\n\t\t\tinfo.si_code = FPE_FLTINV;\n\t\t\tinfo.si_addr = (void __user *)regs->pc;\n\t\t\tforce_sig_info(SIGFPE, &info, tsk);\n\t\t}\n\n\t\tregs->pc = nextpc;\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nasmlinkage void do_fpu_error(unsigned long r4, unsigned long r5,\n\t\t\t     unsigned long r6, unsigned long r7,\n\t\t\t     struct pt_regs regs)\n{\n\tstruct task_struct *tsk = current;\n\tsiginfo_t info;\n\n\tif (ieee_fpe_handler (&regs))\n\t\treturn;\n\n\tregs.pc += 2;\n\tinfo.si_signo = SIGFPE;\n\tinfo.si_errno = 0;\n\tinfo.si_code = FPE_FLTINV;\n\tinfo.si_addr = (void __user *)regs.pc;\n\tforce_sig_info(SIGFPE, &info, tsk);\n}\n\n/**\n * fpu_init - Initialize FPU registers\n * @fpu: Pointer to software emulated FPU registers.\n */\nstatic void fpu_init(struct sh_fpu_soft_struct *fpu)\n{\n\tint i;\n\n\tfpu->fpscr = FPSCR_INIT;\n\tfpu->fpul = 0;\n\n\tfor (i = 0; i < 16; i++) {\n\t\tfpu->fp_regs[i] = 0;\n\t\tfpu->xfp_regs[i]= 0;\n\t}\n}\n\n/**\n * do_fpu_inst - Handle reserved instructions for FPU emulation\n * @inst: instruction code.\n * @regs: registers on stack.\n */\nint do_fpu_inst(unsigned short inst, struct pt_regs *regs)\n{\n\tstruct task_struct *tsk = current;\n\tstruct sh_fpu_soft_struct *fpu = &(tsk->thread.xstate->softfpu);\n\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);\n\n\tif (!(task_thread_info(tsk)->status & TS_USEDFPU)) {\n\t\t/* initialize once. */\n\t\tfpu_init(fpu);\n\t\ttask_thread_info(tsk)->status |= TS_USEDFPU;\n\t}\n\n\treturn fpu_emulate(inst, fpu, regs);\n}\n", "/*\n * Page fault handler for SH with an MMU.\n *\n *  Copyright (C) 1999  Niibe Yutaka\n *  Copyright (C) 2003 - 2009  Paul Mundt\n *\n *  Based on linux/arch/i386/mm/fault.c:\n *   Copyright (C) 1995  Linus Torvalds\n *\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n */\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/hardirq.h>\n#include <linux/kprobes.h>\n#include <linux/perf_event.h>\n#include <asm/io_trapped.h>\n#include <asm/system.h>\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n\nstatic inline int notify_page_fault(struct pt_regs *regs, int trap)\n{\n\tint ret = 0;\n\n\tif (kprobes_built_in() && !user_mode(regs)) {\n\t\tpreempt_disable();\n\t\tif (kprobe_running() && kprobe_fault_handler(regs, trap))\n\t\t\tret = 1;\n\t\tpreempt_enable();\n\t}\n\n\treturn ret;\n}\n\nstatic inline pmd_t *vmalloc_sync_one(pgd_t *pgd, unsigned long address)\n{\n\tunsigned index = pgd_index(address);\n\tpgd_t *pgd_k;\n\tpud_t *pud, *pud_k;\n\tpmd_t *pmd, *pmd_k;\n\n\tpgd += index;\n\tpgd_k = init_mm.pgd + index;\n\n\tif (!pgd_present(*pgd_k))\n\t\treturn NULL;\n\n\tpud = pud_offset(pgd, address);\n\tpud_k = pud_offset(pgd_k, address);\n\tif (!pud_present(*pud_k))\n\t\treturn NULL;\n\n\tif (!pud_present(*pud))\n\t    set_pud(pud, *pud_k);\n\n\tpmd = pmd_offset(pud, address);\n\tpmd_k = pmd_offset(pud_k, address);\n\tif (!pmd_present(*pmd_k))\n\t\treturn NULL;\n\n\tif (!pmd_present(*pmd))\n\t\tset_pmd(pmd, *pmd_k);\n\telse {\n\t\t/*\n\t\t * The page tables are fully synchronised so there must\n\t\t * be another reason for the fault. Return NULL here to\n\t\t * signal that we have not taken care of the fault.\n\t\t */\n\t\tBUG_ON(pmd_page(*pmd) != pmd_page(*pmd_k));\n\t\treturn NULL;\n\t}\n\n\treturn pmd_k;\n}\n\n/*\n * Handle a fault on the vmalloc or module mapping area\n */\nstatic noinline int vmalloc_fault(unsigned long address)\n{\n\tpgd_t *pgd_k;\n\tpmd_t *pmd_k;\n\tpte_t *pte_k;\n\n\t/* Make sure we are in vmalloc/module/P3 area: */\n\tif (!(address >= VMALLOC_START && address < P3_ADDR_MAX))\n\t\treturn -1;\n\n\t/*\n\t * Synchronize this task's top level page-table\n\t * with the 'reference' page table.\n\t *\n\t * Do _not_ use \"current\" here. We might be inside\n\t * an interrupt in the middle of a task switch..\n\t */\n\tpgd_k = get_TTB();\n\tpmd_k = vmalloc_sync_one(pgd_k, address);\n\tif (!pmd_k)\n\t\treturn -1;\n\n\tpte_k = pte_offset_kernel(pmd_k, address);\n\tif (!pte_present(*pte_k))\n\t\treturn -1;\n\n\treturn 0;\n}\n\nstatic int fault_in_kernel_space(unsigned long address)\n{\n\treturn address >= TASK_SIZE;\n}\n\n/*\n * This routine handles page faults.  It determines the address,\n * and the problem, and then passes it off to one of the appropriate\n * routines.\n */\nasmlinkage void __kprobes do_page_fault(struct pt_regs *regs,\n\t\t\t\t\tunsigned long writeaccess,\n\t\t\t\t\tunsigned long address)\n{\n\tunsigned long vec;\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct * vma;\n\tint si_code;\n\tint fault;\n\tsiginfo_t info;\n\n\ttsk = current;\n\tmm = tsk->mm;\n\tsi_code = SEGV_MAPERR;\n\tvec = lookup_exception_vector();\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t */\n\tif (unlikely(fault_in_kernel_space(address))) {\n\t\tif (vmalloc_fault(address) >= 0)\n\t\t\treturn;\n\t\tif (notify_page_fault(regs, vec))\n\t\t\treturn;\n\n\t\tgoto bad_area_nosemaphore;\n\t}\n\n\tif (unlikely(notify_page_fault(regs, vec)))\n\t\treturn;\n\n\t/* Only enable interrupts if they were on before the fault */\n\tif ((regs->sr & SR_IMASK) != SR_IMASK)\n\t\tlocal_irq_enable();\n\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);\n\n\t/*\n\t * If we're in an interrupt, have no user context or are running\n\t * in an atomic region then we must not take the fault:\n\t */\n\tif (in_atomic() || !mm)\n\t\tgoto no_context;\n\n\tdown_read(&mm->mmap_sem);\n\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n\n\t/*\n\t * Ok, we have a good vm_area for this memory access, so\n\t * we can handle it..\n\t */\ngood_area:\n\tsi_code = SEGV_ACCERR;\n\tif (writeaccess) {\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t} else {\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tgoto bad_area;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, writeaccess ? FAULT_FLAG_WRITE : 0);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR) {\n\t\ttsk->maj_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1,\n\t\t\t\t     regs, address);\n\t} else {\n\t\ttsk->min_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1,\n\t\t\t\t     regs, address);\n\t}\n\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n\t/*\n\t * Something tried to access memory that isn't in our memory map..\n\t * Fix it, but check if it's kernel or user first..\n\t */\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\tif (user_mode(regs)) {\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code = si_code;\n\t\tinfo.si_addr = (void *) address;\n\t\tforce_sig_info(SIGSEGV, &info, tsk);\n\t\treturn;\n\t}\n\nno_context:\n\t/* Are we prepared to handle this kernel fault?  */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\tif (handle_trapped_io(regs, address))\n\t\treturn;\n/*\n * Oops. The kernel tried to access some bad page. We'll have to\n * terminate things with extreme prejudice.\n *\n */\n\n\tbust_spinlocks(1);\n\n\tif (oops_may_print()) {\n\t\tunsigned long page;\n\n\t\tif (address < PAGE_SIZE)\n\t\t\tprintk(KERN_ALERT \"Unable to handle kernel NULL \"\n\t\t\t\t\t  \"pointer dereference\");\n\t\telse\n\t\t\tprintk(KERN_ALERT \"Unable to handle kernel paging \"\n\t\t\t\t\t  \"request\");\n\t\tprintk(\" at virtual address %08lx\\n\", address);\n\t\tprintk(KERN_ALERT \"pc = %08lx\\n\", regs->pc);\n\t\tpage = (unsigned long)get_TTB();\n\t\tif (page) {\n\t\t\tpage = ((__typeof__(page) *)page)[address >> PGDIR_SHIFT];\n\t\t\tprintk(KERN_ALERT \"*pde = %08lx\\n\", page);\n\t\t\tif (page & _PAGE_PRESENT) {\n\t\t\t\tpage &= PAGE_MASK;\n\t\t\t\taddress &= 0x003ff000;\n\t\t\t\tpage = ((__typeof__(page) *)\n\t\t\t\t\t\t__va(page))[address >>\n\t\t\t\t\t\t\t    PAGE_SHIFT];\n\t\t\t\tprintk(KERN_ALERT \"*pte = %08lx\\n\", page);\n\t\t\t}\n\t\t}\n\t}\n\n\tdie(\"Oops\", regs, writeaccess);\n\tbust_spinlocks(0);\n\tdo_exit(SIGKILL);\n\n/*\n * We ran out of memory, or some other thing happened to us that made\n * us unable to handle the page fault gracefully.\n */\nout_of_memory:\n\tup_read(&mm->mmap_sem);\n\tif (!user_mode(regs))\n\t\tgoto no_context;\n\tpagefault_out_of_memory();\n\treturn;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\n\t/*\n\t * Send a sigbus, regardless of whether we were in kernel\n\t * or user mode.\n\t */\n\tinfo.si_signo = SIGBUS;\n\tinfo.si_errno = 0;\n\tinfo.si_code = BUS_ADRERR;\n\tinfo.si_addr = (void *)address;\n\tforce_sig_info(SIGBUS, &info, tsk);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (!user_mode(regs))\n\t\tgoto no_context;\n}\n\n/*\n * Called with interrupts disabled.\n */\nasmlinkage int __kprobes\nhandle_tlbmiss(struct pt_regs *regs, unsigned long writeaccess,\n\t       unsigned long address)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tpte_t entry;\n\n\t/*\n\t * We don't take page faults for P1, P2, and parts of P4, these\n\t * are always mapped, whether it be due to legacy behaviour in\n\t * 29-bit mode, or due to PMB configuration in 32-bit mode.\n\t */\n\tif (address >= P3SEG && address < P3_ADDR_MAX) {\n\t\tpgd = pgd_offset_k(address);\n\t} else {\n\t\tif (unlikely(address >= TASK_SIZE || !current->mm))\n\t\t\treturn 1;\n\n\t\tpgd = pgd_offset(current->mm, address);\n\t}\n\n\tpud = pud_offset(pgd, address);\n\tif (pud_none_or_clear_bad(pud))\n\t\treturn 1;\n\tpmd = pmd_offset(pud, address);\n\tif (pmd_none_or_clear_bad(pmd))\n\t\treturn 1;\n\tpte = pte_offset_kernel(pmd, address);\n\tentry = *pte;\n\tif (unlikely(pte_none(entry) || pte_not_present(entry)))\n\t\treturn 1;\n\tif (unlikely(writeaccess && !pte_write(entry)))\n\t\treturn 1;\n\n\tif (writeaccess)\n\t\tentry = pte_mkdirty(entry);\n\tentry = pte_mkyoung(entry);\n\n\tset_pte(pte, entry);\n\n#if defined(CONFIG_CPU_SH4) && !defined(CONFIG_SMP)\n\t/*\n\t * SH-4 does not set MMUCR.RC to the corresponding TLB entry in\n\t * the case of an initial page write exception, so we need to\n\t * flush it in order to avoid potential TLB entry duplication.\n\t */\n\tif (writeaccess == 2)\n\t\tlocal_flush_tlb_one(get_asid(), address & PAGE_MASK);\n#endif\n\n\tupdate_mmu_cache(NULL, address, pte);\n\n\treturn 0;\n}\n", "/*\n * arch/sh/mm/tlb-flush_64.c\n *\n * Copyright (C) 2000, 2001  Paolo Alberelli\n * Copyright (C) 2003  Richard Curnow (/proc/tlb, bug fixes)\n * Copyright (C) 2003 - 2009 Paul Mundt\n *\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n */\n#include <linux/signal.h>\n#include <linux/rwsem.h>\n#include <linux/sched.h>\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/ptrace.h>\n#include <linux/mman.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/perf_event.h>\n#include <linux/interrupt.h>\n#include <asm/system.h>\n#include <asm/io.h>\n#include <asm/tlb.h>\n#include <asm/uaccess.h>\n#include <asm/pgalloc.h>\n#include <asm/mmu_context.h>\n\nextern void die(const char *,struct pt_regs *,long);\n\n#define PFLAG(val,flag)   (( (val) & (flag) ) ? #flag : \"\" )\n#define PPROT(flag) PFLAG(pgprot_val(prot),flag)\n\nstatic inline void print_prots(pgprot_t prot)\n{\n\tprintk(\"prot is 0x%016llx\\n\",pgprot_val(prot));\n\n\tprintk(\"%s %s %s %s %s\\n\",PPROT(_PAGE_SHARED),PPROT(_PAGE_READ),\n\t       PPROT(_PAGE_EXECUTE),PPROT(_PAGE_WRITE),PPROT(_PAGE_USER));\n}\n\nstatic inline void print_vma(struct vm_area_struct *vma)\n{\n\tprintk(\"vma start 0x%08lx\\n\", vma->vm_start);\n\tprintk(\"vma end   0x%08lx\\n\", vma->vm_end);\n\n\tprint_prots(vma->vm_page_prot);\n\tprintk(\"vm_flags 0x%08lx\\n\", vma->vm_flags);\n}\n\nstatic inline void print_task(struct task_struct *tsk)\n{\n\tprintk(\"Task pid %d\\n\", task_pid_nr(tsk));\n}\n\nstatic pte_t *lookup_pte(struct mm_struct *mm, unsigned long address)\n{\n\tpgd_t *dir;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tpte_t entry;\n\n\tdir = pgd_offset(mm, address);\n\tif (pgd_none(*dir))\n\t\treturn NULL;\n\n\tpud = pud_offset(dir, address);\n\tif (pud_none(*pud))\n\t\treturn NULL;\n\n\tpmd = pmd_offset(pud, address);\n\tif (pmd_none(*pmd))\n\t\treturn NULL;\n\n\tpte = pte_offset_kernel(pmd, address);\n\tentry = *pte;\n\tif (pte_none(entry) || !pte_present(entry))\n\t\treturn NULL;\n\n\treturn pte;\n}\n\n/*\n * This routine handles page faults.  It determines the address,\n * and the problem, and then passes it off to one of the appropriate\n * routines.\n */\nasmlinkage void do_page_fault(struct pt_regs *regs, unsigned long writeaccess,\n\t\t\t      unsigned long textaccess, unsigned long address)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct * vma;\n\tconst struct exception_table_entry *fixup;\n\tpte_t *pte;\n\tint fault;\n\n\t/* SIM\n\t * Note this is now called with interrupts still disabled\n\t * This is to cope with being called for a missing IO port\n\t * address with interrupts disabled. This should be fixed as\n\t * soon as we have a better 'fast path' miss handler.\n\t *\n\t * Plus take care how you try and debug this stuff.\n\t * For example, writing debug data to a port which you\n\t * have just faulted on is not going to work.\n\t */\n\n\ttsk = current;\n\tmm = tsk->mm;\n\n\t/* Not an IO address, so reenable interrupts */\n\tlocal_irq_enable();\n\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);\n\n\t/*\n\t * If we're in an interrupt or have no user\n\t * context, we must not take the fault..\n\t */\n\tif (in_atomic() || !mm)\n\t\tgoto no_context;\n\n\t/* TLB misses upon some cache flushes get done under cli() */\n\tdown_read(&mm->mmap_sem);\n\n\tvma = find_vma(mm, address);\n\n\tif (!vma) {\n#ifdef DEBUG_FAULT\n\t\tprint_task(tsk);\n\t\tprintk(\"%s:%d fault, address is 0x%08x PC %016Lx textaccess %d writeaccess %d\\n\",\n\t\t       __func__, __LINE__,\n\t\t       address,regs->pc,textaccess,writeaccess);\n\t\tshow_regs(regs);\n#endif\n\t\tgoto bad_area;\n\t}\n\tif (vma->vm_start <= address) {\n\t\tgoto good_area;\n\t}\n\n\tif (!(vma->vm_flags & VM_GROWSDOWN)) {\n#ifdef DEBUG_FAULT\n\t\tprint_task(tsk);\n\t\tprintk(\"%s:%d fault, address is 0x%08x PC %016Lx textaccess %d writeaccess %d\\n\",\n\t\t       __func__, __LINE__,\n\t\t       address,regs->pc,textaccess,writeaccess);\n\t\tshow_regs(regs);\n\n\t\tprint_vma(vma);\n#endif\n\t\tgoto bad_area;\n\t}\n\tif (expand_stack(vma, address)) {\n#ifdef DEBUG_FAULT\n\t\tprint_task(tsk);\n\t\tprintk(\"%s:%d fault, address is 0x%08x PC %016Lx textaccess %d writeaccess %d\\n\",\n\t\t       __func__, __LINE__,\n\t\t       address,regs->pc,textaccess,writeaccess);\n\t\tshow_regs(regs);\n#endif\n\t\tgoto bad_area;\n\t}\n/*\n * Ok, we have a good vm_area for this memory access, so\n * we can handle it..\n */\ngood_area:\n\tif (textaccess) {\n\t\tif (!(vma->vm_flags & VM_EXEC))\n\t\t\tgoto bad_area;\n\t} else {\n\t\tif (writeaccess) {\n\t\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\t\tgoto bad_area;\n\t\t} else {\n\t\t\tif (!(vma->vm_flags & VM_READ))\n\t\t\t\tgoto bad_area;\n\t\t}\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, writeaccess ? FAULT_FLAG_WRITE : 0);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\n\tif (fault & VM_FAULT_MAJOR) {\n\t\ttsk->maj_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1,\n\t\t\t\t     regs, address);\n\t} else {\n\t\ttsk->min_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1,\n\t\t\t\t     regs, address);\n\t}\n\n\t/* If we get here, the page fault has been handled.  Do the TLB refill\n\t   now from the newly-setup PTE, to avoid having to fault again right\n\t   away on the same instruction. */\n\tpte = lookup_pte (mm, address);\n\tif (!pte) {\n\t\t/* From empirical evidence, we can get here, due to\n\t\t   !pte_present(pte).  (e.g. if a swap-in occurs, and the page\n\t\t   is swapped back out again before the process that wanted it\n\t\t   gets rescheduled?) */\n\t\tgoto no_pte;\n\t}\n\n\t__do_tlb_refill(address, textaccess, pte);\n\nno_pte:\n\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n/*\n * Something tried to access memory that isn't in our memory map..\n * Fix it, but check if it's kernel or user first..\n */\nbad_area:\n#ifdef DEBUG_FAULT\n\tprintk(\"fault:bad area\\n\");\n#endif\n\tup_read(&mm->mmap_sem);\n\n\tif (user_mode(regs)) {\n\t\tstatic int count=0;\n\t\tsiginfo_t info;\n\t\tif (count < 4) {\n\t\t\t/* This is really to help debug faults when starting\n\t\t\t * usermode, so only need a few */\n\t\t\tcount++;\n\t\t\tprintk(\"user mode bad_area address=%08lx pid=%d (%s) pc=%08lx\\n\",\n\t\t\t\taddress, task_pid_nr(current), current->comm,\n\t\t\t\t(unsigned long) regs->pc);\n#if 0\n\t\t\tshow_regs(regs);\n#endif\n\t\t}\n\t\tif (is_global_init(tsk)) {\n\t\t\tpanic(\"INIT had user mode bad_area\\n\");\n\t\t}\n\t\ttsk->thread.address = address;\n\t\ttsk->thread.error_code = writeaccess;\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_addr = (void *) address;\n\t\tforce_sig_info(SIGSEGV, &info, tsk);\n\t\treturn;\n\t}\n\nno_context:\n#ifdef DEBUG_FAULT\n\tprintk(\"fault:No context\\n\");\n#endif\n\t/* Are we prepared to handle this kernel fault?  */\n\tfixup = search_exception_tables(regs->pc);\n\tif (fixup) {\n\t\tregs->pc = fixup->fixup;\n\t\treturn;\n\t}\n\n/*\n * Oops. The kernel tried to access some bad page. We'll have to\n * terminate things with extreme prejudice.\n *\n */\n\tif (address < PAGE_SIZE)\n\t\tprintk(KERN_ALERT \"Unable to handle kernel NULL pointer dereference\");\n\telse\n\t\tprintk(KERN_ALERT \"Unable to handle kernel paging request\");\n\tprintk(\" at virtual address %08lx\\n\", address);\n\tprintk(KERN_ALERT \"pc = %08Lx%08Lx\\n\", regs->pc >> 32, regs->pc & 0xffffffff);\n\tdie(\"Oops\", regs, writeaccess);\n\tdo_exit(SIGKILL);\n\n/*\n * We ran out of memory, or some other thing happened to us that made\n * us unable to handle the page fault gracefully.\n */\nout_of_memory:\n\tup_read(&mm->mmap_sem);\n\tif (!user_mode(regs))\n\t\tgoto no_context;\n\tpagefault_out_of_memory();\n\treturn;\n\ndo_sigbus:\n\tprintk(\"fault:Do sigbus\\n\");\n\tup_read(&mm->mmap_sem);\n\n\t/*\n\t * Send a sigbus, regardless of whether we were in kernel\n\t * or user mode.\n\t */\n\ttsk->thread.address = address;\n\ttsk->thread.error_code = writeaccess;\n\ttsk->thread.trap_no = 14;\n\tforce_sig(SIGBUS, tsk);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (!user_mode(regs))\n\t\tgoto no_context;\n}\n\nvoid local_flush_tlb_one(unsigned long asid, unsigned long page)\n{\n\tunsigned long long match, pteh=0, lpage;\n\tunsigned long tlb;\n\n\t/*\n\t * Sign-extend based on neff.\n\t */\n\tlpage = neff_sign_extend(page);\n\tmatch = (asid << PTEH_ASID_SHIFT) | PTEH_VALID;\n\tmatch |= lpage;\n\n\tfor_each_itlb_entry(tlb) {\n\t\tasm volatile (\"getcfg\t%1, 0, %0\"\n\t\t\t      : \"=r\" (pteh)\n\t\t\t      : \"r\" (tlb) );\n\n\t\tif (pteh == match) {\n\t\t\t__flush_tlb_slot(tlb);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfor_each_dtlb_entry(tlb) {\n\t\tasm volatile (\"getcfg\t%1, 0, %0\"\n\t\t\t      : \"=r\" (pteh)\n\t\t\t      : \"r\" (tlb) );\n\n\t\tif (pteh == match) {\n\t\t\t__flush_tlb_slot(tlb);\n\t\t\tbreak;\n\t\t}\n\n\t}\n}\n\nvoid local_flush_tlb_page(struct vm_area_struct *vma, unsigned long page)\n{\n\tunsigned long flags;\n\n\tif (vma->vm_mm) {\n\t\tpage &= PAGE_MASK;\n\t\tlocal_irq_save(flags);\n\t\tlocal_flush_tlb_one(get_asid(), page);\n\t\tlocal_irq_restore(flags);\n\t}\n}\n\nvoid local_flush_tlb_range(struct vm_area_struct *vma, unsigned long start,\n\t\t\t   unsigned long end)\n{\n\tunsigned long flags;\n\tunsigned long long match, pteh=0, pteh_epn, pteh_low;\n\tunsigned long tlb;\n\tunsigned int cpu = smp_processor_id();\n\tstruct mm_struct *mm;\n\n\tmm = vma->vm_mm;\n\tif (cpu_context(cpu, mm) == NO_CONTEXT)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tstart &= PAGE_MASK;\n\tend &= PAGE_MASK;\n\n\tmatch = (cpu_asid(cpu, mm) << PTEH_ASID_SHIFT) | PTEH_VALID;\n\n\t/* Flush ITLB */\n\tfor_each_itlb_entry(tlb) {\n\t\tasm volatile (\"getcfg\t%1, 0, %0\"\n\t\t\t      : \"=r\" (pteh)\n\t\t\t      : \"r\" (tlb) );\n\n\t\tpteh_epn = pteh & PAGE_MASK;\n\t\tpteh_low = pteh & ~PAGE_MASK;\n\n\t\tif (pteh_low == match && pteh_epn >= start && pteh_epn <= end)\n\t\t\t__flush_tlb_slot(tlb);\n\t}\n\n\t/* Flush DTLB */\n\tfor_each_dtlb_entry(tlb) {\n\t\tasm volatile (\"getcfg\t%1, 0, %0\"\n\t\t\t      : \"=r\" (pteh)\n\t\t\t      : \"r\" (tlb) );\n\n\t\tpteh_epn = pteh & PAGE_MASK;\n\t\tpteh_low = pteh & ~PAGE_MASK;\n\n\t\tif (pteh_low == match && pteh_epn >= start && pteh_epn <= end)\n\t\t\t__flush_tlb_slot(tlb);\n\t}\n\n\tlocal_irq_restore(flags);\n}\n\nvoid local_flush_tlb_mm(struct mm_struct *mm)\n{\n\tunsigned long flags;\n\tunsigned int cpu = smp_processor_id();\n\n\tif (cpu_context(cpu, mm) == NO_CONTEXT)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tcpu_context(cpu, mm) = NO_CONTEXT;\n\tif (mm == current->mm)\n\t\tactivate_context(mm, cpu);\n\n\tlocal_irq_restore(flags);\n}\n\nvoid local_flush_tlb_all(void)\n{\n\t/* Invalidate all, including shared pages, excluding fixed TLBs */\n\tunsigned long flags, tlb;\n\n\tlocal_irq_save(flags);\n\n\t/* Flush each ITLB entry */\n\tfor_each_itlb_entry(tlb)\n\t\t__flush_tlb_slot(tlb);\n\n\t/* Flush each DTLB entry */\n\tfor_each_dtlb_entry(tlb)\n\t\t__flush_tlb_slot(tlb);\n\n\tlocal_irq_restore(flags);\n}\n\nvoid local_flush_tlb_kernel_range(unsigned long start, unsigned long end)\n{\n        /* FIXME: Optimize this later.. */\n        flush_tlb_all();\n}\n\nvoid __flush_tlb_global(void)\n{\n\tflush_tlb_all();\n}\n\nvoid __update_tlb(struct vm_area_struct *vma, unsigned long address, pte_t pte)\n{\n}\n", "/* Performance event support for sparc64.\n *\n * Copyright (C) 2009, 2010 David S. Miller <davem@davemloft.net>\n *\n * This code is based almost entirely upon the x86 perf event\n * code, which is:\n *\n *  Copyright (C) 2008 Thomas Gleixner <tglx@linutronix.de>\n *  Copyright (C) 2008-2009 Red Hat, Inc., Ingo Molnar\n *  Copyright (C) 2009 Jaswinder Singh Rajput\n *  Copyright (C) 2009 Advanced Micro Devices, Inc., Robert Richter\n *  Copyright (C) 2008-2009 Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>\n */\n\n#include <linux/perf_event.h>\n#include <linux/kprobes.h>\n#include <linux/ftrace.h>\n#include <linux/kernel.h>\n#include <linux/kdebug.h>\n#include <linux/mutex.h>\n\n#include <asm/stacktrace.h>\n#include <asm/cpudata.h>\n#include <asm/uaccess.h>\n#include <asm/atomic.h>\n#include <asm/nmi.h>\n#include <asm/pcr.h>\n\n#include \"kernel.h\"\n#include \"kstack.h\"\n\n/* Sparc64 chips have two performance counters, 32-bits each, with\n * overflow interrupts generated on transition from 0xffffffff to 0.\n * The counters are accessed in one go using a 64-bit register.\n *\n * Both counters are controlled using a single control register.  The\n * only way to stop all sampling is to clear all of the context (user,\n * supervisor, hypervisor) sampling enable bits.  But these bits apply\n * to both counters, thus the two counters can't be enabled/disabled\n * individually.\n *\n * The control register has two event fields, one for each of the two\n * counters.  It's thus nearly impossible to have one counter going\n * while keeping the other one stopped.  Therefore it is possible to\n * get overflow interrupts for counters not currently \"in use\" and\n * that condition must be checked in the overflow interrupt handler.\n *\n * So we use a hack, in that we program inactive counters with the\n * \"sw_count0\" and \"sw_count1\" events.  These count how many times\n * the instruction \"sethi %hi(0xfc000), %g0\" is executed.  It's an\n * unusual way to encode a NOP and therefore will not trigger in\n * normal code.\n */\n\n#define MAX_HWEVENTS\t\t\t2\n#define MAX_PERIOD\t\t\t((1UL << 32) - 1)\n\n#define PIC_UPPER_INDEX\t\t\t0\n#define PIC_LOWER_INDEX\t\t\t1\n#define PIC_NO_INDEX\t\t\t-1\n\nstruct cpu_hw_events {\n\t/* Number of events currently scheduled onto this cpu.\n\t * This tells how many entries in the arrays below\n\t * are valid.\n\t */\n\tint\t\t\tn_events;\n\n\t/* Number of new events added since the last hw_perf_disable().\n\t * This works because the perf event layer always adds new\n\t * events inside of a perf_{disable,enable}() sequence.\n\t */\n\tint\t\t\tn_added;\n\n\t/* Array of events current scheduled on this cpu.  */\n\tstruct perf_event\t*event[MAX_HWEVENTS];\n\n\t/* Array of encoded longs, specifying the %pcr register\n\t * encoding and the mask of PIC counters this even can\n\t * be scheduled on.  See perf_event_encode() et al.\n\t */\n\tunsigned long\t\tevents[MAX_HWEVENTS];\n\n\t/* The current counter index assigned to an event.  When the\n\t * event hasn't been programmed into the cpu yet, this will\n\t * hold PIC_NO_INDEX.  The event->hw.idx value tells us where\n\t * we ought to schedule the event.\n\t */\n\tint\t\t\tcurrent_idx[MAX_HWEVENTS];\n\n\t/* Software copy of %pcr register on this cpu.  */\n\tu64\t\t\tpcr;\n\n\t/* Enabled/disable state.  */\n\tint\t\t\tenabled;\n\n\tunsigned int\t\tgroup_flag;\n};\nDEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events) = { .enabled = 1, };\n\n/* An event map describes the characteristics of a performance\n * counter event.  In particular it gives the encoding as well as\n * a mask telling which counters the event can be measured on.\n */\nstruct perf_event_map {\n\tu16\tencoding;\n\tu8\tpic_mask;\n#define PIC_NONE\t0x00\n#define PIC_UPPER\t0x01\n#define PIC_LOWER\t0x02\n};\n\n/* Encode a perf_event_map entry into a long.  */\nstatic unsigned long perf_event_encode(const struct perf_event_map *pmap)\n{\n\treturn ((unsigned long) pmap->encoding << 16) | pmap->pic_mask;\n}\n\nstatic u8 perf_event_get_msk(unsigned long val)\n{\n\treturn val & 0xff;\n}\n\nstatic u64 perf_event_get_enc(unsigned long val)\n{\n\treturn val >> 16;\n}\n\n#define C(x) PERF_COUNT_HW_CACHE_##x\n\n#define CACHE_OP_UNSUPPORTED\t0xfffe\n#define CACHE_OP_NONSENSE\t0xffff\n\ntypedef struct perf_event_map cache_map_t\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX];\n\nstruct sparc_pmu {\n\tconst struct perf_event_map\t*(*event_map)(int);\n\tconst cache_map_t\t\t*cache_map;\n\tint\t\t\t\tmax_events;\n\tint\t\t\t\tupper_shift;\n\tint\t\t\t\tlower_shift;\n\tint\t\t\t\tevent_mask;\n\tint\t\t\t\thv_bit;\n\tint\t\t\t\tirq_bit;\n\tint\t\t\t\tupper_nop;\n\tint\t\t\t\tlower_nop;\n};\n\nstatic const struct perf_event_map ultra3_perfmon_event_map[] = {\n\t[PERF_COUNT_HW_CPU_CYCLES] = { 0x0000, PIC_UPPER | PIC_LOWER },\n\t[PERF_COUNT_HW_INSTRUCTIONS] = { 0x0001, PIC_UPPER | PIC_LOWER },\n\t[PERF_COUNT_HW_CACHE_REFERENCES] = { 0x0009, PIC_LOWER },\n\t[PERF_COUNT_HW_CACHE_MISSES] = { 0x0009, PIC_UPPER },\n};\n\nstatic const struct perf_event_map *ultra3_event_map(int event_id)\n{\n\treturn &ultra3_perfmon_event_map[event_id];\n}\n\nstatic const cache_map_t ultra3_cache_map = {\n[C(L1D)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x09, PIC_LOWER, },\n\t\t[C(RESULT_MISS)] = { 0x09, PIC_UPPER, },\n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x0a, PIC_LOWER },\n\t\t[C(RESULT_MISS)] = { 0x0a, PIC_UPPER },\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(L1I)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x09, PIC_LOWER, },\n\t\t[C(RESULT_MISS)] = { 0x09, PIC_UPPER, },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_NONSENSE },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_NONSENSE },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(LL)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x0c, PIC_LOWER, },\n\t\t[C(RESULT_MISS)] = { 0x0c, PIC_UPPER, },\n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x0c, PIC_LOWER },\n\t\t[C(RESULT_MISS)] = { 0x0c, PIC_UPPER },\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(DTLB)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0x12, PIC_UPPER, },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(ITLB)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0x11, PIC_UPPER, },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(BPU)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n};\n\nstatic const struct sparc_pmu ultra3_pmu = {\n\t.event_map\t= ultra3_event_map,\n\t.cache_map\t= &ultra3_cache_map,\n\t.max_events\t= ARRAY_SIZE(ultra3_perfmon_event_map),\n\t.upper_shift\t= 11,\n\t.lower_shift\t= 4,\n\t.event_mask\t= 0x3f,\n\t.upper_nop\t= 0x1c,\n\t.lower_nop\t= 0x14,\n};\n\n/* Niagara1 is very limited.  The upper PIC is hard-locked to count\n * only instructions, so it is free running which creates all kinds of\n * problems.  Some hardware designs make one wonder if the creator\n * even looked at how this stuff gets used by software.\n */\nstatic const struct perf_event_map niagara1_perfmon_event_map[] = {\n\t[PERF_COUNT_HW_CPU_CYCLES] = { 0x00, PIC_UPPER },\n\t[PERF_COUNT_HW_INSTRUCTIONS] = { 0x00, PIC_UPPER },\n\t[PERF_COUNT_HW_CACHE_REFERENCES] = { 0, PIC_NONE },\n\t[PERF_COUNT_HW_CACHE_MISSES] = { 0x03, PIC_LOWER },\n};\n\nstatic const struct perf_event_map *niagara1_event_map(int event_id)\n{\n\treturn &niagara1_perfmon_event_map[event_id];\n}\n\nstatic const cache_map_t niagara1_cache_map = {\n[C(L1D)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0x03, PIC_LOWER, },\n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0x03, PIC_LOWER, },\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(L1I)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x00, PIC_UPPER },\n\t\t[C(RESULT_MISS)] = { 0x02, PIC_LOWER, },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_NONSENSE },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_NONSENSE },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(LL)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0x07, PIC_LOWER, },\n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0x07, PIC_LOWER, },\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(DTLB)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0x05, PIC_LOWER, },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(ITLB)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0x04, PIC_LOWER, },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(BPU)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n};\n\nstatic const struct sparc_pmu niagara1_pmu = {\n\t.event_map\t= niagara1_event_map,\n\t.cache_map\t= &niagara1_cache_map,\n\t.max_events\t= ARRAY_SIZE(niagara1_perfmon_event_map),\n\t.upper_shift\t= 0,\n\t.lower_shift\t= 4,\n\t.event_mask\t= 0x7,\n\t.upper_nop\t= 0x0,\n\t.lower_nop\t= 0x0,\n};\n\nstatic const struct perf_event_map niagara2_perfmon_event_map[] = {\n\t[PERF_COUNT_HW_CPU_CYCLES] = { 0x02ff, PIC_UPPER | PIC_LOWER },\n\t[PERF_COUNT_HW_INSTRUCTIONS] = { 0x02ff, PIC_UPPER | PIC_LOWER },\n\t[PERF_COUNT_HW_CACHE_REFERENCES] = { 0x0208, PIC_UPPER | PIC_LOWER },\n\t[PERF_COUNT_HW_CACHE_MISSES] = { 0x0302, PIC_UPPER | PIC_LOWER },\n\t[PERF_COUNT_HW_BRANCH_INSTRUCTIONS] = { 0x0201, PIC_UPPER | PIC_LOWER },\n\t[PERF_COUNT_HW_BRANCH_MISSES] = { 0x0202, PIC_UPPER | PIC_LOWER },\n};\n\nstatic const struct perf_event_map *niagara2_event_map(int event_id)\n{\n\treturn &niagara2_perfmon_event_map[event_id];\n}\n\nstatic const cache_map_t niagara2_cache_map = {\n[C(L1D)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x0208, PIC_UPPER | PIC_LOWER, },\n\t\t[C(RESULT_MISS)] = { 0x0302, PIC_UPPER | PIC_LOWER, },\n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x0210, PIC_UPPER | PIC_LOWER, },\n\t\t[C(RESULT_MISS)] = { 0x0302, PIC_UPPER | PIC_LOWER, },\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(L1I)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x02ff, PIC_UPPER | PIC_LOWER, },\n\t\t[C(RESULT_MISS)] = { 0x0301, PIC_UPPER | PIC_LOWER, },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_NONSENSE },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_NONSENSE },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(LL)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x0208, PIC_UPPER | PIC_LOWER, },\n\t\t[C(RESULT_MISS)] = { 0x0330, PIC_UPPER | PIC_LOWER, },\n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = { 0x0210, PIC_UPPER | PIC_LOWER, },\n\t\t[C(RESULT_MISS)] = { 0x0320, PIC_UPPER | PIC_LOWER, },\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(DTLB)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0x0b08, PIC_UPPER | PIC_LOWER, },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(ITLB)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { 0xb04, PIC_UPPER | PIC_LOWER, },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n[C(BPU)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = { CACHE_OP_UNSUPPORTED },\n\t\t[C(RESULT_MISS)] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = { CACHE_OP_UNSUPPORTED },\n\t\t[ C(RESULT_MISS)   ] = { CACHE_OP_UNSUPPORTED },\n\t},\n},\n};\n\nstatic const struct sparc_pmu niagara2_pmu = {\n\t.event_map\t= niagara2_event_map,\n\t.cache_map\t= &niagara2_cache_map,\n\t.max_events\t= ARRAY_SIZE(niagara2_perfmon_event_map),\n\t.upper_shift\t= 19,\n\t.lower_shift\t= 6,\n\t.event_mask\t= 0xfff,\n\t.hv_bit\t\t= 0x8,\n\t.irq_bit\t= 0x30,\n\t.upper_nop\t= 0x220,\n\t.lower_nop\t= 0x220,\n};\n\nstatic const struct sparc_pmu *sparc_pmu __read_mostly;\n\nstatic u64 event_encoding(u64 event_id, int idx)\n{\n\tif (idx == PIC_UPPER_INDEX)\n\t\tevent_id <<= sparc_pmu->upper_shift;\n\telse\n\t\tevent_id <<= sparc_pmu->lower_shift;\n\treturn event_id;\n}\n\nstatic u64 mask_for_index(int idx)\n{\n\treturn event_encoding(sparc_pmu->event_mask, idx);\n}\n\nstatic u64 nop_for_index(int idx)\n{\n\treturn event_encoding(idx == PIC_UPPER_INDEX ?\n\t\t\t      sparc_pmu->upper_nop :\n\t\t\t      sparc_pmu->lower_nop, idx);\n}\n\nstatic inline void sparc_pmu_enable_event(struct cpu_hw_events *cpuc, struct hw_perf_event *hwc, int idx)\n{\n\tu64 val, mask = mask_for_index(idx);\n\n\tval = cpuc->pcr;\n\tval &= ~mask;\n\tval |= hwc->config;\n\tcpuc->pcr = val;\n\n\tpcr_ops->write(cpuc->pcr);\n}\n\nstatic inline void sparc_pmu_disable_event(struct cpu_hw_events *cpuc, struct hw_perf_event *hwc, int idx)\n{\n\tu64 mask = mask_for_index(idx);\n\tu64 nop = nop_for_index(idx);\n\tu64 val;\n\n\tval = cpuc->pcr;\n\tval &= ~mask;\n\tval |= nop;\n\tcpuc->pcr = val;\n\n\tpcr_ops->write(cpuc->pcr);\n}\n\nstatic u32 read_pmc(int idx)\n{\n\tu64 val;\n\n\tread_pic(val);\n\tif (idx == PIC_UPPER_INDEX)\n\t\tval >>= 32;\n\n\treturn val & 0xffffffff;\n}\n\nstatic void write_pmc(int idx, u64 val)\n{\n\tu64 shift, mask, pic;\n\n\tshift = 0;\n\tif (idx == PIC_UPPER_INDEX)\n\t\tshift = 32;\n\n\tmask = ((u64) 0xffffffff) << shift;\n\tval <<= shift;\n\n\tread_pic(pic);\n\tpic &= ~mask;\n\tpic |= val;\n\twrite_pic(pic);\n}\n\nstatic u64 sparc_perf_event_update(struct perf_event *event,\n\t\t\t\t   struct hw_perf_event *hwc, int idx)\n{\n\tint shift = 64 - 32;\n\tu64 prev_raw_count, new_raw_count;\n\ts64 delta;\n\nagain:\n\tprev_raw_count = local64_read(&hwc->prev_count);\n\tnew_raw_count = read_pmc(idx);\n\n\tif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\n\t\t\t     new_raw_count) != prev_raw_count)\n\t\tgoto again;\n\n\tdelta = (new_raw_count << shift) - (prev_raw_count << shift);\n\tdelta >>= shift;\n\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &hwc->period_left);\n\n\treturn new_raw_count;\n}\n\nstatic int sparc_perf_event_set_period(struct perf_event *event,\n\t\t\t\t       struct hw_perf_event *hwc, int idx)\n{\n\ts64 left = local64_read(&hwc->period_left);\n\ts64 period = hwc->sample_period;\n\tint ret = 0;\n\n\tif (unlikely(left <= -period)) {\n\t\tleft = period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\tif (unlikely(left <= 0)) {\n\t\tleft += period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\tif (left > MAX_PERIOD)\n\t\tleft = MAX_PERIOD;\n\n\tlocal64_set(&hwc->prev_count, (u64)-left);\n\n\twrite_pmc(idx, (u64)(-left) & 0xffffffff);\n\n\tperf_event_update_userpage(event);\n\n\treturn ret;\n}\n\n/* If performance event entries have been added, move existing\n * events around (if necessary) and then assign new entries to\n * counters.\n */\nstatic u64 maybe_change_configuration(struct cpu_hw_events *cpuc, u64 pcr)\n{\n\tint i;\n\n\tif (!cpuc->n_added)\n\t\tgoto out;\n\n\t/* Read in the counters which are moving.  */\n\tfor (i = 0; i < cpuc->n_events; i++) {\n\t\tstruct perf_event *cp = cpuc->event[i];\n\n\t\tif (cpuc->current_idx[i] != PIC_NO_INDEX &&\n\t\t    cpuc->current_idx[i] != cp->hw.idx) {\n\t\t\tsparc_perf_event_update(cp, &cp->hw,\n\t\t\t\t\t\tcpuc->current_idx[i]);\n\t\t\tcpuc->current_idx[i] = PIC_NO_INDEX;\n\t\t}\n\t}\n\n\t/* Assign to counters all unassigned events.  */\n\tfor (i = 0; i < cpuc->n_events; i++) {\n\t\tstruct perf_event *cp = cpuc->event[i];\n\t\tstruct hw_perf_event *hwc = &cp->hw;\n\t\tint idx = hwc->idx;\n\t\tu64 enc;\n\n\t\tif (cpuc->current_idx[i] != PIC_NO_INDEX)\n\t\t\tcontinue;\n\n\t\tsparc_perf_event_set_period(cp, hwc, idx);\n\t\tcpuc->current_idx[i] = idx;\n\n\t\tenc = perf_event_get_enc(cpuc->events[i]);\n\t\tpcr &= ~mask_for_index(idx);\n\t\tif (hwc->state & PERF_HES_STOPPED)\n\t\t\tpcr |= nop_for_index(idx);\n\t\telse\n\t\t\tpcr |= event_encoding(enc, idx);\n\t}\nout:\n\treturn pcr;\n}\n\nstatic void sparc_pmu_enable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tu64 pcr;\n\n\tif (cpuc->enabled)\n\t\treturn;\n\n\tcpuc->enabled = 1;\n\tbarrier();\n\n\tpcr = cpuc->pcr;\n\tif (!cpuc->n_events) {\n\t\tpcr = 0;\n\t} else {\n\t\tpcr = maybe_change_configuration(cpuc, pcr);\n\n\t\t/* We require that all of the events have the same\n\t\t * configuration, so just fetch the settings from the\n\t\t * first entry.\n\t\t */\n\t\tcpuc->pcr = pcr | cpuc->event[0]->hw.config_base;\n\t}\n\n\tpcr_ops->write(cpuc->pcr);\n}\n\nstatic void sparc_pmu_disable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tu64 val;\n\n\tif (!cpuc->enabled)\n\t\treturn;\n\n\tcpuc->enabled = 0;\n\tcpuc->n_added = 0;\n\n\tval = cpuc->pcr;\n\tval &= ~(PCR_UTRACE | PCR_STRACE |\n\t\t sparc_pmu->hv_bit | sparc_pmu->irq_bit);\n\tcpuc->pcr = val;\n\n\tpcr_ops->write(cpuc->pcr);\n}\n\nstatic int active_event_index(struct cpu_hw_events *cpuc,\n\t\t\t      struct perf_event *event)\n{\n\tint i;\n\n\tfor (i = 0; i < cpuc->n_events; i++) {\n\t\tif (cpuc->event[i] == event)\n\t\t\tbreak;\n\t}\n\tBUG_ON(i == cpuc->n_events);\n\treturn cpuc->current_idx[i];\n}\n\nstatic void sparc_pmu_start(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint idx = active_event_index(cpuc, event);\n\n\tif (flags & PERF_EF_RELOAD) {\n\t\tWARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));\n\t\tsparc_perf_event_set_period(event, &event->hw, idx);\n\t}\n\n\tevent->hw.state = 0;\n\n\tsparc_pmu_enable_event(cpuc, &event->hw, idx);\n}\n\nstatic void sparc_pmu_stop(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint idx = active_event_index(cpuc, event);\n\n\tif (!(event->hw.state & PERF_HES_STOPPED)) {\n\t\tsparc_pmu_disable_event(cpuc, &event->hw, idx);\n\t\tevent->hw.state |= PERF_HES_STOPPED;\n\t}\n\n\tif (!(event->hw.state & PERF_HES_UPTODATE) && (flags & PERF_EF_UPDATE)) {\n\t\tsparc_perf_event_update(event, &event->hw, idx);\n\t\tevent->hw.state |= PERF_HES_UPTODATE;\n\t}\n}\n\nstatic void sparc_pmu_del(struct perf_event *event, int _flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tunsigned long flags;\n\tint i;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tfor (i = 0; i < cpuc->n_events; i++) {\n\t\tif (event == cpuc->event[i]) {\n\t\t\t/* Absorb the final count and turn off the\n\t\t\t * event.\n\t\t\t */\n\t\t\tsparc_pmu_stop(event, PERF_EF_UPDATE);\n\n\t\t\t/* Shift remaining entries down into\n\t\t\t * the existing slot.\n\t\t\t */\n\t\t\twhile (++i < cpuc->n_events) {\n\t\t\t\tcpuc->event[i - 1] = cpuc->event[i];\n\t\t\t\tcpuc->events[i - 1] = cpuc->events[i];\n\t\t\t\tcpuc->current_idx[i - 1] =\n\t\t\t\t\tcpuc->current_idx[i];\n\t\t\t}\n\n\t\t\tperf_event_update_userpage(event);\n\n\t\t\tcpuc->n_events--;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\nstatic void sparc_pmu_read(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint idx = active_event_index(cpuc, event);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tsparc_perf_event_update(event, hwc, idx);\n}\n\nstatic atomic_t active_events = ATOMIC_INIT(0);\nstatic DEFINE_MUTEX(pmc_grab_mutex);\n\nstatic void perf_stop_nmi_watchdog(void *unused)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\tstop_nmi_watchdog(NULL);\n\tcpuc->pcr = pcr_ops->read();\n}\n\nvoid perf_event_grab_pmc(void)\n{\n\tif (atomic_inc_not_zero(&active_events))\n\t\treturn;\n\n\tmutex_lock(&pmc_grab_mutex);\n\tif (atomic_read(&active_events) == 0) {\n\t\tif (atomic_read(&nmi_active) > 0) {\n\t\t\ton_each_cpu(perf_stop_nmi_watchdog, NULL, 1);\n\t\t\tBUG_ON(atomic_read(&nmi_active) != 0);\n\t\t}\n\t\tatomic_inc(&active_events);\n\t}\n\tmutex_unlock(&pmc_grab_mutex);\n}\n\nvoid perf_event_release_pmc(void)\n{\n\tif (atomic_dec_and_mutex_lock(&active_events, &pmc_grab_mutex)) {\n\t\tif (atomic_read(&nmi_active) == 0)\n\t\t\ton_each_cpu(start_nmi_watchdog, NULL, 1);\n\t\tmutex_unlock(&pmc_grab_mutex);\n\t}\n}\n\nstatic const struct perf_event_map *sparc_map_cache_event(u64 config)\n{\n\tunsigned int cache_type, cache_op, cache_result;\n\tconst struct perf_event_map *pmap;\n\n\tif (!sparc_pmu->cache_map)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tcache_type = (config >>  0) & 0xff;\n\tif (cache_type >= PERF_COUNT_HW_CACHE_MAX)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tcache_op = (config >>  8) & 0xff;\n\tif (cache_op >= PERF_COUNT_HW_CACHE_OP_MAX)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tcache_result = (config >> 16) & 0xff;\n\tif (cache_result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tpmap = &((*sparc_pmu->cache_map)[cache_type][cache_op][cache_result]);\n\n\tif (pmap->encoding == CACHE_OP_UNSUPPORTED)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tif (pmap->encoding == CACHE_OP_NONSENSE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn pmap;\n}\n\nstatic void hw_perf_event_destroy(struct perf_event *event)\n{\n\tperf_event_release_pmc();\n}\n\n/* Make sure all events can be scheduled into the hardware at\n * the same time.  This is simplified by the fact that we only\n * need to support 2 simultaneous HW events.\n *\n * As a side effect, the evts[]->hw.idx values will be assigned\n * on success.  These are pending indexes.  When the events are\n * actually programmed into the chip, these values will propagate\n * to the per-cpu cpuc->current_idx[] slots, see the code in\n * maybe_change_configuration() for details.\n */\nstatic int sparc_check_constraints(struct perf_event **evts,\n\t\t\t\t   unsigned long *events, int n_ev)\n{\n\tu8 msk0 = 0, msk1 = 0;\n\tint idx0 = 0;\n\n\t/* This case is possible when we are invoked from\n\t * hw_perf_group_sched_in().\n\t */\n\tif (!n_ev)\n\t\treturn 0;\n\n\tif (n_ev > MAX_HWEVENTS)\n\t\treturn -1;\n\n\tmsk0 = perf_event_get_msk(events[0]);\n\tif (n_ev == 1) {\n\t\tif (msk0 & PIC_LOWER)\n\t\t\tidx0 = 1;\n\t\tgoto success;\n\t}\n\tBUG_ON(n_ev != 2);\n\tmsk1 = perf_event_get_msk(events[1]);\n\n\t/* If both events can go on any counter, OK.  */\n\tif (msk0 == (PIC_UPPER | PIC_LOWER) &&\n\t    msk1 == (PIC_UPPER | PIC_LOWER))\n\t\tgoto success;\n\n\t/* If one event is limited to a specific counter,\n\t * and the other can go on both, OK.\n\t */\n\tif ((msk0 == PIC_UPPER || msk0 == PIC_LOWER) &&\n\t    msk1 == (PIC_UPPER | PIC_LOWER)) {\n\t\tif (msk0 & PIC_LOWER)\n\t\t\tidx0 = 1;\n\t\tgoto success;\n\t}\n\n\tif ((msk1 == PIC_UPPER || msk1 == PIC_LOWER) &&\n\t    msk0 == (PIC_UPPER | PIC_LOWER)) {\n\t\tif (msk1 & PIC_UPPER)\n\t\t\tidx0 = 1;\n\t\tgoto success;\n\t}\n\n\t/* If the events are fixed to different counters, OK.  */\n\tif ((msk0 == PIC_UPPER && msk1 == PIC_LOWER) ||\n\t    (msk0 == PIC_LOWER && msk1 == PIC_UPPER)) {\n\t\tif (msk0 & PIC_LOWER)\n\t\t\tidx0 = 1;\n\t\tgoto success;\n\t}\n\n\t/* Otherwise, there is a conflict.  */\n\treturn -1;\n\nsuccess:\n\tevts[0]->hw.idx = idx0;\n\tif (n_ev == 2)\n\t\tevts[1]->hw.idx = idx0 ^ 1;\n\treturn 0;\n}\n\nstatic int check_excludes(struct perf_event **evts, int n_prev, int n_new)\n{\n\tint eu = 0, ek = 0, eh = 0;\n\tstruct perf_event *event;\n\tint i, n, first;\n\n\tn = n_prev + n_new;\n\tif (n <= 1)\n\t\treturn 0;\n\n\tfirst = 1;\n\tfor (i = 0; i < n; i++) {\n\t\tevent = evts[i];\n\t\tif (first) {\n\t\t\teu = event->attr.exclude_user;\n\t\t\tek = event->attr.exclude_kernel;\n\t\t\teh = event->attr.exclude_hv;\n\t\t\tfirst = 0;\n\t\t} else if (event->attr.exclude_user != eu ||\n\t\t\t   event->attr.exclude_kernel != ek ||\n\t\t\t   event->attr.exclude_hv != eh) {\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int collect_events(struct perf_event *group, int max_count,\n\t\t\t  struct perf_event *evts[], unsigned long *events,\n\t\t\t  int *current_idx)\n{\n\tstruct perf_event *event;\n\tint n = 0;\n\n\tif (!is_software_event(group)) {\n\t\tif (n >= max_count)\n\t\t\treturn -1;\n\t\tevts[n] = group;\n\t\tevents[n] = group->hw.event_base;\n\t\tcurrent_idx[n++] = PIC_NO_INDEX;\n\t}\n\tlist_for_each_entry(event, &group->sibling_list, group_entry) {\n\t\tif (!is_software_event(event) &&\n\t\t    event->state != PERF_EVENT_STATE_OFF) {\n\t\t\tif (n >= max_count)\n\t\t\t\treturn -1;\n\t\t\tevts[n] = event;\n\t\t\tevents[n] = event->hw.event_base;\n\t\t\tcurrent_idx[n++] = PIC_NO_INDEX;\n\t\t}\n\t}\n\treturn n;\n}\n\nstatic int sparc_pmu_add(struct perf_event *event, int ef_flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint n0, ret = -EAGAIN;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tn0 = cpuc->n_events;\n\tif (n0 >= MAX_HWEVENTS)\n\t\tgoto out;\n\n\tcpuc->event[n0] = event;\n\tcpuc->events[n0] = event->hw.event_base;\n\tcpuc->current_idx[n0] = PIC_NO_INDEX;\n\n\tevent->hw.state = PERF_HES_UPTODATE;\n\tif (!(ef_flags & PERF_EF_START))\n\t\tevent->hw.state |= PERF_HES_STOPPED;\n\n\t/*\n\t * If group events scheduling transaction was started,\n\t * skip the schedulability test here, it will be performed\n\t * at commit time(->commit_txn) as a whole\n\t */\n\tif (cpuc->group_flag & PERF_EVENT_TXN)\n\t\tgoto nocheck;\n\n\tif (check_excludes(cpuc->event, n0, 1))\n\t\tgoto out;\n\tif (sparc_check_constraints(cpuc->event, cpuc->events, n0 + 1))\n\t\tgoto out;\n\nnocheck:\n\tcpuc->n_events++;\n\tcpuc->n_added++;\n\n\tret = 0;\nout:\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\n\nstatic int sparc_pmu_event_init(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\tstruct perf_event *evts[MAX_HWEVENTS];\n\tstruct hw_perf_event *hwc = &event->hw;\n\tunsigned long events[MAX_HWEVENTS];\n\tint current_idx_dmy[MAX_HWEVENTS];\n\tconst struct perf_event_map *pmap;\n\tint n;\n\n\tif (atomic_read(&nmi_active) < 0)\n\t\treturn -ENODEV;\n\n\tswitch (attr->type) {\n\tcase PERF_TYPE_HARDWARE:\n\t\tif (attr->config >= sparc_pmu->max_events)\n\t\t\treturn -EINVAL;\n\t\tpmap = sparc_pmu->event_map(attr->config);\n\t\tbreak;\n\n\tcase PERF_TYPE_HW_CACHE:\n\t\tpmap = sparc_map_cache_event(attr->config);\n\t\tif (IS_ERR(pmap))\n\t\t\treturn PTR_ERR(pmap);\n\t\tbreak;\n\n\tcase PERF_TYPE_RAW:\n\t\tpmap = NULL;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOENT;\n\n\t}\n\n\tif (pmap) {\n\t\thwc->event_base = perf_event_encode(pmap);\n\t} else {\n\t\t/*\n\t\t * User gives us \"(encoding << 16) | pic_mask\" for\n\t\t * PERF_TYPE_RAW events.\n\t\t */\n\t\thwc->event_base = attr->config;\n\t}\n\n\t/* We save the enable bits in the config_base.  */\n\thwc->config_base = sparc_pmu->irq_bit;\n\tif (!attr->exclude_user)\n\t\thwc->config_base |= PCR_UTRACE;\n\tif (!attr->exclude_kernel)\n\t\thwc->config_base |= PCR_STRACE;\n\tif (!attr->exclude_hv)\n\t\thwc->config_base |= sparc_pmu->hv_bit;\n\n\tn = 0;\n\tif (event->group_leader != event) {\n\t\tn = collect_events(event->group_leader,\n\t\t\t\t   MAX_HWEVENTS - 1,\n\t\t\t\t   evts, events, current_idx_dmy);\n\t\tif (n < 0)\n\t\t\treturn -EINVAL;\n\t}\n\tevents[n] = hwc->event_base;\n\tevts[n] = event;\n\n\tif (check_excludes(evts, n, 1))\n\t\treturn -EINVAL;\n\n\tif (sparc_check_constraints(evts, events, n + 1))\n\t\treturn -EINVAL;\n\n\thwc->idx = PIC_NO_INDEX;\n\n\t/* Try to do all error checking before this point, as unwinding\n\t * state after grabbing the PMC is difficult.\n\t */\n\tperf_event_grab_pmc();\n\tevent->destroy = hw_perf_event_destroy;\n\n\tif (!hwc->sample_period) {\n\t\thwc->sample_period = MAX_PERIOD;\n\t\thwc->last_period = hwc->sample_period;\n\t\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\t}\n\n\treturn 0;\n}\n\n/*\n * Start group events scheduling transaction\n * Set the flag to make pmu::enable() not perform the\n * schedulability test, it will be performed at commit time\n */\nstatic void sparc_pmu_start_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\n\n\tperf_pmu_disable(pmu);\n\tcpuhw->group_flag |= PERF_EVENT_TXN;\n}\n\n/*\n * Stop group events scheduling transaction\n * Clear the flag and pmu::enable() will perform the\n * schedulability test.\n */\nstatic void sparc_pmu_cancel_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\n\n\tcpuhw->group_flag &= ~PERF_EVENT_TXN;\n\tperf_pmu_enable(pmu);\n}\n\n/*\n * Commit group events scheduling transaction\n * Perform the group schedulability test as a whole\n * Return 0 if success\n */\nstatic int sparc_pmu_commit_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint n;\n\n\tif (!sparc_pmu)\n\t\treturn -EINVAL;\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\tn = cpuc->n_events;\n\tif (check_excludes(cpuc->event, 0, n))\n\t\treturn -EINVAL;\n\tif (sparc_check_constraints(cpuc->event, cpuc->events, n))\n\t\treturn -EAGAIN;\n\n\tcpuc->group_flag &= ~PERF_EVENT_TXN;\n\tperf_pmu_enable(pmu);\n\treturn 0;\n}\n\nstatic struct pmu pmu = {\n\t.pmu_enable\t= sparc_pmu_enable,\n\t.pmu_disable\t= sparc_pmu_disable,\n\t.event_init\t= sparc_pmu_event_init,\n\t.add\t\t= sparc_pmu_add,\n\t.del\t\t= sparc_pmu_del,\n\t.start\t\t= sparc_pmu_start,\n\t.stop\t\t= sparc_pmu_stop,\n\t.read\t\t= sparc_pmu_read,\n\t.start_txn\t= sparc_pmu_start_txn,\n\t.cancel_txn\t= sparc_pmu_cancel_txn,\n\t.commit_txn\t= sparc_pmu_commit_txn,\n};\n\nvoid perf_event_print_debug(void)\n{\n\tunsigned long flags;\n\tu64 pcr, pic;\n\tint cpu;\n\n\tif (!sparc_pmu)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tcpu = smp_processor_id();\n\n\tpcr = pcr_ops->read();\n\tread_pic(pic);\n\n\tpr_info(\"\\n\");\n\tpr_info(\"CPU#%d: PCR[%016llx] PIC[%016llx]\\n\",\n\t\tcpu, pcr, pic);\n\n\tlocal_irq_restore(flags);\n}\n\nstatic int __kprobes perf_event_nmi_handler(struct notifier_block *self,\n\t\t\t\t\t    unsigned long cmd, void *__args)\n{\n\tstruct die_args *args = __args;\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct pt_regs *regs;\n\tint i;\n\n\tif (!atomic_read(&active_events))\n\t\treturn NOTIFY_DONE;\n\n\tswitch (cmd) {\n\tcase DIE_NMI:\n\t\tbreak;\n\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n\n\tregs = args->regs;\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\n\t/* If the PMU has the TOE IRQ enable bits, we need to do a\n\t * dummy write to the %pcr to clear the overflow bits and thus\n\t * the interrupt.\n\t *\n\t * Do this before we peek at the counters to determine\n\t * overflow so we don't lose any events.\n\t */\n\tif (sparc_pmu->irq_bit)\n\t\tpcr_ops->write(cpuc->pcr);\n\n\tfor (i = 0; i < cpuc->n_events; i++) {\n\t\tstruct perf_event *event = cpuc->event[i];\n\t\tint idx = cpuc->current_idx[i];\n\t\tstruct hw_perf_event *hwc;\n\t\tu64 val;\n\n\t\thwc = &event->hw;\n\t\tval = sparc_perf_event_update(event, hwc, idx);\n\t\tif (val & (1ULL << 31))\n\t\t\tcontinue;\n\n\t\tdata.period = event->hw.last_period;\n\t\tif (!sparc_perf_event_set_period(event, hwc, idx))\n\t\t\tcontinue;\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tsparc_pmu_stop(event, 0);\n\t}\n\n\treturn NOTIFY_STOP;\n}\n\nstatic __read_mostly struct notifier_block perf_event_nmi_notifier = {\n\t.notifier_call\t\t= perf_event_nmi_handler,\n};\n\nstatic bool __init supported_pmu(void)\n{\n\tif (!strcmp(sparc_pmu_type, \"ultra3\") ||\n\t    !strcmp(sparc_pmu_type, \"ultra3+\") ||\n\t    !strcmp(sparc_pmu_type, \"ultra3i\") ||\n\t    !strcmp(sparc_pmu_type, \"ultra4+\")) {\n\t\tsparc_pmu = &ultra3_pmu;\n\t\treturn true;\n\t}\n\tif (!strcmp(sparc_pmu_type, \"niagara\")) {\n\t\tsparc_pmu = &niagara1_pmu;\n\t\treturn true;\n\t}\n\tif (!strcmp(sparc_pmu_type, \"niagara2\")) {\n\t\tsparc_pmu = &niagara2_pmu;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nint __init init_hw_perf_events(void)\n{\n\tpr_info(\"Performance events: \");\n\n\tif (!supported_pmu()) {\n\t\tpr_cont(\"No support for PMU type '%s'\\n\", sparc_pmu_type);\n\t\treturn 0;\n\t}\n\n\tpr_cont(\"Supported PMU type is '%s'\\n\", sparc_pmu_type);\n\n\tperf_pmu_register(&pmu, \"cpu\", PERF_TYPE_RAW);\n\tregister_die_notifier(&perf_event_nmi_notifier);\n\n\treturn 0;\n}\nearly_initcall(init_hw_perf_events);\n\nvoid perf_callchain_kernel(struct perf_callchain_entry *entry,\n\t\t\t   struct pt_regs *regs)\n{\n\tunsigned long ksp, fp;\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\tint graph = 0;\n#endif\n\n\tstack_trace_flush();\n\n\tperf_callchain_store(entry, regs->tpc);\n\n\tksp = regs->u_regs[UREG_I6];\n\tfp = ksp + STACK_BIAS;\n\tdo {\n\t\tstruct sparc_stackf *sf;\n\t\tstruct pt_regs *regs;\n\t\tunsigned long pc;\n\n\t\tif (!kstack_valid(current_thread_info(), fp))\n\t\t\tbreak;\n\n\t\tsf = (struct sparc_stackf *) fp;\n\t\tregs = (struct pt_regs *) (sf + 1);\n\n\t\tif (kstack_is_trap_frame(current_thread_info(), regs)) {\n\t\t\tif (user_mode(regs))\n\t\t\t\tbreak;\n\t\t\tpc = regs->tpc;\n\t\t\tfp = regs->u_regs[UREG_I6] + STACK_BIAS;\n\t\t} else {\n\t\t\tpc = sf->callers_pc;\n\t\t\tfp = (unsigned long)sf->fp + STACK_BIAS;\n\t\t}\n\t\tperf_callchain_store(entry, pc);\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\t\tif ((pc + 8UL) == (unsigned long) &return_to_handler) {\n\t\t\tint index = current->curr_ret_stack;\n\t\t\tif (current->ret_stack && index >= graph) {\n\t\t\t\tpc = current->ret_stack[index - graph].ret;\n\t\t\t\tperf_callchain_store(entry, pc);\n\t\t\t\tgraph++;\n\t\t\t}\n\t\t}\n#endif\n\t} while (entry->nr < PERF_MAX_STACK_DEPTH);\n}\n\nstatic void perf_callchain_user_64(struct perf_callchain_entry *entry,\n\t\t\t\t   struct pt_regs *regs)\n{\n\tunsigned long ufp;\n\n\tperf_callchain_store(entry, regs->tpc);\n\n\tufp = regs->u_regs[UREG_I6] + STACK_BIAS;\n\tdo {\n\t\tstruct sparc_stackf *usf, sf;\n\t\tunsigned long pc;\n\n\t\tusf = (struct sparc_stackf *) ufp;\n\t\tif (__copy_from_user_inatomic(&sf, usf, sizeof(sf)))\n\t\t\tbreak;\n\n\t\tpc = sf.callers_pc;\n\t\tufp = (unsigned long)sf.fp + STACK_BIAS;\n\t\tperf_callchain_store(entry, pc);\n\t} while (entry->nr < PERF_MAX_STACK_DEPTH);\n}\n\nstatic void perf_callchain_user_32(struct perf_callchain_entry *entry,\n\t\t\t\t   struct pt_regs *regs)\n{\n\tunsigned long ufp;\n\n\tperf_callchain_store(entry, regs->tpc);\n\n\tufp = regs->u_regs[UREG_I6] & 0xffffffffUL;\n\tdo {\n\t\tstruct sparc_stackf32 *usf, sf;\n\t\tunsigned long pc;\n\n\t\tusf = (struct sparc_stackf32 *) ufp;\n\t\tif (__copy_from_user_inatomic(&sf, usf, sizeof(sf)))\n\t\t\tbreak;\n\n\t\tpc = sf.callers_pc;\n\t\tufp = (unsigned long)sf.fp;\n\t\tperf_callchain_store(entry, pc);\n\t} while (entry->nr < PERF_MAX_STACK_DEPTH);\n}\n\nvoid\nperf_callchain_user(struct perf_callchain_entry *entry, struct pt_regs *regs)\n{\n\tflushw_user();\n\tif (test_thread_flag(TIF_32BIT))\n\t\tperf_callchain_user_32(entry, regs);\n\telse\n\t\tperf_callchain_user_64(entry, regs);\n}\n", "/*\n * unaligned.c: Unaligned load/store trap handling with special\n *              cases for the kernel to do them more quickly.\n *\n * Copyright (C) 1996 David S. Miller (davem@caip.rutgers.edu)\n * Copyright (C) 1996 Jakub Jelinek (jj@sunsite.mff.cuni.cz)\n */\n\n\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <asm/ptrace.h>\n#include <asm/processor.h>\n#include <asm/system.h>\n#include <asm/uaccess.h>\n#include <linux/smp.h>\n#include <linux/perf_event.h>\n\nenum direction {\n\tload,    /* ld, ldd, ldh, ldsh */\n\tstore,   /* st, std, sth, stsh */\n\tboth,    /* Swap, ldstub, etc. */\n\tfpload,\n\tfpstore,\n\tinvalid,\n};\n\nstatic inline enum direction decode_direction(unsigned int insn)\n{\n\tunsigned long tmp = (insn >> 21) & 1;\n\n\tif(!tmp)\n\t\treturn load;\n\telse {\n\t\tif(((insn>>19)&0x3f) == 15)\n\t\t\treturn both;\n\t\telse\n\t\t\treturn store;\n\t}\n}\n\n/* 8 = double-word, 4 = word, 2 = half-word */\nstatic inline int decode_access_size(unsigned int insn)\n{\n\tinsn = (insn >> 19) & 3;\n\n\tif(!insn)\n\t\treturn 4;\n\telse if(insn == 3)\n\t\treturn 8;\n\telse if(insn == 2)\n\t\treturn 2;\n\telse {\n\t\tprintk(\"Impossible unaligned trap. insn=%08x\\n\", insn);\n\t\tdie_if_kernel(\"Byte sized unaligned access?!?!\", current->thread.kregs);\n\t\treturn 4; /* just to keep gcc happy. */\n\t}\n}\n\n/* 0x400000 = signed, 0 = unsigned */\nstatic inline int decode_signedness(unsigned int insn)\n{\n\treturn (insn & 0x400000);\n}\n\nstatic inline void maybe_flush_windows(unsigned int rs1, unsigned int rs2,\n\t\t\t\t       unsigned int rd)\n{\n\tif(rs2 >= 16 || rs1 >= 16 || rd >= 16) {\n\t\t/* Wheee... */\n\t\t__asm__ __volatile__(\"save %sp, -0x40, %sp\\n\\t\"\n\t\t\t\t     \"save %sp, -0x40, %sp\\n\\t\"\n\t\t\t\t     \"save %sp, -0x40, %sp\\n\\t\"\n\t\t\t\t     \"save %sp, -0x40, %sp\\n\\t\"\n\t\t\t\t     \"save %sp, -0x40, %sp\\n\\t\"\n\t\t\t\t     \"save %sp, -0x40, %sp\\n\\t\"\n\t\t\t\t     \"save %sp, -0x40, %sp\\n\\t\"\n\t\t\t\t     \"restore; restore; restore; restore;\\n\\t\"\n\t\t\t\t     \"restore; restore; restore;\\n\\t\");\n\t}\n}\n\nstatic inline int sign_extend_imm13(int imm)\n{\n\treturn imm << 19 >> 19;\n}\n\nstatic inline unsigned long fetch_reg(unsigned int reg, struct pt_regs *regs)\n{\n\tstruct reg_window32 *win;\n\n\tif(reg < 16)\n\t\treturn (!reg ? 0 : regs->u_regs[reg]);\n\n\t/* Ho hum, the slightly complicated case. */\n\twin = (struct reg_window32 *) regs->u_regs[UREG_FP];\n\treturn win->locals[reg - 16]; /* yes, I know what this does... */\n}\n\nstatic inline unsigned long safe_fetch_reg(unsigned int reg, struct pt_regs *regs)\n{\n\tstruct reg_window32 __user *win;\n\tunsigned long ret;\n\n\tif (reg < 16)\n\t\treturn (!reg ? 0 : regs->u_regs[reg]);\n\n\t/* Ho hum, the slightly complicated case. */\n\twin = (struct reg_window32 __user *) regs->u_regs[UREG_FP];\n\n\tif ((unsigned long)win & 3)\n\t\treturn -1;\n\n\tif (get_user(ret, &win->locals[reg - 16]))\n\t\treturn -1;\n\n\treturn ret;\n}\n\nstatic inline unsigned long *fetch_reg_addr(unsigned int reg, struct pt_regs *regs)\n{\n\tstruct reg_window32 *win;\n\n\tif(reg < 16)\n\t\treturn &regs->u_regs[reg];\n\twin = (struct reg_window32 *) regs->u_regs[UREG_FP];\n\treturn &win->locals[reg - 16];\n}\n\nstatic unsigned long compute_effective_address(struct pt_regs *regs,\n\t\t\t\t\t       unsigned int insn)\n{\n\tunsigned int rs1 = (insn >> 14) & 0x1f;\n\tunsigned int rs2 = insn & 0x1f;\n\tunsigned int rd = (insn >> 25) & 0x1f;\n\n\tif(insn & 0x2000) {\n\t\tmaybe_flush_windows(rs1, 0, rd);\n\t\treturn (fetch_reg(rs1, regs) + sign_extend_imm13(insn));\n\t} else {\n\t\tmaybe_flush_windows(rs1, rs2, rd);\n\t\treturn (fetch_reg(rs1, regs) + fetch_reg(rs2, regs));\n\t}\n}\n\nunsigned long safe_compute_effective_address(struct pt_regs *regs,\n\t\t\t\t\t     unsigned int insn)\n{\n\tunsigned int rs1 = (insn >> 14) & 0x1f;\n\tunsigned int rs2 = insn & 0x1f;\n\tunsigned int rd = (insn >> 25) & 0x1f;\n\n\tif(insn & 0x2000) {\n\t\tmaybe_flush_windows(rs1, 0, rd);\n\t\treturn (safe_fetch_reg(rs1, regs) + sign_extend_imm13(insn));\n\t} else {\n\t\tmaybe_flush_windows(rs1, rs2, rd);\n\t\treturn (safe_fetch_reg(rs1, regs) + safe_fetch_reg(rs2, regs));\n\t}\n}\n\n/* This is just to make gcc think panic does return... */\nstatic void unaligned_panic(char *str)\n{\n\tpanic(str);\n}\n\n/* una_asm.S */\nextern int do_int_load(unsigned long *dest_reg, int size,\n\t\t       unsigned long *saddr, int is_signed);\nextern int __do_int_store(unsigned long *dst_addr, int size,\n\t\t\t  unsigned long *src_val);\n\nstatic int do_int_store(int reg_num, int size, unsigned long *dst_addr,\n\t\t\tstruct pt_regs *regs)\n{\n\tunsigned long zero[2] = { 0, 0 };\n\tunsigned long *src_val;\n\n\tif (reg_num)\n\t\tsrc_val = fetch_reg_addr(reg_num, regs);\n\telse {\n\t\tsrc_val = &zero[0];\n\t\tif (size == 8)\n\t\t\tzero[1] = fetch_reg(1, regs);\n\t}\n\treturn __do_int_store(dst_addr, size, src_val);\n}\n\nextern void smp_capture(void);\nextern void smp_release(void);\n\nstatic inline void advance(struct pt_regs *regs)\n{\n\tregs->pc   = regs->npc;\n\tregs->npc += 4;\n}\n\nstatic inline int floating_point_load_or_store_p(unsigned int insn)\n{\n\treturn (insn >> 24) & 1;\n}\n\nstatic inline int ok_for_kernel(unsigned int insn)\n{\n\treturn !floating_point_load_or_store_p(insn);\n}\n\nstatic void kernel_mna_trap_fault(struct pt_regs *regs, unsigned int insn)\n{\n\tunsigned long g2 = regs->u_regs [UREG_G2];\n\tunsigned long fixup = search_extables_range(regs->pc, &g2);\n\n\tif (!fixup) {\n\t\tunsigned long address = compute_effective_address(regs, insn);\n        \tif(address < PAGE_SIZE) {\n                \tprintk(KERN_ALERT \"Unable to handle kernel NULL pointer dereference in mna handler\");\n        \t} else\n                \tprintk(KERN_ALERT \"Unable to handle kernel paging request in mna handler\");\n\t        printk(KERN_ALERT \" at virtual address %08lx\\n\",address);\n\t\tprintk(KERN_ALERT \"current->{mm,active_mm}->context = %08lx\\n\",\n\t\t\t(current->mm ? current->mm->context :\n\t\t\tcurrent->active_mm->context));\n\t\tprintk(KERN_ALERT \"current->{mm,active_mm}->pgd = %08lx\\n\",\n\t\t\t(current->mm ? (unsigned long) current->mm->pgd :\n\t\t\t(unsigned long) current->active_mm->pgd));\n\t        die_if_kernel(\"Oops\", regs);\n\t\t/* Not reached */\n\t}\n\tregs->pc = fixup;\n\tregs->npc = regs->pc + 4;\n\tregs->u_regs [UREG_G2] = g2;\n}\n\nasmlinkage void kernel_unaligned_trap(struct pt_regs *regs, unsigned int insn)\n{\n\tenum direction dir = decode_direction(insn);\n\tint size = decode_access_size(insn);\n\n\tif(!ok_for_kernel(insn) || dir == both) {\n\t\tprintk(\"Unsupported unaligned load/store trap for kernel at <%08lx>.\\n\",\n\t\t       regs->pc);\n\t\tunaligned_panic(\"Wheee. Kernel does fpu/atomic unaligned load/store.\");\n\t} else {\n\t\tunsigned long addr = compute_effective_address(regs, insn);\n\t\tint err;\n\n\t\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, addr);\n\t\tswitch (dir) {\n\t\tcase load:\n\t\t\terr = do_int_load(fetch_reg_addr(((insn>>25)&0x1f),\n\t\t\t\t\t\t\t regs),\n\t\t\t\t\t  size, (unsigned long *) addr,\n\t\t\t\t\t  decode_signedness(insn));\n\t\t\tbreak;\n\n\t\tcase store:\n\t\t\terr = do_int_store(((insn>>25)&0x1f), size,\n\t\t\t\t\t   (unsigned long *) addr, regs);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpanic(\"Impossible kernel unaligned trap.\");\n\t\t\t/* Not reached... */\n\t\t}\n\t\tif (err)\n\t\t\tkernel_mna_trap_fault(regs, insn);\n\t\telse\n\t\t\tadvance(regs);\n\t}\n}\n\nstatic inline int ok_for_user(struct pt_regs *regs, unsigned int insn,\n\t\t\t      enum direction dir)\n{\n\tunsigned int reg;\n\tint check = (dir == load) ? VERIFY_READ : VERIFY_WRITE;\n\tint size = ((insn >> 19) & 3) == 3 ? 8 : 4;\n\n\tif ((regs->pc | regs->npc) & 3)\n\t\treturn 0;\n\n\t/* Must access_ok() in all the necessary places. */\n#define WINREG_ADDR(regnum) \\\n\t((void __user *)(((unsigned long *)regs->u_regs[UREG_FP])+(regnum)))\n\n\treg = (insn >> 25) & 0x1f;\n\tif (reg >= 16) {\n\t\tif (!access_ok(check, WINREG_ADDR(reg - 16), size))\n\t\t\treturn -EFAULT;\n\t}\n\treg = (insn >> 14) & 0x1f;\n\tif (reg >= 16) {\n\t\tif (!access_ok(check, WINREG_ADDR(reg - 16), size))\n\t\t\treturn -EFAULT;\n\t}\n\tif (!(insn & 0x2000)) {\n\t\treg = (insn & 0x1f);\n\t\tif (reg >= 16) {\n\t\t\tif (!access_ok(check, WINREG_ADDR(reg - 16), size))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t}\n#undef WINREG_ADDR\n\treturn 0;\n}\n\nstatic void user_mna_trap_fault(struct pt_regs *regs, unsigned int insn)\n{\n\tsiginfo_t info;\n\n\tinfo.si_signo = SIGBUS;\n\tinfo.si_errno = 0;\n\tinfo.si_code = BUS_ADRALN;\n\tinfo.si_addr = (void __user *)safe_compute_effective_address(regs, insn);\n\tinfo.si_trapno = 0;\n\tsend_sig_info(SIGBUS, &info, current);\n}\n\nasmlinkage void user_unaligned_trap(struct pt_regs *regs, unsigned int insn)\n{\n\tenum direction dir;\n\n\tif(!(current->thread.flags & SPARC_FLAG_UNALIGNED) ||\n\t   (((insn >> 30) & 3) != 3))\n\t\tgoto kill_user;\n\tdir = decode_direction(insn);\n\tif(!ok_for_user(regs, insn, dir)) {\n\t\tgoto kill_user;\n\t} else {\n\t\tint err, size = decode_access_size(insn);\n\t\tunsigned long addr;\n\n\t\tif(floating_point_load_or_store_p(insn)) {\n\t\t\tprintk(\"User FPU load/store unaligned unsupported.\\n\");\n\t\t\tgoto kill_user;\n\t\t}\n\n\t\taddr = compute_effective_address(regs, insn);\n\t\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, addr);\n\t\tswitch(dir) {\n\t\tcase load:\n\t\t\terr = do_int_load(fetch_reg_addr(((insn>>25)&0x1f),\n\t\t\t\t\t\t\t regs),\n\t\t\t\t\t  size, (unsigned long *) addr,\n\t\t\t\t\t  decode_signedness(insn));\n\t\t\tbreak;\n\n\t\tcase store:\n\t\t\terr = do_int_store(((insn>>25)&0x1f), size,\n\t\t\t\t\t   (unsigned long *) addr, regs);\n\t\t\tbreak;\n\n\t\tcase both:\n\t\t\t/*\n\t\t\t * This was supported in 2.4. However, we question\n\t\t\t * the value of SWAP instruction across word boundaries.\n\t\t\t */\n\t\t\tprintk(\"Unaligned SWAP unsupported.\\n\");\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tunaligned_panic(\"Impossible user unaligned trap.\");\n\t\t\tgoto out;\n\t\t}\n\t\tif (err)\n\t\t\tgoto kill_user;\n\t\telse\n\t\t\tadvance(regs);\n\t\tgoto out;\n\t}\n\nkill_user:\n\tuser_mna_trap_fault(regs, insn);\nout:\n\t;\n}\n", "/*\n * unaligned.c: Unaligned load/store trap handling with special\n *              cases for the kernel to do them more quickly.\n *\n * Copyright (C) 1996,2008 David S. Miller (davem@davemloft.net)\n * Copyright (C) 1996,1997 Jakub Jelinek (jj@sunsite.mff.cuni.cz)\n */\n\n\n#include <linux/jiffies.h>\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <asm/asi.h>\n#include <asm/ptrace.h>\n#include <asm/pstate.h>\n#include <asm/processor.h>\n#include <asm/system.h>\n#include <asm/uaccess.h>\n#include <linux/smp.h>\n#include <linux/bitops.h>\n#include <linux/perf_event.h>\n#include <linux/ratelimit.h>\n#include <asm/fpumacro.h>\n\nenum direction {\n\tload,    /* ld, ldd, ldh, ldsh */\n\tstore,   /* st, std, sth, stsh */\n\tboth,    /* Swap, ldstub, cas, ... */\n\tfpld,\n\tfpst,\n\tinvalid,\n};\n\nstatic inline enum direction decode_direction(unsigned int insn)\n{\n\tunsigned long tmp = (insn >> 21) & 1;\n\n\tif (!tmp)\n\t\treturn load;\n\telse {\n\t\tswitch ((insn>>19)&0xf) {\n\t\tcase 15: /* swap* */\n\t\t\treturn both;\n\t\tdefault:\n\t\t\treturn store;\n\t\t}\n\t}\n}\n\n/* 16 = double-word, 8 = extra-word, 4 = word, 2 = half-word */\nstatic inline int decode_access_size(struct pt_regs *regs, unsigned int insn)\n{\n\tunsigned int tmp;\n\n\ttmp = ((insn >> 19) & 0xf);\n\tif (tmp == 11 || tmp == 14) /* ldx/stx */\n\t\treturn 8;\n\ttmp &= 3;\n\tif (!tmp)\n\t\treturn 4;\n\telse if (tmp == 3)\n\t\treturn 16;\t/* ldd/std - Although it is actually 8 */\n\telse if (tmp == 2)\n\t\treturn 2;\n\telse {\n\t\tprintk(\"Impossible unaligned trap. insn=%08x\\n\", insn);\n\t\tdie_if_kernel(\"Byte sized unaligned access?!?!\", regs);\n\n\t\t/* GCC should never warn that control reaches the end\n\t\t * of this function without returning a value because\n\t\t * die_if_kernel() is marked with attribute 'noreturn'.\n\t\t * Alas, some versions do...\n\t\t */\n\n\t\treturn 0;\n\t}\n}\n\nstatic inline int decode_asi(unsigned int insn, struct pt_regs *regs)\n{\n\tif (insn & 0x800000) {\n\t\tif (insn & 0x2000)\n\t\t\treturn (unsigned char)(regs->tstate >> 24);\t/* %asi */\n\t\telse\n\t\t\treturn (unsigned char)(insn >> 5);\t\t/* imm_asi */\n\t} else\n\t\treturn ASI_P;\n}\n\n/* 0x400000 = signed, 0 = unsigned */\nstatic inline int decode_signedness(unsigned int insn)\n{\n\treturn (insn & 0x400000);\n}\n\nstatic inline void maybe_flush_windows(unsigned int rs1, unsigned int rs2,\n\t\t\t\t       unsigned int rd, int from_kernel)\n{\n\tif (rs2 >= 16 || rs1 >= 16 || rd >= 16) {\n\t\tif (from_kernel != 0)\n\t\t\t__asm__ __volatile__(\"flushw\");\n\t\telse\n\t\t\tflushw_user();\n\t}\n}\n\nstatic inline long sign_extend_imm13(long imm)\n{\n\treturn imm << 51 >> 51;\n}\n\nstatic unsigned long fetch_reg(unsigned int reg, struct pt_regs *regs)\n{\n\tunsigned long value;\n\t\n\tif (reg < 16)\n\t\treturn (!reg ? 0 : regs->u_regs[reg]);\n\tif (regs->tstate & TSTATE_PRIV) {\n\t\tstruct reg_window *win;\n\t\twin = (struct reg_window *)(regs->u_regs[UREG_FP] + STACK_BIAS);\n\t\tvalue = win->locals[reg - 16];\n\t} else if (test_thread_flag(TIF_32BIT)) {\n\t\tstruct reg_window32 __user *win32;\n\t\twin32 = (struct reg_window32 __user *)((unsigned long)((u32)regs->u_regs[UREG_FP]));\n\t\tget_user(value, &win32->locals[reg - 16]);\n\t} else {\n\t\tstruct reg_window __user *win;\n\t\twin = (struct reg_window __user *)(regs->u_regs[UREG_FP] + STACK_BIAS);\n\t\tget_user(value, &win->locals[reg - 16]);\n\t}\n\treturn value;\n}\n\nstatic unsigned long *fetch_reg_addr(unsigned int reg, struct pt_regs *regs)\n{\n\tif (reg < 16)\n\t\treturn &regs->u_regs[reg];\n\tif (regs->tstate & TSTATE_PRIV) {\n\t\tstruct reg_window *win;\n\t\twin = (struct reg_window *)(regs->u_regs[UREG_FP] + STACK_BIAS);\n\t\treturn &win->locals[reg - 16];\n\t} else if (test_thread_flag(TIF_32BIT)) {\n\t\tstruct reg_window32 *win32;\n\t\twin32 = (struct reg_window32 *)((unsigned long)((u32)regs->u_regs[UREG_FP]));\n\t\treturn (unsigned long *)&win32->locals[reg - 16];\n\t} else {\n\t\tstruct reg_window *win;\n\t\twin = (struct reg_window *)(regs->u_regs[UREG_FP] + STACK_BIAS);\n\t\treturn &win->locals[reg - 16];\n\t}\n}\n\nunsigned long compute_effective_address(struct pt_regs *regs,\n\t\t\t\t\tunsigned int insn, unsigned int rd)\n{\n\tunsigned int rs1 = (insn >> 14) & 0x1f;\n\tunsigned int rs2 = insn & 0x1f;\n\tint from_kernel = (regs->tstate & TSTATE_PRIV) != 0;\n\n\tif (insn & 0x2000) {\n\t\tmaybe_flush_windows(rs1, 0, rd, from_kernel);\n\t\treturn (fetch_reg(rs1, regs) + sign_extend_imm13(insn));\n\t} else {\n\t\tmaybe_flush_windows(rs1, rs2, rd, from_kernel);\n\t\treturn (fetch_reg(rs1, regs) + fetch_reg(rs2, regs));\n\t}\n}\n\n/* This is just to make gcc think die_if_kernel does return... */\nstatic void __used unaligned_panic(char *str, struct pt_regs *regs)\n{\n\tdie_if_kernel(str, regs);\n}\n\nextern int do_int_load(unsigned long *dest_reg, int size,\n\t\t       unsigned long *saddr, int is_signed, int asi);\n\t\nextern int __do_int_store(unsigned long *dst_addr, int size,\n\t\t\t  unsigned long src_val, int asi);\n\nstatic inline int do_int_store(int reg_num, int size, unsigned long *dst_addr,\n\t\t\t       struct pt_regs *regs, int asi, int orig_asi)\n{\n\tunsigned long zero = 0;\n\tunsigned long *src_val_p = &zero;\n\tunsigned long src_val;\n\n\tif (size == 16) {\n\t\tsize = 8;\n\t\tzero = (((long)(reg_num ?\n\t\t        (unsigned)fetch_reg(reg_num, regs) : 0)) << 32) |\n\t\t\t(unsigned)fetch_reg(reg_num + 1, regs);\n\t} else if (reg_num) {\n\t\tsrc_val_p = fetch_reg_addr(reg_num, regs);\n\t}\n\tsrc_val = *src_val_p;\n\tif (unlikely(asi != orig_asi)) {\n\t\tswitch (size) {\n\t\tcase 2:\n\t\t\tsrc_val = swab16(src_val);\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\tsrc_val = swab32(src_val);\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tsrc_val = swab64(src_val);\n\t\t\tbreak;\n\t\tcase 16:\n\t\tdefault:\n\t\t\tBUG();\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn __do_int_store(dst_addr, size, src_val, asi);\n}\n\nstatic inline void advance(struct pt_regs *regs)\n{\n\tregs->tpc   = regs->tnpc;\n\tregs->tnpc += 4;\n\tif (test_thread_flag(TIF_32BIT)) {\n\t\tregs->tpc &= 0xffffffff;\n\t\tregs->tnpc &= 0xffffffff;\n\t}\n}\n\nstatic inline int floating_point_load_or_store_p(unsigned int insn)\n{\n\treturn (insn >> 24) & 1;\n}\n\nstatic inline int ok_for_kernel(unsigned int insn)\n{\n\treturn !floating_point_load_or_store_p(insn);\n}\n\nstatic void kernel_mna_trap_fault(int fixup_tstate_asi)\n{\n\tstruct pt_regs *regs = current_thread_info()->kern_una_regs;\n\tunsigned int insn = current_thread_info()->kern_una_insn;\n\tconst struct exception_table_entry *entry;\n\n\tentry = search_exception_tables(regs->tpc);\n\tif (!entry) {\n\t\tunsigned long address;\n\n\t\taddress = compute_effective_address(regs, insn,\n\t\t\t\t\t\t    ((insn >> 25) & 0x1f));\n        \tif (address < PAGE_SIZE) {\n                \tprintk(KERN_ALERT \"Unable to handle kernel NULL \"\n\t\t\t       \"pointer dereference in mna handler\");\n        \t} else\n                \tprintk(KERN_ALERT \"Unable to handle kernel paging \"\n\t\t\t       \"request in mna handler\");\n\t        printk(KERN_ALERT \" at virtual address %016lx\\n\",address);\n\t\tprintk(KERN_ALERT \"current->{active_,}mm->context = %016lx\\n\",\n\t\t\t(current->mm ? CTX_HWBITS(current->mm->context) :\n\t\t\tCTX_HWBITS(current->active_mm->context)));\n\t\tprintk(KERN_ALERT \"current->{active_,}mm->pgd = %016lx\\n\",\n\t\t\t(current->mm ? (unsigned long) current->mm->pgd :\n\t\t\t(unsigned long) current->active_mm->pgd));\n\t        die_if_kernel(\"Oops\", regs);\n\t\t/* Not reached */\n\t}\n\tregs->tpc = entry->fixup;\n\tregs->tnpc = regs->tpc + 4;\n\n\tif (fixup_tstate_asi) {\n\t\tregs->tstate &= ~TSTATE_ASI;\n\t\tregs->tstate |= (ASI_AIUS << 24UL);\n\t}\n}\n\nstatic void log_unaligned(struct pt_regs *regs)\n{\n\tstatic DEFINE_RATELIMIT_STATE(ratelimit, 5 * HZ, 5);\n\n\tif (__ratelimit(&ratelimit)) {\n\t\tprintk(\"Kernel unaligned access at TPC[%lx] %pS\\n\",\n\t\t       regs->tpc, (void *) regs->tpc);\n\t}\n}\n\nasmlinkage void kernel_unaligned_trap(struct pt_regs *regs, unsigned int insn)\n{\n\tenum direction dir = decode_direction(insn);\n\tint size = decode_access_size(regs, insn);\n\tint orig_asi, asi;\n\n\tcurrent_thread_info()->kern_una_regs = regs;\n\tcurrent_thread_info()->kern_una_insn = insn;\n\n\torig_asi = asi = decode_asi(insn, regs);\n\n\t/* If this is a {get,put}_user() on an unaligned userspace pointer,\n\t * just signal a fault and do not log the event.\n\t */\n\tif (asi == ASI_AIUS) {\n\t\tkernel_mna_trap_fault(0);\n\t\treturn;\n\t}\n\n\tlog_unaligned(regs);\n\n\tif (!ok_for_kernel(insn) || dir == both) {\n\t\tprintk(\"Unsupported unaligned load/store trap for kernel \"\n\t\t       \"at <%016lx>.\\n\", regs->tpc);\n\t\tunaligned_panic(\"Kernel does fpu/atomic \"\n\t\t\t\t\"unaligned load/store.\", regs);\n\n\t\tkernel_mna_trap_fault(0);\n\t} else {\n\t\tunsigned long addr, *reg_addr;\n\t\tint err;\n\n\t\taddr = compute_effective_address(regs, insn,\n\t\t\t\t\t\t ((insn >> 25) & 0x1f));\n\t\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, addr);\n\t\tswitch (asi) {\n\t\tcase ASI_NL:\n\t\tcase ASI_AIUPL:\n\t\tcase ASI_AIUSL:\n\t\tcase ASI_PL:\n\t\tcase ASI_SL:\n\t\tcase ASI_PNFL:\n\t\tcase ASI_SNFL:\n\t\t\tasi &= ~0x08;\n\t\t\tbreak;\n\t\t}\n\t\tswitch (dir) {\n\t\tcase load:\n\t\t\treg_addr = fetch_reg_addr(((insn>>25)&0x1f), regs);\n\t\t\terr = do_int_load(reg_addr, size,\n\t\t\t\t\t  (unsigned long *) addr,\n\t\t\t\t\t  decode_signedness(insn), asi);\n\t\t\tif (likely(!err) && unlikely(asi != orig_asi)) {\n\t\t\t\tunsigned long val_in = *reg_addr;\n\t\t\t\tswitch (size) {\n\t\t\t\tcase 2:\n\t\t\t\t\tval_in = swab16(val_in);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 4:\n\t\t\t\t\tval_in = swab32(val_in);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 8:\n\t\t\t\t\tval_in = swab64(val_in);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 16:\n\t\t\t\tdefault:\n\t\t\t\t\tBUG();\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\t*reg_addr = val_in;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase store:\n\t\t\terr = do_int_store(((insn>>25)&0x1f), size,\n\t\t\t\t\t   (unsigned long *) addr, regs,\n\t\t\t\t\t   asi, orig_asi);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tpanic(\"Impossible kernel unaligned trap.\");\n\t\t\t/* Not reached... */\n\t\t}\n\t\tif (unlikely(err))\n\t\t\tkernel_mna_trap_fault(1);\n\t\telse\n\t\t\tadvance(regs);\n\t}\n}\n\nstatic char popc_helper[] = {\n0, 1, 1, 2, 1, 2, 2, 3,\n1, 2, 2, 3, 2, 3, 3, 4, \n};\n\nint handle_popc(u32 insn, struct pt_regs *regs)\n{\n\tu64 value;\n\tint ret, i, rd = ((insn >> 25) & 0x1f);\n\tint from_kernel = (regs->tstate & TSTATE_PRIV) != 0;\n\t                        \n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);\n\tif (insn & 0x2000) {\n\t\tmaybe_flush_windows(0, 0, rd, from_kernel);\n\t\tvalue = sign_extend_imm13(insn);\n\t} else {\n\t\tmaybe_flush_windows(0, insn & 0x1f, rd, from_kernel);\n\t\tvalue = fetch_reg(insn & 0x1f, regs);\n\t}\n\tfor (ret = 0, i = 0; i < 16; i++) {\n\t\tret += popc_helper[value & 0xf];\n\t\tvalue >>= 4;\n\t}\n\tif (rd < 16) {\n\t\tif (rd)\n\t\t\tregs->u_regs[rd] = ret;\n\t} else {\n\t\tif (test_thread_flag(TIF_32BIT)) {\n\t\t\tstruct reg_window32 __user *win32;\n\t\t\twin32 = (struct reg_window32 __user *)((unsigned long)((u32)regs->u_regs[UREG_FP]));\n\t\t\tput_user(ret, &win32->locals[rd - 16]);\n\t\t} else {\n\t\t\tstruct reg_window __user *win;\n\t\t\twin = (struct reg_window __user *)(regs->u_regs[UREG_FP] + STACK_BIAS);\n\t\t\tput_user(ret, &win->locals[rd - 16]);\n\t\t}\n\t}\n\tadvance(regs);\n\treturn 1;\n}\n\nextern void do_fpother(struct pt_regs *regs);\nextern void do_privact(struct pt_regs *regs);\nextern void spitfire_data_access_exception(struct pt_regs *regs,\n\t\t\t\t\t   unsigned long sfsr,\n\t\t\t\t\t   unsigned long sfar);\nextern void sun4v_data_access_exception(struct pt_regs *regs,\n\t\t\t\t\tunsigned long addr,\n\t\t\t\t\tunsigned long type_ctx);\n\nint handle_ldf_stq(u32 insn, struct pt_regs *regs)\n{\n\tunsigned long addr = compute_effective_address(regs, insn, 0);\n\tint freg = ((insn >> 25) & 0x1e) | ((insn >> 20) & 0x20);\n\tstruct fpustate *f = FPUSTATE;\n\tint asi = decode_asi(insn, regs);\n\tint flag = (freg < 32) ? FPRS_DL : FPRS_DU;\n\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);\n\n\tsave_and_clear_fpu();\n\tcurrent_thread_info()->xfsr[0] &= ~0x1c000;\n\tif (freg & 3) {\n\t\tcurrent_thread_info()->xfsr[0] |= (6 << 14) /* invalid_fp_register */;\n\t\tdo_fpother(regs);\n\t\treturn 0;\n\t}\n\tif (insn & 0x200000) {\n\t\t/* STQ */\n\t\tu64 first = 0, second = 0;\n\t\t\n\t\tif (current_thread_info()->fpsaved[0] & flag) {\n\t\t\tfirst = *(u64 *)&f->regs[freg];\n\t\t\tsecond = *(u64 *)&f->regs[freg+2];\n\t\t}\n\t\tif (asi < 0x80) {\n\t\t\tdo_privact(regs);\n\t\t\treturn 1;\n\t\t}\n\t\tswitch (asi) {\n\t\tcase ASI_P:\n\t\tcase ASI_S: break;\n\t\tcase ASI_PL:\n\t\tcase ASI_SL: \n\t\t\t{\n\t\t\t\t/* Need to convert endians */\n\t\t\t\tu64 tmp = __swab64p(&first);\n\t\t\t\t\n\t\t\t\tfirst = __swab64p(&second);\n\t\t\t\tsecond = tmp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tdefault:\n\t\t\tif (tlb_type == hypervisor)\n\t\t\t\tsun4v_data_access_exception(regs, addr, 0);\n\t\t\telse\n\t\t\t\tspitfire_data_access_exception(regs, 0, addr);\n\t\t\treturn 1;\n\t\t}\n\t\tif (put_user (first >> 32, (u32 __user *)addr) ||\n\t\t    __put_user ((u32)first, (u32 __user *)(addr + 4)) ||\n\t\t    __put_user (second >> 32, (u32 __user *)(addr + 8)) ||\n\t\t    __put_user ((u32)second, (u32 __user *)(addr + 12))) {\n\t\t\tif (tlb_type == hypervisor)\n\t\t\t\tsun4v_data_access_exception(regs, addr, 0);\n\t\t\telse\n\t\t\t\tspitfire_data_access_exception(regs, 0, addr);\n\t\t    \treturn 1;\n\t\t}\n\t} else {\n\t\t/* LDF, LDDF, LDQF */\n\t\tu32 data[4] __attribute__ ((aligned(8)));\n\t\tint size, i;\n\t\tint err;\n\n\t\tif (asi < 0x80) {\n\t\t\tdo_privact(regs);\n\t\t\treturn 1;\n\t\t} else if (asi > ASI_SNFL) {\n\t\t\tif (tlb_type == hypervisor)\n\t\t\t\tsun4v_data_access_exception(regs, addr, 0);\n\t\t\telse\n\t\t\t\tspitfire_data_access_exception(regs, 0, addr);\n\t\t\treturn 1;\n\t\t}\n\t\tswitch (insn & 0x180000) {\n\t\tcase 0x000000: size = 1; break;\n\t\tcase 0x100000: size = 4; break;\n\t\tdefault: size = 2; break;\n\t\t}\n\t\tfor (i = 0; i < size; i++)\n\t\t\tdata[i] = 0;\n\t\t\n\t\terr = get_user (data[0], (u32 __user *) addr);\n\t\tif (!err) {\n\t\t\tfor (i = 1; i < size; i++)\n\t\t\t\terr |= __get_user (data[i], (u32 __user *)(addr + 4*i));\n\t\t}\n\t\tif (err && !(asi & 0x2 /* NF */)) {\n\t\t\tif (tlb_type == hypervisor)\n\t\t\t\tsun4v_data_access_exception(regs, addr, 0);\n\t\t\telse\n\t\t\t\tspitfire_data_access_exception(regs, 0, addr);\n\t\t\treturn 1;\n\t\t}\n\t\tif (asi & 0x8) /* Little */ {\n\t\t\tu64 tmp;\n\n\t\t\tswitch (size) {\n\t\t\tcase 1: data[0] = le32_to_cpup(data + 0); break;\n\t\t\tdefault:*(u64 *)(data + 0) = le64_to_cpup((u64 *)(data + 0));\n\t\t\t\tbreak;\n\t\t\tcase 4: tmp = le64_to_cpup((u64 *)(data + 0));\n\t\t\t\t*(u64 *)(data + 0) = le64_to_cpup((u64 *)(data + 2));\n\t\t\t\t*(u64 *)(data + 2) = tmp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!(current_thread_info()->fpsaved[0] & FPRS_FEF)) {\n\t\t\tcurrent_thread_info()->fpsaved[0] = FPRS_FEF;\n\t\t\tcurrent_thread_info()->gsr[0] = 0;\n\t\t}\n\t\tif (!(current_thread_info()->fpsaved[0] & flag)) {\n\t\t\tif (freg < 32)\n\t\t\t\tmemset(f->regs, 0, 32*sizeof(u32));\n\t\t\telse\n\t\t\t\tmemset(f->regs+32, 0, 32*sizeof(u32));\n\t\t}\n\t\tmemcpy(f->regs + freg, data, size * 4);\n\t\tcurrent_thread_info()->fpsaved[0] |= flag;\n\t}\n\tadvance(regs);\n\treturn 1;\n}\n\nvoid handle_ld_nf(u32 insn, struct pt_regs *regs)\n{\n\tint rd = ((insn >> 25) & 0x1f);\n\tint from_kernel = (regs->tstate & TSTATE_PRIV) != 0;\n\tunsigned long *reg;\n\t                        \n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);\n\n\tmaybe_flush_windows(0, 0, rd, from_kernel);\n\treg = fetch_reg_addr(rd, regs);\n\tif (from_kernel || rd < 16) {\n\t\treg[0] = 0;\n\t\tif ((insn & 0x780000) == 0x180000)\n\t\t\treg[1] = 0;\n\t} else if (test_thread_flag(TIF_32BIT)) {\n\t\tput_user(0, (int __user *) reg);\n\t\tif ((insn & 0x780000) == 0x180000)\n\t\t\tput_user(0, ((int __user *) reg) + 1);\n\t} else {\n\t\tput_user(0, (unsigned long __user *) reg);\n\t\tif ((insn & 0x780000) == 0x180000)\n\t\t\tput_user(0, (unsigned long __user *) reg + 1);\n\t}\n\tadvance(regs);\n}\n\nvoid handle_lddfmna(struct pt_regs *regs, unsigned long sfar, unsigned long sfsr)\n{\n\tunsigned long pc = regs->tpc;\n\tunsigned long tstate = regs->tstate;\n\tu32 insn;\n\tu64 value;\n\tu8 freg;\n\tint flag;\n\tstruct fpustate *f = FPUSTATE;\n\n\tif (tstate & TSTATE_PRIV)\n\t\tdie_if_kernel(\"lddfmna from kernel\", regs);\n\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, sfar);\n\tif (test_thread_flag(TIF_32BIT))\n\t\tpc = (u32)pc;\n\tif (get_user(insn, (u32 __user *) pc) != -EFAULT) {\n\t\tint asi = decode_asi(insn, regs);\n\t\tu32 first, second;\n\t\tint err;\n\n\t\tif ((asi > ASI_SNFL) ||\n\t\t    (asi < ASI_P))\n\t\t\tgoto daex;\n\t\tfirst = second = 0;\n\t\terr = get_user(first, (u32 __user *)sfar);\n\t\tif (!err)\n\t\t\terr = get_user(second, (u32 __user *)(sfar + 4));\n\t\tif (err) {\n\t\t\tif (!(asi & 0x2))\n\t\t\t\tgoto daex;\n\t\t\tfirst = second = 0;\n\t\t}\n\t\tsave_and_clear_fpu();\n\t\tfreg = ((insn >> 25) & 0x1e) | ((insn >> 20) & 0x20);\n\t\tvalue = (((u64)first) << 32) | second;\n\t\tif (asi & 0x8) /* Little */\n\t\t\tvalue = __swab64p(&value);\n\t\tflag = (freg < 32) ? FPRS_DL : FPRS_DU;\n\t\tif (!(current_thread_info()->fpsaved[0] & FPRS_FEF)) {\n\t\t\tcurrent_thread_info()->fpsaved[0] = FPRS_FEF;\n\t\t\tcurrent_thread_info()->gsr[0] = 0;\n\t\t}\n\t\tif (!(current_thread_info()->fpsaved[0] & flag)) {\n\t\t\tif (freg < 32)\n\t\t\t\tmemset(f->regs, 0, 32*sizeof(u32));\n\t\t\telse\n\t\t\t\tmemset(f->regs+32, 0, 32*sizeof(u32));\n\t\t}\n\t\t*(u64 *)(f->regs + freg) = value;\n\t\tcurrent_thread_info()->fpsaved[0] |= flag;\n\t} else {\ndaex:\n\t\tif (tlb_type == hypervisor)\n\t\t\tsun4v_data_access_exception(regs, sfar, sfsr);\n\t\telse\n\t\t\tspitfire_data_access_exception(regs, sfsr, sfar);\n\t\treturn;\n\t}\n\tadvance(regs);\n}\n\nvoid handle_stdfmna(struct pt_regs *regs, unsigned long sfar, unsigned long sfsr)\n{\n\tunsigned long pc = regs->tpc;\n\tunsigned long tstate = regs->tstate;\n\tu32 insn;\n\tu64 value;\n\tu8 freg;\n\tint flag;\n\tstruct fpustate *f = FPUSTATE;\n\n\tif (tstate & TSTATE_PRIV)\n\t\tdie_if_kernel(\"stdfmna from kernel\", regs);\n\tperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, sfar);\n\tif (test_thread_flag(TIF_32BIT))\n\t\tpc = (u32)pc;\n\tif (get_user(insn, (u32 __user *) pc) != -EFAULT) {\n\t\tint asi = decode_asi(insn, regs);\n\t\tfreg = ((insn >> 25) & 0x1e) | ((insn >> 20) & 0x20);\n\t\tvalue = 0;\n\t\tflag = (freg < 32) ? FPRS_DL : FPRS_DU;\n\t\tif ((asi > ASI_SNFL) ||\n\t\t    (asi < ASI_P))\n\t\t\tgoto daex;\n\t\tsave_and_clear_fpu();\n\t\tif (current_thread_info()->fpsaved[0] & flag)\n\t\t\tvalue = *(u64 *)&f->regs[freg];\n\t\tswitch (asi) {\n\t\tcase ASI_P:\n\t\tcase ASI_S: break;\n\t\tcase ASI_PL:\n\t\tcase ASI_SL: \n\t\t\tvalue = __swab64p(&value); break;\n\t\tdefault: goto daex;\n\t\t}\n\t\tif (put_user (value >> 32, (u32 __user *) sfar) ||\n\t\t    __put_user ((u32)value, (u32 __user *)(sfar + 4)))\n\t\t\tgoto daex;\n\t} else {\ndaex:\n\t\tif (tlb_type == hypervisor)\n\t\t\tsun4v_data_access_exception(regs, sfar, sfsr);\n\t\telse\n\t\t\tspitfire_data_access_exception(regs, sfsr, sfar);\n\t\treturn;\n\t}\n\tadvance(regs);\n}\n", "/* visemul.c: Emulation of VIS instructions.\n *\n * Copyright (C) 2006 David S. Miller (davem@davemloft.net)\n */\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/thread_info.h>\n#include <linux/perf_event.h>\n\n#include <asm/ptrace.h>\n#include <asm/pstate.h>\n#include <asm/system.h>\n#include <asm/fpumacro.h>\n#include <asm/uaccess.h>\n\n/* OPF field of various VIS instructions.  */\n\n/* 000111011 - four 16-bit packs  */\n#define FPACK16_OPF\t0x03b\n\n/* 000111010 - two 32-bit packs  */\n#define FPACK32_OPF\t0x03a\n\n/* 000111101 - four 16-bit packs  */\n#define FPACKFIX_OPF\t0x03d\n\n/* 001001101 - four 16-bit expands  */\n#define FEXPAND_OPF\t0x04d\n\n/* 001001011 - two 32-bit merges */\n#define FPMERGE_OPF\t0x04b\n\n/* 000110001 - 8-by-16-bit partitoned product  */\n#define FMUL8x16_OPF\t0x031\n\n/* 000110011 - 8-by-16-bit upper alpha partitioned product  */\n#define FMUL8x16AU_OPF\t0x033\n\n/* 000110101 - 8-by-16-bit lower alpha partitioned product  */\n#define FMUL8x16AL_OPF\t0x035\n\n/* 000110110 - upper 8-by-16-bit partitioned product  */\n#define FMUL8SUx16_OPF\t0x036\n\n/* 000110111 - lower 8-by-16-bit partitioned product  */\n#define FMUL8ULx16_OPF\t0x037\n\n/* 000111000 - upper 8-by-16-bit partitioned product  */\n#define FMULD8SUx16_OPF\t0x038\n\n/* 000111001 - lower unsigned 8-by-16-bit partitioned product  */\n#define FMULD8ULx16_OPF\t0x039\n\n/* 000101000 - four 16-bit compare; set rd if src1 > src2  */\n#define FCMPGT16_OPF\t0x028\n\n/* 000101100 - two 32-bit compare; set rd if src1 > src2  */\n#define FCMPGT32_OPF\t0x02c\n\n/* 000100000 - four 16-bit compare; set rd if src1 <= src2  */\n#define FCMPLE16_OPF\t0x020\n\n/* 000100100 - two 32-bit compare; set rd if src1 <= src2  */\n#define FCMPLE32_OPF\t0x024\n\n/* 000100010 - four 16-bit compare; set rd if src1 != src2  */\n#define FCMPNE16_OPF\t0x022\n\n/* 000100110 - two 32-bit compare; set rd if src1 != src2  */\n#define FCMPNE32_OPF\t0x026\n\n/* 000101010 - four 16-bit compare; set rd if src1 == src2  */\n#define FCMPEQ16_OPF\t0x02a\n\n/* 000101110 - two 32-bit compare; set rd if src1 == src2  */\n#define FCMPEQ32_OPF\t0x02e\n\n/* 000000000 - Eight 8-bit edge boundary processing  */\n#define EDGE8_OPF\t0x000\n\n/* 000000001 - Eight 8-bit edge boundary processing, no CC */\n#define EDGE8N_OPF\t0x001\n\n/* 000000010 - Eight 8-bit edge boundary processing, little-endian  */\n#define EDGE8L_OPF\t0x002\n\n/* 000000011 - Eight 8-bit edge boundary processing, little-endian, no CC  */\n#define EDGE8LN_OPF\t0x003\n\n/* 000000100 - Four 16-bit edge boundary processing  */\n#define EDGE16_OPF\t0x004\n\n/* 000000101 - Four 16-bit edge boundary processing, no CC  */\n#define EDGE16N_OPF\t0x005\n\n/* 000000110 - Four 16-bit edge boundary processing, little-endian  */\n#define EDGE16L_OPF\t0x006\n\n/* 000000111 - Four 16-bit edge boundary processing, little-endian, no CC  */\n#define EDGE16LN_OPF\t0x007\n\n/* 000001000 - Two 32-bit edge boundary processing  */\n#define EDGE32_OPF\t0x008\n\n/* 000001001 - Two 32-bit edge boundary processing, no CC  */\n#define EDGE32N_OPF\t0x009\n\n/* 000001010 - Two 32-bit edge boundary processing, little-endian  */\n#define EDGE32L_OPF\t0x00a\n\n/* 000001011 - Two 32-bit edge boundary processing, little-endian, no CC  */\n#define EDGE32LN_OPF\t0x00b\n\n/* 000111110 - distance between 8 8-bit components  */\n#define PDIST_OPF\t0x03e\n\n/* 000010000 - convert 8-bit 3-D address to blocked byte address  */\n#define ARRAY8_OPF\t0x010\n\n/* 000010010 - convert 16-bit 3-D address to blocked byte address  */\n#define ARRAY16_OPF\t0x012\n\n/* 000010100 - convert 32-bit 3-D address to blocked byte address  */\n#define ARRAY32_OPF\t0x014\n\n/* 000011001 - Set the GSR.MASK field in preparation for a BSHUFFLE  */\n#define BMASK_OPF\t0x019\n\n/* 001001100 - Permute bytes as specified by GSR.MASK  */\n#define BSHUFFLE_OPF\t0x04c\n\n#define VIS_OPF_SHIFT\t5\n#define VIS_OPF_MASK\t(0x1ff << VIS_OPF_SHIFT)\n\n#define RS1(INSN)\t(((INSN) >> 14) & 0x1f)\n#define RS2(INSN)\t(((INSN) >>  0) & 0x1f)\n#define RD(INSN)\t(((INSN) >> 25) & 0x1f)\n\nstatic inline void maybe_flush_windows(unsigned int rs1, unsigned int rs2,\n\t\t\t\t       unsigned int rd, int from_kernel)\n{\n\tif (rs2 >= 16 || rs1 >= 16 || rd >= 16) {\n\t\tif (from_kernel != 0)\n\t\t\t__asm__ __volatile__(\"flushw\");\n\t\telse\n\t\t\tflushw_user();\n\t}\n}\n\nstatic unsigned long fetch_reg(unsigned int reg, struct pt_regs *regs)\n{\n\tunsigned long value;\n\t\n\tif (reg < 16)\n\t\treturn (!reg ? 0 : regs->u_regs[reg]);\n\tif (regs->tstate & TSTATE_PRIV) {\n\t\tstruct reg_window *win;\n\t\twin = (struct reg_window *)(regs->u_regs[UREG_FP] + STACK_BIAS);\n\t\tvalue = win->locals[reg - 16];\n\t} else if (test_thread_flag(TIF_32BIT)) {\n\t\tstruct reg_window32 __user *win32;\n\t\twin32 = (struct reg_window32 __user *)((unsigned long)((u32)regs->u_regs[UREG_FP]));\n\t\tget_user(value, &win32->locals[reg - 16]);\n\t} else {\n\t\tstruct reg_window __user *win;\n\t\twin = (struct reg_window __user *)(regs->u_regs[UREG_FP] + STACK_BIAS);\n\t\tget_user(value, &win->locals[reg - 16]);\n\t}\n\treturn value;\n}\n\nstatic inline unsigned long __user *__fetch_reg_addr_user(unsigned int reg,\n\t\t\t\t\t\t\t  struct pt_regs *regs)\n{\n\tBUG_ON(reg < 16);\n\tBUG_ON(regs->tstate & TSTATE_PRIV);\n\n\tif (test_thread_flag(TIF_32BIT)) {\n\t\tstruct reg_window32 __user *win32;\n\t\twin32 = (struct reg_window32 __user *)((unsigned long)((u32)regs->u_regs[UREG_FP]));\n\t\treturn (unsigned long __user *)&win32->locals[reg - 16];\n\t} else {\n\t\tstruct reg_window __user *win;\n\t\twin = (struct reg_window __user *)(regs->u_regs[UREG_FP] + STACK_BIAS);\n\t\treturn &win->locals[reg - 16];\n\t}\n}\n\nstatic inline unsigned long *__fetch_reg_addr_kern(unsigned int reg,\n\t\t\t\t\t\t   struct pt_regs *regs)\n{\n\tBUG_ON(reg >= 16);\n\tBUG_ON(regs->tstate & TSTATE_PRIV);\n\n\treturn &regs->u_regs[reg];\n}\n\nstatic void store_reg(struct pt_regs *regs, unsigned long val, unsigned long rd)\n{\n\tif (rd < 16) {\n\t\tunsigned long *rd_kern = __fetch_reg_addr_kern(rd, regs);\n\n\t\t*rd_kern = val;\n\t} else {\n\t\tunsigned long __user *rd_user = __fetch_reg_addr_user(rd, regs);\n\n\t\tif (test_thread_flag(TIF_32BIT))\n\t\t\t__put_user((u32)val, (u32 __user *)rd_user);\n\t\telse\n\t\t\t__put_user(val, rd_user);\n\t}\n}\n\nstatic inline unsigned long fpd_regval(struct fpustate *f,\n\t\t\t\t       unsigned int insn_regnum)\n{\n\tinsn_regnum = (((insn_regnum & 1) << 5) |\n\t\t       (insn_regnum & 0x1e));\n\n\treturn *(unsigned long *) &f->regs[insn_regnum];\n}\n\nstatic inline unsigned long *fpd_regaddr(struct fpustate *f,\n\t\t\t\t\t unsigned int insn_regnum)\n{\n\tinsn_regnum = (((insn_regnum & 1) << 5) |\n\t\t       (insn_regnum & 0x1e));\n\n\treturn (unsigned long *) &f->regs[insn_regnum];\n}\n\nstatic inline unsigned int fps_regval(struct fpustate *f,\n\t\t\t\t      unsigned int insn_regnum)\n{\n\treturn f->regs[insn_regnum];\n}\n\nstatic inline unsigned int *fps_regaddr(struct fpustate *f,\n\t\t\t\t\tunsigned int insn_regnum)\n{\n\treturn &f->regs[insn_regnum];\n}\n\nstruct edge_tab {\n\tu16 left, right;\n};\nstatic struct edge_tab edge8_tab[8] = {\n\t{ 0xff, 0x80 },\n\t{ 0x7f, 0xc0 },\n\t{ 0x3f, 0xe0 },\n\t{ 0x1f, 0xf0 },\n\t{ 0x0f, 0xf8 },\n\t{ 0x07, 0xfc },\n\t{ 0x03, 0xfe },\n\t{ 0x01, 0xff },\n};\nstatic struct edge_tab edge8_tab_l[8] = {\n\t{ 0xff, 0x01 },\n\t{ 0xfe, 0x03 },\n\t{ 0xfc, 0x07 },\n\t{ 0xf8, 0x0f },\n\t{ 0xf0, 0x1f },\n\t{ 0xe0, 0x3f },\n\t{ 0xc0, 0x7f },\n\t{ 0x80, 0xff },\n};\nstatic struct edge_tab edge16_tab[4] = {\n\t{ 0xf, 0x8 },\n\t{ 0x7, 0xc },\n\t{ 0x3, 0xe },\n\t{ 0x1, 0xf },\n};\nstatic struct edge_tab edge16_tab_l[4] = {\n\t{ 0xf, 0x1 },\n\t{ 0xe, 0x3 },\n\t{ 0xc, 0x7 },\n\t{ 0x8, 0xf },\n};\nstatic struct edge_tab edge32_tab[2] = {\n\t{ 0x3, 0x2 },\n\t{ 0x1, 0x3 },\n};\nstatic struct edge_tab edge32_tab_l[2] = {\n\t{ 0x3, 0x1 },\n\t{ 0x2, 0x3 },\n};\n\nstatic void edge(struct pt_regs *regs, unsigned int insn, unsigned int opf)\n{\n\tunsigned long orig_rs1, rs1, orig_rs2, rs2, rd_val;\n\tu16 left, right;\n\n\tmaybe_flush_windows(RS1(insn), RS2(insn), RD(insn), 0);\n\torig_rs1 = rs1 = fetch_reg(RS1(insn), regs);\n\torig_rs2 = rs2 = fetch_reg(RS2(insn), regs);\n\n\tif (test_thread_flag(TIF_32BIT)) {\n\t\trs1 = rs1 & 0xffffffff;\n\t\trs2 = rs2 & 0xffffffff;\n\t}\n\tswitch (opf) {\n\tdefault:\n\tcase EDGE8_OPF:\n\tcase EDGE8N_OPF:\n\t\tleft = edge8_tab[rs1 & 0x7].left;\n\t\tright = edge8_tab[rs2 & 0x7].right;\n\t\tbreak;\n\tcase EDGE8L_OPF:\n\tcase EDGE8LN_OPF:\n\t\tleft = edge8_tab_l[rs1 & 0x7].left;\n\t\tright = edge8_tab_l[rs2 & 0x7].right;\n\t\tbreak;\n\n\tcase EDGE16_OPF:\n\tcase EDGE16N_OPF:\n\t\tleft = edge16_tab[(rs1 >> 1) & 0x3].left;\n\t\tright = edge16_tab[(rs2 >> 1) & 0x3].right;\n\t\tbreak;\n\n\tcase EDGE16L_OPF:\n\tcase EDGE16LN_OPF:\n\t\tleft = edge16_tab_l[(rs1 >> 1) & 0x3].left;\n\t\tright = edge16_tab_l[(rs2 >> 1) & 0x3].right;\n\t\tbreak;\n\n\tcase EDGE32_OPF:\n\tcase EDGE32N_OPF:\n\t\tleft = edge32_tab[(rs1 >> 2) & 0x1].left;\n\t\tright = edge32_tab[(rs2 >> 2) & 0x1].right;\n\t\tbreak;\n\n\tcase EDGE32L_OPF:\n\tcase EDGE32LN_OPF:\n\t\tleft = edge32_tab_l[(rs1 >> 2) & 0x1].left;\n\t\tright = edge32_tab_l[(rs2 >> 2) & 0x1].right;\n\t\tbreak;\n\t}\n\n\tif ((rs1 & ~0x7UL) == (rs2 & ~0x7UL))\n\t\trd_val = right & left;\n\telse\n\t\trd_val = left;\n\n\tstore_reg(regs, rd_val, RD(insn));\n\n\tswitch (opf) {\n\tcase EDGE8_OPF:\n\tcase EDGE8L_OPF:\n\tcase EDGE16_OPF:\n\tcase EDGE16L_OPF:\n\tcase EDGE32_OPF:\n\tcase EDGE32L_OPF: {\n\t\tunsigned long ccr, tstate;\n\n\t\t__asm__ __volatile__(\"subcc\t%1, %2, %%g0\\n\\t\"\n\t\t\t\t     \"rd\t%%ccr, %0\"\n\t\t\t\t     : \"=r\" (ccr)\n\t\t\t\t     : \"r\" (orig_rs1), \"r\" (orig_rs2)\n\t\t\t\t     : \"cc\");\n\t\ttstate = regs->tstate & ~(TSTATE_XCC | TSTATE_ICC);\n\t\tregs->tstate = tstate | (ccr << 32UL);\n\t}\n\t}\n}\n\nstatic void array(struct pt_regs *regs, unsigned int insn, unsigned int opf)\n{\n\tunsigned long rs1, rs2, rd_val;\n\tunsigned int bits, bits_mask;\n\n\tmaybe_flush_windows(RS1(insn), RS2(insn), RD(insn), 0);\n\trs1 = fetch_reg(RS1(insn), regs);\n\trs2 = fetch_reg(RS2(insn), regs);\n\n\tbits = (rs2 > 5 ? 5 : rs2);\n\tbits_mask = (1UL << bits) - 1UL;\n\n\trd_val = ((((rs1 >> 11) & 0x3) <<  0) |\n\t\t  (((rs1 >> 33) & 0x3) <<  2) |\n\t\t  (((rs1 >> 55) & 0x1) <<  4) |\n\t\t  (((rs1 >> 13) & 0xf) <<  5) |\n\t\t  (((rs1 >> 35) & 0xf) <<  9) |\n\t\t  (((rs1 >> 56) & 0xf) << 13) |\n\t\t  (((rs1 >> 17) & bits_mask) << 17) |\n\t\t  (((rs1 >> 39) & bits_mask) << (17 + bits)) |\n\t\t  (((rs1 >> 60) & 0xf)       << (17 + (2*bits))));\n\n\tswitch (opf) {\n\tcase ARRAY16_OPF:\n\t\trd_val <<= 1;\n\t\tbreak;\n\n\tcase ARRAY32_OPF:\n\t\trd_val <<= 2;\n\t}\n\n\tstore_reg(regs, rd_val, RD(insn));\n}\n\nstatic void bmask(struct pt_regs *regs, unsigned int insn)\n{\n\tunsigned long rs1, rs2, rd_val, gsr;\n\n\tmaybe_flush_windows(RS1(insn), RS2(insn), RD(insn), 0);\n\trs1 = fetch_reg(RS1(insn), regs);\n\trs2 = fetch_reg(RS2(insn), regs);\n\trd_val = rs1 + rs2;\n\n\tstore_reg(regs, rd_val, RD(insn));\n\n\tgsr = current_thread_info()->gsr[0] & 0xffffffff;\n\tgsr |= rd_val << 32UL;\n\tcurrent_thread_info()->gsr[0] = gsr;\n}\n\nstatic void bshuffle(struct pt_regs *regs, unsigned int insn)\n{\n\tstruct fpustate *f = FPUSTATE;\n\tunsigned long rs1, rs2, rd_val;\n\tunsigned long bmask, i;\n\n\tbmask = current_thread_info()->gsr[0] >> 32UL;\n\n\trs1 = fpd_regval(f, RS1(insn));\n\trs2 = fpd_regval(f, RS2(insn));\n\n\trd_val = 0UL;\n\tfor (i = 0; i < 8; i++) {\n\t\tunsigned long which = (bmask >> (i * 4)) & 0xf;\n\t\tunsigned long byte;\n\n\t\tif (which < 8)\n\t\t\tbyte = (rs1 >> (which * 8)) & 0xff;\n\t\telse\n\t\t\tbyte = (rs2 >> ((which-8)*8)) & 0xff;\n\t\trd_val |= (byte << (i * 8));\n\t}\n\n\t*fpd_regaddr(f, RD(insn)) = rd_val;\n}\n\nstatic void pdist(struct pt_regs *regs, unsigned int insn)\n{\n\tstruct fpustate *f = FPUSTATE;\n\tunsigned long rs1, rs2, *rd, rd_val;\n\tunsigned long i;\n\n\trs1 = fpd_regval(f, RS1(insn));\n\trs2 = fpd_regval(f, RS2(insn));\n\trd = fpd_regaddr(f, RD(insn));\n\n\trd_val = *rd;\n\n\tfor (i = 0; i < 8; i++) {\n\t\ts16 s1, s2;\n\n\t\ts1 = (rs1 >> (56 - (i * 8))) & 0xff;\n\t\ts2 = (rs2 >> (56 - (i * 8))) & 0xff;\n\n\t\t/* Absolute value of difference. */\n\t\ts1 -= s2;\n\t\tif (s1 < 0)\n\t\t\ts1 = ~s1 + 1;\n\n\t\trd_val += s1;\n\t}\n\n\t*rd = rd_val;\n}\n\nstatic void pformat(struct pt_regs *regs, unsigned int insn, unsigned int opf)\n{\n\tstruct fpustate *f = FPUSTATE;\n\tunsigned long rs1, rs2, gsr, scale, rd_val;\n\n\tgsr = current_thread_info()->gsr[0];\n\tscale = (gsr >> 3) & (opf == FPACK16_OPF ? 0xf : 0x1f);\n\tswitch (opf) {\n\tcase FPACK16_OPF: {\n\t\tunsigned long byte;\n\n\t\trs2 = fpd_regval(f, RS2(insn));\n\t\trd_val = 0;\n\t\tfor (byte = 0; byte < 4; byte++) {\n\t\t\tunsigned int val;\n\t\t\ts16 src = (rs2 >> (byte * 16UL)) & 0xffffUL;\n\t\t\tint scaled = src << scale;\n\t\t\tint from_fixed = scaled >> 7;\n\n\t\t\tval = ((from_fixed < 0) ?\n\t\t\t       0 :\n\t\t\t       (from_fixed > 255) ?\n\t\t\t       255 : from_fixed);\n\n\t\t\trd_val |= (val << (8 * byte));\n\t\t}\n\t\t*fps_regaddr(f, RD(insn)) = rd_val;\n\t\tbreak;\n\t}\n\n\tcase FPACK32_OPF: {\n\t\tunsigned long word;\n\n\t\trs1 = fpd_regval(f, RS1(insn));\n\t\trs2 = fpd_regval(f, RS2(insn));\n\t\trd_val = (rs1 << 8) & ~(0x000000ff000000ffUL);\n\t\tfor (word = 0; word < 2; word++) {\n\t\t\tunsigned long val;\n\t\t\ts32 src = (rs2 >> (word * 32UL));\n\t\t\ts64 scaled = src << scale;\n\t\t\ts64 from_fixed = scaled >> 23;\n\n\t\t\tval = ((from_fixed < 0) ?\n\t\t\t       0 :\n\t\t\t       (from_fixed > 255) ?\n\t\t\t       255 : from_fixed);\n\n\t\t\trd_val |= (val << (32 * word));\n\t\t}\n\t\t*fpd_regaddr(f, RD(insn)) = rd_val;\n\t\tbreak;\n\t}\n\n\tcase FPACKFIX_OPF: {\n\t\tunsigned long word;\n\n\t\trs2 = fpd_regval(f, RS2(insn));\n\n\t\trd_val = 0;\n\t\tfor (word = 0; word < 2; word++) {\n\t\t\tlong val;\n\t\t\ts32 src = (rs2 >> (word * 32UL));\n\t\t\ts64 scaled = src << scale;\n\t\t\ts64 from_fixed = scaled >> 16;\n\n\t\t\tval = ((from_fixed < -32768) ?\n\t\t\t       -32768 :\n\t\t\t       (from_fixed > 32767) ?\n\t\t\t       32767 : from_fixed);\n\n\t\t\trd_val |= ((val & 0xffff) << (word * 16));\n\t\t}\n\t\t*fps_regaddr(f, RD(insn)) = rd_val;\n\t\tbreak;\n\t}\n\n\tcase FEXPAND_OPF: {\n\t\tunsigned long byte;\n\n\t\trs2 = fps_regval(f, RS2(insn));\n\n\t\trd_val = 0;\n\t\tfor (byte = 0; byte < 4; byte++) {\n\t\t\tunsigned long val;\n\t\t\tu8 src = (rs2 >> (byte * 8)) & 0xff;\n\n\t\t\tval = src << 4;\n\n\t\t\trd_val |= (val << (byte * 16));\n\t\t}\n\t\t*fpd_regaddr(f, RD(insn)) = rd_val;\n\t\tbreak;\n\t}\n\n\tcase FPMERGE_OPF: {\n\t\trs1 = fps_regval(f, RS1(insn));\n\t\trs2 = fps_regval(f, RS2(insn));\n\n\t\trd_val = (((rs2 & 0x000000ff) <<  0) |\n\t\t\t  ((rs1 & 0x000000ff) <<  8) |\n\t\t\t  ((rs2 & 0x0000ff00) <<  8) |\n\t\t\t  ((rs1 & 0x0000ff00) << 16) |\n\t\t\t  ((rs2 & 0x00ff0000) << 16) |\n\t\t\t  ((rs1 & 0x00ff0000) << 24) |\n\t\t\t  ((rs2 & 0xff000000) << 24) |\n\t\t\t  ((rs1 & 0xff000000) << 32));\n\t\t*fpd_regaddr(f, RD(insn)) = rd_val;\n\t\tbreak;\n\t}\n\t}\n}\n\nstatic void pmul(struct pt_regs *regs, unsigned int insn, unsigned int opf)\n{\n\tstruct fpustate *f = FPUSTATE;\n\tunsigned long rs1, rs2, rd_val;\n\n\tswitch (opf) {\n\tcase FMUL8x16_OPF: {\n\t\tunsigned long byte;\n\n\t\trs1 = fps_regval(f, RS1(insn));\n\t\trs2 = fpd_regval(f, RS2(insn));\n\n\t\trd_val = 0;\n\t\tfor (byte = 0; byte < 4; byte++) {\n\t\t\tu16 src1 = (rs1 >> (byte *  8)) & 0x00ff;\n\t\t\ts16 src2 = (rs2 >> (byte * 16)) & 0xffff;\n\t\t\tu32 prod = src1 * src2;\n\t\t\tu16 scaled = ((prod & 0x00ffff00) >> 8);\n\n\t\t\t/* Round up.  */\n\t\t\tif (prod & 0x80)\n\t\t\t\tscaled++;\n\t\t\trd_val |= ((scaled & 0xffffUL) << (byte * 16UL));\n\t\t}\n\n\t\t*fpd_regaddr(f, RD(insn)) = rd_val;\n\t\tbreak;\n\t}\n\n\tcase FMUL8x16AU_OPF:\n\tcase FMUL8x16AL_OPF: {\n\t\tunsigned long byte;\n\t\ts16 src2;\n\n\t\trs1 = fps_regval(f, RS1(insn));\n\t\trs2 = fps_regval(f, RS2(insn));\n\n\t\trd_val = 0;\n\t\tsrc2 = rs2 >> (opf == FMUL8x16AU_OPF ? 16 : 0);\n\t\tfor (byte = 0; byte < 4; byte++) {\n\t\t\tu16 src1 = (rs1 >> (byte * 8)) & 0x00ff;\n\t\t\tu32 prod = src1 * src2;\n\t\t\tu16 scaled = ((prod & 0x00ffff00) >> 8);\n\n\t\t\t/* Round up.  */\n\t\t\tif (prod & 0x80)\n\t\t\t\tscaled++;\n\t\t\trd_val |= ((scaled & 0xffffUL) << (byte * 16UL));\n\t\t}\n\n\t\t*fpd_regaddr(f, RD(insn)) = rd_val;\n\t\tbreak;\n\t}\n\n\tcase FMUL8SUx16_OPF:\n\tcase FMUL8ULx16_OPF: {\n\t\tunsigned long byte, ushift;\n\n\t\trs1 = fpd_regval(f, RS1(insn));\n\t\trs2 = fpd_regval(f, RS2(insn));\n\n\t\trd_val = 0;\n\t\tushift = (opf == FMUL8SUx16_OPF) ? 8 : 0;\n\t\tfor (byte = 0; byte < 4; byte++) {\n\t\t\tu16 src1;\n\t\t\ts16 src2;\n\t\t\tu32 prod;\n\t\t\tu16 scaled;\n\n\t\t\tsrc1 = ((rs1 >> ((16 * byte) + ushift)) & 0x00ff);\n\t\t\tsrc2 = ((rs2 >> (16 * byte)) & 0xffff);\n\t\t\tprod = src1 * src2;\n\t\t\tscaled = ((prod & 0x00ffff00) >> 8);\n\n\t\t\t/* Round up.  */\n\t\t\tif (prod & 0x80)\n\t\t\t\tscaled++;\n\t\t\trd_val |= ((scaled & 0xffffUL) << (byte * 16UL));\n\t\t}\n\n\t\t*fpd_regaddr(f, RD(insn)) = rd_val;\n\t\tbreak;\n\t}\n\n\tcase FMULD8SUx16_OPF:\n\tcase FMULD8ULx16_OPF: {\n\t\tunsigned long byte, ushift;\n\n\t\trs1 = fps_regval(f, RS1(insn));\n\t\trs2 = fps_regval(f, RS2(insn));\n\n\t\trd_val = 0;\n\t\tushift = (opf == FMULD8SUx16_OPF) ? 8 : 0;\n\t\tfor (byte = 0; byte < 2; byte++) {\n\t\t\tu16 src1;\n\t\t\ts16 src2;\n\t\t\tu32 prod;\n\t\t\tu16 scaled;\n\n\t\t\tsrc1 = ((rs1 >> ((16 * byte) + ushift)) & 0x00ff);\n\t\t\tsrc2 = ((rs2 >> (16 * byte)) & 0xffff);\n\t\t\tprod = src1 * src2;\n\t\t\tscaled = ((prod & 0x00ffff00) >> 8);\n\n\t\t\t/* Round up.  */\n\t\t\tif (prod & 0x80)\n\t\t\t\tscaled++;\n\t\t\trd_val |= ((scaled & 0xffffUL) <<\n\t\t\t\t   ((byte * 32UL) + 7UL));\n\t\t}\n\t\t*fpd_regaddr(f, RD(insn)) = rd_val;\n\t\tbreak;\n\t}\n\t}\n}\n\nstatic void pcmp(struct pt_regs *regs, unsigned int insn, unsigned int opf)\n{\n\tstruct fpustate *f = FPUSTATE;\n\tunsigned long rs1, rs2, rd_val, i;\n\n\trs1 = fpd_regval(f, RS1(insn));\n\trs2 = fpd_regval(f, RS2(insn));\n\n\trd_val = 0;\n\n\tswitch (opf) {\n\tcase FCMPGT16_OPF:\n\t\tfor (i = 0; i < 4; i++) {\n\t\t\ts16 a = (rs1 >> (i * 16)) & 0xffff;\n\t\t\ts16 b = (rs2 >> (i * 16)) & 0xffff;\n\n\t\t\tif (a > b)\n\t\t\t\trd_val |= 1 << i;\n\t\t}\n\t\tbreak;\n\n\tcase FCMPGT32_OPF:\n\t\tfor (i = 0; i < 2; i++) {\n\t\t\ts32 a = (rs1 >> (i * 32)) & 0xffff;\n\t\t\ts32 b = (rs2 >> (i * 32)) & 0xffff;\n\n\t\t\tif (a > b)\n\t\t\t\trd_val |= 1 << i;\n\t\t}\n\t\tbreak;\n\n\tcase FCMPLE16_OPF:\n\t\tfor (i = 0; i < 4; i++) {\n\t\t\ts16 a = (rs1 >> (i * 16)) & 0xffff;\n\t\t\ts16 b = (rs2 >> (i * 16)) & 0xffff;\n\n\t\t\tif (a <= b)\n\t\t\t\trd_val |= 1 << i;\n\t\t}\n\t\tbreak;\n\n\tcase FCMPLE32_OPF:\n\t\tfor (i = 0; i < 2; i++) {\n\t\t\ts32 a = (rs1 >> (i * 32)) & 0xffff;\n\t\t\ts32 b = (rs2 >> (i * 32)) & 0xffff;\n\n\t\t\tif (a <= b)\n\t\t\t\trd_val |= 1 << i;\n\t\t}\n\t\tbreak;\n\n\tcase FCMPNE16_OPF:\n\t\tfor (i = 0; i < 4; i++) {\n\t\t\ts16 a = (rs1 >> (i * 16)) & 0xffff;\n\t\t\ts16 b = (rs2 >> (i * 16)) & 0xffff;\n\n\t\t\tif (a != b)\n\t\t\t\trd_val |= 1 << i;\n\t\t}\n\t\tbreak;\n\n\tcase FCMPNE32_OPF:\n\t\tfor (i = 0; i < 2; i++) {\n\t\t\ts32 a = (rs1 >> (i * 32)) & 0xffff;\n\t\t\ts32 b = (rs2 >> (i * 32)) & 0xffff;\n\n\t\t\tif (a != b)\n\t\t\t\trd_val |= 1 << i;\n\t\t}\n\t\tbreak;\n\n\tcase FCMPEQ16_OPF:\n\t\tfor (i = 0; i < 4; i++) {\n\t\t\ts16 a = (rs1 >> (i * 16)) & 0xffff;\n\t\t\ts16 b = (rs2 >> (i * 16)) & 0xffff;\n\n\t\t\tif (a == b)\n\t\t\t\trd_val |= 1 << i;\n\t\t}\n\t\tbreak;\n\n\tcase FCMPEQ32_OPF:\n\t\tfor (i = 0; i < 2; i++) {\n\t\t\ts32 a = (rs1 >> (i * 32)) & 0xffff;\n\t\t\ts32 b = (rs2 >> (i * 32)) & 0xffff;\n\n\t\t\tif (a == b)\n\t\t\t\trd_val |= 1 << i;\n\t\t}\n\t\tbreak;\n\t}\n\n\tmaybe_flush_windows(0, 0, RD(insn), 0);\n\tstore_reg(regs, rd_val, RD(insn));\n}\n\n/* Emulate the VIS instructions which are not implemented in\n * hardware on Niagara.\n */\nint vis_emul(struct pt_regs *regs, unsigned int insn)\n{\n\tunsigned long pc = regs->tpc;\n\tunsigned int opf;\n\n\tBUG_ON(regs->tstate & TSTATE_PRIV);\n\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);\n\n\tif (test_thread_flag(TIF_32BIT))\n\t\tpc = (u32)pc;\n\n\tif (get_user(insn, (u32 __user *) pc))\n\t\treturn -EFAULT;\n\n\tsave_and_clear_fpu();\n\n\topf = (insn & VIS_OPF_MASK) >> VIS_OPF_SHIFT;\n\tswitch (opf) {\n\tdefault:\n\t\treturn -EINVAL;\n\n\t/* Pixel Formatting Instructions.  */\n\tcase FPACK16_OPF:\n\tcase FPACK32_OPF:\n\tcase FPACKFIX_OPF:\n\tcase FEXPAND_OPF:\n\tcase FPMERGE_OPF:\n\t\tpformat(regs, insn, opf);\n\t\tbreak;\n\n\t/* Partitioned Multiply Instructions  */\n\tcase FMUL8x16_OPF:\n\tcase FMUL8x16AU_OPF:\n\tcase FMUL8x16AL_OPF:\n\tcase FMUL8SUx16_OPF:\n\tcase FMUL8ULx16_OPF:\n\tcase FMULD8SUx16_OPF:\n\tcase FMULD8ULx16_OPF:\n\t\tpmul(regs, insn, opf);\n\t\tbreak;\n\n\t/* Pixel Compare Instructions  */\n\tcase FCMPGT16_OPF:\n\tcase FCMPGT32_OPF:\n\tcase FCMPLE16_OPF:\n\tcase FCMPLE32_OPF:\n\tcase FCMPNE16_OPF:\n\tcase FCMPNE32_OPF:\n\tcase FCMPEQ16_OPF:\n\tcase FCMPEQ32_OPF:\n\t\tpcmp(regs, insn, opf);\n\t\tbreak;\n\n\t/* Edge Handling Instructions  */\n\tcase EDGE8_OPF:\n\tcase EDGE8N_OPF:\n\tcase EDGE8L_OPF:\n\tcase EDGE8LN_OPF:\n\tcase EDGE16_OPF:\n\tcase EDGE16N_OPF:\n\tcase EDGE16L_OPF:\n\tcase EDGE16LN_OPF:\n\tcase EDGE32_OPF:\n\tcase EDGE32N_OPF:\n\tcase EDGE32L_OPF:\n\tcase EDGE32LN_OPF:\n\t\tedge(regs, insn, opf);\n\t\tbreak;\n\n\t/* Pixel Component Distance  */\n\tcase PDIST_OPF:\n\t\tpdist(regs, insn);\n\t\tbreak;\n\n\t/* Three-Dimensional Array Addressing Instructions  */\n\tcase ARRAY8_OPF:\n\tcase ARRAY16_OPF:\n\tcase ARRAY32_OPF:\n\t\tarray(regs, insn, opf);\n\t\tbreak;\n\n\t/* Byte Mask and Shuffle Instructions  */\n\tcase BMASK_OPF:\n\t\tbmask(regs, insn);\n\t\tbreak;\n\n\tcase BSHUFFLE_OPF:\n\t\tbshuffle(regs, insn);\n\t\tbreak;\n\t}\n\n\tregs->tpc = regs->tnpc;\n\tregs->tnpc += 4;\n\treturn 0;\n}\n", "/*\n * arch/sparc/math-emu/math.c\n *\n * Copyright (C) 1998 Peter Maydell (pmaydell@chiark.greenend.org.uk)\n * Copyright (C) 1997, 1999 Jakub Jelinek (jj@ultra.linux.cz)\n * Copyright (C) 1999 David S. Miller (davem@redhat.com)\n *\n * This is a good place to start if you're trying to understand the\n * emulation code, because it's pretty simple. What we do is\n * essentially analyse the instruction to work out what the operation\n * is and which registers are involved. We then execute the appropriate\n * FXXXX function. [The floating point queue introduces a minor wrinkle;\n * see below...]\n * The fxxxxx.c files each emulate a single insn. They look relatively\n * simple because the complexity is hidden away in an unholy tangle\n * of preprocessor macros.\n *\n * The first layer of macros is single.h, double.h, quad.h. Generally\n * these files define macros for working with floating point numbers\n * of the three IEEE formats. FP_ADD_D(R,A,B) is for adding doubles,\n * for instance. These macros are usually defined as calls to more\n * generic macros (in this case _FP_ADD(D,2,R,X,Y) where the number\n * of machine words required to store the given IEEE format is passed\n * as a parameter. [double.h and co check the number of bits in a word\n * and define FP_ADD_D & co appropriately].\n * The generic macros are defined in op-common.h. This is where all\n * the grotty stuff like handling NaNs is coded. To handle the possible\n * word sizes macros in op-common.h use macros like _FP_FRAC_SLL_##wc()\n * where wc is the 'number of machine words' parameter (here 2).\n * These are defined in the third layer of macros: op-1.h, op-2.h\n * and op-4.h. These handle operations on floating point numbers composed\n * of 1,2 and 4 machine words respectively. [For example, on sparc64\n * doubles are one machine word so macros in double.h eventually use\n * constructs in op-1.h, but on sparc32 they use op-2.h definitions.]\n * soft-fp.h is on the same level as op-common.h, and defines some\n * macros which are independent of both word size and FP format.\n * Finally, sfp-machine.h is the machine dependent part of the\n * code: it defines the word size and what type a word is. It also\n * defines how _FP_MUL_MEAT_t() maps to _FP_MUL_MEAT_n_* : op-n.h\n * provide several possible flavours of multiply algorithm, most\n * of which require that you supply some form of asm or C primitive to\n * do the actual multiply. (such asm primitives should be defined\n * in sfp-machine.h too). udivmodti4.c is the same sort of thing.\n *\n * There may be some errors here because I'm working from a\n * SPARC architecture manual V9, and what I really want is V8...\n * Also, the insns which can generate exceptions seem to be a\n * greater subset of the FPops than for V9 (for example, FCMPED\n * has to be emulated on V8). So I think I'm going to have\n * to emulate them all just to be on the safe side...\n *\n * Emulation routines originate from soft-fp package, which is\n * part of glibc and has appropriate copyrights in it (allegedly).\n *\n * NB: on sparc int == long == 4 bytes, long long == 8 bytes.\n * Most bits of the kernel seem to go for long rather than int,\n * so we follow that practice...\n */\n\n/* TODO:\n * fpsave() saves the FP queue but fpload() doesn't reload it.\n * Therefore when we context switch or change FPU ownership\n * we have to check to see if the queue had anything in it and\n * emulate it if it did. This is going to be a pain.\n */\n\n#include <linux/types.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/perf_event.h>\n#include <asm/uaccess.h>\n\n#include \"sfp-util_32.h\"\n#include <math-emu/soft-fp.h>\n#include <math-emu/single.h>\n#include <math-emu/double.h>\n#include <math-emu/quad.h>\n\n#define FLOATFUNC(x) extern int x(void *,void *,void *)\n\n/* The Vn labels indicate what version of the SPARC architecture gas thinks\n * each insn is. This is from the binutils source :->\n */\n/* quadword instructions */\n#define FSQRTQ\t0x02b\t\t/* v8 */\n#define FADDQ\t0x043\t\t/* v8 */\n#define FSUBQ\t0x047\t\t/* v8 */\n#define FMULQ\t0x04b\t\t/* v8 */\n#define FDIVQ\t0x04f\t\t/* v8 */\n#define FDMULQ\t0x06e\t\t/* v8 */\n#define FQTOS\t0x0c7\t\t/* v8 */\n#define FQTOD\t0x0cb\t\t/* v8 */\n#define FITOQ\t0x0cc\t\t/* v8 */\n#define FSTOQ\t0x0cd\t\t/* v8 */\n#define FDTOQ\t0x0ce\t\t/* v8 */\n#define FQTOI\t0x0d3\t\t/* v8 */\n#define FCMPQ\t0x053\t\t/* v8 */\n#define FCMPEQ\t0x057\t\t/* v8 */\n/* single/double instructions (subnormal): should all work */\n#define FSQRTS\t0x029\t\t/* v7 */\n#define FSQRTD\t0x02a\t\t/* v7 */\n#define FADDS\t0x041\t\t/* v6 */\n#define FADDD\t0x042\t\t/* v6 */\n#define FSUBS\t0x045\t\t/* v6 */\n#define FSUBD\t0x046\t\t/* v6 */\n#define FMULS\t0x049\t\t/* v6 */\n#define FMULD\t0x04a\t\t/* v6 */\n#define FDIVS\t0x04d\t\t/* v6 */\n#define FDIVD\t0x04e\t\t/* v6 */\n#define FSMULD\t0x069\t\t/* v6 */\n#define FDTOS\t0x0c6\t\t/* v6 */\n#define FSTOD\t0x0c9\t\t/* v6 */\n#define FSTOI\t0x0d1\t\t/* v6 */\n#define FDTOI\t0x0d2\t\t/* v6 */\n#define FABSS\t0x009\t\t/* v6 */\n#define FCMPS\t0x051\t\t/* v6 */\n#define FCMPES\t0x055\t\t/* v6 */\n#define FCMPD\t0x052\t\t/* v6 */\n#define FCMPED\t0x056\t\t/* v6 */\n#define FMOVS\t0x001\t\t/* v6 */\n#define FNEGS\t0x005\t\t/* v6 */\n#define FITOS\t0x0c4\t\t/* v6 */\n#define FITOD\t0x0c8\t\t/* v6 */\n\n#define FSR_TEM_SHIFT\t23UL\n#define FSR_TEM_MASK\t(0x1fUL << FSR_TEM_SHIFT)\n#define FSR_AEXC_SHIFT\t5UL\n#define FSR_AEXC_MASK\t(0x1fUL << FSR_AEXC_SHIFT)\n#define FSR_CEXC_SHIFT\t0UL\n#define FSR_CEXC_MASK\t(0x1fUL << FSR_CEXC_SHIFT)\n\nstatic int do_one_mathemu(u32 insn, unsigned long *fsr, unsigned long *fregs);\n\n/* Unlike the Sparc64 version (which has a struct fpustate), we\n * pass the taskstruct corresponding to the task which currently owns the\n * FPU. This is partly because we don't have the fpustate struct and\n * partly because the task owning the FPU isn't always current (as is\n * the case for the Sparc64 port). This is probably SMP-related...\n * This function returns 1 if all queued insns were emulated successfully.\n * The test for unimplemented FPop in kernel mode has been moved into\n * kernel/traps.c for simplicity.\n */\nint do_mathemu(struct pt_regs *regs, struct task_struct *fpt)\n{\n\t/* regs->pc isn't necessarily the PC at which the offending insn is sitting.\n\t * The FPU maintains a queue of FPops which cause traps.\n\t * When it hits an instruction that requires that the trapped op succeeded\n\t * (usually because it reads a reg. that the trapped op wrote) then it\n\t * causes this exception. We need to emulate all the insns on the queue\n\t * and then allow the op to proceed.\n\t * This code should also handle the case where the trap was precise,\n\t * in which case the queue length is zero and regs->pc points at the\n\t * single FPop to be emulated. (this case is untested, though :->)\n\t * You'll need this case if you want to be able to emulate all FPops\n\t * because the FPU either doesn't exist or has been software-disabled.\n\t * [The UltraSPARC makes FP a precise trap; this isn't as stupid as it\n\t * might sound because the Ultra does funky things with a superscalar\n\t * architecture.]\n\t */\n\n\t/* You wouldn't believe how often I typed 'ftp' when I meant 'fpt' :-> */\n\n\tint i;\n\tint retcode = 0;                               /* assume all succeed */\n\tunsigned long insn;\n\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);\n\n#ifdef DEBUG_MATHEMU\n\tprintk(\"In do_mathemu()... pc is %08lx\\n\", regs->pc);\n\tprintk(\"fpqdepth is %ld\\n\", fpt->thread.fpqdepth);\n\tfor (i = 0; i < fpt->thread.fpqdepth; i++)\n\t\tprintk(\"%d: %08lx at %08lx\\n\", i, fpt->thread.fpqueue[i].insn,\n\t\t       (unsigned long)fpt->thread.fpqueue[i].insn_addr);\n#endif\n\n\tif (fpt->thread.fpqdepth == 0) {                   /* no queue, guilty insn is at regs->pc */\n#ifdef DEBUG_MATHEMU\n\t\tprintk(\"precise trap at %08lx\\n\", regs->pc);\n#endif\n\t\tif (!get_user(insn, (u32 __user *) regs->pc)) {\n\t\t\tretcode = do_one_mathemu(insn, &fpt->thread.fsr, fpt->thread.float_regs);\n\t\t\tif (retcode) {\n\t\t\t\t/* in this case we need to fix up PC & nPC */\n\t\t\t\tregs->pc = regs->npc;\n\t\t\t\tregs->npc += 4;\n\t\t\t}\n\t\t}\n\t\treturn retcode;\n\t}\n\n\t/* Normal case: need to empty the queue... */\n\tfor (i = 0; i < fpt->thread.fpqdepth; i++) {\n\t\tretcode = do_one_mathemu(fpt->thread.fpqueue[i].insn, &(fpt->thread.fsr), fpt->thread.float_regs);\n\t\tif (!retcode)                               /* insn failed, no point doing any more */\n\t\t\tbreak;\n\t}\n\t/* Now empty the queue and clear the queue_not_empty flag */\n\tif (retcode)\n\t\tfpt->thread.fsr &= ~(0x3000 | FSR_CEXC_MASK);\n\telse\n\t\tfpt->thread.fsr &= ~0x3000;\n\tfpt->thread.fpqdepth = 0;\n\n\treturn retcode;\n}\n\n/* All routines returning an exception to raise should detect\n * such exceptions _before_ rounding to be consistent with\n * the behavior of the hardware in the implemented cases\n * (and thus with the recommendations in the V9 architecture\n * manual).\n *\n * We return 0 if a SIGFPE should be sent, 1 otherwise.\n */\nstatic inline int record_exception(unsigned long *pfsr, int eflag)\n{\n\tunsigned long fsr = *pfsr;\n\tint would_trap;\n\n\t/* Determine if this exception would have generated a trap. */\n\twould_trap = (fsr & ((long)eflag << FSR_TEM_SHIFT)) != 0UL;\n\n\t/* If trapping, we only want to signal one bit. */\n\tif (would_trap != 0) {\n\t\teflag &= ((fsr & FSR_TEM_MASK) >> FSR_TEM_SHIFT);\n\t\tif ((eflag & (eflag - 1)) != 0) {\n\t\t\tif (eflag & FP_EX_INVALID)\n\t\t\t\teflag = FP_EX_INVALID;\n\t\t\telse if (eflag & FP_EX_OVERFLOW)\n\t\t\t\teflag = FP_EX_OVERFLOW;\n\t\t\telse if (eflag & FP_EX_UNDERFLOW)\n\t\t\t\teflag = FP_EX_UNDERFLOW;\n\t\t\telse if (eflag & FP_EX_DIVZERO)\n\t\t\t\teflag = FP_EX_DIVZERO;\n\t\t\telse if (eflag & FP_EX_INEXACT)\n\t\t\t\teflag = FP_EX_INEXACT;\n\t\t}\n\t}\n\n\t/* Set CEXC, here is the rule:\n\t *\n\t *    In general all FPU ops will set one and only one\n\t *    bit in the CEXC field, this is always the case\n\t *    when the IEEE exception trap is enabled in TEM.\n\t */\n\tfsr &= ~(FSR_CEXC_MASK);\n\tfsr |= ((long)eflag << FSR_CEXC_SHIFT);\n\n\t/* Set the AEXC field, rule is:\n\t *\n\t *    If a trap would not be generated, the\n\t *    CEXC just generated is OR'd into the\n\t *    existing value of AEXC.\n\t */\n\tif (would_trap == 0)\n\t\tfsr |= ((long)eflag << FSR_AEXC_SHIFT);\n\n\t/* If trapping, indicate fault trap type IEEE. */\n\tif (would_trap != 0)\n\t\tfsr |= (1UL << 14);\n\n\t*pfsr = fsr;\n\n\treturn (would_trap ? 0 : 1);\n}\n\ntypedef union {\n\tu32 s;\n\tu64 d;\n\tu64 q[2];\n} *argp;\n\nstatic int do_one_mathemu(u32 insn, unsigned long *pfsr, unsigned long *fregs)\n{\n\t/* Emulate the given insn, updating fsr and fregs appropriately. */\n\tint type = 0;\n\t/* r is rd, b is rs2 and a is rs1. The *u arg tells\n\t   whether the argument should be packed/unpacked (0 - do not unpack/pack, 1 - unpack/pack)\n\t   non-u args tells the size of the argument (0 - no argument, 1 - single, 2 - double, 3 - quad */\n#define TYPE(dummy, r, ru, b, bu, a, au) type = (au << 2) | (a << 0) | (bu << 5) | (b << 3) | (ru << 8) | (r << 6)\n\tint freg;\n\targp rs1 = NULL, rs2 = NULL, rd = NULL;\n\tFP_DECL_EX;\n\tFP_DECL_S(SA); FP_DECL_S(SB); FP_DECL_S(SR);\n\tFP_DECL_D(DA); FP_DECL_D(DB); FP_DECL_D(DR);\n\tFP_DECL_Q(QA); FP_DECL_Q(QB); FP_DECL_Q(QR);\n\tint IR;\n\tlong fsr;\n\n#ifdef DEBUG_MATHEMU\n\tprintk(\"In do_mathemu(), emulating %08lx\\n\", insn);\n#endif\n\n\tif ((insn & 0xc1f80000) == 0x81a00000)\t/* FPOP1 */ {\n\t\tswitch ((insn >> 5) & 0x1ff) {\n\t\tcase FSQRTQ: TYPE(3,3,1,3,1,0,0); break;\n\t\tcase FADDQ:\n\t\tcase FSUBQ:\n\t\tcase FMULQ:\n\t\tcase FDIVQ: TYPE(3,3,1,3,1,3,1); break;\n\t\tcase FDMULQ: TYPE(3,3,1,2,1,2,1); break;\n\t\tcase FQTOS: TYPE(3,1,1,3,1,0,0); break;\n\t\tcase FQTOD: TYPE(3,2,1,3,1,0,0); break;\n\t\tcase FITOQ: TYPE(3,3,1,1,0,0,0); break;\n\t\tcase FSTOQ: TYPE(3,3,1,1,1,0,0); break;\n\t\tcase FDTOQ: TYPE(3,3,1,2,1,0,0); break;\n\t\tcase FQTOI: TYPE(3,1,0,3,1,0,0); break;\n\t\tcase FSQRTS: TYPE(2,1,1,1,1,0,0); break;\n\t\tcase FSQRTD: TYPE(2,2,1,2,1,0,0); break;\n\t\tcase FADDD:\n\t\tcase FSUBD:\n\t\tcase FMULD:\n\t\tcase FDIVD: TYPE(2,2,1,2,1,2,1); break;\n\t\tcase FADDS:\n\t\tcase FSUBS:\n\t\tcase FMULS:\n\t\tcase FDIVS: TYPE(2,1,1,1,1,1,1); break;\n\t\tcase FSMULD: TYPE(2,2,1,1,1,1,1); break;\n\t\tcase FDTOS: TYPE(2,1,1,2,1,0,0); break;\n\t\tcase FSTOD: TYPE(2,2,1,1,1,0,0); break;\n\t\tcase FSTOI: TYPE(2,1,0,1,1,0,0); break;\n\t\tcase FDTOI: TYPE(2,1,0,2,1,0,0); break;\n\t\tcase FITOS: TYPE(2,1,1,1,0,0,0); break;\n\t\tcase FITOD: TYPE(2,2,1,1,0,0,0); break;\n\t\tcase FMOVS:\n\t\tcase FABSS:\n\t\tcase FNEGS: TYPE(2,1,0,1,0,0,0); break;\n\t\t}\n\t} else if ((insn & 0xc1f80000) == 0x81a80000)\t/* FPOP2 */ {\n\t\tswitch ((insn >> 5) & 0x1ff) {\n\t\tcase FCMPS: TYPE(3,0,0,1,1,1,1); break;\n\t\tcase FCMPES: TYPE(3,0,0,1,1,1,1); break;\n\t\tcase FCMPD: TYPE(3,0,0,2,1,2,1); break;\n\t\tcase FCMPED: TYPE(3,0,0,2,1,2,1); break;\n\t\tcase FCMPQ: TYPE(3,0,0,3,1,3,1); break;\n\t\tcase FCMPEQ: TYPE(3,0,0,3,1,3,1); break;\n\t\t}\n\t}\n\n\tif (!type) {\t/* oops, didn't recognise that FPop */\n#ifdef DEBUG_MATHEMU\n\t\tprintk(\"attempt to emulate unrecognised FPop!\\n\");\n#endif\n\t\treturn 0;\n\t}\n\n\t/* Decode the registers to be used */\n\tfreg = (*pfsr >> 14) & 0xf;\n\n\t*pfsr &= ~0x1c000;\t\t\t\t/* clear the traptype bits */\n\t\n\tfreg = ((insn >> 14) & 0x1f);\n\tswitch (type & 0x3) {\t\t\t\t/* is rs1 single, double or quad? */\n\tcase 3:\n\t\tif (freg & 3) {\t\t\t\t/* quadwords must have bits 4&5 of the */\n\t\t\t\t\t\t\t/* encoded reg. number set to zero. */\n\t\t\t*pfsr |= (6 << 14);\n\t\t\treturn 0;\t\t\t/* simulate invalid_fp_register exception */\n\t\t}\n\t/* fall through */\n\tcase 2:\n\t\tif (freg & 1) {\t\t\t\t/* doublewords must have bit 5 zeroed */\n\t\t\t*pfsr |= (6 << 14);\n\t\t\treturn 0;\n\t\t}\n\t}\n\trs1 = (argp)&fregs[freg];\n\tswitch (type & 0x7) {\n\tcase 7: FP_UNPACK_QP (QA, rs1); break;\n\tcase 6: FP_UNPACK_DP (DA, rs1); break;\n\tcase 5: FP_UNPACK_SP (SA, rs1); break;\n\t}\n\tfreg = (insn & 0x1f);\n\tswitch ((type >> 3) & 0x3) {\t\t\t/* same again for rs2 */\n\tcase 3:\n\t\tif (freg & 3) {\t\t\t\t/* quadwords must have bits 4&5 of the */\n\t\t\t\t\t\t\t/* encoded reg. number set to zero. */\n\t\t\t*pfsr |= (6 << 14);\n\t\t\treturn 0;\t\t\t/* simulate invalid_fp_register exception */\n\t\t}\n\t/* fall through */\n\tcase 2:\n\t\tif (freg & 1) {\t\t\t\t/* doublewords must have bit 5 zeroed */\n\t\t\t*pfsr |= (6 << 14);\n\t\t\treturn 0;\n\t\t}\n\t}\n\trs2 = (argp)&fregs[freg];\n\tswitch ((type >> 3) & 0x7) {\n\tcase 7: FP_UNPACK_QP (QB, rs2); break;\n\tcase 6: FP_UNPACK_DP (DB, rs2); break;\n\tcase 5: FP_UNPACK_SP (SB, rs2); break;\n\t}\n\tfreg = ((insn >> 25) & 0x1f);\n\tswitch ((type >> 6) & 0x3) {\t\t\t/* and finally rd. This one's a bit different */\n\tcase 0:\t\t\t\t\t\t/* dest is fcc. (this must be FCMPQ or FCMPEQ) */\n\t\tif (freg) {\t\t\t\t/* V8 has only one set of condition codes, so */\n\t\t\t\t\t\t\t/* anything but 0 in the rd field is an error */\n\t\t\t*pfsr |= (6 << 14);\t\t/* (should probably flag as invalid opcode */\n\t\t\treturn 0;\t\t\t/* but SIGFPE will do :-> ) */\n\t\t}\n\t\tbreak;\n\tcase 3:\n\t\tif (freg & 3) {\t\t\t\t/* quadwords must have bits 4&5 of the */\n\t\t\t\t\t\t\t/* encoded reg. number set to zero. */\n\t\t\t*pfsr |= (6 << 14);\n\t\t\treturn 0;\t\t\t/* simulate invalid_fp_register exception */\n\t\t}\n\t/* fall through */\n\tcase 2:\n\t\tif (freg & 1) {\t\t\t\t/* doublewords must have bit 5 zeroed */\n\t\t\t*pfsr |= (6 << 14);\n\t\t\treturn 0;\n\t\t}\n\t/* fall through */\n\tcase 1:\n\t\trd = (void *)&fregs[freg];\n\t\tbreak;\n\t}\n#ifdef DEBUG_MATHEMU\n\tprintk(\"executing insn...\\n\");\n#endif\n\t/* do the Right Thing */\n\tswitch ((insn >> 5) & 0x1ff) {\n\t/* + */\n\tcase FADDS: FP_ADD_S (SR, SA, SB); break;\n\tcase FADDD: FP_ADD_D (DR, DA, DB); break;\n\tcase FADDQ: FP_ADD_Q (QR, QA, QB); break;\n\t/* - */\n\tcase FSUBS: FP_SUB_S (SR, SA, SB); break;\n\tcase FSUBD: FP_SUB_D (DR, DA, DB); break;\n\tcase FSUBQ: FP_SUB_Q (QR, QA, QB); break;\n\t/* * */\n\tcase FMULS: FP_MUL_S (SR, SA, SB); break;\n\tcase FSMULD: FP_CONV (D, S, 2, 1, DA, SA);\n\t\t     FP_CONV (D, S, 2, 1, DB, SB);\n\tcase FMULD: FP_MUL_D (DR, DA, DB); break;\n\tcase FDMULQ: FP_CONV (Q, D, 4, 2, QA, DA);\n\t\t     FP_CONV (Q, D, 4, 2, QB, DB);\n\tcase FMULQ: FP_MUL_Q (QR, QA, QB); break;\n\t/* / */\n\tcase FDIVS: FP_DIV_S (SR, SA, SB); break;\n\tcase FDIVD: FP_DIV_D (DR, DA, DB); break;\n\tcase FDIVQ: FP_DIV_Q (QR, QA, QB); break;\n\t/* sqrt */\n\tcase FSQRTS: FP_SQRT_S (SR, SB); break;\n\tcase FSQRTD: FP_SQRT_D (DR, DB); break;\n\tcase FSQRTQ: FP_SQRT_Q (QR, QB); break;\n\t/* mov */\n\tcase FMOVS: rd->s = rs2->s; break;\n\tcase FABSS: rd->s = rs2->s & 0x7fffffff; break;\n\tcase FNEGS: rd->s = rs2->s ^ 0x80000000; break;\n\t/* float to int */\n\tcase FSTOI: FP_TO_INT_S (IR, SB, 32, 1); break;\n\tcase FDTOI: FP_TO_INT_D (IR, DB, 32, 1); break;\n\tcase FQTOI: FP_TO_INT_Q (IR, QB, 32, 1); break;\n\t/* int to float */\n\tcase FITOS: IR = rs2->s; FP_FROM_INT_S (SR, IR, 32, int); break;\n\tcase FITOD: IR = rs2->s; FP_FROM_INT_D (DR, IR, 32, int); break;\n\tcase FITOQ: IR = rs2->s; FP_FROM_INT_Q (QR, IR, 32, int); break;\n\t/* float to float */\n\tcase FSTOD: FP_CONV (D, S, 2, 1, DR, SB); break;\n\tcase FSTOQ: FP_CONV (Q, S, 4, 1, QR, SB); break;\n\tcase FDTOQ: FP_CONV (Q, D, 4, 2, QR, DB); break;\n\tcase FDTOS: FP_CONV (S, D, 1, 2, SR, DB); break;\n\tcase FQTOS: FP_CONV (S, Q, 1, 4, SR, QB); break;\n\tcase FQTOD: FP_CONV (D, Q, 2, 4, DR, QB); break;\n\t/* comparison */\n\tcase FCMPS:\n\tcase FCMPES:\n\t\tFP_CMP_S(IR, SB, SA, 3);\n\t\tif (IR == 3 &&\n\t\t    (((insn >> 5) & 0x1ff) == FCMPES ||\n\t\t     FP_ISSIGNAN_S(SA) ||\n\t\t     FP_ISSIGNAN_S(SB)))\n\t\t\tFP_SET_EXCEPTION (FP_EX_INVALID);\n\t\tbreak;\n\tcase FCMPD:\n\tcase FCMPED:\n\t\tFP_CMP_D(IR, DB, DA, 3);\n\t\tif (IR == 3 &&\n\t\t    (((insn >> 5) & 0x1ff) == FCMPED ||\n\t\t     FP_ISSIGNAN_D(DA) ||\n\t\t     FP_ISSIGNAN_D(DB)))\n\t\t\tFP_SET_EXCEPTION (FP_EX_INVALID);\n\t\tbreak;\n\tcase FCMPQ:\n\tcase FCMPEQ:\n\t\tFP_CMP_Q(IR, QB, QA, 3);\n\t\tif (IR == 3 &&\n\t\t    (((insn >> 5) & 0x1ff) == FCMPEQ ||\n\t\t     FP_ISSIGNAN_Q(QA) ||\n\t\t     FP_ISSIGNAN_Q(QB)))\n\t\t\tFP_SET_EXCEPTION (FP_EX_INVALID);\n\t}\n\tif (!FP_INHIBIT_RESULTS) {\n\t\tswitch ((type >> 6) & 0x7) {\n\t\tcase 0: fsr = *pfsr;\n\t\t\tif (IR == -1) IR = 2;\n\t\t\t/* fcc is always fcc0 */\n\t\t\tfsr &= ~0xc00; fsr |= (IR << 10); break;\n\t\t\t*pfsr = fsr;\n\t\t\tbreak;\n\t\tcase 1: rd->s = IR; break;\n\t\tcase 5: FP_PACK_SP (rd, SR); break;\n\t\tcase 6: FP_PACK_DP (rd, DR); break;\n\t\tcase 7: FP_PACK_QP (rd, QR); break;\n\t\t}\n\t}\n\tif (_fex == 0)\n\t\treturn 1;\t\t\t\t/* success! */\n\treturn record_exception(pfsr, _fex);\n}\n", "/*\n * arch/sparc64/math-emu/math.c\n *\n * Copyright (C) 1997,1999 Jakub Jelinek (jj@ultra.linux.cz)\n * Copyright (C) 1999 David S. Miller (davem@redhat.com)\n *\n * Emulation routines originate from soft-fp package, which is part\n * of glibc and has appropriate copyrights in it.\n */\n\n#include <linux/types.h>\n#include <linux/sched.h>\n#include <linux/errno.h>\n#include <linux/perf_event.h>\n\n#include <asm/fpumacro.h>\n#include <asm/ptrace.h>\n#include <asm/uaccess.h>\n\n#include \"sfp-util_64.h\"\n#include <math-emu/soft-fp.h>\n#include <math-emu/single.h>\n#include <math-emu/double.h>\n#include <math-emu/quad.h>\n\n/* QUAD - ftt == 3 */\n#define FMOVQ\t0x003\n#define FNEGQ\t0x007\n#define FABSQ\t0x00b\n#define FSQRTQ\t0x02b\n#define FADDQ\t0x043\n#define FSUBQ\t0x047\n#define FMULQ\t0x04b\n#define FDIVQ\t0x04f\n#define FDMULQ\t0x06e\n#define FQTOX\t0x083\n#define FXTOQ\t0x08c\n#define FQTOS\t0x0c7\n#define FQTOD\t0x0cb\n#define FITOQ\t0x0cc\n#define FSTOQ\t0x0cd\n#define FDTOQ\t0x0ce\n#define FQTOI\t0x0d3\n/* SUBNORMAL - ftt == 2 */\n#define FSQRTS\t0x029\n#define FSQRTD\t0x02a\n#define FADDS\t0x041\n#define FADDD\t0x042\n#define FSUBS\t0x045\n#define FSUBD\t0x046\n#define FMULS\t0x049\n#define FMULD\t0x04a\n#define FDIVS\t0x04d\n#define FDIVD\t0x04e\n#define FSMULD\t0x069\n#define FSTOX\t0x081\n#define FDTOX\t0x082\n#define FDTOS\t0x0c6\n#define FSTOD\t0x0c9\n#define FSTOI\t0x0d1\n#define FDTOI\t0x0d2\n#define FXTOS\t0x084 /* Only Ultra-III generates this. */\n#define FXTOD\t0x088 /* Only Ultra-III generates this. */\n#if 0\t/* Optimized inline in sparc64/kernel/entry.S */\n#define FITOS\t0x0c4 /* Only Ultra-III generates this. */\n#endif\n#define FITOD\t0x0c8 /* Only Ultra-III generates this. */\n/* FPOP2 */\n#define FCMPQ\t0x053\n#define FCMPEQ\t0x057\n#define FMOVQ0\t0x003\n#define FMOVQ1\t0x043\n#define FMOVQ2\t0x083\n#define FMOVQ3\t0x0c3\n#define FMOVQI\t0x103\n#define FMOVQX\t0x183\n#define FMOVQZ\t0x027\n#define FMOVQLE\t0x047\n#define FMOVQLZ 0x067\n#define FMOVQNZ\t0x0a7\n#define FMOVQGZ\t0x0c7\n#define FMOVQGE 0x0e7\n\n#define FSR_TEM_SHIFT\t23UL\n#define FSR_TEM_MASK\t(0x1fUL << FSR_TEM_SHIFT)\n#define FSR_AEXC_SHIFT\t5UL\n#define FSR_AEXC_MASK\t(0x1fUL << FSR_AEXC_SHIFT)\n#define FSR_CEXC_SHIFT\t0UL\n#define FSR_CEXC_MASK\t(0x1fUL << FSR_CEXC_SHIFT)\n\n/* All routines returning an exception to raise should detect\n * such exceptions _before_ rounding to be consistent with\n * the behavior of the hardware in the implemented cases\n * (and thus with the recommendations in the V9 architecture\n * manual).\n *\n * We return 0 if a SIGFPE should be sent, 1 otherwise.\n */\nstatic inline int record_exception(struct pt_regs *regs, int eflag)\n{\n\tu64 fsr = current_thread_info()->xfsr[0];\n\tint would_trap;\n\n\t/* Determine if this exception would have generated a trap. */\n\twould_trap = (fsr & ((long)eflag << FSR_TEM_SHIFT)) != 0UL;\n\n\t/* If trapping, we only want to signal one bit. */\n\tif(would_trap != 0) {\n\t\teflag &= ((fsr & FSR_TEM_MASK) >> FSR_TEM_SHIFT);\n\t\tif((eflag & (eflag - 1)) != 0) {\n\t\t\tif(eflag & FP_EX_INVALID)\n\t\t\t\teflag = FP_EX_INVALID;\n\t\t\telse if(eflag & FP_EX_OVERFLOW)\n\t\t\t\teflag = FP_EX_OVERFLOW;\n\t\t\telse if(eflag & FP_EX_UNDERFLOW)\n\t\t\t\teflag = FP_EX_UNDERFLOW;\n\t\t\telse if(eflag & FP_EX_DIVZERO)\n\t\t\t\teflag = FP_EX_DIVZERO;\n\t\t\telse if(eflag & FP_EX_INEXACT)\n\t\t\t\teflag = FP_EX_INEXACT;\n\t\t}\n\t}\n\n\t/* Set CEXC, here is the rule:\n\t *\n\t *    In general all FPU ops will set one and only one\n\t *    bit in the CEXC field, this is always the case\n\t *    when the IEEE exception trap is enabled in TEM.\n\t */\n\tfsr &= ~(FSR_CEXC_MASK);\n\tfsr |= ((long)eflag << FSR_CEXC_SHIFT);\n\n\t/* Set the AEXC field, rule is:\n\t *\n\t *    If a trap would not be generated, the\n\t *    CEXC just generated is OR'd into the\n\t *    existing value of AEXC.\n\t */\n\tif(would_trap == 0)\n\t\tfsr |= ((long)eflag << FSR_AEXC_SHIFT);\n\n\t/* If trapping, indicate fault trap type IEEE. */\n\tif(would_trap != 0)\n\t\tfsr |= (1UL << 14);\n\n\tcurrent_thread_info()->xfsr[0] = fsr;\n\n\t/* If we will not trap, advance the program counter over\n\t * the instruction being handled.\n\t */\n\tif(would_trap == 0) {\n\t\tregs->tpc = regs->tnpc;\n\t\tregs->tnpc += 4;\n\t}\n\n\treturn (would_trap ? 0 : 1);\n}\n\ntypedef union {\n\tu32 s;\n\tu64 d;\n\tu64 q[2];\n} *argp;\n\nint do_mathemu(struct pt_regs *regs, struct fpustate *f)\n{\n\tunsigned long pc = regs->tpc;\n\tunsigned long tstate = regs->tstate;\n\tu32 insn = 0;\n\tint type = 0;\n\t/* ftt tells which ftt it may happen in, r is rd, b is rs2 and a is rs1. The *u arg tells\n\t   whether the argument should be packed/unpacked (0 - do not unpack/pack, 1 - unpack/pack)\n\t   non-u args tells the size of the argument (0 - no argument, 1 - single, 2 - double, 3 - quad */\n#define TYPE(ftt, r, ru, b, bu, a, au) type = (au << 2) | (a << 0) | (bu << 5) | (b << 3) | (ru << 8) | (r << 6) | (ftt << 9)\n\tint freg;\n\tstatic u64 zero[2] = { 0L, 0L };\n\tint flags;\n\tFP_DECL_EX;\n\tFP_DECL_S(SA); FP_DECL_S(SB); FP_DECL_S(SR);\n\tFP_DECL_D(DA); FP_DECL_D(DB); FP_DECL_D(DR);\n\tFP_DECL_Q(QA); FP_DECL_Q(QB); FP_DECL_Q(QR);\n\tint IR;\n\tlong XR, xfsr;\n\n\tif (tstate & TSTATE_PRIV)\n\t\tdie_if_kernel(\"unfinished/unimplemented FPop from kernel\", regs);\n\tperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);\n\tif (test_thread_flag(TIF_32BIT))\n\t\tpc = (u32)pc;\n\tif (get_user(insn, (u32 __user *) pc) != -EFAULT) {\n\t\tif ((insn & 0xc1f80000) == 0x81a00000) /* FPOP1 */ {\n\t\t\tswitch ((insn >> 5) & 0x1ff) {\n\t\t\t/* QUAD - ftt == 3 */\n\t\t\tcase FMOVQ:\n\t\t\tcase FNEGQ:\n\t\t\tcase FABSQ: TYPE(3,3,0,3,0,0,0); break;\n\t\t\tcase FSQRTQ: TYPE(3,3,1,3,1,0,0); break;\n\t\t\tcase FADDQ:\n\t\t\tcase FSUBQ:\n\t\t\tcase FMULQ:\n\t\t\tcase FDIVQ: TYPE(3,3,1,3,1,3,1); break;\n\t\t\tcase FDMULQ: TYPE(3,3,1,2,1,2,1); break;\n\t\t\tcase FQTOX: TYPE(3,2,0,3,1,0,0); break;\n\t\t\tcase FXTOQ: TYPE(3,3,1,2,0,0,0); break;\n\t\t\tcase FQTOS: TYPE(3,1,1,3,1,0,0); break;\n\t\t\tcase FQTOD: TYPE(3,2,1,3,1,0,0); break;\n\t\t\tcase FITOQ: TYPE(3,3,1,1,0,0,0); break;\n\t\t\tcase FSTOQ: TYPE(3,3,1,1,1,0,0); break;\n\t\t\tcase FDTOQ: TYPE(3,3,1,2,1,0,0); break;\n\t\t\tcase FQTOI: TYPE(3,1,0,3,1,0,0); break;\n\n\t\t\t/* We can get either unimplemented or unfinished\n\t\t\t * for these cases.  Pre-Niagara systems generate\n\t\t\t * unfinished fpop for SUBNORMAL cases, and Niagara\n\t\t\t * always gives unimplemented fpop for fsqrt{s,d}.\n\t\t\t */\n\t\t\tcase FSQRTS: {\n\t\t\t\tunsigned long x = current_thread_info()->xfsr[0];\n\n\t\t\t\tx = (x >> 14) & 0xf;\n\t\t\t\tTYPE(x,1,1,1,1,0,0);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tcase FSQRTD: {\n\t\t\t\tunsigned long x = current_thread_info()->xfsr[0];\n\n\t\t\t\tx = (x >> 14) & 0xf;\n\t\t\t\tTYPE(x,2,1,2,1,0,0);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* SUBNORMAL - ftt == 2 */\n\t\t\tcase FADDD:\n\t\t\tcase FSUBD:\n\t\t\tcase FMULD:\n\t\t\tcase FDIVD: TYPE(2,2,1,2,1,2,1); break;\n\t\t\tcase FADDS:\n\t\t\tcase FSUBS:\n\t\t\tcase FMULS:\n\t\t\tcase FDIVS: TYPE(2,1,1,1,1,1,1); break;\n\t\t\tcase FSMULD: TYPE(2,2,1,1,1,1,1); break;\n\t\t\tcase FSTOX: TYPE(2,2,0,1,1,0,0); break;\n\t\t\tcase FDTOX: TYPE(2,2,0,2,1,0,0); break;\n\t\t\tcase FDTOS: TYPE(2,1,1,2,1,0,0); break;\n\t\t\tcase FSTOD: TYPE(2,2,1,1,1,0,0); break;\n\t\t\tcase FSTOI: TYPE(2,1,0,1,1,0,0); break;\n\t\t\tcase FDTOI: TYPE(2,1,0,2,1,0,0); break;\n\n\t\t\t/* Only Ultra-III generates these */\n\t\t\tcase FXTOS: TYPE(2,1,1,2,0,0,0); break;\n\t\t\tcase FXTOD: TYPE(2,2,1,2,0,0,0); break;\n#if 0\t\t\t/* Optimized inline in sparc64/kernel/entry.S */\n\t\t\tcase FITOS: TYPE(2,1,1,1,0,0,0); break;\n#endif\n\t\t\tcase FITOD: TYPE(2,2,1,1,0,0,0); break;\n\t\t\t}\n\t\t}\n\t\telse if ((insn & 0xc1f80000) == 0x81a80000) /* FPOP2 */ {\n\t\t\tIR = 2;\n\t\t\tswitch ((insn >> 5) & 0x1ff) {\n\t\t\tcase FCMPQ: TYPE(3,0,0,3,1,3,1); break;\n\t\t\tcase FCMPEQ: TYPE(3,0,0,3,1,3,1); break;\n\t\t\t/* Now the conditional fmovq support */\n\t\t\tcase FMOVQ0:\n\t\t\tcase FMOVQ1:\n\t\t\tcase FMOVQ2:\n\t\t\tcase FMOVQ3:\n\t\t\t\t/* fmovq %fccX, %fY, %fZ */\n\t\t\t\tif (!((insn >> 11) & 3))\n\t\t\t\t\tXR = current_thread_info()->xfsr[0] >> 10;\n\t\t\t\telse\n\t\t\t\t\tXR = current_thread_info()->xfsr[0] >> (30 + ((insn >> 10) & 0x6));\n\t\t\t\tXR &= 3;\n\t\t\t\tIR = 0;\n\t\t\t\tswitch ((insn >> 14) & 0x7) {\n\t\t\t\t/* case 0: IR = 0; break; */\t\t\t/* Never */\n\t\t\t\tcase 1: if (XR) IR = 1; break;\t\t\t/* Not Equal */\n\t\t\t\tcase 2: if (XR == 1 || XR == 2) IR = 1; break;\t/* Less or Greater */\n\t\t\t\tcase 3: if (XR & 1) IR = 1; break;\t\t/* Unordered or Less */\n\t\t\t\tcase 4: if (XR == 1) IR = 1; break;\t\t/* Less */\n\t\t\t\tcase 5: if (XR & 2) IR = 1; break;\t\t/* Unordered or Greater */\n\t\t\t\tcase 6: if (XR == 2) IR = 1; break;\t\t/* Greater */\n\t\t\t\tcase 7: if (XR == 3) IR = 1; break;\t\t/* Unordered */\n\t\t\t\t}\n\t\t\t\tif ((insn >> 14) & 8)\n\t\t\t\t\tIR ^= 1;\n\t\t\t\tbreak;\n\t\t\tcase FMOVQI:\n\t\t\tcase FMOVQX:\n\t\t\t\t/* fmovq %[ix]cc, %fY, %fZ */\n\t\t\t\tXR = regs->tstate >> 32;\n\t\t\t\tif ((insn >> 5) & 0x80)\n\t\t\t\t\tXR >>= 4;\n\t\t\t\tXR &= 0xf;\n\t\t\t\tIR = 0;\n\t\t\t\tfreg = ((XR >> 2) ^ XR) & 2;\n\t\t\t\tswitch ((insn >> 14) & 0x7) {\n\t\t\t\t/* case 0: IR = 0; break; */\t\t\t/* Never */\n\t\t\t\tcase 1: if (XR & 4) IR = 1; break;\t\t/* Equal */\n\t\t\t\tcase 2: if ((XR & 4) || freg) IR = 1; break;\t/* Less or Equal */\n\t\t\t\tcase 3: if (freg) IR = 1; break;\t\t/* Less */\n\t\t\t\tcase 4: if (XR & 5) IR = 1; break;\t\t/* Less or Equal Unsigned */\n\t\t\t\tcase 5: if (XR & 1) IR = 1; break;\t\t/* Carry Set */\n\t\t\t\tcase 6: if (XR & 8) IR = 1; break;\t\t/* Negative */\n\t\t\t\tcase 7: if (XR & 2) IR = 1; break;\t\t/* Overflow Set */\n\t\t\t\t}\n\t\t\t\tif ((insn >> 14) & 8)\n\t\t\t\t\tIR ^= 1;\n\t\t\t\tbreak;\n\t\t\tcase FMOVQZ:\n\t\t\tcase FMOVQLE:\n\t\t\tcase FMOVQLZ:\n\t\t\tcase FMOVQNZ:\n\t\t\tcase FMOVQGZ:\n\t\t\tcase FMOVQGE:\n\t\t\t\tfreg = (insn >> 14) & 0x1f;\n\t\t\t\tif (!freg)\n\t\t\t\t\tXR = 0;\n\t\t\t\telse if (freg < 16)\n\t\t\t\t\tXR = regs->u_regs[freg];\n\t\t\t\telse if (test_thread_flag(TIF_32BIT)) {\n\t\t\t\t\tstruct reg_window32 __user *win32;\n\t\t\t\t\tflushw_user ();\n\t\t\t\t\twin32 = (struct reg_window32 __user *)((unsigned long)((u32)regs->u_regs[UREG_FP]));\n\t\t\t\t\tget_user(XR, &win32->locals[freg - 16]);\n\t\t\t\t} else {\n\t\t\t\t\tstruct reg_window __user *win;\n\t\t\t\t\tflushw_user ();\n\t\t\t\t\twin = (struct reg_window __user *)(regs->u_regs[UREG_FP] + STACK_BIAS);\n\t\t\t\t\tget_user(XR, &win->locals[freg - 16]);\n\t\t\t\t}\n\t\t\t\tIR = 0;\n\t\t\t\tswitch ((insn >> 10) & 3) {\n\t\t\t\tcase 1: if (!XR) IR = 1; break;\t\t\t/* Register Zero */\n\t\t\t\tcase 2: if (XR <= 0) IR = 1; break;\t\t/* Register Less Than or Equal to Zero */\n\t\t\t\tcase 3: if (XR < 0) IR = 1; break;\t\t/* Register Less Than Zero */\n\t\t\t\t}\n\t\t\t\tif ((insn >> 10) & 4)\n\t\t\t\t\tIR ^= 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (IR == 0) {\n\t\t\t\t/* The fmov test was false. Do a nop instead */\n\t\t\t\tcurrent_thread_info()->xfsr[0] &= ~(FSR_CEXC_MASK);\n\t\t\t\tregs->tpc = regs->tnpc;\n\t\t\t\tregs->tnpc += 4;\n\t\t\t\treturn 1;\n\t\t\t} else if (IR == 1) {\n\t\t\t\t/* Change the instruction into plain fmovq */\n\t\t\t\tinsn = (insn & 0x3e00001f) | 0x81a00060;\n\t\t\t\tTYPE(3,3,0,3,0,0,0); \n\t\t\t}\n\t\t}\n\t}\n\tif (type) {\n\t\targp rs1 = NULL, rs2 = NULL, rd = NULL;\n\t\t\n\t\tfreg = (current_thread_info()->xfsr[0] >> 14) & 0xf;\n\t\tif (freg != (type >> 9))\n\t\t\tgoto err;\n\t\tcurrent_thread_info()->xfsr[0] &= ~0x1c000;\n\t\tfreg = ((insn >> 14) & 0x1f);\n\t\tswitch (type & 0x3) {\n\t\tcase 3: if (freg & 2) {\n\t\t\t\tcurrent_thread_info()->xfsr[0] |= (6 << 14) /* invalid_fp_register */;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\tcase 2: freg = ((freg & 1) << 5) | (freg & 0x1e);\n\t\tcase 1: rs1 = (argp)&f->regs[freg];\n\t\t\tflags = (freg < 32) ? FPRS_DL : FPRS_DU; \n\t\t\tif (!(current_thread_info()->fpsaved[0] & flags))\n\t\t\t\trs1 = (argp)&zero;\n\t\t\tbreak;\n\t\t}\n\t\tswitch (type & 0x7) {\n\t\tcase 7: FP_UNPACK_QP (QA, rs1); break;\n\t\tcase 6: FP_UNPACK_DP (DA, rs1); break;\n\t\tcase 5: FP_UNPACK_SP (SA, rs1); break;\n\t\t}\n\t\tfreg = (insn & 0x1f);\n\t\tswitch ((type >> 3) & 0x3) {\n\t\tcase 3: if (freg & 2) {\n\t\t\t\tcurrent_thread_info()->xfsr[0] |= (6 << 14) /* invalid_fp_register */;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\tcase 2: freg = ((freg & 1) << 5) | (freg & 0x1e);\n\t\tcase 1: rs2 = (argp)&f->regs[freg];\n\t\t\tflags = (freg < 32) ? FPRS_DL : FPRS_DU; \n\t\t\tif (!(current_thread_info()->fpsaved[0] & flags))\n\t\t\t\trs2 = (argp)&zero;\n\t\t\tbreak;\n\t\t}\n\t\tswitch ((type >> 3) & 0x7) {\n\t\tcase 7: FP_UNPACK_QP (QB, rs2); break;\n\t\tcase 6: FP_UNPACK_DP (DB, rs2); break;\n\t\tcase 5: FP_UNPACK_SP (SB, rs2); break;\n\t\t}\n\t\tfreg = ((insn >> 25) & 0x1f);\n\t\tswitch ((type >> 6) & 0x3) {\n\t\tcase 3: if (freg & 2) {\n\t\t\t\tcurrent_thread_info()->xfsr[0] |= (6 << 14) /* invalid_fp_register */;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\tcase 2: freg = ((freg & 1) << 5) | (freg & 0x1e);\n\t\tcase 1: rd = (argp)&f->regs[freg];\n\t\t\tflags = (freg < 32) ? FPRS_DL : FPRS_DU; \n\t\t\tif (!(current_thread_info()->fpsaved[0] & FPRS_FEF)) {\n\t\t\t\tcurrent_thread_info()->fpsaved[0] = FPRS_FEF;\n\t\t\t\tcurrent_thread_info()->gsr[0] = 0;\n\t\t\t}\n\t\t\tif (!(current_thread_info()->fpsaved[0] & flags)) {\n\t\t\t\tif (freg < 32)\n\t\t\t\t\tmemset(f->regs, 0, 32*sizeof(u32));\n\t\t\t\telse\n\t\t\t\t\tmemset(f->regs+32, 0, 32*sizeof(u32));\n\t\t\t}\n\t\t\tcurrent_thread_info()->fpsaved[0] |= flags;\n\t\t\tbreak;\n\t\t}\n\t\tswitch ((insn >> 5) & 0x1ff) {\n\t\t/* + */\n\t\tcase FADDS: FP_ADD_S (SR, SA, SB); break;\n\t\tcase FADDD: FP_ADD_D (DR, DA, DB); break;\n\t\tcase FADDQ: FP_ADD_Q (QR, QA, QB); break;\n\t\t/* - */\n\t\tcase FSUBS: FP_SUB_S (SR, SA, SB); break;\n\t\tcase FSUBD: FP_SUB_D (DR, DA, DB); break;\n\t\tcase FSUBQ: FP_SUB_Q (QR, QA, QB); break;\n\t\t/* * */\n\t\tcase FMULS: FP_MUL_S (SR, SA, SB); break;\n\t\tcase FSMULD: FP_CONV (D, S, 1, 1, DA, SA);\n\t\t\t     FP_CONV (D, S, 1, 1, DB, SB);\n\t\tcase FMULD: FP_MUL_D (DR, DA, DB); break;\n\t\tcase FDMULQ: FP_CONV (Q, D, 2, 1, QA, DA);\n\t\t\t     FP_CONV (Q, D, 2, 1, QB, DB);\n\t\tcase FMULQ: FP_MUL_Q (QR, QA, QB); break;\n\t\t/* / */\n\t\tcase FDIVS: FP_DIV_S (SR, SA, SB); break;\n\t\tcase FDIVD: FP_DIV_D (DR, DA, DB); break;\n\t\tcase FDIVQ: FP_DIV_Q (QR, QA, QB); break;\n\t\t/* sqrt */\n\t\tcase FSQRTS: FP_SQRT_S (SR, SB); break;\n\t\tcase FSQRTD: FP_SQRT_D (DR, DB); break;\n\t\tcase FSQRTQ: FP_SQRT_Q (QR, QB); break;\n\t\t/* mov */\n\t\tcase FMOVQ: rd->q[0] = rs2->q[0]; rd->q[1] = rs2->q[1]; break;\n\t\tcase FABSQ: rd->q[0] = rs2->q[0] & 0x7fffffffffffffffUL; rd->q[1] = rs2->q[1]; break;\n\t\tcase FNEGQ: rd->q[0] = rs2->q[0] ^ 0x8000000000000000UL; rd->q[1] = rs2->q[1]; break;\n\t\t/* float to int */\n\t\tcase FSTOI: FP_TO_INT_S (IR, SB, 32, 1); break;\n\t\tcase FDTOI: FP_TO_INT_D (IR, DB, 32, 1); break;\n\t\tcase FQTOI: FP_TO_INT_Q (IR, QB, 32, 1); break;\n\t\tcase FSTOX: FP_TO_INT_S (XR, SB, 64, 1); break;\n\t\tcase FDTOX: FP_TO_INT_D (XR, DB, 64, 1); break;\n\t\tcase FQTOX: FP_TO_INT_Q (XR, QB, 64, 1); break;\n\t\t/* int to float */\n\t\tcase FITOQ: IR = rs2->s; FP_FROM_INT_Q (QR, IR, 32, int); break;\n\t\tcase FXTOQ: XR = rs2->d; FP_FROM_INT_Q (QR, XR, 64, long); break;\n\t\t/* Only Ultra-III generates these */\n\t\tcase FXTOS: XR = rs2->d; FP_FROM_INT_S (SR, XR, 64, long); break;\n\t\tcase FXTOD: XR = rs2->d; FP_FROM_INT_D (DR, XR, 64, long); break;\n#if 0\t\t/* Optimized inline in sparc64/kernel/entry.S */\n\t\tcase FITOS: IR = rs2->s; FP_FROM_INT_S (SR, IR, 32, int); break;\n#endif\n\t\tcase FITOD: IR = rs2->s; FP_FROM_INT_D (DR, IR, 32, int); break;\n\t\t/* float to float */\n\t\tcase FSTOD: FP_CONV (D, S, 1, 1, DR, SB); break;\n\t\tcase FSTOQ: FP_CONV (Q, S, 2, 1, QR, SB); break;\n\t\tcase FDTOQ: FP_CONV (Q, D, 2, 1, QR, DB); break;\n\t\tcase FDTOS: FP_CONV (S, D, 1, 1, SR, DB); break;\n\t\tcase FQTOS: FP_CONV (S, Q, 1, 2, SR, QB); break;\n\t\tcase FQTOD: FP_CONV (D, Q, 1, 2, DR, QB); break;\n\t\t/* comparison */\n\t\tcase FCMPQ:\n\t\tcase FCMPEQ:\n\t\t\tFP_CMP_Q(XR, QB, QA, 3);\n\t\t\tif (XR == 3 &&\n\t\t\t    (((insn >> 5) & 0x1ff) == FCMPEQ ||\n\t\t\t     FP_ISSIGNAN_Q(QA) ||\n\t\t\t     FP_ISSIGNAN_Q(QB)))\n\t\t\t\tFP_SET_EXCEPTION (FP_EX_INVALID);\n\t\t}\n\t\tif (!FP_INHIBIT_RESULTS) {\n\t\t\tswitch ((type >> 6) & 0x7) {\n\t\t\tcase 0: xfsr = current_thread_info()->xfsr[0];\n\t\t\t\tif (XR == -1) XR = 2;\n\t\t\t\tswitch (freg & 3) {\n\t\t\t\t/* fcc0, 1, 2, 3 */\n\t\t\t\tcase 0: xfsr &= ~0xc00; xfsr |= (XR << 10); break;\n\t\t\t\tcase 1: xfsr &= ~0x300000000UL; xfsr |= (XR << 32); break;\n\t\t\t\tcase 2: xfsr &= ~0xc00000000UL; xfsr |= (XR << 34); break;\n\t\t\t\tcase 3: xfsr &= ~0x3000000000UL; xfsr |= (XR << 36); break;\n\t\t\t\t}\n\t\t\t\tcurrent_thread_info()->xfsr[0] = xfsr;\n\t\t\t\tbreak;\n\t\t\tcase 1: rd->s = IR; break;\n\t\t\tcase 2: rd->d = XR; break;\n\t\t\tcase 5: FP_PACK_SP (rd, SR); break;\n\t\t\tcase 6: FP_PACK_DP (rd, DR); break;\n\t\t\tcase 7: FP_PACK_QP (rd, QR); break;\n\t\t\t}\n\t\t}\n\n\t\tif(_fex != 0)\n\t\t\treturn record_exception(regs, _fex);\n\n\t\t/* Success and no exceptions detected. */\n\t\tcurrent_thread_info()->xfsr[0] &= ~(FSR_CEXC_MASK);\n\t\tregs->tpc = regs->tnpc;\n\t\tregs->tnpc += 4;\n\t\treturn 1;\n\t}\nerr:\treturn 0;\n}\n", "/*\n * fault.c:  Page fault handlers for the Sparc.\n *\n * Copyright (C) 1995 David S. Miller (davem@caip.rutgers.edu)\n * Copyright (C) 1996 Eddie C. Dost (ecd@skynet.be)\n * Copyright (C) 1997 Jakub Jelinek (jj@sunsite.mff.cuni.cz)\n */\n\n#include <asm/head.h>\n\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/sched.h>\n#include <linux/ptrace.h>\n#include <linux/mman.h>\n#include <linux/threads.h>\n#include <linux/kernel.h>\n#include <linux/signal.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/perf_event.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/kdebug.h>\n\n#include <asm/system.h>\n#include <asm/page.h>\n#include <asm/pgtable.h>\n#include <asm/memreg.h>\n#include <asm/openprom.h>\n#include <asm/oplib.h>\n#include <asm/smp.h>\n#include <asm/traps.h>\n#include <asm/uaccess.h>\n\nextern int prom_node_root;\n\nint show_unhandled_signals = 1;\n\n/* At boot time we determine these two values necessary for setting\n * up the segment maps and page table entries (pte's).\n */\n\nint num_segmaps, num_contexts;\nint invalid_segment;\n\n/* various Virtual Address Cache parameters we find at boot time... */\n\nint vac_size, vac_linesize, vac_do_hw_vac_flushes;\nint vac_entries_per_context, vac_entries_per_segment;\nint vac_entries_per_page;\n\n/* Return how much physical memory we have.  */\nunsigned long probe_memory(void)\n{\n\tunsigned long total = 0;\n\tint i;\n\n\tfor (i = 0; sp_banks[i].num_bytes; i++)\n\t\ttotal += sp_banks[i].num_bytes;\n\n\treturn total;\n}\n\nextern void sun4c_complete_all_stores(void);\n\n/* Whee, a level 15 NMI interrupt memory error.  Let's have fun... */\nasmlinkage void sparc_lvl15_nmi(struct pt_regs *regs, unsigned long serr,\n\t\t\t\tunsigned long svaddr, unsigned long aerr,\n\t\t\t\tunsigned long avaddr)\n{\n\tsun4c_complete_all_stores();\n\tprintk(\"FAULT: NMI received\\n\");\n\tprintk(\"SREGS: Synchronous Error %08lx\\n\", serr);\n\tprintk(\"       Synchronous Vaddr %08lx\\n\", svaddr);\n\tprintk(\"      Asynchronous Error %08lx\\n\", aerr);\n\tprintk(\"      Asynchronous Vaddr %08lx\\n\", avaddr);\n\tif (sun4c_memerr_reg)\n\t\tprintk(\"     Memory Parity Error %08lx\\n\", *sun4c_memerr_reg);\n\tprintk(\"REGISTER DUMP:\\n\");\n\tshow_regs(regs);\n\tprom_halt();\n}\n\nstatic void unhandled_fault(unsigned long, struct task_struct *,\n\t\tstruct pt_regs *) __attribute__ ((noreturn));\n\nstatic void unhandled_fault(unsigned long address, struct task_struct *tsk,\n                     struct pt_regs *regs)\n{\n\tif((unsigned long) address < PAGE_SIZE) {\n\t\tprintk(KERN_ALERT\n\t\t    \"Unable to handle kernel NULL pointer dereference\\n\");\n\t} else {\n\t\tprintk(KERN_ALERT \"Unable to handle kernel paging request \"\n\t\t       \"at virtual address %08lx\\n\", address);\n\t}\n\tprintk(KERN_ALERT \"tsk->{mm,active_mm}->context = %08lx\\n\",\n\t\t(tsk->mm ? tsk->mm->context : tsk->active_mm->context));\n\tprintk(KERN_ALERT \"tsk->{mm,active_mm}->pgd = %08lx\\n\",\n\t\t(tsk->mm ? (unsigned long) tsk->mm->pgd :\n\t\t \t(unsigned long) tsk->active_mm->pgd));\n\tdie_if_kernel(\"Oops\", regs);\n}\n\nasmlinkage int lookup_fault(unsigned long pc, unsigned long ret_pc, \n\t\t\t    unsigned long address)\n{\n\tstruct pt_regs regs;\n\tunsigned long g2;\n\tunsigned int insn;\n\tint i;\n\t\n\ti = search_extables_range(ret_pc, &g2);\n\tswitch (i) {\n\tcase 3:\n\t\t/* load & store will be handled by fixup */\n\t\treturn 3;\n\n\tcase 1:\n\t\t/* store will be handled by fixup, load will bump out */\n\t\t/* for _to_ macros */\n\t\tinsn = *((unsigned int *) pc);\n\t\tif ((insn >> 21) & 1)\n\t\t\treturn 1;\n\t\tbreak;\n\n\tcase 2:\n\t\t/* load will be handled by fixup, store will bump out */\n\t\t/* for _from_ macros */\n\t\tinsn = *((unsigned int *) pc);\n\t\tif (!((insn >> 21) & 1) || ((insn>>19)&0x3f) == 15)\n\t\t\treturn 2; \n\t\tbreak; \n\n\tdefault:\n\t\tbreak;\n\t}\n\n\tmemset(&regs, 0, sizeof (regs));\n\tregs.pc = pc;\n\tregs.npc = pc + 4;\n\t__asm__ __volatile__(\n\t\t\"rd %%psr, %0\\n\\t\"\n\t\t\"nop\\n\\t\"\n\t\t\"nop\\n\\t\"\n\t\t\"nop\\n\" : \"=r\" (regs.psr));\n\tunhandled_fault(address, current, &regs);\n\n\t/* Not reached */\n\treturn 0;\n}\n\nstatic inline void\nshow_signal_msg(struct pt_regs *regs, int sig, int code,\n\t\tunsigned long address, struct task_struct *tsk)\n{\n\tif (!unhandled_signal(tsk, sig))\n\t\treturn;\n\n\tif (!printk_ratelimit())\n\t\treturn;\n\n\tprintk(\"%s%s[%d]: segfault at %lx ip %p (rpc %p) sp %p error %x\",\n\t       task_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,\n\t       tsk->comm, task_pid_nr(tsk), address,\n\t       (void *)regs->pc, (void *)regs->u_regs[UREG_I7],\n\t       (void *)regs->u_regs[UREG_FP], code);\n\n\tprint_vma_addr(KERN_CONT \" in \", regs->pc);\n\n\tprintk(KERN_CONT \"\\n\");\n}\n\nstatic void __do_fault_siginfo(int code, int sig, struct pt_regs *regs,\n\t\t\t       unsigned long addr)\n{\n\tsiginfo_t info;\n\n\tinfo.si_signo = sig;\n\tinfo.si_code = code;\n\tinfo.si_errno = 0;\n\tinfo.si_addr = (void __user *) addr;\n\tinfo.si_trapno = 0;\n\n\tif (unlikely(show_unhandled_signals))\n\t\tshow_signal_msg(regs, sig, info.si_code,\n\t\t\t\taddr, current);\n\n\tforce_sig_info (sig, &info, current);\n}\n\nextern unsigned long safe_compute_effective_address(struct pt_regs *,\n\t\t\t\t\t\t    unsigned int);\n\nstatic unsigned long compute_si_addr(struct pt_regs *regs, int text_fault)\n{\n\tunsigned int insn;\n\n\tif (text_fault)\n\t\treturn regs->pc;\n\n\tif (regs->psr & PSR_PS) {\n\t\tinsn = *(unsigned int *) regs->pc;\n\t} else {\n\t\t__get_user(insn, (unsigned int *) regs->pc);\n\t}\n\n\treturn safe_compute_effective_address(regs, insn);\n}\n\nstatic noinline void do_fault_siginfo(int code, int sig, struct pt_regs *regs,\n\t\t\t\t      int text_fault)\n{\n\tunsigned long addr = compute_si_addr(regs, text_fault);\n\n\t__do_fault_siginfo(code, sig, regs, addr);\n}\n\nasmlinkage void do_sparc_fault(struct pt_regs *regs, int text_fault, int write,\n\t\t\t       unsigned long address)\n{\n\tstruct vm_area_struct *vma;\n\tstruct task_struct *tsk = current;\n\tstruct mm_struct *mm = tsk->mm;\n\tunsigned int fixup;\n\tunsigned long g2;\n\tint from_user = !(regs->psr & PSR_PS);\n\tint fault, code;\n\n\tif(text_fault)\n\t\taddress = regs->pc;\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t */\n\tcode = SEGV_MAPERR;\n\tif (!ARCH_SUN4C && address >= TASK_SIZE)\n\t\tgoto vmalloc_fault;\n\n\t/*\n\t * If we're in an interrupt or have no user\n\t * context, we must not take the fault..\n\t */\n        if (in_atomic() || !mm)\n                goto no_context;\n\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);\n\n\tdown_read(&mm->mmap_sem);\n\n\t/*\n\t * The kernel referencing a bad kernel pointer can lock up\n\t * a sun4c machine completely, so we must attempt recovery.\n\t */\n\tif(!from_user && address >= PAGE_OFFSET)\n\t\tgoto bad_area;\n\n\tvma = find_vma(mm, address);\n\tif(!vma)\n\t\tgoto bad_area;\n\tif(vma->vm_start <= address)\n\t\tgoto good_area;\n\tif(!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif(expand_stack(vma, address))\n\t\tgoto bad_area;\n\t/*\n\t * Ok, we have a good vm_area for this memory access, so\n\t * we can handle it..\n\t */\ngood_area:\n\tcode = SEGV_ACCERR;\n\tif(write) {\n\t\tif(!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t} else {\n\t\t/* Allow reads even for write-only mappings */\n\t\tif(!(vma->vm_flags & (VM_READ | VM_EXEC)))\n\t\t\tgoto bad_area;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, write ? FAULT_FLAG_WRITE : 0);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR) {\n\t\tcurrent->maj_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs, address);\n\t} else {\n\t\tcurrent->min_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs, address);\n\t}\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n\t/*\n\t * Something tried to access memory that isn't in our memory map..\n\t * Fix it, but check if it's kernel or user first..\n\t */\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (from_user) {\n\t\tdo_fault_siginfo(code, SIGSEGV, regs, text_fault);\n\t\treturn;\n\t}\n\n\t/* Is this in ex_table? */\nno_context:\n\tg2 = regs->u_regs[UREG_G2];\n\tif (!from_user) {\n\t\tfixup = search_extables_range(regs->pc, &g2);\n\t\tif (fixup > 10) { /* Values below are reserved for other things */\n\t\t\textern const unsigned __memset_start[];\n\t\t\textern const unsigned __memset_end[];\n\t\t\textern const unsigned __csum_partial_copy_start[];\n\t\t\textern const unsigned __csum_partial_copy_end[];\n\n#ifdef DEBUG_EXCEPTIONS\n\t\t\tprintk(\"Exception: PC<%08lx> faddr<%08lx>\\n\", regs->pc, address);\n\t\t\tprintk(\"EX_TABLE: insn<%08lx> fixup<%08x> g2<%08lx>\\n\",\n\t\t\t\tregs->pc, fixup, g2);\n#endif\n\t\t\tif ((regs->pc >= (unsigned long)__memset_start &&\n\t\t\t     regs->pc < (unsigned long)__memset_end) ||\n\t\t\t    (regs->pc >= (unsigned long)__csum_partial_copy_start &&\n\t\t\t     regs->pc < (unsigned long)__csum_partial_copy_end)) {\n\t\t\t        regs->u_regs[UREG_I4] = address;\n\t\t\t\tregs->u_regs[UREG_I5] = regs->pc;\n\t\t\t}\n\t\t\tregs->u_regs[UREG_G2] = g2;\n\t\t\tregs->pc = fixup;\n\t\t\tregs->npc = regs->pc + 4;\n\t\t\treturn;\n\t\t}\n\t}\n\t\n\tunhandled_fault (address, tsk, regs);\n\tdo_exit(SIGKILL);\n\n/*\n * We ran out of memory, or some other thing happened to us that made\n * us unable to handle the page fault gracefully.\n */\nout_of_memory:\n\tup_read(&mm->mmap_sem);\n\tif (from_user) {\n\t\tpagefault_out_of_memory();\n\t\treturn;\n\t}\n\tgoto no_context;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\tdo_fault_siginfo(BUS_ADRERR, SIGBUS, regs, text_fault);\n\tif (!from_user)\n\t\tgoto no_context;\n\nvmalloc_fault:\n\t{\n\t\t/*\n\t\t * Synchronize this task's top level page-table\n\t\t * with the 'reference' page table.\n\t\t */\n\t\tint offset = pgd_index(address);\n\t\tpgd_t *pgd, *pgd_k;\n\t\tpmd_t *pmd, *pmd_k;\n\n\t\tpgd = tsk->active_mm->pgd + offset;\n\t\tpgd_k = init_mm.pgd + offset;\n\n\t\tif (!pgd_present(*pgd)) {\n\t\t\tif (!pgd_present(*pgd_k))\n\t\t\t\tgoto bad_area_nosemaphore;\n\t\t\tpgd_val(*pgd) = pgd_val(*pgd_k);\n\t\t\treturn;\n\t\t}\n\n\t\tpmd = pmd_offset(pgd, address);\n\t\tpmd_k = pmd_offset(pgd_k, address);\n\n\t\tif (pmd_present(*pmd) || !pmd_present(*pmd_k))\n\t\t\tgoto bad_area_nosemaphore;\n\t\t*pmd = *pmd_k;\n\t\treturn;\n\t}\n}\n\nasmlinkage void do_sun4c_fault(struct pt_regs *regs, int text_fault, int write,\n\t\t\t       unsigned long address)\n{\n\textern void sun4c_update_mmu_cache(struct vm_area_struct *,\n\t\t\t\t\t   unsigned long,pte_t *);\n\textern pte_t *sun4c_pte_offset_kernel(pmd_t *,unsigned long);\n\tstruct task_struct *tsk = current;\n\tstruct mm_struct *mm = tsk->mm;\n\tpgd_t *pgdp;\n\tpte_t *ptep;\n\n\tif (text_fault) {\n\t\taddress = regs->pc;\n\t} else if (!write &&\n\t\t   !(regs->psr & PSR_PS)) {\n\t\tunsigned int insn, __user *ip;\n\n\t\tip = (unsigned int __user *)regs->pc;\n\t\tif (!get_user(insn, ip)) {\n\t\t\tif ((insn & 0xc1680000) == 0xc0680000)\n\t\t\t\twrite = 1;\n\t\t}\n\t}\n\n\tif (!mm) {\n\t\t/* We are oopsing. */\n\t\tdo_sparc_fault(regs, text_fault, write, address);\n\t\tBUG();\t/* P3 Oops already, you bitch */\n\t}\n\n\tpgdp = pgd_offset(mm, address);\n\tptep = sun4c_pte_offset_kernel((pmd_t *) pgdp, address);\n\n\tif (pgd_val(*pgdp)) {\n\t    if (write) {\n\t\tif ((pte_val(*ptep) & (_SUN4C_PAGE_WRITE|_SUN4C_PAGE_PRESENT))\n\t\t\t\t   == (_SUN4C_PAGE_WRITE|_SUN4C_PAGE_PRESENT)) {\n\t\t\tunsigned long flags;\n\n\t\t\t*ptep = __pte(pte_val(*ptep) | _SUN4C_PAGE_ACCESSED |\n\t\t\t\t      _SUN4C_PAGE_MODIFIED |\n\t\t\t\t      _SUN4C_PAGE_VALID |\n\t\t\t\t      _SUN4C_PAGE_DIRTY);\n\n\t\t\tlocal_irq_save(flags);\n\t\t\tif (sun4c_get_segmap(address) != invalid_segment) {\n\t\t\t\tsun4c_put_pte(address, pte_val(*ptep));\n\t\t\t\tlocal_irq_restore(flags);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tlocal_irq_restore(flags);\n\t\t}\n\t    } else {\n\t\tif ((pte_val(*ptep) & (_SUN4C_PAGE_READ|_SUN4C_PAGE_PRESENT))\n\t\t\t\t   == (_SUN4C_PAGE_READ|_SUN4C_PAGE_PRESENT)) {\n\t\t\tunsigned long flags;\n\n\t\t\t*ptep = __pte(pte_val(*ptep) | _SUN4C_PAGE_ACCESSED |\n\t\t\t\t      _SUN4C_PAGE_VALID);\n\n\t\t\tlocal_irq_save(flags);\n\t\t\tif (sun4c_get_segmap(address) != invalid_segment) {\n\t\t\t\tsun4c_put_pte(address, pte_val(*ptep));\n\t\t\t\tlocal_irq_restore(flags);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tlocal_irq_restore(flags);\n\t\t}\n\t    }\n\t}\n\n\t/* This conditional is 'interesting'. */\n\tif (pgd_val(*pgdp) && !(write && !(pte_val(*ptep) & _SUN4C_PAGE_WRITE))\n\t    && (pte_val(*ptep) & _SUN4C_PAGE_VALID))\n\t\t/* Note: It is safe to not grab the MMAP semaphore here because\n\t\t *       we know that update_mmu_cache() will not sleep for\n\t\t *       any reason (at least not in the current implementation)\n\t\t *       and therefore there is no danger of another thread getting\n\t\t *       on the CPU and doing a shrink_mmap() on this vma.\n\t\t */\n\t\tsun4c_update_mmu_cache (find_vma(current->mm, address), address,\n\t\t\t\t\tptep);\n\telse\n\t\tdo_sparc_fault(regs, text_fault, write, address);\n}\n\n/* This always deals with user addresses. */\nstatic void force_user_fault(unsigned long address, int write)\n{\n\tstruct vm_area_struct *vma;\n\tstruct task_struct *tsk = current;\n\tstruct mm_struct *mm = tsk->mm;\n\tint code;\n\n\tcode = SEGV_MAPERR;\n\n\tdown_read(&mm->mmap_sem);\n\tvma = find_vma(mm, address);\n\tif(!vma)\n\t\tgoto bad_area;\n\tif(vma->vm_start <= address)\n\t\tgoto good_area;\n\tif(!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif(expand_stack(vma, address))\n\t\tgoto bad_area;\ngood_area:\n\tcode = SEGV_ACCERR;\n\tif(write) {\n\t\tif(!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t} else {\n\t\tif(!(vma->vm_flags & (VM_READ | VM_EXEC)))\n\t\t\tgoto bad_area;\n\t}\n\tswitch (handle_mm_fault(mm, vma, address, write ? FAULT_FLAG_WRITE : 0)) {\n\tcase VM_FAULT_SIGBUS:\n\tcase VM_FAULT_OOM:\n\t\tgoto do_sigbus;\n\t}\n\tup_read(&mm->mmap_sem);\n\treturn;\nbad_area:\n\tup_read(&mm->mmap_sem);\n\t__do_fault_siginfo(code, SIGSEGV, tsk->thread.kregs, address);\n\treturn;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\t__do_fault_siginfo(BUS_ADRERR, SIGBUS, tsk->thread.kregs, address);\n}\n\nstatic void check_stack_aligned(unsigned long sp)\n{\n\tif (sp & 0x7UL)\n\t\tforce_sig(SIGILL, current);\n}\n\nvoid window_overflow_fault(void)\n{\n\tunsigned long sp;\n\n\tsp = current_thread_info()->rwbuf_stkptrs[0];\n\tif(((sp + 0x38) & PAGE_MASK) != (sp & PAGE_MASK))\n\t\tforce_user_fault(sp + 0x38, 1);\n\tforce_user_fault(sp, 1);\n\n\tcheck_stack_aligned(sp);\n}\n\nvoid window_underflow_fault(unsigned long sp)\n{\n\tif(((sp + 0x38) & PAGE_MASK) != (sp & PAGE_MASK))\n\t\tforce_user_fault(sp + 0x38, 0);\n\tforce_user_fault(sp, 0);\n\n\tcheck_stack_aligned(sp);\n}\n\nvoid window_ret_fault(struct pt_regs *regs)\n{\n\tunsigned long sp;\n\n\tsp = regs->u_regs[UREG_FP];\n\tif(((sp + 0x38) & PAGE_MASK) != (sp & PAGE_MASK))\n\t\tforce_user_fault(sp + 0x38, 0);\n\tforce_user_fault(sp, 0);\n\n\tcheck_stack_aligned(sp);\n}\n", "/*\n * arch/sparc64/mm/fault.c: Page fault handlers for the 64-bit Sparc.\n *\n * Copyright (C) 1996, 2008 David S. Miller (davem@davemloft.net)\n * Copyright (C) 1997, 1999 Jakub Jelinek (jj@ultra.linux.cz)\n */\n\n#include <asm/head.h>\n\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/sched.h>\n#include <linux/ptrace.h>\n#include <linux/mman.h>\n#include <linux/signal.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/perf_event.h>\n#include <linux/interrupt.h>\n#include <linux/kprobes.h>\n#include <linux/kdebug.h>\n#include <linux/percpu.h>\n\n#include <asm/page.h>\n#include <asm/pgtable.h>\n#include <asm/openprom.h>\n#include <asm/oplib.h>\n#include <asm/uaccess.h>\n#include <asm/asi.h>\n#include <asm/lsu.h>\n#include <asm/sections.h>\n#include <asm/mmu_context.h>\n\nint show_unhandled_signals = 1;\n\nstatic inline __kprobes int notify_page_fault(struct pt_regs *regs)\n{\n\tint ret = 0;\n\n\t/* kprobe_running() needs smp_processor_id() */\n\tif (kprobes_built_in() && !user_mode(regs)) {\n\t\tpreempt_disable();\n\t\tif (kprobe_running() && kprobe_fault_handler(regs, 0))\n\t\t\tret = 1;\n\t\tpreempt_enable();\n\t}\n\treturn ret;\n}\n\nstatic void __kprobes unhandled_fault(unsigned long address,\n\t\t\t\t      struct task_struct *tsk,\n\t\t\t\t      struct pt_regs *regs)\n{\n\tif ((unsigned long) address < PAGE_SIZE) {\n\t\tprintk(KERN_ALERT \"Unable to handle kernel NULL \"\n\t\t       \"pointer dereference\\n\");\n\t} else {\n\t\tprintk(KERN_ALERT \"Unable to handle kernel paging request \"\n\t\t       \"at virtual address %016lx\\n\", (unsigned long)address);\n\t}\n\tprintk(KERN_ALERT \"tsk->{mm,active_mm}->context = %016lx\\n\",\n\t       (tsk->mm ?\n\t\tCTX_HWBITS(tsk->mm->context) :\n\t\tCTX_HWBITS(tsk->active_mm->context)));\n\tprintk(KERN_ALERT \"tsk->{mm,active_mm}->pgd = %016lx\\n\",\n\t       (tsk->mm ? (unsigned long) tsk->mm->pgd :\n\t\t          (unsigned long) tsk->active_mm->pgd));\n\tdie_if_kernel(\"Oops\", regs);\n}\n\nstatic void __kprobes bad_kernel_pc(struct pt_regs *regs, unsigned long vaddr)\n{\n\tprintk(KERN_CRIT \"OOPS: Bogus kernel PC [%016lx] in fault handler\\n\",\n\t       regs->tpc);\n\tprintk(KERN_CRIT \"OOPS: RPC [%016lx]\\n\", regs->u_regs[15]);\n\tprintk(\"OOPS: RPC <%pS>\\n\", (void *) regs->u_regs[15]);\n\tprintk(KERN_CRIT \"OOPS: Fault was to vaddr[%lx]\\n\", vaddr);\n\tdump_stack();\n\tunhandled_fault(regs->tpc, current, regs);\n}\n\n/*\n * We now make sure that mmap_sem is held in all paths that call \n * this. Additionally, to prevent kswapd from ripping ptes from\n * under us, raise interrupts around the time that we look at the\n * pte, kswapd will have to wait to get his smp ipi response from\n * us. vmtruncate likewise. This saves us having to get pte lock.\n */\nstatic unsigned int get_user_insn(unsigned long tpc)\n{\n\tpgd_t *pgdp = pgd_offset(current->mm, tpc);\n\tpud_t *pudp;\n\tpmd_t *pmdp;\n\tpte_t *ptep, pte;\n\tunsigned long pa;\n\tu32 insn = 0;\n\tunsigned long pstate;\n\n\tif (pgd_none(*pgdp))\n\t\tgoto outret;\n\tpudp = pud_offset(pgdp, tpc);\n\tif (pud_none(*pudp))\n\t\tgoto outret;\n\tpmdp = pmd_offset(pudp, tpc);\n\tif (pmd_none(*pmdp))\n\t\tgoto outret;\n\n\t/* This disables preemption for us as well. */\n\t__asm__ __volatile__(\"rdpr %%pstate, %0\" : \"=r\" (pstate));\n\t__asm__ __volatile__(\"wrpr %0, %1, %%pstate\"\n\t\t\t\t: : \"r\" (pstate), \"i\" (PSTATE_IE));\n\tptep = pte_offset_map(pmdp, tpc);\n\tpte = *ptep;\n\tif (!pte_present(pte))\n\t\tgoto out;\n\n\tpa  = (pte_pfn(pte) << PAGE_SHIFT);\n\tpa += (tpc & ~PAGE_MASK);\n\n\t/* Use phys bypass so we don't pollute dtlb/dcache. */\n\t__asm__ __volatile__(\"lduwa [%1] %2, %0\"\n\t\t\t     : \"=r\" (insn)\n\t\t\t     : \"r\" (pa), \"i\" (ASI_PHYS_USE_EC));\n\nout:\n\tpte_unmap(ptep);\n\t__asm__ __volatile__(\"wrpr %0, 0x0, %%pstate\" : : \"r\" (pstate));\noutret:\n\treturn insn;\n}\n\nstatic inline void\nshow_signal_msg(struct pt_regs *regs, int sig, int code,\n\t\tunsigned long address, struct task_struct *tsk)\n{\n\tif (!unhandled_signal(tsk, sig))\n\t\treturn;\n\n\tif (!printk_ratelimit())\n\t\treturn;\n\n\tprintk(\"%s%s[%d]: segfault at %lx ip %p (rpc %p) sp %p error %x\",\n\t       task_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,\n\t       tsk->comm, task_pid_nr(tsk), address,\n\t       (void *)regs->tpc, (void *)regs->u_regs[UREG_I7],\n\t       (void *)regs->u_regs[UREG_FP], code);\n\n\tprint_vma_addr(KERN_CONT \" in \", regs->tpc);\n\n\tprintk(KERN_CONT \"\\n\");\n}\n\nextern unsigned long compute_effective_address(struct pt_regs *, unsigned int, unsigned int);\n\nstatic void do_fault_siginfo(int code, int sig, struct pt_regs *regs,\n\t\t\t     unsigned int insn, int fault_code)\n{\n\tunsigned long addr;\n\tsiginfo_t info;\n\n\tinfo.si_code = code;\n\tinfo.si_signo = sig;\n\tinfo.si_errno = 0;\n\tif (fault_code & FAULT_CODE_ITLB)\n\t\taddr = regs->tpc;\n\telse\n\t\taddr = compute_effective_address(regs, insn, 0);\n\tinfo.si_addr = (void __user *) addr;\n\tinfo.si_trapno = 0;\n\n\tif (unlikely(show_unhandled_signals))\n\t\tshow_signal_msg(regs, sig, code, addr, current);\n\n\tforce_sig_info(sig, &info, current);\n}\n\nextern int handle_ldf_stq(u32, struct pt_regs *);\nextern int handle_ld_nf(u32, struct pt_regs *);\n\nstatic unsigned int get_fault_insn(struct pt_regs *regs, unsigned int insn)\n{\n\tif (!insn) {\n\t\tif (!regs->tpc || (regs->tpc & 0x3))\n\t\t\treturn 0;\n\t\tif (regs->tstate & TSTATE_PRIV) {\n\t\t\tinsn = *(unsigned int *) regs->tpc;\n\t\t} else {\n\t\t\tinsn = get_user_insn(regs->tpc);\n\t\t}\n\t}\n\treturn insn;\n}\n\nstatic void __kprobes do_kernel_fault(struct pt_regs *regs, int si_code,\n\t\t\t\t      int fault_code, unsigned int insn,\n\t\t\t\t      unsigned long address)\n{\n\tunsigned char asi = ASI_P;\n \n\tif ((!insn) && (regs->tstate & TSTATE_PRIV))\n\t\tgoto cannot_handle;\n\n\t/* If user insn could be read (thus insn is zero), that\n\t * is fine.  We will just gun down the process with a signal\n\t * in that case.\n\t */\n\n\tif (!(fault_code & (FAULT_CODE_WRITE|FAULT_CODE_ITLB)) &&\n\t    (insn & 0xc0800000) == 0xc0800000) {\n\t\tif (insn & 0x2000)\n\t\t\tasi = (regs->tstate >> 24);\n\t\telse\n\t\t\tasi = (insn >> 5);\n\t\tif ((asi & 0xf2) == 0x82) {\n\t\t\tif (insn & 0x1000000) {\n\t\t\t\thandle_ldf_stq(insn, regs);\n\t\t\t} else {\n\t\t\t\t/* This was a non-faulting load. Just clear the\n\t\t\t\t * destination register(s) and continue with the next\n\t\t\t\t * instruction. -jj\n\t\t\t\t */\n\t\t\t\thandle_ld_nf(insn, regs);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t}\n\t\t\n\t/* Is this in ex_table? */\n\tif (regs->tstate & TSTATE_PRIV) {\n\t\tconst struct exception_table_entry *entry;\n\n\t\tentry = search_exception_tables(regs->tpc);\n\t\tif (entry) {\n\t\t\tregs->tpc = entry->fixup;\n\t\t\tregs->tnpc = regs->tpc + 4;\n\t\t\treturn;\n\t\t}\n\t} else {\n\t\t/* The si_code was set to make clear whether\n\t\t * this was a SEGV_MAPERR or SEGV_ACCERR fault.\n\t\t */\n\t\tdo_fault_siginfo(si_code, SIGSEGV, regs, insn, fault_code);\n\t\treturn;\n\t}\n\ncannot_handle:\n\tunhandled_fault (address, current, regs);\n}\n\nstatic void noinline __kprobes bogus_32bit_fault_tpc(struct pt_regs *regs)\n{\n\tstatic int times;\n\n\tif (times++ < 10)\n\t\tprintk(KERN_ERR \"FAULT[%s:%d]: 32-bit process reports \"\n\t\t       \"64-bit TPC [%lx]\\n\",\n\t\t       current->comm, current->pid,\n\t\t       regs->tpc);\n\tshow_regs(regs);\n}\n\nstatic void noinline __kprobes bogus_32bit_fault_address(struct pt_regs *regs,\n\t\t\t\t\t\t\t unsigned long addr)\n{\n\tstatic int times;\n\n\tif (times++ < 10)\n\t\tprintk(KERN_ERR \"FAULT[%s:%d]: 32-bit process \"\n\t\t       \"reports 64-bit fault address [%lx]\\n\",\n\t\t       current->comm, current->pid, addr);\n\tshow_regs(regs);\n}\n\nasmlinkage void __kprobes do_sparc64_fault(struct pt_regs *regs)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned int insn = 0;\n\tint si_code, fault_code, fault;\n\tunsigned long address, mm_rss;\n\n\tfault_code = get_thread_fault_code();\n\n\tif (notify_page_fault(regs))\n\t\treturn;\n\n\tsi_code = SEGV_MAPERR;\n\taddress = current_thread_info()->fault_address;\n\n\tif ((fault_code & FAULT_CODE_ITLB) &&\n\t    (fault_code & FAULT_CODE_DTLB))\n\t\tBUG();\n\n\tif (test_thread_flag(TIF_32BIT)) {\n\t\tif (!(regs->tstate & TSTATE_PRIV)) {\n\t\t\tif (unlikely((regs->tpc >> 32) != 0)) {\n\t\t\t\tbogus_32bit_fault_tpc(regs);\n\t\t\t\tgoto intr_or_no_mm;\n\t\t\t}\n\t\t}\n\t\tif (unlikely((address >> 32) != 0)) {\n\t\t\tbogus_32bit_fault_address(regs, address);\n\t\t\tgoto intr_or_no_mm;\n\t\t}\n\t}\n\n\tif (regs->tstate & TSTATE_PRIV) {\n\t\tunsigned long tpc = regs->tpc;\n\n\t\t/* Sanity check the PC. */\n\t\tif ((tpc >= KERNBASE && tpc < (unsigned long) __init_end) ||\n\t\t    (tpc >= MODULES_VADDR && tpc < MODULES_END)) {\n\t\t\t/* Valid, no problems... */\n\t\t} else {\n\t\t\tbad_kernel_pc(regs, address);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/*\n\t * If we're in an interrupt or have no user\n\t * context, we must not take the fault..\n\t */\n\tif (in_atomic() || !mm)\n\t\tgoto intr_or_no_mm;\n\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);\n\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tif ((regs->tstate & TSTATE_PRIV) &&\n\t\t    !search_exception_tables(regs->tpc)) {\n\t\t\tinsn = get_fault_insn(regs, insn);\n\t\t\tgoto handle_kernel_fault;\n\t\t}\n\t\tdown_read(&mm->mmap_sem);\n\t}\n\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\n\t/* Pure DTLB misses do not tell us whether the fault causing\n\t * load/store/atomic was a write or not, it only says that there\n\t * was no match.  So in such a case we (carefully) read the\n\t * instruction to try and figure this out.  It's an optimization\n\t * so it's ok if we can't do this.\n\t *\n\t * Special hack, window spill/fill knows the exact fault type.\n\t */\n\tif (((fault_code &\n\t      (FAULT_CODE_DTLB | FAULT_CODE_WRITE | FAULT_CODE_WINFIXUP)) == FAULT_CODE_DTLB) &&\n\t    (vma->vm_flags & VM_WRITE) != 0) {\n\t\tinsn = get_fault_insn(regs, 0);\n\t\tif (!insn)\n\t\t\tgoto continue_fault;\n\t\t/* All loads, stores and atomics have bits 30 and 31 both set\n\t\t * in the instruction.  Bit 21 is set in all stores, but we\n\t\t * have to avoid prefetches which also have bit 21 set.\n\t\t */\n\t\tif ((insn & 0xc0200000) == 0xc0200000 &&\n\t\t    (insn & 0x01780000) != 0x01680000) {\n\t\t\t/* Don't bother updating thread struct value,\n\t\t\t * because update_mmu_cache only cares which tlb\n\t\t\t * the access came from.\n\t\t\t */\n\t\t\tfault_code |= FAULT_CODE_WRITE;\n\t\t}\n\t}\ncontinue_fault:\n\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif (!(fault_code & FAULT_CODE_WRITE)) {\n\t\t/* Non-faulting loads shouldn't expand stack. */\n\t\tinsn = get_fault_insn(regs, insn);\n\t\tif ((insn & 0xc0800000) == 0xc0800000) {\n\t\t\tunsigned char asi;\n\n\t\t\tif (insn & 0x2000)\n\t\t\t\tasi = (regs->tstate >> 24);\n\t\t\telse\n\t\t\t\tasi = (insn >> 5);\n\t\t\tif ((asi & 0xf2) == 0x82)\n\t\t\t\tgoto bad_area;\n\t\t}\n\t}\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n\t/*\n\t * Ok, we have a good vm_area for this memory access, so\n\t * we can handle it..\n\t */\ngood_area:\n\tsi_code = SEGV_ACCERR;\n\n\t/* If we took a ITLB miss on a non-executable page, catch\n\t * that here.\n\t */\n\tif ((fault_code & FAULT_CODE_ITLB) && !(vma->vm_flags & VM_EXEC)) {\n\t\tBUG_ON(address != regs->tpc);\n\t\tBUG_ON(regs->tstate & TSTATE_PRIV);\n\t\tgoto bad_area;\n\t}\n\n\tif (fault_code & FAULT_CODE_WRITE) {\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\n\t\t/* Spitfire has an icache which does not snoop\n\t\t * processor stores.  Later processors do...\n\t\t */\n\t\tif (tlb_type == spitfire &&\n\t\t    (vma->vm_flags & VM_EXEC) != 0 &&\n\t\t    vma->vm_file != NULL)\n\t\t\tset_thread_fault_code(fault_code |\n\t\t\t\t\t      FAULT_CODE_BLKCOMMIT);\n\t} else {\n\t\t/* Allow reads even for write-only mappings */\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC)))\n\t\t\tgoto bad_area;\n\t}\n\n\tfault = handle_mm_fault(mm, vma, address, (fault_code & FAULT_CODE_WRITE) ? FAULT_FLAG_WRITE : 0);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR) {\n\t\tcurrent->maj_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs, address);\n\t} else {\n\t\tcurrent->min_flt++;\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs, address);\n\t}\n\tup_read(&mm->mmap_sem);\n\n\tmm_rss = get_mm_rss(mm);\n#ifdef CONFIG_HUGETLB_PAGE\n\tmm_rss -= (mm->context.huge_pte_count * (HPAGE_SIZE / PAGE_SIZE));\n#endif\n\tif (unlikely(mm_rss >\n\t\t     mm->context.tsb_block[MM_TSB_BASE].tsb_rss_limit))\n\t\ttsb_grow(mm, MM_TSB_BASE, mm_rss);\n#ifdef CONFIG_HUGETLB_PAGE\n\tmm_rss = mm->context.huge_pte_count;\n\tif (unlikely(mm_rss >\n\t\t     mm->context.tsb_block[MM_TSB_HUGE].tsb_rss_limit))\n\t\ttsb_grow(mm, MM_TSB_HUGE, mm_rss);\n#endif\n\treturn;\n\n\t/*\n\t * Something tried to access memory that isn't in our memory map..\n\t * Fix it, but check if it's kernel or user first..\n\t */\nbad_area:\n\tinsn = get_fault_insn(regs, insn);\n\tup_read(&mm->mmap_sem);\n\nhandle_kernel_fault:\n\tdo_kernel_fault(regs, si_code, fault_code, insn, address);\n\treturn;\n\n/*\n * We ran out of memory, or some other thing happened to us that made\n * us unable to handle the page fault gracefully.\n */\nout_of_memory:\n\tinsn = get_fault_insn(regs, insn);\n\tup_read(&mm->mmap_sem);\n\tif (!(regs->tstate & TSTATE_PRIV)) {\n\t\tpagefault_out_of_memory();\n\t\treturn;\n\t}\n\tgoto handle_kernel_fault;\n\nintr_or_no_mm:\n\tinsn = get_fault_insn(regs, 0);\n\tgoto handle_kernel_fault;\n\ndo_sigbus:\n\tinsn = get_fault_insn(regs, insn);\n\tup_read(&mm->mmap_sem);\n\n\t/*\n\t * Send a sigbus, regardless of whether we were in kernel\n\t * or user mode.\n\t */\n\tdo_fault_siginfo(BUS_ADRERR, SIGBUS, regs, insn, fault_code);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (regs->tstate & TSTATE_PRIV)\n\t\tgoto handle_kernel_fault;\n}\n", "/*\n * Performance events x86 architecture code\n *\n *  Copyright (C) 2008 Thomas Gleixner <tglx@linutronix.de>\n *  Copyright (C) 2008-2009 Red Hat, Inc., Ingo Molnar\n *  Copyright (C) 2009 Jaswinder Singh Rajput\n *  Copyright (C) 2009 Advanced Micro Devices, Inc., Robert Richter\n *  Copyright (C) 2008-2009 Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>\n *  Copyright (C) 2009 Intel Corporation, <markus.t.metzger@intel.com>\n *  Copyright (C) 2009 Google, Inc., Stephane Eranian\n *\n *  For licencing details see kernel-base/COPYING\n */\n\n#include <linux/perf_event.h>\n#include <linux/capability.h>\n#include <linux/notifier.h>\n#include <linux/hardirq.h>\n#include <linux/kprobes.h>\n#include <linux/module.h>\n#include <linux/kdebug.h>\n#include <linux/sched.h>\n#include <linux/uaccess.h>\n#include <linux/slab.h>\n#include <linux/highmem.h>\n#include <linux/cpu.h>\n#include <linux/bitops.h>\n\n#include <asm/apic.h>\n#include <asm/stacktrace.h>\n#include <asm/nmi.h>\n#include <asm/compat.h>\n#include <asm/smp.h>\n#include <asm/alternative.h>\n\n#if 0\n#undef wrmsrl\n#define wrmsrl(msr, val) \t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\ttrace_printk(\"wrmsrl(%lx, %lx)\\n\", (unsigned long)(msr),\\\n\t\t\t(unsigned long)(val));\t\t\t\\\n\tnative_write_msr((msr), (u32)((u64)(val)), \t\t\\\n\t\t\t(u32)((u64)(val) >> 32));\t\t\\\n} while (0)\n#endif\n\n/*\n * best effort, GUP based copy_from_user() that assumes IRQ or NMI context\n */\nstatic unsigned long\ncopy_from_user_nmi(void *to, const void __user *from, unsigned long n)\n{\n\tunsigned long offset, addr = (unsigned long)from;\n\tunsigned long size, len = 0;\n\tstruct page *page;\n\tvoid *map;\n\tint ret;\n\n\tdo {\n\t\tret = __get_user_pages_fast(addr, 1, 0, &page);\n\t\tif (!ret)\n\t\t\tbreak;\n\n\t\toffset = addr & (PAGE_SIZE - 1);\n\t\tsize = min(PAGE_SIZE - offset, n - len);\n\n\t\tmap = kmap_atomic(page);\n\t\tmemcpy(to, map+offset, size);\n\t\tkunmap_atomic(map);\n\t\tput_page(page);\n\n\t\tlen  += size;\n\t\tto   += size;\n\t\taddr += size;\n\n\t} while (len < n);\n\n\treturn len;\n}\n\nstruct event_constraint {\n\tunion {\n\t\tunsigned long\tidxmsk[BITS_TO_LONGS(X86_PMC_IDX_MAX)];\n\t\tu64\t\tidxmsk64;\n\t};\n\tu64\tcode;\n\tu64\tcmask;\n\tint\tweight;\n};\n\nstruct amd_nb {\n\tint nb_id;  /* NorthBridge id */\n\tint refcnt; /* reference count */\n\tstruct perf_event *owners[X86_PMC_IDX_MAX];\n\tstruct event_constraint event_constraints[X86_PMC_IDX_MAX];\n};\n\nstruct intel_percore;\n\n#define MAX_LBR_ENTRIES\t\t16\n\nstruct cpu_hw_events {\n\t/*\n\t * Generic x86 PMC bits\n\t */\n\tstruct perf_event\t*events[X86_PMC_IDX_MAX]; /* in counter order */\n\tunsigned long\t\tactive_mask[BITS_TO_LONGS(X86_PMC_IDX_MAX)];\n\tunsigned long\t\trunning[BITS_TO_LONGS(X86_PMC_IDX_MAX)];\n\tint\t\t\tenabled;\n\n\tint\t\t\tn_events;\n\tint\t\t\tn_added;\n\tint\t\t\tn_txn;\n\tint\t\t\tassign[X86_PMC_IDX_MAX]; /* event to counter assignment */\n\tu64\t\t\ttags[X86_PMC_IDX_MAX];\n\tstruct perf_event\t*event_list[X86_PMC_IDX_MAX]; /* in enabled order */\n\n\tunsigned int\t\tgroup_flag;\n\n\t/*\n\t * Intel DebugStore bits\n\t */\n\tstruct debug_store\t*ds;\n\tu64\t\t\tpebs_enabled;\n\n\t/*\n\t * Intel LBR bits\n\t */\n\tint\t\t\t\tlbr_users;\n\tvoid\t\t\t\t*lbr_context;\n\tstruct perf_branch_stack\tlbr_stack;\n\tstruct perf_branch_entry\tlbr_entries[MAX_LBR_ENTRIES];\n\n\t/*\n\t * Intel percore register state.\n\t * Coordinate shared resources between HT threads.\n\t */\n\tint\t\t\t\tpercore_used; /* Used by this CPU? */\n\tstruct intel_percore\t\t*per_core;\n\n\t/*\n\t * AMD specific bits\n\t */\n\tstruct amd_nb\t\t*amd_nb;\n};\n\n#define __EVENT_CONSTRAINT(c, n, m, w) {\\\n\t{ .idxmsk64 = (n) },\t\t\\\n\t.code = (c),\t\t\t\\\n\t.cmask = (m),\t\t\t\\\n\t.weight = (w),\t\t\t\\\n}\n\n#define EVENT_CONSTRAINT(c, n, m)\t\\\n\t__EVENT_CONSTRAINT(c, n, m, HWEIGHT(n))\n\n/*\n * Constraint on the Event code.\n */\n#define INTEL_EVENT_CONSTRAINT(c, n)\t\\\n\tEVENT_CONSTRAINT(c, n, ARCH_PERFMON_EVENTSEL_EVENT)\n\n/*\n * Constraint on the Event code + UMask + fixed-mask\n *\n * filter mask to validate fixed counter events.\n * the following filters disqualify for fixed counters:\n *  - inv\n *  - edge\n *  - cnt-mask\n *  The other filters are supported by fixed counters.\n *  The any-thread option is supported starting with v3.\n */\n#define FIXED_EVENT_CONSTRAINT(c, n)\t\\\n\tEVENT_CONSTRAINT(c, (1ULL << (32+n)), X86_RAW_EVENT_MASK)\n\n/*\n * Constraint on the Event code + UMask\n */\n#define INTEL_UEVENT_CONSTRAINT(c, n)\t\\\n\tEVENT_CONSTRAINT(c, n, INTEL_ARCH_EVENT_MASK)\n\n#define EVENT_CONSTRAINT_END\t\t\\\n\tEVENT_CONSTRAINT(0, 0, 0)\n\n#define for_each_event_constraint(e, c)\t\\\n\tfor ((e) = (c); (e)->weight; (e)++)\n\n/*\n * Extra registers for specific events.\n * Some events need large masks and require external MSRs.\n * Define a mapping to these extra registers.\n */\nstruct extra_reg {\n\tunsigned int\t\tevent;\n\tunsigned int\t\tmsr;\n\tu64\t\t\tconfig_mask;\n\tu64\t\t\tvalid_mask;\n};\n\n#define EVENT_EXTRA_REG(e, ms, m, vm) {\t\\\n\t.event = (e),\t\t\\\n\t.msr = (ms),\t\t\\\n\t.config_mask = (m),\t\\\n\t.valid_mask = (vm),\t\\\n\t}\n#define INTEL_EVENT_EXTRA_REG(event, msr, vm)\t\\\n\tEVENT_EXTRA_REG(event, msr, ARCH_PERFMON_EVENTSEL_EVENT, vm)\n#define EVENT_EXTRA_END EVENT_EXTRA_REG(0, 0, 0, 0)\n\nunion perf_capabilities {\n\tstruct {\n\t\tu64\tlbr_format    : 6;\n\t\tu64\tpebs_trap     : 1;\n\t\tu64\tpebs_arch_reg : 1;\n\t\tu64\tpebs_format   : 4;\n\t\tu64\tsmm_freeze    : 1;\n\t};\n\tu64\tcapabilities;\n};\n\n/*\n * struct x86_pmu - generic x86 pmu\n */\nstruct x86_pmu {\n\t/*\n\t * Generic x86 PMC bits\n\t */\n\tconst char\t*name;\n\tint\t\tversion;\n\tint\t\t(*handle_irq)(struct pt_regs *);\n\tvoid\t\t(*disable_all)(void);\n\tvoid\t\t(*enable_all)(int added);\n\tvoid\t\t(*enable)(struct perf_event *);\n\tvoid\t\t(*disable)(struct perf_event *);\n\tvoid\t\t(*hw_watchdog_set_attr)(struct perf_event_attr *attr);\n\tint\t\t(*hw_config)(struct perf_event *event);\n\tint\t\t(*schedule_events)(struct cpu_hw_events *cpuc, int n, int *assign);\n\tunsigned\teventsel;\n\tunsigned\tperfctr;\n\tu64\t\t(*event_map)(int);\n\tint\t\tmax_events;\n\tint\t\tnum_counters;\n\tint\t\tnum_counters_fixed;\n\tint\t\tcntval_bits;\n\tu64\t\tcntval_mask;\n\tint\t\tapic;\n\tu64\t\tmax_period;\n\tstruct event_constraint *\n\t\t\t(*get_event_constraints)(struct cpu_hw_events *cpuc,\n\t\t\t\t\t\t struct perf_event *event);\n\n\tvoid\t\t(*put_event_constraints)(struct cpu_hw_events *cpuc,\n\t\t\t\t\t\t struct perf_event *event);\n\tstruct event_constraint *event_constraints;\n\tstruct event_constraint *percore_constraints;\n\tvoid\t\t(*quirks)(void);\n\tint\t\tperfctr_second_write;\n\n\tint\t\t(*cpu_prepare)(int cpu);\n\tvoid\t\t(*cpu_starting)(int cpu);\n\tvoid\t\t(*cpu_dying)(int cpu);\n\tvoid\t\t(*cpu_dead)(int cpu);\n\n\t/*\n\t * Intel Arch Perfmon v2+\n\t */\n\tu64\t\t\tintel_ctrl;\n\tunion perf_capabilities intel_cap;\n\n\t/*\n\t * Intel DebugStore bits\n\t */\n\tint\t\tbts, pebs;\n\tint\t\tbts_active, pebs_active;\n\tint\t\tpebs_record_size;\n\tvoid\t\t(*drain_pebs)(struct pt_regs *regs);\n\tstruct event_constraint *pebs_constraints;\n\n\t/*\n\t * Intel LBR\n\t */\n\tunsigned long\tlbr_tos, lbr_from, lbr_to; /* MSR base regs       */\n\tint\t\tlbr_nr;\t\t\t   /* hardware stack size */\n\n\t/*\n\t * Extra registers for events\n\t */\n\tstruct extra_reg *extra_regs;\n};\n\nstatic struct x86_pmu x86_pmu __read_mostly;\n\nstatic DEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events) = {\n\t.enabled = 1,\n};\n\nstatic int x86_perf_event_set_period(struct perf_event *event);\n\n/*\n * Generalized hw caching related hw_event table, filled\n * in on a per model basis. A value of 0 means\n * 'not supported', -1 means 'hw_event makes no sense on\n * this CPU', any other value means the raw hw_event\n * ID.\n */\n\n#define C(x) PERF_COUNT_HW_CACHE_##x\n\nstatic u64 __read_mostly hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX];\nstatic u64 __read_mostly hw_cache_extra_regs\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX];\n\nvoid hw_nmi_watchdog_set_attr(struct perf_event_attr *wd_attr)\n{\n\tif (x86_pmu.hw_watchdog_set_attr)\n\t\tx86_pmu.hw_watchdog_set_attr(wd_attr);\n}\n\n/*\n * Propagate event elapsed time into the generic event.\n * Can only be executed on the CPU where the event is active.\n * Returns the delta events processed.\n */\nstatic u64\nx86_perf_event_update(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint shift = 64 - x86_pmu.cntval_bits;\n\tu64 prev_raw_count, new_raw_count;\n\tint idx = hwc->idx;\n\ts64 delta;\n\n\tif (idx == X86_PMC_IDX_FIXED_BTS)\n\t\treturn 0;\n\n\t/*\n\t * Careful: an NMI might modify the previous event value.\n\t *\n\t * Our tactic to handle this is to first atomically read and\n\t * exchange a new raw count - then add that new-prev delta\n\t * count to the generic event atomically:\n\t */\nagain:\n\tprev_raw_count = local64_read(&hwc->prev_count);\n\trdmsrl(hwc->event_base, new_raw_count);\n\n\tif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\n\t\t\t\t\tnew_raw_count) != prev_raw_count)\n\t\tgoto again;\n\n\t/*\n\t * Now we have the new raw value and have updated the prev\n\t * timestamp already. We can now calculate the elapsed delta\n\t * (event-)time and add that to the generic event.\n\t *\n\t * Careful, not all hw sign-extends above the physical width\n\t * of the count.\n\t */\n\tdelta = (new_raw_count << shift) - (prev_raw_count << shift);\n\tdelta >>= shift;\n\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &hwc->period_left);\n\n\treturn new_raw_count;\n}\n\nstatic inline int x86_pmu_addr_offset(int index)\n{\n\tint offset;\n\n\t/* offset = X86_FEATURE_PERFCTR_CORE ? index << 1 : index */\n\talternative_io(ASM_NOP2,\n\t\t       \"shll $1, %%eax\",\n\t\t       X86_FEATURE_PERFCTR_CORE,\n\t\t       \"=a\" (offset),\n\t\t       \"a\"  (index));\n\n\treturn offset;\n}\n\nstatic inline unsigned int x86_pmu_config_addr(int index)\n{\n\treturn x86_pmu.eventsel + x86_pmu_addr_offset(index);\n}\n\nstatic inline unsigned int x86_pmu_event_addr(int index)\n{\n\treturn x86_pmu.perfctr + x86_pmu_addr_offset(index);\n}\n\n/*\n * Find and validate any extra registers to set up.\n */\nstatic int x86_pmu_extra_regs(u64 config, struct perf_event *event)\n{\n\tstruct extra_reg *er;\n\n\tevent->hw.extra_reg = 0;\n\tevent->hw.extra_config = 0;\n\n\tif (!x86_pmu.extra_regs)\n\t\treturn 0;\n\n\tfor (er = x86_pmu.extra_regs; er->msr; er++) {\n\t\tif (er->event != (config & er->config_mask))\n\t\t\tcontinue;\n\t\tif (event->attr.config1 & ~er->valid_mask)\n\t\t\treturn -EINVAL;\n\t\tevent->hw.extra_reg = er->msr;\n\t\tevent->hw.extra_config = event->attr.config1;\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic atomic_t active_events;\nstatic DEFINE_MUTEX(pmc_reserve_mutex);\n\n#ifdef CONFIG_X86_LOCAL_APIC\n\nstatic bool reserve_pmc_hardware(void)\n{\n\tint i;\n\n\tfor (i = 0; i < x86_pmu.num_counters; i++) {\n\t\tif (!reserve_perfctr_nmi(x86_pmu_event_addr(i)))\n\t\t\tgoto perfctr_fail;\n\t}\n\n\tfor (i = 0; i < x86_pmu.num_counters; i++) {\n\t\tif (!reserve_evntsel_nmi(x86_pmu_config_addr(i)))\n\t\t\tgoto eventsel_fail;\n\t}\n\n\treturn true;\n\neventsel_fail:\n\tfor (i--; i >= 0; i--)\n\t\trelease_evntsel_nmi(x86_pmu_config_addr(i));\n\n\ti = x86_pmu.num_counters;\n\nperfctr_fail:\n\tfor (i--; i >= 0; i--)\n\t\trelease_perfctr_nmi(x86_pmu_event_addr(i));\n\n\treturn false;\n}\n\nstatic void release_pmc_hardware(void)\n{\n\tint i;\n\n\tfor (i = 0; i < x86_pmu.num_counters; i++) {\n\t\trelease_perfctr_nmi(x86_pmu_event_addr(i));\n\t\trelease_evntsel_nmi(x86_pmu_config_addr(i));\n\t}\n}\n\n#else\n\nstatic bool reserve_pmc_hardware(void) { return true; }\nstatic void release_pmc_hardware(void) {}\n\n#endif\n\nstatic bool check_hw_exists(void)\n{\n\tu64 val, val_new = 0;\n\tint i, reg, ret = 0;\n\n\t/*\n\t * Check to see if the BIOS enabled any of the counters, if so\n\t * complain and bail.\n\t */\n\tfor (i = 0; i < x86_pmu.num_counters; i++) {\n\t\treg = x86_pmu_config_addr(i);\n\t\tret = rdmsrl_safe(reg, &val);\n\t\tif (ret)\n\t\t\tgoto msr_fail;\n\t\tif (val & ARCH_PERFMON_EVENTSEL_ENABLE)\n\t\t\tgoto bios_fail;\n\t}\n\n\tif (x86_pmu.num_counters_fixed) {\n\t\treg = MSR_ARCH_PERFMON_FIXED_CTR_CTRL;\n\t\tret = rdmsrl_safe(reg, &val);\n\t\tif (ret)\n\t\t\tgoto msr_fail;\n\t\tfor (i = 0; i < x86_pmu.num_counters_fixed; i++) {\n\t\t\tif (val & (0x03 << i*4))\n\t\t\t\tgoto bios_fail;\n\t\t}\n\t}\n\n\t/*\n\t * Now write a value and read it back to see if it matches,\n\t * this is needed to detect certain hardware emulators (qemu/kvm)\n\t * that don't trap on the MSR access and always return 0s.\n\t */\n\tval = 0xabcdUL;\n\tret = checking_wrmsrl(x86_pmu_event_addr(0), val);\n\tret |= rdmsrl_safe(x86_pmu_event_addr(0), &val_new);\n\tif (ret || val != val_new)\n\t\tgoto msr_fail;\n\n\treturn true;\n\nbios_fail:\n\t/*\n\t * We still allow the PMU driver to operate:\n\t */\n\tprintk(KERN_CONT \"Broken BIOS detected, complain to your hardware vendor.\\n\");\n\tprintk(KERN_ERR FW_BUG \"the BIOS has corrupted hw-PMU resources (MSR %x is %Lx)\\n\", reg, val);\n\n\treturn true;\n\nmsr_fail:\n\tprintk(KERN_CONT \"Broken PMU hardware detected, using software events only.\\n\");\n\n\treturn false;\n}\n\nstatic void reserve_ds_buffers(void);\nstatic void release_ds_buffers(void);\n\nstatic void hw_perf_event_destroy(struct perf_event *event)\n{\n\tif (atomic_dec_and_mutex_lock(&active_events, &pmc_reserve_mutex)) {\n\t\trelease_pmc_hardware();\n\t\trelease_ds_buffers();\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n}\n\nstatic inline int x86_pmu_initialized(void)\n{\n\treturn x86_pmu.handle_irq != NULL;\n}\n\nstatic inline int\nset_ext_hw_attr(struct hw_perf_event *hwc, struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\tunsigned int cache_type, cache_op, cache_result;\n\tu64 config, val;\n\n\tconfig = attr->config;\n\n\tcache_type = (config >>  0) & 0xff;\n\tif (cache_type >= PERF_COUNT_HW_CACHE_MAX)\n\t\treturn -EINVAL;\n\n\tcache_op = (config >>  8) & 0xff;\n\tif (cache_op >= PERF_COUNT_HW_CACHE_OP_MAX)\n\t\treturn -EINVAL;\n\n\tcache_result = (config >> 16) & 0xff;\n\tif (cache_result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn -EINVAL;\n\n\tval = hw_cache_event_ids[cache_type][cache_op][cache_result];\n\n\tif (val == 0)\n\t\treturn -ENOENT;\n\n\tif (val == -1)\n\t\treturn -EINVAL;\n\n\thwc->config |= val;\n\tattr->config1 = hw_cache_extra_regs[cache_type][cache_op][cache_result];\n\treturn x86_pmu_extra_regs(val, event);\n}\n\nstatic int x86_setup_perfctr(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 config;\n\n\tif (!is_sampling_event(event)) {\n\t\thwc->sample_period = x86_pmu.max_period;\n\t\thwc->last_period = hwc->sample_period;\n\t\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\t} else {\n\t\t/*\n\t\t * If we have a PMU initialized but no APIC\n\t\t * interrupts, we cannot sample hardware\n\t\t * events (user-space has to fall back and\n\t\t * sample via a hrtimer based software event):\n\t\t */\n\t\tif (!x86_pmu.apic)\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\t/*\n\t * Do not allow config1 (extended registers) to propagate,\n\t * there's no sane user-space generalization yet:\n\t */\n\tif (attr->type == PERF_TYPE_RAW)\n\t\treturn 0;\n\n\tif (attr->type == PERF_TYPE_HW_CACHE)\n\t\treturn set_ext_hw_attr(hwc, event);\n\n\tif (attr->config >= x86_pmu.max_events)\n\t\treturn -EINVAL;\n\n\t/*\n\t * The generic map:\n\t */\n\tconfig = x86_pmu.event_map(attr->config);\n\n\tif (config == 0)\n\t\treturn -ENOENT;\n\n\tif (config == -1LL)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Branch tracing:\n\t */\n\tif (attr->config == PERF_COUNT_HW_BRANCH_INSTRUCTIONS &&\n\t    !attr->freq && hwc->sample_period == 1) {\n\t\t/* BTS is not supported by this architecture. */\n\t\tif (!x86_pmu.bts_active)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\t/* BTS is currently only allowed for user-mode. */\n\t\tif (!attr->exclude_kernel)\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\thwc->config |= config;\n\n\treturn 0;\n}\n\nstatic int x86_pmu_hw_config(struct perf_event *event)\n{\n\tif (event->attr.precise_ip) {\n\t\tint precise = 0;\n\n\t\t/* Support for constant skid */\n\t\tif (x86_pmu.pebs_active) {\n\t\t\tprecise++;\n\n\t\t\t/* Support for IP fixup */\n\t\t\tif (x86_pmu.lbr_nr)\n\t\t\t\tprecise++;\n\t\t}\n\n\t\tif (event->attr.precise_ip > precise)\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\t/*\n\t * Generate PMC IRQs:\n\t * (keep 'enabled' bit clear for now)\n\t */\n\tevent->hw.config = ARCH_PERFMON_EVENTSEL_INT;\n\n\t/*\n\t * Count user and OS events unless requested not to\n\t */\n\tif (!event->attr.exclude_user)\n\t\tevent->hw.config |= ARCH_PERFMON_EVENTSEL_USR;\n\tif (!event->attr.exclude_kernel)\n\t\tevent->hw.config |= ARCH_PERFMON_EVENTSEL_OS;\n\n\tif (event->attr.type == PERF_TYPE_RAW)\n\t\tevent->hw.config |= event->attr.config & X86_RAW_EVENT_MASK;\n\n\treturn x86_setup_perfctr(event);\n}\n\n/*\n * Setup the hardware configuration for a given attr_type\n */\nstatic int __x86_pmu_event_init(struct perf_event *event)\n{\n\tint err;\n\n\tif (!x86_pmu_initialized())\n\t\treturn -ENODEV;\n\n\terr = 0;\n\tif (!atomic_inc_not_zero(&active_events)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_read(&active_events) == 0) {\n\t\t\tif (!reserve_pmc_hardware())\n\t\t\t\terr = -EBUSY;\n\t\t\telse\n\t\t\t\treserve_ds_buffers();\n\t\t}\n\t\tif (!err)\n\t\t\tatomic_inc(&active_events);\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n\tif (err)\n\t\treturn err;\n\n\tevent->destroy = hw_perf_event_destroy;\n\n\tevent->hw.idx = -1;\n\tevent->hw.last_cpu = -1;\n\tevent->hw.last_tag = ~0ULL;\n\n\treturn x86_pmu.hw_config(event);\n}\n\nstatic void x86_pmu_disable_all(void)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint idx;\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tu64 val;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\t\trdmsrl(x86_pmu_config_addr(idx), val);\n\t\tif (!(val & ARCH_PERFMON_EVENTSEL_ENABLE))\n\t\t\tcontinue;\n\t\tval &= ~ARCH_PERFMON_EVENTSEL_ENABLE;\n\t\twrmsrl(x86_pmu_config_addr(idx), val);\n\t}\n}\n\nstatic void x86_pmu_disable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\tif (!x86_pmu_initialized())\n\t\treturn;\n\n\tif (!cpuc->enabled)\n\t\treturn;\n\n\tcpuc->n_added = 0;\n\tcpuc->enabled = 0;\n\tbarrier();\n\n\tx86_pmu.disable_all();\n}\n\nstatic inline void __x86_pmu_enable_event(struct hw_perf_event *hwc,\n\t\t\t\t\t  u64 enable_mask)\n{\n\tif (hwc->extra_reg)\n\t\twrmsrl(hwc->extra_reg, hwc->extra_config);\n\twrmsrl(hwc->config_base, hwc->config | enable_mask);\n}\n\nstatic void x86_pmu_enable_all(int added)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint idx;\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tstruct hw_perf_event *hwc = &cpuc->events[idx]->hw;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\t__x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);\n\t}\n}\n\nstatic struct pmu pmu;\n\nstatic inline int is_x86_event(struct perf_event *event)\n{\n\treturn event->pmu == &pmu;\n}\n\nstatic int x86_schedule_events(struct cpu_hw_events *cpuc, int n, int *assign)\n{\n\tstruct event_constraint *c, *constraints[X86_PMC_IDX_MAX];\n\tunsigned long used_mask[BITS_TO_LONGS(X86_PMC_IDX_MAX)];\n\tint i, j, w, wmax, num = 0;\n\tstruct hw_perf_event *hwc;\n\n\tbitmap_zero(used_mask, X86_PMC_IDX_MAX);\n\n\tfor (i = 0; i < n; i++) {\n\t\tc = x86_pmu.get_event_constraints(cpuc, cpuc->event_list[i]);\n\t\tconstraints[i] = c;\n\t}\n\n\t/*\n\t * fastpath, try to reuse previous register\n\t */\n\tfor (i = 0; i < n; i++) {\n\t\thwc = &cpuc->event_list[i]->hw;\n\t\tc = constraints[i];\n\n\t\t/* never assigned */\n\t\tif (hwc->idx == -1)\n\t\t\tbreak;\n\n\t\t/* constraint still honored */\n\t\tif (!test_bit(hwc->idx, c->idxmsk))\n\t\t\tbreak;\n\n\t\t/* not already used */\n\t\tif (test_bit(hwc->idx, used_mask))\n\t\t\tbreak;\n\n\t\t__set_bit(hwc->idx, used_mask);\n\t\tif (assign)\n\t\t\tassign[i] = hwc->idx;\n\t}\n\tif (i == n)\n\t\tgoto done;\n\n\t/*\n\t * begin slow path\n\t */\n\n\tbitmap_zero(used_mask, X86_PMC_IDX_MAX);\n\n\t/*\n\t * weight = number of possible counters\n\t *\n\t * 1    = most constrained, only works on one counter\n\t * wmax = least constrained, works on any counter\n\t *\n\t * assign events to counters starting with most\n\t * constrained events.\n\t */\n\twmax = x86_pmu.num_counters;\n\n\t/*\n\t * when fixed event counters are present,\n\t * wmax is incremented by 1 to account\n\t * for one more choice\n\t */\n\tif (x86_pmu.num_counters_fixed)\n\t\twmax++;\n\n\tfor (w = 1, num = n; num && w <= wmax; w++) {\n\t\t/* for each event */\n\t\tfor (i = 0; num && i < n; i++) {\n\t\t\tc = constraints[i];\n\t\t\thwc = &cpuc->event_list[i]->hw;\n\n\t\t\tif (c->weight != w)\n\t\t\t\tcontinue;\n\n\t\t\tfor_each_set_bit(j, c->idxmsk, X86_PMC_IDX_MAX) {\n\t\t\t\tif (!test_bit(j, used_mask))\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (j == X86_PMC_IDX_MAX)\n\t\t\t\tbreak;\n\n\t\t\t__set_bit(j, used_mask);\n\n\t\t\tif (assign)\n\t\t\t\tassign[i] = j;\n\t\t\tnum--;\n\t\t}\n\t}\ndone:\n\t/*\n\t * scheduling failed or is just a simulation,\n\t * free resources if necessary\n\t */\n\tif (!assign || num) {\n\t\tfor (i = 0; i < n; i++) {\n\t\t\tif (x86_pmu.put_event_constraints)\n\t\t\t\tx86_pmu.put_event_constraints(cpuc, cpuc->event_list[i]);\n\t\t}\n\t}\n\treturn num ? -ENOSPC : 0;\n}\n\n/*\n * dogrp: true if must collect siblings events (group)\n * returns total number of events and error code\n */\nstatic int collect_events(struct cpu_hw_events *cpuc, struct perf_event *leader, bool dogrp)\n{\n\tstruct perf_event *event;\n\tint n, max_count;\n\n\tmax_count = x86_pmu.num_counters + x86_pmu.num_counters_fixed;\n\n\t/* current number of events already accepted */\n\tn = cpuc->n_events;\n\n\tif (is_x86_event(leader)) {\n\t\tif (n >= max_count)\n\t\t\treturn -ENOSPC;\n\t\tcpuc->event_list[n] = leader;\n\t\tn++;\n\t}\n\tif (!dogrp)\n\t\treturn n;\n\n\tlist_for_each_entry(event, &leader->sibling_list, group_entry) {\n\t\tif (!is_x86_event(event) ||\n\t\t    event->state <= PERF_EVENT_STATE_OFF)\n\t\t\tcontinue;\n\n\t\tif (n >= max_count)\n\t\t\treturn -ENOSPC;\n\n\t\tcpuc->event_list[n] = event;\n\t\tn++;\n\t}\n\treturn n;\n}\n\nstatic inline void x86_assign_hw_event(struct perf_event *event,\n\t\t\t\tstruct cpu_hw_events *cpuc, int i)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\thwc->idx = cpuc->assign[i];\n\thwc->last_cpu = smp_processor_id();\n\thwc->last_tag = ++cpuc->tags[i];\n\n\tif (hwc->idx == X86_PMC_IDX_FIXED_BTS) {\n\t\thwc->config_base = 0;\n\t\thwc->event_base\t= 0;\n\t} else if (hwc->idx >= X86_PMC_IDX_FIXED) {\n\t\thwc->config_base = MSR_ARCH_PERFMON_FIXED_CTR_CTRL;\n\t\thwc->event_base = MSR_ARCH_PERFMON_FIXED_CTR0 + (hwc->idx - X86_PMC_IDX_FIXED);\n\t} else {\n\t\thwc->config_base = x86_pmu_config_addr(hwc->idx);\n\t\thwc->event_base  = x86_pmu_event_addr(hwc->idx);\n\t}\n}\n\nstatic inline int match_prev_assignment(struct hw_perf_event *hwc,\n\t\t\t\t\tstruct cpu_hw_events *cpuc,\n\t\t\t\t\tint i)\n{\n\treturn hwc->idx == cpuc->assign[i] &&\n\t\thwc->last_cpu == smp_processor_id() &&\n\t\thwc->last_tag == cpuc->tags[i];\n}\n\nstatic void x86_pmu_start(struct perf_event *event, int flags);\nstatic void x86_pmu_stop(struct perf_event *event, int flags);\n\nstatic void x86_pmu_enable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct perf_event *event;\n\tstruct hw_perf_event *hwc;\n\tint i, added = cpuc->n_added;\n\n\tif (!x86_pmu_initialized())\n\t\treturn;\n\n\tif (cpuc->enabled)\n\t\treturn;\n\n\tif (cpuc->n_added) {\n\t\tint n_running = cpuc->n_events - cpuc->n_added;\n\t\t/*\n\t\t * apply assignment obtained either from\n\t\t * hw_perf_group_sched_in() or x86_pmu_enable()\n\t\t *\n\t\t * step1: save events moving to new counters\n\t\t * step2: reprogram moved events into new counters\n\t\t */\n\t\tfor (i = 0; i < n_running; i++) {\n\t\t\tevent = cpuc->event_list[i];\n\t\t\thwc = &event->hw;\n\n\t\t\t/*\n\t\t\t * we can avoid reprogramming counter if:\n\t\t\t * - assigned same counter as last time\n\t\t\t * - running on same CPU as last time\n\t\t\t * - no other event has used the counter since\n\t\t\t */\n\t\t\tif (hwc->idx == -1 ||\n\t\t\t    match_prev_assignment(hwc, cpuc, i))\n\t\t\t\tcontinue;\n\n\t\t\t/*\n\t\t\t * Ensure we don't accidentally enable a stopped\n\t\t\t * counter simply because we rescheduled.\n\t\t\t */\n\t\t\tif (hwc->state & PERF_HES_STOPPED)\n\t\t\t\thwc->state |= PERF_HES_ARCH;\n\n\t\t\tx86_pmu_stop(event, PERF_EF_UPDATE);\n\t\t}\n\n\t\tfor (i = 0; i < cpuc->n_events; i++) {\n\t\t\tevent = cpuc->event_list[i];\n\t\t\thwc = &event->hw;\n\n\t\t\tif (!match_prev_assignment(hwc, cpuc, i))\n\t\t\t\tx86_assign_hw_event(event, cpuc, i);\n\t\t\telse if (i < n_running)\n\t\t\t\tcontinue;\n\n\t\t\tif (hwc->state & PERF_HES_ARCH)\n\t\t\t\tcontinue;\n\n\t\t\tx86_pmu_start(event, PERF_EF_RELOAD);\n\t\t}\n\t\tcpuc->n_added = 0;\n\t\tperf_events_lapic_init();\n\t}\n\n\tcpuc->enabled = 1;\n\tbarrier();\n\n\tx86_pmu.enable_all(added);\n}\n\nstatic inline void x86_pmu_disable_event(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\twrmsrl(hwc->config_base, hwc->config);\n}\n\nstatic DEFINE_PER_CPU(u64 [X86_PMC_IDX_MAX], pmc_prev_left);\n\n/*\n * Set the next IRQ period, based on the hwc->period_left value.\n * To be called with the event disabled in hw:\n */\nstatic int\nx86_perf_event_set_period(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\ts64 left = local64_read(&hwc->period_left);\n\ts64 period = hwc->sample_period;\n\tint ret = 0, idx = hwc->idx;\n\n\tif (idx == X86_PMC_IDX_FIXED_BTS)\n\t\treturn 0;\n\n\t/*\n\t * If we are way outside a reasonable range then just skip forward:\n\t */\n\tif (unlikely(left <= -period)) {\n\t\tleft = period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\tif (unlikely(left <= 0)) {\n\t\tleft += period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\t/*\n\t * Quirk: certain CPUs dont like it if just 1 hw_event is left:\n\t */\n\tif (unlikely(left < 2))\n\t\tleft = 2;\n\n\tif (left > x86_pmu.max_period)\n\t\tleft = x86_pmu.max_period;\n\n\tper_cpu(pmc_prev_left[idx], smp_processor_id()) = left;\n\n\t/*\n\t * The hw event starts counting from this event offset,\n\t * mark it to be able to extra future deltas:\n\t */\n\tlocal64_set(&hwc->prev_count, (u64)-left);\n\n\twrmsrl(hwc->event_base, (u64)(-left) & x86_pmu.cntval_mask);\n\n\t/*\n\t * Due to erratum on certan cpu we need\n\t * a second write to be sure the register\n\t * is updated properly\n\t */\n\tif (x86_pmu.perfctr_second_write) {\n\t\twrmsrl(hwc->event_base,\n\t\t\t(u64)(-left) & x86_pmu.cntval_mask);\n\t}\n\n\tperf_event_update_userpage(event);\n\n\treturn ret;\n}\n\nstatic void x86_pmu_enable_event(struct perf_event *event)\n{\n\tif (__this_cpu_read(cpu_hw_events.enabled))\n\t\t__x86_pmu_enable_event(&event->hw,\n\t\t\t\t       ARCH_PERFMON_EVENTSEL_ENABLE);\n}\n\n/*\n * Add a single event to the PMU.\n *\n * The event is added to the group of enabled events\n * but only if it can be scehduled with existing events.\n */\nstatic int x86_pmu_add(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct hw_perf_event *hwc;\n\tint assign[X86_PMC_IDX_MAX];\n\tint n, n0, ret;\n\n\thwc = &event->hw;\n\n\tperf_pmu_disable(event->pmu);\n\tn0 = cpuc->n_events;\n\tret = n = collect_events(cpuc, event, false);\n\tif (ret < 0)\n\t\tgoto out;\n\n\thwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;\n\tif (!(flags & PERF_EF_START))\n\t\thwc->state |= PERF_HES_ARCH;\n\n\t/*\n\t * If group events scheduling transaction was started,\n\t * skip the schedulability test here, it will be performed\n\t * at commit time (->commit_txn) as a whole\n\t */\n\tif (cpuc->group_flag & PERF_EVENT_TXN)\n\t\tgoto done_collect;\n\n\tret = x86_pmu.schedule_events(cpuc, n, assign);\n\tif (ret)\n\t\tgoto out;\n\t/*\n\t * copy new assignment, now we know it is possible\n\t * will be used by hw_perf_enable()\n\t */\n\tmemcpy(cpuc->assign, assign, n*sizeof(int));\n\ndone_collect:\n\tcpuc->n_events = n;\n\tcpuc->n_added += n - n0;\n\tcpuc->n_txn += n - n0;\n\n\tret = 0;\nout:\n\tperf_pmu_enable(event->pmu);\n\treturn ret;\n}\n\nstatic void x86_pmu_start(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint idx = event->hw.idx;\n\n\tif (WARN_ON_ONCE(!(event->hw.state & PERF_HES_STOPPED)))\n\t\treturn;\n\n\tif (WARN_ON_ONCE(idx == -1))\n\t\treturn;\n\n\tif (flags & PERF_EF_RELOAD) {\n\t\tWARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));\n\t\tx86_perf_event_set_period(event);\n\t}\n\n\tevent->hw.state = 0;\n\n\tcpuc->events[idx] = event;\n\t__set_bit(idx, cpuc->active_mask);\n\t__set_bit(idx, cpuc->running);\n\tx86_pmu.enable(event);\n\tperf_event_update_userpage(event);\n}\n\nvoid perf_event_print_debug(void)\n{\n\tu64 ctrl, status, overflow, pmc_ctrl, pmc_count, prev_left, fixed;\n\tu64 pebs;\n\tstruct cpu_hw_events *cpuc;\n\tunsigned long flags;\n\tint cpu, idx;\n\n\tif (!x86_pmu.num_counters)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tcpu = smp_processor_id();\n\tcpuc = &per_cpu(cpu_hw_events, cpu);\n\n\tif (x86_pmu.version >= 2) {\n\t\trdmsrl(MSR_CORE_PERF_GLOBAL_CTRL, ctrl);\n\t\trdmsrl(MSR_CORE_PERF_GLOBAL_STATUS, status);\n\t\trdmsrl(MSR_CORE_PERF_GLOBAL_OVF_CTRL, overflow);\n\t\trdmsrl(MSR_ARCH_PERFMON_FIXED_CTR_CTRL, fixed);\n\t\trdmsrl(MSR_IA32_PEBS_ENABLE, pebs);\n\n\t\tpr_info(\"\\n\");\n\t\tpr_info(\"CPU#%d: ctrl:       %016llx\\n\", cpu, ctrl);\n\t\tpr_info(\"CPU#%d: status:     %016llx\\n\", cpu, status);\n\t\tpr_info(\"CPU#%d: overflow:   %016llx\\n\", cpu, overflow);\n\t\tpr_info(\"CPU#%d: fixed:      %016llx\\n\", cpu, fixed);\n\t\tpr_info(\"CPU#%d: pebs:       %016llx\\n\", cpu, pebs);\n\t}\n\tpr_info(\"CPU#%d: active:     %016llx\\n\", cpu, *(u64 *)cpuc->active_mask);\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\trdmsrl(x86_pmu_config_addr(idx), pmc_ctrl);\n\t\trdmsrl(x86_pmu_event_addr(idx), pmc_count);\n\n\t\tprev_left = per_cpu(pmc_prev_left[idx], cpu);\n\n\t\tpr_info(\"CPU#%d:   gen-PMC%d ctrl:  %016llx\\n\",\n\t\t\tcpu, idx, pmc_ctrl);\n\t\tpr_info(\"CPU#%d:   gen-PMC%d count: %016llx\\n\",\n\t\t\tcpu, idx, pmc_count);\n\t\tpr_info(\"CPU#%d:   gen-PMC%d left:  %016llx\\n\",\n\t\t\tcpu, idx, prev_left);\n\t}\n\tfor (idx = 0; idx < x86_pmu.num_counters_fixed; idx++) {\n\t\trdmsrl(MSR_ARCH_PERFMON_FIXED_CTR0 + idx, pmc_count);\n\n\t\tpr_info(\"CPU#%d: fixed-PMC%d count: %016llx\\n\",\n\t\t\tcpu, idx, pmc_count);\n\t}\n\tlocal_irq_restore(flags);\n}\n\nstatic void x86_pmu_stop(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (__test_and_clear_bit(hwc->idx, cpuc->active_mask)) {\n\t\tx86_pmu.disable(event);\n\t\tcpuc->events[hwc->idx] = NULL;\n\t\tWARN_ON_ONCE(hwc->state & PERF_HES_STOPPED);\n\t\thwc->state |= PERF_HES_STOPPED;\n\t}\n\n\tif ((flags & PERF_EF_UPDATE) && !(hwc->state & PERF_HES_UPTODATE)) {\n\t\t/*\n\t\t * Drain the remaining delta count out of a event\n\t\t * that we are disabling:\n\t\t */\n\t\tx86_perf_event_update(event);\n\t\thwc->state |= PERF_HES_UPTODATE;\n\t}\n}\n\nstatic void x86_pmu_del(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint i;\n\n\t/*\n\t * If we're called during a txn, we don't need to do anything.\n\t * The events never got scheduled and ->cancel_txn will truncate\n\t * the event_list.\n\t */\n\tif (cpuc->group_flag & PERF_EVENT_TXN)\n\t\treturn;\n\n\tx86_pmu_stop(event, PERF_EF_UPDATE);\n\n\tfor (i = 0; i < cpuc->n_events; i++) {\n\t\tif (event == cpuc->event_list[i]) {\n\n\t\t\tif (x86_pmu.put_event_constraints)\n\t\t\t\tx86_pmu.put_event_constraints(cpuc, event);\n\n\t\t\twhile (++i < cpuc->n_events)\n\t\t\t\tcpuc->event_list[i-1] = cpuc->event_list[i];\n\n\t\t\t--cpuc->n_events;\n\t\t\tbreak;\n\t\t}\n\t}\n\tperf_event_update_userpage(event);\n}\n\nstatic int x86_pmu_handle_irq(struct pt_regs *regs)\n{\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct perf_event *event;\n\tint idx, handled = 0;\n\tu64 val;\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\n\t/*\n\t * Some chipsets need to unmask the LVTPC in a particular spot\n\t * inside the nmi handler.  As a result, the unmasking was pushed\n\t * into all the nmi handlers.\n\t *\n\t * This generic handler doesn't seem to have any issues where the\n\t * unmasking occurs so it was left at the top.\n\t */\n\tapic_write(APIC_LVTPC, APIC_DM_NMI);\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tif (!test_bit(idx, cpuc->active_mask)) {\n\t\t\t/*\n\t\t\t * Though we deactivated the counter some cpus\n\t\t\t * might still deliver spurious interrupts still\n\t\t\t * in flight. Catch them:\n\t\t\t */\n\t\t\tif (__test_and_clear_bit(idx, cpuc->running))\n\t\t\t\thandled++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tevent = cpuc->events[idx];\n\n\t\tval = x86_perf_event_update(event);\n\t\tif (val & (1ULL << (x86_pmu.cntval_bits - 1)))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * event overflow\n\t\t */\n\t\thandled++;\n\t\tdata.period\t= event->hw.last_period;\n\n\t\tif (!x86_perf_event_set_period(event))\n\t\t\tcontinue;\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tx86_pmu_stop(event, 0);\n\t}\n\n\tif (handled)\n\t\tinc_irq_stat(apic_perf_irqs);\n\n\treturn handled;\n}\n\nvoid perf_events_lapic_init(void)\n{\n\tif (!x86_pmu.apic || !x86_pmu_initialized())\n\t\treturn;\n\n\t/*\n\t * Always use NMI for PMU\n\t */\n\tapic_write(APIC_LVTPC, APIC_DM_NMI);\n}\n\nstruct pmu_nmi_state {\n\tunsigned int\tmarked;\n\tint\t\thandled;\n};\n\nstatic DEFINE_PER_CPU(struct pmu_nmi_state, pmu_nmi);\n\nstatic int __kprobes\nperf_event_nmi_handler(struct notifier_block *self,\n\t\t\t unsigned long cmd, void *__args)\n{\n\tstruct die_args *args = __args;\n\tunsigned int this_nmi;\n\tint handled;\n\n\tif (!atomic_read(&active_events))\n\t\treturn NOTIFY_DONE;\n\n\tswitch (cmd) {\n\tcase DIE_NMI:\n\t\tbreak;\n\tcase DIE_NMIUNKNOWN:\n\t\tthis_nmi = percpu_read(irq_stat.__nmi_count);\n\t\tif (this_nmi != __this_cpu_read(pmu_nmi.marked))\n\t\t\t/* let the kernel handle the unknown nmi */\n\t\t\treturn NOTIFY_DONE;\n\t\t/*\n\t\t * This one is a PMU back-to-back nmi. Two events\n\t\t * trigger 'simultaneously' raising two back-to-back\n\t\t * NMIs. If the first NMI handles both, the latter\n\t\t * will be empty and daze the CPU. So, we drop it to\n\t\t * avoid false-positive 'unknown nmi' messages.\n\t\t */\n\t\treturn NOTIFY_STOP;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n\n\thandled = x86_pmu.handle_irq(args->regs);\n\tif (!handled)\n\t\treturn NOTIFY_DONE;\n\n\tthis_nmi = percpu_read(irq_stat.__nmi_count);\n\tif ((handled > 1) ||\n\t\t/* the next nmi could be a back-to-back nmi */\n\t    ((__this_cpu_read(pmu_nmi.marked) == this_nmi) &&\n\t     (__this_cpu_read(pmu_nmi.handled) > 1))) {\n\t\t/*\n\t\t * We could have two subsequent back-to-back nmis: The\n\t\t * first handles more than one counter, the 2nd\n\t\t * handles only one counter and the 3rd handles no\n\t\t * counter.\n\t\t *\n\t\t * This is the 2nd nmi because the previous was\n\t\t * handling more than one counter. We will mark the\n\t\t * next (3rd) and then drop it if unhandled.\n\t\t */\n\t\t__this_cpu_write(pmu_nmi.marked, this_nmi + 1);\n\t\t__this_cpu_write(pmu_nmi.handled, handled);\n\t}\n\n\treturn NOTIFY_STOP;\n}\n\nstatic __read_mostly struct notifier_block perf_event_nmi_notifier = {\n\t.notifier_call\t\t= perf_event_nmi_handler,\n\t.next\t\t\t= NULL,\n\t.priority\t\t= NMI_LOCAL_LOW_PRIOR,\n};\n\nstatic struct event_constraint unconstrained;\nstatic struct event_constraint emptyconstraint;\n\nstatic struct event_constraint *\nx86_get_event_constraints(struct cpu_hw_events *cpuc, struct perf_event *event)\n{\n\tstruct event_constraint *c;\n\n\tif (x86_pmu.event_constraints) {\n\t\tfor_each_event_constraint(c, x86_pmu.event_constraints) {\n\t\t\tif ((event->hw.config & c->cmask) == c->code)\n\t\t\t\treturn c;\n\t\t}\n\t}\n\n\treturn &unconstrained;\n}\n\n#include \"perf_event_amd.c\"\n#include \"perf_event_p6.c\"\n#include \"perf_event_p4.c\"\n#include \"perf_event_intel_lbr.c\"\n#include \"perf_event_intel_ds.c\"\n#include \"perf_event_intel.c\"\n\nstatic int __cpuinit\nx86_pmu_notifier(struct notifier_block *self, unsigned long action, void *hcpu)\n{\n\tunsigned int cpu = (long)hcpu;\n\tint ret = NOTIFY_OK;\n\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_UP_PREPARE:\n\t\tif (x86_pmu.cpu_prepare)\n\t\t\tret = x86_pmu.cpu_prepare(cpu);\n\t\tbreak;\n\n\tcase CPU_STARTING:\n\t\tif (x86_pmu.cpu_starting)\n\t\t\tx86_pmu.cpu_starting(cpu);\n\t\tbreak;\n\n\tcase CPU_DYING:\n\t\tif (x86_pmu.cpu_dying)\n\t\t\tx86_pmu.cpu_dying(cpu);\n\t\tbreak;\n\n\tcase CPU_UP_CANCELED:\n\tcase CPU_DEAD:\n\t\tif (x86_pmu.cpu_dead)\n\t\t\tx86_pmu.cpu_dead(cpu);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic void __init pmu_check_apic(void)\n{\n\tif (cpu_has_apic)\n\t\treturn;\n\n\tx86_pmu.apic = 0;\n\tpr_info(\"no APIC, boot with the \\\"lapic\\\" boot parameter to force-enable it.\\n\");\n\tpr_info(\"no hardware sampling interrupt available.\\n\");\n}\n\nstatic int __init init_hw_perf_events(void)\n{\n\tstruct event_constraint *c;\n\tint err;\n\n\tpr_info(\"Performance Events: \");\n\n\tswitch (boot_cpu_data.x86_vendor) {\n\tcase X86_VENDOR_INTEL:\n\t\terr = intel_pmu_init();\n\t\tbreak;\n\tcase X86_VENDOR_AMD:\n\t\terr = amd_pmu_init();\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\tif (err != 0) {\n\t\tpr_cont(\"no PMU driver, software events only.\\n\");\n\t\treturn 0;\n\t}\n\n\tpmu_check_apic();\n\n\t/* sanity check that the hardware exists or is emulated */\n\tif (!check_hw_exists())\n\t\treturn 0;\n\n\tpr_cont(\"%s PMU driver.\\n\", x86_pmu.name);\n\n\tif (x86_pmu.quirks)\n\t\tx86_pmu.quirks();\n\n\tif (x86_pmu.num_counters > X86_PMC_MAX_GENERIC) {\n\t\tWARN(1, KERN_ERR \"hw perf events %d > max(%d), clipping!\",\n\t\t     x86_pmu.num_counters, X86_PMC_MAX_GENERIC);\n\t\tx86_pmu.num_counters = X86_PMC_MAX_GENERIC;\n\t}\n\tx86_pmu.intel_ctrl = (1 << x86_pmu.num_counters) - 1;\n\n\tif (x86_pmu.num_counters_fixed > X86_PMC_MAX_FIXED) {\n\t\tWARN(1, KERN_ERR \"hw perf events fixed %d > max(%d), clipping!\",\n\t\t     x86_pmu.num_counters_fixed, X86_PMC_MAX_FIXED);\n\t\tx86_pmu.num_counters_fixed = X86_PMC_MAX_FIXED;\n\t}\n\n\tx86_pmu.intel_ctrl |=\n\t\t((1LL << x86_pmu.num_counters_fixed)-1) << X86_PMC_IDX_FIXED;\n\n\tperf_events_lapic_init();\n\tregister_die_notifier(&perf_event_nmi_notifier);\n\n\tunconstrained = (struct event_constraint)\n\t\t__EVENT_CONSTRAINT(0, (1ULL << x86_pmu.num_counters) - 1,\n\t\t\t\t   0, x86_pmu.num_counters);\n\n\tif (x86_pmu.event_constraints) {\n\t\tfor_each_event_constraint(c, x86_pmu.event_constraints) {\n\t\t\tif (c->cmask != X86_RAW_EVENT_MASK)\n\t\t\t\tcontinue;\n\n\t\t\tc->idxmsk64 |= (1ULL << x86_pmu.num_counters) - 1;\n\t\t\tc->weight += x86_pmu.num_counters;\n\t\t}\n\t}\n\n\tpr_info(\"... version:                %d\\n\",     x86_pmu.version);\n\tpr_info(\"... bit width:              %d\\n\",     x86_pmu.cntval_bits);\n\tpr_info(\"... generic registers:      %d\\n\",     x86_pmu.num_counters);\n\tpr_info(\"... value mask:             %016Lx\\n\", x86_pmu.cntval_mask);\n\tpr_info(\"... max period:             %016Lx\\n\", x86_pmu.max_period);\n\tpr_info(\"... fixed-purpose events:   %d\\n\",     x86_pmu.num_counters_fixed);\n\tpr_info(\"... event mask:             %016Lx\\n\", x86_pmu.intel_ctrl);\n\n\tperf_pmu_register(&pmu, \"cpu\", PERF_TYPE_RAW);\n\tperf_cpu_notifier(x86_pmu_notifier);\n\n\treturn 0;\n}\nearly_initcall(init_hw_perf_events);\n\nstatic inline void x86_pmu_read(struct perf_event *event)\n{\n\tx86_perf_event_update(event);\n}\n\n/*\n * Start group events scheduling transaction\n * Set the flag to make pmu::enable() not perform the\n * schedulability test, it will be performed at commit time\n */\nstatic void x86_pmu_start_txn(struct pmu *pmu)\n{\n\tperf_pmu_disable(pmu);\n\t__this_cpu_or(cpu_hw_events.group_flag, PERF_EVENT_TXN);\n\t__this_cpu_write(cpu_hw_events.n_txn, 0);\n}\n\n/*\n * Stop group events scheduling transaction\n * Clear the flag and pmu::enable() will perform the\n * schedulability test.\n */\nstatic void x86_pmu_cancel_txn(struct pmu *pmu)\n{\n\t__this_cpu_and(cpu_hw_events.group_flag, ~PERF_EVENT_TXN);\n\t/*\n\t * Truncate the collected events.\n\t */\n\t__this_cpu_sub(cpu_hw_events.n_added, __this_cpu_read(cpu_hw_events.n_txn));\n\t__this_cpu_sub(cpu_hw_events.n_events, __this_cpu_read(cpu_hw_events.n_txn));\n\tperf_pmu_enable(pmu);\n}\n\n/*\n * Commit group events scheduling transaction\n * Perform the group schedulability test as a whole\n * Return 0 if success\n */\nstatic int x86_pmu_commit_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint assign[X86_PMC_IDX_MAX];\n\tint n, ret;\n\n\tn = cpuc->n_events;\n\n\tif (!x86_pmu_initialized())\n\t\treturn -EAGAIN;\n\n\tret = x86_pmu.schedule_events(cpuc, n, assign);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * copy new assignment, now we know it is possible\n\t * will be used by hw_perf_enable()\n\t */\n\tmemcpy(cpuc->assign, assign, n*sizeof(int));\n\n\tcpuc->group_flag &= ~PERF_EVENT_TXN;\n\tperf_pmu_enable(pmu);\n\treturn 0;\n}\n\n/*\n * validate that we can schedule this event\n */\nstatic int validate_event(struct perf_event *event)\n{\n\tstruct cpu_hw_events *fake_cpuc;\n\tstruct event_constraint *c;\n\tint ret = 0;\n\n\tfake_cpuc = kmalloc(sizeof(*fake_cpuc), GFP_KERNEL | __GFP_ZERO);\n\tif (!fake_cpuc)\n\t\treturn -ENOMEM;\n\n\tc = x86_pmu.get_event_constraints(fake_cpuc, event);\n\n\tif (!c || !c->weight)\n\t\tret = -ENOSPC;\n\n\tif (x86_pmu.put_event_constraints)\n\t\tx86_pmu.put_event_constraints(fake_cpuc, event);\n\n\tkfree(fake_cpuc);\n\n\treturn ret;\n}\n\n/*\n * validate a single event group\n *\n * validation include:\n *\t- check events are compatible which each other\n *\t- events do not compete for the same counter\n *\t- number of events <= number of counters\n *\n * validation ensures the group can be loaded onto the\n * PMU if it was the only group available.\n */\nstatic int validate_group(struct perf_event *event)\n{\n\tstruct perf_event *leader = event->group_leader;\n\tstruct cpu_hw_events *fake_cpuc;\n\tint ret, n;\n\n\tret = -ENOMEM;\n\tfake_cpuc = kmalloc(sizeof(*fake_cpuc), GFP_KERNEL | __GFP_ZERO);\n\tif (!fake_cpuc)\n\t\tgoto out;\n\n\t/*\n\t * the event is not yet connected with its\n\t * siblings therefore we must first collect\n\t * existing siblings, then add the new event\n\t * before we can simulate the scheduling\n\t */\n\tret = -ENOSPC;\n\tn = collect_events(fake_cpuc, leader, true);\n\tif (n < 0)\n\t\tgoto out_free;\n\n\tfake_cpuc->n_events = n;\n\tn = collect_events(fake_cpuc, event, false);\n\tif (n < 0)\n\t\tgoto out_free;\n\n\tfake_cpuc->n_events = n;\n\n\tret = x86_pmu.schedule_events(fake_cpuc, n, NULL);\n\nout_free:\n\tkfree(fake_cpuc);\nout:\n\treturn ret;\n}\n\nstatic int x86_pmu_event_init(struct perf_event *event)\n{\n\tstruct pmu *tmp;\n\tint err;\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_RAW:\n\tcase PERF_TYPE_HARDWARE:\n\tcase PERF_TYPE_HW_CACHE:\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\terr = __x86_pmu_event_init(event);\n\tif (!err) {\n\t\t/*\n\t\t * we temporarily connect event to its pmu\n\t\t * such that validate_group() can classify\n\t\t * it as an x86 event using is_x86_event()\n\t\t */\n\t\ttmp = event->pmu;\n\t\tevent->pmu = &pmu;\n\n\t\tif (event->group_leader != event)\n\t\t\terr = validate_group(event);\n\t\telse\n\t\t\terr = validate_event(event);\n\n\t\tevent->pmu = tmp;\n\t}\n\tif (err) {\n\t\tif (event->destroy)\n\t\t\tevent->destroy(event);\n\t}\n\n\treturn err;\n}\n\nstatic struct pmu pmu = {\n\t.pmu_enable\t= x86_pmu_enable,\n\t.pmu_disable\t= x86_pmu_disable,\n\n\t.event_init\t= x86_pmu_event_init,\n\n\t.add\t\t= x86_pmu_add,\n\t.del\t\t= x86_pmu_del,\n\t.start\t\t= x86_pmu_start,\n\t.stop\t\t= x86_pmu_stop,\n\t.read\t\t= x86_pmu_read,\n\n\t.start_txn\t= x86_pmu_start_txn,\n\t.cancel_txn\t= x86_pmu_cancel_txn,\n\t.commit_txn\t= x86_pmu_commit_txn,\n};\n\n/*\n * callchain support\n */\n\nstatic int backtrace_stack(void *data, char *name)\n{\n\treturn 0;\n}\n\nstatic void backtrace_address(void *data, unsigned long addr, int reliable)\n{\n\tstruct perf_callchain_entry *entry = data;\n\n\tperf_callchain_store(entry, addr);\n}\n\nstatic const struct stacktrace_ops backtrace_ops = {\n\t.stack\t\t\t= backtrace_stack,\n\t.address\t\t= backtrace_address,\n\t.walk_stack\t\t= print_context_stack_bp,\n};\n\nvoid\nperf_callchain_kernel(struct perf_callchain_entry *entry, struct pt_regs *regs)\n{\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest()) {\n\t\t/* TODO: We don't support guest os callchain now */\n\t\treturn;\n\t}\n\n\tperf_callchain_store(entry, regs->ip);\n\n\tdump_trace(NULL, regs, NULL, 0, &backtrace_ops, entry);\n}\n\n#ifdef CONFIG_COMPAT\nstatic inline int\nperf_callchain_user32(struct pt_regs *regs, struct perf_callchain_entry *entry)\n{\n\t/* 32-bit process in 64-bit kernel. */\n\tstruct stack_frame_ia32 frame;\n\tconst void __user *fp;\n\n\tif (!test_thread_flag(TIF_IA32))\n\t\treturn 0;\n\n\tfp = compat_ptr(regs->bp);\n\twhile (entry->nr < PERF_MAX_STACK_DEPTH) {\n\t\tunsigned long bytes;\n\t\tframe.next_frame     = 0;\n\t\tframe.return_address = 0;\n\n\t\tbytes = copy_from_user_nmi(&frame, fp, sizeof(frame));\n\t\tif (bytes != sizeof(frame))\n\t\t\tbreak;\n\n\t\tif (fp < compat_ptr(regs->sp))\n\t\t\tbreak;\n\n\t\tperf_callchain_store(entry, frame.return_address);\n\t\tfp = compat_ptr(frame.next_frame);\n\t}\n\treturn 1;\n}\n#else\nstatic inline int\nperf_callchain_user32(struct pt_regs *regs, struct perf_callchain_entry *entry)\n{\n    return 0;\n}\n#endif\n\nvoid\nperf_callchain_user(struct perf_callchain_entry *entry, struct pt_regs *regs)\n{\n\tstruct stack_frame frame;\n\tconst void __user *fp;\n\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest()) {\n\t\t/* TODO: We don't support guest os callchain now */\n\t\treturn;\n\t}\n\n\tfp = (void __user *)regs->bp;\n\n\tperf_callchain_store(entry, regs->ip);\n\n\tif (perf_callchain_user32(regs, entry))\n\t\treturn;\n\n\twhile (entry->nr < PERF_MAX_STACK_DEPTH) {\n\t\tunsigned long bytes;\n\t\tframe.next_frame\t     = NULL;\n\t\tframe.return_address = 0;\n\n\t\tbytes = copy_from_user_nmi(&frame, fp, sizeof(frame));\n\t\tif (bytes != sizeof(frame))\n\t\t\tbreak;\n\n\t\tif ((unsigned long)fp < regs->sp)\n\t\t\tbreak;\n\n\t\tperf_callchain_store(entry, frame.return_address);\n\t\tfp = frame.next_frame;\n\t}\n}\n\nunsigned long perf_instruction_pointer(struct pt_regs *regs)\n{\n\tunsigned long ip;\n\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest())\n\t\tip = perf_guest_cbs->get_guest_ip();\n\telse\n\t\tip = instruction_pointer(regs);\n\n\treturn ip;\n}\n\nunsigned long perf_misc_flags(struct pt_regs *regs)\n{\n\tint misc = 0;\n\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest()) {\n\t\tif (perf_guest_cbs->is_user_mode())\n\t\t\tmisc |= PERF_RECORD_MISC_GUEST_USER;\n\t\telse\n\t\t\tmisc |= PERF_RECORD_MISC_GUEST_KERNEL;\n\t} else {\n\t\tif (user_mode(regs))\n\t\t\tmisc |= PERF_RECORD_MISC_USER;\n\t\telse\n\t\t\tmisc |= PERF_RECORD_MISC_KERNEL;\n\t}\n\n\tif (regs->flags & PERF_EFLAGS_EXACT)\n\t\tmisc |= PERF_RECORD_MISC_EXACT_IP;\n\n\treturn misc;\n}\n", "#ifdef CONFIG_CPU_SUP_INTEL\n\n#define MAX_EXTRA_REGS 2\n\n/*\n * Per register state.\n */\nstruct er_account {\n\tint\t\t\tref;\t\t/* reference count */\n\tunsigned int\t\textra_reg;\t/* extra MSR number */\n\tu64\t\t\textra_config;\t/* extra MSR config */\n};\n\n/*\n * Per core state\n * This used to coordinate shared registers for HT threads.\n */\nstruct intel_percore {\n\traw_spinlock_t\t\tlock;\t\t/* protect structure */\n\tstruct er_account\tregs[MAX_EXTRA_REGS];\n\tint\t\t\trefcnt;\t\t/* number of threads */\n\tunsigned\t\tcore_id;\n};\n\n/*\n * Intel PerfMon, used on Core and later.\n */\nstatic u64 intel_perfmon_event_map[PERF_COUNT_HW_MAX] __read_mostly =\n{\n  [PERF_COUNT_HW_CPU_CYCLES]\t\t= 0x003c,\n  [PERF_COUNT_HW_INSTRUCTIONS]\t\t= 0x00c0,\n  [PERF_COUNT_HW_CACHE_REFERENCES]\t= 0x4f2e,\n  [PERF_COUNT_HW_CACHE_MISSES]\t\t= 0x412e,\n  [PERF_COUNT_HW_BRANCH_INSTRUCTIONS]\t= 0x00c4,\n  [PERF_COUNT_HW_BRANCH_MISSES]\t\t= 0x00c5,\n  [PERF_COUNT_HW_BUS_CYCLES]\t\t= 0x013c,\n};\n\nstatic struct event_constraint intel_core_event_constraints[] __read_mostly =\n{\n\tINTEL_EVENT_CONSTRAINT(0x11, 0x2), /* FP_ASSIST */\n\tINTEL_EVENT_CONSTRAINT(0x12, 0x2), /* MUL */\n\tINTEL_EVENT_CONSTRAINT(0x13, 0x2), /* DIV */\n\tINTEL_EVENT_CONSTRAINT(0x14, 0x1), /* CYCLES_DIV_BUSY */\n\tINTEL_EVENT_CONSTRAINT(0x19, 0x2), /* DELAYED_BYPASS */\n\tINTEL_EVENT_CONSTRAINT(0xc1, 0x1), /* FP_COMP_INSTR_RET */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_core2_event_constraints[] __read_mostly =\n{\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0), /* INST_RETIRED.ANY */\n\tFIXED_EVENT_CONSTRAINT(0x003c, 1), /* CPU_CLK_UNHALTED.CORE */\n\t/*\n\t * Core2 has Fixed Counter 2 listed as CPU_CLK_UNHALTED.REF and event\n\t * 0x013c as CPU_CLK_UNHALTED.BUS and specifies there is a fixed\n\t * ratio between these counters.\n\t */\n\t/* FIXED_EVENT_CONSTRAINT(0x013c, 2),  CPU_CLK_UNHALTED.REF */\n\tINTEL_EVENT_CONSTRAINT(0x10, 0x1), /* FP_COMP_OPS_EXE */\n\tINTEL_EVENT_CONSTRAINT(0x11, 0x2), /* FP_ASSIST */\n\tINTEL_EVENT_CONSTRAINT(0x12, 0x2), /* MUL */\n\tINTEL_EVENT_CONSTRAINT(0x13, 0x2), /* DIV */\n\tINTEL_EVENT_CONSTRAINT(0x14, 0x1), /* CYCLES_DIV_BUSY */\n\tINTEL_EVENT_CONSTRAINT(0x18, 0x1), /* IDLE_DURING_DIV */\n\tINTEL_EVENT_CONSTRAINT(0x19, 0x2), /* DELAYED_BYPASS */\n\tINTEL_EVENT_CONSTRAINT(0xa1, 0x1), /* RS_UOPS_DISPATCH_CYCLES */\n\tINTEL_EVENT_CONSTRAINT(0xc9, 0x1), /* ITLB_MISS_RETIRED (T30-9) */\n\tINTEL_EVENT_CONSTRAINT(0xcb, 0x1), /* MEM_LOAD_RETIRED */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_nehalem_event_constraints[] __read_mostly =\n{\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0), /* INST_RETIRED.ANY */\n\tFIXED_EVENT_CONSTRAINT(0x003c, 1), /* CPU_CLK_UNHALTED.CORE */\n\t/* FIXED_EVENT_CONSTRAINT(0x013c, 2), CPU_CLK_UNHALTED.REF */\n\tINTEL_EVENT_CONSTRAINT(0x40, 0x3), /* L1D_CACHE_LD */\n\tINTEL_EVENT_CONSTRAINT(0x41, 0x3), /* L1D_CACHE_ST */\n\tINTEL_EVENT_CONSTRAINT(0x42, 0x3), /* L1D_CACHE_LOCK */\n\tINTEL_EVENT_CONSTRAINT(0x43, 0x3), /* L1D_ALL_REF */\n\tINTEL_EVENT_CONSTRAINT(0x48, 0x3), /* L1D_PEND_MISS */\n\tINTEL_EVENT_CONSTRAINT(0x4e, 0x3), /* L1D_PREFETCH */\n\tINTEL_EVENT_CONSTRAINT(0x51, 0x3), /* L1D */\n\tINTEL_EVENT_CONSTRAINT(0x63, 0x3), /* CACHE_LOCK_CYCLES */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct extra_reg intel_nehalem_extra_regs[] __read_mostly =\n{\n\tINTEL_EVENT_EXTRA_REG(0xb7, MSR_OFFCORE_RSP_0, 0xffff),\n\tEVENT_EXTRA_END\n};\n\nstatic struct event_constraint intel_nehalem_percore_constraints[] __read_mostly =\n{\n\tINTEL_EVENT_CONSTRAINT(0xb7, 0),\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_westmere_event_constraints[] __read_mostly =\n{\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0), /* INST_RETIRED.ANY */\n\tFIXED_EVENT_CONSTRAINT(0x003c, 1), /* CPU_CLK_UNHALTED.CORE */\n\t/* FIXED_EVENT_CONSTRAINT(0x013c, 2), CPU_CLK_UNHALTED.REF */\n\tINTEL_EVENT_CONSTRAINT(0x51, 0x3), /* L1D */\n\tINTEL_EVENT_CONSTRAINT(0x60, 0x1), /* OFFCORE_REQUESTS_OUTSTANDING */\n\tINTEL_EVENT_CONSTRAINT(0x63, 0x3), /* CACHE_LOCK_CYCLES */\n\tINTEL_EVENT_CONSTRAINT(0xb3, 0x1), /* SNOOPQ_REQUEST_OUTSTANDING */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_snb_event_constraints[] __read_mostly =\n{\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0), /* INST_RETIRED.ANY */\n\tFIXED_EVENT_CONSTRAINT(0x003c, 1), /* CPU_CLK_UNHALTED.CORE */\n\t/* FIXED_EVENT_CONSTRAINT(0x013c, 2), CPU_CLK_UNHALTED.REF */\n\tINTEL_EVENT_CONSTRAINT(0x48, 0x4), /* L1D_PEND_MISS.PENDING */\n\tINTEL_EVENT_CONSTRAINT(0xb7, 0x1), /* OFF_CORE_RESPONSE_0 */\n\tINTEL_EVENT_CONSTRAINT(0xbb, 0x8), /* OFF_CORE_RESPONSE_1 */\n\tINTEL_UEVENT_CONSTRAINT(0x01c0, 0x2), /* INST_RETIRED.PREC_DIST */\n\tINTEL_EVENT_CONSTRAINT(0xcd, 0x8), /* MEM_TRANS_RETIRED.LOAD_LATENCY */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct extra_reg intel_westmere_extra_regs[] __read_mostly =\n{\n\tINTEL_EVENT_EXTRA_REG(0xb7, MSR_OFFCORE_RSP_0, 0xffff),\n\tINTEL_EVENT_EXTRA_REG(0xbb, MSR_OFFCORE_RSP_1, 0xffff),\n\tEVENT_EXTRA_END\n};\n\nstatic struct event_constraint intel_westmere_percore_constraints[] __read_mostly =\n{\n\tINTEL_EVENT_CONSTRAINT(0xb7, 0),\n\tINTEL_EVENT_CONSTRAINT(0xbb, 0),\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_gen_event_constraints[] __read_mostly =\n{\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0), /* INST_RETIRED.ANY */\n\tFIXED_EVENT_CONSTRAINT(0x003c, 1), /* CPU_CLK_UNHALTED.CORE */\n\t/* FIXED_EVENT_CONSTRAINT(0x013c, 2), CPU_CLK_UNHALTED.REF */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic u64 intel_pmu_event_map(int hw_event)\n{\n\treturn intel_perfmon_event_map[hw_event];\n}\n\nstatic __initconst const u64 snb_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0xf1d0, /* MEM_UOP_RETIRED.LOADS        */\n\t\t[ C(RESULT_MISS)   ] = 0x0151, /* L1D.REPLACEMENT              */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0xf2d0, /* MEM_UOP_RETIRED.STORES       */\n\t\t[ C(RESULT_MISS)   ] = 0x0851, /* L1D.ALL_M_REPLACEMENT        */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x024e, /* HW_PRE_REQ.DL1_MISS          */\n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0280, /* ICACHE.MISSES */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t/* OFFCORE_RESPONSE.ANY_DATA.LOCAL_CACHE */\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t/* OFFCORE_RESPONSE.ANY_DATA.ANY_LLC_MISS */\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t/* OFFCORE_RESPONSE.ANY_RFO.LOCAL_CACHE */\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t/* OFFCORE_RESPONSE.ANY_RFO.ANY_LLC_MISS */\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t/* OFFCORE_RESPONSE.PREFETCH.LOCAL_CACHE */\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t/* OFFCORE_RESPONSE.PREFETCH.ANY_LLC_MISS */\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x81d0, /* MEM_UOP_RETIRED.ALL_LOADS */\n\t\t[ C(RESULT_MISS)   ] = 0x0108, /* DTLB_LOAD_MISSES.CAUSES_A_WALK */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x82d0, /* MEM_UOP_RETIRED.ALL_STORES */\n\t\t[ C(RESULT_MISS)   ] = 0x0149, /* DTLB_STORE_MISSES.MISS_CAUSES_A_WALK */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x1085, /* ITLB_MISSES.STLB_HIT         */\n\t\t[ C(RESULT_MISS)   ] = 0x0185, /* ITLB_MISSES.CAUSES_A_WALK    */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c4, /* BR_INST_RETIRED.ALL_BRANCHES */\n\t\t[ C(RESULT_MISS)   ] = 0x00c5, /* BR_MISP_RETIRED.ALL_BRANCHES */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n};\n\nstatic __initconst const u64 westmere_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x010b, /* MEM_INST_RETIRED.LOADS       */\n\t\t[ C(RESULT_MISS)   ] = 0x0151, /* L1D.REPL                     */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x020b, /* MEM_INST_RETURED.STORES      */\n\t\t[ C(RESULT_MISS)   ] = 0x0251, /* L1D.M_REPL                   */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x014e, /* L1D_PREFETCH.REQUESTS        */\n\t\t[ C(RESULT_MISS)   ] = 0x024e, /* L1D_PREFETCH.MISS            */\n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0380, /* L1I.READS                    */\n\t\t[ C(RESULT_MISS)   ] = 0x0280, /* L1I.MISSES                   */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t/* OFFCORE_RESPONSE.ANY_DATA.LOCAL_CACHE */\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t/* OFFCORE_RESPONSE.ANY_DATA.ANY_LLC_MISS */\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t/*\n\t * Use RFO, not WRITEBACK, because a write miss would typically occur\n\t * on RFO.\n\t */\n\t[ C(OP_WRITE) ] = {\n\t\t/* OFFCORE_RESPONSE.ANY_RFO.LOCAL_CACHE */\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t/* OFFCORE_RESPONSE.ANY_RFO.ANY_LLC_MISS */\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t/* OFFCORE_RESPONSE.PREFETCH.LOCAL_CACHE */\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t/* OFFCORE_RESPONSE.PREFETCH.ANY_LLC_MISS */\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x010b, /* MEM_INST_RETIRED.LOADS       */\n\t\t[ C(RESULT_MISS)   ] = 0x0108, /* DTLB_LOAD_MISSES.ANY         */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x020b, /* MEM_INST_RETURED.STORES      */\n\t\t[ C(RESULT_MISS)   ] = 0x010c, /* MEM_STORE_RETIRED.DTLB_MISS  */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x01c0, /* INST_RETIRED.ANY_P           */\n\t\t[ C(RESULT_MISS)   ] = 0x0185, /* ITLB_MISSES.ANY              */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c4, /* BR_INST_RETIRED.ALL_BRANCHES */\n\t\t[ C(RESULT_MISS)   ] = 0x03e8, /* BPU_CLEARS.ANY               */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n};\n\n/*\n * Nehalem/Westmere MSR_OFFCORE_RESPONSE bits;\n * See IA32 SDM Vol 3B 30.6.1.3\n */\n\n#define NHM_DMND_DATA_RD\t(1 << 0)\n#define NHM_DMND_RFO\t\t(1 << 1)\n#define NHM_DMND_IFETCH\t\t(1 << 2)\n#define NHM_DMND_WB\t\t(1 << 3)\n#define NHM_PF_DATA_RD\t\t(1 << 4)\n#define NHM_PF_DATA_RFO\t\t(1 << 5)\n#define NHM_PF_IFETCH\t\t(1 << 6)\n#define NHM_OFFCORE_OTHER\t(1 << 7)\n#define NHM_UNCORE_HIT\t\t(1 << 8)\n#define NHM_OTHER_CORE_HIT_SNP\t(1 << 9)\n#define NHM_OTHER_CORE_HITM\t(1 << 10)\n        \t\t\t/* reserved */\n#define NHM_REMOTE_CACHE_FWD\t(1 << 12)\n#define NHM_REMOTE_DRAM\t\t(1 << 13)\n#define NHM_LOCAL_DRAM\t\t(1 << 14)\n#define NHM_NON_DRAM\t\t(1 << 15)\n\n#define NHM_ALL_DRAM\t\t(NHM_REMOTE_DRAM|NHM_LOCAL_DRAM)\n\n#define NHM_DMND_READ\t\t(NHM_DMND_DATA_RD)\n#define NHM_DMND_WRITE\t\t(NHM_DMND_RFO|NHM_DMND_WB)\n#define NHM_DMND_PREFETCH\t(NHM_PF_DATA_RD|NHM_PF_DATA_RFO)\n\n#define NHM_L3_HIT\t(NHM_UNCORE_HIT|NHM_OTHER_CORE_HIT_SNP|NHM_OTHER_CORE_HITM)\n#define NHM_L3_MISS\t(NHM_NON_DRAM|NHM_ALL_DRAM|NHM_REMOTE_CACHE_FWD)\n#define NHM_L3_ACCESS\t(NHM_L3_HIT|NHM_L3_MISS)\n\nstatic __initconst const u64 nehalem_hw_cache_extra_regs\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = NHM_DMND_READ|NHM_L3_ACCESS,\n\t\t[ C(RESULT_MISS)   ] = NHM_DMND_READ|NHM_L3_MISS,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = NHM_DMND_WRITE|NHM_L3_ACCESS,\n\t\t[ C(RESULT_MISS)   ] = NHM_DMND_WRITE|NHM_L3_MISS,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = NHM_DMND_PREFETCH|NHM_L3_ACCESS,\n\t\t[ C(RESULT_MISS)   ] = NHM_DMND_PREFETCH|NHM_L3_MISS,\n\t},\n }\n};\n\nstatic __initconst const u64 nehalem_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x010b, /* MEM_INST_RETIRED.LOADS       */\n\t\t[ C(RESULT_MISS)   ] = 0x0151, /* L1D.REPL                     */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x020b, /* MEM_INST_RETURED.STORES      */\n\t\t[ C(RESULT_MISS)   ] = 0x0251, /* L1D.M_REPL                   */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x014e, /* L1D_PREFETCH.REQUESTS        */\n\t\t[ C(RESULT_MISS)   ] = 0x024e, /* L1D_PREFETCH.MISS            */\n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0380, /* L1I.READS                    */\n\t\t[ C(RESULT_MISS)   ] = 0x0280, /* L1I.MISSES                   */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t/* OFFCORE_RESPONSE.ANY_DATA.LOCAL_CACHE */\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t/* OFFCORE_RESPONSE.ANY_DATA.ANY_LLC_MISS */\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t/*\n\t * Use RFO, not WRITEBACK, because a write miss would typically occur\n\t * on RFO.\n\t */\n\t[ C(OP_WRITE) ] = {\n\t\t/* OFFCORE_RESPONSE.ANY_RFO.LOCAL_CACHE */\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t/* OFFCORE_RESPONSE.ANY_RFO.ANY_LLC_MISS */\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t/* OFFCORE_RESPONSE.PREFETCH.LOCAL_CACHE */\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t/* OFFCORE_RESPONSE.PREFETCH.ANY_LLC_MISS */\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0f40, /* L1D_CACHE_LD.MESI   (alias)  */\n\t\t[ C(RESULT_MISS)   ] = 0x0108, /* DTLB_LOAD_MISSES.ANY         */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0f41, /* L1D_CACHE_ST.MESI   (alias)  */\n\t\t[ C(RESULT_MISS)   ] = 0x010c, /* MEM_STORE_RETIRED.DTLB_MISS  */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x01c0, /* INST_RETIRED.ANY_P           */\n\t\t[ C(RESULT_MISS)   ] = 0x20c8, /* ITLB_MISS_RETIRED            */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c4, /* BR_INST_RETIRED.ALL_BRANCHES */\n\t\t[ C(RESULT_MISS)   ] = 0x03e8, /* BPU_CLEARS.ANY               */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n};\n\nstatic __initconst const u64 core2_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0f40, /* L1D_CACHE_LD.MESI          */\n\t\t[ C(RESULT_MISS)   ] = 0x0140, /* L1D_CACHE_LD.I_STATE       */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0f41, /* L1D_CACHE_ST.MESI          */\n\t\t[ C(RESULT_MISS)   ] = 0x0141, /* L1D_CACHE_ST.I_STATE       */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x104e, /* L1D_PREFETCH.REQUESTS      */\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0080, /* L1I.READS                  */\n\t\t[ C(RESULT_MISS)   ] = 0x0081, /* L1I.MISSES                 */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x4f29, /* L2_LD.MESI                 */\n\t\t[ C(RESULT_MISS)   ] = 0x4129, /* L2_LD.ISTATE               */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x4f2A, /* L2_ST.MESI                 */\n\t\t[ C(RESULT_MISS)   ] = 0x412A, /* L2_ST.ISTATE               */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0f40, /* L1D_CACHE_LD.MESI  (alias) */\n\t\t[ C(RESULT_MISS)   ] = 0x0208, /* DTLB_MISSES.MISS_LD        */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0f41, /* L1D_CACHE_ST.MESI  (alias) */\n\t\t[ C(RESULT_MISS)   ] = 0x0808, /* DTLB_MISSES.MISS_ST        */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c0, /* INST_RETIRED.ANY_P         */\n\t\t[ C(RESULT_MISS)   ] = 0x1282, /* ITLBMISSES                 */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c4, /* BR_INST_RETIRED.ANY        */\n\t\t[ C(RESULT_MISS)   ] = 0x00c5, /* BP_INST_RETIRED.MISPRED    */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n};\n\nstatic __initconst const u64 atom_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x2140, /* L1D_CACHE.LD               */\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x2240, /* L1D_CACHE.ST               */\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0380, /* L1I.READS                  */\n\t\t[ C(RESULT_MISS)   ] = 0x0280, /* L1I.MISSES                 */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x4f29, /* L2_LD.MESI                 */\n\t\t[ C(RESULT_MISS)   ] = 0x4129, /* L2_LD.ISTATE               */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x4f2A, /* L2_ST.MESI                 */\n\t\t[ C(RESULT_MISS)   ] = 0x412A, /* L2_ST.ISTATE               */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x2140, /* L1D_CACHE_LD.MESI  (alias) */\n\t\t[ C(RESULT_MISS)   ] = 0x0508, /* DTLB_MISSES.MISS_LD        */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x2240, /* L1D_CACHE_ST.MESI  (alias) */\n\t\t[ C(RESULT_MISS)   ] = 0x0608, /* DTLB_MISSES.MISS_ST        */\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c0, /* INST_RETIRED.ANY_P         */\n\t\t[ C(RESULT_MISS)   ] = 0x0282, /* ITLB.MISSES                */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c4, /* BR_INST_RETIRED.ANY        */\n\t\t[ C(RESULT_MISS)   ] = 0x00c5, /* BP_INST_RETIRED.MISPRED    */\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n};\n\nstatic void intel_pmu_disable_all(void)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\twrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, 0);\n\n\tif (test_bit(X86_PMC_IDX_FIXED_BTS, cpuc->active_mask))\n\t\tintel_pmu_disable_bts();\n\n\tintel_pmu_pebs_disable_all();\n\tintel_pmu_lbr_disable_all();\n}\n\nstatic void intel_pmu_enable_all(int added)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\tintel_pmu_pebs_enable_all();\n\tintel_pmu_lbr_enable_all();\n\twrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, x86_pmu.intel_ctrl);\n\n\tif (test_bit(X86_PMC_IDX_FIXED_BTS, cpuc->active_mask)) {\n\t\tstruct perf_event *event =\n\t\t\tcpuc->events[X86_PMC_IDX_FIXED_BTS];\n\n\t\tif (WARN_ON_ONCE(!event))\n\t\t\treturn;\n\n\t\tintel_pmu_enable_bts(event->hw.config);\n\t}\n}\n\n/*\n * Workaround for:\n *   Intel Errata AAK100 (model 26)\n *   Intel Errata AAP53  (model 30)\n *   Intel Errata BD53   (model 44)\n *\n * The official story:\n *   These chips need to be 'reset' when adding counters by programming the\n *   magic three (non-counting) events 0x4300B5, 0x4300D2, and 0x4300B1 either\n *   in sequence on the same PMC or on different PMCs.\n *\n * In practise it appears some of these events do in fact count, and\n * we need to programm all 4 events.\n */\nstatic void intel_pmu_nhm_workaround(void)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstatic const unsigned long nhm_magic[4] = {\n\t\t0x4300B5,\n\t\t0x4300D2,\n\t\t0x4300B1,\n\t\t0x4300B1\n\t};\n\tstruct perf_event *event;\n\tint i;\n\n\t/*\n\t * The Errata requires below steps:\n\t * 1) Clear MSR_IA32_PEBS_ENABLE and MSR_CORE_PERF_GLOBAL_CTRL;\n\t * 2) Configure 4 PERFEVTSELx with the magic events and clear\n\t *    the corresponding PMCx;\n\t * 3) set bit0~bit3 of MSR_CORE_PERF_GLOBAL_CTRL;\n\t * 4) Clear MSR_CORE_PERF_GLOBAL_CTRL;\n\t * 5) Clear 4 pairs of ERFEVTSELx and PMCx;\n\t */\n\n\t/*\n\t * The real steps we choose are a little different from above.\n\t * A) To reduce MSR operations, we don't run step 1) as they\n\t *    are already cleared before this function is called;\n\t * B) Call x86_perf_event_update to save PMCx before configuring\n\t *    PERFEVTSELx with magic number;\n\t * C) With step 5), we do clear only when the PERFEVTSELx is\n\t *    not used currently.\n\t * D) Call x86_perf_event_set_period to restore PMCx;\n\t */\n\n\t/* We always operate 4 pairs of PERF Counters */\n\tfor (i = 0; i < 4; i++) {\n\t\tevent = cpuc->events[i];\n\t\tif (event)\n\t\t\tx86_perf_event_update(event);\n\t}\n\n\tfor (i = 0; i < 4; i++) {\n\t\twrmsrl(MSR_ARCH_PERFMON_EVENTSEL0 + i, nhm_magic[i]);\n\t\twrmsrl(MSR_ARCH_PERFMON_PERFCTR0 + i, 0x0);\n\t}\n\n\twrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, 0xf);\n\twrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, 0x0);\n\n\tfor (i = 0; i < 4; i++) {\n\t\tevent = cpuc->events[i];\n\n\t\tif (event) {\n\t\t\tx86_perf_event_set_period(event);\n\t\t\t__x86_pmu_enable_event(&event->hw,\n\t\t\t\t\tARCH_PERFMON_EVENTSEL_ENABLE);\n\t\t} else\n\t\t\twrmsrl(MSR_ARCH_PERFMON_EVENTSEL0 + i, 0x0);\n\t}\n}\n\nstatic void intel_pmu_nhm_enable_all(int added)\n{\n\tif (added)\n\t\tintel_pmu_nhm_workaround();\n\tintel_pmu_enable_all(added);\n}\n\nstatic inline u64 intel_pmu_get_status(void)\n{\n\tu64 status;\n\n\trdmsrl(MSR_CORE_PERF_GLOBAL_STATUS, status);\n\n\treturn status;\n}\n\nstatic inline void intel_pmu_ack_status(u64 ack)\n{\n\twrmsrl(MSR_CORE_PERF_GLOBAL_OVF_CTRL, ack);\n}\n\nstatic void intel_pmu_disable_fixed(struct hw_perf_event *hwc)\n{\n\tint idx = hwc->idx - X86_PMC_IDX_FIXED;\n\tu64 ctrl_val, mask;\n\n\tmask = 0xfULL << (idx * 4);\n\n\trdmsrl(hwc->config_base, ctrl_val);\n\tctrl_val &= ~mask;\n\twrmsrl(hwc->config_base, ctrl_val);\n}\n\nstatic void intel_pmu_disable_event(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (unlikely(hwc->idx == X86_PMC_IDX_FIXED_BTS)) {\n\t\tintel_pmu_disable_bts();\n\t\tintel_pmu_drain_bts_buffer();\n\t\treturn;\n\t}\n\n\tif (unlikely(hwc->config_base == MSR_ARCH_PERFMON_FIXED_CTR_CTRL)) {\n\t\tintel_pmu_disable_fixed(hwc);\n\t\treturn;\n\t}\n\n\tx86_pmu_disable_event(event);\n\n\tif (unlikely(event->attr.precise_ip))\n\t\tintel_pmu_pebs_disable(event);\n}\n\nstatic void intel_pmu_enable_fixed(struct hw_perf_event *hwc)\n{\n\tint idx = hwc->idx - X86_PMC_IDX_FIXED;\n\tu64 ctrl_val, bits, mask;\n\n\t/*\n\t * Enable IRQ generation (0x8),\n\t * and enable ring-3 counting (0x2) and ring-0 counting (0x1)\n\t * if requested:\n\t */\n\tbits = 0x8ULL;\n\tif (hwc->config & ARCH_PERFMON_EVENTSEL_USR)\n\t\tbits |= 0x2;\n\tif (hwc->config & ARCH_PERFMON_EVENTSEL_OS)\n\t\tbits |= 0x1;\n\n\t/*\n\t * ANY bit is supported in v3 and up\n\t */\n\tif (x86_pmu.version > 2 && hwc->config & ARCH_PERFMON_EVENTSEL_ANY)\n\t\tbits |= 0x4;\n\n\tbits <<= (idx * 4);\n\tmask = 0xfULL << (idx * 4);\n\n\trdmsrl(hwc->config_base, ctrl_val);\n\tctrl_val &= ~mask;\n\tctrl_val |= bits;\n\twrmsrl(hwc->config_base, ctrl_val);\n}\n\nstatic void intel_pmu_enable_event(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (unlikely(hwc->idx == X86_PMC_IDX_FIXED_BTS)) {\n\t\tif (!__this_cpu_read(cpu_hw_events.enabled))\n\t\t\treturn;\n\n\t\tintel_pmu_enable_bts(hwc->config);\n\t\treturn;\n\t}\n\n\tif (unlikely(hwc->config_base == MSR_ARCH_PERFMON_FIXED_CTR_CTRL)) {\n\t\tintel_pmu_enable_fixed(hwc);\n\t\treturn;\n\t}\n\n\tif (unlikely(event->attr.precise_ip))\n\t\tintel_pmu_pebs_enable(event);\n\n\t__x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);\n}\n\n/*\n * Save and restart an expired event. Called by NMI contexts,\n * so it has to be careful about preempting normal event ops:\n */\nstatic int intel_pmu_save_and_restart(struct perf_event *event)\n{\n\tx86_perf_event_update(event);\n\treturn x86_perf_event_set_period(event);\n}\n\nstatic void intel_pmu_reset(void)\n{\n\tstruct debug_store *ds = __this_cpu_read(cpu_hw_events.ds);\n\tunsigned long flags;\n\tint idx;\n\n\tif (!x86_pmu.num_counters)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tprintk(\"clearing PMU state on CPU#%d\\n\", smp_processor_id());\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tchecking_wrmsrl(x86_pmu_config_addr(idx), 0ull);\n\t\tchecking_wrmsrl(x86_pmu_event_addr(idx),  0ull);\n\t}\n\tfor (idx = 0; idx < x86_pmu.num_counters_fixed; idx++)\n\t\tchecking_wrmsrl(MSR_ARCH_PERFMON_FIXED_CTR0 + idx, 0ull);\n\n\tif (ds)\n\t\tds->bts_index = ds->bts_buffer_base;\n\n\tlocal_irq_restore(flags);\n}\n\n/*\n * This handler is triggered by the local APIC, so the APIC IRQ handling\n * rules apply:\n */\nstatic int intel_pmu_handle_irq(struct pt_regs *regs)\n{\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tint bit, loops;\n\tu64 status;\n\tint handled;\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\n\t/*\n\t * Some chipsets need to unmask the LVTPC in a particular spot\n\t * inside the nmi handler.  As a result, the unmasking was pushed\n\t * into all the nmi handlers.\n\t *\n\t * This handler doesn't seem to have any issues with the unmasking\n\t * so it was left at the top.\n\t */\n\tapic_write(APIC_LVTPC, APIC_DM_NMI);\n\n\tintel_pmu_disable_all();\n\thandled = intel_pmu_drain_bts_buffer();\n\tstatus = intel_pmu_get_status();\n\tif (!status) {\n\t\tintel_pmu_enable_all(0);\n\t\treturn handled;\n\t}\n\n\tloops = 0;\nagain:\n\tintel_pmu_ack_status(status);\n\tif (++loops > 100) {\n\t\tWARN_ONCE(1, \"perfevents: irq loop stuck!\\n\");\n\t\tperf_event_print_debug();\n\t\tintel_pmu_reset();\n\t\tgoto done;\n\t}\n\n\tinc_irq_stat(apic_perf_irqs);\n\n\tintel_pmu_lbr_read();\n\n\t/*\n\t * PEBS overflow sets bit 62 in the global status register\n\t */\n\tif (__test_and_clear_bit(62, (unsigned long *)&status)) {\n\t\thandled++;\n\t\tx86_pmu.drain_pebs(regs);\n\t}\n\n\tfor_each_set_bit(bit, (unsigned long *)&status, X86_PMC_IDX_MAX) {\n\t\tstruct perf_event *event = cpuc->events[bit];\n\n\t\thandled++;\n\n\t\tif (!test_bit(bit, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\tif (!intel_pmu_save_and_restart(event))\n\t\t\tcontinue;\n\n\t\tdata.period = event->hw.last_period;\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tx86_pmu_stop(event, 0);\n\t}\n\n\t/*\n\t * Repeat if there is more work to be done:\n\t */\n\tstatus = intel_pmu_get_status();\n\tif (status)\n\t\tgoto again;\n\ndone:\n\tintel_pmu_enable_all(0);\n\treturn handled;\n}\n\nstatic struct event_constraint *\nintel_bts_constraints(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tunsigned int hw_event, bts_event;\n\n\tif (event->attr.freq)\n\t\treturn NULL;\n\n\thw_event = hwc->config & INTEL_ARCH_EVENT_MASK;\n\tbts_event = x86_pmu.event_map(PERF_COUNT_HW_BRANCH_INSTRUCTIONS);\n\n\tif (unlikely(hw_event == bts_event && hwc->sample_period == 1))\n\t\treturn &bts_constraint;\n\n\treturn NULL;\n}\n\nstatic struct event_constraint *\nintel_percore_constraints(struct cpu_hw_events *cpuc, struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tunsigned int e = hwc->config & ARCH_PERFMON_EVENTSEL_EVENT;\n\tstruct event_constraint *c;\n\tstruct intel_percore *pc;\n\tstruct er_account *era;\n\tint i;\n\tint free_slot;\n\tint found;\n\n\tif (!x86_pmu.percore_constraints || hwc->extra_alloc)\n\t\treturn NULL;\n\n\tfor (c = x86_pmu.percore_constraints; c->cmask; c++) {\n\t\tif (e != c->code)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Allocate resource per core.\n\t\t */\n\t\tpc = cpuc->per_core;\n\t\tif (!pc)\n\t\t\tbreak;\n\t\tc = &emptyconstraint;\n\t\traw_spin_lock(&pc->lock);\n\t\tfree_slot = -1;\n\t\tfound = 0;\n\t\tfor (i = 0; i < MAX_EXTRA_REGS; i++) {\n\t\t\tera = &pc->regs[i];\n\t\t\tif (era->ref > 0 && hwc->extra_reg == era->extra_reg) {\n\t\t\t\t/* Allow sharing same config */\n\t\t\t\tif (hwc->extra_config == era->extra_config) {\n\t\t\t\t\tera->ref++;\n\t\t\t\t\tcpuc->percore_used = 1;\n\t\t\t\t\thwc->extra_alloc = 1;\n\t\t\t\t\tc = NULL;\n\t\t\t\t}\n\t\t\t\t/* else conflict */\n\t\t\t\tfound = 1;\n\t\t\t\tbreak;\n\t\t\t} else if (era->ref == 0 && free_slot == -1)\n\t\t\t\tfree_slot = i;\n\t\t}\n\t\tif (!found && free_slot != -1) {\n\t\t\tera = &pc->regs[free_slot];\n\t\t\tera->ref = 1;\n\t\t\tera->extra_reg = hwc->extra_reg;\n\t\t\tera->extra_config = hwc->extra_config;\n\t\t\tcpuc->percore_used = 1;\n\t\t\thwc->extra_alloc = 1;\n\t\t\tc = NULL;\n\t\t}\n\t\traw_spin_unlock(&pc->lock);\n\t\treturn c;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct event_constraint *\nintel_get_event_constraints(struct cpu_hw_events *cpuc, struct perf_event *event)\n{\n\tstruct event_constraint *c;\n\n\tc = intel_bts_constraints(event);\n\tif (c)\n\t\treturn c;\n\n\tc = intel_pebs_constraints(event);\n\tif (c)\n\t\treturn c;\n\n\tc = intel_percore_constraints(cpuc, event);\n\tif (c)\n\t\treturn c;\n\n\treturn x86_get_event_constraints(cpuc, event);\n}\n\nstatic void intel_put_event_constraints(struct cpu_hw_events *cpuc,\n\t\t\t\t\tstruct perf_event *event)\n{\n\tstruct extra_reg *er;\n\tstruct intel_percore *pc;\n\tstruct er_account *era;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint i, allref;\n\n\tif (!cpuc->percore_used)\n\t\treturn;\n\n\tfor (er = x86_pmu.extra_regs; er->msr; er++) {\n\t\tif (er->event != (hwc->config & er->config_mask))\n\t\t\tcontinue;\n\n\t\tpc = cpuc->per_core;\n\t\traw_spin_lock(&pc->lock);\n\t\tfor (i = 0; i < MAX_EXTRA_REGS; i++) {\n\t\t\tera = &pc->regs[i];\n\t\t\tif (era->ref > 0 &&\n\t\t\t    era->extra_config == hwc->extra_config &&\n\t\t\t    era->extra_reg == er->msr) {\n\t\t\t\tera->ref--;\n\t\t\t\thwc->extra_alloc = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tallref = 0;\n\t\tfor (i = 0; i < MAX_EXTRA_REGS; i++)\n\t\t\tallref += pc->regs[i].ref;\n\t\tif (allref == 0)\n\t\t\tcpuc->percore_used = 0;\n\t\traw_spin_unlock(&pc->lock);\n\t\tbreak;\n\t}\n}\n\nstatic int intel_pmu_hw_config(struct perf_event *event)\n{\n\tint ret = x86_pmu_hw_config(event);\n\n\tif (ret)\n\t\treturn ret;\n\n\tif (event->attr.precise_ip &&\n\t    (event->hw.config & X86_RAW_EVENT_MASK) == 0x003c) {\n\t\t/*\n\t\t * Use an alternative encoding for CPU_CLK_UNHALTED.THREAD_P\n\t\t * (0x003c) so that we can use it with PEBS.\n\t\t *\n\t\t * The regular CPU_CLK_UNHALTED.THREAD_P event (0x003c) isn't\n\t\t * PEBS capable. However we can use INST_RETIRED.ANY_P\n\t\t * (0x00c0), which is a PEBS capable event, to get the same\n\t\t * count.\n\t\t *\n\t\t * INST_RETIRED.ANY_P counts the number of cycles that retires\n\t\t * CNTMASK instructions. By setting CNTMASK to a value (16)\n\t\t * larger than the maximum number of instructions that can be\n\t\t * retired per cycle (4) and then inverting the condition, we\n\t\t * count all cycles that retire 16 or less instructions, which\n\t\t * is every cycle.\n\t\t *\n\t\t * Thereby we gain a PEBS capable cycle counter.\n\t\t */\n\t\tu64 alt_config = 0x108000c0; /* INST_RETIRED.TOTAL_CYCLES */\n\n\t\talt_config |= (event->hw.config & ~X86_RAW_EVENT_MASK);\n\t\tevent->hw.config = alt_config;\n\t}\n\n\tif (event->attr.type != PERF_TYPE_RAW)\n\t\treturn 0;\n\n\tif (!(event->attr.config & ARCH_PERFMON_EVENTSEL_ANY))\n\t\treturn 0;\n\n\tif (x86_pmu.version < 3)\n\t\treturn -EINVAL;\n\n\tif (perf_paranoid_cpu() && !capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\n\tevent->hw.config |= ARCH_PERFMON_EVENTSEL_ANY;\n\n\treturn 0;\n}\n\nstatic __initconst const struct x86_pmu core_pmu = {\n\t.name\t\t\t= \"core\",\n\t.handle_irq\t\t= x86_pmu_handle_irq,\n\t.disable_all\t\t= x86_pmu_disable_all,\n\t.enable_all\t\t= x86_pmu_enable_all,\n\t.enable\t\t\t= x86_pmu_enable_event,\n\t.disable\t\t= x86_pmu_disable_event,\n\t.hw_config\t\t= x86_pmu_hw_config,\n\t.schedule_events\t= x86_schedule_events,\n\t.eventsel\t\t= MSR_ARCH_PERFMON_EVENTSEL0,\n\t.perfctr\t\t= MSR_ARCH_PERFMON_PERFCTR0,\n\t.event_map\t\t= intel_pmu_event_map,\n\t.max_events\t\t= ARRAY_SIZE(intel_perfmon_event_map),\n\t.apic\t\t\t= 1,\n\t/*\n\t * Intel PMCs cannot be accessed sanely above 32 bit width,\n\t * so we install an artificial 1<<31 period regardless of\n\t * the generic event period:\n\t */\n\t.max_period\t\t= (1ULL << 31) - 1,\n\t.get_event_constraints\t= intel_get_event_constraints,\n\t.put_event_constraints\t= intel_put_event_constraints,\n\t.event_constraints\t= intel_core_event_constraints,\n};\n\nstatic int intel_pmu_cpu_prepare(int cpu)\n{\n\tstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\n\n\tif (!cpu_has_ht_siblings())\n\t\treturn NOTIFY_OK;\n\n\tcpuc->per_core = kzalloc_node(sizeof(struct intel_percore),\n\t\t\t\t      GFP_KERNEL, cpu_to_node(cpu));\n\tif (!cpuc->per_core)\n\t\treturn NOTIFY_BAD;\n\n\traw_spin_lock_init(&cpuc->per_core->lock);\n\tcpuc->per_core->core_id = -1;\n\treturn NOTIFY_OK;\n}\n\nstatic void intel_pmu_cpu_starting(int cpu)\n{\n\tstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\n\tint core_id = topology_core_id(cpu);\n\tint i;\n\n\tinit_debug_store_on_cpu(cpu);\n\t/*\n\t * Deal with CPUs that don't clear their LBRs on power-up.\n\t */\n\tintel_pmu_lbr_reset();\n\n\tif (!cpu_has_ht_siblings())\n\t\treturn;\n\n\tfor_each_cpu(i, topology_thread_cpumask(cpu)) {\n\t\tstruct intel_percore *pc = per_cpu(cpu_hw_events, i).per_core;\n\n\t\tif (pc && pc->core_id == core_id) {\n\t\t\tkfree(cpuc->per_core);\n\t\t\tcpuc->per_core = pc;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tcpuc->per_core->core_id = core_id;\n\tcpuc->per_core->refcnt++;\n}\n\nstatic void intel_pmu_cpu_dying(int cpu)\n{\n\tstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\n\tstruct intel_percore *pc = cpuc->per_core;\n\n\tif (pc) {\n\t\tif (pc->core_id == -1 || --pc->refcnt == 0)\n\t\t\tkfree(pc);\n\t\tcpuc->per_core = NULL;\n\t}\n\n\tfini_debug_store_on_cpu(cpu);\n}\n\nstatic __initconst const struct x86_pmu intel_pmu = {\n\t.name\t\t\t= \"Intel\",\n\t.handle_irq\t\t= intel_pmu_handle_irq,\n\t.disable_all\t\t= intel_pmu_disable_all,\n\t.enable_all\t\t= intel_pmu_enable_all,\n\t.enable\t\t\t= intel_pmu_enable_event,\n\t.disable\t\t= intel_pmu_disable_event,\n\t.hw_config\t\t= intel_pmu_hw_config,\n\t.schedule_events\t= x86_schedule_events,\n\t.eventsel\t\t= MSR_ARCH_PERFMON_EVENTSEL0,\n\t.perfctr\t\t= MSR_ARCH_PERFMON_PERFCTR0,\n\t.event_map\t\t= intel_pmu_event_map,\n\t.max_events\t\t= ARRAY_SIZE(intel_perfmon_event_map),\n\t.apic\t\t\t= 1,\n\t/*\n\t * Intel PMCs cannot be accessed sanely above 32 bit width,\n\t * so we install an artificial 1<<31 period regardless of\n\t * the generic event period:\n\t */\n\t.max_period\t\t= (1ULL << 31) - 1,\n\t.get_event_constraints\t= intel_get_event_constraints,\n\t.put_event_constraints\t= intel_put_event_constraints,\n\n\t.cpu_prepare\t\t= intel_pmu_cpu_prepare,\n\t.cpu_starting\t\t= intel_pmu_cpu_starting,\n\t.cpu_dying\t\t= intel_pmu_cpu_dying,\n};\n\nstatic void intel_clovertown_quirks(void)\n{\n\t/*\n\t * PEBS is unreliable due to:\n\t *\n\t *   AJ67  - PEBS may experience CPL leaks\n\t *   AJ68  - PEBS PMI may be delayed by one event\n\t *   AJ69  - GLOBAL_STATUS[62] will only be set when DEBUGCTL[12]\n\t *   AJ106 - FREEZE_LBRS_ON_PMI doesn't work in combination with PEBS\n\t *\n\t * AJ67 could be worked around by restricting the OS/USR flags.\n\t * AJ69 could be worked around by setting PMU_FREEZE_ON_PMI.\n\t *\n\t * AJ106 could possibly be worked around by not allowing LBR\n\t *       usage from PEBS, including the fixup.\n\t * AJ68  could possibly be worked around by always programming\n\t *\t a pebs_event_reset[0] value and coping with the lost events.\n\t *\n\t * But taken together it might just make sense to not enable PEBS on\n\t * these chips.\n\t */\n\tprintk(KERN_WARNING \"PEBS disabled due to CPU errata.\\n\");\n\tx86_pmu.pebs = 0;\n\tx86_pmu.pebs_constraints = NULL;\n}\n\nstatic __init int intel_pmu_init(void)\n{\n\tunion cpuid10_edx edx;\n\tunion cpuid10_eax eax;\n\tunsigned int unused;\n\tunsigned int ebx;\n\tint version;\n\n\tif (!cpu_has(&boot_cpu_data, X86_FEATURE_ARCH_PERFMON)) {\n\t\tswitch (boot_cpu_data.x86) {\n\t\tcase 0x6:\n\t\t\treturn p6_pmu_init();\n\t\tcase 0xf:\n\t\t\treturn p4_pmu_init();\n\t\t}\n\t\treturn -ENODEV;\n\t}\n\n\t/*\n\t * Check whether the Architectural PerfMon supports\n\t * Branch Misses Retired hw_event or not.\n\t */\n\tcpuid(10, &eax.full, &ebx, &unused, &edx.full);\n\tif (eax.split.mask_length <= ARCH_PERFMON_BRANCH_MISSES_RETIRED)\n\t\treturn -ENODEV;\n\n\tversion = eax.split.version_id;\n\tif (version < 2)\n\t\tx86_pmu = core_pmu;\n\telse\n\t\tx86_pmu = intel_pmu;\n\n\tx86_pmu.version\t\t\t= version;\n\tx86_pmu.num_counters\t\t= eax.split.num_counters;\n\tx86_pmu.cntval_bits\t\t= eax.split.bit_width;\n\tx86_pmu.cntval_mask\t\t= (1ULL << eax.split.bit_width) - 1;\n\n\t/*\n\t * Quirk: v2 perfmon does not report fixed-purpose events, so\n\t * assume at least 3 events:\n\t */\n\tif (version > 1)\n\t\tx86_pmu.num_counters_fixed = max((int)edx.split.num_counters_fixed, 3);\n\n\t/*\n\t * v2 and above have a perf capabilities MSR\n\t */\n\tif (version > 1) {\n\t\tu64 capabilities;\n\n\t\trdmsrl(MSR_IA32_PERF_CAPABILITIES, capabilities);\n\t\tx86_pmu.intel_cap.capabilities = capabilities;\n\t}\n\n\tintel_ds_init();\n\n\t/*\n\t * Install the hw-cache-events table:\n\t */\n\tswitch (boot_cpu_data.x86_model) {\n\tcase 14: /* 65 nm core solo/duo, \"Yonah\" */\n\t\tpr_cont(\"Core events, \");\n\t\tbreak;\n\n\tcase 15: /* original 65 nm celeron/pentium/core2/xeon, \"Merom\"/\"Conroe\" */\n\t\tx86_pmu.quirks = intel_clovertown_quirks;\n\tcase 22: /* single-core 65 nm celeron/core2solo \"Merom-L\"/\"Conroe-L\" */\n\tcase 23: /* current 45 nm celeron/core2/xeon \"Penryn\"/\"Wolfdale\" */\n\tcase 29: /* six-core 45 nm xeon \"Dunnington\" */\n\t\tmemcpy(hw_cache_event_ids, core2_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\n\t\tintel_pmu_lbr_init_core();\n\n\t\tx86_pmu.event_constraints = intel_core2_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_core2_pebs_event_constraints;\n\t\tpr_cont(\"Core2 events, \");\n\t\tbreak;\n\n\tcase 26: /* 45 nm nehalem, \"Bloomfield\" */\n\tcase 30: /* 45 nm nehalem, \"Lynnfield\" */\n\tcase 46: /* 45 nm nehalem-ex, \"Beckton\" */\n\t\tmemcpy(hw_cache_event_ids, nehalem_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, nehalem_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_nhm();\n\n\t\tx86_pmu.event_constraints = intel_nehalem_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_nehalem_pebs_event_constraints;\n\t\tx86_pmu.percore_constraints = intel_nehalem_percore_constraints;\n\t\tx86_pmu.enable_all = intel_pmu_nhm_enable_all;\n\t\tx86_pmu.extra_regs = intel_nehalem_extra_regs;\n\n\t\t/* UOPS_ISSUED.STALLED_CYCLES */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] = 0x180010e;\n\t\t/* UOPS_EXECUTED.CORE_ACTIVE_CYCLES,c=1,i=1 */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_BACKEND] = 0x1803fb1;\n\n\t\tif (ebx & 0x40) {\n\t\t\t/*\n\t\t\t * Erratum AAJ80 detected, we work it around by using\n\t\t\t * the BR_MISP_EXEC.ANY event. This will over-count\n\t\t\t * branch-misses, but it's still much better than the\n\t\t\t * architectural event which is often completely bogus:\n\t\t\t */\n\t\t\tintel_perfmon_event_map[PERF_COUNT_HW_BRANCH_MISSES] = 0x7f89;\n\n\t\t\tpr_cont(\"erratum AAJ80 worked around, \");\n\t\t}\n\t\tpr_cont(\"Nehalem events, \");\n\t\tbreak;\n\n\tcase 28: /* Atom */\n\t\tmemcpy(hw_cache_event_ids, atom_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\n\t\tintel_pmu_lbr_init_atom();\n\n\t\tx86_pmu.event_constraints = intel_gen_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_atom_pebs_event_constraints;\n\t\tpr_cont(\"Atom events, \");\n\t\tbreak;\n\n\tcase 37: /* 32 nm nehalem, \"Clarkdale\" */\n\tcase 44: /* 32 nm nehalem, \"Gulftown\" */\n\tcase 47: /* 32 nm Xeon E7 */\n\t\tmemcpy(hw_cache_event_ids, westmere_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, nehalem_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_nhm();\n\n\t\tx86_pmu.event_constraints = intel_westmere_event_constraints;\n\t\tx86_pmu.percore_constraints = intel_westmere_percore_constraints;\n\t\tx86_pmu.enable_all = intel_pmu_nhm_enable_all;\n\t\tx86_pmu.pebs_constraints = intel_westmere_pebs_event_constraints;\n\t\tx86_pmu.extra_regs = intel_westmere_extra_regs;\n\n\t\t/* UOPS_ISSUED.STALLED_CYCLES */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] = 0x180010e;\n\t\t/* UOPS_EXECUTED.CORE_ACTIVE_CYCLES,c=1,i=1 */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_BACKEND] = 0x1803fb1;\n\n\t\tpr_cont(\"Westmere events, \");\n\t\tbreak;\n\n\tcase 42: /* SandyBridge */\n\t\tmemcpy(hw_cache_event_ids, snb_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\n\t\tintel_pmu_lbr_init_nhm();\n\n\t\tx86_pmu.event_constraints = intel_snb_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_snb_pebs_events;\n\n\t\t/* UOPS_ISSUED.ANY,c=1,i=1 to count stall cycles */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] = 0x180010e;\n\t\t/* UOPS_DISPATCHED.THREAD,c=1,i=1 to count stall cycles*/\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_BACKEND] = 0x18001b1;\n\n\t\tpr_cont(\"SandyBridge events, \");\n\t\tbreak;\n\n\tdefault:\n\t\t/*\n\t\t * default constraints for v2 and up\n\t\t */\n\t\tx86_pmu.event_constraints = intel_gen_event_constraints;\n\t\tpr_cont(\"generic architected perfmon, \");\n\t}\n\treturn 0;\n}\n\n#else /* CONFIG_CPU_SUP_INTEL */\n\nstatic int intel_pmu_init(void)\n{\n\treturn 0;\n}\n\n#endif /* CONFIG_CPU_SUP_INTEL */\n", "#ifdef CONFIG_CPU_SUP_INTEL\n\n/* The maximal number of PEBS events: */\n#define MAX_PEBS_EVENTS\t\t4\n\n/* The size of a BTS record in bytes: */\n#define BTS_RECORD_SIZE\t\t24\n\n#define BTS_BUFFER_SIZE\t\t(PAGE_SIZE << 4)\n#define PEBS_BUFFER_SIZE\tPAGE_SIZE\n\n/*\n * pebs_record_32 for p4 and core not supported\n\nstruct pebs_record_32 {\n\tu32 flags, ip;\n\tu32 ax, bc, cx, dx;\n\tu32 si, di, bp, sp;\n};\n\n */\n\nstruct pebs_record_core {\n\tu64 flags, ip;\n\tu64 ax, bx, cx, dx;\n\tu64 si, di, bp, sp;\n\tu64 r8,  r9,  r10, r11;\n\tu64 r12, r13, r14, r15;\n};\n\nstruct pebs_record_nhm {\n\tu64 flags, ip;\n\tu64 ax, bx, cx, dx;\n\tu64 si, di, bp, sp;\n\tu64 r8,  r9,  r10, r11;\n\tu64 r12, r13, r14, r15;\n\tu64 status, dla, dse, lat;\n};\n\n/*\n * A debug store configuration.\n *\n * We only support architectures that use 64bit fields.\n */\nstruct debug_store {\n\tu64\tbts_buffer_base;\n\tu64\tbts_index;\n\tu64\tbts_absolute_maximum;\n\tu64\tbts_interrupt_threshold;\n\tu64\tpebs_buffer_base;\n\tu64\tpebs_index;\n\tu64\tpebs_absolute_maximum;\n\tu64\tpebs_interrupt_threshold;\n\tu64\tpebs_event_reset[MAX_PEBS_EVENTS];\n};\n\nstatic void init_debug_store_on_cpu(int cpu)\n{\n\tstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\n\n\tif (!ds)\n\t\treturn;\n\n\twrmsr_on_cpu(cpu, MSR_IA32_DS_AREA,\n\t\t     (u32)((u64)(unsigned long)ds),\n\t\t     (u32)((u64)(unsigned long)ds >> 32));\n}\n\nstatic void fini_debug_store_on_cpu(int cpu)\n{\n\tif (!per_cpu(cpu_hw_events, cpu).ds)\n\t\treturn;\n\n\twrmsr_on_cpu(cpu, MSR_IA32_DS_AREA, 0, 0);\n}\n\nstatic int alloc_pebs_buffer(int cpu)\n{\n\tstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\n\tint node = cpu_to_node(cpu);\n\tint max, thresh = 1; /* always use a single PEBS record */\n\tvoid *buffer;\n\n\tif (!x86_pmu.pebs)\n\t\treturn 0;\n\n\tbuffer = kmalloc_node(PEBS_BUFFER_SIZE, GFP_KERNEL | __GFP_ZERO, node);\n\tif (unlikely(!buffer))\n\t\treturn -ENOMEM;\n\n\tmax = PEBS_BUFFER_SIZE / x86_pmu.pebs_record_size;\n\n\tds->pebs_buffer_base = (u64)(unsigned long)buffer;\n\tds->pebs_index = ds->pebs_buffer_base;\n\tds->pebs_absolute_maximum = ds->pebs_buffer_base +\n\t\tmax * x86_pmu.pebs_record_size;\n\n\tds->pebs_interrupt_threshold = ds->pebs_buffer_base +\n\t\tthresh * x86_pmu.pebs_record_size;\n\n\treturn 0;\n}\n\nstatic void release_pebs_buffer(int cpu)\n{\n\tstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\n\n\tif (!ds || !x86_pmu.pebs)\n\t\treturn;\n\n\tkfree((void *)(unsigned long)ds->pebs_buffer_base);\n\tds->pebs_buffer_base = 0;\n}\n\nstatic int alloc_bts_buffer(int cpu)\n{\n\tstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\n\tint node = cpu_to_node(cpu);\n\tint max, thresh;\n\tvoid *buffer;\n\n\tif (!x86_pmu.bts)\n\t\treturn 0;\n\n\tbuffer = kmalloc_node(BTS_BUFFER_SIZE, GFP_KERNEL | __GFP_ZERO, node);\n\tif (unlikely(!buffer))\n\t\treturn -ENOMEM;\n\n\tmax = BTS_BUFFER_SIZE / BTS_RECORD_SIZE;\n\tthresh = max / 16;\n\n\tds->bts_buffer_base = (u64)(unsigned long)buffer;\n\tds->bts_index = ds->bts_buffer_base;\n\tds->bts_absolute_maximum = ds->bts_buffer_base +\n\t\tmax * BTS_RECORD_SIZE;\n\tds->bts_interrupt_threshold = ds->bts_absolute_maximum -\n\t\tthresh * BTS_RECORD_SIZE;\n\n\treturn 0;\n}\n\nstatic void release_bts_buffer(int cpu)\n{\n\tstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\n\n\tif (!ds || !x86_pmu.bts)\n\t\treturn;\n\n\tkfree((void *)(unsigned long)ds->bts_buffer_base);\n\tds->bts_buffer_base = 0;\n}\n\nstatic int alloc_ds_buffer(int cpu)\n{\n\tint node = cpu_to_node(cpu);\n\tstruct debug_store *ds;\n\n\tds = kmalloc_node(sizeof(*ds), GFP_KERNEL | __GFP_ZERO, node);\n\tif (unlikely(!ds))\n\t\treturn -ENOMEM;\n\n\tper_cpu(cpu_hw_events, cpu).ds = ds;\n\n\treturn 0;\n}\n\nstatic void release_ds_buffer(int cpu)\n{\n\tstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\n\n\tif (!ds)\n\t\treturn;\n\n\tper_cpu(cpu_hw_events, cpu).ds = NULL;\n\tkfree(ds);\n}\n\nstatic void release_ds_buffers(void)\n{\n\tint cpu;\n\n\tif (!x86_pmu.bts && !x86_pmu.pebs)\n\t\treturn;\n\n\tget_online_cpus();\n\tfor_each_online_cpu(cpu)\n\t\tfini_debug_store_on_cpu(cpu);\n\n\tfor_each_possible_cpu(cpu) {\n\t\trelease_pebs_buffer(cpu);\n\t\trelease_bts_buffer(cpu);\n\t\trelease_ds_buffer(cpu);\n\t}\n\tput_online_cpus();\n}\n\nstatic void reserve_ds_buffers(void)\n{\n\tint bts_err = 0, pebs_err = 0;\n\tint cpu;\n\n\tx86_pmu.bts_active = 0;\n\tx86_pmu.pebs_active = 0;\n\n\tif (!x86_pmu.bts && !x86_pmu.pebs)\n\t\treturn;\n\n\tif (!x86_pmu.bts)\n\t\tbts_err = 1;\n\n\tif (!x86_pmu.pebs)\n\t\tpebs_err = 1;\n\n\tget_online_cpus();\n\n\tfor_each_possible_cpu(cpu) {\n\t\tif (alloc_ds_buffer(cpu)) {\n\t\t\tbts_err = 1;\n\t\t\tpebs_err = 1;\n\t\t}\n\n\t\tif (!bts_err && alloc_bts_buffer(cpu))\n\t\t\tbts_err = 1;\n\n\t\tif (!pebs_err && alloc_pebs_buffer(cpu))\n\t\t\tpebs_err = 1;\n\n\t\tif (bts_err && pebs_err)\n\t\t\tbreak;\n\t}\n\n\tif (bts_err) {\n\t\tfor_each_possible_cpu(cpu)\n\t\t\trelease_bts_buffer(cpu);\n\t}\n\n\tif (pebs_err) {\n\t\tfor_each_possible_cpu(cpu)\n\t\t\trelease_pebs_buffer(cpu);\n\t}\n\n\tif (bts_err && pebs_err) {\n\t\tfor_each_possible_cpu(cpu)\n\t\t\trelease_ds_buffer(cpu);\n\t} else {\n\t\tif (x86_pmu.bts && !bts_err)\n\t\t\tx86_pmu.bts_active = 1;\n\n\t\tif (x86_pmu.pebs && !pebs_err)\n\t\t\tx86_pmu.pebs_active = 1;\n\n\t\tfor_each_online_cpu(cpu)\n\t\t\tinit_debug_store_on_cpu(cpu);\n\t}\n\n\tput_online_cpus();\n}\n\n/*\n * BTS\n */\n\nstatic struct event_constraint bts_constraint =\n\tEVENT_CONSTRAINT(0, 1ULL << X86_PMC_IDX_FIXED_BTS, 0);\n\nstatic void intel_pmu_enable_bts(u64 config)\n{\n\tunsigned long debugctlmsr;\n\n\tdebugctlmsr = get_debugctlmsr();\n\n\tdebugctlmsr |= DEBUGCTLMSR_TR;\n\tdebugctlmsr |= DEBUGCTLMSR_BTS;\n\tdebugctlmsr |= DEBUGCTLMSR_BTINT;\n\n\tif (!(config & ARCH_PERFMON_EVENTSEL_OS))\n\t\tdebugctlmsr |= DEBUGCTLMSR_BTS_OFF_OS;\n\n\tif (!(config & ARCH_PERFMON_EVENTSEL_USR))\n\t\tdebugctlmsr |= DEBUGCTLMSR_BTS_OFF_USR;\n\n\tupdate_debugctlmsr(debugctlmsr);\n}\n\nstatic void intel_pmu_disable_bts(void)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tunsigned long debugctlmsr;\n\n\tif (!cpuc->ds)\n\t\treturn;\n\n\tdebugctlmsr = get_debugctlmsr();\n\n\tdebugctlmsr &=\n\t\t~(DEBUGCTLMSR_TR | DEBUGCTLMSR_BTS | DEBUGCTLMSR_BTINT |\n\t\t  DEBUGCTLMSR_BTS_OFF_OS | DEBUGCTLMSR_BTS_OFF_USR);\n\n\tupdate_debugctlmsr(debugctlmsr);\n}\n\nstatic int intel_pmu_drain_bts_buffer(void)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct debug_store *ds = cpuc->ds;\n\tstruct bts_record {\n\t\tu64\tfrom;\n\t\tu64\tto;\n\t\tu64\tflags;\n\t};\n\tstruct perf_event *event = cpuc->events[X86_PMC_IDX_FIXED_BTS];\n\tstruct bts_record *at, *top;\n\tstruct perf_output_handle handle;\n\tstruct perf_event_header header;\n\tstruct perf_sample_data data;\n\tstruct pt_regs regs;\n\n\tif (!event)\n\t\treturn 0;\n\n\tif (!x86_pmu.bts_active)\n\t\treturn 0;\n\n\tat  = (struct bts_record *)(unsigned long)ds->bts_buffer_base;\n\ttop = (struct bts_record *)(unsigned long)ds->bts_index;\n\n\tif (top <= at)\n\t\treturn 0;\n\n\tds->bts_index = ds->bts_buffer_base;\n\n\tperf_sample_data_init(&data, 0);\n\tdata.period = event->hw.last_period;\n\tregs.ip     = 0;\n\n\t/*\n\t * Prepare a generic sample, i.e. fill in the invariant fields.\n\t * We will overwrite the from and to address before we output\n\t * the sample.\n\t */\n\tperf_prepare_sample(&header, &data, event, &regs);\n\n\tif (perf_output_begin(&handle, event, header.size * (top - at), 1))\n\t\treturn 1;\n\n\tfor (; at < top; at++) {\n\t\tdata.ip\t\t= at->from;\n\t\tdata.addr\t= at->to;\n\n\t\tperf_output_sample(&handle, &header, &data, event);\n\t}\n\n\tperf_output_end(&handle);\n\n\t/* There's new data available. */\n\tevent->hw.interrupts++;\n\tevent->pending_kill = POLL_IN;\n\treturn 1;\n}\n\n/*\n * PEBS\n */\nstatic struct event_constraint intel_core2_pebs_event_constraints[] = {\n\tINTEL_UEVENT_CONSTRAINT(0x00c0, 0x1), /* INST_RETIRED.ANY */\n\tINTEL_UEVENT_CONSTRAINT(0xfec1, 0x1), /* X87_OPS_RETIRED.ANY */\n\tINTEL_UEVENT_CONSTRAINT(0x00c5, 0x1), /* BR_INST_RETIRED.MISPRED */\n\tINTEL_UEVENT_CONSTRAINT(0x1fc7, 0x1), /* SIMD_INST_RETURED.ANY */\n\tINTEL_EVENT_CONSTRAINT(0xcb, 0x1),    /* MEM_LOAD_RETIRED.* */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_atom_pebs_event_constraints[] = {\n\tINTEL_UEVENT_CONSTRAINT(0x00c0, 0x1), /* INST_RETIRED.ANY */\n\tINTEL_UEVENT_CONSTRAINT(0x00c5, 0x1), /* MISPREDICTED_BRANCH_RETIRED */\n\tINTEL_EVENT_CONSTRAINT(0xcb, 0x1),    /* MEM_LOAD_RETIRED.* */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_nehalem_pebs_event_constraints[] = {\n\tINTEL_EVENT_CONSTRAINT(0x0b, 0xf),    /* MEM_INST_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0x0f, 0xf),    /* MEM_UNCORE_RETIRED.* */\n\tINTEL_UEVENT_CONSTRAINT(0x010c, 0xf), /* MEM_STORE_RETIRED.DTLB_MISS */\n\tINTEL_EVENT_CONSTRAINT(0xc0, 0xf),    /* INST_RETIRED.ANY */\n\tINTEL_EVENT_CONSTRAINT(0xc2, 0xf),    /* UOPS_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xc4, 0xf),    /* BR_INST_RETIRED.* */\n\tINTEL_UEVENT_CONSTRAINT(0x02c5, 0xf), /* BR_MISP_RETIRED.NEAR_CALL */\n\tINTEL_EVENT_CONSTRAINT(0xc7, 0xf),    /* SSEX_UOPS_RETIRED.* */\n\tINTEL_UEVENT_CONSTRAINT(0x20c8, 0xf), /* ITLB_MISS_RETIRED */\n\tINTEL_EVENT_CONSTRAINT(0xcb, 0xf),    /* MEM_LOAD_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xf7, 0xf),    /* FP_ASSIST.* */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_westmere_pebs_event_constraints[] = {\n\tINTEL_EVENT_CONSTRAINT(0x0b, 0xf),    /* MEM_INST_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0x0f, 0xf),    /* MEM_UNCORE_RETIRED.* */\n\tINTEL_UEVENT_CONSTRAINT(0x010c, 0xf), /* MEM_STORE_RETIRED.DTLB_MISS */\n\tINTEL_EVENT_CONSTRAINT(0xc0, 0xf),    /* INSTR_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xc2, 0xf),    /* UOPS_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xc4, 0xf),    /* BR_INST_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xc5, 0xf),    /* BR_MISP_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xc7, 0xf),    /* SSEX_UOPS_RETIRED.* */\n\tINTEL_UEVENT_CONSTRAINT(0x20c8, 0xf), /* ITLB_MISS_RETIRED */\n\tINTEL_EVENT_CONSTRAINT(0xcb, 0xf),    /* MEM_LOAD_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xf7, 0xf),    /* FP_ASSIST.* */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_snb_pebs_events[] = {\n\tINTEL_UEVENT_CONSTRAINT(0x01c0, 0x2), /* INST_RETIRED.PRECDIST */\n\tINTEL_UEVENT_CONSTRAINT(0x01c2, 0xf), /* UOPS_RETIRED.ALL */\n\tINTEL_UEVENT_CONSTRAINT(0x02c2, 0xf), /* UOPS_RETIRED.RETIRE_SLOTS */\n\tINTEL_EVENT_CONSTRAINT(0xc4, 0xf),    /* BR_INST_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xc5, 0xf),    /* BR_MISP_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xcd, 0x8),    /* MEM_TRANS_RETIRED.* */\n\tINTEL_UEVENT_CONSTRAINT(0x11d0, 0xf), /* MEM_UOP_RETIRED.STLB_MISS_LOADS */\n\tINTEL_UEVENT_CONSTRAINT(0x12d0, 0xf), /* MEM_UOP_RETIRED.STLB_MISS_STORES */\n\tINTEL_UEVENT_CONSTRAINT(0x21d0, 0xf), /* MEM_UOP_RETIRED.LOCK_LOADS */\n\tINTEL_UEVENT_CONSTRAINT(0x22d0, 0xf), /* MEM_UOP_RETIRED.LOCK_STORES */\n\tINTEL_UEVENT_CONSTRAINT(0x41d0, 0xf), /* MEM_UOP_RETIRED.SPLIT_LOADS */\n\tINTEL_UEVENT_CONSTRAINT(0x42d0, 0xf), /* MEM_UOP_RETIRED.SPLIT_STORES */\n\tINTEL_UEVENT_CONSTRAINT(0x81d0, 0xf), /* MEM_UOP_RETIRED.ANY_LOADS */\n\tINTEL_UEVENT_CONSTRAINT(0x82d0, 0xf), /* MEM_UOP_RETIRED.ANY_STORES */\n\tINTEL_EVENT_CONSTRAINT(0xd1, 0xf),    /* MEM_LOAD_UOPS_RETIRED.* */\n\tINTEL_EVENT_CONSTRAINT(0xd2, 0xf),    /* MEM_LOAD_UOPS_LLC_HIT_RETIRED.* */\n\tINTEL_UEVENT_CONSTRAINT(0x02d4, 0xf), /* MEM_LOAD_UOPS_MISC_RETIRED.LLC_MISS */\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint *\nintel_pebs_constraints(struct perf_event *event)\n{\n\tstruct event_constraint *c;\n\n\tif (!event->attr.precise_ip)\n\t\treturn NULL;\n\n\tif (x86_pmu.pebs_constraints) {\n\t\tfor_each_event_constraint(c, x86_pmu.pebs_constraints) {\n\t\t\tif ((event->hw.config & c->cmask) == c->code)\n\t\t\t\treturn c;\n\t\t}\n\t}\n\n\treturn &emptyconstraint;\n}\n\nstatic void intel_pmu_pebs_enable(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\thwc->config &= ~ARCH_PERFMON_EVENTSEL_INT;\n\n\tcpuc->pebs_enabled |= 1ULL << hwc->idx;\n\tWARN_ON_ONCE(cpuc->enabled);\n\n\tif (x86_pmu.intel_cap.pebs_trap && event->attr.precise_ip > 1)\n\t\tintel_pmu_lbr_enable(event);\n}\n\nstatic void intel_pmu_pebs_disable(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tcpuc->pebs_enabled &= ~(1ULL << hwc->idx);\n\tif (cpuc->enabled)\n\t\twrmsrl(MSR_IA32_PEBS_ENABLE, cpuc->pebs_enabled);\n\n\thwc->config |= ARCH_PERFMON_EVENTSEL_INT;\n\n\tif (x86_pmu.intel_cap.pebs_trap && event->attr.precise_ip > 1)\n\t\tintel_pmu_lbr_disable(event);\n}\n\nstatic void intel_pmu_pebs_enable_all(void)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\tif (cpuc->pebs_enabled)\n\t\twrmsrl(MSR_IA32_PEBS_ENABLE, cpuc->pebs_enabled);\n}\n\nstatic void intel_pmu_pebs_disable_all(void)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\n\tif (cpuc->pebs_enabled)\n\t\twrmsrl(MSR_IA32_PEBS_ENABLE, 0);\n}\n\n#include <asm/insn.h>\n\nstatic inline bool kernel_ip(unsigned long ip)\n{\n#ifdef CONFIG_X86_32\n\treturn ip > PAGE_OFFSET;\n#else\n\treturn (long)ip < 0;\n#endif\n}\n\nstatic int intel_pmu_pebs_fixup_ip(struct pt_regs *regs)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tunsigned long from = cpuc->lbr_entries[0].from;\n\tunsigned long old_to, to = cpuc->lbr_entries[0].to;\n\tunsigned long ip = regs->ip;\n\n\t/*\n\t * We don't need to fixup if the PEBS assist is fault like\n\t */\n\tif (!x86_pmu.intel_cap.pebs_trap)\n\t\treturn 1;\n\n\t/*\n\t * No LBR entry, no basic block, no rewinding\n\t */\n\tif (!cpuc->lbr_stack.nr || !from || !to)\n\t\treturn 0;\n\n\t/*\n\t * Basic blocks should never cross user/kernel boundaries\n\t */\n\tif (kernel_ip(ip) != kernel_ip(to))\n\t\treturn 0;\n\n\t/*\n\t * unsigned math, either ip is before the start (impossible) or\n\t * the basic block is larger than 1 page (sanity)\n\t */\n\tif ((ip - to) > PAGE_SIZE)\n\t\treturn 0;\n\n\t/*\n\t * We sampled a branch insn, rewind using the LBR stack\n\t */\n\tif (ip == to) {\n\t\tregs->ip = from;\n\t\treturn 1;\n\t}\n\n\tdo {\n\t\tstruct insn insn;\n\t\tu8 buf[MAX_INSN_SIZE];\n\t\tvoid *kaddr;\n\n\t\told_to = to;\n\t\tif (!kernel_ip(ip)) {\n\t\t\tint bytes, size = MAX_INSN_SIZE;\n\n\t\t\tbytes = copy_from_user_nmi(buf, (void __user *)to, size);\n\t\t\tif (bytes != size)\n\t\t\t\treturn 0;\n\n\t\t\tkaddr = buf;\n\t\t} else\n\t\t\tkaddr = (void *)to;\n\n\t\tkernel_insn_init(&insn, kaddr);\n\t\tinsn_get_length(&insn);\n\t\tto += insn.length;\n\t} while (to < ip);\n\n\tif (to == ip) {\n\t\tregs->ip = old_to;\n\t\treturn 1;\n\t}\n\n\t/*\n\t * Even though we decoded the basic block, the instruction stream\n\t * never matched the given IP, either the TO or the IP got corrupted.\n\t */\n\treturn 0;\n}\n\nstatic int intel_pmu_save_and_restart(struct perf_event *event);\n\nstatic void __intel_pmu_pebs_event(struct perf_event *event,\n\t\t\t\t   struct pt_regs *iregs, void *__pebs)\n{\n\t/*\n\t * We cast to pebs_record_core since that is a subset of\n\t * both formats and we don't use the other fields in this\n\t * routine.\n\t */\n\tstruct pebs_record_core *pebs = __pebs;\n\tstruct perf_sample_data data;\n\tstruct pt_regs regs;\n\n\tif (!intel_pmu_save_and_restart(event))\n\t\treturn;\n\n\tperf_sample_data_init(&data, 0);\n\tdata.period = event->hw.last_period;\n\n\t/*\n\t * We use the interrupt regs as a base because the PEBS record\n\t * does not contain a full regs set, specifically it seems to\n\t * lack segment descriptors, which get used by things like\n\t * user_mode().\n\t *\n\t * In the simple case fix up only the IP and BP,SP regs, for\n\t * PERF_SAMPLE_IP and PERF_SAMPLE_CALLCHAIN to function properly.\n\t * A possible PERF_SAMPLE_REGS will have to transfer all regs.\n\t */\n\tregs = *iregs;\n\tregs.ip = pebs->ip;\n\tregs.bp = pebs->bp;\n\tregs.sp = pebs->sp;\n\n\tif (event->attr.precise_ip > 1 && intel_pmu_pebs_fixup_ip(&regs))\n\t\tregs.flags |= PERF_EFLAGS_EXACT;\n\telse\n\t\tregs.flags &= ~PERF_EFLAGS_EXACT;\n\n\tif (perf_event_overflow(event, &data, &regs))\n\t\tx86_pmu_stop(event, 0);\n}\n\nstatic void intel_pmu_drain_pebs_core(struct pt_regs *iregs)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct debug_store *ds = cpuc->ds;\n\tstruct perf_event *event = cpuc->events[0]; /* PMC0 only */\n\tstruct pebs_record_core *at, *top;\n\tint n;\n\n\tif (!x86_pmu.pebs_active)\n\t\treturn;\n\n\tat  = (struct pebs_record_core *)(unsigned long)ds->pebs_buffer_base;\n\ttop = (struct pebs_record_core *)(unsigned long)ds->pebs_index;\n\n\t/*\n\t * Whatever else happens, drain the thing\n\t */\n\tds->pebs_index = ds->pebs_buffer_base;\n\n\tif (!test_bit(0, cpuc->active_mask))\n\t\treturn;\n\n\tWARN_ON_ONCE(!event);\n\n\tif (!event->attr.precise_ip)\n\t\treturn;\n\n\tn = top - at;\n\tif (n <= 0)\n\t\treturn;\n\n\t/*\n\t * Should not happen, we program the threshold at 1 and do not\n\t * set a reset value.\n\t */\n\tWARN_ON_ONCE(n > 1);\n\tat += n - 1;\n\n\t__intel_pmu_pebs_event(event, iregs, at);\n}\n\nstatic void intel_pmu_drain_pebs_nhm(struct pt_regs *iregs)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tstruct debug_store *ds = cpuc->ds;\n\tstruct pebs_record_nhm *at, *top;\n\tstruct perf_event *event = NULL;\n\tu64 status = 0;\n\tint bit, n;\n\n\tif (!x86_pmu.pebs_active)\n\t\treturn;\n\n\tat  = (struct pebs_record_nhm *)(unsigned long)ds->pebs_buffer_base;\n\ttop = (struct pebs_record_nhm *)(unsigned long)ds->pebs_index;\n\n\tds->pebs_index = ds->pebs_buffer_base;\n\n\tn = top - at;\n\tif (n <= 0)\n\t\treturn;\n\n\t/*\n\t * Should not happen, we program the threshold at 1 and do not\n\t * set a reset value.\n\t */\n\tWARN_ON_ONCE(n > MAX_PEBS_EVENTS);\n\n\tfor ( ; at < top; at++) {\n\t\tfor_each_set_bit(bit, (unsigned long *)&at->status, MAX_PEBS_EVENTS) {\n\t\t\tevent = cpuc->events[bit];\n\t\t\tif (!test_bit(bit, cpuc->active_mask))\n\t\t\t\tcontinue;\n\n\t\t\tWARN_ON_ONCE(!event);\n\n\t\t\tif (!event->attr.precise_ip)\n\t\t\t\tcontinue;\n\n\t\t\tif (__test_and_set_bit(bit, (unsigned long *)&status))\n\t\t\t\tcontinue;\n\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!event || bit >= MAX_PEBS_EVENTS)\n\t\t\tcontinue;\n\n\t\t__intel_pmu_pebs_event(event, iregs, at);\n\t}\n}\n\n/*\n * BTS, PEBS probe and setup\n */\n\nstatic void intel_ds_init(void)\n{\n\t/*\n\t * No support for 32bit formats\n\t */\n\tif (!boot_cpu_has(X86_FEATURE_DTES64))\n\t\treturn;\n\n\tx86_pmu.bts  = boot_cpu_has(X86_FEATURE_BTS);\n\tx86_pmu.pebs = boot_cpu_has(X86_FEATURE_PEBS);\n\tif (x86_pmu.pebs) {\n\t\tchar pebs_type = x86_pmu.intel_cap.pebs_trap ?  '+' : '-';\n\t\tint format = x86_pmu.intel_cap.pebs_format;\n\n\t\tswitch (format) {\n\t\tcase 0:\n\t\t\tprintk(KERN_CONT \"PEBS fmt0%c, \", pebs_type);\n\t\t\tx86_pmu.pebs_record_size = sizeof(struct pebs_record_core);\n\t\t\tx86_pmu.drain_pebs = intel_pmu_drain_pebs_core;\n\t\t\tbreak;\n\n\t\tcase 1:\n\t\t\tprintk(KERN_CONT \"PEBS fmt1%c, \", pebs_type);\n\t\t\tx86_pmu.pebs_record_size = sizeof(struct pebs_record_nhm);\n\t\t\tx86_pmu.drain_pebs = intel_pmu_drain_pebs_nhm;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tprintk(KERN_CONT \"no PEBS fmt%d%c, \", format, pebs_type);\n\t\t\tx86_pmu.pebs = 0;\n\t\t}\n\t}\n}\n\n#else /* CONFIG_CPU_SUP_INTEL */\n\nstatic void reserve_ds_buffers(void)\n{\n}\n\nstatic void release_ds_buffers(void)\n{\n}\n\n#endif /* CONFIG_CPU_SUP_INTEL */\n", "/*\n * Netburst Performance Events (P4, old Xeon)\n *\n *  Copyright (C) 2010 Parallels, Inc., Cyrill Gorcunov <gorcunov@openvz.org>\n *  Copyright (C) 2010 Intel Corporation, Lin Ming <ming.m.lin@intel.com>\n *\n *  For licencing details see kernel-base/COPYING\n */\n\n#ifdef CONFIG_CPU_SUP_INTEL\n\n#include <asm/perf_event_p4.h>\n\n#define P4_CNTR_LIMIT 3\n/*\n * array indices: 0,1 - HT threads, used with HT enabled cpu\n */\nstruct p4_event_bind {\n\tunsigned int opcode;\t\t\t/* Event code and ESCR selector */\n\tunsigned int escr_msr[2];\t\t/* ESCR MSR for this event */\n\tunsigned int escr_emask;\t\t/* valid ESCR EventMask bits */\n\tunsigned int shared;\t\t\t/* event is shared across threads */\n\tchar cntr[2][P4_CNTR_LIMIT];\t\t/* counter index (offset), -1 on abscence */\n};\n\nstruct p4_pebs_bind {\n\tunsigned int metric_pebs;\n\tunsigned int metric_vert;\n};\n\n/* it sets P4_PEBS_ENABLE_UOP_TAG as well */\n#define P4_GEN_PEBS_BIND(name, pebs, vert)\t\t\t\\\n\t[P4_PEBS_METRIC__##name] = {\t\t\t\t\\\n\t\t.metric_pebs = pebs | P4_PEBS_ENABLE_UOP_TAG,\t\\\n\t\t.metric_vert = vert,\t\t\t\t\\\n\t}\n\n/*\n * note we have P4_PEBS_ENABLE_UOP_TAG always set here\n *\n * it's needed for mapping P4_PEBS_CONFIG_METRIC_MASK bits of\n * event configuration to find out which values are to be\n * written into MSR_IA32_PEBS_ENABLE and MSR_P4_PEBS_MATRIX_VERT\n * resgisters\n */\nstatic struct p4_pebs_bind p4_pebs_bind_map[] = {\n\tP4_GEN_PEBS_BIND(1stl_cache_load_miss_retired,\t0x0000001, 0x0000001),\n\tP4_GEN_PEBS_BIND(2ndl_cache_load_miss_retired,\t0x0000002, 0x0000001),\n\tP4_GEN_PEBS_BIND(dtlb_load_miss_retired,\t0x0000004, 0x0000001),\n\tP4_GEN_PEBS_BIND(dtlb_store_miss_retired,\t0x0000004, 0x0000002),\n\tP4_GEN_PEBS_BIND(dtlb_all_miss_retired,\t\t0x0000004, 0x0000003),\n\tP4_GEN_PEBS_BIND(tagged_mispred_branch,\t\t0x0018000, 0x0000010),\n\tP4_GEN_PEBS_BIND(mob_load_replay_retired,\t0x0000200, 0x0000001),\n\tP4_GEN_PEBS_BIND(split_load_retired,\t\t0x0000400, 0x0000001),\n\tP4_GEN_PEBS_BIND(split_store_retired,\t\t0x0000400, 0x0000002),\n};\n\n/*\n * Note that we don't use CCCR1 here, there is an\n * exception for P4_BSQ_ALLOCATION but we just have\n * no workaround\n *\n * consider this binding as resources which particular\n * event may borrow, it doesn't contain EventMask,\n * Tags and friends -- they are left to a caller\n */\nstatic struct p4_event_bind p4_event_bind_map[] = {\n\t[P4_EVENT_TC_DELIVER_MODE] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_TC_DELIVER_MODE),\n\t\t.escr_msr\t= { MSR_P4_TC_ESCR0, MSR_P4_TC_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_TC_DELIVER_MODE, DD)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_TC_DELIVER_MODE, DB)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_TC_DELIVER_MODE, DI)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_TC_DELIVER_MODE, BD)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_TC_DELIVER_MODE, BB)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_TC_DELIVER_MODE, BI)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_TC_DELIVER_MODE, ID),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {4, 5, -1}, {6, 7, -1} },\n\t},\n\t[P4_EVENT_BPU_FETCH_REQUEST] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_BPU_FETCH_REQUEST),\n\t\t.escr_msr\t= { MSR_P4_BPU_ESCR0, MSR_P4_BPU_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BPU_FETCH_REQUEST, TCMISS),\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_ITLB_REFERENCE] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_ITLB_REFERENCE),\n\t\t.escr_msr\t= { MSR_P4_ITLB_ESCR0, MSR_P4_ITLB_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_ITLB_REFERENCE, HIT)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_ITLB_REFERENCE, MISS)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_ITLB_REFERENCE, HIT_UK),\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_MEMORY_CANCEL] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_MEMORY_CANCEL),\n\t\t.escr_msr\t= { MSR_P4_DAC_ESCR0, MSR_P4_DAC_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MEMORY_CANCEL, ST_RB_FULL)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MEMORY_CANCEL, 64K_CONF),\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_MEMORY_COMPLETE] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_MEMORY_COMPLETE),\n\t\t.escr_msr\t= { MSR_P4_SAAT_ESCR0 , MSR_P4_SAAT_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MEMORY_COMPLETE, LSC)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MEMORY_COMPLETE, SSC),\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_LOAD_PORT_REPLAY] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_LOAD_PORT_REPLAY),\n\t\t.escr_msr\t= { MSR_P4_SAAT_ESCR0, MSR_P4_SAAT_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_LOAD_PORT_REPLAY, SPLIT_LD),\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_STORE_PORT_REPLAY] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_STORE_PORT_REPLAY),\n\t\t.escr_msr\t= { MSR_P4_SAAT_ESCR0 ,  MSR_P4_SAAT_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_STORE_PORT_REPLAY, SPLIT_ST),\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_MOB_LOAD_REPLAY] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_MOB_LOAD_REPLAY),\n\t\t.escr_msr\t= { MSR_P4_MOB_ESCR0, MSR_P4_MOB_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MOB_LOAD_REPLAY, NO_STA)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MOB_LOAD_REPLAY, NO_STD)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MOB_LOAD_REPLAY, PARTIAL_DATA)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MOB_LOAD_REPLAY, UNALGN_ADDR),\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_PAGE_WALK_TYPE] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_PAGE_WALK_TYPE),\n\t\t.escr_msr\t= { MSR_P4_PMH_ESCR0, MSR_P4_PMH_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_PAGE_WALK_TYPE, DTMISS)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_PAGE_WALK_TYPE, ITMISS),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_BSQ_CACHE_REFERENCE] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_BSQ_CACHE_REFERENCE),\n\t\t.escr_msr\t= { MSR_P4_BSU_ESCR0, MSR_P4_BSU_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_2ndL_HITS)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_2ndL_HITE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_2ndL_HITM)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_3rdL_HITS)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_3rdL_HITE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_3rdL_HITM)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_2ndL_MISS)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_3rdL_MISS)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, WR_2ndL_MISS),\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_IOQ_ALLOCATION] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_IOQ_ALLOCATION),\n\t\t.escr_msr\t= { MSR_P4_FSB_ESCR0, MSR_P4_FSB_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, DEFAULT)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, ALL_READ)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, ALL_WRITE)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, MEM_UC)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, MEM_WC)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, MEM_WT)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, MEM_WP)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, MEM_WB)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, OWN)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, OTHER)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ALLOCATION, PREFETCH),\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_IOQ_ACTIVE_ENTRIES] = {\t/* shared ESCR */\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_IOQ_ACTIVE_ENTRIES),\n\t\t.escr_msr\t= { MSR_P4_FSB_ESCR1,  MSR_P4_FSB_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, DEFAULT)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, ALL_READ)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, ALL_WRITE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, MEM_UC)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, MEM_WC)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, MEM_WT)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, MEM_WP)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, MEM_WB)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, OWN)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, OTHER)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_IOQ_ACTIVE_ENTRIES, PREFETCH),\n\t\t.cntr\t\t= { {2, -1, -1}, {3, -1, -1} },\n\t},\n\t[P4_EVENT_FSB_DATA_ACTIVITY] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_FSB_DATA_ACTIVITY),\n\t\t.escr_msr\t= { MSR_P4_FSB_ESCR0, MSR_P4_FSB_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FSB_DATA_ACTIVITY, DRDY_DRV)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FSB_DATA_ACTIVITY, DRDY_OWN)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FSB_DATA_ACTIVITY, DRDY_OTHER)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FSB_DATA_ACTIVITY, DBSY_DRV)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FSB_DATA_ACTIVITY, DBSY_OWN)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FSB_DATA_ACTIVITY, DBSY_OTHER),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_BSQ_ALLOCATION] = {\t\t/* shared ESCR, broken CCCR1 */\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_BSQ_ALLOCATION),\n\t\t.escr_msr\t= { MSR_P4_BSU_ESCR0, MSR_P4_BSU_ESCR0 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_TYPE0)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_TYPE1)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_LEN0)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_LEN1)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_IO_TYPE)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_LOCK_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_CACHE_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_SPLIT_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_DEM_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, REQ_ORD_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, MEM_TYPE0)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, MEM_TYPE1)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ALLOCATION, MEM_TYPE2),\n\t\t.cntr\t\t= { {0, -1, -1}, {1, -1, -1} },\n\t},\n\t[P4_EVENT_BSQ_ACTIVE_ENTRIES] = {\t/* shared ESCR */\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_BSQ_ACTIVE_ENTRIES),\n\t\t.escr_msr\t= { MSR_P4_BSU_ESCR1 , MSR_P4_BSU_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_TYPE0)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_TYPE1)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_LEN0)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_LEN1)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_IO_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_LOCK_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_CACHE_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_SPLIT_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_DEM_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, REQ_ORD_TYPE)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, MEM_TYPE0)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, MEM_TYPE1)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_ACTIVE_ENTRIES, MEM_TYPE2),\n\t\t.cntr\t\t= { {2, -1, -1}, {3, -1, -1} },\n\t},\n\t[P4_EVENT_SSE_INPUT_ASSIST] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_SSE_INPUT_ASSIST),\n\t\t.escr_msr\t= { MSR_P4_FIRM_ESCR0, MSR_P4_FIRM_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_SSE_INPUT_ASSIST, ALL),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_PACKED_SP_UOP] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_PACKED_SP_UOP),\n\t\t.escr_msr\t= { MSR_P4_FIRM_ESCR0, MSR_P4_FIRM_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_PACKED_SP_UOP, ALL),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_PACKED_DP_UOP] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_PACKED_DP_UOP),\n\t\t.escr_msr\t= { MSR_P4_FIRM_ESCR0, MSR_P4_FIRM_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_PACKED_DP_UOP, ALL),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_SCALAR_SP_UOP] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_SCALAR_SP_UOP),\n\t\t.escr_msr\t= { MSR_P4_FIRM_ESCR0, MSR_P4_FIRM_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_SCALAR_SP_UOP, ALL),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_SCALAR_DP_UOP] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_SCALAR_DP_UOP),\n\t\t.escr_msr\t= { MSR_P4_FIRM_ESCR0, MSR_P4_FIRM_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_SCALAR_DP_UOP, ALL),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_64BIT_MMX_UOP] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_64BIT_MMX_UOP),\n\t\t.escr_msr\t= { MSR_P4_FIRM_ESCR0, MSR_P4_FIRM_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_64BIT_MMX_UOP, ALL),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_128BIT_MMX_UOP] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_128BIT_MMX_UOP),\n\t\t.escr_msr\t= { MSR_P4_FIRM_ESCR0, MSR_P4_FIRM_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_128BIT_MMX_UOP, ALL),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_X87_FP_UOP] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_X87_FP_UOP),\n\t\t.escr_msr\t= { MSR_P4_FIRM_ESCR0, MSR_P4_FIRM_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_X87_FP_UOP, ALL),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_TC_MISC] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_TC_MISC),\n\t\t.escr_msr\t= { MSR_P4_TC_ESCR0, MSR_P4_TC_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_TC_MISC, FLUSH),\n\t\t.cntr\t\t= { {4, 5, -1}, {6, 7, -1} },\n\t},\n\t[P4_EVENT_GLOBAL_POWER_EVENTS] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_GLOBAL_POWER_EVENTS),\n\t\t.escr_msr\t= { MSR_P4_FSB_ESCR0, MSR_P4_FSB_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_GLOBAL_POWER_EVENTS, RUNNING),\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_TC_MS_XFER] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_TC_MS_XFER),\n\t\t.escr_msr\t= { MSR_P4_MS_ESCR0, MSR_P4_MS_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_TC_MS_XFER, CISC),\n\t\t.cntr\t\t= { {4, 5, -1}, {6, 7, -1} },\n\t},\n\t[P4_EVENT_UOP_QUEUE_WRITES] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_UOP_QUEUE_WRITES),\n\t\t.escr_msr\t= { MSR_P4_MS_ESCR0, MSR_P4_MS_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_UOP_QUEUE_WRITES, FROM_TC_BUILD)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_UOP_QUEUE_WRITES, FROM_TC_DELIVER)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_UOP_QUEUE_WRITES, FROM_ROM),\n\t\t.cntr\t\t= { {4, 5, -1}, {6, 7, -1} },\n\t},\n\t[P4_EVENT_RETIRED_MISPRED_BRANCH_TYPE] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_RETIRED_MISPRED_BRANCH_TYPE),\n\t\t.escr_msr\t= { MSR_P4_TBPU_ESCR0 , MSR_P4_TBPU_ESCR0 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_MISPRED_BRANCH_TYPE, CONDITIONAL)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_MISPRED_BRANCH_TYPE, CALL)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_MISPRED_BRANCH_TYPE, RETURN)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_MISPRED_BRANCH_TYPE, INDIRECT),\n\t\t.cntr\t\t= { {4, 5, -1}, {6, 7, -1} },\n\t},\n\t[P4_EVENT_RETIRED_BRANCH_TYPE] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_RETIRED_BRANCH_TYPE),\n\t\t.escr_msr\t= { MSR_P4_TBPU_ESCR0 , MSR_P4_TBPU_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_BRANCH_TYPE, CONDITIONAL)\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_BRANCH_TYPE, CALL)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_BRANCH_TYPE, RETURN)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_BRANCH_TYPE, INDIRECT),\n\t\t.cntr\t\t= { {4, 5, -1}, {6, 7, -1} },\n\t},\n\t[P4_EVENT_RESOURCE_STALL] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_RESOURCE_STALL),\n\t\t.escr_msr\t= { MSR_P4_ALF_ESCR0, MSR_P4_ALF_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RESOURCE_STALL, SBFULL),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_WC_BUFFER] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_WC_BUFFER),\n\t\t.escr_msr\t= { MSR_P4_DAC_ESCR0, MSR_P4_DAC_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_WC_BUFFER, WCB_EVICTS)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_WC_BUFFER, WCB_FULL_EVICTS),\n\t\t.shared\t\t= 1,\n\t\t.cntr\t\t= { {8, 9, -1}, {10, 11, -1} },\n\t},\n\t[P4_EVENT_B2B_CYCLES] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_B2B_CYCLES),\n\t\t.escr_msr\t= { MSR_P4_FSB_ESCR0, MSR_P4_FSB_ESCR1 },\n\t\t.escr_emask\t= 0,\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_BNR] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_BNR),\n\t\t.escr_msr\t= { MSR_P4_FSB_ESCR0, MSR_P4_FSB_ESCR1 },\n\t\t.escr_emask\t= 0,\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_SNOOP] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_SNOOP),\n\t\t.escr_msr\t= { MSR_P4_FSB_ESCR0, MSR_P4_FSB_ESCR1 },\n\t\t.escr_emask\t= 0,\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_RESPONSE] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_RESPONSE),\n\t\t.escr_msr\t= { MSR_P4_FSB_ESCR0, MSR_P4_FSB_ESCR1 },\n\t\t.escr_emask\t= 0,\n\t\t.cntr\t\t= { {0, -1, -1}, {2, -1, -1} },\n\t},\n\t[P4_EVENT_FRONT_END_EVENT] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_FRONT_END_EVENT),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR2, MSR_P4_CRU_ESCR3 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FRONT_END_EVENT, NBOGUS)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FRONT_END_EVENT, BOGUS),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_EXECUTION_EVENT] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_EXECUTION_EVENT),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR2, MSR_P4_CRU_ESCR3 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, NBOGUS0)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, NBOGUS1)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, NBOGUS2)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, NBOGUS3)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, BOGUS0)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, BOGUS1)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, BOGUS2)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, BOGUS3),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_REPLAY_EVENT] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_REPLAY_EVENT),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR2, MSR_P4_CRU_ESCR3 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_REPLAY_EVENT, NBOGUS)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_REPLAY_EVENT, BOGUS),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_INSTR_RETIRED] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_INSTR_RETIRED),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR0, MSR_P4_CRU_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_INSTR_RETIRED, NBOGUSNTAG)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_INSTR_RETIRED, NBOGUSTAG)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_INSTR_RETIRED, BOGUSNTAG)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_INSTR_RETIRED, BOGUSTAG),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_UOPS_RETIRED] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_UOPS_RETIRED),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR0, MSR_P4_CRU_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_UOPS_RETIRED, NBOGUS)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_UOPS_RETIRED, BOGUS),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_UOP_TYPE] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_UOP_TYPE),\n\t\t.escr_msr\t= { MSR_P4_RAT_ESCR0, MSR_P4_RAT_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_UOP_TYPE, TAGLOADS)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_UOP_TYPE, TAGSTORES),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_BRANCH_RETIRED] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_BRANCH_RETIRED),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR2, MSR_P4_CRU_ESCR3 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BRANCH_RETIRED, MMNP)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BRANCH_RETIRED, MMNM)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BRANCH_RETIRED, MMTP)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BRANCH_RETIRED, MMTM),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_MISPRED_BRANCH_RETIRED] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_MISPRED_BRANCH_RETIRED),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR0, MSR_P4_CRU_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MISPRED_BRANCH_RETIRED, NBOGUS),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_X87_ASSIST] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_X87_ASSIST),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR2, MSR_P4_CRU_ESCR3 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_X87_ASSIST, FPSU)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_X87_ASSIST, FPSO)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_X87_ASSIST, POAO)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_X87_ASSIST, POAU)\t\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_X87_ASSIST, PREA),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_MACHINE_CLEAR] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_MACHINE_CLEAR),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR2, MSR_P4_CRU_ESCR3 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MACHINE_CLEAR, CLEAR)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MACHINE_CLEAR, MOCLEAR)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MACHINE_CLEAR, SMCLEAR),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n\t[P4_EVENT_INSTR_COMPLETED] = {\n\t\t.opcode\t\t= P4_OPCODE(P4_EVENT_INSTR_COMPLETED),\n\t\t.escr_msr\t= { MSR_P4_CRU_ESCR0, MSR_P4_CRU_ESCR1 },\n\t\t.escr_emask\t=\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_INSTR_COMPLETED, NBOGUS)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_INSTR_COMPLETED, BOGUS),\n\t\t.cntr\t\t= { {12, 13, 16}, {14, 15, 17} },\n\t},\n};\n\n#define P4_GEN_CACHE_EVENT(event, bit, metric)\t\t\t\t  \\\n\tp4_config_pack_escr(P4_ESCR_EVENT(event)\t\t\t| \\\n\t\t\t    P4_ESCR_EMASK_BIT(event, bit))\t\t| \\\n\tp4_config_pack_cccr(metric\t\t\t\t\t| \\\n\t\t\t    P4_CCCR_ESEL(P4_OPCODE_ESEL(P4_OPCODE(event))))\n\nstatic __initconst const u64 p4_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = P4_GEN_CACHE_EVENT(P4_EVENT_REPLAY_EVENT, NBOGUS,\n\t\t\t\t\t\tP4_PEBS_METRIC__1stl_cache_load_miss_retired),\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = P4_GEN_CACHE_EVENT(P4_EVENT_REPLAY_EVENT, NBOGUS,\n\t\t\t\t\t\tP4_PEBS_METRIC__2ndl_cache_load_miss_retired),\n\t},\n},\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = P4_GEN_CACHE_EVENT(P4_EVENT_REPLAY_EVENT, NBOGUS,\n\t\t\t\t\t\tP4_PEBS_METRIC__dtlb_load_miss_retired),\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = P4_GEN_CACHE_EVENT(P4_EVENT_REPLAY_EVENT, NBOGUS,\n\t\t\t\t\t\tP4_PEBS_METRIC__dtlb_store_miss_retired),\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = P4_GEN_CACHE_EVENT(P4_EVENT_ITLB_REFERENCE, HIT,\n\t\t\t\t\t\tP4_PEBS_METRIC__none),\n\t\t[ C(RESULT_MISS)   ] = P4_GEN_CACHE_EVENT(P4_EVENT_ITLB_REFERENCE, MISS,\n\t\t\t\t\t\tP4_PEBS_METRIC__none),\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n};\n\nstatic u64 p4_general_events[PERF_COUNT_HW_MAX] = {\n  /* non-halted CPU clocks */\n  [PERF_COUNT_HW_CPU_CYCLES] =\n\tp4_config_pack_escr(P4_ESCR_EVENT(P4_EVENT_GLOBAL_POWER_EVENTS)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_GLOBAL_POWER_EVENTS, RUNNING)),\n\n  /*\n   * retired instructions\n   * in a sake of simplicity we don't use the FSB tagging\n   */\n  [PERF_COUNT_HW_INSTRUCTIONS] =\n\tp4_config_pack_escr(P4_ESCR_EVENT(P4_EVENT_INSTR_RETIRED)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_INSTR_RETIRED, NBOGUSNTAG)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_INSTR_RETIRED, BOGUSNTAG)),\n\n  /* cache hits */\n  [PERF_COUNT_HW_CACHE_REFERENCES] =\n\tp4_config_pack_escr(P4_ESCR_EVENT(P4_EVENT_BSQ_CACHE_REFERENCE)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_2ndL_HITS)\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_2ndL_HITE)\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_2ndL_HITM)\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_3rdL_HITS)\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_3rdL_HITE)\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_3rdL_HITM)),\n\n  /* cache misses */\n  [PERF_COUNT_HW_CACHE_MISSES] =\n\tp4_config_pack_escr(P4_ESCR_EVENT(P4_EVENT_BSQ_CACHE_REFERENCE)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_2ndL_MISS)\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, RD_3rdL_MISS)\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_BSQ_CACHE_REFERENCE, WR_2ndL_MISS)),\n\n  /* branch instructions retired */\n  [PERF_COUNT_HW_BRANCH_INSTRUCTIONS] =\n\tp4_config_pack_escr(P4_ESCR_EVENT(P4_EVENT_RETIRED_BRANCH_TYPE)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_BRANCH_TYPE, CONDITIONAL)\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_BRANCH_TYPE, CALL)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_BRANCH_TYPE, RETURN)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_RETIRED_BRANCH_TYPE, INDIRECT)),\n\n  /* mispredicted branches retired */\n  [PERF_COUNT_HW_BRANCH_MISSES]\t=\n\tp4_config_pack_escr(P4_ESCR_EVENT(P4_EVENT_MISPRED_BRANCH_RETIRED)\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_MISPRED_BRANCH_RETIRED, NBOGUS)),\n\n  /* bus ready clocks (cpu is driving #DRDY_DRV\\#DRDY_OWN):  */\n  [PERF_COUNT_HW_BUS_CYCLES] =\n\tp4_config_pack_escr(P4_ESCR_EVENT(P4_EVENT_FSB_DATA_ACTIVITY)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FSB_DATA_ACTIVITY, DRDY_DRV)\t\t|\n\t\tP4_ESCR_EMASK_BIT(P4_EVENT_FSB_DATA_ACTIVITY, DRDY_OWN))\t|\n\tp4_config_pack_cccr(P4_CCCR_EDGE | P4_CCCR_COMPARE),\n};\n\nstatic struct p4_event_bind *p4_config_get_bind(u64 config)\n{\n\tunsigned int evnt = p4_config_unpack_event(config);\n\tstruct p4_event_bind *bind = NULL;\n\n\tif (evnt < ARRAY_SIZE(p4_event_bind_map))\n\t\tbind = &p4_event_bind_map[evnt];\n\n\treturn bind;\n}\n\nstatic u64 p4_pmu_event_map(int hw_event)\n{\n\tstruct p4_event_bind *bind;\n\tunsigned int esel;\n\tu64 config;\n\n\tconfig = p4_general_events[hw_event];\n\tbind = p4_config_get_bind(config);\n\tesel = P4_OPCODE_ESEL(bind->opcode);\n\tconfig |= p4_config_pack_cccr(P4_CCCR_ESEL(esel));\n\n\treturn config;\n}\n\n/* check cpu model specifics */\nstatic bool p4_event_match_cpu_model(unsigned int event_idx)\n{\n\t/* INSTR_COMPLETED event only exist for model 3, 4, 6 (Prescott) */\n\tif (event_idx == P4_EVENT_INSTR_COMPLETED) {\n\t\tif (boot_cpu_data.x86_model != 3 &&\n\t\t\tboot_cpu_data.x86_model != 4 &&\n\t\t\tboot_cpu_data.x86_model != 6)\n\t\t\treturn false;\n\t}\n\n\t/*\n\t * For info\n\t * - IQ_ESCR0, IQ_ESCR1 only for models 1 and 2\n\t */\n\n\treturn true;\n}\n\nstatic int p4_validate_raw_event(struct perf_event *event)\n{\n\tunsigned int v, emask;\n\n\t/* User data may have out-of-bound event index */\n\tv = p4_config_unpack_event(event->attr.config);\n\tif (v >= ARRAY_SIZE(p4_event_bind_map))\n\t\treturn -EINVAL;\n\n\t/* It may be unsupported: */\n\tif (!p4_event_match_cpu_model(v))\n\t\treturn -EINVAL;\n\n\t/*\n\t * NOTE: P4_CCCR_THREAD_ANY has not the same meaning as\n\t * in Architectural Performance Monitoring, it means not\n\t * on _which_ logical cpu to count but rather _when_, ie it\n\t * depends on logical cpu state -- count event if one cpu active,\n\t * none, both or any, so we just allow user to pass any value\n\t * desired.\n\t *\n\t * In turn we always set Tx_OS/Tx_USR bits bound to logical\n\t * cpu without their propagation to another cpu\n\t */\n\n\t/*\n\t * if an event is shared across the logical threads\n\t * the user needs special permissions to be able to use it\n\t */\n\tif (p4_ht_active() && p4_event_bind_map[v].shared) {\n\t\tif (perf_paranoid_cpu() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\t/* ESCR EventMask bits may be invalid */\n\temask = p4_config_unpack_escr(event->attr.config) & P4_ESCR_EVENTMASK_MASK;\n\tif (emask & ~p4_event_bind_map[v].escr_emask)\n\t\treturn -EINVAL;\n\n\t/*\n\t * it may have some invalid PEBS bits\n\t */\n\tif (p4_config_pebs_has(event->attr.config, P4_PEBS_CONFIG_ENABLE))\n\t\treturn -EINVAL;\n\n\tv = p4_config_unpack_metric(event->attr.config);\n\tif (v >= ARRAY_SIZE(p4_pebs_bind_map))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void p4_hw_watchdog_set_attr(struct perf_event_attr *wd_attr)\n{\n\t/*\n\t * Watchdog ticks are special on Netburst, we use\n\t * that named \"non-sleeping\" ticks as recommended\n\t * by Intel SDM Vol3b.\n\t */\n\tWARN_ON_ONCE(wd_attr->type\t!= PERF_TYPE_HARDWARE ||\n\t\t     wd_attr->config\t!= PERF_COUNT_HW_CPU_CYCLES);\n\n\twd_attr->type\t= PERF_TYPE_RAW;\n\twd_attr->config\t=\n\t\tp4_config_pack_escr(P4_ESCR_EVENT(P4_EVENT_EXECUTION_EVENT)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, NBOGUS0)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, NBOGUS1)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, NBOGUS2)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, NBOGUS3)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, BOGUS0)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, BOGUS1)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, BOGUS2)\t\t|\n\t\t\tP4_ESCR_EMASK_BIT(P4_EVENT_EXECUTION_EVENT, BOGUS3))\t\t|\n\t\tp4_config_pack_cccr(P4_CCCR_THRESHOLD(15) | P4_CCCR_COMPLEMENT\t\t|\n\t\t\tP4_CCCR_COMPARE);\n}\n\nstatic int p4_hw_config(struct perf_event *event)\n{\n\tint cpu = get_cpu();\n\tint rc = 0;\n\tu32 escr, cccr;\n\n\t/*\n\t * the reason we use cpu that early is that: if we get scheduled\n\t * first time on the same cpu -- we will not need swap thread\n\t * specific flags in config (and will save some cpu cycles)\n\t */\n\n\tcccr = p4_default_cccr_conf(cpu);\n\tescr = p4_default_escr_conf(cpu, event->attr.exclude_kernel,\n\t\t\t\t\t event->attr.exclude_user);\n\tevent->hw.config = p4_config_pack_escr(escr) |\n\t\t\t   p4_config_pack_cccr(cccr);\n\n\tif (p4_ht_active() && p4_ht_thread(cpu))\n\t\tevent->hw.config = p4_set_ht_bit(event->hw.config);\n\n\tif (event->attr.type == PERF_TYPE_RAW) {\n\t\tstruct p4_event_bind *bind;\n\t\tunsigned int esel;\n\t\t/*\n\t\t * Clear bits we reserve to be managed by kernel itself\n\t\t * and never allowed from a user space\n\t\t */\n\t\t event->attr.config &= P4_CONFIG_MASK;\n\n\t\trc = p4_validate_raw_event(event);\n\t\tif (rc)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * Note that for RAW events we allow user to use P4_CCCR_RESERVED\n\t\t * bits since we keep additional info here (for cache events and etc)\n\t\t */\n\t\tevent->hw.config |= event->attr.config;\n\t\tbind = p4_config_get_bind(event->attr.config);\n\t\tif (!bind) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tesel = P4_OPCODE_ESEL(bind->opcode);\n\t\tevent->hw.config |= p4_config_pack_cccr(P4_CCCR_ESEL(esel));\n\t}\n\n\trc = x86_setup_perfctr(event);\nout:\n\tput_cpu();\n\treturn rc;\n}\n\nstatic inline int p4_pmu_clear_cccr_ovf(struct hw_perf_event *hwc)\n{\n\tu64 v;\n\n\t/* an official way for overflow indication */\n\trdmsrl(hwc->config_base, v);\n\tif (v & P4_CCCR_OVF) {\n\t\twrmsrl(hwc->config_base, v & ~P4_CCCR_OVF);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * In some circumstances the overflow might issue an NMI but did\n\t * not set P4_CCCR_OVF bit. Because a counter holds a negative value\n\t * we simply check for high bit being set, if it's cleared it means\n\t * the counter has reached zero value and continued counting before\n\t * real NMI signal was received:\n\t */\n\trdmsrl(hwc->event_base, v);\n\tif (!(v & ARCH_P4_UNFLAGGED_BIT))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void p4_pmu_disable_pebs(void)\n{\n\t/*\n\t * FIXME\n\t *\n\t * It's still allowed that two threads setup same cache\n\t * events so we can't simply clear metrics until we knew\n\t * no one is depending on us, so we need kind of counter\n\t * for \"ReplayEvent\" users.\n\t *\n\t * What is more complex -- RAW events, if user (for some\n\t * reason) will pass some cache event metric with improper\n\t * event opcode -- it's fine from hardware point of view\n\t * but completely nonsense from \"meaning\" of such action.\n\t *\n\t * So at moment let leave metrics turned on forever -- it's\n\t * ok for now but need to be revisited!\n\t *\n\t * (void)checking_wrmsrl(MSR_IA32_PEBS_ENABLE, (u64)0);\n\t * (void)checking_wrmsrl(MSR_P4_PEBS_MATRIX_VERT, (u64)0);\n\t */\n}\n\nstatic inline void p4_pmu_disable_event(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t/*\n\t * If event gets disabled while counter is in overflowed\n\t * state we need to clear P4_CCCR_OVF, otherwise interrupt get\n\t * asserted again and again\n\t */\n\t(void)checking_wrmsrl(hwc->config_base,\n\t\t(u64)(p4_config_unpack_cccr(hwc->config)) &\n\t\t\t~P4_CCCR_ENABLE & ~P4_CCCR_OVF & ~P4_CCCR_RESERVED);\n}\n\nstatic void p4_pmu_disable_all(void)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint idx;\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\t\tp4_pmu_disable_event(event);\n\t}\n\n\tp4_pmu_disable_pebs();\n}\n\n/* configuration must be valid */\nstatic void p4_pmu_enable_pebs(u64 config)\n{\n\tstruct p4_pebs_bind *bind;\n\tunsigned int idx;\n\n\tBUILD_BUG_ON(P4_PEBS_METRIC__max > P4_PEBS_CONFIG_METRIC_MASK);\n\n\tidx = p4_config_unpack_metric(config);\n\tif (idx == P4_PEBS_METRIC__none)\n\t\treturn;\n\n\tbind = &p4_pebs_bind_map[idx];\n\n\t(void)checking_wrmsrl(MSR_IA32_PEBS_ENABLE,\t(u64)bind->metric_pebs);\n\t(void)checking_wrmsrl(MSR_P4_PEBS_MATRIX_VERT,\t(u64)bind->metric_vert);\n}\n\nstatic void p4_pmu_enable_event(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint thread = p4_ht_config_thread(hwc->config);\n\tu64 escr_conf = p4_config_unpack_escr(p4_clear_ht_bit(hwc->config));\n\tunsigned int idx = p4_config_unpack_event(hwc->config);\n\tstruct p4_event_bind *bind;\n\tu64 escr_addr, cccr;\n\n\tbind = &p4_event_bind_map[idx];\n\tescr_addr = (u64)bind->escr_msr[thread];\n\n\t/*\n\t * - we dont support cascaded counters yet\n\t * - and counter 1 is broken (erratum)\n\t */\n\tWARN_ON_ONCE(p4_is_event_cascaded(hwc->config));\n\tWARN_ON_ONCE(hwc->idx == 1);\n\n\t/* we need a real Event value */\n\tescr_conf &= ~P4_ESCR_EVENT_MASK;\n\tescr_conf |= P4_ESCR_EVENT(P4_OPCODE_EVNT(bind->opcode));\n\n\tcccr = p4_config_unpack_cccr(hwc->config);\n\n\t/*\n\t * it could be Cache event so we need to write metrics\n\t * into additional MSRs\n\t */\n\tp4_pmu_enable_pebs(hwc->config);\n\n\t(void)checking_wrmsrl(escr_addr, escr_conf);\n\t(void)checking_wrmsrl(hwc->config_base,\n\t\t\t\t(cccr & ~P4_CCCR_RESERVED) | P4_CCCR_ENABLE);\n}\n\nstatic void p4_pmu_enable_all(int added)\n{\n\tstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\n\tint idx;\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\t\tp4_pmu_enable_event(event);\n\t}\n}\n\nstatic int p4_pmu_handle_irq(struct pt_regs *regs)\n{\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct perf_event *event;\n\tstruct hw_perf_event *hwc;\n\tint idx, handled = 0;\n\tu64 val;\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tint overflow;\n\n\t\tif (!test_bit(idx, cpuc->active_mask)) {\n\t\t\t/* catch in-flight IRQs */\n\t\t\tif (__test_and_clear_bit(idx, cpuc->running))\n\t\t\t\thandled++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tevent = cpuc->events[idx];\n\t\thwc = &event->hw;\n\n\t\tWARN_ON_ONCE(hwc->idx != idx);\n\n\t\t/* it might be unflagged overflow */\n\t\toverflow = p4_pmu_clear_cccr_ovf(hwc);\n\n\t\tval = x86_perf_event_update(event);\n\t\tif (!overflow && (val & (1ULL << (x86_pmu.cntval_bits - 1))))\n\t\t\tcontinue;\n\n\t\thandled += overflow;\n\n\t\t/* event overflow for sure */\n\t\tdata.period = event->hw.last_period;\n\n\t\tif (!x86_perf_event_set_period(event))\n\t\t\tcontinue;\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tx86_pmu_stop(event, 0);\n\t}\n\n\tif (handled)\n\t\tinc_irq_stat(apic_perf_irqs);\n\n\t/*\n\t * When dealing with the unmasking of the LVTPC on P4 perf hw, it has\n\t * been observed that the OVF bit flag has to be cleared first _before_\n\t * the LVTPC can be unmasked.\n\t *\n\t * The reason is the NMI line will continue to be asserted while the OVF\n\t * bit is set.  This causes a second NMI to generate if the LVTPC is\n\t * unmasked before the OVF bit is cleared, leading to unknown NMI\n\t * messages.\n\t */\n\tapic_write(APIC_LVTPC, APIC_DM_NMI);\n\n\treturn handled;\n}\n\n/*\n * swap thread specific fields according to a thread\n * we are going to run on\n */\nstatic void p4_pmu_swap_config_ts(struct hw_perf_event *hwc, int cpu)\n{\n\tu32 escr, cccr;\n\n\t/*\n\t * we either lucky and continue on same cpu or no HT support\n\t */\n\tif (!p4_should_swap_ts(hwc->config, cpu))\n\t\treturn;\n\n\t/*\n\t * the event is migrated from an another logical\n\t * cpu, so we need to swap thread specific flags\n\t */\n\n\tescr = p4_config_unpack_escr(hwc->config);\n\tcccr = p4_config_unpack_cccr(hwc->config);\n\n\tif (p4_ht_thread(cpu)) {\n\t\tcccr &= ~P4_CCCR_OVF_PMI_T0;\n\t\tcccr |= P4_CCCR_OVF_PMI_T1;\n\t\tif (escr & P4_ESCR_T0_OS) {\n\t\t\tescr &= ~P4_ESCR_T0_OS;\n\t\t\tescr |= P4_ESCR_T1_OS;\n\t\t}\n\t\tif (escr & P4_ESCR_T0_USR) {\n\t\t\tescr &= ~P4_ESCR_T0_USR;\n\t\t\tescr |= P4_ESCR_T1_USR;\n\t\t}\n\t\thwc->config  = p4_config_pack_escr(escr);\n\t\thwc->config |= p4_config_pack_cccr(cccr);\n\t\thwc->config |= P4_CONFIG_HT;\n\t} else {\n\t\tcccr &= ~P4_CCCR_OVF_PMI_T1;\n\t\tcccr |= P4_CCCR_OVF_PMI_T0;\n\t\tif (escr & P4_ESCR_T1_OS) {\n\t\t\tescr &= ~P4_ESCR_T1_OS;\n\t\t\tescr |= P4_ESCR_T0_OS;\n\t\t}\n\t\tif (escr & P4_ESCR_T1_USR) {\n\t\t\tescr &= ~P4_ESCR_T1_USR;\n\t\t\tescr |= P4_ESCR_T0_USR;\n\t\t}\n\t\thwc->config  = p4_config_pack_escr(escr);\n\t\thwc->config |= p4_config_pack_cccr(cccr);\n\t\thwc->config &= ~P4_CONFIG_HT;\n\t}\n}\n\n/*\n * ESCR address hashing is tricky, ESCRs are not sequential\n * in memory but all starts from MSR_P4_BSU_ESCR0 (0x03a0) and\n * the metric between any ESCRs is laid in range [0xa0,0xe1]\n *\n * so we make ~70% filled hashtable\n */\n\n#define P4_ESCR_MSR_BASE\t\t0x000003a0\n#define P4_ESCR_MSR_MAX\t\t\t0x000003e1\n#define P4_ESCR_MSR_TABLE_SIZE\t\t(P4_ESCR_MSR_MAX - P4_ESCR_MSR_BASE + 1)\n#define P4_ESCR_MSR_IDX(msr)\t\t(msr - P4_ESCR_MSR_BASE)\n#define P4_ESCR_MSR_TABLE_ENTRY(msr)\t[P4_ESCR_MSR_IDX(msr)] = msr\n\nstatic const unsigned int p4_escr_table[P4_ESCR_MSR_TABLE_SIZE] = {\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_ALF_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_ALF_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_BPU_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_BPU_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_BSU_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_BSU_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_CRU_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_CRU_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_CRU_ESCR2),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_CRU_ESCR3),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_CRU_ESCR4),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_CRU_ESCR5),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_DAC_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_DAC_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_FIRM_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_FIRM_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_FLAME_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_FLAME_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_FSB_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_FSB_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_IQ_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_IQ_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_IS_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_IS_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_ITLB_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_ITLB_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_IX_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_IX_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_MOB_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_MOB_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_MS_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_MS_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_PMH_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_PMH_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_RAT_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_RAT_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_SAAT_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_SAAT_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_SSU_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_SSU_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_TBPU_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_TBPU_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_TC_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_TC_ESCR1),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_U2L_ESCR0),\n\tP4_ESCR_MSR_TABLE_ENTRY(MSR_P4_U2L_ESCR1),\n};\n\nstatic int p4_get_escr_idx(unsigned int addr)\n{\n\tunsigned int idx = P4_ESCR_MSR_IDX(addr);\n\n\tif (unlikely(idx >= P4_ESCR_MSR_TABLE_SIZE\t||\n\t\t\t!p4_escr_table[idx]\t\t||\n\t\t\tp4_escr_table[idx] != addr)) {\n\t\tWARN_ONCE(1, \"P4 PMU: Wrong address passed: %x\\n\", addr);\n\t\treturn -1;\n\t}\n\n\treturn idx;\n}\n\nstatic int p4_next_cntr(int thread, unsigned long *used_mask,\n\t\t\tstruct p4_event_bind *bind)\n{\n\tint i, j;\n\n\tfor (i = 0; i < P4_CNTR_LIMIT; i++) {\n\t\tj = bind->cntr[thread][i];\n\t\tif (j != -1 && !test_bit(j, used_mask))\n\t\t\treturn j;\n\t}\n\n\treturn -1;\n}\n\nstatic int p4_pmu_schedule_events(struct cpu_hw_events *cpuc, int n, int *assign)\n{\n\tunsigned long used_mask[BITS_TO_LONGS(X86_PMC_IDX_MAX)];\n\tunsigned long escr_mask[BITS_TO_LONGS(P4_ESCR_MSR_TABLE_SIZE)];\n\tint cpu = smp_processor_id();\n\tstruct hw_perf_event *hwc;\n\tstruct p4_event_bind *bind;\n\tunsigned int i, thread, num;\n\tint cntr_idx, escr_idx;\n\n\tbitmap_zero(used_mask, X86_PMC_IDX_MAX);\n\tbitmap_zero(escr_mask, P4_ESCR_MSR_TABLE_SIZE);\n\n\tfor (i = 0, num = n; i < n; i++, num--) {\n\n\t\thwc = &cpuc->event_list[i]->hw;\n\t\tthread = p4_ht_thread(cpu);\n\t\tbind = p4_config_get_bind(hwc->config);\n\t\tescr_idx = p4_get_escr_idx(bind->escr_msr[thread]);\n\t\tif (unlikely(escr_idx == -1))\n\t\t\tgoto done;\n\n\t\tif (hwc->idx != -1 && !p4_should_swap_ts(hwc->config, cpu)) {\n\t\t\tcntr_idx = hwc->idx;\n\t\t\tif (assign)\n\t\t\t\tassign[i] = hwc->idx;\n\t\t\tgoto reserve;\n\t\t}\n\n\t\tcntr_idx = p4_next_cntr(thread, used_mask, bind);\n\t\tif (cntr_idx == -1 || test_bit(escr_idx, escr_mask))\n\t\t\tgoto done;\n\n\t\tp4_pmu_swap_config_ts(hwc, cpu);\n\t\tif (assign)\n\t\t\tassign[i] = cntr_idx;\nreserve:\n\t\tset_bit(cntr_idx, used_mask);\n\t\tset_bit(escr_idx, escr_mask);\n\t}\n\ndone:\n\treturn num ? -ENOSPC : 0;\n}\n\nstatic __initconst const struct x86_pmu p4_pmu = {\n\t.name\t\t\t= \"Netburst P4/Xeon\",\n\t.handle_irq\t\t= p4_pmu_handle_irq,\n\t.disable_all\t\t= p4_pmu_disable_all,\n\t.enable_all\t\t= p4_pmu_enable_all,\n\t.enable\t\t\t= p4_pmu_enable_event,\n\t.disable\t\t= p4_pmu_disable_event,\n\t.eventsel\t\t= MSR_P4_BPU_CCCR0,\n\t.perfctr\t\t= MSR_P4_BPU_PERFCTR0,\n\t.event_map\t\t= p4_pmu_event_map,\n\t.max_events\t\t= ARRAY_SIZE(p4_general_events),\n\t.get_event_constraints\t= x86_get_event_constraints,\n\t/*\n\t * IF HT disabled we may need to use all\n\t * ARCH_P4_MAX_CCCR counters simulaneously\n\t * though leave it restricted at moment assuming\n\t * HT is on\n\t */\n\t.num_counters\t\t= ARCH_P4_MAX_CCCR,\n\t.apic\t\t\t= 1,\n\t.cntval_bits\t\t= ARCH_P4_CNTRVAL_BITS,\n\t.cntval_mask\t\t= ARCH_P4_CNTRVAL_MASK,\n\t.max_period\t\t= (1ULL << (ARCH_P4_CNTRVAL_BITS - 1)) - 1,\n\t.hw_watchdog_set_attr\t= p4_hw_watchdog_set_attr,\n\t.hw_config\t\t= p4_hw_config,\n\t.schedule_events\t= p4_pmu_schedule_events,\n\t/*\n\t * This handles erratum N15 in intel doc 249199-029,\n\t * the counter may not be updated correctly on write\n\t * so we need a second write operation to do the trick\n\t * (the official workaround didn't work)\n\t *\n\t * the former idea is taken from OProfile code\n\t */\n\t.perfctr_second_write\t= 1,\n};\n\nstatic __init int p4_pmu_init(void)\n{\n\tunsigned int low, high;\n\n\t/* If we get stripped -- indexing fails */\n\tBUILD_BUG_ON(ARCH_P4_MAX_CCCR > X86_PMC_MAX_GENERIC);\n\n\trdmsr(MSR_IA32_MISC_ENABLE, low, high);\n\tif (!(low & (1 << 7))) {\n\t\tpr_cont(\"unsupported Netburst CPU model %d \",\n\t\t\tboot_cpu_data.x86_model);\n\t\treturn -ENODEV;\n\t}\n\n\tmemcpy(hw_cache_event_ids, p4_hw_cache_event_ids,\n\t\tsizeof(hw_cache_event_ids));\n\n\tpr_cont(\"Netburst events, \");\n\n\tx86_pmu = p4_pmu;\n\n\treturn 0;\n}\n\n#endif /* CONFIG_CPU_SUP_INTEL */\n", "/*\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the\n * Free Software Foundation; either version 2, or (at your option) any\n * later version.\n *\n * This program is distributed in the hope that it will be useful, but\n * WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * General Public License for more details.\n *\n */\n\n/*\n * Copyright (C) 2004 Amit S. Kale <amitkale@linsyssoft.com>\n * Copyright (C) 2000-2001 VERITAS Software Corporation.\n * Copyright (C) 2002 Andi Kleen, SuSE Labs\n * Copyright (C) 2004 LinSysSoft Technologies Pvt. Ltd.\n * Copyright (C) 2007 MontaVista Software, Inc.\n * Copyright (C) 2007-2008 Jason Wessel, Wind River Systems, Inc.\n */\n/****************************************************************************\n *  Contributor:     Lake Stevens Instrument Division$\n *  Written by:      Glenn Engel $\n *  Updated by:\t     Amit Kale<akale@veritas.com>\n *  Updated by:\t     Tom Rini <trini@kernel.crashing.org>\n *  Updated by:\t     Jason Wessel <jason.wessel@windriver.com>\n *  Modified for 386 by Jim Kingdon, Cygnus Support.\n *  Origianl kgdb, compatibility with 2.1.xx kernel by\n *  David Grothe <dave@gcom.com>\n *  Integrated into 2.2.5 kernel by Tigran Aivazian <tigran@sco.com>\n *  X86_64 changes from Andi Kleen's patch merged by Jim Houston\n */\n#include <linux/spinlock.h>\n#include <linux/kdebug.h>\n#include <linux/string.h>\n#include <linux/kernel.h>\n#include <linux/ptrace.h>\n#include <linux/sched.h>\n#include <linux/delay.h>\n#include <linux/kgdb.h>\n#include <linux/init.h>\n#include <linux/smp.h>\n#include <linux/nmi.h>\n#include <linux/hw_breakpoint.h>\n\n#include <asm/debugreg.h>\n#include <asm/apicdef.h>\n#include <asm/system.h>\n#include <asm/apic.h>\n#include <asm/nmi.h>\n\nstruct dbg_reg_def_t dbg_reg_def[DBG_MAX_REG_NUM] =\n{\n#ifdef CONFIG_X86_32\n\t{ \"ax\", 4, offsetof(struct pt_regs, ax) },\n\t{ \"cx\", 4, offsetof(struct pt_regs, cx) },\n\t{ \"dx\", 4, offsetof(struct pt_regs, dx) },\n\t{ \"bx\", 4, offsetof(struct pt_regs, bx) },\n\t{ \"sp\", 4, offsetof(struct pt_regs, sp) },\n\t{ \"bp\", 4, offsetof(struct pt_regs, bp) },\n\t{ \"si\", 4, offsetof(struct pt_regs, si) },\n\t{ \"di\", 4, offsetof(struct pt_regs, di) },\n\t{ \"ip\", 4, offsetof(struct pt_regs, ip) },\n\t{ \"flags\", 4, offsetof(struct pt_regs, flags) },\n\t{ \"cs\", 4, offsetof(struct pt_regs, cs) },\n\t{ \"ss\", 4, offsetof(struct pt_regs, ss) },\n\t{ \"ds\", 4, offsetof(struct pt_regs, ds) },\n\t{ \"es\", 4, offsetof(struct pt_regs, es) },\n\t{ \"fs\", 4, -1 },\n\t{ \"gs\", 4, -1 },\n#else\n\t{ \"ax\", 8, offsetof(struct pt_regs, ax) },\n\t{ \"bx\", 8, offsetof(struct pt_regs, bx) },\n\t{ \"cx\", 8, offsetof(struct pt_regs, cx) },\n\t{ \"dx\", 8, offsetof(struct pt_regs, dx) },\n\t{ \"si\", 8, offsetof(struct pt_regs, dx) },\n\t{ \"di\", 8, offsetof(struct pt_regs, di) },\n\t{ \"bp\", 8, offsetof(struct pt_regs, bp) },\n\t{ \"sp\", 8, offsetof(struct pt_regs, sp) },\n\t{ \"r8\", 8, offsetof(struct pt_regs, r8) },\n\t{ \"r9\", 8, offsetof(struct pt_regs, r9) },\n\t{ \"r10\", 8, offsetof(struct pt_regs, r10) },\n\t{ \"r11\", 8, offsetof(struct pt_regs, r11) },\n\t{ \"r12\", 8, offsetof(struct pt_regs, r12) },\n\t{ \"r13\", 8, offsetof(struct pt_regs, r13) },\n\t{ \"r14\", 8, offsetof(struct pt_regs, r14) },\n\t{ \"r15\", 8, offsetof(struct pt_regs, r15) },\n\t{ \"ip\", 8, offsetof(struct pt_regs, ip) },\n\t{ \"flags\", 4, offsetof(struct pt_regs, flags) },\n\t{ \"cs\", 4, offsetof(struct pt_regs, cs) },\n\t{ \"ss\", 4, offsetof(struct pt_regs, ss) },\n#endif\n};\n\nint dbg_set_reg(int regno, void *mem, struct pt_regs *regs)\n{\n\tif (\n#ifdef CONFIG_X86_32\n\t    regno == GDB_SS || regno == GDB_FS || regno == GDB_GS ||\n#endif\n\t    regno == GDB_SP || regno == GDB_ORIG_AX)\n\t\treturn 0;\n\n\tif (dbg_reg_def[regno].offset != -1)\n\t\tmemcpy((void *)regs + dbg_reg_def[regno].offset, mem,\n\t\t       dbg_reg_def[regno].size);\n\treturn 0;\n}\n\nchar *dbg_get_reg(int regno, void *mem, struct pt_regs *regs)\n{\n\tif (regno == GDB_ORIG_AX) {\n\t\tmemcpy(mem, &regs->orig_ax, sizeof(regs->orig_ax));\n\t\treturn \"orig_ax\";\n\t}\n\tif (regno >= DBG_MAX_REG_NUM || regno < 0)\n\t\treturn NULL;\n\n\tif (dbg_reg_def[regno].offset != -1)\n\t\tmemcpy(mem, (void *)regs + dbg_reg_def[regno].offset,\n\t\t       dbg_reg_def[regno].size);\n\n#ifdef CONFIG_X86_32\n\tswitch (regno) {\n\tcase GDB_SS:\n\t\tif (!user_mode_vm(regs))\n\t\t\t*(unsigned long *)mem = __KERNEL_DS;\n\t\tbreak;\n\tcase GDB_SP:\n\t\tif (!user_mode_vm(regs))\n\t\t\t*(unsigned long *)mem = kernel_stack_pointer(regs);\n\t\tbreak;\n\tcase GDB_GS:\n\tcase GDB_FS:\n\t\t*(unsigned long *)mem = 0xFFFF;\n\t\tbreak;\n\t}\n#endif\n\treturn dbg_reg_def[regno].name;\n}\n\n/**\n *\tsleeping_thread_to_gdb_regs - Convert ptrace regs to GDB regs\n *\t@gdb_regs: A pointer to hold the registers in the order GDB wants.\n *\t@p: The &struct task_struct of the desired process.\n *\n *\tConvert the register values of the sleeping process in @p to\n *\tthe format that GDB expects.\n *\tThis function is called when kgdb does not have access to the\n *\t&struct pt_regs and therefore it should fill the gdb registers\n *\t@gdb_regs with what has\tbeen saved in &struct thread_struct\n *\tthread field during switch_to.\n */\nvoid sleeping_thread_to_gdb_regs(unsigned long *gdb_regs, struct task_struct *p)\n{\n#ifndef CONFIG_X86_32\n\tu32 *gdb_regs32 = (u32 *)gdb_regs;\n#endif\n\tgdb_regs[GDB_AX]\t= 0;\n\tgdb_regs[GDB_BX]\t= 0;\n\tgdb_regs[GDB_CX]\t= 0;\n\tgdb_regs[GDB_DX]\t= 0;\n\tgdb_regs[GDB_SI]\t= 0;\n\tgdb_regs[GDB_DI]\t= 0;\n\tgdb_regs[GDB_BP]\t= *(unsigned long *)p->thread.sp;\n#ifdef CONFIG_X86_32\n\tgdb_regs[GDB_DS]\t= __KERNEL_DS;\n\tgdb_regs[GDB_ES]\t= __KERNEL_DS;\n\tgdb_regs[GDB_PS]\t= 0;\n\tgdb_regs[GDB_CS]\t= __KERNEL_CS;\n\tgdb_regs[GDB_PC]\t= p->thread.ip;\n\tgdb_regs[GDB_SS]\t= __KERNEL_DS;\n\tgdb_regs[GDB_FS]\t= 0xFFFF;\n\tgdb_regs[GDB_GS]\t= 0xFFFF;\n#else\n\tgdb_regs32[GDB_PS]\t= *(unsigned long *)(p->thread.sp + 8);\n\tgdb_regs32[GDB_CS]\t= __KERNEL_CS;\n\tgdb_regs32[GDB_SS]\t= __KERNEL_DS;\n\tgdb_regs[GDB_PC]\t= 0;\n\tgdb_regs[GDB_R8]\t= 0;\n\tgdb_regs[GDB_R9]\t= 0;\n\tgdb_regs[GDB_R10]\t= 0;\n\tgdb_regs[GDB_R11]\t= 0;\n\tgdb_regs[GDB_R12]\t= 0;\n\tgdb_regs[GDB_R13]\t= 0;\n\tgdb_regs[GDB_R14]\t= 0;\n\tgdb_regs[GDB_R15]\t= 0;\n#endif\n\tgdb_regs[GDB_SP]\t= p->thread.sp;\n}\n\nstatic struct hw_breakpoint {\n\tunsigned\t\tenabled;\n\tunsigned long\t\taddr;\n\tint\t\t\tlen;\n\tint\t\t\ttype;\n\tstruct perf_event\t* __percpu *pev;\n} breakinfo[HBP_NUM];\n\nstatic unsigned long early_dr7;\n\nstatic void kgdb_correct_hw_break(void)\n{\n\tint breakno;\n\n\tfor (breakno = 0; breakno < HBP_NUM; breakno++) {\n\t\tstruct perf_event *bp;\n\t\tstruct arch_hw_breakpoint *info;\n\t\tint val;\n\t\tint cpu = raw_smp_processor_id();\n\t\tif (!breakinfo[breakno].enabled)\n\t\t\tcontinue;\n\t\tif (dbg_is_early) {\n\t\t\tset_debugreg(breakinfo[breakno].addr, breakno);\n\t\t\tearly_dr7 |= encode_dr7(breakno,\n\t\t\t\t\t\tbreakinfo[breakno].len,\n\t\t\t\t\t\tbreakinfo[breakno].type);\n\t\t\tset_debugreg(early_dr7, 7);\n\t\t\tcontinue;\n\t\t}\n\t\tbp = *per_cpu_ptr(breakinfo[breakno].pev, cpu);\n\t\tinfo = counter_arch_bp(bp);\n\t\tif (bp->attr.disabled != 1)\n\t\t\tcontinue;\n\t\tbp->attr.bp_addr = breakinfo[breakno].addr;\n\t\tbp->attr.bp_len = breakinfo[breakno].len;\n\t\tbp->attr.bp_type = breakinfo[breakno].type;\n\t\tinfo->address = breakinfo[breakno].addr;\n\t\tinfo->len = breakinfo[breakno].len;\n\t\tinfo->type = breakinfo[breakno].type;\n\t\tval = arch_install_hw_breakpoint(bp);\n\t\tif (!val)\n\t\t\tbp->attr.disabled = 0;\n\t}\n\tif (!dbg_is_early)\n\t\thw_breakpoint_restore();\n}\n\nstatic int hw_break_reserve_slot(int breakno)\n{\n\tint cpu;\n\tint cnt = 0;\n\tstruct perf_event **pevent;\n\n\tif (dbg_is_early)\n\t\treturn 0;\n\n\tfor_each_online_cpu(cpu) {\n\t\tcnt++;\n\t\tpevent = per_cpu_ptr(breakinfo[breakno].pev, cpu);\n\t\tif (dbg_reserve_bp_slot(*pevent))\n\t\t\tgoto fail;\n\t}\n\n\treturn 0;\n\nfail:\n\tfor_each_online_cpu(cpu) {\n\t\tcnt--;\n\t\tif (!cnt)\n\t\t\tbreak;\n\t\tpevent = per_cpu_ptr(breakinfo[breakno].pev, cpu);\n\t\tdbg_release_bp_slot(*pevent);\n\t}\n\treturn -1;\n}\n\nstatic int hw_break_release_slot(int breakno)\n{\n\tstruct perf_event **pevent;\n\tint cpu;\n\n\tif (dbg_is_early)\n\t\treturn 0;\n\n\tfor_each_online_cpu(cpu) {\n\t\tpevent = per_cpu_ptr(breakinfo[breakno].pev, cpu);\n\t\tif (dbg_release_bp_slot(*pevent))\n\t\t\t/*\n\t\t\t * The debugger is responsible for handing the retry on\n\t\t\t * remove failure.\n\t\t\t */\n\t\t\treturn -1;\n\t}\n\treturn 0;\n}\n\nstatic int\nkgdb_remove_hw_break(unsigned long addr, int len, enum kgdb_bptype bptype)\n{\n\tint i;\n\n\tfor (i = 0; i < HBP_NUM; i++)\n\t\tif (breakinfo[i].addr == addr && breakinfo[i].enabled)\n\t\t\tbreak;\n\tif (i == HBP_NUM)\n\t\treturn -1;\n\n\tif (hw_break_release_slot(i)) {\n\t\tprintk(KERN_ERR \"Cannot remove hw breakpoint at %lx\\n\", addr);\n\t\treturn -1;\n\t}\n\tbreakinfo[i].enabled = 0;\n\n\treturn 0;\n}\n\nstatic void kgdb_remove_all_hw_break(void)\n{\n\tint i;\n\tint cpu = raw_smp_processor_id();\n\tstruct perf_event *bp;\n\n\tfor (i = 0; i < HBP_NUM; i++) {\n\t\tif (!breakinfo[i].enabled)\n\t\t\tcontinue;\n\t\tbp = *per_cpu_ptr(breakinfo[i].pev, cpu);\n\t\tif (!bp->attr.disabled) {\n\t\t\tarch_uninstall_hw_breakpoint(bp);\n\t\t\tbp->attr.disabled = 1;\n\t\t\tcontinue;\n\t\t}\n\t\tif (dbg_is_early)\n\t\t\tearly_dr7 &= ~encode_dr7(i, breakinfo[i].len,\n\t\t\t\t\t\t breakinfo[i].type);\n\t\telse if (hw_break_release_slot(i))\n\t\t\tprintk(KERN_ERR \"KGDB: hw bpt remove failed %lx\\n\",\n\t\t\t       breakinfo[i].addr);\n\t\tbreakinfo[i].enabled = 0;\n\t}\n}\n\nstatic int\nkgdb_set_hw_break(unsigned long addr, int len, enum kgdb_bptype bptype)\n{\n\tint i;\n\n\tfor (i = 0; i < HBP_NUM; i++)\n\t\tif (!breakinfo[i].enabled)\n\t\t\tbreak;\n\tif (i == HBP_NUM)\n\t\treturn -1;\n\n\tswitch (bptype) {\n\tcase BP_HARDWARE_BREAKPOINT:\n\t\tlen = 1;\n\t\tbreakinfo[i].type = X86_BREAKPOINT_EXECUTE;\n\t\tbreak;\n\tcase BP_WRITE_WATCHPOINT:\n\t\tbreakinfo[i].type = X86_BREAKPOINT_WRITE;\n\t\tbreak;\n\tcase BP_ACCESS_WATCHPOINT:\n\t\tbreakinfo[i].type = X86_BREAKPOINT_RW;\n\t\tbreak;\n\tdefault:\n\t\treturn -1;\n\t}\n\tswitch (len) {\n\tcase 1:\n\t\tbreakinfo[i].len = X86_BREAKPOINT_LEN_1;\n\t\tbreak;\n\tcase 2:\n\t\tbreakinfo[i].len = X86_BREAKPOINT_LEN_2;\n\t\tbreak;\n\tcase 4:\n\t\tbreakinfo[i].len = X86_BREAKPOINT_LEN_4;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase 8:\n\t\tbreakinfo[i].len = X86_BREAKPOINT_LEN_8;\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn -1;\n\t}\n\tbreakinfo[i].addr = addr;\n\tif (hw_break_reserve_slot(i)) {\n\t\tbreakinfo[i].addr = 0;\n\t\treturn -1;\n\t}\n\tbreakinfo[i].enabled = 1;\n\n\treturn 0;\n}\n\n/**\n *\tkgdb_disable_hw_debug - Disable hardware debugging while we in kgdb.\n *\t@regs: Current &struct pt_regs.\n *\n *\tThis function will be called if the particular architecture must\n *\tdisable hardware debugging while it is processing gdb packets or\n *\thandling exception.\n */\nstatic void kgdb_disable_hw_debug(struct pt_regs *regs)\n{\n\tint i;\n\tint cpu = raw_smp_processor_id();\n\tstruct perf_event *bp;\n\n\t/* Disable hardware debugging while we are in kgdb: */\n\tset_debugreg(0UL, 7);\n\tfor (i = 0; i < HBP_NUM; i++) {\n\t\tif (!breakinfo[i].enabled)\n\t\t\tcontinue;\n\t\tif (dbg_is_early) {\n\t\t\tearly_dr7 &= ~encode_dr7(i, breakinfo[i].len,\n\t\t\t\t\t\t breakinfo[i].type);\n\t\t\tcontinue;\n\t\t}\n\t\tbp = *per_cpu_ptr(breakinfo[i].pev, cpu);\n\t\tif (bp->attr.disabled == 1)\n\t\t\tcontinue;\n\t\tarch_uninstall_hw_breakpoint(bp);\n\t\tbp->attr.disabled = 1;\n\t}\n}\n\n#ifdef CONFIG_SMP\n/**\n *\tkgdb_roundup_cpus - Get other CPUs into a holding pattern\n *\t@flags: Current IRQ state\n *\n *\tOn SMP systems, we need to get the attention of the other CPUs\n *\tand get them be in a known state.  This should do what is needed\n *\tto get the other CPUs to call kgdb_wait(). Note that on some arches,\n *\tthe NMI approach is not used for rounding up all the CPUs. For example,\n *\tin case of MIPS, smp_call_function() is used to roundup CPUs. In\n *\tthis case, we have to make sure that interrupts are enabled before\n *\tcalling smp_call_function(). The argument to this function is\n *\tthe flags that will be used when restoring the interrupts. There is\n *\tlocal_irq_save() call before kgdb_roundup_cpus().\n *\n *\tOn non-SMP systems, this is not called.\n */\nvoid kgdb_roundup_cpus(unsigned long flags)\n{\n\tapic->send_IPI_allbutself(APIC_DM_NMI);\n}\n#endif\n\n/**\n *\tkgdb_arch_handle_exception - Handle architecture specific GDB packets.\n *\t@vector: The error vector of the exception that happened.\n *\t@signo: The signal number of the exception that happened.\n *\t@err_code: The error code of the exception that happened.\n *\t@remcom_in_buffer: The buffer of the packet we have read.\n *\t@remcom_out_buffer: The buffer of %BUFMAX bytes to write a packet into.\n *\t@regs: The &struct pt_regs of the current process.\n *\n *\tThis function MUST handle the 'c' and 's' command packets,\n *\tas well packets to set / remove a hardware breakpoint, if used.\n *\tIf there are additional packets which the hardware needs to handle,\n *\tthey are handled here.  The code should return -1 if it wants to\n *\tprocess more packets, and a %0 or %1 if it wants to exit from the\n *\tkgdb callback.\n */\nint kgdb_arch_handle_exception(int e_vector, int signo, int err_code,\n\t\t\t       char *remcomInBuffer, char *remcomOutBuffer,\n\t\t\t       struct pt_regs *linux_regs)\n{\n\tunsigned long addr;\n\tchar *ptr;\n\n\tswitch (remcomInBuffer[0]) {\n\tcase 'c':\n\tcase 's':\n\t\t/* try to read optional parameter, pc unchanged if no parm */\n\t\tptr = &remcomInBuffer[1];\n\t\tif (kgdb_hex2long(&ptr, &addr))\n\t\t\tlinux_regs->ip = addr;\n\tcase 'D':\n\tcase 'k':\n\t\t/* clear the trace bit */\n\t\tlinux_regs->flags &= ~X86_EFLAGS_TF;\n\t\tatomic_set(&kgdb_cpu_doing_single_step, -1);\n\n\t\t/* set the trace bit if we're stepping */\n\t\tif (remcomInBuffer[0] == 's') {\n\t\t\tlinux_regs->flags |= X86_EFLAGS_TF;\n\t\t\tatomic_set(&kgdb_cpu_doing_single_step,\n\t\t\t\t   raw_smp_processor_id());\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\t/* this means that we do not want to exit from the handler: */\n\treturn -1;\n}\n\nstatic inline int\nsingle_step_cont(struct pt_regs *regs, struct die_args *args)\n{\n\t/*\n\t * Single step exception from kernel space to user space so\n\t * eat the exception and continue the process:\n\t */\n\tprintk(KERN_ERR \"KGDB: trap/step from kernel to user space, \"\n\t\t\t\"resuming...\\n\");\n\tkgdb_arch_handle_exception(args->trapnr, args->signr,\n\t\t\t\t   args->err, \"c\", \"\", regs);\n\t/*\n\t * Reset the BS bit in dr6 (pointed by args->err) to\n\t * denote completion of processing\n\t */\n\t(*(unsigned long *)ERR_PTR(args->err)) &= ~DR_STEP;\n\n\treturn NOTIFY_STOP;\n}\n\nstatic int was_in_debug_nmi[NR_CPUS];\n\nstatic int __kgdb_notify(struct die_args *args, unsigned long cmd)\n{\n\tstruct pt_regs *regs = args->regs;\n\n\tswitch (cmd) {\n\tcase DIE_NMI:\n\t\tif (atomic_read(&kgdb_active) != -1) {\n\t\t\t/* KGDB CPU roundup */\n\t\t\tkgdb_nmicallback(raw_smp_processor_id(), regs);\n\t\t\twas_in_debug_nmi[raw_smp_processor_id()] = 1;\n\t\t\ttouch_nmi_watchdog();\n\t\t\treturn NOTIFY_STOP;\n\t\t}\n\t\treturn NOTIFY_DONE;\n\n\tcase DIE_NMIUNKNOWN:\n\t\tif (was_in_debug_nmi[raw_smp_processor_id()]) {\n\t\t\twas_in_debug_nmi[raw_smp_processor_id()] = 0;\n\t\t\treturn NOTIFY_STOP;\n\t\t}\n\t\treturn NOTIFY_DONE;\n\n\tcase DIE_DEBUG:\n\t\tif (atomic_read(&kgdb_cpu_doing_single_step) != -1) {\n\t\t\tif (user_mode(regs))\n\t\t\t\treturn single_step_cont(regs, args);\n\t\t\tbreak;\n\t\t} else if (test_thread_flag(TIF_SINGLESTEP))\n\t\t\t/* This means a user thread is single stepping\n\t\t\t * a system call which should be ignored\n\t\t\t */\n\t\t\treturn NOTIFY_DONE;\n\t\t/* fall through */\n\tdefault:\n\t\tif (user_mode(regs))\n\t\t\treturn NOTIFY_DONE;\n\t}\n\n\tif (kgdb_handle_exception(args->trapnr, args->signr, cmd, regs))\n\t\treturn NOTIFY_DONE;\n\n\t/* Must touch watchdog before return to normal operation */\n\ttouch_nmi_watchdog();\n\treturn NOTIFY_STOP;\n}\n\nint kgdb_ll_trap(int cmd, const char *str,\n\t\t struct pt_regs *regs, long err, int trap, int sig)\n{\n\tstruct die_args args = {\n\t\t.regs\t= regs,\n\t\t.str\t= str,\n\t\t.err\t= err,\n\t\t.trapnr\t= trap,\n\t\t.signr\t= sig,\n\n\t};\n\n\tif (!kgdb_io_module_registered)\n\t\treturn NOTIFY_DONE;\n\n\treturn __kgdb_notify(&args, cmd);\n}\n\nstatic int\nkgdb_notify(struct notifier_block *self, unsigned long cmd, void *ptr)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tlocal_irq_save(flags);\n\tret = __kgdb_notify(ptr, cmd);\n\tlocal_irq_restore(flags);\n\n\treturn ret;\n}\n\nstatic struct notifier_block kgdb_notifier = {\n\t.notifier_call\t= kgdb_notify,\n\n\t/*\n\t * Lowest-prio notifier priority, we want to be notified last:\n\t */\n\t.priority\t= NMI_LOCAL_LOW_PRIOR,\n};\n\n/**\n *\tkgdb_arch_init - Perform any architecture specific initalization.\n *\n *\tThis function will handle the initalization of any architecture\n *\tspecific callbacks.\n */\nint kgdb_arch_init(void)\n{\n\treturn register_die_notifier(&kgdb_notifier);\n}\n\nstatic void kgdb_hw_overflow_handler(struct perf_event *event,\n\t\tstruct perf_sample_data *data, struct pt_regs *regs)\n{\n\tstruct task_struct *tsk = current;\n\tint i;\n\n\tfor (i = 0; i < 4; i++)\n\t\tif (breakinfo[i].enabled)\n\t\t\ttsk->thread.debugreg6 |= (DR_TRAP0 << i);\n}\n\nvoid kgdb_arch_late(void)\n{\n\tint i, cpu;\n\tstruct perf_event_attr attr;\n\tstruct perf_event **pevent;\n\n\t/*\n\t * Pre-allocate the hw breakpoint structions in the non-atomic\n\t * portion of kgdb because this operation requires mutexs to\n\t * complete.\n\t */\n\thw_breakpoint_init(&attr);\n\tattr.bp_addr = (unsigned long)kgdb_arch_init;\n\tattr.bp_len = HW_BREAKPOINT_LEN_1;\n\tattr.bp_type = HW_BREAKPOINT_W;\n\tattr.disabled = 1;\n\tfor (i = 0; i < HBP_NUM; i++) {\n\t\tif (breakinfo[i].pev)\n\t\t\tcontinue;\n\t\tbreakinfo[i].pev = register_wide_hw_breakpoint(&attr, NULL);\n\t\tif (IS_ERR((void * __force)breakinfo[i].pev)) {\n\t\t\tprintk(KERN_ERR \"kgdb: Could not allocate hw\"\n\t\t\t       \"breakpoints\\nDisabling the kernel debugger\\n\");\n\t\t\tbreakinfo[i].pev = NULL;\n\t\t\tkgdb_arch_exit();\n\t\t\treturn;\n\t\t}\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tpevent = per_cpu_ptr(breakinfo[i].pev, cpu);\n\t\t\tpevent[0]->hw.sample_period = 1;\n\t\t\tpevent[0]->overflow_handler = kgdb_hw_overflow_handler;\n\t\t\tif (pevent[0]->destroy != NULL) {\n\t\t\t\tpevent[0]->destroy = NULL;\n\t\t\t\trelease_bp_slot(*pevent);\n\t\t\t}\n\t\t}\n\t}\n}\n\n/**\n *\tkgdb_arch_exit - Perform any architecture specific uninitalization.\n *\n *\tThis function will handle the uninitalization of any architecture\n *\tspecific callbacks, for dynamic registration and unregistration.\n */\nvoid kgdb_arch_exit(void)\n{\n\tint i;\n\tfor (i = 0; i < 4; i++) {\n\t\tif (breakinfo[i].pev) {\n\t\t\tunregister_wide_hw_breakpoint(breakinfo[i].pev);\n\t\t\tbreakinfo[i].pev = NULL;\n\t\t}\n\t}\n\tunregister_die_notifier(&kgdb_notifier);\n}\n\n/**\n *\n *\tkgdb_skipexception - Bail out of KGDB when we've been triggered.\n *\t@exception: Exception vector number\n *\t@regs: Current &struct pt_regs.\n *\n *\tOn some architectures we need to skip a breakpoint exception when\n *\tit occurs after a breakpoint has been removed.\n *\n * Skip an int3 exception when it occurs after a breakpoint has been\n * removed. Backtrack eip by 1 since the int3 would have caused it to\n * increment by 1.\n */\nint kgdb_skipexception(int exception, struct pt_regs *regs)\n{\n\tif (exception == 3 && kgdb_isremovedbreak(regs->ip - 1)) {\n\t\tregs->ip -= 1;\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nunsigned long kgdb_arch_pc(int exception, struct pt_regs *regs)\n{\n\tif (exception == 3)\n\t\treturn instruction_pointer(regs) - 1;\n\treturn instruction_pointer(regs);\n}\n\nvoid kgdb_arch_set_pc(struct pt_regs *regs, unsigned long ip)\n{\n\tregs->ip = ip;\n}\n\nstruct kgdb_arch arch_kgdb_ops = {\n\t/* Breakpoint instruction: */\n\t.gdb_bpt_instr\t\t= { 0xcc },\n\t.flags\t\t\t= KGDB_HW_BREAKPOINT,\n\t.set_hw_breakpoint\t= kgdb_set_hw_break,\n\t.remove_hw_breakpoint\t= kgdb_remove_hw_break,\n\t.disable_hw_break\t= kgdb_disable_hw_debug,\n\t.remove_all_hw_break\t= kgdb_remove_all_hw_break,\n\t.correct_hw_break\t= kgdb_correct_hw_break,\n};\n", "/* By Ross Biro 1/23/92 */\n/*\n * Pentium III FXSR, SSE support\n *\tGareth Hughes <gareth@valinux.com>, May 2000\n */\n\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/errno.h>\n#include <linux/slab.h>\n#include <linux/ptrace.h>\n#include <linux/regset.h>\n#include <linux/tracehook.h>\n#include <linux/user.h>\n#include <linux/elf.h>\n#include <linux/security.h>\n#include <linux/audit.h>\n#include <linux/seccomp.h>\n#include <linux/signal.h>\n#include <linux/perf_event.h>\n#include <linux/hw_breakpoint.h>\n\n#include <asm/uaccess.h>\n#include <asm/pgtable.h>\n#include <asm/system.h>\n#include <asm/processor.h>\n#include <asm/i387.h>\n#include <asm/debugreg.h>\n#include <asm/ldt.h>\n#include <asm/desc.h>\n#include <asm/prctl.h>\n#include <asm/proto.h>\n#include <asm/hw_breakpoint.h>\n\n#include \"tls.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/syscalls.h>\n\nenum x86_regset {\n\tREGSET_GENERAL,\n\tREGSET_FP,\n\tREGSET_XFP,\n\tREGSET_IOPERM64 = REGSET_XFP,\n\tREGSET_XSTATE,\n\tREGSET_TLS,\n\tREGSET_IOPERM32,\n};\n\nstruct pt_regs_offset {\n\tconst char *name;\n\tint offset;\n};\n\n#define REG_OFFSET_NAME(r) {.name = #r, .offset = offsetof(struct pt_regs, r)}\n#define REG_OFFSET_END {.name = NULL, .offset = 0}\n\nstatic const struct pt_regs_offset regoffset_table[] = {\n#ifdef CONFIG_X86_64\n\tREG_OFFSET_NAME(r15),\n\tREG_OFFSET_NAME(r14),\n\tREG_OFFSET_NAME(r13),\n\tREG_OFFSET_NAME(r12),\n\tREG_OFFSET_NAME(r11),\n\tREG_OFFSET_NAME(r10),\n\tREG_OFFSET_NAME(r9),\n\tREG_OFFSET_NAME(r8),\n#endif\n\tREG_OFFSET_NAME(bx),\n\tREG_OFFSET_NAME(cx),\n\tREG_OFFSET_NAME(dx),\n\tREG_OFFSET_NAME(si),\n\tREG_OFFSET_NAME(di),\n\tREG_OFFSET_NAME(bp),\n\tREG_OFFSET_NAME(ax),\n#ifdef CONFIG_X86_32\n\tREG_OFFSET_NAME(ds),\n\tREG_OFFSET_NAME(es),\n\tREG_OFFSET_NAME(fs),\n\tREG_OFFSET_NAME(gs),\n#endif\n\tREG_OFFSET_NAME(orig_ax),\n\tREG_OFFSET_NAME(ip),\n\tREG_OFFSET_NAME(cs),\n\tREG_OFFSET_NAME(flags),\n\tREG_OFFSET_NAME(sp),\n\tREG_OFFSET_NAME(ss),\n\tREG_OFFSET_END,\n};\n\n/**\n * regs_query_register_offset() - query register offset from its name\n * @name:\tthe name of a register\n *\n * regs_query_register_offset() returns the offset of a register in struct\n * pt_regs from its name. If the name is invalid, this returns -EINVAL;\n */\nint regs_query_register_offset(const char *name)\n{\n\tconst struct pt_regs_offset *roff;\n\tfor (roff = regoffset_table; roff->name != NULL; roff++)\n\t\tif (!strcmp(roff->name, name))\n\t\t\treturn roff->offset;\n\treturn -EINVAL;\n}\n\n/**\n * regs_query_register_name() - query register name from its offset\n * @offset:\tthe offset of a register in struct pt_regs.\n *\n * regs_query_register_name() returns the name of a register from its\n * offset in struct pt_regs. If the @offset is invalid, this returns NULL;\n */\nconst char *regs_query_register_name(unsigned int offset)\n{\n\tconst struct pt_regs_offset *roff;\n\tfor (roff = regoffset_table; roff->name != NULL; roff++)\n\t\tif (roff->offset == offset)\n\t\t\treturn roff->name;\n\treturn NULL;\n}\n\nstatic const int arg_offs_table[] = {\n#ifdef CONFIG_X86_32\n\t[0] = offsetof(struct pt_regs, ax),\n\t[1] = offsetof(struct pt_regs, dx),\n\t[2] = offsetof(struct pt_regs, cx)\n#else /* CONFIG_X86_64 */\n\t[0] = offsetof(struct pt_regs, di),\n\t[1] = offsetof(struct pt_regs, si),\n\t[2] = offsetof(struct pt_regs, dx),\n\t[3] = offsetof(struct pt_regs, cx),\n\t[4] = offsetof(struct pt_regs, r8),\n\t[5] = offsetof(struct pt_regs, r9)\n#endif\n};\n\n/*\n * does not yet catch signals sent when the child dies.\n * in exit.c or in signal.c.\n */\n\n/*\n * Determines which flags the user has access to [1 = access, 0 = no access].\n */\n#define FLAG_MASK_32\t\t((unsigned long)\t\t\t\\\n\t\t\t\t (X86_EFLAGS_CF | X86_EFLAGS_PF |\t\\\n\t\t\t\t  X86_EFLAGS_AF | X86_EFLAGS_ZF |\t\\\n\t\t\t\t  X86_EFLAGS_SF | X86_EFLAGS_TF |\t\\\n\t\t\t\t  X86_EFLAGS_DF | X86_EFLAGS_OF |\t\\\n\t\t\t\t  X86_EFLAGS_RF | X86_EFLAGS_AC))\n\n/*\n * Determines whether a value may be installed in a segment register.\n */\nstatic inline bool invalid_selector(u16 value)\n{\n\treturn unlikely(value != 0 && (value & SEGMENT_RPL_MASK) != USER_RPL);\n}\n\n#ifdef CONFIG_X86_32\n\n#define FLAG_MASK\t\tFLAG_MASK_32\n\nstatic unsigned long *pt_regs_access(struct pt_regs *regs, unsigned long regno)\n{\n\tBUILD_BUG_ON(offsetof(struct pt_regs, bx) != 0);\n\treturn &regs->bx + (regno >> 2);\n}\n\nstatic u16 get_segment_reg(struct task_struct *task, unsigned long offset)\n{\n\t/*\n\t * Returning the value truncates it to 16 bits.\n\t */\n\tunsigned int retval;\n\tif (offset != offsetof(struct user_regs_struct, gs))\n\t\tretval = *pt_regs_access(task_pt_regs(task), offset);\n\telse {\n\t\tif (task == current)\n\t\t\tretval = get_user_gs(task_pt_regs(task));\n\t\telse\n\t\t\tretval = task_user_gs(task);\n\t}\n\treturn retval;\n}\n\nstatic int set_segment_reg(struct task_struct *task,\n\t\t\t   unsigned long offset, u16 value)\n{\n\t/*\n\t * The value argument was already truncated to 16 bits.\n\t */\n\tif (invalid_selector(value))\n\t\treturn -EIO;\n\n\t/*\n\t * For %cs and %ss we cannot permit a null selector.\n\t * We can permit a bogus selector as long as it has USER_RPL.\n\t * Null selectors are fine for other segment registers, but\n\t * we will never get back to user mode with invalid %cs or %ss\n\t * and will take the trap in iret instead.  Much code relies\n\t * on user_mode() to distinguish a user trap frame (which can\n\t * safely use invalid selectors) from a kernel trap frame.\n\t */\n\tswitch (offset) {\n\tcase offsetof(struct user_regs_struct, cs):\n\tcase offsetof(struct user_regs_struct, ss):\n\t\tif (unlikely(value == 0))\n\t\t\treturn -EIO;\n\n\tdefault:\n\t\t*pt_regs_access(task_pt_regs(task), offset) = value;\n\t\tbreak;\n\n\tcase offsetof(struct user_regs_struct, gs):\n\t\tif (task == current)\n\t\t\tset_user_gs(task_pt_regs(task), value);\n\t\telse\n\t\t\ttask_user_gs(task) = value;\n\t}\n\n\treturn 0;\n}\n\n#else  /* CONFIG_X86_64 */\n\n#define FLAG_MASK\t\t(FLAG_MASK_32 | X86_EFLAGS_NT)\n\nstatic unsigned long *pt_regs_access(struct pt_regs *regs, unsigned long offset)\n{\n\tBUILD_BUG_ON(offsetof(struct pt_regs, r15) != 0);\n\treturn &regs->r15 + (offset / sizeof(regs->r15));\n}\n\nstatic u16 get_segment_reg(struct task_struct *task, unsigned long offset)\n{\n\t/*\n\t * Returning the value truncates it to 16 bits.\n\t */\n\tunsigned int seg;\n\n\tswitch (offset) {\n\tcase offsetof(struct user_regs_struct, fs):\n\t\tif (task == current) {\n\t\t\t/* Older gas can't assemble movq %?s,%r?? */\n\t\t\tasm(\"movl %%fs,%0\" : \"=r\" (seg));\n\t\t\treturn seg;\n\t\t}\n\t\treturn task->thread.fsindex;\n\tcase offsetof(struct user_regs_struct, gs):\n\t\tif (task == current) {\n\t\t\tasm(\"movl %%gs,%0\" : \"=r\" (seg));\n\t\t\treturn seg;\n\t\t}\n\t\treturn task->thread.gsindex;\n\tcase offsetof(struct user_regs_struct, ds):\n\t\tif (task == current) {\n\t\t\tasm(\"movl %%ds,%0\" : \"=r\" (seg));\n\t\t\treturn seg;\n\t\t}\n\t\treturn task->thread.ds;\n\tcase offsetof(struct user_regs_struct, es):\n\t\tif (task == current) {\n\t\t\tasm(\"movl %%es,%0\" : \"=r\" (seg));\n\t\t\treturn seg;\n\t\t}\n\t\treturn task->thread.es;\n\n\tcase offsetof(struct user_regs_struct, cs):\n\tcase offsetof(struct user_regs_struct, ss):\n\t\tbreak;\n\t}\n\treturn *pt_regs_access(task_pt_regs(task), offset);\n}\n\nstatic int set_segment_reg(struct task_struct *task,\n\t\t\t   unsigned long offset, u16 value)\n{\n\t/*\n\t * The value argument was already truncated to 16 bits.\n\t */\n\tif (invalid_selector(value))\n\t\treturn -EIO;\n\n\tswitch (offset) {\n\tcase offsetof(struct user_regs_struct,fs):\n\t\t/*\n\t\t * If this is setting fs as for normal 64-bit use but\n\t\t * setting fs_base has implicitly changed it, leave it.\n\t\t */\n\t\tif ((value == FS_TLS_SEL && task->thread.fsindex == 0 &&\n\t\t     task->thread.fs != 0) ||\n\t\t    (value == 0 && task->thread.fsindex == FS_TLS_SEL &&\n\t\t     task->thread.fs == 0))\n\t\t\tbreak;\n\t\ttask->thread.fsindex = value;\n\t\tif (task == current)\n\t\t\tloadsegment(fs, task->thread.fsindex);\n\t\tbreak;\n\tcase offsetof(struct user_regs_struct,gs):\n\t\t/*\n\t\t * If this is setting gs as for normal 64-bit use but\n\t\t * setting gs_base has implicitly changed it, leave it.\n\t\t */\n\t\tif ((value == GS_TLS_SEL && task->thread.gsindex == 0 &&\n\t\t     task->thread.gs != 0) ||\n\t\t    (value == 0 && task->thread.gsindex == GS_TLS_SEL &&\n\t\t     task->thread.gs == 0))\n\t\t\tbreak;\n\t\ttask->thread.gsindex = value;\n\t\tif (task == current)\n\t\t\tload_gs_index(task->thread.gsindex);\n\t\tbreak;\n\tcase offsetof(struct user_regs_struct,ds):\n\t\ttask->thread.ds = value;\n\t\tif (task == current)\n\t\t\tloadsegment(ds, task->thread.ds);\n\t\tbreak;\n\tcase offsetof(struct user_regs_struct,es):\n\t\ttask->thread.es = value;\n\t\tif (task == current)\n\t\t\tloadsegment(es, task->thread.es);\n\t\tbreak;\n\n\t\t/*\n\t\t * Can't actually change these in 64-bit mode.\n\t\t */\n\tcase offsetof(struct user_regs_struct,cs):\n\t\tif (unlikely(value == 0))\n\t\t\treturn -EIO;\n#ifdef CONFIG_IA32_EMULATION\n\t\tif (test_tsk_thread_flag(task, TIF_IA32))\n\t\t\ttask_pt_regs(task)->cs = value;\n#endif\n\t\tbreak;\n\tcase offsetof(struct user_regs_struct,ss):\n\t\tif (unlikely(value == 0))\n\t\t\treturn -EIO;\n#ifdef CONFIG_IA32_EMULATION\n\t\tif (test_tsk_thread_flag(task, TIF_IA32))\n\t\t\ttask_pt_regs(task)->ss = value;\n#endif\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\n#endif\t/* CONFIG_X86_32 */\n\nstatic unsigned long get_flags(struct task_struct *task)\n{\n\tunsigned long retval = task_pt_regs(task)->flags;\n\n\t/*\n\t * If the debugger set TF, hide it from the readout.\n\t */\n\tif (test_tsk_thread_flag(task, TIF_FORCED_TF))\n\t\tretval &= ~X86_EFLAGS_TF;\n\n\treturn retval;\n}\n\nstatic int set_flags(struct task_struct *task, unsigned long value)\n{\n\tstruct pt_regs *regs = task_pt_regs(task);\n\n\t/*\n\t * If the user value contains TF, mark that\n\t * it was not \"us\" (the debugger) that set it.\n\t * If not, make sure it stays set if we had.\n\t */\n\tif (value & X86_EFLAGS_TF)\n\t\tclear_tsk_thread_flag(task, TIF_FORCED_TF);\n\telse if (test_tsk_thread_flag(task, TIF_FORCED_TF))\n\t\tvalue |= X86_EFLAGS_TF;\n\n\tregs->flags = (regs->flags & ~FLAG_MASK) | (value & FLAG_MASK);\n\n\treturn 0;\n}\n\nstatic int putreg(struct task_struct *child,\n\t\t  unsigned long offset, unsigned long value)\n{\n\tswitch (offset) {\n\tcase offsetof(struct user_regs_struct, cs):\n\tcase offsetof(struct user_regs_struct, ds):\n\tcase offsetof(struct user_regs_struct, es):\n\tcase offsetof(struct user_regs_struct, fs):\n\tcase offsetof(struct user_regs_struct, gs):\n\tcase offsetof(struct user_regs_struct, ss):\n\t\treturn set_segment_reg(child, offset, value);\n\n\tcase offsetof(struct user_regs_struct, flags):\n\t\treturn set_flags(child, value);\n\n#ifdef CONFIG_X86_64\n\tcase offsetof(struct user_regs_struct,fs_base):\n\t\tif (value >= TASK_SIZE_OF(child))\n\t\t\treturn -EIO;\n\t\t/*\n\t\t * When changing the segment base, use do_arch_prctl\n\t\t * to set either thread.fs or thread.fsindex and the\n\t\t * corresponding GDT slot.\n\t\t */\n\t\tif (child->thread.fs != value)\n\t\t\treturn do_arch_prctl(child, ARCH_SET_FS, value);\n\t\treturn 0;\n\tcase offsetof(struct user_regs_struct,gs_base):\n\t\t/*\n\t\t * Exactly the same here as the %fs handling above.\n\t\t */\n\t\tif (value >= TASK_SIZE_OF(child))\n\t\t\treturn -EIO;\n\t\tif (child->thread.gs != value)\n\t\t\treturn do_arch_prctl(child, ARCH_SET_GS, value);\n\t\treturn 0;\n#endif\n\t}\n\n\t*pt_regs_access(task_pt_regs(child), offset) = value;\n\treturn 0;\n}\n\nstatic unsigned long getreg(struct task_struct *task, unsigned long offset)\n{\n\tswitch (offset) {\n\tcase offsetof(struct user_regs_struct, cs):\n\tcase offsetof(struct user_regs_struct, ds):\n\tcase offsetof(struct user_regs_struct, es):\n\tcase offsetof(struct user_regs_struct, fs):\n\tcase offsetof(struct user_regs_struct, gs):\n\tcase offsetof(struct user_regs_struct, ss):\n\t\treturn get_segment_reg(task, offset);\n\n\tcase offsetof(struct user_regs_struct, flags):\n\t\treturn get_flags(task);\n\n#ifdef CONFIG_X86_64\n\tcase offsetof(struct user_regs_struct, fs_base): {\n\t\t/*\n\t\t * do_arch_prctl may have used a GDT slot instead of\n\t\t * the MSR.  To userland, it appears the same either\n\t\t * way, except the %fs segment selector might not be 0.\n\t\t */\n\t\tunsigned int seg = task->thread.fsindex;\n\t\tif (task->thread.fs != 0)\n\t\t\treturn task->thread.fs;\n\t\tif (task == current)\n\t\t\tasm(\"movl %%fs,%0\" : \"=r\" (seg));\n\t\tif (seg != FS_TLS_SEL)\n\t\t\treturn 0;\n\t\treturn get_desc_base(&task->thread.tls_array[FS_TLS]);\n\t}\n\tcase offsetof(struct user_regs_struct, gs_base): {\n\t\t/*\n\t\t * Exactly the same here as the %fs handling above.\n\t\t */\n\t\tunsigned int seg = task->thread.gsindex;\n\t\tif (task->thread.gs != 0)\n\t\t\treturn task->thread.gs;\n\t\tif (task == current)\n\t\t\tasm(\"movl %%gs,%0\" : \"=r\" (seg));\n\t\tif (seg != GS_TLS_SEL)\n\t\t\treturn 0;\n\t\treturn get_desc_base(&task->thread.tls_array[GS_TLS]);\n\t}\n#endif\n\t}\n\n\treturn *pt_regs_access(task_pt_regs(task), offset);\n}\n\nstatic int genregs_get(struct task_struct *target,\n\t\t       const struct user_regset *regset,\n\t\t       unsigned int pos, unsigned int count,\n\t\t       void *kbuf, void __user *ubuf)\n{\n\tif (kbuf) {\n\t\tunsigned long *k = kbuf;\n\t\twhile (count >= sizeof(*k)) {\n\t\t\t*k++ = getreg(target, pos);\n\t\t\tcount -= sizeof(*k);\n\t\t\tpos += sizeof(*k);\n\t\t}\n\t} else {\n\t\tunsigned long __user *u = ubuf;\n\t\twhile (count >= sizeof(*u)) {\n\t\t\tif (__put_user(getreg(target, pos), u++))\n\t\t\t\treturn -EFAULT;\n\t\t\tcount -= sizeof(*u);\n\t\t\tpos += sizeof(*u);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int genregs_set(struct task_struct *target,\n\t\t       const struct user_regset *regset,\n\t\t       unsigned int pos, unsigned int count,\n\t\t       const void *kbuf, const void __user *ubuf)\n{\n\tint ret = 0;\n\tif (kbuf) {\n\t\tconst unsigned long *k = kbuf;\n\t\twhile (count >= sizeof(*k) && !ret) {\n\t\t\tret = putreg(target, pos, *k++);\n\t\t\tcount -= sizeof(*k);\n\t\t\tpos += sizeof(*k);\n\t\t}\n\t} else {\n\t\tconst unsigned long  __user *u = ubuf;\n\t\twhile (count >= sizeof(*u) && !ret) {\n\t\t\tunsigned long word;\n\t\t\tret = __get_user(word, u++);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tret = putreg(target, pos, word);\n\t\t\tcount -= sizeof(*u);\n\t\t\tpos += sizeof(*u);\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic void ptrace_triggered(struct perf_event *bp,\n\t\t\t     struct perf_sample_data *data,\n\t\t\t     struct pt_regs *regs)\n{\n\tint i;\n\tstruct thread_struct *thread = &(current->thread);\n\n\t/*\n\t * Store in the virtual DR6 register the fact that the breakpoint\n\t * was hit so the thread's debugger will see it.\n\t */\n\tfor (i = 0; i < HBP_NUM; i++) {\n\t\tif (thread->ptrace_bps[i] == bp)\n\t\t\tbreak;\n\t}\n\n\tthread->debugreg6 |= (DR_TRAP0 << i);\n}\n\n/*\n * Walk through every ptrace breakpoints for this thread and\n * build the dr7 value on top of their attributes.\n *\n */\nstatic unsigned long ptrace_get_dr7(struct perf_event *bp[])\n{\n\tint i;\n\tint dr7 = 0;\n\tstruct arch_hw_breakpoint *info;\n\n\tfor (i = 0; i < HBP_NUM; i++) {\n\t\tif (bp[i] && !bp[i]->attr.disabled) {\n\t\t\tinfo = counter_arch_bp(bp[i]);\n\t\t\tdr7 |= encode_dr7(i, info->len, info->type);\n\t\t}\n\t}\n\n\treturn dr7;\n}\n\nstatic int\nptrace_modify_breakpoint(struct perf_event *bp, int len, int type,\n\t\t\t struct task_struct *tsk, int disabled)\n{\n\tint err;\n\tint gen_len, gen_type;\n\tstruct perf_event_attr attr;\n\n\t/*\n\t * We should have at least an inactive breakpoint at this\n\t * slot. It means the user is writing dr7 without having\n\t * written the address register first\n\t */\n\tif (!bp)\n\t\treturn -EINVAL;\n\n\terr = arch_bp_generic_fields(len, type, &gen_len, &gen_type);\n\tif (err)\n\t\treturn err;\n\n\tattr = bp->attr;\n\tattr.bp_len = gen_len;\n\tattr.bp_type = gen_type;\n\tattr.disabled = disabled;\n\n\treturn modify_user_hw_breakpoint(bp, &attr);\n}\n\n/*\n * Handle ptrace writes to debug register 7.\n */\nstatic int ptrace_write_dr7(struct task_struct *tsk, unsigned long data)\n{\n\tstruct thread_struct *thread = &(tsk->thread);\n\tunsigned long old_dr7;\n\tint i, orig_ret = 0, rc = 0;\n\tint enabled, second_pass = 0;\n\tunsigned len, type;\n\tstruct perf_event *bp;\n\n\tif (ptrace_get_breakpoints(tsk) < 0)\n\t\treturn -ESRCH;\n\n\tdata &= ~DR_CONTROL_RESERVED;\n\told_dr7 = ptrace_get_dr7(thread->ptrace_bps);\nrestore:\n\t/*\n\t * Loop through all the hardware breakpoints, making the\n\t * appropriate changes to each.\n\t */\n\tfor (i = 0; i < HBP_NUM; i++) {\n\t\tenabled = decode_dr7(data, i, &len, &type);\n\t\tbp = thread->ptrace_bps[i];\n\n\t\tif (!enabled) {\n\t\t\tif (bp) {\n\t\t\t\t/*\n\t\t\t\t * Don't unregister the breakpoints right-away,\n\t\t\t\t * unless all register_user_hw_breakpoint()\n\t\t\t\t * requests have succeeded. This prevents\n\t\t\t\t * any window of opportunity for debug\n\t\t\t\t * register grabbing by other users.\n\t\t\t\t */\n\t\t\t\tif (!second_pass)\n\t\t\t\t\tcontinue;\n\n\t\t\t\trc = ptrace_modify_breakpoint(bp, len, type,\n\t\t\t\t\t\t\t      tsk, 1);\n\t\t\t\tif (rc)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\trc = ptrace_modify_breakpoint(bp, len, type, tsk, 0);\n\t\tif (rc)\n\t\t\tbreak;\n\t}\n\t/*\n\t * Make a second pass to free the remaining unused breakpoints\n\t * or to restore the original breakpoints if an error occurred.\n\t */\n\tif (!second_pass) {\n\t\tsecond_pass = 1;\n\t\tif (rc < 0) {\n\t\t\torig_ret = rc;\n\t\t\tdata = old_dr7;\n\t\t}\n\t\tgoto restore;\n\t}\n\n\tptrace_put_breakpoints(tsk);\n\n\treturn ((orig_ret < 0) ? orig_ret : rc);\n}\n\n/*\n * Handle PTRACE_PEEKUSR calls for the debug register area.\n */\nstatic unsigned long ptrace_get_debugreg(struct task_struct *tsk, int n)\n{\n\tstruct thread_struct *thread = &(tsk->thread);\n\tunsigned long val = 0;\n\n\tif (n < HBP_NUM) {\n\t\tstruct perf_event *bp;\n\n\t\tif (ptrace_get_breakpoints(tsk) < 0)\n\t\t\treturn -ESRCH;\n\n\t\tbp = thread->ptrace_bps[n];\n\t\tif (!bp)\n\t\t\tval = 0;\n\t\telse\n\t\t\tval = bp->hw.info.address;\n\n\t\tptrace_put_breakpoints(tsk);\n\t} else if (n == 6) {\n\t\tval = thread->debugreg6;\n\t } else if (n == 7) {\n\t\tval = thread->ptrace_dr7;\n\t}\n\treturn val;\n}\n\nstatic int ptrace_set_breakpoint_addr(struct task_struct *tsk, int nr,\n\t\t\t\t      unsigned long addr)\n{\n\tstruct perf_event *bp;\n\tstruct thread_struct *t = &tsk->thread;\n\tstruct perf_event_attr attr;\n\tint err = 0;\n\n\tif (ptrace_get_breakpoints(tsk) < 0)\n\t\treturn -ESRCH;\n\n\tif (!t->ptrace_bps[nr]) {\n\t\tptrace_breakpoint_init(&attr);\n\t\t/*\n\t\t * Put stub len and type to register (reserve) an inactive but\n\t\t * correct bp\n\t\t */\n\t\tattr.bp_addr = addr;\n\t\tattr.bp_len = HW_BREAKPOINT_LEN_1;\n\t\tattr.bp_type = HW_BREAKPOINT_W;\n\t\tattr.disabled = 1;\n\n\t\tbp = register_user_hw_breakpoint(&attr, ptrace_triggered, tsk);\n\n\t\t/*\n\t\t * CHECKME: the previous code returned -EIO if the addr wasn't\n\t\t * a valid task virtual addr. The new one will return -EINVAL in\n\t\t *  this case.\n\t\t * -EINVAL may be what we want for in-kernel breakpoints users,\n\t\t * but -EIO looks better for ptrace, since we refuse a register\n\t\t * writing for the user. And anyway this is the previous\n\t\t * behaviour.\n\t\t */\n\t\tif (IS_ERR(bp)) {\n\t\t\terr = PTR_ERR(bp);\n\t\t\tgoto put;\n\t\t}\n\n\t\tt->ptrace_bps[nr] = bp;\n\t} else {\n\t\tbp = t->ptrace_bps[nr];\n\n\t\tattr = bp->attr;\n\t\tattr.bp_addr = addr;\n\t\terr = modify_user_hw_breakpoint(bp, &attr);\n\t}\n\nput:\n\tptrace_put_breakpoints(tsk);\n\treturn err;\n}\n\n/*\n * Handle PTRACE_POKEUSR calls for the debug register area.\n */\nint ptrace_set_debugreg(struct task_struct *tsk, int n, unsigned long val)\n{\n\tstruct thread_struct *thread = &(tsk->thread);\n\tint rc = 0;\n\n\t/* There are no DR4 or DR5 registers */\n\tif (n == 4 || n == 5)\n\t\treturn -EIO;\n\n\tif (n == 6) {\n\t\tthread->debugreg6 = val;\n\t\tgoto ret_path;\n\t}\n\tif (n < HBP_NUM) {\n\t\trc = ptrace_set_breakpoint_addr(tsk, n, val);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\t/* All that's left is DR7 */\n\tif (n == 7) {\n\t\trc = ptrace_write_dr7(tsk, val);\n\t\tif (!rc)\n\t\t\tthread->ptrace_dr7 = val;\n\t}\n\nret_path:\n\treturn rc;\n}\n\n/*\n * These access the current or another (stopped) task's io permission\n * bitmap for debugging or core dump.\n */\nstatic int ioperm_active(struct task_struct *target,\n\t\t\t const struct user_regset *regset)\n{\n\treturn target->thread.io_bitmap_max / regset->size;\n}\n\nstatic int ioperm_get(struct task_struct *target,\n\t\t      const struct user_regset *regset,\n\t\t      unsigned int pos, unsigned int count,\n\t\t      void *kbuf, void __user *ubuf)\n{\n\tif (!target->thread.io_bitmap_ptr)\n\t\treturn -ENXIO;\n\n\treturn user_regset_copyout(&pos, &count, &kbuf, &ubuf,\n\t\t\t\t   target->thread.io_bitmap_ptr,\n\t\t\t\t   0, IO_BITMAP_BYTES);\n}\n\n/*\n * Called by kernel/ptrace.c when detaching..\n *\n * Make sure the single step bit is not set.\n */\nvoid ptrace_disable(struct task_struct *child)\n{\n\tuser_disable_single_step(child);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(child, TIF_SYSCALL_EMU);\n#endif\n}\n\n#if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION\nstatic const struct user_regset_view user_x86_32_view; /* Initialized below. */\n#endif\n\nlong arch_ptrace(struct task_struct *child, long request,\n\t\t unsigned long addr, unsigned long data)\n{\n\tint ret;\n\tunsigned long __user *datap = (unsigned long __user *)data;\n\n\tswitch (request) {\n\t/* read the word at location addr in the USER area. */\n\tcase PTRACE_PEEKUSR: {\n\t\tunsigned long tmp;\n\n\t\tret = -EIO;\n\t\tif ((addr & (sizeof(data) - 1)) || addr >= sizeof(struct user))\n\t\t\tbreak;\n\n\t\ttmp = 0;  /* Default return condition */\n\t\tif (addr < sizeof(struct user_regs_struct))\n\t\t\ttmp = getreg(child, addr);\n\t\telse if (addr >= offsetof(struct user, u_debugreg[0]) &&\n\t\t\t addr <= offsetof(struct user, u_debugreg[7])) {\n\t\t\taddr -= offsetof(struct user, u_debugreg[0]);\n\t\t\ttmp = ptrace_get_debugreg(child, addr / sizeof(data));\n\t\t}\n\t\tret = put_user(tmp, datap);\n\t\tbreak;\n\t}\n\n\tcase PTRACE_POKEUSR: /* write the word at location addr in the USER area */\n\t\tret = -EIO;\n\t\tif ((addr & (sizeof(data) - 1)) || addr >= sizeof(struct user))\n\t\t\tbreak;\n\n\t\tif (addr < sizeof(struct user_regs_struct))\n\t\t\tret = putreg(child, addr, data);\n\t\telse if (addr >= offsetof(struct user, u_debugreg[0]) &&\n\t\t\t addr <= offsetof(struct user, u_debugreg[7])) {\n\t\t\taddr -= offsetof(struct user, u_debugreg[0]);\n\t\t\tret = ptrace_set_debugreg(child,\n\t\t\t\t\t\t  addr / sizeof(data), data);\n\t\t}\n\t\tbreak;\n\n\tcase PTRACE_GETREGS:\t/* Get all gp regs from the child. */\n\t\treturn copy_regset_to_user(child,\n\t\t\t\t\t   task_user_regset_view(current),\n\t\t\t\t\t   REGSET_GENERAL,\n\t\t\t\t\t   0, sizeof(struct user_regs_struct),\n\t\t\t\t\t   datap);\n\n\tcase PTRACE_SETREGS:\t/* Set all gp regs in the child. */\n\t\treturn copy_regset_from_user(child,\n\t\t\t\t\t     task_user_regset_view(current),\n\t\t\t\t\t     REGSET_GENERAL,\n\t\t\t\t\t     0, sizeof(struct user_regs_struct),\n\t\t\t\t\t     datap);\n\n\tcase PTRACE_GETFPREGS:\t/* Get the child FPU state. */\n\t\treturn copy_regset_to_user(child,\n\t\t\t\t\t   task_user_regset_view(current),\n\t\t\t\t\t   REGSET_FP,\n\t\t\t\t\t   0, sizeof(struct user_i387_struct),\n\t\t\t\t\t   datap);\n\n\tcase PTRACE_SETFPREGS:\t/* Set the child FPU state. */\n\t\treturn copy_regset_from_user(child,\n\t\t\t\t\t     task_user_regset_view(current),\n\t\t\t\t\t     REGSET_FP,\n\t\t\t\t\t     0, sizeof(struct user_i387_struct),\n\t\t\t\t\t     datap);\n\n#ifdef CONFIG_X86_32\n\tcase PTRACE_GETFPXREGS:\t/* Get the child extended FPU state. */\n\t\treturn copy_regset_to_user(child, &user_x86_32_view,\n\t\t\t\t\t   REGSET_XFP,\n\t\t\t\t\t   0, sizeof(struct user_fxsr_struct),\n\t\t\t\t\t   datap) ? -EIO : 0;\n\n\tcase PTRACE_SETFPXREGS:\t/* Set the child extended FPU state. */\n\t\treturn copy_regset_from_user(child, &user_x86_32_view,\n\t\t\t\t\t     REGSET_XFP,\n\t\t\t\t\t     0, sizeof(struct user_fxsr_struct),\n\t\t\t\t\t     datap) ? -EIO : 0;\n#endif\n\n#if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION\n\tcase PTRACE_GET_THREAD_AREA:\n\t\tif ((int) addr < 0)\n\t\t\treturn -EIO;\n\t\tret = do_get_thread_area(child, addr,\n\t\t\t\t\t(struct user_desc __user *)data);\n\t\tbreak;\n\n\tcase PTRACE_SET_THREAD_AREA:\n\t\tif ((int) addr < 0)\n\t\t\treturn -EIO;\n\t\tret = do_set_thread_area(child, addr,\n\t\t\t\t\t(struct user_desc __user *)data, 0);\n\t\tbreak;\n#endif\n\n#ifdef CONFIG_X86_64\n\t\t/* normal 64bit interface to access TLS data.\n\t\t   Works just like arch_prctl, except that the arguments\n\t\t   are reversed. */\n\tcase PTRACE_ARCH_PRCTL:\n\t\tret = do_arch_prctl(child, data, addr);\n\t\tbreak;\n#endif\n\n\tdefault:\n\t\tret = ptrace_request(child, request, addr, data);\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n#ifdef CONFIG_IA32_EMULATION\n\n#include <linux/compat.h>\n#include <linux/syscalls.h>\n#include <asm/ia32.h>\n#include <asm/user32.h>\n\n#define R32(l,q)\t\t\t\t\t\t\t\\\n\tcase offsetof(struct user32, regs.l):\t\t\t\t\\\n\t\tregs->q = value; break\n\n#define SEG32(rs)\t\t\t\t\t\t\t\\\n\tcase offsetof(struct user32, regs.rs):\t\t\t\t\\\n\t\treturn set_segment_reg(child,\t\t\t\t\\\n\t\t\t\t       offsetof(struct user_regs_struct, rs), \\\n\t\t\t\t       value);\t\t\t\t\\\n\t\tbreak\n\nstatic int putreg32(struct task_struct *child, unsigned regno, u32 value)\n{\n\tstruct pt_regs *regs = task_pt_regs(child);\n\n\tswitch (regno) {\n\n\tSEG32(cs);\n\tSEG32(ds);\n\tSEG32(es);\n\tSEG32(fs);\n\tSEG32(gs);\n\tSEG32(ss);\n\n\tR32(ebx, bx);\n\tR32(ecx, cx);\n\tR32(edx, dx);\n\tR32(edi, di);\n\tR32(esi, si);\n\tR32(ebp, bp);\n\tR32(eax, ax);\n\tR32(eip, ip);\n\tR32(esp, sp);\n\n\tcase offsetof(struct user32, regs.orig_eax):\n\t\t/*\n\t\t * A 32-bit debugger setting orig_eax means to restore\n\t\t * the state of the task restarting a 32-bit syscall.\n\t\t * Make sure we interpret the -ERESTART* codes correctly\n\t\t * in case the task is not actually still sitting at the\n\t\t * exit from a 32-bit syscall with TS_COMPAT still set.\n\t\t */\n\t\tregs->orig_ax = value;\n\t\tif (syscall_get_nr(child, regs) >= 0)\n\t\t\ttask_thread_info(child)->status |= TS_COMPAT;\n\t\tbreak;\n\n\tcase offsetof(struct user32, regs.eflags):\n\t\treturn set_flags(child, value);\n\n\tcase offsetof(struct user32, u_debugreg[0]) ...\n\t\toffsetof(struct user32, u_debugreg[7]):\n\t\tregno -= offsetof(struct user32, u_debugreg[0]);\n\t\treturn ptrace_set_debugreg(child, regno / 4, value);\n\n\tdefault:\n\t\tif (regno > sizeof(struct user32) || (regno & 3))\n\t\t\treturn -EIO;\n\n\t\t/*\n\t\t * Other dummy fields in the virtual user structure\n\t\t * are ignored\n\t\t */\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\n#undef R32\n#undef SEG32\n\n#define R32(l,q)\t\t\t\t\t\t\t\\\n\tcase offsetof(struct user32, regs.l):\t\t\t\t\\\n\t\t*val = regs->q; break\n\n#define SEG32(rs)\t\t\t\t\t\t\t\\\n\tcase offsetof(struct user32, regs.rs):\t\t\t\t\\\n\t\t*val = get_segment_reg(child,\t\t\t\t\\\n\t\t\t\t       offsetof(struct user_regs_struct, rs)); \\\n\t\tbreak\n\nstatic int getreg32(struct task_struct *child, unsigned regno, u32 *val)\n{\n\tstruct pt_regs *regs = task_pt_regs(child);\n\n\tswitch (regno) {\n\n\tSEG32(ds);\n\tSEG32(es);\n\tSEG32(fs);\n\tSEG32(gs);\n\n\tR32(cs, cs);\n\tR32(ss, ss);\n\tR32(ebx, bx);\n\tR32(ecx, cx);\n\tR32(edx, dx);\n\tR32(edi, di);\n\tR32(esi, si);\n\tR32(ebp, bp);\n\tR32(eax, ax);\n\tR32(orig_eax, orig_ax);\n\tR32(eip, ip);\n\tR32(esp, sp);\n\n\tcase offsetof(struct user32, regs.eflags):\n\t\t*val = get_flags(child);\n\t\tbreak;\n\n\tcase offsetof(struct user32, u_debugreg[0]) ...\n\t\toffsetof(struct user32, u_debugreg[7]):\n\t\tregno -= offsetof(struct user32, u_debugreg[0]);\n\t\t*val = ptrace_get_debugreg(child, regno / 4);\n\t\tbreak;\n\n\tdefault:\n\t\tif (regno > sizeof(struct user32) || (regno & 3))\n\t\t\treturn -EIO;\n\n\t\t/*\n\t\t * Other dummy fields in the virtual user structure\n\t\t * are ignored\n\t\t */\n\t\t*val = 0;\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\n#undef R32\n#undef SEG32\n\nstatic int genregs32_get(struct task_struct *target,\n\t\t\t const struct user_regset *regset,\n\t\t\t unsigned int pos, unsigned int count,\n\t\t\t void *kbuf, void __user *ubuf)\n{\n\tif (kbuf) {\n\t\tcompat_ulong_t *k = kbuf;\n\t\twhile (count >= sizeof(*k)) {\n\t\t\tgetreg32(target, pos, k++);\n\t\t\tcount -= sizeof(*k);\n\t\t\tpos += sizeof(*k);\n\t\t}\n\t} else {\n\t\tcompat_ulong_t __user *u = ubuf;\n\t\twhile (count >= sizeof(*u)) {\n\t\t\tcompat_ulong_t word;\n\t\t\tgetreg32(target, pos, &word);\n\t\t\tif (__put_user(word, u++))\n\t\t\t\treturn -EFAULT;\n\t\t\tcount -= sizeof(*u);\n\t\t\tpos += sizeof(*u);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int genregs32_set(struct task_struct *target,\n\t\t\t const struct user_regset *regset,\n\t\t\t unsigned int pos, unsigned int count,\n\t\t\t const void *kbuf, const void __user *ubuf)\n{\n\tint ret = 0;\n\tif (kbuf) {\n\t\tconst compat_ulong_t *k = kbuf;\n\t\twhile (count >= sizeof(*k) && !ret) {\n\t\t\tret = putreg32(target, pos, *k++);\n\t\t\tcount -= sizeof(*k);\n\t\t\tpos += sizeof(*k);\n\t\t}\n\t} else {\n\t\tconst compat_ulong_t __user *u = ubuf;\n\t\twhile (count >= sizeof(*u) && !ret) {\n\t\t\tcompat_ulong_t word;\n\t\t\tret = __get_user(word, u++);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tret = putreg32(target, pos, word);\n\t\t\tcount -= sizeof(*u);\n\t\t\tpos += sizeof(*u);\n\t\t}\n\t}\n\treturn ret;\n}\n\nlong compat_arch_ptrace(struct task_struct *child, compat_long_t request,\n\t\t\tcompat_ulong_t caddr, compat_ulong_t cdata)\n{\n\tunsigned long addr = caddr;\n\tunsigned long data = cdata;\n\tvoid __user *datap = compat_ptr(data);\n\tint ret;\n\t__u32 val;\n\n\tswitch (request) {\n\tcase PTRACE_PEEKUSR:\n\t\tret = getreg32(child, addr, &val);\n\t\tif (ret == 0)\n\t\t\tret = put_user(val, (__u32 __user *)datap);\n\t\tbreak;\n\n\tcase PTRACE_POKEUSR:\n\t\tret = putreg32(child, addr, data);\n\t\tbreak;\n\n\tcase PTRACE_GETREGS:\t/* Get all gp regs from the child. */\n\t\treturn copy_regset_to_user(child, &user_x86_32_view,\n\t\t\t\t\t   REGSET_GENERAL,\n\t\t\t\t\t   0, sizeof(struct user_regs_struct32),\n\t\t\t\t\t   datap);\n\n\tcase PTRACE_SETREGS:\t/* Set all gp regs in the child. */\n\t\treturn copy_regset_from_user(child, &user_x86_32_view,\n\t\t\t\t\t     REGSET_GENERAL, 0,\n\t\t\t\t\t     sizeof(struct user_regs_struct32),\n\t\t\t\t\t     datap);\n\n\tcase PTRACE_GETFPREGS:\t/* Get the child FPU state. */\n\t\treturn copy_regset_to_user(child, &user_x86_32_view,\n\t\t\t\t\t   REGSET_FP, 0,\n\t\t\t\t\t   sizeof(struct user_i387_ia32_struct),\n\t\t\t\t\t   datap);\n\n\tcase PTRACE_SETFPREGS:\t/* Set the child FPU state. */\n\t\treturn copy_regset_from_user(\n\t\t\tchild, &user_x86_32_view, REGSET_FP,\n\t\t\t0, sizeof(struct user_i387_ia32_struct), datap);\n\n\tcase PTRACE_GETFPXREGS:\t/* Get the child extended FPU state. */\n\t\treturn copy_regset_to_user(child, &user_x86_32_view,\n\t\t\t\t\t   REGSET_XFP, 0,\n\t\t\t\t\t   sizeof(struct user32_fxsr_struct),\n\t\t\t\t\t   datap);\n\n\tcase PTRACE_SETFPXREGS:\t/* Set the child extended FPU state. */\n\t\treturn copy_regset_from_user(child, &user_x86_32_view,\n\t\t\t\t\t     REGSET_XFP, 0,\n\t\t\t\t\t     sizeof(struct user32_fxsr_struct),\n\t\t\t\t\t     datap);\n\n\tcase PTRACE_GET_THREAD_AREA:\n\tcase PTRACE_SET_THREAD_AREA:\n\t\treturn arch_ptrace(child, request, addr, data);\n\n\tdefault:\n\t\treturn compat_ptrace_request(child, request, addr, data);\n\t}\n\n\treturn ret;\n}\n\n#endif\t/* CONFIG_IA32_EMULATION */\n\n#ifdef CONFIG_X86_64\n\nstatic struct user_regset x86_64_regsets[] __read_mostly = {\n\t[REGSET_GENERAL] = {\n\t\t.core_note_type = NT_PRSTATUS,\n\t\t.n = sizeof(struct user_regs_struct) / sizeof(long),\n\t\t.size = sizeof(long), .align = sizeof(long),\n\t\t.get = genregs_get, .set = genregs_set\n\t},\n\t[REGSET_FP] = {\n\t\t.core_note_type = NT_PRFPREG,\n\t\t.n = sizeof(struct user_i387_struct) / sizeof(long),\n\t\t.size = sizeof(long), .align = sizeof(long),\n\t\t.active = xfpregs_active, .get = xfpregs_get, .set = xfpregs_set\n\t},\n\t[REGSET_XSTATE] = {\n\t\t.core_note_type = NT_X86_XSTATE,\n\t\t.size = sizeof(u64), .align = sizeof(u64),\n\t\t.active = xstateregs_active, .get = xstateregs_get,\n\t\t.set = xstateregs_set\n\t},\n\t[REGSET_IOPERM64] = {\n\t\t.core_note_type = NT_386_IOPERM,\n\t\t.n = IO_BITMAP_LONGS,\n\t\t.size = sizeof(long), .align = sizeof(long),\n\t\t.active = ioperm_active, .get = ioperm_get\n\t},\n};\n\nstatic const struct user_regset_view user_x86_64_view = {\n\t.name = \"x86_64\", .e_machine = EM_X86_64,\n\t.regsets = x86_64_regsets, .n = ARRAY_SIZE(x86_64_regsets)\n};\n\n#else  /* CONFIG_X86_32 */\n\n#define user_regs_struct32\tuser_regs_struct\n#define genregs32_get\t\tgenregs_get\n#define genregs32_set\t\tgenregs_set\n\n#define user_i387_ia32_struct\tuser_i387_struct\n#define user32_fxsr_struct\tuser_fxsr_struct\n\n#endif\t/* CONFIG_X86_64 */\n\n#if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION\nstatic struct user_regset x86_32_regsets[] __read_mostly = {\n\t[REGSET_GENERAL] = {\n\t\t.core_note_type = NT_PRSTATUS,\n\t\t.n = sizeof(struct user_regs_struct32) / sizeof(u32),\n\t\t.size = sizeof(u32), .align = sizeof(u32),\n\t\t.get = genregs32_get, .set = genregs32_set\n\t},\n\t[REGSET_FP] = {\n\t\t.core_note_type = NT_PRFPREG,\n\t\t.n = sizeof(struct user_i387_ia32_struct) / sizeof(u32),\n\t\t.size = sizeof(u32), .align = sizeof(u32),\n\t\t.active = fpregs_active, .get = fpregs_get, .set = fpregs_set\n\t},\n\t[REGSET_XFP] = {\n\t\t.core_note_type = NT_PRXFPREG,\n\t\t.n = sizeof(struct user32_fxsr_struct) / sizeof(u32),\n\t\t.size = sizeof(u32), .align = sizeof(u32),\n\t\t.active = xfpregs_active, .get = xfpregs_get, .set = xfpregs_set\n\t},\n\t[REGSET_XSTATE] = {\n\t\t.core_note_type = NT_X86_XSTATE,\n\t\t.size = sizeof(u64), .align = sizeof(u64),\n\t\t.active = xstateregs_active, .get = xstateregs_get,\n\t\t.set = xstateregs_set\n\t},\n\t[REGSET_TLS] = {\n\t\t.core_note_type = NT_386_TLS,\n\t\t.n = GDT_ENTRY_TLS_ENTRIES, .bias = GDT_ENTRY_TLS_MIN,\n\t\t.size = sizeof(struct user_desc),\n\t\t.align = sizeof(struct user_desc),\n\t\t.active = regset_tls_active,\n\t\t.get = regset_tls_get, .set = regset_tls_set\n\t},\n\t[REGSET_IOPERM32] = {\n\t\t.core_note_type = NT_386_IOPERM,\n\t\t.n = IO_BITMAP_BYTES / sizeof(u32),\n\t\t.size = sizeof(u32), .align = sizeof(u32),\n\t\t.active = ioperm_active, .get = ioperm_get\n\t},\n};\n\nstatic const struct user_regset_view user_x86_32_view = {\n\t.name = \"i386\", .e_machine = EM_386,\n\t.regsets = x86_32_regsets, .n = ARRAY_SIZE(x86_32_regsets)\n};\n#endif\n\n/*\n * This represents bytes 464..511 in the memory layout exported through\n * the REGSET_XSTATE interface.\n */\nu64 xstate_fx_sw_bytes[USER_XSTATE_FX_SW_WORDS];\n\nvoid update_regset_xstate_info(unsigned int size, u64 xstate_mask)\n{\n#ifdef CONFIG_X86_64\n\tx86_64_regsets[REGSET_XSTATE].n = size / sizeof(u64);\n#endif\n#if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION\n\tx86_32_regsets[REGSET_XSTATE].n = size / sizeof(u64);\n#endif\n\txstate_fx_sw_bytes[USER_XSTATE_XCR0_WORD] = xstate_mask;\n}\n\nconst struct user_regset_view *task_user_regset_view(struct task_struct *task)\n{\n#ifdef CONFIG_IA32_EMULATION\n\tif (test_tsk_thread_flag(task, TIF_IA32))\n#endif\n#if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION\n\t\treturn &user_x86_32_view;\n#endif\n#ifdef CONFIG_X86_64\n\treturn &user_x86_64_view;\n#endif\n}\n\nstatic void fill_sigtrap_info(struct task_struct *tsk,\n\t\t\t\tstruct pt_regs *regs,\n\t\t\t\tint error_code, int si_code,\n\t\t\t\tstruct siginfo *info)\n{\n\ttsk->thread.trap_no = 1;\n\ttsk->thread.error_code = error_code;\n\n\tmemset(info, 0, sizeof(*info));\n\tinfo->si_signo = SIGTRAP;\n\tinfo->si_code = si_code;\n\tinfo->si_addr = user_mode_vm(regs) ? (void __user *)regs->ip : NULL;\n}\n\nvoid user_single_step_siginfo(struct task_struct *tsk,\n\t\t\t\tstruct pt_regs *regs,\n\t\t\t\tstruct siginfo *info)\n{\n\tfill_sigtrap_info(tsk, regs, 0, TRAP_BRKPT, info);\n}\n\nvoid send_sigtrap(struct task_struct *tsk, struct pt_regs *regs,\n\t\t\t\t\t int error_code, int si_code)\n{\n\tstruct siginfo info;\n\n\tfill_sigtrap_info(tsk, regs, error_code, si_code, &info);\n\t/* Send us the fake SIGTRAP */\n\tforce_sig_info(SIGTRAP, &info, tsk);\n}\n\n\n#ifdef CONFIG_X86_32\n# define IS_IA32\t1\n#elif defined CONFIG_IA32_EMULATION\n# define IS_IA32\tis_compat_task()\n#else\n# define IS_IA32\t0\n#endif\n\n/*\n * We must return the syscall number to actually look up in the table.\n * This can be -1L to skip running any syscall at all.\n */\nlong syscall_trace_enter(struct pt_regs *regs)\n{\n\tlong ret = 0;\n\n\t/*\n\t * If we stepped into a sysenter/syscall insn, it trapped in\n\t * kernel mode; do_debug() cleared TF and set TIF_SINGLESTEP.\n\t * If user-mode had set TF itself, then it's still clear from\n\t * do_debug() and we need to set it again to restore the user\n\t * state.  If we entered on the slow path, TF was already set.\n\t */\n\tif (test_thread_flag(TIF_SINGLESTEP))\n\t\tregs->flags |= X86_EFLAGS_TF;\n\n\t/* do the secure computing check first */\n\tsecure_computing(regs->orig_ax);\n\n\tif (unlikely(test_thread_flag(TIF_SYSCALL_EMU)))\n\t\tret = -1L;\n\n\tif ((ret || test_thread_flag(TIF_SYSCALL_TRACE)) &&\n\t    tracehook_report_syscall_entry(regs))\n\t\tret = -1L;\n\n\tif (unlikely(test_thread_flag(TIF_SYSCALL_TRACEPOINT)))\n\t\ttrace_sys_enter(regs, regs->orig_ax);\n\n\tif (unlikely(current->audit_context)) {\n\t\tif (IS_IA32)\n\t\t\taudit_syscall_entry(AUDIT_ARCH_I386,\n\t\t\t\t\t    regs->orig_ax,\n\t\t\t\t\t    regs->bx, regs->cx,\n\t\t\t\t\t    regs->dx, regs->si);\n#ifdef CONFIG_X86_64\n\t\telse\n\t\t\taudit_syscall_entry(AUDIT_ARCH_X86_64,\n\t\t\t\t\t    regs->orig_ax,\n\t\t\t\t\t    regs->di, regs->si,\n\t\t\t\t\t    regs->dx, regs->r10);\n#endif\n\t}\n\n\treturn ret ?: regs->orig_ax;\n}\n\nvoid syscall_trace_leave(struct pt_regs *regs)\n{\n\tbool step;\n\n\tif (unlikely(current->audit_context))\n\t\taudit_syscall_exit(AUDITSC_RESULT(regs->ax), regs->ax);\n\n\tif (unlikely(test_thread_flag(TIF_SYSCALL_TRACEPOINT)))\n\t\ttrace_sys_exit(regs, regs->ax);\n\n\t/*\n\t * If TIF_SYSCALL_EMU is set, we only get here because of\n\t * TIF_SINGLESTEP (i.e. this is PTRACE_SYSEMU_SINGLESTEP).\n\t * We already reported this syscall instruction in\n\t * syscall_trace_enter().\n\t */\n\tstep = unlikely(test_thread_flag(TIF_SINGLESTEP)) &&\n\t\t\t!test_thread_flag(TIF_SYSCALL_EMU);\n\tif (step || test_thread_flag(TIF_SYSCALL_TRACE))\n\t\ttracehook_report_syscall_exit(regs, step);\n}\n", "/*\n *  Copyright (C) 1995  Linus Torvalds\n *  Copyright (C) 2001, 2002 Andi Kleen, SuSE Labs.\n *  Copyright (C) 2008-2009, Red Hat Inc., Ingo Molnar\n */\n#include <linux/magic.h>\t\t/* STACK_END_MAGIC\t\t*/\n#include <linux/sched.h>\t\t/* test_thread_flag(), ...\t*/\n#include <linux/kdebug.h>\t\t/* oops_begin/end, ...\t\t*/\n#include <linux/module.h>\t\t/* search_exception_table\t*/\n#include <linux/bootmem.h>\t\t/* max_low_pfn\t\t\t*/\n#include <linux/kprobes.h>\t\t/* __kprobes, ...\t\t*/\n#include <linux/mmiotrace.h>\t\t/* kmmio_handler, ...\t\t*/\n#include <linux/perf_event.h>\t\t/* perf_sw_event\t\t*/\n#include <linux/hugetlb.h>\t\t/* hstate_index_to_shift\t*/\n#include <linux/prefetch.h>\t\t/* prefetchw\t\t\t*/\n\n#include <asm/traps.h>\t\t\t/* dotraplinkage, ...\t\t*/\n#include <asm/pgalloc.h>\t\t/* pgd_*(), ...\t\t\t*/\n#include <asm/kmemcheck.h>\t\t/* kmemcheck_*(), ...\t\t*/\n\n/*\n * Page fault error code bits:\n *\n *   bit 0 ==\t 0: no page found\t1: protection fault\n *   bit 1 ==\t 0: read access\t\t1: write access\n *   bit 2 ==\t 0: kernel-mode access\t1: user-mode access\n *   bit 3 ==\t\t\t\t1: use of reserved bit detected\n *   bit 4 ==\t\t\t\t1: fault was an instruction fetch\n */\nenum x86_pf_error_code {\n\n\tPF_PROT\t\t=\t\t1 << 0,\n\tPF_WRITE\t=\t\t1 << 1,\n\tPF_USER\t\t=\t\t1 << 2,\n\tPF_RSVD\t\t=\t\t1 << 3,\n\tPF_INSTR\t=\t\t1 << 4,\n};\n\n/*\n * Returns 0 if mmiotrace is disabled, or if the fault is not\n * handled by mmiotrace:\n */\nstatic inline int __kprobes\nkmmio_fault(struct pt_regs *regs, unsigned long addr)\n{\n\tif (unlikely(is_kmmio_active()))\n\t\tif (kmmio_handler(regs, addr) == 1)\n\t\t\treturn -1;\n\treturn 0;\n}\n\nstatic inline int __kprobes notify_page_fault(struct pt_regs *regs)\n{\n\tint ret = 0;\n\n\t/* kprobe_running() needs smp_processor_id() */\n\tif (kprobes_built_in() && !user_mode_vm(regs)) {\n\t\tpreempt_disable();\n\t\tif (kprobe_running() && kprobe_fault_handler(regs, 14))\n\t\t\tret = 1;\n\t\tpreempt_enable();\n\t}\n\n\treturn ret;\n}\n\n/*\n * Prefetch quirks:\n *\n * 32-bit mode:\n *\n *   Sometimes AMD Athlon/Opteron CPUs report invalid exceptions on prefetch.\n *   Check that here and ignore it.\n *\n * 64-bit mode:\n *\n *   Sometimes the CPU reports invalid exceptions on prefetch.\n *   Check that here and ignore it.\n *\n * Opcode checker based on code by Richard Brunner.\n */\nstatic inline int\ncheck_prefetch_opcode(struct pt_regs *regs, unsigned char *instr,\n\t\t      unsigned char opcode, int *prefetch)\n{\n\tunsigned char instr_hi = opcode & 0xf0;\n\tunsigned char instr_lo = opcode & 0x0f;\n\n\tswitch (instr_hi) {\n\tcase 0x20:\n\tcase 0x30:\n\t\t/*\n\t\t * Values 0x26,0x2E,0x36,0x3E are valid x86 prefixes.\n\t\t * In X86_64 long mode, the CPU will signal invalid\n\t\t * opcode if some of these prefixes are present so\n\t\t * X86_64 will never get here anyway\n\t\t */\n\t\treturn ((instr_lo & 7) == 0x6);\n#ifdef CONFIG_X86_64\n\tcase 0x40:\n\t\t/*\n\t\t * In AMD64 long mode 0x40..0x4F are valid REX prefixes\n\t\t * Need to figure out under what instruction mode the\n\t\t * instruction was issued. Could check the LDT for lm,\n\t\t * but for now it's good enough to assume that long\n\t\t * mode only uses well known segments or kernel.\n\t\t */\n\t\treturn (!user_mode(regs)) || (regs->cs == __USER_CS);\n#endif\n\tcase 0x60:\n\t\t/* 0x64 thru 0x67 are valid prefixes in all modes. */\n\t\treturn (instr_lo & 0xC) == 0x4;\n\tcase 0xF0:\n\t\t/* 0xF0, 0xF2, 0xF3 are valid prefixes in all modes. */\n\t\treturn !instr_lo || (instr_lo>>1) == 1;\n\tcase 0x00:\n\t\t/* Prefetch instruction is 0x0F0D or 0x0F18 */\n\t\tif (probe_kernel_address(instr, opcode))\n\t\t\treturn 0;\n\n\t\t*prefetch = (instr_lo == 0xF) &&\n\t\t\t(opcode == 0x0D || opcode == 0x18);\n\t\treturn 0;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic int\nis_prefetch(struct pt_regs *regs, unsigned long error_code, unsigned long addr)\n{\n\tunsigned char *max_instr;\n\tunsigned char *instr;\n\tint prefetch = 0;\n\n\t/*\n\t * If it was a exec (instruction fetch) fault on NX page, then\n\t * do not ignore the fault:\n\t */\n\tif (error_code & PF_INSTR)\n\t\treturn 0;\n\n\tinstr = (void *)convert_ip_to_linear(current, regs);\n\tmax_instr = instr + 15;\n\n\tif (user_mode(regs) && instr >= (unsigned char *)TASK_SIZE)\n\t\treturn 0;\n\n\twhile (instr < max_instr) {\n\t\tunsigned char opcode;\n\n\t\tif (probe_kernel_address(instr, opcode))\n\t\t\tbreak;\n\n\t\tinstr++;\n\n\t\tif (!check_prefetch_opcode(regs, instr, opcode, &prefetch))\n\t\t\tbreak;\n\t}\n\treturn prefetch;\n}\n\nstatic void\nforce_sig_info_fault(int si_signo, int si_code, unsigned long address,\n\t\t     struct task_struct *tsk, int fault)\n{\n\tunsigned lsb = 0;\n\tsiginfo_t info;\n\n\tinfo.si_signo\t= si_signo;\n\tinfo.si_errno\t= 0;\n\tinfo.si_code\t= si_code;\n\tinfo.si_addr\t= (void __user *)address;\n\tif (fault & VM_FAULT_HWPOISON_LARGE)\n\t\tlsb = hstate_index_to_shift(VM_FAULT_GET_HINDEX(fault)); \n\tif (fault & VM_FAULT_HWPOISON)\n\t\tlsb = PAGE_SHIFT;\n\tinfo.si_addr_lsb = lsb;\n\n\tforce_sig_info(si_signo, &info, tsk);\n}\n\nDEFINE_SPINLOCK(pgd_lock);\nLIST_HEAD(pgd_list);\n\n#ifdef CONFIG_X86_32\nstatic inline pmd_t *vmalloc_sync_one(pgd_t *pgd, unsigned long address)\n{\n\tunsigned index = pgd_index(address);\n\tpgd_t *pgd_k;\n\tpud_t *pud, *pud_k;\n\tpmd_t *pmd, *pmd_k;\n\n\tpgd += index;\n\tpgd_k = init_mm.pgd + index;\n\n\tif (!pgd_present(*pgd_k))\n\t\treturn NULL;\n\n\t/*\n\t * set_pgd(pgd, *pgd_k); here would be useless on PAE\n\t * and redundant with the set_pmd() on non-PAE. As would\n\t * set_pud.\n\t */\n\tpud = pud_offset(pgd, address);\n\tpud_k = pud_offset(pgd_k, address);\n\tif (!pud_present(*pud_k))\n\t\treturn NULL;\n\n\tpmd = pmd_offset(pud, address);\n\tpmd_k = pmd_offset(pud_k, address);\n\tif (!pmd_present(*pmd_k))\n\t\treturn NULL;\n\n\tif (!pmd_present(*pmd))\n\t\tset_pmd(pmd, *pmd_k);\n\telse\n\t\tBUG_ON(pmd_page(*pmd) != pmd_page(*pmd_k));\n\n\treturn pmd_k;\n}\n\nvoid vmalloc_sync_all(void)\n{\n\tunsigned long address;\n\n\tif (SHARED_KERNEL_PMD)\n\t\treturn;\n\n\tfor (address = VMALLOC_START & PMD_MASK;\n\t     address >= TASK_SIZE && address < FIXADDR_TOP;\n\t     address += PMD_SIZE) {\n\t\tstruct page *page;\n\n\t\tspin_lock(&pgd_lock);\n\t\tlist_for_each_entry(page, &pgd_list, lru) {\n\t\t\tspinlock_t *pgt_lock;\n\t\t\tpmd_t *ret;\n\n\t\t\t/* the pgt_lock only for Xen */\n\t\t\tpgt_lock = &pgd_page_get_mm(page)->page_table_lock;\n\n\t\t\tspin_lock(pgt_lock);\n\t\t\tret = vmalloc_sync_one(page_address(page), address);\n\t\t\tspin_unlock(pgt_lock);\n\n\t\t\tif (!ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&pgd_lock);\n\t}\n}\n\n/*\n * 32-bit:\n *\n *   Handle a fault on the vmalloc or module mapping area\n */\nstatic noinline __kprobes int vmalloc_fault(unsigned long address)\n{\n\tunsigned long pgd_paddr;\n\tpmd_t *pmd_k;\n\tpte_t *pte_k;\n\n\t/* Make sure we are in vmalloc area: */\n\tif (!(address >= VMALLOC_START && address < VMALLOC_END))\n\t\treturn -1;\n\n\tWARN_ON_ONCE(in_nmi());\n\n\t/*\n\t * Synchronize this task's top level page-table\n\t * with the 'reference' page table.\n\t *\n\t * Do _not_ use \"current\" here. We might be inside\n\t * an interrupt in the middle of a task switch..\n\t */\n\tpgd_paddr = read_cr3();\n\tpmd_k = vmalloc_sync_one(__va(pgd_paddr), address);\n\tif (!pmd_k)\n\t\treturn -1;\n\n\tpte_k = pte_offset_kernel(pmd_k, address);\n\tif (!pte_present(*pte_k))\n\t\treturn -1;\n\n\treturn 0;\n}\n\n/*\n * Did it hit the DOS screen memory VA from vm86 mode?\n */\nstatic inline void\ncheck_v8086_mode(struct pt_regs *regs, unsigned long address,\n\t\t struct task_struct *tsk)\n{\n\tunsigned long bit;\n\n\tif (!v8086_mode(regs))\n\t\treturn;\n\n\tbit = (address - 0xA0000) >> PAGE_SHIFT;\n\tif (bit < 32)\n\t\ttsk->thread.screen_bitmap |= 1 << bit;\n}\n\nstatic bool low_pfn(unsigned long pfn)\n{\n\treturn pfn < max_low_pfn;\n}\n\nstatic void dump_pagetable(unsigned long address)\n{\n\tpgd_t *base = __va(read_cr3());\n\tpgd_t *pgd = &base[pgd_index(address)];\n\tpmd_t *pmd;\n\tpte_t *pte;\n\n#ifdef CONFIG_X86_PAE\n\tprintk(\"*pdpt = %016Lx \", pgd_val(*pgd));\n\tif (!low_pfn(pgd_val(*pgd) >> PAGE_SHIFT) || !pgd_present(*pgd))\n\t\tgoto out;\n#endif\n\tpmd = pmd_offset(pud_offset(pgd, address), address);\n\tprintk(KERN_CONT \"*pde = %0*Lx \", sizeof(*pmd) * 2, (u64)pmd_val(*pmd));\n\n\t/*\n\t * We must not directly access the pte in the highpte\n\t * case if the page table is located in highmem.\n\t * And let's rather not kmap-atomic the pte, just in case\n\t * it's allocated already:\n\t */\n\tif (!low_pfn(pmd_pfn(*pmd)) || !pmd_present(*pmd) || pmd_large(*pmd))\n\t\tgoto out;\n\n\tpte = pte_offset_kernel(pmd, address);\n\tprintk(\"*pte = %0*Lx \", sizeof(*pte) * 2, (u64)pte_val(*pte));\nout:\n\tprintk(\"\\n\");\n}\n\n#else /* CONFIG_X86_64: */\n\nvoid vmalloc_sync_all(void)\n{\n\tsync_global_pgds(VMALLOC_START & PGDIR_MASK, VMALLOC_END);\n}\n\n/*\n * 64-bit:\n *\n *   Handle a fault on the vmalloc area\n *\n * This assumes no large pages in there.\n */\nstatic noinline __kprobes int vmalloc_fault(unsigned long address)\n{\n\tpgd_t *pgd, *pgd_ref;\n\tpud_t *pud, *pud_ref;\n\tpmd_t *pmd, *pmd_ref;\n\tpte_t *pte, *pte_ref;\n\n\t/* Make sure we are in vmalloc area: */\n\tif (!(address >= VMALLOC_START && address < VMALLOC_END))\n\t\treturn -1;\n\n\tWARN_ON_ONCE(in_nmi());\n\n\t/*\n\t * Copy kernel mappings over when needed. This can also\n\t * happen within a race in page table update. In the later\n\t * case just flush:\n\t */\n\tpgd = pgd_offset(current->active_mm, address);\n\tpgd_ref = pgd_offset_k(address);\n\tif (pgd_none(*pgd_ref))\n\t\treturn -1;\n\n\tif (pgd_none(*pgd))\n\t\tset_pgd(pgd, *pgd_ref);\n\telse\n\t\tBUG_ON(pgd_page_vaddr(*pgd) != pgd_page_vaddr(*pgd_ref));\n\n\t/*\n\t * Below here mismatches are bugs because these lower tables\n\t * are shared:\n\t */\n\n\tpud = pud_offset(pgd, address);\n\tpud_ref = pud_offset(pgd_ref, address);\n\tif (pud_none(*pud_ref))\n\t\treturn -1;\n\n\tif (pud_none(*pud) || pud_page_vaddr(*pud) != pud_page_vaddr(*pud_ref))\n\t\tBUG();\n\n\tpmd = pmd_offset(pud, address);\n\tpmd_ref = pmd_offset(pud_ref, address);\n\tif (pmd_none(*pmd_ref))\n\t\treturn -1;\n\n\tif (pmd_none(*pmd) || pmd_page(*pmd) != pmd_page(*pmd_ref))\n\t\tBUG();\n\n\tpte_ref = pte_offset_kernel(pmd_ref, address);\n\tif (!pte_present(*pte_ref))\n\t\treturn -1;\n\n\tpte = pte_offset_kernel(pmd, address);\n\n\t/*\n\t * Don't use pte_page here, because the mappings can point\n\t * outside mem_map, and the NUMA hash lookup cannot handle\n\t * that:\n\t */\n\tif (!pte_present(*pte) || pte_pfn(*pte) != pte_pfn(*pte_ref))\n\t\tBUG();\n\n\treturn 0;\n}\n\nstatic const char errata93_warning[] =\nKERN_ERR \n\"******* Your BIOS seems to not contain a fix for K8 errata #93\\n\"\n\"******* Working around it, but it may cause SEGVs or burn power.\\n\"\n\"******* Please consider a BIOS update.\\n\"\n\"******* Disabling USB legacy in the BIOS may also help.\\n\";\n\n/*\n * No vm86 mode in 64-bit mode:\n */\nstatic inline void\ncheck_v8086_mode(struct pt_regs *regs, unsigned long address,\n\t\t struct task_struct *tsk)\n{\n}\n\nstatic int bad_address(void *p)\n{\n\tunsigned long dummy;\n\n\treturn probe_kernel_address((unsigned long *)p, dummy);\n}\n\nstatic void dump_pagetable(unsigned long address)\n{\n\tpgd_t *base = __va(read_cr3() & PHYSICAL_PAGE_MASK);\n\tpgd_t *pgd = base + pgd_index(address);\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\n\tif (bad_address(pgd))\n\t\tgoto bad;\n\n\tprintk(\"PGD %lx \", pgd_val(*pgd));\n\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\n\tpud = pud_offset(pgd, address);\n\tif (bad_address(pud))\n\t\tgoto bad;\n\n\tprintk(\"PUD %lx \", pud_val(*pud));\n\tif (!pud_present(*pud) || pud_large(*pud))\n\t\tgoto out;\n\n\tpmd = pmd_offset(pud, address);\n\tif (bad_address(pmd))\n\t\tgoto bad;\n\n\tprintk(\"PMD %lx \", pmd_val(*pmd));\n\tif (!pmd_present(*pmd) || pmd_large(*pmd))\n\t\tgoto out;\n\n\tpte = pte_offset_kernel(pmd, address);\n\tif (bad_address(pte))\n\t\tgoto bad;\n\n\tprintk(\"PTE %lx\", pte_val(*pte));\nout:\n\tprintk(\"\\n\");\n\treturn;\nbad:\n\tprintk(\"BAD\\n\");\n}\n\n#endif /* CONFIG_X86_64 */\n\n/*\n * Workaround for K8 erratum #93 & buggy BIOS.\n *\n * BIOS SMM functions are required to use a specific workaround\n * to avoid corruption of the 64bit RIP register on C stepping K8.\n *\n * A lot of BIOS that didn't get tested properly miss this.\n *\n * The OS sees this as a page fault with the upper 32bits of RIP cleared.\n * Try to work around it here.\n *\n * Note we only handle faults in kernel here.\n * Does nothing on 32-bit.\n */\nstatic int is_errata93(struct pt_regs *regs, unsigned long address)\n{\n#ifdef CONFIG_X86_64\n\tif (address != regs->ip)\n\t\treturn 0;\n\n\tif ((address >> 32) != 0)\n\t\treturn 0;\n\n\taddress |= 0xffffffffUL << 32;\n\tif ((address >= (u64)_stext && address <= (u64)_etext) ||\n\t    (address >= MODULES_VADDR && address <= MODULES_END)) {\n\t\tprintk_once(errata93_warning);\n\t\tregs->ip = address;\n\t\treturn 1;\n\t}\n#endif\n\treturn 0;\n}\n\n/*\n * Work around K8 erratum #100 K8 in compat mode occasionally jumps\n * to illegal addresses >4GB.\n *\n * We catch this in the page fault handler because these addresses\n * are not reachable. Just detect this case and return.  Any code\n * segment in LDT is compatibility mode.\n */\nstatic int is_errata100(struct pt_regs *regs, unsigned long address)\n{\n#ifdef CONFIG_X86_64\n\tif ((regs->cs == __USER32_CS || (regs->cs & (1<<2))) && (address >> 32))\n\t\treturn 1;\n#endif\n\treturn 0;\n}\n\nstatic int is_f00f_bug(struct pt_regs *regs, unsigned long address)\n{\n#ifdef CONFIG_X86_F00F_BUG\n\tunsigned long nr;\n\n\t/*\n\t * Pentium F0 0F C7 C8 bug workaround:\n\t */\n\tif (boot_cpu_data.f00f_bug) {\n\t\tnr = (address - idt_descr.address) >> 3;\n\n\t\tif (nr == 6) {\n\t\t\tdo_invalid_op(regs, 0);\n\t\t\treturn 1;\n\t\t}\n\t}\n#endif\n\treturn 0;\n}\n\nstatic const char nx_warning[] = KERN_CRIT\n\"kernel tried to execute NX-protected page - exploit attempt? (uid: %d)\\n\";\n\nstatic void\nshow_fault_oops(struct pt_regs *regs, unsigned long error_code,\n\t\tunsigned long address)\n{\n\tif (!oops_may_print())\n\t\treturn;\n\n\tif (error_code & PF_INSTR) {\n\t\tunsigned int level;\n\n\t\tpte_t *pte = lookup_address(address, &level);\n\n\t\tif (pte && pte_present(*pte) && !pte_exec(*pte))\n\t\t\tprintk(nx_warning, current_uid());\n\t}\n\n\tprintk(KERN_ALERT \"BUG: unable to handle kernel \");\n\tif (address < PAGE_SIZE)\n\t\tprintk(KERN_CONT \"NULL pointer dereference\");\n\telse\n\t\tprintk(KERN_CONT \"paging request\");\n\n\tprintk(KERN_CONT \" at %p\\n\", (void *) address);\n\tprintk(KERN_ALERT \"IP:\");\n\tprintk_address(regs->ip, 1);\n\n\tdump_pagetable(address);\n}\n\nstatic noinline void\npgtable_bad(struct pt_regs *regs, unsigned long error_code,\n\t    unsigned long address)\n{\n\tstruct task_struct *tsk;\n\tunsigned long flags;\n\tint sig;\n\n\tflags = oops_begin();\n\ttsk = current;\n\tsig = SIGKILL;\n\n\tprintk(KERN_ALERT \"%s: Corrupted page table at address %lx\\n\",\n\t       tsk->comm, address);\n\tdump_pagetable(address);\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_no\t= 14;\n\ttsk->thread.error_code\t= error_code;\n\n\tif (__die(\"Bad pagetable\", regs, error_code))\n\t\tsig = 0;\n\n\toops_end(flags, regs, sig);\n}\n\nstatic noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long *stackend;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tstackend = end_of_stack(tsk);\n\tif (tsk != &init_task && *stackend != STACK_END_MAGIC)\n\t\tprintk(KERN_ALERT \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_no\t= 14;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_EMERG \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}\n\n/*\n * Print out info about fatal segfaults, if the show_unhandled_signals\n * sysctl is set:\n */\nstatic inline void\nshow_signal_msg(struct pt_regs *regs, unsigned long error_code,\n\t\tunsigned long address, struct task_struct *tsk)\n{\n\tif (!unhandled_signal(tsk, SIGSEGV))\n\t\treturn;\n\n\tif (!printk_ratelimit())\n\t\treturn;\n\n\tprintk(\"%s%s[%d]: segfault at %lx ip %p sp %p error %lx\",\n\t\ttask_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,\n\t\ttsk->comm, task_pid_nr(tsk), address,\n\t\t(void *)regs->ip, (void *)regs->sp, error_code);\n\n\tprint_vma_addr(KERN_CONT \" in \", regs->ip);\n\n\tprintk(KERN_CONT \"\\n\");\n}\n\nstatic void\n__bad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,\n\t\t       unsigned long address, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * It's possible to have interrupts off here:\n\t\t */\n\t\tlocal_irq_enable();\n\n\t\t/*\n\t\t * Valid to do another page fault here because this one came\n\t\t * from user space:\n\t\t */\n\t\tif (is_prefetch(regs, error_code, address))\n\t\t\treturn;\n\n\t\tif (is_errata100(regs, address))\n\t\t\treturn;\n\n\t\tif (unlikely(show_unhandled_signals))\n\t\t\tshow_signal_msg(regs, error_code, address, tsk);\n\n\t\t/* Kernel addresses are always protection faults: */\n\t\ttsk->thread.cr2\t\t= address;\n\t\ttsk->thread.error_code\t= error_code | (address >= TASK_SIZE);\n\t\ttsk->thread.trap_no\t= 14;\n\n\t\tforce_sig_info_fault(SIGSEGV, si_code, address, tsk, 0);\n\n\t\treturn;\n\t}\n\n\tif (is_f00f_bug(regs, address))\n\t\treturn;\n\n\tno_context(regs, error_code, address);\n}\n\nstatic noinline void\nbad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,\n\t\t     unsigned long address)\n{\n\t__bad_area_nosemaphore(regs, error_code, address, SEGV_MAPERR);\n}\n\nstatic void\n__bad_area(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int si_code)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\t/*\n\t * Something tried to access memory that isn't in our memory map..\n\t * Fix it, but check if it's kernel or user first..\n\t */\n\tup_read(&mm->mmap_sem);\n\n\t__bad_area_nosemaphore(regs, error_code, address, si_code);\n}\n\nstatic noinline void\nbad_area(struct pt_regs *regs, unsigned long error_code, unsigned long address)\n{\n\t__bad_area(regs, error_code, address, SEGV_MAPERR);\n}\n\nstatic noinline void\nbad_area_access_error(struct pt_regs *regs, unsigned long error_code,\n\t\t      unsigned long address)\n{\n\t__bad_area(regs, error_code, address, SEGV_ACCERR);\n}\n\n/* TODO: fixup for \"mm-invoke-oom-killer-from-page-fault.patch\" */\nstatic void\nout_of_memory(struct pt_regs *regs, unsigned long error_code,\n\t      unsigned long address)\n{\n\t/*\n\t * We ran out of memory, call the OOM killer, and return the userspace\n\t * (which will retry the fault, or kill us if we got oom-killed):\n\t */\n\tup_read(&current->mm->mmap_sem);\n\n\tpagefault_out_of_memory();\n}\n\nstatic void\ndo_sigbus(struct pt_regs *regs, unsigned long error_code, unsigned long address,\n\t  unsigned int fault)\n{\n\tstruct task_struct *tsk = current;\n\tstruct mm_struct *mm = tsk->mm;\n\tint code = BUS_ADRERR;\n\n\tup_read(&mm->mmap_sem);\n\n\t/* Kernel mode? Handle exceptions or die: */\n\tif (!(error_code & PF_USER)) {\n\t\tno_context(regs, error_code, address);\n\t\treturn;\n\t}\n\n\t/* User-space => ok to do another page fault: */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.error_code\t= error_code;\n\ttsk->thread.trap_no\t= 14;\n\n#ifdef CONFIG_MEMORY_FAILURE\n\tif (fault & (VM_FAULT_HWPOISON|VM_FAULT_HWPOISON_LARGE)) {\n\t\tprintk(KERN_ERR\n\t\"MCE: Killing %s:%d due to hardware memory corruption fault at %lx\\n\",\n\t\t\ttsk->comm, tsk->pid, address);\n\t\tcode = BUS_MCEERR_AR;\n\t}\n#endif\n\tforce_sig_info_fault(SIGBUS, code, address, tsk, fault);\n}\n\nstatic noinline int\nmm_fault_error(struct pt_regs *regs, unsigned long error_code,\n\t       unsigned long address, unsigned int fault)\n{\n\t/*\n\t * Pagefault was interrupted by SIGKILL. We have no reason to\n\t * continue pagefault.\n\t */\n\tif (fatal_signal_pending(current)) {\n\t\tif (!(fault & VM_FAULT_RETRY))\n\t\t\tup_read(&current->mm->mmap_sem);\n\t\tif (!(error_code & PF_USER))\n\t\t\tno_context(regs, error_code, address);\n\t\treturn 1;\n\t}\n\tif (!(fault & VM_FAULT_ERROR))\n\t\treturn 0;\n\n\tif (fault & VM_FAULT_OOM) {\n\t\t/* Kernel mode? Handle exceptions or die: */\n\t\tif (!(error_code & PF_USER)) {\n\t\t\tup_read(&current->mm->mmap_sem);\n\t\t\tno_context(regs, error_code, address);\n\t\t\treturn 1;\n\t\t}\n\n\t\tout_of_memory(regs, error_code, address);\n\t} else {\n\t\tif (fault & (VM_FAULT_SIGBUS|VM_FAULT_HWPOISON|\n\t\t\t     VM_FAULT_HWPOISON_LARGE))\n\t\t\tdo_sigbus(regs, error_code, address, fault);\n\t\telse\n\t\t\tBUG();\n\t}\n\treturn 1;\n}\n\nstatic int spurious_fault_check(unsigned long error_code, pte_t *pte)\n{\n\tif ((error_code & PF_WRITE) && !pte_write(*pte))\n\t\treturn 0;\n\n\tif ((error_code & PF_INSTR) && !pte_exec(*pte))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n/*\n * Handle a spurious fault caused by a stale TLB entry.\n *\n * This allows us to lazily refresh the TLB when increasing the\n * permissions of a kernel page (RO -> RW or NX -> X).  Doing it\n * eagerly is very expensive since that implies doing a full\n * cross-processor TLB flush, even if no stale TLB entries exist\n * on other processors.\n *\n * There are no security implications to leaving a stale TLB when\n * increasing the permissions on a page.\n */\nstatic noinline __kprobes int\nspurious_fault(unsigned long error_code, unsigned long address)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tint ret;\n\n\t/* Reserved-bit violation or user access to kernel space? */\n\tif (error_code & (PF_USER | PF_RSVD))\n\t\treturn 0;\n\n\tpgd = init_mm.pgd + pgd_index(address);\n\tif (!pgd_present(*pgd))\n\t\treturn 0;\n\n\tpud = pud_offset(pgd, address);\n\tif (!pud_present(*pud))\n\t\treturn 0;\n\n\tif (pud_large(*pud))\n\t\treturn spurious_fault_check(error_code, (pte_t *) pud);\n\n\tpmd = pmd_offset(pud, address);\n\tif (!pmd_present(*pmd))\n\t\treturn 0;\n\n\tif (pmd_large(*pmd))\n\t\treturn spurious_fault_check(error_code, (pte_t *) pmd);\n\n\t/*\n\t * Note: don't use pte_present() here, since it returns true\n\t * if the _PAGE_PROTNONE bit is set.  However, this aliases the\n\t * _PAGE_GLOBAL bit, which for kernel pages give false positives\n\t * when CONFIG_DEBUG_PAGEALLOC is used.\n\t */\n\tpte = pte_offset_kernel(pmd, address);\n\tif (!(pte_flags(*pte) & _PAGE_PRESENT))\n\t\treturn 0;\n\n\tret = spurious_fault_check(error_code, pte);\n\tif (!ret)\n\t\treturn 0;\n\n\t/*\n\t * Make sure we have permissions in PMD.\n\t * If not, then there's a bug in the page tables:\n\t */\n\tret = spurious_fault_check(error_code, (pte_t *) pmd);\n\tWARN_ONCE(!ret, \"PMD has incorrect permission bits\\n\");\n\n\treturn ret;\n}\n\nint show_unhandled_signals = 1;\n\nstatic inline int\naccess_error(unsigned long error_code, struct vm_area_struct *vma)\n{\n\tif (error_code & PF_WRITE) {\n\t\t/* write, present and write, not present: */\n\t\tif (unlikely(!(vma->vm_flags & VM_WRITE)))\n\t\t\treturn 1;\n\t\treturn 0;\n\t}\n\n\t/* read, present: */\n\tif (unlikely(error_code & PF_PROT))\n\t\treturn 1;\n\n\t/* read, not present: */\n\tif (unlikely(!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE))))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int fault_in_kernel_space(unsigned long address)\n{\n\treturn address >= TASK_SIZE_MAX;\n}\n\n/*\n * This routine handles page faults.  It determines the address,\n * and the problem, and then passes it off to one of the appropriate\n * routines.\n */\ndotraplinkage void __kprobes\ndo_page_fault(struct pt_regs *regs, unsigned long error_code)\n{\n\tstruct vm_area_struct *vma;\n\tstruct task_struct *tsk;\n\tunsigned long address;\n\tstruct mm_struct *mm;\n\tint fault;\n\tint write = error_code & PF_WRITE;\n\tunsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE |\n\t\t\t\t\t(write ? FAULT_FLAG_WRITE : 0);\n\n\ttsk = current;\n\tmm = tsk->mm;\n\n\t/* Get the faulting address: */\n\taddress = read_cr2();\n\n\t/*\n\t * Detect and handle instructions that would cause a page fault for\n\t * both a tracked kernel page and a userspace page.\n\t */\n\tif (kmemcheck_active(regs))\n\t\tkmemcheck_hide(regs);\n\tprefetchw(&mm->mmap_sem);\n\n\tif (unlikely(kmmio_fault(regs, address)))\n\t\treturn;\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t *\n\t * This verifies that the fault happens in kernel space\n\t * (error_code & 4) == 0, and that the fault was not a\n\t * protection error (error_code & 9) == 0.\n\t */\n\tif (unlikely(fault_in_kernel_space(address))) {\n\t\tif (!(error_code & (PF_RSVD | PF_USER | PF_PROT))) {\n\t\t\tif (vmalloc_fault(address) >= 0)\n\t\t\t\treturn;\n\n\t\t\tif (kmemcheck_fault(regs, address, error_code))\n\t\t\t\treturn;\n\t\t}\n\n\t\t/* Can handle a stale RO->RW TLB: */\n\t\tif (spurious_fault(error_code, address))\n\t\t\treturn;\n\n\t\t/* kprobes don't want to hook the spurious faults: */\n\t\tif (notify_page_fault(regs))\n\t\t\treturn;\n\t\t/*\n\t\t * Don't take the mm semaphore here. If we fixup a prefetch\n\t\t * fault we could otherwise deadlock:\n\t\t */\n\t\tbad_area_nosemaphore(regs, error_code, address);\n\n\t\treturn;\n\t}\n\n\t/* kprobes don't want to hook the spurious faults: */\n\tif (unlikely(notify_page_fault(regs)))\n\t\treturn;\n\t/*\n\t * It's safe to allow irq's after cr2 has been saved and the\n\t * vmalloc fault has been handled.\n\t *\n\t * User-mode registers count as a user access even for any\n\t * potential system fault or CPU buglet:\n\t */\n\tif (user_mode_vm(regs)) {\n\t\tlocal_irq_enable();\n\t\terror_code |= PF_USER;\n\t} else {\n\t\tif (regs->flags & X86_EFLAGS_IF)\n\t\t\tlocal_irq_enable();\n\t}\n\n\tif (unlikely(error_code & PF_RSVD))\n\t\tpgtable_bad(regs, error_code, address);\n\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);\n\n\t/*\n\t * If we're in an interrupt, have no user context or are running\n\t * in an atomic region then we must not take the fault:\n\t */\n\tif (unlikely(in_atomic() || !mm)) {\n\t\tbad_area_nosemaphore(regs, error_code, address);\n\t\treturn;\n\t}\n\n\t/*\n\t * When running in the kernel we expect faults to occur only to\n\t * addresses in user space.  All other faults represent errors in\n\t * the kernel and should generate an OOPS.  Unfortunately, in the\n\t * case of an erroneous fault occurring in a code path which already\n\t * holds mmap_sem we will deadlock attempting to validate the fault\n\t * against the address space.  Luckily the kernel only validly\n\t * references user space from well defined areas of code, which are\n\t * listed in the exceptions table.\n\t *\n\t * As the vast majority of faults will be valid we will only perform\n\t * the source reference check when there is a possibility of a\n\t * deadlock. Attempt to lock the address space, if we cannot we then\n\t * validate the source. If this is invalid we can skip the address\n\t * space check, thus avoiding the deadlock:\n\t */\n\tif (unlikely(!down_read_trylock(&mm->mmap_sem))) {\n\t\tif ((error_code & PF_USER) == 0 &&\n\t\t    !search_exception_tables(regs->ip)) {\n\t\t\tbad_area_nosemaphore(regs, error_code, address);\n\t\t\treturn;\n\t\t}\nretry:\n\t\tdown_read(&mm->mmap_sem);\n\t} else {\n\t\t/*\n\t\t * The above down_read_trylock() might have succeeded in\n\t\t * which case we'll have missed the might_sleep() from\n\t\t * down_read():\n\t\t */\n\t\tmight_sleep();\n\t}\n\n\tvma = find_vma(mm, address);\n\tif (unlikely(!vma)) {\n\t\tbad_area(regs, error_code, address);\n\t\treturn;\n\t}\n\tif (likely(vma->vm_start <= address))\n\t\tgoto good_area;\n\tif (unlikely(!(vma->vm_flags & VM_GROWSDOWN))) {\n\t\tbad_area(regs, error_code, address);\n\t\treturn;\n\t}\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * Accessing the stack below %sp is always a bug.\n\t\t * The large cushion allows instructions like enter\n\t\t * and pusha to work. (\"enter $65535, $31\" pushes\n\t\t * 32 pointers and then decrements %sp by 65535.)\n\t\t */\n\t\tif (unlikely(address + 65536 + 32 * sizeof(unsigned long) < regs->sp)) {\n\t\t\tbad_area(regs, error_code, address);\n\t\t\treturn;\n\t\t}\n\t}\n\tif (unlikely(expand_stack(vma, address))) {\n\t\tbad_area(regs, error_code, address);\n\t\treturn;\n\t}\n\n\t/*\n\t * Ok, we have a good vm_area for this memory access, so\n\t * we can handle it..\n\t */\ngood_area:\n\tif (unlikely(access_error(error_code, vma))) {\n\t\tbad_area_access_error(regs, error_code, address);\n\t\treturn;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault:\n\t */\n\tfault = handle_mm_fault(mm, vma, address, flags);\n\n\tif (unlikely(fault & (VM_FAULT_RETRY|VM_FAULT_ERROR))) {\n\t\tif (mm_fault_error(regs, error_code, address, fault))\n\t\t\treturn;\n\t}\n\n\t/*\n\t * Major/minor page fault accounting is only done on the\n\t * initial attempt. If we go through a retry, it is extremely\n\t * likely that the page will be found in page cache at that point.\n\t */\n\tif (flags & FAULT_FLAG_ALLOW_RETRY) {\n\t\tif (fault & VM_FAULT_MAJOR) {\n\t\t\ttsk->maj_flt++;\n\t\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1,\n\t\t\t\t      regs, address);\n\t\t} else {\n\t\t\ttsk->min_flt++;\n\t\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1,\n\t\t\t\t      regs, address);\n\t\t}\n\t\tif (fault & VM_FAULT_RETRY) {\n\t\t\t/* Clear FAULT_FLAG_ALLOW_RETRY to avoid any risk\n\t\t\t * of starvation. */\n\t\t\tflags &= ~FAULT_FLAG_ALLOW_RETRY;\n\t\t\tgoto retry;\n\t\t}\n\t}\n\n\tcheck_v8086_mode(regs, address, tsk);\n\n\tup_read(&mm->mmap_sem);\n}\n", "/*\n * Performance events:\n *\n *    Copyright (C) 2008-2009, Thomas Gleixner <tglx@linutronix.de>\n *    Copyright (C) 2008-2011, Red Hat, Inc., Ingo Molnar\n *    Copyright (C) 2008-2011, Red Hat, Inc., Peter Zijlstra\n *\n * Data type definitions, declarations, prototypes.\n *\n *    Started by: Thomas Gleixner and Ingo Molnar\n *\n * For licencing details see kernel-base/COPYING\n */\n#ifndef _LINUX_PERF_EVENT_H\n#define _LINUX_PERF_EVENT_H\n\n#include <linux/types.h>\n#include <linux/ioctl.h>\n#include <asm/byteorder.h>\n\n/*\n * User-space ABI bits:\n */\n\n/*\n * attr.type\n */\nenum perf_type_id {\n\tPERF_TYPE_HARDWARE\t\t\t= 0,\n\tPERF_TYPE_SOFTWARE\t\t\t= 1,\n\tPERF_TYPE_TRACEPOINT\t\t\t= 2,\n\tPERF_TYPE_HW_CACHE\t\t\t= 3,\n\tPERF_TYPE_RAW\t\t\t\t= 4,\n\tPERF_TYPE_BREAKPOINT\t\t\t= 5,\n\n\tPERF_TYPE_MAX,\t\t\t\t/* non-ABI */\n};\n\n/*\n * Generalized performance event event_id types, used by the\n * attr.event_id parameter of the sys_perf_event_open()\n * syscall:\n */\nenum perf_hw_id {\n\t/*\n\t * Common hardware events, generalized by the kernel:\n\t */\n\tPERF_COUNT_HW_CPU_CYCLES\t\t= 0,\n\tPERF_COUNT_HW_INSTRUCTIONS\t\t= 1,\n\tPERF_COUNT_HW_CACHE_REFERENCES\t\t= 2,\n\tPERF_COUNT_HW_CACHE_MISSES\t\t= 3,\n\tPERF_COUNT_HW_BRANCH_INSTRUCTIONS\t= 4,\n\tPERF_COUNT_HW_BRANCH_MISSES\t\t= 5,\n\tPERF_COUNT_HW_BUS_CYCLES\t\t= 6,\n\tPERF_COUNT_HW_STALLED_CYCLES_FRONTEND\t= 7,\n\tPERF_COUNT_HW_STALLED_CYCLES_BACKEND\t= 8,\n\n\tPERF_COUNT_HW_MAX,\t\t\t/* non-ABI */\n};\n\n/*\n * Generalized hardware cache events:\n *\n *       { L1-D, L1-I, LLC, ITLB, DTLB, BPU } x\n *       { read, write, prefetch } x\n *       { accesses, misses }\n */\nenum perf_hw_cache_id {\n\tPERF_COUNT_HW_CACHE_L1D\t\t\t= 0,\n\tPERF_COUNT_HW_CACHE_L1I\t\t\t= 1,\n\tPERF_COUNT_HW_CACHE_LL\t\t\t= 2,\n\tPERF_COUNT_HW_CACHE_DTLB\t\t= 3,\n\tPERF_COUNT_HW_CACHE_ITLB\t\t= 4,\n\tPERF_COUNT_HW_CACHE_BPU\t\t\t= 5,\n\n\tPERF_COUNT_HW_CACHE_MAX,\t\t/* non-ABI */\n};\n\nenum perf_hw_cache_op_id {\n\tPERF_COUNT_HW_CACHE_OP_READ\t\t= 0,\n\tPERF_COUNT_HW_CACHE_OP_WRITE\t\t= 1,\n\tPERF_COUNT_HW_CACHE_OP_PREFETCH\t\t= 2,\n\n\tPERF_COUNT_HW_CACHE_OP_MAX,\t\t/* non-ABI */\n};\n\nenum perf_hw_cache_op_result_id {\n\tPERF_COUNT_HW_CACHE_RESULT_ACCESS\t= 0,\n\tPERF_COUNT_HW_CACHE_RESULT_MISS\t\t= 1,\n\n\tPERF_COUNT_HW_CACHE_RESULT_MAX,\t\t/* non-ABI */\n};\n\n/*\n * Special \"software\" events provided by the kernel, even if the hardware\n * does not support performance events. These events measure various\n * physical and sw events of the kernel (and allow the profiling of them as\n * well):\n */\nenum perf_sw_ids {\n\tPERF_COUNT_SW_CPU_CLOCK\t\t\t= 0,\n\tPERF_COUNT_SW_TASK_CLOCK\t\t= 1,\n\tPERF_COUNT_SW_PAGE_FAULTS\t\t= 2,\n\tPERF_COUNT_SW_CONTEXT_SWITCHES\t\t= 3,\n\tPERF_COUNT_SW_CPU_MIGRATIONS\t\t= 4,\n\tPERF_COUNT_SW_PAGE_FAULTS_MIN\t\t= 5,\n\tPERF_COUNT_SW_PAGE_FAULTS_MAJ\t\t= 6,\n\tPERF_COUNT_SW_ALIGNMENT_FAULTS\t\t= 7,\n\tPERF_COUNT_SW_EMULATION_FAULTS\t\t= 8,\n\n\tPERF_COUNT_SW_MAX,\t\t\t/* non-ABI */\n};\n\n/*\n * Bits that can be set in attr.sample_type to request information\n * in the overflow packets.\n */\nenum perf_event_sample_format {\n\tPERF_SAMPLE_IP\t\t\t\t= 1U << 0,\n\tPERF_SAMPLE_TID\t\t\t\t= 1U << 1,\n\tPERF_SAMPLE_TIME\t\t\t= 1U << 2,\n\tPERF_SAMPLE_ADDR\t\t\t= 1U << 3,\n\tPERF_SAMPLE_READ\t\t\t= 1U << 4,\n\tPERF_SAMPLE_CALLCHAIN\t\t\t= 1U << 5,\n\tPERF_SAMPLE_ID\t\t\t\t= 1U << 6,\n\tPERF_SAMPLE_CPU\t\t\t\t= 1U << 7,\n\tPERF_SAMPLE_PERIOD\t\t\t= 1U << 8,\n\tPERF_SAMPLE_STREAM_ID\t\t\t= 1U << 9,\n\tPERF_SAMPLE_RAW\t\t\t\t= 1U << 10,\n\n\tPERF_SAMPLE_MAX = 1U << 11,\t\t/* non-ABI */\n};\n\n/*\n * The format of the data returned by read() on a perf event fd,\n * as specified by attr.read_format:\n *\n * struct read_format {\n *\t{ u64\t\tvalue;\n *\t  { u64\t\ttime_enabled; } && PERF_FORMAT_TOTAL_TIME_ENABLED\n *\t  { u64\t\ttime_running; } && PERF_FORMAT_TOTAL_TIME_RUNNING\n *\t  { u64\t\tid;           } && PERF_FORMAT_ID\n *\t} && !PERF_FORMAT_GROUP\n *\n *\t{ u64\t\tnr;\n *\t  { u64\t\ttime_enabled; } && PERF_FORMAT_TOTAL_TIME_ENABLED\n *\t  { u64\t\ttime_running; } && PERF_FORMAT_TOTAL_TIME_RUNNING\n *\t  { u64\t\tvalue;\n *\t    { u64\tid;           } && PERF_FORMAT_ID\n *\t  }\t\tcntr[nr];\n *\t} && PERF_FORMAT_GROUP\n * };\n */\nenum perf_event_read_format {\n\tPERF_FORMAT_TOTAL_TIME_ENABLED\t\t= 1U << 0,\n\tPERF_FORMAT_TOTAL_TIME_RUNNING\t\t= 1U << 1,\n\tPERF_FORMAT_ID\t\t\t\t= 1U << 2,\n\tPERF_FORMAT_GROUP\t\t\t= 1U << 3,\n\n\tPERF_FORMAT_MAX = 1U << 4,\t\t/* non-ABI */\n};\n\n#define PERF_ATTR_SIZE_VER0\t64\t/* sizeof first published struct */\n\n/*\n * Hardware event_id to monitor via a performance monitoring event:\n */\nstruct perf_event_attr {\n\n\t/*\n\t * Major type: hardware/software/tracepoint/etc.\n\t */\n\t__u32\t\t\ttype;\n\n\t/*\n\t * Size of the attr structure, for fwd/bwd compat.\n\t */\n\t__u32\t\t\tsize;\n\n\t/*\n\t * Type specific configuration information.\n\t */\n\t__u64\t\t\tconfig;\n\n\tunion {\n\t\t__u64\t\tsample_period;\n\t\t__u64\t\tsample_freq;\n\t};\n\n\t__u64\t\t\tsample_type;\n\t__u64\t\t\tread_format;\n\n\t__u64\t\t\tdisabled       :  1, /* off by default        */\n\t\t\t\tinherit\t       :  1, /* children inherit it   */\n\t\t\t\tpinned\t       :  1, /* must always be on PMU */\n\t\t\t\texclusive      :  1, /* only group on PMU     */\n\t\t\t\texclude_user   :  1, /* don't count user      */\n\t\t\t\texclude_kernel :  1, /* ditto kernel          */\n\t\t\t\texclude_hv     :  1, /* ditto hypervisor      */\n\t\t\t\texclude_idle   :  1, /* don't count when idle */\n\t\t\t\tmmap           :  1, /* include mmap data     */\n\t\t\t\tcomm\t       :  1, /* include comm data     */\n\t\t\t\tfreq           :  1, /* use freq, not period  */\n\t\t\t\tinherit_stat   :  1, /* per task counts       */\n\t\t\t\tenable_on_exec :  1, /* next exec enables     */\n\t\t\t\ttask           :  1, /* trace fork/exit       */\n\t\t\t\twatermark      :  1, /* wakeup_watermark      */\n\t\t\t\t/*\n\t\t\t\t * precise_ip:\n\t\t\t\t *\n\t\t\t\t *  0 - SAMPLE_IP can have arbitrary skid\n\t\t\t\t *  1 - SAMPLE_IP must have constant skid\n\t\t\t\t *  2 - SAMPLE_IP requested to have 0 skid\n\t\t\t\t *  3 - SAMPLE_IP must have 0 skid\n\t\t\t\t *\n\t\t\t\t *  See also PERF_RECORD_MISC_EXACT_IP\n\t\t\t\t */\n\t\t\t\tprecise_ip     :  2, /* skid constraint       */\n\t\t\t\tmmap_data      :  1, /* non-exec mmap data    */\n\t\t\t\tsample_id_all  :  1, /* sample_type all events */\n\n\t\t\t\t__reserved_1   : 45;\n\n\tunion {\n\t\t__u32\t\twakeup_events;\t  /* wakeup every n events */\n\t\t__u32\t\twakeup_watermark; /* bytes before wakeup   */\n\t};\n\n\t__u32\t\t\tbp_type;\n\tunion {\n\t\t__u64\t\tbp_addr;\n\t\t__u64\t\tconfig1; /* extension of config */\n\t};\n\tunion {\n\t\t__u64\t\tbp_len;\n\t\t__u64\t\tconfig2; /* extension of config1 */\n\t};\n};\n\n/*\n * Ioctls that can be done on a perf event fd:\n */\n#define PERF_EVENT_IOC_ENABLE\t\t_IO ('$', 0)\n#define PERF_EVENT_IOC_DISABLE\t\t_IO ('$', 1)\n#define PERF_EVENT_IOC_REFRESH\t\t_IO ('$', 2)\n#define PERF_EVENT_IOC_RESET\t\t_IO ('$', 3)\n#define PERF_EVENT_IOC_PERIOD\t\t_IOW('$', 4, __u64)\n#define PERF_EVENT_IOC_SET_OUTPUT\t_IO ('$', 5)\n#define PERF_EVENT_IOC_SET_FILTER\t_IOW('$', 6, char *)\n\nenum perf_event_ioc_flags {\n\tPERF_IOC_FLAG_GROUP\t\t= 1U << 0,\n};\n\n/*\n * Structure of the page that can be mapped via mmap\n */\nstruct perf_event_mmap_page {\n\t__u32\tversion;\t\t/* version number of this structure */\n\t__u32\tcompat_version;\t\t/* lowest version this is compat with */\n\n\t/*\n\t * Bits needed to read the hw events in user-space.\n\t *\n\t *   u32 seq;\n\t *   s64 count;\n\t *\n\t *   do {\n\t *     seq = pc->lock;\n\t *\n\t *     barrier()\n\t *     if (pc->index) {\n\t *       count = pmc_read(pc->index - 1);\n\t *       count += pc->offset;\n\t *     } else\n\t *       goto regular_read;\n\t *\n\t *     barrier();\n\t *   } while (pc->lock != seq);\n\t *\n\t * NOTE: for obvious reason this only works on self-monitoring\n\t *       processes.\n\t */\n\t__u32\tlock;\t\t\t/* seqlock for synchronization */\n\t__u32\tindex;\t\t\t/* hardware event identifier */\n\t__s64\toffset;\t\t\t/* add to hardware event value */\n\t__u64\ttime_enabled;\t\t/* time event active */\n\t__u64\ttime_running;\t\t/* time event on cpu */\n\n\t\t/*\n\t\t * Hole for extension of the self monitor capabilities\n\t\t */\n\n\t__u64\t__reserved[123];\t/* align to 1k */\n\n\t/*\n\t * Control data for the mmap() data buffer.\n\t *\n\t * User-space reading the @data_head value should issue an rmb(), on\n\t * SMP capable platforms, after reading this value -- see\n\t * perf_event_wakeup().\n\t *\n\t * When the mapping is PROT_WRITE the @data_tail value should be\n\t * written by userspace to reflect the last read data. In this case\n\t * the kernel will not over-write unread data.\n\t */\n\t__u64   data_head;\t\t/* head in the data section */\n\t__u64\tdata_tail;\t\t/* user-space written tail */\n};\n\n#define PERF_RECORD_MISC_CPUMODE_MASK\t\t(7 << 0)\n#define PERF_RECORD_MISC_CPUMODE_UNKNOWN\t(0 << 0)\n#define PERF_RECORD_MISC_KERNEL\t\t\t(1 << 0)\n#define PERF_RECORD_MISC_USER\t\t\t(2 << 0)\n#define PERF_RECORD_MISC_HYPERVISOR\t\t(3 << 0)\n#define PERF_RECORD_MISC_GUEST_KERNEL\t\t(4 << 0)\n#define PERF_RECORD_MISC_GUEST_USER\t\t(5 << 0)\n\n/*\n * Indicates that the content of PERF_SAMPLE_IP points to\n * the actual instruction that triggered the event. See also\n * perf_event_attr::precise_ip.\n */\n#define PERF_RECORD_MISC_EXACT_IP\t\t(1 << 14)\n/*\n * Reserve the last bit to indicate some extended misc field\n */\n#define PERF_RECORD_MISC_EXT_RESERVED\t\t(1 << 15)\n\nstruct perf_event_header {\n\t__u32\ttype;\n\t__u16\tmisc;\n\t__u16\tsize;\n};\n\nenum perf_event_type {\n\n\t/*\n\t * If perf_event_attr.sample_id_all is set then all event types will\n\t * have the sample_type selected fields related to where/when\n\t * (identity) an event took place (TID, TIME, ID, CPU, STREAM_ID)\n\t * described in PERF_RECORD_SAMPLE below, it will be stashed just after\n\t * the perf_event_header and the fields already present for the existing\n\t * fields, i.e. at the end of the payload. That way a newer perf.data\n\t * file will be supported by older perf tools, with these new optional\n\t * fields being ignored.\n\t *\n\t * The MMAP events record the PROT_EXEC mappings so that we can\n\t * correlate userspace IPs to code. They have the following structure:\n\t *\n\t * struct {\n\t *\tstruct perf_event_header\theader;\n\t *\n\t *\tu32\t\t\t\tpid, tid;\n\t *\tu64\t\t\t\taddr;\n\t *\tu64\t\t\t\tlen;\n\t *\tu64\t\t\t\tpgoff;\n\t *\tchar\t\t\t\tfilename[];\n\t * };\n\t */\n\tPERF_RECORD_MMAP\t\t\t= 1,\n\n\t/*\n\t * struct {\n\t *\tstruct perf_event_header\theader;\n\t *\tu64\t\t\t\tid;\n\t *\tu64\t\t\t\tlost;\n\t * };\n\t */\n\tPERF_RECORD_LOST\t\t\t= 2,\n\n\t/*\n\t * struct {\n\t *\tstruct perf_event_header\theader;\n\t *\n\t *\tu32\t\t\t\tpid, tid;\n\t *\tchar\t\t\t\tcomm[];\n\t * };\n\t */\n\tPERF_RECORD_COMM\t\t\t= 3,\n\n\t/*\n\t * struct {\n\t *\tstruct perf_event_header\theader;\n\t *\tu32\t\t\t\tpid, ppid;\n\t *\tu32\t\t\t\ttid, ptid;\n\t *\tu64\t\t\t\ttime;\n\t * };\n\t */\n\tPERF_RECORD_EXIT\t\t\t= 4,\n\n\t/*\n\t * struct {\n\t *\tstruct perf_event_header\theader;\n\t *\tu64\t\t\t\ttime;\n\t *\tu64\t\t\t\tid;\n\t *\tu64\t\t\t\tstream_id;\n\t * };\n\t */\n\tPERF_RECORD_THROTTLE\t\t\t= 5,\n\tPERF_RECORD_UNTHROTTLE\t\t\t= 6,\n\n\t/*\n\t * struct {\n\t *\tstruct perf_event_header\theader;\n\t *\tu32\t\t\t\tpid, ppid;\n\t *\tu32\t\t\t\ttid, ptid;\n\t *\tu64\t\t\t\ttime;\n\t * };\n\t */\n\tPERF_RECORD_FORK\t\t\t= 7,\n\n\t/*\n\t * struct {\n\t *\tstruct perf_event_header\theader;\n\t *\tu32\t\t\t\tpid, tid;\n\t *\n\t *\tstruct read_format\t\tvalues;\n\t * };\n\t */\n\tPERF_RECORD_READ\t\t\t= 8,\n\n\t/*\n\t * struct {\n\t *\tstruct perf_event_header\theader;\n\t *\n\t *\t{ u64\t\t\tip;\t  } && PERF_SAMPLE_IP\n\t *\t{ u32\t\t\tpid, tid; } && PERF_SAMPLE_TID\n\t *\t{ u64\t\t\ttime;     } && PERF_SAMPLE_TIME\n\t *\t{ u64\t\t\taddr;     } && PERF_SAMPLE_ADDR\n\t *\t{ u64\t\t\tid;\t  } && PERF_SAMPLE_ID\n\t *\t{ u64\t\t\tstream_id;} && PERF_SAMPLE_STREAM_ID\n\t *\t{ u32\t\t\tcpu, res; } && PERF_SAMPLE_CPU\n\t *\t{ u64\t\t\tperiod;   } && PERF_SAMPLE_PERIOD\n\t *\n\t *\t{ struct read_format\tvalues;\t  } && PERF_SAMPLE_READ\n\t *\n\t *\t{ u64\t\t\tnr,\n\t *\t  u64\t\t\tips[nr];  } && PERF_SAMPLE_CALLCHAIN\n\t *\n\t *\t#\n\t *\t# The RAW record below is opaque data wrt the ABI\n\t *\t#\n\t *\t# That is, the ABI doesn't make any promises wrt to\n\t *\t# the stability of its content, it may vary depending\n\t *\t# on event, hardware, kernel version and phase of\n\t *\t# the moon.\n\t *\t#\n\t *\t# In other words, PERF_SAMPLE_RAW contents are not an ABI.\n\t *\t#\n\t *\n\t *\t{ u32\t\t\tsize;\n\t *\t  char                  data[size];}&& PERF_SAMPLE_RAW\n\t * };\n\t */\n\tPERF_RECORD_SAMPLE\t\t\t= 9,\n\n\tPERF_RECORD_MAX,\t\t\t/* non-ABI */\n};\n\nenum perf_callchain_context {\n\tPERF_CONTEXT_HV\t\t\t= (__u64)-32,\n\tPERF_CONTEXT_KERNEL\t\t= (__u64)-128,\n\tPERF_CONTEXT_USER\t\t= (__u64)-512,\n\n\tPERF_CONTEXT_GUEST\t\t= (__u64)-2048,\n\tPERF_CONTEXT_GUEST_KERNEL\t= (__u64)-2176,\n\tPERF_CONTEXT_GUEST_USER\t\t= (__u64)-2560,\n\n\tPERF_CONTEXT_MAX\t\t= (__u64)-4095,\n};\n\n#define PERF_FLAG_FD_NO_GROUP\t\t(1U << 0)\n#define PERF_FLAG_FD_OUTPUT\t\t(1U << 1)\n#define PERF_FLAG_PID_CGROUP\t\t(1U << 2) /* pid=cgroup id, per-cpu mode only */\n\n#ifdef __KERNEL__\n/*\n * Kernel-internal data types and definitions:\n */\n\n#ifdef CONFIG_PERF_EVENTS\n# include <linux/cgroup.h>\n# include <asm/perf_event.h>\n# include <asm/local64.h>\n#endif\n\nstruct perf_guest_info_callbacks {\n\tint\t\t\t\t(*is_in_guest)(void);\n\tint\t\t\t\t(*is_user_mode)(void);\n\tunsigned long\t\t\t(*get_guest_ip)(void);\n};\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n#include <asm/hw_breakpoint.h>\n#endif\n\n#include <linux/list.h>\n#include <linux/mutex.h>\n#include <linux/rculist.h>\n#include <linux/rcupdate.h>\n#include <linux/spinlock.h>\n#include <linux/hrtimer.h>\n#include <linux/fs.h>\n#include <linux/pid_namespace.h>\n#include <linux/workqueue.h>\n#include <linux/ftrace.h>\n#include <linux/cpu.h>\n#include <linux/irq_work.h>\n#include <linux/jump_label.h>\n#include <asm/atomic.h>\n#include <asm/local.h>\n\n#define PERF_MAX_STACK_DEPTH\t\t255\n\nstruct perf_callchain_entry {\n\t__u64\t\t\t\tnr;\n\t__u64\t\t\t\tip[PERF_MAX_STACK_DEPTH];\n};\n\nstruct perf_raw_record {\n\tu32\t\t\t\tsize;\n\tvoid\t\t\t\t*data;\n};\n\nstruct perf_branch_entry {\n\t__u64\t\t\t\tfrom;\n\t__u64\t\t\t\tto;\n\t__u64\t\t\t\tflags;\n};\n\nstruct perf_branch_stack {\n\t__u64\t\t\t\tnr;\n\tstruct perf_branch_entry\tentries[0];\n};\n\nstruct task_struct;\n\n/**\n * struct hw_perf_event - performance event hardware details:\n */\nstruct hw_perf_event {\n#ifdef CONFIG_PERF_EVENTS\n\tunion {\n\t\tstruct { /* hardware */\n\t\t\tu64\t\tconfig;\n\t\t\tu64\t\tlast_tag;\n\t\t\tunsigned long\tconfig_base;\n\t\t\tunsigned long\tevent_base;\n\t\t\tint\t\tidx;\n\t\t\tint\t\tlast_cpu;\n\t\t\tunsigned int\textra_reg;\n\t\t\tu64\t\textra_config;\n\t\t\tint\t\textra_alloc;\n\t\t};\n\t\tstruct { /* software */\n\t\t\tstruct hrtimer\thrtimer;\n\t\t};\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\t\tstruct { /* breakpoint */\n\t\t\tstruct arch_hw_breakpoint\tinfo;\n\t\t\tstruct list_head\t\tbp_list;\n\t\t\t/*\n\t\t\t * Crufty hack to avoid the chicken and egg\n\t\t\t * problem hw_breakpoint has with context\n\t\t\t * creation and event initalization.\n\t\t\t */\n\t\t\tstruct task_struct\t\t*bp_target;\n\t\t};\n#endif\n\t};\n\tint\t\t\t\tstate;\n\tlocal64_t\t\t\tprev_count;\n\tu64\t\t\t\tsample_period;\n\tu64\t\t\t\tlast_period;\n\tlocal64_t\t\t\tperiod_left;\n\tu64\t\t\t\tinterrupts;\n\n\tu64\t\t\t\tfreq_time_stamp;\n\tu64\t\t\t\tfreq_count_stamp;\n#endif\n};\n\n/*\n * hw_perf_event::state flags\n */\n#define PERF_HES_STOPPED\t0x01 /* the counter is stopped */\n#define PERF_HES_UPTODATE\t0x02 /* event->count up-to-date */\n#define PERF_HES_ARCH\t\t0x04\n\nstruct perf_event;\n\n/*\n * Common implementation detail of pmu::{start,commit,cancel}_txn\n */\n#define PERF_EVENT_TXN 0x1\n\n/**\n * struct pmu - generic performance monitoring unit\n */\nstruct pmu {\n\tstruct list_head\t\tentry;\n\n\tstruct device\t\t\t*dev;\n\tchar\t\t\t\t*name;\n\tint\t\t\t\ttype;\n\n\tint * __percpu\t\t\tpmu_disable_count;\n\tstruct perf_cpu_context * __percpu pmu_cpu_context;\n\tint\t\t\t\ttask_ctx_nr;\n\n\t/*\n\t * Fully disable/enable this PMU, can be used to protect from the PMI\n\t * as well as for lazy/batch writing of the MSRs.\n\t */\n\tvoid (*pmu_enable)\t\t(struct pmu *pmu); /* optional */\n\tvoid (*pmu_disable)\t\t(struct pmu *pmu); /* optional */\n\n\t/*\n\t * Try and initialize the event for this PMU.\n\t * Should return -ENOENT when the @event doesn't match this PMU.\n\t */\n\tint (*event_init)\t\t(struct perf_event *event);\n\n#define PERF_EF_START\t0x01\t\t/* start the counter when adding    */\n#define PERF_EF_RELOAD\t0x02\t\t/* reload the counter when starting */\n#define PERF_EF_UPDATE\t0x04\t\t/* update the counter when stopping */\n\n\t/*\n\t * Adds/Removes a counter to/from the PMU, can be done inside\n\t * a transaction, see the ->*_txn() methods.\n\t */\n\tint  (*add)\t\t\t(struct perf_event *event, int flags);\n\tvoid (*del)\t\t\t(struct perf_event *event, int flags);\n\n\t/*\n\t * Starts/Stops a counter present on the PMU. The PMI handler\n\t * should stop the counter when perf_event_overflow() returns\n\t * !0. ->start() will be used to continue.\n\t */\n\tvoid (*start)\t\t\t(struct perf_event *event, int flags);\n\tvoid (*stop)\t\t\t(struct perf_event *event, int flags);\n\n\t/*\n\t * Updates the counter value of the event.\n\t */\n\tvoid (*read)\t\t\t(struct perf_event *event);\n\n\t/*\n\t * Group events scheduling is treated as a transaction, add\n\t * group events as a whole and perform one schedulability test.\n\t * If the test fails, roll back the whole group\n\t *\n\t * Start the transaction, after this ->add() doesn't need to\n\t * do schedulability tests.\n\t */\n\tvoid (*start_txn)\t\t(struct pmu *pmu); /* optional */\n\t/*\n\t * If ->start_txn() disabled the ->add() schedulability test\n\t * then ->commit_txn() is required to perform one. On success\n\t * the transaction is closed. On error the transaction is kept\n\t * open until ->cancel_txn() is called.\n\t */\n\tint  (*commit_txn)\t\t(struct pmu *pmu); /* optional */\n\t/*\n\t * Will cancel the transaction, assumes ->del() is called\n\t * for each successful ->add() during the transaction.\n\t */\n\tvoid (*cancel_txn)\t\t(struct pmu *pmu); /* optional */\n};\n\n/**\n * enum perf_event_active_state - the states of a event\n */\nenum perf_event_active_state {\n\tPERF_EVENT_STATE_ERROR\t\t= -2,\n\tPERF_EVENT_STATE_OFF\t\t= -1,\n\tPERF_EVENT_STATE_INACTIVE\t=  0,\n\tPERF_EVENT_STATE_ACTIVE\t\t=  1,\n};\n\nstruct file;\nstruct perf_sample_data;\n\ntypedef void (*perf_overflow_handler_t)(struct perf_event *,\n\t\t\t\t\tstruct perf_sample_data *,\n\t\t\t\t\tstruct pt_regs *regs);\n\nenum perf_group_flag {\n\tPERF_GROUP_SOFTWARE\t\t= 0x1,\n};\n\n#define SWEVENT_HLIST_BITS\t\t8\n#define SWEVENT_HLIST_SIZE\t\t(1 << SWEVENT_HLIST_BITS)\n\nstruct swevent_hlist {\n\tstruct hlist_head\t\theads[SWEVENT_HLIST_SIZE];\n\tstruct rcu_head\t\t\trcu_head;\n};\n\n#define PERF_ATTACH_CONTEXT\t0x01\n#define PERF_ATTACH_GROUP\t0x02\n#define PERF_ATTACH_TASK\t0x04\n\n#ifdef CONFIG_CGROUP_PERF\n/*\n * perf_cgroup_info keeps track of time_enabled for a cgroup.\n * This is a per-cpu dynamically allocated data structure.\n */\nstruct perf_cgroup_info {\n\tu64\t\t\t\ttime;\n\tu64\t\t\t\ttimestamp;\n};\n\nstruct perf_cgroup {\n\tstruct\t\t\t\tcgroup_subsys_state css;\n\tstruct\t\t\t\tperf_cgroup_info *info;\t/* timing info, one per cpu */\n};\n#endif\n\nstruct ring_buffer;\n\n/**\n * struct perf_event - performance event kernel representation:\n */\nstruct perf_event {\n#ifdef CONFIG_PERF_EVENTS\n\tstruct list_head\t\tgroup_entry;\n\tstruct list_head\t\tevent_entry;\n\tstruct list_head\t\tsibling_list;\n\tstruct hlist_node\t\thlist_entry;\n\tint\t\t\t\tnr_siblings;\n\tint\t\t\t\tgroup_flags;\n\tstruct perf_event\t\t*group_leader;\n\tstruct pmu\t\t\t*pmu;\n\n\tenum perf_event_active_state\tstate;\n\tunsigned int\t\t\tattach_state;\n\tlocal64_t\t\t\tcount;\n\tatomic64_t\t\t\tchild_count;\n\n\t/*\n\t * These are the total time in nanoseconds that the event\n\t * has been enabled (i.e. eligible to run, and the task has\n\t * been scheduled in, if this is a per-task event)\n\t * and running (scheduled onto the CPU), respectively.\n\t *\n\t * They are computed from tstamp_enabled, tstamp_running and\n\t * tstamp_stopped when the event is in INACTIVE or ACTIVE state.\n\t */\n\tu64\t\t\t\ttotal_time_enabled;\n\tu64\t\t\t\ttotal_time_running;\n\n\t/*\n\t * These are timestamps used for computing total_time_enabled\n\t * and total_time_running when the event is in INACTIVE or\n\t * ACTIVE state, measured in nanoseconds from an arbitrary point\n\t * in time.\n\t * tstamp_enabled: the notional time when the event was enabled\n\t * tstamp_running: the notional time when the event was scheduled on\n\t * tstamp_stopped: in INACTIVE state, the notional time when the\n\t *\tevent was scheduled off.\n\t */\n\tu64\t\t\t\ttstamp_enabled;\n\tu64\t\t\t\ttstamp_running;\n\tu64\t\t\t\ttstamp_stopped;\n\n\t/*\n\t * timestamp shadows the actual context timing but it can\n\t * be safely used in NMI interrupt context. It reflects the\n\t * context time as it was when the event was last scheduled in.\n\t *\n\t * ctx_time already accounts for ctx->timestamp. Therefore to\n\t * compute ctx_time for a sample, simply add perf_clock().\n\t */\n\tu64\t\t\t\tshadow_ctx_time;\n\n\tstruct perf_event_attr\t\tattr;\n\tu16\t\t\t\theader_size;\n\tu16\t\t\t\tid_header_size;\n\tu16\t\t\t\tread_size;\n\tstruct hw_perf_event\t\thw;\n\n\tstruct perf_event_context\t*ctx;\n\tstruct file\t\t\t*filp;\n\n\t/*\n\t * These accumulate total time (in nanoseconds) that children\n\t * events have been enabled and running, respectively.\n\t */\n\tatomic64_t\t\t\tchild_total_time_enabled;\n\tatomic64_t\t\t\tchild_total_time_running;\n\n\t/*\n\t * Protect attach/detach and child_list:\n\t */\n\tstruct mutex\t\t\tchild_mutex;\n\tstruct list_head\t\tchild_list;\n\tstruct perf_event\t\t*parent;\n\n\tint\t\t\t\toncpu;\n\tint\t\t\t\tcpu;\n\n\tstruct list_head\t\towner_entry;\n\tstruct task_struct\t\t*owner;\n\n\t/* mmap bits */\n\tstruct mutex\t\t\tmmap_mutex;\n\tatomic_t\t\t\tmmap_count;\n\tint\t\t\t\tmmap_locked;\n\tstruct user_struct\t\t*mmap_user;\n\tstruct ring_buffer\t\t*rb;\n\n\t/* poll related */\n\twait_queue_head_t\t\twaitq;\n\tstruct fasync_struct\t\t*fasync;\n\n\t/* delayed work for NMIs and such */\n\tint\t\t\t\tpending_wakeup;\n\tint\t\t\t\tpending_kill;\n\tint\t\t\t\tpending_disable;\n\tstruct irq_work\t\t\tpending;\n\n\tatomic_t\t\t\tevent_limit;\n\n\tvoid (*destroy)(struct perf_event *);\n\tstruct rcu_head\t\t\trcu_head;\n\n\tstruct pid_namespace\t\t*ns;\n\tu64\t\t\t\tid;\n\n\tperf_overflow_handler_t\t\toverflow_handler;\n\n#ifdef CONFIG_EVENT_TRACING\n\tstruct ftrace_event_call\t*tp_event;\n\tstruct event_filter\t\t*filter;\n#endif\n\n#ifdef CONFIG_CGROUP_PERF\n\tstruct perf_cgroup\t\t*cgrp; /* cgroup event is attach to */\n\tint\t\t\t\tcgrp_defer_enabled;\n#endif\n\n#endif /* CONFIG_PERF_EVENTS */\n};\n\nenum perf_event_context_type {\n\ttask_context,\n\tcpu_context,\n};\n\n/**\n * struct perf_event_context - event context structure\n *\n * Used as a container for task events and CPU events as well:\n */\nstruct perf_event_context {\n\tstruct pmu\t\t\t*pmu;\n\tenum perf_event_context_type\ttype;\n\t/*\n\t * Protect the states of the events in the list,\n\t * nr_active, and the list:\n\t */\n\traw_spinlock_t\t\t\tlock;\n\t/*\n\t * Protect the list of events.  Locking either mutex or lock\n\t * is sufficient to ensure the list doesn't change; to change\n\t * the list you need to lock both the mutex and the spinlock.\n\t */\n\tstruct mutex\t\t\tmutex;\n\n\tstruct list_head\t\tpinned_groups;\n\tstruct list_head\t\tflexible_groups;\n\tstruct list_head\t\tevent_list;\n\tint\t\t\t\tnr_events;\n\tint\t\t\t\tnr_active;\n\tint\t\t\t\tis_active;\n\tint\t\t\t\tnr_stat;\n\tint\t\t\t\trotate_disable;\n\tatomic_t\t\t\trefcount;\n\tstruct task_struct\t\t*task;\n\n\t/*\n\t * Context clock, runs when context enabled.\n\t */\n\tu64\t\t\t\ttime;\n\tu64\t\t\t\ttimestamp;\n\n\t/*\n\t * These fields let us detect when two contexts have both\n\t * been cloned (inherited) from a common ancestor.\n\t */\n\tstruct perf_event_context\t*parent_ctx;\n\tu64\t\t\t\tparent_gen;\n\tu64\t\t\t\tgeneration;\n\tint\t\t\t\tpin_count;\n\tint\t\t\t\tnr_cgroups; /* cgroup events present */\n\tstruct rcu_head\t\t\trcu_head;\n};\n\n/*\n * Number of contexts where an event can trigger:\n *\ttask, softirq, hardirq, nmi.\n */\n#define PERF_NR_CONTEXTS\t4\n\n/**\n * struct perf_event_cpu_context - per cpu event context structure\n */\nstruct perf_cpu_context {\n\tstruct perf_event_context\tctx;\n\tstruct perf_event_context\t*task_ctx;\n\tint\t\t\t\tactive_oncpu;\n\tint\t\t\t\texclusive;\n\tstruct list_head\t\trotation_list;\n\tint\t\t\t\tjiffies_interval;\n\tstruct pmu\t\t\t*active_pmu;\n\tstruct perf_cgroup\t\t*cgrp;\n};\n\nstruct perf_output_handle {\n\tstruct perf_event\t\t*event;\n\tstruct ring_buffer\t\t*rb;\n\tunsigned long\t\t\twakeup;\n\tunsigned long\t\t\tsize;\n\tvoid\t\t\t\t*addr;\n\tint\t\t\t\tpage;\n\tint\t\t\t\tsample;\n};\n\n#ifdef CONFIG_PERF_EVENTS\n\nextern int perf_pmu_register(struct pmu *pmu, char *name, int type);\nextern void perf_pmu_unregister(struct pmu *pmu);\n\nextern int perf_num_counters(void);\nextern const char *perf_pmu_name(void);\nextern void __perf_event_task_sched_in(struct task_struct *task);\nextern void __perf_event_task_sched_out(struct task_struct *task, struct task_struct *next);\nextern int perf_event_init_task(struct task_struct *child);\nextern void perf_event_exit_task(struct task_struct *child);\nextern void perf_event_free_task(struct task_struct *task);\nextern void perf_event_delayed_put(struct task_struct *task);\nextern void perf_event_print_debug(void);\nextern void perf_pmu_disable(struct pmu *pmu);\nextern void perf_pmu_enable(struct pmu *pmu);\nextern int perf_event_task_disable(void);\nextern int perf_event_task_enable(void);\nextern void perf_event_update_userpage(struct perf_event *event);\nextern int perf_event_release_kernel(struct perf_event *event);\nextern struct perf_event *\nperf_event_create_kernel_counter(struct perf_event_attr *attr,\n\t\t\t\tint cpu,\n\t\t\t\tstruct task_struct *task,\n\t\t\t\tperf_overflow_handler_t callback);\nextern u64 perf_event_read_value(struct perf_event *event,\n\t\t\t\t u64 *enabled, u64 *running);\n\nstruct perf_sample_data {\n\tu64\t\t\t\ttype;\n\n\tu64\t\t\t\tip;\n\tstruct {\n\t\tu32\tpid;\n\t\tu32\ttid;\n\t}\t\t\t\ttid_entry;\n\tu64\t\t\t\ttime;\n\tu64\t\t\t\taddr;\n\tu64\t\t\t\tid;\n\tu64\t\t\t\tstream_id;\n\tstruct {\n\t\tu32\tcpu;\n\t\tu32\treserved;\n\t}\t\t\t\tcpu_entry;\n\tu64\t\t\t\tperiod;\n\tstruct perf_callchain_entry\t*callchain;\n\tstruct perf_raw_record\t\t*raw;\n};\n\nstatic inline void perf_sample_data_init(struct perf_sample_data *data, u64 addr)\n{\n\tdata->addr = addr;\n\tdata->raw  = NULL;\n}\n\nextern void perf_output_sample(struct perf_output_handle *handle,\n\t\t\t       struct perf_event_header *header,\n\t\t\t       struct perf_sample_data *data,\n\t\t\t       struct perf_event *event);\nextern void perf_prepare_sample(struct perf_event_header *header,\n\t\t\t\tstruct perf_sample_data *data,\n\t\t\t\tstruct perf_event *event,\n\t\t\t\tstruct pt_regs *regs);\n\nextern int perf_event_overflow(struct perf_event *event,\n\t\t\t\t struct perf_sample_data *data,\n\t\t\t\t struct pt_regs *regs);\n\nstatic inline bool is_sampling_event(struct perf_event *event)\n{\n\treturn event->attr.sample_period != 0;\n}\n\n/*\n * Return 1 for a software event, 0 for a hardware event\n */\nstatic inline int is_software_event(struct perf_event *event)\n{\n\treturn event->pmu->task_ctx_nr == perf_sw_context;\n}\n\nextern struct jump_label_key perf_swevent_enabled[PERF_COUNT_SW_MAX];\n\nextern void __perf_sw_event(u32, u64, struct pt_regs *, u64);\n\n#ifndef perf_arch_fetch_caller_regs\nstatic inline void perf_arch_fetch_caller_regs(struct pt_regs *regs, unsigned long ip) { }\n#endif\n\n/*\n * Take a snapshot of the regs. Skip ip and frame pointer to\n * the nth caller. We only need a few of the regs:\n * - ip for PERF_SAMPLE_IP\n * - cs for user_mode() tests\n * - bp for callchains\n * - eflags, for future purposes, just in case\n */\nstatic inline void perf_fetch_caller_regs(struct pt_regs *regs)\n{\n\tmemset(regs, 0, sizeof(*regs));\n\n\tperf_arch_fetch_caller_regs(regs, CALLER_ADDR0);\n}\n\nstatic __always_inline void\nperf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)\n{\n\tstruct pt_regs hot_regs;\n\n\tif (static_branch(&perf_swevent_enabled[event_id])) {\n\t\tif (!regs) {\n\t\t\tperf_fetch_caller_regs(&hot_regs);\n\t\t\tregs = &hot_regs;\n\t\t}\n\t\t__perf_sw_event(event_id, nr, regs, addr);\n\t}\n}\n\nextern struct jump_label_key perf_sched_events;\n\nstatic inline void perf_event_task_sched_in(struct task_struct *task)\n{\n\tif (static_branch(&perf_sched_events))\n\t\t__perf_event_task_sched_in(task);\n}\n\nstatic inline void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next)\n{\n\tperf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, NULL, 0);\n\n\t__perf_event_task_sched_out(task, next);\n}\n\nextern void perf_event_mmap(struct vm_area_struct *vma);\nextern struct perf_guest_info_callbacks *perf_guest_cbs;\nextern int perf_register_guest_info_callbacks(struct perf_guest_info_callbacks *callbacks);\nextern int perf_unregister_guest_info_callbacks(struct perf_guest_info_callbacks *callbacks);\n\nextern void perf_event_comm(struct task_struct *tsk);\nextern void perf_event_fork(struct task_struct *tsk);\n\n/* Callchains */\nDECLARE_PER_CPU(struct perf_callchain_entry, perf_callchain_entry);\n\nextern void perf_callchain_user(struct perf_callchain_entry *entry, struct pt_regs *regs);\nextern void perf_callchain_kernel(struct perf_callchain_entry *entry, struct pt_regs *regs);\n\nstatic inline void perf_callchain_store(struct perf_callchain_entry *entry, u64 ip)\n{\n\tif (entry->nr < PERF_MAX_STACK_DEPTH)\n\t\tentry->ip[entry->nr++] = ip;\n}\n\nextern int sysctl_perf_event_paranoid;\nextern int sysctl_perf_event_mlock;\nextern int sysctl_perf_event_sample_rate;\n\nextern int perf_proc_update_handler(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *lenp,\n\t\tloff_t *ppos);\n\nstatic inline bool perf_paranoid_tracepoint_raw(void)\n{\n\treturn sysctl_perf_event_paranoid > -1;\n}\n\nstatic inline bool perf_paranoid_cpu(void)\n{\n\treturn sysctl_perf_event_paranoid > 0;\n}\n\nstatic inline bool perf_paranoid_kernel(void)\n{\n\treturn sysctl_perf_event_paranoid > 1;\n}\n\nextern void perf_event_init(void);\nextern void perf_tp_event(u64 addr, u64 count, void *record,\n\t\t\t  int entry_size, struct pt_regs *regs,\n\t\t\t  struct hlist_head *head, int rctx);\nextern void perf_bp_event(struct perf_event *event, void *data);\n\n#ifndef perf_misc_flags\n# define perf_misc_flags(regs) \\\n\t\t(user_mode(regs) ? PERF_RECORD_MISC_USER : PERF_RECORD_MISC_KERNEL)\n# define perf_instruction_pointer(regs)\tinstruction_pointer(regs)\n#endif\n\nextern int perf_output_begin(struct perf_output_handle *handle,\n\t\t\t     struct perf_event *event, unsigned int size,\n\t\t\t     int sample);\nextern void perf_output_end(struct perf_output_handle *handle);\nextern void perf_output_copy(struct perf_output_handle *handle,\n\t\t\t     const void *buf, unsigned int len);\nextern int perf_swevent_get_recursion_context(void);\nextern void perf_swevent_put_recursion_context(int rctx);\nextern void perf_event_enable(struct perf_event *event);\nextern void perf_event_disable(struct perf_event *event);\nextern void perf_event_task_tick(void);\n#else\nstatic inline void\nperf_event_task_sched_in(struct task_struct *task)\t\t\t{ }\nstatic inline void\nperf_event_task_sched_out(struct task_struct *task,\n\t\t\t    struct task_struct *next)\t\t\t{ }\nstatic inline int perf_event_init_task(struct task_struct *child)\t{ return 0; }\nstatic inline void perf_event_exit_task(struct task_struct *child)\t{ }\nstatic inline void perf_event_free_task(struct task_struct *task)\t{ }\nstatic inline void perf_event_delayed_put(struct task_struct *task)\t{ }\nstatic inline void perf_event_print_debug(void)\t\t\t\t{ }\nstatic inline int perf_event_task_disable(void)\t\t\t\t{ return -EINVAL; }\nstatic inline int perf_event_task_enable(void)\t\t\t\t{ return -EINVAL; }\n\nstatic inline void\nperf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)\t{ }\nstatic inline void\nperf_bp_event(struct perf_event *event, void *data)\t\t\t{ }\n\nstatic inline int perf_register_guest_info_callbacks\n(struct perf_guest_info_callbacks *callbacks)\t\t\t\t{ return 0; }\nstatic inline int perf_unregister_guest_info_callbacks\n(struct perf_guest_info_callbacks *callbacks)\t\t\t\t{ return 0; }\n\nstatic inline void perf_event_mmap(struct vm_area_struct *vma)\t\t{ }\nstatic inline void perf_event_comm(struct task_struct *tsk)\t\t{ }\nstatic inline void perf_event_fork(struct task_struct *tsk)\t\t{ }\nstatic inline void perf_event_init(void)\t\t\t\t{ }\nstatic inline int  perf_swevent_get_recursion_context(void)\t\t{ return -1; }\nstatic inline void perf_swevent_put_recursion_context(int rctx)\t\t{ }\nstatic inline void perf_event_enable(struct perf_event *event)\t\t{ }\nstatic inline void perf_event_disable(struct perf_event *event)\t\t{ }\nstatic inline void perf_event_task_tick(void)\t\t\t\t{ }\n#endif\n\n#define perf_output_put(handle, x) perf_output_copy((handle), &(x), sizeof(x))\n\n/*\n * This has to have a higher priority than migration_notifier in sched.c.\n */\n#define perf_cpu_notifier(fn)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic struct notifier_block fn##_nb __cpuinitdata =\t\t\\\n\t\t{ .notifier_call = fn, .priority = CPU_PRI_PERF };\t\\\n\tfn(&fn##_nb, (unsigned long)CPU_UP_PREPARE,\t\t\t\\\n\t\t(void *)(unsigned long)smp_processor_id());\t\t\\\n\tfn(&fn##_nb, (unsigned long)CPU_STARTING,\t\t\t\\\n\t\t(void *)(unsigned long)smp_processor_id());\t\t\\\n\tfn(&fn##_nb, (unsigned long)CPU_ONLINE,\t\t\t\t\\\n\t\t(void *)(unsigned long)smp_processor_id());\t\t\\\n\tregister_cpu_notifier(&fn##_nb);\t\t\t\t\\\n} while (0)\n\n#endif /* __KERNEL__ */\n#endif /* _LINUX_PERF_EVENT_H */\n", "/*\n * Performance events core code:\n *\n *  Copyright (C) 2008 Thomas Gleixner <tglx@linutronix.de>\n *  Copyright (C) 2008-2011 Red Hat, Inc., Ingo Molnar\n *  Copyright (C) 2008-2011 Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>\n *  Copyright  \ufffd  2009 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>\n *\n * For licensing details see kernel-base/COPYING\n */\n\n#include <linux/fs.h>\n#include <linux/mm.h>\n#include <linux/cpu.h>\n#include <linux/smp.h>\n#include <linux/idr.h>\n#include <linux/file.h>\n#include <linux/poll.h>\n#include <linux/slab.h>\n#include <linux/hash.h>\n#include <linux/sysfs.h>\n#include <linux/dcache.h>\n#include <linux/percpu.h>\n#include <linux/ptrace.h>\n#include <linux/reboot.h>\n#include <linux/vmstat.h>\n#include <linux/device.h>\n#include <linux/vmalloc.h>\n#include <linux/hardirq.h>\n#include <linux/rculist.h>\n#include <linux/uaccess.h>\n#include <linux/syscalls.h>\n#include <linux/anon_inodes.h>\n#include <linux/kernel_stat.h>\n#include <linux/perf_event.h>\n#include <linux/ftrace_event.h>\n#include <linux/hw_breakpoint.h>\n\n#include \"internal.h\"\n\n#include <asm/irq_regs.h>\n\nstruct remote_function_call {\n\tstruct task_struct\t*p;\n\tint\t\t\t(*func)(void *info);\n\tvoid\t\t\t*info;\n\tint\t\t\tret;\n};\n\nstatic void remote_function(void *data)\n{\n\tstruct remote_function_call *tfc = data;\n\tstruct task_struct *p = tfc->p;\n\n\tif (p) {\n\t\ttfc->ret = -EAGAIN;\n\t\tif (task_cpu(p) != smp_processor_id() || !task_curr(p))\n\t\t\treturn;\n\t}\n\n\ttfc->ret = tfc->func(tfc->info);\n}\n\n/**\n * task_function_call - call a function on the cpu on which a task runs\n * @p:\t\tthe task to evaluate\n * @func:\tthe function to be called\n * @info:\tthe function call argument\n *\n * Calls the function @func when the task is currently running. This might\n * be on the current CPU, which just calls the function directly\n *\n * returns: @func return value, or\n *\t    -ESRCH  - when the process isn't running\n *\t    -EAGAIN - when the process moved away\n */\nstatic int\ntask_function_call(struct task_struct *p, int (*func) (void *info), void *info)\n{\n\tstruct remote_function_call data = {\n\t\t.p\t= p,\n\t\t.func\t= func,\n\t\t.info\t= info,\n\t\t.ret\t= -ESRCH, /* No such (running) process */\n\t};\n\n\tif (task_curr(p))\n\t\tsmp_call_function_single(task_cpu(p), remote_function, &data, 1);\n\n\treturn data.ret;\n}\n\n/**\n * cpu_function_call - call a function on the cpu\n * @func:\tthe function to be called\n * @info:\tthe function call argument\n *\n * Calls the function @func on the remote cpu.\n *\n * returns: @func return value or -ENXIO when the cpu is offline\n */\nstatic int cpu_function_call(int cpu, int (*func) (void *info), void *info)\n{\n\tstruct remote_function_call data = {\n\t\t.p\t= NULL,\n\t\t.func\t= func,\n\t\t.info\t= info,\n\t\t.ret\t= -ENXIO, /* No such CPU */\n\t};\n\n\tsmp_call_function_single(cpu, remote_function, &data, 1);\n\n\treturn data.ret;\n}\n\n#define PERF_FLAG_ALL (PERF_FLAG_FD_NO_GROUP |\\\n\t\t       PERF_FLAG_FD_OUTPUT  |\\\n\t\t       PERF_FLAG_PID_CGROUP)\n\nenum event_type_t {\n\tEVENT_FLEXIBLE = 0x1,\n\tEVENT_PINNED = 0x2,\n\tEVENT_ALL = EVENT_FLEXIBLE | EVENT_PINNED,\n};\n\n/*\n * perf_sched_events : >0 events exist\n * perf_cgroup_events: >0 per-cpu cgroup events exist on this cpu\n */\nstruct jump_label_key perf_sched_events __read_mostly;\nstatic DEFINE_PER_CPU(atomic_t, perf_cgroup_events);\n\nstatic atomic_t nr_mmap_events __read_mostly;\nstatic atomic_t nr_comm_events __read_mostly;\nstatic atomic_t nr_task_events __read_mostly;\n\nstatic LIST_HEAD(pmus);\nstatic DEFINE_MUTEX(pmus_lock);\nstatic struct srcu_struct pmus_srcu;\n\n/*\n * perf event paranoia level:\n *  -1 - not paranoid at all\n *   0 - disallow raw tracepoint access for unpriv\n *   1 - disallow cpu events for unpriv\n *   2 - disallow kernel profiling for unpriv\n */\nint sysctl_perf_event_paranoid __read_mostly = 1;\n\n/* Minimum for 512 kiB + 1 user control page */\nint sysctl_perf_event_mlock __read_mostly = 512 + (PAGE_SIZE / 1024); /* 'free' kiB per user */\n\n/*\n * max perf event sample rate\n */\n#define DEFAULT_MAX_SAMPLE_RATE 100000\nint sysctl_perf_event_sample_rate __read_mostly = DEFAULT_MAX_SAMPLE_RATE;\nstatic int max_samples_per_tick __read_mostly =\n\tDIV_ROUND_UP(DEFAULT_MAX_SAMPLE_RATE, HZ);\n\nint perf_proc_update_handler(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *lenp,\n\t\tloff_t *ppos)\n{\n\tint ret = proc_dointvec(table, write, buffer, lenp, ppos);\n\n\tif (ret || !write)\n\t\treturn ret;\n\n\tmax_samples_per_tick = DIV_ROUND_UP(sysctl_perf_event_sample_rate, HZ);\n\n\treturn 0;\n}\n\nstatic atomic64_t perf_event_id;\n\nstatic void cpu_ctx_sched_out(struct perf_cpu_context *cpuctx,\n\t\t\t      enum event_type_t event_type);\n\nstatic void cpu_ctx_sched_in(struct perf_cpu_context *cpuctx,\n\t\t\t     enum event_type_t event_type,\n\t\t\t     struct task_struct *task);\n\nstatic void update_context_time(struct perf_event_context *ctx);\nstatic u64 perf_event_time(struct perf_event *event);\n\nvoid __weak perf_event_print_debug(void)\t{ }\n\nextern __weak const char *perf_pmu_name(void)\n{\n\treturn \"pmu\";\n}\n\nstatic inline u64 perf_clock(void)\n{\n\treturn local_clock();\n}\n\nstatic inline struct perf_cpu_context *\n__get_cpu_context(struct perf_event_context *ctx)\n{\n\treturn this_cpu_ptr(ctx->pmu->pmu_cpu_context);\n}\n\nstatic void perf_ctx_lock(struct perf_cpu_context *cpuctx,\n\t\t\t  struct perf_event_context *ctx)\n{\n\traw_spin_lock(&cpuctx->ctx.lock);\n\tif (ctx)\n\t\traw_spin_lock(&ctx->lock);\n}\n\nstatic void perf_ctx_unlock(struct perf_cpu_context *cpuctx,\n\t\t\t    struct perf_event_context *ctx)\n{\n\tif (ctx)\n\t\traw_spin_unlock(&ctx->lock);\n\traw_spin_unlock(&cpuctx->ctx.lock);\n}\n\n#ifdef CONFIG_CGROUP_PERF\n\n/*\n * Must ensure cgroup is pinned (css_get) before calling\n * this function. In other words, we cannot call this function\n * if there is no cgroup event for the current CPU context.\n */\nstatic inline struct perf_cgroup *\nperf_cgroup_from_task(struct task_struct *task)\n{\n\treturn container_of(task_subsys_state(task, perf_subsys_id),\n\t\t\tstruct perf_cgroup, css);\n}\n\nstatic inline bool\nperf_cgroup_match(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_cpu_context *cpuctx = __get_cpu_context(ctx);\n\n\treturn !event->cgrp || event->cgrp == cpuctx->cgrp;\n}\n\nstatic inline void perf_get_cgroup(struct perf_event *event)\n{\n\tcss_get(&event->cgrp->css);\n}\n\nstatic inline void perf_put_cgroup(struct perf_event *event)\n{\n\tcss_put(&event->cgrp->css);\n}\n\nstatic inline void perf_detach_cgroup(struct perf_event *event)\n{\n\tperf_put_cgroup(event);\n\tevent->cgrp = NULL;\n}\n\nstatic inline int is_cgroup_event(struct perf_event *event)\n{\n\treturn event->cgrp != NULL;\n}\n\nstatic inline u64 perf_cgroup_event_time(struct perf_event *event)\n{\n\tstruct perf_cgroup_info *t;\n\n\tt = per_cpu_ptr(event->cgrp->info, event->cpu);\n\treturn t->time;\n}\n\nstatic inline void __update_cgrp_time(struct perf_cgroup *cgrp)\n{\n\tstruct perf_cgroup_info *info;\n\tu64 now;\n\n\tnow = perf_clock();\n\n\tinfo = this_cpu_ptr(cgrp->info);\n\n\tinfo->time += now - info->timestamp;\n\tinfo->timestamp = now;\n}\n\nstatic inline void update_cgrp_time_from_cpuctx(struct perf_cpu_context *cpuctx)\n{\n\tstruct perf_cgroup *cgrp_out = cpuctx->cgrp;\n\tif (cgrp_out)\n\t\t__update_cgrp_time(cgrp_out);\n}\n\nstatic inline void update_cgrp_time_from_event(struct perf_event *event)\n{\n\tstruct perf_cgroup *cgrp;\n\n\t/*\n\t * ensure we access cgroup data only when needed and\n\t * when we know the cgroup is pinned (css_get)\n\t */\n\tif (!is_cgroup_event(event))\n\t\treturn;\n\n\tcgrp = perf_cgroup_from_task(current);\n\t/*\n\t * Do not update time when cgroup is not active\n\t */\n\tif (cgrp == event->cgrp)\n\t\t__update_cgrp_time(event->cgrp);\n}\n\nstatic inline void\nperf_cgroup_set_timestamp(struct task_struct *task,\n\t\t\t  struct perf_event_context *ctx)\n{\n\tstruct perf_cgroup *cgrp;\n\tstruct perf_cgroup_info *info;\n\n\t/*\n\t * ctx->lock held by caller\n\t * ensure we do not access cgroup data\n\t * unless we have the cgroup pinned (css_get)\n\t */\n\tif (!task || !ctx->nr_cgroups)\n\t\treturn;\n\n\tcgrp = perf_cgroup_from_task(task);\n\tinfo = this_cpu_ptr(cgrp->info);\n\tinfo->timestamp = ctx->timestamp;\n}\n\n#define PERF_CGROUP_SWOUT\t0x1 /* cgroup switch out every event */\n#define PERF_CGROUP_SWIN\t0x2 /* cgroup switch in events based on task */\n\n/*\n * reschedule events based on the cgroup constraint of task.\n *\n * mode SWOUT : schedule out everything\n * mode SWIN : schedule in based on cgroup for next\n */\nvoid perf_cgroup_switch(struct task_struct *task, int mode)\n{\n\tstruct perf_cpu_context *cpuctx;\n\tstruct pmu *pmu;\n\tunsigned long flags;\n\n\t/*\n\t * disable interrupts to avoid geting nr_cgroup\n\t * changes via __perf_event_disable(). Also\n\t * avoids preemption.\n\t */\n\tlocal_irq_save(flags);\n\n\t/*\n\t * we reschedule only in the presence of cgroup\n\t * constrained events.\n\t */\n\trcu_read_lock();\n\n\tlist_for_each_entry_rcu(pmu, &pmus, entry) {\n\t\tcpuctx = this_cpu_ptr(pmu->pmu_cpu_context);\n\n\t\t/*\n\t\t * perf_cgroup_events says at least one\n\t\t * context on this CPU has cgroup events.\n\t\t *\n\t\t * ctx->nr_cgroups reports the number of cgroup\n\t\t * events for a context.\n\t\t */\n\t\tif (cpuctx->ctx.nr_cgroups > 0) {\n\t\t\tperf_ctx_lock(cpuctx, cpuctx->task_ctx);\n\t\t\tperf_pmu_disable(cpuctx->ctx.pmu);\n\n\t\t\tif (mode & PERF_CGROUP_SWOUT) {\n\t\t\t\tcpu_ctx_sched_out(cpuctx, EVENT_ALL);\n\t\t\t\t/*\n\t\t\t\t * must not be done before ctxswout due\n\t\t\t\t * to event_filter_match() in event_sched_out()\n\t\t\t\t */\n\t\t\t\tcpuctx->cgrp = NULL;\n\t\t\t}\n\n\t\t\tif (mode & PERF_CGROUP_SWIN) {\n\t\t\t\tWARN_ON_ONCE(cpuctx->cgrp);\n\t\t\t\t/* set cgrp before ctxsw in to\n\t\t\t\t * allow event_filter_match() to not\n\t\t\t\t * have to pass task around\n\t\t\t\t */\n\t\t\t\tcpuctx->cgrp = perf_cgroup_from_task(task);\n\t\t\t\tcpu_ctx_sched_in(cpuctx, EVENT_ALL, task);\n\t\t\t}\n\t\t\tperf_pmu_enable(cpuctx->ctx.pmu);\n\t\t\tperf_ctx_unlock(cpuctx, cpuctx->task_ctx);\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\n\tlocal_irq_restore(flags);\n}\n\nstatic inline void perf_cgroup_sched_out(struct task_struct *task)\n{\n\tperf_cgroup_switch(task, PERF_CGROUP_SWOUT);\n}\n\nstatic inline void perf_cgroup_sched_in(struct task_struct *task)\n{\n\tperf_cgroup_switch(task, PERF_CGROUP_SWIN);\n}\n\nstatic inline int perf_cgroup_connect(int fd, struct perf_event *event,\n\t\t\t\t      struct perf_event_attr *attr,\n\t\t\t\t      struct perf_event *group_leader)\n{\n\tstruct perf_cgroup *cgrp;\n\tstruct cgroup_subsys_state *css;\n\tstruct file *file;\n\tint ret = 0, fput_needed;\n\n\tfile = fget_light(fd, &fput_needed);\n\tif (!file)\n\t\treturn -EBADF;\n\n\tcss = cgroup_css_from_dir(file, perf_subsys_id);\n\tif (IS_ERR(css)) {\n\t\tret = PTR_ERR(css);\n\t\tgoto out;\n\t}\n\n\tcgrp = container_of(css, struct perf_cgroup, css);\n\tevent->cgrp = cgrp;\n\n\t/* must be done before we fput() the file */\n\tperf_get_cgroup(event);\n\n\t/*\n\t * all events in a group must monitor\n\t * the same cgroup because a task belongs\n\t * to only one perf cgroup at a time\n\t */\n\tif (group_leader && group_leader->cgrp != cgrp) {\n\t\tperf_detach_cgroup(event);\n\t\tret = -EINVAL;\n\t}\nout:\n\tfput_light(file, fput_needed);\n\treturn ret;\n}\n\nstatic inline void\nperf_cgroup_set_shadow_time(struct perf_event *event, u64 now)\n{\n\tstruct perf_cgroup_info *t;\n\tt = per_cpu_ptr(event->cgrp->info, event->cpu);\n\tevent->shadow_ctx_time = now - t->timestamp;\n}\n\nstatic inline void\nperf_cgroup_defer_enabled(struct perf_event *event)\n{\n\t/*\n\t * when the current task's perf cgroup does not match\n\t * the event's, we need to remember to call the\n\t * perf_mark_enable() function the first time a task with\n\t * a matching perf cgroup is scheduled in.\n\t */\n\tif (is_cgroup_event(event) && !perf_cgroup_match(event))\n\t\tevent->cgrp_defer_enabled = 1;\n}\n\nstatic inline void\nperf_cgroup_mark_enabled(struct perf_event *event,\n\t\t\t struct perf_event_context *ctx)\n{\n\tstruct perf_event *sub;\n\tu64 tstamp = perf_event_time(event);\n\n\tif (!event->cgrp_defer_enabled)\n\t\treturn;\n\n\tevent->cgrp_defer_enabled = 0;\n\n\tevent->tstamp_enabled = tstamp - event->total_time_enabled;\n\tlist_for_each_entry(sub, &event->sibling_list, group_entry) {\n\t\tif (sub->state >= PERF_EVENT_STATE_INACTIVE) {\n\t\t\tsub->tstamp_enabled = tstamp - sub->total_time_enabled;\n\t\t\tsub->cgrp_defer_enabled = 0;\n\t\t}\n\t}\n}\n#else /* !CONFIG_CGROUP_PERF */\n\nstatic inline bool\nperf_cgroup_match(struct perf_event *event)\n{\n\treturn true;\n}\n\nstatic inline void perf_detach_cgroup(struct perf_event *event)\n{}\n\nstatic inline int is_cgroup_event(struct perf_event *event)\n{\n\treturn 0;\n}\n\nstatic inline u64 perf_cgroup_event_cgrp_time(struct perf_event *event)\n{\n\treturn 0;\n}\n\nstatic inline void update_cgrp_time_from_event(struct perf_event *event)\n{\n}\n\nstatic inline void update_cgrp_time_from_cpuctx(struct perf_cpu_context *cpuctx)\n{\n}\n\nstatic inline void perf_cgroup_sched_out(struct task_struct *task)\n{\n}\n\nstatic inline void perf_cgroup_sched_in(struct task_struct *task)\n{\n}\n\nstatic inline int perf_cgroup_connect(pid_t pid, struct perf_event *event,\n\t\t\t\t      struct perf_event_attr *attr,\n\t\t\t\t      struct perf_event *group_leader)\n{\n\treturn -EINVAL;\n}\n\nstatic inline void\nperf_cgroup_set_timestamp(struct task_struct *task,\n\t\t\t  struct perf_event_context *ctx)\n{\n}\n\nvoid\nperf_cgroup_switch(struct task_struct *task, struct task_struct *next)\n{\n}\n\nstatic inline void\nperf_cgroup_set_shadow_time(struct perf_event *event, u64 now)\n{\n}\n\nstatic inline u64 perf_cgroup_event_time(struct perf_event *event)\n{\n\treturn 0;\n}\n\nstatic inline void\nperf_cgroup_defer_enabled(struct perf_event *event)\n{\n}\n\nstatic inline void\nperf_cgroup_mark_enabled(struct perf_event *event,\n\t\t\t struct perf_event_context *ctx)\n{\n}\n#endif\n\nvoid perf_pmu_disable(struct pmu *pmu)\n{\n\tint *count = this_cpu_ptr(pmu->pmu_disable_count);\n\tif (!(*count)++)\n\t\tpmu->pmu_disable(pmu);\n}\n\nvoid perf_pmu_enable(struct pmu *pmu)\n{\n\tint *count = this_cpu_ptr(pmu->pmu_disable_count);\n\tif (!--(*count))\n\t\tpmu->pmu_enable(pmu);\n}\n\nstatic DEFINE_PER_CPU(struct list_head, rotation_list);\n\n/*\n * perf_pmu_rotate_start() and perf_rotate_context() are fully serialized\n * because they're strictly cpu affine and rotate_start is called with IRQs\n * disabled, while rotate_context is called from IRQ context.\n */\nstatic void perf_pmu_rotate_start(struct pmu *pmu)\n{\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(pmu->pmu_cpu_context);\n\tstruct list_head *head = &__get_cpu_var(rotation_list);\n\n\tWARN_ON(!irqs_disabled());\n\n\tif (list_empty(&cpuctx->rotation_list))\n\t\tlist_add(&cpuctx->rotation_list, head);\n}\n\nstatic void get_ctx(struct perf_event_context *ctx)\n{\n\tWARN_ON(!atomic_inc_not_zero(&ctx->refcount));\n}\n\nstatic void put_ctx(struct perf_event_context *ctx)\n{\n\tif (atomic_dec_and_test(&ctx->refcount)) {\n\t\tif (ctx->parent_ctx)\n\t\t\tput_ctx(ctx->parent_ctx);\n\t\tif (ctx->task)\n\t\t\tput_task_struct(ctx->task);\n\t\tkfree_rcu(ctx, rcu_head);\n\t}\n}\n\nstatic void unclone_ctx(struct perf_event_context *ctx)\n{\n\tif (ctx->parent_ctx) {\n\t\tput_ctx(ctx->parent_ctx);\n\t\tctx->parent_ctx = NULL;\n\t}\n}\n\nstatic u32 perf_event_pid(struct perf_event *event, struct task_struct *p)\n{\n\t/*\n\t * only top level events have the pid namespace they were created in\n\t */\n\tif (event->parent)\n\t\tevent = event->parent;\n\n\treturn task_tgid_nr_ns(p, event->ns);\n}\n\nstatic u32 perf_event_tid(struct perf_event *event, struct task_struct *p)\n{\n\t/*\n\t * only top level events have the pid namespace they were created in\n\t */\n\tif (event->parent)\n\t\tevent = event->parent;\n\n\treturn task_pid_nr_ns(p, event->ns);\n}\n\n/*\n * If we inherit events we want to return the parent event id\n * to userspace.\n */\nstatic u64 primary_event_id(struct perf_event *event)\n{\n\tu64 id = event->id;\n\n\tif (event->parent)\n\t\tid = event->parent->id;\n\n\treturn id;\n}\n\n/*\n * Get the perf_event_context for a task and lock it.\n * This has to cope with with the fact that until it is locked,\n * the context could get moved to another task.\n */\nstatic struct perf_event_context *\nperf_lock_task_context(struct task_struct *task, int ctxn, unsigned long *flags)\n{\n\tstruct perf_event_context *ctx;\n\n\trcu_read_lock();\nretry:\n\tctx = rcu_dereference(task->perf_event_ctxp[ctxn]);\n\tif (ctx) {\n\t\t/*\n\t\t * If this context is a clone of another, it might\n\t\t * get swapped for another underneath us by\n\t\t * perf_event_task_sched_out, though the\n\t\t * rcu_read_lock() protects us from any context\n\t\t * getting freed.  Lock the context and check if it\n\t\t * got swapped before we could get the lock, and retry\n\t\t * if so.  If we locked the right context, then it\n\t\t * can't get swapped on us any more.\n\t\t */\n\t\traw_spin_lock_irqsave(&ctx->lock, *flags);\n\t\tif (ctx != rcu_dereference(task->perf_event_ctxp[ctxn])) {\n\t\t\traw_spin_unlock_irqrestore(&ctx->lock, *flags);\n\t\t\tgoto retry;\n\t\t}\n\n\t\tif (!atomic_inc_not_zero(&ctx->refcount)) {\n\t\t\traw_spin_unlock_irqrestore(&ctx->lock, *flags);\n\t\t\tctx = NULL;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn ctx;\n}\n\n/*\n * Get the context for a task and increment its pin_count so it\n * can't get swapped to another task.  This also increments its\n * reference count so that the context can't get freed.\n */\nstatic struct perf_event_context *\nperf_pin_task_context(struct task_struct *task, int ctxn)\n{\n\tstruct perf_event_context *ctx;\n\tunsigned long flags;\n\n\tctx = perf_lock_task_context(task, ctxn, &flags);\n\tif (ctx) {\n\t\t++ctx->pin_count;\n\t\traw_spin_unlock_irqrestore(&ctx->lock, flags);\n\t}\n\treturn ctx;\n}\n\nstatic void perf_unpin_context(struct perf_event_context *ctx)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&ctx->lock, flags);\n\t--ctx->pin_count;\n\traw_spin_unlock_irqrestore(&ctx->lock, flags);\n}\n\n/*\n * Update the record of the current time in a context.\n */\nstatic void update_context_time(struct perf_event_context *ctx)\n{\n\tu64 now = perf_clock();\n\n\tctx->time += now - ctx->timestamp;\n\tctx->timestamp = now;\n}\n\nstatic u64 perf_event_time(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\n\tif (is_cgroup_event(event))\n\t\treturn perf_cgroup_event_time(event);\n\n\treturn ctx ? ctx->time : 0;\n}\n\n/*\n * Update the total_time_enabled and total_time_running fields for a event.\n * The caller of this function needs to hold the ctx->lock.\n */\nstatic void update_event_times(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tu64 run_end;\n\n\tif (event->state < PERF_EVENT_STATE_INACTIVE ||\n\t    event->group_leader->state < PERF_EVENT_STATE_INACTIVE)\n\t\treturn;\n\t/*\n\t * in cgroup mode, time_enabled represents\n\t * the time the event was enabled AND active\n\t * tasks were in the monitored cgroup. This is\n\t * independent of the activity of the context as\n\t * there may be a mix of cgroup and non-cgroup events.\n\t *\n\t * That is why we treat cgroup events differently\n\t * here.\n\t */\n\tif (is_cgroup_event(event))\n\t\trun_end = perf_event_time(event);\n\telse if (ctx->is_active)\n\t\trun_end = ctx->time;\n\telse\n\t\trun_end = event->tstamp_stopped;\n\n\tevent->total_time_enabled = run_end - event->tstamp_enabled;\n\n\tif (event->state == PERF_EVENT_STATE_INACTIVE)\n\t\trun_end = event->tstamp_stopped;\n\telse\n\t\trun_end = perf_event_time(event);\n\n\tevent->total_time_running = run_end - event->tstamp_running;\n\n}\n\n/*\n * Update total_time_enabled and total_time_running for all events in a group.\n */\nstatic void update_group_times(struct perf_event *leader)\n{\n\tstruct perf_event *event;\n\n\tupdate_event_times(leader);\n\tlist_for_each_entry(event, &leader->sibling_list, group_entry)\n\t\tupdate_event_times(event);\n}\n\nstatic struct list_head *\nctx_group_list(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tif (event->attr.pinned)\n\t\treturn &ctx->pinned_groups;\n\telse\n\t\treturn &ctx->flexible_groups;\n}\n\n/*\n * Add a event from the lists for its context.\n * Must be called with ctx->mutex and ctx->lock held.\n */\nstatic void\nlist_add_event(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tWARN_ON_ONCE(event->attach_state & PERF_ATTACH_CONTEXT);\n\tevent->attach_state |= PERF_ATTACH_CONTEXT;\n\n\t/*\n\t * If we're a stand alone event or group leader, we go to the context\n\t * list, group events are kept attached to the group so that\n\t * perf_group_detach can, at all times, locate all siblings.\n\t */\n\tif (event->group_leader == event) {\n\t\tstruct list_head *list;\n\n\t\tif (is_software_event(event))\n\t\t\tevent->group_flags |= PERF_GROUP_SOFTWARE;\n\n\t\tlist = ctx_group_list(event, ctx);\n\t\tlist_add_tail(&event->group_entry, list);\n\t}\n\n\tif (is_cgroup_event(event))\n\t\tctx->nr_cgroups++;\n\n\tlist_add_rcu(&event->event_entry, &ctx->event_list);\n\tif (!ctx->nr_events)\n\t\tperf_pmu_rotate_start(ctx->pmu);\n\tctx->nr_events++;\n\tif (event->attr.inherit_stat)\n\t\tctx->nr_stat++;\n}\n\n/*\n * Called at perf_event creation and when events are attached/detached from a\n * group.\n */\nstatic void perf_event__read_size(struct perf_event *event)\n{\n\tint entry = sizeof(u64); /* value */\n\tint size = 0;\n\tint nr = 1;\n\n\tif (event->attr.read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tsize += sizeof(u64);\n\n\tif (event->attr.read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tsize += sizeof(u64);\n\n\tif (event->attr.read_format & PERF_FORMAT_ID)\n\t\tentry += sizeof(u64);\n\n\tif (event->attr.read_format & PERF_FORMAT_GROUP) {\n\t\tnr += event->group_leader->nr_siblings;\n\t\tsize += sizeof(u64);\n\t}\n\n\tsize += entry * nr;\n\tevent->read_size = size;\n}\n\nstatic void perf_event__header_size(struct perf_event *event)\n{\n\tstruct perf_sample_data *data;\n\tu64 sample_type = event->attr.sample_type;\n\tu16 size = 0;\n\n\tperf_event__read_size(event);\n\n\tif (sample_type & PERF_SAMPLE_IP)\n\t\tsize += sizeof(data->ip);\n\n\tif (sample_type & PERF_SAMPLE_ADDR)\n\t\tsize += sizeof(data->addr);\n\n\tif (sample_type & PERF_SAMPLE_PERIOD)\n\t\tsize += sizeof(data->period);\n\n\tif (sample_type & PERF_SAMPLE_READ)\n\t\tsize += event->read_size;\n\n\tevent->header_size = size;\n}\n\nstatic void perf_event__id_header_size(struct perf_event *event)\n{\n\tstruct perf_sample_data *data;\n\tu64 sample_type = event->attr.sample_type;\n\tu16 size = 0;\n\n\tif (sample_type & PERF_SAMPLE_TID)\n\t\tsize += sizeof(data->tid_entry);\n\n\tif (sample_type & PERF_SAMPLE_TIME)\n\t\tsize += sizeof(data->time);\n\n\tif (sample_type & PERF_SAMPLE_ID)\n\t\tsize += sizeof(data->id);\n\n\tif (sample_type & PERF_SAMPLE_STREAM_ID)\n\t\tsize += sizeof(data->stream_id);\n\n\tif (sample_type & PERF_SAMPLE_CPU)\n\t\tsize += sizeof(data->cpu_entry);\n\n\tevent->id_header_size = size;\n}\n\nstatic void perf_group_attach(struct perf_event *event)\n{\n\tstruct perf_event *group_leader = event->group_leader, *pos;\n\n\t/*\n\t * We can have double attach due to group movement in perf_event_open.\n\t */\n\tif (event->attach_state & PERF_ATTACH_GROUP)\n\t\treturn;\n\n\tevent->attach_state |= PERF_ATTACH_GROUP;\n\n\tif (group_leader == event)\n\t\treturn;\n\n\tif (group_leader->group_flags & PERF_GROUP_SOFTWARE &&\n\t\t\t!is_software_event(event))\n\t\tgroup_leader->group_flags &= ~PERF_GROUP_SOFTWARE;\n\n\tlist_add_tail(&event->group_entry, &group_leader->sibling_list);\n\tgroup_leader->nr_siblings++;\n\n\tperf_event__header_size(group_leader);\n\n\tlist_for_each_entry(pos, &group_leader->sibling_list, group_entry)\n\t\tperf_event__header_size(pos);\n}\n\n/*\n * Remove a event from the lists for its context.\n * Must be called with ctx->mutex and ctx->lock held.\n */\nstatic void\nlist_del_event(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tstruct perf_cpu_context *cpuctx;\n\t/*\n\t * We can have double detach due to exit/hot-unplug + close.\n\t */\n\tif (!(event->attach_state & PERF_ATTACH_CONTEXT))\n\t\treturn;\n\n\tevent->attach_state &= ~PERF_ATTACH_CONTEXT;\n\n\tif (is_cgroup_event(event)) {\n\t\tctx->nr_cgroups--;\n\t\tcpuctx = __get_cpu_context(ctx);\n\t\t/*\n\t\t * if there are no more cgroup events\n\t\t * then cler cgrp to avoid stale pointer\n\t\t * in update_cgrp_time_from_cpuctx()\n\t\t */\n\t\tif (!ctx->nr_cgroups)\n\t\t\tcpuctx->cgrp = NULL;\n\t}\n\n\tctx->nr_events--;\n\tif (event->attr.inherit_stat)\n\t\tctx->nr_stat--;\n\n\tlist_del_rcu(&event->event_entry);\n\n\tif (event->group_leader == event)\n\t\tlist_del_init(&event->group_entry);\n\n\tupdate_group_times(event);\n\n\t/*\n\t * If event was in error state, then keep it\n\t * that way, otherwise bogus counts will be\n\t * returned on read(). The only way to get out\n\t * of error state is by explicit re-enabling\n\t * of the event\n\t */\n\tif (event->state > PERF_EVENT_STATE_OFF)\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n}\n\nstatic void perf_group_detach(struct perf_event *event)\n{\n\tstruct perf_event *sibling, *tmp;\n\tstruct list_head *list = NULL;\n\n\t/*\n\t * We can have double detach due to exit/hot-unplug + close.\n\t */\n\tif (!(event->attach_state & PERF_ATTACH_GROUP))\n\t\treturn;\n\n\tevent->attach_state &= ~PERF_ATTACH_GROUP;\n\n\t/*\n\t * If this is a sibling, remove it from its group.\n\t */\n\tif (event->group_leader != event) {\n\t\tlist_del_init(&event->group_entry);\n\t\tevent->group_leader->nr_siblings--;\n\t\tgoto out;\n\t}\n\n\tif (!list_empty(&event->group_entry))\n\t\tlist = &event->group_entry;\n\n\t/*\n\t * If this was a group event with sibling events then\n\t * upgrade the siblings to singleton events by adding them\n\t * to whatever list we are on.\n\t */\n\tlist_for_each_entry_safe(sibling, tmp, &event->sibling_list, group_entry) {\n\t\tif (list)\n\t\t\tlist_move_tail(&sibling->group_entry, list);\n\t\tsibling->group_leader = sibling;\n\n\t\t/* Inherit group flags from the previous leader */\n\t\tsibling->group_flags = event->group_flags;\n\t}\n\nout:\n\tperf_event__header_size(event->group_leader);\n\n\tlist_for_each_entry(tmp, &event->group_leader->sibling_list, group_entry)\n\t\tperf_event__header_size(tmp);\n}\n\nstatic inline int\nevent_filter_match(struct perf_event *event)\n{\n\treturn (event->cpu == -1 || event->cpu == smp_processor_id())\n\t    && perf_cgroup_match(event);\n}\n\nstatic void\nevent_sched_out(struct perf_event *event,\n\t\t  struct perf_cpu_context *cpuctx,\n\t\t  struct perf_event_context *ctx)\n{\n\tu64 tstamp = perf_event_time(event);\n\tu64 delta;\n\t/*\n\t * An event which could not be activated because of\n\t * filter mismatch still needs to have its timings\n\t * maintained, otherwise bogus information is return\n\t * via read() for time_enabled, time_running:\n\t */\n\tif (event->state == PERF_EVENT_STATE_INACTIVE\n\t    && !event_filter_match(event)) {\n\t\tdelta = tstamp - event->tstamp_stopped;\n\t\tevent->tstamp_running += delta;\n\t\tevent->tstamp_stopped = tstamp;\n\t}\n\n\tif (event->state != PERF_EVENT_STATE_ACTIVE)\n\t\treturn;\n\n\tevent->state = PERF_EVENT_STATE_INACTIVE;\n\tif (event->pending_disable) {\n\t\tevent->pending_disable = 0;\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\t}\n\tevent->tstamp_stopped = tstamp;\n\tevent->pmu->del(event, 0);\n\tevent->oncpu = -1;\n\n\tif (!is_software_event(event))\n\t\tcpuctx->active_oncpu--;\n\tctx->nr_active--;\n\tif (event->attr.exclusive || !cpuctx->active_oncpu)\n\t\tcpuctx->exclusive = 0;\n}\n\nstatic void\ngroup_sched_out(struct perf_event *group_event,\n\t\tstruct perf_cpu_context *cpuctx,\n\t\tstruct perf_event_context *ctx)\n{\n\tstruct perf_event *event;\n\tint state = group_event->state;\n\n\tevent_sched_out(group_event, cpuctx, ctx);\n\n\t/*\n\t * Schedule out siblings (if any):\n\t */\n\tlist_for_each_entry(event, &group_event->sibling_list, group_entry)\n\t\tevent_sched_out(event, cpuctx, ctx);\n\n\tif (state == PERF_EVENT_STATE_ACTIVE && group_event->attr.exclusive)\n\t\tcpuctx->exclusive = 0;\n}\n\n/*\n * Cross CPU call to remove a performance event\n *\n * We disable the event on the hardware level first. After that we\n * remove it from the context list.\n */\nstatic int __perf_remove_from_context(void *info)\n{\n\tstruct perf_event *event = info;\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_cpu_context *cpuctx = __get_cpu_context(ctx);\n\n\traw_spin_lock(&ctx->lock);\n\tevent_sched_out(event, cpuctx, ctx);\n\tlist_del_event(event, ctx);\n\tif (!ctx->nr_events && cpuctx->task_ctx == ctx) {\n\t\tctx->is_active = 0;\n\t\tcpuctx->task_ctx = NULL;\n\t}\n\traw_spin_unlock(&ctx->lock);\n\n\treturn 0;\n}\n\n\n/*\n * Remove the event from a task's (or a CPU's) list of events.\n *\n * CPU events are removed with a smp call. For task events we only\n * call when the task is on a CPU.\n *\n * If event->ctx is a cloned context, callers must make sure that\n * every task struct that event->ctx->task could possibly point to\n * remains valid.  This is OK when called from perf_release since\n * that only calls us on the top-level context, which can't be a clone.\n * When called from perf_event_exit_task, it's OK because the\n * context has been detached from its task.\n */\nstatic void perf_remove_from_context(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct task_struct *task = ctx->task;\n\n\tlockdep_assert_held(&ctx->mutex);\n\n\tif (!task) {\n\t\t/*\n\t\t * Per cpu events are removed via an smp call and\n\t\t * the removal is always successful.\n\t\t */\n\t\tcpu_function_call(event->cpu, __perf_remove_from_context, event);\n\t\treturn;\n\t}\n\nretry:\n\tif (!task_function_call(task, __perf_remove_from_context, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\t/*\n\t * If we failed to find a running task, but find the context active now\n\t * that we've acquired the ctx->lock, retry.\n\t */\n\tif (ctx->is_active) {\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * Since the task isn't running, its safe to remove the event, us\n\t * holding the ctx->lock ensures the task won't get scheduled in.\n\t */\n\tlist_del_event(event, ctx);\n\traw_spin_unlock_irq(&ctx->lock);\n}\n\n/*\n * Cross CPU call to disable a performance event\n */\nstatic int __perf_event_disable(void *info)\n{\n\tstruct perf_event *event = info;\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_cpu_context *cpuctx = __get_cpu_context(ctx);\n\n\t/*\n\t * If this is a per-task event, need to check whether this\n\t * event's task is the current task on this cpu.\n\t *\n\t * Can trigger due to concurrent perf_event_context_sched_out()\n\t * flipping contexts around.\n\t */\n\tif (ctx->task && cpuctx->task_ctx != ctx)\n\t\treturn -EINVAL;\n\n\traw_spin_lock(&ctx->lock);\n\n\t/*\n\t * If the event is on, turn it off.\n\t * If it is in error state, leave it in error state.\n\t */\n\tif (event->state >= PERF_EVENT_STATE_INACTIVE) {\n\t\tupdate_context_time(ctx);\n\t\tupdate_cgrp_time_from_event(event);\n\t\tupdate_group_times(event);\n\t\tif (event == event->group_leader)\n\t\t\tgroup_sched_out(event, cpuctx, ctx);\n\t\telse\n\t\t\tevent_sched_out(event, cpuctx, ctx);\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\t}\n\n\traw_spin_unlock(&ctx->lock);\n\n\treturn 0;\n}\n\n/*\n * Disable a event.\n *\n * If event->ctx is a cloned context, callers must make sure that\n * every task struct that event->ctx->task could possibly point to\n * remains valid.  This condition is satisifed when called through\n * perf_event_for_each_child or perf_event_for_each because they\n * hold the top-level event's child_mutex, so any descendant that\n * goes to exit will block in sync_child_event.\n * When called from perf_pending_event it's OK because event->ctx\n * is the current context on this CPU and preemption is disabled,\n * hence we can't get into perf_event_task_sched_out for this context.\n */\nvoid perf_event_disable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct task_struct *task = ctx->task;\n\n\tif (!task) {\n\t\t/*\n\t\t * Disable the event on the cpu that it's on\n\t\t */\n\t\tcpu_function_call(event->cpu, __perf_event_disable, event);\n\t\treturn;\n\t}\n\nretry:\n\tif (!task_function_call(task, __perf_event_disable, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\t/*\n\t * If the event is still active, we need to retry the cross-call.\n\t */\n\tif (event->state == PERF_EVENT_STATE_ACTIVE) {\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\t/*\n\t\t * Reload the task pointer, it might have been changed by\n\t\t * a concurrent perf_event_context_sched_out().\n\t\t */\n\t\ttask = ctx->task;\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * Since we have the lock this context can't be scheduled\n\t * in, so we can change the state safely.\n\t */\n\tif (event->state == PERF_EVENT_STATE_INACTIVE) {\n\t\tupdate_group_times(event);\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\t}\n\traw_spin_unlock_irq(&ctx->lock);\n}\n\nstatic void perf_set_shadow_time(struct perf_event *event,\n\t\t\t\t struct perf_event_context *ctx,\n\t\t\t\t u64 tstamp)\n{\n\t/*\n\t * use the correct time source for the time snapshot\n\t *\n\t * We could get by without this by leveraging the\n\t * fact that to get to this function, the caller\n\t * has most likely already called update_context_time()\n\t * and update_cgrp_time_xx() and thus both timestamp\n\t * are identical (or very close). Given that tstamp is,\n\t * already adjusted for cgroup, we could say that:\n\t *    tstamp - ctx->timestamp\n\t * is equivalent to\n\t *    tstamp - cgrp->timestamp.\n\t *\n\t * Then, in perf_output_read(), the calculation would\n\t * work with no changes because:\n\t * - event is guaranteed scheduled in\n\t * - no scheduled out in between\n\t * - thus the timestamp would be the same\n\t *\n\t * But this is a bit hairy.\n\t *\n\t * So instead, we have an explicit cgroup call to remain\n\t * within the time time source all along. We believe it\n\t * is cleaner and simpler to understand.\n\t */\n\tif (is_cgroup_event(event))\n\t\tperf_cgroup_set_shadow_time(event, tstamp);\n\telse\n\t\tevent->shadow_ctx_time = tstamp - ctx->timestamp;\n}\n\n#define MAX_INTERRUPTS (~0ULL)\n\nstatic void perf_log_throttle(struct perf_event *event, int enable);\n\nstatic int\nevent_sched_in(struct perf_event *event,\n\t\t struct perf_cpu_context *cpuctx,\n\t\t struct perf_event_context *ctx)\n{\n\tu64 tstamp = perf_event_time(event);\n\n\tif (event->state <= PERF_EVENT_STATE_OFF)\n\t\treturn 0;\n\n\tevent->state = PERF_EVENT_STATE_ACTIVE;\n\tevent->oncpu = smp_processor_id();\n\n\t/*\n\t * Unthrottle events, since we scheduled we might have missed several\n\t * ticks already, also for a heavily scheduling task there is little\n\t * guarantee it'll get a tick in a timely manner.\n\t */\n\tif (unlikely(event->hw.interrupts == MAX_INTERRUPTS)) {\n\t\tperf_log_throttle(event, 1);\n\t\tevent->hw.interrupts = 0;\n\t}\n\n\t/*\n\t * The new state must be visible before we turn it on in the hardware:\n\t */\n\tsmp_wmb();\n\n\tif (event->pmu->add(event, PERF_EF_START)) {\n\t\tevent->state = PERF_EVENT_STATE_INACTIVE;\n\t\tevent->oncpu = -1;\n\t\treturn -EAGAIN;\n\t}\n\n\tevent->tstamp_running += tstamp - event->tstamp_stopped;\n\n\tperf_set_shadow_time(event, ctx, tstamp);\n\n\tif (!is_software_event(event))\n\t\tcpuctx->active_oncpu++;\n\tctx->nr_active++;\n\n\tif (event->attr.exclusive)\n\t\tcpuctx->exclusive = 1;\n\n\treturn 0;\n}\n\nstatic int\ngroup_sched_in(struct perf_event *group_event,\n\t       struct perf_cpu_context *cpuctx,\n\t       struct perf_event_context *ctx)\n{\n\tstruct perf_event *event, *partial_group = NULL;\n\tstruct pmu *pmu = group_event->pmu;\n\tu64 now = ctx->time;\n\tbool simulate = false;\n\n\tif (group_event->state == PERF_EVENT_STATE_OFF)\n\t\treturn 0;\n\n\tpmu->start_txn(pmu);\n\n\tif (event_sched_in(group_event, cpuctx, ctx)) {\n\t\tpmu->cancel_txn(pmu);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * Schedule in siblings as one group (if any):\n\t */\n\tlist_for_each_entry(event, &group_event->sibling_list, group_entry) {\n\t\tif (event_sched_in(event, cpuctx, ctx)) {\n\t\t\tpartial_group = event;\n\t\t\tgoto group_error;\n\t\t}\n\t}\n\n\tif (!pmu->commit_txn(pmu))\n\t\treturn 0;\n\ngroup_error:\n\t/*\n\t * Groups can be scheduled in as one unit only, so undo any\n\t * partial group before returning:\n\t * The events up to the failed event are scheduled out normally,\n\t * tstamp_stopped will be updated.\n\t *\n\t * The failed events and the remaining siblings need to have\n\t * their timings updated as if they had gone thru event_sched_in()\n\t * and event_sched_out(). This is required to get consistent timings\n\t * across the group. This also takes care of the case where the group\n\t * could never be scheduled by ensuring tstamp_stopped is set to mark\n\t * the time the event was actually stopped, such that time delta\n\t * calculation in update_event_times() is correct.\n\t */\n\tlist_for_each_entry(event, &group_event->sibling_list, group_entry) {\n\t\tif (event == partial_group)\n\t\t\tsimulate = true;\n\n\t\tif (simulate) {\n\t\t\tevent->tstamp_running += now - event->tstamp_stopped;\n\t\t\tevent->tstamp_stopped = now;\n\t\t} else {\n\t\t\tevent_sched_out(event, cpuctx, ctx);\n\t\t}\n\t}\n\tevent_sched_out(group_event, cpuctx, ctx);\n\n\tpmu->cancel_txn(pmu);\n\n\treturn -EAGAIN;\n}\n\n/*\n * Work out whether we can put this event group on the CPU now.\n */\nstatic int group_can_go_on(struct perf_event *event,\n\t\t\t   struct perf_cpu_context *cpuctx,\n\t\t\t   int can_add_hw)\n{\n\t/*\n\t * Groups consisting entirely of software events can always go on.\n\t */\n\tif (event->group_flags & PERF_GROUP_SOFTWARE)\n\t\treturn 1;\n\t/*\n\t * If an exclusive group is already on, no other hardware\n\t * events can go on.\n\t */\n\tif (cpuctx->exclusive)\n\t\treturn 0;\n\t/*\n\t * If this group is exclusive and there are already\n\t * events on the CPU, it can't go on.\n\t */\n\tif (event->attr.exclusive && cpuctx->active_oncpu)\n\t\treturn 0;\n\t/*\n\t * Otherwise, try to add it if all previous groups were able\n\t * to go on.\n\t */\n\treturn can_add_hw;\n}\n\nstatic void add_event_to_ctx(struct perf_event *event,\n\t\t\t       struct perf_event_context *ctx)\n{\n\tu64 tstamp = perf_event_time(event);\n\n\tlist_add_event(event, ctx);\n\tperf_group_attach(event);\n\tevent->tstamp_enabled = tstamp;\n\tevent->tstamp_running = tstamp;\n\tevent->tstamp_stopped = tstamp;\n}\n\nstatic void task_ctx_sched_out(struct perf_event_context *ctx);\nstatic void\nctx_sched_in(struct perf_event_context *ctx,\n\t     struct perf_cpu_context *cpuctx,\n\t     enum event_type_t event_type,\n\t     struct task_struct *task);\n\nstatic void perf_event_sched_in(struct perf_cpu_context *cpuctx,\n\t\t\t\tstruct perf_event_context *ctx,\n\t\t\t\tstruct task_struct *task)\n{\n\tcpu_ctx_sched_in(cpuctx, EVENT_PINNED, task);\n\tif (ctx)\n\t\tctx_sched_in(ctx, cpuctx, EVENT_PINNED, task);\n\tcpu_ctx_sched_in(cpuctx, EVENT_FLEXIBLE, task);\n\tif (ctx)\n\t\tctx_sched_in(ctx, cpuctx, EVENT_FLEXIBLE, task);\n}\n\n/*\n * Cross CPU call to install and enable a performance event\n *\n * Must be called with ctx->mutex held\n */\nstatic int  __perf_install_in_context(void *info)\n{\n\tstruct perf_event *event = info;\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_cpu_context *cpuctx = __get_cpu_context(ctx);\n\tstruct perf_event_context *task_ctx = cpuctx->task_ctx;\n\tstruct task_struct *task = current;\n\n\tperf_ctx_lock(cpuctx, task_ctx);\n\tperf_pmu_disable(cpuctx->ctx.pmu);\n\n\t/*\n\t * If there was an active task_ctx schedule it out.\n\t */\n\tif (task_ctx)\n\t\ttask_ctx_sched_out(task_ctx);\n\n\t/*\n\t * If the context we're installing events in is not the\n\t * active task_ctx, flip them.\n\t */\n\tif (ctx->task && task_ctx != ctx) {\n\t\tif (task_ctx)\n\t\t\traw_spin_unlock(&task_ctx->lock);\n\t\traw_spin_lock(&ctx->lock);\n\t\ttask_ctx = ctx;\n\t}\n\n\tif (task_ctx) {\n\t\tcpuctx->task_ctx = task_ctx;\n\t\ttask = task_ctx->task;\n\t}\n\n\tcpu_ctx_sched_out(cpuctx, EVENT_ALL);\n\n\tupdate_context_time(ctx);\n\t/*\n\t * update cgrp time only if current cgrp\n\t * matches event->cgrp. Must be done before\n\t * calling add_event_to_ctx()\n\t */\n\tupdate_cgrp_time_from_event(event);\n\n\tadd_event_to_ctx(event, ctx);\n\n\t/*\n\t * Schedule everything back in\n\t */\n\tperf_event_sched_in(cpuctx, task_ctx, task);\n\n\tperf_pmu_enable(cpuctx->ctx.pmu);\n\tperf_ctx_unlock(cpuctx, task_ctx);\n\n\treturn 0;\n}\n\n/*\n * Attach a performance event to a context\n *\n * First we add the event to the list with the hardware enable bit\n * in event->hw_config cleared.\n *\n * If the event is attached to a task which is on a CPU we use a smp\n * call to enable it in the task context. The task might have been\n * scheduled away, but we check this in the smp call again.\n */\nstatic void\nperf_install_in_context(struct perf_event_context *ctx,\n\t\t\tstruct perf_event *event,\n\t\t\tint cpu)\n{\n\tstruct task_struct *task = ctx->task;\n\n\tlockdep_assert_held(&ctx->mutex);\n\n\tevent->ctx = ctx;\n\n\tif (!task) {\n\t\t/*\n\t\t * Per cpu events are installed via an smp call and\n\t\t * the install is always successful.\n\t\t */\n\t\tcpu_function_call(cpu, __perf_install_in_context, event);\n\t\treturn;\n\t}\n\nretry:\n\tif (!task_function_call(task, __perf_install_in_context, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\t/*\n\t * If we failed to find a running task, but find the context active now\n\t * that we've acquired the ctx->lock, retry.\n\t */\n\tif (ctx->is_active) {\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * Since the task isn't running, its safe to add the event, us holding\n\t * the ctx->lock ensures the task won't get scheduled in.\n\t */\n\tadd_event_to_ctx(event, ctx);\n\traw_spin_unlock_irq(&ctx->lock);\n}\n\n/*\n * Put a event into inactive state and update time fields.\n * Enabling the leader of a group effectively enables all\n * the group members that aren't explicitly disabled, so we\n * have to update their ->tstamp_enabled also.\n * Note: this works for group members as well as group leaders\n * since the non-leader members' sibling_lists will be empty.\n */\nstatic void __perf_event_mark_enabled(struct perf_event *event,\n\t\t\t\t\tstruct perf_event_context *ctx)\n{\n\tstruct perf_event *sub;\n\tu64 tstamp = perf_event_time(event);\n\n\tevent->state = PERF_EVENT_STATE_INACTIVE;\n\tevent->tstamp_enabled = tstamp - event->total_time_enabled;\n\tlist_for_each_entry(sub, &event->sibling_list, group_entry) {\n\t\tif (sub->state >= PERF_EVENT_STATE_INACTIVE)\n\t\t\tsub->tstamp_enabled = tstamp - sub->total_time_enabled;\n\t}\n}\n\n/*\n * Cross CPU call to enable a performance event\n */\nstatic int __perf_event_enable(void *info)\n{\n\tstruct perf_event *event = info;\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *leader = event->group_leader;\n\tstruct perf_cpu_context *cpuctx = __get_cpu_context(ctx);\n\tint err;\n\n\tif (WARN_ON_ONCE(!ctx->is_active))\n\t\treturn -EINVAL;\n\n\traw_spin_lock(&ctx->lock);\n\tupdate_context_time(ctx);\n\n\tif (event->state >= PERF_EVENT_STATE_INACTIVE)\n\t\tgoto unlock;\n\n\t/*\n\t * set current task's cgroup time reference point\n\t */\n\tperf_cgroup_set_timestamp(current, ctx);\n\n\t__perf_event_mark_enabled(event, ctx);\n\n\tif (!event_filter_match(event)) {\n\t\tif (is_cgroup_event(event))\n\t\t\tperf_cgroup_defer_enabled(event);\n\t\tgoto unlock;\n\t}\n\n\t/*\n\t * If the event is in a group and isn't the group leader,\n\t * then don't put it on unless the group is on.\n\t */\n\tif (leader != event && leader->state != PERF_EVENT_STATE_ACTIVE)\n\t\tgoto unlock;\n\n\tif (!group_can_go_on(event, cpuctx, 1)) {\n\t\terr = -EEXIST;\n\t} else {\n\t\tif (event == leader)\n\t\t\terr = group_sched_in(event, cpuctx, ctx);\n\t\telse\n\t\t\terr = event_sched_in(event, cpuctx, ctx);\n\t}\n\n\tif (err) {\n\t\t/*\n\t\t * If this event can't go on and it's part of a\n\t\t * group, then the whole group has to come off.\n\t\t */\n\t\tif (leader != event)\n\t\t\tgroup_sched_out(leader, cpuctx, ctx);\n\t\tif (leader->attr.pinned) {\n\t\t\tupdate_group_times(leader);\n\t\t\tleader->state = PERF_EVENT_STATE_ERROR;\n\t\t}\n\t}\n\nunlock:\n\traw_spin_unlock(&ctx->lock);\n\n\treturn 0;\n}\n\n/*\n * Enable a event.\n *\n * If event->ctx is a cloned context, callers must make sure that\n * every task struct that event->ctx->task could possibly point to\n * remains valid.  This condition is satisfied when called through\n * perf_event_for_each_child or perf_event_for_each as described\n * for perf_event_disable.\n */\nvoid perf_event_enable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct task_struct *task = ctx->task;\n\n\tif (!task) {\n\t\t/*\n\t\t * Enable the event on the cpu that it's on\n\t\t */\n\t\tcpu_function_call(event->cpu, __perf_event_enable, event);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irq(&ctx->lock);\n\tif (event->state >= PERF_EVENT_STATE_INACTIVE)\n\t\tgoto out;\n\n\t/*\n\t * If the event is in error state, clear that first.\n\t * That way, if we see the event in error state below, we\n\t * know that it has gone back into error state, as distinct\n\t * from the task having been scheduled away before the\n\t * cross-call arrived.\n\t */\n\tif (event->state == PERF_EVENT_STATE_ERROR)\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\nretry:\n\tif (!ctx->is_active) {\n\t\t__perf_event_mark_enabled(event, ctx);\n\t\tgoto out;\n\t}\n\n\traw_spin_unlock_irq(&ctx->lock);\n\n\tif (!task_function_call(task, __perf_event_enable, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\n\t/*\n\t * If the context is active and the event is still off,\n\t * we need to retry the cross-call.\n\t */\n\tif (ctx->is_active && event->state == PERF_EVENT_STATE_OFF) {\n\t\t/*\n\t\t * task could have been flipped by a concurrent\n\t\t * perf_event_context_sched_out()\n\t\t */\n\t\ttask = ctx->task;\n\t\tgoto retry;\n\t}\n\nout:\n\traw_spin_unlock_irq(&ctx->lock);\n}\n\nstatic int perf_event_refresh(struct perf_event *event, int refresh)\n{\n\t/*\n\t * not supported on inherited events\n\t */\n\tif (event->attr.inherit || !is_sampling_event(event))\n\t\treturn -EINVAL;\n\n\tatomic_add(refresh, &event->event_limit);\n\tperf_event_enable(event);\n\n\treturn 0;\n}\n\nstatic void ctx_sched_out(struct perf_event_context *ctx,\n\t\t\t  struct perf_cpu_context *cpuctx,\n\t\t\t  enum event_type_t event_type)\n{\n\tstruct perf_event *event;\n\tint is_active = ctx->is_active;\n\n\tctx->is_active &= ~event_type;\n\tif (likely(!ctx->nr_events))\n\t\treturn;\n\n\tupdate_context_time(ctx);\n\tupdate_cgrp_time_from_cpuctx(cpuctx);\n\tif (!ctx->nr_active)\n\t\treturn;\n\n\tperf_pmu_disable(ctx->pmu);\n\tif ((is_active & EVENT_PINNED) && (event_type & EVENT_PINNED)) {\n\t\tlist_for_each_entry(event, &ctx->pinned_groups, group_entry)\n\t\t\tgroup_sched_out(event, cpuctx, ctx);\n\t}\n\n\tif ((is_active & EVENT_FLEXIBLE) && (event_type & EVENT_FLEXIBLE)) {\n\t\tlist_for_each_entry(event, &ctx->flexible_groups, group_entry)\n\t\t\tgroup_sched_out(event, cpuctx, ctx);\n\t}\n\tperf_pmu_enable(ctx->pmu);\n}\n\n/*\n * Test whether two contexts are equivalent, i.e. whether they\n * have both been cloned from the same version of the same context\n * and they both have the same number of enabled events.\n * If the number of enabled events is the same, then the set\n * of enabled events should be the same, because these are both\n * inherited contexts, therefore we can't access individual events\n * in them directly with an fd; we can only enable/disable all\n * events via prctl, or enable/disable all events in a family\n * via ioctl, which will have the same effect on both contexts.\n */\nstatic int context_equiv(struct perf_event_context *ctx1,\n\t\t\t struct perf_event_context *ctx2)\n{\n\treturn ctx1->parent_ctx && ctx1->parent_ctx == ctx2->parent_ctx\n\t\t&& ctx1->parent_gen == ctx2->parent_gen\n\t\t&& !ctx1->pin_count && !ctx2->pin_count;\n}\n\nstatic void __perf_event_sync_stat(struct perf_event *event,\n\t\t\t\t     struct perf_event *next_event)\n{\n\tu64 value;\n\n\tif (!event->attr.inherit_stat)\n\t\treturn;\n\n\t/*\n\t * Update the event value, we cannot use perf_event_read()\n\t * because we're in the middle of a context switch and have IRQs\n\t * disabled, which upsets smp_call_function_single(), however\n\t * we know the event must be on the current CPU, therefore we\n\t * don't need to use it.\n\t */\n\tswitch (event->state) {\n\tcase PERF_EVENT_STATE_ACTIVE:\n\t\tevent->pmu->read(event);\n\t\t/* fall-through */\n\n\tcase PERF_EVENT_STATE_INACTIVE:\n\t\tupdate_event_times(event);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/*\n\t * In order to keep per-task stats reliable we need to flip the event\n\t * values when we flip the contexts.\n\t */\n\tvalue = local64_read(&next_event->count);\n\tvalue = local64_xchg(&event->count, value);\n\tlocal64_set(&next_event->count, value);\n\n\tswap(event->total_time_enabled, next_event->total_time_enabled);\n\tswap(event->total_time_running, next_event->total_time_running);\n\n\t/*\n\t * Since we swizzled the values, update the user visible data too.\n\t */\n\tperf_event_update_userpage(event);\n\tperf_event_update_userpage(next_event);\n}\n\n#define list_next_entry(pos, member) \\\n\tlist_entry(pos->member.next, typeof(*pos), member)\n\nstatic void perf_event_sync_stat(struct perf_event_context *ctx,\n\t\t\t\t   struct perf_event_context *next_ctx)\n{\n\tstruct perf_event *event, *next_event;\n\n\tif (!ctx->nr_stat)\n\t\treturn;\n\n\tupdate_context_time(ctx);\n\n\tevent = list_first_entry(&ctx->event_list,\n\t\t\t\t   struct perf_event, event_entry);\n\n\tnext_event = list_first_entry(&next_ctx->event_list,\n\t\t\t\t\tstruct perf_event, event_entry);\n\n\twhile (&event->event_entry != &ctx->event_list &&\n\t       &next_event->event_entry != &next_ctx->event_list) {\n\n\t\t__perf_event_sync_stat(event, next_event);\n\n\t\tevent = list_next_entry(event, event_entry);\n\t\tnext_event = list_next_entry(next_event, event_entry);\n\t}\n}\n\nstatic void perf_event_context_sched_out(struct task_struct *task, int ctxn,\n\t\t\t\t\t struct task_struct *next)\n{\n\tstruct perf_event_context *ctx = task->perf_event_ctxp[ctxn];\n\tstruct perf_event_context *next_ctx;\n\tstruct perf_event_context *parent;\n\tstruct perf_cpu_context *cpuctx;\n\tint do_switch = 1;\n\n\tif (likely(!ctx))\n\t\treturn;\n\n\tcpuctx = __get_cpu_context(ctx);\n\tif (!cpuctx->task_ctx)\n\t\treturn;\n\n\trcu_read_lock();\n\tparent = rcu_dereference(ctx->parent_ctx);\n\tnext_ctx = next->perf_event_ctxp[ctxn];\n\tif (parent && next_ctx &&\n\t    rcu_dereference(next_ctx->parent_ctx) == parent) {\n\t\t/*\n\t\t * Looks like the two contexts are clones, so we might be\n\t\t * able to optimize the context switch.  We lock both\n\t\t * contexts and check that they are clones under the\n\t\t * lock (including re-checking that neither has been\n\t\t * uncloned in the meantime).  It doesn't matter which\n\t\t * order we take the locks because no other cpu could\n\t\t * be trying to lock both of these tasks.\n\t\t */\n\t\traw_spin_lock(&ctx->lock);\n\t\traw_spin_lock_nested(&next_ctx->lock, SINGLE_DEPTH_NESTING);\n\t\tif (context_equiv(ctx, next_ctx)) {\n\t\t\t/*\n\t\t\t * XXX do we need a memory barrier of sorts\n\t\t\t * wrt to rcu_dereference() of perf_event_ctxp\n\t\t\t */\n\t\t\ttask->perf_event_ctxp[ctxn] = next_ctx;\n\t\t\tnext->perf_event_ctxp[ctxn] = ctx;\n\t\t\tctx->task = next;\n\t\t\tnext_ctx->task = task;\n\t\t\tdo_switch = 0;\n\n\t\t\tperf_event_sync_stat(ctx, next_ctx);\n\t\t}\n\t\traw_spin_unlock(&next_ctx->lock);\n\t\traw_spin_unlock(&ctx->lock);\n\t}\n\trcu_read_unlock();\n\n\tif (do_switch) {\n\t\traw_spin_lock(&ctx->lock);\n\t\tctx_sched_out(ctx, cpuctx, EVENT_ALL);\n\t\tcpuctx->task_ctx = NULL;\n\t\traw_spin_unlock(&ctx->lock);\n\t}\n}\n\n#define for_each_task_context_nr(ctxn)\t\t\t\t\t\\\n\tfor ((ctxn) = 0; (ctxn) < perf_nr_task_contexts; (ctxn)++)\n\n/*\n * Called from scheduler to remove the events of the current task,\n * with interrupts disabled.\n *\n * We stop each event and update the event value in event->count.\n *\n * This does not protect us against NMI, but disable()\n * sets the disabled bit in the control field of event _before_\n * accessing the event control register. If a NMI hits, then it will\n * not restart the event.\n */\nvoid __perf_event_task_sched_out(struct task_struct *task,\n\t\t\t\t struct task_struct *next)\n{\n\tint ctxn;\n\n\tfor_each_task_context_nr(ctxn)\n\t\tperf_event_context_sched_out(task, ctxn, next);\n\n\t/*\n\t * if cgroup events exist on this CPU, then we need\n\t * to check if we have to switch out PMU state.\n\t * cgroup event are system-wide mode only\n\t */\n\tif (atomic_read(&__get_cpu_var(perf_cgroup_events)))\n\t\tperf_cgroup_sched_out(task);\n}\n\nstatic void task_ctx_sched_out(struct perf_event_context *ctx)\n{\n\tstruct perf_cpu_context *cpuctx = __get_cpu_context(ctx);\n\n\tif (!cpuctx->task_ctx)\n\t\treturn;\n\n\tif (WARN_ON_ONCE(ctx != cpuctx->task_ctx))\n\t\treturn;\n\n\tctx_sched_out(ctx, cpuctx, EVENT_ALL);\n\tcpuctx->task_ctx = NULL;\n}\n\n/*\n * Called with IRQs disabled\n */\nstatic void cpu_ctx_sched_out(struct perf_cpu_context *cpuctx,\n\t\t\t      enum event_type_t event_type)\n{\n\tctx_sched_out(&cpuctx->ctx, cpuctx, event_type);\n}\n\nstatic void\nctx_pinned_sched_in(struct perf_event_context *ctx,\n\t\t    struct perf_cpu_context *cpuctx)\n{\n\tstruct perf_event *event;\n\n\tlist_for_each_entry(event, &ctx->pinned_groups, group_entry) {\n\t\tif (event->state <= PERF_EVENT_STATE_OFF)\n\t\t\tcontinue;\n\t\tif (!event_filter_match(event))\n\t\t\tcontinue;\n\n\t\t/* may need to reset tstamp_enabled */\n\t\tif (is_cgroup_event(event))\n\t\t\tperf_cgroup_mark_enabled(event, ctx);\n\n\t\tif (group_can_go_on(event, cpuctx, 1))\n\t\t\tgroup_sched_in(event, cpuctx, ctx);\n\n\t\t/*\n\t\t * If this pinned group hasn't been scheduled,\n\t\t * put it in error state.\n\t\t */\n\t\tif (event->state == PERF_EVENT_STATE_INACTIVE) {\n\t\t\tupdate_group_times(event);\n\t\t\tevent->state = PERF_EVENT_STATE_ERROR;\n\t\t}\n\t}\n}\n\nstatic void\nctx_flexible_sched_in(struct perf_event_context *ctx,\n\t\t      struct perf_cpu_context *cpuctx)\n{\n\tstruct perf_event *event;\n\tint can_add_hw = 1;\n\n\tlist_for_each_entry(event, &ctx->flexible_groups, group_entry) {\n\t\t/* Ignore events in OFF or ERROR state */\n\t\tif (event->state <= PERF_EVENT_STATE_OFF)\n\t\t\tcontinue;\n\t\t/*\n\t\t * Listen to the 'cpu' scheduling filter constraint\n\t\t * of events:\n\t\t */\n\t\tif (!event_filter_match(event))\n\t\t\tcontinue;\n\n\t\t/* may need to reset tstamp_enabled */\n\t\tif (is_cgroup_event(event))\n\t\t\tperf_cgroup_mark_enabled(event, ctx);\n\n\t\tif (group_can_go_on(event, cpuctx, can_add_hw)) {\n\t\t\tif (group_sched_in(event, cpuctx, ctx))\n\t\t\t\tcan_add_hw = 0;\n\t\t}\n\t}\n}\n\nstatic void\nctx_sched_in(struct perf_event_context *ctx,\n\t     struct perf_cpu_context *cpuctx,\n\t     enum event_type_t event_type,\n\t     struct task_struct *task)\n{\n\tu64 now;\n\tint is_active = ctx->is_active;\n\n\tctx->is_active |= event_type;\n\tif (likely(!ctx->nr_events))\n\t\treturn;\n\n\tnow = perf_clock();\n\tctx->timestamp = now;\n\tperf_cgroup_set_timestamp(task, ctx);\n\t/*\n\t * First go through the list and put on any pinned groups\n\t * in order to give them the best chance of going on.\n\t */\n\tif (!(is_active & EVENT_PINNED) && (event_type & EVENT_PINNED))\n\t\tctx_pinned_sched_in(ctx, cpuctx);\n\n\t/* Then walk through the lower prio flexible groups */\n\tif (!(is_active & EVENT_FLEXIBLE) && (event_type & EVENT_FLEXIBLE))\n\t\tctx_flexible_sched_in(ctx, cpuctx);\n}\n\nstatic void cpu_ctx_sched_in(struct perf_cpu_context *cpuctx,\n\t\t\t     enum event_type_t event_type,\n\t\t\t     struct task_struct *task)\n{\n\tstruct perf_event_context *ctx = &cpuctx->ctx;\n\n\tctx_sched_in(ctx, cpuctx, event_type, task);\n}\n\nstatic void perf_event_context_sched_in(struct perf_event_context *ctx,\n\t\t\t\t\tstruct task_struct *task)\n{\n\tstruct perf_cpu_context *cpuctx;\n\n\tcpuctx = __get_cpu_context(ctx);\n\tif (cpuctx->task_ctx == ctx)\n\t\treturn;\n\n\tperf_ctx_lock(cpuctx, ctx);\n\tperf_pmu_disable(ctx->pmu);\n\t/*\n\t * We want to keep the following priority order:\n\t * cpu pinned (that don't need to move), task pinned,\n\t * cpu flexible, task flexible.\n\t */\n\tcpu_ctx_sched_out(cpuctx, EVENT_FLEXIBLE);\n\n\tperf_event_sched_in(cpuctx, ctx, task);\n\n\tcpuctx->task_ctx = ctx;\n\n\tperf_pmu_enable(ctx->pmu);\n\tperf_ctx_unlock(cpuctx, ctx);\n\n\t/*\n\t * Since these rotations are per-cpu, we need to ensure the\n\t * cpu-context we got scheduled on is actually rotating.\n\t */\n\tperf_pmu_rotate_start(ctx->pmu);\n}\n\n/*\n * Called from scheduler to add the events of the current task\n * with interrupts disabled.\n *\n * We restore the event value and then enable it.\n *\n * This does not protect us against NMI, but enable()\n * sets the enabled bit in the control field of event _before_\n * accessing the event control register. If a NMI hits, then it will\n * keep the event running.\n */\nvoid __perf_event_task_sched_in(struct task_struct *task)\n{\n\tstruct perf_event_context *ctx;\n\tint ctxn;\n\n\tfor_each_task_context_nr(ctxn) {\n\t\tctx = task->perf_event_ctxp[ctxn];\n\t\tif (likely(!ctx))\n\t\t\tcontinue;\n\n\t\tperf_event_context_sched_in(ctx, task);\n\t}\n\t/*\n\t * if cgroup events exist on this CPU, then we need\n\t * to check if we have to switch in PMU state.\n\t * cgroup event are system-wide mode only\n\t */\n\tif (atomic_read(&__get_cpu_var(perf_cgroup_events)))\n\t\tperf_cgroup_sched_in(task);\n}\n\nstatic u64 perf_calculate_period(struct perf_event *event, u64 nsec, u64 count)\n{\n\tu64 frequency = event->attr.sample_freq;\n\tu64 sec = NSEC_PER_SEC;\n\tu64 divisor, dividend;\n\n\tint count_fls, nsec_fls, frequency_fls, sec_fls;\n\n\tcount_fls = fls64(count);\n\tnsec_fls = fls64(nsec);\n\tfrequency_fls = fls64(frequency);\n\tsec_fls = 30;\n\n\t/*\n\t * We got @count in @nsec, with a target of sample_freq HZ\n\t * the target period becomes:\n\t *\n\t *             @count * 10^9\n\t * period = -------------------\n\t *          @nsec * sample_freq\n\t *\n\t */\n\n\t/*\n\t * Reduce accuracy by one bit such that @a and @b converge\n\t * to a similar magnitude.\n\t */\n#define REDUCE_FLS(a, b)\t\t\\\ndo {\t\t\t\t\t\\\n\tif (a##_fls > b##_fls) {\t\\\n\t\ta >>= 1;\t\t\\\n\t\ta##_fls--;\t\t\\\n\t} else {\t\t\t\\\n\t\tb >>= 1;\t\t\\\n\t\tb##_fls--;\t\t\\\n\t}\t\t\t\t\\\n} while (0)\n\n\t/*\n\t * Reduce accuracy until either term fits in a u64, then proceed with\n\t * the other, so that finally we can do a u64/u64 division.\n\t */\n\twhile (count_fls + sec_fls > 64 && nsec_fls + frequency_fls > 64) {\n\t\tREDUCE_FLS(nsec, frequency);\n\t\tREDUCE_FLS(sec, count);\n\t}\n\n\tif (count_fls + sec_fls > 64) {\n\t\tdivisor = nsec * frequency;\n\n\t\twhile (count_fls + sec_fls > 64) {\n\t\t\tREDUCE_FLS(count, sec);\n\t\t\tdivisor >>= 1;\n\t\t}\n\n\t\tdividend = count * sec;\n\t} else {\n\t\tdividend = count * sec;\n\n\t\twhile (nsec_fls + frequency_fls > 64) {\n\t\t\tREDUCE_FLS(nsec, frequency);\n\t\t\tdividend >>= 1;\n\t\t}\n\n\t\tdivisor = nsec * frequency;\n\t}\n\n\tif (!divisor)\n\t\treturn dividend;\n\n\treturn div64_u64(dividend, divisor);\n}\n\nstatic void perf_adjust_period(struct perf_event *event, u64 nsec, u64 count)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\ts64 period, sample_period;\n\ts64 delta;\n\n\tperiod = perf_calculate_period(event, nsec, count);\n\n\tdelta = (s64)(period - hwc->sample_period);\n\tdelta = (delta + 7) / 8; /* low pass filter */\n\n\tsample_period = hwc->sample_period + delta;\n\n\tif (!sample_period)\n\t\tsample_period = 1;\n\n\thwc->sample_period = sample_period;\n\n\tif (local64_read(&hwc->period_left) > 8*sample_period) {\n\t\tevent->pmu->stop(event, PERF_EF_UPDATE);\n\t\tlocal64_set(&hwc->period_left, 0);\n\t\tevent->pmu->start(event, PERF_EF_RELOAD);\n\t}\n}\n\nstatic void perf_ctx_adjust_freq(struct perf_event_context *ctx, u64 period)\n{\n\tstruct perf_event *event;\n\tstruct hw_perf_event *hwc;\n\tu64 interrupts, now;\n\ts64 delta;\n\n\tlist_for_each_entry_rcu(event, &ctx->event_list, event_entry) {\n\t\tif (event->state != PERF_EVENT_STATE_ACTIVE)\n\t\t\tcontinue;\n\n\t\tif (!event_filter_match(event))\n\t\t\tcontinue;\n\n\t\thwc = &event->hw;\n\n\t\tinterrupts = hwc->interrupts;\n\t\thwc->interrupts = 0;\n\n\t\t/*\n\t\t * unthrottle events on the tick\n\t\t */\n\t\tif (interrupts == MAX_INTERRUPTS) {\n\t\t\tperf_log_throttle(event, 1);\n\t\t\tevent->pmu->start(event, 0);\n\t\t}\n\n\t\tif (!event->attr.freq || !event->attr.sample_freq)\n\t\t\tcontinue;\n\n\t\tevent->pmu->read(event);\n\t\tnow = local64_read(&event->count);\n\t\tdelta = now - hwc->freq_count_stamp;\n\t\thwc->freq_count_stamp = now;\n\n\t\tif (delta > 0)\n\t\t\tperf_adjust_period(event, period, delta);\n\t}\n}\n\n/*\n * Round-robin a context's events:\n */\nstatic void rotate_ctx(struct perf_event_context *ctx)\n{\n\t/*\n\t * Rotate the first entry last of non-pinned groups. Rotation might be\n\t * disabled by the inheritance code.\n\t */\n\tif (!ctx->rotate_disable)\n\t\tlist_rotate_left(&ctx->flexible_groups);\n}\n\n/*\n * perf_pmu_rotate_start() and perf_rotate_context() are fully serialized\n * because they're strictly cpu affine and rotate_start is called with IRQs\n * disabled, while rotate_context is called from IRQ context.\n */\nstatic void perf_rotate_context(struct perf_cpu_context *cpuctx)\n{\n\tu64 interval = (u64)cpuctx->jiffies_interval * TICK_NSEC;\n\tstruct perf_event_context *ctx = NULL;\n\tint rotate = 0, remove = 1;\n\n\tif (cpuctx->ctx.nr_events) {\n\t\tremove = 0;\n\t\tif (cpuctx->ctx.nr_events != cpuctx->ctx.nr_active)\n\t\t\trotate = 1;\n\t}\n\n\tctx = cpuctx->task_ctx;\n\tif (ctx && ctx->nr_events) {\n\t\tremove = 0;\n\t\tif (ctx->nr_events != ctx->nr_active)\n\t\t\trotate = 1;\n\t}\n\n\tperf_ctx_lock(cpuctx, cpuctx->task_ctx);\n\tperf_pmu_disable(cpuctx->ctx.pmu);\n\tperf_ctx_adjust_freq(&cpuctx->ctx, interval);\n\tif (ctx)\n\t\tperf_ctx_adjust_freq(ctx, interval);\n\n\tif (!rotate)\n\t\tgoto done;\n\n\tcpu_ctx_sched_out(cpuctx, EVENT_FLEXIBLE);\n\tif (ctx)\n\t\tctx_sched_out(ctx, cpuctx, EVENT_FLEXIBLE);\n\n\trotate_ctx(&cpuctx->ctx);\n\tif (ctx)\n\t\trotate_ctx(ctx);\n\n\tperf_event_sched_in(cpuctx, ctx, current);\n\ndone:\n\tif (remove)\n\t\tlist_del_init(&cpuctx->rotation_list);\n\n\tperf_pmu_enable(cpuctx->ctx.pmu);\n\tperf_ctx_unlock(cpuctx, cpuctx->task_ctx);\n}\n\nvoid perf_event_task_tick(void)\n{\n\tstruct list_head *head = &__get_cpu_var(rotation_list);\n\tstruct perf_cpu_context *cpuctx, *tmp;\n\n\tWARN_ON(!irqs_disabled());\n\n\tlist_for_each_entry_safe(cpuctx, tmp, head, rotation_list) {\n\t\tif (cpuctx->jiffies_interval == 1 ||\n\t\t\t\t!(jiffies % cpuctx->jiffies_interval))\n\t\t\tperf_rotate_context(cpuctx);\n\t}\n}\n\nstatic int event_enable_on_exec(struct perf_event *event,\n\t\t\t\tstruct perf_event_context *ctx)\n{\n\tif (!event->attr.enable_on_exec)\n\t\treturn 0;\n\n\tevent->attr.enable_on_exec = 0;\n\tif (event->state >= PERF_EVENT_STATE_INACTIVE)\n\t\treturn 0;\n\n\t__perf_event_mark_enabled(event, ctx);\n\n\treturn 1;\n}\n\n/*\n * Enable all of a task's events that have been marked enable-on-exec.\n * This expects task == current.\n */\nstatic void perf_event_enable_on_exec(struct perf_event_context *ctx)\n{\n\tstruct perf_event *event;\n\tunsigned long flags;\n\tint enabled = 0;\n\tint ret;\n\n\tlocal_irq_save(flags);\n\tif (!ctx || !ctx->nr_events)\n\t\tgoto out;\n\n\t/*\n\t * We must ctxsw out cgroup events to avoid conflict\n\t * when invoking perf_task_event_sched_in() later on\n\t * in this function. Otherwise we end up trying to\n\t * ctxswin cgroup events which are already scheduled\n\t * in.\n\t */\n\tperf_cgroup_sched_out(current);\n\n\traw_spin_lock(&ctx->lock);\n\ttask_ctx_sched_out(ctx);\n\n\tlist_for_each_entry(event, &ctx->pinned_groups, group_entry) {\n\t\tret = event_enable_on_exec(event, ctx);\n\t\tif (ret)\n\t\t\tenabled = 1;\n\t}\n\n\tlist_for_each_entry(event, &ctx->flexible_groups, group_entry) {\n\t\tret = event_enable_on_exec(event, ctx);\n\t\tif (ret)\n\t\t\tenabled = 1;\n\t}\n\n\t/*\n\t * Unclone this context if we enabled any event.\n\t */\n\tif (enabled)\n\t\tunclone_ctx(ctx);\n\n\traw_spin_unlock(&ctx->lock);\n\n\t/*\n\t * Also calls ctxswin for cgroup events, if any:\n\t */\n\tperf_event_context_sched_in(ctx, ctx->task);\nout:\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Cross CPU call to read the hardware event\n */\nstatic void __perf_event_read(void *info)\n{\n\tstruct perf_event *event = info;\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_cpu_context *cpuctx = __get_cpu_context(ctx);\n\n\t/*\n\t * If this is a task context, we need to check whether it is\n\t * the current task context of this cpu.  If not it has been\n\t * scheduled out before the smp call arrived.  In that case\n\t * event->count would have been updated to a recent sample\n\t * when the event was scheduled out.\n\t */\n\tif (ctx->task && cpuctx->task_ctx != ctx)\n\t\treturn;\n\n\traw_spin_lock(&ctx->lock);\n\tif (ctx->is_active) {\n\t\tupdate_context_time(ctx);\n\t\tupdate_cgrp_time_from_event(event);\n\t}\n\tupdate_event_times(event);\n\tif (event->state == PERF_EVENT_STATE_ACTIVE)\n\t\tevent->pmu->read(event);\n\traw_spin_unlock(&ctx->lock);\n}\n\nstatic inline u64 perf_event_count(struct perf_event *event)\n{\n\treturn local64_read(&event->count) + atomic64_read(&event->child_count);\n}\n\nstatic u64 perf_event_read(struct perf_event *event)\n{\n\t/*\n\t * If event is enabled and currently active on a CPU, update the\n\t * value in the event structure:\n\t */\n\tif (event->state == PERF_EVENT_STATE_ACTIVE) {\n\t\tsmp_call_function_single(event->oncpu,\n\t\t\t\t\t __perf_event_read, event, 1);\n\t} else if (event->state == PERF_EVENT_STATE_INACTIVE) {\n\t\tstruct perf_event_context *ctx = event->ctx;\n\t\tunsigned long flags;\n\n\t\traw_spin_lock_irqsave(&ctx->lock, flags);\n\t\t/*\n\t\t * may read while context is not active\n\t\t * (e.g., thread is blocked), in that case\n\t\t * we cannot update context time\n\t\t */\n\t\tif (ctx->is_active) {\n\t\t\tupdate_context_time(ctx);\n\t\t\tupdate_cgrp_time_from_event(event);\n\t\t}\n\t\tupdate_event_times(event);\n\t\traw_spin_unlock_irqrestore(&ctx->lock, flags);\n\t}\n\n\treturn perf_event_count(event);\n}\n\n/*\n * Callchain support\n */\n\nstruct callchain_cpus_entries {\n\tstruct rcu_head\t\t\trcu_head;\n\tstruct perf_callchain_entry\t*cpu_entries[0];\n};\n\nstatic DEFINE_PER_CPU(int, callchain_recursion[PERF_NR_CONTEXTS]);\nstatic atomic_t nr_callchain_events;\nstatic DEFINE_MUTEX(callchain_mutex);\nstruct callchain_cpus_entries *callchain_cpus_entries;\n\n\n__weak void perf_callchain_kernel(struct perf_callchain_entry *entry,\n\t\t\t\t  struct pt_regs *regs)\n{\n}\n\n__weak void perf_callchain_user(struct perf_callchain_entry *entry,\n\t\t\t\tstruct pt_regs *regs)\n{\n}\n\nstatic void release_callchain_buffers_rcu(struct rcu_head *head)\n{\n\tstruct callchain_cpus_entries *entries;\n\tint cpu;\n\n\tentries = container_of(head, struct callchain_cpus_entries, rcu_head);\n\n\tfor_each_possible_cpu(cpu)\n\t\tkfree(entries->cpu_entries[cpu]);\n\n\tkfree(entries);\n}\n\nstatic void release_callchain_buffers(void)\n{\n\tstruct callchain_cpus_entries *entries;\n\n\tentries = callchain_cpus_entries;\n\trcu_assign_pointer(callchain_cpus_entries, NULL);\n\tcall_rcu(&entries->rcu_head, release_callchain_buffers_rcu);\n}\n\nstatic int alloc_callchain_buffers(void)\n{\n\tint cpu;\n\tint size;\n\tstruct callchain_cpus_entries *entries;\n\n\t/*\n\t * We can't use the percpu allocation API for data that can be\n\t * accessed from NMI. Use a temporary manual per cpu allocation\n\t * until that gets sorted out.\n\t */\n\tsize = offsetof(struct callchain_cpus_entries, cpu_entries[nr_cpu_ids]);\n\n\tentries = kzalloc(size, GFP_KERNEL);\n\tif (!entries)\n\t\treturn -ENOMEM;\n\n\tsize = sizeof(struct perf_callchain_entry) * PERF_NR_CONTEXTS;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tentries->cpu_entries[cpu] = kmalloc_node(size, GFP_KERNEL,\n\t\t\t\t\t\t\t cpu_to_node(cpu));\n\t\tif (!entries->cpu_entries[cpu])\n\t\t\tgoto fail;\n\t}\n\n\trcu_assign_pointer(callchain_cpus_entries, entries);\n\n\treturn 0;\n\nfail:\n\tfor_each_possible_cpu(cpu)\n\t\tkfree(entries->cpu_entries[cpu]);\n\tkfree(entries);\n\n\treturn -ENOMEM;\n}\n\nstatic int get_callchain_buffers(void)\n{\n\tint err = 0;\n\tint count;\n\n\tmutex_lock(&callchain_mutex);\n\n\tcount = atomic_inc_return(&nr_callchain_events);\n\tif (WARN_ON_ONCE(count < 1)) {\n\t\terr = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\tif (count > 1) {\n\t\t/* If the allocation failed, give up */\n\t\tif (!callchain_cpus_entries)\n\t\t\terr = -ENOMEM;\n\t\tgoto exit;\n\t}\n\n\terr = alloc_callchain_buffers();\n\tif (err)\n\t\trelease_callchain_buffers();\nexit:\n\tmutex_unlock(&callchain_mutex);\n\n\treturn err;\n}\n\nstatic void put_callchain_buffers(void)\n{\n\tif (atomic_dec_and_mutex_lock(&nr_callchain_events, &callchain_mutex)) {\n\t\trelease_callchain_buffers();\n\t\tmutex_unlock(&callchain_mutex);\n\t}\n}\n\nstatic int get_recursion_context(int *recursion)\n{\n\tint rctx;\n\n\tif (in_nmi())\n\t\trctx = 3;\n\telse if (in_irq())\n\t\trctx = 2;\n\telse if (in_softirq())\n\t\trctx = 1;\n\telse\n\t\trctx = 0;\n\n\tif (recursion[rctx])\n\t\treturn -1;\n\n\trecursion[rctx]++;\n\tbarrier();\n\n\treturn rctx;\n}\n\nstatic inline void put_recursion_context(int *recursion, int rctx)\n{\n\tbarrier();\n\trecursion[rctx]--;\n}\n\nstatic struct perf_callchain_entry *get_callchain_entry(int *rctx)\n{\n\tint cpu;\n\tstruct callchain_cpus_entries *entries;\n\n\t*rctx = get_recursion_context(__get_cpu_var(callchain_recursion));\n\tif (*rctx == -1)\n\t\treturn NULL;\n\n\tentries = rcu_dereference(callchain_cpus_entries);\n\tif (!entries)\n\t\treturn NULL;\n\n\tcpu = smp_processor_id();\n\n\treturn &entries->cpu_entries[cpu][*rctx];\n}\n\nstatic void\nput_callchain_entry(int rctx)\n{\n\tput_recursion_context(__get_cpu_var(callchain_recursion), rctx);\n}\n\nstatic struct perf_callchain_entry *perf_callchain(struct pt_regs *regs)\n{\n\tint rctx;\n\tstruct perf_callchain_entry *entry;\n\n\n\tentry = get_callchain_entry(&rctx);\n\tif (rctx == -1)\n\t\treturn NULL;\n\n\tif (!entry)\n\t\tgoto exit_put;\n\n\tentry->nr = 0;\n\n\tif (!user_mode(regs)) {\n\t\tperf_callchain_store(entry, PERF_CONTEXT_KERNEL);\n\t\tperf_callchain_kernel(entry, regs);\n\t\tif (current->mm)\n\t\t\tregs = task_pt_regs(current);\n\t\telse\n\t\t\tregs = NULL;\n\t}\n\n\tif (regs) {\n\t\tperf_callchain_store(entry, PERF_CONTEXT_USER);\n\t\tperf_callchain_user(entry, regs);\n\t}\n\nexit_put:\n\tput_callchain_entry(rctx);\n\n\treturn entry;\n}\n\n/*\n * Initialize the perf_event context in a task_struct:\n */\nstatic void __perf_event_init_context(struct perf_event_context *ctx)\n{\n\traw_spin_lock_init(&ctx->lock);\n\tmutex_init(&ctx->mutex);\n\tINIT_LIST_HEAD(&ctx->pinned_groups);\n\tINIT_LIST_HEAD(&ctx->flexible_groups);\n\tINIT_LIST_HEAD(&ctx->event_list);\n\tatomic_set(&ctx->refcount, 1);\n}\n\nstatic struct perf_event_context *\nalloc_perf_context(struct pmu *pmu, struct task_struct *task)\n{\n\tstruct perf_event_context *ctx;\n\n\tctx = kzalloc(sizeof(struct perf_event_context), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn NULL;\n\n\t__perf_event_init_context(ctx);\n\tif (task) {\n\t\tctx->task = task;\n\t\tget_task_struct(task);\n\t}\n\tctx->pmu = pmu;\n\n\treturn ctx;\n}\n\nstatic struct task_struct *\nfind_lively_task_by_vpid(pid_t vpid)\n{\n\tstruct task_struct *task;\n\tint err;\n\n\trcu_read_lock();\n\tif (!vpid)\n\t\ttask = current;\n\telse\n\t\ttask = find_task_by_vpid(vpid);\n\tif (task)\n\t\tget_task_struct(task);\n\trcu_read_unlock();\n\n\tif (!task)\n\t\treturn ERR_PTR(-ESRCH);\n\n\t/* Reuse ptrace permission checks for now. */\n\terr = -EACCES;\n\tif (!ptrace_may_access(task, PTRACE_MODE_READ))\n\t\tgoto errout;\n\n\treturn task;\nerrout:\n\tput_task_struct(task);\n\treturn ERR_PTR(err);\n\n}\n\n/*\n * Returns a matching context with refcount and pincount.\n */\nstatic struct perf_event_context *\nfind_get_context(struct pmu *pmu, struct task_struct *task, int cpu)\n{\n\tstruct perf_event_context *ctx;\n\tstruct perf_cpu_context *cpuctx;\n\tunsigned long flags;\n\tint ctxn, err;\n\n\tif (!task) {\n\t\t/* Must be root to operate on a CPU event: */\n\t\tif (perf_paranoid_cpu() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn ERR_PTR(-EACCES);\n\n\t\t/*\n\t\t * We could be clever and allow to attach a event to an\n\t\t * offline CPU and activate it when the CPU comes up, but\n\t\t * that's for later.\n\t\t */\n\t\tif (!cpu_online(cpu))\n\t\t\treturn ERR_PTR(-ENODEV);\n\n\t\tcpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);\n\t\tctx = &cpuctx->ctx;\n\t\tget_ctx(ctx);\n\t\t++ctx->pin_count;\n\n\t\treturn ctx;\n\t}\n\n\terr = -EINVAL;\n\tctxn = pmu->task_ctx_nr;\n\tif (ctxn < 0)\n\t\tgoto errout;\n\nretry:\n\tctx = perf_lock_task_context(task, ctxn, &flags);\n\tif (ctx) {\n\t\tunclone_ctx(ctx);\n\t\t++ctx->pin_count;\n\t\traw_spin_unlock_irqrestore(&ctx->lock, flags);\n\t} else {\n\t\tctx = alloc_perf_context(pmu, task);\n\t\terr = -ENOMEM;\n\t\tif (!ctx)\n\t\t\tgoto errout;\n\n\t\terr = 0;\n\t\tmutex_lock(&task->perf_event_mutex);\n\t\t/*\n\t\t * If it has already passed perf_event_exit_task().\n\t\t * we must see PF_EXITING, it takes this mutex too.\n\t\t */\n\t\tif (task->flags & PF_EXITING)\n\t\t\terr = -ESRCH;\n\t\telse if (task->perf_event_ctxp[ctxn])\n\t\t\terr = -EAGAIN;\n\t\telse {\n\t\t\tget_ctx(ctx);\n\t\t\t++ctx->pin_count;\n\t\t\trcu_assign_pointer(task->perf_event_ctxp[ctxn], ctx);\n\t\t}\n\t\tmutex_unlock(&task->perf_event_mutex);\n\n\t\tif (unlikely(err)) {\n\t\t\tput_ctx(ctx);\n\n\t\t\tif (err == -EAGAIN)\n\t\t\t\tgoto retry;\n\t\t\tgoto errout;\n\t\t}\n\t}\n\n\treturn ctx;\n\nerrout:\n\treturn ERR_PTR(err);\n}\n\nstatic void perf_event_free_filter(struct perf_event *event);\n\nstatic void free_event_rcu(struct rcu_head *head)\n{\n\tstruct perf_event *event;\n\n\tevent = container_of(head, struct perf_event, rcu_head);\n\tif (event->ns)\n\t\tput_pid_ns(event->ns);\n\tperf_event_free_filter(event);\n\tkfree(event);\n}\n\nstatic void ring_buffer_put(struct ring_buffer *rb);\n\nstatic void free_event(struct perf_event *event)\n{\n\tirq_work_sync(&event->pending);\n\n\tif (!event->parent) {\n\t\tif (event->attach_state & PERF_ATTACH_TASK)\n\t\t\tjump_label_dec(&perf_sched_events);\n\t\tif (event->attr.mmap || event->attr.mmap_data)\n\t\t\tatomic_dec(&nr_mmap_events);\n\t\tif (event->attr.comm)\n\t\t\tatomic_dec(&nr_comm_events);\n\t\tif (event->attr.task)\n\t\t\tatomic_dec(&nr_task_events);\n\t\tif (event->attr.sample_type & PERF_SAMPLE_CALLCHAIN)\n\t\t\tput_callchain_buffers();\n\t\tif (is_cgroup_event(event)) {\n\t\t\tatomic_dec(&per_cpu(perf_cgroup_events, event->cpu));\n\t\t\tjump_label_dec(&perf_sched_events);\n\t\t}\n\t}\n\n\tif (event->rb) {\n\t\tring_buffer_put(event->rb);\n\t\tevent->rb = NULL;\n\t}\n\n\tif (is_cgroup_event(event))\n\t\tperf_detach_cgroup(event);\n\n\tif (event->destroy)\n\t\tevent->destroy(event);\n\n\tif (event->ctx)\n\t\tput_ctx(event->ctx);\n\n\tcall_rcu(&event->rcu_head, free_event_rcu);\n}\n\nint perf_event_release_kernel(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\t/*\n\t * There are two ways this annotation is useful:\n\t *\n\t *  1) there is a lock recursion from perf_event_exit_task\n\t *     see the comment there.\n\t *\n\t *  2) there is a lock-inversion with mmap_sem through\n\t *     perf_event_read_group(), which takes faults while\n\t *     holding ctx->mutex, however this is called after\n\t *     the last filedesc died, so there is no possibility\n\t *     to trigger the AB-BA case.\n\t */\n\tmutex_lock_nested(&ctx->mutex, SINGLE_DEPTH_NESTING);\n\traw_spin_lock_irq(&ctx->lock);\n\tperf_group_detach(event);\n\traw_spin_unlock_irq(&ctx->lock);\n\tperf_remove_from_context(event);\n\tmutex_unlock(&ctx->mutex);\n\n\tfree_event(event);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(perf_event_release_kernel);\n\n/*\n * Called when the last reference to the file is gone.\n */\nstatic int perf_release(struct inode *inode, struct file *file)\n{\n\tstruct perf_event *event = file->private_data;\n\tstruct task_struct *owner;\n\n\tfile->private_data = NULL;\n\n\trcu_read_lock();\n\towner = ACCESS_ONCE(event->owner);\n\t/*\n\t * Matches the smp_wmb() in perf_event_exit_task(). If we observe\n\t * !owner it means the list deletion is complete and we can indeed\n\t * free this event, otherwise we need to serialize on\n\t * owner->perf_event_mutex.\n\t */\n\tsmp_read_barrier_depends();\n\tif (owner) {\n\t\t/*\n\t\t * Since delayed_put_task_struct() also drops the last\n\t\t * task reference we can safely take a new reference\n\t\t * while holding the rcu_read_lock().\n\t\t */\n\t\tget_task_struct(owner);\n\t}\n\trcu_read_unlock();\n\n\tif (owner) {\n\t\tmutex_lock(&owner->perf_event_mutex);\n\t\t/*\n\t\t * We have to re-check the event->owner field, if it is cleared\n\t\t * we raced with perf_event_exit_task(), acquiring the mutex\n\t\t * ensured they're done, and we can proceed with freeing the\n\t\t * event.\n\t\t */\n\t\tif (event->owner)\n\t\t\tlist_del_init(&event->owner_entry);\n\t\tmutex_unlock(&owner->perf_event_mutex);\n\t\tput_task_struct(owner);\n\t}\n\n\treturn perf_event_release_kernel(event);\n}\n\nu64 perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *running)\n{\n\tstruct perf_event *child;\n\tu64 total = 0;\n\n\t*enabled = 0;\n\t*running = 0;\n\n\tmutex_lock(&event->child_mutex);\n\ttotal += perf_event_read(event);\n\t*enabled += event->total_time_enabled +\n\t\t\tatomic64_read(&event->child_total_time_enabled);\n\t*running += event->total_time_running +\n\t\t\tatomic64_read(&event->child_total_time_running);\n\n\tlist_for_each_entry(child, &event->child_list, child_list) {\n\t\ttotal += perf_event_read(child);\n\t\t*enabled += child->total_time_enabled;\n\t\t*running += child->total_time_running;\n\t}\n\tmutex_unlock(&event->child_mutex);\n\n\treturn total;\n}\nEXPORT_SYMBOL_GPL(perf_event_read_value);\n\nstatic int perf_event_read_group(struct perf_event *event,\n\t\t\t\t   u64 read_format, char __user *buf)\n{\n\tstruct perf_event *leader = event->group_leader, *sub;\n\tint n = 0, size = 0, ret = -EFAULT;\n\tstruct perf_event_context *ctx = leader->ctx;\n\tu64 values[5];\n\tu64 count, enabled, running;\n\n\tmutex_lock(&ctx->mutex);\n\tcount = perf_event_read_value(leader, &enabled, &running);\n\n\tvalues[n++] = 1 + leader->nr_siblings;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tvalues[n++] = enabled;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tvalues[n++] = running;\n\tvalues[n++] = count;\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(leader);\n\n\tsize = n * sizeof(u64);\n\n\tif (copy_to_user(buf, values, size))\n\t\tgoto unlock;\n\n\tret = size;\n\n\tlist_for_each_entry(sub, &leader->sibling_list, group_entry) {\n\t\tn = 0;\n\n\t\tvalues[n++] = perf_event_read_value(sub, &enabled, &running);\n\t\tif (read_format & PERF_FORMAT_ID)\n\t\t\tvalues[n++] = primary_event_id(sub);\n\n\t\tsize = n * sizeof(u64);\n\n\t\tif (copy_to_user(buf + ret, values, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tret += size;\n\t}\nunlock:\n\tmutex_unlock(&ctx->mutex);\n\n\treturn ret;\n}\n\nstatic int perf_event_read_one(struct perf_event *event,\n\t\t\t\t u64 read_format, char __user *buf)\n{\n\tu64 enabled, running;\n\tu64 values[4];\n\tint n = 0;\n\n\tvalues[n++] = perf_event_read_value(event, &enabled, &running);\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tvalues[n++] = enabled;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tvalues[n++] = running;\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(event);\n\n\tif (copy_to_user(buf, values, n * sizeof(u64)))\n\t\treturn -EFAULT;\n\n\treturn n * sizeof(u64);\n}\n\n/*\n * Read the performance event - simple non blocking version for now\n */\nstatic ssize_t\nperf_read_hw(struct perf_event *event, char __user *buf, size_t count)\n{\n\tu64 read_format = event->attr.read_format;\n\tint ret;\n\n\t/*\n\t * Return end-of-file for a read on a event that is in\n\t * error state (i.e. because it was pinned but it couldn't be\n\t * scheduled on to the CPU at some point).\n\t */\n\tif (event->state == PERF_EVENT_STATE_ERROR)\n\t\treturn 0;\n\n\tif (count < event->read_size)\n\t\treturn -ENOSPC;\n\n\tWARN_ON_ONCE(event->ctx->parent_ctx);\n\tif (read_format & PERF_FORMAT_GROUP)\n\t\tret = perf_event_read_group(event, read_format, buf);\n\telse\n\t\tret = perf_event_read_one(event, read_format, buf);\n\n\treturn ret;\n}\n\nstatic ssize_t\nperf_read(struct file *file, char __user *buf, size_t count, loff_t *ppos)\n{\n\tstruct perf_event *event = file->private_data;\n\n\treturn perf_read_hw(event, buf, count);\n}\n\nstatic unsigned int perf_poll(struct file *file, poll_table *wait)\n{\n\tstruct perf_event *event = file->private_data;\n\tstruct ring_buffer *rb;\n\tunsigned int events = POLL_HUP;\n\n\trcu_read_lock();\n\trb = rcu_dereference(event->rb);\n\tif (rb)\n\t\tevents = atomic_xchg(&rb->poll, 0);\n\trcu_read_unlock();\n\n\tpoll_wait(file, &event->waitq, wait);\n\n\treturn events;\n}\n\nstatic void perf_event_reset(struct perf_event *event)\n{\n\t(void)perf_event_read(event);\n\tlocal64_set(&event->count, 0);\n\tperf_event_update_userpage(event);\n}\n\n/*\n * Holding the top-level event's child_mutex means that any\n * descendant process that has inherited this event will block\n * in sync_child_event if it goes to exit, thus satisfying the\n * task existence requirements of perf_event_enable/disable.\n */\nstatic void perf_event_for_each_child(struct perf_event *event,\n\t\t\t\t\tvoid (*func)(struct perf_event *))\n{\n\tstruct perf_event *child;\n\n\tWARN_ON_ONCE(event->ctx->parent_ctx);\n\tmutex_lock(&event->child_mutex);\n\tfunc(event);\n\tlist_for_each_entry(child, &event->child_list, child_list)\n\t\tfunc(child);\n\tmutex_unlock(&event->child_mutex);\n}\n\nstatic void perf_event_for_each(struct perf_event *event,\n\t\t\t\t  void (*func)(struct perf_event *))\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *sibling;\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\tevent = event->group_leader;\n\n\tperf_event_for_each_child(event, func);\n\tfunc(event);\n\tlist_for_each_entry(sibling, &event->sibling_list, group_entry)\n\t\tperf_event_for_each_child(event, func);\n\tmutex_unlock(&ctx->mutex);\n}\n\nstatic int perf_event_period(struct perf_event *event, u64 __user *arg)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tint ret = 0;\n\tu64 value;\n\n\tif (!is_sampling_event(event))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&value, arg, sizeof(value)))\n\t\treturn -EFAULT;\n\n\tif (!value)\n\t\treturn -EINVAL;\n\n\traw_spin_lock_irq(&ctx->lock);\n\tif (event->attr.freq) {\n\t\tif (value > sysctl_perf_event_sample_rate) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tevent->attr.sample_freq = value;\n\t} else {\n\t\tevent->attr.sample_period = value;\n\t\tevent->hw.sample_period = value;\n\t}\nunlock:\n\traw_spin_unlock_irq(&ctx->lock);\n\n\treturn ret;\n}\n\nstatic const struct file_operations perf_fops;\n\nstatic struct perf_event *perf_fget_light(int fd, int *fput_needed)\n{\n\tstruct file *file;\n\n\tfile = fget_light(fd, fput_needed);\n\tif (!file)\n\t\treturn ERR_PTR(-EBADF);\n\n\tif (file->f_op != &perf_fops) {\n\t\tfput_light(file, *fput_needed);\n\t\t*fput_needed = 0;\n\t\treturn ERR_PTR(-EBADF);\n\t}\n\n\treturn file->private_data;\n}\n\nstatic int perf_event_set_output(struct perf_event *event,\n\t\t\t\t struct perf_event *output_event);\nstatic int perf_event_set_filter(struct perf_event *event, void __user *arg);\n\nstatic long perf_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct perf_event *event = file->private_data;\n\tvoid (*func)(struct perf_event *);\n\tu32 flags = arg;\n\n\tswitch (cmd) {\n\tcase PERF_EVENT_IOC_ENABLE:\n\t\tfunc = perf_event_enable;\n\t\tbreak;\n\tcase PERF_EVENT_IOC_DISABLE:\n\t\tfunc = perf_event_disable;\n\t\tbreak;\n\tcase PERF_EVENT_IOC_RESET:\n\t\tfunc = perf_event_reset;\n\t\tbreak;\n\n\tcase PERF_EVENT_IOC_REFRESH:\n\t\treturn perf_event_refresh(event, arg);\n\n\tcase PERF_EVENT_IOC_PERIOD:\n\t\treturn perf_event_period(event, (u64 __user *)arg);\n\n\tcase PERF_EVENT_IOC_SET_OUTPUT:\n\t{\n\t\tstruct perf_event *output_event = NULL;\n\t\tint fput_needed = 0;\n\t\tint ret;\n\n\t\tif (arg != -1) {\n\t\t\toutput_event = perf_fget_light(arg, &fput_needed);\n\t\t\tif (IS_ERR(output_event))\n\t\t\t\treturn PTR_ERR(output_event);\n\t\t}\n\n\t\tret = perf_event_set_output(event, output_event);\n\t\tif (output_event)\n\t\t\tfput_light(output_event->filp, fput_needed);\n\n\t\treturn ret;\n\t}\n\n\tcase PERF_EVENT_IOC_SET_FILTER:\n\t\treturn perf_event_set_filter(event, (void __user *)arg);\n\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (flags & PERF_IOC_FLAG_GROUP)\n\t\tperf_event_for_each(event, func);\n\telse\n\t\tperf_event_for_each_child(event, func);\n\n\treturn 0;\n}\n\nint perf_event_task_enable(void)\n{\n\tstruct perf_event *event;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_for_each_entry(event, &current->perf_event_list, owner_entry)\n\t\tperf_event_for_each_child(event, perf_event_enable);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\treturn 0;\n}\n\nint perf_event_task_disable(void)\n{\n\tstruct perf_event *event;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_for_each_entry(event, &current->perf_event_list, owner_entry)\n\t\tperf_event_for_each_child(event, perf_event_disable);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\treturn 0;\n}\n\n#ifndef PERF_EVENT_INDEX_OFFSET\n# define PERF_EVENT_INDEX_OFFSET 0\n#endif\n\nstatic int perf_event_index(struct perf_event *event)\n{\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn 0;\n\n\tif (event->state != PERF_EVENT_STATE_ACTIVE)\n\t\treturn 0;\n\n\treturn event->hw.idx + 1 - PERF_EVENT_INDEX_OFFSET;\n}\n\nstatic void calc_timer_values(struct perf_event *event,\n\t\t\t\tu64 *running,\n\t\t\t\tu64 *enabled)\n{\n\tu64 now, ctx_time;\n\n\tnow = perf_clock();\n\tctx_time = event->shadow_ctx_time + now;\n\t*enabled = ctx_time - event->tstamp_enabled;\n\t*running = ctx_time - event->tstamp_running;\n}\n\n/*\n * Callers need to ensure there can be no nesting of this function, otherwise\n * the seqlock logic goes bad. We can not serialize this because the arch\n * code calls this from NMI context.\n */\nvoid perf_event_update_userpage(struct perf_event *event)\n{\n\tstruct perf_event_mmap_page *userpg;\n\tstruct ring_buffer *rb;\n\tu64 enabled, running;\n\n\trcu_read_lock();\n\t/*\n\t * compute total_time_enabled, total_time_running\n\t * based on snapshot values taken when the event\n\t * was last scheduled in.\n\t *\n\t * we cannot simply called update_context_time()\n\t * because of locking issue as we can be called in\n\t * NMI context\n\t */\n\tcalc_timer_values(event, &enabled, &running);\n\trb = rcu_dereference(event->rb);\n\tif (!rb)\n\t\tgoto unlock;\n\n\tuserpg = rb->user_page;\n\n\t/*\n\t * Disable preemption so as to not let the corresponding user-space\n\t * spin too long if we get preempted.\n\t */\n\tpreempt_disable();\n\t++userpg->lock;\n\tbarrier();\n\tuserpg->index = perf_event_index(event);\n\tuserpg->offset = perf_event_count(event);\n\tif (event->state == PERF_EVENT_STATE_ACTIVE)\n\t\tuserpg->offset -= local64_read(&event->hw.prev_count);\n\n\tuserpg->time_enabled = enabled +\n\t\t\tatomic64_read(&event->child_total_time_enabled);\n\n\tuserpg->time_running = running +\n\t\t\tatomic64_read(&event->child_total_time_running);\n\n\tbarrier();\n\t++userpg->lock;\n\tpreempt_enable();\nunlock:\n\trcu_read_unlock();\n}\n\nstatic int perf_mmap_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct perf_event *event = vma->vm_file->private_data;\n\tstruct ring_buffer *rb;\n\tint ret = VM_FAULT_SIGBUS;\n\n\tif (vmf->flags & FAULT_FLAG_MKWRITE) {\n\t\tif (vmf->pgoff == 0)\n\t\t\tret = 0;\n\t\treturn ret;\n\t}\n\n\trcu_read_lock();\n\trb = rcu_dereference(event->rb);\n\tif (!rb)\n\t\tgoto unlock;\n\n\tif (vmf->pgoff && (vmf->flags & FAULT_FLAG_WRITE))\n\t\tgoto unlock;\n\n\tvmf->page = perf_mmap_to_page(rb, vmf->pgoff);\n\tif (!vmf->page)\n\t\tgoto unlock;\n\n\tget_page(vmf->page);\n\tvmf->page->mapping = vma->vm_file->f_mapping;\n\tvmf->page->index   = vmf->pgoff;\n\n\tret = 0;\nunlock:\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic void rb_free_rcu(struct rcu_head *rcu_head)\n{\n\tstruct ring_buffer *rb;\n\n\trb = container_of(rcu_head, struct ring_buffer, rcu_head);\n\trb_free(rb);\n}\n\nstatic struct ring_buffer *ring_buffer_get(struct perf_event *event)\n{\n\tstruct ring_buffer *rb;\n\n\trcu_read_lock();\n\trb = rcu_dereference(event->rb);\n\tif (rb) {\n\t\tif (!atomic_inc_not_zero(&rb->refcount))\n\t\t\trb = NULL;\n\t}\n\trcu_read_unlock();\n\n\treturn rb;\n}\n\nstatic void ring_buffer_put(struct ring_buffer *rb)\n{\n\tif (!atomic_dec_and_test(&rb->refcount))\n\t\treturn;\n\n\tcall_rcu(&rb->rcu_head, rb_free_rcu);\n}\n\nstatic void perf_mmap_open(struct vm_area_struct *vma)\n{\n\tstruct perf_event *event = vma->vm_file->private_data;\n\n\tatomic_inc(&event->mmap_count);\n}\n\nstatic void perf_mmap_close(struct vm_area_struct *vma)\n{\n\tstruct perf_event *event = vma->vm_file->private_data;\n\n\tif (atomic_dec_and_mutex_lock(&event->mmap_count, &event->mmap_mutex)) {\n\t\tunsigned long size = perf_data_size(event->rb);\n\t\tstruct user_struct *user = event->mmap_user;\n\t\tstruct ring_buffer *rb = event->rb;\n\n\t\tatomic_long_sub((size >> PAGE_SHIFT) + 1, &user->locked_vm);\n\t\tvma->vm_mm->locked_vm -= event->mmap_locked;\n\t\trcu_assign_pointer(event->rb, NULL);\n\t\tmutex_unlock(&event->mmap_mutex);\n\n\t\tring_buffer_put(rb);\n\t\tfree_uid(user);\n\t}\n}\n\nstatic const struct vm_operations_struct perf_mmap_vmops = {\n\t.open\t\t= perf_mmap_open,\n\t.close\t\t= perf_mmap_close,\n\t.fault\t\t= perf_mmap_fault,\n\t.page_mkwrite\t= perf_mmap_fault,\n};\n\nstatic int perf_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tstruct perf_event *event = file->private_data;\n\tunsigned long user_locked, user_lock_limit;\n\tstruct user_struct *user = current_user();\n\tunsigned long locked, lock_limit;\n\tstruct ring_buffer *rb;\n\tunsigned long vma_size;\n\tunsigned long nr_pages;\n\tlong user_extra, extra;\n\tint ret = 0, flags = 0;\n\n\t/*\n\t * Don't allow mmap() of inherited per-task counters. This would\n\t * create a performance issue due to all children writing to the\n\t * same rb.\n\t */\n\tif (event->cpu == -1 && event->attr.inherit)\n\t\treturn -EINVAL;\n\n\tif (!(vma->vm_flags & VM_SHARED))\n\t\treturn -EINVAL;\n\n\tvma_size = vma->vm_end - vma->vm_start;\n\tnr_pages = (vma_size / PAGE_SIZE) - 1;\n\n\t/*\n\t * If we have rb pages ensure they're a power-of-two number, so we\n\t * can do bitmasks instead of modulo.\n\t */\n\tif (nr_pages != 0 && !is_power_of_2(nr_pages))\n\t\treturn -EINVAL;\n\n\tif (vma_size != PAGE_SIZE * (1 + nr_pages))\n\t\treturn -EINVAL;\n\n\tif (vma->vm_pgoff != 0)\n\t\treturn -EINVAL;\n\n\tWARN_ON_ONCE(event->ctx->parent_ctx);\n\tmutex_lock(&event->mmap_mutex);\n\tif (event->rb) {\n\t\tif (event->rb->nr_pages == nr_pages)\n\t\t\tatomic_inc(&event->rb->refcount);\n\t\telse\n\t\t\tret = -EINVAL;\n\t\tgoto unlock;\n\t}\n\n\tuser_extra = nr_pages + 1;\n\tuser_lock_limit = sysctl_perf_event_mlock >> (PAGE_SHIFT - 10);\n\n\t/*\n\t * Increase the limit linearly with more CPUs:\n\t */\n\tuser_lock_limit *= num_online_cpus();\n\n\tuser_locked = atomic_long_read(&user->locked_vm) + user_extra;\n\n\textra = 0;\n\tif (user_locked > user_lock_limit)\n\t\textra = user_locked - user_lock_limit;\n\n\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\tlock_limit >>= PAGE_SHIFT;\n\tlocked = vma->vm_mm->locked_vm + extra;\n\n\tif ((locked > lock_limit) && perf_paranoid_tracepoint_raw() &&\n\t\t!capable(CAP_IPC_LOCK)) {\n\t\tret = -EPERM;\n\t\tgoto unlock;\n\t}\n\n\tWARN_ON(event->rb);\n\n\tif (vma->vm_flags & VM_WRITE)\n\t\tflags |= RING_BUFFER_WRITABLE;\n\n\trb = rb_alloc(nr_pages, \n\t\tevent->attr.watermark ? event->attr.wakeup_watermark : 0,\n\t\tevent->cpu, flags);\n\n\tif (!rb) {\n\t\tret = -ENOMEM;\n\t\tgoto unlock;\n\t}\n\trcu_assign_pointer(event->rb, rb);\n\n\tatomic_long_add(user_extra, &user->locked_vm);\n\tevent->mmap_locked = extra;\n\tevent->mmap_user = get_current_user();\n\tvma->vm_mm->locked_vm += event->mmap_locked;\n\nunlock:\n\tif (!ret)\n\t\tatomic_inc(&event->mmap_count);\n\tmutex_unlock(&event->mmap_mutex);\n\n\tvma->vm_flags |= VM_RESERVED;\n\tvma->vm_ops = &perf_mmap_vmops;\n\n\treturn ret;\n}\n\nstatic int perf_fasync(int fd, struct file *filp, int on)\n{\n\tstruct inode *inode = filp->f_path.dentry->d_inode;\n\tstruct perf_event *event = filp->private_data;\n\tint retval;\n\n\tmutex_lock(&inode->i_mutex);\n\tretval = fasync_helper(fd, filp, on, &event->fasync);\n\tmutex_unlock(&inode->i_mutex);\n\n\tif (retval < 0)\n\t\treturn retval;\n\n\treturn 0;\n}\n\nstatic const struct file_operations perf_fops = {\n\t.llseek\t\t\t= no_llseek,\n\t.release\t\t= perf_release,\n\t.read\t\t\t= perf_read,\n\t.poll\t\t\t= perf_poll,\n\t.unlocked_ioctl\t\t= perf_ioctl,\n\t.compat_ioctl\t\t= perf_ioctl,\n\t.mmap\t\t\t= perf_mmap,\n\t.fasync\t\t\t= perf_fasync,\n};\n\n/*\n * Perf event wakeup\n *\n * If there's data, ensure we set the poll() state and publish everything\n * to user-space before waking everybody up.\n */\n\nvoid perf_event_wakeup(struct perf_event *event)\n{\n\twake_up_all(&event->waitq);\n\n\tif (event->pending_kill) {\n\t\tkill_fasync(&event->fasync, SIGIO, event->pending_kill);\n\t\tevent->pending_kill = 0;\n\t}\n}\n\nstatic void perf_pending_event(struct irq_work *entry)\n{\n\tstruct perf_event *event = container_of(entry,\n\t\t\tstruct perf_event, pending);\n\n\tif (event->pending_disable) {\n\t\tevent->pending_disable = 0;\n\t\t__perf_event_disable(event);\n\t}\n\n\tif (event->pending_wakeup) {\n\t\tevent->pending_wakeup = 0;\n\t\tperf_event_wakeup(event);\n\t}\n}\n\n/*\n * We assume there is only KVM supporting the callbacks.\n * Later on, we might change it to a list if there is\n * another virtualization implementation supporting the callbacks.\n */\nstruct perf_guest_info_callbacks *perf_guest_cbs;\n\nint perf_register_guest_info_callbacks(struct perf_guest_info_callbacks *cbs)\n{\n\tperf_guest_cbs = cbs;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(perf_register_guest_info_callbacks);\n\nint perf_unregister_guest_info_callbacks(struct perf_guest_info_callbacks *cbs)\n{\n\tperf_guest_cbs = NULL;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(perf_unregister_guest_info_callbacks);\n\nstatic void __perf_event_header__init_id(struct perf_event_header *header,\n\t\t\t\t\t struct perf_sample_data *data,\n\t\t\t\t\t struct perf_event *event)\n{\n\tu64 sample_type = event->attr.sample_type;\n\n\tdata->type = sample_type;\n\theader->size += event->id_header_size;\n\n\tif (sample_type & PERF_SAMPLE_TID) {\n\t\t/* namespace issues */\n\t\tdata->tid_entry.pid = perf_event_pid(event, current);\n\t\tdata->tid_entry.tid = perf_event_tid(event, current);\n\t}\n\n\tif (sample_type & PERF_SAMPLE_TIME)\n\t\tdata->time = perf_clock();\n\n\tif (sample_type & PERF_SAMPLE_ID)\n\t\tdata->id = primary_event_id(event);\n\n\tif (sample_type & PERF_SAMPLE_STREAM_ID)\n\t\tdata->stream_id = event->id;\n\n\tif (sample_type & PERF_SAMPLE_CPU) {\n\t\tdata->cpu_entry.cpu\t = raw_smp_processor_id();\n\t\tdata->cpu_entry.reserved = 0;\n\t}\n}\n\nvoid perf_event_header__init_id(struct perf_event_header *header,\n\t\t\t\tstruct perf_sample_data *data,\n\t\t\t\tstruct perf_event *event)\n{\n\tif (event->attr.sample_id_all)\n\t\t__perf_event_header__init_id(header, data, event);\n}\n\nstatic void __perf_event__output_id_sample(struct perf_output_handle *handle,\n\t\t\t\t\t   struct perf_sample_data *data)\n{\n\tu64 sample_type = data->type;\n\n\tif (sample_type & PERF_SAMPLE_TID)\n\t\tperf_output_put(handle, data->tid_entry);\n\n\tif (sample_type & PERF_SAMPLE_TIME)\n\t\tperf_output_put(handle, data->time);\n\n\tif (sample_type & PERF_SAMPLE_ID)\n\t\tperf_output_put(handle, data->id);\n\n\tif (sample_type & PERF_SAMPLE_STREAM_ID)\n\t\tperf_output_put(handle, data->stream_id);\n\n\tif (sample_type & PERF_SAMPLE_CPU)\n\t\tperf_output_put(handle, data->cpu_entry);\n}\n\nvoid perf_event__output_id_sample(struct perf_event *event,\n\t\t\t\t  struct perf_output_handle *handle,\n\t\t\t\t  struct perf_sample_data *sample)\n{\n\tif (event->attr.sample_id_all)\n\t\t__perf_event__output_id_sample(handle, sample);\n}\n\nstatic void perf_output_read_one(struct perf_output_handle *handle,\n\t\t\t\t struct perf_event *event,\n\t\t\t\t u64 enabled, u64 running)\n{\n\tu64 read_format = event->attr.read_format;\n\tu64 values[4];\n\tint n = 0;\n\n\tvalues[n++] = perf_event_count(event);\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED) {\n\t\tvalues[n++] = enabled +\n\t\t\tatomic64_read(&event->child_total_time_enabled);\n\t}\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING) {\n\t\tvalues[n++] = running +\n\t\t\tatomic64_read(&event->child_total_time_running);\n\t}\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(event);\n\n\t__output_copy(handle, values, n * sizeof(u64));\n}\n\n/*\n * XXX PERF_FORMAT_GROUP vs inherited events seems difficult.\n */\nstatic void perf_output_read_group(struct perf_output_handle *handle,\n\t\t\t    struct perf_event *event,\n\t\t\t    u64 enabled, u64 running)\n{\n\tstruct perf_event *leader = event->group_leader, *sub;\n\tu64 read_format = event->attr.read_format;\n\tu64 values[5];\n\tint n = 0;\n\n\tvalues[n++] = 1 + leader->nr_siblings;\n\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tvalues[n++] = enabled;\n\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tvalues[n++] = running;\n\n\tif (leader != event)\n\t\tleader->pmu->read(leader);\n\n\tvalues[n++] = perf_event_count(leader);\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(leader);\n\n\t__output_copy(handle, values, n * sizeof(u64));\n\n\tlist_for_each_entry(sub, &leader->sibling_list, group_entry) {\n\t\tn = 0;\n\n\t\tif (sub != event)\n\t\t\tsub->pmu->read(sub);\n\n\t\tvalues[n++] = perf_event_count(sub);\n\t\tif (read_format & PERF_FORMAT_ID)\n\t\t\tvalues[n++] = primary_event_id(sub);\n\n\t\t__output_copy(handle, values, n * sizeof(u64));\n\t}\n}\n\n#define PERF_FORMAT_TOTAL_TIMES (PERF_FORMAT_TOTAL_TIME_ENABLED|\\\n\t\t\t\t PERF_FORMAT_TOTAL_TIME_RUNNING)\n\nstatic void perf_output_read(struct perf_output_handle *handle,\n\t\t\t     struct perf_event *event)\n{\n\tu64 enabled = 0, running = 0;\n\tu64 read_format = event->attr.read_format;\n\n\t/*\n\t * compute total_time_enabled, total_time_running\n\t * based on snapshot values taken when the event\n\t * was last scheduled in.\n\t *\n\t * we cannot simply called update_context_time()\n\t * because of locking issue as we are called in\n\t * NMI context\n\t */\n\tif (read_format & PERF_FORMAT_TOTAL_TIMES)\n\t\tcalc_timer_values(event, &enabled, &running);\n\n\tif (event->attr.read_format & PERF_FORMAT_GROUP)\n\t\tperf_output_read_group(handle, event, enabled, running);\n\telse\n\t\tperf_output_read_one(handle, event, enabled, running);\n}\n\nvoid perf_output_sample(struct perf_output_handle *handle,\n\t\t\tstruct perf_event_header *header,\n\t\t\tstruct perf_sample_data *data,\n\t\t\tstruct perf_event *event)\n{\n\tu64 sample_type = data->type;\n\n\tperf_output_put(handle, *header);\n\n\tif (sample_type & PERF_SAMPLE_IP)\n\t\tperf_output_put(handle, data->ip);\n\n\tif (sample_type & PERF_SAMPLE_TID)\n\t\tperf_output_put(handle, data->tid_entry);\n\n\tif (sample_type & PERF_SAMPLE_TIME)\n\t\tperf_output_put(handle, data->time);\n\n\tif (sample_type & PERF_SAMPLE_ADDR)\n\t\tperf_output_put(handle, data->addr);\n\n\tif (sample_type & PERF_SAMPLE_ID)\n\t\tperf_output_put(handle, data->id);\n\n\tif (sample_type & PERF_SAMPLE_STREAM_ID)\n\t\tperf_output_put(handle, data->stream_id);\n\n\tif (sample_type & PERF_SAMPLE_CPU)\n\t\tperf_output_put(handle, data->cpu_entry);\n\n\tif (sample_type & PERF_SAMPLE_PERIOD)\n\t\tperf_output_put(handle, data->period);\n\n\tif (sample_type & PERF_SAMPLE_READ)\n\t\tperf_output_read(handle, event);\n\n\tif (sample_type & PERF_SAMPLE_CALLCHAIN) {\n\t\tif (data->callchain) {\n\t\t\tint size = 1;\n\n\t\t\tif (data->callchain)\n\t\t\t\tsize += data->callchain->nr;\n\n\t\t\tsize *= sizeof(u64);\n\n\t\t\t__output_copy(handle, data->callchain, size);\n\t\t} else {\n\t\t\tu64 nr = 0;\n\t\t\tperf_output_put(handle, nr);\n\t\t}\n\t}\n\n\tif (sample_type & PERF_SAMPLE_RAW) {\n\t\tif (data->raw) {\n\t\t\tperf_output_put(handle, data->raw->size);\n\t\t\t__output_copy(handle, data->raw->data,\n\t\t\t\t\t   data->raw->size);\n\t\t} else {\n\t\t\tstruct {\n\t\t\t\tu32\tsize;\n\t\t\t\tu32\tdata;\n\t\t\t} raw = {\n\t\t\t\t.size = sizeof(u32),\n\t\t\t\t.data = 0,\n\t\t\t};\n\t\t\tperf_output_put(handle, raw);\n\t\t}\n\t}\n}\n\nvoid perf_prepare_sample(struct perf_event_header *header,\n\t\t\t struct perf_sample_data *data,\n\t\t\t struct perf_event *event,\n\t\t\t struct pt_regs *regs)\n{\n\tu64 sample_type = event->attr.sample_type;\n\n\theader->type = PERF_RECORD_SAMPLE;\n\theader->size = sizeof(*header) + event->header_size;\n\n\theader->misc = 0;\n\theader->misc |= perf_misc_flags(regs);\n\n\t__perf_event_header__init_id(header, data, event);\n\n\tif (sample_type & PERF_SAMPLE_IP)\n\t\tdata->ip = perf_instruction_pointer(regs);\n\n\tif (sample_type & PERF_SAMPLE_CALLCHAIN) {\n\t\tint size = 1;\n\n\t\tdata->callchain = perf_callchain(regs);\n\n\t\tif (data->callchain)\n\t\t\tsize += data->callchain->nr;\n\n\t\theader->size += size * sizeof(u64);\n\t}\n\n\tif (sample_type & PERF_SAMPLE_RAW) {\n\t\tint size = sizeof(u32);\n\n\t\tif (data->raw)\n\t\t\tsize += data->raw->size;\n\t\telse\n\t\t\tsize += sizeof(u32);\n\n\t\tWARN_ON_ONCE(size & (sizeof(u64)-1));\n\t\theader->size += size;\n\t}\n}\n\nstatic void perf_event_output(struct perf_event *event,\n\t\t\t\tstruct perf_sample_data *data,\n\t\t\t\tstruct pt_regs *regs)\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_event_header header;\n\n\t/* protect the callchain buffers */\n\trcu_read_lock();\n\n\tperf_prepare_sample(&header, data, event, regs);\n\n\tif (perf_output_begin(&handle, event, header.size, 1))\n\t\tgoto exit;\n\n\tperf_output_sample(&handle, &header, data, event);\n\n\tperf_output_end(&handle);\n\nexit:\n\trcu_read_unlock();\n}\n\n/*\n * read event_id\n */\n\nstruct perf_read_event {\n\tstruct perf_event_header\theader;\n\n\tu32\t\t\t\tpid;\n\tu32\t\t\t\ttid;\n};\n\nstatic void\nperf_event_read_event(struct perf_event *event,\n\t\t\tstruct task_struct *task)\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tstruct perf_read_event read_event = {\n\t\t.header = {\n\t\t\t.type = PERF_RECORD_READ,\n\t\t\t.misc = 0,\n\t\t\t.size = sizeof(read_event) + event->read_size,\n\t\t},\n\t\t.pid = perf_event_pid(event, task),\n\t\t.tid = perf_event_tid(event, task),\n\t};\n\tint ret;\n\n\tperf_event_header__init_id(&read_event.header, &sample, event);\n\tret = perf_output_begin(&handle, event, read_event.header.size, 0);\n\tif (ret)\n\t\treturn;\n\n\tperf_output_put(&handle, read_event);\n\tperf_output_read(&handle, event);\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\n}\n\n/*\n * task tracking -- fork/exit\n *\n * enabled by: attr.comm | attr.mmap | attr.mmap_data | attr.task\n */\n\nstruct perf_task_event {\n\tstruct task_struct\t\t*task;\n\tstruct perf_event_context\t*task_ctx;\n\n\tstruct {\n\t\tstruct perf_event_header\theader;\n\n\t\tu32\t\t\t\tpid;\n\t\tu32\t\t\t\tppid;\n\t\tu32\t\t\t\ttid;\n\t\tu32\t\t\t\tptid;\n\t\tu64\t\t\t\ttime;\n\t} event_id;\n};\n\nstatic void perf_event_task_output(struct perf_event *event,\n\t\t\t\t     struct perf_task_event *task_event)\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data\tsample;\n\tstruct task_struct *task = task_event->task;\n\tint ret, size = task_event->event_id.header.size;\n\n\tperf_event_header__init_id(&task_event->event_id.header, &sample, event);\n\n\tret = perf_output_begin(&handle, event,\n\t\t\t\ttask_event->event_id.header.size, 0);\n\tif (ret)\n\t\tgoto out;\n\n\ttask_event->event_id.pid = perf_event_pid(event, task);\n\ttask_event->event_id.ppid = perf_event_pid(event, current);\n\n\ttask_event->event_id.tid = perf_event_tid(event, task);\n\ttask_event->event_id.ptid = perf_event_tid(event, current);\n\n\tperf_output_put(&handle, task_event->event_id);\n\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\nout:\n\ttask_event->event_id.header.size = size;\n}\n\nstatic int perf_event_task_match(struct perf_event *event)\n{\n\tif (event->state < PERF_EVENT_STATE_INACTIVE)\n\t\treturn 0;\n\n\tif (!event_filter_match(event))\n\t\treturn 0;\n\n\tif (event->attr.comm || event->attr.mmap ||\n\t    event->attr.mmap_data || event->attr.task)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void perf_event_task_ctx(struct perf_event_context *ctx,\n\t\t\t\t  struct perf_task_event *task_event)\n{\n\tstruct perf_event *event;\n\n\tlist_for_each_entry_rcu(event, &ctx->event_list, event_entry) {\n\t\tif (perf_event_task_match(event))\n\t\t\tperf_event_task_output(event, task_event);\n\t}\n}\n\nstatic void perf_event_task_event(struct perf_task_event *task_event)\n{\n\tstruct perf_cpu_context *cpuctx;\n\tstruct perf_event_context *ctx;\n\tstruct pmu *pmu;\n\tint ctxn;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(pmu, &pmus, entry) {\n\t\tcpuctx = get_cpu_ptr(pmu->pmu_cpu_context);\n\t\tif (cpuctx->active_pmu != pmu)\n\t\t\tgoto next;\n\t\tperf_event_task_ctx(&cpuctx->ctx, task_event);\n\n\t\tctx = task_event->task_ctx;\n\t\tif (!ctx) {\n\t\t\tctxn = pmu->task_ctx_nr;\n\t\t\tif (ctxn < 0)\n\t\t\t\tgoto next;\n\t\t\tctx = rcu_dereference(current->perf_event_ctxp[ctxn]);\n\t\t}\n\t\tif (ctx)\n\t\t\tperf_event_task_ctx(ctx, task_event);\nnext:\n\t\tput_cpu_ptr(pmu->pmu_cpu_context);\n\t}\n\trcu_read_unlock();\n}\n\nstatic void perf_event_task(struct task_struct *task,\n\t\t\t      struct perf_event_context *task_ctx,\n\t\t\t      int new)\n{\n\tstruct perf_task_event task_event;\n\n\tif (!atomic_read(&nr_comm_events) &&\n\t    !atomic_read(&nr_mmap_events) &&\n\t    !atomic_read(&nr_task_events))\n\t\treturn;\n\n\ttask_event = (struct perf_task_event){\n\t\t.task\t  = task,\n\t\t.task_ctx = task_ctx,\n\t\t.event_id    = {\n\t\t\t.header = {\n\t\t\t\t.type = new ? PERF_RECORD_FORK : PERF_RECORD_EXIT,\n\t\t\t\t.misc = 0,\n\t\t\t\t.size = sizeof(task_event.event_id),\n\t\t\t},\n\t\t\t/* .pid  */\n\t\t\t/* .ppid */\n\t\t\t/* .tid  */\n\t\t\t/* .ptid */\n\t\t\t.time = perf_clock(),\n\t\t},\n\t};\n\n\tperf_event_task_event(&task_event);\n}\n\nvoid perf_event_fork(struct task_struct *task)\n{\n\tperf_event_task(task, NULL, 1);\n}\n\n/*\n * comm tracking\n */\n\nstruct perf_comm_event {\n\tstruct task_struct\t*task;\n\tchar\t\t\t*comm;\n\tint\t\t\tcomm_size;\n\n\tstruct {\n\t\tstruct perf_event_header\theader;\n\n\t\tu32\t\t\t\tpid;\n\t\tu32\t\t\t\ttid;\n\t} event_id;\n};\n\nstatic void perf_event_comm_output(struct perf_event *event,\n\t\t\t\t     struct perf_comm_event *comm_event)\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tint size = comm_event->event_id.header.size;\n\tint ret;\n\n\tperf_event_header__init_id(&comm_event->event_id.header, &sample, event);\n\tret = perf_output_begin(&handle, event,\n\t\t\t\tcomm_event->event_id.header.size, 0);\n\n\tif (ret)\n\t\tgoto out;\n\n\tcomm_event->event_id.pid = perf_event_pid(event, comm_event->task);\n\tcomm_event->event_id.tid = perf_event_tid(event, comm_event->task);\n\n\tperf_output_put(&handle, comm_event->event_id);\n\t__output_copy(&handle, comm_event->comm,\n\t\t\t\t   comm_event->comm_size);\n\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\nout:\n\tcomm_event->event_id.header.size = size;\n}\n\nstatic int perf_event_comm_match(struct perf_event *event)\n{\n\tif (event->state < PERF_EVENT_STATE_INACTIVE)\n\t\treturn 0;\n\n\tif (!event_filter_match(event))\n\t\treturn 0;\n\n\tif (event->attr.comm)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void perf_event_comm_ctx(struct perf_event_context *ctx,\n\t\t\t\t  struct perf_comm_event *comm_event)\n{\n\tstruct perf_event *event;\n\n\tlist_for_each_entry_rcu(event, &ctx->event_list, event_entry) {\n\t\tif (perf_event_comm_match(event))\n\t\t\tperf_event_comm_output(event, comm_event);\n\t}\n}\n\nstatic void perf_event_comm_event(struct perf_comm_event *comm_event)\n{\n\tstruct perf_cpu_context *cpuctx;\n\tstruct perf_event_context *ctx;\n\tchar comm[TASK_COMM_LEN];\n\tunsigned int size;\n\tstruct pmu *pmu;\n\tint ctxn;\n\n\tmemset(comm, 0, sizeof(comm));\n\tstrlcpy(comm, comm_event->task->comm, sizeof(comm));\n\tsize = ALIGN(strlen(comm)+1, sizeof(u64));\n\n\tcomm_event->comm = comm;\n\tcomm_event->comm_size = size;\n\n\tcomm_event->event_id.header.size = sizeof(comm_event->event_id) + size;\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(pmu, &pmus, entry) {\n\t\tcpuctx = get_cpu_ptr(pmu->pmu_cpu_context);\n\t\tif (cpuctx->active_pmu != pmu)\n\t\t\tgoto next;\n\t\tperf_event_comm_ctx(&cpuctx->ctx, comm_event);\n\n\t\tctxn = pmu->task_ctx_nr;\n\t\tif (ctxn < 0)\n\t\t\tgoto next;\n\n\t\tctx = rcu_dereference(current->perf_event_ctxp[ctxn]);\n\t\tif (ctx)\n\t\t\tperf_event_comm_ctx(ctx, comm_event);\nnext:\n\t\tput_cpu_ptr(pmu->pmu_cpu_context);\n\t}\n\trcu_read_unlock();\n}\n\nvoid perf_event_comm(struct task_struct *task)\n{\n\tstruct perf_comm_event comm_event;\n\tstruct perf_event_context *ctx;\n\tint ctxn;\n\n\tfor_each_task_context_nr(ctxn) {\n\t\tctx = task->perf_event_ctxp[ctxn];\n\t\tif (!ctx)\n\t\t\tcontinue;\n\n\t\tperf_event_enable_on_exec(ctx);\n\t}\n\n\tif (!atomic_read(&nr_comm_events))\n\t\treturn;\n\n\tcomm_event = (struct perf_comm_event){\n\t\t.task\t= task,\n\t\t/* .comm      */\n\t\t/* .comm_size */\n\t\t.event_id  = {\n\t\t\t.header = {\n\t\t\t\t.type = PERF_RECORD_COMM,\n\t\t\t\t.misc = 0,\n\t\t\t\t/* .size */\n\t\t\t},\n\t\t\t/* .pid */\n\t\t\t/* .tid */\n\t\t},\n\t};\n\n\tperf_event_comm_event(&comm_event);\n}\n\n/*\n * mmap tracking\n */\n\nstruct perf_mmap_event {\n\tstruct vm_area_struct\t*vma;\n\n\tconst char\t\t*file_name;\n\tint\t\t\tfile_size;\n\n\tstruct {\n\t\tstruct perf_event_header\theader;\n\n\t\tu32\t\t\t\tpid;\n\t\tu32\t\t\t\ttid;\n\t\tu64\t\t\t\tstart;\n\t\tu64\t\t\t\tlen;\n\t\tu64\t\t\t\tpgoff;\n\t} event_id;\n};\n\nstatic void perf_event_mmap_output(struct perf_event *event,\n\t\t\t\t     struct perf_mmap_event *mmap_event)\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tint size = mmap_event->event_id.header.size;\n\tint ret;\n\n\tperf_event_header__init_id(&mmap_event->event_id.header, &sample, event);\n\tret = perf_output_begin(&handle, event,\n\t\t\t\tmmap_event->event_id.header.size, 0);\n\tif (ret)\n\t\tgoto out;\n\n\tmmap_event->event_id.pid = perf_event_pid(event, current);\n\tmmap_event->event_id.tid = perf_event_tid(event, current);\n\n\tperf_output_put(&handle, mmap_event->event_id);\n\t__output_copy(&handle, mmap_event->file_name,\n\t\t\t\t   mmap_event->file_size);\n\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\nout:\n\tmmap_event->event_id.header.size = size;\n}\n\nstatic int perf_event_mmap_match(struct perf_event *event,\n\t\t\t\t   struct perf_mmap_event *mmap_event,\n\t\t\t\t   int executable)\n{\n\tif (event->state < PERF_EVENT_STATE_INACTIVE)\n\t\treturn 0;\n\n\tif (!event_filter_match(event))\n\t\treturn 0;\n\n\tif ((!executable && event->attr.mmap_data) ||\n\t    (executable && event->attr.mmap))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void perf_event_mmap_ctx(struct perf_event_context *ctx,\n\t\t\t\t  struct perf_mmap_event *mmap_event,\n\t\t\t\t  int executable)\n{\n\tstruct perf_event *event;\n\n\tlist_for_each_entry_rcu(event, &ctx->event_list, event_entry) {\n\t\tif (perf_event_mmap_match(event, mmap_event, executable))\n\t\t\tperf_event_mmap_output(event, mmap_event);\n\t}\n}\n\nstatic void perf_event_mmap_event(struct perf_mmap_event *mmap_event)\n{\n\tstruct perf_cpu_context *cpuctx;\n\tstruct perf_event_context *ctx;\n\tstruct vm_area_struct *vma = mmap_event->vma;\n\tstruct file *file = vma->vm_file;\n\tunsigned int size;\n\tchar tmp[16];\n\tchar *buf = NULL;\n\tconst char *name;\n\tstruct pmu *pmu;\n\tint ctxn;\n\n\tmemset(tmp, 0, sizeof(tmp));\n\n\tif (file) {\n\t\t/*\n\t\t * d_path works from the end of the rb backwards, so we\n\t\t * need to add enough zero bytes after the string to handle\n\t\t * the 64bit alignment we do later.\n\t\t */\n\t\tbuf = kzalloc(PATH_MAX + sizeof(u64), GFP_KERNEL);\n\t\tif (!buf) {\n\t\t\tname = strncpy(tmp, \"//enomem\", sizeof(tmp));\n\t\t\tgoto got_name;\n\t\t}\n\t\tname = d_path(&file->f_path, buf, PATH_MAX);\n\t\tif (IS_ERR(name)) {\n\t\t\tname = strncpy(tmp, \"//toolong\", sizeof(tmp));\n\t\t\tgoto got_name;\n\t\t}\n\t} else {\n\t\tif (arch_vma_name(mmap_event->vma)) {\n\t\t\tname = strncpy(tmp, arch_vma_name(mmap_event->vma),\n\t\t\t\t       sizeof(tmp));\n\t\t\tgoto got_name;\n\t\t}\n\n\t\tif (!vma->vm_mm) {\n\t\t\tname = strncpy(tmp, \"[vdso]\", sizeof(tmp));\n\t\t\tgoto got_name;\n\t\t} else if (vma->vm_start <= vma->vm_mm->start_brk &&\n\t\t\t\tvma->vm_end >= vma->vm_mm->brk) {\n\t\t\tname = strncpy(tmp, \"[heap]\", sizeof(tmp));\n\t\t\tgoto got_name;\n\t\t} else if (vma->vm_start <= vma->vm_mm->start_stack &&\n\t\t\t\tvma->vm_end >= vma->vm_mm->start_stack) {\n\t\t\tname = strncpy(tmp, \"[stack]\", sizeof(tmp));\n\t\t\tgoto got_name;\n\t\t}\n\n\t\tname = strncpy(tmp, \"//anon\", sizeof(tmp));\n\t\tgoto got_name;\n\t}\n\ngot_name:\n\tsize = ALIGN(strlen(name)+1, sizeof(u64));\n\n\tmmap_event->file_name = name;\n\tmmap_event->file_size = size;\n\n\tmmap_event->event_id.header.size = sizeof(mmap_event->event_id) + size;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(pmu, &pmus, entry) {\n\t\tcpuctx = get_cpu_ptr(pmu->pmu_cpu_context);\n\t\tif (cpuctx->active_pmu != pmu)\n\t\t\tgoto next;\n\t\tperf_event_mmap_ctx(&cpuctx->ctx, mmap_event,\n\t\t\t\t\tvma->vm_flags & VM_EXEC);\n\n\t\tctxn = pmu->task_ctx_nr;\n\t\tif (ctxn < 0)\n\t\t\tgoto next;\n\n\t\tctx = rcu_dereference(current->perf_event_ctxp[ctxn]);\n\t\tif (ctx) {\n\t\t\tperf_event_mmap_ctx(ctx, mmap_event,\n\t\t\t\t\tvma->vm_flags & VM_EXEC);\n\t\t}\nnext:\n\t\tput_cpu_ptr(pmu->pmu_cpu_context);\n\t}\n\trcu_read_unlock();\n\n\tkfree(buf);\n}\n\nvoid perf_event_mmap(struct vm_area_struct *vma)\n{\n\tstruct perf_mmap_event mmap_event;\n\n\tif (!atomic_read(&nr_mmap_events))\n\t\treturn;\n\n\tmmap_event = (struct perf_mmap_event){\n\t\t.vma\t= vma,\n\t\t/* .file_name */\n\t\t/* .file_size */\n\t\t.event_id  = {\n\t\t\t.header = {\n\t\t\t\t.type = PERF_RECORD_MMAP,\n\t\t\t\t.misc = PERF_RECORD_MISC_USER,\n\t\t\t\t/* .size */\n\t\t\t},\n\t\t\t/* .pid */\n\t\t\t/* .tid */\n\t\t\t.start  = vma->vm_start,\n\t\t\t.len    = vma->vm_end - vma->vm_start,\n\t\t\t.pgoff  = (u64)vma->vm_pgoff << PAGE_SHIFT,\n\t\t},\n\t};\n\n\tperf_event_mmap_event(&mmap_event);\n}\n\n/*\n * IRQ throttle logging\n */\n\nstatic void perf_log_throttle(struct perf_event *event, int enable)\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tint ret;\n\n\tstruct {\n\t\tstruct perf_event_header\theader;\n\t\tu64\t\t\t\ttime;\n\t\tu64\t\t\t\tid;\n\t\tu64\t\t\t\tstream_id;\n\t} throttle_event = {\n\t\t.header = {\n\t\t\t.type = PERF_RECORD_THROTTLE,\n\t\t\t.misc = 0,\n\t\t\t.size = sizeof(throttle_event),\n\t\t},\n\t\t.time\t\t= perf_clock(),\n\t\t.id\t\t= primary_event_id(event),\n\t\t.stream_id\t= event->id,\n\t};\n\n\tif (enable)\n\t\tthrottle_event.header.type = PERF_RECORD_UNTHROTTLE;\n\n\tperf_event_header__init_id(&throttle_event.header, &sample, event);\n\n\tret = perf_output_begin(&handle, event,\n\t\t\t\tthrottle_event.header.size, 0);\n\tif (ret)\n\t\treturn;\n\n\tperf_output_put(&handle, throttle_event);\n\tperf_event__output_id_sample(event, &handle, &sample);\n\tperf_output_end(&handle);\n}\n\n/*\n * Generic event overflow handling, sampling.\n */\n\nstatic int __perf_event_overflow(struct perf_event *event,\n\t\t\t\t   int throttle, struct perf_sample_data *data,\n\t\t\t\t   struct pt_regs *regs)\n{\n\tint events = atomic_read(&event->event_limit);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint ret = 0;\n\n\t/*\n\t * Non-sampling counters might still use the PMI to fold short\n\t * hardware counters, ignore those.\n\t */\n\tif (unlikely(!is_sampling_event(event)))\n\t\treturn 0;\n\n\tif (unlikely(hwc->interrupts >= max_samples_per_tick)) {\n\t\tif (throttle) {\n\t\t\thwc->interrupts = MAX_INTERRUPTS;\n\t\t\tperf_log_throttle(event, 0);\n\t\t\tret = 1;\n\t\t}\n\t} else\n\t\thwc->interrupts++;\n\n\tif (event->attr.freq) {\n\t\tu64 now = perf_clock();\n\t\ts64 delta = now - hwc->freq_time_stamp;\n\n\t\thwc->freq_time_stamp = now;\n\n\t\tif (delta > 0 && delta < 2*TICK_NSEC)\n\t\t\tperf_adjust_period(event, delta, hwc->last_period);\n\t}\n\n\t/*\n\t * XXX event_limit might not quite work as expected on inherited\n\t * events\n\t */\n\n\tevent->pending_kill = POLL_IN;\n\tif (events && atomic_dec_and_test(&event->event_limit)) {\n\t\tret = 1;\n\t\tevent->pending_kill = POLL_HUP;\n\t\tevent->pending_disable = 1;\n\t\tirq_work_queue(&event->pending);\n\t}\n\n\tif (event->overflow_handler)\n\t\tevent->overflow_handler(event, data, regs);\n\telse\n\t\tperf_event_output(event, data, regs);\n\n\tif (event->fasync && event->pending_kill) {\n\t\tevent->pending_wakeup = 1;\n\t\tirq_work_queue(&event->pending);\n\t}\n\n\treturn ret;\n}\n\nint perf_event_overflow(struct perf_event *event,\n\t\t\t  struct perf_sample_data *data,\n\t\t\t  struct pt_regs *regs)\n{\n\treturn __perf_event_overflow(event, 1, data, regs);\n}\n\n/*\n * Generic software event infrastructure\n */\n\nstruct swevent_htable {\n\tstruct swevent_hlist\t\t*swevent_hlist;\n\tstruct mutex\t\t\thlist_mutex;\n\tint\t\t\t\thlist_refcount;\n\n\t/* Recursion avoidance in each contexts */\n\tint\t\t\t\trecursion[PERF_NR_CONTEXTS];\n};\n\nstatic DEFINE_PER_CPU(struct swevent_htable, swevent_htable);\n\n/*\n * We directly increment event->count and keep a second value in\n * event->hw.period_left to count intervals. This period event\n * is kept in the range [-sample_period, 0] so that we can use the\n * sign as trigger.\n */\n\nstatic u64 perf_swevent_set_period(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 period = hwc->last_period;\n\tu64 nr, offset;\n\ts64 old, val;\n\n\thwc->last_period = hwc->sample_period;\n\nagain:\n\told = val = local64_read(&hwc->period_left);\n\tif (val < 0)\n\t\treturn 0;\n\n\tnr = div64_u64(period + val, period);\n\toffset = nr * period;\n\tval -= offset;\n\tif (local64_cmpxchg(&hwc->period_left, old, val) != old)\n\t\tgoto again;\n\n\treturn nr;\n}\n\nstatic void perf_swevent_overflow(struct perf_event *event, u64 overflow,\n\t\t\t\t    struct perf_sample_data *data,\n\t\t\t\t    struct pt_regs *regs)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint throttle = 0;\n\n\tdata->period = event->hw.last_period;\n\tif (!overflow)\n\t\toverflow = perf_swevent_set_period(event);\n\n\tif (hwc->interrupts == MAX_INTERRUPTS)\n\t\treturn;\n\n\tfor (; overflow; overflow--) {\n\t\tif (__perf_event_overflow(event, throttle,\n\t\t\t\t\t    data, regs)) {\n\t\t\t/*\n\t\t\t * We inhibit the overflow from happening when\n\t\t\t * hwc->interrupts == MAX_INTERRUPTS.\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\t\tthrottle = 1;\n\t}\n}\n\nstatic void perf_swevent_event(struct perf_event *event, u64 nr,\n\t\t\t       struct perf_sample_data *data,\n\t\t\t       struct pt_regs *regs)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tlocal64_add(nr, &event->count);\n\n\tif (!regs)\n\t\treturn;\n\n\tif (!is_sampling_event(event))\n\t\treturn;\n\n\tif (nr == 1 && hwc->sample_period == 1 && !event->attr.freq)\n\t\treturn perf_swevent_overflow(event, 1, data, regs);\n\n\tif (local64_add_negative(nr, &hwc->period_left))\n\t\treturn;\n\n\tperf_swevent_overflow(event, 0, data, regs);\n}\n\nstatic int perf_exclude_event(struct perf_event *event,\n\t\t\t      struct pt_regs *regs)\n{\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn 1;\n\n\tif (regs) {\n\t\tif (event->attr.exclude_user && user_mode(regs))\n\t\t\treturn 1;\n\n\t\tif (event->attr.exclude_kernel && !user_mode(regs))\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int perf_swevent_match(struct perf_event *event,\n\t\t\t\tenum perf_type_id type,\n\t\t\t\tu32 event_id,\n\t\t\t\tstruct perf_sample_data *data,\n\t\t\t\tstruct pt_regs *regs)\n{\n\tif (event->attr.type != type)\n\t\treturn 0;\n\n\tif (event->attr.config != event_id)\n\t\treturn 0;\n\n\tif (perf_exclude_event(event, regs))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic inline u64 swevent_hash(u64 type, u32 event_id)\n{\n\tu64 val = event_id | (type << 32);\n\n\treturn hash_64(val, SWEVENT_HLIST_BITS);\n}\n\nstatic inline struct hlist_head *\n__find_swevent_head(struct swevent_hlist *hlist, u64 type, u32 event_id)\n{\n\tu64 hash = swevent_hash(type, event_id);\n\n\treturn &hlist->heads[hash];\n}\n\n/* For the read side: events when they trigger */\nstatic inline struct hlist_head *\nfind_swevent_head_rcu(struct swevent_htable *swhash, u64 type, u32 event_id)\n{\n\tstruct swevent_hlist *hlist;\n\n\thlist = rcu_dereference(swhash->swevent_hlist);\n\tif (!hlist)\n\t\treturn NULL;\n\n\treturn __find_swevent_head(hlist, type, event_id);\n}\n\n/* For the event head insertion and removal in the hlist */\nstatic inline struct hlist_head *\nfind_swevent_head(struct swevent_htable *swhash, struct perf_event *event)\n{\n\tstruct swevent_hlist *hlist;\n\tu32 event_id = event->attr.config;\n\tu64 type = event->attr.type;\n\n\t/*\n\t * Event scheduling is always serialized against hlist allocation\n\t * and release. Which makes the protected version suitable here.\n\t * The context lock guarantees that.\n\t */\n\thlist = rcu_dereference_protected(swhash->swevent_hlist,\n\t\t\t\t\t  lockdep_is_held(&event->ctx->lock));\n\tif (!hlist)\n\t\treturn NULL;\n\n\treturn __find_swevent_head(hlist, type, event_id);\n}\n\nstatic void do_perf_sw_event(enum perf_type_id type, u32 event_id,\n\t\t\t\t    u64 nr,\n\t\t\t\t    struct perf_sample_data *data,\n\t\t\t\t    struct pt_regs *regs)\n{\n\tstruct swevent_htable *swhash = &__get_cpu_var(swevent_htable);\n\tstruct perf_event *event;\n\tstruct hlist_node *node;\n\tstruct hlist_head *head;\n\n\trcu_read_lock();\n\thead = find_swevent_head_rcu(swhash, type, event_id);\n\tif (!head)\n\t\tgoto end;\n\n\thlist_for_each_entry_rcu(event, node, head, hlist_entry) {\n\t\tif (perf_swevent_match(event, type, event_id, data, regs))\n\t\t\tperf_swevent_event(event, nr, data, regs);\n\t}\nend:\n\trcu_read_unlock();\n}\n\nint perf_swevent_get_recursion_context(void)\n{\n\tstruct swevent_htable *swhash = &__get_cpu_var(swevent_htable);\n\n\treturn get_recursion_context(swhash->recursion);\n}\nEXPORT_SYMBOL_GPL(perf_swevent_get_recursion_context);\n\ninline void perf_swevent_put_recursion_context(int rctx)\n{\n\tstruct swevent_htable *swhash = &__get_cpu_var(swevent_htable);\n\n\tput_recursion_context(swhash->recursion, rctx);\n}\n\nvoid __perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)\n{\n\tstruct perf_sample_data data;\n\tint rctx;\n\n\tpreempt_disable_notrace();\n\trctx = perf_swevent_get_recursion_context();\n\tif (rctx < 0)\n\t\treturn;\n\n\tperf_sample_data_init(&data, addr);\n\n\tdo_perf_sw_event(PERF_TYPE_SOFTWARE, event_id, nr, &data, regs);\n\n\tperf_swevent_put_recursion_context(rctx);\n\tpreempt_enable_notrace();\n}\n\nstatic void perf_swevent_read(struct perf_event *event)\n{\n}\n\nstatic int perf_swevent_add(struct perf_event *event, int flags)\n{\n\tstruct swevent_htable *swhash = &__get_cpu_var(swevent_htable);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct hlist_head *head;\n\n\tif (is_sampling_event(event)) {\n\t\thwc->last_period = hwc->sample_period;\n\t\tperf_swevent_set_period(event);\n\t}\n\n\thwc->state = !(flags & PERF_EF_START);\n\n\thead = find_swevent_head(swhash, event);\n\tif (WARN_ON_ONCE(!head))\n\t\treturn -EINVAL;\n\n\thlist_add_head_rcu(&event->hlist_entry, head);\n\n\treturn 0;\n}\n\nstatic void perf_swevent_del(struct perf_event *event, int flags)\n{\n\thlist_del_rcu(&event->hlist_entry);\n}\n\nstatic void perf_swevent_start(struct perf_event *event, int flags)\n{\n\tevent->hw.state = 0;\n}\n\nstatic void perf_swevent_stop(struct perf_event *event, int flags)\n{\n\tevent->hw.state = PERF_HES_STOPPED;\n}\n\n/* Deref the hlist from the update side */\nstatic inline struct swevent_hlist *\nswevent_hlist_deref(struct swevent_htable *swhash)\n{\n\treturn rcu_dereference_protected(swhash->swevent_hlist,\n\t\t\t\t\t lockdep_is_held(&swhash->hlist_mutex));\n}\n\nstatic void swevent_hlist_release(struct swevent_htable *swhash)\n{\n\tstruct swevent_hlist *hlist = swevent_hlist_deref(swhash);\n\n\tif (!hlist)\n\t\treturn;\n\n\trcu_assign_pointer(swhash->swevent_hlist, NULL);\n\tkfree_rcu(hlist, rcu_head);\n}\n\nstatic void swevent_hlist_put_cpu(struct perf_event *event, int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\n\tif (!--swhash->hlist_refcount)\n\t\tswevent_hlist_release(swhash);\n\n\tmutex_unlock(&swhash->hlist_mutex);\n}\n\nstatic void swevent_hlist_put(struct perf_event *event)\n{\n\tint cpu;\n\n\tif (event->cpu != -1) {\n\t\tswevent_hlist_put_cpu(event, event->cpu);\n\t\treturn;\n\t}\n\n\tfor_each_possible_cpu(cpu)\n\t\tswevent_hlist_put_cpu(event, cpu);\n}\n\nstatic int swevent_hlist_get_cpu(struct perf_event *event, int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\tint err = 0;\n\n\tmutex_lock(&swhash->hlist_mutex);\n\n\tif (!swevent_hlist_deref(swhash) && cpu_online(cpu)) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc(sizeof(*hlist), GFP_KERNEL);\n\t\tif (!hlist) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto exit;\n\t\t}\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tswhash->hlist_refcount++;\nexit:\n\tmutex_unlock(&swhash->hlist_mutex);\n\n\treturn err;\n}\n\nstatic int swevent_hlist_get(struct perf_event *event)\n{\n\tint err;\n\tint cpu, failed_cpu;\n\n\tif (event->cpu != -1)\n\t\treturn swevent_hlist_get_cpu(event, event->cpu);\n\n\tget_online_cpus();\n\tfor_each_possible_cpu(cpu) {\n\t\terr = swevent_hlist_get_cpu(event, cpu);\n\t\tif (err) {\n\t\t\tfailed_cpu = cpu;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\tput_online_cpus();\n\n\treturn 0;\nfail:\n\tfor_each_possible_cpu(cpu) {\n\t\tif (cpu == failed_cpu)\n\t\t\tbreak;\n\t\tswevent_hlist_put_cpu(event, cpu);\n\t}\n\n\tput_online_cpus();\n\treturn err;\n}\n\nstruct jump_label_key perf_swevent_enabled[PERF_COUNT_SW_MAX];\n\nstatic void sw_perf_event_destroy(struct perf_event *event)\n{\n\tu64 event_id = event->attr.config;\n\n\tWARN_ON(event->parent);\n\n\tjump_label_dec(&perf_swevent_enabled[event_id]);\n\tswevent_hlist_put(event);\n}\n\nstatic int perf_swevent_init(struct perf_event *event)\n{\n\tint event_id = event->attr.config;\n\n\tif (event->attr.type != PERF_TYPE_SOFTWARE)\n\t\treturn -ENOENT;\n\n\tswitch (event_id) {\n\tcase PERF_COUNT_SW_CPU_CLOCK:\n\tcase PERF_COUNT_SW_TASK_CLOCK:\n\t\treturn -ENOENT;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (event_id >= PERF_COUNT_SW_MAX)\n\t\treturn -ENOENT;\n\n\tif (!event->parent) {\n\t\tint err;\n\n\t\terr = swevent_hlist_get(event);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tjump_label_inc(&perf_swevent_enabled[event_id]);\n\t\tevent->destroy = sw_perf_event_destroy;\n\t}\n\n\treturn 0;\n}\n\nstatic struct pmu perf_swevent = {\n\t.task_ctx_nr\t= perf_sw_context,\n\n\t.event_init\t= perf_swevent_init,\n\t.add\t\t= perf_swevent_add,\n\t.del\t\t= perf_swevent_del,\n\t.start\t\t= perf_swevent_start,\n\t.stop\t\t= perf_swevent_stop,\n\t.read\t\t= perf_swevent_read,\n};\n\n#ifdef CONFIG_EVENT_TRACING\n\nstatic int perf_tp_filter_match(struct perf_event *event,\n\t\t\t\tstruct perf_sample_data *data)\n{\n\tvoid *record = data->raw->data;\n\n\tif (likely(!event->filter) || filter_match_preds(event->filter, record))\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic int perf_tp_event_match(struct perf_event *event,\n\t\t\t\tstruct perf_sample_data *data,\n\t\t\t\tstruct pt_regs *regs)\n{\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn 0;\n\t/*\n\t * All tracepoints are from kernel-space.\n\t */\n\tif (event->attr.exclude_kernel)\n\t\treturn 0;\n\n\tif (!perf_tp_filter_match(event, data))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nvoid perf_tp_event(u64 addr, u64 count, void *record, int entry_size,\n\t\t   struct pt_regs *regs, struct hlist_head *head, int rctx)\n{\n\tstruct perf_sample_data data;\n\tstruct perf_event *event;\n\tstruct hlist_node *node;\n\n\tstruct perf_raw_record raw = {\n\t\t.size = entry_size,\n\t\t.data = record,\n\t};\n\n\tperf_sample_data_init(&data, addr);\n\tdata.raw = &raw;\n\n\thlist_for_each_entry_rcu(event, node, head, hlist_entry) {\n\t\tif (perf_tp_event_match(event, &data, regs))\n\t\t\tperf_swevent_event(event, count, &data, regs);\n\t}\n\n\tperf_swevent_put_recursion_context(rctx);\n}\nEXPORT_SYMBOL_GPL(perf_tp_event);\n\nstatic void tp_perf_event_destroy(struct perf_event *event)\n{\n\tperf_trace_destroy(event);\n}\n\nstatic int perf_tp_event_init(struct perf_event *event)\n{\n\tint err;\n\n\tif (event->attr.type != PERF_TYPE_TRACEPOINT)\n\t\treturn -ENOENT;\n\n\terr = perf_trace_init(event);\n\tif (err)\n\t\treturn err;\n\n\tevent->destroy = tp_perf_event_destroy;\n\n\treturn 0;\n}\n\nstatic struct pmu perf_tracepoint = {\n\t.task_ctx_nr\t= perf_sw_context,\n\n\t.event_init\t= perf_tp_event_init,\n\t.add\t\t= perf_trace_add,\n\t.del\t\t= perf_trace_del,\n\t.start\t\t= perf_swevent_start,\n\t.stop\t\t= perf_swevent_stop,\n\t.read\t\t= perf_swevent_read,\n};\n\nstatic inline void perf_tp_register(void)\n{\n\tperf_pmu_register(&perf_tracepoint, \"tracepoint\", PERF_TYPE_TRACEPOINT);\n}\n\nstatic int perf_event_set_filter(struct perf_event *event, void __user *arg)\n{\n\tchar *filter_str;\n\tint ret;\n\n\tif (event->attr.type != PERF_TYPE_TRACEPOINT)\n\t\treturn -EINVAL;\n\n\tfilter_str = strndup_user(arg, PAGE_SIZE);\n\tif (IS_ERR(filter_str))\n\t\treturn PTR_ERR(filter_str);\n\n\tret = ftrace_profile_set_filter(event, event->attr.config, filter_str);\n\n\tkfree(filter_str);\n\treturn ret;\n}\n\nstatic void perf_event_free_filter(struct perf_event *event)\n{\n\tftrace_profile_free_filter(event);\n}\n\n#else\n\nstatic inline void perf_tp_register(void)\n{\n}\n\nstatic int perf_event_set_filter(struct perf_event *event, void __user *arg)\n{\n\treturn -ENOENT;\n}\n\nstatic void perf_event_free_filter(struct perf_event *event)\n{\n}\n\n#endif /* CONFIG_EVENT_TRACING */\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\nvoid perf_bp_event(struct perf_event *bp, void *data)\n{\n\tstruct perf_sample_data sample;\n\tstruct pt_regs *regs = data;\n\n\tperf_sample_data_init(&sample, bp->attr.bp_addr);\n\n\tif (!bp->hw.state && !perf_exclude_event(bp, regs))\n\t\tperf_swevent_event(bp, 1, &sample, regs);\n}\n#endif\n\n/*\n * hrtimer based swevent callback\n */\n\nstatic enum hrtimer_restart perf_swevent_hrtimer(struct hrtimer *hrtimer)\n{\n\tenum hrtimer_restart ret = HRTIMER_RESTART;\n\tstruct perf_sample_data data;\n\tstruct pt_regs *regs;\n\tstruct perf_event *event;\n\tu64 period;\n\n\tevent = container_of(hrtimer, struct perf_event, hw.hrtimer);\n\n\tif (event->state != PERF_EVENT_STATE_ACTIVE)\n\t\treturn HRTIMER_NORESTART;\n\n\tevent->pmu->read(event);\n\n\tperf_sample_data_init(&data, 0);\n\tdata.period = event->hw.last_period;\n\tregs = get_irq_regs();\n\n\tif (regs && !perf_exclude_event(event, regs)) {\n\t\tif (!(event->attr.exclude_idle && current->pid == 0))\n\t\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\t\tret = HRTIMER_NORESTART;\n\t}\n\n\tperiod = max_t(u64, 10000, event->hw.sample_period);\n\thrtimer_forward_now(hrtimer, ns_to_ktime(period));\n\n\treturn ret;\n}\n\nstatic void perf_swevent_start_hrtimer(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\ts64 period;\n\n\tif (!is_sampling_event(event))\n\t\treturn;\n\n\tperiod = local64_read(&hwc->period_left);\n\tif (period) {\n\t\tif (period < 0)\n\t\t\tperiod = 10000;\n\n\t\tlocal64_set(&hwc->period_left, 0);\n\t} else {\n\t\tperiod = max_t(u64, 10000, hwc->sample_period);\n\t}\n\t__hrtimer_start_range_ns(&hwc->hrtimer,\n\t\t\t\tns_to_ktime(period), 0,\n\t\t\t\tHRTIMER_MODE_REL_PINNED, 0);\n}\n\nstatic void perf_swevent_cancel_hrtimer(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (is_sampling_event(event)) {\n\t\tktime_t remaining = hrtimer_get_remaining(&hwc->hrtimer);\n\t\tlocal64_set(&hwc->period_left, ktime_to_ns(remaining));\n\n\t\thrtimer_cancel(&hwc->hrtimer);\n\t}\n}\n\nstatic void perf_swevent_init_hrtimer(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (!is_sampling_event(event))\n\t\treturn;\n\n\thrtimer_init(&hwc->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\thwc->hrtimer.function = perf_swevent_hrtimer;\n\n\t/*\n\t * Since hrtimers have a fixed rate, we can do a static freq->period\n\t * mapping and avoid the whole period adjust feedback stuff.\n\t */\n\tif (event->attr.freq) {\n\t\tlong freq = event->attr.sample_freq;\n\n\t\tevent->attr.sample_period = NSEC_PER_SEC / freq;\n\t\thwc->sample_period = event->attr.sample_period;\n\t\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\t\tevent->attr.freq = 0;\n\t}\n}\n\n/*\n * Software event: cpu wall time clock\n */\n\nstatic void cpu_clock_event_update(struct perf_event *event)\n{\n\ts64 prev;\n\tu64 now;\n\n\tnow = local_clock();\n\tprev = local64_xchg(&event->hw.prev_count, now);\n\tlocal64_add(now - prev, &event->count);\n}\n\nstatic void cpu_clock_event_start(struct perf_event *event, int flags)\n{\n\tlocal64_set(&event->hw.prev_count, local_clock());\n\tperf_swevent_start_hrtimer(event);\n}\n\nstatic void cpu_clock_event_stop(struct perf_event *event, int flags)\n{\n\tperf_swevent_cancel_hrtimer(event);\n\tcpu_clock_event_update(event);\n}\n\nstatic int cpu_clock_event_add(struct perf_event *event, int flags)\n{\n\tif (flags & PERF_EF_START)\n\t\tcpu_clock_event_start(event, flags);\n\n\treturn 0;\n}\n\nstatic void cpu_clock_event_del(struct perf_event *event, int flags)\n{\n\tcpu_clock_event_stop(event, flags);\n}\n\nstatic void cpu_clock_event_read(struct perf_event *event)\n{\n\tcpu_clock_event_update(event);\n}\n\nstatic int cpu_clock_event_init(struct perf_event *event)\n{\n\tif (event->attr.type != PERF_TYPE_SOFTWARE)\n\t\treturn -ENOENT;\n\n\tif (event->attr.config != PERF_COUNT_SW_CPU_CLOCK)\n\t\treturn -ENOENT;\n\n\tperf_swevent_init_hrtimer(event);\n\n\treturn 0;\n}\n\nstatic struct pmu perf_cpu_clock = {\n\t.task_ctx_nr\t= perf_sw_context,\n\n\t.event_init\t= cpu_clock_event_init,\n\t.add\t\t= cpu_clock_event_add,\n\t.del\t\t= cpu_clock_event_del,\n\t.start\t\t= cpu_clock_event_start,\n\t.stop\t\t= cpu_clock_event_stop,\n\t.read\t\t= cpu_clock_event_read,\n};\n\n/*\n * Software event: task time clock\n */\n\nstatic void task_clock_event_update(struct perf_event *event, u64 now)\n{\n\tu64 prev;\n\ts64 delta;\n\n\tprev = local64_xchg(&event->hw.prev_count, now);\n\tdelta = now - prev;\n\tlocal64_add(delta, &event->count);\n}\n\nstatic void task_clock_event_start(struct perf_event *event, int flags)\n{\n\tlocal64_set(&event->hw.prev_count, event->ctx->time);\n\tperf_swevent_start_hrtimer(event);\n}\n\nstatic void task_clock_event_stop(struct perf_event *event, int flags)\n{\n\tperf_swevent_cancel_hrtimer(event);\n\ttask_clock_event_update(event, event->ctx->time);\n}\n\nstatic int task_clock_event_add(struct perf_event *event, int flags)\n{\n\tif (flags & PERF_EF_START)\n\t\ttask_clock_event_start(event, flags);\n\n\treturn 0;\n}\n\nstatic void task_clock_event_del(struct perf_event *event, int flags)\n{\n\ttask_clock_event_stop(event, PERF_EF_UPDATE);\n}\n\nstatic void task_clock_event_read(struct perf_event *event)\n{\n\tu64 now = perf_clock();\n\tu64 delta = now - event->ctx->timestamp;\n\tu64 time = event->ctx->time + delta;\n\n\ttask_clock_event_update(event, time);\n}\n\nstatic int task_clock_event_init(struct perf_event *event)\n{\n\tif (event->attr.type != PERF_TYPE_SOFTWARE)\n\t\treturn -ENOENT;\n\n\tif (event->attr.config != PERF_COUNT_SW_TASK_CLOCK)\n\t\treturn -ENOENT;\n\n\tperf_swevent_init_hrtimer(event);\n\n\treturn 0;\n}\n\nstatic struct pmu perf_task_clock = {\n\t.task_ctx_nr\t= perf_sw_context,\n\n\t.event_init\t= task_clock_event_init,\n\t.add\t\t= task_clock_event_add,\n\t.del\t\t= task_clock_event_del,\n\t.start\t\t= task_clock_event_start,\n\t.stop\t\t= task_clock_event_stop,\n\t.read\t\t= task_clock_event_read,\n};\n\nstatic void perf_pmu_nop_void(struct pmu *pmu)\n{\n}\n\nstatic int perf_pmu_nop_int(struct pmu *pmu)\n{\n\treturn 0;\n}\n\nstatic void perf_pmu_start_txn(struct pmu *pmu)\n{\n\tperf_pmu_disable(pmu);\n}\n\nstatic int perf_pmu_commit_txn(struct pmu *pmu)\n{\n\tperf_pmu_enable(pmu);\n\treturn 0;\n}\n\nstatic void perf_pmu_cancel_txn(struct pmu *pmu)\n{\n\tperf_pmu_enable(pmu);\n}\n\n/*\n * Ensures all contexts with the same task_ctx_nr have the same\n * pmu_cpu_context too.\n */\nstatic void *find_pmu_context(int ctxn)\n{\n\tstruct pmu *pmu;\n\n\tif (ctxn < 0)\n\t\treturn NULL;\n\n\tlist_for_each_entry(pmu, &pmus, entry) {\n\t\tif (pmu->task_ctx_nr == ctxn)\n\t\t\treturn pmu->pmu_cpu_context;\n\t}\n\n\treturn NULL;\n}\n\nstatic void update_pmu_context(struct pmu *pmu, struct pmu *old_pmu)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct perf_cpu_context *cpuctx;\n\n\t\tcpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);\n\n\t\tif (cpuctx->active_pmu == old_pmu)\n\t\t\tcpuctx->active_pmu = pmu;\n\t}\n}\n\nstatic void free_pmu_context(struct pmu *pmu)\n{\n\tstruct pmu *i;\n\n\tmutex_lock(&pmus_lock);\n\t/*\n\t * Like a real lame refcount.\n\t */\n\tlist_for_each_entry(i, &pmus, entry) {\n\t\tif (i->pmu_cpu_context == pmu->pmu_cpu_context) {\n\t\t\tupdate_pmu_context(i, pmu);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tfree_percpu(pmu->pmu_cpu_context);\nout:\n\tmutex_unlock(&pmus_lock);\n}\nstatic struct idr pmu_idr;\n\nstatic ssize_t\ntype_show(struct device *dev, struct device_attribute *attr, char *page)\n{\n\tstruct pmu *pmu = dev_get_drvdata(dev);\n\n\treturn snprintf(page, PAGE_SIZE-1, \"%d\\n\", pmu->type);\n}\n\nstatic struct device_attribute pmu_dev_attrs[] = {\n       __ATTR_RO(type),\n       __ATTR_NULL,\n};\n\nstatic int pmu_bus_running;\nstatic struct bus_type pmu_bus = {\n\t.name\t\t= \"event_source\",\n\t.dev_attrs\t= pmu_dev_attrs,\n};\n\nstatic void pmu_dev_release(struct device *dev)\n{\n\tkfree(dev);\n}\n\nstatic int pmu_dev_alloc(struct pmu *pmu)\n{\n\tint ret = -ENOMEM;\n\n\tpmu->dev = kzalloc(sizeof(struct device), GFP_KERNEL);\n\tif (!pmu->dev)\n\t\tgoto out;\n\n\tdevice_initialize(pmu->dev);\n\tret = dev_set_name(pmu->dev, \"%s\", pmu->name);\n\tif (ret)\n\t\tgoto free_dev;\n\n\tdev_set_drvdata(pmu->dev, pmu);\n\tpmu->dev->bus = &pmu_bus;\n\tpmu->dev->release = pmu_dev_release;\n\tret = device_add(pmu->dev);\n\tif (ret)\n\t\tgoto free_dev;\n\nout:\n\treturn ret;\n\nfree_dev:\n\tput_device(pmu->dev);\n\tgoto out;\n}\n\nstatic struct lock_class_key cpuctx_mutex;\nstatic struct lock_class_key cpuctx_lock;\n\nint perf_pmu_register(struct pmu *pmu, char *name, int type)\n{\n\tint cpu, ret;\n\n\tmutex_lock(&pmus_lock);\n\tret = -ENOMEM;\n\tpmu->pmu_disable_count = alloc_percpu(int);\n\tif (!pmu->pmu_disable_count)\n\t\tgoto unlock;\n\n\tpmu->type = -1;\n\tif (!name)\n\t\tgoto skip_type;\n\tpmu->name = name;\n\n\tif (type < 0) {\n\t\tint err = idr_pre_get(&pmu_idr, GFP_KERNEL);\n\t\tif (!err)\n\t\t\tgoto free_pdc;\n\n\t\terr = idr_get_new_above(&pmu_idr, pmu, PERF_TYPE_MAX, &type);\n\t\tif (err) {\n\t\t\tret = err;\n\t\t\tgoto free_pdc;\n\t\t}\n\t}\n\tpmu->type = type;\n\n\tif (pmu_bus_running) {\n\t\tret = pmu_dev_alloc(pmu);\n\t\tif (ret)\n\t\t\tgoto free_idr;\n\t}\n\nskip_type:\n\tpmu->pmu_cpu_context = find_pmu_context(pmu->task_ctx_nr);\n\tif (pmu->pmu_cpu_context)\n\t\tgoto got_cpu_context;\n\n\tpmu->pmu_cpu_context = alloc_percpu(struct perf_cpu_context);\n\tif (!pmu->pmu_cpu_context)\n\t\tgoto free_dev;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct perf_cpu_context *cpuctx;\n\n\t\tcpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);\n\t\t__perf_event_init_context(&cpuctx->ctx);\n\t\tlockdep_set_class(&cpuctx->ctx.mutex, &cpuctx_mutex);\n\t\tlockdep_set_class(&cpuctx->ctx.lock, &cpuctx_lock);\n\t\tcpuctx->ctx.type = cpu_context;\n\t\tcpuctx->ctx.pmu = pmu;\n\t\tcpuctx->jiffies_interval = 1;\n\t\tINIT_LIST_HEAD(&cpuctx->rotation_list);\n\t\tcpuctx->active_pmu = pmu;\n\t}\n\ngot_cpu_context:\n\tif (!pmu->start_txn) {\n\t\tif (pmu->pmu_enable) {\n\t\t\t/*\n\t\t\t * If we have pmu_enable/pmu_disable calls, install\n\t\t\t * transaction stubs that use that to try and batch\n\t\t\t * hardware accesses.\n\t\t\t */\n\t\t\tpmu->start_txn  = perf_pmu_start_txn;\n\t\t\tpmu->commit_txn = perf_pmu_commit_txn;\n\t\t\tpmu->cancel_txn = perf_pmu_cancel_txn;\n\t\t} else {\n\t\t\tpmu->start_txn  = perf_pmu_nop_void;\n\t\t\tpmu->commit_txn = perf_pmu_nop_int;\n\t\t\tpmu->cancel_txn = perf_pmu_nop_void;\n\t\t}\n\t}\n\n\tif (!pmu->pmu_enable) {\n\t\tpmu->pmu_enable  = perf_pmu_nop_void;\n\t\tpmu->pmu_disable = perf_pmu_nop_void;\n\t}\n\n\tlist_add_rcu(&pmu->entry, &pmus);\n\tret = 0;\nunlock:\n\tmutex_unlock(&pmus_lock);\n\n\treturn ret;\n\nfree_dev:\n\tdevice_del(pmu->dev);\n\tput_device(pmu->dev);\n\nfree_idr:\n\tif (pmu->type >= PERF_TYPE_MAX)\n\t\tidr_remove(&pmu_idr, pmu->type);\n\nfree_pdc:\n\tfree_percpu(pmu->pmu_disable_count);\n\tgoto unlock;\n}\n\nvoid perf_pmu_unregister(struct pmu *pmu)\n{\n\tmutex_lock(&pmus_lock);\n\tlist_del_rcu(&pmu->entry);\n\tmutex_unlock(&pmus_lock);\n\n\t/*\n\t * We dereference the pmu list under both SRCU and regular RCU, so\n\t * synchronize against both of those.\n\t */\n\tsynchronize_srcu(&pmus_srcu);\n\tsynchronize_rcu();\n\n\tfree_percpu(pmu->pmu_disable_count);\n\tif (pmu->type >= PERF_TYPE_MAX)\n\t\tidr_remove(&pmu_idr, pmu->type);\n\tdevice_del(pmu->dev);\n\tput_device(pmu->dev);\n\tfree_pmu_context(pmu);\n}\n\nstruct pmu *perf_init_event(struct perf_event *event)\n{\n\tstruct pmu *pmu = NULL;\n\tint idx;\n\tint ret;\n\n\tidx = srcu_read_lock(&pmus_srcu);\n\n\trcu_read_lock();\n\tpmu = idr_find(&pmu_idr, event->attr.type);\n\trcu_read_unlock();\n\tif (pmu) {\n\t\tret = pmu->event_init(event);\n\t\tif (ret)\n\t\t\tpmu = ERR_PTR(ret);\n\t\tgoto unlock;\n\t}\n\n\tlist_for_each_entry_rcu(pmu, &pmus, entry) {\n\t\tret = pmu->event_init(event);\n\t\tif (!ret)\n\t\t\tgoto unlock;\n\n\t\tif (ret != -ENOENT) {\n\t\t\tpmu = ERR_PTR(ret);\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\tpmu = ERR_PTR(-ENOENT);\nunlock:\n\tsrcu_read_unlock(&pmus_srcu, idx);\n\n\treturn pmu;\n}\n\n/*\n * Allocate and initialize a event structure\n */\nstatic struct perf_event *\nperf_event_alloc(struct perf_event_attr *attr, int cpu,\n\t\t struct task_struct *task,\n\t\t struct perf_event *group_leader,\n\t\t struct perf_event *parent_event,\n\t\t perf_overflow_handler_t overflow_handler)\n{\n\tstruct pmu *pmu;\n\tstruct perf_event *event;\n\tstruct hw_perf_event *hwc;\n\tlong err;\n\n\tif ((unsigned)cpu >= nr_cpu_ids) {\n\t\tif (!task || cpu != -1)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tevent = kzalloc(sizeof(*event), GFP_KERNEL);\n\tif (!event)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/*\n\t * Single events are their own group leaders, with an\n\t * empty sibling list:\n\t */\n\tif (!group_leader)\n\t\tgroup_leader = event;\n\n\tmutex_init(&event->child_mutex);\n\tINIT_LIST_HEAD(&event->child_list);\n\n\tINIT_LIST_HEAD(&event->group_entry);\n\tINIT_LIST_HEAD(&event->event_entry);\n\tINIT_LIST_HEAD(&event->sibling_list);\n\tinit_waitqueue_head(&event->waitq);\n\tinit_irq_work(&event->pending, perf_pending_event);\n\n\tmutex_init(&event->mmap_mutex);\n\n\tevent->cpu\t\t= cpu;\n\tevent->attr\t\t= *attr;\n\tevent->group_leader\t= group_leader;\n\tevent->pmu\t\t= NULL;\n\tevent->oncpu\t\t= -1;\n\n\tevent->parent\t\t= parent_event;\n\n\tevent->ns\t\t= get_pid_ns(current->nsproxy->pid_ns);\n\tevent->id\t\t= atomic64_inc_return(&perf_event_id);\n\n\tevent->state\t\t= PERF_EVENT_STATE_INACTIVE;\n\n\tif (task) {\n\t\tevent->attach_state = PERF_ATTACH_TASK;\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\t\t/*\n\t\t * hw_breakpoint is a bit difficult here..\n\t\t */\n\t\tif (attr->type == PERF_TYPE_BREAKPOINT)\n\t\t\tevent->hw.bp_target = task;\n#endif\n\t}\n\n\tif (!overflow_handler && parent_event)\n\t\toverflow_handler = parent_event->overflow_handler;\n\n\tevent->overflow_handler\t= overflow_handler;\n\n\tif (attr->disabled)\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\n\tpmu = NULL;\n\n\thwc = &event->hw;\n\thwc->sample_period = attr->sample_period;\n\tif (attr->freq && attr->sample_freq)\n\t\thwc->sample_period = 1;\n\thwc->last_period = hwc->sample_period;\n\n\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\n\t/*\n\t * we currently do not support PERF_FORMAT_GROUP on inherited events\n\t */\n\tif (attr->inherit && (attr->read_format & PERF_FORMAT_GROUP))\n\t\tgoto done;\n\n\tpmu = perf_init_event(event);\n\ndone:\n\terr = 0;\n\tif (!pmu)\n\t\terr = -EINVAL;\n\telse if (IS_ERR(pmu))\n\t\terr = PTR_ERR(pmu);\n\n\tif (err) {\n\t\tif (event->ns)\n\t\t\tput_pid_ns(event->ns);\n\t\tkfree(event);\n\t\treturn ERR_PTR(err);\n\t}\n\n\tevent->pmu = pmu;\n\n\tif (!event->parent) {\n\t\tif (event->attach_state & PERF_ATTACH_TASK)\n\t\t\tjump_label_inc(&perf_sched_events);\n\t\tif (event->attr.mmap || event->attr.mmap_data)\n\t\t\tatomic_inc(&nr_mmap_events);\n\t\tif (event->attr.comm)\n\t\t\tatomic_inc(&nr_comm_events);\n\t\tif (event->attr.task)\n\t\t\tatomic_inc(&nr_task_events);\n\t\tif (event->attr.sample_type & PERF_SAMPLE_CALLCHAIN) {\n\t\t\terr = get_callchain_buffers();\n\t\t\tif (err) {\n\t\t\t\tfree_event(event);\n\t\t\t\treturn ERR_PTR(err);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn event;\n}\n\nstatic int perf_copy_attr(struct perf_event_attr __user *uattr,\n\t\t\t  struct perf_event_attr *attr)\n{\n\tu32 size;\n\tint ret;\n\n\tif (!access_ok(VERIFY_WRITE, uattr, PERF_ATTR_SIZE_VER0))\n\t\treturn -EFAULT;\n\n\t/*\n\t * zero the full structure, so that a short copy will be nice.\n\t */\n\tmemset(attr, 0, sizeof(*attr));\n\n\tret = get_user(size, &uattr->size);\n\tif (ret)\n\t\treturn ret;\n\n\tif (size > PAGE_SIZE)\t/* silly large */\n\t\tgoto err_size;\n\n\tif (!size)\t\t/* abi compat */\n\t\tsize = PERF_ATTR_SIZE_VER0;\n\n\tif (size < PERF_ATTR_SIZE_VER0)\n\t\tgoto err_size;\n\n\t/*\n\t * If we're handed a bigger struct than we know of,\n\t * ensure all the unknown bits are 0 - i.e. new\n\t * user-space does not rely on any kernel feature\n\t * extensions we dont know about yet.\n\t */\n\tif (size > sizeof(*attr)) {\n\t\tunsigned char __user *addr;\n\t\tunsigned char __user *end;\n\t\tunsigned char val;\n\n\t\taddr = (void __user *)uattr + sizeof(*attr);\n\t\tend  = (void __user *)uattr + size;\n\n\t\tfor (; addr < end; addr++) {\n\t\t\tret = get_user(val, addr);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tif (val)\n\t\t\t\tgoto err_size;\n\t\t}\n\t\tsize = sizeof(*attr);\n\t}\n\n\tret = copy_from_user(attr, uattr, size);\n\tif (ret)\n\t\treturn -EFAULT;\n\n\t/*\n\t * If the type exists, the corresponding creation will verify\n\t * the attr->config.\n\t */\n\tif (attr->type >= PERF_TYPE_MAX)\n\t\treturn -EINVAL;\n\n\tif (attr->__reserved_1)\n\t\treturn -EINVAL;\n\n\tif (attr->sample_type & ~(PERF_SAMPLE_MAX-1))\n\t\treturn -EINVAL;\n\n\tif (attr->read_format & ~(PERF_FORMAT_MAX-1))\n\t\treturn -EINVAL;\n\nout:\n\treturn ret;\n\nerr_size:\n\tput_user(sizeof(*attr), &uattr->size);\n\tret = -E2BIG;\n\tgoto out;\n}\n\nstatic int\nperf_event_set_output(struct perf_event *event, struct perf_event *output_event)\n{\n\tstruct ring_buffer *rb = NULL, *old_rb = NULL;\n\tint ret = -EINVAL;\n\n\tif (!output_event)\n\t\tgoto set;\n\n\t/* don't allow circular references */\n\tif (event == output_event)\n\t\tgoto out;\n\n\t/*\n\t * Don't allow cross-cpu buffers\n\t */\n\tif (output_event->cpu != event->cpu)\n\t\tgoto out;\n\n\t/*\n\t * If its not a per-cpu rb, it must be the same task.\n\t */\n\tif (output_event->cpu == -1 && output_event->ctx != event->ctx)\n\t\tgoto out;\n\nset:\n\tmutex_lock(&event->mmap_mutex);\n\t/* Can't redirect output if we've got an active mmap() */\n\tif (atomic_read(&event->mmap_count))\n\t\tgoto unlock;\n\n\tif (output_event) {\n\t\t/* get the rb we want to redirect to */\n\t\trb = ring_buffer_get(output_event);\n\t\tif (!rb)\n\t\t\tgoto unlock;\n\t}\n\n\told_rb = event->rb;\n\trcu_assign_pointer(event->rb, rb);\n\tret = 0;\nunlock:\n\tmutex_unlock(&event->mmap_mutex);\n\n\tif (old_rb)\n\t\tring_buffer_put(old_rb);\nout:\n\treturn ret;\n}\n\n/**\n * sys_perf_event_open - open a performance event, associate it to a task/cpu\n *\n * @attr_uptr:\tevent_id type attributes for monitoring/sampling\n * @pid:\t\ttarget pid\n * @cpu:\t\ttarget cpu\n * @group_fd:\t\tgroup leader event fd\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct file *group_file = NULL;\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint fput_needed = 0;\n\tint err;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tevent_fd = get_unused_fd_flags(O_RDWR);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\tgroup_leader = perf_fget_light(group_fd, &fput_needed);\n\t\tif (IS_ERR(group_leader)) {\n\t\t\terr = PTR_ERR(group_leader);\n\t\t\tgoto err_fd;\n\t\t}\n\t\tgroup_file = group_leader->filp;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_task;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err)\n\t\t\tgoto err_alloc;\n\t\t/*\n\t\t * one more event:\n\t\t * - that has cgroup constraint on event->cpu\n\t\t * - that may need work on context switch\n\t\t */\n\t\tatomic_inc(&per_cpu(perf_cgroup_events, event->cpu));\n\t\tjump_label_inc(&perf_sched_events);\n\t}\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\tif (group_leader->ctx->type != ctx->type)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event, O_RDWR);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tevent->filp = event_file;\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tperf_install_in_context(ctx, group_leader, cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, cpu);\n\t++ctx->generation;\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfput_light(group_file, fput_needed);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfput_light(group_file, fput_needed);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n\n/**\n * perf_event_create_kernel_counter\n *\n * @attr: attributes of the counter to create\n * @cpu: cpu in which the counter is bound\n * @task: task to profile (NULL for percpu)\n */\nstruct perf_event *\nperf_event_create_kernel_counter(struct perf_event_attr *attr, int cpu,\n\t\t\t\t struct task_struct *task,\n\t\t\t\t perf_overflow_handler_t overflow_handler)\n{\n\tstruct perf_event_context *ctx;\n\tstruct perf_event *event;\n\tint err;\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\n\tevent = perf_event_alloc(attr, cpu, task, NULL, NULL, overflow_handler);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err;\n\t}\n\n\tctx = find_get_context(event->pmu, task, cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_free;\n\t}\n\n\tevent->filp = NULL;\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\tperf_install_in_context(ctx, event, cpu);\n\t++ctx->generation;\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\treturn event;\n\nerr_free:\n\tfree_event(event);\nerr:\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(perf_event_create_kernel_counter);\n\nstatic void sync_child_event(struct perf_event *child_event,\n\t\t\t       struct task_struct *child)\n{\n\tstruct perf_event *parent_event = child_event->parent;\n\tu64 child_val;\n\n\tif (child_event->attr.inherit_stat)\n\t\tperf_event_read_event(child_event, child);\n\n\tchild_val = perf_event_count(child_event);\n\n\t/*\n\t * Add back the child's count to the parent's count:\n\t */\n\tatomic64_add(child_val, &parent_event->child_count);\n\tatomic64_add(child_event->total_time_enabled,\n\t\t     &parent_event->child_total_time_enabled);\n\tatomic64_add(child_event->total_time_running,\n\t\t     &parent_event->child_total_time_running);\n\n\t/*\n\t * Remove this event from the parent's list\n\t */\n\tWARN_ON_ONCE(parent_event->ctx->parent_ctx);\n\tmutex_lock(&parent_event->child_mutex);\n\tlist_del_init(&child_event->child_list);\n\tmutex_unlock(&parent_event->child_mutex);\n\n\t/*\n\t * Release the parent event, if this was the last\n\t * reference to it.\n\t */\n\tfput(parent_event->filp);\n}\n\nstatic void\n__perf_event_exit_task(struct perf_event *child_event,\n\t\t\t struct perf_event_context *child_ctx,\n\t\t\t struct task_struct *child)\n{\n\tif (child_event->parent) {\n\t\traw_spin_lock_irq(&child_ctx->lock);\n\t\tperf_group_detach(child_event);\n\t\traw_spin_unlock_irq(&child_ctx->lock);\n\t}\n\n\tperf_remove_from_context(child_event);\n\n\t/*\n\t * It can happen that the parent exits first, and has events\n\t * that are still around due to the child reference. These\n\t * events need to be zapped.\n\t */\n\tif (child_event->parent) {\n\t\tsync_child_event(child_event, child);\n\t\tfree_event(child_event);\n\t}\n}\n\nstatic void perf_event_exit_task_context(struct task_struct *child, int ctxn)\n{\n\tstruct perf_event *child_event, *tmp;\n\tstruct perf_event_context *child_ctx;\n\tunsigned long flags;\n\n\tif (likely(!child->perf_event_ctxp[ctxn])) {\n\t\tperf_event_task(child, NULL, 0);\n\t\treturn;\n\t}\n\n\tlocal_irq_save(flags);\n\t/*\n\t * We can't reschedule here because interrupts are disabled,\n\t * and either child is current or it is a task that can't be\n\t * scheduled, so we are now safe from rescheduling changing\n\t * our context.\n\t */\n\tchild_ctx = rcu_dereference_raw(child->perf_event_ctxp[ctxn]);\n\n\t/*\n\t * Take the context lock here so that if find_get_context is\n\t * reading child->perf_event_ctxp, we wait until it has\n\t * incremented the context's refcount before we do put_ctx below.\n\t */\n\traw_spin_lock(&child_ctx->lock);\n\ttask_ctx_sched_out(child_ctx);\n\tchild->perf_event_ctxp[ctxn] = NULL;\n\t/*\n\t * If this context is a clone; unclone it so it can't get\n\t * swapped to another process while we're removing all\n\t * the events from it.\n\t */\n\tunclone_ctx(child_ctx);\n\tupdate_context_time(child_ctx);\n\traw_spin_unlock_irqrestore(&child_ctx->lock, flags);\n\n\t/*\n\t * Report the task dead after unscheduling the events so that we\n\t * won't get any samples after PERF_RECORD_EXIT. We can however still\n\t * get a few PERF_RECORD_READ events.\n\t */\n\tperf_event_task(child, child_ctx, 0);\n\n\t/*\n\t * We can recurse on the same lock type through:\n\t *\n\t *   __perf_event_exit_task()\n\t *     sync_child_event()\n\t *       fput(parent_event->filp)\n\t *         perf_release()\n\t *           mutex_lock(&ctx->mutex)\n\t *\n\t * But since its the parent context it won't be the same instance.\n\t */\n\tmutex_lock(&child_ctx->mutex);\n\nagain:\n\tlist_for_each_entry_safe(child_event, tmp, &child_ctx->pinned_groups,\n\t\t\t\t group_entry)\n\t\t__perf_event_exit_task(child_event, child_ctx, child);\n\n\tlist_for_each_entry_safe(child_event, tmp, &child_ctx->flexible_groups,\n\t\t\t\t group_entry)\n\t\t__perf_event_exit_task(child_event, child_ctx, child);\n\n\t/*\n\t * If the last event was a group event, it will have appended all\n\t * its siblings to the list, but we obtained 'tmp' before that which\n\t * will still point to the list head terminating the iteration.\n\t */\n\tif (!list_empty(&child_ctx->pinned_groups) ||\n\t    !list_empty(&child_ctx->flexible_groups))\n\t\tgoto again;\n\n\tmutex_unlock(&child_ctx->mutex);\n\n\tput_ctx(child_ctx);\n}\n\n/*\n * When a child task exits, feed back event values to parent events.\n */\nvoid perf_event_exit_task(struct task_struct *child)\n{\n\tstruct perf_event *event, *tmp;\n\tint ctxn;\n\n\tmutex_lock(&child->perf_event_mutex);\n\tlist_for_each_entry_safe(event, tmp, &child->perf_event_list,\n\t\t\t\t owner_entry) {\n\t\tlist_del_init(&event->owner_entry);\n\n\t\t/*\n\t\t * Ensure the list deletion is visible before we clear\n\t\t * the owner, closes a race against perf_release() where\n\t\t * we need to serialize on the owner->perf_event_mutex.\n\t\t */\n\t\tsmp_wmb();\n\t\tevent->owner = NULL;\n\t}\n\tmutex_unlock(&child->perf_event_mutex);\n\n\tfor_each_task_context_nr(ctxn)\n\t\tperf_event_exit_task_context(child, ctxn);\n}\n\nstatic void perf_free_event(struct perf_event *event,\n\t\t\t    struct perf_event_context *ctx)\n{\n\tstruct perf_event *parent = event->parent;\n\n\tif (WARN_ON_ONCE(!parent))\n\t\treturn;\n\n\tmutex_lock(&parent->child_mutex);\n\tlist_del_init(&event->child_list);\n\tmutex_unlock(&parent->child_mutex);\n\n\tfput(parent->filp);\n\n\tperf_group_detach(event);\n\tlist_del_event(event, ctx);\n\tfree_event(event);\n}\n\n/*\n * free an unexposed, unused context as created by inheritance by\n * perf_event_init_task below, used by fork() in case of fail.\n */\nvoid perf_event_free_task(struct task_struct *task)\n{\n\tstruct perf_event_context *ctx;\n\tstruct perf_event *event, *tmp;\n\tint ctxn;\n\n\tfor_each_task_context_nr(ctxn) {\n\t\tctx = task->perf_event_ctxp[ctxn];\n\t\tif (!ctx)\n\t\t\tcontinue;\n\n\t\tmutex_lock(&ctx->mutex);\nagain:\n\t\tlist_for_each_entry_safe(event, tmp, &ctx->pinned_groups,\n\t\t\t\tgroup_entry)\n\t\t\tperf_free_event(event, ctx);\n\n\t\tlist_for_each_entry_safe(event, tmp, &ctx->flexible_groups,\n\t\t\t\tgroup_entry)\n\t\t\tperf_free_event(event, ctx);\n\n\t\tif (!list_empty(&ctx->pinned_groups) ||\n\t\t\t\t!list_empty(&ctx->flexible_groups))\n\t\t\tgoto again;\n\n\t\tmutex_unlock(&ctx->mutex);\n\n\t\tput_ctx(ctx);\n\t}\n}\n\nvoid perf_event_delayed_put(struct task_struct *task)\n{\n\tint ctxn;\n\n\tfor_each_task_context_nr(ctxn)\n\t\tWARN_ON_ONCE(task->perf_event_ctxp[ctxn]);\n}\n\n/*\n * inherit a event from parent task to child task:\n */\nstatic struct perf_event *\ninherit_event(struct perf_event *parent_event,\n\t      struct task_struct *parent,\n\t      struct perf_event_context *parent_ctx,\n\t      struct task_struct *child,\n\t      struct perf_event *group_leader,\n\t      struct perf_event_context *child_ctx)\n{\n\tstruct perf_event *child_event;\n\tunsigned long flags;\n\n\t/*\n\t * Instead of creating recursive hierarchies of events,\n\t * we link inherited events back to the original parent,\n\t * which has a filp for sure, which we use as the reference\n\t * count:\n\t */\n\tif (parent_event->parent)\n\t\tparent_event = parent_event->parent;\n\n\tchild_event = perf_event_alloc(&parent_event->attr,\n\t\t\t\t\t   parent_event->cpu,\n\t\t\t\t\t   child,\n\t\t\t\t\t   group_leader, parent_event,\n\t\t\t\t\t   NULL);\n\tif (IS_ERR(child_event))\n\t\treturn child_event;\n\tget_ctx(child_ctx);\n\n\t/*\n\t * Make the child state follow the state of the parent event,\n\t * not its attr.disabled bit.  We hold the parent's mutex,\n\t * so we won't race with perf_event_{en, dis}able_family.\n\t */\n\tif (parent_event->state >= PERF_EVENT_STATE_INACTIVE)\n\t\tchild_event->state = PERF_EVENT_STATE_INACTIVE;\n\telse\n\t\tchild_event->state = PERF_EVENT_STATE_OFF;\n\n\tif (parent_event->attr.freq) {\n\t\tu64 sample_period = parent_event->hw.sample_period;\n\t\tstruct hw_perf_event *hwc = &child_event->hw;\n\n\t\thwc->sample_period = sample_period;\n\t\thwc->last_period   = sample_period;\n\n\t\tlocal64_set(&hwc->period_left, sample_period);\n\t}\n\n\tchild_event->ctx = child_ctx;\n\tchild_event->overflow_handler = parent_event->overflow_handler;\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(child_event);\n\tperf_event__id_header_size(child_event);\n\n\t/*\n\t * Link it up in the child's context:\n\t */\n\traw_spin_lock_irqsave(&child_ctx->lock, flags);\n\tadd_event_to_ctx(child_event, child_ctx);\n\traw_spin_unlock_irqrestore(&child_ctx->lock, flags);\n\n\t/*\n\t * Get a reference to the parent filp - we will fput it\n\t * when the child event exits. This is safe to do because\n\t * we are in the parent and we know that the filp still\n\t * exists and has a nonzero count:\n\t */\n\tatomic_long_inc(&parent_event->filp->f_count);\n\n\t/*\n\t * Link this into the parent event's child list\n\t */\n\tWARN_ON_ONCE(parent_event->ctx->parent_ctx);\n\tmutex_lock(&parent_event->child_mutex);\n\tlist_add_tail(&child_event->child_list, &parent_event->child_list);\n\tmutex_unlock(&parent_event->child_mutex);\n\n\treturn child_event;\n}\n\nstatic int inherit_group(struct perf_event *parent_event,\n\t      struct task_struct *parent,\n\t      struct perf_event_context *parent_ctx,\n\t      struct task_struct *child,\n\t      struct perf_event_context *child_ctx)\n{\n\tstruct perf_event *leader;\n\tstruct perf_event *sub;\n\tstruct perf_event *child_ctr;\n\n\tleader = inherit_event(parent_event, parent, parent_ctx,\n\t\t\t\t child, NULL, child_ctx);\n\tif (IS_ERR(leader))\n\t\treturn PTR_ERR(leader);\n\tlist_for_each_entry(sub, &parent_event->sibling_list, group_entry) {\n\t\tchild_ctr = inherit_event(sub, parent, parent_ctx,\n\t\t\t\t\t    child, leader, child_ctx);\n\t\tif (IS_ERR(child_ctr))\n\t\t\treturn PTR_ERR(child_ctr);\n\t}\n\treturn 0;\n}\n\nstatic int\ninherit_task_group(struct perf_event *event, struct task_struct *parent,\n\t\t   struct perf_event_context *parent_ctx,\n\t\t   struct task_struct *child, int ctxn,\n\t\t   int *inherited_all)\n{\n\tint ret;\n\tstruct perf_event_context *child_ctx;\n\n\tif (!event->attr.inherit) {\n\t\t*inherited_all = 0;\n\t\treturn 0;\n\t}\n\n\tchild_ctx = child->perf_event_ctxp[ctxn];\n\tif (!child_ctx) {\n\t\t/*\n\t\t * This is executed from the parent task context, so\n\t\t * inherit events that have been marked for cloning.\n\t\t * First allocate and initialize a context for the\n\t\t * child.\n\t\t */\n\n\t\tchild_ctx = alloc_perf_context(event->pmu, child);\n\t\tif (!child_ctx)\n\t\t\treturn -ENOMEM;\n\n\t\tchild->perf_event_ctxp[ctxn] = child_ctx;\n\t}\n\n\tret = inherit_group(event, parent, parent_ctx,\n\t\t\t    child, child_ctx);\n\n\tif (ret)\n\t\t*inherited_all = 0;\n\n\treturn ret;\n}\n\n/*\n * Initialize the perf_event context in task_struct\n */\nint perf_event_init_context(struct task_struct *child, int ctxn)\n{\n\tstruct perf_event_context *child_ctx, *parent_ctx;\n\tstruct perf_event_context *cloned_ctx;\n\tstruct perf_event *event;\n\tstruct task_struct *parent = current;\n\tint inherited_all = 1;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tif (likely(!parent->perf_event_ctxp[ctxn]))\n\t\treturn 0;\n\n\t/*\n\t * If the parent's context is a clone, pin it so it won't get\n\t * swapped under us.\n\t */\n\tparent_ctx = perf_pin_task_context(parent, ctxn);\n\n\t/*\n\t * No need to check if parent_ctx != NULL here; since we saw\n\t * it non-NULL earlier, the only reason for it to become NULL\n\t * is if we exit, and since we're currently in the middle of\n\t * a fork we can't be exiting at the same time.\n\t */\n\n\t/*\n\t * Lock the parent list. No need to lock the child - not PID\n\t * hashed yet and not running, so nobody can access it.\n\t */\n\tmutex_lock(&parent_ctx->mutex);\n\n\t/*\n\t * We dont have to disable NMIs - we are only looking at\n\t * the list, not manipulating it:\n\t */\n\tlist_for_each_entry(event, &parent_ctx->pinned_groups, group_entry) {\n\t\tret = inherit_task_group(event, parent, parent_ctx,\n\t\t\t\t\t child, ctxn, &inherited_all);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\t/*\n\t * We can't hold ctx->lock when iterating the ->flexible_group list due\n\t * to allocations, but we need to prevent rotation because\n\t * rotate_ctx() will change the list from interrupt context.\n\t */\n\traw_spin_lock_irqsave(&parent_ctx->lock, flags);\n\tparent_ctx->rotate_disable = 1;\n\traw_spin_unlock_irqrestore(&parent_ctx->lock, flags);\n\n\tlist_for_each_entry(event, &parent_ctx->flexible_groups, group_entry) {\n\t\tret = inherit_task_group(event, parent, parent_ctx,\n\t\t\t\t\t child, ctxn, &inherited_all);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\traw_spin_lock_irqsave(&parent_ctx->lock, flags);\n\tparent_ctx->rotate_disable = 0;\n\n\tchild_ctx = child->perf_event_ctxp[ctxn];\n\n\tif (child_ctx && inherited_all) {\n\t\t/*\n\t\t * Mark the child context as a clone of the parent\n\t\t * context, or of whatever the parent is a clone of.\n\t\t *\n\t\t * Note that if the parent is a clone, the holding of\n\t\t * parent_ctx->lock avoids it from being uncloned.\n\t\t */\n\t\tcloned_ctx = parent_ctx->parent_ctx;\n\t\tif (cloned_ctx) {\n\t\t\tchild_ctx->parent_ctx = cloned_ctx;\n\t\t\tchild_ctx->parent_gen = parent_ctx->parent_gen;\n\t\t} else {\n\t\t\tchild_ctx->parent_ctx = parent_ctx;\n\t\t\tchild_ctx->parent_gen = parent_ctx->generation;\n\t\t}\n\t\tget_ctx(child_ctx->parent_ctx);\n\t}\n\n\traw_spin_unlock_irqrestore(&parent_ctx->lock, flags);\n\tmutex_unlock(&parent_ctx->mutex);\n\n\tperf_unpin_context(parent_ctx);\n\tput_ctx(parent_ctx);\n\n\treturn ret;\n}\n\n/*\n * Initialize the perf_event context in task_struct\n */\nint perf_event_init_task(struct task_struct *child)\n{\n\tint ctxn, ret;\n\n\tmemset(child->perf_event_ctxp, 0, sizeof(child->perf_event_ctxp));\n\tmutex_init(&child->perf_event_mutex);\n\tINIT_LIST_HEAD(&child->perf_event_list);\n\n\tfor_each_task_context_nr(ctxn) {\n\t\tret = perf_event_init_context(child, ctxn);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void __init perf_event_init_all_cpus(void)\n{\n\tstruct swevent_htable *swhash;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tswhash = &per_cpu(swevent_htable, cpu);\n\t\tmutex_init(&swhash->hlist_mutex);\n\t\tINIT_LIST_HEAD(&per_cpu(rotation_list, cpu));\n\t}\n}\n\nstatic void __cpuinit perf_event_init_cpu(int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tif (swhash->hlist_refcount > 0) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc_node(sizeof(*hlist), GFP_KERNEL, cpu_to_node(cpu));\n\t\tWARN_ON(!hlist);\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tmutex_unlock(&swhash->hlist_mutex);\n}\n\n#if defined CONFIG_HOTPLUG_CPU || defined CONFIG_KEXEC\nstatic void perf_pmu_rotate_stop(struct pmu *pmu)\n{\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(pmu->pmu_cpu_context);\n\n\tWARN_ON(!irqs_disabled());\n\n\tlist_del_init(&cpuctx->rotation_list);\n}\n\nstatic void __perf_event_exit_context(void *__info)\n{\n\tstruct perf_event_context *ctx = __info;\n\tstruct perf_event *event, *tmp;\n\n\tperf_pmu_rotate_stop(ctx->pmu);\n\n\tlist_for_each_entry_safe(event, tmp, &ctx->pinned_groups, group_entry)\n\t\t__perf_remove_from_context(event);\n\tlist_for_each_entry_safe(event, tmp, &ctx->flexible_groups, group_entry)\n\t\t__perf_remove_from_context(event);\n}\n\nstatic void perf_event_exit_cpu_context(int cpu)\n{\n\tstruct perf_event_context *ctx;\n\tstruct pmu *pmu;\n\tint idx;\n\n\tidx = srcu_read_lock(&pmus_srcu);\n\tlist_for_each_entry_rcu(pmu, &pmus, entry) {\n\t\tctx = &per_cpu_ptr(pmu->pmu_cpu_context, cpu)->ctx;\n\n\t\tmutex_lock(&ctx->mutex);\n\t\tsmp_call_function_single(cpu, __perf_event_exit_context, ctx, 1);\n\t\tmutex_unlock(&ctx->mutex);\n\t}\n\tsrcu_read_unlock(&pmus_srcu, idx);\n}\n\nstatic void perf_event_exit_cpu(int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tswevent_hlist_release(swhash);\n\tmutex_unlock(&swhash->hlist_mutex);\n\n\tperf_event_exit_cpu_context(cpu);\n}\n#else\nstatic inline void perf_event_exit_cpu(int cpu) { }\n#endif\n\nstatic int\nperf_reboot(struct notifier_block *notifier, unsigned long val, void *v)\n{\n\tint cpu;\n\n\tfor_each_online_cpu(cpu)\n\t\tperf_event_exit_cpu(cpu);\n\n\treturn NOTIFY_OK;\n}\n\n/*\n * Run the perf reboot notifier at the very last possible moment so that\n * the generic watchdog code runs as long as possible.\n */\nstatic struct notifier_block perf_reboot_notifier = {\n\t.notifier_call = perf_reboot,\n\t.priority = INT_MIN,\n};\n\nstatic int __cpuinit\nperf_cpu_notify(struct notifier_block *self, unsigned long action, void *hcpu)\n{\n\tunsigned int cpu = (long)hcpu;\n\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\n\tcase CPU_UP_PREPARE:\n\tcase CPU_DOWN_FAILED:\n\t\tperf_event_init_cpu(cpu);\n\t\tbreak;\n\n\tcase CPU_UP_CANCELED:\n\tcase CPU_DOWN_PREPARE:\n\t\tperf_event_exit_cpu(cpu);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nvoid __init perf_event_init(void)\n{\n\tint ret;\n\n\tidr_init(&pmu_idr);\n\n\tperf_event_init_all_cpus();\n\tinit_srcu_struct(&pmus_srcu);\n\tperf_pmu_register(&perf_swevent, \"software\", PERF_TYPE_SOFTWARE);\n\tperf_pmu_register(&perf_cpu_clock, NULL, -1);\n\tperf_pmu_register(&perf_task_clock, NULL, -1);\n\tperf_tp_register();\n\tperf_cpu_notifier(perf_cpu_notify);\n\tregister_reboot_notifier(&perf_reboot_notifier);\n\n\tret = init_hw_breakpoint();\n\tWARN(ret, \"hw_breakpoint initialization failed with: %d\", ret);\n}\n\nstatic int __init perf_event_sysfs_init(void)\n{\n\tstruct pmu *pmu;\n\tint ret;\n\n\tmutex_lock(&pmus_lock);\n\n\tret = bus_register(&pmu_bus);\n\tif (ret)\n\t\tgoto unlock;\n\n\tlist_for_each_entry(pmu, &pmus, entry) {\n\t\tif (!pmu->name || pmu->type < 0)\n\t\t\tcontinue;\n\n\t\tret = pmu_dev_alloc(pmu);\n\t\tWARN(ret, \"Failed to register pmu: %s, reason %d\\n\", pmu->name, ret);\n\t}\n\tpmu_bus_running = 1;\n\tret = 0;\n\nunlock:\n\tmutex_unlock(&pmus_lock);\n\n\treturn ret;\n}\ndevice_initcall(perf_event_sysfs_init);\n\n#ifdef CONFIG_CGROUP_PERF\nstatic struct cgroup_subsys_state *perf_cgroup_create(\n\tstruct cgroup_subsys *ss, struct cgroup *cont)\n{\n\tstruct perf_cgroup *jc;\n\n\tjc = kzalloc(sizeof(*jc), GFP_KERNEL);\n\tif (!jc)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tjc->info = alloc_percpu(struct perf_cgroup_info);\n\tif (!jc->info) {\n\t\tkfree(jc);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\treturn &jc->css;\n}\n\nstatic void perf_cgroup_destroy(struct cgroup_subsys *ss,\n\t\t\t\tstruct cgroup *cont)\n{\n\tstruct perf_cgroup *jc;\n\tjc = container_of(cgroup_subsys_state(cont, perf_subsys_id),\n\t\t\t  struct perf_cgroup, css);\n\tfree_percpu(jc->info);\n\tkfree(jc);\n}\n\nstatic int __perf_cgroup_move(void *info)\n{\n\tstruct task_struct *task = info;\n\tperf_cgroup_switch(task, PERF_CGROUP_SWOUT | PERF_CGROUP_SWIN);\n\treturn 0;\n}\n\nstatic void\nperf_cgroup_attach_task(struct cgroup *cgrp, struct task_struct *task)\n{\n\ttask_function_call(task, __perf_cgroup_move, task);\n}\n\nstatic void perf_cgroup_exit(struct cgroup_subsys *ss, struct cgroup *cgrp,\n\t\tstruct cgroup *old_cgrp, struct task_struct *task)\n{\n\t/*\n\t * cgroup_exit() is called in the copy_process() failure path.\n\t * Ignore this case since the task hasn't ran yet, this avoids\n\t * trying to poke a half freed task state from generic code.\n\t */\n\tif (!(task->flags & PF_EXITING))\n\t\treturn;\n\n\tperf_cgroup_attach_task(cgrp, task);\n}\n\nstruct cgroup_subsys perf_subsys = {\n\t.name\t\t= \"perf_event\",\n\t.subsys_id\t= perf_subsys_id,\n\t.create\t\t= perf_cgroup_create,\n\t.destroy\t= perf_cgroup_destroy,\n\t.exit\t\t= perf_cgroup_exit,\n\t.attach_task\t= perf_cgroup_attach_task,\n};\n#endif /* CONFIG_CGROUP_PERF */\n", "#ifndef _KERNEL_EVENTS_INTERNAL_H\n#define _KERNEL_EVENTS_INTERNAL_H\n\n#define RING_BUFFER_WRITABLE\t\t0x01\n\nstruct ring_buffer {\n\tatomic_t\t\t\trefcount;\n\tstruct rcu_head\t\t\trcu_head;\n#ifdef CONFIG_PERF_USE_VMALLOC\n\tstruct work_struct\t\twork;\n\tint\t\t\t\tpage_order;\t/* allocation order  */\n#endif\n\tint\t\t\t\tnr_pages;\t/* nr of data pages  */\n\tint\t\t\t\twritable;\t/* are we writable   */\n\n\tatomic_t\t\t\tpoll;\t\t/* POLL_ for wakeups */\n\n\tlocal_t\t\t\t\thead;\t\t/* write position    */\n\tlocal_t\t\t\t\tnest;\t\t/* nested writers    */\n\tlocal_t\t\t\t\tevents;\t\t/* event limit       */\n\tlocal_t\t\t\t\twakeup;\t\t/* wakeup stamp      */\n\tlocal_t\t\t\t\tlost;\t\t/* nr records lost   */\n\n\tlong\t\t\t\twatermark;\t/* wakeup watermark  */\n\n\tstruct perf_event_mmap_page\t*user_page;\n\tvoid\t\t\t\t*data_pages[0];\n};\n\nextern void rb_free(struct ring_buffer *rb);\nextern struct ring_buffer *\nrb_alloc(int nr_pages, long watermark, int cpu, int flags);\nextern void perf_event_wakeup(struct perf_event *event);\n\nextern void\nperf_event_header__init_id(struct perf_event_header *header,\n\t\t\t   struct perf_sample_data *data,\n\t\t\t   struct perf_event *event);\nextern void\nperf_event__output_id_sample(struct perf_event *event,\n\t\t\t     struct perf_output_handle *handle,\n\t\t\t     struct perf_sample_data *sample);\n\nextern struct page *\nperf_mmap_to_page(struct ring_buffer *rb, unsigned long pgoff);\n\n#ifdef CONFIG_PERF_USE_VMALLOC\n/*\n * Back perf_mmap() with vmalloc memory.\n *\n * Required for architectures that have d-cache aliasing issues.\n */\n\nstatic inline int page_order(struct ring_buffer *rb)\n{\n\treturn rb->page_order;\n}\n\n#else\n\nstatic inline int page_order(struct ring_buffer *rb)\n{\n\treturn 0;\n}\n#endif\n\nstatic unsigned long perf_data_size(struct ring_buffer *rb)\n{\n\treturn rb->nr_pages << (PAGE_SHIFT + page_order(rb));\n}\n\nstatic inline void\n__output_copy(struct perf_output_handle *handle,\n\t\t   const void *buf, unsigned int len)\n{\n\tdo {\n\t\tunsigned long size = min_t(unsigned long, handle->size, len);\n\n\t\tmemcpy(handle->addr, buf, size);\n\n\t\tlen -= size;\n\t\thandle->addr += size;\n\t\tbuf += size;\n\t\thandle->size -= size;\n\t\tif (!handle->size) {\n\t\t\tstruct ring_buffer *rb = handle->rb;\n\n\t\t\thandle->page++;\n\t\t\thandle->page &= rb->nr_pages - 1;\n\t\t\thandle->addr = rb->data_pages[handle->page];\n\t\t\thandle->size = PAGE_SIZE << page_order(rb);\n\t\t}\n\t} while (len);\n}\n\n#endif /* _KERNEL_EVENTS_INTERNAL_H */\n", "/*\n * Performance events ring-buffer code:\n *\n *  Copyright (C) 2008 Thomas Gleixner <tglx@linutronix.de>\n *  Copyright (C) 2008-2011 Red Hat, Inc., Ingo Molnar\n *  Copyright (C) 2008-2011 Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>\n *  Copyright  \ufffd  2009 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>\n *\n * For licensing details see kernel-base/COPYING\n */\n\n#include <linux/perf_event.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n\n#include \"internal.h\"\n\nstatic bool perf_output_space(struct ring_buffer *rb, unsigned long tail,\n\t\t\t      unsigned long offset, unsigned long head)\n{\n\tunsigned long mask;\n\n\tif (!rb->writable)\n\t\treturn true;\n\n\tmask = perf_data_size(rb) - 1;\n\n\toffset = (offset - tail) & mask;\n\thead   = (head   - tail) & mask;\n\n\tif ((int)(head - offset) < 0)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void perf_output_wakeup(struct perf_output_handle *handle)\n{\n\tatomic_set(&handle->rb->poll, POLL_IN);\n\n\thandle->event->pending_wakeup = 1;\n\tirq_work_queue(&handle->event->pending);\n}\n\n/*\n * We need to ensure a later event_id doesn't publish a head when a former\n * event isn't done writing. However since we need to deal with NMIs we\n * cannot fully serialize things.\n *\n * We only publish the head (and generate a wakeup) when the outer-most\n * event completes.\n */\nstatic void perf_output_get_handle(struct perf_output_handle *handle)\n{\n\tstruct ring_buffer *rb = handle->rb;\n\n\tpreempt_disable();\n\tlocal_inc(&rb->nest);\n\thandle->wakeup = local_read(&rb->wakeup);\n}\n\nstatic void perf_output_put_handle(struct perf_output_handle *handle)\n{\n\tstruct ring_buffer *rb = handle->rb;\n\tunsigned long head;\n\nagain:\n\thead = local_read(&rb->head);\n\n\t/*\n\t * IRQ/NMI can happen here, which means we can miss a head update.\n\t */\n\n\tif (!local_dec_and_test(&rb->nest))\n\t\tgoto out;\n\n\t/*\n\t * Publish the known good head. Rely on the full barrier implied\n\t * by atomic_dec_and_test() order the rb->head read and this\n\t * write.\n\t */\n\trb->user_page->data_head = head;\n\n\t/*\n\t * Now check if we missed an update, rely on the (compiler)\n\t * barrier in atomic_dec_and_test() to re-read rb->head.\n\t */\n\tif (unlikely(head != local_read(&rb->head))) {\n\t\tlocal_inc(&rb->nest);\n\t\tgoto again;\n\t}\n\n\tif (handle->wakeup != local_read(&rb->wakeup))\n\t\tperf_output_wakeup(handle);\n\nout:\n\tpreempt_enable();\n}\n\nint perf_output_begin(struct perf_output_handle *handle,\n\t\t      struct perf_event *event, unsigned int size,\n\t\t      int sample)\n{\n\tstruct ring_buffer *rb;\n\tunsigned long tail, offset, head;\n\tint have_lost;\n\tstruct perf_sample_data sample_data;\n\tstruct {\n\t\tstruct perf_event_header header;\n\t\tu64\t\t\t id;\n\t\tu64\t\t\t lost;\n\t} lost_event;\n\n\trcu_read_lock();\n\t/*\n\t * For inherited events we send all the output towards the parent.\n\t */\n\tif (event->parent)\n\t\tevent = event->parent;\n\n\trb = rcu_dereference(event->rb);\n\tif (!rb)\n\t\tgoto out;\n\n\thandle->rb\t= rb;\n\thandle->event\t= event;\n\thandle->sample\t= sample;\n\n\tif (!rb->nr_pages)\n\t\tgoto out;\n\n\thave_lost = local_read(&rb->lost);\n\tif (have_lost) {\n\t\tlost_event.header.size = sizeof(lost_event);\n\t\tperf_event_header__init_id(&lost_event.header, &sample_data,\n\t\t\t\t\t   event);\n\t\tsize += lost_event.header.size;\n\t}\n\n\tperf_output_get_handle(handle);\n\n\tdo {\n\t\t/*\n\t\t * Userspace could choose to issue a mb() before updating the\n\t\t * tail pointer. So that all reads will be completed before the\n\t\t * write is issued.\n\t\t */\n\t\ttail = ACCESS_ONCE(rb->user_page->data_tail);\n\t\tsmp_rmb();\n\t\toffset = head = local_read(&rb->head);\n\t\thead += size;\n\t\tif (unlikely(!perf_output_space(rb, tail, offset, head)))\n\t\t\tgoto fail;\n\t} while (local_cmpxchg(&rb->head, offset, head) != offset);\n\n\tif (head - local_read(&rb->wakeup) > rb->watermark)\n\t\tlocal_add(rb->watermark, &rb->wakeup);\n\n\thandle->page = offset >> (PAGE_SHIFT + page_order(rb));\n\thandle->page &= rb->nr_pages - 1;\n\thandle->size = offset & ((PAGE_SIZE << page_order(rb)) - 1);\n\thandle->addr = rb->data_pages[handle->page];\n\thandle->addr += handle->size;\n\thandle->size = (PAGE_SIZE << page_order(rb)) - handle->size;\n\n\tif (have_lost) {\n\t\tlost_event.header.type = PERF_RECORD_LOST;\n\t\tlost_event.header.misc = 0;\n\t\tlost_event.id          = event->id;\n\t\tlost_event.lost        = local_xchg(&rb->lost, 0);\n\n\t\tperf_output_put(handle, lost_event);\n\t\tperf_event__output_id_sample(event, handle, &sample_data);\n\t}\n\n\treturn 0;\n\nfail:\n\tlocal_inc(&rb->lost);\n\tperf_output_put_handle(handle);\nout:\n\trcu_read_unlock();\n\n\treturn -ENOSPC;\n}\n\nvoid perf_output_copy(struct perf_output_handle *handle,\n\t\t      const void *buf, unsigned int len)\n{\n\t__output_copy(handle, buf, len);\n}\n\nvoid perf_output_end(struct perf_output_handle *handle)\n{\n\tstruct perf_event *event = handle->event;\n\tstruct ring_buffer *rb = handle->rb;\n\n\tif (handle->sample && !event->attr.watermark) {\n\t\tint wakeup_events = event->attr.wakeup_events;\n\n\t\tif (wakeup_events) {\n\t\t\tint events = local_inc_return(&rb->events);\n\t\t\tif (events >= wakeup_events) {\n\t\t\t\tlocal_sub(wakeup_events, &rb->events);\n\t\t\t\tlocal_inc(&rb->wakeup);\n\t\t\t}\n\t\t}\n\t}\n\n\tperf_output_put_handle(handle);\n\trcu_read_unlock();\n}\n\nstatic void\nring_buffer_init(struct ring_buffer *rb, long watermark, int flags)\n{\n\tlong max_size = perf_data_size(rb);\n\n\tif (watermark)\n\t\trb->watermark = min(max_size, watermark);\n\n\tif (!rb->watermark)\n\t\trb->watermark = max_size / 2;\n\n\tif (flags & RING_BUFFER_WRITABLE)\n\t\trb->writable = 1;\n\n\tatomic_set(&rb->refcount, 1);\n}\n\n#ifndef CONFIG_PERF_USE_VMALLOC\n\n/*\n * Back perf_mmap() with regular GFP_KERNEL-0 pages.\n */\n\nstruct page *\nperf_mmap_to_page(struct ring_buffer *rb, unsigned long pgoff)\n{\n\tif (pgoff > rb->nr_pages)\n\t\treturn NULL;\n\n\tif (pgoff == 0)\n\t\treturn virt_to_page(rb->user_page);\n\n\treturn virt_to_page(rb->data_pages[pgoff - 1]);\n}\n\nstatic void *perf_mmap_alloc_page(int cpu)\n{\n\tstruct page *page;\n\tint node;\n\n\tnode = (cpu == -1) ? cpu : cpu_to_node(cpu);\n\tpage = alloc_pages_node(node, GFP_KERNEL | __GFP_ZERO, 0);\n\tif (!page)\n\t\treturn NULL;\n\n\treturn page_address(page);\n}\n\nstruct ring_buffer *rb_alloc(int nr_pages, long watermark, int cpu, int flags)\n{\n\tstruct ring_buffer *rb;\n\tunsigned long size;\n\tint i;\n\n\tsize = sizeof(struct ring_buffer);\n\tsize += nr_pages * sizeof(void *);\n\n\trb = kzalloc(size, GFP_KERNEL);\n\tif (!rb)\n\t\tgoto fail;\n\n\trb->user_page = perf_mmap_alloc_page(cpu);\n\tif (!rb->user_page)\n\t\tgoto fail_user_page;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\trb->data_pages[i] = perf_mmap_alloc_page(cpu);\n\t\tif (!rb->data_pages[i])\n\t\t\tgoto fail_data_pages;\n\t}\n\n\trb->nr_pages = nr_pages;\n\n\tring_buffer_init(rb, watermark, flags);\n\n\treturn rb;\n\nfail_data_pages:\n\tfor (i--; i >= 0; i--)\n\t\tfree_page((unsigned long)rb->data_pages[i]);\n\n\tfree_page((unsigned long)rb->user_page);\n\nfail_user_page:\n\tkfree(rb);\n\nfail:\n\treturn NULL;\n}\n\nstatic void perf_mmap_free_page(unsigned long addr)\n{\n\tstruct page *page = virt_to_page((void *)addr);\n\n\tpage->mapping = NULL;\n\t__free_page(page);\n}\n\nvoid rb_free(struct ring_buffer *rb)\n{\n\tint i;\n\n\tperf_mmap_free_page((unsigned long)rb->user_page);\n\tfor (i = 0; i < rb->nr_pages; i++)\n\t\tperf_mmap_free_page((unsigned long)rb->data_pages[i]);\n\tkfree(rb);\n}\n\n#else\n\nstruct page *\nperf_mmap_to_page(struct ring_buffer *rb, unsigned long pgoff)\n{\n\tif (pgoff > (1UL << page_order(rb)))\n\t\treturn NULL;\n\n\treturn vmalloc_to_page((void *)rb->user_page + pgoff * PAGE_SIZE);\n}\n\nstatic void perf_mmap_unmark_page(void *addr)\n{\n\tstruct page *page = vmalloc_to_page(addr);\n\n\tpage->mapping = NULL;\n}\n\nstatic void rb_free_work(struct work_struct *work)\n{\n\tstruct ring_buffer *rb;\n\tvoid *base;\n\tint i, nr;\n\n\trb = container_of(work, struct ring_buffer, work);\n\tnr = 1 << page_order(rb);\n\n\tbase = rb->user_page;\n\tfor (i = 0; i < nr + 1; i++)\n\t\tperf_mmap_unmark_page(base + (i * PAGE_SIZE));\n\n\tvfree(base);\n\tkfree(rb);\n}\n\nvoid rb_free(struct ring_buffer *rb)\n{\n\tschedule_work(&rb->work);\n}\n\nstruct ring_buffer *rb_alloc(int nr_pages, long watermark, int cpu, int flags)\n{\n\tstruct ring_buffer *rb;\n\tunsigned long size;\n\tvoid *all_buf;\n\n\tsize = sizeof(struct ring_buffer);\n\tsize += sizeof(void *);\n\n\trb = kzalloc(size, GFP_KERNEL);\n\tif (!rb)\n\t\tgoto fail;\n\n\tINIT_WORK(&rb->work, rb_free_work);\n\n\tall_buf = vmalloc_user((nr_pages + 1) * PAGE_SIZE);\n\tif (!all_buf)\n\t\tgoto fail_all_buf;\n\n\trb->user_page = all_buf;\n\trb->data_pages[0] = all_buf + PAGE_SIZE;\n\trb->page_order = ilog2(nr_pages);\n\trb->nr_pages = 1;\n\n\tring_buffer_init(rb, watermark, flags);\n\n\treturn rb;\n\nfail_all_buf:\n\tkfree(rb);\n\nfail:\n\treturn NULL;\n}\n\n#endif\n", "/*\n *  kernel/sched.c\n *\n *  Kernel scheduler and related syscalls\n *\n *  Copyright (C) 1991-2002  Linus Torvalds\n *\n *  1996-12-23  Modified by Dave Grothe to fix bugs in semaphores and\n *\t\tmake semaphores SMP safe\n *  1998-11-19\tImplemented schedule_timeout() and related stuff\n *\t\tby Andrea Arcangeli\n *  2002-01-04\tNew ultra-scalable O(1) scheduler by Ingo Molnar:\n *\t\thybrid priority-list and round-robin design with\n *\t\tan array-switch method of distributing timeslices\n *\t\tand per-CPU runqueues.  Cleanups and useful suggestions\n *\t\tby Davide Libenzi, preemptible kernel bits by Robert Love.\n *  2003-09-03\tInteractivity tuning by Con Kolivas.\n *  2004-04-02\tScheduler domains code by Nick Piggin\n *  2007-04-15  Work begun on replacing all interactivity tuning with a\n *              fair scheduling design by Con Kolivas.\n *  2007-05-05  Load balancing (smp-nice) and other improvements\n *              by Peter Williams\n *  2007-05-06  Interactivity improvements to CFS by Mike Galbraith\n *  2007-07-01  Group scheduling enhancements by Srivatsa Vaddagiri\n *  2007-11-29  RT balancing improvements by Steven Rostedt, Gregory Haskins,\n *              Thomas Gleixner, Mike Kravetz\n */\n\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/nmi.h>\n#include <linux/init.h>\n#include <linux/uaccess.h>\n#include <linux/highmem.h>\n#include <asm/mmu_context.h>\n#include <linux/interrupt.h>\n#include <linux/capability.h>\n#include <linux/completion.h>\n#include <linux/kernel_stat.h>\n#include <linux/debug_locks.h>\n#include <linux/perf_event.h>\n#include <linux/security.h>\n#include <linux/notifier.h>\n#include <linux/profile.h>\n#include <linux/freezer.h>\n#include <linux/vmalloc.h>\n#include <linux/blkdev.h>\n#include <linux/delay.h>\n#include <linux/pid_namespace.h>\n#include <linux/smp.h>\n#include <linux/threads.h>\n#include <linux/timer.h>\n#include <linux/rcupdate.h>\n#include <linux/cpu.h>\n#include <linux/cpuset.h>\n#include <linux/percpu.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/stop_machine.h>\n#include <linux/sysctl.h>\n#include <linux/syscalls.h>\n#include <linux/times.h>\n#include <linux/tsacct_kern.h>\n#include <linux/kprobes.h>\n#include <linux/delayacct.h>\n#include <linux/unistd.h>\n#include <linux/pagemap.h>\n#include <linux/hrtimer.h>\n#include <linux/tick.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/ftrace.h>\n#include <linux/slab.h>\n\n#include <asm/tlb.h>\n#include <asm/irq_regs.h>\n#include <asm/mutex.h>\n\n#include \"sched_cpupri.h\"\n#include \"workqueue_sched.h\"\n#include \"sched_autogroup.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/sched.h>\n\n/*\n * Convert user-nice values [ -20 ... 0 ... 19 ]\n * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],\n * and back.\n */\n#define NICE_TO_PRIO(nice)\t(MAX_RT_PRIO + (nice) + 20)\n#define PRIO_TO_NICE(prio)\t((prio) - MAX_RT_PRIO - 20)\n#define TASK_NICE(p)\t\tPRIO_TO_NICE((p)->static_prio)\n\n/*\n * 'User priority' is the nice value converted to something we\n * can work with better when scaling various scheduler parameters,\n * it's a [ 0 ... 39 ] range.\n */\n#define USER_PRIO(p)\t\t((p)-MAX_RT_PRIO)\n#define TASK_USER_PRIO(p)\tUSER_PRIO((p)->static_prio)\n#define MAX_USER_PRIO\t\t(USER_PRIO(MAX_PRIO))\n\n/*\n * Helpers for converting nanosecond timing to jiffy resolution\n */\n#define NS_TO_JIFFIES(TIME)\t((unsigned long)(TIME) / (NSEC_PER_SEC / HZ))\n\n#define NICE_0_LOAD\t\tSCHED_LOAD_SCALE\n#define NICE_0_SHIFT\t\tSCHED_LOAD_SHIFT\n\n/*\n * These are the 'tuning knobs' of the scheduler:\n *\n * default timeslice is 100 msecs (used only for SCHED_RR tasks).\n * Timeslices get refilled after they expire.\n */\n#define DEF_TIMESLICE\t\t(100 * HZ / 1000)\n\n/*\n * single value that denotes runtime == period, ie unlimited time.\n */\n#define RUNTIME_INF\t((u64)~0ULL)\n\nstatic inline int rt_policy(int policy)\n{\n\tif (unlikely(policy == SCHED_FIFO || policy == SCHED_RR))\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic inline int task_has_rt_policy(struct task_struct *p)\n{\n\treturn rt_policy(p->policy);\n}\n\n/*\n * This is the priority-queue data structure of the RT scheduling class:\n */\nstruct rt_prio_array {\n\tDECLARE_BITMAP(bitmap, MAX_RT_PRIO+1); /* include 1 bit for delimiter */\n\tstruct list_head queue[MAX_RT_PRIO];\n};\n\nstruct rt_bandwidth {\n\t/* nests inside the rq lock: */\n\traw_spinlock_t\t\trt_runtime_lock;\n\tktime_t\t\t\trt_period;\n\tu64\t\t\trt_runtime;\n\tstruct hrtimer\t\trt_period_timer;\n};\n\nstatic struct rt_bandwidth def_rt_bandwidth;\n\nstatic int do_sched_rt_period_timer(struct rt_bandwidth *rt_b, int overrun);\n\nstatic enum hrtimer_restart sched_rt_period_timer(struct hrtimer *timer)\n{\n\tstruct rt_bandwidth *rt_b =\n\t\tcontainer_of(timer, struct rt_bandwidth, rt_period_timer);\n\tktime_t now;\n\tint overrun;\n\tint idle = 0;\n\n\tfor (;;) {\n\t\tnow = hrtimer_cb_get_time(timer);\n\t\toverrun = hrtimer_forward(timer, now, rt_b->rt_period);\n\n\t\tif (!overrun)\n\t\t\tbreak;\n\n\t\tidle = do_sched_rt_period_timer(rt_b, overrun);\n\t}\n\n\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;\n}\n\nstatic\nvoid init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)\n{\n\trt_b->rt_period = ns_to_ktime(period);\n\trt_b->rt_runtime = runtime;\n\n\traw_spin_lock_init(&rt_b->rt_runtime_lock);\n\n\thrtimer_init(&rt_b->rt_period_timer,\n\t\t\tCLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\trt_b->rt_period_timer.function = sched_rt_period_timer;\n}\n\nstatic inline int rt_bandwidth_enabled(void)\n{\n\treturn sysctl_sched_rt_runtime >= 0;\n}\n\nstatic void start_rt_bandwidth(struct rt_bandwidth *rt_b)\n{\n\tktime_t now;\n\n\tif (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF)\n\t\treturn;\n\n\tif (hrtimer_active(&rt_b->rt_period_timer))\n\t\treturn;\n\n\traw_spin_lock(&rt_b->rt_runtime_lock);\n\tfor (;;) {\n\t\tunsigned long delta;\n\t\tktime_t soft, hard;\n\n\t\tif (hrtimer_active(&rt_b->rt_period_timer))\n\t\t\tbreak;\n\n\t\tnow = hrtimer_cb_get_time(&rt_b->rt_period_timer);\n\t\thrtimer_forward(&rt_b->rt_period_timer, now, rt_b->rt_period);\n\n\t\tsoft = hrtimer_get_softexpires(&rt_b->rt_period_timer);\n\t\thard = hrtimer_get_expires(&rt_b->rt_period_timer);\n\t\tdelta = ktime_to_ns(ktime_sub(hard, soft));\n\t\t__hrtimer_start_range_ns(&rt_b->rt_period_timer, soft, delta,\n\t\t\t\tHRTIMER_MODE_ABS_PINNED, 0);\n\t}\n\traw_spin_unlock(&rt_b->rt_runtime_lock);\n}\n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic void destroy_rt_bandwidth(struct rt_bandwidth *rt_b)\n{\n\thrtimer_cancel(&rt_b->rt_period_timer);\n}\n#endif\n\n/*\n * sched_domains_mutex serializes calls to init_sched_domains,\n * detach_destroy_domains and partition_sched_domains.\n */\nstatic DEFINE_MUTEX(sched_domains_mutex);\n\n#ifdef CONFIG_CGROUP_SCHED\n\n#include <linux/cgroup.h>\n\nstruct cfs_rq;\n\nstatic LIST_HEAD(task_groups);\n\n/* task group related information */\nstruct task_group {\n\tstruct cgroup_subsys_state css;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t/* schedulable entities of this group on each cpu */\n\tstruct sched_entity **se;\n\t/* runqueue \"owned\" by this group on each cpu */\n\tstruct cfs_rq **cfs_rq;\n\tunsigned long shares;\n\n\tatomic_t load_weight;\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tstruct sched_rt_entity **rt_se;\n\tstruct rt_rq **rt_rq;\n\n\tstruct rt_bandwidth rt_bandwidth;\n#endif\n\n\tstruct rcu_head rcu;\n\tstruct list_head list;\n\n\tstruct task_group *parent;\n\tstruct list_head siblings;\n\tstruct list_head children;\n\n#ifdef CONFIG_SCHED_AUTOGROUP\n\tstruct autogroup *autogroup;\n#endif\n};\n\n/* task_group_lock serializes the addition/removal of task groups */\nstatic DEFINE_SPINLOCK(task_group_lock);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\n# define ROOT_TASK_GROUP_LOAD\tNICE_0_LOAD\n\n/*\n * A weight of 0 or 1 can cause arithmetics problems.\n * A weight of a cfs_rq is the sum of weights of which entities\n * are queued on this cfs_rq, so a weight of a entity should not be\n * too large, so as the shares value of a task group.\n * (The default weight is 1024 - so there's no practical\n *  limitation from this.)\n */\n#define MIN_SHARES\t2\n#define MAX_SHARES\t(1UL << (18 + SCHED_LOAD_RESOLUTION))\n\nstatic int root_task_group_load = ROOT_TASK_GROUP_LOAD;\n#endif\n\n/* Default task group.\n *\tEvery task in system belong to this group at bootup.\n */\nstruct task_group root_task_group;\n\n#endif\t/* CONFIG_CGROUP_SCHED */\n\n/* CFS-related fields in a runqueue */\nstruct cfs_rq {\n\tstruct load_weight load;\n\tunsigned long nr_running;\n\n\tu64 exec_clock;\n\tu64 min_vruntime;\n#ifndef CONFIG_64BIT\n\tu64 min_vruntime_copy;\n#endif\n\n\tstruct rb_root tasks_timeline;\n\tstruct rb_node *rb_leftmost;\n\n\tstruct list_head tasks;\n\tstruct list_head *balance_iterator;\n\n\t/*\n\t * 'curr' points to currently running entity on this cfs_rq.\n\t * It is set to NULL otherwise (i.e when none are currently running).\n\t */\n\tstruct sched_entity *curr, *next, *last, *skip;\n\n#ifdef\tCONFIG_SCHED_DEBUG\n\tunsigned int nr_spread_over;\n#endif\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tstruct rq *rq;\t/* cpu runqueue to which this cfs_rq is attached */\n\n\t/*\n\t * leaf cfs_rqs are those that hold tasks (lowest schedulable entity in\n\t * a hierarchy). Non-leaf lrqs hold other higher schedulable entities\n\t * (like users, containers etc.)\n\t *\n\t * leaf_cfs_rq_list ties together list of leaf cfs_rq's in a cpu. This\n\t * list is used during load balance.\n\t */\n\tint on_list;\n\tstruct list_head leaf_cfs_rq_list;\n\tstruct task_group *tg;\t/* group that \"owns\" this runqueue */\n\n#ifdef CONFIG_SMP\n\t/*\n\t * the part of load.weight contributed by tasks\n\t */\n\tunsigned long task_weight;\n\n\t/*\n\t *   h_load = weight * f(tg)\n\t *\n\t * Where f(tg) is the recursive weight fraction assigned to\n\t * this group.\n\t */\n\tunsigned long h_load;\n\n\t/*\n\t * Maintaining per-cpu shares distribution for group scheduling\n\t *\n\t * load_stamp is the last time we updated the load average\n\t * load_last is the last time we updated the load average and saw load\n\t * load_unacc_exec_time is currently unaccounted execution time\n\t */\n\tu64 load_avg;\n\tu64 load_period;\n\tu64 load_stamp, load_last, load_unacc_exec_time;\n\n\tunsigned long load_contribution;\n#endif\n#endif\n};\n\n/* Real-Time classes' related field in a runqueue: */\nstruct rt_rq {\n\tstruct rt_prio_array active;\n\tunsigned long rt_nr_running;\n#if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED\n\tstruct {\n\t\tint curr; /* highest queued rt task prio */\n#ifdef CONFIG_SMP\n\t\tint next; /* next highest */\n#endif\n\t} highest_prio;\n#endif\n#ifdef CONFIG_SMP\n\tunsigned long rt_nr_migratory;\n\tunsigned long rt_nr_total;\n\tint overloaded;\n\tstruct plist_head pushable_tasks;\n#endif\n\tint rt_throttled;\n\tu64 rt_time;\n\tu64 rt_runtime;\n\t/* Nests inside the rq lock: */\n\traw_spinlock_t rt_runtime_lock;\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tunsigned long rt_nr_boosted;\n\n\tstruct rq *rq;\n\tstruct list_head leaf_rt_rq_list;\n\tstruct task_group *tg;\n#endif\n};\n\n#ifdef CONFIG_SMP\n\n/*\n * We add the notion of a root-domain which will be used to define per-domain\n * variables. Each exclusive cpuset essentially defines an island domain by\n * fully partitioning the member cpus from any other cpuset. Whenever a new\n * exclusive cpuset is created, we also create and attach a new root-domain\n * object.\n *\n */\nstruct root_domain {\n\tatomic_t refcount;\n\tstruct rcu_head rcu;\n\tcpumask_var_t span;\n\tcpumask_var_t online;\n\n\t/*\n\t * The \"RT overload\" flag: it gets set if a CPU has more than\n\t * one runnable RT task.\n\t */\n\tcpumask_var_t rto_mask;\n\tatomic_t rto_count;\n\tstruct cpupri cpupri;\n};\n\n/*\n * By default the system creates a single root-domain with all cpus as\n * members (mimicking the global state we have today).\n */\nstatic struct root_domain def_root_domain;\n\n#endif /* CONFIG_SMP */\n\n/*\n * This is the main, per-CPU runqueue data structure.\n *\n * Locking rule: those places that want to lock multiple runqueues\n * (such as the load balancing or the thread migration code), lock\n * acquire operations must be ordered by ascending &runqueue.\n */\nstruct rq {\n\t/* runqueue lock: */\n\traw_spinlock_t lock;\n\n\t/*\n\t * nr_running and cpu_load should be in the same cacheline because\n\t * remote CPUs use both these fields when doing load calculation.\n\t */\n\tunsigned long nr_running;\n\t#define CPU_LOAD_IDX_MAX 5\n\tunsigned long cpu_load[CPU_LOAD_IDX_MAX];\n\tunsigned long last_load_update_tick;\n#ifdef CONFIG_NO_HZ\n\tu64 nohz_stamp;\n\tunsigned char nohz_balance_kick;\n#endif\n\tint skip_clock_update;\n\n\t/* capture load from *all* tasks on this cpu: */\n\tstruct load_weight load;\n\tunsigned long nr_load_updates;\n\tu64 nr_switches;\n\n\tstruct cfs_rq cfs;\n\tstruct rt_rq rt;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t/* list of leaf cfs_rq on this cpu: */\n\tstruct list_head leaf_cfs_rq_list;\n#endif\n#ifdef CONFIG_RT_GROUP_SCHED\n\tstruct list_head leaf_rt_rq_list;\n#endif\n\n\t/*\n\t * This is part of a global counter where only the total sum\n\t * over all CPUs matters. A task can increase this counter on\n\t * one CPU and if it got migrated afterwards it may decrease\n\t * it on another CPU. Always updated under the runqueue lock:\n\t */\n\tunsigned long nr_uninterruptible;\n\n\tstruct task_struct *curr, *idle, *stop;\n\tunsigned long next_balance;\n\tstruct mm_struct *prev_mm;\n\n\tu64 clock;\n\tu64 clock_task;\n\n\tatomic_t nr_iowait;\n\n#ifdef CONFIG_SMP\n\tstruct root_domain *rd;\n\tstruct sched_domain *sd;\n\n\tunsigned long cpu_power;\n\n\tunsigned char idle_at_tick;\n\t/* For active balancing */\n\tint post_schedule;\n\tint active_balance;\n\tint push_cpu;\n\tstruct cpu_stop_work active_balance_work;\n\t/* cpu of this runqueue: */\n\tint cpu;\n\tint online;\n\n\tunsigned long avg_load_per_task;\n\n\tu64 rt_avg;\n\tu64 age_stamp;\n\tu64 idle_stamp;\n\tu64 avg_idle;\n#endif\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n\tu64 prev_irq_time;\n#endif\n\n\t/* calc_load related fields */\n\tunsigned long calc_load_update;\n\tlong calc_load_active;\n\n#ifdef CONFIG_SCHED_HRTICK\n#ifdef CONFIG_SMP\n\tint hrtick_csd_pending;\n\tstruct call_single_data hrtick_csd;\n#endif\n\tstruct hrtimer hrtick_timer;\n#endif\n\n#ifdef CONFIG_SCHEDSTATS\n\t/* latency stats */\n\tstruct sched_info rq_sched_info;\n\tunsigned long long rq_cpu_time;\n\t/* could above be rq->cfs_rq.exec_clock + rq->rt_rq.rt_runtime ? */\n\n\t/* sys_sched_yield() stats */\n\tunsigned int yld_count;\n\n\t/* schedule() stats */\n\tunsigned int sched_switch;\n\tunsigned int sched_count;\n\tunsigned int sched_goidle;\n\n\t/* try_to_wake_up() stats */\n\tunsigned int ttwu_count;\n\tunsigned int ttwu_local;\n#endif\n\n#ifdef CONFIG_SMP\n\tstruct task_struct *wake_list;\n#endif\n};\n\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);\n\n\nstatic void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}\n\n#define rcu_dereference_check_sched_domain(p) \\\n\trcu_dereference_check((p), \\\n\t\t\t      rcu_read_lock_held() || \\\n\t\t\t      lockdep_is_held(&sched_domains_mutex))\n\n/*\n * The domain tree (rq->sd) is protected by RCU's quiescent state transition.\n * See detach_destroy_domains: synchronize_sched for details.\n *\n * The domain tree of any CPU may only be accessed from within\n * preempt-disabled sections.\n */\n#define for_each_domain(cpu, __sd) \\\n\tfor (__sd = rcu_dereference_check_sched_domain(cpu_rq(cpu)->sd); __sd; __sd = __sd->parent)\n\n#define cpu_rq(cpu)\t\t(&per_cpu(runqueues, (cpu)))\n#define this_rq()\t\t(&__get_cpu_var(runqueues))\n#define task_rq(p)\t\tcpu_rq(task_cpu(p))\n#define cpu_curr(cpu)\t\t(cpu_rq(cpu)->curr)\n#define raw_rq()\t\t(&__raw_get_cpu_var(runqueues))\n\n#ifdef CONFIG_CGROUP_SCHED\n\n/*\n * Return the group to which this tasks belongs.\n *\n * We use task_subsys_state_check() and extend the RCU verification with\n * pi->lock and rq->lock because cpu_cgroup_attach() holds those locks for each\n * task it moves into the cgroup. Therefore by holding either of those locks,\n * we pin the task to the current cgroup.\n */\nstatic inline struct task_group *task_group(struct task_struct *p)\n{\n\tstruct task_group *tg;\n\tstruct cgroup_subsys_state *css;\n\n\tcss = task_subsys_state_check(p, cpu_cgroup_subsys_id,\n\t\t\tlockdep_is_held(&p->pi_lock) ||\n\t\t\tlockdep_is_held(&task_rq(p)->lock));\n\ttg = container_of(css, struct task_group, css);\n\n\treturn autogroup_task_group(p, tg);\n}\n\n/* Change a task's cfs_rq and parent entity if it moves across CPUs/groups */\nstatic inline void set_task_rq(struct task_struct *p, unsigned int cpu)\n{\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tp->se.cfs_rq = task_group(p)->cfs_rq[cpu];\n\tp->se.parent = task_group(p)->se[cpu];\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tp->rt.rt_rq  = task_group(p)->rt_rq[cpu];\n\tp->rt.parent = task_group(p)->rt_se[cpu];\n#endif\n}\n\n#else /* CONFIG_CGROUP_SCHED */\n\nstatic inline void set_task_rq(struct task_struct *p, unsigned int cpu) { }\nstatic inline struct task_group *task_group(struct task_struct *p)\n{\n\treturn NULL;\n}\n\n#endif /* CONFIG_CGROUP_SCHED */\n\nstatic void update_rq_clock_task(struct rq *rq, s64 delta);\n\nstatic void update_rq_clock(struct rq *rq)\n{\n\ts64 delta;\n\n\tif (rq->skip_clock_update > 0)\n\t\treturn;\n\n\tdelta = sched_clock_cpu(cpu_of(rq)) - rq->clock;\n\trq->clock += delta;\n\tupdate_rq_clock_task(rq, delta);\n}\n\n/*\n * Tunables that become constants when CONFIG_SCHED_DEBUG is off:\n */\n#ifdef CONFIG_SCHED_DEBUG\n# define const_debug __read_mostly\n#else\n# define const_debug static const\n#endif\n\n/**\n * runqueue_is_locked - Returns true if the current cpu runqueue is locked\n * @cpu: the processor in question.\n *\n * This interface allows printk to be called with the runqueue lock\n * held and know whether or not it is OK to wake up the klogd.\n */\nint runqueue_is_locked(int cpu)\n{\n\treturn raw_spin_is_locked(&cpu_rq(cpu)->lock);\n}\n\n/*\n * Debugging: various feature bits\n */\n\n#define SCHED_FEAT(name, enabled)\t\\\n\t__SCHED_FEAT_##name ,\n\nenum {\n#include \"sched_features.h\"\n};\n\n#undef SCHED_FEAT\n\n#define SCHED_FEAT(name, enabled)\t\\\n\t(1UL << __SCHED_FEAT_##name) * enabled |\n\nconst_debug unsigned int sysctl_sched_features =\n#include \"sched_features.h\"\n\t0;\n\n#undef SCHED_FEAT\n\n#ifdef CONFIG_SCHED_DEBUG\n#define SCHED_FEAT(name, enabled)\t\\\n\t#name ,\n\nstatic __read_mostly char *sched_feat_names[] = {\n#include \"sched_features.h\"\n\tNULL\n};\n\n#undef SCHED_FEAT\n\nstatic int sched_feat_show(struct seq_file *m, void *v)\n{\n\tint i;\n\n\tfor (i = 0; sched_feat_names[i]; i++) {\n\t\tif (!(sysctl_sched_features & (1UL << i)))\n\t\t\tseq_puts(m, \"NO_\");\n\t\tseq_printf(m, \"%s \", sched_feat_names[i]);\n\t}\n\tseq_puts(m, \"\\n\");\n\n\treturn 0;\n}\n\nstatic ssize_t\nsched_feat_write(struct file *filp, const char __user *ubuf,\n\t\tsize_t cnt, loff_t *ppos)\n{\n\tchar buf[64];\n\tchar *cmp;\n\tint neg = 0;\n\tint i;\n\n\tif (cnt > 63)\n\t\tcnt = 63;\n\n\tif (copy_from_user(&buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\tcmp = strstrip(buf);\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\tfor (i = 0; sched_feat_names[i]; i++) {\n\t\tif (strcmp(cmp, sched_feat_names[i]) == 0) {\n\t\t\tif (neg)\n\t\t\t\tsysctl_sched_features &= ~(1UL << i);\n\t\t\telse\n\t\t\t\tsysctl_sched_features |= (1UL << i);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!sched_feat_names[i])\n\t\treturn -EINVAL;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic int sched_feat_open(struct inode *inode, struct file *filp)\n{\n\treturn single_open(filp, sched_feat_show, NULL);\n}\n\nstatic const struct file_operations sched_feat_fops = {\n\t.open\t\t= sched_feat_open,\n\t.write\t\t= sched_feat_write,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= single_release,\n};\n\nstatic __init int sched_init_debug(void)\n{\n\tdebugfs_create_file(\"sched_features\", 0644, NULL, NULL,\n\t\t\t&sched_feat_fops);\n\n\treturn 0;\n}\nlate_initcall(sched_init_debug);\n\n#endif\n\n#define sched_feat(x) (sysctl_sched_features & (1UL << __SCHED_FEAT_##x))\n\n/*\n * Number of tasks to iterate in a single balance run.\n * Limited because this is done with IRQs disabled.\n */\nconst_debug unsigned int sysctl_sched_nr_migrate = 32;\n\n/*\n * period over which we average the RT time consumption, measured\n * in ms.\n *\n * default: 1s\n */\nconst_debug unsigned int sysctl_sched_time_avg = MSEC_PER_SEC;\n\n/*\n * period over which we measure -rt task cpu usage in us.\n * default: 1s\n */\nunsigned int sysctl_sched_rt_period = 1000000;\n\nstatic __read_mostly int scheduler_running;\n\n/*\n * part of the period that we allow rt tasks to run in us.\n * default: 0.95s\n */\nint sysctl_sched_rt_runtime = 950000;\n\nstatic inline u64 global_rt_period(void)\n{\n\treturn (u64)sysctl_sched_rt_period * NSEC_PER_USEC;\n}\n\nstatic inline u64 global_rt_runtime(void)\n{\n\tif (sysctl_sched_rt_runtime < 0)\n\t\treturn RUNTIME_INF;\n\n\treturn (u64)sysctl_sched_rt_runtime * NSEC_PER_USEC;\n}\n\n#ifndef prepare_arch_switch\n# define prepare_arch_switch(next)\tdo { } while (0)\n#endif\n#ifndef finish_arch_switch\n# define finish_arch_switch(prev)\tdo { } while (0)\n#endif\n\nstatic inline int task_current(struct rq *rq, struct task_struct *p)\n{\n\treturn rq->curr == p;\n}\n\nstatic inline int task_running(struct rq *rq, struct task_struct *p)\n{\n#ifdef CONFIG_SMP\n\treturn p->on_cpu;\n#else\n\treturn task_current(rq, p);\n#endif\n}\n\n#ifndef __ARCH_WANT_UNLOCKED_CTXSW\nstatic inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)\n{\n#ifdef CONFIG_SMP\n\t/*\n\t * We can optimise this out completely for !SMP, because the\n\t * SMP rebalancing from interrupt is the only thing that cares\n\t * here.\n\t */\n\tnext->on_cpu = 1;\n#endif\n}\n\nstatic inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)\n{\n#ifdef CONFIG_SMP\n\t/*\n\t * After ->on_cpu is cleared, the task can be moved to a different CPU.\n\t * We must ensure this doesn't happen until the switch is completely\n\t * finished.\n\t */\n\tsmp_wmb();\n\tprev->on_cpu = 0;\n#endif\n#ifdef CONFIG_DEBUG_SPINLOCK\n\t/* this is a valid case when another task releases the spinlock */\n\trq->lock.owner = current;\n#endif\n\t/*\n\t * If we are tracking spinlock dependencies then we have to\n\t * fix up the runqueue lock - which gets 'carried over' from\n\t * prev into current:\n\t */\n\tspin_acquire(&rq->lock.dep_map, 0, 0, _THIS_IP_);\n\n\traw_spin_unlock_irq(&rq->lock);\n}\n\n#else /* __ARCH_WANT_UNLOCKED_CTXSW */\nstatic inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)\n{\n#ifdef CONFIG_SMP\n\t/*\n\t * We can optimise this out completely for !SMP, because the\n\t * SMP rebalancing from interrupt is the only thing that cares\n\t * here.\n\t */\n\tnext->on_cpu = 1;\n#endif\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\traw_spin_unlock_irq(&rq->lock);\n#else\n\traw_spin_unlock(&rq->lock);\n#endif\n}\n\nstatic inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)\n{\n#ifdef CONFIG_SMP\n\t/*\n\t * After ->on_cpu is cleared, the task can be moved to a different CPU.\n\t * We must ensure this doesn't happen until the switch is completely\n\t * finished.\n\t */\n\tsmp_wmb();\n\tprev->on_cpu = 0;\n#endif\n#ifndef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\tlocal_irq_enable();\n#endif\n}\n#endif /* __ARCH_WANT_UNLOCKED_CTXSW */\n\n/*\n * __task_rq_lock - lock the rq @p resides on.\n */\nstatic inline struct rq *__task_rq_lock(struct task_struct *p)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tfor (;;) {\n\t\trq = task_rq(p);\n\t\traw_spin_lock(&rq->lock);\n\t\tif (likely(rq == task_rq(p)))\n\t\t\treturn rq;\n\t\traw_spin_unlock(&rq->lock);\n\t}\n}\n\n/*\n * task_rq_lock - lock p->pi_lock and lock the rq @p resides on.\n */\nstatic struct rq *task_rq_lock(struct task_struct *p, unsigned long *flags)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tfor (;;) {\n\t\traw_spin_lock_irqsave(&p->pi_lock, *flags);\n\t\trq = task_rq(p);\n\t\traw_spin_lock(&rq->lock);\n\t\tif (likely(rq == task_rq(p)))\n\t\t\treturn rq;\n\t\traw_spin_unlock(&rq->lock);\n\t\traw_spin_unlock_irqrestore(&p->pi_lock, *flags);\n\t}\n}\n\nstatic void __task_rq_unlock(struct rq *rq)\n\t__releases(rq->lock)\n{\n\traw_spin_unlock(&rq->lock);\n}\n\nstatic inline void\ntask_rq_unlock(struct rq *rq, struct task_struct *p, unsigned long *flags)\n\t__releases(rq->lock)\n\t__releases(p->pi_lock)\n{\n\traw_spin_unlock(&rq->lock);\n\traw_spin_unlock_irqrestore(&p->pi_lock, *flags);\n}\n\n/*\n * this_rq_lock - lock this runqueue and disable interrupts.\n */\nstatic struct rq *this_rq_lock(void)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tlocal_irq_disable();\n\trq = this_rq();\n\traw_spin_lock(&rq->lock);\n\n\treturn rq;\n}\n\n#ifdef CONFIG_SCHED_HRTICK\n/*\n * Use HR-timers to deliver accurate preemption points.\n *\n * Its all a bit involved since we cannot program an hrt while holding the\n * rq->lock. So what we do is store a state in in rq->hrtick_* and ask for a\n * reschedule event.\n *\n * When we get rescheduled we reprogram the hrtick_timer outside of the\n * rq->lock.\n */\n\n/*\n * Use hrtick when:\n *  - enabled by features\n *  - hrtimer is actually high res\n */\nstatic inline int hrtick_enabled(struct rq *rq)\n{\n\tif (!sched_feat(HRTICK))\n\t\treturn 0;\n\tif (!cpu_active(cpu_of(rq)))\n\t\treturn 0;\n\treturn hrtimer_is_hres_active(&rq->hrtick_timer);\n}\n\nstatic void hrtick_clear(struct rq *rq)\n{\n\tif (hrtimer_active(&rq->hrtick_timer))\n\t\thrtimer_cancel(&rq->hrtick_timer);\n}\n\n/*\n * High-resolution timer tick.\n * Runs from hardirq context with interrupts disabled.\n */\nstatic enum hrtimer_restart hrtick(struct hrtimer *timer)\n{\n\tstruct rq *rq = container_of(timer, struct rq, hrtick_timer);\n\n\tWARN_ON_ONCE(cpu_of(rq) != smp_processor_id());\n\n\traw_spin_lock(&rq->lock);\n\tupdate_rq_clock(rq);\n\trq->curr->sched_class->task_tick(rq, rq->curr, 1);\n\traw_spin_unlock(&rq->lock);\n\n\treturn HRTIMER_NORESTART;\n}\n\n#ifdef CONFIG_SMP\n/*\n * called from hardirq (IPI) context\n */\nstatic void __hrtick_start(void *arg)\n{\n\tstruct rq *rq = arg;\n\n\traw_spin_lock(&rq->lock);\n\thrtimer_restart(&rq->hrtick_timer);\n\trq->hrtick_csd_pending = 0;\n\traw_spin_unlock(&rq->lock);\n}\n\n/*\n * Called to set the hrtick timer state.\n *\n * called with rq->lock held and irqs disabled\n */\nstatic void hrtick_start(struct rq *rq, u64 delay)\n{\n\tstruct hrtimer *timer = &rq->hrtick_timer;\n\tktime_t time = ktime_add_ns(timer->base->get_time(), delay);\n\n\thrtimer_set_expires(timer, time);\n\n\tif (rq == this_rq()) {\n\t\thrtimer_restart(timer);\n\t} else if (!rq->hrtick_csd_pending) {\n\t\t__smp_call_function_single(cpu_of(rq), &rq->hrtick_csd, 0);\n\t\trq->hrtick_csd_pending = 1;\n\t}\n}\n\nstatic int\nhotplug_hrtick(struct notifier_block *nfb, unsigned long action, void *hcpu)\n{\n\tint cpu = (int)(long)hcpu;\n\n\tswitch (action) {\n\tcase CPU_UP_CANCELED:\n\tcase CPU_UP_CANCELED_FROZEN:\n\tcase CPU_DOWN_PREPARE:\n\tcase CPU_DOWN_PREPARE_FROZEN:\n\tcase CPU_DEAD:\n\tcase CPU_DEAD_FROZEN:\n\t\thrtick_clear(cpu_rq(cpu));\n\t\treturn NOTIFY_OK;\n\t}\n\n\treturn NOTIFY_DONE;\n}\n\nstatic __init void init_hrtick(void)\n{\n\thotcpu_notifier(hotplug_hrtick, 0);\n}\n#else\n/*\n * Called to set the hrtick timer state.\n *\n * called with rq->lock held and irqs disabled\n */\nstatic void hrtick_start(struct rq *rq, u64 delay)\n{\n\t__hrtimer_start_range_ns(&rq->hrtick_timer, ns_to_ktime(delay), 0,\n\t\t\tHRTIMER_MODE_REL_PINNED, 0);\n}\n\nstatic inline void init_hrtick(void)\n{\n}\n#endif /* CONFIG_SMP */\n\nstatic void init_rq_hrtick(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\trq->hrtick_csd_pending = 0;\n\n\trq->hrtick_csd.flags = 0;\n\trq->hrtick_csd.func = __hrtick_start;\n\trq->hrtick_csd.info = rq;\n#endif\n\n\thrtimer_init(&rq->hrtick_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\trq->hrtick_timer.function = hrtick;\n}\n#else\t/* CONFIG_SCHED_HRTICK */\nstatic inline void hrtick_clear(struct rq *rq)\n{\n}\n\nstatic inline void init_rq_hrtick(struct rq *rq)\n{\n}\n\nstatic inline void init_hrtick(void)\n{\n}\n#endif\t/* CONFIG_SCHED_HRTICK */\n\n/*\n * resched_task - mark a task 'to be rescheduled now'.\n *\n * On UP this means the setting of the need_resched flag, on SMP it\n * might also involve a cross-CPU call to trigger the scheduler on\n * the target CPU.\n */\n#ifdef CONFIG_SMP\n\n#ifndef tsk_is_polling\n#define tsk_is_polling(t) test_tsk_thread_flag(t, TIF_POLLING_NRFLAG)\n#endif\n\nstatic void resched_task(struct task_struct *p)\n{\n\tint cpu;\n\n\tassert_raw_spin_locked(&task_rq(p)->lock);\n\n\tif (test_tsk_need_resched(p))\n\t\treturn;\n\n\tset_tsk_need_resched(p);\n\n\tcpu = task_cpu(p);\n\tif (cpu == smp_processor_id())\n\t\treturn;\n\n\t/* NEED_RESCHED must be visible before we test polling */\n\tsmp_mb();\n\tif (!tsk_is_polling(p))\n\t\tsmp_send_reschedule(cpu);\n}\n\nstatic void resched_cpu(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long flags;\n\n\tif (!raw_spin_trylock_irqsave(&rq->lock, flags))\n\t\treturn;\n\tresched_task(cpu_curr(cpu));\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n}\n\n#ifdef CONFIG_NO_HZ\n/*\n * In the semi idle case, use the nearest busy cpu for migrating timers\n * from an idle cpu.  This is good for power-savings.\n *\n * We don't do similar optimization for completely idle system, as\n * selecting an idle cpu will add more delays to the timers than intended\n * (as that cpu's timer base may not be uptodate wrt jiffies etc).\n */\nint get_nohz_timer_target(void)\n{\n\tint cpu = smp_processor_id();\n\tint i;\n\tstruct sched_domain *sd;\n\n\trcu_read_lock();\n\tfor_each_domain(cpu, sd) {\n\t\tfor_each_cpu(i, sched_domain_span(sd)) {\n\t\t\tif (!idle_cpu(i)) {\n\t\t\t\tcpu = i;\n\t\t\t\tgoto unlock;\n\t\t\t}\n\t\t}\n\t}\nunlock:\n\trcu_read_unlock();\n\treturn cpu;\n}\n/*\n * When add_timer_on() enqueues a timer into the timer wheel of an\n * idle CPU then this timer might expire before the next timer event\n * which is scheduled to wake up that CPU. In case of a completely\n * idle system the next event might even be infinite time into the\n * future. wake_up_idle_cpu() ensures that the CPU is woken up and\n * leaves the inner idle loop so the newly added timer is taken into\n * account when the CPU goes back to idle and evaluates the timer\n * wheel for the next timer event.\n */\nvoid wake_up_idle_cpu(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tif (cpu == smp_processor_id())\n\t\treturn;\n\n\t/*\n\t * This is safe, as this function is called with the timer\n\t * wheel base lock of (cpu) held. When the CPU is on the way\n\t * to idle and has not yet set rq->curr to idle then it will\n\t * be serialized on the timer wheel base lock and take the new\n\t * timer into account automatically.\n\t */\n\tif (rq->curr != rq->idle)\n\t\treturn;\n\n\t/*\n\t * We can set TIF_RESCHED on the idle task of the other CPU\n\t * lockless. The worst case is that the other CPU runs the\n\t * idle task through an additional NOOP schedule()\n\t */\n\tset_tsk_need_resched(rq->idle);\n\n\t/* NEED_RESCHED must be visible before we test polling */\n\tsmp_mb();\n\tif (!tsk_is_polling(rq->idle))\n\t\tsmp_send_reschedule(cpu);\n}\n\n#endif /* CONFIG_NO_HZ */\n\nstatic u64 sched_avg_period(void)\n{\n\treturn (u64)sysctl_sched_time_avg * NSEC_PER_MSEC / 2;\n}\n\nstatic void sched_avg_update(struct rq *rq)\n{\n\ts64 period = sched_avg_period();\n\n\twhile ((s64)(rq->clock - rq->age_stamp) > period) {\n\t\t/*\n\t\t * Inline assembly required to prevent the compiler\n\t\t * optimising this loop into a divmod call.\n\t\t * See __iter_div_u64_rem() for another example of this.\n\t\t */\n\t\tasm(\"\" : \"+rm\" (rq->age_stamp));\n\t\trq->age_stamp += period;\n\t\trq->rt_avg /= 2;\n\t}\n}\n\nstatic void sched_rt_avg_update(struct rq *rq, u64 rt_delta)\n{\n\trq->rt_avg += rt_delta;\n\tsched_avg_update(rq);\n}\n\n#else /* !CONFIG_SMP */\nstatic void resched_task(struct task_struct *p)\n{\n\tassert_raw_spin_locked(&task_rq(p)->lock);\n\tset_tsk_need_resched(p);\n}\n\nstatic void sched_rt_avg_update(struct rq *rq, u64 rt_delta)\n{\n}\n\nstatic void sched_avg_update(struct rq *rq)\n{\n}\n#endif /* CONFIG_SMP */\n\n#if BITS_PER_LONG == 32\n# define WMULT_CONST\t(~0UL)\n#else\n# define WMULT_CONST\t(1UL << 32)\n#endif\n\n#define WMULT_SHIFT\t32\n\n/*\n * Shift right and round:\n */\n#define SRR(x, y) (((x) + (1UL << ((y) - 1))) >> (y))\n\n/*\n * delta *= weight / lw\n */\nstatic unsigned long\ncalc_delta_mine(unsigned long delta_exec, unsigned long weight,\n\t\tstruct load_weight *lw)\n{\n\tu64 tmp;\n\n\t/*\n\t * weight can be less than 2^SCHED_LOAD_RESOLUTION for task group sched\n\t * entities since MIN_SHARES = 2. Treat weight as 1 if less than\n\t * 2^SCHED_LOAD_RESOLUTION.\n\t */\n\tif (likely(weight > (1UL << SCHED_LOAD_RESOLUTION)))\n\t\ttmp = (u64)delta_exec * scale_load_down(weight);\n\telse\n\t\ttmp = (u64)delta_exec;\n\n\tif (!lw->inv_weight) {\n\t\tunsigned long w = scale_load_down(lw->weight);\n\n\t\tif (BITS_PER_LONG > 32 && unlikely(w >= WMULT_CONST))\n\t\t\tlw->inv_weight = 1;\n\t\telse if (unlikely(!w))\n\t\t\tlw->inv_weight = WMULT_CONST;\n\t\telse\n\t\t\tlw->inv_weight = WMULT_CONST / w;\n\t}\n\n\t/*\n\t * Check whether we'd overflow the 64-bit multiplication:\n\t */\n\tif (unlikely(tmp > WMULT_CONST))\n\t\ttmp = SRR(SRR(tmp, WMULT_SHIFT/2) * lw->inv_weight,\n\t\t\tWMULT_SHIFT/2);\n\telse\n\t\ttmp = SRR(tmp * lw->inv_weight, WMULT_SHIFT);\n\n\treturn (unsigned long)min(tmp, (u64)(unsigned long)LONG_MAX);\n}\n\nstatic inline void update_load_add(struct load_weight *lw, unsigned long inc)\n{\n\tlw->weight += inc;\n\tlw->inv_weight = 0;\n}\n\nstatic inline void update_load_sub(struct load_weight *lw, unsigned long dec)\n{\n\tlw->weight -= dec;\n\tlw->inv_weight = 0;\n}\n\nstatic inline void update_load_set(struct load_weight *lw, unsigned long w)\n{\n\tlw->weight = w;\n\tlw->inv_weight = 0;\n}\n\n/*\n * To aid in avoiding the subversion of \"niceness\" due to uneven distribution\n * of tasks with abnormal \"nice\" values across CPUs the contribution that\n * each task makes to its run queue's load is weighted according to its\n * scheduling class and \"nice\" value. For SCHED_NORMAL tasks this is just a\n * scaled version of the new time slice allocation that they receive on time\n * slice expiry etc.\n */\n\n#define WEIGHT_IDLEPRIO                3\n#define WMULT_IDLEPRIO         1431655765\n\n/*\n * Nice levels are multiplicative, with a gentle 10% change for every\n * nice level changed. I.e. when a CPU-bound task goes from nice 0 to\n * nice 1, it will get ~10% less CPU time than another CPU-bound task\n * that remained on nice 0.\n *\n * The \"10% effect\" is relative and cumulative: from _any_ nice level,\n * if you go up 1 level, it's -10% CPU usage, if you go down 1 level\n * it's +10% CPU usage. (to achieve that we use a multiplier of 1.25.\n * If a task goes up by ~10% and another task goes down by ~10% then\n * the relative distance between them is ~25%.)\n */\nstatic const int prio_to_weight[40] = {\n /* -20 */     88761,     71755,     56483,     46273,     36291,\n /* -15 */     29154,     23254,     18705,     14949,     11916,\n /* -10 */      9548,      7620,      6100,      4904,      3906,\n /*  -5 */      3121,      2501,      1991,      1586,      1277,\n /*   0 */      1024,       820,       655,       526,       423,\n /*   5 */       335,       272,       215,       172,       137,\n /*  10 */       110,        87,        70,        56,        45,\n /*  15 */        36,        29,        23,        18,        15,\n};\n\n/*\n * Inverse (2^32/x) values of the prio_to_weight[] array, precalculated.\n *\n * In cases where the weight does not change often, we can use the\n * precalculated inverse to speed up arithmetics by turning divisions\n * into multiplications:\n */\nstatic const u32 prio_to_wmult[40] = {\n /* -20 */     48388,     59856,     76040,     92818,    118348,\n /* -15 */    147320,    184698,    229616,    287308,    360437,\n /* -10 */    449829,    563644,    704093,    875809,   1099582,\n /*  -5 */   1376151,   1717300,   2157191,   2708050,   3363326,\n /*   0 */   4194304,   5237765,   6557202,   8165337,  10153587,\n /*   5 */  12820798,  15790321,  19976592,  24970740,  31350126,\n /*  10 */  39045157,  49367440,  61356676,  76695844,  95443717,\n /*  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,\n};\n\n/* Time spent by the tasks of the cpu accounting group executing in ... */\nenum cpuacct_stat_index {\n\tCPUACCT_STAT_USER,\t/* ... user mode */\n\tCPUACCT_STAT_SYSTEM,\t/* ... kernel mode */\n\n\tCPUACCT_STAT_NSTATS,\n};\n\n#ifdef CONFIG_CGROUP_CPUACCT\nstatic void cpuacct_charge(struct task_struct *tsk, u64 cputime);\nstatic void cpuacct_update_stats(struct task_struct *tsk,\n\t\tenum cpuacct_stat_index idx, cputime_t val);\n#else\nstatic inline void cpuacct_charge(struct task_struct *tsk, u64 cputime) {}\nstatic inline void cpuacct_update_stats(struct task_struct *tsk,\n\t\tenum cpuacct_stat_index idx, cputime_t val) {}\n#endif\n\nstatic inline void inc_cpu_load(struct rq *rq, unsigned long load)\n{\n\tupdate_load_add(&rq->load, load);\n}\n\nstatic inline void dec_cpu_load(struct rq *rq, unsigned long load)\n{\n\tupdate_load_sub(&rq->load, load);\n}\n\n#if (defined(CONFIG_SMP) && defined(CONFIG_FAIR_GROUP_SCHED)) || defined(CONFIG_RT_GROUP_SCHED)\ntypedef int (*tg_visitor)(struct task_group *, void *);\n\n/*\n * Iterate the full tree, calling @down when first entering a node and @up when\n * leaving it for the final time.\n */\nstatic int walk_tg_tree(tg_visitor down, tg_visitor up, void *data)\n{\n\tstruct task_group *parent, *child;\n\tint ret;\n\n\trcu_read_lock();\n\tparent = &root_task_group;\ndown:\n\tret = (*down)(parent, data);\n\tif (ret)\n\t\tgoto out_unlock;\n\tlist_for_each_entry_rcu(child, &parent->children, siblings) {\n\t\tparent = child;\n\t\tgoto down;\n\nup:\n\t\tcontinue;\n\t}\n\tret = (*up)(parent, data);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tchild = parent;\n\tparent = parent->parent;\n\tif (parent)\n\t\tgoto up;\nout_unlock:\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic int tg_nop(struct task_group *tg, void *data)\n{\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_SMP\n/* Used instead of source_load when we know the type == 0 */\nstatic unsigned long weighted_cpuload(const int cpu)\n{\n\treturn cpu_rq(cpu)->load.weight;\n}\n\n/*\n * Return a low guess at the load of a migration-source cpu weighted\n * according to the scheduling class and \"nice\" value.\n *\n * We want to under-estimate the load of migration sources, to\n * balance conservatively.\n */\nstatic unsigned long source_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(cpu);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn min(rq->cpu_load[type-1], total);\n}\n\n/*\n * Return a high guess at the load of a migration-target cpu weighted\n * according to the scheduling class and \"nice\" value.\n */\nstatic unsigned long target_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(cpu);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn max(rq->cpu_load[type-1], total);\n}\n\nstatic unsigned long power_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_power;\n}\n\nstatic int task_hot(struct task_struct *p, u64 now, struct sched_domain *sd);\n\nstatic unsigned long cpu_avg_load_per_task(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long nr_running = ACCESS_ONCE(rq->nr_running);\n\n\tif (nr_running)\n\t\trq->avg_load_per_task = rq->load.weight / nr_running;\n\telse\n\t\trq->avg_load_per_task = 0;\n\n\treturn rq->avg_load_per_task;\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\n/*\n * Compute the cpu's hierarchical load factor for each task group.\n * This needs to be done in a top-down fashion because the load of a child\n * group is a fraction of its parents load.\n */\nstatic int tg_load_down(struct task_group *tg, void *data)\n{\n\tunsigned long load;\n\tlong cpu = (long)data;\n\n\tif (!tg->parent) {\n\t\tload = cpu_rq(cpu)->load.weight;\n\t} else {\n\t\tload = tg->parent->cfs_rq[cpu]->h_load;\n\t\tload *= tg->se[cpu]->load.weight;\n\t\tload /= tg->parent->cfs_rq[cpu]->load.weight + 1;\n\t}\n\n\ttg->cfs_rq[cpu]->h_load = load;\n\n\treturn 0;\n}\n\nstatic void update_h_load(long cpu)\n{\n\twalk_tg_tree(tg_load_down, tg_nop, (void *)cpu);\n}\n\n#endif\n\n#ifdef CONFIG_PREEMPT\n\nstatic void double_rq_lock(struct rq *rq1, struct rq *rq2);\n\n/*\n * fair double_lock_balance: Safely acquires both rq->locks in a fair\n * way at the expense of forcing extra atomic operations in all\n * invocations.  This assures that the double_lock is acquired using the\n * same underlying policy as the spinlock_t on this architecture, which\n * reduces latency compared to the unfair variant below.  However, it\n * also adds more overhead and therefore may reduce throughput.\n */\nstatic inline int _double_lock_balance(struct rq *this_rq, struct rq *busiest)\n\t__releases(this_rq->lock)\n\t__acquires(busiest->lock)\n\t__acquires(this_rq->lock)\n{\n\traw_spin_unlock(&this_rq->lock);\n\tdouble_rq_lock(this_rq, busiest);\n\n\treturn 1;\n}\n\n#else\n/*\n * Unfair double_lock_balance: Optimizes throughput at the expense of\n * latency by eliminating extra atomic operations when the locks are\n * already in proper order on entry.  This favors lower cpu-ids and will\n * grant the double lock to lower cpus over higher ids under contention,\n * regardless of entry order into the function.\n */\nstatic int _double_lock_balance(struct rq *this_rq, struct rq *busiest)\n\t__releases(this_rq->lock)\n\t__acquires(busiest->lock)\n\t__acquires(this_rq->lock)\n{\n\tint ret = 0;\n\n\tif (unlikely(!raw_spin_trylock(&busiest->lock))) {\n\t\tif (busiest < this_rq) {\n\t\t\traw_spin_unlock(&this_rq->lock);\n\t\t\traw_spin_lock(&busiest->lock);\n\t\t\traw_spin_lock_nested(&this_rq->lock,\n\t\t\t\t\t      SINGLE_DEPTH_NESTING);\n\t\t\tret = 1;\n\t\t} else\n\t\t\traw_spin_lock_nested(&busiest->lock,\n\t\t\t\t\t      SINGLE_DEPTH_NESTING);\n\t}\n\treturn ret;\n}\n\n#endif /* CONFIG_PREEMPT */\n\n/*\n * double_lock_balance - lock the busiest runqueue, this_rq is locked already.\n */\nstatic int double_lock_balance(struct rq *this_rq, struct rq *busiest)\n{\n\tif (unlikely(!irqs_disabled())) {\n\t\t/* printk() doesn't work good under rq->lock */\n\t\traw_spin_unlock(&this_rq->lock);\n\t\tBUG_ON(1);\n\t}\n\n\treturn _double_lock_balance(this_rq, busiest);\n}\n\nstatic inline void double_unlock_balance(struct rq *this_rq, struct rq *busiest)\n\t__releases(busiest->lock)\n{\n\traw_spin_unlock(&busiest->lock);\n\tlock_set_subclass(&this_rq->lock.dep_map, 0, _RET_IP_);\n}\n\n/*\n * double_rq_lock - safely lock two runqueues\n *\n * Note this does not disable interrupts like task_rq_lock,\n * you need to do so manually before calling.\n */\nstatic void double_rq_lock(struct rq *rq1, struct rq *rq2)\n\t__acquires(rq1->lock)\n\t__acquires(rq2->lock)\n{\n\tBUG_ON(!irqs_disabled());\n\tif (rq1 == rq2) {\n\t\traw_spin_lock(&rq1->lock);\n\t\t__acquire(rq2->lock);\t/* Fake it out ;) */\n\t} else {\n\t\tif (rq1 < rq2) {\n\t\t\traw_spin_lock(&rq1->lock);\n\t\t\traw_spin_lock_nested(&rq2->lock, SINGLE_DEPTH_NESTING);\n\t\t} else {\n\t\t\traw_spin_lock(&rq2->lock);\n\t\t\traw_spin_lock_nested(&rq1->lock, SINGLE_DEPTH_NESTING);\n\t\t}\n\t}\n}\n\n/*\n * double_rq_unlock - safely unlock two runqueues\n *\n * Note this does not restore interrupts like task_rq_unlock,\n * you need to do so manually after calling.\n */\nstatic void double_rq_unlock(struct rq *rq1, struct rq *rq2)\n\t__releases(rq1->lock)\n\t__releases(rq2->lock)\n{\n\traw_spin_unlock(&rq1->lock);\n\tif (rq1 != rq2)\n\t\traw_spin_unlock(&rq2->lock);\n\telse\n\t\t__release(rq2->lock);\n}\n\n#else /* CONFIG_SMP */\n\n/*\n * double_rq_lock - safely lock two runqueues\n *\n * Note this does not disable interrupts like task_rq_lock,\n * you need to do so manually before calling.\n */\nstatic void double_rq_lock(struct rq *rq1, struct rq *rq2)\n\t__acquires(rq1->lock)\n\t__acquires(rq2->lock)\n{\n\tBUG_ON(!irqs_disabled());\n\tBUG_ON(rq1 != rq2);\n\traw_spin_lock(&rq1->lock);\n\t__acquire(rq2->lock);\t/* Fake it out ;) */\n}\n\n/*\n * double_rq_unlock - safely unlock two runqueues\n *\n * Note this does not restore interrupts like task_rq_unlock,\n * you need to do so manually after calling.\n */\nstatic void double_rq_unlock(struct rq *rq1, struct rq *rq2)\n\t__releases(rq1->lock)\n\t__releases(rq2->lock)\n{\n\tBUG_ON(rq1 != rq2);\n\traw_spin_unlock(&rq1->lock);\n\t__release(rq2->lock);\n}\n\n#endif\n\nstatic void calc_load_account_idle(struct rq *this_rq);\nstatic void update_sysctl(void);\nstatic int get_update_sysctl_factor(void);\nstatic void update_cpu_load(struct rq *this_rq);\n\nstatic inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)\n{\n\tset_task_rq(p, cpu);\n#ifdef CONFIG_SMP\n\t/*\n\t * After ->cpu is set up to a new value, task_rq_lock(p, ...) can be\n\t * successfuly executed on another CPU. We must ensure that updates of\n\t * per-task data have been completed by this moment.\n\t */\n\tsmp_wmb();\n\ttask_thread_info(p)->cpu = cpu;\n#endif\n}\n\nstatic const struct sched_class rt_sched_class;\n\n#define sched_class_highest (&stop_sched_class)\n#define for_each_class(class) \\\n   for (class = sched_class_highest; class; class = class->next)\n\n#include \"sched_stats.h\"\n\nstatic void inc_nr_running(struct rq *rq)\n{\n\trq->nr_running++;\n}\n\nstatic void dec_nr_running(struct rq *rq)\n{\n\trq->nr_running--;\n}\n\nstatic void set_load_weight(struct task_struct *p)\n{\n\tint prio = p->static_prio - MAX_RT_PRIO;\n\tstruct load_weight *load = &p->se.load;\n\n\t/*\n\t * SCHED_IDLE tasks get minimal weight:\n\t */\n\tif (p->policy == SCHED_IDLE) {\n\t\tload->weight = scale_load(WEIGHT_IDLEPRIO);\n\t\tload->inv_weight = WMULT_IDLEPRIO;\n\t\treturn;\n\t}\n\n\tload->weight = scale_load(prio_to_weight[prio]);\n\tload->inv_weight = prio_to_wmult[prio];\n}\n\nstatic void enqueue_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tupdate_rq_clock(rq);\n\tsched_info_queued(p);\n\tp->sched_class->enqueue_task(rq, p, flags);\n}\n\nstatic void dequeue_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tupdate_rq_clock(rq);\n\tsched_info_dequeued(p);\n\tp->sched_class->dequeue_task(rq, p, flags);\n}\n\n/*\n * activate_task - move a task to the runqueue.\n */\nstatic void activate_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tif (task_contributes_to_load(p))\n\t\trq->nr_uninterruptible--;\n\n\tenqueue_task(rq, p, flags);\n\tinc_nr_running(rq);\n}\n\n/*\n * deactivate_task - remove a task from the runqueue.\n */\nstatic void deactivate_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tif (task_contributes_to_load(p))\n\t\trq->nr_uninterruptible++;\n\n\tdequeue_task(rq, p, flags);\n\tdec_nr_running(rq);\n}\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n\n/*\n * There are no locks covering percpu hardirq/softirq time.\n * They are only modified in account_system_vtime, on corresponding CPU\n * with interrupts disabled. So, writes are safe.\n * They are read and saved off onto struct rq in update_rq_clock().\n * This may result in other CPU reading this CPU's irq time and can\n * race with irq/account_system_vtime on this CPU. We would either get old\n * or new value with a side effect of accounting a slice of irq time to wrong\n * task when irq is in progress while we read rq->clock. That is a worthy\n * compromise in place of having locks on each irq in account_system_time.\n */\nstatic DEFINE_PER_CPU(u64, cpu_hardirq_time);\nstatic DEFINE_PER_CPU(u64, cpu_softirq_time);\n\nstatic DEFINE_PER_CPU(u64, irq_start_time);\nstatic int sched_clock_irqtime;\n\nvoid enable_sched_clock_irqtime(void)\n{\n\tsched_clock_irqtime = 1;\n}\n\nvoid disable_sched_clock_irqtime(void)\n{\n\tsched_clock_irqtime = 0;\n}\n\n#ifndef CONFIG_64BIT\nstatic DEFINE_PER_CPU(seqcount_t, irq_time_seq);\n\nstatic inline void irq_time_write_begin(void)\n{\n\t__this_cpu_inc(irq_time_seq.sequence);\n\tsmp_wmb();\n}\n\nstatic inline void irq_time_write_end(void)\n{\n\tsmp_wmb();\n\t__this_cpu_inc(irq_time_seq.sequence);\n}\n\nstatic inline u64 irq_time_read(int cpu)\n{\n\tu64 irq_time;\n\tunsigned seq;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&per_cpu(irq_time_seq, cpu));\n\t\tirq_time = per_cpu(cpu_softirq_time, cpu) +\n\t\t\t   per_cpu(cpu_hardirq_time, cpu);\n\t} while (read_seqcount_retry(&per_cpu(irq_time_seq, cpu), seq));\n\n\treturn irq_time;\n}\n#else /* CONFIG_64BIT */\nstatic inline void irq_time_write_begin(void)\n{\n}\n\nstatic inline void irq_time_write_end(void)\n{\n}\n\nstatic inline u64 irq_time_read(int cpu)\n{\n\treturn per_cpu(cpu_softirq_time, cpu) + per_cpu(cpu_hardirq_time, cpu);\n}\n#endif /* CONFIG_64BIT */\n\n/*\n * Called before incrementing preempt_count on {soft,}irq_enter\n * and before decrementing preempt_count on {soft,}irq_exit.\n */\nvoid account_system_vtime(struct task_struct *curr)\n{\n\tunsigned long flags;\n\ts64 delta;\n\tint cpu;\n\n\tif (!sched_clock_irqtime)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tcpu = smp_processor_id();\n\tdelta = sched_clock_cpu(cpu) - __this_cpu_read(irq_start_time);\n\t__this_cpu_add(irq_start_time, delta);\n\n\tirq_time_write_begin();\n\t/*\n\t * We do not account for softirq time from ksoftirqd here.\n\t * We want to continue accounting softirq time to ksoftirqd thread\n\t * in that case, so as not to confuse scheduler with a special task\n\t * that do not consume any time, but still wants to run.\n\t */\n\tif (hardirq_count())\n\t\t__this_cpu_add(cpu_hardirq_time, delta);\n\telse if (in_serving_softirq() && curr != this_cpu_ksoftirqd())\n\t\t__this_cpu_add(cpu_softirq_time, delta);\n\n\tirq_time_write_end();\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(account_system_vtime);\n\nstatic void update_rq_clock_task(struct rq *rq, s64 delta)\n{\n\ts64 irq_delta;\n\n\tirq_delta = irq_time_read(cpu_of(rq)) - rq->prev_irq_time;\n\n\t/*\n\t * Since irq_time is only updated on {soft,}irq_exit, we might run into\n\t * this case when a previous update_rq_clock() happened inside a\n\t * {soft,}irq region.\n\t *\n\t * When this happens, we stop ->clock_task and only update the\n\t * prev_irq_time stamp to account for the part that fit, so that a next\n\t * update will consume the rest. This ensures ->clock_task is\n\t * monotonic.\n\t *\n\t * It does however cause some slight miss-attribution of {soft,}irq\n\t * time, a more accurate solution would be to update the irq_time using\n\t * the current rq->clock timestamp, except that would require using\n\t * atomic ops.\n\t */\n\tif (irq_delta > delta)\n\t\tirq_delta = delta;\n\n\trq->prev_irq_time += irq_delta;\n\tdelta -= irq_delta;\n\trq->clock_task += delta;\n\n\tif (irq_delta && sched_feat(NONIRQ_POWER))\n\t\tsched_rt_avg_update(rq, irq_delta);\n}\n\nstatic int irqtime_account_hi_update(void)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tunsigned long flags;\n\tu64 latest_ns;\n\tint ret = 0;\n\n\tlocal_irq_save(flags);\n\tlatest_ns = this_cpu_read(cpu_hardirq_time);\n\tif (cputime64_gt(nsecs_to_cputime64(latest_ns), cpustat->irq))\n\t\tret = 1;\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\n\nstatic int irqtime_account_si_update(void)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tunsigned long flags;\n\tu64 latest_ns;\n\tint ret = 0;\n\n\tlocal_irq_save(flags);\n\tlatest_ns = this_cpu_read(cpu_softirq_time);\n\tif (cputime64_gt(nsecs_to_cputime64(latest_ns), cpustat->softirq))\n\t\tret = 1;\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\n\n#else /* CONFIG_IRQ_TIME_ACCOUNTING */\n\n#define sched_clock_irqtime\t(0)\n\nstatic void update_rq_clock_task(struct rq *rq, s64 delta)\n{\n\trq->clock_task += delta;\n}\n\n#endif /* CONFIG_IRQ_TIME_ACCOUNTING */\n\n#include \"sched_idletask.c\"\n#include \"sched_fair.c\"\n#include \"sched_rt.c\"\n#include \"sched_autogroup.c\"\n#include \"sched_stoptask.c\"\n#ifdef CONFIG_SCHED_DEBUG\n# include \"sched_debug.c\"\n#endif\n\nvoid sched_set_stop_task(int cpu, struct task_struct *stop)\n{\n\tstruct sched_param param = { .sched_priority = MAX_RT_PRIO - 1 };\n\tstruct task_struct *old_stop = cpu_rq(cpu)->stop;\n\n\tif (stop) {\n\t\t/*\n\t\t * Make it appear like a SCHED_FIFO task, its something\n\t\t * userspace knows about and won't get confused about.\n\t\t *\n\t\t * Also, it will make PI more or less work without too\n\t\t * much confusion -- but then, stop work should not\n\t\t * rely on PI working anyway.\n\t\t */\n\t\tsched_setscheduler_nocheck(stop, SCHED_FIFO, &param);\n\n\t\tstop->sched_class = &stop_sched_class;\n\t}\n\n\tcpu_rq(cpu)->stop = stop;\n\n\tif (old_stop) {\n\t\t/*\n\t\t * Reset it back to a normal scheduling class so that\n\t\t * it can die in pieces.\n\t\t */\n\t\told_stop->sched_class = &rt_sched_class;\n\t}\n}\n\n/*\n * __normal_prio - return the priority that is based on the static prio\n */\nstatic inline int __normal_prio(struct task_struct *p)\n{\n\treturn p->static_prio;\n}\n\n/*\n * Calculate the expected normal priority: i.e. priority\n * without taking RT-inheritance into account. Might be\n * boosted by interactivity modifiers. Changes upon fork,\n * setprio syscalls, and whenever the interactivity\n * estimator recalculates.\n */\nstatic inline int normal_prio(struct task_struct *p)\n{\n\tint prio;\n\n\tif (task_has_rt_policy(p))\n\t\tprio = MAX_RT_PRIO-1 - p->rt_priority;\n\telse\n\t\tprio = __normal_prio(p);\n\treturn prio;\n}\n\n/*\n * Calculate the current priority, i.e. the priority\n * taken into account by the scheduler. This value might\n * be boosted by RT tasks, or might be boosted by\n * interactivity modifiers. Will be RT if the task got\n * RT-boosted. If not then it returns p->normal_prio.\n */\nstatic int effective_prio(struct task_struct *p)\n{\n\tp->normal_prio = normal_prio(p);\n\t/*\n\t * If we are RT tasks or we were boosted to RT priority,\n\t * keep the priority unchanged. Otherwise, update priority\n\t * to the normal priority:\n\t */\n\tif (!rt_prio(p->prio))\n\t\treturn p->normal_prio;\n\treturn p->prio;\n}\n\n/**\n * task_curr - is this task currently executing on a CPU?\n * @p: the task in question.\n */\ninline int task_curr(const struct task_struct *p)\n{\n\treturn cpu_curr(task_cpu(p)) == p;\n}\n\nstatic inline void check_class_changed(struct rq *rq, struct task_struct *p,\n\t\t\t\t       const struct sched_class *prev_class,\n\t\t\t\t       int oldprio)\n{\n\tif (prev_class != p->sched_class) {\n\t\tif (prev_class->switched_from)\n\t\t\tprev_class->switched_from(rq, p);\n\t\tp->sched_class->switched_to(rq, p);\n\t} else if (oldprio != p->prio)\n\t\tp->sched_class->prio_changed(rq, p, oldprio);\n}\n\nstatic void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags)\n{\n\tconst struct sched_class *class;\n\n\tif (p->sched_class == rq->curr->sched_class) {\n\t\trq->curr->sched_class->check_preempt_curr(rq, p, flags);\n\t} else {\n\t\tfor_each_class(class) {\n\t\t\tif (class == rq->curr->sched_class)\n\t\t\t\tbreak;\n\t\t\tif (class == p->sched_class) {\n\t\t\t\tresched_task(rq->curr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * A queue event has occurred, and we're going to schedule.  In\n\t * this case, we can save a useless back to back clock update.\n\t */\n\tif (rq->curr->on_rq && test_tsk_need_resched(rq->curr))\n\t\trq->skip_clock_update = 1;\n}\n\n#ifdef CONFIG_SMP\n/*\n * Is this task likely cache-hot:\n */\nstatic int\ntask_hot(struct task_struct *p, u64 now, struct sched_domain *sd)\n{\n\ts64 delta;\n\n\tif (p->sched_class != &fair_sched_class)\n\t\treturn 0;\n\n\tif (unlikely(p->policy == SCHED_IDLE))\n\t\treturn 0;\n\n\t/*\n\t * Buddy candidates are cache hot:\n\t */\n\tif (sched_feat(CACHE_HOT_BUDDY) && this_rq()->nr_running &&\n\t\t\t(&p->se == cfs_rq_of(&p->se)->next ||\n\t\t\t &p->se == cfs_rq_of(&p->se)->last))\n\t\treturn 1;\n\n\tif (sysctl_sched_migration_cost == -1)\n\t\treturn 1;\n\tif (sysctl_sched_migration_cost == 0)\n\t\treturn 0;\n\n\tdelta = now - p->se.exec_start;\n\n\treturn delta < (s64)sysctl_sched_migration_cost;\n}\n\nvoid set_task_cpu(struct task_struct *p, unsigned int new_cpu)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\t/*\n\t * We should never call set_task_cpu() on a blocked task,\n\t * ttwu() will sort out the placement.\n\t */\n\tWARN_ON_ONCE(p->state != TASK_RUNNING && p->state != TASK_WAKING &&\n\t\t\t!(task_thread_info(p)->preempt_count & PREEMPT_ACTIVE));\n\n#ifdef CONFIG_LOCKDEP\n\t/*\n\t * The caller should hold either p->pi_lock or rq->lock, when changing\n\t * a task's CPU. ->pi_lock for waking tasks, rq->lock for runnable tasks.\n\t *\n\t * sched_move_task() holds both and thus holding either pins the cgroup,\n\t * see set_task_rq().\n\t *\n\t * Furthermore, all task_rq users should acquire both locks, see\n\t * task_rq_lock().\n\t */\n\tWARN_ON_ONCE(debug_locks && !(lockdep_is_held(&p->pi_lock) ||\n\t\t\t\t      lockdep_is_held(&task_rq(p)->lock)));\n#endif\n#endif\n\n\ttrace_sched_migrate_task(p, new_cpu);\n\n\tif (task_cpu(p) != new_cpu) {\n\t\tp->se.nr_migrations++;\n\t\tperf_sw_event(PERF_COUNT_SW_CPU_MIGRATIONS, 1, NULL, 0);\n\t}\n\n\t__set_task_cpu(p, new_cpu);\n}\n\nstruct migration_arg {\n\tstruct task_struct *task;\n\tint dest_cpu;\n};\n\nstatic int migration_cpu_stop(void *data);\n\n/*\n * wait_task_inactive - wait for a thread to unschedule.\n *\n * If @match_state is nonzero, it's the @p->state value just checked and\n * not expected to change.  If it changes, i.e. @p might have woken up,\n * then return zero.  When we succeed in waiting for @p to be off its CPU,\n * we return a positive number (its total switch count).  If a second call\n * a short while later returns the same number, the caller can be sure that\n * @p has remained unscheduled the whole time.\n *\n * The caller must ensure that the task *will* unschedule sometime soon,\n * else this function might spin for a *long* time. This function can't\n * be called with interrupts off, or it may introduce deadlock with\n * smp_call_function() if an IPI is sent by the same process we are\n * waiting to become inactive.\n */\nunsigned long wait_task_inactive(struct task_struct *p, long match_state)\n{\n\tunsigned long flags;\n\tint running, on_rq;\n\tunsigned long ncsw;\n\tstruct rq *rq;\n\n\tfor (;;) {\n\t\t/*\n\t\t * We do the initial early heuristics without holding\n\t\t * any task-queue locks at all. We'll only try to get\n\t\t * the runqueue lock when things look like they will\n\t\t * work out!\n\t\t */\n\t\trq = task_rq(p);\n\n\t\t/*\n\t\t * If the task is actively running on another CPU\n\t\t * still, just relax and busy-wait without holding\n\t\t * any locks.\n\t\t *\n\t\t * NOTE! Since we don't hold any locks, it's not\n\t\t * even sure that \"rq\" stays as the right runqueue!\n\t\t * But we don't care, since \"task_running()\" will\n\t\t * return false if the runqueue has changed and p\n\t\t * is actually now running somewhere else!\n\t\t */\n\t\twhile (task_running(rq, p)) {\n\t\t\tif (match_state && unlikely(p->state != match_state))\n\t\t\t\treturn 0;\n\t\t\tcpu_relax();\n\t\t}\n\n\t\t/*\n\t\t * Ok, time to look more closely! We need the rq\n\t\t * lock now, to be *sure*. If we're wrong, we'll\n\t\t * just go back and repeat.\n\t\t */\n\t\trq = task_rq_lock(p, &flags);\n\t\ttrace_sched_wait_task(p);\n\t\trunning = task_running(rq, p);\n\t\ton_rq = p->on_rq;\n\t\tncsw = 0;\n\t\tif (!match_state || p->state == match_state)\n\t\t\tncsw = p->nvcsw | LONG_MIN; /* sets MSB */\n\t\ttask_rq_unlock(rq, p, &flags);\n\n\t\t/*\n\t\t * If it changed from the expected state, bail out now.\n\t\t */\n\t\tif (unlikely(!ncsw))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Was it really running after all now that we\n\t\t * checked with the proper locks actually held?\n\t\t *\n\t\t * Oops. Go back and try again..\n\t\t */\n\t\tif (unlikely(running)) {\n\t\t\tcpu_relax();\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * It's not enough that it's not actively running,\n\t\t * it must be off the runqueue _entirely_, and not\n\t\t * preempted!\n\t\t *\n\t\t * So if it was still runnable (but just not actively\n\t\t * running right now), it's preempted, and we should\n\t\t * yield - it could be a while.\n\t\t */\n\t\tif (unlikely(on_rq)) {\n\t\t\tktime_t to = ktime_set(0, NSEC_PER_SEC/HZ);\n\n\t\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\t\tschedule_hrtimeout(&to, HRTIMER_MODE_REL);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Ahh, all good. It wasn't running, and it wasn't\n\t\t * runnable, which means that it will never become\n\t\t * running in the future either. We're all done!\n\t\t */\n\t\tbreak;\n\t}\n\n\treturn ncsw;\n}\n\n/***\n * kick_process - kick a running thread to enter/exit the kernel\n * @p: the to-be-kicked thread\n *\n * Cause a process which is running on another CPU to enter\n * kernel-mode, without any delay. (to get signals handled.)\n *\n * NOTE: this function doesn't have to take the runqueue lock,\n * because all it wants to ensure is that the remote task enters\n * the kernel. If the IPI races and the task has been migrated\n * to another CPU then no harm is done and the purpose has been\n * achieved as well.\n */\nvoid kick_process(struct task_struct *p)\n{\n\tint cpu;\n\n\tpreempt_disable();\n\tcpu = task_cpu(p);\n\tif ((cpu != smp_processor_id()) && task_curr(p))\n\t\tsmp_send_reschedule(cpu);\n\tpreempt_enable();\n}\nEXPORT_SYMBOL_GPL(kick_process);\n#endif /* CONFIG_SMP */\n\n#ifdef CONFIG_SMP\n/*\n * ->cpus_allowed is protected by both rq->lock and p->pi_lock\n */\nstatic int select_fallback_rq(int cpu, struct task_struct *p)\n{\n\tint dest_cpu;\n\tconst struct cpumask *nodemask = cpumask_of_node(cpu_to_node(cpu));\n\n\t/* Look for allowed, online CPU in same node. */\n\tfor_each_cpu_and(dest_cpu, nodemask, cpu_active_mask)\n\t\tif (cpumask_test_cpu(dest_cpu, &p->cpus_allowed))\n\t\t\treturn dest_cpu;\n\n\t/* Any allowed, online CPU? */\n\tdest_cpu = cpumask_any_and(&p->cpus_allowed, cpu_active_mask);\n\tif (dest_cpu < nr_cpu_ids)\n\t\treturn dest_cpu;\n\n\t/* No more Mr. Nice Guy. */\n\tdest_cpu = cpuset_cpus_allowed_fallback(p);\n\t/*\n\t * Don't tell them about moving exiting tasks or\n\t * kernel threads (both mm NULL), since they never\n\t * leave kernel.\n\t */\n\tif (p->mm && printk_ratelimit()) {\n\t\tprintk(KERN_INFO \"process %d (%s) no longer affine to cpu%d\\n\",\n\t\t\t\ttask_pid_nr(p), p->comm, cpu);\n\t}\n\n\treturn dest_cpu;\n}\n\n/*\n * The caller (fork, wakeup) owns p->pi_lock, ->cpus_allowed is stable.\n */\nstatic inline\nint select_task_rq(struct task_struct *p, int sd_flags, int wake_flags)\n{\n\tint cpu = p->sched_class->select_task_rq(p, sd_flags, wake_flags);\n\n\t/*\n\t * In order not to call set_task_cpu() on a blocking task we need\n\t * to rely on ttwu() to place the task on a valid ->cpus_allowed\n\t * cpu.\n\t *\n\t * Since this is common to all placement strategies, this lives here.\n\t *\n\t * [ this allows ->select_task() to simply return task_cpu(p) and\n\t *   not worry about this generic constraint ]\n\t */\n\tif (unlikely(!cpumask_test_cpu(cpu, &p->cpus_allowed) ||\n\t\t     !cpu_online(cpu)))\n\t\tcpu = select_fallback_rq(task_cpu(p), p);\n\n\treturn cpu;\n}\n\nstatic void update_avg(u64 *avg, u64 sample)\n{\n\ts64 diff = sample - *avg;\n\t*avg += diff >> 3;\n}\n#endif\n\nstatic void\nttwu_stat(struct task_struct *p, int cpu, int wake_flags)\n{\n#ifdef CONFIG_SCHEDSTATS\n\tstruct rq *rq = this_rq();\n\n#ifdef CONFIG_SMP\n\tint this_cpu = smp_processor_id();\n\n\tif (cpu == this_cpu) {\n\t\tschedstat_inc(rq, ttwu_local);\n\t\tschedstat_inc(p, se.statistics.nr_wakeups_local);\n\t} else {\n\t\tstruct sched_domain *sd;\n\n\t\tschedstat_inc(p, se.statistics.nr_wakeups_remote);\n\t\trcu_read_lock();\n\t\tfor_each_domain(this_cpu, sd) {\n\t\t\tif (cpumask_test_cpu(cpu, sched_domain_span(sd))) {\n\t\t\t\tschedstat_inc(sd, ttwu_wake_remote);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tif (wake_flags & WF_MIGRATED)\n\t\tschedstat_inc(p, se.statistics.nr_wakeups_migrate);\n\n#endif /* CONFIG_SMP */\n\n\tschedstat_inc(rq, ttwu_count);\n\tschedstat_inc(p, se.statistics.nr_wakeups);\n\n\tif (wake_flags & WF_SYNC)\n\t\tschedstat_inc(p, se.statistics.nr_wakeups_sync);\n\n#endif /* CONFIG_SCHEDSTATS */\n}\n\nstatic void ttwu_activate(struct rq *rq, struct task_struct *p, int en_flags)\n{\n\tactivate_task(rq, p, en_flags);\n\tp->on_rq = 1;\n\n\t/* if a worker is waking up, notify workqueue */\n\tif (p->flags & PF_WQ_WORKER)\n\t\twq_worker_waking_up(p, cpu_of(rq));\n}\n\n/*\n * Mark the task runnable and perform wakeup-preemption.\n */\nstatic void\nttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags)\n{\n\ttrace_sched_wakeup(p, true);\n\tcheck_preempt_curr(rq, p, wake_flags);\n\n\tp->state = TASK_RUNNING;\n#ifdef CONFIG_SMP\n\tif (p->sched_class->task_woken)\n\t\tp->sched_class->task_woken(rq, p);\n\n\tif (unlikely(rq->idle_stamp)) {\n\t\tu64 delta = rq->clock - rq->idle_stamp;\n\t\tu64 max = 2*sysctl_sched_migration_cost;\n\n\t\tif (delta > max)\n\t\t\trq->avg_idle = max;\n\t\telse\n\t\t\tupdate_avg(&rq->avg_idle, delta);\n\t\trq->idle_stamp = 0;\n\t}\n#endif\n}\n\nstatic void\nttwu_do_activate(struct rq *rq, struct task_struct *p, int wake_flags)\n{\n#ifdef CONFIG_SMP\n\tif (p->sched_contributes_to_load)\n\t\trq->nr_uninterruptible--;\n#endif\n\n\tttwu_activate(rq, p, ENQUEUE_WAKEUP | ENQUEUE_WAKING);\n\tttwu_do_wakeup(rq, p, wake_flags);\n}\n\n/*\n * Called in case the task @p isn't fully descheduled from its runqueue,\n * in this case we must do a remote wakeup. Its a 'light' wakeup though,\n * since all we need to do is flip p->state to TASK_RUNNING, since\n * the task is still ->on_rq.\n */\nstatic int ttwu_remote(struct task_struct *p, int wake_flags)\n{\n\tstruct rq *rq;\n\tint ret = 0;\n\n\trq = __task_rq_lock(p);\n\tif (p->on_rq) {\n\t\tttwu_do_wakeup(rq, p, wake_flags);\n\t\tret = 1;\n\t}\n\t__task_rq_unlock(rq);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_SMP\nstatic void sched_ttwu_pending(void)\n{\n\tstruct rq *rq = this_rq();\n\tstruct task_struct *list = xchg(&rq->wake_list, NULL);\n\n\tif (!list)\n\t\treturn;\n\n\traw_spin_lock(&rq->lock);\n\n\twhile (list) {\n\t\tstruct task_struct *p = list;\n\t\tlist = list->wake_entry;\n\t\tttwu_do_activate(rq, p, 0);\n\t}\n\n\traw_spin_unlock(&rq->lock);\n}\n\nvoid scheduler_ipi(void)\n{\n\tsched_ttwu_pending();\n}\n\nstatic void ttwu_queue_remote(struct task_struct *p, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct task_struct *next = rq->wake_list;\n\n\tfor (;;) {\n\t\tstruct task_struct *old = next;\n\n\t\tp->wake_entry = next;\n\t\tnext = cmpxchg(&rq->wake_list, old, p);\n\t\tif (next == old)\n\t\t\tbreak;\n\t}\n\n\tif (!next)\n\t\tsmp_send_reschedule(cpu);\n}\n\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\nstatic int ttwu_activate_remote(struct task_struct *p, int wake_flags)\n{\n\tstruct rq *rq;\n\tint ret = 0;\n\n\trq = __task_rq_lock(p);\n\tif (p->on_cpu) {\n\t\tttwu_activate(rq, p, ENQUEUE_WAKEUP);\n\t\tttwu_do_wakeup(rq, p, wake_flags);\n\t\tret = 1;\n\t}\n\t__task_rq_unlock(rq);\n\n\treturn ret;\n\n}\n#endif /* __ARCH_WANT_INTERRUPTS_ON_CTXSW */\n#endif /* CONFIG_SMP */\n\nstatic void ttwu_queue(struct task_struct *p, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n#if defined(CONFIG_SMP)\n\tif (sched_feat(TTWU_QUEUE) && cpu != smp_processor_id()) {\n\t\tsched_clock_cpu(cpu); /* sync clocks x-cpu */\n\t\tttwu_queue_remote(p, cpu);\n\t\treturn;\n\t}\n#endif\n\n\traw_spin_lock(&rq->lock);\n\tttwu_do_activate(rq, p, 0);\n\traw_spin_unlock(&rq->lock);\n}\n\n/**\n * try_to_wake_up - wake up a thread\n * @p: the thread to be awakened\n * @state: the mask of task states that can be woken\n * @wake_flags: wake modifier flags (WF_*)\n *\n * Put it on the run-queue if it's not already there. The \"current\"\n * thread is always on the run-queue (except when the actual\n * re-schedule is in progress), and as such you're allowed to do\n * the simpler \"current->state = TASK_RUNNING\" to mark yourself\n * runnable without the overhead of this.\n *\n * Returns %true if @p was woken up, %false if it was already running\n * or @state didn't match @p's state.\n */\nstatic int\ntry_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)\n{\n\tunsigned long flags;\n\tint cpu, success = 0;\n\n\tsmp_wmb();\n\traw_spin_lock_irqsave(&p->pi_lock, flags);\n\tif (!(p->state & state))\n\t\tgoto out;\n\n\tsuccess = 1; /* we're going to change ->state */\n\tcpu = task_cpu(p);\n\n\tif (p->on_rq && ttwu_remote(p, wake_flags))\n\t\tgoto stat;\n\n#ifdef CONFIG_SMP\n\t/*\n\t * If the owning (remote) cpu is still in the middle of schedule() with\n\t * this task as prev, wait until its done referencing the task.\n\t */\n\twhile (p->on_cpu) {\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\t\t/*\n\t\t * In case the architecture enables interrupts in\n\t\t * context_switch(), we cannot busy wait, since that\n\t\t * would lead to deadlocks when an interrupt hits and\n\t\t * tries to wake up @prev. So bail and do a complete\n\t\t * remote wakeup.\n\t\t */\n\t\tif (ttwu_activate_remote(p, wake_flags))\n\t\t\tgoto stat;\n#else\n\t\tcpu_relax();\n#endif\n\t}\n\t/*\n\t * Pairs with the smp_wmb() in finish_lock_switch().\n\t */\n\tsmp_rmb();\n\n\tp->sched_contributes_to_load = !!task_contributes_to_load(p);\n\tp->state = TASK_WAKING;\n\n\tif (p->sched_class->task_waking)\n\t\tp->sched_class->task_waking(p);\n\n\tcpu = select_task_rq(p, SD_BALANCE_WAKE, wake_flags);\n\tif (task_cpu(p) != cpu) {\n\t\twake_flags |= WF_MIGRATED;\n\t\tset_task_cpu(p, cpu);\n\t}\n#endif /* CONFIG_SMP */\n\n\tttwu_queue(p, cpu);\nstat:\n\tttwu_stat(p, cpu, wake_flags);\nout:\n\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\n\treturn success;\n}\n\n/**\n * try_to_wake_up_local - try to wake up a local task with rq lock held\n * @p: the thread to be awakened\n *\n * Put @p on the run-queue if it's not already there. The caller must\n * ensure that this_rq() is locked, @p is bound to this_rq() and not\n * the current task.\n */\nstatic void try_to_wake_up_local(struct task_struct *p)\n{\n\tstruct rq *rq = task_rq(p);\n\n\tBUG_ON(rq != this_rq());\n\tBUG_ON(p == current);\n\tlockdep_assert_held(&rq->lock);\n\n\tif (!raw_spin_trylock(&p->pi_lock)) {\n\t\traw_spin_unlock(&rq->lock);\n\t\traw_spin_lock(&p->pi_lock);\n\t\traw_spin_lock(&rq->lock);\n\t}\n\n\tif (!(p->state & TASK_NORMAL))\n\t\tgoto out;\n\n\tif (!p->on_rq)\n\t\tttwu_activate(rq, p, ENQUEUE_WAKEUP);\n\n\tttwu_do_wakeup(rq, p, 0);\n\tttwu_stat(p, smp_processor_id(), 0);\nout:\n\traw_spin_unlock(&p->pi_lock);\n}\n\n/**\n * wake_up_process - Wake up a specific process\n * @p: The process to be woken up.\n *\n * Attempt to wake up the nominated process and move it to the set of runnable\n * processes.  Returns 1 if the process was woken up, 0 if it was already\n * running.\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nint wake_up_process(struct task_struct *p)\n{\n\treturn try_to_wake_up(p, TASK_ALL, 0);\n}\nEXPORT_SYMBOL(wake_up_process);\n\nint wake_up_state(struct task_struct *p, unsigned int state)\n{\n\treturn try_to_wake_up(p, state, 0);\n}\n\n/*\n * Perform scheduler related setup for a newly forked process p.\n * p is forked by current.\n *\n * __sched_fork() is basic setup used by init_idle() too:\n */\nstatic void __sched_fork(struct task_struct *p)\n{\n\tp->on_rq\t\t\t= 0;\n\n\tp->se.on_rq\t\t\t= 0;\n\tp->se.exec_start\t\t= 0;\n\tp->se.sum_exec_runtime\t\t= 0;\n\tp->se.prev_sum_exec_runtime\t= 0;\n\tp->se.nr_migrations\t\t= 0;\n\tp->se.vruntime\t\t\t= 0;\n\tINIT_LIST_HEAD(&p->se.group_node);\n\n#ifdef CONFIG_SCHEDSTATS\n\tmemset(&p->se.statistics, 0, sizeof(p->se.statistics));\n#endif\n\n\tINIT_LIST_HEAD(&p->rt.run_list);\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\tINIT_HLIST_HEAD(&p->preempt_notifiers);\n#endif\n}\n\n/*\n * fork()/clone()-time setup:\n */\nvoid sched_fork(struct task_struct *p)\n{\n\tunsigned long flags;\n\tint cpu = get_cpu();\n\n\t__sched_fork(p);\n\t/*\n\t * We mark the process as running here. This guarantees that\n\t * nobody will actually run it, and a signal or other external\n\t * event cannot wake it up and insert it on the runqueue either.\n\t */\n\tp->state = TASK_RUNNING;\n\n\t/*\n\t * Revert to default priority/policy on fork if requested.\n\t */\n\tif (unlikely(p->sched_reset_on_fork)) {\n\t\tif (p->policy == SCHED_FIFO || p->policy == SCHED_RR) {\n\t\t\tp->policy = SCHED_NORMAL;\n\t\t\tp->normal_prio = p->static_prio;\n\t\t}\n\n\t\tif (PRIO_TO_NICE(p->static_prio) < 0) {\n\t\t\tp->static_prio = NICE_TO_PRIO(0);\n\t\t\tp->normal_prio = p->static_prio;\n\t\t\tset_load_weight(p);\n\t\t}\n\n\t\t/*\n\t\t * We don't need the reset flag anymore after the fork. It has\n\t\t * fulfilled its duty:\n\t\t */\n\t\tp->sched_reset_on_fork = 0;\n\t}\n\n\t/*\n\t * Make sure we do not leak PI boosting priority to the child.\n\t */\n\tp->prio = current->normal_prio;\n\n\tif (!rt_prio(p->prio))\n\t\tp->sched_class = &fair_sched_class;\n\n\tif (p->sched_class->task_fork)\n\t\tp->sched_class->task_fork(p);\n\n\t/*\n\t * The child is not yet in the pid-hash so no cgroup attach races,\n\t * and the cgroup is pinned to this child due to cgroup_fork()\n\t * is ran before sched_fork().\n\t *\n\t * Silence PROVE_RCU.\n\t */\n\traw_spin_lock_irqsave(&p->pi_lock, flags);\n\tset_task_cpu(p, cpu);\n\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\n#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)\n\tif (likely(sched_info_on()))\n\t\tmemset(&p->sched_info, 0, sizeof(p->sched_info));\n#endif\n#if defined(CONFIG_SMP)\n\tp->on_cpu = 0;\n#endif\n#ifdef CONFIG_PREEMPT\n\t/* Want to start with kernel preemption disabled. */\n\ttask_thread_info(p)->preempt_count = 1;\n#endif\n#ifdef CONFIG_SMP\n\tplist_node_init(&p->pushable_tasks, MAX_PRIO);\n#endif\n\n\tput_cpu();\n}\n\n/*\n * wake_up_new_task - wake up a newly created task for the first time.\n *\n * This function will do some initial scheduler statistics housekeeping\n * that must be done for every newly created context, then puts the task\n * on the runqueue and wakes it.\n */\nvoid wake_up_new_task(struct task_struct *p)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\n\traw_spin_lock_irqsave(&p->pi_lock, flags);\n#ifdef CONFIG_SMP\n\t/*\n\t * Fork balancing, do it here and not earlier because:\n\t *  - cpus_allowed can change in the fork path\n\t *  - any previously selected cpu might disappear through hotplug\n\t */\n\tset_task_cpu(p, select_task_rq(p, SD_BALANCE_FORK, 0));\n#endif\n\n\trq = __task_rq_lock(p);\n\tactivate_task(rq, p, 0);\n\tp->on_rq = 1;\n\ttrace_sched_wakeup_new(p, true);\n\tcheck_preempt_curr(rq, p, WF_FORK);\n#ifdef CONFIG_SMP\n\tif (p->sched_class->task_woken)\n\t\tp->sched_class->task_woken(rq, p);\n#endif\n\ttask_rq_unlock(rq, p, &flags);\n}\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\n/**\n * preempt_notifier_register - tell me when current is being preempted & rescheduled\n * @notifier: notifier struct to register\n */\nvoid preempt_notifier_register(struct preempt_notifier *notifier)\n{\n\thlist_add_head(&notifier->link, &current->preempt_notifiers);\n}\nEXPORT_SYMBOL_GPL(preempt_notifier_register);\n\n/**\n * preempt_notifier_unregister - no longer interested in preemption notifications\n * @notifier: notifier struct to unregister\n *\n * This is safe to call from within a preemption notifier.\n */\nvoid preempt_notifier_unregister(struct preempt_notifier *notifier)\n{\n\thlist_del(&notifier->link);\n}\nEXPORT_SYMBOL_GPL(preempt_notifier_unregister);\n\nstatic void fire_sched_in_preempt_notifiers(struct task_struct *curr)\n{\n\tstruct preempt_notifier *notifier;\n\tstruct hlist_node *node;\n\n\thlist_for_each_entry(notifier, node, &curr->preempt_notifiers, link)\n\t\tnotifier->ops->sched_in(notifier, raw_smp_processor_id());\n}\n\nstatic void\nfire_sched_out_preempt_notifiers(struct task_struct *curr,\n\t\t\t\t struct task_struct *next)\n{\n\tstruct preempt_notifier *notifier;\n\tstruct hlist_node *node;\n\n\thlist_for_each_entry(notifier, node, &curr->preempt_notifiers, link)\n\t\tnotifier->ops->sched_out(notifier, next);\n}\n\n#else /* !CONFIG_PREEMPT_NOTIFIERS */\n\nstatic void fire_sched_in_preempt_notifiers(struct task_struct *curr)\n{\n}\n\nstatic void\nfire_sched_out_preempt_notifiers(struct task_struct *curr,\n\t\t\t\t struct task_struct *next)\n{\n}\n\n#endif /* CONFIG_PREEMPT_NOTIFIERS */\n\n/**\n * prepare_task_switch - prepare to switch tasks\n * @rq: the runqueue preparing to switch\n * @prev: the current task that is being switched out\n * @next: the task we are going to switch to.\n *\n * This is called with the rq lock held and interrupts off. It must\n * be paired with a subsequent finish_task_switch after the context\n * switch.\n *\n * prepare_task_switch sets up locking and calls architecture specific\n * hooks.\n */\nstatic inline void\nprepare_task_switch(struct rq *rq, struct task_struct *prev,\n\t\t    struct task_struct *next)\n{\n\tsched_info_switch(prev, next);\n\tperf_event_task_sched_out(prev, next);\n\tfire_sched_out_preempt_notifiers(prev, next);\n\tprepare_lock_switch(rq, next);\n\tprepare_arch_switch(next);\n\ttrace_sched_switch(prev, next);\n}\n\n/**\n * finish_task_switch - clean up after a task-switch\n * @rq: runqueue associated with task-switch\n * @prev: the thread we just switched away from.\n *\n * finish_task_switch must be called after the context switch, paired\n * with a prepare_task_switch call before the context switch.\n * finish_task_switch will reconcile locking set up by prepare_task_switch,\n * and do any other architecture-specific cleanup actions.\n *\n * Note that we may have delayed dropping an mm in context_switch(). If\n * so, we finish that here outside of the runqueue lock. (Doing it\n * with the lock held can cause deadlocks; see schedule() for\n * details.)\n */\nstatic void finish_task_switch(struct rq *rq, struct task_struct *prev)\n\t__releases(rq->lock)\n{\n\tstruct mm_struct *mm = rq->prev_mm;\n\tlong prev_state;\n\n\trq->prev_mm = NULL;\n\n\t/*\n\t * A task struct has one reference for the use as \"current\".\n\t * If a task dies, then it sets TASK_DEAD in tsk->state and calls\n\t * schedule one last time. The schedule call will never return, and\n\t * the scheduled task must drop that reference.\n\t * The test for TASK_DEAD must occur while the runqueue locks are\n\t * still held, otherwise prev could be scheduled on another cpu, die\n\t * there before we look at prev->state, and then the reference would\n\t * be dropped twice.\n\t *\t\tManfred Spraul <manfred@colorfullife.com>\n\t */\n\tprev_state = prev->state;\n\tfinish_arch_switch(prev);\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\tlocal_irq_disable();\n#endif /* __ARCH_WANT_INTERRUPTS_ON_CTXSW */\n\tperf_event_task_sched_in(current);\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\tlocal_irq_enable();\n#endif /* __ARCH_WANT_INTERRUPTS_ON_CTXSW */\n\tfinish_lock_switch(rq, prev);\n\n\tfire_sched_in_preempt_notifiers(current);\n\tif (mm)\n\t\tmmdrop(mm);\n\tif (unlikely(prev_state == TASK_DEAD)) {\n\t\t/*\n\t\t * Remove function-return probe instances associated with this\n\t\t * task and put them back on the free list.\n\t\t */\n\t\tkprobe_flush_task(prev);\n\t\tput_task_struct(prev);\n\t}\n}\n\n#ifdef CONFIG_SMP\n\n/* assumes rq->lock is held */\nstatic inline void pre_schedule(struct rq *rq, struct task_struct *prev)\n{\n\tif (prev->sched_class->pre_schedule)\n\t\tprev->sched_class->pre_schedule(rq, prev);\n}\n\n/* rq->lock is NOT held, but preemption is disabled */\nstatic inline void post_schedule(struct rq *rq)\n{\n\tif (rq->post_schedule) {\n\t\tunsigned long flags;\n\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tif (rq->curr->sched_class->post_schedule)\n\t\t\trq->curr->sched_class->post_schedule(rq);\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\n\t\trq->post_schedule = 0;\n\t}\n}\n\n#else\n\nstatic inline void pre_schedule(struct rq *rq, struct task_struct *p)\n{\n}\n\nstatic inline void post_schedule(struct rq *rq)\n{\n}\n\n#endif\n\n/**\n * schedule_tail - first thing a freshly forked thread must call.\n * @prev: the thread we just switched away from.\n */\nasmlinkage void schedule_tail(struct task_struct *prev)\n\t__releases(rq->lock)\n{\n\tstruct rq *rq = this_rq();\n\n\tfinish_task_switch(rq, prev);\n\n\t/*\n\t * FIXME: do we need to worry about rq being invalidated by the\n\t * task_switch?\n\t */\n\tpost_schedule(rq);\n\n#ifdef __ARCH_WANT_UNLOCKED_CTXSW\n\t/* In this case, finish_task_switch does not reenable preemption */\n\tpreempt_enable();\n#endif\n\tif (current->set_child_tid)\n\t\tput_user(task_pid_vnr(current), current->set_child_tid);\n}\n\n/*\n * context_switch - switch to the new MM and the new\n * thread's register state.\n */\nstatic inline void\ncontext_switch(struct rq *rq, struct task_struct *prev,\n\t       struct task_struct *next)\n{\n\tstruct mm_struct *mm, *oldmm;\n\n\tprepare_task_switch(rq, prev, next);\n\n\tmm = next->mm;\n\toldmm = prev->active_mm;\n\t/*\n\t * For paravirt, this is coupled with an exit in switch_to to\n\t * combine the page table reload and the switch backend into\n\t * one hypercall.\n\t */\n\tarch_start_context_switch(prev);\n\n\tif (!mm) {\n\t\tnext->active_mm = oldmm;\n\t\tatomic_inc(&oldmm->mm_count);\n\t\tenter_lazy_tlb(oldmm, next);\n\t} else\n\t\tswitch_mm(oldmm, mm, next);\n\n\tif (!prev->mm) {\n\t\tprev->active_mm = NULL;\n\t\trq->prev_mm = oldmm;\n\t}\n\t/*\n\t * Since the runqueue lock will be released by the next\n\t * task (which is an invalid locking op but in the case\n\t * of the scheduler it's an obvious special-case), so we\n\t * do an early lockdep release here:\n\t */\n#ifndef __ARCH_WANT_UNLOCKED_CTXSW\n\tspin_release(&rq->lock.dep_map, 1, _THIS_IP_);\n#endif\n\n\t/* Here we just switch the register state and the stack. */\n\tswitch_to(prev, next, prev);\n\n\tbarrier();\n\t/*\n\t * this_rq must be evaluated again because prev may have moved\n\t * CPUs since it called schedule(), thus the 'rq' on its stack\n\t * frame will be invalid.\n\t */\n\tfinish_task_switch(this_rq(), prev);\n}\n\n/*\n * nr_running, nr_uninterruptible and nr_context_switches:\n *\n * externally visible scheduler statistics: current number of runnable\n * threads, current number of uninterruptible-sleeping threads, total\n * number of context switches performed since bootup.\n */\nunsigned long nr_running(void)\n{\n\tunsigned long i, sum = 0;\n\n\tfor_each_online_cpu(i)\n\t\tsum += cpu_rq(i)->nr_running;\n\n\treturn sum;\n}\n\nunsigned long nr_uninterruptible(void)\n{\n\tunsigned long i, sum = 0;\n\n\tfor_each_possible_cpu(i)\n\t\tsum += cpu_rq(i)->nr_uninterruptible;\n\n\t/*\n\t * Since we read the counters lockless, it might be slightly\n\t * inaccurate. Do not allow it to go below zero though:\n\t */\n\tif (unlikely((long)sum < 0))\n\t\tsum = 0;\n\n\treturn sum;\n}\n\nunsigned long long nr_context_switches(void)\n{\n\tint i;\n\tunsigned long long sum = 0;\n\n\tfor_each_possible_cpu(i)\n\t\tsum += cpu_rq(i)->nr_switches;\n\n\treturn sum;\n}\n\nunsigned long nr_iowait(void)\n{\n\tunsigned long i, sum = 0;\n\n\tfor_each_possible_cpu(i)\n\t\tsum += atomic_read(&cpu_rq(i)->nr_iowait);\n\n\treturn sum;\n}\n\nunsigned long nr_iowait_cpu(int cpu)\n{\n\tstruct rq *this = cpu_rq(cpu);\n\treturn atomic_read(&this->nr_iowait);\n}\n\nunsigned long this_cpu_load(void)\n{\n\tstruct rq *this = this_rq();\n\treturn this->cpu_load[0];\n}\n\n\n/* Variables and functions for calc_load */\nstatic atomic_long_t calc_load_tasks;\nstatic unsigned long calc_load_update;\nunsigned long avenrun[3];\nEXPORT_SYMBOL(avenrun);\n\nstatic long calc_load_fold_active(struct rq *this_rq)\n{\n\tlong nr_active, delta = 0;\n\n\tnr_active = this_rq->nr_running;\n\tnr_active += (long) this_rq->nr_uninterruptible;\n\n\tif (nr_active != this_rq->calc_load_active) {\n\t\tdelta = nr_active - this_rq->calc_load_active;\n\t\tthis_rq->calc_load_active = nr_active;\n\t}\n\n\treturn delta;\n}\n\nstatic unsigned long\ncalc_load(unsigned long load, unsigned long exp, unsigned long active)\n{\n\tload *= exp;\n\tload += active * (FIXED_1 - exp);\n\tload += 1UL << (FSHIFT - 1);\n\treturn load >> FSHIFT;\n}\n\n#ifdef CONFIG_NO_HZ\n/*\n * For NO_HZ we delay the active fold to the next LOAD_FREQ update.\n *\n * When making the ILB scale, we should try to pull this in as well.\n */\nstatic atomic_long_t calc_load_tasks_idle;\n\nstatic void calc_load_account_idle(struct rq *this_rq)\n{\n\tlong delta;\n\n\tdelta = calc_load_fold_active(this_rq);\n\tif (delta)\n\t\tatomic_long_add(delta, &calc_load_tasks_idle);\n}\n\nstatic long calc_load_fold_idle(void)\n{\n\tlong delta = 0;\n\n\t/*\n\t * Its got a race, we don't care...\n\t */\n\tif (atomic_long_read(&calc_load_tasks_idle))\n\t\tdelta = atomic_long_xchg(&calc_load_tasks_idle, 0);\n\n\treturn delta;\n}\n\n/**\n * fixed_power_int - compute: x^n, in O(log n) time\n *\n * @x:         base of the power\n * @frac_bits: fractional bits of @x\n * @n:         power to raise @x to.\n *\n * By exploiting the relation between the definition of the natural power\n * function: x^n := x*x*...*x (x multiplied by itself for n times), and\n * the binary encoding of numbers used by computers: n := \\Sum n_i * 2^i,\n * (where: n_i \\elem {0, 1}, the binary vector representing n),\n * we find: x^n := x^(\\Sum n_i * 2^i) := \\Prod x^(n_i * 2^i), which is\n * of course trivially computable in O(log_2 n), the length of our binary\n * vector.\n */\nstatic unsigned long\nfixed_power_int(unsigned long x, unsigned int frac_bits, unsigned int n)\n{\n\tunsigned long result = 1UL << frac_bits;\n\n\tif (n) for (;;) {\n\t\tif (n & 1) {\n\t\t\tresult *= x;\n\t\t\tresult += 1UL << (frac_bits - 1);\n\t\t\tresult >>= frac_bits;\n\t\t}\n\t\tn >>= 1;\n\t\tif (!n)\n\t\t\tbreak;\n\t\tx *= x;\n\t\tx += 1UL << (frac_bits - 1);\n\t\tx >>= frac_bits;\n\t}\n\n\treturn result;\n}\n\n/*\n * a1 = a0 * e + a * (1 - e)\n *\n * a2 = a1 * e + a * (1 - e)\n *    = (a0 * e + a * (1 - e)) * e + a * (1 - e)\n *    = a0 * e^2 + a * (1 - e) * (1 + e)\n *\n * a3 = a2 * e + a * (1 - e)\n *    = (a0 * e^2 + a * (1 - e) * (1 + e)) * e + a * (1 - e)\n *    = a0 * e^3 + a * (1 - e) * (1 + e + e^2)\n *\n *  ...\n *\n * an = a0 * e^n + a * (1 - e) * (1 + e + ... + e^n-1) [1]\n *    = a0 * e^n + a * (1 - e) * (1 - e^n)/(1 - e)\n *    = a0 * e^n + a * (1 - e^n)\n *\n * [1] application of the geometric series:\n *\n *              n         1 - x^(n+1)\n *     S_n := \\Sum x^i = -------------\n *             i=0          1 - x\n */\nstatic unsigned long\ncalc_load_n(unsigned long load, unsigned long exp,\n\t    unsigned long active, unsigned int n)\n{\n\n\treturn calc_load(load, fixed_power_int(exp, FSHIFT, n), active);\n}\n\n/*\n * NO_HZ can leave us missing all per-cpu ticks calling\n * calc_load_account_active(), but since an idle CPU folds its delta into\n * calc_load_tasks_idle per calc_load_account_idle(), all we need to do is fold\n * in the pending idle delta if our idle period crossed a load cycle boundary.\n *\n * Once we've updated the global active value, we need to apply the exponential\n * weights adjusted to the number of cycles missed.\n */\nstatic void calc_global_nohz(unsigned long ticks)\n{\n\tlong delta, active, n;\n\n\tif (time_before(jiffies, calc_load_update))\n\t\treturn;\n\n\t/*\n\t * If we crossed a calc_load_update boundary, make sure to fold\n\t * any pending idle changes, the respective CPUs might have\n\t * missed the tick driven calc_load_account_active() update\n\t * due to NO_HZ.\n\t */\n\tdelta = calc_load_fold_idle();\n\tif (delta)\n\t\tatomic_long_add(delta, &calc_load_tasks);\n\n\t/*\n\t * If we were idle for multiple load cycles, apply them.\n\t */\n\tif (ticks >= LOAD_FREQ) {\n\t\tn = ticks / LOAD_FREQ;\n\n\t\tactive = atomic_long_read(&calc_load_tasks);\n\t\tactive = active > 0 ? active * FIXED_1 : 0;\n\n\t\tavenrun[0] = calc_load_n(avenrun[0], EXP_1, active, n);\n\t\tavenrun[1] = calc_load_n(avenrun[1], EXP_5, active, n);\n\t\tavenrun[2] = calc_load_n(avenrun[2], EXP_15, active, n);\n\n\t\tcalc_load_update += n * LOAD_FREQ;\n\t}\n\n\t/*\n\t * Its possible the remainder of the above division also crosses\n\t * a LOAD_FREQ period, the regular check in calc_global_load()\n\t * which comes after this will take care of that.\n\t *\n\t * Consider us being 11 ticks before a cycle completion, and us\n\t * sleeping for 4*LOAD_FREQ + 22 ticks, then the above code will\n\t * age us 4 cycles, and the test in calc_global_load() will\n\t * pick up the final one.\n\t */\n}\n#else\nstatic void calc_load_account_idle(struct rq *this_rq)\n{\n}\n\nstatic inline long calc_load_fold_idle(void)\n{\n\treturn 0;\n}\n\nstatic void calc_global_nohz(unsigned long ticks)\n{\n}\n#endif\n\n/**\n * get_avenrun - get the load average array\n * @loads:\tpointer to dest load array\n * @offset:\toffset to add\n * @shift:\tshift count to shift the result left\n *\n * These values are estimates at best, so no need for locking.\n */\nvoid get_avenrun(unsigned long *loads, unsigned long offset, int shift)\n{\n\tloads[0] = (avenrun[0] + offset) << shift;\n\tloads[1] = (avenrun[1] + offset) << shift;\n\tloads[2] = (avenrun[2] + offset) << shift;\n}\n\n/*\n * calc_load - update the avenrun load estimates 10 ticks after the\n * CPUs have updated calc_load_tasks.\n */\nvoid calc_global_load(unsigned long ticks)\n{\n\tlong active;\n\n\tcalc_global_nohz(ticks);\n\n\tif (time_before(jiffies, calc_load_update + 10))\n\t\treturn;\n\n\tactive = atomic_long_read(&calc_load_tasks);\n\tactive = active > 0 ? active * FIXED_1 : 0;\n\n\tavenrun[0] = calc_load(avenrun[0], EXP_1, active);\n\tavenrun[1] = calc_load(avenrun[1], EXP_5, active);\n\tavenrun[2] = calc_load(avenrun[2], EXP_15, active);\n\n\tcalc_load_update += LOAD_FREQ;\n}\n\n/*\n * Called from update_cpu_load() to periodically update this CPU's\n * active count.\n */\nstatic void calc_load_account_active(struct rq *this_rq)\n{\n\tlong delta;\n\n\tif (time_before(jiffies, this_rq->calc_load_update))\n\t\treturn;\n\n\tdelta  = calc_load_fold_active(this_rq);\n\tdelta += calc_load_fold_idle();\n\tif (delta)\n\t\tatomic_long_add(delta, &calc_load_tasks);\n\n\tthis_rq->calc_load_update += LOAD_FREQ;\n}\n\n/*\n * The exact cpuload at various idx values, calculated at every tick would be\n * load = (2^idx - 1) / 2^idx * load + 1 / 2^idx * cur_load\n *\n * If a cpu misses updates for n-1 ticks (as it was idle) and update gets called\n * on nth tick when cpu may be busy, then we have:\n * load = ((2^idx - 1) / 2^idx)^(n-1) * load\n * load = (2^idx - 1) / 2^idx) * load + 1 / 2^idx * cur_load\n *\n * decay_load_missed() below does efficient calculation of\n * load = ((2^idx - 1) / 2^idx)^(n-1) * load\n * avoiding 0..n-1 loop doing load = ((2^idx - 1) / 2^idx) * load\n *\n * The calculation is approximated on a 128 point scale.\n * degrade_zero_ticks is the number of ticks after which load at any\n * particular idx is approximated to be zero.\n * degrade_factor is a precomputed table, a row for each load idx.\n * Each column corresponds to degradation factor for a power of two ticks,\n * based on 128 point scale.\n * Example:\n * row 2, col 3 (=12) says that the degradation at load idx 2 after\n * 8 ticks is 12/128 (which is an approximation of exact factor 3^8/4^8).\n *\n * With this power of 2 load factors, we can degrade the load n times\n * by looking at 1 bits in n and doing as many mult/shift instead of\n * n mult/shifts needed by the exact degradation.\n */\n#define DEGRADE_SHIFT\t\t7\nstatic const unsigned char\n\t\tdegrade_zero_ticks[CPU_LOAD_IDX_MAX] = {0, 8, 32, 64, 128};\nstatic const unsigned char\n\t\tdegrade_factor[CPU_LOAD_IDX_MAX][DEGRADE_SHIFT + 1] = {\n\t\t\t\t\t{0, 0, 0, 0, 0, 0, 0, 0},\n\t\t\t\t\t{64, 32, 8, 0, 0, 0, 0, 0},\n\t\t\t\t\t{96, 72, 40, 12, 1, 0, 0},\n\t\t\t\t\t{112, 98, 75, 43, 15, 1, 0},\n\t\t\t\t\t{120, 112, 98, 76, 45, 16, 2} };\n\n/*\n * Update cpu_load for any missed ticks, due to tickless idle. The backlog\n * would be when CPU is idle and so we just decay the old load without\n * adding any new load.\n */\nstatic unsigned long\ndecay_load_missed(unsigned long load, unsigned long missed_updates, int idx)\n{\n\tint j = 0;\n\n\tif (!missed_updates)\n\t\treturn load;\n\n\tif (missed_updates >= degrade_zero_ticks[idx])\n\t\treturn 0;\n\n\tif (idx == 1)\n\t\treturn load >> missed_updates;\n\n\twhile (missed_updates) {\n\t\tif (missed_updates % 2)\n\t\t\tload = (load * degrade_factor[idx][j]) >> DEGRADE_SHIFT;\n\n\t\tmissed_updates >>= 1;\n\t\tj++;\n\t}\n\treturn load;\n}\n\n/*\n * Update rq->cpu_load[] statistics. This function is usually called every\n * scheduler tick (TICK_NSEC). With tickless idle this will not be called\n * every tick. We fix it up based on jiffies.\n */\nstatic void update_cpu_load(struct rq *this_rq)\n{\n\tunsigned long this_load = this_rq->load.weight;\n\tunsigned long curr_jiffies = jiffies;\n\tunsigned long pending_updates;\n\tint i, scale;\n\n\tthis_rq->nr_load_updates++;\n\n\t/* Avoid repeated calls on same jiffy, when moving in and out of idle */\n\tif (curr_jiffies == this_rq->last_load_update_tick)\n\t\treturn;\n\n\tpending_updates = curr_jiffies - this_rq->last_load_update_tick;\n\tthis_rq->last_load_update_tick = curr_jiffies;\n\n\t/* Update our load: */\n\tthis_rq->cpu_load[0] = this_load; /* Fasttrack for idx 0 */\n\tfor (i = 1, scale = 2; i < CPU_LOAD_IDX_MAX; i++, scale += scale) {\n\t\tunsigned long old_load, new_load;\n\n\t\t/* scale is effectively 1 << i now, and >> i divides by scale */\n\n\t\told_load = this_rq->cpu_load[i];\n\t\told_load = decay_load_missed(old_load, pending_updates - 1, i);\n\t\tnew_load = this_load;\n\t\t/*\n\t\t * Round up the averaging division if load is increasing. This\n\t\t * prevents us from getting stuck on 9 if the load is 10, for\n\t\t * example.\n\t\t */\n\t\tif (new_load > old_load)\n\t\t\tnew_load += scale - 1;\n\n\t\tthis_rq->cpu_load[i] = (old_load * (scale - 1) + new_load) >> i;\n\t}\n\n\tsched_avg_update(this_rq);\n}\n\nstatic void update_cpu_load_active(struct rq *this_rq)\n{\n\tupdate_cpu_load(this_rq);\n\n\tcalc_load_account_active(this_rq);\n}\n\n#ifdef CONFIG_SMP\n\n/*\n * sched_exec - execve() is a valuable balancing opportunity, because at\n * this point the task has the smallest effective memory and cache footprint.\n */\nvoid sched_exec(void)\n{\n\tstruct task_struct *p = current;\n\tunsigned long flags;\n\tint dest_cpu;\n\n\traw_spin_lock_irqsave(&p->pi_lock, flags);\n\tdest_cpu = p->sched_class->select_task_rq(p, SD_BALANCE_EXEC, 0);\n\tif (dest_cpu == smp_processor_id())\n\t\tgoto unlock;\n\n\tif (likely(cpu_active(dest_cpu))) {\n\t\tstruct migration_arg arg = { p, dest_cpu };\n\n\t\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\t\tstop_one_cpu(task_cpu(p), migration_cpu_stop, &arg);\n\t\treturn;\n\t}\nunlock:\n\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n}\n\n#endif\n\nDEFINE_PER_CPU(struct kernel_stat, kstat);\n\nEXPORT_PER_CPU_SYMBOL(kstat);\n\n/*\n * Return any ns on the sched_clock that have not yet been accounted in\n * @p in case that task is currently running.\n *\n * Called with task_rq_lock() held on @rq.\n */\nstatic u64 do_task_delta_exec(struct task_struct *p, struct rq *rq)\n{\n\tu64 ns = 0;\n\n\tif (task_current(rq, p)) {\n\t\tupdate_rq_clock(rq);\n\t\tns = rq->clock_task - p->se.exec_start;\n\t\tif ((s64)ns < 0)\n\t\t\tns = 0;\n\t}\n\n\treturn ns;\n}\n\nunsigned long long task_delta_exec(struct task_struct *p)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tu64 ns = 0;\n\n\trq = task_rq_lock(p, &flags);\n\tns = do_task_delta_exec(p, rq);\n\ttask_rq_unlock(rq, p, &flags);\n\n\treturn ns;\n}\n\n/*\n * Return accounted runtime for the task.\n * In case the task is currently running, return the runtime plus current's\n * pending runtime that have not been accounted yet.\n */\nunsigned long long task_sched_runtime(struct task_struct *p)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tu64 ns = 0;\n\n\trq = task_rq_lock(p, &flags);\n\tns = p->se.sum_exec_runtime + do_task_delta_exec(p, rq);\n\ttask_rq_unlock(rq, p, &flags);\n\n\treturn ns;\n}\n\n/*\n * Return sum_exec_runtime for the thread group.\n * In case the task is currently running, return the sum plus current's\n * pending runtime that have not been accounted yet.\n *\n * Note that the thread group might have other running tasks as well,\n * so the return value not includes other pending runtime that other\n * running tasks might have.\n */\nunsigned long long thread_group_sched_runtime(struct task_struct *p)\n{\n\tstruct task_cputime totals;\n\tunsigned long flags;\n\tstruct rq *rq;\n\tu64 ns;\n\n\trq = task_rq_lock(p, &flags);\n\tthread_group_cputime(p, &totals);\n\tns = totals.sum_exec_runtime + do_task_delta_exec(p, rq);\n\ttask_rq_unlock(rq, p, &flags);\n\n\treturn ns;\n}\n\n/*\n * Account user cpu time to a process.\n * @p: the process that the cpu time gets accounted to\n * @cputime: the cpu time spent in user space since the last update\n * @cputime_scaled: cputime scaled by cpu frequency\n */\nvoid account_user_time(struct task_struct *p, cputime_t cputime,\n\t\t       cputime_t cputime_scaled)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tcputime64_t tmp;\n\n\t/* Add user time to process. */\n\tp->utime = cputime_add(p->utime, cputime);\n\tp->utimescaled = cputime_add(p->utimescaled, cputime_scaled);\n\taccount_group_user_time(p, cputime);\n\n\t/* Add user time to cpustat. */\n\ttmp = cputime_to_cputime64(cputime);\n\tif (TASK_NICE(p) > 0)\n\t\tcpustat->nice = cputime64_add(cpustat->nice, tmp);\n\telse\n\t\tcpustat->user = cputime64_add(cpustat->user, tmp);\n\n\tcpuacct_update_stats(p, CPUACCT_STAT_USER, cputime);\n\t/* Account for user time used */\n\tacct_update_integrals(p);\n}\n\n/*\n * Account guest cpu time to a process.\n * @p: the process that the cpu time gets accounted to\n * @cputime: the cpu time spent in virtual machine since the last update\n * @cputime_scaled: cputime scaled by cpu frequency\n */\nstatic void account_guest_time(struct task_struct *p, cputime_t cputime,\n\t\t\t       cputime_t cputime_scaled)\n{\n\tcputime64_t tmp;\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\n\ttmp = cputime_to_cputime64(cputime);\n\n\t/* Add guest time to process. */\n\tp->utime = cputime_add(p->utime, cputime);\n\tp->utimescaled = cputime_add(p->utimescaled, cputime_scaled);\n\taccount_group_user_time(p, cputime);\n\tp->gtime = cputime_add(p->gtime, cputime);\n\n\t/* Add guest time to cpustat. */\n\tif (TASK_NICE(p) > 0) {\n\t\tcpustat->nice = cputime64_add(cpustat->nice, tmp);\n\t\tcpustat->guest_nice = cputime64_add(cpustat->guest_nice, tmp);\n\t} else {\n\t\tcpustat->user = cputime64_add(cpustat->user, tmp);\n\t\tcpustat->guest = cputime64_add(cpustat->guest, tmp);\n\t}\n}\n\n/*\n * Account system cpu time to a process and desired cpustat field\n * @p: the process that the cpu time gets accounted to\n * @cputime: the cpu time spent in kernel space since the last update\n * @cputime_scaled: cputime scaled by cpu frequency\n * @target_cputime64: pointer to cpustat field that has to be updated\n */\nstatic inline\nvoid __account_system_time(struct task_struct *p, cputime_t cputime,\n\t\t\tcputime_t cputime_scaled, cputime64_t *target_cputime64)\n{\n\tcputime64_t tmp = cputime_to_cputime64(cputime);\n\n\t/* Add system time to process. */\n\tp->stime = cputime_add(p->stime, cputime);\n\tp->stimescaled = cputime_add(p->stimescaled, cputime_scaled);\n\taccount_group_system_time(p, cputime);\n\n\t/* Add system time to cpustat. */\n\t*target_cputime64 = cputime64_add(*target_cputime64, tmp);\n\tcpuacct_update_stats(p, CPUACCT_STAT_SYSTEM, cputime);\n\n\t/* Account for system time used */\n\tacct_update_integrals(p);\n}\n\n/*\n * Account system cpu time to a process.\n * @p: the process that the cpu time gets accounted to\n * @hardirq_offset: the offset to subtract from hardirq_count()\n * @cputime: the cpu time spent in kernel space since the last update\n * @cputime_scaled: cputime scaled by cpu frequency\n */\nvoid account_system_time(struct task_struct *p, int hardirq_offset,\n\t\t\t cputime_t cputime, cputime_t cputime_scaled)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tcputime64_t *target_cputime64;\n\n\tif ((p->flags & PF_VCPU) && (irq_count() - hardirq_offset == 0)) {\n\t\taccount_guest_time(p, cputime, cputime_scaled);\n\t\treturn;\n\t}\n\n\tif (hardirq_count() - hardirq_offset)\n\t\ttarget_cputime64 = &cpustat->irq;\n\telse if (in_serving_softirq())\n\t\ttarget_cputime64 = &cpustat->softirq;\n\telse\n\t\ttarget_cputime64 = &cpustat->system;\n\n\t__account_system_time(p, cputime, cputime_scaled, target_cputime64);\n}\n\n/*\n * Account for involuntary wait time.\n * @cputime: the cpu time spent in involuntary wait\n */\nvoid account_steal_time(cputime_t cputime)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tcputime64_t cputime64 = cputime_to_cputime64(cputime);\n\n\tcpustat->steal = cputime64_add(cpustat->steal, cputime64);\n}\n\n/*\n * Account for idle time.\n * @cputime: the cpu time spent in idle wait\n */\nvoid account_idle_time(cputime_t cputime)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tcputime64_t cputime64 = cputime_to_cputime64(cputime);\n\tstruct rq *rq = this_rq();\n\n\tif (atomic_read(&rq->nr_iowait) > 0)\n\t\tcpustat->iowait = cputime64_add(cpustat->iowait, cputime64);\n\telse\n\t\tcpustat->idle = cputime64_add(cpustat->idle, cputime64);\n}\n\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n/*\n * Account a tick to a process and cpustat\n * @p: the process that the cpu time gets accounted to\n * @user_tick: is the tick from userspace\n * @rq: the pointer to rq\n *\n * Tick demultiplexing follows the order\n * - pending hardirq update\n * - pending softirq update\n * - user_time\n * - idle_time\n * - system time\n *   - check for guest_time\n *   - else account as system_time\n *\n * Check for hardirq is done both for system and user time as there is\n * no timer going off while we are on hardirq and hence we may never get an\n * opportunity to update it solely in system time.\n * p->stime and friends are only updated on system time and not on irq\n * softirq as those do not count in task exec_runtime any more.\n */\nstatic void irqtime_account_process_tick(struct task_struct *p, int user_tick,\n\t\t\t\t\t\tstruct rq *rq)\n{\n\tcputime_t one_jiffy_scaled = cputime_to_scaled(cputime_one_jiffy);\n\tcputime64_t tmp = cputime_to_cputime64(cputime_one_jiffy);\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\n\tif (irqtime_account_hi_update()) {\n\t\tcpustat->irq = cputime64_add(cpustat->irq, tmp);\n\t} else if (irqtime_account_si_update()) {\n\t\tcpustat->softirq = cputime64_add(cpustat->softirq, tmp);\n\t} else if (this_cpu_ksoftirqd() == p) {\n\t\t/*\n\t\t * ksoftirqd time do not get accounted in cpu_softirq_time.\n\t\t * So, we have to handle it separately here.\n\t\t * Also, p->stime needs to be updated for ksoftirqd.\n\t\t */\n\t\t__account_system_time(p, cputime_one_jiffy, one_jiffy_scaled,\n\t\t\t\t\t&cpustat->softirq);\n\t} else if (user_tick) {\n\t\taccount_user_time(p, cputime_one_jiffy, one_jiffy_scaled);\n\t} else if (p == rq->idle) {\n\t\taccount_idle_time(cputime_one_jiffy);\n\t} else if (p->flags & PF_VCPU) { /* System time or guest time */\n\t\taccount_guest_time(p, cputime_one_jiffy, one_jiffy_scaled);\n\t} else {\n\t\t__account_system_time(p, cputime_one_jiffy, one_jiffy_scaled,\n\t\t\t\t\t&cpustat->system);\n\t}\n}\n\nstatic void irqtime_account_idle_ticks(int ticks)\n{\n\tint i;\n\tstruct rq *rq = this_rq();\n\n\tfor (i = 0; i < ticks; i++)\n\t\tirqtime_account_process_tick(current, 0, rq);\n}\n#else /* CONFIG_IRQ_TIME_ACCOUNTING */\nstatic void irqtime_account_idle_ticks(int ticks) {}\nstatic void irqtime_account_process_tick(struct task_struct *p, int user_tick,\n\t\t\t\t\t\tstruct rq *rq) {}\n#endif /* CONFIG_IRQ_TIME_ACCOUNTING */\n\n/*\n * Account a single tick of cpu time.\n * @p: the process that the cpu time gets accounted to\n * @user_tick: indicates if the tick is a user or a system tick\n */\nvoid account_process_tick(struct task_struct *p, int user_tick)\n{\n\tcputime_t one_jiffy_scaled = cputime_to_scaled(cputime_one_jiffy);\n\tstruct rq *rq = this_rq();\n\n\tif (sched_clock_irqtime) {\n\t\tirqtime_account_process_tick(p, user_tick, rq);\n\t\treturn;\n\t}\n\n\tif (user_tick)\n\t\taccount_user_time(p, cputime_one_jiffy, one_jiffy_scaled);\n\telse if ((p != rq->idle) || (irq_count() != HARDIRQ_OFFSET))\n\t\taccount_system_time(p, HARDIRQ_OFFSET, cputime_one_jiffy,\n\t\t\t\t    one_jiffy_scaled);\n\telse\n\t\taccount_idle_time(cputime_one_jiffy);\n}\n\n/*\n * Account multiple ticks of steal time.\n * @p: the process from which the cpu time has been stolen\n * @ticks: number of stolen ticks\n */\nvoid account_steal_ticks(unsigned long ticks)\n{\n\taccount_steal_time(jiffies_to_cputime(ticks));\n}\n\n/*\n * Account multiple ticks of idle time.\n * @ticks: number of stolen ticks\n */\nvoid account_idle_ticks(unsigned long ticks)\n{\n\n\tif (sched_clock_irqtime) {\n\t\tirqtime_account_idle_ticks(ticks);\n\t\treturn;\n\t}\n\n\taccount_idle_time(jiffies_to_cputime(ticks));\n}\n\n#endif\n\n/*\n * Use precise platform statistics if available:\n */\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING\nvoid task_times(struct task_struct *p, cputime_t *ut, cputime_t *st)\n{\n\t*ut = p->utime;\n\t*st = p->stime;\n}\n\nvoid thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *st)\n{\n\tstruct task_cputime cputime;\n\n\tthread_group_cputime(p, &cputime);\n\n\t*ut = cputime.utime;\n\t*st = cputime.stime;\n}\n#else\n\n#ifndef nsecs_to_cputime\n# define nsecs_to_cputime(__nsecs)\tnsecs_to_jiffies(__nsecs)\n#endif\n\nvoid task_times(struct task_struct *p, cputime_t *ut, cputime_t *st)\n{\n\tcputime_t rtime, utime = p->utime, total = cputime_add(utime, p->stime);\n\n\t/*\n\t * Use CFS's precise accounting:\n\t */\n\trtime = nsecs_to_cputime(p->se.sum_exec_runtime);\n\n\tif (total) {\n\t\tu64 temp = rtime;\n\n\t\ttemp *= utime;\n\t\tdo_div(temp, total);\n\t\tutime = (cputime_t)temp;\n\t} else\n\t\tutime = rtime;\n\n\t/*\n\t * Compare with previous values, to keep monotonicity:\n\t */\n\tp->prev_utime = max(p->prev_utime, utime);\n\tp->prev_stime = max(p->prev_stime, cputime_sub(rtime, p->prev_utime));\n\n\t*ut = p->prev_utime;\n\t*st = p->prev_stime;\n}\n\n/*\n * Must be called with siglock held.\n */\nvoid thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *st)\n{\n\tstruct signal_struct *sig = p->signal;\n\tstruct task_cputime cputime;\n\tcputime_t rtime, utime, total;\n\n\tthread_group_cputime(p, &cputime);\n\n\ttotal = cputime_add(cputime.utime, cputime.stime);\n\trtime = nsecs_to_cputime(cputime.sum_exec_runtime);\n\n\tif (total) {\n\t\tu64 temp = rtime;\n\n\t\ttemp *= cputime.utime;\n\t\tdo_div(temp, total);\n\t\tutime = (cputime_t)temp;\n\t} else\n\t\tutime = rtime;\n\n\tsig->prev_utime = max(sig->prev_utime, utime);\n\tsig->prev_stime = max(sig->prev_stime,\n\t\t\t      cputime_sub(rtime, sig->prev_utime));\n\n\t*ut = sig->prev_utime;\n\t*st = sig->prev_stime;\n}\n#endif\n\n/*\n * This function gets called by the timer code, with HZ frequency.\n * We call it with interrupts disabled.\n */\nvoid scheduler_tick(void)\n{\n\tint cpu = smp_processor_id();\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct task_struct *curr = rq->curr;\n\n\tsched_clock_tick();\n\n\traw_spin_lock(&rq->lock);\n\tupdate_rq_clock(rq);\n\tupdate_cpu_load_active(rq);\n\tcurr->sched_class->task_tick(rq, curr, 0);\n\traw_spin_unlock(&rq->lock);\n\n\tperf_event_task_tick();\n\n#ifdef CONFIG_SMP\n\trq->idle_at_tick = idle_cpu(cpu);\n\ttrigger_load_balance(rq, cpu);\n#endif\n}\n\nnotrace unsigned long get_parent_ip(unsigned long addr)\n{\n\tif (in_lock_functions(addr)) {\n\t\taddr = CALLER_ADDR2;\n\t\tif (in_lock_functions(addr))\n\t\t\taddr = CALLER_ADDR3;\n\t}\n\treturn addr;\n}\n\n#if defined(CONFIG_PREEMPT) && (defined(CONFIG_DEBUG_PREEMPT) || \\\n\t\t\t\tdefined(CONFIG_PREEMPT_TRACER))\n\nvoid __kprobes add_preempt_count(int val)\n{\n#ifdef CONFIG_DEBUG_PREEMPT\n\t/*\n\t * Underflow?\n\t */\n\tif (DEBUG_LOCKS_WARN_ON((preempt_count() < 0)))\n\t\treturn;\n#endif\n\tpreempt_count() += val;\n#ifdef CONFIG_DEBUG_PREEMPT\n\t/*\n\t * Spinlock count overflowing soon?\n\t */\n\tDEBUG_LOCKS_WARN_ON((preempt_count() & PREEMPT_MASK) >=\n\t\t\t\tPREEMPT_MASK - 10);\n#endif\n\tif (preempt_count() == val)\n\t\ttrace_preempt_off(CALLER_ADDR0, get_parent_ip(CALLER_ADDR1));\n}\nEXPORT_SYMBOL(add_preempt_count);\n\nvoid __kprobes sub_preempt_count(int val)\n{\n#ifdef CONFIG_DEBUG_PREEMPT\n\t/*\n\t * Underflow?\n\t */\n\tif (DEBUG_LOCKS_WARN_ON(val > preempt_count()))\n\t\treturn;\n\t/*\n\t * Is the spinlock portion underflowing?\n\t */\n\tif (DEBUG_LOCKS_WARN_ON((val < PREEMPT_MASK) &&\n\t\t\t!(preempt_count() & PREEMPT_MASK)))\n\t\treturn;\n#endif\n\n\tif (preempt_count() == val)\n\t\ttrace_preempt_on(CALLER_ADDR0, get_parent_ip(CALLER_ADDR1));\n\tpreempt_count() -= val;\n}\nEXPORT_SYMBOL(sub_preempt_count);\n\n#endif\n\n/*\n * Print scheduling while atomic bug:\n */\nstatic noinline void __schedule_bug(struct task_struct *prev)\n{\n\tstruct pt_regs *regs = get_irq_regs();\n\n\tprintk(KERN_ERR \"BUG: scheduling while atomic: %s/%d/0x%08x\\n\",\n\t\tprev->comm, prev->pid, preempt_count());\n\n\tdebug_show_held_locks(prev);\n\tprint_modules();\n\tif (irqs_disabled())\n\t\tprint_irqtrace_events(prev);\n\n\tif (regs)\n\t\tshow_regs(regs);\n\telse\n\t\tdump_stack();\n}\n\n/*\n * Various schedule()-time debugging checks and statistics:\n */\nstatic inline void schedule_debug(struct task_struct *prev)\n{\n\t/*\n\t * Test if we are atomic. Since do_exit() needs to call into\n\t * schedule() atomically, we ignore that path for now.\n\t * Otherwise, whine if we are scheduling when we should not be.\n\t */\n\tif (unlikely(in_atomic_preempt_off() && !prev->exit_state))\n\t\t__schedule_bug(prev);\n\n\tprofile_hit(SCHED_PROFILING, __builtin_return_address(0));\n\n\tschedstat_inc(this_rq(), sched_count);\n}\n\nstatic void put_prev_task(struct rq *rq, struct task_struct *prev)\n{\n\tif (prev->on_rq || rq->skip_clock_update < 0)\n\t\tupdate_rq_clock(rq);\n\tprev->sched_class->put_prev_task(rq, prev);\n}\n\n/*\n * Pick up the highest-prio task:\n */\nstatic inline struct task_struct *\npick_next_task(struct rq *rq)\n{\n\tconst struct sched_class *class;\n\tstruct task_struct *p;\n\n\t/*\n\t * Optimization: we know that if all tasks are in\n\t * the fair class we can call that function directly:\n\t */\n\tif (likely(rq->nr_running == rq->cfs.nr_running)) {\n\t\tp = fair_sched_class.pick_next_task(rq);\n\t\tif (likely(p))\n\t\t\treturn p;\n\t}\n\n\tfor_each_class(class) {\n\t\tp = class->pick_next_task(rq);\n\t\tif (p)\n\t\t\treturn p;\n\t}\n\n\tBUG(); /* the idle class will always have a runnable task */\n}\n\n/*\n * schedule() is the main scheduler function.\n */\nasmlinkage void __sched schedule(void)\n{\n\tstruct task_struct *prev, *next;\n\tunsigned long *switch_count;\n\tstruct rq *rq;\n\tint cpu;\n\nneed_resched:\n\tpreempt_disable();\n\tcpu = smp_processor_id();\n\trq = cpu_rq(cpu);\n\trcu_note_context_switch(cpu);\n\tprev = rq->curr;\n\n\tschedule_debug(prev);\n\n\tif (sched_feat(HRTICK))\n\t\thrtick_clear(rq);\n\n\traw_spin_lock_irq(&rq->lock);\n\n\tswitch_count = &prev->nivcsw;\n\tif (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) {\n\t\tif (unlikely(signal_pending_state(prev->state, prev))) {\n\t\t\tprev->state = TASK_RUNNING;\n\t\t} else {\n\t\t\tdeactivate_task(rq, prev, DEQUEUE_SLEEP);\n\t\t\tprev->on_rq = 0;\n\n\t\t\t/*\n\t\t\t * If a worker went to sleep, notify and ask workqueue\n\t\t\t * whether it wants to wake up a task to maintain\n\t\t\t * concurrency.\n\t\t\t */\n\t\t\tif (prev->flags & PF_WQ_WORKER) {\n\t\t\t\tstruct task_struct *to_wakeup;\n\n\t\t\t\tto_wakeup = wq_worker_sleeping(prev, cpu);\n\t\t\t\tif (to_wakeup)\n\t\t\t\t\ttry_to_wake_up_local(to_wakeup);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * If we are going to sleep and we have plugged IO\n\t\t\t * queued, make sure to submit it to avoid deadlocks.\n\t\t\t */\n\t\t\tif (blk_needs_flush_plug(prev)) {\n\t\t\t\traw_spin_unlock(&rq->lock);\n\t\t\t\tblk_schedule_flush_plug(prev);\n\t\t\t\traw_spin_lock(&rq->lock);\n\t\t\t}\n\t\t}\n\t\tswitch_count = &prev->nvcsw;\n\t}\n\n\tpre_schedule(rq, prev);\n\n\tif (unlikely(!rq->nr_running))\n\t\tidle_balance(cpu, rq);\n\n\tput_prev_task(rq, prev);\n\tnext = pick_next_task(rq);\n\tclear_tsk_need_resched(prev);\n\trq->skip_clock_update = 0;\n\n\tif (likely(prev != next)) {\n\t\trq->nr_switches++;\n\t\trq->curr = next;\n\t\t++*switch_count;\n\n\t\tcontext_switch(rq, prev, next); /* unlocks the rq */\n\t\t/*\n\t\t * The context switch have flipped the stack from under us\n\t\t * and restored the local variables which were saved when\n\t\t * this task called schedule() in the past. prev == current\n\t\t * is still correct, but it can be moved to another cpu/rq.\n\t\t */\n\t\tcpu = smp_processor_id();\n\t\trq = cpu_rq(cpu);\n\t} else\n\t\traw_spin_unlock_irq(&rq->lock);\n\n\tpost_schedule(rq);\n\n\tpreempt_enable_no_resched();\n\tif (need_resched())\n\t\tgoto need_resched;\n}\nEXPORT_SYMBOL(schedule);\n\n#ifdef CONFIG_MUTEX_SPIN_ON_OWNER\n\nstatic inline bool owner_running(struct mutex *lock, struct task_struct *owner)\n{\n\tbool ret = false;\n\n\trcu_read_lock();\n\tif (lock->owner != owner)\n\t\tgoto fail;\n\n\t/*\n\t * Ensure we emit the owner->on_cpu, dereference _after_ checking\n\t * lock->owner still matches owner, if that fails, owner might\n\t * point to free()d memory, if it still matches, the rcu_read_lock()\n\t * ensures the memory stays valid.\n\t */\n\tbarrier();\n\n\tret = owner->on_cpu;\nfail:\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n/*\n * Look out! \"owner\" is an entirely speculative pointer\n * access and not reliable.\n */\nint mutex_spin_on_owner(struct mutex *lock, struct task_struct *owner)\n{\n\tif (!sched_feat(OWNER_SPIN))\n\t\treturn 0;\n\n\twhile (owner_running(lock, owner)) {\n\t\tif (need_resched())\n\t\t\treturn 0;\n\n\t\tarch_mutex_cpu_relax();\n\t}\n\n\t/*\n\t * If the owner changed to another task there is likely\n\t * heavy contention, stop spinning.\n\t */\n\tif (lock->owner)\n\t\treturn 0;\n\n\treturn 1;\n}\n#endif\n\n#ifdef CONFIG_PREEMPT\n/*\n * this is the entry point to schedule() from in-kernel preemption\n * off of preempt_enable. Kernel preemptions off return from interrupt\n * occur there and call schedule directly.\n */\nasmlinkage void __sched notrace preempt_schedule(void)\n{\n\tstruct thread_info *ti = current_thread_info();\n\n\t/*\n\t * If there is a non-zero preempt_count or interrupts are disabled,\n\t * we do not want to preempt the current task. Just return..\n\t */\n\tif (likely(ti->preempt_count || irqs_disabled()))\n\t\treturn;\n\n\tdo {\n\t\tadd_preempt_count_notrace(PREEMPT_ACTIVE);\n\t\tschedule();\n\t\tsub_preempt_count_notrace(PREEMPT_ACTIVE);\n\n\t\t/*\n\t\t * Check again in case we missed a preemption opportunity\n\t\t * between schedule and now.\n\t\t */\n\t\tbarrier();\n\t} while (need_resched());\n}\nEXPORT_SYMBOL(preempt_schedule);\n\n/*\n * this is the entry point to schedule() from kernel preemption\n * off of irq context.\n * Note, that this is called and return with irqs disabled. This will\n * protect us against recursive calling from irq.\n */\nasmlinkage void __sched preempt_schedule_irq(void)\n{\n\tstruct thread_info *ti = current_thread_info();\n\n\t/* Catch callers which need to be fixed */\n\tBUG_ON(ti->preempt_count || !irqs_disabled());\n\n\tdo {\n\t\tadd_preempt_count(PREEMPT_ACTIVE);\n\t\tlocal_irq_enable();\n\t\tschedule();\n\t\tlocal_irq_disable();\n\t\tsub_preempt_count(PREEMPT_ACTIVE);\n\n\t\t/*\n\t\t * Check again in case we missed a preemption opportunity\n\t\t * between schedule and now.\n\t\t */\n\t\tbarrier();\n\t} while (need_resched());\n}\n\n#endif /* CONFIG_PREEMPT */\n\nint default_wake_function(wait_queue_t *curr, unsigned mode, int wake_flags,\n\t\t\t  void *key)\n{\n\treturn try_to_wake_up(curr->private, mode, wake_flags);\n}\nEXPORT_SYMBOL(default_wake_function);\n\n/*\n * The core wakeup function. Non-exclusive wakeups (nr_exclusive == 0) just\n * wake everything up. If it's an exclusive wakeup (nr_exclusive == small +ve\n * number) then we wake all the non-exclusive tasks and one exclusive task.\n *\n * There are circumstances in which we can try to wake a task which has already\n * started to run but is not in state TASK_RUNNING. try_to_wake_up() returns\n * zero in this (rare) case, and we handle it by continuing to scan the queue.\n */\nstatic void __wake_up_common(wait_queue_head_t *q, unsigned int mode,\n\t\t\tint nr_exclusive, int wake_flags, void *key)\n{\n\twait_queue_t *curr, *next;\n\n\tlist_for_each_entry_safe(curr, next, &q->task_list, task_list) {\n\t\tunsigned flags = curr->flags;\n\n\t\tif (curr->func(curr, mode, wake_flags, key) &&\n\t\t\t\t(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)\n\t\t\tbreak;\n\t}\n}\n\n/**\n * __wake_up - wake up threads blocked on a waitqueue.\n * @q: the waitqueue\n * @mode: which threads\n * @nr_exclusive: how many wake-one or wake-many threads to wake up\n * @key: is directly passed to the wakeup function\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nvoid __wake_up(wait_queue_head_t *q, unsigned int mode,\n\t\t\tint nr_exclusive, void *key)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&q->lock, flags);\n\t__wake_up_common(q, mode, nr_exclusive, 0, key);\n\tspin_unlock_irqrestore(&q->lock, flags);\n}\nEXPORT_SYMBOL(__wake_up);\n\n/*\n * Same as __wake_up but called with the spinlock in wait_queue_head_t held.\n */\nvoid __wake_up_locked(wait_queue_head_t *q, unsigned int mode)\n{\n\t__wake_up_common(q, mode, 1, 0, NULL);\n}\nEXPORT_SYMBOL_GPL(__wake_up_locked);\n\nvoid __wake_up_locked_key(wait_queue_head_t *q, unsigned int mode, void *key)\n{\n\t__wake_up_common(q, mode, 1, 0, key);\n}\nEXPORT_SYMBOL_GPL(__wake_up_locked_key);\n\n/**\n * __wake_up_sync_key - wake up threads blocked on a waitqueue.\n * @q: the waitqueue\n * @mode: which threads\n * @nr_exclusive: how many wake-one or wake-many threads to wake up\n * @key: opaque value to be passed to wakeup targets\n *\n * The sync wakeup differs that the waker knows that it will schedule\n * away soon, so while the target thread will be woken up, it will not\n * be migrated to another CPU - ie. the two threads are 'synchronized'\n * with each other. This can prevent needless bouncing between CPUs.\n *\n * On UP it can prevent extra preemption.\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nvoid __wake_up_sync_key(wait_queue_head_t *q, unsigned int mode,\n\t\t\tint nr_exclusive, void *key)\n{\n\tunsigned long flags;\n\tint wake_flags = WF_SYNC;\n\n\tif (unlikely(!q))\n\t\treturn;\n\n\tif (unlikely(!nr_exclusive))\n\t\twake_flags = 0;\n\n\tspin_lock_irqsave(&q->lock, flags);\n\t__wake_up_common(q, mode, nr_exclusive, wake_flags, key);\n\tspin_unlock_irqrestore(&q->lock, flags);\n}\nEXPORT_SYMBOL_GPL(__wake_up_sync_key);\n\n/*\n * __wake_up_sync - see __wake_up_sync_key()\n */\nvoid __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)\n{\n\t__wake_up_sync_key(q, mode, nr_exclusive, NULL);\n}\nEXPORT_SYMBOL_GPL(__wake_up_sync);\t/* For internal use only */\n\n/**\n * complete: - signals a single thread waiting on this completion\n * @x:  holds the state of this particular completion\n *\n * This will wake up a single thread waiting on this completion. Threads will be\n * awakened in the same order in which they were queued.\n *\n * See also complete_all(), wait_for_completion() and related routines.\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nvoid complete(struct completion *x)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&x->wait.lock, flags);\n\tx->done++;\n\t__wake_up_common(&x->wait, TASK_NORMAL, 1, 0, NULL);\n\tspin_unlock_irqrestore(&x->wait.lock, flags);\n}\nEXPORT_SYMBOL(complete);\n\n/**\n * complete_all: - signals all threads waiting on this completion\n * @x:  holds the state of this particular completion\n *\n * This will wake up all threads waiting on this particular completion event.\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nvoid complete_all(struct completion *x)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&x->wait.lock, flags);\n\tx->done += UINT_MAX/2;\n\t__wake_up_common(&x->wait, TASK_NORMAL, 0, 0, NULL);\n\tspin_unlock_irqrestore(&x->wait.lock, flags);\n}\nEXPORT_SYMBOL(complete_all);\n\nstatic inline long __sched\ndo_wait_for_common(struct completion *x, long timeout, int state)\n{\n\tif (!x->done) {\n\t\tDECLARE_WAITQUEUE(wait, current);\n\n\t\t__add_wait_queue_tail_exclusive(&x->wait, &wait);\n\t\tdo {\n\t\t\tif (signal_pending_state(state, current)) {\n\t\t\t\ttimeout = -ERESTARTSYS;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t__set_current_state(state);\n\t\t\tspin_unlock_irq(&x->wait.lock);\n\t\t\ttimeout = schedule_timeout(timeout);\n\t\t\tspin_lock_irq(&x->wait.lock);\n\t\t} while (!x->done && timeout);\n\t\t__remove_wait_queue(&x->wait, &wait);\n\t\tif (!x->done)\n\t\t\treturn timeout;\n\t}\n\tx->done--;\n\treturn timeout ?: 1;\n}\n\nstatic long __sched\nwait_for_common(struct completion *x, long timeout, int state)\n{\n\tmight_sleep();\n\n\tspin_lock_irq(&x->wait.lock);\n\ttimeout = do_wait_for_common(x, timeout, state);\n\tspin_unlock_irq(&x->wait.lock);\n\treturn timeout;\n}\n\n/**\n * wait_for_completion: - waits for completion of a task\n * @x:  holds the state of this particular completion\n *\n * This waits to be signaled for completion of a specific task. It is NOT\n * interruptible and there is no timeout.\n *\n * See also similar routines (i.e. wait_for_completion_timeout()) with timeout\n * and interrupt capability. Also see complete().\n */\nvoid __sched wait_for_completion(struct completion *x)\n{\n\twait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE);\n}\nEXPORT_SYMBOL(wait_for_completion);\n\n/**\n * wait_for_completion_timeout: - waits for completion of a task (w/timeout)\n * @x:  holds the state of this particular completion\n * @timeout:  timeout value in jiffies\n *\n * This waits for either a completion of a specific task to be signaled or for a\n * specified timeout to expire. The timeout is in jiffies. It is not\n * interruptible.\n */\nunsigned long __sched\nwait_for_completion_timeout(struct completion *x, unsigned long timeout)\n{\n\treturn wait_for_common(x, timeout, TASK_UNINTERRUPTIBLE);\n}\nEXPORT_SYMBOL(wait_for_completion_timeout);\n\n/**\n * wait_for_completion_interruptible: - waits for completion of a task (w/intr)\n * @x:  holds the state of this particular completion\n *\n * This waits for completion of a specific task to be signaled. It is\n * interruptible.\n */\nint __sched wait_for_completion_interruptible(struct completion *x)\n{\n\tlong t = wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_INTERRUPTIBLE);\n\tif (t == -ERESTARTSYS)\n\t\treturn t;\n\treturn 0;\n}\nEXPORT_SYMBOL(wait_for_completion_interruptible);\n\n/**\n * wait_for_completion_interruptible_timeout: - waits for completion (w/(to,intr))\n * @x:  holds the state of this particular completion\n * @timeout:  timeout value in jiffies\n *\n * This waits for either a completion of a specific task to be signaled or for a\n * specified timeout to expire. It is interruptible. The timeout is in jiffies.\n */\nlong __sched\nwait_for_completion_interruptible_timeout(struct completion *x,\n\t\t\t\t\t  unsigned long timeout)\n{\n\treturn wait_for_common(x, timeout, TASK_INTERRUPTIBLE);\n}\nEXPORT_SYMBOL(wait_for_completion_interruptible_timeout);\n\n/**\n * wait_for_completion_killable: - waits for completion of a task (killable)\n * @x:  holds the state of this particular completion\n *\n * This waits to be signaled for completion of a specific task. It can be\n * interrupted by a kill signal.\n */\nint __sched wait_for_completion_killable(struct completion *x)\n{\n\tlong t = wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_KILLABLE);\n\tif (t == -ERESTARTSYS)\n\t\treturn t;\n\treturn 0;\n}\nEXPORT_SYMBOL(wait_for_completion_killable);\n\n/**\n * wait_for_completion_killable_timeout: - waits for completion of a task (w/(to,killable))\n * @x:  holds the state of this particular completion\n * @timeout:  timeout value in jiffies\n *\n * This waits for either a completion of a specific task to be\n * signaled or for a specified timeout to expire. It can be\n * interrupted by a kill signal. The timeout is in jiffies.\n */\nlong __sched\nwait_for_completion_killable_timeout(struct completion *x,\n\t\t\t\t     unsigned long timeout)\n{\n\treturn wait_for_common(x, timeout, TASK_KILLABLE);\n}\nEXPORT_SYMBOL(wait_for_completion_killable_timeout);\n\n/**\n *\ttry_wait_for_completion - try to decrement a completion without blocking\n *\t@x:\tcompletion structure\n *\n *\tReturns: 0 if a decrement cannot be done without blocking\n *\t\t 1 if a decrement succeeded.\n *\n *\tIf a completion is being used as a counting completion,\n *\tattempt to decrement the counter without blocking. This\n *\tenables us to avoid waiting if the resource the completion\n *\tis protecting is not available.\n */\nbool try_wait_for_completion(struct completion *x)\n{\n\tunsigned long flags;\n\tint ret = 1;\n\n\tspin_lock_irqsave(&x->wait.lock, flags);\n\tif (!x->done)\n\t\tret = 0;\n\telse\n\t\tx->done--;\n\tspin_unlock_irqrestore(&x->wait.lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(try_wait_for_completion);\n\n/**\n *\tcompletion_done - Test to see if a completion has any waiters\n *\t@x:\tcompletion structure\n *\n *\tReturns: 0 if there are waiters (wait_for_completion() in progress)\n *\t\t 1 if there are no waiters.\n *\n */\nbool completion_done(struct completion *x)\n{\n\tunsigned long flags;\n\tint ret = 1;\n\n\tspin_lock_irqsave(&x->wait.lock, flags);\n\tif (!x->done)\n\t\tret = 0;\n\tspin_unlock_irqrestore(&x->wait.lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(completion_done);\n\nstatic long __sched\nsleep_on_common(wait_queue_head_t *q, int state, long timeout)\n{\n\tunsigned long flags;\n\twait_queue_t wait;\n\n\tinit_waitqueue_entry(&wait, current);\n\n\t__set_current_state(state);\n\n\tspin_lock_irqsave(&q->lock, flags);\n\t__add_wait_queue(q, &wait);\n\tspin_unlock(&q->lock);\n\ttimeout = schedule_timeout(timeout);\n\tspin_lock_irq(&q->lock);\n\t__remove_wait_queue(q, &wait);\n\tspin_unlock_irqrestore(&q->lock, flags);\n\n\treturn timeout;\n}\n\nvoid __sched interruptible_sleep_on(wait_queue_head_t *q)\n{\n\tsleep_on_common(q, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);\n}\nEXPORT_SYMBOL(interruptible_sleep_on);\n\nlong __sched\ninterruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)\n{\n\treturn sleep_on_common(q, TASK_INTERRUPTIBLE, timeout);\n}\nEXPORT_SYMBOL(interruptible_sleep_on_timeout);\n\nvoid __sched sleep_on(wait_queue_head_t *q)\n{\n\tsleep_on_common(q, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);\n}\nEXPORT_SYMBOL(sleep_on);\n\nlong __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)\n{\n\treturn sleep_on_common(q, TASK_UNINTERRUPTIBLE, timeout);\n}\nEXPORT_SYMBOL(sleep_on_timeout);\n\n#ifdef CONFIG_RT_MUTEXES\n\n/*\n * rt_mutex_setprio - set the current priority of a task\n * @p: task\n * @prio: prio value (kernel-internal form)\n *\n * This function changes the 'effective' priority of a task. It does\n * not touch ->normal_prio like __setscheduler().\n *\n * Used by the rt_mutex code to implement priority inheritance logic.\n */\nvoid rt_mutex_setprio(struct task_struct *p, int prio)\n{\n\tint oldprio, on_rq, running;\n\tstruct rq *rq;\n\tconst struct sched_class *prev_class;\n\n\tBUG_ON(prio < 0 || prio > MAX_PRIO);\n\n\trq = __task_rq_lock(p);\n\n\ttrace_sched_pi_setprio(p, prio);\n\toldprio = p->prio;\n\tprev_class = p->sched_class;\n\ton_rq = p->on_rq;\n\trunning = task_current(rq, p);\n\tif (on_rq)\n\t\tdequeue_task(rq, p, 0);\n\tif (running)\n\t\tp->sched_class->put_prev_task(rq, p);\n\n\tif (rt_prio(prio))\n\t\tp->sched_class = &rt_sched_class;\n\telse\n\t\tp->sched_class = &fair_sched_class;\n\n\tp->prio = prio;\n\n\tif (running)\n\t\tp->sched_class->set_curr_task(rq);\n\tif (on_rq)\n\t\tenqueue_task(rq, p, oldprio < prio ? ENQUEUE_HEAD : 0);\n\n\tcheck_class_changed(rq, p, prev_class, oldprio);\n\t__task_rq_unlock(rq);\n}\n\n#endif\n\nvoid set_user_nice(struct task_struct *p, long nice)\n{\n\tint old_prio, delta, on_rq;\n\tunsigned long flags;\n\tstruct rq *rq;\n\n\tif (TASK_NICE(p) == nice || nice < -20 || nice > 19)\n\t\treturn;\n\t/*\n\t * We have to be careful, if called from sys_setpriority(),\n\t * the task might be in the middle of scheduling on another CPU.\n\t */\n\trq = task_rq_lock(p, &flags);\n\t/*\n\t * The RT priorities are set via sched_setscheduler(), but we still\n\t * allow the 'normal' nice value to be set - but as expected\n\t * it wont have any effect on scheduling until the task is\n\t * SCHED_FIFO/SCHED_RR:\n\t */\n\tif (task_has_rt_policy(p)) {\n\t\tp->static_prio = NICE_TO_PRIO(nice);\n\t\tgoto out_unlock;\n\t}\n\ton_rq = p->on_rq;\n\tif (on_rq)\n\t\tdequeue_task(rq, p, 0);\n\n\tp->static_prio = NICE_TO_PRIO(nice);\n\tset_load_weight(p);\n\told_prio = p->prio;\n\tp->prio = effective_prio(p);\n\tdelta = p->prio - old_prio;\n\n\tif (on_rq) {\n\t\tenqueue_task(rq, p, 0);\n\t\t/*\n\t\t * If the task increased its priority or is running and\n\t\t * lowered its priority, then reschedule its CPU:\n\t\t */\n\t\tif (delta < 0 || (delta > 0 && task_running(rq, p)))\n\t\t\tresched_task(rq->curr);\n\t}\nout_unlock:\n\ttask_rq_unlock(rq, p, &flags);\n}\nEXPORT_SYMBOL(set_user_nice);\n\n/*\n * can_nice - check if a task can reduce its nice value\n * @p: task\n * @nice: nice value\n */\nint can_nice(const struct task_struct *p, const int nice)\n{\n\t/* convert nice value [19,-20] to rlimit style value [1,40] */\n\tint nice_rlim = 20 - nice;\n\n\treturn (nice_rlim <= task_rlimit(p, RLIMIT_NICE) ||\n\t\tcapable(CAP_SYS_NICE));\n}\n\n#ifdef __ARCH_WANT_SYS_NICE\n\n/*\n * sys_nice - change the priority of the current process.\n * @increment: priority increment\n *\n * sys_setpriority is a more generic, but much slower function that\n * does similar things.\n */\nSYSCALL_DEFINE1(nice, int, increment)\n{\n\tlong nice, retval;\n\n\t/*\n\t * Setpriority might change our priority at the same moment.\n\t * We don't have to worry. Conceptually one call occurs first\n\t * and we have a single winner.\n\t */\n\tif (increment < -40)\n\t\tincrement = -40;\n\tif (increment > 40)\n\t\tincrement = 40;\n\n\tnice = TASK_NICE(current) + increment;\n\tif (nice < -20)\n\t\tnice = -20;\n\tif (nice > 19)\n\t\tnice = 19;\n\n\tif (increment < 0 && !can_nice(current, nice))\n\t\treturn -EPERM;\n\n\tretval = security_task_setnice(current, nice);\n\tif (retval)\n\t\treturn retval;\n\n\tset_user_nice(current, nice);\n\treturn 0;\n}\n\n#endif\n\n/**\n * task_prio - return the priority value of a given task.\n * @p: the task in question.\n *\n * This is the priority value as seen by users in /proc.\n * RT tasks are offset by -200. Normal tasks are centered\n * around 0, value goes from -16 to +15.\n */\nint task_prio(const struct task_struct *p)\n{\n\treturn p->prio - MAX_RT_PRIO;\n}\n\n/**\n * task_nice - return the nice value of a given task.\n * @p: the task in question.\n */\nint task_nice(const struct task_struct *p)\n{\n\treturn TASK_NICE(p);\n}\nEXPORT_SYMBOL(task_nice);\n\n/**\n * idle_cpu - is a given cpu idle currently?\n * @cpu: the processor in question.\n */\nint idle_cpu(int cpu)\n{\n\treturn cpu_curr(cpu) == cpu_rq(cpu)->idle;\n}\n\n/**\n * idle_task - return the idle task for a given cpu.\n * @cpu: the processor in question.\n */\nstruct task_struct *idle_task(int cpu)\n{\n\treturn cpu_rq(cpu)->idle;\n}\n\n/**\n * find_process_by_pid - find a process with a matching PID value.\n * @pid: the pid in question.\n */\nstatic struct task_struct *find_process_by_pid(pid_t pid)\n{\n\treturn pid ? find_task_by_vpid(pid) : current;\n}\n\n/* Actually do priority change: must hold rq lock. */\nstatic void\n__setscheduler(struct rq *rq, struct task_struct *p, int policy, int prio)\n{\n\tp->policy = policy;\n\tp->rt_priority = prio;\n\tp->normal_prio = normal_prio(p);\n\t/* we are holding p->pi_lock already */\n\tp->prio = rt_mutex_getprio(p);\n\tif (rt_prio(p->prio))\n\t\tp->sched_class = &rt_sched_class;\n\telse\n\t\tp->sched_class = &fair_sched_class;\n\tset_load_weight(p);\n}\n\n/*\n * check the target process has a UID that matches the current process's\n */\nstatic bool check_same_owner(struct task_struct *p)\n{\n\tconst struct cred *cred = current_cred(), *pcred;\n\tbool match;\n\n\trcu_read_lock();\n\tpcred = __task_cred(p);\n\tif (cred->user->user_ns == pcred->user->user_ns)\n\t\tmatch = (cred->euid == pcred->euid ||\n\t\t\t cred->euid == pcred->uid);\n\telse\n\t\tmatch = false;\n\trcu_read_unlock();\n\treturn match;\n}\n\nstatic int __sched_setscheduler(struct task_struct *p, int policy,\n\t\t\t\tconst struct sched_param *param, bool user)\n{\n\tint retval, oldprio, oldpolicy = -1, on_rq, running;\n\tunsigned long flags;\n\tconst struct sched_class *prev_class;\n\tstruct rq *rq;\n\tint reset_on_fork;\n\n\t/* may grab non-irq protected spin_locks */\n\tBUG_ON(in_interrupt());\nrecheck:\n\t/* double check policy once rq lock held */\n\tif (policy < 0) {\n\t\treset_on_fork = p->sched_reset_on_fork;\n\t\tpolicy = oldpolicy = p->policy;\n\t} else {\n\t\treset_on_fork = !!(policy & SCHED_RESET_ON_FORK);\n\t\tpolicy &= ~SCHED_RESET_ON_FORK;\n\n\t\tif (policy != SCHED_FIFO && policy != SCHED_RR &&\n\t\t\t\tpolicy != SCHED_NORMAL && policy != SCHED_BATCH &&\n\t\t\t\tpolicy != SCHED_IDLE)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Valid priorities for SCHED_FIFO and SCHED_RR are\n\t * 1..MAX_USER_RT_PRIO-1, valid priority for SCHED_NORMAL,\n\t * SCHED_BATCH and SCHED_IDLE is 0.\n\t */\n\tif (param->sched_priority < 0 ||\n\t    (p->mm && param->sched_priority > MAX_USER_RT_PRIO-1) ||\n\t    (!p->mm && param->sched_priority > MAX_RT_PRIO-1))\n\t\treturn -EINVAL;\n\tif (rt_policy(policy) != (param->sched_priority != 0))\n\t\treturn -EINVAL;\n\n\t/*\n\t * Allow unprivileged RT tasks to decrease priority:\n\t */\n\tif (user && !capable(CAP_SYS_NICE)) {\n\t\tif (rt_policy(policy)) {\n\t\t\tunsigned long rlim_rtprio =\n\t\t\t\t\ttask_rlimit(p, RLIMIT_RTPRIO);\n\n\t\t\t/* can't set/change the rt policy */\n\t\t\tif (policy != p->policy && !rlim_rtprio)\n\t\t\t\treturn -EPERM;\n\n\t\t\t/* can't increase priority */\n\t\t\tif (param->sched_priority > p->rt_priority &&\n\t\t\t    param->sched_priority > rlim_rtprio)\n\t\t\t\treturn -EPERM;\n\t\t}\n\n\t\t/*\n\t\t * Treat SCHED_IDLE as nice 20. Only allow a switch to\n\t\t * SCHED_NORMAL if the RLIMIT_NICE would normally permit it.\n\t\t */\n\t\tif (p->policy == SCHED_IDLE && policy != SCHED_IDLE) {\n\t\t\tif (!can_nice(p, TASK_NICE(p)))\n\t\t\t\treturn -EPERM;\n\t\t}\n\n\t\t/* can't change other user's priorities */\n\t\tif (!check_same_owner(p))\n\t\t\treturn -EPERM;\n\n\t\t/* Normal users shall not reset the sched_reset_on_fork flag */\n\t\tif (p->sched_reset_on_fork && !reset_on_fork)\n\t\t\treturn -EPERM;\n\t}\n\n\tif (user) {\n\t\tretval = security_task_setscheduler(p);\n\t\tif (retval)\n\t\t\treturn retval;\n\t}\n\n\t/*\n\t * make sure no PI-waiters arrive (or leave) while we are\n\t * changing the priority of the task:\n\t *\n\t * To be able to change p->policy safely, the appropriate\n\t * runqueue lock must be held.\n\t */\n\trq = task_rq_lock(p, &flags);\n\n\t/*\n\t * Changing the policy of the stop threads its a very bad idea\n\t */\n\tif (p == rq->stop) {\n\t\ttask_rq_unlock(rq, p, &flags);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * If not changing anything there's no need to proceed further:\n\t */\n\tif (unlikely(policy == p->policy && (!rt_policy(policy) ||\n\t\t\tparam->sched_priority == p->rt_priority))) {\n\n\t\t__task_rq_unlock(rq);\n\t\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\t\treturn 0;\n\t}\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tif (user) {\n\t\t/*\n\t\t * Do not allow realtime tasks into groups that have no runtime\n\t\t * assigned.\n\t\t */\n\t\tif (rt_bandwidth_enabled() && rt_policy(policy) &&\n\t\t\t\ttask_group(p)->rt_bandwidth.rt_runtime == 0 &&\n\t\t\t\t!task_group_is_autogroup(task_group(p))) {\n\t\t\ttask_rq_unlock(rq, p, &flags);\n\t\t\treturn -EPERM;\n\t\t}\n\t}\n#endif\n\n\t/* recheck policy now with rq lock held */\n\tif (unlikely(oldpolicy != -1 && oldpolicy != p->policy)) {\n\t\tpolicy = oldpolicy = -1;\n\t\ttask_rq_unlock(rq, p, &flags);\n\t\tgoto recheck;\n\t}\n\ton_rq = p->on_rq;\n\trunning = task_current(rq, p);\n\tif (on_rq)\n\t\tdeactivate_task(rq, p, 0);\n\tif (running)\n\t\tp->sched_class->put_prev_task(rq, p);\n\n\tp->sched_reset_on_fork = reset_on_fork;\n\n\toldprio = p->prio;\n\tprev_class = p->sched_class;\n\t__setscheduler(rq, p, policy, param->sched_priority);\n\n\tif (running)\n\t\tp->sched_class->set_curr_task(rq);\n\tif (on_rq)\n\t\tactivate_task(rq, p, 0);\n\n\tcheck_class_changed(rq, p, prev_class, oldprio);\n\ttask_rq_unlock(rq, p, &flags);\n\n\trt_mutex_adjust_pi(p);\n\n\treturn 0;\n}\n\n/**\n * sched_setscheduler - change the scheduling policy and/or RT priority of a thread.\n * @p: the task in question.\n * @policy: new policy.\n * @param: structure containing the new RT priority.\n *\n * NOTE that the task may be already dead.\n */\nint sched_setscheduler(struct task_struct *p, int policy,\n\t\t       const struct sched_param *param)\n{\n\treturn __sched_setscheduler(p, policy, param, true);\n}\nEXPORT_SYMBOL_GPL(sched_setscheduler);\n\n/**\n * sched_setscheduler_nocheck - change the scheduling policy and/or RT priority of a thread from kernelspace.\n * @p: the task in question.\n * @policy: new policy.\n * @param: structure containing the new RT priority.\n *\n * Just like sched_setscheduler, only don't bother checking if the\n * current context has permission.  For example, this is needed in\n * stop_machine(): we create temporary high priority worker threads,\n * but our caller might not have that capability.\n */\nint sched_setscheduler_nocheck(struct task_struct *p, int policy,\n\t\t\t       const struct sched_param *param)\n{\n\treturn __sched_setscheduler(p, policy, param, false);\n}\n\nstatic int\ndo_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param)\n{\n\tstruct sched_param lparam;\n\tstruct task_struct *p;\n\tint retval;\n\n\tif (!param || pid < 0)\n\t\treturn -EINVAL;\n\tif (copy_from_user(&lparam, param, sizeof(struct sched_param)))\n\t\treturn -EFAULT;\n\n\trcu_read_lock();\n\tretval = -ESRCH;\n\tp = find_process_by_pid(pid);\n\tif (p != NULL)\n\t\tretval = sched_setscheduler(p, policy, &lparam);\n\trcu_read_unlock();\n\n\treturn retval;\n}\n\n/**\n * sys_sched_setscheduler - set/change the scheduler policy and RT priority\n * @pid: the pid in question.\n * @policy: new policy.\n * @param: structure containing the new RT priority.\n */\nSYSCALL_DEFINE3(sched_setscheduler, pid_t, pid, int, policy,\n\t\tstruct sched_param __user *, param)\n{\n\t/* negative values for policy are not valid */\n\tif (policy < 0)\n\t\treturn -EINVAL;\n\n\treturn do_sched_setscheduler(pid, policy, param);\n}\n\n/**\n * sys_sched_setparam - set/change the RT priority of a thread\n * @pid: the pid in question.\n * @param: structure containing the new RT priority.\n */\nSYSCALL_DEFINE2(sched_setparam, pid_t, pid, struct sched_param __user *, param)\n{\n\treturn do_sched_setscheduler(pid, -1, param);\n}\n\n/**\n * sys_sched_getscheduler - get the policy (scheduling class) of a thread\n * @pid: the pid in question.\n */\nSYSCALL_DEFINE1(sched_getscheduler, pid_t, pid)\n{\n\tstruct task_struct *p;\n\tint retval;\n\n\tif (pid < 0)\n\t\treturn -EINVAL;\n\n\tretval = -ESRCH;\n\trcu_read_lock();\n\tp = find_process_by_pid(pid);\n\tif (p) {\n\t\tretval = security_task_getscheduler(p);\n\t\tif (!retval)\n\t\t\tretval = p->policy\n\t\t\t\t| (p->sched_reset_on_fork ? SCHED_RESET_ON_FORK : 0);\n\t}\n\trcu_read_unlock();\n\treturn retval;\n}\n\n/**\n * sys_sched_getparam - get the RT priority of a thread\n * @pid: the pid in question.\n * @param: structure containing the RT priority.\n */\nSYSCALL_DEFINE2(sched_getparam, pid_t, pid, struct sched_param __user *, param)\n{\n\tstruct sched_param lp;\n\tstruct task_struct *p;\n\tint retval;\n\n\tif (!param || pid < 0)\n\t\treturn -EINVAL;\n\n\trcu_read_lock();\n\tp = find_process_by_pid(pid);\n\tretval = -ESRCH;\n\tif (!p)\n\t\tgoto out_unlock;\n\n\tretval = security_task_getscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\tlp.sched_priority = p->rt_priority;\n\trcu_read_unlock();\n\n\t/*\n\t * This one might sleep, we cannot do it with a spinlock held ...\n\t */\n\tretval = copy_to_user(param, &lp, sizeof(*param)) ? -EFAULT : 0;\n\n\treturn retval;\n\nout_unlock:\n\trcu_read_unlock();\n\treturn retval;\n}\n\nlong sched_setaffinity(pid_t pid, const struct cpumask *in_mask)\n{\n\tcpumask_var_t cpus_allowed, new_mask;\n\tstruct task_struct *p;\n\tint retval;\n\n\tget_online_cpus();\n\trcu_read_lock();\n\n\tp = find_process_by_pid(pid);\n\tif (!p) {\n\t\trcu_read_unlock();\n\t\tput_online_cpus();\n\t\treturn -ESRCH;\n\t}\n\n\t/* Prevent p going away */\n\tget_task_struct(p);\n\trcu_read_unlock();\n\n\tif (!alloc_cpumask_var(&cpus_allowed, GFP_KERNEL)) {\n\t\tretval = -ENOMEM;\n\t\tgoto out_put_task;\n\t}\n\tif (!alloc_cpumask_var(&new_mask, GFP_KERNEL)) {\n\t\tretval = -ENOMEM;\n\t\tgoto out_free_cpus_allowed;\n\t}\n\tretval = -EPERM;\n\tif (!check_same_owner(p) && !task_ns_capable(p, CAP_SYS_NICE))\n\t\tgoto out_unlock;\n\n\tretval = security_task_setscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\tcpuset_cpus_allowed(p, cpus_allowed);\n\tcpumask_and(new_mask, in_mask, cpus_allowed);\nagain:\n\tretval = set_cpus_allowed_ptr(p, new_mask);\n\n\tif (!retval) {\n\t\tcpuset_cpus_allowed(p, cpus_allowed);\n\t\tif (!cpumask_subset(new_mask, cpus_allowed)) {\n\t\t\t/*\n\t\t\t * We must have raced with a concurrent cpuset\n\t\t\t * update. Just reset the cpus_allowed to the\n\t\t\t * cpuset's cpus_allowed\n\t\t\t */\n\t\t\tcpumask_copy(new_mask, cpus_allowed);\n\t\t\tgoto again;\n\t\t}\n\t}\nout_unlock:\n\tfree_cpumask_var(new_mask);\nout_free_cpus_allowed:\n\tfree_cpumask_var(cpus_allowed);\nout_put_task:\n\tput_task_struct(p);\n\tput_online_cpus();\n\treturn retval;\n}\n\nstatic int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len,\n\t\t\t     struct cpumask *new_mask)\n{\n\tif (len < cpumask_size())\n\t\tcpumask_clear(new_mask);\n\telse if (len > cpumask_size())\n\t\tlen = cpumask_size();\n\n\treturn copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0;\n}\n\n/**\n * sys_sched_setaffinity - set the cpu affinity of a process\n * @pid: pid of the process\n * @len: length in bytes of the bitmask pointed to by user_mask_ptr\n * @user_mask_ptr: user-space pointer to the new cpu mask\n */\nSYSCALL_DEFINE3(sched_setaffinity, pid_t, pid, unsigned int, len,\n\t\tunsigned long __user *, user_mask_ptr)\n{\n\tcpumask_var_t new_mask;\n\tint retval;\n\n\tif (!alloc_cpumask_var(&new_mask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tretval = get_user_cpu_mask(user_mask_ptr, len, new_mask);\n\tif (retval == 0)\n\t\tretval = sched_setaffinity(pid, new_mask);\n\tfree_cpumask_var(new_mask);\n\treturn retval;\n}\n\nlong sched_getaffinity(pid_t pid, struct cpumask *mask)\n{\n\tstruct task_struct *p;\n\tunsigned long flags;\n\tint retval;\n\n\tget_online_cpus();\n\trcu_read_lock();\n\n\tretval = -ESRCH;\n\tp = find_process_by_pid(pid);\n\tif (!p)\n\t\tgoto out_unlock;\n\n\tretval = security_task_getscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\traw_spin_lock_irqsave(&p->pi_lock, flags);\n\tcpumask_and(mask, &p->cpus_allowed, cpu_online_mask);\n\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\nout_unlock:\n\trcu_read_unlock();\n\tput_online_cpus();\n\n\treturn retval;\n}\n\n/**\n * sys_sched_getaffinity - get the cpu affinity of a process\n * @pid: pid of the process\n * @len: length in bytes of the bitmask pointed to by user_mask_ptr\n * @user_mask_ptr: user-space pointer to hold the current cpu mask\n */\nSYSCALL_DEFINE3(sched_getaffinity, pid_t, pid, unsigned int, len,\n\t\tunsigned long __user *, user_mask_ptr)\n{\n\tint ret;\n\tcpumask_var_t mask;\n\n\tif ((len * BITS_PER_BYTE) < nr_cpu_ids)\n\t\treturn -EINVAL;\n\tif (len & (sizeof(unsigned long)-1))\n\t\treturn -EINVAL;\n\n\tif (!alloc_cpumask_var(&mask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tret = sched_getaffinity(pid, mask);\n\tif (ret == 0) {\n\t\tsize_t retlen = min_t(size_t, len, cpumask_size());\n\n\t\tif (copy_to_user(user_mask_ptr, mask, retlen))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = retlen;\n\t}\n\tfree_cpumask_var(mask);\n\n\treturn ret;\n}\n\n/**\n * sys_sched_yield - yield the current processor to other threads.\n *\n * This function yields the current CPU to other tasks. If there are no\n * other threads running on this CPU then this function will return.\n */\nSYSCALL_DEFINE0(sched_yield)\n{\n\tstruct rq *rq = this_rq_lock();\n\n\tschedstat_inc(rq, yld_count);\n\tcurrent->sched_class->yield_task(rq);\n\n\t/*\n\t * Since we are going to call schedule() anyway, there's\n\t * no need to preempt or enable interrupts:\n\t */\n\t__release(rq->lock);\n\tspin_release(&rq->lock.dep_map, 1, _THIS_IP_);\n\tdo_raw_spin_unlock(&rq->lock);\n\tpreempt_enable_no_resched();\n\n\tschedule();\n\n\treturn 0;\n}\n\nstatic inline int should_resched(void)\n{\n\treturn need_resched() && !(preempt_count() & PREEMPT_ACTIVE);\n}\n\nstatic void __cond_resched(void)\n{\n\tadd_preempt_count(PREEMPT_ACTIVE);\n\tschedule();\n\tsub_preempt_count(PREEMPT_ACTIVE);\n}\n\nint __sched _cond_resched(void)\n{\n\tif (should_resched()) {\n\t\t__cond_resched();\n\t\treturn 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(_cond_resched);\n\n/*\n * __cond_resched_lock() - if a reschedule is pending, drop the given lock,\n * call schedule, and on return reacquire the lock.\n *\n * This works OK both with and without CONFIG_PREEMPT. We do strange low-level\n * operations here to prevent schedule() from being called twice (once via\n * spin_unlock(), once by hand).\n */\nint __cond_resched_lock(spinlock_t *lock)\n{\n\tint resched = should_resched();\n\tint ret = 0;\n\n\tlockdep_assert_held(lock);\n\n\tif (spin_needbreak(lock) || resched) {\n\t\tspin_unlock(lock);\n\t\tif (resched)\n\t\t\t__cond_resched();\n\t\telse\n\t\t\tcpu_relax();\n\t\tret = 1;\n\t\tspin_lock(lock);\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__cond_resched_lock);\n\nint __sched __cond_resched_softirq(void)\n{\n\tBUG_ON(!in_softirq());\n\n\tif (should_resched()) {\n\t\tlocal_bh_enable();\n\t\t__cond_resched();\n\t\tlocal_bh_disable();\n\t\treturn 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(__cond_resched_softirq);\n\n/**\n * yield - yield the current processor to other threads.\n *\n * This is a shortcut for kernel-space yielding - it marks the\n * thread runnable and calls sys_sched_yield().\n */\nvoid __sched yield(void)\n{\n\tset_current_state(TASK_RUNNING);\n\tsys_sched_yield();\n}\nEXPORT_SYMBOL(yield);\n\n/**\n * yield_to - yield the current processor to another thread in\n * your thread group, or accelerate that thread toward the\n * processor it's on.\n * @p: target task\n * @preempt: whether task preemption is allowed or not\n *\n * It's the caller's job to ensure that the target task struct\n * can't go away on us before we can do any checks.\n *\n * Returns true if we indeed boosted the target task.\n */\nbool __sched yield_to(struct task_struct *p, bool preempt)\n{\n\tstruct task_struct *curr = current;\n\tstruct rq *rq, *p_rq;\n\tunsigned long flags;\n\tbool yielded = 0;\n\n\tlocal_irq_save(flags);\n\trq = this_rq();\n\nagain:\n\tp_rq = task_rq(p);\n\tdouble_rq_lock(rq, p_rq);\n\twhile (task_rq(p) != p_rq) {\n\t\tdouble_rq_unlock(rq, p_rq);\n\t\tgoto again;\n\t}\n\n\tif (!curr->sched_class->yield_to_task)\n\t\tgoto out;\n\n\tif (curr->sched_class != p->sched_class)\n\t\tgoto out;\n\n\tif (task_running(p_rq, p) || p->state)\n\t\tgoto out;\n\n\tyielded = curr->sched_class->yield_to_task(rq, p, preempt);\n\tif (yielded) {\n\t\tschedstat_inc(rq, yld_count);\n\t\t/*\n\t\t * Make p's CPU reschedule; pick_next_entity takes care of\n\t\t * fairness.\n\t\t */\n\t\tif (preempt && rq != p_rq)\n\t\t\tresched_task(p_rq->curr);\n\t}\n\nout:\n\tdouble_rq_unlock(rq, p_rq);\n\tlocal_irq_restore(flags);\n\n\tif (yielded)\n\t\tschedule();\n\n\treturn yielded;\n}\nEXPORT_SYMBOL_GPL(yield_to);\n\n/*\n * This task is about to go to sleep on IO. Increment rq->nr_iowait so\n * that process accounting knows that this is a task in IO wait state.\n */\nvoid __sched io_schedule(void)\n{\n\tstruct rq *rq = raw_rq();\n\n\tdelayacct_blkio_start();\n\tatomic_inc(&rq->nr_iowait);\n\tblk_flush_plug(current);\n\tcurrent->in_iowait = 1;\n\tschedule();\n\tcurrent->in_iowait = 0;\n\tatomic_dec(&rq->nr_iowait);\n\tdelayacct_blkio_end();\n}\nEXPORT_SYMBOL(io_schedule);\n\nlong __sched io_schedule_timeout(long timeout)\n{\n\tstruct rq *rq = raw_rq();\n\tlong ret;\n\n\tdelayacct_blkio_start();\n\tatomic_inc(&rq->nr_iowait);\n\tblk_flush_plug(current);\n\tcurrent->in_iowait = 1;\n\tret = schedule_timeout(timeout);\n\tcurrent->in_iowait = 0;\n\tatomic_dec(&rq->nr_iowait);\n\tdelayacct_blkio_end();\n\treturn ret;\n}\n\n/**\n * sys_sched_get_priority_max - return maximum RT priority.\n * @policy: scheduling class.\n *\n * this syscall returns the maximum rt_priority that can be used\n * by a given scheduling class.\n */\nSYSCALL_DEFINE1(sched_get_priority_max, int, policy)\n{\n\tint ret = -EINVAL;\n\n\tswitch (policy) {\n\tcase SCHED_FIFO:\n\tcase SCHED_RR:\n\t\tret = MAX_USER_RT_PRIO-1;\n\t\tbreak;\n\tcase SCHED_NORMAL:\n\tcase SCHED_BATCH:\n\tcase SCHED_IDLE:\n\t\tret = 0;\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\n/**\n * sys_sched_get_priority_min - return minimum RT priority.\n * @policy: scheduling class.\n *\n * this syscall returns the minimum rt_priority that can be used\n * by a given scheduling class.\n */\nSYSCALL_DEFINE1(sched_get_priority_min, int, policy)\n{\n\tint ret = -EINVAL;\n\n\tswitch (policy) {\n\tcase SCHED_FIFO:\n\tcase SCHED_RR:\n\t\tret = 1;\n\t\tbreak;\n\tcase SCHED_NORMAL:\n\tcase SCHED_BATCH:\n\tcase SCHED_IDLE:\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\n/**\n * sys_sched_rr_get_interval - return the default timeslice of a process.\n * @pid: pid of the process.\n * @interval: userspace pointer to the timeslice value.\n *\n * this syscall writes the default timeslice value of a given process\n * into the user-space timespec buffer. A value of '0' means infinity.\n */\nSYSCALL_DEFINE2(sched_rr_get_interval, pid_t, pid,\n\t\tstruct timespec __user *, interval)\n{\n\tstruct task_struct *p;\n\tunsigned int time_slice;\n\tunsigned long flags;\n\tstruct rq *rq;\n\tint retval;\n\tstruct timespec t;\n\n\tif (pid < 0)\n\t\treturn -EINVAL;\n\n\tretval = -ESRCH;\n\trcu_read_lock();\n\tp = find_process_by_pid(pid);\n\tif (!p)\n\t\tgoto out_unlock;\n\n\tretval = security_task_getscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\trq = task_rq_lock(p, &flags);\n\ttime_slice = p->sched_class->get_rr_interval(rq, p);\n\ttask_rq_unlock(rq, p, &flags);\n\n\trcu_read_unlock();\n\tjiffies_to_timespec(time_slice, &t);\n\tretval = copy_to_user(interval, &t, sizeof(t)) ? -EFAULT : 0;\n\treturn retval;\n\nout_unlock:\n\trcu_read_unlock();\n\treturn retval;\n}\n\nstatic const char stat_nam[] = TASK_STATE_TO_CHAR_STR;\n\nvoid sched_show_task(struct task_struct *p)\n{\n\tunsigned long free = 0;\n\tunsigned state;\n\n\tstate = p->state ? __ffs(p->state) + 1 : 0;\n\tprintk(KERN_INFO \"%-15.15s %c\", p->comm,\n\t\tstate < sizeof(stat_nam) - 1 ? stat_nam[state] : '?');\n#if BITS_PER_LONG == 32\n\tif (state == TASK_RUNNING)\n\t\tprintk(KERN_CONT \" running  \");\n\telse\n\t\tprintk(KERN_CONT \" %08lx \", thread_saved_pc(p));\n#else\n\tif (state == TASK_RUNNING)\n\t\tprintk(KERN_CONT \"  running task    \");\n\telse\n\t\tprintk(KERN_CONT \" %016lx \", thread_saved_pc(p));\n#endif\n#ifdef CONFIG_DEBUG_STACK_USAGE\n\tfree = stack_not_used(p);\n#endif\n\tprintk(KERN_CONT \"%5lu %5d %6d 0x%08lx\\n\", free,\n\t\ttask_pid_nr(p), task_pid_nr(p->real_parent),\n\t\t(unsigned long)task_thread_info(p)->flags);\n\n\tshow_stack(p, NULL);\n}\n\nvoid show_state_filter(unsigned long state_filter)\n{\n\tstruct task_struct *g, *p;\n\n#if BITS_PER_LONG == 32\n\tprintk(KERN_INFO\n\t\t\"  task                PC stack   pid father\\n\");\n#else\n\tprintk(KERN_INFO\n\t\t\"  task                        PC stack   pid father\\n\");\n#endif\n\tread_lock(&tasklist_lock);\n\tdo_each_thread(g, p) {\n\t\t/*\n\t\t * reset the NMI-timeout, listing all files on a slow\n\t\t * console might take a lot of time:\n\t\t */\n\t\ttouch_nmi_watchdog();\n\t\tif (!state_filter || (p->state & state_filter))\n\t\t\tsched_show_task(p);\n\t} while_each_thread(g, p);\n\n\ttouch_all_softlockup_watchdogs();\n\n#ifdef CONFIG_SCHED_DEBUG\n\tsysrq_sched_debug_show();\n#endif\n\tread_unlock(&tasklist_lock);\n\t/*\n\t * Only show locks if all tasks are dumped:\n\t */\n\tif (!state_filter)\n\t\tdebug_show_all_locks();\n}\n\nvoid __cpuinit init_idle_bootup_task(struct task_struct *idle)\n{\n\tidle->sched_class = &idle_sched_class;\n}\n\n/**\n * init_idle - set up an idle thread for a given CPU\n * @idle: task in question\n * @cpu: cpu the idle task belongs to\n *\n * NOTE: this function does not set the idle thread's NEED_RESCHED\n * flag, to make booting more robust.\n */\nvoid __cpuinit init_idle(struct task_struct *idle, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&rq->lock, flags);\n\n\t__sched_fork(idle);\n\tidle->state = TASK_RUNNING;\n\tidle->se.exec_start = sched_clock();\n\n\tdo_set_cpus_allowed(idle, cpumask_of(cpu));\n\t/*\n\t * We're having a chicken and egg problem, even though we are\n\t * holding rq->lock, the cpu isn't yet set to this cpu so the\n\t * lockdep check in task_group() will fail.\n\t *\n\t * Similar case to sched_fork(). / Alternatively we could\n\t * use task_rq_lock() here and obtain the other rq->lock.\n\t *\n\t * Silence PROVE_RCU\n\t */\n\trcu_read_lock();\n\t__set_task_cpu(idle, cpu);\n\trcu_read_unlock();\n\n\trq->curr = rq->idle = idle;\n#if defined(CONFIG_SMP)\n\tidle->on_cpu = 1;\n#endif\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\n\t/* Set the preempt count _outside_ the spinlocks! */\n\ttask_thread_info(idle)->preempt_count = 0;\n\n\t/*\n\t * The idle tasks have their own, simple scheduling class:\n\t */\n\tidle->sched_class = &idle_sched_class;\n\tftrace_graph_init_idle_task(idle, cpu);\n}\n\n/*\n * In a system that switches off the HZ timer nohz_cpu_mask\n * indicates which cpus entered this state. This is used\n * in the rcu update to wait only for active cpus. For system\n * which do not switch off the HZ timer nohz_cpu_mask should\n * always be CPU_BITS_NONE.\n */\ncpumask_var_t nohz_cpu_mask;\n\n/*\n * Increase the granularity value when there are more CPUs,\n * because with more CPUs the 'effective latency' as visible\n * to users decreases. But the relationship is not linear,\n * so pick a second-best guess by going with the log2 of the\n * number of CPUs.\n *\n * This idea comes from the SD scheduler of Con Kolivas:\n */\nstatic int get_update_sysctl_factor(void)\n{\n\tunsigned int cpus = min_t(int, num_online_cpus(), 8);\n\tunsigned int factor;\n\n\tswitch (sysctl_sched_tunable_scaling) {\n\tcase SCHED_TUNABLESCALING_NONE:\n\t\tfactor = 1;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LINEAR:\n\t\tfactor = cpus;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LOG:\n\tdefault:\n\t\tfactor = 1 + ilog2(cpus);\n\t\tbreak;\n\t}\n\n\treturn factor;\n}\n\nstatic void update_sysctl(void)\n{\n\tunsigned int factor = get_update_sysctl_factor();\n\n#define SET_SYSCTL(name) \\\n\t(sysctl_##name = (factor) * normalized_sysctl_##name)\n\tSET_SYSCTL(sched_min_granularity);\n\tSET_SYSCTL(sched_latency);\n\tSET_SYSCTL(sched_wakeup_granularity);\n#undef SET_SYSCTL\n}\n\nstatic inline void sched_init_granularity(void)\n{\n\tupdate_sysctl();\n}\n\n#ifdef CONFIG_SMP\nvoid do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)\n{\n\tif (p->sched_class && p->sched_class->set_cpus_allowed)\n\t\tp->sched_class->set_cpus_allowed(p, new_mask);\n\telse {\n\t\tcpumask_copy(&p->cpus_allowed, new_mask);\n\t\tp->rt.nr_cpus_allowed = cpumask_weight(new_mask);\n\t}\n}\n\n/*\n * This is how migration works:\n *\n * 1) we invoke migration_cpu_stop() on the target CPU using\n *    stop_one_cpu().\n * 2) stopper starts to run (implicitly forcing the migrated thread\n *    off the CPU)\n * 3) it checks whether the migrated task is still in the wrong runqueue.\n * 4) if it's in the wrong runqueue then the migration thread removes\n *    it and puts it into the right queue.\n * 5) stopper completes and stop_one_cpu() returns and the migration\n *    is done.\n */\n\n/*\n * Change a given task's CPU affinity. Migrate the thread to a\n * proper CPU and schedule it away if the CPU it's executing on\n * is removed from the allowed bitmask.\n *\n * NOTE: the caller must have a valid reference to the task, the\n * task must not exit() & deallocate itself prematurely. The\n * call is not atomic; no spinlocks may be held.\n */\nint set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tunsigned int dest_cpu;\n\tint ret = 0;\n\n\trq = task_rq_lock(p, &flags);\n\n\tif (cpumask_equal(&p->cpus_allowed, new_mask))\n\t\tgoto out;\n\n\tif (!cpumask_intersects(new_mask, cpu_active_mask)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (unlikely((p->flags & PF_THREAD_BOUND) && p != current)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tdo_set_cpus_allowed(p, new_mask);\n\n\t/* Can the task run on the task's current CPU? If so, we're done */\n\tif (cpumask_test_cpu(task_cpu(p), new_mask))\n\t\tgoto out;\n\n\tdest_cpu = cpumask_any_and(cpu_active_mask, new_mask);\n\tif (p->on_rq) {\n\t\tstruct migration_arg arg = { p, dest_cpu };\n\t\t/* Need help from migration thread: drop lock and wait. */\n\t\ttask_rq_unlock(rq, p, &flags);\n\t\tstop_one_cpu(cpu_of(rq), migration_cpu_stop, &arg);\n\t\ttlb_migrate_finish(p->mm);\n\t\treturn 0;\n\t}\nout:\n\ttask_rq_unlock(rq, p, &flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(set_cpus_allowed_ptr);\n\n/*\n * Move (not current) task off this cpu, onto dest cpu. We're doing\n * this because either it can't run here any more (set_cpus_allowed()\n * away from this CPU, or CPU going down), or because we're\n * attempting to rebalance this task on exec (sched_exec).\n *\n * So we race with normal scheduler movements, but that's OK, as long\n * as the task is no longer on this CPU.\n *\n * Returns non-zero if task was successfully migrated.\n */\nstatic int __migrate_task(struct task_struct *p, int src_cpu, int dest_cpu)\n{\n\tstruct rq *rq_dest, *rq_src;\n\tint ret = 0;\n\n\tif (unlikely(!cpu_active(dest_cpu)))\n\t\treturn ret;\n\n\trq_src = cpu_rq(src_cpu);\n\trq_dest = cpu_rq(dest_cpu);\n\n\traw_spin_lock(&p->pi_lock);\n\tdouble_rq_lock(rq_src, rq_dest);\n\t/* Already moved. */\n\tif (task_cpu(p) != src_cpu)\n\t\tgoto done;\n\t/* Affinity changed (again). */\n\tif (!cpumask_test_cpu(dest_cpu, &p->cpus_allowed))\n\t\tgoto fail;\n\n\t/*\n\t * If we're not on a rq, the next wake-up will ensure we're\n\t * placed properly.\n\t */\n\tif (p->on_rq) {\n\t\tdeactivate_task(rq_src, p, 0);\n\t\tset_task_cpu(p, dest_cpu);\n\t\tactivate_task(rq_dest, p, 0);\n\t\tcheck_preempt_curr(rq_dest, p, 0);\n\t}\ndone:\n\tret = 1;\nfail:\n\tdouble_rq_unlock(rq_src, rq_dest);\n\traw_spin_unlock(&p->pi_lock);\n\treturn ret;\n}\n\n/*\n * migration_cpu_stop - this will be executed by a highprio stopper thread\n * and performs thread migration by bumping thread off CPU then\n * 'pushing' onto another runqueue.\n */\nstatic int migration_cpu_stop(void *data)\n{\n\tstruct migration_arg *arg = data;\n\n\t/*\n\t * The original target cpu might have gone down and we might\n\t * be on another cpu but it doesn't matter.\n\t */\n\tlocal_irq_disable();\n\t__migrate_task(arg->task, raw_smp_processor_id(), arg->dest_cpu);\n\tlocal_irq_enable();\n\treturn 0;\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\n\n/*\n * Ensures that the idle task is using init_mm right before its cpu goes\n * offline.\n */\nvoid idle_task_exit(void)\n{\n\tstruct mm_struct *mm = current->active_mm;\n\n\tBUG_ON(cpu_online(smp_processor_id()));\n\n\tif (mm != &init_mm)\n\t\tswitch_mm(mm, &init_mm, current);\n\tmmdrop(mm);\n}\n\n/*\n * While a dead CPU has no uninterruptible tasks queued at this point,\n * it might still have a nonzero ->nr_uninterruptible counter, because\n * for performance reasons the counter is not stricly tracking tasks to\n * their home CPUs. So we just add the counter to another CPU's counter,\n * to keep the global sum constant after CPU-down:\n */\nstatic void migrate_nr_uninterruptible(struct rq *rq_src)\n{\n\tstruct rq *rq_dest = cpu_rq(cpumask_any(cpu_active_mask));\n\n\trq_dest->nr_uninterruptible += rq_src->nr_uninterruptible;\n\trq_src->nr_uninterruptible = 0;\n}\n\n/*\n * remove the tasks which were accounted by rq from calc_load_tasks.\n */\nstatic void calc_global_load_remove(struct rq *rq)\n{\n\tatomic_long_sub(rq->calc_load_active, &calc_load_tasks);\n\trq->calc_load_active = 0;\n}\n\n/*\n * Migrate all tasks from the rq, sleeping tasks will be migrated by\n * try_to_wake_up()->select_task_rq().\n *\n * Called with rq->lock held even though we'er in stop_machine() and\n * there's no concurrency possible, we hold the required locks anyway\n * because of lock validation efforts.\n */\nstatic void migrate_tasks(unsigned int dead_cpu)\n{\n\tstruct rq *rq = cpu_rq(dead_cpu);\n\tstruct task_struct *next, *stop = rq->stop;\n\tint dest_cpu;\n\n\t/*\n\t * Fudge the rq selection such that the below task selection loop\n\t * doesn't get stuck on the currently eligible stop task.\n\t *\n\t * We're currently inside stop_machine() and the rq is either stuck\n\t * in the stop_machine_cpu_stop() loop, or we're executing this code,\n\t * either way we should never end up calling schedule() until we're\n\t * done here.\n\t */\n\trq->stop = NULL;\n\n\tfor ( ; ; ) {\n\t\t/*\n\t\t * There's this thread running, bail when that's the only\n\t\t * remaining thread.\n\t\t */\n\t\tif (rq->nr_running == 1)\n\t\t\tbreak;\n\n\t\tnext = pick_next_task(rq);\n\t\tBUG_ON(!next);\n\t\tnext->sched_class->put_prev_task(rq, next);\n\n\t\t/* Find suitable destination for @next, with force if needed. */\n\t\tdest_cpu = select_fallback_rq(dead_cpu, next);\n\t\traw_spin_unlock(&rq->lock);\n\n\t\t__migrate_task(next, dead_cpu, dest_cpu);\n\n\t\traw_spin_lock(&rq->lock);\n\t}\n\n\trq->stop = stop;\n}\n\n#endif /* CONFIG_HOTPLUG_CPU */\n\n#if defined(CONFIG_SCHED_DEBUG) && defined(CONFIG_SYSCTL)\n\nstatic struct ctl_table sd_ctl_dir[] = {\n\t{\n\t\t.procname\t= \"sched_domain\",\n\t\t.mode\t\t= 0555,\n\t},\n\t{}\n};\n\nstatic struct ctl_table sd_ctl_root[] = {\n\t{\n\t\t.procname\t= \"kernel\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= sd_ctl_dir,\n\t},\n\t{}\n};\n\nstatic struct ctl_table *sd_alloc_ctl_entry(int n)\n{\n\tstruct ctl_table *entry =\n\t\tkcalloc(n, sizeof(struct ctl_table), GFP_KERNEL);\n\n\treturn entry;\n}\n\nstatic void sd_free_ctl_entry(struct ctl_table **tablep)\n{\n\tstruct ctl_table *entry;\n\n\t/*\n\t * In the intermediate directories, both the child directory and\n\t * procname are dynamically allocated and could fail but the mode\n\t * will always be set. In the lowest directory the names are\n\t * static strings and all have proc handlers.\n\t */\n\tfor (entry = *tablep; entry->mode; entry++) {\n\t\tif (entry->child)\n\t\t\tsd_free_ctl_entry(&entry->child);\n\t\tif (entry->proc_handler == NULL)\n\t\t\tkfree(entry->procname);\n\t}\n\n\tkfree(*tablep);\n\t*tablep = NULL;\n}\n\nstatic void\nset_table_entry(struct ctl_table *entry,\n\t\tconst char *procname, void *data, int maxlen,\n\t\tmode_t mode, proc_handler *proc_handler)\n{\n\tentry->procname = procname;\n\tentry->data = data;\n\tentry->maxlen = maxlen;\n\tentry->mode = mode;\n\tentry->proc_handler = proc_handler;\n}\n\nstatic struct ctl_table *\nsd_alloc_ctl_domain_table(struct sched_domain *sd)\n{\n\tstruct ctl_table *table = sd_alloc_ctl_entry(13);\n\n\tif (table == NULL)\n\t\treturn NULL;\n\n\tset_table_entry(&table[0], \"min_interval\", &sd->min_interval,\n\t\tsizeof(long), 0644, proc_doulongvec_minmax);\n\tset_table_entry(&table[1], \"max_interval\", &sd->max_interval,\n\t\tsizeof(long), 0644, proc_doulongvec_minmax);\n\tset_table_entry(&table[2], \"busy_idx\", &sd->busy_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[3], \"idle_idx\", &sd->idle_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[4], \"newidle_idx\", &sd->newidle_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[5], \"wake_idx\", &sd->wake_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[6], \"forkexec_idx\", &sd->forkexec_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[7], \"busy_factor\", &sd->busy_factor,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[8], \"imbalance_pct\", &sd->imbalance_pct,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[9], \"cache_nice_tries\",\n\t\t&sd->cache_nice_tries,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[10], \"flags\", &sd->flags,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[11], \"name\", sd->name,\n\t\tCORENAME_MAX_SIZE, 0444, proc_dostring);\n\t/* &table[12] is terminator */\n\n\treturn table;\n}\n\nstatic ctl_table *sd_alloc_ctl_cpu_table(int cpu)\n{\n\tstruct ctl_table *entry, *table;\n\tstruct sched_domain *sd;\n\tint domain_num = 0, i;\n\tchar buf[32];\n\n\tfor_each_domain(cpu, sd)\n\t\tdomain_num++;\n\tentry = table = sd_alloc_ctl_entry(domain_num + 1);\n\tif (table == NULL)\n\t\treturn NULL;\n\n\ti = 0;\n\tfor_each_domain(cpu, sd) {\n\t\tsnprintf(buf, 32, \"domain%d\", i);\n\t\tentry->procname = kstrdup(buf, GFP_KERNEL);\n\t\tentry->mode = 0555;\n\t\tentry->child = sd_alloc_ctl_domain_table(sd);\n\t\tentry++;\n\t\ti++;\n\t}\n\treturn table;\n}\n\nstatic struct ctl_table_header *sd_sysctl_header;\nstatic void register_sched_domain_sysctl(void)\n{\n\tint i, cpu_num = num_possible_cpus();\n\tstruct ctl_table *entry = sd_alloc_ctl_entry(cpu_num + 1);\n\tchar buf[32];\n\n\tWARN_ON(sd_ctl_dir[0].child);\n\tsd_ctl_dir[0].child = entry;\n\n\tif (entry == NULL)\n\t\treturn;\n\n\tfor_each_possible_cpu(i) {\n\t\tsnprintf(buf, 32, \"cpu%d\", i);\n\t\tentry->procname = kstrdup(buf, GFP_KERNEL);\n\t\tentry->mode = 0555;\n\t\tentry->child = sd_alloc_ctl_cpu_table(i);\n\t\tentry++;\n\t}\n\n\tWARN_ON(sd_sysctl_header);\n\tsd_sysctl_header = register_sysctl_table(sd_ctl_root);\n}\n\n/* may be called multiple times per register */\nstatic void unregister_sched_domain_sysctl(void)\n{\n\tif (sd_sysctl_header)\n\t\tunregister_sysctl_table(sd_sysctl_header);\n\tsd_sysctl_header = NULL;\n\tif (sd_ctl_dir[0].child)\n\t\tsd_free_ctl_entry(&sd_ctl_dir[0].child);\n}\n#else\nstatic void register_sched_domain_sysctl(void)\n{\n}\nstatic void unregister_sched_domain_sysctl(void)\n{\n}\n#endif\n\nstatic void set_rq_online(struct rq *rq)\n{\n\tif (!rq->online) {\n\t\tconst struct sched_class *class;\n\n\t\tcpumask_set_cpu(rq->cpu, rq->rd->online);\n\t\trq->online = 1;\n\n\t\tfor_each_class(class) {\n\t\t\tif (class->rq_online)\n\t\t\t\tclass->rq_online(rq);\n\t\t}\n\t}\n}\n\nstatic void set_rq_offline(struct rq *rq)\n{\n\tif (rq->online) {\n\t\tconst struct sched_class *class;\n\n\t\tfor_each_class(class) {\n\t\t\tif (class->rq_offline)\n\t\t\t\tclass->rq_offline(rq);\n\t\t}\n\n\t\tcpumask_clear_cpu(rq->cpu, rq->rd->online);\n\t\trq->online = 0;\n\t}\n}\n\n/*\n * migration_call - callback that gets triggered when a CPU is added.\n * Here we can start up the necessary migration thread for the new CPU.\n */\nstatic int __cpuinit\nmigration_call(struct notifier_block *nfb, unsigned long action, void *hcpu)\n{\n\tint cpu = (long)hcpu;\n\tunsigned long flags;\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\n\tcase CPU_UP_PREPARE:\n\t\trq->calc_load_update = calc_load_update;\n\t\tbreak;\n\n\tcase CPU_ONLINE:\n\t\t/* Update our root-domain */\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tif (rq->rd) {\n\t\t\tBUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));\n\n\t\t\tset_rq_online(rq);\n\t\t}\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\t\tbreak;\n\n#ifdef CONFIG_HOTPLUG_CPU\n\tcase CPU_DYING:\n\t\tsched_ttwu_pending();\n\t\t/* Update our root-domain */\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tif (rq->rd) {\n\t\t\tBUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));\n\t\t\tset_rq_offline(rq);\n\t\t}\n\t\tmigrate_tasks(cpu);\n\t\tBUG_ON(rq->nr_running != 1); /* the migration thread */\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\n\t\tmigrate_nr_uninterruptible(rq);\n\t\tcalc_global_load_remove(rq);\n\t\tbreak;\n#endif\n\t}\n\n\tupdate_max_interval();\n\n\treturn NOTIFY_OK;\n}\n\n/*\n * Register at high priority so that task migration (migrate_all_tasks)\n * happens before everything else.  This has to be lower priority than\n * the notifier in the perf_event subsystem, though.\n */\nstatic struct notifier_block __cpuinitdata migration_notifier = {\n\t.notifier_call = migration_call,\n\t.priority = CPU_PRI_MIGRATION,\n};\n\nstatic int __cpuinit sched_cpu_active(struct notifier_block *nfb,\n\t\t\t\t      unsigned long action, void *hcpu)\n{\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_ONLINE:\n\tcase CPU_DOWN_FAILED:\n\t\tset_cpu_active((long)hcpu, true);\n\t\treturn NOTIFY_OK;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nstatic int __cpuinit sched_cpu_inactive(struct notifier_block *nfb,\n\t\t\t\t\tunsigned long action, void *hcpu)\n{\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_DOWN_PREPARE:\n\t\tset_cpu_active((long)hcpu, false);\n\t\treturn NOTIFY_OK;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nstatic int __init migration_init(void)\n{\n\tvoid *cpu = (void *)(long)smp_processor_id();\n\tint err;\n\n\t/* Initialize migration for the boot CPU */\n\terr = migration_call(&migration_notifier, CPU_UP_PREPARE, cpu);\n\tBUG_ON(err == NOTIFY_BAD);\n\tmigration_call(&migration_notifier, CPU_ONLINE, cpu);\n\tregister_cpu_notifier(&migration_notifier);\n\n\t/* Register cpu active notifiers */\n\tcpu_notifier(sched_cpu_active, CPU_PRI_SCHED_ACTIVE);\n\tcpu_notifier(sched_cpu_inactive, CPU_PRI_SCHED_INACTIVE);\n\n\treturn 0;\n}\nearly_initcall(migration_init);\n#endif\n\n#ifdef CONFIG_SMP\n\nstatic cpumask_var_t sched_domains_tmpmask; /* sched_domains_mutex */\n\n#ifdef CONFIG_SCHED_DEBUG\n\nstatic __read_mostly int sched_domain_debug_enabled;\n\nstatic int __init sched_domain_debug_setup(char *str)\n{\n\tsched_domain_debug_enabled = 1;\n\n\treturn 0;\n}\nearly_param(\"sched_debug\", sched_domain_debug_setup);\n\nstatic int sched_domain_debug_one(struct sched_domain *sd, int cpu, int level,\n\t\t\t\t  struct cpumask *groupmask)\n{\n\tstruct sched_group *group = sd->groups;\n\tchar str[256];\n\n\tcpulist_scnprintf(str, sizeof(str), sched_domain_span(sd));\n\tcpumask_clear(groupmask);\n\n\tprintk(KERN_DEBUG \"%*s domain %d: \", level, \"\", level);\n\n\tif (!(sd->flags & SD_LOAD_BALANCE)) {\n\t\tprintk(\"does not load-balance\\n\");\n\t\tif (sd->parent)\n\t\t\tprintk(KERN_ERR \"ERROR: !SD_LOAD_BALANCE domain\"\n\t\t\t\t\t\" has parent\");\n\t\treturn -1;\n\t}\n\n\tprintk(KERN_CONT \"span %s level %s\\n\", str, sd->name);\n\n\tif (!cpumask_test_cpu(cpu, sched_domain_span(sd))) {\n\t\tprintk(KERN_ERR \"ERROR: domain->span does not contain \"\n\t\t\t\t\"CPU%d\\n\", cpu);\n\t}\n\tif (!cpumask_test_cpu(cpu, sched_group_cpus(group))) {\n\t\tprintk(KERN_ERR \"ERROR: domain->groups does not contain\"\n\t\t\t\t\" CPU%d\\n\", cpu);\n\t}\n\n\tprintk(KERN_DEBUG \"%*s groups:\", level + 1, \"\");\n\tdo {\n\t\tif (!group) {\n\t\t\tprintk(\"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: group is NULL\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!group->cpu_power) {\n\t\t\tprintk(KERN_CONT \"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: domain->cpu_power not \"\n\t\t\t\t\t\"set\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!cpumask_weight(sched_group_cpus(group))) {\n\t\t\tprintk(KERN_CONT \"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: empty group\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (cpumask_intersects(groupmask, sched_group_cpus(group))) {\n\t\t\tprintk(KERN_CONT \"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: repeated CPUs\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tcpumask_or(groupmask, groupmask, sched_group_cpus(group));\n\n\t\tcpulist_scnprintf(str, sizeof(str), sched_group_cpus(group));\n\n\t\tprintk(KERN_CONT \" %s\", str);\n\t\tif (group->cpu_power != SCHED_POWER_SCALE) {\n\t\t\tprintk(KERN_CONT \" (cpu_power = %d)\",\n\t\t\t\tgroup->cpu_power);\n\t\t}\n\n\t\tgroup = group->next;\n\t} while (group != sd->groups);\n\tprintk(KERN_CONT \"\\n\");\n\n\tif (!cpumask_equal(sched_domain_span(sd), groupmask))\n\t\tprintk(KERN_ERR \"ERROR: groups don't span domain->span\\n\");\n\n\tif (sd->parent &&\n\t    !cpumask_subset(groupmask, sched_domain_span(sd->parent)))\n\t\tprintk(KERN_ERR \"ERROR: parent span is not a superset \"\n\t\t\t\"of domain->span\\n\");\n\treturn 0;\n}\n\nstatic void sched_domain_debug(struct sched_domain *sd, int cpu)\n{\n\tint level = 0;\n\n\tif (!sched_domain_debug_enabled)\n\t\treturn;\n\n\tif (!sd) {\n\t\tprintk(KERN_DEBUG \"CPU%d attaching NULL sched-domain.\\n\", cpu);\n\t\treturn;\n\t}\n\n\tprintk(KERN_DEBUG \"CPU%d attaching sched-domain:\\n\", cpu);\n\n\tfor (;;) {\n\t\tif (sched_domain_debug_one(sd, cpu, level, sched_domains_tmpmask))\n\t\t\tbreak;\n\t\tlevel++;\n\t\tsd = sd->parent;\n\t\tif (!sd)\n\t\t\tbreak;\n\t}\n}\n#else /* !CONFIG_SCHED_DEBUG */\n# define sched_domain_debug(sd, cpu) do { } while (0)\n#endif /* CONFIG_SCHED_DEBUG */\n\nstatic int sd_degenerate(struct sched_domain *sd)\n{\n\tif (cpumask_weight(sched_domain_span(sd)) == 1)\n\t\treturn 1;\n\n\t/* Following flags need at least 2 groups */\n\tif (sd->flags & (SD_LOAD_BALANCE |\n\t\t\t SD_BALANCE_NEWIDLE |\n\t\t\t SD_BALANCE_FORK |\n\t\t\t SD_BALANCE_EXEC |\n\t\t\t SD_SHARE_CPUPOWER |\n\t\t\t SD_SHARE_PKG_RESOURCES)) {\n\t\tif (sd->groups != sd->groups->next)\n\t\t\treturn 0;\n\t}\n\n\t/* Following flags don't use groups */\n\tif (sd->flags & (SD_WAKE_AFFINE))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int\nsd_parent_degenerate(struct sched_domain *sd, struct sched_domain *parent)\n{\n\tunsigned long cflags = sd->flags, pflags = parent->flags;\n\n\tif (sd_degenerate(parent))\n\t\treturn 1;\n\n\tif (!cpumask_equal(sched_domain_span(sd), sched_domain_span(parent)))\n\t\treturn 0;\n\n\t/* Flags needing groups don't count if only 1 group in parent */\n\tif (parent->groups == parent->groups->next) {\n\t\tpflags &= ~(SD_LOAD_BALANCE |\n\t\t\t\tSD_BALANCE_NEWIDLE |\n\t\t\t\tSD_BALANCE_FORK |\n\t\t\t\tSD_BALANCE_EXEC |\n\t\t\t\tSD_SHARE_CPUPOWER |\n\t\t\t\tSD_SHARE_PKG_RESOURCES);\n\t\tif (nr_node_ids == 1)\n\t\t\tpflags &= ~SD_SERIALIZE;\n\t}\n\tif (~cflags & pflags)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic void free_rootdomain(struct rcu_head *rcu)\n{\n\tstruct root_domain *rd = container_of(rcu, struct root_domain, rcu);\n\n\tcpupri_cleanup(&rd->cpupri);\n\tfree_cpumask_var(rd->rto_mask);\n\tfree_cpumask_var(rd->online);\n\tfree_cpumask_var(rd->span);\n\tkfree(rd);\n}\n\nstatic void rq_attach_root(struct rq *rq, struct root_domain *rd)\n{\n\tstruct root_domain *old_rd = NULL;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&rq->lock, flags);\n\n\tif (rq->rd) {\n\t\told_rd = rq->rd;\n\n\t\tif (cpumask_test_cpu(rq->cpu, old_rd->online))\n\t\t\tset_rq_offline(rq);\n\n\t\tcpumask_clear_cpu(rq->cpu, old_rd->span);\n\n\t\t/*\n\t\t * If we dont want to free the old_rt yet then\n\t\t * set old_rd to NULL to skip the freeing later\n\t\t * in this function:\n\t\t */\n\t\tif (!atomic_dec_and_test(&old_rd->refcount))\n\t\t\told_rd = NULL;\n\t}\n\n\tatomic_inc(&rd->refcount);\n\trq->rd = rd;\n\n\tcpumask_set_cpu(rq->cpu, rd->span);\n\tif (cpumask_test_cpu(rq->cpu, cpu_active_mask))\n\t\tset_rq_online(rq);\n\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\n\tif (old_rd)\n\t\tcall_rcu_sched(&old_rd->rcu, free_rootdomain);\n}\n\nstatic int init_rootdomain(struct root_domain *rd)\n{\n\tmemset(rd, 0, sizeof(*rd));\n\n\tif (!alloc_cpumask_var(&rd->span, GFP_KERNEL))\n\t\tgoto out;\n\tif (!alloc_cpumask_var(&rd->online, GFP_KERNEL))\n\t\tgoto free_span;\n\tif (!alloc_cpumask_var(&rd->rto_mask, GFP_KERNEL))\n\t\tgoto free_online;\n\n\tif (cpupri_init(&rd->cpupri) != 0)\n\t\tgoto free_rto_mask;\n\treturn 0;\n\nfree_rto_mask:\n\tfree_cpumask_var(rd->rto_mask);\nfree_online:\n\tfree_cpumask_var(rd->online);\nfree_span:\n\tfree_cpumask_var(rd->span);\nout:\n\treturn -ENOMEM;\n}\n\nstatic void init_defrootdomain(void)\n{\n\tinit_rootdomain(&def_root_domain);\n\n\tatomic_set(&def_root_domain.refcount, 1);\n}\n\nstatic struct root_domain *alloc_rootdomain(void)\n{\n\tstruct root_domain *rd;\n\n\trd = kmalloc(sizeof(*rd), GFP_KERNEL);\n\tif (!rd)\n\t\treturn NULL;\n\n\tif (init_rootdomain(rd) != 0) {\n\t\tkfree(rd);\n\t\treturn NULL;\n\t}\n\n\treturn rd;\n}\n\nstatic void free_sched_domain(struct rcu_head *rcu)\n{\n\tstruct sched_domain *sd = container_of(rcu, struct sched_domain, rcu);\n\tif (atomic_dec_and_test(&sd->groups->ref))\n\t\tkfree(sd->groups);\n\tkfree(sd);\n}\n\nstatic void destroy_sched_domain(struct sched_domain *sd, int cpu)\n{\n\tcall_rcu(&sd->rcu, free_sched_domain);\n}\n\nstatic void destroy_sched_domains(struct sched_domain *sd, int cpu)\n{\n\tfor (; sd; sd = sd->parent)\n\t\tdestroy_sched_domain(sd, cpu);\n}\n\n/*\n * Attach the domain 'sd' to 'cpu' as its base domain. Callers must\n * hold the hotplug lock.\n */\nstatic void\ncpu_attach_domain(struct sched_domain *sd, struct root_domain *rd, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct sched_domain *tmp;\n\n\t/* Remove the sched domains which do not contribute to scheduling. */\n\tfor (tmp = sd; tmp; ) {\n\t\tstruct sched_domain *parent = tmp->parent;\n\t\tif (!parent)\n\t\t\tbreak;\n\n\t\tif (sd_parent_degenerate(tmp, parent)) {\n\t\t\ttmp->parent = parent->parent;\n\t\t\tif (parent->parent)\n\t\t\t\tparent->parent->child = tmp;\n\t\t\tdestroy_sched_domain(parent, cpu);\n\t\t} else\n\t\t\ttmp = tmp->parent;\n\t}\n\n\tif (sd && sd_degenerate(sd)) {\n\t\ttmp = sd;\n\t\tsd = sd->parent;\n\t\tdestroy_sched_domain(tmp, cpu);\n\t\tif (sd)\n\t\t\tsd->child = NULL;\n\t}\n\n\tsched_domain_debug(sd, cpu);\n\n\trq_attach_root(rq, rd);\n\ttmp = rq->sd;\n\trcu_assign_pointer(rq->sd, sd);\n\tdestroy_sched_domains(tmp, cpu);\n}\n\n/* cpus with isolated domains */\nstatic cpumask_var_t cpu_isolated_map;\n\n/* Setup the mask of cpus configured for isolated domains */\nstatic int __init isolated_cpu_setup(char *str)\n{\n\talloc_bootmem_cpumask_var(&cpu_isolated_map);\n\tcpulist_parse(str, cpu_isolated_map);\n\treturn 1;\n}\n\n__setup(\"isolcpus=\", isolated_cpu_setup);\n\n#define SD_NODES_PER_DOMAIN 16\n\n#ifdef CONFIG_NUMA\n\n/**\n * find_next_best_node - find the next node to include in a sched_domain\n * @node: node whose sched_domain we're building\n * @used_nodes: nodes already in the sched_domain\n *\n * Find the next node to include in a given scheduling domain. Simply\n * finds the closest node not already in the @used_nodes map.\n *\n * Should use nodemask_t.\n */\nstatic int find_next_best_node(int node, nodemask_t *used_nodes)\n{\n\tint i, n, val, min_val, best_node = -1;\n\n\tmin_val = INT_MAX;\n\n\tfor (i = 0; i < nr_node_ids; i++) {\n\t\t/* Start at @node */\n\t\tn = (node + i) % nr_node_ids;\n\n\t\tif (!nr_cpus_node(n))\n\t\t\tcontinue;\n\n\t\t/* Skip already used nodes */\n\t\tif (node_isset(n, *used_nodes))\n\t\t\tcontinue;\n\n\t\t/* Simple min distance search */\n\t\tval = node_distance(node, n);\n\n\t\tif (val < min_val) {\n\t\t\tmin_val = val;\n\t\t\tbest_node = n;\n\t\t}\n\t}\n\n\tif (best_node != -1)\n\t\tnode_set(best_node, *used_nodes);\n\treturn best_node;\n}\n\n/**\n * sched_domain_node_span - get a cpumask for a node's sched_domain\n * @node: node whose cpumask we're constructing\n * @span: resulting cpumask\n *\n * Given a node, construct a good cpumask for its sched_domain to span. It\n * should be one that prevents unnecessary balancing, but also spreads tasks\n * out optimally.\n */\nstatic void sched_domain_node_span(int node, struct cpumask *span)\n{\n\tnodemask_t used_nodes;\n\tint i;\n\n\tcpumask_clear(span);\n\tnodes_clear(used_nodes);\n\n\tcpumask_or(span, span, cpumask_of_node(node));\n\tnode_set(node, used_nodes);\n\n\tfor (i = 1; i < SD_NODES_PER_DOMAIN; i++) {\n\t\tint next_node = find_next_best_node(node, &used_nodes);\n\t\tif (next_node < 0)\n\t\t\tbreak;\n\t\tcpumask_or(span, span, cpumask_of_node(next_node));\n\t}\n}\n\nstatic const struct cpumask *cpu_node_mask(int cpu)\n{\n\tlockdep_assert_held(&sched_domains_mutex);\n\n\tsched_domain_node_span(cpu_to_node(cpu), sched_domains_tmpmask);\n\n\treturn sched_domains_tmpmask;\n}\n\nstatic const struct cpumask *cpu_allnodes_mask(int cpu)\n{\n\treturn cpu_possible_mask;\n}\n#endif /* CONFIG_NUMA */\n\nstatic const struct cpumask *cpu_cpu_mask(int cpu)\n{\n\treturn cpumask_of_node(cpu_to_node(cpu));\n}\n\nint sched_smt_power_savings = 0, sched_mc_power_savings = 0;\n\nstruct sd_data {\n\tstruct sched_domain **__percpu sd;\n\tstruct sched_group **__percpu sg;\n};\n\nstruct s_data {\n\tstruct sched_domain ** __percpu sd;\n\tstruct root_domain\t*rd;\n};\n\nenum s_alloc {\n\tsa_rootdomain,\n\tsa_sd,\n\tsa_sd_storage,\n\tsa_none,\n};\n\nstruct sched_domain_topology_level;\n\ntypedef struct sched_domain *(*sched_domain_init_f)(struct sched_domain_topology_level *tl, int cpu);\ntypedef const struct cpumask *(*sched_domain_mask_f)(int cpu);\n\nstruct sched_domain_topology_level {\n\tsched_domain_init_f init;\n\tsched_domain_mask_f mask;\n\tstruct sd_data      data;\n};\n\n/*\n * Assumes the sched_domain tree is fully constructed\n */\nstatic int get_group(int cpu, struct sd_data *sdd, struct sched_group **sg)\n{\n\tstruct sched_domain *sd = *per_cpu_ptr(sdd->sd, cpu);\n\tstruct sched_domain *child = sd->child;\n\n\tif (child)\n\t\tcpu = cpumask_first(sched_domain_span(child));\n\n\tif (sg)\n\t\t*sg = *per_cpu_ptr(sdd->sg, cpu);\n\n\treturn cpu;\n}\n\n/*\n * build_sched_groups takes the cpumask we wish to span, and a pointer\n * to a function which identifies what group(along with sched group) a CPU\n * belongs to. The return value of group_fn must be a >= 0 and < nr_cpu_ids\n * (due to the fact that we keep track of groups covered with a struct cpumask).\n *\n * build_sched_groups will build a circular linked list of the groups\n * covered by the given span, and will set each group's ->cpumask correctly,\n * and ->cpu_power to 0.\n */\nstatic void\nbuild_sched_groups(struct sched_domain *sd)\n{\n\tstruct sched_group *first = NULL, *last = NULL;\n\tstruct sd_data *sdd = sd->private;\n\tconst struct cpumask *span = sched_domain_span(sd);\n\tstruct cpumask *covered;\n\tint i;\n\n\tlockdep_assert_held(&sched_domains_mutex);\n\tcovered = sched_domains_tmpmask;\n\n\tcpumask_clear(covered);\n\n\tfor_each_cpu(i, span) {\n\t\tstruct sched_group *sg;\n\t\tint group = get_group(i, sdd, &sg);\n\t\tint j;\n\n\t\tif (cpumask_test_cpu(i, covered))\n\t\t\tcontinue;\n\n\t\tcpumask_clear(sched_group_cpus(sg));\n\t\tsg->cpu_power = 0;\n\n\t\tfor_each_cpu(j, span) {\n\t\t\tif (get_group(j, sdd, NULL) != group)\n\t\t\t\tcontinue;\n\n\t\t\tcpumask_set_cpu(j, covered);\n\t\t\tcpumask_set_cpu(j, sched_group_cpus(sg));\n\t\t}\n\n\t\tif (!first)\n\t\t\tfirst = sg;\n\t\tif (last)\n\t\t\tlast->next = sg;\n\t\tlast = sg;\n\t}\n\tlast->next = first;\n}\n\n/*\n * Initialize sched groups cpu_power.\n *\n * cpu_power indicates the capacity of sched group, which is used while\n * distributing the load between different sched groups in a sched domain.\n * Typically cpu_power for all the groups in a sched domain will be same unless\n * there are asymmetries in the topology. If there are asymmetries, group\n * having more cpu_power will pickup more load compared to the group having\n * less cpu_power.\n */\nstatic void init_sched_groups_power(int cpu, struct sched_domain *sd)\n{\n\tWARN_ON(!sd || !sd->groups);\n\n\tif (cpu != group_first_cpu(sd->groups))\n\t\treturn;\n\n\tsd->groups->group_weight = cpumask_weight(sched_group_cpus(sd->groups));\n\n\tupdate_group_power(sd, cpu);\n}\n\n/*\n * Initializers for schedule domains\n * Non-inlined to reduce accumulated stack pressure in build_sched_domains()\n */\n\n#ifdef CONFIG_SCHED_DEBUG\n# define SD_INIT_NAME(sd, type)\t\tsd->name = #type\n#else\n# define SD_INIT_NAME(sd, type)\t\tdo { } while (0)\n#endif\n\n#define SD_INIT_FUNC(type)\t\t\t\t\t\t\\\nstatic noinline struct sched_domain *\t\t\t\t\t\\\nsd_init_##type(struct sched_domain_topology_level *tl, int cpu) \t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct sched_domain *sd = *per_cpu_ptr(tl->data.sd, cpu);\t\\\n\t*sd = SD_##type##_INIT;\t\t\t\t\t\t\\\n\tSD_INIT_NAME(sd, type);\t\t\t\t\t\t\\\n\tsd->private = &tl->data;\t\t\t\t\t\\\n\treturn sd;\t\t\t\t\t\t\t\\\n}\n\nSD_INIT_FUNC(CPU)\n#ifdef CONFIG_NUMA\n SD_INIT_FUNC(ALLNODES)\n SD_INIT_FUNC(NODE)\n#endif\n#ifdef CONFIG_SCHED_SMT\n SD_INIT_FUNC(SIBLING)\n#endif\n#ifdef CONFIG_SCHED_MC\n SD_INIT_FUNC(MC)\n#endif\n#ifdef CONFIG_SCHED_BOOK\n SD_INIT_FUNC(BOOK)\n#endif\n\nstatic int default_relax_domain_level = -1;\nint sched_domain_level_max;\n\nstatic int __init setup_relax_domain_level(char *str)\n{\n\tunsigned long val;\n\n\tval = simple_strtoul(str, NULL, 0);\n\tif (val < sched_domain_level_max)\n\t\tdefault_relax_domain_level = val;\n\n\treturn 1;\n}\n__setup(\"relax_domain_level=\", setup_relax_domain_level);\n\nstatic void set_domain_attribute(struct sched_domain *sd,\n\t\t\t\t struct sched_domain_attr *attr)\n{\n\tint request;\n\n\tif (!attr || attr->relax_domain_level < 0) {\n\t\tif (default_relax_domain_level < 0)\n\t\t\treturn;\n\t\telse\n\t\t\trequest = default_relax_domain_level;\n\t} else\n\t\trequest = attr->relax_domain_level;\n\tif (request < sd->level) {\n\t\t/* turn off idle balance on this domain */\n\t\tsd->flags &= ~(SD_BALANCE_WAKE|SD_BALANCE_NEWIDLE);\n\t} else {\n\t\t/* turn on idle balance on this domain */\n\t\tsd->flags |= (SD_BALANCE_WAKE|SD_BALANCE_NEWIDLE);\n\t}\n}\n\nstatic void __sdt_free(const struct cpumask *cpu_map);\nstatic int __sdt_alloc(const struct cpumask *cpu_map);\n\nstatic void __free_domain_allocs(struct s_data *d, enum s_alloc what,\n\t\t\t\t const struct cpumask *cpu_map)\n{\n\tswitch (what) {\n\tcase sa_rootdomain:\n\t\tif (!atomic_read(&d->rd->refcount))\n\t\t\tfree_rootdomain(&d->rd->rcu); /* fall through */\n\tcase sa_sd:\n\t\tfree_percpu(d->sd); /* fall through */\n\tcase sa_sd_storage:\n\t\t__sdt_free(cpu_map); /* fall through */\n\tcase sa_none:\n\t\tbreak;\n\t}\n}\n\nstatic enum s_alloc __visit_domain_allocation_hell(struct s_data *d,\n\t\t\t\t\t\t   const struct cpumask *cpu_map)\n{\n\tmemset(d, 0, sizeof(*d));\n\n\tif (__sdt_alloc(cpu_map))\n\t\treturn sa_sd_storage;\n\td->sd = alloc_percpu(struct sched_domain *);\n\tif (!d->sd)\n\t\treturn sa_sd_storage;\n\td->rd = alloc_rootdomain();\n\tif (!d->rd)\n\t\treturn sa_sd;\n\treturn sa_rootdomain;\n}\n\n/*\n * NULL the sd_data elements we've used to build the sched_domain and\n * sched_group structure so that the subsequent __free_domain_allocs()\n * will not free the data we're using.\n */\nstatic void claim_allocations(int cpu, struct sched_domain *sd)\n{\n\tstruct sd_data *sdd = sd->private;\n\tstruct sched_group *sg = sd->groups;\n\n\tWARN_ON_ONCE(*per_cpu_ptr(sdd->sd, cpu) != sd);\n\t*per_cpu_ptr(sdd->sd, cpu) = NULL;\n\n\tif (cpu == cpumask_first(sched_group_cpus(sg))) {\n\t\tWARN_ON_ONCE(*per_cpu_ptr(sdd->sg, cpu) != sg);\n\t\t*per_cpu_ptr(sdd->sg, cpu) = NULL;\n\t}\n}\n\n#ifdef CONFIG_SCHED_SMT\nstatic const struct cpumask *cpu_smt_mask(int cpu)\n{\n\treturn topology_thread_cpumask(cpu);\n}\n#endif\n\n/*\n * Topology list, bottom-up.\n */\nstatic struct sched_domain_topology_level default_topology[] = {\n#ifdef CONFIG_SCHED_SMT\n\t{ sd_init_SIBLING, cpu_smt_mask, },\n#endif\n#ifdef CONFIG_SCHED_MC\n\t{ sd_init_MC, cpu_coregroup_mask, },\n#endif\n#ifdef CONFIG_SCHED_BOOK\n\t{ sd_init_BOOK, cpu_book_mask, },\n#endif\n\t{ sd_init_CPU, cpu_cpu_mask, },\n#ifdef CONFIG_NUMA\n\t{ sd_init_NODE, cpu_node_mask, },\n\t{ sd_init_ALLNODES, cpu_allnodes_mask, },\n#endif\n\t{ NULL, },\n};\n\nstatic struct sched_domain_topology_level *sched_domain_topology = default_topology;\n\nstatic int __sdt_alloc(const struct cpumask *cpu_map)\n{\n\tstruct sched_domain_topology_level *tl;\n\tint j;\n\n\tfor (tl = sched_domain_topology; tl->init; tl++) {\n\t\tstruct sd_data *sdd = &tl->data;\n\n\t\tsdd->sd = alloc_percpu(struct sched_domain *);\n\t\tif (!sdd->sd)\n\t\t\treturn -ENOMEM;\n\n\t\tsdd->sg = alloc_percpu(struct sched_group *);\n\t\tif (!sdd->sg)\n\t\t\treturn -ENOMEM;\n\n\t\tfor_each_cpu(j, cpu_map) {\n\t\t\tstruct sched_domain *sd;\n\t\t\tstruct sched_group *sg;\n\n\t\t       \tsd = kzalloc_node(sizeof(struct sched_domain) + cpumask_size(),\n\t\t\t\t\tGFP_KERNEL, cpu_to_node(j));\n\t\t\tif (!sd)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\t*per_cpu_ptr(sdd->sd, j) = sd;\n\n\t\t\tsg = kzalloc_node(sizeof(struct sched_group) + cpumask_size(),\n\t\t\t\t\tGFP_KERNEL, cpu_to_node(j));\n\t\t\tif (!sg)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\t*per_cpu_ptr(sdd->sg, j) = sg;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void __sdt_free(const struct cpumask *cpu_map)\n{\n\tstruct sched_domain_topology_level *tl;\n\tint j;\n\n\tfor (tl = sched_domain_topology; tl->init; tl++) {\n\t\tstruct sd_data *sdd = &tl->data;\n\n\t\tfor_each_cpu(j, cpu_map) {\n\t\t\tkfree(*per_cpu_ptr(sdd->sd, j));\n\t\t\tkfree(*per_cpu_ptr(sdd->sg, j));\n\t\t}\n\t\tfree_percpu(sdd->sd);\n\t\tfree_percpu(sdd->sg);\n\t}\n}\n\nstruct sched_domain *build_sched_domain(struct sched_domain_topology_level *tl,\n\t\tstruct s_data *d, const struct cpumask *cpu_map,\n\t\tstruct sched_domain_attr *attr, struct sched_domain *child,\n\t\tint cpu)\n{\n\tstruct sched_domain *sd = tl->init(tl, cpu);\n\tif (!sd)\n\t\treturn child;\n\n\tset_domain_attribute(sd, attr);\n\tcpumask_and(sched_domain_span(sd), cpu_map, tl->mask(cpu));\n\tif (child) {\n\t\tsd->level = child->level + 1;\n\t\tsched_domain_level_max = max(sched_domain_level_max, sd->level);\n\t\tchild->parent = sd;\n\t}\n\tsd->child = child;\n\n\treturn sd;\n}\n\n/*\n * Build sched domains for a given set of cpus and attach the sched domains\n * to the individual cpus\n */\nstatic int build_sched_domains(const struct cpumask *cpu_map,\n\t\t\t       struct sched_domain_attr *attr)\n{\n\tenum s_alloc alloc_state = sa_none;\n\tstruct sched_domain *sd;\n\tstruct s_data d;\n\tint i, ret = -ENOMEM;\n\n\talloc_state = __visit_domain_allocation_hell(&d, cpu_map);\n\tif (alloc_state != sa_rootdomain)\n\t\tgoto error;\n\n\t/* Set up domains for cpus specified by the cpu_map. */\n\tfor_each_cpu(i, cpu_map) {\n\t\tstruct sched_domain_topology_level *tl;\n\n\t\tsd = NULL;\n\t\tfor (tl = sched_domain_topology; tl->init; tl++)\n\t\t\tsd = build_sched_domain(tl, &d, cpu_map, attr, sd, i);\n\n\t\twhile (sd->child)\n\t\t\tsd = sd->child;\n\n\t\t*per_cpu_ptr(d.sd, i) = sd;\n\t}\n\n\t/* Build the groups for the domains */\n\tfor_each_cpu(i, cpu_map) {\n\t\tfor (sd = *per_cpu_ptr(d.sd, i); sd; sd = sd->parent) {\n\t\t\tsd->span_weight = cpumask_weight(sched_domain_span(sd));\n\t\t\tget_group(i, sd->private, &sd->groups);\n\t\t\tatomic_inc(&sd->groups->ref);\n\n\t\t\tif (i != cpumask_first(sched_domain_span(sd)))\n\t\t\t\tcontinue;\n\n\t\t\tbuild_sched_groups(sd);\n\t\t}\n\t}\n\n\t/* Calculate CPU power for physical packages and nodes */\n\tfor (i = nr_cpumask_bits-1; i >= 0; i--) {\n\t\tif (!cpumask_test_cpu(i, cpu_map))\n\t\t\tcontinue;\n\n\t\tfor (sd = *per_cpu_ptr(d.sd, i); sd; sd = sd->parent) {\n\t\t\tclaim_allocations(i, sd);\n\t\t\tinit_sched_groups_power(i, sd);\n\t\t}\n\t}\n\n\t/* Attach the domains */\n\trcu_read_lock();\n\tfor_each_cpu(i, cpu_map) {\n\t\tsd = *per_cpu_ptr(d.sd, i);\n\t\tcpu_attach_domain(sd, d.rd, i);\n\t}\n\trcu_read_unlock();\n\n\tret = 0;\nerror:\n\t__free_domain_allocs(&d, alloc_state, cpu_map);\n\treturn ret;\n}\n\nstatic cpumask_var_t *doms_cur;\t/* current sched domains */\nstatic int ndoms_cur;\t\t/* number of sched domains in 'doms_cur' */\nstatic struct sched_domain_attr *dattr_cur;\n\t\t\t\t/* attribues of custom domains in 'doms_cur' */\n\n/*\n * Special case: If a kmalloc of a doms_cur partition (array of\n * cpumask) fails, then fallback to a single sched domain,\n * as determined by the single cpumask fallback_doms.\n */\nstatic cpumask_var_t fallback_doms;\n\n/*\n * arch_update_cpu_topology lets virtualized architectures update the\n * cpu core maps. It is supposed to return 1 if the topology changed\n * or 0 if it stayed the same.\n */\nint __attribute__((weak)) arch_update_cpu_topology(void)\n{\n\treturn 0;\n}\n\ncpumask_var_t *alloc_sched_domains(unsigned int ndoms)\n{\n\tint i;\n\tcpumask_var_t *doms;\n\n\tdoms = kmalloc(sizeof(*doms) * ndoms, GFP_KERNEL);\n\tif (!doms)\n\t\treturn NULL;\n\tfor (i = 0; i < ndoms; i++) {\n\t\tif (!alloc_cpumask_var(&doms[i], GFP_KERNEL)) {\n\t\t\tfree_sched_domains(doms, i);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\treturn doms;\n}\n\nvoid free_sched_domains(cpumask_var_t doms[], unsigned int ndoms)\n{\n\tunsigned int i;\n\tfor (i = 0; i < ndoms; i++)\n\t\tfree_cpumask_var(doms[i]);\n\tkfree(doms);\n}\n\n/*\n * Set up scheduler domains and groups. Callers must hold the hotplug lock.\n * For now this just excludes isolated cpus, but could be used to\n * exclude other special cases in the future.\n */\nstatic int init_sched_domains(const struct cpumask *cpu_map)\n{\n\tint err;\n\n\tarch_update_cpu_topology();\n\tndoms_cur = 1;\n\tdoms_cur = alloc_sched_domains(ndoms_cur);\n\tif (!doms_cur)\n\t\tdoms_cur = &fallback_doms;\n\tcpumask_andnot(doms_cur[0], cpu_map, cpu_isolated_map);\n\tdattr_cur = NULL;\n\terr = build_sched_domains(doms_cur[0], NULL);\n\tregister_sched_domain_sysctl();\n\n\treturn err;\n}\n\n/*\n * Detach sched domains from a group of cpus specified in cpu_map\n * These cpus will now be attached to the NULL domain\n */\nstatic void detach_destroy_domains(const struct cpumask *cpu_map)\n{\n\tint i;\n\n\trcu_read_lock();\n\tfor_each_cpu(i, cpu_map)\n\t\tcpu_attach_domain(NULL, &def_root_domain, i);\n\trcu_read_unlock();\n}\n\n/* handle null as \"default\" */\nstatic int dattrs_equal(struct sched_domain_attr *cur, int idx_cur,\n\t\t\tstruct sched_domain_attr *new, int idx_new)\n{\n\tstruct sched_domain_attr tmp;\n\n\t/* fast path */\n\tif (!new && !cur)\n\t\treturn 1;\n\n\ttmp = SD_ATTR_INIT;\n\treturn !memcmp(cur ? (cur + idx_cur) : &tmp,\n\t\t\tnew ? (new + idx_new) : &tmp,\n\t\t\tsizeof(struct sched_domain_attr));\n}\n\n/*\n * Partition sched domains as specified by the 'ndoms_new'\n * cpumasks in the array doms_new[] of cpumasks. This compares\n * doms_new[] to the current sched domain partitioning, doms_cur[].\n * It destroys each deleted domain and builds each new domain.\n *\n * 'doms_new' is an array of cpumask_var_t's of length 'ndoms_new'.\n * The masks don't intersect (don't overlap.) We should setup one\n * sched domain for each mask. CPUs not in any of the cpumasks will\n * not be load balanced. If the same cpumask appears both in the\n * current 'doms_cur' domains and in the new 'doms_new', we can leave\n * it as it is.\n *\n * The passed in 'doms_new' should be allocated using\n * alloc_sched_domains.  This routine takes ownership of it and will\n * free_sched_domains it when done with it. If the caller failed the\n * alloc call, then it can pass in doms_new == NULL && ndoms_new == 1,\n * and partition_sched_domains() will fallback to the single partition\n * 'fallback_doms', it also forces the domains to be rebuilt.\n *\n * If doms_new == NULL it will be replaced with cpu_online_mask.\n * ndoms_new == 0 is a special case for destroying existing domains,\n * and it will not create the default domain.\n *\n * Call with hotplug lock held\n */\nvoid partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],\n\t\t\t     struct sched_domain_attr *dattr_new)\n{\n\tint i, j, n;\n\tint new_topology;\n\n\tmutex_lock(&sched_domains_mutex);\n\n\t/* always unregister in case we don't destroy any domains */\n\tunregister_sched_domain_sysctl();\n\n\t/* Let architecture update cpu core mappings. */\n\tnew_topology = arch_update_cpu_topology();\n\n\tn = doms_new ? ndoms_new : 0;\n\n\t/* Destroy deleted domains */\n\tfor (i = 0; i < ndoms_cur; i++) {\n\t\tfor (j = 0; j < n && !new_topology; j++) {\n\t\t\tif (cpumask_equal(doms_cur[i], doms_new[j])\n\t\t\t    && dattrs_equal(dattr_cur, i, dattr_new, j))\n\t\t\t\tgoto match1;\n\t\t}\n\t\t/* no match - a current sched domain not in new doms_new[] */\n\t\tdetach_destroy_domains(doms_cur[i]);\nmatch1:\n\t\t;\n\t}\n\n\tif (doms_new == NULL) {\n\t\tndoms_cur = 0;\n\t\tdoms_new = &fallback_doms;\n\t\tcpumask_andnot(doms_new[0], cpu_active_mask, cpu_isolated_map);\n\t\tWARN_ON_ONCE(dattr_new);\n\t}\n\n\t/* Build new domains */\n\tfor (i = 0; i < ndoms_new; i++) {\n\t\tfor (j = 0; j < ndoms_cur && !new_topology; j++) {\n\t\t\tif (cpumask_equal(doms_new[i], doms_cur[j])\n\t\t\t    && dattrs_equal(dattr_new, i, dattr_cur, j))\n\t\t\t\tgoto match2;\n\t\t}\n\t\t/* no match - add a new doms_new */\n\t\tbuild_sched_domains(doms_new[i], dattr_new ? dattr_new + i : NULL);\nmatch2:\n\t\t;\n\t}\n\n\t/* Remember the new sched domains */\n\tif (doms_cur != &fallback_doms)\n\t\tfree_sched_domains(doms_cur, ndoms_cur);\n\tkfree(dattr_cur);\t/* kfree(NULL) is safe */\n\tdoms_cur = doms_new;\n\tdattr_cur = dattr_new;\n\tndoms_cur = ndoms_new;\n\n\tregister_sched_domain_sysctl();\n\n\tmutex_unlock(&sched_domains_mutex);\n}\n\n#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)\nstatic void reinit_sched_domains(void)\n{\n\tget_online_cpus();\n\n\t/* Destroy domains first to force the rebuild */\n\tpartition_sched_domains(0, NULL, NULL);\n\n\trebuild_sched_domains();\n\tput_online_cpus();\n}\n\nstatic ssize_t sched_power_savings_store(const char *buf, size_t count, int smt)\n{\n\tunsigned int level = 0;\n\n\tif (sscanf(buf, \"%u\", &level) != 1)\n\t\treturn -EINVAL;\n\n\t/*\n\t * level is always be positive so don't check for\n\t * level < POWERSAVINGS_BALANCE_NONE which is 0\n\t * What happens on 0 or 1 byte write,\n\t * need to check for count as well?\n\t */\n\n\tif (level >= MAX_POWERSAVINGS_BALANCE_LEVELS)\n\t\treturn -EINVAL;\n\n\tif (smt)\n\t\tsched_smt_power_savings = level;\n\telse\n\t\tsched_mc_power_savings = level;\n\n\treinit_sched_domains();\n\n\treturn count;\n}\n\n#ifdef CONFIG_SCHED_MC\nstatic ssize_t sched_mc_power_savings_show(struct sysdev_class *class,\n\t\t\t\t\t   struct sysdev_class_attribute *attr,\n\t\t\t\t\t   char *page)\n{\n\treturn sprintf(page, \"%u\\n\", sched_mc_power_savings);\n}\nstatic ssize_t sched_mc_power_savings_store(struct sysdev_class *class,\n\t\t\t\t\t    struct sysdev_class_attribute *attr,\n\t\t\t\t\t    const char *buf, size_t count)\n{\n\treturn sched_power_savings_store(buf, count, 0);\n}\nstatic SYSDEV_CLASS_ATTR(sched_mc_power_savings, 0644,\n\t\t\t sched_mc_power_savings_show,\n\t\t\t sched_mc_power_savings_store);\n#endif\n\n#ifdef CONFIG_SCHED_SMT\nstatic ssize_t sched_smt_power_savings_show(struct sysdev_class *dev,\n\t\t\t\t\t    struct sysdev_class_attribute *attr,\n\t\t\t\t\t    char *page)\n{\n\treturn sprintf(page, \"%u\\n\", sched_smt_power_savings);\n}\nstatic ssize_t sched_smt_power_savings_store(struct sysdev_class *dev,\n\t\t\t\t\t     struct sysdev_class_attribute *attr,\n\t\t\t\t\t     const char *buf, size_t count)\n{\n\treturn sched_power_savings_store(buf, count, 1);\n}\nstatic SYSDEV_CLASS_ATTR(sched_smt_power_savings, 0644,\n\t\t   sched_smt_power_savings_show,\n\t\t   sched_smt_power_savings_store);\n#endif\n\nint __init sched_create_sysfs_power_savings_entries(struct sysdev_class *cls)\n{\n\tint err = 0;\n\n#ifdef CONFIG_SCHED_SMT\n\tif (smt_capable())\n\t\terr = sysfs_create_file(&cls->kset.kobj,\n\t\t\t\t\t&attr_sched_smt_power_savings.attr);\n#endif\n#ifdef CONFIG_SCHED_MC\n\tif (!err && mc_capable())\n\t\terr = sysfs_create_file(&cls->kset.kobj,\n\t\t\t\t\t&attr_sched_mc_power_savings.attr);\n#endif\n\treturn err;\n}\n#endif /* CONFIG_SCHED_MC || CONFIG_SCHED_SMT */\n\n/*\n * Update cpusets according to cpu_active mask.  If cpusets are\n * disabled, cpuset_update_active_cpus() becomes a simple wrapper\n * around partition_sched_domains().\n */\nstatic int cpuset_cpu_active(struct notifier_block *nfb, unsigned long action,\n\t\t\t     void *hcpu)\n{\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_ONLINE:\n\tcase CPU_DOWN_FAILED:\n\t\tcpuset_update_active_cpus();\n\t\treturn NOTIFY_OK;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nstatic int cpuset_cpu_inactive(struct notifier_block *nfb, unsigned long action,\n\t\t\t       void *hcpu)\n{\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_DOWN_PREPARE:\n\t\tcpuset_update_active_cpus();\n\t\treturn NOTIFY_OK;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nstatic int update_runtime(struct notifier_block *nfb,\n\t\t\t\tunsigned long action, void *hcpu)\n{\n\tint cpu = (int)(long)hcpu;\n\n\tswitch (action) {\n\tcase CPU_DOWN_PREPARE:\n\tcase CPU_DOWN_PREPARE_FROZEN:\n\t\tdisable_runtime(cpu_rq(cpu));\n\t\treturn NOTIFY_OK;\n\n\tcase CPU_DOWN_FAILED:\n\tcase CPU_DOWN_FAILED_FROZEN:\n\tcase CPU_ONLINE:\n\tcase CPU_ONLINE_FROZEN:\n\t\tenable_runtime(cpu_rq(cpu));\n\t\treturn NOTIFY_OK;\n\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nvoid __init sched_init_smp(void)\n{\n\tcpumask_var_t non_isolated_cpus;\n\n\talloc_cpumask_var(&non_isolated_cpus, GFP_KERNEL);\n\talloc_cpumask_var(&fallback_doms, GFP_KERNEL);\n\n\tget_online_cpus();\n\tmutex_lock(&sched_domains_mutex);\n\tinit_sched_domains(cpu_active_mask);\n\tcpumask_andnot(non_isolated_cpus, cpu_possible_mask, cpu_isolated_map);\n\tif (cpumask_empty(non_isolated_cpus))\n\t\tcpumask_set_cpu(smp_processor_id(), non_isolated_cpus);\n\tmutex_unlock(&sched_domains_mutex);\n\tput_online_cpus();\n\n\thotcpu_notifier(cpuset_cpu_active, CPU_PRI_CPUSET_ACTIVE);\n\thotcpu_notifier(cpuset_cpu_inactive, CPU_PRI_CPUSET_INACTIVE);\n\n\t/* RT runtime code needs to handle some hotplug events */\n\thotcpu_notifier(update_runtime, 0);\n\n\tinit_hrtick();\n\n\t/* Move init over to a non-isolated CPU */\n\tif (set_cpus_allowed_ptr(current, non_isolated_cpus) < 0)\n\t\tBUG();\n\tsched_init_granularity();\n\tfree_cpumask_var(non_isolated_cpus);\n\n\tinit_sched_rt_class();\n}\n#else\nvoid __init sched_init_smp(void)\n{\n\tsched_init_granularity();\n}\n#endif /* CONFIG_SMP */\n\nconst_debug unsigned int sysctl_timer_migration = 1;\n\nint in_sched_functions(unsigned long addr)\n{\n\treturn in_lock_functions(addr) ||\n\t\t(addr >= (unsigned long)__sched_text_start\n\t\t&& addr < (unsigned long)__sched_text_end);\n}\n\nstatic void init_cfs_rq(struct cfs_rq *cfs_rq, struct rq *rq)\n{\n\tcfs_rq->tasks_timeline = RB_ROOT;\n\tINIT_LIST_HEAD(&cfs_rq->tasks);\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tcfs_rq->rq = rq;\n\t/* allow initial update_cfs_load() to truncate */\n#ifdef CONFIG_SMP\n\tcfs_rq->load_stamp = 1;\n#endif\n#endif\n\tcfs_rq->min_vruntime = (u64)(-(1LL << 20));\n}\n\nstatic void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq)\n{\n\tstruct rt_prio_array *array;\n\tint i;\n\n\tarray = &rt_rq->active;\n\tfor (i = 0; i < MAX_RT_PRIO; i++) {\n\t\tINIT_LIST_HEAD(array->queue + i);\n\t\t__clear_bit(i, array->bitmap);\n\t}\n\t/* delimiter for bitsearch: */\n\t__set_bit(MAX_RT_PRIO, array->bitmap);\n\n#if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED\n\trt_rq->highest_prio.curr = MAX_RT_PRIO;\n#ifdef CONFIG_SMP\n\trt_rq->highest_prio.next = MAX_RT_PRIO;\n#endif\n#endif\n#ifdef CONFIG_SMP\n\trt_rq->rt_nr_migratory = 0;\n\trt_rq->overloaded = 0;\n\tplist_head_init_raw(&rt_rq->pushable_tasks, &rq->lock);\n#endif\n\n\trt_rq->rt_time = 0;\n\trt_rq->rt_throttled = 0;\n\trt_rq->rt_runtime = 0;\n\traw_spin_lock_init(&rt_rq->rt_runtime_lock);\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\trt_rq->rt_nr_boosted = 0;\n\trt_rq->rq = rq;\n#endif\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq,\n\t\t\t\tstruct sched_entity *se, int cpu,\n\t\t\t\tstruct sched_entity *parent)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\ttg->cfs_rq[cpu] = cfs_rq;\n\tinit_cfs_rq(cfs_rq, rq);\n\tcfs_rq->tg = tg;\n\n\ttg->se[cpu] = se;\n\t/* se could be NULL for root_task_group */\n\tif (!se)\n\t\treturn;\n\n\tif (!parent)\n\t\tse->cfs_rq = &rq->cfs;\n\telse\n\t\tse->cfs_rq = parent->my_q;\n\n\tse->my_q = cfs_rq;\n\tupdate_load_set(&se->load, 0);\n\tse->parent = parent;\n}\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic void init_tg_rt_entry(struct task_group *tg, struct rt_rq *rt_rq,\n\t\tstruct sched_rt_entity *rt_se, int cpu,\n\t\tstruct sched_rt_entity *parent)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\ttg->rt_rq[cpu] = rt_rq;\n\tinit_rt_rq(rt_rq, rq);\n\trt_rq->tg = tg;\n\trt_rq->rt_runtime = tg->rt_bandwidth.rt_runtime;\n\n\ttg->rt_se[cpu] = rt_se;\n\tif (!rt_se)\n\t\treturn;\n\n\tif (!parent)\n\t\trt_se->rt_rq = &rq->rt;\n\telse\n\t\trt_se->rt_rq = parent->my_q;\n\n\trt_se->my_q = rt_rq;\n\trt_se->parent = parent;\n\tINIT_LIST_HEAD(&rt_se->run_list);\n}\n#endif\n\nvoid __init sched_init(void)\n{\n\tint i, j;\n\tunsigned long alloc_size = 0, ptr;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\talloc_size += 2 * nr_cpu_ids * sizeof(void **);\n#endif\n#ifdef CONFIG_RT_GROUP_SCHED\n\talloc_size += 2 * nr_cpu_ids * sizeof(void **);\n#endif\n#ifdef CONFIG_CPUMASK_OFFSTACK\n\talloc_size += num_possible_cpus() * cpumask_size();\n#endif\n\tif (alloc_size) {\n\t\tptr = (unsigned long)kzalloc(alloc_size, GFP_NOWAIT);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t\troot_task_group.se = (struct sched_entity **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n\t\troot_task_group.cfs_rq = (struct cfs_rq **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n#ifdef CONFIG_RT_GROUP_SCHED\n\t\troot_task_group.rt_se = (struct sched_rt_entity **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n\t\troot_task_group.rt_rq = (struct rt_rq **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n#endif /* CONFIG_RT_GROUP_SCHED */\n#ifdef CONFIG_CPUMASK_OFFSTACK\n\t\tfor_each_possible_cpu(i) {\n\t\t\tper_cpu(load_balance_tmpmask, i) = (void *)ptr;\n\t\t\tptr += cpumask_size();\n\t\t}\n#endif /* CONFIG_CPUMASK_OFFSTACK */\n\t}\n\n#ifdef CONFIG_SMP\n\tinit_defrootdomain();\n#endif\n\n\tinit_rt_bandwidth(&def_rt_bandwidth,\n\t\t\tglobal_rt_period(), global_rt_runtime());\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tinit_rt_bandwidth(&root_task_group.rt_bandwidth,\n\t\t\tglobal_rt_period(), global_rt_runtime());\n#endif /* CONFIG_RT_GROUP_SCHED */\n\n#ifdef CONFIG_CGROUP_SCHED\n\tlist_add(&root_task_group.list, &task_groups);\n\tINIT_LIST_HEAD(&root_task_group.children);\n\tautogroup_init(&init_task);\n#endif /* CONFIG_CGROUP_SCHED */\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct rq *rq;\n\n\t\trq = cpu_rq(i);\n\t\traw_spin_lock_init(&rq->lock);\n\t\trq->nr_running = 0;\n\t\trq->calc_load_active = 0;\n\t\trq->calc_load_update = jiffies + LOAD_FREQ;\n\t\tinit_cfs_rq(&rq->cfs, rq);\n\t\tinit_rt_rq(&rq->rt, rq);\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t\troot_task_group.shares = root_task_group_load;\n\t\tINIT_LIST_HEAD(&rq->leaf_cfs_rq_list);\n\t\t/*\n\t\t * How much cpu bandwidth does root_task_group get?\n\t\t *\n\t\t * In case of task-groups formed thr' the cgroup filesystem, it\n\t\t * gets 100% of the cpu resources in the system. This overall\n\t\t * system cpu resource is divided among the tasks of\n\t\t * root_task_group and its child task-groups in a fair manner,\n\t\t * based on each entity's (task or task-group's) weight\n\t\t * (se->load.weight).\n\t\t *\n\t\t * In other words, if root_task_group has 10 tasks of weight\n\t\t * 1024) and two child groups A0 and A1 (of weight 1024 each),\n\t\t * then A0's share of the cpu resource is:\n\t\t *\n\t\t *\tA0's bandwidth = 1024 / (10*1024 + 1024 + 1024) = 8.33%\n\t\t *\n\t\t * We achieve this by letting root_task_group's tasks sit\n\t\t * directly in rq->cfs (i.e root_task_group->se[] = NULL).\n\t\t */\n\t\tinit_tg_cfs_entry(&root_task_group, &rq->cfs, NULL, i, NULL);\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n\t\trq->rt.rt_runtime = def_rt_bandwidth.rt_runtime;\n#ifdef CONFIG_RT_GROUP_SCHED\n\t\tINIT_LIST_HEAD(&rq->leaf_rt_rq_list);\n\t\tinit_tg_rt_entry(&root_task_group, &rq->rt, NULL, i, NULL);\n#endif\n\n\t\tfor (j = 0; j < CPU_LOAD_IDX_MAX; j++)\n\t\t\trq->cpu_load[j] = 0;\n\n\t\trq->last_load_update_tick = jiffies;\n\n#ifdef CONFIG_SMP\n\t\trq->sd = NULL;\n\t\trq->rd = NULL;\n\t\trq->cpu_power = SCHED_POWER_SCALE;\n\t\trq->post_schedule = 0;\n\t\trq->active_balance = 0;\n\t\trq->next_balance = jiffies;\n\t\trq->push_cpu = 0;\n\t\trq->cpu = i;\n\t\trq->online = 0;\n\t\trq->idle_stamp = 0;\n\t\trq->avg_idle = 2*sysctl_sched_migration_cost;\n\t\trq_attach_root(rq, &def_root_domain);\n#ifdef CONFIG_NO_HZ\n\t\trq->nohz_balance_kick = 0;\n\t\tinit_sched_softirq_csd(&per_cpu(remote_sched_softirq_cb, i));\n#endif\n#endif\n\t\tinit_rq_hrtick(rq);\n\t\tatomic_set(&rq->nr_iowait, 0);\n\t}\n\n\tset_load_weight(&init_task);\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\tINIT_HLIST_HEAD(&init_task.preempt_notifiers);\n#endif\n\n#ifdef CONFIG_SMP\n\topen_softirq(SCHED_SOFTIRQ, run_rebalance_domains);\n#endif\n\n#ifdef CONFIG_RT_MUTEXES\n\tplist_head_init_raw(&init_task.pi_waiters, &init_task.pi_lock);\n#endif\n\n\t/*\n\t * The boot idle thread does lazy MMU switching as well:\n\t */\n\tatomic_inc(&init_mm.mm_count);\n\tenter_lazy_tlb(&init_mm, current);\n\n\t/*\n\t * Make us the idle thread. Technically, schedule() should not be\n\t * called from this thread, however somewhere below it might be,\n\t * but because we are the idle thread, we just pick up running again\n\t * when this runqueue becomes \"idle\".\n\t */\n\tinit_idle(current, smp_processor_id());\n\n\tcalc_load_update = jiffies + LOAD_FREQ;\n\n\t/*\n\t * During early bootup we pretend to be a normal task:\n\t */\n\tcurrent->sched_class = &fair_sched_class;\n\n\t/* Allocate the nohz_cpu_mask if CONFIG_CPUMASK_OFFSTACK */\n\tzalloc_cpumask_var(&nohz_cpu_mask, GFP_NOWAIT);\n#ifdef CONFIG_SMP\n\tzalloc_cpumask_var(&sched_domains_tmpmask, GFP_NOWAIT);\n#ifdef CONFIG_NO_HZ\n\tzalloc_cpumask_var(&nohz.idle_cpus_mask, GFP_NOWAIT);\n\talloc_cpumask_var(&nohz.grp_idle_mask, GFP_NOWAIT);\n\tatomic_set(&nohz.load_balancer, nr_cpu_ids);\n\tatomic_set(&nohz.first_pick_cpu, nr_cpu_ids);\n\tatomic_set(&nohz.second_pick_cpu, nr_cpu_ids);\n#endif\n\t/* May be allocated at isolcpus cmdline parse time */\n\tif (cpu_isolated_map == NULL)\n\t\tzalloc_cpumask_var(&cpu_isolated_map, GFP_NOWAIT);\n#endif /* SMP */\n\n\tscheduler_running = 1;\n}\n\n#ifdef CONFIG_DEBUG_SPINLOCK_SLEEP\nstatic inline int preempt_count_equals(int preempt_offset)\n{\n\tint nested = (preempt_count() & ~PREEMPT_ACTIVE) + rcu_preempt_depth();\n\n\treturn (nested == preempt_offset);\n}\n\nvoid __might_sleep(const char *file, int line, int preempt_offset)\n{\n#ifdef in_atomic\n\tstatic unsigned long prev_jiffy;\t/* ratelimiting */\n\n\tif ((preempt_count_equals(preempt_offset) && !irqs_disabled()) ||\n\t    system_state != SYSTEM_RUNNING || oops_in_progress)\n\t\treturn;\n\tif (time_before(jiffies, prev_jiffy + HZ) && prev_jiffy)\n\t\treturn;\n\tprev_jiffy = jiffies;\n\n\tprintk(KERN_ERR\n\t\t\"BUG: sleeping function called from invalid context at %s:%d\\n\",\n\t\t\tfile, line);\n\tprintk(KERN_ERR\n\t\t\"in_atomic(): %d, irqs_disabled(): %d, pid: %d, name: %s\\n\",\n\t\t\tin_atomic(), irqs_disabled(),\n\t\t\tcurrent->pid, current->comm);\n\n\tdebug_show_held_locks(current);\n\tif (irqs_disabled())\n\t\tprint_irqtrace_events(current);\n\tdump_stack();\n#endif\n}\nEXPORT_SYMBOL(__might_sleep);\n#endif\n\n#ifdef CONFIG_MAGIC_SYSRQ\nstatic void normalize_task(struct rq *rq, struct task_struct *p)\n{\n\tconst struct sched_class *prev_class = p->sched_class;\n\tint old_prio = p->prio;\n\tint on_rq;\n\n\ton_rq = p->on_rq;\n\tif (on_rq)\n\t\tdeactivate_task(rq, p, 0);\n\t__setscheduler(rq, p, SCHED_NORMAL, 0);\n\tif (on_rq) {\n\t\tactivate_task(rq, p, 0);\n\t\tresched_task(rq->curr);\n\t}\n\n\tcheck_class_changed(rq, p, prev_class, old_prio);\n}\n\nvoid normalize_rt_tasks(void)\n{\n\tstruct task_struct *g, *p;\n\tunsigned long flags;\n\tstruct rq *rq;\n\n\tread_lock_irqsave(&tasklist_lock, flags);\n\tdo_each_thread(g, p) {\n\t\t/*\n\t\t * Only normalize user tasks:\n\t\t */\n\t\tif (!p->mm)\n\t\t\tcontinue;\n\n\t\tp->se.exec_start\t\t= 0;\n#ifdef CONFIG_SCHEDSTATS\n\t\tp->se.statistics.wait_start\t= 0;\n\t\tp->se.statistics.sleep_start\t= 0;\n\t\tp->se.statistics.block_start\t= 0;\n#endif\n\n\t\tif (!rt_task(p)) {\n\t\t\t/*\n\t\t\t * Renice negative nice level userspace\n\t\t\t * tasks back to 0:\n\t\t\t */\n\t\t\tif (TASK_NICE(p) < 0 && p->mm)\n\t\t\t\tset_user_nice(p, 0);\n\t\t\tcontinue;\n\t\t}\n\n\t\traw_spin_lock(&p->pi_lock);\n\t\trq = __task_rq_lock(p);\n\n\t\tnormalize_task(rq, p);\n\n\t\t__task_rq_unlock(rq);\n\t\traw_spin_unlock(&p->pi_lock);\n\t} while_each_thread(g, p);\n\n\tread_unlock_irqrestore(&tasklist_lock, flags);\n}\n\n#endif /* CONFIG_MAGIC_SYSRQ */\n\n#if defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB)\n/*\n * These functions are only useful for the IA64 MCA handling, or kdb.\n *\n * They can only be called when the whole system has been\n * stopped - every CPU needs to be quiescent, and no scheduling\n * activity can take place. Using them for anything else would\n * be a serious bug, and as a result, they aren't even visible\n * under any other configuration.\n */\n\n/**\n * curr_task - return the current task for a given cpu.\n * @cpu: the processor in question.\n *\n * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!\n */\nstruct task_struct *curr_task(int cpu)\n{\n\treturn cpu_curr(cpu);\n}\n\n#endif /* defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB) */\n\n#ifdef CONFIG_IA64\n/**\n * set_curr_task - set the current task for a given cpu.\n * @cpu: the processor in question.\n * @p: the task pointer to set.\n *\n * Description: This function must only be used when non-maskable interrupts\n * are serviced on a separate stack. It allows the architecture to switch the\n * notion of the current task on a cpu in a non-blocking manner. This function\n * must be called with all CPU's synchronized, and interrupts disabled, the\n * and caller must save the original value of the current task (see\n * curr_task() above) and restore that value before reenabling interrupts and\n * re-starting the system.\n *\n * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!\n */\nvoid set_curr_task(int cpu, struct task_struct *p)\n{\n\tcpu_curr(cpu) = p;\n}\n\n#endif\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void free_fair_sched_group(struct task_group *tg)\n{\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tif (tg->cfs_rq)\n\t\t\tkfree(tg->cfs_rq[i]);\n\t\tif (tg->se)\n\t\t\tkfree(tg->se[i]);\n\t}\n\n\tkfree(tg->cfs_rq);\n\tkfree(tg->se);\n}\n\nstatic\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se;\n\tint i;\n\n\ttg->cfs_rq = kzalloc(sizeof(cfs_rq) * nr_cpu_ids, GFP_KERNEL);\n\tif (!tg->cfs_rq)\n\t\tgoto err;\n\ttg->se = kzalloc(sizeof(se) * nr_cpu_ids, GFP_KERNEL);\n\tif (!tg->se)\n\t\tgoto err;\n\n\ttg->shares = NICE_0_LOAD;\n\n\tfor_each_possible_cpu(i) {\n\t\tcfs_rq = kzalloc_node(sizeof(struct cfs_rq),\n\t\t\t\t      GFP_KERNEL, cpu_to_node(i));\n\t\tif (!cfs_rq)\n\t\t\tgoto err;\n\n\t\tse = kzalloc_node(sizeof(struct sched_entity),\n\t\t\t\t  GFP_KERNEL, cpu_to_node(i));\n\t\tif (!se)\n\t\t\tgoto err_free_rq;\n\n\t\tinit_tg_cfs_entry(tg, cfs_rq, se, i, parent->se[i]);\n\t}\n\n\treturn 1;\n\nerr_free_rq:\n\tkfree(cfs_rq);\nerr:\n\treturn 0;\n}\n\nstatic inline void unregister_fair_sched_group(struct task_group *tg, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long flags;\n\n\t/*\n\t* Only empty task groups can be destroyed; so we can speculatively\n\t* check on_list without danger of it being re-added.\n\t*/\n\tif (!tg->cfs_rq[cpu]->on_list)\n\t\treturn;\n\n\traw_spin_lock_irqsave(&rq->lock, flags);\n\tlist_del_leaf_cfs_rq(tg->cfs_rq[cpu]);\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n}\n#else /* !CONFG_FAIR_GROUP_SCHED */\nstatic inline void free_fair_sched_group(struct task_group *tg)\n{\n}\n\nstatic inline\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\treturn 1;\n}\n\nstatic inline void unregister_fair_sched_group(struct task_group *tg, int cpu)\n{\n}\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic void free_rt_sched_group(struct task_group *tg)\n{\n\tint i;\n\n\tdestroy_rt_bandwidth(&tg->rt_bandwidth);\n\n\tfor_each_possible_cpu(i) {\n\t\tif (tg->rt_rq)\n\t\t\tkfree(tg->rt_rq[i]);\n\t\tif (tg->rt_se)\n\t\t\tkfree(tg->rt_se[i]);\n\t}\n\n\tkfree(tg->rt_rq);\n\tkfree(tg->rt_se);\n}\n\nstatic\nint alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\tstruct rt_rq *rt_rq;\n\tstruct sched_rt_entity *rt_se;\n\tint i;\n\n\ttg->rt_rq = kzalloc(sizeof(rt_rq) * nr_cpu_ids, GFP_KERNEL);\n\tif (!tg->rt_rq)\n\t\tgoto err;\n\ttg->rt_se = kzalloc(sizeof(rt_se) * nr_cpu_ids, GFP_KERNEL);\n\tif (!tg->rt_se)\n\t\tgoto err;\n\n\tinit_rt_bandwidth(&tg->rt_bandwidth,\n\t\t\tktime_to_ns(def_rt_bandwidth.rt_period), 0);\n\n\tfor_each_possible_cpu(i) {\n\t\trt_rq = kzalloc_node(sizeof(struct rt_rq),\n\t\t\t\t     GFP_KERNEL, cpu_to_node(i));\n\t\tif (!rt_rq)\n\t\t\tgoto err;\n\n\t\trt_se = kzalloc_node(sizeof(struct sched_rt_entity),\n\t\t\t\t     GFP_KERNEL, cpu_to_node(i));\n\t\tif (!rt_se)\n\t\t\tgoto err_free_rq;\n\n\t\tinit_tg_rt_entry(tg, rt_rq, rt_se, i, parent->rt_se[i]);\n\t}\n\n\treturn 1;\n\nerr_free_rq:\n\tkfree(rt_rq);\nerr:\n\treturn 0;\n}\n#else /* !CONFIG_RT_GROUP_SCHED */\nstatic inline void free_rt_sched_group(struct task_group *tg)\n{\n}\n\nstatic inline\nint alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\treturn 1;\n}\n#endif /* CONFIG_RT_GROUP_SCHED */\n\n#ifdef CONFIG_CGROUP_SCHED\nstatic void free_sched_group(struct task_group *tg)\n{\n\tfree_fair_sched_group(tg);\n\tfree_rt_sched_group(tg);\n\tautogroup_free(tg);\n\tkfree(tg);\n}\n\n/* allocate runqueue etc for a new task group */\nstruct task_group *sched_create_group(struct task_group *parent)\n{\n\tstruct task_group *tg;\n\tunsigned long flags;\n\n\ttg = kzalloc(sizeof(*tg), GFP_KERNEL);\n\tif (!tg)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (!alloc_fair_sched_group(tg, parent))\n\t\tgoto err;\n\n\tif (!alloc_rt_sched_group(tg, parent))\n\t\tgoto err;\n\n\tspin_lock_irqsave(&task_group_lock, flags);\n\tlist_add_rcu(&tg->list, &task_groups);\n\n\tWARN_ON(!parent); /* root should already exist */\n\n\ttg->parent = parent;\n\tINIT_LIST_HEAD(&tg->children);\n\tlist_add_rcu(&tg->siblings, &parent->children);\n\tspin_unlock_irqrestore(&task_group_lock, flags);\n\n\treturn tg;\n\nerr:\n\tfree_sched_group(tg);\n\treturn ERR_PTR(-ENOMEM);\n}\n\n/* rcu callback to free various structures associated with a task group */\nstatic void free_sched_group_rcu(struct rcu_head *rhp)\n{\n\t/* now it should be safe to free those cfs_rqs */\n\tfree_sched_group(container_of(rhp, struct task_group, rcu));\n}\n\n/* Destroy runqueue etc associated with a task group */\nvoid sched_destroy_group(struct task_group *tg)\n{\n\tunsigned long flags;\n\tint i;\n\n\t/* end participation in shares distribution */\n\tfor_each_possible_cpu(i)\n\t\tunregister_fair_sched_group(tg, i);\n\n\tspin_lock_irqsave(&task_group_lock, flags);\n\tlist_del_rcu(&tg->list);\n\tlist_del_rcu(&tg->siblings);\n\tspin_unlock_irqrestore(&task_group_lock, flags);\n\n\t/* wait for possible concurrent references to cfs_rqs complete */\n\tcall_rcu(&tg->rcu, free_sched_group_rcu);\n}\n\n/* change task's runqueue when it moves between groups.\n *\tThe caller of this function should have put the task in its new group\n *\tby now. This function just updates tsk->se.cfs_rq and tsk->se.parent to\n *\treflect its new group.\n */\nvoid sched_move_task(struct task_struct *tsk)\n{\n\tint on_rq, running;\n\tunsigned long flags;\n\tstruct rq *rq;\n\n\trq = task_rq_lock(tsk, &flags);\n\n\trunning = task_current(rq, tsk);\n\ton_rq = tsk->on_rq;\n\n\tif (on_rq)\n\t\tdequeue_task(rq, tsk, 0);\n\tif (unlikely(running))\n\t\ttsk->sched_class->put_prev_task(rq, tsk);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tif (tsk->sched_class->task_move_group)\n\t\ttsk->sched_class->task_move_group(tsk, on_rq);\n\telse\n#endif\n\t\tset_task_rq(tsk, task_cpu(tsk));\n\n\tif (unlikely(running))\n\t\ttsk->sched_class->set_curr_task(rq);\n\tif (on_rq)\n\t\tenqueue_task(rq, tsk, 0);\n\n\ttask_rq_unlock(rq, tsk, &flags);\n}\n#endif /* CONFIG_CGROUP_SCHED */\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic DEFINE_MUTEX(shares_mutex);\n\nint sched_group_set_shares(struct task_group *tg, unsigned long shares)\n{\n\tint i;\n\tunsigned long flags;\n\n\t/*\n\t * We can't change the weight of the root cgroup.\n\t */\n\tif (!tg->se[0])\n\t\treturn -EINVAL;\n\n\tif (shares < MIN_SHARES)\n\t\tshares = MIN_SHARES;\n\telse if (shares > MAX_SHARES)\n\t\tshares = MAX_SHARES;\n\n\tmutex_lock(&shares_mutex);\n\tif (tg->shares == shares)\n\t\tgoto done;\n\n\ttg->shares = shares;\n\tfor_each_possible_cpu(i) {\n\t\tstruct rq *rq = cpu_rq(i);\n\t\tstruct sched_entity *se;\n\n\t\tse = tg->se[i];\n\t\t/* Propagate contribution to hierarchy */\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tfor_each_sched_entity(se)\n\t\t\tupdate_cfs_shares(group_cfs_rq(se));\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\t}\n\ndone:\n\tmutex_unlock(&shares_mutex);\n\treturn 0;\n}\n\nunsigned long sched_group_shares(struct task_group *tg)\n{\n\treturn tg->shares;\n}\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\n/*\n * Ensure that the real time constraints are schedulable.\n */\nstatic DEFINE_MUTEX(rt_constraints_mutex);\n\nstatic unsigned long to_ratio(u64 period, u64 runtime)\n{\n\tif (runtime == RUNTIME_INF)\n\t\treturn 1ULL << 20;\n\n\treturn div64_u64(runtime << 20, period);\n}\n\n/* Must be called with tasklist_lock held */\nstatic inline int tg_has_rt_tasks(struct task_group *tg)\n{\n\tstruct task_struct *g, *p;\n\n\tdo_each_thread(g, p) {\n\t\tif (rt_task(p) && rt_rq_of_se(&p->rt)->tg == tg)\n\t\t\treturn 1;\n\t} while_each_thread(g, p);\n\n\treturn 0;\n}\n\nstruct rt_schedulable_data {\n\tstruct task_group *tg;\n\tu64 rt_period;\n\tu64 rt_runtime;\n};\n\nstatic int tg_schedulable(struct task_group *tg, void *data)\n{\n\tstruct rt_schedulable_data *d = data;\n\tstruct task_group *child;\n\tunsigned long total, sum = 0;\n\tu64 period, runtime;\n\n\tperiod = ktime_to_ns(tg->rt_bandwidth.rt_period);\n\truntime = tg->rt_bandwidth.rt_runtime;\n\n\tif (tg == d->tg) {\n\t\tperiod = d->rt_period;\n\t\truntime = d->rt_runtime;\n\t}\n\n\t/*\n\t * Cannot have more runtime than the period.\n\t */\n\tif (runtime > period && runtime != RUNTIME_INF)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Ensure we don't starve existing RT tasks.\n\t */\n\tif (rt_bandwidth_enabled() && !runtime && tg_has_rt_tasks(tg))\n\t\treturn -EBUSY;\n\n\ttotal = to_ratio(period, runtime);\n\n\t/*\n\t * Nobody can have more than the global setting allows.\n\t */\n\tif (total > to_ratio(global_rt_period(), global_rt_runtime()))\n\t\treturn -EINVAL;\n\n\t/*\n\t * The sum of our children's runtime should not exceed our own.\n\t */\n\tlist_for_each_entry_rcu(child, &tg->children, siblings) {\n\t\tperiod = ktime_to_ns(child->rt_bandwidth.rt_period);\n\t\truntime = child->rt_bandwidth.rt_runtime;\n\n\t\tif (child == d->tg) {\n\t\t\tperiod = d->rt_period;\n\t\t\truntime = d->rt_runtime;\n\t\t}\n\n\t\tsum += to_ratio(period, runtime);\n\t}\n\n\tif (sum > total)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int __rt_schedulable(struct task_group *tg, u64 period, u64 runtime)\n{\n\tstruct rt_schedulable_data data = {\n\t\t.tg = tg,\n\t\t.rt_period = period,\n\t\t.rt_runtime = runtime,\n\t};\n\n\treturn walk_tg_tree(tg_schedulable, tg_nop, &data);\n}\n\nstatic int tg_set_bandwidth(struct task_group *tg,\n\t\tu64 rt_period, u64 rt_runtime)\n{\n\tint i, err = 0;\n\n\tmutex_lock(&rt_constraints_mutex);\n\tread_lock(&tasklist_lock);\n\terr = __rt_schedulable(tg, rt_period, rt_runtime);\n\tif (err)\n\t\tgoto unlock;\n\n\traw_spin_lock_irq(&tg->rt_bandwidth.rt_runtime_lock);\n\ttg->rt_bandwidth.rt_period = ns_to_ktime(rt_period);\n\ttg->rt_bandwidth.rt_runtime = rt_runtime;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct rt_rq *rt_rq = tg->rt_rq[i];\n\n\t\traw_spin_lock(&rt_rq->rt_runtime_lock);\n\t\trt_rq->rt_runtime = rt_runtime;\n\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);\n\t}\n\traw_spin_unlock_irq(&tg->rt_bandwidth.rt_runtime_lock);\nunlock:\n\tread_unlock(&tasklist_lock);\n\tmutex_unlock(&rt_constraints_mutex);\n\n\treturn err;\n}\n\nint sched_group_set_rt_runtime(struct task_group *tg, long rt_runtime_us)\n{\n\tu64 rt_runtime, rt_period;\n\n\trt_period = ktime_to_ns(tg->rt_bandwidth.rt_period);\n\trt_runtime = (u64)rt_runtime_us * NSEC_PER_USEC;\n\tif (rt_runtime_us < 0)\n\t\trt_runtime = RUNTIME_INF;\n\n\treturn tg_set_bandwidth(tg, rt_period, rt_runtime);\n}\n\nlong sched_group_rt_runtime(struct task_group *tg)\n{\n\tu64 rt_runtime_us;\n\n\tif (tg->rt_bandwidth.rt_runtime == RUNTIME_INF)\n\t\treturn -1;\n\n\trt_runtime_us = tg->rt_bandwidth.rt_runtime;\n\tdo_div(rt_runtime_us, NSEC_PER_USEC);\n\treturn rt_runtime_us;\n}\n\nint sched_group_set_rt_period(struct task_group *tg, long rt_period_us)\n{\n\tu64 rt_runtime, rt_period;\n\n\trt_period = (u64)rt_period_us * NSEC_PER_USEC;\n\trt_runtime = tg->rt_bandwidth.rt_runtime;\n\n\tif (rt_period == 0)\n\t\treturn -EINVAL;\n\n\treturn tg_set_bandwidth(tg, rt_period, rt_runtime);\n}\n\nlong sched_group_rt_period(struct task_group *tg)\n{\n\tu64 rt_period_us;\n\n\trt_period_us = ktime_to_ns(tg->rt_bandwidth.rt_period);\n\tdo_div(rt_period_us, NSEC_PER_USEC);\n\treturn rt_period_us;\n}\n\nstatic int sched_rt_global_constraints(void)\n{\n\tu64 runtime, period;\n\tint ret = 0;\n\n\tif (sysctl_sched_rt_period <= 0)\n\t\treturn -EINVAL;\n\n\truntime = global_rt_runtime();\n\tperiod = global_rt_period();\n\n\t/*\n\t * Sanity check on the sysctl variables.\n\t */\n\tif (runtime > period && runtime != RUNTIME_INF)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&rt_constraints_mutex);\n\tread_lock(&tasklist_lock);\n\tret = __rt_schedulable(NULL, 0, 0);\n\tread_unlock(&tasklist_lock);\n\tmutex_unlock(&rt_constraints_mutex);\n\n\treturn ret;\n}\n\nint sched_rt_can_attach(struct task_group *tg, struct task_struct *tsk)\n{\n\t/* Don't accept realtime tasks when there is no way for them to run */\n\tif (rt_task(tsk) && tg->rt_bandwidth.rt_runtime == 0)\n\t\treturn 0;\n\n\treturn 1;\n}\n\n#else /* !CONFIG_RT_GROUP_SCHED */\nstatic int sched_rt_global_constraints(void)\n{\n\tunsigned long flags;\n\tint i;\n\n\tif (sysctl_sched_rt_period <= 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * There's always some RT tasks in the root group\n\t * -- migration, kstopmachine etc..\n\t */\n\tif (sysctl_sched_rt_runtime == 0)\n\t\treturn -EBUSY;\n\n\traw_spin_lock_irqsave(&def_rt_bandwidth.rt_runtime_lock, flags);\n\tfor_each_possible_cpu(i) {\n\t\tstruct rt_rq *rt_rq = &cpu_rq(i)->rt;\n\n\t\traw_spin_lock(&rt_rq->rt_runtime_lock);\n\t\trt_rq->rt_runtime = global_rt_runtime();\n\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);\n\t}\n\traw_spin_unlock_irqrestore(&def_rt_bandwidth.rt_runtime_lock, flags);\n\n\treturn 0;\n}\n#endif /* CONFIG_RT_GROUP_SCHED */\n\nint sched_rt_handler(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *lenp,\n\t\tloff_t *ppos)\n{\n\tint ret;\n\tint old_period, old_runtime;\n\tstatic DEFINE_MUTEX(mutex);\n\n\tmutex_lock(&mutex);\n\told_period = sysctl_sched_rt_period;\n\told_runtime = sysctl_sched_rt_runtime;\n\n\tret = proc_dointvec(table, write, buffer, lenp, ppos);\n\n\tif (!ret && write) {\n\t\tret = sched_rt_global_constraints();\n\t\tif (ret) {\n\t\t\tsysctl_sched_rt_period = old_period;\n\t\t\tsysctl_sched_rt_runtime = old_runtime;\n\t\t} else {\n\t\t\tdef_rt_bandwidth.rt_runtime = global_rt_runtime();\n\t\t\tdef_rt_bandwidth.rt_period =\n\t\t\t\tns_to_ktime(global_rt_period());\n\t\t}\n\t}\n\tmutex_unlock(&mutex);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_CGROUP_SCHED\n\n/* return corresponding task_group object of a cgroup */\nstatic inline struct task_group *cgroup_tg(struct cgroup *cgrp)\n{\n\treturn container_of(cgroup_subsys_state(cgrp, cpu_cgroup_subsys_id),\n\t\t\t    struct task_group, css);\n}\n\nstatic struct cgroup_subsys_state *\ncpu_cgroup_create(struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tstruct task_group *tg, *parent;\n\n\tif (!cgrp->parent) {\n\t\t/* This is early initialization for the top cgroup */\n\t\treturn &root_task_group.css;\n\t}\n\n\tparent = cgroup_tg(cgrp->parent);\n\ttg = sched_create_group(parent);\n\tif (IS_ERR(tg))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\treturn &tg->css;\n}\n\nstatic void\ncpu_cgroup_destroy(struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tstruct task_group *tg = cgroup_tg(cgrp);\n\n\tsched_destroy_group(tg);\n}\n\nstatic int\ncpu_cgroup_can_attach_task(struct cgroup *cgrp, struct task_struct *tsk)\n{\n#ifdef CONFIG_RT_GROUP_SCHED\n\tif (!sched_rt_can_attach(cgroup_tg(cgrp), tsk))\n\t\treturn -EINVAL;\n#else\n\t/* We don't support RT-tasks being in separate groups */\n\tif (tsk->sched_class != &fair_sched_class)\n\t\treturn -EINVAL;\n#endif\n\treturn 0;\n}\n\nstatic void\ncpu_cgroup_attach_task(struct cgroup *cgrp, struct task_struct *tsk)\n{\n\tsched_move_task(tsk);\n}\n\nstatic void\ncpu_cgroup_exit(struct cgroup_subsys *ss, struct cgroup *cgrp,\n\t\tstruct cgroup *old_cgrp, struct task_struct *task)\n{\n\t/*\n\t * cgroup_exit() is called in the copy_process() failure path.\n\t * Ignore this case since the task hasn't ran yet, this avoids\n\t * trying to poke a half freed task state from generic code.\n\t */\n\tif (!(task->flags & PF_EXITING))\n\t\treturn;\n\n\tsched_move_task(task);\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic int cpu_shares_write_u64(struct cgroup *cgrp, struct cftype *cftype,\n\t\t\t\tu64 shareval)\n{\n\treturn sched_group_set_shares(cgroup_tg(cgrp), scale_load(shareval));\n}\n\nstatic u64 cpu_shares_read_u64(struct cgroup *cgrp, struct cftype *cft)\n{\n\tstruct task_group *tg = cgroup_tg(cgrp);\n\n\treturn (u64) scale_load_down(tg->shares);\n}\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic int cpu_rt_runtime_write(struct cgroup *cgrp, struct cftype *cft,\n\t\t\t\ts64 val)\n{\n\treturn sched_group_set_rt_runtime(cgroup_tg(cgrp), val);\n}\n\nstatic s64 cpu_rt_runtime_read(struct cgroup *cgrp, struct cftype *cft)\n{\n\treturn sched_group_rt_runtime(cgroup_tg(cgrp));\n}\n\nstatic int cpu_rt_period_write_uint(struct cgroup *cgrp, struct cftype *cftype,\n\t\tu64 rt_period_us)\n{\n\treturn sched_group_set_rt_period(cgroup_tg(cgrp), rt_period_us);\n}\n\nstatic u64 cpu_rt_period_read_uint(struct cgroup *cgrp, struct cftype *cft)\n{\n\treturn sched_group_rt_period(cgroup_tg(cgrp));\n}\n#endif /* CONFIG_RT_GROUP_SCHED */\n\nstatic struct cftype cpu_files[] = {\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t{\n\t\t.name = \"shares\",\n\t\t.read_u64 = cpu_shares_read_u64,\n\t\t.write_u64 = cpu_shares_write_u64,\n\t},\n#endif\n#ifdef CONFIG_RT_GROUP_SCHED\n\t{\n\t\t.name = \"rt_runtime_us\",\n\t\t.read_s64 = cpu_rt_runtime_read,\n\t\t.write_s64 = cpu_rt_runtime_write,\n\t},\n\t{\n\t\t.name = \"rt_period_us\",\n\t\t.read_u64 = cpu_rt_period_read_uint,\n\t\t.write_u64 = cpu_rt_period_write_uint,\n\t},\n#endif\n};\n\nstatic int cpu_cgroup_populate(struct cgroup_subsys *ss, struct cgroup *cont)\n{\n\treturn cgroup_add_files(cont, ss, cpu_files, ARRAY_SIZE(cpu_files));\n}\n\nstruct cgroup_subsys cpu_cgroup_subsys = {\n\t.name\t\t= \"cpu\",\n\t.create\t\t= cpu_cgroup_create,\n\t.destroy\t= cpu_cgroup_destroy,\n\t.can_attach_task = cpu_cgroup_can_attach_task,\n\t.attach_task\t= cpu_cgroup_attach_task,\n\t.exit\t\t= cpu_cgroup_exit,\n\t.populate\t= cpu_cgroup_populate,\n\t.subsys_id\t= cpu_cgroup_subsys_id,\n\t.early_init\t= 1,\n};\n\n#endif\t/* CONFIG_CGROUP_SCHED */\n\n#ifdef CONFIG_CGROUP_CPUACCT\n\n/*\n * CPU accounting code for task groups.\n *\n * Based on the work by Paul Menage (menage@google.com) and Balbir Singh\n * (balbir@in.ibm.com).\n */\n\n/* track cpu usage of a group of tasks and its child groups */\nstruct cpuacct {\n\tstruct cgroup_subsys_state css;\n\t/* cpuusage holds pointer to a u64-type object on every cpu */\n\tu64 __percpu *cpuusage;\n\tstruct percpu_counter cpustat[CPUACCT_STAT_NSTATS];\n\tstruct cpuacct *parent;\n};\n\nstruct cgroup_subsys cpuacct_subsys;\n\n/* return cpu accounting group corresponding to this container */\nstatic inline struct cpuacct *cgroup_ca(struct cgroup *cgrp)\n{\n\treturn container_of(cgroup_subsys_state(cgrp, cpuacct_subsys_id),\n\t\t\t    struct cpuacct, css);\n}\n\n/* return cpu accounting group to which this task belongs */\nstatic inline struct cpuacct *task_ca(struct task_struct *tsk)\n{\n\treturn container_of(task_subsys_state(tsk, cpuacct_subsys_id),\n\t\t\t    struct cpuacct, css);\n}\n\n/* create a new cpu accounting group */\nstatic struct cgroup_subsys_state *cpuacct_create(\n\tstruct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tstruct cpuacct *ca = kzalloc(sizeof(*ca), GFP_KERNEL);\n\tint i;\n\n\tif (!ca)\n\t\tgoto out;\n\n\tca->cpuusage = alloc_percpu(u64);\n\tif (!ca->cpuusage)\n\t\tgoto out_free_ca;\n\n\tfor (i = 0; i < CPUACCT_STAT_NSTATS; i++)\n\t\tif (percpu_counter_init(&ca->cpustat[i], 0))\n\t\t\tgoto out_free_counters;\n\n\tif (cgrp->parent)\n\t\tca->parent = cgroup_ca(cgrp->parent);\n\n\treturn &ca->css;\n\nout_free_counters:\n\twhile (--i >= 0)\n\t\tpercpu_counter_destroy(&ca->cpustat[i]);\n\tfree_percpu(ca->cpuusage);\nout_free_ca:\n\tkfree(ca);\nout:\n\treturn ERR_PTR(-ENOMEM);\n}\n\n/* destroy an existing cpu accounting group */\nstatic void\ncpuacct_destroy(struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgrp);\n\tint i;\n\n\tfor (i = 0; i < CPUACCT_STAT_NSTATS; i++)\n\t\tpercpu_counter_destroy(&ca->cpustat[i]);\n\tfree_percpu(ca->cpuusage);\n\tkfree(ca);\n}\n\nstatic u64 cpuacct_cpuusage_read(struct cpuacct *ca, int cpu)\n{\n\tu64 *cpuusage = per_cpu_ptr(ca->cpuusage, cpu);\n\tu64 data;\n\n#ifndef CONFIG_64BIT\n\t/*\n\t * Take rq->lock to make 64-bit read safe on 32-bit platforms.\n\t */\n\traw_spin_lock_irq(&cpu_rq(cpu)->lock);\n\tdata = *cpuusage;\n\traw_spin_unlock_irq(&cpu_rq(cpu)->lock);\n#else\n\tdata = *cpuusage;\n#endif\n\n\treturn data;\n}\n\nstatic void cpuacct_cpuusage_write(struct cpuacct *ca, int cpu, u64 val)\n{\n\tu64 *cpuusage = per_cpu_ptr(ca->cpuusage, cpu);\n\n#ifndef CONFIG_64BIT\n\t/*\n\t * Take rq->lock to make 64-bit write safe on 32-bit platforms.\n\t */\n\traw_spin_lock_irq(&cpu_rq(cpu)->lock);\n\t*cpuusage = val;\n\traw_spin_unlock_irq(&cpu_rq(cpu)->lock);\n#else\n\t*cpuusage = val;\n#endif\n}\n\n/* return total cpu usage (in nanoseconds) of a group */\nstatic u64 cpuusage_read(struct cgroup *cgrp, struct cftype *cft)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgrp);\n\tu64 totalcpuusage = 0;\n\tint i;\n\n\tfor_each_present_cpu(i)\n\t\ttotalcpuusage += cpuacct_cpuusage_read(ca, i);\n\n\treturn totalcpuusage;\n}\n\nstatic int cpuusage_write(struct cgroup *cgrp, struct cftype *cftype,\n\t\t\t\t\t\t\t\tu64 reset)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgrp);\n\tint err = 0;\n\tint i;\n\n\tif (reset) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tfor_each_present_cpu(i)\n\t\tcpuacct_cpuusage_write(ca, i, 0);\n\nout:\n\treturn err;\n}\n\nstatic int cpuacct_percpu_seq_read(struct cgroup *cgroup, struct cftype *cft,\n\t\t\t\t   struct seq_file *m)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgroup);\n\tu64 percpu;\n\tint i;\n\n\tfor_each_present_cpu(i) {\n\t\tpercpu = cpuacct_cpuusage_read(ca, i);\n\t\tseq_printf(m, \"%llu \", (unsigned long long) percpu);\n\t}\n\tseq_printf(m, \"\\n\");\n\treturn 0;\n}\n\nstatic const char *cpuacct_stat_desc[] = {\n\t[CPUACCT_STAT_USER] = \"user\",\n\t[CPUACCT_STAT_SYSTEM] = \"system\",\n};\n\nstatic int cpuacct_stats_show(struct cgroup *cgrp, struct cftype *cft,\n\t\tstruct cgroup_map_cb *cb)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgrp);\n\tint i;\n\n\tfor (i = 0; i < CPUACCT_STAT_NSTATS; i++) {\n\t\ts64 val = percpu_counter_read(&ca->cpustat[i]);\n\t\tval = cputime64_to_clock_t(val);\n\t\tcb->fill(cb, cpuacct_stat_desc[i], val);\n\t}\n\treturn 0;\n}\n\nstatic struct cftype files[] = {\n\t{\n\t\t.name = \"usage\",\n\t\t.read_u64 = cpuusage_read,\n\t\t.write_u64 = cpuusage_write,\n\t},\n\t{\n\t\t.name = \"usage_percpu\",\n\t\t.read_seq_string = cpuacct_percpu_seq_read,\n\t},\n\t{\n\t\t.name = \"stat\",\n\t\t.read_map = cpuacct_stats_show,\n\t},\n};\n\nstatic int cpuacct_populate(struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\treturn cgroup_add_files(cgrp, ss, files, ARRAY_SIZE(files));\n}\n\n/*\n * charge this task's execution time to its accounting group.\n *\n * called with rq->lock held.\n */\nstatic void cpuacct_charge(struct task_struct *tsk, u64 cputime)\n{\n\tstruct cpuacct *ca;\n\tint cpu;\n\n\tif (unlikely(!cpuacct_subsys.active))\n\t\treturn;\n\n\tcpu = task_cpu(tsk);\n\n\trcu_read_lock();\n\n\tca = task_ca(tsk);\n\n\tfor (; ca; ca = ca->parent) {\n\t\tu64 *cpuusage = per_cpu_ptr(ca->cpuusage, cpu);\n\t\t*cpuusage += cputime;\n\t}\n\n\trcu_read_unlock();\n}\n\n/*\n * When CONFIG_VIRT_CPU_ACCOUNTING is enabled one jiffy can be very large\n * in cputime_t units. As a result, cpuacct_update_stats calls\n * percpu_counter_add with values large enough to always overflow the\n * per cpu batch limit causing bad SMP scalability.\n *\n * To fix this we scale percpu_counter_batch by cputime_one_jiffy so we\n * batch the same amount of time with CONFIG_VIRT_CPU_ACCOUNTING disabled\n * and enabled. We cap it at INT_MAX which is the largest allowed batch value.\n */\n#ifdef CONFIG_SMP\n#define CPUACCT_BATCH\t\\\n\tmin_t(long, percpu_counter_batch * cputime_one_jiffy, INT_MAX)\n#else\n#define CPUACCT_BATCH\t0\n#endif\n\n/*\n * Charge the system/user time to the task's accounting group.\n */\nstatic void cpuacct_update_stats(struct task_struct *tsk,\n\t\tenum cpuacct_stat_index idx, cputime_t val)\n{\n\tstruct cpuacct *ca;\n\tint batch = CPUACCT_BATCH;\n\n\tif (unlikely(!cpuacct_subsys.active))\n\t\treturn;\n\n\trcu_read_lock();\n\tca = task_ca(tsk);\n\n\tdo {\n\t\t__percpu_counter_add(&ca->cpustat[idx], val, batch);\n\t\tca = ca->parent;\n\t} while (ca);\n\trcu_read_unlock();\n}\n\nstruct cgroup_subsys cpuacct_subsys = {\n\t.name = \"cpuacct\",\n\t.create = cpuacct_create,\n\t.destroy = cpuacct_destroy,\n\t.populate = cpuacct_populate,\n\t.subsys_id = cpuacct_subsys_id,\n};\n#endif\t/* CONFIG_CGROUP_CPUACCT */\n\n", "/*\n * Detect hard and soft lockups on a system\n *\n * started by Don Zickus, Copyright (C) 2010 Red Hat, Inc.\n *\n * this code detects hard lockups: incidents in where on a CPU\n * the kernel does not respond to anything except NMI.\n *\n * Note: Most of this code is borrowed heavily from softlockup.c,\n * so thanks to Ingo for the initial implementation.\n * Some chunks also taken from arch/x86/kernel/apic/nmi.c, thanks\n * to those contributors as well.\n */\n\n#include <linux/mm.h>\n#include <linux/cpu.h>\n#include <linux/nmi.h>\n#include <linux/init.h>\n#include <linux/delay.h>\n#include <linux/freezer.h>\n#include <linux/kthread.h>\n#include <linux/lockdep.h>\n#include <linux/notifier.h>\n#include <linux/module.h>\n#include <linux/sysctl.h>\n\n#include <asm/irq_regs.h>\n#include <linux/perf_event.h>\n\nint watchdog_enabled = 1;\nint __read_mostly watchdog_thresh = 10;\n\nstatic DEFINE_PER_CPU(unsigned long, watchdog_touch_ts);\nstatic DEFINE_PER_CPU(struct task_struct *, softlockup_watchdog);\nstatic DEFINE_PER_CPU(struct hrtimer, watchdog_hrtimer);\nstatic DEFINE_PER_CPU(bool, softlockup_touch_sync);\nstatic DEFINE_PER_CPU(bool, soft_watchdog_warn);\n#ifdef CONFIG_HARDLOCKUP_DETECTOR\nstatic DEFINE_PER_CPU(bool, hard_watchdog_warn);\nstatic DEFINE_PER_CPU(bool, watchdog_nmi_touch);\nstatic DEFINE_PER_CPU(unsigned long, hrtimer_interrupts);\nstatic DEFINE_PER_CPU(unsigned long, hrtimer_interrupts_saved);\nstatic DEFINE_PER_CPU(struct perf_event *, watchdog_ev);\n#endif\n\n/* boot commands */\n/*\n * Should we panic when a soft-lockup or hard-lockup occurs:\n */\n#ifdef CONFIG_HARDLOCKUP_DETECTOR\nstatic int hardlockup_panic =\n\t\t\tCONFIG_BOOTPARAM_HARDLOCKUP_PANIC_VALUE;\n\nstatic int __init hardlockup_panic_setup(char *str)\n{\n\tif (!strncmp(str, \"panic\", 5))\n\t\thardlockup_panic = 1;\n\telse if (!strncmp(str, \"nopanic\", 7))\n\t\thardlockup_panic = 0;\n\telse if (!strncmp(str, \"0\", 1))\n\t\twatchdog_enabled = 0;\n\treturn 1;\n}\n__setup(\"nmi_watchdog=\", hardlockup_panic_setup);\n#endif\n\nunsigned int __read_mostly softlockup_panic =\n\t\t\tCONFIG_BOOTPARAM_SOFTLOCKUP_PANIC_VALUE;\n\nstatic int __init softlockup_panic_setup(char *str)\n{\n\tsoftlockup_panic = simple_strtoul(str, NULL, 0);\n\n\treturn 1;\n}\n__setup(\"softlockup_panic=\", softlockup_panic_setup);\n\nstatic int __init nowatchdog_setup(char *str)\n{\n\twatchdog_enabled = 0;\n\treturn 1;\n}\n__setup(\"nowatchdog\", nowatchdog_setup);\n\n/* deprecated */\nstatic int __init nosoftlockup_setup(char *str)\n{\n\twatchdog_enabled = 0;\n\treturn 1;\n}\n__setup(\"nosoftlockup\", nosoftlockup_setup);\n/*  */\n\n/*\n * Hard-lockup warnings should be triggered after just a few seconds. Soft-\n * lockups can have false positives under extreme conditions. So we generally\n * want a higher threshold for soft lockups than for hard lockups. So we couple\n * the thresholds with a factor: we make the soft threshold twice the amount of\n * time the hard threshold is.\n */\nstatic int get_softlockup_thresh(void)\n{\n\treturn watchdog_thresh * 2;\n}\n\n/*\n * Returns seconds, approximately.  We don't need nanosecond\n * resolution, and we don't need to waste time with a big divide when\n * 2^30ns == 1.074s.\n */\nstatic unsigned long get_timestamp(int this_cpu)\n{\n\treturn cpu_clock(this_cpu) >> 30LL;  /* 2^30 ~= 10^9 */\n}\n\nstatic unsigned long get_sample_period(void)\n{\n\t/*\n\t * convert watchdog_thresh from seconds to ns\n\t * the divide by 5 is to give hrtimer 5 chances to\n\t * increment before the hardlockup detector generates\n\t * a warning\n\t */\n\treturn get_softlockup_thresh() * (NSEC_PER_SEC / 5);\n}\n\n/* Commands for resetting the watchdog */\nstatic void __touch_watchdog(void)\n{\n\tint this_cpu = smp_processor_id();\n\n\t__this_cpu_write(watchdog_touch_ts, get_timestamp(this_cpu));\n}\n\nvoid touch_softlockup_watchdog(void)\n{\n\t__this_cpu_write(watchdog_touch_ts, 0);\n}\nEXPORT_SYMBOL(touch_softlockup_watchdog);\n\nvoid touch_all_softlockup_watchdogs(void)\n{\n\tint cpu;\n\n\t/*\n\t * this is done lockless\n\t * do we care if a 0 races with a timestamp?\n\t * all it means is the softlock check starts one cycle later\n\t */\n\tfor_each_online_cpu(cpu)\n\t\tper_cpu(watchdog_touch_ts, cpu) = 0;\n}\n\n#ifdef CONFIG_HARDLOCKUP_DETECTOR\nvoid touch_nmi_watchdog(void)\n{\n\tif (watchdog_enabled) {\n\t\tunsigned cpu;\n\n\t\tfor_each_present_cpu(cpu) {\n\t\t\tif (per_cpu(watchdog_nmi_touch, cpu) != true)\n\t\t\t\tper_cpu(watchdog_nmi_touch, cpu) = true;\n\t\t}\n\t}\n\ttouch_softlockup_watchdog();\n}\nEXPORT_SYMBOL(touch_nmi_watchdog);\n\n#endif\n\nvoid touch_softlockup_watchdog_sync(void)\n{\n\t__raw_get_cpu_var(softlockup_touch_sync) = true;\n\t__raw_get_cpu_var(watchdog_touch_ts) = 0;\n}\n\n#ifdef CONFIG_HARDLOCKUP_DETECTOR\n/* watchdog detector functions */\nstatic int is_hardlockup(void)\n{\n\tunsigned long hrint = __this_cpu_read(hrtimer_interrupts);\n\n\tif (__this_cpu_read(hrtimer_interrupts_saved) == hrint)\n\t\treturn 1;\n\n\t__this_cpu_write(hrtimer_interrupts_saved, hrint);\n\treturn 0;\n}\n#endif\n\nstatic int is_softlockup(unsigned long touch_ts)\n{\n\tunsigned long now = get_timestamp(smp_processor_id());\n\n\t/* Warn about unreasonable delays: */\n\tif (time_after(now, touch_ts + get_softlockup_thresh()))\n\t\treturn now - touch_ts;\n\n\treturn 0;\n}\n\n#ifdef CONFIG_HARDLOCKUP_DETECTOR\nvoid __weak hw_nmi_watchdog_set_attr(struct perf_event_attr *wd_attr) { }\n\nstatic struct perf_event_attr wd_hw_attr = {\n\t.type\t\t= PERF_TYPE_HARDWARE,\n\t.config\t\t= PERF_COUNT_HW_CPU_CYCLES,\n\t.size\t\t= sizeof(struct perf_event_attr),\n\t.pinned\t\t= 1,\n\t.disabled\t= 1,\n};\n\n/* Callback function for perf event subsystem */\nstatic void watchdog_overflow_callback(struct perf_event *event,\n\t\t struct perf_sample_data *data,\n\t\t struct pt_regs *regs)\n{\n\t/* Ensure the watchdog never gets throttled */\n\tevent->hw.interrupts = 0;\n\n\tif (__this_cpu_read(watchdog_nmi_touch) == true) {\n\t\t__this_cpu_write(watchdog_nmi_touch, false);\n\t\treturn;\n\t}\n\n\t/* check for a hardlockup\n\t * This is done by making sure our timer interrupt\n\t * is incrementing.  The timer interrupt should have\n\t * fired multiple times before we overflow'd.  If it hasn't\n\t * then this is a good indication the cpu is stuck\n\t */\n\tif (is_hardlockup()) {\n\t\tint this_cpu = smp_processor_id();\n\n\t\t/* only print hardlockups once */\n\t\tif (__this_cpu_read(hard_watchdog_warn) == true)\n\t\t\treturn;\n\n\t\tif (hardlockup_panic)\n\t\t\tpanic(\"Watchdog detected hard LOCKUP on cpu %d\", this_cpu);\n\t\telse\n\t\t\tWARN(1, \"Watchdog detected hard LOCKUP on cpu %d\", this_cpu);\n\n\t\t__this_cpu_write(hard_watchdog_warn, true);\n\t\treturn;\n\t}\n\n\t__this_cpu_write(hard_watchdog_warn, false);\n\treturn;\n}\nstatic void watchdog_interrupt_count(void)\n{\n\t__this_cpu_inc(hrtimer_interrupts);\n}\n#else\nstatic inline void watchdog_interrupt_count(void) { return; }\n#endif /* CONFIG_HARDLOCKUP_DETECTOR */\n\n/* watchdog kicker functions */\nstatic enum hrtimer_restart watchdog_timer_fn(struct hrtimer *hrtimer)\n{\n\tunsigned long touch_ts = __this_cpu_read(watchdog_touch_ts);\n\tstruct pt_regs *regs = get_irq_regs();\n\tint duration;\n\n\t/* kick the hardlockup detector */\n\twatchdog_interrupt_count();\n\n\t/* kick the softlockup detector */\n\twake_up_process(__this_cpu_read(softlockup_watchdog));\n\n\t/* .. and repeat */\n\thrtimer_forward_now(hrtimer, ns_to_ktime(get_sample_period()));\n\n\tif (touch_ts == 0) {\n\t\tif (unlikely(__this_cpu_read(softlockup_touch_sync))) {\n\t\t\t/*\n\t\t\t * If the time stamp was touched atomically\n\t\t\t * make sure the scheduler tick is up to date.\n\t\t\t */\n\t\t\t__this_cpu_write(softlockup_touch_sync, false);\n\t\t\tsched_clock_tick();\n\t\t}\n\t\t__touch_watchdog();\n\t\treturn HRTIMER_RESTART;\n\t}\n\n\t/* check for a softlockup\n\t * This is done by making sure a high priority task is\n\t * being scheduled.  The task touches the watchdog to\n\t * indicate it is getting cpu time.  If it hasn't then\n\t * this is a good indication some task is hogging the cpu\n\t */\n\tduration = is_softlockup(touch_ts);\n\tif (unlikely(duration)) {\n\t\t/* only warn once */\n\t\tif (__this_cpu_read(soft_watchdog_warn) == true)\n\t\t\treturn HRTIMER_RESTART;\n\n\t\tprintk(KERN_ERR \"BUG: soft lockup - CPU#%d stuck for %us! [%s:%d]\\n\",\n\t\t\tsmp_processor_id(), duration,\n\t\t\tcurrent->comm, task_pid_nr(current));\n\t\tprint_modules();\n\t\tprint_irqtrace_events(current);\n\t\tif (regs)\n\t\t\tshow_regs(regs);\n\t\telse\n\t\t\tdump_stack();\n\n\t\tif (softlockup_panic)\n\t\t\tpanic(\"softlockup: hung tasks\");\n\t\t__this_cpu_write(soft_watchdog_warn, true);\n\t} else\n\t\t__this_cpu_write(soft_watchdog_warn, false);\n\n\treturn HRTIMER_RESTART;\n}\n\n\n/*\n * The watchdog thread - touches the timestamp.\n */\nstatic int watchdog(void *unused)\n{\n\tstatic struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };\n\tstruct hrtimer *hrtimer = &__raw_get_cpu_var(watchdog_hrtimer);\n\n\tsched_setscheduler(current, SCHED_FIFO, &param);\n\n\t/* initialize timestamp */\n\t__touch_watchdog();\n\n\t/* kick off the timer for the hardlockup detector */\n\t/* done here because hrtimer_start can only pin to smp_processor_id() */\n\thrtimer_start(hrtimer, ns_to_ktime(get_sample_period()),\n\t\t      HRTIMER_MODE_REL_PINNED);\n\n\tset_current_state(TASK_INTERRUPTIBLE);\n\t/*\n\t * Run briefly once per second to reset the softlockup timestamp.\n\t * If this gets delayed for more than 60 seconds then the\n\t * debug-printout triggers in watchdog_timer_fn().\n\t */\n\twhile (!kthread_should_stop()) {\n\t\t__touch_watchdog();\n\t\tschedule();\n\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\n\treturn 0;\n}\n\n\n#ifdef CONFIG_HARDLOCKUP_DETECTOR\nstatic int watchdog_nmi_enable(int cpu)\n{\n\tstruct perf_event_attr *wd_attr;\n\tstruct perf_event *event = per_cpu(watchdog_ev, cpu);\n\n\t/* is it already setup and enabled? */\n\tif (event && event->state > PERF_EVENT_STATE_OFF)\n\t\tgoto out;\n\n\t/* it is setup but not enabled */\n\tif (event != NULL)\n\t\tgoto out_enable;\n\n\twd_attr = &wd_hw_attr;\n\twd_attr->sample_period = hw_nmi_get_sample_period(watchdog_thresh);\n\thw_nmi_watchdog_set_attr(wd_attr);\n\n\t/* Try to register using hardware perf events */\n\tevent = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback);\n\tif (!IS_ERR(event)) {\n\t\tprintk(KERN_INFO \"NMI watchdog enabled, takes one hw-pmu counter.\\n\");\n\t\tgoto out_save;\n\t}\n\n\n\t/* vary the KERN level based on the returned errno */\n\tif (PTR_ERR(event) == -EOPNOTSUPP)\n\t\tprintk(KERN_INFO \"NMI watchdog disabled (cpu%i): not supported (no LAPIC?)\\n\", cpu);\n\telse if (PTR_ERR(event) == -ENOENT)\n\t\tprintk(KERN_WARNING \"NMI watchdog disabled (cpu%i): hardware events not enabled\\n\", cpu);\n\telse\n\t\tprintk(KERN_ERR \"NMI watchdog disabled (cpu%i): unable to create perf event: %ld\\n\", cpu, PTR_ERR(event));\n\treturn PTR_ERR(event);\n\n\t/* success path */\nout_save:\n\tper_cpu(watchdog_ev, cpu) = event;\nout_enable:\n\tperf_event_enable(per_cpu(watchdog_ev, cpu));\nout:\n\treturn 0;\n}\n\nstatic void watchdog_nmi_disable(int cpu)\n{\n\tstruct perf_event *event = per_cpu(watchdog_ev, cpu);\n\n\tif (event) {\n\t\tperf_event_disable(event);\n\t\tper_cpu(watchdog_ev, cpu) = NULL;\n\n\t\t/* should be in cleanup, but blocks oprofile */\n\t\tperf_event_release_kernel(event);\n\t}\n\treturn;\n}\n#else\nstatic int watchdog_nmi_enable(int cpu) { return 0; }\nstatic void watchdog_nmi_disable(int cpu) { return; }\n#endif /* CONFIG_HARDLOCKUP_DETECTOR */\n\n/* prepare/enable/disable routines */\nstatic void watchdog_prepare_cpu(int cpu)\n{\n\tstruct hrtimer *hrtimer = &per_cpu(watchdog_hrtimer, cpu);\n\n\tWARN_ON(per_cpu(softlockup_watchdog, cpu));\n\thrtimer_init(hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\thrtimer->function = watchdog_timer_fn;\n}\n\nstatic int watchdog_enable(int cpu)\n{\n\tstruct task_struct *p = per_cpu(softlockup_watchdog, cpu);\n\tint err = 0;\n\n\t/* enable the perf event */\n\terr = watchdog_nmi_enable(cpu);\n\n\t/* Regardless of err above, fall through and start softlockup */\n\n\t/* create the watchdog thread */\n\tif (!p) {\n\t\tp = kthread_create(watchdog, (void *)(unsigned long)cpu, \"watchdog/%d\", cpu);\n\t\tif (IS_ERR(p)) {\n\t\t\tprintk(KERN_ERR \"softlockup watchdog for %i failed\\n\", cpu);\n\t\t\tif (!err) {\n\t\t\t\t/* if hardlockup hasn't already set this */\n\t\t\t\terr = PTR_ERR(p);\n\t\t\t\t/* and disable the perf event */\n\t\t\t\twatchdog_nmi_disable(cpu);\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\t\tkthread_bind(p, cpu);\n\t\tper_cpu(watchdog_touch_ts, cpu) = 0;\n\t\tper_cpu(softlockup_watchdog, cpu) = p;\n\t\twake_up_process(p);\n\t}\n\nout:\n\treturn err;\n}\n\nstatic void watchdog_disable(int cpu)\n{\n\tstruct task_struct *p = per_cpu(softlockup_watchdog, cpu);\n\tstruct hrtimer *hrtimer = &per_cpu(watchdog_hrtimer, cpu);\n\n\t/*\n\t * cancel the timer first to stop incrementing the stats\n\t * and waking up the kthread\n\t */\n\thrtimer_cancel(hrtimer);\n\n\t/* disable the perf event */\n\twatchdog_nmi_disable(cpu);\n\n\t/* stop the watchdog thread */\n\tif (p) {\n\t\tper_cpu(softlockup_watchdog, cpu) = NULL;\n\t\tkthread_stop(p);\n\t}\n}\n\nstatic void watchdog_enable_all_cpus(void)\n{\n\tint cpu;\n\n\twatchdog_enabled = 0;\n\n\tfor_each_online_cpu(cpu)\n\t\tif (!watchdog_enable(cpu))\n\t\t\t/* if any cpu succeeds, watchdog is considered\n\t\t\t   enabled for the system */\n\t\t\twatchdog_enabled = 1;\n\n\tif (!watchdog_enabled)\n\t\tprintk(KERN_ERR \"watchdog: failed to be enabled on some cpus\\n\");\n\n}\n\nstatic void watchdog_disable_all_cpus(void)\n{\n\tint cpu;\n\n\tfor_each_online_cpu(cpu)\n\t\twatchdog_disable(cpu);\n\n\t/* if all watchdogs are disabled, then they are disabled for the system */\n\twatchdog_enabled = 0;\n}\n\n\n/* sysctl functions */\n#ifdef CONFIG_SYSCTL\n/*\n * proc handler for /proc/sys/kernel/nmi_watchdog,watchdog_thresh\n */\n\nint proc_dowatchdog(struct ctl_table *table, int write,\n\t\t    void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint ret;\n\n\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\tif (ret || !write)\n\t\tgoto out;\n\n\tif (watchdog_enabled && watchdog_thresh)\n\t\twatchdog_enable_all_cpus();\n\telse\n\t\twatchdog_disable_all_cpus();\n\nout:\n\treturn ret;\n}\n#endif /* CONFIG_SYSCTL */\n\n\n/*\n * Create/destroy watchdog threads as CPUs come and go:\n */\nstatic int __cpuinit\ncpu_callback(struct notifier_block *nfb, unsigned long action, void *hcpu)\n{\n\tint hotcpu = (unsigned long)hcpu;\n\n\tswitch (action) {\n\tcase CPU_UP_PREPARE:\n\tcase CPU_UP_PREPARE_FROZEN:\n\t\twatchdog_prepare_cpu(hotcpu);\n\t\tbreak;\n\tcase CPU_ONLINE:\n\tcase CPU_ONLINE_FROZEN:\n\t\tif (watchdog_enabled)\n\t\t\twatchdog_enable(hotcpu);\n\t\tbreak;\n#ifdef CONFIG_HOTPLUG_CPU\n\tcase CPU_UP_CANCELED:\n\tcase CPU_UP_CANCELED_FROZEN:\n\t\twatchdog_disable(hotcpu);\n\t\tbreak;\n\tcase CPU_DEAD:\n\tcase CPU_DEAD_FROZEN:\n\t\twatchdog_disable(hotcpu);\n\t\tbreak;\n#endif /* CONFIG_HOTPLUG_CPU */\n\t}\n\n\t/*\n\t * hardlockup and softlockup are not important enough\n\t * to block cpu bring up.  Just always succeed and\n\t * rely on printk output to flag problems.\n\t */\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block __cpuinitdata cpu_nfb = {\n\t.notifier_call = cpu_callback\n};\n\nvoid __init lockup_detector_init(void)\n{\n\tvoid *cpu = (void *)(long)smp_processor_id();\n\tint err;\n\n\terr = cpu_callback(&cpu_nfb, CPU_UP_PREPARE, cpu);\n\tWARN_ON(notifier_to_errno(err));\n\n\tcpu_callback(&cpu_nfb, CPU_ONLINE, cpu);\n\tregister_cpu_notifier(&cpu_nfb);\n\n\treturn;\n}\n", "/*\n * data_breakpoint.c - Sample HW Breakpoint file to watch kernel data address\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.\n *\n * usage: insmod data_breakpoint.ko ksym=<ksym_name>\n *\n * This file is a kernel module that places a breakpoint over ksym_name kernel\n * variable using Hardware Breakpoint register. The corresponding handler which\n * prints a backtrace is invoked every time a write operation is performed on\n * that variable.\n *\n * Copyright (C) IBM Corporation, 2009\n *\n * Author: K.Prasad <prasad@linux.vnet.ibm.com>\n */\n#include <linux/module.h>\t/* Needed by all modules */\n#include <linux/kernel.h>\t/* Needed for KERN_INFO */\n#include <linux/init.h>\t\t/* Needed for the macros */\n#include <linux/kallsyms.h>\n\n#include <linux/perf_event.h>\n#include <linux/hw_breakpoint.h>\n\nstruct perf_event * __percpu *sample_hbp;\n\nstatic char ksym_name[KSYM_NAME_LEN] = \"pid_max\";\nmodule_param_string(ksym, ksym_name, KSYM_NAME_LEN, S_IRUGO);\nMODULE_PARM_DESC(ksym, \"Kernel symbol to monitor; this module will report any\"\n\t\t\t\" write operations on the kernel symbol\");\n\nstatic void sample_hbp_handler(struct perf_event *bp,\n\t\t\t       struct perf_sample_data *data,\n\t\t\t       struct pt_regs *regs)\n{\n\tprintk(KERN_INFO \"%s value is changed\\n\", ksym_name);\n\tdump_stack();\n\tprintk(KERN_INFO \"Dump stack from sample_hbp_handler\\n\");\n}\n\nstatic int __init hw_break_module_init(void)\n{\n\tint ret;\n\tstruct perf_event_attr attr;\n\n\thw_breakpoint_init(&attr);\n\tattr.bp_addr = kallsyms_lookup_name(ksym_name);\n\tattr.bp_len = HW_BREAKPOINT_LEN_4;\n\tattr.bp_type = HW_BREAKPOINT_W | HW_BREAKPOINT_R;\n\n\tsample_hbp = register_wide_hw_breakpoint(&attr, sample_hbp_handler);\n\tif (IS_ERR((void __force *)sample_hbp)) {\n\t\tret = PTR_ERR((void __force *)sample_hbp);\n\t\tgoto fail;\n\t}\n\n\tprintk(KERN_INFO \"HW Breakpoint for %s write installed\\n\", ksym_name);\n\n\treturn 0;\n\nfail:\n\tprintk(KERN_INFO \"Breakpoint registration failed\\n\");\n\n\treturn ret;\n}\n\nstatic void __exit hw_break_module_exit(void)\n{\n\tunregister_wide_hw_breakpoint(sample_hbp);\n\tprintk(KERN_INFO \"HW Breakpoint for %s write uninstalled\\n\", ksym_name);\n}\n\nmodule_init(hw_break_module_init);\nmodule_exit(hw_break_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"K.Prasad\");\nMODULE_DESCRIPTION(\"ksym breakpoint\");\n"], "filenames": ["arch/alpha/kernel/perf_event.c", "arch/arm/kernel/perf_event_v6.c", "arch/arm/kernel/perf_event_v7.c", "arch/arm/kernel/perf_event_xscale.c", "arch/arm/kernel/ptrace.c", "arch/arm/kernel/swp_emulate.c", "arch/arm/mm/fault.c", "arch/mips/kernel/perf_event.c", "arch/mips/kernel/traps.c", "arch/mips/kernel/unaligned.c", "arch/mips/math-emu/cp1emu.c", "arch/mips/mm/fault.c", "arch/powerpc/include/asm/emulated_ops.h", "arch/powerpc/kernel/perf_event.c", "arch/powerpc/kernel/perf_event_fsl_emb.c", "arch/powerpc/kernel/ptrace.c", "arch/powerpc/mm/fault.c", "arch/s390/mm/fault.c", "arch/sh/kernel/ptrace_32.c", "arch/sh/kernel/traps_32.c", "arch/sh/kernel/traps_64.c", "arch/sh/math-emu/math.c", "arch/sh/mm/fault_32.c", "arch/sh/mm/tlbflush_64.c", "arch/sparc/kernel/perf_event.c", "arch/sparc/kernel/unaligned_32.c", "arch/sparc/kernel/unaligned_64.c", "arch/sparc/kernel/visemul.c", "arch/sparc/math-emu/math_32.c", "arch/sparc/math-emu/math_64.c", "arch/sparc/mm/fault_32.c", "arch/sparc/mm/fault_64.c", "arch/x86/kernel/cpu/perf_event.c", "arch/x86/kernel/cpu/perf_event_intel.c", "arch/x86/kernel/cpu/perf_event_intel_ds.c", "arch/x86/kernel/cpu/perf_event_p4.c", "arch/x86/kernel/kgdb.c", "arch/x86/kernel/ptrace.c", "arch/x86/mm/fault.c", "include/linux/perf_event.h", "kernel/events/core.c", "kernel/events/internal.h", "kernel/events/ring_buffer.c", "kernel/sched.c", "kernel/watchdog.c", "samples/hw_breakpoint/data_breakpoint.c"], "buggy_code_start_loc": [850, 482, 790, 254, 399, 186, 321, 530, 581, 114, 275, 148, 81, 1210, 571, 885, 176, 302, 66, 396, 437, 623, 163, 119, 1280, 250, 320, 805, 167, 187, 254, 328, 1342, 1006, 343, 973, 611, 531, 1062, 685, 3975, 29, 41, 2223, 214, 44], "buggy_code_end_loc": [851, 483, 791, 587, 400, 187, 326, 531, 644, 521, 277, 163, 89, 1350, 648, 886, 334, 353, 67, 397, 669, 624, 218, 208, 1281, 342, 651, 806, 168, 188, 310, 442, 1343, 1007, 620, 974, 612, 532, 1169, 1148, 5236, 30, 131, 2224, 215, 45], "fixing_code_start_loc": [850, 482, 790, 254, 399, 186, 321, 530, 581, 114, 275, 148, 81, 1210, 571, 885, 176, 302, 66, 396, 437, 623, 163, 119, 1280, 250, 320, 805, 167, 187, 254, 328, 1342, 1006, 343, 973, 611, 531, 1062, 685, 3975, 28, 41, 2223, 214, 44], "fixing_code_end_loc": [851, 483, 791, 587, 400, 187, 326, 531, 644, 520, 276, 161, 89, 1350, 648, 886, 334, 353, 67, 397, 669, 624, 218, 208, 1281, 342, 651, 806, 168, 188, 308, 440, 1343, 1007, 620, 974, 612, 532, 1169, 1146, 5229, 28, 126, 2224, 215, 45], "type": "CWE-400", "message": "The Performance Events subsystem in the Linux kernel before 3.1 does not properly handle event overflows associated with PERF_COUNT_SW_CPU_CLOCK events, which allows local users to cause a denial of service (system hang) via a crafted application.", "other": {"cve": {"id": "CVE-2011-2918", "sourceIdentifier": "secalert@redhat.com", "published": "2012-05-24T23:55:02.167", "lastModified": "2023-02-13T04:32:10.680", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The Performance Events subsystem in the Linux kernel before 3.1 does not properly handle event overflows associated with PERF_COUNT_SW_CPU_CLOCK events, which allows local users to cause a denial of service (system hang) via a crafted application."}, {"lang": "es", "value": "El subsistema Performance Events en el kernel de Linux antes de v3.1 no trata correctamente los desbordamientos de eventos asociados con eventos PERF_COUNT_SW_CPU_CLOCK, lo que permite a usuarios locales causar una denegaci\u00f3n de servicio (bloqueo del sistema) a trav\u00e9s de una aplicaci\u00f3n modificada."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-400"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.1", "matchCriteriaId": "156989A4-23D9-434A-B512-9C0F3583D13D"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=a8b0ca17b80e92faab46ee7179ba9e99ccb61233", "source": "secalert@redhat.com"}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.1", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2011/08/16/1", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=730706", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/a8b0ca17b80e92faab46ee7179ba9e99ccb61233", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/a8b0ca17b80e92faab46ee7179ba9e99ccb61233"}}