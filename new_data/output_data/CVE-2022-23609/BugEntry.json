{"buggy_code": ["import ast\nimport base64\nimport io\nimport logging\nimport random\nfrom html import escape\nfrom os import remove\n\nimport magic\nfrom flask import Flask, abort, request\nfrom PIL import Image\n\napp = Flask(__name__)\napp.logger.setLevel(logging.DEBUG)\napp.config[\"MAX_CONTENT_LENGTH\"] = 2 * (10**5)\n# allow a maximum of 200kb length. no image is (usually) larger than 200kb, so this is good.\n\n\ndef get_config():\n    f = open(\"config\", \"r\")\n    conf = ast.literal_eval(f.read())\n    f.close()\n\n    cache_size = conf[\"max_cache_size\"]\n    host = conf[\"host\"]\n    port = conf[\"port\"]\n\n    return cache_size, host, port\n\n\ndef allowed_file(enc_data):\n    mimetype = magic.from_buffer(enc_data, mime=True)\n\n    if mimetype[:5] != \"image\":\n        return enc_data, False\n    else:\n        return enc_data, True\n\n\n@app.route(\"/\")\ndef index():\n    return \"<h1>401 - Unauthorised</h1>\"\n\n\n@app.errorhandler(404)\ndef errorhandle(e):\n    return abort(401)\n\n\n@app.route(\"/Y2hlY2tfY2FjaGVkX2ZpbGVz\", methods=[\"POST\"])\ndef check_cache():\n    # Explanation on Caching\n    # -- BV\n    # Originally, I was going to have the script cache here, where it actually checks the cache. However,\n    # I soon realised that the all_files file is only modified in the other function to add to the file.\n    # Because of this, we only really need to check the cache status there, and see if it is over the\n    # cache limit in that function. The only thing we have to do here in relation to caching is moving items\n    # to the end of the array.\n\n    f = open(\"all_files\", \"r\")\n    db = ast.literal_eval(f.read())\n    f.close()\n\n    tmp = request.json\n    x = str({\"title\": tmp[\"title\"], \"singer\": tmp[\"singer\"], \"album\": tmp[\"album\"]})\n    data = ast.literal_eval(x)\n\n    if data[\"title\"][:9] == \"[PAUSED] \":\n        data[\"title\"] = data[\"title\"][9::]\n\n    del tmp\n\n    if type(data) != dict:\n        typeprov = type(data)\n        print(\n            \"Not provided a dict, instead a \"\n            + str(typeprov)\n            + \" was provided. Returning NoneType.\"\n        )\n        return \"None\"\n\n    for key in db:\n        print(f\"Analysing data {data} against item(s) {key[0]}.\")\n        if data == key[0]:\n            print(\"Found match for \" + str(data) + \" in database.\")\n\n            # caching\n            new_data = []\n            for y in db:\n                if y[0] == data:\n                    x = y\n                else:\n                    new_data.append(y)\n            new_data.append(x)\n\n            f = open(\"all_files\", \"w\")\n            f.write(str(new_data))\n            f.close()\n\n            print(\"Returning dictionary: \" + str(key))\n            return str(key)\n\n    print(\"No match found. Returning NoneType.\")\n    return \"None\"\n\n\n@app.route(\"/bGVhdmVfcmlnaHRfbm93\", methods=[\"POST\"])\ndef uploadimage():\n    # print(request.json)\n    if not request.json or \"image\" not in request.json:\n        print(\"No data sent or no image provided. Aborting with 400.\")\n        abort(400)\n\n    im_b64 = request.json[\"image\"]\n    img_bytes = base64.b64decode(im_b64.encode(\"utf-8\"))\n    img_bytes, valid = allowed_file(img_bytes)\n    if not valid:\n        return escape({\"entry\": \"False\"})\n    img = Image.open(io.BytesIO(img_bytes))\n\n    file_ending = img.format\n    print(f\"File has filetype {file_ending}.\")\n\n    if file_ending == \"JPEG\":\n        file_ending = \".jpg\"\n    else:\n        file_ending = \".png\"\n\n    one_hundred_million = 100000000\n    lots_of_nine = 999999999\n\n    file_name = None\n\n    f = open(\"all_files\", \"r\")\n    all_files = ast.literal_eval(f.read())\n    f.close()\n\n    attempt = 0\n\n    while file_name is None or file_name in all_files:\n        if attempt <= 1000:\n            file_name = random.randint(one_hundred_million, lots_of_nine)\n\n            file_name = base64.b64encode(str(file_name).encode(\"utf-8\")).decode(\"utf-8\")\n\n            print(f\"Trying new file name: {file_name}\")\n        else:\n            attempt = 0\n            one_hundred_million += 100000\n            lots_of_nine += 1000000\n\n            while one_hundred_million >= lots_of_nine:\n                one_hundred_million -= 10000\n\n            one_hundred_million -= 10000\n\n    print(f\"Successful file name: {file_name}\")\n\n    title = request.json[\"title\"]\n    if title[:9] == \"[PAUSED] \":\n        title = title[9::]\n\n    singer = request.json[\"singer\"]\n    album = request.json[\"album\"]\n\n    file_db_entry = [\n        {\"title\": title, \"singer\": singer, \"album\": album},\n        file_name,\n        file_ending,\n    ]\n    print(f\"New db entry: {file_db_entry}\")\n\n    all_files.append(file_db_entry)\n\n    # caching\n    # we want a limit of X amount of files as defined by the config\n\n    # 1. see how long the list is\n    # 2. if it is over get_config()'s cache limit, delete value [0]\n    # 3. delete it on disk.\n\n    cache, x, y = get_config()\n    del x\n    del y\n\n    length = len(all_files)\n    while (\n        length > cache\n    ):  # if it is not over the limit, it will skip. if it is, it does this.\n        # if we have gone over our cache limit, let's delete the first entry.\n        filename = all_files[0][1] + all_files[0][2]\n        remove(filename)\n        del all_files[0]\n        length = len(all_files)\n\n    f = open(\"all_files\", \"w\")\n    f.write(str(all_files))\n    f.close()\n\n    file_name = file_name + file_ending\n\n    img.save(file_name)\n\n    print(f\"Saved {file_name} from {file_db_entry}.\")\n    print(f\"Returning {file_db_entry}.\")\n    return escape(str({\"entry\": file_db_entry}))\n\n\ndef run_server_api():\n    cache, host, port = get_config()\n    app.run(host=host, port=port)\n\n\nif __name__ == \"__main__\":\n    run_server_api()\n"], "fixing_code": ["import ast\nimport base64\nimport io\nimport logging\nimport random\nfrom html import escape\nfrom os import remove\nimport werkzeug.utils\nimport magic\nfrom flask import Flask, abort, request\nfrom PIL import Image\n\napp = Flask(__name__)\napp.logger.setLevel(logging.DEBUG)\napp.config[\"MAX_CONTENT_LENGTH\"] = 2 * (10**5)\n# allow a maximum of 200kb length. no image is (usually) larger than 200kb, so this is good.\n\n\ndef get_config():\n    f = open(\"config\", \"r\")\n    conf = ast.literal_eval(f.read())\n    f.close()\n\n    cache_size = conf[\"max_cache_size\"]\n    host = conf[\"host\"]\n    port = conf[\"port\"]\n\n    return cache_size, host, port\n\n\ndef allowed_file(enc_data):\n    mimetype = magic.from_buffer(enc_data, mime=True)\n\n    if mimetype[:5] != \"image\":\n        return enc_data, False\n    else:\n        return enc_data, True\n\n\n@app.route(\"/\")\ndef index():\n    return \"<h1>401 - Unauthorised</h1>\"\n\n\n@app.errorhandler(404)\ndef errorhandle(e):\n    return abort(401)\n\n\n@app.route(\"/Y2hlY2tfY2FjaGVkX2ZpbGVz\", methods=[\"POST\"])\ndef check_cache():\n    # Explanation on Caching\n    # -- BV\n    # Originally, I was going to have the script cache here, where it actually checks the cache. However,\n    # I soon realised that the all_files file is only modified in the other function to add to the file.\n    # Because of this, we only really need to check the cache status there, and see if it is over the\n    # cache limit in that function. The only thing we have to do here in relation to caching is moving items\n    # to the end of the array.\n\n    f = open(\"all_files\", \"r\")\n    db = ast.literal_eval(f.read())\n    f.close()\n\n    tmp = request.json\n    x = str({\"title\": tmp[\"title\"], \"singer\": tmp[\"singer\"], \"album\": tmp[\"album\"]})\n    data = ast.literal_eval(x)\n\n    if data[\"title\"][:9] == \"[PAUSED] \":\n        data[\"title\"] = data[\"title\"][9::]\n\n    del tmp\n\n    if type(data) != dict:\n        typeprov = type(data)\n        print(\n            \"Not provided a dict, instead a \"\n            + str(typeprov)\n            + \" was provided. Returning NoneType.\"\n        )\n        return \"None\"\n\n    for key in db:\n        print(f\"Analysing data {data} against item(s) {key[0]}.\")\n        if data == key[0]:\n            print(\"Found match for \" + str(data) + \" in database.\")\n\n            # caching\n            new_data = []\n            for y in db:\n                if y[0] == data:\n                    x = y\n                else:\n                    new_data.append(y)\n            new_data.append(x)\n\n            f = open(\"all_files\", \"w\")\n            f.write(str(new_data))\n            f.close()\n\n            print(\"Returning dictionary: \" + str(key))\n            return str(key)\n\n    print(\"No match found. Returning NoneType.\")\n    return \"None\"\n\n\n@app.route(\"/bGVhdmVfcmlnaHRfbm93\", methods=[\"POST\"])\ndef uploadimage():\n    # print(request.json)\n    if not request.json or \"image\" not in request.json:\n        print(\"No data sent or no image provided. Aborting with 400.\")\n        abort(400)\n\n    im_b64 = request.json[\"image\"]\n    img_bytes = base64.b64decode(im_b64.encode(\"utf-8\"))\n    img_bytes, valid = allowed_file(img_bytes)\n    if not valid:\n        return escape({\"entry\": \"False\"})\n    img = Image.open(io.BytesIO(img_bytes))\n\n    file_ending = img.format\n    print(f\"File has filetype {file_ending}.\")\n\n    if file_ending == \"JPEG\":\n        file_ending = \".jpg\"\n    else:\n        file_ending = \".png\"\n\n    one_hundred_million = 100000000\n    lots_of_nine = 999999999\n\n    file_name = None\n\n    f = open(\"all_files\", \"r\")\n    all_files = ast.literal_eval(f.read())\n    f.close()\n\n    attempt = 0\n\n    while file_name is None or file_name in all_files:\n        if attempt <= 1000:\n            file_name = random.randint(one_hundred_million, lots_of_nine)\n\n            file_name = base64.b64encode(str(file_name).encode(\"utf-8\")).decode(\"utf-8\")\n\n            print(f\"Trying new file name: {file_name}\")\n        else:\n            attempt = 0\n            one_hundred_million += 100000\n            lots_of_nine += 1000000\n\n            while one_hundred_million >= lots_of_nine:\n                one_hundred_million -= 10000\n\n            one_hundred_million -= 10000\n\n    print(f\"Successful file name: {file_name}\")\n\n    title = request.json[\"title\"]\n    if title[:9] == \"[PAUSED] \":\n        title = title[9::]\n\n    singer = request.json[\"singer\"]\n    album = request.json[\"album\"]\n\n    file_db_entry = [\n        {\"title\": title, \"singer\": singer, \"album\": album},\n        file_name,\n        file_ending,\n    ]\n    print(f\"New db entry: {file_db_entry}\")\n\n    all_files.append(file_db_entry)\n\n    # caching\n    # we want a limit of X amount of files as defined by the config\n\n    # 1. see how long the list is\n    # 2. if it is over get_config()'s cache limit, delete value [0]\n    # 3. delete it on disk.\n\n    cache, x, y = get_config()\n    del x\n    del y\n\n    length = len(all_files)\n    while (\n        length > cache\n    ):  # if it is not over the limit, it will skip. if it is, it does this.\n        # if we have gone over our cache limit, let's delete the first entry.\n        filename = all_files[0][1] + all_files[0][2]\n        remove(werkzeug.utils.secure_filename(filename))\n        del all_files[0]\n        length = len(all_files)\n\n    f = open(\"all_files\", \"w\")\n    f.write(str(all_files))\n    f.close()\n\n    file_name = file_name + file_ending\n\n    img.save(file_name)\n\n    print(f\"Saved {file_name} from {file_db_entry}.\")\n    print(f\"Returning {file_db_entry}.\")\n    return escape(str({\"entry\": file_db_entry}))\n\n\ndef run_server_api():\n    cache, host, port = get_config()\n    app.run(host=host, port=port)\n\n\nif __name__ == \"__main__\":\n    run_server_api()\n"], "filenames": ["upload/server.py"], "buggy_code_start_loc": [8], "buggy_code_end_loc": [193], "fixing_code_start_loc": [8], "fixing_code_end_loc": [193], "type": "CWE-22", "message": "iTunesRPC-Remastered is a Discord Rich Presence for iTunes on Windows utility. In affected versions iTunesRPC-Remastered did not properly sanitize user input used to remove files leading to file deletion only limited by the process permissions. Users are advised to upgrade as soon as possible.", "other": {"cve": {"id": "CVE-2022-23609", "sourceIdentifier": "security-advisories@github.com", "published": "2022-02-04T23:15:15.617", "lastModified": "2022-02-11T03:09:58.493", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "iTunesRPC-Remastered is a Discord Rich Presence for iTunes on Windows utility. In affected versions iTunesRPC-Remastered did not properly sanitize user input used to remove files leading to file deletion only limited by the process permissions. Users are advised to upgrade as soon as possible."}, {"lang": "es", "value": "iTunesRPC-Remastered es una utilidad de Discord Rich Presence para iTunes en Windows. En las versiones afectadas, iTunesRPC-Remastered no saneaba correctamente las entradas del usuario usadas para eliminar archivos, conllevando a un borrado de archivos s\u00f3lo limitado por los permisos del proceso. Es recomendado a usuarios actualizar lo antes posible"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.1, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.2}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:L/I:L/A:L", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "LOW", "baseScore": 8.3, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.7}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 6.4}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 4.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-22"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-22"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:itunesrpc-remastered_project:itunesrpc-remastered:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.1.0", "versionEndExcluding": "3.1.1", "matchCriteriaId": "6D3E948B-A56D-4ECC-B60E-E055C2A26A1F"}]}]}], "references": [{"url": "https://github.com/bildsben/iTunesRPC-Remastered/commit/1eb1e5428f0926b2829a0bbbb65b0d946e608593", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/bildsben/iTunesRPC-Remastered/security/advisories/GHSA-cc8j-fr7v-7r6q", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/bildsben/iTunesRPC-Remastered/commit/1eb1e5428f0926b2829a0bbbb65b0d946e608593"}}