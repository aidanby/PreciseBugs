{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/* Copyright (c) 2016 Facebook\n */\n#include <linux/bpf.h>\n#include <linux/jhash.h>\n#include <linux/filter.h>\n#include <linux/kernel.h>\n#include <linux/stacktrace.h>\n#include <linux/perf_event.h>\n#include <linux/irq_work.h>\n#include <linux/btf_ids.h>\n#include <linux/buildid.h>\n#include \"percpu_freelist.h\"\n\n#define STACK_CREATE_FLAG_MASK\t\t\t\t\t\\\n\t(BPF_F_NUMA_NODE | BPF_F_RDONLY | BPF_F_WRONLY |\t\\\n\t BPF_F_STACK_BUILD_ID)\n\nstruct stack_map_bucket {\n\tstruct pcpu_freelist_node fnode;\n\tu32 hash;\n\tu32 nr;\n\tu64 data[];\n};\n\nstruct bpf_stack_map {\n\tstruct bpf_map map;\n\tvoid *elems;\n\tstruct pcpu_freelist freelist;\n\tu32 n_buckets;\n\tstruct stack_map_bucket *buckets[];\n};\n\n/* irq_work to run up_read() for build_id lookup in nmi context */\nstruct stack_map_irq_work {\n\tstruct irq_work irq_work;\n\tstruct mm_struct *mm;\n};\n\nstatic void do_up_read(struct irq_work *entry)\n{\n\tstruct stack_map_irq_work *work;\n\n\tif (WARN_ON_ONCE(IS_ENABLED(CONFIG_PREEMPT_RT)))\n\t\treturn;\n\n\twork = container_of(entry, struct stack_map_irq_work, irq_work);\n\tmmap_read_unlock_non_owner(work->mm);\n}\n\nstatic DEFINE_PER_CPU(struct stack_map_irq_work, up_read_work);\n\nstatic inline bool stack_map_use_build_id(struct bpf_map *map)\n{\n\treturn (map->map_flags & BPF_F_STACK_BUILD_ID);\n}\n\nstatic inline int stack_map_data_size(struct bpf_map *map)\n{\n\treturn stack_map_use_build_id(map) ?\n\t\tsizeof(struct bpf_stack_build_id) : sizeof(u64);\n}\n\nstatic int prealloc_elems_and_freelist(struct bpf_stack_map *smap)\n{\n\tu32 elem_size = sizeof(struct stack_map_bucket) + smap->map.value_size;\n\tint err;\n\n\tsmap->elems = bpf_map_area_alloc(elem_size * smap->map.max_entries,\n\t\t\t\t\t smap->map.numa_node);\n\tif (!smap->elems)\n\t\treturn -ENOMEM;\n\n\terr = pcpu_freelist_init(&smap->freelist);\n\tif (err)\n\t\tgoto free_elems;\n\n\tpcpu_freelist_populate(&smap->freelist, smap->elems, elem_size,\n\t\t\t       smap->map.max_entries);\n\treturn 0;\n\nfree_elems:\n\tbpf_map_area_free(smap->elems);\n\treturn err;\n}\n\n/* Called from syscall */\nstatic struct bpf_map *stack_map_alloc(union bpf_attr *attr)\n{\n\tu32 value_size = attr->value_size;\n\tstruct bpf_stack_map *smap;\n\tu64 cost, n_buckets;\n\tint err;\n\n\tif (!bpf_capable())\n\t\treturn ERR_PTR(-EPERM);\n\n\tif (attr->map_flags & ~STACK_CREATE_FLAG_MASK)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/* check sanity of attributes */\n\tif (attr->max_entries == 0 || attr->key_size != 4 ||\n\t    value_size < 8 || value_size % 8)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tBUILD_BUG_ON(sizeof(struct bpf_stack_build_id) % sizeof(u64));\n\tif (attr->map_flags & BPF_F_STACK_BUILD_ID) {\n\t\tif (value_size % sizeof(struct bpf_stack_build_id) ||\n\t\t    value_size / sizeof(struct bpf_stack_build_id)\n\t\t    > sysctl_perf_event_max_stack)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t} else if (value_size / 8 > sysctl_perf_event_max_stack)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/* hash table size must be power of 2 */\n\tn_buckets = roundup_pow_of_two(attr->max_entries);\n\tif (!n_buckets)\n\t\treturn ERR_PTR(-E2BIG);\n\n\tcost = n_buckets * sizeof(struct stack_map_bucket *) + sizeof(*smap);\n\tcost += n_buckets * (value_size + sizeof(struct stack_map_bucket));\n\tsmap = bpf_map_area_alloc(cost, bpf_map_attr_numa_node(attr));\n\tif (!smap)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbpf_map_init_from_attr(&smap->map, attr);\n\tsmap->map.value_size = value_size;\n\tsmap->n_buckets = n_buckets;\n\n\terr = get_callchain_buffers(sysctl_perf_event_max_stack);\n\tif (err)\n\t\tgoto free_smap;\n\n\terr = prealloc_elems_and_freelist(smap);\n\tif (err)\n\t\tgoto put_buffers;\n\n\treturn &smap->map;\n\nput_buffers:\n\tput_callchain_buffers();\nfree_smap:\n\tbpf_map_area_free(smap);\n\treturn ERR_PTR(err);\n}\n\nstatic void stack_map_get_build_id_offset(struct bpf_stack_build_id *id_offs,\n\t\t\t\t\t  u64 *ips, u32 trace_nr, bool user)\n{\n\tint i;\n\tstruct vm_area_struct *vma;\n\tbool irq_work_busy = false;\n\tstruct stack_map_irq_work *work = NULL;\n\n\tif (irqs_disabled()) {\n\t\tif (!IS_ENABLED(CONFIG_PREEMPT_RT)) {\n\t\t\twork = this_cpu_ptr(&up_read_work);\n\t\t\tif (irq_work_is_busy(&work->irq_work)) {\n\t\t\t\t/* cannot queue more up_read, fallback */\n\t\t\t\tirq_work_busy = true;\n\t\t\t}\n\t\t} else {\n\t\t\t/*\n\t\t\t * PREEMPT_RT does not allow to trylock mmap sem in\n\t\t\t * interrupt disabled context. Force the fallback code.\n\t\t\t */\n\t\t\tirq_work_busy = true;\n\t\t}\n\t}\n\n\t/*\n\t * We cannot do up_read() when the irq is disabled, because of\n\t * risk to deadlock with rq_lock. To do build_id lookup when the\n\t * irqs are disabled, we need to run up_read() in irq_work. We use\n\t * a percpu variable to do the irq_work. If the irq_work is\n\t * already used by another lookup, we fall back to report ips.\n\t *\n\t * Same fallback is used for kernel stack (!user) on a stackmap\n\t * with build_id.\n\t */\n\tif (!user || !current || !current->mm || irq_work_busy ||\n\t    !mmap_read_trylock(current->mm)) {\n\t\t/* cannot access current->mm, fall back to ips */\n\t\tfor (i = 0; i < trace_nr; i++) {\n\t\t\tid_offs[i].status = BPF_STACK_BUILD_ID_IP;\n\t\t\tid_offs[i].ip = ips[i];\n\t\t\tmemset(id_offs[i].build_id, 0, BUILD_ID_SIZE_MAX);\n\t\t}\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < trace_nr; i++) {\n\t\tvma = find_vma(current->mm, ips[i]);\n\t\tif (!vma || build_id_parse(vma, id_offs[i].build_id, NULL)) {\n\t\t\t/* per entry fall back to ips */\n\t\t\tid_offs[i].status = BPF_STACK_BUILD_ID_IP;\n\t\t\tid_offs[i].ip = ips[i];\n\t\t\tmemset(id_offs[i].build_id, 0, BUILD_ID_SIZE_MAX);\n\t\t\tcontinue;\n\t\t}\n\t\tid_offs[i].offset = (vma->vm_pgoff << PAGE_SHIFT) + ips[i]\n\t\t\t- vma->vm_start;\n\t\tid_offs[i].status = BPF_STACK_BUILD_ID_VALID;\n\t}\n\n\tif (!work) {\n\t\tmmap_read_unlock(current->mm);\n\t} else {\n\t\twork->mm = current->mm;\n\n\t\t/* The lock will be released once we're out of interrupt\n\t\t * context. Tell lockdep that we've released it now so\n\t\t * it doesn't complain that we forgot to release it.\n\t\t */\n\t\trwsem_release(&current->mm->mmap_lock.dep_map, _RET_IP_);\n\t\tirq_work_queue(&work->irq_work);\n\t}\n}\n\nstatic struct perf_callchain_entry *\nget_callchain_entry_for_task(struct task_struct *task, u32 init_nr)\n{\n#ifdef CONFIG_STACKTRACE\n\tstruct perf_callchain_entry *entry;\n\tint rctx;\n\n\tentry = get_callchain_entry(&rctx);\n\n\tif (!entry)\n\t\treturn NULL;\n\n\tentry->nr = init_nr +\n\t\tstack_trace_save_tsk(task, (unsigned long *)(entry->ip + init_nr),\n\t\t\t\t     sysctl_perf_event_max_stack - init_nr, 0);\n\n\t/* stack_trace_save_tsk() works on unsigned long array, while\n\t * perf_callchain_entry uses u64 array. For 32-bit systems, it is\n\t * necessary to fix this mismatch.\n\t */\n\tif (__BITS_PER_LONG != 64) {\n\t\tunsigned long *from = (unsigned long *) entry->ip;\n\t\tu64 *to = entry->ip;\n\t\tint i;\n\n\t\t/* copy data from the end to avoid using extra buffer */\n\t\tfor (i = entry->nr - 1; i >= (int)init_nr; i--)\n\t\t\tto[i] = (u64)(from[i]);\n\t}\n\n\tput_callchain_entry(rctx);\n\n\treturn entry;\n#else /* CONFIG_STACKTRACE */\n\treturn NULL;\n#endif\n}\n\nstatic long __bpf_get_stackid(struct bpf_map *map,\n\t\t\t      struct perf_callchain_entry *trace, u64 flags)\n{\n\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);\n\tstruct stack_map_bucket *bucket, *new_bucket, *old_bucket;\n\tu32 max_depth = map->value_size / stack_map_data_size(map);\n\t/* stack_map_alloc() checks that max_depth <= sysctl_perf_event_max_stack */\n\tu32 init_nr = sysctl_perf_event_max_stack - max_depth;\n\tu32 skip = flags & BPF_F_SKIP_FIELD_MASK;\n\tu32 hash, id, trace_nr, trace_len;\n\tbool user = flags & BPF_F_USER_STACK;\n\tu64 *ips;\n\tbool hash_matches;\n\n\t/* get_perf_callchain() guarantees that trace->nr >= init_nr\n\t * and trace-nr <= sysctl_perf_event_max_stack, so trace_nr <= max_depth\n\t */\n\ttrace_nr = trace->nr - init_nr;\n\n\tif (trace_nr <= skip)\n\t\t/* skipping more than usable stack trace */\n\t\treturn -EFAULT;\n\n\ttrace_nr -= skip;\n\ttrace_len = trace_nr * sizeof(u64);\n\tips = trace->ip + skip + init_nr;\n\thash = jhash2((u32 *)ips, trace_len / sizeof(u32), 0);\n\tid = hash & (smap->n_buckets - 1);\n\tbucket = READ_ONCE(smap->buckets[id]);\n\n\thash_matches = bucket && bucket->hash == hash;\n\t/* fast cmp */\n\tif (hash_matches && flags & BPF_F_FAST_STACK_CMP)\n\t\treturn id;\n\n\tif (stack_map_use_build_id(map)) {\n\t\t/* for build_id+offset, pop a bucket before slow cmp */\n\t\tnew_bucket = (struct stack_map_bucket *)\n\t\t\tpcpu_freelist_pop(&smap->freelist);\n\t\tif (unlikely(!new_bucket))\n\t\t\treturn -ENOMEM;\n\t\tnew_bucket->nr = trace_nr;\n\t\tstack_map_get_build_id_offset(\n\t\t\t(struct bpf_stack_build_id *)new_bucket->data,\n\t\t\tips, trace_nr, user);\n\t\ttrace_len = trace_nr * sizeof(struct bpf_stack_build_id);\n\t\tif (hash_matches && bucket->nr == trace_nr &&\n\t\t    memcmp(bucket->data, new_bucket->data, trace_len) == 0) {\n\t\t\tpcpu_freelist_push(&smap->freelist, &new_bucket->fnode);\n\t\t\treturn id;\n\t\t}\n\t\tif (bucket && !(flags & BPF_F_REUSE_STACKID)) {\n\t\t\tpcpu_freelist_push(&smap->freelist, &new_bucket->fnode);\n\t\t\treturn -EEXIST;\n\t\t}\n\t} else {\n\t\tif (hash_matches && bucket->nr == trace_nr &&\n\t\t    memcmp(bucket->data, ips, trace_len) == 0)\n\t\t\treturn id;\n\t\tif (bucket && !(flags & BPF_F_REUSE_STACKID))\n\t\t\treturn -EEXIST;\n\n\t\tnew_bucket = (struct stack_map_bucket *)\n\t\t\tpcpu_freelist_pop(&smap->freelist);\n\t\tif (unlikely(!new_bucket))\n\t\t\treturn -ENOMEM;\n\t\tmemcpy(new_bucket->data, ips, trace_len);\n\t}\n\n\tnew_bucket->hash = hash;\n\tnew_bucket->nr = trace_nr;\n\n\told_bucket = xchg(&smap->buckets[id], new_bucket);\n\tif (old_bucket)\n\t\tpcpu_freelist_push(&smap->freelist, &old_bucket->fnode);\n\treturn id;\n}\n\nBPF_CALL_3(bpf_get_stackid, struct pt_regs *, regs, struct bpf_map *, map,\n\t   u64, flags)\n{\n\tu32 max_depth = map->value_size / stack_map_data_size(map);\n\t/* stack_map_alloc() checks that max_depth <= sysctl_perf_event_max_stack */\n\tu32 init_nr = sysctl_perf_event_max_stack - max_depth;\n\tbool user = flags & BPF_F_USER_STACK;\n\tstruct perf_callchain_entry *trace;\n\tbool kernel = !user;\n\n\tif (unlikely(flags & ~(BPF_F_SKIP_FIELD_MASK | BPF_F_USER_STACK |\n\t\t\t       BPF_F_FAST_STACK_CMP | BPF_F_REUSE_STACKID)))\n\t\treturn -EINVAL;\n\n\ttrace = get_perf_callchain(regs, init_nr, kernel, user,\n\t\t\t\t   sysctl_perf_event_max_stack, false, false);\n\n\tif (unlikely(!trace))\n\t\t/* couldn't fetch the stack trace */\n\t\treturn -EFAULT;\n\n\treturn __bpf_get_stackid(map, trace, flags);\n}\n\nconst struct bpf_func_proto bpf_get_stackid_proto = {\n\t.func\t\t= bpf_get_stackid,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nstatic __u64 count_kernel_ip(struct perf_callchain_entry *trace)\n{\n\t__u64 nr_kernel = 0;\n\n\twhile (nr_kernel < trace->nr) {\n\t\tif (trace->ip[nr_kernel] == PERF_CONTEXT_USER)\n\t\t\tbreak;\n\t\tnr_kernel++;\n\t}\n\treturn nr_kernel;\n}\n\nBPF_CALL_3(bpf_get_stackid_pe, struct bpf_perf_event_data_kern *, ctx,\n\t   struct bpf_map *, map, u64, flags)\n{\n\tstruct perf_event *event = ctx->event;\n\tstruct perf_callchain_entry *trace;\n\tbool kernel, user;\n\t__u64 nr_kernel;\n\tint ret;\n\n\t/* perf_sample_data doesn't have callchain, use bpf_get_stackid */\n\tif (!(event->attr.sample_type & __PERF_SAMPLE_CALLCHAIN_EARLY))\n\t\treturn bpf_get_stackid((unsigned long)(ctx->regs),\n\t\t\t\t       (unsigned long) map, flags, 0, 0);\n\n\tif (unlikely(flags & ~(BPF_F_SKIP_FIELD_MASK | BPF_F_USER_STACK |\n\t\t\t       BPF_F_FAST_STACK_CMP | BPF_F_REUSE_STACKID)))\n\t\treturn -EINVAL;\n\n\tuser = flags & BPF_F_USER_STACK;\n\tkernel = !user;\n\n\ttrace = ctx->data->callchain;\n\tif (unlikely(!trace))\n\t\treturn -EFAULT;\n\n\tnr_kernel = count_kernel_ip(trace);\n\n\tif (kernel) {\n\t\t__u64 nr = trace->nr;\n\n\t\ttrace->nr = nr_kernel;\n\t\tret = __bpf_get_stackid(map, trace, flags);\n\n\t\t/* restore nr */\n\t\ttrace->nr = nr;\n\t} else { /* user */\n\t\tu64 skip = flags & BPF_F_SKIP_FIELD_MASK;\n\n\t\tskip += nr_kernel;\n\t\tif (skip > BPF_F_SKIP_FIELD_MASK)\n\t\t\treturn -EFAULT;\n\n\t\tflags = (flags & ~BPF_F_SKIP_FIELD_MASK) | skip;\n\t\tret = __bpf_get_stackid(map, trace, flags);\n\t}\n\treturn ret;\n}\n\nconst struct bpf_func_proto bpf_get_stackid_proto_pe = {\n\t.func\t\t= bpf_get_stackid_pe,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nstatic long __bpf_get_stack(struct pt_regs *regs, struct task_struct *task,\n\t\t\t    struct perf_callchain_entry *trace_in,\n\t\t\t    void *buf, u32 size, u64 flags)\n{\n\tu32 init_nr, trace_nr, copy_len, elem_size, num_elem;\n\tbool user_build_id = flags & BPF_F_USER_BUILD_ID;\n\tu32 skip = flags & BPF_F_SKIP_FIELD_MASK;\n\tbool user = flags & BPF_F_USER_STACK;\n\tstruct perf_callchain_entry *trace;\n\tbool kernel = !user;\n\tint err = -EINVAL;\n\tu64 *ips;\n\n\tif (unlikely(flags & ~(BPF_F_SKIP_FIELD_MASK | BPF_F_USER_STACK |\n\t\t\t       BPF_F_USER_BUILD_ID)))\n\t\tgoto clear;\n\tif (kernel && user_build_id)\n\t\tgoto clear;\n\n\telem_size = (user && user_build_id) ? sizeof(struct bpf_stack_build_id)\n\t\t\t\t\t    : sizeof(u64);\n\tif (unlikely(size % elem_size))\n\t\tgoto clear;\n\n\t/* cannot get valid user stack for task without user_mode regs */\n\tif (task && user && !user_mode(regs))\n\t\tgoto err_fault;\n\n\tnum_elem = size / elem_size;\n\tif (sysctl_perf_event_max_stack < num_elem)\n\t\tinit_nr = 0;\n\telse\n\t\tinit_nr = sysctl_perf_event_max_stack - num_elem;\n\n\tif (trace_in)\n\t\ttrace = trace_in;\n\telse if (kernel && task)\n\t\ttrace = get_callchain_entry_for_task(task, init_nr);\n\telse\n\t\ttrace = get_perf_callchain(regs, init_nr, kernel, user,\n\t\t\t\t\t   sysctl_perf_event_max_stack,\n\t\t\t\t\t   false, false);\n\tif (unlikely(!trace))\n\t\tgoto err_fault;\n\n\ttrace_nr = trace->nr - init_nr;\n\tif (trace_nr < skip)\n\t\tgoto err_fault;\n\n\ttrace_nr -= skip;\n\ttrace_nr = (trace_nr <= num_elem) ? trace_nr : num_elem;\n\tcopy_len = trace_nr * elem_size;\n\tips = trace->ip + skip + init_nr;\n\tif (user && user_build_id)\n\t\tstack_map_get_build_id_offset(buf, ips, trace_nr, user);\n\telse\n\t\tmemcpy(buf, ips, copy_len);\n\n\tif (size > copy_len)\n\t\tmemset(buf + copy_len, 0, size - copy_len);\n\treturn copy_len;\n\nerr_fault:\n\terr = -EFAULT;\nclear:\n\tmemset(buf, 0, size);\n\treturn err;\n}\n\nBPF_CALL_4(bpf_get_stack, struct pt_regs *, regs, void *, buf, u32, size,\n\t   u64, flags)\n{\n\treturn __bpf_get_stack(regs, NULL, NULL, buf, size, flags);\n}\n\nconst struct bpf_func_proto bpf_get_stack_proto = {\n\t.func\t\t= bpf_get_stack,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_get_task_stack, struct task_struct *, task, void *, buf,\n\t   u32, size, u64, flags)\n{\n\tstruct pt_regs *regs;\n\tlong res;\n\n\tif (!try_get_task_stack(task))\n\t\treturn -EFAULT;\n\n\tregs = task_pt_regs(task);\n\tres = __bpf_get_stack(regs, task, NULL, buf, size, flags);\n\tput_task_stack(task);\n\n\treturn res;\n}\n\nconst struct bpf_func_proto bpf_get_task_stack_proto = {\n\t.func\t\t= bpf_get_task_stack,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID,\n\t.arg1_btf_id\t= &btf_task_struct_ids[0],\n\t.arg2_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_get_stack_pe, struct bpf_perf_event_data_kern *, ctx,\n\t   void *, buf, u32, size, u64, flags)\n{\n\tstruct pt_regs *regs = (struct pt_regs *)(ctx->regs);\n\tstruct perf_event *event = ctx->event;\n\tstruct perf_callchain_entry *trace;\n\tbool kernel, user;\n\tint err = -EINVAL;\n\t__u64 nr_kernel;\n\n\tif (!(event->attr.sample_type & __PERF_SAMPLE_CALLCHAIN_EARLY))\n\t\treturn __bpf_get_stack(regs, NULL, NULL, buf, size, flags);\n\n\tif (unlikely(flags & ~(BPF_F_SKIP_FIELD_MASK | BPF_F_USER_STACK |\n\t\t\t       BPF_F_USER_BUILD_ID)))\n\t\tgoto clear;\n\n\tuser = flags & BPF_F_USER_STACK;\n\tkernel = !user;\n\n\terr = -EFAULT;\n\ttrace = ctx->data->callchain;\n\tif (unlikely(!trace))\n\t\tgoto clear;\n\n\tnr_kernel = count_kernel_ip(trace);\n\n\tif (kernel) {\n\t\t__u64 nr = trace->nr;\n\n\t\ttrace->nr = nr_kernel;\n\t\terr = __bpf_get_stack(regs, NULL, trace, buf, size, flags);\n\n\t\t/* restore nr */\n\t\ttrace->nr = nr;\n\t} else { /* user */\n\t\tu64 skip = flags & BPF_F_SKIP_FIELD_MASK;\n\n\t\tskip += nr_kernel;\n\t\tif (skip > BPF_F_SKIP_FIELD_MASK)\n\t\t\tgoto clear;\n\n\t\tflags = (flags & ~BPF_F_SKIP_FIELD_MASK) | skip;\n\t\terr = __bpf_get_stack(regs, NULL, trace, buf, size, flags);\n\t}\n\treturn err;\n\nclear:\n\tmemset(buf, 0, size);\n\treturn err;\n\n}\n\nconst struct bpf_func_proto bpf_get_stack_proto_pe = {\n\t.func\t\t= bpf_get_stack_pe,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\n/* Called from eBPF program */\nstatic void *stack_map_lookup_elem(struct bpf_map *map, void *key)\n{\n\treturn ERR_PTR(-EOPNOTSUPP);\n}\n\n/* Called from syscall */\nint bpf_stackmap_copy(struct bpf_map *map, void *key, void *value)\n{\n\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);\n\tstruct stack_map_bucket *bucket, *old_bucket;\n\tu32 id = *(u32 *)key, trace_len;\n\n\tif (unlikely(id >= smap->n_buckets))\n\t\treturn -ENOENT;\n\n\tbucket = xchg(&smap->buckets[id], NULL);\n\tif (!bucket)\n\t\treturn -ENOENT;\n\n\ttrace_len = bucket->nr * stack_map_data_size(map);\n\tmemcpy(value, bucket->data, trace_len);\n\tmemset(value + trace_len, 0, map->value_size - trace_len);\n\n\told_bucket = xchg(&smap->buckets[id], bucket);\n\tif (old_bucket)\n\t\tpcpu_freelist_push(&smap->freelist, &old_bucket->fnode);\n\treturn 0;\n}\n\nstatic int stack_map_get_next_key(struct bpf_map *map, void *key,\n\t\t\t\t  void *next_key)\n{\n\tstruct bpf_stack_map *smap = container_of(map,\n\t\t\t\t\t\t  struct bpf_stack_map, map);\n\tu32 id;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tif (!key) {\n\t\tid = 0;\n\t} else {\n\t\tid = *(u32 *)key;\n\t\tif (id >= smap->n_buckets || !smap->buckets[id])\n\t\t\tid = 0;\n\t\telse\n\t\t\tid++;\n\t}\n\n\twhile (id < smap->n_buckets && !smap->buckets[id])\n\t\tid++;\n\n\tif (id >= smap->n_buckets)\n\t\treturn -ENOENT;\n\n\t*(u32 *)next_key = id;\n\treturn 0;\n}\n\nstatic int stack_map_update_elem(struct bpf_map *map, void *key, void *value,\n\t\t\t\t u64 map_flags)\n{\n\treturn -EINVAL;\n}\n\n/* Called from syscall or from eBPF program */\nstatic int stack_map_delete_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);\n\tstruct stack_map_bucket *old_bucket;\n\tu32 id = *(u32 *)key;\n\n\tif (unlikely(id >= smap->n_buckets))\n\t\treturn -E2BIG;\n\n\told_bucket = xchg(&smap->buckets[id], NULL);\n\tif (old_bucket) {\n\t\tpcpu_freelist_push(&smap->freelist, &old_bucket->fnode);\n\t\treturn 0;\n\t} else {\n\t\treturn -ENOENT;\n\t}\n}\n\n/* Called when map->refcnt goes to zero, either from workqueue or from syscall */\nstatic void stack_map_free(struct bpf_map *map)\n{\n\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);\n\n\tbpf_map_area_free(smap->elems);\n\tpcpu_freelist_destroy(&smap->freelist);\n\tbpf_map_area_free(smap);\n\tput_callchain_buffers();\n}\n\nstatic int stack_trace_map_btf_id;\nconst struct bpf_map_ops stack_trace_map_ops = {\n\t.map_meta_equal = bpf_map_meta_equal,\n\t.map_alloc = stack_map_alloc,\n\t.map_free = stack_map_free,\n\t.map_get_next_key = stack_map_get_next_key,\n\t.map_lookup_elem = stack_map_lookup_elem,\n\t.map_update_elem = stack_map_update_elem,\n\t.map_delete_elem = stack_map_delete_elem,\n\t.map_check_btf = map_check_no_btf,\n\t.map_btf_name = \"bpf_stack_map\",\n\t.map_btf_id = &stack_trace_map_btf_id,\n};\n\nstatic int __init stack_map_init(void)\n{\n\tint cpu;\n\tstruct stack_map_irq_work *work;\n\n\tfor_each_possible_cpu(cpu) {\n\t\twork = per_cpu_ptr(&up_read_work, cpu);\n\t\tinit_irq_work(&work->irq_work, do_up_read);\n\t}\n\treturn 0;\n}\nsubsys_initcall(stack_map_init);\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/* Copyright (c) 2016 Facebook\n */\n#include <linux/bpf.h>\n#include <linux/jhash.h>\n#include <linux/filter.h>\n#include <linux/kernel.h>\n#include <linux/stacktrace.h>\n#include <linux/perf_event.h>\n#include <linux/irq_work.h>\n#include <linux/btf_ids.h>\n#include <linux/buildid.h>\n#include \"percpu_freelist.h\"\n\n#define STACK_CREATE_FLAG_MASK\t\t\t\t\t\\\n\t(BPF_F_NUMA_NODE | BPF_F_RDONLY | BPF_F_WRONLY |\t\\\n\t BPF_F_STACK_BUILD_ID)\n\nstruct stack_map_bucket {\n\tstruct pcpu_freelist_node fnode;\n\tu32 hash;\n\tu32 nr;\n\tu64 data[];\n};\n\nstruct bpf_stack_map {\n\tstruct bpf_map map;\n\tvoid *elems;\n\tstruct pcpu_freelist freelist;\n\tu32 n_buckets;\n\tstruct stack_map_bucket *buckets[];\n};\n\n/* irq_work to run up_read() for build_id lookup in nmi context */\nstruct stack_map_irq_work {\n\tstruct irq_work irq_work;\n\tstruct mm_struct *mm;\n};\n\nstatic void do_up_read(struct irq_work *entry)\n{\n\tstruct stack_map_irq_work *work;\n\n\tif (WARN_ON_ONCE(IS_ENABLED(CONFIG_PREEMPT_RT)))\n\t\treturn;\n\n\twork = container_of(entry, struct stack_map_irq_work, irq_work);\n\tmmap_read_unlock_non_owner(work->mm);\n}\n\nstatic DEFINE_PER_CPU(struct stack_map_irq_work, up_read_work);\n\nstatic inline bool stack_map_use_build_id(struct bpf_map *map)\n{\n\treturn (map->map_flags & BPF_F_STACK_BUILD_ID);\n}\n\nstatic inline int stack_map_data_size(struct bpf_map *map)\n{\n\treturn stack_map_use_build_id(map) ?\n\t\tsizeof(struct bpf_stack_build_id) : sizeof(u64);\n}\n\nstatic int prealloc_elems_and_freelist(struct bpf_stack_map *smap)\n{\n\tu64 elem_size = sizeof(struct stack_map_bucket) +\n\t\t\t(u64)smap->map.value_size;\n\tint err;\n\n\tsmap->elems = bpf_map_area_alloc(elem_size * smap->map.max_entries,\n\t\t\t\t\t smap->map.numa_node);\n\tif (!smap->elems)\n\t\treturn -ENOMEM;\n\n\terr = pcpu_freelist_init(&smap->freelist);\n\tif (err)\n\t\tgoto free_elems;\n\n\tpcpu_freelist_populate(&smap->freelist, smap->elems, elem_size,\n\t\t\t       smap->map.max_entries);\n\treturn 0;\n\nfree_elems:\n\tbpf_map_area_free(smap->elems);\n\treturn err;\n}\n\n/* Called from syscall */\nstatic struct bpf_map *stack_map_alloc(union bpf_attr *attr)\n{\n\tu32 value_size = attr->value_size;\n\tstruct bpf_stack_map *smap;\n\tu64 cost, n_buckets;\n\tint err;\n\n\tif (!bpf_capable())\n\t\treturn ERR_PTR(-EPERM);\n\n\tif (attr->map_flags & ~STACK_CREATE_FLAG_MASK)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/* check sanity of attributes */\n\tif (attr->max_entries == 0 || attr->key_size != 4 ||\n\t    value_size < 8 || value_size % 8)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tBUILD_BUG_ON(sizeof(struct bpf_stack_build_id) % sizeof(u64));\n\tif (attr->map_flags & BPF_F_STACK_BUILD_ID) {\n\t\tif (value_size % sizeof(struct bpf_stack_build_id) ||\n\t\t    value_size / sizeof(struct bpf_stack_build_id)\n\t\t    > sysctl_perf_event_max_stack)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t} else if (value_size / 8 > sysctl_perf_event_max_stack)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/* hash table size must be power of 2 */\n\tn_buckets = roundup_pow_of_two(attr->max_entries);\n\tif (!n_buckets)\n\t\treturn ERR_PTR(-E2BIG);\n\n\tcost = n_buckets * sizeof(struct stack_map_bucket *) + sizeof(*smap);\n\tcost += n_buckets * (value_size + sizeof(struct stack_map_bucket));\n\tsmap = bpf_map_area_alloc(cost, bpf_map_attr_numa_node(attr));\n\tif (!smap)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbpf_map_init_from_attr(&smap->map, attr);\n\tsmap->map.value_size = value_size;\n\tsmap->n_buckets = n_buckets;\n\n\terr = get_callchain_buffers(sysctl_perf_event_max_stack);\n\tif (err)\n\t\tgoto free_smap;\n\n\terr = prealloc_elems_and_freelist(smap);\n\tif (err)\n\t\tgoto put_buffers;\n\n\treturn &smap->map;\n\nput_buffers:\n\tput_callchain_buffers();\nfree_smap:\n\tbpf_map_area_free(smap);\n\treturn ERR_PTR(err);\n}\n\nstatic void stack_map_get_build_id_offset(struct bpf_stack_build_id *id_offs,\n\t\t\t\t\t  u64 *ips, u32 trace_nr, bool user)\n{\n\tint i;\n\tstruct vm_area_struct *vma;\n\tbool irq_work_busy = false;\n\tstruct stack_map_irq_work *work = NULL;\n\n\tif (irqs_disabled()) {\n\t\tif (!IS_ENABLED(CONFIG_PREEMPT_RT)) {\n\t\t\twork = this_cpu_ptr(&up_read_work);\n\t\t\tif (irq_work_is_busy(&work->irq_work)) {\n\t\t\t\t/* cannot queue more up_read, fallback */\n\t\t\t\tirq_work_busy = true;\n\t\t\t}\n\t\t} else {\n\t\t\t/*\n\t\t\t * PREEMPT_RT does not allow to trylock mmap sem in\n\t\t\t * interrupt disabled context. Force the fallback code.\n\t\t\t */\n\t\t\tirq_work_busy = true;\n\t\t}\n\t}\n\n\t/*\n\t * We cannot do up_read() when the irq is disabled, because of\n\t * risk to deadlock with rq_lock. To do build_id lookup when the\n\t * irqs are disabled, we need to run up_read() in irq_work. We use\n\t * a percpu variable to do the irq_work. If the irq_work is\n\t * already used by another lookup, we fall back to report ips.\n\t *\n\t * Same fallback is used for kernel stack (!user) on a stackmap\n\t * with build_id.\n\t */\n\tif (!user || !current || !current->mm || irq_work_busy ||\n\t    !mmap_read_trylock(current->mm)) {\n\t\t/* cannot access current->mm, fall back to ips */\n\t\tfor (i = 0; i < trace_nr; i++) {\n\t\t\tid_offs[i].status = BPF_STACK_BUILD_ID_IP;\n\t\t\tid_offs[i].ip = ips[i];\n\t\t\tmemset(id_offs[i].build_id, 0, BUILD_ID_SIZE_MAX);\n\t\t}\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < trace_nr; i++) {\n\t\tvma = find_vma(current->mm, ips[i]);\n\t\tif (!vma || build_id_parse(vma, id_offs[i].build_id, NULL)) {\n\t\t\t/* per entry fall back to ips */\n\t\t\tid_offs[i].status = BPF_STACK_BUILD_ID_IP;\n\t\t\tid_offs[i].ip = ips[i];\n\t\t\tmemset(id_offs[i].build_id, 0, BUILD_ID_SIZE_MAX);\n\t\t\tcontinue;\n\t\t}\n\t\tid_offs[i].offset = (vma->vm_pgoff << PAGE_SHIFT) + ips[i]\n\t\t\t- vma->vm_start;\n\t\tid_offs[i].status = BPF_STACK_BUILD_ID_VALID;\n\t}\n\n\tif (!work) {\n\t\tmmap_read_unlock(current->mm);\n\t} else {\n\t\twork->mm = current->mm;\n\n\t\t/* The lock will be released once we're out of interrupt\n\t\t * context. Tell lockdep that we've released it now so\n\t\t * it doesn't complain that we forgot to release it.\n\t\t */\n\t\trwsem_release(&current->mm->mmap_lock.dep_map, _RET_IP_);\n\t\tirq_work_queue(&work->irq_work);\n\t}\n}\n\nstatic struct perf_callchain_entry *\nget_callchain_entry_for_task(struct task_struct *task, u32 init_nr)\n{\n#ifdef CONFIG_STACKTRACE\n\tstruct perf_callchain_entry *entry;\n\tint rctx;\n\n\tentry = get_callchain_entry(&rctx);\n\n\tif (!entry)\n\t\treturn NULL;\n\n\tentry->nr = init_nr +\n\t\tstack_trace_save_tsk(task, (unsigned long *)(entry->ip + init_nr),\n\t\t\t\t     sysctl_perf_event_max_stack - init_nr, 0);\n\n\t/* stack_trace_save_tsk() works on unsigned long array, while\n\t * perf_callchain_entry uses u64 array. For 32-bit systems, it is\n\t * necessary to fix this mismatch.\n\t */\n\tif (__BITS_PER_LONG != 64) {\n\t\tunsigned long *from = (unsigned long *) entry->ip;\n\t\tu64 *to = entry->ip;\n\t\tint i;\n\n\t\t/* copy data from the end to avoid using extra buffer */\n\t\tfor (i = entry->nr - 1; i >= (int)init_nr; i--)\n\t\t\tto[i] = (u64)(from[i]);\n\t}\n\n\tput_callchain_entry(rctx);\n\n\treturn entry;\n#else /* CONFIG_STACKTRACE */\n\treturn NULL;\n#endif\n}\n\nstatic long __bpf_get_stackid(struct bpf_map *map,\n\t\t\t      struct perf_callchain_entry *trace, u64 flags)\n{\n\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);\n\tstruct stack_map_bucket *bucket, *new_bucket, *old_bucket;\n\tu32 max_depth = map->value_size / stack_map_data_size(map);\n\t/* stack_map_alloc() checks that max_depth <= sysctl_perf_event_max_stack */\n\tu32 init_nr = sysctl_perf_event_max_stack - max_depth;\n\tu32 skip = flags & BPF_F_SKIP_FIELD_MASK;\n\tu32 hash, id, trace_nr, trace_len;\n\tbool user = flags & BPF_F_USER_STACK;\n\tu64 *ips;\n\tbool hash_matches;\n\n\t/* get_perf_callchain() guarantees that trace->nr >= init_nr\n\t * and trace-nr <= sysctl_perf_event_max_stack, so trace_nr <= max_depth\n\t */\n\ttrace_nr = trace->nr - init_nr;\n\n\tif (trace_nr <= skip)\n\t\t/* skipping more than usable stack trace */\n\t\treturn -EFAULT;\n\n\ttrace_nr -= skip;\n\ttrace_len = trace_nr * sizeof(u64);\n\tips = trace->ip + skip + init_nr;\n\thash = jhash2((u32 *)ips, trace_len / sizeof(u32), 0);\n\tid = hash & (smap->n_buckets - 1);\n\tbucket = READ_ONCE(smap->buckets[id]);\n\n\thash_matches = bucket && bucket->hash == hash;\n\t/* fast cmp */\n\tif (hash_matches && flags & BPF_F_FAST_STACK_CMP)\n\t\treturn id;\n\n\tif (stack_map_use_build_id(map)) {\n\t\t/* for build_id+offset, pop a bucket before slow cmp */\n\t\tnew_bucket = (struct stack_map_bucket *)\n\t\t\tpcpu_freelist_pop(&smap->freelist);\n\t\tif (unlikely(!new_bucket))\n\t\t\treturn -ENOMEM;\n\t\tnew_bucket->nr = trace_nr;\n\t\tstack_map_get_build_id_offset(\n\t\t\t(struct bpf_stack_build_id *)new_bucket->data,\n\t\t\tips, trace_nr, user);\n\t\ttrace_len = trace_nr * sizeof(struct bpf_stack_build_id);\n\t\tif (hash_matches && bucket->nr == trace_nr &&\n\t\t    memcmp(bucket->data, new_bucket->data, trace_len) == 0) {\n\t\t\tpcpu_freelist_push(&smap->freelist, &new_bucket->fnode);\n\t\t\treturn id;\n\t\t}\n\t\tif (bucket && !(flags & BPF_F_REUSE_STACKID)) {\n\t\t\tpcpu_freelist_push(&smap->freelist, &new_bucket->fnode);\n\t\t\treturn -EEXIST;\n\t\t}\n\t} else {\n\t\tif (hash_matches && bucket->nr == trace_nr &&\n\t\t    memcmp(bucket->data, ips, trace_len) == 0)\n\t\t\treturn id;\n\t\tif (bucket && !(flags & BPF_F_REUSE_STACKID))\n\t\t\treturn -EEXIST;\n\n\t\tnew_bucket = (struct stack_map_bucket *)\n\t\t\tpcpu_freelist_pop(&smap->freelist);\n\t\tif (unlikely(!new_bucket))\n\t\t\treturn -ENOMEM;\n\t\tmemcpy(new_bucket->data, ips, trace_len);\n\t}\n\n\tnew_bucket->hash = hash;\n\tnew_bucket->nr = trace_nr;\n\n\told_bucket = xchg(&smap->buckets[id], new_bucket);\n\tif (old_bucket)\n\t\tpcpu_freelist_push(&smap->freelist, &old_bucket->fnode);\n\treturn id;\n}\n\nBPF_CALL_3(bpf_get_stackid, struct pt_regs *, regs, struct bpf_map *, map,\n\t   u64, flags)\n{\n\tu32 max_depth = map->value_size / stack_map_data_size(map);\n\t/* stack_map_alloc() checks that max_depth <= sysctl_perf_event_max_stack */\n\tu32 init_nr = sysctl_perf_event_max_stack - max_depth;\n\tbool user = flags & BPF_F_USER_STACK;\n\tstruct perf_callchain_entry *trace;\n\tbool kernel = !user;\n\n\tif (unlikely(flags & ~(BPF_F_SKIP_FIELD_MASK | BPF_F_USER_STACK |\n\t\t\t       BPF_F_FAST_STACK_CMP | BPF_F_REUSE_STACKID)))\n\t\treturn -EINVAL;\n\n\ttrace = get_perf_callchain(regs, init_nr, kernel, user,\n\t\t\t\t   sysctl_perf_event_max_stack, false, false);\n\n\tif (unlikely(!trace))\n\t\t/* couldn't fetch the stack trace */\n\t\treturn -EFAULT;\n\n\treturn __bpf_get_stackid(map, trace, flags);\n}\n\nconst struct bpf_func_proto bpf_get_stackid_proto = {\n\t.func\t\t= bpf_get_stackid,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nstatic __u64 count_kernel_ip(struct perf_callchain_entry *trace)\n{\n\t__u64 nr_kernel = 0;\n\n\twhile (nr_kernel < trace->nr) {\n\t\tif (trace->ip[nr_kernel] == PERF_CONTEXT_USER)\n\t\t\tbreak;\n\t\tnr_kernel++;\n\t}\n\treturn nr_kernel;\n}\n\nBPF_CALL_3(bpf_get_stackid_pe, struct bpf_perf_event_data_kern *, ctx,\n\t   struct bpf_map *, map, u64, flags)\n{\n\tstruct perf_event *event = ctx->event;\n\tstruct perf_callchain_entry *trace;\n\tbool kernel, user;\n\t__u64 nr_kernel;\n\tint ret;\n\n\t/* perf_sample_data doesn't have callchain, use bpf_get_stackid */\n\tif (!(event->attr.sample_type & __PERF_SAMPLE_CALLCHAIN_EARLY))\n\t\treturn bpf_get_stackid((unsigned long)(ctx->regs),\n\t\t\t\t       (unsigned long) map, flags, 0, 0);\n\n\tif (unlikely(flags & ~(BPF_F_SKIP_FIELD_MASK | BPF_F_USER_STACK |\n\t\t\t       BPF_F_FAST_STACK_CMP | BPF_F_REUSE_STACKID)))\n\t\treturn -EINVAL;\n\n\tuser = flags & BPF_F_USER_STACK;\n\tkernel = !user;\n\n\ttrace = ctx->data->callchain;\n\tif (unlikely(!trace))\n\t\treturn -EFAULT;\n\n\tnr_kernel = count_kernel_ip(trace);\n\n\tif (kernel) {\n\t\t__u64 nr = trace->nr;\n\n\t\ttrace->nr = nr_kernel;\n\t\tret = __bpf_get_stackid(map, trace, flags);\n\n\t\t/* restore nr */\n\t\ttrace->nr = nr;\n\t} else { /* user */\n\t\tu64 skip = flags & BPF_F_SKIP_FIELD_MASK;\n\n\t\tskip += nr_kernel;\n\t\tif (skip > BPF_F_SKIP_FIELD_MASK)\n\t\t\treturn -EFAULT;\n\n\t\tflags = (flags & ~BPF_F_SKIP_FIELD_MASK) | skip;\n\t\tret = __bpf_get_stackid(map, trace, flags);\n\t}\n\treturn ret;\n}\n\nconst struct bpf_func_proto bpf_get_stackid_proto_pe = {\n\t.func\t\t= bpf_get_stackid_pe,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nstatic long __bpf_get_stack(struct pt_regs *regs, struct task_struct *task,\n\t\t\t    struct perf_callchain_entry *trace_in,\n\t\t\t    void *buf, u32 size, u64 flags)\n{\n\tu32 init_nr, trace_nr, copy_len, elem_size, num_elem;\n\tbool user_build_id = flags & BPF_F_USER_BUILD_ID;\n\tu32 skip = flags & BPF_F_SKIP_FIELD_MASK;\n\tbool user = flags & BPF_F_USER_STACK;\n\tstruct perf_callchain_entry *trace;\n\tbool kernel = !user;\n\tint err = -EINVAL;\n\tu64 *ips;\n\n\tif (unlikely(flags & ~(BPF_F_SKIP_FIELD_MASK | BPF_F_USER_STACK |\n\t\t\t       BPF_F_USER_BUILD_ID)))\n\t\tgoto clear;\n\tif (kernel && user_build_id)\n\t\tgoto clear;\n\n\telem_size = (user && user_build_id) ? sizeof(struct bpf_stack_build_id)\n\t\t\t\t\t    : sizeof(u64);\n\tif (unlikely(size % elem_size))\n\t\tgoto clear;\n\n\t/* cannot get valid user stack for task without user_mode regs */\n\tif (task && user && !user_mode(regs))\n\t\tgoto err_fault;\n\n\tnum_elem = size / elem_size;\n\tif (sysctl_perf_event_max_stack < num_elem)\n\t\tinit_nr = 0;\n\telse\n\t\tinit_nr = sysctl_perf_event_max_stack - num_elem;\n\n\tif (trace_in)\n\t\ttrace = trace_in;\n\telse if (kernel && task)\n\t\ttrace = get_callchain_entry_for_task(task, init_nr);\n\telse\n\t\ttrace = get_perf_callchain(regs, init_nr, kernel, user,\n\t\t\t\t\t   sysctl_perf_event_max_stack,\n\t\t\t\t\t   false, false);\n\tif (unlikely(!trace))\n\t\tgoto err_fault;\n\n\ttrace_nr = trace->nr - init_nr;\n\tif (trace_nr < skip)\n\t\tgoto err_fault;\n\n\ttrace_nr -= skip;\n\ttrace_nr = (trace_nr <= num_elem) ? trace_nr : num_elem;\n\tcopy_len = trace_nr * elem_size;\n\tips = trace->ip + skip + init_nr;\n\tif (user && user_build_id)\n\t\tstack_map_get_build_id_offset(buf, ips, trace_nr, user);\n\telse\n\t\tmemcpy(buf, ips, copy_len);\n\n\tif (size > copy_len)\n\t\tmemset(buf + copy_len, 0, size - copy_len);\n\treturn copy_len;\n\nerr_fault:\n\terr = -EFAULT;\nclear:\n\tmemset(buf, 0, size);\n\treturn err;\n}\n\nBPF_CALL_4(bpf_get_stack, struct pt_regs *, regs, void *, buf, u32, size,\n\t   u64, flags)\n{\n\treturn __bpf_get_stack(regs, NULL, NULL, buf, size, flags);\n}\n\nconst struct bpf_func_proto bpf_get_stack_proto = {\n\t.func\t\t= bpf_get_stack,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_get_task_stack, struct task_struct *, task, void *, buf,\n\t   u32, size, u64, flags)\n{\n\tstruct pt_regs *regs;\n\tlong res;\n\n\tif (!try_get_task_stack(task))\n\t\treturn -EFAULT;\n\n\tregs = task_pt_regs(task);\n\tres = __bpf_get_stack(regs, task, NULL, buf, size, flags);\n\tput_task_stack(task);\n\n\treturn res;\n}\n\nconst struct bpf_func_proto bpf_get_task_stack_proto = {\n\t.func\t\t= bpf_get_task_stack,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID,\n\t.arg1_btf_id\t= &btf_task_struct_ids[0],\n\t.arg2_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_get_stack_pe, struct bpf_perf_event_data_kern *, ctx,\n\t   void *, buf, u32, size, u64, flags)\n{\n\tstruct pt_regs *regs = (struct pt_regs *)(ctx->regs);\n\tstruct perf_event *event = ctx->event;\n\tstruct perf_callchain_entry *trace;\n\tbool kernel, user;\n\tint err = -EINVAL;\n\t__u64 nr_kernel;\n\n\tif (!(event->attr.sample_type & __PERF_SAMPLE_CALLCHAIN_EARLY))\n\t\treturn __bpf_get_stack(regs, NULL, NULL, buf, size, flags);\n\n\tif (unlikely(flags & ~(BPF_F_SKIP_FIELD_MASK | BPF_F_USER_STACK |\n\t\t\t       BPF_F_USER_BUILD_ID)))\n\t\tgoto clear;\n\n\tuser = flags & BPF_F_USER_STACK;\n\tkernel = !user;\n\n\terr = -EFAULT;\n\ttrace = ctx->data->callchain;\n\tif (unlikely(!trace))\n\t\tgoto clear;\n\n\tnr_kernel = count_kernel_ip(trace);\n\n\tif (kernel) {\n\t\t__u64 nr = trace->nr;\n\n\t\ttrace->nr = nr_kernel;\n\t\terr = __bpf_get_stack(regs, NULL, trace, buf, size, flags);\n\n\t\t/* restore nr */\n\t\ttrace->nr = nr;\n\t} else { /* user */\n\t\tu64 skip = flags & BPF_F_SKIP_FIELD_MASK;\n\n\t\tskip += nr_kernel;\n\t\tif (skip > BPF_F_SKIP_FIELD_MASK)\n\t\t\tgoto clear;\n\n\t\tflags = (flags & ~BPF_F_SKIP_FIELD_MASK) | skip;\n\t\terr = __bpf_get_stack(regs, NULL, trace, buf, size, flags);\n\t}\n\treturn err;\n\nclear:\n\tmemset(buf, 0, size);\n\treturn err;\n\n}\n\nconst struct bpf_func_proto bpf_get_stack_proto_pe = {\n\t.func\t\t= bpf_get_stack_pe,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\n/* Called from eBPF program */\nstatic void *stack_map_lookup_elem(struct bpf_map *map, void *key)\n{\n\treturn ERR_PTR(-EOPNOTSUPP);\n}\n\n/* Called from syscall */\nint bpf_stackmap_copy(struct bpf_map *map, void *key, void *value)\n{\n\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);\n\tstruct stack_map_bucket *bucket, *old_bucket;\n\tu32 id = *(u32 *)key, trace_len;\n\n\tif (unlikely(id >= smap->n_buckets))\n\t\treturn -ENOENT;\n\n\tbucket = xchg(&smap->buckets[id], NULL);\n\tif (!bucket)\n\t\treturn -ENOENT;\n\n\ttrace_len = bucket->nr * stack_map_data_size(map);\n\tmemcpy(value, bucket->data, trace_len);\n\tmemset(value + trace_len, 0, map->value_size - trace_len);\n\n\told_bucket = xchg(&smap->buckets[id], bucket);\n\tif (old_bucket)\n\t\tpcpu_freelist_push(&smap->freelist, &old_bucket->fnode);\n\treturn 0;\n}\n\nstatic int stack_map_get_next_key(struct bpf_map *map, void *key,\n\t\t\t\t  void *next_key)\n{\n\tstruct bpf_stack_map *smap = container_of(map,\n\t\t\t\t\t\t  struct bpf_stack_map, map);\n\tu32 id;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tif (!key) {\n\t\tid = 0;\n\t} else {\n\t\tid = *(u32 *)key;\n\t\tif (id >= smap->n_buckets || !smap->buckets[id])\n\t\t\tid = 0;\n\t\telse\n\t\t\tid++;\n\t}\n\n\twhile (id < smap->n_buckets && !smap->buckets[id])\n\t\tid++;\n\n\tif (id >= smap->n_buckets)\n\t\treturn -ENOENT;\n\n\t*(u32 *)next_key = id;\n\treturn 0;\n}\n\nstatic int stack_map_update_elem(struct bpf_map *map, void *key, void *value,\n\t\t\t\t u64 map_flags)\n{\n\treturn -EINVAL;\n}\n\n/* Called from syscall or from eBPF program */\nstatic int stack_map_delete_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);\n\tstruct stack_map_bucket *old_bucket;\n\tu32 id = *(u32 *)key;\n\n\tif (unlikely(id >= smap->n_buckets))\n\t\treturn -E2BIG;\n\n\told_bucket = xchg(&smap->buckets[id], NULL);\n\tif (old_bucket) {\n\t\tpcpu_freelist_push(&smap->freelist, &old_bucket->fnode);\n\t\treturn 0;\n\t} else {\n\t\treturn -ENOENT;\n\t}\n}\n\n/* Called when map->refcnt goes to zero, either from workqueue or from syscall */\nstatic void stack_map_free(struct bpf_map *map)\n{\n\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);\n\n\tbpf_map_area_free(smap->elems);\n\tpcpu_freelist_destroy(&smap->freelist);\n\tbpf_map_area_free(smap);\n\tput_callchain_buffers();\n}\n\nstatic int stack_trace_map_btf_id;\nconst struct bpf_map_ops stack_trace_map_ops = {\n\t.map_meta_equal = bpf_map_meta_equal,\n\t.map_alloc = stack_map_alloc,\n\t.map_free = stack_map_free,\n\t.map_get_next_key = stack_map_get_next_key,\n\t.map_lookup_elem = stack_map_lookup_elem,\n\t.map_update_elem = stack_map_update_elem,\n\t.map_delete_elem = stack_map_delete_elem,\n\t.map_check_btf = map_check_no_btf,\n\t.map_btf_name = \"bpf_stack_map\",\n\t.map_btf_id = &stack_trace_map_btf_id,\n};\n\nstatic int __init stack_map_init(void)\n{\n\tint cpu;\n\tstruct stack_map_irq_work *work;\n\n\tfor_each_possible_cpu(cpu) {\n\t\twork = per_cpu_ptr(&up_read_work, cpu);\n\t\tinit_irq_work(&work->irq_work, do_up_read);\n\t}\n\treturn 0;\n}\nsubsys_initcall(stack_map_init);\n"], "filenames": ["kernel/bpf/stackmap.c"], "buggy_code_start_loc": [66], "buggy_code_end_loc": [67], "fixing_code_start_loc": [66], "fixing_code_end_loc": [68], "type": "CWE-190", "message": "prealloc_elems_and_freelist in kernel/bpf/stackmap.c in the Linux kernel before 5.14.12 allows unprivileged users to trigger an eBPF multiplication integer overflow with a resultant out-of-bounds write.", "other": {"cve": {"id": "CVE-2021-41864", "sourceIdentifier": "cve@mitre.org", "published": "2021-10-02T00:15:07.503", "lastModified": "2022-03-25T18:27:33.480", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "prealloc_elems_and_freelist in kernel/bpf/stackmap.c in the Linux kernel before 5.14.12 allows unprivileged users to trigger an eBPF multiplication integer overflow with a resultant out-of-bounds write."}, {"lang": "es", "value": "prealloc_elems_and_freelist en kernel/bpf/stackmap.c en el kernel de Linux antes de la versi\u00f3n 5.14.12 permite a usuarios sin privilegios desencadenar un desbordamiento de enteros en la multiplicaci\u00f3n de eBPF con una escritura fuera de los l\u00edmites resultante."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-190"}]}], "configurations": [{"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.14.12", "matchCriteriaId": "B9BB09ED-8714-4B35-B1F9-49BAE7D9BFE6"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:33:*:*:*:*:*:*:*", "matchCriteriaId": "E460AA51-FCDA-46B9-AE97-E6676AA5E194"}, {"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:34:*:*:*:*:*:*:*", "matchCriteriaId": "A930E247-0B43-43CB-98FF-6CE7B8189835"}, {"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:35:*:*:*:*:*:*:*", "matchCriteriaId": "80E516C0-98A4-4ADE-B69F-66A772E2BAAA"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:netapp:cloud_backup:-:*:*:*:*:*:*:*", "matchCriteriaId": "5C2089EE-5D7F-47EC-8EA5-0F69790564C4"}, {"vulnerable": true, "criteria": "cpe:2.3:a:netapp:hci_management_node:-:*:*:*:*:*:*:*", "matchCriteriaId": "A3C19813-E823-456A-B1CE-EC0684CE1953"}, {"vulnerable": true, "criteria": "cpe:2.3:a:netapp:solidfire:-:*:*:*:*:*:*:*", "matchCriteriaId": "A6E9EF0C-AFA8-4F7B-9FDC-1E0F7C26E737"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h410c_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "234DEFE0-5CE5-4B0A-96B8-5D227CB8ED31"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h410c:-:*:*:*:*:*:*:*", "matchCriteriaId": "CDDF61B7-EC5C-467C-B710-B89F502CD04F"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h300s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "6770B6C3-732E-4E22-BF1C-2D2FD610061C"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h300s:-:*:*:*:*:*:*:*", "matchCriteriaId": "9F9C8C20-42EB-4AB5-BD97-212DEB070C43"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h500s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "7FFF7106-ED78-49BA-9EC5-B889E3685D53"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h500s:-:*:*:*:*:*:*:*", "matchCriteriaId": "E63D8B0F-006E-4801-BF9D-1C001BBFB4F9"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h700s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "56409CEC-5A1E-4450-AA42-641E459CC2AF"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h700s:-:*:*:*:*:*:*:*", "matchCriteriaId": "B06F4839-D16A-4A61-9BB5-55B13F41E47F"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h300e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "108A2215-50FB-4074-94CF-C130FA14566D"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h300e:-:*:*:*:*:*:*:*", "matchCriteriaId": "7AFC73CE-ABB9-42D3-9A71-3F5BC5381E0E"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h500e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "32F0B6C0-F930-480D-962B-3F4EFDCC13C7"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h500e:-:*:*:*:*:*:*:*", "matchCriteriaId": "803BC414-B250-4E3A-A478-A3881340D6B8"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h700e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "0FEB3337-BFDE-462A-908B-176F92053CEC"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h700e:-:*:*:*:*:*:*:*", "matchCriteriaId": "736AEAE9-782B-4F71-9893-DED53367E102"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h410s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "D0B4AD8A-F172-4558-AEC6-FF424BA2D912"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h410s:-:*:*:*:*:*:*:*", "matchCriteriaId": "8497A4C9-8474-4A62-8331-3FE862ED4098"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:solidfire_baseboard_management_controller_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "FB9B8171-F6CA-427D-81E0-6536D3BBFA8D"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:solidfire_baseboard_management_controller:-:*:*:*:*:*:*:*", "matchCriteriaId": "090AA6F4-4404-4E26-82AB-C3A22636F276"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:10.0:*:*:*:*:*:*:*", "matchCriteriaId": "07B237A9-69A3-4A9C-9DA0-4E06BD37AE73"}]}]}], "references": [{"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.14.12", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf.git/commit/?id=30e29a9a2bc6a4888335a6ede968b75cd329657a", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/30e29a9a2bc6a4888335a6ede968b75cd329657a", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2021/12/msg00012.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2022/03/msg00012.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/7BLLVKYAIETEORUPTFO3TR3C33ZPFXQM/", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/LAT3RERO6QBKSPJBNNRWY3D4NCGTFOS7/", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/SYKURLXBB2555ASWMPDNMBUPD6AG2JKQ/", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20211029-0004/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.debian.org/security/2022/dsa-5096", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/30e29a9a2bc6a4888335a6ede968b75cd329657a"}}