{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/*\n * AMD Cryptographic Coprocessor (CCP) driver\n *\n * Copyright (C) 2013-2019 Advanced Micro Devices, Inc.\n *\n * Author: Tom Lendacky <thomas.lendacky@amd.com>\n * Author: Gary R Hook <gary.hook@amd.com>\n */\n\n#include <linux/dma-mapping.h>\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/interrupt.h>\n#include <crypto/scatterwalk.h>\n#include <crypto/des.h>\n#include <linux/ccp.h>\n\n#include \"ccp-dev.h\"\n\n/* SHA initial context values */\nstatic const __be32 ccp_sha1_init[SHA1_DIGEST_SIZE / sizeof(__be32)] = {\n\tcpu_to_be32(SHA1_H0), cpu_to_be32(SHA1_H1),\n\tcpu_to_be32(SHA1_H2), cpu_to_be32(SHA1_H3),\n\tcpu_to_be32(SHA1_H4),\n};\n\nstatic const __be32 ccp_sha224_init[SHA256_DIGEST_SIZE / sizeof(__be32)] = {\n\tcpu_to_be32(SHA224_H0), cpu_to_be32(SHA224_H1),\n\tcpu_to_be32(SHA224_H2), cpu_to_be32(SHA224_H3),\n\tcpu_to_be32(SHA224_H4), cpu_to_be32(SHA224_H5),\n\tcpu_to_be32(SHA224_H6), cpu_to_be32(SHA224_H7),\n};\n\nstatic const __be32 ccp_sha256_init[SHA256_DIGEST_SIZE / sizeof(__be32)] = {\n\tcpu_to_be32(SHA256_H0), cpu_to_be32(SHA256_H1),\n\tcpu_to_be32(SHA256_H2), cpu_to_be32(SHA256_H3),\n\tcpu_to_be32(SHA256_H4), cpu_to_be32(SHA256_H5),\n\tcpu_to_be32(SHA256_H6), cpu_to_be32(SHA256_H7),\n};\n\nstatic const __be64 ccp_sha384_init[SHA512_DIGEST_SIZE / sizeof(__be64)] = {\n\tcpu_to_be64(SHA384_H0), cpu_to_be64(SHA384_H1),\n\tcpu_to_be64(SHA384_H2), cpu_to_be64(SHA384_H3),\n\tcpu_to_be64(SHA384_H4), cpu_to_be64(SHA384_H5),\n\tcpu_to_be64(SHA384_H6), cpu_to_be64(SHA384_H7),\n};\n\nstatic const __be64 ccp_sha512_init[SHA512_DIGEST_SIZE / sizeof(__be64)] = {\n\tcpu_to_be64(SHA512_H0), cpu_to_be64(SHA512_H1),\n\tcpu_to_be64(SHA512_H2), cpu_to_be64(SHA512_H3),\n\tcpu_to_be64(SHA512_H4), cpu_to_be64(SHA512_H5),\n\tcpu_to_be64(SHA512_H6), cpu_to_be64(SHA512_H7),\n};\n\n#define\tCCP_NEW_JOBID(ccp)\t((ccp->vdata->version == CCP_VERSION(3, 0)) ? \\\n\t\t\t\t\tccp_gen_jobid(ccp) : 0)\n\nstatic u32 ccp_gen_jobid(struct ccp_device *ccp)\n{\n\treturn atomic_inc_return(&ccp->current_id) & CCP_JOBID_MASK;\n}\n\nstatic void ccp_sg_free(struct ccp_sg_workarea *wa)\n{\n\tif (wa->dma_count)\n\t\tdma_unmap_sg(wa->dma_dev, wa->dma_sg_head, wa->nents, wa->dma_dir);\n\n\twa->dma_count = 0;\n}\n\nstatic int ccp_init_sg_workarea(struct ccp_sg_workarea *wa, struct device *dev,\n\t\t\t\tstruct scatterlist *sg, u64 len,\n\t\t\t\tenum dma_data_direction dma_dir)\n{\n\tmemset(wa, 0, sizeof(*wa));\n\n\twa->sg = sg;\n\tif (!sg)\n\t\treturn 0;\n\n\twa->nents = sg_nents_for_len(sg, len);\n\tif (wa->nents < 0)\n\t\treturn wa->nents;\n\n\twa->bytes_left = len;\n\twa->sg_used = 0;\n\n\tif (len == 0)\n\t\treturn 0;\n\n\tif (dma_dir == DMA_NONE)\n\t\treturn 0;\n\n\twa->dma_sg = sg;\n\twa->dma_sg_head = sg;\n\twa->dma_dev = dev;\n\twa->dma_dir = dma_dir;\n\twa->dma_count = dma_map_sg(dev, sg, wa->nents, dma_dir);\n\tif (!wa->dma_count)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void ccp_update_sg_workarea(struct ccp_sg_workarea *wa, unsigned int len)\n{\n\tunsigned int nbytes = min_t(u64, len, wa->bytes_left);\n\tunsigned int sg_combined_len = 0;\n\n\tif (!wa->sg)\n\t\treturn;\n\n\twa->sg_used += nbytes;\n\twa->bytes_left -= nbytes;\n\tif (wa->sg_used == sg_dma_len(wa->dma_sg)) {\n\t\t/* Advance to the next DMA scatterlist entry */\n\t\twa->dma_sg = sg_next(wa->dma_sg);\n\n\t\t/* In the case that the DMA mapped scatterlist has entries\n\t\t * that have been merged, the non-DMA mapped scatterlist\n\t\t * must be advanced multiple times for each merged entry.\n\t\t * This ensures that the current non-DMA mapped entry\n\t\t * corresponds to the current DMA mapped entry.\n\t\t */\n\t\tdo {\n\t\t\tsg_combined_len += wa->sg->length;\n\t\t\twa->sg = sg_next(wa->sg);\n\t\t} while (wa->sg_used > sg_combined_len);\n\n\t\twa->sg_used = 0;\n\t}\n}\n\nstatic void ccp_dm_free(struct ccp_dm_workarea *wa)\n{\n\tif (wa->length <= CCP_DMAPOOL_MAX_SIZE) {\n\t\tif (wa->address)\n\t\t\tdma_pool_free(wa->dma_pool, wa->address,\n\t\t\t\t      wa->dma.address);\n\t} else {\n\t\tif (wa->dma.address)\n\t\t\tdma_unmap_single(wa->dev, wa->dma.address, wa->length,\n\t\t\t\t\t wa->dma.dir);\n\t\tkfree(wa->address);\n\t}\n\n\twa->address = NULL;\n\twa->dma.address = 0;\n}\n\nstatic int ccp_init_dm_workarea(struct ccp_dm_workarea *wa,\n\t\t\t\tstruct ccp_cmd_queue *cmd_q,\n\t\t\t\tunsigned int len,\n\t\t\t\tenum dma_data_direction dir)\n{\n\tmemset(wa, 0, sizeof(*wa));\n\n\tif (!len)\n\t\treturn 0;\n\n\twa->dev = cmd_q->ccp->dev;\n\twa->length = len;\n\n\tif (len <= CCP_DMAPOOL_MAX_SIZE) {\n\t\twa->dma_pool = cmd_q->dma_pool;\n\n\t\twa->address = dma_pool_zalloc(wa->dma_pool, GFP_KERNEL,\n\t\t\t\t\t     &wa->dma.address);\n\t\tif (!wa->address)\n\t\t\treturn -ENOMEM;\n\n\t\twa->dma.length = CCP_DMAPOOL_MAX_SIZE;\n\n\t} else {\n\t\twa->address = kzalloc(len, GFP_KERNEL);\n\t\tif (!wa->address)\n\t\t\treturn -ENOMEM;\n\n\t\twa->dma.address = dma_map_single(wa->dev, wa->address, len,\n\t\t\t\t\t\t dir);\n\t\tif (dma_mapping_error(wa->dev, wa->dma.address))\n\t\t\treturn -ENOMEM;\n\n\t\twa->dma.length = len;\n\t}\n\twa->dma.dir = dir;\n\n\treturn 0;\n}\n\nstatic int ccp_set_dm_area(struct ccp_dm_workarea *wa, unsigned int wa_offset,\n\t\t\t   struct scatterlist *sg, unsigned int sg_offset,\n\t\t\t   unsigned int len)\n{\n\tWARN_ON(!wa->address);\n\n\tif (len > (wa->length - wa_offset))\n\t\treturn -EINVAL;\n\n\tscatterwalk_map_and_copy(wa->address + wa_offset, sg, sg_offset, len,\n\t\t\t\t 0);\n\treturn 0;\n}\n\nstatic void ccp_get_dm_area(struct ccp_dm_workarea *wa, unsigned int wa_offset,\n\t\t\t    struct scatterlist *sg, unsigned int sg_offset,\n\t\t\t    unsigned int len)\n{\n\tWARN_ON(!wa->address);\n\n\tscatterwalk_map_and_copy(wa->address + wa_offset, sg, sg_offset, len,\n\t\t\t\t 1);\n}\n\nstatic int ccp_reverse_set_dm_area(struct ccp_dm_workarea *wa,\n\t\t\t\t   unsigned int wa_offset,\n\t\t\t\t   struct scatterlist *sg,\n\t\t\t\t   unsigned int sg_offset,\n\t\t\t\t   unsigned int len)\n{\n\tu8 *p, *q;\n\tint\trc;\n\n\trc = ccp_set_dm_area(wa, wa_offset, sg, sg_offset, len);\n\tif (rc)\n\t\treturn rc;\n\n\tp = wa->address + wa_offset;\n\tq = p + len - 1;\n\twhile (p < q) {\n\t\t*p = *p ^ *q;\n\t\t*q = *p ^ *q;\n\t\t*p = *p ^ *q;\n\t\tp++;\n\t\tq--;\n\t}\n\treturn 0;\n}\n\nstatic void ccp_reverse_get_dm_area(struct ccp_dm_workarea *wa,\n\t\t\t\t    unsigned int wa_offset,\n\t\t\t\t    struct scatterlist *sg,\n\t\t\t\t    unsigned int sg_offset,\n\t\t\t\t    unsigned int len)\n{\n\tu8 *p, *q;\n\n\tp = wa->address + wa_offset;\n\tq = p + len - 1;\n\twhile (p < q) {\n\t\t*p = *p ^ *q;\n\t\t*q = *p ^ *q;\n\t\t*p = *p ^ *q;\n\t\tp++;\n\t\tq--;\n\t}\n\n\tccp_get_dm_area(wa, wa_offset, sg, sg_offset, len);\n}\n\nstatic void ccp_free_data(struct ccp_data *data, struct ccp_cmd_queue *cmd_q)\n{\n\tccp_dm_free(&data->dm_wa);\n\tccp_sg_free(&data->sg_wa);\n}\n\nstatic int ccp_init_data(struct ccp_data *data, struct ccp_cmd_queue *cmd_q,\n\t\t\t struct scatterlist *sg, u64 sg_len,\n\t\t\t unsigned int dm_len,\n\t\t\t enum dma_data_direction dir)\n{\n\tint ret;\n\n\tmemset(data, 0, sizeof(*data));\n\n\tret = ccp_init_sg_workarea(&data->sg_wa, cmd_q->ccp->dev, sg, sg_len,\n\t\t\t\t   dir);\n\tif (ret)\n\t\tgoto e_err;\n\n\tret = ccp_init_dm_workarea(&data->dm_wa, cmd_q, dm_len, dir);\n\tif (ret)\n\t\tgoto e_err;\n\n\treturn 0;\n\ne_err:\n\tccp_free_data(data, cmd_q);\n\n\treturn ret;\n}\n\nstatic unsigned int ccp_queue_buf(struct ccp_data *data, unsigned int from)\n{\n\tstruct ccp_sg_workarea *sg_wa = &data->sg_wa;\n\tstruct ccp_dm_workarea *dm_wa = &data->dm_wa;\n\tunsigned int buf_count, nbytes;\n\n\t/* Clear the buffer if setting it */\n\tif (!from)\n\t\tmemset(dm_wa->address, 0, dm_wa->length);\n\n\tif (!sg_wa->sg)\n\t\treturn 0;\n\n\t/* Perform the copy operation\n\t *   nbytes will always be <= UINT_MAX because dm_wa->length is\n\t *   an unsigned int\n\t */\n\tnbytes = min_t(u64, sg_wa->bytes_left, dm_wa->length);\n\tscatterwalk_map_and_copy(dm_wa->address, sg_wa->sg, sg_wa->sg_used,\n\t\t\t\t nbytes, from);\n\n\t/* Update the structures and generate the count */\n\tbuf_count = 0;\n\twhile (sg_wa->bytes_left && (buf_count < dm_wa->length)) {\n\t\tnbytes = min(sg_dma_len(sg_wa->dma_sg) - sg_wa->sg_used,\n\t\t\t     dm_wa->length - buf_count);\n\t\tnbytes = min_t(u64, sg_wa->bytes_left, nbytes);\n\n\t\tbuf_count += nbytes;\n\t\tccp_update_sg_workarea(sg_wa, nbytes);\n\t}\n\n\treturn buf_count;\n}\n\nstatic unsigned int ccp_fill_queue_buf(struct ccp_data *data)\n{\n\treturn ccp_queue_buf(data, 0);\n}\n\nstatic unsigned int ccp_empty_queue_buf(struct ccp_data *data)\n{\n\treturn ccp_queue_buf(data, 1);\n}\n\nstatic void ccp_prepare_data(struct ccp_data *src, struct ccp_data *dst,\n\t\t\t     struct ccp_op *op, unsigned int block_size,\n\t\t\t     bool blocksize_op)\n{\n\tunsigned int sg_src_len, sg_dst_len, op_len;\n\n\t/* The CCP can only DMA from/to one address each per operation. This\n\t * requires that we find the smallest DMA area between the source\n\t * and destination. The resulting len values will always be <= UINT_MAX\n\t * because the dma length is an unsigned int.\n\t */\n\tsg_src_len = sg_dma_len(src->sg_wa.dma_sg) - src->sg_wa.sg_used;\n\tsg_src_len = min_t(u64, src->sg_wa.bytes_left, sg_src_len);\n\n\tif (dst) {\n\t\tsg_dst_len = sg_dma_len(dst->sg_wa.dma_sg) - dst->sg_wa.sg_used;\n\t\tsg_dst_len = min_t(u64, src->sg_wa.bytes_left, sg_dst_len);\n\t\top_len = min(sg_src_len, sg_dst_len);\n\t} else {\n\t\top_len = sg_src_len;\n\t}\n\n\t/* The data operation length will be at least block_size in length\n\t * or the smaller of available sg room remaining for the source or\n\t * the destination\n\t */\n\top_len = max(op_len, block_size);\n\n\t/* Unless we have to buffer data, there's no reason to wait */\n\top->soc = 0;\n\n\tif (sg_src_len < block_size) {\n\t\t/* Not enough data in the sg element, so it\n\t\t * needs to be buffered into a blocksize chunk\n\t\t */\n\t\tint cp_len = ccp_fill_queue_buf(src);\n\n\t\top->soc = 1;\n\t\top->src.u.dma.address = src->dm_wa.dma.address;\n\t\top->src.u.dma.offset = 0;\n\t\top->src.u.dma.length = (blocksize_op) ? block_size : cp_len;\n\t} else {\n\t\t/* Enough data in the sg element, but we need to\n\t\t * adjust for any previously copied data\n\t\t */\n\t\top->src.u.dma.address = sg_dma_address(src->sg_wa.dma_sg);\n\t\top->src.u.dma.offset = src->sg_wa.sg_used;\n\t\top->src.u.dma.length = op_len & ~(block_size - 1);\n\n\t\tccp_update_sg_workarea(&src->sg_wa, op->src.u.dma.length);\n\t}\n\n\tif (dst) {\n\t\tif (sg_dst_len < block_size) {\n\t\t\t/* Not enough room in the sg element or we're on the\n\t\t\t * last piece of data (when using padding), so the\n\t\t\t * output needs to be buffered into a blocksize chunk\n\t\t\t */\n\t\t\top->soc = 1;\n\t\t\top->dst.u.dma.address = dst->dm_wa.dma.address;\n\t\t\top->dst.u.dma.offset = 0;\n\t\t\top->dst.u.dma.length = op->src.u.dma.length;\n\t\t} else {\n\t\t\t/* Enough room in the sg element, but we need to\n\t\t\t * adjust for any previously used area\n\t\t\t */\n\t\t\top->dst.u.dma.address = sg_dma_address(dst->sg_wa.dma_sg);\n\t\t\top->dst.u.dma.offset = dst->sg_wa.sg_used;\n\t\t\top->dst.u.dma.length = op->src.u.dma.length;\n\t\t}\n\t}\n}\n\nstatic void ccp_process_data(struct ccp_data *src, struct ccp_data *dst,\n\t\t\t     struct ccp_op *op)\n{\n\top->init = 0;\n\n\tif (dst) {\n\t\tif (op->dst.u.dma.address == dst->dm_wa.dma.address)\n\t\t\tccp_empty_queue_buf(dst);\n\t\telse\n\t\t\tccp_update_sg_workarea(&dst->sg_wa,\n\t\t\t\t\t       op->dst.u.dma.length);\n\t}\n}\n\nstatic int ccp_copy_to_from_sb(struct ccp_cmd_queue *cmd_q,\n\t\t\t       struct ccp_dm_workarea *wa, u32 jobid, u32 sb,\n\t\t\t       u32 byte_swap, bool from)\n{\n\tstruct ccp_op op;\n\n\tmemset(&op, 0, sizeof(op));\n\n\top.cmd_q = cmd_q;\n\top.jobid = jobid;\n\top.eom = 1;\n\n\tif (from) {\n\t\top.soc = 1;\n\t\top.src.type = CCP_MEMTYPE_SB;\n\t\top.src.u.sb = sb;\n\t\top.dst.type = CCP_MEMTYPE_SYSTEM;\n\t\top.dst.u.dma.address = wa->dma.address;\n\t\top.dst.u.dma.length = wa->length;\n\t} else {\n\t\top.src.type = CCP_MEMTYPE_SYSTEM;\n\t\top.src.u.dma.address = wa->dma.address;\n\t\top.src.u.dma.length = wa->length;\n\t\top.dst.type = CCP_MEMTYPE_SB;\n\t\top.dst.u.sb = sb;\n\t}\n\n\top.u.passthru.byte_swap = byte_swap;\n\n\treturn cmd_q->ccp->vdata->perform->passthru(&op);\n}\n\nstatic int ccp_copy_to_sb(struct ccp_cmd_queue *cmd_q,\n\t\t\t  struct ccp_dm_workarea *wa, u32 jobid, u32 sb,\n\t\t\t  u32 byte_swap)\n{\n\treturn ccp_copy_to_from_sb(cmd_q, wa, jobid, sb, byte_swap, false);\n}\n\nstatic int ccp_copy_from_sb(struct ccp_cmd_queue *cmd_q,\n\t\t\t    struct ccp_dm_workarea *wa, u32 jobid, u32 sb,\n\t\t\t    u32 byte_swap)\n{\n\treturn ccp_copy_to_from_sb(cmd_q, wa, jobid, sb, byte_swap, true);\n}\n\nstatic noinline_for_stack int\nccp_run_aes_cmac_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_aes_engine *aes = &cmd->u.aes;\n\tstruct ccp_dm_workarea key, ctx;\n\tstruct ccp_data src;\n\tstruct ccp_op op;\n\tunsigned int dm_offset;\n\tint ret;\n\n\tif (!((aes->key_len == AES_KEYSIZE_128) ||\n\t      (aes->key_len == AES_KEYSIZE_192) ||\n\t      (aes->key_len == AES_KEYSIZE_256)))\n\t\treturn -EINVAL;\n\n\tif (aes->src_len & (AES_BLOCK_SIZE - 1))\n\t\treturn -EINVAL;\n\n\tif (aes->iv_len != AES_BLOCK_SIZE)\n\t\treturn -EINVAL;\n\n\tif (!aes->key || !aes->iv || !aes->src)\n\t\treturn -EINVAL;\n\n\tif (aes->cmac_final) {\n\t\tif (aes->cmac_key_len != AES_BLOCK_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\tif (!aes->cmac_key)\n\t\t\treturn -EINVAL;\n\t}\n\n\tBUILD_BUG_ON(CCP_AES_KEY_SB_COUNT != 1);\n\tBUILD_BUG_ON(CCP_AES_CTX_SB_COUNT != 1);\n\n\tret = -EIO;\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\top.sb_key = cmd_q->sb_key;\n\top.sb_ctx = cmd_q->sb_ctx;\n\top.init = 1;\n\top.u.aes.type = aes->type;\n\top.u.aes.mode = aes->mode;\n\top.u.aes.action = aes->action;\n\n\t/* All supported key sizes fit in a single (32-byte) SB entry\n\t * and must be in little endian format. Use the 256-bit byte\n\t * swap passthru option to convert from big endian to little\n\t * endian.\n\t */\n\tret = ccp_init_dm_workarea(&key, cmd_q,\n\t\t\t\t   CCP_AES_KEY_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret)\n\t\treturn ret;\n\n\tdm_offset = CCP_SB_BYTES - aes->key_len;\n\tret = ccp_set_dm_area(&key, dm_offset, aes->key, 0, aes->key_len);\n\tif (ret)\n\t\tgoto e_key;\n\tret = ccp_copy_to_sb(cmd_q, &key, op.jobid, op.sb_key,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_key;\n\t}\n\n\t/* The AES context fits in a single (32-byte) SB entry and\n\t * must be in little endian format. Use the 256-bit byte swap\n\t * passthru option to convert from big endian to little endian.\n\t */\n\tret = ccp_init_dm_workarea(&ctx, cmd_q,\n\t\t\t\t   CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto e_key;\n\n\tdm_offset = CCP_SB_BYTES - AES_BLOCK_SIZE;\n\tret = ccp_set_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\tif (ret)\n\t\tgoto e_ctx;\n\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_ctx;\n\t}\n\n\t/* Send data to the CCP AES engine */\n\tret = ccp_init_data(&src, cmd_q, aes->src, aes->src_len,\n\t\t\t    AES_BLOCK_SIZE, DMA_TO_DEVICE);\n\tif (ret)\n\t\tgoto e_ctx;\n\n\twhile (src.sg_wa.bytes_left) {\n\t\tccp_prepare_data(&src, NULL, &op, AES_BLOCK_SIZE, true);\n\t\tif (aes->cmac_final && !src.sg_wa.bytes_left) {\n\t\t\top.eom = 1;\n\n\t\t\t/* Push the K1/K2 key to the CCP now */\n\t\t\tret = ccp_copy_from_sb(cmd_q, &ctx, op.jobid,\n\t\t\t\t\t       op.sb_ctx,\n\t\t\t\t\t       CCP_PASSTHRU_BYTESWAP_256BIT);\n\t\t\tif (ret) {\n\t\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\t\tgoto e_src;\n\t\t\t}\n\n\t\t\tret = ccp_set_dm_area(&ctx, 0, aes->cmac_key, 0,\n\t\t\t\t\t      aes->cmac_key_len);\n\t\t\tif (ret)\n\t\t\t\tgoto e_src;\n\t\t\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\t\t\tif (ret) {\n\t\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\t\tgoto e_src;\n\t\t\t}\n\t\t}\n\n\t\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_src;\n\t\t}\n\n\t\tccp_process_data(&src, NULL, &op);\n\t}\n\n\t/* Retrieve the AES context - convert from LE to BE using\n\t * 32-byte (256-bit) byteswapping\n\t */\n\tret = ccp_copy_from_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t       CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_src;\n\t}\n\n\t/* ...but we only need AES_BLOCK_SIZE bytes */\n\tdm_offset = CCP_SB_BYTES - AES_BLOCK_SIZE;\n\tccp_get_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\ne_src:\n\tccp_free_data(&src, cmd_q);\n\ne_ctx:\n\tccp_dm_free(&ctx);\n\ne_key:\n\tccp_dm_free(&key);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int\nccp_run_aes_gcm_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_aes_engine *aes = &cmd->u.aes;\n\tstruct ccp_dm_workarea key, ctx, final_wa, tag;\n\tstruct ccp_data src, dst;\n\tstruct ccp_data aad;\n\tstruct ccp_op op;\n\tunsigned int dm_offset;\n\tunsigned int authsize;\n\tunsigned int jobid;\n\tunsigned int ilen;\n\tbool in_place = true; /* Default value */\n\t__be64 *final;\n\tint ret;\n\n\tstruct scatterlist *p_inp, sg_inp[2];\n\tstruct scatterlist *p_tag, sg_tag[2];\n\tstruct scatterlist *p_outp, sg_outp[2];\n\tstruct scatterlist *p_aad;\n\n\tif (!aes->iv)\n\t\treturn -EINVAL;\n\n\tif (!((aes->key_len == AES_KEYSIZE_128) ||\n\t\t(aes->key_len == AES_KEYSIZE_192) ||\n\t\t(aes->key_len == AES_KEYSIZE_256)))\n\t\treturn -EINVAL;\n\n\tif (!aes->key) /* Gotta have a key SGL */\n\t\treturn -EINVAL;\n\n\t/* Zero defaults to 16 bytes, the maximum size */\n\tauthsize = aes->authsize ? aes->authsize : AES_BLOCK_SIZE;\n\tswitch (authsize) {\n\tcase 16:\n\tcase 15:\n\tcase 14:\n\tcase 13:\n\tcase 12:\n\tcase 8:\n\tcase 4:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t/* First, decompose the source buffer into AAD & PT,\n\t * and the destination buffer into AAD, CT & tag, or\n\t * the input into CT & tag.\n\t * It is expected that the input and output SGs will\n\t * be valid, even if the AAD and input lengths are 0.\n\t */\n\tp_aad = aes->src;\n\tp_inp = scatterwalk_ffwd(sg_inp, aes->src, aes->aad_len);\n\tp_outp = scatterwalk_ffwd(sg_outp, aes->dst, aes->aad_len);\n\tif (aes->action == CCP_AES_ACTION_ENCRYPT) {\n\t\tilen = aes->src_len;\n\t\tp_tag = scatterwalk_ffwd(sg_tag, p_outp, ilen);\n\t} else {\n\t\t/* Input length for decryption includes tag */\n\t\tilen = aes->src_len - authsize;\n\t\tp_tag = scatterwalk_ffwd(sg_tag, p_inp, ilen);\n\t}\n\n\tjobid = CCP_NEW_JOBID(cmd_q->ccp);\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = jobid;\n\top.sb_key = cmd_q->sb_key; /* Pre-allocated */\n\top.sb_ctx = cmd_q->sb_ctx; /* Pre-allocated */\n\top.init = 1;\n\top.u.aes.type = aes->type;\n\n\t/* Copy the key to the LSB */\n\tret = ccp_init_dm_workarea(&key, cmd_q,\n\t\t\t\t   CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret)\n\t\treturn ret;\n\n\tdm_offset = CCP_SB_BYTES - aes->key_len;\n\tret = ccp_set_dm_area(&key, dm_offset, aes->key, 0, aes->key_len);\n\tif (ret)\n\t\tgoto e_key;\n\tret = ccp_copy_to_sb(cmd_q, &key, op.jobid, op.sb_key,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_key;\n\t}\n\n\t/* Copy the context (IV) to the LSB.\n\t * There is an assumption here that the IV is 96 bits in length, plus\n\t * a nonce of 32 bits. If no IV is present, use a zeroed buffer.\n\t */\n\tret = ccp_init_dm_workarea(&ctx, cmd_q,\n\t\t\t\t   CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto e_key;\n\n\tdm_offset = CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES - aes->iv_len;\n\tret = ccp_set_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\tif (ret)\n\t\tgoto e_ctx;\n\n\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_ctx;\n\t}\n\n\top.init = 1;\n\tif (aes->aad_len > 0) {\n\t\t/* Step 1: Run a GHASH over the Additional Authenticated Data */\n\t\tret = ccp_init_data(&aad, cmd_q, p_aad, aes->aad_len,\n\t\t\t\t    AES_BLOCK_SIZE,\n\t\t\t\t    DMA_TO_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_ctx;\n\n\t\top.u.aes.mode = CCP_AES_MODE_GHASH;\n\t\top.u.aes.action = CCP_AES_GHASHAAD;\n\n\t\twhile (aad.sg_wa.bytes_left) {\n\t\t\tccp_prepare_data(&aad, NULL, &op, AES_BLOCK_SIZE, true);\n\n\t\t\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\t\t\tif (ret) {\n\t\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\t\tgoto e_aad;\n\t\t\t}\n\n\t\t\tccp_process_data(&aad, NULL, &op);\n\t\t\top.init = 0;\n\t\t}\n\t}\n\n\top.u.aes.mode = CCP_AES_MODE_GCTR;\n\top.u.aes.action = aes->action;\n\n\tif (ilen > 0) {\n\t\t/* Step 2: Run a GCTR over the plaintext */\n\t\tin_place = (sg_virt(p_inp) == sg_virt(p_outp)) ? true : false;\n\n\t\tret = ccp_init_data(&src, cmd_q, p_inp, ilen,\n\t\t\t\t    AES_BLOCK_SIZE,\n\t\t\t\t    in_place ? DMA_BIDIRECTIONAL\n\t\t\t\t\t     : DMA_TO_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_ctx;\n\n\t\tif (in_place) {\n\t\t\tdst = src;\n\t\t} else {\n\t\t\tret = ccp_init_data(&dst, cmd_q, p_outp, ilen,\n\t\t\t\t\t    AES_BLOCK_SIZE, DMA_FROM_DEVICE);\n\t\t\tif (ret)\n\t\t\t\tgoto e_src;\n\t\t}\n\n\t\top.soc = 0;\n\t\top.eom = 0;\n\t\top.init = 1;\n\t\twhile (src.sg_wa.bytes_left) {\n\t\t\tccp_prepare_data(&src, &dst, &op, AES_BLOCK_SIZE, true);\n\t\t\tif (!src.sg_wa.bytes_left) {\n\t\t\t\tunsigned int nbytes = ilen % AES_BLOCK_SIZE;\n\n\t\t\t\tif (nbytes) {\n\t\t\t\t\top.eom = 1;\n\t\t\t\t\top.u.aes.size = (nbytes * 8) - 1;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\t\t\tif (ret) {\n\t\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\t\tgoto e_dst;\n\t\t\t}\n\n\t\t\tccp_process_data(&src, &dst, &op);\n\t\t\top.init = 0;\n\t\t}\n\t}\n\n\t/* Step 3: Update the IV portion of the context with the original IV */\n\tret = ccp_copy_from_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t       CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\tret = ccp_set_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\tif (ret)\n\t\tgoto e_dst;\n\n\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\t/* Step 4: Concatenate the lengths of the AAD and source, and\n\t * hash that 16 byte buffer.\n\t */\n\tret = ccp_init_dm_workarea(&final_wa, cmd_q, AES_BLOCK_SIZE,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto e_dst;\n\tfinal = (__be64 *)final_wa.address;\n\tfinal[0] = cpu_to_be64(aes->aad_len * 8);\n\tfinal[1] = cpu_to_be64(ilen * 8);\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = jobid;\n\top.sb_key = cmd_q->sb_key; /* Pre-allocated */\n\top.sb_ctx = cmd_q->sb_ctx; /* Pre-allocated */\n\top.init = 1;\n\top.u.aes.type = aes->type;\n\top.u.aes.mode = CCP_AES_MODE_GHASH;\n\top.u.aes.action = CCP_AES_GHASHFINAL;\n\top.src.type = CCP_MEMTYPE_SYSTEM;\n\top.src.u.dma.address = final_wa.dma.address;\n\top.src.u.dma.length = AES_BLOCK_SIZE;\n\top.dst.type = CCP_MEMTYPE_SYSTEM;\n\top.dst.u.dma.address = final_wa.dma.address;\n\top.dst.u.dma.length = AES_BLOCK_SIZE;\n\top.eom = 1;\n\top.u.aes.size = 0;\n\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\tif (ret)\n\t\tgoto e_dst;\n\n\tif (aes->action == CCP_AES_ACTION_ENCRYPT) {\n\t\t/* Put the ciphered tag after the ciphertext. */\n\t\tccp_get_dm_area(&final_wa, 0, p_tag, 0, authsize);\n\t} else {\n\t\t/* Does this ciphered tag match the input? */\n\t\tret = ccp_init_dm_workarea(&tag, cmd_q, authsize,\n\t\t\t\t\t   DMA_BIDIRECTIONAL);\n\t\tif (ret)\n\t\t\tgoto e_tag;\n\t\tret = ccp_set_dm_area(&tag, 0, p_tag, 0, authsize);\n\t\tif (ret)\n\t\t\tgoto e_tag;\n\n\t\tret = crypto_memneq(tag.address, final_wa.address,\n\t\t\t\t    authsize) ? -EBADMSG : 0;\n\t\tccp_dm_free(&tag);\n\t}\n\ne_tag:\n\tccp_dm_free(&final_wa);\n\ne_dst:\n\tif (ilen > 0 && !in_place)\n\t\tccp_free_data(&dst, cmd_q);\n\ne_src:\n\tif (ilen > 0)\n\t\tccp_free_data(&src, cmd_q);\n\ne_aad:\n\tif (aes->aad_len)\n\t\tccp_free_data(&aad, cmd_q);\n\ne_ctx:\n\tccp_dm_free(&ctx);\n\ne_key:\n\tccp_dm_free(&key);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int\nccp_run_aes_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_aes_engine *aes = &cmd->u.aes;\n\tstruct ccp_dm_workarea key, ctx;\n\tstruct ccp_data src, dst;\n\tstruct ccp_op op;\n\tunsigned int dm_offset;\n\tbool in_place = false;\n\tint ret;\n\n\tif (!((aes->key_len == AES_KEYSIZE_128) ||\n\t      (aes->key_len == AES_KEYSIZE_192) ||\n\t      (aes->key_len == AES_KEYSIZE_256)))\n\t\treturn -EINVAL;\n\n\tif (((aes->mode == CCP_AES_MODE_ECB) ||\n\t     (aes->mode == CCP_AES_MODE_CBC)) &&\n\t    (aes->src_len & (AES_BLOCK_SIZE - 1)))\n\t\treturn -EINVAL;\n\n\tif (!aes->key || !aes->src || !aes->dst)\n\t\treturn -EINVAL;\n\n\tif (aes->mode != CCP_AES_MODE_ECB) {\n\t\tif (aes->iv_len != AES_BLOCK_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\tif (!aes->iv)\n\t\t\treturn -EINVAL;\n\t}\n\n\tBUILD_BUG_ON(CCP_AES_KEY_SB_COUNT != 1);\n\tBUILD_BUG_ON(CCP_AES_CTX_SB_COUNT != 1);\n\n\tret = -EIO;\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\top.sb_key = cmd_q->sb_key;\n\top.sb_ctx = cmd_q->sb_ctx;\n\top.init = (aes->mode == CCP_AES_MODE_ECB) ? 0 : 1;\n\top.u.aes.type = aes->type;\n\top.u.aes.mode = aes->mode;\n\top.u.aes.action = aes->action;\n\n\t/* All supported key sizes fit in a single (32-byte) SB entry\n\t * and must be in little endian format. Use the 256-bit byte\n\t * swap passthru option to convert from big endian to little\n\t * endian.\n\t */\n\tret = ccp_init_dm_workarea(&key, cmd_q,\n\t\t\t\t   CCP_AES_KEY_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret)\n\t\treturn ret;\n\n\tdm_offset = CCP_SB_BYTES - aes->key_len;\n\tret = ccp_set_dm_area(&key, dm_offset, aes->key, 0, aes->key_len);\n\tif (ret)\n\t\tgoto e_key;\n\tret = ccp_copy_to_sb(cmd_q, &key, op.jobid, op.sb_key,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_key;\n\t}\n\n\t/* The AES context fits in a single (32-byte) SB entry and\n\t * must be in little endian format. Use the 256-bit byte swap\n\t * passthru option to convert from big endian to little endian.\n\t */\n\tret = ccp_init_dm_workarea(&ctx, cmd_q,\n\t\t\t\t   CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto e_key;\n\n\tif (aes->mode != CCP_AES_MODE_ECB) {\n\t\t/* Load the AES context - convert to LE */\n\t\tdm_offset = CCP_SB_BYTES - AES_BLOCK_SIZE;\n\t\tret = ccp_set_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\t\tif (ret)\n\t\t\tgoto e_ctx;\n\t\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_ctx;\n\t\t}\n\t}\n\tswitch (aes->mode) {\n\tcase CCP_AES_MODE_CFB: /* CFB128 only */\n\tcase CCP_AES_MODE_CTR:\n\t\top.u.aes.size = AES_BLOCK_SIZE * BITS_PER_BYTE - 1;\n\t\tbreak;\n\tdefault:\n\t\top.u.aes.size = 0;\n\t}\n\n\t/* Prepare the input and output data workareas. For in-place\n\t * operations we need to set the dma direction to BIDIRECTIONAL\n\t * and copy the src workarea to the dst workarea.\n\t */\n\tif (sg_virt(aes->src) == sg_virt(aes->dst))\n\t\tin_place = true;\n\n\tret = ccp_init_data(&src, cmd_q, aes->src, aes->src_len,\n\t\t\t    AES_BLOCK_SIZE,\n\t\t\t    in_place ? DMA_BIDIRECTIONAL : DMA_TO_DEVICE);\n\tif (ret)\n\t\tgoto e_ctx;\n\n\tif (in_place) {\n\t\tdst = src;\n\t} else {\n\t\tret = ccp_init_data(&dst, cmd_q, aes->dst, aes->src_len,\n\t\t\t\t    AES_BLOCK_SIZE, DMA_FROM_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_src;\n\t}\n\n\t/* Send data to the CCP AES engine */\n\twhile (src.sg_wa.bytes_left) {\n\t\tccp_prepare_data(&src, &dst, &op, AES_BLOCK_SIZE, true);\n\t\tif (!src.sg_wa.bytes_left) {\n\t\t\top.eom = 1;\n\n\t\t\t/* Since we don't retrieve the AES context in ECB\n\t\t\t * mode we have to wait for the operation to complete\n\t\t\t * on the last piece of data\n\t\t\t */\n\t\t\tif (aes->mode == CCP_AES_MODE_ECB)\n\t\t\t\top.soc = 1;\n\t\t}\n\n\t\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_dst;\n\t\t}\n\n\t\tccp_process_data(&src, &dst, &op);\n\t}\n\n\tif (aes->mode != CCP_AES_MODE_ECB) {\n\t\t/* Retrieve the AES context - convert from LE to BE using\n\t\t * 32-byte (256-bit) byteswapping\n\t\t */\n\t\tret = ccp_copy_from_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t\t       CCP_PASSTHRU_BYTESWAP_256BIT);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_dst;\n\t\t}\n\n\t\t/* ...but we only need AES_BLOCK_SIZE bytes */\n\t\tdm_offset = CCP_SB_BYTES - AES_BLOCK_SIZE;\n\t\tccp_get_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\t}\n\ne_dst:\n\tif (!in_place)\n\t\tccp_free_data(&dst, cmd_q);\n\ne_src:\n\tccp_free_data(&src, cmd_q);\n\ne_ctx:\n\tccp_dm_free(&ctx);\n\ne_key:\n\tccp_dm_free(&key);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int\nccp_run_xts_aes_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_xts_aes_engine *xts = &cmd->u.xts;\n\tstruct ccp_dm_workarea key, ctx;\n\tstruct ccp_data src, dst;\n\tstruct ccp_op op;\n\tunsigned int unit_size, dm_offset;\n\tbool in_place = false;\n\tunsigned int sb_count;\n\tenum ccp_aes_type aestype;\n\tint ret;\n\n\tswitch (xts->unit_size) {\n\tcase CCP_XTS_AES_UNIT_SIZE_16:\n\t\tunit_size = 16;\n\t\tbreak;\n\tcase CCP_XTS_AES_UNIT_SIZE_512:\n\t\tunit_size = 512;\n\t\tbreak;\n\tcase CCP_XTS_AES_UNIT_SIZE_1024:\n\t\tunit_size = 1024;\n\t\tbreak;\n\tcase CCP_XTS_AES_UNIT_SIZE_2048:\n\t\tunit_size = 2048;\n\t\tbreak;\n\tcase CCP_XTS_AES_UNIT_SIZE_4096:\n\t\tunit_size = 4096;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (xts->key_len == AES_KEYSIZE_128)\n\t\taestype = CCP_AES_TYPE_128;\n\telse if (xts->key_len == AES_KEYSIZE_256)\n\t\taestype = CCP_AES_TYPE_256;\n\telse\n\t\treturn -EINVAL;\n\n\tif (!xts->final && (xts->src_len & (AES_BLOCK_SIZE - 1)))\n\t\treturn -EINVAL;\n\n\tif (xts->iv_len != AES_BLOCK_SIZE)\n\t\treturn -EINVAL;\n\n\tif (!xts->key || !xts->iv || !xts->src || !xts->dst)\n\t\treturn -EINVAL;\n\n\tBUILD_BUG_ON(CCP_XTS_AES_KEY_SB_COUNT != 1);\n\tBUILD_BUG_ON(CCP_XTS_AES_CTX_SB_COUNT != 1);\n\n\tret = -EIO;\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\top.sb_key = cmd_q->sb_key;\n\top.sb_ctx = cmd_q->sb_ctx;\n\top.init = 1;\n\top.u.xts.type = aestype;\n\top.u.xts.action = xts->action;\n\top.u.xts.unit_size = xts->unit_size;\n\n\t/* A version 3 device only supports 128-bit keys, which fits into a\n\t * single SB entry. A version 5 device uses a 512-bit vector, so two\n\t * SB entries.\n\t */\n\tif (cmd_q->ccp->vdata->version == CCP_VERSION(3, 0))\n\t\tsb_count = CCP_XTS_AES_KEY_SB_COUNT;\n\telse\n\t\tsb_count = CCP5_XTS_AES_KEY_SB_COUNT;\n\tret = ccp_init_dm_workarea(&key, cmd_q,\n\t\t\t\t   sb_count * CCP_SB_BYTES,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret)\n\t\treturn ret;\n\n\tif (cmd_q->ccp->vdata->version == CCP_VERSION(3, 0)) {\n\t\t/* All supported key sizes must be in little endian format.\n\t\t * Use the 256-bit byte swap passthru option to convert from\n\t\t * big endian to little endian.\n\t\t */\n\t\tdm_offset = CCP_SB_BYTES - AES_KEYSIZE_128;\n\t\tret = ccp_set_dm_area(&key, dm_offset, xts->key, 0, xts->key_len);\n\t\tif (ret)\n\t\t\tgoto e_key;\n\t\tret = ccp_set_dm_area(&key, 0, xts->key, xts->key_len, xts->key_len);\n\t\tif (ret)\n\t\t\tgoto e_key;\n\t} else {\n\t\t/* Version 5 CCPs use a 512-bit space for the key: each portion\n\t\t * occupies 256 bits, or one entire slot, and is zero-padded.\n\t\t */\n\t\tunsigned int pad;\n\n\t\tdm_offset = CCP_SB_BYTES;\n\t\tpad = dm_offset - xts->key_len;\n\t\tret = ccp_set_dm_area(&key, pad, xts->key, 0, xts->key_len);\n\t\tif (ret)\n\t\t\tgoto e_key;\n\t\tret = ccp_set_dm_area(&key, dm_offset + pad, xts->key,\n\t\t\t\t      xts->key_len, xts->key_len);\n\t\tif (ret)\n\t\t\tgoto e_key;\n\t}\n\tret = ccp_copy_to_sb(cmd_q, &key, op.jobid, op.sb_key,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_key;\n\t}\n\n\t/* The AES context fits in a single (32-byte) SB entry and\n\t * for XTS is already in little endian format so no byte swapping\n\t * is needed.\n\t */\n\tret = ccp_init_dm_workarea(&ctx, cmd_q,\n\t\t\t\t   CCP_XTS_AES_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto e_key;\n\n\tret = ccp_set_dm_area(&ctx, 0, xts->iv, 0, xts->iv_len);\n\tif (ret)\n\t\tgoto e_ctx;\n\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_NOOP);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_ctx;\n\t}\n\n\t/* Prepare the input and output data workareas. For in-place\n\t * operations we need to set the dma direction to BIDIRECTIONAL\n\t * and copy the src workarea to the dst workarea.\n\t */\n\tif (sg_virt(xts->src) == sg_virt(xts->dst))\n\t\tin_place = true;\n\n\tret = ccp_init_data(&src, cmd_q, xts->src, xts->src_len,\n\t\t\t    unit_size,\n\t\t\t    in_place ? DMA_BIDIRECTIONAL : DMA_TO_DEVICE);\n\tif (ret)\n\t\tgoto e_ctx;\n\n\tif (in_place) {\n\t\tdst = src;\n\t} else {\n\t\tret = ccp_init_data(&dst, cmd_q, xts->dst, xts->src_len,\n\t\t\t\t    unit_size, DMA_FROM_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_src;\n\t}\n\n\t/* Send data to the CCP AES engine */\n\twhile (src.sg_wa.bytes_left) {\n\t\tccp_prepare_data(&src, &dst, &op, unit_size, true);\n\t\tif (!src.sg_wa.bytes_left)\n\t\t\top.eom = 1;\n\n\t\tret = cmd_q->ccp->vdata->perform->xts_aes(&op);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_dst;\n\t\t}\n\n\t\tccp_process_data(&src, &dst, &op);\n\t}\n\n\t/* Retrieve the AES context - convert from LE to BE using\n\t * 32-byte (256-bit) byteswapping\n\t */\n\tret = ccp_copy_from_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t       CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\t/* ...but we only need AES_BLOCK_SIZE bytes */\n\tdm_offset = CCP_SB_BYTES - AES_BLOCK_SIZE;\n\tccp_get_dm_area(&ctx, dm_offset, xts->iv, 0, xts->iv_len);\n\ne_dst:\n\tif (!in_place)\n\t\tccp_free_data(&dst, cmd_q);\n\ne_src:\n\tccp_free_data(&src, cmd_q);\n\ne_ctx:\n\tccp_dm_free(&ctx);\n\ne_key:\n\tccp_dm_free(&key);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int\nccp_run_des3_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_des3_engine *des3 = &cmd->u.des3;\n\n\tstruct ccp_dm_workarea key, ctx;\n\tstruct ccp_data src, dst;\n\tstruct ccp_op op;\n\tunsigned int dm_offset;\n\tunsigned int len_singlekey;\n\tbool in_place = false;\n\tint ret;\n\n\t/* Error checks */\n\tif (cmd_q->ccp->vdata->version < CCP_VERSION(5, 0))\n\t\treturn -EINVAL;\n\n\tif (!cmd_q->ccp->vdata->perform->des3)\n\t\treturn -EINVAL;\n\n\tif (des3->key_len != DES3_EDE_KEY_SIZE)\n\t\treturn -EINVAL;\n\n\tif (((des3->mode == CCP_DES3_MODE_ECB) ||\n\t\t(des3->mode == CCP_DES3_MODE_CBC)) &&\n\t\t(des3->src_len & (DES3_EDE_BLOCK_SIZE - 1)))\n\t\treturn -EINVAL;\n\n\tif (!des3->key || !des3->src || !des3->dst)\n\t\treturn -EINVAL;\n\n\tif (des3->mode != CCP_DES3_MODE_ECB) {\n\t\tif (des3->iv_len != DES3_EDE_BLOCK_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\tif (!des3->iv)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* Zero out all the fields of the command desc */\n\tmemset(&op, 0, sizeof(op));\n\n\t/* Set up the Function field */\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\top.sb_key = cmd_q->sb_key;\n\n\top.init = (des3->mode == CCP_DES3_MODE_ECB) ? 0 : 1;\n\top.u.des3.type = des3->type;\n\top.u.des3.mode = des3->mode;\n\top.u.des3.action = des3->action;\n\n\t/*\n\t * All supported key sizes fit in a single (32-byte) KSB entry and\n\t * (like AES) must be in little endian format. Use the 256-bit byte\n\t * swap passthru option to convert from big endian to little endian.\n\t */\n\tret = ccp_init_dm_workarea(&key, cmd_q,\n\t\t\t\t   CCP_DES3_KEY_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * The contents of the key triplet are in the reverse order of what\n\t * is required by the engine. Copy the 3 pieces individually to put\n\t * them where they belong.\n\t */\n\tdm_offset = CCP_SB_BYTES - des3->key_len; /* Basic offset */\n\n\tlen_singlekey = des3->key_len / 3;\n\tret = ccp_set_dm_area(&key, dm_offset + 2 * len_singlekey,\n\t\t\t      des3->key, 0, len_singlekey);\n\tif (ret)\n\t\tgoto e_key;\n\tret = ccp_set_dm_area(&key, dm_offset + len_singlekey,\n\t\t\t      des3->key, len_singlekey, len_singlekey);\n\tif (ret)\n\t\tgoto e_key;\n\tret = ccp_set_dm_area(&key, dm_offset,\n\t\t\t      des3->key, 2 * len_singlekey, len_singlekey);\n\tif (ret)\n\t\tgoto e_key;\n\n\t/* Copy the key to the SB */\n\tret = ccp_copy_to_sb(cmd_q, &key, op.jobid, op.sb_key,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_key;\n\t}\n\n\t/*\n\t * The DES3 context fits in a single (32-byte) KSB entry and\n\t * must be in little endian format. Use the 256-bit byte swap\n\t * passthru option to convert from big endian to little endian.\n\t */\n\tif (des3->mode != CCP_DES3_MODE_ECB) {\n\t\top.sb_ctx = cmd_q->sb_ctx;\n\n\t\tret = ccp_init_dm_workarea(&ctx, cmd_q,\n\t\t\t\t\t   CCP_DES3_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t\t   DMA_BIDIRECTIONAL);\n\t\tif (ret)\n\t\t\tgoto e_key;\n\n\t\t/* Load the context into the LSB */\n\t\tdm_offset = CCP_SB_BYTES - des3->iv_len;\n\t\tret = ccp_set_dm_area(&ctx, dm_offset, des3->iv, 0,\n\t\t\t\t      des3->iv_len);\n\t\tif (ret)\n\t\t\tgoto e_ctx;\n\n\t\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_ctx;\n\t\t}\n\t}\n\n\t/*\n\t * Prepare the input and output data workareas. For in-place\n\t * operations we need to set the dma direction to BIDIRECTIONAL\n\t * and copy the src workarea to the dst workarea.\n\t */\n\tif (sg_virt(des3->src) == sg_virt(des3->dst))\n\t\tin_place = true;\n\n\tret = ccp_init_data(&src, cmd_q, des3->src, des3->src_len,\n\t\t\tDES3_EDE_BLOCK_SIZE,\n\t\t\tin_place ? DMA_BIDIRECTIONAL : DMA_TO_DEVICE);\n\tif (ret)\n\t\tgoto e_ctx;\n\n\tif (in_place)\n\t\tdst = src;\n\telse {\n\t\tret = ccp_init_data(&dst, cmd_q, des3->dst, des3->src_len,\n\t\t\t\tDES3_EDE_BLOCK_SIZE, DMA_FROM_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_src;\n\t}\n\n\t/* Send data to the CCP DES3 engine */\n\twhile (src.sg_wa.bytes_left) {\n\t\tccp_prepare_data(&src, &dst, &op, DES3_EDE_BLOCK_SIZE, true);\n\t\tif (!src.sg_wa.bytes_left) {\n\t\t\top.eom = 1;\n\n\t\t\t/* Since we don't retrieve the context in ECB mode\n\t\t\t * we have to wait for the operation to complete\n\t\t\t * on the last piece of data\n\t\t\t */\n\t\t\top.soc = 0;\n\t\t}\n\n\t\tret = cmd_q->ccp->vdata->perform->des3(&op);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_dst;\n\t\t}\n\n\t\tccp_process_data(&src, &dst, &op);\n\t}\n\n\tif (des3->mode != CCP_DES3_MODE_ECB) {\n\t\t/* Retrieve the context and make BE */\n\t\tret = ccp_copy_from_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t\t       CCP_PASSTHRU_BYTESWAP_256BIT);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_dst;\n\t\t}\n\n\t\t/* ...but we only need the last DES3_EDE_BLOCK_SIZE bytes */\n\t\tccp_get_dm_area(&ctx, dm_offset, des3->iv, 0,\n\t\t\t\tDES3_EDE_BLOCK_SIZE);\n\t}\ne_dst:\n\tif (!in_place)\n\t\tccp_free_data(&dst, cmd_q);\n\ne_src:\n\tccp_free_data(&src, cmd_q);\n\ne_ctx:\n\tif (des3->mode != CCP_DES3_MODE_ECB)\n\t\tccp_dm_free(&ctx);\n\ne_key:\n\tccp_dm_free(&key);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int\nccp_run_sha_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_sha_engine *sha = &cmd->u.sha;\n\tstruct ccp_dm_workarea ctx;\n\tstruct ccp_data src;\n\tstruct ccp_op op;\n\tunsigned int ioffset, ooffset;\n\tunsigned int digest_size;\n\tint sb_count;\n\tconst void *init;\n\tu64 block_size;\n\tint ctx_size;\n\tint ret;\n\n\tswitch (sha->type) {\n\tcase CCP_SHA_TYPE_1:\n\t\tif (sha->ctx_len < SHA1_DIGEST_SIZE)\n\t\t\treturn -EINVAL;\n\t\tblock_size = SHA1_BLOCK_SIZE;\n\t\tbreak;\n\tcase CCP_SHA_TYPE_224:\n\t\tif (sha->ctx_len < SHA224_DIGEST_SIZE)\n\t\t\treturn -EINVAL;\n\t\tblock_size = SHA224_BLOCK_SIZE;\n\t\tbreak;\n\tcase CCP_SHA_TYPE_256:\n\t\tif (sha->ctx_len < SHA256_DIGEST_SIZE)\n\t\t\treturn -EINVAL;\n\t\tblock_size = SHA256_BLOCK_SIZE;\n\t\tbreak;\n\tcase CCP_SHA_TYPE_384:\n\t\tif (cmd_q->ccp->vdata->version < CCP_VERSION(4, 0)\n\t\t    || sha->ctx_len < SHA384_DIGEST_SIZE)\n\t\t\treturn -EINVAL;\n\t\tblock_size = SHA384_BLOCK_SIZE;\n\t\tbreak;\n\tcase CCP_SHA_TYPE_512:\n\t\tif (cmd_q->ccp->vdata->version < CCP_VERSION(4, 0)\n\t\t    || sha->ctx_len < SHA512_DIGEST_SIZE)\n\t\t\treturn -EINVAL;\n\t\tblock_size = SHA512_BLOCK_SIZE;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (!sha->ctx)\n\t\treturn -EINVAL;\n\n\tif (!sha->final && (sha->src_len & (block_size - 1)))\n\t\treturn -EINVAL;\n\n\t/* The version 3 device can't handle zero-length input */\n\tif (cmd_q->ccp->vdata->version == CCP_VERSION(3, 0)) {\n\n\t\tif (!sha->src_len) {\n\t\t\tunsigned int digest_len;\n\t\t\tconst u8 *sha_zero;\n\n\t\t\t/* Not final, just return */\n\t\t\tif (!sha->final)\n\t\t\t\treturn 0;\n\n\t\t\t/* CCP can't do a zero length sha operation so the\n\t\t\t * caller must buffer the data.\n\t\t\t */\n\t\t\tif (sha->msg_bits)\n\t\t\t\treturn -EINVAL;\n\n\t\t\t/* The CCP cannot perform zero-length sha operations\n\t\t\t * so the caller is required to buffer data for the\n\t\t\t * final operation. However, a sha operation for a\n\t\t\t * message with a total length of zero is valid so\n\t\t\t * known values are required to supply the result.\n\t\t\t */\n\t\t\tswitch (sha->type) {\n\t\t\tcase CCP_SHA_TYPE_1:\n\t\t\t\tsha_zero = sha1_zero_message_hash;\n\t\t\t\tdigest_len = SHA1_DIGEST_SIZE;\n\t\t\t\tbreak;\n\t\t\tcase CCP_SHA_TYPE_224:\n\t\t\t\tsha_zero = sha224_zero_message_hash;\n\t\t\t\tdigest_len = SHA224_DIGEST_SIZE;\n\t\t\t\tbreak;\n\t\t\tcase CCP_SHA_TYPE_256:\n\t\t\t\tsha_zero = sha256_zero_message_hash;\n\t\t\t\tdigest_len = SHA256_DIGEST_SIZE;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tscatterwalk_map_and_copy((void *)sha_zero, sha->ctx, 0,\n\t\t\t\t\t\t digest_len, 1);\n\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/* Set variables used throughout */\n\tswitch (sha->type) {\n\tcase CCP_SHA_TYPE_1:\n\t\tdigest_size = SHA1_DIGEST_SIZE;\n\t\tinit = (void *) ccp_sha1_init;\n\t\tctx_size = SHA1_DIGEST_SIZE;\n\t\tsb_count = 1;\n\t\tif (cmd_q->ccp->vdata->version != CCP_VERSION(3, 0))\n\t\t\tooffset = ioffset = CCP_SB_BYTES - SHA1_DIGEST_SIZE;\n\t\telse\n\t\t\tooffset = ioffset = 0;\n\t\tbreak;\n\tcase CCP_SHA_TYPE_224:\n\t\tdigest_size = SHA224_DIGEST_SIZE;\n\t\tinit = (void *) ccp_sha224_init;\n\t\tctx_size = SHA256_DIGEST_SIZE;\n\t\tsb_count = 1;\n\t\tioffset = 0;\n\t\tif (cmd_q->ccp->vdata->version != CCP_VERSION(3, 0))\n\t\t\tooffset = CCP_SB_BYTES - SHA224_DIGEST_SIZE;\n\t\telse\n\t\t\tooffset = 0;\n\t\tbreak;\n\tcase CCP_SHA_TYPE_256:\n\t\tdigest_size = SHA256_DIGEST_SIZE;\n\t\tinit = (void *) ccp_sha256_init;\n\t\tctx_size = SHA256_DIGEST_SIZE;\n\t\tsb_count = 1;\n\t\tooffset = ioffset = 0;\n\t\tbreak;\n\tcase CCP_SHA_TYPE_384:\n\t\tdigest_size = SHA384_DIGEST_SIZE;\n\t\tinit = (void *) ccp_sha384_init;\n\t\tctx_size = SHA512_DIGEST_SIZE;\n\t\tsb_count = 2;\n\t\tioffset = 0;\n\t\tooffset = 2 * CCP_SB_BYTES - SHA384_DIGEST_SIZE;\n\t\tbreak;\n\tcase CCP_SHA_TYPE_512:\n\t\tdigest_size = SHA512_DIGEST_SIZE;\n\t\tinit = (void *) ccp_sha512_init;\n\t\tctx_size = SHA512_DIGEST_SIZE;\n\t\tsb_count = 2;\n\t\tooffset = ioffset = 0;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tgoto e_data;\n\t}\n\n\t/* For zero-length plaintext the src pointer is ignored;\n\t * otherwise both parts must be valid\n\t */\n\tif (sha->src_len && !sha->src)\n\t\treturn -EINVAL;\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\top.sb_ctx = cmd_q->sb_ctx; /* Pre-allocated */\n\top.u.sha.type = sha->type;\n\top.u.sha.msg_bits = sha->msg_bits;\n\n\t/* For SHA1/224/256 the context fits in a single (32-byte) SB entry;\n\t * SHA384/512 require 2 adjacent SB slots, with the right half in the\n\t * first slot, and the left half in the second. Each portion must then\n\t * be in little endian format: use the 256-bit byte swap option.\n\t */\n\tret = ccp_init_dm_workarea(&ctx, cmd_q, sb_count * CCP_SB_BYTES,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\treturn ret;\n\tif (sha->first) {\n\t\tswitch (sha->type) {\n\t\tcase CCP_SHA_TYPE_1:\n\t\tcase CCP_SHA_TYPE_224:\n\t\tcase CCP_SHA_TYPE_256:\n\t\t\tmemcpy(ctx.address + ioffset, init, ctx_size);\n\t\t\tbreak;\n\t\tcase CCP_SHA_TYPE_384:\n\t\tcase CCP_SHA_TYPE_512:\n\t\t\tmemcpy(ctx.address + ctx_size / 2, init,\n\t\t\t       ctx_size / 2);\n\t\t\tmemcpy(ctx.address, init + ctx_size / 2,\n\t\t\t       ctx_size / 2);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto e_ctx;\n\t\t}\n\t} else {\n\t\t/* Restore the context */\n\t\tret = ccp_set_dm_area(&ctx, 0, sha->ctx, 0,\n\t\t\t\t      sb_count * CCP_SB_BYTES);\n\t\tif (ret)\n\t\t\tgoto e_ctx;\n\t}\n\n\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_ctx;\n\t}\n\n\tif (sha->src) {\n\t\t/* Send data to the CCP SHA engine; block_size is set above */\n\t\tret = ccp_init_data(&src, cmd_q, sha->src, sha->src_len,\n\t\t\t\t    block_size, DMA_TO_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_ctx;\n\n\t\twhile (src.sg_wa.bytes_left) {\n\t\t\tccp_prepare_data(&src, NULL, &op, block_size, false);\n\t\t\tif (sha->final && !src.sg_wa.bytes_left)\n\t\t\t\top.eom = 1;\n\n\t\t\tret = cmd_q->ccp->vdata->perform->sha(&op);\n\t\t\tif (ret) {\n\t\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\t\tgoto e_data;\n\t\t\t}\n\n\t\t\tccp_process_data(&src, NULL, &op);\n\t\t}\n\t} else {\n\t\top.eom = 1;\n\t\tret = cmd_q->ccp->vdata->perform->sha(&op);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_data;\n\t\t}\n\t}\n\n\t/* Retrieve the SHA context - convert from LE to BE using\n\t * 32-byte (256-bit) byteswapping to BE\n\t */\n\tret = ccp_copy_from_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t       CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_data;\n\t}\n\n\tif (sha->final) {\n\t\t/* Finishing up, so get the digest */\n\t\tswitch (sha->type) {\n\t\tcase CCP_SHA_TYPE_1:\n\t\tcase CCP_SHA_TYPE_224:\n\t\tcase CCP_SHA_TYPE_256:\n\t\t\tccp_get_dm_area(&ctx, ooffset,\n\t\t\t\t\tsha->ctx, 0,\n\t\t\t\t\tdigest_size);\n\t\t\tbreak;\n\t\tcase CCP_SHA_TYPE_384:\n\t\tcase CCP_SHA_TYPE_512:\n\t\t\tccp_get_dm_area(&ctx, 0,\n\t\t\t\t\tsha->ctx, LSB_ITEM_SIZE - ooffset,\n\t\t\t\t\tLSB_ITEM_SIZE);\n\t\t\tccp_get_dm_area(&ctx, LSB_ITEM_SIZE + ooffset,\n\t\t\t\t\tsha->ctx, 0,\n\t\t\t\t\tLSB_ITEM_SIZE - ooffset);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto e_data;\n\t\t}\n\t} else {\n\t\t/* Stash the context */\n\t\tccp_get_dm_area(&ctx, 0, sha->ctx, 0,\n\t\t\t\tsb_count * CCP_SB_BYTES);\n\t}\n\n\tif (sha->final && sha->opad) {\n\t\t/* HMAC operation, recursively perform final SHA */\n\t\tstruct ccp_cmd hmac_cmd;\n\t\tstruct scatterlist sg;\n\t\tu8 *hmac_buf;\n\n\t\tif (sha->opad_len != block_size) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto e_data;\n\t\t}\n\n\t\thmac_buf = kmalloc(block_size + digest_size, GFP_KERNEL);\n\t\tif (!hmac_buf) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto e_data;\n\t\t}\n\t\tsg_init_one(&sg, hmac_buf, block_size + digest_size);\n\n\t\tscatterwalk_map_and_copy(hmac_buf, sha->opad, 0, block_size, 0);\n\t\tswitch (sha->type) {\n\t\tcase CCP_SHA_TYPE_1:\n\t\tcase CCP_SHA_TYPE_224:\n\t\tcase CCP_SHA_TYPE_256:\n\t\t\tmemcpy(hmac_buf + block_size,\n\t\t\t       ctx.address + ooffset,\n\t\t\t       digest_size);\n\t\t\tbreak;\n\t\tcase CCP_SHA_TYPE_384:\n\t\tcase CCP_SHA_TYPE_512:\n\t\t\tmemcpy(hmac_buf + block_size,\n\t\t\t       ctx.address + LSB_ITEM_SIZE + ooffset,\n\t\t\t       LSB_ITEM_SIZE);\n\t\t\tmemcpy(hmac_buf + block_size +\n\t\t\t       (LSB_ITEM_SIZE - ooffset),\n\t\t\t       ctx.address,\n\t\t\t       LSB_ITEM_SIZE);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tkfree(hmac_buf);\n\t\t\tret = -EINVAL;\n\t\t\tgoto e_data;\n\t\t}\n\n\t\tmemset(&hmac_cmd, 0, sizeof(hmac_cmd));\n\t\thmac_cmd.engine = CCP_ENGINE_SHA;\n\t\thmac_cmd.u.sha.type = sha->type;\n\t\thmac_cmd.u.sha.ctx = sha->ctx;\n\t\thmac_cmd.u.sha.ctx_len = sha->ctx_len;\n\t\thmac_cmd.u.sha.src = &sg;\n\t\thmac_cmd.u.sha.src_len = block_size + digest_size;\n\t\thmac_cmd.u.sha.opad = NULL;\n\t\thmac_cmd.u.sha.opad_len = 0;\n\t\thmac_cmd.u.sha.first = 1;\n\t\thmac_cmd.u.sha.final = 1;\n\t\thmac_cmd.u.sha.msg_bits = (block_size + digest_size) << 3;\n\n\t\tret = ccp_run_sha_cmd(cmd_q, &hmac_cmd);\n\t\tif (ret)\n\t\t\tcmd->engine_error = hmac_cmd.engine_error;\n\n\t\tkfree(hmac_buf);\n\t}\n\ne_data:\n\tif (sha->src)\n\t\tccp_free_data(&src, cmd_q);\n\ne_ctx:\n\tccp_dm_free(&ctx);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int\nccp_run_rsa_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_rsa_engine *rsa = &cmd->u.rsa;\n\tstruct ccp_dm_workarea exp, src, dst;\n\tstruct ccp_op op;\n\tunsigned int sb_count, i_len, o_len;\n\tint ret;\n\n\t/* Check against the maximum allowable size, in bits */\n\tif (rsa->key_size > cmd_q->ccp->vdata->rsamax)\n\t\treturn -EINVAL;\n\n\tif (!rsa->exp || !rsa->mod || !rsa->src || !rsa->dst)\n\t\treturn -EINVAL;\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\n\t/* The RSA modulus must precede the message being acted upon, so\n\t * it must be copied to a DMA area where the message and the\n\t * modulus can be concatenated.  Therefore the input buffer\n\t * length required is twice the output buffer length (which\n\t * must be a multiple of 256-bits).  Compute o_len, i_len in bytes.\n\t * Buffer sizes must be a multiple of 32 bytes; rounding up may be\n\t * required.\n\t */\n\to_len = 32 * ((rsa->key_size + 255) / 256);\n\ti_len = o_len * 2;\n\n\tsb_count = 0;\n\tif (cmd_q->ccp->vdata->version < CCP_VERSION(5, 0)) {\n\t\t/* sb_count is the number of storage block slots required\n\t\t * for the modulus.\n\t\t */\n\t\tsb_count = o_len / CCP_SB_BYTES;\n\t\top.sb_key = cmd_q->ccp->vdata->perform->sballoc(cmd_q,\n\t\t\t\t\t\t\t\tsb_count);\n\t\tif (!op.sb_key)\n\t\t\treturn -EIO;\n\t} else {\n\t\t/* A version 5 device allows a modulus size that will not fit\n\t\t * in the LSB, so the command will transfer it from memory.\n\t\t * Set the sb key to the default, even though it's not used.\n\t\t */\n\t\top.sb_key = cmd_q->sb_key;\n\t}\n\n\t/* The RSA exponent must be in little endian format. Reverse its\n\t * byte order.\n\t */\n\tret = ccp_init_dm_workarea(&exp, cmd_q, o_len, DMA_TO_DEVICE);\n\tif (ret)\n\t\tgoto e_sb;\n\n\tret = ccp_reverse_set_dm_area(&exp, 0, rsa->exp, 0, rsa->exp_len);\n\tif (ret)\n\t\tgoto e_exp;\n\n\tif (cmd_q->ccp->vdata->version < CCP_VERSION(5, 0)) {\n\t\t/* Copy the exponent to the local storage block, using\n\t\t * as many 32-byte blocks as were allocated above. It's\n\t\t * already little endian, so no further change is required.\n\t\t */\n\t\tret = ccp_copy_to_sb(cmd_q, &exp, op.jobid, op.sb_key,\n\t\t\t\t     CCP_PASSTHRU_BYTESWAP_NOOP);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_exp;\n\t\t}\n\t} else {\n\t\t/* The exponent can be retrieved from memory via DMA. */\n\t\top.exp.u.dma.address = exp.dma.address;\n\t\top.exp.u.dma.offset = 0;\n\t}\n\n\t/* Concatenate the modulus and the message. Both the modulus and\n\t * the operands must be in little endian format.  Since the input\n\t * is in big endian format it must be converted.\n\t */\n\tret = ccp_init_dm_workarea(&src, cmd_q, i_len, DMA_TO_DEVICE);\n\tif (ret)\n\t\tgoto e_exp;\n\n\tret = ccp_reverse_set_dm_area(&src, 0, rsa->mod, 0, rsa->mod_len);\n\tif (ret)\n\t\tgoto e_src;\n\tret = ccp_reverse_set_dm_area(&src, o_len, rsa->src, 0, rsa->src_len);\n\tif (ret)\n\t\tgoto e_src;\n\n\t/* Prepare the output area for the operation */\n\tret = ccp_init_dm_workarea(&dst, cmd_q, o_len, DMA_FROM_DEVICE);\n\tif (ret)\n\t\tgoto e_src;\n\n\top.soc = 1;\n\top.src.u.dma.address = src.dma.address;\n\top.src.u.dma.offset = 0;\n\top.src.u.dma.length = i_len;\n\top.dst.u.dma.address = dst.dma.address;\n\top.dst.u.dma.offset = 0;\n\top.dst.u.dma.length = o_len;\n\n\top.u.rsa.mod_size = rsa->key_size;\n\top.u.rsa.input_len = i_len;\n\n\tret = cmd_q->ccp->vdata->perform->rsa(&op);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\tccp_reverse_get_dm_area(&dst, 0, rsa->dst, 0, rsa->mod_len);\n\ne_dst:\n\tccp_dm_free(&dst);\n\ne_src:\n\tccp_dm_free(&src);\n\ne_exp:\n\tccp_dm_free(&exp);\n\ne_sb:\n\tif (sb_count)\n\t\tcmd_q->ccp->vdata->perform->sbfree(cmd_q, op.sb_key, sb_count);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int\nccp_run_passthru_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_passthru_engine *pt = &cmd->u.passthru;\n\tstruct ccp_dm_workarea mask;\n\tstruct ccp_data src, dst;\n\tstruct ccp_op op;\n\tbool in_place = false;\n\tunsigned int i;\n\tint ret = 0;\n\n\tif (!pt->final && (pt->src_len & (CCP_PASSTHRU_BLOCKSIZE - 1)))\n\t\treturn -EINVAL;\n\n\tif (!pt->src || !pt->dst)\n\t\treturn -EINVAL;\n\n\tif (pt->bit_mod != CCP_PASSTHRU_BITWISE_NOOP) {\n\t\tif (pt->mask_len != CCP_PASSTHRU_MASKSIZE)\n\t\t\treturn -EINVAL;\n\t\tif (!pt->mask)\n\t\t\treturn -EINVAL;\n\t}\n\n\tBUILD_BUG_ON(CCP_PASSTHRU_SB_COUNT != 1);\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\n\tif (pt->bit_mod != CCP_PASSTHRU_BITWISE_NOOP) {\n\t\t/* Load the mask */\n\t\top.sb_key = cmd_q->sb_key;\n\n\t\tret = ccp_init_dm_workarea(&mask, cmd_q,\n\t\t\t\t\t   CCP_PASSTHRU_SB_COUNT *\n\t\t\t\t\t   CCP_SB_BYTES,\n\t\t\t\t\t   DMA_TO_DEVICE);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = ccp_set_dm_area(&mask, 0, pt->mask, 0, pt->mask_len);\n\t\tif (ret)\n\t\t\tgoto e_mask;\n\t\tret = ccp_copy_to_sb(cmd_q, &mask, op.jobid, op.sb_key,\n\t\t\t\t     CCP_PASSTHRU_BYTESWAP_NOOP);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_mask;\n\t\t}\n\t}\n\n\t/* Prepare the input and output data workareas. For in-place\n\t * operations we need to set the dma direction to BIDIRECTIONAL\n\t * and copy the src workarea to the dst workarea.\n\t */\n\tif (sg_virt(pt->src) == sg_virt(pt->dst))\n\t\tin_place = true;\n\n\tret = ccp_init_data(&src, cmd_q, pt->src, pt->src_len,\n\t\t\t    CCP_PASSTHRU_MASKSIZE,\n\t\t\t    in_place ? DMA_BIDIRECTIONAL : DMA_TO_DEVICE);\n\tif (ret)\n\t\tgoto e_mask;\n\n\tif (in_place) {\n\t\tdst = src;\n\t} else {\n\t\tret = ccp_init_data(&dst, cmd_q, pt->dst, pt->src_len,\n\t\t\t\t    CCP_PASSTHRU_MASKSIZE, DMA_FROM_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_src;\n\t}\n\n\t/* Send data to the CCP Passthru engine\n\t *   Because the CCP engine works on a single source and destination\n\t *   dma address at a time, each entry in the source scatterlist\n\t *   (after the dma_map_sg call) must be less than or equal to the\n\t *   (remaining) length in the destination scatterlist entry and the\n\t *   length must be a multiple of CCP_PASSTHRU_BLOCKSIZE\n\t */\n\tdst.sg_wa.sg_used = 0;\n\tfor (i = 1; i <= src.sg_wa.dma_count; i++) {\n\t\tif (!dst.sg_wa.sg ||\n\t\t    (sg_dma_len(dst.sg_wa.sg) < sg_dma_len(src.sg_wa.sg))) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto e_dst;\n\t\t}\n\n\t\tif (i == src.sg_wa.dma_count) {\n\t\t\top.eom = 1;\n\t\t\top.soc = 1;\n\t\t}\n\n\t\top.src.type = CCP_MEMTYPE_SYSTEM;\n\t\top.src.u.dma.address = sg_dma_address(src.sg_wa.sg);\n\t\top.src.u.dma.offset = 0;\n\t\top.src.u.dma.length = sg_dma_len(src.sg_wa.sg);\n\n\t\top.dst.type = CCP_MEMTYPE_SYSTEM;\n\t\top.dst.u.dma.address = sg_dma_address(dst.sg_wa.sg);\n\t\top.dst.u.dma.offset = dst.sg_wa.sg_used;\n\t\top.dst.u.dma.length = op.src.u.dma.length;\n\n\t\tret = cmd_q->ccp->vdata->perform->passthru(&op);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_dst;\n\t\t}\n\n\t\tdst.sg_wa.sg_used += sg_dma_len(src.sg_wa.sg);\n\t\tif (dst.sg_wa.sg_used == sg_dma_len(dst.sg_wa.sg)) {\n\t\t\tdst.sg_wa.sg = sg_next(dst.sg_wa.sg);\n\t\t\tdst.sg_wa.sg_used = 0;\n\t\t}\n\t\tsrc.sg_wa.sg = sg_next(src.sg_wa.sg);\n\t}\n\ne_dst:\n\tif (!in_place)\n\t\tccp_free_data(&dst, cmd_q);\n\ne_src:\n\tccp_free_data(&src, cmd_q);\n\ne_mask:\n\tif (pt->bit_mod != CCP_PASSTHRU_BITWISE_NOOP)\n\t\tccp_dm_free(&mask);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int\nccp_run_passthru_nomap_cmd(struct ccp_cmd_queue *cmd_q,\n\t\t\t\t      struct ccp_cmd *cmd)\n{\n\tstruct ccp_passthru_nomap_engine *pt = &cmd->u.passthru_nomap;\n\tstruct ccp_dm_workarea mask;\n\tstruct ccp_op op;\n\tint ret;\n\n\tif (!pt->final && (pt->src_len & (CCP_PASSTHRU_BLOCKSIZE - 1)))\n\t\treturn -EINVAL;\n\n\tif (!pt->src_dma || !pt->dst_dma)\n\t\treturn -EINVAL;\n\n\tif (pt->bit_mod != CCP_PASSTHRU_BITWISE_NOOP) {\n\t\tif (pt->mask_len != CCP_PASSTHRU_MASKSIZE)\n\t\t\treturn -EINVAL;\n\t\tif (!pt->mask)\n\t\t\treturn -EINVAL;\n\t}\n\n\tBUILD_BUG_ON(CCP_PASSTHRU_SB_COUNT != 1);\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\n\tif (pt->bit_mod != CCP_PASSTHRU_BITWISE_NOOP) {\n\t\t/* Load the mask */\n\t\top.sb_key = cmd_q->sb_key;\n\n\t\tmask.length = pt->mask_len;\n\t\tmask.dma.address = pt->mask;\n\t\tmask.dma.length = pt->mask_len;\n\n\t\tret = ccp_copy_to_sb(cmd_q, &mask, op.jobid, op.sb_key,\n\t\t\t\t     CCP_PASSTHRU_BYTESWAP_NOOP);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t/* Send data to the CCP Passthru engine */\n\top.eom = 1;\n\top.soc = 1;\n\n\top.src.type = CCP_MEMTYPE_SYSTEM;\n\top.src.u.dma.address = pt->src_dma;\n\top.src.u.dma.offset = 0;\n\top.src.u.dma.length = pt->src_len;\n\n\top.dst.type = CCP_MEMTYPE_SYSTEM;\n\top.dst.u.dma.address = pt->dst_dma;\n\top.dst.u.dma.offset = 0;\n\top.dst.u.dma.length = pt->src_len;\n\n\tret = cmd_q->ccp->vdata->perform->passthru(&op);\n\tif (ret)\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\n\treturn ret;\n}\n\nstatic int ccp_run_ecc_mm_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_ecc_engine *ecc = &cmd->u.ecc;\n\tstruct ccp_dm_workarea src, dst;\n\tstruct ccp_op op;\n\tint ret;\n\tu8 *save;\n\n\tif (!ecc->u.mm.operand_1 ||\n\t    (ecc->u.mm.operand_1_len > CCP_ECC_MODULUS_BYTES))\n\t\treturn -EINVAL;\n\n\tif (ecc->function != CCP_ECC_FUNCTION_MINV_384BIT)\n\t\tif (!ecc->u.mm.operand_2 ||\n\t\t    (ecc->u.mm.operand_2_len > CCP_ECC_MODULUS_BYTES))\n\t\t\treturn -EINVAL;\n\n\tif (!ecc->u.mm.result ||\n\t    (ecc->u.mm.result_len < CCP_ECC_MODULUS_BYTES))\n\t\treturn -EINVAL;\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\n\t/* Concatenate the modulus and the operands. Both the modulus and\n\t * the operands must be in little endian format.  Since the input\n\t * is in big endian format it must be converted and placed in a\n\t * fixed length buffer.\n\t */\n\tret = ccp_init_dm_workarea(&src, cmd_q, CCP_ECC_SRC_BUF_SIZE,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Save the workarea address since it is updated in order to perform\n\t * the concatenation\n\t */\n\tsave = src.address;\n\n\t/* Copy the ECC modulus */\n\tret = ccp_reverse_set_dm_area(&src, 0, ecc->mod, 0, ecc->mod_len);\n\tif (ret)\n\t\tgoto e_src;\n\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\n\t/* Copy the first operand */\n\tret = ccp_reverse_set_dm_area(&src, 0, ecc->u.mm.operand_1, 0,\n\t\t\t\t      ecc->u.mm.operand_1_len);\n\tif (ret)\n\t\tgoto e_src;\n\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\n\tif (ecc->function != CCP_ECC_FUNCTION_MINV_384BIT) {\n\t\t/* Copy the second operand */\n\t\tret = ccp_reverse_set_dm_area(&src, 0, ecc->u.mm.operand_2, 0,\n\t\t\t\t\t      ecc->u.mm.operand_2_len);\n\t\tif (ret)\n\t\t\tgoto e_src;\n\t\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\t}\n\n\t/* Restore the workarea address */\n\tsrc.address = save;\n\n\t/* Prepare the output area for the operation */\n\tret = ccp_init_dm_workarea(&dst, cmd_q, CCP_ECC_DST_BUF_SIZE,\n\t\t\t\t   DMA_FROM_DEVICE);\n\tif (ret)\n\t\tgoto e_src;\n\n\top.soc = 1;\n\top.src.u.dma.address = src.dma.address;\n\top.src.u.dma.offset = 0;\n\top.src.u.dma.length = src.length;\n\top.dst.u.dma.address = dst.dma.address;\n\top.dst.u.dma.offset = 0;\n\top.dst.u.dma.length = dst.length;\n\n\top.u.ecc.function = cmd->u.ecc.function;\n\n\tret = cmd_q->ccp->vdata->perform->ecc(&op);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\tecc->ecc_result = le16_to_cpup(\n\t\t(const __le16 *)(dst.address + CCP_ECC_RESULT_OFFSET));\n\tif (!(ecc->ecc_result & CCP_ECC_RESULT_SUCCESS)) {\n\t\tret = -EIO;\n\t\tgoto e_dst;\n\t}\n\n\t/* Save the ECC result */\n\tccp_reverse_get_dm_area(&dst, 0, ecc->u.mm.result, 0,\n\t\t\t\tCCP_ECC_MODULUS_BYTES);\n\ne_dst:\n\tccp_dm_free(&dst);\n\ne_src:\n\tccp_dm_free(&src);\n\n\treturn ret;\n}\n\nstatic int ccp_run_ecc_pm_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_ecc_engine *ecc = &cmd->u.ecc;\n\tstruct ccp_dm_workarea src, dst;\n\tstruct ccp_op op;\n\tint ret;\n\tu8 *save;\n\n\tif (!ecc->u.pm.point_1.x ||\n\t    (ecc->u.pm.point_1.x_len > CCP_ECC_MODULUS_BYTES) ||\n\t    !ecc->u.pm.point_1.y ||\n\t    (ecc->u.pm.point_1.y_len > CCP_ECC_MODULUS_BYTES))\n\t\treturn -EINVAL;\n\n\tif (ecc->function == CCP_ECC_FUNCTION_PADD_384BIT) {\n\t\tif (!ecc->u.pm.point_2.x ||\n\t\t    (ecc->u.pm.point_2.x_len > CCP_ECC_MODULUS_BYTES) ||\n\t\t    !ecc->u.pm.point_2.y ||\n\t\t    (ecc->u.pm.point_2.y_len > CCP_ECC_MODULUS_BYTES))\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (!ecc->u.pm.domain_a ||\n\t\t    (ecc->u.pm.domain_a_len > CCP_ECC_MODULUS_BYTES))\n\t\t\treturn -EINVAL;\n\n\t\tif (ecc->function == CCP_ECC_FUNCTION_PMUL_384BIT)\n\t\t\tif (!ecc->u.pm.scalar ||\n\t\t\t    (ecc->u.pm.scalar_len > CCP_ECC_MODULUS_BYTES))\n\t\t\t\treturn -EINVAL;\n\t}\n\n\tif (!ecc->u.pm.result.x ||\n\t    (ecc->u.pm.result.x_len < CCP_ECC_MODULUS_BYTES) ||\n\t    !ecc->u.pm.result.y ||\n\t    (ecc->u.pm.result.y_len < CCP_ECC_MODULUS_BYTES))\n\t\treturn -EINVAL;\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\n\t/* Concatenate the modulus and the operands. Both the modulus and\n\t * the operands must be in little endian format.  Since the input\n\t * is in big endian format it must be converted and placed in a\n\t * fixed length buffer.\n\t */\n\tret = ccp_init_dm_workarea(&src, cmd_q, CCP_ECC_SRC_BUF_SIZE,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Save the workarea address since it is updated in order to perform\n\t * the concatenation\n\t */\n\tsave = src.address;\n\n\t/* Copy the ECC modulus */\n\tret = ccp_reverse_set_dm_area(&src, 0, ecc->mod, 0, ecc->mod_len);\n\tif (ret)\n\t\tgoto e_src;\n\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\n\t/* Copy the first point X and Y coordinate */\n\tret = ccp_reverse_set_dm_area(&src, 0, ecc->u.pm.point_1.x, 0,\n\t\t\t\t      ecc->u.pm.point_1.x_len);\n\tif (ret)\n\t\tgoto e_src;\n\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\tret = ccp_reverse_set_dm_area(&src, 0, ecc->u.pm.point_1.y, 0,\n\t\t\t\t      ecc->u.pm.point_1.y_len);\n\tif (ret)\n\t\tgoto e_src;\n\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\n\t/* Set the first point Z coordinate to 1 */\n\t*src.address = 0x01;\n\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\n\tif (ecc->function == CCP_ECC_FUNCTION_PADD_384BIT) {\n\t\t/* Copy the second point X and Y coordinate */\n\t\tret = ccp_reverse_set_dm_area(&src, 0, ecc->u.pm.point_2.x, 0,\n\t\t\t\t\t      ecc->u.pm.point_2.x_len);\n\t\tif (ret)\n\t\t\tgoto e_src;\n\t\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\t\tret = ccp_reverse_set_dm_area(&src, 0, ecc->u.pm.point_2.y, 0,\n\t\t\t\t\t      ecc->u.pm.point_2.y_len);\n\t\tif (ret)\n\t\t\tgoto e_src;\n\t\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\n\t\t/* Set the second point Z coordinate to 1 */\n\t\t*src.address = 0x01;\n\t\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\t} else {\n\t\t/* Copy the Domain \"a\" parameter */\n\t\tret = ccp_reverse_set_dm_area(&src, 0, ecc->u.pm.domain_a, 0,\n\t\t\t\t\t      ecc->u.pm.domain_a_len);\n\t\tif (ret)\n\t\t\tgoto e_src;\n\t\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\n\t\tif (ecc->function == CCP_ECC_FUNCTION_PMUL_384BIT) {\n\t\t\t/* Copy the scalar value */\n\t\t\tret = ccp_reverse_set_dm_area(&src, 0,\n\t\t\t\t\t\t      ecc->u.pm.scalar, 0,\n\t\t\t\t\t\t      ecc->u.pm.scalar_len);\n\t\t\tif (ret)\n\t\t\t\tgoto e_src;\n\t\t\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\t\t}\n\t}\n\n\t/* Restore the workarea address */\n\tsrc.address = save;\n\n\t/* Prepare the output area for the operation */\n\tret = ccp_init_dm_workarea(&dst, cmd_q, CCP_ECC_DST_BUF_SIZE,\n\t\t\t\t   DMA_FROM_DEVICE);\n\tif (ret)\n\t\tgoto e_src;\n\n\top.soc = 1;\n\top.src.u.dma.address = src.dma.address;\n\top.src.u.dma.offset = 0;\n\top.src.u.dma.length = src.length;\n\top.dst.u.dma.address = dst.dma.address;\n\top.dst.u.dma.offset = 0;\n\top.dst.u.dma.length = dst.length;\n\n\top.u.ecc.function = cmd->u.ecc.function;\n\n\tret = cmd_q->ccp->vdata->perform->ecc(&op);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\tecc->ecc_result = le16_to_cpup(\n\t\t(const __le16 *)(dst.address + CCP_ECC_RESULT_OFFSET));\n\tif (!(ecc->ecc_result & CCP_ECC_RESULT_SUCCESS)) {\n\t\tret = -EIO;\n\t\tgoto e_dst;\n\t}\n\n\t/* Save the workarea address since it is updated as we walk through\n\t * to copy the point math result\n\t */\n\tsave = dst.address;\n\n\t/* Save the ECC result X and Y coordinates */\n\tccp_reverse_get_dm_area(&dst, 0, ecc->u.pm.result.x, 0,\n\t\t\t\tCCP_ECC_MODULUS_BYTES);\n\tdst.address += CCP_ECC_OUTPUT_SIZE;\n\tccp_reverse_get_dm_area(&dst, 0, ecc->u.pm.result.y, 0,\n\t\t\t\tCCP_ECC_MODULUS_BYTES);\n\n\t/* Restore the workarea address */\n\tdst.address = save;\n\ne_dst:\n\tccp_dm_free(&dst);\n\ne_src:\n\tccp_dm_free(&src);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int\nccp_run_ecc_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_ecc_engine *ecc = &cmd->u.ecc;\n\n\tecc->ecc_result = 0;\n\n\tif (!ecc->mod ||\n\t    (ecc->mod_len > CCP_ECC_MODULUS_BYTES))\n\t\treturn -EINVAL;\n\n\tswitch (ecc->function) {\n\tcase CCP_ECC_FUNCTION_MMUL_384BIT:\n\tcase CCP_ECC_FUNCTION_MADD_384BIT:\n\tcase CCP_ECC_FUNCTION_MINV_384BIT:\n\t\treturn ccp_run_ecc_mm_cmd(cmd_q, cmd);\n\n\tcase CCP_ECC_FUNCTION_PADD_384BIT:\n\tcase CCP_ECC_FUNCTION_PMUL_384BIT:\n\tcase CCP_ECC_FUNCTION_PDBL_384BIT:\n\t\treturn ccp_run_ecc_pm_cmd(cmd_q, cmd);\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nint ccp_run_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tint ret;\n\n\tcmd->engine_error = 0;\n\tcmd_q->cmd_error = 0;\n\tcmd_q->int_rcvd = 0;\n\tcmd_q->free_slots = cmd_q->ccp->vdata->perform->get_free_slots(cmd_q);\n\n\tswitch (cmd->engine) {\n\tcase CCP_ENGINE_AES:\n\t\tswitch (cmd->u.aes.mode) {\n\t\tcase CCP_AES_MODE_CMAC:\n\t\t\tret = ccp_run_aes_cmac_cmd(cmd_q, cmd);\n\t\t\tbreak;\n\t\tcase CCP_AES_MODE_GCM:\n\t\t\tret = ccp_run_aes_gcm_cmd(cmd_q, cmd);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = ccp_run_aes_cmd(cmd_q, cmd);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase CCP_ENGINE_XTS_AES_128:\n\t\tret = ccp_run_xts_aes_cmd(cmd_q, cmd);\n\t\tbreak;\n\tcase CCP_ENGINE_DES3:\n\t\tret = ccp_run_des3_cmd(cmd_q, cmd);\n\t\tbreak;\n\tcase CCP_ENGINE_SHA:\n\t\tret = ccp_run_sha_cmd(cmd_q, cmd);\n\t\tbreak;\n\tcase CCP_ENGINE_RSA:\n\t\tret = ccp_run_rsa_cmd(cmd_q, cmd);\n\t\tbreak;\n\tcase CCP_ENGINE_PASSTHRU:\n\t\tif (cmd->flags & CCP_CMD_PASSTHRU_NO_DMA_MAP)\n\t\t\tret = ccp_run_passthru_nomap_cmd(cmd_q, cmd);\n\t\telse\n\t\t\tret = ccp_run_passthru_cmd(cmd_q, cmd);\n\t\tbreak;\n\tcase CCP_ENGINE_ECC:\n\t\tret = ccp_run_ecc_cmd(cmd_q, cmd);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/*\n * AMD Cryptographic Coprocessor (CCP) driver\n *\n * Copyright (C) 2013-2019 Advanced Micro Devices, Inc.\n *\n * Author: Tom Lendacky <thomas.lendacky@amd.com>\n * Author: Gary R Hook <gary.hook@amd.com>\n */\n\n#include <linux/dma-mapping.h>\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/interrupt.h>\n#include <crypto/scatterwalk.h>\n#include <crypto/des.h>\n#include <linux/ccp.h>\n\n#include \"ccp-dev.h\"\n\n/* SHA initial context values */\nstatic const __be32 ccp_sha1_init[SHA1_DIGEST_SIZE / sizeof(__be32)] = {\n\tcpu_to_be32(SHA1_H0), cpu_to_be32(SHA1_H1),\n\tcpu_to_be32(SHA1_H2), cpu_to_be32(SHA1_H3),\n\tcpu_to_be32(SHA1_H4),\n};\n\nstatic const __be32 ccp_sha224_init[SHA256_DIGEST_SIZE / sizeof(__be32)] = {\n\tcpu_to_be32(SHA224_H0), cpu_to_be32(SHA224_H1),\n\tcpu_to_be32(SHA224_H2), cpu_to_be32(SHA224_H3),\n\tcpu_to_be32(SHA224_H4), cpu_to_be32(SHA224_H5),\n\tcpu_to_be32(SHA224_H6), cpu_to_be32(SHA224_H7),\n};\n\nstatic const __be32 ccp_sha256_init[SHA256_DIGEST_SIZE / sizeof(__be32)] = {\n\tcpu_to_be32(SHA256_H0), cpu_to_be32(SHA256_H1),\n\tcpu_to_be32(SHA256_H2), cpu_to_be32(SHA256_H3),\n\tcpu_to_be32(SHA256_H4), cpu_to_be32(SHA256_H5),\n\tcpu_to_be32(SHA256_H6), cpu_to_be32(SHA256_H7),\n};\n\nstatic const __be64 ccp_sha384_init[SHA512_DIGEST_SIZE / sizeof(__be64)] = {\n\tcpu_to_be64(SHA384_H0), cpu_to_be64(SHA384_H1),\n\tcpu_to_be64(SHA384_H2), cpu_to_be64(SHA384_H3),\n\tcpu_to_be64(SHA384_H4), cpu_to_be64(SHA384_H5),\n\tcpu_to_be64(SHA384_H6), cpu_to_be64(SHA384_H7),\n};\n\nstatic const __be64 ccp_sha512_init[SHA512_DIGEST_SIZE / sizeof(__be64)] = {\n\tcpu_to_be64(SHA512_H0), cpu_to_be64(SHA512_H1),\n\tcpu_to_be64(SHA512_H2), cpu_to_be64(SHA512_H3),\n\tcpu_to_be64(SHA512_H4), cpu_to_be64(SHA512_H5),\n\tcpu_to_be64(SHA512_H6), cpu_to_be64(SHA512_H7),\n};\n\n#define\tCCP_NEW_JOBID(ccp)\t((ccp->vdata->version == CCP_VERSION(3, 0)) ? \\\n\t\t\t\t\tccp_gen_jobid(ccp) : 0)\n\nstatic u32 ccp_gen_jobid(struct ccp_device *ccp)\n{\n\treturn atomic_inc_return(&ccp->current_id) & CCP_JOBID_MASK;\n}\n\nstatic void ccp_sg_free(struct ccp_sg_workarea *wa)\n{\n\tif (wa->dma_count)\n\t\tdma_unmap_sg(wa->dma_dev, wa->dma_sg_head, wa->nents, wa->dma_dir);\n\n\twa->dma_count = 0;\n}\n\nstatic int ccp_init_sg_workarea(struct ccp_sg_workarea *wa, struct device *dev,\n\t\t\t\tstruct scatterlist *sg, u64 len,\n\t\t\t\tenum dma_data_direction dma_dir)\n{\n\tmemset(wa, 0, sizeof(*wa));\n\n\twa->sg = sg;\n\tif (!sg)\n\t\treturn 0;\n\n\twa->nents = sg_nents_for_len(sg, len);\n\tif (wa->nents < 0)\n\t\treturn wa->nents;\n\n\twa->bytes_left = len;\n\twa->sg_used = 0;\n\n\tif (len == 0)\n\t\treturn 0;\n\n\tif (dma_dir == DMA_NONE)\n\t\treturn 0;\n\n\twa->dma_sg = sg;\n\twa->dma_sg_head = sg;\n\twa->dma_dev = dev;\n\twa->dma_dir = dma_dir;\n\twa->dma_count = dma_map_sg(dev, sg, wa->nents, dma_dir);\n\tif (!wa->dma_count)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void ccp_update_sg_workarea(struct ccp_sg_workarea *wa, unsigned int len)\n{\n\tunsigned int nbytes = min_t(u64, len, wa->bytes_left);\n\tunsigned int sg_combined_len = 0;\n\n\tif (!wa->sg)\n\t\treturn;\n\n\twa->sg_used += nbytes;\n\twa->bytes_left -= nbytes;\n\tif (wa->sg_used == sg_dma_len(wa->dma_sg)) {\n\t\t/* Advance to the next DMA scatterlist entry */\n\t\twa->dma_sg = sg_next(wa->dma_sg);\n\n\t\t/* In the case that the DMA mapped scatterlist has entries\n\t\t * that have been merged, the non-DMA mapped scatterlist\n\t\t * must be advanced multiple times for each merged entry.\n\t\t * This ensures that the current non-DMA mapped entry\n\t\t * corresponds to the current DMA mapped entry.\n\t\t */\n\t\tdo {\n\t\t\tsg_combined_len += wa->sg->length;\n\t\t\twa->sg = sg_next(wa->sg);\n\t\t} while (wa->sg_used > sg_combined_len);\n\n\t\twa->sg_used = 0;\n\t}\n}\n\nstatic void ccp_dm_free(struct ccp_dm_workarea *wa)\n{\n\tif (wa->length <= CCP_DMAPOOL_MAX_SIZE) {\n\t\tif (wa->address)\n\t\t\tdma_pool_free(wa->dma_pool, wa->address,\n\t\t\t\t      wa->dma.address);\n\t} else {\n\t\tif (wa->dma.address)\n\t\t\tdma_unmap_single(wa->dev, wa->dma.address, wa->length,\n\t\t\t\t\t wa->dma.dir);\n\t\tkfree(wa->address);\n\t}\n\n\twa->address = NULL;\n\twa->dma.address = 0;\n}\n\nstatic int ccp_init_dm_workarea(struct ccp_dm_workarea *wa,\n\t\t\t\tstruct ccp_cmd_queue *cmd_q,\n\t\t\t\tunsigned int len,\n\t\t\t\tenum dma_data_direction dir)\n{\n\tmemset(wa, 0, sizeof(*wa));\n\n\tif (!len)\n\t\treturn 0;\n\n\twa->dev = cmd_q->ccp->dev;\n\twa->length = len;\n\n\tif (len <= CCP_DMAPOOL_MAX_SIZE) {\n\t\twa->dma_pool = cmd_q->dma_pool;\n\n\t\twa->address = dma_pool_zalloc(wa->dma_pool, GFP_KERNEL,\n\t\t\t\t\t     &wa->dma.address);\n\t\tif (!wa->address)\n\t\t\treturn -ENOMEM;\n\n\t\twa->dma.length = CCP_DMAPOOL_MAX_SIZE;\n\n\t} else {\n\t\twa->address = kzalloc(len, GFP_KERNEL);\n\t\tif (!wa->address)\n\t\t\treturn -ENOMEM;\n\n\t\twa->dma.address = dma_map_single(wa->dev, wa->address, len,\n\t\t\t\t\t\t dir);\n\t\tif (dma_mapping_error(wa->dev, wa->dma.address))\n\t\t\treturn -ENOMEM;\n\n\t\twa->dma.length = len;\n\t}\n\twa->dma.dir = dir;\n\n\treturn 0;\n}\n\nstatic int ccp_set_dm_area(struct ccp_dm_workarea *wa, unsigned int wa_offset,\n\t\t\t   struct scatterlist *sg, unsigned int sg_offset,\n\t\t\t   unsigned int len)\n{\n\tWARN_ON(!wa->address);\n\n\tif (len > (wa->length - wa_offset))\n\t\treturn -EINVAL;\n\n\tscatterwalk_map_and_copy(wa->address + wa_offset, sg, sg_offset, len,\n\t\t\t\t 0);\n\treturn 0;\n}\n\nstatic void ccp_get_dm_area(struct ccp_dm_workarea *wa, unsigned int wa_offset,\n\t\t\t    struct scatterlist *sg, unsigned int sg_offset,\n\t\t\t    unsigned int len)\n{\n\tWARN_ON(!wa->address);\n\n\tscatterwalk_map_and_copy(wa->address + wa_offset, sg, sg_offset, len,\n\t\t\t\t 1);\n}\n\nstatic int ccp_reverse_set_dm_area(struct ccp_dm_workarea *wa,\n\t\t\t\t   unsigned int wa_offset,\n\t\t\t\t   struct scatterlist *sg,\n\t\t\t\t   unsigned int sg_offset,\n\t\t\t\t   unsigned int len)\n{\n\tu8 *p, *q;\n\tint\trc;\n\n\trc = ccp_set_dm_area(wa, wa_offset, sg, sg_offset, len);\n\tif (rc)\n\t\treturn rc;\n\n\tp = wa->address + wa_offset;\n\tq = p + len - 1;\n\twhile (p < q) {\n\t\t*p = *p ^ *q;\n\t\t*q = *p ^ *q;\n\t\t*p = *p ^ *q;\n\t\tp++;\n\t\tq--;\n\t}\n\treturn 0;\n}\n\nstatic void ccp_reverse_get_dm_area(struct ccp_dm_workarea *wa,\n\t\t\t\t    unsigned int wa_offset,\n\t\t\t\t    struct scatterlist *sg,\n\t\t\t\t    unsigned int sg_offset,\n\t\t\t\t    unsigned int len)\n{\n\tu8 *p, *q;\n\n\tp = wa->address + wa_offset;\n\tq = p + len - 1;\n\twhile (p < q) {\n\t\t*p = *p ^ *q;\n\t\t*q = *p ^ *q;\n\t\t*p = *p ^ *q;\n\t\tp++;\n\t\tq--;\n\t}\n\n\tccp_get_dm_area(wa, wa_offset, sg, sg_offset, len);\n}\n\nstatic void ccp_free_data(struct ccp_data *data, struct ccp_cmd_queue *cmd_q)\n{\n\tccp_dm_free(&data->dm_wa);\n\tccp_sg_free(&data->sg_wa);\n}\n\nstatic int ccp_init_data(struct ccp_data *data, struct ccp_cmd_queue *cmd_q,\n\t\t\t struct scatterlist *sg, u64 sg_len,\n\t\t\t unsigned int dm_len,\n\t\t\t enum dma_data_direction dir)\n{\n\tint ret;\n\n\tmemset(data, 0, sizeof(*data));\n\n\tret = ccp_init_sg_workarea(&data->sg_wa, cmd_q->ccp->dev, sg, sg_len,\n\t\t\t\t   dir);\n\tif (ret)\n\t\tgoto e_err;\n\n\tret = ccp_init_dm_workarea(&data->dm_wa, cmd_q, dm_len, dir);\n\tif (ret)\n\t\tgoto e_err;\n\n\treturn 0;\n\ne_err:\n\tccp_free_data(data, cmd_q);\n\n\treturn ret;\n}\n\nstatic unsigned int ccp_queue_buf(struct ccp_data *data, unsigned int from)\n{\n\tstruct ccp_sg_workarea *sg_wa = &data->sg_wa;\n\tstruct ccp_dm_workarea *dm_wa = &data->dm_wa;\n\tunsigned int buf_count, nbytes;\n\n\t/* Clear the buffer if setting it */\n\tif (!from)\n\t\tmemset(dm_wa->address, 0, dm_wa->length);\n\n\tif (!sg_wa->sg)\n\t\treturn 0;\n\n\t/* Perform the copy operation\n\t *   nbytes will always be <= UINT_MAX because dm_wa->length is\n\t *   an unsigned int\n\t */\n\tnbytes = min_t(u64, sg_wa->bytes_left, dm_wa->length);\n\tscatterwalk_map_and_copy(dm_wa->address, sg_wa->sg, sg_wa->sg_used,\n\t\t\t\t nbytes, from);\n\n\t/* Update the structures and generate the count */\n\tbuf_count = 0;\n\twhile (sg_wa->bytes_left && (buf_count < dm_wa->length)) {\n\t\tnbytes = min(sg_dma_len(sg_wa->dma_sg) - sg_wa->sg_used,\n\t\t\t     dm_wa->length - buf_count);\n\t\tnbytes = min_t(u64, sg_wa->bytes_left, nbytes);\n\n\t\tbuf_count += nbytes;\n\t\tccp_update_sg_workarea(sg_wa, nbytes);\n\t}\n\n\treturn buf_count;\n}\n\nstatic unsigned int ccp_fill_queue_buf(struct ccp_data *data)\n{\n\treturn ccp_queue_buf(data, 0);\n}\n\nstatic unsigned int ccp_empty_queue_buf(struct ccp_data *data)\n{\n\treturn ccp_queue_buf(data, 1);\n}\n\nstatic void ccp_prepare_data(struct ccp_data *src, struct ccp_data *dst,\n\t\t\t     struct ccp_op *op, unsigned int block_size,\n\t\t\t     bool blocksize_op)\n{\n\tunsigned int sg_src_len, sg_dst_len, op_len;\n\n\t/* The CCP can only DMA from/to one address each per operation. This\n\t * requires that we find the smallest DMA area between the source\n\t * and destination. The resulting len values will always be <= UINT_MAX\n\t * because the dma length is an unsigned int.\n\t */\n\tsg_src_len = sg_dma_len(src->sg_wa.dma_sg) - src->sg_wa.sg_used;\n\tsg_src_len = min_t(u64, src->sg_wa.bytes_left, sg_src_len);\n\n\tif (dst) {\n\t\tsg_dst_len = sg_dma_len(dst->sg_wa.dma_sg) - dst->sg_wa.sg_used;\n\t\tsg_dst_len = min_t(u64, src->sg_wa.bytes_left, sg_dst_len);\n\t\top_len = min(sg_src_len, sg_dst_len);\n\t} else {\n\t\top_len = sg_src_len;\n\t}\n\n\t/* The data operation length will be at least block_size in length\n\t * or the smaller of available sg room remaining for the source or\n\t * the destination\n\t */\n\top_len = max(op_len, block_size);\n\n\t/* Unless we have to buffer data, there's no reason to wait */\n\top->soc = 0;\n\n\tif (sg_src_len < block_size) {\n\t\t/* Not enough data in the sg element, so it\n\t\t * needs to be buffered into a blocksize chunk\n\t\t */\n\t\tint cp_len = ccp_fill_queue_buf(src);\n\n\t\top->soc = 1;\n\t\top->src.u.dma.address = src->dm_wa.dma.address;\n\t\top->src.u.dma.offset = 0;\n\t\top->src.u.dma.length = (blocksize_op) ? block_size : cp_len;\n\t} else {\n\t\t/* Enough data in the sg element, but we need to\n\t\t * adjust for any previously copied data\n\t\t */\n\t\top->src.u.dma.address = sg_dma_address(src->sg_wa.dma_sg);\n\t\top->src.u.dma.offset = src->sg_wa.sg_used;\n\t\top->src.u.dma.length = op_len & ~(block_size - 1);\n\n\t\tccp_update_sg_workarea(&src->sg_wa, op->src.u.dma.length);\n\t}\n\n\tif (dst) {\n\t\tif (sg_dst_len < block_size) {\n\t\t\t/* Not enough room in the sg element or we're on the\n\t\t\t * last piece of data (when using padding), so the\n\t\t\t * output needs to be buffered into a blocksize chunk\n\t\t\t */\n\t\t\top->soc = 1;\n\t\t\top->dst.u.dma.address = dst->dm_wa.dma.address;\n\t\t\top->dst.u.dma.offset = 0;\n\t\t\top->dst.u.dma.length = op->src.u.dma.length;\n\t\t} else {\n\t\t\t/* Enough room in the sg element, but we need to\n\t\t\t * adjust for any previously used area\n\t\t\t */\n\t\t\top->dst.u.dma.address = sg_dma_address(dst->sg_wa.dma_sg);\n\t\t\top->dst.u.dma.offset = dst->sg_wa.sg_used;\n\t\t\top->dst.u.dma.length = op->src.u.dma.length;\n\t\t}\n\t}\n}\n\nstatic void ccp_process_data(struct ccp_data *src, struct ccp_data *dst,\n\t\t\t     struct ccp_op *op)\n{\n\top->init = 0;\n\n\tif (dst) {\n\t\tif (op->dst.u.dma.address == dst->dm_wa.dma.address)\n\t\t\tccp_empty_queue_buf(dst);\n\t\telse\n\t\t\tccp_update_sg_workarea(&dst->sg_wa,\n\t\t\t\t\t       op->dst.u.dma.length);\n\t}\n}\n\nstatic int ccp_copy_to_from_sb(struct ccp_cmd_queue *cmd_q,\n\t\t\t       struct ccp_dm_workarea *wa, u32 jobid, u32 sb,\n\t\t\t       u32 byte_swap, bool from)\n{\n\tstruct ccp_op op;\n\n\tmemset(&op, 0, sizeof(op));\n\n\top.cmd_q = cmd_q;\n\top.jobid = jobid;\n\top.eom = 1;\n\n\tif (from) {\n\t\top.soc = 1;\n\t\top.src.type = CCP_MEMTYPE_SB;\n\t\top.src.u.sb = sb;\n\t\top.dst.type = CCP_MEMTYPE_SYSTEM;\n\t\top.dst.u.dma.address = wa->dma.address;\n\t\top.dst.u.dma.length = wa->length;\n\t} else {\n\t\top.src.type = CCP_MEMTYPE_SYSTEM;\n\t\top.src.u.dma.address = wa->dma.address;\n\t\top.src.u.dma.length = wa->length;\n\t\top.dst.type = CCP_MEMTYPE_SB;\n\t\top.dst.u.sb = sb;\n\t}\n\n\top.u.passthru.byte_swap = byte_swap;\n\n\treturn cmd_q->ccp->vdata->perform->passthru(&op);\n}\n\nstatic int ccp_copy_to_sb(struct ccp_cmd_queue *cmd_q,\n\t\t\t  struct ccp_dm_workarea *wa, u32 jobid, u32 sb,\n\t\t\t  u32 byte_swap)\n{\n\treturn ccp_copy_to_from_sb(cmd_q, wa, jobid, sb, byte_swap, false);\n}\n\nstatic int ccp_copy_from_sb(struct ccp_cmd_queue *cmd_q,\n\t\t\t    struct ccp_dm_workarea *wa, u32 jobid, u32 sb,\n\t\t\t    u32 byte_swap)\n{\n\treturn ccp_copy_to_from_sb(cmd_q, wa, jobid, sb, byte_swap, true);\n}\n\nstatic noinline_for_stack int\nccp_run_aes_cmac_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_aes_engine *aes = &cmd->u.aes;\n\tstruct ccp_dm_workarea key, ctx;\n\tstruct ccp_data src;\n\tstruct ccp_op op;\n\tunsigned int dm_offset;\n\tint ret;\n\n\tif (!((aes->key_len == AES_KEYSIZE_128) ||\n\t      (aes->key_len == AES_KEYSIZE_192) ||\n\t      (aes->key_len == AES_KEYSIZE_256)))\n\t\treturn -EINVAL;\n\n\tif (aes->src_len & (AES_BLOCK_SIZE - 1))\n\t\treturn -EINVAL;\n\n\tif (aes->iv_len != AES_BLOCK_SIZE)\n\t\treturn -EINVAL;\n\n\tif (!aes->key || !aes->iv || !aes->src)\n\t\treturn -EINVAL;\n\n\tif (aes->cmac_final) {\n\t\tif (aes->cmac_key_len != AES_BLOCK_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\tif (!aes->cmac_key)\n\t\t\treturn -EINVAL;\n\t}\n\n\tBUILD_BUG_ON(CCP_AES_KEY_SB_COUNT != 1);\n\tBUILD_BUG_ON(CCP_AES_CTX_SB_COUNT != 1);\n\n\tret = -EIO;\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\top.sb_key = cmd_q->sb_key;\n\top.sb_ctx = cmd_q->sb_ctx;\n\top.init = 1;\n\top.u.aes.type = aes->type;\n\top.u.aes.mode = aes->mode;\n\top.u.aes.action = aes->action;\n\n\t/* All supported key sizes fit in a single (32-byte) SB entry\n\t * and must be in little endian format. Use the 256-bit byte\n\t * swap passthru option to convert from big endian to little\n\t * endian.\n\t */\n\tret = ccp_init_dm_workarea(&key, cmd_q,\n\t\t\t\t   CCP_AES_KEY_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret)\n\t\treturn ret;\n\n\tdm_offset = CCP_SB_BYTES - aes->key_len;\n\tret = ccp_set_dm_area(&key, dm_offset, aes->key, 0, aes->key_len);\n\tif (ret)\n\t\tgoto e_key;\n\tret = ccp_copy_to_sb(cmd_q, &key, op.jobid, op.sb_key,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_key;\n\t}\n\n\t/* The AES context fits in a single (32-byte) SB entry and\n\t * must be in little endian format. Use the 256-bit byte swap\n\t * passthru option to convert from big endian to little endian.\n\t */\n\tret = ccp_init_dm_workarea(&ctx, cmd_q,\n\t\t\t\t   CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto e_key;\n\n\tdm_offset = CCP_SB_BYTES - AES_BLOCK_SIZE;\n\tret = ccp_set_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\tif (ret)\n\t\tgoto e_ctx;\n\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_ctx;\n\t}\n\n\t/* Send data to the CCP AES engine */\n\tret = ccp_init_data(&src, cmd_q, aes->src, aes->src_len,\n\t\t\t    AES_BLOCK_SIZE, DMA_TO_DEVICE);\n\tif (ret)\n\t\tgoto e_ctx;\n\n\twhile (src.sg_wa.bytes_left) {\n\t\tccp_prepare_data(&src, NULL, &op, AES_BLOCK_SIZE, true);\n\t\tif (aes->cmac_final && !src.sg_wa.bytes_left) {\n\t\t\top.eom = 1;\n\n\t\t\t/* Push the K1/K2 key to the CCP now */\n\t\t\tret = ccp_copy_from_sb(cmd_q, &ctx, op.jobid,\n\t\t\t\t\t       op.sb_ctx,\n\t\t\t\t\t       CCP_PASSTHRU_BYTESWAP_256BIT);\n\t\t\tif (ret) {\n\t\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\t\tgoto e_src;\n\t\t\t}\n\n\t\t\tret = ccp_set_dm_area(&ctx, 0, aes->cmac_key, 0,\n\t\t\t\t\t      aes->cmac_key_len);\n\t\t\tif (ret)\n\t\t\t\tgoto e_src;\n\t\t\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\t\t\tif (ret) {\n\t\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\t\tgoto e_src;\n\t\t\t}\n\t\t}\n\n\t\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_src;\n\t\t}\n\n\t\tccp_process_data(&src, NULL, &op);\n\t}\n\n\t/* Retrieve the AES context - convert from LE to BE using\n\t * 32-byte (256-bit) byteswapping\n\t */\n\tret = ccp_copy_from_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t       CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_src;\n\t}\n\n\t/* ...but we only need AES_BLOCK_SIZE bytes */\n\tdm_offset = CCP_SB_BYTES - AES_BLOCK_SIZE;\n\tccp_get_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\ne_src:\n\tccp_free_data(&src, cmd_q);\n\ne_ctx:\n\tccp_dm_free(&ctx);\n\ne_key:\n\tccp_dm_free(&key);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int\nccp_run_aes_gcm_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_aes_engine *aes = &cmd->u.aes;\n\tstruct ccp_dm_workarea key, ctx, final_wa, tag;\n\tstruct ccp_data src, dst;\n\tstruct ccp_data aad;\n\tstruct ccp_op op;\n\tunsigned int dm_offset;\n\tunsigned int authsize;\n\tunsigned int jobid;\n\tunsigned int ilen;\n\tbool in_place = true; /* Default value */\n\t__be64 *final;\n\tint ret;\n\n\tstruct scatterlist *p_inp, sg_inp[2];\n\tstruct scatterlist *p_tag, sg_tag[2];\n\tstruct scatterlist *p_outp, sg_outp[2];\n\tstruct scatterlist *p_aad;\n\n\tif (!aes->iv)\n\t\treturn -EINVAL;\n\n\tif (!((aes->key_len == AES_KEYSIZE_128) ||\n\t\t(aes->key_len == AES_KEYSIZE_192) ||\n\t\t(aes->key_len == AES_KEYSIZE_256)))\n\t\treturn -EINVAL;\n\n\tif (!aes->key) /* Gotta have a key SGL */\n\t\treturn -EINVAL;\n\n\t/* Zero defaults to 16 bytes, the maximum size */\n\tauthsize = aes->authsize ? aes->authsize : AES_BLOCK_SIZE;\n\tswitch (authsize) {\n\tcase 16:\n\tcase 15:\n\tcase 14:\n\tcase 13:\n\tcase 12:\n\tcase 8:\n\tcase 4:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t/* First, decompose the source buffer into AAD & PT,\n\t * and the destination buffer into AAD, CT & tag, or\n\t * the input into CT & tag.\n\t * It is expected that the input and output SGs will\n\t * be valid, even if the AAD and input lengths are 0.\n\t */\n\tp_aad = aes->src;\n\tp_inp = scatterwalk_ffwd(sg_inp, aes->src, aes->aad_len);\n\tp_outp = scatterwalk_ffwd(sg_outp, aes->dst, aes->aad_len);\n\tif (aes->action == CCP_AES_ACTION_ENCRYPT) {\n\t\tilen = aes->src_len;\n\t\tp_tag = scatterwalk_ffwd(sg_tag, p_outp, ilen);\n\t} else {\n\t\t/* Input length for decryption includes tag */\n\t\tilen = aes->src_len - authsize;\n\t\tp_tag = scatterwalk_ffwd(sg_tag, p_inp, ilen);\n\t}\n\n\tjobid = CCP_NEW_JOBID(cmd_q->ccp);\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = jobid;\n\top.sb_key = cmd_q->sb_key; /* Pre-allocated */\n\top.sb_ctx = cmd_q->sb_ctx; /* Pre-allocated */\n\top.init = 1;\n\top.u.aes.type = aes->type;\n\n\t/* Copy the key to the LSB */\n\tret = ccp_init_dm_workarea(&key, cmd_q,\n\t\t\t\t   CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret)\n\t\treturn ret;\n\n\tdm_offset = CCP_SB_BYTES - aes->key_len;\n\tret = ccp_set_dm_area(&key, dm_offset, aes->key, 0, aes->key_len);\n\tif (ret)\n\t\tgoto e_key;\n\tret = ccp_copy_to_sb(cmd_q, &key, op.jobid, op.sb_key,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_key;\n\t}\n\n\t/* Copy the context (IV) to the LSB.\n\t * There is an assumption here that the IV is 96 bits in length, plus\n\t * a nonce of 32 bits. If no IV is present, use a zeroed buffer.\n\t */\n\tret = ccp_init_dm_workarea(&ctx, cmd_q,\n\t\t\t\t   CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto e_key;\n\n\tdm_offset = CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES - aes->iv_len;\n\tret = ccp_set_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\tif (ret)\n\t\tgoto e_ctx;\n\n\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_ctx;\n\t}\n\n\top.init = 1;\n\tif (aes->aad_len > 0) {\n\t\t/* Step 1: Run a GHASH over the Additional Authenticated Data */\n\t\tret = ccp_init_data(&aad, cmd_q, p_aad, aes->aad_len,\n\t\t\t\t    AES_BLOCK_SIZE,\n\t\t\t\t    DMA_TO_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_ctx;\n\n\t\top.u.aes.mode = CCP_AES_MODE_GHASH;\n\t\top.u.aes.action = CCP_AES_GHASHAAD;\n\n\t\twhile (aad.sg_wa.bytes_left) {\n\t\t\tccp_prepare_data(&aad, NULL, &op, AES_BLOCK_SIZE, true);\n\n\t\t\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\t\t\tif (ret) {\n\t\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\t\tgoto e_aad;\n\t\t\t}\n\n\t\t\tccp_process_data(&aad, NULL, &op);\n\t\t\top.init = 0;\n\t\t}\n\t}\n\n\top.u.aes.mode = CCP_AES_MODE_GCTR;\n\top.u.aes.action = aes->action;\n\n\tif (ilen > 0) {\n\t\t/* Step 2: Run a GCTR over the plaintext */\n\t\tin_place = (sg_virt(p_inp) == sg_virt(p_outp)) ? true : false;\n\n\t\tret = ccp_init_data(&src, cmd_q, p_inp, ilen,\n\t\t\t\t    AES_BLOCK_SIZE,\n\t\t\t\t    in_place ? DMA_BIDIRECTIONAL\n\t\t\t\t\t     : DMA_TO_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_aad;\n\n\t\tif (in_place) {\n\t\t\tdst = src;\n\t\t} else {\n\t\t\tret = ccp_init_data(&dst, cmd_q, p_outp, ilen,\n\t\t\t\t\t    AES_BLOCK_SIZE, DMA_FROM_DEVICE);\n\t\t\tif (ret)\n\t\t\t\tgoto e_src;\n\t\t}\n\n\t\top.soc = 0;\n\t\top.eom = 0;\n\t\top.init = 1;\n\t\twhile (src.sg_wa.bytes_left) {\n\t\t\tccp_prepare_data(&src, &dst, &op, AES_BLOCK_SIZE, true);\n\t\t\tif (!src.sg_wa.bytes_left) {\n\t\t\t\tunsigned int nbytes = ilen % AES_BLOCK_SIZE;\n\n\t\t\t\tif (nbytes) {\n\t\t\t\t\top.eom = 1;\n\t\t\t\t\top.u.aes.size = (nbytes * 8) - 1;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\t\t\tif (ret) {\n\t\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\t\tgoto e_dst;\n\t\t\t}\n\n\t\t\tccp_process_data(&src, &dst, &op);\n\t\t\top.init = 0;\n\t\t}\n\t}\n\n\t/* Step 3: Update the IV portion of the context with the original IV */\n\tret = ccp_copy_from_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t       CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\tret = ccp_set_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\tif (ret)\n\t\tgoto e_dst;\n\n\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\t/* Step 4: Concatenate the lengths of the AAD and source, and\n\t * hash that 16 byte buffer.\n\t */\n\tret = ccp_init_dm_workarea(&final_wa, cmd_q, AES_BLOCK_SIZE,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto e_dst;\n\tfinal = (__be64 *)final_wa.address;\n\tfinal[0] = cpu_to_be64(aes->aad_len * 8);\n\tfinal[1] = cpu_to_be64(ilen * 8);\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = jobid;\n\top.sb_key = cmd_q->sb_key; /* Pre-allocated */\n\top.sb_ctx = cmd_q->sb_ctx; /* Pre-allocated */\n\top.init = 1;\n\top.u.aes.type = aes->type;\n\top.u.aes.mode = CCP_AES_MODE_GHASH;\n\top.u.aes.action = CCP_AES_GHASHFINAL;\n\top.src.type = CCP_MEMTYPE_SYSTEM;\n\top.src.u.dma.address = final_wa.dma.address;\n\top.src.u.dma.length = AES_BLOCK_SIZE;\n\top.dst.type = CCP_MEMTYPE_SYSTEM;\n\top.dst.u.dma.address = final_wa.dma.address;\n\top.dst.u.dma.length = AES_BLOCK_SIZE;\n\top.eom = 1;\n\top.u.aes.size = 0;\n\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\tif (ret)\n\t\tgoto e_final_wa;\n\n\tif (aes->action == CCP_AES_ACTION_ENCRYPT) {\n\t\t/* Put the ciphered tag after the ciphertext. */\n\t\tccp_get_dm_area(&final_wa, 0, p_tag, 0, authsize);\n\t} else {\n\t\t/* Does this ciphered tag match the input? */\n\t\tret = ccp_init_dm_workarea(&tag, cmd_q, authsize,\n\t\t\t\t\t   DMA_BIDIRECTIONAL);\n\t\tif (ret)\n\t\t\tgoto e_final_wa;\n\t\tret = ccp_set_dm_area(&tag, 0, p_tag, 0, authsize);\n\t\tif (ret) {\n\t\t\tccp_dm_free(&tag);\n\t\t\tgoto e_final_wa;\n\t\t}\n\n\t\tret = crypto_memneq(tag.address, final_wa.address,\n\t\t\t\t    authsize) ? -EBADMSG : 0;\n\t\tccp_dm_free(&tag);\n\t}\n\ne_final_wa:\n\tccp_dm_free(&final_wa);\n\ne_dst:\n\tif (ilen > 0 && !in_place)\n\t\tccp_free_data(&dst, cmd_q);\n\ne_src:\n\tif (ilen > 0)\n\t\tccp_free_data(&src, cmd_q);\n\ne_aad:\n\tif (aes->aad_len)\n\t\tccp_free_data(&aad, cmd_q);\n\ne_ctx:\n\tccp_dm_free(&ctx);\n\ne_key:\n\tccp_dm_free(&key);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int\nccp_run_aes_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_aes_engine *aes = &cmd->u.aes;\n\tstruct ccp_dm_workarea key, ctx;\n\tstruct ccp_data src, dst;\n\tstruct ccp_op op;\n\tunsigned int dm_offset;\n\tbool in_place = false;\n\tint ret;\n\n\tif (!((aes->key_len == AES_KEYSIZE_128) ||\n\t      (aes->key_len == AES_KEYSIZE_192) ||\n\t      (aes->key_len == AES_KEYSIZE_256)))\n\t\treturn -EINVAL;\n\n\tif (((aes->mode == CCP_AES_MODE_ECB) ||\n\t     (aes->mode == CCP_AES_MODE_CBC)) &&\n\t    (aes->src_len & (AES_BLOCK_SIZE - 1)))\n\t\treturn -EINVAL;\n\n\tif (!aes->key || !aes->src || !aes->dst)\n\t\treturn -EINVAL;\n\n\tif (aes->mode != CCP_AES_MODE_ECB) {\n\t\tif (aes->iv_len != AES_BLOCK_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\tif (!aes->iv)\n\t\t\treturn -EINVAL;\n\t}\n\n\tBUILD_BUG_ON(CCP_AES_KEY_SB_COUNT != 1);\n\tBUILD_BUG_ON(CCP_AES_CTX_SB_COUNT != 1);\n\n\tret = -EIO;\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\top.sb_key = cmd_q->sb_key;\n\top.sb_ctx = cmd_q->sb_ctx;\n\top.init = (aes->mode == CCP_AES_MODE_ECB) ? 0 : 1;\n\top.u.aes.type = aes->type;\n\top.u.aes.mode = aes->mode;\n\top.u.aes.action = aes->action;\n\n\t/* All supported key sizes fit in a single (32-byte) SB entry\n\t * and must be in little endian format. Use the 256-bit byte\n\t * swap passthru option to convert from big endian to little\n\t * endian.\n\t */\n\tret = ccp_init_dm_workarea(&key, cmd_q,\n\t\t\t\t   CCP_AES_KEY_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret)\n\t\treturn ret;\n\n\tdm_offset = CCP_SB_BYTES - aes->key_len;\n\tret = ccp_set_dm_area(&key, dm_offset, aes->key, 0, aes->key_len);\n\tif (ret)\n\t\tgoto e_key;\n\tret = ccp_copy_to_sb(cmd_q, &key, op.jobid, op.sb_key,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_key;\n\t}\n\n\t/* The AES context fits in a single (32-byte) SB entry and\n\t * must be in little endian format. Use the 256-bit byte swap\n\t * passthru option to convert from big endian to little endian.\n\t */\n\tret = ccp_init_dm_workarea(&ctx, cmd_q,\n\t\t\t\t   CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto e_key;\n\n\tif (aes->mode != CCP_AES_MODE_ECB) {\n\t\t/* Load the AES context - convert to LE */\n\t\tdm_offset = CCP_SB_BYTES - AES_BLOCK_SIZE;\n\t\tret = ccp_set_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\t\tif (ret)\n\t\t\tgoto e_ctx;\n\t\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_ctx;\n\t\t}\n\t}\n\tswitch (aes->mode) {\n\tcase CCP_AES_MODE_CFB: /* CFB128 only */\n\tcase CCP_AES_MODE_CTR:\n\t\top.u.aes.size = AES_BLOCK_SIZE * BITS_PER_BYTE - 1;\n\t\tbreak;\n\tdefault:\n\t\top.u.aes.size = 0;\n\t}\n\n\t/* Prepare the input and output data workareas. For in-place\n\t * operations we need to set the dma direction to BIDIRECTIONAL\n\t * and copy the src workarea to the dst workarea.\n\t */\n\tif (sg_virt(aes->src) == sg_virt(aes->dst))\n\t\tin_place = true;\n\n\tret = ccp_init_data(&src, cmd_q, aes->src, aes->src_len,\n\t\t\t    AES_BLOCK_SIZE,\n\t\t\t    in_place ? DMA_BIDIRECTIONAL : DMA_TO_DEVICE);\n\tif (ret)\n\t\tgoto e_ctx;\n\n\tif (in_place) {\n\t\tdst = src;\n\t} else {\n\t\tret = ccp_init_data(&dst, cmd_q, aes->dst, aes->src_len,\n\t\t\t\t    AES_BLOCK_SIZE, DMA_FROM_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_src;\n\t}\n\n\t/* Send data to the CCP AES engine */\n\twhile (src.sg_wa.bytes_left) {\n\t\tccp_prepare_data(&src, &dst, &op, AES_BLOCK_SIZE, true);\n\t\tif (!src.sg_wa.bytes_left) {\n\t\t\top.eom = 1;\n\n\t\t\t/* Since we don't retrieve the AES context in ECB\n\t\t\t * mode we have to wait for the operation to complete\n\t\t\t * on the last piece of data\n\t\t\t */\n\t\t\tif (aes->mode == CCP_AES_MODE_ECB)\n\t\t\t\top.soc = 1;\n\t\t}\n\n\t\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_dst;\n\t\t}\n\n\t\tccp_process_data(&src, &dst, &op);\n\t}\n\n\tif (aes->mode != CCP_AES_MODE_ECB) {\n\t\t/* Retrieve the AES context - convert from LE to BE using\n\t\t * 32-byte (256-bit) byteswapping\n\t\t */\n\t\tret = ccp_copy_from_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t\t       CCP_PASSTHRU_BYTESWAP_256BIT);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_dst;\n\t\t}\n\n\t\t/* ...but we only need AES_BLOCK_SIZE bytes */\n\t\tdm_offset = CCP_SB_BYTES - AES_BLOCK_SIZE;\n\t\tccp_get_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\t}\n\ne_dst:\n\tif (!in_place)\n\t\tccp_free_data(&dst, cmd_q);\n\ne_src:\n\tccp_free_data(&src, cmd_q);\n\ne_ctx:\n\tccp_dm_free(&ctx);\n\ne_key:\n\tccp_dm_free(&key);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int\nccp_run_xts_aes_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_xts_aes_engine *xts = &cmd->u.xts;\n\tstruct ccp_dm_workarea key, ctx;\n\tstruct ccp_data src, dst;\n\tstruct ccp_op op;\n\tunsigned int unit_size, dm_offset;\n\tbool in_place = false;\n\tunsigned int sb_count;\n\tenum ccp_aes_type aestype;\n\tint ret;\n\n\tswitch (xts->unit_size) {\n\tcase CCP_XTS_AES_UNIT_SIZE_16:\n\t\tunit_size = 16;\n\t\tbreak;\n\tcase CCP_XTS_AES_UNIT_SIZE_512:\n\t\tunit_size = 512;\n\t\tbreak;\n\tcase CCP_XTS_AES_UNIT_SIZE_1024:\n\t\tunit_size = 1024;\n\t\tbreak;\n\tcase CCP_XTS_AES_UNIT_SIZE_2048:\n\t\tunit_size = 2048;\n\t\tbreak;\n\tcase CCP_XTS_AES_UNIT_SIZE_4096:\n\t\tunit_size = 4096;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (xts->key_len == AES_KEYSIZE_128)\n\t\taestype = CCP_AES_TYPE_128;\n\telse if (xts->key_len == AES_KEYSIZE_256)\n\t\taestype = CCP_AES_TYPE_256;\n\telse\n\t\treturn -EINVAL;\n\n\tif (!xts->final && (xts->src_len & (AES_BLOCK_SIZE - 1)))\n\t\treturn -EINVAL;\n\n\tif (xts->iv_len != AES_BLOCK_SIZE)\n\t\treturn -EINVAL;\n\n\tif (!xts->key || !xts->iv || !xts->src || !xts->dst)\n\t\treturn -EINVAL;\n\n\tBUILD_BUG_ON(CCP_XTS_AES_KEY_SB_COUNT != 1);\n\tBUILD_BUG_ON(CCP_XTS_AES_CTX_SB_COUNT != 1);\n\n\tret = -EIO;\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\top.sb_key = cmd_q->sb_key;\n\top.sb_ctx = cmd_q->sb_ctx;\n\top.init = 1;\n\top.u.xts.type = aestype;\n\top.u.xts.action = xts->action;\n\top.u.xts.unit_size = xts->unit_size;\n\n\t/* A version 3 device only supports 128-bit keys, which fits into a\n\t * single SB entry. A version 5 device uses a 512-bit vector, so two\n\t * SB entries.\n\t */\n\tif (cmd_q->ccp->vdata->version == CCP_VERSION(3, 0))\n\t\tsb_count = CCP_XTS_AES_KEY_SB_COUNT;\n\telse\n\t\tsb_count = CCP5_XTS_AES_KEY_SB_COUNT;\n\tret = ccp_init_dm_workarea(&key, cmd_q,\n\t\t\t\t   sb_count * CCP_SB_BYTES,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret)\n\t\treturn ret;\n\n\tif (cmd_q->ccp->vdata->version == CCP_VERSION(3, 0)) {\n\t\t/* All supported key sizes must be in little endian format.\n\t\t * Use the 256-bit byte swap passthru option to convert from\n\t\t * big endian to little endian.\n\t\t */\n\t\tdm_offset = CCP_SB_BYTES - AES_KEYSIZE_128;\n\t\tret = ccp_set_dm_area(&key, dm_offset, xts->key, 0, xts->key_len);\n\t\tif (ret)\n\t\t\tgoto e_key;\n\t\tret = ccp_set_dm_area(&key, 0, xts->key, xts->key_len, xts->key_len);\n\t\tif (ret)\n\t\t\tgoto e_key;\n\t} else {\n\t\t/* Version 5 CCPs use a 512-bit space for the key: each portion\n\t\t * occupies 256 bits, or one entire slot, and is zero-padded.\n\t\t */\n\t\tunsigned int pad;\n\n\t\tdm_offset = CCP_SB_BYTES;\n\t\tpad = dm_offset - xts->key_len;\n\t\tret = ccp_set_dm_area(&key, pad, xts->key, 0, xts->key_len);\n\t\tif (ret)\n\t\t\tgoto e_key;\n\t\tret = ccp_set_dm_area(&key, dm_offset + pad, xts->key,\n\t\t\t\t      xts->key_len, xts->key_len);\n\t\tif (ret)\n\t\t\tgoto e_key;\n\t}\n\tret = ccp_copy_to_sb(cmd_q, &key, op.jobid, op.sb_key,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_key;\n\t}\n\n\t/* The AES context fits in a single (32-byte) SB entry and\n\t * for XTS is already in little endian format so no byte swapping\n\t * is needed.\n\t */\n\tret = ccp_init_dm_workarea(&ctx, cmd_q,\n\t\t\t\t   CCP_XTS_AES_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto e_key;\n\n\tret = ccp_set_dm_area(&ctx, 0, xts->iv, 0, xts->iv_len);\n\tif (ret)\n\t\tgoto e_ctx;\n\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_NOOP);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_ctx;\n\t}\n\n\t/* Prepare the input and output data workareas. For in-place\n\t * operations we need to set the dma direction to BIDIRECTIONAL\n\t * and copy the src workarea to the dst workarea.\n\t */\n\tif (sg_virt(xts->src) == sg_virt(xts->dst))\n\t\tin_place = true;\n\n\tret = ccp_init_data(&src, cmd_q, xts->src, xts->src_len,\n\t\t\t    unit_size,\n\t\t\t    in_place ? DMA_BIDIRECTIONAL : DMA_TO_DEVICE);\n\tif (ret)\n\t\tgoto e_ctx;\n\n\tif (in_place) {\n\t\tdst = src;\n\t} else {\n\t\tret = ccp_init_data(&dst, cmd_q, xts->dst, xts->src_len,\n\t\t\t\t    unit_size, DMA_FROM_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_src;\n\t}\n\n\t/* Send data to the CCP AES engine */\n\twhile (src.sg_wa.bytes_left) {\n\t\tccp_prepare_data(&src, &dst, &op, unit_size, true);\n\t\tif (!src.sg_wa.bytes_left)\n\t\t\top.eom = 1;\n\n\t\tret = cmd_q->ccp->vdata->perform->xts_aes(&op);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_dst;\n\t\t}\n\n\t\tccp_process_data(&src, &dst, &op);\n\t}\n\n\t/* Retrieve the AES context - convert from LE to BE using\n\t * 32-byte (256-bit) byteswapping\n\t */\n\tret = ccp_copy_from_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t       CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\t/* ...but we only need AES_BLOCK_SIZE bytes */\n\tdm_offset = CCP_SB_BYTES - AES_BLOCK_SIZE;\n\tccp_get_dm_area(&ctx, dm_offset, xts->iv, 0, xts->iv_len);\n\ne_dst:\n\tif (!in_place)\n\t\tccp_free_data(&dst, cmd_q);\n\ne_src:\n\tccp_free_data(&src, cmd_q);\n\ne_ctx:\n\tccp_dm_free(&ctx);\n\ne_key:\n\tccp_dm_free(&key);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int\nccp_run_des3_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_des3_engine *des3 = &cmd->u.des3;\n\n\tstruct ccp_dm_workarea key, ctx;\n\tstruct ccp_data src, dst;\n\tstruct ccp_op op;\n\tunsigned int dm_offset;\n\tunsigned int len_singlekey;\n\tbool in_place = false;\n\tint ret;\n\n\t/* Error checks */\n\tif (cmd_q->ccp->vdata->version < CCP_VERSION(5, 0))\n\t\treturn -EINVAL;\n\n\tif (!cmd_q->ccp->vdata->perform->des3)\n\t\treturn -EINVAL;\n\n\tif (des3->key_len != DES3_EDE_KEY_SIZE)\n\t\treturn -EINVAL;\n\n\tif (((des3->mode == CCP_DES3_MODE_ECB) ||\n\t\t(des3->mode == CCP_DES3_MODE_CBC)) &&\n\t\t(des3->src_len & (DES3_EDE_BLOCK_SIZE - 1)))\n\t\treturn -EINVAL;\n\n\tif (!des3->key || !des3->src || !des3->dst)\n\t\treturn -EINVAL;\n\n\tif (des3->mode != CCP_DES3_MODE_ECB) {\n\t\tif (des3->iv_len != DES3_EDE_BLOCK_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\tif (!des3->iv)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* Zero out all the fields of the command desc */\n\tmemset(&op, 0, sizeof(op));\n\n\t/* Set up the Function field */\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\top.sb_key = cmd_q->sb_key;\n\n\top.init = (des3->mode == CCP_DES3_MODE_ECB) ? 0 : 1;\n\top.u.des3.type = des3->type;\n\top.u.des3.mode = des3->mode;\n\top.u.des3.action = des3->action;\n\n\t/*\n\t * All supported key sizes fit in a single (32-byte) KSB entry and\n\t * (like AES) must be in little endian format. Use the 256-bit byte\n\t * swap passthru option to convert from big endian to little endian.\n\t */\n\tret = ccp_init_dm_workarea(&key, cmd_q,\n\t\t\t\t   CCP_DES3_KEY_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * The contents of the key triplet are in the reverse order of what\n\t * is required by the engine. Copy the 3 pieces individually to put\n\t * them where they belong.\n\t */\n\tdm_offset = CCP_SB_BYTES - des3->key_len; /* Basic offset */\n\n\tlen_singlekey = des3->key_len / 3;\n\tret = ccp_set_dm_area(&key, dm_offset + 2 * len_singlekey,\n\t\t\t      des3->key, 0, len_singlekey);\n\tif (ret)\n\t\tgoto e_key;\n\tret = ccp_set_dm_area(&key, dm_offset + len_singlekey,\n\t\t\t      des3->key, len_singlekey, len_singlekey);\n\tif (ret)\n\t\tgoto e_key;\n\tret = ccp_set_dm_area(&key, dm_offset,\n\t\t\t      des3->key, 2 * len_singlekey, len_singlekey);\n\tif (ret)\n\t\tgoto e_key;\n\n\t/* Copy the key to the SB */\n\tret = ccp_copy_to_sb(cmd_q, &key, op.jobid, op.sb_key,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_key;\n\t}\n\n\t/*\n\t * The DES3 context fits in a single (32-byte) KSB entry and\n\t * must be in little endian format. Use the 256-bit byte swap\n\t * passthru option to convert from big endian to little endian.\n\t */\n\tif (des3->mode != CCP_DES3_MODE_ECB) {\n\t\top.sb_ctx = cmd_q->sb_ctx;\n\n\t\tret = ccp_init_dm_workarea(&ctx, cmd_q,\n\t\t\t\t\t   CCP_DES3_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t\t   DMA_BIDIRECTIONAL);\n\t\tif (ret)\n\t\t\tgoto e_key;\n\n\t\t/* Load the context into the LSB */\n\t\tdm_offset = CCP_SB_BYTES - des3->iv_len;\n\t\tret = ccp_set_dm_area(&ctx, dm_offset, des3->iv, 0,\n\t\t\t\t      des3->iv_len);\n\t\tif (ret)\n\t\t\tgoto e_ctx;\n\n\t\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_ctx;\n\t\t}\n\t}\n\n\t/*\n\t * Prepare the input and output data workareas. For in-place\n\t * operations we need to set the dma direction to BIDIRECTIONAL\n\t * and copy the src workarea to the dst workarea.\n\t */\n\tif (sg_virt(des3->src) == sg_virt(des3->dst))\n\t\tin_place = true;\n\n\tret = ccp_init_data(&src, cmd_q, des3->src, des3->src_len,\n\t\t\tDES3_EDE_BLOCK_SIZE,\n\t\t\tin_place ? DMA_BIDIRECTIONAL : DMA_TO_DEVICE);\n\tif (ret)\n\t\tgoto e_ctx;\n\n\tif (in_place)\n\t\tdst = src;\n\telse {\n\t\tret = ccp_init_data(&dst, cmd_q, des3->dst, des3->src_len,\n\t\t\t\tDES3_EDE_BLOCK_SIZE, DMA_FROM_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_src;\n\t}\n\n\t/* Send data to the CCP DES3 engine */\n\twhile (src.sg_wa.bytes_left) {\n\t\tccp_prepare_data(&src, &dst, &op, DES3_EDE_BLOCK_SIZE, true);\n\t\tif (!src.sg_wa.bytes_left) {\n\t\t\top.eom = 1;\n\n\t\t\t/* Since we don't retrieve the context in ECB mode\n\t\t\t * we have to wait for the operation to complete\n\t\t\t * on the last piece of data\n\t\t\t */\n\t\t\top.soc = 0;\n\t\t}\n\n\t\tret = cmd_q->ccp->vdata->perform->des3(&op);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_dst;\n\t\t}\n\n\t\tccp_process_data(&src, &dst, &op);\n\t}\n\n\tif (des3->mode != CCP_DES3_MODE_ECB) {\n\t\t/* Retrieve the context and make BE */\n\t\tret = ccp_copy_from_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t\t       CCP_PASSTHRU_BYTESWAP_256BIT);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_dst;\n\t\t}\n\n\t\t/* ...but we only need the last DES3_EDE_BLOCK_SIZE bytes */\n\t\tccp_get_dm_area(&ctx, dm_offset, des3->iv, 0,\n\t\t\t\tDES3_EDE_BLOCK_SIZE);\n\t}\ne_dst:\n\tif (!in_place)\n\t\tccp_free_data(&dst, cmd_q);\n\ne_src:\n\tccp_free_data(&src, cmd_q);\n\ne_ctx:\n\tif (des3->mode != CCP_DES3_MODE_ECB)\n\t\tccp_dm_free(&ctx);\n\ne_key:\n\tccp_dm_free(&key);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int\nccp_run_sha_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_sha_engine *sha = &cmd->u.sha;\n\tstruct ccp_dm_workarea ctx;\n\tstruct ccp_data src;\n\tstruct ccp_op op;\n\tunsigned int ioffset, ooffset;\n\tunsigned int digest_size;\n\tint sb_count;\n\tconst void *init;\n\tu64 block_size;\n\tint ctx_size;\n\tint ret;\n\n\tswitch (sha->type) {\n\tcase CCP_SHA_TYPE_1:\n\t\tif (sha->ctx_len < SHA1_DIGEST_SIZE)\n\t\t\treturn -EINVAL;\n\t\tblock_size = SHA1_BLOCK_SIZE;\n\t\tbreak;\n\tcase CCP_SHA_TYPE_224:\n\t\tif (sha->ctx_len < SHA224_DIGEST_SIZE)\n\t\t\treturn -EINVAL;\n\t\tblock_size = SHA224_BLOCK_SIZE;\n\t\tbreak;\n\tcase CCP_SHA_TYPE_256:\n\t\tif (sha->ctx_len < SHA256_DIGEST_SIZE)\n\t\t\treturn -EINVAL;\n\t\tblock_size = SHA256_BLOCK_SIZE;\n\t\tbreak;\n\tcase CCP_SHA_TYPE_384:\n\t\tif (cmd_q->ccp->vdata->version < CCP_VERSION(4, 0)\n\t\t    || sha->ctx_len < SHA384_DIGEST_SIZE)\n\t\t\treturn -EINVAL;\n\t\tblock_size = SHA384_BLOCK_SIZE;\n\t\tbreak;\n\tcase CCP_SHA_TYPE_512:\n\t\tif (cmd_q->ccp->vdata->version < CCP_VERSION(4, 0)\n\t\t    || sha->ctx_len < SHA512_DIGEST_SIZE)\n\t\t\treturn -EINVAL;\n\t\tblock_size = SHA512_BLOCK_SIZE;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (!sha->ctx)\n\t\treturn -EINVAL;\n\n\tif (!sha->final && (sha->src_len & (block_size - 1)))\n\t\treturn -EINVAL;\n\n\t/* The version 3 device can't handle zero-length input */\n\tif (cmd_q->ccp->vdata->version == CCP_VERSION(3, 0)) {\n\n\t\tif (!sha->src_len) {\n\t\t\tunsigned int digest_len;\n\t\t\tconst u8 *sha_zero;\n\n\t\t\t/* Not final, just return */\n\t\t\tif (!sha->final)\n\t\t\t\treturn 0;\n\n\t\t\t/* CCP can't do a zero length sha operation so the\n\t\t\t * caller must buffer the data.\n\t\t\t */\n\t\t\tif (sha->msg_bits)\n\t\t\t\treturn -EINVAL;\n\n\t\t\t/* The CCP cannot perform zero-length sha operations\n\t\t\t * so the caller is required to buffer data for the\n\t\t\t * final operation. However, a sha operation for a\n\t\t\t * message with a total length of zero is valid so\n\t\t\t * known values are required to supply the result.\n\t\t\t */\n\t\t\tswitch (sha->type) {\n\t\t\tcase CCP_SHA_TYPE_1:\n\t\t\t\tsha_zero = sha1_zero_message_hash;\n\t\t\t\tdigest_len = SHA1_DIGEST_SIZE;\n\t\t\t\tbreak;\n\t\t\tcase CCP_SHA_TYPE_224:\n\t\t\t\tsha_zero = sha224_zero_message_hash;\n\t\t\t\tdigest_len = SHA224_DIGEST_SIZE;\n\t\t\t\tbreak;\n\t\t\tcase CCP_SHA_TYPE_256:\n\t\t\t\tsha_zero = sha256_zero_message_hash;\n\t\t\t\tdigest_len = SHA256_DIGEST_SIZE;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tscatterwalk_map_and_copy((void *)sha_zero, sha->ctx, 0,\n\t\t\t\t\t\t digest_len, 1);\n\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/* Set variables used throughout */\n\tswitch (sha->type) {\n\tcase CCP_SHA_TYPE_1:\n\t\tdigest_size = SHA1_DIGEST_SIZE;\n\t\tinit = (void *) ccp_sha1_init;\n\t\tctx_size = SHA1_DIGEST_SIZE;\n\t\tsb_count = 1;\n\t\tif (cmd_q->ccp->vdata->version != CCP_VERSION(3, 0))\n\t\t\tooffset = ioffset = CCP_SB_BYTES - SHA1_DIGEST_SIZE;\n\t\telse\n\t\t\tooffset = ioffset = 0;\n\t\tbreak;\n\tcase CCP_SHA_TYPE_224:\n\t\tdigest_size = SHA224_DIGEST_SIZE;\n\t\tinit = (void *) ccp_sha224_init;\n\t\tctx_size = SHA256_DIGEST_SIZE;\n\t\tsb_count = 1;\n\t\tioffset = 0;\n\t\tif (cmd_q->ccp->vdata->version != CCP_VERSION(3, 0))\n\t\t\tooffset = CCP_SB_BYTES - SHA224_DIGEST_SIZE;\n\t\telse\n\t\t\tooffset = 0;\n\t\tbreak;\n\tcase CCP_SHA_TYPE_256:\n\t\tdigest_size = SHA256_DIGEST_SIZE;\n\t\tinit = (void *) ccp_sha256_init;\n\t\tctx_size = SHA256_DIGEST_SIZE;\n\t\tsb_count = 1;\n\t\tooffset = ioffset = 0;\n\t\tbreak;\n\tcase CCP_SHA_TYPE_384:\n\t\tdigest_size = SHA384_DIGEST_SIZE;\n\t\tinit = (void *) ccp_sha384_init;\n\t\tctx_size = SHA512_DIGEST_SIZE;\n\t\tsb_count = 2;\n\t\tioffset = 0;\n\t\tooffset = 2 * CCP_SB_BYTES - SHA384_DIGEST_SIZE;\n\t\tbreak;\n\tcase CCP_SHA_TYPE_512:\n\t\tdigest_size = SHA512_DIGEST_SIZE;\n\t\tinit = (void *) ccp_sha512_init;\n\t\tctx_size = SHA512_DIGEST_SIZE;\n\t\tsb_count = 2;\n\t\tooffset = ioffset = 0;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tgoto e_data;\n\t}\n\n\t/* For zero-length plaintext the src pointer is ignored;\n\t * otherwise both parts must be valid\n\t */\n\tif (sha->src_len && !sha->src)\n\t\treturn -EINVAL;\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\top.sb_ctx = cmd_q->sb_ctx; /* Pre-allocated */\n\top.u.sha.type = sha->type;\n\top.u.sha.msg_bits = sha->msg_bits;\n\n\t/* For SHA1/224/256 the context fits in a single (32-byte) SB entry;\n\t * SHA384/512 require 2 adjacent SB slots, with the right half in the\n\t * first slot, and the left half in the second. Each portion must then\n\t * be in little endian format: use the 256-bit byte swap option.\n\t */\n\tret = ccp_init_dm_workarea(&ctx, cmd_q, sb_count * CCP_SB_BYTES,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\treturn ret;\n\tif (sha->first) {\n\t\tswitch (sha->type) {\n\t\tcase CCP_SHA_TYPE_1:\n\t\tcase CCP_SHA_TYPE_224:\n\t\tcase CCP_SHA_TYPE_256:\n\t\t\tmemcpy(ctx.address + ioffset, init, ctx_size);\n\t\t\tbreak;\n\t\tcase CCP_SHA_TYPE_384:\n\t\tcase CCP_SHA_TYPE_512:\n\t\t\tmemcpy(ctx.address + ctx_size / 2, init,\n\t\t\t       ctx_size / 2);\n\t\t\tmemcpy(ctx.address, init + ctx_size / 2,\n\t\t\t       ctx_size / 2);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto e_ctx;\n\t\t}\n\t} else {\n\t\t/* Restore the context */\n\t\tret = ccp_set_dm_area(&ctx, 0, sha->ctx, 0,\n\t\t\t\t      sb_count * CCP_SB_BYTES);\n\t\tif (ret)\n\t\t\tgoto e_ctx;\n\t}\n\n\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_ctx;\n\t}\n\n\tif (sha->src) {\n\t\t/* Send data to the CCP SHA engine; block_size is set above */\n\t\tret = ccp_init_data(&src, cmd_q, sha->src, sha->src_len,\n\t\t\t\t    block_size, DMA_TO_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_ctx;\n\n\t\twhile (src.sg_wa.bytes_left) {\n\t\t\tccp_prepare_data(&src, NULL, &op, block_size, false);\n\t\t\tif (sha->final && !src.sg_wa.bytes_left)\n\t\t\t\top.eom = 1;\n\n\t\t\tret = cmd_q->ccp->vdata->perform->sha(&op);\n\t\t\tif (ret) {\n\t\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\t\tgoto e_data;\n\t\t\t}\n\n\t\t\tccp_process_data(&src, NULL, &op);\n\t\t}\n\t} else {\n\t\top.eom = 1;\n\t\tret = cmd_q->ccp->vdata->perform->sha(&op);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_data;\n\t\t}\n\t}\n\n\t/* Retrieve the SHA context - convert from LE to BE using\n\t * 32-byte (256-bit) byteswapping to BE\n\t */\n\tret = ccp_copy_from_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t       CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_data;\n\t}\n\n\tif (sha->final) {\n\t\t/* Finishing up, so get the digest */\n\t\tswitch (sha->type) {\n\t\tcase CCP_SHA_TYPE_1:\n\t\tcase CCP_SHA_TYPE_224:\n\t\tcase CCP_SHA_TYPE_256:\n\t\t\tccp_get_dm_area(&ctx, ooffset,\n\t\t\t\t\tsha->ctx, 0,\n\t\t\t\t\tdigest_size);\n\t\t\tbreak;\n\t\tcase CCP_SHA_TYPE_384:\n\t\tcase CCP_SHA_TYPE_512:\n\t\t\tccp_get_dm_area(&ctx, 0,\n\t\t\t\t\tsha->ctx, LSB_ITEM_SIZE - ooffset,\n\t\t\t\t\tLSB_ITEM_SIZE);\n\t\t\tccp_get_dm_area(&ctx, LSB_ITEM_SIZE + ooffset,\n\t\t\t\t\tsha->ctx, 0,\n\t\t\t\t\tLSB_ITEM_SIZE - ooffset);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto e_data;\n\t\t}\n\t} else {\n\t\t/* Stash the context */\n\t\tccp_get_dm_area(&ctx, 0, sha->ctx, 0,\n\t\t\t\tsb_count * CCP_SB_BYTES);\n\t}\n\n\tif (sha->final && sha->opad) {\n\t\t/* HMAC operation, recursively perform final SHA */\n\t\tstruct ccp_cmd hmac_cmd;\n\t\tstruct scatterlist sg;\n\t\tu8 *hmac_buf;\n\n\t\tif (sha->opad_len != block_size) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto e_data;\n\t\t}\n\n\t\thmac_buf = kmalloc(block_size + digest_size, GFP_KERNEL);\n\t\tif (!hmac_buf) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto e_data;\n\t\t}\n\t\tsg_init_one(&sg, hmac_buf, block_size + digest_size);\n\n\t\tscatterwalk_map_and_copy(hmac_buf, sha->opad, 0, block_size, 0);\n\t\tswitch (sha->type) {\n\t\tcase CCP_SHA_TYPE_1:\n\t\tcase CCP_SHA_TYPE_224:\n\t\tcase CCP_SHA_TYPE_256:\n\t\t\tmemcpy(hmac_buf + block_size,\n\t\t\t       ctx.address + ooffset,\n\t\t\t       digest_size);\n\t\t\tbreak;\n\t\tcase CCP_SHA_TYPE_384:\n\t\tcase CCP_SHA_TYPE_512:\n\t\t\tmemcpy(hmac_buf + block_size,\n\t\t\t       ctx.address + LSB_ITEM_SIZE + ooffset,\n\t\t\t       LSB_ITEM_SIZE);\n\t\t\tmemcpy(hmac_buf + block_size +\n\t\t\t       (LSB_ITEM_SIZE - ooffset),\n\t\t\t       ctx.address,\n\t\t\t       LSB_ITEM_SIZE);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tkfree(hmac_buf);\n\t\t\tret = -EINVAL;\n\t\t\tgoto e_data;\n\t\t}\n\n\t\tmemset(&hmac_cmd, 0, sizeof(hmac_cmd));\n\t\thmac_cmd.engine = CCP_ENGINE_SHA;\n\t\thmac_cmd.u.sha.type = sha->type;\n\t\thmac_cmd.u.sha.ctx = sha->ctx;\n\t\thmac_cmd.u.sha.ctx_len = sha->ctx_len;\n\t\thmac_cmd.u.sha.src = &sg;\n\t\thmac_cmd.u.sha.src_len = block_size + digest_size;\n\t\thmac_cmd.u.sha.opad = NULL;\n\t\thmac_cmd.u.sha.opad_len = 0;\n\t\thmac_cmd.u.sha.first = 1;\n\t\thmac_cmd.u.sha.final = 1;\n\t\thmac_cmd.u.sha.msg_bits = (block_size + digest_size) << 3;\n\n\t\tret = ccp_run_sha_cmd(cmd_q, &hmac_cmd);\n\t\tif (ret)\n\t\t\tcmd->engine_error = hmac_cmd.engine_error;\n\n\t\tkfree(hmac_buf);\n\t}\n\ne_data:\n\tif (sha->src)\n\t\tccp_free_data(&src, cmd_q);\n\ne_ctx:\n\tccp_dm_free(&ctx);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int\nccp_run_rsa_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_rsa_engine *rsa = &cmd->u.rsa;\n\tstruct ccp_dm_workarea exp, src, dst;\n\tstruct ccp_op op;\n\tunsigned int sb_count, i_len, o_len;\n\tint ret;\n\n\t/* Check against the maximum allowable size, in bits */\n\tif (rsa->key_size > cmd_q->ccp->vdata->rsamax)\n\t\treturn -EINVAL;\n\n\tif (!rsa->exp || !rsa->mod || !rsa->src || !rsa->dst)\n\t\treturn -EINVAL;\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\n\t/* The RSA modulus must precede the message being acted upon, so\n\t * it must be copied to a DMA area where the message and the\n\t * modulus can be concatenated.  Therefore the input buffer\n\t * length required is twice the output buffer length (which\n\t * must be a multiple of 256-bits).  Compute o_len, i_len in bytes.\n\t * Buffer sizes must be a multiple of 32 bytes; rounding up may be\n\t * required.\n\t */\n\to_len = 32 * ((rsa->key_size + 255) / 256);\n\ti_len = o_len * 2;\n\n\tsb_count = 0;\n\tif (cmd_q->ccp->vdata->version < CCP_VERSION(5, 0)) {\n\t\t/* sb_count is the number of storage block slots required\n\t\t * for the modulus.\n\t\t */\n\t\tsb_count = o_len / CCP_SB_BYTES;\n\t\top.sb_key = cmd_q->ccp->vdata->perform->sballoc(cmd_q,\n\t\t\t\t\t\t\t\tsb_count);\n\t\tif (!op.sb_key)\n\t\t\treturn -EIO;\n\t} else {\n\t\t/* A version 5 device allows a modulus size that will not fit\n\t\t * in the LSB, so the command will transfer it from memory.\n\t\t * Set the sb key to the default, even though it's not used.\n\t\t */\n\t\top.sb_key = cmd_q->sb_key;\n\t}\n\n\t/* The RSA exponent must be in little endian format. Reverse its\n\t * byte order.\n\t */\n\tret = ccp_init_dm_workarea(&exp, cmd_q, o_len, DMA_TO_DEVICE);\n\tif (ret)\n\t\tgoto e_sb;\n\n\tret = ccp_reverse_set_dm_area(&exp, 0, rsa->exp, 0, rsa->exp_len);\n\tif (ret)\n\t\tgoto e_exp;\n\n\tif (cmd_q->ccp->vdata->version < CCP_VERSION(5, 0)) {\n\t\t/* Copy the exponent to the local storage block, using\n\t\t * as many 32-byte blocks as were allocated above. It's\n\t\t * already little endian, so no further change is required.\n\t\t */\n\t\tret = ccp_copy_to_sb(cmd_q, &exp, op.jobid, op.sb_key,\n\t\t\t\t     CCP_PASSTHRU_BYTESWAP_NOOP);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_exp;\n\t\t}\n\t} else {\n\t\t/* The exponent can be retrieved from memory via DMA. */\n\t\top.exp.u.dma.address = exp.dma.address;\n\t\top.exp.u.dma.offset = 0;\n\t}\n\n\t/* Concatenate the modulus and the message. Both the modulus and\n\t * the operands must be in little endian format.  Since the input\n\t * is in big endian format it must be converted.\n\t */\n\tret = ccp_init_dm_workarea(&src, cmd_q, i_len, DMA_TO_DEVICE);\n\tif (ret)\n\t\tgoto e_exp;\n\n\tret = ccp_reverse_set_dm_area(&src, 0, rsa->mod, 0, rsa->mod_len);\n\tif (ret)\n\t\tgoto e_src;\n\tret = ccp_reverse_set_dm_area(&src, o_len, rsa->src, 0, rsa->src_len);\n\tif (ret)\n\t\tgoto e_src;\n\n\t/* Prepare the output area for the operation */\n\tret = ccp_init_dm_workarea(&dst, cmd_q, o_len, DMA_FROM_DEVICE);\n\tif (ret)\n\t\tgoto e_src;\n\n\top.soc = 1;\n\top.src.u.dma.address = src.dma.address;\n\top.src.u.dma.offset = 0;\n\top.src.u.dma.length = i_len;\n\top.dst.u.dma.address = dst.dma.address;\n\top.dst.u.dma.offset = 0;\n\top.dst.u.dma.length = o_len;\n\n\top.u.rsa.mod_size = rsa->key_size;\n\top.u.rsa.input_len = i_len;\n\n\tret = cmd_q->ccp->vdata->perform->rsa(&op);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\tccp_reverse_get_dm_area(&dst, 0, rsa->dst, 0, rsa->mod_len);\n\ne_dst:\n\tccp_dm_free(&dst);\n\ne_src:\n\tccp_dm_free(&src);\n\ne_exp:\n\tccp_dm_free(&exp);\n\ne_sb:\n\tif (sb_count)\n\t\tcmd_q->ccp->vdata->perform->sbfree(cmd_q, op.sb_key, sb_count);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int\nccp_run_passthru_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_passthru_engine *pt = &cmd->u.passthru;\n\tstruct ccp_dm_workarea mask;\n\tstruct ccp_data src, dst;\n\tstruct ccp_op op;\n\tbool in_place = false;\n\tunsigned int i;\n\tint ret = 0;\n\n\tif (!pt->final && (pt->src_len & (CCP_PASSTHRU_BLOCKSIZE - 1)))\n\t\treturn -EINVAL;\n\n\tif (!pt->src || !pt->dst)\n\t\treturn -EINVAL;\n\n\tif (pt->bit_mod != CCP_PASSTHRU_BITWISE_NOOP) {\n\t\tif (pt->mask_len != CCP_PASSTHRU_MASKSIZE)\n\t\t\treturn -EINVAL;\n\t\tif (!pt->mask)\n\t\t\treturn -EINVAL;\n\t}\n\n\tBUILD_BUG_ON(CCP_PASSTHRU_SB_COUNT != 1);\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\n\tif (pt->bit_mod != CCP_PASSTHRU_BITWISE_NOOP) {\n\t\t/* Load the mask */\n\t\top.sb_key = cmd_q->sb_key;\n\n\t\tret = ccp_init_dm_workarea(&mask, cmd_q,\n\t\t\t\t\t   CCP_PASSTHRU_SB_COUNT *\n\t\t\t\t\t   CCP_SB_BYTES,\n\t\t\t\t\t   DMA_TO_DEVICE);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = ccp_set_dm_area(&mask, 0, pt->mask, 0, pt->mask_len);\n\t\tif (ret)\n\t\t\tgoto e_mask;\n\t\tret = ccp_copy_to_sb(cmd_q, &mask, op.jobid, op.sb_key,\n\t\t\t\t     CCP_PASSTHRU_BYTESWAP_NOOP);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_mask;\n\t\t}\n\t}\n\n\t/* Prepare the input and output data workareas. For in-place\n\t * operations we need to set the dma direction to BIDIRECTIONAL\n\t * and copy the src workarea to the dst workarea.\n\t */\n\tif (sg_virt(pt->src) == sg_virt(pt->dst))\n\t\tin_place = true;\n\n\tret = ccp_init_data(&src, cmd_q, pt->src, pt->src_len,\n\t\t\t    CCP_PASSTHRU_MASKSIZE,\n\t\t\t    in_place ? DMA_BIDIRECTIONAL : DMA_TO_DEVICE);\n\tif (ret)\n\t\tgoto e_mask;\n\n\tif (in_place) {\n\t\tdst = src;\n\t} else {\n\t\tret = ccp_init_data(&dst, cmd_q, pt->dst, pt->src_len,\n\t\t\t\t    CCP_PASSTHRU_MASKSIZE, DMA_FROM_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_src;\n\t}\n\n\t/* Send data to the CCP Passthru engine\n\t *   Because the CCP engine works on a single source and destination\n\t *   dma address at a time, each entry in the source scatterlist\n\t *   (after the dma_map_sg call) must be less than or equal to the\n\t *   (remaining) length in the destination scatterlist entry and the\n\t *   length must be a multiple of CCP_PASSTHRU_BLOCKSIZE\n\t */\n\tdst.sg_wa.sg_used = 0;\n\tfor (i = 1; i <= src.sg_wa.dma_count; i++) {\n\t\tif (!dst.sg_wa.sg ||\n\t\t    (sg_dma_len(dst.sg_wa.sg) < sg_dma_len(src.sg_wa.sg))) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto e_dst;\n\t\t}\n\n\t\tif (i == src.sg_wa.dma_count) {\n\t\t\top.eom = 1;\n\t\t\top.soc = 1;\n\t\t}\n\n\t\top.src.type = CCP_MEMTYPE_SYSTEM;\n\t\top.src.u.dma.address = sg_dma_address(src.sg_wa.sg);\n\t\top.src.u.dma.offset = 0;\n\t\top.src.u.dma.length = sg_dma_len(src.sg_wa.sg);\n\n\t\top.dst.type = CCP_MEMTYPE_SYSTEM;\n\t\top.dst.u.dma.address = sg_dma_address(dst.sg_wa.sg);\n\t\top.dst.u.dma.offset = dst.sg_wa.sg_used;\n\t\top.dst.u.dma.length = op.src.u.dma.length;\n\n\t\tret = cmd_q->ccp->vdata->perform->passthru(&op);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\tgoto e_dst;\n\t\t}\n\n\t\tdst.sg_wa.sg_used += sg_dma_len(src.sg_wa.sg);\n\t\tif (dst.sg_wa.sg_used == sg_dma_len(dst.sg_wa.sg)) {\n\t\t\tdst.sg_wa.sg = sg_next(dst.sg_wa.sg);\n\t\t\tdst.sg_wa.sg_used = 0;\n\t\t}\n\t\tsrc.sg_wa.sg = sg_next(src.sg_wa.sg);\n\t}\n\ne_dst:\n\tif (!in_place)\n\t\tccp_free_data(&dst, cmd_q);\n\ne_src:\n\tccp_free_data(&src, cmd_q);\n\ne_mask:\n\tif (pt->bit_mod != CCP_PASSTHRU_BITWISE_NOOP)\n\t\tccp_dm_free(&mask);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int\nccp_run_passthru_nomap_cmd(struct ccp_cmd_queue *cmd_q,\n\t\t\t\t      struct ccp_cmd *cmd)\n{\n\tstruct ccp_passthru_nomap_engine *pt = &cmd->u.passthru_nomap;\n\tstruct ccp_dm_workarea mask;\n\tstruct ccp_op op;\n\tint ret;\n\n\tif (!pt->final && (pt->src_len & (CCP_PASSTHRU_BLOCKSIZE - 1)))\n\t\treturn -EINVAL;\n\n\tif (!pt->src_dma || !pt->dst_dma)\n\t\treturn -EINVAL;\n\n\tif (pt->bit_mod != CCP_PASSTHRU_BITWISE_NOOP) {\n\t\tif (pt->mask_len != CCP_PASSTHRU_MASKSIZE)\n\t\t\treturn -EINVAL;\n\t\tif (!pt->mask)\n\t\t\treturn -EINVAL;\n\t}\n\n\tBUILD_BUG_ON(CCP_PASSTHRU_SB_COUNT != 1);\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\n\tif (pt->bit_mod != CCP_PASSTHRU_BITWISE_NOOP) {\n\t\t/* Load the mask */\n\t\top.sb_key = cmd_q->sb_key;\n\n\t\tmask.length = pt->mask_len;\n\t\tmask.dma.address = pt->mask;\n\t\tmask.dma.length = pt->mask_len;\n\n\t\tret = ccp_copy_to_sb(cmd_q, &mask, op.jobid, op.sb_key,\n\t\t\t\t     CCP_PASSTHRU_BYTESWAP_NOOP);\n\t\tif (ret) {\n\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t/* Send data to the CCP Passthru engine */\n\top.eom = 1;\n\top.soc = 1;\n\n\top.src.type = CCP_MEMTYPE_SYSTEM;\n\top.src.u.dma.address = pt->src_dma;\n\top.src.u.dma.offset = 0;\n\top.src.u.dma.length = pt->src_len;\n\n\top.dst.type = CCP_MEMTYPE_SYSTEM;\n\top.dst.u.dma.address = pt->dst_dma;\n\top.dst.u.dma.offset = 0;\n\top.dst.u.dma.length = pt->src_len;\n\n\tret = cmd_q->ccp->vdata->perform->passthru(&op);\n\tif (ret)\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\n\treturn ret;\n}\n\nstatic int ccp_run_ecc_mm_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_ecc_engine *ecc = &cmd->u.ecc;\n\tstruct ccp_dm_workarea src, dst;\n\tstruct ccp_op op;\n\tint ret;\n\tu8 *save;\n\n\tif (!ecc->u.mm.operand_1 ||\n\t    (ecc->u.mm.operand_1_len > CCP_ECC_MODULUS_BYTES))\n\t\treturn -EINVAL;\n\n\tif (ecc->function != CCP_ECC_FUNCTION_MINV_384BIT)\n\t\tif (!ecc->u.mm.operand_2 ||\n\t\t    (ecc->u.mm.operand_2_len > CCP_ECC_MODULUS_BYTES))\n\t\t\treturn -EINVAL;\n\n\tif (!ecc->u.mm.result ||\n\t    (ecc->u.mm.result_len < CCP_ECC_MODULUS_BYTES))\n\t\treturn -EINVAL;\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\n\t/* Concatenate the modulus and the operands. Both the modulus and\n\t * the operands must be in little endian format.  Since the input\n\t * is in big endian format it must be converted and placed in a\n\t * fixed length buffer.\n\t */\n\tret = ccp_init_dm_workarea(&src, cmd_q, CCP_ECC_SRC_BUF_SIZE,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Save the workarea address since it is updated in order to perform\n\t * the concatenation\n\t */\n\tsave = src.address;\n\n\t/* Copy the ECC modulus */\n\tret = ccp_reverse_set_dm_area(&src, 0, ecc->mod, 0, ecc->mod_len);\n\tif (ret)\n\t\tgoto e_src;\n\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\n\t/* Copy the first operand */\n\tret = ccp_reverse_set_dm_area(&src, 0, ecc->u.mm.operand_1, 0,\n\t\t\t\t      ecc->u.mm.operand_1_len);\n\tif (ret)\n\t\tgoto e_src;\n\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\n\tif (ecc->function != CCP_ECC_FUNCTION_MINV_384BIT) {\n\t\t/* Copy the second operand */\n\t\tret = ccp_reverse_set_dm_area(&src, 0, ecc->u.mm.operand_2, 0,\n\t\t\t\t\t      ecc->u.mm.operand_2_len);\n\t\tif (ret)\n\t\t\tgoto e_src;\n\t\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\t}\n\n\t/* Restore the workarea address */\n\tsrc.address = save;\n\n\t/* Prepare the output area for the operation */\n\tret = ccp_init_dm_workarea(&dst, cmd_q, CCP_ECC_DST_BUF_SIZE,\n\t\t\t\t   DMA_FROM_DEVICE);\n\tif (ret)\n\t\tgoto e_src;\n\n\top.soc = 1;\n\top.src.u.dma.address = src.dma.address;\n\top.src.u.dma.offset = 0;\n\top.src.u.dma.length = src.length;\n\top.dst.u.dma.address = dst.dma.address;\n\top.dst.u.dma.offset = 0;\n\top.dst.u.dma.length = dst.length;\n\n\top.u.ecc.function = cmd->u.ecc.function;\n\n\tret = cmd_q->ccp->vdata->perform->ecc(&op);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\tecc->ecc_result = le16_to_cpup(\n\t\t(const __le16 *)(dst.address + CCP_ECC_RESULT_OFFSET));\n\tif (!(ecc->ecc_result & CCP_ECC_RESULT_SUCCESS)) {\n\t\tret = -EIO;\n\t\tgoto e_dst;\n\t}\n\n\t/* Save the ECC result */\n\tccp_reverse_get_dm_area(&dst, 0, ecc->u.mm.result, 0,\n\t\t\t\tCCP_ECC_MODULUS_BYTES);\n\ne_dst:\n\tccp_dm_free(&dst);\n\ne_src:\n\tccp_dm_free(&src);\n\n\treturn ret;\n}\n\nstatic int ccp_run_ecc_pm_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_ecc_engine *ecc = &cmd->u.ecc;\n\tstruct ccp_dm_workarea src, dst;\n\tstruct ccp_op op;\n\tint ret;\n\tu8 *save;\n\n\tif (!ecc->u.pm.point_1.x ||\n\t    (ecc->u.pm.point_1.x_len > CCP_ECC_MODULUS_BYTES) ||\n\t    !ecc->u.pm.point_1.y ||\n\t    (ecc->u.pm.point_1.y_len > CCP_ECC_MODULUS_BYTES))\n\t\treturn -EINVAL;\n\n\tif (ecc->function == CCP_ECC_FUNCTION_PADD_384BIT) {\n\t\tif (!ecc->u.pm.point_2.x ||\n\t\t    (ecc->u.pm.point_2.x_len > CCP_ECC_MODULUS_BYTES) ||\n\t\t    !ecc->u.pm.point_2.y ||\n\t\t    (ecc->u.pm.point_2.y_len > CCP_ECC_MODULUS_BYTES))\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (!ecc->u.pm.domain_a ||\n\t\t    (ecc->u.pm.domain_a_len > CCP_ECC_MODULUS_BYTES))\n\t\t\treturn -EINVAL;\n\n\t\tif (ecc->function == CCP_ECC_FUNCTION_PMUL_384BIT)\n\t\t\tif (!ecc->u.pm.scalar ||\n\t\t\t    (ecc->u.pm.scalar_len > CCP_ECC_MODULUS_BYTES))\n\t\t\t\treturn -EINVAL;\n\t}\n\n\tif (!ecc->u.pm.result.x ||\n\t    (ecc->u.pm.result.x_len < CCP_ECC_MODULUS_BYTES) ||\n\t    !ecc->u.pm.result.y ||\n\t    (ecc->u.pm.result.y_len < CCP_ECC_MODULUS_BYTES))\n\t\treturn -EINVAL;\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = CCP_NEW_JOBID(cmd_q->ccp);\n\n\t/* Concatenate the modulus and the operands. Both the modulus and\n\t * the operands must be in little endian format.  Since the input\n\t * is in big endian format it must be converted and placed in a\n\t * fixed length buffer.\n\t */\n\tret = ccp_init_dm_workarea(&src, cmd_q, CCP_ECC_SRC_BUF_SIZE,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Save the workarea address since it is updated in order to perform\n\t * the concatenation\n\t */\n\tsave = src.address;\n\n\t/* Copy the ECC modulus */\n\tret = ccp_reverse_set_dm_area(&src, 0, ecc->mod, 0, ecc->mod_len);\n\tif (ret)\n\t\tgoto e_src;\n\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\n\t/* Copy the first point X and Y coordinate */\n\tret = ccp_reverse_set_dm_area(&src, 0, ecc->u.pm.point_1.x, 0,\n\t\t\t\t      ecc->u.pm.point_1.x_len);\n\tif (ret)\n\t\tgoto e_src;\n\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\tret = ccp_reverse_set_dm_area(&src, 0, ecc->u.pm.point_1.y, 0,\n\t\t\t\t      ecc->u.pm.point_1.y_len);\n\tif (ret)\n\t\tgoto e_src;\n\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\n\t/* Set the first point Z coordinate to 1 */\n\t*src.address = 0x01;\n\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\n\tif (ecc->function == CCP_ECC_FUNCTION_PADD_384BIT) {\n\t\t/* Copy the second point X and Y coordinate */\n\t\tret = ccp_reverse_set_dm_area(&src, 0, ecc->u.pm.point_2.x, 0,\n\t\t\t\t\t      ecc->u.pm.point_2.x_len);\n\t\tif (ret)\n\t\t\tgoto e_src;\n\t\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\t\tret = ccp_reverse_set_dm_area(&src, 0, ecc->u.pm.point_2.y, 0,\n\t\t\t\t\t      ecc->u.pm.point_2.y_len);\n\t\tif (ret)\n\t\t\tgoto e_src;\n\t\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\n\t\t/* Set the second point Z coordinate to 1 */\n\t\t*src.address = 0x01;\n\t\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\t} else {\n\t\t/* Copy the Domain \"a\" parameter */\n\t\tret = ccp_reverse_set_dm_area(&src, 0, ecc->u.pm.domain_a, 0,\n\t\t\t\t\t      ecc->u.pm.domain_a_len);\n\t\tif (ret)\n\t\t\tgoto e_src;\n\t\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\n\t\tif (ecc->function == CCP_ECC_FUNCTION_PMUL_384BIT) {\n\t\t\t/* Copy the scalar value */\n\t\t\tret = ccp_reverse_set_dm_area(&src, 0,\n\t\t\t\t\t\t      ecc->u.pm.scalar, 0,\n\t\t\t\t\t\t      ecc->u.pm.scalar_len);\n\t\t\tif (ret)\n\t\t\t\tgoto e_src;\n\t\t\tsrc.address += CCP_ECC_OPERAND_SIZE;\n\t\t}\n\t}\n\n\t/* Restore the workarea address */\n\tsrc.address = save;\n\n\t/* Prepare the output area for the operation */\n\tret = ccp_init_dm_workarea(&dst, cmd_q, CCP_ECC_DST_BUF_SIZE,\n\t\t\t\t   DMA_FROM_DEVICE);\n\tif (ret)\n\t\tgoto e_src;\n\n\top.soc = 1;\n\top.src.u.dma.address = src.dma.address;\n\top.src.u.dma.offset = 0;\n\top.src.u.dma.length = src.length;\n\top.dst.u.dma.address = dst.dma.address;\n\top.dst.u.dma.offset = 0;\n\top.dst.u.dma.length = dst.length;\n\n\top.u.ecc.function = cmd->u.ecc.function;\n\n\tret = cmd_q->ccp->vdata->perform->ecc(&op);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\tecc->ecc_result = le16_to_cpup(\n\t\t(const __le16 *)(dst.address + CCP_ECC_RESULT_OFFSET));\n\tif (!(ecc->ecc_result & CCP_ECC_RESULT_SUCCESS)) {\n\t\tret = -EIO;\n\t\tgoto e_dst;\n\t}\n\n\t/* Save the workarea address since it is updated as we walk through\n\t * to copy the point math result\n\t */\n\tsave = dst.address;\n\n\t/* Save the ECC result X and Y coordinates */\n\tccp_reverse_get_dm_area(&dst, 0, ecc->u.pm.result.x, 0,\n\t\t\t\tCCP_ECC_MODULUS_BYTES);\n\tdst.address += CCP_ECC_OUTPUT_SIZE;\n\tccp_reverse_get_dm_area(&dst, 0, ecc->u.pm.result.y, 0,\n\t\t\t\tCCP_ECC_MODULUS_BYTES);\n\n\t/* Restore the workarea address */\n\tdst.address = save;\n\ne_dst:\n\tccp_dm_free(&dst);\n\ne_src:\n\tccp_dm_free(&src);\n\n\treturn ret;\n}\n\nstatic noinline_for_stack int\nccp_run_ecc_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_ecc_engine *ecc = &cmd->u.ecc;\n\n\tecc->ecc_result = 0;\n\n\tif (!ecc->mod ||\n\t    (ecc->mod_len > CCP_ECC_MODULUS_BYTES))\n\t\treturn -EINVAL;\n\n\tswitch (ecc->function) {\n\tcase CCP_ECC_FUNCTION_MMUL_384BIT:\n\tcase CCP_ECC_FUNCTION_MADD_384BIT:\n\tcase CCP_ECC_FUNCTION_MINV_384BIT:\n\t\treturn ccp_run_ecc_mm_cmd(cmd_q, cmd);\n\n\tcase CCP_ECC_FUNCTION_PADD_384BIT:\n\tcase CCP_ECC_FUNCTION_PMUL_384BIT:\n\tcase CCP_ECC_FUNCTION_PDBL_384BIT:\n\t\treturn ccp_run_ecc_pm_cmd(cmd_q, cmd);\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nint ccp_run_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tint ret;\n\n\tcmd->engine_error = 0;\n\tcmd_q->cmd_error = 0;\n\tcmd_q->int_rcvd = 0;\n\tcmd_q->free_slots = cmd_q->ccp->vdata->perform->get_free_slots(cmd_q);\n\n\tswitch (cmd->engine) {\n\tcase CCP_ENGINE_AES:\n\t\tswitch (cmd->u.aes.mode) {\n\t\tcase CCP_AES_MODE_CMAC:\n\t\t\tret = ccp_run_aes_cmac_cmd(cmd_q, cmd);\n\t\t\tbreak;\n\t\tcase CCP_AES_MODE_GCM:\n\t\t\tret = ccp_run_aes_gcm_cmd(cmd_q, cmd);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = ccp_run_aes_cmd(cmd_q, cmd);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase CCP_ENGINE_XTS_AES_128:\n\t\tret = ccp_run_xts_aes_cmd(cmd_q, cmd);\n\t\tbreak;\n\tcase CCP_ENGINE_DES3:\n\t\tret = ccp_run_des3_cmd(cmd_q, cmd);\n\t\tbreak;\n\tcase CCP_ENGINE_SHA:\n\t\tret = ccp_run_sha_cmd(cmd_q, cmd);\n\t\tbreak;\n\tcase CCP_ENGINE_RSA:\n\t\tret = ccp_run_rsa_cmd(cmd_q, cmd);\n\t\tbreak;\n\tcase CCP_ENGINE_PASSTHRU:\n\t\tif (cmd->flags & CCP_CMD_PASSTHRU_NO_DMA_MAP)\n\t\t\tret = ccp_run_passthru_nomap_cmd(cmd_q, cmd);\n\t\telse\n\t\t\tret = ccp_run_passthru_cmd(cmd_q, cmd);\n\t\tbreak;\n\tcase CCP_ENGINE_ECC:\n\t\tret = ccp_run_ecc_cmd(cmd_q, cmd);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n"], "filenames": ["drivers/crypto/ccp/ccp-ops.c"], "buggy_code_start_loc": [781], "buggy_code_end_loc": [887], "fixing_code_start_loc": [781], "fixing_code_end_loc": [889], "type": "CWE-401", "message": "A memory leak flaw was found in the Linux kernel in the ccp_run_aes_gcm_cmd() function in drivers/crypto/ccp/ccp-ops.c, which allows attackers to cause a denial of service (memory consumption). This vulnerability is similar with the older CVE-2019-18808.", "other": {"cve": {"id": "CVE-2021-3744", "sourceIdentifier": "secalert@redhat.com", "published": "2022-03-04T16:15:08.817", "lastModified": "2023-02-12T23:42:42.287", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "A memory leak flaw was found in the Linux kernel in the ccp_run_aes_gcm_cmd() function in drivers/crypto/ccp/ccp-ops.c, which allows attackers to cause a denial of service (memory consumption). This vulnerability is similar with the older CVE-2019-18808."}, {"lang": "es", "value": "Se ha encontrado un fallo de p\u00e9rdida de memoria en el kernel de Linux en la funci\u00f3n ccp_run_aes_gcm_cmd() en el archivo drivers/crypto/ccp/ccp-ops.c, que permite a atacantes causar una denegaci\u00f3n de servicio (consumo de memoria). Esta vulnerabilidad es similar a la anterior CVE-2019-18808"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "secalert@redhat.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-401"}]}, {"source": "nvd@nist.gov", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-401"}]}], "configurations": [{"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.15", "matchCriteriaId": "037A6DFB-B41D-4CC7-86C1-A201809B79C4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.15:-:*:*:*:*:*:*", "matchCriteriaId": "40D9C0D1-0F32-4A2B-9840-1072F5497540"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.15:rc1:*:*:*:*:*:*", "matchCriteriaId": "E46C74C6-B76B-4C94-A6A4-FD2FFF62D644"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.15:rc2:*:*:*:*:*:*", "matchCriteriaId": "60134C3A-06E4-48C1-B04F-2903732A4E56"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.15:rc3:*:*:*:*:*:*", "matchCriteriaId": "0460DA88-8FE1-46A2-9DDA-1F1ABA552E71"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:33:*:*:*:*:*:*:*", "matchCriteriaId": "E460AA51-FCDA-46B9-AE97-E6676AA5E194"}, {"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:34:*:*:*:*:*:*:*", "matchCriteriaId": "A930E247-0B43-43CB-98FF-6CE7B8189835"}, {"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:35:*:*:*:*:*:*:*", "matchCriteriaId": "80E516C0-98A4-4ADE-B69F-66A772E2BAAA"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:10.0:*:*:*:*:*:*:*", "matchCriteriaId": "07B237A9-69A3-4A9C-9DA0-4E06BD37AE73"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:redhat:build_of_quarkus:2.0:*:*:*:*:*:*:*", "matchCriteriaId": "8D2076F4-560A-4A96-A6E7-EA45037194DB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:redhat:codeready_linux_builder:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "93A089E2-D66E-455C-969A-3140D991BAF4"}, {"vulnerable": true, "criteria": "cpe:2.3:a:redhat:codeready_linux_builder_eus:8.6:*:*:*:*:*:*:*", "matchCriteriaId": "8BE16CC2-C6B4-4B73-98A1-F28475A92F49"}, {"vulnerable": true, "criteria": "cpe:2.3:a:redhat:codeready_linux_builder_for_power_little_endian:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "5F48D0CB-CB06-4456-B918-6549BC6C7892"}, {"vulnerable": true, "criteria": "cpe:2.3:a:redhat:codeready_linux_builder_for_power_little_endian_eus:8.6:*:*:*:*:*:*:*", "matchCriteriaId": "845B853C-8F99-4987-AA8E-76078CE6A977"}, {"vulnerable": true, "criteria": "cpe:2.3:a:redhat:developer_tools:1.0:*:*:*:*:*:*:*", "matchCriteriaId": "60937D60-6B78-400F-8D30-7FCF328659A1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_eus:8.6:*:*:*:*:*:*:*", "matchCriteriaId": "6C3741B8-851F-475D-B428-523F4F722350"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_ibm_z_systems_eus:8.6:*:*:*:*:*:*:*", "matchCriteriaId": "9EF5C4AC-CA69-41E3-AD93-7AC21931374A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_power_little_endian_eus:8.6:*:*:*:*:*:*:*", "matchCriteriaId": "729C515E-1DD3-466D-A50B-AFE058FFC94A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time:8:*:*:*:*:*:*:*", "matchCriteriaId": "CBF9BCF3-187F-410A-96CA-9C47D3ED6924"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time:8.6:*:*:*:*:*:*:*", "matchCriteriaId": "4023C74B-8CB5-4351-A645-DBFD8BDBFD32"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time_for_nfv:8:*:*:*:*:*:*:*", "matchCriteriaId": "E5CB3640-F55B-4127-875A-2F52D873D179"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time_for_nfv_tus:8.6:*:*:*:*:*:*:*", "matchCriteriaId": "6D5DE3C5-B090-4CE7-9AF2-DEB379D7D5FC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_eus:8.6:*:*:*:*:*:*:*", "matchCriteriaId": "0DFE17EF-9FAB-4C79-A778-22923413C015"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_for_power_little_endian_update_services_for_sap_solutions:8.6:*:*:*:*:*:*:*", "matchCriteriaId": "871A5C26-DB7B-4870-A5B2-5DD24C90B4A7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_tus:8.6:*:*:*:*:*:*:*", "matchCriteriaId": "1272DF03-7674-4BD4-8E64-94004B195448"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_update_services_for_sap_solutions:8.6:*:*:*:*:*:*:*", "matchCriteriaId": "7614E5D3-4643-4CAE-9578-9BB9D558211F"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:redhat:virtualization_host:4.0:*:*:*:*:*:*:*", "matchCriteriaId": "BB28F9AF-3D06-4532-B397-96D7E4792503"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:o:redhat:enterprise_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "F4CFF558-3C47-480D-A2F0-BABF26042943"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:oracle:communications_cloud_native_core_binding_support_function:22.1.3:*:*:*:*:*:*:*", "matchCriteriaId": "6EDB6772-7FDB-45FF-8D72-952902A7EE56"}, {"vulnerable": true, "criteria": "cpe:2.3:a:oracle:communications_cloud_native_core_network_exposure_function:22.1.1:*:*:*:*:*:*:*", "matchCriteriaId": "9955F62A-75D3-4347-9AD3-5947FC365838"}, {"vulnerable": true, "criteria": "cpe:2.3:a:oracle:communications_cloud_native_core_policy:22.2.0:*:*:*:*:*:*:*", "matchCriteriaId": "7A6D77C7-A2F4-4700-AB5A-3EC853496ECA"}]}]}], "references": [{"url": "http://www.openwall.com/lists/oss-security/2021/09/14/1", "source": "secalert@redhat.com", "tags": ["Exploit", "Mailing List", "Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=2000627", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/505d9dcb0f7ddf9d075e729523a33d38642ae680", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://kernel.googlesource.com/pub/scm/linux/kernel/git/herbert/crypto-2.6/+/505d9dcb0f7ddf9d075e729523a33d38642ae680%5E%21/#F0", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2022/03/msg00012.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce%40lists.fedoraproject.org/message/7BLLVKYAIETEORUPTFO3TR3C33ZPFXQM/", "source": "secalert@redhat.com"}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce%40lists.fedoraproject.org/message/LAT3RERO6QBKSPJBNNRWY3D4NCGTFOS7/", "source": "secalert@redhat.com"}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce%40lists.fedoraproject.org/message/SYKURLXBB2555ASWMPDNMBUPD6AG2JKQ/", "source": "secalert@redhat.com"}, {"url": "https://seclists.org/oss-sec/2021/q3/164", "source": "secalert@redhat.com", "tags": ["Exploit", "Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://www.debian.org/security/2022/dsa-5096", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://www.oracle.com/security-alerts/cpujul2022.html", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/505d9dcb0f7ddf9d075e729523a33d38642ae680"}}