{"buggy_code": ["/*\n * Broadcom NetXtreme-E RoCE driver.\n *\n * Copyright (c) 2016 - 2017, Broadcom. All rights reserved.  The term\n * Broadcom refers to Broadcom Limited and/or its subsidiaries.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * BSD license below:\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in\n *    the documentation and/or other materials provided with the\n *    distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS''\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS\n * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR\n * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\n * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN\n * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n * Description: IB Verbs interpreter\n */\n\n#include <linux/interrupt.h>\n#include <linux/types.h>\n#include <linux/pci.h>\n#include <linux/netdevice.h>\n#include <linux/if_ether.h>\n\n#include <rdma/ib_verbs.h>\n#include <rdma/ib_user_verbs.h>\n#include <rdma/ib_umem.h>\n#include <rdma/ib_addr.h>\n#include <rdma/ib_mad.h>\n#include <rdma/ib_cache.h>\n#include <rdma/uverbs_ioctl.h>\n\n#include \"bnxt_ulp.h\"\n\n#include \"roce_hsi.h\"\n#include \"qplib_res.h\"\n#include \"qplib_sp.h\"\n#include \"qplib_fp.h\"\n#include \"qplib_rcfw.h\"\n\n#include \"bnxt_re.h\"\n#include \"ib_verbs.h\"\n#include <rdma/bnxt_re-abi.h>\n\nstatic int __from_ib_access_flags(int iflags)\n{\n\tint qflags = 0;\n\n\tif (iflags & IB_ACCESS_LOCAL_WRITE)\n\t\tqflags |= BNXT_QPLIB_ACCESS_LOCAL_WRITE;\n\tif (iflags & IB_ACCESS_REMOTE_READ)\n\t\tqflags |= BNXT_QPLIB_ACCESS_REMOTE_READ;\n\tif (iflags & IB_ACCESS_REMOTE_WRITE)\n\t\tqflags |= BNXT_QPLIB_ACCESS_REMOTE_WRITE;\n\tif (iflags & IB_ACCESS_REMOTE_ATOMIC)\n\t\tqflags |= BNXT_QPLIB_ACCESS_REMOTE_ATOMIC;\n\tif (iflags & IB_ACCESS_MW_BIND)\n\t\tqflags |= BNXT_QPLIB_ACCESS_MW_BIND;\n\tif (iflags & IB_ZERO_BASED)\n\t\tqflags |= BNXT_QPLIB_ACCESS_ZERO_BASED;\n\tif (iflags & IB_ACCESS_ON_DEMAND)\n\t\tqflags |= BNXT_QPLIB_ACCESS_ON_DEMAND;\n\treturn qflags;\n};\n\nstatic enum ib_access_flags __to_ib_access_flags(int qflags)\n{\n\tenum ib_access_flags iflags = 0;\n\n\tif (qflags & BNXT_QPLIB_ACCESS_LOCAL_WRITE)\n\t\tiflags |= IB_ACCESS_LOCAL_WRITE;\n\tif (qflags & BNXT_QPLIB_ACCESS_REMOTE_WRITE)\n\t\tiflags |= IB_ACCESS_REMOTE_WRITE;\n\tif (qflags & BNXT_QPLIB_ACCESS_REMOTE_READ)\n\t\tiflags |= IB_ACCESS_REMOTE_READ;\n\tif (qflags & BNXT_QPLIB_ACCESS_REMOTE_ATOMIC)\n\t\tiflags |= IB_ACCESS_REMOTE_ATOMIC;\n\tif (qflags & BNXT_QPLIB_ACCESS_MW_BIND)\n\t\tiflags |= IB_ACCESS_MW_BIND;\n\tif (qflags & BNXT_QPLIB_ACCESS_ZERO_BASED)\n\t\tiflags |= IB_ZERO_BASED;\n\tif (qflags & BNXT_QPLIB_ACCESS_ON_DEMAND)\n\t\tiflags |= IB_ACCESS_ON_DEMAND;\n\treturn iflags;\n};\n\nstatic int bnxt_re_build_sgl(struct ib_sge *ib_sg_list,\n\t\t\t     struct bnxt_qplib_sge *sg_list, int num)\n{\n\tint i, total = 0;\n\n\tfor (i = 0; i < num; i++) {\n\t\tsg_list[i].addr = ib_sg_list[i].addr;\n\t\tsg_list[i].lkey = ib_sg_list[i].lkey;\n\t\tsg_list[i].size = ib_sg_list[i].length;\n\t\ttotal += sg_list[i].size;\n\t}\n\treturn total;\n}\n\n/* Device */\nint bnxt_re_query_device(struct ib_device *ibdev,\n\t\t\t struct ib_device_attr *ib_attr,\n\t\t\t struct ib_udata *udata)\n{\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(ibdev, ibdev);\n\tstruct bnxt_qplib_dev_attr *dev_attr = &rdev->dev_attr;\n\n\tmemset(ib_attr, 0, sizeof(*ib_attr));\n\tmemcpy(&ib_attr->fw_ver, dev_attr->fw_ver,\n\t       min(sizeof(dev_attr->fw_ver),\n\t\t   sizeof(ib_attr->fw_ver)));\n\tbnxt_qplib_get_guid(rdev->netdev->dev_addr,\n\t\t\t    (u8 *)&ib_attr->sys_image_guid);\n\tib_attr->max_mr_size = BNXT_RE_MAX_MR_SIZE;\n\tib_attr->page_size_cap = BNXT_RE_PAGE_SIZE_4K | BNXT_RE_PAGE_SIZE_2M;\n\n\tib_attr->vendor_id = rdev->en_dev->pdev->vendor;\n\tib_attr->vendor_part_id = rdev->en_dev->pdev->device;\n\tib_attr->hw_ver = rdev->en_dev->pdev->subsystem_device;\n\tib_attr->max_qp = dev_attr->max_qp;\n\tib_attr->max_qp_wr = dev_attr->max_qp_wqes;\n\tib_attr->device_cap_flags =\n\t\t\t\t    IB_DEVICE_CURR_QP_STATE_MOD\n\t\t\t\t    | IB_DEVICE_RC_RNR_NAK_GEN\n\t\t\t\t    | IB_DEVICE_SHUTDOWN_PORT\n\t\t\t\t    | IB_DEVICE_SYS_IMAGE_GUID\n\t\t\t\t    | IB_DEVICE_LOCAL_DMA_LKEY\n\t\t\t\t    | IB_DEVICE_RESIZE_MAX_WR\n\t\t\t\t    | IB_DEVICE_PORT_ACTIVE_EVENT\n\t\t\t\t    | IB_DEVICE_N_NOTIFY_CQ\n\t\t\t\t    | IB_DEVICE_MEM_WINDOW\n\t\t\t\t    | IB_DEVICE_MEM_WINDOW_TYPE_2B\n\t\t\t\t    | IB_DEVICE_MEM_MGT_EXTENSIONS;\n\tib_attr->max_send_sge = dev_attr->max_qp_sges;\n\tib_attr->max_recv_sge = dev_attr->max_qp_sges;\n\tib_attr->max_sge_rd = dev_attr->max_qp_sges;\n\tib_attr->max_cq = dev_attr->max_cq;\n\tib_attr->max_cqe = dev_attr->max_cq_wqes;\n\tib_attr->max_mr = dev_attr->max_mr;\n\tib_attr->max_pd = dev_attr->max_pd;\n\tib_attr->max_qp_rd_atom = dev_attr->max_qp_rd_atom;\n\tib_attr->max_qp_init_rd_atom = dev_attr->max_qp_init_rd_atom;\n\tib_attr->atomic_cap = IB_ATOMIC_NONE;\n\tib_attr->masked_atomic_cap = IB_ATOMIC_NONE;\n\n\tib_attr->max_ee_rd_atom = 0;\n\tib_attr->max_res_rd_atom = 0;\n\tib_attr->max_ee_init_rd_atom = 0;\n\tib_attr->max_ee = 0;\n\tib_attr->max_rdd = 0;\n\tib_attr->max_mw = dev_attr->max_mw;\n\tib_attr->max_raw_ipv6_qp = 0;\n\tib_attr->max_raw_ethy_qp = dev_attr->max_raw_ethy_qp;\n\tib_attr->max_mcast_grp = 0;\n\tib_attr->max_mcast_qp_attach = 0;\n\tib_attr->max_total_mcast_qp_attach = 0;\n\tib_attr->max_ah = dev_attr->max_ah;\n\n\tib_attr->max_fmr = 0;\n\tib_attr->max_map_per_fmr = 0;\n\n\tib_attr->max_srq = dev_attr->max_srq;\n\tib_attr->max_srq_wr = dev_attr->max_srq_wqes;\n\tib_attr->max_srq_sge = dev_attr->max_srq_sges;\n\n\tib_attr->max_fast_reg_page_list_len = MAX_PBL_LVL_1_PGS;\n\n\tib_attr->max_pkeys = 1;\n\tib_attr->local_ca_ack_delay = BNXT_RE_DEFAULT_ACK_DELAY;\n\treturn 0;\n}\n\nint bnxt_re_modify_device(struct ib_device *ibdev,\n\t\t\t  int device_modify_mask,\n\t\t\t  struct ib_device_modify *device_modify)\n{\n\tswitch (device_modify_mask) {\n\tcase IB_DEVICE_MODIFY_SYS_IMAGE_GUID:\n\t\t/* Modify the GUID requires the modification of the GID table */\n\t\t/* GUID should be made as READ-ONLY */\n\t\tbreak;\n\tcase IB_DEVICE_MODIFY_NODE_DESC:\n\t\t/* Node Desc should be made as READ-ONLY */\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\n/* Port */\nint bnxt_re_query_port(struct ib_device *ibdev, u8 port_num,\n\t\t       struct ib_port_attr *port_attr)\n{\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(ibdev, ibdev);\n\tstruct bnxt_qplib_dev_attr *dev_attr = &rdev->dev_attr;\n\n\tmemset(port_attr, 0, sizeof(*port_attr));\n\n\tif (netif_running(rdev->netdev) && netif_carrier_ok(rdev->netdev)) {\n\t\tport_attr->state = IB_PORT_ACTIVE;\n\t\tport_attr->phys_state = IB_PORT_PHYS_STATE_LINK_UP;\n\t} else {\n\t\tport_attr->state = IB_PORT_DOWN;\n\t\tport_attr->phys_state = IB_PORT_PHYS_STATE_DISABLED;\n\t}\n\tport_attr->max_mtu = IB_MTU_4096;\n\tport_attr->active_mtu = iboe_get_mtu(rdev->netdev->mtu);\n\tport_attr->gid_tbl_len = dev_attr->max_sgid;\n\tport_attr->port_cap_flags = IB_PORT_CM_SUP | IB_PORT_REINIT_SUP |\n\t\t\t\t    IB_PORT_DEVICE_MGMT_SUP |\n\t\t\t\t    IB_PORT_VENDOR_CLASS_SUP;\n\tport_attr->ip_gids = true;\n\n\tport_attr->max_msg_sz = (u32)BNXT_RE_MAX_MR_SIZE_LOW;\n\tport_attr->bad_pkey_cntr = 0;\n\tport_attr->qkey_viol_cntr = 0;\n\tport_attr->pkey_tbl_len = dev_attr->max_pkey;\n\tport_attr->lid = 0;\n\tport_attr->sm_lid = 0;\n\tport_attr->lmc = 0;\n\tport_attr->max_vl_num = 4;\n\tport_attr->sm_sl = 0;\n\tport_attr->subnet_timeout = 0;\n\tport_attr->init_type_reply = 0;\n\tport_attr->active_speed = rdev->active_speed;\n\tport_attr->active_width = rdev->active_width;\n\n\treturn 0;\n}\n\nint bnxt_re_get_port_immutable(struct ib_device *ibdev, u8 port_num,\n\t\t\t       struct ib_port_immutable *immutable)\n{\n\tstruct ib_port_attr port_attr;\n\n\tif (bnxt_re_query_port(ibdev, port_num, &port_attr))\n\t\treturn -EINVAL;\n\n\timmutable->pkey_tbl_len = port_attr.pkey_tbl_len;\n\timmutable->gid_tbl_len = port_attr.gid_tbl_len;\n\timmutable->core_cap_flags = RDMA_CORE_PORT_IBA_ROCE;\n\timmutable->core_cap_flags |= RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP;\n\timmutable->max_mad_size = IB_MGMT_MAD_SIZE;\n\treturn 0;\n}\n\nvoid bnxt_re_query_fw_str(struct ib_device *ibdev, char *str)\n{\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(ibdev, ibdev);\n\n\tsnprintf(str, IB_FW_VERSION_NAME_MAX, \"%d.%d.%d.%d\",\n\t\t rdev->dev_attr.fw_ver[0], rdev->dev_attr.fw_ver[1],\n\t\t rdev->dev_attr.fw_ver[2], rdev->dev_attr.fw_ver[3]);\n}\n\nint bnxt_re_query_pkey(struct ib_device *ibdev, u8 port_num,\n\t\t       u16 index, u16 *pkey)\n{\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(ibdev, ibdev);\n\n\t/* Ignore port_num */\n\n\tmemset(pkey, 0, sizeof(*pkey));\n\treturn bnxt_qplib_get_pkey(&rdev->qplib_res,\n\t\t\t\t   &rdev->qplib_res.pkey_tbl, index, pkey);\n}\n\nint bnxt_re_query_gid(struct ib_device *ibdev, u8 port_num,\n\t\t      int index, union ib_gid *gid)\n{\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(ibdev, ibdev);\n\tint rc = 0;\n\n\t/* Ignore port_num */\n\tmemset(gid, 0, sizeof(*gid));\n\trc = bnxt_qplib_get_sgid(&rdev->qplib_res,\n\t\t\t\t &rdev->qplib_res.sgid_tbl, index,\n\t\t\t\t (struct bnxt_qplib_gid *)gid);\n\treturn rc;\n}\n\nint bnxt_re_del_gid(const struct ib_gid_attr *attr, void **context)\n{\n\tint rc = 0;\n\tstruct bnxt_re_gid_ctx *ctx, **ctx_tbl;\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(attr->device, ibdev);\n\tstruct bnxt_qplib_sgid_tbl *sgid_tbl = &rdev->qplib_res.sgid_tbl;\n\tstruct bnxt_qplib_gid *gid_to_del;\n\tu16 vlan_id = 0xFFFF;\n\n\t/* Delete the entry from the hardware */\n\tctx = *context;\n\tif (!ctx)\n\t\treturn -EINVAL;\n\n\tif (sgid_tbl && sgid_tbl->active) {\n\t\tif (ctx->idx >= sgid_tbl->max)\n\t\t\treturn -EINVAL;\n\t\tgid_to_del = &sgid_tbl->tbl[ctx->idx].gid;\n\t\tvlan_id = sgid_tbl->tbl[ctx->idx].vlan_id;\n\t\t/* DEL_GID is called in WQ context(netdevice_event_work_handler)\n\t\t * or via the ib_unregister_device path. In the former case QP1\n\t\t * may not be destroyed yet, in which case just return as FW\n\t\t * needs that entry to be present and will fail it's deletion.\n\t\t * We could get invoked again after QP1 is destroyed OR get an\n\t\t * ADD_GID call with a different GID value for the same index\n\t\t * where we issue MODIFY_GID cmd to update the GID entry -- TBD\n\t\t */\n\t\tif (ctx->idx == 0 &&\n\t\t    rdma_link_local_addr((struct in6_addr *)gid_to_del) &&\n\t\t    ctx->refcnt == 1 && rdev->qp1_sqp) {\n\t\t\tdev_dbg(rdev_to_dev(rdev),\n\t\t\t\t\"Trying to delete GID0 while QP1 is alive\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tctx->refcnt--;\n\t\tif (!ctx->refcnt) {\n\t\t\trc = bnxt_qplib_del_sgid(sgid_tbl, gid_to_del,\n\t\t\t\t\t\t vlan_id,  true);\n\t\t\tif (rc) {\n\t\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\t\"Failed to remove GID: %#x\", rc);\n\t\t\t} else {\n\t\t\t\tctx_tbl = sgid_tbl->ctx;\n\t\t\t\tctx_tbl[ctx->idx] = NULL;\n\t\t\t\tkfree(ctx);\n\t\t\t}\n\t\t}\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\treturn rc;\n}\n\nint bnxt_re_add_gid(const struct ib_gid_attr *attr, void **context)\n{\n\tint rc;\n\tu32 tbl_idx = 0;\n\tu16 vlan_id = 0xFFFF;\n\tstruct bnxt_re_gid_ctx *ctx, **ctx_tbl;\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(attr->device, ibdev);\n\tstruct bnxt_qplib_sgid_tbl *sgid_tbl = &rdev->qplib_res.sgid_tbl;\n\n\trc = rdma_read_gid_l2_fields(attr, &vlan_id, NULL);\n\tif (rc)\n\t\treturn rc;\n\n\trc = bnxt_qplib_add_sgid(sgid_tbl, (struct bnxt_qplib_gid *)&attr->gid,\n\t\t\t\t rdev->qplib_res.netdev->dev_addr,\n\t\t\t\t vlan_id, true, &tbl_idx);\n\tif (rc == -EALREADY) {\n\t\tctx_tbl = sgid_tbl->ctx;\n\t\tctx_tbl[tbl_idx]->refcnt++;\n\t\t*context = ctx_tbl[tbl_idx];\n\t\treturn 0;\n\t}\n\n\tif (rc < 0) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to add GID: %#x\", rc);\n\t\treturn rc;\n\t}\n\n\tctx = kmalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\tctx_tbl = sgid_tbl->ctx;\n\tctx->idx = tbl_idx;\n\tctx->refcnt = 1;\n\tctx_tbl[tbl_idx] = ctx;\n\t*context = ctx;\n\n\treturn rc;\n}\n\nenum rdma_link_layer bnxt_re_get_link_layer(struct ib_device *ibdev,\n\t\t\t\t\t    u8 port_num)\n{\n\treturn IB_LINK_LAYER_ETHERNET;\n}\n\n#define\tBNXT_RE_FENCE_PBL_SIZE\tDIV_ROUND_UP(BNXT_RE_FENCE_BYTES, PAGE_SIZE)\n\nstatic void bnxt_re_create_fence_wqe(struct bnxt_re_pd *pd)\n{\n\tstruct bnxt_re_fence_data *fence = &pd->fence;\n\tstruct ib_mr *ib_mr = &fence->mr->ib_mr;\n\tstruct bnxt_qplib_swqe *wqe = &fence->bind_wqe;\n\n\tmemset(wqe, 0, sizeof(*wqe));\n\twqe->type = BNXT_QPLIB_SWQE_TYPE_BIND_MW;\n\twqe->wr_id = BNXT_QPLIB_FENCE_WRID;\n\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SIGNAL_COMP;\n\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_UC_FENCE;\n\twqe->bind.zero_based = false;\n\twqe->bind.parent_l_key = ib_mr->lkey;\n\twqe->bind.va = (u64)(unsigned long)fence->va;\n\twqe->bind.length = fence->size;\n\twqe->bind.access_cntl = __from_ib_access_flags(IB_ACCESS_REMOTE_READ);\n\twqe->bind.mw_type = SQ_BIND_MW_TYPE_TYPE1;\n\n\t/* Save the initial rkey in fence structure for now;\n\t * wqe->bind.r_key will be set at (re)bind time.\n\t */\n\tfence->bind_rkey = ib_inc_rkey(fence->mw->rkey);\n}\n\nstatic int bnxt_re_bind_fence_mw(struct bnxt_qplib_qp *qplib_qp)\n{\n\tstruct bnxt_re_qp *qp = container_of(qplib_qp, struct bnxt_re_qp,\n\t\t\t\t\t     qplib_qp);\n\tstruct ib_pd *ib_pd = qp->ib_qp.pd;\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tstruct bnxt_re_fence_data *fence = &pd->fence;\n\tstruct bnxt_qplib_swqe *fence_wqe = &fence->bind_wqe;\n\tstruct bnxt_qplib_swqe wqe;\n\tint rc;\n\n\tmemcpy(&wqe, fence_wqe, sizeof(wqe));\n\twqe.bind.r_key = fence->bind_rkey;\n\tfence->bind_rkey = ib_inc_rkey(fence->bind_rkey);\n\n\tdev_dbg(rdev_to_dev(qp->rdev),\n\t\t\"Posting bind fence-WQE: rkey: %#x QP: %d PD: %p\\n\",\n\t\twqe.bind.r_key, qp->qplib_qp.id, pd);\n\trc = bnxt_qplib_post_send(&qp->qplib_qp, &wqe);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(qp->rdev), \"Failed to bind fence-WQE\\n\");\n\t\treturn rc;\n\t}\n\tbnxt_qplib_post_send_db(&qp->qplib_qp);\n\n\treturn rc;\n}\n\nstatic void bnxt_re_destroy_fence_mr(struct bnxt_re_pd *pd)\n{\n\tstruct bnxt_re_fence_data *fence = &pd->fence;\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct device *dev = &rdev->en_dev->pdev->dev;\n\tstruct bnxt_re_mr *mr = fence->mr;\n\n\tif (fence->mw) {\n\t\tbnxt_re_dealloc_mw(fence->mw);\n\t\tfence->mw = NULL;\n\t}\n\tif (mr) {\n\t\tif (mr->ib_mr.rkey)\n\t\t\tbnxt_qplib_dereg_mrw(&rdev->qplib_res, &mr->qplib_mr,\n\t\t\t\t\t     true);\n\t\tif (mr->ib_mr.lkey)\n\t\t\tbnxt_qplib_free_mrw(&rdev->qplib_res, &mr->qplib_mr);\n\t\tkfree(mr);\n\t\tfence->mr = NULL;\n\t}\n\tif (fence->dma_addr) {\n\t\tdma_unmap_single(dev, fence->dma_addr, BNXT_RE_FENCE_BYTES,\n\t\t\t\t DMA_BIDIRECTIONAL);\n\t\tfence->dma_addr = 0;\n\t}\n}\n\nstatic int bnxt_re_create_fence_mr(struct bnxt_re_pd *pd)\n{\n\tint mr_access_flags = IB_ACCESS_LOCAL_WRITE | IB_ACCESS_MW_BIND;\n\tstruct bnxt_re_fence_data *fence = &pd->fence;\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct device *dev = &rdev->en_dev->pdev->dev;\n\tstruct bnxt_re_mr *mr = NULL;\n\tdma_addr_t dma_addr = 0;\n\tstruct ib_mw *mw;\n\tu64 pbl_tbl;\n\tint rc;\n\n\tdma_addr = dma_map_single(dev, fence->va, BNXT_RE_FENCE_BYTES,\n\t\t\t\t  DMA_BIDIRECTIONAL);\n\trc = dma_mapping_error(dev, dma_addr);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to dma-map fence-MR-mem\\n\");\n\t\trc = -EIO;\n\t\tfence->dma_addr = 0;\n\t\tgoto fail;\n\t}\n\tfence->dma_addr = dma_addr;\n\n\t/* Allocate a MR */\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr) {\n\t\trc = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tfence->mr = mr;\n\tmr->rdev = rdev;\n\tmr->qplib_mr.pd = &pd->qplib_pd;\n\tmr->qplib_mr.type = CMDQ_ALLOCATE_MRW_MRW_FLAGS_PMR;\n\tmr->qplib_mr.flags = __from_ib_access_flags(mr_access_flags);\n\trc = bnxt_qplib_alloc_mrw(&rdev->qplib_res, &mr->qplib_mr);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to alloc fence-HW-MR\\n\");\n\t\tgoto fail;\n\t}\n\n\t/* Register MR */\n\tmr->ib_mr.lkey = mr->qplib_mr.lkey;\n\tmr->qplib_mr.va = (u64)(unsigned long)fence->va;\n\tmr->qplib_mr.total_size = BNXT_RE_FENCE_BYTES;\n\tpbl_tbl = dma_addr;\n\trc = bnxt_qplib_reg_mr(&rdev->qplib_res, &mr->qplib_mr, &pbl_tbl,\n\t\t\t       BNXT_RE_FENCE_PBL_SIZE, false, PAGE_SIZE);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to register fence-MR\\n\");\n\t\tgoto fail;\n\t}\n\tmr->ib_mr.rkey = mr->qplib_mr.rkey;\n\n\t/* Create a fence MW only for kernel consumers */\n\tmw = bnxt_re_alloc_mw(&pd->ib_pd, IB_MW_TYPE_1, NULL);\n\tif (IS_ERR(mw)) {\n\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\"Failed to create fence-MW for PD: %p\\n\", pd);\n\t\trc = PTR_ERR(mw);\n\t\tgoto fail;\n\t}\n\tfence->mw = mw;\n\n\tbnxt_re_create_fence_wqe(pd);\n\treturn 0;\n\nfail:\n\tbnxt_re_destroy_fence_mr(pd);\n\treturn rc;\n}\n\n/* Protection Domains */\nvoid bnxt_re_dealloc_pd(struct ib_pd *ib_pd, struct ib_udata *udata)\n{\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\n\tbnxt_re_destroy_fence_mr(pd);\n\n\tif (pd->qplib_pd.id)\n\t\tbnxt_qplib_dealloc_pd(&rdev->qplib_res, &rdev->qplib_res.pd_tbl,\n\t\t\t\t      &pd->qplib_pd);\n}\n\nint bnxt_re_alloc_pd(struct ib_pd *ibpd, struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = ibpd->device;\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(ibdev, ibdev);\n\tstruct bnxt_re_ucontext *ucntx = rdma_udata_to_drv_context(\n\t\tudata, struct bnxt_re_ucontext, ib_uctx);\n\tstruct bnxt_re_pd *pd = container_of(ibpd, struct bnxt_re_pd, ib_pd);\n\tint rc;\n\n\tpd->rdev = rdev;\n\tif (bnxt_qplib_alloc_pd(&rdev->qplib_res.pd_tbl, &pd->qplib_pd)) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to allocate HW PD\");\n\t\trc = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\tif (udata) {\n\t\tstruct bnxt_re_pd_resp resp;\n\n\t\tif (!ucntx->dpi.dbr) {\n\t\t\t/* Allocate DPI in alloc_pd to avoid failing of\n\t\t\t * ibv_devinfo and family of application when DPIs\n\t\t\t * are depleted.\n\t\t\t */\n\t\t\tif (bnxt_qplib_alloc_dpi(&rdev->qplib_res.dpi_tbl,\n\t\t\t\t\t\t &ucntx->dpi, ucntx)) {\n\t\t\t\trc = -ENOMEM;\n\t\t\t\tgoto dbfail;\n\t\t\t}\n\t\t}\n\n\t\tresp.pdid = pd->qplib_pd.id;\n\t\t/* Still allow mapping this DBR to the new user PD. */\n\t\tresp.dpi = ucntx->dpi.dpi;\n\t\tresp.dbr = (u64)ucntx->dpi.umdbr;\n\n\t\trc = ib_copy_to_udata(udata, &resp, sizeof(resp));\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Failed to copy user response\\n\");\n\t\t\tgoto dbfail;\n\t\t}\n\t}\n\n\tif (!udata)\n\t\tif (bnxt_re_create_fence_mr(pd))\n\t\t\tdev_warn(rdev_to_dev(rdev),\n\t\t\t\t \"Failed to create Fence-MR\\n\");\n\treturn 0;\ndbfail:\n\tbnxt_qplib_dealloc_pd(&rdev->qplib_res, &rdev->qplib_res.pd_tbl,\n\t\t\t      &pd->qplib_pd);\nfail:\n\treturn rc;\n}\n\n/* Address Handles */\nvoid bnxt_re_destroy_ah(struct ib_ah *ib_ah, u32 flags)\n{\n\tstruct bnxt_re_ah *ah = container_of(ib_ah, struct bnxt_re_ah, ib_ah);\n\tstruct bnxt_re_dev *rdev = ah->rdev;\n\n\tbnxt_qplib_destroy_ah(&rdev->qplib_res, &ah->qplib_ah,\n\t\t\t      !(flags & RDMA_DESTROY_AH_SLEEPABLE));\n}\n\nstatic u8 bnxt_re_stack_to_dev_nw_type(enum rdma_network_type ntype)\n{\n\tu8 nw_type;\n\n\tswitch (ntype) {\n\tcase RDMA_NETWORK_IPV4:\n\t\tnw_type = CMDQ_CREATE_AH_TYPE_V2IPV4;\n\t\tbreak;\n\tcase RDMA_NETWORK_IPV6:\n\t\tnw_type = CMDQ_CREATE_AH_TYPE_V2IPV6;\n\t\tbreak;\n\tdefault:\n\t\tnw_type = CMDQ_CREATE_AH_TYPE_V1;\n\t\tbreak;\n\t}\n\treturn nw_type;\n}\n\nint bnxt_re_create_ah(struct ib_ah *ib_ah, struct rdma_ah_attr *ah_attr,\n\t\t      u32 flags, struct ib_udata *udata)\n{\n\tstruct ib_pd *ib_pd = ib_ah->pd;\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tconst struct ib_global_route *grh = rdma_ah_read_grh(ah_attr);\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tconst struct ib_gid_attr *sgid_attr;\n\tstruct bnxt_re_ah *ah = container_of(ib_ah, struct bnxt_re_ah, ib_ah);\n\tu8 nw_type;\n\tint rc;\n\n\tif (!(rdma_ah_get_ah_flags(ah_attr) & IB_AH_GRH)) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to alloc AH: GRH not set\");\n\t\treturn -EINVAL;\n\t}\n\n\tah->rdev = rdev;\n\tah->qplib_ah.pd = &pd->qplib_pd;\n\n\t/* Supply the configuration for the HW */\n\tmemcpy(ah->qplib_ah.dgid.data, grh->dgid.raw,\n\t       sizeof(union ib_gid));\n\t/*\n\t * If RoCE V2 is enabled, stack will have two entries for\n\t * each GID entry. Avoiding this duplicte entry in HW. Dividing\n\t * the GID index by 2 for RoCE V2\n\t */\n\tah->qplib_ah.sgid_index = grh->sgid_index / 2;\n\tah->qplib_ah.host_sgid_index = grh->sgid_index;\n\tah->qplib_ah.traffic_class = grh->traffic_class;\n\tah->qplib_ah.flow_label = grh->flow_label;\n\tah->qplib_ah.hop_limit = grh->hop_limit;\n\tah->qplib_ah.sl = rdma_ah_get_sl(ah_attr);\n\n\tsgid_attr = grh->sgid_attr;\n\t/* Get network header type for this GID */\n\tnw_type = rdma_gid_attr_network_type(sgid_attr);\n\tah->qplib_ah.nw_type = bnxt_re_stack_to_dev_nw_type(nw_type);\n\n\tmemcpy(ah->qplib_ah.dmac, ah_attr->roce.dmac, ETH_ALEN);\n\trc = bnxt_qplib_create_ah(&rdev->qplib_res, &ah->qplib_ah,\n\t\t\t\t  !(flags & RDMA_CREATE_AH_SLEEPABLE));\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to allocate HW AH\");\n\t\treturn rc;\n\t}\n\n\t/* Write AVID to shared page. */\n\tif (udata) {\n\t\tstruct bnxt_re_ucontext *uctx = rdma_udata_to_drv_context(\n\t\t\tudata, struct bnxt_re_ucontext, ib_uctx);\n\t\tunsigned long flag;\n\t\tu32 *wrptr;\n\n\t\tspin_lock_irqsave(&uctx->sh_lock, flag);\n\t\twrptr = (u32 *)(uctx->shpg + BNXT_RE_AVID_OFFT);\n\t\t*wrptr = ah->qplib_ah.id;\n\t\twmb(); /* make sure cache is updated. */\n\t\tspin_unlock_irqrestore(&uctx->sh_lock, flag);\n\t}\n\n\treturn 0;\n}\n\nint bnxt_re_modify_ah(struct ib_ah *ib_ah, struct rdma_ah_attr *ah_attr)\n{\n\treturn 0;\n}\n\nint bnxt_re_query_ah(struct ib_ah *ib_ah, struct rdma_ah_attr *ah_attr)\n{\n\tstruct bnxt_re_ah *ah = container_of(ib_ah, struct bnxt_re_ah, ib_ah);\n\n\tah_attr->type = ib_ah->type;\n\trdma_ah_set_sl(ah_attr, ah->qplib_ah.sl);\n\tmemcpy(ah_attr->roce.dmac, ah->qplib_ah.dmac, ETH_ALEN);\n\trdma_ah_set_grh(ah_attr, NULL, 0,\n\t\t\tah->qplib_ah.host_sgid_index,\n\t\t\t0, ah->qplib_ah.traffic_class);\n\trdma_ah_set_dgid_raw(ah_attr, ah->qplib_ah.dgid.data);\n\trdma_ah_set_port_num(ah_attr, 1);\n\trdma_ah_set_static_rate(ah_attr, 0);\n\treturn 0;\n}\n\nunsigned long bnxt_re_lock_cqs(struct bnxt_re_qp *qp)\n\t__acquires(&qp->scq->cq_lock) __acquires(&qp->rcq->cq_lock)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->scq->cq_lock, flags);\n\tif (qp->rcq != qp->scq)\n\t\tspin_lock(&qp->rcq->cq_lock);\n\telse\n\t\t__acquire(&qp->rcq->cq_lock);\n\n\treturn flags;\n}\n\nvoid bnxt_re_unlock_cqs(struct bnxt_re_qp *qp,\n\t\t\tunsigned long flags)\n\t__releases(&qp->scq->cq_lock) __releases(&qp->rcq->cq_lock)\n{\n\tif (qp->rcq != qp->scq)\n\t\tspin_unlock(&qp->rcq->cq_lock);\n\telse\n\t\t__release(&qp->rcq->cq_lock);\n\tspin_unlock_irqrestore(&qp->scq->cq_lock, flags);\n}\n\n/* Queue Pairs */\nint bnxt_re_destroy_qp(struct ib_qp *ib_qp, struct ib_udata *udata)\n{\n\tstruct bnxt_re_qp *qp = container_of(ib_qp, struct bnxt_re_qp, ib_qp);\n\tstruct bnxt_re_dev *rdev = qp->rdev;\n\tunsigned int flags;\n\tint rc;\n\n\tbnxt_qplib_flush_cqn_wq(&qp->qplib_qp);\n\trc = bnxt_qplib_destroy_qp(&rdev->qplib_res, &qp->qplib_qp);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to destroy HW QP\");\n\t\treturn rc;\n\t}\n\n\tif (rdma_is_kernel_res(&qp->ib_qp.res)) {\n\t\tflags = bnxt_re_lock_cqs(qp);\n\t\tbnxt_qplib_clean_qp(&qp->qplib_qp);\n\t\tbnxt_re_unlock_cqs(qp, flags);\n\t}\n\n\tbnxt_qplib_free_qp_res(&rdev->qplib_res, &qp->qplib_qp);\n\n\tif (ib_qp->qp_type == IB_QPT_GSI && rdev->qp1_sqp) {\n\t\tbnxt_qplib_destroy_ah(&rdev->qplib_res, &rdev->sqp_ah->qplib_ah,\n\t\t\t\t      false);\n\n\t\tbnxt_qplib_clean_qp(&qp->qplib_qp);\n\t\trc = bnxt_qplib_destroy_qp(&rdev->qplib_res,\n\t\t\t\t\t   &rdev->qp1_sqp->qplib_qp);\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Failed to destroy Shadow QP\");\n\t\t\treturn rc;\n\t\t}\n\t\tbnxt_qplib_free_qp_res(&rdev->qplib_res,\n\t\t\t\t       &rdev->qp1_sqp->qplib_qp);\n\t\tmutex_lock(&rdev->qp_lock);\n\t\tlist_del(&rdev->qp1_sqp->list);\n\t\tatomic_dec(&rdev->qp_count);\n\t\tmutex_unlock(&rdev->qp_lock);\n\n\t\tkfree(rdev->sqp_ah);\n\t\tkfree(rdev->qp1_sqp);\n\t\trdev->qp1_sqp = NULL;\n\t\trdev->sqp_ah = NULL;\n\t}\n\n\tib_umem_release(qp->rumem);\n\tib_umem_release(qp->sumem);\n\n\tmutex_lock(&rdev->qp_lock);\n\tlist_del(&qp->list);\n\tatomic_dec(&rdev->qp_count);\n\tmutex_unlock(&rdev->qp_lock);\n\tkfree(qp);\n\treturn 0;\n}\n\nstatic u8 __from_ib_qp_type(enum ib_qp_type type)\n{\n\tswitch (type) {\n\tcase IB_QPT_GSI:\n\t\treturn CMDQ_CREATE_QP1_TYPE_GSI;\n\tcase IB_QPT_RC:\n\t\treturn CMDQ_CREATE_QP_TYPE_RC;\n\tcase IB_QPT_UD:\n\t\treturn CMDQ_CREATE_QP_TYPE_UD;\n\tdefault:\n\t\treturn IB_QPT_MAX;\n\t}\n}\n\nstatic int bnxt_re_init_user_qp(struct bnxt_re_dev *rdev, struct bnxt_re_pd *pd,\n\t\t\t\tstruct bnxt_re_qp *qp, struct ib_udata *udata)\n{\n\tstruct bnxt_re_qp_req ureq;\n\tstruct bnxt_qplib_qp *qplib_qp = &qp->qplib_qp;\n\tstruct ib_umem *umem;\n\tint bytes = 0, psn_sz;\n\tstruct bnxt_re_ucontext *cntx = rdma_udata_to_drv_context(\n\t\tudata, struct bnxt_re_ucontext, ib_uctx);\n\n\tif (ib_copy_from_udata(&ureq, udata, sizeof(ureq)))\n\t\treturn -EFAULT;\n\n\tbytes = (qplib_qp->sq.max_wqe * BNXT_QPLIB_MAX_SQE_ENTRY_SIZE);\n\t/* Consider mapping PSN search memory only for RC QPs. */\n\tif (qplib_qp->type == CMDQ_CREATE_QP_TYPE_RC) {\n\t\tpsn_sz = bnxt_qplib_is_chip_gen_p5(&rdev->chip_ctx) ?\n\t\t\t\t\tsizeof(struct sq_psn_search_ext) :\n\t\t\t\t\tsizeof(struct sq_psn_search);\n\t\tbytes += (qplib_qp->sq.max_wqe * psn_sz);\n\t}\n\tbytes = PAGE_ALIGN(bytes);\n\tumem = ib_umem_get(udata, ureq.qpsva, bytes, IB_ACCESS_LOCAL_WRITE, 1);\n\tif (IS_ERR(umem))\n\t\treturn PTR_ERR(umem);\n\n\tqp->sumem = umem;\n\tqplib_qp->sq.sg_info.sglist = umem->sg_head.sgl;\n\tqplib_qp->sq.sg_info.npages = ib_umem_num_pages(umem);\n\tqplib_qp->sq.sg_info.nmap = umem->nmap;\n\tqplib_qp->qp_handle = ureq.qp_handle;\n\n\tif (!qp->qplib_qp.srq) {\n\t\tbytes = (qplib_qp->rq.max_wqe * BNXT_QPLIB_MAX_RQE_ENTRY_SIZE);\n\t\tbytes = PAGE_ALIGN(bytes);\n\t\tumem = ib_umem_get(udata, ureq.qprva, bytes,\n\t\t\t\t   IB_ACCESS_LOCAL_WRITE, 1);\n\t\tif (IS_ERR(umem))\n\t\t\tgoto rqfail;\n\t\tqp->rumem = umem;\n\t\tqplib_qp->rq.sg_info.sglist = umem->sg_head.sgl;\n\t\tqplib_qp->rq.sg_info.npages = ib_umem_num_pages(umem);\n\t\tqplib_qp->rq.sg_info.nmap = umem->nmap;\n\t}\n\n\tqplib_qp->dpi = &cntx->dpi;\n\treturn 0;\nrqfail:\n\tib_umem_release(qp->sumem);\n\tqp->sumem = NULL;\n\tmemset(&qplib_qp->sq.sg_info, 0, sizeof(qplib_qp->sq.sg_info));\n\n\treturn PTR_ERR(umem);\n}\n\nstatic struct bnxt_re_ah *bnxt_re_create_shadow_qp_ah\n\t\t\t\t(struct bnxt_re_pd *pd,\n\t\t\t\t struct bnxt_qplib_res *qp1_res,\n\t\t\t\t struct bnxt_qplib_qp *qp1_qp)\n{\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct bnxt_re_ah *ah;\n\tunion ib_gid sgid;\n\tint rc;\n\n\tah = kzalloc(sizeof(*ah), GFP_KERNEL);\n\tif (!ah)\n\t\treturn NULL;\n\n\tah->rdev = rdev;\n\tah->qplib_ah.pd = &pd->qplib_pd;\n\n\trc = bnxt_re_query_gid(&rdev->ibdev, 1, 0, &sgid);\n\tif (rc)\n\t\tgoto fail;\n\n\t/* supply the dgid data same as sgid */\n\tmemcpy(ah->qplib_ah.dgid.data, &sgid.raw,\n\t       sizeof(union ib_gid));\n\tah->qplib_ah.sgid_index = 0;\n\n\tah->qplib_ah.traffic_class = 0;\n\tah->qplib_ah.flow_label = 0;\n\tah->qplib_ah.hop_limit = 1;\n\tah->qplib_ah.sl = 0;\n\t/* Have DMAC same as SMAC */\n\tether_addr_copy(ah->qplib_ah.dmac, rdev->netdev->dev_addr);\n\n\trc = bnxt_qplib_create_ah(&rdev->qplib_res, &ah->qplib_ah, false);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\"Failed to allocate HW AH for Shadow QP\");\n\t\tgoto fail;\n\t}\n\n\treturn ah;\n\nfail:\n\tkfree(ah);\n\treturn NULL;\n}\n\nstatic struct bnxt_re_qp *bnxt_re_create_shadow_qp\n\t\t\t\t(struct bnxt_re_pd *pd,\n\t\t\t\t struct bnxt_qplib_res *qp1_res,\n\t\t\t\t struct bnxt_qplib_qp *qp1_qp)\n{\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct bnxt_re_qp *qp;\n\tint rc;\n\n\tqp = kzalloc(sizeof(*qp), GFP_KERNEL);\n\tif (!qp)\n\t\treturn NULL;\n\n\tqp->rdev = rdev;\n\n\t/* Initialize the shadow QP structure from the QP1 values */\n\tether_addr_copy(qp->qplib_qp.smac, rdev->netdev->dev_addr);\n\n\tqp->qplib_qp.pd = &pd->qplib_pd;\n\tqp->qplib_qp.qp_handle = (u64)(unsigned long)(&qp->qplib_qp);\n\tqp->qplib_qp.type = IB_QPT_UD;\n\n\tqp->qplib_qp.max_inline_data = 0;\n\tqp->qplib_qp.sig_type = true;\n\n\t/* Shadow QP SQ depth should be same as QP1 RQ depth */\n\tqp->qplib_qp.sq.max_wqe = qp1_qp->rq.max_wqe;\n\tqp->qplib_qp.sq.max_sge = 2;\n\t/* Q full delta can be 1 since it is internal QP */\n\tqp->qplib_qp.sq.q_full_delta = 1;\n\n\tqp->qplib_qp.scq = qp1_qp->scq;\n\tqp->qplib_qp.rcq = qp1_qp->rcq;\n\n\tqp->qplib_qp.rq.max_wqe = qp1_qp->rq.max_wqe;\n\tqp->qplib_qp.rq.max_sge = qp1_qp->rq.max_sge;\n\t/* Q full delta can be 1 since it is internal QP */\n\tqp->qplib_qp.rq.q_full_delta = 1;\n\n\tqp->qplib_qp.mtu = qp1_qp->mtu;\n\n\tqp->qplib_qp.sq_hdr_buf_size = 0;\n\tqp->qplib_qp.rq_hdr_buf_size = BNXT_QPLIB_MAX_GRH_HDR_SIZE_IPV6;\n\tqp->qplib_qp.dpi = &rdev->dpi_privileged;\n\n\trc = bnxt_qplib_create_qp(qp1_res, &qp->qplib_qp);\n\tif (rc)\n\t\tgoto fail;\n\n\trdev->sqp_id = qp->qplib_qp.id;\n\n\tspin_lock_init(&qp->sq_lock);\n\tINIT_LIST_HEAD(&qp->list);\n\tmutex_lock(&rdev->qp_lock);\n\tlist_add_tail(&qp->list, &rdev->qp_list);\n\tatomic_inc(&rdev->qp_count);\n\tmutex_unlock(&rdev->qp_lock);\n\treturn qp;\nfail:\n\tkfree(qp);\n\treturn NULL;\n}\n\nstruct ib_qp *bnxt_re_create_qp(struct ib_pd *ib_pd,\n\t\t\t\tstruct ib_qp_init_attr *qp_init_attr,\n\t\t\t\tstruct ib_udata *udata)\n{\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct bnxt_qplib_dev_attr *dev_attr = &rdev->dev_attr;\n\tstruct bnxt_re_qp *qp;\n\tstruct bnxt_re_cq *cq;\n\tstruct bnxt_re_srq *srq;\n\tint rc, entries;\n\n\tif ((qp_init_attr->cap.max_send_wr > dev_attr->max_qp_wqes) ||\n\t    (qp_init_attr->cap.max_recv_wr > dev_attr->max_qp_wqes) ||\n\t    (qp_init_attr->cap.max_send_sge > dev_attr->max_qp_sges) ||\n\t    (qp_init_attr->cap.max_recv_sge > dev_attr->max_qp_sges) ||\n\t    (qp_init_attr->cap.max_inline_data > dev_attr->max_inline_data))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tqp = kzalloc(sizeof(*qp), GFP_KERNEL);\n\tif (!qp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tqp->rdev = rdev;\n\tether_addr_copy(qp->qplib_qp.smac, rdev->netdev->dev_addr);\n\tqp->qplib_qp.pd = &pd->qplib_pd;\n\tqp->qplib_qp.qp_handle = (u64)(unsigned long)(&qp->qplib_qp);\n\tqp->qplib_qp.type = __from_ib_qp_type(qp_init_attr->qp_type);\n\n\tif (qp_init_attr->qp_type == IB_QPT_GSI &&\n\t    bnxt_qplib_is_chip_gen_p5(&rdev->chip_ctx))\n\t\tqp->qplib_qp.type = CMDQ_CREATE_QP_TYPE_GSI;\n\tif (qp->qplib_qp.type == IB_QPT_MAX) {\n\t\tdev_err(rdev_to_dev(rdev), \"QP type 0x%x not supported\",\n\t\t\tqp->qplib_qp.type);\n\t\trc = -EINVAL;\n\t\tgoto fail;\n\t}\n\n\tqp->qplib_qp.max_inline_data = qp_init_attr->cap.max_inline_data;\n\tqp->qplib_qp.sig_type = ((qp_init_attr->sq_sig_type ==\n\t\t\t\t  IB_SIGNAL_ALL_WR) ? true : false);\n\n\tqp->qplib_qp.sq.max_sge = qp_init_attr->cap.max_send_sge;\n\tif (qp->qplib_qp.sq.max_sge > dev_attr->max_qp_sges)\n\t\tqp->qplib_qp.sq.max_sge = dev_attr->max_qp_sges;\n\n\tif (qp_init_attr->send_cq) {\n\t\tcq = container_of(qp_init_attr->send_cq, struct bnxt_re_cq,\n\t\t\t\t  ib_cq);\n\t\tif (!cq) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"Send CQ not found\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto fail;\n\t\t}\n\t\tqp->qplib_qp.scq = &cq->qplib_cq;\n\t\tqp->scq = cq;\n\t}\n\n\tif (qp_init_attr->recv_cq) {\n\t\tcq = container_of(qp_init_attr->recv_cq, struct bnxt_re_cq,\n\t\t\t\t  ib_cq);\n\t\tif (!cq) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"Receive CQ not found\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto fail;\n\t\t}\n\t\tqp->qplib_qp.rcq = &cq->qplib_cq;\n\t\tqp->rcq = cq;\n\t}\n\n\tif (qp_init_attr->srq) {\n\t\tsrq = container_of(qp_init_attr->srq, struct bnxt_re_srq,\n\t\t\t\t   ib_srq);\n\t\tif (!srq) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"SRQ not found\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto fail;\n\t\t}\n\t\tqp->qplib_qp.srq = &srq->qplib_srq;\n\t\tqp->qplib_qp.rq.max_wqe = 0;\n\t} else {\n\t\t/* Allocate 1 more than what's provided so posting max doesn't\n\t\t * mean empty\n\t\t */\n\t\tentries = roundup_pow_of_two(qp_init_attr->cap.max_recv_wr + 1);\n\t\tqp->qplib_qp.rq.max_wqe = min_t(u32, entries,\n\t\t\t\t\t\tdev_attr->max_qp_wqes + 1);\n\n\t\tqp->qplib_qp.rq.q_full_delta = qp->qplib_qp.rq.max_wqe -\n\t\t\t\t\t\tqp_init_attr->cap.max_recv_wr;\n\n\t\tqp->qplib_qp.rq.max_sge = qp_init_attr->cap.max_recv_sge;\n\t\tif (qp->qplib_qp.rq.max_sge > dev_attr->max_qp_sges)\n\t\t\tqp->qplib_qp.rq.max_sge = dev_attr->max_qp_sges;\n\t}\n\n\tqp->qplib_qp.mtu = ib_mtu_enum_to_int(iboe_get_mtu(rdev->netdev->mtu));\n\n\tif (qp_init_attr->qp_type == IB_QPT_GSI &&\n\t    !(bnxt_qplib_is_chip_gen_p5(&rdev->chip_ctx))) {\n\t\t/* Allocate 1 more than what's provided */\n\t\tentries = roundup_pow_of_two(qp_init_attr->cap.max_send_wr + 1);\n\t\tqp->qplib_qp.sq.max_wqe = min_t(u32, entries,\n\t\t\t\t\t\tdev_attr->max_qp_wqes + 1);\n\t\tqp->qplib_qp.sq.q_full_delta = qp->qplib_qp.sq.max_wqe -\n\t\t\t\t\t\tqp_init_attr->cap.max_send_wr;\n\t\tqp->qplib_qp.rq.max_sge = dev_attr->max_qp_sges;\n\t\tif (qp->qplib_qp.rq.max_sge > dev_attr->max_qp_sges)\n\t\t\tqp->qplib_qp.rq.max_sge = dev_attr->max_qp_sges;\n\t\tqp->qplib_qp.sq.max_sge++;\n\t\tif (qp->qplib_qp.sq.max_sge > dev_attr->max_qp_sges)\n\t\t\tqp->qplib_qp.sq.max_sge = dev_attr->max_qp_sges;\n\n\t\tqp->qplib_qp.rq_hdr_buf_size =\n\t\t\t\t\tBNXT_QPLIB_MAX_QP1_RQ_HDR_SIZE_V2;\n\n\t\tqp->qplib_qp.sq_hdr_buf_size =\n\t\t\t\t\tBNXT_QPLIB_MAX_QP1_SQ_HDR_SIZE_V2;\n\t\tqp->qplib_qp.dpi = &rdev->dpi_privileged;\n\t\trc = bnxt_qplib_create_qp1(&rdev->qplib_res, &qp->qplib_qp);\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"Failed to create HW QP1\");\n\t\t\tgoto fail;\n\t\t}\n\t\t/* Create a shadow QP to handle the QP1 traffic */\n\t\trdev->qp1_sqp = bnxt_re_create_shadow_qp(pd, &rdev->qplib_res,\n\t\t\t\t\t\t\t &qp->qplib_qp);\n\t\tif (!rdev->qp1_sqp) {\n\t\t\trc = -EINVAL;\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Failed to create Shadow QP for QP1\");\n\t\t\tgoto qp_destroy;\n\t\t}\n\t\trdev->sqp_ah = bnxt_re_create_shadow_qp_ah(pd, &rdev->qplib_res,\n\t\t\t\t\t\t\t   &qp->qplib_qp);\n\t\tif (!rdev->sqp_ah) {\n\t\t\tbnxt_qplib_destroy_qp(&rdev->qplib_res,\n\t\t\t\t\t      &rdev->qp1_sqp->qplib_qp);\n\t\t\trc = -EINVAL;\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Failed to create AH entry for ShadowQP\");\n\t\t\tgoto qp_destroy;\n\t\t}\n\n\t} else {\n\t\t/* Allocate 128 + 1 more than what's provided */\n\t\tentries = roundup_pow_of_two(qp_init_attr->cap.max_send_wr +\n\t\t\t\t\t     BNXT_QPLIB_RESERVED_QP_WRS + 1);\n\t\tqp->qplib_qp.sq.max_wqe = min_t(u32, entries,\n\t\t\t\t\t\tdev_attr->max_qp_wqes +\n\t\t\t\t\t\tBNXT_QPLIB_RESERVED_QP_WRS + 1);\n\t\tqp->qplib_qp.sq.q_full_delta = BNXT_QPLIB_RESERVED_QP_WRS + 1;\n\n\t\t/*\n\t\t * Reserving one slot for Phantom WQE. Application can\n\t\t * post one extra entry in this case. But allowing this to avoid\n\t\t * unexpected Queue full condition\n\t\t */\n\n\t\tqp->qplib_qp.sq.q_full_delta -= 1;\n\n\t\tqp->qplib_qp.max_rd_atomic = dev_attr->max_qp_rd_atom;\n\t\tqp->qplib_qp.max_dest_rd_atomic = dev_attr->max_qp_init_rd_atom;\n\t\tif (udata) {\n\t\t\trc = bnxt_re_init_user_qp(rdev, pd, qp, udata);\n\t\t\tif (rc)\n\t\t\t\tgoto fail;\n\t\t} else {\n\t\t\tqp->qplib_qp.dpi = &rdev->dpi_privileged;\n\t\t}\n\n\t\trc = bnxt_qplib_create_qp(&rdev->qplib_res, &qp->qplib_qp);\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"Failed to create HW QP\");\n\t\t\tgoto free_umem;\n\t\t}\n\t}\n\n\tqp->ib_qp.qp_num = qp->qplib_qp.id;\n\tspin_lock_init(&qp->sq_lock);\n\tspin_lock_init(&qp->rq_lock);\n\n\tif (udata) {\n\t\tstruct bnxt_re_qp_resp resp;\n\n\t\tresp.qpid = qp->ib_qp.qp_num;\n\t\tresp.rsvd = 0;\n\t\trc = ib_copy_to_udata(udata, &resp, sizeof(resp));\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"Failed to copy QP udata\");\n\t\t\tgoto qp_destroy;\n\t\t}\n\t}\n\tINIT_LIST_HEAD(&qp->list);\n\tmutex_lock(&rdev->qp_lock);\n\tlist_add_tail(&qp->list, &rdev->qp_list);\n\tatomic_inc(&rdev->qp_count);\n\tmutex_unlock(&rdev->qp_lock);\n\n\treturn &qp->ib_qp;\nqp_destroy:\n\tbnxt_qplib_destroy_qp(&rdev->qplib_res, &qp->qplib_qp);\nfree_umem:\n\tib_umem_release(qp->rumem);\n\tib_umem_release(qp->sumem);\nfail:\n\tkfree(qp);\n\treturn ERR_PTR(rc);\n}\n\nstatic u8 __from_ib_qp_state(enum ib_qp_state state)\n{\n\tswitch (state) {\n\tcase IB_QPS_RESET:\n\t\treturn CMDQ_MODIFY_QP_NEW_STATE_RESET;\n\tcase IB_QPS_INIT:\n\t\treturn CMDQ_MODIFY_QP_NEW_STATE_INIT;\n\tcase IB_QPS_RTR:\n\t\treturn CMDQ_MODIFY_QP_NEW_STATE_RTR;\n\tcase IB_QPS_RTS:\n\t\treturn CMDQ_MODIFY_QP_NEW_STATE_RTS;\n\tcase IB_QPS_SQD:\n\t\treturn CMDQ_MODIFY_QP_NEW_STATE_SQD;\n\tcase IB_QPS_SQE:\n\t\treturn CMDQ_MODIFY_QP_NEW_STATE_SQE;\n\tcase IB_QPS_ERR:\n\tdefault:\n\t\treturn CMDQ_MODIFY_QP_NEW_STATE_ERR;\n\t}\n}\n\nstatic enum ib_qp_state __to_ib_qp_state(u8 state)\n{\n\tswitch (state) {\n\tcase CMDQ_MODIFY_QP_NEW_STATE_RESET:\n\t\treturn IB_QPS_RESET;\n\tcase CMDQ_MODIFY_QP_NEW_STATE_INIT:\n\t\treturn IB_QPS_INIT;\n\tcase CMDQ_MODIFY_QP_NEW_STATE_RTR:\n\t\treturn IB_QPS_RTR;\n\tcase CMDQ_MODIFY_QP_NEW_STATE_RTS:\n\t\treturn IB_QPS_RTS;\n\tcase CMDQ_MODIFY_QP_NEW_STATE_SQD:\n\t\treturn IB_QPS_SQD;\n\tcase CMDQ_MODIFY_QP_NEW_STATE_SQE:\n\t\treturn IB_QPS_SQE;\n\tcase CMDQ_MODIFY_QP_NEW_STATE_ERR:\n\tdefault:\n\t\treturn IB_QPS_ERR;\n\t}\n}\n\nstatic u32 __from_ib_mtu(enum ib_mtu mtu)\n{\n\tswitch (mtu) {\n\tcase IB_MTU_256:\n\t\treturn CMDQ_MODIFY_QP_PATH_MTU_MTU_256;\n\tcase IB_MTU_512:\n\t\treturn CMDQ_MODIFY_QP_PATH_MTU_MTU_512;\n\tcase IB_MTU_1024:\n\t\treturn CMDQ_MODIFY_QP_PATH_MTU_MTU_1024;\n\tcase IB_MTU_2048:\n\t\treturn CMDQ_MODIFY_QP_PATH_MTU_MTU_2048;\n\tcase IB_MTU_4096:\n\t\treturn CMDQ_MODIFY_QP_PATH_MTU_MTU_4096;\n\tdefault:\n\t\treturn CMDQ_MODIFY_QP_PATH_MTU_MTU_2048;\n\t}\n}\n\nstatic enum ib_mtu __to_ib_mtu(u32 mtu)\n{\n\tswitch (mtu & CREQ_QUERY_QP_RESP_SB_PATH_MTU_MASK) {\n\tcase CMDQ_MODIFY_QP_PATH_MTU_MTU_256:\n\t\treturn IB_MTU_256;\n\tcase CMDQ_MODIFY_QP_PATH_MTU_MTU_512:\n\t\treturn IB_MTU_512;\n\tcase CMDQ_MODIFY_QP_PATH_MTU_MTU_1024:\n\t\treturn IB_MTU_1024;\n\tcase CMDQ_MODIFY_QP_PATH_MTU_MTU_2048:\n\t\treturn IB_MTU_2048;\n\tcase CMDQ_MODIFY_QP_PATH_MTU_MTU_4096:\n\t\treturn IB_MTU_4096;\n\tdefault:\n\t\treturn IB_MTU_2048;\n\t}\n}\n\n/* Shared Receive Queues */\nvoid bnxt_re_destroy_srq(struct ib_srq *ib_srq, struct ib_udata *udata)\n{\n\tstruct bnxt_re_srq *srq = container_of(ib_srq, struct bnxt_re_srq,\n\t\t\t\t\t       ib_srq);\n\tstruct bnxt_re_dev *rdev = srq->rdev;\n\tstruct bnxt_qplib_srq *qplib_srq = &srq->qplib_srq;\n\tstruct bnxt_qplib_nq *nq = NULL;\n\n\tif (qplib_srq->cq)\n\t\tnq = qplib_srq->cq->nq;\n\tbnxt_qplib_destroy_srq(&rdev->qplib_res, qplib_srq);\n\tib_umem_release(srq->umem);\n\tatomic_dec(&rdev->srq_count);\n\tif (nq)\n\t\tnq->budget--;\n}\n\nstatic int bnxt_re_init_user_srq(struct bnxt_re_dev *rdev,\n\t\t\t\t struct bnxt_re_pd *pd,\n\t\t\t\t struct bnxt_re_srq *srq,\n\t\t\t\t struct ib_udata *udata)\n{\n\tstruct bnxt_re_srq_req ureq;\n\tstruct bnxt_qplib_srq *qplib_srq = &srq->qplib_srq;\n\tstruct ib_umem *umem;\n\tint bytes = 0;\n\tstruct bnxt_re_ucontext *cntx = rdma_udata_to_drv_context(\n\t\tudata, struct bnxt_re_ucontext, ib_uctx);\n\n\tif (ib_copy_from_udata(&ureq, udata, sizeof(ureq)))\n\t\treturn -EFAULT;\n\n\tbytes = (qplib_srq->max_wqe * BNXT_QPLIB_MAX_RQE_ENTRY_SIZE);\n\tbytes = PAGE_ALIGN(bytes);\n\tumem = ib_umem_get(udata, ureq.srqva, bytes, IB_ACCESS_LOCAL_WRITE, 1);\n\tif (IS_ERR(umem))\n\t\treturn PTR_ERR(umem);\n\n\tsrq->umem = umem;\n\tqplib_srq->sg_info.sglist = umem->sg_head.sgl;\n\tqplib_srq->sg_info.npages = ib_umem_num_pages(umem);\n\tqplib_srq->sg_info.nmap = umem->nmap;\n\tqplib_srq->srq_handle = ureq.srq_handle;\n\tqplib_srq->dpi = &cntx->dpi;\n\n\treturn 0;\n}\n\nint bnxt_re_create_srq(struct ib_srq *ib_srq,\n\t\t       struct ib_srq_init_attr *srq_init_attr,\n\t\t       struct ib_udata *udata)\n{\n\tstruct ib_pd *ib_pd = ib_srq->pd;\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct bnxt_qplib_dev_attr *dev_attr = &rdev->dev_attr;\n\tstruct bnxt_re_srq *srq =\n\t\tcontainer_of(ib_srq, struct bnxt_re_srq, ib_srq);\n\tstruct bnxt_qplib_nq *nq = NULL;\n\tint rc, entries;\n\n\tif (srq_init_attr->attr.max_wr >= dev_attr->max_srq_wqes) {\n\t\tdev_err(rdev_to_dev(rdev), \"Create CQ failed - max exceeded\");\n\t\trc = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\tif (srq_init_attr->srq_type != IB_SRQT_BASIC) {\n\t\trc = -EOPNOTSUPP;\n\t\tgoto exit;\n\t}\n\n\tsrq->rdev = rdev;\n\tsrq->qplib_srq.pd = &pd->qplib_pd;\n\tsrq->qplib_srq.dpi = &rdev->dpi_privileged;\n\t/* Allocate 1 more than what's provided so posting max doesn't\n\t * mean empty\n\t */\n\tentries = roundup_pow_of_two(srq_init_attr->attr.max_wr + 1);\n\tif (entries > dev_attr->max_srq_wqes + 1)\n\t\tentries = dev_attr->max_srq_wqes + 1;\n\n\tsrq->qplib_srq.max_wqe = entries;\n\tsrq->qplib_srq.max_sge = srq_init_attr->attr.max_sge;\n\tsrq->qplib_srq.threshold = srq_init_attr->attr.srq_limit;\n\tsrq->srq_limit = srq_init_attr->attr.srq_limit;\n\tsrq->qplib_srq.eventq_hw_ring_id = rdev->nq[0].ring_id;\n\tnq = &rdev->nq[0];\n\n\tif (udata) {\n\t\trc = bnxt_re_init_user_srq(rdev, pd, srq, udata);\n\t\tif (rc)\n\t\t\tgoto fail;\n\t}\n\n\trc = bnxt_qplib_create_srq(&rdev->qplib_res, &srq->qplib_srq);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Create HW SRQ failed!\");\n\t\tgoto fail;\n\t}\n\n\tif (udata) {\n\t\tstruct bnxt_re_srq_resp resp;\n\n\t\tresp.srqid = srq->qplib_srq.id;\n\t\trc = ib_copy_to_udata(udata, &resp, sizeof(resp));\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"SRQ copy to udata failed!\");\n\t\t\tbnxt_qplib_destroy_srq(&rdev->qplib_res,\n\t\t\t\t\t       &srq->qplib_srq);\n\t\t\tgoto exit;\n\t\t}\n\t}\n\tif (nq)\n\t\tnq->budget++;\n\tatomic_inc(&rdev->srq_count);\n\n\treturn 0;\n\nfail:\n\tib_umem_release(srq->umem);\nexit:\n\treturn rc;\n}\n\nint bnxt_re_modify_srq(struct ib_srq *ib_srq, struct ib_srq_attr *srq_attr,\n\t\t       enum ib_srq_attr_mask srq_attr_mask,\n\t\t       struct ib_udata *udata)\n{\n\tstruct bnxt_re_srq *srq = container_of(ib_srq, struct bnxt_re_srq,\n\t\t\t\t\t       ib_srq);\n\tstruct bnxt_re_dev *rdev = srq->rdev;\n\tint rc;\n\n\tswitch (srq_attr_mask) {\n\tcase IB_SRQ_MAX_WR:\n\t\t/* SRQ resize is not supported */\n\t\tbreak;\n\tcase IB_SRQ_LIMIT:\n\t\t/* Change the SRQ threshold */\n\t\tif (srq_attr->srq_limit > srq->qplib_srq.max_wqe)\n\t\t\treturn -EINVAL;\n\n\t\tsrq->qplib_srq.threshold = srq_attr->srq_limit;\n\t\trc = bnxt_qplib_modify_srq(&rdev->qplib_res, &srq->qplib_srq);\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"Modify HW SRQ failed!\");\n\t\t\treturn rc;\n\t\t}\n\t\t/* On success, update the shadow */\n\t\tsrq->srq_limit = srq_attr->srq_limit;\n\t\t/* No need to Build and send response back to udata */\n\t\tbreak;\n\tdefault:\n\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\"Unsupported srq_attr_mask 0x%x\", srq_attr_mask);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nint bnxt_re_query_srq(struct ib_srq *ib_srq, struct ib_srq_attr *srq_attr)\n{\n\tstruct bnxt_re_srq *srq = container_of(ib_srq, struct bnxt_re_srq,\n\t\t\t\t\t       ib_srq);\n\tstruct bnxt_re_srq tsrq;\n\tstruct bnxt_re_dev *rdev = srq->rdev;\n\tint rc;\n\n\t/* Get live SRQ attr */\n\ttsrq.qplib_srq.id = srq->qplib_srq.id;\n\trc = bnxt_qplib_query_srq(&rdev->qplib_res, &tsrq.qplib_srq);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Query HW SRQ failed!\");\n\t\treturn rc;\n\t}\n\tsrq_attr->max_wr = srq->qplib_srq.max_wqe;\n\tsrq_attr->max_sge = srq->qplib_srq.max_sge;\n\tsrq_attr->srq_limit = tsrq.qplib_srq.threshold;\n\n\treturn 0;\n}\n\nint bnxt_re_post_srq_recv(struct ib_srq *ib_srq, const struct ib_recv_wr *wr,\n\t\t\t  const struct ib_recv_wr **bad_wr)\n{\n\tstruct bnxt_re_srq *srq = container_of(ib_srq, struct bnxt_re_srq,\n\t\t\t\t\t       ib_srq);\n\tstruct bnxt_qplib_swqe wqe;\n\tunsigned long flags;\n\tint rc = 0;\n\n\tspin_lock_irqsave(&srq->lock, flags);\n\twhile (wr) {\n\t\t/* Transcribe each ib_recv_wr to qplib_swqe */\n\t\twqe.num_sge = wr->num_sge;\n\t\tbnxt_re_build_sgl(wr->sg_list, wqe.sg_list, wr->num_sge);\n\t\twqe.wr_id = wr->wr_id;\n\t\twqe.type = BNXT_QPLIB_SWQE_TYPE_RECV;\n\n\t\trc = bnxt_qplib_post_srq_recv(&srq->qplib_srq, &wqe);\n\t\tif (rc) {\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\twr = wr->next;\n\t}\n\tspin_unlock_irqrestore(&srq->lock, flags);\n\n\treturn rc;\n}\nstatic int bnxt_re_modify_shadow_qp(struct bnxt_re_dev *rdev,\n\t\t\t\t    struct bnxt_re_qp *qp1_qp,\n\t\t\t\t    int qp_attr_mask)\n{\n\tstruct bnxt_re_qp *qp = rdev->qp1_sqp;\n\tint rc = 0;\n\n\tif (qp_attr_mask & IB_QP_STATE) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_STATE;\n\t\tqp->qplib_qp.state = qp1_qp->qplib_qp.state;\n\t}\n\tif (qp_attr_mask & IB_QP_PKEY_INDEX) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_PKEY;\n\t\tqp->qplib_qp.pkey_index = qp1_qp->qplib_qp.pkey_index;\n\t}\n\n\tif (qp_attr_mask & IB_QP_QKEY) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_QKEY;\n\t\t/* Using a Random  QKEY */\n\t\tqp->qplib_qp.qkey = 0x81818181;\n\t}\n\tif (qp_attr_mask & IB_QP_SQ_PSN) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_SQ_PSN;\n\t\tqp->qplib_qp.sq.psn = qp1_qp->qplib_qp.sq.psn;\n\t}\n\n\trc = bnxt_qplib_modify_qp(&rdev->qplib_res, &qp->qplib_qp);\n\tif (rc)\n\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\"Failed to modify Shadow QP for QP1\");\n\treturn rc;\n}\n\nint bnxt_re_modify_qp(struct ib_qp *ib_qp, struct ib_qp_attr *qp_attr,\n\t\t      int qp_attr_mask, struct ib_udata *udata)\n{\n\tstruct bnxt_re_qp *qp = container_of(ib_qp, struct bnxt_re_qp, ib_qp);\n\tstruct bnxt_re_dev *rdev = qp->rdev;\n\tstruct bnxt_qplib_dev_attr *dev_attr = &rdev->dev_attr;\n\tenum ib_qp_state curr_qp_state, new_qp_state;\n\tint rc, entries;\n\tunsigned int flags;\n\tu8 nw_type;\n\n\tqp->qplib_qp.modify_flags = 0;\n\tif (qp_attr_mask & IB_QP_STATE) {\n\t\tcurr_qp_state = __to_ib_qp_state(qp->qplib_qp.cur_qp_state);\n\t\tnew_qp_state = qp_attr->qp_state;\n\t\tif (!ib_modify_qp_is_ok(curr_qp_state, new_qp_state,\n\t\t\t\t\tib_qp->qp_type, qp_attr_mask)) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Invalid attribute mask: %#x specified \",\n\t\t\t\tqp_attr_mask);\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"for qpn: %#x type: %#x\",\n\t\t\t\tib_qp->qp_num, ib_qp->qp_type);\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"curr_qp_state=0x%x, new_qp_state=0x%x\\n\",\n\t\t\t\tcurr_qp_state, new_qp_state);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_STATE;\n\t\tqp->qplib_qp.state = __from_ib_qp_state(qp_attr->qp_state);\n\n\t\tif (!qp->sumem &&\n\t\t    qp->qplib_qp.state == CMDQ_MODIFY_QP_NEW_STATE_ERR) {\n\t\t\tdev_dbg(rdev_to_dev(rdev),\n\t\t\t\t\"Move QP = %p to flush list\\n\",\n\t\t\t\tqp);\n\t\t\tflags = bnxt_re_lock_cqs(qp);\n\t\t\tbnxt_qplib_add_flush_qp(&qp->qplib_qp);\n\t\t\tbnxt_re_unlock_cqs(qp, flags);\n\t\t}\n\t\tif (!qp->sumem &&\n\t\t    qp->qplib_qp.state == CMDQ_MODIFY_QP_NEW_STATE_RESET) {\n\t\t\tdev_dbg(rdev_to_dev(rdev),\n\t\t\t\t\"Move QP = %p out of flush list\\n\",\n\t\t\t\tqp);\n\t\t\tflags = bnxt_re_lock_cqs(qp);\n\t\t\tbnxt_qplib_clean_qp(&qp->qplib_qp);\n\t\t\tbnxt_re_unlock_cqs(qp, flags);\n\t\t}\n\t}\n\tif (qp_attr_mask & IB_QP_EN_SQD_ASYNC_NOTIFY) {\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_EN_SQD_ASYNC_NOTIFY;\n\t\tqp->qplib_qp.en_sqd_async_notify = true;\n\t}\n\tif (qp_attr_mask & IB_QP_ACCESS_FLAGS) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_ACCESS;\n\t\tqp->qplib_qp.access =\n\t\t\t__from_ib_access_flags(qp_attr->qp_access_flags);\n\t\t/* LOCAL_WRITE access must be set to allow RC receive */\n\t\tqp->qplib_qp.access |= BNXT_QPLIB_ACCESS_LOCAL_WRITE;\n\t\t/* Temp: Set all params on QP as of now */\n\t\tqp->qplib_qp.access |= CMDQ_MODIFY_QP_ACCESS_REMOTE_WRITE;\n\t\tqp->qplib_qp.access |= CMDQ_MODIFY_QP_ACCESS_REMOTE_READ;\n\t}\n\tif (qp_attr_mask & IB_QP_PKEY_INDEX) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_PKEY;\n\t\tqp->qplib_qp.pkey_index = qp_attr->pkey_index;\n\t}\n\tif (qp_attr_mask & IB_QP_QKEY) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_QKEY;\n\t\tqp->qplib_qp.qkey = qp_attr->qkey;\n\t}\n\tif (qp_attr_mask & IB_QP_AV) {\n\t\tconst struct ib_global_route *grh =\n\t\t\trdma_ah_read_grh(&qp_attr->ah_attr);\n\t\tconst struct ib_gid_attr *sgid_attr;\n\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_DGID |\n\t\t\t\t     CMDQ_MODIFY_QP_MODIFY_MASK_FLOW_LABEL |\n\t\t\t\t     CMDQ_MODIFY_QP_MODIFY_MASK_SGID_INDEX |\n\t\t\t\t     CMDQ_MODIFY_QP_MODIFY_MASK_HOP_LIMIT |\n\t\t\t\t     CMDQ_MODIFY_QP_MODIFY_MASK_TRAFFIC_CLASS |\n\t\t\t\t     CMDQ_MODIFY_QP_MODIFY_MASK_DEST_MAC |\n\t\t\t\t     CMDQ_MODIFY_QP_MODIFY_MASK_VLAN_ID;\n\t\tmemcpy(qp->qplib_qp.ah.dgid.data, grh->dgid.raw,\n\t\t       sizeof(qp->qplib_qp.ah.dgid.data));\n\t\tqp->qplib_qp.ah.flow_label = grh->flow_label;\n\t\t/* If RoCE V2 is enabled, stack will have two entries for\n\t\t * each GID entry. Avoiding this duplicte entry in HW. Dividing\n\t\t * the GID index by 2 for RoCE V2\n\t\t */\n\t\tqp->qplib_qp.ah.sgid_index = grh->sgid_index / 2;\n\t\tqp->qplib_qp.ah.host_sgid_index = grh->sgid_index;\n\t\tqp->qplib_qp.ah.hop_limit = grh->hop_limit;\n\t\tqp->qplib_qp.ah.traffic_class = grh->traffic_class;\n\t\tqp->qplib_qp.ah.sl = rdma_ah_get_sl(&qp_attr->ah_attr);\n\t\tether_addr_copy(qp->qplib_qp.ah.dmac,\n\t\t\t\tqp_attr->ah_attr.roce.dmac);\n\n\t\tsgid_attr = qp_attr->ah_attr.grh.sgid_attr;\n\t\trc = rdma_read_gid_l2_fields(sgid_attr, NULL,\n\t\t\t\t\t     &qp->qplib_qp.smac[0]);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tnw_type = rdma_gid_attr_network_type(sgid_attr);\n\t\tswitch (nw_type) {\n\t\tcase RDMA_NETWORK_IPV4:\n\t\t\tqp->qplib_qp.nw_type =\n\t\t\t\tCMDQ_MODIFY_QP_NETWORK_TYPE_ROCEV2_IPV4;\n\t\t\tbreak;\n\t\tcase RDMA_NETWORK_IPV6:\n\t\t\tqp->qplib_qp.nw_type =\n\t\t\t\tCMDQ_MODIFY_QP_NETWORK_TYPE_ROCEV2_IPV6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tqp->qplib_qp.nw_type =\n\t\t\t\tCMDQ_MODIFY_QP_NETWORK_TYPE_ROCEV1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (qp_attr_mask & IB_QP_PATH_MTU) {\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_PATH_MTU;\n\t\tqp->qplib_qp.path_mtu = __from_ib_mtu(qp_attr->path_mtu);\n\t\tqp->qplib_qp.mtu = ib_mtu_enum_to_int(qp_attr->path_mtu);\n\t} else if (qp_attr->qp_state == IB_QPS_RTR) {\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_PATH_MTU;\n\t\tqp->qplib_qp.path_mtu =\n\t\t\t__from_ib_mtu(iboe_get_mtu(rdev->netdev->mtu));\n\t\tqp->qplib_qp.mtu =\n\t\t\tib_mtu_enum_to_int(iboe_get_mtu(rdev->netdev->mtu));\n\t}\n\n\tif (qp_attr_mask & IB_QP_TIMEOUT) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_TIMEOUT;\n\t\tqp->qplib_qp.timeout = qp_attr->timeout;\n\t}\n\tif (qp_attr_mask & IB_QP_RETRY_CNT) {\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_RETRY_CNT;\n\t\tqp->qplib_qp.retry_cnt = qp_attr->retry_cnt;\n\t}\n\tif (qp_attr_mask & IB_QP_RNR_RETRY) {\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_RNR_RETRY;\n\t\tqp->qplib_qp.rnr_retry = qp_attr->rnr_retry;\n\t}\n\tif (qp_attr_mask & IB_QP_MIN_RNR_TIMER) {\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_MIN_RNR_TIMER;\n\t\tqp->qplib_qp.min_rnr_timer = qp_attr->min_rnr_timer;\n\t}\n\tif (qp_attr_mask & IB_QP_RQ_PSN) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_RQ_PSN;\n\t\tqp->qplib_qp.rq.psn = qp_attr->rq_psn;\n\t}\n\tif (qp_attr_mask & IB_QP_MAX_QP_RD_ATOMIC) {\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_MAX_RD_ATOMIC;\n\t\t/* Cap the max_rd_atomic to device max */\n\t\tqp->qplib_qp.max_rd_atomic = min_t(u32, qp_attr->max_rd_atomic,\n\t\t\t\t\t\t   dev_attr->max_qp_rd_atom);\n\t}\n\tif (qp_attr_mask & IB_QP_SQ_PSN) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_SQ_PSN;\n\t\tqp->qplib_qp.sq.psn = qp_attr->sq_psn;\n\t}\n\tif (qp_attr_mask & IB_QP_MAX_DEST_RD_ATOMIC) {\n\t\tif (qp_attr->max_dest_rd_atomic >\n\t\t    dev_attr->max_qp_init_rd_atom) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"max_dest_rd_atomic requested%d is > dev_max%d\",\n\t\t\t\tqp_attr->max_dest_rd_atomic,\n\t\t\t\tdev_attr->max_qp_init_rd_atom);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_MAX_DEST_RD_ATOMIC;\n\t\tqp->qplib_qp.max_dest_rd_atomic = qp_attr->max_dest_rd_atomic;\n\t}\n\tif (qp_attr_mask & IB_QP_CAP) {\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_SQ_SIZE |\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_RQ_SIZE |\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_SQ_SGE |\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_RQ_SGE |\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_MAX_INLINE_DATA;\n\t\tif ((qp_attr->cap.max_send_wr >= dev_attr->max_qp_wqes) ||\n\t\t    (qp_attr->cap.max_recv_wr >= dev_attr->max_qp_wqes) ||\n\t\t    (qp_attr->cap.max_send_sge >= dev_attr->max_qp_sges) ||\n\t\t    (qp_attr->cap.max_recv_sge >= dev_attr->max_qp_sges) ||\n\t\t    (qp_attr->cap.max_inline_data >=\n\t\t\t\t\t\tdev_attr->max_inline_data)) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Create QP failed - max exceeded\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tentries = roundup_pow_of_two(qp_attr->cap.max_send_wr);\n\t\tqp->qplib_qp.sq.max_wqe = min_t(u32, entries,\n\t\t\t\t\t\tdev_attr->max_qp_wqes + 1);\n\t\tqp->qplib_qp.sq.q_full_delta = qp->qplib_qp.sq.max_wqe -\n\t\t\t\t\t\tqp_attr->cap.max_send_wr;\n\t\t/*\n\t\t * Reserving one slot for Phantom WQE. Some application can\n\t\t * post one extra entry in this case. Allowing this to avoid\n\t\t * unexpected Queue full condition\n\t\t */\n\t\tqp->qplib_qp.sq.q_full_delta -= 1;\n\t\tqp->qplib_qp.sq.max_sge = qp_attr->cap.max_send_sge;\n\t\tif (qp->qplib_qp.rq.max_wqe) {\n\t\t\tentries = roundup_pow_of_two(qp_attr->cap.max_recv_wr);\n\t\t\tqp->qplib_qp.rq.max_wqe =\n\t\t\t\tmin_t(u32, entries, dev_attr->max_qp_wqes + 1);\n\t\t\tqp->qplib_qp.rq.q_full_delta = qp->qplib_qp.rq.max_wqe -\n\t\t\t\t\t\t       qp_attr->cap.max_recv_wr;\n\t\t\tqp->qplib_qp.rq.max_sge = qp_attr->cap.max_recv_sge;\n\t\t} else {\n\t\t\t/* SRQ was used prior, just ignore the RQ caps */\n\t\t}\n\t}\n\tif (qp_attr_mask & IB_QP_DEST_QPN) {\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_DEST_QP_ID;\n\t\tqp->qplib_qp.dest_qpn = qp_attr->dest_qp_num;\n\t}\n\trc = bnxt_qplib_modify_qp(&rdev->qplib_res, &qp->qplib_qp);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to modify HW QP\");\n\t\treturn rc;\n\t}\n\tif (ib_qp->qp_type == IB_QPT_GSI && rdev->qp1_sqp)\n\t\trc = bnxt_re_modify_shadow_qp(rdev, qp, qp_attr_mask);\n\treturn rc;\n}\n\nint bnxt_re_query_qp(struct ib_qp *ib_qp, struct ib_qp_attr *qp_attr,\n\t\t     int qp_attr_mask, struct ib_qp_init_attr *qp_init_attr)\n{\n\tstruct bnxt_re_qp *qp = container_of(ib_qp, struct bnxt_re_qp, ib_qp);\n\tstruct bnxt_re_dev *rdev = qp->rdev;\n\tstruct bnxt_qplib_qp *qplib_qp;\n\tint rc;\n\n\tqplib_qp = kzalloc(sizeof(*qplib_qp), GFP_KERNEL);\n\tif (!qplib_qp)\n\t\treturn -ENOMEM;\n\n\tqplib_qp->id = qp->qplib_qp.id;\n\tqplib_qp->ah.host_sgid_index = qp->qplib_qp.ah.host_sgid_index;\n\n\trc = bnxt_qplib_query_qp(&rdev->qplib_res, qplib_qp);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to query HW QP\");\n\t\tgoto out;\n\t}\n\tqp_attr->qp_state = __to_ib_qp_state(qplib_qp->state);\n\tqp_attr->en_sqd_async_notify = qplib_qp->en_sqd_async_notify ? 1 : 0;\n\tqp_attr->qp_access_flags = __to_ib_access_flags(qplib_qp->access);\n\tqp_attr->pkey_index = qplib_qp->pkey_index;\n\tqp_attr->qkey = qplib_qp->qkey;\n\tqp_attr->ah_attr.type = RDMA_AH_ATTR_TYPE_ROCE;\n\trdma_ah_set_grh(&qp_attr->ah_attr, NULL, qplib_qp->ah.flow_label,\n\t\t\tqplib_qp->ah.host_sgid_index,\n\t\t\tqplib_qp->ah.hop_limit,\n\t\t\tqplib_qp->ah.traffic_class);\n\trdma_ah_set_dgid_raw(&qp_attr->ah_attr, qplib_qp->ah.dgid.data);\n\trdma_ah_set_sl(&qp_attr->ah_attr, qplib_qp->ah.sl);\n\tether_addr_copy(qp_attr->ah_attr.roce.dmac, qplib_qp->ah.dmac);\n\tqp_attr->path_mtu = __to_ib_mtu(qplib_qp->path_mtu);\n\tqp_attr->timeout = qplib_qp->timeout;\n\tqp_attr->retry_cnt = qplib_qp->retry_cnt;\n\tqp_attr->rnr_retry = qplib_qp->rnr_retry;\n\tqp_attr->min_rnr_timer = qplib_qp->min_rnr_timer;\n\tqp_attr->rq_psn = qplib_qp->rq.psn;\n\tqp_attr->max_rd_atomic = qplib_qp->max_rd_atomic;\n\tqp_attr->sq_psn = qplib_qp->sq.psn;\n\tqp_attr->max_dest_rd_atomic = qplib_qp->max_dest_rd_atomic;\n\tqp_init_attr->sq_sig_type = qplib_qp->sig_type ? IB_SIGNAL_ALL_WR :\n\t\t\t\t\t\t\t IB_SIGNAL_REQ_WR;\n\tqp_attr->dest_qp_num = qplib_qp->dest_qpn;\n\n\tqp_attr->cap.max_send_wr = qp->qplib_qp.sq.max_wqe;\n\tqp_attr->cap.max_send_sge = qp->qplib_qp.sq.max_sge;\n\tqp_attr->cap.max_recv_wr = qp->qplib_qp.rq.max_wqe;\n\tqp_attr->cap.max_recv_sge = qp->qplib_qp.rq.max_sge;\n\tqp_attr->cap.max_inline_data = qp->qplib_qp.max_inline_data;\n\tqp_init_attr->cap = qp_attr->cap;\n\nout:\n\tkfree(qplib_qp);\n\treturn rc;\n}\n\n/* Routine for sending QP1 packets for RoCE V1 an V2\n */\nstatic int bnxt_re_build_qp1_send_v2(struct bnxt_re_qp *qp,\n\t\t\t\t     const struct ib_send_wr *wr,\n\t\t\t\t     struct bnxt_qplib_swqe *wqe,\n\t\t\t\t     int payload_size)\n{\n\tstruct bnxt_re_ah *ah = container_of(ud_wr(wr)->ah, struct bnxt_re_ah,\n\t\t\t\t\t     ib_ah);\n\tstruct bnxt_qplib_ah *qplib_ah = &ah->qplib_ah;\n\tconst struct ib_gid_attr *sgid_attr = ah->ib_ah.sgid_attr;\n\tstruct bnxt_qplib_sge sge;\n\tu8 nw_type;\n\tu16 ether_type;\n\tunion ib_gid dgid;\n\tbool is_eth = false;\n\tbool is_vlan = false;\n\tbool is_grh = false;\n\tbool is_udp = false;\n\tu8 ip_version = 0;\n\tu16 vlan_id = 0xFFFF;\n\tvoid *buf;\n\tint i, rc = 0;\n\n\tmemset(&qp->qp1_hdr, 0, sizeof(qp->qp1_hdr));\n\n\trc = rdma_read_gid_l2_fields(sgid_attr, &vlan_id, NULL);\n\tif (rc)\n\t\treturn rc;\n\n\t/* Get network header type for this GID */\n\tnw_type = rdma_gid_attr_network_type(sgid_attr);\n\tswitch (nw_type) {\n\tcase RDMA_NETWORK_IPV4:\n\t\tnw_type = BNXT_RE_ROCEV2_IPV4_PACKET;\n\t\tbreak;\n\tcase RDMA_NETWORK_IPV6:\n\t\tnw_type = BNXT_RE_ROCEV2_IPV6_PACKET;\n\t\tbreak;\n\tdefault:\n\t\tnw_type = BNXT_RE_ROCE_V1_PACKET;\n\t\tbreak;\n\t}\n\tmemcpy(&dgid.raw, &qplib_ah->dgid, 16);\n\tis_udp = sgid_attr->gid_type == IB_GID_TYPE_ROCE_UDP_ENCAP;\n\tif (is_udp) {\n\t\tif (ipv6_addr_v4mapped((struct in6_addr *)&sgid_attr->gid)) {\n\t\t\tip_version = 4;\n\t\t\tether_type = ETH_P_IP;\n\t\t} else {\n\t\t\tip_version = 6;\n\t\t\tether_type = ETH_P_IPV6;\n\t\t}\n\t\tis_grh = false;\n\t} else {\n\t\tether_type = ETH_P_IBOE;\n\t\tis_grh = true;\n\t}\n\n\tis_eth = true;\n\tis_vlan = (vlan_id && (vlan_id < 0x1000)) ? true : false;\n\n\tib_ud_header_init(payload_size, !is_eth, is_eth, is_vlan, is_grh,\n\t\t\t  ip_version, is_udp, 0, &qp->qp1_hdr);\n\n\t/* ETH */\n\tether_addr_copy(qp->qp1_hdr.eth.dmac_h, ah->qplib_ah.dmac);\n\tether_addr_copy(qp->qp1_hdr.eth.smac_h, qp->qplib_qp.smac);\n\n\t/* For vlan, check the sgid for vlan existence */\n\n\tif (!is_vlan) {\n\t\tqp->qp1_hdr.eth.type = cpu_to_be16(ether_type);\n\t} else {\n\t\tqp->qp1_hdr.vlan.type = cpu_to_be16(ether_type);\n\t\tqp->qp1_hdr.vlan.tag = cpu_to_be16(vlan_id);\n\t}\n\n\tif (is_grh || (ip_version == 6)) {\n\t\tmemcpy(qp->qp1_hdr.grh.source_gid.raw, sgid_attr->gid.raw,\n\t\t       sizeof(sgid_attr->gid));\n\t\tmemcpy(qp->qp1_hdr.grh.destination_gid.raw, qplib_ah->dgid.data,\n\t\t       sizeof(sgid_attr->gid));\n\t\tqp->qp1_hdr.grh.hop_limit     = qplib_ah->hop_limit;\n\t}\n\n\tif (ip_version == 4) {\n\t\tqp->qp1_hdr.ip4.tos = 0;\n\t\tqp->qp1_hdr.ip4.id = 0;\n\t\tqp->qp1_hdr.ip4.frag_off = htons(IP_DF);\n\t\tqp->qp1_hdr.ip4.ttl = qplib_ah->hop_limit;\n\n\t\tmemcpy(&qp->qp1_hdr.ip4.saddr, sgid_attr->gid.raw + 12, 4);\n\t\tmemcpy(&qp->qp1_hdr.ip4.daddr, qplib_ah->dgid.data + 12, 4);\n\t\tqp->qp1_hdr.ip4.check = ib_ud_ip4_csum(&qp->qp1_hdr);\n\t}\n\n\tif (is_udp) {\n\t\tqp->qp1_hdr.udp.dport = htons(ROCE_V2_UDP_DPORT);\n\t\tqp->qp1_hdr.udp.sport = htons(0x8CD1);\n\t\tqp->qp1_hdr.udp.csum = 0;\n\t}\n\n\t/* BTH */\n\tif (wr->opcode == IB_WR_SEND_WITH_IMM) {\n\t\tqp->qp1_hdr.bth.opcode = IB_OPCODE_UD_SEND_ONLY_WITH_IMMEDIATE;\n\t\tqp->qp1_hdr.immediate_present = 1;\n\t} else {\n\t\tqp->qp1_hdr.bth.opcode = IB_OPCODE_UD_SEND_ONLY;\n\t}\n\tif (wr->send_flags & IB_SEND_SOLICITED)\n\t\tqp->qp1_hdr.bth.solicited_event = 1;\n\t/* pad_count */\n\tqp->qp1_hdr.bth.pad_count = (4 - payload_size) & 3;\n\n\t/* P_key for QP1 is for all members */\n\tqp->qp1_hdr.bth.pkey = cpu_to_be16(0xFFFF);\n\tqp->qp1_hdr.bth.destination_qpn = IB_QP1;\n\tqp->qp1_hdr.bth.ack_req = 0;\n\tqp->send_psn++;\n\tqp->send_psn &= BTH_PSN_MASK;\n\tqp->qp1_hdr.bth.psn = cpu_to_be32(qp->send_psn);\n\t/* DETH */\n\t/* Use the priviledged Q_Key for QP1 */\n\tqp->qp1_hdr.deth.qkey = cpu_to_be32(IB_QP1_QKEY);\n\tqp->qp1_hdr.deth.source_qpn = IB_QP1;\n\n\t/* Pack the QP1 to the transmit buffer */\n\tbuf = bnxt_qplib_get_qp1_sq_buf(&qp->qplib_qp, &sge);\n\tif (buf) {\n\t\tib_ud_header_pack(&qp->qp1_hdr, buf);\n\t\tfor (i = wqe->num_sge; i; i--) {\n\t\t\twqe->sg_list[i].addr = wqe->sg_list[i - 1].addr;\n\t\t\twqe->sg_list[i].lkey = wqe->sg_list[i - 1].lkey;\n\t\t\twqe->sg_list[i].size = wqe->sg_list[i - 1].size;\n\t\t}\n\n\t\t/*\n\t\t * Max Header buf size for IPV6 RoCE V2 is 86,\n\t\t * which is same as the QP1 SQ header buffer.\n\t\t * Header buf size for IPV4 RoCE V2 can be 66.\n\t\t * ETH(14) + VLAN(4)+ IP(20) + UDP (8) + BTH(20).\n\t\t * Subtract 20 bytes from QP1 SQ header buf size\n\t\t */\n\t\tif (is_udp && ip_version == 4)\n\t\t\tsge.size -= 20;\n\t\t/*\n\t\t * Max Header buf size for RoCE V1 is 78.\n\t\t * ETH(14) + VLAN(4) + GRH(40) + BTH(20).\n\t\t * Subtract 8 bytes from QP1 SQ header buf size\n\t\t */\n\t\tif (!is_udp)\n\t\t\tsge.size -= 8;\n\n\t\t/* Subtract 4 bytes for non vlan packets */\n\t\tif (!is_vlan)\n\t\t\tsge.size -= 4;\n\n\t\twqe->sg_list[0].addr = sge.addr;\n\t\twqe->sg_list[0].lkey = sge.lkey;\n\t\twqe->sg_list[0].size = sge.size;\n\t\twqe->num_sge++;\n\n\t} else {\n\t\tdev_err(rdev_to_dev(qp->rdev), \"QP1 buffer is empty!\");\n\t\trc = -ENOMEM;\n\t}\n\treturn rc;\n}\n\n/* For the MAD layer, it only provides the recv SGE the size of\n * ib_grh + MAD datagram.  No Ethernet headers, Ethertype, BTH, DETH,\n * nor RoCE iCRC.  The Cu+ solution must provide buffer for the entire\n * receive packet (334 bytes) with no VLAN and then copy the GRH\n * and the MAD datagram out to the provided SGE.\n */\nstatic int bnxt_re_build_qp1_shadow_qp_recv(struct bnxt_re_qp *qp,\n\t\t\t\t\t    const struct ib_recv_wr *wr,\n\t\t\t\t\t    struct bnxt_qplib_swqe *wqe,\n\t\t\t\t\t    int payload_size)\n{\n\tstruct bnxt_qplib_sge ref, sge;\n\tu32 rq_prod_index;\n\tstruct bnxt_re_sqp_entries *sqp_entry;\n\n\trq_prod_index = bnxt_qplib_get_rq_prod_index(&qp->qplib_qp);\n\n\tif (!bnxt_qplib_get_qp1_rq_buf(&qp->qplib_qp, &sge))\n\t\treturn -ENOMEM;\n\n\t/* Create 1 SGE to receive the entire\n\t * ethernet packet\n\t */\n\t/* Save the reference from ULP */\n\tref.addr = wqe->sg_list[0].addr;\n\tref.lkey = wqe->sg_list[0].lkey;\n\tref.size = wqe->sg_list[0].size;\n\n\tsqp_entry = &qp->rdev->sqp_tbl[rq_prod_index];\n\n\t/* SGE 1 */\n\twqe->sg_list[0].addr = sge.addr;\n\twqe->sg_list[0].lkey = sge.lkey;\n\twqe->sg_list[0].size = BNXT_QPLIB_MAX_QP1_RQ_HDR_SIZE_V2;\n\tsge.size -= wqe->sg_list[0].size;\n\n\tsqp_entry->sge.addr = ref.addr;\n\tsqp_entry->sge.lkey = ref.lkey;\n\tsqp_entry->sge.size = ref.size;\n\t/* Store the wrid for reporting completion */\n\tsqp_entry->wrid = wqe->wr_id;\n\t/* change the wqe->wrid to table index */\n\twqe->wr_id = rq_prod_index;\n\treturn 0;\n}\n\nstatic int is_ud_qp(struct bnxt_re_qp *qp)\n{\n\treturn (qp->qplib_qp.type == CMDQ_CREATE_QP_TYPE_UD ||\n\t\tqp->qplib_qp.type == CMDQ_CREATE_QP_TYPE_GSI);\n}\n\nstatic int bnxt_re_build_send_wqe(struct bnxt_re_qp *qp,\n\t\t\t\t  const struct ib_send_wr *wr,\n\t\t\t\t  struct bnxt_qplib_swqe *wqe)\n{\n\tstruct bnxt_re_ah *ah = NULL;\n\n\tif (is_ud_qp(qp)) {\n\t\tah = container_of(ud_wr(wr)->ah, struct bnxt_re_ah, ib_ah);\n\t\twqe->send.q_key = ud_wr(wr)->remote_qkey;\n\t\twqe->send.dst_qp = ud_wr(wr)->remote_qpn;\n\t\twqe->send.avid = ah->qplib_ah.id;\n\t}\n\tswitch (wr->opcode) {\n\tcase IB_WR_SEND:\n\t\twqe->type = BNXT_QPLIB_SWQE_TYPE_SEND;\n\t\tbreak;\n\tcase IB_WR_SEND_WITH_IMM:\n\t\twqe->type = BNXT_QPLIB_SWQE_TYPE_SEND_WITH_IMM;\n\t\twqe->send.imm_data = wr->ex.imm_data;\n\t\tbreak;\n\tcase IB_WR_SEND_WITH_INV:\n\t\twqe->type = BNXT_QPLIB_SWQE_TYPE_SEND_WITH_INV;\n\t\twqe->send.inv_key = wr->ex.invalidate_rkey;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\tif (wr->send_flags & IB_SEND_SIGNALED)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SIGNAL_COMP;\n\tif (wr->send_flags & IB_SEND_FENCE)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_UC_FENCE;\n\tif (wr->send_flags & IB_SEND_SOLICITED)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SOLICIT_EVENT;\n\tif (wr->send_flags & IB_SEND_INLINE)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_INLINE;\n\n\treturn 0;\n}\n\nstatic int bnxt_re_build_rdma_wqe(const struct ib_send_wr *wr,\n\t\t\t\t  struct bnxt_qplib_swqe *wqe)\n{\n\tswitch (wr->opcode) {\n\tcase IB_WR_RDMA_WRITE:\n\t\twqe->type = BNXT_QPLIB_SWQE_TYPE_RDMA_WRITE;\n\t\tbreak;\n\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\twqe->type = BNXT_QPLIB_SWQE_TYPE_RDMA_WRITE_WITH_IMM;\n\t\twqe->rdma.imm_data = wr->ex.imm_data;\n\t\tbreak;\n\tcase IB_WR_RDMA_READ:\n\t\twqe->type = BNXT_QPLIB_SWQE_TYPE_RDMA_READ;\n\t\twqe->rdma.inv_key = wr->ex.invalidate_rkey;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\twqe->rdma.remote_va = rdma_wr(wr)->remote_addr;\n\twqe->rdma.r_key = rdma_wr(wr)->rkey;\n\tif (wr->send_flags & IB_SEND_SIGNALED)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SIGNAL_COMP;\n\tif (wr->send_flags & IB_SEND_FENCE)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_UC_FENCE;\n\tif (wr->send_flags & IB_SEND_SOLICITED)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SOLICIT_EVENT;\n\tif (wr->send_flags & IB_SEND_INLINE)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_INLINE;\n\n\treturn 0;\n}\n\nstatic int bnxt_re_build_atomic_wqe(const struct ib_send_wr *wr,\n\t\t\t\t    struct bnxt_qplib_swqe *wqe)\n{\n\tswitch (wr->opcode) {\n\tcase IB_WR_ATOMIC_CMP_AND_SWP:\n\t\twqe->type = BNXT_QPLIB_SWQE_TYPE_ATOMIC_CMP_AND_SWP;\n\t\twqe->atomic.cmp_data = atomic_wr(wr)->compare_add;\n\t\twqe->atomic.swap_data = atomic_wr(wr)->swap;\n\t\tbreak;\n\tcase IB_WR_ATOMIC_FETCH_AND_ADD:\n\t\twqe->type = BNXT_QPLIB_SWQE_TYPE_ATOMIC_FETCH_AND_ADD;\n\t\twqe->atomic.cmp_data = atomic_wr(wr)->compare_add;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\twqe->atomic.remote_va = atomic_wr(wr)->remote_addr;\n\twqe->atomic.r_key = atomic_wr(wr)->rkey;\n\tif (wr->send_flags & IB_SEND_SIGNALED)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SIGNAL_COMP;\n\tif (wr->send_flags & IB_SEND_FENCE)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_UC_FENCE;\n\tif (wr->send_flags & IB_SEND_SOLICITED)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SOLICIT_EVENT;\n\treturn 0;\n}\n\nstatic int bnxt_re_build_inv_wqe(const struct ib_send_wr *wr,\n\t\t\t\t struct bnxt_qplib_swqe *wqe)\n{\n\twqe->type = BNXT_QPLIB_SWQE_TYPE_LOCAL_INV;\n\twqe->local_inv.inv_l_key = wr->ex.invalidate_rkey;\n\n\t/* Need unconditional fence for local invalidate\n\t * opcode to work as expected.\n\t */\n\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_UC_FENCE;\n\n\tif (wr->send_flags & IB_SEND_SIGNALED)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SIGNAL_COMP;\n\tif (wr->send_flags & IB_SEND_SOLICITED)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SOLICIT_EVENT;\n\n\treturn 0;\n}\n\nstatic int bnxt_re_build_reg_wqe(const struct ib_reg_wr *wr,\n\t\t\t\t struct bnxt_qplib_swqe *wqe)\n{\n\tstruct bnxt_re_mr *mr = container_of(wr->mr, struct bnxt_re_mr, ib_mr);\n\tstruct bnxt_qplib_frpl *qplib_frpl = &mr->qplib_frpl;\n\tint access = wr->access;\n\n\twqe->frmr.pbl_ptr = (__le64 *)qplib_frpl->hwq.pbl_ptr[0];\n\twqe->frmr.pbl_dma_ptr = qplib_frpl->hwq.pbl_dma_ptr[0];\n\twqe->frmr.page_list = mr->pages;\n\twqe->frmr.page_list_len = mr->npages;\n\twqe->frmr.levels = qplib_frpl->hwq.level + 1;\n\twqe->type = BNXT_QPLIB_SWQE_TYPE_REG_MR;\n\n\t/* Need unconditional fence for reg_mr\n\t * opcode to function as expected.\n\t */\n\n\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_UC_FENCE;\n\n\tif (wr->wr.send_flags & IB_SEND_SIGNALED)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SIGNAL_COMP;\n\n\tif (access & IB_ACCESS_LOCAL_WRITE)\n\t\twqe->frmr.access_cntl |= SQ_FR_PMR_ACCESS_CNTL_LOCAL_WRITE;\n\tif (access & IB_ACCESS_REMOTE_READ)\n\t\twqe->frmr.access_cntl |= SQ_FR_PMR_ACCESS_CNTL_REMOTE_READ;\n\tif (access & IB_ACCESS_REMOTE_WRITE)\n\t\twqe->frmr.access_cntl |= SQ_FR_PMR_ACCESS_CNTL_REMOTE_WRITE;\n\tif (access & IB_ACCESS_REMOTE_ATOMIC)\n\t\twqe->frmr.access_cntl |= SQ_FR_PMR_ACCESS_CNTL_REMOTE_ATOMIC;\n\tif (access & IB_ACCESS_MW_BIND)\n\t\twqe->frmr.access_cntl |= SQ_FR_PMR_ACCESS_CNTL_WINDOW_BIND;\n\n\twqe->frmr.l_key = wr->key;\n\twqe->frmr.length = wr->mr->length;\n\twqe->frmr.pbl_pg_sz_log = (wr->mr->page_size >> PAGE_SHIFT_4K) - 1;\n\twqe->frmr.va = wr->mr->iova;\n\treturn 0;\n}\n\nstatic int bnxt_re_copy_inline_data(struct bnxt_re_dev *rdev,\n\t\t\t\t    const struct ib_send_wr *wr,\n\t\t\t\t    struct bnxt_qplib_swqe *wqe)\n{\n\t/*  Copy the inline data to the data  field */\n\tu8 *in_data;\n\tu32 i, sge_len;\n\tvoid *sge_addr;\n\n\tin_data = wqe->inline_data;\n\tfor (i = 0; i < wr->num_sge; i++) {\n\t\tsge_addr = (void *)(unsigned long)\n\t\t\t\twr->sg_list[i].addr;\n\t\tsge_len = wr->sg_list[i].length;\n\n\t\tif ((sge_len + wqe->inline_len) >\n\t\t    BNXT_QPLIB_SWQE_MAX_INLINE_LENGTH) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Inline data size requested > supported value\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tsge_len = wr->sg_list[i].length;\n\n\t\tmemcpy(in_data, sge_addr, sge_len);\n\t\tin_data += wr->sg_list[i].length;\n\t\twqe->inline_len += wr->sg_list[i].length;\n\t}\n\treturn wqe->inline_len;\n}\n\nstatic int bnxt_re_copy_wr_payload(struct bnxt_re_dev *rdev,\n\t\t\t\t   const struct ib_send_wr *wr,\n\t\t\t\t   struct bnxt_qplib_swqe *wqe)\n{\n\tint payload_sz = 0;\n\n\tif (wr->send_flags & IB_SEND_INLINE)\n\t\tpayload_sz = bnxt_re_copy_inline_data(rdev, wr, wqe);\n\telse\n\t\tpayload_sz = bnxt_re_build_sgl(wr->sg_list, wqe->sg_list,\n\t\t\t\t\t       wqe->num_sge);\n\n\treturn payload_sz;\n}\n\nstatic void bnxt_ud_qp_hw_stall_workaround(struct bnxt_re_qp *qp)\n{\n\tif ((qp->ib_qp.qp_type == IB_QPT_UD ||\n\t     qp->ib_qp.qp_type == IB_QPT_GSI ||\n\t     qp->ib_qp.qp_type == IB_QPT_RAW_ETHERTYPE) &&\n\t     qp->qplib_qp.wqe_cnt == BNXT_RE_UD_QP_HW_STALL) {\n\t\tint qp_attr_mask;\n\t\tstruct ib_qp_attr qp_attr;\n\n\t\tqp_attr_mask = IB_QP_STATE;\n\t\tqp_attr.qp_state = IB_QPS_RTS;\n\t\tbnxt_re_modify_qp(&qp->ib_qp, &qp_attr, qp_attr_mask, NULL);\n\t\tqp->qplib_qp.wqe_cnt = 0;\n\t}\n}\n\nstatic int bnxt_re_post_send_shadow_qp(struct bnxt_re_dev *rdev,\n\t\t\t\t       struct bnxt_re_qp *qp,\n\t\t\t\t       const struct ib_send_wr *wr)\n{\n\tstruct bnxt_qplib_swqe wqe;\n\tint rc = 0, payload_sz = 0;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->sq_lock, flags);\n\tmemset(&wqe, 0, sizeof(wqe));\n\twhile (wr) {\n\t\t/* House keeping */\n\t\tmemset(&wqe, 0, sizeof(wqe));\n\n\t\t/* Common */\n\t\twqe.num_sge = wr->num_sge;\n\t\tif (wr->num_sge > qp->qplib_qp.sq.max_sge) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Limit exceeded for Send SGEs\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto bad;\n\t\t}\n\n\t\tpayload_sz = bnxt_re_copy_wr_payload(qp->rdev, wr, &wqe);\n\t\tif (payload_sz < 0) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto bad;\n\t\t}\n\t\twqe.wr_id = wr->wr_id;\n\n\t\twqe.type = BNXT_QPLIB_SWQE_TYPE_SEND;\n\n\t\trc = bnxt_re_build_send_wqe(qp, wr, &wqe);\n\t\tif (!rc)\n\t\t\trc = bnxt_qplib_post_send(&qp->qplib_qp, &wqe);\nbad:\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Post send failed opcode = %#x rc = %d\",\n\t\t\t\twr->opcode, rc);\n\t\t\tbreak;\n\t\t}\n\t\twr = wr->next;\n\t}\n\tbnxt_qplib_post_send_db(&qp->qplib_qp);\n\tbnxt_ud_qp_hw_stall_workaround(qp);\n\tspin_unlock_irqrestore(&qp->sq_lock, flags);\n\treturn rc;\n}\n\nint bnxt_re_post_send(struct ib_qp *ib_qp, const struct ib_send_wr *wr,\n\t\t      const struct ib_send_wr **bad_wr)\n{\n\tstruct bnxt_re_qp *qp = container_of(ib_qp, struct bnxt_re_qp, ib_qp);\n\tstruct bnxt_qplib_swqe wqe;\n\tint rc = 0, payload_sz = 0;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->sq_lock, flags);\n\twhile (wr) {\n\t\t/* House keeping */\n\t\tmemset(&wqe, 0, sizeof(wqe));\n\n\t\t/* Common */\n\t\twqe.num_sge = wr->num_sge;\n\t\tif (wr->num_sge > qp->qplib_qp.sq.max_sge) {\n\t\t\tdev_err(rdev_to_dev(qp->rdev),\n\t\t\t\t\"Limit exceeded for Send SGEs\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto bad;\n\t\t}\n\n\t\tpayload_sz = bnxt_re_copy_wr_payload(qp->rdev, wr, &wqe);\n\t\tif (payload_sz < 0) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto bad;\n\t\t}\n\t\twqe.wr_id = wr->wr_id;\n\n\t\tswitch (wr->opcode) {\n\t\tcase IB_WR_SEND:\n\t\tcase IB_WR_SEND_WITH_IMM:\n\t\t\tif (qp->qplib_qp.type == CMDQ_CREATE_QP1_TYPE_GSI) {\n\t\t\t\trc = bnxt_re_build_qp1_send_v2(qp, wr, &wqe,\n\t\t\t\t\t\t\t       payload_sz);\n\t\t\t\tif (rc)\n\t\t\t\t\tgoto bad;\n\t\t\t\twqe.rawqp1.lflags |=\n\t\t\t\t\tSQ_SEND_RAWETH_QP1_LFLAGS_ROCE_CRC;\n\t\t\t}\n\t\t\tswitch (wr->send_flags) {\n\t\t\tcase IB_SEND_IP_CSUM:\n\t\t\t\twqe.rawqp1.lflags |=\n\t\t\t\t\tSQ_SEND_RAWETH_QP1_LFLAGS_IP_CHKSUM;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* fall through */\n\t\tcase IB_WR_SEND_WITH_INV:\n\t\t\trc = bnxt_re_build_send_wqe(qp, wr, &wqe);\n\t\t\tbreak;\n\t\tcase IB_WR_RDMA_WRITE:\n\t\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\tcase IB_WR_RDMA_READ:\n\t\t\trc = bnxt_re_build_rdma_wqe(wr, &wqe);\n\t\t\tbreak;\n\t\tcase IB_WR_ATOMIC_CMP_AND_SWP:\n\t\tcase IB_WR_ATOMIC_FETCH_AND_ADD:\n\t\t\trc = bnxt_re_build_atomic_wqe(wr, &wqe);\n\t\t\tbreak;\n\t\tcase IB_WR_RDMA_READ_WITH_INV:\n\t\t\tdev_err(rdev_to_dev(qp->rdev),\n\t\t\t\t\"RDMA Read with Invalidate is not supported\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto bad;\n\t\tcase IB_WR_LOCAL_INV:\n\t\t\trc = bnxt_re_build_inv_wqe(wr, &wqe);\n\t\t\tbreak;\n\t\tcase IB_WR_REG_MR:\n\t\t\trc = bnxt_re_build_reg_wqe(reg_wr(wr), &wqe);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t/* Unsupported WRs */\n\t\t\tdev_err(rdev_to_dev(qp->rdev),\n\t\t\t\t\"WR (%#x) is not supported\", wr->opcode);\n\t\t\trc = -EINVAL;\n\t\t\tgoto bad;\n\t\t}\n\t\tif (!rc)\n\t\t\trc = bnxt_qplib_post_send(&qp->qplib_qp, &wqe);\nbad:\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(qp->rdev),\n\t\t\t\t\"post_send failed op:%#x qps = %#x rc = %d\\n\",\n\t\t\t\twr->opcode, qp->qplib_qp.state, rc);\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\twr = wr->next;\n\t}\n\tbnxt_qplib_post_send_db(&qp->qplib_qp);\n\tbnxt_ud_qp_hw_stall_workaround(qp);\n\tspin_unlock_irqrestore(&qp->sq_lock, flags);\n\n\treturn rc;\n}\n\nstatic int bnxt_re_post_recv_shadow_qp(struct bnxt_re_dev *rdev,\n\t\t\t\t       struct bnxt_re_qp *qp,\n\t\t\t\t       const struct ib_recv_wr *wr)\n{\n\tstruct bnxt_qplib_swqe wqe;\n\tint rc = 0;\n\n\tmemset(&wqe, 0, sizeof(wqe));\n\twhile (wr) {\n\t\t/* House keeping */\n\t\tmemset(&wqe, 0, sizeof(wqe));\n\n\t\t/* Common */\n\t\twqe.num_sge = wr->num_sge;\n\t\tif (wr->num_sge > qp->qplib_qp.rq.max_sge) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Limit exceeded for Receive SGEs\");\n\t\t\trc = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tbnxt_re_build_sgl(wr->sg_list, wqe.sg_list, wr->num_sge);\n\t\twqe.wr_id = wr->wr_id;\n\t\twqe.type = BNXT_QPLIB_SWQE_TYPE_RECV;\n\n\t\trc = bnxt_qplib_post_recv(&qp->qplib_qp, &wqe);\n\t\tif (rc)\n\t\t\tbreak;\n\n\t\twr = wr->next;\n\t}\n\tif (!rc)\n\t\tbnxt_qplib_post_recv_db(&qp->qplib_qp);\n\treturn rc;\n}\n\nint bnxt_re_post_recv(struct ib_qp *ib_qp, const struct ib_recv_wr *wr,\n\t\t      const struct ib_recv_wr **bad_wr)\n{\n\tstruct bnxt_re_qp *qp = container_of(ib_qp, struct bnxt_re_qp, ib_qp);\n\tstruct bnxt_qplib_swqe wqe;\n\tint rc = 0, payload_sz = 0;\n\tunsigned long flags;\n\tu32 count = 0;\n\n\tspin_lock_irqsave(&qp->rq_lock, flags);\n\twhile (wr) {\n\t\t/* House keeping */\n\t\tmemset(&wqe, 0, sizeof(wqe));\n\n\t\t/* Common */\n\t\twqe.num_sge = wr->num_sge;\n\t\tif (wr->num_sge > qp->qplib_qp.rq.max_sge) {\n\t\t\tdev_err(rdev_to_dev(qp->rdev),\n\t\t\t\t\"Limit exceeded for Receive SGEs\");\n\t\t\trc = -EINVAL;\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\n\t\tpayload_sz = bnxt_re_build_sgl(wr->sg_list, wqe.sg_list,\n\t\t\t\t\t       wr->num_sge);\n\t\twqe.wr_id = wr->wr_id;\n\t\twqe.type = BNXT_QPLIB_SWQE_TYPE_RECV;\n\n\t\tif (ib_qp->qp_type == IB_QPT_GSI &&\n\t\t    qp->qplib_qp.type != CMDQ_CREATE_QP_TYPE_GSI)\n\t\t\trc = bnxt_re_build_qp1_shadow_qp_recv(qp, wr, &wqe,\n\t\t\t\t\t\t\t      payload_sz);\n\t\tif (!rc)\n\t\t\trc = bnxt_qplib_post_recv(&qp->qplib_qp, &wqe);\n\t\tif (rc) {\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Ring DB if the RQEs posted reaches a threshold value */\n\t\tif (++count >= BNXT_RE_RQ_WQE_THRESHOLD) {\n\t\t\tbnxt_qplib_post_recv_db(&qp->qplib_qp);\n\t\t\tcount = 0;\n\t\t}\n\n\t\twr = wr->next;\n\t}\n\n\tif (count)\n\t\tbnxt_qplib_post_recv_db(&qp->qplib_qp);\n\n\tspin_unlock_irqrestore(&qp->rq_lock, flags);\n\n\treturn rc;\n}\n\n/* Completion Queues */\nvoid bnxt_re_destroy_cq(struct ib_cq *ib_cq, struct ib_udata *udata)\n{\n\tstruct bnxt_re_cq *cq;\n\tstruct bnxt_qplib_nq *nq;\n\tstruct bnxt_re_dev *rdev;\n\n\tcq = container_of(ib_cq, struct bnxt_re_cq, ib_cq);\n\trdev = cq->rdev;\n\tnq = cq->qplib_cq.nq;\n\n\tbnxt_qplib_destroy_cq(&rdev->qplib_res, &cq->qplib_cq);\n\tib_umem_release(cq->umem);\n\n\tatomic_dec(&rdev->cq_count);\n\tnq->budget--;\n\tkfree(cq->cql);\n}\n\nint bnxt_re_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,\n\t\t      struct ib_udata *udata)\n{\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(ibcq->device, ibdev);\n\tstruct bnxt_qplib_dev_attr *dev_attr = &rdev->dev_attr;\n\tstruct bnxt_re_cq *cq = container_of(ibcq, struct bnxt_re_cq, ib_cq);\n\tint rc, entries;\n\tint cqe = attr->cqe;\n\tstruct bnxt_qplib_nq *nq = NULL;\n\tunsigned int nq_alloc_cnt;\n\n\t/* Validate CQ fields */\n\tif (cqe < 1 || cqe > dev_attr->max_cq_wqes) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to create CQ -max exceeded\");\n\t\treturn -EINVAL;\n\t}\n\n\tcq->rdev = rdev;\n\tcq->qplib_cq.cq_handle = (u64)(unsigned long)(&cq->qplib_cq);\n\n\tentries = roundup_pow_of_two(cqe + 1);\n\tif (entries > dev_attr->max_cq_wqes + 1)\n\t\tentries = dev_attr->max_cq_wqes + 1;\n\n\tif (udata) {\n\t\tstruct bnxt_re_cq_req req;\n\t\tstruct bnxt_re_ucontext *uctx = rdma_udata_to_drv_context(\n\t\t\tudata, struct bnxt_re_ucontext, ib_uctx);\n\t\tif (ib_copy_from_udata(&req, udata, sizeof(req))) {\n\t\t\trc = -EFAULT;\n\t\t\tgoto fail;\n\t\t}\n\n\t\tcq->umem = ib_umem_get(udata, req.cq_va,\n\t\t\t\t       entries * sizeof(struct cq_base),\n\t\t\t\t       IB_ACCESS_LOCAL_WRITE, 1);\n\t\tif (IS_ERR(cq->umem)) {\n\t\t\trc = PTR_ERR(cq->umem);\n\t\t\tgoto fail;\n\t\t}\n\t\tcq->qplib_cq.sg_info.sglist = cq->umem->sg_head.sgl;\n\t\tcq->qplib_cq.sg_info.npages = ib_umem_num_pages(cq->umem);\n\t\tcq->qplib_cq.sg_info.nmap = cq->umem->nmap;\n\t\tcq->qplib_cq.dpi = &uctx->dpi;\n\t} else {\n\t\tcq->max_cql = min_t(u32, entries, MAX_CQL_PER_POLL);\n\t\tcq->cql = kcalloc(cq->max_cql, sizeof(struct bnxt_qplib_cqe),\n\t\t\t\t  GFP_KERNEL);\n\t\tif (!cq->cql) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto fail;\n\t\t}\n\n\t\tcq->qplib_cq.dpi = &rdev->dpi_privileged;\n\t}\n\t/*\n\t * Allocating the NQ in a round robin fashion. nq_alloc_cnt is a\n\t * used for getting the NQ index.\n\t */\n\tnq_alloc_cnt = atomic_inc_return(&rdev->nq_alloc_cnt);\n\tnq = &rdev->nq[nq_alloc_cnt % (rdev->num_msix - 1)];\n\tcq->qplib_cq.max_wqe = entries;\n\tcq->qplib_cq.cnq_hw_ring_id = nq->ring_id;\n\tcq->qplib_cq.nq\t= nq;\n\n\trc = bnxt_qplib_create_cq(&rdev->qplib_res, &cq->qplib_cq);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to create HW CQ\");\n\t\tgoto fail;\n\t}\n\n\tcq->ib_cq.cqe = entries;\n\tcq->cq_period = cq->qplib_cq.period;\n\tnq->budget++;\n\n\tatomic_inc(&rdev->cq_count);\n\tspin_lock_init(&cq->cq_lock);\n\n\tif (udata) {\n\t\tstruct bnxt_re_cq_resp resp;\n\n\t\tresp.cqid = cq->qplib_cq.id;\n\t\tresp.tail = cq->qplib_cq.hwq.cons;\n\t\tresp.phase = cq->qplib_cq.period;\n\t\tresp.rsvd = 0;\n\t\trc = ib_copy_to_udata(udata, &resp, sizeof(resp));\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"Failed to copy CQ udata\");\n\t\t\tbnxt_qplib_destroy_cq(&rdev->qplib_res, &cq->qplib_cq);\n\t\t\tgoto c2fail;\n\t\t}\n\t}\n\n\treturn 0;\n\nc2fail:\n\tib_umem_release(cq->umem);\nfail:\n\tkfree(cq->cql);\n\treturn rc;\n}\n\nstatic u8 __req_to_ib_wc_status(u8 qstatus)\n{\n\tswitch (qstatus) {\n\tcase CQ_REQ_STATUS_OK:\n\t\treturn IB_WC_SUCCESS;\n\tcase CQ_REQ_STATUS_BAD_RESPONSE_ERR:\n\t\treturn IB_WC_BAD_RESP_ERR;\n\tcase CQ_REQ_STATUS_LOCAL_LENGTH_ERR:\n\t\treturn IB_WC_LOC_LEN_ERR;\n\tcase CQ_REQ_STATUS_LOCAL_QP_OPERATION_ERR:\n\t\treturn IB_WC_LOC_QP_OP_ERR;\n\tcase CQ_REQ_STATUS_LOCAL_PROTECTION_ERR:\n\t\treturn IB_WC_LOC_PROT_ERR;\n\tcase CQ_REQ_STATUS_MEMORY_MGT_OPERATION_ERR:\n\t\treturn IB_WC_GENERAL_ERR;\n\tcase CQ_REQ_STATUS_REMOTE_INVALID_REQUEST_ERR:\n\t\treturn IB_WC_REM_INV_REQ_ERR;\n\tcase CQ_REQ_STATUS_REMOTE_ACCESS_ERR:\n\t\treturn IB_WC_REM_ACCESS_ERR;\n\tcase CQ_REQ_STATUS_REMOTE_OPERATION_ERR:\n\t\treturn IB_WC_REM_OP_ERR;\n\tcase CQ_REQ_STATUS_RNR_NAK_RETRY_CNT_ERR:\n\t\treturn IB_WC_RNR_RETRY_EXC_ERR;\n\tcase CQ_REQ_STATUS_TRANSPORT_RETRY_CNT_ERR:\n\t\treturn IB_WC_RETRY_EXC_ERR;\n\tcase CQ_REQ_STATUS_WORK_REQUEST_FLUSHED_ERR:\n\t\treturn IB_WC_WR_FLUSH_ERR;\n\tdefault:\n\t\treturn IB_WC_GENERAL_ERR;\n\t}\n\treturn 0;\n}\n\nstatic u8 __rawqp1_to_ib_wc_status(u8 qstatus)\n{\n\tswitch (qstatus) {\n\tcase CQ_RES_RAWETH_QP1_STATUS_OK:\n\t\treturn IB_WC_SUCCESS;\n\tcase CQ_RES_RAWETH_QP1_STATUS_LOCAL_ACCESS_ERROR:\n\t\treturn IB_WC_LOC_ACCESS_ERR;\n\tcase CQ_RES_RAWETH_QP1_STATUS_HW_LOCAL_LENGTH_ERR:\n\t\treturn IB_WC_LOC_LEN_ERR;\n\tcase CQ_RES_RAWETH_QP1_STATUS_LOCAL_PROTECTION_ERR:\n\t\treturn IB_WC_LOC_PROT_ERR;\n\tcase CQ_RES_RAWETH_QP1_STATUS_LOCAL_QP_OPERATION_ERR:\n\t\treturn IB_WC_LOC_QP_OP_ERR;\n\tcase CQ_RES_RAWETH_QP1_STATUS_MEMORY_MGT_OPERATION_ERR:\n\t\treturn IB_WC_GENERAL_ERR;\n\tcase CQ_RES_RAWETH_QP1_STATUS_WORK_REQUEST_FLUSHED_ERR:\n\t\treturn IB_WC_WR_FLUSH_ERR;\n\tcase CQ_RES_RAWETH_QP1_STATUS_HW_FLUSH_ERR:\n\t\treturn IB_WC_WR_FLUSH_ERR;\n\tdefault:\n\t\treturn IB_WC_GENERAL_ERR;\n\t}\n}\n\nstatic u8 __rc_to_ib_wc_status(u8 qstatus)\n{\n\tswitch (qstatus) {\n\tcase CQ_RES_RC_STATUS_OK:\n\t\treturn IB_WC_SUCCESS;\n\tcase CQ_RES_RC_STATUS_LOCAL_ACCESS_ERROR:\n\t\treturn IB_WC_LOC_ACCESS_ERR;\n\tcase CQ_RES_RC_STATUS_LOCAL_LENGTH_ERR:\n\t\treturn IB_WC_LOC_LEN_ERR;\n\tcase CQ_RES_RC_STATUS_LOCAL_PROTECTION_ERR:\n\t\treturn IB_WC_LOC_PROT_ERR;\n\tcase CQ_RES_RC_STATUS_LOCAL_QP_OPERATION_ERR:\n\t\treturn IB_WC_LOC_QP_OP_ERR;\n\tcase CQ_RES_RC_STATUS_MEMORY_MGT_OPERATION_ERR:\n\t\treturn IB_WC_GENERAL_ERR;\n\tcase CQ_RES_RC_STATUS_REMOTE_INVALID_REQUEST_ERR:\n\t\treturn IB_WC_REM_INV_REQ_ERR;\n\tcase CQ_RES_RC_STATUS_WORK_REQUEST_FLUSHED_ERR:\n\t\treturn IB_WC_WR_FLUSH_ERR;\n\tcase CQ_RES_RC_STATUS_HW_FLUSH_ERR:\n\t\treturn IB_WC_WR_FLUSH_ERR;\n\tdefault:\n\t\treturn IB_WC_GENERAL_ERR;\n\t}\n}\n\nstatic void bnxt_re_process_req_wc(struct ib_wc *wc, struct bnxt_qplib_cqe *cqe)\n{\n\tswitch (cqe->type) {\n\tcase BNXT_QPLIB_SWQE_TYPE_SEND:\n\t\twc->opcode = IB_WC_SEND;\n\t\tbreak;\n\tcase BNXT_QPLIB_SWQE_TYPE_SEND_WITH_IMM:\n\t\twc->opcode = IB_WC_SEND;\n\t\twc->wc_flags |= IB_WC_WITH_IMM;\n\t\tbreak;\n\tcase BNXT_QPLIB_SWQE_TYPE_SEND_WITH_INV:\n\t\twc->opcode = IB_WC_SEND;\n\t\twc->wc_flags |= IB_WC_WITH_INVALIDATE;\n\t\tbreak;\n\tcase BNXT_QPLIB_SWQE_TYPE_RDMA_WRITE:\n\t\twc->opcode = IB_WC_RDMA_WRITE;\n\t\tbreak;\n\tcase BNXT_QPLIB_SWQE_TYPE_RDMA_WRITE_WITH_IMM:\n\t\twc->opcode = IB_WC_RDMA_WRITE;\n\t\twc->wc_flags |= IB_WC_WITH_IMM;\n\t\tbreak;\n\tcase BNXT_QPLIB_SWQE_TYPE_RDMA_READ:\n\t\twc->opcode = IB_WC_RDMA_READ;\n\t\tbreak;\n\tcase BNXT_QPLIB_SWQE_TYPE_ATOMIC_CMP_AND_SWP:\n\t\twc->opcode = IB_WC_COMP_SWAP;\n\t\tbreak;\n\tcase BNXT_QPLIB_SWQE_TYPE_ATOMIC_FETCH_AND_ADD:\n\t\twc->opcode = IB_WC_FETCH_ADD;\n\t\tbreak;\n\tcase BNXT_QPLIB_SWQE_TYPE_LOCAL_INV:\n\t\twc->opcode = IB_WC_LOCAL_INV;\n\t\tbreak;\n\tcase BNXT_QPLIB_SWQE_TYPE_REG_MR:\n\t\twc->opcode = IB_WC_REG_MR;\n\t\tbreak;\n\tdefault:\n\t\twc->opcode = IB_WC_SEND;\n\t\tbreak;\n\t}\n\n\twc->status = __req_to_ib_wc_status(cqe->status);\n}\n\nstatic int bnxt_re_check_packet_type(u16 raweth_qp1_flags,\n\t\t\t\t     u16 raweth_qp1_flags2)\n{\n\tbool is_ipv6 = false, is_ipv4 = false;\n\n\t/* raweth_qp1_flags Bit 9-6 indicates itype */\n\tif ((raweth_qp1_flags & CQ_RES_RAWETH_QP1_RAWETH_QP1_FLAGS_ITYPE_ROCE)\n\t    != CQ_RES_RAWETH_QP1_RAWETH_QP1_FLAGS_ITYPE_ROCE)\n\t\treturn -1;\n\n\tif (raweth_qp1_flags2 &\n\t    CQ_RES_RAWETH_QP1_RAWETH_QP1_FLAGS2_IP_CS_CALC &&\n\t    raweth_qp1_flags2 &\n\t    CQ_RES_RAWETH_QP1_RAWETH_QP1_FLAGS2_L4_CS_CALC) {\n\t\t/* raweth_qp1_flags2 Bit 8 indicates ip_type. 0-v4 1 - v6 */\n\t\t(raweth_qp1_flags2 &\n\t\t CQ_RES_RAWETH_QP1_RAWETH_QP1_FLAGS2_IP_TYPE) ?\n\t\t\t(is_ipv6 = true) : (is_ipv4 = true);\n\t\treturn ((is_ipv6) ?\n\t\t\t BNXT_RE_ROCEV2_IPV6_PACKET :\n\t\t\t BNXT_RE_ROCEV2_IPV4_PACKET);\n\t} else {\n\t\treturn BNXT_RE_ROCE_V1_PACKET;\n\t}\n}\n\nstatic int bnxt_re_to_ib_nw_type(int nw_type)\n{\n\tu8 nw_hdr_type = 0xFF;\n\n\tswitch (nw_type) {\n\tcase BNXT_RE_ROCE_V1_PACKET:\n\t\tnw_hdr_type = RDMA_NETWORK_ROCE_V1;\n\t\tbreak;\n\tcase BNXT_RE_ROCEV2_IPV4_PACKET:\n\t\tnw_hdr_type = RDMA_NETWORK_IPV4;\n\t\tbreak;\n\tcase BNXT_RE_ROCEV2_IPV6_PACKET:\n\t\tnw_hdr_type = RDMA_NETWORK_IPV6;\n\t\tbreak;\n\t}\n\treturn nw_hdr_type;\n}\n\nstatic bool bnxt_re_is_loopback_packet(struct bnxt_re_dev *rdev,\n\t\t\t\t       void *rq_hdr_buf)\n{\n\tu8 *tmp_buf = NULL;\n\tstruct ethhdr *eth_hdr;\n\tu16 eth_type;\n\tbool rc = false;\n\n\ttmp_buf = (u8 *)rq_hdr_buf;\n\t/*\n\t * If dest mac is not same as I/F mac, this could be a\n\t * loopback address or multicast address, check whether\n\t * it is a loopback packet\n\t */\n\tif (!ether_addr_equal(tmp_buf, rdev->netdev->dev_addr)) {\n\t\ttmp_buf += 4;\n\t\t/* Check the  ether type */\n\t\teth_hdr = (struct ethhdr *)tmp_buf;\n\t\teth_type = ntohs(eth_hdr->h_proto);\n\t\tswitch (eth_type) {\n\t\tcase ETH_P_IBOE:\n\t\t\trc = true;\n\t\t\tbreak;\n\t\tcase ETH_P_IP:\n\t\tcase ETH_P_IPV6: {\n\t\t\tu32 len;\n\t\t\tstruct udphdr *udp_hdr;\n\n\t\t\tlen = (eth_type == ETH_P_IP ? sizeof(struct iphdr) :\n\t\t\t\t\t\t      sizeof(struct ipv6hdr));\n\t\t\ttmp_buf += sizeof(struct ethhdr) + len;\n\t\t\tudp_hdr = (struct udphdr *)tmp_buf;\n\t\t\tif (ntohs(udp_hdr->dest) ==\n\t\t\t\t    ROCE_V2_UDP_DPORT)\n\t\t\t\trc = true;\n\t\t\tbreak;\n\t\t\t}\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn rc;\n}\n\nstatic int bnxt_re_process_raw_qp_pkt_rx(struct bnxt_re_qp *qp1_qp,\n\t\t\t\t\t struct bnxt_qplib_cqe *cqe)\n{\n\tstruct bnxt_re_dev *rdev = qp1_qp->rdev;\n\tstruct bnxt_re_sqp_entries *sqp_entry = NULL;\n\tstruct bnxt_re_qp *qp = rdev->qp1_sqp;\n\tstruct ib_send_wr *swr;\n\tstruct ib_ud_wr udwr;\n\tstruct ib_recv_wr rwr;\n\tint pkt_type = 0;\n\tu32 tbl_idx;\n\tvoid *rq_hdr_buf;\n\tdma_addr_t rq_hdr_buf_map;\n\tdma_addr_t shrq_hdr_buf_map;\n\tu32 offset = 0;\n\tu32 skip_bytes = 0;\n\tstruct ib_sge s_sge[2];\n\tstruct ib_sge r_sge[2];\n\tint rc;\n\n\tmemset(&udwr, 0, sizeof(udwr));\n\tmemset(&rwr, 0, sizeof(rwr));\n\tmemset(&s_sge, 0, sizeof(s_sge));\n\tmemset(&r_sge, 0, sizeof(r_sge));\n\n\tswr = &udwr.wr;\n\ttbl_idx = cqe->wr_id;\n\n\trq_hdr_buf = qp1_qp->qplib_qp.rq_hdr_buf +\n\t\t\t(tbl_idx * qp1_qp->qplib_qp.rq_hdr_buf_size);\n\trq_hdr_buf_map = bnxt_qplib_get_qp_buf_from_index(&qp1_qp->qplib_qp,\n\t\t\t\t\t\t\t  tbl_idx);\n\n\t/* Shadow QP header buffer */\n\tshrq_hdr_buf_map = bnxt_qplib_get_qp_buf_from_index(&qp->qplib_qp,\n\t\t\t\t\t\t\t    tbl_idx);\n\tsqp_entry = &rdev->sqp_tbl[tbl_idx];\n\n\t/* Store this cqe */\n\tmemcpy(&sqp_entry->cqe, cqe, sizeof(struct bnxt_qplib_cqe));\n\tsqp_entry->qp1_qp = qp1_qp;\n\n\t/* Find packet type from the cqe */\n\n\tpkt_type = bnxt_re_check_packet_type(cqe->raweth_qp1_flags,\n\t\t\t\t\t     cqe->raweth_qp1_flags2);\n\tif (pkt_type < 0) {\n\t\tdev_err(rdev_to_dev(rdev), \"Invalid packet\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Adjust the offset for the user buffer and post in the rq */\n\n\tif (pkt_type == BNXT_RE_ROCEV2_IPV4_PACKET)\n\t\toffset = 20;\n\n\t/*\n\t * QP1 loopback packet has 4 bytes of internal header before\n\t * ether header. Skip these four bytes.\n\t */\n\tif (bnxt_re_is_loopback_packet(rdev, rq_hdr_buf))\n\t\tskip_bytes = 4;\n\n\t/* First send SGE . Skip the ether header*/\n\ts_sge[0].addr = rq_hdr_buf_map + BNXT_QPLIB_MAX_QP1_RQ_ETH_HDR_SIZE\n\t\t\t+ skip_bytes;\n\ts_sge[0].lkey = 0xFFFFFFFF;\n\ts_sge[0].length = offset ? BNXT_QPLIB_MAX_GRH_HDR_SIZE_IPV4 :\n\t\t\t\tBNXT_QPLIB_MAX_GRH_HDR_SIZE_IPV6;\n\n\t/* Second Send SGE */\n\ts_sge[1].addr = s_sge[0].addr + s_sge[0].length +\n\t\t\tBNXT_QPLIB_MAX_QP1_RQ_BDETH_HDR_SIZE;\n\tif (pkt_type != BNXT_RE_ROCE_V1_PACKET)\n\t\ts_sge[1].addr += 8;\n\ts_sge[1].lkey = 0xFFFFFFFF;\n\ts_sge[1].length = 256;\n\n\t/* First recv SGE */\n\n\tr_sge[0].addr = shrq_hdr_buf_map;\n\tr_sge[0].lkey = 0xFFFFFFFF;\n\tr_sge[0].length = 40;\n\n\tr_sge[1].addr = sqp_entry->sge.addr + offset;\n\tr_sge[1].lkey = sqp_entry->sge.lkey;\n\tr_sge[1].length = BNXT_QPLIB_MAX_GRH_HDR_SIZE_IPV6 + 256 - offset;\n\n\t/* Create receive work request */\n\trwr.num_sge = 2;\n\trwr.sg_list = r_sge;\n\trwr.wr_id = tbl_idx;\n\trwr.next = NULL;\n\n\trc = bnxt_re_post_recv_shadow_qp(rdev, qp, &rwr);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\"Failed to post Rx buffers to shadow QP\");\n\t\treturn -ENOMEM;\n\t}\n\n\tswr->num_sge = 2;\n\tswr->sg_list = s_sge;\n\tswr->wr_id = tbl_idx;\n\tswr->opcode = IB_WR_SEND;\n\tswr->next = NULL;\n\n\tudwr.ah = &rdev->sqp_ah->ib_ah;\n\tudwr.remote_qpn = rdev->qp1_sqp->qplib_qp.id;\n\tudwr.remote_qkey = rdev->qp1_sqp->qplib_qp.qkey;\n\n\t/* post data received  in the send queue */\n\trc = bnxt_re_post_send_shadow_qp(rdev, qp, swr);\n\n\treturn 0;\n}\n\nstatic void bnxt_re_process_res_rawqp1_wc(struct ib_wc *wc,\n\t\t\t\t\t  struct bnxt_qplib_cqe *cqe)\n{\n\twc->opcode = IB_WC_RECV;\n\twc->status = __rawqp1_to_ib_wc_status(cqe->status);\n\twc->wc_flags |= IB_WC_GRH;\n}\n\nstatic bool bnxt_re_is_vlan_pkt(struct bnxt_qplib_cqe *orig_cqe,\n\t\t\t\tu16 *vid, u8 *sl)\n{\n\tbool ret = false;\n\tu32 metadata;\n\tu16 tpid;\n\n\tmetadata = orig_cqe->raweth_qp1_metadata;\n\tif (orig_cqe->raweth_qp1_flags2 &\n\t\tCQ_RES_RAWETH_QP1_RAWETH_QP1_FLAGS2_META_FORMAT_VLAN) {\n\t\ttpid = ((metadata &\n\t\t\t CQ_RES_RAWETH_QP1_RAWETH_QP1_METADATA_TPID_MASK) >>\n\t\t\t CQ_RES_RAWETH_QP1_RAWETH_QP1_METADATA_TPID_SFT);\n\t\tif (tpid == ETH_P_8021Q) {\n\t\t\t*vid = metadata &\n\t\t\t       CQ_RES_RAWETH_QP1_RAWETH_QP1_METADATA_VID_MASK;\n\t\t\t*sl = (metadata &\n\t\t\t       CQ_RES_RAWETH_QP1_RAWETH_QP1_METADATA_PRI_MASK) >>\n\t\t\t       CQ_RES_RAWETH_QP1_RAWETH_QP1_METADATA_PRI_SFT;\n\t\t\tret = true;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic void bnxt_re_process_res_rc_wc(struct ib_wc *wc,\n\t\t\t\t      struct bnxt_qplib_cqe *cqe)\n{\n\twc->opcode = IB_WC_RECV;\n\twc->status = __rc_to_ib_wc_status(cqe->status);\n\n\tif (cqe->flags & CQ_RES_RC_FLAGS_IMM)\n\t\twc->wc_flags |= IB_WC_WITH_IMM;\n\tif (cqe->flags & CQ_RES_RC_FLAGS_INV)\n\t\twc->wc_flags |= IB_WC_WITH_INVALIDATE;\n\tif ((cqe->flags & (CQ_RES_RC_FLAGS_RDMA | CQ_RES_RC_FLAGS_IMM)) ==\n\t    (CQ_RES_RC_FLAGS_RDMA | CQ_RES_RC_FLAGS_IMM))\n\t\twc->opcode = IB_WC_RECV_RDMA_WITH_IMM;\n}\n\nstatic void bnxt_re_process_res_shadow_qp_wc(struct bnxt_re_qp *qp,\n\t\t\t\t\t     struct ib_wc *wc,\n\t\t\t\t\t     struct bnxt_qplib_cqe *cqe)\n{\n\tstruct bnxt_re_dev *rdev = qp->rdev;\n\tstruct bnxt_re_qp *qp1_qp = NULL;\n\tstruct bnxt_qplib_cqe *orig_cqe = NULL;\n\tstruct bnxt_re_sqp_entries *sqp_entry = NULL;\n\tint nw_type;\n\tu32 tbl_idx;\n\tu16 vlan_id;\n\tu8 sl;\n\n\ttbl_idx = cqe->wr_id;\n\n\tsqp_entry = &rdev->sqp_tbl[tbl_idx];\n\tqp1_qp = sqp_entry->qp1_qp;\n\torig_cqe = &sqp_entry->cqe;\n\n\twc->wr_id = sqp_entry->wrid;\n\twc->byte_len = orig_cqe->length;\n\twc->qp = &qp1_qp->ib_qp;\n\n\twc->ex.imm_data = orig_cqe->immdata;\n\twc->src_qp = orig_cqe->src_qp;\n\tmemcpy(wc->smac, orig_cqe->smac, ETH_ALEN);\n\tif (bnxt_re_is_vlan_pkt(orig_cqe, &vlan_id, &sl)) {\n\t\twc->vlan_id = vlan_id;\n\t\twc->sl = sl;\n\t\twc->wc_flags |= IB_WC_WITH_VLAN;\n\t}\n\twc->port_num = 1;\n\twc->vendor_err = orig_cqe->status;\n\n\twc->opcode = IB_WC_RECV;\n\twc->status = __rawqp1_to_ib_wc_status(orig_cqe->status);\n\twc->wc_flags |= IB_WC_GRH;\n\n\tnw_type = bnxt_re_check_packet_type(orig_cqe->raweth_qp1_flags,\n\t\t\t\t\t    orig_cqe->raweth_qp1_flags2);\n\tif (nw_type >= 0) {\n\t\twc->network_hdr_type = bnxt_re_to_ib_nw_type(nw_type);\n\t\twc->wc_flags |= IB_WC_WITH_NETWORK_HDR_TYPE;\n\t}\n}\n\nstatic void bnxt_re_process_res_ud_wc(struct bnxt_re_qp *qp,\n\t\t\t\t      struct ib_wc *wc,\n\t\t\t\t      struct bnxt_qplib_cqe *cqe)\n{\n\tu8 nw_type;\n\n\twc->opcode = IB_WC_RECV;\n\twc->status = __rc_to_ib_wc_status(cqe->status);\n\n\tif (cqe->flags & CQ_RES_UD_FLAGS_IMM)\n\t\twc->wc_flags |= IB_WC_WITH_IMM;\n\t/* report only on GSI QP for Thor */\n\tif (qp->qplib_qp.type == CMDQ_CREATE_QP_TYPE_GSI) {\n\t\twc->wc_flags |= IB_WC_GRH;\n\t\tmemcpy(wc->smac, cqe->smac, ETH_ALEN);\n\t\twc->wc_flags |= IB_WC_WITH_SMAC;\n\t\tif (cqe->flags & CQ_RES_UD_FLAGS_META_FORMAT_VLAN) {\n\t\t\twc->vlan_id = (cqe->cfa_meta & 0xFFF);\n\t\t\tif (wc->vlan_id < 0x1000)\n\t\t\t\twc->wc_flags |= IB_WC_WITH_VLAN;\n\t\t}\n\t\tnw_type = (cqe->flags & CQ_RES_UD_FLAGS_ROCE_IP_VER_MASK) >>\n\t\t\t   CQ_RES_UD_FLAGS_ROCE_IP_VER_SFT;\n\t\twc->network_hdr_type = bnxt_re_to_ib_nw_type(nw_type);\n\t\twc->wc_flags |= IB_WC_WITH_NETWORK_HDR_TYPE;\n\t}\n\n}\n\nstatic int send_phantom_wqe(struct bnxt_re_qp *qp)\n{\n\tstruct bnxt_qplib_qp *lib_qp = &qp->qplib_qp;\n\tunsigned long flags;\n\tint rc = 0;\n\n\tspin_lock_irqsave(&qp->sq_lock, flags);\n\n\trc = bnxt_re_bind_fence_mw(lib_qp);\n\tif (!rc) {\n\t\tlib_qp->sq.phantom_wqe_cnt++;\n\t\tdev_dbg(&lib_qp->sq.hwq.pdev->dev,\n\t\t\t\"qp %#x sq->prod %#x sw_prod %#x phantom_wqe_cnt %d\\n\",\n\t\t\tlib_qp->id, lib_qp->sq.hwq.prod,\n\t\t\tHWQ_CMP(lib_qp->sq.hwq.prod, &lib_qp->sq.hwq),\n\t\t\tlib_qp->sq.phantom_wqe_cnt);\n\t}\n\n\tspin_unlock_irqrestore(&qp->sq_lock, flags);\n\treturn rc;\n}\n\nint bnxt_re_poll_cq(struct ib_cq *ib_cq, int num_entries, struct ib_wc *wc)\n{\n\tstruct bnxt_re_cq *cq = container_of(ib_cq, struct bnxt_re_cq, ib_cq);\n\tstruct bnxt_re_qp *qp;\n\tstruct bnxt_qplib_cqe *cqe;\n\tint i, ncqe, budget;\n\tstruct bnxt_qplib_q *sq;\n\tstruct bnxt_qplib_qp *lib_qp;\n\tu32 tbl_idx;\n\tstruct bnxt_re_sqp_entries *sqp_entry = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cq->cq_lock, flags);\n\tbudget = min_t(u32, num_entries, cq->max_cql);\n\tnum_entries = budget;\n\tif (!cq->cql) {\n\t\tdev_err(rdev_to_dev(cq->rdev), \"POLL CQ : no CQL to use\");\n\t\tgoto exit;\n\t}\n\tcqe = &cq->cql[0];\n\twhile (budget) {\n\t\tlib_qp = NULL;\n\t\tncqe = bnxt_qplib_poll_cq(&cq->qplib_cq, cqe, budget, &lib_qp);\n\t\tif (lib_qp) {\n\t\t\tsq = &lib_qp->sq;\n\t\t\tif (sq->send_phantom) {\n\t\t\t\tqp = container_of(lib_qp,\n\t\t\t\t\t\t  struct bnxt_re_qp, qplib_qp);\n\t\t\t\tif (send_phantom_wqe(qp) == -ENOMEM)\n\t\t\t\t\tdev_err(rdev_to_dev(cq->rdev),\n\t\t\t\t\t\t\"Phantom failed! Scheduled to send again\\n\");\n\t\t\t\telse\n\t\t\t\t\tsq->send_phantom = false;\n\t\t\t}\n\t\t}\n\t\tif (ncqe < budget)\n\t\t\tncqe += bnxt_qplib_process_flush_list(&cq->qplib_cq,\n\t\t\t\t\t\t\t      cqe + ncqe,\n\t\t\t\t\t\t\t      budget - ncqe);\n\n\t\tif (!ncqe)\n\t\t\tbreak;\n\n\t\tfor (i = 0; i < ncqe; i++, cqe++) {\n\t\t\t/* Transcribe each qplib_wqe back to ib_wc */\n\t\t\tmemset(wc, 0, sizeof(*wc));\n\n\t\t\twc->wr_id = cqe->wr_id;\n\t\t\twc->byte_len = cqe->length;\n\t\t\tqp = container_of\n\t\t\t\t((struct bnxt_qplib_qp *)\n\t\t\t\t (unsigned long)(cqe->qp_handle),\n\t\t\t\t struct bnxt_re_qp, qplib_qp);\n\t\t\tif (!qp) {\n\t\t\t\tdev_err(rdev_to_dev(cq->rdev),\n\t\t\t\t\t\"POLL CQ : bad QP handle\");\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\twc->qp = &qp->ib_qp;\n\t\t\twc->ex.imm_data = cqe->immdata;\n\t\t\twc->src_qp = cqe->src_qp;\n\t\t\tmemcpy(wc->smac, cqe->smac, ETH_ALEN);\n\t\t\twc->port_num = 1;\n\t\t\twc->vendor_err = cqe->status;\n\n\t\t\tswitch (cqe->opcode) {\n\t\t\tcase CQ_BASE_CQE_TYPE_REQ:\n\t\t\t\tif (qp->rdev->qp1_sqp && qp->qplib_qp.id ==\n\t\t\t\t    qp->rdev->qp1_sqp->qplib_qp.id) {\n\t\t\t\t\t/* Handle this completion with\n\t\t\t\t\t * the stored completion\n\t\t\t\t\t */\n\t\t\t\t\tmemset(wc, 0, sizeof(*wc));\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tbnxt_re_process_req_wc(wc, cqe);\n\t\t\t\tbreak;\n\t\t\tcase CQ_BASE_CQE_TYPE_RES_RAWETH_QP1:\n\t\t\t\tif (!cqe->status) {\n\t\t\t\t\tint rc = 0;\n\n\t\t\t\t\trc = bnxt_re_process_raw_qp_pkt_rx\n\t\t\t\t\t\t\t\t(qp, cqe);\n\t\t\t\t\tif (!rc) {\n\t\t\t\t\t\tmemset(wc, 0, sizeof(*wc));\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tcqe->status = -1;\n\t\t\t\t}\n\t\t\t\t/* Errors need not be looped back.\n\t\t\t\t * But change the wr_id to the one\n\t\t\t\t * stored in the table\n\t\t\t\t */\n\t\t\t\ttbl_idx = cqe->wr_id;\n\t\t\t\tsqp_entry = &cq->rdev->sqp_tbl[tbl_idx];\n\t\t\t\twc->wr_id = sqp_entry->wrid;\n\t\t\t\tbnxt_re_process_res_rawqp1_wc(wc, cqe);\n\t\t\t\tbreak;\n\t\t\tcase CQ_BASE_CQE_TYPE_RES_RC:\n\t\t\t\tbnxt_re_process_res_rc_wc(wc, cqe);\n\t\t\t\tbreak;\n\t\t\tcase CQ_BASE_CQE_TYPE_RES_UD:\n\t\t\t\tif (qp->rdev->qp1_sqp && qp->qplib_qp.id ==\n\t\t\t\t    qp->rdev->qp1_sqp->qplib_qp.id) {\n\t\t\t\t\t/* Handle this completion with\n\t\t\t\t\t * the stored completion\n\t\t\t\t\t */\n\t\t\t\t\tif (cqe->status) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tbnxt_re_process_res_shadow_qp_wc\n\t\t\t\t\t\t\t\t(qp, wc, cqe);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbnxt_re_process_res_ud_wc(qp, wc, cqe);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tdev_err(rdev_to_dev(cq->rdev),\n\t\t\t\t\t\"POLL CQ : type 0x%x not handled\",\n\t\t\t\t\tcqe->opcode);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\twc++;\n\t\t\tbudget--;\n\t\t}\n\t}\nexit:\n\tspin_unlock_irqrestore(&cq->cq_lock, flags);\n\treturn num_entries - budget;\n}\n\nint bnxt_re_req_notify_cq(struct ib_cq *ib_cq,\n\t\t\t  enum ib_cq_notify_flags ib_cqn_flags)\n{\n\tstruct bnxt_re_cq *cq = container_of(ib_cq, struct bnxt_re_cq, ib_cq);\n\tint type = 0, rc = 0;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cq->cq_lock, flags);\n\t/* Trigger on the very next completion */\n\tif (ib_cqn_flags & IB_CQ_NEXT_COMP)\n\t\ttype = DBC_DBC_TYPE_CQ_ARMALL;\n\t/* Trigger on the next solicited completion */\n\telse if (ib_cqn_flags & IB_CQ_SOLICITED)\n\t\ttype = DBC_DBC_TYPE_CQ_ARMSE;\n\n\t/* Poll to see if there are missed events */\n\tif ((ib_cqn_flags & IB_CQ_REPORT_MISSED_EVENTS) &&\n\t    !(bnxt_qplib_is_cq_empty(&cq->qplib_cq))) {\n\t\trc = 1;\n\t\tgoto exit;\n\t}\n\tbnxt_qplib_req_notify_cq(&cq->qplib_cq, type);\n\nexit:\n\tspin_unlock_irqrestore(&cq->cq_lock, flags);\n\treturn rc;\n}\n\n/* Memory Regions */\nstruct ib_mr *bnxt_re_get_dma_mr(struct ib_pd *ib_pd, int mr_access_flags)\n{\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct bnxt_re_mr *mr;\n\tu64 pbl = 0;\n\tint rc;\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmr->rdev = rdev;\n\tmr->qplib_mr.pd = &pd->qplib_pd;\n\tmr->qplib_mr.flags = __from_ib_access_flags(mr_access_flags);\n\tmr->qplib_mr.type = CMDQ_ALLOCATE_MRW_MRW_FLAGS_PMR;\n\n\t/* Allocate and register 0 as the address */\n\trc = bnxt_qplib_alloc_mrw(&rdev->qplib_res, &mr->qplib_mr);\n\tif (rc)\n\t\tgoto fail;\n\n\tmr->qplib_mr.hwq.level = PBL_LVL_MAX;\n\tmr->qplib_mr.total_size = -1; /* Infinte length */\n\trc = bnxt_qplib_reg_mr(&rdev->qplib_res, &mr->qplib_mr, &pbl, 0, false,\n\t\t\t       PAGE_SIZE);\n\tif (rc)\n\t\tgoto fail_mr;\n\n\tmr->ib_mr.lkey = mr->qplib_mr.lkey;\n\tif (mr_access_flags & (IB_ACCESS_REMOTE_WRITE | IB_ACCESS_REMOTE_READ |\n\t\t\t       IB_ACCESS_REMOTE_ATOMIC))\n\t\tmr->ib_mr.rkey = mr->ib_mr.lkey;\n\tatomic_inc(&rdev->mr_count);\n\n\treturn &mr->ib_mr;\n\nfail_mr:\n\tbnxt_qplib_free_mrw(&rdev->qplib_res, &mr->qplib_mr);\nfail:\n\tkfree(mr);\n\treturn ERR_PTR(rc);\n}\n\nint bnxt_re_dereg_mr(struct ib_mr *ib_mr, struct ib_udata *udata)\n{\n\tstruct bnxt_re_mr *mr = container_of(ib_mr, struct bnxt_re_mr, ib_mr);\n\tstruct bnxt_re_dev *rdev = mr->rdev;\n\tint rc;\n\n\trc = bnxt_qplib_free_mrw(&rdev->qplib_res, &mr->qplib_mr);\n\tif (rc)\n\t\tdev_err(rdev_to_dev(rdev), \"Dereg MR failed: %#x\\n\", rc);\n\n\tif (mr->pages) {\n\t\trc = bnxt_qplib_free_fast_reg_page_list(&rdev->qplib_res,\n\t\t\t\t\t\t\t&mr->qplib_frpl);\n\t\tkfree(mr->pages);\n\t\tmr->npages = 0;\n\t\tmr->pages = NULL;\n\t}\n\tib_umem_release(mr->ib_umem);\n\n\tkfree(mr);\n\tatomic_dec(&rdev->mr_count);\n\treturn rc;\n}\n\nstatic int bnxt_re_set_page(struct ib_mr *ib_mr, u64 addr)\n{\n\tstruct bnxt_re_mr *mr = container_of(ib_mr, struct bnxt_re_mr, ib_mr);\n\n\tif (unlikely(mr->npages == mr->qplib_frpl.max_pg_ptrs))\n\t\treturn -ENOMEM;\n\n\tmr->pages[mr->npages++] = addr;\n\treturn 0;\n}\n\nint bnxt_re_map_mr_sg(struct ib_mr *ib_mr, struct scatterlist *sg, int sg_nents,\n\t\t      unsigned int *sg_offset)\n{\n\tstruct bnxt_re_mr *mr = container_of(ib_mr, struct bnxt_re_mr, ib_mr);\n\n\tmr->npages = 0;\n\treturn ib_sg_to_pages(ib_mr, sg, sg_nents, sg_offset, bnxt_re_set_page);\n}\n\nstruct ib_mr *bnxt_re_alloc_mr(struct ib_pd *ib_pd, enum ib_mr_type type,\n\t\t\t       u32 max_num_sg, struct ib_udata *udata)\n{\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct bnxt_re_mr *mr = NULL;\n\tint rc;\n\n\tif (type != IB_MR_TYPE_MEM_REG) {\n\t\tdev_dbg(rdev_to_dev(rdev), \"MR type 0x%x not supported\", type);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tif (max_num_sg > MAX_PBL_LVL_1_PGS)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmr->rdev = rdev;\n\tmr->qplib_mr.pd = &pd->qplib_pd;\n\tmr->qplib_mr.flags = BNXT_QPLIB_FR_PMR;\n\tmr->qplib_mr.type = CMDQ_ALLOCATE_MRW_MRW_FLAGS_PMR;\n\n\trc = bnxt_qplib_alloc_mrw(&rdev->qplib_res, &mr->qplib_mr);\n\tif (rc)\n\t\tgoto bail;\n\n\tmr->ib_mr.lkey = mr->qplib_mr.lkey;\n\tmr->ib_mr.rkey = mr->ib_mr.lkey;\n\n\tmr->pages = kcalloc(max_num_sg, sizeof(u64), GFP_KERNEL);\n\tif (!mr->pages) {\n\t\trc = -ENOMEM;\n\t\tgoto fail;\n\t}\n\trc = bnxt_qplib_alloc_fast_reg_page_list(&rdev->qplib_res,\n\t\t\t\t\t\t &mr->qplib_frpl, max_num_sg);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\"Failed to allocate HW FR page list\");\n\t\tgoto fail_mr;\n\t}\n\n\tatomic_inc(&rdev->mr_count);\n\treturn &mr->ib_mr;\n\nfail_mr:\n\tkfree(mr->pages);\nfail:\n\tbnxt_qplib_free_mrw(&rdev->qplib_res, &mr->qplib_mr);\nbail:\n\tkfree(mr);\n\treturn ERR_PTR(rc);\n}\n\nstruct ib_mw *bnxt_re_alloc_mw(struct ib_pd *ib_pd, enum ib_mw_type type,\n\t\t\t       struct ib_udata *udata)\n{\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct bnxt_re_mw *mw;\n\tint rc;\n\n\tmw = kzalloc(sizeof(*mw), GFP_KERNEL);\n\tif (!mw)\n\t\treturn ERR_PTR(-ENOMEM);\n\tmw->rdev = rdev;\n\tmw->qplib_mw.pd = &pd->qplib_pd;\n\n\tmw->qplib_mw.type = (type == IB_MW_TYPE_1 ?\n\t\t\t       CMDQ_ALLOCATE_MRW_MRW_FLAGS_MW_TYPE1 :\n\t\t\t       CMDQ_ALLOCATE_MRW_MRW_FLAGS_MW_TYPE2B);\n\trc = bnxt_qplib_alloc_mrw(&rdev->qplib_res, &mw->qplib_mw);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Allocate MW failed!\");\n\t\tgoto fail;\n\t}\n\tmw->ib_mw.rkey = mw->qplib_mw.rkey;\n\n\tatomic_inc(&rdev->mw_count);\n\treturn &mw->ib_mw;\n\nfail:\n\tkfree(mw);\n\treturn ERR_PTR(rc);\n}\n\nint bnxt_re_dealloc_mw(struct ib_mw *ib_mw)\n{\n\tstruct bnxt_re_mw *mw = container_of(ib_mw, struct bnxt_re_mw, ib_mw);\n\tstruct bnxt_re_dev *rdev = mw->rdev;\n\tint rc;\n\n\trc = bnxt_qplib_free_mrw(&rdev->qplib_res, &mw->qplib_mw);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Free MW failed: %#x\\n\", rc);\n\t\treturn rc;\n\t}\n\n\tkfree(mw);\n\tatomic_dec(&rdev->mw_count);\n\treturn rc;\n}\n\nstatic int bnxt_re_page_size_ok(int page_shift)\n{\n\tswitch (page_shift) {\n\tcase CMDQ_REGISTER_MR_LOG2_PBL_PG_SIZE_PG_4K:\n\tcase CMDQ_REGISTER_MR_LOG2_PBL_PG_SIZE_PG_8K:\n\tcase CMDQ_REGISTER_MR_LOG2_PBL_PG_SIZE_PG_64K:\n\tcase CMDQ_REGISTER_MR_LOG2_PBL_PG_SIZE_PG_2M:\n\tcase CMDQ_REGISTER_MR_LOG2_PBL_PG_SIZE_PG_256K:\n\tcase CMDQ_REGISTER_MR_LOG2_PBL_PG_SIZE_PG_1M:\n\tcase CMDQ_REGISTER_MR_LOG2_PBL_PG_SIZE_PG_4M:\n\tcase CMDQ_REGISTER_MR_LOG2_PBL_PG_SIZE_PG_1G:\n\t\treturn 1;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic int fill_umem_pbl_tbl(struct ib_umem *umem, u64 *pbl_tbl_orig,\n\t\t\t     int page_shift)\n{\n\tu64 *pbl_tbl = pbl_tbl_orig;\n\tu64 page_size =  BIT_ULL(page_shift);\n\tstruct ib_block_iter biter;\n\n\trdma_for_each_block(umem->sg_head.sgl, &biter, umem->nmap, page_size)\n\t\t*pbl_tbl++ = rdma_block_iter_dma_address(&biter);\n\n\treturn pbl_tbl - pbl_tbl_orig;\n}\n\n/* uverbs */\nstruct ib_mr *bnxt_re_reg_user_mr(struct ib_pd *ib_pd, u64 start, u64 length,\n\t\t\t\t  u64 virt_addr, int mr_access_flags,\n\t\t\t\t  struct ib_udata *udata)\n{\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct bnxt_re_mr *mr;\n\tstruct ib_umem *umem;\n\tu64 *pbl_tbl = NULL;\n\tint umem_pgs, page_shift, rc;\n\n\tif (length > BNXT_RE_MAX_MR_SIZE) {\n\t\tdev_err(rdev_to_dev(rdev), \"MR Size: %lld > Max supported:%lld\\n\",\n\t\t\tlength, BNXT_RE_MAX_MR_SIZE);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmr->rdev = rdev;\n\tmr->qplib_mr.pd = &pd->qplib_pd;\n\tmr->qplib_mr.flags = __from_ib_access_flags(mr_access_flags);\n\tmr->qplib_mr.type = CMDQ_ALLOCATE_MRW_MRW_FLAGS_MR;\n\n\trc = bnxt_qplib_alloc_mrw(&rdev->qplib_res, &mr->qplib_mr);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to allocate MR\");\n\t\tgoto free_mr;\n\t}\n\t/* The fixed portion of the rkey is the same as the lkey */\n\tmr->ib_mr.rkey = mr->qplib_mr.rkey;\n\n\tumem = ib_umem_get(udata, start, length, mr_access_flags, 0);\n\tif (IS_ERR(umem)) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to get umem\");\n\t\trc = -EFAULT;\n\t\tgoto free_mrw;\n\t}\n\tmr->ib_umem = umem;\n\n\tmr->qplib_mr.va = virt_addr;\n\tumem_pgs = ib_umem_page_count(umem);\n\tif (!umem_pgs) {\n\t\tdev_err(rdev_to_dev(rdev), \"umem is invalid!\");\n\t\trc = -EINVAL;\n\t\tgoto free_umem;\n\t}\n\tmr->qplib_mr.total_size = length;\n\n\tpbl_tbl = kcalloc(umem_pgs, sizeof(u64 *), GFP_KERNEL);\n\tif (!pbl_tbl) {\n\t\trc = -ENOMEM;\n\t\tgoto free_umem;\n\t}\n\n\tpage_shift = __ffs(ib_umem_find_best_pgsz(umem,\n\t\t\t\tBNXT_RE_PAGE_SIZE_4K | BNXT_RE_PAGE_SIZE_2M,\n\t\t\t\tvirt_addr));\n\n\tif (!bnxt_re_page_size_ok(page_shift)) {\n\t\tdev_err(rdev_to_dev(rdev), \"umem page size unsupported!\");\n\t\trc = -EFAULT;\n\t\tgoto fail;\n\t}\n\n\tif (page_shift == BNXT_RE_PAGE_SHIFT_4K &&\n\t    length > BNXT_RE_MAX_MR_SIZE_LOW) {\n\t\tdev_err(rdev_to_dev(rdev), \"Requested MR Sz:%llu Max sup:%llu\",\n\t\t\tlength,\t(u64)BNXT_RE_MAX_MR_SIZE_LOW);\n\t\trc = -EINVAL;\n\t\tgoto fail;\n\t}\n\n\t/* Map umem buf ptrs to the PBL */\n\tumem_pgs = fill_umem_pbl_tbl(umem, pbl_tbl, page_shift);\n\trc = bnxt_qplib_reg_mr(&rdev->qplib_res, &mr->qplib_mr, pbl_tbl,\n\t\t\t       umem_pgs, false, 1 << page_shift);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to register user MR\");\n\t\tgoto fail;\n\t}\n\n\tkfree(pbl_tbl);\n\n\tmr->ib_mr.lkey = mr->qplib_mr.lkey;\n\tmr->ib_mr.rkey = mr->qplib_mr.lkey;\n\tatomic_inc(&rdev->mr_count);\n\n\treturn &mr->ib_mr;\nfail:\n\tkfree(pbl_tbl);\nfree_umem:\n\tib_umem_release(umem);\nfree_mrw:\n\tbnxt_qplib_free_mrw(&rdev->qplib_res, &mr->qplib_mr);\nfree_mr:\n\tkfree(mr);\n\treturn ERR_PTR(rc);\n}\n\nint bnxt_re_alloc_ucontext(struct ib_ucontext *ctx, struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = ctx->device;\n\tstruct bnxt_re_ucontext *uctx =\n\t\tcontainer_of(ctx, struct bnxt_re_ucontext, ib_uctx);\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(ibdev, ibdev);\n\tstruct bnxt_qplib_dev_attr *dev_attr = &rdev->dev_attr;\n\tstruct bnxt_re_uctx_resp resp;\n\tu32 chip_met_rev_num = 0;\n\tint rc;\n\n\tdev_dbg(rdev_to_dev(rdev), \"ABI version requested %u\",\n\t\tibdev->ops.uverbs_abi_ver);\n\n\tif (ibdev->ops.uverbs_abi_ver != BNXT_RE_ABI_VERSION) {\n\t\tdev_dbg(rdev_to_dev(rdev), \" is different from the device %d \",\n\t\t\tBNXT_RE_ABI_VERSION);\n\t\treturn -EPERM;\n\t}\n\n\tuctx->rdev = rdev;\n\n\tuctx->shpg = (void *)__get_free_page(GFP_KERNEL);\n\tif (!uctx->shpg) {\n\t\trc = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tspin_lock_init(&uctx->sh_lock);\n\n\tresp.comp_mask = BNXT_RE_UCNTX_CMASK_HAVE_CCTX;\n\tchip_met_rev_num = rdev->chip_ctx.chip_num;\n\tchip_met_rev_num |= ((u32)rdev->chip_ctx.chip_rev & 0xFF) <<\n\t\t\t     BNXT_RE_CHIP_ID0_CHIP_REV_SFT;\n\tchip_met_rev_num |= ((u32)rdev->chip_ctx.chip_metal & 0xFF) <<\n\t\t\t     BNXT_RE_CHIP_ID0_CHIP_MET_SFT;\n\tresp.chip_id0 = chip_met_rev_num;\n\t/* Future extension of chip info */\n\tresp.chip_id1 = 0;\n\t/*Temp, Use xa_alloc instead */\n\tresp.dev_id = rdev->en_dev->pdev->devfn;\n\tresp.max_qp = rdev->qplib_ctx.qpc_count;\n\tresp.pg_size = PAGE_SIZE;\n\tresp.cqe_sz = sizeof(struct cq_base);\n\tresp.max_cqd = dev_attr->max_cq_wqes;\n\tresp.rsvd    = 0;\n\n\trc = ib_copy_to_udata(udata, &resp, min(udata->outlen, sizeof(resp)));\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to copy user context\");\n\t\trc = -EFAULT;\n\t\tgoto cfail;\n\t}\n\n\treturn 0;\ncfail:\n\tfree_page((unsigned long)uctx->shpg);\n\tuctx->shpg = NULL;\nfail:\n\treturn rc;\n}\n\nvoid bnxt_re_dealloc_ucontext(struct ib_ucontext *ib_uctx)\n{\n\tstruct bnxt_re_ucontext *uctx = container_of(ib_uctx,\n\t\t\t\t\t\t   struct bnxt_re_ucontext,\n\t\t\t\t\t\t   ib_uctx);\n\n\tstruct bnxt_re_dev *rdev = uctx->rdev;\n\n\tif (uctx->shpg)\n\t\tfree_page((unsigned long)uctx->shpg);\n\n\tif (uctx->dpi.dbr) {\n\t\t/* Free DPI only if this is the first PD allocated by the\n\t\t * application and mark the context dpi as NULL\n\t\t */\n\t\tbnxt_qplib_dealloc_dpi(&rdev->qplib_res,\n\t\t\t\t       &rdev->qplib_res.dpi_tbl, &uctx->dpi);\n\t\tuctx->dpi.dbr = NULL;\n\t}\n}\n\n/* Helper function to mmap the virtual memory from user app */\nint bnxt_re_mmap(struct ib_ucontext *ib_uctx, struct vm_area_struct *vma)\n{\n\tstruct bnxt_re_ucontext *uctx = container_of(ib_uctx,\n\t\t\t\t\t\t   struct bnxt_re_ucontext,\n\t\t\t\t\t\t   ib_uctx);\n\tstruct bnxt_re_dev *rdev = uctx->rdev;\n\tu64 pfn;\n\n\tif (vma->vm_end - vma->vm_start != PAGE_SIZE)\n\t\treturn -EINVAL;\n\n\tif (vma->vm_pgoff) {\n\t\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n\t\tif (io_remap_pfn_range(vma, vma->vm_start, vma->vm_pgoff,\n\t\t\t\t       PAGE_SIZE, vma->vm_page_prot)) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"Failed to map DPI\");\n\t\t\treturn -EAGAIN;\n\t\t}\n\t} else {\n\t\tpfn = virt_to_phys(uctx->shpg) >> PAGE_SHIFT;\n\t\tif (remap_pfn_range(vma, vma->vm_start,\n\t\t\t\t    pfn, PAGE_SIZE, vma->vm_page_prot)) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Failed to map shared page\");\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\treturn 0;\n}\n"], "fixing_code": ["/*\n * Broadcom NetXtreme-E RoCE driver.\n *\n * Copyright (c) 2016 - 2017, Broadcom. All rights reserved.  The term\n * Broadcom refers to Broadcom Limited and/or its subsidiaries.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * BSD license below:\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in\n *    the documentation and/or other materials provided with the\n *    distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS''\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS\n * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR\n * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\n * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN\n * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n * Description: IB Verbs interpreter\n */\n\n#include <linux/interrupt.h>\n#include <linux/types.h>\n#include <linux/pci.h>\n#include <linux/netdevice.h>\n#include <linux/if_ether.h>\n\n#include <rdma/ib_verbs.h>\n#include <rdma/ib_user_verbs.h>\n#include <rdma/ib_umem.h>\n#include <rdma/ib_addr.h>\n#include <rdma/ib_mad.h>\n#include <rdma/ib_cache.h>\n#include <rdma/uverbs_ioctl.h>\n\n#include \"bnxt_ulp.h\"\n\n#include \"roce_hsi.h\"\n#include \"qplib_res.h\"\n#include \"qplib_sp.h\"\n#include \"qplib_fp.h\"\n#include \"qplib_rcfw.h\"\n\n#include \"bnxt_re.h\"\n#include \"ib_verbs.h\"\n#include <rdma/bnxt_re-abi.h>\n\nstatic int __from_ib_access_flags(int iflags)\n{\n\tint qflags = 0;\n\n\tif (iflags & IB_ACCESS_LOCAL_WRITE)\n\t\tqflags |= BNXT_QPLIB_ACCESS_LOCAL_WRITE;\n\tif (iflags & IB_ACCESS_REMOTE_READ)\n\t\tqflags |= BNXT_QPLIB_ACCESS_REMOTE_READ;\n\tif (iflags & IB_ACCESS_REMOTE_WRITE)\n\t\tqflags |= BNXT_QPLIB_ACCESS_REMOTE_WRITE;\n\tif (iflags & IB_ACCESS_REMOTE_ATOMIC)\n\t\tqflags |= BNXT_QPLIB_ACCESS_REMOTE_ATOMIC;\n\tif (iflags & IB_ACCESS_MW_BIND)\n\t\tqflags |= BNXT_QPLIB_ACCESS_MW_BIND;\n\tif (iflags & IB_ZERO_BASED)\n\t\tqflags |= BNXT_QPLIB_ACCESS_ZERO_BASED;\n\tif (iflags & IB_ACCESS_ON_DEMAND)\n\t\tqflags |= BNXT_QPLIB_ACCESS_ON_DEMAND;\n\treturn qflags;\n};\n\nstatic enum ib_access_flags __to_ib_access_flags(int qflags)\n{\n\tenum ib_access_flags iflags = 0;\n\n\tif (qflags & BNXT_QPLIB_ACCESS_LOCAL_WRITE)\n\t\tiflags |= IB_ACCESS_LOCAL_WRITE;\n\tif (qflags & BNXT_QPLIB_ACCESS_REMOTE_WRITE)\n\t\tiflags |= IB_ACCESS_REMOTE_WRITE;\n\tif (qflags & BNXT_QPLIB_ACCESS_REMOTE_READ)\n\t\tiflags |= IB_ACCESS_REMOTE_READ;\n\tif (qflags & BNXT_QPLIB_ACCESS_REMOTE_ATOMIC)\n\t\tiflags |= IB_ACCESS_REMOTE_ATOMIC;\n\tif (qflags & BNXT_QPLIB_ACCESS_MW_BIND)\n\t\tiflags |= IB_ACCESS_MW_BIND;\n\tif (qflags & BNXT_QPLIB_ACCESS_ZERO_BASED)\n\t\tiflags |= IB_ZERO_BASED;\n\tif (qflags & BNXT_QPLIB_ACCESS_ON_DEMAND)\n\t\tiflags |= IB_ACCESS_ON_DEMAND;\n\treturn iflags;\n};\n\nstatic int bnxt_re_build_sgl(struct ib_sge *ib_sg_list,\n\t\t\t     struct bnxt_qplib_sge *sg_list, int num)\n{\n\tint i, total = 0;\n\n\tfor (i = 0; i < num; i++) {\n\t\tsg_list[i].addr = ib_sg_list[i].addr;\n\t\tsg_list[i].lkey = ib_sg_list[i].lkey;\n\t\tsg_list[i].size = ib_sg_list[i].length;\n\t\ttotal += sg_list[i].size;\n\t}\n\treturn total;\n}\n\n/* Device */\nint bnxt_re_query_device(struct ib_device *ibdev,\n\t\t\t struct ib_device_attr *ib_attr,\n\t\t\t struct ib_udata *udata)\n{\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(ibdev, ibdev);\n\tstruct bnxt_qplib_dev_attr *dev_attr = &rdev->dev_attr;\n\n\tmemset(ib_attr, 0, sizeof(*ib_attr));\n\tmemcpy(&ib_attr->fw_ver, dev_attr->fw_ver,\n\t       min(sizeof(dev_attr->fw_ver),\n\t\t   sizeof(ib_attr->fw_ver)));\n\tbnxt_qplib_get_guid(rdev->netdev->dev_addr,\n\t\t\t    (u8 *)&ib_attr->sys_image_guid);\n\tib_attr->max_mr_size = BNXT_RE_MAX_MR_SIZE;\n\tib_attr->page_size_cap = BNXT_RE_PAGE_SIZE_4K | BNXT_RE_PAGE_SIZE_2M;\n\n\tib_attr->vendor_id = rdev->en_dev->pdev->vendor;\n\tib_attr->vendor_part_id = rdev->en_dev->pdev->device;\n\tib_attr->hw_ver = rdev->en_dev->pdev->subsystem_device;\n\tib_attr->max_qp = dev_attr->max_qp;\n\tib_attr->max_qp_wr = dev_attr->max_qp_wqes;\n\tib_attr->device_cap_flags =\n\t\t\t\t    IB_DEVICE_CURR_QP_STATE_MOD\n\t\t\t\t    | IB_DEVICE_RC_RNR_NAK_GEN\n\t\t\t\t    | IB_DEVICE_SHUTDOWN_PORT\n\t\t\t\t    | IB_DEVICE_SYS_IMAGE_GUID\n\t\t\t\t    | IB_DEVICE_LOCAL_DMA_LKEY\n\t\t\t\t    | IB_DEVICE_RESIZE_MAX_WR\n\t\t\t\t    | IB_DEVICE_PORT_ACTIVE_EVENT\n\t\t\t\t    | IB_DEVICE_N_NOTIFY_CQ\n\t\t\t\t    | IB_DEVICE_MEM_WINDOW\n\t\t\t\t    | IB_DEVICE_MEM_WINDOW_TYPE_2B\n\t\t\t\t    | IB_DEVICE_MEM_MGT_EXTENSIONS;\n\tib_attr->max_send_sge = dev_attr->max_qp_sges;\n\tib_attr->max_recv_sge = dev_attr->max_qp_sges;\n\tib_attr->max_sge_rd = dev_attr->max_qp_sges;\n\tib_attr->max_cq = dev_attr->max_cq;\n\tib_attr->max_cqe = dev_attr->max_cq_wqes;\n\tib_attr->max_mr = dev_attr->max_mr;\n\tib_attr->max_pd = dev_attr->max_pd;\n\tib_attr->max_qp_rd_atom = dev_attr->max_qp_rd_atom;\n\tib_attr->max_qp_init_rd_atom = dev_attr->max_qp_init_rd_atom;\n\tib_attr->atomic_cap = IB_ATOMIC_NONE;\n\tib_attr->masked_atomic_cap = IB_ATOMIC_NONE;\n\n\tib_attr->max_ee_rd_atom = 0;\n\tib_attr->max_res_rd_atom = 0;\n\tib_attr->max_ee_init_rd_atom = 0;\n\tib_attr->max_ee = 0;\n\tib_attr->max_rdd = 0;\n\tib_attr->max_mw = dev_attr->max_mw;\n\tib_attr->max_raw_ipv6_qp = 0;\n\tib_attr->max_raw_ethy_qp = dev_attr->max_raw_ethy_qp;\n\tib_attr->max_mcast_grp = 0;\n\tib_attr->max_mcast_qp_attach = 0;\n\tib_attr->max_total_mcast_qp_attach = 0;\n\tib_attr->max_ah = dev_attr->max_ah;\n\n\tib_attr->max_fmr = 0;\n\tib_attr->max_map_per_fmr = 0;\n\n\tib_attr->max_srq = dev_attr->max_srq;\n\tib_attr->max_srq_wr = dev_attr->max_srq_wqes;\n\tib_attr->max_srq_sge = dev_attr->max_srq_sges;\n\n\tib_attr->max_fast_reg_page_list_len = MAX_PBL_LVL_1_PGS;\n\n\tib_attr->max_pkeys = 1;\n\tib_attr->local_ca_ack_delay = BNXT_RE_DEFAULT_ACK_DELAY;\n\treturn 0;\n}\n\nint bnxt_re_modify_device(struct ib_device *ibdev,\n\t\t\t  int device_modify_mask,\n\t\t\t  struct ib_device_modify *device_modify)\n{\n\tswitch (device_modify_mask) {\n\tcase IB_DEVICE_MODIFY_SYS_IMAGE_GUID:\n\t\t/* Modify the GUID requires the modification of the GID table */\n\t\t/* GUID should be made as READ-ONLY */\n\t\tbreak;\n\tcase IB_DEVICE_MODIFY_NODE_DESC:\n\t\t/* Node Desc should be made as READ-ONLY */\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\n/* Port */\nint bnxt_re_query_port(struct ib_device *ibdev, u8 port_num,\n\t\t       struct ib_port_attr *port_attr)\n{\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(ibdev, ibdev);\n\tstruct bnxt_qplib_dev_attr *dev_attr = &rdev->dev_attr;\n\n\tmemset(port_attr, 0, sizeof(*port_attr));\n\n\tif (netif_running(rdev->netdev) && netif_carrier_ok(rdev->netdev)) {\n\t\tport_attr->state = IB_PORT_ACTIVE;\n\t\tport_attr->phys_state = IB_PORT_PHYS_STATE_LINK_UP;\n\t} else {\n\t\tport_attr->state = IB_PORT_DOWN;\n\t\tport_attr->phys_state = IB_PORT_PHYS_STATE_DISABLED;\n\t}\n\tport_attr->max_mtu = IB_MTU_4096;\n\tport_attr->active_mtu = iboe_get_mtu(rdev->netdev->mtu);\n\tport_attr->gid_tbl_len = dev_attr->max_sgid;\n\tport_attr->port_cap_flags = IB_PORT_CM_SUP | IB_PORT_REINIT_SUP |\n\t\t\t\t    IB_PORT_DEVICE_MGMT_SUP |\n\t\t\t\t    IB_PORT_VENDOR_CLASS_SUP;\n\tport_attr->ip_gids = true;\n\n\tport_attr->max_msg_sz = (u32)BNXT_RE_MAX_MR_SIZE_LOW;\n\tport_attr->bad_pkey_cntr = 0;\n\tport_attr->qkey_viol_cntr = 0;\n\tport_attr->pkey_tbl_len = dev_attr->max_pkey;\n\tport_attr->lid = 0;\n\tport_attr->sm_lid = 0;\n\tport_attr->lmc = 0;\n\tport_attr->max_vl_num = 4;\n\tport_attr->sm_sl = 0;\n\tport_attr->subnet_timeout = 0;\n\tport_attr->init_type_reply = 0;\n\tport_attr->active_speed = rdev->active_speed;\n\tport_attr->active_width = rdev->active_width;\n\n\treturn 0;\n}\n\nint bnxt_re_get_port_immutable(struct ib_device *ibdev, u8 port_num,\n\t\t\t       struct ib_port_immutable *immutable)\n{\n\tstruct ib_port_attr port_attr;\n\n\tif (bnxt_re_query_port(ibdev, port_num, &port_attr))\n\t\treturn -EINVAL;\n\n\timmutable->pkey_tbl_len = port_attr.pkey_tbl_len;\n\timmutable->gid_tbl_len = port_attr.gid_tbl_len;\n\timmutable->core_cap_flags = RDMA_CORE_PORT_IBA_ROCE;\n\timmutable->core_cap_flags |= RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP;\n\timmutable->max_mad_size = IB_MGMT_MAD_SIZE;\n\treturn 0;\n}\n\nvoid bnxt_re_query_fw_str(struct ib_device *ibdev, char *str)\n{\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(ibdev, ibdev);\n\n\tsnprintf(str, IB_FW_VERSION_NAME_MAX, \"%d.%d.%d.%d\",\n\t\t rdev->dev_attr.fw_ver[0], rdev->dev_attr.fw_ver[1],\n\t\t rdev->dev_attr.fw_ver[2], rdev->dev_attr.fw_ver[3]);\n}\n\nint bnxt_re_query_pkey(struct ib_device *ibdev, u8 port_num,\n\t\t       u16 index, u16 *pkey)\n{\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(ibdev, ibdev);\n\n\t/* Ignore port_num */\n\n\tmemset(pkey, 0, sizeof(*pkey));\n\treturn bnxt_qplib_get_pkey(&rdev->qplib_res,\n\t\t\t\t   &rdev->qplib_res.pkey_tbl, index, pkey);\n}\n\nint bnxt_re_query_gid(struct ib_device *ibdev, u8 port_num,\n\t\t      int index, union ib_gid *gid)\n{\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(ibdev, ibdev);\n\tint rc = 0;\n\n\t/* Ignore port_num */\n\tmemset(gid, 0, sizeof(*gid));\n\trc = bnxt_qplib_get_sgid(&rdev->qplib_res,\n\t\t\t\t &rdev->qplib_res.sgid_tbl, index,\n\t\t\t\t (struct bnxt_qplib_gid *)gid);\n\treturn rc;\n}\n\nint bnxt_re_del_gid(const struct ib_gid_attr *attr, void **context)\n{\n\tint rc = 0;\n\tstruct bnxt_re_gid_ctx *ctx, **ctx_tbl;\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(attr->device, ibdev);\n\tstruct bnxt_qplib_sgid_tbl *sgid_tbl = &rdev->qplib_res.sgid_tbl;\n\tstruct bnxt_qplib_gid *gid_to_del;\n\tu16 vlan_id = 0xFFFF;\n\n\t/* Delete the entry from the hardware */\n\tctx = *context;\n\tif (!ctx)\n\t\treturn -EINVAL;\n\n\tif (sgid_tbl && sgid_tbl->active) {\n\t\tif (ctx->idx >= sgid_tbl->max)\n\t\t\treturn -EINVAL;\n\t\tgid_to_del = &sgid_tbl->tbl[ctx->idx].gid;\n\t\tvlan_id = sgid_tbl->tbl[ctx->idx].vlan_id;\n\t\t/* DEL_GID is called in WQ context(netdevice_event_work_handler)\n\t\t * or via the ib_unregister_device path. In the former case QP1\n\t\t * may not be destroyed yet, in which case just return as FW\n\t\t * needs that entry to be present and will fail it's deletion.\n\t\t * We could get invoked again after QP1 is destroyed OR get an\n\t\t * ADD_GID call with a different GID value for the same index\n\t\t * where we issue MODIFY_GID cmd to update the GID entry -- TBD\n\t\t */\n\t\tif (ctx->idx == 0 &&\n\t\t    rdma_link_local_addr((struct in6_addr *)gid_to_del) &&\n\t\t    ctx->refcnt == 1 && rdev->qp1_sqp) {\n\t\t\tdev_dbg(rdev_to_dev(rdev),\n\t\t\t\t\"Trying to delete GID0 while QP1 is alive\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tctx->refcnt--;\n\t\tif (!ctx->refcnt) {\n\t\t\trc = bnxt_qplib_del_sgid(sgid_tbl, gid_to_del,\n\t\t\t\t\t\t vlan_id,  true);\n\t\t\tif (rc) {\n\t\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\t\"Failed to remove GID: %#x\", rc);\n\t\t\t} else {\n\t\t\t\tctx_tbl = sgid_tbl->ctx;\n\t\t\t\tctx_tbl[ctx->idx] = NULL;\n\t\t\t\tkfree(ctx);\n\t\t\t}\n\t\t}\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\treturn rc;\n}\n\nint bnxt_re_add_gid(const struct ib_gid_attr *attr, void **context)\n{\n\tint rc;\n\tu32 tbl_idx = 0;\n\tu16 vlan_id = 0xFFFF;\n\tstruct bnxt_re_gid_ctx *ctx, **ctx_tbl;\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(attr->device, ibdev);\n\tstruct bnxt_qplib_sgid_tbl *sgid_tbl = &rdev->qplib_res.sgid_tbl;\n\n\trc = rdma_read_gid_l2_fields(attr, &vlan_id, NULL);\n\tif (rc)\n\t\treturn rc;\n\n\trc = bnxt_qplib_add_sgid(sgid_tbl, (struct bnxt_qplib_gid *)&attr->gid,\n\t\t\t\t rdev->qplib_res.netdev->dev_addr,\n\t\t\t\t vlan_id, true, &tbl_idx);\n\tif (rc == -EALREADY) {\n\t\tctx_tbl = sgid_tbl->ctx;\n\t\tctx_tbl[tbl_idx]->refcnt++;\n\t\t*context = ctx_tbl[tbl_idx];\n\t\treturn 0;\n\t}\n\n\tif (rc < 0) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to add GID: %#x\", rc);\n\t\treturn rc;\n\t}\n\n\tctx = kmalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\tctx_tbl = sgid_tbl->ctx;\n\tctx->idx = tbl_idx;\n\tctx->refcnt = 1;\n\tctx_tbl[tbl_idx] = ctx;\n\t*context = ctx;\n\n\treturn rc;\n}\n\nenum rdma_link_layer bnxt_re_get_link_layer(struct ib_device *ibdev,\n\t\t\t\t\t    u8 port_num)\n{\n\treturn IB_LINK_LAYER_ETHERNET;\n}\n\n#define\tBNXT_RE_FENCE_PBL_SIZE\tDIV_ROUND_UP(BNXT_RE_FENCE_BYTES, PAGE_SIZE)\n\nstatic void bnxt_re_create_fence_wqe(struct bnxt_re_pd *pd)\n{\n\tstruct bnxt_re_fence_data *fence = &pd->fence;\n\tstruct ib_mr *ib_mr = &fence->mr->ib_mr;\n\tstruct bnxt_qplib_swqe *wqe = &fence->bind_wqe;\n\n\tmemset(wqe, 0, sizeof(*wqe));\n\twqe->type = BNXT_QPLIB_SWQE_TYPE_BIND_MW;\n\twqe->wr_id = BNXT_QPLIB_FENCE_WRID;\n\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SIGNAL_COMP;\n\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_UC_FENCE;\n\twqe->bind.zero_based = false;\n\twqe->bind.parent_l_key = ib_mr->lkey;\n\twqe->bind.va = (u64)(unsigned long)fence->va;\n\twqe->bind.length = fence->size;\n\twqe->bind.access_cntl = __from_ib_access_flags(IB_ACCESS_REMOTE_READ);\n\twqe->bind.mw_type = SQ_BIND_MW_TYPE_TYPE1;\n\n\t/* Save the initial rkey in fence structure for now;\n\t * wqe->bind.r_key will be set at (re)bind time.\n\t */\n\tfence->bind_rkey = ib_inc_rkey(fence->mw->rkey);\n}\n\nstatic int bnxt_re_bind_fence_mw(struct bnxt_qplib_qp *qplib_qp)\n{\n\tstruct bnxt_re_qp *qp = container_of(qplib_qp, struct bnxt_re_qp,\n\t\t\t\t\t     qplib_qp);\n\tstruct ib_pd *ib_pd = qp->ib_qp.pd;\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tstruct bnxt_re_fence_data *fence = &pd->fence;\n\tstruct bnxt_qplib_swqe *fence_wqe = &fence->bind_wqe;\n\tstruct bnxt_qplib_swqe wqe;\n\tint rc;\n\n\tmemcpy(&wqe, fence_wqe, sizeof(wqe));\n\twqe.bind.r_key = fence->bind_rkey;\n\tfence->bind_rkey = ib_inc_rkey(fence->bind_rkey);\n\n\tdev_dbg(rdev_to_dev(qp->rdev),\n\t\t\"Posting bind fence-WQE: rkey: %#x QP: %d PD: %p\\n\",\n\t\twqe.bind.r_key, qp->qplib_qp.id, pd);\n\trc = bnxt_qplib_post_send(&qp->qplib_qp, &wqe);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(qp->rdev), \"Failed to bind fence-WQE\\n\");\n\t\treturn rc;\n\t}\n\tbnxt_qplib_post_send_db(&qp->qplib_qp);\n\n\treturn rc;\n}\n\nstatic void bnxt_re_destroy_fence_mr(struct bnxt_re_pd *pd)\n{\n\tstruct bnxt_re_fence_data *fence = &pd->fence;\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct device *dev = &rdev->en_dev->pdev->dev;\n\tstruct bnxt_re_mr *mr = fence->mr;\n\n\tif (fence->mw) {\n\t\tbnxt_re_dealloc_mw(fence->mw);\n\t\tfence->mw = NULL;\n\t}\n\tif (mr) {\n\t\tif (mr->ib_mr.rkey)\n\t\t\tbnxt_qplib_dereg_mrw(&rdev->qplib_res, &mr->qplib_mr,\n\t\t\t\t\t     true);\n\t\tif (mr->ib_mr.lkey)\n\t\t\tbnxt_qplib_free_mrw(&rdev->qplib_res, &mr->qplib_mr);\n\t\tkfree(mr);\n\t\tfence->mr = NULL;\n\t}\n\tif (fence->dma_addr) {\n\t\tdma_unmap_single(dev, fence->dma_addr, BNXT_RE_FENCE_BYTES,\n\t\t\t\t DMA_BIDIRECTIONAL);\n\t\tfence->dma_addr = 0;\n\t}\n}\n\nstatic int bnxt_re_create_fence_mr(struct bnxt_re_pd *pd)\n{\n\tint mr_access_flags = IB_ACCESS_LOCAL_WRITE | IB_ACCESS_MW_BIND;\n\tstruct bnxt_re_fence_data *fence = &pd->fence;\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct device *dev = &rdev->en_dev->pdev->dev;\n\tstruct bnxt_re_mr *mr = NULL;\n\tdma_addr_t dma_addr = 0;\n\tstruct ib_mw *mw;\n\tu64 pbl_tbl;\n\tint rc;\n\n\tdma_addr = dma_map_single(dev, fence->va, BNXT_RE_FENCE_BYTES,\n\t\t\t\t  DMA_BIDIRECTIONAL);\n\trc = dma_mapping_error(dev, dma_addr);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to dma-map fence-MR-mem\\n\");\n\t\trc = -EIO;\n\t\tfence->dma_addr = 0;\n\t\tgoto fail;\n\t}\n\tfence->dma_addr = dma_addr;\n\n\t/* Allocate a MR */\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr) {\n\t\trc = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tfence->mr = mr;\n\tmr->rdev = rdev;\n\tmr->qplib_mr.pd = &pd->qplib_pd;\n\tmr->qplib_mr.type = CMDQ_ALLOCATE_MRW_MRW_FLAGS_PMR;\n\tmr->qplib_mr.flags = __from_ib_access_flags(mr_access_flags);\n\trc = bnxt_qplib_alloc_mrw(&rdev->qplib_res, &mr->qplib_mr);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to alloc fence-HW-MR\\n\");\n\t\tgoto fail;\n\t}\n\n\t/* Register MR */\n\tmr->ib_mr.lkey = mr->qplib_mr.lkey;\n\tmr->qplib_mr.va = (u64)(unsigned long)fence->va;\n\tmr->qplib_mr.total_size = BNXT_RE_FENCE_BYTES;\n\tpbl_tbl = dma_addr;\n\trc = bnxt_qplib_reg_mr(&rdev->qplib_res, &mr->qplib_mr, &pbl_tbl,\n\t\t\t       BNXT_RE_FENCE_PBL_SIZE, false, PAGE_SIZE);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to register fence-MR\\n\");\n\t\tgoto fail;\n\t}\n\tmr->ib_mr.rkey = mr->qplib_mr.rkey;\n\n\t/* Create a fence MW only for kernel consumers */\n\tmw = bnxt_re_alloc_mw(&pd->ib_pd, IB_MW_TYPE_1, NULL);\n\tif (IS_ERR(mw)) {\n\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\"Failed to create fence-MW for PD: %p\\n\", pd);\n\t\trc = PTR_ERR(mw);\n\t\tgoto fail;\n\t}\n\tfence->mw = mw;\n\n\tbnxt_re_create_fence_wqe(pd);\n\treturn 0;\n\nfail:\n\tbnxt_re_destroy_fence_mr(pd);\n\treturn rc;\n}\n\n/* Protection Domains */\nvoid bnxt_re_dealloc_pd(struct ib_pd *ib_pd, struct ib_udata *udata)\n{\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\n\tbnxt_re_destroy_fence_mr(pd);\n\n\tif (pd->qplib_pd.id)\n\t\tbnxt_qplib_dealloc_pd(&rdev->qplib_res, &rdev->qplib_res.pd_tbl,\n\t\t\t\t      &pd->qplib_pd);\n}\n\nint bnxt_re_alloc_pd(struct ib_pd *ibpd, struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = ibpd->device;\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(ibdev, ibdev);\n\tstruct bnxt_re_ucontext *ucntx = rdma_udata_to_drv_context(\n\t\tudata, struct bnxt_re_ucontext, ib_uctx);\n\tstruct bnxt_re_pd *pd = container_of(ibpd, struct bnxt_re_pd, ib_pd);\n\tint rc;\n\n\tpd->rdev = rdev;\n\tif (bnxt_qplib_alloc_pd(&rdev->qplib_res.pd_tbl, &pd->qplib_pd)) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to allocate HW PD\");\n\t\trc = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\tif (udata) {\n\t\tstruct bnxt_re_pd_resp resp;\n\n\t\tif (!ucntx->dpi.dbr) {\n\t\t\t/* Allocate DPI in alloc_pd to avoid failing of\n\t\t\t * ibv_devinfo and family of application when DPIs\n\t\t\t * are depleted.\n\t\t\t */\n\t\t\tif (bnxt_qplib_alloc_dpi(&rdev->qplib_res.dpi_tbl,\n\t\t\t\t\t\t &ucntx->dpi, ucntx)) {\n\t\t\t\trc = -ENOMEM;\n\t\t\t\tgoto dbfail;\n\t\t\t}\n\t\t}\n\n\t\tresp.pdid = pd->qplib_pd.id;\n\t\t/* Still allow mapping this DBR to the new user PD. */\n\t\tresp.dpi = ucntx->dpi.dpi;\n\t\tresp.dbr = (u64)ucntx->dpi.umdbr;\n\n\t\trc = ib_copy_to_udata(udata, &resp, sizeof(resp));\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Failed to copy user response\\n\");\n\t\t\tgoto dbfail;\n\t\t}\n\t}\n\n\tif (!udata)\n\t\tif (bnxt_re_create_fence_mr(pd))\n\t\t\tdev_warn(rdev_to_dev(rdev),\n\t\t\t\t \"Failed to create Fence-MR\\n\");\n\treturn 0;\ndbfail:\n\tbnxt_qplib_dealloc_pd(&rdev->qplib_res, &rdev->qplib_res.pd_tbl,\n\t\t\t      &pd->qplib_pd);\nfail:\n\treturn rc;\n}\n\n/* Address Handles */\nvoid bnxt_re_destroy_ah(struct ib_ah *ib_ah, u32 flags)\n{\n\tstruct bnxt_re_ah *ah = container_of(ib_ah, struct bnxt_re_ah, ib_ah);\n\tstruct bnxt_re_dev *rdev = ah->rdev;\n\n\tbnxt_qplib_destroy_ah(&rdev->qplib_res, &ah->qplib_ah,\n\t\t\t      !(flags & RDMA_DESTROY_AH_SLEEPABLE));\n}\n\nstatic u8 bnxt_re_stack_to_dev_nw_type(enum rdma_network_type ntype)\n{\n\tu8 nw_type;\n\n\tswitch (ntype) {\n\tcase RDMA_NETWORK_IPV4:\n\t\tnw_type = CMDQ_CREATE_AH_TYPE_V2IPV4;\n\t\tbreak;\n\tcase RDMA_NETWORK_IPV6:\n\t\tnw_type = CMDQ_CREATE_AH_TYPE_V2IPV6;\n\t\tbreak;\n\tdefault:\n\t\tnw_type = CMDQ_CREATE_AH_TYPE_V1;\n\t\tbreak;\n\t}\n\treturn nw_type;\n}\n\nint bnxt_re_create_ah(struct ib_ah *ib_ah, struct rdma_ah_attr *ah_attr,\n\t\t      u32 flags, struct ib_udata *udata)\n{\n\tstruct ib_pd *ib_pd = ib_ah->pd;\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tconst struct ib_global_route *grh = rdma_ah_read_grh(ah_attr);\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tconst struct ib_gid_attr *sgid_attr;\n\tstruct bnxt_re_ah *ah = container_of(ib_ah, struct bnxt_re_ah, ib_ah);\n\tu8 nw_type;\n\tint rc;\n\n\tif (!(rdma_ah_get_ah_flags(ah_attr) & IB_AH_GRH)) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to alloc AH: GRH not set\");\n\t\treturn -EINVAL;\n\t}\n\n\tah->rdev = rdev;\n\tah->qplib_ah.pd = &pd->qplib_pd;\n\n\t/* Supply the configuration for the HW */\n\tmemcpy(ah->qplib_ah.dgid.data, grh->dgid.raw,\n\t       sizeof(union ib_gid));\n\t/*\n\t * If RoCE V2 is enabled, stack will have two entries for\n\t * each GID entry. Avoiding this duplicte entry in HW. Dividing\n\t * the GID index by 2 for RoCE V2\n\t */\n\tah->qplib_ah.sgid_index = grh->sgid_index / 2;\n\tah->qplib_ah.host_sgid_index = grh->sgid_index;\n\tah->qplib_ah.traffic_class = grh->traffic_class;\n\tah->qplib_ah.flow_label = grh->flow_label;\n\tah->qplib_ah.hop_limit = grh->hop_limit;\n\tah->qplib_ah.sl = rdma_ah_get_sl(ah_attr);\n\n\tsgid_attr = grh->sgid_attr;\n\t/* Get network header type for this GID */\n\tnw_type = rdma_gid_attr_network_type(sgid_attr);\n\tah->qplib_ah.nw_type = bnxt_re_stack_to_dev_nw_type(nw_type);\n\n\tmemcpy(ah->qplib_ah.dmac, ah_attr->roce.dmac, ETH_ALEN);\n\trc = bnxt_qplib_create_ah(&rdev->qplib_res, &ah->qplib_ah,\n\t\t\t\t  !(flags & RDMA_CREATE_AH_SLEEPABLE));\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to allocate HW AH\");\n\t\treturn rc;\n\t}\n\n\t/* Write AVID to shared page. */\n\tif (udata) {\n\t\tstruct bnxt_re_ucontext *uctx = rdma_udata_to_drv_context(\n\t\t\tudata, struct bnxt_re_ucontext, ib_uctx);\n\t\tunsigned long flag;\n\t\tu32 *wrptr;\n\n\t\tspin_lock_irqsave(&uctx->sh_lock, flag);\n\t\twrptr = (u32 *)(uctx->shpg + BNXT_RE_AVID_OFFT);\n\t\t*wrptr = ah->qplib_ah.id;\n\t\twmb(); /* make sure cache is updated. */\n\t\tspin_unlock_irqrestore(&uctx->sh_lock, flag);\n\t}\n\n\treturn 0;\n}\n\nint bnxt_re_modify_ah(struct ib_ah *ib_ah, struct rdma_ah_attr *ah_attr)\n{\n\treturn 0;\n}\n\nint bnxt_re_query_ah(struct ib_ah *ib_ah, struct rdma_ah_attr *ah_attr)\n{\n\tstruct bnxt_re_ah *ah = container_of(ib_ah, struct bnxt_re_ah, ib_ah);\n\n\tah_attr->type = ib_ah->type;\n\trdma_ah_set_sl(ah_attr, ah->qplib_ah.sl);\n\tmemcpy(ah_attr->roce.dmac, ah->qplib_ah.dmac, ETH_ALEN);\n\trdma_ah_set_grh(ah_attr, NULL, 0,\n\t\t\tah->qplib_ah.host_sgid_index,\n\t\t\t0, ah->qplib_ah.traffic_class);\n\trdma_ah_set_dgid_raw(ah_attr, ah->qplib_ah.dgid.data);\n\trdma_ah_set_port_num(ah_attr, 1);\n\trdma_ah_set_static_rate(ah_attr, 0);\n\treturn 0;\n}\n\nunsigned long bnxt_re_lock_cqs(struct bnxt_re_qp *qp)\n\t__acquires(&qp->scq->cq_lock) __acquires(&qp->rcq->cq_lock)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->scq->cq_lock, flags);\n\tif (qp->rcq != qp->scq)\n\t\tspin_lock(&qp->rcq->cq_lock);\n\telse\n\t\t__acquire(&qp->rcq->cq_lock);\n\n\treturn flags;\n}\n\nvoid bnxt_re_unlock_cqs(struct bnxt_re_qp *qp,\n\t\t\tunsigned long flags)\n\t__releases(&qp->scq->cq_lock) __releases(&qp->rcq->cq_lock)\n{\n\tif (qp->rcq != qp->scq)\n\t\tspin_unlock(&qp->rcq->cq_lock);\n\telse\n\t\t__release(&qp->rcq->cq_lock);\n\tspin_unlock_irqrestore(&qp->scq->cq_lock, flags);\n}\n\n/* Queue Pairs */\nint bnxt_re_destroy_qp(struct ib_qp *ib_qp, struct ib_udata *udata)\n{\n\tstruct bnxt_re_qp *qp = container_of(ib_qp, struct bnxt_re_qp, ib_qp);\n\tstruct bnxt_re_dev *rdev = qp->rdev;\n\tunsigned int flags;\n\tint rc;\n\n\tbnxt_qplib_flush_cqn_wq(&qp->qplib_qp);\n\trc = bnxt_qplib_destroy_qp(&rdev->qplib_res, &qp->qplib_qp);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to destroy HW QP\");\n\t\treturn rc;\n\t}\n\n\tif (rdma_is_kernel_res(&qp->ib_qp.res)) {\n\t\tflags = bnxt_re_lock_cqs(qp);\n\t\tbnxt_qplib_clean_qp(&qp->qplib_qp);\n\t\tbnxt_re_unlock_cqs(qp, flags);\n\t}\n\n\tbnxt_qplib_free_qp_res(&rdev->qplib_res, &qp->qplib_qp);\n\n\tif (ib_qp->qp_type == IB_QPT_GSI && rdev->qp1_sqp) {\n\t\tbnxt_qplib_destroy_ah(&rdev->qplib_res, &rdev->sqp_ah->qplib_ah,\n\t\t\t\t      false);\n\n\t\tbnxt_qplib_clean_qp(&qp->qplib_qp);\n\t\trc = bnxt_qplib_destroy_qp(&rdev->qplib_res,\n\t\t\t\t\t   &rdev->qp1_sqp->qplib_qp);\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Failed to destroy Shadow QP\");\n\t\t\treturn rc;\n\t\t}\n\t\tbnxt_qplib_free_qp_res(&rdev->qplib_res,\n\t\t\t\t       &rdev->qp1_sqp->qplib_qp);\n\t\tmutex_lock(&rdev->qp_lock);\n\t\tlist_del(&rdev->qp1_sqp->list);\n\t\tatomic_dec(&rdev->qp_count);\n\t\tmutex_unlock(&rdev->qp_lock);\n\n\t\tkfree(rdev->sqp_ah);\n\t\tkfree(rdev->qp1_sqp);\n\t\trdev->qp1_sqp = NULL;\n\t\trdev->sqp_ah = NULL;\n\t}\n\n\tib_umem_release(qp->rumem);\n\tib_umem_release(qp->sumem);\n\n\tmutex_lock(&rdev->qp_lock);\n\tlist_del(&qp->list);\n\tatomic_dec(&rdev->qp_count);\n\tmutex_unlock(&rdev->qp_lock);\n\tkfree(qp);\n\treturn 0;\n}\n\nstatic u8 __from_ib_qp_type(enum ib_qp_type type)\n{\n\tswitch (type) {\n\tcase IB_QPT_GSI:\n\t\treturn CMDQ_CREATE_QP1_TYPE_GSI;\n\tcase IB_QPT_RC:\n\t\treturn CMDQ_CREATE_QP_TYPE_RC;\n\tcase IB_QPT_UD:\n\t\treturn CMDQ_CREATE_QP_TYPE_UD;\n\tdefault:\n\t\treturn IB_QPT_MAX;\n\t}\n}\n\nstatic int bnxt_re_init_user_qp(struct bnxt_re_dev *rdev, struct bnxt_re_pd *pd,\n\t\t\t\tstruct bnxt_re_qp *qp, struct ib_udata *udata)\n{\n\tstruct bnxt_re_qp_req ureq;\n\tstruct bnxt_qplib_qp *qplib_qp = &qp->qplib_qp;\n\tstruct ib_umem *umem;\n\tint bytes = 0, psn_sz;\n\tstruct bnxt_re_ucontext *cntx = rdma_udata_to_drv_context(\n\t\tudata, struct bnxt_re_ucontext, ib_uctx);\n\n\tif (ib_copy_from_udata(&ureq, udata, sizeof(ureq)))\n\t\treturn -EFAULT;\n\n\tbytes = (qplib_qp->sq.max_wqe * BNXT_QPLIB_MAX_SQE_ENTRY_SIZE);\n\t/* Consider mapping PSN search memory only for RC QPs. */\n\tif (qplib_qp->type == CMDQ_CREATE_QP_TYPE_RC) {\n\t\tpsn_sz = bnxt_qplib_is_chip_gen_p5(&rdev->chip_ctx) ?\n\t\t\t\t\tsizeof(struct sq_psn_search_ext) :\n\t\t\t\t\tsizeof(struct sq_psn_search);\n\t\tbytes += (qplib_qp->sq.max_wqe * psn_sz);\n\t}\n\tbytes = PAGE_ALIGN(bytes);\n\tumem = ib_umem_get(udata, ureq.qpsva, bytes, IB_ACCESS_LOCAL_WRITE, 1);\n\tif (IS_ERR(umem))\n\t\treturn PTR_ERR(umem);\n\n\tqp->sumem = umem;\n\tqplib_qp->sq.sg_info.sglist = umem->sg_head.sgl;\n\tqplib_qp->sq.sg_info.npages = ib_umem_num_pages(umem);\n\tqplib_qp->sq.sg_info.nmap = umem->nmap;\n\tqplib_qp->qp_handle = ureq.qp_handle;\n\n\tif (!qp->qplib_qp.srq) {\n\t\tbytes = (qplib_qp->rq.max_wqe * BNXT_QPLIB_MAX_RQE_ENTRY_SIZE);\n\t\tbytes = PAGE_ALIGN(bytes);\n\t\tumem = ib_umem_get(udata, ureq.qprva, bytes,\n\t\t\t\t   IB_ACCESS_LOCAL_WRITE, 1);\n\t\tif (IS_ERR(umem))\n\t\t\tgoto rqfail;\n\t\tqp->rumem = umem;\n\t\tqplib_qp->rq.sg_info.sglist = umem->sg_head.sgl;\n\t\tqplib_qp->rq.sg_info.npages = ib_umem_num_pages(umem);\n\t\tqplib_qp->rq.sg_info.nmap = umem->nmap;\n\t}\n\n\tqplib_qp->dpi = &cntx->dpi;\n\treturn 0;\nrqfail:\n\tib_umem_release(qp->sumem);\n\tqp->sumem = NULL;\n\tmemset(&qplib_qp->sq.sg_info, 0, sizeof(qplib_qp->sq.sg_info));\n\n\treturn PTR_ERR(umem);\n}\n\nstatic struct bnxt_re_ah *bnxt_re_create_shadow_qp_ah\n\t\t\t\t(struct bnxt_re_pd *pd,\n\t\t\t\t struct bnxt_qplib_res *qp1_res,\n\t\t\t\t struct bnxt_qplib_qp *qp1_qp)\n{\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct bnxt_re_ah *ah;\n\tunion ib_gid sgid;\n\tint rc;\n\n\tah = kzalloc(sizeof(*ah), GFP_KERNEL);\n\tif (!ah)\n\t\treturn NULL;\n\n\tah->rdev = rdev;\n\tah->qplib_ah.pd = &pd->qplib_pd;\n\n\trc = bnxt_re_query_gid(&rdev->ibdev, 1, 0, &sgid);\n\tif (rc)\n\t\tgoto fail;\n\n\t/* supply the dgid data same as sgid */\n\tmemcpy(ah->qplib_ah.dgid.data, &sgid.raw,\n\t       sizeof(union ib_gid));\n\tah->qplib_ah.sgid_index = 0;\n\n\tah->qplib_ah.traffic_class = 0;\n\tah->qplib_ah.flow_label = 0;\n\tah->qplib_ah.hop_limit = 1;\n\tah->qplib_ah.sl = 0;\n\t/* Have DMAC same as SMAC */\n\tether_addr_copy(ah->qplib_ah.dmac, rdev->netdev->dev_addr);\n\n\trc = bnxt_qplib_create_ah(&rdev->qplib_res, &ah->qplib_ah, false);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\"Failed to allocate HW AH for Shadow QP\");\n\t\tgoto fail;\n\t}\n\n\treturn ah;\n\nfail:\n\tkfree(ah);\n\treturn NULL;\n}\n\nstatic struct bnxt_re_qp *bnxt_re_create_shadow_qp\n\t\t\t\t(struct bnxt_re_pd *pd,\n\t\t\t\t struct bnxt_qplib_res *qp1_res,\n\t\t\t\t struct bnxt_qplib_qp *qp1_qp)\n{\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct bnxt_re_qp *qp;\n\tint rc;\n\n\tqp = kzalloc(sizeof(*qp), GFP_KERNEL);\n\tif (!qp)\n\t\treturn NULL;\n\n\tqp->rdev = rdev;\n\n\t/* Initialize the shadow QP structure from the QP1 values */\n\tether_addr_copy(qp->qplib_qp.smac, rdev->netdev->dev_addr);\n\n\tqp->qplib_qp.pd = &pd->qplib_pd;\n\tqp->qplib_qp.qp_handle = (u64)(unsigned long)(&qp->qplib_qp);\n\tqp->qplib_qp.type = IB_QPT_UD;\n\n\tqp->qplib_qp.max_inline_data = 0;\n\tqp->qplib_qp.sig_type = true;\n\n\t/* Shadow QP SQ depth should be same as QP1 RQ depth */\n\tqp->qplib_qp.sq.max_wqe = qp1_qp->rq.max_wqe;\n\tqp->qplib_qp.sq.max_sge = 2;\n\t/* Q full delta can be 1 since it is internal QP */\n\tqp->qplib_qp.sq.q_full_delta = 1;\n\n\tqp->qplib_qp.scq = qp1_qp->scq;\n\tqp->qplib_qp.rcq = qp1_qp->rcq;\n\n\tqp->qplib_qp.rq.max_wqe = qp1_qp->rq.max_wqe;\n\tqp->qplib_qp.rq.max_sge = qp1_qp->rq.max_sge;\n\t/* Q full delta can be 1 since it is internal QP */\n\tqp->qplib_qp.rq.q_full_delta = 1;\n\n\tqp->qplib_qp.mtu = qp1_qp->mtu;\n\n\tqp->qplib_qp.sq_hdr_buf_size = 0;\n\tqp->qplib_qp.rq_hdr_buf_size = BNXT_QPLIB_MAX_GRH_HDR_SIZE_IPV6;\n\tqp->qplib_qp.dpi = &rdev->dpi_privileged;\n\n\trc = bnxt_qplib_create_qp(qp1_res, &qp->qplib_qp);\n\tif (rc)\n\t\tgoto fail;\n\n\trdev->sqp_id = qp->qplib_qp.id;\n\n\tspin_lock_init(&qp->sq_lock);\n\tINIT_LIST_HEAD(&qp->list);\n\tmutex_lock(&rdev->qp_lock);\n\tlist_add_tail(&qp->list, &rdev->qp_list);\n\tatomic_inc(&rdev->qp_count);\n\tmutex_unlock(&rdev->qp_lock);\n\treturn qp;\nfail:\n\tkfree(qp);\n\treturn NULL;\n}\n\nstruct ib_qp *bnxt_re_create_qp(struct ib_pd *ib_pd,\n\t\t\t\tstruct ib_qp_init_attr *qp_init_attr,\n\t\t\t\tstruct ib_udata *udata)\n{\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct bnxt_qplib_dev_attr *dev_attr = &rdev->dev_attr;\n\tstruct bnxt_re_qp *qp;\n\tstruct bnxt_re_cq *cq;\n\tstruct bnxt_re_srq *srq;\n\tint rc, entries;\n\n\tif ((qp_init_attr->cap.max_send_wr > dev_attr->max_qp_wqes) ||\n\t    (qp_init_attr->cap.max_recv_wr > dev_attr->max_qp_wqes) ||\n\t    (qp_init_attr->cap.max_send_sge > dev_attr->max_qp_sges) ||\n\t    (qp_init_attr->cap.max_recv_sge > dev_attr->max_qp_sges) ||\n\t    (qp_init_attr->cap.max_inline_data > dev_attr->max_inline_data))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tqp = kzalloc(sizeof(*qp), GFP_KERNEL);\n\tif (!qp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tqp->rdev = rdev;\n\tether_addr_copy(qp->qplib_qp.smac, rdev->netdev->dev_addr);\n\tqp->qplib_qp.pd = &pd->qplib_pd;\n\tqp->qplib_qp.qp_handle = (u64)(unsigned long)(&qp->qplib_qp);\n\tqp->qplib_qp.type = __from_ib_qp_type(qp_init_attr->qp_type);\n\n\tif (qp_init_attr->qp_type == IB_QPT_GSI &&\n\t    bnxt_qplib_is_chip_gen_p5(&rdev->chip_ctx))\n\t\tqp->qplib_qp.type = CMDQ_CREATE_QP_TYPE_GSI;\n\tif (qp->qplib_qp.type == IB_QPT_MAX) {\n\t\tdev_err(rdev_to_dev(rdev), \"QP type 0x%x not supported\",\n\t\t\tqp->qplib_qp.type);\n\t\trc = -EINVAL;\n\t\tgoto fail;\n\t}\n\n\tqp->qplib_qp.max_inline_data = qp_init_attr->cap.max_inline_data;\n\tqp->qplib_qp.sig_type = ((qp_init_attr->sq_sig_type ==\n\t\t\t\t  IB_SIGNAL_ALL_WR) ? true : false);\n\n\tqp->qplib_qp.sq.max_sge = qp_init_attr->cap.max_send_sge;\n\tif (qp->qplib_qp.sq.max_sge > dev_attr->max_qp_sges)\n\t\tqp->qplib_qp.sq.max_sge = dev_attr->max_qp_sges;\n\n\tif (qp_init_attr->send_cq) {\n\t\tcq = container_of(qp_init_attr->send_cq, struct bnxt_re_cq,\n\t\t\t\t  ib_cq);\n\t\tif (!cq) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"Send CQ not found\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto fail;\n\t\t}\n\t\tqp->qplib_qp.scq = &cq->qplib_cq;\n\t\tqp->scq = cq;\n\t}\n\n\tif (qp_init_attr->recv_cq) {\n\t\tcq = container_of(qp_init_attr->recv_cq, struct bnxt_re_cq,\n\t\t\t\t  ib_cq);\n\t\tif (!cq) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"Receive CQ not found\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto fail;\n\t\t}\n\t\tqp->qplib_qp.rcq = &cq->qplib_cq;\n\t\tqp->rcq = cq;\n\t}\n\n\tif (qp_init_attr->srq) {\n\t\tsrq = container_of(qp_init_attr->srq, struct bnxt_re_srq,\n\t\t\t\t   ib_srq);\n\t\tif (!srq) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"SRQ not found\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto fail;\n\t\t}\n\t\tqp->qplib_qp.srq = &srq->qplib_srq;\n\t\tqp->qplib_qp.rq.max_wqe = 0;\n\t} else {\n\t\t/* Allocate 1 more than what's provided so posting max doesn't\n\t\t * mean empty\n\t\t */\n\t\tentries = roundup_pow_of_two(qp_init_attr->cap.max_recv_wr + 1);\n\t\tqp->qplib_qp.rq.max_wqe = min_t(u32, entries,\n\t\t\t\t\t\tdev_attr->max_qp_wqes + 1);\n\n\t\tqp->qplib_qp.rq.q_full_delta = qp->qplib_qp.rq.max_wqe -\n\t\t\t\t\t\tqp_init_attr->cap.max_recv_wr;\n\n\t\tqp->qplib_qp.rq.max_sge = qp_init_attr->cap.max_recv_sge;\n\t\tif (qp->qplib_qp.rq.max_sge > dev_attr->max_qp_sges)\n\t\t\tqp->qplib_qp.rq.max_sge = dev_attr->max_qp_sges;\n\t}\n\n\tqp->qplib_qp.mtu = ib_mtu_enum_to_int(iboe_get_mtu(rdev->netdev->mtu));\n\n\tif (qp_init_attr->qp_type == IB_QPT_GSI &&\n\t    !(bnxt_qplib_is_chip_gen_p5(&rdev->chip_ctx))) {\n\t\t/* Allocate 1 more than what's provided */\n\t\tentries = roundup_pow_of_two(qp_init_attr->cap.max_send_wr + 1);\n\t\tqp->qplib_qp.sq.max_wqe = min_t(u32, entries,\n\t\t\t\t\t\tdev_attr->max_qp_wqes + 1);\n\t\tqp->qplib_qp.sq.q_full_delta = qp->qplib_qp.sq.max_wqe -\n\t\t\t\t\t\tqp_init_attr->cap.max_send_wr;\n\t\tqp->qplib_qp.rq.max_sge = dev_attr->max_qp_sges;\n\t\tif (qp->qplib_qp.rq.max_sge > dev_attr->max_qp_sges)\n\t\t\tqp->qplib_qp.rq.max_sge = dev_attr->max_qp_sges;\n\t\tqp->qplib_qp.sq.max_sge++;\n\t\tif (qp->qplib_qp.sq.max_sge > dev_attr->max_qp_sges)\n\t\t\tqp->qplib_qp.sq.max_sge = dev_attr->max_qp_sges;\n\n\t\tqp->qplib_qp.rq_hdr_buf_size =\n\t\t\t\t\tBNXT_QPLIB_MAX_QP1_RQ_HDR_SIZE_V2;\n\n\t\tqp->qplib_qp.sq_hdr_buf_size =\n\t\t\t\t\tBNXT_QPLIB_MAX_QP1_SQ_HDR_SIZE_V2;\n\t\tqp->qplib_qp.dpi = &rdev->dpi_privileged;\n\t\trc = bnxt_qplib_create_qp1(&rdev->qplib_res, &qp->qplib_qp);\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"Failed to create HW QP1\");\n\t\t\tgoto fail;\n\t\t}\n\t\t/* Create a shadow QP to handle the QP1 traffic */\n\t\trdev->qp1_sqp = bnxt_re_create_shadow_qp(pd, &rdev->qplib_res,\n\t\t\t\t\t\t\t &qp->qplib_qp);\n\t\tif (!rdev->qp1_sqp) {\n\t\t\trc = -EINVAL;\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Failed to create Shadow QP for QP1\");\n\t\t\tgoto qp_destroy;\n\t\t}\n\t\trdev->sqp_ah = bnxt_re_create_shadow_qp_ah(pd, &rdev->qplib_res,\n\t\t\t\t\t\t\t   &qp->qplib_qp);\n\t\tif (!rdev->sqp_ah) {\n\t\t\tbnxt_qplib_destroy_qp(&rdev->qplib_res,\n\t\t\t\t\t      &rdev->qp1_sqp->qplib_qp);\n\t\t\trc = -EINVAL;\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Failed to create AH entry for ShadowQP\");\n\t\t\tgoto qp_destroy;\n\t\t}\n\n\t} else {\n\t\t/* Allocate 128 + 1 more than what's provided */\n\t\tentries = roundup_pow_of_two(qp_init_attr->cap.max_send_wr +\n\t\t\t\t\t     BNXT_QPLIB_RESERVED_QP_WRS + 1);\n\t\tqp->qplib_qp.sq.max_wqe = min_t(u32, entries,\n\t\t\t\t\t\tdev_attr->max_qp_wqes +\n\t\t\t\t\t\tBNXT_QPLIB_RESERVED_QP_WRS + 1);\n\t\tqp->qplib_qp.sq.q_full_delta = BNXT_QPLIB_RESERVED_QP_WRS + 1;\n\n\t\t/*\n\t\t * Reserving one slot for Phantom WQE. Application can\n\t\t * post one extra entry in this case. But allowing this to avoid\n\t\t * unexpected Queue full condition\n\t\t */\n\n\t\tqp->qplib_qp.sq.q_full_delta -= 1;\n\n\t\tqp->qplib_qp.max_rd_atomic = dev_attr->max_qp_rd_atom;\n\t\tqp->qplib_qp.max_dest_rd_atomic = dev_attr->max_qp_init_rd_atom;\n\t\tif (udata) {\n\t\t\trc = bnxt_re_init_user_qp(rdev, pd, qp, udata);\n\t\t\tif (rc)\n\t\t\t\tgoto fail;\n\t\t} else {\n\t\t\tqp->qplib_qp.dpi = &rdev->dpi_privileged;\n\t\t}\n\n\t\trc = bnxt_qplib_create_qp(&rdev->qplib_res, &qp->qplib_qp);\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"Failed to create HW QP\");\n\t\t\tgoto free_umem;\n\t\t}\n\t}\n\n\tqp->ib_qp.qp_num = qp->qplib_qp.id;\n\tspin_lock_init(&qp->sq_lock);\n\tspin_lock_init(&qp->rq_lock);\n\n\tif (udata) {\n\t\tstruct bnxt_re_qp_resp resp;\n\n\t\tresp.qpid = qp->ib_qp.qp_num;\n\t\tresp.rsvd = 0;\n\t\trc = ib_copy_to_udata(udata, &resp, sizeof(resp));\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"Failed to copy QP udata\");\n\t\t\tgoto qp_destroy;\n\t\t}\n\t}\n\tINIT_LIST_HEAD(&qp->list);\n\tmutex_lock(&rdev->qp_lock);\n\tlist_add_tail(&qp->list, &rdev->qp_list);\n\tatomic_inc(&rdev->qp_count);\n\tmutex_unlock(&rdev->qp_lock);\n\n\treturn &qp->ib_qp;\nqp_destroy:\n\tbnxt_qplib_destroy_qp(&rdev->qplib_res, &qp->qplib_qp);\nfree_umem:\n\tib_umem_release(qp->rumem);\n\tib_umem_release(qp->sumem);\nfail:\n\tkfree(qp);\n\treturn ERR_PTR(rc);\n}\n\nstatic u8 __from_ib_qp_state(enum ib_qp_state state)\n{\n\tswitch (state) {\n\tcase IB_QPS_RESET:\n\t\treturn CMDQ_MODIFY_QP_NEW_STATE_RESET;\n\tcase IB_QPS_INIT:\n\t\treturn CMDQ_MODIFY_QP_NEW_STATE_INIT;\n\tcase IB_QPS_RTR:\n\t\treturn CMDQ_MODIFY_QP_NEW_STATE_RTR;\n\tcase IB_QPS_RTS:\n\t\treturn CMDQ_MODIFY_QP_NEW_STATE_RTS;\n\tcase IB_QPS_SQD:\n\t\treturn CMDQ_MODIFY_QP_NEW_STATE_SQD;\n\tcase IB_QPS_SQE:\n\t\treturn CMDQ_MODIFY_QP_NEW_STATE_SQE;\n\tcase IB_QPS_ERR:\n\tdefault:\n\t\treturn CMDQ_MODIFY_QP_NEW_STATE_ERR;\n\t}\n}\n\nstatic enum ib_qp_state __to_ib_qp_state(u8 state)\n{\n\tswitch (state) {\n\tcase CMDQ_MODIFY_QP_NEW_STATE_RESET:\n\t\treturn IB_QPS_RESET;\n\tcase CMDQ_MODIFY_QP_NEW_STATE_INIT:\n\t\treturn IB_QPS_INIT;\n\tcase CMDQ_MODIFY_QP_NEW_STATE_RTR:\n\t\treturn IB_QPS_RTR;\n\tcase CMDQ_MODIFY_QP_NEW_STATE_RTS:\n\t\treturn IB_QPS_RTS;\n\tcase CMDQ_MODIFY_QP_NEW_STATE_SQD:\n\t\treturn IB_QPS_SQD;\n\tcase CMDQ_MODIFY_QP_NEW_STATE_SQE:\n\t\treturn IB_QPS_SQE;\n\tcase CMDQ_MODIFY_QP_NEW_STATE_ERR:\n\tdefault:\n\t\treturn IB_QPS_ERR;\n\t}\n}\n\nstatic u32 __from_ib_mtu(enum ib_mtu mtu)\n{\n\tswitch (mtu) {\n\tcase IB_MTU_256:\n\t\treturn CMDQ_MODIFY_QP_PATH_MTU_MTU_256;\n\tcase IB_MTU_512:\n\t\treturn CMDQ_MODIFY_QP_PATH_MTU_MTU_512;\n\tcase IB_MTU_1024:\n\t\treturn CMDQ_MODIFY_QP_PATH_MTU_MTU_1024;\n\tcase IB_MTU_2048:\n\t\treturn CMDQ_MODIFY_QP_PATH_MTU_MTU_2048;\n\tcase IB_MTU_4096:\n\t\treturn CMDQ_MODIFY_QP_PATH_MTU_MTU_4096;\n\tdefault:\n\t\treturn CMDQ_MODIFY_QP_PATH_MTU_MTU_2048;\n\t}\n}\n\nstatic enum ib_mtu __to_ib_mtu(u32 mtu)\n{\n\tswitch (mtu & CREQ_QUERY_QP_RESP_SB_PATH_MTU_MASK) {\n\tcase CMDQ_MODIFY_QP_PATH_MTU_MTU_256:\n\t\treturn IB_MTU_256;\n\tcase CMDQ_MODIFY_QP_PATH_MTU_MTU_512:\n\t\treturn IB_MTU_512;\n\tcase CMDQ_MODIFY_QP_PATH_MTU_MTU_1024:\n\t\treturn IB_MTU_1024;\n\tcase CMDQ_MODIFY_QP_PATH_MTU_MTU_2048:\n\t\treturn IB_MTU_2048;\n\tcase CMDQ_MODIFY_QP_PATH_MTU_MTU_4096:\n\t\treturn IB_MTU_4096;\n\tdefault:\n\t\treturn IB_MTU_2048;\n\t}\n}\n\n/* Shared Receive Queues */\nvoid bnxt_re_destroy_srq(struct ib_srq *ib_srq, struct ib_udata *udata)\n{\n\tstruct bnxt_re_srq *srq = container_of(ib_srq, struct bnxt_re_srq,\n\t\t\t\t\t       ib_srq);\n\tstruct bnxt_re_dev *rdev = srq->rdev;\n\tstruct bnxt_qplib_srq *qplib_srq = &srq->qplib_srq;\n\tstruct bnxt_qplib_nq *nq = NULL;\n\n\tif (qplib_srq->cq)\n\t\tnq = qplib_srq->cq->nq;\n\tbnxt_qplib_destroy_srq(&rdev->qplib_res, qplib_srq);\n\tib_umem_release(srq->umem);\n\tatomic_dec(&rdev->srq_count);\n\tif (nq)\n\t\tnq->budget--;\n}\n\nstatic int bnxt_re_init_user_srq(struct bnxt_re_dev *rdev,\n\t\t\t\t struct bnxt_re_pd *pd,\n\t\t\t\t struct bnxt_re_srq *srq,\n\t\t\t\t struct ib_udata *udata)\n{\n\tstruct bnxt_re_srq_req ureq;\n\tstruct bnxt_qplib_srq *qplib_srq = &srq->qplib_srq;\n\tstruct ib_umem *umem;\n\tint bytes = 0;\n\tstruct bnxt_re_ucontext *cntx = rdma_udata_to_drv_context(\n\t\tudata, struct bnxt_re_ucontext, ib_uctx);\n\n\tif (ib_copy_from_udata(&ureq, udata, sizeof(ureq)))\n\t\treturn -EFAULT;\n\n\tbytes = (qplib_srq->max_wqe * BNXT_QPLIB_MAX_RQE_ENTRY_SIZE);\n\tbytes = PAGE_ALIGN(bytes);\n\tumem = ib_umem_get(udata, ureq.srqva, bytes, IB_ACCESS_LOCAL_WRITE, 1);\n\tif (IS_ERR(umem))\n\t\treturn PTR_ERR(umem);\n\n\tsrq->umem = umem;\n\tqplib_srq->sg_info.sglist = umem->sg_head.sgl;\n\tqplib_srq->sg_info.npages = ib_umem_num_pages(umem);\n\tqplib_srq->sg_info.nmap = umem->nmap;\n\tqplib_srq->srq_handle = ureq.srq_handle;\n\tqplib_srq->dpi = &cntx->dpi;\n\n\treturn 0;\n}\n\nint bnxt_re_create_srq(struct ib_srq *ib_srq,\n\t\t       struct ib_srq_init_attr *srq_init_attr,\n\t\t       struct ib_udata *udata)\n{\n\tstruct ib_pd *ib_pd = ib_srq->pd;\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct bnxt_qplib_dev_attr *dev_attr = &rdev->dev_attr;\n\tstruct bnxt_re_srq *srq =\n\t\tcontainer_of(ib_srq, struct bnxt_re_srq, ib_srq);\n\tstruct bnxt_qplib_nq *nq = NULL;\n\tint rc, entries;\n\n\tif (srq_init_attr->attr.max_wr >= dev_attr->max_srq_wqes) {\n\t\tdev_err(rdev_to_dev(rdev), \"Create CQ failed - max exceeded\");\n\t\trc = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\tif (srq_init_attr->srq_type != IB_SRQT_BASIC) {\n\t\trc = -EOPNOTSUPP;\n\t\tgoto exit;\n\t}\n\n\tsrq->rdev = rdev;\n\tsrq->qplib_srq.pd = &pd->qplib_pd;\n\tsrq->qplib_srq.dpi = &rdev->dpi_privileged;\n\t/* Allocate 1 more than what's provided so posting max doesn't\n\t * mean empty\n\t */\n\tentries = roundup_pow_of_two(srq_init_attr->attr.max_wr + 1);\n\tif (entries > dev_attr->max_srq_wqes + 1)\n\t\tentries = dev_attr->max_srq_wqes + 1;\n\n\tsrq->qplib_srq.max_wqe = entries;\n\tsrq->qplib_srq.max_sge = srq_init_attr->attr.max_sge;\n\tsrq->qplib_srq.threshold = srq_init_attr->attr.srq_limit;\n\tsrq->srq_limit = srq_init_attr->attr.srq_limit;\n\tsrq->qplib_srq.eventq_hw_ring_id = rdev->nq[0].ring_id;\n\tnq = &rdev->nq[0];\n\n\tif (udata) {\n\t\trc = bnxt_re_init_user_srq(rdev, pd, srq, udata);\n\t\tif (rc)\n\t\t\tgoto fail;\n\t}\n\n\trc = bnxt_qplib_create_srq(&rdev->qplib_res, &srq->qplib_srq);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Create HW SRQ failed!\");\n\t\tgoto fail;\n\t}\n\n\tif (udata) {\n\t\tstruct bnxt_re_srq_resp resp;\n\n\t\tresp.srqid = srq->qplib_srq.id;\n\t\trc = ib_copy_to_udata(udata, &resp, sizeof(resp));\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"SRQ copy to udata failed!\");\n\t\t\tbnxt_qplib_destroy_srq(&rdev->qplib_res,\n\t\t\t\t\t       &srq->qplib_srq);\n\t\t\tgoto fail;\n\t\t}\n\t}\n\tif (nq)\n\t\tnq->budget++;\n\tatomic_inc(&rdev->srq_count);\n\n\treturn 0;\n\nfail:\n\tib_umem_release(srq->umem);\nexit:\n\treturn rc;\n}\n\nint bnxt_re_modify_srq(struct ib_srq *ib_srq, struct ib_srq_attr *srq_attr,\n\t\t       enum ib_srq_attr_mask srq_attr_mask,\n\t\t       struct ib_udata *udata)\n{\n\tstruct bnxt_re_srq *srq = container_of(ib_srq, struct bnxt_re_srq,\n\t\t\t\t\t       ib_srq);\n\tstruct bnxt_re_dev *rdev = srq->rdev;\n\tint rc;\n\n\tswitch (srq_attr_mask) {\n\tcase IB_SRQ_MAX_WR:\n\t\t/* SRQ resize is not supported */\n\t\tbreak;\n\tcase IB_SRQ_LIMIT:\n\t\t/* Change the SRQ threshold */\n\t\tif (srq_attr->srq_limit > srq->qplib_srq.max_wqe)\n\t\t\treturn -EINVAL;\n\n\t\tsrq->qplib_srq.threshold = srq_attr->srq_limit;\n\t\trc = bnxt_qplib_modify_srq(&rdev->qplib_res, &srq->qplib_srq);\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"Modify HW SRQ failed!\");\n\t\t\treturn rc;\n\t\t}\n\t\t/* On success, update the shadow */\n\t\tsrq->srq_limit = srq_attr->srq_limit;\n\t\t/* No need to Build and send response back to udata */\n\t\tbreak;\n\tdefault:\n\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\"Unsupported srq_attr_mask 0x%x\", srq_attr_mask);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nint bnxt_re_query_srq(struct ib_srq *ib_srq, struct ib_srq_attr *srq_attr)\n{\n\tstruct bnxt_re_srq *srq = container_of(ib_srq, struct bnxt_re_srq,\n\t\t\t\t\t       ib_srq);\n\tstruct bnxt_re_srq tsrq;\n\tstruct bnxt_re_dev *rdev = srq->rdev;\n\tint rc;\n\n\t/* Get live SRQ attr */\n\ttsrq.qplib_srq.id = srq->qplib_srq.id;\n\trc = bnxt_qplib_query_srq(&rdev->qplib_res, &tsrq.qplib_srq);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Query HW SRQ failed!\");\n\t\treturn rc;\n\t}\n\tsrq_attr->max_wr = srq->qplib_srq.max_wqe;\n\tsrq_attr->max_sge = srq->qplib_srq.max_sge;\n\tsrq_attr->srq_limit = tsrq.qplib_srq.threshold;\n\n\treturn 0;\n}\n\nint bnxt_re_post_srq_recv(struct ib_srq *ib_srq, const struct ib_recv_wr *wr,\n\t\t\t  const struct ib_recv_wr **bad_wr)\n{\n\tstruct bnxt_re_srq *srq = container_of(ib_srq, struct bnxt_re_srq,\n\t\t\t\t\t       ib_srq);\n\tstruct bnxt_qplib_swqe wqe;\n\tunsigned long flags;\n\tint rc = 0;\n\n\tspin_lock_irqsave(&srq->lock, flags);\n\twhile (wr) {\n\t\t/* Transcribe each ib_recv_wr to qplib_swqe */\n\t\twqe.num_sge = wr->num_sge;\n\t\tbnxt_re_build_sgl(wr->sg_list, wqe.sg_list, wr->num_sge);\n\t\twqe.wr_id = wr->wr_id;\n\t\twqe.type = BNXT_QPLIB_SWQE_TYPE_RECV;\n\n\t\trc = bnxt_qplib_post_srq_recv(&srq->qplib_srq, &wqe);\n\t\tif (rc) {\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\twr = wr->next;\n\t}\n\tspin_unlock_irqrestore(&srq->lock, flags);\n\n\treturn rc;\n}\nstatic int bnxt_re_modify_shadow_qp(struct bnxt_re_dev *rdev,\n\t\t\t\t    struct bnxt_re_qp *qp1_qp,\n\t\t\t\t    int qp_attr_mask)\n{\n\tstruct bnxt_re_qp *qp = rdev->qp1_sqp;\n\tint rc = 0;\n\n\tif (qp_attr_mask & IB_QP_STATE) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_STATE;\n\t\tqp->qplib_qp.state = qp1_qp->qplib_qp.state;\n\t}\n\tif (qp_attr_mask & IB_QP_PKEY_INDEX) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_PKEY;\n\t\tqp->qplib_qp.pkey_index = qp1_qp->qplib_qp.pkey_index;\n\t}\n\n\tif (qp_attr_mask & IB_QP_QKEY) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_QKEY;\n\t\t/* Using a Random  QKEY */\n\t\tqp->qplib_qp.qkey = 0x81818181;\n\t}\n\tif (qp_attr_mask & IB_QP_SQ_PSN) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_SQ_PSN;\n\t\tqp->qplib_qp.sq.psn = qp1_qp->qplib_qp.sq.psn;\n\t}\n\n\trc = bnxt_qplib_modify_qp(&rdev->qplib_res, &qp->qplib_qp);\n\tif (rc)\n\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\"Failed to modify Shadow QP for QP1\");\n\treturn rc;\n}\n\nint bnxt_re_modify_qp(struct ib_qp *ib_qp, struct ib_qp_attr *qp_attr,\n\t\t      int qp_attr_mask, struct ib_udata *udata)\n{\n\tstruct bnxt_re_qp *qp = container_of(ib_qp, struct bnxt_re_qp, ib_qp);\n\tstruct bnxt_re_dev *rdev = qp->rdev;\n\tstruct bnxt_qplib_dev_attr *dev_attr = &rdev->dev_attr;\n\tenum ib_qp_state curr_qp_state, new_qp_state;\n\tint rc, entries;\n\tunsigned int flags;\n\tu8 nw_type;\n\n\tqp->qplib_qp.modify_flags = 0;\n\tif (qp_attr_mask & IB_QP_STATE) {\n\t\tcurr_qp_state = __to_ib_qp_state(qp->qplib_qp.cur_qp_state);\n\t\tnew_qp_state = qp_attr->qp_state;\n\t\tif (!ib_modify_qp_is_ok(curr_qp_state, new_qp_state,\n\t\t\t\t\tib_qp->qp_type, qp_attr_mask)) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Invalid attribute mask: %#x specified \",\n\t\t\t\tqp_attr_mask);\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"for qpn: %#x type: %#x\",\n\t\t\t\tib_qp->qp_num, ib_qp->qp_type);\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"curr_qp_state=0x%x, new_qp_state=0x%x\\n\",\n\t\t\t\tcurr_qp_state, new_qp_state);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_STATE;\n\t\tqp->qplib_qp.state = __from_ib_qp_state(qp_attr->qp_state);\n\n\t\tif (!qp->sumem &&\n\t\t    qp->qplib_qp.state == CMDQ_MODIFY_QP_NEW_STATE_ERR) {\n\t\t\tdev_dbg(rdev_to_dev(rdev),\n\t\t\t\t\"Move QP = %p to flush list\\n\",\n\t\t\t\tqp);\n\t\t\tflags = bnxt_re_lock_cqs(qp);\n\t\t\tbnxt_qplib_add_flush_qp(&qp->qplib_qp);\n\t\t\tbnxt_re_unlock_cqs(qp, flags);\n\t\t}\n\t\tif (!qp->sumem &&\n\t\t    qp->qplib_qp.state == CMDQ_MODIFY_QP_NEW_STATE_RESET) {\n\t\t\tdev_dbg(rdev_to_dev(rdev),\n\t\t\t\t\"Move QP = %p out of flush list\\n\",\n\t\t\t\tqp);\n\t\t\tflags = bnxt_re_lock_cqs(qp);\n\t\t\tbnxt_qplib_clean_qp(&qp->qplib_qp);\n\t\t\tbnxt_re_unlock_cqs(qp, flags);\n\t\t}\n\t}\n\tif (qp_attr_mask & IB_QP_EN_SQD_ASYNC_NOTIFY) {\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_EN_SQD_ASYNC_NOTIFY;\n\t\tqp->qplib_qp.en_sqd_async_notify = true;\n\t}\n\tif (qp_attr_mask & IB_QP_ACCESS_FLAGS) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_ACCESS;\n\t\tqp->qplib_qp.access =\n\t\t\t__from_ib_access_flags(qp_attr->qp_access_flags);\n\t\t/* LOCAL_WRITE access must be set to allow RC receive */\n\t\tqp->qplib_qp.access |= BNXT_QPLIB_ACCESS_LOCAL_WRITE;\n\t\t/* Temp: Set all params on QP as of now */\n\t\tqp->qplib_qp.access |= CMDQ_MODIFY_QP_ACCESS_REMOTE_WRITE;\n\t\tqp->qplib_qp.access |= CMDQ_MODIFY_QP_ACCESS_REMOTE_READ;\n\t}\n\tif (qp_attr_mask & IB_QP_PKEY_INDEX) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_PKEY;\n\t\tqp->qplib_qp.pkey_index = qp_attr->pkey_index;\n\t}\n\tif (qp_attr_mask & IB_QP_QKEY) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_QKEY;\n\t\tqp->qplib_qp.qkey = qp_attr->qkey;\n\t}\n\tif (qp_attr_mask & IB_QP_AV) {\n\t\tconst struct ib_global_route *grh =\n\t\t\trdma_ah_read_grh(&qp_attr->ah_attr);\n\t\tconst struct ib_gid_attr *sgid_attr;\n\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_DGID |\n\t\t\t\t     CMDQ_MODIFY_QP_MODIFY_MASK_FLOW_LABEL |\n\t\t\t\t     CMDQ_MODIFY_QP_MODIFY_MASK_SGID_INDEX |\n\t\t\t\t     CMDQ_MODIFY_QP_MODIFY_MASK_HOP_LIMIT |\n\t\t\t\t     CMDQ_MODIFY_QP_MODIFY_MASK_TRAFFIC_CLASS |\n\t\t\t\t     CMDQ_MODIFY_QP_MODIFY_MASK_DEST_MAC |\n\t\t\t\t     CMDQ_MODIFY_QP_MODIFY_MASK_VLAN_ID;\n\t\tmemcpy(qp->qplib_qp.ah.dgid.data, grh->dgid.raw,\n\t\t       sizeof(qp->qplib_qp.ah.dgid.data));\n\t\tqp->qplib_qp.ah.flow_label = grh->flow_label;\n\t\t/* If RoCE V2 is enabled, stack will have two entries for\n\t\t * each GID entry. Avoiding this duplicte entry in HW. Dividing\n\t\t * the GID index by 2 for RoCE V2\n\t\t */\n\t\tqp->qplib_qp.ah.sgid_index = grh->sgid_index / 2;\n\t\tqp->qplib_qp.ah.host_sgid_index = grh->sgid_index;\n\t\tqp->qplib_qp.ah.hop_limit = grh->hop_limit;\n\t\tqp->qplib_qp.ah.traffic_class = grh->traffic_class;\n\t\tqp->qplib_qp.ah.sl = rdma_ah_get_sl(&qp_attr->ah_attr);\n\t\tether_addr_copy(qp->qplib_qp.ah.dmac,\n\t\t\t\tqp_attr->ah_attr.roce.dmac);\n\n\t\tsgid_attr = qp_attr->ah_attr.grh.sgid_attr;\n\t\trc = rdma_read_gid_l2_fields(sgid_attr, NULL,\n\t\t\t\t\t     &qp->qplib_qp.smac[0]);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tnw_type = rdma_gid_attr_network_type(sgid_attr);\n\t\tswitch (nw_type) {\n\t\tcase RDMA_NETWORK_IPV4:\n\t\t\tqp->qplib_qp.nw_type =\n\t\t\t\tCMDQ_MODIFY_QP_NETWORK_TYPE_ROCEV2_IPV4;\n\t\t\tbreak;\n\t\tcase RDMA_NETWORK_IPV6:\n\t\t\tqp->qplib_qp.nw_type =\n\t\t\t\tCMDQ_MODIFY_QP_NETWORK_TYPE_ROCEV2_IPV6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tqp->qplib_qp.nw_type =\n\t\t\t\tCMDQ_MODIFY_QP_NETWORK_TYPE_ROCEV1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (qp_attr_mask & IB_QP_PATH_MTU) {\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_PATH_MTU;\n\t\tqp->qplib_qp.path_mtu = __from_ib_mtu(qp_attr->path_mtu);\n\t\tqp->qplib_qp.mtu = ib_mtu_enum_to_int(qp_attr->path_mtu);\n\t} else if (qp_attr->qp_state == IB_QPS_RTR) {\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_PATH_MTU;\n\t\tqp->qplib_qp.path_mtu =\n\t\t\t__from_ib_mtu(iboe_get_mtu(rdev->netdev->mtu));\n\t\tqp->qplib_qp.mtu =\n\t\t\tib_mtu_enum_to_int(iboe_get_mtu(rdev->netdev->mtu));\n\t}\n\n\tif (qp_attr_mask & IB_QP_TIMEOUT) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_TIMEOUT;\n\t\tqp->qplib_qp.timeout = qp_attr->timeout;\n\t}\n\tif (qp_attr_mask & IB_QP_RETRY_CNT) {\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_RETRY_CNT;\n\t\tqp->qplib_qp.retry_cnt = qp_attr->retry_cnt;\n\t}\n\tif (qp_attr_mask & IB_QP_RNR_RETRY) {\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_RNR_RETRY;\n\t\tqp->qplib_qp.rnr_retry = qp_attr->rnr_retry;\n\t}\n\tif (qp_attr_mask & IB_QP_MIN_RNR_TIMER) {\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_MIN_RNR_TIMER;\n\t\tqp->qplib_qp.min_rnr_timer = qp_attr->min_rnr_timer;\n\t}\n\tif (qp_attr_mask & IB_QP_RQ_PSN) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_RQ_PSN;\n\t\tqp->qplib_qp.rq.psn = qp_attr->rq_psn;\n\t}\n\tif (qp_attr_mask & IB_QP_MAX_QP_RD_ATOMIC) {\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_MAX_RD_ATOMIC;\n\t\t/* Cap the max_rd_atomic to device max */\n\t\tqp->qplib_qp.max_rd_atomic = min_t(u32, qp_attr->max_rd_atomic,\n\t\t\t\t\t\t   dev_attr->max_qp_rd_atom);\n\t}\n\tif (qp_attr_mask & IB_QP_SQ_PSN) {\n\t\tqp->qplib_qp.modify_flags |= CMDQ_MODIFY_QP_MODIFY_MASK_SQ_PSN;\n\t\tqp->qplib_qp.sq.psn = qp_attr->sq_psn;\n\t}\n\tif (qp_attr_mask & IB_QP_MAX_DEST_RD_ATOMIC) {\n\t\tif (qp_attr->max_dest_rd_atomic >\n\t\t    dev_attr->max_qp_init_rd_atom) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"max_dest_rd_atomic requested%d is > dev_max%d\",\n\t\t\t\tqp_attr->max_dest_rd_atomic,\n\t\t\t\tdev_attr->max_qp_init_rd_atom);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_MAX_DEST_RD_ATOMIC;\n\t\tqp->qplib_qp.max_dest_rd_atomic = qp_attr->max_dest_rd_atomic;\n\t}\n\tif (qp_attr_mask & IB_QP_CAP) {\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_SQ_SIZE |\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_RQ_SIZE |\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_SQ_SGE |\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_RQ_SGE |\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_MAX_INLINE_DATA;\n\t\tif ((qp_attr->cap.max_send_wr >= dev_attr->max_qp_wqes) ||\n\t\t    (qp_attr->cap.max_recv_wr >= dev_attr->max_qp_wqes) ||\n\t\t    (qp_attr->cap.max_send_sge >= dev_attr->max_qp_sges) ||\n\t\t    (qp_attr->cap.max_recv_sge >= dev_attr->max_qp_sges) ||\n\t\t    (qp_attr->cap.max_inline_data >=\n\t\t\t\t\t\tdev_attr->max_inline_data)) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Create QP failed - max exceeded\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tentries = roundup_pow_of_two(qp_attr->cap.max_send_wr);\n\t\tqp->qplib_qp.sq.max_wqe = min_t(u32, entries,\n\t\t\t\t\t\tdev_attr->max_qp_wqes + 1);\n\t\tqp->qplib_qp.sq.q_full_delta = qp->qplib_qp.sq.max_wqe -\n\t\t\t\t\t\tqp_attr->cap.max_send_wr;\n\t\t/*\n\t\t * Reserving one slot for Phantom WQE. Some application can\n\t\t * post one extra entry in this case. Allowing this to avoid\n\t\t * unexpected Queue full condition\n\t\t */\n\t\tqp->qplib_qp.sq.q_full_delta -= 1;\n\t\tqp->qplib_qp.sq.max_sge = qp_attr->cap.max_send_sge;\n\t\tif (qp->qplib_qp.rq.max_wqe) {\n\t\t\tentries = roundup_pow_of_two(qp_attr->cap.max_recv_wr);\n\t\t\tqp->qplib_qp.rq.max_wqe =\n\t\t\t\tmin_t(u32, entries, dev_attr->max_qp_wqes + 1);\n\t\t\tqp->qplib_qp.rq.q_full_delta = qp->qplib_qp.rq.max_wqe -\n\t\t\t\t\t\t       qp_attr->cap.max_recv_wr;\n\t\t\tqp->qplib_qp.rq.max_sge = qp_attr->cap.max_recv_sge;\n\t\t} else {\n\t\t\t/* SRQ was used prior, just ignore the RQ caps */\n\t\t}\n\t}\n\tif (qp_attr_mask & IB_QP_DEST_QPN) {\n\t\tqp->qplib_qp.modify_flags |=\n\t\t\t\tCMDQ_MODIFY_QP_MODIFY_MASK_DEST_QP_ID;\n\t\tqp->qplib_qp.dest_qpn = qp_attr->dest_qp_num;\n\t}\n\trc = bnxt_qplib_modify_qp(&rdev->qplib_res, &qp->qplib_qp);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to modify HW QP\");\n\t\treturn rc;\n\t}\n\tif (ib_qp->qp_type == IB_QPT_GSI && rdev->qp1_sqp)\n\t\trc = bnxt_re_modify_shadow_qp(rdev, qp, qp_attr_mask);\n\treturn rc;\n}\n\nint bnxt_re_query_qp(struct ib_qp *ib_qp, struct ib_qp_attr *qp_attr,\n\t\t     int qp_attr_mask, struct ib_qp_init_attr *qp_init_attr)\n{\n\tstruct bnxt_re_qp *qp = container_of(ib_qp, struct bnxt_re_qp, ib_qp);\n\tstruct bnxt_re_dev *rdev = qp->rdev;\n\tstruct bnxt_qplib_qp *qplib_qp;\n\tint rc;\n\n\tqplib_qp = kzalloc(sizeof(*qplib_qp), GFP_KERNEL);\n\tif (!qplib_qp)\n\t\treturn -ENOMEM;\n\n\tqplib_qp->id = qp->qplib_qp.id;\n\tqplib_qp->ah.host_sgid_index = qp->qplib_qp.ah.host_sgid_index;\n\n\trc = bnxt_qplib_query_qp(&rdev->qplib_res, qplib_qp);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to query HW QP\");\n\t\tgoto out;\n\t}\n\tqp_attr->qp_state = __to_ib_qp_state(qplib_qp->state);\n\tqp_attr->en_sqd_async_notify = qplib_qp->en_sqd_async_notify ? 1 : 0;\n\tqp_attr->qp_access_flags = __to_ib_access_flags(qplib_qp->access);\n\tqp_attr->pkey_index = qplib_qp->pkey_index;\n\tqp_attr->qkey = qplib_qp->qkey;\n\tqp_attr->ah_attr.type = RDMA_AH_ATTR_TYPE_ROCE;\n\trdma_ah_set_grh(&qp_attr->ah_attr, NULL, qplib_qp->ah.flow_label,\n\t\t\tqplib_qp->ah.host_sgid_index,\n\t\t\tqplib_qp->ah.hop_limit,\n\t\t\tqplib_qp->ah.traffic_class);\n\trdma_ah_set_dgid_raw(&qp_attr->ah_attr, qplib_qp->ah.dgid.data);\n\trdma_ah_set_sl(&qp_attr->ah_attr, qplib_qp->ah.sl);\n\tether_addr_copy(qp_attr->ah_attr.roce.dmac, qplib_qp->ah.dmac);\n\tqp_attr->path_mtu = __to_ib_mtu(qplib_qp->path_mtu);\n\tqp_attr->timeout = qplib_qp->timeout;\n\tqp_attr->retry_cnt = qplib_qp->retry_cnt;\n\tqp_attr->rnr_retry = qplib_qp->rnr_retry;\n\tqp_attr->min_rnr_timer = qplib_qp->min_rnr_timer;\n\tqp_attr->rq_psn = qplib_qp->rq.psn;\n\tqp_attr->max_rd_atomic = qplib_qp->max_rd_atomic;\n\tqp_attr->sq_psn = qplib_qp->sq.psn;\n\tqp_attr->max_dest_rd_atomic = qplib_qp->max_dest_rd_atomic;\n\tqp_init_attr->sq_sig_type = qplib_qp->sig_type ? IB_SIGNAL_ALL_WR :\n\t\t\t\t\t\t\t IB_SIGNAL_REQ_WR;\n\tqp_attr->dest_qp_num = qplib_qp->dest_qpn;\n\n\tqp_attr->cap.max_send_wr = qp->qplib_qp.sq.max_wqe;\n\tqp_attr->cap.max_send_sge = qp->qplib_qp.sq.max_sge;\n\tqp_attr->cap.max_recv_wr = qp->qplib_qp.rq.max_wqe;\n\tqp_attr->cap.max_recv_sge = qp->qplib_qp.rq.max_sge;\n\tqp_attr->cap.max_inline_data = qp->qplib_qp.max_inline_data;\n\tqp_init_attr->cap = qp_attr->cap;\n\nout:\n\tkfree(qplib_qp);\n\treturn rc;\n}\n\n/* Routine for sending QP1 packets for RoCE V1 an V2\n */\nstatic int bnxt_re_build_qp1_send_v2(struct bnxt_re_qp *qp,\n\t\t\t\t     const struct ib_send_wr *wr,\n\t\t\t\t     struct bnxt_qplib_swqe *wqe,\n\t\t\t\t     int payload_size)\n{\n\tstruct bnxt_re_ah *ah = container_of(ud_wr(wr)->ah, struct bnxt_re_ah,\n\t\t\t\t\t     ib_ah);\n\tstruct bnxt_qplib_ah *qplib_ah = &ah->qplib_ah;\n\tconst struct ib_gid_attr *sgid_attr = ah->ib_ah.sgid_attr;\n\tstruct bnxt_qplib_sge sge;\n\tu8 nw_type;\n\tu16 ether_type;\n\tunion ib_gid dgid;\n\tbool is_eth = false;\n\tbool is_vlan = false;\n\tbool is_grh = false;\n\tbool is_udp = false;\n\tu8 ip_version = 0;\n\tu16 vlan_id = 0xFFFF;\n\tvoid *buf;\n\tint i, rc = 0;\n\n\tmemset(&qp->qp1_hdr, 0, sizeof(qp->qp1_hdr));\n\n\trc = rdma_read_gid_l2_fields(sgid_attr, &vlan_id, NULL);\n\tif (rc)\n\t\treturn rc;\n\n\t/* Get network header type for this GID */\n\tnw_type = rdma_gid_attr_network_type(sgid_attr);\n\tswitch (nw_type) {\n\tcase RDMA_NETWORK_IPV4:\n\t\tnw_type = BNXT_RE_ROCEV2_IPV4_PACKET;\n\t\tbreak;\n\tcase RDMA_NETWORK_IPV6:\n\t\tnw_type = BNXT_RE_ROCEV2_IPV6_PACKET;\n\t\tbreak;\n\tdefault:\n\t\tnw_type = BNXT_RE_ROCE_V1_PACKET;\n\t\tbreak;\n\t}\n\tmemcpy(&dgid.raw, &qplib_ah->dgid, 16);\n\tis_udp = sgid_attr->gid_type == IB_GID_TYPE_ROCE_UDP_ENCAP;\n\tif (is_udp) {\n\t\tif (ipv6_addr_v4mapped((struct in6_addr *)&sgid_attr->gid)) {\n\t\t\tip_version = 4;\n\t\t\tether_type = ETH_P_IP;\n\t\t} else {\n\t\t\tip_version = 6;\n\t\t\tether_type = ETH_P_IPV6;\n\t\t}\n\t\tis_grh = false;\n\t} else {\n\t\tether_type = ETH_P_IBOE;\n\t\tis_grh = true;\n\t}\n\n\tis_eth = true;\n\tis_vlan = (vlan_id && (vlan_id < 0x1000)) ? true : false;\n\n\tib_ud_header_init(payload_size, !is_eth, is_eth, is_vlan, is_grh,\n\t\t\t  ip_version, is_udp, 0, &qp->qp1_hdr);\n\n\t/* ETH */\n\tether_addr_copy(qp->qp1_hdr.eth.dmac_h, ah->qplib_ah.dmac);\n\tether_addr_copy(qp->qp1_hdr.eth.smac_h, qp->qplib_qp.smac);\n\n\t/* For vlan, check the sgid for vlan existence */\n\n\tif (!is_vlan) {\n\t\tqp->qp1_hdr.eth.type = cpu_to_be16(ether_type);\n\t} else {\n\t\tqp->qp1_hdr.vlan.type = cpu_to_be16(ether_type);\n\t\tqp->qp1_hdr.vlan.tag = cpu_to_be16(vlan_id);\n\t}\n\n\tif (is_grh || (ip_version == 6)) {\n\t\tmemcpy(qp->qp1_hdr.grh.source_gid.raw, sgid_attr->gid.raw,\n\t\t       sizeof(sgid_attr->gid));\n\t\tmemcpy(qp->qp1_hdr.grh.destination_gid.raw, qplib_ah->dgid.data,\n\t\t       sizeof(sgid_attr->gid));\n\t\tqp->qp1_hdr.grh.hop_limit     = qplib_ah->hop_limit;\n\t}\n\n\tif (ip_version == 4) {\n\t\tqp->qp1_hdr.ip4.tos = 0;\n\t\tqp->qp1_hdr.ip4.id = 0;\n\t\tqp->qp1_hdr.ip4.frag_off = htons(IP_DF);\n\t\tqp->qp1_hdr.ip4.ttl = qplib_ah->hop_limit;\n\n\t\tmemcpy(&qp->qp1_hdr.ip4.saddr, sgid_attr->gid.raw + 12, 4);\n\t\tmemcpy(&qp->qp1_hdr.ip4.daddr, qplib_ah->dgid.data + 12, 4);\n\t\tqp->qp1_hdr.ip4.check = ib_ud_ip4_csum(&qp->qp1_hdr);\n\t}\n\n\tif (is_udp) {\n\t\tqp->qp1_hdr.udp.dport = htons(ROCE_V2_UDP_DPORT);\n\t\tqp->qp1_hdr.udp.sport = htons(0x8CD1);\n\t\tqp->qp1_hdr.udp.csum = 0;\n\t}\n\n\t/* BTH */\n\tif (wr->opcode == IB_WR_SEND_WITH_IMM) {\n\t\tqp->qp1_hdr.bth.opcode = IB_OPCODE_UD_SEND_ONLY_WITH_IMMEDIATE;\n\t\tqp->qp1_hdr.immediate_present = 1;\n\t} else {\n\t\tqp->qp1_hdr.bth.opcode = IB_OPCODE_UD_SEND_ONLY;\n\t}\n\tif (wr->send_flags & IB_SEND_SOLICITED)\n\t\tqp->qp1_hdr.bth.solicited_event = 1;\n\t/* pad_count */\n\tqp->qp1_hdr.bth.pad_count = (4 - payload_size) & 3;\n\n\t/* P_key for QP1 is for all members */\n\tqp->qp1_hdr.bth.pkey = cpu_to_be16(0xFFFF);\n\tqp->qp1_hdr.bth.destination_qpn = IB_QP1;\n\tqp->qp1_hdr.bth.ack_req = 0;\n\tqp->send_psn++;\n\tqp->send_psn &= BTH_PSN_MASK;\n\tqp->qp1_hdr.bth.psn = cpu_to_be32(qp->send_psn);\n\t/* DETH */\n\t/* Use the priviledged Q_Key for QP1 */\n\tqp->qp1_hdr.deth.qkey = cpu_to_be32(IB_QP1_QKEY);\n\tqp->qp1_hdr.deth.source_qpn = IB_QP1;\n\n\t/* Pack the QP1 to the transmit buffer */\n\tbuf = bnxt_qplib_get_qp1_sq_buf(&qp->qplib_qp, &sge);\n\tif (buf) {\n\t\tib_ud_header_pack(&qp->qp1_hdr, buf);\n\t\tfor (i = wqe->num_sge; i; i--) {\n\t\t\twqe->sg_list[i].addr = wqe->sg_list[i - 1].addr;\n\t\t\twqe->sg_list[i].lkey = wqe->sg_list[i - 1].lkey;\n\t\t\twqe->sg_list[i].size = wqe->sg_list[i - 1].size;\n\t\t}\n\n\t\t/*\n\t\t * Max Header buf size for IPV6 RoCE V2 is 86,\n\t\t * which is same as the QP1 SQ header buffer.\n\t\t * Header buf size for IPV4 RoCE V2 can be 66.\n\t\t * ETH(14) + VLAN(4)+ IP(20) + UDP (8) + BTH(20).\n\t\t * Subtract 20 bytes from QP1 SQ header buf size\n\t\t */\n\t\tif (is_udp && ip_version == 4)\n\t\t\tsge.size -= 20;\n\t\t/*\n\t\t * Max Header buf size for RoCE V1 is 78.\n\t\t * ETH(14) + VLAN(4) + GRH(40) + BTH(20).\n\t\t * Subtract 8 bytes from QP1 SQ header buf size\n\t\t */\n\t\tif (!is_udp)\n\t\t\tsge.size -= 8;\n\n\t\t/* Subtract 4 bytes for non vlan packets */\n\t\tif (!is_vlan)\n\t\t\tsge.size -= 4;\n\n\t\twqe->sg_list[0].addr = sge.addr;\n\t\twqe->sg_list[0].lkey = sge.lkey;\n\t\twqe->sg_list[0].size = sge.size;\n\t\twqe->num_sge++;\n\n\t} else {\n\t\tdev_err(rdev_to_dev(qp->rdev), \"QP1 buffer is empty!\");\n\t\trc = -ENOMEM;\n\t}\n\treturn rc;\n}\n\n/* For the MAD layer, it only provides the recv SGE the size of\n * ib_grh + MAD datagram.  No Ethernet headers, Ethertype, BTH, DETH,\n * nor RoCE iCRC.  The Cu+ solution must provide buffer for the entire\n * receive packet (334 bytes) with no VLAN and then copy the GRH\n * and the MAD datagram out to the provided SGE.\n */\nstatic int bnxt_re_build_qp1_shadow_qp_recv(struct bnxt_re_qp *qp,\n\t\t\t\t\t    const struct ib_recv_wr *wr,\n\t\t\t\t\t    struct bnxt_qplib_swqe *wqe,\n\t\t\t\t\t    int payload_size)\n{\n\tstruct bnxt_qplib_sge ref, sge;\n\tu32 rq_prod_index;\n\tstruct bnxt_re_sqp_entries *sqp_entry;\n\n\trq_prod_index = bnxt_qplib_get_rq_prod_index(&qp->qplib_qp);\n\n\tif (!bnxt_qplib_get_qp1_rq_buf(&qp->qplib_qp, &sge))\n\t\treturn -ENOMEM;\n\n\t/* Create 1 SGE to receive the entire\n\t * ethernet packet\n\t */\n\t/* Save the reference from ULP */\n\tref.addr = wqe->sg_list[0].addr;\n\tref.lkey = wqe->sg_list[0].lkey;\n\tref.size = wqe->sg_list[0].size;\n\n\tsqp_entry = &qp->rdev->sqp_tbl[rq_prod_index];\n\n\t/* SGE 1 */\n\twqe->sg_list[0].addr = sge.addr;\n\twqe->sg_list[0].lkey = sge.lkey;\n\twqe->sg_list[0].size = BNXT_QPLIB_MAX_QP1_RQ_HDR_SIZE_V2;\n\tsge.size -= wqe->sg_list[0].size;\n\n\tsqp_entry->sge.addr = ref.addr;\n\tsqp_entry->sge.lkey = ref.lkey;\n\tsqp_entry->sge.size = ref.size;\n\t/* Store the wrid for reporting completion */\n\tsqp_entry->wrid = wqe->wr_id;\n\t/* change the wqe->wrid to table index */\n\twqe->wr_id = rq_prod_index;\n\treturn 0;\n}\n\nstatic int is_ud_qp(struct bnxt_re_qp *qp)\n{\n\treturn (qp->qplib_qp.type == CMDQ_CREATE_QP_TYPE_UD ||\n\t\tqp->qplib_qp.type == CMDQ_CREATE_QP_TYPE_GSI);\n}\n\nstatic int bnxt_re_build_send_wqe(struct bnxt_re_qp *qp,\n\t\t\t\t  const struct ib_send_wr *wr,\n\t\t\t\t  struct bnxt_qplib_swqe *wqe)\n{\n\tstruct bnxt_re_ah *ah = NULL;\n\n\tif (is_ud_qp(qp)) {\n\t\tah = container_of(ud_wr(wr)->ah, struct bnxt_re_ah, ib_ah);\n\t\twqe->send.q_key = ud_wr(wr)->remote_qkey;\n\t\twqe->send.dst_qp = ud_wr(wr)->remote_qpn;\n\t\twqe->send.avid = ah->qplib_ah.id;\n\t}\n\tswitch (wr->opcode) {\n\tcase IB_WR_SEND:\n\t\twqe->type = BNXT_QPLIB_SWQE_TYPE_SEND;\n\t\tbreak;\n\tcase IB_WR_SEND_WITH_IMM:\n\t\twqe->type = BNXT_QPLIB_SWQE_TYPE_SEND_WITH_IMM;\n\t\twqe->send.imm_data = wr->ex.imm_data;\n\t\tbreak;\n\tcase IB_WR_SEND_WITH_INV:\n\t\twqe->type = BNXT_QPLIB_SWQE_TYPE_SEND_WITH_INV;\n\t\twqe->send.inv_key = wr->ex.invalidate_rkey;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\tif (wr->send_flags & IB_SEND_SIGNALED)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SIGNAL_COMP;\n\tif (wr->send_flags & IB_SEND_FENCE)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_UC_FENCE;\n\tif (wr->send_flags & IB_SEND_SOLICITED)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SOLICIT_EVENT;\n\tif (wr->send_flags & IB_SEND_INLINE)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_INLINE;\n\n\treturn 0;\n}\n\nstatic int bnxt_re_build_rdma_wqe(const struct ib_send_wr *wr,\n\t\t\t\t  struct bnxt_qplib_swqe *wqe)\n{\n\tswitch (wr->opcode) {\n\tcase IB_WR_RDMA_WRITE:\n\t\twqe->type = BNXT_QPLIB_SWQE_TYPE_RDMA_WRITE;\n\t\tbreak;\n\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\twqe->type = BNXT_QPLIB_SWQE_TYPE_RDMA_WRITE_WITH_IMM;\n\t\twqe->rdma.imm_data = wr->ex.imm_data;\n\t\tbreak;\n\tcase IB_WR_RDMA_READ:\n\t\twqe->type = BNXT_QPLIB_SWQE_TYPE_RDMA_READ;\n\t\twqe->rdma.inv_key = wr->ex.invalidate_rkey;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\twqe->rdma.remote_va = rdma_wr(wr)->remote_addr;\n\twqe->rdma.r_key = rdma_wr(wr)->rkey;\n\tif (wr->send_flags & IB_SEND_SIGNALED)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SIGNAL_COMP;\n\tif (wr->send_flags & IB_SEND_FENCE)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_UC_FENCE;\n\tif (wr->send_flags & IB_SEND_SOLICITED)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SOLICIT_EVENT;\n\tif (wr->send_flags & IB_SEND_INLINE)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_INLINE;\n\n\treturn 0;\n}\n\nstatic int bnxt_re_build_atomic_wqe(const struct ib_send_wr *wr,\n\t\t\t\t    struct bnxt_qplib_swqe *wqe)\n{\n\tswitch (wr->opcode) {\n\tcase IB_WR_ATOMIC_CMP_AND_SWP:\n\t\twqe->type = BNXT_QPLIB_SWQE_TYPE_ATOMIC_CMP_AND_SWP;\n\t\twqe->atomic.cmp_data = atomic_wr(wr)->compare_add;\n\t\twqe->atomic.swap_data = atomic_wr(wr)->swap;\n\t\tbreak;\n\tcase IB_WR_ATOMIC_FETCH_AND_ADD:\n\t\twqe->type = BNXT_QPLIB_SWQE_TYPE_ATOMIC_FETCH_AND_ADD;\n\t\twqe->atomic.cmp_data = atomic_wr(wr)->compare_add;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\twqe->atomic.remote_va = atomic_wr(wr)->remote_addr;\n\twqe->atomic.r_key = atomic_wr(wr)->rkey;\n\tif (wr->send_flags & IB_SEND_SIGNALED)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SIGNAL_COMP;\n\tif (wr->send_flags & IB_SEND_FENCE)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_UC_FENCE;\n\tif (wr->send_flags & IB_SEND_SOLICITED)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SOLICIT_EVENT;\n\treturn 0;\n}\n\nstatic int bnxt_re_build_inv_wqe(const struct ib_send_wr *wr,\n\t\t\t\t struct bnxt_qplib_swqe *wqe)\n{\n\twqe->type = BNXT_QPLIB_SWQE_TYPE_LOCAL_INV;\n\twqe->local_inv.inv_l_key = wr->ex.invalidate_rkey;\n\n\t/* Need unconditional fence for local invalidate\n\t * opcode to work as expected.\n\t */\n\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_UC_FENCE;\n\n\tif (wr->send_flags & IB_SEND_SIGNALED)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SIGNAL_COMP;\n\tif (wr->send_flags & IB_SEND_SOLICITED)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SOLICIT_EVENT;\n\n\treturn 0;\n}\n\nstatic int bnxt_re_build_reg_wqe(const struct ib_reg_wr *wr,\n\t\t\t\t struct bnxt_qplib_swqe *wqe)\n{\n\tstruct bnxt_re_mr *mr = container_of(wr->mr, struct bnxt_re_mr, ib_mr);\n\tstruct bnxt_qplib_frpl *qplib_frpl = &mr->qplib_frpl;\n\tint access = wr->access;\n\n\twqe->frmr.pbl_ptr = (__le64 *)qplib_frpl->hwq.pbl_ptr[0];\n\twqe->frmr.pbl_dma_ptr = qplib_frpl->hwq.pbl_dma_ptr[0];\n\twqe->frmr.page_list = mr->pages;\n\twqe->frmr.page_list_len = mr->npages;\n\twqe->frmr.levels = qplib_frpl->hwq.level + 1;\n\twqe->type = BNXT_QPLIB_SWQE_TYPE_REG_MR;\n\n\t/* Need unconditional fence for reg_mr\n\t * opcode to function as expected.\n\t */\n\n\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_UC_FENCE;\n\n\tif (wr->wr.send_flags & IB_SEND_SIGNALED)\n\t\twqe->flags |= BNXT_QPLIB_SWQE_FLAGS_SIGNAL_COMP;\n\n\tif (access & IB_ACCESS_LOCAL_WRITE)\n\t\twqe->frmr.access_cntl |= SQ_FR_PMR_ACCESS_CNTL_LOCAL_WRITE;\n\tif (access & IB_ACCESS_REMOTE_READ)\n\t\twqe->frmr.access_cntl |= SQ_FR_PMR_ACCESS_CNTL_REMOTE_READ;\n\tif (access & IB_ACCESS_REMOTE_WRITE)\n\t\twqe->frmr.access_cntl |= SQ_FR_PMR_ACCESS_CNTL_REMOTE_WRITE;\n\tif (access & IB_ACCESS_REMOTE_ATOMIC)\n\t\twqe->frmr.access_cntl |= SQ_FR_PMR_ACCESS_CNTL_REMOTE_ATOMIC;\n\tif (access & IB_ACCESS_MW_BIND)\n\t\twqe->frmr.access_cntl |= SQ_FR_PMR_ACCESS_CNTL_WINDOW_BIND;\n\n\twqe->frmr.l_key = wr->key;\n\twqe->frmr.length = wr->mr->length;\n\twqe->frmr.pbl_pg_sz_log = (wr->mr->page_size >> PAGE_SHIFT_4K) - 1;\n\twqe->frmr.va = wr->mr->iova;\n\treturn 0;\n}\n\nstatic int bnxt_re_copy_inline_data(struct bnxt_re_dev *rdev,\n\t\t\t\t    const struct ib_send_wr *wr,\n\t\t\t\t    struct bnxt_qplib_swqe *wqe)\n{\n\t/*  Copy the inline data to the data  field */\n\tu8 *in_data;\n\tu32 i, sge_len;\n\tvoid *sge_addr;\n\n\tin_data = wqe->inline_data;\n\tfor (i = 0; i < wr->num_sge; i++) {\n\t\tsge_addr = (void *)(unsigned long)\n\t\t\t\twr->sg_list[i].addr;\n\t\tsge_len = wr->sg_list[i].length;\n\n\t\tif ((sge_len + wqe->inline_len) >\n\t\t    BNXT_QPLIB_SWQE_MAX_INLINE_LENGTH) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Inline data size requested > supported value\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tsge_len = wr->sg_list[i].length;\n\n\t\tmemcpy(in_data, sge_addr, sge_len);\n\t\tin_data += wr->sg_list[i].length;\n\t\twqe->inline_len += wr->sg_list[i].length;\n\t}\n\treturn wqe->inline_len;\n}\n\nstatic int bnxt_re_copy_wr_payload(struct bnxt_re_dev *rdev,\n\t\t\t\t   const struct ib_send_wr *wr,\n\t\t\t\t   struct bnxt_qplib_swqe *wqe)\n{\n\tint payload_sz = 0;\n\n\tif (wr->send_flags & IB_SEND_INLINE)\n\t\tpayload_sz = bnxt_re_copy_inline_data(rdev, wr, wqe);\n\telse\n\t\tpayload_sz = bnxt_re_build_sgl(wr->sg_list, wqe->sg_list,\n\t\t\t\t\t       wqe->num_sge);\n\n\treturn payload_sz;\n}\n\nstatic void bnxt_ud_qp_hw_stall_workaround(struct bnxt_re_qp *qp)\n{\n\tif ((qp->ib_qp.qp_type == IB_QPT_UD ||\n\t     qp->ib_qp.qp_type == IB_QPT_GSI ||\n\t     qp->ib_qp.qp_type == IB_QPT_RAW_ETHERTYPE) &&\n\t     qp->qplib_qp.wqe_cnt == BNXT_RE_UD_QP_HW_STALL) {\n\t\tint qp_attr_mask;\n\t\tstruct ib_qp_attr qp_attr;\n\n\t\tqp_attr_mask = IB_QP_STATE;\n\t\tqp_attr.qp_state = IB_QPS_RTS;\n\t\tbnxt_re_modify_qp(&qp->ib_qp, &qp_attr, qp_attr_mask, NULL);\n\t\tqp->qplib_qp.wqe_cnt = 0;\n\t}\n}\n\nstatic int bnxt_re_post_send_shadow_qp(struct bnxt_re_dev *rdev,\n\t\t\t\t       struct bnxt_re_qp *qp,\n\t\t\t\t       const struct ib_send_wr *wr)\n{\n\tstruct bnxt_qplib_swqe wqe;\n\tint rc = 0, payload_sz = 0;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->sq_lock, flags);\n\tmemset(&wqe, 0, sizeof(wqe));\n\twhile (wr) {\n\t\t/* House keeping */\n\t\tmemset(&wqe, 0, sizeof(wqe));\n\n\t\t/* Common */\n\t\twqe.num_sge = wr->num_sge;\n\t\tif (wr->num_sge > qp->qplib_qp.sq.max_sge) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Limit exceeded for Send SGEs\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto bad;\n\t\t}\n\n\t\tpayload_sz = bnxt_re_copy_wr_payload(qp->rdev, wr, &wqe);\n\t\tif (payload_sz < 0) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto bad;\n\t\t}\n\t\twqe.wr_id = wr->wr_id;\n\n\t\twqe.type = BNXT_QPLIB_SWQE_TYPE_SEND;\n\n\t\trc = bnxt_re_build_send_wqe(qp, wr, &wqe);\n\t\tif (!rc)\n\t\t\trc = bnxt_qplib_post_send(&qp->qplib_qp, &wqe);\nbad:\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Post send failed opcode = %#x rc = %d\",\n\t\t\t\twr->opcode, rc);\n\t\t\tbreak;\n\t\t}\n\t\twr = wr->next;\n\t}\n\tbnxt_qplib_post_send_db(&qp->qplib_qp);\n\tbnxt_ud_qp_hw_stall_workaround(qp);\n\tspin_unlock_irqrestore(&qp->sq_lock, flags);\n\treturn rc;\n}\n\nint bnxt_re_post_send(struct ib_qp *ib_qp, const struct ib_send_wr *wr,\n\t\t      const struct ib_send_wr **bad_wr)\n{\n\tstruct bnxt_re_qp *qp = container_of(ib_qp, struct bnxt_re_qp, ib_qp);\n\tstruct bnxt_qplib_swqe wqe;\n\tint rc = 0, payload_sz = 0;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->sq_lock, flags);\n\twhile (wr) {\n\t\t/* House keeping */\n\t\tmemset(&wqe, 0, sizeof(wqe));\n\n\t\t/* Common */\n\t\twqe.num_sge = wr->num_sge;\n\t\tif (wr->num_sge > qp->qplib_qp.sq.max_sge) {\n\t\t\tdev_err(rdev_to_dev(qp->rdev),\n\t\t\t\t\"Limit exceeded for Send SGEs\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto bad;\n\t\t}\n\n\t\tpayload_sz = bnxt_re_copy_wr_payload(qp->rdev, wr, &wqe);\n\t\tif (payload_sz < 0) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto bad;\n\t\t}\n\t\twqe.wr_id = wr->wr_id;\n\n\t\tswitch (wr->opcode) {\n\t\tcase IB_WR_SEND:\n\t\tcase IB_WR_SEND_WITH_IMM:\n\t\t\tif (qp->qplib_qp.type == CMDQ_CREATE_QP1_TYPE_GSI) {\n\t\t\t\trc = bnxt_re_build_qp1_send_v2(qp, wr, &wqe,\n\t\t\t\t\t\t\t       payload_sz);\n\t\t\t\tif (rc)\n\t\t\t\t\tgoto bad;\n\t\t\t\twqe.rawqp1.lflags |=\n\t\t\t\t\tSQ_SEND_RAWETH_QP1_LFLAGS_ROCE_CRC;\n\t\t\t}\n\t\t\tswitch (wr->send_flags) {\n\t\t\tcase IB_SEND_IP_CSUM:\n\t\t\t\twqe.rawqp1.lflags |=\n\t\t\t\t\tSQ_SEND_RAWETH_QP1_LFLAGS_IP_CHKSUM;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* fall through */\n\t\tcase IB_WR_SEND_WITH_INV:\n\t\t\trc = bnxt_re_build_send_wqe(qp, wr, &wqe);\n\t\t\tbreak;\n\t\tcase IB_WR_RDMA_WRITE:\n\t\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\tcase IB_WR_RDMA_READ:\n\t\t\trc = bnxt_re_build_rdma_wqe(wr, &wqe);\n\t\t\tbreak;\n\t\tcase IB_WR_ATOMIC_CMP_AND_SWP:\n\t\tcase IB_WR_ATOMIC_FETCH_AND_ADD:\n\t\t\trc = bnxt_re_build_atomic_wqe(wr, &wqe);\n\t\t\tbreak;\n\t\tcase IB_WR_RDMA_READ_WITH_INV:\n\t\t\tdev_err(rdev_to_dev(qp->rdev),\n\t\t\t\t\"RDMA Read with Invalidate is not supported\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto bad;\n\t\tcase IB_WR_LOCAL_INV:\n\t\t\trc = bnxt_re_build_inv_wqe(wr, &wqe);\n\t\t\tbreak;\n\t\tcase IB_WR_REG_MR:\n\t\t\trc = bnxt_re_build_reg_wqe(reg_wr(wr), &wqe);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t/* Unsupported WRs */\n\t\t\tdev_err(rdev_to_dev(qp->rdev),\n\t\t\t\t\"WR (%#x) is not supported\", wr->opcode);\n\t\t\trc = -EINVAL;\n\t\t\tgoto bad;\n\t\t}\n\t\tif (!rc)\n\t\t\trc = bnxt_qplib_post_send(&qp->qplib_qp, &wqe);\nbad:\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(qp->rdev),\n\t\t\t\t\"post_send failed op:%#x qps = %#x rc = %d\\n\",\n\t\t\t\twr->opcode, qp->qplib_qp.state, rc);\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\twr = wr->next;\n\t}\n\tbnxt_qplib_post_send_db(&qp->qplib_qp);\n\tbnxt_ud_qp_hw_stall_workaround(qp);\n\tspin_unlock_irqrestore(&qp->sq_lock, flags);\n\n\treturn rc;\n}\n\nstatic int bnxt_re_post_recv_shadow_qp(struct bnxt_re_dev *rdev,\n\t\t\t\t       struct bnxt_re_qp *qp,\n\t\t\t\t       const struct ib_recv_wr *wr)\n{\n\tstruct bnxt_qplib_swqe wqe;\n\tint rc = 0;\n\n\tmemset(&wqe, 0, sizeof(wqe));\n\twhile (wr) {\n\t\t/* House keeping */\n\t\tmemset(&wqe, 0, sizeof(wqe));\n\n\t\t/* Common */\n\t\twqe.num_sge = wr->num_sge;\n\t\tif (wr->num_sge > qp->qplib_qp.rq.max_sge) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Limit exceeded for Receive SGEs\");\n\t\t\trc = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tbnxt_re_build_sgl(wr->sg_list, wqe.sg_list, wr->num_sge);\n\t\twqe.wr_id = wr->wr_id;\n\t\twqe.type = BNXT_QPLIB_SWQE_TYPE_RECV;\n\n\t\trc = bnxt_qplib_post_recv(&qp->qplib_qp, &wqe);\n\t\tif (rc)\n\t\t\tbreak;\n\n\t\twr = wr->next;\n\t}\n\tif (!rc)\n\t\tbnxt_qplib_post_recv_db(&qp->qplib_qp);\n\treturn rc;\n}\n\nint bnxt_re_post_recv(struct ib_qp *ib_qp, const struct ib_recv_wr *wr,\n\t\t      const struct ib_recv_wr **bad_wr)\n{\n\tstruct bnxt_re_qp *qp = container_of(ib_qp, struct bnxt_re_qp, ib_qp);\n\tstruct bnxt_qplib_swqe wqe;\n\tint rc = 0, payload_sz = 0;\n\tunsigned long flags;\n\tu32 count = 0;\n\n\tspin_lock_irqsave(&qp->rq_lock, flags);\n\twhile (wr) {\n\t\t/* House keeping */\n\t\tmemset(&wqe, 0, sizeof(wqe));\n\n\t\t/* Common */\n\t\twqe.num_sge = wr->num_sge;\n\t\tif (wr->num_sge > qp->qplib_qp.rq.max_sge) {\n\t\t\tdev_err(rdev_to_dev(qp->rdev),\n\t\t\t\t\"Limit exceeded for Receive SGEs\");\n\t\t\trc = -EINVAL;\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\n\t\tpayload_sz = bnxt_re_build_sgl(wr->sg_list, wqe.sg_list,\n\t\t\t\t\t       wr->num_sge);\n\t\twqe.wr_id = wr->wr_id;\n\t\twqe.type = BNXT_QPLIB_SWQE_TYPE_RECV;\n\n\t\tif (ib_qp->qp_type == IB_QPT_GSI &&\n\t\t    qp->qplib_qp.type != CMDQ_CREATE_QP_TYPE_GSI)\n\t\t\trc = bnxt_re_build_qp1_shadow_qp_recv(qp, wr, &wqe,\n\t\t\t\t\t\t\t      payload_sz);\n\t\tif (!rc)\n\t\t\trc = bnxt_qplib_post_recv(&qp->qplib_qp, &wqe);\n\t\tif (rc) {\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Ring DB if the RQEs posted reaches a threshold value */\n\t\tif (++count >= BNXT_RE_RQ_WQE_THRESHOLD) {\n\t\t\tbnxt_qplib_post_recv_db(&qp->qplib_qp);\n\t\t\tcount = 0;\n\t\t}\n\n\t\twr = wr->next;\n\t}\n\n\tif (count)\n\t\tbnxt_qplib_post_recv_db(&qp->qplib_qp);\n\n\tspin_unlock_irqrestore(&qp->rq_lock, flags);\n\n\treturn rc;\n}\n\n/* Completion Queues */\nvoid bnxt_re_destroy_cq(struct ib_cq *ib_cq, struct ib_udata *udata)\n{\n\tstruct bnxt_re_cq *cq;\n\tstruct bnxt_qplib_nq *nq;\n\tstruct bnxt_re_dev *rdev;\n\n\tcq = container_of(ib_cq, struct bnxt_re_cq, ib_cq);\n\trdev = cq->rdev;\n\tnq = cq->qplib_cq.nq;\n\n\tbnxt_qplib_destroy_cq(&rdev->qplib_res, &cq->qplib_cq);\n\tib_umem_release(cq->umem);\n\n\tatomic_dec(&rdev->cq_count);\n\tnq->budget--;\n\tkfree(cq->cql);\n}\n\nint bnxt_re_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,\n\t\t      struct ib_udata *udata)\n{\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(ibcq->device, ibdev);\n\tstruct bnxt_qplib_dev_attr *dev_attr = &rdev->dev_attr;\n\tstruct bnxt_re_cq *cq = container_of(ibcq, struct bnxt_re_cq, ib_cq);\n\tint rc, entries;\n\tint cqe = attr->cqe;\n\tstruct bnxt_qplib_nq *nq = NULL;\n\tunsigned int nq_alloc_cnt;\n\n\t/* Validate CQ fields */\n\tif (cqe < 1 || cqe > dev_attr->max_cq_wqes) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to create CQ -max exceeded\");\n\t\treturn -EINVAL;\n\t}\n\n\tcq->rdev = rdev;\n\tcq->qplib_cq.cq_handle = (u64)(unsigned long)(&cq->qplib_cq);\n\n\tentries = roundup_pow_of_two(cqe + 1);\n\tif (entries > dev_attr->max_cq_wqes + 1)\n\t\tentries = dev_attr->max_cq_wqes + 1;\n\n\tif (udata) {\n\t\tstruct bnxt_re_cq_req req;\n\t\tstruct bnxt_re_ucontext *uctx = rdma_udata_to_drv_context(\n\t\t\tudata, struct bnxt_re_ucontext, ib_uctx);\n\t\tif (ib_copy_from_udata(&req, udata, sizeof(req))) {\n\t\t\trc = -EFAULT;\n\t\t\tgoto fail;\n\t\t}\n\n\t\tcq->umem = ib_umem_get(udata, req.cq_va,\n\t\t\t\t       entries * sizeof(struct cq_base),\n\t\t\t\t       IB_ACCESS_LOCAL_WRITE, 1);\n\t\tif (IS_ERR(cq->umem)) {\n\t\t\trc = PTR_ERR(cq->umem);\n\t\t\tgoto fail;\n\t\t}\n\t\tcq->qplib_cq.sg_info.sglist = cq->umem->sg_head.sgl;\n\t\tcq->qplib_cq.sg_info.npages = ib_umem_num_pages(cq->umem);\n\t\tcq->qplib_cq.sg_info.nmap = cq->umem->nmap;\n\t\tcq->qplib_cq.dpi = &uctx->dpi;\n\t} else {\n\t\tcq->max_cql = min_t(u32, entries, MAX_CQL_PER_POLL);\n\t\tcq->cql = kcalloc(cq->max_cql, sizeof(struct bnxt_qplib_cqe),\n\t\t\t\t  GFP_KERNEL);\n\t\tif (!cq->cql) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto fail;\n\t\t}\n\n\t\tcq->qplib_cq.dpi = &rdev->dpi_privileged;\n\t}\n\t/*\n\t * Allocating the NQ in a round robin fashion. nq_alloc_cnt is a\n\t * used for getting the NQ index.\n\t */\n\tnq_alloc_cnt = atomic_inc_return(&rdev->nq_alloc_cnt);\n\tnq = &rdev->nq[nq_alloc_cnt % (rdev->num_msix - 1)];\n\tcq->qplib_cq.max_wqe = entries;\n\tcq->qplib_cq.cnq_hw_ring_id = nq->ring_id;\n\tcq->qplib_cq.nq\t= nq;\n\n\trc = bnxt_qplib_create_cq(&rdev->qplib_res, &cq->qplib_cq);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to create HW CQ\");\n\t\tgoto fail;\n\t}\n\n\tcq->ib_cq.cqe = entries;\n\tcq->cq_period = cq->qplib_cq.period;\n\tnq->budget++;\n\n\tatomic_inc(&rdev->cq_count);\n\tspin_lock_init(&cq->cq_lock);\n\n\tif (udata) {\n\t\tstruct bnxt_re_cq_resp resp;\n\n\t\tresp.cqid = cq->qplib_cq.id;\n\t\tresp.tail = cq->qplib_cq.hwq.cons;\n\t\tresp.phase = cq->qplib_cq.period;\n\t\tresp.rsvd = 0;\n\t\trc = ib_copy_to_udata(udata, &resp, sizeof(resp));\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"Failed to copy CQ udata\");\n\t\t\tbnxt_qplib_destroy_cq(&rdev->qplib_res, &cq->qplib_cq);\n\t\t\tgoto c2fail;\n\t\t}\n\t}\n\n\treturn 0;\n\nc2fail:\n\tib_umem_release(cq->umem);\nfail:\n\tkfree(cq->cql);\n\treturn rc;\n}\n\nstatic u8 __req_to_ib_wc_status(u8 qstatus)\n{\n\tswitch (qstatus) {\n\tcase CQ_REQ_STATUS_OK:\n\t\treturn IB_WC_SUCCESS;\n\tcase CQ_REQ_STATUS_BAD_RESPONSE_ERR:\n\t\treturn IB_WC_BAD_RESP_ERR;\n\tcase CQ_REQ_STATUS_LOCAL_LENGTH_ERR:\n\t\treturn IB_WC_LOC_LEN_ERR;\n\tcase CQ_REQ_STATUS_LOCAL_QP_OPERATION_ERR:\n\t\treturn IB_WC_LOC_QP_OP_ERR;\n\tcase CQ_REQ_STATUS_LOCAL_PROTECTION_ERR:\n\t\treturn IB_WC_LOC_PROT_ERR;\n\tcase CQ_REQ_STATUS_MEMORY_MGT_OPERATION_ERR:\n\t\treturn IB_WC_GENERAL_ERR;\n\tcase CQ_REQ_STATUS_REMOTE_INVALID_REQUEST_ERR:\n\t\treturn IB_WC_REM_INV_REQ_ERR;\n\tcase CQ_REQ_STATUS_REMOTE_ACCESS_ERR:\n\t\treturn IB_WC_REM_ACCESS_ERR;\n\tcase CQ_REQ_STATUS_REMOTE_OPERATION_ERR:\n\t\treturn IB_WC_REM_OP_ERR;\n\tcase CQ_REQ_STATUS_RNR_NAK_RETRY_CNT_ERR:\n\t\treturn IB_WC_RNR_RETRY_EXC_ERR;\n\tcase CQ_REQ_STATUS_TRANSPORT_RETRY_CNT_ERR:\n\t\treturn IB_WC_RETRY_EXC_ERR;\n\tcase CQ_REQ_STATUS_WORK_REQUEST_FLUSHED_ERR:\n\t\treturn IB_WC_WR_FLUSH_ERR;\n\tdefault:\n\t\treturn IB_WC_GENERAL_ERR;\n\t}\n\treturn 0;\n}\n\nstatic u8 __rawqp1_to_ib_wc_status(u8 qstatus)\n{\n\tswitch (qstatus) {\n\tcase CQ_RES_RAWETH_QP1_STATUS_OK:\n\t\treturn IB_WC_SUCCESS;\n\tcase CQ_RES_RAWETH_QP1_STATUS_LOCAL_ACCESS_ERROR:\n\t\treturn IB_WC_LOC_ACCESS_ERR;\n\tcase CQ_RES_RAWETH_QP1_STATUS_HW_LOCAL_LENGTH_ERR:\n\t\treturn IB_WC_LOC_LEN_ERR;\n\tcase CQ_RES_RAWETH_QP1_STATUS_LOCAL_PROTECTION_ERR:\n\t\treturn IB_WC_LOC_PROT_ERR;\n\tcase CQ_RES_RAWETH_QP1_STATUS_LOCAL_QP_OPERATION_ERR:\n\t\treturn IB_WC_LOC_QP_OP_ERR;\n\tcase CQ_RES_RAWETH_QP1_STATUS_MEMORY_MGT_OPERATION_ERR:\n\t\treturn IB_WC_GENERAL_ERR;\n\tcase CQ_RES_RAWETH_QP1_STATUS_WORK_REQUEST_FLUSHED_ERR:\n\t\treturn IB_WC_WR_FLUSH_ERR;\n\tcase CQ_RES_RAWETH_QP1_STATUS_HW_FLUSH_ERR:\n\t\treturn IB_WC_WR_FLUSH_ERR;\n\tdefault:\n\t\treturn IB_WC_GENERAL_ERR;\n\t}\n}\n\nstatic u8 __rc_to_ib_wc_status(u8 qstatus)\n{\n\tswitch (qstatus) {\n\tcase CQ_RES_RC_STATUS_OK:\n\t\treturn IB_WC_SUCCESS;\n\tcase CQ_RES_RC_STATUS_LOCAL_ACCESS_ERROR:\n\t\treturn IB_WC_LOC_ACCESS_ERR;\n\tcase CQ_RES_RC_STATUS_LOCAL_LENGTH_ERR:\n\t\treturn IB_WC_LOC_LEN_ERR;\n\tcase CQ_RES_RC_STATUS_LOCAL_PROTECTION_ERR:\n\t\treturn IB_WC_LOC_PROT_ERR;\n\tcase CQ_RES_RC_STATUS_LOCAL_QP_OPERATION_ERR:\n\t\treturn IB_WC_LOC_QP_OP_ERR;\n\tcase CQ_RES_RC_STATUS_MEMORY_MGT_OPERATION_ERR:\n\t\treturn IB_WC_GENERAL_ERR;\n\tcase CQ_RES_RC_STATUS_REMOTE_INVALID_REQUEST_ERR:\n\t\treturn IB_WC_REM_INV_REQ_ERR;\n\tcase CQ_RES_RC_STATUS_WORK_REQUEST_FLUSHED_ERR:\n\t\treturn IB_WC_WR_FLUSH_ERR;\n\tcase CQ_RES_RC_STATUS_HW_FLUSH_ERR:\n\t\treturn IB_WC_WR_FLUSH_ERR;\n\tdefault:\n\t\treturn IB_WC_GENERAL_ERR;\n\t}\n}\n\nstatic void bnxt_re_process_req_wc(struct ib_wc *wc, struct bnxt_qplib_cqe *cqe)\n{\n\tswitch (cqe->type) {\n\tcase BNXT_QPLIB_SWQE_TYPE_SEND:\n\t\twc->opcode = IB_WC_SEND;\n\t\tbreak;\n\tcase BNXT_QPLIB_SWQE_TYPE_SEND_WITH_IMM:\n\t\twc->opcode = IB_WC_SEND;\n\t\twc->wc_flags |= IB_WC_WITH_IMM;\n\t\tbreak;\n\tcase BNXT_QPLIB_SWQE_TYPE_SEND_WITH_INV:\n\t\twc->opcode = IB_WC_SEND;\n\t\twc->wc_flags |= IB_WC_WITH_INVALIDATE;\n\t\tbreak;\n\tcase BNXT_QPLIB_SWQE_TYPE_RDMA_WRITE:\n\t\twc->opcode = IB_WC_RDMA_WRITE;\n\t\tbreak;\n\tcase BNXT_QPLIB_SWQE_TYPE_RDMA_WRITE_WITH_IMM:\n\t\twc->opcode = IB_WC_RDMA_WRITE;\n\t\twc->wc_flags |= IB_WC_WITH_IMM;\n\t\tbreak;\n\tcase BNXT_QPLIB_SWQE_TYPE_RDMA_READ:\n\t\twc->opcode = IB_WC_RDMA_READ;\n\t\tbreak;\n\tcase BNXT_QPLIB_SWQE_TYPE_ATOMIC_CMP_AND_SWP:\n\t\twc->opcode = IB_WC_COMP_SWAP;\n\t\tbreak;\n\tcase BNXT_QPLIB_SWQE_TYPE_ATOMIC_FETCH_AND_ADD:\n\t\twc->opcode = IB_WC_FETCH_ADD;\n\t\tbreak;\n\tcase BNXT_QPLIB_SWQE_TYPE_LOCAL_INV:\n\t\twc->opcode = IB_WC_LOCAL_INV;\n\t\tbreak;\n\tcase BNXT_QPLIB_SWQE_TYPE_REG_MR:\n\t\twc->opcode = IB_WC_REG_MR;\n\t\tbreak;\n\tdefault:\n\t\twc->opcode = IB_WC_SEND;\n\t\tbreak;\n\t}\n\n\twc->status = __req_to_ib_wc_status(cqe->status);\n}\n\nstatic int bnxt_re_check_packet_type(u16 raweth_qp1_flags,\n\t\t\t\t     u16 raweth_qp1_flags2)\n{\n\tbool is_ipv6 = false, is_ipv4 = false;\n\n\t/* raweth_qp1_flags Bit 9-6 indicates itype */\n\tif ((raweth_qp1_flags & CQ_RES_RAWETH_QP1_RAWETH_QP1_FLAGS_ITYPE_ROCE)\n\t    != CQ_RES_RAWETH_QP1_RAWETH_QP1_FLAGS_ITYPE_ROCE)\n\t\treturn -1;\n\n\tif (raweth_qp1_flags2 &\n\t    CQ_RES_RAWETH_QP1_RAWETH_QP1_FLAGS2_IP_CS_CALC &&\n\t    raweth_qp1_flags2 &\n\t    CQ_RES_RAWETH_QP1_RAWETH_QP1_FLAGS2_L4_CS_CALC) {\n\t\t/* raweth_qp1_flags2 Bit 8 indicates ip_type. 0-v4 1 - v6 */\n\t\t(raweth_qp1_flags2 &\n\t\t CQ_RES_RAWETH_QP1_RAWETH_QP1_FLAGS2_IP_TYPE) ?\n\t\t\t(is_ipv6 = true) : (is_ipv4 = true);\n\t\treturn ((is_ipv6) ?\n\t\t\t BNXT_RE_ROCEV2_IPV6_PACKET :\n\t\t\t BNXT_RE_ROCEV2_IPV4_PACKET);\n\t} else {\n\t\treturn BNXT_RE_ROCE_V1_PACKET;\n\t}\n}\n\nstatic int bnxt_re_to_ib_nw_type(int nw_type)\n{\n\tu8 nw_hdr_type = 0xFF;\n\n\tswitch (nw_type) {\n\tcase BNXT_RE_ROCE_V1_PACKET:\n\t\tnw_hdr_type = RDMA_NETWORK_ROCE_V1;\n\t\tbreak;\n\tcase BNXT_RE_ROCEV2_IPV4_PACKET:\n\t\tnw_hdr_type = RDMA_NETWORK_IPV4;\n\t\tbreak;\n\tcase BNXT_RE_ROCEV2_IPV6_PACKET:\n\t\tnw_hdr_type = RDMA_NETWORK_IPV6;\n\t\tbreak;\n\t}\n\treturn nw_hdr_type;\n}\n\nstatic bool bnxt_re_is_loopback_packet(struct bnxt_re_dev *rdev,\n\t\t\t\t       void *rq_hdr_buf)\n{\n\tu8 *tmp_buf = NULL;\n\tstruct ethhdr *eth_hdr;\n\tu16 eth_type;\n\tbool rc = false;\n\n\ttmp_buf = (u8 *)rq_hdr_buf;\n\t/*\n\t * If dest mac is not same as I/F mac, this could be a\n\t * loopback address or multicast address, check whether\n\t * it is a loopback packet\n\t */\n\tif (!ether_addr_equal(tmp_buf, rdev->netdev->dev_addr)) {\n\t\ttmp_buf += 4;\n\t\t/* Check the  ether type */\n\t\teth_hdr = (struct ethhdr *)tmp_buf;\n\t\teth_type = ntohs(eth_hdr->h_proto);\n\t\tswitch (eth_type) {\n\t\tcase ETH_P_IBOE:\n\t\t\trc = true;\n\t\t\tbreak;\n\t\tcase ETH_P_IP:\n\t\tcase ETH_P_IPV6: {\n\t\t\tu32 len;\n\t\t\tstruct udphdr *udp_hdr;\n\n\t\t\tlen = (eth_type == ETH_P_IP ? sizeof(struct iphdr) :\n\t\t\t\t\t\t      sizeof(struct ipv6hdr));\n\t\t\ttmp_buf += sizeof(struct ethhdr) + len;\n\t\t\tudp_hdr = (struct udphdr *)tmp_buf;\n\t\t\tif (ntohs(udp_hdr->dest) ==\n\t\t\t\t    ROCE_V2_UDP_DPORT)\n\t\t\t\trc = true;\n\t\t\tbreak;\n\t\t\t}\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn rc;\n}\n\nstatic int bnxt_re_process_raw_qp_pkt_rx(struct bnxt_re_qp *qp1_qp,\n\t\t\t\t\t struct bnxt_qplib_cqe *cqe)\n{\n\tstruct bnxt_re_dev *rdev = qp1_qp->rdev;\n\tstruct bnxt_re_sqp_entries *sqp_entry = NULL;\n\tstruct bnxt_re_qp *qp = rdev->qp1_sqp;\n\tstruct ib_send_wr *swr;\n\tstruct ib_ud_wr udwr;\n\tstruct ib_recv_wr rwr;\n\tint pkt_type = 0;\n\tu32 tbl_idx;\n\tvoid *rq_hdr_buf;\n\tdma_addr_t rq_hdr_buf_map;\n\tdma_addr_t shrq_hdr_buf_map;\n\tu32 offset = 0;\n\tu32 skip_bytes = 0;\n\tstruct ib_sge s_sge[2];\n\tstruct ib_sge r_sge[2];\n\tint rc;\n\n\tmemset(&udwr, 0, sizeof(udwr));\n\tmemset(&rwr, 0, sizeof(rwr));\n\tmemset(&s_sge, 0, sizeof(s_sge));\n\tmemset(&r_sge, 0, sizeof(r_sge));\n\n\tswr = &udwr.wr;\n\ttbl_idx = cqe->wr_id;\n\n\trq_hdr_buf = qp1_qp->qplib_qp.rq_hdr_buf +\n\t\t\t(tbl_idx * qp1_qp->qplib_qp.rq_hdr_buf_size);\n\trq_hdr_buf_map = bnxt_qplib_get_qp_buf_from_index(&qp1_qp->qplib_qp,\n\t\t\t\t\t\t\t  tbl_idx);\n\n\t/* Shadow QP header buffer */\n\tshrq_hdr_buf_map = bnxt_qplib_get_qp_buf_from_index(&qp->qplib_qp,\n\t\t\t\t\t\t\t    tbl_idx);\n\tsqp_entry = &rdev->sqp_tbl[tbl_idx];\n\n\t/* Store this cqe */\n\tmemcpy(&sqp_entry->cqe, cqe, sizeof(struct bnxt_qplib_cqe));\n\tsqp_entry->qp1_qp = qp1_qp;\n\n\t/* Find packet type from the cqe */\n\n\tpkt_type = bnxt_re_check_packet_type(cqe->raweth_qp1_flags,\n\t\t\t\t\t     cqe->raweth_qp1_flags2);\n\tif (pkt_type < 0) {\n\t\tdev_err(rdev_to_dev(rdev), \"Invalid packet\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Adjust the offset for the user buffer and post in the rq */\n\n\tif (pkt_type == BNXT_RE_ROCEV2_IPV4_PACKET)\n\t\toffset = 20;\n\n\t/*\n\t * QP1 loopback packet has 4 bytes of internal header before\n\t * ether header. Skip these four bytes.\n\t */\n\tif (bnxt_re_is_loopback_packet(rdev, rq_hdr_buf))\n\t\tskip_bytes = 4;\n\n\t/* First send SGE . Skip the ether header*/\n\ts_sge[0].addr = rq_hdr_buf_map + BNXT_QPLIB_MAX_QP1_RQ_ETH_HDR_SIZE\n\t\t\t+ skip_bytes;\n\ts_sge[0].lkey = 0xFFFFFFFF;\n\ts_sge[0].length = offset ? BNXT_QPLIB_MAX_GRH_HDR_SIZE_IPV4 :\n\t\t\t\tBNXT_QPLIB_MAX_GRH_HDR_SIZE_IPV6;\n\n\t/* Second Send SGE */\n\ts_sge[1].addr = s_sge[0].addr + s_sge[0].length +\n\t\t\tBNXT_QPLIB_MAX_QP1_RQ_BDETH_HDR_SIZE;\n\tif (pkt_type != BNXT_RE_ROCE_V1_PACKET)\n\t\ts_sge[1].addr += 8;\n\ts_sge[1].lkey = 0xFFFFFFFF;\n\ts_sge[1].length = 256;\n\n\t/* First recv SGE */\n\n\tr_sge[0].addr = shrq_hdr_buf_map;\n\tr_sge[0].lkey = 0xFFFFFFFF;\n\tr_sge[0].length = 40;\n\n\tr_sge[1].addr = sqp_entry->sge.addr + offset;\n\tr_sge[1].lkey = sqp_entry->sge.lkey;\n\tr_sge[1].length = BNXT_QPLIB_MAX_GRH_HDR_SIZE_IPV6 + 256 - offset;\n\n\t/* Create receive work request */\n\trwr.num_sge = 2;\n\trwr.sg_list = r_sge;\n\trwr.wr_id = tbl_idx;\n\trwr.next = NULL;\n\n\trc = bnxt_re_post_recv_shadow_qp(rdev, qp, &rwr);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\"Failed to post Rx buffers to shadow QP\");\n\t\treturn -ENOMEM;\n\t}\n\n\tswr->num_sge = 2;\n\tswr->sg_list = s_sge;\n\tswr->wr_id = tbl_idx;\n\tswr->opcode = IB_WR_SEND;\n\tswr->next = NULL;\n\n\tudwr.ah = &rdev->sqp_ah->ib_ah;\n\tudwr.remote_qpn = rdev->qp1_sqp->qplib_qp.id;\n\tudwr.remote_qkey = rdev->qp1_sqp->qplib_qp.qkey;\n\n\t/* post data received  in the send queue */\n\trc = bnxt_re_post_send_shadow_qp(rdev, qp, swr);\n\n\treturn 0;\n}\n\nstatic void bnxt_re_process_res_rawqp1_wc(struct ib_wc *wc,\n\t\t\t\t\t  struct bnxt_qplib_cqe *cqe)\n{\n\twc->opcode = IB_WC_RECV;\n\twc->status = __rawqp1_to_ib_wc_status(cqe->status);\n\twc->wc_flags |= IB_WC_GRH;\n}\n\nstatic bool bnxt_re_is_vlan_pkt(struct bnxt_qplib_cqe *orig_cqe,\n\t\t\t\tu16 *vid, u8 *sl)\n{\n\tbool ret = false;\n\tu32 metadata;\n\tu16 tpid;\n\n\tmetadata = orig_cqe->raweth_qp1_metadata;\n\tif (orig_cqe->raweth_qp1_flags2 &\n\t\tCQ_RES_RAWETH_QP1_RAWETH_QP1_FLAGS2_META_FORMAT_VLAN) {\n\t\ttpid = ((metadata &\n\t\t\t CQ_RES_RAWETH_QP1_RAWETH_QP1_METADATA_TPID_MASK) >>\n\t\t\t CQ_RES_RAWETH_QP1_RAWETH_QP1_METADATA_TPID_SFT);\n\t\tif (tpid == ETH_P_8021Q) {\n\t\t\t*vid = metadata &\n\t\t\t       CQ_RES_RAWETH_QP1_RAWETH_QP1_METADATA_VID_MASK;\n\t\t\t*sl = (metadata &\n\t\t\t       CQ_RES_RAWETH_QP1_RAWETH_QP1_METADATA_PRI_MASK) >>\n\t\t\t       CQ_RES_RAWETH_QP1_RAWETH_QP1_METADATA_PRI_SFT;\n\t\t\tret = true;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic void bnxt_re_process_res_rc_wc(struct ib_wc *wc,\n\t\t\t\t      struct bnxt_qplib_cqe *cqe)\n{\n\twc->opcode = IB_WC_RECV;\n\twc->status = __rc_to_ib_wc_status(cqe->status);\n\n\tif (cqe->flags & CQ_RES_RC_FLAGS_IMM)\n\t\twc->wc_flags |= IB_WC_WITH_IMM;\n\tif (cqe->flags & CQ_RES_RC_FLAGS_INV)\n\t\twc->wc_flags |= IB_WC_WITH_INVALIDATE;\n\tif ((cqe->flags & (CQ_RES_RC_FLAGS_RDMA | CQ_RES_RC_FLAGS_IMM)) ==\n\t    (CQ_RES_RC_FLAGS_RDMA | CQ_RES_RC_FLAGS_IMM))\n\t\twc->opcode = IB_WC_RECV_RDMA_WITH_IMM;\n}\n\nstatic void bnxt_re_process_res_shadow_qp_wc(struct bnxt_re_qp *qp,\n\t\t\t\t\t     struct ib_wc *wc,\n\t\t\t\t\t     struct bnxt_qplib_cqe *cqe)\n{\n\tstruct bnxt_re_dev *rdev = qp->rdev;\n\tstruct bnxt_re_qp *qp1_qp = NULL;\n\tstruct bnxt_qplib_cqe *orig_cqe = NULL;\n\tstruct bnxt_re_sqp_entries *sqp_entry = NULL;\n\tint nw_type;\n\tu32 tbl_idx;\n\tu16 vlan_id;\n\tu8 sl;\n\n\ttbl_idx = cqe->wr_id;\n\n\tsqp_entry = &rdev->sqp_tbl[tbl_idx];\n\tqp1_qp = sqp_entry->qp1_qp;\n\torig_cqe = &sqp_entry->cqe;\n\n\twc->wr_id = sqp_entry->wrid;\n\twc->byte_len = orig_cqe->length;\n\twc->qp = &qp1_qp->ib_qp;\n\n\twc->ex.imm_data = orig_cqe->immdata;\n\twc->src_qp = orig_cqe->src_qp;\n\tmemcpy(wc->smac, orig_cqe->smac, ETH_ALEN);\n\tif (bnxt_re_is_vlan_pkt(orig_cqe, &vlan_id, &sl)) {\n\t\twc->vlan_id = vlan_id;\n\t\twc->sl = sl;\n\t\twc->wc_flags |= IB_WC_WITH_VLAN;\n\t}\n\twc->port_num = 1;\n\twc->vendor_err = orig_cqe->status;\n\n\twc->opcode = IB_WC_RECV;\n\twc->status = __rawqp1_to_ib_wc_status(orig_cqe->status);\n\twc->wc_flags |= IB_WC_GRH;\n\n\tnw_type = bnxt_re_check_packet_type(orig_cqe->raweth_qp1_flags,\n\t\t\t\t\t    orig_cqe->raweth_qp1_flags2);\n\tif (nw_type >= 0) {\n\t\twc->network_hdr_type = bnxt_re_to_ib_nw_type(nw_type);\n\t\twc->wc_flags |= IB_WC_WITH_NETWORK_HDR_TYPE;\n\t}\n}\n\nstatic void bnxt_re_process_res_ud_wc(struct bnxt_re_qp *qp,\n\t\t\t\t      struct ib_wc *wc,\n\t\t\t\t      struct bnxt_qplib_cqe *cqe)\n{\n\tu8 nw_type;\n\n\twc->opcode = IB_WC_RECV;\n\twc->status = __rc_to_ib_wc_status(cqe->status);\n\n\tif (cqe->flags & CQ_RES_UD_FLAGS_IMM)\n\t\twc->wc_flags |= IB_WC_WITH_IMM;\n\t/* report only on GSI QP for Thor */\n\tif (qp->qplib_qp.type == CMDQ_CREATE_QP_TYPE_GSI) {\n\t\twc->wc_flags |= IB_WC_GRH;\n\t\tmemcpy(wc->smac, cqe->smac, ETH_ALEN);\n\t\twc->wc_flags |= IB_WC_WITH_SMAC;\n\t\tif (cqe->flags & CQ_RES_UD_FLAGS_META_FORMAT_VLAN) {\n\t\t\twc->vlan_id = (cqe->cfa_meta & 0xFFF);\n\t\t\tif (wc->vlan_id < 0x1000)\n\t\t\t\twc->wc_flags |= IB_WC_WITH_VLAN;\n\t\t}\n\t\tnw_type = (cqe->flags & CQ_RES_UD_FLAGS_ROCE_IP_VER_MASK) >>\n\t\t\t   CQ_RES_UD_FLAGS_ROCE_IP_VER_SFT;\n\t\twc->network_hdr_type = bnxt_re_to_ib_nw_type(nw_type);\n\t\twc->wc_flags |= IB_WC_WITH_NETWORK_HDR_TYPE;\n\t}\n\n}\n\nstatic int send_phantom_wqe(struct bnxt_re_qp *qp)\n{\n\tstruct bnxt_qplib_qp *lib_qp = &qp->qplib_qp;\n\tunsigned long flags;\n\tint rc = 0;\n\n\tspin_lock_irqsave(&qp->sq_lock, flags);\n\n\trc = bnxt_re_bind_fence_mw(lib_qp);\n\tif (!rc) {\n\t\tlib_qp->sq.phantom_wqe_cnt++;\n\t\tdev_dbg(&lib_qp->sq.hwq.pdev->dev,\n\t\t\t\"qp %#x sq->prod %#x sw_prod %#x phantom_wqe_cnt %d\\n\",\n\t\t\tlib_qp->id, lib_qp->sq.hwq.prod,\n\t\t\tHWQ_CMP(lib_qp->sq.hwq.prod, &lib_qp->sq.hwq),\n\t\t\tlib_qp->sq.phantom_wqe_cnt);\n\t}\n\n\tspin_unlock_irqrestore(&qp->sq_lock, flags);\n\treturn rc;\n}\n\nint bnxt_re_poll_cq(struct ib_cq *ib_cq, int num_entries, struct ib_wc *wc)\n{\n\tstruct bnxt_re_cq *cq = container_of(ib_cq, struct bnxt_re_cq, ib_cq);\n\tstruct bnxt_re_qp *qp;\n\tstruct bnxt_qplib_cqe *cqe;\n\tint i, ncqe, budget;\n\tstruct bnxt_qplib_q *sq;\n\tstruct bnxt_qplib_qp *lib_qp;\n\tu32 tbl_idx;\n\tstruct bnxt_re_sqp_entries *sqp_entry = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cq->cq_lock, flags);\n\tbudget = min_t(u32, num_entries, cq->max_cql);\n\tnum_entries = budget;\n\tif (!cq->cql) {\n\t\tdev_err(rdev_to_dev(cq->rdev), \"POLL CQ : no CQL to use\");\n\t\tgoto exit;\n\t}\n\tcqe = &cq->cql[0];\n\twhile (budget) {\n\t\tlib_qp = NULL;\n\t\tncqe = bnxt_qplib_poll_cq(&cq->qplib_cq, cqe, budget, &lib_qp);\n\t\tif (lib_qp) {\n\t\t\tsq = &lib_qp->sq;\n\t\t\tif (sq->send_phantom) {\n\t\t\t\tqp = container_of(lib_qp,\n\t\t\t\t\t\t  struct bnxt_re_qp, qplib_qp);\n\t\t\t\tif (send_phantom_wqe(qp) == -ENOMEM)\n\t\t\t\t\tdev_err(rdev_to_dev(cq->rdev),\n\t\t\t\t\t\t\"Phantom failed! Scheduled to send again\\n\");\n\t\t\t\telse\n\t\t\t\t\tsq->send_phantom = false;\n\t\t\t}\n\t\t}\n\t\tif (ncqe < budget)\n\t\t\tncqe += bnxt_qplib_process_flush_list(&cq->qplib_cq,\n\t\t\t\t\t\t\t      cqe + ncqe,\n\t\t\t\t\t\t\t      budget - ncqe);\n\n\t\tif (!ncqe)\n\t\t\tbreak;\n\n\t\tfor (i = 0; i < ncqe; i++, cqe++) {\n\t\t\t/* Transcribe each qplib_wqe back to ib_wc */\n\t\t\tmemset(wc, 0, sizeof(*wc));\n\n\t\t\twc->wr_id = cqe->wr_id;\n\t\t\twc->byte_len = cqe->length;\n\t\t\tqp = container_of\n\t\t\t\t((struct bnxt_qplib_qp *)\n\t\t\t\t (unsigned long)(cqe->qp_handle),\n\t\t\t\t struct bnxt_re_qp, qplib_qp);\n\t\t\tif (!qp) {\n\t\t\t\tdev_err(rdev_to_dev(cq->rdev),\n\t\t\t\t\t\"POLL CQ : bad QP handle\");\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\twc->qp = &qp->ib_qp;\n\t\t\twc->ex.imm_data = cqe->immdata;\n\t\t\twc->src_qp = cqe->src_qp;\n\t\t\tmemcpy(wc->smac, cqe->smac, ETH_ALEN);\n\t\t\twc->port_num = 1;\n\t\t\twc->vendor_err = cqe->status;\n\n\t\t\tswitch (cqe->opcode) {\n\t\t\tcase CQ_BASE_CQE_TYPE_REQ:\n\t\t\t\tif (qp->rdev->qp1_sqp && qp->qplib_qp.id ==\n\t\t\t\t    qp->rdev->qp1_sqp->qplib_qp.id) {\n\t\t\t\t\t/* Handle this completion with\n\t\t\t\t\t * the stored completion\n\t\t\t\t\t */\n\t\t\t\t\tmemset(wc, 0, sizeof(*wc));\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tbnxt_re_process_req_wc(wc, cqe);\n\t\t\t\tbreak;\n\t\t\tcase CQ_BASE_CQE_TYPE_RES_RAWETH_QP1:\n\t\t\t\tif (!cqe->status) {\n\t\t\t\t\tint rc = 0;\n\n\t\t\t\t\trc = bnxt_re_process_raw_qp_pkt_rx\n\t\t\t\t\t\t\t\t(qp, cqe);\n\t\t\t\t\tif (!rc) {\n\t\t\t\t\t\tmemset(wc, 0, sizeof(*wc));\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tcqe->status = -1;\n\t\t\t\t}\n\t\t\t\t/* Errors need not be looped back.\n\t\t\t\t * But change the wr_id to the one\n\t\t\t\t * stored in the table\n\t\t\t\t */\n\t\t\t\ttbl_idx = cqe->wr_id;\n\t\t\t\tsqp_entry = &cq->rdev->sqp_tbl[tbl_idx];\n\t\t\t\twc->wr_id = sqp_entry->wrid;\n\t\t\t\tbnxt_re_process_res_rawqp1_wc(wc, cqe);\n\t\t\t\tbreak;\n\t\t\tcase CQ_BASE_CQE_TYPE_RES_RC:\n\t\t\t\tbnxt_re_process_res_rc_wc(wc, cqe);\n\t\t\t\tbreak;\n\t\t\tcase CQ_BASE_CQE_TYPE_RES_UD:\n\t\t\t\tif (qp->rdev->qp1_sqp && qp->qplib_qp.id ==\n\t\t\t\t    qp->rdev->qp1_sqp->qplib_qp.id) {\n\t\t\t\t\t/* Handle this completion with\n\t\t\t\t\t * the stored completion\n\t\t\t\t\t */\n\t\t\t\t\tif (cqe->status) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tbnxt_re_process_res_shadow_qp_wc\n\t\t\t\t\t\t\t\t(qp, wc, cqe);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbnxt_re_process_res_ud_wc(qp, wc, cqe);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tdev_err(rdev_to_dev(cq->rdev),\n\t\t\t\t\t\"POLL CQ : type 0x%x not handled\",\n\t\t\t\t\tcqe->opcode);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\twc++;\n\t\t\tbudget--;\n\t\t}\n\t}\nexit:\n\tspin_unlock_irqrestore(&cq->cq_lock, flags);\n\treturn num_entries - budget;\n}\n\nint bnxt_re_req_notify_cq(struct ib_cq *ib_cq,\n\t\t\t  enum ib_cq_notify_flags ib_cqn_flags)\n{\n\tstruct bnxt_re_cq *cq = container_of(ib_cq, struct bnxt_re_cq, ib_cq);\n\tint type = 0, rc = 0;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cq->cq_lock, flags);\n\t/* Trigger on the very next completion */\n\tif (ib_cqn_flags & IB_CQ_NEXT_COMP)\n\t\ttype = DBC_DBC_TYPE_CQ_ARMALL;\n\t/* Trigger on the next solicited completion */\n\telse if (ib_cqn_flags & IB_CQ_SOLICITED)\n\t\ttype = DBC_DBC_TYPE_CQ_ARMSE;\n\n\t/* Poll to see if there are missed events */\n\tif ((ib_cqn_flags & IB_CQ_REPORT_MISSED_EVENTS) &&\n\t    !(bnxt_qplib_is_cq_empty(&cq->qplib_cq))) {\n\t\trc = 1;\n\t\tgoto exit;\n\t}\n\tbnxt_qplib_req_notify_cq(&cq->qplib_cq, type);\n\nexit:\n\tspin_unlock_irqrestore(&cq->cq_lock, flags);\n\treturn rc;\n}\n\n/* Memory Regions */\nstruct ib_mr *bnxt_re_get_dma_mr(struct ib_pd *ib_pd, int mr_access_flags)\n{\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct bnxt_re_mr *mr;\n\tu64 pbl = 0;\n\tint rc;\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmr->rdev = rdev;\n\tmr->qplib_mr.pd = &pd->qplib_pd;\n\tmr->qplib_mr.flags = __from_ib_access_flags(mr_access_flags);\n\tmr->qplib_mr.type = CMDQ_ALLOCATE_MRW_MRW_FLAGS_PMR;\n\n\t/* Allocate and register 0 as the address */\n\trc = bnxt_qplib_alloc_mrw(&rdev->qplib_res, &mr->qplib_mr);\n\tif (rc)\n\t\tgoto fail;\n\n\tmr->qplib_mr.hwq.level = PBL_LVL_MAX;\n\tmr->qplib_mr.total_size = -1; /* Infinte length */\n\trc = bnxt_qplib_reg_mr(&rdev->qplib_res, &mr->qplib_mr, &pbl, 0, false,\n\t\t\t       PAGE_SIZE);\n\tif (rc)\n\t\tgoto fail_mr;\n\n\tmr->ib_mr.lkey = mr->qplib_mr.lkey;\n\tif (mr_access_flags & (IB_ACCESS_REMOTE_WRITE | IB_ACCESS_REMOTE_READ |\n\t\t\t       IB_ACCESS_REMOTE_ATOMIC))\n\t\tmr->ib_mr.rkey = mr->ib_mr.lkey;\n\tatomic_inc(&rdev->mr_count);\n\n\treturn &mr->ib_mr;\n\nfail_mr:\n\tbnxt_qplib_free_mrw(&rdev->qplib_res, &mr->qplib_mr);\nfail:\n\tkfree(mr);\n\treturn ERR_PTR(rc);\n}\n\nint bnxt_re_dereg_mr(struct ib_mr *ib_mr, struct ib_udata *udata)\n{\n\tstruct bnxt_re_mr *mr = container_of(ib_mr, struct bnxt_re_mr, ib_mr);\n\tstruct bnxt_re_dev *rdev = mr->rdev;\n\tint rc;\n\n\trc = bnxt_qplib_free_mrw(&rdev->qplib_res, &mr->qplib_mr);\n\tif (rc)\n\t\tdev_err(rdev_to_dev(rdev), \"Dereg MR failed: %#x\\n\", rc);\n\n\tif (mr->pages) {\n\t\trc = bnxt_qplib_free_fast_reg_page_list(&rdev->qplib_res,\n\t\t\t\t\t\t\t&mr->qplib_frpl);\n\t\tkfree(mr->pages);\n\t\tmr->npages = 0;\n\t\tmr->pages = NULL;\n\t}\n\tib_umem_release(mr->ib_umem);\n\n\tkfree(mr);\n\tatomic_dec(&rdev->mr_count);\n\treturn rc;\n}\n\nstatic int bnxt_re_set_page(struct ib_mr *ib_mr, u64 addr)\n{\n\tstruct bnxt_re_mr *mr = container_of(ib_mr, struct bnxt_re_mr, ib_mr);\n\n\tif (unlikely(mr->npages == mr->qplib_frpl.max_pg_ptrs))\n\t\treturn -ENOMEM;\n\n\tmr->pages[mr->npages++] = addr;\n\treturn 0;\n}\n\nint bnxt_re_map_mr_sg(struct ib_mr *ib_mr, struct scatterlist *sg, int sg_nents,\n\t\t      unsigned int *sg_offset)\n{\n\tstruct bnxt_re_mr *mr = container_of(ib_mr, struct bnxt_re_mr, ib_mr);\n\n\tmr->npages = 0;\n\treturn ib_sg_to_pages(ib_mr, sg, sg_nents, sg_offset, bnxt_re_set_page);\n}\n\nstruct ib_mr *bnxt_re_alloc_mr(struct ib_pd *ib_pd, enum ib_mr_type type,\n\t\t\t       u32 max_num_sg, struct ib_udata *udata)\n{\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct bnxt_re_mr *mr = NULL;\n\tint rc;\n\n\tif (type != IB_MR_TYPE_MEM_REG) {\n\t\tdev_dbg(rdev_to_dev(rdev), \"MR type 0x%x not supported\", type);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tif (max_num_sg > MAX_PBL_LVL_1_PGS)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmr->rdev = rdev;\n\tmr->qplib_mr.pd = &pd->qplib_pd;\n\tmr->qplib_mr.flags = BNXT_QPLIB_FR_PMR;\n\tmr->qplib_mr.type = CMDQ_ALLOCATE_MRW_MRW_FLAGS_PMR;\n\n\trc = bnxt_qplib_alloc_mrw(&rdev->qplib_res, &mr->qplib_mr);\n\tif (rc)\n\t\tgoto bail;\n\n\tmr->ib_mr.lkey = mr->qplib_mr.lkey;\n\tmr->ib_mr.rkey = mr->ib_mr.lkey;\n\n\tmr->pages = kcalloc(max_num_sg, sizeof(u64), GFP_KERNEL);\n\tif (!mr->pages) {\n\t\trc = -ENOMEM;\n\t\tgoto fail;\n\t}\n\trc = bnxt_qplib_alloc_fast_reg_page_list(&rdev->qplib_res,\n\t\t\t\t\t\t &mr->qplib_frpl, max_num_sg);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\"Failed to allocate HW FR page list\");\n\t\tgoto fail_mr;\n\t}\n\n\tatomic_inc(&rdev->mr_count);\n\treturn &mr->ib_mr;\n\nfail_mr:\n\tkfree(mr->pages);\nfail:\n\tbnxt_qplib_free_mrw(&rdev->qplib_res, &mr->qplib_mr);\nbail:\n\tkfree(mr);\n\treturn ERR_PTR(rc);\n}\n\nstruct ib_mw *bnxt_re_alloc_mw(struct ib_pd *ib_pd, enum ib_mw_type type,\n\t\t\t       struct ib_udata *udata)\n{\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct bnxt_re_mw *mw;\n\tint rc;\n\n\tmw = kzalloc(sizeof(*mw), GFP_KERNEL);\n\tif (!mw)\n\t\treturn ERR_PTR(-ENOMEM);\n\tmw->rdev = rdev;\n\tmw->qplib_mw.pd = &pd->qplib_pd;\n\n\tmw->qplib_mw.type = (type == IB_MW_TYPE_1 ?\n\t\t\t       CMDQ_ALLOCATE_MRW_MRW_FLAGS_MW_TYPE1 :\n\t\t\t       CMDQ_ALLOCATE_MRW_MRW_FLAGS_MW_TYPE2B);\n\trc = bnxt_qplib_alloc_mrw(&rdev->qplib_res, &mw->qplib_mw);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Allocate MW failed!\");\n\t\tgoto fail;\n\t}\n\tmw->ib_mw.rkey = mw->qplib_mw.rkey;\n\n\tatomic_inc(&rdev->mw_count);\n\treturn &mw->ib_mw;\n\nfail:\n\tkfree(mw);\n\treturn ERR_PTR(rc);\n}\n\nint bnxt_re_dealloc_mw(struct ib_mw *ib_mw)\n{\n\tstruct bnxt_re_mw *mw = container_of(ib_mw, struct bnxt_re_mw, ib_mw);\n\tstruct bnxt_re_dev *rdev = mw->rdev;\n\tint rc;\n\n\trc = bnxt_qplib_free_mrw(&rdev->qplib_res, &mw->qplib_mw);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Free MW failed: %#x\\n\", rc);\n\t\treturn rc;\n\t}\n\n\tkfree(mw);\n\tatomic_dec(&rdev->mw_count);\n\treturn rc;\n}\n\nstatic int bnxt_re_page_size_ok(int page_shift)\n{\n\tswitch (page_shift) {\n\tcase CMDQ_REGISTER_MR_LOG2_PBL_PG_SIZE_PG_4K:\n\tcase CMDQ_REGISTER_MR_LOG2_PBL_PG_SIZE_PG_8K:\n\tcase CMDQ_REGISTER_MR_LOG2_PBL_PG_SIZE_PG_64K:\n\tcase CMDQ_REGISTER_MR_LOG2_PBL_PG_SIZE_PG_2M:\n\tcase CMDQ_REGISTER_MR_LOG2_PBL_PG_SIZE_PG_256K:\n\tcase CMDQ_REGISTER_MR_LOG2_PBL_PG_SIZE_PG_1M:\n\tcase CMDQ_REGISTER_MR_LOG2_PBL_PG_SIZE_PG_4M:\n\tcase CMDQ_REGISTER_MR_LOG2_PBL_PG_SIZE_PG_1G:\n\t\treturn 1;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic int fill_umem_pbl_tbl(struct ib_umem *umem, u64 *pbl_tbl_orig,\n\t\t\t     int page_shift)\n{\n\tu64 *pbl_tbl = pbl_tbl_orig;\n\tu64 page_size =  BIT_ULL(page_shift);\n\tstruct ib_block_iter biter;\n\n\trdma_for_each_block(umem->sg_head.sgl, &biter, umem->nmap, page_size)\n\t\t*pbl_tbl++ = rdma_block_iter_dma_address(&biter);\n\n\treturn pbl_tbl - pbl_tbl_orig;\n}\n\n/* uverbs */\nstruct ib_mr *bnxt_re_reg_user_mr(struct ib_pd *ib_pd, u64 start, u64 length,\n\t\t\t\t  u64 virt_addr, int mr_access_flags,\n\t\t\t\t  struct ib_udata *udata)\n{\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct bnxt_re_mr *mr;\n\tstruct ib_umem *umem;\n\tu64 *pbl_tbl = NULL;\n\tint umem_pgs, page_shift, rc;\n\n\tif (length > BNXT_RE_MAX_MR_SIZE) {\n\t\tdev_err(rdev_to_dev(rdev), \"MR Size: %lld > Max supported:%lld\\n\",\n\t\t\tlength, BNXT_RE_MAX_MR_SIZE);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmr->rdev = rdev;\n\tmr->qplib_mr.pd = &pd->qplib_pd;\n\tmr->qplib_mr.flags = __from_ib_access_flags(mr_access_flags);\n\tmr->qplib_mr.type = CMDQ_ALLOCATE_MRW_MRW_FLAGS_MR;\n\n\trc = bnxt_qplib_alloc_mrw(&rdev->qplib_res, &mr->qplib_mr);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to allocate MR\");\n\t\tgoto free_mr;\n\t}\n\t/* The fixed portion of the rkey is the same as the lkey */\n\tmr->ib_mr.rkey = mr->qplib_mr.rkey;\n\n\tumem = ib_umem_get(udata, start, length, mr_access_flags, 0);\n\tif (IS_ERR(umem)) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to get umem\");\n\t\trc = -EFAULT;\n\t\tgoto free_mrw;\n\t}\n\tmr->ib_umem = umem;\n\n\tmr->qplib_mr.va = virt_addr;\n\tumem_pgs = ib_umem_page_count(umem);\n\tif (!umem_pgs) {\n\t\tdev_err(rdev_to_dev(rdev), \"umem is invalid!\");\n\t\trc = -EINVAL;\n\t\tgoto free_umem;\n\t}\n\tmr->qplib_mr.total_size = length;\n\n\tpbl_tbl = kcalloc(umem_pgs, sizeof(u64 *), GFP_KERNEL);\n\tif (!pbl_tbl) {\n\t\trc = -ENOMEM;\n\t\tgoto free_umem;\n\t}\n\n\tpage_shift = __ffs(ib_umem_find_best_pgsz(umem,\n\t\t\t\tBNXT_RE_PAGE_SIZE_4K | BNXT_RE_PAGE_SIZE_2M,\n\t\t\t\tvirt_addr));\n\n\tif (!bnxt_re_page_size_ok(page_shift)) {\n\t\tdev_err(rdev_to_dev(rdev), \"umem page size unsupported!\");\n\t\trc = -EFAULT;\n\t\tgoto fail;\n\t}\n\n\tif (page_shift == BNXT_RE_PAGE_SHIFT_4K &&\n\t    length > BNXT_RE_MAX_MR_SIZE_LOW) {\n\t\tdev_err(rdev_to_dev(rdev), \"Requested MR Sz:%llu Max sup:%llu\",\n\t\t\tlength,\t(u64)BNXT_RE_MAX_MR_SIZE_LOW);\n\t\trc = -EINVAL;\n\t\tgoto fail;\n\t}\n\n\t/* Map umem buf ptrs to the PBL */\n\tumem_pgs = fill_umem_pbl_tbl(umem, pbl_tbl, page_shift);\n\trc = bnxt_qplib_reg_mr(&rdev->qplib_res, &mr->qplib_mr, pbl_tbl,\n\t\t\t       umem_pgs, false, 1 << page_shift);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to register user MR\");\n\t\tgoto fail;\n\t}\n\n\tkfree(pbl_tbl);\n\n\tmr->ib_mr.lkey = mr->qplib_mr.lkey;\n\tmr->ib_mr.rkey = mr->qplib_mr.lkey;\n\tatomic_inc(&rdev->mr_count);\n\n\treturn &mr->ib_mr;\nfail:\n\tkfree(pbl_tbl);\nfree_umem:\n\tib_umem_release(umem);\nfree_mrw:\n\tbnxt_qplib_free_mrw(&rdev->qplib_res, &mr->qplib_mr);\nfree_mr:\n\tkfree(mr);\n\treturn ERR_PTR(rc);\n}\n\nint bnxt_re_alloc_ucontext(struct ib_ucontext *ctx, struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = ctx->device;\n\tstruct bnxt_re_ucontext *uctx =\n\t\tcontainer_of(ctx, struct bnxt_re_ucontext, ib_uctx);\n\tstruct bnxt_re_dev *rdev = to_bnxt_re_dev(ibdev, ibdev);\n\tstruct bnxt_qplib_dev_attr *dev_attr = &rdev->dev_attr;\n\tstruct bnxt_re_uctx_resp resp;\n\tu32 chip_met_rev_num = 0;\n\tint rc;\n\n\tdev_dbg(rdev_to_dev(rdev), \"ABI version requested %u\",\n\t\tibdev->ops.uverbs_abi_ver);\n\n\tif (ibdev->ops.uverbs_abi_ver != BNXT_RE_ABI_VERSION) {\n\t\tdev_dbg(rdev_to_dev(rdev), \" is different from the device %d \",\n\t\t\tBNXT_RE_ABI_VERSION);\n\t\treturn -EPERM;\n\t}\n\n\tuctx->rdev = rdev;\n\n\tuctx->shpg = (void *)__get_free_page(GFP_KERNEL);\n\tif (!uctx->shpg) {\n\t\trc = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tspin_lock_init(&uctx->sh_lock);\n\n\tresp.comp_mask = BNXT_RE_UCNTX_CMASK_HAVE_CCTX;\n\tchip_met_rev_num = rdev->chip_ctx.chip_num;\n\tchip_met_rev_num |= ((u32)rdev->chip_ctx.chip_rev & 0xFF) <<\n\t\t\t     BNXT_RE_CHIP_ID0_CHIP_REV_SFT;\n\tchip_met_rev_num |= ((u32)rdev->chip_ctx.chip_metal & 0xFF) <<\n\t\t\t     BNXT_RE_CHIP_ID0_CHIP_MET_SFT;\n\tresp.chip_id0 = chip_met_rev_num;\n\t/* Future extension of chip info */\n\tresp.chip_id1 = 0;\n\t/*Temp, Use xa_alloc instead */\n\tresp.dev_id = rdev->en_dev->pdev->devfn;\n\tresp.max_qp = rdev->qplib_ctx.qpc_count;\n\tresp.pg_size = PAGE_SIZE;\n\tresp.cqe_sz = sizeof(struct cq_base);\n\tresp.max_cqd = dev_attr->max_cq_wqes;\n\tresp.rsvd    = 0;\n\n\trc = ib_copy_to_udata(udata, &resp, min(udata->outlen, sizeof(resp)));\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Failed to copy user context\");\n\t\trc = -EFAULT;\n\t\tgoto cfail;\n\t}\n\n\treturn 0;\ncfail:\n\tfree_page((unsigned long)uctx->shpg);\n\tuctx->shpg = NULL;\nfail:\n\treturn rc;\n}\n\nvoid bnxt_re_dealloc_ucontext(struct ib_ucontext *ib_uctx)\n{\n\tstruct bnxt_re_ucontext *uctx = container_of(ib_uctx,\n\t\t\t\t\t\t   struct bnxt_re_ucontext,\n\t\t\t\t\t\t   ib_uctx);\n\n\tstruct bnxt_re_dev *rdev = uctx->rdev;\n\n\tif (uctx->shpg)\n\t\tfree_page((unsigned long)uctx->shpg);\n\n\tif (uctx->dpi.dbr) {\n\t\t/* Free DPI only if this is the first PD allocated by the\n\t\t * application and mark the context dpi as NULL\n\t\t */\n\t\tbnxt_qplib_dealloc_dpi(&rdev->qplib_res,\n\t\t\t\t       &rdev->qplib_res.dpi_tbl, &uctx->dpi);\n\t\tuctx->dpi.dbr = NULL;\n\t}\n}\n\n/* Helper function to mmap the virtual memory from user app */\nint bnxt_re_mmap(struct ib_ucontext *ib_uctx, struct vm_area_struct *vma)\n{\n\tstruct bnxt_re_ucontext *uctx = container_of(ib_uctx,\n\t\t\t\t\t\t   struct bnxt_re_ucontext,\n\t\t\t\t\t\t   ib_uctx);\n\tstruct bnxt_re_dev *rdev = uctx->rdev;\n\tu64 pfn;\n\n\tif (vma->vm_end - vma->vm_start != PAGE_SIZE)\n\t\treturn -EINVAL;\n\n\tif (vma->vm_pgoff) {\n\t\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n\t\tif (io_remap_pfn_range(vma, vma->vm_start, vma->vm_pgoff,\n\t\t\t\t       PAGE_SIZE, vma->vm_page_prot)) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"Failed to map DPI\");\n\t\t\treturn -EAGAIN;\n\t\t}\n\t} else {\n\t\tpfn = virt_to_phys(uctx->shpg) >> PAGE_SHIFT;\n\t\tif (remap_pfn_range(vma, vma->vm_start,\n\t\t\t\t    pfn, PAGE_SIZE, vma->vm_page_prot)) {\n\t\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\t\"Failed to map shared page\");\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\treturn 0;\n}\n"], "filenames": ["drivers/infiniband/hw/bnxt_re/ib_verbs.c"], "buggy_code_start_loc": [1401], "buggy_code_end_loc": [1402], "fixing_code_start_loc": [1401], "fixing_code_end_loc": [1402], "type": "CWE-401", "message": "A memory leak in the bnxt_re_create_srq() function in drivers/infiniband/hw/bnxt_re/ib_verbs.c in the Linux kernel through 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering copy to udata failures, aka CID-4a9d46a9fe14.", "other": {"cve": {"id": "CVE-2019-19077", "sourceIdentifier": "cve@mitre.org", "published": "2019-11-18T06:15:13.497", "lastModified": "2020-08-24T17:37:01.140", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "A memory leak in the bnxt_re_create_srq() function in drivers/infiniband/hw/bnxt_re/ib_verbs.c in the Linux kernel through 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering copy to udata failures, aka CID-4a9d46a9fe14."}, {"lang": "es", "value": "Una p\u00e9rdida de memoria en la funci\u00f3n bnxt_re_create_srq() en el archivo drivers/infiniband/hw/bnxt_re/ib_verbs.c en el kernel de Linux versiones hasta la versi\u00f3n  5.3.11, permite a atacantes causar una denegaci\u00f3n de servicio (consumo de memoria) al desencadenar fallos de la funci\u00f3n de la copia hacia udata, tambi\u00e9n se conoce como CID -4a9d46a9fe14."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-401"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "5.3.11", "matchCriteriaId": "EB2904AC-AD7A-498D-8619-CBB421E9165D"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:lts:*:*:*", "matchCriteriaId": "23A7C53F-B80F-4E6A-AFA9-58EEA84BE11D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:19.10:*:*:*:*:*:*:*", "matchCriteriaId": "A31C8344-3E02-4EB8-8BD8-4C84B7959624"}, {"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:leap:15.1:*:*:*:*:*:*:*", "matchCriteriaId": "B620311B-34A3-48A6-82DF-6F078D7A4493"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2019-12/msg00029.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/4a9d46a9fe14401f21df69cea97c62396d5fb053", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20191205-0001/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4258-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4284-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/4a9d46a9fe14401f21df69cea97c62396d5fb053"}}