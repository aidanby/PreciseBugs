{"buggy_code": ["Changes\n=======\n\n1.25.8 (2020-01-20)\n-------------------\n\n* Drop support for EOL Python 3.4 (Pull #1774)\n\n* Optimize _encode_invalid_chars (Pull #1787)\n\n\n1.25.7 (2019-11-11)\n-------------------\n\n* Preserve ``chunked`` parameter on retries (Pull #1715, Pull #1734)\n\n* Allow unset ``SERVER_SOFTWARE`` in App Engine (Pull #1704, Issue #1470)\n\n* Fix issue where URL fragment was sent within the request target. (Pull #1732)\n\n* Fix issue where an empty query section in a URL would fail to parse. (Pull #1732)\n\n* Remove TLS 1.3 support in SecureTransport due to Apple removing support (Pull #1703)\n\n\n1.25.6 (2019-09-24)\n-------------------\n\n* Fix issue where tilde (``~``) characters were incorrectly\n  percent-encoded in the path. (Pull #1692)\n\n\n1.25.5 (2019-09-19)\n-------------------\n\n* Add mitigation for BPO-37428 affecting Python <3.7.4 and OpenSSL 1.1.1+ which\n  caused certificate verification to be enabled when using ``cert_reqs=CERT_NONE``.\n  (Issue #1682)\n\n\n1.25.4 (2019-09-19)\n-------------------\n\n* Propagate Retry-After header settings to subsequent retries. (Pull #1607)\n\n* Fix edge case where Retry-After header was still respected even when\n  explicitly opted out of. (Pull #1607)\n\n* Remove dependency on ``rfc3986`` for URL parsing.\n\n* Fix issue where URLs containing invalid characters within ``Url.auth`` would\n  raise an exception instead of percent-encoding those characters.\n\n* Add support for ``HTTPResponse.auto_close = False`` which makes HTTP responses\n  work well with BufferedReaders and other ``io`` module features. (Pull #1652)\n\n* Percent-encode invalid characters in URL for ``HTTPConnectionPool.request()`` (Pull #1673)\n\n\n1.25.3 (2019-05-23)\n-------------------\n\n* Change ``HTTPSConnection`` to load system CA certificates\n  when ``ca_certs``, ``ca_cert_dir``, and ``ssl_context`` are\n  unspecified. (Pull #1608, Issue #1603)\n\n* Upgrade bundled rfc3986 to v1.3.2. (Pull #1609, Issue #1605)\n\n\n1.25.2 (2019-04-28)\n-------------------\n\n* Change ``is_ipaddress`` to not detect IPvFuture addresses. (Pull #1583)\n\n* Change ``parse_url`` to percent-encode invalid characters within the\n  path, query, and target components. (Pull #1586)\n\n\n1.25.1 (2019-04-24)\n-------------------\n\n* Add support for Google's ``Brotli`` package. (Pull #1572, Pull #1579)\n\n* Upgrade bundled rfc3986 to v1.3.1 (Pull #1578)\n\n\n1.25 (2019-04-22)\n-----------------\n\n* Require and validate certificates by default when using HTTPS (Pull #1507)\n\n* Upgraded ``urllib3.utils.parse_url()`` to be RFC 3986 compliant. (Pull #1487)\n\n* Added support for ``key_password`` for ``HTTPSConnectionPool`` to use\n  encrypted ``key_file`` without creating your own ``SSLContext`` object. (Pull #1489)\n\n* Add TLSv1.3 support to CPython, pyOpenSSL, and SecureTransport ``SSLContext``\n  implementations. (Pull #1496)\n\n* Switched the default multipart header encoder from RFC 2231 to HTML 5 working draft. (Issue #303, PR #1492)\n\n* Fixed issue where OpenSSL would block if an encrypted client private key was\n  given and no password was given. Instead an ``SSLError`` is raised. (Pull #1489)\n\n* Added support for Brotli content encoding. It is enabled automatically if\n  ``brotlipy`` package is installed which can be requested with\n  ``urllib3[brotli]`` extra. (Pull #1532)\n\n* Drop ciphers using DSS key exchange from default TLS cipher suites.\n  Improve default ciphers when using SecureTransport. (Pull #1496)\n\n* Implemented a more efficient ``HTTPResponse.__iter__()`` method. (Issue #1483)\n\n1.24.3 (2019-05-01)\n-------------------\n\n* Apply fix for CVE-2019-9740. (Pull #1591)\n\n1.24.2 (2019-04-17)\n-------------------\n\n* Don't load system certificates by default when any other ``ca_certs``, ``ca_certs_dir`` or\n  ``ssl_context`` parameters are specified.\n\n* Remove Authorization header regardless of case when redirecting to cross-site. (Issue #1510)\n\n* Add support for IPv6 addresses in subjectAltName section of certificates. (Issue #1269)\n\n\n1.24.1 (2018-11-02)\n-------------------\n\n* Remove quadratic behavior within ``GzipDecoder.decompress()`` (Issue #1467)\n\n* Restored functionality of ``ciphers`` parameter for ``create_urllib3_context()``. (Issue #1462)\n\n\n1.24 (2018-10-16)\n-----------------\n\n* Allow key_server_hostname to be specified when initializing a PoolManager to allow custom SNI to be overridden. (Pull #1449)\n\n* Test against Python 3.7 on AppVeyor. (Pull #1453)\n\n* Early-out ipv6 checks when running on App Engine. (Pull #1450)\n\n* Change ambiguous description of backoff_factor (Pull #1436)\n\n* Add ability to handle multiple Content-Encodings (Issue #1441 and Pull #1442)\n\n* Skip DNS names that can't be idna-decoded when using pyOpenSSL (Issue #1405).\n\n* Add a server_hostname parameter to HTTPSConnection which allows for\n  overriding the SNI hostname sent in the handshake. (Pull #1397)\n\n* Drop support for EOL Python 2.6 (Pull #1429 and Pull #1430)\n\n* Fixed bug where responses with header Content-Type: message/* erroneously\n  raised HeaderParsingError, resulting in a warning being logged. (Pull #1439)\n\n* Move urllib3 to src/urllib3 (Pull #1409)\n\n\n1.23 (2018-06-04)\n-----------------\n\n* Allow providing a list of headers to strip from requests when redirecting\n  to a different host. Defaults to the ``Authorization`` header. Different\n  headers can be set via ``Retry.remove_headers_on_redirect``. (Issue #1316)\n\n* Fix ``util.selectors._fileobj_to_fd`` to accept ``long`` (Issue #1247).\n\n* Dropped Python 3.3 support. (Pull #1242)\n\n* Put the connection back in the pool when calling stream() or read_chunked() on\n  a chunked HEAD response. (Issue #1234)\n\n* Fixed pyOpenSSL-specific ssl client authentication issue when clients\n  attempted to auth via certificate + chain (Issue #1060)\n\n* Add the port to the connectionpool connect print (Pull #1251)\n\n* Don't use the ``uuid`` module to create multipart data boundaries. (Pull #1380)\n\n* ``read_chunked()`` on a closed response returns no chunks. (Issue #1088)\n\n* Add Python 2.6 support to ``contrib.securetransport`` (Pull #1359)\n\n* Added support for auth info in url for SOCKS proxy (Pull #1363)\n\n\n1.22 (2017-07-20)\n-----------------\n\n* Fixed missing brackets in ``HTTP CONNECT`` when connecting to IPv6 address via\n  IPv6 proxy. (Issue #1222)\n\n* Made the connection pool retry on ``SSLError``.  The original ``SSLError``\n  is available on ``MaxRetryError.reason``. (Issue #1112)\n\n* Drain and release connection before recursing on retry/redirect.  Fixes\n  deadlocks with a blocking connectionpool. (Issue #1167)\n\n* Fixed compatibility for cookiejar. (Issue #1229)\n\n* pyopenssl: Use vendored version of ``six``. (Issue #1231)\n\n\n1.21.1 (2017-05-02)\n-------------------\n\n* Fixed SecureTransport issue that would cause long delays in response body\n  delivery. (Pull #1154)\n\n* Fixed regression in 1.21 that threw exceptions when users passed the\n  ``socket_options`` flag to the ``PoolManager``.  (Issue #1165)\n\n* Fixed regression in 1.21 that threw exceptions when users passed the\n  ``assert_hostname`` or ``assert_fingerprint`` flag to the ``PoolManager``.\n  (Pull #1157)\n\n\n1.21 (2017-04-25)\n-----------------\n\n* Improved performance of certain selector system calls on Python 3.5 and\n  later. (Pull #1095)\n\n* Resolved issue where the PyOpenSSL backend would not wrap SysCallError\n  exceptions appropriately when sending data. (Pull #1125)\n\n* Selectors now detects a monkey-patched select module after import for modules\n  that patch the select module like eventlet, greenlet. (Pull #1128)\n\n* Reduced memory consumption when streaming zlib-compressed responses\n  (as opposed to raw deflate streams). (Pull #1129)\n\n* Connection pools now use the entire request context when constructing the\n  pool key. (Pull #1016)\n\n* ``PoolManager.connection_from_*`` methods now accept a new keyword argument,\n  ``pool_kwargs``, which are merged with the existing ``connection_pool_kw``.\n  (Pull #1016)\n\n* Add retry counter for ``status_forcelist``. (Issue #1147)\n\n* Added ``contrib`` module for using SecureTransport on macOS:\n  ``urllib3.contrib.securetransport``.  (Pull #1122)\n\n* urllib3 now only normalizes the case of ``http://`` and ``https://`` schemes:\n  for schemes it does not recognise, it assumes they are case-sensitive and\n  leaves them unchanged.\n  (Issue #1080)\n\n\n1.20 (2017-01-19)\n-----------------\n\n* Added support for waiting for I/O using selectors other than select,\n  improving urllib3's behaviour with large numbers of concurrent connections.\n  (Pull #1001)\n\n* Updated the date for the system clock check. (Issue #1005)\n\n* ConnectionPools now correctly consider hostnames to be case-insensitive.\n  (Issue #1032)\n\n* Outdated versions of PyOpenSSL now cause the PyOpenSSL contrib module\n  to fail when it is injected, rather than at first use. (Pull #1063)\n\n* Outdated versions of cryptography now cause the PyOpenSSL contrib module\n  to fail when it is injected, rather than at first use. (Issue #1044)\n\n* Automatically attempt to rewind a file-like body object when a request is\n  retried or redirected. (Pull #1039)\n\n* Fix some bugs that occur when modules incautiously patch the queue module.\n  (Pull #1061)\n\n* Prevent retries from occurring on read timeouts for which the request method\n  was not in the method whitelist. (Issue #1059)\n\n* Changed the PyOpenSSL contrib module to lazily load idna to avoid\n  unnecessarily bloating the memory of programs that don't need it. (Pull\n  #1076)\n\n* Add support for IPv6 literals with zone identifiers. (Pull #1013)\n\n* Added support for socks5h:// and socks4a:// schemes when working with SOCKS\n  proxies, and controlled remote DNS appropriately. (Issue #1035)\n\n\n1.19.1 (2016-11-16)\n-------------------\n\n* Fixed AppEngine import that didn't function on Python 3.5. (Pull #1025)\n\n\n1.19 (2016-11-03)\n-----------------\n\n* urllib3 now respects Retry-After headers on 413, 429, and 503 responses when\n  using the default retry logic. (Pull #955)\n\n* Remove markers from setup.py to assist ancient setuptools versions. (Issue\n  #986)\n\n* Disallow superscripts and other integerish things in URL ports. (Issue #989)\n\n* Allow urllib3's HTTPResponse.stream() method to continue to work with\n  non-httplib underlying FPs. (Pull #990)\n\n* Empty filenames in multipart headers are now emitted as such, rather than\n  being suppressed. (Issue #1015)\n\n* Prefer user-supplied Host headers on chunked uploads. (Issue #1009)\n\n\n1.18.1 (2016-10-27)\n-------------------\n\n* CVE-2016-9015. Users who are using urllib3 version 1.17 or 1.18 along with\n  PyOpenSSL injection and OpenSSL 1.1.0 *must* upgrade to this version. This\n  release fixes a vulnerability whereby urllib3 in the above configuration\n  would silently fail to validate TLS certificates due to erroneously setting\n  invalid flags in OpenSSL's ``SSL_CTX_set_verify`` function. These erroneous\n  flags do not cause a problem in OpenSSL versions before 1.1.0, which\n  interprets the presence of any flag as requesting certificate validation.\n\n  There is no PR for this patch, as it was prepared for simultaneous disclosure\n  and release. The master branch received the same fix in PR #1010.\n\n\n1.18 (2016-09-26)\n-----------------\n\n* Fixed incorrect message for IncompleteRead exception. (PR #973)\n\n* Accept ``iPAddress`` subject alternative name fields in TLS certificates.\n  (Issue #258)\n\n* Fixed consistency of ``HTTPResponse.closed`` between Python 2 and 3.\n  (Issue #977)\n\n* Fixed handling of wildcard certificates when using PyOpenSSL. (Issue #979)\n\n\n1.17 (2016-09-06)\n-----------------\n\n* Accept ``SSLContext`` objects for use in SSL/TLS negotiation. (Issue #835)\n\n* ConnectionPool debug log now includes scheme, host, and port. (Issue #897)\n\n* Substantially refactored documentation. (Issue #887)\n\n* Used URLFetch default timeout on AppEngine, rather than hardcoding our own.\n  (Issue #858)\n\n* Normalize the scheme and host in the URL parser (Issue #833)\n\n* ``HTTPResponse`` contains the last ``Retry`` object, which now also\n  contains retries history. (Issue #848)\n\n* Timeout can no longer be set as boolean, and must be greater than zero.\n  (PR #924)\n\n* Removed pyasn1 and ndg-httpsclient from dependencies used for PyOpenSSL. We\n  now use cryptography and idna, both of which are already dependencies of\n  PyOpenSSL. (PR #930)\n\n* Fixed infinite loop in ``stream`` when amt=None. (Issue #928)\n\n* Try to use the operating system's certificates when we are using an\n  ``SSLContext``. (PR #941)\n\n* Updated cipher suite list to allow ChaCha20+Poly1305. AES-GCM is preferred to\n  ChaCha20, but ChaCha20 is then preferred to everything else. (PR #947)\n\n* Updated cipher suite list to remove 3DES-based cipher suites. (PR #958)\n\n* Removed the cipher suite fallback to allow HIGH ciphers. (PR #958)\n\n* Implemented ``length_remaining`` to determine remaining content\n  to be read. (PR #949)\n\n* Implemented ``enforce_content_length`` to enable exceptions when\n  incomplete data chunks are received. (PR #949)\n\n* Dropped connection start, dropped connection reset, redirect, forced retry,\n  and new HTTPS connection log levels to DEBUG, from INFO. (PR #967)\n\n\n1.16 (2016-06-11)\n-----------------\n\n* Disable IPv6 DNS when IPv6 connections are not possible. (Issue #840)\n\n* Provide ``key_fn_by_scheme`` pool keying mechanism that can be\n  overridden. (Issue #830)\n\n* Normalize scheme and host to lowercase for pool keys, and include\n  ``source_address``. (Issue #830)\n\n* Cleaner exception chain in Python 3 for ``_make_request``.\n  (Issue #861)\n\n* Fixed installing ``urllib3[socks]`` extra. (Issue #864)\n\n* Fixed signature of ``ConnectionPool.close`` so it can actually safely be\n  called by subclasses. (Issue #873)\n\n* Retain ``release_conn`` state across retries. (Issues #651, #866)\n\n* Add customizable ``HTTPConnectionPool.ResponseCls``, which defaults to\n  ``HTTPResponse`` but can be replaced with a subclass. (Issue #879)\n\n\n1.15.1 (2016-04-11)\n-------------------\n\n* Fix packaging to include backports module. (Issue #841)\n\n\n1.15 (2016-04-06)\n-----------------\n\n* Added Retry(raise_on_status=False). (Issue #720)\n\n* Always use setuptools, no more distutils fallback. (Issue #785)\n\n* Dropped support for Python 3.2. (Issue #786)\n\n* Chunked transfer encoding when requesting with ``chunked=True``.\n  (Issue #790)\n\n* Fixed regression with IPv6 port parsing. (Issue #801)\n\n* Append SNIMissingWarning messages to allow users to specify it in\n  the PYTHONWARNINGS environment variable. (Issue #816)\n\n* Handle unicode headers in Py2. (Issue #818)\n\n* Log certificate when there is a hostname mismatch. (Issue #820)\n\n* Preserve order of request/response headers. (Issue #821)\n\n\n1.14 (2015-12-29)\n-----------------\n\n* contrib: SOCKS proxy support! (Issue #762)\n\n* Fixed AppEngine handling of transfer-encoding header and bug\n  in Timeout defaults checking. (Issue #763)\n\n\n1.13.1 (2015-12-18)\n-------------------\n\n* Fixed regression in IPv6 + SSL for match_hostname. (Issue #761)\n\n\n1.13 (2015-12-14)\n-----------------\n\n* Fixed ``pip install urllib3[secure]`` on modern pip. (Issue #706)\n\n* pyopenssl: Fixed SSL3_WRITE_PENDING error. (Issue #717)\n\n* pyopenssl: Support for TLSv1.1 and TLSv1.2. (Issue #696)\n\n* Close connections more defensively on exception. (Issue #734)\n\n* Adjusted ``read_chunked`` to handle gzipped, chunk-encoded bodies without\n  repeatedly flushing the decoder, to function better on Jython. (Issue #743)\n\n* Accept ``ca_cert_dir`` for SSL-related PoolManager configuration. (Issue #758)\n\n\n1.12 (2015-09-03)\n-----------------\n\n* Rely on ``six`` for importing ``httplib`` to work around\n  conflicts with other Python 3 shims. (Issue #688)\n\n* Add support for directories of certificate authorities, as supported by\n  OpenSSL. (Issue #701)\n\n* New exception: ``NewConnectionError``, raised when we fail to establish\n  a new connection, usually ``ECONNREFUSED`` socket error.\n\n\n1.11 (2015-07-21)\n-----------------\n\n* When ``ca_certs`` is given, ``cert_reqs`` defaults to\n  ``'CERT_REQUIRED'``. (Issue #650)\n\n* ``pip install urllib3[secure]`` will install Certifi and\n  PyOpenSSL as dependencies. (Issue #678)\n\n* Made ``HTTPHeaderDict`` usable as a ``headers`` input value\n  (Issues #632, #679)\n\n* Added `urllib3.contrib.appengine <https://urllib3.readthedocs.io/en/latest/contrib.html#google-app-engine>`_\n  which has an ``AppEngineManager`` for using ``URLFetch`` in a\n  Google AppEngine environment. (Issue #664)\n\n* Dev: Added test suite for AppEngine. (Issue #631)\n\n* Fix performance regression when using PyOpenSSL. (Issue #626)\n\n* Passing incorrect scheme (e.g. ``foo://``) will raise\n  ``ValueError`` instead of ``AssertionError`` (backwards\n  compatible for now, but please migrate). (Issue #640)\n\n* Fix pools not getting replenished when an error occurs during a\n  request using ``release_conn=False``. (Issue #644)\n\n* Fix pool-default headers not applying for url-encoded requests\n  like GET. (Issue #657)\n\n* log.warning in Python 3 when headers are skipped due to parsing\n  errors. (Issue #642)\n\n* Close and discard connections if an error occurs during read.\n  (Issue #660)\n\n* Fix host parsing for IPv6 proxies. (Issue #668)\n\n* Separate warning type SubjectAltNameWarning, now issued once\n  per host. (Issue #671)\n\n* Fix ``httplib.IncompleteRead`` not getting converted to\n  ``ProtocolError`` when using ``HTTPResponse.stream()``\n  (Issue #674)\n\n1.10.4 (2015-05-03)\n-------------------\n\n* Migrate tests to Tornado 4. (Issue #594)\n\n* Append default warning configuration rather than overwrite.\n  (Issue #603)\n\n* Fix streaming decoding regression. (Issue #595)\n\n* Fix chunked requests losing state across keep-alive connections.\n  (Issue #599)\n\n* Fix hanging when chunked HEAD response has no body. (Issue #605)\n\n\n1.10.3 (2015-04-21)\n-------------------\n\n* Emit ``InsecurePlatformWarning`` when SSLContext object is missing.\n  (Issue #558)\n\n* Fix regression of duplicate header keys being discarded.\n  (Issue #563)\n\n* ``Response.stream()`` returns a generator for chunked responses.\n  (Issue #560)\n\n* Set upper-bound timeout when waiting for a socket in PyOpenSSL.\n  (Issue #585)\n\n* Work on platforms without `ssl` module for plain HTTP requests.\n  (Issue #587)\n\n* Stop relying on the stdlib's default cipher list. (Issue #588)\n\n\n1.10.2 (2015-02-25)\n-------------------\n\n* Fix file descriptor leakage on retries. (Issue #548)\n\n* Removed RC4 from default cipher list. (Issue #551)\n\n* Header performance improvements. (Issue #544)\n\n* Fix PoolManager not obeying redirect retry settings. (Issue #553)\n\n\n1.10.1 (2015-02-10)\n-------------------\n\n* Pools can be used as context managers. (Issue #545)\n\n* Don't re-use connections which experienced an SSLError. (Issue #529)\n\n* Don't fail when gzip decoding an empty stream. (Issue #535)\n\n* Add sha256 support for fingerprint verification. (Issue #540)\n\n* Fixed handling of header values containing commas. (Issue #533)\n\n\n1.10 (2014-12-14)\n-----------------\n\n* Disabled SSLv3. (Issue #473)\n\n* Add ``Url.url`` property to return the composed url string. (Issue #394)\n\n* Fixed PyOpenSSL + gevent ``WantWriteError``. (Issue #412)\n\n* ``MaxRetryError.reason`` will always be an exception, not string.\n  (Issue #481)\n\n* Fixed SSL-related timeouts not being detected as timeouts. (Issue #492)\n\n* Py3: Use ``ssl.create_default_context()`` when available. (Issue #473)\n\n* Emit ``InsecureRequestWarning`` for *every* insecure HTTPS request.\n  (Issue #496)\n\n* Emit ``SecurityWarning`` when certificate has no ``subjectAltName``.\n  (Issue #499)\n\n* Close and discard sockets which experienced SSL-related errors.\n  (Issue #501)\n\n* Handle ``body`` param in ``.request(...)``. (Issue #513)\n\n* Respect timeout with HTTPS proxy. (Issue #505)\n\n* PyOpenSSL: Handle ZeroReturnError exception. (Issue #520)\n\n\n1.9.1 (2014-09-13)\n------------------\n\n* Apply socket arguments before binding. (Issue #427)\n\n* More careful checks if fp-like object is closed. (Issue #435)\n\n* Fixed packaging issues of some development-related files not\n  getting included. (Issue #440)\n\n* Allow performing *only* fingerprint verification. (Issue #444)\n\n* Emit ``SecurityWarning`` if system clock is waaay off. (Issue #445)\n\n* Fixed PyOpenSSL compatibility with PyPy. (Issue #450)\n\n* Fixed ``BrokenPipeError`` and ``ConnectionError`` handling in Py3.\n  (Issue #443)\n\n\n\n1.9 (2014-07-04)\n----------------\n\n* Shuffled around development-related files. If you're maintaining a distro\n  package of urllib3, you may need to tweak things. (Issue #415)\n\n* Unverified HTTPS requests will trigger a warning on the first request. See\n  our new `security documentation\n  <https://urllib3.readthedocs.io/en/latest/security.html>`_ for details.\n  (Issue #426)\n\n* New retry logic and ``urllib3.util.retry.Retry`` configuration object.\n  (Issue #326)\n\n* All raised exceptions should now wrapped in a\n  ``urllib3.exceptions.HTTPException``-extending exception. (Issue #326)\n\n* All errors during a retry-enabled request should be wrapped in\n  ``urllib3.exceptions.MaxRetryError``, including timeout-related exceptions\n  which were previously exempt. Underlying error is accessible from the\n  ``.reason`` property. (Issue #326)\n\n* ``urllib3.exceptions.ConnectionError`` renamed to\n  ``urllib3.exceptions.ProtocolError``. (Issue #326)\n\n* Errors during response read (such as IncompleteRead) are now wrapped in\n  ``urllib3.exceptions.ProtocolError``. (Issue #418)\n\n* Requesting an empty host will raise ``urllib3.exceptions.LocationValueError``.\n  (Issue #417)\n\n* Catch read timeouts over SSL connections as\n  ``urllib3.exceptions.ReadTimeoutError``. (Issue #419)\n\n* Apply socket arguments before connecting. (Issue #427)\n\n\n1.8.3 (2014-06-23)\n------------------\n\n* Fix TLS verification when using a proxy in Python 3.4.1. (Issue #385)\n\n* Add ``disable_cache`` option to ``urllib3.util.make_headers``. (Issue #393)\n\n* Wrap ``socket.timeout`` exception with\n  ``urllib3.exceptions.ReadTimeoutError``. (Issue #399)\n\n* Fixed proxy-related bug where connections were being reused incorrectly.\n  (Issues #366, #369)\n\n* Added ``socket_options`` keyword parameter which allows to define\n  ``setsockopt`` configuration of new sockets. (Issue #397)\n\n* Removed ``HTTPConnection.tcp_nodelay`` in favor of\n  ``HTTPConnection.default_socket_options``. (Issue #397)\n\n* Fixed ``TypeError`` bug in Python 2.6.4. (Issue #411)\n\n\n1.8.2 (2014-04-17)\n------------------\n\n* Fix ``urllib3.util`` not being included in the package.\n\n\n1.8.1 (2014-04-17)\n------------------\n\n* Fix AppEngine bug of HTTPS requests going out as HTTP. (Issue #356)\n\n* Don't install ``dummyserver`` into ``site-packages`` as it's only needed\n  for the test suite. (Issue #362)\n\n* Added support for specifying ``source_address``. (Issue #352)\n\n\n1.8 (2014-03-04)\n----------------\n\n* Improved url parsing in ``urllib3.util.parse_url`` (properly parse '@' in\n  username, and blank ports like 'hostname:').\n\n* New ``urllib3.connection`` module which contains all the HTTPConnection\n  objects.\n\n* Several ``urllib3.util.Timeout``-related fixes. Also changed constructor\n  signature to a more sensible order. [Backwards incompatible]\n  (Issues #252, #262, #263)\n\n* Use ``backports.ssl_match_hostname`` if it's installed. (Issue #274)\n\n* Added ``.tell()`` method to ``urllib3.response.HTTPResponse`` which\n  returns the number of bytes read so far. (Issue #277)\n\n* Support for platforms without threading. (Issue #289)\n\n* Expand default-port comparison in ``HTTPConnectionPool.is_same_host``\n  to allow a pool with no specified port to be considered equal to to an\n  HTTP/HTTPS url with port 80/443 explicitly provided. (Issue #305)\n\n* Improved default SSL/TLS settings to avoid vulnerabilities.\n  (Issue #309)\n\n* Fixed ``urllib3.poolmanager.ProxyManager`` not retrying on connect errors.\n  (Issue #310)\n\n* Disable Nagle's Algorithm on the socket for non-proxies. A subset of requests\n  will send the entire HTTP request ~200 milliseconds faster; however, some of\n  the resulting TCP packets will be smaller. (Issue #254)\n\n* Increased maximum number of SubjectAltNames in ``urllib3.contrib.pyopenssl``\n  from the default 64 to 1024 in a single certificate. (Issue #318)\n\n* Headers are now passed and stored as a custom\n  ``urllib3.collections_.HTTPHeaderDict`` object rather than a plain ``dict``.\n  (Issue #329, #333)\n\n* Headers no longer lose their case on Python 3. (Issue #236)\n\n* ``urllib3.contrib.pyopenssl`` now uses the operating system's default CA\n  certificates on inject. (Issue #332)\n\n* Requests with ``retries=False`` will immediately raise any exceptions without\n  wrapping them in ``MaxRetryError``. (Issue #348)\n\n* Fixed open socket leak with SSL-related failures. (Issue #344, #348)\n\n\n1.7.1 (2013-09-25)\n------------------\n\n* Added granular timeout support with new ``urllib3.util.Timeout`` class.\n  (Issue #231)\n\n* Fixed Python 3.4 support. (Issue #238)\n\n\n1.7 (2013-08-14)\n----------------\n\n* More exceptions are now pickle-able, with tests. (Issue #174)\n\n* Fixed redirecting with relative URLs in Location header. (Issue #178)\n\n* Support for relative urls in ``Location: ...`` header. (Issue #179)\n\n* ``urllib3.response.HTTPResponse`` now inherits from ``io.IOBase`` for bonus\n  file-like functionality. (Issue #187)\n\n* Passing ``assert_hostname=False`` when creating a HTTPSConnectionPool will\n  skip hostname verification for SSL connections. (Issue #194)\n\n* New method ``urllib3.response.HTTPResponse.stream(...)`` which acts as a\n  generator wrapped around ``.read(...)``. (Issue #198)\n\n* IPv6 url parsing enforces brackets around the hostname. (Issue #199)\n\n* Fixed thread race condition in\n  ``urllib3.poolmanager.PoolManager.connection_from_host(...)`` (Issue #204)\n\n* ``ProxyManager`` requests now include non-default port in ``Host: ...``\n  header. (Issue #217)\n\n* Added HTTPS proxy support in ``ProxyManager``. (Issue #170 #139)\n\n* New ``RequestField`` object can be passed to the ``fields=...`` param which\n  can specify headers. (Issue #220)\n\n* Raise ``urllib3.exceptions.ProxyError`` when connecting to proxy fails.\n  (Issue #221)\n\n* Use international headers when posting file names. (Issue #119)\n\n* Improved IPv6 support. (Issue #203)\n\n\n1.6 (2013-04-25)\n----------------\n\n* Contrib: Optional SNI support for Py2 using PyOpenSSL. (Issue #156)\n\n* ``ProxyManager`` automatically adds ``Host: ...`` header if not given.\n\n* Improved SSL-related code. ``cert_req`` now optionally takes a string like\n  \"REQUIRED\" or \"NONE\". Same with ``ssl_version`` takes strings like \"SSLv23\"\n  The string values reflect the suffix of the respective constant variable.\n  (Issue #130)\n\n* Vendored ``socksipy`` now based on Anorov's fork which handles unexpectedly\n  closed proxy connections and larger read buffers. (Issue #135)\n\n* Ensure the connection is closed if no data is received, fixes connection leak\n  on some platforms. (Issue #133)\n\n* Added SNI support for SSL/TLS connections on Py32+. (Issue #89)\n\n* Tests fixed to be compatible with Py26 again. (Issue #125)\n\n* Added ability to choose SSL version by passing an ``ssl.PROTOCOL_*`` constant\n  to the ``ssl_version`` parameter of ``HTTPSConnectionPool``. (Issue #109)\n\n* Allow an explicit content type to be specified when encoding file fields.\n  (Issue #126)\n\n* Exceptions are now pickleable, with tests. (Issue #101)\n\n* Fixed default headers not getting passed in some cases. (Issue #99)\n\n* Treat \"content-encoding\" header value as case-insensitive, per RFC 2616\n  Section 3.5. (Issue #110)\n\n* \"Connection Refused\" SocketErrors will get retried rather than raised.\n  (Issue #92)\n\n* Updated vendored ``six``, no longer overrides the global ``six`` module\n  namespace. (Issue #113)\n\n* ``urllib3.exceptions.MaxRetryError`` contains a ``reason`` property holding\n  the exception that prompted the final retry. If ``reason is None`` then it\n  was due to a redirect. (Issue #92, #114)\n\n* Fixed ``PoolManager.urlopen()`` from not redirecting more than once.\n  (Issue #149)\n\n* Don't assume ``Content-Type: text/plain`` for multi-part encoding parameters\n  that are not files. (Issue #111)\n\n* Pass `strict` param down to ``httplib.HTTPConnection``. (Issue #122)\n\n* Added mechanism to verify SSL certificates by fingerprint (md5, sha1) or\n  against an arbitrary hostname (when connecting by IP or for misconfigured\n  servers). (Issue #140)\n\n* Streaming decompression support. (Issue #159)\n\n\n1.5 (2012-08-02)\n----------------\n\n* Added ``urllib3.add_stderr_logger()`` for quickly enabling STDERR debug\n  logging in urllib3.\n\n* Native full URL parsing (including auth, path, query, fragment) available in\n  ``urllib3.util.parse_url(url)``.\n\n* Built-in redirect will switch method to 'GET' if status code is 303.\n  (Issue #11)\n\n* ``urllib3.PoolManager`` strips the scheme and host before sending the request\n  uri. (Issue #8)\n\n* New ``urllib3.exceptions.DecodeError`` exception for when automatic decoding,\n  based on the Content-Type header, fails.\n\n* Fixed bug with pool depletion and leaking connections (Issue #76). Added\n  explicit connection closing on pool eviction. Added\n  ``urllib3.PoolManager.clear()``.\n\n* 99% -> 100% unit test coverage.\n\n\n1.4 (2012-06-16)\n----------------\n\n* Minor AppEngine-related fixes.\n\n* Switched from ``mimetools.choose_boundary`` to ``uuid.uuid4()``.\n\n* Improved url parsing. (Issue #73)\n\n* IPv6 url support. (Issue #72)\n\n\n1.3 (2012-03-25)\n----------------\n\n* Removed pre-1.0 deprecated API.\n\n* Refactored helpers into a ``urllib3.util`` submodule.\n\n* Fixed multipart encoding to support list-of-tuples for keys with multiple\n  values. (Issue #48)\n\n* Fixed multiple Set-Cookie headers in response not getting merged properly in\n  Python 3. (Issue #53)\n\n* AppEngine support with Py27. (Issue #61)\n\n* Minor ``encode_multipart_formdata`` fixes related to Python 3 strings vs\n  bytes.\n\n\n1.2.2 (2012-02-06)\n------------------\n\n* Fixed packaging bug of not shipping ``test-requirements.txt``. (Issue #47)\n\n\n1.2.1 (2012-02-05)\n------------------\n\n* Fixed another bug related to when ``ssl`` module is not available. (Issue #41)\n\n* Location parsing errors now raise ``urllib3.exceptions.LocationParseError``\n  which inherits from ``ValueError``.\n\n\n1.2 (2012-01-29)\n----------------\n\n* Added Python 3 support (tested on 3.2.2)\n\n* Dropped Python 2.5 support (tested on 2.6.7, 2.7.2)\n\n* Use ``select.poll`` instead of ``select.select`` for platforms that support\n  it.\n\n* Use ``Queue.LifoQueue`` instead of ``Queue.Queue`` for more aggressive\n  connection reusing. Configurable by overriding ``ConnectionPool.QueueCls``.\n\n* Fixed ``ImportError`` during install when ``ssl`` module is not available.\n  (Issue #41)\n\n* Fixed ``PoolManager`` redirects between schemes (such as HTTP -> HTTPS) not\n  completing properly. (Issue #28, uncovered by Issue #10 in v1.1)\n\n* Ported ``dummyserver`` to use ``tornado`` instead of ``webob`` +\n  ``eventlet``. Removed extraneous unsupported dummyserver testing backends.\n  Added socket-level tests.\n\n* More tests. Achievement Unlocked: 99% Coverage.\n\n\n1.1 (2012-01-07)\n----------------\n\n* Refactored ``dummyserver`` to its own root namespace module (used for\n  testing).\n\n* Added hostname verification for ``VerifiedHTTPSConnection`` by vendoring in\n  Py32's ``ssl_match_hostname``. (Issue #25)\n\n* Fixed cross-host HTTP redirects when using ``PoolManager``. (Issue #10)\n\n* Fixed ``decode_content`` being ignored when set through ``urlopen``. (Issue\n  #27)\n\n* Fixed timeout-related bugs. (Issues #17, #23)\n\n\n1.0.2 (2011-11-04)\n------------------\n\n* Fixed typo in ``VerifiedHTTPSConnection`` which would only present as a bug if\n  you're using the object manually. (Thanks pyos)\n\n* Made RecentlyUsedContainer (and consequently PoolManager) more thread-safe by\n  wrapping the access log in a mutex. (Thanks @christer)\n\n* Made RecentlyUsedContainer more dict-like (corrected ``__delitem__`` and\n  ``__getitem__`` behaviour), with tests. Shouldn't affect core urllib3 code.\n\n\n1.0.1 (2011-10-10)\n------------------\n\n* Fixed a bug where the same connection would get returned into the pool twice,\n  causing extraneous \"HttpConnectionPool is full\" log warnings.\n\n\n1.0 (2011-10-08)\n----------------\n\n* Added ``PoolManager`` with LRU expiration of connections (tested and\n  documented).\n* Added ``ProxyManager`` (needs tests, docs, and confirmation that it works\n  with HTTPS proxies).\n* Added optional partial-read support for responses when\n  ``preload_content=False``. You can now make requests and just read the headers\n  without loading the content.\n* Made response decoding optional (default on, same as before).\n* Added optional explicit boundary string for ``encode_multipart_formdata``.\n* Convenience request methods are now inherited from ``RequestMethods``. Old\n  helpers like ``get_url`` and ``post_url`` should be abandoned in favour of\n  the new ``request(method, url, ...)``.\n* Refactored code to be even more decoupled, reusable, and extendable.\n* License header added to ``.py`` files.\n* Embiggened the documentation: Lots of Sphinx-friendly docstrings in the code\n  and docs in ``docs/`` and on https://urllib3.readthedocs.io/.\n* Embettered all the things!\n* Started writing this file.\n\n\n0.4.1 (2011-07-17)\n------------------\n\n* Minor bug fixes, code cleanup.\n\n\n0.4 (2011-03-01)\n----------------\n\n* Better unicode support.\n* Added ``VerifiedHTTPSConnection``.\n* Added ``NTLMConnectionPool`` in contrib.\n* Minor improvements.\n\n\n0.3.1 (2010-07-13)\n------------------\n\n* Added ``assert_host_name`` optional parameter. Now compatible with proxies.\n\n\n0.3 (2009-12-10)\n----------------\n\n* Added HTTPS support.\n* Minor bug fixes.\n* Refactored, broken backwards compatibility with 0.2.\n* API to be treated as stable from this version forward.\n\n\n0.2 (2008-11-17)\n----------------\n\n* Added unit tests.\n* Bug fixes.\n\n\n0.1 (2008-11-16)\n----------------\n\n* First release.\n", "from __future__ import absolute_import\nimport datetime\nimport logging\nimport os\nimport socket\nfrom socket import error as SocketError, timeout as SocketTimeout\nimport warnings\nfrom .packages import six\nfrom .packages.six.moves.http_client import HTTPConnection as _HTTPConnection\nfrom .packages.six.moves.http_client import HTTPException  # noqa: F401\n\ntry:  # Compiled with SSL?\n    import ssl\n\n    BaseSSLError = ssl.SSLError\nexcept (ImportError, AttributeError):  # Platform-specific: No SSL.\n    ssl = None\n\n    class BaseSSLError(BaseException):\n        pass\n\n\ntry:\n    # Python 3: not a no-op, we're adding this to the namespace so it can be imported.\n    ConnectionError = ConnectionError\nexcept NameError:\n    # Python 2\n    class ConnectionError(Exception):\n        pass\n\n\nfrom .exceptions import (\n    NewConnectionError,\n    ConnectTimeoutError,\n    SubjectAltNameWarning,\n    SystemTimeWarning,\n)\nfrom .packages.ssl_match_hostname import match_hostname, CertificateError\n\nfrom .util.ssl_ import (\n    resolve_cert_reqs,\n    resolve_ssl_version,\n    assert_fingerprint,\n    create_urllib3_context,\n    ssl_wrap_socket,\n)\n\n\nfrom .util import connection\n\nfrom ._collections import HTTPHeaderDict\n\nlog = logging.getLogger(__name__)\n\nport_by_scheme = {\"http\": 80, \"https\": 443}\n\n# When it comes time to update this value as a part of regular maintenance\n# (ie test_recent_date is failing) update it to ~6 months before the current date.\nRECENT_DATE = datetime.date(2019, 1, 1)\n\n\nclass DummyConnection(object):\n    \"\"\"Used to detect a failed ConnectionCls import.\"\"\"\n\n    pass\n\n\nclass HTTPConnection(_HTTPConnection, object):\n    \"\"\"\n    Based on httplib.HTTPConnection but provides an extra constructor\n    backwards-compatibility layer between older and newer Pythons.\n\n    Additional keyword parameters are used to configure attributes of the connection.\n    Accepted parameters include:\n\n      - ``strict``: See the documentation on :class:`urllib3.connectionpool.HTTPConnectionPool`\n      - ``source_address``: Set the source address for the current connection.\n      - ``socket_options``: Set specific options on the underlying socket. If not specified, then\n        defaults are loaded from ``HTTPConnection.default_socket_options`` which includes disabling\n        Nagle's algorithm (sets TCP_NODELAY to 1) unless the connection is behind a proxy.\n\n        For example, if you wish to enable TCP Keep Alive in addition to the defaults,\n        you might pass::\n\n            HTTPConnection.default_socket_options + [\n                (socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1),\n            ]\n\n        Or you may want to disable the defaults by passing an empty list (e.g., ``[]``).\n    \"\"\"\n\n    default_port = port_by_scheme[\"http\"]\n\n    #: Disable Nagle's algorithm by default.\n    #: ``[(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)]``\n    default_socket_options = [(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)]\n\n    #: Whether this connection verifies the host's certificate.\n    is_verified = False\n\n    def __init__(self, *args, **kw):\n        if not six.PY2:\n            kw.pop(\"strict\", None)\n\n        # Pre-set source_address.\n        self.source_address = kw.get(\"source_address\")\n\n        #: The socket options provided by the user. If no options are\n        #: provided, we use the default options.\n        self.socket_options = kw.pop(\"socket_options\", self.default_socket_options)\n\n        _HTTPConnection.__init__(self, *args, **kw)\n\n    @property\n    def host(self):\n        \"\"\"\n        Getter method to remove any trailing dots that indicate the hostname is an FQDN.\n\n        In general, SSL certificates don't include the trailing dot indicating a\n        fully-qualified domain name, and thus, they don't validate properly when\n        checked against a domain name that includes the dot. In addition, some\n        servers may not expect to receive the trailing dot when provided.\n\n        However, the hostname with trailing dot is critical to DNS resolution; doing a\n        lookup with the trailing dot will properly only resolve the appropriate FQDN,\n        whereas a lookup without a trailing dot will search the system's search domain\n        list. Thus, it's important to keep the original host around for use only in\n        those cases where it's appropriate (i.e., when doing DNS lookup to establish the\n        actual TCP connection across which we're going to send HTTP requests).\n        \"\"\"\n        return self._dns_host.rstrip(\".\")\n\n    @host.setter\n    def host(self, value):\n        \"\"\"\n        Setter for the `host` property.\n\n        We assume that only urllib3 uses the _dns_host attribute; httplib itself\n        only uses `host`, and it seems reasonable that other libraries follow suit.\n        \"\"\"\n        self._dns_host = value\n\n    def _new_conn(self):\n        \"\"\" Establish a socket connection and set nodelay settings on it.\n\n        :return: New socket connection.\n        \"\"\"\n        extra_kw = {}\n        if self.source_address:\n            extra_kw[\"source_address\"] = self.source_address\n\n        if self.socket_options:\n            extra_kw[\"socket_options\"] = self.socket_options\n\n        try:\n            conn = connection.create_connection(\n                (self._dns_host, self.port), self.timeout, **extra_kw\n            )\n\n        except SocketTimeout:\n            raise ConnectTimeoutError(\n                self,\n                \"Connection to %s timed out. (connect timeout=%s)\"\n                % (self.host, self.timeout),\n            )\n\n        except SocketError as e:\n            raise NewConnectionError(\n                self, \"Failed to establish a new connection: %s\" % e\n            )\n\n        return conn\n\n    def _prepare_conn(self, conn):\n        self.sock = conn\n        # Google App Engine's httplib does not define _tunnel_host\n        if getattr(self, \"_tunnel_host\", None):\n            # TODO: Fix tunnel so it doesn't depend on self.sock state.\n            self._tunnel()\n            # Mark this connection as not reusable\n            self.auto_open = 0\n\n    def connect(self):\n        conn = self._new_conn()\n        self._prepare_conn(conn)\n\n    def request_chunked(self, method, url, body=None, headers=None):\n        \"\"\"\n        Alternative to the common request method, which sends the\n        body with chunked encoding and not as one block\n        \"\"\"\n        headers = HTTPHeaderDict(headers if headers is not None else {})\n        skip_accept_encoding = \"accept-encoding\" in headers\n        skip_host = \"host\" in headers\n        self.putrequest(\n            method, url, skip_accept_encoding=skip_accept_encoding, skip_host=skip_host\n        )\n        for header, value in headers.items():\n            self.putheader(header, value)\n        if \"transfer-encoding\" not in headers:\n            self.putheader(\"Transfer-Encoding\", \"chunked\")\n        self.endheaders()\n\n        if body is not None:\n            stringish_types = six.string_types + (bytes,)\n            if isinstance(body, stringish_types):\n                body = (body,)\n            for chunk in body:\n                if not chunk:\n                    continue\n                if not isinstance(chunk, bytes):\n                    chunk = chunk.encode(\"utf8\")\n                len_str = hex(len(chunk))[2:]\n                self.send(len_str.encode(\"utf-8\"))\n                self.send(b\"\\r\\n\")\n                self.send(chunk)\n                self.send(b\"\\r\\n\")\n\n        # After the if clause, to always have a closed body\n        self.send(b\"0\\r\\n\\r\\n\")\n\n\nclass HTTPSConnection(HTTPConnection):\n    default_port = port_by_scheme[\"https\"]\n\n    ssl_version = None\n\n    def __init__(\n        self,\n        host,\n        port=None,\n        key_file=None,\n        cert_file=None,\n        key_password=None,\n        strict=None,\n        timeout=socket._GLOBAL_DEFAULT_TIMEOUT,\n        ssl_context=None,\n        server_hostname=None,\n        **kw\n    ):\n\n        HTTPConnection.__init__(self, host, port, strict=strict, timeout=timeout, **kw)\n\n        self.key_file = key_file\n        self.cert_file = cert_file\n        self.key_password = key_password\n        self.ssl_context = ssl_context\n        self.server_hostname = server_hostname\n\n        # Required property for Google AppEngine 1.9.0 which otherwise causes\n        # HTTPS requests to go out as HTTP. (See Issue #356)\n        self._protocol = \"https\"\n\n\nclass VerifiedHTTPSConnection(HTTPSConnection):\n    \"\"\"\n    Based on httplib.HTTPSConnection but wraps the socket with\n    SSL certification.\n    \"\"\"\n\n    cert_reqs = None\n    ca_certs = None\n    ca_cert_dir = None\n    ssl_version = None\n    assert_fingerprint = None\n\n    def set_cert(\n        self,\n        key_file=None,\n        cert_file=None,\n        cert_reqs=None,\n        key_password=None,\n        ca_certs=None,\n        assert_hostname=None,\n        assert_fingerprint=None,\n        ca_cert_dir=None,\n    ):\n        \"\"\"\n        This method should only be called once, before the connection is used.\n        \"\"\"\n        # If cert_reqs is not provided we'll assume CERT_REQUIRED unless we also\n        # have an SSLContext object in which case we'll use its verify_mode.\n        if cert_reqs is None:\n            if self.ssl_context is not None:\n                cert_reqs = self.ssl_context.verify_mode\n            else:\n                cert_reqs = resolve_cert_reqs(None)\n\n        self.key_file = key_file\n        self.cert_file = cert_file\n        self.cert_reqs = cert_reqs\n        self.key_password = key_password\n        self.assert_hostname = assert_hostname\n        self.assert_fingerprint = assert_fingerprint\n        self.ca_certs = ca_certs and os.path.expanduser(ca_certs)\n        self.ca_cert_dir = ca_cert_dir and os.path.expanduser(ca_cert_dir)\n\n    def connect(self):\n        # Add certificate verification\n        conn = self._new_conn()\n        hostname = self.host\n\n        # Google App Engine's httplib does not define _tunnel_host\n        if getattr(self, \"_tunnel_host\", None):\n            self.sock = conn\n            # Calls self._set_hostport(), so self.host is\n            # self._tunnel_host below.\n            self._tunnel()\n            # Mark this connection as not reusable\n            self.auto_open = 0\n\n            # Override the host with the one we're requesting data from.\n            hostname = self._tunnel_host\n\n        server_hostname = hostname\n        if self.server_hostname is not None:\n            server_hostname = self.server_hostname\n\n        is_time_off = datetime.date.today() < RECENT_DATE\n        if is_time_off:\n            warnings.warn(\n                (\n                    \"System time is way off (before {0}). This will probably \"\n                    \"lead to SSL verification errors\"\n                ).format(RECENT_DATE),\n                SystemTimeWarning,\n            )\n\n        # Wrap socket using verification with the root certs in\n        # trusted_root_certs\n        default_ssl_context = False\n        if self.ssl_context is None:\n            default_ssl_context = True\n            self.ssl_context = create_urllib3_context(\n                ssl_version=resolve_ssl_version(self.ssl_version),\n                cert_reqs=resolve_cert_reqs(self.cert_reqs),\n            )\n\n        context = self.ssl_context\n        context.verify_mode = resolve_cert_reqs(self.cert_reqs)\n\n        # Try to load OS default certs if none are given.\n        # Works well on Windows (requires Python3.4+)\n        if (\n            not self.ca_certs\n            and not self.ca_cert_dir\n            and default_ssl_context\n            and hasattr(context, \"load_default_certs\")\n        ):\n            context.load_default_certs()\n\n        self.sock = ssl_wrap_socket(\n            sock=conn,\n            keyfile=self.key_file,\n            certfile=self.cert_file,\n            key_password=self.key_password,\n            ca_certs=self.ca_certs,\n            ca_cert_dir=self.ca_cert_dir,\n            server_hostname=server_hostname,\n            ssl_context=context,\n        )\n\n        if self.assert_fingerprint:\n            assert_fingerprint(\n                self.sock.getpeercert(binary_form=True), self.assert_fingerprint\n            )\n        elif (\n            context.verify_mode != ssl.CERT_NONE\n            and not getattr(context, \"check_hostname\", False)\n            and self.assert_hostname is not False\n        ):\n            # While urllib3 attempts to always turn off hostname matching from\n            # the TLS library, this cannot always be done. So we check whether\n            # the TLS Library still thinks it's matching hostnames.\n            cert = self.sock.getpeercert()\n            if not cert.get(\"subjectAltName\", ()):\n                warnings.warn(\n                    (\n                        \"Certificate for {0} has no `subjectAltName`, falling back to check for a \"\n                        \"`commonName` for now. This feature is being removed by major browsers and \"\n                        \"deprecated by RFC 2818. (See https://github.com/urllib3/urllib3/issues/497 \"\n                        \"for details.)\".format(hostname)\n                    ),\n                    SubjectAltNameWarning,\n                )\n            _match_hostname(cert, self.assert_hostname or server_hostname)\n\n        self.is_verified = (\n            context.verify_mode == ssl.CERT_REQUIRED\n            or self.assert_fingerprint is not None\n        )\n\n\ndef _match_hostname(cert, asserted_hostname):\n    try:\n        match_hostname(cert, asserted_hostname)\n    except CertificateError as e:\n        log.warning(\n            \"Certificate did not match expected hostname: %s. Certificate: %s\",\n            asserted_hostname,\n            cert,\n        )\n        # Add cert to exception and reraise so client code can inspect\n        # the cert when catching the exception, if they want to\n        e._peer_cert = cert\n        raise\n\n\nif ssl:\n    # Make a copy for testing.\n    UnverifiedHTTPSConnection = HTTPSConnection\n    HTTPSConnection = VerifiedHTTPSConnection\nelse:\n    HTTPSConnection = DummyConnection\n", "import io\nimport logging\nimport socket\nimport sys\nimport time\nimport warnings\nimport pytest\n\nimport mock\n\nfrom .. import TARPIT_HOST, VALID_SOURCE_ADDRESSES, INVALID_SOURCE_ADDRESSES\nfrom ..port_helpers import find_unused_port\nfrom urllib3 import encode_multipart_formdata, HTTPConnectionPool\nfrom urllib3.exceptions import (\n    ConnectTimeoutError,\n    EmptyPoolError,\n    DecodeError,\n    MaxRetryError,\n    ReadTimeoutError,\n    NewConnectionError,\n    UnrewindableBodyError,\n)\nfrom urllib3.packages.six import b, u\nfrom urllib3.packages.six.moves.urllib.parse import urlencode\nfrom urllib3.util.retry import Retry, RequestHistory\nfrom urllib3.util.timeout import Timeout\n\nfrom test import SHORT_TIMEOUT, LONG_TIMEOUT\nfrom dummyserver.testcase import HTTPDummyServerTestCase, SocketDummyServerTestCase\nfrom dummyserver.server import NoIPv6Warning, HAS_IPV6_AND_DNS\n\nfrom threading import Event\n\npytestmark = pytest.mark.flaky\n\nlog = logging.getLogger(\"urllib3.connectionpool\")\nlog.setLevel(logging.NOTSET)\nlog.addHandler(logging.StreamHandler(sys.stdout))\n\n\ndef wait_for_socket(ready_event):\n    ready_event.wait()\n    ready_event.clear()\n\n\nclass TestConnectionPoolTimeouts(SocketDummyServerTestCase):\n    def test_timeout_float(self):\n        block_event = Event()\n        ready_event = self.start_basic_handler(block_send=block_event, num=2)\n\n        with HTTPConnectionPool(self.host, self.port, retries=False) as pool:\n            wait_for_socket(ready_event)\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\", timeout=SHORT_TIMEOUT)\n            block_event.set()  # Release block\n\n            # Shouldn't raise this time\n            wait_for_socket(ready_event)\n            block_event.set()  # Pre-release block\n            pool.request(\"GET\", \"/\", timeout=LONG_TIMEOUT)\n\n    def test_conn_closed(self):\n        block_event = Event()\n        self.start_basic_handler(block_send=block_event, num=1)\n\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=SHORT_TIMEOUT, retries=False\n        ) as pool:\n            conn = pool._get_conn()\n            pool._put_conn(conn)\n            try:\n                with pytest.raises(ReadTimeoutError):\n                    pool.urlopen(\"GET\", \"/\")\n                if conn.sock:\n                    with pytest.raises(socket.error):\n                        conn.sock.recv(1024)\n            finally:\n                pool._put_conn(conn)\n\n            block_event.set()\n\n    def test_timeout(self):\n        # Requests should time out when expected\n        block_event = Event()\n        ready_event = self.start_basic_handler(block_send=block_event, num=3)\n\n        # Pool-global timeout\n        short_timeout = Timeout(read=SHORT_TIMEOUT)\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=short_timeout, retries=False\n        ) as pool:\n            wait_for_socket(ready_event)\n            block_event.clear()\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\")\n            block_event.set()  # Release request\n\n        # Request-specific timeouts should raise errors\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=short_timeout, retries=False\n        ) as pool:\n            wait_for_socket(ready_event)\n            now = time.time()\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\", timeout=LONG_TIMEOUT)\n            delta = time.time() - now\n\n            message = \"timeout was pool-level SHORT_TIMEOUT rather than request-level LONG_TIMEOUT\"\n            assert delta >= LONG_TIMEOUT, message\n            block_event.set()  # Release request\n\n            # Timeout passed directly to request should raise a request timeout\n            wait_for_socket(ready_event)\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\", timeout=SHORT_TIMEOUT)\n            block_event.set()  # Release request\n\n    def test_connect_timeout(self):\n        url = \"/\"\n        host, port = TARPIT_HOST, 80\n        timeout = Timeout(connect=SHORT_TIMEOUT)\n\n        # Pool-global timeout\n        with HTTPConnectionPool(host, port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            with pytest.raises(ConnectTimeoutError):\n                pool._make_request(conn, \"GET\", url)\n\n            # Retries\n            retries = Retry(connect=0)\n            with pytest.raises(MaxRetryError):\n                pool.request(\"GET\", url, retries=retries)\n\n        # Request-specific connection timeouts\n        big_timeout = Timeout(read=LONG_TIMEOUT, connect=LONG_TIMEOUT)\n        with HTTPConnectionPool(host, port, timeout=big_timeout, retries=False) as pool:\n            conn = pool._get_conn()\n            with pytest.raises(ConnectTimeoutError):\n                pool._make_request(conn, \"GET\", url, timeout=timeout)\n\n            pool._put_conn(conn)\n            with pytest.raises(ConnectTimeoutError):\n                pool.request(\"GET\", url, timeout=timeout)\n\n    def test_total_applies_connect(self):\n        host, port = TARPIT_HOST, 80\n\n        timeout = Timeout(total=None, connect=SHORT_TIMEOUT)\n        with HTTPConnectionPool(host, port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            try:\n                with pytest.raises(ConnectTimeoutError):\n                    pool._make_request(conn, \"GET\", \"/\")\n            finally:\n                conn.close()\n\n        timeout = Timeout(connect=3, read=5, total=SHORT_TIMEOUT)\n        with HTTPConnectionPool(host, port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            try:\n                with pytest.raises(ConnectTimeoutError):\n                    pool._make_request(conn, \"GET\", \"/\")\n            finally:\n                conn.close()\n\n    def test_total_timeout(self):\n        block_event = Event()\n        ready_event = self.start_basic_handler(block_send=block_event, num=2)\n\n        wait_for_socket(ready_event)\n        # This will get the socket to raise an EAGAIN on the read\n        timeout = Timeout(connect=3, read=SHORT_TIMEOUT)\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=timeout, retries=False\n        ) as pool:\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\")\n\n            block_event.set()\n            wait_for_socket(ready_event)\n            block_event.clear()\n\n        # The connect should succeed and this should hit the read timeout\n        timeout = Timeout(connect=3, read=5, total=SHORT_TIMEOUT)\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=timeout, retries=False\n        ) as pool:\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\")\n\n    def test_create_connection_timeout(self):\n        self.start_basic_handler(block_send=Event(), num=0)  # needed for self.port\n\n        timeout = Timeout(connect=SHORT_TIMEOUT, total=LONG_TIMEOUT)\n        with HTTPConnectionPool(\n            TARPIT_HOST, self.port, timeout=timeout, retries=False\n        ) as pool:\n            conn = pool._new_conn()\n            with pytest.raises(ConnectTimeoutError):\n                conn.connect()\n\n\nclass TestConnectionPool(HTTPDummyServerTestCase):\n    def test_get(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/specific_method\", fields={\"method\": \"GET\"})\n            assert r.status == 200, r.data\n\n    def test_post_url(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"POST\", \"/specific_method\", fields={\"method\": \"POST\"})\n            assert r.status == 200, r.data\n\n    def test_urlopen_put(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.urlopen(\"PUT\", \"/specific_method?method=PUT\")\n            assert r.status == 200, r.data\n\n    def test_wrong_specific_method(self):\n        # To make sure the dummy server is actually returning failed responses\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/specific_method\", fields={\"method\": \"POST\"})\n            assert r.status == 400, r.data\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"POST\", \"/specific_method\", fields={\"method\": \"GET\"})\n            assert r.status == 400, r.data\n\n    def test_upload(self):\n        data = \"I'm in ur multipart form-data, hazing a cheezburgr\"\n        fields = {\n            \"upload_param\": \"filefield\",\n            \"upload_filename\": \"lolcat.txt\",\n            \"upload_size\": len(data),\n            \"filefield\": (\"lolcat.txt\", data),\n        }\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"POST\", \"/upload\", fields=fields)\n            assert r.status == 200, r.data\n\n    def test_one_name_multiple_values(self):\n        fields = [(\"foo\", \"a\"), (\"foo\", \"b\")]\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            # urlencode\n            r = pool.request(\"GET\", \"/echo\", fields=fields)\n            assert r.data == b\"foo=a&foo=b\"\n\n            # multipart\n            r = pool.request(\"POST\", \"/echo\", fields=fields)\n            assert r.data.count(b'name=\"foo\"') == 2\n\n    def test_request_method_body(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            body = b\"hi\"\n            r = pool.request(\"POST\", \"/echo\", body=body)\n            assert r.data == body\n\n            fields = [(\"hi\", \"hello\")]\n            with pytest.raises(TypeError):\n                pool.request(\"POST\", \"/echo\", body=body, fields=fields)\n\n    def test_unicode_upload(self):\n        fieldname = u(\"myfile\")\n        filename = u(\"\\xe2\\x99\\xa5.txt\")\n        data = u(\"\\xe2\\x99\\xa5\").encode(\"utf8\")\n        size = len(data)\n\n        fields = {\n            u(\"upload_param\"): fieldname,\n            u(\"upload_filename\"): filename,\n            u(\"upload_size\"): size,\n            fieldname: (filename, data),\n        }\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"POST\", \"/upload\", fields=fields)\n            assert r.status == 200, r.data\n\n    def test_nagle(self):\n        \"\"\" Test that connections have TCP_NODELAY turned on \"\"\"\n        # This test needs to be here in order to be run. socket.create_connection actually tries\n        # to connect to the host provided so we need a dummyserver to be running.\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            conn = pool._get_conn()\n            try:\n                pool._make_request(conn, \"GET\", \"/\")\n                tcp_nodelay_setting = conn.sock.getsockopt(\n                    socket.IPPROTO_TCP, socket.TCP_NODELAY\n                )\n                assert tcp_nodelay_setting\n            finally:\n                conn.close()\n\n    def test_socket_options(self):\n        \"\"\"Test that connections accept socket options.\"\"\"\n        # This test needs to be here in order to be run. socket.create_connection actually tries to\n        # connect to the host provided so we need a dummyserver to be running.\n        with HTTPConnectionPool(\n            self.host,\n            self.port,\n            socket_options=[(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)],\n        ) as pool:\n            s = pool._new_conn()._new_conn()  # Get the socket\n            try:\n                using_keepalive = (\n                    s.getsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE) > 0\n                )\n                assert using_keepalive\n            finally:\n                s.close()\n\n    def test_disable_default_socket_options(self):\n        \"\"\"Test that passing None disables all socket options.\"\"\"\n        # This test needs to be here in order to be run. socket.create_connection actually tries\n        # to connect to the host provided so we need a dummyserver to be running.\n        with HTTPConnectionPool(self.host, self.port, socket_options=None) as pool:\n            s = pool._new_conn()._new_conn()\n            try:\n                using_nagle = s.getsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY) == 0\n                assert using_nagle\n            finally:\n                s.close()\n\n    def test_defaults_are_applied(self):\n        \"\"\"Test that modifying the default socket options works.\"\"\"\n        # This test needs to be here in order to be run. socket.create_connection actually tries\n        # to connect to the host provided so we need a dummyserver to be running.\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            # Get the HTTPConnection instance\n            conn = pool._new_conn()\n            try:\n                # Update the default socket options\n                conn.default_socket_options += [\n                    (socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)\n                ]\n                s = conn._new_conn()\n                nagle_disabled = (\n                    s.getsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY) > 0\n                )\n                using_keepalive = (\n                    s.getsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE) > 0\n                )\n                assert nagle_disabled\n                assert using_keepalive\n            finally:\n                conn.close()\n                s.close()\n\n    def test_connection_error_retries(self):\n        \"\"\" ECONNREFUSED error should raise a connection error, with retries \"\"\"\n        port = find_unused_port()\n        with HTTPConnectionPool(self.host, port) as pool:\n            with pytest.raises(MaxRetryError) as e:\n                pool.request(\"GET\", \"/\", retries=Retry(connect=3))\n            assert type(e.value.reason) == NewConnectionError\n\n    def test_timeout_success(self):\n        timeout = Timeout(connect=3, read=5, total=None)\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            pool.request(\"GET\", \"/\")\n            # This should not raise a \"Timeout already started\" error\n            pool.request(\"GET\", \"/\")\n\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            # This should also not raise a \"Timeout already started\" error\n            pool.request(\"GET\", \"/\")\n\n        timeout = Timeout(total=None)\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            pool.request(\"GET\", \"/\")\n\n    def test_tunnel(self):\n        # note the actual httplib.py has no tests for this functionality\n        timeout = Timeout(total=None)\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            try:\n                conn.set_tunnel(self.host, self.port)\n                conn._tunnel = mock.Mock(return_value=None)\n                pool._make_request(conn, \"GET\", \"/\")\n                conn._tunnel.assert_called_once_with()\n            finally:\n                conn.close()\n\n        # test that it's not called when tunnel is not set\n        timeout = Timeout(total=None)\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            try:\n                conn._tunnel = mock.Mock(return_value=None)\n                pool._make_request(conn, \"GET\", \"/\")\n                assert not conn._tunnel.called\n            finally:\n                conn.close()\n\n    def test_redirect(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"}, redirect=False)\n            assert r.status == 303\n\n            r = pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"})\n            assert r.status == 200\n            assert r.data == b\"Dummy server!\"\n\n    def test_bad_connect(self):\n        with HTTPConnectionPool(\"badhost.invalid\", self.port) as pool:\n            with pytest.raises(MaxRetryError) as e:\n                pool.request(\"GET\", \"/\", retries=5)\n            assert type(e.value.reason) == NewConnectionError\n\n    def test_keepalive(self):\n        with HTTPConnectionPool(self.host, self.port, block=True, maxsize=1) as pool:\n            r = pool.request(\"GET\", \"/keepalive?close=0\")\n            r = pool.request(\"GET\", \"/keepalive?close=0\")\n\n            assert r.status == 200\n            assert pool.num_connections == 1\n            assert pool.num_requests == 2\n\n    def test_keepalive_close(self):\n        with HTTPConnectionPool(\n            self.host, self.port, block=True, maxsize=1, timeout=2\n        ) as pool:\n            r = pool.request(\n                \"GET\", \"/keepalive?close=1\", retries=0, headers={\"Connection\": \"close\"}\n            )\n\n            assert pool.num_connections == 1\n\n            # The dummyserver will have responded with Connection:close,\n            # and httplib will properly cleanup the socket.\n\n            # We grab the HTTPConnection object straight from the Queue,\n            # because _get_conn() is where the check & reset occurs\n            # pylint: disable-msg=W0212\n            conn = pool.pool.get()\n            assert conn.sock is None\n            pool._put_conn(conn)\n\n            # Now with keep-alive\n            r = pool.request(\n                \"GET\",\n                \"/keepalive?close=0\",\n                retries=0,\n                headers={\"Connection\": \"keep-alive\"},\n            )\n\n            # The dummyserver responded with Connection:keep-alive, the connection\n            # persists.\n            conn = pool.pool.get()\n            assert conn.sock is not None\n            pool._put_conn(conn)\n\n            # Another request asking the server to close the connection. This one\n            # should get cleaned up for the next request.\n            r = pool.request(\n                \"GET\", \"/keepalive?close=1\", retries=0, headers={\"Connection\": \"close\"}\n            )\n\n            assert r.status == 200\n\n            conn = pool.pool.get()\n            assert conn.sock is None\n            pool._put_conn(conn)\n\n            # Next request\n            r = pool.request(\"GET\", \"/keepalive?close=0\")\n\n    def test_post_with_urlencode(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            data = {\"banana\": \"hammock\", \"lol\": \"cat\"}\n            r = pool.request(\"POST\", \"/echo\", fields=data, encode_multipart=False)\n            assert r.data.decode(\"utf-8\") == urlencode(data)\n\n    def test_post_with_multipart(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            data = {\"banana\": \"hammock\", \"lol\": \"cat\"}\n            r = pool.request(\"POST\", \"/echo\", fields=data, encode_multipart=True)\n            body = r.data.split(b\"\\r\\n\")\n\n            encoded_data = encode_multipart_formdata(data)[0]\n            expected_body = encoded_data.split(b\"\\r\\n\")\n\n            # TODO: Get rid of extra parsing stuff when you can specify\n            # a custom boundary to encode_multipart_formdata\n            \"\"\"\n            We need to loop the return lines because a timestamp is attached\n            from within encode_multipart_formdata. When the server echos back\n            the data, it has the timestamp from when the data was encoded, which\n            is not equivalent to when we run encode_multipart_formdata on\n            the data again.\n            \"\"\"\n            for i, line in enumerate(body):\n                if line.startswith(b\"--\"):\n                    continue\n\n                assert body[i] == expected_body[i]\n\n    def test_post_with_multipart__iter__(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            data = {\"hello\": \"world\"}\n            r = pool.request(\n                \"POST\",\n                \"/echo\",\n                fields=data,\n                preload_content=False,\n                multipart_boundary=\"boundary\",\n                encode_multipart=True,\n            )\n\n            chunks = [chunk for chunk in r]\n            assert chunks == [\n                b\"--boundary\\r\\n\",\n                b'Content-Disposition: form-data; name=\"hello\"\\r\\n',\n                b\"\\r\\n\",\n                b\"world\\r\\n\",\n                b\"--boundary--\\r\\n\",\n            ]\n\n    def test_check_gzip(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\", \"/encodingrequest\", headers={\"accept-encoding\": \"gzip\"}\n            )\n            assert r.headers.get(\"content-encoding\") == \"gzip\"\n            assert r.data == b\"hello, world!\"\n\n    def test_check_deflate(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\", \"/encodingrequest\", headers={\"accept-encoding\": \"deflate\"}\n            )\n            assert r.headers.get(\"content-encoding\") == \"deflate\"\n            assert r.data == b\"hello, world!\"\n\n    def test_bad_decode(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            with pytest.raises(DecodeError):\n                pool.request(\n                    \"GET\",\n                    \"/encodingrequest\",\n                    headers={\"accept-encoding\": \"garbage-deflate\"},\n                )\n\n            with pytest.raises(DecodeError):\n                pool.request(\n                    \"GET\",\n                    \"/encodingrequest\",\n                    headers={\"accept-encoding\": \"garbage-gzip\"},\n                )\n\n    def test_connection_count(self):\n        with HTTPConnectionPool(self.host, self.port, maxsize=1) as pool:\n            pool.request(\"GET\", \"/\")\n            pool.request(\"GET\", \"/\")\n            pool.request(\"GET\", \"/\")\n\n            assert pool.num_connections == 1\n            assert pool.num_requests == 3\n\n    def test_connection_count_bigpool(self):\n        with HTTPConnectionPool(self.host, self.port, maxsize=16) as http_pool:\n            http_pool.request(\"GET\", \"/\")\n            http_pool.request(\"GET\", \"/\")\n            http_pool.request(\"GET\", \"/\")\n\n            assert http_pool.num_connections == 1\n            assert http_pool.num_requests == 3\n\n    def test_partial_response(self):\n        with HTTPConnectionPool(self.host, self.port, maxsize=1) as pool:\n            req_data = {\"lol\": \"cat\"}\n            resp_data = urlencode(req_data).encode(\"utf-8\")\n\n            r = pool.request(\"GET\", \"/echo\", fields=req_data, preload_content=False)\n\n            assert r.read(5) == resp_data[:5]\n            assert r.read() == resp_data[5:]\n\n    def test_lazy_load_twice(self):\n        # This test is sad and confusing. Need to figure out what's\n        # going on with partial reads and socket reuse.\n\n        with HTTPConnectionPool(\n            self.host, self.port, block=True, maxsize=1, timeout=2\n        ) as pool:\n            payload_size = 1024 * 2\n            first_chunk = 512\n\n            boundary = \"foo\"\n\n            req_data = {\"count\": \"a\" * payload_size}\n            resp_data = encode_multipart_formdata(req_data, boundary=boundary)[0]\n\n            req2_data = {\"count\": \"b\" * payload_size}\n            resp2_data = encode_multipart_formdata(req2_data, boundary=boundary)[0]\n\n            r1 = pool.request(\n                \"POST\",\n                \"/echo\",\n                fields=req_data,\n                multipart_boundary=boundary,\n                preload_content=False,\n            )\n\n            assert r1.read(first_chunk) == resp_data[:first_chunk]\n\n            try:\n                r2 = pool.request(\n                    \"POST\",\n                    \"/echo\",\n                    fields=req2_data,\n                    multipart_boundary=boundary,\n                    preload_content=False,\n                    pool_timeout=0.001,\n                )\n\n                # This branch should generally bail here, but maybe someday it will\n                # work? Perhaps by some sort of magic. Consider it a TODO.\n\n                assert r2.read(first_chunk) == resp2_data[:first_chunk]\n\n                assert r1.read() == resp_data[first_chunk:]\n                assert r2.read() == resp2_data[first_chunk:]\n                assert pool.num_requests == 2\n\n            except EmptyPoolError:\n                assert r1.read() == resp_data[first_chunk:]\n                assert pool.num_requests == 1\n\n            assert pool.num_connections == 1\n\n    def test_for_double_release(self):\n        MAXSIZE = 5\n\n        # Check default state\n        with HTTPConnectionPool(self.host, self.port, maxsize=MAXSIZE) as pool:\n            assert pool.num_connections == 0\n            assert pool.pool.qsize() == MAXSIZE\n\n            # Make an empty slot for testing\n            pool.pool.get()\n            assert pool.pool.qsize() == MAXSIZE - 1\n\n            # Check state after simple request\n            pool.urlopen(\"GET\", \"/\")\n            assert pool.pool.qsize() == MAXSIZE - 1\n\n            # Check state without release\n            pool.urlopen(\"GET\", \"/\", preload_content=False)\n            assert pool.pool.qsize() == MAXSIZE - 2\n\n            pool.urlopen(\"GET\", \"/\")\n            assert pool.pool.qsize() == MAXSIZE - 2\n\n            # Check state after read\n            pool.urlopen(\"GET\", \"/\").data\n            assert pool.pool.qsize() == MAXSIZE - 2\n\n            pool.urlopen(\"GET\", \"/\")\n            assert pool.pool.qsize() == MAXSIZE - 2\n\n    def test_release_conn_parameter(self):\n        MAXSIZE = 5\n        with HTTPConnectionPool(self.host, self.port, maxsize=MAXSIZE) as pool:\n            assert pool.pool.qsize() == MAXSIZE\n\n            # Make request without releasing connection\n            pool.request(\"GET\", \"/\", release_conn=False, preload_content=False)\n            assert pool.pool.qsize() == MAXSIZE - 1\n\n    def test_dns_error(self):\n        with HTTPConnectionPool(\n            \"thishostdoesnotexist.invalid\", self.port, timeout=0.001\n        ) as pool:\n            with pytest.raises(MaxRetryError):\n                pool.request(\"GET\", \"/test\", retries=2)\n\n    def test_percent_encode_invalid_target_chars(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/echo_params?q=\\r&k=\\n \\n\")\n            assert r.data == b\"[('k', '\\\\n \\\\n'), ('q', '\\\\r')]\"\n\n    def test_source_address(self):\n        for addr, is_ipv6 in VALID_SOURCE_ADDRESSES:\n            if is_ipv6 and not HAS_IPV6_AND_DNS:\n                warnings.warn(\"No IPv6 support: skipping.\", NoIPv6Warning)\n                continue\n            with HTTPConnectionPool(\n                self.host, self.port, source_address=addr, retries=False\n            ) as pool:\n                r = pool.request(\"GET\", \"/source_address\")\n                assert r.data == b(addr[0])\n\n    def test_source_address_error(self):\n        for addr in INVALID_SOURCE_ADDRESSES:\n            with HTTPConnectionPool(\n                self.host, self.port, source_address=addr, retries=False\n            ) as pool:\n                with pytest.raises(NewConnectionError):\n                    pool.request(\"GET\", \"/source_address?{0}\".format(addr))\n\n    def test_stream_keepalive(self):\n        x = 2\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            for _ in range(x):\n                response = pool.request(\n                    \"GET\",\n                    \"/chunked\",\n                    headers={\"Connection\": \"keep-alive\"},\n                    preload_content=False,\n                    retries=False,\n                )\n                for chunk in response.stream():\n                    assert chunk == b\"123\"\n\n            assert pool.num_connections == 1\n            assert pool.num_requests == x\n\n    def test_read_chunked_short_circuit(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            response = pool.request(\"GET\", \"/chunked\", preload_content=False)\n            response.read()\n            with pytest.raises(StopIteration):\n                next(response.read_chunked())\n\n    def test_read_chunked_on_closed_response(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            response = pool.request(\"GET\", \"/chunked\", preload_content=False)\n            response.close()\n            with pytest.raises(StopIteration):\n                next(response.read_chunked())\n\n    def test_chunked_gzip(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            response = pool.request(\n                \"GET\", \"/chunked_gzip\", preload_content=False, decode_content=True\n            )\n\n            assert b\"123\" * 4 == response.read()\n\n    def test_cleanup_on_connection_error(self):\n        \"\"\"\n        Test that connections are recycled to the pool on\n        connection errors where no http response is received.\n        \"\"\"\n        poolsize = 3\n        with HTTPConnectionPool(\n            self.host, self.port, maxsize=poolsize, block=True\n        ) as http:\n            assert http.pool.qsize() == poolsize\n\n            # force a connection error by supplying a non-existent\n            # url. We won't get a response for this  and so the\n            # conn won't be implicitly returned to the pool.\n            with pytest.raises(MaxRetryError):\n                http.request(\n                    \"GET\",\n                    \"/redirect\",\n                    fields={\"target\": \"/\"},\n                    release_conn=False,\n                    retries=0,\n                )\n\n            r = http.request(\n                \"GET\",\n                \"/redirect\",\n                fields={\"target\": \"/\"},\n                release_conn=False,\n                retries=1,\n            )\n            r.release_conn()\n\n            # the pool should still contain poolsize elements\n            assert http.pool.qsize() == http.pool.maxsize\n\n    def test_mixed_case_hostname(self):\n        with HTTPConnectionPool(\"LoCaLhOsT\", self.port) as pool:\n            response = pool.request(\"GET\", \"http://LoCaLhOsT:%d/\" % self.port)\n            assert response.status == 200\n\n\nclass TestRetry(HTTPDummyServerTestCase):\n    def test_max_retry(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            with pytest.raises(MaxRetryError):\n                pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"}, retries=0)\n\n    def test_disabled_retry(self):\n        \"\"\" Disabled retries should disable redirect handling. \"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"}, retries=False)\n            assert r.status == 303\n\n            r = pool.request(\n                \"GET\",\n                \"/redirect\",\n                fields={\"target\": \"/\"},\n                retries=Retry(redirect=False),\n            )\n            assert r.status == 303\n\n        with HTTPConnectionPool(\n            \"thishostdoesnotexist.invalid\", self.port, timeout=0.001\n        ) as pool:\n            with pytest.raises(NewConnectionError):\n                pool.request(\"GET\", \"/test\", retries=False)\n\n    def test_read_retries(self):\n        \"\"\" Should retry for status codes in the whitelist \"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            retry = Retry(read=1, status_forcelist=[418])\n            resp = pool.request(\n                \"GET\",\n                \"/successful_retry\",\n                headers={\"test-name\": \"test_read_retries\"},\n                retries=retry,\n            )\n            assert resp.status == 200\n\n    def test_read_total_retries(self):\n        \"\"\" HTTP response w/ status code in the whitelist should be retried \"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_read_total_retries\"}\n            retry = Retry(total=1, status_forcelist=[418])\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 200\n\n    def test_retries_wrong_whitelist(self):\n        \"\"\"HTTP response w/ status code not in whitelist shouldn't be retried\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            retry = Retry(total=1, status_forcelist=[202])\n            resp = pool.request(\n                \"GET\",\n                \"/successful_retry\",\n                headers={\"test-name\": \"test_wrong_whitelist\"},\n                retries=retry,\n            )\n            assert resp.status == 418\n\n    def test_default_method_whitelist_retried(self):\n        \"\"\" urllib3 should retry methods in the default method whitelist \"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            retry = Retry(total=1, status_forcelist=[418])\n            resp = pool.request(\n                \"OPTIONS\",\n                \"/successful_retry\",\n                headers={\"test-name\": \"test_default_whitelist\"},\n                retries=retry,\n            )\n            assert resp.status == 200\n\n    def test_retries_wrong_method_list(self):\n        \"\"\"Method not in our whitelist should not be retried, even if code matches\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_wrong_method_whitelist\"}\n            retry = Retry(total=1, status_forcelist=[418], method_whitelist=[\"POST\"])\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 418\n\n    def test_read_retries_unsuccessful(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_read_retries_unsuccessful\"}\n            resp = pool.request(\"GET\", \"/successful_retry\", headers=headers, retries=1)\n            assert resp.status == 418\n\n    def test_retry_reuse_safe(self):\n        \"\"\" It should be possible to reuse a Retry object across requests \"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_retry_safe\"}\n            retry = Retry(total=1, status_forcelist=[418])\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 200\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 200\n\n    def test_retry_return_in_response(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_retry_return_in_response\"}\n            retry = Retry(total=2, status_forcelist=[418])\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 200\n            assert resp.retries.total == 1\n            assert resp.retries.history == (\n                RequestHistory(\"GET\", \"/successful_retry\", None, 418, None),\n            )\n\n    def test_retry_redirect_history(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            resp = pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"})\n            assert resp.status == 200\n            assert resp.retries.history == (\n                RequestHistory(\"GET\", \"/redirect?target=%2F\", None, 303, \"/\"),\n            )\n\n    def test_multi_redirect_history(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\",\n                \"/multi_redirect\",\n                fields={\"redirect_codes\": \"303,302,200\"},\n                redirect=False,\n            )\n            assert r.status == 303\n            assert r.retries.history == tuple()\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\",\n                \"/multi_redirect\",\n                retries=10,\n                fields={\"redirect_codes\": \"303,302,301,307,302,200\"},\n            )\n            assert r.status == 200\n            assert r.data == b\"Done redirecting\"\n\n            expected = [\n                (303, \"/multi_redirect?redirect_codes=302,301,307,302,200\"),\n                (302, \"/multi_redirect?redirect_codes=301,307,302,200\"),\n                (301, \"/multi_redirect?redirect_codes=307,302,200\"),\n                (307, \"/multi_redirect?redirect_codes=302,200\"),\n                (302, \"/multi_redirect?redirect_codes=200\"),\n            ]\n            actual = [\n                (history.status, history.redirect_location)\n                for history in r.retries.history\n            ]\n            assert actual == expected\n\n\nclass TestRetryAfter(HTTPDummyServerTestCase):\n    def test_retry_after(self):\n        # Request twice in a second to get a 429 response.\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"429 Too Many Requests\"},\n                retries=False,\n            )\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"429 Too Many Requests\"},\n                retries=False,\n            )\n            assert r.status == 429\n\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"429 Too Many Requests\"},\n                retries=True,\n            )\n            assert r.status == 200\n\n            # Request twice in a second to get a 503 response.\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"503 Service Unavailable\"},\n                retries=False,\n            )\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"503 Service Unavailable\"},\n                retries=False,\n            )\n            assert r.status == 503\n\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"503 Service Unavailable\"},\n                retries=True,\n            )\n            assert r.status == 200\n\n            # Ignore Retry-After header on status which is not defined in\n            # Retry.RETRY_AFTER_STATUS_CODES.\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"418 I'm a teapot\"},\n                retries=True,\n            )\n            assert r.status == 418\n\n    def test_redirect_after(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/redirect_after\", retries=False)\n            assert r.status == 303\n\n            t = time.time()\n            r = pool.request(\"GET\", \"/redirect_after\")\n            assert r.status == 200\n            delta = time.time() - t\n            assert delta >= 1\n\n            t = time.time()\n            timestamp = t + 2\n            r = pool.request(\"GET\", \"/redirect_after?date=\" + str(timestamp))\n            assert r.status == 200\n            delta = time.time() - t\n            assert delta >= 1\n\n            # Retry-After is past\n            t = time.time()\n            timestamp = t - 1\n            r = pool.request(\"GET\", \"/redirect_after?date=\" + str(timestamp))\n            delta = time.time() - t\n            assert r.status == 200\n            assert delta < 1\n\n\nclass TestFileBodiesOnRetryOrRedirect(HTTPDummyServerTestCase):\n    def test_retries_put_filehandle(self):\n        \"\"\"HTTP PUT retry with a file-like object should not timeout\"\"\"\n        with HTTPConnectionPool(self.host, self.port, timeout=0.1) as pool:\n            retry = Retry(total=3, status_forcelist=[418])\n            # httplib reads in 8k chunks; use a larger content length\n            content_length = 65535\n            data = b\"A\" * content_length\n            uploaded_file = io.BytesIO(data)\n            headers = {\n                \"test-name\": \"test_retries_put_filehandle\",\n                \"Content-Length\": str(content_length),\n            }\n            resp = pool.urlopen(\n                \"PUT\",\n                \"/successful_retry\",\n                headers=headers,\n                retries=retry,\n                body=uploaded_file,\n                assert_same_host=False,\n                redirect=False,\n            )\n            assert resp.status == 200\n\n    def test_redirect_put_file(self):\n        \"\"\"PUT with file object should work with a redirection response\"\"\"\n        with HTTPConnectionPool(self.host, self.port, timeout=0.1) as pool:\n            retry = Retry(total=3, status_forcelist=[418])\n            # httplib reads in 8k chunks; use a larger content length\n            content_length = 65535\n            data = b\"A\" * content_length\n            uploaded_file = io.BytesIO(data)\n            headers = {\n                \"test-name\": \"test_redirect_put_file\",\n                \"Content-Length\": str(content_length),\n            }\n            url = \"/redirect?target=/echo&status=307\"\n            resp = pool.urlopen(\n                \"PUT\",\n                url,\n                headers=headers,\n                retries=retry,\n                body=uploaded_file,\n                assert_same_host=False,\n                redirect=True,\n            )\n            assert resp.status == 200\n            assert resp.data == data\n\n    def test_redirect_with_failed_tell(self):\n        \"\"\"Abort request if failed to get a position from tell()\"\"\"\n\n        class BadTellObject(io.BytesIO):\n            def tell(self):\n                raise IOError\n\n        body = BadTellObject(b\"the data\")\n        url = \"/redirect?target=/successful_retry\"\n        # httplib uses fileno if Content-Length isn't supplied,\n        # which is unsupported by BytesIO.\n        headers = {\"Content-Length\": \"8\"}\n        with HTTPConnectionPool(self.host, self.port, timeout=0.1) as pool:\n            with pytest.raises(UnrewindableBodyError) as e:\n                pool.urlopen(\"PUT\", url, headers=headers, body=body)\n            assert \"Unable to record file position for\" in str(e.value)\n\n\nclass TestRetryPoolSize(HTTPDummyServerTestCase):\n    def test_pool_size_retry(self):\n        retries = Retry(total=1, raise_on_status=False, status_forcelist=[404])\n        with HTTPConnectionPool(\n            self.host, self.port, maxsize=10, retries=retries, block=True\n        ) as pool:\n            pool.urlopen(\"GET\", \"/not_found\", preload_content=False)\n            assert pool.num_connections == 1\n\n\nclass TestRedirectPoolSize(HTTPDummyServerTestCase):\n    def test_pool_size_redirect(self):\n        retries = Retry(\n            total=1, raise_on_status=False, status_forcelist=[404], redirect=True\n        )\n        with HTTPConnectionPool(\n            self.host, self.port, maxsize=10, retries=retries, block=True\n        ) as pool:\n            pool.urlopen(\"GET\", \"/redirect\", preload_content=False)\n            assert pool.num_connections == 1\n"], "fixing_code": ["Changes\n=======\n\nmaster (dev)\n------------\n\n* Raise ``ValueError`` if control characters are given in\n  the ``method`` parameter of ``HTTPConnection.request()`` (Pull #1800)\n\n\n1.25.8 (2020-01-20)\n-------------------\n\n* Drop support for EOL Python 3.4 (Pull #1774)\n\n* Optimize _encode_invalid_chars (Pull #1787)\n\n\n1.25.7 (2019-11-11)\n-------------------\n\n* Preserve ``chunked`` parameter on retries (Pull #1715, Pull #1734)\n\n* Allow unset ``SERVER_SOFTWARE`` in App Engine (Pull #1704, Issue #1470)\n\n* Fix issue where URL fragment was sent within the request target. (Pull #1732)\n\n* Fix issue where an empty query section in a URL would fail to parse. (Pull #1732)\n\n* Remove TLS 1.3 support in SecureTransport due to Apple removing support (Pull #1703)\n\n\n1.25.6 (2019-09-24)\n-------------------\n\n* Fix issue where tilde (``~``) characters were incorrectly\n  percent-encoded in the path. (Pull #1692)\n\n\n1.25.5 (2019-09-19)\n-------------------\n\n* Add mitigation for BPO-37428 affecting Python <3.7.4 and OpenSSL 1.1.1+ which\n  caused certificate verification to be enabled when using ``cert_reqs=CERT_NONE``.\n  (Issue #1682)\n\n\n1.25.4 (2019-09-19)\n-------------------\n\n* Propagate Retry-After header settings to subsequent retries. (Pull #1607)\n\n* Fix edge case where Retry-After header was still respected even when\n  explicitly opted out of. (Pull #1607)\n\n* Remove dependency on ``rfc3986`` for URL parsing.\n\n* Fix issue where URLs containing invalid characters within ``Url.auth`` would\n  raise an exception instead of percent-encoding those characters.\n\n* Add support for ``HTTPResponse.auto_close = False`` which makes HTTP responses\n  work well with BufferedReaders and other ``io`` module features. (Pull #1652)\n\n* Percent-encode invalid characters in URL for ``HTTPConnectionPool.request()`` (Pull #1673)\n\n\n1.25.3 (2019-05-23)\n-------------------\n\n* Change ``HTTPSConnection`` to load system CA certificates\n  when ``ca_certs``, ``ca_cert_dir``, and ``ssl_context`` are\n  unspecified. (Pull #1608, Issue #1603)\n\n* Upgrade bundled rfc3986 to v1.3.2. (Pull #1609, Issue #1605)\n\n\n1.25.2 (2019-04-28)\n-------------------\n\n* Change ``is_ipaddress`` to not detect IPvFuture addresses. (Pull #1583)\n\n* Change ``parse_url`` to percent-encode invalid characters within the\n  path, query, and target components. (Pull #1586)\n\n\n1.25.1 (2019-04-24)\n-------------------\n\n* Add support for Google's ``Brotli`` package. (Pull #1572, Pull #1579)\n\n* Upgrade bundled rfc3986 to v1.3.1 (Pull #1578)\n\n\n1.25 (2019-04-22)\n-----------------\n\n* Require and validate certificates by default when using HTTPS (Pull #1507)\n\n* Upgraded ``urllib3.utils.parse_url()`` to be RFC 3986 compliant. (Pull #1487)\n\n* Added support for ``key_password`` for ``HTTPSConnectionPool`` to use\n  encrypted ``key_file`` without creating your own ``SSLContext`` object. (Pull #1489)\n\n* Add TLSv1.3 support to CPython, pyOpenSSL, and SecureTransport ``SSLContext``\n  implementations. (Pull #1496)\n\n* Switched the default multipart header encoder from RFC 2231 to HTML 5 working draft. (Issue #303, PR #1492)\n\n* Fixed issue where OpenSSL would block if an encrypted client private key was\n  given and no password was given. Instead an ``SSLError`` is raised. (Pull #1489)\n\n* Added support for Brotli content encoding. It is enabled automatically if\n  ``brotlipy`` package is installed which can be requested with\n  ``urllib3[brotli]`` extra. (Pull #1532)\n\n* Drop ciphers using DSS key exchange from default TLS cipher suites.\n  Improve default ciphers when using SecureTransport. (Pull #1496)\n\n* Implemented a more efficient ``HTTPResponse.__iter__()`` method. (Issue #1483)\n\n1.24.3 (2019-05-01)\n-------------------\n\n* Apply fix for CVE-2019-9740. (Pull #1591)\n\n1.24.2 (2019-04-17)\n-------------------\n\n* Don't load system certificates by default when any other ``ca_certs``, ``ca_certs_dir`` or\n  ``ssl_context`` parameters are specified.\n\n* Remove Authorization header regardless of case when redirecting to cross-site. (Issue #1510)\n\n* Add support for IPv6 addresses in subjectAltName section of certificates. (Issue #1269)\n\n\n1.24.1 (2018-11-02)\n-------------------\n\n* Remove quadratic behavior within ``GzipDecoder.decompress()`` (Issue #1467)\n\n* Restored functionality of ``ciphers`` parameter for ``create_urllib3_context()``. (Issue #1462)\n\n\n1.24 (2018-10-16)\n-----------------\n\n* Allow key_server_hostname to be specified when initializing a PoolManager to allow custom SNI to be overridden. (Pull #1449)\n\n* Test against Python 3.7 on AppVeyor. (Pull #1453)\n\n* Early-out ipv6 checks when running on App Engine. (Pull #1450)\n\n* Change ambiguous description of backoff_factor (Pull #1436)\n\n* Add ability to handle multiple Content-Encodings (Issue #1441 and Pull #1442)\n\n* Skip DNS names that can't be idna-decoded when using pyOpenSSL (Issue #1405).\n\n* Add a server_hostname parameter to HTTPSConnection which allows for\n  overriding the SNI hostname sent in the handshake. (Pull #1397)\n\n* Drop support for EOL Python 2.6 (Pull #1429 and Pull #1430)\n\n* Fixed bug where responses with header Content-Type: message/* erroneously\n  raised HeaderParsingError, resulting in a warning being logged. (Pull #1439)\n\n* Move urllib3 to src/urllib3 (Pull #1409)\n\n\n1.23 (2018-06-04)\n-----------------\n\n* Allow providing a list of headers to strip from requests when redirecting\n  to a different host. Defaults to the ``Authorization`` header. Different\n  headers can be set via ``Retry.remove_headers_on_redirect``. (Issue #1316)\n\n* Fix ``util.selectors._fileobj_to_fd`` to accept ``long`` (Issue #1247).\n\n* Dropped Python 3.3 support. (Pull #1242)\n\n* Put the connection back in the pool when calling stream() or read_chunked() on\n  a chunked HEAD response. (Issue #1234)\n\n* Fixed pyOpenSSL-specific ssl client authentication issue when clients\n  attempted to auth via certificate + chain (Issue #1060)\n\n* Add the port to the connectionpool connect print (Pull #1251)\n\n* Don't use the ``uuid`` module to create multipart data boundaries. (Pull #1380)\n\n* ``read_chunked()`` on a closed response returns no chunks. (Issue #1088)\n\n* Add Python 2.6 support to ``contrib.securetransport`` (Pull #1359)\n\n* Added support for auth info in url for SOCKS proxy (Pull #1363)\n\n\n1.22 (2017-07-20)\n-----------------\n\n* Fixed missing brackets in ``HTTP CONNECT`` when connecting to IPv6 address via\n  IPv6 proxy. (Issue #1222)\n\n* Made the connection pool retry on ``SSLError``.  The original ``SSLError``\n  is available on ``MaxRetryError.reason``. (Issue #1112)\n\n* Drain and release connection before recursing on retry/redirect.  Fixes\n  deadlocks with a blocking connectionpool. (Issue #1167)\n\n* Fixed compatibility for cookiejar. (Issue #1229)\n\n* pyopenssl: Use vendored version of ``six``. (Issue #1231)\n\n\n1.21.1 (2017-05-02)\n-------------------\n\n* Fixed SecureTransport issue that would cause long delays in response body\n  delivery. (Pull #1154)\n\n* Fixed regression in 1.21 that threw exceptions when users passed the\n  ``socket_options`` flag to the ``PoolManager``.  (Issue #1165)\n\n* Fixed regression in 1.21 that threw exceptions when users passed the\n  ``assert_hostname`` or ``assert_fingerprint`` flag to the ``PoolManager``.\n  (Pull #1157)\n\n\n1.21 (2017-04-25)\n-----------------\n\n* Improved performance of certain selector system calls on Python 3.5 and\n  later. (Pull #1095)\n\n* Resolved issue where the PyOpenSSL backend would not wrap SysCallError\n  exceptions appropriately when sending data. (Pull #1125)\n\n* Selectors now detects a monkey-patched select module after import for modules\n  that patch the select module like eventlet, greenlet. (Pull #1128)\n\n* Reduced memory consumption when streaming zlib-compressed responses\n  (as opposed to raw deflate streams). (Pull #1129)\n\n* Connection pools now use the entire request context when constructing the\n  pool key. (Pull #1016)\n\n* ``PoolManager.connection_from_*`` methods now accept a new keyword argument,\n  ``pool_kwargs``, which are merged with the existing ``connection_pool_kw``.\n  (Pull #1016)\n\n* Add retry counter for ``status_forcelist``. (Issue #1147)\n\n* Added ``contrib`` module for using SecureTransport on macOS:\n  ``urllib3.contrib.securetransport``.  (Pull #1122)\n\n* urllib3 now only normalizes the case of ``http://`` and ``https://`` schemes:\n  for schemes it does not recognise, it assumes they are case-sensitive and\n  leaves them unchanged.\n  (Issue #1080)\n\n\n1.20 (2017-01-19)\n-----------------\n\n* Added support for waiting for I/O using selectors other than select,\n  improving urllib3's behaviour with large numbers of concurrent connections.\n  (Pull #1001)\n\n* Updated the date for the system clock check. (Issue #1005)\n\n* ConnectionPools now correctly consider hostnames to be case-insensitive.\n  (Issue #1032)\n\n* Outdated versions of PyOpenSSL now cause the PyOpenSSL contrib module\n  to fail when it is injected, rather than at first use. (Pull #1063)\n\n* Outdated versions of cryptography now cause the PyOpenSSL contrib module\n  to fail when it is injected, rather than at first use. (Issue #1044)\n\n* Automatically attempt to rewind a file-like body object when a request is\n  retried or redirected. (Pull #1039)\n\n* Fix some bugs that occur when modules incautiously patch the queue module.\n  (Pull #1061)\n\n* Prevent retries from occurring on read timeouts for which the request method\n  was not in the method whitelist. (Issue #1059)\n\n* Changed the PyOpenSSL contrib module to lazily load idna to avoid\n  unnecessarily bloating the memory of programs that don't need it. (Pull\n  #1076)\n\n* Add support for IPv6 literals with zone identifiers. (Pull #1013)\n\n* Added support for socks5h:// and socks4a:// schemes when working with SOCKS\n  proxies, and controlled remote DNS appropriately. (Issue #1035)\n\n\n1.19.1 (2016-11-16)\n-------------------\n\n* Fixed AppEngine import that didn't function on Python 3.5. (Pull #1025)\n\n\n1.19 (2016-11-03)\n-----------------\n\n* urllib3 now respects Retry-After headers on 413, 429, and 503 responses when\n  using the default retry logic. (Pull #955)\n\n* Remove markers from setup.py to assist ancient setuptools versions. (Issue\n  #986)\n\n* Disallow superscripts and other integerish things in URL ports. (Issue #989)\n\n* Allow urllib3's HTTPResponse.stream() method to continue to work with\n  non-httplib underlying FPs. (Pull #990)\n\n* Empty filenames in multipart headers are now emitted as such, rather than\n  being suppressed. (Issue #1015)\n\n* Prefer user-supplied Host headers on chunked uploads. (Issue #1009)\n\n\n1.18.1 (2016-10-27)\n-------------------\n\n* CVE-2016-9015. Users who are using urllib3 version 1.17 or 1.18 along with\n  PyOpenSSL injection and OpenSSL 1.1.0 *must* upgrade to this version. This\n  release fixes a vulnerability whereby urllib3 in the above configuration\n  would silently fail to validate TLS certificates due to erroneously setting\n  invalid flags in OpenSSL's ``SSL_CTX_set_verify`` function. These erroneous\n  flags do not cause a problem in OpenSSL versions before 1.1.0, which\n  interprets the presence of any flag as requesting certificate validation.\n\n  There is no PR for this patch, as it was prepared for simultaneous disclosure\n  and release. The master branch received the same fix in PR #1010.\n\n\n1.18 (2016-09-26)\n-----------------\n\n* Fixed incorrect message for IncompleteRead exception. (PR #973)\n\n* Accept ``iPAddress`` subject alternative name fields in TLS certificates.\n  (Issue #258)\n\n* Fixed consistency of ``HTTPResponse.closed`` between Python 2 and 3.\n  (Issue #977)\n\n* Fixed handling of wildcard certificates when using PyOpenSSL. (Issue #979)\n\n\n1.17 (2016-09-06)\n-----------------\n\n* Accept ``SSLContext`` objects for use in SSL/TLS negotiation. (Issue #835)\n\n* ConnectionPool debug log now includes scheme, host, and port. (Issue #897)\n\n* Substantially refactored documentation. (Issue #887)\n\n* Used URLFetch default timeout on AppEngine, rather than hardcoding our own.\n  (Issue #858)\n\n* Normalize the scheme and host in the URL parser (Issue #833)\n\n* ``HTTPResponse`` contains the last ``Retry`` object, which now also\n  contains retries history. (Issue #848)\n\n* Timeout can no longer be set as boolean, and must be greater than zero.\n  (PR #924)\n\n* Removed pyasn1 and ndg-httpsclient from dependencies used for PyOpenSSL. We\n  now use cryptography and idna, both of which are already dependencies of\n  PyOpenSSL. (PR #930)\n\n* Fixed infinite loop in ``stream`` when amt=None. (Issue #928)\n\n* Try to use the operating system's certificates when we are using an\n  ``SSLContext``. (PR #941)\n\n* Updated cipher suite list to allow ChaCha20+Poly1305. AES-GCM is preferred to\n  ChaCha20, but ChaCha20 is then preferred to everything else. (PR #947)\n\n* Updated cipher suite list to remove 3DES-based cipher suites. (PR #958)\n\n* Removed the cipher suite fallback to allow HIGH ciphers. (PR #958)\n\n* Implemented ``length_remaining`` to determine remaining content\n  to be read. (PR #949)\n\n* Implemented ``enforce_content_length`` to enable exceptions when\n  incomplete data chunks are received. (PR #949)\n\n* Dropped connection start, dropped connection reset, redirect, forced retry,\n  and new HTTPS connection log levels to DEBUG, from INFO. (PR #967)\n\n\n1.16 (2016-06-11)\n-----------------\n\n* Disable IPv6 DNS when IPv6 connections are not possible. (Issue #840)\n\n* Provide ``key_fn_by_scheme`` pool keying mechanism that can be\n  overridden. (Issue #830)\n\n* Normalize scheme and host to lowercase for pool keys, and include\n  ``source_address``. (Issue #830)\n\n* Cleaner exception chain in Python 3 for ``_make_request``.\n  (Issue #861)\n\n* Fixed installing ``urllib3[socks]`` extra. (Issue #864)\n\n* Fixed signature of ``ConnectionPool.close`` so it can actually safely be\n  called by subclasses. (Issue #873)\n\n* Retain ``release_conn`` state across retries. (Issues #651, #866)\n\n* Add customizable ``HTTPConnectionPool.ResponseCls``, which defaults to\n  ``HTTPResponse`` but can be replaced with a subclass. (Issue #879)\n\n\n1.15.1 (2016-04-11)\n-------------------\n\n* Fix packaging to include backports module. (Issue #841)\n\n\n1.15 (2016-04-06)\n-----------------\n\n* Added Retry(raise_on_status=False). (Issue #720)\n\n* Always use setuptools, no more distutils fallback. (Issue #785)\n\n* Dropped support for Python 3.2. (Issue #786)\n\n* Chunked transfer encoding when requesting with ``chunked=True``.\n  (Issue #790)\n\n* Fixed regression with IPv6 port parsing. (Issue #801)\n\n* Append SNIMissingWarning messages to allow users to specify it in\n  the PYTHONWARNINGS environment variable. (Issue #816)\n\n* Handle unicode headers in Py2. (Issue #818)\n\n* Log certificate when there is a hostname mismatch. (Issue #820)\n\n* Preserve order of request/response headers. (Issue #821)\n\n\n1.14 (2015-12-29)\n-----------------\n\n* contrib: SOCKS proxy support! (Issue #762)\n\n* Fixed AppEngine handling of transfer-encoding header and bug\n  in Timeout defaults checking. (Issue #763)\n\n\n1.13.1 (2015-12-18)\n-------------------\n\n* Fixed regression in IPv6 + SSL for match_hostname. (Issue #761)\n\n\n1.13 (2015-12-14)\n-----------------\n\n* Fixed ``pip install urllib3[secure]`` on modern pip. (Issue #706)\n\n* pyopenssl: Fixed SSL3_WRITE_PENDING error. (Issue #717)\n\n* pyopenssl: Support for TLSv1.1 and TLSv1.2. (Issue #696)\n\n* Close connections more defensively on exception. (Issue #734)\n\n* Adjusted ``read_chunked`` to handle gzipped, chunk-encoded bodies without\n  repeatedly flushing the decoder, to function better on Jython. (Issue #743)\n\n* Accept ``ca_cert_dir`` for SSL-related PoolManager configuration. (Issue #758)\n\n\n1.12 (2015-09-03)\n-----------------\n\n* Rely on ``six`` for importing ``httplib`` to work around\n  conflicts with other Python 3 shims. (Issue #688)\n\n* Add support for directories of certificate authorities, as supported by\n  OpenSSL. (Issue #701)\n\n* New exception: ``NewConnectionError``, raised when we fail to establish\n  a new connection, usually ``ECONNREFUSED`` socket error.\n\n\n1.11 (2015-07-21)\n-----------------\n\n* When ``ca_certs`` is given, ``cert_reqs`` defaults to\n  ``'CERT_REQUIRED'``. (Issue #650)\n\n* ``pip install urllib3[secure]`` will install Certifi and\n  PyOpenSSL as dependencies. (Issue #678)\n\n* Made ``HTTPHeaderDict`` usable as a ``headers`` input value\n  (Issues #632, #679)\n\n* Added `urllib3.contrib.appengine <https://urllib3.readthedocs.io/en/latest/contrib.html#google-app-engine>`_\n  which has an ``AppEngineManager`` for using ``URLFetch`` in a\n  Google AppEngine environment. (Issue #664)\n\n* Dev: Added test suite for AppEngine. (Issue #631)\n\n* Fix performance regression when using PyOpenSSL. (Issue #626)\n\n* Passing incorrect scheme (e.g. ``foo://``) will raise\n  ``ValueError`` instead of ``AssertionError`` (backwards\n  compatible for now, but please migrate). (Issue #640)\n\n* Fix pools not getting replenished when an error occurs during a\n  request using ``release_conn=False``. (Issue #644)\n\n* Fix pool-default headers not applying for url-encoded requests\n  like GET. (Issue #657)\n\n* log.warning in Python 3 when headers are skipped due to parsing\n  errors. (Issue #642)\n\n* Close and discard connections if an error occurs during read.\n  (Issue #660)\n\n* Fix host parsing for IPv6 proxies. (Issue #668)\n\n* Separate warning type SubjectAltNameWarning, now issued once\n  per host. (Issue #671)\n\n* Fix ``httplib.IncompleteRead`` not getting converted to\n  ``ProtocolError`` when using ``HTTPResponse.stream()``\n  (Issue #674)\n\n1.10.4 (2015-05-03)\n-------------------\n\n* Migrate tests to Tornado 4. (Issue #594)\n\n* Append default warning configuration rather than overwrite.\n  (Issue #603)\n\n* Fix streaming decoding regression. (Issue #595)\n\n* Fix chunked requests losing state across keep-alive connections.\n  (Issue #599)\n\n* Fix hanging when chunked HEAD response has no body. (Issue #605)\n\n\n1.10.3 (2015-04-21)\n-------------------\n\n* Emit ``InsecurePlatformWarning`` when SSLContext object is missing.\n  (Issue #558)\n\n* Fix regression of duplicate header keys being discarded.\n  (Issue #563)\n\n* ``Response.stream()`` returns a generator for chunked responses.\n  (Issue #560)\n\n* Set upper-bound timeout when waiting for a socket in PyOpenSSL.\n  (Issue #585)\n\n* Work on platforms without `ssl` module for plain HTTP requests.\n  (Issue #587)\n\n* Stop relying on the stdlib's default cipher list. (Issue #588)\n\n\n1.10.2 (2015-02-25)\n-------------------\n\n* Fix file descriptor leakage on retries. (Issue #548)\n\n* Removed RC4 from default cipher list. (Issue #551)\n\n* Header performance improvements. (Issue #544)\n\n* Fix PoolManager not obeying redirect retry settings. (Issue #553)\n\n\n1.10.1 (2015-02-10)\n-------------------\n\n* Pools can be used as context managers. (Issue #545)\n\n* Don't re-use connections which experienced an SSLError. (Issue #529)\n\n* Don't fail when gzip decoding an empty stream. (Issue #535)\n\n* Add sha256 support for fingerprint verification. (Issue #540)\n\n* Fixed handling of header values containing commas. (Issue #533)\n\n\n1.10 (2014-12-14)\n-----------------\n\n* Disabled SSLv3. (Issue #473)\n\n* Add ``Url.url`` property to return the composed url string. (Issue #394)\n\n* Fixed PyOpenSSL + gevent ``WantWriteError``. (Issue #412)\n\n* ``MaxRetryError.reason`` will always be an exception, not string.\n  (Issue #481)\n\n* Fixed SSL-related timeouts not being detected as timeouts. (Issue #492)\n\n* Py3: Use ``ssl.create_default_context()`` when available. (Issue #473)\n\n* Emit ``InsecureRequestWarning`` for *every* insecure HTTPS request.\n  (Issue #496)\n\n* Emit ``SecurityWarning`` when certificate has no ``subjectAltName``.\n  (Issue #499)\n\n* Close and discard sockets which experienced SSL-related errors.\n  (Issue #501)\n\n* Handle ``body`` param in ``.request(...)``. (Issue #513)\n\n* Respect timeout with HTTPS proxy. (Issue #505)\n\n* PyOpenSSL: Handle ZeroReturnError exception. (Issue #520)\n\n\n1.9.1 (2014-09-13)\n------------------\n\n* Apply socket arguments before binding. (Issue #427)\n\n* More careful checks if fp-like object is closed. (Issue #435)\n\n* Fixed packaging issues of some development-related files not\n  getting included. (Issue #440)\n\n* Allow performing *only* fingerprint verification. (Issue #444)\n\n* Emit ``SecurityWarning`` if system clock is waaay off. (Issue #445)\n\n* Fixed PyOpenSSL compatibility with PyPy. (Issue #450)\n\n* Fixed ``BrokenPipeError`` and ``ConnectionError`` handling in Py3.\n  (Issue #443)\n\n\n\n1.9 (2014-07-04)\n----------------\n\n* Shuffled around development-related files. If you're maintaining a distro\n  package of urllib3, you may need to tweak things. (Issue #415)\n\n* Unverified HTTPS requests will trigger a warning on the first request. See\n  our new `security documentation\n  <https://urllib3.readthedocs.io/en/latest/security.html>`_ for details.\n  (Issue #426)\n\n* New retry logic and ``urllib3.util.retry.Retry`` configuration object.\n  (Issue #326)\n\n* All raised exceptions should now wrapped in a\n  ``urllib3.exceptions.HTTPException``-extending exception. (Issue #326)\n\n* All errors during a retry-enabled request should be wrapped in\n  ``urllib3.exceptions.MaxRetryError``, including timeout-related exceptions\n  which were previously exempt. Underlying error is accessible from the\n  ``.reason`` property. (Issue #326)\n\n* ``urllib3.exceptions.ConnectionError`` renamed to\n  ``urllib3.exceptions.ProtocolError``. (Issue #326)\n\n* Errors during response read (such as IncompleteRead) are now wrapped in\n  ``urllib3.exceptions.ProtocolError``. (Issue #418)\n\n* Requesting an empty host will raise ``urllib3.exceptions.LocationValueError``.\n  (Issue #417)\n\n* Catch read timeouts over SSL connections as\n  ``urllib3.exceptions.ReadTimeoutError``. (Issue #419)\n\n* Apply socket arguments before connecting. (Issue #427)\n\n\n1.8.3 (2014-06-23)\n------------------\n\n* Fix TLS verification when using a proxy in Python 3.4.1. (Issue #385)\n\n* Add ``disable_cache`` option to ``urllib3.util.make_headers``. (Issue #393)\n\n* Wrap ``socket.timeout`` exception with\n  ``urllib3.exceptions.ReadTimeoutError``. (Issue #399)\n\n* Fixed proxy-related bug where connections were being reused incorrectly.\n  (Issues #366, #369)\n\n* Added ``socket_options`` keyword parameter which allows to define\n  ``setsockopt`` configuration of new sockets. (Issue #397)\n\n* Removed ``HTTPConnection.tcp_nodelay`` in favor of\n  ``HTTPConnection.default_socket_options``. (Issue #397)\n\n* Fixed ``TypeError`` bug in Python 2.6.4. (Issue #411)\n\n\n1.8.2 (2014-04-17)\n------------------\n\n* Fix ``urllib3.util`` not being included in the package.\n\n\n1.8.1 (2014-04-17)\n------------------\n\n* Fix AppEngine bug of HTTPS requests going out as HTTP. (Issue #356)\n\n* Don't install ``dummyserver`` into ``site-packages`` as it's only needed\n  for the test suite. (Issue #362)\n\n* Added support for specifying ``source_address``. (Issue #352)\n\n\n1.8 (2014-03-04)\n----------------\n\n* Improved url parsing in ``urllib3.util.parse_url`` (properly parse '@' in\n  username, and blank ports like 'hostname:').\n\n* New ``urllib3.connection`` module which contains all the HTTPConnection\n  objects.\n\n* Several ``urllib3.util.Timeout``-related fixes. Also changed constructor\n  signature to a more sensible order. [Backwards incompatible]\n  (Issues #252, #262, #263)\n\n* Use ``backports.ssl_match_hostname`` if it's installed. (Issue #274)\n\n* Added ``.tell()`` method to ``urllib3.response.HTTPResponse`` which\n  returns the number of bytes read so far. (Issue #277)\n\n* Support for platforms without threading. (Issue #289)\n\n* Expand default-port comparison in ``HTTPConnectionPool.is_same_host``\n  to allow a pool with no specified port to be considered equal to to an\n  HTTP/HTTPS url with port 80/443 explicitly provided. (Issue #305)\n\n* Improved default SSL/TLS settings to avoid vulnerabilities.\n  (Issue #309)\n\n* Fixed ``urllib3.poolmanager.ProxyManager`` not retrying on connect errors.\n  (Issue #310)\n\n* Disable Nagle's Algorithm on the socket for non-proxies. A subset of requests\n  will send the entire HTTP request ~200 milliseconds faster; however, some of\n  the resulting TCP packets will be smaller. (Issue #254)\n\n* Increased maximum number of SubjectAltNames in ``urllib3.contrib.pyopenssl``\n  from the default 64 to 1024 in a single certificate. (Issue #318)\n\n* Headers are now passed and stored as a custom\n  ``urllib3.collections_.HTTPHeaderDict`` object rather than a plain ``dict``.\n  (Issue #329, #333)\n\n* Headers no longer lose their case on Python 3. (Issue #236)\n\n* ``urllib3.contrib.pyopenssl`` now uses the operating system's default CA\n  certificates on inject. (Issue #332)\n\n* Requests with ``retries=False`` will immediately raise any exceptions without\n  wrapping them in ``MaxRetryError``. (Issue #348)\n\n* Fixed open socket leak with SSL-related failures. (Issue #344, #348)\n\n\n1.7.1 (2013-09-25)\n------------------\n\n* Added granular timeout support with new ``urllib3.util.Timeout`` class.\n  (Issue #231)\n\n* Fixed Python 3.4 support. (Issue #238)\n\n\n1.7 (2013-08-14)\n----------------\n\n* More exceptions are now pickle-able, with tests. (Issue #174)\n\n* Fixed redirecting with relative URLs in Location header. (Issue #178)\n\n* Support for relative urls in ``Location: ...`` header. (Issue #179)\n\n* ``urllib3.response.HTTPResponse`` now inherits from ``io.IOBase`` for bonus\n  file-like functionality. (Issue #187)\n\n* Passing ``assert_hostname=False`` when creating a HTTPSConnectionPool will\n  skip hostname verification for SSL connections. (Issue #194)\n\n* New method ``urllib3.response.HTTPResponse.stream(...)`` which acts as a\n  generator wrapped around ``.read(...)``. (Issue #198)\n\n* IPv6 url parsing enforces brackets around the hostname. (Issue #199)\n\n* Fixed thread race condition in\n  ``urllib3.poolmanager.PoolManager.connection_from_host(...)`` (Issue #204)\n\n* ``ProxyManager`` requests now include non-default port in ``Host: ...``\n  header. (Issue #217)\n\n* Added HTTPS proxy support in ``ProxyManager``. (Issue #170 #139)\n\n* New ``RequestField`` object can be passed to the ``fields=...`` param which\n  can specify headers. (Issue #220)\n\n* Raise ``urllib3.exceptions.ProxyError`` when connecting to proxy fails.\n  (Issue #221)\n\n* Use international headers when posting file names. (Issue #119)\n\n* Improved IPv6 support. (Issue #203)\n\n\n1.6 (2013-04-25)\n----------------\n\n* Contrib: Optional SNI support for Py2 using PyOpenSSL. (Issue #156)\n\n* ``ProxyManager`` automatically adds ``Host: ...`` header if not given.\n\n* Improved SSL-related code. ``cert_req`` now optionally takes a string like\n  \"REQUIRED\" or \"NONE\". Same with ``ssl_version`` takes strings like \"SSLv23\"\n  The string values reflect the suffix of the respective constant variable.\n  (Issue #130)\n\n* Vendored ``socksipy`` now based on Anorov's fork which handles unexpectedly\n  closed proxy connections and larger read buffers. (Issue #135)\n\n* Ensure the connection is closed if no data is received, fixes connection leak\n  on some platforms. (Issue #133)\n\n* Added SNI support for SSL/TLS connections on Py32+. (Issue #89)\n\n* Tests fixed to be compatible with Py26 again. (Issue #125)\n\n* Added ability to choose SSL version by passing an ``ssl.PROTOCOL_*`` constant\n  to the ``ssl_version`` parameter of ``HTTPSConnectionPool``. (Issue #109)\n\n* Allow an explicit content type to be specified when encoding file fields.\n  (Issue #126)\n\n* Exceptions are now pickleable, with tests. (Issue #101)\n\n* Fixed default headers not getting passed in some cases. (Issue #99)\n\n* Treat \"content-encoding\" header value as case-insensitive, per RFC 2616\n  Section 3.5. (Issue #110)\n\n* \"Connection Refused\" SocketErrors will get retried rather than raised.\n  (Issue #92)\n\n* Updated vendored ``six``, no longer overrides the global ``six`` module\n  namespace. (Issue #113)\n\n* ``urllib3.exceptions.MaxRetryError`` contains a ``reason`` property holding\n  the exception that prompted the final retry. If ``reason is None`` then it\n  was due to a redirect. (Issue #92, #114)\n\n* Fixed ``PoolManager.urlopen()`` from not redirecting more than once.\n  (Issue #149)\n\n* Don't assume ``Content-Type: text/plain`` for multi-part encoding parameters\n  that are not files. (Issue #111)\n\n* Pass `strict` param down to ``httplib.HTTPConnection``. (Issue #122)\n\n* Added mechanism to verify SSL certificates by fingerprint (md5, sha1) or\n  against an arbitrary hostname (when connecting by IP or for misconfigured\n  servers). (Issue #140)\n\n* Streaming decompression support. (Issue #159)\n\n\n1.5 (2012-08-02)\n----------------\n\n* Added ``urllib3.add_stderr_logger()`` for quickly enabling STDERR debug\n  logging in urllib3.\n\n* Native full URL parsing (including auth, path, query, fragment) available in\n  ``urllib3.util.parse_url(url)``.\n\n* Built-in redirect will switch method to 'GET' if status code is 303.\n  (Issue #11)\n\n* ``urllib3.PoolManager`` strips the scheme and host before sending the request\n  uri. (Issue #8)\n\n* New ``urllib3.exceptions.DecodeError`` exception for when automatic decoding,\n  based on the Content-Type header, fails.\n\n* Fixed bug with pool depletion and leaking connections (Issue #76). Added\n  explicit connection closing on pool eviction. Added\n  ``urllib3.PoolManager.clear()``.\n\n* 99% -> 100% unit test coverage.\n\n\n1.4 (2012-06-16)\n----------------\n\n* Minor AppEngine-related fixes.\n\n* Switched from ``mimetools.choose_boundary`` to ``uuid.uuid4()``.\n\n* Improved url parsing. (Issue #73)\n\n* IPv6 url support. (Issue #72)\n\n\n1.3 (2012-03-25)\n----------------\n\n* Removed pre-1.0 deprecated API.\n\n* Refactored helpers into a ``urllib3.util`` submodule.\n\n* Fixed multipart encoding to support list-of-tuples for keys with multiple\n  values. (Issue #48)\n\n* Fixed multiple Set-Cookie headers in response not getting merged properly in\n  Python 3. (Issue #53)\n\n* AppEngine support with Py27. (Issue #61)\n\n* Minor ``encode_multipart_formdata`` fixes related to Python 3 strings vs\n  bytes.\n\n\n1.2.2 (2012-02-06)\n------------------\n\n* Fixed packaging bug of not shipping ``test-requirements.txt``. (Issue #47)\n\n\n1.2.1 (2012-02-05)\n------------------\n\n* Fixed another bug related to when ``ssl`` module is not available. (Issue #41)\n\n* Location parsing errors now raise ``urllib3.exceptions.LocationParseError``\n  which inherits from ``ValueError``.\n\n\n1.2 (2012-01-29)\n----------------\n\n* Added Python 3 support (tested on 3.2.2)\n\n* Dropped Python 2.5 support (tested on 2.6.7, 2.7.2)\n\n* Use ``select.poll`` instead of ``select.select`` for platforms that support\n  it.\n\n* Use ``Queue.LifoQueue`` instead of ``Queue.Queue`` for more aggressive\n  connection reusing. Configurable by overriding ``ConnectionPool.QueueCls``.\n\n* Fixed ``ImportError`` during install when ``ssl`` module is not available.\n  (Issue #41)\n\n* Fixed ``PoolManager`` redirects between schemes (such as HTTP -> HTTPS) not\n  completing properly. (Issue #28, uncovered by Issue #10 in v1.1)\n\n* Ported ``dummyserver`` to use ``tornado`` instead of ``webob`` +\n  ``eventlet``. Removed extraneous unsupported dummyserver testing backends.\n  Added socket-level tests.\n\n* More tests. Achievement Unlocked: 99% Coverage.\n\n\n1.1 (2012-01-07)\n----------------\n\n* Refactored ``dummyserver`` to its own root namespace module (used for\n  testing).\n\n* Added hostname verification for ``VerifiedHTTPSConnection`` by vendoring in\n  Py32's ``ssl_match_hostname``. (Issue #25)\n\n* Fixed cross-host HTTP redirects when using ``PoolManager``. (Issue #10)\n\n* Fixed ``decode_content`` being ignored when set through ``urlopen``. (Issue\n  #27)\n\n* Fixed timeout-related bugs. (Issues #17, #23)\n\n\n1.0.2 (2011-11-04)\n------------------\n\n* Fixed typo in ``VerifiedHTTPSConnection`` which would only present as a bug if\n  you're using the object manually. (Thanks pyos)\n\n* Made RecentlyUsedContainer (and consequently PoolManager) more thread-safe by\n  wrapping the access log in a mutex. (Thanks @christer)\n\n* Made RecentlyUsedContainer more dict-like (corrected ``__delitem__`` and\n  ``__getitem__`` behaviour), with tests. Shouldn't affect core urllib3 code.\n\n\n1.0.1 (2011-10-10)\n------------------\n\n* Fixed a bug where the same connection would get returned into the pool twice,\n  causing extraneous \"HttpConnectionPool is full\" log warnings.\n\n\n1.0 (2011-10-08)\n----------------\n\n* Added ``PoolManager`` with LRU expiration of connections (tested and\n  documented).\n* Added ``ProxyManager`` (needs tests, docs, and confirmation that it works\n  with HTTPS proxies).\n* Added optional partial-read support for responses when\n  ``preload_content=False``. You can now make requests and just read the headers\n  without loading the content.\n* Made response decoding optional (default on, same as before).\n* Added optional explicit boundary string for ``encode_multipart_formdata``.\n* Convenience request methods are now inherited from ``RequestMethods``. Old\n  helpers like ``get_url`` and ``post_url`` should be abandoned in favour of\n  the new ``request(method, url, ...)``.\n* Refactored code to be even more decoupled, reusable, and extendable.\n* License header added to ``.py`` files.\n* Embiggened the documentation: Lots of Sphinx-friendly docstrings in the code\n  and docs in ``docs/`` and on https://urllib3.readthedocs.io/.\n* Embettered all the things!\n* Started writing this file.\n\n\n0.4.1 (2011-07-17)\n------------------\n\n* Minor bug fixes, code cleanup.\n\n\n0.4 (2011-03-01)\n----------------\n\n* Better unicode support.\n* Added ``VerifiedHTTPSConnection``.\n* Added ``NTLMConnectionPool`` in contrib.\n* Minor improvements.\n\n\n0.3.1 (2010-07-13)\n------------------\n\n* Added ``assert_host_name`` optional parameter. Now compatible with proxies.\n\n\n0.3 (2009-12-10)\n----------------\n\n* Added HTTPS support.\n* Minor bug fixes.\n* Refactored, broken backwards compatibility with 0.2.\n* API to be treated as stable from this version forward.\n\n\n0.2 (2008-11-17)\n----------------\n\n* Added unit tests.\n* Bug fixes.\n\n\n0.1 (2008-11-16)\n----------------\n\n* First release.\n", "from __future__ import absolute_import\nimport re\nimport datetime\nimport logging\nimport os\nimport socket\nfrom socket import error as SocketError, timeout as SocketTimeout\nimport warnings\nfrom .packages import six\nfrom .packages.six.moves.http_client import HTTPConnection as _HTTPConnection\nfrom .packages.six.moves.http_client import HTTPException  # noqa: F401\n\ntry:  # Compiled with SSL?\n    import ssl\n\n    BaseSSLError = ssl.SSLError\nexcept (ImportError, AttributeError):  # Platform-specific: No SSL.\n    ssl = None\n\n    class BaseSSLError(BaseException):\n        pass\n\n\ntry:\n    # Python 3: not a no-op, we're adding this to the namespace so it can be imported.\n    ConnectionError = ConnectionError\nexcept NameError:\n    # Python 2\n    class ConnectionError(Exception):\n        pass\n\n\nfrom .exceptions import (\n    NewConnectionError,\n    ConnectTimeoutError,\n    SubjectAltNameWarning,\n    SystemTimeWarning,\n)\nfrom .packages.ssl_match_hostname import match_hostname, CertificateError\n\nfrom .util.ssl_ import (\n    resolve_cert_reqs,\n    resolve_ssl_version,\n    assert_fingerprint,\n    create_urllib3_context,\n    ssl_wrap_socket,\n)\n\n\nfrom .util import connection\n\nfrom ._collections import HTTPHeaderDict\n\nlog = logging.getLogger(__name__)\n\nport_by_scheme = {\"http\": 80, \"https\": 443}\n\n# When it comes time to update this value as a part of regular maintenance\n# (ie test_recent_date is failing) update it to ~6 months before the current date.\nRECENT_DATE = datetime.date(2019, 1, 1)\n\n_CONTAINS_CONTROL_CHAR_RE = re.compile(r\"[^-!#$%&'*+.^_`|~0-9a-zA-Z]\")\n\n\nclass DummyConnection(object):\n    \"\"\"Used to detect a failed ConnectionCls import.\"\"\"\n\n    pass\n\n\nclass HTTPConnection(_HTTPConnection, object):\n    \"\"\"\n    Based on httplib.HTTPConnection but provides an extra constructor\n    backwards-compatibility layer between older and newer Pythons.\n\n    Additional keyword parameters are used to configure attributes of the connection.\n    Accepted parameters include:\n\n      - ``strict``: See the documentation on :class:`urllib3.connectionpool.HTTPConnectionPool`\n      - ``source_address``: Set the source address for the current connection.\n      - ``socket_options``: Set specific options on the underlying socket. If not specified, then\n        defaults are loaded from ``HTTPConnection.default_socket_options`` which includes disabling\n        Nagle's algorithm (sets TCP_NODELAY to 1) unless the connection is behind a proxy.\n\n        For example, if you wish to enable TCP Keep Alive in addition to the defaults,\n        you might pass::\n\n            HTTPConnection.default_socket_options + [\n                (socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1),\n            ]\n\n        Or you may want to disable the defaults by passing an empty list (e.g., ``[]``).\n    \"\"\"\n\n    default_port = port_by_scheme[\"http\"]\n\n    #: Disable Nagle's algorithm by default.\n    #: ``[(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)]``\n    default_socket_options = [(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)]\n\n    #: Whether this connection verifies the host's certificate.\n    is_verified = False\n\n    def __init__(self, *args, **kw):\n        if not six.PY2:\n            kw.pop(\"strict\", None)\n\n        # Pre-set source_address.\n        self.source_address = kw.get(\"source_address\")\n\n        #: The socket options provided by the user. If no options are\n        #: provided, we use the default options.\n        self.socket_options = kw.pop(\"socket_options\", self.default_socket_options)\n\n        _HTTPConnection.__init__(self, *args, **kw)\n\n    @property\n    def host(self):\n        \"\"\"\n        Getter method to remove any trailing dots that indicate the hostname is an FQDN.\n\n        In general, SSL certificates don't include the trailing dot indicating a\n        fully-qualified domain name, and thus, they don't validate properly when\n        checked against a domain name that includes the dot. In addition, some\n        servers may not expect to receive the trailing dot when provided.\n\n        However, the hostname with trailing dot is critical to DNS resolution; doing a\n        lookup with the trailing dot will properly only resolve the appropriate FQDN,\n        whereas a lookup without a trailing dot will search the system's search domain\n        list. Thus, it's important to keep the original host around for use only in\n        those cases where it's appropriate (i.e., when doing DNS lookup to establish the\n        actual TCP connection across which we're going to send HTTP requests).\n        \"\"\"\n        return self._dns_host.rstrip(\".\")\n\n    @host.setter\n    def host(self, value):\n        \"\"\"\n        Setter for the `host` property.\n\n        We assume that only urllib3 uses the _dns_host attribute; httplib itself\n        only uses `host`, and it seems reasonable that other libraries follow suit.\n        \"\"\"\n        self._dns_host = value\n\n    def _new_conn(self):\n        \"\"\" Establish a socket connection and set nodelay settings on it.\n\n        :return: New socket connection.\n        \"\"\"\n        extra_kw = {}\n        if self.source_address:\n            extra_kw[\"source_address\"] = self.source_address\n\n        if self.socket_options:\n            extra_kw[\"socket_options\"] = self.socket_options\n\n        try:\n            conn = connection.create_connection(\n                (self._dns_host, self.port), self.timeout, **extra_kw\n            )\n\n        except SocketTimeout:\n            raise ConnectTimeoutError(\n                self,\n                \"Connection to %s timed out. (connect timeout=%s)\"\n                % (self.host, self.timeout),\n            )\n\n        except SocketError as e:\n            raise NewConnectionError(\n                self, \"Failed to establish a new connection: %s\" % e\n            )\n\n        return conn\n\n    def _prepare_conn(self, conn):\n        self.sock = conn\n        # Google App Engine's httplib does not define _tunnel_host\n        if getattr(self, \"_tunnel_host\", None):\n            # TODO: Fix tunnel so it doesn't depend on self.sock state.\n            self._tunnel()\n            # Mark this connection as not reusable\n            self.auto_open = 0\n\n    def connect(self):\n        conn = self._new_conn()\n        self._prepare_conn(conn)\n\n    def putrequest(self, method, url, *args, **kwargs):\n        \"\"\"Send a request to the server\"\"\"\n        match = _CONTAINS_CONTROL_CHAR_RE.search(method)\n        if match:\n            raise ValueError(\n                \"Method cannot contain non-token characters %r (found at least %r)\"\n                % (method, match.group())\n            )\n\n        return _HTTPConnection.putrequest(self, method, url, *args, **kwargs)\n\n    def request_chunked(self, method, url, body=None, headers=None):\n        \"\"\"\n        Alternative to the common request method, which sends the\n        body with chunked encoding and not as one block\n        \"\"\"\n        headers = HTTPHeaderDict(headers if headers is not None else {})\n        skip_accept_encoding = \"accept-encoding\" in headers\n        skip_host = \"host\" in headers\n        self.putrequest(\n            method, url, skip_accept_encoding=skip_accept_encoding, skip_host=skip_host\n        )\n        for header, value in headers.items():\n            self.putheader(header, value)\n        if \"transfer-encoding\" not in headers:\n            self.putheader(\"Transfer-Encoding\", \"chunked\")\n        self.endheaders()\n\n        if body is not None:\n            stringish_types = six.string_types + (bytes,)\n            if isinstance(body, stringish_types):\n                body = (body,)\n            for chunk in body:\n                if not chunk:\n                    continue\n                if not isinstance(chunk, bytes):\n                    chunk = chunk.encode(\"utf8\")\n                len_str = hex(len(chunk))[2:]\n                self.send(len_str.encode(\"utf-8\"))\n                self.send(b\"\\r\\n\")\n                self.send(chunk)\n                self.send(b\"\\r\\n\")\n\n        # After the if clause, to always have a closed body\n        self.send(b\"0\\r\\n\\r\\n\")\n\n\nclass HTTPSConnection(HTTPConnection):\n    default_port = port_by_scheme[\"https\"]\n\n    ssl_version = None\n\n    def __init__(\n        self,\n        host,\n        port=None,\n        key_file=None,\n        cert_file=None,\n        key_password=None,\n        strict=None,\n        timeout=socket._GLOBAL_DEFAULT_TIMEOUT,\n        ssl_context=None,\n        server_hostname=None,\n        **kw\n    ):\n\n        HTTPConnection.__init__(self, host, port, strict=strict, timeout=timeout, **kw)\n\n        self.key_file = key_file\n        self.cert_file = cert_file\n        self.key_password = key_password\n        self.ssl_context = ssl_context\n        self.server_hostname = server_hostname\n\n        # Required property for Google AppEngine 1.9.0 which otherwise causes\n        # HTTPS requests to go out as HTTP. (See Issue #356)\n        self._protocol = \"https\"\n\n\nclass VerifiedHTTPSConnection(HTTPSConnection):\n    \"\"\"\n    Based on httplib.HTTPSConnection but wraps the socket with\n    SSL certification.\n    \"\"\"\n\n    cert_reqs = None\n    ca_certs = None\n    ca_cert_dir = None\n    ssl_version = None\n    assert_fingerprint = None\n\n    def set_cert(\n        self,\n        key_file=None,\n        cert_file=None,\n        cert_reqs=None,\n        key_password=None,\n        ca_certs=None,\n        assert_hostname=None,\n        assert_fingerprint=None,\n        ca_cert_dir=None,\n    ):\n        \"\"\"\n        This method should only be called once, before the connection is used.\n        \"\"\"\n        # If cert_reqs is not provided we'll assume CERT_REQUIRED unless we also\n        # have an SSLContext object in which case we'll use its verify_mode.\n        if cert_reqs is None:\n            if self.ssl_context is not None:\n                cert_reqs = self.ssl_context.verify_mode\n            else:\n                cert_reqs = resolve_cert_reqs(None)\n\n        self.key_file = key_file\n        self.cert_file = cert_file\n        self.cert_reqs = cert_reqs\n        self.key_password = key_password\n        self.assert_hostname = assert_hostname\n        self.assert_fingerprint = assert_fingerprint\n        self.ca_certs = ca_certs and os.path.expanduser(ca_certs)\n        self.ca_cert_dir = ca_cert_dir and os.path.expanduser(ca_cert_dir)\n\n    def connect(self):\n        # Add certificate verification\n        conn = self._new_conn()\n        hostname = self.host\n\n        # Google App Engine's httplib does not define _tunnel_host\n        if getattr(self, \"_tunnel_host\", None):\n            self.sock = conn\n            # Calls self._set_hostport(), so self.host is\n            # self._tunnel_host below.\n            self._tunnel()\n            # Mark this connection as not reusable\n            self.auto_open = 0\n\n            # Override the host with the one we're requesting data from.\n            hostname = self._tunnel_host\n\n        server_hostname = hostname\n        if self.server_hostname is not None:\n            server_hostname = self.server_hostname\n\n        is_time_off = datetime.date.today() < RECENT_DATE\n        if is_time_off:\n            warnings.warn(\n                (\n                    \"System time is way off (before {0}). This will probably \"\n                    \"lead to SSL verification errors\"\n                ).format(RECENT_DATE),\n                SystemTimeWarning,\n            )\n\n        # Wrap socket using verification with the root certs in\n        # trusted_root_certs\n        default_ssl_context = False\n        if self.ssl_context is None:\n            default_ssl_context = True\n            self.ssl_context = create_urllib3_context(\n                ssl_version=resolve_ssl_version(self.ssl_version),\n                cert_reqs=resolve_cert_reqs(self.cert_reqs),\n            )\n\n        context = self.ssl_context\n        context.verify_mode = resolve_cert_reqs(self.cert_reqs)\n\n        # Try to load OS default certs if none are given.\n        # Works well on Windows (requires Python3.4+)\n        if (\n            not self.ca_certs\n            and not self.ca_cert_dir\n            and default_ssl_context\n            and hasattr(context, \"load_default_certs\")\n        ):\n            context.load_default_certs()\n\n        self.sock = ssl_wrap_socket(\n            sock=conn,\n            keyfile=self.key_file,\n            certfile=self.cert_file,\n            key_password=self.key_password,\n            ca_certs=self.ca_certs,\n            ca_cert_dir=self.ca_cert_dir,\n            server_hostname=server_hostname,\n            ssl_context=context,\n        )\n\n        if self.assert_fingerprint:\n            assert_fingerprint(\n                self.sock.getpeercert(binary_form=True), self.assert_fingerprint\n            )\n        elif (\n            context.verify_mode != ssl.CERT_NONE\n            and not getattr(context, \"check_hostname\", False)\n            and self.assert_hostname is not False\n        ):\n            # While urllib3 attempts to always turn off hostname matching from\n            # the TLS library, this cannot always be done. So we check whether\n            # the TLS Library still thinks it's matching hostnames.\n            cert = self.sock.getpeercert()\n            if not cert.get(\"subjectAltName\", ()):\n                warnings.warn(\n                    (\n                        \"Certificate for {0} has no `subjectAltName`, falling back to check for a \"\n                        \"`commonName` for now. This feature is being removed by major browsers and \"\n                        \"deprecated by RFC 2818. (See https://github.com/urllib3/urllib3/issues/497 \"\n                        \"for details.)\".format(hostname)\n                    ),\n                    SubjectAltNameWarning,\n                )\n            _match_hostname(cert, self.assert_hostname or server_hostname)\n\n        self.is_verified = (\n            context.verify_mode == ssl.CERT_REQUIRED\n            or self.assert_fingerprint is not None\n        )\n\n\ndef _match_hostname(cert, asserted_hostname):\n    try:\n        match_hostname(cert, asserted_hostname)\n    except CertificateError as e:\n        log.warning(\n            \"Certificate did not match expected hostname: %s. Certificate: %s\",\n            asserted_hostname,\n            cert,\n        )\n        # Add cert to exception and reraise so client code can inspect\n        # the cert when catching the exception, if they want to\n        e._peer_cert = cert\n        raise\n\n\nif ssl:\n    # Make a copy for testing.\n    UnverifiedHTTPSConnection = HTTPSConnection\n    HTTPSConnection = VerifiedHTTPSConnection\nelse:\n    HTTPSConnection = DummyConnection\n", "import io\nimport logging\nimport socket\nimport sys\nimport time\nimport warnings\nimport pytest\n\nimport mock\n\nfrom .. import TARPIT_HOST, VALID_SOURCE_ADDRESSES, INVALID_SOURCE_ADDRESSES\nfrom ..port_helpers import find_unused_port\nfrom urllib3 import encode_multipart_formdata, HTTPConnectionPool\nfrom urllib3.exceptions import (\n    ConnectTimeoutError,\n    EmptyPoolError,\n    DecodeError,\n    MaxRetryError,\n    ReadTimeoutError,\n    NewConnectionError,\n    UnrewindableBodyError,\n)\nfrom urllib3.packages.six import b, u\nfrom urllib3.packages.six.moves.urllib.parse import urlencode\nfrom urllib3.util.retry import Retry, RequestHistory\nfrom urllib3.util.timeout import Timeout\n\nfrom test import SHORT_TIMEOUT, LONG_TIMEOUT\nfrom dummyserver.testcase import HTTPDummyServerTestCase, SocketDummyServerTestCase\nfrom dummyserver.server import NoIPv6Warning, HAS_IPV6_AND_DNS\n\nfrom threading import Event\n\npytestmark = pytest.mark.flaky\n\nlog = logging.getLogger(\"urllib3.connectionpool\")\nlog.setLevel(logging.NOTSET)\nlog.addHandler(logging.StreamHandler(sys.stdout))\n\n\ndef wait_for_socket(ready_event):\n    ready_event.wait()\n    ready_event.clear()\n\n\nclass TestConnectionPoolTimeouts(SocketDummyServerTestCase):\n    def test_timeout_float(self):\n        block_event = Event()\n        ready_event = self.start_basic_handler(block_send=block_event, num=2)\n\n        with HTTPConnectionPool(self.host, self.port, retries=False) as pool:\n            wait_for_socket(ready_event)\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\", timeout=SHORT_TIMEOUT)\n            block_event.set()  # Release block\n\n            # Shouldn't raise this time\n            wait_for_socket(ready_event)\n            block_event.set()  # Pre-release block\n            pool.request(\"GET\", \"/\", timeout=LONG_TIMEOUT)\n\n    def test_conn_closed(self):\n        block_event = Event()\n        self.start_basic_handler(block_send=block_event, num=1)\n\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=SHORT_TIMEOUT, retries=False\n        ) as pool:\n            conn = pool._get_conn()\n            pool._put_conn(conn)\n            try:\n                with pytest.raises(ReadTimeoutError):\n                    pool.urlopen(\"GET\", \"/\")\n                if conn.sock:\n                    with pytest.raises(socket.error):\n                        conn.sock.recv(1024)\n            finally:\n                pool._put_conn(conn)\n\n            block_event.set()\n\n    def test_timeout(self):\n        # Requests should time out when expected\n        block_event = Event()\n        ready_event = self.start_basic_handler(block_send=block_event, num=3)\n\n        # Pool-global timeout\n        short_timeout = Timeout(read=SHORT_TIMEOUT)\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=short_timeout, retries=False\n        ) as pool:\n            wait_for_socket(ready_event)\n            block_event.clear()\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\")\n            block_event.set()  # Release request\n\n        # Request-specific timeouts should raise errors\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=short_timeout, retries=False\n        ) as pool:\n            wait_for_socket(ready_event)\n            now = time.time()\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\", timeout=LONG_TIMEOUT)\n            delta = time.time() - now\n\n            message = \"timeout was pool-level SHORT_TIMEOUT rather than request-level LONG_TIMEOUT\"\n            assert delta >= LONG_TIMEOUT, message\n            block_event.set()  # Release request\n\n            # Timeout passed directly to request should raise a request timeout\n            wait_for_socket(ready_event)\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\", timeout=SHORT_TIMEOUT)\n            block_event.set()  # Release request\n\n    def test_connect_timeout(self):\n        url = \"/\"\n        host, port = TARPIT_HOST, 80\n        timeout = Timeout(connect=SHORT_TIMEOUT)\n\n        # Pool-global timeout\n        with HTTPConnectionPool(host, port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            with pytest.raises(ConnectTimeoutError):\n                pool._make_request(conn, \"GET\", url)\n\n            # Retries\n            retries = Retry(connect=0)\n            with pytest.raises(MaxRetryError):\n                pool.request(\"GET\", url, retries=retries)\n\n        # Request-specific connection timeouts\n        big_timeout = Timeout(read=LONG_TIMEOUT, connect=LONG_TIMEOUT)\n        with HTTPConnectionPool(host, port, timeout=big_timeout, retries=False) as pool:\n            conn = pool._get_conn()\n            with pytest.raises(ConnectTimeoutError):\n                pool._make_request(conn, \"GET\", url, timeout=timeout)\n\n            pool._put_conn(conn)\n            with pytest.raises(ConnectTimeoutError):\n                pool.request(\"GET\", url, timeout=timeout)\n\n    def test_total_applies_connect(self):\n        host, port = TARPIT_HOST, 80\n\n        timeout = Timeout(total=None, connect=SHORT_TIMEOUT)\n        with HTTPConnectionPool(host, port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            try:\n                with pytest.raises(ConnectTimeoutError):\n                    pool._make_request(conn, \"GET\", \"/\")\n            finally:\n                conn.close()\n\n        timeout = Timeout(connect=3, read=5, total=SHORT_TIMEOUT)\n        with HTTPConnectionPool(host, port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            try:\n                with pytest.raises(ConnectTimeoutError):\n                    pool._make_request(conn, \"GET\", \"/\")\n            finally:\n                conn.close()\n\n    def test_total_timeout(self):\n        block_event = Event()\n        ready_event = self.start_basic_handler(block_send=block_event, num=2)\n\n        wait_for_socket(ready_event)\n        # This will get the socket to raise an EAGAIN on the read\n        timeout = Timeout(connect=3, read=SHORT_TIMEOUT)\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=timeout, retries=False\n        ) as pool:\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\")\n\n            block_event.set()\n            wait_for_socket(ready_event)\n            block_event.clear()\n\n        # The connect should succeed and this should hit the read timeout\n        timeout = Timeout(connect=3, read=5, total=SHORT_TIMEOUT)\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=timeout, retries=False\n        ) as pool:\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\")\n\n    def test_create_connection_timeout(self):\n        self.start_basic_handler(block_send=Event(), num=0)  # needed for self.port\n\n        timeout = Timeout(connect=SHORT_TIMEOUT, total=LONG_TIMEOUT)\n        with HTTPConnectionPool(\n            TARPIT_HOST, self.port, timeout=timeout, retries=False\n        ) as pool:\n            conn = pool._new_conn()\n            with pytest.raises(ConnectTimeoutError):\n                conn.connect()\n\n\nclass TestConnectionPool(HTTPDummyServerTestCase):\n    def test_get(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/specific_method\", fields={\"method\": \"GET\"})\n            assert r.status == 200, r.data\n\n    def test_post_url(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"POST\", \"/specific_method\", fields={\"method\": \"POST\"})\n            assert r.status == 200, r.data\n\n    def test_urlopen_put(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.urlopen(\"PUT\", \"/specific_method?method=PUT\")\n            assert r.status == 200, r.data\n\n    def test_wrong_specific_method(self):\n        # To make sure the dummy server is actually returning failed responses\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/specific_method\", fields={\"method\": \"POST\"})\n            assert r.status == 400, r.data\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"POST\", \"/specific_method\", fields={\"method\": \"GET\"})\n            assert r.status == 400, r.data\n\n    def test_upload(self):\n        data = \"I'm in ur multipart form-data, hazing a cheezburgr\"\n        fields = {\n            \"upload_param\": \"filefield\",\n            \"upload_filename\": \"lolcat.txt\",\n            \"upload_size\": len(data),\n            \"filefield\": (\"lolcat.txt\", data),\n        }\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"POST\", \"/upload\", fields=fields)\n            assert r.status == 200, r.data\n\n    def test_one_name_multiple_values(self):\n        fields = [(\"foo\", \"a\"), (\"foo\", \"b\")]\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            # urlencode\n            r = pool.request(\"GET\", \"/echo\", fields=fields)\n            assert r.data == b\"foo=a&foo=b\"\n\n            # multipart\n            r = pool.request(\"POST\", \"/echo\", fields=fields)\n            assert r.data.count(b'name=\"foo\"') == 2\n\n    def test_request_method_body(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            body = b\"hi\"\n            r = pool.request(\"POST\", \"/echo\", body=body)\n            assert r.data == body\n\n            fields = [(\"hi\", \"hello\")]\n            with pytest.raises(TypeError):\n                pool.request(\"POST\", \"/echo\", body=body, fields=fields)\n\n    def test_unicode_upload(self):\n        fieldname = u(\"myfile\")\n        filename = u(\"\\xe2\\x99\\xa5.txt\")\n        data = u(\"\\xe2\\x99\\xa5\").encode(\"utf8\")\n        size = len(data)\n\n        fields = {\n            u(\"upload_param\"): fieldname,\n            u(\"upload_filename\"): filename,\n            u(\"upload_size\"): size,\n            fieldname: (filename, data),\n        }\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"POST\", \"/upload\", fields=fields)\n            assert r.status == 200, r.data\n\n    def test_nagle(self):\n        \"\"\" Test that connections have TCP_NODELAY turned on \"\"\"\n        # This test needs to be here in order to be run. socket.create_connection actually tries\n        # to connect to the host provided so we need a dummyserver to be running.\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            conn = pool._get_conn()\n            try:\n                pool._make_request(conn, \"GET\", \"/\")\n                tcp_nodelay_setting = conn.sock.getsockopt(\n                    socket.IPPROTO_TCP, socket.TCP_NODELAY\n                )\n                assert tcp_nodelay_setting\n            finally:\n                conn.close()\n\n    def test_socket_options(self):\n        \"\"\"Test that connections accept socket options.\"\"\"\n        # This test needs to be here in order to be run. socket.create_connection actually tries to\n        # connect to the host provided so we need a dummyserver to be running.\n        with HTTPConnectionPool(\n            self.host,\n            self.port,\n            socket_options=[(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)],\n        ) as pool:\n            s = pool._new_conn()._new_conn()  # Get the socket\n            try:\n                using_keepalive = (\n                    s.getsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE) > 0\n                )\n                assert using_keepalive\n            finally:\n                s.close()\n\n    def test_disable_default_socket_options(self):\n        \"\"\"Test that passing None disables all socket options.\"\"\"\n        # This test needs to be here in order to be run. socket.create_connection actually tries\n        # to connect to the host provided so we need a dummyserver to be running.\n        with HTTPConnectionPool(self.host, self.port, socket_options=None) as pool:\n            s = pool._new_conn()._new_conn()\n            try:\n                using_nagle = s.getsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY) == 0\n                assert using_nagle\n            finally:\n                s.close()\n\n    def test_defaults_are_applied(self):\n        \"\"\"Test that modifying the default socket options works.\"\"\"\n        # This test needs to be here in order to be run. socket.create_connection actually tries\n        # to connect to the host provided so we need a dummyserver to be running.\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            # Get the HTTPConnection instance\n            conn = pool._new_conn()\n            try:\n                # Update the default socket options\n                conn.default_socket_options += [\n                    (socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)\n                ]\n                s = conn._new_conn()\n                nagle_disabled = (\n                    s.getsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY) > 0\n                )\n                using_keepalive = (\n                    s.getsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE) > 0\n                )\n                assert nagle_disabled\n                assert using_keepalive\n            finally:\n                conn.close()\n                s.close()\n\n    def test_connection_error_retries(self):\n        \"\"\" ECONNREFUSED error should raise a connection error, with retries \"\"\"\n        port = find_unused_port()\n        with HTTPConnectionPool(self.host, port) as pool:\n            with pytest.raises(MaxRetryError) as e:\n                pool.request(\"GET\", \"/\", retries=Retry(connect=3))\n            assert type(e.value.reason) == NewConnectionError\n\n    def test_timeout_success(self):\n        timeout = Timeout(connect=3, read=5, total=None)\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            pool.request(\"GET\", \"/\")\n            # This should not raise a \"Timeout already started\" error\n            pool.request(\"GET\", \"/\")\n\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            # This should also not raise a \"Timeout already started\" error\n            pool.request(\"GET\", \"/\")\n\n        timeout = Timeout(total=None)\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            pool.request(\"GET\", \"/\")\n\n    def test_tunnel(self):\n        # note the actual httplib.py has no tests for this functionality\n        timeout = Timeout(total=None)\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            try:\n                conn.set_tunnel(self.host, self.port)\n                conn._tunnel = mock.Mock(return_value=None)\n                pool._make_request(conn, \"GET\", \"/\")\n                conn._tunnel.assert_called_once_with()\n            finally:\n                conn.close()\n\n        # test that it's not called when tunnel is not set\n        timeout = Timeout(total=None)\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            try:\n                conn._tunnel = mock.Mock(return_value=None)\n                pool._make_request(conn, \"GET\", \"/\")\n                assert not conn._tunnel.called\n            finally:\n                conn.close()\n\n    def test_redirect(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"}, redirect=False)\n            assert r.status == 303\n\n            r = pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"})\n            assert r.status == 200\n            assert r.data == b\"Dummy server!\"\n\n    def test_bad_connect(self):\n        with HTTPConnectionPool(\"badhost.invalid\", self.port) as pool:\n            with pytest.raises(MaxRetryError) as e:\n                pool.request(\"GET\", \"/\", retries=5)\n            assert type(e.value.reason) == NewConnectionError\n\n    def test_keepalive(self):\n        with HTTPConnectionPool(self.host, self.port, block=True, maxsize=1) as pool:\n            r = pool.request(\"GET\", \"/keepalive?close=0\")\n            r = pool.request(\"GET\", \"/keepalive?close=0\")\n\n            assert r.status == 200\n            assert pool.num_connections == 1\n            assert pool.num_requests == 2\n\n    def test_keepalive_close(self):\n        with HTTPConnectionPool(\n            self.host, self.port, block=True, maxsize=1, timeout=2\n        ) as pool:\n            r = pool.request(\n                \"GET\", \"/keepalive?close=1\", retries=0, headers={\"Connection\": \"close\"}\n            )\n\n            assert pool.num_connections == 1\n\n            # The dummyserver will have responded with Connection:close,\n            # and httplib will properly cleanup the socket.\n\n            # We grab the HTTPConnection object straight from the Queue,\n            # because _get_conn() is where the check & reset occurs\n            # pylint: disable-msg=W0212\n            conn = pool.pool.get()\n            assert conn.sock is None\n            pool._put_conn(conn)\n\n            # Now with keep-alive\n            r = pool.request(\n                \"GET\",\n                \"/keepalive?close=0\",\n                retries=0,\n                headers={\"Connection\": \"keep-alive\"},\n            )\n\n            # The dummyserver responded with Connection:keep-alive, the connection\n            # persists.\n            conn = pool.pool.get()\n            assert conn.sock is not None\n            pool._put_conn(conn)\n\n            # Another request asking the server to close the connection. This one\n            # should get cleaned up for the next request.\n            r = pool.request(\n                \"GET\", \"/keepalive?close=1\", retries=0, headers={\"Connection\": \"close\"}\n            )\n\n            assert r.status == 200\n\n            conn = pool.pool.get()\n            assert conn.sock is None\n            pool._put_conn(conn)\n\n            # Next request\n            r = pool.request(\"GET\", \"/keepalive?close=0\")\n\n    def test_post_with_urlencode(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            data = {\"banana\": \"hammock\", \"lol\": \"cat\"}\n            r = pool.request(\"POST\", \"/echo\", fields=data, encode_multipart=False)\n            assert r.data.decode(\"utf-8\") == urlencode(data)\n\n    def test_post_with_multipart(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            data = {\"banana\": \"hammock\", \"lol\": \"cat\"}\n            r = pool.request(\"POST\", \"/echo\", fields=data, encode_multipart=True)\n            body = r.data.split(b\"\\r\\n\")\n\n            encoded_data = encode_multipart_formdata(data)[0]\n            expected_body = encoded_data.split(b\"\\r\\n\")\n\n            # TODO: Get rid of extra parsing stuff when you can specify\n            # a custom boundary to encode_multipart_formdata\n            \"\"\"\n            We need to loop the return lines because a timestamp is attached\n            from within encode_multipart_formdata. When the server echos back\n            the data, it has the timestamp from when the data was encoded, which\n            is not equivalent to when we run encode_multipart_formdata on\n            the data again.\n            \"\"\"\n            for i, line in enumerate(body):\n                if line.startswith(b\"--\"):\n                    continue\n\n                assert body[i] == expected_body[i]\n\n    def test_post_with_multipart__iter__(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            data = {\"hello\": \"world\"}\n            r = pool.request(\n                \"POST\",\n                \"/echo\",\n                fields=data,\n                preload_content=False,\n                multipart_boundary=\"boundary\",\n                encode_multipart=True,\n            )\n\n            chunks = [chunk for chunk in r]\n            assert chunks == [\n                b\"--boundary\\r\\n\",\n                b'Content-Disposition: form-data; name=\"hello\"\\r\\n',\n                b\"\\r\\n\",\n                b\"world\\r\\n\",\n                b\"--boundary--\\r\\n\",\n            ]\n\n    def test_check_gzip(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\", \"/encodingrequest\", headers={\"accept-encoding\": \"gzip\"}\n            )\n            assert r.headers.get(\"content-encoding\") == \"gzip\"\n            assert r.data == b\"hello, world!\"\n\n    def test_check_deflate(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\", \"/encodingrequest\", headers={\"accept-encoding\": \"deflate\"}\n            )\n            assert r.headers.get(\"content-encoding\") == \"deflate\"\n            assert r.data == b\"hello, world!\"\n\n    def test_bad_decode(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            with pytest.raises(DecodeError):\n                pool.request(\n                    \"GET\",\n                    \"/encodingrequest\",\n                    headers={\"accept-encoding\": \"garbage-deflate\"},\n                )\n\n            with pytest.raises(DecodeError):\n                pool.request(\n                    \"GET\",\n                    \"/encodingrequest\",\n                    headers={\"accept-encoding\": \"garbage-gzip\"},\n                )\n\n    def test_connection_count(self):\n        with HTTPConnectionPool(self.host, self.port, maxsize=1) as pool:\n            pool.request(\"GET\", \"/\")\n            pool.request(\"GET\", \"/\")\n            pool.request(\"GET\", \"/\")\n\n            assert pool.num_connections == 1\n            assert pool.num_requests == 3\n\n    def test_connection_count_bigpool(self):\n        with HTTPConnectionPool(self.host, self.port, maxsize=16) as http_pool:\n            http_pool.request(\"GET\", \"/\")\n            http_pool.request(\"GET\", \"/\")\n            http_pool.request(\"GET\", \"/\")\n\n            assert http_pool.num_connections == 1\n            assert http_pool.num_requests == 3\n\n    def test_partial_response(self):\n        with HTTPConnectionPool(self.host, self.port, maxsize=1) as pool:\n            req_data = {\"lol\": \"cat\"}\n            resp_data = urlencode(req_data).encode(\"utf-8\")\n\n            r = pool.request(\"GET\", \"/echo\", fields=req_data, preload_content=False)\n\n            assert r.read(5) == resp_data[:5]\n            assert r.read() == resp_data[5:]\n\n    def test_lazy_load_twice(self):\n        # This test is sad and confusing. Need to figure out what's\n        # going on with partial reads and socket reuse.\n\n        with HTTPConnectionPool(\n            self.host, self.port, block=True, maxsize=1, timeout=2\n        ) as pool:\n            payload_size = 1024 * 2\n            first_chunk = 512\n\n            boundary = \"foo\"\n\n            req_data = {\"count\": \"a\" * payload_size}\n            resp_data = encode_multipart_formdata(req_data, boundary=boundary)[0]\n\n            req2_data = {\"count\": \"b\" * payload_size}\n            resp2_data = encode_multipart_formdata(req2_data, boundary=boundary)[0]\n\n            r1 = pool.request(\n                \"POST\",\n                \"/echo\",\n                fields=req_data,\n                multipart_boundary=boundary,\n                preload_content=False,\n            )\n\n            assert r1.read(first_chunk) == resp_data[:first_chunk]\n\n            try:\n                r2 = pool.request(\n                    \"POST\",\n                    \"/echo\",\n                    fields=req2_data,\n                    multipart_boundary=boundary,\n                    preload_content=False,\n                    pool_timeout=0.001,\n                )\n\n                # This branch should generally bail here, but maybe someday it will\n                # work? Perhaps by some sort of magic. Consider it a TODO.\n\n                assert r2.read(first_chunk) == resp2_data[:first_chunk]\n\n                assert r1.read() == resp_data[first_chunk:]\n                assert r2.read() == resp2_data[first_chunk:]\n                assert pool.num_requests == 2\n\n            except EmptyPoolError:\n                assert r1.read() == resp_data[first_chunk:]\n                assert pool.num_requests == 1\n\n            assert pool.num_connections == 1\n\n    def test_for_double_release(self):\n        MAXSIZE = 5\n\n        # Check default state\n        with HTTPConnectionPool(self.host, self.port, maxsize=MAXSIZE) as pool:\n            assert pool.num_connections == 0\n            assert pool.pool.qsize() == MAXSIZE\n\n            # Make an empty slot for testing\n            pool.pool.get()\n            assert pool.pool.qsize() == MAXSIZE - 1\n\n            # Check state after simple request\n            pool.urlopen(\"GET\", \"/\")\n            assert pool.pool.qsize() == MAXSIZE - 1\n\n            # Check state without release\n            pool.urlopen(\"GET\", \"/\", preload_content=False)\n            assert pool.pool.qsize() == MAXSIZE - 2\n\n            pool.urlopen(\"GET\", \"/\")\n            assert pool.pool.qsize() == MAXSIZE - 2\n\n            # Check state after read\n            pool.urlopen(\"GET\", \"/\").data\n            assert pool.pool.qsize() == MAXSIZE - 2\n\n            pool.urlopen(\"GET\", \"/\")\n            assert pool.pool.qsize() == MAXSIZE - 2\n\n    def test_release_conn_parameter(self):\n        MAXSIZE = 5\n        with HTTPConnectionPool(self.host, self.port, maxsize=MAXSIZE) as pool:\n            assert pool.pool.qsize() == MAXSIZE\n\n            # Make request without releasing connection\n            pool.request(\"GET\", \"/\", release_conn=False, preload_content=False)\n            assert pool.pool.qsize() == MAXSIZE - 1\n\n    def test_dns_error(self):\n        with HTTPConnectionPool(\n            \"thishostdoesnotexist.invalid\", self.port, timeout=0.001\n        ) as pool:\n            with pytest.raises(MaxRetryError):\n                pool.request(\"GET\", \"/test\", retries=2)\n\n    @pytest.mark.parametrize(\"char\", [\" \", \"\\r\", \"\\n\", \"\\x00\"])\n    def test_invalid_method_not_allowed(self, char):\n        with pytest.raises(ValueError):\n            with HTTPConnectionPool(self.host, self.port) as pool:\n                pool.request(\"GET\" + char, \"/\")\n\n    def test_percent_encode_invalid_target_chars(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/echo_params?q=\\r&k=\\n \\n\")\n            assert r.data == b\"[('k', '\\\\n \\\\n'), ('q', '\\\\r')]\"\n\n    def test_source_address(self):\n        for addr, is_ipv6 in VALID_SOURCE_ADDRESSES:\n            if is_ipv6 and not HAS_IPV6_AND_DNS:\n                warnings.warn(\"No IPv6 support: skipping.\", NoIPv6Warning)\n                continue\n            with HTTPConnectionPool(\n                self.host, self.port, source_address=addr, retries=False\n            ) as pool:\n                r = pool.request(\"GET\", \"/source_address\")\n                assert r.data == b(addr[0])\n\n    def test_source_address_error(self):\n        for addr in INVALID_SOURCE_ADDRESSES:\n            with HTTPConnectionPool(\n                self.host, self.port, source_address=addr, retries=False\n            ) as pool:\n                with pytest.raises(NewConnectionError):\n                    pool.request(\"GET\", \"/source_address?{0}\".format(addr))\n\n    def test_stream_keepalive(self):\n        x = 2\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            for _ in range(x):\n                response = pool.request(\n                    \"GET\",\n                    \"/chunked\",\n                    headers={\"Connection\": \"keep-alive\"},\n                    preload_content=False,\n                    retries=False,\n                )\n                for chunk in response.stream():\n                    assert chunk == b\"123\"\n\n            assert pool.num_connections == 1\n            assert pool.num_requests == x\n\n    def test_read_chunked_short_circuit(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            response = pool.request(\"GET\", \"/chunked\", preload_content=False)\n            response.read()\n            with pytest.raises(StopIteration):\n                next(response.read_chunked())\n\n    def test_read_chunked_on_closed_response(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            response = pool.request(\"GET\", \"/chunked\", preload_content=False)\n            response.close()\n            with pytest.raises(StopIteration):\n                next(response.read_chunked())\n\n    def test_chunked_gzip(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            response = pool.request(\n                \"GET\", \"/chunked_gzip\", preload_content=False, decode_content=True\n            )\n\n            assert b\"123\" * 4 == response.read()\n\n    def test_cleanup_on_connection_error(self):\n        \"\"\"\n        Test that connections are recycled to the pool on\n        connection errors where no http response is received.\n        \"\"\"\n        poolsize = 3\n        with HTTPConnectionPool(\n            self.host, self.port, maxsize=poolsize, block=True\n        ) as http:\n            assert http.pool.qsize() == poolsize\n\n            # force a connection error by supplying a non-existent\n            # url. We won't get a response for this  and so the\n            # conn won't be implicitly returned to the pool.\n            with pytest.raises(MaxRetryError):\n                http.request(\n                    \"GET\",\n                    \"/redirect\",\n                    fields={\"target\": \"/\"},\n                    release_conn=False,\n                    retries=0,\n                )\n\n            r = http.request(\n                \"GET\",\n                \"/redirect\",\n                fields={\"target\": \"/\"},\n                release_conn=False,\n                retries=1,\n            )\n            r.release_conn()\n\n            # the pool should still contain poolsize elements\n            assert http.pool.qsize() == http.pool.maxsize\n\n    def test_mixed_case_hostname(self):\n        with HTTPConnectionPool(\"LoCaLhOsT\", self.port) as pool:\n            response = pool.request(\"GET\", \"http://LoCaLhOsT:%d/\" % self.port)\n            assert response.status == 200\n\n\nclass TestRetry(HTTPDummyServerTestCase):\n    def test_max_retry(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            with pytest.raises(MaxRetryError):\n                pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"}, retries=0)\n\n    def test_disabled_retry(self):\n        \"\"\" Disabled retries should disable redirect handling. \"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"}, retries=False)\n            assert r.status == 303\n\n            r = pool.request(\n                \"GET\",\n                \"/redirect\",\n                fields={\"target\": \"/\"},\n                retries=Retry(redirect=False),\n            )\n            assert r.status == 303\n\n        with HTTPConnectionPool(\n            \"thishostdoesnotexist.invalid\", self.port, timeout=0.001\n        ) as pool:\n            with pytest.raises(NewConnectionError):\n                pool.request(\"GET\", \"/test\", retries=False)\n\n    def test_read_retries(self):\n        \"\"\" Should retry for status codes in the whitelist \"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            retry = Retry(read=1, status_forcelist=[418])\n            resp = pool.request(\n                \"GET\",\n                \"/successful_retry\",\n                headers={\"test-name\": \"test_read_retries\"},\n                retries=retry,\n            )\n            assert resp.status == 200\n\n    def test_read_total_retries(self):\n        \"\"\" HTTP response w/ status code in the whitelist should be retried \"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_read_total_retries\"}\n            retry = Retry(total=1, status_forcelist=[418])\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 200\n\n    def test_retries_wrong_whitelist(self):\n        \"\"\"HTTP response w/ status code not in whitelist shouldn't be retried\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            retry = Retry(total=1, status_forcelist=[202])\n            resp = pool.request(\n                \"GET\",\n                \"/successful_retry\",\n                headers={\"test-name\": \"test_wrong_whitelist\"},\n                retries=retry,\n            )\n            assert resp.status == 418\n\n    def test_default_method_whitelist_retried(self):\n        \"\"\" urllib3 should retry methods in the default method whitelist \"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            retry = Retry(total=1, status_forcelist=[418])\n            resp = pool.request(\n                \"OPTIONS\",\n                \"/successful_retry\",\n                headers={\"test-name\": \"test_default_whitelist\"},\n                retries=retry,\n            )\n            assert resp.status == 200\n\n    def test_retries_wrong_method_list(self):\n        \"\"\"Method not in our whitelist should not be retried, even if code matches\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_wrong_method_whitelist\"}\n            retry = Retry(total=1, status_forcelist=[418], method_whitelist=[\"POST\"])\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 418\n\n    def test_read_retries_unsuccessful(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_read_retries_unsuccessful\"}\n            resp = pool.request(\"GET\", \"/successful_retry\", headers=headers, retries=1)\n            assert resp.status == 418\n\n    def test_retry_reuse_safe(self):\n        \"\"\" It should be possible to reuse a Retry object across requests \"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_retry_safe\"}\n            retry = Retry(total=1, status_forcelist=[418])\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 200\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 200\n\n    def test_retry_return_in_response(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_retry_return_in_response\"}\n            retry = Retry(total=2, status_forcelist=[418])\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 200\n            assert resp.retries.total == 1\n            assert resp.retries.history == (\n                RequestHistory(\"GET\", \"/successful_retry\", None, 418, None),\n            )\n\n    def test_retry_redirect_history(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            resp = pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"})\n            assert resp.status == 200\n            assert resp.retries.history == (\n                RequestHistory(\"GET\", \"/redirect?target=%2F\", None, 303, \"/\"),\n            )\n\n    def test_multi_redirect_history(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\",\n                \"/multi_redirect\",\n                fields={\"redirect_codes\": \"303,302,200\"},\n                redirect=False,\n            )\n            assert r.status == 303\n            assert r.retries.history == tuple()\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\",\n                \"/multi_redirect\",\n                retries=10,\n                fields={\"redirect_codes\": \"303,302,301,307,302,200\"},\n            )\n            assert r.status == 200\n            assert r.data == b\"Done redirecting\"\n\n            expected = [\n                (303, \"/multi_redirect?redirect_codes=302,301,307,302,200\"),\n                (302, \"/multi_redirect?redirect_codes=301,307,302,200\"),\n                (301, \"/multi_redirect?redirect_codes=307,302,200\"),\n                (307, \"/multi_redirect?redirect_codes=302,200\"),\n                (302, \"/multi_redirect?redirect_codes=200\"),\n            ]\n            actual = [\n                (history.status, history.redirect_location)\n                for history in r.retries.history\n            ]\n            assert actual == expected\n\n\nclass TestRetryAfter(HTTPDummyServerTestCase):\n    def test_retry_after(self):\n        # Request twice in a second to get a 429 response.\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"429 Too Many Requests\"},\n                retries=False,\n            )\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"429 Too Many Requests\"},\n                retries=False,\n            )\n            assert r.status == 429\n\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"429 Too Many Requests\"},\n                retries=True,\n            )\n            assert r.status == 200\n\n            # Request twice in a second to get a 503 response.\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"503 Service Unavailable\"},\n                retries=False,\n            )\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"503 Service Unavailable\"},\n                retries=False,\n            )\n            assert r.status == 503\n\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"503 Service Unavailable\"},\n                retries=True,\n            )\n            assert r.status == 200\n\n            # Ignore Retry-After header on status which is not defined in\n            # Retry.RETRY_AFTER_STATUS_CODES.\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"418 I'm a teapot\"},\n                retries=True,\n            )\n            assert r.status == 418\n\n    def test_redirect_after(self):\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/redirect_after\", retries=False)\n            assert r.status == 303\n\n            t = time.time()\n            r = pool.request(\"GET\", \"/redirect_after\")\n            assert r.status == 200\n            delta = time.time() - t\n            assert delta >= 1\n\n            t = time.time()\n            timestamp = t + 2\n            r = pool.request(\"GET\", \"/redirect_after?date=\" + str(timestamp))\n            assert r.status == 200\n            delta = time.time() - t\n            assert delta >= 1\n\n            # Retry-After is past\n            t = time.time()\n            timestamp = t - 1\n            r = pool.request(\"GET\", \"/redirect_after?date=\" + str(timestamp))\n            delta = time.time() - t\n            assert r.status == 200\n            assert delta < 1\n\n\nclass TestFileBodiesOnRetryOrRedirect(HTTPDummyServerTestCase):\n    def test_retries_put_filehandle(self):\n        \"\"\"HTTP PUT retry with a file-like object should not timeout\"\"\"\n        with HTTPConnectionPool(self.host, self.port, timeout=0.1) as pool:\n            retry = Retry(total=3, status_forcelist=[418])\n            # httplib reads in 8k chunks; use a larger content length\n            content_length = 65535\n            data = b\"A\" * content_length\n            uploaded_file = io.BytesIO(data)\n            headers = {\n                \"test-name\": \"test_retries_put_filehandle\",\n                \"Content-Length\": str(content_length),\n            }\n            resp = pool.urlopen(\n                \"PUT\",\n                \"/successful_retry\",\n                headers=headers,\n                retries=retry,\n                body=uploaded_file,\n                assert_same_host=False,\n                redirect=False,\n            )\n            assert resp.status == 200\n\n    def test_redirect_put_file(self):\n        \"\"\"PUT with file object should work with a redirection response\"\"\"\n        with HTTPConnectionPool(self.host, self.port, timeout=0.1) as pool:\n            retry = Retry(total=3, status_forcelist=[418])\n            # httplib reads in 8k chunks; use a larger content length\n            content_length = 65535\n            data = b\"A\" * content_length\n            uploaded_file = io.BytesIO(data)\n            headers = {\n                \"test-name\": \"test_redirect_put_file\",\n                \"Content-Length\": str(content_length),\n            }\n            url = \"/redirect?target=/echo&status=307\"\n            resp = pool.urlopen(\n                \"PUT\",\n                url,\n                headers=headers,\n                retries=retry,\n                body=uploaded_file,\n                assert_same_host=False,\n                redirect=True,\n            )\n            assert resp.status == 200\n            assert resp.data == data\n\n    def test_redirect_with_failed_tell(self):\n        \"\"\"Abort request if failed to get a position from tell()\"\"\"\n\n        class BadTellObject(io.BytesIO):\n            def tell(self):\n                raise IOError\n\n        body = BadTellObject(b\"the data\")\n        url = \"/redirect?target=/successful_retry\"\n        # httplib uses fileno if Content-Length isn't supplied,\n        # which is unsupported by BytesIO.\n        headers = {\"Content-Length\": \"8\"}\n        with HTTPConnectionPool(self.host, self.port, timeout=0.1) as pool:\n            with pytest.raises(UnrewindableBodyError) as e:\n                pool.urlopen(\"PUT\", url, headers=headers, body=body)\n            assert \"Unable to record file position for\" in str(e.value)\n\n\nclass TestRetryPoolSize(HTTPDummyServerTestCase):\n    def test_pool_size_retry(self):\n        retries = Retry(total=1, raise_on_status=False, status_forcelist=[404])\n        with HTTPConnectionPool(\n            self.host, self.port, maxsize=10, retries=retries, block=True\n        ) as pool:\n            pool.urlopen(\"GET\", \"/not_found\", preload_content=False)\n            assert pool.num_connections == 1\n\n\nclass TestRedirectPoolSize(HTTPDummyServerTestCase):\n    def test_pool_size_redirect(self):\n        retries = Retry(\n            total=1, raise_on_status=False, status_forcelist=[404], redirect=True\n        )\n        with HTTPConnectionPool(\n            self.host, self.port, maxsize=10, retries=retries, block=True\n        ) as pool:\n            pool.urlopen(\"GET\", \"/redirect\", preload_content=False)\n            assert pool.num_connections == 1\n"], "filenames": ["CHANGES.rst", "src/urllib3/connection.py", "test/with_dummyserver/test_connectionpool.py"], "buggy_code_start_loc": [2, 1, 678], "buggy_code_end_loc": [2, 185, 678], "fixing_code_start_loc": [3, 2, 679], "fixing_code_end_loc": [10, 200, 685], "type": "CWE-74", "message": "urllib3 before 1.25.9 allows CRLF injection if the attacker controls the HTTP request method, as demonstrated by inserting CR and LF control characters in the first argument of putrequest(). NOTE: this is similar to CVE-2020-26116.", "other": {"cve": {"id": "CVE-2020-26137", "sourceIdentifier": "cve@mitre.org", "published": "2020-09-30T18:15:26.773", "lastModified": "2023-01-31T21:36:43.377", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "urllib3 before 1.25.9 allows CRLF injection if the attacker controls the HTTP request method, as demonstrated by inserting CR and LF control characters in the first argument of putrequest(). NOTE: this is similar to CVE-2020-26116."}, {"lang": "es", "value": "urllib3 versiones anteriores a 1.25.9, permite una inyecci\u00f3n de CRLF si el atacante controla el m\u00e9todo de petici\u00f3n HTTP, como es demostrado al insertar caracteres de control CR y LF en el primer argumento de la funci\u00f3n putrequest().&#xa0;NOTA: esto es similar a CVE-2020-26116"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:L/I:L/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "NONE", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 3.9, "impactScore": 2.5}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:P/I:P/A:N", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "NONE", "baseScore": 6.4}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 4.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-74"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:python:urllib3:*:*:*:*:*:*:*:*", "versionEndExcluding": "1.25.9", "matchCriteriaId": "3EF5A26A-E388-4422-933C-6E2028A1C158"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:esm:*:*:*", "matchCriteriaId": "7A5301BF-1402-4BE0-A0F8-69FBE79BC6D6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:lts:*:*:*", "matchCriteriaId": "23A7C53F-B80F-4E6A-AFA9-58EEA84BE11D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:20.04:*:*:*:lts:*:*:*", "matchCriteriaId": "902B8056-9E37-443B-8905-8AA93E2447FB"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:oracle:communications_cloud_native_core_network_function_cloud_native_environment:22.2.0:*:*:*:*:*:*:*", "matchCriteriaId": "4B77BFB7-C105-4A42-A9A4-45EF4EC8556F"}, {"vulnerable": true, "criteria": "cpe:2.3:a:oracle:zfs_storage_appliance_kit:8.8:*:*:*:*:*:*:*", "matchCriteriaId": "D3E503FB-6279-4D4A-91D8-E237ECF9D2B0"}]}]}], "references": [{"url": "https://bugs.python.org/issue39603", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Vendor Advisory"]}, {"url": "https://github.com/urllib3/urllib3/commit/1dd69c5c5982fae7c87a620d487c2ebf7a6b436b", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/urllib3/urllib3/pull/1800", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2021/06/msg00015.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4570-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.oracle.com/security-alerts/cpujul2022.html", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://www.oracle.com/security-alerts/cpuoct2021.html", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/urllib3/urllib3/commit/1dd69c5c5982fae7c87a620d487c2ebf7a6b436b"}}