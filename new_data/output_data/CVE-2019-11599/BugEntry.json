{"buggy_code": ["/*\n * Copyright (c) 2005 Topspin Communications.  All rights reserved.\n * Copyright (c) 2005, 2006 Cisco Systems.  All rights reserved.\n * Copyright (c) 2005 Mellanox Technologies. All rights reserved.\n * Copyright (c) 2005 Voltaire, Inc. All rights reserved.\n * Copyright (c) 2005 PathScale, Inc. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n */\n\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/device.h>\n#include <linux/err.h>\n#include <linux/fs.h>\n#include <linux/poll.h>\n#include <linux/sched.h>\n#include <linux/file.h>\n#include <linux/cdev.h>\n#include <linux/anon_inodes.h>\n#include <linux/slab.h>\n#include <linux/sched/mm.h>\n\n#include <linux/uaccess.h>\n\n#include <rdma/ib.h>\n#include <rdma/uverbs_std_types.h>\n\n#include \"uverbs.h\"\n#include \"core_priv.h\"\n#include \"rdma_core.h\"\n\nMODULE_AUTHOR(\"Roland Dreier\");\nMODULE_DESCRIPTION(\"InfiniBand userspace verbs access\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\n\nenum {\n\tIB_UVERBS_MAJOR       = 231,\n\tIB_UVERBS_BASE_MINOR  = 192,\n\tIB_UVERBS_MAX_DEVICES = RDMA_MAX_PORTS,\n\tIB_UVERBS_NUM_FIXED_MINOR = 32,\n\tIB_UVERBS_NUM_DYNAMIC_MINOR = IB_UVERBS_MAX_DEVICES - IB_UVERBS_NUM_FIXED_MINOR,\n};\n\n#define IB_UVERBS_BASE_DEV\tMKDEV(IB_UVERBS_MAJOR, IB_UVERBS_BASE_MINOR)\n\nstatic dev_t dynamic_uverbs_dev;\nstatic struct class *uverbs_class;\n\nstatic DEFINE_IDA(uverbs_ida);\nstatic void ib_uverbs_add_one(struct ib_device *device);\nstatic void ib_uverbs_remove_one(struct ib_device *device, void *client_data);\n\n/*\n * Must be called with the ufile->device->disassociate_srcu held, and the lock\n * must be held until use of the ucontext is finished.\n */\nstruct ib_ucontext *ib_uverbs_get_ucontext_file(struct ib_uverbs_file *ufile)\n{\n\t/*\n\t * We do not hold the hw_destroy_rwsem lock for this flow, instead\n\t * srcu is used. It does not matter if someone races this with\n\t * get_context, we get NULL or valid ucontext.\n\t */\n\tstruct ib_ucontext *ucontext = smp_load_acquire(&ufile->ucontext);\n\n\tif (!srcu_dereference(ufile->device->ib_dev,\n\t\t\t      &ufile->device->disassociate_srcu))\n\t\treturn ERR_PTR(-EIO);\n\n\tif (!ucontext)\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn ucontext;\n}\nEXPORT_SYMBOL(ib_uverbs_get_ucontext_file);\n\nint uverbs_dealloc_mw(struct ib_mw *mw)\n{\n\tstruct ib_pd *pd = mw->pd;\n\tint ret;\n\n\tret = mw->device->ops.dealloc_mw(mw);\n\tif (!ret)\n\t\tatomic_dec(&pd->usecnt);\n\treturn ret;\n}\n\nstatic void ib_uverbs_release_dev(struct device *device)\n{\n\tstruct ib_uverbs_device *dev =\n\t\t\tcontainer_of(device, struct ib_uverbs_device, dev);\n\n\tuverbs_destroy_api(dev->uapi);\n\tcleanup_srcu_struct(&dev->disassociate_srcu);\n\tkfree(dev);\n}\n\nstatic void ib_uverbs_release_async_event_file(struct kref *ref)\n{\n\tstruct ib_uverbs_async_event_file *file =\n\t\tcontainer_of(ref, struct ib_uverbs_async_event_file, ref);\n\n\tkfree(file);\n}\n\nvoid ib_uverbs_release_ucq(struct ib_uverbs_file *file,\n\t\t\t  struct ib_uverbs_completion_event_file *ev_file,\n\t\t\t  struct ib_ucq_object *uobj)\n{\n\tstruct ib_uverbs_event *evt, *tmp;\n\n\tif (ev_file) {\n\t\tspin_lock_irq(&ev_file->ev_queue.lock);\n\t\tlist_for_each_entry_safe(evt, tmp, &uobj->comp_list, obj_list) {\n\t\t\tlist_del(&evt->list);\n\t\t\tkfree(evt);\n\t\t}\n\t\tspin_unlock_irq(&ev_file->ev_queue.lock);\n\n\t\tuverbs_uobject_put(&ev_file->uobj);\n\t}\n\n\tspin_lock_irq(&file->async_file->ev_queue.lock);\n\tlist_for_each_entry_safe(evt, tmp, &uobj->async_list, obj_list) {\n\t\tlist_del(&evt->list);\n\t\tkfree(evt);\n\t}\n\tspin_unlock_irq(&file->async_file->ev_queue.lock);\n}\n\nvoid ib_uverbs_release_uevent(struct ib_uverbs_file *file,\n\t\t\t      struct ib_uevent_object *uobj)\n{\n\tstruct ib_uverbs_event *evt, *tmp;\n\n\tspin_lock_irq(&file->async_file->ev_queue.lock);\n\tlist_for_each_entry_safe(evt, tmp, &uobj->event_list, obj_list) {\n\t\tlist_del(&evt->list);\n\t\tkfree(evt);\n\t}\n\tspin_unlock_irq(&file->async_file->ev_queue.lock);\n}\n\nvoid ib_uverbs_detach_umcast(struct ib_qp *qp,\n\t\t\t     struct ib_uqp_object *uobj)\n{\n\tstruct ib_uverbs_mcast_entry *mcast, *tmp;\n\n\tlist_for_each_entry_safe(mcast, tmp, &uobj->mcast_list, list) {\n\t\tib_detach_mcast(qp, &mcast->gid, mcast->lid);\n\t\tlist_del(&mcast->list);\n\t\tkfree(mcast);\n\t}\n}\n\nstatic void ib_uverbs_comp_dev(struct ib_uverbs_device *dev)\n{\n\tcomplete(&dev->comp);\n}\n\nvoid ib_uverbs_release_file(struct kref *ref)\n{\n\tstruct ib_uverbs_file *file =\n\t\tcontainer_of(ref, struct ib_uverbs_file, ref);\n\tstruct ib_device *ib_dev;\n\tint srcu_key;\n\n\trelease_ufile_idr_uobject(file);\n\n\tsrcu_key = srcu_read_lock(&file->device->disassociate_srcu);\n\tib_dev = srcu_dereference(file->device->ib_dev,\n\t\t\t\t  &file->device->disassociate_srcu);\n\tif (ib_dev && !ib_dev->ops.disassociate_ucontext)\n\t\tmodule_put(ib_dev->owner);\n\tsrcu_read_unlock(&file->device->disassociate_srcu, srcu_key);\n\n\tif (atomic_dec_and_test(&file->device->refcount))\n\t\tib_uverbs_comp_dev(file->device);\n\n\tif (file->async_file)\n\t\tkref_put(&file->async_file->ref,\n\t\t\t ib_uverbs_release_async_event_file);\n\tput_device(&file->device->dev);\n\tkfree(file);\n}\n\nstatic ssize_t ib_uverbs_event_read(struct ib_uverbs_event_queue *ev_queue,\n\t\t\t\t    struct ib_uverbs_file *uverbs_file,\n\t\t\t\t    struct file *filp, char __user *buf,\n\t\t\t\t    size_t count, loff_t *pos,\n\t\t\t\t    size_t eventsz)\n{\n\tstruct ib_uverbs_event *event;\n\tint ret = 0;\n\n\tspin_lock_irq(&ev_queue->lock);\n\n\twhile (list_empty(&ev_queue->event_list)) {\n\t\tspin_unlock_irq(&ev_queue->lock);\n\n\t\tif (filp->f_flags & O_NONBLOCK)\n\t\t\treturn -EAGAIN;\n\n\t\tif (wait_event_interruptible(ev_queue->poll_wait,\n\t\t\t\t\t     (!list_empty(&ev_queue->event_list) ||\n\t\t\t/* The barriers built into wait_event_interruptible()\n\t\t\t * and wake_up() guarentee this will see the null set\n\t\t\t * without using RCU\n\t\t\t */\n\t\t\t\t\t     !uverbs_file->device->ib_dev)))\n\t\t\treturn -ERESTARTSYS;\n\n\t\t/* If device was disassociated and no event exists set an error */\n\t\tif (list_empty(&ev_queue->event_list) &&\n\t\t    !uverbs_file->device->ib_dev)\n\t\t\treturn -EIO;\n\n\t\tspin_lock_irq(&ev_queue->lock);\n\t}\n\n\tevent = list_entry(ev_queue->event_list.next, struct ib_uverbs_event, list);\n\n\tif (eventsz > count) {\n\t\tret   = -EINVAL;\n\t\tevent = NULL;\n\t} else {\n\t\tlist_del(ev_queue->event_list.next);\n\t\tif (event->counter) {\n\t\t\t++(*event->counter);\n\t\t\tlist_del(&event->obj_list);\n\t\t}\n\t}\n\n\tspin_unlock_irq(&ev_queue->lock);\n\n\tif (event) {\n\t\tif (copy_to_user(buf, event, eventsz))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = eventsz;\n\t}\n\n\tkfree(event);\n\n\treturn ret;\n}\n\nstatic ssize_t ib_uverbs_async_event_read(struct file *filp, char __user *buf,\n\t\t\t\t\t  size_t count, loff_t *pos)\n{\n\tstruct ib_uverbs_async_event_file *file = filp->private_data;\n\n\treturn ib_uverbs_event_read(&file->ev_queue, file->uverbs_file, filp,\n\t\t\t\t    buf, count, pos,\n\t\t\t\t    sizeof(struct ib_uverbs_async_event_desc));\n}\n\nstatic ssize_t ib_uverbs_comp_event_read(struct file *filp, char __user *buf,\n\t\t\t\t\t size_t count, loff_t *pos)\n{\n\tstruct ib_uverbs_completion_event_file *comp_ev_file =\n\t\tfilp->private_data;\n\n\treturn ib_uverbs_event_read(&comp_ev_file->ev_queue,\n\t\t\t\t    comp_ev_file->uobj.ufile, filp,\n\t\t\t\t    buf, count, pos,\n\t\t\t\t    sizeof(struct ib_uverbs_comp_event_desc));\n}\n\nstatic __poll_t ib_uverbs_event_poll(struct ib_uverbs_event_queue *ev_queue,\n\t\t\t\t\t struct file *filp,\n\t\t\t\t\t struct poll_table_struct *wait)\n{\n\t__poll_t pollflags = 0;\n\n\tpoll_wait(filp, &ev_queue->poll_wait, wait);\n\n\tspin_lock_irq(&ev_queue->lock);\n\tif (!list_empty(&ev_queue->event_list))\n\t\tpollflags = EPOLLIN | EPOLLRDNORM;\n\tspin_unlock_irq(&ev_queue->lock);\n\n\treturn pollflags;\n}\n\nstatic __poll_t ib_uverbs_async_event_poll(struct file *filp,\n\t\t\t\t\t       struct poll_table_struct *wait)\n{\n\treturn ib_uverbs_event_poll(filp->private_data, filp, wait);\n}\n\nstatic __poll_t ib_uverbs_comp_event_poll(struct file *filp,\n\t\t\t\t\t      struct poll_table_struct *wait)\n{\n\tstruct ib_uverbs_completion_event_file *comp_ev_file =\n\t\tfilp->private_data;\n\n\treturn ib_uverbs_event_poll(&comp_ev_file->ev_queue, filp, wait);\n}\n\nstatic int ib_uverbs_async_event_fasync(int fd, struct file *filp, int on)\n{\n\tstruct ib_uverbs_event_queue *ev_queue = filp->private_data;\n\n\treturn fasync_helper(fd, filp, on, &ev_queue->async_queue);\n}\n\nstatic int ib_uverbs_comp_event_fasync(int fd, struct file *filp, int on)\n{\n\tstruct ib_uverbs_completion_event_file *comp_ev_file =\n\t\tfilp->private_data;\n\n\treturn fasync_helper(fd, filp, on, &comp_ev_file->ev_queue.async_queue);\n}\n\nstatic int ib_uverbs_async_event_close(struct inode *inode, struct file *filp)\n{\n\tstruct ib_uverbs_async_event_file *file = filp->private_data;\n\tstruct ib_uverbs_file *uverbs_file = file->uverbs_file;\n\tstruct ib_uverbs_event *entry, *tmp;\n\tint closed_already = 0;\n\n\tmutex_lock(&uverbs_file->device->lists_mutex);\n\tspin_lock_irq(&file->ev_queue.lock);\n\tclosed_already = file->ev_queue.is_closed;\n\tfile->ev_queue.is_closed = 1;\n\tlist_for_each_entry_safe(entry, tmp, &file->ev_queue.event_list, list) {\n\t\tif (entry->counter)\n\t\t\tlist_del(&entry->obj_list);\n\t\tkfree(entry);\n\t}\n\tspin_unlock_irq(&file->ev_queue.lock);\n\tif (!closed_already) {\n\t\tlist_del(&file->list);\n\t\tib_unregister_event_handler(&uverbs_file->event_handler);\n\t}\n\tmutex_unlock(&uverbs_file->device->lists_mutex);\n\n\tkref_put(&uverbs_file->ref, ib_uverbs_release_file);\n\tkref_put(&file->ref, ib_uverbs_release_async_event_file);\n\n\treturn 0;\n}\n\nstatic int ib_uverbs_comp_event_close(struct inode *inode, struct file *filp)\n{\n\tstruct ib_uobject *uobj = filp->private_data;\n\tstruct ib_uverbs_completion_event_file *file = container_of(\n\t\tuobj, struct ib_uverbs_completion_event_file, uobj);\n\tstruct ib_uverbs_event *entry, *tmp;\n\n\tspin_lock_irq(&file->ev_queue.lock);\n\tlist_for_each_entry_safe(entry, tmp, &file->ev_queue.event_list, list) {\n\t\tif (entry->counter)\n\t\t\tlist_del(&entry->obj_list);\n\t\tkfree(entry);\n\t}\n\tfile->ev_queue.is_closed = 1;\n\tspin_unlock_irq(&file->ev_queue.lock);\n\n\tuverbs_close_fd(filp);\n\n\treturn 0;\n}\n\nconst struct file_operations uverbs_event_fops = {\n\t.owner\t = THIS_MODULE,\n\t.read\t = ib_uverbs_comp_event_read,\n\t.poll    = ib_uverbs_comp_event_poll,\n\t.release = ib_uverbs_comp_event_close,\n\t.fasync  = ib_uverbs_comp_event_fasync,\n\t.llseek\t = no_llseek,\n};\n\nstatic const struct file_operations uverbs_async_event_fops = {\n\t.owner\t = THIS_MODULE,\n\t.read\t = ib_uverbs_async_event_read,\n\t.poll    = ib_uverbs_async_event_poll,\n\t.release = ib_uverbs_async_event_close,\n\t.fasync  = ib_uverbs_async_event_fasync,\n\t.llseek\t = no_llseek,\n};\n\nvoid ib_uverbs_comp_handler(struct ib_cq *cq, void *cq_context)\n{\n\tstruct ib_uverbs_event_queue   *ev_queue = cq_context;\n\tstruct ib_ucq_object\t       *uobj;\n\tstruct ib_uverbs_event\t       *entry;\n\tunsigned long\t\t\tflags;\n\n\tif (!ev_queue)\n\t\treturn;\n\n\tspin_lock_irqsave(&ev_queue->lock, flags);\n\tif (ev_queue->is_closed) {\n\t\tspin_unlock_irqrestore(&ev_queue->lock, flags);\n\t\treturn;\n\t}\n\n\tentry = kmalloc(sizeof(*entry), GFP_ATOMIC);\n\tif (!entry) {\n\t\tspin_unlock_irqrestore(&ev_queue->lock, flags);\n\t\treturn;\n\t}\n\n\tuobj = container_of(cq->uobject, struct ib_ucq_object, uobject);\n\n\tentry->desc.comp.cq_handle = cq->uobject->user_handle;\n\tentry->counter\t\t   = &uobj->comp_events_reported;\n\n\tlist_add_tail(&entry->list, &ev_queue->event_list);\n\tlist_add_tail(&entry->obj_list, &uobj->comp_list);\n\tspin_unlock_irqrestore(&ev_queue->lock, flags);\n\n\twake_up_interruptible(&ev_queue->poll_wait);\n\tkill_fasync(&ev_queue->async_queue, SIGIO, POLL_IN);\n}\n\nstatic void ib_uverbs_async_handler(struct ib_uverbs_file *file,\n\t\t\t\t    __u64 element, __u64 event,\n\t\t\t\t    struct list_head *obj_list,\n\t\t\t\t    u32 *counter)\n{\n\tstruct ib_uverbs_event *entry;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&file->async_file->ev_queue.lock, flags);\n\tif (file->async_file->ev_queue.is_closed) {\n\t\tspin_unlock_irqrestore(&file->async_file->ev_queue.lock, flags);\n\t\treturn;\n\t}\n\n\tentry = kmalloc(sizeof(*entry), GFP_ATOMIC);\n\tif (!entry) {\n\t\tspin_unlock_irqrestore(&file->async_file->ev_queue.lock, flags);\n\t\treturn;\n\t}\n\n\tentry->desc.async.element    = element;\n\tentry->desc.async.event_type = event;\n\tentry->desc.async.reserved   = 0;\n\tentry->counter               = counter;\n\n\tlist_add_tail(&entry->list, &file->async_file->ev_queue.event_list);\n\tif (obj_list)\n\t\tlist_add_tail(&entry->obj_list, obj_list);\n\tspin_unlock_irqrestore(&file->async_file->ev_queue.lock, flags);\n\n\twake_up_interruptible(&file->async_file->ev_queue.poll_wait);\n\tkill_fasync(&file->async_file->ev_queue.async_queue, SIGIO, POLL_IN);\n}\n\nvoid ib_uverbs_cq_event_handler(struct ib_event *event, void *context_ptr)\n{\n\tstruct ib_ucq_object *uobj = container_of(event->element.cq->uobject,\n\t\t\t\t\t\t  struct ib_ucq_object, uobject);\n\n\tib_uverbs_async_handler(uobj->uobject.ufile, uobj->uobject.user_handle,\n\t\t\t\tevent->event, &uobj->async_list,\n\t\t\t\t&uobj->async_events_reported);\n}\n\nvoid ib_uverbs_qp_event_handler(struct ib_event *event, void *context_ptr)\n{\n\tstruct ib_uevent_object *uobj;\n\n\t/* for XRC target qp's, check that qp is live */\n\tif (!event->element.qp->uobject)\n\t\treturn;\n\n\tuobj = container_of(event->element.qp->uobject,\n\t\t\t    struct ib_uevent_object, uobject);\n\n\tib_uverbs_async_handler(context_ptr, uobj->uobject.user_handle,\n\t\t\t\tevent->event, &uobj->event_list,\n\t\t\t\t&uobj->events_reported);\n}\n\nvoid ib_uverbs_wq_event_handler(struct ib_event *event, void *context_ptr)\n{\n\tstruct ib_uevent_object *uobj = container_of(event->element.wq->uobject,\n\t\t\t\t\t\t  struct ib_uevent_object, uobject);\n\n\tib_uverbs_async_handler(context_ptr, uobj->uobject.user_handle,\n\t\t\t\tevent->event, &uobj->event_list,\n\t\t\t\t&uobj->events_reported);\n}\n\nvoid ib_uverbs_srq_event_handler(struct ib_event *event, void *context_ptr)\n{\n\tstruct ib_uevent_object *uobj;\n\n\tuobj = container_of(event->element.srq->uobject,\n\t\t\t    struct ib_uevent_object, uobject);\n\n\tib_uverbs_async_handler(context_ptr, uobj->uobject.user_handle,\n\t\t\t\tevent->event, &uobj->event_list,\n\t\t\t\t&uobj->events_reported);\n}\n\nvoid ib_uverbs_event_handler(struct ib_event_handler *handler,\n\t\t\t     struct ib_event *event)\n{\n\tstruct ib_uverbs_file *file =\n\t\tcontainer_of(handler, struct ib_uverbs_file, event_handler);\n\n\tib_uverbs_async_handler(file, event->element.port_num, event->event,\n\t\t\t\tNULL, NULL);\n}\n\nvoid ib_uverbs_free_async_event_file(struct ib_uverbs_file *file)\n{\n\tkref_put(&file->async_file->ref, ib_uverbs_release_async_event_file);\n\tfile->async_file = NULL;\n}\n\nvoid ib_uverbs_init_event_queue(struct ib_uverbs_event_queue *ev_queue)\n{\n\tspin_lock_init(&ev_queue->lock);\n\tINIT_LIST_HEAD(&ev_queue->event_list);\n\tinit_waitqueue_head(&ev_queue->poll_wait);\n\tev_queue->is_closed   = 0;\n\tev_queue->async_queue = NULL;\n}\n\nstruct file *ib_uverbs_alloc_async_event_file(struct ib_uverbs_file *uverbs_file,\n\t\t\t\t\t      struct ib_device\t*ib_dev)\n{\n\tstruct ib_uverbs_async_event_file *ev_file;\n\tstruct file *filp;\n\n\tev_file = kzalloc(sizeof(*ev_file), GFP_KERNEL);\n\tif (!ev_file)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tib_uverbs_init_event_queue(&ev_file->ev_queue);\n\tev_file->uverbs_file = uverbs_file;\n\tkref_get(&ev_file->uverbs_file->ref);\n\tkref_init(&ev_file->ref);\n\tfilp = anon_inode_getfile(\"[infinibandevent]\", &uverbs_async_event_fops,\n\t\t\t\t  ev_file, O_RDONLY);\n\tif (IS_ERR(filp))\n\t\tgoto err_put_refs;\n\n\tmutex_lock(&uverbs_file->device->lists_mutex);\n\tlist_add_tail(&ev_file->list,\n\t\t      &uverbs_file->device->uverbs_events_file_list);\n\tmutex_unlock(&uverbs_file->device->lists_mutex);\n\n\tWARN_ON(uverbs_file->async_file);\n\tuverbs_file->async_file = ev_file;\n\tkref_get(&uverbs_file->async_file->ref);\n\tINIT_IB_EVENT_HANDLER(&uverbs_file->event_handler,\n\t\t\t      ib_dev,\n\t\t\t      ib_uverbs_event_handler);\n\tib_register_event_handler(&uverbs_file->event_handler);\n\t/* At that point async file stuff was fully set */\n\n\treturn filp;\n\nerr_put_refs:\n\tkref_put(&ev_file->uverbs_file->ref, ib_uverbs_release_file);\n\tkref_put(&ev_file->ref, ib_uverbs_release_async_event_file);\n\treturn filp;\n}\n\nstatic ssize_t verify_hdr(struct ib_uverbs_cmd_hdr *hdr,\n\t\t\t  struct ib_uverbs_ex_cmd_hdr *ex_hdr, size_t count,\n\t\t\t  const struct uverbs_api_write_method *method_elm)\n{\n\tif (method_elm->is_ex) {\n\t\tcount -= sizeof(*hdr) + sizeof(*ex_hdr);\n\n\t\tif ((hdr->in_words + ex_hdr->provider_in_words) * 8 != count)\n\t\t\treturn -EINVAL;\n\n\t\tif (hdr->in_words * 8 < method_elm->req_size)\n\t\t\treturn -ENOSPC;\n\n\t\tif (ex_hdr->cmd_hdr_reserved)\n\t\t\treturn -EINVAL;\n\n\t\tif (ex_hdr->response) {\n\t\t\tif (!hdr->out_words && !ex_hdr->provider_out_words)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tif (hdr->out_words * 8 < method_elm->resp_size)\n\t\t\t\treturn -ENOSPC;\n\n\t\t\tif (!access_ok(u64_to_user_ptr(ex_hdr->response),\n\t\t\t\t       (hdr->out_words + ex_hdr->provider_out_words) * 8))\n\t\t\t\treturn -EFAULT;\n\t\t} else {\n\t\t\tif (hdr->out_words || ex_hdr->provider_out_words)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\t/* not extended command */\n\tif (hdr->in_words * 4 != count)\n\t\treturn -EINVAL;\n\n\tif (count < method_elm->req_size + sizeof(hdr)) {\n\t\t/*\n\t\t * rdma-core v18 and v19 have a bug where they send DESTROY_CQ\n\t\t * with a 16 byte write instead of 24. Old kernels didn't\n\t\t * check the size so they allowed this. Now that the size is\n\t\t * checked provide a compatibility work around to not break\n\t\t * those userspaces.\n\t\t */\n\t\tif (hdr->command == IB_USER_VERBS_CMD_DESTROY_CQ &&\n\t\t    count == 16) {\n\t\t\thdr->in_words = 6;\n\t\t\treturn 0;\n\t\t}\n\t\treturn -ENOSPC;\n\t}\n\tif (hdr->out_words * 4 < method_elm->resp_size)\n\t\treturn -ENOSPC;\n\n\treturn 0;\n}\n\nstatic ssize_t ib_uverbs_write(struct file *filp, const char __user *buf,\n\t\t\t     size_t count, loff_t *pos)\n{\n\tstruct ib_uverbs_file *file = filp->private_data;\n\tconst struct uverbs_api_write_method *method_elm;\n\tstruct uverbs_api *uapi = file->device->uapi;\n\tstruct ib_uverbs_ex_cmd_hdr ex_hdr;\n\tstruct ib_uverbs_cmd_hdr hdr;\n\tstruct uverbs_attr_bundle bundle;\n\tint srcu_key;\n\tssize_t ret;\n\n\tif (!ib_safe_file_access(filp)) {\n\t\tpr_err_once(\"uverbs_write: process %d (%s) changed security contexts after opening file descriptor, this is not allowed.\\n\",\n\t\t\t    task_tgid_vnr(current), current->comm);\n\t\treturn -EACCES;\n\t}\n\n\tif (count < sizeof(hdr))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&hdr, buf, sizeof(hdr)))\n\t\treturn -EFAULT;\n\n\tmethod_elm = uapi_get_method(uapi, hdr.command);\n\tif (IS_ERR(method_elm))\n\t\treturn PTR_ERR(method_elm);\n\n\tif (method_elm->is_ex) {\n\t\tif (count < (sizeof(hdr) + sizeof(ex_hdr)))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&ex_hdr, buf + sizeof(hdr), sizeof(ex_hdr)))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = verify_hdr(&hdr, &ex_hdr, count, method_elm);\n\tif (ret)\n\t\treturn ret;\n\n\tsrcu_key = srcu_read_lock(&file->device->disassociate_srcu);\n\n\tbuf += sizeof(hdr);\n\n\tmemset(bundle.attr_present, 0, sizeof(bundle.attr_present));\n\tbundle.ufile = file;\n\tbundle.context = NULL; /* only valid if bundle has uobject */\n\tif (!method_elm->is_ex) {\n\t\tsize_t in_len = hdr.in_words * 4 - sizeof(hdr);\n\t\tsize_t out_len = hdr.out_words * 4;\n\t\tu64 response = 0;\n\n\t\tif (method_elm->has_udata) {\n\t\t\tbundle.driver_udata.inlen =\n\t\t\t\tin_len - method_elm->req_size;\n\t\t\tin_len = method_elm->req_size;\n\t\t\tif (bundle.driver_udata.inlen)\n\t\t\t\tbundle.driver_udata.inbuf = buf + in_len;\n\t\t\telse\n\t\t\t\tbundle.driver_udata.inbuf = NULL;\n\t\t} else {\n\t\t\tmemset(&bundle.driver_udata, 0,\n\t\t\t       sizeof(bundle.driver_udata));\n\t\t}\n\n\t\tif (method_elm->has_resp) {\n\t\t\t/*\n\t\t\t * The macros check that if has_resp is set\n\t\t\t * then the command request structure starts\n\t\t\t * with a '__aligned u64 response' member.\n\t\t\t */\n\t\t\tret = get_user(response, (const u64 *)buf);\n\t\t\tif (ret)\n\t\t\t\tgoto out_unlock;\n\n\t\t\tif (method_elm->has_udata) {\n\t\t\t\tbundle.driver_udata.outlen =\n\t\t\t\t\tout_len - method_elm->resp_size;\n\t\t\t\tout_len = method_elm->resp_size;\n\t\t\t\tif (bundle.driver_udata.outlen)\n\t\t\t\t\tbundle.driver_udata.outbuf =\n\t\t\t\t\t\tu64_to_user_ptr(response +\n\t\t\t\t\t\t\t\tout_len);\n\t\t\t\telse\n\t\t\t\t\tbundle.driver_udata.outbuf = NULL;\n\t\t\t}\n\t\t} else {\n\t\t\tbundle.driver_udata.outlen = 0;\n\t\t\tbundle.driver_udata.outbuf = NULL;\n\t\t}\n\n\t\tib_uverbs_init_udata_buf_or_null(\n\t\t\t&bundle.ucore, buf, u64_to_user_ptr(response),\n\t\t\tin_len, out_len);\n\t} else {\n\t\tbuf += sizeof(ex_hdr);\n\n\t\tib_uverbs_init_udata_buf_or_null(&bundle.ucore, buf,\n\t\t\t\t\tu64_to_user_ptr(ex_hdr.response),\n\t\t\t\t\thdr.in_words * 8, hdr.out_words * 8);\n\n\t\tib_uverbs_init_udata_buf_or_null(\n\t\t\t&bundle.driver_udata, buf + bundle.ucore.inlen,\n\t\t\tu64_to_user_ptr(ex_hdr.response) + bundle.ucore.outlen,\n\t\t\tex_hdr.provider_in_words * 8,\n\t\t\tex_hdr.provider_out_words * 8);\n\n\t}\n\n\tret = method_elm->handler(&bundle);\nout_unlock:\n\tsrcu_read_unlock(&file->device->disassociate_srcu, srcu_key);\n\treturn (ret) ? : count;\n}\n\nstatic int ib_uverbs_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\tstruct ib_uverbs_file *file = filp->private_data;\n\tstruct ib_ucontext *ucontext;\n\tint ret = 0;\n\tint srcu_key;\n\n\tsrcu_key = srcu_read_lock(&file->device->disassociate_srcu);\n\tucontext = ib_uverbs_get_ucontext_file(file);\n\tif (IS_ERR(ucontext)) {\n\t\tret = PTR_ERR(ucontext);\n\t\tgoto out;\n\t}\n\n\tret = ucontext->device->ops.mmap(ucontext, vma);\nout:\n\tsrcu_read_unlock(&file->device->disassociate_srcu, srcu_key);\n\treturn ret;\n}\n\n/*\n * Each time we map IO memory into user space this keeps track of the mapping.\n * When the device is hot-unplugged we 'zap' the mmaps in user space to point\n * to the zero page and allow the hot unplug to proceed.\n *\n * This is necessary for cases like PCI physical hot unplug as the actual BAR\n * memory may vanish after this and access to it from userspace could MCE.\n *\n * RDMA drivers supporting disassociation must have their user space designed\n * to cope in some way with their IO pages going to the zero page.\n */\nstruct rdma_umap_priv {\n\tstruct vm_area_struct *vma;\n\tstruct list_head list;\n};\n\nstatic const struct vm_operations_struct rdma_umap_ops;\n\nstatic void rdma_umap_priv_init(struct rdma_umap_priv *priv,\n\t\t\t\tstruct vm_area_struct *vma)\n{\n\tstruct ib_uverbs_file *ufile = vma->vm_file->private_data;\n\n\tpriv->vma = vma;\n\tvma->vm_private_data = priv;\n\tvma->vm_ops = &rdma_umap_ops;\n\n\tmutex_lock(&ufile->umap_lock);\n\tlist_add(&priv->list, &ufile->umaps);\n\tmutex_unlock(&ufile->umap_lock);\n}\n\n/*\n * The VMA has been dup'd, initialize the vm_private_data with a new tracking\n * struct\n */\nstatic void rdma_umap_open(struct vm_area_struct *vma)\n{\n\tstruct ib_uverbs_file *ufile = vma->vm_file->private_data;\n\tstruct rdma_umap_priv *opriv = vma->vm_private_data;\n\tstruct rdma_umap_priv *priv;\n\n\tif (!opriv)\n\t\treturn;\n\n\t/* We are racing with disassociation */\n\tif (!down_read_trylock(&ufile->hw_destroy_rwsem))\n\t\tgoto out_zap;\n\t/*\n\t * Disassociation already completed, the VMA should already be zapped.\n\t */\n\tif (!ufile->ucontext)\n\t\tgoto out_unlock;\n\n\tpriv = kzalloc(sizeof(*priv), GFP_KERNEL);\n\tif (!priv)\n\t\tgoto out_unlock;\n\trdma_umap_priv_init(priv, vma);\n\n\tup_read(&ufile->hw_destroy_rwsem);\n\treturn;\n\nout_unlock:\n\tup_read(&ufile->hw_destroy_rwsem);\nout_zap:\n\t/*\n\t * We can't allow the VMA to be created with the actual IO pages, that\n\t * would break our API contract, and it can't be stopped at this\n\t * point, so zap it.\n\t */\n\tvma->vm_private_data = NULL;\n\tzap_vma_ptes(vma, vma->vm_start, vma->vm_end - vma->vm_start);\n}\n\nstatic void rdma_umap_close(struct vm_area_struct *vma)\n{\n\tstruct ib_uverbs_file *ufile = vma->vm_file->private_data;\n\tstruct rdma_umap_priv *priv = vma->vm_private_data;\n\n\tif (!priv)\n\t\treturn;\n\n\t/*\n\t * The vma holds a reference on the struct file that created it, which\n\t * in turn means that the ib_uverbs_file is guaranteed to exist at\n\t * this point.\n\t */\n\tmutex_lock(&ufile->umap_lock);\n\tlist_del(&priv->list);\n\tmutex_unlock(&ufile->umap_lock);\n\tkfree(priv);\n}\n\nstatic const struct vm_operations_struct rdma_umap_ops = {\n\t.open = rdma_umap_open,\n\t.close = rdma_umap_close,\n};\n\nstatic struct rdma_umap_priv *rdma_user_mmap_pre(struct ib_ucontext *ucontext,\n\t\t\t\t\t\t struct vm_area_struct *vma,\n\t\t\t\t\t\t unsigned long size)\n{\n\tstruct ib_uverbs_file *ufile = ucontext->ufile;\n\tstruct rdma_umap_priv *priv;\n\n\tif (vma->vm_end - vma->vm_start != size)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/* Driver is using this wrong, must be called by ib_uverbs_mmap */\n\tif (WARN_ON(!vma->vm_file ||\n\t\t    vma->vm_file->private_data != ufile))\n\t\treturn ERR_PTR(-EINVAL);\n\tlockdep_assert_held(&ufile->device->disassociate_srcu);\n\n\tpriv = kzalloc(sizeof(*priv), GFP_KERNEL);\n\tif (!priv)\n\t\treturn ERR_PTR(-ENOMEM);\n\treturn priv;\n}\n\n/*\n * Map IO memory into a process. This is to be called by drivers as part of\n * their mmap() functions if they wish to send something like PCI-E BAR memory\n * to userspace.\n */\nint rdma_user_mmap_io(struct ib_ucontext *ucontext, struct vm_area_struct *vma,\n\t\t      unsigned long pfn, unsigned long size, pgprot_t prot)\n{\n\tstruct rdma_umap_priv *priv = rdma_user_mmap_pre(ucontext, vma, size);\n\n\tif (IS_ERR(priv))\n\t\treturn PTR_ERR(priv);\n\n\tvma->vm_page_prot = prot;\n\tif (io_remap_pfn_range(vma, vma->vm_start, pfn, size, prot)) {\n\t\tkfree(priv);\n\t\treturn -EAGAIN;\n\t}\n\n\trdma_umap_priv_init(priv, vma);\n\treturn 0;\n}\nEXPORT_SYMBOL(rdma_user_mmap_io);\n\n/*\n * The page case is here for a slightly different reason, the driver expects\n * to be able to free the page it is sharing to user space when it destroys\n * its ucontext, which means we need to zap the user space references.\n *\n * We could handle this differently by providing an API to allocate a shared\n * page and then only freeing the shared page when the last ufile is\n * destroyed.\n */\nint rdma_user_mmap_page(struct ib_ucontext *ucontext,\n\t\t\tstruct vm_area_struct *vma, struct page *page,\n\t\t\tunsigned long size)\n{\n\tstruct rdma_umap_priv *priv = rdma_user_mmap_pre(ucontext, vma, size);\n\n\tif (IS_ERR(priv))\n\t\treturn PTR_ERR(priv);\n\n\tif (remap_pfn_range(vma, vma->vm_start, page_to_pfn(page), size,\n\t\t\t    vma->vm_page_prot)) {\n\t\tkfree(priv);\n\t\treturn -EAGAIN;\n\t}\n\n\trdma_umap_priv_init(priv, vma);\n\treturn 0;\n}\nEXPORT_SYMBOL(rdma_user_mmap_page);\n\nvoid uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)\n{\n\tstruct rdma_umap_priv *priv, *next_priv;\n\n\tlockdep_assert_held(&ufile->hw_destroy_rwsem);\n\n\twhile (1) {\n\t\tstruct mm_struct *mm = NULL;\n\n\t\t/* Get an arbitrary mm pointer that hasn't been cleaned yet */\n\t\tmutex_lock(&ufile->umap_lock);\n\t\twhile (!list_empty(&ufile->umaps)) {\n\t\t\tint ret;\n\n\t\t\tpriv = list_first_entry(&ufile->umaps,\n\t\t\t\t\t\tstruct rdma_umap_priv, list);\n\t\t\tmm = priv->vma->vm_mm;\n\t\t\tret = mmget_not_zero(mm);\n\t\t\tif (!ret) {\n\t\t\t\tlist_del_init(&priv->list);\n\t\t\t\tmm = NULL;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tmutex_unlock(&ufile->umap_lock);\n\t\tif (!mm)\n\t\t\treturn;\n\n\t\t/*\n\t\t * The umap_lock is nested under mmap_sem since it used within\n\t\t * the vma_ops callbacks, so we have to clean the list one mm\n\t\t * at a time to get the lock ordering right. Typically there\n\t\t * will only be one mm, so no big deal.\n\t\t */\n\t\tdown_write(&mm->mmap_sem);\n\t\tmutex_lock(&ufile->umap_lock);\n\t\tlist_for_each_entry_safe (priv, next_priv, &ufile->umaps,\n\t\t\t\t\t  list) {\n\t\t\tstruct vm_area_struct *vma = priv->vma;\n\n\t\t\tif (vma->vm_mm != mm)\n\t\t\t\tcontinue;\n\t\t\tlist_del_init(&priv->list);\n\n\t\t\tzap_vma_ptes(vma, vma->vm_start,\n\t\t\t\t     vma->vm_end - vma->vm_start);\n\t\t\tvma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);\n\t\t}\n\t\tmutex_unlock(&ufile->umap_lock);\n\t\tup_write(&mm->mmap_sem);\n\t\tmmput(mm);\n\t}\n}\n\n/*\n * ib_uverbs_open() does not need the BKL:\n *\n *  - the ib_uverbs_device structures are properly reference counted and\n *    everything else is purely local to the file being created, so\n *    races against other open calls are not a problem;\n *  - there is no ioctl method to race against;\n *  - the open method will either immediately run -ENXIO, or all\n *    required initialization will be done.\n */\nstatic int ib_uverbs_open(struct inode *inode, struct file *filp)\n{\n\tstruct ib_uverbs_device *dev;\n\tstruct ib_uverbs_file *file;\n\tstruct ib_device *ib_dev;\n\tint ret;\n\tint module_dependent;\n\tint srcu_key;\n\n\tdev = container_of(inode->i_cdev, struct ib_uverbs_device, cdev);\n\tif (!atomic_inc_not_zero(&dev->refcount))\n\t\treturn -ENXIO;\n\n\tget_device(&dev->dev);\n\tsrcu_key = srcu_read_lock(&dev->disassociate_srcu);\n\tmutex_lock(&dev->lists_mutex);\n\tib_dev = srcu_dereference(dev->ib_dev,\n\t\t\t\t  &dev->disassociate_srcu);\n\tif (!ib_dev) {\n\t\tret = -EIO;\n\t\tgoto err;\n\t}\n\n\t/* In case IB device supports disassociate ucontext, there is no hard\n\t * dependency between uverbs device and its low level device.\n\t */\n\tmodule_dependent = !(ib_dev->ops.disassociate_ucontext);\n\n\tif (module_dependent) {\n\t\tif (!try_module_get(ib_dev->owner)) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\tfile = kzalloc(sizeof(*file), GFP_KERNEL);\n\tif (!file) {\n\t\tret = -ENOMEM;\n\t\tif (module_dependent)\n\t\t\tgoto err_module;\n\n\t\tgoto err;\n\t}\n\n\tfile->device\t = dev;\n\tkref_init(&file->ref);\n\tmutex_init(&file->ucontext_lock);\n\n\tspin_lock_init(&file->uobjects_lock);\n\tINIT_LIST_HEAD(&file->uobjects);\n\tinit_rwsem(&file->hw_destroy_rwsem);\n\tmutex_init(&file->umap_lock);\n\tINIT_LIST_HEAD(&file->umaps);\n\n\tfilp->private_data = file;\n\tlist_add_tail(&file->list, &dev->uverbs_file_list);\n\tmutex_unlock(&dev->lists_mutex);\n\tsrcu_read_unlock(&dev->disassociate_srcu, srcu_key);\n\n\tsetup_ufile_idr_uobject(file);\n\n\treturn nonseekable_open(inode, filp);\n\nerr_module:\n\tmodule_put(ib_dev->owner);\n\nerr:\n\tmutex_unlock(&dev->lists_mutex);\n\tsrcu_read_unlock(&dev->disassociate_srcu, srcu_key);\n\tif (atomic_dec_and_test(&dev->refcount))\n\t\tib_uverbs_comp_dev(dev);\n\n\tput_device(&dev->dev);\n\treturn ret;\n}\n\nstatic int ib_uverbs_close(struct inode *inode, struct file *filp)\n{\n\tstruct ib_uverbs_file *file = filp->private_data;\n\n\tuverbs_destroy_ufile_hw(file, RDMA_REMOVE_CLOSE);\n\n\tmutex_lock(&file->device->lists_mutex);\n\tlist_del_init(&file->list);\n\tmutex_unlock(&file->device->lists_mutex);\n\n\tkref_put(&file->ref, ib_uverbs_release_file);\n\n\treturn 0;\n}\n\nstatic const struct file_operations uverbs_fops = {\n\t.owner\t = THIS_MODULE,\n\t.write\t = ib_uverbs_write,\n\t.open\t = ib_uverbs_open,\n\t.release = ib_uverbs_close,\n\t.llseek\t = no_llseek,\n\t.unlocked_ioctl = ib_uverbs_ioctl,\n\t.compat_ioctl = ib_uverbs_ioctl,\n};\n\nstatic const struct file_operations uverbs_mmap_fops = {\n\t.owner\t = THIS_MODULE,\n\t.write\t = ib_uverbs_write,\n\t.mmap    = ib_uverbs_mmap,\n\t.open\t = ib_uverbs_open,\n\t.release = ib_uverbs_close,\n\t.llseek\t = no_llseek,\n\t.unlocked_ioctl = ib_uverbs_ioctl,\n\t.compat_ioctl = ib_uverbs_ioctl,\n};\n\nstatic struct ib_client uverbs_client = {\n\t.name   = \"uverbs\",\n\t.no_kverbs_req = true,\n\t.add    = ib_uverbs_add_one,\n\t.remove = ib_uverbs_remove_one\n};\n\nstatic ssize_t ibdev_show(struct device *device, struct device_attribute *attr,\n\t\t\t  char *buf)\n{\n\tstruct ib_uverbs_device *dev =\n\t\t\tcontainer_of(device, struct ib_uverbs_device, dev);\n\tint ret = -ENODEV;\n\tint srcu_key;\n\tstruct ib_device *ib_dev;\n\n\tsrcu_key = srcu_read_lock(&dev->disassociate_srcu);\n\tib_dev = srcu_dereference(dev->ib_dev, &dev->disassociate_srcu);\n\tif (ib_dev)\n\t\tret = sprintf(buf, \"%s\\n\", dev_name(&ib_dev->dev));\n\tsrcu_read_unlock(&dev->disassociate_srcu, srcu_key);\n\n\treturn ret;\n}\nstatic DEVICE_ATTR_RO(ibdev);\n\nstatic ssize_t abi_version_show(struct device *device,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct ib_uverbs_device *dev =\n\t\t\tcontainer_of(device, struct ib_uverbs_device, dev);\n\tint ret = -ENODEV;\n\tint srcu_key;\n\tstruct ib_device *ib_dev;\n\n\tsrcu_key = srcu_read_lock(&dev->disassociate_srcu);\n\tib_dev = srcu_dereference(dev->ib_dev, &dev->disassociate_srcu);\n\tif (ib_dev)\n\t\tret = sprintf(buf, \"%d\\n\", ib_dev->uverbs_abi_ver);\n\tsrcu_read_unlock(&dev->disassociate_srcu, srcu_key);\n\n\treturn ret;\n}\nstatic DEVICE_ATTR_RO(abi_version);\n\nstatic struct attribute *ib_dev_attrs[] = {\n\t&dev_attr_abi_version.attr,\n\t&dev_attr_ibdev.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group dev_attr_group = {\n\t.attrs = ib_dev_attrs,\n};\n\nstatic CLASS_ATTR_STRING(abi_version, S_IRUGO,\n\t\t\t __stringify(IB_USER_VERBS_ABI_VERSION));\n\nstatic int ib_uverbs_create_uapi(struct ib_device *device,\n\t\t\t\t struct ib_uverbs_device *uverbs_dev)\n{\n\tstruct uverbs_api *uapi;\n\n\tuapi = uverbs_alloc_api(device);\n\tif (IS_ERR(uapi))\n\t\treturn PTR_ERR(uapi);\n\n\tuverbs_dev->uapi = uapi;\n\treturn 0;\n}\n\nstatic void ib_uverbs_add_one(struct ib_device *device)\n{\n\tint devnum;\n\tdev_t base;\n\tstruct ib_uverbs_device *uverbs_dev;\n\tint ret;\n\n\tif (!device->ops.alloc_ucontext)\n\t\treturn;\n\n\tuverbs_dev = kzalloc(sizeof(*uverbs_dev), GFP_KERNEL);\n\tif (!uverbs_dev)\n\t\treturn;\n\n\tret = init_srcu_struct(&uverbs_dev->disassociate_srcu);\n\tif (ret) {\n\t\tkfree(uverbs_dev);\n\t\treturn;\n\t}\n\n\tdevice_initialize(&uverbs_dev->dev);\n\tuverbs_dev->dev.class = uverbs_class;\n\tuverbs_dev->dev.parent = device->dev.parent;\n\tuverbs_dev->dev.release = ib_uverbs_release_dev;\n\tuverbs_dev->groups[0] = &dev_attr_group;\n\tuverbs_dev->dev.groups = uverbs_dev->groups;\n\tatomic_set(&uverbs_dev->refcount, 1);\n\tinit_completion(&uverbs_dev->comp);\n\tuverbs_dev->xrcd_tree = RB_ROOT;\n\tmutex_init(&uverbs_dev->xrcd_tree_mutex);\n\tmutex_init(&uverbs_dev->lists_mutex);\n\tINIT_LIST_HEAD(&uverbs_dev->uverbs_file_list);\n\tINIT_LIST_HEAD(&uverbs_dev->uverbs_events_file_list);\n\trcu_assign_pointer(uverbs_dev->ib_dev, device);\n\tuverbs_dev->num_comp_vectors = device->num_comp_vectors;\n\n\tdevnum = ida_alloc_max(&uverbs_ida, IB_UVERBS_MAX_DEVICES - 1,\n\t\t\t       GFP_KERNEL);\n\tif (devnum < 0)\n\t\tgoto err;\n\tuverbs_dev->devnum = devnum;\n\tif (devnum >= IB_UVERBS_NUM_FIXED_MINOR)\n\t\tbase = dynamic_uverbs_dev + devnum - IB_UVERBS_NUM_FIXED_MINOR;\n\telse\n\t\tbase = IB_UVERBS_BASE_DEV + devnum;\n\n\tif (ib_uverbs_create_uapi(device, uverbs_dev))\n\t\tgoto err_uapi;\n\n\tuverbs_dev->dev.devt = base;\n\tdev_set_name(&uverbs_dev->dev, \"uverbs%d\", uverbs_dev->devnum);\n\n\tcdev_init(&uverbs_dev->cdev,\n\t\t  device->ops.mmap ? &uverbs_mmap_fops : &uverbs_fops);\n\tuverbs_dev->cdev.owner = THIS_MODULE;\n\n\tret = cdev_device_add(&uverbs_dev->cdev, &uverbs_dev->dev);\n\tif (ret)\n\t\tgoto err_uapi;\n\n\tib_set_client_data(device, &uverbs_client, uverbs_dev);\n\treturn;\n\nerr_uapi:\n\tida_free(&uverbs_ida, devnum);\nerr:\n\tif (atomic_dec_and_test(&uverbs_dev->refcount))\n\t\tib_uverbs_comp_dev(uverbs_dev);\n\twait_for_completion(&uverbs_dev->comp);\n\tput_device(&uverbs_dev->dev);\n\treturn;\n}\n\nstatic void ib_uverbs_free_hw_resources(struct ib_uverbs_device *uverbs_dev,\n\t\t\t\t\tstruct ib_device *ib_dev)\n{\n\tstruct ib_uverbs_file *file;\n\tstruct ib_uverbs_async_event_file *event_file;\n\tstruct ib_event event;\n\n\t/* Pending running commands to terminate */\n\tuverbs_disassociate_api_pre(uverbs_dev);\n\tevent.event = IB_EVENT_DEVICE_FATAL;\n\tevent.element.port_num = 0;\n\tevent.device = ib_dev;\n\n\tmutex_lock(&uverbs_dev->lists_mutex);\n\twhile (!list_empty(&uverbs_dev->uverbs_file_list)) {\n\t\tfile = list_first_entry(&uverbs_dev->uverbs_file_list,\n\t\t\t\t\tstruct ib_uverbs_file, list);\n\t\tlist_del_init(&file->list);\n\t\tkref_get(&file->ref);\n\n\t\t/* We must release the mutex before going ahead and calling\n\t\t * uverbs_cleanup_ufile, as it might end up indirectly calling\n\t\t * uverbs_close, for example due to freeing the resources (e.g\n\t\t * mmput).\n\t\t */\n\t\tmutex_unlock(&uverbs_dev->lists_mutex);\n\n\t\tib_uverbs_event_handler(&file->event_handler, &event);\n\t\tuverbs_destroy_ufile_hw(file, RDMA_REMOVE_DRIVER_REMOVE);\n\t\tkref_put(&file->ref, ib_uverbs_release_file);\n\n\t\tmutex_lock(&uverbs_dev->lists_mutex);\n\t}\n\n\twhile (!list_empty(&uverbs_dev->uverbs_events_file_list)) {\n\t\tevent_file = list_first_entry(&uverbs_dev->\n\t\t\t\t\t      uverbs_events_file_list,\n\t\t\t\t\t      struct ib_uverbs_async_event_file,\n\t\t\t\t\t      list);\n\t\tspin_lock_irq(&event_file->ev_queue.lock);\n\t\tevent_file->ev_queue.is_closed = 1;\n\t\tspin_unlock_irq(&event_file->ev_queue.lock);\n\n\t\tlist_del(&event_file->list);\n\t\tib_unregister_event_handler(\n\t\t\t&event_file->uverbs_file->event_handler);\n\t\tevent_file->uverbs_file->event_handler.device =\n\t\t\tNULL;\n\n\t\twake_up_interruptible(&event_file->ev_queue.poll_wait);\n\t\tkill_fasync(&event_file->ev_queue.async_queue, SIGIO, POLL_IN);\n\t}\n\tmutex_unlock(&uverbs_dev->lists_mutex);\n\n\tuverbs_disassociate_api(uverbs_dev->uapi);\n}\n\nstatic void ib_uverbs_remove_one(struct ib_device *device, void *client_data)\n{\n\tstruct ib_uverbs_device *uverbs_dev = client_data;\n\tint wait_clients = 1;\n\n\tif (!uverbs_dev)\n\t\treturn;\n\n\tcdev_device_del(&uverbs_dev->cdev, &uverbs_dev->dev);\n\tida_free(&uverbs_ida, uverbs_dev->devnum);\n\n\tif (device->ops.disassociate_ucontext) {\n\t\t/* We disassociate HW resources and immediately return.\n\t\t * Userspace will see a EIO errno for all future access.\n\t\t * Upon returning, ib_device may be freed internally and is not\n\t\t * valid any more.\n\t\t * uverbs_device is still available until all clients close\n\t\t * their files, then the uverbs device ref count will be zero\n\t\t * and its resources will be freed.\n\t\t * Note: At this point no more files can be opened since the\n\t\t * cdev was deleted, however active clients can still issue\n\t\t * commands and close their open files.\n\t\t */\n\t\tib_uverbs_free_hw_resources(uverbs_dev, device);\n\t\twait_clients = 0;\n\t}\n\n\tif (atomic_dec_and_test(&uverbs_dev->refcount))\n\t\tib_uverbs_comp_dev(uverbs_dev);\n\tif (wait_clients)\n\t\twait_for_completion(&uverbs_dev->comp);\n\n\tput_device(&uverbs_dev->dev);\n}\n\nstatic char *uverbs_devnode(struct device *dev, umode_t *mode)\n{\n\tif (mode)\n\t\t*mode = 0666;\n\treturn kasprintf(GFP_KERNEL, \"infiniband/%s\", dev_name(dev));\n}\n\nstatic int __init ib_uverbs_init(void)\n{\n\tint ret;\n\n\tret = register_chrdev_region(IB_UVERBS_BASE_DEV,\n\t\t\t\t     IB_UVERBS_NUM_FIXED_MINOR,\n\t\t\t\t     \"infiniband_verbs\");\n\tif (ret) {\n\t\tpr_err(\"user_verbs: couldn't register device number\\n\");\n\t\tgoto out;\n\t}\n\n\tret = alloc_chrdev_region(&dynamic_uverbs_dev, 0,\n\t\t\t\t  IB_UVERBS_NUM_DYNAMIC_MINOR,\n\t\t\t\t  \"infiniband_verbs\");\n\tif (ret) {\n\t\tpr_err(\"couldn't register dynamic device number\\n\");\n\t\tgoto out_alloc;\n\t}\n\n\tuverbs_class = class_create(THIS_MODULE, \"infiniband_verbs\");\n\tif (IS_ERR(uverbs_class)) {\n\t\tret = PTR_ERR(uverbs_class);\n\t\tpr_err(\"user_verbs: couldn't create class infiniband_verbs\\n\");\n\t\tgoto out_chrdev;\n\t}\n\n\tuverbs_class->devnode = uverbs_devnode;\n\n\tret = class_create_file(uverbs_class, &class_attr_abi_version.attr);\n\tif (ret) {\n\t\tpr_err(\"user_verbs: couldn't create abi_version attribute\\n\");\n\t\tgoto out_class;\n\t}\n\n\tret = ib_register_client(&uverbs_client);\n\tif (ret) {\n\t\tpr_err(\"user_verbs: couldn't register client\\n\");\n\t\tgoto out_class;\n\t}\n\n\treturn 0;\n\nout_class:\n\tclass_destroy(uverbs_class);\n\nout_chrdev:\n\tunregister_chrdev_region(dynamic_uverbs_dev,\n\t\t\t\t IB_UVERBS_NUM_DYNAMIC_MINOR);\n\nout_alloc:\n\tunregister_chrdev_region(IB_UVERBS_BASE_DEV,\n\t\t\t\t IB_UVERBS_NUM_FIXED_MINOR);\n\nout:\n\treturn ret;\n}\n\nstatic void __exit ib_uverbs_cleanup(void)\n{\n\tib_unregister_client(&uverbs_client);\n\tclass_destroy(uverbs_class);\n\tunregister_chrdev_region(IB_UVERBS_BASE_DEV,\n\t\t\t\t IB_UVERBS_NUM_FIXED_MINOR);\n\tunregister_chrdev_region(dynamic_uverbs_dev,\n\t\t\t\t IB_UVERBS_NUM_DYNAMIC_MINOR);\n}\n\nmodule_init(ib_uverbs_init);\nmodule_exit(ib_uverbs_cleanup);\n", "// SPDX-License-Identifier: GPL-2.0\n#include <linux/mm.h>\n#include <linux/vmacache.h>\n#include <linux/hugetlb.h>\n#include <linux/huge_mm.h>\n#include <linux/mount.h>\n#include <linux/seq_file.h>\n#include <linux/highmem.h>\n#include <linux/ptrace.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/mempolicy.h>\n#include <linux/rmap.h>\n#include <linux/swap.h>\n#include <linux/sched/mm.h>\n#include <linux/swapops.h>\n#include <linux/mmu_notifier.h>\n#include <linux/page_idle.h>\n#include <linux/shmem_fs.h>\n#include <linux/uaccess.h>\n#include <linux/pkeys.h>\n\n#include <asm/elf.h>\n#include <asm/tlb.h>\n#include <asm/tlbflush.h>\n#include \"internal.h\"\n\n#define SEQ_PUT_DEC(str, val) \\\n\t\tseq_put_decimal_ull_width(m, str, (val) << (PAGE_SHIFT-10), 8)\nvoid task_mem(struct seq_file *m, struct mm_struct *mm)\n{\n\tunsigned long text, lib, swap, anon, file, shmem;\n\tunsigned long hiwater_vm, total_vm, hiwater_rss, total_rss;\n\n\tanon = get_mm_counter(mm, MM_ANONPAGES);\n\tfile = get_mm_counter(mm, MM_FILEPAGES);\n\tshmem = get_mm_counter(mm, MM_SHMEMPAGES);\n\n\t/*\n\t * Note: to minimize their overhead, mm maintains hiwater_vm and\n\t * hiwater_rss only when about to *lower* total_vm or rss.  Any\n\t * collector of these hiwater stats must therefore get total_vm\n\t * and rss too, which will usually be the higher.  Barriers? not\n\t * worth the effort, such snapshots can always be inconsistent.\n\t */\n\thiwater_vm = total_vm = mm->total_vm;\n\tif (hiwater_vm < mm->hiwater_vm)\n\t\thiwater_vm = mm->hiwater_vm;\n\thiwater_rss = total_rss = anon + file + shmem;\n\tif (hiwater_rss < mm->hiwater_rss)\n\t\thiwater_rss = mm->hiwater_rss;\n\n\t/* split executable areas between text and lib */\n\ttext = PAGE_ALIGN(mm->end_code) - (mm->start_code & PAGE_MASK);\n\ttext = min(text, mm->exec_vm << PAGE_SHIFT);\n\tlib = (mm->exec_vm << PAGE_SHIFT) - text;\n\n\tswap = get_mm_counter(mm, MM_SWAPENTS);\n\tSEQ_PUT_DEC(\"VmPeak:\\t\", hiwater_vm);\n\tSEQ_PUT_DEC(\" kB\\nVmSize:\\t\", total_vm);\n\tSEQ_PUT_DEC(\" kB\\nVmLck:\\t\", mm->locked_vm);\n\tSEQ_PUT_DEC(\" kB\\nVmPin:\\t\", atomic64_read(&mm->pinned_vm));\n\tSEQ_PUT_DEC(\" kB\\nVmHWM:\\t\", hiwater_rss);\n\tSEQ_PUT_DEC(\" kB\\nVmRSS:\\t\", total_rss);\n\tSEQ_PUT_DEC(\" kB\\nRssAnon:\\t\", anon);\n\tSEQ_PUT_DEC(\" kB\\nRssFile:\\t\", file);\n\tSEQ_PUT_DEC(\" kB\\nRssShmem:\\t\", shmem);\n\tSEQ_PUT_DEC(\" kB\\nVmData:\\t\", mm->data_vm);\n\tSEQ_PUT_DEC(\" kB\\nVmStk:\\t\", mm->stack_vm);\n\tseq_put_decimal_ull_width(m,\n\t\t    \" kB\\nVmExe:\\t\", text >> 10, 8);\n\tseq_put_decimal_ull_width(m,\n\t\t    \" kB\\nVmLib:\\t\", lib >> 10, 8);\n\tseq_put_decimal_ull_width(m,\n\t\t    \" kB\\nVmPTE:\\t\", mm_pgtables_bytes(mm) >> 10, 8);\n\tSEQ_PUT_DEC(\" kB\\nVmSwap:\\t\", swap);\n\tseq_puts(m, \" kB\\n\");\n\thugetlb_report_usage(m, mm);\n}\n#undef SEQ_PUT_DEC\n\nunsigned long task_vsize(struct mm_struct *mm)\n{\n\treturn PAGE_SIZE * mm->total_vm;\n}\n\nunsigned long task_statm(struct mm_struct *mm,\n\t\t\t unsigned long *shared, unsigned long *text,\n\t\t\t unsigned long *data, unsigned long *resident)\n{\n\t*shared = get_mm_counter(mm, MM_FILEPAGES) +\n\t\t\tget_mm_counter(mm, MM_SHMEMPAGES);\n\t*text = (PAGE_ALIGN(mm->end_code) - (mm->start_code & PAGE_MASK))\n\t\t\t\t\t\t\t\t>> PAGE_SHIFT;\n\t*data = mm->data_vm + mm->stack_vm;\n\t*resident = *shared + get_mm_counter(mm, MM_ANONPAGES);\n\treturn mm->total_vm;\n}\n\n#ifdef CONFIG_NUMA\n/*\n * Save get_task_policy() for show_numa_map().\n */\nstatic void hold_task_mempolicy(struct proc_maps_private *priv)\n{\n\tstruct task_struct *task = priv->task;\n\n\ttask_lock(task);\n\tpriv->task_mempolicy = get_task_policy(task);\n\tmpol_get(priv->task_mempolicy);\n\ttask_unlock(task);\n}\nstatic void release_task_mempolicy(struct proc_maps_private *priv)\n{\n\tmpol_put(priv->task_mempolicy);\n}\n#else\nstatic void hold_task_mempolicy(struct proc_maps_private *priv)\n{\n}\nstatic void release_task_mempolicy(struct proc_maps_private *priv)\n{\n}\n#endif\n\nstatic void vma_stop(struct proc_maps_private *priv)\n{\n\tstruct mm_struct *mm = priv->mm;\n\n\trelease_task_mempolicy(priv);\n\tup_read(&mm->mmap_sem);\n\tmmput(mm);\n}\n\nstatic struct vm_area_struct *\nm_next_vma(struct proc_maps_private *priv, struct vm_area_struct *vma)\n{\n\tif (vma == priv->tail_vma)\n\t\treturn NULL;\n\treturn vma->vm_next ?: priv->tail_vma;\n}\n\nstatic void m_cache_vma(struct seq_file *m, struct vm_area_struct *vma)\n{\n\tif (m->count < m->size)\t/* vma is copied successfully */\n\t\tm->version = m_next_vma(m->private, vma) ? vma->vm_end : -1UL;\n}\n\nstatic void *m_start(struct seq_file *m, loff_t *ppos)\n{\n\tstruct proc_maps_private *priv = m->private;\n\tunsigned long last_addr = m->version;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tunsigned int pos = *ppos;\n\n\t/* See m_cache_vma(). Zero at the start or after lseek. */\n\tif (last_addr == -1UL)\n\t\treturn NULL;\n\n\tpriv->task = get_proc_task(priv->inode);\n\tif (!priv->task)\n\t\treturn ERR_PTR(-ESRCH);\n\n\tmm = priv->mm;\n\tif (!mm || !mmget_not_zero(mm))\n\t\treturn NULL;\n\n\tdown_read(&mm->mmap_sem);\n\thold_task_mempolicy(priv);\n\tpriv->tail_vma = get_gate_vma(mm);\n\n\tif (last_addr) {\n\t\tvma = find_vma(mm, last_addr - 1);\n\t\tif (vma && vma->vm_start <= last_addr)\n\t\t\tvma = m_next_vma(priv, vma);\n\t\tif (vma)\n\t\t\treturn vma;\n\t}\n\n\tm->version = 0;\n\tif (pos < mm->map_count) {\n\t\tfor (vma = mm->mmap; pos; pos--) {\n\t\t\tm->version = vma->vm_start;\n\t\t\tvma = vma->vm_next;\n\t\t}\n\t\treturn vma;\n\t}\n\n\t/* we do not bother to update m->version in this case */\n\tif (pos == mm->map_count && priv->tail_vma)\n\t\treturn priv->tail_vma;\n\n\tvma_stop(priv);\n\treturn NULL;\n}\n\nstatic void *m_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct proc_maps_private *priv = m->private;\n\tstruct vm_area_struct *next;\n\n\t(*pos)++;\n\tnext = m_next_vma(priv, v);\n\tif (!next)\n\t\tvma_stop(priv);\n\treturn next;\n}\n\nstatic void m_stop(struct seq_file *m, void *v)\n{\n\tstruct proc_maps_private *priv = m->private;\n\n\tif (!IS_ERR_OR_NULL(v))\n\t\tvma_stop(priv);\n\tif (priv->task) {\n\t\tput_task_struct(priv->task);\n\t\tpriv->task = NULL;\n\t}\n}\n\nstatic int proc_maps_open(struct inode *inode, struct file *file,\n\t\t\tconst struct seq_operations *ops, int psize)\n{\n\tstruct proc_maps_private *priv = __seq_open_private(file, ops, psize);\n\n\tif (!priv)\n\t\treturn -ENOMEM;\n\n\tpriv->inode = inode;\n\tpriv->mm = proc_mem_open(inode, PTRACE_MODE_READ);\n\tif (IS_ERR(priv->mm)) {\n\t\tint err = PTR_ERR(priv->mm);\n\n\t\tseq_release_private(inode, file);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int proc_map_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *seq = file->private_data;\n\tstruct proc_maps_private *priv = seq->private;\n\n\tif (priv->mm)\n\t\tmmdrop(priv->mm);\n\n\treturn seq_release_private(inode, file);\n}\n\nstatic int do_maps_open(struct inode *inode, struct file *file,\n\t\t\tconst struct seq_operations *ops)\n{\n\treturn proc_maps_open(inode, file, ops,\n\t\t\t\tsizeof(struct proc_maps_private));\n}\n\n/*\n * Indicate if the VMA is a stack for the given task; for\n * /proc/PID/maps that is the stack of the main task.\n */\nstatic int is_stack(struct vm_area_struct *vma)\n{\n\t/*\n\t * We make no effort to guess what a given thread considers to be\n\t * its \"stack\".  It's not even well-defined for programs written\n\t * languages like Go.\n\t */\n\treturn vma->vm_start <= vma->vm_mm->start_stack &&\n\t\tvma->vm_end >= vma->vm_mm->start_stack;\n}\n\nstatic void show_vma_header_prefix(struct seq_file *m,\n\t\t\t\t   unsigned long start, unsigned long end,\n\t\t\t\t   vm_flags_t flags, unsigned long long pgoff,\n\t\t\t\t   dev_t dev, unsigned long ino)\n{\n\tseq_setwidth(m, 25 + sizeof(void *) * 6 - 1);\n\tseq_put_hex_ll(m, NULL, start, 8);\n\tseq_put_hex_ll(m, \"-\", end, 8);\n\tseq_putc(m, ' ');\n\tseq_putc(m, flags & VM_READ ? 'r' : '-');\n\tseq_putc(m, flags & VM_WRITE ? 'w' : '-');\n\tseq_putc(m, flags & VM_EXEC ? 'x' : '-');\n\tseq_putc(m, flags & VM_MAYSHARE ? 's' : 'p');\n\tseq_put_hex_ll(m, \" \", pgoff, 8);\n\tseq_put_hex_ll(m, \" \", MAJOR(dev), 2);\n\tseq_put_hex_ll(m, \":\", MINOR(dev), 2);\n\tseq_put_decimal_ull(m, \" \", ino);\n\tseq_putc(m, ' ');\n}\n\nstatic void\nshow_map_vma(struct seq_file *m, struct vm_area_struct *vma)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct file *file = vma->vm_file;\n\tvm_flags_t flags = vma->vm_flags;\n\tunsigned long ino = 0;\n\tunsigned long long pgoff = 0;\n\tunsigned long start, end;\n\tdev_t dev = 0;\n\tconst char *name = NULL;\n\n\tif (file) {\n\t\tstruct inode *inode = file_inode(vma->vm_file);\n\t\tdev = inode->i_sb->s_dev;\n\t\tino = inode->i_ino;\n\t\tpgoff = ((loff_t)vma->vm_pgoff) << PAGE_SHIFT;\n\t}\n\n\tstart = vma->vm_start;\n\tend = vma->vm_end;\n\tshow_vma_header_prefix(m, start, end, flags, pgoff, dev, ino);\n\n\t/*\n\t * Print the dentry name for named mappings, and a\n\t * special [heap] marker for the heap:\n\t */\n\tif (file) {\n\t\tseq_pad(m, ' ');\n\t\tseq_file_path(m, file, \"\\n\");\n\t\tgoto done;\n\t}\n\n\tif (vma->vm_ops && vma->vm_ops->name) {\n\t\tname = vma->vm_ops->name(vma);\n\t\tif (name)\n\t\t\tgoto done;\n\t}\n\n\tname = arch_vma_name(vma);\n\tif (!name) {\n\t\tif (!mm) {\n\t\t\tname = \"[vdso]\";\n\t\t\tgoto done;\n\t\t}\n\n\t\tif (vma->vm_start <= mm->brk &&\n\t\t    vma->vm_end >= mm->start_brk) {\n\t\t\tname = \"[heap]\";\n\t\t\tgoto done;\n\t\t}\n\n\t\tif (is_stack(vma))\n\t\t\tname = \"[stack]\";\n\t}\n\ndone:\n\tif (name) {\n\t\tseq_pad(m, ' ');\n\t\tseq_puts(m, name);\n\t}\n\tseq_putc(m, '\\n');\n}\n\nstatic int show_map(struct seq_file *m, void *v)\n{\n\tshow_map_vma(m, v);\n\tm_cache_vma(m, v);\n\treturn 0;\n}\n\nstatic const struct seq_operations proc_pid_maps_op = {\n\t.start\t= m_start,\n\t.next\t= m_next,\n\t.stop\t= m_stop,\n\t.show\t= show_map\n};\n\nstatic int pid_maps_open(struct inode *inode, struct file *file)\n{\n\treturn do_maps_open(inode, file, &proc_pid_maps_op);\n}\n\nconst struct file_operations proc_pid_maps_operations = {\n\t.open\t\t= pid_maps_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= proc_map_release,\n};\n\n/*\n * Proportional Set Size(PSS): my share of RSS.\n *\n * PSS of a process is the count of pages it has in memory, where each\n * page is divided by the number of processes sharing it.  So if a\n * process has 1000 pages all to itself, and 1000 shared with one other\n * process, its PSS will be 1500.\n *\n * To keep (accumulated) division errors low, we adopt a 64bit\n * fixed-point pss counter to minimize division errors. So (pss >>\n * PSS_SHIFT) would be the real byte count.\n *\n * A shift of 12 before division means (assuming 4K page size):\n * \t- 1M 3-user-pages add up to 8KB errors;\n * \t- supports mapcount up to 2^24, or 16M;\n * \t- supports PSS up to 2^52 bytes, or 4PB.\n */\n#define PSS_SHIFT 12\n\n#ifdef CONFIG_PROC_PAGE_MONITOR\nstruct mem_size_stats {\n\tunsigned long resident;\n\tunsigned long shared_clean;\n\tunsigned long shared_dirty;\n\tunsigned long private_clean;\n\tunsigned long private_dirty;\n\tunsigned long referenced;\n\tunsigned long anonymous;\n\tunsigned long lazyfree;\n\tunsigned long anonymous_thp;\n\tunsigned long shmem_thp;\n\tunsigned long swap;\n\tunsigned long shared_hugetlb;\n\tunsigned long private_hugetlb;\n\tu64 pss;\n\tu64 pss_locked;\n\tu64 swap_pss;\n\tbool check_shmem_swap;\n};\n\nstatic void smaps_account(struct mem_size_stats *mss, struct page *page,\n\t\tbool compound, bool young, bool dirty, bool locked)\n{\n\tint i, nr = compound ? 1 << compound_order(page) : 1;\n\tunsigned long size = nr * PAGE_SIZE;\n\n\tif (PageAnon(page)) {\n\t\tmss->anonymous += size;\n\t\tif (!PageSwapBacked(page) && !dirty && !PageDirty(page))\n\t\t\tmss->lazyfree += size;\n\t}\n\n\tmss->resident += size;\n\t/* Accumulate the size in pages that have been accessed. */\n\tif (young || page_is_young(page) || PageReferenced(page))\n\t\tmss->referenced += size;\n\n\t/*\n\t * page_count(page) == 1 guarantees the page is mapped exactly once.\n\t * If any subpage of the compound page mapped with PTE it would elevate\n\t * page_count().\n\t */\n\tif (page_count(page) == 1) {\n\t\tif (dirty || PageDirty(page))\n\t\t\tmss->private_dirty += size;\n\t\telse\n\t\t\tmss->private_clean += size;\n\t\tmss->pss += (u64)size << PSS_SHIFT;\n\t\tif (locked)\n\t\t\tmss->pss_locked += (u64)size << PSS_SHIFT;\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < nr; i++, page++) {\n\t\tint mapcount = page_mapcount(page);\n\t\tunsigned long pss = (PAGE_SIZE << PSS_SHIFT);\n\n\t\tif (mapcount >= 2) {\n\t\t\tif (dirty || PageDirty(page))\n\t\t\t\tmss->shared_dirty += PAGE_SIZE;\n\t\t\telse\n\t\t\t\tmss->shared_clean += PAGE_SIZE;\n\t\t\tmss->pss += pss / mapcount;\n\t\t\tif (locked)\n\t\t\t\tmss->pss_locked += pss / mapcount;\n\t\t} else {\n\t\t\tif (dirty || PageDirty(page))\n\t\t\t\tmss->private_dirty += PAGE_SIZE;\n\t\t\telse\n\t\t\t\tmss->private_clean += PAGE_SIZE;\n\t\t\tmss->pss += pss;\n\t\t\tif (locked)\n\t\t\t\tmss->pss_locked += pss;\n\t\t}\n\t}\n}\n\n#ifdef CONFIG_SHMEM\nstatic int smaps_pte_hole(unsigned long addr, unsigned long end,\n\t\tstruct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\n\tmss->swap += shmem_partial_swap_usage(\n\t\t\twalk->vma->vm_file->f_mapping, addr, end);\n\n\treturn 0;\n}\n#endif\n\nstatic void smaps_pte_entry(pte_t *pte, unsigned long addr,\n\t\tstruct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tbool locked = !!(vma->vm_flags & VM_LOCKED);\n\tstruct page *page = NULL;\n\n\tif (pte_present(*pte)) {\n\t\tpage = vm_normal_page(vma, addr, *pte);\n\t} else if (is_swap_pte(*pte)) {\n\t\tswp_entry_t swpent = pte_to_swp_entry(*pte);\n\n\t\tif (!non_swap_entry(swpent)) {\n\t\t\tint mapcount;\n\n\t\t\tmss->swap += PAGE_SIZE;\n\t\t\tmapcount = swp_swapcount(swpent);\n\t\t\tif (mapcount >= 2) {\n\t\t\t\tu64 pss_delta = (u64)PAGE_SIZE << PSS_SHIFT;\n\n\t\t\t\tdo_div(pss_delta, mapcount);\n\t\t\t\tmss->swap_pss += pss_delta;\n\t\t\t} else {\n\t\t\t\tmss->swap_pss += (u64)PAGE_SIZE << PSS_SHIFT;\n\t\t\t}\n\t\t} else if (is_migration_entry(swpent))\n\t\t\tpage = migration_entry_to_page(swpent);\n\t\telse if (is_device_private_entry(swpent))\n\t\t\tpage = device_private_entry_to_page(swpent);\n\t} else if (unlikely(IS_ENABLED(CONFIG_SHMEM) && mss->check_shmem_swap\n\t\t\t\t\t\t\t&& pte_none(*pte))) {\n\t\tpage = find_get_entry(vma->vm_file->f_mapping,\n\t\t\t\t\t\tlinear_page_index(vma, addr));\n\t\tif (!page)\n\t\t\treturn;\n\n\t\tif (xa_is_value(page))\n\t\t\tmss->swap += PAGE_SIZE;\n\t\telse\n\t\t\tput_page(page);\n\n\t\treturn;\n\t}\n\n\tif (!page)\n\t\treturn;\n\n\tsmaps_account(mss, page, false, pte_young(*pte), pte_dirty(*pte), locked);\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic void smaps_pmd_entry(pmd_t *pmd, unsigned long addr,\n\t\tstruct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tbool locked = !!(vma->vm_flags & VM_LOCKED);\n\tstruct page *page;\n\n\t/* FOLL_DUMP will return -EFAULT on huge zero page */\n\tpage = follow_trans_huge_pmd(vma, addr, pmd, FOLL_DUMP);\n\tif (IS_ERR_OR_NULL(page))\n\t\treturn;\n\tif (PageAnon(page))\n\t\tmss->anonymous_thp += HPAGE_PMD_SIZE;\n\telse if (PageSwapBacked(page))\n\t\tmss->shmem_thp += HPAGE_PMD_SIZE;\n\telse if (is_zone_device_page(page))\n\t\t/* pass */;\n\telse\n\t\tVM_BUG_ON_PAGE(1, page);\n\tsmaps_account(mss, page, true, pmd_young(*pmd), pmd_dirty(*pmd), locked);\n}\n#else\nstatic void smaps_pmd_entry(pmd_t *pmd, unsigned long addr,\n\t\tstruct mm_walk *walk)\n{\n}\n#endif\n\nstatic int smaps_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,\n\t\t\t   struct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->vma;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\tptl = pmd_trans_huge_lock(pmd, vma);\n\tif (ptl) {\n\t\tif (pmd_present(*pmd))\n\t\t\tsmaps_pmd_entry(pmd, addr, walk);\n\t\tspin_unlock(ptl);\n\t\tgoto out;\n\t}\n\n\tif (pmd_trans_unstable(pmd))\n\t\tgoto out;\n\t/*\n\t * The mmap_sem held all the way back in m_start() is what\n\t * keeps khugepaged out of here and from collapsing things\n\t * in here.\n\t */\n\tpte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\n\tfor (; addr != end; pte++, addr += PAGE_SIZE)\n\t\tsmaps_pte_entry(pte, addr, walk);\n\tpte_unmap_unlock(pte - 1, ptl);\nout:\n\tcond_resched();\n\treturn 0;\n}\n\nstatic void show_smap_vma_flags(struct seq_file *m, struct vm_area_struct *vma)\n{\n\t/*\n\t * Don't forget to update Documentation/ on changes.\n\t */\n\tstatic const char mnemonics[BITS_PER_LONG][2] = {\n\t\t/*\n\t\t * In case if we meet a flag we don't know about.\n\t\t */\n\t\t[0 ... (BITS_PER_LONG-1)] = \"??\",\n\n\t\t[ilog2(VM_READ)]\t= \"rd\",\n\t\t[ilog2(VM_WRITE)]\t= \"wr\",\n\t\t[ilog2(VM_EXEC)]\t= \"ex\",\n\t\t[ilog2(VM_SHARED)]\t= \"sh\",\n\t\t[ilog2(VM_MAYREAD)]\t= \"mr\",\n\t\t[ilog2(VM_MAYWRITE)]\t= \"mw\",\n\t\t[ilog2(VM_MAYEXEC)]\t= \"me\",\n\t\t[ilog2(VM_MAYSHARE)]\t= \"ms\",\n\t\t[ilog2(VM_GROWSDOWN)]\t= \"gd\",\n\t\t[ilog2(VM_PFNMAP)]\t= \"pf\",\n\t\t[ilog2(VM_DENYWRITE)]\t= \"dw\",\n#ifdef CONFIG_X86_INTEL_MPX\n\t\t[ilog2(VM_MPX)]\t\t= \"mp\",\n#endif\n\t\t[ilog2(VM_LOCKED)]\t= \"lo\",\n\t\t[ilog2(VM_IO)]\t\t= \"io\",\n\t\t[ilog2(VM_SEQ_READ)]\t= \"sr\",\n\t\t[ilog2(VM_RAND_READ)]\t= \"rr\",\n\t\t[ilog2(VM_DONTCOPY)]\t= \"dc\",\n\t\t[ilog2(VM_DONTEXPAND)]\t= \"de\",\n\t\t[ilog2(VM_ACCOUNT)]\t= \"ac\",\n\t\t[ilog2(VM_NORESERVE)]\t= \"nr\",\n\t\t[ilog2(VM_HUGETLB)]\t= \"ht\",\n\t\t[ilog2(VM_SYNC)]\t= \"sf\",\n\t\t[ilog2(VM_ARCH_1)]\t= \"ar\",\n\t\t[ilog2(VM_WIPEONFORK)]\t= \"wf\",\n\t\t[ilog2(VM_DONTDUMP)]\t= \"dd\",\n#ifdef CONFIG_MEM_SOFT_DIRTY\n\t\t[ilog2(VM_SOFTDIRTY)]\t= \"sd\",\n#endif\n\t\t[ilog2(VM_MIXEDMAP)]\t= \"mm\",\n\t\t[ilog2(VM_HUGEPAGE)]\t= \"hg\",\n\t\t[ilog2(VM_NOHUGEPAGE)]\t= \"nh\",\n\t\t[ilog2(VM_MERGEABLE)]\t= \"mg\",\n\t\t[ilog2(VM_UFFD_MISSING)]= \"um\",\n\t\t[ilog2(VM_UFFD_WP)]\t= \"uw\",\n#ifdef CONFIG_ARCH_HAS_PKEYS\n\t\t/* These come out via ProtectionKey: */\n\t\t[ilog2(VM_PKEY_BIT0)]\t= \"\",\n\t\t[ilog2(VM_PKEY_BIT1)]\t= \"\",\n\t\t[ilog2(VM_PKEY_BIT2)]\t= \"\",\n\t\t[ilog2(VM_PKEY_BIT3)]\t= \"\",\n#if VM_PKEY_BIT4\n\t\t[ilog2(VM_PKEY_BIT4)]\t= \"\",\n#endif\n#endif /* CONFIG_ARCH_HAS_PKEYS */\n\t};\n\tsize_t i;\n\n\tseq_puts(m, \"VmFlags: \");\n\tfor (i = 0; i < BITS_PER_LONG; i++) {\n\t\tif (!mnemonics[i][0])\n\t\t\tcontinue;\n\t\tif (vma->vm_flags & (1UL << i)) {\n\t\t\tseq_putc(m, mnemonics[i][0]);\n\t\t\tseq_putc(m, mnemonics[i][1]);\n\t\t\tseq_putc(m, ' ');\n\t\t}\n\t}\n\tseq_putc(m, '\\n');\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\nstatic int smaps_hugetlb_range(pte_t *pte, unsigned long hmask,\n\t\t\t\t unsigned long addr, unsigned long end,\n\t\t\t\t struct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tstruct page *page = NULL;\n\n\tif (pte_present(*pte)) {\n\t\tpage = vm_normal_page(vma, addr, *pte);\n\t} else if (is_swap_pte(*pte)) {\n\t\tswp_entry_t swpent = pte_to_swp_entry(*pte);\n\n\t\tif (is_migration_entry(swpent))\n\t\t\tpage = migration_entry_to_page(swpent);\n\t\telse if (is_device_private_entry(swpent))\n\t\t\tpage = device_private_entry_to_page(swpent);\n\t}\n\tif (page) {\n\t\tint mapcount = page_mapcount(page);\n\n\t\tif (mapcount >= 2)\n\t\t\tmss->shared_hugetlb += huge_page_size(hstate_vma(vma));\n\t\telse\n\t\t\tmss->private_hugetlb += huge_page_size(hstate_vma(vma));\n\t}\n\treturn 0;\n}\n#endif /* HUGETLB_PAGE */\n\nstatic void smap_gather_stats(struct vm_area_struct *vma,\n\t\t\t     struct mem_size_stats *mss)\n{\n\tstruct mm_walk smaps_walk = {\n\t\t.pmd_entry = smaps_pte_range,\n#ifdef CONFIG_HUGETLB_PAGE\n\t\t.hugetlb_entry = smaps_hugetlb_range,\n#endif\n\t\t.mm = vma->vm_mm,\n\t};\n\n\tsmaps_walk.private = mss;\n\n#ifdef CONFIG_SHMEM\n\t/* In case of smaps_rollup, reset the value from previous vma */\n\tmss->check_shmem_swap = false;\n\tif (vma->vm_file && shmem_mapping(vma->vm_file->f_mapping)) {\n\t\t/*\n\t\t * For shared or readonly shmem mappings we know that all\n\t\t * swapped out pages belong to the shmem object, and we can\n\t\t * obtain the swap value much more efficiently. For private\n\t\t * writable mappings, we might have COW pages that are\n\t\t * not affected by the parent swapped out pages of the shmem\n\t\t * object, so we have to distinguish them during the page walk.\n\t\t * Unless we know that the shmem object (or the part mapped by\n\t\t * our VMA) has no swapped out pages at all.\n\t\t */\n\t\tunsigned long shmem_swapped = shmem_swap_usage(vma);\n\n\t\tif (!shmem_swapped || (vma->vm_flags & VM_SHARED) ||\n\t\t\t\t\t!(vma->vm_flags & VM_WRITE)) {\n\t\t\tmss->swap += shmem_swapped;\n\t\t} else {\n\t\t\tmss->check_shmem_swap = true;\n\t\t\tsmaps_walk.pte_hole = smaps_pte_hole;\n\t\t}\n\t}\n#endif\n\t/* mmap_sem is held in m_start */\n\twalk_page_vma(vma, &smaps_walk);\n}\n\n#define SEQ_PUT_DEC(str, val) \\\n\t\tseq_put_decimal_ull_width(m, str, (val) >> 10, 8)\n\n/* Show the contents common for smaps and smaps_rollup */\nstatic void __show_smap(struct seq_file *m, const struct mem_size_stats *mss)\n{\n\tSEQ_PUT_DEC(\"Rss:            \", mss->resident);\n\tSEQ_PUT_DEC(\" kB\\nPss:            \", mss->pss >> PSS_SHIFT);\n\tSEQ_PUT_DEC(\" kB\\nShared_Clean:   \", mss->shared_clean);\n\tSEQ_PUT_DEC(\" kB\\nShared_Dirty:   \", mss->shared_dirty);\n\tSEQ_PUT_DEC(\" kB\\nPrivate_Clean:  \", mss->private_clean);\n\tSEQ_PUT_DEC(\" kB\\nPrivate_Dirty:  \", mss->private_dirty);\n\tSEQ_PUT_DEC(\" kB\\nReferenced:     \", mss->referenced);\n\tSEQ_PUT_DEC(\" kB\\nAnonymous:      \", mss->anonymous);\n\tSEQ_PUT_DEC(\" kB\\nLazyFree:       \", mss->lazyfree);\n\tSEQ_PUT_DEC(\" kB\\nAnonHugePages:  \", mss->anonymous_thp);\n\tSEQ_PUT_DEC(\" kB\\nShmemPmdMapped: \", mss->shmem_thp);\n\tSEQ_PUT_DEC(\" kB\\nShared_Hugetlb: \", mss->shared_hugetlb);\n\tseq_put_decimal_ull_width(m, \" kB\\nPrivate_Hugetlb: \",\n\t\t\t\t  mss->private_hugetlb >> 10, 7);\n\tSEQ_PUT_DEC(\" kB\\nSwap:           \", mss->swap);\n\tSEQ_PUT_DEC(\" kB\\nSwapPss:        \",\n\t\t\t\t\tmss->swap_pss >> PSS_SHIFT);\n\tSEQ_PUT_DEC(\" kB\\nLocked:         \",\n\t\t\t\t\tmss->pss_locked >> PSS_SHIFT);\n\tseq_puts(m, \" kB\\n\");\n}\n\nstatic int show_smap(struct seq_file *m, void *v)\n{\n\tstruct vm_area_struct *vma = v;\n\tstruct mem_size_stats mss;\n\n\tmemset(&mss, 0, sizeof(mss));\n\n\tsmap_gather_stats(vma, &mss);\n\n\tshow_map_vma(m, vma);\n\n\tSEQ_PUT_DEC(\"Size:           \", vma->vm_end - vma->vm_start);\n\tSEQ_PUT_DEC(\" kB\\nKernelPageSize: \", vma_kernel_pagesize(vma));\n\tSEQ_PUT_DEC(\" kB\\nMMUPageSize:    \", vma_mmu_pagesize(vma));\n\tseq_puts(m, \" kB\\n\");\n\n\t__show_smap(m, &mss);\n\n\tseq_printf(m, \"THPeligible:    %d\\n\", transparent_hugepage_enabled(vma));\n\n\tif (arch_pkeys_enabled())\n\t\tseq_printf(m, \"ProtectionKey:  %8u\\n\", vma_pkey(vma));\n\tshow_smap_vma_flags(m, vma);\n\n\tm_cache_vma(m, vma);\n\n\treturn 0;\n}\n\nstatic int show_smaps_rollup(struct seq_file *m, void *v)\n{\n\tstruct proc_maps_private *priv = m->private;\n\tstruct mem_size_stats mss;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long last_vma_end = 0;\n\tint ret = 0;\n\n\tpriv->task = get_proc_task(priv->inode);\n\tif (!priv->task)\n\t\treturn -ESRCH;\n\n\tmm = priv->mm;\n\tif (!mm || !mmget_not_zero(mm)) {\n\t\tret = -ESRCH;\n\t\tgoto out_put_task;\n\t}\n\n\tmemset(&mss, 0, sizeof(mss));\n\n\tdown_read(&mm->mmap_sem);\n\thold_task_mempolicy(priv);\n\n\tfor (vma = priv->mm->mmap; vma; vma = vma->vm_next) {\n\t\tsmap_gather_stats(vma, &mss);\n\t\tlast_vma_end = vma->vm_end;\n\t}\n\n\tshow_vma_header_prefix(m, priv->mm->mmap->vm_start,\n\t\t\t       last_vma_end, 0, 0, 0, 0);\n\tseq_pad(m, ' ');\n\tseq_puts(m, \"[rollup]\\n\");\n\n\t__show_smap(m, &mss);\n\n\trelease_task_mempolicy(priv);\n\tup_read(&mm->mmap_sem);\n\tmmput(mm);\n\nout_put_task:\n\tput_task_struct(priv->task);\n\tpriv->task = NULL;\n\n\treturn ret;\n}\n#undef SEQ_PUT_DEC\n\nstatic const struct seq_operations proc_pid_smaps_op = {\n\t.start\t= m_start,\n\t.next\t= m_next,\n\t.stop\t= m_stop,\n\t.show\t= show_smap\n};\n\nstatic int pid_smaps_open(struct inode *inode, struct file *file)\n{\n\treturn do_maps_open(inode, file, &proc_pid_smaps_op);\n}\n\nstatic int smaps_rollup_open(struct inode *inode, struct file *file)\n{\n\tint ret;\n\tstruct proc_maps_private *priv;\n\n\tpriv = kzalloc(sizeof(*priv), GFP_KERNEL_ACCOUNT);\n\tif (!priv)\n\t\treturn -ENOMEM;\n\n\tret = single_open(file, show_smaps_rollup, priv);\n\tif (ret)\n\t\tgoto out_free;\n\n\tpriv->inode = inode;\n\tpriv->mm = proc_mem_open(inode, PTRACE_MODE_READ);\n\tif (IS_ERR(priv->mm)) {\n\t\tret = PTR_ERR(priv->mm);\n\n\t\tsingle_release(inode, file);\n\t\tgoto out_free;\n\t}\n\n\treturn 0;\n\nout_free:\n\tkfree(priv);\n\treturn ret;\n}\n\nstatic int smaps_rollup_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *seq = file->private_data;\n\tstruct proc_maps_private *priv = seq->private;\n\n\tif (priv->mm)\n\t\tmmdrop(priv->mm);\n\n\tkfree(priv);\n\treturn single_release(inode, file);\n}\n\nconst struct file_operations proc_pid_smaps_operations = {\n\t.open\t\t= pid_smaps_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= proc_map_release,\n};\n\nconst struct file_operations proc_pid_smaps_rollup_operations = {\n\t.open\t\t= smaps_rollup_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= smaps_rollup_release,\n};\n\nenum clear_refs_types {\n\tCLEAR_REFS_ALL = 1,\n\tCLEAR_REFS_ANON,\n\tCLEAR_REFS_MAPPED,\n\tCLEAR_REFS_SOFT_DIRTY,\n\tCLEAR_REFS_MM_HIWATER_RSS,\n\tCLEAR_REFS_LAST,\n};\n\nstruct clear_refs_private {\n\tenum clear_refs_types type;\n};\n\n#ifdef CONFIG_MEM_SOFT_DIRTY\nstatic inline void clear_soft_dirty(struct vm_area_struct *vma,\n\t\tunsigned long addr, pte_t *pte)\n{\n\t/*\n\t * The soft-dirty tracker uses #PF-s to catch writes\n\t * to pages, so write-protect the pte as well. See the\n\t * Documentation/admin-guide/mm/soft-dirty.rst for full description\n\t * of how soft-dirty works.\n\t */\n\tpte_t ptent = *pte;\n\n\tif (pte_present(ptent)) {\n\t\tpte_t old_pte;\n\n\t\told_pte = ptep_modify_prot_start(vma, addr, pte);\n\t\tptent = pte_wrprotect(old_pte);\n\t\tptent = pte_clear_soft_dirty(ptent);\n\t\tptep_modify_prot_commit(vma, addr, pte, old_pte, ptent);\n\t} else if (is_swap_pte(ptent)) {\n\t\tptent = pte_swp_clear_soft_dirty(ptent);\n\t\tset_pte_at(vma->vm_mm, addr, pte, ptent);\n\t}\n}\n#else\nstatic inline void clear_soft_dirty(struct vm_area_struct *vma,\n\t\tunsigned long addr, pte_t *pte)\n{\n}\n#endif\n\n#if defined(CONFIG_MEM_SOFT_DIRTY) && defined(CONFIG_TRANSPARENT_HUGEPAGE)\nstatic inline void clear_soft_dirty_pmd(struct vm_area_struct *vma,\n\t\tunsigned long addr, pmd_t *pmdp)\n{\n\tpmd_t old, pmd = *pmdp;\n\n\tif (pmd_present(pmd)) {\n\t\t/* See comment in change_huge_pmd() */\n\t\told = pmdp_invalidate(vma, addr, pmdp);\n\t\tif (pmd_dirty(old))\n\t\t\tpmd = pmd_mkdirty(pmd);\n\t\tif (pmd_young(old))\n\t\t\tpmd = pmd_mkyoung(pmd);\n\n\t\tpmd = pmd_wrprotect(pmd);\n\t\tpmd = pmd_clear_soft_dirty(pmd);\n\n\t\tset_pmd_at(vma->vm_mm, addr, pmdp, pmd);\n\t} else if (is_migration_entry(pmd_to_swp_entry(pmd))) {\n\t\tpmd = pmd_swp_clear_soft_dirty(pmd);\n\t\tset_pmd_at(vma->vm_mm, addr, pmdp, pmd);\n\t}\n}\n#else\nstatic inline void clear_soft_dirty_pmd(struct vm_area_struct *vma,\n\t\tunsigned long addr, pmd_t *pmdp)\n{\n}\n#endif\n\nstatic int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,\n\t\t\t\tunsigned long end, struct mm_walk *walk)\n{\n\tstruct clear_refs_private *cp = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tpte_t *pte, ptent;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\n\tptl = pmd_trans_huge_lock(pmd, vma);\n\tif (ptl) {\n\t\tif (cp->type == CLEAR_REFS_SOFT_DIRTY) {\n\t\t\tclear_soft_dirty_pmd(vma, addr, pmd);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto out;\n\n\t\tpage = pmd_page(*pmd);\n\n\t\t/* Clear accessed and referenced bits. */\n\t\tpmdp_test_and_clear_young(vma, addr, pmd);\n\t\ttest_and_clear_page_young(page);\n\t\tClearPageReferenced(page);\nout:\n\t\tspin_unlock(ptl);\n\t\treturn 0;\n\t}\n\n\tif (pmd_trans_unstable(pmd))\n\t\treturn 0;\n\n\tpte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\n\tfor (; addr != end; pte++, addr += PAGE_SIZE) {\n\t\tptent = *pte;\n\n\t\tif (cp->type == CLEAR_REFS_SOFT_DIRTY) {\n\t\t\tclear_soft_dirty(vma, addr, pte);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!pte_present(ptent))\n\t\t\tcontinue;\n\n\t\tpage = vm_normal_page(vma, addr, ptent);\n\t\tif (!page)\n\t\t\tcontinue;\n\n\t\t/* Clear accessed and referenced bits. */\n\t\tptep_test_and_clear_young(vma, addr, pte);\n\t\ttest_and_clear_page_young(page);\n\t\tClearPageReferenced(page);\n\t}\n\tpte_unmap_unlock(pte - 1, ptl);\n\tcond_resched();\n\treturn 0;\n}\n\nstatic int clear_refs_test_walk(unsigned long start, unsigned long end,\n\t\t\t\tstruct mm_walk *walk)\n{\n\tstruct clear_refs_private *cp = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\n\tif (vma->vm_flags & VM_PFNMAP)\n\t\treturn 1;\n\n\t/*\n\t * Writing 1 to /proc/pid/clear_refs affects all pages.\n\t * Writing 2 to /proc/pid/clear_refs only affects anonymous pages.\n\t * Writing 3 to /proc/pid/clear_refs only affects file mapped pages.\n\t * Writing 4 to /proc/pid/clear_refs affects all pages.\n\t */\n\tif (cp->type == CLEAR_REFS_ANON && vma->vm_file)\n\t\treturn 1;\n\tif (cp->type == CLEAR_REFS_MAPPED && !vma->vm_file)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic ssize_t clear_refs_write(struct file *file, const char __user *buf,\n\t\t\t\tsize_t count, loff_t *ppos)\n{\n\tstruct task_struct *task;\n\tchar buffer[PROC_NUMBUF];\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tenum clear_refs_types type;\n\tstruct mmu_gather tlb;\n\tint itype;\n\tint rv;\n\n\tmemset(buffer, 0, sizeof(buffer));\n\tif (count > sizeof(buffer) - 1)\n\t\tcount = sizeof(buffer) - 1;\n\tif (copy_from_user(buffer, buf, count))\n\t\treturn -EFAULT;\n\trv = kstrtoint(strstrip(buffer), 10, &itype);\n\tif (rv < 0)\n\t\treturn rv;\n\ttype = (enum clear_refs_types)itype;\n\tif (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)\n\t\treturn -EINVAL;\n\n\ttask = get_proc_task(file_inode(file));\n\tif (!task)\n\t\treturn -ESRCH;\n\tmm = get_task_mm(task);\n\tif (mm) {\n\t\tstruct mmu_notifier_range range;\n\t\tstruct clear_refs_private cp = {\n\t\t\t.type = type,\n\t\t};\n\t\tstruct mm_walk clear_refs_walk = {\n\t\t\t.pmd_entry = clear_refs_pte_range,\n\t\t\t.test_walk = clear_refs_test_walk,\n\t\t\t.mm = mm,\n\t\t\t.private = &cp,\n\t\t};\n\n\t\tif (type == CLEAR_REFS_MM_HIWATER_RSS) {\n\t\t\tif (down_write_killable(&mm->mmap_sem)) {\n\t\t\t\tcount = -EINTR;\n\t\t\t\tgoto out_mm;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Writing 5 to /proc/pid/clear_refs resets the peak\n\t\t\t * resident set size to this mm's current rss value.\n\t\t\t */\n\t\t\treset_mm_hiwater_rss(mm);\n\t\t\tup_write(&mm->mmap_sem);\n\t\t\tgoto out_mm;\n\t\t}\n\n\t\tdown_read(&mm->mmap_sem);\n\t\ttlb_gather_mmu(&tlb, mm, 0, -1);\n\t\tif (type == CLEAR_REFS_SOFT_DIRTY) {\n\t\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\t\t\tif (!(vma->vm_flags & VM_SOFTDIRTY))\n\t\t\t\t\tcontinue;\n\t\t\t\tup_read(&mm->mmap_sem);\n\t\t\t\tif (down_write_killable(&mm->mmap_sem)) {\n\t\t\t\t\tcount = -EINTR;\n\t\t\t\t\tgoto out_mm;\n\t\t\t\t}\n\t\t\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\t\t\t\tvma->vm_flags &= ~VM_SOFTDIRTY;\n\t\t\t\t\tvma_set_page_prot(vma);\n\t\t\t\t}\n\t\t\t\tdowngrade_write(&mm->mmap_sem);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tmmu_notifier_range_init(&range, mm, 0, -1UL);\n\t\t\tmmu_notifier_invalidate_range_start(&range);\n\t\t}\n\t\twalk_page_range(0, mm->highest_vm_end, &clear_refs_walk);\n\t\tif (type == CLEAR_REFS_SOFT_DIRTY)\n\t\t\tmmu_notifier_invalidate_range_end(&range);\n\t\ttlb_finish_mmu(&tlb, 0, -1);\n\t\tup_read(&mm->mmap_sem);\nout_mm:\n\t\tmmput(mm);\n\t}\n\tput_task_struct(task);\n\n\treturn count;\n}\n\nconst struct file_operations proc_clear_refs_operations = {\n\t.write\t\t= clear_refs_write,\n\t.llseek\t\t= noop_llseek,\n};\n\ntypedef struct {\n\tu64 pme;\n} pagemap_entry_t;\n\nstruct pagemapread {\n\tint pos, len;\t\t/* units: PM_ENTRY_BYTES, not bytes */\n\tpagemap_entry_t *buffer;\n\tbool show_pfn;\n};\n\n#define PAGEMAP_WALK_SIZE\t(PMD_SIZE)\n#define PAGEMAP_WALK_MASK\t(PMD_MASK)\n\n#define PM_ENTRY_BYTES\t\tsizeof(pagemap_entry_t)\n#define PM_PFRAME_BITS\t\t55\n#define PM_PFRAME_MASK\t\tGENMASK_ULL(PM_PFRAME_BITS - 1, 0)\n#define PM_SOFT_DIRTY\t\tBIT_ULL(55)\n#define PM_MMAP_EXCLUSIVE\tBIT_ULL(56)\n#define PM_FILE\t\t\tBIT_ULL(61)\n#define PM_SWAP\t\t\tBIT_ULL(62)\n#define PM_PRESENT\t\tBIT_ULL(63)\n\n#define PM_END_OF_BUFFER    1\n\nstatic inline pagemap_entry_t make_pme(u64 frame, u64 flags)\n{\n\treturn (pagemap_entry_t) { .pme = (frame & PM_PFRAME_MASK) | flags };\n}\n\nstatic int add_to_pagemap(unsigned long addr, pagemap_entry_t *pme,\n\t\t\t  struct pagemapread *pm)\n{\n\tpm->buffer[pm->pos++] = *pme;\n\tif (pm->pos >= pm->len)\n\t\treturn PM_END_OF_BUFFER;\n\treturn 0;\n}\n\nstatic int pagemap_pte_hole(unsigned long start, unsigned long end,\n\t\t\t\tstruct mm_walk *walk)\n{\n\tstruct pagemapread *pm = walk->private;\n\tunsigned long addr = start;\n\tint err = 0;\n\n\twhile (addr < end) {\n\t\tstruct vm_area_struct *vma = find_vma(walk->mm, addr);\n\t\tpagemap_entry_t pme = make_pme(0, 0);\n\t\t/* End of address space hole, which we mark as non-present. */\n\t\tunsigned long hole_end;\n\n\t\tif (vma)\n\t\t\thole_end = min(end, vma->vm_start);\n\t\telse\n\t\t\thole_end = end;\n\n\t\tfor (; addr < hole_end; addr += PAGE_SIZE) {\n\t\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tif (!vma)\n\t\t\tbreak;\n\n\t\t/* Addresses in the VMA. */\n\t\tif (vma->vm_flags & VM_SOFTDIRTY)\n\t\t\tpme = make_pme(0, PM_SOFT_DIRTY);\n\t\tfor (; addr < min(end, vma->vm_end); addr += PAGE_SIZE) {\n\t\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\treturn err;\n}\n\nstatic pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,\n\t\tstruct vm_area_struct *vma, unsigned long addr, pte_t pte)\n{\n\tu64 frame = 0, flags = 0;\n\tstruct page *page = NULL;\n\n\tif (pte_present(pte)) {\n\t\tif (pm->show_pfn)\n\t\t\tframe = pte_pfn(pte);\n\t\tflags |= PM_PRESENT;\n\t\tpage = _vm_normal_page(vma, addr, pte, true);\n\t\tif (pte_soft_dirty(pte))\n\t\t\tflags |= PM_SOFT_DIRTY;\n\t} else if (is_swap_pte(pte)) {\n\t\tswp_entry_t entry;\n\t\tif (pte_swp_soft_dirty(pte))\n\t\t\tflags |= PM_SOFT_DIRTY;\n\t\tentry = pte_to_swp_entry(pte);\n\t\tif (pm->show_pfn)\n\t\t\tframe = swp_type(entry) |\n\t\t\t\t(swp_offset(entry) << MAX_SWAPFILES_SHIFT);\n\t\tflags |= PM_SWAP;\n\t\tif (is_migration_entry(entry))\n\t\t\tpage = migration_entry_to_page(entry);\n\n\t\tif (is_device_private_entry(entry))\n\t\t\tpage = device_private_entry_to_page(entry);\n\t}\n\n\tif (page && !PageAnon(page))\n\t\tflags |= PM_FILE;\n\tif (page && page_mapcount(page) == 1)\n\t\tflags |= PM_MMAP_EXCLUSIVE;\n\tif (vma->vm_flags & VM_SOFTDIRTY)\n\t\tflags |= PM_SOFT_DIRTY;\n\n\treturn make_pme(frame, flags);\n}\n\nstatic int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,\n\t\t\t     struct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->vma;\n\tstruct pagemapread *pm = walk->private;\n\tspinlock_t *ptl;\n\tpte_t *pte, *orig_pte;\n\tint err = 0;\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tptl = pmd_trans_huge_lock(pmdp, vma);\n\tif (ptl) {\n\t\tu64 flags = 0, frame = 0;\n\t\tpmd_t pmd = *pmdp;\n\t\tstruct page *page = NULL;\n\n\t\tif (vma->vm_flags & VM_SOFTDIRTY)\n\t\t\tflags |= PM_SOFT_DIRTY;\n\n\t\tif (pmd_present(pmd)) {\n\t\t\tpage = pmd_page(pmd);\n\n\t\t\tflags |= PM_PRESENT;\n\t\t\tif (pmd_soft_dirty(pmd))\n\t\t\t\tflags |= PM_SOFT_DIRTY;\n\t\t\tif (pm->show_pfn)\n\t\t\t\tframe = pmd_pfn(pmd) +\n\t\t\t\t\t((addr & ~PMD_MASK) >> PAGE_SHIFT);\n\t\t}\n#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION\n\t\telse if (is_swap_pmd(pmd)) {\n\t\t\tswp_entry_t entry = pmd_to_swp_entry(pmd);\n\t\t\tunsigned long offset;\n\n\t\t\tif (pm->show_pfn) {\n\t\t\t\toffset = swp_offset(entry) +\n\t\t\t\t\t((addr & ~PMD_MASK) >> PAGE_SHIFT);\n\t\t\t\tframe = swp_type(entry) |\n\t\t\t\t\t(offset << MAX_SWAPFILES_SHIFT);\n\t\t\t}\n\t\t\tflags |= PM_SWAP;\n\t\t\tif (pmd_swp_soft_dirty(pmd))\n\t\t\t\tflags |= PM_SOFT_DIRTY;\n\t\t\tVM_BUG_ON(!is_pmd_migration_entry(pmd));\n\t\t\tpage = migration_entry_to_page(entry);\n\t\t}\n#endif\n\n\t\tif (page && page_mapcount(page) == 1)\n\t\t\tflags |= PM_MMAP_EXCLUSIVE;\n\n\t\tfor (; addr != end; addr += PAGE_SIZE) {\n\t\t\tpagemap_entry_t pme = make_pme(frame, flags);\n\n\t\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (pm->show_pfn) {\n\t\t\t\tif (flags & PM_PRESENT)\n\t\t\t\t\tframe++;\n\t\t\t\telse if (flags & PM_SWAP)\n\t\t\t\t\tframe += (1 << MAX_SWAPFILES_SHIFT);\n\t\t\t}\n\t\t}\n\t\tspin_unlock(ptl);\n\t\treturn err;\n\t}\n\n\tif (pmd_trans_unstable(pmdp))\n\t\treturn 0;\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n\n\t/*\n\t * We can assume that @vma always points to a valid one and @end never\n\t * goes beyond vma->vm_end.\n\t */\n\torig_pte = pte = pte_offset_map_lock(walk->mm, pmdp, addr, &ptl);\n\tfor (; addr < end; pte++, addr += PAGE_SIZE) {\n\t\tpagemap_entry_t pme;\n\n\t\tpme = pte_to_pagemap_entry(pm, vma, addr, *pte);\n\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\tpte_unmap_unlock(orig_pte, ptl);\n\n\tcond_resched();\n\n\treturn err;\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\n/* This function walks within one hugetlb entry in the single call */\nstatic int pagemap_hugetlb_range(pte_t *ptep, unsigned long hmask,\n\t\t\t\t unsigned long addr, unsigned long end,\n\t\t\t\t struct mm_walk *walk)\n{\n\tstruct pagemapread *pm = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tu64 flags = 0, frame = 0;\n\tint err = 0;\n\tpte_t pte;\n\n\tif (vma->vm_flags & VM_SOFTDIRTY)\n\t\tflags |= PM_SOFT_DIRTY;\n\n\tpte = huge_ptep_get(ptep);\n\tif (pte_present(pte)) {\n\t\tstruct page *page = pte_page(pte);\n\n\t\tif (!PageAnon(page))\n\t\t\tflags |= PM_FILE;\n\n\t\tif (page_mapcount(page) == 1)\n\t\t\tflags |= PM_MMAP_EXCLUSIVE;\n\n\t\tflags |= PM_PRESENT;\n\t\tif (pm->show_pfn)\n\t\t\tframe = pte_pfn(pte) +\n\t\t\t\t((addr & ~hmask) >> PAGE_SHIFT);\n\t}\n\n\tfor (; addr != end; addr += PAGE_SIZE) {\n\t\tpagemap_entry_t pme = make_pme(frame, flags);\n\n\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (pm->show_pfn && (flags & PM_PRESENT))\n\t\t\tframe++;\n\t}\n\n\tcond_resched();\n\n\treturn err;\n}\n#endif /* HUGETLB_PAGE */\n\n/*\n * /proc/pid/pagemap - an array mapping virtual pages to pfns\n *\n * For each page in the address space, this file contains one 64-bit entry\n * consisting of the following:\n *\n * Bits 0-54  page frame number (PFN) if present\n * Bits 0-4   swap type if swapped\n * Bits 5-54  swap offset if swapped\n * Bit  55    pte is soft-dirty (see Documentation/admin-guide/mm/soft-dirty.rst)\n * Bit  56    page exclusively mapped\n * Bits 57-60 zero\n * Bit  61    page is file-page or shared-anon\n * Bit  62    page swapped\n * Bit  63    page present\n *\n * If the page is not present but in swap, then the PFN contains an\n * encoding of the swap file number and the page's offset into the\n * swap. Unmapped pages return a null PFN. This allows determining\n * precisely which pages are mapped (or in swap) and comparing mapped\n * pages between processes.\n *\n * Efficient users of this interface will use /proc/pid/maps to\n * determine which areas of memory are actually mapped and llseek to\n * skip over unmapped regions.\n */\nstatic ssize_t pagemap_read(struct file *file, char __user *buf,\n\t\t\t    size_t count, loff_t *ppos)\n{\n\tstruct mm_struct *mm = file->private_data;\n\tstruct pagemapread pm;\n\tstruct mm_walk pagemap_walk = {};\n\tunsigned long src;\n\tunsigned long svpfn;\n\tunsigned long start_vaddr;\n\tunsigned long end_vaddr;\n\tint ret = 0, copied = 0;\n\n\tif (!mm || !mmget_not_zero(mm))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\t/* file position must be aligned */\n\tif ((*ppos % PM_ENTRY_BYTES) || (count % PM_ENTRY_BYTES))\n\t\tgoto out_mm;\n\n\tret = 0;\n\tif (!count)\n\t\tgoto out_mm;\n\n\t/* do not disclose physical addresses: attack vector */\n\tpm.show_pfn = file_ns_capable(file, &init_user_ns, CAP_SYS_ADMIN);\n\n\tpm.len = (PAGEMAP_WALK_SIZE >> PAGE_SHIFT);\n\tpm.buffer = kmalloc_array(pm.len, PM_ENTRY_BYTES, GFP_KERNEL);\n\tret = -ENOMEM;\n\tif (!pm.buffer)\n\t\tgoto out_mm;\n\n\tpagemap_walk.pmd_entry = pagemap_pmd_range;\n\tpagemap_walk.pte_hole = pagemap_pte_hole;\n#ifdef CONFIG_HUGETLB_PAGE\n\tpagemap_walk.hugetlb_entry = pagemap_hugetlb_range;\n#endif\n\tpagemap_walk.mm = mm;\n\tpagemap_walk.private = &pm;\n\n\tsrc = *ppos;\n\tsvpfn = src / PM_ENTRY_BYTES;\n\tstart_vaddr = svpfn << PAGE_SHIFT;\n\tend_vaddr = mm->task_size;\n\n\t/* watch out for wraparound */\n\tif (svpfn > mm->task_size >> PAGE_SHIFT)\n\t\tstart_vaddr = end_vaddr;\n\n\t/*\n\t * The odds are that this will stop walking way\n\t * before end_vaddr, because the length of the\n\t * user buffer is tracked in \"pm\", and the walk\n\t * will stop when we hit the end of the buffer.\n\t */\n\tret = 0;\n\twhile (count && (start_vaddr < end_vaddr)) {\n\t\tint len;\n\t\tunsigned long end;\n\n\t\tpm.pos = 0;\n\t\tend = (start_vaddr + PAGEMAP_WALK_SIZE) & PAGEMAP_WALK_MASK;\n\t\t/* overflow ? */\n\t\tif (end < start_vaddr || end > end_vaddr)\n\t\t\tend = end_vaddr;\n\t\tdown_read(&mm->mmap_sem);\n\t\tret = walk_page_range(start_vaddr, end, &pagemap_walk);\n\t\tup_read(&mm->mmap_sem);\n\t\tstart_vaddr = end;\n\n\t\tlen = min(count, PM_ENTRY_BYTES * pm.pos);\n\t\tif (copy_to_user(buf, pm.buffer, len)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tcopied += len;\n\t\tbuf += len;\n\t\tcount -= len;\n\t}\n\t*ppos += copied;\n\tif (!ret || ret == PM_END_OF_BUFFER)\n\t\tret = copied;\n\nout_free:\n\tkfree(pm.buffer);\nout_mm:\n\tmmput(mm);\nout:\n\treturn ret;\n}\n\nstatic int pagemap_open(struct inode *inode, struct file *file)\n{\n\tstruct mm_struct *mm;\n\n\tmm = proc_mem_open(inode, PTRACE_MODE_READ);\n\tif (IS_ERR(mm))\n\t\treturn PTR_ERR(mm);\n\tfile->private_data = mm;\n\treturn 0;\n}\n\nstatic int pagemap_release(struct inode *inode, struct file *file)\n{\n\tstruct mm_struct *mm = file->private_data;\n\n\tif (mm)\n\t\tmmdrop(mm);\n\treturn 0;\n}\n\nconst struct file_operations proc_pagemap_operations = {\n\t.llseek\t\t= mem_lseek, /* borrow this */\n\t.read\t\t= pagemap_read,\n\t.open\t\t= pagemap_open,\n\t.release\t= pagemap_release,\n};\n#endif /* CONFIG_PROC_PAGE_MONITOR */\n\n#ifdef CONFIG_NUMA\n\nstruct numa_maps {\n\tunsigned long pages;\n\tunsigned long anon;\n\tunsigned long active;\n\tunsigned long writeback;\n\tunsigned long mapcount_max;\n\tunsigned long dirty;\n\tunsigned long swapcache;\n\tunsigned long node[MAX_NUMNODES];\n};\n\nstruct numa_maps_private {\n\tstruct proc_maps_private proc_maps;\n\tstruct numa_maps md;\n};\n\nstatic void gather_stats(struct page *page, struct numa_maps *md, int pte_dirty,\n\t\t\tunsigned long nr_pages)\n{\n\tint count = page_mapcount(page);\n\n\tmd->pages += nr_pages;\n\tif (pte_dirty || PageDirty(page))\n\t\tmd->dirty += nr_pages;\n\n\tif (PageSwapCache(page))\n\t\tmd->swapcache += nr_pages;\n\n\tif (PageActive(page) || PageUnevictable(page))\n\t\tmd->active += nr_pages;\n\n\tif (PageWriteback(page))\n\t\tmd->writeback += nr_pages;\n\n\tif (PageAnon(page))\n\t\tmd->anon += nr_pages;\n\n\tif (count > md->mapcount_max)\n\t\tmd->mapcount_max = count;\n\n\tmd->node[page_to_nid(page)] += nr_pages;\n}\n\nstatic struct page *can_gather_numa_stats(pte_t pte, struct vm_area_struct *vma,\n\t\tunsigned long addr)\n{\n\tstruct page *page;\n\tint nid;\n\n\tif (!pte_present(pte))\n\t\treturn NULL;\n\n\tpage = vm_normal_page(vma, addr, pte);\n\tif (!page)\n\t\treturn NULL;\n\n\tif (PageReserved(page))\n\t\treturn NULL;\n\n\tnid = page_to_nid(page);\n\tif (!node_isset(nid, node_states[N_MEMORY]))\n\t\treturn NULL;\n\n\treturn page;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic struct page *can_gather_numa_stats_pmd(pmd_t pmd,\n\t\t\t\t\t      struct vm_area_struct *vma,\n\t\t\t\t\t      unsigned long addr)\n{\n\tstruct page *page;\n\tint nid;\n\n\tif (!pmd_present(pmd))\n\t\treturn NULL;\n\n\tpage = vm_normal_page_pmd(vma, addr, pmd);\n\tif (!page)\n\t\treturn NULL;\n\n\tif (PageReserved(page))\n\t\treturn NULL;\n\n\tnid = page_to_nid(page);\n\tif (!node_isset(nid, node_states[N_MEMORY]))\n\t\treturn NULL;\n\n\treturn page;\n}\n#endif\n\nstatic int gather_pte_stats(pmd_t *pmd, unsigned long addr,\n\t\tunsigned long end, struct mm_walk *walk)\n{\n\tstruct numa_maps *md = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tspinlock_t *ptl;\n\tpte_t *orig_pte;\n\tpte_t *pte;\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tptl = pmd_trans_huge_lock(pmd, vma);\n\tif (ptl) {\n\t\tstruct page *page;\n\n\t\tpage = can_gather_numa_stats_pmd(*pmd, vma, addr);\n\t\tif (page)\n\t\t\tgather_stats(page, md, pmd_dirty(*pmd),\n\t\t\t\t     HPAGE_PMD_SIZE/PAGE_SIZE);\n\t\tspin_unlock(ptl);\n\t\treturn 0;\n\t}\n\n\tif (pmd_trans_unstable(pmd))\n\t\treturn 0;\n#endif\n\torig_pte = pte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);\n\tdo {\n\t\tstruct page *page = can_gather_numa_stats(*pte, vma, addr);\n\t\tif (!page)\n\t\t\tcontinue;\n\t\tgather_stats(page, md, pte_dirty(*pte), 1);\n\n\t} while (pte++, addr += PAGE_SIZE, addr != end);\n\tpte_unmap_unlock(orig_pte, ptl);\n\tcond_resched();\n\treturn 0;\n}\n#ifdef CONFIG_HUGETLB_PAGE\nstatic int gather_hugetlb_stats(pte_t *pte, unsigned long hmask,\n\t\tunsigned long addr, unsigned long end, struct mm_walk *walk)\n{\n\tpte_t huge_pte = huge_ptep_get(pte);\n\tstruct numa_maps *md;\n\tstruct page *page;\n\n\tif (!pte_present(huge_pte))\n\t\treturn 0;\n\n\tpage = pte_page(huge_pte);\n\tif (!page)\n\t\treturn 0;\n\n\tmd = walk->private;\n\tgather_stats(page, md, pte_dirty(huge_pte), 1);\n\treturn 0;\n}\n\n#else\nstatic int gather_hugetlb_stats(pte_t *pte, unsigned long hmask,\n\t\tunsigned long addr, unsigned long end, struct mm_walk *walk)\n{\n\treturn 0;\n}\n#endif\n\n/*\n * Display pages allocated per node and memory policy via /proc.\n */\nstatic int show_numa_map(struct seq_file *m, void *v)\n{\n\tstruct numa_maps_private *numa_priv = m->private;\n\tstruct proc_maps_private *proc_priv = &numa_priv->proc_maps;\n\tstruct vm_area_struct *vma = v;\n\tstruct numa_maps *md = &numa_priv->md;\n\tstruct file *file = vma->vm_file;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct mm_walk walk = {\n\t\t.hugetlb_entry = gather_hugetlb_stats,\n\t\t.pmd_entry = gather_pte_stats,\n\t\t.private = md,\n\t\t.mm = mm,\n\t};\n\tstruct mempolicy *pol;\n\tchar buffer[64];\n\tint nid;\n\n\tif (!mm)\n\t\treturn 0;\n\n\t/* Ensure we start with an empty set of numa_maps statistics. */\n\tmemset(md, 0, sizeof(*md));\n\n\tpol = __get_vma_policy(vma, vma->vm_start);\n\tif (pol) {\n\t\tmpol_to_str(buffer, sizeof(buffer), pol);\n\t\tmpol_cond_put(pol);\n\t} else {\n\t\tmpol_to_str(buffer, sizeof(buffer), proc_priv->task_mempolicy);\n\t}\n\n\tseq_printf(m, \"%08lx %s\", vma->vm_start, buffer);\n\n\tif (file) {\n\t\tseq_puts(m, \" file=\");\n\t\tseq_file_path(m, file, \"\\n\\t= \");\n\t} else if (vma->vm_start <= mm->brk && vma->vm_end >= mm->start_brk) {\n\t\tseq_puts(m, \" heap\");\n\t} else if (is_stack(vma)) {\n\t\tseq_puts(m, \" stack\");\n\t}\n\n\tif (is_vm_hugetlb_page(vma))\n\t\tseq_puts(m, \" huge\");\n\n\t/* mmap_sem is held by m_start */\n\twalk_page_vma(vma, &walk);\n\n\tif (!md->pages)\n\t\tgoto out;\n\n\tif (md->anon)\n\t\tseq_printf(m, \" anon=%lu\", md->anon);\n\n\tif (md->dirty)\n\t\tseq_printf(m, \" dirty=%lu\", md->dirty);\n\n\tif (md->pages != md->anon && md->pages != md->dirty)\n\t\tseq_printf(m, \" mapped=%lu\", md->pages);\n\n\tif (md->mapcount_max > 1)\n\t\tseq_printf(m, \" mapmax=%lu\", md->mapcount_max);\n\n\tif (md->swapcache)\n\t\tseq_printf(m, \" swapcache=%lu\", md->swapcache);\n\n\tif (md->active < md->pages && !is_vm_hugetlb_page(vma))\n\t\tseq_printf(m, \" active=%lu\", md->active);\n\n\tif (md->writeback)\n\t\tseq_printf(m, \" writeback=%lu\", md->writeback);\n\n\tfor_each_node_state(nid, N_MEMORY)\n\t\tif (md->node[nid])\n\t\t\tseq_printf(m, \" N%d=%lu\", nid, md->node[nid]);\n\n\tseq_printf(m, \" kernelpagesize_kB=%lu\", vma_kernel_pagesize(vma) >> 10);\nout:\n\tseq_putc(m, '\\n');\n\tm_cache_vma(m, vma);\n\treturn 0;\n}\n\nstatic const struct seq_operations proc_pid_numa_maps_op = {\n\t.start  = m_start,\n\t.next   = m_next,\n\t.stop   = m_stop,\n\t.show   = show_numa_map,\n};\n\nstatic int pid_numa_maps_open(struct inode *inode, struct file *file)\n{\n\treturn proc_maps_open(inode, file, &proc_pid_numa_maps_op,\n\t\t\t\tsizeof(struct numa_maps_private));\n}\n\nconst struct file_operations proc_pid_numa_maps_operations = {\n\t.open\t\t= pid_numa_maps_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= proc_map_release,\n};\n\n#endif /* CONFIG_NUMA */\n", "/*\n *  fs/userfaultfd.c\n *\n *  Copyright (C) 2007  Davide Libenzi <davidel@xmailserver.org>\n *  Copyright (C) 2008-2009 Red Hat, Inc.\n *  Copyright (C) 2015  Red Hat, Inc.\n *\n *  This work is licensed under the terms of the GNU GPL, version 2. See\n *  the COPYING file in the top-level directory.\n *\n *  Some part derived from fs/eventfd.c (anon inode setup) and\n *  mm/ksm.c (mm hashing).\n */\n\n#include <linux/list.h>\n#include <linux/hashtable.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/poll.h>\n#include <linux/slab.h>\n#include <linux/seq_file.h>\n#include <linux/file.h>\n#include <linux/bug.h>\n#include <linux/anon_inodes.h>\n#include <linux/syscalls.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/mempolicy.h>\n#include <linux/ioctl.h>\n#include <linux/security.h>\n#include <linux/hugetlb.h>\n\nstatic struct kmem_cache *userfaultfd_ctx_cachep __read_mostly;\n\nenum userfaultfd_state {\n\tUFFD_STATE_WAIT_API,\n\tUFFD_STATE_RUNNING,\n};\n\n/*\n * Start with fault_pending_wqh and fault_wqh so they're more likely\n * to be in the same cacheline.\n */\nstruct userfaultfd_ctx {\n\t/* waitqueue head for the pending (i.e. not read) userfaults */\n\twait_queue_head_t fault_pending_wqh;\n\t/* waitqueue head for the userfaults */\n\twait_queue_head_t fault_wqh;\n\t/* waitqueue head for the pseudo fd to wakeup poll/read */\n\twait_queue_head_t fd_wqh;\n\t/* waitqueue head for events */\n\twait_queue_head_t event_wqh;\n\t/* a refile sequence protected by fault_pending_wqh lock */\n\tstruct seqcount refile_seq;\n\t/* pseudo fd refcounting */\n\trefcount_t refcount;\n\t/* userfaultfd syscall flags */\n\tunsigned int flags;\n\t/* features requested from the userspace */\n\tunsigned int features;\n\t/* state machine */\n\tenum userfaultfd_state state;\n\t/* released */\n\tbool released;\n\t/* memory mappings are changing because of non-cooperative event */\n\tbool mmap_changing;\n\t/* mm with one ore more vmas attached to this userfaultfd_ctx */\n\tstruct mm_struct *mm;\n};\n\nstruct userfaultfd_fork_ctx {\n\tstruct userfaultfd_ctx *orig;\n\tstruct userfaultfd_ctx *new;\n\tstruct list_head list;\n};\n\nstruct userfaultfd_unmap_ctx {\n\tstruct userfaultfd_ctx *ctx;\n\tunsigned long start;\n\tunsigned long end;\n\tstruct list_head list;\n};\n\nstruct userfaultfd_wait_queue {\n\tstruct uffd_msg msg;\n\twait_queue_entry_t wq;\n\tstruct userfaultfd_ctx *ctx;\n\tbool waken;\n};\n\nstruct userfaultfd_wake_range {\n\tunsigned long start;\n\tunsigned long len;\n};\n\nstatic int userfaultfd_wake_function(wait_queue_entry_t *wq, unsigned mode,\n\t\t\t\t     int wake_flags, void *key)\n{\n\tstruct userfaultfd_wake_range *range = key;\n\tint ret;\n\tstruct userfaultfd_wait_queue *uwq;\n\tunsigned long start, len;\n\n\tuwq = container_of(wq, struct userfaultfd_wait_queue, wq);\n\tret = 0;\n\t/* len == 0 means wake all */\n\tstart = range->start;\n\tlen = range->len;\n\tif (len && (start > uwq->msg.arg.pagefault.address ||\n\t\t    start + len <= uwq->msg.arg.pagefault.address))\n\t\tgoto out;\n\tWRITE_ONCE(uwq->waken, true);\n\t/*\n\t * The Program-Order guarantees provided by the scheduler\n\t * ensure uwq->waken is visible before the task is woken.\n\t */\n\tret = wake_up_state(wq->private, mode);\n\tif (ret) {\n\t\t/*\n\t\t * Wake only once, autoremove behavior.\n\t\t *\n\t\t * After the effect of list_del_init is visible to the other\n\t\t * CPUs, the waitqueue may disappear from under us, see the\n\t\t * !list_empty_careful() in handle_userfault().\n\t\t *\n\t\t * try_to_wake_up() has an implicit smp_mb(), and the\n\t\t * wq->private is read before calling the extern function\n\t\t * \"wake_up_state\" (which in turns calls try_to_wake_up).\n\t\t */\n\t\tlist_del_init(&wq->entry);\n\t}\nout:\n\treturn ret;\n}\n\n/**\n * userfaultfd_ctx_get - Acquires a reference to the internal userfaultfd\n * context.\n * @ctx: [in] Pointer to the userfaultfd context.\n */\nstatic void userfaultfd_ctx_get(struct userfaultfd_ctx *ctx)\n{\n\trefcount_inc(&ctx->refcount);\n}\n\n/**\n * userfaultfd_ctx_put - Releases a reference to the internal userfaultfd\n * context.\n * @ctx: [in] Pointer to userfaultfd context.\n *\n * The userfaultfd context reference must have been previously acquired either\n * with userfaultfd_ctx_get() or userfaultfd_ctx_fdget().\n */\nstatic void userfaultfd_ctx_put(struct userfaultfd_ctx *ctx)\n{\n\tif (refcount_dec_and_test(&ctx->refcount)) {\n\t\tVM_BUG_ON(spin_is_locked(&ctx->fault_pending_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->fault_pending_wqh));\n\t\tVM_BUG_ON(spin_is_locked(&ctx->fault_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->fault_wqh));\n\t\tVM_BUG_ON(spin_is_locked(&ctx->event_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->event_wqh));\n\t\tVM_BUG_ON(spin_is_locked(&ctx->fd_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->fd_wqh));\n\t\tmmdrop(ctx->mm);\n\t\tkmem_cache_free(userfaultfd_ctx_cachep, ctx);\n\t}\n}\n\nstatic inline void msg_init(struct uffd_msg *msg)\n{\n\tBUILD_BUG_ON(sizeof(struct uffd_msg) != 32);\n\t/*\n\t * Must use memset to zero out the paddings or kernel data is\n\t * leaked to userland.\n\t */\n\tmemset(msg, 0, sizeof(struct uffd_msg));\n}\n\nstatic inline struct uffd_msg userfault_msg(unsigned long address,\n\t\t\t\t\t    unsigned int flags,\n\t\t\t\t\t    unsigned long reason,\n\t\t\t\t\t    unsigned int features)\n{\n\tstruct uffd_msg msg;\n\tmsg_init(&msg);\n\tmsg.event = UFFD_EVENT_PAGEFAULT;\n\tmsg.arg.pagefault.address = address;\n\tif (flags & FAULT_FLAG_WRITE)\n\t\t/*\n\t\t * If UFFD_FEATURE_PAGEFAULT_FLAG_WP was set in the\n\t\t * uffdio_api.features and UFFD_PAGEFAULT_FLAG_WRITE\n\t\t * was not set in a UFFD_EVENT_PAGEFAULT, it means it\n\t\t * was a read fault, otherwise if set it means it's\n\t\t * a write fault.\n\t\t */\n\t\tmsg.arg.pagefault.flags |= UFFD_PAGEFAULT_FLAG_WRITE;\n\tif (reason & VM_UFFD_WP)\n\t\t/*\n\t\t * If UFFD_FEATURE_PAGEFAULT_FLAG_WP was set in the\n\t\t * uffdio_api.features and UFFD_PAGEFAULT_FLAG_WP was\n\t\t * not set in a UFFD_EVENT_PAGEFAULT, it means it was\n\t\t * a missing fault, otherwise if set it means it's a\n\t\t * write protect fault.\n\t\t */\n\t\tmsg.arg.pagefault.flags |= UFFD_PAGEFAULT_FLAG_WP;\n\tif (features & UFFD_FEATURE_THREAD_ID)\n\t\tmsg.arg.pagefault.feat.ptid = task_pid_vnr(current);\n\treturn msg;\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\n/*\n * Same functionality as userfaultfd_must_wait below with modifications for\n * hugepmd ranges.\n */\nstatic inline bool userfaultfd_huge_must_wait(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t struct vm_area_struct *vma,\n\t\t\t\t\t unsigned long address,\n\t\t\t\t\t unsigned long flags,\n\t\t\t\t\t unsigned long reason)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tpte_t *ptep, pte;\n\tbool ret = true;\n\n\tVM_BUG_ON(!rwsem_is_locked(&mm->mmap_sem));\n\n\tptep = huge_pte_offset(mm, address, vma_mmu_pagesize(vma));\n\n\tif (!ptep)\n\t\tgoto out;\n\n\tret = false;\n\tpte = huge_ptep_get(ptep);\n\n\t/*\n\t * Lockless access: we're in a wait_event so it's ok if it\n\t * changes under us.\n\t */\n\tif (huge_pte_none(pte))\n\t\tret = true;\n\tif (!huge_pte_write(pte) && (reason & VM_UFFD_WP))\n\t\tret = true;\nout:\n\treturn ret;\n}\n#else\nstatic inline bool userfaultfd_huge_must_wait(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t struct vm_area_struct *vma,\n\t\t\t\t\t unsigned long address,\n\t\t\t\t\t unsigned long flags,\n\t\t\t\t\t unsigned long reason)\n{\n\treturn false;\t/* should never get here */\n}\n#endif /* CONFIG_HUGETLB_PAGE */\n\n/*\n * Verify the pagetables are still not ok after having reigstered into\n * the fault_pending_wqh to avoid userland having to UFFDIO_WAKE any\n * userfault that has already been resolved, if userfaultfd_read and\n * UFFDIO_COPY|ZEROPAGE are being run simultaneously on two different\n * threads.\n */\nstatic inline bool userfaultfd_must_wait(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t unsigned long address,\n\t\t\t\t\t unsigned long flags,\n\t\t\t\t\t unsigned long reason)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd, _pmd;\n\tpte_t *pte;\n\tbool ret = true;\n\n\tVM_BUG_ON(!rwsem_is_locked(&mm->mmap_sem));\n\n\tpgd = pgd_offset(mm, address);\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\tp4d = p4d_offset(pgd, address);\n\tif (!p4d_present(*p4d))\n\t\tgoto out;\n\tpud = pud_offset(p4d, address);\n\tif (!pud_present(*pud))\n\t\tgoto out;\n\tpmd = pmd_offset(pud, address);\n\t/*\n\t * READ_ONCE must function as a barrier with narrower scope\n\t * and it must be equivalent to:\n\t *\t_pmd = *pmd; barrier();\n\t *\n\t * This is to deal with the instability (as in\n\t * pmd_trans_unstable) of the pmd.\n\t */\n\t_pmd = READ_ONCE(*pmd);\n\tif (pmd_none(_pmd))\n\t\tgoto out;\n\n\tret = false;\n\tif (!pmd_present(_pmd))\n\t\tgoto out;\n\n\tif (pmd_trans_huge(_pmd))\n\t\tgoto out;\n\n\t/*\n\t * the pmd is stable (as in !pmd_trans_unstable) so we can re-read it\n\t * and use the standard pte_offset_map() instead of parsing _pmd.\n\t */\n\tpte = pte_offset_map(pmd, address);\n\t/*\n\t * Lockless access: we're in a wait_event so it's ok if it\n\t * changes under us.\n\t */\n\tif (pte_none(*pte))\n\t\tret = true;\n\tpte_unmap(pte);\n\nout:\n\treturn ret;\n}\n\n/*\n * The locking rules involved in returning VM_FAULT_RETRY depending on\n * FAULT_FLAG_ALLOW_RETRY, FAULT_FLAG_RETRY_NOWAIT and\n * FAULT_FLAG_KILLABLE are not straightforward. The \"Caution\"\n * recommendation in __lock_page_or_retry is not an understatement.\n *\n * If FAULT_FLAG_ALLOW_RETRY is set, the mmap_sem must be released\n * before returning VM_FAULT_RETRY only if FAULT_FLAG_RETRY_NOWAIT is\n * not set.\n *\n * If FAULT_FLAG_ALLOW_RETRY is set but FAULT_FLAG_KILLABLE is not\n * set, VM_FAULT_RETRY can still be returned if and only if there are\n * fatal_signal_pending()s, and the mmap_sem must be released before\n * returning it.\n */\nvm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)\n{\n\tstruct mm_struct *mm = vmf->vma->vm_mm;\n\tstruct userfaultfd_ctx *ctx;\n\tstruct userfaultfd_wait_queue uwq;\n\tvm_fault_t ret = VM_FAULT_SIGBUS;\n\tbool must_wait, return_to_userland;\n\tlong blocking_state;\n\n\t/*\n\t * We don't do userfault handling for the final child pid update.\n\t *\n\t * We also don't do userfault handling during\n\t * coredumping. hugetlbfs has the special\n\t * follow_hugetlb_page() to skip missing pages in the\n\t * FOLL_DUMP case, anon memory also checks for FOLL_DUMP with\n\t * the no_page_table() helper in follow_page_mask(), but the\n\t * shmem_vm_ops->fault method is invoked even during\n\t * coredumping without mmap_sem and it ends up here.\n\t */\n\tif (current->flags & (PF_EXITING|PF_DUMPCORE))\n\t\tgoto out;\n\n\t/*\n\t * Coredumping runs without mmap_sem so we can only check that\n\t * the mmap_sem is held, if PF_DUMPCORE was not set.\n\t */\n\tWARN_ON_ONCE(!rwsem_is_locked(&mm->mmap_sem));\n\n\tctx = vmf->vma->vm_userfaultfd_ctx.ctx;\n\tif (!ctx)\n\t\tgoto out;\n\n\tBUG_ON(ctx->mm != mm);\n\n\tVM_BUG_ON(reason & ~(VM_UFFD_MISSING|VM_UFFD_WP));\n\tVM_BUG_ON(!(reason & VM_UFFD_MISSING) ^ !!(reason & VM_UFFD_WP));\n\n\tif (ctx->features & UFFD_FEATURE_SIGBUS)\n\t\tgoto out;\n\n\t/*\n\t * If it's already released don't get it. This avoids to loop\n\t * in __get_user_pages if userfaultfd_release waits on the\n\t * caller of handle_userfault to release the mmap_sem.\n\t */\n\tif (unlikely(READ_ONCE(ctx->released))) {\n\t\t/*\n\t\t * Don't return VM_FAULT_SIGBUS in this case, so a non\n\t\t * cooperative manager can close the uffd after the\n\t\t * last UFFDIO_COPY, without risking to trigger an\n\t\t * involuntary SIGBUS if the process was starting the\n\t\t * userfaultfd while the userfaultfd was still armed\n\t\t * (but after the last UFFDIO_COPY). If the uffd\n\t\t * wasn't already closed when the userfault reached\n\t\t * this point, that would normally be solved by\n\t\t * userfaultfd_must_wait returning 'false'.\n\t\t *\n\t\t * If we were to return VM_FAULT_SIGBUS here, the non\n\t\t * cooperative manager would be instead forced to\n\t\t * always call UFFDIO_UNREGISTER before it can safely\n\t\t * close the uffd.\n\t\t */\n\t\tret = VM_FAULT_NOPAGE;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Check that we can return VM_FAULT_RETRY.\n\t *\n\t * NOTE: it should become possible to return VM_FAULT_RETRY\n\t * even if FAULT_FLAG_TRIED is set without leading to gup()\n\t * -EBUSY failures, if the userfaultfd is to be extended for\n\t * VM_UFFD_WP tracking and we intend to arm the userfault\n\t * without first stopping userland access to the memory. For\n\t * VM_UFFD_MISSING userfaults this is enough for now.\n\t */\n\tif (unlikely(!(vmf->flags & FAULT_FLAG_ALLOW_RETRY))) {\n\t\t/*\n\t\t * Validate the invariant that nowait must allow retry\n\t\t * to be sure not to return SIGBUS erroneously on\n\t\t * nowait invocations.\n\t\t */\n\t\tBUG_ON(vmf->flags & FAULT_FLAG_RETRY_NOWAIT);\n#ifdef CONFIG_DEBUG_VM\n\t\tif (printk_ratelimit()) {\n\t\t\tprintk(KERN_WARNING\n\t\t\t       \"FAULT_FLAG_ALLOW_RETRY missing %x\\n\",\n\t\t\t       vmf->flags);\n\t\t\tdump_stack();\n\t\t}\n#endif\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Handle nowait, not much to do other than tell it to retry\n\t * and wait.\n\t */\n\tret = VM_FAULT_RETRY;\n\tif (vmf->flags & FAULT_FLAG_RETRY_NOWAIT)\n\t\tgoto out;\n\n\t/* take the reference before dropping the mmap_sem */\n\tuserfaultfd_ctx_get(ctx);\n\n\tinit_waitqueue_func_entry(&uwq.wq, userfaultfd_wake_function);\n\tuwq.wq.private = current;\n\tuwq.msg = userfault_msg(vmf->address, vmf->flags, reason,\n\t\t\tctx->features);\n\tuwq.ctx = ctx;\n\tuwq.waken = false;\n\n\treturn_to_userland =\n\t\t(vmf->flags & (FAULT_FLAG_USER|FAULT_FLAG_KILLABLE)) ==\n\t\t(FAULT_FLAG_USER|FAULT_FLAG_KILLABLE);\n\tblocking_state = return_to_userland ? TASK_INTERRUPTIBLE :\n\t\t\t TASK_KILLABLE;\n\n\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t/*\n\t * After the __add_wait_queue the uwq is visible to userland\n\t * through poll/read().\n\t */\n\t__add_wait_queue(&ctx->fault_pending_wqh, &uwq.wq);\n\t/*\n\t * The smp_mb() after __set_current_state prevents the reads\n\t * following the spin_unlock to happen before the list_add in\n\t * __add_wait_queue.\n\t */\n\tset_current_state(blocking_state);\n\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\tif (!is_vm_hugetlb_page(vmf->vma))\n\t\tmust_wait = userfaultfd_must_wait(ctx, vmf->address, vmf->flags,\n\t\t\t\t\t\t  reason);\n\telse\n\t\tmust_wait = userfaultfd_huge_must_wait(ctx, vmf->vma,\n\t\t\t\t\t\t       vmf->address,\n\t\t\t\t\t\t       vmf->flags, reason);\n\tup_read(&mm->mmap_sem);\n\n\tif (likely(must_wait && !READ_ONCE(ctx->released) &&\n\t\t   (return_to_userland ? !signal_pending(current) :\n\t\t    !fatal_signal_pending(current)))) {\n\t\twake_up_poll(&ctx->fd_wqh, EPOLLIN);\n\t\tschedule();\n\t\tret |= VM_FAULT_MAJOR;\n\n\t\t/*\n\t\t * False wakeups can orginate even from rwsem before\n\t\t * up_read() however userfaults will wait either for a\n\t\t * targeted wakeup on the specific uwq waitqueue from\n\t\t * wake_userfault() or for signals or for uffd\n\t\t * release.\n\t\t */\n\t\twhile (!READ_ONCE(uwq.waken)) {\n\t\t\t/*\n\t\t\t * This needs the full smp_store_mb()\n\t\t\t * guarantee as the state write must be\n\t\t\t * visible to other CPUs before reading\n\t\t\t * uwq.waken from other CPUs.\n\t\t\t */\n\t\t\tset_current_state(blocking_state);\n\t\t\tif (READ_ONCE(uwq.waken) ||\n\t\t\t    READ_ONCE(ctx->released) ||\n\t\t\t    (return_to_userland ? signal_pending(current) :\n\t\t\t     fatal_signal_pending(current)))\n\t\t\t\tbreak;\n\t\t\tschedule();\n\t\t}\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\n\tif (return_to_userland) {\n\t\tif (signal_pending(current) &&\n\t\t    !fatal_signal_pending(current)) {\n\t\t\t/*\n\t\t\t * If we got a SIGSTOP or SIGCONT and this is\n\t\t\t * a normal userland page fault, just let\n\t\t\t * userland return so the signal will be\n\t\t\t * handled and gdb debugging works.  The page\n\t\t\t * fault code immediately after we return from\n\t\t\t * this function is going to release the\n\t\t\t * mmap_sem and it's not depending on it\n\t\t\t * (unlike gup would if we were not to return\n\t\t\t * VM_FAULT_RETRY).\n\t\t\t *\n\t\t\t * If a fatal signal is pending we still take\n\t\t\t * the streamlined VM_FAULT_RETRY failure path\n\t\t\t * and there's no need to retake the mmap_sem\n\t\t\t * in such case.\n\t\t\t */\n\t\t\tdown_read(&mm->mmap_sem);\n\t\t\tret = VM_FAULT_NOPAGE;\n\t\t}\n\t}\n\n\t/*\n\t * Here we race with the list_del; list_add in\n\t * userfaultfd_ctx_read(), however because we don't ever run\n\t * list_del_init() to refile across the two lists, the prev\n\t * and next pointers will never point to self. list_add also\n\t * would never let any of the two pointers to point to\n\t * self. So list_empty_careful won't risk to see both pointers\n\t * pointing to self at any time during the list refile. The\n\t * only case where list_del_init() is called is the full\n\t * removal in the wake function and there we don't re-list_add\n\t * and it's fine not to block on the spinlock. The uwq on this\n\t * kernel stack can be released after the list_del_init.\n\t */\n\tif (!list_empty_careful(&uwq.wq.entry)) {\n\t\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t\t/*\n\t\t * No need of list_del_init(), the uwq on the stack\n\t\t * will be freed shortly anyway.\n\t\t */\n\t\tlist_del(&uwq.wq.entry);\n\t\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\t}\n\n\t/*\n\t * ctx may go away after this if the userfault pseudo fd is\n\t * already released.\n\t */\n\tuserfaultfd_ctx_put(ctx);\n\nout:\n\treturn ret;\n}\n\nstatic void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t      struct userfaultfd_wait_queue *ewq)\n{\n\tstruct userfaultfd_ctx *release_new_ctx;\n\n\tif (WARN_ON_ONCE(current->flags & PF_EXITING))\n\t\tgoto out;\n\n\tewq->ctx = ctx;\n\tinit_waitqueue_entry(&ewq->wq, current);\n\trelease_new_ctx = NULL;\n\n\tspin_lock(&ctx->event_wqh.lock);\n\t/*\n\t * After the __add_wait_queue the uwq is visible to userland\n\t * through poll/read().\n\t */\n\t__add_wait_queue(&ctx->event_wqh, &ewq->wq);\n\tfor (;;) {\n\t\tset_current_state(TASK_KILLABLE);\n\t\tif (ewq->msg.event == 0)\n\t\t\tbreak;\n\t\tif (READ_ONCE(ctx->released) ||\n\t\t    fatal_signal_pending(current)) {\n\t\t\t/*\n\t\t\t * &ewq->wq may be queued in fork_event, but\n\t\t\t * __remove_wait_queue ignores the head\n\t\t\t * parameter. It would be a problem if it\n\t\t\t * didn't.\n\t\t\t */\n\t\t\t__remove_wait_queue(&ctx->event_wqh, &ewq->wq);\n\t\t\tif (ewq->msg.event == UFFD_EVENT_FORK) {\n\t\t\t\tstruct userfaultfd_ctx *new;\n\n\t\t\t\tnew = (struct userfaultfd_ctx *)\n\t\t\t\t\t(unsigned long)\n\t\t\t\t\tewq->msg.arg.reserved.reserved1;\n\t\t\t\trelease_new_ctx = new;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tspin_unlock(&ctx->event_wqh.lock);\n\n\t\twake_up_poll(&ctx->fd_wqh, EPOLLIN);\n\t\tschedule();\n\n\t\tspin_lock(&ctx->event_wqh.lock);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\tspin_unlock(&ctx->event_wqh.lock);\n\n\tif (release_new_ctx) {\n\t\tstruct vm_area_struct *vma;\n\t\tstruct mm_struct *mm = release_new_ctx->mm;\n\n\t\t/* the various vma->vm_userfaultfd_ctx still points to it */\n\t\tdown_write(&mm->mmap_sem);\n\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next)\n\t\t\tif (vma->vm_userfaultfd_ctx.ctx == release_new_ctx) {\n\t\t\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t\t\t\tvma->vm_flags &= ~(VM_UFFD_WP | VM_UFFD_MISSING);\n\t\t\t}\n\t\tup_write(&mm->mmap_sem);\n\n\t\tuserfaultfd_ctx_put(release_new_ctx);\n\t}\n\n\t/*\n\t * ctx may go away after this if the userfault pseudo fd is\n\t * already released.\n\t */\nout:\n\tWRITE_ONCE(ctx->mmap_changing, false);\n\tuserfaultfd_ctx_put(ctx);\n}\n\nstatic void userfaultfd_event_complete(struct userfaultfd_ctx *ctx,\n\t\t\t\t       struct userfaultfd_wait_queue *ewq)\n{\n\tewq->msg.event = 0;\n\twake_up_locked(&ctx->event_wqh);\n\t__remove_wait_queue(&ctx->event_wqh, &ewq->wq);\n}\n\nint dup_userfaultfd(struct vm_area_struct *vma, struct list_head *fcs)\n{\n\tstruct userfaultfd_ctx *ctx = NULL, *octx;\n\tstruct userfaultfd_fork_ctx *fctx;\n\n\toctx = vma->vm_userfaultfd_ctx.ctx;\n\tif (!octx || !(octx->features & UFFD_FEATURE_EVENT_FORK)) {\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t\tvma->vm_flags &= ~(VM_UFFD_WP | VM_UFFD_MISSING);\n\t\treturn 0;\n\t}\n\n\tlist_for_each_entry(fctx, fcs, list)\n\t\tif (fctx->orig == octx) {\n\t\t\tctx = fctx->new;\n\t\t\tbreak;\n\t\t}\n\n\tif (!ctx) {\n\t\tfctx = kmalloc(sizeof(*fctx), GFP_KERNEL);\n\t\tif (!fctx)\n\t\t\treturn -ENOMEM;\n\n\t\tctx = kmem_cache_alloc(userfaultfd_ctx_cachep, GFP_KERNEL);\n\t\tif (!ctx) {\n\t\t\tkfree(fctx);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\trefcount_set(&ctx->refcount, 1);\n\t\tctx->flags = octx->flags;\n\t\tctx->state = UFFD_STATE_RUNNING;\n\t\tctx->features = octx->features;\n\t\tctx->released = false;\n\t\tctx->mmap_changing = false;\n\t\tctx->mm = vma->vm_mm;\n\t\tmmgrab(ctx->mm);\n\n\t\tuserfaultfd_ctx_get(octx);\n\t\tWRITE_ONCE(octx->mmap_changing, true);\n\t\tfctx->orig = octx;\n\t\tfctx->new = ctx;\n\t\tlist_add_tail(&fctx->list, fcs);\n\t}\n\n\tvma->vm_userfaultfd_ctx.ctx = ctx;\n\treturn 0;\n}\n\nstatic void dup_fctx(struct userfaultfd_fork_ctx *fctx)\n{\n\tstruct userfaultfd_ctx *ctx = fctx->orig;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tmsg_init(&ewq.msg);\n\n\tewq.msg.event = UFFD_EVENT_FORK;\n\tewq.msg.arg.reserved.reserved1 = (unsigned long)fctx->new;\n\n\tuserfaultfd_event_wait_completion(ctx, &ewq);\n}\n\nvoid dup_userfaultfd_complete(struct list_head *fcs)\n{\n\tstruct userfaultfd_fork_ctx *fctx, *n;\n\n\tlist_for_each_entry_safe(fctx, n, fcs, list) {\n\t\tdup_fctx(fctx);\n\t\tlist_del(&fctx->list);\n\t\tkfree(fctx);\n\t}\n}\n\nvoid mremap_userfaultfd_prep(struct vm_area_struct *vma,\n\t\t\t     struct vm_userfaultfd_ctx *vm_ctx)\n{\n\tstruct userfaultfd_ctx *ctx;\n\n\tctx = vma->vm_userfaultfd_ctx.ctx;\n\n\tif (!ctx)\n\t\treturn;\n\n\tif (ctx->features & UFFD_FEATURE_EVENT_REMAP) {\n\t\tvm_ctx->ctx = ctx;\n\t\tuserfaultfd_ctx_get(ctx);\n\t\tWRITE_ONCE(ctx->mmap_changing, true);\n\t} else {\n\t\t/* Drop uffd context if remap feature not enabled */\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t\tvma->vm_flags &= ~(VM_UFFD_WP | VM_UFFD_MISSING);\n\t}\n}\n\nvoid mremap_userfaultfd_complete(struct vm_userfaultfd_ctx *vm_ctx,\n\t\t\t\t unsigned long from, unsigned long to,\n\t\t\t\t unsigned long len)\n{\n\tstruct userfaultfd_ctx *ctx = vm_ctx->ctx;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tif (!ctx)\n\t\treturn;\n\n\tif (to & ~PAGE_MASK) {\n\t\tuserfaultfd_ctx_put(ctx);\n\t\treturn;\n\t}\n\n\tmsg_init(&ewq.msg);\n\n\tewq.msg.event = UFFD_EVENT_REMAP;\n\tewq.msg.arg.remap.from = from;\n\tewq.msg.arg.remap.to = to;\n\tewq.msg.arg.remap.len = len;\n\n\tuserfaultfd_event_wait_completion(ctx, &ewq);\n}\n\nbool userfaultfd_remove(struct vm_area_struct *vma,\n\t\t\tunsigned long start, unsigned long end)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct userfaultfd_ctx *ctx;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tctx = vma->vm_userfaultfd_ctx.ctx;\n\tif (!ctx || !(ctx->features & UFFD_FEATURE_EVENT_REMOVE))\n\t\treturn true;\n\n\tuserfaultfd_ctx_get(ctx);\n\tWRITE_ONCE(ctx->mmap_changing, true);\n\tup_read(&mm->mmap_sem);\n\n\tmsg_init(&ewq.msg);\n\n\tewq.msg.event = UFFD_EVENT_REMOVE;\n\tewq.msg.arg.remove.start = start;\n\tewq.msg.arg.remove.end = end;\n\n\tuserfaultfd_event_wait_completion(ctx, &ewq);\n\n\treturn false;\n}\n\nstatic bool has_unmap_ctx(struct userfaultfd_ctx *ctx, struct list_head *unmaps,\n\t\t\t  unsigned long start, unsigned long end)\n{\n\tstruct userfaultfd_unmap_ctx *unmap_ctx;\n\n\tlist_for_each_entry(unmap_ctx, unmaps, list)\n\t\tif (unmap_ctx->ctx == ctx && unmap_ctx->start == start &&\n\t\t    unmap_ctx->end == end)\n\t\t\treturn true;\n\n\treturn false;\n}\n\nint userfaultfd_unmap_prep(struct vm_area_struct *vma,\n\t\t\t   unsigned long start, unsigned long end,\n\t\t\t   struct list_head *unmaps)\n{\n\tfor ( ; vma && vma->vm_start < end; vma = vma->vm_next) {\n\t\tstruct userfaultfd_unmap_ctx *unmap_ctx;\n\t\tstruct userfaultfd_ctx *ctx = vma->vm_userfaultfd_ctx.ctx;\n\n\t\tif (!ctx || !(ctx->features & UFFD_FEATURE_EVENT_UNMAP) ||\n\t\t    has_unmap_ctx(ctx, unmaps, start, end))\n\t\t\tcontinue;\n\n\t\tunmap_ctx = kzalloc(sizeof(*unmap_ctx), GFP_KERNEL);\n\t\tif (!unmap_ctx)\n\t\t\treturn -ENOMEM;\n\n\t\tuserfaultfd_ctx_get(ctx);\n\t\tWRITE_ONCE(ctx->mmap_changing, true);\n\t\tunmap_ctx->ctx = ctx;\n\t\tunmap_ctx->start = start;\n\t\tunmap_ctx->end = end;\n\t\tlist_add_tail(&unmap_ctx->list, unmaps);\n\t}\n\n\treturn 0;\n}\n\nvoid userfaultfd_unmap_complete(struct mm_struct *mm, struct list_head *uf)\n{\n\tstruct userfaultfd_unmap_ctx *ctx, *n;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tlist_for_each_entry_safe(ctx, n, uf, list) {\n\t\tmsg_init(&ewq.msg);\n\n\t\tewq.msg.event = UFFD_EVENT_UNMAP;\n\t\tewq.msg.arg.remove.start = ctx->start;\n\t\tewq.msg.arg.remove.end = ctx->end;\n\n\t\tuserfaultfd_event_wait_completion(ctx->ctx, &ewq);\n\n\t\tlist_del(&ctx->list);\n\t\tkfree(ctx);\n\t}\n}\n\nstatic int userfaultfd_release(struct inode *inode, struct file *file)\n{\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev;\n\t/* len == 0 means wake all */\n\tstruct userfaultfd_wake_range range = { .len = 0, };\n\tunsigned long new_flags;\n\n\tWRITE_ONCE(ctx->released, true);\n\n\tif (!mmget_not_zero(mm))\n\t\tgoto wakeup;\n\n\t/*\n\t * Flush page faults out of all CPUs. NOTE: all page faults\n\t * must be retried without returning VM_FAULT_SIGBUS if\n\t * userfaultfd_ctx_get() succeeds but vma->vma_userfault_ctx\n\t * changes while handle_userfault released the mmap_sem. So\n\t * it's critical that released is set to true (above), before\n\t * taking the mmap_sem for writing.\n\t */\n\tdown_write(&mm->mmap_sem);\n\tprev = NULL;\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tcond_resched();\n\t\tBUG_ON(!!vma->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(vma->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\t\tif (vma->vm_userfaultfd_ctx.ctx != ctx) {\n\t\t\tprev = vma;\n\t\t\tcontinue;\n\t\t}\n\t\tnew_flags = vma->vm_flags & ~(VM_UFFD_MISSING | VM_UFFD_WP);\n\t\tprev = vma_merge(mm, prev, vma->vm_start, vma->vm_end,\n\t\t\t\t new_flags, vma->anon_vma,\n\t\t\t\t vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t NULL_VM_UFFD_CTX);\n\t\tif (prev)\n\t\t\tvma = prev;\n\t\telse\n\t\t\tprev = vma;\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t}\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\nwakeup:\n\t/*\n\t * After no new page faults can wait on this fault_*wqh, flush\n\t * the last page faults that may have been already waiting on\n\t * the fault_*wqh.\n\t */\n\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t__wake_up_locked_key(&ctx->fault_pending_wqh, TASK_NORMAL, &range);\n\t__wake_up(&ctx->fault_wqh, TASK_NORMAL, 1, &range);\n\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\t/* Flush pending events that may still wait on event_wqh */\n\twake_up_all(&ctx->event_wqh);\n\n\twake_up_poll(&ctx->fd_wqh, EPOLLHUP);\n\tuserfaultfd_ctx_put(ctx);\n\treturn 0;\n}\n\n/* fault_pending_wqh.lock must be hold by the caller */\nstatic inline struct userfaultfd_wait_queue *find_userfault_in(\n\t\twait_queue_head_t *wqh)\n{\n\twait_queue_entry_t *wq;\n\tstruct userfaultfd_wait_queue *uwq;\n\n\tlockdep_assert_held(&wqh->lock);\n\n\tuwq = NULL;\n\tif (!waitqueue_active(wqh))\n\t\tgoto out;\n\t/* walk in reverse to provide FIFO behavior to read userfaults */\n\twq = list_last_entry(&wqh->head, typeof(*wq), entry);\n\tuwq = container_of(wq, struct userfaultfd_wait_queue, wq);\nout:\n\treturn uwq;\n}\n\nstatic inline struct userfaultfd_wait_queue *find_userfault(\n\t\tstruct userfaultfd_ctx *ctx)\n{\n\treturn find_userfault_in(&ctx->fault_pending_wqh);\n}\n\nstatic inline struct userfaultfd_wait_queue *find_userfault_evt(\n\t\tstruct userfaultfd_ctx *ctx)\n{\n\treturn find_userfault_in(&ctx->event_wqh);\n}\n\nstatic __poll_t userfaultfd_poll(struct file *file, poll_table *wait)\n{\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\t__poll_t ret;\n\n\tpoll_wait(file, &ctx->fd_wqh, wait);\n\n\tswitch (ctx->state) {\n\tcase UFFD_STATE_WAIT_API:\n\t\treturn EPOLLERR;\n\tcase UFFD_STATE_RUNNING:\n\t\t/*\n\t\t * poll() never guarantees that read won't block.\n\t\t * userfaults can be waken before they're read().\n\t\t */\n\t\tif (unlikely(!(file->f_flags & O_NONBLOCK)))\n\t\t\treturn EPOLLERR;\n\t\t/*\n\t\t * lockless access to see if there are pending faults\n\t\t * __pollwait last action is the add_wait_queue but\n\t\t * the spin_unlock would allow the waitqueue_active to\n\t\t * pass above the actual list_add inside\n\t\t * add_wait_queue critical section. So use a full\n\t\t * memory barrier to serialize the list_add write of\n\t\t * add_wait_queue() with the waitqueue_active read\n\t\t * below.\n\t\t */\n\t\tret = 0;\n\t\tsmp_mb();\n\t\tif (waitqueue_active(&ctx->fault_pending_wqh))\n\t\t\tret = EPOLLIN;\n\t\telse if (waitqueue_active(&ctx->event_wqh))\n\t\t\tret = EPOLLIN;\n\n\t\treturn ret;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn EPOLLERR;\n\t}\n}\n\nstatic const struct file_operations userfaultfd_fops;\n\nstatic int resolve_userfault_fork(struct userfaultfd_ctx *ctx,\n\t\t\t\t  struct userfaultfd_ctx *new,\n\t\t\t\t  struct uffd_msg *msg)\n{\n\tint fd;\n\n\tfd = anon_inode_getfd(\"[userfaultfd]\", &userfaultfd_fops, new,\n\t\t\t      O_RDWR | (new->flags & UFFD_SHARED_FCNTL_FLAGS));\n\tif (fd < 0)\n\t\treturn fd;\n\n\tmsg->arg.reserved.reserved1 = 0;\n\tmsg->arg.fork.ufd = fd;\n\treturn 0;\n}\n\nstatic ssize_t userfaultfd_ctx_read(struct userfaultfd_ctx *ctx, int no_wait,\n\t\t\t\t    struct uffd_msg *msg)\n{\n\tssize_t ret;\n\tDECLARE_WAITQUEUE(wait, current);\n\tstruct userfaultfd_wait_queue *uwq;\n\t/*\n\t * Handling fork event requires sleeping operations, so\n\t * we drop the event_wqh lock, then do these ops, then\n\t * lock it back and wake up the waiter. While the lock is\n\t * dropped the ewq may go away so we keep track of it\n\t * carefully.\n\t */\n\tLIST_HEAD(fork_event);\n\tstruct userfaultfd_ctx *fork_nctx = NULL;\n\n\t/* always take the fd_wqh lock before the fault_pending_wqh lock */\n\tspin_lock_irq(&ctx->fd_wqh.lock);\n\t__add_wait_queue(&ctx->fd_wqh, &wait);\n\tfor (;;) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t\tuwq = find_userfault(ctx);\n\t\tif (uwq) {\n\t\t\t/*\n\t\t\t * Use a seqcount to repeat the lockless check\n\t\t\t * in wake_userfault() to avoid missing\n\t\t\t * wakeups because during the refile both\n\t\t\t * waitqueue could become empty if this is the\n\t\t\t * only userfault.\n\t\t\t */\n\t\t\twrite_seqcount_begin(&ctx->refile_seq);\n\n\t\t\t/*\n\t\t\t * The fault_pending_wqh.lock prevents the uwq\n\t\t\t * to disappear from under us.\n\t\t\t *\n\t\t\t * Refile this userfault from\n\t\t\t * fault_pending_wqh to fault_wqh, it's not\n\t\t\t * pending anymore after we read it.\n\t\t\t *\n\t\t\t * Use list_del() by hand (as\n\t\t\t * userfaultfd_wake_function also uses\n\t\t\t * list_del_init() by hand) to be sure nobody\n\t\t\t * changes __remove_wait_queue() to use\n\t\t\t * list_del_init() in turn breaking the\n\t\t\t * !list_empty_careful() check in\n\t\t\t * handle_userfault(). The uwq->wq.head list\n\t\t\t * must never be empty at any time during the\n\t\t\t * refile, or the waitqueue could disappear\n\t\t\t * from under us. The \"wait_queue_head_t\"\n\t\t\t * parameter of __remove_wait_queue() is unused\n\t\t\t * anyway.\n\t\t\t */\n\t\t\tlist_del(&uwq->wq.entry);\n\t\t\tadd_wait_queue(&ctx->fault_wqh, &uwq->wq);\n\n\t\t\twrite_seqcount_end(&ctx->refile_seq);\n\n\t\t\t/* careful to always initialize msg if ret == 0 */\n\t\t\t*msg = uwq->msg;\n\t\t\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\t\tspin_lock(&ctx->event_wqh.lock);\n\t\tuwq = find_userfault_evt(ctx);\n\t\tif (uwq) {\n\t\t\t*msg = uwq->msg;\n\n\t\t\tif (uwq->msg.event == UFFD_EVENT_FORK) {\n\t\t\t\tfork_nctx = (struct userfaultfd_ctx *)\n\t\t\t\t\t(unsigned long)\n\t\t\t\t\tuwq->msg.arg.reserved.reserved1;\n\t\t\t\tlist_move(&uwq->wq.entry, &fork_event);\n\t\t\t\t/*\n\t\t\t\t * fork_nctx can be freed as soon as\n\t\t\t\t * we drop the lock, unless we take a\n\t\t\t\t * reference on it.\n\t\t\t\t */\n\t\t\t\tuserfaultfd_ctx_get(fork_nctx);\n\t\t\t\tspin_unlock(&ctx->event_wqh.lock);\n\t\t\t\tret = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tuserfaultfd_event_complete(ctx, uwq);\n\t\t\tspin_unlock(&ctx->event_wqh.lock);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&ctx->event_wqh.lock);\n\n\t\tif (signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (no_wait) {\n\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock_irq(&ctx->fd_wqh.lock);\n\t\tschedule();\n\t\tspin_lock_irq(&ctx->fd_wqh.lock);\n\t}\n\t__remove_wait_queue(&ctx->fd_wqh, &wait);\n\t__set_current_state(TASK_RUNNING);\n\tspin_unlock_irq(&ctx->fd_wqh.lock);\n\n\tif (!ret && msg->event == UFFD_EVENT_FORK) {\n\t\tret = resolve_userfault_fork(ctx, fork_nctx, msg);\n\t\tspin_lock(&ctx->event_wqh.lock);\n\t\tif (!list_empty(&fork_event)) {\n\t\t\t/*\n\t\t\t * The fork thread didn't abort, so we can\n\t\t\t * drop the temporary refcount.\n\t\t\t */\n\t\t\tuserfaultfd_ctx_put(fork_nctx);\n\n\t\t\tuwq = list_first_entry(&fork_event,\n\t\t\t\t\t       typeof(*uwq),\n\t\t\t\t\t       wq.entry);\n\t\t\t/*\n\t\t\t * If fork_event list wasn't empty and in turn\n\t\t\t * the event wasn't already released by fork\n\t\t\t * (the event is allocated on fork kernel\n\t\t\t * stack), put the event back to its place in\n\t\t\t * the event_wq. fork_event head will be freed\n\t\t\t * as soon as we return so the event cannot\n\t\t\t * stay queued there no matter the current\n\t\t\t * \"ret\" value.\n\t\t\t */\n\t\t\tlist_del(&uwq->wq.entry);\n\t\t\t__add_wait_queue(&ctx->event_wqh, &uwq->wq);\n\n\t\t\t/*\n\t\t\t * Leave the event in the waitqueue and report\n\t\t\t * error to userland if we failed to resolve\n\t\t\t * the userfault fork.\n\t\t\t */\n\t\t\tif (likely(!ret))\n\t\t\t\tuserfaultfd_event_complete(ctx, uwq);\n\t\t} else {\n\t\t\t/*\n\t\t\t * Here the fork thread aborted and the\n\t\t\t * refcount from the fork thread on fork_nctx\n\t\t\t * has already been released. We still hold\n\t\t\t * the reference we took before releasing the\n\t\t\t * lock above. If resolve_userfault_fork\n\t\t\t * failed we've to drop it because the\n\t\t\t * fork_nctx has to be freed in such case. If\n\t\t\t * it succeeded we'll hold it because the new\n\t\t\t * uffd references it.\n\t\t\t */\n\t\t\tif (ret)\n\t\t\t\tuserfaultfd_ctx_put(fork_nctx);\n\t\t}\n\t\tspin_unlock(&ctx->event_wqh.lock);\n\t}\n\n\treturn ret;\n}\n\nstatic ssize_t userfaultfd_read(struct file *file, char __user *buf,\n\t\t\t\tsize_t count, loff_t *ppos)\n{\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\tssize_t _ret, ret = 0;\n\tstruct uffd_msg msg;\n\tint no_wait = file->f_flags & O_NONBLOCK;\n\n\tif (ctx->state == UFFD_STATE_WAIT_API)\n\t\treturn -EINVAL;\n\n\tfor (;;) {\n\t\tif (count < sizeof(msg))\n\t\t\treturn ret ? ret : -EINVAL;\n\t\t_ret = userfaultfd_ctx_read(ctx, no_wait, &msg);\n\t\tif (_ret < 0)\n\t\t\treturn ret ? ret : _ret;\n\t\tif (copy_to_user((__u64 __user *) buf, &msg, sizeof(msg)))\n\t\t\treturn ret ? ret : -EFAULT;\n\t\tret += sizeof(msg);\n\t\tbuf += sizeof(msg);\n\t\tcount -= sizeof(msg);\n\t\t/*\n\t\t * Allow to read more than one fault at time but only\n\t\t * block if waiting for the very first one.\n\t\t */\n\t\tno_wait = O_NONBLOCK;\n\t}\n}\n\nstatic void __wake_userfault(struct userfaultfd_ctx *ctx,\n\t\t\t     struct userfaultfd_wake_range *range)\n{\n\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t/* wake all in the range and autoremove */\n\tif (waitqueue_active(&ctx->fault_pending_wqh))\n\t\t__wake_up_locked_key(&ctx->fault_pending_wqh, TASK_NORMAL,\n\t\t\t\t     range);\n\tif (waitqueue_active(&ctx->fault_wqh))\n\t\t__wake_up(&ctx->fault_wqh, TASK_NORMAL, 1, range);\n\tspin_unlock(&ctx->fault_pending_wqh.lock);\n}\n\nstatic __always_inline void wake_userfault(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t   struct userfaultfd_wake_range *range)\n{\n\tunsigned seq;\n\tbool need_wakeup;\n\n\t/*\n\t * To be sure waitqueue_active() is not reordered by the CPU\n\t * before the pagetable update, use an explicit SMP memory\n\t * barrier here. PT lock release or up_read(mmap_sem) still\n\t * have release semantics that can allow the\n\t * waitqueue_active() to be reordered before the pte update.\n\t */\n\tsmp_mb();\n\n\t/*\n\t * Use waitqueue_active because it's very frequent to\n\t * change the address space atomically even if there are no\n\t * userfaults yet. So we take the spinlock only when we're\n\t * sure we've userfaults to wake.\n\t */\n\tdo {\n\t\tseq = read_seqcount_begin(&ctx->refile_seq);\n\t\tneed_wakeup = waitqueue_active(&ctx->fault_pending_wqh) ||\n\t\t\twaitqueue_active(&ctx->fault_wqh);\n\t\tcond_resched();\n\t} while (read_seqcount_retry(&ctx->refile_seq, seq));\n\tif (need_wakeup)\n\t\t__wake_userfault(ctx, range);\n}\n\nstatic __always_inline int validate_range(struct mm_struct *mm,\n\t\t\t\t\t  __u64 start, __u64 len)\n{\n\t__u64 task_size = mm->task_size;\n\n\tif (start & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\tif (len & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\tif (!len)\n\t\treturn -EINVAL;\n\tif (start < mmap_min_addr)\n\t\treturn -EINVAL;\n\tif (start >= task_size)\n\t\treturn -EINVAL;\n\tif (len > task_size - start)\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic inline bool vma_can_userfault(struct vm_area_struct *vma)\n{\n\treturn vma_is_anonymous(vma) || is_vm_hugetlb_page(vma) ||\n\t\tvma_is_shmem(vma);\n}\n\nstatic int userfaultfd_register(struct userfaultfd_ctx *ctx,\n\t\t\t\tunsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_register uffdio_register;\n\tstruct uffdio_register __user *user_uffdio_register;\n\tunsigned long vm_flags, new_flags;\n\tbool found;\n\tbool basic_ioctls;\n\tunsigned long start, end, vma_end;\n\n\tuser_uffdio_register = (struct uffdio_register __user *) arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_register, user_uffdio_register,\n\t\t\t   sizeof(uffdio_register)-sizeof(__u64)))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (!uffdio_register.mode)\n\t\tgoto out;\n\tif (uffdio_register.mode & ~(UFFDIO_REGISTER_MODE_MISSING|\n\t\t\t\t     UFFDIO_REGISTER_MODE_WP))\n\t\tgoto out;\n\tvm_flags = 0;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_MISSING)\n\t\tvm_flags |= VM_UFFD_MISSING;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_WP) {\n\t\tvm_flags |= VM_UFFD_WP;\n\t\t/*\n\t\t * FIXME: remove the below error constraint by\n\t\t * implementing the wprotect tracking mode.\n\t\t */\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = validate_range(mm, uffdio_register.range.start,\n\t\t\t     uffdio_register.range.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_register.range.start;\n\tend = start + uffdio_register.range.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tbasic_ioctls = false;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/* check not compatible vmas */\n\t\tret = -EINVAL;\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * UFFDIO_COPY will fill file holes even without\n\t\t * PROT_WRITE. This check enforces that if this is a\n\t\t * MAP_SHARED, the process has write permission to the backing\n\t\t * file. If VM_MAYWRITE is set it also enforces that on a\n\t\t * MAP_SHARED vma: there is no F_WRITE_SEAL and no further\n\t\t * F_WRITE_SEAL can be taken until the vma is destroyed.\n\t\t */\n\t\tret = -EPERM;\n\t\tif (unlikely(!(cur->vm_flags & VM_MAYWRITE)))\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * If this vma contains ending address, and huge pages\n\t\t * check alignment.\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur) && end <= cur->vm_end &&\n\t\t    end > cur->vm_start) {\n\t\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(cur);\n\n\t\t\tret = -EINVAL;\n\n\t\t\tif (end & (vma_hpagesize - 1))\n\t\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/*\n\t\t * Check that this vma isn't already owned by a\n\t\t * different userfaultfd. We can't allow more than one\n\t\t * userfaultfd to own a single vma simultaneously or we\n\t\t * wouldn't know which one to deliver the userfaults to.\n\t\t */\n\t\tret = -EBUSY;\n\t\tif (cur->vm_userfaultfd_ctx.ctx &&\n\t\t    cur->vm_userfaultfd_ctx.ctx != ctx)\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * Note vmas containing huge pages\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur))\n\t\t\tbasic_ioctls = true;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\t\tBUG_ON(vma->vm_userfaultfd_ctx.ctx &&\n\t\t       vma->vm_userfaultfd_ctx.ctx != ctx);\n\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (vma->vm_userfaultfd_ctx.ctx == ctx &&\n\t\t    (vma->vm_flags & vm_flags) == vm_flags)\n\t\t\tgoto skip;\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tnew_flags = (vma->vm_flags & ~vm_flags) | vm_flags;\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t ((struct vm_userfaultfd_ctx){ ctx }));\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx.ctx = ctx;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\n\tif (!ret) {\n\t\t/*\n\t\t * Now that we scanned all vmas we can already tell\n\t\t * userland which ioctls methods are guaranteed to\n\t\t * succeed on this range.\n\t\t */\n\t\tif (put_user(basic_ioctls ? UFFD_API_RANGE_IOCTLS_BASIC :\n\t\t\t     UFFD_API_RANGE_IOCTLS,\n\t\t\t     &user_uffdio_register->ioctls))\n\t\t\tret = -EFAULT;\n\t}\nout:\n\treturn ret;\n}\n\nstatic int userfaultfd_unregister(struct userfaultfd_ctx *ctx,\n\t\t\t\t  unsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_range uffdio_unregister;\n\tunsigned long new_flags;\n\tbool found;\n\tunsigned long start, end, vma_end;\n\tconst void __user *buf = (void __user *)arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_unregister, buf, sizeof(uffdio_unregister)))\n\t\tgoto out;\n\n\tret = validate_range(mm, uffdio_unregister.start,\n\t\t\t     uffdio_unregister.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_unregister.start;\n\tend = start + uffdio_unregister.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tret = -EINVAL;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/*\n\t\t * Check not compatible vmas, not strictly required\n\t\t * here as not compatible vmas cannot have an\n\t\t * userfaultfd_ctx registered on them, but this\n\t\t * provides for more strict behavior to notice\n\t\t * unregistration errors.\n\t\t */\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (!vma->vm_userfaultfd_ctx.ctx)\n\t\t\tgoto skip;\n\n\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tif (userfaultfd_missing(vma)) {\n\t\t\t/*\n\t\t\t * Wake any concurrent pending userfault while\n\t\t\t * we unregister, so they will not hang\n\t\t\t * permanently and it avoids userland to call\n\t\t\t * UFFDIO_WAKE explicitly.\n\t\t\t */\n\t\t\tstruct userfaultfd_wake_range range;\n\t\t\trange.start = start;\n\t\t\trange.len = vma_end - start;\n\t\t\twake_userfault(vma->vm_userfaultfd_ctx.ctx, &range);\n\t\t}\n\n\t\tnew_flags = vma->vm_flags & ~(VM_UFFD_MISSING | VM_UFFD_WP);\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t NULL_VM_UFFD_CTX);\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\nout:\n\treturn ret;\n}\n\n/*\n * userfaultfd_wake may be used in combination with the\n * UFFDIO_*_MODE_DONTWAKE to wakeup userfaults in batches.\n */\nstatic int userfaultfd_wake(struct userfaultfd_ctx *ctx,\n\t\t\t    unsigned long arg)\n{\n\tint ret;\n\tstruct uffdio_range uffdio_wake;\n\tstruct userfaultfd_wake_range range;\n\tconst void __user *buf = (void __user *)arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_wake, buf, sizeof(uffdio_wake)))\n\t\tgoto out;\n\n\tret = validate_range(ctx->mm, uffdio_wake.start, uffdio_wake.len);\n\tif (ret)\n\t\tgoto out;\n\n\trange.start = uffdio_wake.start;\n\trange.len = uffdio_wake.len;\n\n\t/*\n\t * len == 0 means wake all and we don't want to wake all here,\n\t * so check it again to be sure.\n\t */\n\tVM_BUG_ON(!range.len);\n\n\twake_userfault(ctx, &range);\n\tret = 0;\n\nout:\n\treturn ret;\n}\n\nstatic int userfaultfd_copy(struct userfaultfd_ctx *ctx,\n\t\t\t    unsigned long arg)\n{\n\t__s64 ret;\n\tstruct uffdio_copy uffdio_copy;\n\tstruct uffdio_copy __user *user_uffdio_copy;\n\tstruct userfaultfd_wake_range range;\n\n\tuser_uffdio_copy = (struct uffdio_copy __user *) arg;\n\n\tret = -EAGAIN;\n\tif (READ_ONCE(ctx->mmap_changing))\n\t\tgoto out;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_copy, user_uffdio_copy,\n\t\t\t   /* don't copy \"copy\" last field */\n\t\t\t   sizeof(uffdio_copy)-sizeof(__s64)))\n\t\tgoto out;\n\n\tret = validate_range(ctx->mm, uffdio_copy.dst, uffdio_copy.len);\n\tif (ret)\n\t\tgoto out;\n\t/*\n\t * double check for wraparound just in case. copy_from_user()\n\t * will later check uffdio_copy.src + uffdio_copy.len to fit\n\t * in the userland range.\n\t */\n\tret = -EINVAL;\n\tif (uffdio_copy.src + uffdio_copy.len <= uffdio_copy.src)\n\t\tgoto out;\n\tif (uffdio_copy.mode & ~UFFDIO_COPY_MODE_DONTWAKE)\n\t\tgoto out;\n\tif (mmget_not_zero(ctx->mm)) {\n\t\tret = mcopy_atomic(ctx->mm, uffdio_copy.dst, uffdio_copy.src,\n\t\t\t\t   uffdio_copy.len, &ctx->mmap_changing);\n\t\tmmput(ctx->mm);\n\t} else {\n\t\treturn -ESRCH;\n\t}\n\tif (unlikely(put_user(ret, &user_uffdio_copy->copy)))\n\t\treturn -EFAULT;\n\tif (ret < 0)\n\t\tgoto out;\n\tBUG_ON(!ret);\n\t/* len == 0 would wake all */\n\trange.len = ret;\n\tif (!(uffdio_copy.mode & UFFDIO_COPY_MODE_DONTWAKE)) {\n\t\trange.start = uffdio_copy.dst;\n\t\twake_userfault(ctx, &range);\n\t}\n\tret = range.len == uffdio_copy.len ? 0 : -EAGAIN;\nout:\n\treturn ret;\n}\n\nstatic int userfaultfd_zeropage(struct userfaultfd_ctx *ctx,\n\t\t\t\tunsigned long arg)\n{\n\t__s64 ret;\n\tstruct uffdio_zeropage uffdio_zeropage;\n\tstruct uffdio_zeropage __user *user_uffdio_zeropage;\n\tstruct userfaultfd_wake_range range;\n\n\tuser_uffdio_zeropage = (struct uffdio_zeropage __user *) arg;\n\n\tret = -EAGAIN;\n\tif (READ_ONCE(ctx->mmap_changing))\n\t\tgoto out;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_zeropage, user_uffdio_zeropage,\n\t\t\t   /* don't copy \"zeropage\" last field */\n\t\t\t   sizeof(uffdio_zeropage)-sizeof(__s64)))\n\t\tgoto out;\n\n\tret = validate_range(ctx->mm, uffdio_zeropage.range.start,\n\t\t\t     uffdio_zeropage.range.len);\n\tif (ret)\n\t\tgoto out;\n\tret = -EINVAL;\n\tif (uffdio_zeropage.mode & ~UFFDIO_ZEROPAGE_MODE_DONTWAKE)\n\t\tgoto out;\n\n\tif (mmget_not_zero(ctx->mm)) {\n\t\tret = mfill_zeropage(ctx->mm, uffdio_zeropage.range.start,\n\t\t\t\t     uffdio_zeropage.range.len,\n\t\t\t\t     &ctx->mmap_changing);\n\t\tmmput(ctx->mm);\n\t} else {\n\t\treturn -ESRCH;\n\t}\n\tif (unlikely(put_user(ret, &user_uffdio_zeropage->zeropage)))\n\t\treturn -EFAULT;\n\tif (ret < 0)\n\t\tgoto out;\n\t/* len == 0 would wake all */\n\tBUG_ON(!ret);\n\trange.len = ret;\n\tif (!(uffdio_zeropage.mode & UFFDIO_ZEROPAGE_MODE_DONTWAKE)) {\n\t\trange.start = uffdio_zeropage.range.start;\n\t\twake_userfault(ctx, &range);\n\t}\n\tret = range.len == uffdio_zeropage.range.len ? 0 : -EAGAIN;\nout:\n\treturn ret;\n}\n\nstatic inline unsigned int uffd_ctx_features(__u64 user_features)\n{\n\t/*\n\t * For the current set of features the bits just coincide\n\t */\n\treturn (unsigned int)user_features;\n}\n\n/*\n * userland asks for a certain API version and we return which bits\n * and ioctl commands are implemented in this kernel for such API\n * version or -EINVAL if unknown.\n */\nstatic int userfaultfd_api(struct userfaultfd_ctx *ctx,\n\t\t\t   unsigned long arg)\n{\n\tstruct uffdio_api uffdio_api;\n\tvoid __user *buf = (void __user *)arg;\n\tint ret;\n\t__u64 features;\n\n\tret = -EINVAL;\n\tif (ctx->state != UFFD_STATE_WAIT_API)\n\t\tgoto out;\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_api, buf, sizeof(uffdio_api)))\n\t\tgoto out;\n\tfeatures = uffdio_api.features;\n\tif (uffdio_api.api != UFFD_API || (features & ~UFFD_API_FEATURES)) {\n\t\tmemset(&uffdio_api, 0, sizeof(uffdio_api));\n\t\tif (copy_to_user(buf, &uffdio_api, sizeof(uffdio_api)))\n\t\t\tgoto out;\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\t/* report all available features and ioctls to userland */\n\tuffdio_api.features = UFFD_API_FEATURES;\n\tuffdio_api.ioctls = UFFD_API_IOCTLS;\n\tret = -EFAULT;\n\tif (copy_to_user(buf, &uffdio_api, sizeof(uffdio_api)))\n\t\tgoto out;\n\tctx->state = UFFD_STATE_RUNNING;\n\t/* only enable the requested features for this uffd context */\n\tctx->features = uffd_ctx_features(features);\n\tret = 0;\nout:\n\treturn ret;\n}\n\nstatic long userfaultfd_ioctl(struct file *file, unsigned cmd,\n\t\t\t      unsigned long arg)\n{\n\tint ret = -EINVAL;\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\n\tif (cmd != UFFDIO_API && ctx->state == UFFD_STATE_WAIT_API)\n\t\treturn -EINVAL;\n\n\tswitch(cmd) {\n\tcase UFFDIO_API:\n\t\tret = userfaultfd_api(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_REGISTER:\n\t\tret = userfaultfd_register(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_UNREGISTER:\n\t\tret = userfaultfd_unregister(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_WAKE:\n\t\tret = userfaultfd_wake(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_COPY:\n\t\tret = userfaultfd_copy(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_ZEROPAGE:\n\t\tret = userfaultfd_zeropage(ctx, arg);\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\n#ifdef CONFIG_PROC_FS\nstatic void userfaultfd_show_fdinfo(struct seq_file *m, struct file *f)\n{\n\tstruct userfaultfd_ctx *ctx = f->private_data;\n\twait_queue_entry_t *wq;\n\tunsigned long pending = 0, total = 0;\n\n\tspin_lock(&ctx->fault_pending_wqh.lock);\n\tlist_for_each_entry(wq, &ctx->fault_pending_wqh.head, entry) {\n\t\tpending++;\n\t\ttotal++;\n\t}\n\tlist_for_each_entry(wq, &ctx->fault_wqh.head, entry) {\n\t\ttotal++;\n\t}\n\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\t/*\n\t * If more protocols will be added, there will be all shown\n\t * separated by a space. Like this:\n\t *\tprotocols: aa:... bb:...\n\t */\n\tseq_printf(m, \"pending:\\t%lu\\ntotal:\\t%lu\\nAPI:\\t%Lx:%x:%Lx\\n\",\n\t\t   pending, total, UFFD_API, ctx->features,\n\t\t   UFFD_API_IOCTLS|UFFD_API_RANGE_IOCTLS);\n}\n#endif\n\nstatic const struct file_operations userfaultfd_fops = {\n#ifdef CONFIG_PROC_FS\n\t.show_fdinfo\t= userfaultfd_show_fdinfo,\n#endif\n\t.release\t= userfaultfd_release,\n\t.poll\t\t= userfaultfd_poll,\n\t.read\t\t= userfaultfd_read,\n\t.unlocked_ioctl = userfaultfd_ioctl,\n\t.compat_ioctl\t= userfaultfd_ioctl,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic void init_once_userfaultfd_ctx(void *mem)\n{\n\tstruct userfaultfd_ctx *ctx = (struct userfaultfd_ctx *) mem;\n\n\tinit_waitqueue_head(&ctx->fault_pending_wqh);\n\tinit_waitqueue_head(&ctx->fault_wqh);\n\tinit_waitqueue_head(&ctx->event_wqh);\n\tinit_waitqueue_head(&ctx->fd_wqh);\n\tseqcount_init(&ctx->refile_seq);\n}\n\nSYSCALL_DEFINE1(userfaultfd, int, flags)\n{\n\tstruct userfaultfd_ctx *ctx;\n\tint fd;\n\n\tBUG_ON(!current->mm);\n\n\t/* Check the UFFD_* constants for consistency.  */\n\tBUILD_BUG_ON(UFFD_CLOEXEC != O_CLOEXEC);\n\tBUILD_BUG_ON(UFFD_NONBLOCK != O_NONBLOCK);\n\n\tif (flags & ~UFFD_SHARED_FCNTL_FLAGS)\n\t\treturn -EINVAL;\n\n\tctx = kmem_cache_alloc(userfaultfd_ctx_cachep, GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\trefcount_set(&ctx->refcount, 1);\n\tctx->flags = flags;\n\tctx->features = 0;\n\tctx->state = UFFD_STATE_WAIT_API;\n\tctx->released = false;\n\tctx->mmap_changing = false;\n\tctx->mm = current->mm;\n\t/* prevent the mm struct to be freed */\n\tmmgrab(ctx->mm);\n\n\tfd = anon_inode_getfd(\"[userfaultfd]\", &userfaultfd_fops, ctx,\n\t\t\t      O_RDWR | (flags & UFFD_SHARED_FCNTL_FLAGS));\n\tif (fd < 0) {\n\t\tmmdrop(ctx->mm);\n\t\tkmem_cache_free(userfaultfd_ctx_cachep, ctx);\n\t}\n\treturn fd;\n}\n\nstatic int __init userfaultfd_init(void)\n{\n\tuserfaultfd_ctx_cachep = kmem_cache_create(\"userfaultfd_ctx_cache\",\n\t\t\t\t\t\tsizeof(struct userfaultfd_ctx),\n\t\t\t\t\t\t0,\n\t\t\t\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC,\n\t\t\t\t\t\tinit_once_userfaultfd_ctx);\n\treturn 0;\n}\n__initcall(userfaultfd_init);\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_SCHED_MM_H\n#define _LINUX_SCHED_MM_H\n\n#include <linux/kernel.h>\n#include <linux/atomic.h>\n#include <linux/sched.h>\n#include <linux/mm_types.h>\n#include <linux/gfp.h>\n#include <linux/sync_core.h>\n\n/*\n * Routines for handling mm_structs\n */\nextern struct mm_struct *mm_alloc(void);\n\n/**\n * mmgrab() - Pin a &struct mm_struct.\n * @mm: The &struct mm_struct to pin.\n *\n * Make sure that @mm will not get freed even after the owning task\n * exits. This doesn't guarantee that the associated address space\n * will still exist later on and mmget_not_zero() has to be used before\n * accessing it.\n *\n * This is a preferred way to to pin @mm for a longer/unbounded amount\n * of time.\n *\n * Use mmdrop() to release the reference acquired by mmgrab().\n *\n * See also <Documentation/vm/active_mm.rst> for an in-depth explanation\n * of &mm_struct.mm_count vs &mm_struct.mm_users.\n */\nstatic inline void mmgrab(struct mm_struct *mm)\n{\n\tatomic_inc(&mm->mm_count);\n}\n\nextern void __mmdrop(struct mm_struct *mm);\n\nstatic inline void mmdrop(struct mm_struct *mm)\n{\n\t/*\n\t * The implicit full barrier implied by atomic_dec_and_test() is\n\t * required by the membarrier system call before returning to\n\t * user-space, after storing to rq->curr.\n\t */\n\tif (unlikely(atomic_dec_and_test(&mm->mm_count)))\n\t\t__mmdrop(mm);\n}\n\n/**\n * mmget() - Pin the address space associated with a &struct mm_struct.\n * @mm: The address space to pin.\n *\n * Make sure that the address space of the given &struct mm_struct doesn't\n * go away. This does not protect against parts of the address space being\n * modified or freed, however.\n *\n * Never use this function to pin this address space for an\n * unbounded/indefinite amount of time.\n *\n * Use mmput() to release the reference acquired by mmget().\n *\n * See also <Documentation/vm/active_mm.rst> for an in-depth explanation\n * of &mm_struct.mm_count vs &mm_struct.mm_users.\n */\nstatic inline void mmget(struct mm_struct *mm)\n{\n\tatomic_inc(&mm->mm_users);\n}\n\nstatic inline bool mmget_not_zero(struct mm_struct *mm)\n{\n\treturn atomic_inc_not_zero(&mm->mm_users);\n}\n\n/* mmput gets rid of the mappings and all user-space */\nextern void mmput(struct mm_struct *);\n#ifdef CONFIG_MMU\n/* same as above but performs the slow path from the async context. Can\n * be called from the atomic context as well\n */\nvoid mmput_async(struct mm_struct *);\n#endif\n\n/* Grab a reference to a task's mm, if it is not already going away */\nextern struct mm_struct *get_task_mm(struct task_struct *task);\n/*\n * Grab a reference to a task's mm, if it is not already going away\n * and ptrace_may_access with the mode parameter passed to it\n * succeeds.\n */\nextern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode);\n/* Remove the current tasks stale references to the old mm_struct */\nextern void mm_release(struct task_struct *, struct mm_struct *);\n\n#ifdef CONFIG_MEMCG\nextern void mm_update_next_owner(struct mm_struct *mm);\n#else\nstatic inline void mm_update_next_owner(struct mm_struct *mm)\n{\n}\n#endif /* CONFIG_MEMCG */\n\n#ifdef CONFIG_MMU\nextern void arch_pick_mmap_layout(struct mm_struct *mm,\n\t\t\t\t  struct rlimit *rlim_stack);\nextern unsigned long\narch_get_unmapped_area(struct file *, unsigned long, unsigned long,\n\t\t       unsigned long, unsigned long);\nextern unsigned long\narch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,\n\t\t\t  unsigned long len, unsigned long pgoff,\n\t\t\t  unsigned long flags);\n#else\nstatic inline void arch_pick_mmap_layout(struct mm_struct *mm,\n\t\t\t\t\t struct rlimit *rlim_stack) {}\n#endif\n\nstatic inline bool in_vfork(struct task_struct *tsk)\n{\n\tbool ret;\n\n\t/*\n\t * need RCU to access ->real_parent if CLONE_VM was used along with\n\t * CLONE_PARENT.\n\t *\n\t * We check real_parent->mm == tsk->mm because CLONE_VFORK does not\n\t * imply CLONE_VM\n\t *\n\t * CLONE_VFORK can be used with CLONE_PARENT/CLONE_THREAD and thus\n\t * ->real_parent is not necessarily the task doing vfork(), so in\n\t * theory we can't rely on task_lock() if we want to dereference it.\n\t *\n\t * And in this case we can't trust the real_parent->mm == tsk->mm\n\t * check, it can be false negative. But we do not care, if init or\n\t * another oom-unkillable task does this it should blame itself.\n\t */\n\trcu_read_lock();\n\tret = tsk->vfork_done && tsk->real_parent->mm == tsk->mm;\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n/*\n * Applies per-task gfp context to the given allocation flags.\n * PF_MEMALLOC_NOIO implies GFP_NOIO\n * PF_MEMALLOC_NOFS implies GFP_NOFS\n * PF_MEMALLOC_NOCMA implies no allocation from CMA region.\n */\nstatic inline gfp_t current_gfp_context(gfp_t flags)\n{\n\tif (unlikely(current->flags &\n\t\t     (PF_MEMALLOC_NOIO | PF_MEMALLOC_NOFS | PF_MEMALLOC_NOCMA))) {\n\t\t/*\n\t\t * NOIO implies both NOIO and NOFS and it is a weaker context\n\t\t * so always make sure it makes precedence\n\t\t */\n\t\tif (current->flags & PF_MEMALLOC_NOIO)\n\t\t\tflags &= ~(__GFP_IO | __GFP_FS);\n\t\telse if (current->flags & PF_MEMALLOC_NOFS)\n\t\t\tflags &= ~__GFP_FS;\n#ifdef CONFIG_CMA\n\t\tif (current->flags & PF_MEMALLOC_NOCMA)\n\t\t\tflags &= ~__GFP_MOVABLE;\n#endif\n\t}\n\treturn flags;\n}\n\n#ifdef CONFIG_LOCKDEP\nextern void __fs_reclaim_acquire(void);\nextern void __fs_reclaim_release(void);\nextern void fs_reclaim_acquire(gfp_t gfp_mask);\nextern void fs_reclaim_release(gfp_t gfp_mask);\n#else\nstatic inline void __fs_reclaim_acquire(void) { }\nstatic inline void __fs_reclaim_release(void) { }\nstatic inline void fs_reclaim_acquire(gfp_t gfp_mask) { }\nstatic inline void fs_reclaim_release(gfp_t gfp_mask) { }\n#endif\n\n/**\n * memalloc_noio_save - Marks implicit GFP_NOIO allocation scope.\n *\n * This functions marks the beginning of the GFP_NOIO allocation scope.\n * All further allocations will implicitly drop __GFP_IO flag and so\n * they are safe for the IO critical section from the allocation recursion\n * point of view. Use memalloc_noio_restore to end the scope with flags\n * returned by this function.\n *\n * This function is safe to be used from any context.\n */\nstatic inline unsigned int memalloc_noio_save(void)\n{\n\tunsigned int flags = current->flags & PF_MEMALLOC_NOIO;\n\tcurrent->flags |= PF_MEMALLOC_NOIO;\n\treturn flags;\n}\n\n/**\n * memalloc_noio_restore - Ends the implicit GFP_NOIO scope.\n * @flags: Flags to restore.\n *\n * Ends the implicit GFP_NOIO scope started by memalloc_noio_save function.\n * Always make sure that that the given flags is the return value from the\n * pairing memalloc_noio_save call.\n */\nstatic inline void memalloc_noio_restore(unsigned int flags)\n{\n\tcurrent->flags = (current->flags & ~PF_MEMALLOC_NOIO) | flags;\n}\n\n/**\n * memalloc_nofs_save - Marks implicit GFP_NOFS allocation scope.\n *\n * This functions marks the beginning of the GFP_NOFS allocation scope.\n * All further allocations will implicitly drop __GFP_FS flag and so\n * they are safe for the FS critical section from the allocation recursion\n * point of view. Use memalloc_nofs_restore to end the scope with flags\n * returned by this function.\n *\n * This function is safe to be used from any context.\n */\nstatic inline unsigned int memalloc_nofs_save(void)\n{\n\tunsigned int flags = current->flags & PF_MEMALLOC_NOFS;\n\tcurrent->flags |= PF_MEMALLOC_NOFS;\n\treturn flags;\n}\n\n/**\n * memalloc_nofs_restore - Ends the implicit GFP_NOFS scope.\n * @flags: Flags to restore.\n *\n * Ends the implicit GFP_NOFS scope started by memalloc_nofs_save function.\n * Always make sure that that the given flags is the return value from the\n * pairing memalloc_nofs_save call.\n */\nstatic inline void memalloc_nofs_restore(unsigned int flags)\n{\n\tcurrent->flags = (current->flags & ~PF_MEMALLOC_NOFS) | flags;\n}\n\nstatic inline unsigned int memalloc_noreclaim_save(void)\n{\n\tunsigned int flags = current->flags & PF_MEMALLOC;\n\tcurrent->flags |= PF_MEMALLOC;\n\treturn flags;\n}\n\nstatic inline void memalloc_noreclaim_restore(unsigned int flags)\n{\n\tcurrent->flags = (current->flags & ~PF_MEMALLOC) | flags;\n}\n\n#ifdef CONFIG_CMA\nstatic inline unsigned int memalloc_nocma_save(void)\n{\n\tunsigned int flags = current->flags & PF_MEMALLOC_NOCMA;\n\n\tcurrent->flags |= PF_MEMALLOC_NOCMA;\n\treturn flags;\n}\n\nstatic inline void memalloc_nocma_restore(unsigned int flags)\n{\n\tcurrent->flags = (current->flags & ~PF_MEMALLOC_NOCMA) | flags;\n}\n#else\nstatic inline unsigned int memalloc_nocma_save(void)\n{\n\treturn 0;\n}\n\nstatic inline void memalloc_nocma_restore(unsigned int flags)\n{\n}\n#endif\n\n#ifdef CONFIG_MEMCG\n/**\n * memalloc_use_memcg - Starts the remote memcg charging scope.\n * @memcg: memcg to charge.\n *\n * This function marks the beginning of the remote memcg charging scope. All the\n * __GFP_ACCOUNT allocations till the end of the scope will be charged to the\n * given memcg.\n *\n * NOTE: This function is not nesting safe.\n */\nstatic inline void memalloc_use_memcg(struct mem_cgroup *memcg)\n{\n\tWARN_ON_ONCE(current->active_memcg);\n\tcurrent->active_memcg = memcg;\n}\n\n/**\n * memalloc_unuse_memcg - Ends the remote memcg charging scope.\n *\n * This function marks the end of the remote memcg charging scope started by\n * memalloc_use_memcg().\n */\nstatic inline void memalloc_unuse_memcg(void)\n{\n\tcurrent->active_memcg = NULL;\n}\n#else\nstatic inline void memalloc_use_memcg(struct mem_cgroup *memcg)\n{\n}\n\nstatic inline void memalloc_unuse_memcg(void)\n{\n}\n#endif\n\n#ifdef CONFIG_MEMBARRIER\nenum {\n\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_READY\t\t= (1U << 0),\n\tMEMBARRIER_STATE_PRIVATE_EXPEDITED\t\t\t= (1U << 1),\n\tMEMBARRIER_STATE_GLOBAL_EXPEDITED_READY\t\t\t= (1U << 2),\n\tMEMBARRIER_STATE_GLOBAL_EXPEDITED\t\t\t= (1U << 3),\n\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY\t= (1U << 4),\n\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE\t\t= (1U << 5),\n};\n\nenum {\n\tMEMBARRIER_FLAG_SYNC_CORE\t= (1U << 0),\n};\n\n#ifdef CONFIG_ARCH_HAS_MEMBARRIER_CALLBACKS\n#include <asm/membarrier.h>\n#endif\n\nstatic inline void membarrier_mm_sync_core_before_usermode(struct mm_struct *mm)\n{\n\tif (likely(!(atomic_read(&mm->membarrier_state) &\n\t\t     MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE)))\n\t\treturn;\n\tsync_core_before_usermode();\n}\n\nstatic inline void membarrier_execve(struct task_struct *t)\n{\n\tatomic_set(&t->mm->membarrier_state, 0);\n}\n#else\n#ifdef CONFIG_ARCH_HAS_MEMBARRIER_CALLBACKS\nstatic inline void membarrier_arch_switch_mm(struct mm_struct *prev,\n\t\t\t\t\t     struct mm_struct *next,\n\t\t\t\t\t     struct task_struct *tsk)\n{\n}\n#endif\nstatic inline void membarrier_execve(struct task_struct *t)\n{\n}\nstatic inline void membarrier_mm_sync_core_before_usermode(struct mm_struct *mm)\n{\n}\n#endif\n\n#endif /* _LINUX_SCHED_MM_H */\n", "/*\n * mm/mmap.c\n *\n * Written by obz.\n *\n * Address space accounting code\t<alan@lxorguk.ukuu.org.uk>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/backing-dev.h>\n#include <linux/mm.h>\n#include <linux/vmacache.h>\n#include <linux/shm.h>\n#include <linux/mman.h>\n#include <linux/pagemap.h>\n#include <linux/swap.h>\n#include <linux/syscalls.h>\n#include <linux/capability.h>\n#include <linux/init.h>\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/personality.h>\n#include <linux/security.h>\n#include <linux/hugetlb.h>\n#include <linux/shmem_fs.h>\n#include <linux/profile.h>\n#include <linux/export.h>\n#include <linux/mount.h>\n#include <linux/mempolicy.h>\n#include <linux/rmap.h>\n#include <linux/mmu_notifier.h>\n#include <linux/mmdebug.h>\n#include <linux/perf_event.h>\n#include <linux/audit.h>\n#include <linux/khugepaged.h>\n#include <linux/uprobes.h>\n#include <linux/rbtree_augmented.h>\n#include <linux/notifier.h>\n#include <linux/memory.h>\n#include <linux/printk.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/moduleparam.h>\n#include <linux/pkeys.h>\n#include <linux/oom.h>\n\n#include <linux/uaccess.h>\n#include <asm/cacheflush.h>\n#include <asm/tlb.h>\n#include <asm/mmu_context.h>\n\n#include \"internal.h\"\n\n#ifndef arch_mmap_check\n#define arch_mmap_check(addr, len, flags)\t(0)\n#endif\n\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_BITS\nconst int mmap_rnd_bits_min = CONFIG_ARCH_MMAP_RND_BITS_MIN;\nconst int mmap_rnd_bits_max = CONFIG_ARCH_MMAP_RND_BITS_MAX;\nint mmap_rnd_bits __read_mostly = CONFIG_ARCH_MMAP_RND_BITS;\n#endif\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_COMPAT_BITS\nconst int mmap_rnd_compat_bits_min = CONFIG_ARCH_MMAP_RND_COMPAT_BITS_MIN;\nconst int mmap_rnd_compat_bits_max = CONFIG_ARCH_MMAP_RND_COMPAT_BITS_MAX;\nint mmap_rnd_compat_bits __read_mostly = CONFIG_ARCH_MMAP_RND_COMPAT_BITS;\n#endif\n\nstatic bool ignore_rlimit_data;\ncore_param(ignore_rlimit_data, ignore_rlimit_data, bool, 0644);\n\nstatic void unmap_region(struct mm_struct *mm,\n\t\tstruct vm_area_struct *vma, struct vm_area_struct *prev,\n\t\tunsigned long start, unsigned long end);\n\n/* description of effects of mapping type and prot in current implementation.\n * this is due to the limited x86 page protection hardware.  The expected\n * behavior is in parens:\n *\n * map_type\tprot\n *\t\tPROT_NONE\tPROT_READ\tPROT_WRITE\tPROT_EXEC\n * MAP_SHARED\tr: (no) no\tr: (yes) yes\tr: (no) yes\tr: (no) yes\n *\t\tw: (no) no\tw: (no) no\tw: (yes) yes\tw: (no) no\n *\t\tx: (no) no\tx: (no) yes\tx: (no) yes\tx: (yes) yes\n *\n * MAP_PRIVATE\tr: (no) no\tr: (yes) yes\tr: (no) yes\tr: (no) yes\n *\t\tw: (no) no\tw: (no) no\tw: (copy) copy\tw: (no) no\n *\t\tx: (no) no\tx: (no) yes\tx: (no) yes\tx: (yes) yes\n *\n * On arm64, PROT_EXEC has the following behaviour for both MAP_SHARED and\n * MAP_PRIVATE:\n *\t\t\t\t\t\t\t\tr: (no) no\n *\t\t\t\t\t\t\t\tw: (no) no\n *\t\t\t\t\t\t\t\tx: (yes) yes\n */\npgprot_t protection_map[16] __ro_after_init = {\n\t__P000, __P001, __P010, __P011, __P100, __P101, __P110, __P111,\n\t__S000, __S001, __S010, __S011, __S100, __S101, __S110, __S111\n};\n\n#ifndef CONFIG_ARCH_HAS_FILTER_PGPROT\nstatic inline pgprot_t arch_filter_pgprot(pgprot_t prot)\n{\n\treturn prot;\n}\n#endif\n\npgprot_t vm_get_page_prot(unsigned long vm_flags)\n{\n\tpgprot_t ret = __pgprot(pgprot_val(protection_map[vm_flags &\n\t\t\t\t(VM_READ|VM_WRITE|VM_EXEC|VM_SHARED)]) |\n\t\t\tpgprot_val(arch_vm_get_page_prot(vm_flags)));\n\n\treturn arch_filter_pgprot(ret);\n}\nEXPORT_SYMBOL(vm_get_page_prot);\n\nstatic pgprot_t vm_pgprot_modify(pgprot_t oldprot, unsigned long vm_flags)\n{\n\treturn pgprot_modify(oldprot, vm_get_page_prot(vm_flags));\n}\n\n/* Update vma->vm_page_prot to reflect vma->vm_flags. */\nvoid vma_set_page_prot(struct vm_area_struct *vma)\n{\n\tunsigned long vm_flags = vma->vm_flags;\n\tpgprot_t vm_page_prot;\n\n\tvm_page_prot = vm_pgprot_modify(vma->vm_page_prot, vm_flags);\n\tif (vma_wants_writenotify(vma, vm_page_prot)) {\n\t\tvm_flags &= ~VM_SHARED;\n\t\tvm_page_prot = vm_pgprot_modify(vm_page_prot, vm_flags);\n\t}\n\t/* remove_protection_ptes reads vma->vm_page_prot without mmap_sem */\n\tWRITE_ONCE(vma->vm_page_prot, vm_page_prot);\n}\n\n/*\n * Requires inode->i_mapping->i_mmap_rwsem\n */\nstatic void __remove_shared_vm_struct(struct vm_area_struct *vma,\n\t\tstruct file *file, struct address_space *mapping)\n{\n\tif (vma->vm_flags & VM_DENYWRITE)\n\t\tatomic_inc(&file_inode(file)->i_writecount);\n\tif (vma->vm_flags & VM_SHARED)\n\t\tmapping_unmap_writable(mapping);\n\n\tflush_dcache_mmap_lock(mapping);\n\tvma_interval_tree_remove(vma, &mapping->i_mmap);\n\tflush_dcache_mmap_unlock(mapping);\n}\n\n/*\n * Unlink a file-based vm structure from its interval tree, to hide\n * vma from rmap and vmtruncate before freeing its page tables.\n */\nvoid unlink_file_vma(struct vm_area_struct *vma)\n{\n\tstruct file *file = vma->vm_file;\n\n\tif (file) {\n\t\tstruct address_space *mapping = file->f_mapping;\n\t\ti_mmap_lock_write(mapping);\n\t\t__remove_shared_vm_struct(vma, file, mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n}\n\n/*\n * Close a vm structure and free it, returning the next.\n */\nstatic struct vm_area_struct *remove_vma(struct vm_area_struct *vma)\n{\n\tstruct vm_area_struct *next = vma->vm_next;\n\n\tmight_sleep();\n\tif (vma->vm_ops && vma->vm_ops->close)\n\t\tvma->vm_ops->close(vma);\n\tif (vma->vm_file)\n\t\tfput(vma->vm_file);\n\tmpol_put(vma_policy(vma));\n\tvm_area_free(vma);\n\treturn next;\n}\n\nstatic int do_brk_flags(unsigned long addr, unsigned long request, unsigned long flags,\n\t\tstruct list_head *uf);\nSYSCALL_DEFINE1(brk, unsigned long, brk)\n{\n\tunsigned long retval;\n\tunsigned long newbrk, oldbrk, origbrk;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *next;\n\tunsigned long min_brk;\n\tbool populate;\n\tbool downgraded = false;\n\tLIST_HEAD(uf);\n\n\tif (down_write_killable(&mm->mmap_sem))\n\t\treturn -EINTR;\n\n\torigbrk = mm->brk;\n\n#ifdef CONFIG_COMPAT_BRK\n\t/*\n\t * CONFIG_COMPAT_BRK can still be overridden by setting\n\t * randomize_va_space to 2, which will still cause mm->start_brk\n\t * to be arbitrarily shifted\n\t */\n\tif (current->brk_randomized)\n\t\tmin_brk = mm->start_brk;\n\telse\n\t\tmin_brk = mm->end_data;\n#else\n\tmin_brk = mm->start_brk;\n#endif\n\tif (brk < min_brk)\n\t\tgoto out;\n\n\t/*\n\t * Check against rlimit here. If this check is done later after the test\n\t * of oldbrk with newbrk then it can escape the test and let the data\n\t * segment grow beyond its set limit the in case where the limit is\n\t * not page aligned -Ram Gupta\n\t */\n\tif (check_data_rlimit(rlimit(RLIMIT_DATA), brk, mm->start_brk,\n\t\t\t      mm->end_data, mm->start_data))\n\t\tgoto out;\n\n\tnewbrk = PAGE_ALIGN(brk);\n\toldbrk = PAGE_ALIGN(mm->brk);\n\tif (oldbrk == newbrk) {\n\t\tmm->brk = brk;\n\t\tgoto success;\n\t}\n\n\t/*\n\t * Always allow shrinking brk.\n\t * __do_munmap() may downgrade mmap_sem to read.\n\t */\n\tif (brk <= mm->brk) {\n\t\tint ret;\n\n\t\t/*\n\t\t * mm->brk must to be protected by write mmap_sem so update it\n\t\t * before downgrading mmap_sem. When __do_munmap() fails,\n\t\t * mm->brk will be restored from origbrk.\n\t\t */\n\t\tmm->brk = brk;\n\t\tret = __do_munmap(mm, newbrk, oldbrk-newbrk, &uf, true);\n\t\tif (ret < 0) {\n\t\t\tmm->brk = origbrk;\n\t\t\tgoto out;\n\t\t} else if (ret == 1) {\n\t\t\tdowngraded = true;\n\t\t}\n\t\tgoto success;\n\t}\n\n\t/* Check against existing mmap mappings. */\n\tnext = find_vma(mm, oldbrk);\n\tif (next && newbrk + PAGE_SIZE > vm_start_gap(next))\n\t\tgoto out;\n\n\t/* Ok, looks good - let it rip. */\n\tif (do_brk_flags(oldbrk, newbrk-oldbrk, 0, &uf) < 0)\n\t\tgoto out;\n\tmm->brk = brk;\n\nsuccess:\n\tpopulate = newbrk > oldbrk && (mm->def_flags & VM_LOCKED) != 0;\n\tif (downgraded)\n\t\tup_read(&mm->mmap_sem);\n\telse\n\t\tup_write(&mm->mmap_sem);\n\tuserfaultfd_unmap_complete(mm, &uf);\n\tif (populate)\n\t\tmm_populate(oldbrk, newbrk - oldbrk);\n\treturn brk;\n\nout:\n\tretval = origbrk;\n\tup_write(&mm->mmap_sem);\n\treturn retval;\n}\n\nstatic long vma_compute_subtree_gap(struct vm_area_struct *vma)\n{\n\tunsigned long max, prev_end, subtree_gap;\n\n\t/*\n\t * Note: in the rare case of a VM_GROWSDOWN above a VM_GROWSUP, we\n\t * allow two stack_guard_gaps between them here, and when choosing\n\t * an unmapped area; whereas when expanding we only require one.\n\t * That's a little inconsistent, but keeps the code here simpler.\n\t */\n\tmax = vm_start_gap(vma);\n\tif (vma->vm_prev) {\n\t\tprev_end = vm_end_gap(vma->vm_prev);\n\t\tif (max > prev_end)\n\t\t\tmax -= prev_end;\n\t\telse\n\t\t\tmax = 0;\n\t}\n\tif (vma->vm_rb.rb_left) {\n\t\tsubtree_gap = rb_entry(vma->vm_rb.rb_left,\n\t\t\t\tstruct vm_area_struct, vm_rb)->rb_subtree_gap;\n\t\tif (subtree_gap > max)\n\t\t\tmax = subtree_gap;\n\t}\n\tif (vma->vm_rb.rb_right) {\n\t\tsubtree_gap = rb_entry(vma->vm_rb.rb_right,\n\t\t\t\tstruct vm_area_struct, vm_rb)->rb_subtree_gap;\n\t\tif (subtree_gap > max)\n\t\t\tmax = subtree_gap;\n\t}\n\treturn max;\n}\n\n#ifdef CONFIG_DEBUG_VM_RB\nstatic int browse_rb(struct mm_struct *mm)\n{\n\tstruct rb_root *root = &mm->mm_rb;\n\tint i = 0, j, bug = 0;\n\tstruct rb_node *nd, *pn = NULL;\n\tunsigned long prev = 0, pend = 0;\n\n\tfor (nd = rb_first(root); nd; nd = rb_next(nd)) {\n\t\tstruct vm_area_struct *vma;\n\t\tvma = rb_entry(nd, struct vm_area_struct, vm_rb);\n\t\tif (vma->vm_start < prev) {\n\t\t\tpr_emerg(\"vm_start %lx < prev %lx\\n\",\n\t\t\t\t  vma->vm_start, prev);\n\t\t\tbug = 1;\n\t\t}\n\t\tif (vma->vm_start < pend) {\n\t\t\tpr_emerg(\"vm_start %lx < pend %lx\\n\",\n\t\t\t\t  vma->vm_start, pend);\n\t\t\tbug = 1;\n\t\t}\n\t\tif (vma->vm_start > vma->vm_end) {\n\t\t\tpr_emerg(\"vm_start %lx > vm_end %lx\\n\",\n\t\t\t\t  vma->vm_start, vma->vm_end);\n\t\t\tbug = 1;\n\t\t}\n\t\tspin_lock(&mm->page_table_lock);\n\t\tif (vma->rb_subtree_gap != vma_compute_subtree_gap(vma)) {\n\t\t\tpr_emerg(\"free gap %lx, correct %lx\\n\",\n\t\t\t       vma->rb_subtree_gap,\n\t\t\t       vma_compute_subtree_gap(vma));\n\t\t\tbug = 1;\n\t\t}\n\t\tspin_unlock(&mm->page_table_lock);\n\t\ti++;\n\t\tpn = nd;\n\t\tprev = vma->vm_start;\n\t\tpend = vma->vm_end;\n\t}\n\tj = 0;\n\tfor (nd = pn; nd; nd = rb_prev(nd))\n\t\tj++;\n\tif (i != j) {\n\t\tpr_emerg(\"backwards %d, forwards %d\\n\", j, i);\n\t\tbug = 1;\n\t}\n\treturn bug ? -1 : i;\n}\n\nstatic void validate_mm_rb(struct rb_root *root, struct vm_area_struct *ignore)\n{\n\tstruct rb_node *nd;\n\n\tfor (nd = rb_first(root); nd; nd = rb_next(nd)) {\n\t\tstruct vm_area_struct *vma;\n\t\tvma = rb_entry(nd, struct vm_area_struct, vm_rb);\n\t\tVM_BUG_ON_VMA(vma != ignore &&\n\t\t\tvma->rb_subtree_gap != vma_compute_subtree_gap(vma),\n\t\t\tvma);\n\t}\n}\n\nstatic void validate_mm(struct mm_struct *mm)\n{\n\tint bug = 0;\n\tint i = 0;\n\tunsigned long highest_address = 0;\n\tstruct vm_area_struct *vma = mm->mmap;\n\n\twhile (vma) {\n\t\tstruct anon_vma *anon_vma = vma->anon_vma;\n\t\tstruct anon_vma_chain *avc;\n\n\t\tif (anon_vma) {\n\t\t\tanon_vma_lock_read(anon_vma);\n\t\t\tlist_for_each_entry(avc, &vma->anon_vma_chain, same_vma)\n\t\t\t\tanon_vma_interval_tree_verify(avc);\n\t\t\tanon_vma_unlock_read(anon_vma);\n\t\t}\n\n\t\thighest_address = vm_end_gap(vma);\n\t\tvma = vma->vm_next;\n\t\ti++;\n\t}\n\tif (i != mm->map_count) {\n\t\tpr_emerg(\"map_count %d vm_next %d\\n\", mm->map_count, i);\n\t\tbug = 1;\n\t}\n\tif (highest_address != mm->highest_vm_end) {\n\t\tpr_emerg(\"mm->highest_vm_end %lx, found %lx\\n\",\n\t\t\t  mm->highest_vm_end, highest_address);\n\t\tbug = 1;\n\t}\n\ti = browse_rb(mm);\n\tif (i != mm->map_count) {\n\t\tif (i != -1)\n\t\t\tpr_emerg(\"map_count %d rb %d\\n\", mm->map_count, i);\n\t\tbug = 1;\n\t}\n\tVM_BUG_ON_MM(bug, mm);\n}\n#else\n#define validate_mm_rb(root, ignore) do { } while (0)\n#define validate_mm(mm) do { } while (0)\n#endif\n\nRB_DECLARE_CALLBACKS(static, vma_gap_callbacks, struct vm_area_struct, vm_rb,\n\t\t     unsigned long, rb_subtree_gap, vma_compute_subtree_gap)\n\n/*\n * Update augmented rbtree rb_subtree_gap values after vma->vm_start or\n * vma->vm_prev->vm_end values changed, without modifying the vma's position\n * in the rbtree.\n */\nstatic void vma_gap_update(struct vm_area_struct *vma)\n{\n\t/*\n\t * As it turns out, RB_DECLARE_CALLBACKS() already created a callback\n\t * function that does exactly what we want.\n\t */\n\tvma_gap_callbacks_propagate(&vma->vm_rb, NULL);\n}\n\nstatic inline void vma_rb_insert(struct vm_area_struct *vma,\n\t\t\t\t struct rb_root *root)\n{\n\t/* All rb_subtree_gap values must be consistent prior to insertion */\n\tvalidate_mm_rb(root, NULL);\n\n\trb_insert_augmented(&vma->vm_rb, root, &vma_gap_callbacks);\n}\n\nstatic void __vma_rb_erase(struct vm_area_struct *vma, struct rb_root *root)\n{\n\t/*\n\t * Note rb_erase_augmented is a fairly large inline function,\n\t * so make sure we instantiate it only once with our desired\n\t * augmented rbtree callbacks.\n\t */\n\trb_erase_augmented(&vma->vm_rb, root, &vma_gap_callbacks);\n}\n\nstatic __always_inline void vma_rb_erase_ignore(struct vm_area_struct *vma,\n\t\t\t\t\t\tstruct rb_root *root,\n\t\t\t\t\t\tstruct vm_area_struct *ignore)\n{\n\t/*\n\t * All rb_subtree_gap values must be consistent prior to erase,\n\t * with the possible exception of the \"next\" vma being erased if\n\t * next->vm_start was reduced.\n\t */\n\tvalidate_mm_rb(root, ignore);\n\n\t__vma_rb_erase(vma, root);\n}\n\nstatic __always_inline void vma_rb_erase(struct vm_area_struct *vma,\n\t\t\t\t\t struct rb_root *root)\n{\n\t/*\n\t * All rb_subtree_gap values must be consistent prior to erase,\n\t * with the possible exception of the vma being erased.\n\t */\n\tvalidate_mm_rb(root, vma);\n\n\t__vma_rb_erase(vma, root);\n}\n\n/*\n * vma has some anon_vma assigned, and is already inserted on that\n * anon_vma's interval trees.\n *\n * Before updating the vma's vm_start / vm_end / vm_pgoff fields, the\n * vma must be removed from the anon_vma's interval trees using\n * anon_vma_interval_tree_pre_update_vma().\n *\n * After the update, the vma will be reinserted using\n * anon_vma_interval_tree_post_update_vma().\n *\n * The entire update must be protected by exclusive mmap_sem and by\n * the root anon_vma's mutex.\n */\nstatic inline void\nanon_vma_interval_tree_pre_update_vma(struct vm_area_struct *vma)\n{\n\tstruct anon_vma_chain *avc;\n\n\tlist_for_each_entry(avc, &vma->anon_vma_chain, same_vma)\n\t\tanon_vma_interval_tree_remove(avc, &avc->anon_vma->rb_root);\n}\n\nstatic inline void\nanon_vma_interval_tree_post_update_vma(struct vm_area_struct *vma)\n{\n\tstruct anon_vma_chain *avc;\n\n\tlist_for_each_entry(avc, &vma->anon_vma_chain, same_vma)\n\t\tanon_vma_interval_tree_insert(avc, &avc->anon_vma->rb_root);\n}\n\nstatic int find_vma_links(struct mm_struct *mm, unsigned long addr,\n\t\tunsigned long end, struct vm_area_struct **pprev,\n\t\tstruct rb_node ***rb_link, struct rb_node **rb_parent)\n{\n\tstruct rb_node **__rb_link, *__rb_parent, *rb_prev;\n\n\t__rb_link = &mm->mm_rb.rb_node;\n\trb_prev = __rb_parent = NULL;\n\n\twhile (*__rb_link) {\n\t\tstruct vm_area_struct *vma_tmp;\n\n\t\t__rb_parent = *__rb_link;\n\t\tvma_tmp = rb_entry(__rb_parent, struct vm_area_struct, vm_rb);\n\n\t\tif (vma_tmp->vm_end > addr) {\n\t\t\t/* Fail if an existing vma overlaps the area */\n\t\t\tif (vma_tmp->vm_start < end)\n\t\t\t\treturn -ENOMEM;\n\t\t\t__rb_link = &__rb_parent->rb_left;\n\t\t} else {\n\t\t\trb_prev = __rb_parent;\n\t\t\t__rb_link = &__rb_parent->rb_right;\n\t\t}\n\t}\n\n\t*pprev = NULL;\n\tif (rb_prev)\n\t\t*pprev = rb_entry(rb_prev, struct vm_area_struct, vm_rb);\n\t*rb_link = __rb_link;\n\t*rb_parent = __rb_parent;\n\treturn 0;\n}\n\nstatic unsigned long count_vma_pages_range(struct mm_struct *mm,\n\t\tunsigned long addr, unsigned long end)\n{\n\tunsigned long nr_pages = 0;\n\tstruct vm_area_struct *vma;\n\n\t/* Find first overlaping mapping */\n\tvma = find_vma_intersection(mm, addr, end);\n\tif (!vma)\n\t\treturn 0;\n\n\tnr_pages = (min(end, vma->vm_end) -\n\t\tmax(addr, vma->vm_start)) >> PAGE_SHIFT;\n\n\t/* Iterate over the rest of the overlaps */\n\tfor (vma = vma->vm_next; vma; vma = vma->vm_next) {\n\t\tunsigned long overlap_len;\n\n\t\tif (vma->vm_start > end)\n\t\t\tbreak;\n\n\t\toverlap_len = min(end, vma->vm_end) - vma->vm_start;\n\t\tnr_pages += overlap_len >> PAGE_SHIFT;\n\t}\n\n\treturn nr_pages;\n}\n\nvoid __vma_link_rb(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\tstruct rb_node **rb_link, struct rb_node *rb_parent)\n{\n\t/* Update tracking information for the gap following the new vma. */\n\tif (vma->vm_next)\n\t\tvma_gap_update(vma->vm_next);\n\telse\n\t\tmm->highest_vm_end = vm_end_gap(vma);\n\n\t/*\n\t * vma->vm_prev wasn't known when we followed the rbtree to find the\n\t * correct insertion point for that vma. As a result, we could not\n\t * update the vma vm_rb parents rb_subtree_gap values on the way down.\n\t * So, we first insert the vma with a zero rb_subtree_gap value\n\t * (to be consistent with what we did on the way down), and then\n\t * immediately update the gap to the correct value. Finally we\n\t * rebalance the rbtree after all augmented values have been set.\n\t */\n\trb_link_node(&vma->vm_rb, rb_parent, rb_link);\n\tvma->rb_subtree_gap = 0;\n\tvma_gap_update(vma);\n\tvma_rb_insert(vma, &mm->mm_rb);\n}\n\nstatic void __vma_link_file(struct vm_area_struct *vma)\n{\n\tstruct file *file;\n\n\tfile = vma->vm_file;\n\tif (file) {\n\t\tstruct address_space *mapping = file->f_mapping;\n\n\t\tif (vma->vm_flags & VM_DENYWRITE)\n\t\t\tatomic_dec(&file_inode(file)->i_writecount);\n\t\tif (vma->vm_flags & VM_SHARED)\n\t\t\tatomic_inc(&mapping->i_mmap_writable);\n\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_insert(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t}\n}\n\nstatic void\n__vma_link(struct mm_struct *mm, struct vm_area_struct *vma,\n\tstruct vm_area_struct *prev, struct rb_node **rb_link,\n\tstruct rb_node *rb_parent)\n{\n\t__vma_link_list(mm, vma, prev, rb_parent);\n\t__vma_link_rb(mm, vma, rb_link, rb_parent);\n}\n\nstatic void vma_link(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tstruct vm_area_struct *prev, struct rb_node **rb_link,\n\t\t\tstruct rb_node *rb_parent)\n{\n\tstruct address_space *mapping = NULL;\n\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\t\ti_mmap_lock_write(mapping);\n\t}\n\n\t__vma_link(mm, vma, prev, rb_link, rb_parent);\n\t__vma_link_file(vma);\n\n\tif (mapping)\n\t\ti_mmap_unlock_write(mapping);\n\n\tmm->map_count++;\n\tvalidate_mm(mm);\n}\n\n/*\n * Helper for vma_adjust() in the split_vma insert case: insert a vma into the\n * mm's list and rbtree.  It has already been inserted into the interval tree.\n */\nstatic void __insert_vm_struct(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tstruct vm_area_struct *prev;\n\tstruct rb_node **rb_link, *rb_parent;\n\n\tif (find_vma_links(mm, vma->vm_start, vma->vm_end,\n\t\t\t   &prev, &rb_link, &rb_parent))\n\t\tBUG();\n\t__vma_link(mm, vma, prev, rb_link, rb_parent);\n\tmm->map_count++;\n}\n\nstatic __always_inline void __vma_unlink_common(struct mm_struct *mm,\n\t\t\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\t\t\tstruct vm_area_struct *prev,\n\t\t\t\t\t\tbool has_prev,\n\t\t\t\t\t\tstruct vm_area_struct *ignore)\n{\n\tstruct vm_area_struct *next;\n\n\tvma_rb_erase_ignore(vma, &mm->mm_rb, ignore);\n\tnext = vma->vm_next;\n\tif (has_prev)\n\t\tprev->vm_next = next;\n\telse {\n\t\tprev = vma->vm_prev;\n\t\tif (prev)\n\t\t\tprev->vm_next = next;\n\t\telse\n\t\t\tmm->mmap = next;\n\t}\n\tif (next)\n\t\tnext->vm_prev = prev;\n\n\t/* Kill the cache */\n\tvmacache_invalidate(mm);\n}\n\nstatic inline void __vma_unlink_prev(struct mm_struct *mm,\n\t\t\t\t     struct vm_area_struct *vma,\n\t\t\t\t     struct vm_area_struct *prev)\n{\n\t__vma_unlink_common(mm, vma, prev, true, vma);\n}\n\n/*\n * We cannot adjust vm_start, vm_end, vm_pgoff fields of a vma that\n * is already present in an i_mmap tree without adjusting the tree.\n * The following helper function should be used when such adjustments\n * are necessary.  The \"insert\" vma (if any) is to be inserted\n * before we drop the necessary locks.\n */\nint __vma_adjust(struct vm_area_struct *vma, unsigned long start,\n\tunsigned long end, pgoff_t pgoff, struct vm_area_struct *insert,\n\tstruct vm_area_struct *expand)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct vm_area_struct *next = vma->vm_next, *orig_vma = vma;\n\tstruct address_space *mapping = NULL;\n\tstruct rb_root_cached *root = NULL;\n\tstruct anon_vma *anon_vma = NULL;\n\tstruct file *file = vma->vm_file;\n\tbool start_changed = false, end_changed = false;\n\tlong adjust_next = 0;\n\tint remove_next = 0;\n\n\tif (next && !insert) {\n\t\tstruct vm_area_struct *exporter = NULL, *importer = NULL;\n\n\t\tif (end >= next->vm_end) {\n\t\t\t/*\n\t\t\t * vma expands, overlapping all the next, and\n\t\t\t * perhaps the one after too (mprotect case 6).\n\t\t\t * The only other cases that gets here are\n\t\t\t * case 1, case 7 and case 8.\n\t\t\t */\n\t\t\tif (next == expand) {\n\t\t\t\t/*\n\t\t\t\t * The only case where we don't expand \"vma\"\n\t\t\t\t * and we expand \"next\" instead is case 8.\n\t\t\t\t */\n\t\t\t\tVM_WARN_ON(end != next->vm_end);\n\t\t\t\t/*\n\t\t\t\t * remove_next == 3 means we're\n\t\t\t\t * removing \"vma\" and that to do so we\n\t\t\t\t * swapped \"vma\" and \"next\".\n\t\t\t\t */\n\t\t\t\tremove_next = 3;\n\t\t\t\tVM_WARN_ON(file != next->vm_file);\n\t\t\t\tswap(vma, next);\n\t\t\t} else {\n\t\t\t\tVM_WARN_ON(expand != vma);\n\t\t\t\t/*\n\t\t\t\t * case 1, 6, 7, remove_next == 2 is case 6,\n\t\t\t\t * remove_next == 1 is case 1 or 7.\n\t\t\t\t */\n\t\t\t\tremove_next = 1 + (end > next->vm_end);\n\t\t\t\tVM_WARN_ON(remove_next == 2 &&\n\t\t\t\t\t   end != next->vm_next->vm_end);\n\t\t\t\tVM_WARN_ON(remove_next == 1 &&\n\t\t\t\t\t   end != next->vm_end);\n\t\t\t\t/* trim end to next, for case 6 first pass */\n\t\t\t\tend = next->vm_end;\n\t\t\t}\n\n\t\t\texporter = next;\n\t\t\timporter = vma;\n\n\t\t\t/*\n\t\t\t * If next doesn't have anon_vma, import from vma after\n\t\t\t * next, if the vma overlaps with it.\n\t\t\t */\n\t\t\tif (remove_next == 2 && !next->anon_vma)\n\t\t\t\texporter = next->vm_next;\n\n\t\t} else if (end > next->vm_start) {\n\t\t\t/*\n\t\t\t * vma expands, overlapping part of the next:\n\t\t\t * mprotect case 5 shifting the boundary up.\n\t\t\t */\n\t\t\tadjust_next = (end - next->vm_start) >> PAGE_SHIFT;\n\t\t\texporter = next;\n\t\t\timporter = vma;\n\t\t\tVM_WARN_ON(expand != importer);\n\t\t} else if (end < vma->vm_end) {\n\t\t\t/*\n\t\t\t * vma shrinks, and !insert tells it's not\n\t\t\t * split_vma inserting another: so it must be\n\t\t\t * mprotect case 4 shifting the boundary down.\n\t\t\t */\n\t\t\tadjust_next = -((vma->vm_end - end) >> PAGE_SHIFT);\n\t\t\texporter = vma;\n\t\t\timporter = next;\n\t\t\tVM_WARN_ON(expand != importer);\n\t\t}\n\n\t\t/*\n\t\t * Easily overlooked: when mprotect shifts the boundary,\n\t\t * make sure the expanding vma has anon_vma set if the\n\t\t * shrinking vma had, to cover any anon pages imported.\n\t\t */\n\t\tif (exporter && exporter->anon_vma && !importer->anon_vma) {\n\t\t\tint error;\n\n\t\t\timporter->anon_vma = exporter->anon_vma;\n\t\t\terror = anon_vma_clone(importer, exporter);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t}\nagain:\n\tvma_adjust_trans_huge(orig_vma, start, end, adjust_next);\n\n\tif (file) {\n\t\tmapping = file->f_mapping;\n\t\troot = &mapping->i_mmap;\n\t\tuprobe_munmap(vma, vma->vm_start, vma->vm_end);\n\n\t\tif (adjust_next)\n\t\t\tuprobe_munmap(next, next->vm_start, next->vm_end);\n\n\t\ti_mmap_lock_write(mapping);\n\t\tif (insert) {\n\t\t\t/*\n\t\t\t * Put into interval tree now, so instantiated pages\n\t\t\t * are visible to arm/parisc __flush_dcache_page\n\t\t\t * throughout; but we cannot insert into address\n\t\t\t * space until vma start or end is updated.\n\t\t\t */\n\t\t\t__vma_link_file(insert);\n\t\t}\n\t}\n\n\tanon_vma = vma->anon_vma;\n\tif (!anon_vma && adjust_next)\n\t\tanon_vma = next->anon_vma;\n\tif (anon_vma) {\n\t\tVM_WARN_ON(adjust_next && next->anon_vma &&\n\t\t\t   anon_vma != next->anon_vma);\n\t\tanon_vma_lock_write(anon_vma);\n\t\tanon_vma_interval_tree_pre_update_vma(vma);\n\t\tif (adjust_next)\n\t\t\tanon_vma_interval_tree_pre_update_vma(next);\n\t}\n\n\tif (root) {\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_remove(vma, root);\n\t\tif (adjust_next)\n\t\t\tvma_interval_tree_remove(next, root);\n\t}\n\n\tif (start != vma->vm_start) {\n\t\tvma->vm_start = start;\n\t\tstart_changed = true;\n\t}\n\tif (end != vma->vm_end) {\n\t\tvma->vm_end = end;\n\t\tend_changed = true;\n\t}\n\tvma->vm_pgoff = pgoff;\n\tif (adjust_next) {\n\t\tnext->vm_start += adjust_next << PAGE_SHIFT;\n\t\tnext->vm_pgoff += adjust_next;\n\t}\n\n\tif (root) {\n\t\tif (adjust_next)\n\t\t\tvma_interval_tree_insert(next, root);\n\t\tvma_interval_tree_insert(vma, root);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t}\n\n\tif (remove_next) {\n\t\t/*\n\t\t * vma_merge has merged next into vma, and needs\n\t\t * us to remove next before dropping the locks.\n\t\t */\n\t\tif (remove_next != 3)\n\t\t\t__vma_unlink_prev(mm, next, vma);\n\t\telse\n\t\t\t/*\n\t\t\t * vma is not before next if they've been\n\t\t\t * swapped.\n\t\t\t *\n\t\t\t * pre-swap() next->vm_start was reduced so\n\t\t\t * tell validate_mm_rb to ignore pre-swap()\n\t\t\t * \"next\" (which is stored in post-swap()\n\t\t\t * \"vma\").\n\t\t\t */\n\t\t\t__vma_unlink_common(mm, next, NULL, false, vma);\n\t\tif (file)\n\t\t\t__remove_shared_vm_struct(next, file, mapping);\n\t} else if (insert) {\n\t\t/*\n\t\t * split_vma has split insert from vma, and needs\n\t\t * us to insert it before dropping the locks\n\t\t * (it may either follow vma or precede it).\n\t\t */\n\t\t__insert_vm_struct(mm, insert);\n\t} else {\n\t\tif (start_changed)\n\t\t\tvma_gap_update(vma);\n\t\tif (end_changed) {\n\t\t\tif (!next)\n\t\t\t\tmm->highest_vm_end = vm_end_gap(vma);\n\t\t\telse if (!adjust_next)\n\t\t\t\tvma_gap_update(next);\n\t\t}\n\t}\n\n\tif (anon_vma) {\n\t\tanon_vma_interval_tree_post_update_vma(vma);\n\t\tif (adjust_next)\n\t\t\tanon_vma_interval_tree_post_update_vma(next);\n\t\tanon_vma_unlock_write(anon_vma);\n\t}\n\tif (mapping)\n\t\ti_mmap_unlock_write(mapping);\n\n\tif (root) {\n\t\tuprobe_mmap(vma);\n\n\t\tif (adjust_next)\n\t\t\tuprobe_mmap(next);\n\t}\n\n\tif (remove_next) {\n\t\tif (file) {\n\t\t\tuprobe_munmap(next, next->vm_start, next->vm_end);\n\t\t\tfput(file);\n\t\t}\n\t\tif (next->anon_vma)\n\t\t\tanon_vma_merge(vma, next);\n\t\tmm->map_count--;\n\t\tmpol_put(vma_policy(next));\n\t\tvm_area_free(next);\n\t\t/*\n\t\t * In mprotect's case 6 (see comments on vma_merge),\n\t\t * we must remove another next too. It would clutter\n\t\t * up the code too much to do both in one go.\n\t\t */\n\t\tif (remove_next != 3) {\n\t\t\t/*\n\t\t\t * If \"next\" was removed and vma->vm_end was\n\t\t\t * expanded (up) over it, in turn\n\t\t\t * \"next->vm_prev->vm_end\" changed and the\n\t\t\t * \"vma->vm_next\" gap must be updated.\n\t\t\t */\n\t\t\tnext = vma->vm_next;\n\t\t} else {\n\t\t\t/*\n\t\t\t * For the scope of the comment \"next\" and\n\t\t\t * \"vma\" considered pre-swap(): if \"vma\" was\n\t\t\t * removed, next->vm_start was expanded (down)\n\t\t\t * over it and the \"next\" gap must be updated.\n\t\t\t * Because of the swap() the post-swap() \"vma\"\n\t\t\t * actually points to pre-swap() \"next\"\n\t\t\t * (post-swap() \"next\" as opposed is now a\n\t\t\t * dangling pointer).\n\t\t\t */\n\t\t\tnext = vma;\n\t\t}\n\t\tif (remove_next == 2) {\n\t\t\tremove_next = 1;\n\t\t\tend = next->vm_end;\n\t\t\tgoto again;\n\t\t}\n\t\telse if (next)\n\t\t\tvma_gap_update(next);\n\t\telse {\n\t\t\t/*\n\t\t\t * If remove_next == 2 we obviously can't\n\t\t\t * reach this path.\n\t\t\t *\n\t\t\t * If remove_next == 3 we can't reach this\n\t\t\t * path because pre-swap() next is always not\n\t\t\t * NULL. pre-swap() \"next\" is not being\n\t\t\t * removed and its next->vm_end is not altered\n\t\t\t * (and furthermore \"end\" already matches\n\t\t\t * next->vm_end in remove_next == 3).\n\t\t\t *\n\t\t\t * We reach this only in the remove_next == 1\n\t\t\t * case if the \"next\" vma that was removed was\n\t\t\t * the highest vma of the mm. However in such\n\t\t\t * case next->vm_end == \"end\" and the extended\n\t\t\t * \"vma\" has vma->vm_end == next->vm_end so\n\t\t\t * mm->highest_vm_end doesn't need any update\n\t\t\t * in remove_next == 1 case.\n\t\t\t */\n\t\t\tVM_WARN_ON(mm->highest_vm_end != vm_end_gap(vma));\n\t\t}\n\t}\n\tif (insert && file)\n\t\tuprobe_mmap(insert);\n\n\tvalidate_mm(mm);\n\n\treturn 0;\n}\n\n/*\n * If the vma has a ->close operation then the driver probably needs to release\n * per-vma resources, so we don't attempt to merge those.\n */\nstatic inline int is_mergeable_vma(struct vm_area_struct *vma,\n\t\t\t\tstruct file *file, unsigned long vm_flags,\n\t\t\t\tstruct vm_userfaultfd_ctx vm_userfaultfd_ctx)\n{\n\t/*\n\t * VM_SOFTDIRTY should not prevent from VMA merging, if we\n\t * match the flags but dirty bit -- the caller should mark\n\t * merged VMA as dirty. If dirty bit won't be excluded from\n\t * comparison, we increase pressure on the memory system forcing\n\t * the kernel to generate new VMAs when old one could be\n\t * extended instead.\n\t */\n\tif ((vma->vm_flags ^ vm_flags) & ~VM_SOFTDIRTY)\n\t\treturn 0;\n\tif (vma->vm_file != file)\n\t\treturn 0;\n\tif (vma->vm_ops && vma->vm_ops->close)\n\t\treturn 0;\n\tif (!is_mergeable_vm_userfaultfd_ctx(vma, vm_userfaultfd_ctx))\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic inline int is_mergeable_anon_vma(struct anon_vma *anon_vma1,\n\t\t\t\t\tstruct anon_vma *anon_vma2,\n\t\t\t\t\tstruct vm_area_struct *vma)\n{\n\t/*\n\t * The list_is_singular() test is to avoid merging VMA cloned from\n\t * parents. This can improve scalability caused by anon_vma lock.\n\t */\n\tif ((!anon_vma1 || !anon_vma2) && (!vma ||\n\t\tlist_is_singular(&vma->anon_vma_chain)))\n\t\treturn 1;\n\treturn anon_vma1 == anon_vma2;\n}\n\n/*\n * Return true if we can merge this (vm_flags,anon_vma,file,vm_pgoff)\n * in front of (at a lower virtual address and file offset than) the vma.\n *\n * We cannot merge two vmas if they have differently assigned (non-NULL)\n * anon_vmas, nor if same anon_vma is assigned but offsets incompatible.\n *\n * We don't check here for the merged mmap wrapping around the end of pagecache\n * indices (16TB on ia32) because do_mmap_pgoff() does not permit mmap's which\n * wrap, nor mmaps which cover the final page at index -1UL.\n */\nstatic int\ncan_vma_merge_before(struct vm_area_struct *vma, unsigned long vm_flags,\n\t\t     struct anon_vma *anon_vma, struct file *file,\n\t\t     pgoff_t vm_pgoff,\n\t\t     struct vm_userfaultfd_ctx vm_userfaultfd_ctx)\n{\n\tif (is_mergeable_vma(vma, file, vm_flags, vm_userfaultfd_ctx) &&\n\t    is_mergeable_anon_vma(anon_vma, vma->anon_vma, vma)) {\n\t\tif (vma->vm_pgoff == vm_pgoff)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/*\n * Return true if we can merge this (vm_flags,anon_vma,file,vm_pgoff)\n * beyond (at a higher virtual address and file offset than) the vma.\n *\n * We cannot merge two vmas if they have differently assigned (non-NULL)\n * anon_vmas, nor if same anon_vma is assigned but offsets incompatible.\n */\nstatic int\ncan_vma_merge_after(struct vm_area_struct *vma, unsigned long vm_flags,\n\t\t    struct anon_vma *anon_vma, struct file *file,\n\t\t    pgoff_t vm_pgoff,\n\t\t    struct vm_userfaultfd_ctx vm_userfaultfd_ctx)\n{\n\tif (is_mergeable_vma(vma, file, vm_flags, vm_userfaultfd_ctx) &&\n\t    is_mergeable_anon_vma(anon_vma, vma->anon_vma, vma)) {\n\t\tpgoff_t vm_pglen;\n\t\tvm_pglen = vma_pages(vma);\n\t\tif (vma->vm_pgoff + vm_pglen == vm_pgoff)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/*\n * Given a mapping request (addr,end,vm_flags,file,pgoff), figure out\n * whether that can be merged with its predecessor or its successor.\n * Or both (it neatly fills a hole).\n *\n * In most cases - when called for mmap, brk or mremap - [addr,end) is\n * certain not to be mapped by the time vma_merge is called; but when\n * called for mprotect, it is certain to be already mapped (either at\n * an offset within prev, or at the start of next), and the flags of\n * this area are about to be changed to vm_flags - and the no-change\n * case has already been eliminated.\n *\n * The following mprotect cases have to be considered, where AAAA is\n * the area passed down from mprotect_fixup, never extending beyond one\n * vma, PPPPPP is the prev vma specified, and NNNNNN the next vma after:\n *\n *     AAAA             AAAA                AAAA          AAAA\n *    PPPPPPNNNNNN    PPPPPPNNNNNN    PPPPPPNNNNNN    PPPPNNNNXXXX\n *    cannot merge    might become    might become    might become\n *                    PPNNNNNNNNNN    PPPPPPPPPPNN    PPPPPPPPPPPP 6 or\n *    mmap, brk or    case 4 below    case 5 below    PPPPPPPPXXXX 7 or\n *    mremap move:                                    PPPPXXXXXXXX 8\n *        AAAA\n *    PPPP    NNNN    PPPPPPPPPPPP    PPPPPPPPNNNN    PPPPNNNNNNNN\n *    might become    case 1 below    case 2 below    case 3 below\n *\n * It is important for case 8 that the vma NNNN overlapping the\n * region AAAA is never going to extended over XXXX. Instead XXXX must\n * be extended in region AAAA and NNNN must be removed. This way in\n * all cases where vma_merge succeeds, the moment vma_adjust drops the\n * rmap_locks, the properties of the merged vma will be already\n * correct for the whole merged range. Some of those properties like\n * vm_page_prot/vm_flags may be accessed by rmap_walks and they must\n * be correct for the whole merged range immediately after the\n * rmap_locks are released. Otherwise if XXXX would be removed and\n * NNNN would be extended over the XXXX range, remove_migration_ptes\n * or other rmap walkers (if working on addresses beyond the \"end\"\n * parameter) may establish ptes with the wrong permissions of NNNN\n * instead of the right permissions of XXXX.\n */\nstruct vm_area_struct *vma_merge(struct mm_struct *mm,\n\t\t\tstruct vm_area_struct *prev, unsigned long addr,\n\t\t\tunsigned long end, unsigned long vm_flags,\n\t\t\tstruct anon_vma *anon_vma, struct file *file,\n\t\t\tpgoff_t pgoff, struct mempolicy *policy,\n\t\t\tstruct vm_userfaultfd_ctx vm_userfaultfd_ctx)\n{\n\tpgoff_t pglen = (end - addr) >> PAGE_SHIFT;\n\tstruct vm_area_struct *area, *next;\n\tint err;\n\n\t/*\n\t * We later require that vma->vm_flags == vm_flags,\n\t * so this tests vma->vm_flags & VM_SPECIAL, too.\n\t */\n\tif (vm_flags & VM_SPECIAL)\n\t\treturn NULL;\n\n\tif (prev)\n\t\tnext = prev->vm_next;\n\telse\n\t\tnext = mm->mmap;\n\tarea = next;\n\tif (area && area->vm_end == end)\t\t/* cases 6, 7, 8 */\n\t\tnext = next->vm_next;\n\n\t/* verify some invariant that must be enforced by the caller */\n\tVM_WARN_ON(prev && addr <= prev->vm_start);\n\tVM_WARN_ON(area && end > area->vm_end);\n\tVM_WARN_ON(addr >= end);\n\n\t/*\n\t * Can it merge with the predecessor?\n\t */\n\tif (prev && prev->vm_end == addr &&\n\t\t\tmpol_equal(vma_policy(prev), policy) &&\n\t\t\tcan_vma_merge_after(prev, vm_flags,\n\t\t\t\t\t    anon_vma, file, pgoff,\n\t\t\t\t\t    vm_userfaultfd_ctx)) {\n\t\t/*\n\t\t * OK, it can.  Can we now merge in the successor as well?\n\t\t */\n\t\tif (next && end == next->vm_start &&\n\t\t\t\tmpol_equal(policy, vma_policy(next)) &&\n\t\t\t\tcan_vma_merge_before(next, vm_flags,\n\t\t\t\t\t\t     anon_vma, file,\n\t\t\t\t\t\t     pgoff+pglen,\n\t\t\t\t\t\t     vm_userfaultfd_ctx) &&\n\t\t\t\tis_mergeable_anon_vma(prev->anon_vma,\n\t\t\t\t\t\t      next->anon_vma, NULL)) {\n\t\t\t\t\t\t\t/* cases 1, 6 */\n\t\t\terr = __vma_adjust(prev, prev->vm_start,\n\t\t\t\t\t next->vm_end, prev->vm_pgoff, NULL,\n\t\t\t\t\t prev);\n\t\t} else\t\t\t\t\t/* cases 2, 5, 7 */\n\t\t\terr = __vma_adjust(prev, prev->vm_start,\n\t\t\t\t\t end, prev->vm_pgoff, NULL, prev);\n\t\tif (err)\n\t\t\treturn NULL;\n\t\tkhugepaged_enter_vma_merge(prev, vm_flags);\n\t\treturn prev;\n\t}\n\n\t/*\n\t * Can this new request be merged in front of next?\n\t */\n\tif (next && end == next->vm_start &&\n\t\t\tmpol_equal(policy, vma_policy(next)) &&\n\t\t\tcan_vma_merge_before(next, vm_flags,\n\t\t\t\t\t     anon_vma, file, pgoff+pglen,\n\t\t\t\t\t     vm_userfaultfd_ctx)) {\n\t\tif (prev && addr < prev->vm_end)\t/* case 4 */\n\t\t\terr = __vma_adjust(prev, prev->vm_start,\n\t\t\t\t\t addr, prev->vm_pgoff, NULL, next);\n\t\telse {\t\t\t\t\t/* cases 3, 8 */\n\t\t\terr = __vma_adjust(area, addr, next->vm_end,\n\t\t\t\t\t next->vm_pgoff - pglen, NULL, next);\n\t\t\t/*\n\t\t\t * In case 3 area is already equal to next and\n\t\t\t * this is a noop, but in case 8 \"area\" has\n\t\t\t * been removed and next was expanded over it.\n\t\t\t */\n\t\t\tarea = next;\n\t\t}\n\t\tif (err)\n\t\t\treturn NULL;\n\t\tkhugepaged_enter_vma_merge(area, vm_flags);\n\t\treturn area;\n\t}\n\n\treturn NULL;\n}\n\n/*\n * Rough compatbility check to quickly see if it's even worth looking\n * at sharing an anon_vma.\n *\n * They need to have the same vm_file, and the flags can only differ\n * in things that mprotect may change.\n *\n * NOTE! The fact that we share an anon_vma doesn't _have_ to mean that\n * we can merge the two vma's. For example, we refuse to merge a vma if\n * there is a vm_ops->close() function, because that indicates that the\n * driver is doing some kind of reference counting. But that doesn't\n * really matter for the anon_vma sharing case.\n */\nstatic int anon_vma_compatible(struct vm_area_struct *a, struct vm_area_struct *b)\n{\n\treturn a->vm_end == b->vm_start &&\n\t\tmpol_equal(vma_policy(a), vma_policy(b)) &&\n\t\ta->vm_file == b->vm_file &&\n\t\t!((a->vm_flags ^ b->vm_flags) & ~(VM_READ|VM_WRITE|VM_EXEC|VM_SOFTDIRTY)) &&\n\t\tb->vm_pgoff == a->vm_pgoff + ((b->vm_start - a->vm_start) >> PAGE_SHIFT);\n}\n\n/*\n * Do some basic sanity checking to see if we can re-use the anon_vma\n * from 'old'. The 'a'/'b' vma's are in VM order - one of them will be\n * the same as 'old', the other will be the new one that is trying\n * to share the anon_vma.\n *\n * NOTE! This runs with mm_sem held for reading, so it is possible that\n * the anon_vma of 'old' is concurrently in the process of being set up\n * by another page fault trying to merge _that_. But that's ok: if it\n * is being set up, that automatically means that it will be a singleton\n * acceptable for merging, so we can do all of this optimistically. But\n * we do that READ_ONCE() to make sure that we never re-load the pointer.\n *\n * IOW: that the \"list_is_singular()\" test on the anon_vma_chain only\n * matters for the 'stable anon_vma' case (ie the thing we want to avoid\n * is to return an anon_vma that is \"complex\" due to having gone through\n * a fork).\n *\n * We also make sure that the two vma's are compatible (adjacent,\n * and with the same memory policies). That's all stable, even with just\n * a read lock on the mm_sem.\n */\nstatic struct anon_vma *reusable_anon_vma(struct vm_area_struct *old, struct vm_area_struct *a, struct vm_area_struct *b)\n{\n\tif (anon_vma_compatible(a, b)) {\n\t\tstruct anon_vma *anon_vma = READ_ONCE(old->anon_vma);\n\n\t\tif (anon_vma && list_is_singular(&old->anon_vma_chain))\n\t\t\treturn anon_vma;\n\t}\n\treturn NULL;\n}\n\n/*\n * find_mergeable_anon_vma is used by anon_vma_prepare, to check\n * neighbouring vmas for a suitable anon_vma, before it goes off\n * to allocate a new anon_vma.  It checks because a repetitive\n * sequence of mprotects and faults may otherwise lead to distinct\n * anon_vmas being allocated, preventing vma merge in subsequent\n * mprotect.\n */\nstruct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *vma)\n{\n\tstruct anon_vma *anon_vma;\n\tstruct vm_area_struct *near;\n\n\tnear = vma->vm_next;\n\tif (!near)\n\t\tgoto try_prev;\n\n\tanon_vma = reusable_anon_vma(near, vma, near);\n\tif (anon_vma)\n\t\treturn anon_vma;\ntry_prev:\n\tnear = vma->vm_prev;\n\tif (!near)\n\t\tgoto none;\n\n\tanon_vma = reusable_anon_vma(near, near, vma);\n\tif (anon_vma)\n\t\treturn anon_vma;\nnone:\n\t/*\n\t * There's no absolute need to look only at touching neighbours:\n\t * we could search further afield for \"compatible\" anon_vmas.\n\t * But it would probably just be a waste of time searching,\n\t * or lead to too many vmas hanging off the same anon_vma.\n\t * We're trying to allow mprotect remerging later on,\n\t * not trying to minimize memory used for anon_vmas.\n\t */\n\treturn NULL;\n}\n\n/*\n * If a hint addr is less than mmap_min_addr change hint to be as\n * low as possible but still greater than mmap_min_addr\n */\nstatic inline unsigned long round_hint_to_min(unsigned long hint)\n{\n\thint &= PAGE_MASK;\n\tif (((void *)hint != NULL) &&\n\t    (hint < mmap_min_addr))\n\t\treturn PAGE_ALIGN(mmap_min_addr);\n\treturn hint;\n}\n\nstatic inline int mlock_future_check(struct mm_struct *mm,\n\t\t\t\t     unsigned long flags,\n\t\t\t\t     unsigned long len)\n{\n\tunsigned long locked, lock_limit;\n\n\t/*  mlock MCL_FUTURE? */\n\tif (flags & VM_LOCKED) {\n\t\tlocked = len >> PAGE_SHIFT;\n\t\tlocked += mm->locked_vm;\n\t\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\t\tlock_limit >>= PAGE_SHIFT;\n\t\tif (locked > lock_limit && !capable(CAP_IPC_LOCK))\n\t\t\treturn -EAGAIN;\n\t}\n\treturn 0;\n}\n\nstatic inline u64 file_mmap_size_max(struct file *file, struct inode *inode)\n{\n\tif (S_ISREG(inode->i_mode))\n\t\treturn MAX_LFS_FILESIZE;\n\n\tif (S_ISBLK(inode->i_mode))\n\t\treturn MAX_LFS_FILESIZE;\n\n\t/* Special \"we do even unsigned file positions\" case */\n\tif (file->f_mode & FMODE_UNSIGNED_OFFSET)\n\t\treturn 0;\n\n\t/* Yes, random drivers might want more. But I'm tired of buggy drivers */\n\treturn ULONG_MAX;\n}\n\nstatic inline bool file_mmap_ok(struct file *file, struct inode *inode,\n\t\t\t\tunsigned long pgoff, unsigned long len)\n{\n\tu64 maxsize = file_mmap_size_max(file, inode);\n\n\tif (maxsize && len > maxsize)\n\t\treturn false;\n\tmaxsize -= len;\n\tif (pgoff > maxsize >> PAGE_SHIFT)\n\t\treturn false;\n\treturn true;\n}\n\n/*\n * The caller must hold down_write(&current->mm->mmap_sem).\n */\nunsigned long do_mmap(struct file *file, unsigned long addr,\n\t\t\tunsigned long len, unsigned long prot,\n\t\t\tunsigned long flags, vm_flags_t vm_flags,\n\t\t\tunsigned long pgoff, unsigned long *populate,\n\t\t\tstruct list_head *uf)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint pkey = 0;\n\n\t*populate = 0;\n\n\tif (!len)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Does the application expect PROT_READ to imply PROT_EXEC?\n\t *\n\t * (the exception is when the underlying filesystem is noexec\n\t *  mounted, in which case we dont add PROT_EXEC.)\n\t */\n\tif ((prot & PROT_READ) && (current->personality & READ_IMPLIES_EXEC))\n\t\tif (!(file && path_noexec(&file->f_path)))\n\t\t\tprot |= PROT_EXEC;\n\n\t/* force arch specific MAP_FIXED handling in get_unmapped_area */\n\tif (flags & MAP_FIXED_NOREPLACE)\n\t\tflags |= MAP_FIXED;\n\n\tif (!(flags & MAP_FIXED))\n\t\taddr = round_hint_to_min(addr);\n\n\t/* Careful about overflows.. */\n\tlen = PAGE_ALIGN(len);\n\tif (!len)\n\t\treturn -ENOMEM;\n\n\t/* offset overflow? */\n\tif ((pgoff + (len >> PAGE_SHIFT)) < pgoff)\n\t\treturn -EOVERFLOW;\n\n\t/* Too many mappings? */\n\tif (mm->map_count > sysctl_max_map_count)\n\t\treturn -ENOMEM;\n\n\t/* Obtain the address to map to. we verify (or select) it and ensure\n\t * that it represents a valid section of the address space.\n\t */\n\taddr = get_unmapped_area(file, addr, len, pgoff, flags);\n\tif (offset_in_page(addr))\n\t\treturn addr;\n\n\tif (flags & MAP_FIXED_NOREPLACE) {\n\t\tstruct vm_area_struct *vma = find_vma(mm, addr);\n\n\t\tif (vma && vma->vm_start < addr + len)\n\t\t\treturn -EEXIST;\n\t}\n\n\tif (prot == PROT_EXEC) {\n\t\tpkey = execute_only_pkey(mm);\n\t\tif (pkey < 0)\n\t\t\tpkey = 0;\n\t}\n\n\t/* Do simple checking here so the lower-level routines won't have\n\t * to. we assume access permissions have been handled by the open\n\t * of the memory object, so we don't do any here.\n\t */\n\tvm_flags |= calc_vm_prot_bits(prot, pkey) | calc_vm_flag_bits(flags) |\n\t\t\tmm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;\n\n\tif (flags & MAP_LOCKED)\n\t\tif (!can_do_mlock())\n\t\t\treturn -EPERM;\n\n\tif (mlock_future_check(mm, vm_flags, len))\n\t\treturn -EAGAIN;\n\n\tif (file) {\n\t\tstruct inode *inode = file_inode(file);\n\t\tunsigned long flags_mask;\n\n\t\tif (!file_mmap_ok(file, inode, pgoff, len))\n\t\t\treturn -EOVERFLOW;\n\n\t\tflags_mask = LEGACY_MAP_MASK | file->f_op->mmap_supported_flags;\n\n\t\tswitch (flags & MAP_TYPE) {\n\t\tcase MAP_SHARED:\n\t\t\t/*\n\t\t\t * Force use of MAP_SHARED_VALIDATE with non-legacy\n\t\t\t * flags. E.g. MAP_SYNC is dangerous to use with\n\t\t\t * MAP_SHARED as you don't know which consistency model\n\t\t\t * you will get. We silently ignore unsupported flags\n\t\t\t * with MAP_SHARED to preserve backward compatibility.\n\t\t\t */\n\t\t\tflags &= LEGACY_MAP_MASK;\n\t\t\t/* fall through */\n\t\tcase MAP_SHARED_VALIDATE:\n\t\t\tif (flags & ~flags_mask)\n\t\t\t\treturn -EOPNOTSUPP;\n\t\t\tif ((prot&PROT_WRITE) && !(file->f_mode&FMODE_WRITE))\n\t\t\t\treturn -EACCES;\n\n\t\t\t/*\n\t\t\t * Make sure we don't allow writing to an append-only\n\t\t\t * file..\n\t\t\t */\n\t\t\tif (IS_APPEND(inode) && (file->f_mode & FMODE_WRITE))\n\t\t\t\treturn -EACCES;\n\n\t\t\t/*\n\t\t\t * Make sure there are no mandatory locks on the file.\n\t\t\t */\n\t\t\tif (locks_verify_locked(file))\n\t\t\t\treturn -EAGAIN;\n\n\t\t\tvm_flags |= VM_SHARED | VM_MAYSHARE;\n\t\t\tif (!(file->f_mode & FMODE_WRITE))\n\t\t\t\tvm_flags &= ~(VM_MAYWRITE | VM_SHARED);\n\n\t\t\t/* fall through */\n\t\tcase MAP_PRIVATE:\n\t\t\tif (!(file->f_mode & FMODE_READ))\n\t\t\t\treturn -EACCES;\n\t\t\tif (path_noexec(&file->f_path)) {\n\t\t\t\tif (vm_flags & VM_EXEC)\n\t\t\t\t\treturn -EPERM;\n\t\t\t\tvm_flags &= ~VM_MAYEXEC;\n\t\t\t}\n\n\t\t\tif (!file->f_op->mmap)\n\t\t\t\treturn -ENODEV;\n\t\t\tif (vm_flags & (VM_GROWSDOWN|VM_GROWSUP))\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tswitch (flags & MAP_TYPE) {\n\t\tcase MAP_SHARED:\n\t\t\tif (vm_flags & (VM_GROWSDOWN|VM_GROWSUP))\n\t\t\t\treturn -EINVAL;\n\t\t\t/*\n\t\t\t * Ignore pgoff.\n\t\t\t */\n\t\t\tpgoff = 0;\n\t\t\tvm_flags |= VM_SHARED | VM_MAYSHARE;\n\t\t\tbreak;\n\t\tcase MAP_PRIVATE:\n\t\t\t/*\n\t\t\t * Set pgoff according to addr for anon_vma.\n\t\t\t */\n\t\t\tpgoff = addr >> PAGE_SHIFT;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/*\n\t * Set 'VM_NORESERVE' if we should not account for the\n\t * memory use of this mapping.\n\t */\n\tif (flags & MAP_NORESERVE) {\n\t\t/* We honor MAP_NORESERVE if allowed to overcommit */\n\t\tif (sysctl_overcommit_memory != OVERCOMMIT_NEVER)\n\t\t\tvm_flags |= VM_NORESERVE;\n\n\t\t/* hugetlb applies strict overcommit unless MAP_NORESERVE */\n\t\tif (file && is_file_hugepages(file))\n\t\t\tvm_flags |= VM_NORESERVE;\n\t}\n\n\taddr = mmap_region(file, addr, len, vm_flags, pgoff, uf);\n\tif (!IS_ERR_VALUE(addr) &&\n\t    ((vm_flags & VM_LOCKED) ||\n\t     (flags & (MAP_POPULATE | MAP_NONBLOCK)) == MAP_POPULATE))\n\t\t*populate = len;\n\treturn addr;\n}\n\nunsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,\n\t\t\t      unsigned long prot, unsigned long flags,\n\t\t\t      unsigned long fd, unsigned long pgoff)\n{\n\tstruct file *file = NULL;\n\tunsigned long retval;\n\n\tif (!(flags & MAP_ANONYMOUS)) {\n\t\taudit_mmap_fd(fd, flags);\n\t\tfile = fget(fd);\n\t\tif (!file)\n\t\t\treturn -EBADF;\n\t\tif (is_file_hugepages(file))\n\t\t\tlen = ALIGN(len, huge_page_size(hstate_file(file)));\n\t\tretval = -EINVAL;\n\t\tif (unlikely(flags & MAP_HUGETLB && !is_file_hugepages(file)))\n\t\t\tgoto out_fput;\n\t} else if (flags & MAP_HUGETLB) {\n\t\tstruct user_struct *user = NULL;\n\t\tstruct hstate *hs;\n\n\t\ths = hstate_sizelog((flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);\n\t\tif (!hs)\n\t\t\treturn -EINVAL;\n\n\t\tlen = ALIGN(len, huge_page_size(hs));\n\t\t/*\n\t\t * VM_NORESERVE is used because the reservations will be\n\t\t * taken when vm_ops->mmap() is called\n\t\t * A dummy user value is used because we are not locking\n\t\t * memory so no accounting is necessary\n\t\t */\n\t\tfile = hugetlb_file_setup(HUGETLB_ANON_FILE, len,\n\t\t\t\tVM_NORESERVE,\n\t\t\t\t&user, HUGETLB_ANONHUGE_INODE,\n\t\t\t\t(flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);\n\t\tif (IS_ERR(file))\n\t\t\treturn PTR_ERR(file);\n\t}\n\n\tflags &= ~(MAP_EXECUTABLE | MAP_DENYWRITE);\n\n\tretval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);\nout_fput:\n\tif (file)\n\t\tfput(file);\n\treturn retval;\n}\n\nSYSCALL_DEFINE6(mmap_pgoff, unsigned long, addr, unsigned long, len,\n\t\tunsigned long, prot, unsigned long, flags,\n\t\tunsigned long, fd, unsigned long, pgoff)\n{\n\treturn ksys_mmap_pgoff(addr, len, prot, flags, fd, pgoff);\n}\n\n#ifdef __ARCH_WANT_SYS_OLD_MMAP\nstruct mmap_arg_struct {\n\tunsigned long addr;\n\tunsigned long len;\n\tunsigned long prot;\n\tunsigned long flags;\n\tunsigned long fd;\n\tunsigned long offset;\n};\n\nSYSCALL_DEFINE1(old_mmap, struct mmap_arg_struct __user *, arg)\n{\n\tstruct mmap_arg_struct a;\n\n\tif (copy_from_user(&a, arg, sizeof(a)))\n\t\treturn -EFAULT;\n\tif (offset_in_page(a.offset))\n\t\treturn -EINVAL;\n\n\treturn ksys_mmap_pgoff(a.addr, a.len, a.prot, a.flags, a.fd,\n\t\t\t       a.offset >> PAGE_SHIFT);\n}\n#endif /* __ARCH_WANT_SYS_OLD_MMAP */\n\n/*\n * Some shared mappings will want the pages marked read-only\n * to track write events. If so, we'll downgrade vm_page_prot\n * to the private version (using protection_map[] without the\n * VM_SHARED bit).\n */\nint vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot)\n{\n\tvm_flags_t vm_flags = vma->vm_flags;\n\tconst struct vm_operations_struct *vm_ops = vma->vm_ops;\n\n\t/* If it was private or non-writable, the write bit is already clear */\n\tif ((vm_flags & (VM_WRITE|VM_SHARED)) != ((VM_WRITE|VM_SHARED)))\n\t\treturn 0;\n\n\t/* The backer wishes to know when pages are first written to? */\n\tif (vm_ops && (vm_ops->page_mkwrite || vm_ops->pfn_mkwrite))\n\t\treturn 1;\n\n\t/* The open routine did something to the protections that pgprot_modify\n\t * won't preserve? */\n\tif (pgprot_val(vm_page_prot) !=\n\t    pgprot_val(vm_pgprot_modify(vm_page_prot, vm_flags)))\n\t\treturn 0;\n\n\t/* Do we need to track softdirty? */\n\tif (IS_ENABLED(CONFIG_MEM_SOFT_DIRTY) && !(vm_flags & VM_SOFTDIRTY))\n\t\treturn 1;\n\n\t/* Specialty mapping? */\n\tif (vm_flags & VM_PFNMAP)\n\t\treturn 0;\n\n\t/* Can the mapping track the dirty pages? */\n\treturn vma->vm_file && vma->vm_file->f_mapping &&\n\t\tmapping_cap_account_dirty(vma->vm_file->f_mapping);\n}\n\n/*\n * We account for memory if it's a private writeable mapping,\n * not hugepages and VM_NORESERVE wasn't set.\n */\nstatic inline int accountable_mapping(struct file *file, vm_flags_t vm_flags)\n{\n\t/*\n\t * hugetlb has its own accounting separate from the core VM\n\t * VM_HUGETLB may not be set yet so we cannot check for that flag.\n\t */\n\tif (file && is_file_hugepages(file))\n\t\treturn 0;\n\n\treturn (vm_flags & (VM_NORESERVE | VM_SHARED | VM_WRITE)) == VM_WRITE;\n}\n\nunsigned long mmap_region(struct file *file, unsigned long addr,\n\t\tunsigned long len, vm_flags_t vm_flags, unsigned long pgoff,\n\t\tstruct list_head *uf)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma, *prev;\n\tint error;\n\tstruct rb_node **rb_link, *rb_parent;\n\tunsigned long charged = 0;\n\n\t/* Check against address space limit. */\n\tif (!may_expand_vm(mm, vm_flags, len >> PAGE_SHIFT)) {\n\t\tunsigned long nr_pages;\n\n\t\t/*\n\t\t * MAP_FIXED may remove pages of mappings that intersects with\n\t\t * requested mapping. Account for the pages it would unmap.\n\t\t */\n\t\tnr_pages = count_vma_pages_range(mm, addr, addr + len);\n\n\t\tif (!may_expand_vm(mm, vm_flags,\n\t\t\t\t\t(len >> PAGE_SHIFT) - nr_pages))\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* Clear old maps */\n\twhile (find_vma_links(mm, addr, addr + len, &prev, &rb_link,\n\t\t\t      &rb_parent)) {\n\t\tif (do_munmap(mm, addr, len, uf))\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/*\n\t * Private writable mapping: check memory availability\n\t */\n\tif (accountable_mapping(file, vm_flags)) {\n\t\tcharged = len >> PAGE_SHIFT;\n\t\tif (security_vm_enough_memory_mm(mm, charged))\n\t\t\treturn -ENOMEM;\n\t\tvm_flags |= VM_ACCOUNT;\n\t}\n\n\t/*\n\t * Can we just expand an old mapping?\n\t */\n\tvma = vma_merge(mm, prev, addr, addr + len, vm_flags,\n\t\t\tNULL, file, pgoff, NULL, NULL_VM_UFFD_CTX);\n\tif (vma)\n\t\tgoto out;\n\n\t/*\n\t * Determine the object being mapped and call the appropriate\n\t * specific mapper. the address has already been validated, but\n\t * not unmapped, but the maps are removed from the list.\n\t */\n\tvma = vm_area_alloc(mm);\n\tif (!vma) {\n\t\terror = -ENOMEM;\n\t\tgoto unacct_error;\n\t}\n\n\tvma->vm_start = addr;\n\tvma->vm_end = addr + len;\n\tvma->vm_flags = vm_flags;\n\tvma->vm_page_prot = vm_get_page_prot(vm_flags);\n\tvma->vm_pgoff = pgoff;\n\n\tif (file) {\n\t\tif (vm_flags & VM_DENYWRITE) {\n\t\t\terror = deny_write_access(file);\n\t\t\tif (error)\n\t\t\t\tgoto free_vma;\n\t\t}\n\t\tif (vm_flags & VM_SHARED) {\n\t\t\terror = mapping_map_writable(file->f_mapping);\n\t\t\tif (error)\n\t\t\t\tgoto allow_write_and_free_vma;\n\t\t}\n\n\t\t/* ->mmap() can change vma->vm_file, but must guarantee that\n\t\t * vma_link() below can deny write-access if VM_DENYWRITE is set\n\t\t * and map writably if VM_SHARED is set. This usually means the\n\t\t * new file must not have been exposed to user-space, yet.\n\t\t */\n\t\tvma->vm_file = get_file(file);\n\t\terror = call_mmap(file, vma);\n\t\tif (error)\n\t\t\tgoto unmap_and_free_vma;\n\n\t\t/* Can addr have changed??\n\t\t *\n\t\t * Answer: Yes, several device drivers can do it in their\n\t\t *         f_op->mmap method. -DaveM\n\t\t * Bug: If addr is changed, prev, rb_link, rb_parent should\n\t\t *      be updated for vma_link()\n\t\t */\n\t\tWARN_ON_ONCE(addr != vma->vm_start);\n\n\t\taddr = vma->vm_start;\n\t\tvm_flags = vma->vm_flags;\n\t} else if (vm_flags & VM_SHARED) {\n\t\terror = shmem_zero_setup(vma);\n\t\tif (error)\n\t\t\tgoto free_vma;\n\t} else {\n\t\tvma_set_anonymous(vma);\n\t}\n\n\tvma_link(mm, vma, prev, rb_link, rb_parent);\n\t/* Once vma denies write, undo our temporary denial count */\n\tif (file) {\n\t\tif (vm_flags & VM_SHARED)\n\t\t\tmapping_unmap_writable(file->f_mapping);\n\t\tif (vm_flags & VM_DENYWRITE)\n\t\t\tallow_write_access(file);\n\t}\n\tfile = vma->vm_file;\nout:\n\tperf_event_mmap(vma);\n\n\tvm_stat_account(mm, vm_flags, len >> PAGE_SHIFT);\n\tif (vm_flags & VM_LOCKED) {\n\t\tif ((vm_flags & VM_SPECIAL) || vma_is_dax(vma) ||\n\t\t\t\t\tis_vm_hugetlb_page(vma) ||\n\t\t\t\t\tvma == get_gate_vma(current->mm))\n\t\t\tvma->vm_flags &= VM_LOCKED_CLEAR_MASK;\n\t\telse\n\t\t\tmm->locked_vm += (len >> PAGE_SHIFT);\n\t}\n\n\tif (file)\n\t\tuprobe_mmap(vma);\n\n\t/*\n\t * New (or expanded) vma always get soft dirty status.\n\t * Otherwise user-space soft-dirty page tracker won't\n\t * be able to distinguish situation when vma area unmapped,\n\t * then new mapped in-place (which must be aimed as\n\t * a completely new data area).\n\t */\n\tvma->vm_flags |= VM_SOFTDIRTY;\n\n\tvma_set_page_prot(vma);\n\n\treturn addr;\n\nunmap_and_free_vma:\n\tvma->vm_file = NULL;\n\tfput(file);\n\n\t/* Undo any partial mapping done by a device driver. */\n\tunmap_region(mm, vma, prev, vma->vm_start, vma->vm_end);\n\tcharged = 0;\n\tif (vm_flags & VM_SHARED)\n\t\tmapping_unmap_writable(file->f_mapping);\nallow_write_and_free_vma:\n\tif (vm_flags & VM_DENYWRITE)\n\t\tallow_write_access(file);\nfree_vma:\n\tvm_area_free(vma);\nunacct_error:\n\tif (charged)\n\t\tvm_unacct_memory(charged);\n\treturn error;\n}\n\nunsigned long unmapped_area(struct vm_unmapped_area_info *info)\n{\n\t/*\n\t * We implement the search by looking for an rbtree node that\n\t * immediately follows a suitable gap. That is,\n\t * - gap_start = vma->vm_prev->vm_end <= info->high_limit - length;\n\t * - gap_end   = vma->vm_start        >= info->low_limit  + length;\n\t * - gap_end - gap_start >= length\n\t */\n\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long length, low_limit, high_limit, gap_start, gap_end;\n\n\t/* Adjust search length to account for worst case alignment overhead */\n\tlength = info->length + info->align_mask;\n\tif (length < info->length)\n\t\treturn -ENOMEM;\n\n\t/* Adjust search limits by the desired length */\n\tif (info->high_limit < length)\n\t\treturn -ENOMEM;\n\thigh_limit = info->high_limit - length;\n\n\tif (info->low_limit > high_limit)\n\t\treturn -ENOMEM;\n\tlow_limit = info->low_limit + length;\n\n\t/* Check if rbtree root looks promising */\n\tif (RB_EMPTY_ROOT(&mm->mm_rb))\n\t\tgoto check_highest;\n\tvma = rb_entry(mm->mm_rb.rb_node, struct vm_area_struct, vm_rb);\n\tif (vma->rb_subtree_gap < length)\n\t\tgoto check_highest;\n\n\twhile (true) {\n\t\t/* Visit left subtree if it looks promising */\n\t\tgap_end = vm_start_gap(vma);\n\t\tif (gap_end >= low_limit && vma->vm_rb.rb_left) {\n\t\t\tstruct vm_area_struct *left =\n\t\t\t\trb_entry(vma->vm_rb.rb_left,\n\t\t\t\t\t struct vm_area_struct, vm_rb);\n\t\t\tif (left->rb_subtree_gap >= length) {\n\t\t\t\tvma = left;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tgap_start = vma->vm_prev ? vm_end_gap(vma->vm_prev) : 0;\ncheck_current:\n\t\t/* Check if current node has a suitable gap */\n\t\tif (gap_start > high_limit)\n\t\t\treturn -ENOMEM;\n\t\tif (gap_end >= low_limit &&\n\t\t    gap_end > gap_start && gap_end - gap_start >= length)\n\t\t\tgoto found;\n\n\t\t/* Visit right subtree if it looks promising */\n\t\tif (vma->vm_rb.rb_right) {\n\t\t\tstruct vm_area_struct *right =\n\t\t\t\trb_entry(vma->vm_rb.rb_right,\n\t\t\t\t\t struct vm_area_struct, vm_rb);\n\t\t\tif (right->rb_subtree_gap >= length) {\n\t\t\t\tvma = right;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\t/* Go back up the rbtree to find next candidate node */\n\t\twhile (true) {\n\t\t\tstruct rb_node *prev = &vma->vm_rb;\n\t\t\tif (!rb_parent(prev))\n\t\t\t\tgoto check_highest;\n\t\t\tvma = rb_entry(rb_parent(prev),\n\t\t\t\t       struct vm_area_struct, vm_rb);\n\t\t\tif (prev == vma->vm_rb.rb_left) {\n\t\t\t\tgap_start = vm_end_gap(vma->vm_prev);\n\t\t\t\tgap_end = vm_start_gap(vma);\n\t\t\t\tgoto check_current;\n\t\t\t}\n\t\t}\n\t}\n\ncheck_highest:\n\t/* Check highest gap, which does not precede any rbtree node */\n\tgap_start = mm->highest_vm_end;\n\tgap_end = ULONG_MAX;  /* Only for VM_BUG_ON below */\n\tif (gap_start > high_limit)\n\t\treturn -ENOMEM;\n\nfound:\n\t/* We found a suitable gap. Clip it with the original low_limit. */\n\tif (gap_start < info->low_limit)\n\t\tgap_start = info->low_limit;\n\n\t/* Adjust gap address to the desired alignment */\n\tgap_start += (info->align_offset - gap_start) & info->align_mask;\n\n\tVM_BUG_ON(gap_start + info->length > info->high_limit);\n\tVM_BUG_ON(gap_start + info->length > gap_end);\n\treturn gap_start;\n}\n\nunsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long length, low_limit, high_limit, gap_start, gap_end;\n\n\t/* Adjust search length to account for worst case alignment overhead */\n\tlength = info->length + info->align_mask;\n\tif (length < info->length)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Adjust search limits by the desired length.\n\t * See implementation comment at top of unmapped_area().\n\t */\n\tgap_end = info->high_limit;\n\tif (gap_end < length)\n\t\treturn -ENOMEM;\n\thigh_limit = gap_end - length;\n\n\tif (info->low_limit > high_limit)\n\t\treturn -ENOMEM;\n\tlow_limit = info->low_limit + length;\n\n\t/* Check highest gap, which does not precede any rbtree node */\n\tgap_start = mm->highest_vm_end;\n\tif (gap_start <= high_limit)\n\t\tgoto found_highest;\n\n\t/* Check if rbtree root looks promising */\n\tif (RB_EMPTY_ROOT(&mm->mm_rb))\n\t\treturn -ENOMEM;\n\tvma = rb_entry(mm->mm_rb.rb_node, struct vm_area_struct, vm_rb);\n\tif (vma->rb_subtree_gap < length)\n\t\treturn -ENOMEM;\n\n\twhile (true) {\n\t\t/* Visit right subtree if it looks promising */\n\t\tgap_start = vma->vm_prev ? vm_end_gap(vma->vm_prev) : 0;\n\t\tif (gap_start <= high_limit && vma->vm_rb.rb_right) {\n\t\t\tstruct vm_area_struct *right =\n\t\t\t\trb_entry(vma->vm_rb.rb_right,\n\t\t\t\t\t struct vm_area_struct, vm_rb);\n\t\t\tif (right->rb_subtree_gap >= length) {\n\t\t\t\tvma = right;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\ncheck_current:\n\t\t/* Check if current node has a suitable gap */\n\t\tgap_end = vm_start_gap(vma);\n\t\tif (gap_end < low_limit)\n\t\t\treturn -ENOMEM;\n\t\tif (gap_start <= high_limit &&\n\t\t    gap_end > gap_start && gap_end - gap_start >= length)\n\t\t\tgoto found;\n\n\t\t/* Visit left subtree if it looks promising */\n\t\tif (vma->vm_rb.rb_left) {\n\t\t\tstruct vm_area_struct *left =\n\t\t\t\trb_entry(vma->vm_rb.rb_left,\n\t\t\t\t\t struct vm_area_struct, vm_rb);\n\t\t\tif (left->rb_subtree_gap >= length) {\n\t\t\t\tvma = left;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\t/* Go back up the rbtree to find next candidate node */\n\t\twhile (true) {\n\t\t\tstruct rb_node *prev = &vma->vm_rb;\n\t\t\tif (!rb_parent(prev))\n\t\t\t\treturn -ENOMEM;\n\t\t\tvma = rb_entry(rb_parent(prev),\n\t\t\t\t       struct vm_area_struct, vm_rb);\n\t\t\tif (prev == vma->vm_rb.rb_right) {\n\t\t\t\tgap_start = vma->vm_prev ?\n\t\t\t\t\tvm_end_gap(vma->vm_prev) : 0;\n\t\t\t\tgoto check_current;\n\t\t\t}\n\t\t}\n\t}\n\nfound:\n\t/* We found a suitable gap. Clip it with the original high_limit. */\n\tif (gap_end > info->high_limit)\n\t\tgap_end = info->high_limit;\n\nfound_highest:\n\t/* Compute highest gap address at the desired alignment */\n\tgap_end -= info->length;\n\tgap_end -= (gap_end - info->align_offset) & info->align_mask;\n\n\tVM_BUG_ON(gap_end < info->low_limit);\n\tVM_BUG_ON(gap_end < gap_start);\n\treturn gap_end;\n}\n\n\n#ifndef arch_get_mmap_end\n#define arch_get_mmap_end(addr)\t(TASK_SIZE)\n#endif\n\n#ifndef arch_get_mmap_base\n#define arch_get_mmap_base(addr, base) (base)\n#endif\n\n/* Get an address range which is currently unmapped.\n * For shmat() with addr=0.\n *\n * Ugly calling convention alert:\n * Return value with the low bits set means error value,\n * ie\n *\tif (ret & ~PAGE_MASK)\n *\t\terror = ret;\n *\n * This function \"knows\" that -ENOMEM has the bits set.\n */\n#ifndef HAVE_ARCH_UNMAPPED_AREA\nunsigned long\narch_get_unmapped_area(struct file *filp, unsigned long addr,\n\t\tunsigned long len, unsigned long pgoff, unsigned long flags)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma, *prev;\n\tstruct vm_unmapped_area_info info;\n\tconst unsigned long mmap_end = arch_get_mmap_end(addr);\n\n\tif (len > mmap_end - mmap_min_addr)\n\t\treturn -ENOMEM;\n\n\tif (flags & MAP_FIXED)\n\t\treturn addr;\n\n\tif (addr) {\n\t\taddr = PAGE_ALIGN(addr);\n\t\tvma = find_vma_prev(mm, addr, &prev);\n\t\tif (mmap_end - len >= addr && addr >= mmap_min_addr &&\n\t\t    (!vma || addr + len <= vm_start_gap(vma)) &&\n\t\t    (!prev || addr >= vm_end_gap(prev)))\n\t\t\treturn addr;\n\t}\n\n\tinfo.flags = 0;\n\tinfo.length = len;\n\tinfo.low_limit = mm->mmap_base;\n\tinfo.high_limit = mmap_end;\n\tinfo.align_mask = 0;\n\treturn vm_unmapped_area(&info);\n}\n#endif\n\n/*\n * This mmap-allocator allocates new areas top-down from below the\n * stack's low limit (the base):\n */\n#ifndef HAVE_ARCH_UNMAPPED_AREA_TOPDOWN\nunsigned long\narch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,\n\t\t\t  unsigned long len, unsigned long pgoff,\n\t\t\t  unsigned long flags)\n{\n\tstruct vm_area_struct *vma, *prev;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_unmapped_area_info info;\n\tconst unsigned long mmap_end = arch_get_mmap_end(addr);\n\n\t/* requested length too big for entire address space */\n\tif (len > mmap_end - mmap_min_addr)\n\t\treturn -ENOMEM;\n\n\tif (flags & MAP_FIXED)\n\t\treturn addr;\n\n\t/* requesting a specific address */\n\tif (addr) {\n\t\taddr = PAGE_ALIGN(addr);\n\t\tvma = find_vma_prev(mm, addr, &prev);\n\t\tif (mmap_end - len >= addr && addr >= mmap_min_addr &&\n\t\t\t\t(!vma || addr + len <= vm_start_gap(vma)) &&\n\t\t\t\t(!prev || addr >= vm_end_gap(prev)))\n\t\t\treturn addr;\n\t}\n\n\tinfo.flags = VM_UNMAPPED_AREA_TOPDOWN;\n\tinfo.length = len;\n\tinfo.low_limit = max(PAGE_SIZE, mmap_min_addr);\n\tinfo.high_limit = arch_get_mmap_base(addr, mm->mmap_base);\n\tinfo.align_mask = 0;\n\taddr = vm_unmapped_area(&info);\n\n\t/*\n\t * A failed mmap() very likely causes application failure,\n\t * so fall back to the bottom-up function here. This scenario\n\t * can happen with large stack limits and large mmap()\n\t * allocations.\n\t */\n\tif (offset_in_page(addr)) {\n\t\tVM_BUG_ON(addr != -ENOMEM);\n\t\tinfo.flags = 0;\n\t\tinfo.low_limit = TASK_UNMAPPED_BASE;\n\t\tinfo.high_limit = mmap_end;\n\t\taddr = vm_unmapped_area(&info);\n\t}\n\n\treturn addr;\n}\n#endif\n\nunsigned long\nget_unmapped_area(struct file *file, unsigned long addr, unsigned long len,\n\t\tunsigned long pgoff, unsigned long flags)\n{\n\tunsigned long (*get_area)(struct file *, unsigned long,\n\t\t\t\t  unsigned long, unsigned long, unsigned long);\n\n\tunsigned long error = arch_mmap_check(addr, len, flags);\n\tif (error)\n\t\treturn error;\n\n\t/* Careful about overflows.. */\n\tif (len > TASK_SIZE)\n\t\treturn -ENOMEM;\n\n\tget_area = current->mm->get_unmapped_area;\n\tif (file) {\n\t\tif (file->f_op->get_unmapped_area)\n\t\t\tget_area = file->f_op->get_unmapped_area;\n\t} else if (flags & MAP_SHARED) {\n\t\t/*\n\t\t * mmap_region() will call shmem_zero_setup() to create a file,\n\t\t * so use shmem's get_unmapped_area in case it can be huge.\n\t\t * do_mmap_pgoff() will clear pgoff, so match alignment.\n\t\t */\n\t\tpgoff = 0;\n\t\tget_area = shmem_get_unmapped_area;\n\t}\n\n\taddr = get_area(file, addr, len, pgoff, flags);\n\tif (IS_ERR_VALUE(addr))\n\t\treturn addr;\n\n\tif (addr > TASK_SIZE - len)\n\t\treturn -ENOMEM;\n\tif (offset_in_page(addr))\n\t\treturn -EINVAL;\n\n\terror = security_mmap_addr(addr);\n\treturn error ? error : addr;\n}\n\nEXPORT_SYMBOL(get_unmapped_area);\n\n/* Look up the first VMA which satisfies  addr < vm_end,  NULL if none. */\nstruct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct rb_node *rb_node;\n\tstruct vm_area_struct *vma;\n\n\t/* Check the cache first. */\n\tvma = vmacache_find(mm, addr);\n\tif (likely(vma))\n\t\treturn vma;\n\n\trb_node = mm->mm_rb.rb_node;\n\n\twhile (rb_node) {\n\t\tstruct vm_area_struct *tmp;\n\n\t\ttmp = rb_entry(rb_node, struct vm_area_struct, vm_rb);\n\n\t\tif (tmp->vm_end > addr) {\n\t\t\tvma = tmp;\n\t\t\tif (tmp->vm_start <= addr)\n\t\t\t\tbreak;\n\t\t\trb_node = rb_node->rb_left;\n\t\t} else\n\t\t\trb_node = rb_node->rb_right;\n\t}\n\n\tif (vma)\n\t\tvmacache_update(addr, vma);\n\treturn vma;\n}\n\nEXPORT_SYMBOL(find_vma);\n\n/*\n * Same as find_vma, but also return a pointer to the previous VMA in *pprev.\n */\nstruct vm_area_struct *\nfind_vma_prev(struct mm_struct *mm, unsigned long addr,\n\t\t\tstruct vm_area_struct **pprev)\n{\n\tstruct vm_area_struct *vma;\n\n\tvma = find_vma(mm, addr);\n\tif (vma) {\n\t\t*pprev = vma->vm_prev;\n\t} else {\n\t\tstruct rb_node *rb_node = mm->mm_rb.rb_node;\n\t\t*pprev = NULL;\n\t\twhile (rb_node) {\n\t\t\t*pprev = rb_entry(rb_node, struct vm_area_struct, vm_rb);\n\t\t\trb_node = rb_node->rb_right;\n\t\t}\n\t}\n\treturn vma;\n}\n\n/*\n * Verify that the stack growth is acceptable and\n * update accounting. This is shared with both the\n * grow-up and grow-down cases.\n */\nstatic int acct_stack_growth(struct vm_area_struct *vma,\n\t\t\t     unsigned long size, unsigned long grow)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long new_start;\n\n\t/* address space limit tests */\n\tif (!may_expand_vm(mm, vma->vm_flags, grow))\n\t\treturn -ENOMEM;\n\n\t/* Stack limit test */\n\tif (size > rlimit(RLIMIT_STACK))\n\t\treturn -ENOMEM;\n\n\t/* mlock limit tests */\n\tif (vma->vm_flags & VM_LOCKED) {\n\t\tunsigned long locked;\n\t\tunsigned long limit;\n\t\tlocked = mm->locked_vm + grow;\n\t\tlimit = rlimit(RLIMIT_MEMLOCK);\n\t\tlimit >>= PAGE_SHIFT;\n\t\tif (locked > limit && !capable(CAP_IPC_LOCK))\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* Check to ensure the stack will not grow into a hugetlb-only region */\n\tnew_start = (vma->vm_flags & VM_GROWSUP) ? vma->vm_start :\n\t\t\tvma->vm_end - size;\n\tif (is_hugepage_only_range(vma->vm_mm, new_start, size))\n\t\treturn -EFAULT;\n\n\t/*\n\t * Overcommit..  This must be the final test, as it will\n\t * update security statistics.\n\t */\n\tif (security_vm_enough_memory_mm(mm, grow))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\n#if defined(CONFIG_STACK_GROWSUP) || defined(CONFIG_IA64)\n/*\n * PA-RISC uses this for its stack; IA64 for its Register Backing Store.\n * vma is the last one with address > vma->vm_end.  Have to extend vma.\n */\nint expand_upwards(struct vm_area_struct *vma, unsigned long address)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct vm_area_struct *next;\n\tunsigned long gap_addr;\n\tint error = 0;\n\n\tif (!(vma->vm_flags & VM_GROWSUP))\n\t\treturn -EFAULT;\n\n\t/* Guard against exceeding limits of the address space. */\n\taddress &= PAGE_MASK;\n\tif (address >= (TASK_SIZE & PAGE_MASK))\n\t\treturn -ENOMEM;\n\taddress += PAGE_SIZE;\n\n\t/* Enforce stack_guard_gap */\n\tgap_addr = address + stack_guard_gap;\n\n\t/* Guard against overflow */\n\tif (gap_addr < address || gap_addr > TASK_SIZE)\n\t\tgap_addr = TASK_SIZE;\n\n\tnext = vma->vm_next;\n\tif (next && next->vm_start < gap_addr &&\n\t\t\t(next->vm_flags & (VM_WRITE|VM_READ|VM_EXEC))) {\n\t\tif (!(next->vm_flags & VM_GROWSUP))\n\t\t\treturn -ENOMEM;\n\t\t/* Check that both stack segments have the same anon_vma? */\n\t}\n\n\t/* We must make sure the anon_vma is allocated. */\n\tif (unlikely(anon_vma_prepare(vma)))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * vma->vm_start/vm_end cannot change under us because the caller\n\t * is required to hold the mmap_sem in read mode.  We need the\n\t * anon_vma lock to serialize against concurrent expand_stacks.\n\t */\n\tanon_vma_lock_write(vma->anon_vma);\n\n\t/* Somebody else might have raced and expanded it already */\n\tif (address > vma->vm_end) {\n\t\tunsigned long size, grow;\n\n\t\tsize = address - vma->vm_start;\n\t\tgrow = (address - vma->vm_end) >> PAGE_SHIFT;\n\n\t\terror = -ENOMEM;\n\t\tif (vma->vm_pgoff + (size >> PAGE_SHIFT) >= vma->vm_pgoff) {\n\t\t\terror = acct_stack_growth(vma, size, grow);\n\t\t\tif (!error) {\n\t\t\t\t/*\n\t\t\t\t * vma_gap_update() doesn't support concurrent\n\t\t\t\t * updates, but we only hold a shared mmap_sem\n\t\t\t\t * lock here, so we need to protect against\n\t\t\t\t * concurrent vma expansions.\n\t\t\t\t * anon_vma_lock_write() doesn't help here, as\n\t\t\t\t * we don't guarantee that all growable vmas\n\t\t\t\t * in a mm share the same root anon vma.\n\t\t\t\t * So, we reuse mm->page_table_lock to guard\n\t\t\t\t * against concurrent vma expansions.\n\t\t\t\t */\n\t\t\t\tspin_lock(&mm->page_table_lock);\n\t\t\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\t\t\tmm->locked_vm += grow;\n\t\t\t\tvm_stat_account(mm, vma->vm_flags, grow);\n\t\t\t\tanon_vma_interval_tree_pre_update_vma(vma);\n\t\t\t\tvma->vm_end = address;\n\t\t\t\tanon_vma_interval_tree_post_update_vma(vma);\n\t\t\t\tif (vma->vm_next)\n\t\t\t\t\tvma_gap_update(vma->vm_next);\n\t\t\t\telse\n\t\t\t\t\tmm->highest_vm_end = vm_end_gap(vma);\n\t\t\t\tspin_unlock(&mm->page_table_lock);\n\n\t\t\t\tperf_event_mmap(vma);\n\t\t\t}\n\t\t}\n\t}\n\tanon_vma_unlock_write(vma->anon_vma);\n\tkhugepaged_enter_vma_merge(vma, vma->vm_flags);\n\tvalidate_mm(mm);\n\treturn error;\n}\n#endif /* CONFIG_STACK_GROWSUP || CONFIG_IA64 */\n\n/*\n * vma is the first one with address < vma->vm_start.  Have to extend vma.\n */\nint expand_downwards(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long address)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct vm_area_struct *prev;\n\tint error = 0;\n\n\taddress &= PAGE_MASK;\n\tif (address < mmap_min_addr)\n\t\treturn -EPERM;\n\n\t/* Enforce stack_guard_gap */\n\tprev = vma->vm_prev;\n\t/* Check that both stack segments have the same anon_vma? */\n\tif (prev && !(prev->vm_flags & VM_GROWSDOWN) &&\n\t\t\t(prev->vm_flags & (VM_WRITE|VM_READ|VM_EXEC))) {\n\t\tif (address - prev->vm_end < stack_guard_gap)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* We must make sure the anon_vma is allocated. */\n\tif (unlikely(anon_vma_prepare(vma)))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * vma->vm_start/vm_end cannot change under us because the caller\n\t * is required to hold the mmap_sem in read mode.  We need the\n\t * anon_vma lock to serialize against concurrent expand_stacks.\n\t */\n\tanon_vma_lock_write(vma->anon_vma);\n\n\t/* Somebody else might have raced and expanded it already */\n\tif (address < vma->vm_start) {\n\t\tunsigned long size, grow;\n\n\t\tsize = vma->vm_end - address;\n\t\tgrow = (vma->vm_start - address) >> PAGE_SHIFT;\n\n\t\terror = -ENOMEM;\n\t\tif (grow <= vma->vm_pgoff) {\n\t\t\terror = acct_stack_growth(vma, size, grow);\n\t\t\tif (!error) {\n\t\t\t\t/*\n\t\t\t\t * vma_gap_update() doesn't support concurrent\n\t\t\t\t * updates, but we only hold a shared mmap_sem\n\t\t\t\t * lock here, so we need to protect against\n\t\t\t\t * concurrent vma expansions.\n\t\t\t\t * anon_vma_lock_write() doesn't help here, as\n\t\t\t\t * we don't guarantee that all growable vmas\n\t\t\t\t * in a mm share the same root anon vma.\n\t\t\t\t * So, we reuse mm->page_table_lock to guard\n\t\t\t\t * against concurrent vma expansions.\n\t\t\t\t */\n\t\t\t\tspin_lock(&mm->page_table_lock);\n\t\t\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\t\t\tmm->locked_vm += grow;\n\t\t\t\tvm_stat_account(mm, vma->vm_flags, grow);\n\t\t\t\tanon_vma_interval_tree_pre_update_vma(vma);\n\t\t\t\tvma->vm_start = address;\n\t\t\t\tvma->vm_pgoff -= grow;\n\t\t\t\tanon_vma_interval_tree_post_update_vma(vma);\n\t\t\t\tvma_gap_update(vma);\n\t\t\t\tspin_unlock(&mm->page_table_lock);\n\n\t\t\t\tperf_event_mmap(vma);\n\t\t\t}\n\t\t}\n\t}\n\tanon_vma_unlock_write(vma->anon_vma);\n\tkhugepaged_enter_vma_merge(vma, vma->vm_flags);\n\tvalidate_mm(mm);\n\treturn error;\n}\n\n/* enforced gap between the expanding stack and other mappings. */\nunsigned long stack_guard_gap = 256UL<<PAGE_SHIFT;\n\nstatic int __init cmdline_parse_stack_guard_gap(char *p)\n{\n\tunsigned long val;\n\tchar *endptr;\n\n\tval = simple_strtoul(p, &endptr, 10);\n\tif (!*endptr)\n\t\tstack_guard_gap = val << PAGE_SHIFT;\n\n\treturn 0;\n}\n__setup(\"stack_guard_gap=\", cmdline_parse_stack_guard_gap);\n\n#ifdef CONFIG_STACK_GROWSUP\nint expand_stack(struct vm_area_struct *vma, unsigned long address)\n{\n\treturn expand_upwards(vma, address);\n}\n\nstruct vm_area_struct *\nfind_extend_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma, *prev;\n\n\taddr &= PAGE_MASK;\n\tvma = find_vma_prev(mm, addr, &prev);\n\tif (vma && (vma->vm_start <= addr))\n\t\treturn vma;\n\tif (!prev || expand_stack(prev, addr))\n\t\treturn NULL;\n\tif (prev->vm_flags & VM_LOCKED)\n\t\tpopulate_vma_page_range(prev, addr, prev->vm_end, NULL);\n\treturn prev;\n}\n#else\nint expand_stack(struct vm_area_struct *vma, unsigned long address)\n{\n\treturn expand_downwards(vma, address);\n}\n\nstruct vm_area_struct *\nfind_extend_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long start;\n\n\taddr &= PAGE_MASK;\n\tvma = find_vma(mm, addr);\n\tif (!vma)\n\t\treturn NULL;\n\tif (vma->vm_start <= addr)\n\t\treturn vma;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\treturn NULL;\n\tstart = vma->vm_start;\n\tif (expand_stack(vma, addr))\n\t\treturn NULL;\n\tif (vma->vm_flags & VM_LOCKED)\n\t\tpopulate_vma_page_range(vma, addr, start, NULL);\n\treturn vma;\n}\n#endif\n\nEXPORT_SYMBOL_GPL(find_extend_vma);\n\n/*\n * Ok - we have the memory areas we should free on the vma list,\n * so release them, and do the vma updates.\n *\n * Called with the mm semaphore held.\n */\nstatic void remove_vma_list(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tunsigned long nr_accounted = 0;\n\n\t/* Update high watermark before we lower total_vm */\n\tupdate_hiwater_vm(mm);\n\tdo {\n\t\tlong nrpages = vma_pages(vma);\n\n\t\tif (vma->vm_flags & VM_ACCOUNT)\n\t\t\tnr_accounted += nrpages;\n\t\tvm_stat_account(mm, vma->vm_flags, -nrpages);\n\t\tvma = remove_vma(vma);\n\t} while (vma);\n\tvm_unacct_memory(nr_accounted);\n\tvalidate_mm(mm);\n}\n\n/*\n * Get rid of page table information in the indicated region.\n *\n * Called with the mm semaphore held.\n */\nstatic void unmap_region(struct mm_struct *mm,\n\t\tstruct vm_area_struct *vma, struct vm_area_struct *prev,\n\t\tunsigned long start, unsigned long end)\n{\n\tstruct vm_area_struct *next = prev ? prev->vm_next : mm->mmap;\n\tstruct mmu_gather tlb;\n\n\tlru_add_drain();\n\ttlb_gather_mmu(&tlb, mm, start, end);\n\tupdate_hiwater_rss(mm);\n\tunmap_vmas(&tlb, vma, start, end);\n\tfree_pgtables(&tlb, vma, prev ? prev->vm_end : FIRST_USER_ADDRESS,\n\t\t\t\t next ? next->vm_start : USER_PGTABLES_CEILING);\n\ttlb_finish_mmu(&tlb, start, end);\n}\n\n/*\n * Create a list of vma's touched by the unmap, removing them from the mm's\n * vma list as we go..\n */\nstatic void\ndetach_vmas_to_be_unmapped(struct mm_struct *mm, struct vm_area_struct *vma,\n\tstruct vm_area_struct *prev, unsigned long end)\n{\n\tstruct vm_area_struct **insertion_point;\n\tstruct vm_area_struct *tail_vma = NULL;\n\n\tinsertion_point = (prev ? &prev->vm_next : &mm->mmap);\n\tvma->vm_prev = NULL;\n\tdo {\n\t\tvma_rb_erase(vma, &mm->mm_rb);\n\t\tmm->map_count--;\n\t\ttail_vma = vma;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\n\t*insertion_point = vma;\n\tif (vma) {\n\t\tvma->vm_prev = prev;\n\t\tvma_gap_update(vma);\n\t} else\n\t\tmm->highest_vm_end = prev ? vm_end_gap(prev) : 0;\n\ttail_vma->vm_next = NULL;\n\n\t/* Kill the cache */\n\tvmacache_invalidate(mm);\n}\n\n/*\n * __split_vma() bypasses sysctl_max_map_count checking.  We use this where it\n * has already been checked or doesn't make sense to fail.\n */\nint __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\tunsigned long addr, int new_below)\n{\n\tstruct vm_area_struct *new;\n\tint err;\n\n\tif (vma->vm_ops && vma->vm_ops->split) {\n\t\terr = vma->vm_ops->split(vma, addr);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tnew = vm_area_dup(vma);\n\tif (!new)\n\t\treturn -ENOMEM;\n\n\tif (new_below)\n\t\tnew->vm_end = addr;\n\telse {\n\t\tnew->vm_start = addr;\n\t\tnew->vm_pgoff += ((addr - vma->vm_start) >> PAGE_SHIFT);\n\t}\n\n\terr = vma_dup_policy(vma, new);\n\tif (err)\n\t\tgoto out_free_vma;\n\n\terr = anon_vma_clone(new, vma);\n\tif (err)\n\t\tgoto out_free_mpol;\n\n\tif (new->vm_file)\n\t\tget_file(new->vm_file);\n\n\tif (new->vm_ops && new->vm_ops->open)\n\t\tnew->vm_ops->open(new);\n\n\tif (new_below)\n\t\terr = vma_adjust(vma, addr, vma->vm_end, vma->vm_pgoff +\n\t\t\t((addr - new->vm_start) >> PAGE_SHIFT), new);\n\telse\n\t\terr = vma_adjust(vma, vma->vm_start, addr, vma->vm_pgoff, new);\n\n\t/* Success. */\n\tif (!err)\n\t\treturn 0;\n\n\t/* Clean everything up if vma_adjust failed. */\n\tif (new->vm_ops && new->vm_ops->close)\n\t\tnew->vm_ops->close(new);\n\tif (new->vm_file)\n\t\tfput(new->vm_file);\n\tunlink_anon_vmas(new);\n out_free_mpol:\n\tmpol_put(vma_policy(new));\n out_free_vma:\n\tvm_area_free(new);\n\treturn err;\n}\n\n/*\n * Split a vma into two pieces at address 'addr', a new vma is allocated\n * either for the first part or the tail.\n */\nint split_vma(struct mm_struct *mm, struct vm_area_struct *vma,\n\t      unsigned long addr, int new_below)\n{\n\tif (mm->map_count >= sysctl_max_map_count)\n\t\treturn -ENOMEM;\n\n\treturn __split_vma(mm, vma, addr, new_below);\n}\n\n/* Munmap is split into 2 main parts -- this part which finds\n * what needs doing, and the areas themselves, which do the\n * work.  This now handles partial unmappings.\n * Jeremy Fitzhardinge <jeremy@goop.org>\n */\nint __do_munmap(struct mm_struct *mm, unsigned long start, size_t len,\n\t\tstruct list_head *uf, bool downgrade)\n{\n\tunsigned long end;\n\tstruct vm_area_struct *vma, *prev, *last;\n\n\tif ((offset_in_page(start)) || start > TASK_SIZE || len > TASK_SIZE-start)\n\t\treturn -EINVAL;\n\n\tlen = PAGE_ALIGN(len);\n\tif (len == 0)\n\t\treturn -EINVAL;\n\n\t/* Find the first overlapping VMA */\n\tvma = find_vma(mm, start);\n\tif (!vma)\n\t\treturn 0;\n\tprev = vma->vm_prev;\n\t/* we have  start < vma->vm_end  */\n\n\t/* if it doesn't overlap, we have nothing.. */\n\tend = start + len;\n\tif (vma->vm_start >= end)\n\t\treturn 0;\n\n\t/*\n\t * If we need to split any vma, do it now to save pain later.\n\t *\n\t * Note: mremap's move_vma VM_ACCOUNT handling assumes a partially\n\t * unmapped vm_area_struct will remain in use: so lower split_vma\n\t * places tmp vma above, and higher split_vma places tmp vma below.\n\t */\n\tif (start > vma->vm_start) {\n\t\tint error;\n\n\t\t/*\n\t\t * Make sure that map_count on return from munmap() will\n\t\t * not exceed its limit; but let map_count go just above\n\t\t * its limit temporarily, to help free resources as expected.\n\t\t */\n\t\tif (end < vma->vm_end && mm->map_count >= sysctl_max_map_count)\n\t\t\treturn -ENOMEM;\n\n\t\terror = __split_vma(mm, vma, start, 0);\n\t\tif (error)\n\t\t\treturn error;\n\t\tprev = vma;\n\t}\n\n\t/* Does it split the last one? */\n\tlast = find_vma(mm, end);\n\tif (last && end > last->vm_start) {\n\t\tint error = __split_vma(mm, last, end, 1);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\tvma = prev ? prev->vm_next : mm->mmap;\n\n\tif (unlikely(uf)) {\n\t\t/*\n\t\t * If userfaultfd_unmap_prep returns an error the vmas\n\t\t * will remain splitted, but userland will get a\n\t\t * highly unexpected error anyway. This is no\n\t\t * different than the case where the first of the two\n\t\t * __split_vma fails, but we don't undo the first\n\t\t * split, despite we could. This is unlikely enough\n\t\t * failure that it's not worth optimizing it for.\n\t\t */\n\t\tint error = userfaultfd_unmap_prep(vma, start, end, uf);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\t/*\n\t * unlock any mlock()ed ranges before detaching vmas\n\t */\n\tif (mm->locked_vm) {\n\t\tstruct vm_area_struct *tmp = vma;\n\t\twhile (tmp && tmp->vm_start < end) {\n\t\t\tif (tmp->vm_flags & VM_LOCKED) {\n\t\t\t\tmm->locked_vm -= vma_pages(tmp);\n\t\t\t\tmunlock_vma_pages_all(tmp);\n\t\t\t}\n\n\t\t\ttmp = tmp->vm_next;\n\t\t}\n\t}\n\n\t/* Detach vmas from rbtree */\n\tdetach_vmas_to_be_unmapped(mm, vma, prev, end);\n\n\t/*\n\t * mpx unmap needs to be called with mmap_sem held for write.\n\t * It is safe to call it before unmap_region().\n\t */\n\tarch_unmap(mm, vma, start, end);\n\n\tif (downgrade)\n\t\tdowngrade_write(&mm->mmap_sem);\n\n\tunmap_region(mm, vma, prev, start, end);\n\n\t/* Fix up all other VM information */\n\tremove_vma_list(mm, vma);\n\n\treturn downgrade ? 1 : 0;\n}\n\nint do_munmap(struct mm_struct *mm, unsigned long start, size_t len,\n\t      struct list_head *uf)\n{\n\treturn __do_munmap(mm, start, len, uf, false);\n}\n\nstatic int __vm_munmap(unsigned long start, size_t len, bool downgrade)\n{\n\tint ret;\n\tstruct mm_struct *mm = current->mm;\n\tLIST_HEAD(uf);\n\n\tif (down_write_killable(&mm->mmap_sem))\n\t\treturn -EINTR;\n\n\tret = __do_munmap(mm, start, len, &uf, downgrade);\n\t/*\n\t * Returning 1 indicates mmap_sem is downgraded.\n\t * But 1 is not legal return value of vm_munmap() and munmap(), reset\n\t * it to 0 before return.\n\t */\n\tif (ret == 1) {\n\t\tup_read(&mm->mmap_sem);\n\t\tret = 0;\n\t} else\n\t\tup_write(&mm->mmap_sem);\n\n\tuserfaultfd_unmap_complete(mm, &uf);\n\treturn ret;\n}\n\nint vm_munmap(unsigned long start, size_t len)\n{\n\treturn __vm_munmap(start, len, false);\n}\nEXPORT_SYMBOL(vm_munmap);\n\nSYSCALL_DEFINE2(munmap, unsigned long, addr, size_t, len)\n{\n\tprofile_munmap(addr);\n\treturn __vm_munmap(addr, len, true);\n}\n\n\n/*\n * Emulation of deprecated remap_file_pages() syscall.\n */\nSYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,\n\t\tunsigned long, prot, unsigned long, pgoff, unsigned long, flags)\n{\n\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long populate = 0;\n\tunsigned long ret = -EINVAL;\n\tstruct file *file;\n\n\tpr_warn_once(\"%s (%d) uses deprecated remap_file_pages() syscall. See Documentation/vm/remap_file_pages.rst.\\n\",\n\t\t     current->comm, current->pid);\n\n\tif (prot)\n\t\treturn ret;\n\tstart = start & PAGE_MASK;\n\tsize = size & PAGE_MASK;\n\n\tif (start + size <= start)\n\t\treturn ret;\n\n\t/* Does pgoff wrap? */\n\tif (pgoff + (size >> PAGE_SHIFT) < pgoff)\n\t\treturn ret;\n\n\tif (down_write_killable(&mm->mmap_sem))\n\t\treturn -EINTR;\n\n\tvma = find_vma(mm, start);\n\n\tif (!vma || !(vma->vm_flags & VM_SHARED))\n\t\tgoto out;\n\n\tif (start < vma->vm_start)\n\t\tgoto out;\n\n\tif (start + size > vma->vm_end) {\n\t\tstruct vm_area_struct *next;\n\n\t\tfor (next = vma->vm_next; next; next = next->vm_next) {\n\t\t\t/* hole between vmas ? */\n\t\t\tif (next->vm_start != next->vm_prev->vm_end)\n\t\t\t\tgoto out;\n\n\t\t\tif (next->vm_file != vma->vm_file)\n\t\t\t\tgoto out;\n\n\t\t\tif (next->vm_flags != vma->vm_flags)\n\t\t\t\tgoto out;\n\n\t\t\tif (start + size <= next->vm_end)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!next)\n\t\t\tgoto out;\n\t}\n\n\tprot |= vma->vm_flags & VM_READ ? PROT_READ : 0;\n\tprot |= vma->vm_flags & VM_WRITE ? PROT_WRITE : 0;\n\tprot |= vma->vm_flags & VM_EXEC ? PROT_EXEC : 0;\n\n\tflags &= MAP_NONBLOCK;\n\tflags |= MAP_SHARED | MAP_FIXED | MAP_POPULATE;\n\tif (vma->vm_flags & VM_LOCKED) {\n\t\tstruct vm_area_struct *tmp;\n\t\tflags |= MAP_LOCKED;\n\n\t\t/* drop PG_Mlocked flag for over-mapped range */\n\t\tfor (tmp = vma; tmp->vm_start >= start + size;\n\t\t\t\ttmp = tmp->vm_next) {\n\t\t\t/*\n\t\t\t * Split pmd and munlock page on the border\n\t\t\t * of the range.\n\t\t\t */\n\t\t\tvma_adjust_trans_huge(tmp, start, start + size, 0);\n\n\t\t\tmunlock_vma_pages_range(tmp,\n\t\t\t\t\tmax(tmp->vm_start, start),\n\t\t\t\t\tmin(tmp->vm_end, start + size));\n\t\t}\n\t}\n\n\tfile = get_file(vma->vm_file);\n\tret = do_mmap_pgoff(vma->vm_file, start, size,\n\t\t\tprot, flags, pgoff, &populate, NULL);\n\tfput(file);\nout:\n\tup_write(&mm->mmap_sem);\n\tif (populate)\n\t\tmm_populate(ret, populate);\n\tif (!IS_ERR_VALUE(ret))\n\t\tret = 0;\n\treturn ret;\n}\n\n/*\n *  this is really a simplified \"do_mmap\".  it only handles\n *  anonymous maps.  eventually we may be able to do some\n *  brk-specific accounting here.\n */\nstatic int do_brk_flags(unsigned long addr, unsigned long len, unsigned long flags, struct list_head *uf)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma, *prev;\n\tstruct rb_node **rb_link, *rb_parent;\n\tpgoff_t pgoff = addr >> PAGE_SHIFT;\n\tint error;\n\n\t/* Until we need other flags, refuse anything except VM_EXEC. */\n\tif ((flags & (~VM_EXEC)) != 0)\n\t\treturn -EINVAL;\n\tflags |= VM_DATA_DEFAULT_FLAGS | VM_ACCOUNT | mm->def_flags;\n\n\terror = get_unmapped_area(NULL, addr, len, 0, MAP_FIXED);\n\tif (offset_in_page(error))\n\t\treturn error;\n\n\terror = mlock_future_check(mm, mm->def_flags, len);\n\tif (error)\n\t\treturn error;\n\n\t/*\n\t * Clear old maps.  this also does some error checking for us\n\t */\n\twhile (find_vma_links(mm, addr, addr + len, &prev, &rb_link,\n\t\t\t      &rb_parent)) {\n\t\tif (do_munmap(mm, addr, len, uf))\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* Check against address space limits *after* clearing old maps... */\n\tif (!may_expand_vm(mm, flags, len >> PAGE_SHIFT))\n\t\treturn -ENOMEM;\n\n\tif (mm->map_count > sysctl_max_map_count)\n\t\treturn -ENOMEM;\n\n\tif (security_vm_enough_memory_mm(mm, len >> PAGE_SHIFT))\n\t\treturn -ENOMEM;\n\n\t/* Can we just expand an old private anonymous mapping? */\n\tvma = vma_merge(mm, prev, addr, addr + len, flags,\n\t\t\tNULL, NULL, pgoff, NULL, NULL_VM_UFFD_CTX);\n\tif (vma)\n\t\tgoto out;\n\n\t/*\n\t * create a vma struct for an anonymous mapping\n\t */\n\tvma = vm_area_alloc(mm);\n\tif (!vma) {\n\t\tvm_unacct_memory(len >> PAGE_SHIFT);\n\t\treturn -ENOMEM;\n\t}\n\n\tvma_set_anonymous(vma);\n\tvma->vm_start = addr;\n\tvma->vm_end = addr + len;\n\tvma->vm_pgoff = pgoff;\n\tvma->vm_flags = flags;\n\tvma->vm_page_prot = vm_get_page_prot(flags);\n\tvma_link(mm, vma, prev, rb_link, rb_parent);\nout:\n\tperf_event_mmap(vma);\n\tmm->total_vm += len >> PAGE_SHIFT;\n\tmm->data_vm += len >> PAGE_SHIFT;\n\tif (flags & VM_LOCKED)\n\t\tmm->locked_vm += (len >> PAGE_SHIFT);\n\tvma->vm_flags |= VM_SOFTDIRTY;\n\treturn 0;\n}\n\nint vm_brk_flags(unsigned long addr, unsigned long request, unsigned long flags)\n{\n\tstruct mm_struct *mm = current->mm;\n\tunsigned long len;\n\tint ret;\n\tbool populate;\n\tLIST_HEAD(uf);\n\n\tlen = PAGE_ALIGN(request);\n\tif (len < request)\n\t\treturn -ENOMEM;\n\tif (!len)\n\t\treturn 0;\n\n\tif (down_write_killable(&mm->mmap_sem))\n\t\treturn -EINTR;\n\n\tret = do_brk_flags(addr, len, flags, &uf);\n\tpopulate = ((mm->def_flags & VM_LOCKED) != 0);\n\tup_write(&mm->mmap_sem);\n\tuserfaultfd_unmap_complete(mm, &uf);\n\tif (populate && !ret)\n\t\tmm_populate(addr, len);\n\treturn ret;\n}\nEXPORT_SYMBOL(vm_brk_flags);\n\nint vm_brk(unsigned long addr, unsigned long len)\n{\n\treturn vm_brk_flags(addr, len, 0);\n}\nEXPORT_SYMBOL(vm_brk);\n\n/* Release all mmaps. */\nvoid exit_mmap(struct mm_struct *mm)\n{\n\tstruct mmu_gather tlb;\n\tstruct vm_area_struct *vma;\n\tunsigned long nr_accounted = 0;\n\n\t/* mm's last user has gone, and its about to be pulled down */\n\tmmu_notifier_release(mm);\n\n\tif (unlikely(mm_is_oom_victim(mm))) {\n\t\t/*\n\t\t * Manually reap the mm to free as much memory as possible.\n\t\t * Then, as the oom reaper does, set MMF_OOM_SKIP to disregard\n\t\t * this mm from further consideration.  Taking mm->mmap_sem for\n\t\t * write after setting MMF_OOM_SKIP will guarantee that the oom\n\t\t * reaper will not run on this mm again after mmap_sem is\n\t\t * dropped.\n\t\t *\n\t\t * Nothing can be holding mm->mmap_sem here and the above call\n\t\t * to mmu_notifier_release(mm) ensures mmu notifier callbacks in\n\t\t * __oom_reap_task_mm() will not block.\n\t\t *\n\t\t * This needs to be done before calling munlock_vma_pages_all(),\n\t\t * which clears VM_LOCKED, otherwise the oom reaper cannot\n\t\t * reliably test it.\n\t\t */\n\t\t(void)__oom_reap_task_mm(mm);\n\n\t\tset_bit(MMF_OOM_SKIP, &mm->flags);\n\t\tdown_write(&mm->mmap_sem);\n\t\tup_write(&mm->mmap_sem);\n\t}\n\n\tif (mm->locked_vm) {\n\t\tvma = mm->mmap;\n\t\twhile (vma) {\n\t\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\t\tmunlock_vma_pages_all(vma);\n\t\t\tvma = vma->vm_next;\n\t\t}\n\t}\n\n\tarch_exit_mmap(mm);\n\n\tvma = mm->mmap;\n\tif (!vma)\t/* Can happen if dup_mmap() received an OOM */\n\t\treturn;\n\n\tlru_add_drain();\n\tflush_cache_mm(mm);\n\ttlb_gather_mmu(&tlb, mm, 0, -1);\n\t/* update_hiwater_rss(mm) here? but nobody should be looking */\n\t/* Use -1 here to ensure all VMAs in the mm are unmapped */\n\tunmap_vmas(&tlb, vma, 0, -1);\n\tfree_pgtables(&tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);\n\ttlb_finish_mmu(&tlb, 0, -1);\n\n\t/*\n\t * Walk the list again, actually closing and freeing it,\n\t * with preemption enabled, without holding any MM locks.\n\t */\n\twhile (vma) {\n\t\tif (vma->vm_flags & VM_ACCOUNT)\n\t\t\tnr_accounted += vma_pages(vma);\n\t\tvma = remove_vma(vma);\n\t}\n\tvm_unacct_memory(nr_accounted);\n}\n\n/* Insert vm structure into process list sorted by address\n * and into the inode's i_mmap tree.  If vm_file is non-NULL\n * then i_mmap_rwsem is taken here.\n */\nint insert_vm_struct(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tstruct vm_area_struct *prev;\n\tstruct rb_node **rb_link, *rb_parent;\n\n\tif (find_vma_links(mm, vma->vm_start, vma->vm_end,\n\t\t\t   &prev, &rb_link, &rb_parent))\n\t\treturn -ENOMEM;\n\tif ((vma->vm_flags & VM_ACCOUNT) &&\n\t     security_vm_enough_memory_mm(mm, vma_pages(vma)))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * The vm_pgoff of a purely anonymous vma should be irrelevant\n\t * until its first write fault, when page's anon_vma and index\n\t * are set.  But now set the vm_pgoff it will almost certainly\n\t * end up with (unless mremap moves it elsewhere before that\n\t * first wfault), so /proc/pid/maps tells a consistent story.\n\t *\n\t * By setting it to reflect the virtual start address of the\n\t * vma, merges and splits can happen in a seamless way, just\n\t * using the existing file pgoff checks and manipulations.\n\t * Similarly in do_mmap_pgoff and in do_brk.\n\t */\n\tif (vma_is_anonymous(vma)) {\n\t\tBUG_ON(vma->anon_vma);\n\t\tvma->vm_pgoff = vma->vm_start >> PAGE_SHIFT;\n\t}\n\n\tvma_link(mm, vma, prev, rb_link, rb_parent);\n\treturn 0;\n}\n\n/*\n * Copy the vma structure to a new location in the same mm,\n * prior to moving page table entries, to effect an mremap move.\n */\nstruct vm_area_struct *copy_vma(struct vm_area_struct **vmap,\n\tunsigned long addr, unsigned long len, pgoff_t pgoff,\n\tbool *need_rmap_locks)\n{\n\tstruct vm_area_struct *vma = *vmap;\n\tunsigned long vma_start = vma->vm_start;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct vm_area_struct *new_vma, *prev;\n\tstruct rb_node **rb_link, *rb_parent;\n\tbool faulted_in_anon_vma = true;\n\n\t/*\n\t * If anonymous vma has not yet been faulted, update new pgoff\n\t * to match new location, to increase its chance of merging.\n\t */\n\tif (unlikely(vma_is_anonymous(vma) && !vma->anon_vma)) {\n\t\tpgoff = addr >> PAGE_SHIFT;\n\t\tfaulted_in_anon_vma = false;\n\t}\n\n\tif (find_vma_links(mm, addr, addr + len, &prev, &rb_link, &rb_parent))\n\t\treturn NULL;\t/* should never get here */\n\tnew_vma = vma_merge(mm, prev, addr, addr + len, vma->vm_flags,\n\t\t\t    vma->anon_vma, vma->vm_file, pgoff, vma_policy(vma),\n\t\t\t    vma->vm_userfaultfd_ctx);\n\tif (new_vma) {\n\t\t/*\n\t\t * Source vma may have been merged into new_vma\n\t\t */\n\t\tif (unlikely(vma_start >= new_vma->vm_start &&\n\t\t\t     vma_start < new_vma->vm_end)) {\n\t\t\t/*\n\t\t\t * The only way we can get a vma_merge with\n\t\t\t * self during an mremap is if the vma hasn't\n\t\t\t * been faulted in yet and we were allowed to\n\t\t\t * reset the dst vma->vm_pgoff to the\n\t\t\t * destination address of the mremap to allow\n\t\t\t * the merge to happen. mremap must change the\n\t\t\t * vm_pgoff linearity between src and dst vmas\n\t\t\t * (in turn preventing a vma_merge) to be\n\t\t\t * safe. It is only safe to keep the vm_pgoff\n\t\t\t * linear if there are no pages mapped yet.\n\t\t\t */\n\t\t\tVM_BUG_ON_VMA(faulted_in_anon_vma, new_vma);\n\t\t\t*vmap = vma = new_vma;\n\t\t}\n\t\t*need_rmap_locks = (new_vma->vm_pgoff <= vma->vm_pgoff);\n\t} else {\n\t\tnew_vma = vm_area_dup(vma);\n\t\tif (!new_vma)\n\t\t\tgoto out;\n\t\tnew_vma->vm_start = addr;\n\t\tnew_vma->vm_end = addr + len;\n\t\tnew_vma->vm_pgoff = pgoff;\n\t\tif (vma_dup_policy(vma, new_vma))\n\t\t\tgoto out_free_vma;\n\t\tif (anon_vma_clone(new_vma, vma))\n\t\t\tgoto out_free_mempol;\n\t\tif (new_vma->vm_file)\n\t\t\tget_file(new_vma->vm_file);\n\t\tif (new_vma->vm_ops && new_vma->vm_ops->open)\n\t\t\tnew_vma->vm_ops->open(new_vma);\n\t\tvma_link(mm, new_vma, prev, rb_link, rb_parent);\n\t\t*need_rmap_locks = false;\n\t}\n\treturn new_vma;\n\nout_free_mempol:\n\tmpol_put(vma_policy(new_vma));\nout_free_vma:\n\tvm_area_free(new_vma);\nout:\n\treturn NULL;\n}\n\n/*\n * Return true if the calling process may expand its vm space by the passed\n * number of pages\n */\nbool may_expand_vm(struct mm_struct *mm, vm_flags_t flags, unsigned long npages)\n{\n\tif (mm->total_vm + npages > rlimit(RLIMIT_AS) >> PAGE_SHIFT)\n\t\treturn false;\n\n\tif (is_data_mapping(flags) &&\n\t    mm->data_vm + npages > rlimit(RLIMIT_DATA) >> PAGE_SHIFT) {\n\t\t/* Workaround for Valgrind */\n\t\tif (rlimit(RLIMIT_DATA) == 0 &&\n\t\t    mm->data_vm + npages <= rlimit_max(RLIMIT_DATA) >> PAGE_SHIFT)\n\t\t\treturn true;\n\n\t\tpr_warn_once(\"%s (%d): VmData %lu exceed data ulimit %lu. Update limits%s.\\n\",\n\t\t\t     current->comm, current->pid,\n\t\t\t     (mm->data_vm + npages) << PAGE_SHIFT,\n\t\t\t     rlimit(RLIMIT_DATA),\n\t\t\t     ignore_rlimit_data ? \"\" : \" or use boot option ignore_rlimit_data\");\n\n\t\tif (!ignore_rlimit_data)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nvoid vm_stat_account(struct mm_struct *mm, vm_flags_t flags, long npages)\n{\n\tmm->total_vm += npages;\n\n\tif (is_exec_mapping(flags))\n\t\tmm->exec_vm += npages;\n\telse if (is_stack_mapping(flags))\n\t\tmm->stack_vm += npages;\n\telse if (is_data_mapping(flags))\n\t\tmm->data_vm += npages;\n}\n\nstatic vm_fault_t special_mapping_fault(struct vm_fault *vmf);\n\n/*\n * Having a close hook prevents vma merging regardless of flags.\n */\nstatic void special_mapping_close(struct vm_area_struct *vma)\n{\n}\n\nstatic const char *special_mapping_name(struct vm_area_struct *vma)\n{\n\treturn ((struct vm_special_mapping *)vma->vm_private_data)->name;\n}\n\nstatic int special_mapping_mremap(struct vm_area_struct *new_vma)\n{\n\tstruct vm_special_mapping *sm = new_vma->vm_private_data;\n\n\tif (WARN_ON_ONCE(current->mm != new_vma->vm_mm))\n\t\treturn -EFAULT;\n\n\tif (sm->mremap)\n\t\treturn sm->mremap(sm, new_vma);\n\n\treturn 0;\n}\n\nstatic const struct vm_operations_struct special_mapping_vmops = {\n\t.close = special_mapping_close,\n\t.fault = special_mapping_fault,\n\t.mremap = special_mapping_mremap,\n\t.name = special_mapping_name,\n};\n\nstatic const struct vm_operations_struct legacy_special_mapping_vmops = {\n\t.close = special_mapping_close,\n\t.fault = special_mapping_fault,\n};\n\nstatic vm_fault_t special_mapping_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tpgoff_t pgoff;\n\tstruct page **pages;\n\n\tif (vma->vm_ops == &legacy_special_mapping_vmops) {\n\t\tpages = vma->vm_private_data;\n\t} else {\n\t\tstruct vm_special_mapping *sm = vma->vm_private_data;\n\n\t\tif (sm->fault)\n\t\t\treturn sm->fault(sm, vmf->vma, vmf);\n\n\t\tpages = sm->pages;\n\t}\n\n\tfor (pgoff = vmf->pgoff; pgoff && *pages; ++pages)\n\t\tpgoff--;\n\n\tif (*pages) {\n\t\tstruct page *page = *pages;\n\t\tget_page(page);\n\t\tvmf->page = page;\n\t\treturn 0;\n\t}\n\n\treturn VM_FAULT_SIGBUS;\n}\n\nstatic struct vm_area_struct *__install_special_mapping(\n\tstruct mm_struct *mm,\n\tunsigned long addr, unsigned long len,\n\tunsigned long vm_flags, void *priv,\n\tconst struct vm_operations_struct *ops)\n{\n\tint ret;\n\tstruct vm_area_struct *vma;\n\n\tvma = vm_area_alloc(mm);\n\tif (unlikely(vma == NULL))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tvma->vm_start = addr;\n\tvma->vm_end = addr + len;\n\n\tvma->vm_flags = vm_flags | mm->def_flags | VM_DONTEXPAND | VM_SOFTDIRTY;\n\tvma->vm_page_prot = vm_get_page_prot(vma->vm_flags);\n\n\tvma->vm_ops = ops;\n\tvma->vm_private_data = priv;\n\n\tret = insert_vm_struct(mm, vma);\n\tif (ret)\n\t\tgoto out;\n\n\tvm_stat_account(mm, vma->vm_flags, len >> PAGE_SHIFT);\n\n\tperf_event_mmap(vma);\n\n\treturn vma;\n\nout:\n\tvm_area_free(vma);\n\treturn ERR_PTR(ret);\n}\n\nbool vma_is_special_mapping(const struct vm_area_struct *vma,\n\tconst struct vm_special_mapping *sm)\n{\n\treturn vma->vm_private_data == sm &&\n\t\t(vma->vm_ops == &special_mapping_vmops ||\n\t\t vma->vm_ops == &legacy_special_mapping_vmops);\n}\n\n/*\n * Called with mm->mmap_sem held for writing.\n * Insert a new vma covering the given region, with the given flags.\n * Its pages are supplied by the given array of struct page *.\n * The array can be shorter than len >> PAGE_SHIFT if it's null-terminated.\n * The region past the last page supplied will always produce SIGBUS.\n * The array pointer and the pages it points to are assumed to stay alive\n * for as long as this mapping might exist.\n */\nstruct vm_area_struct *_install_special_mapping(\n\tstruct mm_struct *mm,\n\tunsigned long addr, unsigned long len,\n\tunsigned long vm_flags, const struct vm_special_mapping *spec)\n{\n\treturn __install_special_mapping(mm, addr, len, vm_flags, (void *)spec,\n\t\t\t\t\t&special_mapping_vmops);\n}\n\nint install_special_mapping(struct mm_struct *mm,\n\t\t\t    unsigned long addr, unsigned long len,\n\t\t\t    unsigned long vm_flags, struct page **pages)\n{\n\tstruct vm_area_struct *vma = __install_special_mapping(\n\t\tmm, addr, len, vm_flags, (void *)pages,\n\t\t&legacy_special_mapping_vmops);\n\n\treturn PTR_ERR_OR_ZERO(vma);\n}\n\nstatic DEFINE_MUTEX(mm_all_locks_mutex);\n\nstatic void vm_lock_anon_vma(struct mm_struct *mm, struct anon_vma *anon_vma)\n{\n\tif (!test_bit(0, (unsigned long *) &anon_vma->root->rb_root.rb_root.rb_node)) {\n\t\t/*\n\t\t * The LSB of head.next can't change from under us\n\t\t * because we hold the mm_all_locks_mutex.\n\t\t */\n\t\tdown_write_nest_lock(&anon_vma->root->rwsem, &mm->mmap_sem);\n\t\t/*\n\t\t * We can safely modify head.next after taking the\n\t\t * anon_vma->root->rwsem. If some other vma in this mm shares\n\t\t * the same anon_vma we won't take it again.\n\t\t *\n\t\t * No need of atomic instructions here, head.next\n\t\t * can't change from under us thanks to the\n\t\t * anon_vma->root->rwsem.\n\t\t */\n\t\tif (__test_and_set_bit(0, (unsigned long *)\n\t\t\t\t       &anon_vma->root->rb_root.rb_root.rb_node))\n\t\t\tBUG();\n\t}\n}\n\nstatic void vm_lock_mapping(struct mm_struct *mm, struct address_space *mapping)\n{\n\tif (!test_bit(AS_MM_ALL_LOCKS, &mapping->flags)) {\n\t\t/*\n\t\t * AS_MM_ALL_LOCKS can't change from under us because\n\t\t * we hold the mm_all_locks_mutex.\n\t\t *\n\t\t * Operations on ->flags have to be atomic because\n\t\t * even if AS_MM_ALL_LOCKS is stable thanks to the\n\t\t * mm_all_locks_mutex, there may be other cpus\n\t\t * changing other bitflags in parallel to us.\n\t\t */\n\t\tif (test_and_set_bit(AS_MM_ALL_LOCKS, &mapping->flags))\n\t\t\tBUG();\n\t\tdown_write_nest_lock(&mapping->i_mmap_rwsem, &mm->mmap_sem);\n\t}\n}\n\n/*\n * This operation locks against the VM for all pte/vma/mm related\n * operations that could ever happen on a certain mm. This includes\n * vmtruncate, try_to_unmap, and all page faults.\n *\n * The caller must take the mmap_sem in write mode before calling\n * mm_take_all_locks(). The caller isn't allowed to release the\n * mmap_sem until mm_drop_all_locks() returns.\n *\n * mmap_sem in write mode is required in order to block all operations\n * that could modify pagetables and free pages without need of\n * altering the vma layout. It's also needed in write mode to avoid new\n * anon_vmas to be associated with existing vmas.\n *\n * A single task can't take more than one mm_take_all_locks() in a row\n * or it would deadlock.\n *\n * The LSB in anon_vma->rb_root.rb_node and the AS_MM_ALL_LOCKS bitflag in\n * mapping->flags avoid to take the same lock twice, if more than one\n * vma in this mm is backed by the same anon_vma or address_space.\n *\n * We take locks in following order, accordingly to comment at beginning\n * of mm/rmap.c:\n *   - all hugetlbfs_i_mmap_rwsem_key locks (aka mapping->i_mmap_rwsem for\n *     hugetlb mapping);\n *   - all i_mmap_rwsem locks;\n *   - all anon_vma->rwseml\n *\n * We can take all locks within these types randomly because the VM code\n * doesn't nest them and we protected from parallel mm_take_all_locks() by\n * mm_all_locks_mutex.\n *\n * mm_take_all_locks() and mm_drop_all_locks are expensive operations\n * that may have to take thousand of locks.\n *\n * mm_take_all_locks() can fail if it's interrupted by signals.\n */\nint mm_take_all_locks(struct mm_struct *mm)\n{\n\tstruct vm_area_struct *vma;\n\tstruct anon_vma_chain *avc;\n\n\tBUG_ON(down_read_trylock(&mm->mmap_sem));\n\n\tmutex_lock(&mm_all_locks_mutex);\n\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (signal_pending(current))\n\t\t\tgoto out_unlock;\n\t\tif (vma->vm_file && vma->vm_file->f_mapping &&\n\t\t\t\tis_vm_hugetlb_page(vma))\n\t\t\tvm_lock_mapping(mm, vma->vm_file->f_mapping);\n\t}\n\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (signal_pending(current))\n\t\t\tgoto out_unlock;\n\t\tif (vma->vm_file && vma->vm_file->f_mapping &&\n\t\t\t\t!is_vm_hugetlb_page(vma))\n\t\t\tvm_lock_mapping(mm, vma->vm_file->f_mapping);\n\t}\n\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (signal_pending(current))\n\t\t\tgoto out_unlock;\n\t\tif (vma->anon_vma)\n\t\t\tlist_for_each_entry(avc, &vma->anon_vma_chain, same_vma)\n\t\t\t\tvm_lock_anon_vma(mm, avc->anon_vma);\n\t}\n\n\treturn 0;\n\nout_unlock:\n\tmm_drop_all_locks(mm);\n\treturn -EINTR;\n}\n\nstatic void vm_unlock_anon_vma(struct anon_vma *anon_vma)\n{\n\tif (test_bit(0, (unsigned long *) &anon_vma->root->rb_root.rb_root.rb_node)) {\n\t\t/*\n\t\t * The LSB of head.next can't change to 0 from under\n\t\t * us because we hold the mm_all_locks_mutex.\n\t\t *\n\t\t * We must however clear the bitflag before unlocking\n\t\t * the vma so the users using the anon_vma->rb_root will\n\t\t * never see our bitflag.\n\t\t *\n\t\t * No need of atomic instructions here, head.next\n\t\t * can't change from under us until we release the\n\t\t * anon_vma->root->rwsem.\n\t\t */\n\t\tif (!__test_and_clear_bit(0, (unsigned long *)\n\t\t\t\t\t  &anon_vma->root->rb_root.rb_root.rb_node))\n\t\t\tBUG();\n\t\tanon_vma_unlock_write(anon_vma);\n\t}\n}\n\nstatic void vm_unlock_mapping(struct address_space *mapping)\n{\n\tif (test_bit(AS_MM_ALL_LOCKS, &mapping->flags)) {\n\t\t/*\n\t\t * AS_MM_ALL_LOCKS can't change to 0 from under us\n\t\t * because we hold the mm_all_locks_mutex.\n\t\t */\n\t\ti_mmap_unlock_write(mapping);\n\t\tif (!test_and_clear_bit(AS_MM_ALL_LOCKS,\n\t\t\t\t\t&mapping->flags))\n\t\t\tBUG();\n\t}\n}\n\n/*\n * The mmap_sem cannot be released by the caller until\n * mm_drop_all_locks() returns.\n */\nvoid mm_drop_all_locks(struct mm_struct *mm)\n{\n\tstruct vm_area_struct *vma;\n\tstruct anon_vma_chain *avc;\n\n\tBUG_ON(down_read_trylock(&mm->mmap_sem));\n\tBUG_ON(!mutex_is_locked(&mm_all_locks_mutex));\n\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->anon_vma)\n\t\t\tlist_for_each_entry(avc, &vma->anon_vma_chain, same_vma)\n\t\t\t\tvm_unlock_anon_vma(avc->anon_vma);\n\t\tif (vma->vm_file && vma->vm_file->f_mapping)\n\t\t\tvm_unlock_mapping(vma->vm_file->f_mapping);\n\t}\n\n\tmutex_unlock(&mm_all_locks_mutex);\n}\n\n/*\n * initialise the percpu counter for VM\n */\nvoid __init mmap_init(void)\n{\n\tint ret;\n\n\tret = percpu_counter_init(&vm_committed_as, 0, GFP_KERNEL);\n\tVM_BUG_ON(ret);\n}\n\n/*\n * Initialise sysctl_user_reserve_kbytes.\n *\n * This is intended to prevent a user from starting a single memory hogging\n * process, such that they cannot recover (kill the hog) in OVERCOMMIT_NEVER\n * mode.\n *\n * The default value is min(3% of free memory, 128MB)\n * 128MB is enough to recover with sshd/login, bash, and top/kill.\n */\nstatic int init_user_reserve(void)\n{\n\tunsigned long free_kbytes;\n\n\tfree_kbytes = global_zone_page_state(NR_FREE_PAGES) << (PAGE_SHIFT - 10);\n\n\tsysctl_user_reserve_kbytes = min(free_kbytes / 32, 1UL << 17);\n\treturn 0;\n}\nsubsys_initcall(init_user_reserve);\n\n/*\n * Initialise sysctl_admin_reserve_kbytes.\n *\n * The purpose of sysctl_admin_reserve_kbytes is to allow the sys admin\n * to log in and kill a memory hogging process.\n *\n * Systems with more than 256MB will reserve 8MB, enough to recover\n * with sshd, bash, and top in OVERCOMMIT_GUESS. Smaller systems will\n * only reserve 3% of free pages by default.\n */\nstatic int init_admin_reserve(void)\n{\n\tunsigned long free_kbytes;\n\n\tfree_kbytes = global_zone_page_state(NR_FREE_PAGES) << (PAGE_SHIFT - 10);\n\n\tsysctl_admin_reserve_kbytes = min(free_kbytes / 32, 1UL << 13);\n\treturn 0;\n}\nsubsys_initcall(init_admin_reserve);\n\n/*\n * Reinititalise user and admin reserves if memory is added or removed.\n *\n * The default user reserve max is 128MB, and the default max for the\n * admin reserve is 8MB. These are usually, but not always, enough to\n * enable recovery from a memory hogging process using login/sshd, a shell,\n * and tools like top. It may make sense to increase or even disable the\n * reserve depending on the existence of swap or variations in the recovery\n * tools. So, the admin may have changed them.\n *\n * If memory is added and the reserves have been eliminated or increased above\n * the default max, then we'll trust the admin.\n *\n * If memory is removed and there isn't enough free memory, then we\n * need to reset the reserves.\n *\n * Otherwise keep the reserve set by the admin.\n */\nstatic int reserve_mem_notifier(struct notifier_block *nb,\n\t\t\t     unsigned long action, void *data)\n{\n\tunsigned long tmp, free_kbytes;\n\n\tswitch (action) {\n\tcase MEM_ONLINE:\n\t\t/* Default max is 128MB. Leave alone if modified by operator. */\n\t\ttmp = sysctl_user_reserve_kbytes;\n\t\tif (0 < tmp && tmp < (1UL << 17))\n\t\t\tinit_user_reserve();\n\n\t\t/* Default max is 8MB.  Leave alone if modified by operator. */\n\t\ttmp = sysctl_admin_reserve_kbytes;\n\t\tif (0 < tmp && tmp < (1UL << 13))\n\t\t\tinit_admin_reserve();\n\n\t\tbreak;\n\tcase MEM_OFFLINE:\n\t\tfree_kbytes = global_zone_page_state(NR_FREE_PAGES) << (PAGE_SHIFT - 10);\n\n\t\tif (sysctl_user_reserve_kbytes > free_kbytes) {\n\t\t\tinit_user_reserve();\n\t\t\tpr_info(\"vm.user_reserve_kbytes reset to %lu\\n\",\n\t\t\t\tsysctl_user_reserve_kbytes);\n\t\t}\n\n\t\tif (sysctl_admin_reserve_kbytes > free_kbytes) {\n\t\t\tinit_admin_reserve();\n\t\t\tpr_info(\"vm.admin_reserve_kbytes reset to %lu\\n\",\n\t\t\t\tsysctl_admin_reserve_kbytes);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block reserve_mem_nb = {\n\t.notifier_call = reserve_mem_notifier,\n};\n\nstatic int __meminit init_reserve_notifier(void)\n{\n\tif (register_hotmemory_notifier(&reserve_mem_nb))\n\t\tpr_err(\"Failed registering memory add/remove notifier for admin reserve\\n\");\n\n\treturn 0;\n}\nsubsys_initcall(init_reserve_notifier);\n"], "fixing_code": ["/*\n * Copyright (c) 2005 Topspin Communications.  All rights reserved.\n * Copyright (c) 2005, 2006 Cisco Systems.  All rights reserved.\n * Copyright (c) 2005 Mellanox Technologies. All rights reserved.\n * Copyright (c) 2005 Voltaire, Inc. All rights reserved.\n * Copyright (c) 2005 PathScale, Inc. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n */\n\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/device.h>\n#include <linux/err.h>\n#include <linux/fs.h>\n#include <linux/poll.h>\n#include <linux/sched.h>\n#include <linux/file.h>\n#include <linux/cdev.h>\n#include <linux/anon_inodes.h>\n#include <linux/slab.h>\n#include <linux/sched/mm.h>\n\n#include <linux/uaccess.h>\n\n#include <rdma/ib.h>\n#include <rdma/uverbs_std_types.h>\n\n#include \"uverbs.h\"\n#include \"core_priv.h\"\n#include \"rdma_core.h\"\n\nMODULE_AUTHOR(\"Roland Dreier\");\nMODULE_DESCRIPTION(\"InfiniBand userspace verbs access\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\n\nenum {\n\tIB_UVERBS_MAJOR       = 231,\n\tIB_UVERBS_BASE_MINOR  = 192,\n\tIB_UVERBS_MAX_DEVICES = RDMA_MAX_PORTS,\n\tIB_UVERBS_NUM_FIXED_MINOR = 32,\n\tIB_UVERBS_NUM_DYNAMIC_MINOR = IB_UVERBS_MAX_DEVICES - IB_UVERBS_NUM_FIXED_MINOR,\n};\n\n#define IB_UVERBS_BASE_DEV\tMKDEV(IB_UVERBS_MAJOR, IB_UVERBS_BASE_MINOR)\n\nstatic dev_t dynamic_uverbs_dev;\nstatic struct class *uverbs_class;\n\nstatic DEFINE_IDA(uverbs_ida);\nstatic void ib_uverbs_add_one(struct ib_device *device);\nstatic void ib_uverbs_remove_one(struct ib_device *device, void *client_data);\n\n/*\n * Must be called with the ufile->device->disassociate_srcu held, and the lock\n * must be held until use of the ucontext is finished.\n */\nstruct ib_ucontext *ib_uverbs_get_ucontext_file(struct ib_uverbs_file *ufile)\n{\n\t/*\n\t * We do not hold the hw_destroy_rwsem lock for this flow, instead\n\t * srcu is used. It does not matter if someone races this with\n\t * get_context, we get NULL or valid ucontext.\n\t */\n\tstruct ib_ucontext *ucontext = smp_load_acquire(&ufile->ucontext);\n\n\tif (!srcu_dereference(ufile->device->ib_dev,\n\t\t\t      &ufile->device->disassociate_srcu))\n\t\treturn ERR_PTR(-EIO);\n\n\tif (!ucontext)\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn ucontext;\n}\nEXPORT_SYMBOL(ib_uverbs_get_ucontext_file);\n\nint uverbs_dealloc_mw(struct ib_mw *mw)\n{\n\tstruct ib_pd *pd = mw->pd;\n\tint ret;\n\n\tret = mw->device->ops.dealloc_mw(mw);\n\tif (!ret)\n\t\tatomic_dec(&pd->usecnt);\n\treturn ret;\n}\n\nstatic void ib_uverbs_release_dev(struct device *device)\n{\n\tstruct ib_uverbs_device *dev =\n\t\t\tcontainer_of(device, struct ib_uverbs_device, dev);\n\n\tuverbs_destroy_api(dev->uapi);\n\tcleanup_srcu_struct(&dev->disassociate_srcu);\n\tkfree(dev);\n}\n\nstatic void ib_uverbs_release_async_event_file(struct kref *ref)\n{\n\tstruct ib_uverbs_async_event_file *file =\n\t\tcontainer_of(ref, struct ib_uverbs_async_event_file, ref);\n\n\tkfree(file);\n}\n\nvoid ib_uverbs_release_ucq(struct ib_uverbs_file *file,\n\t\t\t  struct ib_uverbs_completion_event_file *ev_file,\n\t\t\t  struct ib_ucq_object *uobj)\n{\n\tstruct ib_uverbs_event *evt, *tmp;\n\n\tif (ev_file) {\n\t\tspin_lock_irq(&ev_file->ev_queue.lock);\n\t\tlist_for_each_entry_safe(evt, tmp, &uobj->comp_list, obj_list) {\n\t\t\tlist_del(&evt->list);\n\t\t\tkfree(evt);\n\t\t}\n\t\tspin_unlock_irq(&ev_file->ev_queue.lock);\n\n\t\tuverbs_uobject_put(&ev_file->uobj);\n\t}\n\n\tspin_lock_irq(&file->async_file->ev_queue.lock);\n\tlist_for_each_entry_safe(evt, tmp, &uobj->async_list, obj_list) {\n\t\tlist_del(&evt->list);\n\t\tkfree(evt);\n\t}\n\tspin_unlock_irq(&file->async_file->ev_queue.lock);\n}\n\nvoid ib_uverbs_release_uevent(struct ib_uverbs_file *file,\n\t\t\t      struct ib_uevent_object *uobj)\n{\n\tstruct ib_uverbs_event *evt, *tmp;\n\n\tspin_lock_irq(&file->async_file->ev_queue.lock);\n\tlist_for_each_entry_safe(evt, tmp, &uobj->event_list, obj_list) {\n\t\tlist_del(&evt->list);\n\t\tkfree(evt);\n\t}\n\tspin_unlock_irq(&file->async_file->ev_queue.lock);\n}\n\nvoid ib_uverbs_detach_umcast(struct ib_qp *qp,\n\t\t\t     struct ib_uqp_object *uobj)\n{\n\tstruct ib_uverbs_mcast_entry *mcast, *tmp;\n\n\tlist_for_each_entry_safe(mcast, tmp, &uobj->mcast_list, list) {\n\t\tib_detach_mcast(qp, &mcast->gid, mcast->lid);\n\t\tlist_del(&mcast->list);\n\t\tkfree(mcast);\n\t}\n}\n\nstatic void ib_uverbs_comp_dev(struct ib_uverbs_device *dev)\n{\n\tcomplete(&dev->comp);\n}\n\nvoid ib_uverbs_release_file(struct kref *ref)\n{\n\tstruct ib_uverbs_file *file =\n\t\tcontainer_of(ref, struct ib_uverbs_file, ref);\n\tstruct ib_device *ib_dev;\n\tint srcu_key;\n\n\trelease_ufile_idr_uobject(file);\n\n\tsrcu_key = srcu_read_lock(&file->device->disassociate_srcu);\n\tib_dev = srcu_dereference(file->device->ib_dev,\n\t\t\t\t  &file->device->disassociate_srcu);\n\tif (ib_dev && !ib_dev->ops.disassociate_ucontext)\n\t\tmodule_put(ib_dev->owner);\n\tsrcu_read_unlock(&file->device->disassociate_srcu, srcu_key);\n\n\tif (atomic_dec_and_test(&file->device->refcount))\n\t\tib_uverbs_comp_dev(file->device);\n\n\tif (file->async_file)\n\t\tkref_put(&file->async_file->ref,\n\t\t\t ib_uverbs_release_async_event_file);\n\tput_device(&file->device->dev);\n\tkfree(file);\n}\n\nstatic ssize_t ib_uverbs_event_read(struct ib_uverbs_event_queue *ev_queue,\n\t\t\t\t    struct ib_uverbs_file *uverbs_file,\n\t\t\t\t    struct file *filp, char __user *buf,\n\t\t\t\t    size_t count, loff_t *pos,\n\t\t\t\t    size_t eventsz)\n{\n\tstruct ib_uverbs_event *event;\n\tint ret = 0;\n\n\tspin_lock_irq(&ev_queue->lock);\n\n\twhile (list_empty(&ev_queue->event_list)) {\n\t\tspin_unlock_irq(&ev_queue->lock);\n\n\t\tif (filp->f_flags & O_NONBLOCK)\n\t\t\treturn -EAGAIN;\n\n\t\tif (wait_event_interruptible(ev_queue->poll_wait,\n\t\t\t\t\t     (!list_empty(&ev_queue->event_list) ||\n\t\t\t/* The barriers built into wait_event_interruptible()\n\t\t\t * and wake_up() guarentee this will see the null set\n\t\t\t * without using RCU\n\t\t\t */\n\t\t\t\t\t     !uverbs_file->device->ib_dev)))\n\t\t\treturn -ERESTARTSYS;\n\n\t\t/* If device was disassociated and no event exists set an error */\n\t\tif (list_empty(&ev_queue->event_list) &&\n\t\t    !uverbs_file->device->ib_dev)\n\t\t\treturn -EIO;\n\n\t\tspin_lock_irq(&ev_queue->lock);\n\t}\n\n\tevent = list_entry(ev_queue->event_list.next, struct ib_uverbs_event, list);\n\n\tif (eventsz > count) {\n\t\tret   = -EINVAL;\n\t\tevent = NULL;\n\t} else {\n\t\tlist_del(ev_queue->event_list.next);\n\t\tif (event->counter) {\n\t\t\t++(*event->counter);\n\t\t\tlist_del(&event->obj_list);\n\t\t}\n\t}\n\n\tspin_unlock_irq(&ev_queue->lock);\n\n\tif (event) {\n\t\tif (copy_to_user(buf, event, eventsz))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = eventsz;\n\t}\n\n\tkfree(event);\n\n\treturn ret;\n}\n\nstatic ssize_t ib_uverbs_async_event_read(struct file *filp, char __user *buf,\n\t\t\t\t\t  size_t count, loff_t *pos)\n{\n\tstruct ib_uverbs_async_event_file *file = filp->private_data;\n\n\treturn ib_uverbs_event_read(&file->ev_queue, file->uverbs_file, filp,\n\t\t\t\t    buf, count, pos,\n\t\t\t\t    sizeof(struct ib_uverbs_async_event_desc));\n}\n\nstatic ssize_t ib_uverbs_comp_event_read(struct file *filp, char __user *buf,\n\t\t\t\t\t size_t count, loff_t *pos)\n{\n\tstruct ib_uverbs_completion_event_file *comp_ev_file =\n\t\tfilp->private_data;\n\n\treturn ib_uverbs_event_read(&comp_ev_file->ev_queue,\n\t\t\t\t    comp_ev_file->uobj.ufile, filp,\n\t\t\t\t    buf, count, pos,\n\t\t\t\t    sizeof(struct ib_uverbs_comp_event_desc));\n}\n\nstatic __poll_t ib_uverbs_event_poll(struct ib_uverbs_event_queue *ev_queue,\n\t\t\t\t\t struct file *filp,\n\t\t\t\t\t struct poll_table_struct *wait)\n{\n\t__poll_t pollflags = 0;\n\n\tpoll_wait(filp, &ev_queue->poll_wait, wait);\n\n\tspin_lock_irq(&ev_queue->lock);\n\tif (!list_empty(&ev_queue->event_list))\n\t\tpollflags = EPOLLIN | EPOLLRDNORM;\n\tspin_unlock_irq(&ev_queue->lock);\n\n\treturn pollflags;\n}\n\nstatic __poll_t ib_uverbs_async_event_poll(struct file *filp,\n\t\t\t\t\t       struct poll_table_struct *wait)\n{\n\treturn ib_uverbs_event_poll(filp->private_data, filp, wait);\n}\n\nstatic __poll_t ib_uverbs_comp_event_poll(struct file *filp,\n\t\t\t\t\t      struct poll_table_struct *wait)\n{\n\tstruct ib_uverbs_completion_event_file *comp_ev_file =\n\t\tfilp->private_data;\n\n\treturn ib_uverbs_event_poll(&comp_ev_file->ev_queue, filp, wait);\n}\n\nstatic int ib_uverbs_async_event_fasync(int fd, struct file *filp, int on)\n{\n\tstruct ib_uverbs_event_queue *ev_queue = filp->private_data;\n\n\treturn fasync_helper(fd, filp, on, &ev_queue->async_queue);\n}\n\nstatic int ib_uverbs_comp_event_fasync(int fd, struct file *filp, int on)\n{\n\tstruct ib_uverbs_completion_event_file *comp_ev_file =\n\t\tfilp->private_data;\n\n\treturn fasync_helper(fd, filp, on, &comp_ev_file->ev_queue.async_queue);\n}\n\nstatic int ib_uverbs_async_event_close(struct inode *inode, struct file *filp)\n{\n\tstruct ib_uverbs_async_event_file *file = filp->private_data;\n\tstruct ib_uverbs_file *uverbs_file = file->uverbs_file;\n\tstruct ib_uverbs_event *entry, *tmp;\n\tint closed_already = 0;\n\n\tmutex_lock(&uverbs_file->device->lists_mutex);\n\tspin_lock_irq(&file->ev_queue.lock);\n\tclosed_already = file->ev_queue.is_closed;\n\tfile->ev_queue.is_closed = 1;\n\tlist_for_each_entry_safe(entry, tmp, &file->ev_queue.event_list, list) {\n\t\tif (entry->counter)\n\t\t\tlist_del(&entry->obj_list);\n\t\tkfree(entry);\n\t}\n\tspin_unlock_irq(&file->ev_queue.lock);\n\tif (!closed_already) {\n\t\tlist_del(&file->list);\n\t\tib_unregister_event_handler(&uverbs_file->event_handler);\n\t}\n\tmutex_unlock(&uverbs_file->device->lists_mutex);\n\n\tkref_put(&uverbs_file->ref, ib_uverbs_release_file);\n\tkref_put(&file->ref, ib_uverbs_release_async_event_file);\n\n\treturn 0;\n}\n\nstatic int ib_uverbs_comp_event_close(struct inode *inode, struct file *filp)\n{\n\tstruct ib_uobject *uobj = filp->private_data;\n\tstruct ib_uverbs_completion_event_file *file = container_of(\n\t\tuobj, struct ib_uverbs_completion_event_file, uobj);\n\tstruct ib_uverbs_event *entry, *tmp;\n\n\tspin_lock_irq(&file->ev_queue.lock);\n\tlist_for_each_entry_safe(entry, tmp, &file->ev_queue.event_list, list) {\n\t\tif (entry->counter)\n\t\t\tlist_del(&entry->obj_list);\n\t\tkfree(entry);\n\t}\n\tfile->ev_queue.is_closed = 1;\n\tspin_unlock_irq(&file->ev_queue.lock);\n\n\tuverbs_close_fd(filp);\n\n\treturn 0;\n}\n\nconst struct file_operations uverbs_event_fops = {\n\t.owner\t = THIS_MODULE,\n\t.read\t = ib_uverbs_comp_event_read,\n\t.poll    = ib_uverbs_comp_event_poll,\n\t.release = ib_uverbs_comp_event_close,\n\t.fasync  = ib_uverbs_comp_event_fasync,\n\t.llseek\t = no_llseek,\n};\n\nstatic const struct file_operations uverbs_async_event_fops = {\n\t.owner\t = THIS_MODULE,\n\t.read\t = ib_uverbs_async_event_read,\n\t.poll    = ib_uverbs_async_event_poll,\n\t.release = ib_uverbs_async_event_close,\n\t.fasync  = ib_uverbs_async_event_fasync,\n\t.llseek\t = no_llseek,\n};\n\nvoid ib_uverbs_comp_handler(struct ib_cq *cq, void *cq_context)\n{\n\tstruct ib_uverbs_event_queue   *ev_queue = cq_context;\n\tstruct ib_ucq_object\t       *uobj;\n\tstruct ib_uverbs_event\t       *entry;\n\tunsigned long\t\t\tflags;\n\n\tif (!ev_queue)\n\t\treturn;\n\n\tspin_lock_irqsave(&ev_queue->lock, flags);\n\tif (ev_queue->is_closed) {\n\t\tspin_unlock_irqrestore(&ev_queue->lock, flags);\n\t\treturn;\n\t}\n\n\tentry = kmalloc(sizeof(*entry), GFP_ATOMIC);\n\tif (!entry) {\n\t\tspin_unlock_irqrestore(&ev_queue->lock, flags);\n\t\treturn;\n\t}\n\n\tuobj = container_of(cq->uobject, struct ib_ucq_object, uobject);\n\n\tentry->desc.comp.cq_handle = cq->uobject->user_handle;\n\tentry->counter\t\t   = &uobj->comp_events_reported;\n\n\tlist_add_tail(&entry->list, &ev_queue->event_list);\n\tlist_add_tail(&entry->obj_list, &uobj->comp_list);\n\tspin_unlock_irqrestore(&ev_queue->lock, flags);\n\n\twake_up_interruptible(&ev_queue->poll_wait);\n\tkill_fasync(&ev_queue->async_queue, SIGIO, POLL_IN);\n}\n\nstatic void ib_uverbs_async_handler(struct ib_uverbs_file *file,\n\t\t\t\t    __u64 element, __u64 event,\n\t\t\t\t    struct list_head *obj_list,\n\t\t\t\t    u32 *counter)\n{\n\tstruct ib_uverbs_event *entry;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&file->async_file->ev_queue.lock, flags);\n\tif (file->async_file->ev_queue.is_closed) {\n\t\tspin_unlock_irqrestore(&file->async_file->ev_queue.lock, flags);\n\t\treturn;\n\t}\n\n\tentry = kmalloc(sizeof(*entry), GFP_ATOMIC);\n\tif (!entry) {\n\t\tspin_unlock_irqrestore(&file->async_file->ev_queue.lock, flags);\n\t\treturn;\n\t}\n\n\tentry->desc.async.element    = element;\n\tentry->desc.async.event_type = event;\n\tentry->desc.async.reserved   = 0;\n\tentry->counter               = counter;\n\n\tlist_add_tail(&entry->list, &file->async_file->ev_queue.event_list);\n\tif (obj_list)\n\t\tlist_add_tail(&entry->obj_list, obj_list);\n\tspin_unlock_irqrestore(&file->async_file->ev_queue.lock, flags);\n\n\twake_up_interruptible(&file->async_file->ev_queue.poll_wait);\n\tkill_fasync(&file->async_file->ev_queue.async_queue, SIGIO, POLL_IN);\n}\n\nvoid ib_uverbs_cq_event_handler(struct ib_event *event, void *context_ptr)\n{\n\tstruct ib_ucq_object *uobj = container_of(event->element.cq->uobject,\n\t\t\t\t\t\t  struct ib_ucq_object, uobject);\n\n\tib_uverbs_async_handler(uobj->uobject.ufile, uobj->uobject.user_handle,\n\t\t\t\tevent->event, &uobj->async_list,\n\t\t\t\t&uobj->async_events_reported);\n}\n\nvoid ib_uverbs_qp_event_handler(struct ib_event *event, void *context_ptr)\n{\n\tstruct ib_uevent_object *uobj;\n\n\t/* for XRC target qp's, check that qp is live */\n\tif (!event->element.qp->uobject)\n\t\treturn;\n\n\tuobj = container_of(event->element.qp->uobject,\n\t\t\t    struct ib_uevent_object, uobject);\n\n\tib_uverbs_async_handler(context_ptr, uobj->uobject.user_handle,\n\t\t\t\tevent->event, &uobj->event_list,\n\t\t\t\t&uobj->events_reported);\n}\n\nvoid ib_uverbs_wq_event_handler(struct ib_event *event, void *context_ptr)\n{\n\tstruct ib_uevent_object *uobj = container_of(event->element.wq->uobject,\n\t\t\t\t\t\t  struct ib_uevent_object, uobject);\n\n\tib_uverbs_async_handler(context_ptr, uobj->uobject.user_handle,\n\t\t\t\tevent->event, &uobj->event_list,\n\t\t\t\t&uobj->events_reported);\n}\n\nvoid ib_uverbs_srq_event_handler(struct ib_event *event, void *context_ptr)\n{\n\tstruct ib_uevent_object *uobj;\n\n\tuobj = container_of(event->element.srq->uobject,\n\t\t\t    struct ib_uevent_object, uobject);\n\n\tib_uverbs_async_handler(context_ptr, uobj->uobject.user_handle,\n\t\t\t\tevent->event, &uobj->event_list,\n\t\t\t\t&uobj->events_reported);\n}\n\nvoid ib_uverbs_event_handler(struct ib_event_handler *handler,\n\t\t\t     struct ib_event *event)\n{\n\tstruct ib_uverbs_file *file =\n\t\tcontainer_of(handler, struct ib_uverbs_file, event_handler);\n\n\tib_uverbs_async_handler(file, event->element.port_num, event->event,\n\t\t\t\tNULL, NULL);\n}\n\nvoid ib_uverbs_free_async_event_file(struct ib_uverbs_file *file)\n{\n\tkref_put(&file->async_file->ref, ib_uverbs_release_async_event_file);\n\tfile->async_file = NULL;\n}\n\nvoid ib_uverbs_init_event_queue(struct ib_uverbs_event_queue *ev_queue)\n{\n\tspin_lock_init(&ev_queue->lock);\n\tINIT_LIST_HEAD(&ev_queue->event_list);\n\tinit_waitqueue_head(&ev_queue->poll_wait);\n\tev_queue->is_closed   = 0;\n\tev_queue->async_queue = NULL;\n}\n\nstruct file *ib_uverbs_alloc_async_event_file(struct ib_uverbs_file *uverbs_file,\n\t\t\t\t\t      struct ib_device\t*ib_dev)\n{\n\tstruct ib_uverbs_async_event_file *ev_file;\n\tstruct file *filp;\n\n\tev_file = kzalloc(sizeof(*ev_file), GFP_KERNEL);\n\tif (!ev_file)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tib_uverbs_init_event_queue(&ev_file->ev_queue);\n\tev_file->uverbs_file = uverbs_file;\n\tkref_get(&ev_file->uverbs_file->ref);\n\tkref_init(&ev_file->ref);\n\tfilp = anon_inode_getfile(\"[infinibandevent]\", &uverbs_async_event_fops,\n\t\t\t\t  ev_file, O_RDONLY);\n\tif (IS_ERR(filp))\n\t\tgoto err_put_refs;\n\n\tmutex_lock(&uverbs_file->device->lists_mutex);\n\tlist_add_tail(&ev_file->list,\n\t\t      &uverbs_file->device->uverbs_events_file_list);\n\tmutex_unlock(&uverbs_file->device->lists_mutex);\n\n\tWARN_ON(uverbs_file->async_file);\n\tuverbs_file->async_file = ev_file;\n\tkref_get(&uverbs_file->async_file->ref);\n\tINIT_IB_EVENT_HANDLER(&uverbs_file->event_handler,\n\t\t\t      ib_dev,\n\t\t\t      ib_uverbs_event_handler);\n\tib_register_event_handler(&uverbs_file->event_handler);\n\t/* At that point async file stuff was fully set */\n\n\treturn filp;\n\nerr_put_refs:\n\tkref_put(&ev_file->uverbs_file->ref, ib_uverbs_release_file);\n\tkref_put(&ev_file->ref, ib_uverbs_release_async_event_file);\n\treturn filp;\n}\n\nstatic ssize_t verify_hdr(struct ib_uverbs_cmd_hdr *hdr,\n\t\t\t  struct ib_uverbs_ex_cmd_hdr *ex_hdr, size_t count,\n\t\t\t  const struct uverbs_api_write_method *method_elm)\n{\n\tif (method_elm->is_ex) {\n\t\tcount -= sizeof(*hdr) + sizeof(*ex_hdr);\n\n\t\tif ((hdr->in_words + ex_hdr->provider_in_words) * 8 != count)\n\t\t\treturn -EINVAL;\n\n\t\tif (hdr->in_words * 8 < method_elm->req_size)\n\t\t\treturn -ENOSPC;\n\n\t\tif (ex_hdr->cmd_hdr_reserved)\n\t\t\treturn -EINVAL;\n\n\t\tif (ex_hdr->response) {\n\t\t\tif (!hdr->out_words && !ex_hdr->provider_out_words)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tif (hdr->out_words * 8 < method_elm->resp_size)\n\t\t\t\treturn -ENOSPC;\n\n\t\t\tif (!access_ok(u64_to_user_ptr(ex_hdr->response),\n\t\t\t\t       (hdr->out_words + ex_hdr->provider_out_words) * 8))\n\t\t\t\treturn -EFAULT;\n\t\t} else {\n\t\t\tif (hdr->out_words || ex_hdr->provider_out_words)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\t/* not extended command */\n\tif (hdr->in_words * 4 != count)\n\t\treturn -EINVAL;\n\n\tif (count < method_elm->req_size + sizeof(hdr)) {\n\t\t/*\n\t\t * rdma-core v18 and v19 have a bug where they send DESTROY_CQ\n\t\t * with a 16 byte write instead of 24. Old kernels didn't\n\t\t * check the size so they allowed this. Now that the size is\n\t\t * checked provide a compatibility work around to not break\n\t\t * those userspaces.\n\t\t */\n\t\tif (hdr->command == IB_USER_VERBS_CMD_DESTROY_CQ &&\n\t\t    count == 16) {\n\t\t\thdr->in_words = 6;\n\t\t\treturn 0;\n\t\t}\n\t\treturn -ENOSPC;\n\t}\n\tif (hdr->out_words * 4 < method_elm->resp_size)\n\t\treturn -ENOSPC;\n\n\treturn 0;\n}\n\nstatic ssize_t ib_uverbs_write(struct file *filp, const char __user *buf,\n\t\t\t     size_t count, loff_t *pos)\n{\n\tstruct ib_uverbs_file *file = filp->private_data;\n\tconst struct uverbs_api_write_method *method_elm;\n\tstruct uverbs_api *uapi = file->device->uapi;\n\tstruct ib_uverbs_ex_cmd_hdr ex_hdr;\n\tstruct ib_uverbs_cmd_hdr hdr;\n\tstruct uverbs_attr_bundle bundle;\n\tint srcu_key;\n\tssize_t ret;\n\n\tif (!ib_safe_file_access(filp)) {\n\t\tpr_err_once(\"uverbs_write: process %d (%s) changed security contexts after opening file descriptor, this is not allowed.\\n\",\n\t\t\t    task_tgid_vnr(current), current->comm);\n\t\treturn -EACCES;\n\t}\n\n\tif (count < sizeof(hdr))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&hdr, buf, sizeof(hdr)))\n\t\treturn -EFAULT;\n\n\tmethod_elm = uapi_get_method(uapi, hdr.command);\n\tif (IS_ERR(method_elm))\n\t\treturn PTR_ERR(method_elm);\n\n\tif (method_elm->is_ex) {\n\t\tif (count < (sizeof(hdr) + sizeof(ex_hdr)))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&ex_hdr, buf + sizeof(hdr), sizeof(ex_hdr)))\n\t\t\treturn -EFAULT;\n\t}\n\n\tret = verify_hdr(&hdr, &ex_hdr, count, method_elm);\n\tif (ret)\n\t\treturn ret;\n\n\tsrcu_key = srcu_read_lock(&file->device->disassociate_srcu);\n\n\tbuf += sizeof(hdr);\n\n\tmemset(bundle.attr_present, 0, sizeof(bundle.attr_present));\n\tbundle.ufile = file;\n\tbundle.context = NULL; /* only valid if bundle has uobject */\n\tif (!method_elm->is_ex) {\n\t\tsize_t in_len = hdr.in_words * 4 - sizeof(hdr);\n\t\tsize_t out_len = hdr.out_words * 4;\n\t\tu64 response = 0;\n\n\t\tif (method_elm->has_udata) {\n\t\t\tbundle.driver_udata.inlen =\n\t\t\t\tin_len - method_elm->req_size;\n\t\t\tin_len = method_elm->req_size;\n\t\t\tif (bundle.driver_udata.inlen)\n\t\t\t\tbundle.driver_udata.inbuf = buf + in_len;\n\t\t\telse\n\t\t\t\tbundle.driver_udata.inbuf = NULL;\n\t\t} else {\n\t\t\tmemset(&bundle.driver_udata, 0,\n\t\t\t       sizeof(bundle.driver_udata));\n\t\t}\n\n\t\tif (method_elm->has_resp) {\n\t\t\t/*\n\t\t\t * The macros check that if has_resp is set\n\t\t\t * then the command request structure starts\n\t\t\t * with a '__aligned u64 response' member.\n\t\t\t */\n\t\t\tret = get_user(response, (const u64 *)buf);\n\t\t\tif (ret)\n\t\t\t\tgoto out_unlock;\n\n\t\t\tif (method_elm->has_udata) {\n\t\t\t\tbundle.driver_udata.outlen =\n\t\t\t\t\tout_len - method_elm->resp_size;\n\t\t\t\tout_len = method_elm->resp_size;\n\t\t\t\tif (bundle.driver_udata.outlen)\n\t\t\t\t\tbundle.driver_udata.outbuf =\n\t\t\t\t\t\tu64_to_user_ptr(response +\n\t\t\t\t\t\t\t\tout_len);\n\t\t\t\telse\n\t\t\t\t\tbundle.driver_udata.outbuf = NULL;\n\t\t\t}\n\t\t} else {\n\t\t\tbundle.driver_udata.outlen = 0;\n\t\t\tbundle.driver_udata.outbuf = NULL;\n\t\t}\n\n\t\tib_uverbs_init_udata_buf_or_null(\n\t\t\t&bundle.ucore, buf, u64_to_user_ptr(response),\n\t\t\tin_len, out_len);\n\t} else {\n\t\tbuf += sizeof(ex_hdr);\n\n\t\tib_uverbs_init_udata_buf_or_null(&bundle.ucore, buf,\n\t\t\t\t\tu64_to_user_ptr(ex_hdr.response),\n\t\t\t\t\thdr.in_words * 8, hdr.out_words * 8);\n\n\t\tib_uverbs_init_udata_buf_or_null(\n\t\t\t&bundle.driver_udata, buf + bundle.ucore.inlen,\n\t\t\tu64_to_user_ptr(ex_hdr.response) + bundle.ucore.outlen,\n\t\t\tex_hdr.provider_in_words * 8,\n\t\t\tex_hdr.provider_out_words * 8);\n\n\t}\n\n\tret = method_elm->handler(&bundle);\nout_unlock:\n\tsrcu_read_unlock(&file->device->disassociate_srcu, srcu_key);\n\treturn (ret) ? : count;\n}\n\nstatic int ib_uverbs_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\tstruct ib_uverbs_file *file = filp->private_data;\n\tstruct ib_ucontext *ucontext;\n\tint ret = 0;\n\tint srcu_key;\n\n\tsrcu_key = srcu_read_lock(&file->device->disassociate_srcu);\n\tucontext = ib_uverbs_get_ucontext_file(file);\n\tif (IS_ERR(ucontext)) {\n\t\tret = PTR_ERR(ucontext);\n\t\tgoto out;\n\t}\n\n\tret = ucontext->device->ops.mmap(ucontext, vma);\nout:\n\tsrcu_read_unlock(&file->device->disassociate_srcu, srcu_key);\n\treturn ret;\n}\n\n/*\n * Each time we map IO memory into user space this keeps track of the mapping.\n * When the device is hot-unplugged we 'zap' the mmaps in user space to point\n * to the zero page and allow the hot unplug to proceed.\n *\n * This is necessary for cases like PCI physical hot unplug as the actual BAR\n * memory may vanish after this and access to it from userspace could MCE.\n *\n * RDMA drivers supporting disassociation must have their user space designed\n * to cope in some way with their IO pages going to the zero page.\n */\nstruct rdma_umap_priv {\n\tstruct vm_area_struct *vma;\n\tstruct list_head list;\n};\n\nstatic const struct vm_operations_struct rdma_umap_ops;\n\nstatic void rdma_umap_priv_init(struct rdma_umap_priv *priv,\n\t\t\t\tstruct vm_area_struct *vma)\n{\n\tstruct ib_uverbs_file *ufile = vma->vm_file->private_data;\n\n\tpriv->vma = vma;\n\tvma->vm_private_data = priv;\n\tvma->vm_ops = &rdma_umap_ops;\n\n\tmutex_lock(&ufile->umap_lock);\n\tlist_add(&priv->list, &ufile->umaps);\n\tmutex_unlock(&ufile->umap_lock);\n}\n\n/*\n * The VMA has been dup'd, initialize the vm_private_data with a new tracking\n * struct\n */\nstatic void rdma_umap_open(struct vm_area_struct *vma)\n{\n\tstruct ib_uverbs_file *ufile = vma->vm_file->private_data;\n\tstruct rdma_umap_priv *opriv = vma->vm_private_data;\n\tstruct rdma_umap_priv *priv;\n\n\tif (!opriv)\n\t\treturn;\n\n\t/* We are racing with disassociation */\n\tif (!down_read_trylock(&ufile->hw_destroy_rwsem))\n\t\tgoto out_zap;\n\t/*\n\t * Disassociation already completed, the VMA should already be zapped.\n\t */\n\tif (!ufile->ucontext)\n\t\tgoto out_unlock;\n\n\tpriv = kzalloc(sizeof(*priv), GFP_KERNEL);\n\tif (!priv)\n\t\tgoto out_unlock;\n\trdma_umap_priv_init(priv, vma);\n\n\tup_read(&ufile->hw_destroy_rwsem);\n\treturn;\n\nout_unlock:\n\tup_read(&ufile->hw_destroy_rwsem);\nout_zap:\n\t/*\n\t * We can't allow the VMA to be created with the actual IO pages, that\n\t * would break our API contract, and it can't be stopped at this\n\t * point, so zap it.\n\t */\n\tvma->vm_private_data = NULL;\n\tzap_vma_ptes(vma, vma->vm_start, vma->vm_end - vma->vm_start);\n}\n\nstatic void rdma_umap_close(struct vm_area_struct *vma)\n{\n\tstruct ib_uverbs_file *ufile = vma->vm_file->private_data;\n\tstruct rdma_umap_priv *priv = vma->vm_private_data;\n\n\tif (!priv)\n\t\treturn;\n\n\t/*\n\t * The vma holds a reference on the struct file that created it, which\n\t * in turn means that the ib_uverbs_file is guaranteed to exist at\n\t * this point.\n\t */\n\tmutex_lock(&ufile->umap_lock);\n\tlist_del(&priv->list);\n\tmutex_unlock(&ufile->umap_lock);\n\tkfree(priv);\n}\n\nstatic const struct vm_operations_struct rdma_umap_ops = {\n\t.open = rdma_umap_open,\n\t.close = rdma_umap_close,\n};\n\nstatic struct rdma_umap_priv *rdma_user_mmap_pre(struct ib_ucontext *ucontext,\n\t\t\t\t\t\t struct vm_area_struct *vma,\n\t\t\t\t\t\t unsigned long size)\n{\n\tstruct ib_uverbs_file *ufile = ucontext->ufile;\n\tstruct rdma_umap_priv *priv;\n\n\tif (vma->vm_end - vma->vm_start != size)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/* Driver is using this wrong, must be called by ib_uverbs_mmap */\n\tif (WARN_ON(!vma->vm_file ||\n\t\t    vma->vm_file->private_data != ufile))\n\t\treturn ERR_PTR(-EINVAL);\n\tlockdep_assert_held(&ufile->device->disassociate_srcu);\n\n\tpriv = kzalloc(sizeof(*priv), GFP_KERNEL);\n\tif (!priv)\n\t\treturn ERR_PTR(-ENOMEM);\n\treturn priv;\n}\n\n/*\n * Map IO memory into a process. This is to be called by drivers as part of\n * their mmap() functions if they wish to send something like PCI-E BAR memory\n * to userspace.\n */\nint rdma_user_mmap_io(struct ib_ucontext *ucontext, struct vm_area_struct *vma,\n\t\t      unsigned long pfn, unsigned long size, pgprot_t prot)\n{\n\tstruct rdma_umap_priv *priv = rdma_user_mmap_pre(ucontext, vma, size);\n\n\tif (IS_ERR(priv))\n\t\treturn PTR_ERR(priv);\n\n\tvma->vm_page_prot = prot;\n\tif (io_remap_pfn_range(vma, vma->vm_start, pfn, size, prot)) {\n\t\tkfree(priv);\n\t\treturn -EAGAIN;\n\t}\n\n\trdma_umap_priv_init(priv, vma);\n\treturn 0;\n}\nEXPORT_SYMBOL(rdma_user_mmap_io);\n\n/*\n * The page case is here for a slightly different reason, the driver expects\n * to be able to free the page it is sharing to user space when it destroys\n * its ucontext, which means we need to zap the user space references.\n *\n * We could handle this differently by providing an API to allocate a shared\n * page and then only freeing the shared page when the last ufile is\n * destroyed.\n */\nint rdma_user_mmap_page(struct ib_ucontext *ucontext,\n\t\t\tstruct vm_area_struct *vma, struct page *page,\n\t\t\tunsigned long size)\n{\n\tstruct rdma_umap_priv *priv = rdma_user_mmap_pre(ucontext, vma, size);\n\n\tif (IS_ERR(priv))\n\t\treturn PTR_ERR(priv);\n\n\tif (remap_pfn_range(vma, vma->vm_start, page_to_pfn(page), size,\n\t\t\t    vma->vm_page_prot)) {\n\t\tkfree(priv);\n\t\treturn -EAGAIN;\n\t}\n\n\trdma_umap_priv_init(priv, vma);\n\treturn 0;\n}\nEXPORT_SYMBOL(rdma_user_mmap_page);\n\nvoid uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)\n{\n\tstruct rdma_umap_priv *priv, *next_priv;\n\n\tlockdep_assert_held(&ufile->hw_destroy_rwsem);\n\n\twhile (1) {\n\t\tstruct mm_struct *mm = NULL;\n\n\t\t/* Get an arbitrary mm pointer that hasn't been cleaned yet */\n\t\tmutex_lock(&ufile->umap_lock);\n\t\twhile (!list_empty(&ufile->umaps)) {\n\t\t\tint ret;\n\n\t\t\tpriv = list_first_entry(&ufile->umaps,\n\t\t\t\t\t\tstruct rdma_umap_priv, list);\n\t\t\tmm = priv->vma->vm_mm;\n\t\t\tret = mmget_not_zero(mm);\n\t\t\tif (!ret) {\n\t\t\t\tlist_del_init(&priv->list);\n\t\t\t\tmm = NULL;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tmutex_unlock(&ufile->umap_lock);\n\t\tif (!mm)\n\t\t\treturn;\n\n\t\t/*\n\t\t * The umap_lock is nested under mmap_sem since it used within\n\t\t * the vma_ops callbacks, so we have to clean the list one mm\n\t\t * at a time to get the lock ordering right. Typically there\n\t\t * will only be one mm, so no big deal.\n\t\t */\n\t\tdown_write(&mm->mmap_sem);\n\t\tif (!mmget_still_valid(mm))\n\t\t\tgoto skip_mm;\n\t\tmutex_lock(&ufile->umap_lock);\n\t\tlist_for_each_entry_safe (priv, next_priv, &ufile->umaps,\n\t\t\t\t\t  list) {\n\t\t\tstruct vm_area_struct *vma = priv->vma;\n\n\t\t\tif (vma->vm_mm != mm)\n\t\t\t\tcontinue;\n\t\t\tlist_del_init(&priv->list);\n\n\t\t\tzap_vma_ptes(vma, vma->vm_start,\n\t\t\t\t     vma->vm_end - vma->vm_start);\n\t\t\tvma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);\n\t\t}\n\t\tmutex_unlock(&ufile->umap_lock);\n\tskip_mm:\n\t\tup_write(&mm->mmap_sem);\n\t\tmmput(mm);\n\t}\n}\n\n/*\n * ib_uverbs_open() does not need the BKL:\n *\n *  - the ib_uverbs_device structures are properly reference counted and\n *    everything else is purely local to the file being created, so\n *    races against other open calls are not a problem;\n *  - there is no ioctl method to race against;\n *  - the open method will either immediately run -ENXIO, or all\n *    required initialization will be done.\n */\nstatic int ib_uverbs_open(struct inode *inode, struct file *filp)\n{\n\tstruct ib_uverbs_device *dev;\n\tstruct ib_uverbs_file *file;\n\tstruct ib_device *ib_dev;\n\tint ret;\n\tint module_dependent;\n\tint srcu_key;\n\n\tdev = container_of(inode->i_cdev, struct ib_uverbs_device, cdev);\n\tif (!atomic_inc_not_zero(&dev->refcount))\n\t\treturn -ENXIO;\n\n\tget_device(&dev->dev);\n\tsrcu_key = srcu_read_lock(&dev->disassociate_srcu);\n\tmutex_lock(&dev->lists_mutex);\n\tib_dev = srcu_dereference(dev->ib_dev,\n\t\t\t\t  &dev->disassociate_srcu);\n\tif (!ib_dev) {\n\t\tret = -EIO;\n\t\tgoto err;\n\t}\n\n\t/* In case IB device supports disassociate ucontext, there is no hard\n\t * dependency between uverbs device and its low level device.\n\t */\n\tmodule_dependent = !(ib_dev->ops.disassociate_ucontext);\n\n\tif (module_dependent) {\n\t\tif (!try_module_get(ib_dev->owner)) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\tfile = kzalloc(sizeof(*file), GFP_KERNEL);\n\tif (!file) {\n\t\tret = -ENOMEM;\n\t\tif (module_dependent)\n\t\t\tgoto err_module;\n\n\t\tgoto err;\n\t}\n\n\tfile->device\t = dev;\n\tkref_init(&file->ref);\n\tmutex_init(&file->ucontext_lock);\n\n\tspin_lock_init(&file->uobjects_lock);\n\tINIT_LIST_HEAD(&file->uobjects);\n\tinit_rwsem(&file->hw_destroy_rwsem);\n\tmutex_init(&file->umap_lock);\n\tINIT_LIST_HEAD(&file->umaps);\n\n\tfilp->private_data = file;\n\tlist_add_tail(&file->list, &dev->uverbs_file_list);\n\tmutex_unlock(&dev->lists_mutex);\n\tsrcu_read_unlock(&dev->disassociate_srcu, srcu_key);\n\n\tsetup_ufile_idr_uobject(file);\n\n\treturn nonseekable_open(inode, filp);\n\nerr_module:\n\tmodule_put(ib_dev->owner);\n\nerr:\n\tmutex_unlock(&dev->lists_mutex);\n\tsrcu_read_unlock(&dev->disassociate_srcu, srcu_key);\n\tif (atomic_dec_and_test(&dev->refcount))\n\t\tib_uverbs_comp_dev(dev);\n\n\tput_device(&dev->dev);\n\treturn ret;\n}\n\nstatic int ib_uverbs_close(struct inode *inode, struct file *filp)\n{\n\tstruct ib_uverbs_file *file = filp->private_data;\n\n\tuverbs_destroy_ufile_hw(file, RDMA_REMOVE_CLOSE);\n\n\tmutex_lock(&file->device->lists_mutex);\n\tlist_del_init(&file->list);\n\tmutex_unlock(&file->device->lists_mutex);\n\n\tkref_put(&file->ref, ib_uverbs_release_file);\n\n\treturn 0;\n}\n\nstatic const struct file_operations uverbs_fops = {\n\t.owner\t = THIS_MODULE,\n\t.write\t = ib_uverbs_write,\n\t.open\t = ib_uverbs_open,\n\t.release = ib_uverbs_close,\n\t.llseek\t = no_llseek,\n\t.unlocked_ioctl = ib_uverbs_ioctl,\n\t.compat_ioctl = ib_uverbs_ioctl,\n};\n\nstatic const struct file_operations uverbs_mmap_fops = {\n\t.owner\t = THIS_MODULE,\n\t.write\t = ib_uverbs_write,\n\t.mmap    = ib_uverbs_mmap,\n\t.open\t = ib_uverbs_open,\n\t.release = ib_uverbs_close,\n\t.llseek\t = no_llseek,\n\t.unlocked_ioctl = ib_uverbs_ioctl,\n\t.compat_ioctl = ib_uverbs_ioctl,\n};\n\nstatic struct ib_client uverbs_client = {\n\t.name   = \"uverbs\",\n\t.no_kverbs_req = true,\n\t.add    = ib_uverbs_add_one,\n\t.remove = ib_uverbs_remove_one\n};\n\nstatic ssize_t ibdev_show(struct device *device, struct device_attribute *attr,\n\t\t\t  char *buf)\n{\n\tstruct ib_uverbs_device *dev =\n\t\t\tcontainer_of(device, struct ib_uverbs_device, dev);\n\tint ret = -ENODEV;\n\tint srcu_key;\n\tstruct ib_device *ib_dev;\n\n\tsrcu_key = srcu_read_lock(&dev->disassociate_srcu);\n\tib_dev = srcu_dereference(dev->ib_dev, &dev->disassociate_srcu);\n\tif (ib_dev)\n\t\tret = sprintf(buf, \"%s\\n\", dev_name(&ib_dev->dev));\n\tsrcu_read_unlock(&dev->disassociate_srcu, srcu_key);\n\n\treturn ret;\n}\nstatic DEVICE_ATTR_RO(ibdev);\n\nstatic ssize_t abi_version_show(struct device *device,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct ib_uverbs_device *dev =\n\t\t\tcontainer_of(device, struct ib_uverbs_device, dev);\n\tint ret = -ENODEV;\n\tint srcu_key;\n\tstruct ib_device *ib_dev;\n\n\tsrcu_key = srcu_read_lock(&dev->disassociate_srcu);\n\tib_dev = srcu_dereference(dev->ib_dev, &dev->disassociate_srcu);\n\tif (ib_dev)\n\t\tret = sprintf(buf, \"%d\\n\", ib_dev->uverbs_abi_ver);\n\tsrcu_read_unlock(&dev->disassociate_srcu, srcu_key);\n\n\treturn ret;\n}\nstatic DEVICE_ATTR_RO(abi_version);\n\nstatic struct attribute *ib_dev_attrs[] = {\n\t&dev_attr_abi_version.attr,\n\t&dev_attr_ibdev.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group dev_attr_group = {\n\t.attrs = ib_dev_attrs,\n};\n\nstatic CLASS_ATTR_STRING(abi_version, S_IRUGO,\n\t\t\t __stringify(IB_USER_VERBS_ABI_VERSION));\n\nstatic int ib_uverbs_create_uapi(struct ib_device *device,\n\t\t\t\t struct ib_uverbs_device *uverbs_dev)\n{\n\tstruct uverbs_api *uapi;\n\n\tuapi = uverbs_alloc_api(device);\n\tif (IS_ERR(uapi))\n\t\treturn PTR_ERR(uapi);\n\n\tuverbs_dev->uapi = uapi;\n\treturn 0;\n}\n\nstatic void ib_uverbs_add_one(struct ib_device *device)\n{\n\tint devnum;\n\tdev_t base;\n\tstruct ib_uverbs_device *uverbs_dev;\n\tint ret;\n\n\tif (!device->ops.alloc_ucontext)\n\t\treturn;\n\n\tuverbs_dev = kzalloc(sizeof(*uverbs_dev), GFP_KERNEL);\n\tif (!uverbs_dev)\n\t\treturn;\n\n\tret = init_srcu_struct(&uverbs_dev->disassociate_srcu);\n\tif (ret) {\n\t\tkfree(uverbs_dev);\n\t\treturn;\n\t}\n\n\tdevice_initialize(&uverbs_dev->dev);\n\tuverbs_dev->dev.class = uverbs_class;\n\tuverbs_dev->dev.parent = device->dev.parent;\n\tuverbs_dev->dev.release = ib_uverbs_release_dev;\n\tuverbs_dev->groups[0] = &dev_attr_group;\n\tuverbs_dev->dev.groups = uverbs_dev->groups;\n\tatomic_set(&uverbs_dev->refcount, 1);\n\tinit_completion(&uverbs_dev->comp);\n\tuverbs_dev->xrcd_tree = RB_ROOT;\n\tmutex_init(&uverbs_dev->xrcd_tree_mutex);\n\tmutex_init(&uverbs_dev->lists_mutex);\n\tINIT_LIST_HEAD(&uverbs_dev->uverbs_file_list);\n\tINIT_LIST_HEAD(&uverbs_dev->uverbs_events_file_list);\n\trcu_assign_pointer(uverbs_dev->ib_dev, device);\n\tuverbs_dev->num_comp_vectors = device->num_comp_vectors;\n\n\tdevnum = ida_alloc_max(&uverbs_ida, IB_UVERBS_MAX_DEVICES - 1,\n\t\t\t       GFP_KERNEL);\n\tif (devnum < 0)\n\t\tgoto err;\n\tuverbs_dev->devnum = devnum;\n\tif (devnum >= IB_UVERBS_NUM_FIXED_MINOR)\n\t\tbase = dynamic_uverbs_dev + devnum - IB_UVERBS_NUM_FIXED_MINOR;\n\telse\n\t\tbase = IB_UVERBS_BASE_DEV + devnum;\n\n\tif (ib_uverbs_create_uapi(device, uverbs_dev))\n\t\tgoto err_uapi;\n\n\tuverbs_dev->dev.devt = base;\n\tdev_set_name(&uverbs_dev->dev, \"uverbs%d\", uverbs_dev->devnum);\n\n\tcdev_init(&uverbs_dev->cdev,\n\t\t  device->ops.mmap ? &uverbs_mmap_fops : &uverbs_fops);\n\tuverbs_dev->cdev.owner = THIS_MODULE;\n\n\tret = cdev_device_add(&uverbs_dev->cdev, &uverbs_dev->dev);\n\tif (ret)\n\t\tgoto err_uapi;\n\n\tib_set_client_data(device, &uverbs_client, uverbs_dev);\n\treturn;\n\nerr_uapi:\n\tida_free(&uverbs_ida, devnum);\nerr:\n\tif (atomic_dec_and_test(&uverbs_dev->refcount))\n\t\tib_uverbs_comp_dev(uverbs_dev);\n\twait_for_completion(&uverbs_dev->comp);\n\tput_device(&uverbs_dev->dev);\n\treturn;\n}\n\nstatic void ib_uverbs_free_hw_resources(struct ib_uverbs_device *uverbs_dev,\n\t\t\t\t\tstruct ib_device *ib_dev)\n{\n\tstruct ib_uverbs_file *file;\n\tstruct ib_uverbs_async_event_file *event_file;\n\tstruct ib_event event;\n\n\t/* Pending running commands to terminate */\n\tuverbs_disassociate_api_pre(uverbs_dev);\n\tevent.event = IB_EVENT_DEVICE_FATAL;\n\tevent.element.port_num = 0;\n\tevent.device = ib_dev;\n\n\tmutex_lock(&uverbs_dev->lists_mutex);\n\twhile (!list_empty(&uverbs_dev->uverbs_file_list)) {\n\t\tfile = list_first_entry(&uverbs_dev->uverbs_file_list,\n\t\t\t\t\tstruct ib_uverbs_file, list);\n\t\tlist_del_init(&file->list);\n\t\tkref_get(&file->ref);\n\n\t\t/* We must release the mutex before going ahead and calling\n\t\t * uverbs_cleanup_ufile, as it might end up indirectly calling\n\t\t * uverbs_close, for example due to freeing the resources (e.g\n\t\t * mmput).\n\t\t */\n\t\tmutex_unlock(&uverbs_dev->lists_mutex);\n\n\t\tib_uverbs_event_handler(&file->event_handler, &event);\n\t\tuverbs_destroy_ufile_hw(file, RDMA_REMOVE_DRIVER_REMOVE);\n\t\tkref_put(&file->ref, ib_uverbs_release_file);\n\n\t\tmutex_lock(&uverbs_dev->lists_mutex);\n\t}\n\n\twhile (!list_empty(&uverbs_dev->uverbs_events_file_list)) {\n\t\tevent_file = list_first_entry(&uverbs_dev->\n\t\t\t\t\t      uverbs_events_file_list,\n\t\t\t\t\t      struct ib_uverbs_async_event_file,\n\t\t\t\t\t      list);\n\t\tspin_lock_irq(&event_file->ev_queue.lock);\n\t\tevent_file->ev_queue.is_closed = 1;\n\t\tspin_unlock_irq(&event_file->ev_queue.lock);\n\n\t\tlist_del(&event_file->list);\n\t\tib_unregister_event_handler(\n\t\t\t&event_file->uverbs_file->event_handler);\n\t\tevent_file->uverbs_file->event_handler.device =\n\t\t\tNULL;\n\n\t\twake_up_interruptible(&event_file->ev_queue.poll_wait);\n\t\tkill_fasync(&event_file->ev_queue.async_queue, SIGIO, POLL_IN);\n\t}\n\tmutex_unlock(&uverbs_dev->lists_mutex);\n\n\tuverbs_disassociate_api(uverbs_dev->uapi);\n}\n\nstatic void ib_uverbs_remove_one(struct ib_device *device, void *client_data)\n{\n\tstruct ib_uverbs_device *uverbs_dev = client_data;\n\tint wait_clients = 1;\n\n\tif (!uverbs_dev)\n\t\treturn;\n\n\tcdev_device_del(&uverbs_dev->cdev, &uverbs_dev->dev);\n\tida_free(&uverbs_ida, uverbs_dev->devnum);\n\n\tif (device->ops.disassociate_ucontext) {\n\t\t/* We disassociate HW resources and immediately return.\n\t\t * Userspace will see a EIO errno for all future access.\n\t\t * Upon returning, ib_device may be freed internally and is not\n\t\t * valid any more.\n\t\t * uverbs_device is still available until all clients close\n\t\t * their files, then the uverbs device ref count will be zero\n\t\t * and its resources will be freed.\n\t\t * Note: At this point no more files can be opened since the\n\t\t * cdev was deleted, however active clients can still issue\n\t\t * commands and close their open files.\n\t\t */\n\t\tib_uverbs_free_hw_resources(uverbs_dev, device);\n\t\twait_clients = 0;\n\t}\n\n\tif (atomic_dec_and_test(&uverbs_dev->refcount))\n\t\tib_uverbs_comp_dev(uverbs_dev);\n\tif (wait_clients)\n\t\twait_for_completion(&uverbs_dev->comp);\n\n\tput_device(&uverbs_dev->dev);\n}\n\nstatic char *uverbs_devnode(struct device *dev, umode_t *mode)\n{\n\tif (mode)\n\t\t*mode = 0666;\n\treturn kasprintf(GFP_KERNEL, \"infiniband/%s\", dev_name(dev));\n}\n\nstatic int __init ib_uverbs_init(void)\n{\n\tint ret;\n\n\tret = register_chrdev_region(IB_UVERBS_BASE_DEV,\n\t\t\t\t     IB_UVERBS_NUM_FIXED_MINOR,\n\t\t\t\t     \"infiniband_verbs\");\n\tif (ret) {\n\t\tpr_err(\"user_verbs: couldn't register device number\\n\");\n\t\tgoto out;\n\t}\n\n\tret = alloc_chrdev_region(&dynamic_uverbs_dev, 0,\n\t\t\t\t  IB_UVERBS_NUM_DYNAMIC_MINOR,\n\t\t\t\t  \"infiniband_verbs\");\n\tif (ret) {\n\t\tpr_err(\"couldn't register dynamic device number\\n\");\n\t\tgoto out_alloc;\n\t}\n\n\tuverbs_class = class_create(THIS_MODULE, \"infiniband_verbs\");\n\tif (IS_ERR(uverbs_class)) {\n\t\tret = PTR_ERR(uverbs_class);\n\t\tpr_err(\"user_verbs: couldn't create class infiniband_verbs\\n\");\n\t\tgoto out_chrdev;\n\t}\n\n\tuverbs_class->devnode = uverbs_devnode;\n\n\tret = class_create_file(uverbs_class, &class_attr_abi_version.attr);\n\tif (ret) {\n\t\tpr_err(\"user_verbs: couldn't create abi_version attribute\\n\");\n\t\tgoto out_class;\n\t}\n\n\tret = ib_register_client(&uverbs_client);\n\tif (ret) {\n\t\tpr_err(\"user_verbs: couldn't register client\\n\");\n\t\tgoto out_class;\n\t}\n\n\treturn 0;\n\nout_class:\n\tclass_destroy(uverbs_class);\n\nout_chrdev:\n\tunregister_chrdev_region(dynamic_uverbs_dev,\n\t\t\t\t IB_UVERBS_NUM_DYNAMIC_MINOR);\n\nout_alloc:\n\tunregister_chrdev_region(IB_UVERBS_BASE_DEV,\n\t\t\t\t IB_UVERBS_NUM_FIXED_MINOR);\n\nout:\n\treturn ret;\n}\n\nstatic void __exit ib_uverbs_cleanup(void)\n{\n\tib_unregister_client(&uverbs_client);\n\tclass_destroy(uverbs_class);\n\tunregister_chrdev_region(IB_UVERBS_BASE_DEV,\n\t\t\t\t IB_UVERBS_NUM_FIXED_MINOR);\n\tunregister_chrdev_region(dynamic_uverbs_dev,\n\t\t\t\t IB_UVERBS_NUM_DYNAMIC_MINOR);\n}\n\nmodule_init(ib_uverbs_init);\nmodule_exit(ib_uverbs_cleanup);\n", "// SPDX-License-Identifier: GPL-2.0\n#include <linux/mm.h>\n#include <linux/vmacache.h>\n#include <linux/hugetlb.h>\n#include <linux/huge_mm.h>\n#include <linux/mount.h>\n#include <linux/seq_file.h>\n#include <linux/highmem.h>\n#include <linux/ptrace.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/mempolicy.h>\n#include <linux/rmap.h>\n#include <linux/swap.h>\n#include <linux/sched/mm.h>\n#include <linux/swapops.h>\n#include <linux/mmu_notifier.h>\n#include <linux/page_idle.h>\n#include <linux/shmem_fs.h>\n#include <linux/uaccess.h>\n#include <linux/pkeys.h>\n\n#include <asm/elf.h>\n#include <asm/tlb.h>\n#include <asm/tlbflush.h>\n#include \"internal.h\"\n\n#define SEQ_PUT_DEC(str, val) \\\n\t\tseq_put_decimal_ull_width(m, str, (val) << (PAGE_SHIFT-10), 8)\nvoid task_mem(struct seq_file *m, struct mm_struct *mm)\n{\n\tunsigned long text, lib, swap, anon, file, shmem;\n\tunsigned long hiwater_vm, total_vm, hiwater_rss, total_rss;\n\n\tanon = get_mm_counter(mm, MM_ANONPAGES);\n\tfile = get_mm_counter(mm, MM_FILEPAGES);\n\tshmem = get_mm_counter(mm, MM_SHMEMPAGES);\n\n\t/*\n\t * Note: to minimize their overhead, mm maintains hiwater_vm and\n\t * hiwater_rss only when about to *lower* total_vm or rss.  Any\n\t * collector of these hiwater stats must therefore get total_vm\n\t * and rss too, which will usually be the higher.  Barriers? not\n\t * worth the effort, such snapshots can always be inconsistent.\n\t */\n\thiwater_vm = total_vm = mm->total_vm;\n\tif (hiwater_vm < mm->hiwater_vm)\n\t\thiwater_vm = mm->hiwater_vm;\n\thiwater_rss = total_rss = anon + file + shmem;\n\tif (hiwater_rss < mm->hiwater_rss)\n\t\thiwater_rss = mm->hiwater_rss;\n\n\t/* split executable areas between text and lib */\n\ttext = PAGE_ALIGN(mm->end_code) - (mm->start_code & PAGE_MASK);\n\ttext = min(text, mm->exec_vm << PAGE_SHIFT);\n\tlib = (mm->exec_vm << PAGE_SHIFT) - text;\n\n\tswap = get_mm_counter(mm, MM_SWAPENTS);\n\tSEQ_PUT_DEC(\"VmPeak:\\t\", hiwater_vm);\n\tSEQ_PUT_DEC(\" kB\\nVmSize:\\t\", total_vm);\n\tSEQ_PUT_DEC(\" kB\\nVmLck:\\t\", mm->locked_vm);\n\tSEQ_PUT_DEC(\" kB\\nVmPin:\\t\", atomic64_read(&mm->pinned_vm));\n\tSEQ_PUT_DEC(\" kB\\nVmHWM:\\t\", hiwater_rss);\n\tSEQ_PUT_DEC(\" kB\\nVmRSS:\\t\", total_rss);\n\tSEQ_PUT_DEC(\" kB\\nRssAnon:\\t\", anon);\n\tSEQ_PUT_DEC(\" kB\\nRssFile:\\t\", file);\n\tSEQ_PUT_DEC(\" kB\\nRssShmem:\\t\", shmem);\n\tSEQ_PUT_DEC(\" kB\\nVmData:\\t\", mm->data_vm);\n\tSEQ_PUT_DEC(\" kB\\nVmStk:\\t\", mm->stack_vm);\n\tseq_put_decimal_ull_width(m,\n\t\t    \" kB\\nVmExe:\\t\", text >> 10, 8);\n\tseq_put_decimal_ull_width(m,\n\t\t    \" kB\\nVmLib:\\t\", lib >> 10, 8);\n\tseq_put_decimal_ull_width(m,\n\t\t    \" kB\\nVmPTE:\\t\", mm_pgtables_bytes(mm) >> 10, 8);\n\tSEQ_PUT_DEC(\" kB\\nVmSwap:\\t\", swap);\n\tseq_puts(m, \" kB\\n\");\n\thugetlb_report_usage(m, mm);\n}\n#undef SEQ_PUT_DEC\n\nunsigned long task_vsize(struct mm_struct *mm)\n{\n\treturn PAGE_SIZE * mm->total_vm;\n}\n\nunsigned long task_statm(struct mm_struct *mm,\n\t\t\t unsigned long *shared, unsigned long *text,\n\t\t\t unsigned long *data, unsigned long *resident)\n{\n\t*shared = get_mm_counter(mm, MM_FILEPAGES) +\n\t\t\tget_mm_counter(mm, MM_SHMEMPAGES);\n\t*text = (PAGE_ALIGN(mm->end_code) - (mm->start_code & PAGE_MASK))\n\t\t\t\t\t\t\t\t>> PAGE_SHIFT;\n\t*data = mm->data_vm + mm->stack_vm;\n\t*resident = *shared + get_mm_counter(mm, MM_ANONPAGES);\n\treturn mm->total_vm;\n}\n\n#ifdef CONFIG_NUMA\n/*\n * Save get_task_policy() for show_numa_map().\n */\nstatic void hold_task_mempolicy(struct proc_maps_private *priv)\n{\n\tstruct task_struct *task = priv->task;\n\n\ttask_lock(task);\n\tpriv->task_mempolicy = get_task_policy(task);\n\tmpol_get(priv->task_mempolicy);\n\ttask_unlock(task);\n}\nstatic void release_task_mempolicy(struct proc_maps_private *priv)\n{\n\tmpol_put(priv->task_mempolicy);\n}\n#else\nstatic void hold_task_mempolicy(struct proc_maps_private *priv)\n{\n}\nstatic void release_task_mempolicy(struct proc_maps_private *priv)\n{\n}\n#endif\n\nstatic void vma_stop(struct proc_maps_private *priv)\n{\n\tstruct mm_struct *mm = priv->mm;\n\n\trelease_task_mempolicy(priv);\n\tup_read(&mm->mmap_sem);\n\tmmput(mm);\n}\n\nstatic struct vm_area_struct *\nm_next_vma(struct proc_maps_private *priv, struct vm_area_struct *vma)\n{\n\tif (vma == priv->tail_vma)\n\t\treturn NULL;\n\treturn vma->vm_next ?: priv->tail_vma;\n}\n\nstatic void m_cache_vma(struct seq_file *m, struct vm_area_struct *vma)\n{\n\tif (m->count < m->size)\t/* vma is copied successfully */\n\t\tm->version = m_next_vma(m->private, vma) ? vma->vm_end : -1UL;\n}\n\nstatic void *m_start(struct seq_file *m, loff_t *ppos)\n{\n\tstruct proc_maps_private *priv = m->private;\n\tunsigned long last_addr = m->version;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tunsigned int pos = *ppos;\n\n\t/* See m_cache_vma(). Zero at the start or after lseek. */\n\tif (last_addr == -1UL)\n\t\treturn NULL;\n\n\tpriv->task = get_proc_task(priv->inode);\n\tif (!priv->task)\n\t\treturn ERR_PTR(-ESRCH);\n\n\tmm = priv->mm;\n\tif (!mm || !mmget_not_zero(mm))\n\t\treturn NULL;\n\n\tdown_read(&mm->mmap_sem);\n\thold_task_mempolicy(priv);\n\tpriv->tail_vma = get_gate_vma(mm);\n\n\tif (last_addr) {\n\t\tvma = find_vma(mm, last_addr - 1);\n\t\tif (vma && vma->vm_start <= last_addr)\n\t\t\tvma = m_next_vma(priv, vma);\n\t\tif (vma)\n\t\t\treturn vma;\n\t}\n\n\tm->version = 0;\n\tif (pos < mm->map_count) {\n\t\tfor (vma = mm->mmap; pos; pos--) {\n\t\t\tm->version = vma->vm_start;\n\t\t\tvma = vma->vm_next;\n\t\t}\n\t\treturn vma;\n\t}\n\n\t/* we do not bother to update m->version in this case */\n\tif (pos == mm->map_count && priv->tail_vma)\n\t\treturn priv->tail_vma;\n\n\tvma_stop(priv);\n\treturn NULL;\n}\n\nstatic void *m_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct proc_maps_private *priv = m->private;\n\tstruct vm_area_struct *next;\n\n\t(*pos)++;\n\tnext = m_next_vma(priv, v);\n\tif (!next)\n\t\tvma_stop(priv);\n\treturn next;\n}\n\nstatic void m_stop(struct seq_file *m, void *v)\n{\n\tstruct proc_maps_private *priv = m->private;\n\n\tif (!IS_ERR_OR_NULL(v))\n\t\tvma_stop(priv);\n\tif (priv->task) {\n\t\tput_task_struct(priv->task);\n\t\tpriv->task = NULL;\n\t}\n}\n\nstatic int proc_maps_open(struct inode *inode, struct file *file,\n\t\t\tconst struct seq_operations *ops, int psize)\n{\n\tstruct proc_maps_private *priv = __seq_open_private(file, ops, psize);\n\n\tif (!priv)\n\t\treturn -ENOMEM;\n\n\tpriv->inode = inode;\n\tpriv->mm = proc_mem_open(inode, PTRACE_MODE_READ);\n\tif (IS_ERR(priv->mm)) {\n\t\tint err = PTR_ERR(priv->mm);\n\n\t\tseq_release_private(inode, file);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int proc_map_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *seq = file->private_data;\n\tstruct proc_maps_private *priv = seq->private;\n\n\tif (priv->mm)\n\t\tmmdrop(priv->mm);\n\n\treturn seq_release_private(inode, file);\n}\n\nstatic int do_maps_open(struct inode *inode, struct file *file,\n\t\t\tconst struct seq_operations *ops)\n{\n\treturn proc_maps_open(inode, file, ops,\n\t\t\t\tsizeof(struct proc_maps_private));\n}\n\n/*\n * Indicate if the VMA is a stack for the given task; for\n * /proc/PID/maps that is the stack of the main task.\n */\nstatic int is_stack(struct vm_area_struct *vma)\n{\n\t/*\n\t * We make no effort to guess what a given thread considers to be\n\t * its \"stack\".  It's not even well-defined for programs written\n\t * languages like Go.\n\t */\n\treturn vma->vm_start <= vma->vm_mm->start_stack &&\n\t\tvma->vm_end >= vma->vm_mm->start_stack;\n}\n\nstatic void show_vma_header_prefix(struct seq_file *m,\n\t\t\t\t   unsigned long start, unsigned long end,\n\t\t\t\t   vm_flags_t flags, unsigned long long pgoff,\n\t\t\t\t   dev_t dev, unsigned long ino)\n{\n\tseq_setwidth(m, 25 + sizeof(void *) * 6 - 1);\n\tseq_put_hex_ll(m, NULL, start, 8);\n\tseq_put_hex_ll(m, \"-\", end, 8);\n\tseq_putc(m, ' ');\n\tseq_putc(m, flags & VM_READ ? 'r' : '-');\n\tseq_putc(m, flags & VM_WRITE ? 'w' : '-');\n\tseq_putc(m, flags & VM_EXEC ? 'x' : '-');\n\tseq_putc(m, flags & VM_MAYSHARE ? 's' : 'p');\n\tseq_put_hex_ll(m, \" \", pgoff, 8);\n\tseq_put_hex_ll(m, \" \", MAJOR(dev), 2);\n\tseq_put_hex_ll(m, \":\", MINOR(dev), 2);\n\tseq_put_decimal_ull(m, \" \", ino);\n\tseq_putc(m, ' ');\n}\n\nstatic void\nshow_map_vma(struct seq_file *m, struct vm_area_struct *vma)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct file *file = vma->vm_file;\n\tvm_flags_t flags = vma->vm_flags;\n\tunsigned long ino = 0;\n\tunsigned long long pgoff = 0;\n\tunsigned long start, end;\n\tdev_t dev = 0;\n\tconst char *name = NULL;\n\n\tif (file) {\n\t\tstruct inode *inode = file_inode(vma->vm_file);\n\t\tdev = inode->i_sb->s_dev;\n\t\tino = inode->i_ino;\n\t\tpgoff = ((loff_t)vma->vm_pgoff) << PAGE_SHIFT;\n\t}\n\n\tstart = vma->vm_start;\n\tend = vma->vm_end;\n\tshow_vma_header_prefix(m, start, end, flags, pgoff, dev, ino);\n\n\t/*\n\t * Print the dentry name for named mappings, and a\n\t * special [heap] marker for the heap:\n\t */\n\tif (file) {\n\t\tseq_pad(m, ' ');\n\t\tseq_file_path(m, file, \"\\n\");\n\t\tgoto done;\n\t}\n\n\tif (vma->vm_ops && vma->vm_ops->name) {\n\t\tname = vma->vm_ops->name(vma);\n\t\tif (name)\n\t\t\tgoto done;\n\t}\n\n\tname = arch_vma_name(vma);\n\tif (!name) {\n\t\tif (!mm) {\n\t\t\tname = \"[vdso]\";\n\t\t\tgoto done;\n\t\t}\n\n\t\tif (vma->vm_start <= mm->brk &&\n\t\t    vma->vm_end >= mm->start_brk) {\n\t\t\tname = \"[heap]\";\n\t\t\tgoto done;\n\t\t}\n\n\t\tif (is_stack(vma))\n\t\t\tname = \"[stack]\";\n\t}\n\ndone:\n\tif (name) {\n\t\tseq_pad(m, ' ');\n\t\tseq_puts(m, name);\n\t}\n\tseq_putc(m, '\\n');\n}\n\nstatic int show_map(struct seq_file *m, void *v)\n{\n\tshow_map_vma(m, v);\n\tm_cache_vma(m, v);\n\treturn 0;\n}\n\nstatic const struct seq_operations proc_pid_maps_op = {\n\t.start\t= m_start,\n\t.next\t= m_next,\n\t.stop\t= m_stop,\n\t.show\t= show_map\n};\n\nstatic int pid_maps_open(struct inode *inode, struct file *file)\n{\n\treturn do_maps_open(inode, file, &proc_pid_maps_op);\n}\n\nconst struct file_operations proc_pid_maps_operations = {\n\t.open\t\t= pid_maps_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= proc_map_release,\n};\n\n/*\n * Proportional Set Size(PSS): my share of RSS.\n *\n * PSS of a process is the count of pages it has in memory, where each\n * page is divided by the number of processes sharing it.  So if a\n * process has 1000 pages all to itself, and 1000 shared with one other\n * process, its PSS will be 1500.\n *\n * To keep (accumulated) division errors low, we adopt a 64bit\n * fixed-point pss counter to minimize division errors. So (pss >>\n * PSS_SHIFT) would be the real byte count.\n *\n * A shift of 12 before division means (assuming 4K page size):\n * \t- 1M 3-user-pages add up to 8KB errors;\n * \t- supports mapcount up to 2^24, or 16M;\n * \t- supports PSS up to 2^52 bytes, or 4PB.\n */\n#define PSS_SHIFT 12\n\n#ifdef CONFIG_PROC_PAGE_MONITOR\nstruct mem_size_stats {\n\tunsigned long resident;\n\tunsigned long shared_clean;\n\tunsigned long shared_dirty;\n\tunsigned long private_clean;\n\tunsigned long private_dirty;\n\tunsigned long referenced;\n\tunsigned long anonymous;\n\tunsigned long lazyfree;\n\tunsigned long anonymous_thp;\n\tunsigned long shmem_thp;\n\tunsigned long swap;\n\tunsigned long shared_hugetlb;\n\tunsigned long private_hugetlb;\n\tu64 pss;\n\tu64 pss_locked;\n\tu64 swap_pss;\n\tbool check_shmem_swap;\n};\n\nstatic void smaps_account(struct mem_size_stats *mss, struct page *page,\n\t\tbool compound, bool young, bool dirty, bool locked)\n{\n\tint i, nr = compound ? 1 << compound_order(page) : 1;\n\tunsigned long size = nr * PAGE_SIZE;\n\n\tif (PageAnon(page)) {\n\t\tmss->anonymous += size;\n\t\tif (!PageSwapBacked(page) && !dirty && !PageDirty(page))\n\t\t\tmss->lazyfree += size;\n\t}\n\n\tmss->resident += size;\n\t/* Accumulate the size in pages that have been accessed. */\n\tif (young || page_is_young(page) || PageReferenced(page))\n\t\tmss->referenced += size;\n\n\t/*\n\t * page_count(page) == 1 guarantees the page is mapped exactly once.\n\t * If any subpage of the compound page mapped with PTE it would elevate\n\t * page_count().\n\t */\n\tif (page_count(page) == 1) {\n\t\tif (dirty || PageDirty(page))\n\t\t\tmss->private_dirty += size;\n\t\telse\n\t\t\tmss->private_clean += size;\n\t\tmss->pss += (u64)size << PSS_SHIFT;\n\t\tif (locked)\n\t\t\tmss->pss_locked += (u64)size << PSS_SHIFT;\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < nr; i++, page++) {\n\t\tint mapcount = page_mapcount(page);\n\t\tunsigned long pss = (PAGE_SIZE << PSS_SHIFT);\n\n\t\tif (mapcount >= 2) {\n\t\t\tif (dirty || PageDirty(page))\n\t\t\t\tmss->shared_dirty += PAGE_SIZE;\n\t\t\telse\n\t\t\t\tmss->shared_clean += PAGE_SIZE;\n\t\t\tmss->pss += pss / mapcount;\n\t\t\tif (locked)\n\t\t\t\tmss->pss_locked += pss / mapcount;\n\t\t} else {\n\t\t\tif (dirty || PageDirty(page))\n\t\t\t\tmss->private_dirty += PAGE_SIZE;\n\t\t\telse\n\t\t\t\tmss->private_clean += PAGE_SIZE;\n\t\t\tmss->pss += pss;\n\t\t\tif (locked)\n\t\t\t\tmss->pss_locked += pss;\n\t\t}\n\t}\n}\n\n#ifdef CONFIG_SHMEM\nstatic int smaps_pte_hole(unsigned long addr, unsigned long end,\n\t\tstruct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\n\tmss->swap += shmem_partial_swap_usage(\n\t\t\twalk->vma->vm_file->f_mapping, addr, end);\n\n\treturn 0;\n}\n#endif\n\nstatic void smaps_pte_entry(pte_t *pte, unsigned long addr,\n\t\tstruct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tbool locked = !!(vma->vm_flags & VM_LOCKED);\n\tstruct page *page = NULL;\n\n\tif (pte_present(*pte)) {\n\t\tpage = vm_normal_page(vma, addr, *pte);\n\t} else if (is_swap_pte(*pte)) {\n\t\tswp_entry_t swpent = pte_to_swp_entry(*pte);\n\n\t\tif (!non_swap_entry(swpent)) {\n\t\t\tint mapcount;\n\n\t\t\tmss->swap += PAGE_SIZE;\n\t\t\tmapcount = swp_swapcount(swpent);\n\t\t\tif (mapcount >= 2) {\n\t\t\t\tu64 pss_delta = (u64)PAGE_SIZE << PSS_SHIFT;\n\n\t\t\t\tdo_div(pss_delta, mapcount);\n\t\t\t\tmss->swap_pss += pss_delta;\n\t\t\t} else {\n\t\t\t\tmss->swap_pss += (u64)PAGE_SIZE << PSS_SHIFT;\n\t\t\t}\n\t\t} else if (is_migration_entry(swpent))\n\t\t\tpage = migration_entry_to_page(swpent);\n\t\telse if (is_device_private_entry(swpent))\n\t\t\tpage = device_private_entry_to_page(swpent);\n\t} else if (unlikely(IS_ENABLED(CONFIG_SHMEM) && mss->check_shmem_swap\n\t\t\t\t\t\t\t&& pte_none(*pte))) {\n\t\tpage = find_get_entry(vma->vm_file->f_mapping,\n\t\t\t\t\t\tlinear_page_index(vma, addr));\n\t\tif (!page)\n\t\t\treturn;\n\n\t\tif (xa_is_value(page))\n\t\t\tmss->swap += PAGE_SIZE;\n\t\telse\n\t\t\tput_page(page);\n\n\t\treturn;\n\t}\n\n\tif (!page)\n\t\treturn;\n\n\tsmaps_account(mss, page, false, pte_young(*pte), pte_dirty(*pte), locked);\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic void smaps_pmd_entry(pmd_t *pmd, unsigned long addr,\n\t\tstruct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tbool locked = !!(vma->vm_flags & VM_LOCKED);\n\tstruct page *page;\n\n\t/* FOLL_DUMP will return -EFAULT on huge zero page */\n\tpage = follow_trans_huge_pmd(vma, addr, pmd, FOLL_DUMP);\n\tif (IS_ERR_OR_NULL(page))\n\t\treturn;\n\tif (PageAnon(page))\n\t\tmss->anonymous_thp += HPAGE_PMD_SIZE;\n\telse if (PageSwapBacked(page))\n\t\tmss->shmem_thp += HPAGE_PMD_SIZE;\n\telse if (is_zone_device_page(page))\n\t\t/* pass */;\n\telse\n\t\tVM_BUG_ON_PAGE(1, page);\n\tsmaps_account(mss, page, true, pmd_young(*pmd), pmd_dirty(*pmd), locked);\n}\n#else\nstatic void smaps_pmd_entry(pmd_t *pmd, unsigned long addr,\n\t\tstruct mm_walk *walk)\n{\n}\n#endif\n\nstatic int smaps_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,\n\t\t\t   struct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->vma;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\tptl = pmd_trans_huge_lock(pmd, vma);\n\tif (ptl) {\n\t\tif (pmd_present(*pmd))\n\t\t\tsmaps_pmd_entry(pmd, addr, walk);\n\t\tspin_unlock(ptl);\n\t\tgoto out;\n\t}\n\n\tif (pmd_trans_unstable(pmd))\n\t\tgoto out;\n\t/*\n\t * The mmap_sem held all the way back in m_start() is what\n\t * keeps khugepaged out of here and from collapsing things\n\t * in here.\n\t */\n\tpte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\n\tfor (; addr != end; pte++, addr += PAGE_SIZE)\n\t\tsmaps_pte_entry(pte, addr, walk);\n\tpte_unmap_unlock(pte - 1, ptl);\nout:\n\tcond_resched();\n\treturn 0;\n}\n\nstatic void show_smap_vma_flags(struct seq_file *m, struct vm_area_struct *vma)\n{\n\t/*\n\t * Don't forget to update Documentation/ on changes.\n\t */\n\tstatic const char mnemonics[BITS_PER_LONG][2] = {\n\t\t/*\n\t\t * In case if we meet a flag we don't know about.\n\t\t */\n\t\t[0 ... (BITS_PER_LONG-1)] = \"??\",\n\n\t\t[ilog2(VM_READ)]\t= \"rd\",\n\t\t[ilog2(VM_WRITE)]\t= \"wr\",\n\t\t[ilog2(VM_EXEC)]\t= \"ex\",\n\t\t[ilog2(VM_SHARED)]\t= \"sh\",\n\t\t[ilog2(VM_MAYREAD)]\t= \"mr\",\n\t\t[ilog2(VM_MAYWRITE)]\t= \"mw\",\n\t\t[ilog2(VM_MAYEXEC)]\t= \"me\",\n\t\t[ilog2(VM_MAYSHARE)]\t= \"ms\",\n\t\t[ilog2(VM_GROWSDOWN)]\t= \"gd\",\n\t\t[ilog2(VM_PFNMAP)]\t= \"pf\",\n\t\t[ilog2(VM_DENYWRITE)]\t= \"dw\",\n#ifdef CONFIG_X86_INTEL_MPX\n\t\t[ilog2(VM_MPX)]\t\t= \"mp\",\n#endif\n\t\t[ilog2(VM_LOCKED)]\t= \"lo\",\n\t\t[ilog2(VM_IO)]\t\t= \"io\",\n\t\t[ilog2(VM_SEQ_READ)]\t= \"sr\",\n\t\t[ilog2(VM_RAND_READ)]\t= \"rr\",\n\t\t[ilog2(VM_DONTCOPY)]\t= \"dc\",\n\t\t[ilog2(VM_DONTEXPAND)]\t= \"de\",\n\t\t[ilog2(VM_ACCOUNT)]\t= \"ac\",\n\t\t[ilog2(VM_NORESERVE)]\t= \"nr\",\n\t\t[ilog2(VM_HUGETLB)]\t= \"ht\",\n\t\t[ilog2(VM_SYNC)]\t= \"sf\",\n\t\t[ilog2(VM_ARCH_1)]\t= \"ar\",\n\t\t[ilog2(VM_WIPEONFORK)]\t= \"wf\",\n\t\t[ilog2(VM_DONTDUMP)]\t= \"dd\",\n#ifdef CONFIG_MEM_SOFT_DIRTY\n\t\t[ilog2(VM_SOFTDIRTY)]\t= \"sd\",\n#endif\n\t\t[ilog2(VM_MIXEDMAP)]\t= \"mm\",\n\t\t[ilog2(VM_HUGEPAGE)]\t= \"hg\",\n\t\t[ilog2(VM_NOHUGEPAGE)]\t= \"nh\",\n\t\t[ilog2(VM_MERGEABLE)]\t= \"mg\",\n\t\t[ilog2(VM_UFFD_MISSING)]= \"um\",\n\t\t[ilog2(VM_UFFD_WP)]\t= \"uw\",\n#ifdef CONFIG_ARCH_HAS_PKEYS\n\t\t/* These come out via ProtectionKey: */\n\t\t[ilog2(VM_PKEY_BIT0)]\t= \"\",\n\t\t[ilog2(VM_PKEY_BIT1)]\t= \"\",\n\t\t[ilog2(VM_PKEY_BIT2)]\t= \"\",\n\t\t[ilog2(VM_PKEY_BIT3)]\t= \"\",\n#if VM_PKEY_BIT4\n\t\t[ilog2(VM_PKEY_BIT4)]\t= \"\",\n#endif\n#endif /* CONFIG_ARCH_HAS_PKEYS */\n\t};\n\tsize_t i;\n\n\tseq_puts(m, \"VmFlags: \");\n\tfor (i = 0; i < BITS_PER_LONG; i++) {\n\t\tif (!mnemonics[i][0])\n\t\t\tcontinue;\n\t\tif (vma->vm_flags & (1UL << i)) {\n\t\t\tseq_putc(m, mnemonics[i][0]);\n\t\t\tseq_putc(m, mnemonics[i][1]);\n\t\t\tseq_putc(m, ' ');\n\t\t}\n\t}\n\tseq_putc(m, '\\n');\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\nstatic int smaps_hugetlb_range(pte_t *pte, unsigned long hmask,\n\t\t\t\t unsigned long addr, unsigned long end,\n\t\t\t\t struct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tstruct page *page = NULL;\n\n\tif (pte_present(*pte)) {\n\t\tpage = vm_normal_page(vma, addr, *pte);\n\t} else if (is_swap_pte(*pte)) {\n\t\tswp_entry_t swpent = pte_to_swp_entry(*pte);\n\n\t\tif (is_migration_entry(swpent))\n\t\t\tpage = migration_entry_to_page(swpent);\n\t\telse if (is_device_private_entry(swpent))\n\t\t\tpage = device_private_entry_to_page(swpent);\n\t}\n\tif (page) {\n\t\tint mapcount = page_mapcount(page);\n\n\t\tif (mapcount >= 2)\n\t\t\tmss->shared_hugetlb += huge_page_size(hstate_vma(vma));\n\t\telse\n\t\t\tmss->private_hugetlb += huge_page_size(hstate_vma(vma));\n\t}\n\treturn 0;\n}\n#endif /* HUGETLB_PAGE */\n\nstatic void smap_gather_stats(struct vm_area_struct *vma,\n\t\t\t     struct mem_size_stats *mss)\n{\n\tstruct mm_walk smaps_walk = {\n\t\t.pmd_entry = smaps_pte_range,\n#ifdef CONFIG_HUGETLB_PAGE\n\t\t.hugetlb_entry = smaps_hugetlb_range,\n#endif\n\t\t.mm = vma->vm_mm,\n\t};\n\n\tsmaps_walk.private = mss;\n\n#ifdef CONFIG_SHMEM\n\t/* In case of smaps_rollup, reset the value from previous vma */\n\tmss->check_shmem_swap = false;\n\tif (vma->vm_file && shmem_mapping(vma->vm_file->f_mapping)) {\n\t\t/*\n\t\t * For shared or readonly shmem mappings we know that all\n\t\t * swapped out pages belong to the shmem object, and we can\n\t\t * obtain the swap value much more efficiently. For private\n\t\t * writable mappings, we might have COW pages that are\n\t\t * not affected by the parent swapped out pages of the shmem\n\t\t * object, so we have to distinguish them during the page walk.\n\t\t * Unless we know that the shmem object (or the part mapped by\n\t\t * our VMA) has no swapped out pages at all.\n\t\t */\n\t\tunsigned long shmem_swapped = shmem_swap_usage(vma);\n\n\t\tif (!shmem_swapped || (vma->vm_flags & VM_SHARED) ||\n\t\t\t\t\t!(vma->vm_flags & VM_WRITE)) {\n\t\t\tmss->swap += shmem_swapped;\n\t\t} else {\n\t\t\tmss->check_shmem_swap = true;\n\t\t\tsmaps_walk.pte_hole = smaps_pte_hole;\n\t\t}\n\t}\n#endif\n\t/* mmap_sem is held in m_start */\n\twalk_page_vma(vma, &smaps_walk);\n}\n\n#define SEQ_PUT_DEC(str, val) \\\n\t\tseq_put_decimal_ull_width(m, str, (val) >> 10, 8)\n\n/* Show the contents common for smaps and smaps_rollup */\nstatic void __show_smap(struct seq_file *m, const struct mem_size_stats *mss)\n{\n\tSEQ_PUT_DEC(\"Rss:            \", mss->resident);\n\tSEQ_PUT_DEC(\" kB\\nPss:            \", mss->pss >> PSS_SHIFT);\n\tSEQ_PUT_DEC(\" kB\\nShared_Clean:   \", mss->shared_clean);\n\tSEQ_PUT_DEC(\" kB\\nShared_Dirty:   \", mss->shared_dirty);\n\tSEQ_PUT_DEC(\" kB\\nPrivate_Clean:  \", mss->private_clean);\n\tSEQ_PUT_DEC(\" kB\\nPrivate_Dirty:  \", mss->private_dirty);\n\tSEQ_PUT_DEC(\" kB\\nReferenced:     \", mss->referenced);\n\tSEQ_PUT_DEC(\" kB\\nAnonymous:      \", mss->anonymous);\n\tSEQ_PUT_DEC(\" kB\\nLazyFree:       \", mss->lazyfree);\n\tSEQ_PUT_DEC(\" kB\\nAnonHugePages:  \", mss->anonymous_thp);\n\tSEQ_PUT_DEC(\" kB\\nShmemPmdMapped: \", mss->shmem_thp);\n\tSEQ_PUT_DEC(\" kB\\nShared_Hugetlb: \", mss->shared_hugetlb);\n\tseq_put_decimal_ull_width(m, \" kB\\nPrivate_Hugetlb: \",\n\t\t\t\t  mss->private_hugetlb >> 10, 7);\n\tSEQ_PUT_DEC(\" kB\\nSwap:           \", mss->swap);\n\tSEQ_PUT_DEC(\" kB\\nSwapPss:        \",\n\t\t\t\t\tmss->swap_pss >> PSS_SHIFT);\n\tSEQ_PUT_DEC(\" kB\\nLocked:         \",\n\t\t\t\t\tmss->pss_locked >> PSS_SHIFT);\n\tseq_puts(m, \" kB\\n\");\n}\n\nstatic int show_smap(struct seq_file *m, void *v)\n{\n\tstruct vm_area_struct *vma = v;\n\tstruct mem_size_stats mss;\n\n\tmemset(&mss, 0, sizeof(mss));\n\n\tsmap_gather_stats(vma, &mss);\n\n\tshow_map_vma(m, vma);\n\n\tSEQ_PUT_DEC(\"Size:           \", vma->vm_end - vma->vm_start);\n\tSEQ_PUT_DEC(\" kB\\nKernelPageSize: \", vma_kernel_pagesize(vma));\n\tSEQ_PUT_DEC(\" kB\\nMMUPageSize:    \", vma_mmu_pagesize(vma));\n\tseq_puts(m, \" kB\\n\");\n\n\t__show_smap(m, &mss);\n\n\tseq_printf(m, \"THPeligible:    %d\\n\", transparent_hugepage_enabled(vma));\n\n\tif (arch_pkeys_enabled())\n\t\tseq_printf(m, \"ProtectionKey:  %8u\\n\", vma_pkey(vma));\n\tshow_smap_vma_flags(m, vma);\n\n\tm_cache_vma(m, vma);\n\n\treturn 0;\n}\n\nstatic int show_smaps_rollup(struct seq_file *m, void *v)\n{\n\tstruct proc_maps_private *priv = m->private;\n\tstruct mem_size_stats mss;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long last_vma_end = 0;\n\tint ret = 0;\n\n\tpriv->task = get_proc_task(priv->inode);\n\tif (!priv->task)\n\t\treturn -ESRCH;\n\n\tmm = priv->mm;\n\tif (!mm || !mmget_not_zero(mm)) {\n\t\tret = -ESRCH;\n\t\tgoto out_put_task;\n\t}\n\n\tmemset(&mss, 0, sizeof(mss));\n\n\tdown_read(&mm->mmap_sem);\n\thold_task_mempolicy(priv);\n\n\tfor (vma = priv->mm->mmap; vma; vma = vma->vm_next) {\n\t\tsmap_gather_stats(vma, &mss);\n\t\tlast_vma_end = vma->vm_end;\n\t}\n\n\tshow_vma_header_prefix(m, priv->mm->mmap->vm_start,\n\t\t\t       last_vma_end, 0, 0, 0, 0);\n\tseq_pad(m, ' ');\n\tseq_puts(m, \"[rollup]\\n\");\n\n\t__show_smap(m, &mss);\n\n\trelease_task_mempolicy(priv);\n\tup_read(&mm->mmap_sem);\n\tmmput(mm);\n\nout_put_task:\n\tput_task_struct(priv->task);\n\tpriv->task = NULL;\n\n\treturn ret;\n}\n#undef SEQ_PUT_DEC\n\nstatic const struct seq_operations proc_pid_smaps_op = {\n\t.start\t= m_start,\n\t.next\t= m_next,\n\t.stop\t= m_stop,\n\t.show\t= show_smap\n};\n\nstatic int pid_smaps_open(struct inode *inode, struct file *file)\n{\n\treturn do_maps_open(inode, file, &proc_pid_smaps_op);\n}\n\nstatic int smaps_rollup_open(struct inode *inode, struct file *file)\n{\n\tint ret;\n\tstruct proc_maps_private *priv;\n\n\tpriv = kzalloc(sizeof(*priv), GFP_KERNEL_ACCOUNT);\n\tif (!priv)\n\t\treturn -ENOMEM;\n\n\tret = single_open(file, show_smaps_rollup, priv);\n\tif (ret)\n\t\tgoto out_free;\n\n\tpriv->inode = inode;\n\tpriv->mm = proc_mem_open(inode, PTRACE_MODE_READ);\n\tif (IS_ERR(priv->mm)) {\n\t\tret = PTR_ERR(priv->mm);\n\n\t\tsingle_release(inode, file);\n\t\tgoto out_free;\n\t}\n\n\treturn 0;\n\nout_free:\n\tkfree(priv);\n\treturn ret;\n}\n\nstatic int smaps_rollup_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *seq = file->private_data;\n\tstruct proc_maps_private *priv = seq->private;\n\n\tif (priv->mm)\n\t\tmmdrop(priv->mm);\n\n\tkfree(priv);\n\treturn single_release(inode, file);\n}\n\nconst struct file_operations proc_pid_smaps_operations = {\n\t.open\t\t= pid_smaps_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= proc_map_release,\n};\n\nconst struct file_operations proc_pid_smaps_rollup_operations = {\n\t.open\t\t= smaps_rollup_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= smaps_rollup_release,\n};\n\nenum clear_refs_types {\n\tCLEAR_REFS_ALL = 1,\n\tCLEAR_REFS_ANON,\n\tCLEAR_REFS_MAPPED,\n\tCLEAR_REFS_SOFT_DIRTY,\n\tCLEAR_REFS_MM_HIWATER_RSS,\n\tCLEAR_REFS_LAST,\n};\n\nstruct clear_refs_private {\n\tenum clear_refs_types type;\n};\n\n#ifdef CONFIG_MEM_SOFT_DIRTY\nstatic inline void clear_soft_dirty(struct vm_area_struct *vma,\n\t\tunsigned long addr, pte_t *pte)\n{\n\t/*\n\t * The soft-dirty tracker uses #PF-s to catch writes\n\t * to pages, so write-protect the pte as well. See the\n\t * Documentation/admin-guide/mm/soft-dirty.rst for full description\n\t * of how soft-dirty works.\n\t */\n\tpte_t ptent = *pte;\n\n\tif (pte_present(ptent)) {\n\t\tpte_t old_pte;\n\n\t\told_pte = ptep_modify_prot_start(vma, addr, pte);\n\t\tptent = pte_wrprotect(old_pte);\n\t\tptent = pte_clear_soft_dirty(ptent);\n\t\tptep_modify_prot_commit(vma, addr, pte, old_pte, ptent);\n\t} else if (is_swap_pte(ptent)) {\n\t\tptent = pte_swp_clear_soft_dirty(ptent);\n\t\tset_pte_at(vma->vm_mm, addr, pte, ptent);\n\t}\n}\n#else\nstatic inline void clear_soft_dirty(struct vm_area_struct *vma,\n\t\tunsigned long addr, pte_t *pte)\n{\n}\n#endif\n\n#if defined(CONFIG_MEM_SOFT_DIRTY) && defined(CONFIG_TRANSPARENT_HUGEPAGE)\nstatic inline void clear_soft_dirty_pmd(struct vm_area_struct *vma,\n\t\tunsigned long addr, pmd_t *pmdp)\n{\n\tpmd_t old, pmd = *pmdp;\n\n\tif (pmd_present(pmd)) {\n\t\t/* See comment in change_huge_pmd() */\n\t\told = pmdp_invalidate(vma, addr, pmdp);\n\t\tif (pmd_dirty(old))\n\t\t\tpmd = pmd_mkdirty(pmd);\n\t\tif (pmd_young(old))\n\t\t\tpmd = pmd_mkyoung(pmd);\n\n\t\tpmd = pmd_wrprotect(pmd);\n\t\tpmd = pmd_clear_soft_dirty(pmd);\n\n\t\tset_pmd_at(vma->vm_mm, addr, pmdp, pmd);\n\t} else if (is_migration_entry(pmd_to_swp_entry(pmd))) {\n\t\tpmd = pmd_swp_clear_soft_dirty(pmd);\n\t\tset_pmd_at(vma->vm_mm, addr, pmdp, pmd);\n\t}\n}\n#else\nstatic inline void clear_soft_dirty_pmd(struct vm_area_struct *vma,\n\t\tunsigned long addr, pmd_t *pmdp)\n{\n}\n#endif\n\nstatic int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,\n\t\t\t\tunsigned long end, struct mm_walk *walk)\n{\n\tstruct clear_refs_private *cp = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tpte_t *pte, ptent;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\n\tptl = pmd_trans_huge_lock(pmd, vma);\n\tif (ptl) {\n\t\tif (cp->type == CLEAR_REFS_SOFT_DIRTY) {\n\t\t\tclear_soft_dirty_pmd(vma, addr, pmd);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto out;\n\n\t\tpage = pmd_page(*pmd);\n\n\t\t/* Clear accessed and referenced bits. */\n\t\tpmdp_test_and_clear_young(vma, addr, pmd);\n\t\ttest_and_clear_page_young(page);\n\t\tClearPageReferenced(page);\nout:\n\t\tspin_unlock(ptl);\n\t\treturn 0;\n\t}\n\n\tif (pmd_trans_unstable(pmd))\n\t\treturn 0;\n\n\tpte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\n\tfor (; addr != end; pte++, addr += PAGE_SIZE) {\n\t\tptent = *pte;\n\n\t\tif (cp->type == CLEAR_REFS_SOFT_DIRTY) {\n\t\t\tclear_soft_dirty(vma, addr, pte);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!pte_present(ptent))\n\t\t\tcontinue;\n\n\t\tpage = vm_normal_page(vma, addr, ptent);\n\t\tif (!page)\n\t\t\tcontinue;\n\n\t\t/* Clear accessed and referenced bits. */\n\t\tptep_test_and_clear_young(vma, addr, pte);\n\t\ttest_and_clear_page_young(page);\n\t\tClearPageReferenced(page);\n\t}\n\tpte_unmap_unlock(pte - 1, ptl);\n\tcond_resched();\n\treturn 0;\n}\n\nstatic int clear_refs_test_walk(unsigned long start, unsigned long end,\n\t\t\t\tstruct mm_walk *walk)\n{\n\tstruct clear_refs_private *cp = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\n\tif (vma->vm_flags & VM_PFNMAP)\n\t\treturn 1;\n\n\t/*\n\t * Writing 1 to /proc/pid/clear_refs affects all pages.\n\t * Writing 2 to /proc/pid/clear_refs only affects anonymous pages.\n\t * Writing 3 to /proc/pid/clear_refs only affects file mapped pages.\n\t * Writing 4 to /proc/pid/clear_refs affects all pages.\n\t */\n\tif (cp->type == CLEAR_REFS_ANON && vma->vm_file)\n\t\treturn 1;\n\tif (cp->type == CLEAR_REFS_MAPPED && !vma->vm_file)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic ssize_t clear_refs_write(struct file *file, const char __user *buf,\n\t\t\t\tsize_t count, loff_t *ppos)\n{\n\tstruct task_struct *task;\n\tchar buffer[PROC_NUMBUF];\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tenum clear_refs_types type;\n\tstruct mmu_gather tlb;\n\tint itype;\n\tint rv;\n\n\tmemset(buffer, 0, sizeof(buffer));\n\tif (count > sizeof(buffer) - 1)\n\t\tcount = sizeof(buffer) - 1;\n\tif (copy_from_user(buffer, buf, count))\n\t\treturn -EFAULT;\n\trv = kstrtoint(strstrip(buffer), 10, &itype);\n\tif (rv < 0)\n\t\treturn rv;\n\ttype = (enum clear_refs_types)itype;\n\tif (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)\n\t\treturn -EINVAL;\n\n\ttask = get_proc_task(file_inode(file));\n\tif (!task)\n\t\treturn -ESRCH;\n\tmm = get_task_mm(task);\n\tif (mm) {\n\t\tstruct mmu_notifier_range range;\n\t\tstruct clear_refs_private cp = {\n\t\t\t.type = type,\n\t\t};\n\t\tstruct mm_walk clear_refs_walk = {\n\t\t\t.pmd_entry = clear_refs_pte_range,\n\t\t\t.test_walk = clear_refs_test_walk,\n\t\t\t.mm = mm,\n\t\t\t.private = &cp,\n\t\t};\n\n\t\tif (type == CLEAR_REFS_MM_HIWATER_RSS) {\n\t\t\tif (down_write_killable(&mm->mmap_sem)) {\n\t\t\t\tcount = -EINTR;\n\t\t\t\tgoto out_mm;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Writing 5 to /proc/pid/clear_refs resets the peak\n\t\t\t * resident set size to this mm's current rss value.\n\t\t\t */\n\t\t\treset_mm_hiwater_rss(mm);\n\t\t\tup_write(&mm->mmap_sem);\n\t\t\tgoto out_mm;\n\t\t}\n\n\t\tdown_read(&mm->mmap_sem);\n\t\ttlb_gather_mmu(&tlb, mm, 0, -1);\n\t\tif (type == CLEAR_REFS_SOFT_DIRTY) {\n\t\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\t\t\tif (!(vma->vm_flags & VM_SOFTDIRTY))\n\t\t\t\t\tcontinue;\n\t\t\t\tup_read(&mm->mmap_sem);\n\t\t\t\tif (down_write_killable(&mm->mmap_sem)) {\n\t\t\t\t\tcount = -EINTR;\n\t\t\t\t\tgoto out_mm;\n\t\t\t\t}\n\t\t\t\t/*\n\t\t\t\t * Avoid to modify vma->vm_flags\n\t\t\t\t * without locked ops while the\n\t\t\t\t * coredump reads the vm_flags.\n\t\t\t\t */\n\t\t\t\tif (!mmget_still_valid(mm)) {\n\t\t\t\t\t/*\n\t\t\t\t\t * Silently return \"count\"\n\t\t\t\t\t * like if get_task_mm()\n\t\t\t\t\t * failed. FIXME: should this\n\t\t\t\t\t * function have returned\n\t\t\t\t\t * -ESRCH if get_task_mm()\n\t\t\t\t\t * failed like if\n\t\t\t\t\t * get_proc_task() fails?\n\t\t\t\t\t */\n\t\t\t\t\tup_write(&mm->mmap_sem);\n\t\t\t\t\tgoto out_mm;\n\t\t\t\t}\n\t\t\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\t\t\t\tvma->vm_flags &= ~VM_SOFTDIRTY;\n\t\t\t\t\tvma_set_page_prot(vma);\n\t\t\t\t}\n\t\t\t\tdowngrade_write(&mm->mmap_sem);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tmmu_notifier_range_init(&range, mm, 0, -1UL);\n\t\t\tmmu_notifier_invalidate_range_start(&range);\n\t\t}\n\t\twalk_page_range(0, mm->highest_vm_end, &clear_refs_walk);\n\t\tif (type == CLEAR_REFS_SOFT_DIRTY)\n\t\t\tmmu_notifier_invalidate_range_end(&range);\n\t\ttlb_finish_mmu(&tlb, 0, -1);\n\t\tup_read(&mm->mmap_sem);\nout_mm:\n\t\tmmput(mm);\n\t}\n\tput_task_struct(task);\n\n\treturn count;\n}\n\nconst struct file_operations proc_clear_refs_operations = {\n\t.write\t\t= clear_refs_write,\n\t.llseek\t\t= noop_llseek,\n};\n\ntypedef struct {\n\tu64 pme;\n} pagemap_entry_t;\n\nstruct pagemapread {\n\tint pos, len;\t\t/* units: PM_ENTRY_BYTES, not bytes */\n\tpagemap_entry_t *buffer;\n\tbool show_pfn;\n};\n\n#define PAGEMAP_WALK_SIZE\t(PMD_SIZE)\n#define PAGEMAP_WALK_MASK\t(PMD_MASK)\n\n#define PM_ENTRY_BYTES\t\tsizeof(pagemap_entry_t)\n#define PM_PFRAME_BITS\t\t55\n#define PM_PFRAME_MASK\t\tGENMASK_ULL(PM_PFRAME_BITS - 1, 0)\n#define PM_SOFT_DIRTY\t\tBIT_ULL(55)\n#define PM_MMAP_EXCLUSIVE\tBIT_ULL(56)\n#define PM_FILE\t\t\tBIT_ULL(61)\n#define PM_SWAP\t\t\tBIT_ULL(62)\n#define PM_PRESENT\t\tBIT_ULL(63)\n\n#define PM_END_OF_BUFFER    1\n\nstatic inline pagemap_entry_t make_pme(u64 frame, u64 flags)\n{\n\treturn (pagemap_entry_t) { .pme = (frame & PM_PFRAME_MASK) | flags };\n}\n\nstatic int add_to_pagemap(unsigned long addr, pagemap_entry_t *pme,\n\t\t\t  struct pagemapread *pm)\n{\n\tpm->buffer[pm->pos++] = *pme;\n\tif (pm->pos >= pm->len)\n\t\treturn PM_END_OF_BUFFER;\n\treturn 0;\n}\n\nstatic int pagemap_pte_hole(unsigned long start, unsigned long end,\n\t\t\t\tstruct mm_walk *walk)\n{\n\tstruct pagemapread *pm = walk->private;\n\tunsigned long addr = start;\n\tint err = 0;\n\n\twhile (addr < end) {\n\t\tstruct vm_area_struct *vma = find_vma(walk->mm, addr);\n\t\tpagemap_entry_t pme = make_pme(0, 0);\n\t\t/* End of address space hole, which we mark as non-present. */\n\t\tunsigned long hole_end;\n\n\t\tif (vma)\n\t\t\thole_end = min(end, vma->vm_start);\n\t\telse\n\t\t\thole_end = end;\n\n\t\tfor (; addr < hole_end; addr += PAGE_SIZE) {\n\t\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tif (!vma)\n\t\t\tbreak;\n\n\t\t/* Addresses in the VMA. */\n\t\tif (vma->vm_flags & VM_SOFTDIRTY)\n\t\t\tpme = make_pme(0, PM_SOFT_DIRTY);\n\t\tfor (; addr < min(end, vma->vm_end); addr += PAGE_SIZE) {\n\t\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\treturn err;\n}\n\nstatic pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,\n\t\tstruct vm_area_struct *vma, unsigned long addr, pte_t pte)\n{\n\tu64 frame = 0, flags = 0;\n\tstruct page *page = NULL;\n\n\tif (pte_present(pte)) {\n\t\tif (pm->show_pfn)\n\t\t\tframe = pte_pfn(pte);\n\t\tflags |= PM_PRESENT;\n\t\tpage = _vm_normal_page(vma, addr, pte, true);\n\t\tif (pte_soft_dirty(pte))\n\t\t\tflags |= PM_SOFT_DIRTY;\n\t} else if (is_swap_pte(pte)) {\n\t\tswp_entry_t entry;\n\t\tif (pte_swp_soft_dirty(pte))\n\t\t\tflags |= PM_SOFT_DIRTY;\n\t\tentry = pte_to_swp_entry(pte);\n\t\tif (pm->show_pfn)\n\t\t\tframe = swp_type(entry) |\n\t\t\t\t(swp_offset(entry) << MAX_SWAPFILES_SHIFT);\n\t\tflags |= PM_SWAP;\n\t\tif (is_migration_entry(entry))\n\t\t\tpage = migration_entry_to_page(entry);\n\n\t\tif (is_device_private_entry(entry))\n\t\t\tpage = device_private_entry_to_page(entry);\n\t}\n\n\tif (page && !PageAnon(page))\n\t\tflags |= PM_FILE;\n\tif (page && page_mapcount(page) == 1)\n\t\tflags |= PM_MMAP_EXCLUSIVE;\n\tif (vma->vm_flags & VM_SOFTDIRTY)\n\t\tflags |= PM_SOFT_DIRTY;\n\n\treturn make_pme(frame, flags);\n}\n\nstatic int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,\n\t\t\t     struct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->vma;\n\tstruct pagemapread *pm = walk->private;\n\tspinlock_t *ptl;\n\tpte_t *pte, *orig_pte;\n\tint err = 0;\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tptl = pmd_trans_huge_lock(pmdp, vma);\n\tif (ptl) {\n\t\tu64 flags = 0, frame = 0;\n\t\tpmd_t pmd = *pmdp;\n\t\tstruct page *page = NULL;\n\n\t\tif (vma->vm_flags & VM_SOFTDIRTY)\n\t\t\tflags |= PM_SOFT_DIRTY;\n\n\t\tif (pmd_present(pmd)) {\n\t\t\tpage = pmd_page(pmd);\n\n\t\t\tflags |= PM_PRESENT;\n\t\t\tif (pmd_soft_dirty(pmd))\n\t\t\t\tflags |= PM_SOFT_DIRTY;\n\t\t\tif (pm->show_pfn)\n\t\t\t\tframe = pmd_pfn(pmd) +\n\t\t\t\t\t((addr & ~PMD_MASK) >> PAGE_SHIFT);\n\t\t}\n#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION\n\t\telse if (is_swap_pmd(pmd)) {\n\t\t\tswp_entry_t entry = pmd_to_swp_entry(pmd);\n\t\t\tunsigned long offset;\n\n\t\t\tif (pm->show_pfn) {\n\t\t\t\toffset = swp_offset(entry) +\n\t\t\t\t\t((addr & ~PMD_MASK) >> PAGE_SHIFT);\n\t\t\t\tframe = swp_type(entry) |\n\t\t\t\t\t(offset << MAX_SWAPFILES_SHIFT);\n\t\t\t}\n\t\t\tflags |= PM_SWAP;\n\t\t\tif (pmd_swp_soft_dirty(pmd))\n\t\t\t\tflags |= PM_SOFT_DIRTY;\n\t\t\tVM_BUG_ON(!is_pmd_migration_entry(pmd));\n\t\t\tpage = migration_entry_to_page(entry);\n\t\t}\n#endif\n\n\t\tif (page && page_mapcount(page) == 1)\n\t\t\tflags |= PM_MMAP_EXCLUSIVE;\n\n\t\tfor (; addr != end; addr += PAGE_SIZE) {\n\t\t\tpagemap_entry_t pme = make_pme(frame, flags);\n\n\t\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (pm->show_pfn) {\n\t\t\t\tif (flags & PM_PRESENT)\n\t\t\t\t\tframe++;\n\t\t\t\telse if (flags & PM_SWAP)\n\t\t\t\t\tframe += (1 << MAX_SWAPFILES_SHIFT);\n\t\t\t}\n\t\t}\n\t\tspin_unlock(ptl);\n\t\treturn err;\n\t}\n\n\tif (pmd_trans_unstable(pmdp))\n\t\treturn 0;\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n\n\t/*\n\t * We can assume that @vma always points to a valid one and @end never\n\t * goes beyond vma->vm_end.\n\t */\n\torig_pte = pte = pte_offset_map_lock(walk->mm, pmdp, addr, &ptl);\n\tfor (; addr < end; pte++, addr += PAGE_SIZE) {\n\t\tpagemap_entry_t pme;\n\n\t\tpme = pte_to_pagemap_entry(pm, vma, addr, *pte);\n\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\tpte_unmap_unlock(orig_pte, ptl);\n\n\tcond_resched();\n\n\treturn err;\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\n/* This function walks within one hugetlb entry in the single call */\nstatic int pagemap_hugetlb_range(pte_t *ptep, unsigned long hmask,\n\t\t\t\t unsigned long addr, unsigned long end,\n\t\t\t\t struct mm_walk *walk)\n{\n\tstruct pagemapread *pm = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tu64 flags = 0, frame = 0;\n\tint err = 0;\n\tpte_t pte;\n\n\tif (vma->vm_flags & VM_SOFTDIRTY)\n\t\tflags |= PM_SOFT_DIRTY;\n\n\tpte = huge_ptep_get(ptep);\n\tif (pte_present(pte)) {\n\t\tstruct page *page = pte_page(pte);\n\n\t\tif (!PageAnon(page))\n\t\t\tflags |= PM_FILE;\n\n\t\tif (page_mapcount(page) == 1)\n\t\t\tflags |= PM_MMAP_EXCLUSIVE;\n\n\t\tflags |= PM_PRESENT;\n\t\tif (pm->show_pfn)\n\t\t\tframe = pte_pfn(pte) +\n\t\t\t\t((addr & ~hmask) >> PAGE_SHIFT);\n\t}\n\n\tfor (; addr != end; addr += PAGE_SIZE) {\n\t\tpagemap_entry_t pme = make_pme(frame, flags);\n\n\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (pm->show_pfn && (flags & PM_PRESENT))\n\t\t\tframe++;\n\t}\n\n\tcond_resched();\n\n\treturn err;\n}\n#endif /* HUGETLB_PAGE */\n\n/*\n * /proc/pid/pagemap - an array mapping virtual pages to pfns\n *\n * For each page in the address space, this file contains one 64-bit entry\n * consisting of the following:\n *\n * Bits 0-54  page frame number (PFN) if present\n * Bits 0-4   swap type if swapped\n * Bits 5-54  swap offset if swapped\n * Bit  55    pte is soft-dirty (see Documentation/admin-guide/mm/soft-dirty.rst)\n * Bit  56    page exclusively mapped\n * Bits 57-60 zero\n * Bit  61    page is file-page or shared-anon\n * Bit  62    page swapped\n * Bit  63    page present\n *\n * If the page is not present but in swap, then the PFN contains an\n * encoding of the swap file number and the page's offset into the\n * swap. Unmapped pages return a null PFN. This allows determining\n * precisely which pages are mapped (or in swap) and comparing mapped\n * pages between processes.\n *\n * Efficient users of this interface will use /proc/pid/maps to\n * determine which areas of memory are actually mapped and llseek to\n * skip over unmapped regions.\n */\nstatic ssize_t pagemap_read(struct file *file, char __user *buf,\n\t\t\t    size_t count, loff_t *ppos)\n{\n\tstruct mm_struct *mm = file->private_data;\n\tstruct pagemapread pm;\n\tstruct mm_walk pagemap_walk = {};\n\tunsigned long src;\n\tunsigned long svpfn;\n\tunsigned long start_vaddr;\n\tunsigned long end_vaddr;\n\tint ret = 0, copied = 0;\n\n\tif (!mm || !mmget_not_zero(mm))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\t/* file position must be aligned */\n\tif ((*ppos % PM_ENTRY_BYTES) || (count % PM_ENTRY_BYTES))\n\t\tgoto out_mm;\n\n\tret = 0;\n\tif (!count)\n\t\tgoto out_mm;\n\n\t/* do not disclose physical addresses: attack vector */\n\tpm.show_pfn = file_ns_capable(file, &init_user_ns, CAP_SYS_ADMIN);\n\n\tpm.len = (PAGEMAP_WALK_SIZE >> PAGE_SHIFT);\n\tpm.buffer = kmalloc_array(pm.len, PM_ENTRY_BYTES, GFP_KERNEL);\n\tret = -ENOMEM;\n\tif (!pm.buffer)\n\t\tgoto out_mm;\n\n\tpagemap_walk.pmd_entry = pagemap_pmd_range;\n\tpagemap_walk.pte_hole = pagemap_pte_hole;\n#ifdef CONFIG_HUGETLB_PAGE\n\tpagemap_walk.hugetlb_entry = pagemap_hugetlb_range;\n#endif\n\tpagemap_walk.mm = mm;\n\tpagemap_walk.private = &pm;\n\n\tsrc = *ppos;\n\tsvpfn = src / PM_ENTRY_BYTES;\n\tstart_vaddr = svpfn << PAGE_SHIFT;\n\tend_vaddr = mm->task_size;\n\n\t/* watch out for wraparound */\n\tif (svpfn > mm->task_size >> PAGE_SHIFT)\n\t\tstart_vaddr = end_vaddr;\n\n\t/*\n\t * The odds are that this will stop walking way\n\t * before end_vaddr, because the length of the\n\t * user buffer is tracked in \"pm\", and the walk\n\t * will stop when we hit the end of the buffer.\n\t */\n\tret = 0;\n\twhile (count && (start_vaddr < end_vaddr)) {\n\t\tint len;\n\t\tunsigned long end;\n\n\t\tpm.pos = 0;\n\t\tend = (start_vaddr + PAGEMAP_WALK_SIZE) & PAGEMAP_WALK_MASK;\n\t\t/* overflow ? */\n\t\tif (end < start_vaddr || end > end_vaddr)\n\t\t\tend = end_vaddr;\n\t\tdown_read(&mm->mmap_sem);\n\t\tret = walk_page_range(start_vaddr, end, &pagemap_walk);\n\t\tup_read(&mm->mmap_sem);\n\t\tstart_vaddr = end;\n\n\t\tlen = min(count, PM_ENTRY_BYTES * pm.pos);\n\t\tif (copy_to_user(buf, pm.buffer, len)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tcopied += len;\n\t\tbuf += len;\n\t\tcount -= len;\n\t}\n\t*ppos += copied;\n\tif (!ret || ret == PM_END_OF_BUFFER)\n\t\tret = copied;\n\nout_free:\n\tkfree(pm.buffer);\nout_mm:\n\tmmput(mm);\nout:\n\treturn ret;\n}\n\nstatic int pagemap_open(struct inode *inode, struct file *file)\n{\n\tstruct mm_struct *mm;\n\n\tmm = proc_mem_open(inode, PTRACE_MODE_READ);\n\tif (IS_ERR(mm))\n\t\treturn PTR_ERR(mm);\n\tfile->private_data = mm;\n\treturn 0;\n}\n\nstatic int pagemap_release(struct inode *inode, struct file *file)\n{\n\tstruct mm_struct *mm = file->private_data;\n\n\tif (mm)\n\t\tmmdrop(mm);\n\treturn 0;\n}\n\nconst struct file_operations proc_pagemap_operations = {\n\t.llseek\t\t= mem_lseek, /* borrow this */\n\t.read\t\t= pagemap_read,\n\t.open\t\t= pagemap_open,\n\t.release\t= pagemap_release,\n};\n#endif /* CONFIG_PROC_PAGE_MONITOR */\n\n#ifdef CONFIG_NUMA\n\nstruct numa_maps {\n\tunsigned long pages;\n\tunsigned long anon;\n\tunsigned long active;\n\tunsigned long writeback;\n\tunsigned long mapcount_max;\n\tunsigned long dirty;\n\tunsigned long swapcache;\n\tunsigned long node[MAX_NUMNODES];\n};\n\nstruct numa_maps_private {\n\tstruct proc_maps_private proc_maps;\n\tstruct numa_maps md;\n};\n\nstatic void gather_stats(struct page *page, struct numa_maps *md, int pte_dirty,\n\t\t\tunsigned long nr_pages)\n{\n\tint count = page_mapcount(page);\n\n\tmd->pages += nr_pages;\n\tif (pte_dirty || PageDirty(page))\n\t\tmd->dirty += nr_pages;\n\n\tif (PageSwapCache(page))\n\t\tmd->swapcache += nr_pages;\n\n\tif (PageActive(page) || PageUnevictable(page))\n\t\tmd->active += nr_pages;\n\n\tif (PageWriteback(page))\n\t\tmd->writeback += nr_pages;\n\n\tif (PageAnon(page))\n\t\tmd->anon += nr_pages;\n\n\tif (count > md->mapcount_max)\n\t\tmd->mapcount_max = count;\n\n\tmd->node[page_to_nid(page)] += nr_pages;\n}\n\nstatic struct page *can_gather_numa_stats(pte_t pte, struct vm_area_struct *vma,\n\t\tunsigned long addr)\n{\n\tstruct page *page;\n\tint nid;\n\n\tif (!pte_present(pte))\n\t\treturn NULL;\n\n\tpage = vm_normal_page(vma, addr, pte);\n\tif (!page)\n\t\treturn NULL;\n\n\tif (PageReserved(page))\n\t\treturn NULL;\n\n\tnid = page_to_nid(page);\n\tif (!node_isset(nid, node_states[N_MEMORY]))\n\t\treturn NULL;\n\n\treturn page;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic struct page *can_gather_numa_stats_pmd(pmd_t pmd,\n\t\t\t\t\t      struct vm_area_struct *vma,\n\t\t\t\t\t      unsigned long addr)\n{\n\tstruct page *page;\n\tint nid;\n\n\tif (!pmd_present(pmd))\n\t\treturn NULL;\n\n\tpage = vm_normal_page_pmd(vma, addr, pmd);\n\tif (!page)\n\t\treturn NULL;\n\n\tif (PageReserved(page))\n\t\treturn NULL;\n\n\tnid = page_to_nid(page);\n\tif (!node_isset(nid, node_states[N_MEMORY]))\n\t\treturn NULL;\n\n\treturn page;\n}\n#endif\n\nstatic int gather_pte_stats(pmd_t *pmd, unsigned long addr,\n\t\tunsigned long end, struct mm_walk *walk)\n{\n\tstruct numa_maps *md = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tspinlock_t *ptl;\n\tpte_t *orig_pte;\n\tpte_t *pte;\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tptl = pmd_trans_huge_lock(pmd, vma);\n\tif (ptl) {\n\t\tstruct page *page;\n\n\t\tpage = can_gather_numa_stats_pmd(*pmd, vma, addr);\n\t\tif (page)\n\t\t\tgather_stats(page, md, pmd_dirty(*pmd),\n\t\t\t\t     HPAGE_PMD_SIZE/PAGE_SIZE);\n\t\tspin_unlock(ptl);\n\t\treturn 0;\n\t}\n\n\tif (pmd_trans_unstable(pmd))\n\t\treturn 0;\n#endif\n\torig_pte = pte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);\n\tdo {\n\t\tstruct page *page = can_gather_numa_stats(*pte, vma, addr);\n\t\tif (!page)\n\t\t\tcontinue;\n\t\tgather_stats(page, md, pte_dirty(*pte), 1);\n\n\t} while (pte++, addr += PAGE_SIZE, addr != end);\n\tpte_unmap_unlock(orig_pte, ptl);\n\tcond_resched();\n\treturn 0;\n}\n#ifdef CONFIG_HUGETLB_PAGE\nstatic int gather_hugetlb_stats(pte_t *pte, unsigned long hmask,\n\t\tunsigned long addr, unsigned long end, struct mm_walk *walk)\n{\n\tpte_t huge_pte = huge_ptep_get(pte);\n\tstruct numa_maps *md;\n\tstruct page *page;\n\n\tif (!pte_present(huge_pte))\n\t\treturn 0;\n\n\tpage = pte_page(huge_pte);\n\tif (!page)\n\t\treturn 0;\n\n\tmd = walk->private;\n\tgather_stats(page, md, pte_dirty(huge_pte), 1);\n\treturn 0;\n}\n\n#else\nstatic int gather_hugetlb_stats(pte_t *pte, unsigned long hmask,\n\t\tunsigned long addr, unsigned long end, struct mm_walk *walk)\n{\n\treturn 0;\n}\n#endif\n\n/*\n * Display pages allocated per node and memory policy via /proc.\n */\nstatic int show_numa_map(struct seq_file *m, void *v)\n{\n\tstruct numa_maps_private *numa_priv = m->private;\n\tstruct proc_maps_private *proc_priv = &numa_priv->proc_maps;\n\tstruct vm_area_struct *vma = v;\n\tstruct numa_maps *md = &numa_priv->md;\n\tstruct file *file = vma->vm_file;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct mm_walk walk = {\n\t\t.hugetlb_entry = gather_hugetlb_stats,\n\t\t.pmd_entry = gather_pte_stats,\n\t\t.private = md,\n\t\t.mm = mm,\n\t};\n\tstruct mempolicy *pol;\n\tchar buffer[64];\n\tint nid;\n\n\tif (!mm)\n\t\treturn 0;\n\n\t/* Ensure we start with an empty set of numa_maps statistics. */\n\tmemset(md, 0, sizeof(*md));\n\n\tpol = __get_vma_policy(vma, vma->vm_start);\n\tif (pol) {\n\t\tmpol_to_str(buffer, sizeof(buffer), pol);\n\t\tmpol_cond_put(pol);\n\t} else {\n\t\tmpol_to_str(buffer, sizeof(buffer), proc_priv->task_mempolicy);\n\t}\n\n\tseq_printf(m, \"%08lx %s\", vma->vm_start, buffer);\n\n\tif (file) {\n\t\tseq_puts(m, \" file=\");\n\t\tseq_file_path(m, file, \"\\n\\t= \");\n\t} else if (vma->vm_start <= mm->brk && vma->vm_end >= mm->start_brk) {\n\t\tseq_puts(m, \" heap\");\n\t} else if (is_stack(vma)) {\n\t\tseq_puts(m, \" stack\");\n\t}\n\n\tif (is_vm_hugetlb_page(vma))\n\t\tseq_puts(m, \" huge\");\n\n\t/* mmap_sem is held by m_start */\n\twalk_page_vma(vma, &walk);\n\n\tif (!md->pages)\n\t\tgoto out;\n\n\tif (md->anon)\n\t\tseq_printf(m, \" anon=%lu\", md->anon);\n\n\tif (md->dirty)\n\t\tseq_printf(m, \" dirty=%lu\", md->dirty);\n\n\tif (md->pages != md->anon && md->pages != md->dirty)\n\t\tseq_printf(m, \" mapped=%lu\", md->pages);\n\n\tif (md->mapcount_max > 1)\n\t\tseq_printf(m, \" mapmax=%lu\", md->mapcount_max);\n\n\tif (md->swapcache)\n\t\tseq_printf(m, \" swapcache=%lu\", md->swapcache);\n\n\tif (md->active < md->pages && !is_vm_hugetlb_page(vma))\n\t\tseq_printf(m, \" active=%lu\", md->active);\n\n\tif (md->writeback)\n\t\tseq_printf(m, \" writeback=%lu\", md->writeback);\n\n\tfor_each_node_state(nid, N_MEMORY)\n\t\tif (md->node[nid])\n\t\t\tseq_printf(m, \" N%d=%lu\", nid, md->node[nid]);\n\n\tseq_printf(m, \" kernelpagesize_kB=%lu\", vma_kernel_pagesize(vma) >> 10);\nout:\n\tseq_putc(m, '\\n');\n\tm_cache_vma(m, vma);\n\treturn 0;\n}\n\nstatic const struct seq_operations proc_pid_numa_maps_op = {\n\t.start  = m_start,\n\t.next   = m_next,\n\t.stop   = m_stop,\n\t.show   = show_numa_map,\n};\n\nstatic int pid_numa_maps_open(struct inode *inode, struct file *file)\n{\n\treturn proc_maps_open(inode, file, &proc_pid_numa_maps_op,\n\t\t\t\tsizeof(struct numa_maps_private));\n}\n\nconst struct file_operations proc_pid_numa_maps_operations = {\n\t.open\t\t= pid_numa_maps_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= proc_map_release,\n};\n\n#endif /* CONFIG_NUMA */\n", "/*\n *  fs/userfaultfd.c\n *\n *  Copyright (C) 2007  Davide Libenzi <davidel@xmailserver.org>\n *  Copyright (C) 2008-2009 Red Hat, Inc.\n *  Copyright (C) 2015  Red Hat, Inc.\n *\n *  This work is licensed under the terms of the GNU GPL, version 2. See\n *  the COPYING file in the top-level directory.\n *\n *  Some part derived from fs/eventfd.c (anon inode setup) and\n *  mm/ksm.c (mm hashing).\n */\n\n#include <linux/list.h>\n#include <linux/hashtable.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/poll.h>\n#include <linux/slab.h>\n#include <linux/seq_file.h>\n#include <linux/file.h>\n#include <linux/bug.h>\n#include <linux/anon_inodes.h>\n#include <linux/syscalls.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/mempolicy.h>\n#include <linux/ioctl.h>\n#include <linux/security.h>\n#include <linux/hugetlb.h>\n\nstatic struct kmem_cache *userfaultfd_ctx_cachep __read_mostly;\n\nenum userfaultfd_state {\n\tUFFD_STATE_WAIT_API,\n\tUFFD_STATE_RUNNING,\n};\n\n/*\n * Start with fault_pending_wqh and fault_wqh so they're more likely\n * to be in the same cacheline.\n */\nstruct userfaultfd_ctx {\n\t/* waitqueue head for the pending (i.e. not read) userfaults */\n\twait_queue_head_t fault_pending_wqh;\n\t/* waitqueue head for the userfaults */\n\twait_queue_head_t fault_wqh;\n\t/* waitqueue head for the pseudo fd to wakeup poll/read */\n\twait_queue_head_t fd_wqh;\n\t/* waitqueue head for events */\n\twait_queue_head_t event_wqh;\n\t/* a refile sequence protected by fault_pending_wqh lock */\n\tstruct seqcount refile_seq;\n\t/* pseudo fd refcounting */\n\trefcount_t refcount;\n\t/* userfaultfd syscall flags */\n\tunsigned int flags;\n\t/* features requested from the userspace */\n\tunsigned int features;\n\t/* state machine */\n\tenum userfaultfd_state state;\n\t/* released */\n\tbool released;\n\t/* memory mappings are changing because of non-cooperative event */\n\tbool mmap_changing;\n\t/* mm with one ore more vmas attached to this userfaultfd_ctx */\n\tstruct mm_struct *mm;\n};\n\nstruct userfaultfd_fork_ctx {\n\tstruct userfaultfd_ctx *orig;\n\tstruct userfaultfd_ctx *new;\n\tstruct list_head list;\n};\n\nstruct userfaultfd_unmap_ctx {\n\tstruct userfaultfd_ctx *ctx;\n\tunsigned long start;\n\tunsigned long end;\n\tstruct list_head list;\n};\n\nstruct userfaultfd_wait_queue {\n\tstruct uffd_msg msg;\n\twait_queue_entry_t wq;\n\tstruct userfaultfd_ctx *ctx;\n\tbool waken;\n};\n\nstruct userfaultfd_wake_range {\n\tunsigned long start;\n\tunsigned long len;\n};\n\nstatic int userfaultfd_wake_function(wait_queue_entry_t *wq, unsigned mode,\n\t\t\t\t     int wake_flags, void *key)\n{\n\tstruct userfaultfd_wake_range *range = key;\n\tint ret;\n\tstruct userfaultfd_wait_queue *uwq;\n\tunsigned long start, len;\n\n\tuwq = container_of(wq, struct userfaultfd_wait_queue, wq);\n\tret = 0;\n\t/* len == 0 means wake all */\n\tstart = range->start;\n\tlen = range->len;\n\tif (len && (start > uwq->msg.arg.pagefault.address ||\n\t\t    start + len <= uwq->msg.arg.pagefault.address))\n\t\tgoto out;\n\tWRITE_ONCE(uwq->waken, true);\n\t/*\n\t * The Program-Order guarantees provided by the scheduler\n\t * ensure uwq->waken is visible before the task is woken.\n\t */\n\tret = wake_up_state(wq->private, mode);\n\tif (ret) {\n\t\t/*\n\t\t * Wake only once, autoremove behavior.\n\t\t *\n\t\t * After the effect of list_del_init is visible to the other\n\t\t * CPUs, the waitqueue may disappear from under us, see the\n\t\t * !list_empty_careful() in handle_userfault().\n\t\t *\n\t\t * try_to_wake_up() has an implicit smp_mb(), and the\n\t\t * wq->private is read before calling the extern function\n\t\t * \"wake_up_state\" (which in turns calls try_to_wake_up).\n\t\t */\n\t\tlist_del_init(&wq->entry);\n\t}\nout:\n\treturn ret;\n}\n\n/**\n * userfaultfd_ctx_get - Acquires a reference to the internal userfaultfd\n * context.\n * @ctx: [in] Pointer to the userfaultfd context.\n */\nstatic void userfaultfd_ctx_get(struct userfaultfd_ctx *ctx)\n{\n\trefcount_inc(&ctx->refcount);\n}\n\n/**\n * userfaultfd_ctx_put - Releases a reference to the internal userfaultfd\n * context.\n * @ctx: [in] Pointer to userfaultfd context.\n *\n * The userfaultfd context reference must have been previously acquired either\n * with userfaultfd_ctx_get() or userfaultfd_ctx_fdget().\n */\nstatic void userfaultfd_ctx_put(struct userfaultfd_ctx *ctx)\n{\n\tif (refcount_dec_and_test(&ctx->refcount)) {\n\t\tVM_BUG_ON(spin_is_locked(&ctx->fault_pending_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->fault_pending_wqh));\n\t\tVM_BUG_ON(spin_is_locked(&ctx->fault_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->fault_wqh));\n\t\tVM_BUG_ON(spin_is_locked(&ctx->event_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->event_wqh));\n\t\tVM_BUG_ON(spin_is_locked(&ctx->fd_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->fd_wqh));\n\t\tmmdrop(ctx->mm);\n\t\tkmem_cache_free(userfaultfd_ctx_cachep, ctx);\n\t}\n}\n\nstatic inline void msg_init(struct uffd_msg *msg)\n{\n\tBUILD_BUG_ON(sizeof(struct uffd_msg) != 32);\n\t/*\n\t * Must use memset to zero out the paddings or kernel data is\n\t * leaked to userland.\n\t */\n\tmemset(msg, 0, sizeof(struct uffd_msg));\n}\n\nstatic inline struct uffd_msg userfault_msg(unsigned long address,\n\t\t\t\t\t    unsigned int flags,\n\t\t\t\t\t    unsigned long reason,\n\t\t\t\t\t    unsigned int features)\n{\n\tstruct uffd_msg msg;\n\tmsg_init(&msg);\n\tmsg.event = UFFD_EVENT_PAGEFAULT;\n\tmsg.arg.pagefault.address = address;\n\tif (flags & FAULT_FLAG_WRITE)\n\t\t/*\n\t\t * If UFFD_FEATURE_PAGEFAULT_FLAG_WP was set in the\n\t\t * uffdio_api.features and UFFD_PAGEFAULT_FLAG_WRITE\n\t\t * was not set in a UFFD_EVENT_PAGEFAULT, it means it\n\t\t * was a read fault, otherwise if set it means it's\n\t\t * a write fault.\n\t\t */\n\t\tmsg.arg.pagefault.flags |= UFFD_PAGEFAULT_FLAG_WRITE;\n\tif (reason & VM_UFFD_WP)\n\t\t/*\n\t\t * If UFFD_FEATURE_PAGEFAULT_FLAG_WP was set in the\n\t\t * uffdio_api.features and UFFD_PAGEFAULT_FLAG_WP was\n\t\t * not set in a UFFD_EVENT_PAGEFAULT, it means it was\n\t\t * a missing fault, otherwise if set it means it's a\n\t\t * write protect fault.\n\t\t */\n\t\tmsg.arg.pagefault.flags |= UFFD_PAGEFAULT_FLAG_WP;\n\tif (features & UFFD_FEATURE_THREAD_ID)\n\t\tmsg.arg.pagefault.feat.ptid = task_pid_vnr(current);\n\treturn msg;\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\n/*\n * Same functionality as userfaultfd_must_wait below with modifications for\n * hugepmd ranges.\n */\nstatic inline bool userfaultfd_huge_must_wait(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t struct vm_area_struct *vma,\n\t\t\t\t\t unsigned long address,\n\t\t\t\t\t unsigned long flags,\n\t\t\t\t\t unsigned long reason)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tpte_t *ptep, pte;\n\tbool ret = true;\n\n\tVM_BUG_ON(!rwsem_is_locked(&mm->mmap_sem));\n\n\tptep = huge_pte_offset(mm, address, vma_mmu_pagesize(vma));\n\n\tif (!ptep)\n\t\tgoto out;\n\n\tret = false;\n\tpte = huge_ptep_get(ptep);\n\n\t/*\n\t * Lockless access: we're in a wait_event so it's ok if it\n\t * changes under us.\n\t */\n\tif (huge_pte_none(pte))\n\t\tret = true;\n\tif (!huge_pte_write(pte) && (reason & VM_UFFD_WP))\n\t\tret = true;\nout:\n\treturn ret;\n}\n#else\nstatic inline bool userfaultfd_huge_must_wait(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t struct vm_area_struct *vma,\n\t\t\t\t\t unsigned long address,\n\t\t\t\t\t unsigned long flags,\n\t\t\t\t\t unsigned long reason)\n{\n\treturn false;\t/* should never get here */\n}\n#endif /* CONFIG_HUGETLB_PAGE */\n\n/*\n * Verify the pagetables are still not ok after having reigstered into\n * the fault_pending_wqh to avoid userland having to UFFDIO_WAKE any\n * userfault that has already been resolved, if userfaultfd_read and\n * UFFDIO_COPY|ZEROPAGE are being run simultaneously on two different\n * threads.\n */\nstatic inline bool userfaultfd_must_wait(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t unsigned long address,\n\t\t\t\t\t unsigned long flags,\n\t\t\t\t\t unsigned long reason)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd, _pmd;\n\tpte_t *pte;\n\tbool ret = true;\n\n\tVM_BUG_ON(!rwsem_is_locked(&mm->mmap_sem));\n\n\tpgd = pgd_offset(mm, address);\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\tp4d = p4d_offset(pgd, address);\n\tif (!p4d_present(*p4d))\n\t\tgoto out;\n\tpud = pud_offset(p4d, address);\n\tif (!pud_present(*pud))\n\t\tgoto out;\n\tpmd = pmd_offset(pud, address);\n\t/*\n\t * READ_ONCE must function as a barrier with narrower scope\n\t * and it must be equivalent to:\n\t *\t_pmd = *pmd; barrier();\n\t *\n\t * This is to deal with the instability (as in\n\t * pmd_trans_unstable) of the pmd.\n\t */\n\t_pmd = READ_ONCE(*pmd);\n\tif (pmd_none(_pmd))\n\t\tgoto out;\n\n\tret = false;\n\tif (!pmd_present(_pmd))\n\t\tgoto out;\n\n\tif (pmd_trans_huge(_pmd))\n\t\tgoto out;\n\n\t/*\n\t * the pmd is stable (as in !pmd_trans_unstable) so we can re-read it\n\t * and use the standard pte_offset_map() instead of parsing _pmd.\n\t */\n\tpte = pte_offset_map(pmd, address);\n\t/*\n\t * Lockless access: we're in a wait_event so it's ok if it\n\t * changes under us.\n\t */\n\tif (pte_none(*pte))\n\t\tret = true;\n\tpte_unmap(pte);\n\nout:\n\treturn ret;\n}\n\n/*\n * The locking rules involved in returning VM_FAULT_RETRY depending on\n * FAULT_FLAG_ALLOW_RETRY, FAULT_FLAG_RETRY_NOWAIT and\n * FAULT_FLAG_KILLABLE are not straightforward. The \"Caution\"\n * recommendation in __lock_page_or_retry is not an understatement.\n *\n * If FAULT_FLAG_ALLOW_RETRY is set, the mmap_sem must be released\n * before returning VM_FAULT_RETRY only if FAULT_FLAG_RETRY_NOWAIT is\n * not set.\n *\n * If FAULT_FLAG_ALLOW_RETRY is set but FAULT_FLAG_KILLABLE is not\n * set, VM_FAULT_RETRY can still be returned if and only if there are\n * fatal_signal_pending()s, and the mmap_sem must be released before\n * returning it.\n */\nvm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)\n{\n\tstruct mm_struct *mm = vmf->vma->vm_mm;\n\tstruct userfaultfd_ctx *ctx;\n\tstruct userfaultfd_wait_queue uwq;\n\tvm_fault_t ret = VM_FAULT_SIGBUS;\n\tbool must_wait, return_to_userland;\n\tlong blocking_state;\n\n\t/*\n\t * We don't do userfault handling for the final child pid update.\n\t *\n\t * We also don't do userfault handling during\n\t * coredumping. hugetlbfs has the special\n\t * follow_hugetlb_page() to skip missing pages in the\n\t * FOLL_DUMP case, anon memory also checks for FOLL_DUMP with\n\t * the no_page_table() helper in follow_page_mask(), but the\n\t * shmem_vm_ops->fault method is invoked even during\n\t * coredumping without mmap_sem and it ends up here.\n\t */\n\tif (current->flags & (PF_EXITING|PF_DUMPCORE))\n\t\tgoto out;\n\n\t/*\n\t * Coredumping runs without mmap_sem so we can only check that\n\t * the mmap_sem is held, if PF_DUMPCORE was not set.\n\t */\n\tWARN_ON_ONCE(!rwsem_is_locked(&mm->mmap_sem));\n\n\tctx = vmf->vma->vm_userfaultfd_ctx.ctx;\n\tif (!ctx)\n\t\tgoto out;\n\n\tBUG_ON(ctx->mm != mm);\n\n\tVM_BUG_ON(reason & ~(VM_UFFD_MISSING|VM_UFFD_WP));\n\tVM_BUG_ON(!(reason & VM_UFFD_MISSING) ^ !!(reason & VM_UFFD_WP));\n\n\tif (ctx->features & UFFD_FEATURE_SIGBUS)\n\t\tgoto out;\n\n\t/*\n\t * If it's already released don't get it. This avoids to loop\n\t * in __get_user_pages if userfaultfd_release waits on the\n\t * caller of handle_userfault to release the mmap_sem.\n\t */\n\tif (unlikely(READ_ONCE(ctx->released))) {\n\t\t/*\n\t\t * Don't return VM_FAULT_SIGBUS in this case, so a non\n\t\t * cooperative manager can close the uffd after the\n\t\t * last UFFDIO_COPY, without risking to trigger an\n\t\t * involuntary SIGBUS if the process was starting the\n\t\t * userfaultfd while the userfaultfd was still armed\n\t\t * (but after the last UFFDIO_COPY). If the uffd\n\t\t * wasn't already closed when the userfault reached\n\t\t * this point, that would normally be solved by\n\t\t * userfaultfd_must_wait returning 'false'.\n\t\t *\n\t\t * If we were to return VM_FAULT_SIGBUS here, the non\n\t\t * cooperative manager would be instead forced to\n\t\t * always call UFFDIO_UNREGISTER before it can safely\n\t\t * close the uffd.\n\t\t */\n\t\tret = VM_FAULT_NOPAGE;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Check that we can return VM_FAULT_RETRY.\n\t *\n\t * NOTE: it should become possible to return VM_FAULT_RETRY\n\t * even if FAULT_FLAG_TRIED is set without leading to gup()\n\t * -EBUSY failures, if the userfaultfd is to be extended for\n\t * VM_UFFD_WP tracking and we intend to arm the userfault\n\t * without first stopping userland access to the memory. For\n\t * VM_UFFD_MISSING userfaults this is enough for now.\n\t */\n\tif (unlikely(!(vmf->flags & FAULT_FLAG_ALLOW_RETRY))) {\n\t\t/*\n\t\t * Validate the invariant that nowait must allow retry\n\t\t * to be sure not to return SIGBUS erroneously on\n\t\t * nowait invocations.\n\t\t */\n\t\tBUG_ON(vmf->flags & FAULT_FLAG_RETRY_NOWAIT);\n#ifdef CONFIG_DEBUG_VM\n\t\tif (printk_ratelimit()) {\n\t\t\tprintk(KERN_WARNING\n\t\t\t       \"FAULT_FLAG_ALLOW_RETRY missing %x\\n\",\n\t\t\t       vmf->flags);\n\t\t\tdump_stack();\n\t\t}\n#endif\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Handle nowait, not much to do other than tell it to retry\n\t * and wait.\n\t */\n\tret = VM_FAULT_RETRY;\n\tif (vmf->flags & FAULT_FLAG_RETRY_NOWAIT)\n\t\tgoto out;\n\n\t/* take the reference before dropping the mmap_sem */\n\tuserfaultfd_ctx_get(ctx);\n\n\tinit_waitqueue_func_entry(&uwq.wq, userfaultfd_wake_function);\n\tuwq.wq.private = current;\n\tuwq.msg = userfault_msg(vmf->address, vmf->flags, reason,\n\t\t\tctx->features);\n\tuwq.ctx = ctx;\n\tuwq.waken = false;\n\n\treturn_to_userland =\n\t\t(vmf->flags & (FAULT_FLAG_USER|FAULT_FLAG_KILLABLE)) ==\n\t\t(FAULT_FLAG_USER|FAULT_FLAG_KILLABLE);\n\tblocking_state = return_to_userland ? TASK_INTERRUPTIBLE :\n\t\t\t TASK_KILLABLE;\n\n\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t/*\n\t * After the __add_wait_queue the uwq is visible to userland\n\t * through poll/read().\n\t */\n\t__add_wait_queue(&ctx->fault_pending_wqh, &uwq.wq);\n\t/*\n\t * The smp_mb() after __set_current_state prevents the reads\n\t * following the spin_unlock to happen before the list_add in\n\t * __add_wait_queue.\n\t */\n\tset_current_state(blocking_state);\n\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\tif (!is_vm_hugetlb_page(vmf->vma))\n\t\tmust_wait = userfaultfd_must_wait(ctx, vmf->address, vmf->flags,\n\t\t\t\t\t\t  reason);\n\telse\n\t\tmust_wait = userfaultfd_huge_must_wait(ctx, vmf->vma,\n\t\t\t\t\t\t       vmf->address,\n\t\t\t\t\t\t       vmf->flags, reason);\n\tup_read(&mm->mmap_sem);\n\n\tif (likely(must_wait && !READ_ONCE(ctx->released) &&\n\t\t   (return_to_userland ? !signal_pending(current) :\n\t\t    !fatal_signal_pending(current)))) {\n\t\twake_up_poll(&ctx->fd_wqh, EPOLLIN);\n\t\tschedule();\n\t\tret |= VM_FAULT_MAJOR;\n\n\t\t/*\n\t\t * False wakeups can orginate even from rwsem before\n\t\t * up_read() however userfaults will wait either for a\n\t\t * targeted wakeup on the specific uwq waitqueue from\n\t\t * wake_userfault() or for signals or for uffd\n\t\t * release.\n\t\t */\n\t\twhile (!READ_ONCE(uwq.waken)) {\n\t\t\t/*\n\t\t\t * This needs the full smp_store_mb()\n\t\t\t * guarantee as the state write must be\n\t\t\t * visible to other CPUs before reading\n\t\t\t * uwq.waken from other CPUs.\n\t\t\t */\n\t\t\tset_current_state(blocking_state);\n\t\t\tif (READ_ONCE(uwq.waken) ||\n\t\t\t    READ_ONCE(ctx->released) ||\n\t\t\t    (return_to_userland ? signal_pending(current) :\n\t\t\t     fatal_signal_pending(current)))\n\t\t\t\tbreak;\n\t\t\tschedule();\n\t\t}\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\n\tif (return_to_userland) {\n\t\tif (signal_pending(current) &&\n\t\t    !fatal_signal_pending(current)) {\n\t\t\t/*\n\t\t\t * If we got a SIGSTOP or SIGCONT and this is\n\t\t\t * a normal userland page fault, just let\n\t\t\t * userland return so the signal will be\n\t\t\t * handled and gdb debugging works.  The page\n\t\t\t * fault code immediately after we return from\n\t\t\t * this function is going to release the\n\t\t\t * mmap_sem and it's not depending on it\n\t\t\t * (unlike gup would if we were not to return\n\t\t\t * VM_FAULT_RETRY).\n\t\t\t *\n\t\t\t * If a fatal signal is pending we still take\n\t\t\t * the streamlined VM_FAULT_RETRY failure path\n\t\t\t * and there's no need to retake the mmap_sem\n\t\t\t * in such case.\n\t\t\t */\n\t\t\tdown_read(&mm->mmap_sem);\n\t\t\tret = VM_FAULT_NOPAGE;\n\t\t}\n\t}\n\n\t/*\n\t * Here we race with the list_del; list_add in\n\t * userfaultfd_ctx_read(), however because we don't ever run\n\t * list_del_init() to refile across the two lists, the prev\n\t * and next pointers will never point to self. list_add also\n\t * would never let any of the two pointers to point to\n\t * self. So list_empty_careful won't risk to see both pointers\n\t * pointing to self at any time during the list refile. The\n\t * only case where list_del_init() is called is the full\n\t * removal in the wake function and there we don't re-list_add\n\t * and it's fine not to block on the spinlock. The uwq on this\n\t * kernel stack can be released after the list_del_init.\n\t */\n\tif (!list_empty_careful(&uwq.wq.entry)) {\n\t\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t\t/*\n\t\t * No need of list_del_init(), the uwq on the stack\n\t\t * will be freed shortly anyway.\n\t\t */\n\t\tlist_del(&uwq.wq.entry);\n\t\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\t}\n\n\t/*\n\t * ctx may go away after this if the userfault pseudo fd is\n\t * already released.\n\t */\n\tuserfaultfd_ctx_put(ctx);\n\nout:\n\treturn ret;\n}\n\nstatic void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t      struct userfaultfd_wait_queue *ewq)\n{\n\tstruct userfaultfd_ctx *release_new_ctx;\n\n\tif (WARN_ON_ONCE(current->flags & PF_EXITING))\n\t\tgoto out;\n\n\tewq->ctx = ctx;\n\tinit_waitqueue_entry(&ewq->wq, current);\n\trelease_new_ctx = NULL;\n\n\tspin_lock(&ctx->event_wqh.lock);\n\t/*\n\t * After the __add_wait_queue the uwq is visible to userland\n\t * through poll/read().\n\t */\n\t__add_wait_queue(&ctx->event_wqh, &ewq->wq);\n\tfor (;;) {\n\t\tset_current_state(TASK_KILLABLE);\n\t\tif (ewq->msg.event == 0)\n\t\t\tbreak;\n\t\tif (READ_ONCE(ctx->released) ||\n\t\t    fatal_signal_pending(current)) {\n\t\t\t/*\n\t\t\t * &ewq->wq may be queued in fork_event, but\n\t\t\t * __remove_wait_queue ignores the head\n\t\t\t * parameter. It would be a problem if it\n\t\t\t * didn't.\n\t\t\t */\n\t\t\t__remove_wait_queue(&ctx->event_wqh, &ewq->wq);\n\t\t\tif (ewq->msg.event == UFFD_EVENT_FORK) {\n\t\t\t\tstruct userfaultfd_ctx *new;\n\n\t\t\t\tnew = (struct userfaultfd_ctx *)\n\t\t\t\t\t(unsigned long)\n\t\t\t\t\tewq->msg.arg.reserved.reserved1;\n\t\t\t\trelease_new_ctx = new;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tspin_unlock(&ctx->event_wqh.lock);\n\n\t\twake_up_poll(&ctx->fd_wqh, EPOLLIN);\n\t\tschedule();\n\n\t\tspin_lock(&ctx->event_wqh.lock);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\tspin_unlock(&ctx->event_wqh.lock);\n\n\tif (release_new_ctx) {\n\t\tstruct vm_area_struct *vma;\n\t\tstruct mm_struct *mm = release_new_ctx->mm;\n\n\t\t/* the various vma->vm_userfaultfd_ctx still points to it */\n\t\tdown_write(&mm->mmap_sem);\n\t\t/* no task can run (and in turn coredump) yet */\n\t\tVM_WARN_ON(!mmget_still_valid(mm));\n\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next)\n\t\t\tif (vma->vm_userfaultfd_ctx.ctx == release_new_ctx) {\n\t\t\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t\t\t\tvma->vm_flags &= ~(VM_UFFD_WP | VM_UFFD_MISSING);\n\t\t\t}\n\t\tup_write(&mm->mmap_sem);\n\n\t\tuserfaultfd_ctx_put(release_new_ctx);\n\t}\n\n\t/*\n\t * ctx may go away after this if the userfault pseudo fd is\n\t * already released.\n\t */\nout:\n\tWRITE_ONCE(ctx->mmap_changing, false);\n\tuserfaultfd_ctx_put(ctx);\n}\n\nstatic void userfaultfd_event_complete(struct userfaultfd_ctx *ctx,\n\t\t\t\t       struct userfaultfd_wait_queue *ewq)\n{\n\tewq->msg.event = 0;\n\twake_up_locked(&ctx->event_wqh);\n\t__remove_wait_queue(&ctx->event_wqh, &ewq->wq);\n}\n\nint dup_userfaultfd(struct vm_area_struct *vma, struct list_head *fcs)\n{\n\tstruct userfaultfd_ctx *ctx = NULL, *octx;\n\tstruct userfaultfd_fork_ctx *fctx;\n\n\toctx = vma->vm_userfaultfd_ctx.ctx;\n\tif (!octx || !(octx->features & UFFD_FEATURE_EVENT_FORK)) {\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t\tvma->vm_flags &= ~(VM_UFFD_WP | VM_UFFD_MISSING);\n\t\treturn 0;\n\t}\n\n\tlist_for_each_entry(fctx, fcs, list)\n\t\tif (fctx->orig == octx) {\n\t\t\tctx = fctx->new;\n\t\t\tbreak;\n\t\t}\n\n\tif (!ctx) {\n\t\tfctx = kmalloc(sizeof(*fctx), GFP_KERNEL);\n\t\tif (!fctx)\n\t\t\treturn -ENOMEM;\n\n\t\tctx = kmem_cache_alloc(userfaultfd_ctx_cachep, GFP_KERNEL);\n\t\tif (!ctx) {\n\t\t\tkfree(fctx);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\trefcount_set(&ctx->refcount, 1);\n\t\tctx->flags = octx->flags;\n\t\tctx->state = UFFD_STATE_RUNNING;\n\t\tctx->features = octx->features;\n\t\tctx->released = false;\n\t\tctx->mmap_changing = false;\n\t\tctx->mm = vma->vm_mm;\n\t\tmmgrab(ctx->mm);\n\n\t\tuserfaultfd_ctx_get(octx);\n\t\tWRITE_ONCE(octx->mmap_changing, true);\n\t\tfctx->orig = octx;\n\t\tfctx->new = ctx;\n\t\tlist_add_tail(&fctx->list, fcs);\n\t}\n\n\tvma->vm_userfaultfd_ctx.ctx = ctx;\n\treturn 0;\n}\n\nstatic void dup_fctx(struct userfaultfd_fork_ctx *fctx)\n{\n\tstruct userfaultfd_ctx *ctx = fctx->orig;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tmsg_init(&ewq.msg);\n\n\tewq.msg.event = UFFD_EVENT_FORK;\n\tewq.msg.arg.reserved.reserved1 = (unsigned long)fctx->new;\n\n\tuserfaultfd_event_wait_completion(ctx, &ewq);\n}\n\nvoid dup_userfaultfd_complete(struct list_head *fcs)\n{\n\tstruct userfaultfd_fork_ctx *fctx, *n;\n\n\tlist_for_each_entry_safe(fctx, n, fcs, list) {\n\t\tdup_fctx(fctx);\n\t\tlist_del(&fctx->list);\n\t\tkfree(fctx);\n\t}\n}\n\nvoid mremap_userfaultfd_prep(struct vm_area_struct *vma,\n\t\t\t     struct vm_userfaultfd_ctx *vm_ctx)\n{\n\tstruct userfaultfd_ctx *ctx;\n\n\tctx = vma->vm_userfaultfd_ctx.ctx;\n\n\tif (!ctx)\n\t\treturn;\n\n\tif (ctx->features & UFFD_FEATURE_EVENT_REMAP) {\n\t\tvm_ctx->ctx = ctx;\n\t\tuserfaultfd_ctx_get(ctx);\n\t\tWRITE_ONCE(ctx->mmap_changing, true);\n\t} else {\n\t\t/* Drop uffd context if remap feature not enabled */\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t\tvma->vm_flags &= ~(VM_UFFD_WP | VM_UFFD_MISSING);\n\t}\n}\n\nvoid mremap_userfaultfd_complete(struct vm_userfaultfd_ctx *vm_ctx,\n\t\t\t\t unsigned long from, unsigned long to,\n\t\t\t\t unsigned long len)\n{\n\tstruct userfaultfd_ctx *ctx = vm_ctx->ctx;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tif (!ctx)\n\t\treturn;\n\n\tif (to & ~PAGE_MASK) {\n\t\tuserfaultfd_ctx_put(ctx);\n\t\treturn;\n\t}\n\n\tmsg_init(&ewq.msg);\n\n\tewq.msg.event = UFFD_EVENT_REMAP;\n\tewq.msg.arg.remap.from = from;\n\tewq.msg.arg.remap.to = to;\n\tewq.msg.arg.remap.len = len;\n\n\tuserfaultfd_event_wait_completion(ctx, &ewq);\n}\n\nbool userfaultfd_remove(struct vm_area_struct *vma,\n\t\t\tunsigned long start, unsigned long end)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct userfaultfd_ctx *ctx;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tctx = vma->vm_userfaultfd_ctx.ctx;\n\tif (!ctx || !(ctx->features & UFFD_FEATURE_EVENT_REMOVE))\n\t\treturn true;\n\n\tuserfaultfd_ctx_get(ctx);\n\tWRITE_ONCE(ctx->mmap_changing, true);\n\tup_read(&mm->mmap_sem);\n\n\tmsg_init(&ewq.msg);\n\n\tewq.msg.event = UFFD_EVENT_REMOVE;\n\tewq.msg.arg.remove.start = start;\n\tewq.msg.arg.remove.end = end;\n\n\tuserfaultfd_event_wait_completion(ctx, &ewq);\n\n\treturn false;\n}\n\nstatic bool has_unmap_ctx(struct userfaultfd_ctx *ctx, struct list_head *unmaps,\n\t\t\t  unsigned long start, unsigned long end)\n{\n\tstruct userfaultfd_unmap_ctx *unmap_ctx;\n\n\tlist_for_each_entry(unmap_ctx, unmaps, list)\n\t\tif (unmap_ctx->ctx == ctx && unmap_ctx->start == start &&\n\t\t    unmap_ctx->end == end)\n\t\t\treturn true;\n\n\treturn false;\n}\n\nint userfaultfd_unmap_prep(struct vm_area_struct *vma,\n\t\t\t   unsigned long start, unsigned long end,\n\t\t\t   struct list_head *unmaps)\n{\n\tfor ( ; vma && vma->vm_start < end; vma = vma->vm_next) {\n\t\tstruct userfaultfd_unmap_ctx *unmap_ctx;\n\t\tstruct userfaultfd_ctx *ctx = vma->vm_userfaultfd_ctx.ctx;\n\n\t\tif (!ctx || !(ctx->features & UFFD_FEATURE_EVENT_UNMAP) ||\n\t\t    has_unmap_ctx(ctx, unmaps, start, end))\n\t\t\tcontinue;\n\n\t\tunmap_ctx = kzalloc(sizeof(*unmap_ctx), GFP_KERNEL);\n\t\tif (!unmap_ctx)\n\t\t\treturn -ENOMEM;\n\n\t\tuserfaultfd_ctx_get(ctx);\n\t\tWRITE_ONCE(ctx->mmap_changing, true);\n\t\tunmap_ctx->ctx = ctx;\n\t\tunmap_ctx->start = start;\n\t\tunmap_ctx->end = end;\n\t\tlist_add_tail(&unmap_ctx->list, unmaps);\n\t}\n\n\treturn 0;\n}\n\nvoid userfaultfd_unmap_complete(struct mm_struct *mm, struct list_head *uf)\n{\n\tstruct userfaultfd_unmap_ctx *ctx, *n;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tlist_for_each_entry_safe(ctx, n, uf, list) {\n\t\tmsg_init(&ewq.msg);\n\n\t\tewq.msg.event = UFFD_EVENT_UNMAP;\n\t\tewq.msg.arg.remove.start = ctx->start;\n\t\tewq.msg.arg.remove.end = ctx->end;\n\n\t\tuserfaultfd_event_wait_completion(ctx->ctx, &ewq);\n\n\t\tlist_del(&ctx->list);\n\t\tkfree(ctx);\n\t}\n}\n\nstatic int userfaultfd_release(struct inode *inode, struct file *file)\n{\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev;\n\t/* len == 0 means wake all */\n\tstruct userfaultfd_wake_range range = { .len = 0, };\n\tunsigned long new_flags;\n\n\tWRITE_ONCE(ctx->released, true);\n\n\tif (!mmget_not_zero(mm))\n\t\tgoto wakeup;\n\n\t/*\n\t * Flush page faults out of all CPUs. NOTE: all page faults\n\t * must be retried without returning VM_FAULT_SIGBUS if\n\t * userfaultfd_ctx_get() succeeds but vma->vma_userfault_ctx\n\t * changes while handle_userfault released the mmap_sem. So\n\t * it's critical that released is set to true (above), before\n\t * taking the mmap_sem for writing.\n\t */\n\tdown_write(&mm->mmap_sem);\n\tif (!mmget_still_valid(mm))\n\t\tgoto skip_mm;\n\tprev = NULL;\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tcond_resched();\n\t\tBUG_ON(!!vma->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(vma->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\t\tif (vma->vm_userfaultfd_ctx.ctx != ctx) {\n\t\t\tprev = vma;\n\t\t\tcontinue;\n\t\t}\n\t\tnew_flags = vma->vm_flags & ~(VM_UFFD_MISSING | VM_UFFD_WP);\n\t\tprev = vma_merge(mm, prev, vma->vm_start, vma->vm_end,\n\t\t\t\t new_flags, vma->anon_vma,\n\t\t\t\t vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t NULL_VM_UFFD_CTX);\n\t\tif (prev)\n\t\t\tvma = prev;\n\t\telse\n\t\t\tprev = vma;\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t}\nskip_mm:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\nwakeup:\n\t/*\n\t * After no new page faults can wait on this fault_*wqh, flush\n\t * the last page faults that may have been already waiting on\n\t * the fault_*wqh.\n\t */\n\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t__wake_up_locked_key(&ctx->fault_pending_wqh, TASK_NORMAL, &range);\n\t__wake_up(&ctx->fault_wqh, TASK_NORMAL, 1, &range);\n\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\t/* Flush pending events that may still wait on event_wqh */\n\twake_up_all(&ctx->event_wqh);\n\n\twake_up_poll(&ctx->fd_wqh, EPOLLHUP);\n\tuserfaultfd_ctx_put(ctx);\n\treturn 0;\n}\n\n/* fault_pending_wqh.lock must be hold by the caller */\nstatic inline struct userfaultfd_wait_queue *find_userfault_in(\n\t\twait_queue_head_t *wqh)\n{\n\twait_queue_entry_t *wq;\n\tstruct userfaultfd_wait_queue *uwq;\n\n\tlockdep_assert_held(&wqh->lock);\n\n\tuwq = NULL;\n\tif (!waitqueue_active(wqh))\n\t\tgoto out;\n\t/* walk in reverse to provide FIFO behavior to read userfaults */\n\twq = list_last_entry(&wqh->head, typeof(*wq), entry);\n\tuwq = container_of(wq, struct userfaultfd_wait_queue, wq);\nout:\n\treturn uwq;\n}\n\nstatic inline struct userfaultfd_wait_queue *find_userfault(\n\t\tstruct userfaultfd_ctx *ctx)\n{\n\treturn find_userfault_in(&ctx->fault_pending_wqh);\n}\n\nstatic inline struct userfaultfd_wait_queue *find_userfault_evt(\n\t\tstruct userfaultfd_ctx *ctx)\n{\n\treturn find_userfault_in(&ctx->event_wqh);\n}\n\nstatic __poll_t userfaultfd_poll(struct file *file, poll_table *wait)\n{\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\t__poll_t ret;\n\n\tpoll_wait(file, &ctx->fd_wqh, wait);\n\n\tswitch (ctx->state) {\n\tcase UFFD_STATE_WAIT_API:\n\t\treturn EPOLLERR;\n\tcase UFFD_STATE_RUNNING:\n\t\t/*\n\t\t * poll() never guarantees that read won't block.\n\t\t * userfaults can be waken before they're read().\n\t\t */\n\t\tif (unlikely(!(file->f_flags & O_NONBLOCK)))\n\t\t\treturn EPOLLERR;\n\t\t/*\n\t\t * lockless access to see if there are pending faults\n\t\t * __pollwait last action is the add_wait_queue but\n\t\t * the spin_unlock would allow the waitqueue_active to\n\t\t * pass above the actual list_add inside\n\t\t * add_wait_queue critical section. So use a full\n\t\t * memory barrier to serialize the list_add write of\n\t\t * add_wait_queue() with the waitqueue_active read\n\t\t * below.\n\t\t */\n\t\tret = 0;\n\t\tsmp_mb();\n\t\tif (waitqueue_active(&ctx->fault_pending_wqh))\n\t\t\tret = EPOLLIN;\n\t\telse if (waitqueue_active(&ctx->event_wqh))\n\t\t\tret = EPOLLIN;\n\n\t\treturn ret;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn EPOLLERR;\n\t}\n}\n\nstatic const struct file_operations userfaultfd_fops;\n\nstatic int resolve_userfault_fork(struct userfaultfd_ctx *ctx,\n\t\t\t\t  struct userfaultfd_ctx *new,\n\t\t\t\t  struct uffd_msg *msg)\n{\n\tint fd;\n\n\tfd = anon_inode_getfd(\"[userfaultfd]\", &userfaultfd_fops, new,\n\t\t\t      O_RDWR | (new->flags & UFFD_SHARED_FCNTL_FLAGS));\n\tif (fd < 0)\n\t\treturn fd;\n\n\tmsg->arg.reserved.reserved1 = 0;\n\tmsg->arg.fork.ufd = fd;\n\treturn 0;\n}\n\nstatic ssize_t userfaultfd_ctx_read(struct userfaultfd_ctx *ctx, int no_wait,\n\t\t\t\t    struct uffd_msg *msg)\n{\n\tssize_t ret;\n\tDECLARE_WAITQUEUE(wait, current);\n\tstruct userfaultfd_wait_queue *uwq;\n\t/*\n\t * Handling fork event requires sleeping operations, so\n\t * we drop the event_wqh lock, then do these ops, then\n\t * lock it back and wake up the waiter. While the lock is\n\t * dropped the ewq may go away so we keep track of it\n\t * carefully.\n\t */\n\tLIST_HEAD(fork_event);\n\tstruct userfaultfd_ctx *fork_nctx = NULL;\n\n\t/* always take the fd_wqh lock before the fault_pending_wqh lock */\n\tspin_lock_irq(&ctx->fd_wqh.lock);\n\t__add_wait_queue(&ctx->fd_wqh, &wait);\n\tfor (;;) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t\tuwq = find_userfault(ctx);\n\t\tif (uwq) {\n\t\t\t/*\n\t\t\t * Use a seqcount to repeat the lockless check\n\t\t\t * in wake_userfault() to avoid missing\n\t\t\t * wakeups because during the refile both\n\t\t\t * waitqueue could become empty if this is the\n\t\t\t * only userfault.\n\t\t\t */\n\t\t\twrite_seqcount_begin(&ctx->refile_seq);\n\n\t\t\t/*\n\t\t\t * The fault_pending_wqh.lock prevents the uwq\n\t\t\t * to disappear from under us.\n\t\t\t *\n\t\t\t * Refile this userfault from\n\t\t\t * fault_pending_wqh to fault_wqh, it's not\n\t\t\t * pending anymore after we read it.\n\t\t\t *\n\t\t\t * Use list_del() by hand (as\n\t\t\t * userfaultfd_wake_function also uses\n\t\t\t * list_del_init() by hand) to be sure nobody\n\t\t\t * changes __remove_wait_queue() to use\n\t\t\t * list_del_init() in turn breaking the\n\t\t\t * !list_empty_careful() check in\n\t\t\t * handle_userfault(). The uwq->wq.head list\n\t\t\t * must never be empty at any time during the\n\t\t\t * refile, or the waitqueue could disappear\n\t\t\t * from under us. The \"wait_queue_head_t\"\n\t\t\t * parameter of __remove_wait_queue() is unused\n\t\t\t * anyway.\n\t\t\t */\n\t\t\tlist_del(&uwq->wq.entry);\n\t\t\tadd_wait_queue(&ctx->fault_wqh, &uwq->wq);\n\n\t\t\twrite_seqcount_end(&ctx->refile_seq);\n\n\t\t\t/* careful to always initialize msg if ret == 0 */\n\t\t\t*msg = uwq->msg;\n\t\t\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\t\tspin_lock(&ctx->event_wqh.lock);\n\t\tuwq = find_userfault_evt(ctx);\n\t\tif (uwq) {\n\t\t\t*msg = uwq->msg;\n\n\t\t\tif (uwq->msg.event == UFFD_EVENT_FORK) {\n\t\t\t\tfork_nctx = (struct userfaultfd_ctx *)\n\t\t\t\t\t(unsigned long)\n\t\t\t\t\tuwq->msg.arg.reserved.reserved1;\n\t\t\t\tlist_move(&uwq->wq.entry, &fork_event);\n\t\t\t\t/*\n\t\t\t\t * fork_nctx can be freed as soon as\n\t\t\t\t * we drop the lock, unless we take a\n\t\t\t\t * reference on it.\n\t\t\t\t */\n\t\t\t\tuserfaultfd_ctx_get(fork_nctx);\n\t\t\t\tspin_unlock(&ctx->event_wqh.lock);\n\t\t\t\tret = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tuserfaultfd_event_complete(ctx, uwq);\n\t\t\tspin_unlock(&ctx->event_wqh.lock);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&ctx->event_wqh.lock);\n\n\t\tif (signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (no_wait) {\n\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock_irq(&ctx->fd_wqh.lock);\n\t\tschedule();\n\t\tspin_lock_irq(&ctx->fd_wqh.lock);\n\t}\n\t__remove_wait_queue(&ctx->fd_wqh, &wait);\n\t__set_current_state(TASK_RUNNING);\n\tspin_unlock_irq(&ctx->fd_wqh.lock);\n\n\tif (!ret && msg->event == UFFD_EVENT_FORK) {\n\t\tret = resolve_userfault_fork(ctx, fork_nctx, msg);\n\t\tspin_lock(&ctx->event_wqh.lock);\n\t\tif (!list_empty(&fork_event)) {\n\t\t\t/*\n\t\t\t * The fork thread didn't abort, so we can\n\t\t\t * drop the temporary refcount.\n\t\t\t */\n\t\t\tuserfaultfd_ctx_put(fork_nctx);\n\n\t\t\tuwq = list_first_entry(&fork_event,\n\t\t\t\t\t       typeof(*uwq),\n\t\t\t\t\t       wq.entry);\n\t\t\t/*\n\t\t\t * If fork_event list wasn't empty and in turn\n\t\t\t * the event wasn't already released by fork\n\t\t\t * (the event is allocated on fork kernel\n\t\t\t * stack), put the event back to its place in\n\t\t\t * the event_wq. fork_event head will be freed\n\t\t\t * as soon as we return so the event cannot\n\t\t\t * stay queued there no matter the current\n\t\t\t * \"ret\" value.\n\t\t\t */\n\t\t\tlist_del(&uwq->wq.entry);\n\t\t\t__add_wait_queue(&ctx->event_wqh, &uwq->wq);\n\n\t\t\t/*\n\t\t\t * Leave the event in the waitqueue and report\n\t\t\t * error to userland if we failed to resolve\n\t\t\t * the userfault fork.\n\t\t\t */\n\t\t\tif (likely(!ret))\n\t\t\t\tuserfaultfd_event_complete(ctx, uwq);\n\t\t} else {\n\t\t\t/*\n\t\t\t * Here the fork thread aborted and the\n\t\t\t * refcount from the fork thread on fork_nctx\n\t\t\t * has already been released. We still hold\n\t\t\t * the reference we took before releasing the\n\t\t\t * lock above. If resolve_userfault_fork\n\t\t\t * failed we've to drop it because the\n\t\t\t * fork_nctx has to be freed in such case. If\n\t\t\t * it succeeded we'll hold it because the new\n\t\t\t * uffd references it.\n\t\t\t */\n\t\t\tif (ret)\n\t\t\t\tuserfaultfd_ctx_put(fork_nctx);\n\t\t}\n\t\tspin_unlock(&ctx->event_wqh.lock);\n\t}\n\n\treturn ret;\n}\n\nstatic ssize_t userfaultfd_read(struct file *file, char __user *buf,\n\t\t\t\tsize_t count, loff_t *ppos)\n{\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\tssize_t _ret, ret = 0;\n\tstruct uffd_msg msg;\n\tint no_wait = file->f_flags & O_NONBLOCK;\n\n\tif (ctx->state == UFFD_STATE_WAIT_API)\n\t\treturn -EINVAL;\n\n\tfor (;;) {\n\t\tif (count < sizeof(msg))\n\t\t\treturn ret ? ret : -EINVAL;\n\t\t_ret = userfaultfd_ctx_read(ctx, no_wait, &msg);\n\t\tif (_ret < 0)\n\t\t\treturn ret ? ret : _ret;\n\t\tif (copy_to_user((__u64 __user *) buf, &msg, sizeof(msg)))\n\t\t\treturn ret ? ret : -EFAULT;\n\t\tret += sizeof(msg);\n\t\tbuf += sizeof(msg);\n\t\tcount -= sizeof(msg);\n\t\t/*\n\t\t * Allow to read more than one fault at time but only\n\t\t * block if waiting for the very first one.\n\t\t */\n\t\tno_wait = O_NONBLOCK;\n\t}\n}\n\nstatic void __wake_userfault(struct userfaultfd_ctx *ctx,\n\t\t\t     struct userfaultfd_wake_range *range)\n{\n\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t/* wake all in the range and autoremove */\n\tif (waitqueue_active(&ctx->fault_pending_wqh))\n\t\t__wake_up_locked_key(&ctx->fault_pending_wqh, TASK_NORMAL,\n\t\t\t\t     range);\n\tif (waitqueue_active(&ctx->fault_wqh))\n\t\t__wake_up(&ctx->fault_wqh, TASK_NORMAL, 1, range);\n\tspin_unlock(&ctx->fault_pending_wqh.lock);\n}\n\nstatic __always_inline void wake_userfault(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t   struct userfaultfd_wake_range *range)\n{\n\tunsigned seq;\n\tbool need_wakeup;\n\n\t/*\n\t * To be sure waitqueue_active() is not reordered by the CPU\n\t * before the pagetable update, use an explicit SMP memory\n\t * barrier here. PT lock release or up_read(mmap_sem) still\n\t * have release semantics that can allow the\n\t * waitqueue_active() to be reordered before the pte update.\n\t */\n\tsmp_mb();\n\n\t/*\n\t * Use waitqueue_active because it's very frequent to\n\t * change the address space atomically even if there are no\n\t * userfaults yet. So we take the spinlock only when we're\n\t * sure we've userfaults to wake.\n\t */\n\tdo {\n\t\tseq = read_seqcount_begin(&ctx->refile_seq);\n\t\tneed_wakeup = waitqueue_active(&ctx->fault_pending_wqh) ||\n\t\t\twaitqueue_active(&ctx->fault_wqh);\n\t\tcond_resched();\n\t} while (read_seqcount_retry(&ctx->refile_seq, seq));\n\tif (need_wakeup)\n\t\t__wake_userfault(ctx, range);\n}\n\nstatic __always_inline int validate_range(struct mm_struct *mm,\n\t\t\t\t\t  __u64 start, __u64 len)\n{\n\t__u64 task_size = mm->task_size;\n\n\tif (start & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\tif (len & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\tif (!len)\n\t\treturn -EINVAL;\n\tif (start < mmap_min_addr)\n\t\treturn -EINVAL;\n\tif (start >= task_size)\n\t\treturn -EINVAL;\n\tif (len > task_size - start)\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic inline bool vma_can_userfault(struct vm_area_struct *vma)\n{\n\treturn vma_is_anonymous(vma) || is_vm_hugetlb_page(vma) ||\n\t\tvma_is_shmem(vma);\n}\n\nstatic int userfaultfd_register(struct userfaultfd_ctx *ctx,\n\t\t\t\tunsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_register uffdio_register;\n\tstruct uffdio_register __user *user_uffdio_register;\n\tunsigned long vm_flags, new_flags;\n\tbool found;\n\tbool basic_ioctls;\n\tunsigned long start, end, vma_end;\n\n\tuser_uffdio_register = (struct uffdio_register __user *) arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_register, user_uffdio_register,\n\t\t\t   sizeof(uffdio_register)-sizeof(__u64)))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (!uffdio_register.mode)\n\t\tgoto out;\n\tif (uffdio_register.mode & ~(UFFDIO_REGISTER_MODE_MISSING|\n\t\t\t\t     UFFDIO_REGISTER_MODE_WP))\n\t\tgoto out;\n\tvm_flags = 0;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_MISSING)\n\t\tvm_flags |= VM_UFFD_MISSING;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_WP) {\n\t\tvm_flags |= VM_UFFD_WP;\n\t\t/*\n\t\t * FIXME: remove the below error constraint by\n\t\t * implementing the wprotect tracking mode.\n\t\t */\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = validate_range(mm, uffdio_register.range.start,\n\t\t\t     uffdio_register.range.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_register.range.start;\n\tend = start + uffdio_register.range.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tif (!mmget_still_valid(mm))\n\t\tgoto out_unlock;\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tbasic_ioctls = false;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/* check not compatible vmas */\n\t\tret = -EINVAL;\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * UFFDIO_COPY will fill file holes even without\n\t\t * PROT_WRITE. This check enforces that if this is a\n\t\t * MAP_SHARED, the process has write permission to the backing\n\t\t * file. If VM_MAYWRITE is set it also enforces that on a\n\t\t * MAP_SHARED vma: there is no F_WRITE_SEAL and no further\n\t\t * F_WRITE_SEAL can be taken until the vma is destroyed.\n\t\t */\n\t\tret = -EPERM;\n\t\tif (unlikely(!(cur->vm_flags & VM_MAYWRITE)))\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * If this vma contains ending address, and huge pages\n\t\t * check alignment.\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur) && end <= cur->vm_end &&\n\t\t    end > cur->vm_start) {\n\t\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(cur);\n\n\t\t\tret = -EINVAL;\n\n\t\t\tif (end & (vma_hpagesize - 1))\n\t\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/*\n\t\t * Check that this vma isn't already owned by a\n\t\t * different userfaultfd. We can't allow more than one\n\t\t * userfaultfd to own a single vma simultaneously or we\n\t\t * wouldn't know which one to deliver the userfaults to.\n\t\t */\n\t\tret = -EBUSY;\n\t\tif (cur->vm_userfaultfd_ctx.ctx &&\n\t\t    cur->vm_userfaultfd_ctx.ctx != ctx)\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * Note vmas containing huge pages\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur))\n\t\t\tbasic_ioctls = true;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\t\tBUG_ON(vma->vm_userfaultfd_ctx.ctx &&\n\t\t       vma->vm_userfaultfd_ctx.ctx != ctx);\n\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (vma->vm_userfaultfd_ctx.ctx == ctx &&\n\t\t    (vma->vm_flags & vm_flags) == vm_flags)\n\t\t\tgoto skip;\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tnew_flags = (vma->vm_flags & ~vm_flags) | vm_flags;\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t ((struct vm_userfaultfd_ctx){ ctx }));\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx.ctx = ctx;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\n\tif (!ret) {\n\t\t/*\n\t\t * Now that we scanned all vmas we can already tell\n\t\t * userland which ioctls methods are guaranteed to\n\t\t * succeed on this range.\n\t\t */\n\t\tif (put_user(basic_ioctls ? UFFD_API_RANGE_IOCTLS_BASIC :\n\t\t\t     UFFD_API_RANGE_IOCTLS,\n\t\t\t     &user_uffdio_register->ioctls))\n\t\t\tret = -EFAULT;\n\t}\nout:\n\treturn ret;\n}\n\nstatic int userfaultfd_unregister(struct userfaultfd_ctx *ctx,\n\t\t\t\t  unsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_range uffdio_unregister;\n\tunsigned long new_flags;\n\tbool found;\n\tunsigned long start, end, vma_end;\n\tconst void __user *buf = (void __user *)arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_unregister, buf, sizeof(uffdio_unregister)))\n\t\tgoto out;\n\n\tret = validate_range(mm, uffdio_unregister.start,\n\t\t\t     uffdio_unregister.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_unregister.start;\n\tend = start + uffdio_unregister.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tif (!mmget_still_valid(mm))\n\t\tgoto out_unlock;\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tret = -EINVAL;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/*\n\t\t * Check not compatible vmas, not strictly required\n\t\t * here as not compatible vmas cannot have an\n\t\t * userfaultfd_ctx registered on them, but this\n\t\t * provides for more strict behavior to notice\n\t\t * unregistration errors.\n\t\t */\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (!vma->vm_userfaultfd_ctx.ctx)\n\t\t\tgoto skip;\n\n\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tif (userfaultfd_missing(vma)) {\n\t\t\t/*\n\t\t\t * Wake any concurrent pending userfault while\n\t\t\t * we unregister, so they will not hang\n\t\t\t * permanently and it avoids userland to call\n\t\t\t * UFFDIO_WAKE explicitly.\n\t\t\t */\n\t\t\tstruct userfaultfd_wake_range range;\n\t\t\trange.start = start;\n\t\t\trange.len = vma_end - start;\n\t\t\twake_userfault(vma->vm_userfaultfd_ctx.ctx, &range);\n\t\t}\n\n\t\tnew_flags = vma->vm_flags & ~(VM_UFFD_MISSING | VM_UFFD_WP);\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t NULL_VM_UFFD_CTX);\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\nout:\n\treturn ret;\n}\n\n/*\n * userfaultfd_wake may be used in combination with the\n * UFFDIO_*_MODE_DONTWAKE to wakeup userfaults in batches.\n */\nstatic int userfaultfd_wake(struct userfaultfd_ctx *ctx,\n\t\t\t    unsigned long arg)\n{\n\tint ret;\n\tstruct uffdio_range uffdio_wake;\n\tstruct userfaultfd_wake_range range;\n\tconst void __user *buf = (void __user *)arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_wake, buf, sizeof(uffdio_wake)))\n\t\tgoto out;\n\n\tret = validate_range(ctx->mm, uffdio_wake.start, uffdio_wake.len);\n\tif (ret)\n\t\tgoto out;\n\n\trange.start = uffdio_wake.start;\n\trange.len = uffdio_wake.len;\n\n\t/*\n\t * len == 0 means wake all and we don't want to wake all here,\n\t * so check it again to be sure.\n\t */\n\tVM_BUG_ON(!range.len);\n\n\twake_userfault(ctx, &range);\n\tret = 0;\n\nout:\n\treturn ret;\n}\n\nstatic int userfaultfd_copy(struct userfaultfd_ctx *ctx,\n\t\t\t    unsigned long arg)\n{\n\t__s64 ret;\n\tstruct uffdio_copy uffdio_copy;\n\tstruct uffdio_copy __user *user_uffdio_copy;\n\tstruct userfaultfd_wake_range range;\n\n\tuser_uffdio_copy = (struct uffdio_copy __user *) arg;\n\n\tret = -EAGAIN;\n\tif (READ_ONCE(ctx->mmap_changing))\n\t\tgoto out;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_copy, user_uffdio_copy,\n\t\t\t   /* don't copy \"copy\" last field */\n\t\t\t   sizeof(uffdio_copy)-sizeof(__s64)))\n\t\tgoto out;\n\n\tret = validate_range(ctx->mm, uffdio_copy.dst, uffdio_copy.len);\n\tif (ret)\n\t\tgoto out;\n\t/*\n\t * double check for wraparound just in case. copy_from_user()\n\t * will later check uffdio_copy.src + uffdio_copy.len to fit\n\t * in the userland range.\n\t */\n\tret = -EINVAL;\n\tif (uffdio_copy.src + uffdio_copy.len <= uffdio_copy.src)\n\t\tgoto out;\n\tif (uffdio_copy.mode & ~UFFDIO_COPY_MODE_DONTWAKE)\n\t\tgoto out;\n\tif (mmget_not_zero(ctx->mm)) {\n\t\tret = mcopy_atomic(ctx->mm, uffdio_copy.dst, uffdio_copy.src,\n\t\t\t\t   uffdio_copy.len, &ctx->mmap_changing);\n\t\tmmput(ctx->mm);\n\t} else {\n\t\treturn -ESRCH;\n\t}\n\tif (unlikely(put_user(ret, &user_uffdio_copy->copy)))\n\t\treturn -EFAULT;\n\tif (ret < 0)\n\t\tgoto out;\n\tBUG_ON(!ret);\n\t/* len == 0 would wake all */\n\trange.len = ret;\n\tif (!(uffdio_copy.mode & UFFDIO_COPY_MODE_DONTWAKE)) {\n\t\trange.start = uffdio_copy.dst;\n\t\twake_userfault(ctx, &range);\n\t}\n\tret = range.len == uffdio_copy.len ? 0 : -EAGAIN;\nout:\n\treturn ret;\n}\n\nstatic int userfaultfd_zeropage(struct userfaultfd_ctx *ctx,\n\t\t\t\tunsigned long arg)\n{\n\t__s64 ret;\n\tstruct uffdio_zeropage uffdio_zeropage;\n\tstruct uffdio_zeropage __user *user_uffdio_zeropage;\n\tstruct userfaultfd_wake_range range;\n\n\tuser_uffdio_zeropage = (struct uffdio_zeropage __user *) arg;\n\n\tret = -EAGAIN;\n\tif (READ_ONCE(ctx->mmap_changing))\n\t\tgoto out;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_zeropage, user_uffdio_zeropage,\n\t\t\t   /* don't copy \"zeropage\" last field */\n\t\t\t   sizeof(uffdio_zeropage)-sizeof(__s64)))\n\t\tgoto out;\n\n\tret = validate_range(ctx->mm, uffdio_zeropage.range.start,\n\t\t\t     uffdio_zeropage.range.len);\n\tif (ret)\n\t\tgoto out;\n\tret = -EINVAL;\n\tif (uffdio_zeropage.mode & ~UFFDIO_ZEROPAGE_MODE_DONTWAKE)\n\t\tgoto out;\n\n\tif (mmget_not_zero(ctx->mm)) {\n\t\tret = mfill_zeropage(ctx->mm, uffdio_zeropage.range.start,\n\t\t\t\t     uffdio_zeropage.range.len,\n\t\t\t\t     &ctx->mmap_changing);\n\t\tmmput(ctx->mm);\n\t} else {\n\t\treturn -ESRCH;\n\t}\n\tif (unlikely(put_user(ret, &user_uffdio_zeropage->zeropage)))\n\t\treturn -EFAULT;\n\tif (ret < 0)\n\t\tgoto out;\n\t/* len == 0 would wake all */\n\tBUG_ON(!ret);\n\trange.len = ret;\n\tif (!(uffdio_zeropage.mode & UFFDIO_ZEROPAGE_MODE_DONTWAKE)) {\n\t\trange.start = uffdio_zeropage.range.start;\n\t\twake_userfault(ctx, &range);\n\t}\n\tret = range.len == uffdio_zeropage.range.len ? 0 : -EAGAIN;\nout:\n\treturn ret;\n}\n\nstatic inline unsigned int uffd_ctx_features(__u64 user_features)\n{\n\t/*\n\t * For the current set of features the bits just coincide\n\t */\n\treturn (unsigned int)user_features;\n}\n\n/*\n * userland asks for a certain API version and we return which bits\n * and ioctl commands are implemented in this kernel for such API\n * version or -EINVAL if unknown.\n */\nstatic int userfaultfd_api(struct userfaultfd_ctx *ctx,\n\t\t\t   unsigned long arg)\n{\n\tstruct uffdio_api uffdio_api;\n\tvoid __user *buf = (void __user *)arg;\n\tint ret;\n\t__u64 features;\n\n\tret = -EINVAL;\n\tif (ctx->state != UFFD_STATE_WAIT_API)\n\t\tgoto out;\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_api, buf, sizeof(uffdio_api)))\n\t\tgoto out;\n\tfeatures = uffdio_api.features;\n\tif (uffdio_api.api != UFFD_API || (features & ~UFFD_API_FEATURES)) {\n\t\tmemset(&uffdio_api, 0, sizeof(uffdio_api));\n\t\tif (copy_to_user(buf, &uffdio_api, sizeof(uffdio_api)))\n\t\t\tgoto out;\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\t/* report all available features and ioctls to userland */\n\tuffdio_api.features = UFFD_API_FEATURES;\n\tuffdio_api.ioctls = UFFD_API_IOCTLS;\n\tret = -EFAULT;\n\tif (copy_to_user(buf, &uffdio_api, sizeof(uffdio_api)))\n\t\tgoto out;\n\tctx->state = UFFD_STATE_RUNNING;\n\t/* only enable the requested features for this uffd context */\n\tctx->features = uffd_ctx_features(features);\n\tret = 0;\nout:\n\treturn ret;\n}\n\nstatic long userfaultfd_ioctl(struct file *file, unsigned cmd,\n\t\t\t      unsigned long arg)\n{\n\tint ret = -EINVAL;\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\n\tif (cmd != UFFDIO_API && ctx->state == UFFD_STATE_WAIT_API)\n\t\treturn -EINVAL;\n\n\tswitch(cmd) {\n\tcase UFFDIO_API:\n\t\tret = userfaultfd_api(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_REGISTER:\n\t\tret = userfaultfd_register(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_UNREGISTER:\n\t\tret = userfaultfd_unregister(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_WAKE:\n\t\tret = userfaultfd_wake(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_COPY:\n\t\tret = userfaultfd_copy(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_ZEROPAGE:\n\t\tret = userfaultfd_zeropage(ctx, arg);\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\n#ifdef CONFIG_PROC_FS\nstatic void userfaultfd_show_fdinfo(struct seq_file *m, struct file *f)\n{\n\tstruct userfaultfd_ctx *ctx = f->private_data;\n\twait_queue_entry_t *wq;\n\tunsigned long pending = 0, total = 0;\n\n\tspin_lock(&ctx->fault_pending_wqh.lock);\n\tlist_for_each_entry(wq, &ctx->fault_pending_wqh.head, entry) {\n\t\tpending++;\n\t\ttotal++;\n\t}\n\tlist_for_each_entry(wq, &ctx->fault_wqh.head, entry) {\n\t\ttotal++;\n\t}\n\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\t/*\n\t * If more protocols will be added, there will be all shown\n\t * separated by a space. Like this:\n\t *\tprotocols: aa:... bb:...\n\t */\n\tseq_printf(m, \"pending:\\t%lu\\ntotal:\\t%lu\\nAPI:\\t%Lx:%x:%Lx\\n\",\n\t\t   pending, total, UFFD_API, ctx->features,\n\t\t   UFFD_API_IOCTLS|UFFD_API_RANGE_IOCTLS);\n}\n#endif\n\nstatic const struct file_operations userfaultfd_fops = {\n#ifdef CONFIG_PROC_FS\n\t.show_fdinfo\t= userfaultfd_show_fdinfo,\n#endif\n\t.release\t= userfaultfd_release,\n\t.poll\t\t= userfaultfd_poll,\n\t.read\t\t= userfaultfd_read,\n\t.unlocked_ioctl = userfaultfd_ioctl,\n\t.compat_ioctl\t= userfaultfd_ioctl,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic void init_once_userfaultfd_ctx(void *mem)\n{\n\tstruct userfaultfd_ctx *ctx = (struct userfaultfd_ctx *) mem;\n\n\tinit_waitqueue_head(&ctx->fault_pending_wqh);\n\tinit_waitqueue_head(&ctx->fault_wqh);\n\tinit_waitqueue_head(&ctx->event_wqh);\n\tinit_waitqueue_head(&ctx->fd_wqh);\n\tseqcount_init(&ctx->refile_seq);\n}\n\nSYSCALL_DEFINE1(userfaultfd, int, flags)\n{\n\tstruct userfaultfd_ctx *ctx;\n\tint fd;\n\n\tBUG_ON(!current->mm);\n\n\t/* Check the UFFD_* constants for consistency.  */\n\tBUILD_BUG_ON(UFFD_CLOEXEC != O_CLOEXEC);\n\tBUILD_BUG_ON(UFFD_NONBLOCK != O_NONBLOCK);\n\n\tif (flags & ~UFFD_SHARED_FCNTL_FLAGS)\n\t\treturn -EINVAL;\n\n\tctx = kmem_cache_alloc(userfaultfd_ctx_cachep, GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\trefcount_set(&ctx->refcount, 1);\n\tctx->flags = flags;\n\tctx->features = 0;\n\tctx->state = UFFD_STATE_WAIT_API;\n\tctx->released = false;\n\tctx->mmap_changing = false;\n\tctx->mm = current->mm;\n\t/* prevent the mm struct to be freed */\n\tmmgrab(ctx->mm);\n\n\tfd = anon_inode_getfd(\"[userfaultfd]\", &userfaultfd_fops, ctx,\n\t\t\t      O_RDWR | (flags & UFFD_SHARED_FCNTL_FLAGS));\n\tif (fd < 0) {\n\t\tmmdrop(ctx->mm);\n\t\tkmem_cache_free(userfaultfd_ctx_cachep, ctx);\n\t}\n\treturn fd;\n}\n\nstatic int __init userfaultfd_init(void)\n{\n\tuserfaultfd_ctx_cachep = kmem_cache_create(\"userfaultfd_ctx_cache\",\n\t\t\t\t\t\tsizeof(struct userfaultfd_ctx),\n\t\t\t\t\t\t0,\n\t\t\t\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC,\n\t\t\t\t\t\tinit_once_userfaultfd_ctx);\n\treturn 0;\n}\n__initcall(userfaultfd_init);\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_SCHED_MM_H\n#define _LINUX_SCHED_MM_H\n\n#include <linux/kernel.h>\n#include <linux/atomic.h>\n#include <linux/sched.h>\n#include <linux/mm_types.h>\n#include <linux/gfp.h>\n#include <linux/sync_core.h>\n\n/*\n * Routines for handling mm_structs\n */\nextern struct mm_struct *mm_alloc(void);\n\n/**\n * mmgrab() - Pin a &struct mm_struct.\n * @mm: The &struct mm_struct to pin.\n *\n * Make sure that @mm will not get freed even after the owning task\n * exits. This doesn't guarantee that the associated address space\n * will still exist later on and mmget_not_zero() has to be used before\n * accessing it.\n *\n * This is a preferred way to to pin @mm for a longer/unbounded amount\n * of time.\n *\n * Use mmdrop() to release the reference acquired by mmgrab().\n *\n * See also <Documentation/vm/active_mm.rst> for an in-depth explanation\n * of &mm_struct.mm_count vs &mm_struct.mm_users.\n */\nstatic inline void mmgrab(struct mm_struct *mm)\n{\n\tatomic_inc(&mm->mm_count);\n}\n\nextern void __mmdrop(struct mm_struct *mm);\n\nstatic inline void mmdrop(struct mm_struct *mm)\n{\n\t/*\n\t * The implicit full barrier implied by atomic_dec_and_test() is\n\t * required by the membarrier system call before returning to\n\t * user-space, after storing to rq->curr.\n\t */\n\tif (unlikely(atomic_dec_and_test(&mm->mm_count)))\n\t\t__mmdrop(mm);\n}\n\n/*\n * This has to be called after a get_task_mm()/mmget_not_zero()\n * followed by taking the mmap_sem for writing before modifying the\n * vmas or anything the coredump pretends not to change from under it.\n *\n * NOTE: find_extend_vma() called from GUP context is the only place\n * that can modify the \"mm\" (notably the vm_start/end) under mmap_sem\n * for reading and outside the context of the process, so it is also\n * the only case that holds the mmap_sem for reading that must call\n * this function. Generally if the mmap_sem is hold for reading\n * there's no need of this check after get_task_mm()/mmget_not_zero().\n *\n * This function can be obsoleted and the check can be removed, after\n * the coredump code will hold the mmap_sem for writing before\n * invoking the ->core_dump methods.\n */\nstatic inline bool mmget_still_valid(struct mm_struct *mm)\n{\n\treturn likely(!mm->core_state);\n}\n\n/**\n * mmget() - Pin the address space associated with a &struct mm_struct.\n * @mm: The address space to pin.\n *\n * Make sure that the address space of the given &struct mm_struct doesn't\n * go away. This does not protect against parts of the address space being\n * modified or freed, however.\n *\n * Never use this function to pin this address space for an\n * unbounded/indefinite amount of time.\n *\n * Use mmput() to release the reference acquired by mmget().\n *\n * See also <Documentation/vm/active_mm.rst> for an in-depth explanation\n * of &mm_struct.mm_count vs &mm_struct.mm_users.\n */\nstatic inline void mmget(struct mm_struct *mm)\n{\n\tatomic_inc(&mm->mm_users);\n}\n\nstatic inline bool mmget_not_zero(struct mm_struct *mm)\n{\n\treturn atomic_inc_not_zero(&mm->mm_users);\n}\n\n/* mmput gets rid of the mappings and all user-space */\nextern void mmput(struct mm_struct *);\n#ifdef CONFIG_MMU\n/* same as above but performs the slow path from the async context. Can\n * be called from the atomic context as well\n */\nvoid mmput_async(struct mm_struct *);\n#endif\n\n/* Grab a reference to a task's mm, if it is not already going away */\nextern struct mm_struct *get_task_mm(struct task_struct *task);\n/*\n * Grab a reference to a task's mm, if it is not already going away\n * and ptrace_may_access with the mode parameter passed to it\n * succeeds.\n */\nextern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode);\n/* Remove the current tasks stale references to the old mm_struct */\nextern void mm_release(struct task_struct *, struct mm_struct *);\n\n#ifdef CONFIG_MEMCG\nextern void mm_update_next_owner(struct mm_struct *mm);\n#else\nstatic inline void mm_update_next_owner(struct mm_struct *mm)\n{\n}\n#endif /* CONFIG_MEMCG */\n\n#ifdef CONFIG_MMU\nextern void arch_pick_mmap_layout(struct mm_struct *mm,\n\t\t\t\t  struct rlimit *rlim_stack);\nextern unsigned long\narch_get_unmapped_area(struct file *, unsigned long, unsigned long,\n\t\t       unsigned long, unsigned long);\nextern unsigned long\narch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,\n\t\t\t  unsigned long len, unsigned long pgoff,\n\t\t\t  unsigned long flags);\n#else\nstatic inline void arch_pick_mmap_layout(struct mm_struct *mm,\n\t\t\t\t\t struct rlimit *rlim_stack) {}\n#endif\n\nstatic inline bool in_vfork(struct task_struct *tsk)\n{\n\tbool ret;\n\n\t/*\n\t * need RCU to access ->real_parent if CLONE_VM was used along with\n\t * CLONE_PARENT.\n\t *\n\t * We check real_parent->mm == tsk->mm because CLONE_VFORK does not\n\t * imply CLONE_VM\n\t *\n\t * CLONE_VFORK can be used with CLONE_PARENT/CLONE_THREAD and thus\n\t * ->real_parent is not necessarily the task doing vfork(), so in\n\t * theory we can't rely on task_lock() if we want to dereference it.\n\t *\n\t * And in this case we can't trust the real_parent->mm == tsk->mm\n\t * check, it can be false negative. But we do not care, if init or\n\t * another oom-unkillable task does this it should blame itself.\n\t */\n\trcu_read_lock();\n\tret = tsk->vfork_done && tsk->real_parent->mm == tsk->mm;\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n/*\n * Applies per-task gfp context to the given allocation flags.\n * PF_MEMALLOC_NOIO implies GFP_NOIO\n * PF_MEMALLOC_NOFS implies GFP_NOFS\n * PF_MEMALLOC_NOCMA implies no allocation from CMA region.\n */\nstatic inline gfp_t current_gfp_context(gfp_t flags)\n{\n\tif (unlikely(current->flags &\n\t\t     (PF_MEMALLOC_NOIO | PF_MEMALLOC_NOFS | PF_MEMALLOC_NOCMA))) {\n\t\t/*\n\t\t * NOIO implies both NOIO and NOFS and it is a weaker context\n\t\t * so always make sure it makes precedence\n\t\t */\n\t\tif (current->flags & PF_MEMALLOC_NOIO)\n\t\t\tflags &= ~(__GFP_IO | __GFP_FS);\n\t\telse if (current->flags & PF_MEMALLOC_NOFS)\n\t\t\tflags &= ~__GFP_FS;\n#ifdef CONFIG_CMA\n\t\tif (current->flags & PF_MEMALLOC_NOCMA)\n\t\t\tflags &= ~__GFP_MOVABLE;\n#endif\n\t}\n\treturn flags;\n}\n\n#ifdef CONFIG_LOCKDEP\nextern void __fs_reclaim_acquire(void);\nextern void __fs_reclaim_release(void);\nextern void fs_reclaim_acquire(gfp_t gfp_mask);\nextern void fs_reclaim_release(gfp_t gfp_mask);\n#else\nstatic inline void __fs_reclaim_acquire(void) { }\nstatic inline void __fs_reclaim_release(void) { }\nstatic inline void fs_reclaim_acquire(gfp_t gfp_mask) { }\nstatic inline void fs_reclaim_release(gfp_t gfp_mask) { }\n#endif\n\n/**\n * memalloc_noio_save - Marks implicit GFP_NOIO allocation scope.\n *\n * This functions marks the beginning of the GFP_NOIO allocation scope.\n * All further allocations will implicitly drop __GFP_IO flag and so\n * they are safe for the IO critical section from the allocation recursion\n * point of view. Use memalloc_noio_restore to end the scope with flags\n * returned by this function.\n *\n * This function is safe to be used from any context.\n */\nstatic inline unsigned int memalloc_noio_save(void)\n{\n\tunsigned int flags = current->flags & PF_MEMALLOC_NOIO;\n\tcurrent->flags |= PF_MEMALLOC_NOIO;\n\treturn flags;\n}\n\n/**\n * memalloc_noio_restore - Ends the implicit GFP_NOIO scope.\n * @flags: Flags to restore.\n *\n * Ends the implicit GFP_NOIO scope started by memalloc_noio_save function.\n * Always make sure that that the given flags is the return value from the\n * pairing memalloc_noio_save call.\n */\nstatic inline void memalloc_noio_restore(unsigned int flags)\n{\n\tcurrent->flags = (current->flags & ~PF_MEMALLOC_NOIO) | flags;\n}\n\n/**\n * memalloc_nofs_save - Marks implicit GFP_NOFS allocation scope.\n *\n * This functions marks the beginning of the GFP_NOFS allocation scope.\n * All further allocations will implicitly drop __GFP_FS flag and so\n * they are safe for the FS critical section from the allocation recursion\n * point of view. Use memalloc_nofs_restore to end the scope with flags\n * returned by this function.\n *\n * This function is safe to be used from any context.\n */\nstatic inline unsigned int memalloc_nofs_save(void)\n{\n\tunsigned int flags = current->flags & PF_MEMALLOC_NOFS;\n\tcurrent->flags |= PF_MEMALLOC_NOFS;\n\treturn flags;\n}\n\n/**\n * memalloc_nofs_restore - Ends the implicit GFP_NOFS scope.\n * @flags: Flags to restore.\n *\n * Ends the implicit GFP_NOFS scope started by memalloc_nofs_save function.\n * Always make sure that that the given flags is the return value from the\n * pairing memalloc_nofs_save call.\n */\nstatic inline void memalloc_nofs_restore(unsigned int flags)\n{\n\tcurrent->flags = (current->flags & ~PF_MEMALLOC_NOFS) | flags;\n}\n\nstatic inline unsigned int memalloc_noreclaim_save(void)\n{\n\tunsigned int flags = current->flags & PF_MEMALLOC;\n\tcurrent->flags |= PF_MEMALLOC;\n\treturn flags;\n}\n\nstatic inline void memalloc_noreclaim_restore(unsigned int flags)\n{\n\tcurrent->flags = (current->flags & ~PF_MEMALLOC) | flags;\n}\n\n#ifdef CONFIG_CMA\nstatic inline unsigned int memalloc_nocma_save(void)\n{\n\tunsigned int flags = current->flags & PF_MEMALLOC_NOCMA;\n\n\tcurrent->flags |= PF_MEMALLOC_NOCMA;\n\treturn flags;\n}\n\nstatic inline void memalloc_nocma_restore(unsigned int flags)\n{\n\tcurrent->flags = (current->flags & ~PF_MEMALLOC_NOCMA) | flags;\n}\n#else\nstatic inline unsigned int memalloc_nocma_save(void)\n{\n\treturn 0;\n}\n\nstatic inline void memalloc_nocma_restore(unsigned int flags)\n{\n}\n#endif\n\n#ifdef CONFIG_MEMCG\n/**\n * memalloc_use_memcg - Starts the remote memcg charging scope.\n * @memcg: memcg to charge.\n *\n * This function marks the beginning of the remote memcg charging scope. All the\n * __GFP_ACCOUNT allocations till the end of the scope will be charged to the\n * given memcg.\n *\n * NOTE: This function is not nesting safe.\n */\nstatic inline void memalloc_use_memcg(struct mem_cgroup *memcg)\n{\n\tWARN_ON_ONCE(current->active_memcg);\n\tcurrent->active_memcg = memcg;\n}\n\n/**\n * memalloc_unuse_memcg - Ends the remote memcg charging scope.\n *\n * This function marks the end of the remote memcg charging scope started by\n * memalloc_use_memcg().\n */\nstatic inline void memalloc_unuse_memcg(void)\n{\n\tcurrent->active_memcg = NULL;\n}\n#else\nstatic inline void memalloc_use_memcg(struct mem_cgroup *memcg)\n{\n}\n\nstatic inline void memalloc_unuse_memcg(void)\n{\n}\n#endif\n\n#ifdef CONFIG_MEMBARRIER\nenum {\n\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_READY\t\t= (1U << 0),\n\tMEMBARRIER_STATE_PRIVATE_EXPEDITED\t\t\t= (1U << 1),\n\tMEMBARRIER_STATE_GLOBAL_EXPEDITED_READY\t\t\t= (1U << 2),\n\tMEMBARRIER_STATE_GLOBAL_EXPEDITED\t\t\t= (1U << 3),\n\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY\t= (1U << 4),\n\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE\t\t= (1U << 5),\n};\n\nenum {\n\tMEMBARRIER_FLAG_SYNC_CORE\t= (1U << 0),\n};\n\n#ifdef CONFIG_ARCH_HAS_MEMBARRIER_CALLBACKS\n#include <asm/membarrier.h>\n#endif\n\nstatic inline void membarrier_mm_sync_core_before_usermode(struct mm_struct *mm)\n{\n\tif (likely(!(atomic_read(&mm->membarrier_state) &\n\t\t     MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE)))\n\t\treturn;\n\tsync_core_before_usermode();\n}\n\nstatic inline void membarrier_execve(struct task_struct *t)\n{\n\tatomic_set(&t->mm->membarrier_state, 0);\n}\n#else\n#ifdef CONFIG_ARCH_HAS_MEMBARRIER_CALLBACKS\nstatic inline void membarrier_arch_switch_mm(struct mm_struct *prev,\n\t\t\t\t\t     struct mm_struct *next,\n\t\t\t\t\t     struct task_struct *tsk)\n{\n}\n#endif\nstatic inline void membarrier_execve(struct task_struct *t)\n{\n}\nstatic inline void membarrier_mm_sync_core_before_usermode(struct mm_struct *mm)\n{\n}\n#endif\n\n#endif /* _LINUX_SCHED_MM_H */\n", "/*\n * mm/mmap.c\n *\n * Written by obz.\n *\n * Address space accounting code\t<alan@lxorguk.ukuu.org.uk>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/backing-dev.h>\n#include <linux/mm.h>\n#include <linux/vmacache.h>\n#include <linux/shm.h>\n#include <linux/mman.h>\n#include <linux/pagemap.h>\n#include <linux/swap.h>\n#include <linux/syscalls.h>\n#include <linux/capability.h>\n#include <linux/init.h>\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/personality.h>\n#include <linux/security.h>\n#include <linux/hugetlb.h>\n#include <linux/shmem_fs.h>\n#include <linux/profile.h>\n#include <linux/export.h>\n#include <linux/mount.h>\n#include <linux/mempolicy.h>\n#include <linux/rmap.h>\n#include <linux/mmu_notifier.h>\n#include <linux/mmdebug.h>\n#include <linux/perf_event.h>\n#include <linux/audit.h>\n#include <linux/khugepaged.h>\n#include <linux/uprobes.h>\n#include <linux/rbtree_augmented.h>\n#include <linux/notifier.h>\n#include <linux/memory.h>\n#include <linux/printk.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/moduleparam.h>\n#include <linux/pkeys.h>\n#include <linux/oom.h>\n#include <linux/sched/mm.h>\n\n#include <linux/uaccess.h>\n#include <asm/cacheflush.h>\n#include <asm/tlb.h>\n#include <asm/mmu_context.h>\n\n#include \"internal.h\"\n\n#ifndef arch_mmap_check\n#define arch_mmap_check(addr, len, flags)\t(0)\n#endif\n\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_BITS\nconst int mmap_rnd_bits_min = CONFIG_ARCH_MMAP_RND_BITS_MIN;\nconst int mmap_rnd_bits_max = CONFIG_ARCH_MMAP_RND_BITS_MAX;\nint mmap_rnd_bits __read_mostly = CONFIG_ARCH_MMAP_RND_BITS;\n#endif\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_COMPAT_BITS\nconst int mmap_rnd_compat_bits_min = CONFIG_ARCH_MMAP_RND_COMPAT_BITS_MIN;\nconst int mmap_rnd_compat_bits_max = CONFIG_ARCH_MMAP_RND_COMPAT_BITS_MAX;\nint mmap_rnd_compat_bits __read_mostly = CONFIG_ARCH_MMAP_RND_COMPAT_BITS;\n#endif\n\nstatic bool ignore_rlimit_data;\ncore_param(ignore_rlimit_data, ignore_rlimit_data, bool, 0644);\n\nstatic void unmap_region(struct mm_struct *mm,\n\t\tstruct vm_area_struct *vma, struct vm_area_struct *prev,\n\t\tunsigned long start, unsigned long end);\n\n/* description of effects of mapping type and prot in current implementation.\n * this is due to the limited x86 page protection hardware.  The expected\n * behavior is in parens:\n *\n * map_type\tprot\n *\t\tPROT_NONE\tPROT_READ\tPROT_WRITE\tPROT_EXEC\n * MAP_SHARED\tr: (no) no\tr: (yes) yes\tr: (no) yes\tr: (no) yes\n *\t\tw: (no) no\tw: (no) no\tw: (yes) yes\tw: (no) no\n *\t\tx: (no) no\tx: (no) yes\tx: (no) yes\tx: (yes) yes\n *\n * MAP_PRIVATE\tr: (no) no\tr: (yes) yes\tr: (no) yes\tr: (no) yes\n *\t\tw: (no) no\tw: (no) no\tw: (copy) copy\tw: (no) no\n *\t\tx: (no) no\tx: (no) yes\tx: (no) yes\tx: (yes) yes\n *\n * On arm64, PROT_EXEC has the following behaviour for both MAP_SHARED and\n * MAP_PRIVATE:\n *\t\t\t\t\t\t\t\tr: (no) no\n *\t\t\t\t\t\t\t\tw: (no) no\n *\t\t\t\t\t\t\t\tx: (yes) yes\n */\npgprot_t protection_map[16] __ro_after_init = {\n\t__P000, __P001, __P010, __P011, __P100, __P101, __P110, __P111,\n\t__S000, __S001, __S010, __S011, __S100, __S101, __S110, __S111\n};\n\n#ifndef CONFIG_ARCH_HAS_FILTER_PGPROT\nstatic inline pgprot_t arch_filter_pgprot(pgprot_t prot)\n{\n\treturn prot;\n}\n#endif\n\npgprot_t vm_get_page_prot(unsigned long vm_flags)\n{\n\tpgprot_t ret = __pgprot(pgprot_val(protection_map[vm_flags &\n\t\t\t\t(VM_READ|VM_WRITE|VM_EXEC|VM_SHARED)]) |\n\t\t\tpgprot_val(arch_vm_get_page_prot(vm_flags)));\n\n\treturn arch_filter_pgprot(ret);\n}\nEXPORT_SYMBOL(vm_get_page_prot);\n\nstatic pgprot_t vm_pgprot_modify(pgprot_t oldprot, unsigned long vm_flags)\n{\n\treturn pgprot_modify(oldprot, vm_get_page_prot(vm_flags));\n}\n\n/* Update vma->vm_page_prot to reflect vma->vm_flags. */\nvoid vma_set_page_prot(struct vm_area_struct *vma)\n{\n\tunsigned long vm_flags = vma->vm_flags;\n\tpgprot_t vm_page_prot;\n\n\tvm_page_prot = vm_pgprot_modify(vma->vm_page_prot, vm_flags);\n\tif (vma_wants_writenotify(vma, vm_page_prot)) {\n\t\tvm_flags &= ~VM_SHARED;\n\t\tvm_page_prot = vm_pgprot_modify(vm_page_prot, vm_flags);\n\t}\n\t/* remove_protection_ptes reads vma->vm_page_prot without mmap_sem */\n\tWRITE_ONCE(vma->vm_page_prot, vm_page_prot);\n}\n\n/*\n * Requires inode->i_mapping->i_mmap_rwsem\n */\nstatic void __remove_shared_vm_struct(struct vm_area_struct *vma,\n\t\tstruct file *file, struct address_space *mapping)\n{\n\tif (vma->vm_flags & VM_DENYWRITE)\n\t\tatomic_inc(&file_inode(file)->i_writecount);\n\tif (vma->vm_flags & VM_SHARED)\n\t\tmapping_unmap_writable(mapping);\n\n\tflush_dcache_mmap_lock(mapping);\n\tvma_interval_tree_remove(vma, &mapping->i_mmap);\n\tflush_dcache_mmap_unlock(mapping);\n}\n\n/*\n * Unlink a file-based vm structure from its interval tree, to hide\n * vma from rmap and vmtruncate before freeing its page tables.\n */\nvoid unlink_file_vma(struct vm_area_struct *vma)\n{\n\tstruct file *file = vma->vm_file;\n\n\tif (file) {\n\t\tstruct address_space *mapping = file->f_mapping;\n\t\ti_mmap_lock_write(mapping);\n\t\t__remove_shared_vm_struct(vma, file, mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n}\n\n/*\n * Close a vm structure and free it, returning the next.\n */\nstatic struct vm_area_struct *remove_vma(struct vm_area_struct *vma)\n{\n\tstruct vm_area_struct *next = vma->vm_next;\n\n\tmight_sleep();\n\tif (vma->vm_ops && vma->vm_ops->close)\n\t\tvma->vm_ops->close(vma);\n\tif (vma->vm_file)\n\t\tfput(vma->vm_file);\n\tmpol_put(vma_policy(vma));\n\tvm_area_free(vma);\n\treturn next;\n}\n\nstatic int do_brk_flags(unsigned long addr, unsigned long request, unsigned long flags,\n\t\tstruct list_head *uf);\nSYSCALL_DEFINE1(brk, unsigned long, brk)\n{\n\tunsigned long retval;\n\tunsigned long newbrk, oldbrk, origbrk;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *next;\n\tunsigned long min_brk;\n\tbool populate;\n\tbool downgraded = false;\n\tLIST_HEAD(uf);\n\n\tif (down_write_killable(&mm->mmap_sem))\n\t\treturn -EINTR;\n\n\torigbrk = mm->brk;\n\n#ifdef CONFIG_COMPAT_BRK\n\t/*\n\t * CONFIG_COMPAT_BRK can still be overridden by setting\n\t * randomize_va_space to 2, which will still cause mm->start_brk\n\t * to be arbitrarily shifted\n\t */\n\tif (current->brk_randomized)\n\t\tmin_brk = mm->start_brk;\n\telse\n\t\tmin_brk = mm->end_data;\n#else\n\tmin_brk = mm->start_brk;\n#endif\n\tif (brk < min_brk)\n\t\tgoto out;\n\n\t/*\n\t * Check against rlimit here. If this check is done later after the test\n\t * of oldbrk with newbrk then it can escape the test and let the data\n\t * segment grow beyond its set limit the in case where the limit is\n\t * not page aligned -Ram Gupta\n\t */\n\tif (check_data_rlimit(rlimit(RLIMIT_DATA), brk, mm->start_brk,\n\t\t\t      mm->end_data, mm->start_data))\n\t\tgoto out;\n\n\tnewbrk = PAGE_ALIGN(brk);\n\toldbrk = PAGE_ALIGN(mm->brk);\n\tif (oldbrk == newbrk) {\n\t\tmm->brk = brk;\n\t\tgoto success;\n\t}\n\n\t/*\n\t * Always allow shrinking brk.\n\t * __do_munmap() may downgrade mmap_sem to read.\n\t */\n\tif (brk <= mm->brk) {\n\t\tint ret;\n\n\t\t/*\n\t\t * mm->brk must to be protected by write mmap_sem so update it\n\t\t * before downgrading mmap_sem. When __do_munmap() fails,\n\t\t * mm->brk will be restored from origbrk.\n\t\t */\n\t\tmm->brk = brk;\n\t\tret = __do_munmap(mm, newbrk, oldbrk-newbrk, &uf, true);\n\t\tif (ret < 0) {\n\t\t\tmm->brk = origbrk;\n\t\t\tgoto out;\n\t\t} else if (ret == 1) {\n\t\t\tdowngraded = true;\n\t\t}\n\t\tgoto success;\n\t}\n\n\t/* Check against existing mmap mappings. */\n\tnext = find_vma(mm, oldbrk);\n\tif (next && newbrk + PAGE_SIZE > vm_start_gap(next))\n\t\tgoto out;\n\n\t/* Ok, looks good - let it rip. */\n\tif (do_brk_flags(oldbrk, newbrk-oldbrk, 0, &uf) < 0)\n\t\tgoto out;\n\tmm->brk = brk;\n\nsuccess:\n\tpopulate = newbrk > oldbrk && (mm->def_flags & VM_LOCKED) != 0;\n\tif (downgraded)\n\t\tup_read(&mm->mmap_sem);\n\telse\n\t\tup_write(&mm->mmap_sem);\n\tuserfaultfd_unmap_complete(mm, &uf);\n\tif (populate)\n\t\tmm_populate(oldbrk, newbrk - oldbrk);\n\treturn brk;\n\nout:\n\tretval = origbrk;\n\tup_write(&mm->mmap_sem);\n\treturn retval;\n}\n\nstatic long vma_compute_subtree_gap(struct vm_area_struct *vma)\n{\n\tunsigned long max, prev_end, subtree_gap;\n\n\t/*\n\t * Note: in the rare case of a VM_GROWSDOWN above a VM_GROWSUP, we\n\t * allow two stack_guard_gaps between them here, and when choosing\n\t * an unmapped area; whereas when expanding we only require one.\n\t * That's a little inconsistent, but keeps the code here simpler.\n\t */\n\tmax = vm_start_gap(vma);\n\tif (vma->vm_prev) {\n\t\tprev_end = vm_end_gap(vma->vm_prev);\n\t\tif (max > prev_end)\n\t\t\tmax -= prev_end;\n\t\telse\n\t\t\tmax = 0;\n\t}\n\tif (vma->vm_rb.rb_left) {\n\t\tsubtree_gap = rb_entry(vma->vm_rb.rb_left,\n\t\t\t\tstruct vm_area_struct, vm_rb)->rb_subtree_gap;\n\t\tif (subtree_gap > max)\n\t\t\tmax = subtree_gap;\n\t}\n\tif (vma->vm_rb.rb_right) {\n\t\tsubtree_gap = rb_entry(vma->vm_rb.rb_right,\n\t\t\t\tstruct vm_area_struct, vm_rb)->rb_subtree_gap;\n\t\tif (subtree_gap > max)\n\t\t\tmax = subtree_gap;\n\t}\n\treturn max;\n}\n\n#ifdef CONFIG_DEBUG_VM_RB\nstatic int browse_rb(struct mm_struct *mm)\n{\n\tstruct rb_root *root = &mm->mm_rb;\n\tint i = 0, j, bug = 0;\n\tstruct rb_node *nd, *pn = NULL;\n\tunsigned long prev = 0, pend = 0;\n\n\tfor (nd = rb_first(root); nd; nd = rb_next(nd)) {\n\t\tstruct vm_area_struct *vma;\n\t\tvma = rb_entry(nd, struct vm_area_struct, vm_rb);\n\t\tif (vma->vm_start < prev) {\n\t\t\tpr_emerg(\"vm_start %lx < prev %lx\\n\",\n\t\t\t\t  vma->vm_start, prev);\n\t\t\tbug = 1;\n\t\t}\n\t\tif (vma->vm_start < pend) {\n\t\t\tpr_emerg(\"vm_start %lx < pend %lx\\n\",\n\t\t\t\t  vma->vm_start, pend);\n\t\t\tbug = 1;\n\t\t}\n\t\tif (vma->vm_start > vma->vm_end) {\n\t\t\tpr_emerg(\"vm_start %lx > vm_end %lx\\n\",\n\t\t\t\t  vma->vm_start, vma->vm_end);\n\t\t\tbug = 1;\n\t\t}\n\t\tspin_lock(&mm->page_table_lock);\n\t\tif (vma->rb_subtree_gap != vma_compute_subtree_gap(vma)) {\n\t\t\tpr_emerg(\"free gap %lx, correct %lx\\n\",\n\t\t\t       vma->rb_subtree_gap,\n\t\t\t       vma_compute_subtree_gap(vma));\n\t\t\tbug = 1;\n\t\t}\n\t\tspin_unlock(&mm->page_table_lock);\n\t\ti++;\n\t\tpn = nd;\n\t\tprev = vma->vm_start;\n\t\tpend = vma->vm_end;\n\t}\n\tj = 0;\n\tfor (nd = pn; nd; nd = rb_prev(nd))\n\t\tj++;\n\tif (i != j) {\n\t\tpr_emerg(\"backwards %d, forwards %d\\n\", j, i);\n\t\tbug = 1;\n\t}\n\treturn bug ? -1 : i;\n}\n\nstatic void validate_mm_rb(struct rb_root *root, struct vm_area_struct *ignore)\n{\n\tstruct rb_node *nd;\n\n\tfor (nd = rb_first(root); nd; nd = rb_next(nd)) {\n\t\tstruct vm_area_struct *vma;\n\t\tvma = rb_entry(nd, struct vm_area_struct, vm_rb);\n\t\tVM_BUG_ON_VMA(vma != ignore &&\n\t\t\tvma->rb_subtree_gap != vma_compute_subtree_gap(vma),\n\t\t\tvma);\n\t}\n}\n\nstatic void validate_mm(struct mm_struct *mm)\n{\n\tint bug = 0;\n\tint i = 0;\n\tunsigned long highest_address = 0;\n\tstruct vm_area_struct *vma = mm->mmap;\n\n\twhile (vma) {\n\t\tstruct anon_vma *anon_vma = vma->anon_vma;\n\t\tstruct anon_vma_chain *avc;\n\n\t\tif (anon_vma) {\n\t\t\tanon_vma_lock_read(anon_vma);\n\t\t\tlist_for_each_entry(avc, &vma->anon_vma_chain, same_vma)\n\t\t\t\tanon_vma_interval_tree_verify(avc);\n\t\t\tanon_vma_unlock_read(anon_vma);\n\t\t}\n\n\t\thighest_address = vm_end_gap(vma);\n\t\tvma = vma->vm_next;\n\t\ti++;\n\t}\n\tif (i != mm->map_count) {\n\t\tpr_emerg(\"map_count %d vm_next %d\\n\", mm->map_count, i);\n\t\tbug = 1;\n\t}\n\tif (highest_address != mm->highest_vm_end) {\n\t\tpr_emerg(\"mm->highest_vm_end %lx, found %lx\\n\",\n\t\t\t  mm->highest_vm_end, highest_address);\n\t\tbug = 1;\n\t}\n\ti = browse_rb(mm);\n\tif (i != mm->map_count) {\n\t\tif (i != -1)\n\t\t\tpr_emerg(\"map_count %d rb %d\\n\", mm->map_count, i);\n\t\tbug = 1;\n\t}\n\tVM_BUG_ON_MM(bug, mm);\n}\n#else\n#define validate_mm_rb(root, ignore) do { } while (0)\n#define validate_mm(mm) do { } while (0)\n#endif\n\nRB_DECLARE_CALLBACKS(static, vma_gap_callbacks, struct vm_area_struct, vm_rb,\n\t\t     unsigned long, rb_subtree_gap, vma_compute_subtree_gap)\n\n/*\n * Update augmented rbtree rb_subtree_gap values after vma->vm_start or\n * vma->vm_prev->vm_end values changed, without modifying the vma's position\n * in the rbtree.\n */\nstatic void vma_gap_update(struct vm_area_struct *vma)\n{\n\t/*\n\t * As it turns out, RB_DECLARE_CALLBACKS() already created a callback\n\t * function that does exactly what we want.\n\t */\n\tvma_gap_callbacks_propagate(&vma->vm_rb, NULL);\n}\n\nstatic inline void vma_rb_insert(struct vm_area_struct *vma,\n\t\t\t\t struct rb_root *root)\n{\n\t/* All rb_subtree_gap values must be consistent prior to insertion */\n\tvalidate_mm_rb(root, NULL);\n\n\trb_insert_augmented(&vma->vm_rb, root, &vma_gap_callbacks);\n}\n\nstatic void __vma_rb_erase(struct vm_area_struct *vma, struct rb_root *root)\n{\n\t/*\n\t * Note rb_erase_augmented is a fairly large inline function,\n\t * so make sure we instantiate it only once with our desired\n\t * augmented rbtree callbacks.\n\t */\n\trb_erase_augmented(&vma->vm_rb, root, &vma_gap_callbacks);\n}\n\nstatic __always_inline void vma_rb_erase_ignore(struct vm_area_struct *vma,\n\t\t\t\t\t\tstruct rb_root *root,\n\t\t\t\t\t\tstruct vm_area_struct *ignore)\n{\n\t/*\n\t * All rb_subtree_gap values must be consistent prior to erase,\n\t * with the possible exception of the \"next\" vma being erased if\n\t * next->vm_start was reduced.\n\t */\n\tvalidate_mm_rb(root, ignore);\n\n\t__vma_rb_erase(vma, root);\n}\n\nstatic __always_inline void vma_rb_erase(struct vm_area_struct *vma,\n\t\t\t\t\t struct rb_root *root)\n{\n\t/*\n\t * All rb_subtree_gap values must be consistent prior to erase,\n\t * with the possible exception of the vma being erased.\n\t */\n\tvalidate_mm_rb(root, vma);\n\n\t__vma_rb_erase(vma, root);\n}\n\n/*\n * vma has some anon_vma assigned, and is already inserted on that\n * anon_vma's interval trees.\n *\n * Before updating the vma's vm_start / vm_end / vm_pgoff fields, the\n * vma must be removed from the anon_vma's interval trees using\n * anon_vma_interval_tree_pre_update_vma().\n *\n * After the update, the vma will be reinserted using\n * anon_vma_interval_tree_post_update_vma().\n *\n * The entire update must be protected by exclusive mmap_sem and by\n * the root anon_vma's mutex.\n */\nstatic inline void\nanon_vma_interval_tree_pre_update_vma(struct vm_area_struct *vma)\n{\n\tstruct anon_vma_chain *avc;\n\n\tlist_for_each_entry(avc, &vma->anon_vma_chain, same_vma)\n\t\tanon_vma_interval_tree_remove(avc, &avc->anon_vma->rb_root);\n}\n\nstatic inline void\nanon_vma_interval_tree_post_update_vma(struct vm_area_struct *vma)\n{\n\tstruct anon_vma_chain *avc;\n\n\tlist_for_each_entry(avc, &vma->anon_vma_chain, same_vma)\n\t\tanon_vma_interval_tree_insert(avc, &avc->anon_vma->rb_root);\n}\n\nstatic int find_vma_links(struct mm_struct *mm, unsigned long addr,\n\t\tunsigned long end, struct vm_area_struct **pprev,\n\t\tstruct rb_node ***rb_link, struct rb_node **rb_parent)\n{\n\tstruct rb_node **__rb_link, *__rb_parent, *rb_prev;\n\n\t__rb_link = &mm->mm_rb.rb_node;\n\trb_prev = __rb_parent = NULL;\n\n\twhile (*__rb_link) {\n\t\tstruct vm_area_struct *vma_tmp;\n\n\t\t__rb_parent = *__rb_link;\n\t\tvma_tmp = rb_entry(__rb_parent, struct vm_area_struct, vm_rb);\n\n\t\tif (vma_tmp->vm_end > addr) {\n\t\t\t/* Fail if an existing vma overlaps the area */\n\t\t\tif (vma_tmp->vm_start < end)\n\t\t\t\treturn -ENOMEM;\n\t\t\t__rb_link = &__rb_parent->rb_left;\n\t\t} else {\n\t\t\trb_prev = __rb_parent;\n\t\t\t__rb_link = &__rb_parent->rb_right;\n\t\t}\n\t}\n\n\t*pprev = NULL;\n\tif (rb_prev)\n\t\t*pprev = rb_entry(rb_prev, struct vm_area_struct, vm_rb);\n\t*rb_link = __rb_link;\n\t*rb_parent = __rb_parent;\n\treturn 0;\n}\n\nstatic unsigned long count_vma_pages_range(struct mm_struct *mm,\n\t\tunsigned long addr, unsigned long end)\n{\n\tunsigned long nr_pages = 0;\n\tstruct vm_area_struct *vma;\n\n\t/* Find first overlaping mapping */\n\tvma = find_vma_intersection(mm, addr, end);\n\tif (!vma)\n\t\treturn 0;\n\n\tnr_pages = (min(end, vma->vm_end) -\n\t\tmax(addr, vma->vm_start)) >> PAGE_SHIFT;\n\n\t/* Iterate over the rest of the overlaps */\n\tfor (vma = vma->vm_next; vma; vma = vma->vm_next) {\n\t\tunsigned long overlap_len;\n\n\t\tif (vma->vm_start > end)\n\t\t\tbreak;\n\n\t\toverlap_len = min(end, vma->vm_end) - vma->vm_start;\n\t\tnr_pages += overlap_len >> PAGE_SHIFT;\n\t}\n\n\treturn nr_pages;\n}\n\nvoid __vma_link_rb(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\tstruct rb_node **rb_link, struct rb_node *rb_parent)\n{\n\t/* Update tracking information for the gap following the new vma. */\n\tif (vma->vm_next)\n\t\tvma_gap_update(vma->vm_next);\n\telse\n\t\tmm->highest_vm_end = vm_end_gap(vma);\n\n\t/*\n\t * vma->vm_prev wasn't known when we followed the rbtree to find the\n\t * correct insertion point for that vma. As a result, we could not\n\t * update the vma vm_rb parents rb_subtree_gap values on the way down.\n\t * So, we first insert the vma with a zero rb_subtree_gap value\n\t * (to be consistent with what we did on the way down), and then\n\t * immediately update the gap to the correct value. Finally we\n\t * rebalance the rbtree after all augmented values have been set.\n\t */\n\trb_link_node(&vma->vm_rb, rb_parent, rb_link);\n\tvma->rb_subtree_gap = 0;\n\tvma_gap_update(vma);\n\tvma_rb_insert(vma, &mm->mm_rb);\n}\n\nstatic void __vma_link_file(struct vm_area_struct *vma)\n{\n\tstruct file *file;\n\n\tfile = vma->vm_file;\n\tif (file) {\n\t\tstruct address_space *mapping = file->f_mapping;\n\n\t\tif (vma->vm_flags & VM_DENYWRITE)\n\t\t\tatomic_dec(&file_inode(file)->i_writecount);\n\t\tif (vma->vm_flags & VM_SHARED)\n\t\t\tatomic_inc(&mapping->i_mmap_writable);\n\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_insert(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t}\n}\n\nstatic void\n__vma_link(struct mm_struct *mm, struct vm_area_struct *vma,\n\tstruct vm_area_struct *prev, struct rb_node **rb_link,\n\tstruct rb_node *rb_parent)\n{\n\t__vma_link_list(mm, vma, prev, rb_parent);\n\t__vma_link_rb(mm, vma, rb_link, rb_parent);\n}\n\nstatic void vma_link(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tstruct vm_area_struct *prev, struct rb_node **rb_link,\n\t\t\tstruct rb_node *rb_parent)\n{\n\tstruct address_space *mapping = NULL;\n\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\t\ti_mmap_lock_write(mapping);\n\t}\n\n\t__vma_link(mm, vma, prev, rb_link, rb_parent);\n\t__vma_link_file(vma);\n\n\tif (mapping)\n\t\ti_mmap_unlock_write(mapping);\n\n\tmm->map_count++;\n\tvalidate_mm(mm);\n}\n\n/*\n * Helper for vma_adjust() in the split_vma insert case: insert a vma into the\n * mm's list and rbtree.  It has already been inserted into the interval tree.\n */\nstatic void __insert_vm_struct(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tstruct vm_area_struct *prev;\n\tstruct rb_node **rb_link, *rb_parent;\n\n\tif (find_vma_links(mm, vma->vm_start, vma->vm_end,\n\t\t\t   &prev, &rb_link, &rb_parent))\n\t\tBUG();\n\t__vma_link(mm, vma, prev, rb_link, rb_parent);\n\tmm->map_count++;\n}\n\nstatic __always_inline void __vma_unlink_common(struct mm_struct *mm,\n\t\t\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\t\t\tstruct vm_area_struct *prev,\n\t\t\t\t\t\tbool has_prev,\n\t\t\t\t\t\tstruct vm_area_struct *ignore)\n{\n\tstruct vm_area_struct *next;\n\n\tvma_rb_erase_ignore(vma, &mm->mm_rb, ignore);\n\tnext = vma->vm_next;\n\tif (has_prev)\n\t\tprev->vm_next = next;\n\telse {\n\t\tprev = vma->vm_prev;\n\t\tif (prev)\n\t\t\tprev->vm_next = next;\n\t\telse\n\t\t\tmm->mmap = next;\n\t}\n\tif (next)\n\t\tnext->vm_prev = prev;\n\n\t/* Kill the cache */\n\tvmacache_invalidate(mm);\n}\n\nstatic inline void __vma_unlink_prev(struct mm_struct *mm,\n\t\t\t\t     struct vm_area_struct *vma,\n\t\t\t\t     struct vm_area_struct *prev)\n{\n\t__vma_unlink_common(mm, vma, prev, true, vma);\n}\n\n/*\n * We cannot adjust vm_start, vm_end, vm_pgoff fields of a vma that\n * is already present in an i_mmap tree without adjusting the tree.\n * The following helper function should be used when such adjustments\n * are necessary.  The \"insert\" vma (if any) is to be inserted\n * before we drop the necessary locks.\n */\nint __vma_adjust(struct vm_area_struct *vma, unsigned long start,\n\tunsigned long end, pgoff_t pgoff, struct vm_area_struct *insert,\n\tstruct vm_area_struct *expand)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct vm_area_struct *next = vma->vm_next, *orig_vma = vma;\n\tstruct address_space *mapping = NULL;\n\tstruct rb_root_cached *root = NULL;\n\tstruct anon_vma *anon_vma = NULL;\n\tstruct file *file = vma->vm_file;\n\tbool start_changed = false, end_changed = false;\n\tlong adjust_next = 0;\n\tint remove_next = 0;\n\n\tif (next && !insert) {\n\t\tstruct vm_area_struct *exporter = NULL, *importer = NULL;\n\n\t\tif (end >= next->vm_end) {\n\t\t\t/*\n\t\t\t * vma expands, overlapping all the next, and\n\t\t\t * perhaps the one after too (mprotect case 6).\n\t\t\t * The only other cases that gets here are\n\t\t\t * case 1, case 7 and case 8.\n\t\t\t */\n\t\t\tif (next == expand) {\n\t\t\t\t/*\n\t\t\t\t * The only case where we don't expand \"vma\"\n\t\t\t\t * and we expand \"next\" instead is case 8.\n\t\t\t\t */\n\t\t\t\tVM_WARN_ON(end != next->vm_end);\n\t\t\t\t/*\n\t\t\t\t * remove_next == 3 means we're\n\t\t\t\t * removing \"vma\" and that to do so we\n\t\t\t\t * swapped \"vma\" and \"next\".\n\t\t\t\t */\n\t\t\t\tremove_next = 3;\n\t\t\t\tVM_WARN_ON(file != next->vm_file);\n\t\t\t\tswap(vma, next);\n\t\t\t} else {\n\t\t\t\tVM_WARN_ON(expand != vma);\n\t\t\t\t/*\n\t\t\t\t * case 1, 6, 7, remove_next == 2 is case 6,\n\t\t\t\t * remove_next == 1 is case 1 or 7.\n\t\t\t\t */\n\t\t\t\tremove_next = 1 + (end > next->vm_end);\n\t\t\t\tVM_WARN_ON(remove_next == 2 &&\n\t\t\t\t\t   end != next->vm_next->vm_end);\n\t\t\t\tVM_WARN_ON(remove_next == 1 &&\n\t\t\t\t\t   end != next->vm_end);\n\t\t\t\t/* trim end to next, for case 6 first pass */\n\t\t\t\tend = next->vm_end;\n\t\t\t}\n\n\t\t\texporter = next;\n\t\t\timporter = vma;\n\n\t\t\t/*\n\t\t\t * If next doesn't have anon_vma, import from vma after\n\t\t\t * next, if the vma overlaps with it.\n\t\t\t */\n\t\t\tif (remove_next == 2 && !next->anon_vma)\n\t\t\t\texporter = next->vm_next;\n\n\t\t} else if (end > next->vm_start) {\n\t\t\t/*\n\t\t\t * vma expands, overlapping part of the next:\n\t\t\t * mprotect case 5 shifting the boundary up.\n\t\t\t */\n\t\t\tadjust_next = (end - next->vm_start) >> PAGE_SHIFT;\n\t\t\texporter = next;\n\t\t\timporter = vma;\n\t\t\tVM_WARN_ON(expand != importer);\n\t\t} else if (end < vma->vm_end) {\n\t\t\t/*\n\t\t\t * vma shrinks, and !insert tells it's not\n\t\t\t * split_vma inserting another: so it must be\n\t\t\t * mprotect case 4 shifting the boundary down.\n\t\t\t */\n\t\t\tadjust_next = -((vma->vm_end - end) >> PAGE_SHIFT);\n\t\t\texporter = vma;\n\t\t\timporter = next;\n\t\t\tVM_WARN_ON(expand != importer);\n\t\t}\n\n\t\t/*\n\t\t * Easily overlooked: when mprotect shifts the boundary,\n\t\t * make sure the expanding vma has anon_vma set if the\n\t\t * shrinking vma had, to cover any anon pages imported.\n\t\t */\n\t\tif (exporter && exporter->anon_vma && !importer->anon_vma) {\n\t\t\tint error;\n\n\t\t\timporter->anon_vma = exporter->anon_vma;\n\t\t\terror = anon_vma_clone(importer, exporter);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t}\nagain:\n\tvma_adjust_trans_huge(orig_vma, start, end, adjust_next);\n\n\tif (file) {\n\t\tmapping = file->f_mapping;\n\t\troot = &mapping->i_mmap;\n\t\tuprobe_munmap(vma, vma->vm_start, vma->vm_end);\n\n\t\tif (adjust_next)\n\t\t\tuprobe_munmap(next, next->vm_start, next->vm_end);\n\n\t\ti_mmap_lock_write(mapping);\n\t\tif (insert) {\n\t\t\t/*\n\t\t\t * Put into interval tree now, so instantiated pages\n\t\t\t * are visible to arm/parisc __flush_dcache_page\n\t\t\t * throughout; but we cannot insert into address\n\t\t\t * space until vma start or end is updated.\n\t\t\t */\n\t\t\t__vma_link_file(insert);\n\t\t}\n\t}\n\n\tanon_vma = vma->anon_vma;\n\tif (!anon_vma && adjust_next)\n\t\tanon_vma = next->anon_vma;\n\tif (anon_vma) {\n\t\tVM_WARN_ON(adjust_next && next->anon_vma &&\n\t\t\t   anon_vma != next->anon_vma);\n\t\tanon_vma_lock_write(anon_vma);\n\t\tanon_vma_interval_tree_pre_update_vma(vma);\n\t\tif (adjust_next)\n\t\t\tanon_vma_interval_tree_pre_update_vma(next);\n\t}\n\n\tif (root) {\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_remove(vma, root);\n\t\tif (adjust_next)\n\t\t\tvma_interval_tree_remove(next, root);\n\t}\n\n\tif (start != vma->vm_start) {\n\t\tvma->vm_start = start;\n\t\tstart_changed = true;\n\t}\n\tif (end != vma->vm_end) {\n\t\tvma->vm_end = end;\n\t\tend_changed = true;\n\t}\n\tvma->vm_pgoff = pgoff;\n\tif (adjust_next) {\n\t\tnext->vm_start += adjust_next << PAGE_SHIFT;\n\t\tnext->vm_pgoff += adjust_next;\n\t}\n\n\tif (root) {\n\t\tif (adjust_next)\n\t\t\tvma_interval_tree_insert(next, root);\n\t\tvma_interval_tree_insert(vma, root);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t}\n\n\tif (remove_next) {\n\t\t/*\n\t\t * vma_merge has merged next into vma, and needs\n\t\t * us to remove next before dropping the locks.\n\t\t */\n\t\tif (remove_next != 3)\n\t\t\t__vma_unlink_prev(mm, next, vma);\n\t\telse\n\t\t\t/*\n\t\t\t * vma is not before next if they've been\n\t\t\t * swapped.\n\t\t\t *\n\t\t\t * pre-swap() next->vm_start was reduced so\n\t\t\t * tell validate_mm_rb to ignore pre-swap()\n\t\t\t * \"next\" (which is stored in post-swap()\n\t\t\t * \"vma\").\n\t\t\t */\n\t\t\t__vma_unlink_common(mm, next, NULL, false, vma);\n\t\tif (file)\n\t\t\t__remove_shared_vm_struct(next, file, mapping);\n\t} else if (insert) {\n\t\t/*\n\t\t * split_vma has split insert from vma, and needs\n\t\t * us to insert it before dropping the locks\n\t\t * (it may either follow vma or precede it).\n\t\t */\n\t\t__insert_vm_struct(mm, insert);\n\t} else {\n\t\tif (start_changed)\n\t\t\tvma_gap_update(vma);\n\t\tif (end_changed) {\n\t\t\tif (!next)\n\t\t\t\tmm->highest_vm_end = vm_end_gap(vma);\n\t\t\telse if (!adjust_next)\n\t\t\t\tvma_gap_update(next);\n\t\t}\n\t}\n\n\tif (anon_vma) {\n\t\tanon_vma_interval_tree_post_update_vma(vma);\n\t\tif (adjust_next)\n\t\t\tanon_vma_interval_tree_post_update_vma(next);\n\t\tanon_vma_unlock_write(anon_vma);\n\t}\n\tif (mapping)\n\t\ti_mmap_unlock_write(mapping);\n\n\tif (root) {\n\t\tuprobe_mmap(vma);\n\n\t\tif (adjust_next)\n\t\t\tuprobe_mmap(next);\n\t}\n\n\tif (remove_next) {\n\t\tif (file) {\n\t\t\tuprobe_munmap(next, next->vm_start, next->vm_end);\n\t\t\tfput(file);\n\t\t}\n\t\tif (next->anon_vma)\n\t\t\tanon_vma_merge(vma, next);\n\t\tmm->map_count--;\n\t\tmpol_put(vma_policy(next));\n\t\tvm_area_free(next);\n\t\t/*\n\t\t * In mprotect's case 6 (see comments on vma_merge),\n\t\t * we must remove another next too. It would clutter\n\t\t * up the code too much to do both in one go.\n\t\t */\n\t\tif (remove_next != 3) {\n\t\t\t/*\n\t\t\t * If \"next\" was removed and vma->vm_end was\n\t\t\t * expanded (up) over it, in turn\n\t\t\t * \"next->vm_prev->vm_end\" changed and the\n\t\t\t * \"vma->vm_next\" gap must be updated.\n\t\t\t */\n\t\t\tnext = vma->vm_next;\n\t\t} else {\n\t\t\t/*\n\t\t\t * For the scope of the comment \"next\" and\n\t\t\t * \"vma\" considered pre-swap(): if \"vma\" was\n\t\t\t * removed, next->vm_start was expanded (down)\n\t\t\t * over it and the \"next\" gap must be updated.\n\t\t\t * Because of the swap() the post-swap() \"vma\"\n\t\t\t * actually points to pre-swap() \"next\"\n\t\t\t * (post-swap() \"next\" as opposed is now a\n\t\t\t * dangling pointer).\n\t\t\t */\n\t\t\tnext = vma;\n\t\t}\n\t\tif (remove_next == 2) {\n\t\t\tremove_next = 1;\n\t\t\tend = next->vm_end;\n\t\t\tgoto again;\n\t\t}\n\t\telse if (next)\n\t\t\tvma_gap_update(next);\n\t\telse {\n\t\t\t/*\n\t\t\t * If remove_next == 2 we obviously can't\n\t\t\t * reach this path.\n\t\t\t *\n\t\t\t * If remove_next == 3 we can't reach this\n\t\t\t * path because pre-swap() next is always not\n\t\t\t * NULL. pre-swap() \"next\" is not being\n\t\t\t * removed and its next->vm_end is not altered\n\t\t\t * (and furthermore \"end\" already matches\n\t\t\t * next->vm_end in remove_next == 3).\n\t\t\t *\n\t\t\t * We reach this only in the remove_next == 1\n\t\t\t * case if the \"next\" vma that was removed was\n\t\t\t * the highest vma of the mm. However in such\n\t\t\t * case next->vm_end == \"end\" and the extended\n\t\t\t * \"vma\" has vma->vm_end == next->vm_end so\n\t\t\t * mm->highest_vm_end doesn't need any update\n\t\t\t * in remove_next == 1 case.\n\t\t\t */\n\t\t\tVM_WARN_ON(mm->highest_vm_end != vm_end_gap(vma));\n\t\t}\n\t}\n\tif (insert && file)\n\t\tuprobe_mmap(insert);\n\n\tvalidate_mm(mm);\n\n\treturn 0;\n}\n\n/*\n * If the vma has a ->close operation then the driver probably needs to release\n * per-vma resources, so we don't attempt to merge those.\n */\nstatic inline int is_mergeable_vma(struct vm_area_struct *vma,\n\t\t\t\tstruct file *file, unsigned long vm_flags,\n\t\t\t\tstruct vm_userfaultfd_ctx vm_userfaultfd_ctx)\n{\n\t/*\n\t * VM_SOFTDIRTY should not prevent from VMA merging, if we\n\t * match the flags but dirty bit -- the caller should mark\n\t * merged VMA as dirty. If dirty bit won't be excluded from\n\t * comparison, we increase pressure on the memory system forcing\n\t * the kernel to generate new VMAs when old one could be\n\t * extended instead.\n\t */\n\tif ((vma->vm_flags ^ vm_flags) & ~VM_SOFTDIRTY)\n\t\treturn 0;\n\tif (vma->vm_file != file)\n\t\treturn 0;\n\tif (vma->vm_ops && vma->vm_ops->close)\n\t\treturn 0;\n\tif (!is_mergeable_vm_userfaultfd_ctx(vma, vm_userfaultfd_ctx))\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic inline int is_mergeable_anon_vma(struct anon_vma *anon_vma1,\n\t\t\t\t\tstruct anon_vma *anon_vma2,\n\t\t\t\t\tstruct vm_area_struct *vma)\n{\n\t/*\n\t * The list_is_singular() test is to avoid merging VMA cloned from\n\t * parents. This can improve scalability caused by anon_vma lock.\n\t */\n\tif ((!anon_vma1 || !anon_vma2) && (!vma ||\n\t\tlist_is_singular(&vma->anon_vma_chain)))\n\t\treturn 1;\n\treturn anon_vma1 == anon_vma2;\n}\n\n/*\n * Return true if we can merge this (vm_flags,anon_vma,file,vm_pgoff)\n * in front of (at a lower virtual address and file offset than) the vma.\n *\n * We cannot merge two vmas if they have differently assigned (non-NULL)\n * anon_vmas, nor if same anon_vma is assigned but offsets incompatible.\n *\n * We don't check here for the merged mmap wrapping around the end of pagecache\n * indices (16TB on ia32) because do_mmap_pgoff() does not permit mmap's which\n * wrap, nor mmaps which cover the final page at index -1UL.\n */\nstatic int\ncan_vma_merge_before(struct vm_area_struct *vma, unsigned long vm_flags,\n\t\t     struct anon_vma *anon_vma, struct file *file,\n\t\t     pgoff_t vm_pgoff,\n\t\t     struct vm_userfaultfd_ctx vm_userfaultfd_ctx)\n{\n\tif (is_mergeable_vma(vma, file, vm_flags, vm_userfaultfd_ctx) &&\n\t    is_mergeable_anon_vma(anon_vma, vma->anon_vma, vma)) {\n\t\tif (vma->vm_pgoff == vm_pgoff)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/*\n * Return true if we can merge this (vm_flags,anon_vma,file,vm_pgoff)\n * beyond (at a higher virtual address and file offset than) the vma.\n *\n * We cannot merge two vmas if they have differently assigned (non-NULL)\n * anon_vmas, nor if same anon_vma is assigned but offsets incompatible.\n */\nstatic int\ncan_vma_merge_after(struct vm_area_struct *vma, unsigned long vm_flags,\n\t\t    struct anon_vma *anon_vma, struct file *file,\n\t\t    pgoff_t vm_pgoff,\n\t\t    struct vm_userfaultfd_ctx vm_userfaultfd_ctx)\n{\n\tif (is_mergeable_vma(vma, file, vm_flags, vm_userfaultfd_ctx) &&\n\t    is_mergeable_anon_vma(anon_vma, vma->anon_vma, vma)) {\n\t\tpgoff_t vm_pglen;\n\t\tvm_pglen = vma_pages(vma);\n\t\tif (vma->vm_pgoff + vm_pglen == vm_pgoff)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/*\n * Given a mapping request (addr,end,vm_flags,file,pgoff), figure out\n * whether that can be merged with its predecessor or its successor.\n * Or both (it neatly fills a hole).\n *\n * In most cases - when called for mmap, brk or mremap - [addr,end) is\n * certain not to be mapped by the time vma_merge is called; but when\n * called for mprotect, it is certain to be already mapped (either at\n * an offset within prev, or at the start of next), and the flags of\n * this area are about to be changed to vm_flags - and the no-change\n * case has already been eliminated.\n *\n * The following mprotect cases have to be considered, where AAAA is\n * the area passed down from mprotect_fixup, never extending beyond one\n * vma, PPPPPP is the prev vma specified, and NNNNNN the next vma after:\n *\n *     AAAA             AAAA                AAAA          AAAA\n *    PPPPPPNNNNNN    PPPPPPNNNNNN    PPPPPPNNNNNN    PPPPNNNNXXXX\n *    cannot merge    might become    might become    might become\n *                    PPNNNNNNNNNN    PPPPPPPPPPNN    PPPPPPPPPPPP 6 or\n *    mmap, brk or    case 4 below    case 5 below    PPPPPPPPXXXX 7 or\n *    mremap move:                                    PPPPXXXXXXXX 8\n *        AAAA\n *    PPPP    NNNN    PPPPPPPPPPPP    PPPPPPPPNNNN    PPPPNNNNNNNN\n *    might become    case 1 below    case 2 below    case 3 below\n *\n * It is important for case 8 that the vma NNNN overlapping the\n * region AAAA is never going to extended over XXXX. Instead XXXX must\n * be extended in region AAAA and NNNN must be removed. This way in\n * all cases where vma_merge succeeds, the moment vma_adjust drops the\n * rmap_locks, the properties of the merged vma will be already\n * correct for the whole merged range. Some of those properties like\n * vm_page_prot/vm_flags may be accessed by rmap_walks and they must\n * be correct for the whole merged range immediately after the\n * rmap_locks are released. Otherwise if XXXX would be removed and\n * NNNN would be extended over the XXXX range, remove_migration_ptes\n * or other rmap walkers (if working on addresses beyond the \"end\"\n * parameter) may establish ptes with the wrong permissions of NNNN\n * instead of the right permissions of XXXX.\n */\nstruct vm_area_struct *vma_merge(struct mm_struct *mm,\n\t\t\tstruct vm_area_struct *prev, unsigned long addr,\n\t\t\tunsigned long end, unsigned long vm_flags,\n\t\t\tstruct anon_vma *anon_vma, struct file *file,\n\t\t\tpgoff_t pgoff, struct mempolicy *policy,\n\t\t\tstruct vm_userfaultfd_ctx vm_userfaultfd_ctx)\n{\n\tpgoff_t pglen = (end - addr) >> PAGE_SHIFT;\n\tstruct vm_area_struct *area, *next;\n\tint err;\n\n\t/*\n\t * We later require that vma->vm_flags == vm_flags,\n\t * so this tests vma->vm_flags & VM_SPECIAL, too.\n\t */\n\tif (vm_flags & VM_SPECIAL)\n\t\treturn NULL;\n\n\tif (prev)\n\t\tnext = prev->vm_next;\n\telse\n\t\tnext = mm->mmap;\n\tarea = next;\n\tif (area && area->vm_end == end)\t\t/* cases 6, 7, 8 */\n\t\tnext = next->vm_next;\n\n\t/* verify some invariant that must be enforced by the caller */\n\tVM_WARN_ON(prev && addr <= prev->vm_start);\n\tVM_WARN_ON(area && end > area->vm_end);\n\tVM_WARN_ON(addr >= end);\n\n\t/*\n\t * Can it merge with the predecessor?\n\t */\n\tif (prev && prev->vm_end == addr &&\n\t\t\tmpol_equal(vma_policy(prev), policy) &&\n\t\t\tcan_vma_merge_after(prev, vm_flags,\n\t\t\t\t\t    anon_vma, file, pgoff,\n\t\t\t\t\t    vm_userfaultfd_ctx)) {\n\t\t/*\n\t\t * OK, it can.  Can we now merge in the successor as well?\n\t\t */\n\t\tif (next && end == next->vm_start &&\n\t\t\t\tmpol_equal(policy, vma_policy(next)) &&\n\t\t\t\tcan_vma_merge_before(next, vm_flags,\n\t\t\t\t\t\t     anon_vma, file,\n\t\t\t\t\t\t     pgoff+pglen,\n\t\t\t\t\t\t     vm_userfaultfd_ctx) &&\n\t\t\t\tis_mergeable_anon_vma(prev->anon_vma,\n\t\t\t\t\t\t      next->anon_vma, NULL)) {\n\t\t\t\t\t\t\t/* cases 1, 6 */\n\t\t\terr = __vma_adjust(prev, prev->vm_start,\n\t\t\t\t\t next->vm_end, prev->vm_pgoff, NULL,\n\t\t\t\t\t prev);\n\t\t} else\t\t\t\t\t/* cases 2, 5, 7 */\n\t\t\terr = __vma_adjust(prev, prev->vm_start,\n\t\t\t\t\t end, prev->vm_pgoff, NULL, prev);\n\t\tif (err)\n\t\t\treturn NULL;\n\t\tkhugepaged_enter_vma_merge(prev, vm_flags);\n\t\treturn prev;\n\t}\n\n\t/*\n\t * Can this new request be merged in front of next?\n\t */\n\tif (next && end == next->vm_start &&\n\t\t\tmpol_equal(policy, vma_policy(next)) &&\n\t\t\tcan_vma_merge_before(next, vm_flags,\n\t\t\t\t\t     anon_vma, file, pgoff+pglen,\n\t\t\t\t\t     vm_userfaultfd_ctx)) {\n\t\tif (prev && addr < prev->vm_end)\t/* case 4 */\n\t\t\terr = __vma_adjust(prev, prev->vm_start,\n\t\t\t\t\t addr, prev->vm_pgoff, NULL, next);\n\t\telse {\t\t\t\t\t/* cases 3, 8 */\n\t\t\terr = __vma_adjust(area, addr, next->vm_end,\n\t\t\t\t\t next->vm_pgoff - pglen, NULL, next);\n\t\t\t/*\n\t\t\t * In case 3 area is already equal to next and\n\t\t\t * this is a noop, but in case 8 \"area\" has\n\t\t\t * been removed and next was expanded over it.\n\t\t\t */\n\t\t\tarea = next;\n\t\t}\n\t\tif (err)\n\t\t\treturn NULL;\n\t\tkhugepaged_enter_vma_merge(area, vm_flags);\n\t\treturn area;\n\t}\n\n\treturn NULL;\n}\n\n/*\n * Rough compatbility check to quickly see if it's even worth looking\n * at sharing an anon_vma.\n *\n * They need to have the same vm_file, and the flags can only differ\n * in things that mprotect may change.\n *\n * NOTE! The fact that we share an anon_vma doesn't _have_ to mean that\n * we can merge the two vma's. For example, we refuse to merge a vma if\n * there is a vm_ops->close() function, because that indicates that the\n * driver is doing some kind of reference counting. But that doesn't\n * really matter for the anon_vma sharing case.\n */\nstatic int anon_vma_compatible(struct vm_area_struct *a, struct vm_area_struct *b)\n{\n\treturn a->vm_end == b->vm_start &&\n\t\tmpol_equal(vma_policy(a), vma_policy(b)) &&\n\t\ta->vm_file == b->vm_file &&\n\t\t!((a->vm_flags ^ b->vm_flags) & ~(VM_READ|VM_WRITE|VM_EXEC|VM_SOFTDIRTY)) &&\n\t\tb->vm_pgoff == a->vm_pgoff + ((b->vm_start - a->vm_start) >> PAGE_SHIFT);\n}\n\n/*\n * Do some basic sanity checking to see if we can re-use the anon_vma\n * from 'old'. The 'a'/'b' vma's are in VM order - one of them will be\n * the same as 'old', the other will be the new one that is trying\n * to share the anon_vma.\n *\n * NOTE! This runs with mm_sem held for reading, so it is possible that\n * the anon_vma of 'old' is concurrently in the process of being set up\n * by another page fault trying to merge _that_. But that's ok: if it\n * is being set up, that automatically means that it will be a singleton\n * acceptable for merging, so we can do all of this optimistically. But\n * we do that READ_ONCE() to make sure that we never re-load the pointer.\n *\n * IOW: that the \"list_is_singular()\" test on the anon_vma_chain only\n * matters for the 'stable anon_vma' case (ie the thing we want to avoid\n * is to return an anon_vma that is \"complex\" due to having gone through\n * a fork).\n *\n * We also make sure that the two vma's are compatible (adjacent,\n * and with the same memory policies). That's all stable, even with just\n * a read lock on the mm_sem.\n */\nstatic struct anon_vma *reusable_anon_vma(struct vm_area_struct *old, struct vm_area_struct *a, struct vm_area_struct *b)\n{\n\tif (anon_vma_compatible(a, b)) {\n\t\tstruct anon_vma *anon_vma = READ_ONCE(old->anon_vma);\n\n\t\tif (anon_vma && list_is_singular(&old->anon_vma_chain))\n\t\t\treturn anon_vma;\n\t}\n\treturn NULL;\n}\n\n/*\n * find_mergeable_anon_vma is used by anon_vma_prepare, to check\n * neighbouring vmas for a suitable anon_vma, before it goes off\n * to allocate a new anon_vma.  It checks because a repetitive\n * sequence of mprotects and faults may otherwise lead to distinct\n * anon_vmas being allocated, preventing vma merge in subsequent\n * mprotect.\n */\nstruct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *vma)\n{\n\tstruct anon_vma *anon_vma;\n\tstruct vm_area_struct *near;\n\n\tnear = vma->vm_next;\n\tif (!near)\n\t\tgoto try_prev;\n\n\tanon_vma = reusable_anon_vma(near, vma, near);\n\tif (anon_vma)\n\t\treturn anon_vma;\ntry_prev:\n\tnear = vma->vm_prev;\n\tif (!near)\n\t\tgoto none;\n\n\tanon_vma = reusable_anon_vma(near, near, vma);\n\tif (anon_vma)\n\t\treturn anon_vma;\nnone:\n\t/*\n\t * There's no absolute need to look only at touching neighbours:\n\t * we could search further afield for \"compatible\" anon_vmas.\n\t * But it would probably just be a waste of time searching,\n\t * or lead to too many vmas hanging off the same anon_vma.\n\t * We're trying to allow mprotect remerging later on,\n\t * not trying to minimize memory used for anon_vmas.\n\t */\n\treturn NULL;\n}\n\n/*\n * If a hint addr is less than mmap_min_addr change hint to be as\n * low as possible but still greater than mmap_min_addr\n */\nstatic inline unsigned long round_hint_to_min(unsigned long hint)\n{\n\thint &= PAGE_MASK;\n\tif (((void *)hint != NULL) &&\n\t    (hint < mmap_min_addr))\n\t\treturn PAGE_ALIGN(mmap_min_addr);\n\treturn hint;\n}\n\nstatic inline int mlock_future_check(struct mm_struct *mm,\n\t\t\t\t     unsigned long flags,\n\t\t\t\t     unsigned long len)\n{\n\tunsigned long locked, lock_limit;\n\n\t/*  mlock MCL_FUTURE? */\n\tif (flags & VM_LOCKED) {\n\t\tlocked = len >> PAGE_SHIFT;\n\t\tlocked += mm->locked_vm;\n\t\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\t\tlock_limit >>= PAGE_SHIFT;\n\t\tif (locked > lock_limit && !capable(CAP_IPC_LOCK))\n\t\t\treturn -EAGAIN;\n\t}\n\treturn 0;\n}\n\nstatic inline u64 file_mmap_size_max(struct file *file, struct inode *inode)\n{\n\tif (S_ISREG(inode->i_mode))\n\t\treturn MAX_LFS_FILESIZE;\n\n\tif (S_ISBLK(inode->i_mode))\n\t\treturn MAX_LFS_FILESIZE;\n\n\t/* Special \"we do even unsigned file positions\" case */\n\tif (file->f_mode & FMODE_UNSIGNED_OFFSET)\n\t\treturn 0;\n\n\t/* Yes, random drivers might want more. But I'm tired of buggy drivers */\n\treturn ULONG_MAX;\n}\n\nstatic inline bool file_mmap_ok(struct file *file, struct inode *inode,\n\t\t\t\tunsigned long pgoff, unsigned long len)\n{\n\tu64 maxsize = file_mmap_size_max(file, inode);\n\n\tif (maxsize && len > maxsize)\n\t\treturn false;\n\tmaxsize -= len;\n\tif (pgoff > maxsize >> PAGE_SHIFT)\n\t\treturn false;\n\treturn true;\n}\n\n/*\n * The caller must hold down_write(&current->mm->mmap_sem).\n */\nunsigned long do_mmap(struct file *file, unsigned long addr,\n\t\t\tunsigned long len, unsigned long prot,\n\t\t\tunsigned long flags, vm_flags_t vm_flags,\n\t\t\tunsigned long pgoff, unsigned long *populate,\n\t\t\tstruct list_head *uf)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint pkey = 0;\n\n\t*populate = 0;\n\n\tif (!len)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Does the application expect PROT_READ to imply PROT_EXEC?\n\t *\n\t * (the exception is when the underlying filesystem is noexec\n\t *  mounted, in which case we dont add PROT_EXEC.)\n\t */\n\tif ((prot & PROT_READ) && (current->personality & READ_IMPLIES_EXEC))\n\t\tif (!(file && path_noexec(&file->f_path)))\n\t\t\tprot |= PROT_EXEC;\n\n\t/* force arch specific MAP_FIXED handling in get_unmapped_area */\n\tif (flags & MAP_FIXED_NOREPLACE)\n\t\tflags |= MAP_FIXED;\n\n\tif (!(flags & MAP_FIXED))\n\t\taddr = round_hint_to_min(addr);\n\n\t/* Careful about overflows.. */\n\tlen = PAGE_ALIGN(len);\n\tif (!len)\n\t\treturn -ENOMEM;\n\n\t/* offset overflow? */\n\tif ((pgoff + (len >> PAGE_SHIFT)) < pgoff)\n\t\treturn -EOVERFLOW;\n\n\t/* Too many mappings? */\n\tif (mm->map_count > sysctl_max_map_count)\n\t\treturn -ENOMEM;\n\n\t/* Obtain the address to map to. we verify (or select) it and ensure\n\t * that it represents a valid section of the address space.\n\t */\n\taddr = get_unmapped_area(file, addr, len, pgoff, flags);\n\tif (offset_in_page(addr))\n\t\treturn addr;\n\n\tif (flags & MAP_FIXED_NOREPLACE) {\n\t\tstruct vm_area_struct *vma = find_vma(mm, addr);\n\n\t\tif (vma && vma->vm_start < addr + len)\n\t\t\treturn -EEXIST;\n\t}\n\n\tif (prot == PROT_EXEC) {\n\t\tpkey = execute_only_pkey(mm);\n\t\tif (pkey < 0)\n\t\t\tpkey = 0;\n\t}\n\n\t/* Do simple checking here so the lower-level routines won't have\n\t * to. we assume access permissions have been handled by the open\n\t * of the memory object, so we don't do any here.\n\t */\n\tvm_flags |= calc_vm_prot_bits(prot, pkey) | calc_vm_flag_bits(flags) |\n\t\t\tmm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;\n\n\tif (flags & MAP_LOCKED)\n\t\tif (!can_do_mlock())\n\t\t\treturn -EPERM;\n\n\tif (mlock_future_check(mm, vm_flags, len))\n\t\treturn -EAGAIN;\n\n\tif (file) {\n\t\tstruct inode *inode = file_inode(file);\n\t\tunsigned long flags_mask;\n\n\t\tif (!file_mmap_ok(file, inode, pgoff, len))\n\t\t\treturn -EOVERFLOW;\n\n\t\tflags_mask = LEGACY_MAP_MASK | file->f_op->mmap_supported_flags;\n\n\t\tswitch (flags & MAP_TYPE) {\n\t\tcase MAP_SHARED:\n\t\t\t/*\n\t\t\t * Force use of MAP_SHARED_VALIDATE with non-legacy\n\t\t\t * flags. E.g. MAP_SYNC is dangerous to use with\n\t\t\t * MAP_SHARED as you don't know which consistency model\n\t\t\t * you will get. We silently ignore unsupported flags\n\t\t\t * with MAP_SHARED to preserve backward compatibility.\n\t\t\t */\n\t\t\tflags &= LEGACY_MAP_MASK;\n\t\t\t/* fall through */\n\t\tcase MAP_SHARED_VALIDATE:\n\t\t\tif (flags & ~flags_mask)\n\t\t\t\treturn -EOPNOTSUPP;\n\t\t\tif ((prot&PROT_WRITE) && !(file->f_mode&FMODE_WRITE))\n\t\t\t\treturn -EACCES;\n\n\t\t\t/*\n\t\t\t * Make sure we don't allow writing to an append-only\n\t\t\t * file..\n\t\t\t */\n\t\t\tif (IS_APPEND(inode) && (file->f_mode & FMODE_WRITE))\n\t\t\t\treturn -EACCES;\n\n\t\t\t/*\n\t\t\t * Make sure there are no mandatory locks on the file.\n\t\t\t */\n\t\t\tif (locks_verify_locked(file))\n\t\t\t\treturn -EAGAIN;\n\n\t\t\tvm_flags |= VM_SHARED | VM_MAYSHARE;\n\t\t\tif (!(file->f_mode & FMODE_WRITE))\n\t\t\t\tvm_flags &= ~(VM_MAYWRITE | VM_SHARED);\n\n\t\t\t/* fall through */\n\t\tcase MAP_PRIVATE:\n\t\t\tif (!(file->f_mode & FMODE_READ))\n\t\t\t\treturn -EACCES;\n\t\t\tif (path_noexec(&file->f_path)) {\n\t\t\t\tif (vm_flags & VM_EXEC)\n\t\t\t\t\treturn -EPERM;\n\t\t\t\tvm_flags &= ~VM_MAYEXEC;\n\t\t\t}\n\n\t\t\tif (!file->f_op->mmap)\n\t\t\t\treturn -ENODEV;\n\t\t\tif (vm_flags & (VM_GROWSDOWN|VM_GROWSUP))\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tswitch (flags & MAP_TYPE) {\n\t\tcase MAP_SHARED:\n\t\t\tif (vm_flags & (VM_GROWSDOWN|VM_GROWSUP))\n\t\t\t\treturn -EINVAL;\n\t\t\t/*\n\t\t\t * Ignore pgoff.\n\t\t\t */\n\t\t\tpgoff = 0;\n\t\t\tvm_flags |= VM_SHARED | VM_MAYSHARE;\n\t\t\tbreak;\n\t\tcase MAP_PRIVATE:\n\t\t\t/*\n\t\t\t * Set pgoff according to addr for anon_vma.\n\t\t\t */\n\t\t\tpgoff = addr >> PAGE_SHIFT;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/*\n\t * Set 'VM_NORESERVE' if we should not account for the\n\t * memory use of this mapping.\n\t */\n\tif (flags & MAP_NORESERVE) {\n\t\t/* We honor MAP_NORESERVE if allowed to overcommit */\n\t\tif (sysctl_overcommit_memory != OVERCOMMIT_NEVER)\n\t\t\tvm_flags |= VM_NORESERVE;\n\n\t\t/* hugetlb applies strict overcommit unless MAP_NORESERVE */\n\t\tif (file && is_file_hugepages(file))\n\t\t\tvm_flags |= VM_NORESERVE;\n\t}\n\n\taddr = mmap_region(file, addr, len, vm_flags, pgoff, uf);\n\tif (!IS_ERR_VALUE(addr) &&\n\t    ((vm_flags & VM_LOCKED) ||\n\t     (flags & (MAP_POPULATE | MAP_NONBLOCK)) == MAP_POPULATE))\n\t\t*populate = len;\n\treturn addr;\n}\n\nunsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,\n\t\t\t      unsigned long prot, unsigned long flags,\n\t\t\t      unsigned long fd, unsigned long pgoff)\n{\n\tstruct file *file = NULL;\n\tunsigned long retval;\n\n\tif (!(flags & MAP_ANONYMOUS)) {\n\t\taudit_mmap_fd(fd, flags);\n\t\tfile = fget(fd);\n\t\tif (!file)\n\t\t\treturn -EBADF;\n\t\tif (is_file_hugepages(file))\n\t\t\tlen = ALIGN(len, huge_page_size(hstate_file(file)));\n\t\tretval = -EINVAL;\n\t\tif (unlikely(flags & MAP_HUGETLB && !is_file_hugepages(file)))\n\t\t\tgoto out_fput;\n\t} else if (flags & MAP_HUGETLB) {\n\t\tstruct user_struct *user = NULL;\n\t\tstruct hstate *hs;\n\n\t\ths = hstate_sizelog((flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);\n\t\tif (!hs)\n\t\t\treturn -EINVAL;\n\n\t\tlen = ALIGN(len, huge_page_size(hs));\n\t\t/*\n\t\t * VM_NORESERVE is used because the reservations will be\n\t\t * taken when vm_ops->mmap() is called\n\t\t * A dummy user value is used because we are not locking\n\t\t * memory so no accounting is necessary\n\t\t */\n\t\tfile = hugetlb_file_setup(HUGETLB_ANON_FILE, len,\n\t\t\t\tVM_NORESERVE,\n\t\t\t\t&user, HUGETLB_ANONHUGE_INODE,\n\t\t\t\t(flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);\n\t\tif (IS_ERR(file))\n\t\t\treturn PTR_ERR(file);\n\t}\n\n\tflags &= ~(MAP_EXECUTABLE | MAP_DENYWRITE);\n\n\tretval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);\nout_fput:\n\tif (file)\n\t\tfput(file);\n\treturn retval;\n}\n\nSYSCALL_DEFINE6(mmap_pgoff, unsigned long, addr, unsigned long, len,\n\t\tunsigned long, prot, unsigned long, flags,\n\t\tunsigned long, fd, unsigned long, pgoff)\n{\n\treturn ksys_mmap_pgoff(addr, len, prot, flags, fd, pgoff);\n}\n\n#ifdef __ARCH_WANT_SYS_OLD_MMAP\nstruct mmap_arg_struct {\n\tunsigned long addr;\n\tunsigned long len;\n\tunsigned long prot;\n\tunsigned long flags;\n\tunsigned long fd;\n\tunsigned long offset;\n};\n\nSYSCALL_DEFINE1(old_mmap, struct mmap_arg_struct __user *, arg)\n{\n\tstruct mmap_arg_struct a;\n\n\tif (copy_from_user(&a, arg, sizeof(a)))\n\t\treturn -EFAULT;\n\tif (offset_in_page(a.offset))\n\t\treturn -EINVAL;\n\n\treturn ksys_mmap_pgoff(a.addr, a.len, a.prot, a.flags, a.fd,\n\t\t\t       a.offset >> PAGE_SHIFT);\n}\n#endif /* __ARCH_WANT_SYS_OLD_MMAP */\n\n/*\n * Some shared mappings will want the pages marked read-only\n * to track write events. If so, we'll downgrade vm_page_prot\n * to the private version (using protection_map[] without the\n * VM_SHARED bit).\n */\nint vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot)\n{\n\tvm_flags_t vm_flags = vma->vm_flags;\n\tconst struct vm_operations_struct *vm_ops = vma->vm_ops;\n\n\t/* If it was private or non-writable, the write bit is already clear */\n\tif ((vm_flags & (VM_WRITE|VM_SHARED)) != ((VM_WRITE|VM_SHARED)))\n\t\treturn 0;\n\n\t/* The backer wishes to know when pages are first written to? */\n\tif (vm_ops && (vm_ops->page_mkwrite || vm_ops->pfn_mkwrite))\n\t\treturn 1;\n\n\t/* The open routine did something to the protections that pgprot_modify\n\t * won't preserve? */\n\tif (pgprot_val(vm_page_prot) !=\n\t    pgprot_val(vm_pgprot_modify(vm_page_prot, vm_flags)))\n\t\treturn 0;\n\n\t/* Do we need to track softdirty? */\n\tif (IS_ENABLED(CONFIG_MEM_SOFT_DIRTY) && !(vm_flags & VM_SOFTDIRTY))\n\t\treturn 1;\n\n\t/* Specialty mapping? */\n\tif (vm_flags & VM_PFNMAP)\n\t\treturn 0;\n\n\t/* Can the mapping track the dirty pages? */\n\treturn vma->vm_file && vma->vm_file->f_mapping &&\n\t\tmapping_cap_account_dirty(vma->vm_file->f_mapping);\n}\n\n/*\n * We account for memory if it's a private writeable mapping,\n * not hugepages and VM_NORESERVE wasn't set.\n */\nstatic inline int accountable_mapping(struct file *file, vm_flags_t vm_flags)\n{\n\t/*\n\t * hugetlb has its own accounting separate from the core VM\n\t * VM_HUGETLB may not be set yet so we cannot check for that flag.\n\t */\n\tif (file && is_file_hugepages(file))\n\t\treturn 0;\n\n\treturn (vm_flags & (VM_NORESERVE | VM_SHARED | VM_WRITE)) == VM_WRITE;\n}\n\nunsigned long mmap_region(struct file *file, unsigned long addr,\n\t\tunsigned long len, vm_flags_t vm_flags, unsigned long pgoff,\n\t\tstruct list_head *uf)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma, *prev;\n\tint error;\n\tstruct rb_node **rb_link, *rb_parent;\n\tunsigned long charged = 0;\n\n\t/* Check against address space limit. */\n\tif (!may_expand_vm(mm, vm_flags, len >> PAGE_SHIFT)) {\n\t\tunsigned long nr_pages;\n\n\t\t/*\n\t\t * MAP_FIXED may remove pages of mappings that intersects with\n\t\t * requested mapping. Account for the pages it would unmap.\n\t\t */\n\t\tnr_pages = count_vma_pages_range(mm, addr, addr + len);\n\n\t\tif (!may_expand_vm(mm, vm_flags,\n\t\t\t\t\t(len >> PAGE_SHIFT) - nr_pages))\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* Clear old maps */\n\twhile (find_vma_links(mm, addr, addr + len, &prev, &rb_link,\n\t\t\t      &rb_parent)) {\n\t\tif (do_munmap(mm, addr, len, uf))\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/*\n\t * Private writable mapping: check memory availability\n\t */\n\tif (accountable_mapping(file, vm_flags)) {\n\t\tcharged = len >> PAGE_SHIFT;\n\t\tif (security_vm_enough_memory_mm(mm, charged))\n\t\t\treturn -ENOMEM;\n\t\tvm_flags |= VM_ACCOUNT;\n\t}\n\n\t/*\n\t * Can we just expand an old mapping?\n\t */\n\tvma = vma_merge(mm, prev, addr, addr + len, vm_flags,\n\t\t\tNULL, file, pgoff, NULL, NULL_VM_UFFD_CTX);\n\tif (vma)\n\t\tgoto out;\n\n\t/*\n\t * Determine the object being mapped and call the appropriate\n\t * specific mapper. the address has already been validated, but\n\t * not unmapped, but the maps are removed from the list.\n\t */\n\tvma = vm_area_alloc(mm);\n\tif (!vma) {\n\t\terror = -ENOMEM;\n\t\tgoto unacct_error;\n\t}\n\n\tvma->vm_start = addr;\n\tvma->vm_end = addr + len;\n\tvma->vm_flags = vm_flags;\n\tvma->vm_page_prot = vm_get_page_prot(vm_flags);\n\tvma->vm_pgoff = pgoff;\n\n\tif (file) {\n\t\tif (vm_flags & VM_DENYWRITE) {\n\t\t\terror = deny_write_access(file);\n\t\t\tif (error)\n\t\t\t\tgoto free_vma;\n\t\t}\n\t\tif (vm_flags & VM_SHARED) {\n\t\t\terror = mapping_map_writable(file->f_mapping);\n\t\t\tif (error)\n\t\t\t\tgoto allow_write_and_free_vma;\n\t\t}\n\n\t\t/* ->mmap() can change vma->vm_file, but must guarantee that\n\t\t * vma_link() below can deny write-access if VM_DENYWRITE is set\n\t\t * and map writably if VM_SHARED is set. This usually means the\n\t\t * new file must not have been exposed to user-space, yet.\n\t\t */\n\t\tvma->vm_file = get_file(file);\n\t\terror = call_mmap(file, vma);\n\t\tif (error)\n\t\t\tgoto unmap_and_free_vma;\n\n\t\t/* Can addr have changed??\n\t\t *\n\t\t * Answer: Yes, several device drivers can do it in their\n\t\t *         f_op->mmap method. -DaveM\n\t\t * Bug: If addr is changed, prev, rb_link, rb_parent should\n\t\t *      be updated for vma_link()\n\t\t */\n\t\tWARN_ON_ONCE(addr != vma->vm_start);\n\n\t\taddr = vma->vm_start;\n\t\tvm_flags = vma->vm_flags;\n\t} else if (vm_flags & VM_SHARED) {\n\t\terror = shmem_zero_setup(vma);\n\t\tif (error)\n\t\t\tgoto free_vma;\n\t} else {\n\t\tvma_set_anonymous(vma);\n\t}\n\n\tvma_link(mm, vma, prev, rb_link, rb_parent);\n\t/* Once vma denies write, undo our temporary denial count */\n\tif (file) {\n\t\tif (vm_flags & VM_SHARED)\n\t\t\tmapping_unmap_writable(file->f_mapping);\n\t\tif (vm_flags & VM_DENYWRITE)\n\t\t\tallow_write_access(file);\n\t}\n\tfile = vma->vm_file;\nout:\n\tperf_event_mmap(vma);\n\n\tvm_stat_account(mm, vm_flags, len >> PAGE_SHIFT);\n\tif (vm_flags & VM_LOCKED) {\n\t\tif ((vm_flags & VM_SPECIAL) || vma_is_dax(vma) ||\n\t\t\t\t\tis_vm_hugetlb_page(vma) ||\n\t\t\t\t\tvma == get_gate_vma(current->mm))\n\t\t\tvma->vm_flags &= VM_LOCKED_CLEAR_MASK;\n\t\telse\n\t\t\tmm->locked_vm += (len >> PAGE_SHIFT);\n\t}\n\n\tif (file)\n\t\tuprobe_mmap(vma);\n\n\t/*\n\t * New (or expanded) vma always get soft dirty status.\n\t * Otherwise user-space soft-dirty page tracker won't\n\t * be able to distinguish situation when vma area unmapped,\n\t * then new mapped in-place (which must be aimed as\n\t * a completely new data area).\n\t */\n\tvma->vm_flags |= VM_SOFTDIRTY;\n\n\tvma_set_page_prot(vma);\n\n\treturn addr;\n\nunmap_and_free_vma:\n\tvma->vm_file = NULL;\n\tfput(file);\n\n\t/* Undo any partial mapping done by a device driver. */\n\tunmap_region(mm, vma, prev, vma->vm_start, vma->vm_end);\n\tcharged = 0;\n\tif (vm_flags & VM_SHARED)\n\t\tmapping_unmap_writable(file->f_mapping);\nallow_write_and_free_vma:\n\tif (vm_flags & VM_DENYWRITE)\n\t\tallow_write_access(file);\nfree_vma:\n\tvm_area_free(vma);\nunacct_error:\n\tif (charged)\n\t\tvm_unacct_memory(charged);\n\treturn error;\n}\n\nunsigned long unmapped_area(struct vm_unmapped_area_info *info)\n{\n\t/*\n\t * We implement the search by looking for an rbtree node that\n\t * immediately follows a suitable gap. That is,\n\t * - gap_start = vma->vm_prev->vm_end <= info->high_limit - length;\n\t * - gap_end   = vma->vm_start        >= info->low_limit  + length;\n\t * - gap_end - gap_start >= length\n\t */\n\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long length, low_limit, high_limit, gap_start, gap_end;\n\n\t/* Adjust search length to account for worst case alignment overhead */\n\tlength = info->length + info->align_mask;\n\tif (length < info->length)\n\t\treturn -ENOMEM;\n\n\t/* Adjust search limits by the desired length */\n\tif (info->high_limit < length)\n\t\treturn -ENOMEM;\n\thigh_limit = info->high_limit - length;\n\n\tif (info->low_limit > high_limit)\n\t\treturn -ENOMEM;\n\tlow_limit = info->low_limit + length;\n\n\t/* Check if rbtree root looks promising */\n\tif (RB_EMPTY_ROOT(&mm->mm_rb))\n\t\tgoto check_highest;\n\tvma = rb_entry(mm->mm_rb.rb_node, struct vm_area_struct, vm_rb);\n\tif (vma->rb_subtree_gap < length)\n\t\tgoto check_highest;\n\n\twhile (true) {\n\t\t/* Visit left subtree if it looks promising */\n\t\tgap_end = vm_start_gap(vma);\n\t\tif (gap_end >= low_limit && vma->vm_rb.rb_left) {\n\t\t\tstruct vm_area_struct *left =\n\t\t\t\trb_entry(vma->vm_rb.rb_left,\n\t\t\t\t\t struct vm_area_struct, vm_rb);\n\t\t\tif (left->rb_subtree_gap >= length) {\n\t\t\t\tvma = left;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tgap_start = vma->vm_prev ? vm_end_gap(vma->vm_prev) : 0;\ncheck_current:\n\t\t/* Check if current node has a suitable gap */\n\t\tif (gap_start > high_limit)\n\t\t\treturn -ENOMEM;\n\t\tif (gap_end >= low_limit &&\n\t\t    gap_end > gap_start && gap_end - gap_start >= length)\n\t\t\tgoto found;\n\n\t\t/* Visit right subtree if it looks promising */\n\t\tif (vma->vm_rb.rb_right) {\n\t\t\tstruct vm_area_struct *right =\n\t\t\t\trb_entry(vma->vm_rb.rb_right,\n\t\t\t\t\t struct vm_area_struct, vm_rb);\n\t\t\tif (right->rb_subtree_gap >= length) {\n\t\t\t\tvma = right;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\t/* Go back up the rbtree to find next candidate node */\n\t\twhile (true) {\n\t\t\tstruct rb_node *prev = &vma->vm_rb;\n\t\t\tif (!rb_parent(prev))\n\t\t\t\tgoto check_highest;\n\t\t\tvma = rb_entry(rb_parent(prev),\n\t\t\t\t       struct vm_area_struct, vm_rb);\n\t\t\tif (prev == vma->vm_rb.rb_left) {\n\t\t\t\tgap_start = vm_end_gap(vma->vm_prev);\n\t\t\t\tgap_end = vm_start_gap(vma);\n\t\t\t\tgoto check_current;\n\t\t\t}\n\t\t}\n\t}\n\ncheck_highest:\n\t/* Check highest gap, which does not precede any rbtree node */\n\tgap_start = mm->highest_vm_end;\n\tgap_end = ULONG_MAX;  /* Only for VM_BUG_ON below */\n\tif (gap_start > high_limit)\n\t\treturn -ENOMEM;\n\nfound:\n\t/* We found a suitable gap. Clip it with the original low_limit. */\n\tif (gap_start < info->low_limit)\n\t\tgap_start = info->low_limit;\n\n\t/* Adjust gap address to the desired alignment */\n\tgap_start += (info->align_offset - gap_start) & info->align_mask;\n\n\tVM_BUG_ON(gap_start + info->length > info->high_limit);\n\tVM_BUG_ON(gap_start + info->length > gap_end);\n\treturn gap_start;\n}\n\nunsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long length, low_limit, high_limit, gap_start, gap_end;\n\n\t/* Adjust search length to account for worst case alignment overhead */\n\tlength = info->length + info->align_mask;\n\tif (length < info->length)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Adjust search limits by the desired length.\n\t * See implementation comment at top of unmapped_area().\n\t */\n\tgap_end = info->high_limit;\n\tif (gap_end < length)\n\t\treturn -ENOMEM;\n\thigh_limit = gap_end - length;\n\n\tif (info->low_limit > high_limit)\n\t\treturn -ENOMEM;\n\tlow_limit = info->low_limit + length;\n\n\t/* Check highest gap, which does not precede any rbtree node */\n\tgap_start = mm->highest_vm_end;\n\tif (gap_start <= high_limit)\n\t\tgoto found_highest;\n\n\t/* Check if rbtree root looks promising */\n\tif (RB_EMPTY_ROOT(&mm->mm_rb))\n\t\treturn -ENOMEM;\n\tvma = rb_entry(mm->mm_rb.rb_node, struct vm_area_struct, vm_rb);\n\tif (vma->rb_subtree_gap < length)\n\t\treturn -ENOMEM;\n\n\twhile (true) {\n\t\t/* Visit right subtree if it looks promising */\n\t\tgap_start = vma->vm_prev ? vm_end_gap(vma->vm_prev) : 0;\n\t\tif (gap_start <= high_limit && vma->vm_rb.rb_right) {\n\t\t\tstruct vm_area_struct *right =\n\t\t\t\trb_entry(vma->vm_rb.rb_right,\n\t\t\t\t\t struct vm_area_struct, vm_rb);\n\t\t\tif (right->rb_subtree_gap >= length) {\n\t\t\t\tvma = right;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\ncheck_current:\n\t\t/* Check if current node has a suitable gap */\n\t\tgap_end = vm_start_gap(vma);\n\t\tif (gap_end < low_limit)\n\t\t\treturn -ENOMEM;\n\t\tif (gap_start <= high_limit &&\n\t\t    gap_end > gap_start && gap_end - gap_start >= length)\n\t\t\tgoto found;\n\n\t\t/* Visit left subtree if it looks promising */\n\t\tif (vma->vm_rb.rb_left) {\n\t\t\tstruct vm_area_struct *left =\n\t\t\t\trb_entry(vma->vm_rb.rb_left,\n\t\t\t\t\t struct vm_area_struct, vm_rb);\n\t\t\tif (left->rb_subtree_gap >= length) {\n\t\t\t\tvma = left;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\t/* Go back up the rbtree to find next candidate node */\n\t\twhile (true) {\n\t\t\tstruct rb_node *prev = &vma->vm_rb;\n\t\t\tif (!rb_parent(prev))\n\t\t\t\treturn -ENOMEM;\n\t\t\tvma = rb_entry(rb_parent(prev),\n\t\t\t\t       struct vm_area_struct, vm_rb);\n\t\t\tif (prev == vma->vm_rb.rb_right) {\n\t\t\t\tgap_start = vma->vm_prev ?\n\t\t\t\t\tvm_end_gap(vma->vm_prev) : 0;\n\t\t\t\tgoto check_current;\n\t\t\t}\n\t\t}\n\t}\n\nfound:\n\t/* We found a suitable gap. Clip it with the original high_limit. */\n\tif (gap_end > info->high_limit)\n\t\tgap_end = info->high_limit;\n\nfound_highest:\n\t/* Compute highest gap address at the desired alignment */\n\tgap_end -= info->length;\n\tgap_end -= (gap_end - info->align_offset) & info->align_mask;\n\n\tVM_BUG_ON(gap_end < info->low_limit);\n\tVM_BUG_ON(gap_end < gap_start);\n\treturn gap_end;\n}\n\n\n#ifndef arch_get_mmap_end\n#define arch_get_mmap_end(addr)\t(TASK_SIZE)\n#endif\n\n#ifndef arch_get_mmap_base\n#define arch_get_mmap_base(addr, base) (base)\n#endif\n\n/* Get an address range which is currently unmapped.\n * For shmat() with addr=0.\n *\n * Ugly calling convention alert:\n * Return value with the low bits set means error value,\n * ie\n *\tif (ret & ~PAGE_MASK)\n *\t\terror = ret;\n *\n * This function \"knows\" that -ENOMEM has the bits set.\n */\n#ifndef HAVE_ARCH_UNMAPPED_AREA\nunsigned long\narch_get_unmapped_area(struct file *filp, unsigned long addr,\n\t\tunsigned long len, unsigned long pgoff, unsigned long flags)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma, *prev;\n\tstruct vm_unmapped_area_info info;\n\tconst unsigned long mmap_end = arch_get_mmap_end(addr);\n\n\tif (len > mmap_end - mmap_min_addr)\n\t\treturn -ENOMEM;\n\n\tif (flags & MAP_FIXED)\n\t\treturn addr;\n\n\tif (addr) {\n\t\taddr = PAGE_ALIGN(addr);\n\t\tvma = find_vma_prev(mm, addr, &prev);\n\t\tif (mmap_end - len >= addr && addr >= mmap_min_addr &&\n\t\t    (!vma || addr + len <= vm_start_gap(vma)) &&\n\t\t    (!prev || addr >= vm_end_gap(prev)))\n\t\t\treturn addr;\n\t}\n\n\tinfo.flags = 0;\n\tinfo.length = len;\n\tinfo.low_limit = mm->mmap_base;\n\tinfo.high_limit = mmap_end;\n\tinfo.align_mask = 0;\n\treturn vm_unmapped_area(&info);\n}\n#endif\n\n/*\n * This mmap-allocator allocates new areas top-down from below the\n * stack's low limit (the base):\n */\n#ifndef HAVE_ARCH_UNMAPPED_AREA_TOPDOWN\nunsigned long\narch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,\n\t\t\t  unsigned long len, unsigned long pgoff,\n\t\t\t  unsigned long flags)\n{\n\tstruct vm_area_struct *vma, *prev;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_unmapped_area_info info;\n\tconst unsigned long mmap_end = arch_get_mmap_end(addr);\n\n\t/* requested length too big for entire address space */\n\tif (len > mmap_end - mmap_min_addr)\n\t\treturn -ENOMEM;\n\n\tif (flags & MAP_FIXED)\n\t\treturn addr;\n\n\t/* requesting a specific address */\n\tif (addr) {\n\t\taddr = PAGE_ALIGN(addr);\n\t\tvma = find_vma_prev(mm, addr, &prev);\n\t\tif (mmap_end - len >= addr && addr >= mmap_min_addr &&\n\t\t\t\t(!vma || addr + len <= vm_start_gap(vma)) &&\n\t\t\t\t(!prev || addr >= vm_end_gap(prev)))\n\t\t\treturn addr;\n\t}\n\n\tinfo.flags = VM_UNMAPPED_AREA_TOPDOWN;\n\tinfo.length = len;\n\tinfo.low_limit = max(PAGE_SIZE, mmap_min_addr);\n\tinfo.high_limit = arch_get_mmap_base(addr, mm->mmap_base);\n\tinfo.align_mask = 0;\n\taddr = vm_unmapped_area(&info);\n\n\t/*\n\t * A failed mmap() very likely causes application failure,\n\t * so fall back to the bottom-up function here. This scenario\n\t * can happen with large stack limits and large mmap()\n\t * allocations.\n\t */\n\tif (offset_in_page(addr)) {\n\t\tVM_BUG_ON(addr != -ENOMEM);\n\t\tinfo.flags = 0;\n\t\tinfo.low_limit = TASK_UNMAPPED_BASE;\n\t\tinfo.high_limit = mmap_end;\n\t\taddr = vm_unmapped_area(&info);\n\t}\n\n\treturn addr;\n}\n#endif\n\nunsigned long\nget_unmapped_area(struct file *file, unsigned long addr, unsigned long len,\n\t\tunsigned long pgoff, unsigned long flags)\n{\n\tunsigned long (*get_area)(struct file *, unsigned long,\n\t\t\t\t  unsigned long, unsigned long, unsigned long);\n\n\tunsigned long error = arch_mmap_check(addr, len, flags);\n\tif (error)\n\t\treturn error;\n\n\t/* Careful about overflows.. */\n\tif (len > TASK_SIZE)\n\t\treturn -ENOMEM;\n\n\tget_area = current->mm->get_unmapped_area;\n\tif (file) {\n\t\tif (file->f_op->get_unmapped_area)\n\t\t\tget_area = file->f_op->get_unmapped_area;\n\t} else if (flags & MAP_SHARED) {\n\t\t/*\n\t\t * mmap_region() will call shmem_zero_setup() to create a file,\n\t\t * so use shmem's get_unmapped_area in case it can be huge.\n\t\t * do_mmap_pgoff() will clear pgoff, so match alignment.\n\t\t */\n\t\tpgoff = 0;\n\t\tget_area = shmem_get_unmapped_area;\n\t}\n\n\taddr = get_area(file, addr, len, pgoff, flags);\n\tif (IS_ERR_VALUE(addr))\n\t\treturn addr;\n\n\tif (addr > TASK_SIZE - len)\n\t\treturn -ENOMEM;\n\tif (offset_in_page(addr))\n\t\treturn -EINVAL;\n\n\terror = security_mmap_addr(addr);\n\treturn error ? error : addr;\n}\n\nEXPORT_SYMBOL(get_unmapped_area);\n\n/* Look up the first VMA which satisfies  addr < vm_end,  NULL if none. */\nstruct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct rb_node *rb_node;\n\tstruct vm_area_struct *vma;\n\n\t/* Check the cache first. */\n\tvma = vmacache_find(mm, addr);\n\tif (likely(vma))\n\t\treturn vma;\n\n\trb_node = mm->mm_rb.rb_node;\n\n\twhile (rb_node) {\n\t\tstruct vm_area_struct *tmp;\n\n\t\ttmp = rb_entry(rb_node, struct vm_area_struct, vm_rb);\n\n\t\tif (tmp->vm_end > addr) {\n\t\t\tvma = tmp;\n\t\t\tif (tmp->vm_start <= addr)\n\t\t\t\tbreak;\n\t\t\trb_node = rb_node->rb_left;\n\t\t} else\n\t\t\trb_node = rb_node->rb_right;\n\t}\n\n\tif (vma)\n\t\tvmacache_update(addr, vma);\n\treturn vma;\n}\n\nEXPORT_SYMBOL(find_vma);\n\n/*\n * Same as find_vma, but also return a pointer to the previous VMA in *pprev.\n */\nstruct vm_area_struct *\nfind_vma_prev(struct mm_struct *mm, unsigned long addr,\n\t\t\tstruct vm_area_struct **pprev)\n{\n\tstruct vm_area_struct *vma;\n\n\tvma = find_vma(mm, addr);\n\tif (vma) {\n\t\t*pprev = vma->vm_prev;\n\t} else {\n\t\tstruct rb_node *rb_node = mm->mm_rb.rb_node;\n\t\t*pprev = NULL;\n\t\twhile (rb_node) {\n\t\t\t*pprev = rb_entry(rb_node, struct vm_area_struct, vm_rb);\n\t\t\trb_node = rb_node->rb_right;\n\t\t}\n\t}\n\treturn vma;\n}\n\n/*\n * Verify that the stack growth is acceptable and\n * update accounting. This is shared with both the\n * grow-up and grow-down cases.\n */\nstatic int acct_stack_growth(struct vm_area_struct *vma,\n\t\t\t     unsigned long size, unsigned long grow)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long new_start;\n\n\t/* address space limit tests */\n\tif (!may_expand_vm(mm, vma->vm_flags, grow))\n\t\treturn -ENOMEM;\n\n\t/* Stack limit test */\n\tif (size > rlimit(RLIMIT_STACK))\n\t\treturn -ENOMEM;\n\n\t/* mlock limit tests */\n\tif (vma->vm_flags & VM_LOCKED) {\n\t\tunsigned long locked;\n\t\tunsigned long limit;\n\t\tlocked = mm->locked_vm + grow;\n\t\tlimit = rlimit(RLIMIT_MEMLOCK);\n\t\tlimit >>= PAGE_SHIFT;\n\t\tif (locked > limit && !capable(CAP_IPC_LOCK))\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* Check to ensure the stack will not grow into a hugetlb-only region */\n\tnew_start = (vma->vm_flags & VM_GROWSUP) ? vma->vm_start :\n\t\t\tvma->vm_end - size;\n\tif (is_hugepage_only_range(vma->vm_mm, new_start, size))\n\t\treturn -EFAULT;\n\n\t/*\n\t * Overcommit..  This must be the final test, as it will\n\t * update security statistics.\n\t */\n\tif (security_vm_enough_memory_mm(mm, grow))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\n#if defined(CONFIG_STACK_GROWSUP) || defined(CONFIG_IA64)\n/*\n * PA-RISC uses this for its stack; IA64 for its Register Backing Store.\n * vma is the last one with address > vma->vm_end.  Have to extend vma.\n */\nint expand_upwards(struct vm_area_struct *vma, unsigned long address)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct vm_area_struct *next;\n\tunsigned long gap_addr;\n\tint error = 0;\n\n\tif (!(vma->vm_flags & VM_GROWSUP))\n\t\treturn -EFAULT;\n\n\t/* Guard against exceeding limits of the address space. */\n\taddress &= PAGE_MASK;\n\tif (address >= (TASK_SIZE & PAGE_MASK))\n\t\treturn -ENOMEM;\n\taddress += PAGE_SIZE;\n\n\t/* Enforce stack_guard_gap */\n\tgap_addr = address + stack_guard_gap;\n\n\t/* Guard against overflow */\n\tif (gap_addr < address || gap_addr > TASK_SIZE)\n\t\tgap_addr = TASK_SIZE;\n\n\tnext = vma->vm_next;\n\tif (next && next->vm_start < gap_addr &&\n\t\t\t(next->vm_flags & (VM_WRITE|VM_READ|VM_EXEC))) {\n\t\tif (!(next->vm_flags & VM_GROWSUP))\n\t\t\treturn -ENOMEM;\n\t\t/* Check that both stack segments have the same anon_vma? */\n\t}\n\n\t/* We must make sure the anon_vma is allocated. */\n\tif (unlikely(anon_vma_prepare(vma)))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * vma->vm_start/vm_end cannot change under us because the caller\n\t * is required to hold the mmap_sem in read mode.  We need the\n\t * anon_vma lock to serialize against concurrent expand_stacks.\n\t */\n\tanon_vma_lock_write(vma->anon_vma);\n\n\t/* Somebody else might have raced and expanded it already */\n\tif (address > vma->vm_end) {\n\t\tunsigned long size, grow;\n\n\t\tsize = address - vma->vm_start;\n\t\tgrow = (address - vma->vm_end) >> PAGE_SHIFT;\n\n\t\terror = -ENOMEM;\n\t\tif (vma->vm_pgoff + (size >> PAGE_SHIFT) >= vma->vm_pgoff) {\n\t\t\terror = acct_stack_growth(vma, size, grow);\n\t\t\tif (!error) {\n\t\t\t\t/*\n\t\t\t\t * vma_gap_update() doesn't support concurrent\n\t\t\t\t * updates, but we only hold a shared mmap_sem\n\t\t\t\t * lock here, so we need to protect against\n\t\t\t\t * concurrent vma expansions.\n\t\t\t\t * anon_vma_lock_write() doesn't help here, as\n\t\t\t\t * we don't guarantee that all growable vmas\n\t\t\t\t * in a mm share the same root anon vma.\n\t\t\t\t * So, we reuse mm->page_table_lock to guard\n\t\t\t\t * against concurrent vma expansions.\n\t\t\t\t */\n\t\t\t\tspin_lock(&mm->page_table_lock);\n\t\t\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\t\t\tmm->locked_vm += grow;\n\t\t\t\tvm_stat_account(mm, vma->vm_flags, grow);\n\t\t\t\tanon_vma_interval_tree_pre_update_vma(vma);\n\t\t\t\tvma->vm_end = address;\n\t\t\t\tanon_vma_interval_tree_post_update_vma(vma);\n\t\t\t\tif (vma->vm_next)\n\t\t\t\t\tvma_gap_update(vma->vm_next);\n\t\t\t\telse\n\t\t\t\t\tmm->highest_vm_end = vm_end_gap(vma);\n\t\t\t\tspin_unlock(&mm->page_table_lock);\n\n\t\t\t\tperf_event_mmap(vma);\n\t\t\t}\n\t\t}\n\t}\n\tanon_vma_unlock_write(vma->anon_vma);\n\tkhugepaged_enter_vma_merge(vma, vma->vm_flags);\n\tvalidate_mm(mm);\n\treturn error;\n}\n#endif /* CONFIG_STACK_GROWSUP || CONFIG_IA64 */\n\n/*\n * vma is the first one with address < vma->vm_start.  Have to extend vma.\n */\nint expand_downwards(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long address)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct vm_area_struct *prev;\n\tint error = 0;\n\n\taddress &= PAGE_MASK;\n\tif (address < mmap_min_addr)\n\t\treturn -EPERM;\n\n\t/* Enforce stack_guard_gap */\n\tprev = vma->vm_prev;\n\t/* Check that both stack segments have the same anon_vma? */\n\tif (prev && !(prev->vm_flags & VM_GROWSDOWN) &&\n\t\t\t(prev->vm_flags & (VM_WRITE|VM_READ|VM_EXEC))) {\n\t\tif (address - prev->vm_end < stack_guard_gap)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* We must make sure the anon_vma is allocated. */\n\tif (unlikely(anon_vma_prepare(vma)))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * vma->vm_start/vm_end cannot change under us because the caller\n\t * is required to hold the mmap_sem in read mode.  We need the\n\t * anon_vma lock to serialize against concurrent expand_stacks.\n\t */\n\tanon_vma_lock_write(vma->anon_vma);\n\n\t/* Somebody else might have raced and expanded it already */\n\tif (address < vma->vm_start) {\n\t\tunsigned long size, grow;\n\n\t\tsize = vma->vm_end - address;\n\t\tgrow = (vma->vm_start - address) >> PAGE_SHIFT;\n\n\t\terror = -ENOMEM;\n\t\tif (grow <= vma->vm_pgoff) {\n\t\t\terror = acct_stack_growth(vma, size, grow);\n\t\t\tif (!error) {\n\t\t\t\t/*\n\t\t\t\t * vma_gap_update() doesn't support concurrent\n\t\t\t\t * updates, but we only hold a shared mmap_sem\n\t\t\t\t * lock here, so we need to protect against\n\t\t\t\t * concurrent vma expansions.\n\t\t\t\t * anon_vma_lock_write() doesn't help here, as\n\t\t\t\t * we don't guarantee that all growable vmas\n\t\t\t\t * in a mm share the same root anon vma.\n\t\t\t\t * So, we reuse mm->page_table_lock to guard\n\t\t\t\t * against concurrent vma expansions.\n\t\t\t\t */\n\t\t\t\tspin_lock(&mm->page_table_lock);\n\t\t\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\t\t\tmm->locked_vm += grow;\n\t\t\t\tvm_stat_account(mm, vma->vm_flags, grow);\n\t\t\t\tanon_vma_interval_tree_pre_update_vma(vma);\n\t\t\t\tvma->vm_start = address;\n\t\t\t\tvma->vm_pgoff -= grow;\n\t\t\t\tanon_vma_interval_tree_post_update_vma(vma);\n\t\t\t\tvma_gap_update(vma);\n\t\t\t\tspin_unlock(&mm->page_table_lock);\n\n\t\t\t\tperf_event_mmap(vma);\n\t\t\t}\n\t\t}\n\t}\n\tanon_vma_unlock_write(vma->anon_vma);\n\tkhugepaged_enter_vma_merge(vma, vma->vm_flags);\n\tvalidate_mm(mm);\n\treturn error;\n}\n\n/* enforced gap between the expanding stack and other mappings. */\nunsigned long stack_guard_gap = 256UL<<PAGE_SHIFT;\n\nstatic int __init cmdline_parse_stack_guard_gap(char *p)\n{\n\tunsigned long val;\n\tchar *endptr;\n\n\tval = simple_strtoul(p, &endptr, 10);\n\tif (!*endptr)\n\t\tstack_guard_gap = val << PAGE_SHIFT;\n\n\treturn 0;\n}\n__setup(\"stack_guard_gap=\", cmdline_parse_stack_guard_gap);\n\n#ifdef CONFIG_STACK_GROWSUP\nint expand_stack(struct vm_area_struct *vma, unsigned long address)\n{\n\treturn expand_upwards(vma, address);\n}\n\nstruct vm_area_struct *\nfind_extend_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma, *prev;\n\n\taddr &= PAGE_MASK;\n\tvma = find_vma_prev(mm, addr, &prev);\n\tif (vma && (vma->vm_start <= addr))\n\t\treturn vma;\n\t/* don't alter vm_end if the coredump is running */\n\tif (!prev || !mmget_still_valid(mm) || expand_stack(prev, addr))\n\t\treturn NULL;\n\tif (prev->vm_flags & VM_LOCKED)\n\t\tpopulate_vma_page_range(prev, addr, prev->vm_end, NULL);\n\treturn prev;\n}\n#else\nint expand_stack(struct vm_area_struct *vma, unsigned long address)\n{\n\treturn expand_downwards(vma, address);\n}\n\nstruct vm_area_struct *\nfind_extend_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long start;\n\n\taddr &= PAGE_MASK;\n\tvma = find_vma(mm, addr);\n\tif (!vma)\n\t\treturn NULL;\n\tif (vma->vm_start <= addr)\n\t\treturn vma;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\treturn NULL;\n\t/* don't alter vm_start if the coredump is running */\n\tif (!mmget_still_valid(mm))\n\t\treturn NULL;\n\tstart = vma->vm_start;\n\tif (expand_stack(vma, addr))\n\t\treturn NULL;\n\tif (vma->vm_flags & VM_LOCKED)\n\t\tpopulate_vma_page_range(vma, addr, start, NULL);\n\treturn vma;\n}\n#endif\n\nEXPORT_SYMBOL_GPL(find_extend_vma);\n\n/*\n * Ok - we have the memory areas we should free on the vma list,\n * so release them, and do the vma updates.\n *\n * Called with the mm semaphore held.\n */\nstatic void remove_vma_list(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tunsigned long nr_accounted = 0;\n\n\t/* Update high watermark before we lower total_vm */\n\tupdate_hiwater_vm(mm);\n\tdo {\n\t\tlong nrpages = vma_pages(vma);\n\n\t\tif (vma->vm_flags & VM_ACCOUNT)\n\t\t\tnr_accounted += nrpages;\n\t\tvm_stat_account(mm, vma->vm_flags, -nrpages);\n\t\tvma = remove_vma(vma);\n\t} while (vma);\n\tvm_unacct_memory(nr_accounted);\n\tvalidate_mm(mm);\n}\n\n/*\n * Get rid of page table information in the indicated region.\n *\n * Called with the mm semaphore held.\n */\nstatic void unmap_region(struct mm_struct *mm,\n\t\tstruct vm_area_struct *vma, struct vm_area_struct *prev,\n\t\tunsigned long start, unsigned long end)\n{\n\tstruct vm_area_struct *next = prev ? prev->vm_next : mm->mmap;\n\tstruct mmu_gather tlb;\n\n\tlru_add_drain();\n\ttlb_gather_mmu(&tlb, mm, start, end);\n\tupdate_hiwater_rss(mm);\n\tunmap_vmas(&tlb, vma, start, end);\n\tfree_pgtables(&tlb, vma, prev ? prev->vm_end : FIRST_USER_ADDRESS,\n\t\t\t\t next ? next->vm_start : USER_PGTABLES_CEILING);\n\ttlb_finish_mmu(&tlb, start, end);\n}\n\n/*\n * Create a list of vma's touched by the unmap, removing them from the mm's\n * vma list as we go..\n */\nstatic void\ndetach_vmas_to_be_unmapped(struct mm_struct *mm, struct vm_area_struct *vma,\n\tstruct vm_area_struct *prev, unsigned long end)\n{\n\tstruct vm_area_struct **insertion_point;\n\tstruct vm_area_struct *tail_vma = NULL;\n\n\tinsertion_point = (prev ? &prev->vm_next : &mm->mmap);\n\tvma->vm_prev = NULL;\n\tdo {\n\t\tvma_rb_erase(vma, &mm->mm_rb);\n\t\tmm->map_count--;\n\t\ttail_vma = vma;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\n\t*insertion_point = vma;\n\tif (vma) {\n\t\tvma->vm_prev = prev;\n\t\tvma_gap_update(vma);\n\t} else\n\t\tmm->highest_vm_end = prev ? vm_end_gap(prev) : 0;\n\ttail_vma->vm_next = NULL;\n\n\t/* Kill the cache */\n\tvmacache_invalidate(mm);\n}\n\n/*\n * __split_vma() bypasses sysctl_max_map_count checking.  We use this where it\n * has already been checked or doesn't make sense to fail.\n */\nint __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\tunsigned long addr, int new_below)\n{\n\tstruct vm_area_struct *new;\n\tint err;\n\n\tif (vma->vm_ops && vma->vm_ops->split) {\n\t\terr = vma->vm_ops->split(vma, addr);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tnew = vm_area_dup(vma);\n\tif (!new)\n\t\treturn -ENOMEM;\n\n\tif (new_below)\n\t\tnew->vm_end = addr;\n\telse {\n\t\tnew->vm_start = addr;\n\t\tnew->vm_pgoff += ((addr - vma->vm_start) >> PAGE_SHIFT);\n\t}\n\n\terr = vma_dup_policy(vma, new);\n\tif (err)\n\t\tgoto out_free_vma;\n\n\terr = anon_vma_clone(new, vma);\n\tif (err)\n\t\tgoto out_free_mpol;\n\n\tif (new->vm_file)\n\t\tget_file(new->vm_file);\n\n\tif (new->vm_ops && new->vm_ops->open)\n\t\tnew->vm_ops->open(new);\n\n\tif (new_below)\n\t\terr = vma_adjust(vma, addr, vma->vm_end, vma->vm_pgoff +\n\t\t\t((addr - new->vm_start) >> PAGE_SHIFT), new);\n\telse\n\t\terr = vma_adjust(vma, vma->vm_start, addr, vma->vm_pgoff, new);\n\n\t/* Success. */\n\tif (!err)\n\t\treturn 0;\n\n\t/* Clean everything up if vma_adjust failed. */\n\tif (new->vm_ops && new->vm_ops->close)\n\t\tnew->vm_ops->close(new);\n\tif (new->vm_file)\n\t\tfput(new->vm_file);\n\tunlink_anon_vmas(new);\n out_free_mpol:\n\tmpol_put(vma_policy(new));\n out_free_vma:\n\tvm_area_free(new);\n\treturn err;\n}\n\n/*\n * Split a vma into two pieces at address 'addr', a new vma is allocated\n * either for the first part or the tail.\n */\nint split_vma(struct mm_struct *mm, struct vm_area_struct *vma,\n\t      unsigned long addr, int new_below)\n{\n\tif (mm->map_count >= sysctl_max_map_count)\n\t\treturn -ENOMEM;\n\n\treturn __split_vma(mm, vma, addr, new_below);\n}\n\n/* Munmap is split into 2 main parts -- this part which finds\n * what needs doing, and the areas themselves, which do the\n * work.  This now handles partial unmappings.\n * Jeremy Fitzhardinge <jeremy@goop.org>\n */\nint __do_munmap(struct mm_struct *mm, unsigned long start, size_t len,\n\t\tstruct list_head *uf, bool downgrade)\n{\n\tunsigned long end;\n\tstruct vm_area_struct *vma, *prev, *last;\n\n\tif ((offset_in_page(start)) || start > TASK_SIZE || len > TASK_SIZE-start)\n\t\treturn -EINVAL;\n\n\tlen = PAGE_ALIGN(len);\n\tif (len == 0)\n\t\treturn -EINVAL;\n\n\t/* Find the first overlapping VMA */\n\tvma = find_vma(mm, start);\n\tif (!vma)\n\t\treturn 0;\n\tprev = vma->vm_prev;\n\t/* we have  start < vma->vm_end  */\n\n\t/* if it doesn't overlap, we have nothing.. */\n\tend = start + len;\n\tif (vma->vm_start >= end)\n\t\treturn 0;\n\n\t/*\n\t * If we need to split any vma, do it now to save pain later.\n\t *\n\t * Note: mremap's move_vma VM_ACCOUNT handling assumes a partially\n\t * unmapped vm_area_struct will remain in use: so lower split_vma\n\t * places tmp vma above, and higher split_vma places tmp vma below.\n\t */\n\tif (start > vma->vm_start) {\n\t\tint error;\n\n\t\t/*\n\t\t * Make sure that map_count on return from munmap() will\n\t\t * not exceed its limit; but let map_count go just above\n\t\t * its limit temporarily, to help free resources as expected.\n\t\t */\n\t\tif (end < vma->vm_end && mm->map_count >= sysctl_max_map_count)\n\t\t\treturn -ENOMEM;\n\n\t\terror = __split_vma(mm, vma, start, 0);\n\t\tif (error)\n\t\t\treturn error;\n\t\tprev = vma;\n\t}\n\n\t/* Does it split the last one? */\n\tlast = find_vma(mm, end);\n\tif (last && end > last->vm_start) {\n\t\tint error = __split_vma(mm, last, end, 1);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\tvma = prev ? prev->vm_next : mm->mmap;\n\n\tif (unlikely(uf)) {\n\t\t/*\n\t\t * If userfaultfd_unmap_prep returns an error the vmas\n\t\t * will remain splitted, but userland will get a\n\t\t * highly unexpected error anyway. This is no\n\t\t * different than the case where the first of the two\n\t\t * __split_vma fails, but we don't undo the first\n\t\t * split, despite we could. This is unlikely enough\n\t\t * failure that it's not worth optimizing it for.\n\t\t */\n\t\tint error = userfaultfd_unmap_prep(vma, start, end, uf);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\t/*\n\t * unlock any mlock()ed ranges before detaching vmas\n\t */\n\tif (mm->locked_vm) {\n\t\tstruct vm_area_struct *tmp = vma;\n\t\twhile (tmp && tmp->vm_start < end) {\n\t\t\tif (tmp->vm_flags & VM_LOCKED) {\n\t\t\t\tmm->locked_vm -= vma_pages(tmp);\n\t\t\t\tmunlock_vma_pages_all(tmp);\n\t\t\t}\n\n\t\t\ttmp = tmp->vm_next;\n\t\t}\n\t}\n\n\t/* Detach vmas from rbtree */\n\tdetach_vmas_to_be_unmapped(mm, vma, prev, end);\n\n\t/*\n\t * mpx unmap needs to be called with mmap_sem held for write.\n\t * It is safe to call it before unmap_region().\n\t */\n\tarch_unmap(mm, vma, start, end);\n\n\tif (downgrade)\n\t\tdowngrade_write(&mm->mmap_sem);\n\n\tunmap_region(mm, vma, prev, start, end);\n\n\t/* Fix up all other VM information */\n\tremove_vma_list(mm, vma);\n\n\treturn downgrade ? 1 : 0;\n}\n\nint do_munmap(struct mm_struct *mm, unsigned long start, size_t len,\n\t      struct list_head *uf)\n{\n\treturn __do_munmap(mm, start, len, uf, false);\n}\n\nstatic int __vm_munmap(unsigned long start, size_t len, bool downgrade)\n{\n\tint ret;\n\tstruct mm_struct *mm = current->mm;\n\tLIST_HEAD(uf);\n\n\tif (down_write_killable(&mm->mmap_sem))\n\t\treturn -EINTR;\n\n\tret = __do_munmap(mm, start, len, &uf, downgrade);\n\t/*\n\t * Returning 1 indicates mmap_sem is downgraded.\n\t * But 1 is not legal return value of vm_munmap() and munmap(), reset\n\t * it to 0 before return.\n\t */\n\tif (ret == 1) {\n\t\tup_read(&mm->mmap_sem);\n\t\tret = 0;\n\t} else\n\t\tup_write(&mm->mmap_sem);\n\n\tuserfaultfd_unmap_complete(mm, &uf);\n\treturn ret;\n}\n\nint vm_munmap(unsigned long start, size_t len)\n{\n\treturn __vm_munmap(start, len, false);\n}\nEXPORT_SYMBOL(vm_munmap);\n\nSYSCALL_DEFINE2(munmap, unsigned long, addr, size_t, len)\n{\n\tprofile_munmap(addr);\n\treturn __vm_munmap(addr, len, true);\n}\n\n\n/*\n * Emulation of deprecated remap_file_pages() syscall.\n */\nSYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,\n\t\tunsigned long, prot, unsigned long, pgoff, unsigned long, flags)\n{\n\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long populate = 0;\n\tunsigned long ret = -EINVAL;\n\tstruct file *file;\n\n\tpr_warn_once(\"%s (%d) uses deprecated remap_file_pages() syscall. See Documentation/vm/remap_file_pages.rst.\\n\",\n\t\t     current->comm, current->pid);\n\n\tif (prot)\n\t\treturn ret;\n\tstart = start & PAGE_MASK;\n\tsize = size & PAGE_MASK;\n\n\tif (start + size <= start)\n\t\treturn ret;\n\n\t/* Does pgoff wrap? */\n\tif (pgoff + (size >> PAGE_SHIFT) < pgoff)\n\t\treturn ret;\n\n\tif (down_write_killable(&mm->mmap_sem))\n\t\treturn -EINTR;\n\n\tvma = find_vma(mm, start);\n\n\tif (!vma || !(vma->vm_flags & VM_SHARED))\n\t\tgoto out;\n\n\tif (start < vma->vm_start)\n\t\tgoto out;\n\n\tif (start + size > vma->vm_end) {\n\t\tstruct vm_area_struct *next;\n\n\t\tfor (next = vma->vm_next; next; next = next->vm_next) {\n\t\t\t/* hole between vmas ? */\n\t\t\tif (next->vm_start != next->vm_prev->vm_end)\n\t\t\t\tgoto out;\n\n\t\t\tif (next->vm_file != vma->vm_file)\n\t\t\t\tgoto out;\n\n\t\t\tif (next->vm_flags != vma->vm_flags)\n\t\t\t\tgoto out;\n\n\t\t\tif (start + size <= next->vm_end)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!next)\n\t\t\tgoto out;\n\t}\n\n\tprot |= vma->vm_flags & VM_READ ? PROT_READ : 0;\n\tprot |= vma->vm_flags & VM_WRITE ? PROT_WRITE : 0;\n\tprot |= vma->vm_flags & VM_EXEC ? PROT_EXEC : 0;\n\n\tflags &= MAP_NONBLOCK;\n\tflags |= MAP_SHARED | MAP_FIXED | MAP_POPULATE;\n\tif (vma->vm_flags & VM_LOCKED) {\n\t\tstruct vm_area_struct *tmp;\n\t\tflags |= MAP_LOCKED;\n\n\t\t/* drop PG_Mlocked flag for over-mapped range */\n\t\tfor (tmp = vma; tmp->vm_start >= start + size;\n\t\t\t\ttmp = tmp->vm_next) {\n\t\t\t/*\n\t\t\t * Split pmd and munlock page on the border\n\t\t\t * of the range.\n\t\t\t */\n\t\t\tvma_adjust_trans_huge(tmp, start, start + size, 0);\n\n\t\t\tmunlock_vma_pages_range(tmp,\n\t\t\t\t\tmax(tmp->vm_start, start),\n\t\t\t\t\tmin(tmp->vm_end, start + size));\n\t\t}\n\t}\n\n\tfile = get_file(vma->vm_file);\n\tret = do_mmap_pgoff(vma->vm_file, start, size,\n\t\t\tprot, flags, pgoff, &populate, NULL);\n\tfput(file);\nout:\n\tup_write(&mm->mmap_sem);\n\tif (populate)\n\t\tmm_populate(ret, populate);\n\tif (!IS_ERR_VALUE(ret))\n\t\tret = 0;\n\treturn ret;\n}\n\n/*\n *  this is really a simplified \"do_mmap\".  it only handles\n *  anonymous maps.  eventually we may be able to do some\n *  brk-specific accounting here.\n */\nstatic int do_brk_flags(unsigned long addr, unsigned long len, unsigned long flags, struct list_head *uf)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma, *prev;\n\tstruct rb_node **rb_link, *rb_parent;\n\tpgoff_t pgoff = addr >> PAGE_SHIFT;\n\tint error;\n\n\t/* Until we need other flags, refuse anything except VM_EXEC. */\n\tif ((flags & (~VM_EXEC)) != 0)\n\t\treturn -EINVAL;\n\tflags |= VM_DATA_DEFAULT_FLAGS | VM_ACCOUNT | mm->def_flags;\n\n\terror = get_unmapped_area(NULL, addr, len, 0, MAP_FIXED);\n\tif (offset_in_page(error))\n\t\treturn error;\n\n\terror = mlock_future_check(mm, mm->def_flags, len);\n\tif (error)\n\t\treturn error;\n\n\t/*\n\t * Clear old maps.  this also does some error checking for us\n\t */\n\twhile (find_vma_links(mm, addr, addr + len, &prev, &rb_link,\n\t\t\t      &rb_parent)) {\n\t\tif (do_munmap(mm, addr, len, uf))\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* Check against address space limits *after* clearing old maps... */\n\tif (!may_expand_vm(mm, flags, len >> PAGE_SHIFT))\n\t\treturn -ENOMEM;\n\n\tif (mm->map_count > sysctl_max_map_count)\n\t\treturn -ENOMEM;\n\n\tif (security_vm_enough_memory_mm(mm, len >> PAGE_SHIFT))\n\t\treturn -ENOMEM;\n\n\t/* Can we just expand an old private anonymous mapping? */\n\tvma = vma_merge(mm, prev, addr, addr + len, flags,\n\t\t\tNULL, NULL, pgoff, NULL, NULL_VM_UFFD_CTX);\n\tif (vma)\n\t\tgoto out;\n\n\t/*\n\t * create a vma struct for an anonymous mapping\n\t */\n\tvma = vm_area_alloc(mm);\n\tif (!vma) {\n\t\tvm_unacct_memory(len >> PAGE_SHIFT);\n\t\treturn -ENOMEM;\n\t}\n\n\tvma_set_anonymous(vma);\n\tvma->vm_start = addr;\n\tvma->vm_end = addr + len;\n\tvma->vm_pgoff = pgoff;\n\tvma->vm_flags = flags;\n\tvma->vm_page_prot = vm_get_page_prot(flags);\n\tvma_link(mm, vma, prev, rb_link, rb_parent);\nout:\n\tperf_event_mmap(vma);\n\tmm->total_vm += len >> PAGE_SHIFT;\n\tmm->data_vm += len >> PAGE_SHIFT;\n\tif (flags & VM_LOCKED)\n\t\tmm->locked_vm += (len >> PAGE_SHIFT);\n\tvma->vm_flags |= VM_SOFTDIRTY;\n\treturn 0;\n}\n\nint vm_brk_flags(unsigned long addr, unsigned long request, unsigned long flags)\n{\n\tstruct mm_struct *mm = current->mm;\n\tunsigned long len;\n\tint ret;\n\tbool populate;\n\tLIST_HEAD(uf);\n\n\tlen = PAGE_ALIGN(request);\n\tif (len < request)\n\t\treturn -ENOMEM;\n\tif (!len)\n\t\treturn 0;\n\n\tif (down_write_killable(&mm->mmap_sem))\n\t\treturn -EINTR;\n\n\tret = do_brk_flags(addr, len, flags, &uf);\n\tpopulate = ((mm->def_flags & VM_LOCKED) != 0);\n\tup_write(&mm->mmap_sem);\n\tuserfaultfd_unmap_complete(mm, &uf);\n\tif (populate && !ret)\n\t\tmm_populate(addr, len);\n\treturn ret;\n}\nEXPORT_SYMBOL(vm_brk_flags);\n\nint vm_brk(unsigned long addr, unsigned long len)\n{\n\treturn vm_brk_flags(addr, len, 0);\n}\nEXPORT_SYMBOL(vm_brk);\n\n/* Release all mmaps. */\nvoid exit_mmap(struct mm_struct *mm)\n{\n\tstruct mmu_gather tlb;\n\tstruct vm_area_struct *vma;\n\tunsigned long nr_accounted = 0;\n\n\t/* mm's last user has gone, and its about to be pulled down */\n\tmmu_notifier_release(mm);\n\n\tif (unlikely(mm_is_oom_victim(mm))) {\n\t\t/*\n\t\t * Manually reap the mm to free as much memory as possible.\n\t\t * Then, as the oom reaper does, set MMF_OOM_SKIP to disregard\n\t\t * this mm from further consideration.  Taking mm->mmap_sem for\n\t\t * write after setting MMF_OOM_SKIP will guarantee that the oom\n\t\t * reaper will not run on this mm again after mmap_sem is\n\t\t * dropped.\n\t\t *\n\t\t * Nothing can be holding mm->mmap_sem here and the above call\n\t\t * to mmu_notifier_release(mm) ensures mmu notifier callbacks in\n\t\t * __oom_reap_task_mm() will not block.\n\t\t *\n\t\t * This needs to be done before calling munlock_vma_pages_all(),\n\t\t * which clears VM_LOCKED, otherwise the oom reaper cannot\n\t\t * reliably test it.\n\t\t */\n\t\t(void)__oom_reap_task_mm(mm);\n\n\t\tset_bit(MMF_OOM_SKIP, &mm->flags);\n\t\tdown_write(&mm->mmap_sem);\n\t\tup_write(&mm->mmap_sem);\n\t}\n\n\tif (mm->locked_vm) {\n\t\tvma = mm->mmap;\n\t\twhile (vma) {\n\t\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\t\tmunlock_vma_pages_all(vma);\n\t\t\tvma = vma->vm_next;\n\t\t}\n\t}\n\n\tarch_exit_mmap(mm);\n\n\tvma = mm->mmap;\n\tif (!vma)\t/* Can happen if dup_mmap() received an OOM */\n\t\treturn;\n\n\tlru_add_drain();\n\tflush_cache_mm(mm);\n\ttlb_gather_mmu(&tlb, mm, 0, -1);\n\t/* update_hiwater_rss(mm) here? but nobody should be looking */\n\t/* Use -1 here to ensure all VMAs in the mm are unmapped */\n\tunmap_vmas(&tlb, vma, 0, -1);\n\tfree_pgtables(&tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);\n\ttlb_finish_mmu(&tlb, 0, -1);\n\n\t/*\n\t * Walk the list again, actually closing and freeing it,\n\t * with preemption enabled, without holding any MM locks.\n\t */\n\twhile (vma) {\n\t\tif (vma->vm_flags & VM_ACCOUNT)\n\t\t\tnr_accounted += vma_pages(vma);\n\t\tvma = remove_vma(vma);\n\t}\n\tvm_unacct_memory(nr_accounted);\n}\n\n/* Insert vm structure into process list sorted by address\n * and into the inode's i_mmap tree.  If vm_file is non-NULL\n * then i_mmap_rwsem is taken here.\n */\nint insert_vm_struct(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tstruct vm_area_struct *prev;\n\tstruct rb_node **rb_link, *rb_parent;\n\n\tif (find_vma_links(mm, vma->vm_start, vma->vm_end,\n\t\t\t   &prev, &rb_link, &rb_parent))\n\t\treturn -ENOMEM;\n\tif ((vma->vm_flags & VM_ACCOUNT) &&\n\t     security_vm_enough_memory_mm(mm, vma_pages(vma)))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * The vm_pgoff of a purely anonymous vma should be irrelevant\n\t * until its first write fault, when page's anon_vma and index\n\t * are set.  But now set the vm_pgoff it will almost certainly\n\t * end up with (unless mremap moves it elsewhere before that\n\t * first wfault), so /proc/pid/maps tells a consistent story.\n\t *\n\t * By setting it to reflect the virtual start address of the\n\t * vma, merges and splits can happen in a seamless way, just\n\t * using the existing file pgoff checks and manipulations.\n\t * Similarly in do_mmap_pgoff and in do_brk.\n\t */\n\tif (vma_is_anonymous(vma)) {\n\t\tBUG_ON(vma->anon_vma);\n\t\tvma->vm_pgoff = vma->vm_start >> PAGE_SHIFT;\n\t}\n\n\tvma_link(mm, vma, prev, rb_link, rb_parent);\n\treturn 0;\n}\n\n/*\n * Copy the vma structure to a new location in the same mm,\n * prior to moving page table entries, to effect an mremap move.\n */\nstruct vm_area_struct *copy_vma(struct vm_area_struct **vmap,\n\tunsigned long addr, unsigned long len, pgoff_t pgoff,\n\tbool *need_rmap_locks)\n{\n\tstruct vm_area_struct *vma = *vmap;\n\tunsigned long vma_start = vma->vm_start;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct vm_area_struct *new_vma, *prev;\n\tstruct rb_node **rb_link, *rb_parent;\n\tbool faulted_in_anon_vma = true;\n\n\t/*\n\t * If anonymous vma has not yet been faulted, update new pgoff\n\t * to match new location, to increase its chance of merging.\n\t */\n\tif (unlikely(vma_is_anonymous(vma) && !vma->anon_vma)) {\n\t\tpgoff = addr >> PAGE_SHIFT;\n\t\tfaulted_in_anon_vma = false;\n\t}\n\n\tif (find_vma_links(mm, addr, addr + len, &prev, &rb_link, &rb_parent))\n\t\treturn NULL;\t/* should never get here */\n\tnew_vma = vma_merge(mm, prev, addr, addr + len, vma->vm_flags,\n\t\t\t    vma->anon_vma, vma->vm_file, pgoff, vma_policy(vma),\n\t\t\t    vma->vm_userfaultfd_ctx);\n\tif (new_vma) {\n\t\t/*\n\t\t * Source vma may have been merged into new_vma\n\t\t */\n\t\tif (unlikely(vma_start >= new_vma->vm_start &&\n\t\t\t     vma_start < new_vma->vm_end)) {\n\t\t\t/*\n\t\t\t * The only way we can get a vma_merge with\n\t\t\t * self during an mremap is if the vma hasn't\n\t\t\t * been faulted in yet and we were allowed to\n\t\t\t * reset the dst vma->vm_pgoff to the\n\t\t\t * destination address of the mremap to allow\n\t\t\t * the merge to happen. mremap must change the\n\t\t\t * vm_pgoff linearity between src and dst vmas\n\t\t\t * (in turn preventing a vma_merge) to be\n\t\t\t * safe. It is only safe to keep the vm_pgoff\n\t\t\t * linear if there are no pages mapped yet.\n\t\t\t */\n\t\t\tVM_BUG_ON_VMA(faulted_in_anon_vma, new_vma);\n\t\t\t*vmap = vma = new_vma;\n\t\t}\n\t\t*need_rmap_locks = (new_vma->vm_pgoff <= vma->vm_pgoff);\n\t} else {\n\t\tnew_vma = vm_area_dup(vma);\n\t\tif (!new_vma)\n\t\t\tgoto out;\n\t\tnew_vma->vm_start = addr;\n\t\tnew_vma->vm_end = addr + len;\n\t\tnew_vma->vm_pgoff = pgoff;\n\t\tif (vma_dup_policy(vma, new_vma))\n\t\t\tgoto out_free_vma;\n\t\tif (anon_vma_clone(new_vma, vma))\n\t\t\tgoto out_free_mempol;\n\t\tif (new_vma->vm_file)\n\t\t\tget_file(new_vma->vm_file);\n\t\tif (new_vma->vm_ops && new_vma->vm_ops->open)\n\t\t\tnew_vma->vm_ops->open(new_vma);\n\t\tvma_link(mm, new_vma, prev, rb_link, rb_parent);\n\t\t*need_rmap_locks = false;\n\t}\n\treturn new_vma;\n\nout_free_mempol:\n\tmpol_put(vma_policy(new_vma));\nout_free_vma:\n\tvm_area_free(new_vma);\nout:\n\treturn NULL;\n}\n\n/*\n * Return true if the calling process may expand its vm space by the passed\n * number of pages\n */\nbool may_expand_vm(struct mm_struct *mm, vm_flags_t flags, unsigned long npages)\n{\n\tif (mm->total_vm + npages > rlimit(RLIMIT_AS) >> PAGE_SHIFT)\n\t\treturn false;\n\n\tif (is_data_mapping(flags) &&\n\t    mm->data_vm + npages > rlimit(RLIMIT_DATA) >> PAGE_SHIFT) {\n\t\t/* Workaround for Valgrind */\n\t\tif (rlimit(RLIMIT_DATA) == 0 &&\n\t\t    mm->data_vm + npages <= rlimit_max(RLIMIT_DATA) >> PAGE_SHIFT)\n\t\t\treturn true;\n\n\t\tpr_warn_once(\"%s (%d): VmData %lu exceed data ulimit %lu. Update limits%s.\\n\",\n\t\t\t     current->comm, current->pid,\n\t\t\t     (mm->data_vm + npages) << PAGE_SHIFT,\n\t\t\t     rlimit(RLIMIT_DATA),\n\t\t\t     ignore_rlimit_data ? \"\" : \" or use boot option ignore_rlimit_data\");\n\n\t\tif (!ignore_rlimit_data)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nvoid vm_stat_account(struct mm_struct *mm, vm_flags_t flags, long npages)\n{\n\tmm->total_vm += npages;\n\n\tif (is_exec_mapping(flags))\n\t\tmm->exec_vm += npages;\n\telse if (is_stack_mapping(flags))\n\t\tmm->stack_vm += npages;\n\telse if (is_data_mapping(flags))\n\t\tmm->data_vm += npages;\n}\n\nstatic vm_fault_t special_mapping_fault(struct vm_fault *vmf);\n\n/*\n * Having a close hook prevents vma merging regardless of flags.\n */\nstatic void special_mapping_close(struct vm_area_struct *vma)\n{\n}\n\nstatic const char *special_mapping_name(struct vm_area_struct *vma)\n{\n\treturn ((struct vm_special_mapping *)vma->vm_private_data)->name;\n}\n\nstatic int special_mapping_mremap(struct vm_area_struct *new_vma)\n{\n\tstruct vm_special_mapping *sm = new_vma->vm_private_data;\n\n\tif (WARN_ON_ONCE(current->mm != new_vma->vm_mm))\n\t\treturn -EFAULT;\n\n\tif (sm->mremap)\n\t\treturn sm->mremap(sm, new_vma);\n\n\treturn 0;\n}\n\nstatic const struct vm_operations_struct special_mapping_vmops = {\n\t.close = special_mapping_close,\n\t.fault = special_mapping_fault,\n\t.mremap = special_mapping_mremap,\n\t.name = special_mapping_name,\n};\n\nstatic const struct vm_operations_struct legacy_special_mapping_vmops = {\n\t.close = special_mapping_close,\n\t.fault = special_mapping_fault,\n};\n\nstatic vm_fault_t special_mapping_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tpgoff_t pgoff;\n\tstruct page **pages;\n\n\tif (vma->vm_ops == &legacy_special_mapping_vmops) {\n\t\tpages = vma->vm_private_data;\n\t} else {\n\t\tstruct vm_special_mapping *sm = vma->vm_private_data;\n\n\t\tif (sm->fault)\n\t\t\treturn sm->fault(sm, vmf->vma, vmf);\n\n\t\tpages = sm->pages;\n\t}\n\n\tfor (pgoff = vmf->pgoff; pgoff && *pages; ++pages)\n\t\tpgoff--;\n\n\tif (*pages) {\n\t\tstruct page *page = *pages;\n\t\tget_page(page);\n\t\tvmf->page = page;\n\t\treturn 0;\n\t}\n\n\treturn VM_FAULT_SIGBUS;\n}\n\nstatic struct vm_area_struct *__install_special_mapping(\n\tstruct mm_struct *mm,\n\tunsigned long addr, unsigned long len,\n\tunsigned long vm_flags, void *priv,\n\tconst struct vm_operations_struct *ops)\n{\n\tint ret;\n\tstruct vm_area_struct *vma;\n\n\tvma = vm_area_alloc(mm);\n\tif (unlikely(vma == NULL))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tvma->vm_start = addr;\n\tvma->vm_end = addr + len;\n\n\tvma->vm_flags = vm_flags | mm->def_flags | VM_DONTEXPAND | VM_SOFTDIRTY;\n\tvma->vm_page_prot = vm_get_page_prot(vma->vm_flags);\n\n\tvma->vm_ops = ops;\n\tvma->vm_private_data = priv;\n\n\tret = insert_vm_struct(mm, vma);\n\tif (ret)\n\t\tgoto out;\n\n\tvm_stat_account(mm, vma->vm_flags, len >> PAGE_SHIFT);\n\n\tperf_event_mmap(vma);\n\n\treturn vma;\n\nout:\n\tvm_area_free(vma);\n\treturn ERR_PTR(ret);\n}\n\nbool vma_is_special_mapping(const struct vm_area_struct *vma,\n\tconst struct vm_special_mapping *sm)\n{\n\treturn vma->vm_private_data == sm &&\n\t\t(vma->vm_ops == &special_mapping_vmops ||\n\t\t vma->vm_ops == &legacy_special_mapping_vmops);\n}\n\n/*\n * Called with mm->mmap_sem held for writing.\n * Insert a new vma covering the given region, with the given flags.\n * Its pages are supplied by the given array of struct page *.\n * The array can be shorter than len >> PAGE_SHIFT if it's null-terminated.\n * The region past the last page supplied will always produce SIGBUS.\n * The array pointer and the pages it points to are assumed to stay alive\n * for as long as this mapping might exist.\n */\nstruct vm_area_struct *_install_special_mapping(\n\tstruct mm_struct *mm,\n\tunsigned long addr, unsigned long len,\n\tunsigned long vm_flags, const struct vm_special_mapping *spec)\n{\n\treturn __install_special_mapping(mm, addr, len, vm_flags, (void *)spec,\n\t\t\t\t\t&special_mapping_vmops);\n}\n\nint install_special_mapping(struct mm_struct *mm,\n\t\t\t    unsigned long addr, unsigned long len,\n\t\t\t    unsigned long vm_flags, struct page **pages)\n{\n\tstruct vm_area_struct *vma = __install_special_mapping(\n\t\tmm, addr, len, vm_flags, (void *)pages,\n\t\t&legacy_special_mapping_vmops);\n\n\treturn PTR_ERR_OR_ZERO(vma);\n}\n\nstatic DEFINE_MUTEX(mm_all_locks_mutex);\n\nstatic void vm_lock_anon_vma(struct mm_struct *mm, struct anon_vma *anon_vma)\n{\n\tif (!test_bit(0, (unsigned long *) &anon_vma->root->rb_root.rb_root.rb_node)) {\n\t\t/*\n\t\t * The LSB of head.next can't change from under us\n\t\t * because we hold the mm_all_locks_mutex.\n\t\t */\n\t\tdown_write_nest_lock(&anon_vma->root->rwsem, &mm->mmap_sem);\n\t\t/*\n\t\t * We can safely modify head.next after taking the\n\t\t * anon_vma->root->rwsem. If some other vma in this mm shares\n\t\t * the same anon_vma we won't take it again.\n\t\t *\n\t\t * No need of atomic instructions here, head.next\n\t\t * can't change from under us thanks to the\n\t\t * anon_vma->root->rwsem.\n\t\t */\n\t\tif (__test_and_set_bit(0, (unsigned long *)\n\t\t\t\t       &anon_vma->root->rb_root.rb_root.rb_node))\n\t\t\tBUG();\n\t}\n}\n\nstatic void vm_lock_mapping(struct mm_struct *mm, struct address_space *mapping)\n{\n\tif (!test_bit(AS_MM_ALL_LOCKS, &mapping->flags)) {\n\t\t/*\n\t\t * AS_MM_ALL_LOCKS can't change from under us because\n\t\t * we hold the mm_all_locks_mutex.\n\t\t *\n\t\t * Operations on ->flags have to be atomic because\n\t\t * even if AS_MM_ALL_LOCKS is stable thanks to the\n\t\t * mm_all_locks_mutex, there may be other cpus\n\t\t * changing other bitflags in parallel to us.\n\t\t */\n\t\tif (test_and_set_bit(AS_MM_ALL_LOCKS, &mapping->flags))\n\t\t\tBUG();\n\t\tdown_write_nest_lock(&mapping->i_mmap_rwsem, &mm->mmap_sem);\n\t}\n}\n\n/*\n * This operation locks against the VM for all pte/vma/mm related\n * operations that could ever happen on a certain mm. This includes\n * vmtruncate, try_to_unmap, and all page faults.\n *\n * The caller must take the mmap_sem in write mode before calling\n * mm_take_all_locks(). The caller isn't allowed to release the\n * mmap_sem until mm_drop_all_locks() returns.\n *\n * mmap_sem in write mode is required in order to block all operations\n * that could modify pagetables and free pages without need of\n * altering the vma layout. It's also needed in write mode to avoid new\n * anon_vmas to be associated with existing vmas.\n *\n * A single task can't take more than one mm_take_all_locks() in a row\n * or it would deadlock.\n *\n * The LSB in anon_vma->rb_root.rb_node and the AS_MM_ALL_LOCKS bitflag in\n * mapping->flags avoid to take the same lock twice, if more than one\n * vma in this mm is backed by the same anon_vma or address_space.\n *\n * We take locks in following order, accordingly to comment at beginning\n * of mm/rmap.c:\n *   - all hugetlbfs_i_mmap_rwsem_key locks (aka mapping->i_mmap_rwsem for\n *     hugetlb mapping);\n *   - all i_mmap_rwsem locks;\n *   - all anon_vma->rwseml\n *\n * We can take all locks within these types randomly because the VM code\n * doesn't nest them and we protected from parallel mm_take_all_locks() by\n * mm_all_locks_mutex.\n *\n * mm_take_all_locks() and mm_drop_all_locks are expensive operations\n * that may have to take thousand of locks.\n *\n * mm_take_all_locks() can fail if it's interrupted by signals.\n */\nint mm_take_all_locks(struct mm_struct *mm)\n{\n\tstruct vm_area_struct *vma;\n\tstruct anon_vma_chain *avc;\n\n\tBUG_ON(down_read_trylock(&mm->mmap_sem));\n\n\tmutex_lock(&mm_all_locks_mutex);\n\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (signal_pending(current))\n\t\t\tgoto out_unlock;\n\t\tif (vma->vm_file && vma->vm_file->f_mapping &&\n\t\t\t\tis_vm_hugetlb_page(vma))\n\t\t\tvm_lock_mapping(mm, vma->vm_file->f_mapping);\n\t}\n\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (signal_pending(current))\n\t\t\tgoto out_unlock;\n\t\tif (vma->vm_file && vma->vm_file->f_mapping &&\n\t\t\t\t!is_vm_hugetlb_page(vma))\n\t\t\tvm_lock_mapping(mm, vma->vm_file->f_mapping);\n\t}\n\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (signal_pending(current))\n\t\t\tgoto out_unlock;\n\t\tif (vma->anon_vma)\n\t\t\tlist_for_each_entry(avc, &vma->anon_vma_chain, same_vma)\n\t\t\t\tvm_lock_anon_vma(mm, avc->anon_vma);\n\t}\n\n\treturn 0;\n\nout_unlock:\n\tmm_drop_all_locks(mm);\n\treturn -EINTR;\n}\n\nstatic void vm_unlock_anon_vma(struct anon_vma *anon_vma)\n{\n\tif (test_bit(0, (unsigned long *) &anon_vma->root->rb_root.rb_root.rb_node)) {\n\t\t/*\n\t\t * The LSB of head.next can't change to 0 from under\n\t\t * us because we hold the mm_all_locks_mutex.\n\t\t *\n\t\t * We must however clear the bitflag before unlocking\n\t\t * the vma so the users using the anon_vma->rb_root will\n\t\t * never see our bitflag.\n\t\t *\n\t\t * No need of atomic instructions here, head.next\n\t\t * can't change from under us until we release the\n\t\t * anon_vma->root->rwsem.\n\t\t */\n\t\tif (!__test_and_clear_bit(0, (unsigned long *)\n\t\t\t\t\t  &anon_vma->root->rb_root.rb_root.rb_node))\n\t\t\tBUG();\n\t\tanon_vma_unlock_write(anon_vma);\n\t}\n}\n\nstatic void vm_unlock_mapping(struct address_space *mapping)\n{\n\tif (test_bit(AS_MM_ALL_LOCKS, &mapping->flags)) {\n\t\t/*\n\t\t * AS_MM_ALL_LOCKS can't change to 0 from under us\n\t\t * because we hold the mm_all_locks_mutex.\n\t\t */\n\t\ti_mmap_unlock_write(mapping);\n\t\tif (!test_and_clear_bit(AS_MM_ALL_LOCKS,\n\t\t\t\t\t&mapping->flags))\n\t\t\tBUG();\n\t}\n}\n\n/*\n * The mmap_sem cannot be released by the caller until\n * mm_drop_all_locks() returns.\n */\nvoid mm_drop_all_locks(struct mm_struct *mm)\n{\n\tstruct vm_area_struct *vma;\n\tstruct anon_vma_chain *avc;\n\n\tBUG_ON(down_read_trylock(&mm->mmap_sem));\n\tBUG_ON(!mutex_is_locked(&mm_all_locks_mutex));\n\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->anon_vma)\n\t\t\tlist_for_each_entry(avc, &vma->anon_vma_chain, same_vma)\n\t\t\t\tvm_unlock_anon_vma(avc->anon_vma);\n\t\tif (vma->vm_file && vma->vm_file->f_mapping)\n\t\t\tvm_unlock_mapping(vma->vm_file->f_mapping);\n\t}\n\n\tmutex_unlock(&mm_all_locks_mutex);\n}\n\n/*\n * initialise the percpu counter for VM\n */\nvoid __init mmap_init(void)\n{\n\tint ret;\n\n\tret = percpu_counter_init(&vm_committed_as, 0, GFP_KERNEL);\n\tVM_BUG_ON(ret);\n}\n\n/*\n * Initialise sysctl_user_reserve_kbytes.\n *\n * This is intended to prevent a user from starting a single memory hogging\n * process, such that they cannot recover (kill the hog) in OVERCOMMIT_NEVER\n * mode.\n *\n * The default value is min(3% of free memory, 128MB)\n * 128MB is enough to recover with sshd/login, bash, and top/kill.\n */\nstatic int init_user_reserve(void)\n{\n\tunsigned long free_kbytes;\n\n\tfree_kbytes = global_zone_page_state(NR_FREE_PAGES) << (PAGE_SHIFT - 10);\n\n\tsysctl_user_reserve_kbytes = min(free_kbytes / 32, 1UL << 17);\n\treturn 0;\n}\nsubsys_initcall(init_user_reserve);\n\n/*\n * Initialise sysctl_admin_reserve_kbytes.\n *\n * The purpose of sysctl_admin_reserve_kbytes is to allow the sys admin\n * to log in and kill a memory hogging process.\n *\n * Systems with more than 256MB will reserve 8MB, enough to recover\n * with sshd, bash, and top in OVERCOMMIT_GUESS. Smaller systems will\n * only reserve 3% of free pages by default.\n */\nstatic int init_admin_reserve(void)\n{\n\tunsigned long free_kbytes;\n\n\tfree_kbytes = global_zone_page_state(NR_FREE_PAGES) << (PAGE_SHIFT - 10);\n\n\tsysctl_admin_reserve_kbytes = min(free_kbytes / 32, 1UL << 13);\n\treturn 0;\n}\nsubsys_initcall(init_admin_reserve);\n\n/*\n * Reinititalise user and admin reserves if memory is added or removed.\n *\n * The default user reserve max is 128MB, and the default max for the\n * admin reserve is 8MB. These are usually, but not always, enough to\n * enable recovery from a memory hogging process using login/sshd, a shell,\n * and tools like top. It may make sense to increase or even disable the\n * reserve depending on the existence of swap or variations in the recovery\n * tools. So, the admin may have changed them.\n *\n * If memory is added and the reserves have been eliminated or increased above\n * the default max, then we'll trust the admin.\n *\n * If memory is removed and there isn't enough free memory, then we\n * need to reset the reserves.\n *\n * Otherwise keep the reserve set by the admin.\n */\nstatic int reserve_mem_notifier(struct notifier_block *nb,\n\t\t\t     unsigned long action, void *data)\n{\n\tunsigned long tmp, free_kbytes;\n\n\tswitch (action) {\n\tcase MEM_ONLINE:\n\t\t/* Default max is 128MB. Leave alone if modified by operator. */\n\t\ttmp = sysctl_user_reserve_kbytes;\n\t\tif (0 < tmp && tmp < (1UL << 17))\n\t\t\tinit_user_reserve();\n\n\t\t/* Default max is 8MB.  Leave alone if modified by operator. */\n\t\ttmp = sysctl_admin_reserve_kbytes;\n\t\tif (0 < tmp && tmp < (1UL << 13))\n\t\t\tinit_admin_reserve();\n\n\t\tbreak;\n\tcase MEM_OFFLINE:\n\t\tfree_kbytes = global_zone_page_state(NR_FREE_PAGES) << (PAGE_SHIFT - 10);\n\n\t\tif (sysctl_user_reserve_kbytes > free_kbytes) {\n\t\t\tinit_user_reserve();\n\t\t\tpr_info(\"vm.user_reserve_kbytes reset to %lu\\n\",\n\t\t\t\tsysctl_user_reserve_kbytes);\n\t\t}\n\n\t\tif (sysctl_admin_reserve_kbytes > free_kbytes) {\n\t\t\tinit_admin_reserve();\n\t\t\tpr_info(\"vm.admin_reserve_kbytes reset to %lu\\n\",\n\t\t\t\tsysctl_admin_reserve_kbytes);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block reserve_mem_nb = {\n\t.notifier_call = reserve_mem_notifier,\n};\n\nstatic int __meminit init_reserve_notifier(void)\n{\n\tif (register_hotmemory_notifier(&reserve_mem_nb))\n\t\tpr_err(\"Failed registering memory add/remove notifier for admin reserve\\n\");\n\n\treturn 0;\n}\nsubsys_initcall(init_reserve_notifier);\n"], "filenames": ["drivers/infiniband/core/uverbs_main.c", "fs/proc/task_mmu.c", "fs/userfaultfd.c", "include/linux/sched/mm.h", "mm/mmap.c"], "buggy_code_start_loc": [995, 1145, 631, 49, 47], "buggy_code_end_loc": [1009, 1145, 1522, 49, 2552], "fixing_code_start_loc": [996, 1146, 632, 50, 48], "fixing_code_end_loc": [1013, 1164, 1532, 71, 2558], "type": "CWE-667", "message": "The coredump implementation in the Linux kernel before 5.0.10 does not use locking or other mechanisms to prevent vma layout or vma flags changes while it runs, which allows local users to obtain sensitive information, cause a denial of service, or possibly have unspecified other impact by triggering a race condition with mmget_not_zero or get_task_mm calls. This is related to fs/userfaultfd.c, mm/mmap.c, fs/proc/task_mmu.c, and drivers/infiniband/core/uverbs_main.c.", "other": {"cve": {"id": "CVE-2019-11599", "sourceIdentifier": "cve@mitre.org", "published": "2019-04-29T18:29:00.243", "lastModified": "2021-06-14T18:15:15.800", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The coredump implementation in the Linux kernel before 5.0.10 does not use locking or other mechanisms to prevent vma layout or vma flags changes while it runs, which allows local users to obtain sensitive information, cause a denial of service, or possibly have unspecified other impact by triggering a race condition with mmget_not_zero or get_task_mm calls. This is related to fs/userfaultfd.c, mm/mmap.c, fs/proc/task_mmu.c, and drivers/infiniband/core/uverbs_main.c."}, {"lang": "es", "value": "La implementaci\u00f3n del coredump en el kernel de Linux en versiones anteriores a 5.0.10,  no utiliza mecanismos de bloqueo u otros mecanismos para evitar cambios en el layout de vma o en los flags vma mientras se ejecuta, lo que permite a los usuarios locales obtener informaci\u00f3n sensible, causar una denegaci\u00f3n de servicio o posiblemente tener otro impacto no especificado al activar una condici\u00f3n de carrera con llamadas mmget_not_zero o get_task_mm. Esto est\u00e1 relacionado con fs/userfaultfd.c, mm/mmap.c, fs/proc/task_mmu.c, y drivers/infiniband/core/uverbs_main.c"}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.0, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.0, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 6.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-667"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.0.10", "matchCriteriaId": "995B7430-4BA4-4979-BD8E-8907622852A8"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2019-07/msg00014.html", "source": "cve@mitre.org"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2019-07/msg00025.html", "source": "cve@mitre.org"}, {"url": "http://packetstormsecurity.com/files/152663/Linux-Missing-Lockdown.html", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory", "VDB Entry"]}, {"url": "http://packetstormsecurity.com/files/153702/Slackware-Security-Advisory-Slackware-14.2-kernel-Updates.html", "source": "cve@mitre.org"}, {"url": "http://www.openwall.com/lists/oss-security/2019/04/29/1", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2019/04/29/2", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2019/04/30/1", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/108113", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:2029", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2019:2043", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2019:3309", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2019:3517", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2020:0100", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2020:0103", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2020:0179", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2020:0543", "source": "cve@mitre.org"}, {"url": "https://bugs.chromium.org/p/project-zero/issues/detail?id=1790", "source": "cve@mitre.org", "tags": ["Mailing List", "Exploit", "Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.14.114", "source": "cve@mitre.org", "tags": ["Mailing List", "Vendor Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.19.37", "source": "cve@mitre.org", "tags": ["Mailing List", "Vendor Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.0.10", "source": "cve@mitre.org", "tags": ["Mailing List", "Vendor Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=04f5866e41fb70690e28397487d8bd8eea7d712a", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/04f5866e41fb70690e28397487d8bd8eea7d712a", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2019/05/msg00041.html", "source": "cve@mitre.org"}, {"url": "https://lists.debian.org/debian-lts-announce/2019/05/msg00042.html", "source": "cve@mitre.org"}, {"url": "https://lists.debian.org/debian-lts-announce/2019/06/msg00011.html", "source": "cve@mitre.org"}, {"url": "https://seclists.org/bugtraq/2019/Jul/33", "source": "cve@mitre.org"}, {"url": "https://seclists.org/bugtraq/2019/Jun/26", "source": "cve@mitre.org"}, {"url": "https://security.netapp.com/advisory/ntap-20190517-0002/", "source": "cve@mitre.org"}, {"url": "https://security.netapp.com/advisory/ntap-20200608-0001/", "source": "cve@mitre.org"}, {"url": "https://support.f5.com/csp/article/K51674118", "source": "cve@mitre.org"}, {"url": "https://support.f5.com/csp/article/K51674118?utm_source=f5support&amp;utm_medium=RSS", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4069-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4069-2/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4095-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4115-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4118-1/", "source": "cve@mitre.org"}, {"url": "https://www.debian.org/security/2019/dsa-4465", "source": "cve@mitre.org"}, {"url": "https://www.exploit-db.com/exploits/46781/", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory", "VDB Entry"]}, {"url": "https://www.oracle.com/security-alerts/cpuApr2021.html", "source": "cve@mitre.org"}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/04f5866e41fb70690e28397487d8bd8eea7d712a"}}