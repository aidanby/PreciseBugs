{"buggy_code": [".. _important_notes_1x:\n\nImportant notes 1.x\n===================\n\nThis section provides information about security and corruption issues.\n\n.. _hashindex_set_bug:\n\nPre-1.1.11 potential index corruption / data loss issue\n-------------------------------------------------------\n\nA bug was discovered in our hashtable code, see issue #4829.\nThe code is used for the client-side chunks cache and the server-side repo index.\n\nAlthough borg uses the hashtables very heavily, the index corruption did not\nhappen too frequently, because it needed specific conditions to happen.\n\nData loss required even more specific conditions, so it should be rare (and\nalso detectable via borg check).\n\nYou might be affected if borg crashed with / complained about:\n\n- AssertionError: Corrupted segment reference count - corrupted index or hints\n- ObjectNotFound: Object with key ... not found in repository ...\n- Index mismatch for key b'...'. (..., ...) != (-1, -1)\n- ValueError: stats_against: key contained in self but not in master_index.\n\nAdvised procedure to fix any related issue in your indexes/caches:\n\n- install fixed borg code (on client AND server)\n- for all of your clients and repos remove the cache by:\n\n  borg delete --cache-only YOURREPO\n\n  (later, the cache will be re-built automatically)\n- for all your repos, rebuild the repo index by:\n\n  borg check --repair YOURREPO\n\n  This will also check all archives and detect if there is any data-loss issue.\n\nAffected branches / releases:\n\n- fd06497 introduced the bug into 1.1-maint branch - it affects all borg 1.1.x since 1.1.0b4.\n- fd06497 introduced the bug into master branch - it affects all borg 1.2.0 alpha releases.\n- c5cd882 introduced the bug into 1.0-maint branch - it affects all borg 1.0.x since 1.0.11rc1.\n\nThe bug was fixed by:\n\n- 701159a fixes the bug in 1.1-maint branch - will be released with borg 1.1.11.\n- fa63150 fixes the bug in master branch - will be released with borg 1.2.0a8.\n- 7bb90b6 fixes the bug in 1.0-maint branch. Branch is EOL, no new release is planned as of now.\n\n.. _broken_validator:\n\nPre-1.1.4 potential data corruption issue\n-----------------------------------------\n\nA data corruption bug was discovered in borg check --repair, see issue #3444.\n\nThis is a 1.1.x regression, releases < 1.1 (e.g. 1.0.x) are not affected.\n\nTo avoid data loss, you must not run borg check --repair using an unfixed version\nof borg 1.1.x. The first official release that has the fix is 1.1.4.\n\nPackage maintainers may have applied the fix to updated packages of 1.1.x (x<4)\nthough, see the package maintainer's package changelog to make sure.\n\nIf you never had missing item metadata chunks, the bug has not affected you\neven if you did run borg check --repair with an unfixed version.\n\nWhen borg check --repair tried to repair corrupt archives that miss item metadata\nchunks, the resync to valid metadata in still present item metadata chunks\nmalfunctioned. This was due to a broken validator that considered all (even valid)\nitem metadata as invalid. As they were considered invalid, borg discarded them.\nPractically, that means the affected files, directories or other fs objects were\ndiscarded from the archive.\n\nDue to the malfunction, the process was extremely slow, but if you let it\ncomplete, borg would have created a \"repaired\" archive that has lost a lot of items.\nIf you interrupted borg check --repair because it was so strangely slow (killing\nborg somehow, e.g. Ctrl-C) the transaction was rolled back and no corruption occurred.\n\nThe log message indicating the precondition for the bug triggering looks like:\n\n    item metadata chunk missing [chunk: 001056_bdee87d...a3e50d]\n\nIf you never had that in your borg check --repair runs, you're not affected.\n\nBut if you're unsure or you actually have seen that, better check your archives.\nBy just using \"borg list repo::archive\" you can see if all expected filesystem\nitems are listed.\n\n.. _tam_vuln:\n\nPre-1.0.9 manifest spoofing vulnerability (CVE-2016-10099)\n----------------------------------------------------------\n\nA flaw in the cryptographic authentication scheme in Borg allowed an attacker\nto spoof the manifest. The attack requires an attacker to be able to\n\n1. insert files (with no additional headers) into backups\n2. gain write access to the repository\n\nThis vulnerability does not disclose plaintext to the attacker, nor does it\naffect the authenticity of existing archives.\n\nThe vulnerability allows an attacker to create a spoofed manifest (the list of archives).\nCreating plausible fake archives may be feasible for small archives, but is unlikely\nfor large archives.\n\nThe fix adds a separate authentication tag to the manifest. For compatibility\nwith prior versions this authentication tag is *not* required by default\nfor existing repositories. Repositories created with 1.0.9 and later require it.\n\nSteps you should take:\n\n1. Upgrade all clients to 1.0.9 or later.\n2. Run ``borg upgrade --tam <repository>`` *on every client* for *each* repository.\n3. This will list all archives, including archive IDs, for easy comparison with your logs.\n4. Done.\n\nPrior versions can access and modify repositories with this measure enabled, however,\nto 1.0.9 or later their modifications are indiscernible from an attack and will\nraise an error until the below procedure is followed. We are aware that this can\nbe annoying in some circumstances, but don't see a way to fix the vulnerability\notherwise.\n\nIn case a version prior to 1.0.9 is used to modify a repository where above procedure\nwas completed, and now you get an error message from other clients:\n\n1. ``borg upgrade --tam --force <repository>`` once with *any* client suffices.\n\nThis attack is mitigated by:\n\n- Noting/logging ``borg list``, ``borg info``, or ``borg create --stats``, which\n  contain the archive IDs.\n\nWe are not aware of others having discovered, disclosed or exploited this vulnerability.\n\nVulnerability time line:\n\n* 2016-11-14: Vulnerability and fix discovered during review of cryptography by Marian Beermann (@enkore)\n* 2016-11-20: First patch\n* 2016-12-20: Released fixed version 1.0.9\n* 2017-01-02: CVE was assigned\n* 2017-01-15: Released fixed version 1.1.0b3 (fix was previously only available from source)\n\n.. _attic013_check_corruption:\n\nPre-1.0.9 potential data loss\n-----------------------------\n\nIf you have archives in your repository that were made with attic <= 0.13\n(and later migrated to borg), running borg check would report errors in these\narchives. See issue #1837.\n\nThe reason for this is a invalid (and useless) metadata key that was\nalways added due to a bug in these old attic versions.\n\nIf you run borg check --repair, things escalate quickly: all archive items\nwith invalid metadata will be killed. Due to that attic bug, that means all\nitems in all archives made with these old attic versions.\n\n\nPre-1.0.4 potential repo corruption\n-----------------------------------\n\nSome external errors (like network or disk I/O errors) could lead to\ncorruption of the backup repository due to issue #1138.\n\nA sign that this happened is if \"E\" status was reported for a file that can\nnot be explained by problems with the source file. If you still have logs from\n\"borg create -v --list\", you can check for \"E\" status.\n\nHere is what could cause corruption and what you can do now:\n\n1) I/O errors (e.g. repo disk errors) while writing data to repo.\n\nThis could lead to corrupted segment files.\n\nFix::\n\n    # check for corrupt chunks / segments:\n    borg check -v --repository-only REPO\n\n    # repair the repo:\n    borg check -v --repository-only --repair REPO\n\n    # make sure everything is fixed:\n    borg check -v --repository-only REPO\n\n2) Unreliable network / unreliable connection to the repo.\n\nThis could lead to archive metadata corruption.\n\nFix::\n\n    # check for corrupt archives:\n    borg check -v --archives-only REPO\n\n    # delete the corrupt archives:\n    borg delete --force REPO::CORRUPT_ARCHIVE\n\n    # make sure everything is fixed:\n    borg check -v --archives-only REPO\n\n3) In case you want to do more intensive checking.\n\nThe best check that everything is ok is to run a dry-run extraction::\n\n    borg extract -v --dry-run REPO::ARCHIVE\n\n.. _changelog_1x:\n\nChange Log 1.x\n==============\n\nVersion 1.3.0a1 (2022-04-15)\n----------------------------\n\nPlease note:\n\nThis is an alpha release, only for testing - do not use this with production repos.\n\nNew features:\n\n- init: new --encryption=(repokey|keyfile)-[blake2-](aes-ocb|chacha20-poly1305)\n\n  - New, better, faster crypto (see encryption-aead diagram in the docs), #6463.\n  - New AEAD cipher suites: AES-OCB and CHACHA20-POLY1305.\n  - Session keys are derived via HKDF from random session id and master key.\n  - Nonces/MessageIVs are counters starting from 0 for each session.\n  - AAD: chunk id, key type, messageIV, sessionID are now authenticated also.\n  - Solves the potential AES-CTR mode counter management issues of the legacy crypto.\n- init: --key-algorithm=argon2 (new default KDF, older pbkdf2 also still available)\n\n  borg key change-passphrase / change-location keeps the key algorithm unchanged.\n- key change-algorithm: to upgrade existing keys to argon2 or downgrade to pbkdf2.\n\n  We recommend you to upgrade unless you have to keep the key compatible with older versions of borg.\n- key change-location: usable for repokey <-> keyfile location change\n- benchmark cpu: display benchmarks of cpu bound stuff\n- export-tar: new --tar-format=PAX (default: GNU)\n- import-tar/export-tar: can use PAX format for ctime and atime support\n- import-tar/export-tar: --tar-format=BORG: roundtrip ALL item metadata, #5830\n- repository: create and use version 2 repos only for now\n- repository: implement PUT2: header crc32, overall xxh64, #1704\n\nOther changes:\n\n- require python >= 3.9, #6315\n- simplify libs setup, #6482\n- unbundle most bundled 3rd party code, use libs, #6316\n- use libdeflate.crc32 (Linux and all others) or zlib.crc32 (macOS)\n- repository: code cleanups / simplifications\n- internal crypto api: speedups / cleanups / refactorings / modernisation\n- remove \"borg upgrade\" support for \"attic backup\" repos\n- remove PassphraseKey code and borg key migrate-to-repokey command\n- OpenBSD: build borg with OpenSSL (not: LibreSSL), #6474\n- remove support for LibreSSL, #6474\n- remove support for OpenSSL < 1.1.1\n\n\nVersion 1.2.0 (2022-02-22 22:02:22 :-)\n--------------------------------------\n\nPlease note:\n\nThis is the first borg 1.2 release, so be careful and read the notes below.\n\nUpgrade notes:\n\nStrictly taken, nothing special is required for upgrading to 1.2, but some\nthings can be recommended:\n\n- do you already want to upgrade? 1.1.x also will get fixes for a while.\n- be careful, first upgrade your less critical / smaller repos.\n- first upgrade to a recent 1.1.x release - especially if you run some older\n  1.1.* or even 1.0.* borg release.\n- using that, run at least one `borg create` (your normal backup), `prune`\n  and especially a `check` to see everything is in a good state.\n- check the output of `borg check` - if there is anything special, consider\n  a `borg check --repair` followed by another `borg check`.\n- if everything is fine so far (borg check reports no issues), you can consider\n  upgrading to 1.2.0. if not, please first fix any already existing issue.\n- if you want to play safer, first **create a backup of your borg repository**.\n- upgrade to latest borg 1.2.x release (you could use the fat binary from\n  github releases page)\n- run `borg compact --cleanup-commits` to clean up a ton of 17 bytes long files\n  in your repo caused by a borg 1.1 bug\n- run `borg check` again (now with borg 1.2.x) and check if there is anything\n  special.\n- run `borg info` (with borg 1.2.x) to build the local pre12-meta cache (can\n  take significant time, but after that it will be fast) - for more details\n  see below.\n- check the compatibility notes (see below) and adapt your scripts, if needed.\n- if you run into any issues, please check the github issue tracker before\n  posting new issues there or elsewhere.\n\nIf you follow this procedure, you can help avoiding that we get a lot of\n\"borg 1.2\" issue reports that are not really 1.2 issues, but existed before\nand maybe just were not noticed.\n\nCompatibility notes:\n\n- matching of path patterns has been aligned with borg storing relative paths.\n  Borg archives file paths without leading slashes. Previously, include/exclude\n  patterns could contain leading slashes. You should check your patterns and\n  remove leading slashes.\n- dropped support / testing for older Pythons, minimum requirement is 3.8.\n  In case your OS does not provide Python >= 3.8, consider using our binary,\n  which does not need an external Python interpreter. Or continue using\n  borg 1.1.x, which is still supported.\n- freeing repository space only happens when \"borg compact\" is invoked.\n- mount: the default for --numeric-ids is False now (same as borg extract)\n- borg create --noatime is deprecated. Not storing atime is the default behaviour\n  now (use --atime if you want to store the atime).\n- list: corrected mix-up of \"isomtime\" and \"mtime\" formats.\n  Previously, \"isomtime\" was the default but produced a verbose human format,\n  while \"mtime\" produced a ISO-8601-like format.\n  The behaviours have been swapped (so \"mtime\" is human, \"isomtime\" is ISO-like),\n  and the default is now \"mtime\".\n  \"isomtime\" is now a real ISO-8601 format (\"T\" between date and time, not a space).\n- create/recreate --list: file status for all files used to get announced *AFTER*\n  the file (with borg < 1.2). Now, file status is announced *BEFORE* the file\n  contents are processed. If the file status changes later (e.g. due to an error\n  or a content change), the updated/final file status will be printed again.\n- removed deprecated-since-long stuff (deprecated since):\n\n  - command \"borg change-passphrase\" (2017-02), use \"borg key ...\"\n  - option \"--keep-tag-files\" (2017-01), use \"--keep-exclude-tags\"\n  - option \"--list-format\" (2017-10), use \"--format\"\n  - option \"--ignore-inode\" (2017-09), use \"--files-cache\" w/o \"inode\"\n  - option \"--no-files-cache\" (2017-09), use \"--files-cache=disabled\"\n- removed BORG_HOSTNAME_IS_UNIQUE env var.\n  to use borg you must implement one of these 2 scenarios:\n\n  - 1) the combination of FQDN and result of uuid.getnode() must be unique\n       and stable (this should be the case for almost everybody, except when\n       having duplicate FQDN *and* MAC address or all-zero MAC address)\n  - 2) if you are aware that 1) is not the case for you, you must set\n       BORG_HOST_ID env var to something unique.\n- exit with 128 + signal number, #5161.\n  if you have scripts expecting rc == 2 for a signal exit, you need to update\n  them to check for >= 128.\n\nFixes:\n\n- diff: reduce memory consumption, fix is_hardlink_master, #6295\n- compact: fix / improve freeable / freed space log output\n\n  - derive really freed space from quota use before/after, #5679\n  - do not say \"freeable\", but \"maybe freeable\" (based on hint, unsure)\n- fix race conditions in internal SaveFile function, #6306 #6028\n- implement internal safe_unlink (was: truncate_and_unlink) function more safely:\n  usually it does not truncate any more, only under \"disk full\" circumstances\n  and only if there is only one hardlink.\n  see: https://github.com/borgbackup/borg/discussions/6286\n\nOther changes:\n\n- info: use a pre12-meta cache to accelerate stats for borg < 1.2 archives.\n  the first time borg info is invoked on a borg 1.1 repo, it can take a\n  rather long time computing and caching some stats values for 1.1 archives,\n  which borg 1.2 archives have in their archive metadata structure.\n  be patient, esp. if you have lots of old archives.\n  following invocations are much faster due to the cache.\n  related change: add archive name to calc_stats progress display.\n- docs:\n\n  - add borg 1.2 upgrade notes, #6217\n  - link to borg placeholders and borg patterns help\n  - init: explain the encryption modes better\n  - clarify usage of patternfile roots\n  - put import-tar docs into same file as export-tar docs\n  - explain the difference between a path that ends with or without a slash,\n    #6297\n\n\nVersion 1.2.0rc1 (2022-02-05)\n-----------------------------\n\nFixes:\n\n- repo::archive location placeholder expansion fixes, #5826, #5998\n- repository: fix intermediate commits, shall be at end of current segment\n- delete: don't commit if nothing was deleted, avoid cache sync, #6060\n- argument parsing: accept some options only once, #6026\n- disallow overwriting of existing keyfiles on init, #6036\n- if ensure_dir() fails, give more informative error message, #5952\n\nNew features:\n\n- delete --force: do not ask when deleting a repo, #5941\n\nOther changes:\n\n- requirements: exclude broken or incompatible-with-pyinstaller setuptools\n- add a requirements.d/development.lock.txt and use it for vagrant\n- tests:\n\n  - added nonce-related tests\n  - refactor: remove assert_true\n  - vagrant: macos box tuning, netbsd box fixes, #5370, #5922\n- docs:\n\n  - update install docs / requirements docs, #6180\n  - borg mount / FUSE \"versions\" view is not experimental any more\n  - --pattern* is not experimental any more, #6134\n  - impact of deleting path/to/repo/nonce, #5858\n  - key export: add examples, #6204\n  - ~/.config/borg/keys is not used for repokey keys, #6107\n  - excluded parent dir's metadata can't restore\n\n\nVersion 1.2.0b4 (2022-01-23)\n----------------------------\n\nFixes:\n\n- create: fix passing device nodes and symlinks to --paths-from-stdin, #6009\n- create --dry-run: fix display of kept tagfile, #5834\n- check --repair: fix missing parameter in \"did not consistently fail\" msg, #5822\n- fix hardlinkable file type check, #6037\n- list: remove placeholders for shake_* hashes, #6082\n- prune: handle case of calling prune_split when there are no archives, #6015\n- benchmark crud: make sure cleanup of borg-test-data files/dir happens, #5630\n- do not show archive name in repository-related error msgs, #6014\n- prettier error msg (no stacktrace) if exclude file is missing, #5734\n- do not require BORG_CONFIG_DIR if BORG_{SECURITY,KEYS}_DIR are set, #5979\n- fix pyinstaller detection for dir-mode, #5897\n- atomically create the CACHE_TAG file, #6028\n- deal with the SaveFile/SyncFile race, docs, see #6056 708a5853\n- avoid expanding path into LHS of formatting operation + tests, #6064 #6063\n- repository: quota / compactable computation fixes\n- info: emit repo info even if repo has 0 archives + test, #6120\n\nNew features:\n\n- check --repair: significantly speed up search for next valid object in segment, #6022\n- check: add progress indicator for archive check, #5809\n- create: add retry_erofs workaround for O_NOATIME issue on volume shadow copies in WSL1, #6024\n- create: allow --files-cache=size (this is potentially dangerous, use on your own risk), #5686\n- import-tar: implement import-tar to complement export-tar, #2233\n- implement BORG_SELFTEST env variable (can be carefully used to speedup borg hosting), #5871\n- key export: print key if path is '-' or not given, #6092\n- list --format: Add command_line to format keys\n\nOther changes:\n\n- pypi metadata: alpha -> beta\n- require python 3.8+, #5975\n- use pyinstaller 4.7\n- allow msgpack 1.0.3\n- upgrade to bundled xxhash to 0.8.1\n- import-tar / export-tar: tar file related changes:\n\n  - check for short tarfile extensions\n  - add .lz4 and .zstd\n  - fix docs about extensions and decompression commands\n- add github codeql analysis, #6148\n- vagrant:\n\n  - box updates / add new boxes / remove outdated and broken boxes\n  - use Python 3.9.10 (incl. binary builds) and 3.10.0\n  - fix pyenv initialisation, #5798\n  - fix vagrant scp on macOS, #5921\n  - use macfuse instead of osxfuse\n- shell completions:\n\n  - update shell completions to 1.1.17, #5923\n  - remove BORG_LIBC completion, since 9914968 borg no longer uses find_library().\n- docs:\n\n  - fixed readme.rst irc webchat link (we use libera chat now, not freenode)\n  - fix exceptions thrown by `setup.py build_man`\n  - check --repair: recommend checking hw before check --repair, #5855\n  - check --verify-data: clarify and document conflict with --repository-only, #5808\n  - serve: improve ssh forced commands docs, #6083\n  - list: improve docs for `borg list` --format, #6061\n  - list: remove --list-format from borg list\n  - FAQ: fix manifest-timestamp path (inside security dir)\n  - fix the broken link to .nix file\n  - document behavior for filesystems with inconsistent inodes, #5770\n  - clarify user_id vs uid for fuse, #5723\n  - clarify pattern usage with commands, #5176\n  - clarify pp vs. pf pattern type, #5300\n  - update referenced freebsd/macOS versions used for binary build, #5942\n  - pull mode: add some warnings, #5827\n  - clarify \"you will need key and passphrase\" borg init warning, #4622\n  - add missing leading slashes in help patterns, #5857\n  - add info on renaming repositories, #5240\n  - check: add notice about defective hardware, #5753\n  - mention tar --compare (compare archive to fs files), #5880\n  - add note about grandfather-father-son backup retention policy / rotation scheme, #6006\n  - permissions note rewritten to make it less confusing\n  - create github security policy\n  - remove leftovers of BORG_HOSTNAME_IS_UNIQUE\n  - excluded parent dir's metadata can't restore. (#6062)\n  - if parent dir is not extracted, we do not have its metadata\n  - clarify who starts the remote agent\n\n\nVersion 1.2.0b3 (2021-05-12)\n----------------------------\n\nFixes:\n\n- create: fix --progress --log-json, #4360#issuecomment-774580052\n- do not load files cache for commands not using it, #5673\n- fix repeated cache tag file writing bug\n\nNew features:\n\n- create/recreate: print preliminary file status early, #5417\n- create/extract: add --noxattrs and --noacls options, #3955\n- create: verbose files cache logging via --debug-topic=files_cache, #5659\n- mount: implement --numeric-ids (default: False!), #2377\n- diff: add --json-lines option\n- info / create --stats: add --iec option to print sizes in powers of 1024.\n\nOther changes:\n\n- create: add --upload-(ratelimit|buffer), deprecate --remote-* options, #5611\n- create/extract/mount: add --numeric-ids, deprecate --numeric-owner option, #5724\n- config: accept non-int value for max_segment_size / storage_quota\n- use PyInstaller v4.3, #5671\n- vagrant: use Python 3.9.5 to build binaries\n- tox.ini: modernize and enable execution without preinstalling deps\n- cleanup code style checks\n- get rid of distutils, use setuptools+packaging\n- github CI: test on Python 3.10-dev\n- check: missing / healed chunks: always tell chunk ID, #5704\n- docs:\n\n  - remove bad /var/cache exclusion in example commands, #5625\n  - misc. fixes and improvements, esp. for macOS\n  - add unsafe workaround to use an old repo copy, #5722\n\n\nVersion 1.2.0b2 (2021-02-06)\n----------------------------\n\nFixes:\n\n- create: do not recurse into duplicate roots, #5603\n- create: only print stats if not ctrl-c'ed, fixes traceback, #5668\n- extract:\n  improve exception handling when setting xattrs, #5092.\n  emit a warning message giving the path, xattr key and error message.\n  continue trying to restore other xattrs and bsdflags of the same file\n  after an exception with xattr-setting happened.\n- export-tar:\n  fix memory leak with ssh: remote repository, #5568.\n  fix potential memory leak with ssh: remote repository with partial extraction.\n- remove empty shadowed_segments lists, #5275\n- fix bad default: manifest.archives.list(consider_checkpoints=False),\n  fixes tracebacks / KeyErros for missing objects in ChunkIndex, #5668\n\nNew features:\n\n- create: improve sparse file support\n\n  - create --sparse (detect sparse file holes) and file map support,\n    only for the \"fixed\" chunker, #14\n  - detect all-zero chunks in read data in \"buzhash\" and \"fixed\" chunkers\n  - cached_hash: use a small LRU cache to accelerate all-zero chunks hashing\n  - use cached_hash also to generate all-zero replacement chunks\n- create --remote-buffer, add a upload buffer for remote repos, #5574\n- prune: keep oldest archive when retention target not met\n\nOther changes:\n\n- use blake2 from python 3.6+ hashlib\n  (this removes the requirement for libb2 and the bundled blake2 code)\n- also accept msgpack up to 1.0.2.\n  exclude 1.0.1 though, which had some issues (not sure they affect borg).\n- create: add repository location to --stats output, #5491\n- check: debug log the segment filename\n- delete: add a --list switch to borg delete, #5116\n- borg debug dump-hints - implemented to e.g. to look at shadow_index\n- Tab completion support for additional archives for 'borg delete'\n- refactor: have one borg.constants.zero all-zero bytes object\n- refactor shadow_index updating repo.put/delete, #5661, #5636.\n- docs:\n\n  - add another case of attempted hardlink usage\n  - fix description of borg upgrade hardlink usage, #5518\n  - use HTTPS everywhere\n  - add examples for --paths-from-stdin, --paths-from-command, --paths-separator, #5644\n  - fix typos/grammar\n  - update docs for dev environment installation instructions\n  - recommend running tests only on installed versions for setup\n  - add badge with current status of package\n- vagrant:\n\n  - use brew install --cask ..., #5557\n  - use Python 3.9.1 and PyInstaller 4.1 to build the borg binary\n\n\nVersion 1.2.0b1 (2020-12-06)\n----------------------------\n\nFixes:\n\n- BORG_CACHE_DIR crashing borg if empty, atomic handling of\n  recursive directory creation, #5216\n- fix --dry-run and --stats coexistence, #5415\n- allow EIO with warning when trying to hardlink, #4336\n- export-tar: set tar format to GNU_FORMAT explicitly, #5274\n- use --timestamp for {utcnow} and {now} if given, #5189\n- make timestamp helper timezone-aware\n\nNew features:\n\n- create: implement --paths-from-stdin and --paths-from-command, see #5492.\n  These switches read paths to archive from stdin. Delimiter can specified\n  by --paths-delimiter=DELIM. Paths read will be added honoring every\n  option but exclusion options and --one-file-system. borg won't recurse\n  into directories.\n- 'obfuscate' pseudo compressor obfuscates compressed chunk size in repo\n- add pyfuse3 (successor of llfuse) as an alternative lowlevel fuse\n  implementation to llfuse (deprecated), #5407.\n  FUSE implementation can be switched via env var BORG_FUSE_IMPL.\n- allow appending to the files cache filename with BORG_FILES_CACHE_SUFFIX\n- create: implement --stdin-mode, --stdin-user and --stdin-group, #5333\n\nOther changes:\n\n- split recursive directory walking/processing into directory walking and\n  item processing.\n- fix warning by importing setuptools before distutils.\n- debug info: include infos about FUSE implementation, #5546\n- testing:\n\n  - add a test for the hashindex corruption bug, #5531 #4829\n  - move away from travis-ci, use github actions, #5528 #5467\n  - test both on fuse2 and fuse3\n  - upload coverage reports to codecov\n  - fix spurious failure in test_cache_files, #5438\n  - add tests for Location.with_timestamp\n  - tox: add a non-fuse env to the envlist\n- vagrant:\n\n  - use python 3.7.latest and pyinstaller 4.0 for binary creation\n  - pyinstaller: compute basepath from spec file location\n  - vagrant: updates/fixes for archlinux box, #5543\n- docs:\n\n  - \"filename with spaces\" example added to exclude file, #5236\n  - add a hint about sleeping computer, #5301\n  - how to adjust macOS >= Catalina security settings, #5303\n  - process/policy for adding new compression algorithms\n  - updated docs about hacked backup client, #5480\n  - improve ansible deployment docs, make it more generic\n  - how to approach borg speed issues, give speed example, #5371\n  - fix mathematical inaccuracy about chunk size, #5336\n  - add example for excluding content using --pattern cli option\n  - clarify borg create's '--one-file-system' option, #4009\n  - improve docs/FAQ about append-only remote repos, #5497\n  - fix reST markup issues, labels\n  - add infos about contributor retirement status\n\n\nVersion 1.2.0a9 (2020-10-05)\n----------------------------\n\nFixes:\n\n- fix memory leak related to preloading, #5202\n- check --repair: fix potential data loss, #5325\n- persist shadow_index in between borg runs, #4830\n- fix hardlinked CACHEDIR.TAG processing, #4911\n- --read-special: .part files also should be regular files, #5217\n- allow server side enforcing of umask, --umask is for the local borg\n  process only (see docs), #4947\n- exit with 128 + signal number, #5161\n- borg config --list does not show last_segment_checked, #5159\n- locking:\n\n  - fix ExclusiveLock race condition bug, #4923\n  - fix race condition in lock migration, #4953\n  - fix locking on openindiana, #5271\n\nNew features:\n\n- --content-from-command: create archive using stdout of given command, #5174\n- allow key-import + BORG_KEY_FILE to create key files\n- build directory-based binary for macOS to avoid Gatekeeper delays\n\nOther changes:\n\n- upgrade bundled zstd to 1.4.5\n- upgrade bundled xxhash to 0.8.0, #5362\n- if self test fails, also point to OS and hardware, #5334\n- misc. shell completions fixes/updates, rewrite zsh completion\n- prettier error message when archive gets too big, #5307\n- stop relying on `false` exiting with status code 1\n- rephrase some warnings, #5164\n- parseformat: unnecessary calls removed, #5169\n- testing:\n\n  - enable Python3.9 env for test suite and VMs, #5373\n  - drop python 3.5, #5344\n  - misc. vagrant fixes/updates\n  - misc. testing fixes, #5196\n- docs:\n\n  - add ssh-agent pull backup method to doc, #5288\n  - mention double --force in prune docs\n  - update Homebrew install instructions, #5185\n  - better description of how cache and rebuilds of it work\n    and how the workaround applies to that\n  - point to borg create --list item flags in recreate usage, #5165\n  - add a note to create from stdin regarding files cache, #5180\n  - add security faq explaining AES-CTR crypto issues, #5254\n  - clarify --exclude-if-present in recreate, #5193\n  - add socat pull mode, #5150, #900\n  - move content of resources doc page to community project, #2088\n  - explain hash collision, #4884\n  - clarify --recompress option, #5154\n\n\nVersion 1.2.0a8 (2020-04-22)\n----------------------------\n\nFixes:\n\n- fixed potential index corruption / data loss issue due to bug in hashindex_set, #4829.\n  Please read and follow the more detailed notes close to the top of this document.\n- fix crash when upgrading erroneous hints file, #4922\n- commit-time free space calc: ignore bad compact map entries, #4796\n- info: if the archive doesn't exist, print a pretty message, #4793\n- --prefix / -P: fix processing, avoid argparse issue, #4769\n- ignore EACCES (errno 13) when hardlinking, #4730\n- add a try catch when formatting the info string, #4818\n- check: do not stumble over invalid item key, #4845\n- update prevalence of env vars to set config and cache paths\n- mount: fix FUSE low linear read speed on large files, #5032\n- extract: fix confusing output of borg extract --list --strip-components, #4934\n- recreate: support --timestamp option, #4745\n- fix ProgressIndicator msgids (JSON output), #4935\n- fuse: set f_namemax in statfs result, #2684\n- accept absolute paths on windows\n- pyinstaller: work around issue with setuptools > 44\n\nNew features:\n\n- chunker speedup (plus regression test)\n- added --consider-checkpoints and related test, #4788\n- added --noflags option, deprecate --nobsdflags option, #4489\n- compact: add --threshold option, #4674\n- mount: add birthtime to FUSE entries\n- support platforms with no os.link, #4901 - if we don't have os.link,\n  we just extract another copy instead of making a hardlink.\n- move sync_file_range to its own extension for better platform compatibility.\n- new --bypass-lock option to bypass locking, e.g. for read-only repos\n- accept absolute paths by removing leading slashes in patterns of all\n  sorts but re: style, #4029\n- delete: new --keep-security-info option\n\nOther changes:\n\n- support msgpack 0.6.2 and 1.0.0, #5065\n- upgrade bundled zstd to 1.4.4\n- upgrade bundled lz4 to 1.9.2\n- upgrade xxhash to 0.7.3\n- require recent enough llfuse for birthtime support, #5064\n- only store compressed data if the result actually is smaller, #4516\n- check: improve error output for matching index size, see #4829\n- ignore --stats when given with --dry-run, but continue, #4373\n- replaced usage of os.statvfs with shutil.disk_usage (better cross-platform support).\n- fuse: remove unneeded version check and compat code, micro opts\n- docs:\n\n  - improve description of path variables\n  - document how to delete data completely, #2929\n  - add FAQ about Borg config dir, #4941\n  - add docs about errors not printed as JSON, #4073\n  - update usage_general.rst.inc\n  - added \"Will move with BORG_CONFIG_DIR variable unless specified.\" to BORG_SECURITY_DIR info.\n  - put BORG_SECURITY_DIR immediately below BORG_CONFIG_DIR (and moved BORG_CACHE_DIR up before them).\n  - add paragraph regarding cache security assumptions, #4900\n  - tell about borg cache security precautions\n  - add FAQ describing difference between a local repo vs. repo on a server.\n  - document how to test exclusion patterns without performing an actual backup\n  - create: tell that \"Calculating size\" time and space needs are caused by --progress\n  - fix/improve documentation for @api decorator, #4674\n  - add a pull backup / push restore how-to, #1552\n  - fix man pages creation, #4752\n  - more general FAQ for backup and retain original paths, #4532\n  - explain difference between --exclude and --pattern, #4118\n  - add FAQ for preventing SSH timeout in extract, #3866\n  - improve password FAQ (decrease pw length, add -w 0 option to base64 to prevent line wrap), #4591\n  - add note about patterns and stored paths, #4160\n  - add upgrade of tools to pip installation how-to, #5090\n  - document one cause of orphaned chunks in check command, #2295\n  - clean up the whole check usage paragraph\n  - FAQ: linked recommended restrictions to ssh public keys on borg servers, #4946\n  - fixed \"doc downplays severity of Nonce reuse issue\", #4883\n  - borg repo restore instructions needed, #3428\n  - new FAQ: A repo is corrupt and must be replaced with an older repo.\n  - clarify borg init's encryption modes\n- native windows port:\n\n  - update README_WINDOWS.rst\n  - updated pyinstaller spec file to support windows builds\n- testing / CI:\n\n  - improved travis config / install script, improved macOS builds\n  - allow osx builds to fail, #4955\n  - Windows 10 build on Appveyor CI\n- vagrant:\n\n  - upgrade pyinstaller to v3.5 + patch\n  - use py369 for binary build, add py380 for tests\n  - fix issue in stretch VM hanging at grub installation\n  - add a debian buster and a ubuntu focal VM\n  - update darwin box to 10.12\n  - upgrade FreeBSD box to 12.1\n  - fix debianoid virtualenv packages\n  - use pyenv in freebsd64 VM\n  - remove the flake8 test\n  - darwin: avoid error if pkg is already installed\n  - debianoid: don't interactively ask questions\n\n\nVersion 1.2.0a7 (2019-09-07)\n----------------------------\n\nFixes:\n\n- slave hardlinks extraction issue, see #4350\n- extract: fix KeyError for \"partial\" extraction, #4607\n- preload chunks for hardlink slaves w/o preloaded master, #4350\n- fix preloading for old remote servers, #4652\n- fix partial extract for hardlinked contentless file types, #4725\n- Repository.open: use stat() to check for repo dir, #4695\n- Repository.check_can_create_repository: use stat() to check, ~ #4695.\n- SecurityManager.known(): check all files, #4614\n- after double-force delete, warn about necessary repair, #4704\n- cope with ANY error when importing pytest into borg.testsuite, #4652\n- fix invalid archive error message\n- setup.py: fix detection of missing Cython\n- filter out selinux xattrs, #4574\n- location arg - should it be optional? #4541\n- enable placeholder usage in --comment, #4559\n- use whitelist approach for borg serve, #4097\n\nNew features:\n\n- minimal native Windows support, see windows readme (work in progress)\n- create: first ctrl-c (SIGINT) triggers checkpoint and abort, #4606\n- new BORG_WORKAROUNDS mechanism, basesyncfile, #4710\n- remove WSL autodetection. if WSL still has this problem, you need to\n  set BORG_WORKAROUNDS=basesyncfile in the borg process environment to\n  work around it.\n- support xxh64 checksum in addition to the hashlib hashes in borg list\n- enable placeholder usage in all extra archive arguments\n- enable placeholder usage in --comment, #4559\n- enable placeholder usage in --glob-archives, #4495\n- ability to use a system-provided version of \"xxhash\"\n- create:\n\n  - changed the default behaviour not to store the atime of fs items. atime is\n    often rather not interesting and fragile - it easily changes even if nothing\n    else has changed and, if stored into the archive, spoils deduplication of\n    the archive metadata stream.\n  - if you give the --noatime option, borg will output a deprecation warning\n    because it is currently ignored / does nothing.\n    Please remove the --noatime option when using borg 1.2.\n  - added a --atime option for storing files' atime into an archive\n\nOther changes:\n\n- argparser: always use REPOSITORY in metavar\n- do not check python/libc for borg serve, #4483\n- small borg compact improvements, #4522\n- compact: log freed space at INFO level\n- tests:\n\n  - tox / travis: add testing on py38-dev\n  - fix broken test that relied on improper zlib assumptions\n  - pure-py msgpack warning shall not make a lot of tests fail, #4558\n  - rename test_mount_hardlinks to test_fuse_mount_hardlinks (master)\n  - vagrant: add up-to-date openindiana box (py35, openssl10)\n  - get rid of confusing coverage warning, #2069\n- docs:\n\n  - reiterate that 'file cache names are absolute' in FAQ,\n    mention bind mount solution, #4738\n  - add restore docs, #4670\n  - updated docs to cover use of temp directory on remote, #4545\n  - add a push-style example to borg-create(1), #4613\n  - timestamps in the files cache are now usually ctime, #4583\n  - benchmark crud: clarify that space is used until compact\n  - update documentation of borg create,\n    corrects a mention of borg 1.1 as a future version.\n  - fix osxfuse github link in installation docs\n  - how to supply a passphrase, use crypto devices, #4549\n  - extract: document limitation \"needs empty destination\",  #4598\n  - update macOS Brew link\n  - add note about software for automating backup\n  - compact: improve docs,\n  - README: new URL for funding options\n\n\nVersion 1.2.0a6 (2019-04-22)\n----------------------------\n\nFixes:\n\n- delete / prune: consider part files correctly for stats, #4507\n- fix \"all archives\" stats considering part files, #4329\n- create: only run stat_simple_attrs() once\n- create: --stats does not work with --dry-run, exit with error msg, #4373\n- give \"invalid repo\" error msg if repo config not found, #4411\n\nNew features:\n\n- display msgpack version as part of sysinfo (e.g. in tracebacks)\n\nOther changes:\n\n- docs:\n\n  - sdd \"SSH Configuration\" section, #4493, #3988, #636, #4485\n  - better document borg check --max-duration, #4473\n  - sorted commands help in multiple steps, #4471\n- testing:\n\n  - travis: use py 3.5.3 and 3.6.7 on macOS to get a pyenv-based python\n    build with openssl 1.1\n  - vagrant: use py 3.5.3 and 3.6.8 on darwin64 VM to build python and\n    borg with openssl 1.1\n  - pytest: -v and default XDISTN to 1, #4481\n\n\nVersion 1.2.0a5 (2019-03-21)\n----------------------------\n\nFixes:\n\n- warn if a file has changed while being backed up, #1750\n- lrucache: regularly remove old FDs, #4427\n- borg command shall terminate with rc 2 for ImportErrors, #4424\n- make freebsd xattr platform code api compatible with linux, #3952\n\nOther changes:\n\n- major setup code refactoring (especially how libraries like openssl, liblz4,\n  libzstd, libb2 are discovered and how it falls back to code bundled with\n  borg), new: uses pkg-config now (and needs python \"pkgconfig\" package\n  installed), #1925\n\n  if you are a borg package maintainer, please try packaging this\n  (see comments in setup.py).\n- Vagrantfile: add zstd, reorder, build env vars, #4444\n- travis: install script improvements\n- update shell completions\n- docs:\n\n  - add a sample logging.conf in docs/misc, #4380\n  - fix spelling errors\n  - update requirements / install docs, #4374\n\n\nVersion 1.2.0a4 (2019-03-11)\n----------------------------\n\nFixes:\n\n- do not use O_NONBLOCK for special files, like FIFOs, block and char devices\n  when using --read-special. fixes backing up FIFOs. fixes to test. #4394\n- more LibreSSL build fixes: LibreSSL has HMAC_CTX_free and HMAC_CTX_new\n\nNew features:\n\n- check: incremental repo check (only checks crc32 for segment entries), #1657\n  borg check --repository-only --max-duration SECONDS ...\n- delete: timestamp for borg delete --info added, #4359\n\nOther changes:\n\n- redo stale lock handling, #3986\n  drop BORG_HOSTNAME_IS_UNIQUE (please use BORG_HOST_ID if needed).\n  borg now always assumes it has a unique host id - either automatically\n  from fqdn plus uuid.getnode() or overridden via BORG_HOST_ID.\n- docs:\n\n  - added Alpine Linux to distribution list\n  - elaborate on append-only mode docs\n- vagrant:\n\n  - darwin: new 10.12 box\n  - freebsd: new 12.0 box\n  - openbsd: new 6.4 box\n  - misc. updates / fixes\n\n\nVersion 1.2.0a3 (2019-02-26)\n----------------------------\n\nFixes:\n\n- LibreSSL build fixes, #4403\n- dummy ACL/xattr code fixes (used by OpenBSD and others), #4403\n- create: fix openat/statat issues for root directory, #4405\n\n\nVersion 1.2.0a2 and earlier (2019-02-24)\n----------------------------------------\n\nNew features:\n\n- compact: \"borg compact\" needs to be used to free repository space by\n  compacting the segments (reading sparse segments, rewriting still needed\n  data to new segments, deleting the sparse segments).\n  Borg < 1.2 invoked compaction automatically at the end of each repository\n  writing command.\n  Borg >= 1.2 does not do that any more to give better speed, more control,\n  more segment file stability (== less stuff moving to newer segments) and\n  more robustness.\n  See the docs about \"borg compact\" for more details.\n- \"borg compact --cleanup-commits\" is to cleanup the tons of 17byte long\n  commit-only segment files caused by borg 1.1.x issue #2850.\n  Invoke this once after upgrading (the server side) borg to 1.2.\n  Compaction now automatically removes unneeded commit-only segment files.\n- prune: Show which rule was applied to keep archive, #2886\n- add fixed blocksize chunker (see --chunker-params docs), #1086\n\nFixes:\n\n- avoid stale filehandle issues, #3265\n- use more FDs, avoid race conditions on active fs, #906, #908, #1038\n- add O_NOFOLLOW to base flags, #908\n- compact:\n\n  - require >10% freeable space in a segment, #2985\n  - repository compaction now automatically removes unneeded 17byte\n    commit-only segments, #2850\n- make swidth available on all posix platforms, #2667\n\nOther changes:\n\n- repository: better speed and less stuff moving around by using separate\n  segment files for manifest DELETEs and PUTs, #3947\n- use pyinstaller v3.3.1 to build binaries\n- update bundled zstd code to 1.3.8, #4210\n- update bundled lz4 code to 1.8.3, #4209\n- msgpack:\n\n  - switch to recent \"msgpack\" pypi pkg name, #3890\n  - wrap msgpack to avoid future compat complications, #3632, #2738\n  - support msgpack 0.6.0 and 0.6.1, #4220, #4308\n\n- llfuse: modernize / simplify llfuse version requirements\n- code refactorings / internal improvements:\n\n  - include size/csize/nfiles[_parts] stats into archive, #3241\n  - calc_stats: use archive stats metadata, if available\n  - crypto: refactored crypto to use an AEAD style API\n  - crypto: new AES-OCB, CHACHA20-POLY1305\n  - create: use less syscalls by not using a python file obj, #906, #3962\n  - diff: refactor the diff functionality to new ItemDiff class, #2475\n  - archive: create FilesystemObjectProcessors class\n  - helpers: make a package, split into smaller modules\n  - xattrs: move to platform package, use cython instead ctypes, #2495\n  - xattrs/acls/bsdflags: misc. code/api optimizations\n  - FUSE: separate creation of filesystem from implementation of llfuse funcs, #3042\n  - FUSE: use unpacker.tell() instead of deprecated write_bytes, #3899\n  - setup.py: move build_man / build_usage code to setup_docs.py\n  - setup.py: update to use a newer Cython/setuptools API for compiling .pyx -> .c, #3788\n  - use python 3.5's os.scandir / os.set_blocking\n  - multithreading preparations (not used yet):\n\n    - item.to_optr(), Item.from_optr()\n    - fix chunker holding the GIL during blocking I/O\n  - C code portability / basic MSC compatibility, #4147, #2677\n- testing:\n\n  - vagrant: new VMs for linux/bsd/darwin, most with OpenSSL 1.1 and py36\n\n\nVersion 1.1.18 (2022-06-05)\n---------------------------\n\nCompatibility notes:\n\n- When upgrading from borg 1.0.x to 1.1.x, please note:\n\n  - read all the compatibility notes for 1.1.0*, starting from 1.1.0b1.\n  - borg upgrade: you do not need to and you also should not run it.\n  - borg might ask some security-related questions once after upgrading.\n    You can answer them either manually or via environment variable.\n    One known case is if you use unencrypted repositories, then it will ask\n    about a unknown unencrypted repository one time.\n  - your first backup with 1.1.x might be significantly slower (it might\n    completely read, chunk, hash a lot files) - this is due to the\n    --files-cache mode change (and happens every time you change mode).\n    You can avoid the one-time slowdown by using the pre-1.1.0rc4-compatible\n    mode (but that is less safe for detecting changed files than the default).\n    See the --files-cache docs for details.\n- 1.1.11 removes WSL autodetection (Windows 10 Subsystem for Linux).\n  If WSL still has a problem with sync_file_range, you need to set\n  BORG_WORKAROUNDS=basesyncfile in the borg process environment to\n  work around the WSL issue.\n- 1.1.14 changes return codes due to a bug fix:\n  In case you have scripts expecting rc == 2 for a signal exit, you need to\n  update them to check for >= 128 (as documented since long).\n- 1.1.15 drops python 3.4 support, minimum requirement is 3.5 now.\n- 1.1.17 install_requires the \"packaging\" pypi package now.\n\nNew features:\n\n- check --repair: significantly speed up search for next valid object in segment, #6022\n- create: add retry_erofs workaround for O_NOATIME issue on volume shadow copies in WSL1, #6024\n- key export: display key if path is '-' or not given, #6092\n- list --format: add command_line to format keys, #6108\n\nFixes:\n\n- check: improve error handling for corrupt archive metadata block,\n  make robust_iterator more robust, #4777\n- diff: support presence change for blkdev, chrdev and fifo items, #6483\n- diff: reduce memory consumption, fix is_hardlink_master\n- init: disallow overwriting of existing keyfiles\n- info: fix authenticated mode repo to show \"Encrypted: No\", #6462\n- info: emit repo info even if repo has 0 archives, #6120\n- list: remove placeholders for shake_* hashes, #6082\n- mount -o versions: give clear error msg instead of crashing\n- show_progress: add finished=true/false to archive_progress json, #6570\n- fix hardlinkable file type check, #6037\n- do not show archive name in error msgs referring to the repository, #6023\n- prettier error msg (no stacktrace) if exclude file is missing, #5734\n- do not require BORG_CONFIG_DIR if BORG_{SECURITY,KEYS}_DIR are set, #5979\n- atomically create the CACHE_TAG file, #6028\n- deal with the SaveFile/SyncFile race, docs, see #6176 5c5b59bc9\n- avoid expanding path into LHS of formatting operation + tests, #6064 #6063\n- repository: quota / compactable computation fixes, #6119.\n  This is mainly to keep the repo code in sync with borg 1.2. As borg 1.1\n  compacts immediately, there was not really an issue with this in 1.1.\n- fix transaction rollback: use files cache filename as found in txn.active, #6353\n- do not load files cache for commands not using it, fixes #5673\n- fix scp repo url parsing for ip v6 addrs, #6526\n- repo::archive location placeholder expansion fixes, #5826, #5998\n\n  - use expanded location for log output\n  - support placeholder expansion for BORG_REPO env var\n- respect umask for created directory and file modes, #6400\n- safer truncate_and_unlink implementation\n\nOther changes:\n\n- upgrade bundled xxhash code to 0.8.1\n- fix xxh64 related build (setup.py and post-0.8.1 patch for static_assert).\n  The patch was required to build the bundled xxhash code on FreeBSD, see\n  https://github.com/Cyan4973/xxHash/pull/670\n- msgpack build: remove endianness macro, #6105\n- update and fix shell completions\n- fuse: remove unneeded version check and compat code\n- delete --force: do not ask when deleting a repo, #5941\n- delete: don't commit if nothing was deleted, avoid cache sync, #6060\n- delete: add repository id and location to prompt\n- compact segments: improve freeable / freed space log output, #5679\n- if ensure_dir() fails, give more informative error message, #5952\n- load_key: no key is same as empty key, #6441\n- better error msg for defect or unsupported repo configs, #6566\n- use hmac.compare_digest instead of ==, #6470\n- implement more standard hashindex.setdefault behaviour\n- remove stray punctuation from secure-erase message\n- add development.lock.txt, use a real python 3.5 to generate frozen reqs\n- setuptools 60.7.0 breaks pyinstaller, #6246\n- setup.py clean2 was added to work around some setuptools customizability limitation.\n- allow extra compiler flags for every extension build\n- C code: make switch fallthrough explicit\n- Cython code: fix \"useless trailing comma\" cython warnings\n- requirements.lock.txt: use the latest cython 0.29.30\n- fix compilation warnings: \u2018PyUnicode_AsUnicode\u2019 is deprecated\n- docs:\n\n  - ~/.config/borg/keys is not used for repokey keys, #6107\n  - excluded parent dir's metadata can't restore, #6062\n  - permissions note rewritten to make it less confusing, #5490\n  - add note about grandfather-father-son backup retention policy / rotation scheme\n  - clarify who starts the remote agent (borg serve)\n  - test/improve pull backup docs, #5903\n  - document the socat pull mode described in #900 #515\u00df\n  - borg serve: improve ssh forced commands docs, #6083\n  - improve docs for borg list --format, #6080\n  - fix the broken link to .nix file\n  - clarify pattern usage with commands, #5176\n  - clarify user_id vs uid for fuse, #5723\n  - fix binary build freebsd/macOS version, #5942\n  - FAQ: fix manifest-timestamp path, #6016\n  - remove duplicate faq entries, #5926\n  - fix sphinx warnings, #5919\n  - virtualisation speed tips\n  - fix values of TAG bytes, #6515\n  - recommend umask for passphrase file perms\n  - update link to ubuntu packages, #6485\n  - clarify on-disk order and size of log entry fields, #6357\n  - do not transform --/--- to unicode dashes\n  - improve linking inside docs, link to borg_placeholders, link to borg_patterns\n  - use same phrasing in misc. help texts\n  - borg init: explain the encryption modes better\n  - explain the difference between a path that ends with or without a slash, #6297\n  - clarify usage of patternfile roots, #6242\n  - borg key export: add examples\n  - updates about features not experimental any more: FUSE \"versions\" view, --pattern*, #6134\n  - fix/update cygwin package requirements\n  - impact of deleting path/to/repo/nonce, #5858\n  - warn about tampered server nonce\n  - mention BORG_FILES_CACHE_SUFFIX as alternative to BORG_FILES_CACHE_TTL, #5602\n  - add a troubleshooting note about \"is not a valid repository\" to the FAQ\n- vagrant / CI / testing:\n\n  - misc. fixes and updates, new python versions\n  - macOS on github: re-enable fuse2 testing by downgrading to older macOS, #6099\n  - fix OpenBSD symlink mode test failure, #2055\n  - use the generic/openbsd6 box\n  - strengthen the test: we can read data w/o nonces\n  - add tests for path/to/repo/nonce deletion\n  - darwin64: backport some tunings from master\n  - darwin64: remove fakeroot, #6314\n  - darwin64: fix vagrant scp, #5921\n  - darwin64: use macfuse instead of osxfuse\n  - add ubuntu \"jammy\" 22.04 LTS VM\n  - adapt memory for openindiana64 and darwin64\n\n\nVersion 1.1.17 (2021-07-12)\n---------------------------\n\nCompatibility notes:\n\n- When upgrading from borg 1.0.x to 1.1.x, please note:\n\n  - read all the compatibility notes for 1.1.0*, starting from 1.1.0b1.\n  - borg upgrade: you do not need to and you also should not run it.\n  - borg might ask some security-related questions once after upgrading.\n    You can answer them either manually or via environment variable.\n    One known case is if you use unencrypted repositories, then it will ask\n    about a unknown unencrypted repository one time.\n  - your first backup with 1.1.x might be significantly slower (it might\n    completely read, chunk, hash a lot files) - this is due to the\n    --files-cache mode change (and happens every time you change mode).\n    You can avoid the one-time slowdown by using the pre-1.1.0rc4-compatible\n    mode (but that is less safe for detecting changed files than the default).\n    See the --files-cache docs for details.\n- 1.1.11 removes WSL autodetection (Windows 10 Subsystem for Linux).\n  If WSL still has a problem with sync_file_range, you need to set\n  BORG_WORKAROUNDS=basesyncfile in the borg process environment to\n  work around the WSL issue.\n- 1.1.14 changes return codes due to a bug fix:\n  In case you have scripts expecting rc == 2 for a signal exit, you need to\n  update them to check for >= 128 (as documented since long).\n- 1.1.15 drops python 3.4 support, minimum requirement is 3.5 now.\n- 1.1.17 install_requires the \"packaging\" pypi package now.\n\nFixes:\n\n- pyinstaller dir-mode: fix pyi detection / LIBPATH treatment, #5897\n- handle crash due to kill stale lock race, #5828\n- fix BORG_CACHE_DIR crashing borg if empty, #5216\n- create --dry-run: fix display of kept tagfile, #5834\n- fix missing parameter in \"did not consistently fail\" msg, #5822\n- missing / healed chunks: always tell chunk ID, #5704\n- benchmark: make sure cleanup happens even on exceptions, #5630\n\nNew features:\n\n- implement BORG_SELFTEST env variable, #5871.\n  this can be used to accelerate borg startup a bit. not recommended for\n  normal usage, but borg mass hosters with a lot of borg invocations can\n  save some resources with this. on my laptop, this saved ~100ms cpu time\n  (sys+user) per borg command invocation.\n- implement BORG_LIBC env variable to give the libc filename, #5870.\n  you can use this if a borg does not find your libc.\n- check: add progress indicator for archive check.\n- allow --files-cache=size (not recommended, make sure you know what you do)\n\nOther changes:\n\n- Python 3.10 now officially supported!\n  we test on py310-dev on github CI since a while and now also on the vagrant\n  machines, so it should work ok.\n- github CI: test on py310 (again)\n- get rid of distutils, use packaging and setuptools.\n  distutils is deprecated and gives warnings on py 3.10.\n- setup.py: rename \"clean\" to \"clean2\" to avoid shadowing the \"clean\" command.\n- remove libc filename fallback for the BSDs (there is no \"usual\" name)\n- cleanup flake8 checks, fix some pep8 violations.\n- docs building: replace deprecated function \".add_stylesheet()\" for Sphinx 4 compatibility\n- docs:\n\n  - add a hint on sleeping computer and ssh connections, #5301\n  - update the documentation on hacked backup client, #5480\n  - improve docs/FAQ about append-only remote repos, #5497\n  - complement the documentation for pattern files and exclude files, #5520\n  - \"filename with spaces\" example added to exclude file, #5236\n    note: no whitespace escaping needed, processed by borg.\n  - add info on renaming repositories, #5240\n  - clarify borg check --verify-data, #5808\n  - add notice about defective hardware to check documentation, #5753\n  - add paragraph added in #5855 to utility documentation source\n  - add missing leading slashes in help patterns, #5857\n  - clarify \"you will need key and passphrase\" borg init warning, #4622\n  - pull mode: add some warnings, #5827\n  - mention tar --compare (compare archive to fs files), #5880\n  - fix typos, backport of #5597\n- vagrant:\n\n  - add py3.7.11 for binary build, also add 3.10-dev.\n  - use latest Cython 0.29.23 for py310 compat fixes.\n  - more RAM for openindiana upgrade plan resolver, it just hangs (swaps?) if\n    there is too little RAM.\n  - fix install_pyenv to adapt to recent changes in pyenv (same as in master now).\n  - use generic/netbsd9 box, copied from master branch.\n\n\nVersion 1.1.16 (2021-03-23)\n---------------------------\n\nFixes:\n\n- setup.py: add special openssl prefix for Apple M1 compatibility\n- do not recurse into duplicate roots, #5603\n- remove empty shadowed_segments lists, #5275, #5614\n- fix libpython load error when borg fat binary / dir-based binary is invoked\n  via a symlink by upgrading pyinstaller to v4.2, #5688\n- config: accept non-int value (like 500M or 100G) for max_segment_size or\n  storage_quota, #5639.\n  please note: when setting a non-int value for this in a repo config,\n  using the repo will require borg >= 1.1.16.\n\nNew features:\n\n- bundled msgpack: drop support for old buffer protocol to support Python 3.10\n- verbose files cache logging via --debug-topic=files_cache, #5659.\n  Use this if you suspect that borg does not detect unmodified files as expected.\n- create/extract: add --noxattrs and --noacls option, #3955.\n  when given with borg create, borg will not get xattrs / ACLs from input files\n  (and thus, it will not archive xattrs / ACLs). when given with borg extract,\n  borg will not read xattrs / ACLs from archive and will not set xattrs / ACLs\n  on extracted files.\n- diff: add --json-lines option, #3765\n- check: debug log segment filename\n- borg debug dump-hints\n\nOther changes:\n\n- Tab completion support for additional archives for 'borg delete'\n- repository: deduplicate code of put and delete, no functional change\n- tests: fix result order issue (sporadic test failure on openindiana)\n- vagrant:\n\n  - upgrade pyinstaller to v4.2, #5671\n  - avoid grub-install asking interactively for device\n  - remove the xenial box\n  - update freebsd box to 12.1\n- docs:\n\n  - update macOS install instructions, #5677\n  - use macFUSE (not osxfuse) for Apple M1 compatibility\n  - update docs for dev environment installation instructions, #5643\n  - fix grammar in faq\n  - recommend running tests only on installed versions for setup\n  - add link back to git-installation\n  - remove /var/cache exclusion in example commands, #5625.\n    This is generally a poor idea and shouldn't be promoted through examples.\n  - add repology.org badge with current packaging status\n  - explain hash collision\n  - add unsafe workaround to use an old repo copy, #5722\n\n\nVersion 1.1.15 (2020-12-25)\n---------------------------\n\nFixes:\n\n- extract:\n\n  - improve exception handling when setting xattrs, #5092.\n  - emit a warning message giving the path, xattr key and error message.\n  - continue trying to restore other xattrs and bsdflags of the same file\n    after an exception with xattr-setting happened.\n- export-tar:\n\n  - set tar format to GNU_FORMAT explicitly, #5274\n  - fix memory leak with ssh: remote repository, #5568\n  - fix potential memory leak with ssh: remote repository with partial extraction\n- create: fix --dry-run and --stats coexistence, #5415\n- use --timestamp for {utcnow} and {now} if given, #5189\n\nNew features:\n\n- create: implement --stdin-mode, --stdin-user and --stdin-group, #5333\n- allow appending the files cache filename with BORG_FILES_CACHE_SUFFIX env var\n\nOther changes:\n\n- drop python 3.4 support, minimum requirement is 3.5 now.\n- enable using libxxhash instead of bundled xxh64 code\n- update llfuse requirements (1.3.8)\n- set cython language_level in some files to fix warnings\n- allow EIO with warning when trying to hardlink\n- PropDict: fail early if internal_dict is not a dict\n- update shell completions\n- tests / CI\n\n  - add a test for the hashindex corruption bug, #5531 #4829\n  - fix spurious failure in test_cache_files, #5438\n  - added a github ci workflow\n  - reduce testing on travis, no macOS, no py3x-dev, #5467\n  - travis: use newer dists, native py on dist\n- vagrant:\n\n  - remove jessie and trusty boxes, #5348 #5383\n  - pyinstaller 4.0, build on py379\n  - binary build on stretch64, #5348\n  - remove easy_install based pip installation\n- docs:\n\n  - clarify '--one-file-system' for btrfs, #5391\n  - add example for excluding content using the --pattern cmd line arg\n  - complement the documentation for pattern files and exclude files, #5524\n  - made ansible playbook more generic, use package instead of pacman. also\n    change state from \"latest\" to \"present\".\n  - complete documentation on append-only remote repos, #5497\n  - internals: rather talk about target size than statistics, #5336\n  - new compression algorithm policy, #1633 #5505\n  - faq: add a hint on sleeping computer, #5301\n  - note requirements for full disk access on macOS Catalina, #5303\n  - fix/improve description of borg upgrade hardlink usage, #5518\n- modernize 1.1 code:\n\n  - drop code/workarounds only needed to support Python 3.4\n  - remove workaround for pre-release py37 argparse bug\n  - removed some outdated comments/docstrings\n  - requirements: remove some restrictions, lock on current versions\n\n\nVersion 1.1.14 (2020-10-07)\n---------------------------\n\nFixes:\n\n- check --repair: fix potential data loss when interrupting it, #5325\n- exit with 128 + signal number (as documented) when borg is killed by a signal, #5161\n- fix hardlinked CACHEDIR.TAG processing, #4911\n- create --read-special: .part files also should be regular files, #5217\n- llfuse dependency: choose least broken 1.3.6/1.3.7.\n  1.3.6 is broken on python 3.9, 1.3.7 is broken on FreeBSD.\n\nOther changes:\n\n- upgrade bundled xxhash to 0.7.4\n- self test: if it fails, also point to OS and hardware, #5334\n- pyinstaller: compute basepath from spec file location\n- prettier error message when archive gets too big, #5307\n- check/recreate are not \"experimental\" any more (but still potentially dangerous):\n\n  - recreate: remove extra confirmation\n  - rephrase some warnings, update docs, #5164\n- shell completions:\n\n  - misc. updates / fixes\n  - support repositories in fish tab completion, #5256\n  - complete $BORG_RECREATE_I_KNOW_WHAT_I_AM_DOING\n  - rewrite zsh completion:\n\n    - completion for almost all optional and positional arguments\n    - completion for Borg environment variables (parameters)\n- use \"allow/deny list\" instead of \"white/black list\" wording\n- declare \"allow_cache_wipe\" marker in setup.cfg to avoid pytest warning\n- vagrant / tests:\n\n  - misc. fixes / updates\n  - use python 3.5.10 for binary build\n  - build directory-based binaries additionally to the single file binaries\n  - add libffi-dev, required to build python\n  - use cryptography<3.0, more recent versions break the jessie box\n  - test on python 3.9\n  - do brew update with /dev/null redirect to avoid \"too much log output\" on travis-ci\n- docs:\n\n  - add ssh-agent pull backup method docs, #5288\n  - how to approach borg speed issues, #5371\n  - mention double --force in prune docs\n  - update Homebrew install instructions, #5185\n  - better description of how cache and rebuilds of it work\n  - point to borg create --list item flags in recreate usage, #5165\n  - add security faq explaining AES-CTR crypto issues, #5254\n  - add a note to create from stdin regarding files cache, #5180\n  - fix borg.1 manpage generation regression, #5211\n  - clarify how exclude options work in recreate, #5193\n  - add section for retired contributors\n  - hint about not misusing private email addresses of contributors for borg support\n\n\nVersion 1.1.13 (2020-06-06)\n---------------------------\n\nCompatibility notes:\n\n- When upgrading from borg 1.0.x to 1.1.x, please note:\n\n  - read all the compatibility notes for 1.1.0*, starting from 1.1.0b1.\n  - borg upgrade: you do not need to and you also should not run it.\n  - borg might ask some security-related questions once after upgrading.\n    You can answer them either manually or via environment variable.\n    One known case is if you use unencrypted repositories, then it will ask\n    about a unknown unencrypted repository one time.\n  - your first backup with 1.1.x might be significantly slower (it might\n    completely read, chunk, hash a lot files) - this is due to the\n    --files-cache mode change (and happens every time you change mode).\n    You can avoid the one-time slowdown by using the pre-1.1.0rc4-compatible\n    mode (but that is less safe for detecting changed files than the default).\n    See the --files-cache docs for details.\n- 1.1.11 removes WSL autodetection (Windows 10 Subsystem for Linux).\n  If WSL still has a problem with sync_file_range, you need to set\n  BORG_WORKAROUNDS=basesyncfile in the borg process environment to\n  work around the WSL issue.\n\nFixes:\n\n- rebuilt using a current Cython version, compatible with python 3.8, #5214\n\n\nVersion 1.1.12 (2020-06-06)\n---------------------------\n\nFixes:\n\n- fix preload-related memory leak, #5202.\n- mount / borgfs (FUSE filesystem):\n\n  - fix FUSE low linear read speed on large files, #5067\n  - fix crash on old llfuse without birthtime attrs, #5064 - accidentally\n    we required llfuse >= 1.3. Now also old llfuse works again.\n  - set f_namemax in statfs result, #2684\n- update precedence of env vars to set config and cache paths, #4894\n- correctly calculate compression ratio, taking header size into account, too\n\nNew features:\n\n- --bypass-lock option to bypass locking with read-only repositories\n\nOther changes:\n\n- upgrade bundled zstd to 1.4.5\n- travis: adding comments and explanations to Travis config / install script,\n  improve macOS builds.\n- tests: test_delete_force: avoid sporadic test setup issues, #5196\n- misc. vagrant fixes\n- the binary for macOS is now built on macOS 10.12\n- the binaries for Linux are now built on Debian 8 \"Jessie\", #3761\n- docs:\n\n  - PlaceholderError not printed as JSON, #4073\n  - \"How important is Borg config?\", #4941\n  - make Sphinx warnings break docs build, #4587\n  - some markup / warning fixes\n  - add \"updating borgbackup.org/releases\" to release checklist, #4999\n  - add \"rendering docs\" to release checklist, #5000\n  - clarify borg init's encryption modes\n  - add note about patterns and stored paths, #4160\n  - add upgrade of tools to pip installation how-to\n  - document one cause of orphaned chunks in check command, #2295\n  - linked recommended restrictions to ssh public keys on borg servers in faq, #4946\n\n\nVersion 1.1.11 (2020-03-08)\n---------------------------\n\nCompatibility notes:\n\n- When upgrading from borg 1.0.x to 1.1.x, please note:\n\n  - read all the compatibility notes for 1.1.0*, starting from 1.1.0b1.\n  - borg upgrade: you do not need to and you also should not run it.\n  - borg might ask some security-related questions once after upgrading.\n    You can answer them either manually or via environment variable.\n    One known case is if you use unencrypted repositories, then it will ask\n    about a unknown unencrypted repository one time.\n  - your first backup with 1.1.x might be significantly slower (it might\n    completely read, chunk, hash a lot files) - this is due to the\n    --files-cache mode change (and happens every time you change mode).\n    You can avoid the one-time slowdown by using the pre-1.1.0rc4-compatible\n    mode (but that is less safe for detecting changed files than the default).\n    See the --files-cache docs for details.\n- 1.1.11 removes WSL autodetection (Windows 10 Subsystem for Linux).\n  If WSL still has a problem with sync_file_range, you need to set\n  BORG_WORKAROUNDS=basesyncfile in the borg process environment to\n  work around the WSL issue.\n\nFixes:\n\n- fixed potential index corruption / data loss issue due to bug in hashindex_set, #4829.\n  Please read and follow the more detailed notes close to the top of this document.\n- upgrade bundled xxhash to 0.7.3, #4891.\n  0.7.2 is the minimum requirement for correct operations on ARMv6 in non-fixup\n  mode, where unaligned memory accesses cause bus errors.\n  0.7.3 adds some speedups and libxxhash 0.7.3 even has a pkg-config file now.\n- upgrade bundled lz4 to 1.9.2\n- upgrade bundled zstd to 1.4.4\n- fix crash when upgrading erroneous hints file, #4922\n- extract:\n\n  - fix KeyError for \"partial\" extraction, #4607\n  - fix \"partial\" extract for hardlinked contentless file types, #4725\n  - fix preloading for old (0.xx) remote servers, #4652\n  - fix confusing output of borg extract --list --strip-components, #4934\n- delete: after double-force delete, warn about necessary repair, #4704\n- create: give invalid repo error msg if repo config not found, #4411\n- mount: fix FUSE mount missing st_birthtime, #4763 #4767\n- check: do not stumble over invalid item key, #4845\n- info: if the archive doesn't exist, print a pretty message, #4793\n- SecurityManager.known(): check all files, #4614\n- Repository.open: use stat() to check for repo dir, #4695\n- Repository.check_can_create_repository: use stat() to check, #4695\n- fix invalid archive error message\n- fix optional/non-optional location arg, #4541\n- commit-time free space calc: ignore bad compact map entries, #4796\n- ignore EACCES (errno 13) when hardlinking the old config, #4730\n- --prefix / -P: fix processing, avoid argparse issue, #4769\n\nNew features:\n\n- enable placeholder usage in all extra archive arguments\n- new BORG_WORKAROUNDS mechanism, basesyncfile, #4710\n- recreate: support --timestamp option, #4745\n- support platforms without os.link (e.g. Android with Termux), #4901.\n  if we don't have os.link, we just extract another copy instead of making a hardlink.\n- support linux platforms without sync_file_range (e.g. Android 7 with Termux), #4905\n\nOther:\n\n- ignore --stats when given with --dry-run, but continue, #4373\n- add some ProgressIndicator msgids to code / fix docs, #4935\n- elaborate on \"Calculating size\" message\n- argparser: always use REPOSITORY in metavar, also use more consistent help phrasing.\n- check: improve error output for matching index size, see #4829\n- docs:\n\n  - changelog: add advisory about hashindex_set bug #4829\n  - better describe BORG_SECURITY_DIR, BORG_CACHE_DIR, #4919\n  - infos about cache security assumptions, #4900\n  - add FAQ describing difference between a local repo vs. repo on a server.\n  - document how to test exclusion patterns without performing an actual backup\n  - timestamps in the files cache are now usually ctime, #4583\n  - fix bad reference to borg compact (does not exist in 1.1), #4660\n  - create: borg 1.1 is not future any more\n  - extract: document limitation \"needs empty destination\", #4598\n  - how to supply a passphrase, use crypto devices, #4549\n  - fix osxfuse github link in installation docs\n  - add example of exclude-norecurse rule in help patterns\n  - update macOS Brew link\n  - add note about software for automating backups, #4581\n  - AUTHORS: mention copyright+license for bundled msgpack\n  - fix various code blocks in the docs, #4708\n  - updated docs to cover use of temp directory on remote, #4545\n  - add restore docs, #4670\n  - add a pull backup / push restore how-to, #1552\n  - add FAQ how to retain original paths, #4532\n  - explain difference between --exclude and --pattern, #4118\n  - add FAQs for SSH connection issues, #3866\n  - improve password FAQ, #4591\n  - reiterate that 'file cache names are absolute' in FAQ\n- tests:\n\n  - cope with ANY error when importing pytest into borg.testsuite, #4652\n  - fix broken test that relied on improper zlib assumptions\n  - test_fuse: filter out selinux xattrs, #4574\n- travis / vagrant:\n\n  - misc python versions removed / changed (due to openssl 1.1 compatibility)\n    or added (3.7 and 3.8, for better borg compatibility testing)\n  - binary building is on python 3.5.9 now\n- vagrant:\n\n  - add new boxes: ubuntu 18.04 and 20.04, debian 10\n  - update boxes: openindiana, darwin, netbsd\n  - remove old boxes: centos 6\n  - darwin: updated osxfuse to 3.10.4\n  - use debian/ubuntu pip/virtualenv packages\n  - rather use python 3.6.2 than 3.6.0, fixes coverage/sqlite3 issue\n  - use requirements.d/development.lock.txt to avoid compat issues\n- travis:\n\n  - darwin: backport some install code / order from master\n  - remove deprecated keyword \"sudo\" from travis config\n  - allow osx builds to fail, #4955\n    this is due to travis-ci frequently being so slow that the OS X builds\n    just fail because they exceed 50 minutes and get killed by travis.\n\n\nVersion 1.1.10 (2019-05-16)\n---------------------------\n\nFixes:\n\n- extract: hang on partial extraction with ssh: repo, when hardlink master\n  is not matched/extracted and borg hangs on related slave hardlink, #4350\n- lrucache: regularly remove old FDs, #4427\n- avoid stale filehandle issues, #3265\n- freebsd: make xattr platform code api compatible with linux, #3952\n- use whitelist approach for borg serve, #4097\n- borg command shall terminate with rc 2 for ImportErrors, #4424\n- create: only run stat_simple_attrs() once, this increases\n  backup with lots of unchanged files performance by ~ 5%.\n- prune: fix incorrect borg prune --stats output with --dry-run, #4373\n- key export: emit user-friendly error if repo key is exported to a directory,\n  #4348\n\nNew features:\n\n- bundle latest supported msgpack-python release (0.5.6), remove msgpack-python\n  from setup.py install_requires - by default we use the bundled code now.\n  optionally, we still support using an external msgpack (see hints in\n  setup.py), but this requires solid requirements management within\n  distributions and is not recommended.\n  borgbackup will break if you upgrade msgpack to an unsupported version.\n- display msgpack version as part of sysinfo (e.g. in tracebacks)\n- timestamp for borg delete --info added, #4359\n- enable placeholder usage in --comment and --glob-archives, #4559, #4495\n\nOther:\n\n- serve: do not check python/libc for borg serve, #4483\n- shell completions: borg diff second archive\n- release scripts: signing binaries with Qubes OS support\n- testing:\n\n  - vagrant: upgrade openbsd box to 6.4\n  - travis-ci: lock test env to py 3.4 compatible versions, #4343\n  - get rid of confusing coverage warning, #2069\n  - rename test_mount_hardlinks to test_fuse_mount_hardlinks,\n    so both can be excluded by \"not test_fuse\".\n  - pure-py msgpack warning shall not make a lot of tests fail, #4558\n- docs:\n\n  - add \"SSH Configuration\" section to \"borg serve\", #3988, #636, #4485\n  - README: new URL for funding options\n  - add a sample logging.conf in docs/misc, #4380\n  - elaborate on append-only mode docs, #3504\n  - installation: added Alpine Linux to distribution list, #4415\n  - usage.html: only modify window.location when redirecting, #4133\n  - add msgpack license to docs/3rd_party/msgpack\n- vagrant / binary builds:\n\n  - use python 3.5.7 for builds\n  - use osxfuse 3.8.3\n\n\nVersion 1.1.9 (2019-02-10)\n--------------------------\n\nCompatibility notes:\n\n- When upgrading from borg 1.0.x to 1.1.x, please note:\n\n  - read all the compatibility notes for 1.1.0*, starting from 1.1.0b1.\n  - borg upgrade: you do not need to and you also should not run it.\n  - borg might ask some security-related questions once after upgrading.\n    You can answer them either manually or via environment variable.\n    One known case is if you use unencrypted repositories, then it will ask\n    about a unknown unencrypted repository one time.\n  - your first backup with 1.1.x might be significantly slower (it might\n    completely read, chunk, hash a lot files) - this is due to the\n    --files-cache mode change (and happens every time you change mode).\n    You can avoid the one-time slowdown by using the pre-1.1.0rc4-compatible\n    mode (but that is less safe for detecting changed files than the default).\n    See the --files-cache docs for details.\n\nFixes:\n\n- security fix: configure FUSE with \"default_permissions\", #3903\n  \"default_permissions\" is now enforced by borg by default to let the\n  kernel check uid/gid/mode based permissions.\n  \"ignore_permissions\" can be given not to enforce \"default_permissions\".\n- make \"hostname\" short, even on misconfigured systems, #4262\n- fix free space calculation on macOS (and others?), #4289\n- config: quit with error message when no key is provided, #4223\n- recover_segment: handle too small segment files correctly, #4272\n- correctly release memoryview, #4243\n- avoid diaper pattern in configparser by opening files, #4263\n- add \"# cython: language_level=3\" directive to .pyx files, #4214\n- info: consider part files for \"This archive\" stats, #3522\n- work around Microsoft WSL issue #645 (sync_file_range), #1961\n\nNew features:\n\n- add --rsh command line option to complement BORG_RSH env var, #1701\n- init: --make-parent-dirs parent1/parent2/repo_dir, #4235\n\nOther:\n\n- add archive name to check --repair output, #3447\n- check for unsupported msgpack versions\n- shell completions:\n\n  - new shell completions for borg 1.1.9\n  - more complete shell completions for borg mount -o\n  - added shell completions for borg help\n  - option arguments for zsh tab completion\n- docs:\n\n  - add FAQ regarding free disk space check, #3905\n  - update BORG_PASSCOMMAND example and clarify variable expansion, #4249\n  - FAQ regarding change of compression settings, #4222\n  - add note about BSD flags to changelog, #4246\n  - improve logging in example automation script\n  - add note about files changing during backup, #4081\n  - work around the backslash issue, #4280\n  - update release workflow using twine (docs, scripts), #4213\n  - add warnings on repository copies to avoid future problems, #4272\n- tests:\n\n  - fix the homebrew 1.9 issues on travis-ci, #4254\n  - fix duplicate test method name, #4311\n\n\nVersion 1.1.8 (2018-12-09)\n--------------------------\n\nFixes:\n\n- enforce storage quota if set by serve-command, #4093\n- invalid locations: give err msg containing parsed location, #4179\n- list repo: add placeholders for hostname and username, #4130\n- on linux, symlinks can't have ACLs, so don't try to set any, #4044\n\nNew features:\n\n- create: added PATH::archive output on INFO log level\n- read a passphrase from a file descriptor specified in the\n  BORG_PASSPHRASE_FD environment variable.\n\nOther:\n\n- docs:\n\n  - option --format is required for some expensive-to-compute values for json\n\n    borg list by default does not compute expensive values except when\n    they are needed. whether they are needed is determined by the format,\n    in standard mode as well as in --json mode.\n  - tell that our binaries are x86/x64 amd/intel, bauerj has ARM\n  - fixed wrong archive name pattern in CRUD benchmark help\n  - fixed link to cachedir spec in docs, #4140\n- tests:\n\n  - stop using fakeroot on travis, avoids sporadic EISDIR errors, #2482\n  - xattr key names must start with \"user.\" on linux\n  - fix code so flake8 3.6 does not complain\n  - explicitly convert environment variable to str, #4136\n  - fix DeprecationWarning: Flags not at the start of the expression, #4137\n  - support pytest4, #4172\n- vagrant:\n\n  - use python 3.5.6 for builds\n\n\nVersion 1.1.7 (2018-08-11)\n--------------------------\n\nCompatibility notes:\n\n- added support for Python 3.7\n\nFixes:\n\n- cache lock: use lock_wait everywhere to fix infinite wait, see #3968\n- don't archive tagged dir when recursing an excluded dir, #3991\n- py37 argparse: work around bad default in py 3.7.0a/b/rc, #3996\n- py37 remove loggerDict.clear() from tearDown method, #3805\n- some fixes for bugs which likely did not result in problems in practice:\n\n  - fixed logic bug in platform module API version check\n  - fixed xattr/acl function prototypes, added missing ones\n\nNew features:\n\n- init: add warning to store both key and passphrase at safe place(s)\n- BORG_HOST_ID env var to work around all-zero MAC address issue, #3985\n- borg debug dump-repo-objs --ghost (dump everything from segment files,\n  including deleted or superseded objects or commit tags)\n- borg debug search-repo-objs (search in repo objects for hex bytes or strings)\n\nOther changes:\n\n- add Python 3.7 support\n- updated shell completions\n- call socket.gethostname only once\n- locking: better logging, add some asserts\n- borg debug dump-repo-objs:\n\n  - filename layout improvements\n  - use repository.scan() to get on-disk order\n- docs:\n\n  - update installation instructions for macOS\n  - added instructions to install fuse via homebrew\n  - improve diff docs\n  - added note that checkpoints inside files requires 1.1+\n  - add link to tempfile module\n  - remove row/column-spanning from docs source, #4000 #3990\n- tests:\n\n  - fetch less data via os.urandom\n  - add py37 env for tox\n  - travis: add 3.7, remove 3.6-dev (we test with -dev in master)\n- vagrant / binary builds:\n\n  - use osxfuse 3.8.2\n  - use own (uptodate) openindiana box\n\n\nVersion 1.1.6 (2018-06-11)\n--------------------------\n\nCompatibility notes:\n\n- 1.1.6 changes:\n\n  - also allow msgpack-python 0.5.6.\n\nFixes:\n\n- fix borg exception handling on ENOSPC error with xattrs, #3808\n- prune: fix/improve overall progress display\n- borg config repo ... does not need cache/manifest/key, #3802\n- debug dump-repo-objs should not depend on a manifest obj\n- pypi package:\n\n  - include .coveragerc, needed by tox.ini\n  - fix package long description, #3854\n\nNew features:\n\n- mount: add uid, gid, umask mount options\n- delete:\n\n  - only commit once, #3823\n  - implement --dry-run, #3822\n- check:\n\n  - show progress while rebuilding missing manifest, #3787\n  - more --repair output\n- borg config --list <repo>, #3612\n\nOther changes:\n\n- update msgpack requirement, #3753\n- update bundled zstd to 1.3.4, #3745\n- update bundled lz4 code to 1.8.2, #3870\n- docs:\n\n  - describe what BORG_LIBZSTD_PREFIX does\n  - fix and deduplicate encryption quickstart docs, #3776\n- vagrant:\n\n  - FUSE for macOS: upgrade 3.7.1 to 3.8.0\n  - exclude macOS High Sierra upgrade on the darwin64 machine\n  - remove borgbackup.egg-info dir in fs_init (after rsync)\n  - use pyenv-based build/test on jessie32/62\n  - use local 32 and 64bit debian jessie boxes\n  - use \"vagrant\" as username for new xenial box\n- travis OS X: use xcode 8.3 (not broken)\n\n\nVersion 1.1.5 (2018-04-01)\n--------------------------\n\nCompatibility notes:\n\n- 1.1.5 changes:\n\n  - require msgpack-python >= 0.4.6 and < 0.5.0.\n    0.5.0+ dropped python 3.4 testing and also caused some other issues because\n    the python package was renamed to msgpack and emitted some FutureWarning.\n\nFixes:\n\n- create --list: fix that it was never showing M status, #3492\n- create: fix timing for first checkpoint (read files cache early, init\n  checkpoint timer after that), see #3394\n- extract: set rc=1 when extracting damaged files with all-zero replacement\n  chunks or with size inconsistencies, #3448\n- diff: consider an empty file as different to a non-existing file, #3688\n- files cache: improve exception handling, #3553\n- ignore exceptions in scandir_inorder() caused by an implicit stat(),\n  also remove unneeded sort, #3545\n- fixed tab completion problem where a space is always added after path even\n  when it shouldn't\n- build: do .h file content checks in binary mode, fixes build issue for\n  non-ascii header files on pure-ascii locale platforms, #3544 #3639\n- borgfs: fix patterns/paths processing, #3551\n- config: add some validation, #3566\n- repository config: add validation for max_segment_size, #3592\n- set cache previous_location on load instead of save\n- remove platform.uname() call which caused library mismatch issues, #3732\n- add exception handler around deprecated platform.linux_distribution() call\n- use same datetime object for {now} and {utcnow}, #3548\n\nNew features:\n\n- create: implement --stdin-name, #3533\n- add chunker_params to borg archive info (--json)\n- BORG_SHOW_SYSINFO=no to hide system information from exceptions\n\nOther changes:\n\n- updated zsh completions for borg 1.1.4\n- files cache related code cleanups\n- be more helpful when parsing invalid --pattern values, #3575\n- be more clear in secure-erase warning message, #3591\n- improve getpass user experience, #3689\n- docs build: unicode problem fixed when using a py27-based sphinx\n- docs:\n\n  - security: explicitly note what happens OUTSIDE the attack model\n  - security: add note about combining compression and encryption\n  - security: describe chunk size / proximity issue, #3687\n  - quickstart: add note about permissions, borg@localhost, #3452\n  - quickstart: add introduction to repositories & archives, #3620\n  - recreate --recompress: add missing metavar, clarify description, #3617\n  - improve logging docs, #3549\n  - add an example for --pattern usage, #3661\n  - clarify path semantics when matching, #3598\n  - link to offline documentation from README, #3502\n  - add docs on how to verify a signed release with GPG, #3634\n  - chunk seed is generated per repository (not: archive)\n  - better formatting of CPU usage documentation, #3554\n  - extend append-only repo rollback docs, #3579\n- tests:\n\n  - fix erroneously skipped zstd compressor tests, #3606\n  - skip a test if argparse is broken, #3705\n- vagrant:\n\n  - xenial64 box now uses username 'vagrant', #3707\n  - move cleanup steps to fs_init, #3706\n  - the boxcutter wheezy boxes are 404, use local ones\n  - update to Python 3.5.5 (for binary builds)\n\n\nVersion 1.1.4 (2017-12-31)\n--------------------------\n\nCompatibility notes:\n\n- When upgrading from borg 1.0.x to 1.1.x, please note:\n\n  - read all the compatibility notes for 1.1.0*, starting from 1.1.0b1.\n  - borg upgrade: you do not need to and you also should not run it.\n  - borg might ask some security-related questions once after upgrading.\n    You can answer them either manually or via environment variable.\n    One known case is if you use unencrypted repositories, then it will ask\n    about a unknown unencrypted repository one time.\n  - your first backup with 1.1.x might be significantly slower (it might\n    completely read, chunk, hash a lot files) - this is due to the\n    --files-cache mode change (and happens every time you change mode).\n    You can avoid the one-time slowdown by using the pre-1.1.0rc4-compatible\n    mode (but that is less safe for detecting changed files than the default).\n    See the --files-cache docs for details.\n- borg 1.1.4 changes:\n\n  - zstd compression is new in borg 1.1.4, older borg can't handle it.\n  - new minimum requirements for the compression libraries - if the required\n    versions (header and lib) can't be found at build time, bundled code will\n    be used:\n\n    - added requirement: libzstd >= 1.3.0 (bundled: 1.3.2)\n    - updated requirement: liblz4 >= 1.7.0 / r129 (bundled: 1.8.0)\n\nFixes:\n\n- check: data corruption fix: fix for borg check --repair malfunction, #3444.\n  See the more detailed notes close to the top of this document.\n- delete: also delete security dir when deleting a repo, #3427\n- prune: fix building the \"borg prune\" man page, #3398\n- init: use given --storage-quota for local repo, #3470\n- init: properly quote repo path in output\n- fix startup delay with dns-only own fqdn resolving, #3471\n\nNew features:\n\n- added zstd compression. try it!\n- added placeholder {reverse-fqdn} for fqdn in reverse notation\n- added BORG_BASE_DIR environment variable, #3338\n\nOther changes:\n\n- list help topics when invalid topic is requested\n- fix lz4 deprecation warning, requires lz4 >= 1.7.0 (r129)\n- add parens for C preprocessor macro argument usages (did not cause malfunction)\n- exclude broken pytest 3.3.0 release\n- updated fish/bash completions\n- init: more clear exception messages for borg create, #3465\n- docs:\n\n  - add auto-generated docs for borg config\n  - don't generate HTML docs page for borgfs, #3404\n  - docs update for lz4 b2 zstd changes\n  - add zstd to compression help, readme, docs\n  - update requirements and install docs about bundled lz4 and zstd\n- refactored build of the compress and crypto.low_level extensions, #3415:\n\n  - move some lib/build related code to setup_{zstd,lz4,b2}.py\n  - bundle lz4 1.8.0 (requirement: >= 1.7.0 / r129)\n  - bundle zstd 1.3.2 (requirement: >= 1.3.0)\n  - blake2 was already bundled\n  - rename BORG_LZ4_PREFIX env var to BORG_LIBLZ4_PREFIX for better consistency:\n    we also have BORG_LIBB2_PREFIX and BORG_LIBZSTD_PREFIX now.\n  - add prefer_system_lib* = True settings to setup.py - by default the build\n    will prefer a shared library over the bundled code, if library and headers\n    can be found and meet the minimum requirements.\n\n\nVersion 1.1.3 (2017-11-27)\n--------------------------\n\nFixes:\n\n- Security Fix for CVE-2017-15914: Incorrect implementation of access controls\n  allows remote users to override repository restrictions in Borg servers.\n  A user able to access a remote Borg SSH server is able to circumvent access\n  controls post-authentication.\n  Affected releases: 1.1.0, 1.1.1, 1.1.2. Releases 1.0.x are NOT affected.\n- crc32: deal with unaligned buffer, add tests - this broke borg on older ARM\n  CPUs that can not deal with unaligned 32bit memory accesses and raise a bus\n  error in such cases. the fix might also improve performance on some CPUs as\n  all 32bit memory accesses by the crc32 code are properly aligned now. #3317\n- mount: fixed support of --consider-part-files and do not show .borg_part_N\n  files by default in the mounted FUSE filesystem. #3347\n- fixed cache/repo timestamp inconsistency message, highlight that information\n  is obtained from security dir (deleting the cache will not bypass this error\n  in case the user knows this is a legitimate repo).\n- borgfs: don't show sub-command in borgfs help, #3287\n- create: show an error when --dry-run and --stats are used together, #3298\n\nNew features:\n\n- mount: added exclusion group options and paths, #2138\n\n  Reused some code to support similar options/paths as borg extract offers -\n  making good use of these to mount only a smaller subset of dirs/files can\n  speed up mounting a lot and also will consume way less memory.\n\n  borg mount [options] repo_or_archive mountpoint path [paths...]\n\n  paths: you can just give some \"root paths\" (like for borg extract) to\n  only partially populate the FUSE filesystem.\n\n  new options: --exclude[-from], --pattern[s-from], --strip-components\n- create/extract: support st_birthtime on platforms supporting it, #3272\n- add \"borg config\" command for querying/setting/deleting config values, #3304\n\nOther changes:\n\n- clean up and simplify packaging (only package committed files, do not install\n  .c/.h/.pyx files)\n- docs:\n\n  - point out tuning options for borg create, #3239\n  - add instructions for using ntfsclone, zerofree, #81\n  - move image backup-related FAQ entries to a new page\n  - clarify key aliases for borg list --format, #3111\n  - mention break-lock in checkpointing FAQ entry, #3328\n  - document sshfs rename workaround, #3315\n  - add FAQ about removing files from existing archives\n  - add FAQ about different prune policies\n  - usage and man page for borgfs, #3216\n  - clarify create --stats duration vs. wall time, #3301\n  - clarify encrypted key format for borg key export, #3296\n  - update release checklist about security fixes\n  - document good and problematic option placements, fix examples, #3356\n  - add note about using --nobsdflags to avoid speed penalty related to\n    bsdflags, #3239\n  - move most of support section to www.borgbackup.org\n\n\nVersion 1.1.2 (2017-11-05)\n--------------------------\n\nFixes:\n\n- fix KeyError crash when talking to borg server < 1.0.7, #3244\n- extract: set bsdflags last (include immutable flag), #3263\n- create: don't do stat() call on excluded-norecurse directory, fix exception\n  handling for stat() call, #3209\n- create --stats: do not count data volume twice when checkpointing, #3224\n- recreate: move chunks_healthy when excluding hardlink master, #3228\n- recreate: get rid of chunks_healthy when rechunking (does not match), #3218\n- check: get rid of already existing not matching chunks_healthy metadata, #3218\n- list: fix stdout broken pipe handling, #3245\n- list/diff: remove tag-file options (not used), #3226\n\nNew features:\n\n- bash, zsh and fish shell auto-completions, see scripts/shell_completions/\n- added BORG_CONFIG_DIR env var, #3083\n\nOther changes:\n\n- docs:\n\n  - clarify using a blank passphrase in keyfile mode\n  - mention \"!\" (exclude-norecurse) type in \"patterns\" help\n  - document to first heal before running borg recreate to re-chunk stuff,\n    because that will have to get rid of chunks_healthy metadata.\n  - more than 23 is not supported for CHUNK_MAX_EXP, #3115\n  - borg does not respect nodump flag by default any more\n  - clarify same-filesystem requirement for borg upgrade, #2083\n  - update / rephrase cygwin / WSL status, #3174\n  - improve docs about --stats, #3260\n- vagrant: openindiana new clang package\n\nAlready contained in 1.1.1 (last minute fix):\n\n- arg parsing: fix fallback function, refactor, #3205. This is a fixup\n  for #3155, which was broken on at least python <= 3.4.2.\n\n\nVersion 1.1.1 (2017-10-22)\n--------------------------\n\nCompatibility notes:\n\n- The deprecated --no-files-cache is not a global/common option any more,\n  but only available for borg create (it is not needed for anything else).\n  Use --files-cache=disabled instead of --no-files-cache.\n- The nodump flag (\"do not back up this file\") is not honoured any more by\n  default because this functionality (esp. if it happened by error or\n  unexpected) was rather confusing and unexplainable at first to users.\n  If you want that \"do not back up NODUMP-flagged files\" behaviour, use:\n  borg create --exclude-nodump ...\n- If you are on Linux and do not need bsdflags archived, consider using\n  ``--nobsdflags`` with ``borg create`` to avoid additional syscalls and\n  speed up backup creation.\n\nFixes:\n\n- borg recreate: correctly compute part file sizes. fixes cosmetic, but\n  annoying issue as borg check complains about size inconsistencies of part\n  files in affected archives. you can solve that by running borg recreate on\n  these archives, see also #3157.\n- bsdflags support: do not open BLK/CHR/LNK files, avoid crashes and\n  slowness, #3130\n- recreate: don't crash on attic archives w/o time_end, #3109\n- don't crash on repository filesystems w/o hardlink support, #3107\n- don't crash in first part of truncate_and_unlink, #3117\n- fix server-side IndexError crash with clients < 1.0.7, #3192\n- don't show traceback if only a global option is given, show help, #3142\n- cache: use SaveFile for more safety, #3158\n- init: fix wrong encryption choices in command line parser, fix missing\n  \"authenticated-blake2\", #3103\n- move --no-files-cache from common to borg create options, #3146\n- fix detection of non-local path (failed on ..filename), #3108\n- logging with fileConfig: set json attr on \"borg\" logger, #3114\n- fix crash with relative BORG_KEY_FILE, #3197\n- show excluded dir with \"x\" for tagged dirs / caches, #3189\n\nNew features:\n\n- create: --nobsdflags and --exclude-nodump options, #3160\n- extract: --nobsdflags option, #3160\n\nOther changes:\n\n- remove annoying hardlinked symlinks warning, #3175\n- vagrant: use self-made FreeBSD 10.3 box, #3022\n- travis: don't brew update, hopefully fixes #2532\n- docs:\n\n  - readme: -e option is required in borg 1.1\n  - add example showing --show-version --show-rc\n  - use --format rather than --list-format (deprecated) in example\n  - update docs about hardlinked symlinks limitation\n\n\nVersion 1.1.0 (2017-10-07)\n--------------------------\n\nCompatibility notes:\n\n- borg command line: do not put options in between positional arguments\n\n  This sometimes works (e.g. it worked in borg 1.0.x), but can easily stop\n  working if we make positional arguments optional (like it happened for\n  borg create's \"paths\" argument in 1.1). There are also places in borg 1.0\n  where we do that, so it doesn't work there in general either. #3356\n\n  Good: borg create -v --stats repo::archive path\n  Good: borg create repo::archive path -v --stats\n  Bad:  borg create repo::archive -v --stats path\n\nFixes:\n\n- fix LD_LIBRARY_PATH restoration for subprocesses, #3077\n- \"auto\" compression: make sure expensive compression is actually better,\n  otherwise store lz4 compressed data we already computed.\n\nOther changes:\n\n- docs:\n\n  - FAQ: we do not implement futile attempts of ETA / progress displays\n  - manpage: fix typos, update homepage\n  - implement simple \"issue\" role for manpage generation, #3075\n\n\nVersion 1.1.0rc4 (2017-10-01)\n-----------------------------\n\nCompatibility notes:\n\n- A borg server >= 1.1.0rc4 does not support borg clients 1.1.0b3-b5. #3033\n- The files cache is now controlled differently and has a new default mode:\n\n  - the files cache now uses ctime by default for improved file change\n    detection safety. You can still use mtime for more speed and less safety.\n  - --ignore-inode is deprecated (use --files-cache=... without \"inode\")\n  - --no-files-cache is deprecated (use --files-cache=disabled)\n\nNew features:\n\n- --files-cache - implement files cache mode control, #911\n  You can now control the files cache mode using this option:\n  --files-cache={ctime,mtime,size,inode,rechunk,disabled}\n  (only some combinations are supported). See the docs for details.\n\nFixes:\n\n- remote progress/logging: deal with partial lines, #2637\n- remote progress: flush json mode output\n- fix subprocess environments, #3050 (and more)\n\nOther changes:\n\n- remove client_supports_log_v3 flag, #3033\n- exclude broken Cython 0.27(.0) in requirements, #3066\n- vagrant:\n\n  - upgrade to FUSE for macOS 3.7.1\n  - use Python 3.5.4 to build the binaries\n- docs:\n\n  - security: change-passphrase only changes the passphrase, #2990\n  - fixed/improved borg create --compression examples, #3034\n  - add note about metadata dedup and --no[ac]time, #2518\n  - twitter account @borgbackup now, better visible, #2948\n  - simplified rate limiting wrapper in FAQ\n\n\nVersion 1.1.0rc3 (2017-09-10)\n-----------------------------\n\nNew features:\n\n- delete: support naming multiple archives, #2958\n\nFixes:\n\n- repo cleanup/write: invalidate cached FDs, #2982\n- fix datetime.isoformat() microseconds issues, #2994\n- recover_segment: use mmap(), lower memory needs, #2987\n\nOther changes:\n\n- with-lock: close segment file before invoking subprocess\n- keymanager: don't depend on optional readline module, #2976\n- docs:\n\n  - fix macOS keychain integration command\n  - show/link new screencasts in README, #2936\n  - document utf-8 locale requirement for json mode, #2273\n- vagrant: clean up shell profile init, user name, #2977\n- test_detect_attic_repo: don't test mount, #2975\n- add debug logging for repository cleanup\n\n\nVersion 1.1.0rc2 (2017-08-28)\n-----------------------------\n\nCompatibility notes:\n\n- list: corrected mix-up of \"isomtime\" and \"mtime\" formats. Previously,\n  \"isomtime\" was the default but produced a verbose human format,\n  while \"mtime\" produced a ISO-8601-like format.\n  The behaviours have been swapped (so \"mtime\" is human, \"isomtime\" is ISO-like),\n  and the default is now \"mtime\".\n  \"isomtime\" is now a real ISO-8601 format (\"T\" between date and time, not a space).\n\nNew features:\n\n- None.\n\nFixes:\n\n- list: fix weird mixup of mtime/isomtime\n- create --timestamp: set start time, #2957\n- ignore corrupt files cache, #2939\n- migrate locks to child PID when daemonize is used\n- fix exitcode of borg serve, #2910\n- only compare contents when chunker params match, #2899\n- umount: try fusermount, then try umount, #2863\n\nOther changes:\n\n- JSON: use a more standard ISO 8601 datetime format, #2376\n- cache: write_archive_index: truncate_and_unlink on error, #2628\n- detect non-upgraded Attic repositories, #1933\n- delete various nogil and threading related lines\n- coala / pylint related improvements\n- docs:\n\n  - renew asciinema/screencasts, #669\n  - create: document exclusion through nodump, #2949\n  - minor formatting fixes\n  - tar: tarpipe example\n  - improve \"with-lock\" and \"info\" docs, #2869\n  - detail how to use macOS/GNOME/KDE keyrings for repo passwords, #392\n- travis: only short-circuit docs-only changes for pull requests\n- vagrant:\n\n  - netbsd: bash is already installed\n  - fix netbsd version in PKG_PATH\n  - add exe location to PATH when we build an exe\n\n\nVersion 1.1.0rc1 (2017-07-24)\n-----------------------------\n\nCompatibility notes:\n\n- delete: removed short option for --cache-only\n\nNew features:\n\n- support borg list repo --format {comment} {bcomment} {end}, #2081\n- key import: allow reading from stdin, #2760\n\nFixes:\n\n- with-lock: avoid creating segment files that might be overwritten later, #1867\n- prune: fix checkpoints processing with --glob-archives\n- FUSE: versions view: keep original file extension at end, #2769\n- fix --last, --first: do not accept values <= 0,\n  fix reversed archive ordering with --last\n- include testsuite data (attic.tar.gz) when installing the package\n- use limited unpacker for outer key, for manifest (both security precautions),\n  #2174 #2175\n- fix bashism in shell scripts, #2820, #2816\n- cleanup endianness detection, create _endian.h,\n  fixes build on alpine linux, #2809\n- fix crash with --no-cache-sync (give known chunk size to chunk_incref), #2853\n\nOther changes:\n\n- FUSE: versions view: linear numbering by archive time\n- split up interval parsing from filtering for --keep-within, #2610\n- add a basic .editorconfig, #2734\n- use archive creation time as mtime for FUSE mount, #2834\n- upgrade FUSE for macOS (osxfuse) from 3.5.8 to 3.6.3, #2706\n- hashindex: speed up by replacing modulo with \"if\" to check for wraparound\n- coala checker / pylint: fixed requirements and .coafile, more ignores\n- borg upgrade: name backup directories as 'before-upgrade', #2811\n- add .mailmap\n- some minor changes suggested by lgtm.com\n- docs:\n\n  - better explanation of the --ignore-inode option relevance, #2800\n  - fix openSUSE command and add openSUSE section\n  - simplify ssh authorized_keys file using \"restrict\", add legacy note, #2121\n  - mount: show usage of archive filters\n  - mount: add repository example, #2462\n  - info: update and add examples, #2765\n  - prune: include example\n  - improved style / formatting\n  - improved/fixed segments_per_dir docs\n  - recreate: fix wrong \"remove unwanted files\" example\n  - reference list of status chars in borg recreate --filter description\n  - update source-install docs about doc build dependencies, #2795\n  - cleanup installation docs\n  - file system requirements, update segs per dir\n  - fix checkpoints/parts reference in FAQ, #2859\n- code:\n\n  - hashindex: don't pass side effect into macro\n  - crypto low_level: don't mutate local bytes()\n  - use dash_open function to open file or \"-\" for stdin/stdout\n  - archiver: argparse cleanup / refactoring\n  - shellpattern: add match_end arg\n- tests: added some additional unit tests, some fixes, #2700 #2710\n- vagrant: fix setup of cygwin, add Debian 9 \"stretch\"\n- travis: don't perform full travis build on docs-only changes, #2531\n\n\nVersion 1.1.0b6 (2017-06-18)\n----------------------------\n\nCompatibility notes:\n\n- Running \"borg init\" via a \"borg serve --append-only\" server will *not* create\n  an append-only repository anymore. Use \"borg init --append-only\" to initialize\n  an append-only repository.\n\n- Repositories in the \"repokey\" and \"repokey-blake2\" modes with an empty passphrase\n  are now treated as unencrypted repositories for security checks (e.g.\n  BORG_UNKNOWN_UNENCRYPTED_REPO_ACCESS_IS_OK).\n\n  Previously there would be no prompts nor messages if an unknown repository\n  in one of these modes with an empty passphrase was encountered. This would\n  allow an attacker to swap a repository, if one assumed that the lack of\n  password prompts was due to a set BORG_PASSPHRASE.\n\n  Since the \"trick\" does not work if BORG_PASSPHRASE is set, this does generally\n  not affect scripts.\n\n- Repositories in the \"authenticated\" mode are now treated as the unencrypted\n  repositories they are.\n\n- The client-side temporary repository cache now holds unencrypted data for better speed.\n\n- borg init: removed the short form of --append-only (-a).\n\n- borg upgrade: removed the short form of --inplace (-i).\n\nNew features:\n\n- reimplemented the RepositoryCache, size-limited caching of decrypted repo\n  contents, integrity checked via xxh64. #2515\n- reduced space usage of chunks.archive.d. Existing caches are migrated during\n  a cache sync. #235 #2638\n- integrity checking using xxh64 for important files used by borg, #1101:\n\n  - repository: index and hints files\n  - cache: chunks and files caches, chunks.archive.d\n- improve cache sync speed, #1729\n- create: new --no-cache-sync option\n- add repository mandatory feature flags infrastructure, #1806\n- Verify most operations against SecurityManager. Location, manifest timestamp\n  and key types are now checked for almost all non-debug commands. #2487\n- implement storage quotas, #2517\n- serve: add --restrict-to-repository, #2589\n- BORG_PASSCOMMAND: use external tool providing the key passphrase, #2573\n- borg export-tar, #2519\n- list: --json-lines instead of --json for archive contents, #2439\n- add --debug-profile option (and also \"borg debug convert-profile\"), #2473\n- implement --glob-archives/-a, #2448\n- normalize authenticated key modes for better naming consistency:\n\n  - rename \"authenticated\" to \"authenticated-blake2\" (uses blake2b)\n  - implement \"authenticated\" mode (uses hmac-sha256)\n\nFixes:\n\n- hashindex: read/write indices >2 GiB on 32bit systems, better error\n  reporting, #2496\n- repository URLs: implement IPv6 address support and also more informative\n  error message when parsing fails.\n- mount: check whether llfuse is installed before asking for passphrase, #2540\n- mount: do pre-mount checks before opening repository, #2541\n- FUSE:\n\n  - fix crash if empty (None) xattr is read, #2534\n  - fix read(2) caching data in metadata cache\n  - fix negative uid/gid crash (fix crash when mounting archives\n    of external drives made on cygwin), #2674\n  - redo ItemCache, on top of object cache\n  - use decrypted cache\n  - remove unnecessary normpaths\n- serve: ignore --append-only when initializing a repository (borg init), #2501\n- serve: fix incorrect type of exception_short for Errors, #2513\n- fix --exclude and --exclude-from recursing into directories, #2469\n- init: don't allow creating nested repositories, #2563\n- --json: fix encryption[mode] not being the cmdline name\n- remote: propagate Error.traceback correctly\n- fix remote logging and progress, #2241\n\n  - implement --debug-topic for remote servers\n  - remote: restore \"Remote:\" prefix (as used in 1.0.x)\n  - rpc negotiate: enable v3 log protocol only for supported clients\n  - fix --progress and logging in general for remote\n- fix parse_version, add tests, #2556\n- repository: truncate segments (and also some other files) before unlinking, #2557\n- recreate: keep timestamps as in original archive, #2384\n- recreate: if single archive is not processed, exit 2\n- patterns: don't recurse with ! / --exclude for pf:, #2509\n- cache sync: fix n^2 behaviour in lookup_name\n- extract: don't write to disk with --stdout (affected non-regular-file items), #2645\n- hashindex: implement KeyError, more tests\n\nOther changes:\n\n- remote: show path in PathNotAllowed\n- consider repokey w/o passphrase == unencrypted, #2169\n- consider authenticated mode == unencrypted, #2503\n- restrict key file names, #2560\n- document follow_symlinks requirements, check libc, use stat and chown\n  with follow_symlinks=False, #2507\n- support common options on the main command, #2508\n- support common options on mid-level commands (e.g. borg *key* export)\n- make --progress a common option\n- increase DEFAULT_SEGMENTS_PER_DIR to 1000\n- chunker: fix invalid use of types (function only used by tests)\n- chunker: don't do uint32_t >> 32\n- FUSE:\n\n  - add instrumentation (--debug and SIGUSR1/SIGINFO)\n  - reduced memory usage for repository mounts by lazily instantiating archives\n  - improved archive load times\n- info: use CacheSynchronizer & HashIndex.stats_against (better performance)\n- docs:\n\n  - init: document --encryption as required\n  - security: OpenSSL usage\n  - security: used implementations; note python libraries\n  - security: security track record of OpenSSL and msgpack\n  - patterns: document denial of service (regex, wildcards)\n  - init: note possible denial of service with \"none\" mode\n  - init: document SHA extension is supported in OpenSSL and thus SHA is\n    faster on AMD Ryzen than blake2b.\n  - book: use A4 format, new builder option format.\n  - book: create appendices\n  - data structures: explain repository compaction\n  - data structures: add chunk layout diagram\n  - data structures: integrity checking\n  - data structures: demingle cache and repo index\n  - Attic FAQ: separate section for attic stuff\n  - FAQ: I get an IntegrityError or similar - what now?\n  - FAQ: Can I use Borg on SMR hard drives?, #2252\n  - FAQ: specify \"using inline shell scripts\"\n  - add systemd warning regarding placeholders, #2543\n  - xattr: document API\n  - add docs/misc/borg-data-flow data flow chart\n  - debugging facilities\n  - README: how to help the project, #2550\n  - README: add bountysource badge, #2558\n  - fresh new theme + tweaking\n  - logo: vectorized (PDF and SVG) versions\n  - frontends: use headlines - you can link to them\n  - mark --pattern, --patterns-from as experimental\n  - highlight experimental features in online docs\n  - remove regex based pattern examples, #2458\n  - nanorst for \"borg help TOPIC\" and --help\n  - split deployment\n  - deployment: hosting repositories\n  - deployment: automated backups to a local hard drive\n  - development: vagrant, windows10 requirements\n  - development: update docs remarks\n  - split usage docs, #2627\n  - usage: avoid bash highlight, [options] instead of <options>\n  - usage: add benchmark page\n  - helpers: truncate_and_unlink doc\n  - don't suggest to leak BORG_PASSPHRASE\n  - internals: columnize rather long ToC [webkit fixup]\n    internals: manifest & feature flags\n  - internals: more HashIndex details\n  - internals: fix ASCII art equations\n  - internals: edited obj graph related sections a bit\n  - internals: layers image + description\n  - fix way too small figures in pdf\n  - index: disable syntax highlight (bash)\n  - improve options formatting, fix accidental block quotes\n\n- testing / checking:\n\n  - add support for using coala, #1366\n  - testsuite: add ArchiverCorruptionTestCase\n  - do not test logger name, #2504\n  - call setup_logging after destroying logging config\n  - testsuite.archiver: normalise pytest.raises vs. assert_raises\n  - add test for preserved intermediate folder permissions, #2477\n  - key: add round-trip test\n  - remove attic dependency of the tests, #2505\n  - enable remote tests on cygwin\n  - tests: suppress tar's future timestamp warning\n  - cache sync: add more refcount tests\n  - repository: add tests, including corruption tests\n\n- vagrant:\n\n  - control VM cpus and pytest workers via env vars VMCPUS and XDISTN\n  - update cleaning workdir\n  - fix openbsd shell\n  - add OpenIndiana\n\n- packaging:\n\n  - binaries: don't bundle libssl\n  - setup.py clean to remove compiled files\n  - fail in borg package if version metadata is very broken (setuptools_scm)\n\n- repo / code structure:\n\n  - create borg.algorithms and borg.crypto packages\n  - algorithms: rename crc32 to checksums\n  - move patterns to module, #2469\n  - gitignore: complete paths for src/ excludes\n  - cache: extract CacheConfig class\n  - implement IntegrityCheckedFile + Detached variant, #2502 #1688\n  - introduce popen_with_error_handling to handle common user errors\n\n\nVersion 1.1.0b5 (2017-04-30)\n----------------------------\n\nCompatibility notes:\n\n- BORG_HOSTNAME_IS_UNIQUE is now on by default.\n- removed --compression-from feature\n- recreate: add --recompress flag, unify --always-recompress and\n  --recompress\n\nFixes:\n\n- catch exception for os.link when hardlinks are not supported, #2405\n- borg rename / recreate: expand placeholders, #2386\n- generic support for hardlinks (files, devices, FIFOs), #2324\n- extract: also create parent dir for device files, if needed, #2358\n- extract: if a hardlink master is not in the to-be-extracted subset,\n  the \"x\" status was not displayed for it, #2351\n- embrace y2038 issue to support 32bit platforms: clamp timestamps to int32,\n  #2347\n- verify_data: fix IntegrityError handling for defect chunks, #2442\n- allow excluding parent and including child, #2314\n\nOther changes:\n\n- refactor compression decision stuff\n- change global compression default to lz4 as well, to be consistent\n  with --compression defaults.\n- placeholders: deny access to internals and other unspecified stuff\n- clearer error message for unrecognized placeholder\n- more clear exception if borg check does not help, #2427\n- vagrant: upgrade FUSE for macOS to 3.5.8, #2346\n- linux binary builds: get rid of glibc 2.13 dependency, #2430\n- docs:\n\n  - placeholders: document escaping\n  - serve: env vars in original commands are ignored\n  - tell what kind of hardlinks we support\n  - more docs about compression\n  - LICENSE: use canonical formulation\n    (\"copyright holders and contributors\" instead of \"author\")\n  - document borg init behaviour via append-only borg serve, #2440\n  - be clear about what buzhash is used for, #2390\n  - add hint about chunker params, #2421\n  - clarify borg upgrade docs, #2436\n  - FAQ to explain warning when running borg check --repair, #2341\n  - repository file system requirements, #2080\n  - pre-install considerations\n  - misc. formatting / crossref fixes\n- tests:\n\n  - enhance travis setuptools_scm situation\n  - add extra test for the hashindex\n  - fix invalid param issue in benchmarks\n\nThese belong to 1.1.0b4 release, but did not make it into changelog by then:\n\n- vagrant: increase memory for parallel testing\n- lz4 compress: lower max. buffer size, exception handling\n- add docstring to do_benchmark_crud\n- patterns help: mention path full-match in intro\n\n\nVersion 1.1.0b4 (2017-03-27)\n----------------------------\n\nCompatibility notes:\n\n- init: the --encryption argument is mandatory now (there are several choices)\n- moved \"borg migrate-to-repokey\" to \"borg key migrate-to-repokey\".\n- \"borg change-passphrase\" is deprecated, use \"borg key change-passphrase\"\n  instead.\n- the --exclude-if-present option now supports tagging a folder with any\n  filesystem object type (file, folder, etc), instead of expecting only files\n  as tags, #1999\n- the --keep-tag-files option has been deprecated in favor of the new\n  --keep-exclude-tags, to account for the change mentioned above.\n- use lz4 compression by default, #2179\n\nNew features:\n\n- JSON API to make developing frontends and automation easier\n  (see :ref:`json_output`)\n\n  - add JSON output to commands: `borg create/list/info --json ...`.\n  - add --log-json option for structured logging output.\n  - add JSON progress information, JSON support for confirmations (yes()).\n- add two new options --pattern and --patterns-from as discussed in #1406\n- new path full match pattern style (pf:) for very fast matching, #2334\n- add 'debug dump-manifest' and 'debug dump-archive' commands\n- add 'borg benchmark crud' command, #1788\n- new 'borg delete --force --force' to delete severely corrupted archives, #1975\n- info: show utilization of maximum archive size, #1452\n- list: add dsize and dcsize keys, #2164\n- paperkey.html: Add interactive html template for printing key backups.\n- key export: add qr html export mode\n- securely erase config file (which might have old encryption key), #2257\n- archived file items: add size to metadata, 'borg extract' and 'borg check' do\n  check the file size for consistency, FUSE uses precomputed size from Item.\n\nFixes:\n\n- fix remote speed regression introduced in 1.1.0b3, #2185\n- fix regression handling timestamps beyond 2262 (revert bigint removal),\n  introduced in 1.1.0b3, #2321\n- clamp (nano)second values to unproblematic range, #2304\n- hashindex: rebuild hashtable if we have too little empty buckets\n  (performance fix), #2246\n- Location regex: fix bad parsing of wrong syntax\n- ignore posix_fadvise errors in repository.py, #2095\n- borg rpc: use limited msgpack.Unpacker (security precaution), #2139\n- Manifest: Make sure manifest timestamp is strictly monotonically increasing.\n- create: handle BackupOSError on a per-path level in one spot\n- create: clarify -x option / meaning of \"same filesystem\"\n- create: don't create hard link refs to failed files\n- archive check: detect and fix missing all-zero replacement chunks, #2180\n- files cache: update inode number when --ignore-inode is used, #2226\n- fix decompression exceptions crashing ``check --verify-data`` and others\n  instead of reporting integrity error, #2224 #2221\n- extract: warning for unextracted big extended attributes, #2258, #2161\n- mount: umount on SIGINT/^C when in foreground\n- mount: handle invalid hard link refs\n- mount: fix huge RAM consumption when mounting a repository (saves number of\n  archives * 8 MiB), #2308\n- hashindex: detect mingw byte order #2073\n- hashindex: fix wrong skip_hint on hashindex_set when encountering tombstones,\n  the regression was introduced in #1748\n- fix ChunkIndex.__contains__ assertion  for big-endian archs\n- fix borg key/debug/benchmark crashing without subcommand, #2240\n- Location: accept //servername/share/path\n- correct/refactor calculation of unique/non-unique chunks\n- extract: fix missing call to ProgressIndicator.finish\n- prune: fix error msg, it is --keep-within, not --within\n- fix \"auto\" compression mode bug (not compressing), #2331\n- fix symlink item fs size computation, #2344\n\nOther changes:\n\n- remote repository: improved async exception processing, #2255 #2225\n- with --compression auto,C, only use C if lz4 achieves at least 3% compression\n- PatternMatcher: only normalize path once, #2338\n- hashindex: separate endian-dependent defs from endian detection\n- migrate-to-repokey: ask using canonical_path() as we do everywhere else.\n- SyncFile: fix use of fd object after close\n- make LoggedIO.close_segment reentrant\n- creating a new segment: use \"xb\" mode, #2099\n- redo key_creator, key_factory, centralise key knowledge, #2272\n- add return code functions, #2199\n- list: only load cache if needed\n- list: files->items, clarifications\n- list: add \"name\" key for consistency with info cmd\n- ArchiveFormatter: add \"start\" key for compatibility with \"info\"\n- RemoteRepository: account rx/tx bytes\n- setup.py build_usage/build_man/build_api fixes\n- Manifest.in: simplify, exclude .so, .dll and .orig, #2066\n- FUSE: get rid of chunk accounting, st_blocks = ceil(size / blocksize).\n- tests:\n\n  - help python development by testing 3.6-dev\n  - test for borg delete --force\n- vagrant:\n\n  - freebsd: some fixes, #2067\n  - darwin64: use osxfuse 3.5.4 for tests / to build binaries\n  - darwin64: improve VM settings\n  - use python 3.5.3 to build binaries, #2078\n  - upgrade pyinstaller from 3.1.1+ to 3.2.1\n  - pyinstaller: use fixed AND freshly compiled bootloader, #2002\n  - pyinstaller: automatically builds bootloader if missing\n- docs:\n\n  - create really nice man pages\n  - faq: mention --remote-ratelimit in bandwidth limit question\n  - fix caskroom link, #2299\n  - docs/security: reiterate that RPC in Borg does no networking\n  - docs/security: counter tracking, #2266\n  - docs/development: update merge remarks\n  - address SSH batch mode in docs, #2202 #2270\n  - add warning about running build_usage on Python >3.4, #2123\n  - one link per distro in the installation page\n  - improve --exclude-if-present and --keep-exclude-tags, #2268\n  - improve automated backup script in doc, #2214\n  - improve remote-path description\n  - update docs for create -C default change (lz4)\n  - document relative path usage, #1868\n  - document snapshot usage, #2178\n  - corrected some stuff in internals+security\n  - internals: move toctree to after the introduction text\n  - clarify metadata kind, manifest ops\n  - key enc: correct / clarify some stuff, link to internals/security\n  - datas: enc: 1.1.x mas different MACs\n  - datas: enc: correct factual error -- no nonce involved there.\n  - make internals.rst an index page and edit it a bit\n  - add \"Cryptography in Borg\" and \"Remote RPC protocol security\" sections\n  - document BORG_HOSTNAME_IS_UNIQUE, #2087\n  - FAQ by categories as proposed by @anarcat in #1802\n  - FAQ: update Which file types, attributes, etc. are *not* preserved?\n  - development: new branching model for git repository\n  - development: define \"ours\" merge strategy for auto-generated files\n  - create: move --exclude note to main doc\n  - create: move item flags to main doc\n  - fix examples using borg init without -e/--encryption\n  - list: don't print key listings in fat (html + man)\n  - remove Python API docs (were very incomplete, build problems on RTFD)\n  - added FAQ section about backing up root partition\n\n\nVersion 1.1.0b3 (2017-01-15)\n----------------------------\n\nCompatibility notes:\n\n- borg init: removed the default of \"--encryption/-e\", #1979\n  This was done so users do a informed decision about -e mode.\n\nBug fixes:\n\n- borg recreate: don't rechunkify unless explicitly told so\n- borg info: fixed bug when called without arguments, #1914\n- borg init: fix free space check crashing if disk is full, #1821\n- borg debug delete/get obj: fix wrong reference to exception\n- fix processing of remote ~/ and ~user/ paths (regressed since 1.1.0b1), #1759\n- posix platform module: only build / import on non-win32 platforms, #2041\n\nNew features:\n\n- new CRC32 implementations that are much faster than the zlib one used previously, #1970\n- add blake2b key modes (use blake2b as MAC). This links against system libb2,\n  if possible, otherwise uses bundled code\n- automatically remove stale locks - set BORG_HOSTNAME_IS_UNIQUE env var\n  to enable stale lock killing. If set, stale locks in both cache and\n  repository are deleted. #562 #1253\n- borg info <repo>: print general repo information, #1680\n- borg check --first / --last / --sort / --prefix, #1663\n- borg mount --first / --last / --sort / --prefix, #1542\n- implement \"health\" item formatter key, #1749\n- BORG_SECURITY_DIR to remember security related infos outside the cache.\n  Key type, location and manifest timestamp checks now survive cache\n  deletion. This also means that you can now delete your cache and avoid\n  previous warnings, since Borg can still tell it's safe.\n- implement BORG_NEW_PASSPHRASE, #1768\n\nOther changes:\n\n- borg recreate:\n\n  - remove special-cased --dry-run\n  - update --help\n  - remove bloat: interruption blah, autocommit blah, resuming blah\n  - re-use existing checkpoint functionality\n  - archiver tests: add check_cache tool - lints refcounts\n\n- fixed cache sync performance regression from 1.1.0b1 onwards, #1940\n- syncing the cache without chunks.archive.d (see :ref:`disable_archive_chunks`)\n  now avoids any merges and is thus faster, #1940\n- borg check --verify-data: faster due to linear on-disk-order scan\n- borg debug-xxx commands removed, we use \"debug xxx\" subcommands now, #1627\n- improve metadata handling speed\n- shortcut hashindex_set by having hashindex_lookup hint about address\n- improve / add progress displays, #1721\n- check for index vs. segment files object count mismatch\n- make RPC protocol more extensible: use named parameters.\n- RemoteRepository: misc. code cleanups / refactors\n- clarify cache/repository README file\n\n- docs:\n\n  - quickstart: add a comment about other (remote) filesystems\n  - quickstart: only give one possible ssh url syntax, all others are\n    documented in usage chapter.\n  - mention file://\n  - document repo URLs / archive location\n  - clarify borg diff help, #980\n  - deployment: synthesize alternative --restrict-to-path example\n  - improve cache / index docs, esp. files cache docs, #1825\n  - document using \"git merge 1.0-maint -s recursive -X rename-threshold=20%\"\n    for avoiding troubles when merging the 1.0-maint branch into master.\n\n- tests:\n\n  - FUSE tests: catch ENOTSUP on freebsd\n  - FUSE tests: test troublesome xattrs last\n  - fix byte range error in test, #1740\n  - use monkeypatch to set env vars, but only on pytest based tests.\n  - point XDG_*_HOME to temp dirs for tests, #1714\n  - remove all BORG_* env vars from the outer environment\n\n\nVersion 1.1.0b2 (2016-10-01)\n----------------------------\n\nBug fixes:\n\n- fix incorrect preservation of delete tags, leading to \"object count mismatch\"\n  on borg check, #1598. This only occurred with 1.1.0b1 (not with 1.0.x) and is\n  normally fixed by running another borg create/delete/prune.\n- fix broken --progress for double-cell paths (e.g. CJK), #1624\n- borg recreate: also catch SIGHUP\n- FUSE:\n\n  - fix hardlinks in versions view, #1599\n  - add parameter check to ItemCache.get to make potential failures more clear\n\nNew features:\n\n- Archiver, RemoteRepository: add --remote-ratelimit (send data)\n- borg help compression, #1582\n- borg check: delete chunks with integrity errors, #1575, so they can be\n  \"repaired\" immediately and maybe healed later.\n- archives filters concept (refactoring/unifying older code)\n\n  - covers --first/--last/--prefix/--sort-by options\n  - currently used for borg list/info/delete\n\nOther changes:\n\n- borg check --verify-data slightly tuned (use get_many())\n- change {utcnow} and {now} to ISO-8601 format (\"T\" date/time separator)\n- repo check: log transaction IDs, improve object count mismatch diagnostic\n- Vagrantfile: use TW's fresh-bootloader pyinstaller branch\n- fix module names in api.rst\n- hashindex: bump api_version\n\n\nVersion 1.1.0b1 (2016-08-28)\n----------------------------\n\nNew features:\n\n- new commands:\n\n  - borg recreate: re-create existing archives, #787 #686 #630 #70, also see\n    #757, #770.\n\n    - selectively remove files/dirs from old archives\n    - re-compress data\n    - re-chunkify data, e.g. to have upgraded Attic / Borg 0.xx archives\n      deduplicate with Borg 1.x archives or to experiment with chunker-params.\n  - borg diff: show differences between archives\n  - borg with-lock: execute a command with the repository locked, #990\n- borg create:\n\n  - Flexible compression with pattern matching on path/filename,\n    and LZ4 heuristic for deciding compressibility, #810, #1007\n  - visit files in inode order (better speed, esp. for large directories and rotating disks)\n  - in-file checkpoints, #1217\n  - increased default checkpoint interval to 30 minutes (was 5 minutes), #896\n  - added uuid archive format tag, #1151\n  - save mountpoint directories with --one-file-system, makes system restore easier, #1033\n  - Linux: added support for some BSD flags, #1050\n  - add 'x' status for excluded paths, #814\n\n    - also means files excluded via UF_NODUMP, #1080\n- borg check:\n\n  - will not produce the \"Checking segments\" output unless new --progress option is passed, #824.\n  - --verify-data to verify data cryptographically on the client, #975\n- borg list, #751, #1179\n\n  - removed {formatkeys}, see \"borg list --help\"\n  - --list-format is deprecated, use --format instead\n  - --format now also applies to listing archives, not only archive contents, #1179\n  - now supports the usual [PATH [PATHS\u2026]] syntax and excludes\n  - new keys: csize, num_chunks, unique_chunks, NUL\n  - supports guaranteed_available hashlib hashes\n    (to avoid varying functionality depending on environment),\n    which includes the SHA1 and SHA2 family as well as MD5\n- borg prune:\n\n  - to visualize the \"thinning out\" better, we now list all archives in\n    reverse time order. rephrase and reorder help text.\n  - implement --keep-last N via --keep-secondly N, also --keep-minutely.\n    assuming that there is not more than 1 backup archive made in 1s,\n    --keep-last N and --keep-secondly N are equivalent, #537\n  - cleanup checkpoints except the latest, #1008\n- borg extract:\n\n  - added --progress, #1449\n  - Linux: limited support for BSD flags, #1050\n- borg info:\n\n  - output is now more similar to borg create --stats, #977\n- borg mount:\n\n  - provide \"borgfs\" wrapper for borg mount, enables usage via fstab, #743\n  - \"versions\" mount option - when used with a repository mount, this gives\n    a merged, versioned view of the files in all archives, #729\n- repository:\n\n  - added progress information to commit/compaction phase (often takes some time when deleting/pruning), #1519\n  - automatic recovery for some forms of repository inconsistency, #858\n  - check free space before going forward with a commit, #1336\n  - improved write performance (esp. for rotating media), #985\n\n    - new IO code for Linux\n    - raised default segment size to approx 512 MiB\n  - improved compaction performance, #1041\n  - reduced client CPU load and improved performance for remote repositories, #940\n\n- options that imply output (--show-rc, --show-version, --list, --stats,\n  --progress) don't need -v/--info to have that output displayed, #865\n- add archive comments (via borg (re)create --comment), #842\n- borg list/prune/delete: also output archive id, #731\n- --show-version: shows/logs the borg version, #725\n- added --debug-topic for granular debug logging, #1447\n- use atomic file writing/updating for configuration and key files, #1377\n- BORG_KEY_FILE environment variable, #1001\n- self-testing module, #970\n\n\nBug fixes:\n\n- list: fixed default output being produced if --format is given with empty parameter, #1489\n- create: fixed overflowing progress line with CJK and similar characters, #1051\n- prune: fixed crash if --prefix resulted in no matches, #1029\n- init: clean up partial repo if passphrase input is aborted, #850\n- info: quote cmdline arguments that have spaces in them\n- fix hardlinks failing in some cases for extracting subtrees, #761\n\nOther changes:\n\n- replace stdlib hmac with OpenSSL, zero-copy decrypt (10-15% increase in\n  performance of hash-lists and extract).\n- improved chunker performance, #1021\n- open repository segment files in exclusive mode (fail-safe), #1134\n- improved error logging, #1440\n- Source:\n\n  - pass meta-data around, #765\n  - move some constants to new constants module\n  - better readability and fewer errors with namedtuples, #823\n  - moved source tree into src/ subdirectory, #1016\n  - made borg.platform a package, #1113\n  - removed dead crypto code, #1032\n  - improved and ported parts of the test suite to py.test, #912\n  - created data classes instead of passing dictionaries around, #981, #1158, #1161\n  - cleaned up imports, #1112\n- Docs:\n\n  - better help texts and sphinx reproduction of usage help:\n\n    - Group options\n    - Nicer list of options in Sphinx\n    - Deduplicate 'Common options' (including --help)\n  - chunker: added some insights by \"Voltara\", #903\n  - clarify what \"deduplicated size\" means\n  - fix / update / add package list entries\n  - added a SaltStack usage example, #956\n  - expanded FAQ\n  - new contributors in AUTHORS!\n- Tests:\n\n  - vagrant: add ubuntu/xenial 64bit - this box has still some issues\n  - ChunkBuffer: add test for leaving partial chunk in buffer, fixes #945\n\n\nVersion 1.0.13 (2019-02-15)\n---------------------------\n\nPlease note: this is very likely the last 1.0.x release, please upgrade to 1.1.x.\n\nBug fixes:\n\n- security fix: configure FUSE with \"default_permissions\", #3903.\n  \"default_permissions\" is now enforced by borg by default to let the\n  kernel check uid/gid/mode based permissions.\n  \"ignore_permissions\" can be given not to enforce \"default_permissions\".\n- xattrs: fix borg exception handling on ENOSPC error, #3808.\n\nNew features:\n\n- Read a passphrase from a file descriptor specified in the\n  BORG_PASSPHRASE_FD environment variable.\n\nOther changes:\n\n- acl platform code: fix acl set return type\n- xattr:\n\n  - add linux {list,get,set}xattr ctypes prototypes\n  - fix darwin flistxattr ctypes prototype\n- testing / travis-ci:\n\n  - fix the homebrew 1.9 issues on travis-ci, #4254\n  - travis OS X: use xcode 8.3 (not broken)\n  - tox.ini: lock requirements\n  - unbreak 1.0-maint on travis, fixes #4123\n- vagrant:\n\n  - misc. fixes\n  - FUSE for macOS: upgrade 3.7.1 to 3.8.3\n  - Python: upgrade 3.5.5 to 3.5.6\n- docs:\n\n  - Update installation instructions for macOS\n  - update release workflow using twine (docs, scripts), #4213\n\nVersion 1.0.12 (2018-04-08)\n---------------------------\n\nBug fixes:\n\n- repository: cleanup/write: invalidate cached FDs, tests\n- serve: fix exitcode, #2910\n- extract: set bsdflags last (include immutable flag), #3263\n- create --timestamp: set start time, #2957\n- create: show excluded dir with \"x\" for tagged dirs / caches, #3189\n- migrate locks to child PID when daemonize is used\n- Buffer: fix wrong thread-local storage use, #2951\n- fix detection of non-local path, #3108\n- fix LDLP restoration for subprocesses, #3077\n- fix subprocess environments (xattr module's fakeroot version check,\n  borg umount, BORG_PASSCOMMAND), #3050\n- remote: deal with partial lines, #2637\n- get rid of datetime.isoformat, use safe parse_timestamp to parse\n  timestamps, #2994\n- build: do .h file content checks in binary mode, fixes build issue for\n  non-ascii header files on pure-ascii locale platforms, #3544 #3639\n- remove platform.uname() call which caused library mismatch issues, #3732\n- add exception handler around deprecated platform.linux_distribution() call\n\nOther changes:\n\n- require msgpack-python >= 0.4.6 and < 0.5.0, see #3753\n- add parens for C preprocessor macro argument usages (did not cause\n  malfunction)\n- ignore corrupt files cache, #2939\n- replace \"modulo\" with \"if\" to check for wraparound in hashmap\n- keymanager: don't depend on optional readline module, #2980\n- exclude broken pytest 3.3.0 release\n- exclude broken Cython 0.27(.0) release, #3066\n- flake8: add some ignores\n- docs:\n\n  - create: document exclusion through nodump\n  - document good and problematic option placements, fix examples, #3356\n  - update docs about hardlinked symlinks limitation\n  - faq: we do not implement futile attempts of ETA / progress displays\n  - simplified rate limiting wrapper in FAQ\n  - twitter account @borgbackup, #2948\n  - add note about metadata dedup and --no[ac]time, #2518\n  - change-passphrase only changes the passphrase, #2990\n  - clarify encrypted key format for borg key export, #3296\n  - document sshfs rename workaround, #3315\n  - update release checklist about security fixes\n  - docs about how to verify a signed release, #3634\n  - chunk seed is generated per /repository/\n- vagrant:\n\n  - use FUSE for macOS 3.7.1 to build the macOS binary\n  - use python 3.5.5 to build the binaries\n  - add exe location to PATH when we build an exe\n  - use https pypi url for wheezy\n  - netbsd: bash is already installed\n  - netbsd: fix netbsd version in PKG_PATH\n  - use self-made FreeBSD 10.3 box, #3022\n  - backport fs_init (including related updates) from 1.1\n  - the boxcutter wheezy boxes are 404, use local ones\n- travis:\n\n  - don't perform full Travis build on docs-only changes, #2531\n  - only short-circuit docs-only changes for pull requests\n\n\nVersion 1.0.11 (2017-07-21)\n---------------------------\n\nBug fixes:\n\n- use limited unpacker for outer key (security precaution), #2174\n- fix paperkey import bug\n\nOther changes:\n\n- change --checkpoint-interval default from 600s to 1800s, #2841.\n  this improves efficiency for big repositories a lot.\n- docs: fix OpenSUSE command and add OpenSUSE section\n- tests: add tests for split_lstring and paperkey\n- vagrant:\n\n  - fix openbsd shell\n  - backport cpu/ram setup from master\n  - add stretch64 VM\n\nVersion 1.0.11rc1 (2017-06-27)\n------------------------------\n\nBug fixes:\n\n- performance: rebuild hashtable if we have too few empty buckets, #2246.\n  this fixes some sporadic, but severe performance breakdowns.\n- Archive: allocate zeros when needed, #2308\n  fixes huge memory usage of mount (8 MiB \u00d7 number of archives)\n- IPv6 address support\n  also: Location: more informative exception when parsing fails\n- borg single-file binary: use pyinstaller v3.2.1, #2396\n  this fixes that the prelink cronjob on some distros kills the\n  borg binary by stripping away parts of it.\n- extract:\n\n  - warning for unextracted big extended attributes, #2258\n  - also create parent dir for device files, if needed.\n  - don't write to disk with --stdout, #2645\n- archive check: detect and fix missing all-zero replacement chunks, #2180\n- fix (de)compression exceptions, #2224 #2221\n- files cache: update inode number, #2226\n- borg rpc: use limited msgpack.Unpacker (security precaution), #2139\n- Manifest: use limited msgpack.Unpacker (security precaution), #2175\n- Location: accept //servername/share/path\n- fix ChunkIndex.__contains__ assertion  for big-endian archs (harmless)\n- create: handle BackupOSError on a per-path level in one spot\n- fix error msg, there is no --keep-last in borg 1.0.x, #2282\n- clamp (nano)second values to unproblematic range, #2304\n- fuse / borg mount:\n\n  - fix st_blocks to be an integer (not float) value\n  - fix negative uid/gid crash (they could come into archives e.g. when\n    backing up external drives under cygwin), #2674\n  - fix crash if empty (None) xattr is read\n  - do pre-mount checks before opening repository\n  - check llfuse is installed before asking for passphrase\n- borg rename: expand placeholders, #2386\n- borg serve: fix forced command lines containing BORG_* env vars\n- fix error msg, it is --keep-within, not --within\n- fix borg key/debug/benchmark crashing without subcommand, #2240\n- chunker: fix invalid use of types, don't do uint32_t >> 32\n- document follow_symlinks requirements, check libc, #2507\n\nNew features:\n\n- added BORG_PASSCOMMAND environment variable, #2573\n- add minimal version of in repository mandatory feature flags, #2134\n\n  This should allow us to make sure older borg versions can be cleanly\n  prevented from doing operations that are no longer safe because of\n  repository format evolution. This allows more fine grained control than\n  just incrementing the manifest version. So for example a change that\n  still allows new archives to be created but would corrupt the repository\n  when an old version tries to delete an archive or check the repository\n  would add the new feature to the check and delete set but leave it out\n  of the write set.\n- borg delete --force --force to delete severely corrupted archives, #1975\n\nOther changes:\n\n- embrace y2038 issue to support 32bit platforms\n- be more clear that this is a \"beyond repair\" case, #2427\n- key file names: limit to 100 characters and remove colons from host name\n- upgrade FUSE for macOS to 3.5.8, #2346\n- split up parsing and filtering for --keep-within, better error message, #2610\n- docs:\n\n  - fix caskroom link, #2299\n  - address SSH batch mode, #2202 #2270\n  - improve remote-path description\n  - document snapshot usage, #2178\n  - document relative path usage, #1868\n  - one link per distro in the installation page\n  - development: new branching model in git repository\n  - kill api page\n  - added FAQ section about backing up root partition\n  - add bountysource badge, #2558\n  - create empty docs.txt requirements, #2694\n  - README: how to help the project\n  - note -v/--verbose requirement on affected options, #2542\n  - document borg init behaviour via append-only borg serve, #2440\n  - be clear about what buzhash is used for (chunking) and want it is not\n    used for (deduplication)- also say already in the readme that we use a\n    cryptohash for dedupe, so people don't worry, #2390\n  - add hint about chunker params to borg upgrade docs, #2421\n  - clarify borg upgrade docs, #2436\n  - quickstart: delete problematic BORG_PASSPHRASE use, #2623\n  - faq: specify \"using inline shell scripts\"\n  - document pattern denial of service, #2624\n- tests:\n\n  - remove attic dependency of the tests, #2505\n  - travis:\n\n    - enhance travis setuptools_scm situation\n    - install fakeroot for Linux\n  - add test for borg delete --force\n  - enable remote tests on cygwin (the cygwin issue that caused these tests\n    to break was fixed in cygwin at least since cygwin 2.8, maybe even since\n    2.7.0).\n  - remove skipping the noatime tests on GNU/Hurd, #2710\n  - fix borg import issue, add comment, #2718\n  - include attic.tar.gz when installing the package\n    also: add include_package_data=True\n\nVersion 1.0.10 (2017-02-13)\n---------------------------\n\nBug fixes:\n\n- Manifest timestamps are now monotonically increasing,\n  this fixes issues when the system clock jumps backwards\n  or is set inconsistently across computers accessing the same repository, #2115\n- Fixed testing regression in 1.0.10rc1 that lead to a hard dependency on\n  py.test >= 3.0, #2112\n\nNew features:\n\n- \"key export\" can now generate a printable HTML page with both a QR code and\n  a human-readable \"paperkey\" representation (and custom text) through the\n  ``--qr-html`` option.\n\n  The same functionality is also available through `paperkey.html <paperkey.html>`_,\n  which is the same HTML page generated by ``--qr-html``. It works with existing\n  \"key export\" files and key files.\n\nOther changes:\n\n- docs:\n\n  - language clarification - \"borg create --one-file-system\" option does not respect\n    mount points, but considers different file systems instead, #2141\n- setup.py: build_api: sort file list for determinism\n\n\nVersion 1.0.10rc1 (2017-01-29)\n------------------------------\n\nBug fixes:\n\n- borg serve: fix transmission data loss of pipe writes, #1268\n  This affects only the cygwin platform (not Linux, BSD, OS X).\n- Avoid triggering an ObjectiveFS bug in xattr retrieval, #1992\n- When running out of buffer memory when reading xattrs, only skip the\n  current file, #1993\n- Fixed \"borg upgrade --tam\" crashing with unencrypted repositories. Since\n  :ref:`the issue <tam_vuln>` is not relevant for unencrypted repositories,\n  it now does nothing and prints an error, #1981.\n- Fixed change-passphrase crashing with unencrypted repositories, #1978\n- Fixed \"borg check repo::archive\" indicating success if \"archive\" does not exist, #1997\n- borg check: print non-exit-code warning if --last or --prefix aren't fulfilled\n- fix bad parsing of wrong repo location syntax\n- create: don't create hard link refs to failed files,\n  mount: handle invalid hard link refs, #2092\n- detect mingw byte order, #2073\n- creating a new segment: use \"xb\" mode, #2099\n- mount: umount on SIGINT/^C when in foreground, #2082\n\nOther changes:\n\n- binary: use fixed AND freshly compiled pyinstaller bootloader, #2002\n- xattr: ignore empty names returned by llistxattr(2) et al\n- Enable the fault handler: install handlers for the SIGSEGV, SIGFPE, SIGABRT,\n  SIGBUS and SIGILL signals to dump the Python traceback.\n- Also print a traceback on SIGUSR2.\n- borg change-passphrase: print key location (simplify making a backup of it)\n- officially support Python 3.6 (setup.py: add Python 3.6 qualifier)\n- tests:\n\n  - vagrant / travis / tox: add Python 3.6 based testing\n  - vagrant: fix openbsd repo, #2042\n  - vagrant: fix the freebsd64 machine, #2037 #2067\n  - vagrant: use python 3.5.3 to build binaries, #2078\n  - vagrant: use osxfuse 3.5.4 for tests / to build binaries\n    vagrant: improve darwin64 VM settings\n  - travis: fix osxfuse install (fixes OS X testing on Travis CI)\n  - travis: require succeeding OS X tests, #2028\n  - travis: use latest pythons for OS X based testing\n  - use pytest-xdist to parallelize testing\n  - fix xattr test race condition, #2047\n  - setup.cfg: fix pytest deprecation warning, #2050\n- docs:\n\n  - language clarification - VM backup FAQ\n  - borg create: document how to back up stdin, #2013\n  - borg upgrade: fix incorrect title levels\n  - add CVE numbers for issues fixed in 1.0.9, #2106\n- fix typos (taken from Debian package patch)\n- remote: include data hexdump in \"unexpected RPC data\" error message\n- remote: log SSH command line at debug level\n- API_VERSION: use numberspaces, #2023\n- remove .github from pypi package, #2051\n- add pip and setuptools to requirements file, #2030\n- SyncFile: fix use of fd object after close (cosmetic)\n- Manifest.in: simplify, exclude \\*.{so,dll,orig}, #2066\n- ignore posix_fadvise errors in repository.py, #2095\n  (works around issues with docker on ARM)\n- make LoggedIO.close_segment reentrant, avoid reentrance\n\n\nVersion 1.0.9 (2016-12-20)\n--------------------------\n\nSecurity fixes:\n\n- A flaw in the cryptographic authentication scheme in Borg allowed an attacker\n  to spoof the manifest. See :ref:`tam_vuln` above for the steps you should\n  take.\n\n  CVE-2016-10099 was assigned to this vulnerability.\n- borg check: When rebuilding the manifest (which should only be needed very rarely)\n  duplicate archive names would be handled on a \"first come first serve\" basis,\n  potentially opening an attack vector to replace archives.\n\n  Example: were there 2 archives named \"foo\" in a repo (which can not happen\n  under normal circumstances, because borg checks if the name is already used)\n  and a \"borg check\" recreated a (previously lost) manifest, the first of the\n  archives it encountered would be in the manifest. The second archive is also\n  still in the repo, but not referenced in the manifest, in this case. If the\n  second archive is the \"correct\" one (and was previously referenced from the\n  manifest), it looks like it got replaced by the first one. In the manifest,\n  it actually got replaced. Both remain in the repo but the \"correct\" one is no\n  longer accessible via normal means - the manifest.\n\n  CVE-2016-10100 was assigned to this vulnerability.\n\nBug fixes:\n\n- borg check:\n\n  - rebuild manifest if it's corrupted\n  - skip corrupted chunks during manifest rebuild\n- fix TypeError in integrity error handler, #1903, #1894\n- fix location parser for archives with @ char (regression introduced in 1.0.8), #1930\n- fix wrong duration/timestamps if system clock jumped during a create\n- fix progress display not updating if system clock jumps backwards\n- fix checkpoint interval being incorrect if system clock jumps\n\nOther changes:\n\n- docs:\n\n  - add python3-devel as a dependency for cygwin-based installation\n  - clarify extract is relative to current directory\n  - FAQ: fix link to changelog\n  - markup fixes\n- tests:\n\n  - test_get\\_(cache|keys)_dir: clean env state, #1897\n  - get back pytest's pretty assertion failures, #1938\n- setup.py build_usage:\n\n  - fixed build_usage not processing all commands\n  - fixed build_usage not generating includes for debug commands\n\n\nVersion 1.0.9rc1 (2016-11-27)\n-----------------------------\n\nBug fixes:\n\n- files cache: fix determination of newest mtime in backup set (which is\n  used in cache cleanup and led to wrong \"A\" [added] status for unchanged\n  files in next backup), #1860.\n\n- borg check:\n\n  - fix incorrectly reporting attic 0.13 and earlier archives as corrupt\n  - handle repo w/o objects gracefully and also bail out early if repo is\n    *completely* empty, #1815.\n- fix tox/pybuild in 1.0-maint\n- at xattr module import time, loggers are not initialized yet\n\nNew features:\n\n- borg umount <mountpoint>\n  exposed already existing umount code via the CLI api, so users can use it,\n  which is more consistent than using borg to mount and fusermount -u (or\n  umount) to un-mount, #1855.\n- implement borg create --noatime --noctime, fixes #1853\n\nOther changes:\n\n- docs:\n\n  - display README correctly on PyPI\n  - improve cache / index docs, esp. files cache docs, fixes #1825\n  - different pattern matching for --exclude, #1779\n  - datetime formatting examples for {now} placeholder, #1822\n  - clarify passphrase mode attic repo upgrade, #1854\n  - clarify --umask usage, #1859\n  - clarify how to choose PR target branch\n  - clarify prune behavior for different archive contents, #1824\n  - fix PDF issues, add logo, fix authors, headings, TOC\n  - move security verification to support section\n  - fix links in standalone README (:ref: tags)\n  - add link to security contact in README\n  - add FAQ about security\n  - move fork differences to FAQ\n  - add more details about resource usage\n- tests: skip remote tests on cygwin, #1268\n- travis:\n\n  - allow OS X failures until the brew cask osxfuse issue is fixed\n  - caskroom osxfuse-beta gone, it's osxfuse now (3.5.3)\n- vagrant:\n\n  - upgrade OSXfuse / FUSE for macOS to 3.5.3\n  - remove llfuse from tox.ini at a central place\n  - do not try to install llfuse on centos6\n  - fix FUSE test for darwin, #1546\n  - add windows virtual machine with cygwin\n  - Vagrantfile cleanup / code deduplication\n\n\nVersion 1.0.8 (2016-10-29)\n--------------------------\n\nBug fixes:\n\n- RemoteRepository: Fix busy wait in call_many, #940\n\nNew features:\n\n- implement borgmajor/borgminor/borgpatch placeholders, #1694\n  {borgversion} was already there (full version string). With the new\n  placeholders you can now also get e.g. 1 or 1.0 or 1.0.8.\n\nOther changes:\n\n- avoid previous_location mismatch, #1741\n\n  due to the changed canonicalization for relative paths in PR #1711 / #1655\n  (implement /./ relpath hack), there would be a changed repo location warning\n  and the user would be asked if this is ok. this would break automation and\n  require manual intervention, which is unwanted.\n\n  thus, we automatically fix the previous_location config entry, if it only\n  changed in the expected way, but still means the same location.\n\n- docs:\n\n  - deployment.rst: do not use bare variables in ansible snippet\n  - add clarification about append-only mode, #1689\n  - setup.py: add comment about requiring llfuse, #1726\n  - update usage.rst / api.rst\n  - repo url / archive location docs + typo fix\n  - quickstart: add a comment about other (remote) filesystems\n\n- vagrant / tests:\n\n  - no chown when rsyncing (fixes boxes w/o vagrant group)\n  - fix FUSE permission issues on linux/freebsd, #1544\n  - skip FUSE test for borg binary + fakeroot\n  - ignore security.selinux xattrs, fixes tests on centos, #1735\n\n\nVersion 1.0.8rc1 (2016-10-17)\n-----------------------------\n\nBug fixes:\n\n- fix signal handling (SIGINT, SIGTERM, SIGHUP), #1620 #1593\n  Fixes e.g. leftover lock files for quickly repeated signals (e.g. Ctrl-C\n  Ctrl-C) or lost connections or systemd sending SIGHUP.\n- progress display: adapt formatting to narrow screens, do not crash, #1628\n- borg create --read-special - fix crash on broken symlink, #1584.\n  also correctly processes broken symlinks. before this regressed to a crash\n  (5b45385) a broken symlink would've been skipped.\n- process_symlink: fix missing backup_io()\n  Fixes a chmod/chown/chgrp/unlink/rename/... crash race between getting\n  dirents and dispatching to process_symlink.\n- yes(): abort on wrong answers, saying so, #1622\n- fixed exception borg serve raised when connection was closed before repository\n  was opened. Add an error message for this.\n- fix read-from-closed-FD issue, #1551\n  (this seems not to get triggered in 1.0.x, but was discovered in master)\n- hashindex: fix iterators (always raise StopIteration when exhausted)\n  (this seems not to get triggered in 1.0.x, but was discovered in master)\n- enable relative paths in ssh:// repo URLs, via /./relpath hack, #1655\n- allow repo paths with colons, #1705\n- update changed repo location immediately after acceptance, #1524\n- fix debug get-obj / delete-obj crash if object not found and remote repo,\n  #1684\n- pyinstaller: use a spec file to build borg.exe binary, exclude osxfuse dylib\n  on Mac OS X (avoids mismatch lib <-> driver), #1619\n\nNew features:\n\n- add \"borg key export\" / \"borg key import\" commands, #1555, so users are able\n  to back up / restore their encryption keys more easily.\n\n  Supported formats are the keyfile format used by borg internally and a\n  special \"paper\" format with by line checksums for printed backups. For the\n  paper format, the import is an interactive process which checks each line as\n  soon as it is input.\n- add \"borg debug-refcount-obj\" to determine a repo objects' referrer counts,\n  #1352\n\nOther changes:\n\n- add \"borg debug ...\" subcommands\n  (borg debug-* still works, but will be removed in borg 1.1)\n- setup.py: Add subcommand support to build_usage.\n- remote: change exception message for unexpected RPC data format to indicate\n  dataflow direction.\n- improved messages / error reporting:\n\n  - IntegrityError: add placeholder for message, so that the message we give\n    appears not only in the traceback, but also in the (short) error message,\n    #1572\n  - borg.key: include chunk id in exception msgs, #1571\n  - better messages for cache newer than repo, #1700\n- vagrant (testing/build VMs):\n\n  - upgrade OSXfuse / FUSE for macOS to 3.5.2\n  - update Debian Wheezy boxes, #1686\n  - openbsd / netbsd: use own boxes, fixes misc rsync installation and\n    FUSE/llfuse related testing issues, #1695 #1696 #1670 #1671 #1728\n- docs:\n\n  - add docs for \"key export\" and \"key import\" commands, #1641\n  - fix inconsistency in FAQ (pv-wrapper).\n  - fix second block in \"Easy to use\" section not showing on GitHub, #1576\n  - add bestpractices badge\n  - link reference docs and faq about BORG_FILES_CACHE_TTL, #1561\n  - improve borg info --help, explain size infos, #1532\n  - add release signing key / security contact to README, #1560\n  - add contribution guidelines for developers\n  - development.rst: add sphinx_rtd_theme to the sphinx install command\n  - adjust border color in borg.css\n  - add debug-info usage help file\n  - internals.rst: fix typos\n  - setup.py: fix build_usage to always process all commands\n  - added docs explaining multiple --restrict-to-path flags, #1602\n  - add more specific warning about write-access debug commands, #1587\n  - clarify FAQ regarding backup of virtual machines, #1672\n- tests:\n\n  - work around FUSE xattr test issue with recent fakeroot\n  - simplify repo/hashindex tests\n  - travis: test FUSE-enabled borg, use trusty to have a recent FUSE\n  - re-enable FUSE tests for RemoteArchiver (no deadlocks any more)\n  - clean env for pytest based tests, #1714\n  - fuse_mount contextmanager: accept any options\n\n\nVersion 1.0.7 (2016-08-19)\n--------------------------\n\nSecurity fixes:\n\n- borg serve: fix security issue with remote repository access, #1428\n  If you used e.g. --restrict-to-path /path/client1/ (with or without trailing\n  slash does not make a difference), it acted like a path prefix match using\n  /path/client1 (note the missing trailing slash) - the code then also allowed\n  working in e.g. /path/client13 or /path/client1000.\n\n  As this could accidentally lead to major security/privacy issues depending on\n  the paths you use, the behaviour was changed to be a strict directory match.\n  That means --restrict-to-path /path/client1 (with or without trailing slash\n  does not make a difference) now uses /path/client1/ internally (note the\n  trailing slash here!) for matching and allows precisely that path AND any\n  path below it. So, /path/client1 is allowed, /path/client1/repo1 is allowed,\n  but not /path/client13 or /path/client1000.\n\n  If you willingly used the undocumented (dangerous) previous behaviour, you\n  may need to rearrange your --restrict-to-path paths now. We are sorry if\n  that causes work for you, but we did not want a potentially dangerous\n  behaviour in the software (not even using a for-backwards-compat option).\n\nBug fixes:\n\n- fixed repeated LockTimeout exceptions when borg serve tried to write into\n  a already write-locked repo (e.g. by a borg mount), #502 part b)\n  This was solved by the fix for #1220 in 1.0.7rc1 already.\n- fix cosmetics + file leftover for \"not a valid borg repository\", #1490\n- Cache: release lock if cache is invalid, #1501\n- borg extract --strip-components: fix leak of preloaded chunk contents\n- Repository, when a InvalidRepository exception happens:\n\n  - fix spurious, empty lock.roster\n  - fix repo not closed cleanly\n\nNew features:\n\n- implement borg debug-info, fixes #1122\n  (just calls already existing code via cli, same output as below tracebacks)\n\nOther changes:\n\n- skip the O_NOATIME test on GNU Hurd, fixes #1315\n  (this is a very minor issue and the GNU Hurd project knows the bug)\n- document using a clean repo to test / build the release\n\n\nVersion 1.0.7rc2 (2016-08-13)\n-----------------------------\n\nBug fixes:\n\n- do not write objects to repository that are bigger than the allowed size,\n  borg will reject reading them, #1451.\n\n  Important: if you created archives with many millions of files or\n  directories, please verify if you can open them successfully,\n  e.g. try a \"borg list REPO::ARCHIVE\".\n- lz4 compression: dynamically enlarge the (de)compression buffer, the static\n  buffer was not big enough for archives with extremely many items, #1453\n- larger item metadata stream chunks, raise archive item limit by 8x, #1452\n- fix untracked segments made by moved DELETEs, #1442\n\n  Impact: Previously (metadata) segments could become untracked when deleting data,\n  these would never be cleaned up.\n- extended attributes (xattrs) related fixes:\n\n  - fixed a race condition in xattrs querying that led to the entire file not\n    being backed up (while logging the error, exit code = 1), #1469\n  - fixed a race condition in xattrs querying that led to a crash, #1462\n  - raise OSError including the error message derived from errno, deal with\n    path being a integer FD\n\nOther changes:\n\n- print active env var override by default, #1467\n- xattr module: refactor code, deduplicate, clean up\n- repository: split object size check into too small and too big\n- add a transaction_id assertion, so borg init on a broken (inconsistent)\n  filesystem does not look like a coding error in borg, but points to the\n  real problem.\n- explain confusing TypeError caused by compat support for old servers, #1456\n- add forgotten usage help file from build_usage\n- refactor/unify buffer code into helpers.Buffer class, add tests\n- docs:\n\n  - document archive limitation, #1452\n  - improve prune examples\n\n\nVersion 1.0.7rc1 (2016-08-05)\n-----------------------------\n\nBug fixes:\n\n- fix repo lock deadlocks (related to lock upgrade), #1220\n- catch unpacker exceptions, resync, #1351\n- fix borg break-lock ignoring BORG_REPO env var, #1324\n- files cache performance fixes (fixes unnecessary re-reading/chunking/\n  hashing of unmodified files for some use cases):\n\n  - fix unintended file cache eviction, #1430\n  - implement BORG_FILES_CACHE_TTL, update FAQ, raise default TTL from 10\n    to 20, #1338\n- FUSE:\n\n  - cache partially read data chunks (performance), #965, #966\n  - always create a root dir, #1125\n- use an OrderedDict for helptext, making the build reproducible, #1346\n- RemoteRepository init: always call close on exceptions, #1370 (cosmetic)\n- ignore stdout/stderr broken pipe errors (cosmetic), #1116\n\nNew features:\n\n- better borg versions management support (useful esp. for borg servers\n  wanting to offer multiple borg versions and for clients wanting to choose\n  a specific server borg version), #1392:\n\n  - add BORG_VERSION environment variable before executing \"borg serve\" via ssh\n  - add new placeholder {borgversion}\n  - substitute placeholders in --remote-path\n\n- borg init --append-only option (makes using the more secure append-only mode\n  more convenient. when used remotely, this requires 1.0.7+ also on the borg\n  server), #1291.\n\nOther changes:\n\n- Vagrantfile:\n\n  - darwin64: upgrade to FUSE for macOS 3.4.1 (aka osxfuse), #1378\n  - xenial64: use user \"ubuntu\", not \"vagrant\" (as usual), #1331\n- tests:\n\n  - fix FUSE tests on OS X, #1433\n- docs:\n\n  - FAQ: add backup using stable filesystem names recommendation\n  - FAQ about glibc compatibility added, #491, glibc-check improved\n  - FAQ: 'A' unchanged file; remove ambiguous entry age sentence.\n  - OS X: install pkg-config to build with FUSE support, fixes #1400\n  - add notes about shell/sudo pitfalls with env. vars, #1380\n  - added platform feature matrix\n- implement borg debug-dump-repo-objs\n\n\nVersion 1.0.6 (2016-07-12)\n--------------------------\n\nBug fixes:\n\n- Linux: handle multiple LD_PRELOAD entries correctly, #1314, #1111\n- Fix crash with unclear message if the libc is not found, #1314, #1111\n\nOther changes:\n\n- tests:\n\n  - Fixed O_NOATIME tests for Solaris and GNU Hurd, #1315\n  - Fixed sparse file tests for (file) systems not supporting it, #1310\n- docs:\n\n  - Fixed syntax highlighting, #1313\n  - misc docs: added data processing overview picture\n\n\nVersion 1.0.6rc1 (2016-07-10)\n-----------------------------\n\nNew features:\n\n- borg check --repair: heal damaged files if missing chunks re-appear (e.g. if\n  the previously missing chunk was added again in a later backup archive),\n  #148. (*) Also improved logging.\n\nBug fixes:\n\n- sync_dir: silence fsync() failing with EINVAL, #1287\n  Some network filesystems (like smbfs) don't support this and we use this in\n  repository code.\n- borg mount (FUSE):\n\n  - fix directories being shadowed when contained paths were also specified,\n    #1295\n  - raise I/O Error (EIO) on damaged files (unless -o allow_damaged_files is\n    used), #1302. (*)\n- borg extract: warn if a damaged file is extracted, #1299. (*)\n- Added some missing return code checks (ChunkIndex._add, hashindex_resize).\n- borg check: fix/optimize initial hash table size, avoids resize of the table.\n\nOther changes:\n\n- tests:\n\n  - add more FUSE tests, #1284\n  - deduplicate FUSE (u)mount code\n  - fix borg binary test issues, #862\n- docs:\n\n  - changelog: added release dates to older borg releases\n  - fix some sphinx (docs generator) warnings, #881\n\nNotes:\n\n(*) Some features depend on information (chunks_healthy list) added to item\nmetadata when a file with missing chunks was \"repaired\" using all-zero\nreplacement chunks. The chunks_healthy list is generated since borg 1.0.4,\nthus borg can't recognize such \"repaired\" (but content-damaged) files if the\nrepair was done with an older borg version.\n\n\nVersion 1.0.5 (2016-07-07)\n--------------------------\n\nBug fixes:\n\n- borg mount: fix FUSE crash in xattr code on Linux introduced in 1.0.4, #1282\n\nOther changes:\n\n- backport some FAQ entries from master branch\n- add release helper scripts\n- Vagrantfile:\n\n  - centos6: no FUSE, don't build binary\n  - add xz for redhat-like dists\n\n\nVersion 1.0.4 (2016-07-07)\n--------------------------\n\nNew features:\n\n- borg serve --append-only, #1168\n  This was included because it was a simple change (append-only functionality\n  was already present via repository config file) and makes better security now\n  practically usable.\n- BORG_REMOTE_PATH environment variable, #1258\n  This was included because it was a simple change (--remote-path cli option\n  was already present) and makes borg much easier to use if you need it.\n- Repository: cleanup incomplete transaction on \"no space left\" condition.\n  In many cases, this can avoid a 100% full repo filesystem (which is very\n  problematic as borg always needs free space - even to delete archives).\n\nBug fixes:\n\n- Fix wrong handling and reporting of OSErrors in borg create, #1138.\n  This was a serious issue: in the context of \"borg create\", errors like\n  repository I/O errors (e.g. disk I/O errors, ssh repo connection errors)\n  were handled badly and did not lead to a crash (which would be good for this\n  case, because the repo transaction would be incomplete and trigger a\n  transaction rollback to clean up).\n  Now, error handling for source files is cleanly separated from every other\n  error handling, so only problematic input files are logged and skipped.\n- Implement fail-safe error handling for borg extract.\n  Note that this isn't nearly as critical as the borg create error handling\n  bug, since nothing is written to the repo. So this was \"merely\" misleading\n  error reporting.\n- Add missing error handler in directory attr restore loop.\n- repo: make sure write data hits disk before the commit tag (#1236) and also\n  sync the containing directory.\n- FUSE: getxattr fail must use errno.ENOATTR, #1126\n  (fixes Mac OS X Finder malfunction: \"zero bytes\" file length, access denied)\n- borg check --repair: do not lose information about the good/original chunks.\n  If we do not lose the original chunk IDs list when \"repairing\" a file\n  (replacing missing chunks with all-zero chunks), we have a chance to \"heal\"\n  the file back into its original state later, in case the chunks re-appear\n  (e.g. in a fresh backup). Healing is not implemented yet, see #148.\n- fixes for --read-special mode:\n\n  - ignore known files cache, #1241\n  - fake regular file mode, #1214\n  - improve symlinks handling, #1215\n- remove passphrase from subprocess environment, #1105\n- Ignore empty index file (will trigger index rebuild), #1195\n- add missing placeholder support for --prefix, #1027\n- improve exception handling for placeholder replacement\n- catch and format exceptions in arg parsing\n- helpers: fix \"undefined name 'e'\" in exception handler\n- better error handling for missing repo manifest, #1043\n- borg delete:\n\n  - make it possible to delete a repo without manifest\n  - borg delete --forced allows one to delete corrupted archives, #1139\n- borg check:\n\n  - make borg check work for empty repo\n  - fix resync and msgpacked item qualifier, #1135\n  - rebuild_manifest: fix crash if 'name' or 'time' key were missing.\n  - better validation of item metadata dicts, #1130\n  - better validation of archive metadata dicts\n- close the repo on exit - even if rollback did not work, #1197.\n  This is rather cosmetic, it avoids repo closing in the destructor.\n\n- tests:\n\n  - fix sparse file test, #1170\n  - flake8: ignore new F405, #1185\n  - catch \"invalid argument\" on cygwin, #257\n  - fix sparseness assertion in test prep, #1264\n\nOther changes:\n\n- make borg build/work on OpenSSL 1.0 and 1.1, #1187\n- docs / help:\n\n  - fix / clarify prune help, #1143\n  - fix \"patterns\" help formatting\n  - add missing docs / help about placeholders\n  - resources: rename atticmatic to borgmatic\n  - document sshd settings, #545\n  - more details about checkpoints, add split trick, #1171\n  - support docs: add freenode web chat link, #1175\n  - add prune visualization / example, #723\n  - add note that Fnmatch is default, #1247\n  - make clear that lzma levels > 6 are a waste of cpu cycles\n  - add a \"do not edit\" note to auto-generated files, #1250\n  - update cygwin installation docs\n- repository interoperability with borg master (1.1dev) branch:\n\n  - borg check: read item metadata keys from manifest, #1147\n  - read v2 hints files, #1235\n  - fix hints file \"unknown version\" error handling bug\n- tests: add tests for format_line\n- llfuse: update version requirement for freebsd\n- Vagrantfile:\n\n  - use openbsd 5.9, #716\n  - do not install llfuse on netbsd (broken)\n  - update OSXfuse to version 3.3.3\n  - use Python 3.5.2 to build the binaries\n- glibc compatibility checker: scripts/glibc_check.py\n- add .eggs to .gitignore\n\n\nVersion 1.0.3 (2016-05-20)\n--------------------------\n\nBug fixes:\n\n- prune: avoid that checkpoints are kept and completed archives are deleted in\n  a prune run), #997\n- prune: fix commandline argument validation - some valid command lines were\n  considered invalid (annoying, but harmless), #942\n- fix capabilities extraction on Linux (set xattrs last, after chown()), #1069\n- repository: fix commit tags being seen in data\n- when probing key files, do binary reads. avoids crash when non-borg binary\n  files are located in borg's key files directory.\n- handle SIGTERM and make a clean exit - avoids orphan lock files.\n- repository cache: don't cache large objects (avoid using lots of temp. disk\n  space), #1063\n\nOther changes:\n\n- Vagrantfile: OS X: update osxfuse / install lzma package, #933\n- setup.py: add check for platform_darwin.c\n- setup.py: on freebsd, use a llfuse release that builds ok\n- docs / help:\n\n  - update readthedocs URLs, #991\n  - add missing docs for \"borg break-lock\", #992\n  - borg create help: add some words to about the archive name\n  - borg create help: document format tags, #894\n\n\nVersion 1.0.2 (2016-04-16)\n--------------------------\n\nBug fixes:\n\n- fix malfunction and potential corruption on (nowadays rather rare) big-endian\n  architectures or bi-endian archs in (rare) BE mode. #886, #889\n\n  cache resync / index merge was malfunctioning due to this, potentially\n  leading to data loss. borg info had cosmetic issues (displayed wrong values).\n\n  note: all (widespread) little-endian archs (like x86/x64) or bi-endian archs\n  in (widespread) LE mode (like ARMEL, MIPSEL, ...) were NOT affected.\n- add overflow and range checks for 1st (special) uint32 of the hashindex\n  values, switch from int32 to uint32.\n- fix so that refcount will never overflow, but just stick to max. value after\n  a overflow would have occurred.\n- borg delete: fix --cache-only for broken caches, #874\n\n  Makes --cache-only idempotent: it won't fail if the cache is already deleted.\n- fixed borg create --one-file-system erroneously traversing into other\n  filesystems (if starting fs device number was 0), #873\n- workaround a bug in Linux fadvise FADV_DONTNEED, #907\n\nOther changes:\n\n- better test coverage for hashindex, incl. overflow testing, checking correct\n  computations so endianness issues would be discovered.\n- reproducible doc for ProgressIndicator*,  make the build reproducible.\n- use latest llfuse for vagrant machines\n- docs:\n\n  - use /path/to/repo in examples, fixes #901\n  - fix confusing usage of \"repo\" as archive name (use \"arch\")\n\n\nVersion 1.0.1 (2016-04-08)\n--------------------------\n\nNew features:\n\nUsually there are no new features in a bugfix release, but these were added\ndue to their high impact on security/safety/speed or because they are fixes\nalso:\n\n- append-only mode for repositories, #809, #36 (see docs)\n- borg create: add --ignore-inode option to make borg detect unmodified files\n  even if your filesystem does not have stable inode numbers (like sshfs and\n  possibly CIFS).\n- add options --warning, --error, --critical for missing log levels, #826.\n  it's not recommended to suppress warnings or errors, but the user may decide\n  this on his own.\n  note: --warning is not given to borg serve so a <= 1.0.0 borg will still\n  work as server (it is not needed as it is the default).\n  do not use --error or --critical when using a <= 1.0.0 borg server.\n\nBug fixes:\n\n- fix silently skipping EIO, #748\n- add context manager for Repository (avoid orphan repository locks), #285\n- do not sleep for >60s while waiting for lock, #773\n- unpack file stats before passing to FUSE\n- fix build on illumos\n- don't try to back up doors or event ports (Solaris and derivatives)\n- remove useless/misleading libc version display, #738\n- test suite: reset exit code of persistent archiver, #844\n- RemoteRepository: clean up pipe if remote open() fails\n- Remote: don't print tracebacks for Error exceptions handled downstream, #792\n- if BORG_PASSPHRASE is present but wrong, don't prompt for password, but fail\n  instead, #791\n- ArchiveChecker: move \"orphaned objects check skipped\" to INFO log level, #826\n- fix capitalization, add ellipses, change log level to debug for 2 messages,\n  #798\n\nOther changes:\n\n- update llfuse requirement, llfuse 1.0 works\n- update OS / dist packages on build machines, #717\n- prefer showing --info over -v in usage help, #859\n- docs:\n\n  - fix cygwin requirements (gcc-g++)\n  - document how to debug / file filesystem issues, #664\n  - fix reproducible build of api docs\n  - RTD theme: CSS !important overwrite, #727\n  - Document logo font. Recreate logo png. Remove GIMP logo file.\n\n\nVersion 1.0.0 (2016-03-05)\n--------------------------\n\nThe major release number change (0.x -> 1.x) indicates bigger incompatible\nchanges, please read the compatibility notes, adapt / test your scripts and\ncheck your backup logs.\n\nCompatibility notes:\n\n- drop support for python 3.2 and 3.3, require 3.4 or 3.5, #221 #65 #490\n  note: we provide binaries that include python 3.5.1 and everything else\n  needed. they are an option in case you are stuck with < 3.4 otherwise.\n- change encryption to be on by default (using \"repokey\" mode)\n- moved keyfile keys from ~/.borg/keys to ~/.config/borg/keys,\n  you can either move them manually or run \"borg upgrade <REPO>\"\n- remove support for --encryption=passphrase,\n  use borg migrate-to-repokey to switch to repokey mode, #97\n- remove deprecated --compression <number>,\n  use --compression zlib,<number> instead\n  in case of 0, you could also use --compression none\n- remove deprecated --hourly/daily/weekly/monthly/yearly\n  use --keep-hourly/daily/weekly/monthly/yearly instead\n- remove deprecated --do-not-cross-mountpoints,\n  use --one-file-system instead\n- disambiguate -p option, #563:\n\n  - -p now is same as --progress\n  - -P now is same as --prefix\n- remove deprecated \"borg verify\",\n  use \"borg extract --dry-run\" instead\n- cleanup environment variable semantics, #355\n  the environment variables used to be \"yes sayers\" when set, this was\n  conceptually generalized to \"automatic answerers\" and they just give their\n  value as answer (as if you typed in that value when being asked).\n  See the \"usage\" / \"Environment Variables\" section of the docs for details.\n- change the builtin default for --chunker-params, create 2MiB chunks, #343\n  --chunker-params new default: 19,23,21,4095 - old default: 10,23,16,4095\n\n  one of the biggest issues with borg < 1.0 (and also attic) was that it had a\n  default target chunk size of 64kiB, thus it created a lot of chunks and thus\n  also a huge chunk management overhead (high RAM and disk usage).\n\n  please note that the new default won't change the chunks that you already\n  have in your repository. the new big chunks do not deduplicate with the old\n  small chunks, so expect your repo to grow at least by the size of every\n  changed file and in the worst case (e.g. if your files cache was lost / is\n  not used) by the size of every file (minus any compression you might use).\n\n  in case you want to see a much lower resource usage immediately (RAM / disk)\n  for chunks management, it might be better to start with a new repo than\n  to continue in the existing repo (with an existing repo, you have to wait\n  until all archives with small chunks get pruned to see a lower resource\n  usage).\n\n  if you used the old --chunker-params default value (or if you did not use\n  --chunker-params option at all) and you'd like to continue using small\n  chunks (and you accept the huge resource usage that comes with that), just\n  use explicitly borg create --chunker-params=10,23,16,4095.\n- archive timestamps: the 'time' timestamp now refers to archive creation\n  start time (was: end time), the new 'time_end' timestamp refers to archive\n  creation end time. This might affect prune if your backups take a long time.\n  if you give a timestamp via cli, this is stored into 'time'. therefore it now\n  needs to mean archive creation start time.\n\nNew features:\n\n- implement password roundtrip, #695\n\nBug fixes:\n\n- remote end does not need cache nor keys directories, do not create them, #701\n- added retry counter for passwords, #703\n\nOther changes:\n\n- fix compiler warnings, #697\n- docs:\n\n  - update README.rst to new changelog location in docs/changes.rst\n  - add Teemu to AUTHORS\n  - changes.rst: fix old chunker params, #698\n  - FAQ: how to limit bandwidth\n\n\nVersion 1.0.0rc2 (2016-02-28)\n-----------------------------\n\nNew features:\n\n- format options for location: user, pid, fqdn, hostname, now, utcnow, user\n- borg list --list-format\n- borg prune -v --list enables the keep/prune list output, #658\n\nBug fixes:\n\n- fix _open_rb noatime handling, #657\n- add a simple archivename validator, #680\n- borg create --stats: show timestamps in localtime, use same labels/formatting\n  as borg info, #651\n- llfuse compatibility fixes (now compatible with: 0.40, 0.41, 0.42)\n\nOther changes:\n\n- it is now possible to use \"pip install borgbackup[fuse]\" to\n  install the llfuse dependency automatically, using the correct version requirement\n  for it. you still need to care about having installed the FUSE / build\n  related OS package first, though, so that building llfuse can succeed.\n- Vagrant: drop Ubuntu Precise (12.04) - does not have Python >= 3.4\n- Vagrant: use pyinstaller v3.1.1 to build binaries\n- docs:\n\n  - borg upgrade: add to docs that only LOCAL repos are supported\n  - borg upgrade also handles borg 0.xx -> 1.0\n  - use pip extras or requirements file to install llfuse\n  - fix order in release process\n  - updated usage docs and other minor / cosmetic fixes\n  - verified borg examples in docs, #644\n  - freebsd dependency installation and FUSE configuration, #649\n  - add example how to restore a raw device, #671\n  - add a hint about the dev headers needed when installing from source\n  - add examples for delete (and handle delete after list, before prune), #656\n  - update example for borg create -v --stats (use iso datetime format), #663\n  - added example to BORG_RSH docs\n  - \"connection closed by remote\": add FAQ entry and point to issue #636\n\n\nVersion 1.0.0rc1 (2016-02-07)\n-----------------------------\n\nNew features:\n\n- borg migrate-to-repokey (\"passphrase\" -> \"repokey\" encryption key mode)\n- implement --short for borg list REPO, #611\n- implement --list for borg extract (consistency with borg create)\n- borg serve: overwrite client's --restrict-to-path with ssh forced command's\n  option value (but keep everything else from the client commandline), #544\n- use $XDG_CONFIG_HOME/keys for keyfile keys (~/.config/borg/keys), #515\n- \"borg upgrade\" moves the keyfile keys to the new location\n- display both archive creation start and end time in \"borg info\", #627\n\n\nBug fixes:\n\n- normalize trailing slashes for the repository path, #606\n- Cache: fix exception handling in __init__, release lock, #610\n\nOther changes:\n\n- suppress unneeded exception context (PEP 409), simpler tracebacks\n- removed special code needed to deal with imperfections / incompatibilities /\n  missing stuff in py 3.2/3.3, simplify code that can be done simpler in 3.4\n- removed some version requirements that were kept on old versions because\n  newer did not support py 3.2 any more\n- use some py 3.4+ stdlib code instead of own/openssl/pypi code:\n\n  - use os.urandom instead of own cython openssl RAND_bytes wrapper, #493\n  - use hashlib.pbkdf2_hmac from py stdlib instead of own openssl wrapper\n  - use hmac.compare_digest instead of == operator (constant time comparison)\n  - use stat.filemode instead of homegrown code\n  - use \"mock\" library from stdlib, #145\n  - remove borg.support (with non-broken argparse copy), it is ok in 3.4+, #358\n- Vagrant: copy CHANGES.rst as symlink, #592\n- cosmetic code cleanups, add flake8 to tox/travis, #4\n- docs / help:\n\n  - make \"borg -h\" output prettier, #591\n  - slightly rephrase prune help\n  - add missing example for --list option of borg create\n  - quote exclude line that includes an asterisk to prevent shell expansion\n  - fix dead link to license\n  - delete Ubuntu Vivid, it is not supported anymore (EOL)\n  - OS X binary does not work for older OS X releases, #629\n  - borg serve's special support for forced/original ssh commands, #544\n  - misc. updates and fixes\n", "import base64\nimport json\nimport os\nimport stat\nimport sys\nimport time\nfrom collections import OrderedDict, defaultdict\nfrom contextlib import contextmanager\nfrom datetime import timedelta\nfrom functools import partial\nfrom getpass import getuser\nfrom io import BytesIO\nfrom itertools import groupby, zip_longest\nfrom typing import Iterator\nfrom shutil import get_terminal_size\n\nfrom .platformflags import is_win32\nfrom .logger import create_logger\n\nlogger = create_logger()\n\nfrom . import xattr\nfrom .chunker import get_chunker, Chunk\nfrom .cache import ChunkListEntry\nfrom .crypto.key import key_factory, UnsupportedPayloadError\nfrom .compress import Compressor, CompressionSpec\nfrom .constants import *  # NOQA\nfrom .crypto.low_level import IntegrityError as IntegrityErrorBase\nfrom .hashindex import ChunkIndex, ChunkIndexEntry, CacheSynchronizer\nfrom .helpers import HardLinkManager\nfrom .helpers import ChunkIteratorFileWrapper, open_item\nfrom .helpers import Error, IntegrityError, set_ec\nfrom .platform import uid2user, user2uid, gid2group, group2gid\nfrom .helpers import parse_timestamp, archive_ts_now\nfrom .helpers import OutputTimestamp, format_timedelta, format_file_size, file_status, FileSize\nfrom .helpers import safe_encode, make_path_safe, remove_surrogates, text_to_json, join_cmd, remove_dotdot_prefixes\nfrom .helpers import StableDict\nfrom .helpers import bin_to_hex\nfrom .helpers import safe_ns\nfrom .helpers import ellipsis_truncate, ProgressIndicatorPercent, log_multi\nfrom .helpers import os_open, flags_normal, flags_dir\nfrom .helpers import os_stat\nfrom .helpers import msgpack\nfrom .helpers import sig_int\nfrom .helpers.lrucache import LRUCache\nfrom .manifest import Manifest\nfrom .patterns import PathPrefixPattern, FnmatchPattern, IECommand\nfrom .item import Item, ArchiveItem, ItemDiff\nfrom .platform import acl_get, acl_set, set_flags, get_flags, swidth, hostname\nfrom .remote import cache_if_remote\nfrom .repository import Repository, LIST_SCAN_LIMIT\nfrom .repoobj import RepoObj\n\nhas_link = hasattr(os, \"link\")\n\n\nclass Statistics:\n    def __init__(self, output_json=False, iec=False):\n        self.output_json = output_json\n        self.iec = iec\n        self.osize = self.usize = self.nfiles = 0\n        self.last_progress = 0  # timestamp when last progress was shown\n        self.files_stats = defaultdict(int)\n        self.chunking_time = 0.0\n        self.hashing_time = 0.0\n        self.rx_bytes = 0\n        self.tx_bytes = 0\n\n    def update(self, size, unique):\n        self.osize += size\n        if unique:\n            self.usize += size\n\n    def __add__(self, other):\n        if not isinstance(other, Statistics):\n            raise TypeError(\"can only add Statistics objects\")\n        stats = Statistics(self.output_json, self.iec)\n        stats.osize = self.osize + other.osize\n        stats.usize = self.usize + other.usize\n        stats.nfiles = self.nfiles + other.nfiles\n        stats.chunking_time = self.chunking_time + other.chunking_time\n        stats.hashing_time = self.hashing_time + other.hashing_time\n        st1, st2 = self.files_stats, other.files_stats\n        stats.files_stats = defaultdict(int, {key: (st1[key] + st2[key]) for key in st1.keys() | st2.keys()})\n\n        return stats\n\n    def __str__(self):\n        hashing_time = format_timedelta(timedelta(seconds=self.hashing_time))\n        chunking_time = format_timedelta(timedelta(seconds=self.chunking_time))\n        return \"\"\"\\\nNumber of files: {stats.nfiles}\nOriginal size: {stats.osize_fmt}\nDeduplicated size: {stats.usize_fmt}\nTime spent in hashing: {hashing_time}\nTime spent in chunking: {chunking_time}\nAdded files: {added_files}\nUnchanged files: {unchanged_files}\nModified files: {modified_files}\nError files: {error_files}\nFiles changed while reading: {files_changed_while_reading}\nBytes read from remote: {stats.rx_bytes}\nBytes sent to remote: {stats.tx_bytes}\n\"\"\".format(\n            stats=self,\n            hashing_time=hashing_time,\n            chunking_time=chunking_time,\n            added_files=self.files_stats[\"A\"],\n            unchanged_files=self.files_stats[\"U\"],\n            modified_files=self.files_stats[\"M\"],\n            error_files=self.files_stats[\"E\"],\n            files_changed_while_reading=self.files_stats[\"C\"],\n        )\n\n    def __repr__(self):\n        return \"<{cls} object at {hash:#x} ({self.osize}, {self.usize})>\".format(\n            cls=type(self).__name__, hash=id(self), self=self\n        )\n\n    def as_dict(self):\n        return {\n            \"original_size\": FileSize(self.osize, iec=self.iec),\n            \"deduplicated_size\": FileSize(self.usize, iec=self.iec),\n            \"nfiles\": self.nfiles,\n            \"hashing_time\": self.hashing_time,\n            \"chunking_time\": self.chunking_time,\n            \"files_stats\": self.files_stats,\n        }\n\n    def as_raw_dict(self):\n        return {\"size\": self.osize, \"nfiles\": self.nfiles}\n\n    @classmethod\n    def from_raw_dict(cls, **kw):\n        self = cls()\n        self.osize = kw[\"size\"]\n        self.nfiles = kw[\"nfiles\"]\n        return self\n\n    @property\n    def osize_fmt(self):\n        return format_file_size(self.osize, iec=self.iec)\n\n    @property\n    def usize_fmt(self):\n        return format_file_size(self.usize, iec=self.iec)\n\n    def show_progress(self, item=None, final=False, stream=None, dt=None):\n        now = time.monotonic()\n        if dt is None or now - self.last_progress > dt:\n            self.last_progress = now\n            if self.output_json:\n                if not final:\n                    data = self.as_dict()\n                    if item:\n                        data.update(text_to_json(\"path\", item.path))\n                else:\n                    data = {}\n                data.update({\"time\": time.time(), \"type\": \"archive_progress\", \"finished\": final})\n                msg = json.dumps(data)\n                end = \"\\n\"\n            else:\n                columns, lines = get_terminal_size()\n                if not final:\n                    msg = \"{0.osize_fmt} O {0.usize_fmt} U {0.nfiles} N \".format(self)\n                    path = remove_surrogates(item.path) if item else \"\"\n                    space = columns - swidth(msg)\n                    if space < 12:\n                        msg = \"\"\n                        space = columns - swidth(msg)\n                    if space >= 8:\n                        msg += ellipsis_truncate(path, space)\n                else:\n                    msg = \" \" * columns\n                end = \"\\r\"\n            print(msg, end=end, file=stream or sys.stderr, flush=True)\n\n\ndef is_special(mode):\n    # file types that get special treatment in --read-special mode\n    return stat.S_ISBLK(mode) or stat.S_ISCHR(mode) or stat.S_ISFIFO(mode)\n\n\nclass BackupError(Exception):\n    \"\"\"\n    Exception raised for non-OSError-based exceptions while accessing backup files.\n    \"\"\"\n\n\nclass BackupOSError(Exception):\n    \"\"\"\n    Wrapper for OSError raised while accessing backup files.\n\n    Borg does different kinds of IO, and IO failures have different consequences.\n    This wrapper represents failures of input file or extraction IO.\n    These are non-critical and are only reported (exit code = 1, warning).\n\n    Any unwrapped IO error is critical and aborts execution (for example repository IO failure).\n    \"\"\"\n\n    def __init__(self, op, os_error):\n        self.op = op\n        self.os_error = os_error\n        self.errno = os_error.errno\n        self.strerror = os_error.strerror\n        self.filename = os_error.filename\n\n    def __str__(self):\n        if self.op:\n            return f\"{self.op}: {self.os_error}\"\n        else:\n            return str(self.os_error)\n\n\nclass BackupIO:\n    op = \"\"\n\n    def __call__(self, op=\"\"):\n        self.op = op\n        return self\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type and issubclass(exc_type, OSError):\n            raise BackupOSError(self.op, exc_val) from exc_val\n\n\nbackup_io = BackupIO()\n\n\ndef backup_io_iter(iterator):\n    backup_io.op = \"read\"\n    while True:\n        with backup_io:\n            try:\n                item = next(iterator)\n            except StopIteration:\n                return\n        yield item\n\n\ndef stat_update_check(st_old, st_curr):\n    \"\"\"\n    this checks for some race conditions between the first filename-based stat()\n    we did before dispatching to the (hopefully correct) file type backup handler\n    and the (hopefully) fd-based fstat() we did in the handler.\n\n    if there is a problematic difference (e.g. file type changed), we rather\n    skip the file than being tricked into a security problem.\n\n    such races should only happen if:\n    - we are backing up a live filesystem (no snapshot, not inactive)\n    - if files change due to normal fs activity at an unfortunate time\n    - if somebody is doing an attack against us\n    \"\"\"\n    # assuming that a file type change implicates a different inode change AND that inode numbers\n    # are not duplicate in a short timeframe, this check is redundant and solved by the ino check:\n    if stat.S_IFMT(st_old.st_mode) != stat.S_IFMT(st_curr.st_mode):\n        # in this case, we dispatched to wrong handler - abort\n        raise BackupError(\"file type changed (race condition), skipping file\")\n    if st_old.st_ino != st_curr.st_ino:\n        # in this case, the hardlinks-related code in create_helper has the wrong inode - abort!\n        raise BackupError(\"file inode changed (race condition), skipping file\")\n    # looks ok, we are still dealing with the same thing - return current stat:\n    return st_curr\n\n\n@contextmanager\ndef OsOpen(*, flags, path=None, parent_fd=None, name=None, noatime=False, op=\"open\"):\n    with backup_io(op):\n        fd = os_open(path=path, parent_fd=parent_fd, name=name, flags=flags, noatime=noatime)\n    try:\n        yield fd\n    finally:\n        # On windows fd is None for directories.\n        if fd is not None:\n            os.close(fd)\n\n\nclass DownloadPipeline:\n    def __init__(self, repository, repo_objs):\n        self.repository = repository\n        self.repo_objs = repo_objs\n\n    def unpack_many(self, ids, *, filter=None, preload=False):\n        \"\"\"\n        Return iterator of items.\n\n        *ids* is a chunk ID list of an item stream. *filter* is a callable\n        to decide whether an item will be yielded. *preload* preloads the data chunks of every yielded item.\n\n        Warning: if *preload* is True then all data chunks of every yielded item have to be retrieved,\n        otherwise preloaded chunks will accumulate in RemoteRepository and create a memory leak.\n        \"\"\"\n        hlids_preloaded = set()\n        unpacker = msgpack.Unpacker(use_list=False)\n        for data in self.fetch_many(ids):\n            unpacker.feed(data)\n            for _item in unpacker:\n                item = Item(internal_dict=_item)\n                if \"chunks\" in item:\n                    item.chunks = [ChunkListEntry(*e) for e in item.chunks]\n                if filter and not filter(item):\n                    continue\n                if preload and \"chunks\" in item:\n                    hlid = item.get(\"hlid\", None)\n                    if hlid is None:\n                        preload_chunks = True\n                    elif hlid in hlids_preloaded:\n                        preload_chunks = False\n                    else:\n                        # not having the hardlink's chunks already preloaded for other hardlink to same inode\n                        preload_chunks = True\n                        hlids_preloaded.add(hlid)\n                    if preload_chunks:\n                        self.repository.preload([c.id for c in item.chunks])\n                yield item\n\n    def fetch_many(self, ids, is_preloaded=False):\n        for id_, cdata in zip(ids, self.repository.get_many(ids, is_preloaded=is_preloaded)):\n            _, data = self.repo_objs.parse(id_, cdata)\n            yield data\n\n\nclass ChunkBuffer:\n    BUFFER_SIZE = 8 * 1024 * 1024\n\n    def __init__(self, key, chunker_params=ITEMS_CHUNKER_PARAMS):\n        self.buffer = BytesIO()\n        self.packer = msgpack.Packer()\n        self.chunks = []\n        self.key = key\n        self.chunker = get_chunker(*chunker_params, seed=self.key.chunk_seed, sparse=False)\n        self.saved_chunks_len = None\n\n    def add(self, item):\n        self.buffer.write(self.packer.pack(item.as_dict()))\n        if self.is_full():\n            self.flush()\n\n    def write_chunk(self, chunk):\n        raise NotImplementedError\n\n    def flush(self, flush=False):\n        if self.buffer.tell() == 0:\n            return\n        self.buffer.seek(0)\n        # The chunker returns a memoryview to its internal buffer,\n        # thus a copy is needed before resuming the chunker iterator.\n        # the metadata stream may produce all-zero chunks, so deal\n        # with CH_ALLOC (and CH_HOLE, for completeness) here.\n        chunks = []\n        for chunk in self.chunker.chunkify(self.buffer):\n            alloc = chunk.meta[\"allocation\"]\n            if alloc == CH_DATA:\n                data = bytes(chunk.data)\n            elif alloc in (CH_ALLOC, CH_HOLE):\n                data = zeros[: chunk.meta[\"size\"]]\n            else:\n                raise ValueError(\"chunk allocation has unsupported value of %r\" % alloc)\n            chunks.append(data)\n        self.buffer.seek(0)\n        self.buffer.truncate(0)\n        # Leave the last partial chunk in the buffer unless flush is True\n        end = None if flush or len(chunks) == 1 else -1\n        for chunk in chunks[:end]:\n            self.chunks.append(self.write_chunk(chunk))\n        if end == -1:\n            self.buffer.write(chunks[-1])\n\n    def is_full(self):\n        return self.buffer.tell() > self.BUFFER_SIZE\n\n    def save_chunks_state(self):\n        # as we only append to self.chunks, remembering the current length is good enough\n        self.saved_chunks_len = len(self.chunks)\n\n    def restore_chunks_state(self):\n        scl = self.saved_chunks_len\n        assert scl is not None, \"forgot to call save_chunks_state?\"\n        tail_chunks = self.chunks[scl:]\n        del self.chunks[scl:]\n        self.saved_chunks_len = None\n        return tail_chunks\n\n\nclass CacheChunkBuffer(ChunkBuffer):\n    def __init__(self, cache, key, stats, chunker_params=ITEMS_CHUNKER_PARAMS):\n        super().__init__(key, chunker_params)\n        self.cache = cache\n        self.stats = stats\n\n    def write_chunk(self, chunk):\n        id_, _ = self.cache.add_chunk(self.key.id_hash(chunk), {}, chunk, stats=self.stats, wait=False)\n        logger.debug(f\"writing item metadata stream chunk {bin_to_hex(id_)}\")\n        self.cache.repository.async_response(wait=False)\n        return id_\n\n\ndef get_item_uid_gid(item, *, numeric, uid_forced=None, gid_forced=None, uid_default=0, gid_default=0):\n    if uid_forced is not None:\n        uid = uid_forced\n    else:\n        uid = None if numeric else user2uid(item.get(\"user\"))\n        uid = item.get(\"uid\") if uid is None else uid\n        if uid is None or uid < 0:\n            uid = uid_default\n    if gid_forced is not None:\n        gid = gid_forced\n    else:\n        gid = None if numeric else group2gid(item.get(\"group\"))\n        gid = item.get(\"gid\") if gid is None else gid\n        if gid is None or gid < 0:\n            gid = gid_default\n    return uid, gid\n\n\ndef archive_get_items(metadata, *, repo_objs, repository):\n    if \"item_ptrs\" in metadata:  # looks like a v2+ archive\n        assert \"items\" not in metadata\n        items = []\n        for id, cdata in zip(metadata.item_ptrs, repository.get_many(metadata.item_ptrs)):\n            _, data = repo_objs.parse(id, cdata)\n            ids = msgpack.unpackb(data)\n            items.extend(ids)\n        return items\n\n    if \"items\" in metadata:  # legacy, v1 archive\n        assert \"item_ptrs\" not in metadata\n        return metadata.items\n\n\ndef archive_put_items(chunk_ids, *, repo_objs, cache=None, stats=None, add_reference=None):\n    \"\"\"gets a (potentially large) list of archive metadata stream chunk ids and writes them to repo objects\"\"\"\n    item_ptrs = []\n    for i in range(0, len(chunk_ids), IDS_PER_CHUNK):\n        data = msgpack.packb(chunk_ids[i : i + IDS_PER_CHUNK])\n        id = repo_objs.id_hash(data)\n        logger.debug(f\"writing item_ptrs chunk {bin_to_hex(id)}\")\n        if cache is not None and stats is not None:\n            cache.add_chunk(id, {}, data, stats=stats)\n        elif add_reference is not None:\n            cdata = repo_objs.format(id, {}, data)\n            add_reference(id, len(data), cdata)\n        else:\n            raise NotImplementedError\n        item_ptrs.append(id)\n    return item_ptrs\n\n\nclass Archive:\n    class DoesNotExist(Error):\n        \"\"\"Archive {} does not exist\"\"\"\n\n    class AlreadyExists(Error):\n        \"\"\"Archive {} already exists\"\"\"\n\n    class IncompatibleFilesystemEncodingError(Error):\n        \"\"\"Failed to encode filename \"{}\" into file system encoding \"{}\". Consider configuring the LANG environment variable.\"\"\"\n\n    def __init__(\n        self,\n        manifest,\n        name,\n        cache=None,\n        create=False,\n        numeric_ids=False,\n        noatime=False,\n        noctime=False,\n        noflags=False,\n        noacls=False,\n        noxattrs=False,\n        progress=False,\n        chunker_params=CHUNKER_PARAMS,\n        start=None,\n        start_monotonic=None,\n        end=None,\n        log_json=False,\n        iec=False,\n    ):\n        self.cwd = os.getcwd()\n        assert isinstance(manifest, Manifest)\n        self.manifest = manifest\n        self.key = manifest.repo_objs.key\n        self.repo_objs = manifest.repo_objs\n        self.repository = manifest.repository\n        self.cache = cache\n        self.stats = Statistics(output_json=log_json, iec=iec)\n        self.iec = iec\n        self.show_progress = progress\n        self.name = name  # overwritten later with name from archive metadata\n        self.name_in_manifest = name  # can differ from .name later (if borg check fixed duplicate archive names)\n        self.comment = None\n        self.numeric_ids = numeric_ids\n        self.noatime = noatime\n        self.noctime = noctime\n        self.noflags = noflags\n        self.noacls = noacls\n        self.noxattrs = noxattrs\n        assert (start is None) == (\n            start_monotonic is None\n        ), \"Logic error: if start is given, start_monotonic must be given as well and vice versa.\"\n        if start is None:\n            start = archive_ts_now()\n            start_monotonic = time.monotonic()\n        self.chunker_params = chunker_params\n        self.start = start\n        self.start_monotonic = start_monotonic\n        if end is None:\n            end = archive_ts_now()\n        self.end = end\n        self.pipeline = DownloadPipeline(self.repository, self.repo_objs)\n        self.create = create\n        if self.create:\n            self.items_buffer = CacheChunkBuffer(self.cache, self.key, self.stats)\n            if name in manifest.archives:\n                raise self.AlreadyExists(name)\n            i = 0\n            while True:\n                self.checkpoint_name = \"{}.checkpoint{}\".format(name, i and (\".%d\" % i) or \"\")\n                if self.checkpoint_name not in manifest.archives:\n                    break\n                i += 1\n        else:\n            info = self.manifest.archives.get(name)\n            if info is None:\n                raise self.DoesNotExist(name)\n            self.load(info.id)\n\n    def _load_meta(self, id):\n        cdata = self.repository.get(id)\n        _, data = self.repo_objs.parse(id, cdata)\n        metadata = ArchiveItem(internal_dict=msgpack.unpackb(data))\n        if metadata.version not in (1, 2):  # legacy: still need to read v1 archives\n            raise Exception(\"Unknown archive metadata version\")\n        # note: metadata.items must not get written to disk!\n        metadata.items = archive_get_items(metadata, repo_objs=self.repo_objs, repository=self.repository)\n        return metadata\n\n    def load(self, id):\n        self.id = id\n        self.metadata = self._load_meta(self.id)\n        self.name = self.metadata.name\n        self.comment = self.metadata.get(\"comment\", \"\")\n\n    @property\n    def ts(self):\n        \"\"\"Timestamp of archive creation (start) in UTC\"\"\"\n        ts = self.metadata.time\n        return parse_timestamp(ts)\n\n    @property\n    def ts_end(self):\n        \"\"\"Timestamp of archive creation (end) in UTC\"\"\"\n        # fall back to time if there is no time_end present in metadata\n        ts = self.metadata.get(\"time_end\") or self.metadata.time\n        return parse_timestamp(ts)\n\n    @property\n    def fpr(self):\n        return bin_to_hex(self.id)\n\n    @property\n    def duration(self):\n        return format_timedelta(self.end - self.start)\n\n    @property\n    def duration_from_meta(self):\n        return format_timedelta(self.ts_end - self.ts)\n\n    def info(self):\n        if self.create:\n            stats = self.stats\n            start = self.start\n            end = self.end\n        else:\n            stats = self.calc_stats(self.cache)\n            start = self.ts\n            end = self.ts_end\n        info = {\n            \"name\": self.name,\n            \"id\": self.fpr,\n            \"start\": OutputTimestamp(start),\n            \"end\": OutputTimestamp(end),\n            \"duration\": (end - start).total_seconds(),\n            \"stats\": stats.as_dict(),\n        }\n        if self.create:\n            info[\"command_line\"] = join_cmd(sys.argv)\n        else:\n            info.update(\n                {\n                    \"command_line\": self.metadata.command_line,\n                    \"hostname\": self.metadata.hostname,\n                    \"username\": self.metadata.username,\n                    \"comment\": self.metadata.get(\"comment\", \"\"),\n                    \"chunker_params\": self.metadata.get(\"chunker_params\", \"\"),\n                }\n            )\n        return info\n\n    def __str__(self):\n        return \"\"\"\\\nRepository: {location}\nArchive name: {0.name}\nArchive fingerprint: {0.fpr}\nTime (start): {start}\nTime (end):   {end}\nDuration: {0.duration}\n\"\"\".format(\n            self,\n            start=OutputTimestamp(self.start),\n            end=OutputTimestamp(self.end),\n            location=self.repository._location.canonical_path(),\n        )\n\n    def __repr__(self):\n        return \"Archive(%r)\" % self.name\n\n    def item_filter(self, item, filter=None):\n        return filter(item) if filter else True\n\n    def iter_items(self, filter=None, preload=False):\n        # note: when calling this with preload=True, later fetch_many() must be called with\n        # is_preloaded=True or the RemoteRepository code will leak memory!\n        yield from self.pipeline.unpack_many(\n            self.metadata.items, preload=preload, filter=lambda item: self.item_filter(item, filter)\n        )\n\n    def add_item(self, item, show_progress=True, stats=None):\n        if show_progress and self.show_progress:\n            if stats is None:\n                stats = self.stats\n            stats.show_progress(item=item, dt=0.2)\n        self.items_buffer.add(item)\n\n    def prepare_checkpoint(self):\n        # we need to flush the archive metadata stream to repo chunks, so that\n        # we have the metadata stream chunks WITHOUT the part file item we add later.\n        # The part file item will then get into its own metadata stream chunk, which we\n        # can easily NOT include into the next checkpoint or the final archive.\n        self.items_buffer.flush(flush=True)\n        # remember the current state of self.chunks, which corresponds to the flushed chunks\n        self.items_buffer.save_chunks_state()\n\n    def write_checkpoint(self):\n        metadata = self.save(self.checkpoint_name)\n        # that .save() has committed the repo.\n        # at next commit, we won't need this checkpoint archive any more because we will then\n        # have either a newer checkpoint archive or the final archive.\n        # so we can already remove it here, the next .save() will then commit this cleanup.\n        # remove its manifest entry, remove its ArchiveItem chunk, remove its item_ptrs chunks:\n        del self.manifest.archives[self.checkpoint_name]\n        self.cache.chunk_decref(self.id, self.stats)\n        for id in metadata.item_ptrs:\n            self.cache.chunk_decref(id, self.stats)\n        # also get rid of that part item, we do not want to have it in next checkpoint or final archive\n        tail_chunks = self.items_buffer.restore_chunks_state()\n        # tail_chunks contain the tail of the archive items metadata stream, not needed for next commit.\n        for id in tail_chunks:\n            self.cache.chunk_decref(id, self.stats)\n\n    def save(self, name=None, comment=None, timestamp=None, stats=None, additional_metadata=None):\n        name = name or self.name\n        if name in self.manifest.archives:\n            raise self.AlreadyExists(name)\n        self.items_buffer.flush(flush=True)\n        item_ptrs = archive_put_items(\n            self.items_buffer.chunks, repo_objs=self.repo_objs, cache=self.cache, stats=self.stats\n        )\n        duration = timedelta(seconds=time.monotonic() - self.start_monotonic)\n        if timestamp is None:\n            end = archive_ts_now()\n            start = end - duration\n        else:\n            start = timestamp\n            end = start + duration\n        self.start = start\n        self.end = end\n        metadata = {\n            \"version\": 2,\n            \"name\": name,\n            \"comment\": comment or \"\",\n            \"item_ptrs\": item_ptrs,  # see #1473\n            \"command_line\": join_cmd(sys.argv),\n            \"hostname\": hostname,\n            \"username\": getuser(),\n            \"time\": start.isoformat(timespec=\"microseconds\"),\n            \"time_end\": end.isoformat(timespec=\"microseconds\"),\n            \"chunker_params\": self.chunker_params,\n        }\n        # we always want to create archives with the addtl. metadata (nfiles, etc.),\n        # because borg info relies on them. so, either use the given stats (from args)\n        # or fall back to self.stats if it was not given.\n        stats = stats or self.stats\n        metadata.update({\"size\": stats.osize, \"nfiles\": stats.nfiles})\n        metadata.update(additional_metadata or {})\n        metadata = ArchiveItem(metadata)\n        data = self.key.pack_and_authenticate_metadata(metadata.as_dict(), context=b\"archive\")\n        self.id = self.repo_objs.id_hash(data)\n        try:\n            self.cache.add_chunk(self.id, {}, data, stats=self.stats)\n        except IntegrityError as err:\n            err_msg = str(err)\n            # hack to avoid changing the RPC protocol by introducing new (more specific) exception class\n            if \"More than allowed put data\" in err_msg:\n                raise Error(\"%s - archive too big (issue #1473)!\" % err_msg)\n            else:\n                raise\n        while self.repository.async_response(wait=True) is not None:\n            pass\n        self.manifest.archives[name] = (self.id, metadata.time)\n        self.manifest.write()\n        self.repository.commit(compact=False)\n        self.cache.commit()\n        return metadata\n\n    def calc_stats(self, cache, want_unique=True):\n        if not want_unique:\n            unique_size = 0\n        else:\n\n            def add(id):\n                entry = cache.chunks[id]\n                archive_index.add(id, 1, entry.size)\n\n            archive_index = ChunkIndex()\n            sync = CacheSynchronizer(archive_index)\n            add(self.id)\n            # we must escape any % char in the archive name, because we use it in a format string, see #6500\n            arch_name_escd = self.name.replace(\"%\", \"%%\")\n            pi = ProgressIndicatorPercent(\n                total=len(self.metadata.items),\n                msg=\"Calculating statistics for archive %s ... %%3.0f%%%%\" % arch_name_escd,\n                msgid=\"archive.calc_stats\",\n            )\n            for id, chunk in zip(self.metadata.items, self.repository.get_many(self.metadata.items)):\n                pi.show(increase=1)\n                add(id)\n                _, data = self.repo_objs.parse(id, chunk)\n                sync.feed(data)\n            unique_size = archive_index.stats_against(cache.chunks)[1]\n            pi.finish()\n\n        stats = Statistics(iec=self.iec)\n        stats.usize = unique_size\n        stats.nfiles = self.metadata.nfiles\n        stats.osize = self.metadata.size\n        return stats\n\n    @contextmanager\n    def extract_helper(self, item, path, hlm, *, dry_run=False):\n        hardlink_set = False\n        # Hard link?\n        if \"hlid\" in item:\n            link_target = hlm.retrieve(id=item.hlid)\n            if link_target is not None and has_link:\n                if not dry_run:\n                    # another hardlink to same inode (same hlid) was extracted previously, just link to it\n                    with backup_io(\"link\"):\n                        os.link(link_target, path, follow_symlinks=False)\n                hardlink_set = True\n        yield hardlink_set\n        if not hardlink_set:\n            if \"hlid\" in item and has_link:\n                # Update entry with extracted item path, so that following hardlinks don't extract twice.\n                # We have hardlinking support, so we will hardlink not extract.\n                hlm.remember(id=item.hlid, info=path)\n            else:\n                # Broken platform with no hardlinking support.\n                # In this case, we *want* to extract twice, because there is no other way.\n                pass\n\n    def extract_item(\n        self,\n        item,\n        *,\n        restore_attrs=True,\n        dry_run=False,\n        stdout=False,\n        sparse=False,\n        hlm=None,\n        pi=None,\n        continue_extraction=False,\n    ):\n        \"\"\"\n        Extract archive item.\n\n        :param item: the item to extract\n        :param restore_attrs: restore file attributes\n        :param dry_run: do not write any data\n        :param stdout: write extracted data to stdout\n        :param sparse: write sparse files (chunk-granularity, independent of the original being sparse)\n        :param hlm: maps hlid to link_target for extracting subtrees with hardlinks correctly\n        :param pi: ProgressIndicatorPercent (or similar) for file extraction progress (in bytes)\n        :param continue_extraction: continue a previously interrupted extraction of same archive\n        \"\"\"\n\n        def same_item(item, st):\n            \"\"\"is the archived item the same as the fs item at same path with stat st?\"\"\"\n            if not stat.S_ISREG(st.st_mode):\n                # we only \"optimize\" for regular files.\n                # other file types are less frequent and have no content extraction we could \"optimize away\".\n                return False\n            if item.mode != st.st_mode or item.size != st.st_size:\n                # the size check catches incomplete previous file extraction\n                return False\n            if item.get(\"mtime\") != st.st_mtime_ns:\n                # note: mtime is \"extracted\" late, after xattrs and ACLs, but before flags.\n                return False\n            # this is good enough for the intended use case:\n            # continuing an extraction of same archive that initially started in an empty directory.\n            # there is a very small risk that \"bsdflags\" of one file are wrong:\n            # if a previous extraction was interrupted between setting the mtime and setting non-default flags.\n            return True\n\n        has_damaged_chunks = \"chunks_healthy\" in item\n        if dry_run or stdout:\n            with self.extract_helper(item, \"\", hlm, dry_run=dry_run or stdout) as hardlink_set:\n                if not hardlink_set:\n                    # it does not really set hardlinks due to dry_run, but we need to behave same\n                    # as non-dry_run concerning fetching preloaded chunks from the pipeline or\n                    # it would get stuck.\n                    if \"chunks\" in item:\n                        item_chunks_size = 0\n                        for data in self.pipeline.fetch_many([c.id for c in item.chunks], is_preloaded=True):\n                            if pi:\n                                pi.show(increase=len(data), info=[remove_surrogates(item.path)])\n                            if stdout:\n                                sys.stdout.buffer.write(data)\n                            item_chunks_size += len(data)\n                        if stdout:\n                            sys.stdout.buffer.flush()\n                        if \"size\" in item:\n                            item_size = item.size\n                            if item_size != item_chunks_size:\n                                raise BackupError(\n                                    \"Size inconsistency detected: size {}, chunks size {}\".format(\n                                        item_size, item_chunks_size\n                                    )\n                                )\n            if has_damaged_chunks:\n                raise BackupError(\"File has damaged (all-zero) chunks. Try running borg check --repair.\")\n            return\n\n        dest = self.cwd\n        path = os.path.join(dest, item.path)\n        # Attempt to remove existing files, ignore errors on failure\n        try:\n            st = os.stat(path, follow_symlinks=False)\n            if continue_extraction and same_item(item, st):\n                return  # done! we already have fully extracted this file in a previous run.\n            elif stat.S_ISDIR(st.st_mode):\n                os.rmdir(path)\n            else:\n                os.unlink(path)\n        except UnicodeEncodeError:\n            raise self.IncompatibleFilesystemEncodingError(path, sys.getfilesystemencoding()) from None\n        except OSError:\n            pass\n\n        def make_parent(path):\n            parent_dir = os.path.dirname(path)\n            if not os.path.exists(parent_dir):\n                os.makedirs(parent_dir)\n\n        mode = item.mode\n        if stat.S_ISREG(mode):\n            with backup_io(\"makedirs\"):\n                make_parent(path)\n            with self.extract_helper(item, path, hlm) as hardlink_set:\n                if hardlink_set:\n                    return\n                with backup_io(\"open\"):\n                    fd = open(path, \"wb\")\n                with fd:\n                    ids = [c.id for c in item.chunks]\n                    for data in self.pipeline.fetch_many(ids, is_preloaded=True):\n                        if pi:\n                            pi.show(increase=len(data), info=[remove_surrogates(item.path)])\n                        with backup_io(\"write\"):\n                            if sparse and zeros.startswith(data):\n                                # all-zero chunk: create a hole in a sparse file\n                                fd.seek(len(data), 1)\n                            else:\n                                fd.write(data)\n                    with backup_io(\"truncate_and_attrs\"):\n                        pos = item_chunks_size = fd.tell()\n                        fd.truncate(pos)\n                        fd.flush()\n                        self.restore_attrs(path, item, fd=fd.fileno())\n                if \"size\" in item:\n                    item_size = item.size\n                    if item_size != item_chunks_size:\n                        raise BackupError(\n                            f\"Size inconsistency detected: size {item_size}, chunks size {item_chunks_size}\"\n                        )\n                if has_damaged_chunks:\n                    raise BackupError(\"File has damaged (all-zero) chunks. Try running borg check --repair.\")\n            return\n        with backup_io:\n            # No repository access beyond this point.\n            if stat.S_ISDIR(mode):\n                make_parent(path)\n                if not os.path.exists(path):\n                    os.mkdir(path)\n                if restore_attrs:\n                    self.restore_attrs(path, item)\n            elif stat.S_ISLNK(mode):\n                make_parent(path)\n                with self.extract_helper(item, path, hlm) as hardlink_set:\n                    if hardlink_set:\n                        # unusual, but possible: this is a hardlinked symlink.\n                        return\n                    target = item.target\n                    try:\n                        os.symlink(target, path)\n                    except UnicodeEncodeError:\n                        raise self.IncompatibleFilesystemEncodingError(target, sys.getfilesystemencoding()) from None\n                    self.restore_attrs(path, item, symlink=True)\n            elif stat.S_ISFIFO(mode):\n                make_parent(path)\n                with self.extract_helper(item, path, hlm) as hardlink_set:\n                    if hardlink_set:\n                        return\n                    os.mkfifo(path)\n                    self.restore_attrs(path, item)\n            elif stat.S_ISCHR(mode) or stat.S_ISBLK(mode):\n                make_parent(path)\n                with self.extract_helper(item, path, hlm) as hardlink_set:\n                    if hardlink_set:\n                        return\n                    os.mknod(path, item.mode, item.rdev)\n                    self.restore_attrs(path, item)\n            else:\n                raise Exception(\"Unknown archive item type %r\" % item.mode)\n\n    def restore_attrs(self, path, item, symlink=False, fd=None):\n        \"\"\"\n        Restore filesystem attributes on *path* (*fd*) from *item*.\n\n        Does not access the repository.\n        \"\"\"\n        backup_io.op = \"attrs\"\n        # This code is a bit of a mess due to OS specific differences.\n        if not is_win32:\n            # by using uid_default = -1 and gid_default = -1, they will not be restored if\n            # the archived item has no information about them.\n            uid, gid = get_item_uid_gid(item, numeric=self.numeric_ids, uid_default=-1, gid_default=-1)\n            # if uid and/or gid is -1, chown will keep it as is and not change it.\n            try:\n                if fd:\n                    os.fchown(fd, uid, gid)\n                else:\n                    os.chown(path, uid, gid, follow_symlinks=False)\n            except OSError:\n                pass\n            if fd:\n                os.fchmod(fd, item.mode)\n            else:\n                # To check whether a particular function in the os module accepts False for its\n                # follow_symlinks parameter, the in operator on supports_follow_symlinks should be\n                # used. However, os.chmod is special as some platforms without a working lchmod() do\n                # have fchmodat(), which has a flag that makes it behave like lchmod(). fchmodat()\n                # is ignored when deciding whether or not os.chmod should be set in\n                # os.supports_follow_symlinks. Work around this by using try/except.\n                try:\n                    os.chmod(path, item.mode, follow_symlinks=False)\n                except NotImplementedError:\n                    if not symlink:\n                        os.chmod(path, item.mode)\n            if not self.noacls:\n                acl_set(path, item, self.numeric_ids, fd=fd)\n            if not self.noxattrs and \"xattrs\" in item:\n                # chown removes Linux capabilities, so set the extended attributes at the end, after chown,\n                # since they include the Linux capabilities in the \"security.capability\" attribute.\n                warning = xattr.set_all(fd or path, item.xattrs, follow_symlinks=False)\n                if warning:\n                    set_ec(EXIT_WARNING)\n            # set timestamps rather late\n            mtime = item.mtime\n            atime = item.atime if \"atime\" in item else mtime\n            if \"birthtime\" in item:\n                birthtime = item.birthtime\n                try:\n                    # This should work on FreeBSD, NetBSD, and Darwin and be harmless on other platforms.\n                    # See utimes(2) on either of the BSDs for details.\n                    if fd:\n                        os.utime(fd, None, ns=(atime, birthtime))\n                    else:\n                        os.utime(path, None, ns=(atime, birthtime), follow_symlinks=False)\n                except OSError:\n                    # some systems don't support calling utime on a symlink\n                    pass\n            try:\n                if fd:\n                    os.utime(fd, None, ns=(atime, mtime))\n                else:\n                    os.utime(path, None, ns=(atime, mtime), follow_symlinks=False)\n            except OSError:\n                # some systems don't support calling utime on a symlink\n                pass\n            # bsdflags include the immutable flag and need to be set last:\n            if not self.noflags and \"bsdflags\" in item:\n                try:\n                    set_flags(path, item.bsdflags, fd=fd)\n                except OSError:\n                    pass\n        else:  # win32\n            # set timestamps rather late\n            mtime = item.mtime\n            atime = item.atime if \"atime\" in item else mtime\n            try:\n                # note: no fd support on win32\n                os.utime(path, None, ns=(atime, mtime))\n            except OSError:\n                # some systems don't support calling utime on a symlink\n                pass\n\n    def set_meta(self, key, value):\n        metadata = self._load_meta(self.id)\n        setattr(metadata, key, value)\n        if \"items\" in metadata:\n            del metadata.items\n        data = msgpack.packb(metadata.as_dict())\n        new_id = self.key.id_hash(data)\n        self.cache.add_chunk(new_id, {}, data, stats=self.stats)\n        self.manifest.archives[self.name] = (new_id, metadata.time)\n        self.cache.chunk_decref(self.id, self.stats)\n        self.id = new_id\n\n    def rename(self, name):\n        if name in self.manifest.archives:\n            raise self.AlreadyExists(name)\n        oldname = self.name\n        self.name = name\n        self.set_meta(\"name\", name)\n        del self.manifest.archives[oldname]\n\n    def delete(self, stats, progress=False, forced=False):\n        class ChunksIndexError(Error):\n            \"\"\"Chunk ID {} missing from chunks index, corrupted chunks index - aborting transaction.\"\"\"\n\n        exception_ignored = object()\n\n        def fetch_async_response(wait=True):\n            try:\n                return self.repository.async_response(wait=wait)\n            except Repository.ObjectNotFound:\n                nonlocal error\n                # object not in repo - strange, but we wanted to delete it anyway.\n                if forced == 0:\n                    raise\n                error = True\n                return exception_ignored  # must not return None here\n\n        def chunk_decref(id, stats):\n            try:\n                self.cache.chunk_decref(id, stats, wait=False)\n            except KeyError:\n                cid = bin_to_hex(id)\n                raise ChunksIndexError(cid)\n            else:\n                fetch_async_response(wait=False)\n\n        error = False\n        try:\n            unpacker = msgpack.Unpacker(use_list=False)\n            items_ids = self.metadata.items\n            pi = ProgressIndicatorPercent(\n                total=len(items_ids), msg=\"Decrementing references %3.0f%%\", msgid=\"archive.delete\"\n            )\n            for i, (items_id, data) in enumerate(zip(items_ids, self.repository.get_many(items_ids))):\n                if progress:\n                    pi.show(i)\n                _, data = self.repo_objs.parse(items_id, data)\n                unpacker.feed(data)\n                chunk_decref(items_id, stats)\n                try:\n                    for item in unpacker:\n                        item = Item(internal_dict=item)\n                        if \"chunks\" in item:\n                            for chunk_id, size in item.chunks:\n                                chunk_decref(chunk_id, stats)\n                except (TypeError, ValueError):\n                    # if items metadata spans multiple chunks and one chunk got dropped somehow,\n                    # it could be that unpacker yields bad types\n                    if forced == 0:\n                        raise\n                    error = True\n            if progress:\n                pi.finish()\n        except (msgpack.UnpackException, Repository.ObjectNotFound):\n            # items metadata corrupted\n            if forced == 0:\n                raise\n            error = True\n\n        # delete the blocks that store all the references that end up being loaded into metadata.items:\n        for id in self.metadata.item_ptrs:\n            chunk_decref(id, stats)\n\n        # in forced delete mode, we try hard to delete at least the manifest entry,\n        # if possible also the archive superblock, even if processing the items raises\n        # some harmless exception.\n        chunk_decref(self.id, stats)\n        del self.manifest.archives[self.name]\n        while fetch_async_response(wait=True) is not None:\n            # we did async deletes, process outstanding results (== exceptions),\n            # so there is nothing pending when we return and our caller wants to commit.\n            pass\n        if error:\n            logger.warning(\"forced deletion succeeded, but the deleted archive was corrupted.\")\n            logger.warning(\"borg check --repair is required to free all space.\")\n\n    @staticmethod\n    def compare_archives_iter(\n        archive1: \"Archive\", archive2: \"Archive\", matcher=None, can_compare_chunk_ids=False\n    ) -> Iterator[ItemDiff]:\n        \"\"\"\n        Yields an ItemDiff instance describing changes/indicating equality.\n\n        :param matcher: PatternMatcher class to restrict results to only matching paths.\n        :param can_compare_chunk_ids: Whether --chunker-params are the same for both archives.\n        \"\"\"\n\n        def compare_items(path: str, item1: Item, item2: Item):\n            return ItemDiff(\n                path,\n                item1,\n                item2,\n                archive1.pipeline.fetch_many([c.id for c in item1.get(\"chunks\", [])]),\n                archive2.pipeline.fetch_many([c.id for c in item2.get(\"chunks\", [])]),\n                can_compare_chunk_ids=can_compare_chunk_ids,\n            )\n\n        orphans_archive1: OrderedDict[str, Item] = OrderedDict()\n        orphans_archive2: OrderedDict[str, Item] = OrderedDict()\n\n        assert matcher is not None, \"matcher must be set\"\n\n        for item1, item2 in zip_longest(\n            archive1.iter_items(lambda item: matcher.match(item.path)),\n            archive2.iter_items(lambda item: matcher.match(item.path)),\n        ):\n            if item1 and item2 and item1.path == item2.path:\n                yield compare_items(item1.path, item1, item2)\n                continue\n            if item1:\n                matching_orphan = orphans_archive2.pop(item1.path, None)\n                if matching_orphan:\n                    yield compare_items(item1.path, item1, matching_orphan)\n                else:\n                    orphans_archive1[item1.path] = item1\n            if item2:\n                matching_orphan = orphans_archive1.pop(item2.path, None)\n                if matching_orphan:\n                    yield compare_items(matching_orphan.path, matching_orphan, item2)\n                else:\n                    orphans_archive2[item2.path] = item2\n        # At this point orphans_* contain items that had no matching partner in the other archive\n        for added in orphans_archive2.values():\n            path = added.path\n            deleted_item = Item.create_deleted(path)\n            yield compare_items(path, deleted_item, added)\n        for deleted in orphans_archive1.values():\n            path = deleted.path\n            deleted_item = Item.create_deleted(path)\n            yield compare_items(path, deleted, deleted_item)\n\n\nclass MetadataCollector:\n    def __init__(self, *, noatime, noctime, nobirthtime, numeric_ids, noflags, noacls, noxattrs):\n        self.noatime = noatime\n        self.noctime = noctime\n        self.numeric_ids = numeric_ids\n        self.noflags = noflags\n        self.noacls = noacls\n        self.noxattrs = noxattrs\n        self.nobirthtime = nobirthtime\n\n    def stat_simple_attrs(self, st):\n        attrs = {}\n        attrs[\"mode\"] = st.st_mode\n        # borg can work with archives only having mtime (very old borg archives do not have\n        # atime/ctime). it can be useful to omit atime/ctime, if they change without the\n        # file content changing - e.g. to get better metadata deduplication.\n        attrs[\"mtime\"] = safe_ns(st.st_mtime_ns)\n        if not self.noatime:\n            attrs[\"atime\"] = safe_ns(st.st_atime_ns)\n        if not self.noctime:\n            attrs[\"ctime\"] = safe_ns(st.st_ctime_ns)\n        if not self.nobirthtime and hasattr(st, \"st_birthtime\"):\n            # sadly, there's no stat_result.st_birthtime_ns\n            attrs[\"birthtime\"] = safe_ns(int(st.st_birthtime * 10**9))\n        attrs[\"uid\"] = st.st_uid\n        attrs[\"gid\"] = st.st_gid\n        if not self.numeric_ids:\n            user = uid2user(st.st_uid)\n            if user is not None:\n                attrs[\"user\"] = user\n            group = gid2group(st.st_gid)\n            if group is not None:\n                attrs[\"group\"] = group\n        return attrs\n\n    def stat_ext_attrs(self, st, path, fd=None):\n        attrs = {}\n        if not self.noflags:\n            with backup_io(\"extended stat (flags)\"):\n                flags = get_flags(path, st, fd=fd)\n            attrs[\"bsdflags\"] = flags\n        if not self.noxattrs:\n            with backup_io(\"extended stat (xattrs)\"):\n                xattrs = xattr.get_all(fd or path, follow_symlinks=False)\n            attrs[\"xattrs\"] = StableDict(xattrs)\n        if not self.noacls:\n            with backup_io(\"extended stat (ACLs)\"):\n                acl_get(path, attrs, st, self.numeric_ids, fd=fd)\n        return attrs\n\n    def stat_attrs(self, st, path, fd=None):\n        attrs = self.stat_simple_attrs(st)\n        attrs.update(self.stat_ext_attrs(st, path, fd=fd))\n        return attrs\n\n\n# remember a few recently used all-zero chunk hashes in this mapping.\n# (hash_func, chunk_length) -> chunk_hash\n# we play safe and have the hash_func in the mapping key, in case we\n# have different hash_funcs within the same borg run.\nzero_chunk_ids = LRUCache(10)  # type: ignore[var-annotated]\n\n\ndef cached_hash(chunk, id_hash):\n    allocation = chunk.meta[\"allocation\"]\n    if allocation == CH_DATA:\n        data = chunk.data\n        chunk_id = id_hash(data)\n    elif allocation in (CH_HOLE, CH_ALLOC):\n        size = chunk.meta[\"size\"]\n        assert size <= len(zeros)\n        data = memoryview(zeros)[:size]\n        try:\n            chunk_id = zero_chunk_ids[(id_hash, size)]\n        except KeyError:\n            chunk_id = id_hash(data)\n            zero_chunk_ids[(id_hash, size)] = chunk_id\n    else:\n        raise ValueError(\"unexpected allocation type\")\n    return chunk_id, data\n\n\nclass ChunksProcessor:\n    # Processes an iterator of chunks for an Item\n\n    def __init__(\n        self,\n        *,\n        key,\n        cache,\n        add_item,\n        prepare_checkpoint,\n        write_checkpoint,\n        checkpoint_interval,\n        checkpoint_volume,\n        rechunkify,\n    ):\n        self.key = key\n        self.cache = cache\n        self.add_item = add_item\n        self.prepare_checkpoint = prepare_checkpoint\n        self.write_checkpoint = write_checkpoint\n        self.rechunkify = rechunkify\n        # time interval based checkpointing\n        self.checkpoint_interval = checkpoint_interval\n        self.last_checkpoint = time.monotonic()\n        # file content volume based checkpointing\n        self.checkpoint_volume = checkpoint_volume\n        self.current_volume = 0\n        self.last_volume_checkpoint = 0\n\n    def write_part_file(self, item):\n        self.prepare_checkpoint()\n        item = Item(internal_dict=item.as_dict())\n        # for borg recreate, we already have a size member in the source item (giving the total file size),\n        # but we consider only a part of the file here, thus we must recompute the size from the chunks:\n        item.get_size(memorize=True, from_chunks=True)\n        item.path += \".borg_part\"\n        self.add_item(item, show_progress=False)\n        self.write_checkpoint()\n\n    def maybe_checkpoint(self, item):\n        checkpoint_done = False\n        sig_int_triggered = sig_int and sig_int.action_triggered()\n        if (\n            sig_int_triggered\n            or (self.checkpoint_interval and time.monotonic() - self.last_checkpoint > self.checkpoint_interval)\n            or (self.checkpoint_volume and self.current_volume - self.last_volume_checkpoint >= self.checkpoint_volume)\n        ):\n            if sig_int_triggered:\n                logger.info(\"checkpoint requested: starting checkpoint creation...\")\n            self.write_part_file(item)\n            checkpoint_done = True\n            self.last_checkpoint = time.monotonic()\n            self.last_volume_checkpoint = self.current_volume\n            if sig_int_triggered:\n                sig_int.action_completed()\n                logger.info(\"checkpoint requested: finished checkpoint creation!\")\n        return checkpoint_done  # whether a checkpoint archive was created\n\n    def process_file_chunks(self, item, cache, stats, show_progress, chunk_iter, chunk_processor=None):\n        if not chunk_processor:\n\n            def chunk_processor(chunk):\n                started_hashing = time.monotonic()\n                chunk_id, data = cached_hash(chunk, self.key.id_hash)\n                stats.hashing_time += time.monotonic() - started_hashing\n                chunk_entry = cache.add_chunk(chunk_id, {}, data, stats=stats, wait=False)\n                self.cache.repository.async_response(wait=False)\n                return chunk_entry\n\n        item.chunks = []\n        # if we rechunkify, we'll get a fundamentally different chunks list, thus we need\n        # to get rid of .chunks_healthy, as it might not correspond to .chunks any more.\n        if self.rechunkify and \"chunks_healthy\" in item:\n            del item.chunks_healthy\n        for chunk in chunk_iter:\n            chunk_entry = chunk_processor(chunk)\n            item.chunks.append(chunk_entry)\n            self.current_volume += chunk_entry[1]\n            if show_progress:\n                stats.show_progress(item=item, dt=0.2)\n            self.maybe_checkpoint(item)\n\n\nclass FilesystemObjectProcessors:\n    # When ported to threading, then this doesn't need chunker, cache, key any more.\n    # write_checkpoint should then be in the item buffer,\n    # and process_file becomes a callback passed to __init__.\n\n    def __init__(\n        self,\n        *,\n        metadata_collector,\n        cache,\n        key,\n        add_item,\n        process_file_chunks,\n        chunker_params,\n        show_progress,\n        sparse,\n        log_json,\n        iec,\n        file_status_printer=None,\n    ):\n        self.metadata_collector = metadata_collector\n        self.cache = cache\n        self.key = key\n        self.add_item = add_item\n        self.process_file_chunks = process_file_chunks\n        self.show_progress = show_progress\n        self.print_file_status = file_status_printer or (lambda *args: None)\n\n        self.hlm = HardLinkManager(id_type=tuple, info_type=(list, type(None)))  # (dev, ino) -> chunks or None\n        self.stats = Statistics(output_json=log_json, iec=iec)  # threading: done by cache (including progress)\n        self.cwd = os.getcwd()\n        self.chunker = get_chunker(*chunker_params, seed=key.chunk_seed, sparse=sparse)\n\n    @contextmanager\n    def create_helper(self, path, st, status=None, hardlinkable=True):\n        sanitized_path = remove_dotdot_prefixes(path)\n        item = Item(path=sanitized_path)\n        hardlinked = hardlinkable and st.st_nlink > 1\n        hl_chunks = None\n        update_map = False\n        if hardlinked:\n            status = \"h\"  # hardlink\n            nothing = object()\n            chunks = self.hlm.retrieve(id=(st.st_ino, st.st_dev), default=nothing)\n            if chunks is nothing:\n                update_map = True\n            elif chunks is not None:\n                hl_chunks = chunks\n            item.hlid = self.hlm.hardlink_id_from_inode(ino=st.st_ino, dev=st.st_dev)\n        yield item, status, hardlinked, hl_chunks\n        self.add_item(item, stats=self.stats)\n        if update_map:\n            # remember the hlid of this fs object and if the item has chunks,\n            # also remember them, so we do not have to re-chunk a hardlink.\n            chunks = item.chunks if \"chunks\" in item else None\n            self.hlm.remember(id=(st.st_ino, st.st_dev), info=chunks)\n\n    def process_dir_with_fd(self, *, path, fd, st):\n        with self.create_helper(path, st, \"d\", hardlinkable=False) as (item, status, hardlinked, hl_chunks):\n            item.update(self.metadata_collector.stat_attrs(st, path, fd=fd))\n            return status\n\n    def process_dir(self, *, path, parent_fd, name, st):\n        with self.create_helper(path, st, \"d\", hardlinkable=False) as (item, status, hardlinked, hl_chunks):\n            with OsOpen(path=path, parent_fd=parent_fd, name=name, flags=flags_dir, noatime=True, op=\"dir_open\") as fd:\n                # fd is None for directories on windows, in that case a race condition check is not possible.\n                if fd is not None:\n                    with backup_io(\"fstat\"):\n                        st = stat_update_check(st, os.fstat(fd))\n                item.update(self.metadata_collector.stat_attrs(st, path, fd=fd))\n                return status\n\n    def process_fifo(self, *, path, parent_fd, name, st):\n        with self.create_helper(path, st, \"f\") as (item, status, hardlinked, hl_chunks):  # fifo\n            with OsOpen(path=path, parent_fd=parent_fd, name=name, flags=flags_normal, noatime=True) as fd:\n                with backup_io(\"fstat\"):\n                    st = stat_update_check(st, os.fstat(fd))\n                item.update(self.metadata_collector.stat_attrs(st, path, fd=fd))\n                return status\n\n    def process_dev(self, *, path, parent_fd, name, st, dev_type):\n        with self.create_helper(path, st, dev_type) as (item, status, hardlinked, hl_chunks):  # char/block device\n            # looks like we can not work fd-based here without causing issues when trying to open/close the device\n            with backup_io(\"stat\"):\n                st = stat_update_check(st, os_stat(path=path, parent_fd=parent_fd, name=name, follow_symlinks=False))\n            item.rdev = st.st_rdev\n            item.update(self.metadata_collector.stat_attrs(st, path))\n            return status\n\n    def process_symlink(self, *, path, parent_fd, name, st):\n        with self.create_helper(path, st, \"s\", hardlinkable=True) as (item, status, hardlinked, hl_chunks):\n            fname = name if name is not None and parent_fd is not None else path\n            with backup_io(\"readlink\"):\n                target = os.readlink(fname, dir_fd=parent_fd)\n            item.target = target\n            item.update(self.metadata_collector.stat_attrs(st, path))  # can't use FD here?\n            return status\n\n    def process_pipe(self, *, path, cache, fd, mode, user=None, group=None):\n        status = \"i\"  # stdin (or other pipe)\n        self.print_file_status(status, path)\n        status = None  # we already printed the status\n        if user is not None:\n            uid = user2uid(user)\n            if uid is None:\n                raise Error(\"no such user: %s\" % user)\n        else:\n            uid = None\n        if group is not None:\n            gid = group2gid(group)\n            if gid is None:\n                raise Error(\"no such group: %s\" % group)\n        else:\n            gid = None\n        t = int(time.time()) * 1000000000\n        item = Item(path=path, mode=mode & 0o107777 | 0o100000, mtime=t, atime=t, ctime=t)  # forcing regular file mode\n        if user is not None:\n            item.user = user\n        if group is not None:\n            item.group = group\n        if uid is not None:\n            item.uid = uid\n        if gid is not None:\n            item.gid = gid\n        try:\n            self.process_file_chunks(\n                item, cache, self.stats, self.show_progress, backup_io_iter(self.chunker.chunkify(fd))\n            )\n        except BackupOSError:\n            # see comments in process_file's exception handler, same issue here.\n            for chunk in item.get(\"chunks\", []):\n                cache.chunk_decref(chunk.id, self.stats, wait=False)\n            raise\n        else:\n            item.get_size(memorize=True)\n            self.stats.nfiles += 1\n            self.add_item(item, stats=self.stats)\n            return status\n\n    def process_file(self, *, path, parent_fd, name, st, cache, flags=flags_normal, last_try=False):\n        with self.create_helper(path, st, None) as (item, status, hardlinked, hl_chunks):  # no status yet\n            with OsOpen(path=path, parent_fd=parent_fd, name=name, flags=flags, noatime=True) as fd:\n                with backup_io(\"fstat\"):\n                    st = stat_update_check(st, os.fstat(fd))\n                item.update(self.metadata_collector.stat_simple_attrs(st))\n                is_special_file = is_special(st.st_mode)\n                if is_special_file:\n                    # we process a special file like a regular file. reflect that in mode,\n                    # so it can be extracted / accessed in FUSE mount like a regular file.\n                    # this needs to be done early, so that part files also get the patched mode.\n                    item.mode = stat.S_IFREG | stat.S_IMODE(item.mode)\n                # we begin processing chunks now (writing or incref'ing them to the repository),\n                # which might require cleanup (see except-branch):\n                try:\n                    if hl_chunks is not None:  # create_helper gave us chunks from a previous hardlink\n                        item.chunks = []\n                        for chunk_id, chunk_size in hl_chunks:\n                            # process one-by-one, so we will know in item.chunks how far we got\n                            chunk_entry = cache.chunk_incref(chunk_id, self.stats)\n                            item.chunks.append(chunk_entry)\n                    else:  # normal case, no \"2nd+\" hardlink\n                        if not is_special_file:\n                            hashed_path = safe_encode(os.path.join(self.cwd, path))\n                            started_hashing = time.monotonic()\n                            path_hash = self.key.id_hash(hashed_path)\n                            self.stats.hashing_time += time.monotonic() - started_hashing\n                            known, ids = cache.file_known_and_unchanged(hashed_path, path_hash, st)\n                        else:\n                            # in --read-special mode, we may be called for special files.\n                            # there should be no information in the cache about special files processed in\n                            # read-special mode, but we better play safe as this was wrong in the past:\n                            hashed_path = path_hash = None\n                            known, ids = False, None\n                        if ids is not None:\n                            # Make sure all ids are available\n                            for id_ in ids:\n                                if not cache.seen_chunk(id_):\n                                    # cache said it is unmodified, but we lost a chunk: process file like modified\n                                    status = \"M\"\n                                    break\n                            else:\n                                item.chunks = []\n                                for chunk_id in ids:\n                                    # process one-by-one, so we will know in item.chunks how far we got\n                                    chunk_entry = cache.chunk_incref(chunk_id, self.stats)\n                                    item.chunks.append(chunk_entry)\n                                status = \"U\"  # regular file, unchanged\n                        else:\n                            status = \"M\" if known else \"A\"  # regular file, modified or added\n                        self.print_file_status(status, path)\n                        # Only chunkify the file if needed\n                        changed_while_backup = False\n                        if \"chunks\" not in item:\n                            with backup_io(\"read\"):\n                                self.process_file_chunks(\n                                    item,\n                                    cache,\n                                    self.stats,\n                                    self.show_progress,\n                                    backup_io_iter(self.chunker.chunkify(None, fd)),\n                                )\n                                self.stats.chunking_time = self.chunker.chunking_time\n                            if not is_win32:  # TODO for win32\n                                with backup_io(\"fstat2\"):\n                                    st2 = os.fstat(fd)\n                                # special files:\n                                # - fifos change naturally, because they are fed from the other side. no problem.\n                                # - blk/chr devices don't change ctime anyway.\n                                changed_while_backup = not is_special_file and st.st_ctime_ns != st2.st_ctime_ns\n                            if changed_while_backup:\n                                # regular file changed while we backed it up, might be inconsistent/corrupt!\n                                if last_try:\n                                    status = \"C\"  # crap! retries did not help.\n                                else:\n                                    raise BackupError(\"file changed while we read it!\")\n                            if not is_special_file and not changed_while_backup:\n                                # we must not memorize special files, because the contents of e.g. a\n                                # block or char device will change without its mtime/size/inode changing.\n                                # also, we must not memorize a potentially inconsistent/corrupt file that\n                                # changed while we backed it up.\n                                cache.memorize_file(hashed_path, path_hash, st, [c.id for c in item.chunks])\n                        self.stats.files_stats[status] += 1  # must be done late\n                        if not changed_while_backup:\n                            status = None  # we already called print_file_status\n                    self.stats.nfiles += 1\n                    item.update(self.metadata_collector.stat_ext_attrs(st, path, fd=fd))\n                    item.get_size(memorize=True)\n                    return status\n                except BackupOSError:\n                    # Something went wrong and we might need to clean up a bit.\n                    # Maybe we have already incref'ed some file content chunks in the repo -\n                    # but we will not add an item (see add_item in create_helper) and thus\n                    # they would be orphaned chunks in case that we commit the transaction.\n                    for chunk in item.get(\"chunks\", []):\n                        cache.chunk_decref(chunk.id, self.stats, wait=False)\n                    # Now that we have cleaned up the chunk references, we can re-raise the exception.\n                    # This will skip processing of this file, but might retry or continue with the next one.\n                    raise\n\n\nclass TarfileObjectProcessors:\n    def __init__(\n        self,\n        *,\n        cache,\n        key,\n        add_item,\n        process_file_chunks,\n        chunker_params,\n        show_progress,\n        log_json,\n        iec,\n        file_status_printer=None,\n    ):\n        self.cache = cache\n        self.key = key\n        self.add_item = add_item\n        self.process_file_chunks = process_file_chunks\n        self.show_progress = show_progress\n        self.print_file_status = file_status_printer or (lambda *args: None)\n\n        self.stats = Statistics(output_json=log_json, iec=iec)  # threading: done by cache (including progress)\n        self.chunker = get_chunker(*chunker_params, seed=key.chunk_seed, sparse=False)\n        self.hlm = HardLinkManager(id_type=str, info_type=list)  # path -> chunks\n\n    @contextmanager\n    def create_helper(self, tarinfo, status=None, type=None):\n        ph = tarinfo.pax_headers\n        if ph and \"BORG.item.version\" in ph:\n            assert ph[\"BORG.item.version\"] == \"1\"\n            meta_bin = base64.b64decode(ph[\"BORG.item.meta\"])\n            meta_dict = msgpack.unpackb(meta_bin, object_hook=StableDict)\n            item = Item(internal_dict=meta_dict)\n        else:\n\n            def s_to_ns(s):\n                return safe_ns(int(float(s) * 1e9))\n\n            item = Item(\n                path=make_path_safe(tarinfo.name),\n                mode=tarinfo.mode | type,\n                uid=tarinfo.uid,\n                gid=tarinfo.gid,\n                mtime=s_to_ns(tarinfo.mtime),\n            )\n            if tarinfo.uname:\n                item.user = tarinfo.uname\n            if tarinfo.gname:\n                item.group = tarinfo.gname\n            if ph:\n                # note: for mtime this is a bit redundant as it is already done by tarfile module,\n                #       but we just do it in our way to be consistent for sure.\n                for name in \"atime\", \"ctime\", \"mtime\":\n                    if name in ph:\n                        ns = s_to_ns(ph[name])\n                        setattr(item, name, ns)\n        yield item, status\n        # if we get here, \"with\"-block worked ok without error/exception, the item was processed ok...\n        self.add_item(item, stats=self.stats)\n\n    def process_dir(self, *, tarinfo, status, type):\n        with self.create_helper(tarinfo, status, type) as (item, status):\n            return status\n\n    def process_fifo(self, *, tarinfo, status, type):\n        with self.create_helper(tarinfo, status, type) as (item, status):\n            return status\n\n    def process_dev(self, *, tarinfo, status, type):\n        with self.create_helper(tarinfo, status, type) as (item, status):\n            item.rdev = os.makedev(tarinfo.devmajor, tarinfo.devminor)\n            return status\n\n    def process_symlink(self, *, tarinfo, status, type):\n        with self.create_helper(tarinfo, status, type) as (item, status):\n            item.target = tarinfo.linkname\n            return status\n\n    def process_hardlink(self, *, tarinfo, status, type):\n        with self.create_helper(tarinfo, status, type) as (item, status):\n            # create a not hardlinked borg item, reusing the chunks, see HardLinkManager.__doc__\n            chunks = self.hlm.retrieve(tarinfo.linkname)\n            if chunks is not None:\n                item.chunks = chunks\n            item.get_size(memorize=True, from_chunks=True)\n            self.stats.nfiles += 1\n            return status\n\n    def process_file(self, *, tarinfo, status, type, tar):\n        with self.create_helper(tarinfo, status, type) as (item, status):\n            self.print_file_status(status, tarinfo.name)\n            status = None  # we already printed the status\n            try:\n                fd = tar.extractfile(tarinfo)\n                self.process_file_chunks(\n                    item, self.cache, self.stats, self.show_progress, backup_io_iter(self.chunker.chunkify(fd))\n                )\n                item.get_size(memorize=True, from_chunks=True)\n                self.stats.nfiles += 1\n                # we need to remember ALL files, see HardLinkManager.__doc__\n                self.hlm.remember(id=tarinfo.name, info=item.chunks)\n                return status\n            except BackupOSError:\n                # see comment in FilesystemObjectProcessors.process_file, same issue here.\n                for chunk in item.get(\"chunks\", []):\n                    self.cache.chunk_decref(chunk.id, self.stats, wait=False)\n                raise\n\n\ndef valid_msgpacked_dict(d, keys_serialized):\n    \"\"\"check if the data <d> looks like a msgpacked dict\"\"\"\n    d_len = len(d)\n    if d_len == 0:\n        return False\n    if d[0] & 0xF0 == 0x80:  # object is a fixmap (up to 15 elements)\n        offs = 1\n    elif d[0] == 0xDE:  # object is a map16 (up to 2^16-1 elements)\n        offs = 3\n    else:\n        # object is not a map (dict)\n        # note: we must not have dicts with > 2^16-1 elements\n        return False\n    if d_len <= offs:\n        return False\n    # is the first dict key a bytestring?\n    if d[offs] & 0xE0 == 0xA0:  # key is a small bytestring (up to 31 chars)\n        pass\n    elif d[offs] in (0xD9, 0xDA, 0xDB):  # key is a str8, str16 or str32\n        pass\n    else:\n        # key is not a bytestring\n        return False\n    # is the bytestring any of the expected key names?\n    key_serialized = d[offs:]\n    return any(key_serialized.startswith(pattern) for pattern in keys_serialized)\n\n\nclass RobustUnpacker:\n    \"\"\"A restartable/robust version of the streaming msgpack unpacker\"\"\"\n\n    def __init__(self, validator, item_keys):\n        super().__init__()\n        self.item_keys = [msgpack.packb(name) for name in item_keys]\n        self.validator = validator\n        self._buffered_data = []\n        self._resync = False\n        self._unpacker = msgpack.Unpacker(object_hook=StableDict)\n\n    def resync(self):\n        self._buffered_data = []\n        self._resync = True\n\n    def feed(self, data):\n        if self._resync:\n            self._buffered_data.append(data)\n        else:\n            self._unpacker.feed(data)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self._resync:\n            data = b\"\".join(self._buffered_data)\n            while self._resync:\n                if not data:\n                    raise StopIteration\n                # Abort early if the data does not look like a serialized item dict\n                if not valid_msgpacked_dict(data, self.item_keys):\n                    data = data[1:]\n                    continue\n                self._unpacker = msgpack.Unpacker(object_hook=StableDict)\n                self._unpacker.feed(data)\n                try:\n                    item = next(self._unpacker)\n                except (msgpack.UnpackException, StopIteration):\n                    # as long as we are resyncing, we also ignore StopIteration\n                    pass\n                else:\n                    if self.validator(item):\n                        self._resync = False\n                        return item\n                data = data[1:]\n        else:\n            return next(self._unpacker)\n\n\nclass ArchiveChecker:\n    def __init__(self):\n        self.error_found = False\n        self.possibly_superseded = set()\n\n    def check(\n        self,\n        repository,\n        *,\n        verify_data=False,\n        repair=False,\n        match=None,\n        sort_by=\"\",\n        first=0,\n        last=0,\n        older=None,\n        newer=None,\n        oldest=None,\n        newest=None,\n    ):\n        \"\"\"Perform a set of checks on 'repository'\n\n        :param repair: enable repair mode, write updated or corrected data into repository\n        :param first/last/sort_by: only check this number of first/last archives ordered by sort_by\n        :param match: only check archives matching this pattern\n        :param older/newer: only check archives older/newer than timedelta from now\n        :param oldest/newest: only check archives older/newer than timedelta from oldest/newest archive timestamp\n        :param verify_data: integrity verification of data referenced by archives\n        \"\"\"\n        logger.info(\"Starting archive consistency check...\")\n        self.check_all = not any((first, last, match, older, newer, oldest, newest))\n        self.repair = repair\n        self.repository = repository\n        self.init_chunks()\n        if not self.chunks:\n            logger.error(\"Repository contains no apparent data at all, cannot continue check/repair.\")\n            return False\n        self.key = self.make_key(repository)\n        self.repo_objs = RepoObj(self.key)\n        if verify_data:\n            self.verify_data()\n        if Manifest.MANIFEST_ID not in self.chunks:\n            logger.error(\"Repository manifest not found!\")\n            self.error_found = True\n            self.manifest = self.rebuild_manifest()\n        else:\n            try:\n                self.manifest = Manifest.load(repository, (Manifest.Operation.CHECK,), key=self.key)\n            except IntegrityErrorBase as exc:\n                logger.error(\"Repository manifest is corrupted: %s\", exc)\n                self.error_found = True\n                del self.chunks[Manifest.MANIFEST_ID]\n                self.manifest = self.rebuild_manifest()\n        self.rebuild_refcounts(\n            match=match, first=first, last=last, sort_by=sort_by, older=older, oldest=oldest, newer=newer, newest=newest\n        )\n        self.orphan_chunks_check()\n        self.finish()\n        if self.error_found:\n            logger.error(\"Archive consistency check complete, problems found.\")\n        else:\n            logger.info(\"Archive consistency check complete, no problems found.\")\n        return self.repair or not self.error_found\n\n    def init_chunks(self):\n        \"\"\"Fetch a list of all object keys from repository\"\"\"\n        # Explicitly set the initial usable hash table capacity to avoid performance issues\n        # due to hash table \"resonance\".\n        # Since reconstruction of archive items can add some new chunks, add 10 % headroom.\n        self.chunks = ChunkIndex(usable=len(self.repository) * 1.1)\n        marker = None\n        while True:\n            result = self.repository.list(limit=LIST_SCAN_LIMIT, marker=marker)\n            if not result:\n                break\n            marker = result[-1]\n            init_entry = ChunkIndexEntry(refcount=0, size=0)\n            for id_ in result:\n                self.chunks[id_] = init_entry\n\n    def make_key(self, repository):\n        attempt = 0\n        for chunkid, _ in self.chunks.iteritems():\n            attempt += 1\n            if attempt > 999:\n                # we did a lot of attempts, but could not create the key via key_factory, give up.\n                break\n            cdata = repository.get(chunkid)\n            try:\n                return key_factory(repository, cdata)\n            except UnsupportedPayloadError:\n                # we get here, if the cdata we got has a corrupted key type byte\n                pass  # ignore it, just try the next chunk\n        if attempt == 0:\n            msg = \"make_key: repository has no chunks at all!\"\n        else:\n            msg = \"make_key: failed to create the key (tried %d chunks)\" % attempt\n        raise IntegrityError(msg)\n\n    def verify_data(self):\n        logger.info(\"Starting cryptographic data integrity verification...\")\n        chunks_count_index = len(self.chunks)\n        chunks_count_segments = 0\n        errors = 0\n        defect_chunks = []\n        pi = ProgressIndicatorPercent(\n            total=chunks_count_index, msg=\"Verifying data %6.2f%%\", step=0.01, msgid=\"check.verify_data\"\n        )\n        state = None\n        while True:\n            chunk_ids, state = self.repository.scan(limit=100, state=state)\n            if not chunk_ids:\n                break\n            chunks_count_segments += len(chunk_ids)\n            chunk_data_iter = self.repository.get_many(chunk_ids)\n            chunk_ids_revd = list(reversed(chunk_ids))\n            while chunk_ids_revd:\n                pi.show()\n                chunk_id = chunk_ids_revd.pop(-1)  # better efficiency\n                try:\n                    encrypted_data = next(chunk_data_iter)\n                except (Repository.ObjectNotFound, IntegrityErrorBase) as err:\n                    self.error_found = True\n                    errors += 1\n                    logger.error(\"chunk %s: %s\", bin_to_hex(chunk_id), err)\n                    if isinstance(err, IntegrityErrorBase):\n                        defect_chunks.append(chunk_id)\n                    # as the exception killed our generator, make a new one for remaining chunks:\n                    if chunk_ids_revd:\n                        chunk_ids = list(reversed(chunk_ids_revd))\n                        chunk_data_iter = self.repository.get_many(chunk_ids)\n                else:\n                    try:\n                        # we must decompress, so it'll call assert_id() in there:\n                        self.repo_objs.parse(chunk_id, encrypted_data, decompress=True)\n                    except IntegrityErrorBase as integrity_error:\n                        self.error_found = True\n                        errors += 1\n                        logger.error(\"chunk %s, integrity error: %s\", bin_to_hex(chunk_id), integrity_error)\n                        defect_chunks.append(chunk_id)\n        pi.finish()\n        if chunks_count_index != chunks_count_segments:\n            logger.error(\"Repo/Chunks index object count vs. segment files object count mismatch.\")\n            logger.error(\n                \"Repo/Chunks index: %d objects != segment files: %d objects\", chunks_count_index, chunks_count_segments\n            )\n        if defect_chunks:\n            if self.repair:\n                # if we kill the defect chunk here, subsequent actions within this \"borg check\"\n                # run will find missing chunks and replace them with all-zero replacement\n                # chunks and flag the files as \"repaired\".\n                # if another backup is done later and the missing chunks get backed up again,\n                # a \"borg check\" afterwards can heal all files where this chunk was missing.\n                logger.warning(\n                    \"Found defect chunks. They will be deleted now, so affected files can \"\n                    \"get repaired now and maybe healed later.\"\n                )\n                for defect_chunk in defect_chunks:\n                    # remote repo (ssh): retry might help for strange network / NIC / RAM errors\n                    # as the chunk will be retransmitted from remote server.\n                    # local repo (fs): as chunks.iteritems loop usually pumps a lot of data through,\n                    # a defect chunk is likely not in the fs cache any more and really gets re-read\n                    # from the underlying media.\n                    try:\n                        encrypted_data = self.repository.get(defect_chunk)\n                        # we must decompress, so it'll call assert_id() in there:\n                        self.repo_objs.parse(defect_chunk, encrypted_data, decompress=True)\n                    except IntegrityErrorBase:\n                        # failed twice -> get rid of this chunk\n                        del self.chunks[defect_chunk]\n                        self.repository.delete(defect_chunk)\n                        logger.debug(\"chunk %s deleted.\", bin_to_hex(defect_chunk))\n                    else:\n                        logger.warning(\"chunk %s not deleted, did not consistently fail.\", bin_to_hex(defect_chunk))\n            else:\n                logger.warning(\n                    \"Found defect chunks. With --repair, they would get deleted, so affected \"\n                    \"files could get repaired then and maybe healed later.\"\n                )\n                for defect_chunk in defect_chunks:\n                    logger.debug(\"chunk %s is defect.\", bin_to_hex(defect_chunk))\n        log = logger.error if errors else logger.info\n        log(\n            \"Finished cryptographic data integrity verification, verified %d chunks with %d integrity errors.\",\n            chunks_count_segments,\n            errors,\n        )\n\n    def rebuild_manifest(self):\n        \"\"\"Rebuild the manifest object if it is missing\n\n        Iterates through all objects in the repository looking for archive metadata blocks.\n        \"\"\"\n\n        def valid_archive(obj):\n            if not isinstance(obj, dict):\n                return False\n            return REQUIRED_ARCHIVE_KEYS.issubset(obj)\n\n        logger.info(\"Rebuilding missing manifest, this might take some time...\")\n        # as we have lost the manifest, we do not know any more what valid item keys we had.\n        # collecting any key we encounter in a damaged repo seems unwise, thus we just use\n        # the hardcoded list from the source code. thus, it is not recommended to rebuild a\n        # lost manifest on a older borg version than the most recent one that was ever used\n        # within this repository (assuming that newer borg versions support more item keys).\n        manifest = Manifest(self.key, self.repository)\n        archive_keys_serialized = [msgpack.packb(name) for name in ARCHIVE_KEYS]\n        pi = ProgressIndicatorPercent(\n            total=len(self.chunks), msg=\"Rebuilding manifest %6.2f%%\", step=0.01, msgid=\"check.rebuild_manifest\"\n        )\n        for chunk_id, _ in self.chunks.iteritems():\n            pi.show()\n            cdata = self.repository.get(chunk_id)\n            try:\n                _, data = self.repo_objs.parse(chunk_id, cdata)\n            except IntegrityErrorBase as exc:\n                logger.error(\"Skipping corrupted chunk: %s\", exc)\n                self.error_found = True\n                continue\n            if not valid_msgpacked_dict(data, archive_keys_serialized):\n                continue\n            if b\"command_line\" not in data or b\"\\xa7version\\x02\" not in data:\n                continue\n            try:\n                archive = msgpack.unpackb(data)\n            # Ignore exceptions that might be raised when feeding msgpack with invalid data\n            except msgpack.UnpackException:\n                continue\n            if valid_archive(archive):\n                archive = ArchiveItem(internal_dict=archive)\n                name = archive.name\n                logger.info(\"Found archive %s\", name)\n                if name in manifest.archives:\n                    i = 1\n                    while True:\n                        new_name = \"%s.%d\" % (name, i)\n                        if new_name not in manifest.archives:\n                            break\n                        i += 1\n                    logger.warning(\"Duplicate archive name %s, storing as %s\", name, new_name)\n                    name = new_name\n                manifest.archives[name] = (chunk_id, archive.time)\n        pi.finish()\n        logger.info(\"Manifest rebuild complete.\")\n        return manifest\n\n    def rebuild_refcounts(\n        self, first=0, last=0, sort_by=\"\", match=None, older=None, newer=None, oldest=None, newest=None\n    ):\n        \"\"\"Rebuild object reference counts by walking the metadata\n\n        Missing and/or incorrect data is repaired when detected\n        \"\"\"\n        # Exclude the manifest from chunks (manifest entry might be already deleted from self.chunks)\n        self.chunks.pop(Manifest.MANIFEST_ID, None)\n\n        def mark_as_possibly_superseded(id_):\n            if self.chunks.get(id_, ChunkIndexEntry(0, 0)).refcount == 0:\n                self.possibly_superseded.add(id_)\n\n        def add_callback(chunk):\n            id_ = self.key.id_hash(chunk)\n            cdata = self.repo_objs.format(id_, {}, chunk)\n            add_reference(id_, len(chunk), cdata)\n            return id_\n\n        def add_reference(id_, size, cdata=None):\n            try:\n                self.chunks.incref(id_)\n            except KeyError:\n                assert cdata is not None\n                self.chunks[id_] = ChunkIndexEntry(refcount=1, size=size)\n                if self.repair:\n                    self.repository.put(id_, cdata)\n\n        def verify_file_chunks(archive_name, item):\n            \"\"\"Verifies that all file chunks are present.\n\n            Missing file chunks will be replaced with new chunks of the same length containing all zeros.\n            If a previously missing file chunk re-appears, the replacement chunk is replaced by the correct one.\n            \"\"\"\n\n            def replacement_chunk(size):\n                chunk = Chunk(None, allocation=CH_ALLOC, size=size)\n                chunk_id, data = cached_hash(chunk, self.key.id_hash)\n                cdata = self.repo_objs.format(chunk_id, {}, data)\n                return chunk_id, size, cdata\n\n            offset = 0\n            chunk_list = []\n            chunks_replaced = False\n            has_chunks_healthy = \"chunks_healthy\" in item\n            chunks_current = item.chunks\n            chunks_healthy = item.chunks_healthy if has_chunks_healthy else chunks_current\n            if has_chunks_healthy and len(chunks_current) != len(chunks_healthy):\n                # should never happen, but there was issue #3218.\n                logger.warning(f\"{archive_name}: {item.path}: Invalid chunks_healthy metadata removed!\")\n                del item.chunks_healthy\n                has_chunks_healthy = False\n                chunks_healthy = chunks_current\n            for chunk_current, chunk_healthy in zip(chunks_current, chunks_healthy):\n                chunk_id, size = chunk_healthy\n                if chunk_id not in self.chunks:\n                    # a chunk of the healthy list is missing\n                    if chunk_current == chunk_healthy:\n                        logger.error(\n                            \"{}: {}: New missing file chunk detected (Byte {}-{}, Chunk {}). \"\n                            \"Replacing with all-zero chunk.\".format(\n                                archive_name, item.path, offset, offset + size, bin_to_hex(chunk_id)\n                            )\n                        )\n                        self.error_found = chunks_replaced = True\n                        chunk_id, size, cdata = replacement_chunk(size)\n                        add_reference(chunk_id, size, cdata)\n                    else:\n                        logger.info(\n                            \"{}: {}: Previously missing file chunk is still missing (Byte {}-{}, Chunk {}). \"\n                            \"It has an all-zero replacement chunk already.\".format(\n                                archive_name, item.path, offset, offset + size, bin_to_hex(chunk_id)\n                            )\n                        )\n                        chunk_id, size = chunk_current\n                        if chunk_id in self.chunks:\n                            add_reference(chunk_id, size)\n                        else:\n                            logger.warning(\n                                \"{}: {}: Missing all-zero replacement chunk detected (Byte {}-{}, Chunk {}). \"\n                                \"Generating new replacement chunk.\".format(\n                                    archive_name, item.path, offset, offset + size, bin_to_hex(chunk_id)\n                                )\n                            )\n                            self.error_found = chunks_replaced = True\n                            chunk_id, size, cdata = replacement_chunk(size)\n                            add_reference(chunk_id, size, cdata)\n                else:\n                    if chunk_current == chunk_healthy:\n                        # normal case, all fine.\n                        add_reference(chunk_id, size)\n                    else:\n                        logger.info(\n                            \"{}: {}: Healed previously missing file chunk! (Byte {}-{}, Chunk {}).\".format(\n                                archive_name, item.path, offset, offset + size, bin_to_hex(chunk_id)\n                            )\n                        )\n                        add_reference(chunk_id, size)\n                        mark_as_possibly_superseded(chunk_current[0])  # maybe orphaned the all-zero replacement chunk\n                chunk_list.append([chunk_id, size])  # list-typed element as chunks_healthy is list-of-lists\n                offset += size\n            if chunks_replaced and not has_chunks_healthy:\n                # if this is first repair, remember the correct chunk IDs, so we can maybe heal the file later\n                item.chunks_healthy = item.chunks\n            if has_chunks_healthy and chunk_list == chunks_healthy:\n                logger.info(f\"{archive_name}: {item.path}: Completely healed previously damaged file!\")\n                del item.chunks_healthy\n            item.chunks = chunk_list\n            if \"size\" in item:\n                item_size = item.size\n                item_chunks_size = item.get_size(from_chunks=True)\n                if item_size != item_chunks_size:\n                    # just warn, but keep the inconsistency, so that borg extract can warn about it.\n                    logger.warning(\n                        \"{}: {}: size inconsistency detected: size {}, chunks size {}\".format(\n                            archive_name, item.path, item_size, item_chunks_size\n                        )\n                    )\n\n        def robust_iterator(archive):\n            \"\"\"Iterates through all archive items\n\n            Missing item chunks will be skipped and the msgpack stream will be restarted\n            \"\"\"\n            item_keys = self.manifest.item_keys\n            required_item_keys = REQUIRED_ITEM_KEYS\n            unpacker = RobustUnpacker(\n                lambda item: isinstance(item, StableDict) and \"path\" in item, self.manifest.item_keys\n            )\n            _state = 0\n\n            def missing_chunk_detector(chunk_id):\n                nonlocal _state\n                if _state % 2 != int(chunk_id not in self.chunks):\n                    _state += 1\n                return _state\n\n            def report(msg, chunk_id, chunk_no):\n                cid = bin_to_hex(chunk_id)\n                msg += \" [chunk: %06d_%s]\" % (chunk_no, cid)  # see \"debug dump-archive-items\"\n                self.error_found = True\n                logger.error(msg)\n\n            def list_keys_safe(keys):\n                return \", \".join(k.decode(errors=\"replace\") if isinstance(k, bytes) else str(k) for k in keys)\n\n            def valid_item(obj):\n                if not isinstance(obj, StableDict):\n                    return False, \"not a dictionary\"\n                keys = set(obj)\n                if not required_item_keys.issubset(keys):\n                    return False, \"missing required keys: \" + list_keys_safe(required_item_keys - keys)\n                if not keys.issubset(item_keys):\n                    return False, \"invalid keys: \" + list_keys_safe(keys - item_keys)\n                return True, \"\"\n\n            i = 0\n            archive_items = archive_get_items(archive, repo_objs=self.repo_objs, repository=repository)\n            for state, items in groupby(archive_items, missing_chunk_detector):\n                items = list(items)\n                if state % 2:\n                    for chunk_id in items:\n                        report(\"item metadata chunk missing\", chunk_id, i)\n                        i += 1\n                    continue\n                if state > 0:\n                    unpacker.resync()\n                for chunk_id, cdata in zip(items, repository.get_many(items)):\n                    try:\n                        _, data = self.repo_objs.parse(chunk_id, cdata)\n                        unpacker.feed(data)\n                        for item in unpacker:\n                            valid, reason = valid_item(item)\n                            if valid:\n                                yield Item(internal_dict=item)\n                            else:\n                                report(\n                                    \"Did not get expected metadata dict when unpacking item metadata (%s)\" % reason,\n                                    chunk_id,\n                                    i,\n                                )\n                    except IntegrityError as integrity_error:\n                        # repo_objs.parse() detected integrity issues.\n                        # maybe the repo gave us a valid cdata, but not for the chunk_id we wanted.\n                        # or the authentication of cdata failed, meaning the encrypted data was corrupted.\n                        report(str(integrity_error), chunk_id, i)\n                    except msgpack.UnpackException:\n                        report(\"Unpacker crashed while unpacking item metadata, trying to resync...\", chunk_id, i)\n                        unpacker.resync()\n                    except Exception:\n                        report(\"Exception while decrypting or unpacking item metadata\", chunk_id, i)\n                        raise\n                    i += 1\n\n        sort_by = sort_by.split(\",\")\n        if any((first, last, match, older, newer, newest, oldest)):\n            archive_infos = self.manifest.archives.list(\n                sort_by=sort_by,\n                match=match,\n                first=first,\n                last=last,\n                oldest=oldest,\n                newest=newest,\n                older=older,\n                newer=newer,\n            )\n            if match and not archive_infos:\n                logger.warning(\"--match-archives %s does not match any archives\", match)\n            if first and len(archive_infos) < first:\n                logger.warning(\"--first %d archives: only found %d archives\", first, len(archive_infos))\n            if last and len(archive_infos) < last:\n                logger.warning(\"--last %d archives: only found %d archives\", last, len(archive_infos))\n        else:\n            archive_infos = self.manifest.archives.list(sort_by=sort_by, consider_checkpoints=True)\n        num_archives = len(archive_infos)\n\n        pi = ProgressIndicatorPercent(\n            total=num_archives, msg=\"Checking archives %3.1f%%\", step=0.1, msgid=\"check.rebuild_refcounts\"\n        )\n        with cache_if_remote(self.repository) as repository:\n            for i, info in enumerate(archive_infos):\n                pi.show(i)\n                logger.info(f\"Analyzing archive {info.name} ({i + 1}/{num_archives})\")\n                archive_id = info.id\n                if archive_id not in self.chunks:\n                    logger.error(\"Archive metadata block %s is missing!\", bin_to_hex(archive_id))\n                    self.error_found = True\n                    del self.manifest.archives[info.name]\n                    continue\n                mark_as_possibly_superseded(archive_id)\n                cdata = self.repository.get(archive_id)\n                try:\n                    _, data = self.repo_objs.parse(archive_id, cdata)\n                except IntegrityError as integrity_error:\n                    logger.error(\"Archive metadata block %s is corrupted: %s\", bin_to_hex(archive_id), integrity_error)\n                    self.error_found = True\n                    del self.manifest.archives[info.name]\n                    continue\n                archive = ArchiveItem(internal_dict=msgpack.unpackb(data))\n                if archive.version != 2:\n                    raise Exception(\"Unknown archive metadata version\")\n                items_buffer = ChunkBuffer(self.key)\n                items_buffer.write_chunk = add_callback\n                for item in robust_iterator(archive):\n                    if \"chunks\" in item:\n                        verify_file_chunks(info.name, item)\n                    items_buffer.add(item)\n                items_buffer.flush(flush=True)\n                for previous_item_id in archive_get_items(\n                    archive, repo_objs=self.repo_objs, repository=self.repository\n                ):\n                    mark_as_possibly_superseded(previous_item_id)\n                for previous_item_ptr in archive.item_ptrs:\n                    mark_as_possibly_superseded(previous_item_ptr)\n                archive.item_ptrs = archive_put_items(\n                    items_buffer.chunks, repo_objs=self.repo_objs, add_reference=add_reference\n                )\n                data = msgpack.packb(archive.as_dict())\n                new_archive_id = self.key.id_hash(data)\n                cdata = self.repo_objs.format(new_archive_id, {}, data)\n                add_reference(new_archive_id, len(data), cdata)\n                self.manifest.archives[info.name] = (new_archive_id, info.ts)\n            pi.finish()\n\n    def orphan_chunks_check(self):\n        if self.check_all:\n            unused = {id_ for id_, entry in self.chunks.iteritems() if entry.refcount == 0}\n            orphaned = unused - self.possibly_superseded\n            if orphaned:\n                logger.error(f\"{len(orphaned)} orphaned objects found!\")\n                for chunk_id in orphaned:\n                    logger.debug(f\"chunk {bin_to_hex(chunk_id)} is orphaned.\")\n                self.error_found = True\n            if self.repair and unused:\n                logger.info(\n                    \"Deleting %d orphaned and %d superseded objects...\" % (len(orphaned), len(self.possibly_superseded))\n                )\n                for id_ in unused:\n                    self.repository.delete(id_)\n                logger.info(\"Finished deleting orphaned/superseded objects.\")\n        else:\n            logger.info(\"Orphaned objects check skipped (needs all archives checked).\")\n\n    def finish(self):\n        if self.repair:\n            logger.info(\"Writing Manifest.\")\n            self.manifest.write()\n            logger.info(\"Committing repo.\")\n            self.repository.commit(compact=False)\n\n\nclass ArchiveRecreater:\n    class Interrupted(Exception):\n        def __init__(self, metadata=None):\n            self.metadata = metadata or {}\n\n    @staticmethod\n    def is_temporary_archive(archive_name):\n        return archive_name.endswith(\".recreate\")\n\n    def __init__(\n        self,\n        manifest,\n        cache,\n        matcher,\n        exclude_caches=False,\n        exclude_if_present=None,\n        keep_exclude_tags=False,\n        chunker_params=None,\n        compression=None,\n        recompress=False,\n        always_recompress=False,\n        dry_run=False,\n        stats=False,\n        progress=False,\n        file_status_printer=None,\n        timestamp=None,\n        checkpoint_interval=1800,\n        checkpoint_volume=0,\n    ):\n        self.manifest = manifest\n        self.repository = manifest.repository\n        self.key = manifest.key\n        self.repo_objs = manifest.repo_objs\n        self.cache = cache\n\n        self.matcher = matcher\n        self.exclude_caches = exclude_caches\n        self.exclude_if_present = exclude_if_present or []\n        self.keep_exclude_tags = keep_exclude_tags\n\n        self.rechunkify = chunker_params is not None\n        if self.rechunkify:\n            logger.debug(\"Rechunking archives to %s\", chunker_params)\n        self.chunker_params = chunker_params or CHUNKER_PARAMS\n        self.recompress = recompress\n        self.always_recompress = always_recompress\n        self.compression = compression or CompressionSpec(\"none\")\n        self.seen_chunks = set()\n\n        self.timestamp = timestamp\n        self.dry_run = dry_run\n        self.stats = stats\n        self.progress = progress\n        self.print_file_status = file_status_printer or (lambda *args: None)\n        self.checkpoint_interval = None if dry_run else checkpoint_interval\n        self.checkpoint_volume = None if dry_run else checkpoint_volume\n\n    def recreate(self, archive_name, comment=None, target_name=None):\n        assert not self.is_temporary_archive(archive_name)\n        archive = self.open_archive(archive_name)\n        target = self.create_target(archive, target_name)\n        if self.exclude_if_present or self.exclude_caches:\n            self.matcher_add_tagged_dirs(archive)\n        if (\n            self.matcher.empty()\n            and not self.recompress\n            and not target.recreate_rechunkify\n            and comment is None\n            and target_name is None\n        ):\n            # nothing to do\n            return False\n        self.process_items(archive, target)\n        replace_original = target_name is None\n        self.save(archive, target, comment, replace_original=replace_original)\n        return True\n\n    def process_items(self, archive, target):\n        matcher = self.matcher\n\n        for item in archive.iter_items():\n            if not matcher.match(item.path):\n                self.print_file_status(\"-\", item.path)  # excluded (either by \"-\" or by \"!\")\n                continue\n            if self.dry_run:\n                self.print_file_status(\"+\", item.path)  # included\n            else:\n                self.process_item(archive, target, item)\n        if self.progress:\n            target.stats.show_progress(final=True)\n\n    def process_item(self, archive, target, item):\n        status = file_status(item.mode)\n        if \"chunks\" in item:\n            self.print_file_status(status, item.path)\n            status = None\n            self.process_chunks(archive, target, item)\n            target.stats.nfiles += 1\n        target.add_item(item, stats=target.stats)\n        self.print_file_status(status, item.path)\n\n    def process_chunks(self, archive, target, item):\n        if not self.recompress and not target.recreate_rechunkify:\n            for chunk_id, size in item.chunks:\n                self.cache.chunk_incref(chunk_id, target.stats)\n            return item.chunks\n        chunk_iterator = self.iter_chunks(archive, target, list(item.chunks))\n        chunk_processor = partial(self.chunk_processor, target)\n        target.process_file_chunks(item, self.cache, target.stats, self.progress, chunk_iterator, chunk_processor)\n\n    def chunk_processor(self, target, chunk):\n        chunk_id, data = cached_hash(chunk, self.key.id_hash)\n        if chunk_id in self.seen_chunks:\n            return self.cache.chunk_incref(chunk_id, target.stats)\n        overwrite = self.recompress\n        if self.recompress and not self.always_recompress and chunk_id in self.cache.chunks:\n            # Check if this chunk is already compressed the way we want it\n            old_meta = self.repo_objs.parse_meta(chunk_id, self.repository.get(chunk_id, read_data=False))\n            compr_hdr = bytes((old_meta[\"ctype\"], old_meta[\"clevel\"]))\n            compressor_cls, level = Compressor.detect(compr_hdr)\n            if (\n                compressor_cls.name == self.repo_objs.compressor.decide({}, data).name\n                and level == self.repo_objs.compressor.level\n            ):\n                # Stored chunk has the same compression method and level as we wanted\n                overwrite = False\n        chunk_entry = self.cache.add_chunk(chunk_id, {}, data, stats=target.stats, overwrite=overwrite, wait=False)\n        self.cache.repository.async_response(wait=False)\n        self.seen_chunks.add(chunk_entry.id)\n        return chunk_entry\n\n    def iter_chunks(self, archive, target, chunks):\n        chunk_iterator = archive.pipeline.fetch_many([chunk_id for chunk_id, _ in chunks])\n        if target.recreate_rechunkify:\n            # The target.chunker will read the file contents through ChunkIteratorFileWrapper chunk-by-chunk\n            # (does not load the entire file into memory)\n            file = ChunkIteratorFileWrapper(chunk_iterator)\n            yield from target.chunker.chunkify(file)\n        else:\n            for chunk in chunk_iterator:\n                yield Chunk(chunk, size=len(chunk), allocation=CH_DATA)\n\n    def save(self, archive, target, comment=None, replace_original=True):\n        if self.dry_run:\n            return\n        if comment is None:\n            comment = archive.metadata.get(\"comment\", \"\")\n\n        # Keep for the statistics if necessary\n        if self.stats:\n            _start = target.start\n\n        if self.timestamp is None:\n            additional_metadata = {\n                \"time\": archive.metadata.time,\n                \"time_end\": archive.metadata.get(\"time_end\") or archive.metadata.time,\n                \"command_line\": archive.metadata.command_line,\n                # but also remember recreate metadata:\n                \"recreate_command_line\": join_cmd(sys.argv),\n            }\n        else:\n            additional_metadata = {\n                \"command_line\": archive.metadata.command_line,\n                # but also remember recreate metadata:\n                \"recreate_command_line\": join_cmd(sys.argv),\n            }\n\n        target.save(comment=comment, timestamp=self.timestamp, additional_metadata=additional_metadata)\n        if replace_original:\n            archive.delete(Statistics(), progress=self.progress)\n            target.rename(archive.name)\n        if self.stats:\n            target.start = _start\n            target.end = archive_ts_now()\n            log_multi(str(target), str(target.stats))\n\n    def matcher_add_tagged_dirs(self, archive):\n        \"\"\"Add excludes to the matcher created by exclude_cache and exclude_if_present.\"\"\"\n\n        def exclude(dir, tag_item):\n            if self.keep_exclude_tags:\n                tag_files.append(PathPrefixPattern(tag_item.path, recurse_dir=False))\n                tagged_dirs.append(FnmatchPattern(dir + \"/\", recurse_dir=False))\n            else:\n                tagged_dirs.append(PathPrefixPattern(dir, recurse_dir=False))\n\n        matcher = self.matcher\n        tag_files = []\n        tagged_dirs = []\n\n        for item in archive.iter_items(\n            filter=lambda item: os.path.basename(item.path) == CACHE_TAG_NAME or matcher.match(item.path)\n        ):\n            dir, tag_file = os.path.split(item.path)\n            if tag_file in self.exclude_if_present:\n                exclude(dir, item)\n            elif self.exclude_caches and tag_file == CACHE_TAG_NAME and stat.S_ISREG(item.mode):\n                file = open_item(archive, item)\n                if file.read(len(CACHE_TAG_CONTENTS)) == CACHE_TAG_CONTENTS:\n                    exclude(dir, item)\n        matcher.add(tag_files, IECommand.Include)\n        matcher.add(tagged_dirs, IECommand.ExcludeNoRecurse)\n\n    def create_target(self, archive, target_name=None):\n        \"\"\"Create target archive.\"\"\"\n        target_name = target_name or archive.name + \".recreate\"\n        target = self.create_target_archive(target_name)\n        # If the archives use the same chunker params, then don't rechunkify\n        source_chunker_params = tuple(archive.metadata.get(\"chunker_params\", []))\n        if len(source_chunker_params) == 4 and isinstance(source_chunker_params[0], int):\n            # this is a borg < 1.2 chunker_params tuple, no chunker algo specified, but we only had buzhash:\n            source_chunker_params = (CH_BUZHASH,) + source_chunker_params\n        target.recreate_rechunkify = self.rechunkify and source_chunker_params != target.chunker_params\n        if target.recreate_rechunkify:\n            logger.debug(\n                \"Rechunking archive from %s to %s\", source_chunker_params or \"(unknown)\", target.chunker_params\n            )\n        target.process_file_chunks = ChunksProcessor(\n            cache=self.cache,\n            key=self.key,\n            add_item=target.add_item,\n            prepare_checkpoint=target.prepare_checkpoint,\n            write_checkpoint=target.write_checkpoint,\n            checkpoint_interval=self.checkpoint_interval,\n            checkpoint_volume=self.checkpoint_volume,\n            rechunkify=target.recreate_rechunkify,\n        ).process_file_chunks\n        target.chunker = get_chunker(*target.chunker_params, seed=self.key.chunk_seed, sparse=False)\n        return target\n\n    def create_target_archive(self, name):\n        target = Archive(\n            self.manifest,\n            name,\n            create=True,\n            progress=self.progress,\n            chunker_params=self.chunker_params,\n            cache=self.cache,\n        )\n        return target\n\n    def open_archive(self, name, **kwargs):\n        return Archive(self.manifest, name, cache=self.cache, **kwargs)\n", "import configparser\nimport os\nimport shutil\nimport stat\nfrom binascii import unhexlify\nfrom collections import namedtuple\nfrom time import perf_counter\n\nfrom .logger import create_logger\n\nlogger = create_logger()\n\nfiles_cache_logger = create_logger(\"borg.debug.files_cache\")\n\nfrom .constants import CACHE_README, FILES_CACHE_MODE_DISABLED\nfrom .hashindex import ChunkIndex, ChunkIndexEntry, CacheSynchronizer\nfrom .helpers import Location\nfrom .helpers import Error\nfrom .helpers import get_cache_dir, get_security_dir\nfrom .helpers import bin_to_hex, parse_stringified_list\nfrom .helpers import format_file_size\nfrom .helpers import safe_ns\nfrom .helpers import yes\nfrom .helpers import remove_surrogates\nfrom .helpers import ProgressIndicatorPercent, ProgressIndicatorMessage\nfrom .helpers import set_ec, EXIT_WARNING\nfrom .helpers import safe_unlink\nfrom .helpers import msgpack\nfrom .helpers.msgpack import int_to_timestamp, timestamp_to_int\nfrom .item import ArchiveItem, ChunkListEntry\nfrom .crypto.key import PlaintextKey\nfrom .crypto.file_integrity import IntegrityCheckedFile, DetachedIntegrityCheckedFile, FileIntegrityError\nfrom .locking import Lock\nfrom .manifest import Manifest\nfrom .platform import SaveFile\nfrom .remote import cache_if_remote\nfrom .repository import LIST_SCAN_LIMIT\n\n# note: cmtime might me either a ctime or a mtime timestamp\nFileCacheEntry = namedtuple(\"FileCacheEntry\", \"age inode size cmtime chunk_ids\")\n\n\nclass SecurityManager:\n    \"\"\"\n    Tracks repositories. Ensures that nothing bad happens (repository swaps,\n    replay attacks, unknown repositories etc.).\n\n    This is complicated by the Cache being initially used for this, while\n    only some commands actually use the Cache, which meant that other commands\n    did not perform these checks.\n\n    Further complications were created by the Cache being a cache, so it\n    could be legitimately deleted, which is annoying because Borg didn't\n    recognize repositories after that.\n\n    Therefore a second location, the security database (see get_security_dir),\n    was introduced which stores this information. However, this means that\n    the code has to deal with a cache existing but no security DB entry,\n    or inconsistencies between the security DB and the cache which have to\n    be reconciled, and also with no cache existing but a security DB entry.\n    \"\"\"\n\n    def __init__(self, repository):\n        self.repository = repository\n        self.dir = get_security_dir(repository.id_str, legacy=(repository.version == 1))\n        self.cache_dir = cache_dir(repository)\n        self.key_type_file = os.path.join(self.dir, \"key-type\")\n        self.location_file = os.path.join(self.dir, \"location\")\n        self.manifest_ts_file = os.path.join(self.dir, \"manifest-timestamp\")\n\n    @staticmethod\n    def destroy(repository, path=None):\n        \"\"\"destroy the security dir for ``repository`` or at ``path``\"\"\"\n        path = path or get_security_dir(repository.id_str, legacy=(repository.version == 1))\n        if os.path.exists(path):\n            shutil.rmtree(path)\n\n    def known(self):\n        return all(os.path.exists(f) for f in (self.key_type_file, self.location_file, self.manifest_ts_file))\n\n    def key_matches(self, key):\n        if not self.known():\n            return False\n        try:\n            with open(self.key_type_file) as fd:\n                type = fd.read()\n                return type == str(key.TYPE)\n        except OSError as exc:\n            logger.warning(\"Could not read/parse key type file: %s\", exc)\n\n    def save(self, manifest, key):\n        logger.debug(\"security: saving state for %s to %s\", self.repository.id_str, self.dir)\n        current_location = self.repository._location.canonical_path()\n        logger.debug(\"security: current location   %s\", current_location)\n        logger.debug(\"security: key type           %s\", str(key.TYPE))\n        logger.debug(\"security: manifest timestamp %s\", manifest.timestamp)\n        with SaveFile(self.location_file) as fd:\n            fd.write(current_location)\n        with SaveFile(self.key_type_file) as fd:\n            fd.write(str(key.TYPE))\n        with SaveFile(self.manifest_ts_file) as fd:\n            fd.write(manifest.timestamp)\n\n    def assert_location_matches(self, cache_config=None):\n        # Warn user before sending data to a relocated repository\n        try:\n            with open(self.location_file) as fd:\n                previous_location = fd.read()\n            logger.debug(\"security: read previous location %r\", previous_location)\n        except FileNotFoundError:\n            logger.debug(\"security: previous location file %s not found\", self.location_file)\n            previous_location = None\n        except OSError as exc:\n            logger.warning(\"Could not read previous location file: %s\", exc)\n            previous_location = None\n        if cache_config and cache_config.previous_location and previous_location != cache_config.previous_location:\n            # Reconcile cache and security dir; we take the cache location.\n            previous_location = cache_config.previous_location\n            logger.debug(\"security: using previous_location of cache: %r\", previous_location)\n\n        repository_location = self.repository._location.canonical_path()\n        if previous_location and previous_location != repository_location:\n            msg = (\n                \"Warning: The repository at location {} was previously located at {}\\n\".format(\n                    repository_location, previous_location\n                )\n                + \"Do you want to continue? [yN] \"\n            )\n            if not yes(\n                msg,\n                false_msg=\"Aborting.\",\n                invalid_msg=\"Invalid answer, aborting.\",\n                retry=False,\n                env_var_override=\"BORG_RELOCATED_REPO_ACCESS_IS_OK\",\n            ):\n                raise Cache.RepositoryAccessAborted()\n            # adapt on-disk config immediately if the new location was accepted\n            logger.debug(\"security: updating location stored in cache and security dir\")\n            with SaveFile(self.location_file) as fd:\n                fd.write(repository_location)\n            if cache_config:\n                cache_config.save()\n\n    def assert_no_manifest_replay(self, manifest, key, cache_config=None):\n        try:\n            with open(self.manifest_ts_file) as fd:\n                timestamp = fd.read()\n            logger.debug(\"security: read manifest timestamp %r\", timestamp)\n        except FileNotFoundError:\n            logger.debug(\"security: manifest timestamp file %s not found\", self.manifest_ts_file)\n            timestamp = \"\"\n        except OSError as exc:\n            logger.warning(\"Could not read previous location file: %s\", exc)\n            timestamp = \"\"\n        if cache_config:\n            timestamp = max(timestamp, cache_config.timestamp or \"\")\n        logger.debug(\"security: determined newest manifest timestamp as %s\", timestamp)\n        # If repository is older than the cache or security dir something fishy is going on\n        if timestamp and timestamp > manifest.timestamp:\n            if isinstance(key, PlaintextKey):\n                raise Cache.RepositoryIDNotUnique()\n            else:\n                raise Cache.RepositoryReplay()\n\n    def assert_key_type(self, key, cache_config=None):\n        # Make sure an encrypted repository has not been swapped for an unencrypted repository\n        if cache_config and cache_config.key_type is not None and cache_config.key_type != str(key.TYPE):\n            raise Cache.EncryptionMethodMismatch()\n        if self.known() and not self.key_matches(key):\n            raise Cache.EncryptionMethodMismatch()\n\n    def assert_secure(self, manifest, key, *, cache_config=None, warn_if_unencrypted=True, lock_wait=None):\n        # warn_if_unencrypted=False is only used for initializing a new repository.\n        # Thus, avoiding asking about a repository that's currently initializing.\n        self.assert_access_unknown(warn_if_unencrypted, manifest, key)\n        if cache_config:\n            self._assert_secure(manifest, key, cache_config)\n        else:\n            cache_config = CacheConfig(self.repository, lock_wait=lock_wait)\n            if cache_config.exists():\n                with cache_config:\n                    self._assert_secure(manifest, key, cache_config)\n            else:\n                self._assert_secure(manifest, key)\n        logger.debug(\"security: repository checks ok, allowing access\")\n\n    def _assert_secure(self, manifest, key, cache_config=None):\n        self.assert_location_matches(cache_config)\n        self.assert_key_type(key, cache_config)\n        self.assert_no_manifest_replay(manifest, key, cache_config)\n        if not self.known():\n            logger.debug(\"security: remembering previously unknown repository\")\n            self.save(manifest, key)\n\n    def assert_access_unknown(self, warn_if_unencrypted, manifest, key):\n        # warn_if_unencrypted=False is only used for initializing a new repository.\n        # Thus, avoiding asking about a repository that's currently initializing.\n        if not key.logically_encrypted and not self.known():\n            msg = (\n                \"Warning: Attempting to access a previously unknown unencrypted repository!\\n\"\n                + \"Do you want to continue? [yN] \"\n            )\n            allow_access = not warn_if_unencrypted or yes(\n                msg,\n                false_msg=\"Aborting.\",\n                invalid_msg=\"Invalid answer, aborting.\",\n                retry=False,\n                env_var_override=\"BORG_UNKNOWN_UNENCRYPTED_REPO_ACCESS_IS_OK\",\n            )\n            if allow_access:\n                if warn_if_unencrypted:\n                    logger.debug(\"security: remembering unknown unencrypted repository (explicitly allowed)\")\n                else:\n                    logger.debug(\"security: initializing unencrypted repository\")\n                self.save(manifest, key)\n            else:\n                raise Cache.CacheInitAbortedError()\n\n\ndef assert_secure(repository, manifest, lock_wait):\n    sm = SecurityManager(repository)\n    sm.assert_secure(manifest, manifest.key, lock_wait=lock_wait)\n\n\ndef recanonicalize_relative_location(cache_location, repository):\n    # borg < 1.0.8rc1 had different canonicalization for the repo location (see #1655 and #1741).\n    repo_location = repository._location.canonical_path()\n    rl = Location(repo_location)\n    cl = Location(cache_location)\n    if (\n        cl.proto == rl.proto\n        and cl.user == rl.user\n        and cl.host == rl.host\n        and cl.port == rl.port\n        and cl.path\n        and rl.path\n        and cl.path.startswith(\"/~/\")\n        and rl.path.startswith(\"/./\")\n        and cl.path[3:] == rl.path[3:]\n    ):\n        # everything is same except the expected change in relative path canonicalization,\n        # update previous_location to avoid warning / user query about changed location:\n        return repo_location\n    else:\n        return cache_location\n\n\ndef cache_dir(repository, path=None):\n    return path or os.path.join(get_cache_dir(), repository.id_str)\n\n\ndef files_cache_name():\n    suffix = os.environ.get(\"BORG_FILES_CACHE_SUFFIX\", \"\")\n    return \"files.\" + suffix if suffix else \"files\"\n\n\ndef discover_files_cache_name(path):\n    return [fn for fn in os.listdir(path) if fn == \"files\" or fn.startswith(\"files.\")][0]\n\n\nclass CacheConfig:\n    def __init__(self, repository, path=None, lock_wait=None):\n        self.repository = repository\n        self.path = cache_dir(repository, path)\n        logger.debug(\"Using %s as cache\", self.path)\n        self.config_path = os.path.join(self.path, \"config\")\n        self.lock = None\n        self.lock_wait = lock_wait\n\n    def __enter__(self):\n        self.open()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def exists(self):\n        return os.path.exists(self.config_path)\n\n    def create(self):\n        assert not self.exists()\n        config = configparser.ConfigParser(interpolation=None)\n        config.add_section(\"cache\")\n        config.set(\"cache\", \"version\", \"1\")\n        config.set(\"cache\", \"repository\", self.repository.id_str)\n        config.set(\"cache\", \"manifest\", \"\")\n        config.add_section(\"integrity\")\n        config.set(\"integrity\", \"manifest\", \"\")\n        with SaveFile(self.config_path) as fd:\n            config.write(fd)\n\n    def open(self):\n        self.lock = Lock(os.path.join(self.path, \"lock\"), exclusive=True, timeout=self.lock_wait).acquire()\n        self.load()\n\n    def load(self):\n        self._config = configparser.ConfigParser(interpolation=None)\n        with open(self.config_path) as fd:\n            self._config.read_file(fd)\n        self._check_upgrade(self.config_path)\n        self.id = self._config.get(\"cache\", \"repository\")\n        self.manifest_id = unhexlify(self._config.get(\"cache\", \"manifest\"))\n        self.timestamp = self._config.get(\"cache\", \"timestamp\", fallback=None)\n        self.key_type = self._config.get(\"cache\", \"key_type\", fallback=None)\n        self.ignored_features = set(parse_stringified_list(self._config.get(\"cache\", \"ignored_features\", fallback=\"\")))\n        self.mandatory_features = set(\n            parse_stringified_list(self._config.get(\"cache\", \"mandatory_features\", fallback=\"\"))\n        )\n        try:\n            self.integrity = dict(self._config.items(\"integrity\"))\n            if self._config.get(\"cache\", \"manifest\") != self.integrity.pop(\"manifest\"):\n                # The cache config file is updated (parsed with ConfigParser, the state of the ConfigParser\n                # is modified and then written out.), not re-created.\n                # Thus, older versions will leave our [integrity] section alone, making the section's data invalid.\n                # Therefore, we also add the manifest ID to this section and\n                # can discern whether an older version interfered by comparing the manifest IDs of this section\n                # and the main [cache] section.\n                self.integrity = {}\n                logger.warning(\"Cache integrity data not available: old Borg version modified the cache.\")\n        except configparser.NoSectionError:\n            logger.debug(\"Cache integrity: No integrity data found (files, chunks). Cache is from old version.\")\n            self.integrity = {}\n        previous_location = self._config.get(\"cache\", \"previous_location\", fallback=None)\n        if previous_location:\n            self.previous_location = recanonicalize_relative_location(previous_location, self.repository)\n        else:\n            self.previous_location = None\n        self._config.set(\"cache\", \"previous_location\", self.repository._location.canonical_path())\n\n    def save(self, manifest=None, key=None):\n        if manifest:\n            self._config.set(\"cache\", \"manifest\", manifest.id_str)\n            self._config.set(\"cache\", \"timestamp\", manifest.timestamp)\n            self._config.set(\"cache\", \"ignored_features\", \",\".join(self.ignored_features))\n            self._config.set(\"cache\", \"mandatory_features\", \",\".join(self.mandatory_features))\n            if not self._config.has_section(\"integrity\"):\n                self._config.add_section(\"integrity\")\n            for file, integrity_data in self.integrity.items():\n                self._config.set(\"integrity\", file, integrity_data)\n            self._config.set(\"integrity\", \"manifest\", manifest.id_str)\n        if key:\n            self._config.set(\"cache\", \"key_type\", str(key.TYPE))\n        with SaveFile(self.config_path) as fd:\n            self._config.write(fd)\n\n    def close(self):\n        if self.lock is not None:\n            self.lock.release()\n            self.lock = None\n\n    def _check_upgrade(self, config_path):\n        try:\n            cache_version = self._config.getint(\"cache\", \"version\")\n            wanted_version = 1\n            if cache_version != wanted_version:\n                self.close()\n                raise Exception(\n                    \"%s has unexpected cache version %d (wanted: %d).\" % (config_path, cache_version, wanted_version)\n                )\n        except configparser.NoSectionError:\n            self.close()\n            raise Exception(\"%s does not look like a Borg cache.\" % config_path) from None\n\n\nclass Cache:\n    \"\"\"Client Side cache\"\"\"\n\n    class RepositoryIDNotUnique(Error):\n        \"\"\"Cache is newer than repository - do you have multiple, independently updated repos with same ID?\"\"\"\n\n    class RepositoryReplay(Error):\n        \"\"\"Cache, or information obtained from the security directory is newer than repository - this is either an attack or unsafe (multiple repos with same ID)\"\"\"\n\n    class CacheInitAbortedError(Error):\n        \"\"\"Cache initialization aborted\"\"\"\n\n    class RepositoryAccessAborted(Error):\n        \"\"\"Repository access aborted\"\"\"\n\n    class EncryptionMethodMismatch(Error):\n        \"\"\"Repository encryption method changed since last access, refusing to continue\"\"\"\n\n    @staticmethod\n    def break_lock(repository, path=None):\n        path = cache_dir(repository, path)\n        Lock(os.path.join(path, \"lock\"), exclusive=True).break_lock()\n\n    @staticmethod\n    def destroy(repository, path=None):\n        \"\"\"destroy the cache for ``repository`` or at ``path``\"\"\"\n        path = path or os.path.join(get_cache_dir(), repository.id_str)\n        config = os.path.join(path, \"config\")\n        if os.path.exists(config):\n            os.remove(config)  # kill config first\n            shutil.rmtree(path)\n\n    def __new__(\n        cls,\n        repository,\n        manifest,\n        path=None,\n        sync=True,\n        warn_if_unencrypted=True,\n        progress=False,\n        lock_wait=None,\n        permit_adhoc_cache=False,\n        cache_mode=FILES_CACHE_MODE_DISABLED,\n        iec=False,\n    ):\n        def local():\n            return LocalCache(\n                manifest=manifest,\n                path=path,\n                sync=sync,\n                warn_if_unencrypted=warn_if_unencrypted,\n                progress=progress,\n                iec=iec,\n                lock_wait=lock_wait,\n                cache_mode=cache_mode,\n            )\n\n        def adhoc():\n            return AdHocCache(manifest=manifest, lock_wait=lock_wait, iec=iec)\n\n        if not permit_adhoc_cache:\n            return local()\n\n        # ad-hoc cache may be permitted, but if the local cache is in sync it'd be stupid to invalidate\n        # it by needlessly using the ad-hoc cache.\n        # Check if the local cache exists and is in sync.\n\n        cache_config = CacheConfig(repository, path, lock_wait)\n        if cache_config.exists():\n            with cache_config:\n                cache_in_sync = cache_config.manifest_id == manifest.id\n            # Don't nest cache locks\n            if cache_in_sync:\n                # Local cache is in sync, use it\n                logger.debug(\"Cache: choosing local cache (in sync)\")\n                return local()\n        logger.debug(\"Cache: choosing ad-hoc cache (local cache does not exist or is not in sync)\")\n        return adhoc()\n\n\nclass CacheStatsMixin:\n    str_format = \"\"\"\\\nOriginal size: {0.total_size}\nDeduplicated size: {0.unique_size}\nUnique chunks: {0.total_unique_chunks}\nTotal chunks: {0.total_chunks}\n\"\"\"\n\n    def __init__(self, iec=False):\n        self.iec = iec\n\n    def __str__(self):\n        return self.str_format.format(self.format_tuple())\n\n    Summary = namedtuple(\"Summary\", [\"total_size\", \"unique_size\", \"total_unique_chunks\", \"total_chunks\"])\n\n    def stats(self):\n        from .archive import Archive\n\n        # XXX: this should really be moved down to `hashindex.pyx`\n        total_size, unique_size, total_unique_chunks, total_chunks = self.chunks.summarize()\n        # since borg 1.2 we have new archive metadata telling the total size per archive,\n        # so we can just sum up all archives to get the \"all archives\" stats:\n        total_size = 0\n        for archive_name in self.manifest.archives:\n            archive = Archive(self.manifest, archive_name)\n            stats = archive.calc_stats(self, want_unique=False)\n            total_size += stats.osize\n        stats = self.Summary(total_size, unique_size, total_unique_chunks, total_chunks)._asdict()\n        return stats\n\n    def format_tuple(self):\n        stats = self.stats()\n        for field in [\"total_size\", \"unique_size\"]:\n            stats[field] = format_file_size(stats[field], iec=self.iec)\n        return self.Summary(**stats)\n\n\nclass LocalCache(CacheStatsMixin):\n    \"\"\"\n    Persistent, local (client-side) cache.\n    \"\"\"\n\n    def __init__(\n        self,\n        manifest,\n        path=None,\n        sync=True,\n        warn_if_unencrypted=True,\n        progress=False,\n        lock_wait=None,\n        cache_mode=FILES_CACHE_MODE_DISABLED,\n        iec=False,\n    ):\n        \"\"\"\n        :param warn_if_unencrypted: print warning if accessing unknown unencrypted repository\n        :param lock_wait: timeout for lock acquisition (int [s] or None [wait forever])\n        :param sync: do :meth:`.sync`\n        :param cache_mode: what shall be compared in the file stat infos vs. cached stat infos comparison\n        \"\"\"\n        CacheStatsMixin.__init__(self, iec=iec)\n        assert isinstance(manifest, Manifest)\n        self.manifest = manifest\n        self.repository = manifest.repository\n        self.key = manifest.key\n        self.repo_objs = manifest.repo_objs\n        self.progress = progress\n        self.cache_mode = cache_mode\n        self.timestamp = None\n        self.txn_active = False\n\n        self.path = cache_dir(self.repository, path)\n        self.security_manager = SecurityManager(self.repository)\n        self.cache_config = CacheConfig(self.repository, self.path, lock_wait)\n\n        # Warn user before sending data to a never seen before unencrypted repository\n        if not os.path.exists(self.path):\n            self.security_manager.assert_access_unknown(warn_if_unencrypted, manifest, self.key)\n            self.create()\n\n        self.open()\n        try:\n            self.security_manager.assert_secure(manifest, self.key, cache_config=self.cache_config)\n\n            if not self.check_cache_compatibility():\n                self.wipe_cache()\n\n            self.update_compatibility()\n\n            if sync and self.manifest.id != self.cache_config.manifest_id:\n                self.sync()\n                self.commit()\n        except:  # noqa\n            self.close()\n            raise\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def create(self):\n        \"\"\"Create a new empty cache at `self.path`\"\"\"\n        os.makedirs(self.path)\n        with open(os.path.join(self.path, \"README\"), \"w\") as fd:\n            fd.write(CACHE_README)\n        self.cache_config.create()\n        ChunkIndex().write(os.path.join(self.path, \"chunks\"))\n        os.makedirs(os.path.join(self.path, \"chunks.archive.d\"))\n        with SaveFile(os.path.join(self.path, files_cache_name()), binary=True):\n            pass  # empty file\n\n    def _do_open(self):\n        self.cache_config.load()\n        with IntegrityCheckedFile(\n            path=os.path.join(self.path, \"chunks\"),\n            write=False,\n            integrity_data=self.cache_config.integrity.get(\"chunks\"),\n        ) as fd:\n            self.chunks = ChunkIndex.read(fd)\n        if \"d\" in self.cache_mode:  # d(isabled)\n            self.files = None\n        else:\n            self._read_files()\n\n    def open(self):\n        if not os.path.isdir(self.path):\n            raise Exception(\"%s Does not look like a Borg cache\" % self.path)\n        self.cache_config.open()\n        self.rollback()\n\n    def close(self):\n        if self.cache_config is not None:\n            self.cache_config.close()\n            self.cache_config = None\n\n    def _read_files(self):\n        self.files = {}\n        self._newest_cmtime = None\n        logger.debug(\"Reading files cache ...\")\n        files_cache_logger.debug(\"FILES-CACHE-LOAD: starting...\")\n        msg = None\n        try:\n            with IntegrityCheckedFile(\n                path=os.path.join(self.path, files_cache_name()),\n                write=False,\n                integrity_data=self.cache_config.integrity.get(files_cache_name()),\n            ) as fd:\n                u = msgpack.Unpacker(use_list=True)\n                while True:\n                    data = fd.read(64 * 1024)\n                    if not data:\n                        break\n                    u.feed(data)\n                    try:\n                        for path_hash, item in u:\n                            entry = FileCacheEntry(*item)\n                            # in the end, this takes about 240 Bytes per file\n                            self.files[path_hash] = msgpack.packb(entry._replace(age=entry.age + 1))\n                    except (TypeError, ValueError) as exc:\n                        msg = \"The files cache seems invalid. [%s]\" % str(exc)\n                        break\n        except OSError as exc:\n            msg = \"The files cache can't be read. [%s]\" % str(exc)\n        except FileIntegrityError as fie:\n            msg = \"The files cache is corrupted. [%s]\" % str(fie)\n        if msg is not None:\n            logger.warning(msg)\n            logger.warning(\"Continuing without files cache - expect lower performance.\")\n            self.files = {}\n        files_cache_logger.debug(\"FILES-CACHE-LOAD: finished, %d entries loaded.\", len(self.files))\n\n    def begin_txn(self):\n        # Initialize transaction snapshot\n        pi = ProgressIndicatorMessage(msgid=\"cache.begin_transaction\")\n        txn_dir = os.path.join(self.path, \"txn.tmp\")\n        os.mkdir(txn_dir)\n        pi.output(\"Initializing cache transaction: Reading config\")\n        shutil.copy(os.path.join(self.path, \"config\"), txn_dir)\n        pi.output(\"Initializing cache transaction: Reading chunks\")\n        shutil.copy(os.path.join(self.path, \"chunks\"), txn_dir)\n        pi.output(\"Initializing cache transaction: Reading files\")\n        try:\n            shutil.copy(os.path.join(self.path, files_cache_name()), txn_dir)\n        except FileNotFoundError:\n            with SaveFile(os.path.join(txn_dir, files_cache_name()), binary=True):\n                pass  # empty file\n        os.replace(txn_dir, os.path.join(self.path, \"txn.active\"))\n        self.txn_active = True\n        pi.finish()\n\n    def commit(self):\n        \"\"\"Commit transaction\"\"\"\n        if not self.txn_active:\n            return\n        self.security_manager.save(self.manifest, self.key)\n        pi = ProgressIndicatorMessage(msgid=\"cache.commit\")\n        if self.files is not None:\n            if self._newest_cmtime is None:\n                # was never set because no files were modified/added\n                self._newest_cmtime = 2**63 - 1  # nanoseconds, good until y2262\n            ttl = int(os.environ.get(\"BORG_FILES_CACHE_TTL\", 20))\n            pi.output(\"Saving files cache\")\n            files_cache_logger.debug(\"FILES-CACHE-SAVE: starting...\")\n            with IntegrityCheckedFile(path=os.path.join(self.path, files_cache_name()), write=True) as fd:\n                entry_count = 0\n                for path_hash, item in self.files.items():\n                    # Only keep files seen in this backup that are older than newest cmtime seen in this backup -\n                    # this is to avoid issues with filesystem snapshots and cmtime granularity.\n                    # Also keep files from older backups that have not reached BORG_FILES_CACHE_TTL yet.\n                    entry = FileCacheEntry(*msgpack.unpackb(item))\n                    if (\n                        entry.age == 0\n                        and timestamp_to_int(entry.cmtime) < self._newest_cmtime\n                        or entry.age > 0\n                        and entry.age < ttl\n                    ):\n                        msgpack.pack((path_hash, entry), fd)\n                        entry_count += 1\n            files_cache_logger.debug(\"FILES-CACHE-KILL: removed all old entries with age >= TTL [%d]\", ttl)\n            files_cache_logger.debug(\n                \"FILES-CACHE-KILL: removed all current entries with newest cmtime %d\", self._newest_cmtime\n            )\n            files_cache_logger.debug(\"FILES-CACHE-SAVE: finished, %d remaining entries saved.\", entry_count)\n            self.cache_config.integrity[files_cache_name()] = fd.integrity_data\n        pi.output(\"Saving chunks cache\")\n        with IntegrityCheckedFile(path=os.path.join(self.path, \"chunks\"), write=True) as fd:\n            self.chunks.write(fd)\n        self.cache_config.integrity[\"chunks\"] = fd.integrity_data\n        pi.output(\"Saving cache config\")\n        self.cache_config.save(self.manifest, self.key)\n        os.replace(os.path.join(self.path, \"txn.active\"), os.path.join(self.path, \"txn.tmp\"))\n        shutil.rmtree(os.path.join(self.path, \"txn.tmp\"))\n        self.txn_active = False\n        pi.finish()\n\n    def rollback(self):\n        \"\"\"Roll back partial and aborted transactions\"\"\"\n        # Remove partial transaction\n        if os.path.exists(os.path.join(self.path, \"txn.tmp\")):\n            shutil.rmtree(os.path.join(self.path, \"txn.tmp\"))\n        # Roll back active transaction\n        txn_dir = os.path.join(self.path, \"txn.active\")\n        if os.path.exists(txn_dir):\n            shutil.copy(os.path.join(txn_dir, \"config\"), self.path)\n            shutil.copy(os.path.join(txn_dir, \"chunks\"), self.path)\n            shutil.copy(os.path.join(txn_dir, discover_files_cache_name(txn_dir)), self.path)\n            txn_tmp = os.path.join(self.path, \"txn.tmp\")\n            os.replace(txn_dir, txn_tmp)\n            if os.path.exists(txn_tmp):\n                shutil.rmtree(txn_tmp)\n        self.txn_active = False\n        self._do_open()\n\n    def sync(self):\n        \"\"\"Re-synchronize chunks cache with repository.\n\n        Maintains a directory with known backup archive indexes, so it only\n        needs to fetch infos from repo and build a chunk index once per backup\n        archive.\n        If out of sync, missing archive indexes get added, outdated indexes\n        get removed and a new master chunks index is built by merging all\n        archive indexes.\n        \"\"\"\n        archive_path = os.path.join(self.path, \"chunks.archive.d\")\n        # Instrumentation\n        processed_item_metadata_bytes = 0\n        processed_item_metadata_chunks = 0\n        compact_chunks_archive_saved_space = 0\n\n        def mkpath(id, suffix=\"\"):\n            id_hex = bin_to_hex(id)\n            path = os.path.join(archive_path, id_hex + suffix)\n            return path\n\n        def cached_archives():\n            if self.do_cache:\n                fns = os.listdir(archive_path)\n                # filenames with 64 hex digits == 256bit,\n                # or compact indices which are 64 hex digits + \".compact\"\n                return {unhexlify(fn) for fn in fns if len(fn) == 64} | {\n                    unhexlify(fn[:64]) for fn in fns if len(fn) == 72 and fn.endswith(\".compact\")\n                }\n            else:\n                return set()\n\n        def repo_archives():\n            return {info.id for info in self.manifest.archives.list()}\n\n        def cleanup_outdated(ids):\n            for id in ids:\n                cleanup_cached_archive(id)\n\n        def cleanup_cached_archive(id, cleanup_compact=True):\n            try:\n                os.unlink(mkpath(id))\n                os.unlink(mkpath(id) + \".integrity\")\n            except FileNotFoundError:\n                pass\n            if not cleanup_compact:\n                return\n            try:\n                os.unlink(mkpath(id, suffix=\".compact\"))\n                os.unlink(mkpath(id, suffix=\".compact\") + \".integrity\")\n            except FileNotFoundError:\n                pass\n\n        def fetch_and_build_idx(archive_id, decrypted_repository, chunk_idx):\n            nonlocal processed_item_metadata_bytes\n            nonlocal processed_item_metadata_chunks\n            csize, data = decrypted_repository.get(archive_id)\n            chunk_idx.add(archive_id, 1, len(data))\n            archive = ArchiveItem(internal_dict=msgpack.unpackb(data))\n            if archive.version not in (1, 2):  # legacy\n                raise Exception(\"Unknown archive metadata version\")\n            if archive.version == 1:\n                items = archive.items\n            elif archive.version == 2:\n                items = []\n                for chunk_id, (csize, data) in zip(archive.item_ptrs, decrypted_repository.get_many(archive.item_ptrs)):\n                    chunk_idx.add(chunk_id, 1, len(data))\n                    ids = msgpack.unpackb(data)\n                    items.extend(ids)\n            sync = CacheSynchronizer(chunk_idx)\n            for item_id, (csize, data) in zip(items, decrypted_repository.get_many(items)):\n                chunk_idx.add(item_id, 1, len(data))\n                processed_item_metadata_bytes += len(data)\n                processed_item_metadata_chunks += 1\n                sync.feed(data)\n            if self.do_cache:\n                write_archive_index(archive_id, chunk_idx)\n\n        def write_archive_index(archive_id, chunk_idx):\n            nonlocal compact_chunks_archive_saved_space\n            compact_chunks_archive_saved_space += chunk_idx.compact()\n            fn = mkpath(archive_id, suffix=\".compact\")\n            fn_tmp = mkpath(archive_id, suffix=\".tmp\")\n            try:\n                with DetachedIntegrityCheckedFile(\n                    path=fn_tmp, write=True, filename=bin_to_hex(archive_id) + \".compact\"\n                ) as fd:\n                    chunk_idx.write(fd)\n            except Exception:\n                safe_unlink(fn_tmp)\n            else:\n                os.replace(fn_tmp, fn)\n\n        def read_archive_index(archive_id, archive_name):\n            archive_chunk_idx_path = mkpath(archive_id)\n            logger.info(\"Reading cached archive chunk index for %s\", archive_name)\n            try:\n                try:\n                    # Attempt to load compact index first\n                    with DetachedIntegrityCheckedFile(path=archive_chunk_idx_path + \".compact\", write=False) as fd:\n                        archive_chunk_idx = ChunkIndex.read(fd, permit_compact=True)\n                    # In case a non-compact index exists, delete it.\n                    cleanup_cached_archive(archive_id, cleanup_compact=False)\n                    # Compact index read - return index, no conversion necessary (below).\n                    return archive_chunk_idx\n                except FileNotFoundError:\n                    # No compact index found, load non-compact index, and convert below.\n                    with DetachedIntegrityCheckedFile(path=archive_chunk_idx_path, write=False) as fd:\n                        archive_chunk_idx = ChunkIndex.read(fd)\n            except FileIntegrityError as fie:\n                logger.error(\"Cached archive chunk index of %s is corrupted: %s\", archive_name, fie)\n                # Delete corrupted index, set warning. A new index must be build.\n                cleanup_cached_archive(archive_id)\n                set_ec(EXIT_WARNING)\n                return None\n\n            # Convert to compact index. Delete the existing index first.\n            logger.debug(\"Found non-compact index for %s, converting to compact.\", archive_name)\n            cleanup_cached_archive(archive_id)\n            write_archive_index(archive_id, archive_chunk_idx)\n            return archive_chunk_idx\n\n        def get_archive_ids_to_names(archive_ids):\n            # Pass once over all archives and build a mapping from ids to names.\n            # The easier approach, doing a similar loop for each archive, has\n            # square complexity and does about a dozen million functions calls\n            # with 1100 archives (which takes 30s CPU seconds _alone_).\n            archive_names = {}\n            for info in self.manifest.archives.list():\n                if info.id in archive_ids:\n                    archive_names[info.id] = info.name\n            assert len(archive_names) == len(archive_ids)\n            return archive_names\n\n        def create_master_idx(chunk_idx):\n            logger.debug(\"Synchronizing chunks index...\")\n            cached_ids = cached_archives()\n            archive_ids = repo_archives()\n            logger.info(\n                \"Cached archive chunk indexes: %d fresh, %d stale, %d need fetching.\",\n                len(archive_ids & cached_ids),\n                len(cached_ids - archive_ids),\n                len(archive_ids - cached_ids),\n            )\n            # deallocates old hashindex, creates empty hashindex:\n            chunk_idx.clear()\n            cleanup_outdated(cached_ids - archive_ids)\n            # Explicitly set the usable initial hash table capacity to avoid performance issues\n            # due to hash table \"resonance\".\n            master_index_capacity = len(self.repository)\n            if archive_ids:\n                chunk_idx = None if not self.do_cache else ChunkIndex(usable=master_index_capacity)\n                pi = ProgressIndicatorPercent(\n                    total=len(archive_ids),\n                    step=0.1,\n                    msg=\"%3.0f%% Syncing chunks index. Processing archive %s.\",\n                    msgid=\"cache.sync\",\n                )\n                archive_ids_to_names = get_archive_ids_to_names(archive_ids)\n                for archive_id, archive_name in archive_ids_to_names.items():\n                    pi.show(info=[remove_surrogates(archive_name)])  # legacy. borg2 always has pure unicode arch names.\n                    if self.do_cache:\n                        if archive_id in cached_ids:\n                            archive_chunk_idx = read_archive_index(archive_id, archive_name)\n                            if archive_chunk_idx is None:\n                                cached_ids.remove(archive_id)\n                        if archive_id not in cached_ids:\n                            # Do not make this an else branch; the FileIntegrityError exception handler\n                            # above can remove *archive_id* from *cached_ids*.\n                            logger.info(\"Fetching and building archive index for %s.\", archive_name)\n                            archive_chunk_idx = ChunkIndex()\n                            fetch_and_build_idx(archive_id, decrypted_repository, archive_chunk_idx)\n                        logger.debug(\"Merging into master chunks index.\")\n                        chunk_idx.merge(archive_chunk_idx)\n                    else:\n                        chunk_idx = chunk_idx or ChunkIndex(usable=master_index_capacity)\n                        logger.info(\"Fetching archive index for %s.\", archive_name)\n                        fetch_and_build_idx(archive_id, decrypted_repository, chunk_idx)\n                pi.finish()\n                logger.debug(\n                    \"Chunks index sync: processed %s (%d chunks) of metadata.\",\n                    format_file_size(processed_item_metadata_bytes),\n                    processed_item_metadata_chunks,\n                )\n                logger.debug(\n                    \"Chunks index sync: compact chunks.archive.d storage saved %s bytes.\",\n                    format_file_size(compact_chunks_archive_saved_space),\n                )\n            logger.debug(\"Chunks index sync done.\")\n            return chunk_idx\n\n        # The cache can be used by a command that e.g. only checks against Manifest.Operation.WRITE,\n        # which does not have to include all flags from Manifest.Operation.READ.\n        # Since the sync will attempt to read archives, check compatibility with Manifest.Operation.READ.\n        self.manifest.check_repository_compatibility((Manifest.Operation.READ,))\n\n        self.begin_txn()\n        with cache_if_remote(self.repository, decrypted_cache=self.repo_objs) as decrypted_repository:\n            # TEMPORARY HACK:\n            # to avoid archive index caching, create a FILE named ~/.cache/borg/REPOID/chunks.archive.d -\n            # this is only recommended if you have a fast, low latency connection to your repo (e.g. if repo is local).\n            self.do_cache = os.path.isdir(archive_path)\n            self.chunks = create_master_idx(self.chunks)\n\n    def check_cache_compatibility(self):\n        my_features = Manifest.SUPPORTED_REPO_FEATURES\n        if self.cache_config.ignored_features & my_features:\n            # The cache might not contain references of chunks that need a feature that is mandatory for some operation\n            # and which this version supports. To avoid corruption while executing that operation force rebuild.\n            return False\n        if not self.cache_config.mandatory_features <= my_features:\n            # The cache was build with consideration to at least one feature that this version does not understand.\n            # This client might misinterpret the cache. Thus force a rebuild.\n            return False\n        return True\n\n    def wipe_cache(self):\n        logger.warning(\"Discarding incompatible cache and forcing a cache rebuild\")\n        archive_path = os.path.join(self.path, \"chunks.archive.d\")\n        if os.path.isdir(archive_path):\n            shutil.rmtree(os.path.join(self.path, \"chunks.archive.d\"))\n            os.makedirs(os.path.join(self.path, \"chunks.archive.d\"))\n        self.chunks = ChunkIndex()\n        with SaveFile(os.path.join(self.path, files_cache_name()), binary=True):\n            pass  # empty file\n        self.cache_config.manifest_id = \"\"\n        self.cache_config._config.set(\"cache\", \"manifest\", \"\")\n\n        self.cache_config.ignored_features = set()\n        self.cache_config.mandatory_features = set()\n\n    def update_compatibility(self):\n        operation_to_features_map = self.manifest.get_all_mandatory_features()\n        my_features = Manifest.SUPPORTED_REPO_FEATURES\n        repo_features = set()\n        for operation, features in operation_to_features_map.items():\n            repo_features.update(features)\n\n        self.cache_config.ignored_features.update(repo_features - my_features)\n        self.cache_config.mandatory_features.update(repo_features & my_features)\n\n    def add_chunk(\n        self, id, meta, data, *, stats, overwrite=False, wait=True, compress=True, size=None, ctype=None, clevel=None\n    ):\n        if not self.txn_active:\n            self.begin_txn()\n        if size is None and compress:\n            size = len(data)  # data is still uncompressed\n        refcount = self.seen_chunk(id, size)\n        if refcount and not overwrite:\n            return self.chunk_incref(id, stats)\n        if size is None:\n            raise ValueError(\"when giving compressed data for a new chunk, the uncompressed size must be given also\")\n        cdata = self.repo_objs.format(id, meta, data, compress=compress, size=size, ctype=ctype, clevel=clevel)\n        self.repository.put(id, cdata, wait=wait)\n        self.chunks.add(id, 1, size)\n        stats.update(size, not refcount)\n        return ChunkListEntry(id, size)\n\n    def seen_chunk(self, id, size=None):\n        refcount, stored_size = self.chunks.get(id, ChunkIndexEntry(0, None))\n        if size is not None and stored_size is not None and size != stored_size:\n            # we already have a chunk with that id, but different size.\n            # this is either a hash collision (unlikely) or corruption or a bug.\n            raise Exception(\n                \"chunk has same id [%r], but different size (stored: %d new: %d)!\" % (id, stored_size, size)\n            )\n        return refcount\n\n    def chunk_incref(self, id, stats, size=None):\n        if not self.txn_active:\n            self.begin_txn()\n        count, _size = self.chunks.incref(id)\n        stats.update(_size, False)\n        return ChunkListEntry(id, _size)\n\n    def chunk_decref(self, id, stats, wait=True):\n        if not self.txn_active:\n            self.begin_txn()\n        count, size = self.chunks.decref(id)\n        if count == 0:\n            del self.chunks[id]\n            self.repository.delete(id, wait=wait)\n            stats.update(-size, True)\n        else:\n            stats.update(-size, False)\n\n    def file_known_and_unchanged(self, hashed_path, path_hash, st):\n        \"\"\"\n        Check if we know the file that has this path_hash (know == it is in our files cache) and\n        whether it is unchanged (the size/inode number/cmtime is same for stuff we check in this cache_mode).\n\n        :param hashed_path: the file's path as we gave it to hash(hashed_path)\n        :param path_hash: hash(hashed_path), to save some memory in the files cache\n        :param st: the file's stat() result\n        :return: known, ids (known is True if we have infos about this file in the cache,\n                             ids is the list of chunk ids IF the file has not changed, otherwise None).\n        \"\"\"\n        if not stat.S_ISREG(st.st_mode):\n            return False, None\n        cache_mode = self.cache_mode\n        if \"d\" in cache_mode:  # d(isabled)\n            files_cache_logger.debug(\"UNKNOWN: files cache disabled\")\n            return False, None\n        # note: r(echunk) does not need the files cache in this method, but the files cache will\n        # be updated and saved to disk to memorize the files. To preserve previous generations in\n        # the cache, this means that it also needs to get loaded from disk first.\n        if \"r\" in cache_mode:  # r(echunk)\n            files_cache_logger.debug(\"UNKNOWN: rechunking enforced\")\n            return False, None\n        entry = self.files.get(path_hash)\n        if not entry:\n            files_cache_logger.debug(\"UNKNOWN: no file metadata in cache for: %r\", hashed_path)\n            return False, None\n        # we know the file!\n        entry = FileCacheEntry(*msgpack.unpackb(entry))\n        if \"s\" in cache_mode and entry.size != st.st_size:\n            files_cache_logger.debug(\"KNOWN-CHANGED: file size has changed: %r\", hashed_path)\n            return True, None\n        if \"i\" in cache_mode and entry.inode != st.st_ino:\n            files_cache_logger.debug(\"KNOWN-CHANGED: file inode number has changed: %r\", hashed_path)\n            return True, None\n        if \"c\" in cache_mode and timestamp_to_int(entry.cmtime) != st.st_ctime_ns:\n            files_cache_logger.debug(\"KNOWN-CHANGED: file ctime has changed: %r\", hashed_path)\n            return True, None\n        elif \"m\" in cache_mode and timestamp_to_int(entry.cmtime) != st.st_mtime_ns:\n            files_cache_logger.debug(\"KNOWN-CHANGED: file mtime has changed: %r\", hashed_path)\n            return True, None\n        # we ignored the inode number in the comparison above or it is still same.\n        # if it is still the same, replacing it in the tuple doesn't change it.\n        # if we ignored it, a reason for doing that is that files were moved to a new\n        # disk / new fs (so a one-time change of inode number is expected) and we wanted\n        # to avoid everything getting chunked again. to be able to re-enable the inode\n        # number comparison in a future backup run (and avoid chunking everything\n        # again at that time), we need to update the inode number in the cache with what\n        # we see in the filesystem.\n        self.files[path_hash] = msgpack.packb(entry._replace(inode=st.st_ino, age=0))\n        return True, entry.chunk_ids\n\n    def memorize_file(self, hashed_path, path_hash, st, ids):\n        if not stat.S_ISREG(st.st_mode):\n            return\n        cache_mode = self.cache_mode\n        # note: r(echunk) modes will update the files cache, d(isabled) mode won't\n        if \"d\" in cache_mode:\n            files_cache_logger.debug(\"FILES-CACHE-NOUPDATE: files cache disabled\")\n            return\n        if \"c\" in cache_mode:\n            cmtime_type = \"ctime\"\n            cmtime_ns = safe_ns(st.st_ctime_ns)\n        elif \"m\" in cache_mode:\n            cmtime_type = \"mtime\"\n            cmtime_ns = safe_ns(st.st_mtime_ns)\n        else:  # neither 'c' nor 'm' in cache_mode, avoid UnboundLocalError\n            cmtime_type = \"ctime\"\n            cmtime_ns = safe_ns(st.st_ctime_ns)\n        entry = FileCacheEntry(\n            age=0, inode=st.st_ino, size=st.st_size, cmtime=int_to_timestamp(cmtime_ns), chunk_ids=ids\n        )\n        self.files[path_hash] = msgpack.packb(entry)\n        self._newest_cmtime = max(self._newest_cmtime or 0, cmtime_ns)\n        files_cache_logger.debug(\n            \"FILES-CACHE-UPDATE: put %r [has %s] <- %r\",\n            entry._replace(chunk_ids=\"[%d entries]\" % len(entry.chunk_ids)),\n            cmtime_type,\n            hashed_path,\n        )\n\n\nclass AdHocCache(CacheStatsMixin):\n    \"\"\"\n    Ad-hoc, non-persistent cache.\n\n    Compared to the standard LocalCache the AdHocCache does not maintain accurate reference count,\n    nor does it provide a files cache (which would require persistence). Chunks that were not added\n    during the current AdHocCache lifetime won't have correct size set (0 bytes) and will\n    have an infinite reference count (MAX_VALUE).\n    \"\"\"\n\n    str_format = \"\"\"\\\nAll archives:                unknown              unknown              unknown\n\n                       Unique chunks         Total chunks\nChunk index:    {0.total_unique_chunks:20d}             unknown\"\"\"\n\n    def __init__(self, manifest, warn_if_unencrypted=True, lock_wait=None, iec=False):\n        CacheStatsMixin.__init__(self, iec=iec)\n        assert isinstance(manifest, Manifest)\n        self.manifest = manifest\n        self.repository = manifest.repository\n        self.key = manifest.key\n        self.repo_objs = manifest.repo_objs\n        self._txn_active = False\n\n        self.security_manager = SecurityManager(self.repository)\n        self.security_manager.assert_secure(manifest, self.key, lock_wait=lock_wait)\n\n        logger.warning(\"Note: --no-cache-sync is an experimental feature.\")\n\n    # Public API\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    files = None  # type: ignore\n    cache_mode = \"d\"\n\n    def file_known_and_unchanged(self, hashed_path, path_hash, st):\n        files_cache_logger.debug(\"UNKNOWN: files cache not implemented\")\n        return False, None\n\n    def memorize_file(self, hashed_path, path_hash, st, ids):\n        pass\n\n    def add_chunk(self, id, meta, data, *, stats, overwrite=False, wait=True, compress=True, size=None):\n        assert not overwrite, \"AdHocCache does not permit overwrites \u2014 trying to use it for recreate?\"\n        if not self._txn_active:\n            self.begin_txn()\n        if size is None and compress:\n            size = len(data)  # data is still uncompressed\n        if size is None:\n            raise ValueError(\"when giving compressed data for a chunk, the uncompressed size must be given also\")\n        refcount = self.seen_chunk(id, size)\n        if refcount:\n            return self.chunk_incref(id, stats, size=size)\n        cdata = self.repo_objs.format(id, meta, data, compress=compress)\n        self.repository.put(id, cdata, wait=wait)\n        self.chunks.add(id, 1, size)\n        stats.update(size, not refcount)\n        return ChunkListEntry(id, size)\n\n    def seen_chunk(self, id, size=None):\n        if not self._txn_active:\n            self.begin_txn()\n        entry = self.chunks.get(id, ChunkIndexEntry(0, None))\n        if entry.refcount and size and not entry.size:\n            # The LocalCache has existing size information and uses *size* to make an effort at detecting collisions.\n            # This is of course not possible for the AdHocCache.\n            # Here *size* is used to update the chunk's size information, which will be zero for existing chunks.\n            self.chunks[id] = entry._replace(size=size)\n        return entry.refcount\n\n    def chunk_incref(self, id, stats, size=None):\n        if not self._txn_active:\n            self.begin_txn()\n        count, _size = self.chunks.incref(id)\n        # When _size is 0 and size is not given, then this chunk has not been locally visited yet (seen_chunk with\n        # size or add_chunk); we can't add references to those (size=0 is invalid) and generally don't try to.\n        size = _size or size\n        assert size\n        stats.update(size, False)\n        return ChunkListEntry(id, size)\n\n    def chunk_decref(self, id, stats, wait=True):\n        if not self._txn_active:\n            self.begin_txn()\n        count, size = self.chunks.decref(id)\n        if count == 0:\n            del self.chunks[id]\n            self.repository.delete(id, wait=wait)\n            stats.update(-size, True)\n        else:\n            stats.update(-size, False)\n\n    def commit(self):\n        if not self._txn_active:\n            return\n        self.security_manager.save(self.manifest, self.key)\n        self._txn_active = False\n\n    def rollback(self):\n        self._txn_active = False\n        del self.chunks\n\n    def begin_txn(self):\n        self._txn_active = True\n        # Explicitly set the initial usable hash table capacity to avoid performance issues\n        # due to hash table \"resonance\".\n        # Since we're creating an archive, add 10 % from the start.\n        num_chunks = len(self.repository)\n        self.chunks = ChunkIndex(usable=num_chunks * 1.1)\n        pi = ProgressIndicatorPercent(\n            total=num_chunks, msg=\"Downloading chunk list... %3.0f%%\", msgid=\"cache.download_chunks\"\n        )\n        t0 = perf_counter()\n        num_requests = 0\n        marker = None\n        while True:\n            result = self.repository.list(limit=LIST_SCAN_LIMIT, marker=marker)\n            num_requests += 1\n            if not result:\n                break\n            pi.show(increase=len(result))\n            marker = result[-1]\n            # All chunks from the repository have a refcount of MAX_VALUE, which is sticky,\n            # therefore we can't/won't delete them. Chunks we added ourselves in this transaction\n            # (e.g. checkpoint archives) are tracked correctly.\n            init_entry = ChunkIndexEntry(refcount=ChunkIndex.MAX_VALUE, size=0)\n            for id_ in result:\n                self.chunks[id_] = init_entry\n        assert len(self.chunks) == num_chunks\n        # LocalCache does not contain the manifest, either.\n        del self.chunks[self.manifest.MANIFEST_ID]\n        duration = perf_counter() - t0 or 0.01\n        pi.finish()\n        logger.debug(\n            \"AdHocCache: downloaded %d chunk IDs in %.2f s (%d requests), ~%s/s\",\n            num_chunks,\n            duration,\n            num_requests,\n            format_file_size(num_chunks * 34 / duration),\n        )\n        # Chunk IDs in a list are encoded in 34 bytes: 1 byte msgpack header, 1 byte length, 32 ID bytes.\n        # Protocol overhead is neglected in this calculation.\n", "import binascii\nimport hmac\nimport os\nimport textwrap\nfrom binascii import a2b_base64, b2a_base64, hexlify\nfrom hashlib import sha256, pbkdf2_hmac\nfrom typing import Literal, Callable, ClassVar\n\nfrom ..logger import create_logger\n\nlogger = create_logger()\n\nimport argon2.low_level\n\nfrom ..constants import *  # NOQA\nfrom ..helpers import StableDict\nfrom ..helpers import Error, IntegrityError\nfrom ..helpers import get_keys_dir, get_security_dir\nfrom ..helpers import get_limited_unpacker\nfrom ..helpers import bin_to_hex\nfrom ..helpers.passphrase import Passphrase, PasswordRetriesExceeded, PassphraseWrong\nfrom ..helpers import msgpack\nfrom ..helpers import workarounds\nfrom ..item import Key, EncryptedKey, want_bytes\nfrom ..manifest import Manifest\nfrom ..platform import SaveFile\nfrom ..repoobj import RepoObj\n\n\nfrom .low_level import AES, bytes_to_int, num_cipher_blocks, hmac_sha256, blake2b_256, hkdf_hmac_sha512\nfrom .low_level import AES256_CTR_HMAC_SHA256, AES256_CTR_BLAKE2b, AES256_OCB, CHACHA20_POLY1305\nfrom . import low_level\n\n# workaround for lost passphrase or key in \"authenticated\" or \"authenticated-blake2\" mode\nAUTHENTICATED_NO_KEY = \"authenticated_no_key\" in workarounds\n\n\nclass UnsupportedPayloadError(Error):\n    \"\"\"Unsupported payload type {}. A newer version is required to access this repository.\"\"\"\n\n\nclass UnsupportedManifestError(Error):\n    \"\"\"Unsupported manifest envelope. A newer version is required to access this repository.\"\"\"\n\n\nclass KeyfileNotFoundError(Error):\n    \"\"\"No key file for repository {} found in {}.\"\"\"\n\n\nclass KeyfileInvalidError(Error):\n    \"\"\"Invalid key file for repository {} found in {}.\"\"\"\n\n\nclass KeyfileMismatchError(Error):\n    \"\"\"Mismatch between repository {} and key file {}.\"\"\"\n\n\nclass RepoKeyNotFoundError(Error):\n    \"\"\"No key entry found in the config of repository {}.\"\"\"\n\n\nclass UnsupportedKeyFormatError(Error):\n    \"\"\"Your borg key is stored in an unsupported format. Try using a newer version of borg.\"\"\"\n\n\nclass TAMRequiredError(IntegrityError):\n    __doc__ = textwrap.dedent(\n        \"\"\"\n    Manifest is unauthenticated, but it is required for this repository. Is somebody attacking you?\n    \"\"\"\n    ).strip()\n    traceback = False\n\n\nclass TAMInvalid(IntegrityError):\n    __doc__ = IntegrityError.__doc__\n    traceback = False\n\n    def __init__(self):\n        # Error message becomes: \"Data integrity error: Manifest authentication did not verify\"\n        super().__init__(\"Manifest authentication did not verify\")\n\n\nclass TAMUnsupportedSuiteError(IntegrityError):\n    \"\"\"Could not verify manifest: Unsupported suite {!r}; a newer version is needed.\"\"\"\n\n    traceback = False\n\n\ndef key_creator(repository, args, *, other_key=None):\n    for key in AVAILABLE_KEY_TYPES:\n        if key.ARG_NAME == args.encryption:\n            assert key.ARG_NAME is not None\n            return key.create(repository, args, other_key=other_key)\n    else:\n        raise ValueError('Invalid encryption mode \"%s\"' % args.encryption)\n\n\ndef key_argument_names():\n    return [key.ARG_NAME for key in AVAILABLE_KEY_TYPES if key.ARG_NAME]\n\n\ndef identify_key(manifest_data):\n    key_type = manifest_data[0]\n    if key_type == KeyType.PASSPHRASE:  # legacy, see comment in KeyType class.\n        return RepoKey\n\n    for key in LEGACY_KEY_TYPES + AVAILABLE_KEY_TYPES:\n        if key.TYPE == key_type:\n            return key\n    else:\n        raise UnsupportedPayloadError(key_type)\n\n\ndef key_factory(repository, manifest_chunk, *, ro_cls=RepoObj):\n    manifest_data = ro_cls.extract_crypted_data(manifest_chunk)\n    assert manifest_data, \"manifest data must not be zero bytes long\"\n    return identify_key(manifest_data).detect(repository, manifest_data)\n\n\ndef tam_required_file(repository):\n    security_dir = get_security_dir(bin_to_hex(repository.id), legacy=(repository.version == 1))\n    return os.path.join(security_dir, \"tam_required\")\n\n\ndef tam_required(repository):\n    file = tam_required_file(repository)\n    return os.path.isfile(file)\n\n\ndef uses_same_chunker_secret(other_key, key):\n    \"\"\"is the chunker secret the same?\"\"\"\n    # avoid breaking the deduplication by a different chunker secret\n    same_chunker_secret = other_key.chunk_seed == key.chunk_seed\n    return same_chunker_secret\n\n\ndef uses_same_id_hash(other_key, key):\n    \"\"\"other_key -> key upgrade: is the id hash the same?\"\"\"\n    # avoid breaking the deduplication by changing the id hash\n    old_sha256_ids = (PlaintextKey,)\n    new_sha256_ids = (PlaintextKey,)\n    old_hmac_sha256_ids = (RepoKey, KeyfileKey, AuthenticatedKey)\n    new_hmac_sha256_ids = (AESOCBRepoKey, AESOCBKeyfileKey, CHPORepoKey, CHPOKeyfileKey, AuthenticatedKey)\n    old_blake2_ids = (Blake2RepoKey, Blake2KeyfileKey, Blake2AuthenticatedKey)\n    new_blake2_ids = (\n        Blake2AESOCBRepoKey,\n        Blake2AESOCBKeyfileKey,\n        Blake2CHPORepoKey,\n        Blake2CHPOKeyfileKey,\n        Blake2AuthenticatedKey,\n    )\n    same_ids = (\n        isinstance(other_key, old_hmac_sha256_ids + new_hmac_sha256_ids)\n        and isinstance(key, new_hmac_sha256_ids)\n        or isinstance(other_key, old_blake2_ids + new_blake2_ids)\n        and isinstance(key, new_blake2_ids)\n        or isinstance(other_key, old_sha256_ids + new_sha256_ids)\n        and isinstance(key, new_sha256_ids)\n    )\n    return same_ids\n\n\nclass KeyBase:\n    # Numeric key type ID, must fit in one byte.\n    TYPE: int = None  # override in subclasses\n    # set of key type IDs the class can handle as input\n    TYPES_ACCEPTABLE: set[int] = None  # override in subclasses\n\n    # Human-readable name\n    NAME = \"UNDEFINED\"\n\n    # Name used in command line / API (e.g. borg init --encryption=...)\n    ARG_NAME = \"UNDEFINED\"\n\n    # Storage type (no key blob storage / keyfile / repo)\n    STORAGE: ClassVar[str] = KeyBlobStorage.NO_STORAGE\n\n    # Seed for the buzhash chunker (borg.algorithms.chunker.Chunker)\n    # type is int\n    chunk_seed: int = None\n\n    # Whether this *particular instance* is encrypted from a practical point of view,\n    # i.e. when it's using encryption with a empty passphrase, then\n    # that may be *technically* called encryption, but for all intents and purposes\n    # that's as good as not encrypting in the first place, and this member should be False.\n    #\n    # The empty passphrase is also special because Borg tries it first when no passphrase\n    # was supplied, and if an empty passphrase works, then Borg won't ask for one.\n    logically_encrypted = False\n\n    def __init__(self, repository):\n        self.TYPE_STR = bytes([self.TYPE])\n        self.repository = repository\n        self.target = None  # key location file path / repo obj\n        self.tam_required = True\n        self.copy_crypt_key = False\n\n    def id_hash(self, data):\n        \"\"\"Return HMAC hash using the \"id\" HMAC key\"\"\"\n        raise NotImplementedError\n\n    def encrypt(self, id, data):\n        pass\n\n    def decrypt(self, id, data):\n        pass\n\n    def assert_id(self, id, data):\n        if id and id != Manifest.MANIFEST_ID:\n            id_computed = self.id_hash(data)\n            if not hmac.compare_digest(id_computed, id):\n                raise IntegrityError(\"Chunk %s: id verification failed\" % bin_to_hex(id))\n\n    def assert_type(self, type_byte, id=None):\n        if type_byte not in self.TYPES_ACCEPTABLE:\n            id_str = bin_to_hex(id) if id is not None else \"(unknown)\"\n            raise IntegrityError(f\"Chunk {id_str}: Invalid encryption envelope\")\n\n    def _tam_key(self, salt, context):\n        return hkdf_hmac_sha512(\n            ikm=self.id_key + self.crypt_key,\n            salt=salt,\n            info=b\"borg-metadata-authentication-\" + context,\n            output_length=64,\n        )\n\n    def pack_and_authenticate_metadata(self, metadata_dict, context=b\"manifest\"):\n        metadata_dict = StableDict(metadata_dict)\n        tam = metadata_dict[\"tam\"] = StableDict({\"type\": \"HKDF_HMAC_SHA512\", \"hmac\": bytes(64), \"salt\": os.urandom(64)})\n        packed = msgpack.packb(metadata_dict)\n        tam_key = self._tam_key(tam[\"salt\"], context)\n        tam[\"hmac\"] = hmac.digest(tam_key, packed, \"sha512\")\n        return msgpack.packb(metadata_dict)\n\n    def unpack_and_verify_manifest(self, data, force_tam_not_required=False):\n        \"\"\"Unpack msgpacked *data* and return (object, did_verify).\"\"\"\n        if data.startswith(b\"\\xc1\" * 4):\n            # This is a manifest from the future, we can't read it.\n            raise UnsupportedManifestError()\n        tam_required = self.tam_required\n        if force_tam_not_required and tam_required:\n            logger.warning(\"Manifest authentication DISABLED.\")\n            tam_required = False\n        data = bytearray(data)\n        unpacker = get_limited_unpacker(\"manifest\")\n        unpacker.feed(data)\n        unpacked = unpacker.unpack()\n        if AUTHENTICATED_NO_KEY:\n            return unpacked, True  # True is a lie.\n        if \"tam\" not in unpacked:\n            if tam_required:\n                raise TAMRequiredError(self.repository._location.canonical_path())\n            else:\n                logger.debug(\"TAM not found and not required\")\n                return unpacked, False\n        tam = unpacked.pop(\"tam\", None)\n        if not isinstance(tam, dict):\n            raise TAMInvalid()\n        tam_type = tam.get(\"type\", \"<none>\")\n        if tam_type != \"HKDF_HMAC_SHA512\":\n            if tam_required:\n                raise TAMUnsupportedSuiteError(repr(tam_type))\n            else:\n                logger.debug(\"Ignoring TAM made with unsupported suite, since TAM is not required: %r\", tam_type)\n                return unpacked, False\n        tam_hmac = tam.get(\"hmac\")\n        tam_salt = tam.get(\"salt\")\n        if not isinstance(tam_salt, (bytes, str)) or not isinstance(tam_hmac, (bytes, str)):\n            raise TAMInvalid()\n        tam_hmac = want_bytes(tam_hmac)  # legacy\n        tam_salt = want_bytes(tam_salt)  # legacy\n        offset = data.index(tam_hmac)\n        data[offset : offset + 64] = bytes(64)\n        tam_key = self._tam_key(tam_salt, context=b\"manifest\")\n        calculated_hmac = hmac.digest(tam_key, data, \"sha512\")\n        if not hmac.compare_digest(calculated_hmac, tam_hmac):\n            raise TAMInvalid()\n        logger.debug(\"TAM-verified manifest\")\n        return unpacked, True\n\n\nclass PlaintextKey(KeyBase):\n    TYPE = KeyType.PLAINTEXT\n    TYPES_ACCEPTABLE = {TYPE}\n    NAME = \"plaintext\"\n    ARG_NAME = \"none\"\n\n    chunk_seed = 0\n    logically_encrypted = False\n\n    def __init__(self, repository):\n        super().__init__(repository)\n        self.tam_required = False\n\n    @classmethod\n    def create(cls, repository, args, **kw):\n        logger.info('Encryption NOT enabled.\\nUse the \"--encryption=repokey|keyfile\" to enable encryption.')\n        return cls(repository)\n\n    @classmethod\n    def detect(cls, repository, manifest_data):\n        return cls(repository)\n\n    def id_hash(self, data):\n        return sha256(data).digest()\n\n    def encrypt(self, id, data):\n        return b\"\".join([self.TYPE_STR, data])\n\n    def decrypt(self, id, data):\n        self.assert_type(data[0], id)\n        return memoryview(data)[1:]\n\n    def _tam_key(self, salt, context):\n        return salt + context\n\n\ndef random_blake2b_256_key():\n    # This might look a bit curious, but is the same construction used in the keyed mode of BLAKE2b.\n    # Why limit the key to 64 bytes and pad it with 64 nulls nonetheless? The answer is that BLAKE2b\n    # has a 128 byte block size, but only 64 bytes of internal state (this is also referred to as a\n    # \"local wide pipe\" design, because the compression function transforms (block, state) => state,\n    # and len(block) >= len(state), hence wide.)\n    # In other words, a key longer than 64 bytes would have simply no advantage, since the function\n    # has no way of propagating more than 64 bytes of entropy internally.\n    # It's padded to a full block so that the key is never buffered internally by blake2b_update, ie.\n    # it remains in a single memory location that can be tracked and could be erased securely, if we\n    # wanted to.\n    return os.urandom(64) + bytes(64)\n\n\nclass ID_BLAKE2b_256:\n    \"\"\"\n    Key mix-in class for using BLAKE2b-256 for the id key.\n\n    The id_key length must be 32 bytes.\n    \"\"\"\n\n    def id_hash(self, data):\n        return blake2b_256(self.id_key, data)\n\n    def init_from_random_data(self):\n        super().init_from_random_data()\n        enc_key = os.urandom(32)\n        enc_hmac_key = random_blake2b_256_key()\n        self.crypt_key = enc_key + enc_hmac_key\n        self.id_key = random_blake2b_256_key()\n\n\nclass ID_HMAC_SHA_256:\n    \"\"\"\n    Key mix-in class for using HMAC-SHA-256 for the id key.\n\n    The id_key length must be 32 bytes.\n    \"\"\"\n\n    def id_hash(self, data):\n        return hmac_sha256(self.id_key, data)\n\n\nclass AESKeyBase(KeyBase):\n    \"\"\"\n    Chunks are encrypted using 256bit AES in Counter Mode (CTR)\n\n    Payload layout: TYPE(1) + HMAC(32) + NONCE(8) + CIPHERTEXT\n\n    To reduce payload size only 8 bytes of the 16 bytes nonce is saved\n    in the payload, the first 8 bytes are always zeros. This does not\n    affect security but limits the maximum repository capacity to\n    only 295 exabytes!\n    \"\"\"\n\n    PAYLOAD_OVERHEAD = 1 + 32 + 8  # TYPE + HMAC + NONCE\n\n    CIPHERSUITE: Callable = None  # override in derived class\n\n    logically_encrypted = True\n\n    def encrypt(self, id, data):\n        # legacy, this is only used by the tests.\n        next_iv = self.cipher.next_iv()\n        return self.cipher.encrypt(data, header=self.TYPE_STR, iv=next_iv)\n\n    def decrypt(self, id, data):\n        self.assert_type(data[0], id)\n        try:\n            return self.cipher.decrypt(data)\n        except IntegrityError as e:\n            raise IntegrityError(f\"Chunk {bin_to_hex(id)}: Could not decrypt [{str(e)}]\")\n\n    def init_from_given_data(self, *, crypt_key, id_key, chunk_seed):\n        assert len(crypt_key) in (32 + 32, 32 + 128)\n        assert len(id_key) in (32, 128)\n        assert isinstance(chunk_seed, int)\n        self.crypt_key = crypt_key\n        self.id_key = id_key\n        self.chunk_seed = chunk_seed\n\n    def init_from_random_data(self):\n        data = os.urandom(100)\n        chunk_seed = bytes_to_int(data[96:100])\n        # Convert to signed int32\n        if chunk_seed & 0x80000000:\n            chunk_seed = chunk_seed - 0xFFFFFFFF - 1\n        self.init_from_given_data(crypt_key=data[0:64], id_key=data[64:96], chunk_seed=chunk_seed)\n\n    def init_ciphers(self, manifest_data=None):\n        enc_key, enc_hmac_key = self.crypt_key[0:32], self.crypt_key[32:]\n        self.cipher = self.CIPHERSUITE(mac_key=enc_hmac_key, enc_key=enc_key, header_len=1, aad_offset=1)\n        if manifest_data is None:\n            nonce = 0\n        else:\n            self.assert_type(manifest_data[0])\n            # manifest_blocks is a safe upper bound on the amount of cipher blocks needed\n            # to encrypt the manifest. depending on the ciphersuite and overhead, it might\n            # be a bit too high, but that does not matter.\n            manifest_blocks = num_cipher_blocks(len(manifest_data))\n            nonce = self.cipher.extract_iv(manifest_data) + manifest_blocks\n        self.cipher.set_iv(nonce)\n\n\nclass FlexiKey:\n    FILE_ID = \"BORG_KEY\"\n    STORAGE: ClassVar[str] = KeyBlobStorage.NO_STORAGE  # override in subclass\n\n    @classmethod\n    def detect(cls, repository, manifest_data):\n        key = cls(repository)\n        target = key.find_key()\n        prompt = \"Enter passphrase for key %s: \" % target\n        passphrase = Passphrase.env_passphrase()\n        if passphrase is None:\n            passphrase = Passphrase()\n            if not key.load(target, passphrase):\n                for retry in range(0, 3):\n                    passphrase = Passphrase.getpass(prompt)\n                    if key.load(target, passphrase):\n                        break\n                else:\n                    raise PasswordRetriesExceeded\n        else:\n            if not key.load(target, passphrase):\n                raise PassphraseWrong\n        key.init_ciphers(manifest_data)\n        key._passphrase = passphrase\n        return key\n\n    def _load(self, key_data, passphrase):\n        cdata = a2b_base64(key_data)\n        data = self.decrypt_key_file(cdata, passphrase)\n        if data:\n            data = msgpack.unpackb(data)\n            key = Key(internal_dict=data)\n            if key.version not in (1, 2):  # legacy: item.Key can still process v1 keys\n                raise UnsupportedKeyFormatError()\n            self.repository_id = key.repository_id\n            self.crypt_key = key.crypt_key\n            self.id_key = key.id_key\n            self.chunk_seed = key.chunk_seed\n            self.tam_required = key.get(\"tam_required\", tam_required(self.repository))\n            return True\n        return False\n\n    def decrypt_key_file(self, data, passphrase):\n        unpacker = get_limited_unpacker(\"key\")\n        unpacker.feed(data)\n        data = unpacker.unpack()\n        encrypted_key = EncryptedKey(internal_dict=data)\n        if encrypted_key.version != 1:\n            raise UnsupportedKeyFormatError()\n        else:\n            self._encrypted_key_algorithm = encrypted_key.algorithm\n            if encrypted_key.algorithm == \"sha256\":\n                return self.decrypt_key_file_pbkdf2(encrypted_key, passphrase)\n            elif encrypted_key.algorithm == \"argon2 chacha20-poly1305\":\n                return self.decrypt_key_file_argon2(encrypted_key, passphrase)\n            else:\n                raise UnsupportedKeyFormatError()\n\n    @staticmethod\n    def pbkdf2(passphrase, salt, iterations, output_len_in_bytes):\n        if os.environ.get(\"BORG_TESTONLY_WEAKEN_KDF\") == \"1\":\n            iterations = 1\n        return pbkdf2_hmac(\"sha256\", passphrase.encode(\"utf-8\"), salt, iterations, output_len_in_bytes)\n\n    @staticmethod\n    def argon2(\n        passphrase: str,\n        output_len_in_bytes: int,\n        salt: bytes,\n        time_cost: int,\n        memory_cost: int,\n        parallelism: int,\n        type: Literal[\"i\", \"d\", \"id\"],\n    ) -> bytes:\n        if os.environ.get(\"BORG_TESTONLY_WEAKEN_KDF\") == \"1\":\n            time_cost = 1\n            parallelism = 1\n            # 8 is the smallest value that avoids the \"Memory cost is too small\" exception\n            memory_cost = 8\n        type_map = {\"i\": argon2.low_level.Type.I, \"d\": argon2.low_level.Type.D, \"id\": argon2.low_level.Type.ID}\n        key = argon2.low_level.hash_secret_raw(\n            secret=passphrase.encode(\"utf-8\"),\n            hash_len=output_len_in_bytes,\n            salt=salt,\n            time_cost=time_cost,\n            memory_cost=memory_cost,\n            parallelism=parallelism,\n            type=type_map[type],\n        )\n        return key\n\n    def decrypt_key_file_pbkdf2(self, encrypted_key, passphrase):\n        key = self.pbkdf2(passphrase, encrypted_key.salt, encrypted_key.iterations, 32)\n        data = AES(key, b\"\\0\" * 16).decrypt(encrypted_key.data)\n        if hmac.compare_digest(hmac_sha256(key, data), encrypted_key.hash):\n            return data\n        return None\n\n    def decrypt_key_file_argon2(self, encrypted_key, passphrase):\n        key = self.argon2(\n            passphrase,\n            output_len_in_bytes=32,\n            salt=encrypted_key.salt,\n            time_cost=encrypted_key.argon2_time_cost,\n            memory_cost=encrypted_key.argon2_memory_cost,\n            parallelism=encrypted_key.argon2_parallelism,\n            type=encrypted_key.argon2_type,\n        )\n        ae_cipher = CHACHA20_POLY1305(key=key, iv=0, header_len=0, aad_offset=0)\n        try:\n            return ae_cipher.decrypt(encrypted_key.data)\n        except low_level.IntegrityError:\n            return None\n\n    def encrypt_key_file(self, data, passphrase, algorithm):\n        if algorithm == \"sha256\":\n            return self.encrypt_key_file_pbkdf2(data, passphrase)\n        elif algorithm == \"argon2 chacha20-poly1305\":\n            return self.encrypt_key_file_argon2(data, passphrase)\n        else:\n            raise ValueError(f\"Unexpected algorithm: {algorithm}\")\n\n    def encrypt_key_file_pbkdf2(self, data, passphrase):\n        salt = os.urandom(32)\n        iterations = PBKDF2_ITERATIONS\n        key = self.pbkdf2(passphrase, salt, iterations, 32)\n        hash = hmac_sha256(key, data)\n        cdata = AES(key, b\"\\0\" * 16).encrypt(data)\n        enc_key = EncryptedKey(version=1, salt=salt, iterations=iterations, algorithm=\"sha256\", hash=hash, data=cdata)\n        return msgpack.packb(enc_key.as_dict())\n\n    def encrypt_key_file_argon2(self, data, passphrase):\n        salt = os.urandom(ARGON2_SALT_BYTES)\n        key = self.argon2(passphrase, output_len_in_bytes=32, salt=salt, **ARGON2_ARGS)\n        ae_cipher = CHACHA20_POLY1305(key=key, iv=0, header_len=0, aad_offset=0)\n        encrypted_key = EncryptedKey(\n            version=1,\n            algorithm=\"argon2 chacha20-poly1305\",\n            salt=salt,\n            data=ae_cipher.encrypt(data),\n            **{\"argon2_\" + k: v for k, v in ARGON2_ARGS.items()},\n        )\n        return msgpack.packb(encrypted_key.as_dict())\n\n    def _save(self, passphrase, algorithm):\n        key = Key(\n            version=2,\n            repository_id=self.repository_id,\n            crypt_key=self.crypt_key,\n            id_key=self.id_key,\n            chunk_seed=self.chunk_seed,\n            tam_required=self.tam_required,\n        )\n        data = self.encrypt_key_file(msgpack.packb(key.as_dict()), passphrase, algorithm)\n        key_data = \"\\n\".join(textwrap.wrap(b2a_base64(data).decode(\"ascii\")))\n        return key_data\n\n    def change_passphrase(self, passphrase=None):\n        if passphrase is None:\n            passphrase = Passphrase.new(allow_empty=True)\n        self.save(self.target, passphrase, algorithm=self._encrypted_key_algorithm)\n\n    @classmethod\n    def create(cls, repository, args, *, other_key=None):\n        key = cls(repository)\n        key.repository_id = repository.id\n        if other_key is not None:\n            if isinstance(other_key, PlaintextKey):\n                raise Error(\"Copying key material from an unencrypted repository is not possible.\")\n            if isinstance(key, AESKeyBase):\n                # user must use an AEADKeyBase subclass (AEAD modes with session keys)\n                raise Error(\"Copying key material to an AES-CTR based mode is insecure and unsupported.\")\n            if not uses_same_id_hash(other_key, key):\n                raise Error(\"You must keep the same ID hash (HMAC-SHA256 or BLAKE2b) or deduplication will break.\")\n            if other_key.copy_crypt_key:\n                # give the user the option to use the same authenticated encryption (AE) key\n                crypt_key = other_key.crypt_key\n            else:\n                # borg transfer re-encrypts all data anyway, thus we can default to a new, random AE key\n                crypt_key = os.urandom(64)\n            key.init_from_given_data(crypt_key=crypt_key, id_key=other_key.id_key, chunk_seed=other_key.chunk_seed)\n            passphrase = other_key._passphrase\n        else:\n            key.init_from_random_data()\n            passphrase = Passphrase.new(allow_empty=True)\n        key.init_ciphers()\n        target = key.get_new_target(args)\n        key.save(target, passphrase, create=True, algorithm=KEY_ALGORITHMS[\"argon2\"])\n        logger.info('Key in \"%s\" created.' % target)\n        logger.info(\"Keep this key safe. Your data will be inaccessible without it.\")\n        return key\n\n    def sanity_check(self, filename, id):\n        file_id = self.FILE_ID.encode() + b\" \"\n        repo_id = hexlify(id)\n        with open(filename, \"rb\") as fd:\n            # we do the magic / id check in binary mode to avoid stumbling over\n            # decoding errors if somebody has binary files in the keys dir for some reason.\n            if fd.read(len(file_id)) != file_id:\n                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)\n            if fd.read(len(repo_id)) != repo_id:\n                raise KeyfileMismatchError(self.repository._location.canonical_path(), filename)\n        # we get here if it really looks like a borg key for this repo,\n        # do some more checks that are close to how borg reads/parses the key.\n        with open(filename, \"r\") as fd:\n            lines = fd.readlines()\n            if len(lines) < 2:\n                logger.warning(f\"borg key sanity check: expected 2+ lines total. [{filename}]\")\n                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)\n            if len(lines[0].rstrip()) > len(file_id) + len(repo_id):\n                logger.warning(f\"borg key sanity check: key line 1 seems too long. [{filename}]\")\n                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)\n            key_b64 = \"\".join(lines[1:])\n            try:\n                key = a2b_base64(key_b64)\n            except binascii.Error:\n                logger.warning(f\"borg key sanity check: key line 2+ does not look like base64. [{filename}]\")\n                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)\n            if len(key) < 20:\n                # this is in no way a precise check, usually we have about 400b key data.\n                logger.warning(\n                    f\"borg key sanity check: binary encrypted key data from key line 2+ suspiciously short.\"\n                    f\" [{filename}]\"\n                )\n                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)\n        # looks good!\n        return filename\n\n    def find_key(self):\n        if self.STORAGE == KeyBlobStorage.KEYFILE:\n            keyfile = self._find_key_file_from_environment()\n            if keyfile is not None:\n                return self.sanity_check(keyfile, self.repository.id)\n            keyfile = self._find_key_in_keys_dir()\n            if keyfile is not None:\n                return keyfile\n            raise KeyfileNotFoundError(self.repository._location.canonical_path(), get_keys_dir())\n        elif self.STORAGE == KeyBlobStorage.REPO:\n            loc = self.repository._location.canonical_path()\n            key = self.repository.load_key()\n            if not key:\n                # if we got an empty key, it means there is no key.\n                raise RepoKeyNotFoundError(loc) from None\n            return loc\n        else:\n            raise TypeError(\"Unsupported borg key storage type\")\n\n    def get_existing_or_new_target(self, args):\n        keyfile = self._find_key_file_from_environment()\n        if keyfile is not None:\n            return keyfile\n        keyfile = self._find_key_in_keys_dir()\n        if keyfile is not None:\n            return keyfile\n        return self._get_new_target_in_keys_dir(args)\n\n    def _find_key_in_keys_dir(self):\n        id = self.repository.id\n        keys_dir = get_keys_dir()\n        for name in os.listdir(keys_dir):\n            filename = os.path.join(keys_dir, name)\n            try:\n                return self.sanity_check(filename, id)\n            except (KeyfileInvalidError, KeyfileMismatchError):\n                pass\n\n    def get_new_target(self, args):\n        if self.STORAGE == KeyBlobStorage.KEYFILE:\n            keyfile = self._find_key_file_from_environment()\n            if keyfile is not None:\n                return keyfile\n            return self._get_new_target_in_keys_dir(args)\n        elif self.STORAGE == KeyBlobStorage.REPO:\n            return self.repository\n        else:\n            raise TypeError(\"Unsupported borg key storage type\")\n\n    def _find_key_file_from_environment(self):\n        keyfile = os.environ.get(\"BORG_KEY_FILE\")\n        if keyfile:\n            return os.path.abspath(keyfile)\n\n    def _get_new_target_in_keys_dir(self, args):\n        filename = args.location.to_key_filename()\n        path = filename\n        i = 1\n        while os.path.exists(path):\n            i += 1\n            path = filename + \".%d\" % i\n        return path\n\n    def load(self, target, passphrase):\n        if self.STORAGE == KeyBlobStorage.KEYFILE:\n            with open(target) as fd:\n                key_data = \"\".join(fd.readlines()[1:])\n        elif self.STORAGE == KeyBlobStorage.REPO:\n            # While the repository is encrypted, we consider a repokey repository with a blank\n            # passphrase an unencrypted repository.\n            self.logically_encrypted = passphrase != \"\"\n\n            # what we get in target is just a repo location, but we already have the repo obj:\n            target = self.repository\n            key_data = target.load_key()\n            if not key_data:\n                # if we got an empty key, it means there is no key.\n                loc = target._location.canonical_path()\n                raise RepoKeyNotFoundError(loc) from None\n            key_data = key_data.decode(\"utf-8\")  # remote repo: msgpack issue #99, getting bytes\n        else:\n            raise TypeError(\"Unsupported borg key storage type\")\n        success = self._load(key_data, passphrase)\n        if success:\n            self.target = target\n        return success\n\n    def save(self, target, passphrase, algorithm, create=False):\n        key_data = self._save(passphrase, algorithm)\n        if self.STORAGE == KeyBlobStorage.KEYFILE:\n            if create and os.path.isfile(target):\n                # if a new keyfile key repository is created, ensure that an existing keyfile of another\n                # keyfile key repo is not accidentally overwritten by careless use of the BORG_KEY_FILE env var.\n                # see issue #6036\n                raise Error('Aborting because key in \"%s\" already exists.' % target)\n            with SaveFile(target) as fd:\n                fd.write(f\"{self.FILE_ID} {bin_to_hex(self.repository_id)}\\n\")\n                fd.write(key_data)\n                fd.write(\"\\n\")\n        elif self.STORAGE == KeyBlobStorage.REPO:\n            self.logically_encrypted = passphrase != \"\"\n            key_data = key_data.encode(\"utf-8\")  # remote repo: msgpack issue #99, giving bytes\n            target.save_key(key_data)\n        else:\n            raise TypeError(\"Unsupported borg key storage type\")\n        self.target = target\n\n    def remove(self, target):\n        if self.STORAGE == KeyBlobStorage.KEYFILE:\n            os.remove(target)\n        elif self.STORAGE == KeyBlobStorage.REPO:\n            target.save_key(b\"\")  # save empty key (no new api at remote repo necessary)\n        else:\n            raise TypeError(\"Unsupported borg key storage type\")\n\n\nclass KeyfileKey(ID_HMAC_SHA_256, AESKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.KEYFILE, KeyType.REPO, KeyType.PASSPHRASE}\n    TYPE = KeyType.KEYFILE\n    NAME = \"key file\"\n    ARG_NAME = \"keyfile\"\n    STORAGE = KeyBlobStorage.KEYFILE\n    CIPHERSUITE = AES256_CTR_HMAC_SHA256\n\n\nclass RepoKey(ID_HMAC_SHA_256, AESKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.KEYFILE, KeyType.REPO, KeyType.PASSPHRASE}\n    TYPE = KeyType.REPO\n    NAME = \"repokey\"\n    ARG_NAME = \"repokey\"\n    STORAGE = KeyBlobStorage.REPO\n    CIPHERSUITE = AES256_CTR_HMAC_SHA256\n\n\nclass Blake2KeyfileKey(ID_BLAKE2b_256, AESKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.BLAKE2KEYFILE, KeyType.BLAKE2REPO}\n    TYPE = KeyType.BLAKE2KEYFILE\n    NAME = \"key file BLAKE2b\"\n    ARG_NAME = \"keyfile-blake2\"\n    STORAGE = KeyBlobStorage.KEYFILE\n    CIPHERSUITE = AES256_CTR_BLAKE2b\n\n\nclass Blake2RepoKey(ID_BLAKE2b_256, AESKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.BLAKE2KEYFILE, KeyType.BLAKE2REPO}\n    TYPE = KeyType.BLAKE2REPO\n    NAME = \"repokey BLAKE2b\"\n    ARG_NAME = \"repokey-blake2\"\n    STORAGE = KeyBlobStorage.REPO\n    CIPHERSUITE = AES256_CTR_BLAKE2b\n\n\nclass AuthenticatedKeyBase(AESKeyBase, FlexiKey):\n    STORAGE = KeyBlobStorage.REPO\n\n    # It's only authenticated, not encrypted.\n    logically_encrypted = False\n\n    def _load(self, key_data, passphrase):\n        if AUTHENTICATED_NO_KEY:\n            # fake _load if we have no key or passphrase\n            NOPE = bytes(32)  # 256 bit all-zero\n            self.repository_id = NOPE\n            self.enc_key = NOPE\n            self.enc_hmac_key = NOPE\n            self.id_key = NOPE\n            self.chunk_seed = 0\n            self.tam_required = False\n            return True\n        return super()._load(key_data, passphrase)\n\n    def load(self, target, passphrase):\n        success = super().load(target, passphrase)\n        self.logically_encrypted = False\n        return success\n\n    def save(self, target, passphrase, algorithm, create=False):\n        super().save(target, passphrase, algorithm, create=create)\n        self.logically_encrypted = False\n\n    def init_ciphers(self, manifest_data=None):\n        if manifest_data is not None:\n            self.assert_type(manifest_data[0])\n\n    def encrypt(self, id, data):\n        return b\"\".join([self.TYPE_STR, data])\n\n    def decrypt(self, id, data):\n        self.assert_type(data[0], id)\n        return memoryview(data)[1:]\n\n\nclass AuthenticatedKey(ID_HMAC_SHA_256, AuthenticatedKeyBase):\n    TYPE = KeyType.AUTHENTICATED\n    TYPES_ACCEPTABLE = {TYPE}\n    NAME = \"authenticated\"\n    ARG_NAME = \"authenticated\"\n\n\nclass Blake2AuthenticatedKey(ID_BLAKE2b_256, AuthenticatedKeyBase):\n    TYPE = KeyType.BLAKE2AUTHENTICATED\n    TYPES_ACCEPTABLE = {TYPE}\n    NAME = \"authenticated BLAKE2b\"\n    ARG_NAME = \"authenticated-blake2\"\n\n\n# ------------ new crypto ------------\n\n\nclass AEADKeyBase(KeyBase):\n    \"\"\"\n    Chunks are encrypted and authenticated using some AEAD ciphersuite\n\n    Layout: suite:4 keytype:4 reserved:8 messageIV:48 sessionID:192 auth_tag:128 payload:... [bits]\n            ^-------------------- AAD ----------------------------^\n    Offsets:0                 1          2            8             32           48 [bytes]\n\n    suite: 1010b for new AEAD crypto, 0000b is old crypto\n    keytype: see constants.KeyType (suite+keytype)\n    reserved: all-zero, for future use\n    messageIV: a counter starting from 0 for all new encrypted messages of one session\n    sessionID: 192bit random, computed once per session (the session key is derived from this)\n    auth_tag: authentication tag output of the AEAD cipher (computed over payload and AAD)\n    payload: encrypted chunk data\n    \"\"\"\n\n    PAYLOAD_OVERHEAD = 1 + 1 + 6 + 24 + 16  # [bytes], see Layout\n\n    CIPHERSUITE: Callable = None  # override in subclass\n\n    logically_encrypted = True\n\n    MAX_IV = 2**48 - 1\n\n    def assert_id(self, id, data):\n        # Comparing the id hash here would not be needed any more for the new AEAD crypto **IF** we\n        # could be sure that chunks were created by normal (not tampered, not evil) borg code:\n        # We put the id into AAD when storing the chunk, so it gets into the authentication tag computation.\n        # when decrypting, we provide the id we **want** as AAD for the auth tag verification, so\n        # decrypting only succeeds if we got the ciphertext we wrote **for that chunk id**.\n        # So, basically the **repository** can not cheat on us by giving us a different chunk.\n        #\n        # **BUT**, if chunks are created by tampered, evil borg code, the borg client code could put\n        # a wrong chunkid into AAD and then AEAD-encrypt-and-auth this and store it into the\n        # repository using this bad chunkid as key (violating the usual chunkid == id_hash(data)).\n        # Later, when reading such a bad chunk, AEAD-auth-and-decrypt would not notice any\n        # issue and decrypt successfully.\n        # Thus, to notice such evil borg activity, we must check for such violations here:\n        if id and id != Manifest.MANIFEST_ID:\n            id_computed = self.id_hash(data)\n            if not hmac.compare_digest(id_computed, id):\n                raise IntegrityError(\"Chunk %s: id verification failed\" % bin_to_hex(id))\n\n    def encrypt(self, id, data):\n        # to encrypt new data in this session we use always self.cipher and self.sessionid\n        reserved = b\"\\0\"\n        iv = self.cipher.next_iv()\n        if iv > self.MAX_IV:  # see the data-structures docs about why the IV range is enough\n            raise IntegrityError(\"IV overflow, should never happen.\")\n        iv_48bit = iv.to_bytes(6, \"big\")\n        header = self.TYPE_STR + reserved + iv_48bit + self.sessionid\n        return self.cipher.encrypt(data, header=header, iv=iv, aad=id)\n\n    def decrypt(self, id, data):\n        # to decrypt existing data, we need to get a cipher configured for the sessionid and iv from header\n        self.assert_type(data[0], id)\n        iv_48bit = data[2:8]\n        sessionid = data[8:32]\n        iv = int.from_bytes(iv_48bit, \"big\")\n        cipher = self._get_cipher(sessionid, iv)\n        try:\n            return cipher.decrypt(data, aad=id)\n        except IntegrityError as e:\n            raise IntegrityError(f\"Chunk {bin_to_hex(id)}: Could not decrypt [{str(e)}]\")\n\n    def init_from_given_data(self, *, crypt_key, id_key, chunk_seed):\n        assert len(crypt_key) in (32 + 32, 32 + 128)\n        assert len(id_key) in (32, 128)\n        assert isinstance(chunk_seed, int)\n        self.crypt_key = crypt_key\n        self.id_key = id_key\n        self.chunk_seed = chunk_seed\n\n    def init_from_random_data(self):\n        data = os.urandom(100)\n        chunk_seed = bytes_to_int(data[96:100])\n        # Convert to signed int32\n        if chunk_seed & 0x80000000:\n            chunk_seed = chunk_seed - 0xFFFFFFFF - 1\n        self.init_from_given_data(crypt_key=data[0:64], id_key=data[64:96], chunk_seed=chunk_seed)\n\n    def _get_session_key(self, sessionid):\n        assert len(sessionid) == 24  # 192bit\n        key = hkdf_hmac_sha512(\n            ikm=self.crypt_key,\n            salt=sessionid,\n            info=b\"borg-session-key-\" + self.CIPHERSUITE.__name__.encode(),\n            output_length=32,\n        )\n        return key\n\n    def _get_cipher(self, sessionid, iv):\n        assert isinstance(iv, int)\n        key = self._get_session_key(sessionid)\n        cipher = self.CIPHERSUITE(key=key, iv=iv, header_len=1 + 1 + 6 + 24, aad_offset=0)\n        return cipher\n\n    def init_ciphers(self, manifest_data=None, iv=0):\n        # in every new session we start with a fresh sessionid and at iv == 0, manifest_data and iv params are ignored\n        self.sessionid = os.urandom(24)\n        self.cipher = self._get_cipher(self.sessionid, iv=0)\n\n\nclass AESOCBKeyfileKey(ID_HMAC_SHA_256, AEADKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.AESOCBKEYFILE, KeyType.AESOCBREPO}\n    TYPE = KeyType.AESOCBKEYFILE\n    NAME = \"key file AES-OCB\"\n    ARG_NAME = \"keyfile-aes-ocb\"\n    STORAGE = KeyBlobStorage.KEYFILE\n    CIPHERSUITE = AES256_OCB\n\n\nclass AESOCBRepoKey(ID_HMAC_SHA_256, AEADKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.AESOCBKEYFILE, KeyType.AESOCBREPO}\n    TYPE = KeyType.AESOCBREPO\n    NAME = \"repokey AES-OCB\"\n    ARG_NAME = \"repokey-aes-ocb\"\n    STORAGE = KeyBlobStorage.REPO\n    CIPHERSUITE = AES256_OCB\n\n\nclass CHPOKeyfileKey(ID_HMAC_SHA_256, AEADKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.CHPOKEYFILE, KeyType.CHPOREPO}\n    TYPE = KeyType.CHPOKEYFILE\n    NAME = \"key file ChaCha20-Poly1305\"\n    ARG_NAME = \"keyfile-chacha20-poly1305\"\n    STORAGE = KeyBlobStorage.KEYFILE\n    CIPHERSUITE = CHACHA20_POLY1305\n\n\nclass CHPORepoKey(ID_HMAC_SHA_256, AEADKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.CHPOKEYFILE, KeyType.CHPOREPO}\n    TYPE = KeyType.CHPOREPO\n    NAME = \"repokey ChaCha20-Poly1305\"\n    ARG_NAME = \"repokey-chacha20-poly1305\"\n    STORAGE = KeyBlobStorage.REPO\n    CIPHERSUITE = CHACHA20_POLY1305\n\n\nclass Blake2AESOCBKeyfileKey(ID_BLAKE2b_256, AEADKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.BLAKE2AESOCBKEYFILE, KeyType.BLAKE2AESOCBREPO}\n    TYPE = KeyType.BLAKE2AESOCBKEYFILE\n    NAME = \"key file BLAKE2b AES-OCB\"\n    ARG_NAME = \"keyfile-blake2-aes-ocb\"\n    STORAGE = KeyBlobStorage.KEYFILE\n    CIPHERSUITE = AES256_OCB\n\n\nclass Blake2AESOCBRepoKey(ID_BLAKE2b_256, AEADKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.BLAKE2AESOCBKEYFILE, KeyType.BLAKE2AESOCBREPO}\n    TYPE = KeyType.BLAKE2AESOCBREPO\n    NAME = \"repokey BLAKE2b AES-OCB\"\n    ARG_NAME = \"repokey-blake2-aes-ocb\"\n    STORAGE = KeyBlobStorage.REPO\n    CIPHERSUITE = AES256_OCB\n\n\nclass Blake2CHPOKeyfileKey(ID_BLAKE2b_256, AEADKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.BLAKE2CHPOKEYFILE, KeyType.BLAKE2CHPOREPO}\n    TYPE = KeyType.BLAKE2CHPOKEYFILE\n    NAME = \"key file BLAKE2b ChaCha20-Poly1305\"\n    ARG_NAME = \"keyfile-blake2-chacha20-poly1305\"\n    STORAGE = KeyBlobStorage.KEYFILE\n    CIPHERSUITE = CHACHA20_POLY1305\n\n\nclass Blake2CHPORepoKey(ID_BLAKE2b_256, AEADKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.BLAKE2CHPOKEYFILE, KeyType.BLAKE2CHPOREPO}\n    TYPE = KeyType.BLAKE2CHPOREPO\n    NAME = \"repokey BLAKE2b ChaCha20-Poly1305\"\n    ARG_NAME = \"repokey-blake2-chacha20-poly1305\"\n    STORAGE = KeyBlobStorage.REPO\n    CIPHERSUITE = CHACHA20_POLY1305\n\n\nLEGACY_KEY_TYPES = (\n    # legacy (AES-CTR based) crypto\n    KeyfileKey,\n    RepoKey,\n    Blake2KeyfileKey,\n    Blake2RepoKey,\n)\n\nAVAILABLE_KEY_TYPES = (\n    # these are available encryption modes for new repositories\n    # not encrypted modes\n    PlaintextKey,\n    AuthenticatedKey,\n    Blake2AuthenticatedKey,\n    # new crypto\n    AESOCBKeyfileKey,\n    AESOCBRepoKey,\n    CHPOKeyfileKey,\n    CHPORepoKey,\n    Blake2AESOCBKeyfileKey,\n    Blake2AESOCBRepoKey,\n    Blake2CHPOKeyfileKey,\n    Blake2CHPORepoKey,\n)\n", "\"\"\"\nwrapping msgpack\n================\n\nWe wrap msgpack here the way we need it - to avoid having lots of clutter in the calling code.\n\nPacking\n-------\n- use_bin_type = True (used by borg since borg 2.0)\n  This is used to generate output according to new msgpack 2.0 spec.\n  This cleanly keeps bytes and str types apart.\n\n- use_bin_type = False (used by borg < 1.3)\n  This creates output according to the older msgpack spec.\n  BAD: str and bytes were packed into same \"raw\" representation.\n\n- unicode_errors = 'surrogateescape'\n  Guess backup applications are one of the rare cases when this needs to be used.\n  It is needed because borg also needs to deal with data that does not cleanly encode/decode using utf-8.\n  There's a lot of crap out there, e.g. in filenames and as a backup tool, we must keep them as good as possible.\n\nUnpacking\n---------\n- raw = False (used by borg since borg 2.0)\n  We already can use this with borg 2.0 due to the type conversion to the desired type in item.py update_internal\n  methods. This type conversion code can be removed in future, when we do not have to deal with data any more\n  that was packed the old way.\n  It will then unpack according to the msgpack 2.0 spec format and directly output bytes or str.\n\n- raw = True (the old way, used by borg < 1.3)\n\n- unicode_errors = 'surrogateescape' -> see description above (will be used when raw is False).\n\nAs of borg 2.0, we have fixed most of the msgpack str/bytes mess, #968.\nBorg now still needs to **read** old repos, archives, keys, ... so we can not yet fix it completely.\nBut from now on, borg only **writes** new data according to the new msgpack 2.0 spec,\nthus we can remove some legacy support in a later borg release (some places are marked with \"legacy\").\n\ncurrent way in msgpack terms\n----------------------------\n\n- pack with use_bin_type=True (according to msgpack 2.0 spec)\n- packs str -> raw and bytes -> bin\n- unpack with raw=False (according to msgpack 2.0 spec, using unicode_errors='surrogateescape')\n- unpacks bin to bytes and raw to str (thus we need to convert to desired type if we want bytes from \"raw\")\n\"\"\"\n\nfrom .datastruct import StableDict\nfrom ..constants import *  # NOQA\n\nfrom msgpack import Packer as mp_Packer\nfrom msgpack import packb as mp_packb\nfrom msgpack import pack as mp_pack\nfrom msgpack import Unpacker as mp_Unpacker\nfrom msgpack import unpackb as mp_unpackb\nfrom msgpack import unpack as mp_unpack\nfrom msgpack import version as mp_version\n\nfrom msgpack import ExtType, Timestamp\nfrom msgpack import OutOfData\n\n\nversion = mp_version\n\nUSE_BIN_TYPE = True\nRAW = False\nUNICODE_ERRORS = \"surrogateescape\"\n\n\nclass PackException(Exception):\n    \"\"\"Exception while msgpack packing\"\"\"\n\n\nclass UnpackException(Exception):\n    \"\"\"Exception while msgpack unpacking\"\"\"\n\n\nclass Packer(mp_Packer):\n    def __init__(\n        self,\n        *,\n        default=None,\n        unicode_errors=UNICODE_ERRORS,\n        use_single_float=False,\n        autoreset=True,\n        use_bin_type=USE_BIN_TYPE,\n        strict_types=False\n    ):\n        assert unicode_errors == UNICODE_ERRORS\n        super().__init__(\n            default=default,\n            unicode_errors=unicode_errors,\n            use_single_float=use_single_float,\n            autoreset=autoreset,\n            use_bin_type=use_bin_type,\n            strict_types=strict_types,\n        )\n\n    def pack(self, obj):\n        try:\n            return super().pack(obj)\n        except Exception as e:\n            raise PackException(e)\n\n\ndef packb(o, *, use_bin_type=USE_BIN_TYPE, unicode_errors=UNICODE_ERRORS, **kwargs):\n    assert unicode_errors == UNICODE_ERRORS\n    try:\n        return mp_packb(o, use_bin_type=use_bin_type, unicode_errors=unicode_errors, **kwargs)\n    except Exception as e:\n        raise PackException(e)\n\n\ndef pack(o, stream, *, use_bin_type=USE_BIN_TYPE, unicode_errors=UNICODE_ERRORS, **kwargs):\n    assert unicode_errors == UNICODE_ERRORS\n    try:\n        return mp_pack(o, stream, use_bin_type=use_bin_type, unicode_errors=unicode_errors, **kwargs)\n    except Exception as e:\n        raise PackException(e)\n\n\nclass Unpacker(mp_Unpacker):\n    def __init__(\n        self,\n        file_like=None,\n        *,\n        read_size=0,\n        use_list=True,\n        raw=RAW,\n        object_hook=None,\n        object_pairs_hook=None,\n        list_hook=None,\n        unicode_errors=UNICODE_ERRORS,\n        max_buffer_size=0,\n        ext_hook=ExtType,\n        strict_map_key=False\n    ):\n        assert raw == RAW\n        assert unicode_errors == UNICODE_ERRORS\n        kw = dict(\n            file_like=file_like,\n            read_size=read_size,\n            use_list=use_list,\n            raw=raw,\n            object_hook=object_hook,\n            object_pairs_hook=object_pairs_hook,\n            list_hook=list_hook,\n            unicode_errors=unicode_errors,\n            max_buffer_size=max_buffer_size,\n            ext_hook=ext_hook,\n            strict_map_key=strict_map_key,\n        )\n        super().__init__(**kw)\n\n    def unpack(self):\n        try:\n            return super().unpack()\n        except OutOfData:\n            raise\n        except Exception as e:\n            raise UnpackException(e)\n\n    def __next__(self):\n        try:\n            return super().__next__()\n        except StopIteration:\n            raise\n        except Exception as e:\n            raise UnpackException(e)\n\n    next = __next__\n\n\ndef unpackb(packed, *, raw=RAW, unicode_errors=UNICODE_ERRORS, strict_map_key=False, **kwargs):\n    assert raw == RAW\n    assert unicode_errors == UNICODE_ERRORS\n    try:\n        kw = dict(raw=raw, unicode_errors=unicode_errors, strict_map_key=strict_map_key)\n        kw.update(kwargs)\n        return mp_unpackb(packed, **kw)\n    except Exception as e:\n        raise UnpackException(e)\n\n\ndef unpack(stream, *, raw=RAW, unicode_errors=UNICODE_ERRORS, strict_map_key=False, **kwargs):\n    assert raw == RAW\n    assert unicode_errors == UNICODE_ERRORS\n    try:\n        kw = dict(raw=raw, unicode_errors=unicode_errors, strict_map_key=strict_map_key)\n        kw.update(kwargs)\n        return mp_unpack(stream, **kw)\n    except Exception as e:\n        raise UnpackException(e)\n\n\n# msgpacking related utilities -----------------------------------------------\n\n\ndef is_slow_msgpack():\n    import msgpack\n    import msgpack.fallback\n\n    return msgpack.Packer is msgpack.fallback.Packer\n\n\ndef is_supported_msgpack():\n    # DO NOT CHANGE OR REMOVE! See also requirements and comments in setup.cfg.\n    import msgpack\n\n    if msgpack.version in []:  # < add bad releases here to deny list\n        return False\n    return (1, 0, 3) <= msgpack.version <= (1, 0, 5)\n\n\ndef get_limited_unpacker(kind):\n    \"\"\"return a limited Unpacker because we should not trust msgpack data received from remote\"\"\"\n    # Note: msgpack >= 0.6.1 auto-computes DoS-safe max values from len(data) for\n    #       unpack(data) or from max_buffer_size for Unpacker(max_buffer_size=N).\n    args = dict(use_list=False, max_buffer_size=3 * max(BUFSIZE, MAX_OBJECT_SIZE))  # return tuples, not lists\n    if kind in (\"server\", \"client\"):\n        pass  # nothing special\n    elif kind in (\"manifest\", \"key\"):\n        args.update(dict(use_list=True, object_hook=StableDict))  # default value\n    else:\n        raise ValueError('kind must be \"server\", \"client\", \"manifest\" or \"key\"')\n    return Unpacker(**args)\n\n\ndef int_to_timestamp(ns):\n    assert isinstance(ns, int)\n    return Timestamp.from_unix_nano(ns)\n\n\ndef timestamp_to_int(ts):\n    assert isinstance(ts, Timestamp)\n    return ts.to_unix_nano()\n", "import abc\nimport argparse\nimport base64\nimport hashlib\nimport json\nimport os\nimport os.path\nimport re\nimport shlex\nimport stat\nimport uuid\nfrom typing import Dict, Set, Tuple, ClassVar, Any, TYPE_CHECKING, Literal\nfrom binascii import hexlify\nfrom collections import Counter, OrderedDict\nfrom datetime import datetime, timezone\nfrom functools import partial\nfrom string import Formatter\n\nfrom ..logger import create_logger\n\nlogger = create_logger()\n\nfrom .errors import Error\nfrom .fs import get_keys_dir, make_path_safe\nfrom .msgpack import Timestamp\nfrom .time import OutputTimestamp, format_time, safe_timestamp\nfrom .. import __version__ as borg_version\nfrom .. import __version_tuple__ as borg_version_tuple\nfrom ..constants import *  # NOQA\n\nif TYPE_CHECKING:\n    from ..item import ItemDiff\n\n\ndef bin_to_hex(binary):\n    return hexlify(binary).decode(\"ascii\")\n\n\ndef safe_decode(s, coding=\"utf-8\", errors=\"surrogateescape\"):\n    \"\"\"decode bytes to str, with round-tripping \"invalid\" bytes\"\"\"\n    if s is None:\n        return None\n    return s.decode(coding, errors)\n\n\ndef safe_encode(s, coding=\"utf-8\", errors=\"surrogateescape\"):\n    \"\"\"encode str to bytes, with round-tripping \"invalid\" bytes\"\"\"\n    if s is None:\n        return None\n    return s.encode(coding, errors)\n\n\ndef remove_surrogates(s, errors=\"replace\"):\n    \"\"\"Replace surrogates generated by fsdecode with '?'\"\"\"\n    return s.encode(\"utf-8\", errors).decode(\"utf-8\")\n\n\ndef binary_to_json(key, value):\n    assert isinstance(key, str)\n    assert isinstance(value, bytes)\n    return {key + \"_b64\": base64.b64encode(value).decode(\"ascii\")}\n\n\ndef text_to_json(key, value):\n    \"\"\"\n    Return a dict made from key/value that can be fed safely into a JSON encoder.\n\n    JSON can only contain pure, valid unicode (but not: unicode with surrogate escapes).\n\n    But sometimes we have to deal with such values and we do it like this:\n    - <key>: value as pure unicode text (surrogate escapes, if any, replaced by ?)\n    - <key>_b64: value as base64 encoded binary representation (only set if value has surrogate-escapes)\n    \"\"\"\n    coding = \"utf-8\"\n    assert isinstance(key, str)\n    assert isinstance(value, str)  # str might contain surrogate escapes\n    data = {}\n    try:\n        value.encode(coding, errors=\"strict\")  # check if pure unicode\n    except UnicodeEncodeError:\n        # value has surrogate escape sequences\n        data[key] = remove_surrogates(value)\n        value_bytes = value.encode(coding, errors=\"surrogateescape\")\n        data.update(binary_to_json(key, value_bytes))\n    else:\n        # value is pure unicode\n        data[key] = value\n        # we do not give the b64 representation, not needed\n    return data\n\n\ndef join_cmd(argv, rs=False):\n    cmd = shlex.join(argv)\n    return remove_surrogates(cmd) if rs else cmd\n\n\ndef eval_escapes(s):\n    \"\"\"Evaluate literal escape sequences in a string (eg `\\\\n` -> `\\n`).\"\"\"\n    return s.encode(\"ascii\", \"backslashreplace\").decode(\"unicode-escape\")\n\n\ndef decode_dict(d, keys, encoding=\"utf-8\", errors=\"surrogateescape\"):\n    for key in keys:\n        if isinstance(d.get(key), bytes):\n            d[key] = d[key].decode(encoding, errors)\n    return d\n\n\ndef positive_int_validator(value):\n    \"\"\"argparse type for positive integers\"\"\"\n    int_value = int(value)\n    if int_value <= 0:\n        raise argparse.ArgumentTypeError(\"A positive integer is required: %s\" % value)\n    return int_value\n\n\ndef interval(s):\n    \"\"\"Convert a string representing a valid interval to a number of hours.\"\"\"\n    multiplier = {\"H\": 1, \"d\": 24, \"w\": 24 * 7, \"m\": 24 * 31, \"y\": 24 * 365}\n\n    if s.endswith(tuple(multiplier.keys())):\n        number = s[:-1]\n        suffix = s[-1]\n    else:\n        # range suffixes in ascending multiplier order\n        ranges = [k for k, v in sorted(multiplier.items(), key=lambda t: t[1])]\n        raise argparse.ArgumentTypeError(f'Unexpected interval time unit \"{s[-1]}\": expected one of {ranges!r}')\n\n    try:\n        hours = int(number) * multiplier[suffix]\n    except ValueError:\n        hours = -1\n\n    if hours <= 0:\n        raise argparse.ArgumentTypeError('Unexpected interval number \"%s\": expected an integer greater than 0' % number)\n\n    return hours\n\n\ndef ChunkerParams(s):\n    params = s.strip().split(\",\")\n    count = len(params)\n    if count == 0:\n        raise argparse.ArgumentTypeError(\"no chunker params given\")\n    algo = params[0].lower()\n    if algo == CH_FAIL and count == 3:\n        block_size = int(params[1])\n        fail_map = str(params[2])\n        return algo, block_size, fail_map\n    if algo == CH_FIXED and 2 <= count <= 3:  # fixed, block_size[, header_size]\n        block_size = int(params[1])\n        header_size = int(params[2]) if count == 3 else 0\n        if block_size < 64:\n            # we are only disallowing the most extreme cases of abuse here - this does NOT imply\n            # that cutting chunks of the minimum allowed size is efficient concerning storage\n            # or in-memory chunk management.\n            # choose the block (chunk) size wisely: if you have a lot of data and you cut\n            # it into very small chunks, you are asking for trouble!\n            raise argparse.ArgumentTypeError(\"block_size must not be less than 64 Bytes\")\n        if block_size > MAX_DATA_SIZE or header_size > MAX_DATA_SIZE:\n            raise argparse.ArgumentTypeError(\n                \"block_size and header_size must not exceed MAX_DATA_SIZE [%d]\" % MAX_DATA_SIZE\n            )\n        return algo, block_size, header_size\n    if algo == \"default\" and count == 1:  # default\n        return CHUNKER_PARAMS\n    # this must stay last as it deals with old-style compat mode (no algorithm, 4 params, buzhash):\n    if algo == CH_BUZHASH and count == 5 or count == 4:  # [buzhash, ]chunk_min, chunk_max, chunk_mask, window_size\n        chunk_min, chunk_max, chunk_mask, window_size = (int(p) for p in params[count - 4 :])\n        if not (chunk_min <= chunk_mask <= chunk_max):\n            raise argparse.ArgumentTypeError(\"required: chunk_min <= chunk_mask <= chunk_max\")\n        if chunk_min < 6:\n            # see comment in 'fixed' algo check\n            raise argparse.ArgumentTypeError(\n                \"min. chunk size exponent must not be less than 6 (2^6 = 64B min. chunk size)\"\n            )\n        if chunk_max > 23:\n            raise argparse.ArgumentTypeError(\n                \"max. chunk size exponent must not be more than 23 (2^23 = 8MiB max. chunk size)\"\n            )\n        return CH_BUZHASH, chunk_min, chunk_max, chunk_mask, window_size\n    raise argparse.ArgumentTypeError(\"invalid chunker params\")\n\n\ndef FilesCacheMode(s):\n    ENTRIES_MAP = dict(ctime=\"c\", mtime=\"m\", size=\"s\", inode=\"i\", rechunk=\"r\", disabled=\"d\")\n    VALID_MODES = (\"cis\", \"ims\", \"cs\", \"ms\", \"cr\", \"mr\", \"d\", \"s\")  # letters in alpha order\n    entries = set(s.strip().split(\",\"))\n    if not entries <= set(ENTRIES_MAP):\n        raise argparse.ArgumentTypeError(\n            \"cache mode must be a comma-separated list of: %s\" % \",\".join(sorted(ENTRIES_MAP))\n        )\n    short_entries = {ENTRIES_MAP[entry] for entry in entries}\n    mode = \"\".join(sorted(short_entries))\n    if mode not in VALID_MODES:\n        raise argparse.ArgumentTypeError(\"cache mode short must be one of: %s\" % \",\".join(VALID_MODES))\n    return mode\n\n\ndef partial_format(format, mapping):\n    \"\"\"\n    Apply format.format_map(mapping) while preserving unknown keys\n\n    Does not support attribute access, indexing and ![rsa] conversions\n    \"\"\"\n    for key, value in mapping.items():\n        key = re.escape(key)\n        format = re.sub(\n            rf\"(?<!\\{{)((\\{{{key}\\}})|(\\{{{key}:[^\\}}]*\\}}))\", lambda match: match.group(1).format_map(mapping), format\n        )\n    return format\n\n\nclass DatetimeWrapper:\n    def __init__(self, dt):\n        self.dt = dt\n\n    def __format__(self, format_spec):\n        if format_spec == \"\":\n            format_spec = ISO_FORMAT_NO_USECS\n        return self.dt.__format__(format_spec)\n\n\nclass PlaceholderError(Error):\n    \"\"\"Formatting Error: \"{}\".format({}): {}({})\"\"\"\n\n\nclass InvalidPlaceholder(PlaceholderError):\n    \"\"\"Invalid placeholder \"{}\" in string: {}\"\"\"\n\n\ndef format_line(format, data):\n    for _, key, _, conversion in Formatter().parse(format):\n        if not key:\n            continue\n        if conversion or key not in data:\n            raise InvalidPlaceholder(key, format)\n    try:\n        return format.format_map(data)\n    except Exception as e:\n        raise PlaceholderError(format, data, e.__class__.__name__, str(e))\n\n\ndef _replace_placeholders(text, overrides={}):\n    \"\"\"Replace placeholders in text with their values.\"\"\"\n    from ..platform import fqdn, hostname, getosusername\n\n    current_time = datetime.now(timezone.utc)\n    data = {\n        \"pid\": os.getpid(),\n        \"fqdn\": fqdn,\n        \"reverse-fqdn\": \".\".join(reversed(fqdn.split(\".\"))),\n        \"hostname\": hostname,\n        \"now\": DatetimeWrapper(current_time.astimezone()),\n        \"utcnow\": DatetimeWrapper(current_time),\n        \"user\": getosusername(),\n        \"uuid4\": str(uuid.uuid4()),\n        \"borgversion\": borg_version,\n        \"borgmajor\": \"%d\" % borg_version_tuple[:1],\n        \"borgminor\": \"%d.%d\" % borg_version_tuple[:2],\n        \"borgpatch\": \"%d.%d.%d\" % borg_version_tuple[:3],\n        **overrides,\n    }\n    return format_line(text, data)\n\n\nclass PlaceholderReplacer:\n    def __init__(self):\n        self.reset()\n\n    def override(self, key, value):\n        self.overrides[key] = value\n\n    def reset(self):\n        self.overrides = {}\n\n    def __call__(self, text, overrides=None):\n        ovr = {}\n        ovr.update(self.overrides)\n        ovr.update(overrides or {})\n        return _replace_placeholders(text, overrides=ovr)\n\n\nreplace_placeholders = PlaceholderReplacer()\n\n\ndef SortBySpec(text):\n    from ..manifest import AI_HUMAN_SORT_KEYS\n\n    for token in text.split(\",\"):\n        if token not in AI_HUMAN_SORT_KEYS:\n            raise argparse.ArgumentTypeError(\"Invalid sort key: %s\" % token)\n    return text.replace(\"timestamp\", \"ts\")\n\n\ndef format_file_size(v, precision=2, sign=False, iec=False):\n    \"\"\"Format file size into a human friendly format\"\"\"\n    fn = sizeof_fmt_iec if iec else sizeof_fmt_decimal\n    return fn(v, suffix=\"B\", sep=\" \", precision=precision, sign=sign)\n\n\nclass FileSize(int):\n    def __new__(cls, value, iec=False):\n        obj = int.__new__(cls, value)\n        obj.iec = iec\n        return obj\n\n    def __format__(self, format_spec):\n        return format_file_size(int(self), iec=self.iec).__format__(format_spec)\n\n\ndef parse_file_size(s):\n    \"\"\"Return int from file size (1234, 55G, 1.7T).\"\"\"\n    if not s:\n        return int(s)  # will raise\n    suffix = s[-1]\n    power = 1000\n    try:\n        factor = {\"K\": power, \"M\": power**2, \"G\": power**3, \"T\": power**4, \"P\": power**5}[suffix]\n        s = s[:-1]\n    except KeyError:\n        factor = 1\n    return int(float(s) * factor)\n\n\ndef parse_storage_quota(storage_quota):\n    parsed = parse_file_size(storage_quota)\n    if parsed < parse_file_size(\"10M\"):\n        raise argparse.ArgumentTypeError(\"quota is too small (%s). At least 10M are required.\" % storage_quota)\n    return parsed\n\n\ndef sizeof_fmt(num, suffix=\"B\", units=None, power=None, sep=\"\", precision=2, sign=False):\n    sign = \"+\" if sign and num > 0 else \"\"\n    fmt = \"{0:{1}.{2}f}{3}{4}{5}\"\n    prec = 0\n    for unit in units[:-1]:\n        if abs(round(num, precision)) < power:\n            break\n        num /= float(power)\n        prec = precision\n    else:\n        unit = units[-1]\n    return fmt.format(num, sign, prec, sep, unit, suffix)\n\n\ndef sizeof_fmt_iec(num, suffix=\"B\", sep=\"\", precision=2, sign=False):\n    return sizeof_fmt(\n        num,\n        suffix=suffix,\n        sep=sep,\n        precision=precision,\n        sign=sign,\n        units=[\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\", \"Yi\"],\n        power=1024,\n    )\n\n\ndef sizeof_fmt_decimal(num, suffix=\"B\", sep=\"\", precision=2, sign=False):\n    return sizeof_fmt(\n        num,\n        suffix=suffix,\n        sep=sep,\n        precision=precision,\n        sign=sign,\n        units=[\"\", \"k\", \"M\", \"G\", \"T\", \"P\", \"E\", \"Z\", \"Y\"],\n        power=1000,\n    )\n\n\ndef format_archive(archive):\n    return \"%-36s %s [%s]\" % (archive.name, format_time(archive.ts), bin_to_hex(archive.id))\n\n\ndef parse_stringified_list(s):\n    items = re.split(\" *, *\", s)\n    return [item for item in items if item != \"\"]\n\n\nclass Location:\n    \"\"\"Object representing a repository location\"\"\"\n\n    # user must not contain \"@\", \":\" or \"/\".\n    # Quoting adduser error message:\n    # \"To avoid problems, the username should consist only of letters, digits,\n    # underscores, periods, at signs and dashes, and not start with a dash\n    # (as defined by IEEE Std 1003.1-2001).\"\n    # We use \"@\" as separator between username and hostname, so we must\n    # disallow it within the pure username part.\n    optional_user_re = r\"\"\"\n        (?:(?P<user>[^@:/]+)@)?\n    \"\"\"\n\n    # path must not contain :: (it ends at :: or string end), but may contain single colons.\n    # to avoid ambiguities with other regexes, it must also not start with \":\" nor with \"//\" nor with \"ssh://\".\n    local_path_re = r\"\"\"\n        (?!(:|//|ssh://|socket://))                         # not starting with \":\" or // or ssh:// or socket://\n        (?P<path>([^:]|(:(?!:)))+)                          # any chars, but no \"::\"\n        \"\"\"\n\n    # file_path must not contain :: (it ends at :: or string end), but may contain single colons.\n    # it must start with a / and that slash is part of the path.\n    file_path_re = r\"\"\"\n        (?P<path>(([^/]*)/([^:]|(:(?!:)))+))                # start opt. servername, then /, then any chars, but no \"::\"\n        \"\"\"\n\n    # abs_path must not contain :: (it ends at :: or string end), but may contain single colons.\n    # it must start with a / and that slash is part of the path.\n    abs_path_re = r\"\"\"\n        (?P<path>(/([^:]|(:(?!:)))+))                       # start with /, then any chars, but no \"::\"\n        \"\"\"\n\n    # host NAME, or host IP ADDRESS (v4 or v6, v6 must be in square brackets)\n    host_re = r\"\"\"\n        (?P<host>(\n            (?!\\[)[^:/]+(?<!\\])     # hostname or v4 addr, not containing : or / (does not match v6 addr: no brackets!)\n            |\n            \\[[0-9a-fA-F:.]+\\])     # ipv6 address in brackets\n        )\n    \"\"\"\n\n    # regexes for misc. kinds of supported location specifiers:\n    ssh_re = re.compile(\n        r\"\"\"\n        (?P<proto>ssh)://                                       # ssh://\n        \"\"\"\n        + optional_user_re\n        + host_re\n        + r\"\"\"                 # user@  (optional), host name or address\n        (?::(?P<port>\\d+))?                                     # :port (optional)\n        \"\"\"\n        + abs_path_re,\n        re.VERBOSE,\n    )  # path\n\n    socket_re = re.compile(\n        r\"\"\"\n        (?P<proto>socket)://                                    # socket://\n        \"\"\"\n        + abs_path_re,\n        re.VERBOSE,\n    )  # path\n\n    file_re = re.compile(\n        r\"\"\"\n        (?P<proto>file)://                                      # file://\n        \"\"\"\n        + file_path_re,\n        re.VERBOSE,\n    )  # servername/path or path\n\n    local_re = re.compile(local_path_re, re.VERBOSE)  # local path\n\n    win_file_re = re.compile(\n        r\"\"\"\n        (?:file://)?                                        # optional file protocol\n        (?P<path>\n            (?:[a-zA-Z]:)?                                  # Drive letter followed by a colon (optional)\n            (?:[^:]+)                                       # Anything which does not contain a :, at least one char\n        )\n        \"\"\",\n        re.VERBOSE,\n    )\n\n    def __init__(self, text=\"\", overrides={}, other=False):\n        self.repo_env_var = \"BORG_OTHER_REPO\" if other else \"BORG_REPO\"\n        self.valid = False\n        self.proto = None\n        self.user = None\n        self._host = None\n        self.port = None\n        self.path = None\n        self.raw = None\n        self.processed = None\n        self.parse(text, overrides)\n\n    def parse(self, text, overrides={}):\n        if not text:\n            # we did not get a text to parse, so we try to fetch from the environment\n            text = os.environ.get(self.repo_env_var)\n            if text is None:\n                return\n\n        self.raw = text  # as given by user, might contain placeholders\n        self.processed = replace_placeholders(self.raw, overrides)  # after placeholder replacement\n        valid = self._parse(self.processed)\n        if valid:\n            self.valid = True\n        else:\n            raise ValueError('Invalid location format: \"%s\"' % self.processed)\n\n    def _parse(self, text):\n        def normpath_special(p):\n            # avoid that normpath strips away our relative path hack and even makes p absolute\n            relative = p.startswith(\"/./\")\n            p = os.path.normpath(p)\n            return (\"/.\" + p) if relative else p\n\n        m = self.ssh_re.match(text)\n        if m:\n            self.proto = m.group(\"proto\")\n            self.user = m.group(\"user\")\n            self._host = m.group(\"host\")\n            self.port = m.group(\"port\") and int(m.group(\"port\")) or None\n            self.path = normpath_special(m.group(\"path\"))\n            return True\n        m = self.file_re.match(text)\n        if m:\n            self.proto = m.group(\"proto\")\n            self.path = normpath_special(m.group(\"path\"))\n            return True\n        m = self.socket_re.match(text)\n        if m:\n            self.proto = m.group(\"proto\")\n            self.path = normpath_special(m.group(\"path\"))\n            return True\n        m = self.local_re.match(text)\n        if m:\n            self.proto = \"file\"\n            self.path = normpath_special(m.group(\"path\"))\n            return True\n        return False\n\n    def __str__(self):\n        items = [\n            \"proto=%r\" % self.proto,\n            \"user=%r\" % self.user,\n            \"host=%r\" % self.host,\n            \"port=%r\" % self.port,\n            \"path=%r\" % self.path,\n        ]\n        return \", \".join(items)\n\n    def to_key_filename(self):\n        name = re.sub(r\"[^\\w]\", \"_\", self.path).strip(\"_\")\n        if self.proto not in (\"file\", \"socket\"):\n            name = re.sub(r\"[^\\w]\", \"_\", self.host) + \"__\" + name\n        if len(name) > 100:\n            # Limit file names to some reasonable length. Most file systems\n            # limit them to 255 [unit of choice]; due to variations in unicode\n            # handling we truncate to 100 *characters*.\n            name = name[:100]\n        return os.path.join(get_keys_dir(), name)\n\n    def __repr__(self):\n        return \"Location(%s)\" % self\n\n    @property\n    def host(self):\n        # strip square brackets used for IPv6 addrs\n        if self._host is not None:\n            return self._host.lstrip(\"[\").rstrip(\"]\")\n\n    def canonical_path(self):\n        if self.proto in (\"file\", \"socket\"):\n            return self.path\n        else:\n            if self.path and self.path.startswith(\"~\"):\n                path = \"/\" + self.path  # /~/x = path x relative to home dir\n            elif self.path and not self.path.startswith(\"/\"):\n                path = \"/./\" + self.path  # /./x = path x relative to cwd\n            else:\n                path = self.path\n            return \"ssh://{}{}{}{}\".format(\n                f\"{self.user}@\" if self.user else \"\",\n                self._host,  # needed for ipv6 addrs\n                f\":{self.port}\" if self.port else \"\",\n                path,\n            )\n\n    def with_timestamp(self, timestamp):\n        # note: this only affects the repository URL/path, not the archive name!\n        return Location(\n            self.raw,\n            overrides={\n                \"now\": DatetimeWrapper(timestamp),\n                \"utcnow\": DatetimeWrapper(timestamp.astimezone(timezone.utc)),\n            },\n        )\n\n\ndef location_validator(proto=None, other=False):\n    def validator(text):\n        try:\n            loc = Location(text, other=other)\n        except ValueError as err:\n            raise argparse.ArgumentTypeError(str(err)) from None\n        if proto is not None and loc.proto != proto:\n            if proto == \"file\":\n                raise argparse.ArgumentTypeError('\"%s\": Repository must be local' % text)\n            else:\n                raise argparse.ArgumentTypeError('\"%s\": Repository must be remote' % text)\n        return loc\n\n    return validator\n\n\ndef relative_time_marker_validator(text: str):\n    time_marker_regex = r\"^\\d+[md]$\"\n    match = re.compile(time_marker_regex).search(text)\n    if not match:\n        raise argparse.ArgumentTypeError(f\"Invalid relative time marker used: {text}\")\n    else:\n        return text\n\n\ndef text_validator(*, name, max_length, min_length=0, invalid_ctrl_chars=\"\\0\", invalid_chars=\"\", no_blanks=False):\n    def validator(text):\n        assert isinstance(text, str)\n        if len(text) < min_length:\n            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [length < {min_length}]')\n        if len(text) > max_length:\n            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [length > {max_length}]')\n        if invalid_ctrl_chars and re.search(f\"[{re.escape(invalid_ctrl_chars)}]\", text):\n            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [invalid control chars detected]')\n        if invalid_chars and re.search(f\"[{re.escape(invalid_chars)}]\", text):\n            raise argparse.ArgumentTypeError(\n                f'Invalid {name}: \"{text}\" [invalid chars detected matching \"{invalid_chars}\"]'\n            )\n        if no_blanks and (text.startswith(\" \") or text.endswith(\" \")):\n            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [leading or trailing blanks detected]')\n        try:\n            text.encode(\"utf-8\", errors=\"strict\")\n        except UnicodeEncodeError:\n            # looks like text contains surrogate-escapes\n            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [contains non-unicode characters]')\n        return text\n\n    return validator\n\n\ncomment_validator = text_validator(name=\"comment\", max_length=10000)\n\n\ndef archivename_validator(text):\n    # we make sure that the archive name can be used as directory name (for borg mount)\n    MAX_PATH = 260  # Windows default. Since Win10, there is a registry setting LongPathsEnabled to get more.\n    MAX_DIRNAME = MAX_PATH - len(\"12345678.123\")\n    SAFETY_MARGIN = 48  # borgfs path: mountpoint / archivename / dir / dir / ... / file\n    MAX_ARCHIVENAME = MAX_DIRNAME - SAFETY_MARGIN\n    invalid_ctrl_chars = \"\".join(chr(i) for i in range(32))\n    # note: \":\" is also an invalid path char on windows, but we can not blacklist it,\n    # because e.g. our {now} placeholder creates ISO-8601 like output like 2022-12-10T20:47:42 .\n    invalid_chars = r\"/\" + r\"\\\"<|>?*\"  # posix + windows\n    validate_text = text_validator(\n        name=\"archive name\",\n        min_length=1,\n        max_length=MAX_ARCHIVENAME,\n        invalid_ctrl_chars=invalid_ctrl_chars,\n        invalid_chars=invalid_chars,\n        no_blanks=True,\n    )\n    return validate_text(text)\n\n\nclass BaseFormatter(metaclass=abc.ABCMeta):\n    format: str\n    static_data: Dict[str, Any]\n    FIXED_KEYS: ClassVar[Dict[str, str]] = {\n        # Formatting aids\n        \"LF\": \"\\n\",\n        \"SPACE\": \" \",\n        \"TAB\": \"\\t\",\n        \"CR\": \"\\r\",\n        \"NUL\": \"\\0\",\n        \"NEWLINE\": \"\\n\",\n        \"NL\": \"\\n\",  # \\n is automatically converted to os.linesep on write\n    }\n    KEY_DESCRIPTIONS: ClassVar[Dict[str, str]] = {\n        \"NEWLINE\": \"OS dependent line separator\",\n        \"NL\": \"alias of NEWLINE\",\n        \"NUL\": \"NUL character for creating print0 / xargs -0 like output\",\n        \"SPACE\": \"space character\",\n        \"TAB\": \"tab character\",\n        \"CR\": \"carriage return character\",\n        \"LF\": \"line feed character\",\n    }\n    KEY_GROUPS: ClassVar[Tuple[Tuple[str, ...], ...]] = ((\"NEWLINE\", \"NL\", \"NUL\", \"SPACE\", \"TAB\", \"CR\", \"LF\"),)\n\n    def __init__(self, format: str, static: Dict[str, Any]) -> None:\n        self.format = partial_format(format, static)\n        self.static_data = static\n\n    @abc.abstractmethod\n    def get_item_data(self, item, jsonline=False) -> dict:\n        raise NotImplementedError\n\n    def format_item(self, item, jsonline=False, sort=False):\n        data = self.get_item_data(item, jsonline)\n        return (\n            f\"{json.dumps(data, cls=BorgJsonEncoder, sort_keys=sort)}\\n\" if jsonline else self.format.format_map(data)\n        )\n\n    @classmethod\n    def keys_help(cls):\n        help = []\n        keys: Set[str] = set()\n        keys.update(cls.KEY_DESCRIPTIONS.keys())\n        keys.update(key for group in cls.KEY_GROUPS for key in group)\n\n        for group in cls.KEY_GROUPS:\n            for key in group:\n                keys.remove(key)\n                text = \"- \" + key\n                if key in cls.KEY_DESCRIPTIONS:\n                    text += \": \" + cls.KEY_DESCRIPTIONS[key]\n                help.append(text)\n            help.append(\"\")\n        assert not keys, str(keys)\n        return \"\\n\".join(help)\n\n\nclass ArchiveFormatter(BaseFormatter):\n    KEY_DESCRIPTIONS = {\n        \"archive\": \"archive name\",\n        \"name\": 'alias of \"archive\"',\n        \"comment\": \"archive comment\",\n        # *start* is the key used by borg-info for this timestamp, this makes the formats more compatible\n        \"start\": \"time (start) of creation of the archive\",\n        \"time\": 'alias of \"start\"',\n        \"end\": \"time (end) of creation of the archive\",\n        \"command_line\": \"command line which was used to create the archive\",\n        \"id\": \"internal ID of the archive\",\n        \"hostname\": \"hostname of host on which this archive was created\",\n        \"username\": \"username of user who created this archive\",\n        \"size\": \"size of this archive (data plus metadata, not considering compression and deduplication)\",\n        \"nfiles\": \"count of files in this archive\",\n    }\n    KEY_GROUPS = (\n        (\"archive\", \"name\", \"comment\", \"id\"),\n        (\"start\", \"time\", \"end\", \"command_line\"),\n        (\"hostname\", \"username\"),\n        (\"size\", \"nfiles\"),\n    )\n\n    def __init__(self, format, repository, manifest, key, *, iec=False):\n        static_data = {}  # here could be stuff on repo level, above archive level\n        static_data.update(self.FIXED_KEYS)\n        super().__init__(format, static_data)\n        self.repository = repository\n        self.manifest = manifest\n        self.key = key\n        self.name = None\n        self.id = None\n        self._archive = None\n        self.iec = iec\n        self.format_keys = {f[1] for f in Formatter().parse(format)}\n        self.call_keys = {\n            \"hostname\": partial(self.get_meta, \"hostname\", \"\"),\n            \"username\": partial(self.get_meta, \"username\", \"\"),\n            \"comment\": partial(self.get_meta, \"comment\", \"\"),\n            \"command_line\": partial(self.get_meta, \"command_line\", \"\"),\n            \"size\": partial(self.get_meta, \"size\", 0),\n            \"nfiles\": partial(self.get_meta, \"nfiles\", 0),\n            \"end\": self.get_ts_end,\n        }\n        self.used_call_keys = set(self.call_keys) & self.format_keys\n\n    def get_item_data(self, archive_info, jsonline=False):\n        self.name = archive_info.name\n        self.id = archive_info.id\n        item_data = {}\n        item_data.update({} if jsonline else self.static_data)\n        item_data.update(\n            {\n                \"name\": archive_info.name,\n                \"archive\": archive_info.name,\n                \"id\": bin_to_hex(archive_info.id),\n                \"time\": self.format_time(archive_info.ts),\n                \"start\": self.format_time(archive_info.ts),\n            }\n        )\n        for key in self.used_call_keys:\n            item_data[key] = self.call_keys[key]()\n\n        # Note: name and comment are validated, should never contain surrogate escapes.\n        # But unsure whether hostname, username, command_line could contain surrogate escapes, play safe:\n        for key in \"hostname\", \"username\", \"command_line\":\n            if key in item_data:\n                item_data.update(text_to_json(key, item_data[key]))\n        return item_data\n\n    @property\n    def archive(self):\n        \"\"\"lazy load / update loaded archive\"\"\"\n        if self._archive is None or self._archive.id != self.id:\n            from ..archive import Archive\n\n            self._archive = Archive(self.manifest, self.name, iec=self.iec)\n        return self._archive\n\n    def get_meta(self, key, default=None):\n        return self.archive.metadata.get(key, default)\n\n    def get_ts_end(self):\n        return self.format_time(self.archive.ts_end)\n\n    def format_time(self, ts):\n        return OutputTimestamp(ts)\n\n\nclass ItemFormatter(BaseFormatter):\n    # we provide the hash algos from python stdlib (except shake_*) and additionally xxh64.\n    # shake_* is not provided because it uses an incompatible .digest() method to support variable length.\n    hash_algorithms = set(hashlib.algorithms_guaranteed).union({\"xxh64\"}).difference({\"shake_128\", \"shake_256\"})\n    KEY_DESCRIPTIONS = {\n        \"type\": \"file type (file, dir, symlink, ...)\",\n        \"mode\": \"file mode (as in stat)\",\n        \"uid\": \"user id of file owner\",\n        \"gid\": \"group id of file owner\",\n        \"user\": \"user name of file owner\",\n        \"group\": \"group name of file owner\",\n        \"path\": \"file path\",\n        \"target\": \"link target for symlinks\",\n        \"hlid\": \"hard link identity (same if hardlinking same fs object)\",\n        \"flags\": \"file flags\",\n        \"extra\": 'prepends {target} with \" -> \" for soft links and \" link to \" for hard links',\n        \"size\": \"file size\",\n        \"dsize\": \"deduplicated size\",\n        \"num_chunks\": \"number of chunks in this file\",\n        \"unique_chunks\": \"number of unique chunks in this file\",\n        \"mtime\": \"file modification time\",\n        \"ctime\": \"file change time\",\n        \"atime\": \"file access time\",\n        \"isomtime\": \"file modification time (ISO 8601 format)\",\n        \"isoctime\": \"file change time (ISO 8601 format)\",\n        \"isoatime\": \"file access time (ISO 8601 format)\",\n        \"xxh64\": \"XXH64 checksum of this file (note: this is NOT a cryptographic hash!)\",\n        \"health\": 'either \"healthy\" (file ok) or \"broken\" (if file has all-zero replacement chunks)',\n        \"archiveid\": \"internal ID of the archive\",\n        \"archivename\": \"name of the archive\",\n    }\n    KEY_GROUPS = (\n        (\"type\", \"mode\", \"uid\", \"gid\", \"user\", \"group\", \"path\", \"target\", \"hlid\", \"flags\"),\n        (\"size\", \"dsize\", \"num_chunks\", \"unique_chunks\"),\n        (\"mtime\", \"ctime\", \"atime\", \"isomtime\", \"isoctime\", \"isoatime\"),\n        tuple(sorted(hash_algorithms)),\n        (\"archiveid\", \"archivename\", \"extra\"),\n        (\"health\",),\n    )\n\n    KEYS_REQUIRING_CACHE = (\"dsize\", \"unique_chunks\")\n\n    @classmethod\n    def format_needs_cache(cls, format):\n        format_keys = {f[1] for f in Formatter().parse(format)}\n        return any(key in cls.KEYS_REQUIRING_CACHE for key in format_keys)\n\n    def __init__(self, archive, format):\n        from ..checksums import StreamingXXH64\n\n        static_data = {\"archivename\": archive.name, \"archiveid\": archive.fpr}\n        static_data.update(self.FIXED_KEYS)\n        super().__init__(format, static_data)\n        self.xxh64 = StreamingXXH64\n        self.archive = archive\n        self.format_keys = {f[1] for f in Formatter().parse(format)}\n        self.call_keys = {\n            \"size\": self.calculate_size,\n            \"dsize\": partial(self.sum_unique_chunks_metadata, lambda chunk: chunk.size),\n            \"num_chunks\": self.calculate_num_chunks,\n            \"unique_chunks\": partial(self.sum_unique_chunks_metadata, lambda chunk: 1),\n            \"isomtime\": partial(self.format_iso_time, \"mtime\"),\n            \"isoctime\": partial(self.format_iso_time, \"ctime\"),\n            \"isoatime\": partial(self.format_iso_time, \"atime\"),\n            \"mtime\": partial(self.format_time, \"mtime\"),\n            \"ctime\": partial(self.format_time, \"ctime\"),\n            \"atime\": partial(self.format_time, \"atime\"),\n        }\n        for hash_function in self.hash_algorithms:\n            self.call_keys[hash_function] = partial(self.hash_item, hash_function)\n        self.used_call_keys = set(self.call_keys) & self.format_keys\n\n    def get_item_data(self, item, jsonline=False):\n        item_data = {}\n        item_data.update({} if jsonline else self.static_data)\n\n        item_data.update(text_to_json(\"path\", item.path))\n        target = item.get(\"target\", \"\")\n        item_data.update(text_to_json(\"target\", target))\n        if not jsonline:\n            item_data[\"extra\"] = \"\" if not target else f\" -> {item_data['target']}\"\n\n        hlid = item.get(\"hlid\")\n        hlid = bin_to_hex(hlid) if hlid else \"\"\n        item_data[\"hlid\"] = hlid\n\n        mode = stat.filemode(item.mode)\n        item_type = mode[0]\n        item_data[\"type\"] = item_type\n        item_data[\"mode\"] = mode\n\n        item_data[\"uid\"] = item.get(\"uid\")  # int or None\n        item_data[\"gid\"] = item.get(\"gid\")  # int or None\n        item_data.update(text_to_json(\"user\", item.get(\"user\", str(item_data[\"uid\"]))))\n        item_data.update(text_to_json(\"group\", item.get(\"group\", str(item_data[\"gid\"]))))\n\n        if jsonline:\n            item_data[\"healthy\"] = \"chunks_healthy\" not in item\n        else:\n            item_data[\"health\"] = \"broken\" if \"chunks_healthy\" in item else \"healthy\"\n        item_data[\"flags\"] = item.get(\"bsdflags\")  # int if flags known, else (if flags unknown) None\n        for key in self.used_call_keys:\n            item_data[key] = self.call_keys[key](item)\n        return item_data\n\n    def sum_unique_chunks_metadata(self, metadata_func, item):\n        \"\"\"\n        sum unique chunks metadata, a unique chunk is a chunk which is referenced globally as often as it is in the\n        item\n\n        item: The item to sum its unique chunks' metadata\n        metadata_func: A function that takes a parameter of type ChunkIndexEntry and returns a number, used to return\n        the metadata needed from the chunk\n        \"\"\"\n        chunk_index = self.archive.cache.chunks\n        chunks = item.get(\"chunks\", [])\n        chunks_counter = Counter(c.id for c in chunks)\n        return sum(metadata_func(c) for c in chunks if chunk_index[c.id].refcount == chunks_counter[c.id])\n\n    def calculate_num_chunks(self, item):\n        return len(item.get(\"chunks\", []))\n\n    def calculate_size(self, item):\n        # note: does not support hardlink slaves, they will be size 0\n        return item.get_size()\n\n    def hash_item(self, hash_function, item):\n        if \"chunks\" not in item:\n            return \"\"\n        if hash_function == \"xxh64\":\n            hash = self.xxh64()\n        elif hash_function in self.hash_algorithms:\n            hash = hashlib.new(hash_function)\n        for data in self.archive.pipeline.fetch_many([c.id for c in item.chunks]):\n            hash.update(data)\n        return hash.hexdigest()\n\n    def format_time(self, key, item):\n        return OutputTimestamp(safe_timestamp(item.get(key) or item.mtime))\n\n    def format_iso_time(self, key, item):\n        return self.format_time(key, item).isoformat()\n\n\nclass DiffFormatter(BaseFormatter):\n    KEY_DESCRIPTIONS = {\n        \"path\": \"archived file path\",\n        \"change\": \"all available changes\",\n        \"content\": \"file content change\",\n        \"mode\": \"file mode change\",\n        \"type\": \"file type change\",\n        \"owner\": \"file owner (user/group) change\",\n        \"user\": \"file user change\",\n        \"group\": \"file group change\",\n        \"link\": \"file link change\",\n        \"directory\": \"file directory change\",\n        \"blkdev\": \"file block device change\",\n        \"chrdev\": \"file character device change\",\n        \"fifo\": \"file fifo change\",\n        \"mtime\": \"file modification time change\",\n        \"ctime\": \"file change time change\",\n        \"isomtime\": \"file modification time change (ISO 8601)\",\n        \"isoctime\": \"file creation time change (ISO 8601)\",\n    }\n    KEY_GROUPS = (\n        (\"path\", \"change\"),\n        (\"content\", \"mode\", \"type\", \"owner\", \"group\", \"user\"),\n        (\"link\", \"directory\", \"blkdev\", \"chrdev\", \"fifo\"),\n        (\"mtime\", \"ctime\", \"isomtime\", \"isoctime\"),\n    )\n    METADATA = (\"mode\", \"type\", \"owner\", \"group\", \"user\", \"mtime\", \"ctime\")\n\n    def __init__(self, format, content_only=False):\n        static_data = {}\n        static_data.update(self.FIXED_KEYS)\n        super().__init__(format or \"{content}{link}{directory}{blkdev}{chrdev}{fifo} {path}{NL}\", static_data)\n        self.content_only = content_only\n        self.format_keys = {f[1] for f in Formatter().parse(format)}\n        self.call_keys = {\n            \"content\": self.format_content,\n            \"mode\": self.format_mode,\n            \"type\": partial(self.format_mode, filetype=True),\n            \"owner\": partial(self.format_owner),\n            \"group\": partial(self.format_owner, spec=\"group\"),\n            \"user\": partial(self.format_owner, spec=\"user\"),\n            \"link\": partial(self.format_other, \"link\"),\n            \"directory\": partial(self.format_other, \"directory\"),\n            \"blkdev\": partial(self.format_other, \"blkdev\"),\n            \"chrdev\": partial(self.format_other, \"chrdev\"),\n            \"fifo\": partial(self.format_other, \"fifo\"),\n            \"mtime\": partial(self.format_time, \"mtime\"),\n            \"ctime\": partial(self.format_time, \"ctime\"),\n            \"isomtime\": partial(self.format_iso_time, \"mtime\"),\n            \"isoctime\": partial(self.format_iso_time, \"ctime\"),\n        }\n        self.used_call_keys = set(self.call_keys) & self.format_keys\n        if self.content_only:\n            self.used_call_keys -= set(self.METADATA)\n\n    def get_item_data(self, item: \"ItemDiff\", jsonline=False) -> dict:\n        diff_data = {}\n        for key in self.used_call_keys:\n            diff_data[key] = self.call_keys[key](item)\n\n        change = []\n        for key in self.call_keys:\n            if key in (\"isomtime\", \"isoctime\"):\n                continue\n            if self.content_only and key in self.METADATA:\n                continue\n            change.append(self.call_keys[key](item))\n        diff_data[\"change\"] = \" \".join([v for v in change if v])\n        diff_data[\"path\"] = item.path\n        diff_data.update({} if jsonline else self.static_data)\n        return diff_data\n\n    def format_other(self, key, diff: \"ItemDiff\"):\n        change = diff.changes().get(key)\n        return f\"{change.diff_type}\".ljust(27) if change else \"\"  # 27 is the length of the content change\n\n    def format_mode(self, diff: \"ItemDiff\", filetype=False):\n        change = diff.type() if filetype else diff.mode()\n        return f\"[{change.diff_data['item1']} -> {change.diff_data['item2']}]\" if change else \"\"\n\n    def format_owner(self, diff: \"ItemDiff\", spec: Literal[\"owner\", \"user\", \"group\"] = \"owner\"):\n        if spec == \"user\":\n            change = diff.user()\n            return f\"[{change.diff_data['item1']} -> {change.diff_data['item2']}]\" if change else \"\"\n        if spec == \"group\":\n            change = diff.group()\n            return f\"[{change.diff_data['item1']} -> {change.diff_data['item2']}]\" if change else \"\"\n        if spec != \"owner\":\n            raise ValueError(f\"Invalid owner spec: {spec}\")\n        change = diff.owner()\n        if change:\n            return \"[{}:{} -> {}:{}]\".format(\n                change.diff_data[\"item1\"][0],\n                change.diff_data[\"item1\"][1],\n                change.diff_data[\"item2\"][0],\n                change.diff_data[\"item2\"][1],\n            )\n        return \"\"\n\n    def format_content(self, diff: \"ItemDiff\"):\n        change = diff.content()\n        if change:\n            if change.diff_type == \"added\":\n                return \"{}: {:>20}\".format(change.diff_type, format_file_size(change.diff_data[\"added\"]))\n            if change.diff_type == \"removed\":\n                return \"{}: {:>18}\".format(change.diff_type, format_file_size(change.diff_data[\"removed\"]))\n            if \"added\" not in change.diff_data and \"removed\" not in change.diff_data:\n                return \"modified:  (can't get size)\"\n            return \"{}: {:>8} {:>8}\".format(\n                change.diff_type,\n                format_file_size(change.diff_data[\"added\"], precision=1, sign=True),\n                format_file_size(-change.diff_data[\"removed\"], precision=1, sign=True),\n            )\n        return \"\"\n\n    def format_time(self, key, diff: \"ItemDiff\"):\n        change = diff.changes().get(key)\n        return f\"[{key}: {change.diff_data['item1']} -> {change.diff_data['item2']}]\" if change else \"\"\n\n    def format_iso_time(self, key, diff: \"ItemDiff\"):\n        change = diff.changes().get(key)\n        return (\n            f\"[{key}: {change.diff_data['item1'].isoformat()} -> {change.diff_data['item2'].isoformat()}]\"\n            if change\n            else \"\"\n        )\n\n\ndef file_status(mode):\n    if stat.S_ISREG(mode):\n        return \"A\"\n    elif stat.S_ISDIR(mode):\n        return \"d\"\n    elif stat.S_ISBLK(mode):\n        return \"b\"\n    elif stat.S_ISCHR(mode):\n        return \"c\"\n    elif stat.S_ISLNK(mode):\n        return \"s\"\n    elif stat.S_ISFIFO(mode):\n        return \"f\"\n    return \"?\"\n\n\ndef clean_lines(lines, lstrip=None, rstrip=None, remove_empty=True, remove_comments=True):\n    \"\"\"\n    clean lines (usually read from a config file):\n\n    1. strip whitespace (left and right), 2. remove empty lines, 3. remove comments.\n\n    note: only \"pure comment lines\" are supported, no support for \"trailing comments\".\n\n    :param lines: input line iterator (e.g. list or open text file) that gives unclean input lines\n    :param lstrip: lstrip call arguments or False, if lstripping is not desired\n    :param rstrip: rstrip call arguments or False, if rstripping is not desired\n    :param remove_comments: remove comment lines (lines starting with \"#\")\n    :param remove_empty: remove empty lines\n    :return: yields processed lines\n    \"\"\"\n    for line in lines:\n        if lstrip is not False:\n            line = line.lstrip(lstrip)\n        if rstrip is not False:\n            line = line.rstrip(rstrip)\n        if remove_empty and not line:\n            continue\n        if remove_comments and line.startswith(\"#\"):\n            continue\n        yield line\n\n\ndef swidth_slice(string, max_width):\n    \"\"\"\n    Return a slice of *max_width* cells from *string*.\n\n    Negative *max_width* means from the end of string.\n\n    *max_width* is in units of character cells (or \"columns\").\n    Latin characters are usually one cell wide, many CJK characters are two cells wide.\n    \"\"\"\n    from ..platform import swidth\n\n    reverse = max_width < 0\n    max_width = abs(max_width)\n    if reverse:\n        string = reversed(string)\n    current_swidth = 0\n    result = []\n    for character in string:\n        current_swidth += swidth(character)\n        if current_swidth > max_width:\n            break\n        result.append(character)\n    if reverse:\n        result.reverse()\n    return \"\".join(result)\n\n\ndef ellipsis_truncate(msg, space):\n    \"\"\"\n    shorten a long string by adding ellipsis between it and return it, example:\n    this_is_a_very_long_string -------> this_is..._string\n    \"\"\"\n    from ..platform import swidth\n\n    ellipsis_width = swidth(\"...\")\n    msg_width = swidth(msg)\n    if space < 8:\n        # if there is very little space, just show ...\n        return \"...\" + \" \" * (space - ellipsis_width)\n    if space < ellipsis_width + msg_width:\n        return f\"{swidth_slice(msg, space // 2 - ellipsis_width)}...{swidth_slice(msg, -space // 2)}\"\n    return msg + \" \" * (space - msg_width)\n\n\nclass BorgJsonEncoder(json.JSONEncoder):\n    def default(self, o):\n        from ..repository import Repository\n        from ..remote import RemoteRepository\n        from ..archive import Archive\n        from ..cache import LocalCache, AdHocCache\n\n        if isinstance(o, Repository) or isinstance(o, RemoteRepository):\n            return {\"id\": bin_to_hex(o.id), \"location\": o._location.canonical_path()}\n        if isinstance(o, Archive):\n            return o.info()\n        if isinstance(o, LocalCache):\n            return {\"path\": o.path, \"stats\": o.stats()}\n        if isinstance(o, AdHocCache):\n            return {\"stats\": o.stats()}\n        if callable(getattr(o, \"to_json\", None)):\n            return o.to_json()\n        return super().default(o)\n\n\ndef basic_json_data(manifest, *, cache=None, extra=None):\n    key = manifest.key\n    data = extra or {}\n    data.update({\"repository\": BorgJsonEncoder().default(manifest.repository), \"encryption\": {\"mode\": key.ARG_NAME}})\n    data[\"repository\"][\"last_modified\"] = OutputTimestamp(manifest.last_timestamp)\n    if key.NAME.startswith(\"key file\"):\n        data[\"encryption\"][\"keyfile\"] = key.find_key()\n    if cache:\n        data[\"cache\"] = cache\n    return data\n\n\ndef json_dump(obj):\n    \"\"\"Dump using BorgJSONEncoder.\"\"\"\n    return json.dumps(obj, sort_keys=True, indent=4, cls=BorgJsonEncoder)\n\n\ndef json_print(obj):\n    print(json_dump(obj))\n\n\ndef prepare_dump_dict(d):\n    def decode_bytes(value):\n        # this should somehow be reversible later, but usual strings should\n        # look nice and chunk ids should mostly show in hex. Use a special\n        # inband signaling character (ASCII DEL) to distinguish between\n        # decoded and hex mode.\n        if not value.startswith(b\"\\x7f\"):\n            try:\n                value = value.decode()\n                return value\n            except UnicodeDecodeError:\n                pass\n        return \"\\u007f\" + bin_to_hex(value)\n\n    def decode_tuple(t):\n        res = []\n        for value in t:\n            if isinstance(value, dict):\n                value = decode(value)\n            elif isinstance(value, tuple) or isinstance(value, list):\n                value = decode_tuple(value)\n            elif isinstance(value, bytes):\n                value = decode_bytes(value)\n            res.append(value)\n        return res\n\n    def decode(d):\n        res = OrderedDict()\n        for key, value in d.items():\n            if isinstance(value, dict):\n                value = decode(value)\n            elif isinstance(value, (tuple, list)):\n                value = decode_tuple(value)\n            elif isinstance(value, bytes):\n                value = decode_bytes(value)\n            elif isinstance(value, Timestamp):\n                value = value.to_unix_nano()\n            if isinstance(key, bytes):\n                key = key.decode()\n            res[key] = value\n        return res\n\n    return decode(d)\n\n\nclass Highlander(argparse.Action):\n    \"\"\"make sure some option is only given once\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.__called = False\n        super().__init__(*args, **kwargs)\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        if self.__called:\n            raise argparse.ArgumentError(self, \"There can be only one.\")\n        self.__called = True\n        setattr(namespace, self.dest, values)\n\n\nclass MakePathSafeAction(Highlander):\n    def __call__(self, parser, namespace, path, option_string=None):\n        try:\n            sanitized_path = make_path_safe(path)\n        except ValueError as e:\n            raise argparse.ArgumentError(self, e)\n        if sanitized_path == \".\":\n            raise argparse.ArgumentError(self, f\"{path!r} is not a valid file name\")\n        setattr(namespace, self.dest, sanitized_path)\n", "import shutil\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom ...archive import ChunkBuffer\nfrom ...constants import *  # NOQA\nfrom ...helpers import bin_to_hex\nfrom ...helpers import msgpack\nfrom ...manifest import Manifest\nfrom ...repository import Repository\nfrom . import cmd, src_file, create_src_archive, open_archive, generate_archiver_tests, RK_ENCRYPTION\n\npytest_generate_tests = lambda metafunc: generate_archiver_tests(metafunc, kinds=\"local,remote,binary\")  # NOQA\n\n\ndef check_cmd_setup(archiver):\n    with patch.object(ChunkBuffer, \"BUFFER_SIZE\", 10):\n        cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n        create_src_archive(archiver, \"archive1\")\n        create_src_archive(archiver, \"archive2\")\n\n\ndef test_check_usage(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n\n    output = cmd(archiver, \"check\", \"-v\", \"--progress\", exit_code=0)\n    assert \"Starting repository check\" in output\n    assert \"Starting archive consistency check\" in output\n    assert \"Checking segments\" in output\n\n    output = cmd(archiver, \"check\", \"-v\", \"--repository-only\", exit_code=0)\n    assert \"Starting repository check\" in output\n    assert \"Starting archive consistency check\" not in output\n    assert \"Checking segments\" not in output\n\n    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", exit_code=0)\n    assert \"Starting repository check\" not in output\n    assert \"Starting archive consistency check\" in output\n\n    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--match-archives=archive2\", exit_code=0)\n    assert \"archive1\" not in output\n\n    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--first=1\", exit_code=0)\n    assert \"archive1\" in output\n    assert \"archive2\" not in output\n\n    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--last=1\", exit_code=0)\n    assert \"archive1\" not in output\n    assert \"archive2\" in output\n\n\ndef test_date_matching(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n\n    shutil.rmtree(archiver.repository_path)\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    earliest_ts = \"2022-11-20T23:59:59\"\n    ts_in_between = \"2022-12-18T23:59:59\"\n    create_src_archive(archiver, \"archive1\", ts=earliest_ts)\n    create_src_archive(archiver, \"archive2\", ts=ts_in_between)\n    create_src_archive(archiver, \"archive3\")\n    cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--oldest=23e\", exit_code=2)\n\n    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--oldest=1m\", exit_code=0)\n    assert \"archive1\" in output\n    assert \"archive2\" in output\n    assert \"archive3\" not in output\n\n    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--newest=1m\", exit_code=0)\n    assert \"archive3\" in output\n    assert \"archive2\" not in output\n    assert \"archive1\" not in output\n\n    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--newer=1d\", exit_code=0)\n    assert \"archive3\" in output\n    assert \"archive1\" not in output\n    assert \"archive2\" not in output\n\n    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--older=1d\", exit_code=0)\n    assert \"archive1\" in output\n    assert \"archive2\" in output\n    assert \"archive3\" not in output\n\n    # check for output when timespan older than the earliest archive is given. Issue #1711\n    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--older=9999m\", exit_code=0)\n    for archive in (\"archive1\", \"archive2\", \"archive3\"):\n        assert archive not in output\n\n\ndef test_missing_file_chunk(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n\n    archive, repository = open_archive(archiver.repository_path, \"archive1\")\n\n    with repository:\n        for item in archive.iter_items():\n            if item.path.endswith(src_file):\n                valid_chunks = item.chunks\n                killed_chunk = valid_chunks[-1]\n                repository.delete(killed_chunk.id)\n                break\n        else:\n            pytest.fail(\"should not happen\")  # convert 'fail'\n        repository.commit(compact=False)\n\n    cmd(archiver, \"check\", exit_code=1)\n    output = cmd(archiver, \"check\", \"--repair\", exit_code=0)\n    assert \"New missing file chunk detected\" in output\n\n    cmd(archiver, \"check\", exit_code=0)\n    output = cmd(archiver, \"list\", \"archive1\", \"--format={health}#{path}{NL}\", exit_code=0)\n    assert \"broken#\" in output\n\n    # check that the file in the old archives has now a different chunk list without the killed chunk\n    for archive_name in (\"archive1\", \"archive2\"):\n        archive, repository = open_archive(archiver.repository_path, archive_name)\n        with repository:\n            for item in archive.iter_items():\n                if item.path.endswith(src_file):\n                    assert valid_chunks != item.chunks\n                    assert killed_chunk not in item.chunks\n                    break\n            else:\n                pytest.fail(\"should not happen\")  # convert 'fail'\n\n    # do a fresh backup (that will include the killed chunk)\n    with patch.object(ChunkBuffer, \"BUFFER_SIZE\", 10):\n        create_src_archive(archiver, \"archive3\")\n\n    # check should be able to heal the file now:\n    output = cmd(archiver, \"check\", \"-v\", \"--repair\", exit_code=0)\n    assert \"Healed previously missing file chunk\" in output\n    assert f\"{src_file}: Completely healed previously damaged file!\" in output\n\n    # check that the file in the old archives has the correct chunks again\n    for archive_name in (\"archive1\", \"archive2\"):\n        archive, repository = open_archive(archiver.repository_path, archive_name)\n        with repository:\n            for item in archive.iter_items():\n                if item.path.endswith(src_file):\n                    assert valid_chunks == item.chunks\n                    break\n            else:\n                pytest.fail(\"should not happen\")\n\n    # list is also all-healthy again\n    output = cmd(archiver, \"list\", \"archive1\", \"--format={health}#{path}{NL}\", exit_code=0)\n    assert \"broken#\" not in output\n\n\ndef test_missing_archive_item_chunk(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n    archive, repository = open_archive(archiver.repository_path, \"archive1\")\n    with repository:\n        repository.delete(archive.metadata.items[0])\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", exit_code=1)\n    cmd(archiver, \"check\", \"--repair\", exit_code=0)\n    cmd(archiver, \"check\", exit_code=0)\n\n\ndef test_missing_archive_metadata(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n    archive, repository = open_archive(archiver.repository_path, \"archive1\")\n    with repository:\n        repository.delete(archive.id)\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", exit_code=1)\n    cmd(archiver, \"check\", \"--repair\", exit_code=0)\n    cmd(archiver, \"check\", exit_code=0)\n\n\ndef test_missing_manifest(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n    archive, repository = open_archive(archiver.repository_path, \"archive1\")\n    with repository:\n        repository.delete(Manifest.MANIFEST_ID)\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", exit_code=1)\n    output = cmd(archiver, \"check\", \"-v\", \"--repair\", exit_code=0)\n    assert \"archive1\" in output\n    assert \"archive2\" in output\n    cmd(archiver, \"check\", exit_code=0)\n\n\ndef test_corrupted_manifest(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n    archive, repository = open_archive(archiver.repository_path, \"archive1\")\n    with repository:\n        manifest = repository.get(Manifest.MANIFEST_ID)\n        corrupted_manifest = manifest + b\"corrupted!\"\n        repository.put(Manifest.MANIFEST_ID, corrupted_manifest)\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", exit_code=1)\n    output = cmd(archiver, \"check\", \"-v\", \"--repair\", exit_code=0)\n    assert \"archive1\" in output\n    assert \"archive2\" in output\n    cmd(archiver, \"check\", exit_code=0)\n\n\ndef test_manifest_rebuild_corrupted_chunk(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n    archive, repository = open_archive(archiver.repository_path, \"archive1\")\n    with repository:\n        manifest = repository.get(Manifest.MANIFEST_ID)\n        corrupted_manifest = manifest + b\"corrupted!\"\n        repository.put(Manifest.MANIFEST_ID, corrupted_manifest)\n        chunk = repository.get(archive.id)\n        corrupted_chunk = chunk + b\"corrupted!\"\n        repository.put(archive.id, corrupted_chunk)\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", exit_code=1)\n    output = cmd(archiver, \"check\", \"-v\", \"--repair\", exit_code=0)\n    assert \"archive2\" in output\n    cmd(archiver, \"check\", exit_code=0)\n\n\ndef test_manifest_rebuild_duplicate_archive(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n    archive, repository = open_archive(archiver.repository_path, \"archive1\")\n    repo_objs = archive.repo_objs\n    with repository:\n        manifest = repository.get(Manifest.MANIFEST_ID)\n        corrupted_manifest = manifest + b\"corrupted!\"\n        repository.put(Manifest.MANIFEST_ID, corrupted_manifest)\n        archive = msgpack.packb(\n            {\n                \"command_line\": \"\",\n                \"item_ptrs\": [],\n                \"hostname\": \"foo\",\n                \"username\": \"bar\",\n                \"name\": \"archive1\",\n                \"time\": \"2016-12-15T18:49:51.849711\",\n                \"version\": 2,\n            }\n        )\n        archive_id = repo_objs.id_hash(archive)\n        repository.put(archive_id, repo_objs.format(archive_id, {}, archive))\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", exit_code=1)\n    cmd(archiver, \"check\", \"--repair\", exit_code=0)\n    output = cmd(archiver, \"rlist\")\n    assert \"archive1\" in output\n    assert \"archive1.1\" in output\n    assert \"archive2\" in output\n\n\ndef test_extra_chunks(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    if archiver.get_kind() == \"remote\":\n        pytest.skip(\"only works locally\")\n    check_cmd_setup(archiver)\n    cmd(archiver, \"check\", exit_code=0)\n    with Repository(archiver.repository_location, exclusive=True) as repository:\n        repository.put(b\"01234567890123456789012345678901\", b\"xxxx\")\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", exit_code=1)\n    cmd(archiver, \"check\", exit_code=1)\n    cmd(archiver, \"check\", \"--repair\", exit_code=0)\n    cmd(archiver, \"check\", exit_code=0)\n    cmd(archiver, \"extract\", \"archive1\", \"--dry-run\", exit_code=0)\n\n\n@pytest.mark.parametrize(\"init_args\", [[\"--encryption=repokey-aes-ocb\"], [\"--encryption\", \"none\"]])\ndef test_verify_data(archivers, request, init_args):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n    shutil.rmtree(archiver.repository_path)\n    cmd(archiver, \"rcreate\", *init_args)\n    create_src_archive(archiver, \"archive1\")\n    archive, repository = open_archive(archiver.repository_path, \"archive1\")\n    with repository:\n        for item in archive.iter_items():\n            if item.path.endswith(src_file):\n                chunk = item.chunks[-1]\n                data = repository.get(chunk.id)\n                data = data[0:100] + b\"x\" + data[101:]\n                repository.put(chunk.id, data)\n                break\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", exit_code=0)\n    output = cmd(archiver, \"check\", \"--verify-data\", exit_code=1)\n    assert bin_to_hex(chunk.id) + \", integrity error\" in output\n\n    # repair (heal is tested in another test)\n    output = cmd(archiver, \"check\", \"--repair\", \"--verify-data\", exit_code=0)\n    assert bin_to_hex(chunk.id) + \", integrity error\" in output\n    assert f\"{src_file}: New missing file chunk detected\" in output\n\n\ndef test_empty_repository(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    if archiver.get_kind() == \"remote\":\n        pytest.skip(\"only works locally\")\n    check_cmd_setup(archiver)\n    with Repository(archiver.repository_location, exclusive=True) as repository:\n        for id_ in repository.list():\n            repository.delete(id_)\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", exit_code=1)\n", "import os\nimport shutil\nfrom datetime import datetime, timezone, timedelta\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom ...cache import Cache, LocalCache\nfrom ...constants import *  # NOQA\nfrom ...crypto.key import TAMRequiredError\nfrom ...helpers import Location, get_security_dir, bin_to_hex\nfrom ...helpers import EXIT_ERROR\nfrom ...helpers import msgpack\nfrom ...manifest import Manifest, MandatoryFeatureUnsupported\nfrom ...remote import RemoteRepository, PathNotAllowed\nfrom ...repository import Repository\nfrom .. import llfuse\nfrom .. import changedir\nfrom . import cmd, _extract_repository_id, open_repository, check_cache, create_test_files, create_src_archive\nfrom . import _set_repository_id, create_regular_file, assert_creates_file, generate_archiver_tests, RK_ENCRYPTION\n\npytest_generate_tests = lambda metafunc: generate_archiver_tests(metafunc, kinds=\"local,remote\")  # NOQA\n\n\ndef get_security_directory(repo_path):\n    repository_id = bin_to_hex(_extract_repository_id(repo_path))\n    return get_security_dir(repository_id)\n\n\ndef add_unknown_feature(repo_path, operation):\n    with Repository(repo_path, exclusive=True) as repository:\n        manifest = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)\n        manifest.config[\"feature_flags\"] = {operation.value: {\"mandatory\": [\"unknown-feature\"]}}\n        manifest.write()\n        repository.commit(compact=False)\n\n\ndef cmd_raises_unknown_feature(archiver, args):\n    if archiver.FORK_DEFAULT:\n        cmd(archiver, *args, exit_code=EXIT_ERROR)\n    else:\n        with pytest.raises(MandatoryFeatureUnsupported) as excinfo:\n            cmd(archiver, *args)\n        assert excinfo.value.args == ([\"unknown-feature\"],)\n\n\ndef test_repository_swap_detection(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    create_test_files(archiver.input_path)\n    os.environ[\"BORG_PASSPHRASE\"] = \"passphrase\"\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    repository_id = _extract_repository_id(archiver.repository_path)\n    cmd(archiver, \"create\", \"test\", \"input\")\n    shutil.rmtree(archiver.repository_path)\n    cmd(archiver, \"rcreate\", \"--encryption=none\")\n    _set_repository_id(archiver.repository_path, repository_id)\n    assert repository_id == _extract_repository_id(archiver.repository_path)\n    if archiver.FORK_DEFAULT:\n        cmd(archiver, \"create\", \"test.2\", \"input\", exit_code=EXIT_ERROR)\n    else:\n        with pytest.raises(Cache.EncryptionMethodMismatch):\n            cmd(archiver, \"create\", \"test.2\", \"input\")\n\n\ndef test_repository_swap_detection2(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    create_test_files(archiver.input_path)\n    original_location = archiver.repository_location\n    archiver.repository_location = original_location + \"_unencrypted\"\n    cmd(archiver, \"rcreate\", \"--encryption=none\")\n    os.environ[\"BORG_PASSPHRASE\"] = \"passphrase\"\n    archiver.repository_location = original_location + \"_encrypted\"\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    cmd(archiver, \"create\", \"test\", \"input\")\n    shutil.rmtree(archiver.repository_path + \"_encrypted\")\n    os.replace(archiver.repository_path + \"_unencrypted\", archiver.repository_path + \"_encrypted\")\n    if archiver.FORK_DEFAULT:\n        cmd(archiver, \"create\", \"test.2\", \"input\", exit_code=EXIT_ERROR)\n    else:\n        with pytest.raises(Cache.RepositoryAccessAborted):\n            cmd(archiver, \"create\", \"test.2\", \"input\")\n\n\ndef test_repository_swap_detection_no_cache(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    create_test_files(archiver.input_path)\n    os.environ[\"BORG_PASSPHRASE\"] = \"passphrase\"\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    repository_id = _extract_repository_id(archiver.repository_path)\n    cmd(archiver, \"create\", \"test\", \"input\")\n    shutil.rmtree(archiver.repository_path)\n    cmd(archiver, \"rcreate\", \"--encryption=none\")\n    _set_repository_id(archiver.repository_path, repository_id)\n    assert repository_id == _extract_repository_id(archiver.repository_path)\n    cmd(archiver, \"rdelete\", \"--cache-only\")\n    if archiver.FORK_DEFAULT:\n        cmd(archiver, \"create\", \"test.2\", \"input\", exit_code=EXIT_ERROR)\n    else:\n        with pytest.raises(Cache.EncryptionMethodMismatch):\n            cmd(archiver, \"create\", \"test.2\", \"input\")\n\n\ndef test_repository_swap_detection2_no_cache(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    original_location = archiver.repository_location\n    create_test_files(archiver.input_path)\n    archiver.repository_location = original_location + \"_unencrypted\"\n    cmd(archiver, \"rcreate\", \"--encryption=none\")\n    os.environ[\"BORG_PASSPHRASE\"] = \"passphrase\"\n    archiver.repository_location = original_location + \"_encrypted\"\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    cmd(archiver, \"create\", \"test\", \"input\")\n    archiver.repository_location = original_location + \"_unencrypted\"\n    cmd(archiver, \"rdelete\", \"--cache-only\")\n    archiver.repository_location = original_location + \"_encrypted\"\n    cmd(archiver, \"rdelete\", \"--cache-only\")\n    shutil.rmtree(archiver.repository_path + \"_encrypted\")\n    os.replace(archiver.repository_path + \"_unencrypted\", archiver.repository_path + \"_encrypted\")\n    if archiver.FORK_DEFAULT:\n        cmd(archiver, \"create\", \"test.2\", \"input\", exit_code=EXIT_ERROR)\n    else:\n        with pytest.raises(Cache.RepositoryAccessAborted):\n            cmd(archiver, \"create\", \"test.2\", \"input\")\n\n\ndef test_repository_swap_detection_repokey_blank_passphrase(archivers, request, monkeypatch):\n    archiver = request.getfixturevalue(archivers)\n    # Check that a repokey repo with a blank passphrase is considered like a plaintext repo.\n    create_test_files(archiver.input_path)\n    # User initializes her repository with her passphrase\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    cmd(archiver, \"create\", \"test\", \"input\")\n    # Attacker replaces it with her own repository, which is encrypted but has no passphrase set\n    shutil.rmtree(archiver.repository_path)\n\n    monkeypatch.setenv(\"BORG_PASSPHRASE\", \"\")\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    # Delete cache & security database, AKA switch to user perspective\n    cmd(archiver, \"rdelete\", \"--cache-only\")\n    shutil.rmtree(get_security_directory(archiver.repository_path))\n\n    monkeypatch.delenv(\"BORG_PASSPHRASE\")\n    # This is the part were the user would be tricked, e.g. she assumes that BORG_PASSPHRASE\n    # is set, while it isn't. Previously this raised no warning,\n    # since the repository is, technically, encrypted.\n    if archiver.FORK_DEFAULT:\n        cmd(archiver, \"create\", \"test.2\", \"input\", exit_code=EXIT_ERROR)\n    else:\n        with pytest.raises(Cache.CacheInitAbortedError):\n            cmd(archiver, \"create\", \"test.2\", \"input\")\n\n\ndef test_repository_move(archivers, request, monkeypatch):\n    archiver = request.getfixturevalue(archivers)\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    security_dir = get_security_directory(archiver.repository_path)\n    os.replace(archiver.repository_path, archiver.repository_path + \"_new\")\n    archiver.repository_location += \"_new\"\n    monkeypatch.setenv(\"BORG_RELOCATED_REPO_ACCESS_IS_OK\", \"yes\")\n    cmd(archiver, \"rinfo\")\n    monkeypatch.delenv(\"BORG_RELOCATED_REPO_ACCESS_IS_OK\")\n    with open(os.path.join(security_dir, \"location\")) as fd:\n        location = fd.read()\n        assert location == Location(archiver.repository_location).canonical_path()\n    # Needs no confirmation anymore\n    cmd(archiver, \"rinfo\")\n    shutil.rmtree(archiver.cache_path)\n    cmd(archiver, \"rinfo\")\n    shutil.rmtree(security_dir)\n    cmd(archiver, \"rinfo\")\n    for file in (\"location\", \"key-type\", \"manifest-timestamp\"):\n        assert os.path.exists(os.path.join(security_dir, file))\n\n\ndef test_security_dir_compat(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    with open(os.path.join(get_security_directory(archiver.repository_path), \"location\"), \"w\") as fd:\n        fd.write(\"something outdated\")\n    # This is fine, because the cache still has the correct information. security_dir and cache can disagree\n    # if older versions are used to confirm a renamed repository.\n    cmd(archiver, \"rinfo\")\n\n\ndef test_unknown_unencrypted(archivers, request, monkeypatch):\n    archiver = request.getfixturevalue(archivers)\n    cmd(archiver, \"rcreate\", \"--encryption=none\")\n    # Ok: repository is known\n    cmd(archiver, \"rinfo\")\n\n    # Ok: repository is still known (through security_dir)\n    shutil.rmtree(archiver.cache_path)\n    cmd(archiver, \"rinfo\")\n\n    # Needs confirmation: cache and security dir both gone (e.g. another host or rm -rf ~)\n    shutil.rmtree(get_security_directory(archiver.repository_path))\n    if archiver.FORK_DEFAULT:\n        cmd(archiver, \"rinfo\", exit_code=EXIT_ERROR)\n    else:\n        with pytest.raises(Cache.CacheInitAbortedError):\n            cmd(archiver, \"rinfo\")\n    monkeypatch.setenv(\"BORG_UNKNOWN_UNENCRYPTED_REPO_ACCESS_IS_OK\", \"yes\")\n    cmd(archiver, \"rinfo\")\n\n\ndef test_unknown_feature_on_create(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    print(cmd(archiver, \"rcreate\", RK_ENCRYPTION))\n    add_unknown_feature(archiver.repository_path, Manifest.Operation.WRITE)\n    cmd_raises_unknown_feature(archiver, [\"create\", \"test\", \"input\"])\n\n\ndef test_unknown_feature_on_cache_sync(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    cmd(archiver, \"rdelete\", \"--cache-only\")\n    add_unknown_feature(archiver.repository_path, Manifest.Operation.READ)\n    cmd_raises_unknown_feature(archiver, [\"create\", \"test\", \"input\"])\n\n\ndef test_unknown_feature_on_change_passphrase(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    print(cmd(archiver, \"rcreate\", RK_ENCRYPTION))\n    add_unknown_feature(archiver.repository_path, Manifest.Operation.CHECK)\n    cmd_raises_unknown_feature(archiver, [\"key\", \"change-passphrase\"])\n\n\ndef test_unknown_feature_on_read(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    print(cmd(archiver, \"rcreate\", RK_ENCRYPTION))\n    cmd(archiver, \"create\", \"test\", \"input\")\n    add_unknown_feature(archiver.repository_path, Manifest.Operation.READ)\n    with changedir(\"output\"):\n        cmd_raises_unknown_feature(archiver, [\"extract\", \"test\"])\n    cmd_raises_unknown_feature(archiver, [\"rlist\"])\n    cmd_raises_unknown_feature(archiver, [\"info\", \"-a\", \"test\"])\n\n\ndef test_unknown_feature_on_rename(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    print(cmd(archiver, \"rcreate\", RK_ENCRYPTION))\n    cmd(archiver, \"create\", \"test\", \"input\")\n    add_unknown_feature(archiver.repository_path, Manifest.Operation.CHECK)\n    cmd_raises_unknown_feature(archiver, [\"rename\", \"test\", \"other\"])\n\n\ndef test_unknown_feature_on_delete(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    print(cmd(archiver, \"rcreate\", RK_ENCRYPTION))\n    cmd(archiver, \"create\", \"test\", \"input\")\n    add_unknown_feature(archiver.repository_path, Manifest.Operation.DELETE)\n    # delete of an archive raises\n    cmd_raises_unknown_feature(archiver, [\"delete\", \"-a\", \"test\"])\n    cmd_raises_unknown_feature(archiver, [\"prune\", \"--keep-daily=3\"])\n    # delete of the whole repository ignores features\n    cmd(archiver, \"rdelete\")\n\n\n@pytest.mark.skipif(not llfuse, reason=\"llfuse not installed\")\ndef test_unknown_feature_on_mount(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    cmd(archiver, \"create\", \"test\", \"input\")\n    add_unknown_feature(archiver.repository_path, Manifest.Operation.READ)\n    mountpoint = os.path.join(archiver.tmpdir, \"mountpoint\")\n    os.mkdir(mountpoint)\n    # XXX this might hang if it doesn't raise an error\n    archiver.repository_location += \"::test\"\n    cmd_raises_unknown_feature(archiver, [\"mount\", mountpoint])\n\n\n@pytest.mark.allow_cache_wipe\ndef test_unknown_mandatory_feature_in_cache(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    remote_repo = archiver.get_kind() == \"remote\"\n    print(cmd(archiver, \"rcreate\", RK_ENCRYPTION))\n\n    with Repository(archiver.repository_path, exclusive=True) as repository:\n        if remote_repo:\n            repository._location = Location(archiver.repository_location)\n        manifest = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)\n        with Cache(repository, manifest) as cache:\n            cache.begin_txn()\n            cache.cache_config.mandatory_features = {\"unknown-feature\"}\n            cache.commit()\n\n    if archiver.FORK_DEFAULT:\n        cmd(archiver, \"create\", \"test\", \"input\")\n    else:\n        called = False\n        wipe_cache_safe = LocalCache.wipe_cache\n\n        def wipe_wrapper(*args):\n            nonlocal called\n            called = True\n            wipe_cache_safe(*args)\n\n        with patch.object(LocalCache, \"wipe_cache\", wipe_wrapper):\n            cmd(archiver, \"create\", \"test\", \"input\")\n\n        assert called\n\n    with Repository(archiver.repository_path, exclusive=True) as repository:\n        if remote_repo:\n            repository._location = Location(archiver.repository_location)\n        manifest = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)\n        with Cache(repository, manifest) as cache:\n            assert cache.cache_config.mandatory_features == set()\n\n\ndef test_check_cache(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    cmd(archiver, \"create\", \"test\", \"input\")\n    with open_repository(archiver) as repository:\n        manifest = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)\n        with Cache(repository, manifest, sync=False) as cache:\n            cache.begin_txn()\n            cache.chunks.incref(list(cache.chunks.iteritems())[0][0])\n            cache.commit()\n    with pytest.raises(AssertionError):\n        check_cache(archiver)\n\n\n#  Begin manifest tests\ndef spoof_manifest(repository):\n    with repository:\n        manifest = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)\n        cdata = manifest.repo_objs.format(\n            Manifest.MANIFEST_ID,\n            {},\n            msgpack.packb(\n                {\n                    \"version\": 1,\n                    \"archives\": {},\n                    \"config\": {},\n                    \"timestamp\": (datetime.now(tz=timezone.utc) + timedelta(days=1)).isoformat(timespec=\"microseconds\"),\n                }\n            ),\n        )\n        repository.put(Manifest.MANIFEST_ID, cdata)\n        repository.commit(compact=False)\n\n\ndef test_fresh_init_tam_required(archiver):\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    repository = Repository(archiver.repository_path, exclusive=True)\n    with repository:\n        manifest = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)\n        cdata = manifest.repo_objs.format(\n            Manifest.MANIFEST_ID,\n            {},\n            msgpack.packb(\n                {\n                    \"version\": 1,\n                    \"archives\": {},\n                    \"timestamp\": (datetime.now(tz=timezone.utc) + timedelta(days=1)).isoformat(timespec=\"microseconds\"),\n                }\n            ),\n        )\n        repository.put(Manifest.MANIFEST_ID, cdata)\n        repository.commit(compact=False)\n\n    with pytest.raises(TAMRequiredError):\n        cmd(archiver, \"rlist\")\n\n\ndef test_not_required(archiver):\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    create_src_archive(archiver, \"archive1234\")\n    repository = Repository(archiver.repository_path, exclusive=True)\n    # Manifest must be authenticated now\n    output = cmd(archiver, \"rlist\", \"--debug\")\n    assert \"archive1234\" in output\n    assert \"TAM-verified manifest\" in output\n    # Try to spoof / modify pre-1.0.9\n    spoof_manifest(repository)\n    # Fails\n    with pytest.raises(TAMRequiredError):\n        cmd(archiver, \"rlist\")\n\n\n# Begin Remote Tests\ndef test_remote_repo_restrict_to_path(remote_archiver):\n    original_location, repo_path = remote_archiver.repository_location, remote_archiver.repository_path\n    # restricted to repo directory itself:\n    with patch.object(RemoteRepository, \"extra_test_args\", [\"--restrict-to-path\", repo_path]):\n        cmd(remote_archiver, \"rcreate\", RK_ENCRYPTION)\n    # restricted to repo directory itself, fail for other directories with same prefix:\n    with patch.object(RemoteRepository, \"extra_test_args\", [\"--restrict-to-path\", repo_path]):\n        with pytest.raises(PathNotAllowed):\n            remote_archiver.repository_location = original_location + \"_0\"\n            cmd(remote_archiver, \"rcreate\", RK_ENCRYPTION)\n    # restricted to a completely different path:\n    with patch.object(RemoteRepository, \"extra_test_args\", [\"--restrict-to-path\", \"/foo\"]):\n        with pytest.raises(PathNotAllowed):\n            remote_archiver.repository_location = original_location + \"_1\"\n            cmd(remote_archiver, \"rcreate\", RK_ENCRYPTION)\n    path_prefix = os.path.dirname(repo_path)\n    # restrict to repo directory's parent directory:\n    with patch.object(RemoteRepository, \"extra_test_args\", [\"--restrict-to-path\", path_prefix]):\n        remote_archiver.repository_location = original_location + \"_2\"\n        cmd(remote_archiver, \"rcreate\", RK_ENCRYPTION)\n    # restrict to repo directory's parent directory and another directory:\n    with patch.object(\n        RemoteRepository, \"extra_test_args\", [\"--restrict-to-path\", \"/foo\", \"--restrict-to-path\", path_prefix]\n    ):\n        remote_archiver.repository_location = original_location + \"_3\"\n        cmd(remote_archiver, \"rcreate\", RK_ENCRYPTION)\n\n\ndef test_remote_repo_restrict_to_repository(remote_archiver):\n    repo_path = remote_archiver.repository_path\n    # restricted to repo directory itself:\n    with patch.object(RemoteRepository, \"extra_test_args\", [\"--restrict-to-repository\", repo_path]):\n        cmd(remote_archiver, \"rcreate\", RK_ENCRYPTION)\n    parent_path = os.path.join(repo_path, \"..\")\n    with patch.object(RemoteRepository, \"extra_test_args\", [\"--restrict-to-repository\", parent_path]):\n        with pytest.raises(PathNotAllowed):\n            cmd(remote_archiver, \"rcreate\", RK_ENCRYPTION)\n\n\ndef test_remote_repo_strip_components_doesnt_leak(remote_archiver):\n    cmd(remote_archiver, \"rcreate\", RK_ENCRYPTION)\n    create_regular_file(remote_archiver.input_path, \"dir/file\", contents=b\"test file contents 1\")\n    create_regular_file(remote_archiver.input_path, \"dir/file2\", contents=b\"test file contents 2\")\n    create_regular_file(remote_archiver.input_path, \"skipped-file1\", contents=b\"test file contents 3\")\n    create_regular_file(remote_archiver.input_path, \"skipped-file2\", contents=b\"test file contents 4\")\n    create_regular_file(remote_archiver.input_path, \"skipped-file3\", contents=b\"test file contents 5\")\n    cmd(remote_archiver, \"create\", \"test\", \"input\")\n    marker = \"cached responses left in RemoteRepository\"\n    with changedir(\"output\"):\n        res = cmd(remote_archiver, \"extract\", \"test\", \"--debug\", \"--strip-components\", \"3\")\n        assert marker not in res\n        with assert_creates_file(\"file\"):\n            res = cmd(remote_archiver, \"extract\", \"test\", \"--debug\", \"--strip-components\", \"2\")\n            assert marker not in res\n        with assert_creates_file(\"dir/file\"):\n            res = cmd(remote_archiver, \"extract\", \"test\", \"--debug\", \"--strip-components\", \"1\")\n            assert marker not in res\n        with assert_creates_file(\"input/dir/file\"):\n            res = cmd(remote_archiver, \"extract\", \"test\", \"--debug\", \"--strip-components\", \"0\")\n            assert marker not in res\n", "import tempfile\nfrom binascii import hexlify, unhexlify, a2b_base64\nfrom unittest.mock import MagicMock\n\nimport pytest\n\nfrom ..crypto.key import bin_to_hex\nfrom ..crypto.key import PlaintextKey, AuthenticatedKey, Blake2AuthenticatedKey\nfrom ..crypto.key import RepoKey, KeyfileKey, Blake2RepoKey, Blake2KeyfileKey\nfrom ..crypto.key import AEADKeyBase\nfrom ..crypto.key import AESOCBRepoKey, AESOCBKeyfileKey, CHPORepoKey, CHPOKeyfileKey\nfrom ..crypto.key import Blake2AESOCBRepoKey, Blake2AESOCBKeyfileKey, Blake2CHPORepoKey, Blake2CHPOKeyfileKey\nfrom ..crypto.key import ID_HMAC_SHA_256, ID_BLAKE2b_256\nfrom ..crypto.key import (\n    TAMRequiredError,\n    TAMInvalid,\n    TAMUnsupportedSuiteError,\n    UnsupportedManifestError,\n    UnsupportedKeyFormatError,\n)\nfrom ..crypto.key import identify_key\nfrom ..crypto.low_level import IntegrityError as IntegrityErrorBase\nfrom ..helpers import IntegrityError\nfrom ..helpers import Location\nfrom ..helpers import StableDict\nfrom ..helpers import msgpack\nfrom ..constants import KEY_ALGORITHMS\n\n\nclass TestKey:\n    class MockArgs:\n        location = Location(tempfile.mkstemp()[1])\n        key_algorithm = \"argon2\"\n\n    keyfile2_key_file = \"\"\"\n        BORG_KEY 0000000000000000000000000000000000000000000000000000000000000000\n        hqlhbGdvcml0aG2mc2hhMjU2pGRhdGHaAN4u2SiN7hqISe3OA8raBWNuvHn1R50ZU7HVCn\n        11vTJNEaj9soxUaIGcW+pAB2N5yYoKMg/sGCMuZa286iJ008DvN99rf/ORfcKrK2GmzslO\n        N3uv9Tk9HtqV/Sq5zgM9xuY9rEeQGDQVQ+AOsFamJqSUrAemGJbJqw9IerXC/jN4XPnX6J\n        pi1cXCFxHfDaEhmWrkdPNoZdirCv/eP/dOVOLmwU58YsS+MvkZNfEa16el/fSb/ENdrwJ/\n        2aYMQrDdk1d5MYzkjotv/KpofNwPXZchu2EwH7OIHWQjEVL1DZWkaGFzaNoAIO/7qn1hr3\n        F84MsMMiqpbz4KVICeBZhfAaTPs4W7BC63qml0ZXJhdGlvbnPOAAGGoKRzYWx02gAgLENQ\n        2uVCoR7EnAoiRzn8J+orbojKtJlNCnQ31SSC8rendmVyc2lvbgE=\"\"\".strip()\n\n    keyfile2_cdata = bytes.fromhex(\n        \"003be7d57280d1a42add9f3f36ea363bbc5e9349ad01ddec0634a54dd02959e70500000000000003ec063d2cbcacba6b\"\n    )\n    keyfile2_id = unhexlify(\"c3fbf14bc001ebcc3cd86e696c13482ed071740927cd7cbe1b01b4bfcee49314\")\n\n    keyfile_blake2_key_file = \"\"\"\n        BORG_KEY 0000000000000000000000000000000000000000000000000000000000000000\n        hqlhbGdvcml0aG2mc2hhMjU2pGRhdGHaAZ7VCsTjbLhC1ipXOyhcGn7YnROEhP24UQvOCi\n        Oar1G+JpwgO9BIYaiCODUpzPuDQEm6WxyTwEneJ3wsuyeqyh7ru2xo9FAUKRf6jcqqZnan\n        ycTfktkUC+CPhKR7W6MTu5fPvy99chyL09/RGdD15aswR5PjNoFu4626sfMrBReyPdlxqt\n        F80m+fbNE/vln2Trqoz9EMHQ3IxjIK4q0m4Aj7TwCu7ZankFtwt898+tYsWE7lb2Ps/gXB\n        F8PM/5wHpYps2AKhDCpwKp5HyqIqlF5IzR2ydL9QP20QBjp/rSi6b+xwrfxNJZfw78f8ef\n        A2Yj7xIsxNQ0kmVmTL/UF6d7+Mw1JfurWrySiDU7QQ+RiZpWUZ0DdReB+e4zn6/KNKC884\n        34SGywADuLIQe2FKU+5jBCbutEyEGILQbAR/cgeLy5+V2XwXMJh4ytwXVIeT6Lk+qhYAdz\n        Klx4ub7XijKcOxJyBE+4k33DAhcfIT2r4/sxgMhXrIOEQPKsMAixzdcqVYkpou+6c4PZeL\n        nr+UjfJwOqK1BlWk1NgwE4GXYIKkaGFzaNoAIAzjUtpBPPh6kItZtHQZvnQG6FpucZNfBC\n        UTHFJg343jqml0ZXJhdGlvbnPOAAGGoKRzYWx02gAgz3YaUZZ/s+UWywj97EY5b4KhtJYi\n        qkPqtDDxs2j/T7+ndmVyc2lvbgE=\"\"\".strip()\n\n    keyfile_blake2_cdata = bytes.fromhex(\n        \"04d6040f5ef80e0a8ac92badcbe3dee83b7a6b53d5c9a58c4eed14964cb10ef591040404040404040d1e65cc1f435027\"\n    )\n    # Verified against b2sum. Entire string passed to BLAKE2, including the padded 64 byte key contained in\n    # keyfile_blake2_key_file above is\n    # 19280471de95185ec27ecb6fc9edbb4f4db26974c315ede1cd505fab4250ce7cd0d081ea66946c\n    # 95f0db934d5f616921efbd869257e8ded2bd9bd93d7f07b1a30000000000000000000000000000\n    # 000000000000000000000000000000000000000000000000000000000000000000000000000000\n    # 00000000000000000000007061796c6f6164\n    #                       p a y l o a d\n    keyfile_blake2_id = bytes.fromhex(\"d8bc68e961c79f99be39061589e5179b2113cd9226e07b08ddd4a1fef7ce93fb\")\n\n    @pytest.fixture\n    def keys_dir(self, request, monkeypatch, tmpdir):\n        monkeypatch.setenv(\"BORG_KEYS_DIR\", str(tmpdir))\n        return tmpdir\n\n    @pytest.fixture(\n        params=(\n            # not encrypted\n            PlaintextKey,\n            AuthenticatedKey,\n            Blake2AuthenticatedKey,\n            # legacy crypto\n            KeyfileKey,\n            Blake2KeyfileKey,\n            RepoKey,\n            Blake2RepoKey,\n            # new crypto\n            AESOCBKeyfileKey,\n            AESOCBRepoKey,\n            Blake2AESOCBKeyfileKey,\n            Blake2AESOCBRepoKey,\n            CHPOKeyfileKey,\n            CHPORepoKey,\n            Blake2CHPOKeyfileKey,\n            Blake2CHPORepoKey,\n        )\n    )\n    def key(self, request, monkeypatch):\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"test\")\n        return request.param.create(self.MockRepository(), self.MockArgs())\n\n    class MockRepository:\n        class _Location:\n            raw = processed = \"/some/place\"\n\n            def canonical_path(self):\n                return self.processed\n\n        _location = _Location()\n        id = bytes(32)\n        id_str = bin_to_hex(id)\n        version = 2\n\n        def save_key(self, data):\n            self.key_data = data\n\n        def load_key(self):\n            return self.key_data\n\n    def test_plaintext(self):\n        key = PlaintextKey.create(None, None)\n        chunk = b\"foo\"\n        id = key.id_hash(chunk)\n        assert hexlify(id) == b\"2c26b46b68ffc68ff99b453c1d30413413422d706483bfa0f98a5e886266e7ae\"\n        assert chunk == key.decrypt(id, key.encrypt(id, chunk))\n\n    def test_keyfile(self, monkeypatch, keys_dir):\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"test\")\n        key = KeyfileKey.create(self.MockRepository(), self.MockArgs())\n        assert key.cipher.next_iv() == 0\n        chunk = b\"ABC\"\n        id = key.id_hash(chunk)\n        manifest = key.encrypt(id, chunk)\n        assert key.cipher.extract_iv(manifest) == 0\n        manifest2 = key.encrypt(id, chunk)\n        assert manifest != manifest2\n        assert key.decrypt(id, manifest) == key.decrypt(id, manifest2)\n        assert key.cipher.extract_iv(manifest2) == 1\n        iv = key.cipher.extract_iv(manifest)\n        key2 = KeyfileKey.detect(self.MockRepository(), manifest)\n        assert key2.cipher.next_iv() >= iv + key2.cipher.block_count(len(manifest) - KeyfileKey.PAYLOAD_OVERHEAD)\n        # Key data sanity check\n        assert len({key2.id_key, key2.crypt_key}) == 2\n        assert key2.chunk_seed != 0\n        chunk = b\"foo\"\n        id = key.id_hash(chunk)\n        assert chunk == key2.decrypt(id, key.encrypt(id, chunk))\n\n    def test_keyfile_kfenv(self, tmpdir, monkeypatch):\n        keyfile = tmpdir.join(\"keyfile\")\n        monkeypatch.setenv(\"BORG_KEY_FILE\", str(keyfile))\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"testkf\")\n        assert not keyfile.exists()\n        key = CHPOKeyfileKey.create(self.MockRepository(), self.MockArgs())\n        assert keyfile.exists()\n        chunk = b\"ABC\"\n        chunk_id = key.id_hash(chunk)\n        chunk_cdata = key.encrypt(chunk_id, chunk)\n        key = CHPOKeyfileKey.detect(self.MockRepository(), chunk_cdata)\n        assert chunk == key.decrypt(chunk_id, chunk_cdata)\n        keyfile.remove()\n        with pytest.raises(FileNotFoundError):\n            CHPOKeyfileKey.detect(self.MockRepository(), chunk_cdata)\n\n    def test_keyfile2(self, monkeypatch, keys_dir):\n        with keys_dir.join(\"keyfile\").open(\"w\") as fd:\n            fd.write(self.keyfile2_key_file)\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"passphrase\")\n        key = KeyfileKey.detect(self.MockRepository(), self.keyfile2_cdata)\n        assert key.decrypt(self.keyfile2_id, self.keyfile2_cdata) == b\"payload\"\n\n    def test_keyfile2_kfenv(self, tmpdir, monkeypatch):\n        keyfile = tmpdir.join(\"keyfile\")\n        with keyfile.open(\"w\") as fd:\n            fd.write(self.keyfile2_key_file)\n        monkeypatch.setenv(\"BORG_KEY_FILE\", str(keyfile))\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"passphrase\")\n        key = KeyfileKey.detect(self.MockRepository(), self.keyfile2_cdata)\n        assert key.decrypt(self.keyfile2_id, self.keyfile2_cdata) == b\"payload\"\n\n    def test_keyfile_blake2(self, monkeypatch, keys_dir):\n        with keys_dir.join(\"keyfile\").open(\"w\") as fd:\n            fd.write(self.keyfile_blake2_key_file)\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"passphrase\")\n        key = Blake2KeyfileKey.detect(self.MockRepository(), self.keyfile_blake2_cdata)\n        assert key.decrypt(self.keyfile_blake2_id, self.keyfile_blake2_cdata) == b\"payload\"\n\n    def _corrupt_byte(self, key, data, offset):\n        data = bytearray(data)\n        # note: we corrupt in a way so that even corruption of the unauthenticated encryption type byte\n        # will trigger an IntegrityError (does not happen while we stay within TYPES_ACCEPTABLE).\n        data[offset] ^= 64\n        with pytest.raises(IntegrityErrorBase):\n            key.decrypt(b\"\", data)\n\n    def test_decrypt_integrity(self, monkeypatch, keys_dir):\n        with keys_dir.join(\"keyfile\").open(\"w\") as fd:\n            fd.write(self.keyfile2_key_file)\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"passphrase\")\n        key = KeyfileKey.detect(self.MockRepository(), self.keyfile2_cdata)\n\n        data = self.keyfile2_cdata\n        for i in range(len(data)):\n            self._corrupt_byte(key, data, i)\n\n        with pytest.raises(IntegrityError):\n            data = bytearray(self.keyfile2_cdata)\n            id = bytearray(key.id_hash(data))  # corrupt chunk id\n            id[12] = 0\n            plaintext = key.decrypt(id, data)\n            key.assert_id(id, plaintext)\n\n    def test_roundtrip(self, key):\n        repository = key.repository\n        plaintext = b\"foo\"\n        id = key.id_hash(plaintext)\n        encrypted = key.encrypt(id, plaintext)\n        identified_key_class = identify_key(encrypted)\n        assert identified_key_class == key.__class__\n        loaded_key = identified_key_class.detect(repository, encrypted)\n        decrypted = loaded_key.decrypt(id, encrypted)\n        assert decrypted == plaintext\n\n    def test_assert_id(self, key):\n        plaintext = b\"123456789\"\n        id = key.id_hash(plaintext)\n        key.assert_id(id, plaintext)\n        id_changed = bytearray(id)\n        id_changed[0] ^= 1\n        if not isinstance(key, AEADKeyBase):\n            with pytest.raises(IntegrityError):\n                key.assert_id(id_changed, plaintext)\n            plaintext_changed = plaintext + b\"1\"\n            with pytest.raises(IntegrityError):\n                key.assert_id(id, plaintext_changed)\n\n    def test_authenticated_encrypt(self, monkeypatch):\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"test\")\n        key = AuthenticatedKey.create(self.MockRepository(), self.MockArgs())\n        assert AuthenticatedKey.id_hash is ID_HMAC_SHA_256.id_hash\n        assert len(key.id_key) == 32\n        plaintext = b\"123456789\"\n        id = key.id_hash(plaintext)\n        authenticated = key.encrypt(id, plaintext)\n        # 0x07 is the key TYPE.\n        assert authenticated == b\"\\x07\" + plaintext\n\n    def test_blake2_authenticated_encrypt(self, monkeypatch):\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"test\")\n        key = Blake2AuthenticatedKey.create(self.MockRepository(), self.MockArgs())\n        assert Blake2AuthenticatedKey.id_hash is ID_BLAKE2b_256.id_hash\n        assert len(key.id_key) == 128\n        plaintext = b\"123456789\"\n        id = key.id_hash(plaintext)\n        authenticated = key.encrypt(id, plaintext)\n        # 0x06 is the key TYPE.\n        assert authenticated == b\"\\x06\" + plaintext\n\n\nclass TestTAM:\n    @pytest.fixture\n    def key(self, monkeypatch):\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"test\")\n        return CHPOKeyfileKey.create(TestKey.MockRepository(), TestKey.MockArgs())\n\n    def test_unpack_future(self, key):\n        blob = b\"\\xc1\\xc1\\xc1\\xc1foobar\"\n        with pytest.raises(UnsupportedManifestError):\n            key.unpack_and_verify_manifest(blob)\n\n        blob = b\"\\xc1\\xc1\\xc1\"\n        with pytest.raises(msgpack.UnpackException):\n            key.unpack_and_verify_manifest(blob)\n\n    def test_missing_when_required(self, key):\n        blob = msgpack.packb({})\n        with pytest.raises(TAMRequiredError):\n            key.unpack_and_verify_manifest(blob)\n\n    def test_missing(self, key):\n        blob = msgpack.packb({})\n        key.tam_required = False\n        unpacked, verified = key.unpack_and_verify_manifest(blob)\n        assert unpacked == {}\n        assert not verified\n\n    def test_unknown_type_when_required(self, key):\n        blob = msgpack.packb({\"tam\": {\"type\": \"HMAC_VOLLBIT\"}})\n        with pytest.raises(TAMUnsupportedSuiteError):\n            key.unpack_and_verify_manifest(blob)\n\n    def test_unknown_type(self, key):\n        blob = msgpack.packb({\"tam\": {\"type\": \"HMAC_VOLLBIT\"}})\n        key.tam_required = False\n        unpacked, verified = key.unpack_and_verify_manifest(blob)\n        assert unpacked == {}\n        assert not verified\n\n    @pytest.mark.parametrize(\n        \"tam, exc\",\n        (\n            ({}, TAMUnsupportedSuiteError),\n            ({\"type\": b\"\\xff\"}, TAMUnsupportedSuiteError),\n            (None, TAMInvalid),\n            (1234, TAMInvalid),\n        ),\n    )\n    def test_invalid(self, key, tam, exc):\n        blob = msgpack.packb({\"tam\": tam})\n        with pytest.raises(exc):\n            key.unpack_and_verify_manifest(blob)\n\n    @pytest.mark.parametrize(\n        \"hmac, salt\",\n        (({}, bytes(64)), (bytes(64), {}), (None, bytes(64)), (bytes(64), None)),\n        ids=[\"ed-b64\", \"b64-ed\", \"n-b64\", \"b64-n\"],\n    )\n    def test_wrong_types(self, key, hmac, salt):\n        data = {\"tam\": {\"type\": \"HKDF_HMAC_SHA512\", \"hmac\": hmac, \"salt\": salt}}\n        tam = data[\"tam\"]\n        if hmac is None:\n            del tam[\"hmac\"]\n        if salt is None:\n            del tam[\"salt\"]\n        blob = msgpack.packb(data)\n        with pytest.raises(TAMInvalid):\n            key.unpack_and_verify_manifest(blob)\n\n    def test_round_trip(self, key):\n        data = {\"foo\": \"bar\"}\n        blob = key.pack_and_authenticate_metadata(data)\n        assert blob.startswith(b\"\\x82\")\n\n        unpacked = msgpack.unpackb(blob)\n        assert unpacked[\"tam\"][\"type\"] == \"HKDF_HMAC_SHA512\"\n\n        unpacked, verified = key.unpack_and_verify_manifest(blob)\n        assert verified\n        assert unpacked[\"foo\"] == \"bar\"\n        assert \"tam\" not in unpacked\n\n    @pytest.mark.parametrize(\"which\", (\"hmac\", \"salt\"))\n    def test_tampered(self, key, which):\n        data = {\"foo\": \"bar\"}\n        blob = key.pack_and_authenticate_metadata(data)\n        assert blob.startswith(b\"\\x82\")\n\n        unpacked = msgpack.unpackb(blob, object_hook=StableDict)\n        assert len(unpacked[\"tam\"][which]) == 64\n        unpacked[\"tam\"][which] = unpacked[\"tam\"][which][0:32] + bytes(32)\n        assert len(unpacked[\"tam\"][which]) == 64\n        blob = msgpack.packb(unpacked)\n\n        with pytest.raises(TAMInvalid):\n            key.unpack_and_verify_manifest(blob)\n\n\ndef test_decrypt_key_file_unsupported_algorithm():\n    \"\"\"We will add more algorithms in the future. We should raise a helpful error.\"\"\"\n    key = CHPOKeyfileKey(None)\n    encrypted = msgpack.packb({\"algorithm\": \"THIS ALGORITHM IS NOT SUPPORTED\", \"version\": 1})\n\n    with pytest.raises(UnsupportedKeyFormatError):\n        key.decrypt_key_file(encrypted, \"hello, pass phrase\")\n\n\ndef test_decrypt_key_file_v2_is_unsupported():\n    \"\"\"There may eventually be a version 2 of the format. For now we should raise a helpful error.\"\"\"\n    key = CHPOKeyfileKey(None)\n    encrypted = msgpack.packb({\"version\": 2})\n\n    with pytest.raises(UnsupportedKeyFormatError):\n        key.decrypt_key_file(encrypted, \"hello, pass phrase\")\n\n\ndef test_key_file_roundtrip(monkeypatch):\n    def to_dict(key):\n        extract = \"repository_id\", \"crypt_key\", \"id_key\", \"chunk_seed\"\n        return {a: getattr(key, a) for a in extract}\n\n    repository = MagicMock(id=b\"repository_id\")\n    monkeypatch.setenv(\"BORG_PASSPHRASE\", \"hello, pass phrase\")\n\n    save_me = AESOCBRepoKey.create(repository, args=MagicMock(key_algorithm=\"argon2\"))\n    saved = repository.save_key.call_args.args[0]\n    repository.load_key.return_value = saved\n    load_me = AESOCBRepoKey.detect(repository, manifest_data=None)\n\n    assert to_dict(load_me) == to_dict(save_me)\n    assert msgpack.unpackb(a2b_base64(saved))[\"algorithm\"] == KEY_ALGORITHMS[\"argon2\"]\n"], "fixing_code": [".. _important_notes_1x:\n\nImportant notes 1.x\n===================\n\nThis section provides information about security and corruption issues.\n\n.. _archives_tam_vuln:\n\nPre-1.2.5 archives spoofing vulnerability (CVE-2023-36811)\n----------------------------------------------------------\n\nA flaw in the cryptographic authentication scheme in Borg allowed an attacker to\nfake archives and potentially indirectly cause backup data loss in the repository.\n\nThe attack requires an attacker to be able to\n\n1. insert files (with no additional headers) into backups\n2. gain write access to the repository\n\nThis vulnerability does not disclose plaintext to the attacker, nor does it\naffect the authenticity of existing archives.\n\nCreating plausible fake archives may be feasible for empty or small archives,\nbut is unlikely for large archives.\n\nThe fix enforces checking the TAM authentication tag of archives at critical\nplaces. Borg now considers archives without TAM as garbage or an attack.\n\nWe are not aware of others having discovered, disclosed or exploited this vulnerability.\n\nBelow, if we speak of borg 1.2.5, we mean a borg version >= 1.2.5 **or** a\nborg version that has the relevant security patches for this vulnerability applied\n(could be also an older version in that case).\n\nSteps you must take to upgrade a repository:\n\n1. Upgrade all clients using this repository to borg 1.2.5.\n   Note: it is not required to upgrade a server, except if the server-side borg\n   is also used as a client (and not just for \"borg serve\").\n\n   Do **not** run ``borg check`` with borg 1.2.5 before completing the upgrade steps.\n\n2. Run ``borg info --debug <repository> 2>&1 | grep TAM | grep -i manifest``.\n\n   a) If you get \"TAM-verified manifest\", continue with 3.\n   b) If you get \"Manifest TAM not found and not required\", run\n      ``borg upgrade --tam --force <repository>`` *on every client*.\n\n3. Run ``borg list --format='{name} {time} tam:{tam}{NL}' <repository>``.\n   \"tam:verified\" means that the archive has a valid TAM authentication.\n   \"tam:none\" is expected as output for archives created by borg <1.0.9.\n   \"tam:none\" could also come from archives created by an attacker.\n   You should verify that \"tam:none\" archives are authentic and not malicious\n   (== have good content, have correct timestamp, can be extracted successfully).\n   In case you find crappy/malicious archives, you must delete them before proceeding.\n   In low-risk, trusted environments, you may decide on your own risk to skip step 3\n   and just trust in everything being OK.\n\n4. If there are no tam:non archives left at this point, you can skip this step.\n   Run ``borg upgrade --archives-tam <repository>``.\n   This will make sure all archives are TAM authenticated (an archive TAM will be added\n   for all archives still missing one).\n   ``borg check`` would consider TAM-less archives as garbage or a potential attack.\n   Optionally run the same command as in step 3 to see that all archives now are \"tam:verified\".\n\n\nVulnerability time line:\n\n* 2023-06-13: Vulnerability discovered during code review by Thomas Waldmann\n* 2023-06-13...: Work on fixing the issue, upgrade procedure, docs.\n* 2023-06-30: CVE was assigned via Github CNA\n* 2023-06-30 .. 2023-08-29: Fixed issue, code review, docs, testing.\n* 2023-08-30: Released fixed version 1.2.5\n\n.. _hashindex_set_bug:\n\nPre-1.1.11 potential index corruption / data loss issue\n-------------------------------------------------------\n\nA bug was discovered in our hashtable code, see issue #4829.\nThe code is used for the client-side chunks cache and the server-side repo index.\n\nAlthough borg uses the hashtables very heavily, the index corruption did not\nhappen too frequently, because it needed specific conditions to happen.\n\nData loss required even more specific conditions, so it should be rare (and\nalso detectable via borg check).\n\nYou might be affected if borg crashed with / complained about:\n\n- AssertionError: Corrupted segment reference count - corrupted index or hints\n- ObjectNotFound: Object with key ... not found in repository ...\n- Index mismatch for key b'...'. (..., ...) != (-1, -1)\n- ValueError: stats_against: key contained in self but not in master_index.\n\nAdvised procedure to fix any related issue in your indexes/caches:\n\n- install fixed borg code (on client AND server)\n- for all of your clients and repos remove the cache by:\n\n  borg delete --cache-only YOURREPO\n\n  (later, the cache will be re-built automatically)\n- for all your repos, rebuild the repo index by:\n\n  borg check --repair YOURREPO\n\n  This will also check all archives and detect if there is any data-loss issue.\n\nAffected branches / releases:\n\n- fd06497 introduced the bug into 1.1-maint branch - it affects all borg 1.1.x since 1.1.0b4.\n- fd06497 introduced the bug into master branch - it affects all borg 1.2.0 alpha releases.\n- c5cd882 introduced the bug into 1.0-maint branch - it affects all borg 1.0.x since 1.0.11rc1.\n\nThe bug was fixed by:\n\n- 701159a fixes the bug in 1.1-maint branch - will be released with borg 1.1.11.\n- fa63150 fixes the bug in master branch - will be released with borg 1.2.0a8.\n- 7bb90b6 fixes the bug in 1.0-maint branch. Branch is EOL, no new release is planned as of now.\n\n.. _broken_validator:\n\nPre-1.1.4 potential data corruption issue\n-----------------------------------------\n\nA data corruption bug was discovered in borg check --repair, see issue #3444.\n\nThis is a 1.1.x regression, releases < 1.1 (e.g. 1.0.x) are not affected.\n\nTo avoid data loss, you must not run borg check --repair using an unfixed version\nof borg 1.1.x. The first official release that has the fix is 1.1.4.\n\nPackage maintainers may have applied the fix to updated packages of 1.1.x (x<4)\nthough, see the package maintainer's package changelog to make sure.\n\nIf you never had missing item metadata chunks, the bug has not affected you\neven if you did run borg check --repair with an unfixed version.\n\nWhen borg check --repair tried to repair corrupt archives that miss item metadata\nchunks, the resync to valid metadata in still present item metadata chunks\nmalfunctioned. This was due to a broken validator that considered all (even valid)\nitem metadata as invalid. As they were considered invalid, borg discarded them.\nPractically, that means the affected files, directories or other fs objects were\ndiscarded from the archive.\n\nDue to the malfunction, the process was extremely slow, but if you let it\ncomplete, borg would have created a \"repaired\" archive that has lost a lot of items.\nIf you interrupted borg check --repair because it was so strangely slow (killing\nborg somehow, e.g. Ctrl-C) the transaction was rolled back and no corruption occurred.\n\nThe log message indicating the precondition for the bug triggering looks like:\n\n    item metadata chunk missing [chunk: 001056_bdee87d...a3e50d]\n\nIf you never had that in your borg check --repair runs, you're not affected.\n\nBut if you're unsure or you actually have seen that, better check your archives.\nBy just using \"borg list repo::archive\" you can see if all expected filesystem\nitems are listed.\n\n.. _tam_vuln:\n\nPre-1.0.9 manifest spoofing vulnerability (CVE-2016-10099)\n----------------------------------------------------------\n\nA flaw in the cryptographic authentication scheme in Borg allowed an attacker\nto spoof the manifest. The attack requires an attacker to be able to\n\n1. insert files (with no additional headers) into backups\n2. gain write access to the repository\n\nThis vulnerability does not disclose plaintext to the attacker, nor does it\naffect the authenticity of existing archives.\n\nThe vulnerability allows an attacker to create a spoofed manifest (the list of archives).\nCreating plausible fake archives may be feasible for small archives, but is unlikely\nfor large archives.\n\nThe fix adds a separate authentication tag to the manifest. For compatibility\nwith prior versions this authentication tag is *not* required by default\nfor existing repositories. Repositories created with 1.0.9 and later require it.\n\nSteps you should take:\n\n1. Upgrade all clients to 1.0.9 or later.\n2. Run ``borg upgrade --tam <repository>`` *on every client* for *each* repository.\n3. This will list all archives, including archive IDs, for easy comparison with your logs.\n4. Done.\n\nPrior versions can access and modify repositories with this measure enabled, however,\nto 1.0.9 or later their modifications are indiscernible from an attack and will\nraise an error until the below procedure is followed. We are aware that this can\nbe annoying in some circumstances, but don't see a way to fix the vulnerability\notherwise.\n\nIn case a version prior to 1.0.9 is used to modify a repository where above procedure\nwas completed, and now you get an error message from other clients:\n\n1. ``borg upgrade --tam --force <repository>`` once with *any* client suffices.\n\nThis attack is mitigated by:\n\n- Noting/logging ``borg list``, ``borg info``, or ``borg create --stats``, which\n  contain the archive IDs.\n\nWe are not aware of others having discovered, disclosed or exploited this vulnerability.\n\nVulnerability time line:\n\n* 2016-11-14: Vulnerability and fix discovered during review of cryptography by Marian Beermann (@enkore)\n* 2016-11-20: First patch\n* 2016-12-20: Released fixed version 1.0.9\n* 2017-01-02: CVE was assigned\n* 2017-01-15: Released fixed version 1.1.0b3 (fix was previously only available from source)\n\n.. _attic013_check_corruption:\n\nPre-1.0.9 potential data loss\n-----------------------------\n\nIf you have archives in your repository that were made with attic <= 0.13\n(and later migrated to borg), running borg check would report errors in these\narchives. See issue #1837.\n\nThe reason for this is a invalid (and useless) metadata key that was\nalways added due to a bug in these old attic versions.\n\nIf you run borg check --repair, things escalate quickly: all archive items\nwith invalid metadata will be killed. Due to that attic bug, that means all\nitems in all archives made with these old attic versions.\n\n\nPre-1.0.4 potential repo corruption\n-----------------------------------\n\nSome external errors (like network or disk I/O errors) could lead to\ncorruption of the backup repository due to issue #1138.\n\nA sign that this happened is if \"E\" status was reported for a file that can\nnot be explained by problems with the source file. If you still have logs from\n\"borg create -v --list\", you can check for \"E\" status.\n\nHere is what could cause corruption and what you can do now:\n\n1) I/O errors (e.g. repo disk errors) while writing data to repo.\n\nThis could lead to corrupted segment files.\n\nFix::\n\n    # check for corrupt chunks / segments:\n    borg check -v --repository-only REPO\n\n    # repair the repo:\n    borg check -v --repository-only --repair REPO\n\n    # make sure everything is fixed:\n    borg check -v --repository-only REPO\n\n2) Unreliable network / unreliable connection to the repo.\n\nThis could lead to archive metadata corruption.\n\nFix::\n\n    # check for corrupt archives:\n    borg check -v --archives-only REPO\n\n    # delete the corrupt archives:\n    borg delete --force REPO::CORRUPT_ARCHIVE\n\n    # make sure everything is fixed:\n    borg check -v --archives-only REPO\n\n3) In case you want to do more intensive checking.\n\nThe best check that everything is ok is to run a dry-run extraction::\n\n    borg extract -v --dry-run REPO::ARCHIVE\n\n.. _changelog_1x:\n\nChange Log 1.x\n==============\n\nVersion 1.3.0a1 (2022-04-15)\n----------------------------\n\nPlease note:\n\nThis is an alpha release, only for testing - do not use this with production repos.\n\nNew features:\n\n- init: new --encryption=(repokey|keyfile)-[blake2-](aes-ocb|chacha20-poly1305)\n\n  - New, better, faster crypto (see encryption-aead diagram in the docs), #6463.\n  - New AEAD cipher suites: AES-OCB and CHACHA20-POLY1305.\n  - Session keys are derived via HKDF from random session id and master key.\n  - Nonces/MessageIVs are counters starting from 0 for each session.\n  - AAD: chunk id, key type, messageIV, sessionID are now authenticated also.\n  - Solves the potential AES-CTR mode counter management issues of the legacy crypto.\n- init: --key-algorithm=argon2 (new default KDF, older pbkdf2 also still available)\n\n  borg key change-passphrase / change-location keeps the key algorithm unchanged.\n- key change-algorithm: to upgrade existing keys to argon2 or downgrade to pbkdf2.\n\n  We recommend you to upgrade unless you have to keep the key compatible with older versions of borg.\n- key change-location: usable for repokey <-> keyfile location change\n- benchmark cpu: display benchmarks of cpu bound stuff\n- export-tar: new --tar-format=PAX (default: GNU)\n- import-tar/export-tar: can use PAX format for ctime and atime support\n- import-tar/export-tar: --tar-format=BORG: roundtrip ALL item metadata, #5830\n- repository: create and use version 2 repos only for now\n- repository: implement PUT2: header crc32, overall xxh64, #1704\n\nOther changes:\n\n- require python >= 3.9, #6315\n- simplify libs setup, #6482\n- unbundle most bundled 3rd party code, use libs, #6316\n- use libdeflate.crc32 (Linux and all others) or zlib.crc32 (macOS)\n- repository: code cleanups / simplifications\n- internal crypto api: speedups / cleanups / refactorings / modernisation\n- remove \"borg upgrade\" support for \"attic backup\" repos\n- remove PassphraseKey code and borg key migrate-to-repokey command\n- OpenBSD: build borg with OpenSSL (not: LibreSSL), #6474\n- remove support for LibreSSL, #6474\n- remove support for OpenSSL < 1.1.1\n\n\nVersion 1.2.0 (2022-02-22 22:02:22 :-)\n--------------------------------------\n\nPlease note:\n\nThis is the first borg 1.2 release, so be careful and read the notes below.\n\nUpgrade notes:\n\nStrictly taken, nothing special is required for upgrading to 1.2, but some\nthings can be recommended:\n\n- do you already want to upgrade? 1.1.x also will get fixes for a while.\n- be careful, first upgrade your less critical / smaller repos.\n- first upgrade to a recent 1.1.x release - especially if you run some older\n  1.1.* or even 1.0.* borg release.\n- using that, run at least one `borg create` (your normal backup), `prune`\n  and especially a `check` to see everything is in a good state.\n- check the output of `borg check` - if there is anything special, consider\n  a `borg check --repair` followed by another `borg check`.\n- if everything is fine so far (borg check reports no issues), you can consider\n  upgrading to 1.2.0. if not, please first fix any already existing issue.\n- if you want to play safer, first **create a backup of your borg repository**.\n- upgrade to latest borg 1.2.x release (you could use the fat binary from\n  github releases page)\n- run `borg compact --cleanup-commits` to clean up a ton of 17 bytes long files\n  in your repo caused by a borg 1.1 bug\n- run `borg check` again (now with borg 1.2.x) and check if there is anything\n  special.\n- run `borg info` (with borg 1.2.x) to build the local pre12-meta cache (can\n  take significant time, but after that it will be fast) - for more details\n  see below.\n- check the compatibility notes (see below) and adapt your scripts, if needed.\n- if you run into any issues, please check the github issue tracker before\n  posting new issues there or elsewhere.\n\nIf you follow this procedure, you can help avoiding that we get a lot of\n\"borg 1.2\" issue reports that are not really 1.2 issues, but existed before\nand maybe just were not noticed.\n\nCompatibility notes:\n\n- matching of path patterns has been aligned with borg storing relative paths.\n  Borg archives file paths without leading slashes. Previously, include/exclude\n  patterns could contain leading slashes. You should check your patterns and\n  remove leading slashes.\n- dropped support / testing for older Pythons, minimum requirement is 3.8.\n  In case your OS does not provide Python >= 3.8, consider using our binary,\n  which does not need an external Python interpreter. Or continue using\n  borg 1.1.x, which is still supported.\n- freeing repository space only happens when \"borg compact\" is invoked.\n- mount: the default for --numeric-ids is False now (same as borg extract)\n- borg create --noatime is deprecated. Not storing atime is the default behaviour\n  now (use --atime if you want to store the atime).\n- list: corrected mix-up of \"isomtime\" and \"mtime\" formats.\n  Previously, \"isomtime\" was the default but produced a verbose human format,\n  while \"mtime\" produced a ISO-8601-like format.\n  The behaviours have been swapped (so \"mtime\" is human, \"isomtime\" is ISO-like),\n  and the default is now \"mtime\".\n  \"isomtime\" is now a real ISO-8601 format (\"T\" between date and time, not a space).\n- create/recreate --list: file status for all files used to get announced *AFTER*\n  the file (with borg < 1.2). Now, file status is announced *BEFORE* the file\n  contents are processed. If the file status changes later (e.g. due to an error\n  or a content change), the updated/final file status will be printed again.\n- removed deprecated-since-long stuff (deprecated since):\n\n  - command \"borg change-passphrase\" (2017-02), use \"borg key ...\"\n  - option \"--keep-tag-files\" (2017-01), use \"--keep-exclude-tags\"\n  - option \"--list-format\" (2017-10), use \"--format\"\n  - option \"--ignore-inode\" (2017-09), use \"--files-cache\" w/o \"inode\"\n  - option \"--no-files-cache\" (2017-09), use \"--files-cache=disabled\"\n- removed BORG_HOSTNAME_IS_UNIQUE env var.\n  to use borg you must implement one of these 2 scenarios:\n\n  - 1) the combination of FQDN and result of uuid.getnode() must be unique\n       and stable (this should be the case for almost everybody, except when\n       having duplicate FQDN *and* MAC address or all-zero MAC address)\n  - 2) if you are aware that 1) is not the case for you, you must set\n       BORG_HOST_ID env var to something unique.\n- exit with 128 + signal number, #5161.\n  if you have scripts expecting rc == 2 for a signal exit, you need to update\n  them to check for >= 128.\n\nFixes:\n\n- diff: reduce memory consumption, fix is_hardlink_master, #6295\n- compact: fix / improve freeable / freed space log output\n\n  - derive really freed space from quota use before/after, #5679\n  - do not say \"freeable\", but \"maybe freeable\" (based on hint, unsure)\n- fix race conditions in internal SaveFile function, #6306 #6028\n- implement internal safe_unlink (was: truncate_and_unlink) function more safely:\n  usually it does not truncate any more, only under \"disk full\" circumstances\n  and only if there is only one hardlink.\n  see: https://github.com/borgbackup/borg/discussions/6286\n\nOther changes:\n\n- info: use a pre12-meta cache to accelerate stats for borg < 1.2 archives.\n  the first time borg info is invoked on a borg 1.1 repo, it can take a\n  rather long time computing and caching some stats values for 1.1 archives,\n  which borg 1.2 archives have in their archive metadata structure.\n  be patient, esp. if you have lots of old archives.\n  following invocations are much faster due to the cache.\n  related change: add archive name to calc_stats progress display.\n- docs:\n\n  - add borg 1.2 upgrade notes, #6217\n  - link to borg placeholders and borg patterns help\n  - init: explain the encryption modes better\n  - clarify usage of patternfile roots\n  - put import-tar docs into same file as export-tar docs\n  - explain the difference between a path that ends with or without a slash,\n    #6297\n\n\nVersion 1.2.0rc1 (2022-02-05)\n-----------------------------\n\nFixes:\n\n- repo::archive location placeholder expansion fixes, #5826, #5998\n- repository: fix intermediate commits, shall be at end of current segment\n- delete: don't commit if nothing was deleted, avoid cache sync, #6060\n- argument parsing: accept some options only once, #6026\n- disallow overwriting of existing keyfiles on init, #6036\n- if ensure_dir() fails, give more informative error message, #5952\n\nNew features:\n\n- delete --force: do not ask when deleting a repo, #5941\n\nOther changes:\n\n- requirements: exclude broken or incompatible-with-pyinstaller setuptools\n- add a requirements.d/development.lock.txt and use it for vagrant\n- tests:\n\n  - added nonce-related tests\n  - refactor: remove assert_true\n  - vagrant: macos box tuning, netbsd box fixes, #5370, #5922\n- docs:\n\n  - update install docs / requirements docs, #6180\n  - borg mount / FUSE \"versions\" view is not experimental any more\n  - --pattern* is not experimental any more, #6134\n  - impact of deleting path/to/repo/nonce, #5858\n  - key export: add examples, #6204\n  - ~/.config/borg/keys is not used for repokey keys, #6107\n  - excluded parent dir's metadata can't restore\n\n\nVersion 1.2.0b4 (2022-01-23)\n----------------------------\n\nFixes:\n\n- create: fix passing device nodes and symlinks to --paths-from-stdin, #6009\n- create --dry-run: fix display of kept tagfile, #5834\n- check --repair: fix missing parameter in \"did not consistently fail\" msg, #5822\n- fix hardlinkable file type check, #6037\n- list: remove placeholders for shake_* hashes, #6082\n- prune: handle case of calling prune_split when there are no archives, #6015\n- benchmark crud: make sure cleanup of borg-test-data files/dir happens, #5630\n- do not show archive name in repository-related error msgs, #6014\n- prettier error msg (no stacktrace) if exclude file is missing, #5734\n- do not require BORG_CONFIG_DIR if BORG_{SECURITY,KEYS}_DIR are set, #5979\n- fix pyinstaller detection for dir-mode, #5897\n- atomically create the CACHE_TAG file, #6028\n- deal with the SaveFile/SyncFile race, docs, see #6056 708a5853\n- avoid expanding path into LHS of formatting operation + tests, #6064 #6063\n- repository: quota / compactable computation fixes\n- info: emit repo info even if repo has 0 archives + test, #6120\n\nNew features:\n\n- check --repair: significantly speed up search for next valid object in segment, #6022\n- check: add progress indicator for archive check, #5809\n- create: add retry_erofs workaround for O_NOATIME issue on volume shadow copies in WSL1, #6024\n- create: allow --files-cache=size (this is potentially dangerous, use on your own risk), #5686\n- import-tar: implement import-tar to complement export-tar, #2233\n- implement BORG_SELFTEST env variable (can be carefully used to speedup borg hosting), #5871\n- key export: print key if path is '-' or not given, #6092\n- list --format: Add command_line to format keys\n\nOther changes:\n\n- pypi metadata: alpha -> beta\n- require python 3.8+, #5975\n- use pyinstaller 4.7\n- allow msgpack 1.0.3\n- upgrade to bundled xxhash to 0.8.1\n- import-tar / export-tar: tar file related changes:\n\n  - check for short tarfile extensions\n  - add .lz4 and .zstd\n  - fix docs about extensions and decompression commands\n- add github codeql analysis, #6148\n- vagrant:\n\n  - box updates / add new boxes / remove outdated and broken boxes\n  - use Python 3.9.10 (incl. binary builds) and 3.10.0\n  - fix pyenv initialisation, #5798\n  - fix vagrant scp on macOS, #5921\n  - use macfuse instead of osxfuse\n- shell completions:\n\n  - update shell completions to 1.1.17, #5923\n  - remove BORG_LIBC completion, since 9914968 borg no longer uses find_library().\n- docs:\n\n  - fixed readme.rst irc webchat link (we use libera chat now, not freenode)\n  - fix exceptions thrown by `setup.py build_man`\n  - check --repair: recommend checking hw before check --repair, #5855\n  - check --verify-data: clarify and document conflict with --repository-only, #5808\n  - serve: improve ssh forced commands docs, #6083\n  - list: improve docs for `borg list` --format, #6061\n  - list: remove --list-format from borg list\n  - FAQ: fix manifest-timestamp path (inside security dir)\n  - fix the broken link to .nix file\n  - document behavior for filesystems with inconsistent inodes, #5770\n  - clarify user_id vs uid for fuse, #5723\n  - clarify pattern usage with commands, #5176\n  - clarify pp vs. pf pattern type, #5300\n  - update referenced freebsd/macOS versions used for binary build, #5942\n  - pull mode: add some warnings, #5827\n  - clarify \"you will need key and passphrase\" borg init warning, #4622\n  - add missing leading slashes in help patterns, #5857\n  - add info on renaming repositories, #5240\n  - check: add notice about defective hardware, #5753\n  - mention tar --compare (compare archive to fs files), #5880\n  - add note about grandfather-father-son backup retention policy / rotation scheme, #6006\n  - permissions note rewritten to make it less confusing\n  - create github security policy\n  - remove leftovers of BORG_HOSTNAME_IS_UNIQUE\n  - excluded parent dir's metadata can't restore. (#6062)\n  - if parent dir is not extracted, we do not have its metadata\n  - clarify who starts the remote agent\n\n\nVersion 1.2.0b3 (2021-05-12)\n----------------------------\n\nFixes:\n\n- create: fix --progress --log-json, #4360#issuecomment-774580052\n- do not load files cache for commands not using it, #5673\n- fix repeated cache tag file writing bug\n\nNew features:\n\n- create/recreate: print preliminary file status early, #5417\n- create/extract: add --noxattrs and --noacls options, #3955\n- create: verbose files cache logging via --debug-topic=files_cache, #5659\n- mount: implement --numeric-ids (default: False!), #2377\n- diff: add --json-lines option\n- info / create --stats: add --iec option to print sizes in powers of 1024.\n\nOther changes:\n\n- create: add --upload-(ratelimit|buffer), deprecate --remote-* options, #5611\n- create/extract/mount: add --numeric-ids, deprecate --numeric-owner option, #5724\n- config: accept non-int value for max_segment_size / storage_quota\n- use PyInstaller v4.3, #5671\n- vagrant: use Python 3.9.5 to build binaries\n- tox.ini: modernize and enable execution without preinstalling deps\n- cleanup code style checks\n- get rid of distutils, use setuptools+packaging\n- github CI: test on Python 3.10-dev\n- check: missing / healed chunks: always tell chunk ID, #5704\n- docs:\n\n  - remove bad /var/cache exclusion in example commands, #5625\n  - misc. fixes and improvements, esp. for macOS\n  - add unsafe workaround to use an old repo copy, #5722\n\n\nVersion 1.2.0b2 (2021-02-06)\n----------------------------\n\nFixes:\n\n- create: do not recurse into duplicate roots, #5603\n- create: only print stats if not ctrl-c'ed, fixes traceback, #5668\n- extract:\n  improve exception handling when setting xattrs, #5092.\n  emit a warning message giving the path, xattr key and error message.\n  continue trying to restore other xattrs and bsdflags of the same file\n  after an exception with xattr-setting happened.\n- export-tar:\n  fix memory leak with ssh: remote repository, #5568.\n  fix potential memory leak with ssh: remote repository with partial extraction.\n- remove empty shadowed_segments lists, #5275\n- fix bad default: manifest.archives.list(consider_checkpoints=False),\n  fixes tracebacks / KeyErros for missing objects in ChunkIndex, #5668\n\nNew features:\n\n- create: improve sparse file support\n\n  - create --sparse (detect sparse file holes) and file map support,\n    only for the \"fixed\" chunker, #14\n  - detect all-zero chunks in read data in \"buzhash\" and \"fixed\" chunkers\n  - cached_hash: use a small LRU cache to accelerate all-zero chunks hashing\n  - use cached_hash also to generate all-zero replacement chunks\n- create --remote-buffer, add a upload buffer for remote repos, #5574\n- prune: keep oldest archive when retention target not met\n\nOther changes:\n\n- use blake2 from python 3.6+ hashlib\n  (this removes the requirement for libb2 and the bundled blake2 code)\n- also accept msgpack up to 1.0.2.\n  exclude 1.0.1 though, which had some issues (not sure they affect borg).\n- create: add repository location to --stats output, #5491\n- check: debug log the segment filename\n- delete: add a --list switch to borg delete, #5116\n- borg debug dump-hints - implemented to e.g. to look at shadow_index\n- Tab completion support for additional archives for 'borg delete'\n- refactor: have one borg.constants.zero all-zero bytes object\n- refactor shadow_index updating repo.put/delete, #5661, #5636.\n- docs:\n\n  - add another case of attempted hardlink usage\n  - fix description of borg upgrade hardlink usage, #5518\n  - use HTTPS everywhere\n  - add examples for --paths-from-stdin, --paths-from-command, --paths-separator, #5644\n  - fix typos/grammar\n  - update docs for dev environment installation instructions\n  - recommend running tests only on installed versions for setup\n  - add badge with current status of package\n- vagrant:\n\n  - use brew install --cask ..., #5557\n  - use Python 3.9.1 and PyInstaller 4.1 to build the borg binary\n\n\nVersion 1.2.0b1 (2020-12-06)\n----------------------------\n\nFixes:\n\n- BORG_CACHE_DIR crashing borg if empty, atomic handling of\n  recursive directory creation, #5216\n- fix --dry-run and --stats coexistence, #5415\n- allow EIO with warning when trying to hardlink, #4336\n- export-tar: set tar format to GNU_FORMAT explicitly, #5274\n- use --timestamp for {utcnow} and {now} if given, #5189\n- make timestamp helper timezone-aware\n\nNew features:\n\n- create: implement --paths-from-stdin and --paths-from-command, see #5492.\n  These switches read paths to archive from stdin. Delimiter can specified\n  by --paths-delimiter=DELIM. Paths read will be added honoring every\n  option but exclusion options and --one-file-system. borg won't recurse\n  into directories.\n- 'obfuscate' pseudo compressor obfuscates compressed chunk size in repo\n- add pyfuse3 (successor of llfuse) as an alternative lowlevel fuse\n  implementation to llfuse (deprecated), #5407.\n  FUSE implementation can be switched via env var BORG_FUSE_IMPL.\n- allow appending to the files cache filename with BORG_FILES_CACHE_SUFFIX\n- create: implement --stdin-mode, --stdin-user and --stdin-group, #5333\n\nOther changes:\n\n- split recursive directory walking/processing into directory walking and\n  item processing.\n- fix warning by importing setuptools before distutils.\n- debug info: include infos about FUSE implementation, #5546\n- testing:\n\n  - add a test for the hashindex corruption bug, #5531 #4829\n  - move away from travis-ci, use github actions, #5528 #5467\n  - test both on fuse2 and fuse3\n  - upload coverage reports to codecov\n  - fix spurious failure in test_cache_files, #5438\n  - add tests for Location.with_timestamp\n  - tox: add a non-fuse env to the envlist\n- vagrant:\n\n  - use python 3.7.latest and pyinstaller 4.0 for binary creation\n  - pyinstaller: compute basepath from spec file location\n  - vagrant: updates/fixes for archlinux box, #5543\n- docs:\n\n  - \"filename with spaces\" example added to exclude file, #5236\n  - add a hint about sleeping computer, #5301\n  - how to adjust macOS >= Catalina security settings, #5303\n  - process/policy for adding new compression algorithms\n  - updated docs about hacked backup client, #5480\n  - improve ansible deployment docs, make it more generic\n  - how to approach borg speed issues, give speed example, #5371\n  - fix mathematical inaccuracy about chunk size, #5336\n  - add example for excluding content using --pattern cli option\n  - clarify borg create's '--one-file-system' option, #4009\n  - improve docs/FAQ about append-only remote repos, #5497\n  - fix reST markup issues, labels\n  - add infos about contributor retirement status\n\n\nVersion 1.2.0a9 (2020-10-05)\n----------------------------\n\nFixes:\n\n- fix memory leak related to preloading, #5202\n- check --repair: fix potential data loss, #5325\n- persist shadow_index in between borg runs, #4830\n- fix hardlinked CACHEDIR.TAG processing, #4911\n- --read-special: .part files also should be regular files, #5217\n- allow server side enforcing of umask, --umask is for the local borg\n  process only (see docs), #4947\n- exit with 128 + signal number, #5161\n- borg config --list does not show last_segment_checked, #5159\n- locking:\n\n  - fix ExclusiveLock race condition bug, #4923\n  - fix race condition in lock migration, #4953\n  - fix locking on openindiana, #5271\n\nNew features:\n\n- --content-from-command: create archive using stdout of given command, #5174\n- allow key-import + BORG_KEY_FILE to create key files\n- build directory-based binary for macOS to avoid Gatekeeper delays\n\nOther changes:\n\n- upgrade bundled zstd to 1.4.5\n- upgrade bundled xxhash to 0.8.0, #5362\n- if self test fails, also point to OS and hardware, #5334\n- misc. shell completions fixes/updates, rewrite zsh completion\n- prettier error message when archive gets too big, #5307\n- stop relying on `false` exiting with status code 1\n- rephrase some warnings, #5164\n- parseformat: unnecessary calls removed, #5169\n- testing:\n\n  - enable Python3.9 env for test suite and VMs, #5373\n  - drop python 3.5, #5344\n  - misc. vagrant fixes/updates\n  - misc. testing fixes, #5196\n- docs:\n\n  - add ssh-agent pull backup method to doc, #5288\n  - mention double --force in prune docs\n  - update Homebrew install instructions, #5185\n  - better description of how cache and rebuilds of it work\n    and how the workaround applies to that\n  - point to borg create --list item flags in recreate usage, #5165\n  - add a note to create from stdin regarding files cache, #5180\n  - add security faq explaining AES-CTR crypto issues, #5254\n  - clarify --exclude-if-present in recreate, #5193\n  - add socat pull mode, #5150, #900\n  - move content of resources doc page to community project, #2088\n  - explain hash collision, #4884\n  - clarify --recompress option, #5154\n\n\nVersion 1.2.0a8 (2020-04-22)\n----------------------------\n\nFixes:\n\n- fixed potential index corruption / data loss issue due to bug in hashindex_set, #4829.\n  Please read and follow the more detailed notes close to the top of this document.\n- fix crash when upgrading erroneous hints file, #4922\n- commit-time free space calc: ignore bad compact map entries, #4796\n- info: if the archive doesn't exist, print a pretty message, #4793\n- --prefix / -P: fix processing, avoid argparse issue, #4769\n- ignore EACCES (errno 13) when hardlinking, #4730\n- add a try catch when formatting the info string, #4818\n- check: do not stumble over invalid item key, #4845\n- update prevalence of env vars to set config and cache paths\n- mount: fix FUSE low linear read speed on large files, #5032\n- extract: fix confusing output of borg extract --list --strip-components, #4934\n- recreate: support --timestamp option, #4745\n- fix ProgressIndicator msgids (JSON output), #4935\n- fuse: set f_namemax in statfs result, #2684\n- accept absolute paths on windows\n- pyinstaller: work around issue with setuptools > 44\n\nNew features:\n\n- chunker speedup (plus regression test)\n- added --consider-checkpoints and related test, #4788\n- added --noflags option, deprecate --nobsdflags option, #4489\n- compact: add --threshold option, #4674\n- mount: add birthtime to FUSE entries\n- support platforms with no os.link, #4901 - if we don't have os.link,\n  we just extract another copy instead of making a hardlink.\n- move sync_file_range to its own extension for better platform compatibility.\n- new --bypass-lock option to bypass locking, e.g. for read-only repos\n- accept absolute paths by removing leading slashes in patterns of all\n  sorts but re: style, #4029\n- delete: new --keep-security-info option\n\nOther changes:\n\n- support msgpack 0.6.2 and 1.0.0, #5065\n- upgrade bundled zstd to 1.4.4\n- upgrade bundled lz4 to 1.9.2\n- upgrade xxhash to 0.7.3\n- require recent enough llfuse for birthtime support, #5064\n- only store compressed data if the result actually is smaller, #4516\n- check: improve error output for matching index size, see #4829\n- ignore --stats when given with --dry-run, but continue, #4373\n- replaced usage of os.statvfs with shutil.disk_usage (better cross-platform support).\n- fuse: remove unneeded version check and compat code, micro opts\n- docs:\n\n  - improve description of path variables\n  - document how to delete data completely, #2929\n  - add FAQ about Borg config dir, #4941\n  - add docs about errors not printed as JSON, #4073\n  - update usage_general.rst.inc\n  - added \"Will move with BORG_CONFIG_DIR variable unless specified.\" to BORG_SECURITY_DIR info.\n  - put BORG_SECURITY_DIR immediately below BORG_CONFIG_DIR (and moved BORG_CACHE_DIR up before them).\n  - add paragraph regarding cache security assumptions, #4900\n  - tell about borg cache security precautions\n  - add FAQ describing difference between a local repo vs. repo on a server.\n  - document how to test exclusion patterns without performing an actual backup\n  - create: tell that \"Calculating size\" time and space needs are caused by --progress\n  - fix/improve documentation for @api decorator, #4674\n  - add a pull backup / push restore how-to, #1552\n  - fix man pages creation, #4752\n  - more general FAQ for backup and retain original paths, #4532\n  - explain difference between --exclude and --pattern, #4118\n  - add FAQ for preventing SSH timeout in extract, #3866\n  - improve password FAQ (decrease pw length, add -w 0 option to base64 to prevent line wrap), #4591\n  - add note about patterns and stored paths, #4160\n  - add upgrade of tools to pip installation how-to, #5090\n  - document one cause of orphaned chunks in check command, #2295\n  - clean up the whole check usage paragraph\n  - FAQ: linked recommended restrictions to ssh public keys on borg servers, #4946\n  - fixed \"doc downplays severity of Nonce reuse issue\", #4883\n  - borg repo restore instructions needed, #3428\n  - new FAQ: A repo is corrupt and must be replaced with an older repo.\n  - clarify borg init's encryption modes\n- native windows port:\n\n  - update README_WINDOWS.rst\n  - updated pyinstaller spec file to support windows builds\n- testing / CI:\n\n  - improved travis config / install script, improved macOS builds\n  - allow osx builds to fail, #4955\n  - Windows 10 build on Appveyor CI\n- vagrant:\n\n  - upgrade pyinstaller to v3.5 + patch\n  - use py369 for binary build, add py380 for tests\n  - fix issue in stretch VM hanging at grub installation\n  - add a debian buster and a ubuntu focal VM\n  - update darwin box to 10.12\n  - upgrade FreeBSD box to 12.1\n  - fix debianoid virtualenv packages\n  - use pyenv in freebsd64 VM\n  - remove the flake8 test\n  - darwin: avoid error if pkg is already installed\n  - debianoid: don't interactively ask questions\n\n\nVersion 1.2.0a7 (2019-09-07)\n----------------------------\n\nFixes:\n\n- slave hardlinks extraction issue, see #4350\n- extract: fix KeyError for \"partial\" extraction, #4607\n- preload chunks for hardlink slaves w/o preloaded master, #4350\n- fix preloading for old remote servers, #4652\n- fix partial extract for hardlinked contentless file types, #4725\n- Repository.open: use stat() to check for repo dir, #4695\n- Repository.check_can_create_repository: use stat() to check, ~ #4695.\n- SecurityManager.known(): check all files, #4614\n- after double-force delete, warn about necessary repair, #4704\n- cope with ANY error when importing pytest into borg.testsuite, #4652\n- fix invalid archive error message\n- setup.py: fix detection of missing Cython\n- filter out selinux xattrs, #4574\n- location arg - should it be optional? #4541\n- enable placeholder usage in --comment, #4559\n- use whitelist approach for borg serve, #4097\n\nNew features:\n\n- minimal native Windows support, see windows readme (work in progress)\n- create: first ctrl-c (SIGINT) triggers checkpoint and abort, #4606\n- new BORG_WORKAROUNDS mechanism, basesyncfile, #4710\n- remove WSL autodetection. if WSL still has this problem, you need to\n  set BORG_WORKAROUNDS=basesyncfile in the borg process environment to\n  work around it.\n- support xxh64 checksum in addition to the hashlib hashes in borg list\n- enable placeholder usage in all extra archive arguments\n- enable placeholder usage in --comment, #4559\n- enable placeholder usage in --glob-archives, #4495\n- ability to use a system-provided version of \"xxhash\"\n- create:\n\n  - changed the default behaviour not to store the atime of fs items. atime is\n    often rather not interesting and fragile - it easily changes even if nothing\n    else has changed and, if stored into the archive, spoils deduplication of\n    the archive metadata stream.\n  - if you give the --noatime option, borg will output a deprecation warning\n    because it is currently ignored / does nothing.\n    Please remove the --noatime option when using borg 1.2.\n  - added a --atime option for storing files' atime into an archive\n\nOther changes:\n\n- argparser: always use REPOSITORY in metavar\n- do not check python/libc for borg serve, #4483\n- small borg compact improvements, #4522\n- compact: log freed space at INFO level\n- tests:\n\n  - tox / travis: add testing on py38-dev\n  - fix broken test that relied on improper zlib assumptions\n  - pure-py msgpack warning shall not make a lot of tests fail, #4558\n  - rename test_mount_hardlinks to test_fuse_mount_hardlinks (master)\n  - vagrant: add up-to-date openindiana box (py35, openssl10)\n  - get rid of confusing coverage warning, #2069\n- docs:\n\n  - reiterate that 'file cache names are absolute' in FAQ,\n    mention bind mount solution, #4738\n  - add restore docs, #4670\n  - updated docs to cover use of temp directory on remote, #4545\n  - add a push-style example to borg-create(1), #4613\n  - timestamps in the files cache are now usually ctime, #4583\n  - benchmark crud: clarify that space is used until compact\n  - update documentation of borg create,\n    corrects a mention of borg 1.1 as a future version.\n  - fix osxfuse github link in installation docs\n  - how to supply a passphrase, use crypto devices, #4549\n  - extract: document limitation \"needs empty destination\",  #4598\n  - update macOS Brew link\n  - add note about software for automating backup\n  - compact: improve docs,\n  - README: new URL for funding options\n\n\nVersion 1.2.0a6 (2019-04-22)\n----------------------------\n\nFixes:\n\n- delete / prune: consider part files correctly for stats, #4507\n- fix \"all archives\" stats considering part files, #4329\n- create: only run stat_simple_attrs() once\n- create: --stats does not work with --dry-run, exit with error msg, #4373\n- give \"invalid repo\" error msg if repo config not found, #4411\n\nNew features:\n\n- display msgpack version as part of sysinfo (e.g. in tracebacks)\n\nOther changes:\n\n- docs:\n\n  - sdd \"SSH Configuration\" section, #4493, #3988, #636, #4485\n  - better document borg check --max-duration, #4473\n  - sorted commands help in multiple steps, #4471\n- testing:\n\n  - travis: use py 3.5.3 and 3.6.7 on macOS to get a pyenv-based python\n    build with openssl 1.1\n  - vagrant: use py 3.5.3 and 3.6.8 on darwin64 VM to build python and\n    borg with openssl 1.1\n  - pytest: -v and default XDISTN to 1, #4481\n\n\nVersion 1.2.0a5 (2019-03-21)\n----------------------------\n\nFixes:\n\n- warn if a file has changed while being backed up, #1750\n- lrucache: regularly remove old FDs, #4427\n- borg command shall terminate with rc 2 for ImportErrors, #4424\n- make freebsd xattr platform code api compatible with linux, #3952\n\nOther changes:\n\n- major setup code refactoring (especially how libraries like openssl, liblz4,\n  libzstd, libb2 are discovered and how it falls back to code bundled with\n  borg), new: uses pkg-config now (and needs python \"pkgconfig\" package\n  installed), #1925\n\n  if you are a borg package maintainer, please try packaging this\n  (see comments in setup.py).\n- Vagrantfile: add zstd, reorder, build env vars, #4444\n- travis: install script improvements\n- update shell completions\n- docs:\n\n  - add a sample logging.conf in docs/misc, #4380\n  - fix spelling errors\n  - update requirements / install docs, #4374\n\n\nVersion 1.2.0a4 (2019-03-11)\n----------------------------\n\nFixes:\n\n- do not use O_NONBLOCK for special files, like FIFOs, block and char devices\n  when using --read-special. fixes backing up FIFOs. fixes to test. #4394\n- more LibreSSL build fixes: LibreSSL has HMAC_CTX_free and HMAC_CTX_new\n\nNew features:\n\n- check: incremental repo check (only checks crc32 for segment entries), #1657\n  borg check --repository-only --max-duration SECONDS ...\n- delete: timestamp for borg delete --info added, #4359\n\nOther changes:\n\n- redo stale lock handling, #3986\n  drop BORG_HOSTNAME_IS_UNIQUE (please use BORG_HOST_ID if needed).\n  borg now always assumes it has a unique host id - either automatically\n  from fqdn plus uuid.getnode() or overridden via BORG_HOST_ID.\n- docs:\n\n  - added Alpine Linux to distribution list\n  - elaborate on append-only mode docs\n- vagrant:\n\n  - darwin: new 10.12 box\n  - freebsd: new 12.0 box\n  - openbsd: new 6.4 box\n  - misc. updates / fixes\n\n\nVersion 1.2.0a3 (2019-02-26)\n----------------------------\n\nFixes:\n\n- LibreSSL build fixes, #4403\n- dummy ACL/xattr code fixes (used by OpenBSD and others), #4403\n- create: fix openat/statat issues for root directory, #4405\n\n\nVersion 1.2.0a2 and earlier (2019-02-24)\n----------------------------------------\n\nNew features:\n\n- compact: \"borg compact\" needs to be used to free repository space by\n  compacting the segments (reading sparse segments, rewriting still needed\n  data to new segments, deleting the sparse segments).\n  Borg < 1.2 invoked compaction automatically at the end of each repository\n  writing command.\n  Borg >= 1.2 does not do that any more to give better speed, more control,\n  more segment file stability (== less stuff moving to newer segments) and\n  more robustness.\n  See the docs about \"borg compact\" for more details.\n- \"borg compact --cleanup-commits\" is to cleanup the tons of 17byte long\n  commit-only segment files caused by borg 1.1.x issue #2850.\n  Invoke this once after upgrading (the server side) borg to 1.2.\n  Compaction now automatically removes unneeded commit-only segment files.\n- prune: Show which rule was applied to keep archive, #2886\n- add fixed blocksize chunker (see --chunker-params docs), #1086\n\nFixes:\n\n- avoid stale filehandle issues, #3265\n- use more FDs, avoid race conditions on active fs, #906, #908, #1038\n- add O_NOFOLLOW to base flags, #908\n- compact:\n\n  - require >10% freeable space in a segment, #2985\n  - repository compaction now automatically removes unneeded 17byte\n    commit-only segments, #2850\n- make swidth available on all posix platforms, #2667\n\nOther changes:\n\n- repository: better speed and less stuff moving around by using separate\n  segment files for manifest DELETEs and PUTs, #3947\n- use pyinstaller v3.3.1 to build binaries\n- update bundled zstd code to 1.3.8, #4210\n- update bundled lz4 code to 1.8.3, #4209\n- msgpack:\n\n  - switch to recent \"msgpack\" pypi pkg name, #3890\n  - wrap msgpack to avoid future compat complications, #3632, #2738\n  - support msgpack 0.6.0 and 0.6.1, #4220, #4308\n\n- llfuse: modernize / simplify llfuse version requirements\n- code refactorings / internal improvements:\n\n  - include size/csize/nfiles[_parts] stats into archive, #3241\n  - calc_stats: use archive stats metadata, if available\n  - crypto: refactored crypto to use an AEAD style API\n  - crypto: new AES-OCB, CHACHA20-POLY1305\n  - create: use less syscalls by not using a python file obj, #906, #3962\n  - diff: refactor the diff functionality to new ItemDiff class, #2475\n  - archive: create FilesystemObjectProcessors class\n  - helpers: make a package, split into smaller modules\n  - xattrs: move to platform package, use cython instead ctypes, #2495\n  - xattrs/acls/bsdflags: misc. code/api optimizations\n  - FUSE: separate creation of filesystem from implementation of llfuse funcs, #3042\n  - FUSE: use unpacker.tell() instead of deprecated write_bytes, #3899\n  - setup.py: move build_man / build_usage code to setup_docs.py\n  - setup.py: update to use a newer Cython/setuptools API for compiling .pyx -> .c, #3788\n  - use python 3.5's os.scandir / os.set_blocking\n  - multithreading preparations (not used yet):\n\n    - item.to_optr(), Item.from_optr()\n    - fix chunker holding the GIL during blocking I/O\n  - C code portability / basic MSC compatibility, #4147, #2677\n- testing:\n\n  - vagrant: new VMs for linux/bsd/darwin, most with OpenSSL 1.1 and py36\n\n\nVersion 1.1.18 (2022-06-05)\n---------------------------\n\nCompatibility notes:\n\n- When upgrading from borg 1.0.x to 1.1.x, please note:\n\n  - read all the compatibility notes for 1.1.0*, starting from 1.1.0b1.\n  - borg upgrade: you do not need to and you also should not run it.\n  - borg might ask some security-related questions once after upgrading.\n    You can answer them either manually or via environment variable.\n    One known case is if you use unencrypted repositories, then it will ask\n    about a unknown unencrypted repository one time.\n  - your first backup with 1.1.x might be significantly slower (it might\n    completely read, chunk, hash a lot files) - this is due to the\n    --files-cache mode change (and happens every time you change mode).\n    You can avoid the one-time slowdown by using the pre-1.1.0rc4-compatible\n    mode (but that is less safe for detecting changed files than the default).\n    See the --files-cache docs for details.\n- 1.1.11 removes WSL autodetection (Windows 10 Subsystem for Linux).\n  If WSL still has a problem with sync_file_range, you need to set\n  BORG_WORKAROUNDS=basesyncfile in the borg process environment to\n  work around the WSL issue.\n- 1.1.14 changes return codes due to a bug fix:\n  In case you have scripts expecting rc == 2 for a signal exit, you need to\n  update them to check for >= 128 (as documented since long).\n- 1.1.15 drops python 3.4 support, minimum requirement is 3.5 now.\n- 1.1.17 install_requires the \"packaging\" pypi package now.\n\nNew features:\n\n- check --repair: significantly speed up search for next valid object in segment, #6022\n- create: add retry_erofs workaround for O_NOATIME issue on volume shadow copies in WSL1, #6024\n- key export: display key if path is '-' or not given, #6092\n- list --format: add command_line to format keys, #6108\n\nFixes:\n\n- check: improve error handling for corrupt archive metadata block,\n  make robust_iterator more robust, #4777\n- diff: support presence change for blkdev, chrdev and fifo items, #6483\n- diff: reduce memory consumption, fix is_hardlink_master\n- init: disallow overwriting of existing keyfiles\n- info: fix authenticated mode repo to show \"Encrypted: No\", #6462\n- info: emit repo info even if repo has 0 archives, #6120\n- list: remove placeholders for shake_* hashes, #6082\n- mount -o versions: give clear error msg instead of crashing\n- show_progress: add finished=true/false to archive_progress json, #6570\n- fix hardlinkable file type check, #6037\n- do not show archive name in error msgs referring to the repository, #6023\n- prettier error msg (no stacktrace) if exclude file is missing, #5734\n- do not require BORG_CONFIG_DIR if BORG_{SECURITY,KEYS}_DIR are set, #5979\n- atomically create the CACHE_TAG file, #6028\n- deal with the SaveFile/SyncFile race, docs, see #6176 5c5b59bc9\n- avoid expanding path into LHS of formatting operation + tests, #6064 #6063\n- repository: quota / compactable computation fixes, #6119.\n  This is mainly to keep the repo code in sync with borg 1.2. As borg 1.1\n  compacts immediately, there was not really an issue with this in 1.1.\n- fix transaction rollback: use files cache filename as found in txn.active, #6353\n- do not load files cache for commands not using it, fixes #5673\n- fix scp repo url parsing for ip v6 addrs, #6526\n- repo::archive location placeholder expansion fixes, #5826, #5998\n\n  - use expanded location for log output\n  - support placeholder expansion for BORG_REPO env var\n- respect umask for created directory and file modes, #6400\n- safer truncate_and_unlink implementation\n\nOther changes:\n\n- upgrade bundled xxhash code to 0.8.1\n- fix xxh64 related build (setup.py and post-0.8.1 patch for static_assert).\n  The patch was required to build the bundled xxhash code on FreeBSD, see\n  https://github.com/Cyan4973/xxHash/pull/670\n- msgpack build: remove endianness macro, #6105\n- update and fix shell completions\n- fuse: remove unneeded version check and compat code\n- delete --force: do not ask when deleting a repo, #5941\n- delete: don't commit if nothing was deleted, avoid cache sync, #6060\n- delete: add repository id and location to prompt\n- compact segments: improve freeable / freed space log output, #5679\n- if ensure_dir() fails, give more informative error message, #5952\n- load_key: no key is same as empty key, #6441\n- better error msg for defect or unsupported repo configs, #6566\n- use hmac.compare_digest instead of ==, #6470\n- implement more standard hashindex.setdefault behaviour\n- remove stray punctuation from secure-erase message\n- add development.lock.txt, use a real python 3.5 to generate frozen reqs\n- setuptools 60.7.0 breaks pyinstaller, #6246\n- setup.py clean2 was added to work around some setuptools customizability limitation.\n- allow extra compiler flags for every extension build\n- C code: make switch fallthrough explicit\n- Cython code: fix \"useless trailing comma\" cython warnings\n- requirements.lock.txt: use the latest cython 0.29.30\n- fix compilation warnings: \u2018PyUnicode_AsUnicode\u2019 is deprecated\n- docs:\n\n  - ~/.config/borg/keys is not used for repokey keys, #6107\n  - excluded parent dir's metadata can't restore, #6062\n  - permissions note rewritten to make it less confusing, #5490\n  - add note about grandfather-father-son backup retention policy / rotation scheme\n  - clarify who starts the remote agent (borg serve)\n  - test/improve pull backup docs, #5903\n  - document the socat pull mode described in #900 #515\u00df\n  - borg serve: improve ssh forced commands docs, #6083\n  - improve docs for borg list --format, #6080\n  - fix the broken link to .nix file\n  - clarify pattern usage with commands, #5176\n  - clarify user_id vs uid for fuse, #5723\n  - fix binary build freebsd/macOS version, #5942\n  - FAQ: fix manifest-timestamp path, #6016\n  - remove duplicate faq entries, #5926\n  - fix sphinx warnings, #5919\n  - virtualisation speed tips\n  - fix values of TAG bytes, #6515\n  - recommend umask for passphrase file perms\n  - update link to ubuntu packages, #6485\n  - clarify on-disk order and size of log entry fields, #6357\n  - do not transform --/--- to unicode dashes\n  - improve linking inside docs, link to borg_placeholders, link to borg_patterns\n  - use same phrasing in misc. help texts\n  - borg init: explain the encryption modes better\n  - explain the difference between a path that ends with or without a slash, #6297\n  - clarify usage of patternfile roots, #6242\n  - borg key export: add examples\n  - updates about features not experimental any more: FUSE \"versions\" view, --pattern*, #6134\n  - fix/update cygwin package requirements\n  - impact of deleting path/to/repo/nonce, #5858\n  - warn about tampered server nonce\n  - mention BORG_FILES_CACHE_SUFFIX as alternative to BORG_FILES_CACHE_TTL, #5602\n  - add a troubleshooting note about \"is not a valid repository\" to the FAQ\n- vagrant / CI / testing:\n\n  - misc. fixes and updates, new python versions\n  - macOS on github: re-enable fuse2 testing by downgrading to older macOS, #6099\n  - fix OpenBSD symlink mode test failure, #2055\n  - use the generic/openbsd6 box\n  - strengthen the test: we can read data w/o nonces\n  - add tests for path/to/repo/nonce deletion\n  - darwin64: backport some tunings from master\n  - darwin64: remove fakeroot, #6314\n  - darwin64: fix vagrant scp, #5921\n  - darwin64: use macfuse instead of osxfuse\n  - add ubuntu \"jammy\" 22.04 LTS VM\n  - adapt memory for openindiana64 and darwin64\n\n\nVersion 1.1.17 (2021-07-12)\n---------------------------\n\nCompatibility notes:\n\n- When upgrading from borg 1.0.x to 1.1.x, please note:\n\n  - read all the compatibility notes for 1.1.0*, starting from 1.1.0b1.\n  - borg upgrade: you do not need to and you also should not run it.\n  - borg might ask some security-related questions once after upgrading.\n    You can answer them either manually or via environment variable.\n    One known case is if you use unencrypted repositories, then it will ask\n    about a unknown unencrypted repository one time.\n  - your first backup with 1.1.x might be significantly slower (it might\n    completely read, chunk, hash a lot files) - this is due to the\n    --files-cache mode change (and happens every time you change mode).\n    You can avoid the one-time slowdown by using the pre-1.1.0rc4-compatible\n    mode (but that is less safe for detecting changed files than the default).\n    See the --files-cache docs for details.\n- 1.1.11 removes WSL autodetection (Windows 10 Subsystem for Linux).\n  If WSL still has a problem with sync_file_range, you need to set\n  BORG_WORKAROUNDS=basesyncfile in the borg process environment to\n  work around the WSL issue.\n- 1.1.14 changes return codes due to a bug fix:\n  In case you have scripts expecting rc == 2 for a signal exit, you need to\n  update them to check for >= 128 (as documented since long).\n- 1.1.15 drops python 3.4 support, minimum requirement is 3.5 now.\n- 1.1.17 install_requires the \"packaging\" pypi package now.\n\nFixes:\n\n- pyinstaller dir-mode: fix pyi detection / LIBPATH treatment, #5897\n- handle crash due to kill stale lock race, #5828\n- fix BORG_CACHE_DIR crashing borg if empty, #5216\n- create --dry-run: fix display of kept tagfile, #5834\n- fix missing parameter in \"did not consistently fail\" msg, #5822\n- missing / healed chunks: always tell chunk ID, #5704\n- benchmark: make sure cleanup happens even on exceptions, #5630\n\nNew features:\n\n- implement BORG_SELFTEST env variable, #5871.\n  this can be used to accelerate borg startup a bit. not recommended for\n  normal usage, but borg mass hosters with a lot of borg invocations can\n  save some resources with this. on my laptop, this saved ~100ms cpu time\n  (sys+user) per borg command invocation.\n- implement BORG_LIBC env variable to give the libc filename, #5870.\n  you can use this if a borg does not find your libc.\n- check: add progress indicator for archive check.\n- allow --files-cache=size (not recommended, make sure you know what you do)\n\nOther changes:\n\n- Python 3.10 now officially supported!\n  we test on py310-dev on github CI since a while and now also on the vagrant\n  machines, so it should work ok.\n- github CI: test on py310 (again)\n- get rid of distutils, use packaging and setuptools.\n  distutils is deprecated and gives warnings on py 3.10.\n- setup.py: rename \"clean\" to \"clean2\" to avoid shadowing the \"clean\" command.\n- remove libc filename fallback for the BSDs (there is no \"usual\" name)\n- cleanup flake8 checks, fix some pep8 violations.\n- docs building: replace deprecated function \".add_stylesheet()\" for Sphinx 4 compatibility\n- docs:\n\n  - add a hint on sleeping computer and ssh connections, #5301\n  - update the documentation on hacked backup client, #5480\n  - improve docs/FAQ about append-only remote repos, #5497\n  - complement the documentation for pattern files and exclude files, #5520\n  - \"filename with spaces\" example added to exclude file, #5236\n    note: no whitespace escaping needed, processed by borg.\n  - add info on renaming repositories, #5240\n  - clarify borg check --verify-data, #5808\n  - add notice about defective hardware to check documentation, #5753\n  - add paragraph added in #5855 to utility documentation source\n  - add missing leading slashes in help patterns, #5857\n  - clarify \"you will need key and passphrase\" borg init warning, #4622\n  - pull mode: add some warnings, #5827\n  - mention tar --compare (compare archive to fs files), #5880\n  - fix typos, backport of #5597\n- vagrant:\n\n  - add py3.7.11 for binary build, also add 3.10-dev.\n  - use latest Cython 0.29.23 for py310 compat fixes.\n  - more RAM for openindiana upgrade plan resolver, it just hangs (swaps?) if\n    there is too little RAM.\n  - fix install_pyenv to adapt to recent changes in pyenv (same as in master now).\n  - use generic/netbsd9 box, copied from master branch.\n\n\nVersion 1.1.16 (2021-03-23)\n---------------------------\n\nFixes:\n\n- setup.py: add special openssl prefix for Apple M1 compatibility\n- do not recurse into duplicate roots, #5603\n- remove empty shadowed_segments lists, #5275, #5614\n- fix libpython load error when borg fat binary / dir-based binary is invoked\n  via a symlink by upgrading pyinstaller to v4.2, #5688\n- config: accept non-int value (like 500M or 100G) for max_segment_size or\n  storage_quota, #5639.\n  please note: when setting a non-int value for this in a repo config,\n  using the repo will require borg >= 1.1.16.\n\nNew features:\n\n- bundled msgpack: drop support for old buffer protocol to support Python 3.10\n- verbose files cache logging via --debug-topic=files_cache, #5659.\n  Use this if you suspect that borg does not detect unmodified files as expected.\n- create/extract: add --noxattrs and --noacls option, #3955.\n  when given with borg create, borg will not get xattrs / ACLs from input files\n  (and thus, it will not archive xattrs / ACLs). when given with borg extract,\n  borg will not read xattrs / ACLs from archive and will not set xattrs / ACLs\n  on extracted files.\n- diff: add --json-lines option, #3765\n- check: debug log segment filename\n- borg debug dump-hints\n\nOther changes:\n\n- Tab completion support for additional archives for 'borg delete'\n- repository: deduplicate code of put and delete, no functional change\n- tests: fix result order issue (sporadic test failure on openindiana)\n- vagrant:\n\n  - upgrade pyinstaller to v4.2, #5671\n  - avoid grub-install asking interactively for device\n  - remove the xenial box\n  - update freebsd box to 12.1\n- docs:\n\n  - update macOS install instructions, #5677\n  - use macFUSE (not osxfuse) for Apple M1 compatibility\n  - update docs for dev environment installation instructions, #5643\n  - fix grammar in faq\n  - recommend running tests only on installed versions for setup\n  - add link back to git-installation\n  - remove /var/cache exclusion in example commands, #5625.\n    This is generally a poor idea and shouldn't be promoted through examples.\n  - add repology.org badge with current packaging status\n  - explain hash collision\n  - add unsafe workaround to use an old repo copy, #5722\n\n\nVersion 1.1.15 (2020-12-25)\n---------------------------\n\nFixes:\n\n- extract:\n\n  - improve exception handling when setting xattrs, #5092.\n  - emit a warning message giving the path, xattr key and error message.\n  - continue trying to restore other xattrs and bsdflags of the same file\n    after an exception with xattr-setting happened.\n- export-tar:\n\n  - set tar format to GNU_FORMAT explicitly, #5274\n  - fix memory leak with ssh: remote repository, #5568\n  - fix potential memory leak with ssh: remote repository with partial extraction\n- create: fix --dry-run and --stats coexistence, #5415\n- use --timestamp for {utcnow} and {now} if given, #5189\n\nNew features:\n\n- create: implement --stdin-mode, --stdin-user and --stdin-group, #5333\n- allow appending the files cache filename with BORG_FILES_CACHE_SUFFIX env var\n\nOther changes:\n\n- drop python 3.4 support, minimum requirement is 3.5 now.\n- enable using libxxhash instead of bundled xxh64 code\n- update llfuse requirements (1.3.8)\n- set cython language_level in some files to fix warnings\n- allow EIO with warning when trying to hardlink\n- PropDict: fail early if internal_dict is not a dict\n- update shell completions\n- tests / CI\n\n  - add a test for the hashindex corruption bug, #5531 #4829\n  - fix spurious failure in test_cache_files, #5438\n  - added a github ci workflow\n  - reduce testing on travis, no macOS, no py3x-dev, #5467\n  - travis: use newer dists, native py on dist\n- vagrant:\n\n  - remove jessie and trusty boxes, #5348 #5383\n  - pyinstaller 4.0, build on py379\n  - binary build on stretch64, #5348\n  - remove easy_install based pip installation\n- docs:\n\n  - clarify '--one-file-system' for btrfs, #5391\n  - add example for excluding content using the --pattern cmd line arg\n  - complement the documentation for pattern files and exclude files, #5524\n  - made ansible playbook more generic, use package instead of pacman. also\n    change state from \"latest\" to \"present\".\n  - complete documentation on append-only remote repos, #5497\n  - internals: rather talk about target size than statistics, #5336\n  - new compression algorithm policy, #1633 #5505\n  - faq: add a hint on sleeping computer, #5301\n  - note requirements for full disk access on macOS Catalina, #5303\n  - fix/improve description of borg upgrade hardlink usage, #5518\n- modernize 1.1 code:\n\n  - drop code/workarounds only needed to support Python 3.4\n  - remove workaround for pre-release py37 argparse bug\n  - removed some outdated comments/docstrings\n  - requirements: remove some restrictions, lock on current versions\n\n\nVersion 1.1.14 (2020-10-07)\n---------------------------\n\nFixes:\n\n- check --repair: fix potential data loss when interrupting it, #5325\n- exit with 128 + signal number (as documented) when borg is killed by a signal, #5161\n- fix hardlinked CACHEDIR.TAG processing, #4911\n- create --read-special: .part files also should be regular files, #5217\n- llfuse dependency: choose least broken 1.3.6/1.3.7.\n  1.3.6 is broken on python 3.9, 1.3.7 is broken on FreeBSD.\n\nOther changes:\n\n- upgrade bundled xxhash to 0.7.4\n- self test: if it fails, also point to OS and hardware, #5334\n- pyinstaller: compute basepath from spec file location\n- prettier error message when archive gets too big, #5307\n- check/recreate are not \"experimental\" any more (but still potentially dangerous):\n\n  - recreate: remove extra confirmation\n  - rephrase some warnings, update docs, #5164\n- shell completions:\n\n  - misc. updates / fixes\n  - support repositories in fish tab completion, #5256\n  - complete $BORG_RECREATE_I_KNOW_WHAT_I_AM_DOING\n  - rewrite zsh completion:\n\n    - completion for almost all optional and positional arguments\n    - completion for Borg environment variables (parameters)\n- use \"allow/deny list\" instead of \"white/black list\" wording\n- declare \"allow_cache_wipe\" marker in setup.cfg to avoid pytest warning\n- vagrant / tests:\n\n  - misc. fixes / updates\n  - use python 3.5.10 for binary build\n  - build directory-based binaries additionally to the single file binaries\n  - add libffi-dev, required to build python\n  - use cryptography<3.0, more recent versions break the jessie box\n  - test on python 3.9\n  - do brew update with /dev/null redirect to avoid \"too much log output\" on travis-ci\n- docs:\n\n  - add ssh-agent pull backup method docs, #5288\n  - how to approach borg speed issues, #5371\n  - mention double --force in prune docs\n  - update Homebrew install instructions, #5185\n  - better description of how cache and rebuilds of it work\n  - point to borg create --list item flags in recreate usage, #5165\n  - add security faq explaining AES-CTR crypto issues, #5254\n  - add a note to create from stdin regarding files cache, #5180\n  - fix borg.1 manpage generation regression, #5211\n  - clarify how exclude options work in recreate, #5193\n  - add section for retired contributors\n  - hint about not misusing private email addresses of contributors for borg support\n\n\nVersion 1.1.13 (2020-06-06)\n---------------------------\n\nCompatibility notes:\n\n- When upgrading from borg 1.0.x to 1.1.x, please note:\n\n  - read all the compatibility notes for 1.1.0*, starting from 1.1.0b1.\n  - borg upgrade: you do not need to and you also should not run it.\n  - borg might ask some security-related questions once after upgrading.\n    You can answer them either manually or via environment variable.\n    One known case is if you use unencrypted repositories, then it will ask\n    about a unknown unencrypted repository one time.\n  - your first backup with 1.1.x might be significantly slower (it might\n    completely read, chunk, hash a lot files) - this is due to the\n    --files-cache mode change (and happens every time you change mode).\n    You can avoid the one-time slowdown by using the pre-1.1.0rc4-compatible\n    mode (but that is less safe for detecting changed files than the default).\n    See the --files-cache docs for details.\n- 1.1.11 removes WSL autodetection (Windows 10 Subsystem for Linux).\n  If WSL still has a problem with sync_file_range, you need to set\n  BORG_WORKAROUNDS=basesyncfile in the borg process environment to\n  work around the WSL issue.\n\nFixes:\n\n- rebuilt using a current Cython version, compatible with python 3.8, #5214\n\n\nVersion 1.1.12 (2020-06-06)\n---------------------------\n\nFixes:\n\n- fix preload-related memory leak, #5202.\n- mount / borgfs (FUSE filesystem):\n\n  - fix FUSE low linear read speed on large files, #5067\n  - fix crash on old llfuse without birthtime attrs, #5064 - accidentally\n    we required llfuse >= 1.3. Now also old llfuse works again.\n  - set f_namemax in statfs result, #2684\n- update precedence of env vars to set config and cache paths, #4894\n- correctly calculate compression ratio, taking header size into account, too\n\nNew features:\n\n- --bypass-lock option to bypass locking with read-only repositories\n\nOther changes:\n\n- upgrade bundled zstd to 1.4.5\n- travis: adding comments and explanations to Travis config / install script,\n  improve macOS builds.\n- tests: test_delete_force: avoid sporadic test setup issues, #5196\n- misc. vagrant fixes\n- the binary for macOS is now built on macOS 10.12\n- the binaries for Linux are now built on Debian 8 \"Jessie\", #3761\n- docs:\n\n  - PlaceholderError not printed as JSON, #4073\n  - \"How important is Borg config?\", #4941\n  - make Sphinx warnings break docs build, #4587\n  - some markup / warning fixes\n  - add \"updating borgbackup.org/releases\" to release checklist, #4999\n  - add \"rendering docs\" to release checklist, #5000\n  - clarify borg init's encryption modes\n  - add note about patterns and stored paths, #4160\n  - add upgrade of tools to pip installation how-to\n  - document one cause of orphaned chunks in check command, #2295\n  - linked recommended restrictions to ssh public keys on borg servers in faq, #4946\n\n\nVersion 1.1.11 (2020-03-08)\n---------------------------\n\nCompatibility notes:\n\n- When upgrading from borg 1.0.x to 1.1.x, please note:\n\n  - read all the compatibility notes for 1.1.0*, starting from 1.1.0b1.\n  - borg upgrade: you do not need to and you also should not run it.\n  - borg might ask some security-related questions once after upgrading.\n    You can answer them either manually or via environment variable.\n    One known case is if you use unencrypted repositories, then it will ask\n    about a unknown unencrypted repository one time.\n  - your first backup with 1.1.x might be significantly slower (it might\n    completely read, chunk, hash a lot files) - this is due to the\n    --files-cache mode change (and happens every time you change mode).\n    You can avoid the one-time slowdown by using the pre-1.1.0rc4-compatible\n    mode (but that is less safe for detecting changed files than the default).\n    See the --files-cache docs for details.\n- 1.1.11 removes WSL autodetection (Windows 10 Subsystem for Linux).\n  If WSL still has a problem with sync_file_range, you need to set\n  BORG_WORKAROUNDS=basesyncfile in the borg process environment to\n  work around the WSL issue.\n\nFixes:\n\n- fixed potential index corruption / data loss issue due to bug in hashindex_set, #4829.\n  Please read and follow the more detailed notes close to the top of this document.\n- upgrade bundled xxhash to 0.7.3, #4891.\n  0.7.2 is the minimum requirement for correct operations on ARMv6 in non-fixup\n  mode, where unaligned memory accesses cause bus errors.\n  0.7.3 adds some speedups and libxxhash 0.7.3 even has a pkg-config file now.\n- upgrade bundled lz4 to 1.9.2\n- upgrade bundled zstd to 1.4.4\n- fix crash when upgrading erroneous hints file, #4922\n- extract:\n\n  - fix KeyError for \"partial\" extraction, #4607\n  - fix \"partial\" extract for hardlinked contentless file types, #4725\n  - fix preloading for old (0.xx) remote servers, #4652\n  - fix confusing output of borg extract --list --strip-components, #4934\n- delete: after double-force delete, warn about necessary repair, #4704\n- create: give invalid repo error msg if repo config not found, #4411\n- mount: fix FUSE mount missing st_birthtime, #4763 #4767\n- check: do not stumble over invalid item key, #4845\n- info: if the archive doesn't exist, print a pretty message, #4793\n- SecurityManager.known(): check all files, #4614\n- Repository.open: use stat() to check for repo dir, #4695\n- Repository.check_can_create_repository: use stat() to check, #4695\n- fix invalid archive error message\n- fix optional/non-optional location arg, #4541\n- commit-time free space calc: ignore bad compact map entries, #4796\n- ignore EACCES (errno 13) when hardlinking the old config, #4730\n- --prefix / -P: fix processing, avoid argparse issue, #4769\n\nNew features:\n\n- enable placeholder usage in all extra archive arguments\n- new BORG_WORKAROUNDS mechanism, basesyncfile, #4710\n- recreate: support --timestamp option, #4745\n- support platforms without os.link (e.g. Android with Termux), #4901.\n  if we don't have os.link, we just extract another copy instead of making a hardlink.\n- support linux platforms without sync_file_range (e.g. Android 7 with Termux), #4905\n\nOther:\n\n- ignore --stats when given with --dry-run, but continue, #4373\n- add some ProgressIndicator msgids to code / fix docs, #4935\n- elaborate on \"Calculating size\" message\n- argparser: always use REPOSITORY in metavar, also use more consistent help phrasing.\n- check: improve error output for matching index size, see #4829\n- docs:\n\n  - changelog: add advisory about hashindex_set bug #4829\n  - better describe BORG_SECURITY_DIR, BORG_CACHE_DIR, #4919\n  - infos about cache security assumptions, #4900\n  - add FAQ describing difference between a local repo vs. repo on a server.\n  - document how to test exclusion patterns without performing an actual backup\n  - timestamps in the files cache are now usually ctime, #4583\n  - fix bad reference to borg compact (does not exist in 1.1), #4660\n  - create: borg 1.1 is not future any more\n  - extract: document limitation \"needs empty destination\", #4598\n  - how to supply a passphrase, use crypto devices, #4549\n  - fix osxfuse github link in installation docs\n  - add example of exclude-norecurse rule in help patterns\n  - update macOS Brew link\n  - add note about software for automating backups, #4581\n  - AUTHORS: mention copyright+license for bundled msgpack\n  - fix various code blocks in the docs, #4708\n  - updated docs to cover use of temp directory on remote, #4545\n  - add restore docs, #4670\n  - add a pull backup / push restore how-to, #1552\n  - add FAQ how to retain original paths, #4532\n  - explain difference between --exclude and --pattern, #4118\n  - add FAQs for SSH connection issues, #3866\n  - improve password FAQ, #4591\n  - reiterate that 'file cache names are absolute' in FAQ\n- tests:\n\n  - cope with ANY error when importing pytest into borg.testsuite, #4652\n  - fix broken test that relied on improper zlib assumptions\n  - test_fuse: filter out selinux xattrs, #4574\n- travis / vagrant:\n\n  - misc python versions removed / changed (due to openssl 1.1 compatibility)\n    or added (3.7 and 3.8, for better borg compatibility testing)\n  - binary building is on python 3.5.9 now\n- vagrant:\n\n  - add new boxes: ubuntu 18.04 and 20.04, debian 10\n  - update boxes: openindiana, darwin, netbsd\n  - remove old boxes: centos 6\n  - darwin: updated osxfuse to 3.10.4\n  - use debian/ubuntu pip/virtualenv packages\n  - rather use python 3.6.2 than 3.6.0, fixes coverage/sqlite3 issue\n  - use requirements.d/development.lock.txt to avoid compat issues\n- travis:\n\n  - darwin: backport some install code / order from master\n  - remove deprecated keyword \"sudo\" from travis config\n  - allow osx builds to fail, #4955\n    this is due to travis-ci frequently being so slow that the OS X builds\n    just fail because they exceed 50 minutes and get killed by travis.\n\n\nVersion 1.1.10 (2019-05-16)\n---------------------------\n\nFixes:\n\n- extract: hang on partial extraction with ssh: repo, when hardlink master\n  is not matched/extracted and borg hangs on related slave hardlink, #4350\n- lrucache: regularly remove old FDs, #4427\n- avoid stale filehandle issues, #3265\n- freebsd: make xattr platform code api compatible with linux, #3952\n- use whitelist approach for borg serve, #4097\n- borg command shall terminate with rc 2 for ImportErrors, #4424\n- create: only run stat_simple_attrs() once, this increases\n  backup with lots of unchanged files performance by ~ 5%.\n- prune: fix incorrect borg prune --stats output with --dry-run, #4373\n- key export: emit user-friendly error if repo key is exported to a directory,\n  #4348\n\nNew features:\n\n- bundle latest supported msgpack-python release (0.5.6), remove msgpack-python\n  from setup.py install_requires - by default we use the bundled code now.\n  optionally, we still support using an external msgpack (see hints in\n  setup.py), but this requires solid requirements management within\n  distributions and is not recommended.\n  borgbackup will break if you upgrade msgpack to an unsupported version.\n- display msgpack version as part of sysinfo (e.g. in tracebacks)\n- timestamp for borg delete --info added, #4359\n- enable placeholder usage in --comment and --glob-archives, #4559, #4495\n\nOther:\n\n- serve: do not check python/libc for borg serve, #4483\n- shell completions: borg diff second archive\n- release scripts: signing binaries with Qubes OS support\n- testing:\n\n  - vagrant: upgrade openbsd box to 6.4\n  - travis-ci: lock test env to py 3.4 compatible versions, #4343\n  - get rid of confusing coverage warning, #2069\n  - rename test_mount_hardlinks to test_fuse_mount_hardlinks,\n    so both can be excluded by \"not test_fuse\".\n  - pure-py msgpack warning shall not make a lot of tests fail, #4558\n- docs:\n\n  - add \"SSH Configuration\" section to \"borg serve\", #3988, #636, #4485\n  - README: new URL for funding options\n  - add a sample logging.conf in docs/misc, #4380\n  - elaborate on append-only mode docs, #3504\n  - installation: added Alpine Linux to distribution list, #4415\n  - usage.html: only modify window.location when redirecting, #4133\n  - add msgpack license to docs/3rd_party/msgpack\n- vagrant / binary builds:\n\n  - use python 3.5.7 for builds\n  - use osxfuse 3.8.3\n\n\nVersion 1.1.9 (2019-02-10)\n--------------------------\n\nCompatibility notes:\n\n- When upgrading from borg 1.0.x to 1.1.x, please note:\n\n  - read all the compatibility notes for 1.1.0*, starting from 1.1.0b1.\n  - borg upgrade: you do not need to and you also should not run it.\n  - borg might ask some security-related questions once after upgrading.\n    You can answer them either manually or via environment variable.\n    One known case is if you use unencrypted repositories, then it will ask\n    about a unknown unencrypted repository one time.\n  - your first backup with 1.1.x might be significantly slower (it might\n    completely read, chunk, hash a lot files) - this is due to the\n    --files-cache mode change (and happens every time you change mode).\n    You can avoid the one-time slowdown by using the pre-1.1.0rc4-compatible\n    mode (but that is less safe for detecting changed files than the default).\n    See the --files-cache docs for details.\n\nFixes:\n\n- security fix: configure FUSE with \"default_permissions\", #3903\n  \"default_permissions\" is now enforced by borg by default to let the\n  kernel check uid/gid/mode based permissions.\n  \"ignore_permissions\" can be given not to enforce \"default_permissions\".\n- make \"hostname\" short, even on misconfigured systems, #4262\n- fix free space calculation on macOS (and others?), #4289\n- config: quit with error message when no key is provided, #4223\n- recover_segment: handle too small segment files correctly, #4272\n- correctly release memoryview, #4243\n- avoid diaper pattern in configparser by opening files, #4263\n- add \"# cython: language_level=3\" directive to .pyx files, #4214\n- info: consider part files for \"This archive\" stats, #3522\n- work around Microsoft WSL issue #645 (sync_file_range), #1961\n\nNew features:\n\n- add --rsh command line option to complement BORG_RSH env var, #1701\n- init: --make-parent-dirs parent1/parent2/repo_dir, #4235\n\nOther:\n\n- add archive name to check --repair output, #3447\n- check for unsupported msgpack versions\n- shell completions:\n\n  - new shell completions for borg 1.1.9\n  - more complete shell completions for borg mount -o\n  - added shell completions for borg help\n  - option arguments for zsh tab completion\n- docs:\n\n  - add FAQ regarding free disk space check, #3905\n  - update BORG_PASSCOMMAND example and clarify variable expansion, #4249\n  - FAQ regarding change of compression settings, #4222\n  - add note about BSD flags to changelog, #4246\n  - improve logging in example automation script\n  - add note about files changing during backup, #4081\n  - work around the backslash issue, #4280\n  - update release workflow using twine (docs, scripts), #4213\n  - add warnings on repository copies to avoid future problems, #4272\n- tests:\n\n  - fix the homebrew 1.9 issues on travis-ci, #4254\n  - fix duplicate test method name, #4311\n\n\nVersion 1.1.8 (2018-12-09)\n--------------------------\n\nFixes:\n\n- enforce storage quota if set by serve-command, #4093\n- invalid locations: give err msg containing parsed location, #4179\n- list repo: add placeholders for hostname and username, #4130\n- on linux, symlinks can't have ACLs, so don't try to set any, #4044\n\nNew features:\n\n- create: added PATH::archive output on INFO log level\n- read a passphrase from a file descriptor specified in the\n  BORG_PASSPHRASE_FD environment variable.\n\nOther:\n\n- docs:\n\n  - option --format is required for some expensive-to-compute values for json\n\n    borg list by default does not compute expensive values except when\n    they are needed. whether they are needed is determined by the format,\n    in standard mode as well as in --json mode.\n  - tell that our binaries are x86/x64 amd/intel, bauerj has ARM\n  - fixed wrong archive name pattern in CRUD benchmark help\n  - fixed link to cachedir spec in docs, #4140\n- tests:\n\n  - stop using fakeroot on travis, avoids sporadic EISDIR errors, #2482\n  - xattr key names must start with \"user.\" on linux\n  - fix code so flake8 3.6 does not complain\n  - explicitly convert environment variable to str, #4136\n  - fix DeprecationWarning: Flags not at the start of the expression, #4137\n  - support pytest4, #4172\n- vagrant:\n\n  - use python 3.5.6 for builds\n\n\nVersion 1.1.7 (2018-08-11)\n--------------------------\n\nCompatibility notes:\n\n- added support for Python 3.7\n\nFixes:\n\n- cache lock: use lock_wait everywhere to fix infinite wait, see #3968\n- don't archive tagged dir when recursing an excluded dir, #3991\n- py37 argparse: work around bad default in py 3.7.0a/b/rc, #3996\n- py37 remove loggerDict.clear() from tearDown method, #3805\n- some fixes for bugs which likely did not result in problems in practice:\n\n  - fixed logic bug in platform module API version check\n  - fixed xattr/acl function prototypes, added missing ones\n\nNew features:\n\n- init: add warning to store both key and passphrase at safe place(s)\n- BORG_HOST_ID env var to work around all-zero MAC address issue, #3985\n- borg debug dump-repo-objs --ghost (dump everything from segment files,\n  including deleted or superseded objects or commit tags)\n- borg debug search-repo-objs (search in repo objects for hex bytes or strings)\n\nOther changes:\n\n- add Python 3.7 support\n- updated shell completions\n- call socket.gethostname only once\n- locking: better logging, add some asserts\n- borg debug dump-repo-objs:\n\n  - filename layout improvements\n  - use repository.scan() to get on-disk order\n- docs:\n\n  - update installation instructions for macOS\n  - added instructions to install fuse via homebrew\n  - improve diff docs\n  - added note that checkpoints inside files requires 1.1+\n  - add link to tempfile module\n  - remove row/column-spanning from docs source, #4000 #3990\n- tests:\n\n  - fetch less data via os.urandom\n  - add py37 env for tox\n  - travis: add 3.7, remove 3.6-dev (we test with -dev in master)\n- vagrant / binary builds:\n\n  - use osxfuse 3.8.2\n  - use own (uptodate) openindiana box\n\n\nVersion 1.1.6 (2018-06-11)\n--------------------------\n\nCompatibility notes:\n\n- 1.1.6 changes:\n\n  - also allow msgpack-python 0.5.6.\n\nFixes:\n\n- fix borg exception handling on ENOSPC error with xattrs, #3808\n- prune: fix/improve overall progress display\n- borg config repo ... does not need cache/manifest/key, #3802\n- debug dump-repo-objs should not depend on a manifest obj\n- pypi package:\n\n  - include .coveragerc, needed by tox.ini\n  - fix package long description, #3854\n\nNew features:\n\n- mount: add uid, gid, umask mount options\n- delete:\n\n  - only commit once, #3823\n  - implement --dry-run, #3822\n- check:\n\n  - show progress while rebuilding missing manifest, #3787\n  - more --repair output\n- borg config --list <repo>, #3612\n\nOther changes:\n\n- update msgpack requirement, #3753\n- update bundled zstd to 1.3.4, #3745\n- update bundled lz4 code to 1.8.2, #3870\n- docs:\n\n  - describe what BORG_LIBZSTD_PREFIX does\n  - fix and deduplicate encryption quickstart docs, #3776\n- vagrant:\n\n  - FUSE for macOS: upgrade 3.7.1 to 3.8.0\n  - exclude macOS High Sierra upgrade on the darwin64 machine\n  - remove borgbackup.egg-info dir in fs_init (after rsync)\n  - use pyenv-based build/test on jessie32/62\n  - use local 32 and 64bit debian jessie boxes\n  - use \"vagrant\" as username for new xenial box\n- travis OS X: use xcode 8.3 (not broken)\n\n\nVersion 1.1.5 (2018-04-01)\n--------------------------\n\nCompatibility notes:\n\n- 1.1.5 changes:\n\n  - require msgpack-python >= 0.4.6 and < 0.5.0.\n    0.5.0+ dropped python 3.4 testing and also caused some other issues because\n    the python package was renamed to msgpack and emitted some FutureWarning.\n\nFixes:\n\n- create --list: fix that it was never showing M status, #3492\n- create: fix timing for first checkpoint (read files cache early, init\n  checkpoint timer after that), see #3394\n- extract: set rc=1 when extracting damaged files with all-zero replacement\n  chunks or with size inconsistencies, #3448\n- diff: consider an empty file as different to a non-existing file, #3688\n- files cache: improve exception handling, #3553\n- ignore exceptions in scandir_inorder() caused by an implicit stat(),\n  also remove unneeded sort, #3545\n- fixed tab completion problem where a space is always added after path even\n  when it shouldn't\n- build: do .h file content checks in binary mode, fixes build issue for\n  non-ascii header files on pure-ascii locale platforms, #3544 #3639\n- borgfs: fix patterns/paths processing, #3551\n- config: add some validation, #3566\n- repository config: add validation for max_segment_size, #3592\n- set cache previous_location on load instead of save\n- remove platform.uname() call which caused library mismatch issues, #3732\n- add exception handler around deprecated platform.linux_distribution() call\n- use same datetime object for {now} and {utcnow}, #3548\n\nNew features:\n\n- create: implement --stdin-name, #3533\n- add chunker_params to borg archive info (--json)\n- BORG_SHOW_SYSINFO=no to hide system information from exceptions\n\nOther changes:\n\n- updated zsh completions for borg 1.1.4\n- files cache related code cleanups\n- be more helpful when parsing invalid --pattern values, #3575\n- be more clear in secure-erase warning message, #3591\n- improve getpass user experience, #3689\n- docs build: unicode problem fixed when using a py27-based sphinx\n- docs:\n\n  - security: explicitly note what happens OUTSIDE the attack model\n  - security: add note about combining compression and encryption\n  - security: describe chunk size / proximity issue, #3687\n  - quickstart: add note about permissions, borg@localhost, #3452\n  - quickstart: add introduction to repositories & archives, #3620\n  - recreate --recompress: add missing metavar, clarify description, #3617\n  - improve logging docs, #3549\n  - add an example for --pattern usage, #3661\n  - clarify path semantics when matching, #3598\n  - link to offline documentation from README, #3502\n  - add docs on how to verify a signed release with GPG, #3634\n  - chunk seed is generated per repository (not: archive)\n  - better formatting of CPU usage documentation, #3554\n  - extend append-only repo rollback docs, #3579\n- tests:\n\n  - fix erroneously skipped zstd compressor tests, #3606\n  - skip a test if argparse is broken, #3705\n- vagrant:\n\n  - xenial64 box now uses username 'vagrant', #3707\n  - move cleanup steps to fs_init, #3706\n  - the boxcutter wheezy boxes are 404, use local ones\n  - update to Python 3.5.5 (for binary builds)\n\n\nVersion 1.1.4 (2017-12-31)\n--------------------------\n\nCompatibility notes:\n\n- When upgrading from borg 1.0.x to 1.1.x, please note:\n\n  - read all the compatibility notes for 1.1.0*, starting from 1.1.0b1.\n  - borg upgrade: you do not need to and you also should not run it.\n  - borg might ask some security-related questions once after upgrading.\n    You can answer them either manually or via environment variable.\n    One known case is if you use unencrypted repositories, then it will ask\n    about a unknown unencrypted repository one time.\n  - your first backup with 1.1.x might be significantly slower (it might\n    completely read, chunk, hash a lot files) - this is due to the\n    --files-cache mode change (and happens every time you change mode).\n    You can avoid the one-time slowdown by using the pre-1.1.0rc4-compatible\n    mode (but that is less safe for detecting changed files than the default).\n    See the --files-cache docs for details.\n- borg 1.1.4 changes:\n\n  - zstd compression is new in borg 1.1.4, older borg can't handle it.\n  - new minimum requirements for the compression libraries - if the required\n    versions (header and lib) can't be found at build time, bundled code will\n    be used:\n\n    - added requirement: libzstd >= 1.3.0 (bundled: 1.3.2)\n    - updated requirement: liblz4 >= 1.7.0 / r129 (bundled: 1.8.0)\n\nFixes:\n\n- check: data corruption fix: fix for borg check --repair malfunction, #3444.\n  See the more detailed notes close to the top of this document.\n- delete: also delete security dir when deleting a repo, #3427\n- prune: fix building the \"borg prune\" man page, #3398\n- init: use given --storage-quota for local repo, #3470\n- init: properly quote repo path in output\n- fix startup delay with dns-only own fqdn resolving, #3471\n\nNew features:\n\n- added zstd compression. try it!\n- added placeholder {reverse-fqdn} for fqdn in reverse notation\n- added BORG_BASE_DIR environment variable, #3338\n\nOther changes:\n\n- list help topics when invalid topic is requested\n- fix lz4 deprecation warning, requires lz4 >= 1.7.0 (r129)\n- add parens for C preprocessor macro argument usages (did not cause malfunction)\n- exclude broken pytest 3.3.0 release\n- updated fish/bash completions\n- init: more clear exception messages for borg create, #3465\n- docs:\n\n  - add auto-generated docs for borg config\n  - don't generate HTML docs page for borgfs, #3404\n  - docs update for lz4 b2 zstd changes\n  - add zstd to compression help, readme, docs\n  - update requirements and install docs about bundled lz4 and zstd\n- refactored build of the compress and crypto.low_level extensions, #3415:\n\n  - move some lib/build related code to setup_{zstd,lz4,b2}.py\n  - bundle lz4 1.8.0 (requirement: >= 1.7.0 / r129)\n  - bundle zstd 1.3.2 (requirement: >= 1.3.0)\n  - blake2 was already bundled\n  - rename BORG_LZ4_PREFIX env var to BORG_LIBLZ4_PREFIX for better consistency:\n    we also have BORG_LIBB2_PREFIX and BORG_LIBZSTD_PREFIX now.\n  - add prefer_system_lib* = True settings to setup.py - by default the build\n    will prefer a shared library over the bundled code, if library and headers\n    can be found and meet the minimum requirements.\n\n\nVersion 1.1.3 (2017-11-27)\n--------------------------\n\nFixes:\n\n- Security Fix for CVE-2017-15914: Incorrect implementation of access controls\n  allows remote users to override repository restrictions in Borg servers.\n  A user able to access a remote Borg SSH server is able to circumvent access\n  controls post-authentication.\n  Affected releases: 1.1.0, 1.1.1, 1.1.2. Releases 1.0.x are NOT affected.\n- crc32: deal with unaligned buffer, add tests - this broke borg on older ARM\n  CPUs that can not deal with unaligned 32bit memory accesses and raise a bus\n  error in such cases. the fix might also improve performance on some CPUs as\n  all 32bit memory accesses by the crc32 code are properly aligned now. #3317\n- mount: fixed support of --consider-part-files and do not show .borg_part_N\n  files by default in the mounted FUSE filesystem. #3347\n- fixed cache/repo timestamp inconsistency message, highlight that information\n  is obtained from security dir (deleting the cache will not bypass this error\n  in case the user knows this is a legitimate repo).\n- borgfs: don't show sub-command in borgfs help, #3287\n- create: show an error when --dry-run and --stats are used together, #3298\n\nNew features:\n\n- mount: added exclusion group options and paths, #2138\n\n  Reused some code to support similar options/paths as borg extract offers -\n  making good use of these to mount only a smaller subset of dirs/files can\n  speed up mounting a lot and also will consume way less memory.\n\n  borg mount [options] repo_or_archive mountpoint path [paths...]\n\n  paths: you can just give some \"root paths\" (like for borg extract) to\n  only partially populate the FUSE filesystem.\n\n  new options: --exclude[-from], --pattern[s-from], --strip-components\n- create/extract: support st_birthtime on platforms supporting it, #3272\n- add \"borg config\" command for querying/setting/deleting config values, #3304\n\nOther changes:\n\n- clean up and simplify packaging (only package committed files, do not install\n  .c/.h/.pyx files)\n- docs:\n\n  - point out tuning options for borg create, #3239\n  - add instructions for using ntfsclone, zerofree, #81\n  - move image backup-related FAQ entries to a new page\n  - clarify key aliases for borg list --format, #3111\n  - mention break-lock in checkpointing FAQ entry, #3328\n  - document sshfs rename workaround, #3315\n  - add FAQ about removing files from existing archives\n  - add FAQ about different prune policies\n  - usage and man page for borgfs, #3216\n  - clarify create --stats duration vs. wall time, #3301\n  - clarify encrypted key format for borg key export, #3296\n  - update release checklist about security fixes\n  - document good and problematic option placements, fix examples, #3356\n  - add note about using --nobsdflags to avoid speed penalty related to\n    bsdflags, #3239\n  - move most of support section to www.borgbackup.org\n\n\nVersion 1.1.2 (2017-11-05)\n--------------------------\n\nFixes:\n\n- fix KeyError crash when talking to borg server < 1.0.7, #3244\n- extract: set bsdflags last (include immutable flag), #3263\n- create: don't do stat() call on excluded-norecurse directory, fix exception\n  handling for stat() call, #3209\n- create --stats: do not count data volume twice when checkpointing, #3224\n- recreate: move chunks_healthy when excluding hardlink master, #3228\n- recreate: get rid of chunks_healthy when rechunking (does not match), #3218\n- check: get rid of already existing not matching chunks_healthy metadata, #3218\n- list: fix stdout broken pipe handling, #3245\n- list/diff: remove tag-file options (not used), #3226\n\nNew features:\n\n- bash, zsh and fish shell auto-completions, see scripts/shell_completions/\n- added BORG_CONFIG_DIR env var, #3083\n\nOther changes:\n\n- docs:\n\n  - clarify using a blank passphrase in keyfile mode\n  - mention \"!\" (exclude-norecurse) type in \"patterns\" help\n  - document to first heal before running borg recreate to re-chunk stuff,\n    because that will have to get rid of chunks_healthy metadata.\n  - more than 23 is not supported for CHUNK_MAX_EXP, #3115\n  - borg does not respect nodump flag by default any more\n  - clarify same-filesystem requirement for borg upgrade, #2083\n  - update / rephrase cygwin / WSL status, #3174\n  - improve docs about --stats, #3260\n- vagrant: openindiana new clang package\n\nAlready contained in 1.1.1 (last minute fix):\n\n- arg parsing: fix fallback function, refactor, #3205. This is a fixup\n  for #3155, which was broken on at least python <= 3.4.2.\n\n\nVersion 1.1.1 (2017-10-22)\n--------------------------\n\nCompatibility notes:\n\n- The deprecated --no-files-cache is not a global/common option any more,\n  but only available for borg create (it is not needed for anything else).\n  Use --files-cache=disabled instead of --no-files-cache.\n- The nodump flag (\"do not back up this file\") is not honoured any more by\n  default because this functionality (esp. if it happened by error or\n  unexpected) was rather confusing and unexplainable at first to users.\n  If you want that \"do not back up NODUMP-flagged files\" behaviour, use:\n  borg create --exclude-nodump ...\n- If you are on Linux and do not need bsdflags archived, consider using\n  ``--nobsdflags`` with ``borg create`` to avoid additional syscalls and\n  speed up backup creation.\n\nFixes:\n\n- borg recreate: correctly compute part file sizes. fixes cosmetic, but\n  annoying issue as borg check complains about size inconsistencies of part\n  files in affected archives. you can solve that by running borg recreate on\n  these archives, see also #3157.\n- bsdflags support: do not open BLK/CHR/LNK files, avoid crashes and\n  slowness, #3130\n- recreate: don't crash on attic archives w/o time_end, #3109\n- don't crash on repository filesystems w/o hardlink support, #3107\n- don't crash in first part of truncate_and_unlink, #3117\n- fix server-side IndexError crash with clients < 1.0.7, #3192\n- don't show traceback if only a global option is given, show help, #3142\n- cache: use SaveFile for more safety, #3158\n- init: fix wrong encryption choices in command line parser, fix missing\n  \"authenticated-blake2\", #3103\n- move --no-files-cache from common to borg create options, #3146\n- fix detection of non-local path (failed on ..filename), #3108\n- logging with fileConfig: set json attr on \"borg\" logger, #3114\n- fix crash with relative BORG_KEY_FILE, #3197\n- show excluded dir with \"x\" for tagged dirs / caches, #3189\n\nNew features:\n\n- create: --nobsdflags and --exclude-nodump options, #3160\n- extract: --nobsdflags option, #3160\n\nOther changes:\n\n- remove annoying hardlinked symlinks warning, #3175\n- vagrant: use self-made FreeBSD 10.3 box, #3022\n- travis: don't brew update, hopefully fixes #2532\n- docs:\n\n  - readme: -e option is required in borg 1.1\n  - add example showing --show-version --show-rc\n  - use --format rather than --list-format (deprecated) in example\n  - update docs about hardlinked symlinks limitation\n\n\nVersion 1.1.0 (2017-10-07)\n--------------------------\n\nCompatibility notes:\n\n- borg command line: do not put options in between positional arguments\n\n  This sometimes works (e.g. it worked in borg 1.0.x), but can easily stop\n  working if we make positional arguments optional (like it happened for\n  borg create's \"paths\" argument in 1.1). There are also places in borg 1.0\n  where we do that, so it doesn't work there in general either. #3356\n\n  Good: borg create -v --stats repo::archive path\n  Good: borg create repo::archive path -v --stats\n  Bad:  borg create repo::archive -v --stats path\n\nFixes:\n\n- fix LD_LIBRARY_PATH restoration for subprocesses, #3077\n- \"auto\" compression: make sure expensive compression is actually better,\n  otherwise store lz4 compressed data we already computed.\n\nOther changes:\n\n- docs:\n\n  - FAQ: we do not implement futile attempts of ETA / progress displays\n  - manpage: fix typos, update homepage\n  - implement simple \"issue\" role for manpage generation, #3075\n\n\nVersion 1.1.0rc4 (2017-10-01)\n-----------------------------\n\nCompatibility notes:\n\n- A borg server >= 1.1.0rc4 does not support borg clients 1.1.0b3-b5. #3033\n- The files cache is now controlled differently and has a new default mode:\n\n  - the files cache now uses ctime by default for improved file change\n    detection safety. You can still use mtime for more speed and less safety.\n  - --ignore-inode is deprecated (use --files-cache=... without \"inode\")\n  - --no-files-cache is deprecated (use --files-cache=disabled)\n\nNew features:\n\n- --files-cache - implement files cache mode control, #911\n  You can now control the files cache mode using this option:\n  --files-cache={ctime,mtime,size,inode,rechunk,disabled}\n  (only some combinations are supported). See the docs for details.\n\nFixes:\n\n- remote progress/logging: deal with partial lines, #2637\n- remote progress: flush json mode output\n- fix subprocess environments, #3050 (and more)\n\nOther changes:\n\n- remove client_supports_log_v3 flag, #3033\n- exclude broken Cython 0.27(.0) in requirements, #3066\n- vagrant:\n\n  - upgrade to FUSE for macOS 3.7.1\n  - use Python 3.5.4 to build the binaries\n- docs:\n\n  - security: change-passphrase only changes the passphrase, #2990\n  - fixed/improved borg create --compression examples, #3034\n  - add note about metadata dedup and --no[ac]time, #2518\n  - twitter account @borgbackup now, better visible, #2948\n  - simplified rate limiting wrapper in FAQ\n\n\nVersion 1.1.0rc3 (2017-09-10)\n-----------------------------\n\nNew features:\n\n- delete: support naming multiple archives, #2958\n\nFixes:\n\n- repo cleanup/write: invalidate cached FDs, #2982\n- fix datetime.isoformat() microseconds issues, #2994\n- recover_segment: use mmap(), lower memory needs, #2987\n\nOther changes:\n\n- with-lock: close segment file before invoking subprocess\n- keymanager: don't depend on optional readline module, #2976\n- docs:\n\n  - fix macOS keychain integration command\n  - show/link new screencasts in README, #2936\n  - document utf-8 locale requirement for json mode, #2273\n- vagrant: clean up shell profile init, user name, #2977\n- test_detect_attic_repo: don't test mount, #2975\n- add debug logging for repository cleanup\n\n\nVersion 1.1.0rc2 (2017-08-28)\n-----------------------------\n\nCompatibility notes:\n\n- list: corrected mix-up of \"isomtime\" and \"mtime\" formats. Previously,\n  \"isomtime\" was the default but produced a verbose human format,\n  while \"mtime\" produced a ISO-8601-like format.\n  The behaviours have been swapped (so \"mtime\" is human, \"isomtime\" is ISO-like),\n  and the default is now \"mtime\".\n  \"isomtime\" is now a real ISO-8601 format (\"T\" between date and time, not a space).\n\nNew features:\n\n- None.\n\nFixes:\n\n- list: fix weird mixup of mtime/isomtime\n- create --timestamp: set start time, #2957\n- ignore corrupt files cache, #2939\n- migrate locks to child PID when daemonize is used\n- fix exitcode of borg serve, #2910\n- only compare contents when chunker params match, #2899\n- umount: try fusermount, then try umount, #2863\n\nOther changes:\n\n- JSON: use a more standard ISO 8601 datetime format, #2376\n- cache: write_archive_index: truncate_and_unlink on error, #2628\n- detect non-upgraded Attic repositories, #1933\n- delete various nogil and threading related lines\n- coala / pylint related improvements\n- docs:\n\n  - renew asciinema/screencasts, #669\n  - create: document exclusion through nodump, #2949\n  - minor formatting fixes\n  - tar: tarpipe example\n  - improve \"with-lock\" and \"info\" docs, #2869\n  - detail how to use macOS/GNOME/KDE keyrings for repo passwords, #392\n- travis: only short-circuit docs-only changes for pull requests\n- vagrant:\n\n  - netbsd: bash is already installed\n  - fix netbsd version in PKG_PATH\n  - add exe location to PATH when we build an exe\n\n\nVersion 1.1.0rc1 (2017-07-24)\n-----------------------------\n\nCompatibility notes:\n\n- delete: removed short option for --cache-only\n\nNew features:\n\n- support borg list repo --format {comment} {bcomment} {end}, #2081\n- key import: allow reading from stdin, #2760\n\nFixes:\n\n- with-lock: avoid creating segment files that might be overwritten later, #1867\n- prune: fix checkpoints processing with --glob-archives\n- FUSE: versions view: keep original file extension at end, #2769\n- fix --last, --first: do not accept values <= 0,\n  fix reversed archive ordering with --last\n- include testsuite data (attic.tar.gz) when installing the package\n- use limited unpacker for outer key, for manifest (both security precautions),\n  #2174 #2175\n- fix bashism in shell scripts, #2820, #2816\n- cleanup endianness detection, create _endian.h,\n  fixes build on alpine linux, #2809\n- fix crash with --no-cache-sync (give known chunk size to chunk_incref), #2853\n\nOther changes:\n\n- FUSE: versions view: linear numbering by archive time\n- split up interval parsing from filtering for --keep-within, #2610\n- add a basic .editorconfig, #2734\n- use archive creation time as mtime for FUSE mount, #2834\n- upgrade FUSE for macOS (osxfuse) from 3.5.8 to 3.6.3, #2706\n- hashindex: speed up by replacing modulo with \"if\" to check for wraparound\n- coala checker / pylint: fixed requirements and .coafile, more ignores\n- borg upgrade: name backup directories as 'before-upgrade', #2811\n- add .mailmap\n- some minor changes suggested by lgtm.com\n- docs:\n\n  - better explanation of the --ignore-inode option relevance, #2800\n  - fix openSUSE command and add openSUSE section\n  - simplify ssh authorized_keys file using \"restrict\", add legacy note, #2121\n  - mount: show usage of archive filters\n  - mount: add repository example, #2462\n  - info: update and add examples, #2765\n  - prune: include example\n  - improved style / formatting\n  - improved/fixed segments_per_dir docs\n  - recreate: fix wrong \"remove unwanted files\" example\n  - reference list of status chars in borg recreate --filter description\n  - update source-install docs about doc build dependencies, #2795\n  - cleanup installation docs\n  - file system requirements, update segs per dir\n  - fix checkpoints/parts reference in FAQ, #2859\n- code:\n\n  - hashindex: don't pass side effect into macro\n  - crypto low_level: don't mutate local bytes()\n  - use dash_open function to open file or \"-\" for stdin/stdout\n  - archiver: argparse cleanup / refactoring\n  - shellpattern: add match_end arg\n- tests: added some additional unit tests, some fixes, #2700 #2710\n- vagrant: fix setup of cygwin, add Debian 9 \"stretch\"\n- travis: don't perform full travis build on docs-only changes, #2531\n\n\nVersion 1.1.0b6 (2017-06-18)\n----------------------------\n\nCompatibility notes:\n\n- Running \"borg init\" via a \"borg serve --append-only\" server will *not* create\n  an append-only repository anymore. Use \"borg init --append-only\" to initialize\n  an append-only repository.\n\n- Repositories in the \"repokey\" and \"repokey-blake2\" modes with an empty passphrase\n  are now treated as unencrypted repositories for security checks (e.g.\n  BORG_UNKNOWN_UNENCRYPTED_REPO_ACCESS_IS_OK).\n\n  Previously there would be no prompts nor messages if an unknown repository\n  in one of these modes with an empty passphrase was encountered. This would\n  allow an attacker to swap a repository, if one assumed that the lack of\n  password prompts was due to a set BORG_PASSPHRASE.\n\n  Since the \"trick\" does not work if BORG_PASSPHRASE is set, this does generally\n  not affect scripts.\n\n- Repositories in the \"authenticated\" mode are now treated as the unencrypted\n  repositories they are.\n\n- The client-side temporary repository cache now holds unencrypted data for better speed.\n\n- borg init: removed the short form of --append-only (-a).\n\n- borg upgrade: removed the short form of --inplace (-i).\n\nNew features:\n\n- reimplemented the RepositoryCache, size-limited caching of decrypted repo\n  contents, integrity checked via xxh64. #2515\n- reduced space usage of chunks.archive.d. Existing caches are migrated during\n  a cache sync. #235 #2638\n- integrity checking using xxh64 for important files used by borg, #1101:\n\n  - repository: index and hints files\n  - cache: chunks and files caches, chunks.archive.d\n- improve cache sync speed, #1729\n- create: new --no-cache-sync option\n- add repository mandatory feature flags infrastructure, #1806\n- Verify most operations against SecurityManager. Location, manifest timestamp\n  and key types are now checked for almost all non-debug commands. #2487\n- implement storage quotas, #2517\n- serve: add --restrict-to-repository, #2589\n- BORG_PASSCOMMAND: use external tool providing the key passphrase, #2573\n- borg export-tar, #2519\n- list: --json-lines instead of --json for archive contents, #2439\n- add --debug-profile option (and also \"borg debug convert-profile\"), #2473\n- implement --glob-archives/-a, #2448\n- normalize authenticated key modes for better naming consistency:\n\n  - rename \"authenticated\" to \"authenticated-blake2\" (uses blake2b)\n  - implement \"authenticated\" mode (uses hmac-sha256)\n\nFixes:\n\n- hashindex: read/write indices >2 GiB on 32bit systems, better error\n  reporting, #2496\n- repository URLs: implement IPv6 address support and also more informative\n  error message when parsing fails.\n- mount: check whether llfuse is installed before asking for passphrase, #2540\n- mount: do pre-mount checks before opening repository, #2541\n- FUSE:\n\n  - fix crash if empty (None) xattr is read, #2534\n  - fix read(2) caching data in metadata cache\n  - fix negative uid/gid crash (fix crash when mounting archives\n    of external drives made on cygwin), #2674\n  - redo ItemCache, on top of object cache\n  - use decrypted cache\n  - remove unnecessary normpaths\n- serve: ignore --append-only when initializing a repository (borg init), #2501\n- serve: fix incorrect type of exception_short for Errors, #2513\n- fix --exclude and --exclude-from recursing into directories, #2469\n- init: don't allow creating nested repositories, #2563\n- --json: fix encryption[mode] not being the cmdline name\n- remote: propagate Error.traceback correctly\n- fix remote logging and progress, #2241\n\n  - implement --debug-topic for remote servers\n  - remote: restore \"Remote:\" prefix (as used in 1.0.x)\n  - rpc negotiate: enable v3 log protocol only for supported clients\n  - fix --progress and logging in general for remote\n- fix parse_version, add tests, #2556\n- repository: truncate segments (and also some other files) before unlinking, #2557\n- recreate: keep timestamps as in original archive, #2384\n- recreate: if single archive is not processed, exit 2\n- patterns: don't recurse with ! / --exclude for pf:, #2509\n- cache sync: fix n^2 behaviour in lookup_name\n- extract: don't write to disk with --stdout (affected non-regular-file items), #2645\n- hashindex: implement KeyError, more tests\n\nOther changes:\n\n- remote: show path in PathNotAllowed\n- consider repokey w/o passphrase == unencrypted, #2169\n- consider authenticated mode == unencrypted, #2503\n- restrict key file names, #2560\n- document follow_symlinks requirements, check libc, use stat and chown\n  with follow_symlinks=False, #2507\n- support common options on the main command, #2508\n- support common options on mid-level commands (e.g. borg *key* export)\n- make --progress a common option\n- increase DEFAULT_SEGMENTS_PER_DIR to 1000\n- chunker: fix invalid use of types (function only used by tests)\n- chunker: don't do uint32_t >> 32\n- FUSE:\n\n  - add instrumentation (--debug and SIGUSR1/SIGINFO)\n  - reduced memory usage for repository mounts by lazily instantiating archives\n  - improved archive load times\n- info: use CacheSynchronizer & HashIndex.stats_against (better performance)\n- docs:\n\n  - init: document --encryption as required\n  - security: OpenSSL usage\n  - security: used implementations; note python libraries\n  - security: security track record of OpenSSL and msgpack\n  - patterns: document denial of service (regex, wildcards)\n  - init: note possible denial of service with \"none\" mode\n  - init: document SHA extension is supported in OpenSSL and thus SHA is\n    faster on AMD Ryzen than blake2b.\n  - book: use A4 format, new builder option format.\n  - book: create appendices\n  - data structures: explain repository compaction\n  - data structures: add chunk layout diagram\n  - data structures: integrity checking\n  - data structures: demingle cache and repo index\n  - Attic FAQ: separate section for attic stuff\n  - FAQ: I get an IntegrityError or similar - what now?\n  - FAQ: Can I use Borg on SMR hard drives?, #2252\n  - FAQ: specify \"using inline shell scripts\"\n  - add systemd warning regarding placeholders, #2543\n  - xattr: document API\n  - add docs/misc/borg-data-flow data flow chart\n  - debugging facilities\n  - README: how to help the project, #2550\n  - README: add bountysource badge, #2558\n  - fresh new theme + tweaking\n  - logo: vectorized (PDF and SVG) versions\n  - frontends: use headlines - you can link to them\n  - mark --pattern, --patterns-from as experimental\n  - highlight experimental features in online docs\n  - remove regex based pattern examples, #2458\n  - nanorst for \"borg help TOPIC\" and --help\n  - split deployment\n  - deployment: hosting repositories\n  - deployment: automated backups to a local hard drive\n  - development: vagrant, windows10 requirements\n  - development: update docs remarks\n  - split usage docs, #2627\n  - usage: avoid bash highlight, [options] instead of <options>\n  - usage: add benchmark page\n  - helpers: truncate_and_unlink doc\n  - don't suggest to leak BORG_PASSPHRASE\n  - internals: columnize rather long ToC [webkit fixup]\n    internals: manifest & feature flags\n  - internals: more HashIndex details\n  - internals: fix ASCII art equations\n  - internals: edited obj graph related sections a bit\n  - internals: layers image + description\n  - fix way too small figures in pdf\n  - index: disable syntax highlight (bash)\n  - improve options formatting, fix accidental block quotes\n\n- testing / checking:\n\n  - add support for using coala, #1366\n  - testsuite: add ArchiverCorruptionTestCase\n  - do not test logger name, #2504\n  - call setup_logging after destroying logging config\n  - testsuite.archiver: normalise pytest.raises vs. assert_raises\n  - add test for preserved intermediate folder permissions, #2477\n  - key: add round-trip test\n  - remove attic dependency of the tests, #2505\n  - enable remote tests on cygwin\n  - tests: suppress tar's future timestamp warning\n  - cache sync: add more refcount tests\n  - repository: add tests, including corruption tests\n\n- vagrant:\n\n  - control VM cpus and pytest workers via env vars VMCPUS and XDISTN\n  - update cleaning workdir\n  - fix openbsd shell\n  - add OpenIndiana\n\n- packaging:\n\n  - binaries: don't bundle libssl\n  - setup.py clean to remove compiled files\n  - fail in borg package if version metadata is very broken (setuptools_scm)\n\n- repo / code structure:\n\n  - create borg.algorithms and borg.crypto packages\n  - algorithms: rename crc32 to checksums\n  - move patterns to module, #2469\n  - gitignore: complete paths for src/ excludes\n  - cache: extract CacheConfig class\n  - implement IntegrityCheckedFile + Detached variant, #2502 #1688\n  - introduce popen_with_error_handling to handle common user errors\n\n\nVersion 1.1.0b5 (2017-04-30)\n----------------------------\n\nCompatibility notes:\n\n- BORG_HOSTNAME_IS_UNIQUE is now on by default.\n- removed --compression-from feature\n- recreate: add --recompress flag, unify --always-recompress and\n  --recompress\n\nFixes:\n\n- catch exception for os.link when hardlinks are not supported, #2405\n- borg rename / recreate: expand placeholders, #2386\n- generic support for hardlinks (files, devices, FIFOs), #2324\n- extract: also create parent dir for device files, if needed, #2358\n- extract: if a hardlink master is not in the to-be-extracted subset,\n  the \"x\" status was not displayed for it, #2351\n- embrace y2038 issue to support 32bit platforms: clamp timestamps to int32,\n  #2347\n- verify_data: fix IntegrityError handling for defect chunks, #2442\n- allow excluding parent and including child, #2314\n\nOther changes:\n\n- refactor compression decision stuff\n- change global compression default to lz4 as well, to be consistent\n  with --compression defaults.\n- placeholders: deny access to internals and other unspecified stuff\n- clearer error message for unrecognized placeholder\n- more clear exception if borg check does not help, #2427\n- vagrant: upgrade FUSE for macOS to 3.5.8, #2346\n- linux binary builds: get rid of glibc 2.13 dependency, #2430\n- docs:\n\n  - placeholders: document escaping\n  - serve: env vars in original commands are ignored\n  - tell what kind of hardlinks we support\n  - more docs about compression\n  - LICENSE: use canonical formulation\n    (\"copyright holders and contributors\" instead of \"author\")\n  - document borg init behaviour via append-only borg serve, #2440\n  - be clear about what buzhash is used for, #2390\n  - add hint about chunker params, #2421\n  - clarify borg upgrade docs, #2436\n  - FAQ to explain warning when running borg check --repair, #2341\n  - repository file system requirements, #2080\n  - pre-install considerations\n  - misc. formatting / crossref fixes\n- tests:\n\n  - enhance travis setuptools_scm situation\n  - add extra test for the hashindex\n  - fix invalid param issue in benchmarks\n\nThese belong to 1.1.0b4 release, but did not make it into changelog by then:\n\n- vagrant: increase memory for parallel testing\n- lz4 compress: lower max. buffer size, exception handling\n- add docstring to do_benchmark_crud\n- patterns help: mention path full-match in intro\n\n\nVersion 1.1.0b4 (2017-03-27)\n----------------------------\n\nCompatibility notes:\n\n- init: the --encryption argument is mandatory now (there are several choices)\n- moved \"borg migrate-to-repokey\" to \"borg key migrate-to-repokey\".\n- \"borg change-passphrase\" is deprecated, use \"borg key change-passphrase\"\n  instead.\n- the --exclude-if-present option now supports tagging a folder with any\n  filesystem object type (file, folder, etc), instead of expecting only files\n  as tags, #1999\n- the --keep-tag-files option has been deprecated in favor of the new\n  --keep-exclude-tags, to account for the change mentioned above.\n- use lz4 compression by default, #2179\n\nNew features:\n\n- JSON API to make developing frontends and automation easier\n  (see :ref:`json_output`)\n\n  - add JSON output to commands: `borg create/list/info --json ...`.\n  - add --log-json option for structured logging output.\n  - add JSON progress information, JSON support for confirmations (yes()).\n- add two new options --pattern and --patterns-from as discussed in #1406\n- new path full match pattern style (pf:) for very fast matching, #2334\n- add 'debug dump-manifest' and 'debug dump-archive' commands\n- add 'borg benchmark crud' command, #1788\n- new 'borg delete --force --force' to delete severely corrupted archives, #1975\n- info: show utilization of maximum archive size, #1452\n- list: add dsize and dcsize keys, #2164\n- paperkey.html: Add interactive html template for printing key backups.\n- key export: add qr html export mode\n- securely erase config file (which might have old encryption key), #2257\n- archived file items: add size to metadata, 'borg extract' and 'borg check' do\n  check the file size for consistency, FUSE uses precomputed size from Item.\n\nFixes:\n\n- fix remote speed regression introduced in 1.1.0b3, #2185\n- fix regression handling timestamps beyond 2262 (revert bigint removal),\n  introduced in 1.1.0b3, #2321\n- clamp (nano)second values to unproblematic range, #2304\n- hashindex: rebuild hashtable if we have too little empty buckets\n  (performance fix), #2246\n- Location regex: fix bad parsing of wrong syntax\n- ignore posix_fadvise errors in repository.py, #2095\n- borg rpc: use limited msgpack.Unpacker (security precaution), #2139\n- Manifest: Make sure manifest timestamp is strictly monotonically increasing.\n- create: handle BackupOSError on a per-path level in one spot\n- create: clarify -x option / meaning of \"same filesystem\"\n- create: don't create hard link refs to failed files\n- archive check: detect and fix missing all-zero replacement chunks, #2180\n- files cache: update inode number when --ignore-inode is used, #2226\n- fix decompression exceptions crashing ``check --verify-data`` and others\n  instead of reporting integrity error, #2224 #2221\n- extract: warning for unextracted big extended attributes, #2258, #2161\n- mount: umount on SIGINT/^C when in foreground\n- mount: handle invalid hard link refs\n- mount: fix huge RAM consumption when mounting a repository (saves number of\n  archives * 8 MiB), #2308\n- hashindex: detect mingw byte order #2073\n- hashindex: fix wrong skip_hint on hashindex_set when encountering tombstones,\n  the regression was introduced in #1748\n- fix ChunkIndex.__contains__ assertion  for big-endian archs\n- fix borg key/debug/benchmark crashing without subcommand, #2240\n- Location: accept //servername/share/path\n- correct/refactor calculation of unique/non-unique chunks\n- extract: fix missing call to ProgressIndicator.finish\n- prune: fix error msg, it is --keep-within, not --within\n- fix \"auto\" compression mode bug (not compressing), #2331\n- fix symlink item fs size computation, #2344\n\nOther changes:\n\n- remote repository: improved async exception processing, #2255 #2225\n- with --compression auto,C, only use C if lz4 achieves at least 3% compression\n- PatternMatcher: only normalize path once, #2338\n- hashindex: separate endian-dependent defs from endian detection\n- migrate-to-repokey: ask using canonical_path() as we do everywhere else.\n- SyncFile: fix use of fd object after close\n- make LoggedIO.close_segment reentrant\n- creating a new segment: use \"xb\" mode, #2099\n- redo key_creator, key_factory, centralise key knowledge, #2272\n- add return code functions, #2199\n- list: only load cache if needed\n- list: files->items, clarifications\n- list: add \"name\" key for consistency with info cmd\n- ArchiveFormatter: add \"start\" key for compatibility with \"info\"\n- RemoteRepository: account rx/tx bytes\n- setup.py build_usage/build_man/build_api fixes\n- Manifest.in: simplify, exclude .so, .dll and .orig, #2066\n- FUSE: get rid of chunk accounting, st_blocks = ceil(size / blocksize).\n- tests:\n\n  - help python development by testing 3.6-dev\n  - test for borg delete --force\n- vagrant:\n\n  - freebsd: some fixes, #2067\n  - darwin64: use osxfuse 3.5.4 for tests / to build binaries\n  - darwin64: improve VM settings\n  - use python 3.5.3 to build binaries, #2078\n  - upgrade pyinstaller from 3.1.1+ to 3.2.1\n  - pyinstaller: use fixed AND freshly compiled bootloader, #2002\n  - pyinstaller: automatically builds bootloader if missing\n- docs:\n\n  - create really nice man pages\n  - faq: mention --remote-ratelimit in bandwidth limit question\n  - fix caskroom link, #2299\n  - docs/security: reiterate that RPC in Borg does no networking\n  - docs/security: counter tracking, #2266\n  - docs/development: update merge remarks\n  - address SSH batch mode in docs, #2202 #2270\n  - add warning about running build_usage on Python >3.4, #2123\n  - one link per distro in the installation page\n  - improve --exclude-if-present and --keep-exclude-tags, #2268\n  - improve automated backup script in doc, #2214\n  - improve remote-path description\n  - update docs for create -C default change (lz4)\n  - document relative path usage, #1868\n  - document snapshot usage, #2178\n  - corrected some stuff in internals+security\n  - internals: move toctree to after the introduction text\n  - clarify metadata kind, manifest ops\n  - key enc: correct / clarify some stuff, link to internals/security\n  - datas: enc: 1.1.x mas different MACs\n  - datas: enc: correct factual error -- no nonce involved there.\n  - make internals.rst an index page and edit it a bit\n  - add \"Cryptography in Borg\" and \"Remote RPC protocol security\" sections\n  - document BORG_HOSTNAME_IS_UNIQUE, #2087\n  - FAQ by categories as proposed by @anarcat in #1802\n  - FAQ: update Which file types, attributes, etc. are *not* preserved?\n  - development: new branching model for git repository\n  - development: define \"ours\" merge strategy for auto-generated files\n  - create: move --exclude note to main doc\n  - create: move item flags to main doc\n  - fix examples using borg init without -e/--encryption\n  - list: don't print key listings in fat (html + man)\n  - remove Python API docs (were very incomplete, build problems on RTFD)\n  - added FAQ section about backing up root partition\n\n\nVersion 1.1.0b3 (2017-01-15)\n----------------------------\n\nCompatibility notes:\n\n- borg init: removed the default of \"--encryption/-e\", #1979\n  This was done so users do a informed decision about -e mode.\n\nBug fixes:\n\n- borg recreate: don't rechunkify unless explicitly told so\n- borg info: fixed bug when called without arguments, #1914\n- borg init: fix free space check crashing if disk is full, #1821\n- borg debug delete/get obj: fix wrong reference to exception\n- fix processing of remote ~/ and ~user/ paths (regressed since 1.1.0b1), #1759\n- posix platform module: only build / import on non-win32 platforms, #2041\n\nNew features:\n\n- new CRC32 implementations that are much faster than the zlib one used previously, #1970\n- add blake2b key modes (use blake2b as MAC). This links against system libb2,\n  if possible, otherwise uses bundled code\n- automatically remove stale locks - set BORG_HOSTNAME_IS_UNIQUE env var\n  to enable stale lock killing. If set, stale locks in both cache and\n  repository are deleted. #562 #1253\n- borg info <repo>: print general repo information, #1680\n- borg check --first / --last / --sort / --prefix, #1663\n- borg mount --first / --last / --sort / --prefix, #1542\n- implement \"health\" item formatter key, #1749\n- BORG_SECURITY_DIR to remember security related infos outside the cache.\n  Key type, location and manifest timestamp checks now survive cache\n  deletion. This also means that you can now delete your cache and avoid\n  previous warnings, since Borg can still tell it's safe.\n- implement BORG_NEW_PASSPHRASE, #1768\n\nOther changes:\n\n- borg recreate:\n\n  - remove special-cased --dry-run\n  - update --help\n  - remove bloat: interruption blah, autocommit blah, resuming blah\n  - re-use existing checkpoint functionality\n  - archiver tests: add check_cache tool - lints refcounts\n\n- fixed cache sync performance regression from 1.1.0b1 onwards, #1940\n- syncing the cache without chunks.archive.d (see :ref:`disable_archive_chunks`)\n  now avoids any merges and is thus faster, #1940\n- borg check --verify-data: faster due to linear on-disk-order scan\n- borg debug-xxx commands removed, we use \"debug xxx\" subcommands now, #1627\n- improve metadata handling speed\n- shortcut hashindex_set by having hashindex_lookup hint about address\n- improve / add progress displays, #1721\n- check for index vs. segment files object count mismatch\n- make RPC protocol more extensible: use named parameters.\n- RemoteRepository: misc. code cleanups / refactors\n- clarify cache/repository README file\n\n- docs:\n\n  - quickstart: add a comment about other (remote) filesystems\n  - quickstart: only give one possible ssh url syntax, all others are\n    documented in usage chapter.\n  - mention file://\n  - document repo URLs / archive location\n  - clarify borg diff help, #980\n  - deployment: synthesize alternative --restrict-to-path example\n  - improve cache / index docs, esp. files cache docs, #1825\n  - document using \"git merge 1.0-maint -s recursive -X rename-threshold=20%\"\n    for avoiding troubles when merging the 1.0-maint branch into master.\n\n- tests:\n\n  - FUSE tests: catch ENOTSUP on freebsd\n  - FUSE tests: test troublesome xattrs last\n  - fix byte range error in test, #1740\n  - use monkeypatch to set env vars, but only on pytest based tests.\n  - point XDG_*_HOME to temp dirs for tests, #1714\n  - remove all BORG_* env vars from the outer environment\n\n\nVersion 1.1.0b2 (2016-10-01)\n----------------------------\n\nBug fixes:\n\n- fix incorrect preservation of delete tags, leading to \"object count mismatch\"\n  on borg check, #1598. This only occurred with 1.1.0b1 (not with 1.0.x) and is\n  normally fixed by running another borg create/delete/prune.\n- fix broken --progress for double-cell paths (e.g. CJK), #1624\n- borg recreate: also catch SIGHUP\n- FUSE:\n\n  - fix hardlinks in versions view, #1599\n  - add parameter check to ItemCache.get to make potential failures more clear\n\nNew features:\n\n- Archiver, RemoteRepository: add --remote-ratelimit (send data)\n- borg help compression, #1582\n- borg check: delete chunks with integrity errors, #1575, so they can be\n  \"repaired\" immediately and maybe healed later.\n- archives filters concept (refactoring/unifying older code)\n\n  - covers --first/--last/--prefix/--sort-by options\n  - currently used for borg list/info/delete\n\nOther changes:\n\n- borg check --verify-data slightly tuned (use get_many())\n- change {utcnow} and {now} to ISO-8601 format (\"T\" date/time separator)\n- repo check: log transaction IDs, improve object count mismatch diagnostic\n- Vagrantfile: use TW's fresh-bootloader pyinstaller branch\n- fix module names in api.rst\n- hashindex: bump api_version\n\n\nVersion 1.1.0b1 (2016-08-28)\n----------------------------\n\nNew features:\n\n- new commands:\n\n  - borg recreate: re-create existing archives, #787 #686 #630 #70, also see\n    #757, #770.\n\n    - selectively remove files/dirs from old archives\n    - re-compress data\n    - re-chunkify data, e.g. to have upgraded Attic / Borg 0.xx archives\n      deduplicate with Borg 1.x archives or to experiment with chunker-params.\n  - borg diff: show differences between archives\n  - borg with-lock: execute a command with the repository locked, #990\n- borg create:\n\n  - Flexible compression with pattern matching on path/filename,\n    and LZ4 heuristic for deciding compressibility, #810, #1007\n  - visit files in inode order (better speed, esp. for large directories and rotating disks)\n  - in-file checkpoints, #1217\n  - increased default checkpoint interval to 30 minutes (was 5 minutes), #896\n  - added uuid archive format tag, #1151\n  - save mountpoint directories with --one-file-system, makes system restore easier, #1033\n  - Linux: added support for some BSD flags, #1050\n  - add 'x' status for excluded paths, #814\n\n    - also means files excluded via UF_NODUMP, #1080\n- borg check:\n\n  - will not produce the \"Checking segments\" output unless new --progress option is passed, #824.\n  - --verify-data to verify data cryptographically on the client, #975\n- borg list, #751, #1179\n\n  - removed {formatkeys}, see \"borg list --help\"\n  - --list-format is deprecated, use --format instead\n  - --format now also applies to listing archives, not only archive contents, #1179\n  - now supports the usual [PATH [PATHS\u2026]] syntax and excludes\n  - new keys: csize, num_chunks, unique_chunks, NUL\n  - supports guaranteed_available hashlib hashes\n    (to avoid varying functionality depending on environment),\n    which includes the SHA1 and SHA2 family as well as MD5\n- borg prune:\n\n  - to visualize the \"thinning out\" better, we now list all archives in\n    reverse time order. rephrase and reorder help text.\n  - implement --keep-last N via --keep-secondly N, also --keep-minutely.\n    assuming that there is not more than 1 backup archive made in 1s,\n    --keep-last N and --keep-secondly N are equivalent, #537\n  - cleanup checkpoints except the latest, #1008\n- borg extract:\n\n  - added --progress, #1449\n  - Linux: limited support for BSD flags, #1050\n- borg info:\n\n  - output is now more similar to borg create --stats, #977\n- borg mount:\n\n  - provide \"borgfs\" wrapper for borg mount, enables usage via fstab, #743\n  - \"versions\" mount option - when used with a repository mount, this gives\n    a merged, versioned view of the files in all archives, #729\n- repository:\n\n  - added progress information to commit/compaction phase (often takes some time when deleting/pruning), #1519\n  - automatic recovery for some forms of repository inconsistency, #858\n  - check free space before going forward with a commit, #1336\n  - improved write performance (esp. for rotating media), #985\n\n    - new IO code for Linux\n    - raised default segment size to approx 512 MiB\n  - improved compaction performance, #1041\n  - reduced client CPU load and improved performance for remote repositories, #940\n\n- options that imply output (--show-rc, --show-version, --list, --stats,\n  --progress) don't need -v/--info to have that output displayed, #865\n- add archive comments (via borg (re)create --comment), #842\n- borg list/prune/delete: also output archive id, #731\n- --show-version: shows/logs the borg version, #725\n- added --debug-topic for granular debug logging, #1447\n- use atomic file writing/updating for configuration and key files, #1377\n- BORG_KEY_FILE environment variable, #1001\n- self-testing module, #970\n\n\nBug fixes:\n\n- list: fixed default output being produced if --format is given with empty parameter, #1489\n- create: fixed overflowing progress line with CJK and similar characters, #1051\n- prune: fixed crash if --prefix resulted in no matches, #1029\n- init: clean up partial repo if passphrase input is aborted, #850\n- info: quote cmdline arguments that have spaces in them\n- fix hardlinks failing in some cases for extracting subtrees, #761\n\nOther changes:\n\n- replace stdlib hmac with OpenSSL, zero-copy decrypt (10-15% increase in\n  performance of hash-lists and extract).\n- improved chunker performance, #1021\n- open repository segment files in exclusive mode (fail-safe), #1134\n- improved error logging, #1440\n- Source:\n\n  - pass meta-data around, #765\n  - move some constants to new constants module\n  - better readability and fewer errors with namedtuples, #823\n  - moved source tree into src/ subdirectory, #1016\n  - made borg.platform a package, #1113\n  - removed dead crypto code, #1032\n  - improved and ported parts of the test suite to py.test, #912\n  - created data classes instead of passing dictionaries around, #981, #1158, #1161\n  - cleaned up imports, #1112\n- Docs:\n\n  - better help texts and sphinx reproduction of usage help:\n\n    - Group options\n    - Nicer list of options in Sphinx\n    - Deduplicate 'Common options' (including --help)\n  - chunker: added some insights by \"Voltara\", #903\n  - clarify what \"deduplicated size\" means\n  - fix / update / add package list entries\n  - added a SaltStack usage example, #956\n  - expanded FAQ\n  - new contributors in AUTHORS!\n- Tests:\n\n  - vagrant: add ubuntu/xenial 64bit - this box has still some issues\n  - ChunkBuffer: add test for leaving partial chunk in buffer, fixes #945\n\n\nVersion 1.0.13 (2019-02-15)\n---------------------------\n\nPlease note: this is very likely the last 1.0.x release, please upgrade to 1.1.x.\n\nBug fixes:\n\n- security fix: configure FUSE with \"default_permissions\", #3903.\n  \"default_permissions\" is now enforced by borg by default to let the\n  kernel check uid/gid/mode based permissions.\n  \"ignore_permissions\" can be given not to enforce \"default_permissions\".\n- xattrs: fix borg exception handling on ENOSPC error, #3808.\n\nNew features:\n\n- Read a passphrase from a file descriptor specified in the\n  BORG_PASSPHRASE_FD environment variable.\n\nOther changes:\n\n- acl platform code: fix acl set return type\n- xattr:\n\n  - add linux {list,get,set}xattr ctypes prototypes\n  - fix darwin flistxattr ctypes prototype\n- testing / travis-ci:\n\n  - fix the homebrew 1.9 issues on travis-ci, #4254\n  - travis OS X: use xcode 8.3 (not broken)\n  - tox.ini: lock requirements\n  - unbreak 1.0-maint on travis, fixes #4123\n- vagrant:\n\n  - misc. fixes\n  - FUSE for macOS: upgrade 3.7.1 to 3.8.3\n  - Python: upgrade 3.5.5 to 3.5.6\n- docs:\n\n  - Update installation instructions for macOS\n  - update release workflow using twine (docs, scripts), #4213\n\nVersion 1.0.12 (2018-04-08)\n---------------------------\n\nBug fixes:\n\n- repository: cleanup/write: invalidate cached FDs, tests\n- serve: fix exitcode, #2910\n- extract: set bsdflags last (include immutable flag), #3263\n- create --timestamp: set start time, #2957\n- create: show excluded dir with \"x\" for tagged dirs / caches, #3189\n- migrate locks to child PID when daemonize is used\n- Buffer: fix wrong thread-local storage use, #2951\n- fix detection of non-local path, #3108\n- fix LDLP restoration for subprocesses, #3077\n- fix subprocess environments (xattr module's fakeroot version check,\n  borg umount, BORG_PASSCOMMAND), #3050\n- remote: deal with partial lines, #2637\n- get rid of datetime.isoformat, use safe parse_timestamp to parse\n  timestamps, #2994\n- build: do .h file content checks in binary mode, fixes build issue for\n  non-ascii header files on pure-ascii locale platforms, #3544 #3639\n- remove platform.uname() call which caused library mismatch issues, #3732\n- add exception handler around deprecated platform.linux_distribution() call\n\nOther changes:\n\n- require msgpack-python >= 0.4.6 and < 0.5.0, see #3753\n- add parens for C preprocessor macro argument usages (did not cause\n  malfunction)\n- ignore corrupt files cache, #2939\n- replace \"modulo\" with \"if\" to check for wraparound in hashmap\n- keymanager: don't depend on optional readline module, #2980\n- exclude broken pytest 3.3.0 release\n- exclude broken Cython 0.27(.0) release, #3066\n- flake8: add some ignores\n- docs:\n\n  - create: document exclusion through nodump\n  - document good and problematic option placements, fix examples, #3356\n  - update docs about hardlinked symlinks limitation\n  - faq: we do not implement futile attempts of ETA / progress displays\n  - simplified rate limiting wrapper in FAQ\n  - twitter account @borgbackup, #2948\n  - add note about metadata dedup and --no[ac]time, #2518\n  - change-passphrase only changes the passphrase, #2990\n  - clarify encrypted key format for borg key export, #3296\n  - document sshfs rename workaround, #3315\n  - update release checklist about security fixes\n  - docs about how to verify a signed release, #3634\n  - chunk seed is generated per /repository/\n- vagrant:\n\n  - use FUSE for macOS 3.7.1 to build the macOS binary\n  - use python 3.5.5 to build the binaries\n  - add exe location to PATH when we build an exe\n  - use https pypi url for wheezy\n  - netbsd: bash is already installed\n  - netbsd: fix netbsd version in PKG_PATH\n  - use self-made FreeBSD 10.3 box, #3022\n  - backport fs_init (including related updates) from 1.1\n  - the boxcutter wheezy boxes are 404, use local ones\n- travis:\n\n  - don't perform full Travis build on docs-only changes, #2531\n  - only short-circuit docs-only changes for pull requests\n\n\nVersion 1.0.11 (2017-07-21)\n---------------------------\n\nBug fixes:\n\n- use limited unpacker for outer key (security precaution), #2174\n- fix paperkey import bug\n\nOther changes:\n\n- change --checkpoint-interval default from 600s to 1800s, #2841.\n  this improves efficiency for big repositories a lot.\n- docs: fix OpenSUSE command and add OpenSUSE section\n- tests: add tests for split_lstring and paperkey\n- vagrant:\n\n  - fix openbsd shell\n  - backport cpu/ram setup from master\n  - add stretch64 VM\n\nVersion 1.0.11rc1 (2017-06-27)\n------------------------------\n\nBug fixes:\n\n- performance: rebuild hashtable if we have too few empty buckets, #2246.\n  this fixes some sporadic, but severe performance breakdowns.\n- Archive: allocate zeros when needed, #2308\n  fixes huge memory usage of mount (8 MiB \u00d7 number of archives)\n- IPv6 address support\n  also: Location: more informative exception when parsing fails\n- borg single-file binary: use pyinstaller v3.2.1, #2396\n  this fixes that the prelink cronjob on some distros kills the\n  borg binary by stripping away parts of it.\n- extract:\n\n  - warning for unextracted big extended attributes, #2258\n  - also create parent dir for device files, if needed.\n  - don't write to disk with --stdout, #2645\n- archive check: detect and fix missing all-zero replacement chunks, #2180\n- fix (de)compression exceptions, #2224 #2221\n- files cache: update inode number, #2226\n- borg rpc: use limited msgpack.Unpacker (security precaution), #2139\n- Manifest: use limited msgpack.Unpacker (security precaution), #2175\n- Location: accept //servername/share/path\n- fix ChunkIndex.__contains__ assertion  for big-endian archs (harmless)\n- create: handle BackupOSError on a per-path level in one spot\n- fix error msg, there is no --keep-last in borg 1.0.x, #2282\n- clamp (nano)second values to unproblematic range, #2304\n- fuse / borg mount:\n\n  - fix st_blocks to be an integer (not float) value\n  - fix negative uid/gid crash (they could come into archives e.g. when\n    backing up external drives under cygwin), #2674\n  - fix crash if empty (None) xattr is read\n  - do pre-mount checks before opening repository\n  - check llfuse is installed before asking for passphrase\n- borg rename: expand placeholders, #2386\n- borg serve: fix forced command lines containing BORG_* env vars\n- fix error msg, it is --keep-within, not --within\n- fix borg key/debug/benchmark crashing without subcommand, #2240\n- chunker: fix invalid use of types, don't do uint32_t >> 32\n- document follow_symlinks requirements, check libc, #2507\n\nNew features:\n\n- added BORG_PASSCOMMAND environment variable, #2573\n- add minimal version of in repository mandatory feature flags, #2134\n\n  This should allow us to make sure older borg versions can be cleanly\n  prevented from doing operations that are no longer safe because of\n  repository format evolution. This allows more fine grained control than\n  just incrementing the manifest version. So for example a change that\n  still allows new archives to be created but would corrupt the repository\n  when an old version tries to delete an archive or check the repository\n  would add the new feature to the check and delete set but leave it out\n  of the write set.\n- borg delete --force --force to delete severely corrupted archives, #1975\n\nOther changes:\n\n- embrace y2038 issue to support 32bit platforms\n- be more clear that this is a \"beyond repair\" case, #2427\n- key file names: limit to 100 characters and remove colons from host name\n- upgrade FUSE for macOS to 3.5.8, #2346\n- split up parsing and filtering for --keep-within, better error message, #2610\n- docs:\n\n  - fix caskroom link, #2299\n  - address SSH batch mode, #2202 #2270\n  - improve remote-path description\n  - document snapshot usage, #2178\n  - document relative path usage, #1868\n  - one link per distro in the installation page\n  - development: new branching model in git repository\n  - kill api page\n  - added FAQ section about backing up root partition\n  - add bountysource badge, #2558\n  - create empty docs.txt requirements, #2694\n  - README: how to help the project\n  - note -v/--verbose requirement on affected options, #2542\n  - document borg init behaviour via append-only borg serve, #2440\n  - be clear about what buzhash is used for (chunking) and want it is not\n    used for (deduplication)- also say already in the readme that we use a\n    cryptohash for dedupe, so people don't worry, #2390\n  - add hint about chunker params to borg upgrade docs, #2421\n  - clarify borg upgrade docs, #2436\n  - quickstart: delete problematic BORG_PASSPHRASE use, #2623\n  - faq: specify \"using inline shell scripts\"\n  - document pattern denial of service, #2624\n- tests:\n\n  - remove attic dependency of the tests, #2505\n  - travis:\n\n    - enhance travis setuptools_scm situation\n    - install fakeroot for Linux\n  - add test for borg delete --force\n  - enable remote tests on cygwin (the cygwin issue that caused these tests\n    to break was fixed in cygwin at least since cygwin 2.8, maybe even since\n    2.7.0).\n  - remove skipping the noatime tests on GNU/Hurd, #2710\n  - fix borg import issue, add comment, #2718\n  - include attic.tar.gz when installing the package\n    also: add include_package_data=True\n\nVersion 1.0.10 (2017-02-13)\n---------------------------\n\nBug fixes:\n\n- Manifest timestamps are now monotonically increasing,\n  this fixes issues when the system clock jumps backwards\n  or is set inconsistently across computers accessing the same repository, #2115\n- Fixed testing regression in 1.0.10rc1 that lead to a hard dependency on\n  py.test >= 3.0, #2112\n\nNew features:\n\n- \"key export\" can now generate a printable HTML page with both a QR code and\n  a human-readable \"paperkey\" representation (and custom text) through the\n  ``--qr-html`` option.\n\n  The same functionality is also available through `paperkey.html <paperkey.html>`_,\n  which is the same HTML page generated by ``--qr-html``. It works with existing\n  \"key export\" files and key files.\n\nOther changes:\n\n- docs:\n\n  - language clarification - \"borg create --one-file-system\" option does not respect\n    mount points, but considers different file systems instead, #2141\n- setup.py: build_api: sort file list for determinism\n\n\nVersion 1.0.10rc1 (2017-01-29)\n------------------------------\n\nBug fixes:\n\n- borg serve: fix transmission data loss of pipe writes, #1268\n  This affects only the cygwin platform (not Linux, BSD, OS X).\n- Avoid triggering an ObjectiveFS bug in xattr retrieval, #1992\n- When running out of buffer memory when reading xattrs, only skip the\n  current file, #1993\n- Fixed \"borg upgrade --tam\" crashing with unencrypted repositories. Since\n  :ref:`the issue <tam_vuln>` is not relevant for unencrypted repositories,\n  it now does nothing and prints an error, #1981.\n- Fixed change-passphrase crashing with unencrypted repositories, #1978\n- Fixed \"borg check repo::archive\" indicating success if \"archive\" does not exist, #1997\n- borg check: print non-exit-code warning if --last or --prefix aren't fulfilled\n- fix bad parsing of wrong repo location syntax\n- create: don't create hard link refs to failed files,\n  mount: handle invalid hard link refs, #2092\n- detect mingw byte order, #2073\n- creating a new segment: use \"xb\" mode, #2099\n- mount: umount on SIGINT/^C when in foreground, #2082\n\nOther changes:\n\n- binary: use fixed AND freshly compiled pyinstaller bootloader, #2002\n- xattr: ignore empty names returned by llistxattr(2) et al\n- Enable the fault handler: install handlers for the SIGSEGV, SIGFPE, SIGABRT,\n  SIGBUS and SIGILL signals to dump the Python traceback.\n- Also print a traceback on SIGUSR2.\n- borg change-passphrase: print key location (simplify making a backup of it)\n- officially support Python 3.6 (setup.py: add Python 3.6 qualifier)\n- tests:\n\n  - vagrant / travis / tox: add Python 3.6 based testing\n  - vagrant: fix openbsd repo, #2042\n  - vagrant: fix the freebsd64 machine, #2037 #2067\n  - vagrant: use python 3.5.3 to build binaries, #2078\n  - vagrant: use osxfuse 3.5.4 for tests / to build binaries\n    vagrant: improve darwin64 VM settings\n  - travis: fix osxfuse install (fixes OS X testing on Travis CI)\n  - travis: require succeeding OS X tests, #2028\n  - travis: use latest pythons for OS X based testing\n  - use pytest-xdist to parallelize testing\n  - fix xattr test race condition, #2047\n  - setup.cfg: fix pytest deprecation warning, #2050\n- docs:\n\n  - language clarification - VM backup FAQ\n  - borg create: document how to back up stdin, #2013\n  - borg upgrade: fix incorrect title levels\n  - add CVE numbers for issues fixed in 1.0.9, #2106\n- fix typos (taken from Debian package patch)\n- remote: include data hexdump in \"unexpected RPC data\" error message\n- remote: log SSH command line at debug level\n- API_VERSION: use numberspaces, #2023\n- remove .github from pypi package, #2051\n- add pip and setuptools to requirements file, #2030\n- SyncFile: fix use of fd object after close (cosmetic)\n- Manifest.in: simplify, exclude \\*.{so,dll,orig}, #2066\n- ignore posix_fadvise errors in repository.py, #2095\n  (works around issues with docker on ARM)\n- make LoggedIO.close_segment reentrant, avoid reentrance\n\n\nVersion 1.0.9 (2016-12-20)\n--------------------------\n\nSecurity fixes:\n\n- A flaw in the cryptographic authentication scheme in Borg allowed an attacker\n  to spoof the manifest. See :ref:`tam_vuln` above for the steps you should\n  take.\n\n  CVE-2016-10099 was assigned to this vulnerability.\n- borg check: When rebuilding the manifest (which should only be needed very rarely)\n  duplicate archive names would be handled on a \"first come first serve\" basis,\n  potentially opening an attack vector to replace archives.\n\n  Example: were there 2 archives named \"foo\" in a repo (which can not happen\n  under normal circumstances, because borg checks if the name is already used)\n  and a \"borg check\" recreated a (previously lost) manifest, the first of the\n  archives it encountered would be in the manifest. The second archive is also\n  still in the repo, but not referenced in the manifest, in this case. If the\n  second archive is the \"correct\" one (and was previously referenced from the\n  manifest), it looks like it got replaced by the first one. In the manifest,\n  it actually got replaced. Both remain in the repo but the \"correct\" one is no\n  longer accessible via normal means - the manifest.\n\n  CVE-2016-10100 was assigned to this vulnerability.\n\nBug fixes:\n\n- borg check:\n\n  - rebuild manifest if it's corrupted\n  - skip corrupted chunks during manifest rebuild\n- fix TypeError in integrity error handler, #1903, #1894\n- fix location parser for archives with @ char (regression introduced in 1.0.8), #1930\n- fix wrong duration/timestamps if system clock jumped during a create\n- fix progress display not updating if system clock jumps backwards\n- fix checkpoint interval being incorrect if system clock jumps\n\nOther changes:\n\n- docs:\n\n  - add python3-devel as a dependency for cygwin-based installation\n  - clarify extract is relative to current directory\n  - FAQ: fix link to changelog\n  - markup fixes\n- tests:\n\n  - test_get\\_(cache|keys)_dir: clean env state, #1897\n  - get back pytest's pretty assertion failures, #1938\n- setup.py build_usage:\n\n  - fixed build_usage not processing all commands\n  - fixed build_usage not generating includes for debug commands\n\n\nVersion 1.0.9rc1 (2016-11-27)\n-----------------------------\n\nBug fixes:\n\n- files cache: fix determination of newest mtime in backup set (which is\n  used in cache cleanup and led to wrong \"A\" [added] status for unchanged\n  files in next backup), #1860.\n\n- borg check:\n\n  - fix incorrectly reporting attic 0.13 and earlier archives as corrupt\n  - handle repo w/o objects gracefully and also bail out early if repo is\n    *completely* empty, #1815.\n- fix tox/pybuild in 1.0-maint\n- at xattr module import time, loggers are not initialized yet\n\nNew features:\n\n- borg umount <mountpoint>\n  exposed already existing umount code via the CLI api, so users can use it,\n  which is more consistent than using borg to mount and fusermount -u (or\n  umount) to un-mount, #1855.\n- implement borg create --noatime --noctime, fixes #1853\n\nOther changes:\n\n- docs:\n\n  - display README correctly on PyPI\n  - improve cache / index docs, esp. files cache docs, fixes #1825\n  - different pattern matching for --exclude, #1779\n  - datetime formatting examples for {now} placeholder, #1822\n  - clarify passphrase mode attic repo upgrade, #1854\n  - clarify --umask usage, #1859\n  - clarify how to choose PR target branch\n  - clarify prune behavior for different archive contents, #1824\n  - fix PDF issues, add logo, fix authors, headings, TOC\n  - move security verification to support section\n  - fix links in standalone README (:ref: tags)\n  - add link to security contact in README\n  - add FAQ about security\n  - move fork differences to FAQ\n  - add more details about resource usage\n- tests: skip remote tests on cygwin, #1268\n- travis:\n\n  - allow OS X failures until the brew cask osxfuse issue is fixed\n  - caskroom osxfuse-beta gone, it's osxfuse now (3.5.3)\n- vagrant:\n\n  - upgrade OSXfuse / FUSE for macOS to 3.5.3\n  - remove llfuse from tox.ini at a central place\n  - do not try to install llfuse on centos6\n  - fix FUSE test for darwin, #1546\n  - add windows virtual machine with cygwin\n  - Vagrantfile cleanup / code deduplication\n\n\nVersion 1.0.8 (2016-10-29)\n--------------------------\n\nBug fixes:\n\n- RemoteRepository: Fix busy wait in call_many, #940\n\nNew features:\n\n- implement borgmajor/borgminor/borgpatch placeholders, #1694\n  {borgversion} was already there (full version string). With the new\n  placeholders you can now also get e.g. 1 or 1.0 or 1.0.8.\n\nOther changes:\n\n- avoid previous_location mismatch, #1741\n\n  due to the changed canonicalization for relative paths in PR #1711 / #1655\n  (implement /./ relpath hack), there would be a changed repo location warning\n  and the user would be asked if this is ok. this would break automation and\n  require manual intervention, which is unwanted.\n\n  thus, we automatically fix the previous_location config entry, if it only\n  changed in the expected way, but still means the same location.\n\n- docs:\n\n  - deployment.rst: do not use bare variables in ansible snippet\n  - add clarification about append-only mode, #1689\n  - setup.py: add comment about requiring llfuse, #1726\n  - update usage.rst / api.rst\n  - repo url / archive location docs + typo fix\n  - quickstart: add a comment about other (remote) filesystems\n\n- vagrant / tests:\n\n  - no chown when rsyncing (fixes boxes w/o vagrant group)\n  - fix FUSE permission issues on linux/freebsd, #1544\n  - skip FUSE test for borg binary + fakeroot\n  - ignore security.selinux xattrs, fixes tests on centos, #1735\n\n\nVersion 1.0.8rc1 (2016-10-17)\n-----------------------------\n\nBug fixes:\n\n- fix signal handling (SIGINT, SIGTERM, SIGHUP), #1620 #1593\n  Fixes e.g. leftover lock files for quickly repeated signals (e.g. Ctrl-C\n  Ctrl-C) or lost connections or systemd sending SIGHUP.\n- progress display: adapt formatting to narrow screens, do not crash, #1628\n- borg create --read-special - fix crash on broken symlink, #1584.\n  also correctly processes broken symlinks. before this regressed to a crash\n  (5b45385) a broken symlink would've been skipped.\n- process_symlink: fix missing backup_io()\n  Fixes a chmod/chown/chgrp/unlink/rename/... crash race between getting\n  dirents and dispatching to process_symlink.\n- yes(): abort on wrong answers, saying so, #1622\n- fixed exception borg serve raised when connection was closed before repository\n  was opened. Add an error message for this.\n- fix read-from-closed-FD issue, #1551\n  (this seems not to get triggered in 1.0.x, but was discovered in master)\n- hashindex: fix iterators (always raise StopIteration when exhausted)\n  (this seems not to get triggered in 1.0.x, but was discovered in master)\n- enable relative paths in ssh:// repo URLs, via /./relpath hack, #1655\n- allow repo paths with colons, #1705\n- update changed repo location immediately after acceptance, #1524\n- fix debug get-obj / delete-obj crash if object not found and remote repo,\n  #1684\n- pyinstaller: use a spec file to build borg.exe binary, exclude osxfuse dylib\n  on Mac OS X (avoids mismatch lib <-> driver), #1619\n\nNew features:\n\n- add \"borg key export\" / \"borg key import\" commands, #1555, so users are able\n  to back up / restore their encryption keys more easily.\n\n  Supported formats are the keyfile format used by borg internally and a\n  special \"paper\" format with by line checksums for printed backups. For the\n  paper format, the import is an interactive process which checks each line as\n  soon as it is input.\n- add \"borg debug-refcount-obj\" to determine a repo objects' referrer counts,\n  #1352\n\nOther changes:\n\n- add \"borg debug ...\" subcommands\n  (borg debug-* still works, but will be removed in borg 1.1)\n- setup.py: Add subcommand support to build_usage.\n- remote: change exception message for unexpected RPC data format to indicate\n  dataflow direction.\n- improved messages / error reporting:\n\n  - IntegrityError: add placeholder for message, so that the message we give\n    appears not only in the traceback, but also in the (short) error message,\n    #1572\n  - borg.key: include chunk id in exception msgs, #1571\n  - better messages for cache newer than repo, #1700\n- vagrant (testing/build VMs):\n\n  - upgrade OSXfuse / FUSE for macOS to 3.5.2\n  - update Debian Wheezy boxes, #1686\n  - openbsd / netbsd: use own boxes, fixes misc rsync installation and\n    FUSE/llfuse related testing issues, #1695 #1696 #1670 #1671 #1728\n- docs:\n\n  - add docs for \"key export\" and \"key import\" commands, #1641\n  - fix inconsistency in FAQ (pv-wrapper).\n  - fix second block in \"Easy to use\" section not showing on GitHub, #1576\n  - add bestpractices badge\n  - link reference docs and faq about BORG_FILES_CACHE_TTL, #1561\n  - improve borg info --help, explain size infos, #1532\n  - add release signing key / security contact to README, #1560\n  - add contribution guidelines for developers\n  - development.rst: add sphinx_rtd_theme to the sphinx install command\n  - adjust border color in borg.css\n  - add debug-info usage help file\n  - internals.rst: fix typos\n  - setup.py: fix build_usage to always process all commands\n  - added docs explaining multiple --restrict-to-path flags, #1602\n  - add more specific warning about write-access debug commands, #1587\n  - clarify FAQ regarding backup of virtual machines, #1672\n- tests:\n\n  - work around FUSE xattr test issue with recent fakeroot\n  - simplify repo/hashindex tests\n  - travis: test FUSE-enabled borg, use trusty to have a recent FUSE\n  - re-enable FUSE tests for RemoteArchiver (no deadlocks any more)\n  - clean env for pytest based tests, #1714\n  - fuse_mount contextmanager: accept any options\n\n\nVersion 1.0.7 (2016-08-19)\n--------------------------\n\nSecurity fixes:\n\n- borg serve: fix security issue with remote repository access, #1428\n  If you used e.g. --restrict-to-path /path/client1/ (with or without trailing\n  slash does not make a difference), it acted like a path prefix match using\n  /path/client1 (note the missing trailing slash) - the code then also allowed\n  working in e.g. /path/client13 or /path/client1000.\n\n  As this could accidentally lead to major security/privacy issues depending on\n  the paths you use, the behaviour was changed to be a strict directory match.\n  That means --restrict-to-path /path/client1 (with or without trailing slash\n  does not make a difference) now uses /path/client1/ internally (note the\n  trailing slash here!) for matching and allows precisely that path AND any\n  path below it. So, /path/client1 is allowed, /path/client1/repo1 is allowed,\n  but not /path/client13 or /path/client1000.\n\n  If you willingly used the undocumented (dangerous) previous behaviour, you\n  may need to rearrange your --restrict-to-path paths now. We are sorry if\n  that causes work for you, but we did not want a potentially dangerous\n  behaviour in the software (not even using a for-backwards-compat option).\n\nBug fixes:\n\n- fixed repeated LockTimeout exceptions when borg serve tried to write into\n  a already write-locked repo (e.g. by a borg mount), #502 part b)\n  This was solved by the fix for #1220 in 1.0.7rc1 already.\n- fix cosmetics + file leftover for \"not a valid borg repository\", #1490\n- Cache: release lock if cache is invalid, #1501\n- borg extract --strip-components: fix leak of preloaded chunk contents\n- Repository, when a InvalidRepository exception happens:\n\n  - fix spurious, empty lock.roster\n  - fix repo not closed cleanly\n\nNew features:\n\n- implement borg debug-info, fixes #1122\n  (just calls already existing code via cli, same output as below tracebacks)\n\nOther changes:\n\n- skip the O_NOATIME test on GNU Hurd, fixes #1315\n  (this is a very minor issue and the GNU Hurd project knows the bug)\n- document using a clean repo to test / build the release\n\n\nVersion 1.0.7rc2 (2016-08-13)\n-----------------------------\n\nBug fixes:\n\n- do not write objects to repository that are bigger than the allowed size,\n  borg will reject reading them, #1451.\n\n  Important: if you created archives with many millions of files or\n  directories, please verify if you can open them successfully,\n  e.g. try a \"borg list REPO::ARCHIVE\".\n- lz4 compression: dynamically enlarge the (de)compression buffer, the static\n  buffer was not big enough for archives with extremely many items, #1453\n- larger item metadata stream chunks, raise archive item limit by 8x, #1452\n- fix untracked segments made by moved DELETEs, #1442\n\n  Impact: Previously (metadata) segments could become untracked when deleting data,\n  these would never be cleaned up.\n- extended attributes (xattrs) related fixes:\n\n  - fixed a race condition in xattrs querying that led to the entire file not\n    being backed up (while logging the error, exit code = 1), #1469\n  - fixed a race condition in xattrs querying that led to a crash, #1462\n  - raise OSError including the error message derived from errno, deal with\n    path being a integer FD\n\nOther changes:\n\n- print active env var override by default, #1467\n- xattr module: refactor code, deduplicate, clean up\n- repository: split object size check into too small and too big\n- add a transaction_id assertion, so borg init on a broken (inconsistent)\n  filesystem does not look like a coding error in borg, but points to the\n  real problem.\n- explain confusing TypeError caused by compat support for old servers, #1456\n- add forgotten usage help file from build_usage\n- refactor/unify buffer code into helpers.Buffer class, add tests\n- docs:\n\n  - document archive limitation, #1452\n  - improve prune examples\n\n\nVersion 1.0.7rc1 (2016-08-05)\n-----------------------------\n\nBug fixes:\n\n- fix repo lock deadlocks (related to lock upgrade), #1220\n- catch unpacker exceptions, resync, #1351\n- fix borg break-lock ignoring BORG_REPO env var, #1324\n- files cache performance fixes (fixes unnecessary re-reading/chunking/\n  hashing of unmodified files for some use cases):\n\n  - fix unintended file cache eviction, #1430\n  - implement BORG_FILES_CACHE_TTL, update FAQ, raise default TTL from 10\n    to 20, #1338\n- FUSE:\n\n  - cache partially read data chunks (performance), #965, #966\n  - always create a root dir, #1125\n- use an OrderedDict for helptext, making the build reproducible, #1346\n- RemoteRepository init: always call close on exceptions, #1370 (cosmetic)\n- ignore stdout/stderr broken pipe errors (cosmetic), #1116\n\nNew features:\n\n- better borg versions management support (useful esp. for borg servers\n  wanting to offer multiple borg versions and for clients wanting to choose\n  a specific server borg version), #1392:\n\n  - add BORG_VERSION environment variable before executing \"borg serve\" via ssh\n  - add new placeholder {borgversion}\n  - substitute placeholders in --remote-path\n\n- borg init --append-only option (makes using the more secure append-only mode\n  more convenient. when used remotely, this requires 1.0.7+ also on the borg\n  server), #1291.\n\nOther changes:\n\n- Vagrantfile:\n\n  - darwin64: upgrade to FUSE for macOS 3.4.1 (aka osxfuse), #1378\n  - xenial64: use user \"ubuntu\", not \"vagrant\" (as usual), #1331\n- tests:\n\n  - fix FUSE tests on OS X, #1433\n- docs:\n\n  - FAQ: add backup using stable filesystem names recommendation\n  - FAQ about glibc compatibility added, #491, glibc-check improved\n  - FAQ: 'A' unchanged file; remove ambiguous entry age sentence.\n  - OS X: install pkg-config to build with FUSE support, fixes #1400\n  - add notes about shell/sudo pitfalls with env. vars, #1380\n  - added platform feature matrix\n- implement borg debug-dump-repo-objs\n\n\nVersion 1.0.6 (2016-07-12)\n--------------------------\n\nBug fixes:\n\n- Linux: handle multiple LD_PRELOAD entries correctly, #1314, #1111\n- Fix crash with unclear message if the libc is not found, #1314, #1111\n\nOther changes:\n\n- tests:\n\n  - Fixed O_NOATIME tests for Solaris and GNU Hurd, #1315\n  - Fixed sparse file tests for (file) systems not supporting it, #1310\n- docs:\n\n  - Fixed syntax highlighting, #1313\n  - misc docs: added data processing overview picture\n\n\nVersion 1.0.6rc1 (2016-07-10)\n-----------------------------\n\nNew features:\n\n- borg check --repair: heal damaged files if missing chunks re-appear (e.g. if\n  the previously missing chunk was added again in a later backup archive),\n  #148. (*) Also improved logging.\n\nBug fixes:\n\n- sync_dir: silence fsync() failing with EINVAL, #1287\n  Some network filesystems (like smbfs) don't support this and we use this in\n  repository code.\n- borg mount (FUSE):\n\n  - fix directories being shadowed when contained paths were also specified,\n    #1295\n  - raise I/O Error (EIO) on damaged files (unless -o allow_damaged_files is\n    used), #1302. (*)\n- borg extract: warn if a damaged file is extracted, #1299. (*)\n- Added some missing return code checks (ChunkIndex._add, hashindex_resize).\n- borg check: fix/optimize initial hash table size, avoids resize of the table.\n\nOther changes:\n\n- tests:\n\n  - add more FUSE tests, #1284\n  - deduplicate FUSE (u)mount code\n  - fix borg binary test issues, #862\n- docs:\n\n  - changelog: added release dates to older borg releases\n  - fix some sphinx (docs generator) warnings, #881\n\nNotes:\n\n(*) Some features depend on information (chunks_healthy list) added to item\nmetadata when a file with missing chunks was \"repaired\" using all-zero\nreplacement chunks. The chunks_healthy list is generated since borg 1.0.4,\nthus borg can't recognize such \"repaired\" (but content-damaged) files if the\nrepair was done with an older borg version.\n\n\nVersion 1.0.5 (2016-07-07)\n--------------------------\n\nBug fixes:\n\n- borg mount: fix FUSE crash in xattr code on Linux introduced in 1.0.4, #1282\n\nOther changes:\n\n- backport some FAQ entries from master branch\n- add release helper scripts\n- Vagrantfile:\n\n  - centos6: no FUSE, don't build binary\n  - add xz for redhat-like dists\n\n\nVersion 1.0.4 (2016-07-07)\n--------------------------\n\nNew features:\n\n- borg serve --append-only, #1168\n  This was included because it was a simple change (append-only functionality\n  was already present via repository config file) and makes better security now\n  practically usable.\n- BORG_REMOTE_PATH environment variable, #1258\n  This was included because it was a simple change (--remote-path cli option\n  was already present) and makes borg much easier to use if you need it.\n- Repository: cleanup incomplete transaction on \"no space left\" condition.\n  In many cases, this can avoid a 100% full repo filesystem (which is very\n  problematic as borg always needs free space - even to delete archives).\n\nBug fixes:\n\n- Fix wrong handling and reporting of OSErrors in borg create, #1138.\n  This was a serious issue: in the context of \"borg create\", errors like\n  repository I/O errors (e.g. disk I/O errors, ssh repo connection errors)\n  were handled badly and did not lead to a crash (which would be good for this\n  case, because the repo transaction would be incomplete and trigger a\n  transaction rollback to clean up).\n  Now, error handling for source files is cleanly separated from every other\n  error handling, so only problematic input files are logged and skipped.\n- Implement fail-safe error handling for borg extract.\n  Note that this isn't nearly as critical as the borg create error handling\n  bug, since nothing is written to the repo. So this was \"merely\" misleading\n  error reporting.\n- Add missing error handler in directory attr restore loop.\n- repo: make sure write data hits disk before the commit tag (#1236) and also\n  sync the containing directory.\n- FUSE: getxattr fail must use errno.ENOATTR, #1126\n  (fixes Mac OS X Finder malfunction: \"zero bytes\" file length, access denied)\n- borg check --repair: do not lose information about the good/original chunks.\n  If we do not lose the original chunk IDs list when \"repairing\" a file\n  (replacing missing chunks with all-zero chunks), we have a chance to \"heal\"\n  the file back into its original state later, in case the chunks re-appear\n  (e.g. in a fresh backup). Healing is not implemented yet, see #148.\n- fixes for --read-special mode:\n\n  - ignore known files cache, #1241\n  - fake regular file mode, #1214\n  - improve symlinks handling, #1215\n- remove passphrase from subprocess environment, #1105\n- Ignore empty index file (will trigger index rebuild), #1195\n- add missing placeholder support for --prefix, #1027\n- improve exception handling for placeholder replacement\n- catch and format exceptions in arg parsing\n- helpers: fix \"undefined name 'e'\" in exception handler\n- better error handling for missing repo manifest, #1043\n- borg delete:\n\n  - make it possible to delete a repo without manifest\n  - borg delete --forced allows one to delete corrupted archives, #1139\n- borg check:\n\n  - make borg check work for empty repo\n  - fix resync and msgpacked item qualifier, #1135\n  - rebuild_manifest: fix crash if 'name' or 'time' key were missing.\n  - better validation of item metadata dicts, #1130\n  - better validation of archive metadata dicts\n- close the repo on exit - even if rollback did not work, #1197.\n  This is rather cosmetic, it avoids repo closing in the destructor.\n\n- tests:\n\n  - fix sparse file test, #1170\n  - flake8: ignore new F405, #1185\n  - catch \"invalid argument\" on cygwin, #257\n  - fix sparseness assertion in test prep, #1264\n\nOther changes:\n\n- make borg build/work on OpenSSL 1.0 and 1.1, #1187\n- docs / help:\n\n  - fix / clarify prune help, #1143\n  - fix \"patterns\" help formatting\n  - add missing docs / help about placeholders\n  - resources: rename atticmatic to borgmatic\n  - document sshd settings, #545\n  - more details about checkpoints, add split trick, #1171\n  - support docs: add freenode web chat link, #1175\n  - add prune visualization / example, #723\n  - add note that Fnmatch is default, #1247\n  - make clear that lzma levels > 6 are a waste of cpu cycles\n  - add a \"do not edit\" note to auto-generated files, #1250\n  - update cygwin installation docs\n- repository interoperability with borg master (1.1dev) branch:\n\n  - borg check: read item metadata keys from manifest, #1147\n  - read v2 hints files, #1235\n  - fix hints file \"unknown version\" error handling bug\n- tests: add tests for format_line\n- llfuse: update version requirement for freebsd\n- Vagrantfile:\n\n  - use openbsd 5.9, #716\n  - do not install llfuse on netbsd (broken)\n  - update OSXfuse to version 3.3.3\n  - use Python 3.5.2 to build the binaries\n- glibc compatibility checker: scripts/glibc_check.py\n- add .eggs to .gitignore\n\n\nVersion 1.0.3 (2016-05-20)\n--------------------------\n\nBug fixes:\n\n- prune: avoid that checkpoints are kept and completed archives are deleted in\n  a prune run), #997\n- prune: fix commandline argument validation - some valid command lines were\n  considered invalid (annoying, but harmless), #942\n- fix capabilities extraction on Linux (set xattrs last, after chown()), #1069\n- repository: fix commit tags being seen in data\n- when probing key files, do binary reads. avoids crash when non-borg binary\n  files are located in borg's key files directory.\n- handle SIGTERM and make a clean exit - avoids orphan lock files.\n- repository cache: don't cache large objects (avoid using lots of temp. disk\n  space), #1063\n\nOther changes:\n\n- Vagrantfile: OS X: update osxfuse / install lzma package, #933\n- setup.py: add check for platform_darwin.c\n- setup.py: on freebsd, use a llfuse release that builds ok\n- docs / help:\n\n  - update readthedocs URLs, #991\n  - add missing docs for \"borg break-lock\", #992\n  - borg create help: add some words to about the archive name\n  - borg create help: document format tags, #894\n\n\nVersion 1.0.2 (2016-04-16)\n--------------------------\n\nBug fixes:\n\n- fix malfunction and potential corruption on (nowadays rather rare) big-endian\n  architectures or bi-endian archs in (rare) BE mode. #886, #889\n\n  cache resync / index merge was malfunctioning due to this, potentially\n  leading to data loss. borg info had cosmetic issues (displayed wrong values).\n\n  note: all (widespread) little-endian archs (like x86/x64) or bi-endian archs\n  in (widespread) LE mode (like ARMEL, MIPSEL, ...) were NOT affected.\n- add overflow and range checks for 1st (special) uint32 of the hashindex\n  values, switch from int32 to uint32.\n- fix so that refcount will never overflow, but just stick to max. value after\n  a overflow would have occurred.\n- borg delete: fix --cache-only for broken caches, #874\n\n  Makes --cache-only idempotent: it won't fail if the cache is already deleted.\n- fixed borg create --one-file-system erroneously traversing into other\n  filesystems (if starting fs device number was 0), #873\n- workaround a bug in Linux fadvise FADV_DONTNEED, #907\n\nOther changes:\n\n- better test coverage for hashindex, incl. overflow testing, checking correct\n  computations so endianness issues would be discovered.\n- reproducible doc for ProgressIndicator*,  make the build reproducible.\n- use latest llfuse for vagrant machines\n- docs:\n\n  - use /path/to/repo in examples, fixes #901\n  - fix confusing usage of \"repo\" as archive name (use \"arch\")\n\n\nVersion 1.0.1 (2016-04-08)\n--------------------------\n\nNew features:\n\nUsually there are no new features in a bugfix release, but these were added\ndue to their high impact on security/safety/speed or because they are fixes\nalso:\n\n- append-only mode for repositories, #809, #36 (see docs)\n- borg create: add --ignore-inode option to make borg detect unmodified files\n  even if your filesystem does not have stable inode numbers (like sshfs and\n  possibly CIFS).\n- add options --warning, --error, --critical for missing log levels, #826.\n  it's not recommended to suppress warnings or errors, but the user may decide\n  this on his own.\n  note: --warning is not given to borg serve so a <= 1.0.0 borg will still\n  work as server (it is not needed as it is the default).\n  do not use --error or --critical when using a <= 1.0.0 borg server.\n\nBug fixes:\n\n- fix silently skipping EIO, #748\n- add context manager for Repository (avoid orphan repository locks), #285\n- do not sleep for >60s while waiting for lock, #773\n- unpack file stats before passing to FUSE\n- fix build on illumos\n- don't try to back up doors or event ports (Solaris and derivatives)\n- remove useless/misleading libc version display, #738\n- test suite: reset exit code of persistent archiver, #844\n- RemoteRepository: clean up pipe if remote open() fails\n- Remote: don't print tracebacks for Error exceptions handled downstream, #792\n- if BORG_PASSPHRASE is present but wrong, don't prompt for password, but fail\n  instead, #791\n- ArchiveChecker: move \"orphaned objects check skipped\" to INFO log level, #826\n- fix capitalization, add ellipses, change log level to debug for 2 messages,\n  #798\n\nOther changes:\n\n- update llfuse requirement, llfuse 1.0 works\n- update OS / dist packages on build machines, #717\n- prefer showing --info over -v in usage help, #859\n- docs:\n\n  - fix cygwin requirements (gcc-g++)\n  - document how to debug / file filesystem issues, #664\n  - fix reproducible build of api docs\n  - RTD theme: CSS !important overwrite, #727\n  - Document logo font. Recreate logo png. Remove GIMP logo file.\n\n\nVersion 1.0.0 (2016-03-05)\n--------------------------\n\nThe major release number change (0.x -> 1.x) indicates bigger incompatible\nchanges, please read the compatibility notes, adapt / test your scripts and\ncheck your backup logs.\n\nCompatibility notes:\n\n- drop support for python 3.2 and 3.3, require 3.4 or 3.5, #221 #65 #490\n  note: we provide binaries that include python 3.5.1 and everything else\n  needed. they are an option in case you are stuck with < 3.4 otherwise.\n- change encryption to be on by default (using \"repokey\" mode)\n- moved keyfile keys from ~/.borg/keys to ~/.config/borg/keys,\n  you can either move them manually or run \"borg upgrade <REPO>\"\n- remove support for --encryption=passphrase,\n  use borg migrate-to-repokey to switch to repokey mode, #97\n- remove deprecated --compression <number>,\n  use --compression zlib,<number> instead\n  in case of 0, you could also use --compression none\n- remove deprecated --hourly/daily/weekly/monthly/yearly\n  use --keep-hourly/daily/weekly/monthly/yearly instead\n- remove deprecated --do-not-cross-mountpoints,\n  use --one-file-system instead\n- disambiguate -p option, #563:\n\n  - -p now is same as --progress\n  - -P now is same as --prefix\n- remove deprecated \"borg verify\",\n  use \"borg extract --dry-run\" instead\n- cleanup environment variable semantics, #355\n  the environment variables used to be \"yes sayers\" when set, this was\n  conceptually generalized to \"automatic answerers\" and they just give their\n  value as answer (as if you typed in that value when being asked).\n  See the \"usage\" / \"Environment Variables\" section of the docs for details.\n- change the builtin default for --chunker-params, create 2MiB chunks, #343\n  --chunker-params new default: 19,23,21,4095 - old default: 10,23,16,4095\n\n  one of the biggest issues with borg < 1.0 (and also attic) was that it had a\n  default target chunk size of 64kiB, thus it created a lot of chunks and thus\n  also a huge chunk management overhead (high RAM and disk usage).\n\n  please note that the new default won't change the chunks that you already\n  have in your repository. the new big chunks do not deduplicate with the old\n  small chunks, so expect your repo to grow at least by the size of every\n  changed file and in the worst case (e.g. if your files cache was lost / is\n  not used) by the size of every file (minus any compression you might use).\n\n  in case you want to see a much lower resource usage immediately (RAM / disk)\n  for chunks management, it might be better to start with a new repo than\n  to continue in the existing repo (with an existing repo, you have to wait\n  until all archives with small chunks get pruned to see a lower resource\n  usage).\n\n  if you used the old --chunker-params default value (or if you did not use\n  --chunker-params option at all) and you'd like to continue using small\n  chunks (and you accept the huge resource usage that comes with that), just\n  use explicitly borg create --chunker-params=10,23,16,4095.\n- archive timestamps: the 'time' timestamp now refers to archive creation\n  start time (was: end time), the new 'time_end' timestamp refers to archive\n  creation end time. This might affect prune if your backups take a long time.\n  if you give a timestamp via cli, this is stored into 'time'. therefore it now\n  needs to mean archive creation start time.\n\nNew features:\n\n- implement password roundtrip, #695\n\nBug fixes:\n\n- remote end does not need cache nor keys directories, do not create them, #701\n- added retry counter for passwords, #703\n\nOther changes:\n\n- fix compiler warnings, #697\n- docs:\n\n  - update README.rst to new changelog location in docs/changes.rst\n  - add Teemu to AUTHORS\n  - changes.rst: fix old chunker params, #698\n  - FAQ: how to limit bandwidth\n\n\nVersion 1.0.0rc2 (2016-02-28)\n-----------------------------\n\nNew features:\n\n- format options for location: user, pid, fqdn, hostname, now, utcnow, user\n- borg list --list-format\n- borg prune -v --list enables the keep/prune list output, #658\n\nBug fixes:\n\n- fix _open_rb noatime handling, #657\n- add a simple archivename validator, #680\n- borg create --stats: show timestamps in localtime, use same labels/formatting\n  as borg info, #651\n- llfuse compatibility fixes (now compatible with: 0.40, 0.41, 0.42)\n\nOther changes:\n\n- it is now possible to use \"pip install borgbackup[fuse]\" to\n  install the llfuse dependency automatically, using the correct version requirement\n  for it. you still need to care about having installed the FUSE / build\n  related OS package first, though, so that building llfuse can succeed.\n- Vagrant: drop Ubuntu Precise (12.04) - does not have Python >= 3.4\n- Vagrant: use pyinstaller v3.1.1 to build binaries\n- docs:\n\n  - borg upgrade: add to docs that only LOCAL repos are supported\n  - borg upgrade also handles borg 0.xx -> 1.0\n  - use pip extras or requirements file to install llfuse\n  - fix order in release process\n  - updated usage docs and other minor / cosmetic fixes\n  - verified borg examples in docs, #644\n  - freebsd dependency installation and FUSE configuration, #649\n  - add example how to restore a raw device, #671\n  - add a hint about the dev headers needed when installing from source\n  - add examples for delete (and handle delete after list, before prune), #656\n  - update example for borg create -v --stats (use iso datetime format), #663\n  - added example to BORG_RSH docs\n  - \"connection closed by remote\": add FAQ entry and point to issue #636\n\n\nVersion 1.0.0rc1 (2016-02-07)\n-----------------------------\n\nNew features:\n\n- borg migrate-to-repokey (\"passphrase\" -> \"repokey\" encryption key mode)\n- implement --short for borg list REPO, #611\n- implement --list for borg extract (consistency with borg create)\n- borg serve: overwrite client's --restrict-to-path with ssh forced command's\n  option value (but keep everything else from the client commandline), #544\n- use $XDG_CONFIG_HOME/keys for keyfile keys (~/.config/borg/keys), #515\n- \"borg upgrade\" moves the keyfile keys to the new location\n- display both archive creation start and end time in \"borg info\", #627\n\n\nBug fixes:\n\n- normalize trailing slashes for the repository path, #606\n- Cache: fix exception handling in __init__, release lock, #610\n\nOther changes:\n\n- suppress unneeded exception context (PEP 409), simpler tracebacks\n- removed special code needed to deal with imperfections / incompatibilities /\n  missing stuff in py 3.2/3.3, simplify code that can be done simpler in 3.4\n- removed some version requirements that were kept on old versions because\n  newer did not support py 3.2 any more\n- use some py 3.4+ stdlib code instead of own/openssl/pypi code:\n\n  - use os.urandom instead of own cython openssl RAND_bytes wrapper, #493\n  - use hashlib.pbkdf2_hmac from py stdlib instead of own openssl wrapper\n  - use hmac.compare_digest instead of == operator (constant time comparison)\n  - use stat.filemode instead of homegrown code\n  - use \"mock\" library from stdlib, #145\n  - remove borg.support (with non-broken argparse copy), it is ok in 3.4+, #358\n- Vagrant: copy CHANGES.rst as symlink, #592\n- cosmetic code cleanups, add flake8 to tox/travis, #4\n- docs / help:\n\n  - make \"borg -h\" output prettier, #591\n  - slightly rephrase prune help\n  - add missing example for --list option of borg create\n  - quote exclude line that includes an asterisk to prevent shell expansion\n  - fix dead link to license\n  - delete Ubuntu Vivid, it is not supported anymore (EOL)\n  - OS X binary does not work for older OS X releases, #629\n  - borg serve's special support for forced/original ssh commands, #544\n  - misc. updates and fixes\n", "import base64\nimport json\nimport os\nimport stat\nimport sys\nimport time\nfrom collections import OrderedDict, defaultdict\nfrom contextlib import contextmanager\nfrom datetime import timedelta\nfrom functools import partial\nfrom getpass import getuser\nfrom io import BytesIO\nfrom itertools import groupby, zip_longest\nfrom typing import Iterator\nfrom shutil import get_terminal_size\n\nfrom .platformflags import is_win32\nfrom .logger import create_logger\n\nlogger = create_logger()\n\nfrom . import xattr\nfrom .chunker import get_chunker, Chunk\nfrom .cache import ChunkListEntry\nfrom .crypto.key import key_factory, UnsupportedPayloadError\nfrom .compress import Compressor, CompressionSpec\nfrom .constants import *  # NOQA\nfrom .crypto.low_level import IntegrityError as IntegrityErrorBase\nfrom .hashindex import ChunkIndex, ChunkIndexEntry, CacheSynchronizer\nfrom .helpers import HardLinkManager\nfrom .helpers import ChunkIteratorFileWrapper, open_item\nfrom .helpers import Error, IntegrityError, set_ec\nfrom .platform import uid2user, user2uid, gid2group, group2gid\nfrom .helpers import parse_timestamp, archive_ts_now\nfrom .helpers import OutputTimestamp, format_timedelta, format_file_size, file_status, FileSize\nfrom .helpers import safe_encode, make_path_safe, remove_surrogates, text_to_json, join_cmd, remove_dotdot_prefixes\nfrom .helpers import StableDict\nfrom .helpers import bin_to_hex\nfrom .helpers import safe_ns\nfrom .helpers import ellipsis_truncate, ProgressIndicatorPercent, log_multi\nfrom .helpers import os_open, flags_normal, flags_dir\nfrom .helpers import os_stat\nfrom .helpers import msgpack\nfrom .helpers import sig_int\nfrom .helpers.lrucache import LRUCache\nfrom .manifest import Manifest\nfrom .patterns import PathPrefixPattern, FnmatchPattern, IECommand\nfrom .item import Item, ArchiveItem, ItemDiff\nfrom .platform import acl_get, acl_set, set_flags, get_flags, swidth, hostname\nfrom .remote import cache_if_remote\nfrom .repository import Repository, LIST_SCAN_LIMIT\nfrom .repoobj import RepoObj\n\nhas_link = hasattr(os, \"link\")\n\n\nclass Statistics:\n    def __init__(self, output_json=False, iec=False):\n        self.output_json = output_json\n        self.iec = iec\n        self.osize = self.usize = self.nfiles = 0\n        self.last_progress = 0  # timestamp when last progress was shown\n        self.files_stats = defaultdict(int)\n        self.chunking_time = 0.0\n        self.hashing_time = 0.0\n        self.rx_bytes = 0\n        self.tx_bytes = 0\n\n    def update(self, size, unique):\n        self.osize += size\n        if unique:\n            self.usize += size\n\n    def __add__(self, other):\n        if not isinstance(other, Statistics):\n            raise TypeError(\"can only add Statistics objects\")\n        stats = Statistics(self.output_json, self.iec)\n        stats.osize = self.osize + other.osize\n        stats.usize = self.usize + other.usize\n        stats.nfiles = self.nfiles + other.nfiles\n        stats.chunking_time = self.chunking_time + other.chunking_time\n        stats.hashing_time = self.hashing_time + other.hashing_time\n        st1, st2 = self.files_stats, other.files_stats\n        stats.files_stats = defaultdict(int, {key: (st1[key] + st2[key]) for key in st1.keys() | st2.keys()})\n\n        return stats\n\n    def __str__(self):\n        hashing_time = format_timedelta(timedelta(seconds=self.hashing_time))\n        chunking_time = format_timedelta(timedelta(seconds=self.chunking_time))\n        return \"\"\"\\\nNumber of files: {stats.nfiles}\nOriginal size: {stats.osize_fmt}\nDeduplicated size: {stats.usize_fmt}\nTime spent in hashing: {hashing_time}\nTime spent in chunking: {chunking_time}\nAdded files: {added_files}\nUnchanged files: {unchanged_files}\nModified files: {modified_files}\nError files: {error_files}\nFiles changed while reading: {files_changed_while_reading}\nBytes read from remote: {stats.rx_bytes}\nBytes sent to remote: {stats.tx_bytes}\n\"\"\".format(\n            stats=self,\n            hashing_time=hashing_time,\n            chunking_time=chunking_time,\n            added_files=self.files_stats[\"A\"],\n            unchanged_files=self.files_stats[\"U\"],\n            modified_files=self.files_stats[\"M\"],\n            error_files=self.files_stats[\"E\"],\n            files_changed_while_reading=self.files_stats[\"C\"],\n        )\n\n    def __repr__(self):\n        return \"<{cls} object at {hash:#x} ({self.osize}, {self.usize})>\".format(\n            cls=type(self).__name__, hash=id(self), self=self\n        )\n\n    def as_dict(self):\n        return {\n            \"original_size\": FileSize(self.osize, iec=self.iec),\n            \"deduplicated_size\": FileSize(self.usize, iec=self.iec),\n            \"nfiles\": self.nfiles,\n            \"hashing_time\": self.hashing_time,\n            \"chunking_time\": self.chunking_time,\n            \"files_stats\": self.files_stats,\n        }\n\n    def as_raw_dict(self):\n        return {\"size\": self.osize, \"nfiles\": self.nfiles}\n\n    @classmethod\n    def from_raw_dict(cls, **kw):\n        self = cls()\n        self.osize = kw[\"size\"]\n        self.nfiles = kw[\"nfiles\"]\n        return self\n\n    @property\n    def osize_fmt(self):\n        return format_file_size(self.osize, iec=self.iec)\n\n    @property\n    def usize_fmt(self):\n        return format_file_size(self.usize, iec=self.iec)\n\n    def show_progress(self, item=None, final=False, stream=None, dt=None):\n        now = time.monotonic()\n        if dt is None or now - self.last_progress > dt:\n            self.last_progress = now\n            if self.output_json:\n                if not final:\n                    data = self.as_dict()\n                    if item:\n                        data.update(text_to_json(\"path\", item.path))\n                else:\n                    data = {}\n                data.update({\"time\": time.time(), \"type\": \"archive_progress\", \"finished\": final})\n                msg = json.dumps(data)\n                end = \"\\n\"\n            else:\n                columns, lines = get_terminal_size()\n                if not final:\n                    msg = \"{0.osize_fmt} O {0.usize_fmt} U {0.nfiles} N \".format(self)\n                    path = remove_surrogates(item.path) if item else \"\"\n                    space = columns - swidth(msg)\n                    if space < 12:\n                        msg = \"\"\n                        space = columns - swidth(msg)\n                    if space >= 8:\n                        msg += ellipsis_truncate(path, space)\n                else:\n                    msg = \" \" * columns\n                end = \"\\r\"\n            print(msg, end=end, file=stream or sys.stderr, flush=True)\n\n\ndef is_special(mode):\n    # file types that get special treatment in --read-special mode\n    return stat.S_ISBLK(mode) or stat.S_ISCHR(mode) or stat.S_ISFIFO(mode)\n\n\nclass BackupError(Exception):\n    \"\"\"\n    Exception raised for non-OSError-based exceptions while accessing backup files.\n    \"\"\"\n\n\nclass BackupOSError(Exception):\n    \"\"\"\n    Wrapper for OSError raised while accessing backup files.\n\n    Borg does different kinds of IO, and IO failures have different consequences.\n    This wrapper represents failures of input file or extraction IO.\n    These are non-critical and are only reported (exit code = 1, warning).\n\n    Any unwrapped IO error is critical and aborts execution (for example repository IO failure).\n    \"\"\"\n\n    def __init__(self, op, os_error):\n        self.op = op\n        self.os_error = os_error\n        self.errno = os_error.errno\n        self.strerror = os_error.strerror\n        self.filename = os_error.filename\n\n    def __str__(self):\n        if self.op:\n            return f\"{self.op}: {self.os_error}\"\n        else:\n            return str(self.os_error)\n\n\nclass BackupIO:\n    op = \"\"\n\n    def __call__(self, op=\"\"):\n        self.op = op\n        return self\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type and issubclass(exc_type, OSError):\n            raise BackupOSError(self.op, exc_val) from exc_val\n\n\nbackup_io = BackupIO()\n\n\ndef backup_io_iter(iterator):\n    backup_io.op = \"read\"\n    while True:\n        with backup_io:\n            try:\n                item = next(iterator)\n            except StopIteration:\n                return\n        yield item\n\n\ndef stat_update_check(st_old, st_curr):\n    \"\"\"\n    this checks for some race conditions between the first filename-based stat()\n    we did before dispatching to the (hopefully correct) file type backup handler\n    and the (hopefully) fd-based fstat() we did in the handler.\n\n    if there is a problematic difference (e.g. file type changed), we rather\n    skip the file than being tricked into a security problem.\n\n    such races should only happen if:\n    - we are backing up a live filesystem (no snapshot, not inactive)\n    - if files change due to normal fs activity at an unfortunate time\n    - if somebody is doing an attack against us\n    \"\"\"\n    # assuming that a file type change implicates a different inode change AND that inode numbers\n    # are not duplicate in a short timeframe, this check is redundant and solved by the ino check:\n    if stat.S_IFMT(st_old.st_mode) != stat.S_IFMT(st_curr.st_mode):\n        # in this case, we dispatched to wrong handler - abort\n        raise BackupError(\"file type changed (race condition), skipping file\")\n    if st_old.st_ino != st_curr.st_ino:\n        # in this case, the hardlinks-related code in create_helper has the wrong inode - abort!\n        raise BackupError(\"file inode changed (race condition), skipping file\")\n    # looks ok, we are still dealing with the same thing - return current stat:\n    return st_curr\n\n\n@contextmanager\ndef OsOpen(*, flags, path=None, parent_fd=None, name=None, noatime=False, op=\"open\"):\n    with backup_io(op):\n        fd = os_open(path=path, parent_fd=parent_fd, name=name, flags=flags, noatime=noatime)\n    try:\n        yield fd\n    finally:\n        # On windows fd is None for directories.\n        if fd is not None:\n            os.close(fd)\n\n\nclass DownloadPipeline:\n    def __init__(self, repository, repo_objs):\n        self.repository = repository\n        self.repo_objs = repo_objs\n\n    def unpack_many(self, ids, *, filter=None, preload=False):\n        \"\"\"\n        Return iterator of items.\n\n        *ids* is a chunk ID list of an item stream. *filter* is a callable\n        to decide whether an item will be yielded. *preload* preloads the data chunks of every yielded item.\n\n        Warning: if *preload* is True then all data chunks of every yielded item have to be retrieved,\n        otherwise preloaded chunks will accumulate in RemoteRepository and create a memory leak.\n        \"\"\"\n        hlids_preloaded = set()\n        unpacker = msgpack.Unpacker(use_list=False)\n        for data in self.fetch_many(ids):\n            unpacker.feed(data)\n            for _item in unpacker:\n                item = Item(internal_dict=_item)\n                if \"chunks\" in item:\n                    item.chunks = [ChunkListEntry(*e) for e in item.chunks]\n                if filter and not filter(item):\n                    continue\n                if preload and \"chunks\" in item:\n                    hlid = item.get(\"hlid\", None)\n                    if hlid is None:\n                        preload_chunks = True\n                    elif hlid in hlids_preloaded:\n                        preload_chunks = False\n                    else:\n                        # not having the hardlink's chunks already preloaded for other hardlink to same inode\n                        preload_chunks = True\n                        hlids_preloaded.add(hlid)\n                    if preload_chunks:\n                        self.repository.preload([c.id for c in item.chunks])\n                yield item\n\n    def fetch_many(self, ids, is_preloaded=False):\n        for id_, cdata in zip(ids, self.repository.get_many(ids, is_preloaded=is_preloaded)):\n            _, data = self.repo_objs.parse(id_, cdata)\n            yield data\n\n\nclass ChunkBuffer:\n    BUFFER_SIZE = 8 * 1024 * 1024\n\n    def __init__(self, key, chunker_params=ITEMS_CHUNKER_PARAMS):\n        self.buffer = BytesIO()\n        self.packer = msgpack.Packer()\n        self.chunks = []\n        self.key = key\n        self.chunker = get_chunker(*chunker_params, seed=self.key.chunk_seed, sparse=False)\n        self.saved_chunks_len = None\n\n    def add(self, item):\n        self.buffer.write(self.packer.pack(item.as_dict()))\n        if self.is_full():\n            self.flush()\n\n    def write_chunk(self, chunk):\n        raise NotImplementedError\n\n    def flush(self, flush=False):\n        if self.buffer.tell() == 0:\n            return\n        self.buffer.seek(0)\n        # The chunker returns a memoryview to its internal buffer,\n        # thus a copy is needed before resuming the chunker iterator.\n        # the metadata stream may produce all-zero chunks, so deal\n        # with CH_ALLOC (and CH_HOLE, for completeness) here.\n        chunks = []\n        for chunk in self.chunker.chunkify(self.buffer):\n            alloc = chunk.meta[\"allocation\"]\n            if alloc == CH_DATA:\n                data = bytes(chunk.data)\n            elif alloc in (CH_ALLOC, CH_HOLE):\n                data = zeros[: chunk.meta[\"size\"]]\n            else:\n                raise ValueError(\"chunk allocation has unsupported value of %r\" % alloc)\n            chunks.append(data)\n        self.buffer.seek(0)\n        self.buffer.truncate(0)\n        # Leave the last partial chunk in the buffer unless flush is True\n        end = None if flush or len(chunks) == 1 else -1\n        for chunk in chunks[:end]:\n            self.chunks.append(self.write_chunk(chunk))\n        if end == -1:\n            self.buffer.write(chunks[-1])\n\n    def is_full(self):\n        return self.buffer.tell() > self.BUFFER_SIZE\n\n    def save_chunks_state(self):\n        # as we only append to self.chunks, remembering the current length is good enough\n        self.saved_chunks_len = len(self.chunks)\n\n    def restore_chunks_state(self):\n        scl = self.saved_chunks_len\n        assert scl is not None, \"forgot to call save_chunks_state?\"\n        tail_chunks = self.chunks[scl:]\n        del self.chunks[scl:]\n        self.saved_chunks_len = None\n        return tail_chunks\n\n\nclass CacheChunkBuffer(ChunkBuffer):\n    def __init__(self, cache, key, stats, chunker_params=ITEMS_CHUNKER_PARAMS):\n        super().__init__(key, chunker_params)\n        self.cache = cache\n        self.stats = stats\n\n    def write_chunk(self, chunk):\n        id_, _ = self.cache.add_chunk(self.key.id_hash(chunk), {}, chunk, stats=self.stats, wait=False)\n        logger.debug(f\"writing item metadata stream chunk {bin_to_hex(id_)}\")\n        self.cache.repository.async_response(wait=False)\n        return id_\n\n\ndef get_item_uid_gid(item, *, numeric, uid_forced=None, gid_forced=None, uid_default=0, gid_default=0):\n    if uid_forced is not None:\n        uid = uid_forced\n    else:\n        uid = None if numeric else user2uid(item.get(\"user\"))\n        uid = item.get(\"uid\") if uid is None else uid\n        if uid is None or uid < 0:\n            uid = uid_default\n    if gid_forced is not None:\n        gid = gid_forced\n    else:\n        gid = None if numeric else group2gid(item.get(\"group\"))\n        gid = item.get(\"gid\") if gid is None else gid\n        if gid is None or gid < 0:\n            gid = gid_default\n    return uid, gid\n\n\ndef archive_get_items(metadata, *, repo_objs, repository):\n    if \"item_ptrs\" in metadata:  # looks like a v2+ archive\n        assert \"items\" not in metadata\n        items = []\n        for id, cdata in zip(metadata.item_ptrs, repository.get_many(metadata.item_ptrs)):\n            _, data = repo_objs.parse(id, cdata)\n            ids = msgpack.unpackb(data)\n            items.extend(ids)\n        return items\n\n    if \"items\" in metadata:  # legacy, v1 archive\n        assert \"item_ptrs\" not in metadata\n        return metadata.items\n\n\ndef archive_put_items(chunk_ids, *, repo_objs, cache=None, stats=None, add_reference=None):\n    \"\"\"gets a (potentially large) list of archive metadata stream chunk ids and writes them to repo objects\"\"\"\n    item_ptrs = []\n    for i in range(0, len(chunk_ids), IDS_PER_CHUNK):\n        data = msgpack.packb(chunk_ids[i : i + IDS_PER_CHUNK])\n        id = repo_objs.id_hash(data)\n        logger.debug(f\"writing item_ptrs chunk {bin_to_hex(id)}\")\n        if cache is not None and stats is not None:\n            cache.add_chunk(id, {}, data, stats=stats)\n        elif add_reference is not None:\n            cdata = repo_objs.format(id, {}, data)\n            add_reference(id, len(data), cdata)\n        else:\n            raise NotImplementedError\n        item_ptrs.append(id)\n    return item_ptrs\n\n\nclass Archive:\n    class DoesNotExist(Error):\n        \"\"\"Archive {} does not exist\"\"\"\n\n    class AlreadyExists(Error):\n        \"\"\"Archive {} already exists\"\"\"\n\n    class IncompatibleFilesystemEncodingError(Error):\n        \"\"\"Failed to encode filename \"{}\" into file system encoding \"{}\". Consider configuring the LANG environment variable.\"\"\"\n\n    def __init__(\n        self,\n        manifest,\n        name,\n        cache=None,\n        create=False,\n        numeric_ids=False,\n        noatime=False,\n        noctime=False,\n        noflags=False,\n        noacls=False,\n        noxattrs=False,\n        progress=False,\n        chunker_params=CHUNKER_PARAMS,\n        start=None,\n        start_monotonic=None,\n        end=None,\n        log_json=False,\n        iec=False,\n    ):\n        self.cwd = os.getcwd()\n        assert isinstance(manifest, Manifest)\n        self.manifest = manifest\n        self.key = manifest.repo_objs.key\n        self.repo_objs = manifest.repo_objs\n        self.repository = manifest.repository\n        self.cache = cache\n        self.stats = Statistics(output_json=log_json, iec=iec)\n        self.iec = iec\n        self.show_progress = progress\n        self.name = name  # overwritten later with name from archive metadata\n        self.name_in_manifest = name  # can differ from .name later (if borg check fixed duplicate archive names)\n        self.comment = None\n        self.tam_verified = False\n        self.numeric_ids = numeric_ids\n        self.noatime = noatime\n        self.noctime = noctime\n        self.noflags = noflags\n        self.noacls = noacls\n        self.noxattrs = noxattrs\n        assert (start is None) == (\n            start_monotonic is None\n        ), \"Logic error: if start is given, start_monotonic must be given as well and vice versa.\"\n        if start is None:\n            start = archive_ts_now()\n            start_monotonic = time.monotonic()\n        self.chunker_params = chunker_params\n        self.start = start\n        self.start_monotonic = start_monotonic\n        if end is None:\n            end = archive_ts_now()\n        self.end = end\n        self.pipeline = DownloadPipeline(self.repository, self.repo_objs)\n        self.create = create\n        if self.create:\n            self.items_buffer = CacheChunkBuffer(self.cache, self.key, self.stats)\n            if name in manifest.archives:\n                raise self.AlreadyExists(name)\n            i = 0\n            while True:\n                self.checkpoint_name = \"{}.checkpoint{}\".format(name, i and (\".%d\" % i) or \"\")\n                if self.checkpoint_name not in manifest.archives:\n                    break\n                i += 1\n        else:\n            info = self.manifest.archives.get(name)\n            if info is None:\n                raise self.DoesNotExist(name)\n            self.load(info.id)\n\n    def _load_meta(self, id):\n        cdata = self.repository.get(id)\n        _, data = self.repo_objs.parse(id, cdata)\n        # we do not require TAM for archives, otherwise we can not even borg list a repo with old archives.\n        archive, self.tam_verified, _ = self.key.unpack_and_verify_archive(data, force_tam_not_required=True)\n        metadata = ArchiveItem(internal_dict=archive)\n        if metadata.version not in (1, 2):  # legacy: still need to read v1 archives\n            raise Exception(\"Unknown archive metadata version\")\n        # note: metadata.items must not get written to disk!\n        metadata.items = archive_get_items(metadata, repo_objs=self.repo_objs, repository=self.repository)\n        return metadata\n\n    def load(self, id):\n        self.id = id\n        self.metadata = self._load_meta(self.id)\n        self.name = self.metadata.name\n        self.comment = self.metadata.get(\"comment\", \"\")\n\n    @property\n    def ts(self):\n        \"\"\"Timestamp of archive creation (start) in UTC\"\"\"\n        ts = self.metadata.time\n        return parse_timestamp(ts)\n\n    @property\n    def ts_end(self):\n        \"\"\"Timestamp of archive creation (end) in UTC\"\"\"\n        # fall back to time if there is no time_end present in metadata\n        ts = self.metadata.get(\"time_end\") or self.metadata.time\n        return parse_timestamp(ts)\n\n    @property\n    def fpr(self):\n        return bin_to_hex(self.id)\n\n    @property\n    def duration(self):\n        return format_timedelta(self.end - self.start)\n\n    @property\n    def duration_from_meta(self):\n        return format_timedelta(self.ts_end - self.ts)\n\n    def info(self):\n        if self.create:\n            stats = self.stats\n            start = self.start\n            end = self.end\n        else:\n            stats = self.calc_stats(self.cache)\n            start = self.ts\n            end = self.ts_end\n        info = {\n            \"name\": self.name,\n            \"id\": self.fpr,\n            \"start\": OutputTimestamp(start),\n            \"end\": OutputTimestamp(end),\n            \"duration\": (end - start).total_seconds(),\n            \"stats\": stats.as_dict(),\n        }\n        if self.create:\n            info[\"command_line\"] = join_cmd(sys.argv)\n        else:\n            info.update(\n                {\n                    \"command_line\": self.metadata.command_line,\n                    \"hostname\": self.metadata.hostname,\n                    \"username\": self.metadata.username,\n                    \"comment\": self.metadata.get(\"comment\", \"\"),\n                    \"chunker_params\": self.metadata.get(\"chunker_params\", \"\"),\n                }\n            )\n        return info\n\n    def __str__(self):\n        return \"\"\"\\\nRepository: {location}\nArchive name: {0.name}\nArchive fingerprint: {0.fpr}\nTime (start): {start}\nTime (end):   {end}\nDuration: {0.duration}\n\"\"\".format(\n            self,\n            start=OutputTimestamp(self.start),\n            end=OutputTimestamp(self.end),\n            location=self.repository._location.canonical_path(),\n        )\n\n    def __repr__(self):\n        return \"Archive(%r)\" % self.name\n\n    def item_filter(self, item, filter=None):\n        return filter(item) if filter else True\n\n    def iter_items(self, filter=None, preload=False):\n        # note: when calling this with preload=True, later fetch_many() must be called with\n        # is_preloaded=True or the RemoteRepository code will leak memory!\n        yield from self.pipeline.unpack_many(\n            self.metadata.items, preload=preload, filter=lambda item: self.item_filter(item, filter)\n        )\n\n    def add_item(self, item, show_progress=True, stats=None):\n        if show_progress and self.show_progress:\n            if stats is None:\n                stats = self.stats\n            stats.show_progress(item=item, dt=0.2)\n        self.items_buffer.add(item)\n\n    def prepare_checkpoint(self):\n        # we need to flush the archive metadata stream to repo chunks, so that\n        # we have the metadata stream chunks WITHOUT the part file item we add later.\n        # The part file item will then get into its own metadata stream chunk, which we\n        # can easily NOT include into the next checkpoint or the final archive.\n        self.items_buffer.flush(flush=True)\n        # remember the current state of self.chunks, which corresponds to the flushed chunks\n        self.items_buffer.save_chunks_state()\n\n    def write_checkpoint(self):\n        metadata = self.save(self.checkpoint_name)\n        # that .save() has committed the repo.\n        # at next commit, we won't need this checkpoint archive any more because we will then\n        # have either a newer checkpoint archive or the final archive.\n        # so we can already remove it here, the next .save() will then commit this cleanup.\n        # remove its manifest entry, remove its ArchiveItem chunk, remove its item_ptrs chunks:\n        del self.manifest.archives[self.checkpoint_name]\n        self.cache.chunk_decref(self.id, self.stats)\n        for id in metadata.item_ptrs:\n            self.cache.chunk_decref(id, self.stats)\n        # also get rid of that part item, we do not want to have it in next checkpoint or final archive\n        tail_chunks = self.items_buffer.restore_chunks_state()\n        # tail_chunks contain the tail of the archive items metadata stream, not needed for next commit.\n        for id in tail_chunks:\n            self.cache.chunk_decref(id, self.stats)\n\n    def save(self, name=None, comment=None, timestamp=None, stats=None, additional_metadata=None):\n        name = name or self.name\n        if name in self.manifest.archives:\n            raise self.AlreadyExists(name)\n        self.items_buffer.flush(flush=True)\n        item_ptrs = archive_put_items(\n            self.items_buffer.chunks, repo_objs=self.repo_objs, cache=self.cache, stats=self.stats\n        )\n        duration = timedelta(seconds=time.monotonic() - self.start_monotonic)\n        if timestamp is None:\n            end = archive_ts_now()\n            start = end - duration\n        else:\n            start = timestamp\n            end = start + duration\n        self.start = start\n        self.end = end\n        metadata = {\n            \"version\": 2,\n            \"name\": name,\n            \"comment\": comment or \"\",\n            \"item_ptrs\": item_ptrs,  # see #1473\n            \"command_line\": join_cmd(sys.argv),\n            \"hostname\": hostname,\n            \"username\": getuser(),\n            \"time\": start.isoformat(timespec=\"microseconds\"),\n            \"time_end\": end.isoformat(timespec=\"microseconds\"),\n            \"chunker_params\": self.chunker_params,\n        }\n        # we always want to create archives with the addtl. metadata (nfiles, etc.),\n        # because borg info relies on them. so, either use the given stats (from args)\n        # or fall back to self.stats if it was not given.\n        stats = stats or self.stats\n        metadata.update({\"size\": stats.osize, \"nfiles\": stats.nfiles})\n        metadata.update(additional_metadata or {})\n        metadata = ArchiveItem(metadata)\n        data = self.key.pack_and_authenticate_metadata(metadata.as_dict(), context=b\"archive\")\n        self.id = self.repo_objs.id_hash(data)\n        try:\n            self.cache.add_chunk(self.id, {}, data, stats=self.stats)\n        except IntegrityError as err:\n            err_msg = str(err)\n            # hack to avoid changing the RPC protocol by introducing new (more specific) exception class\n            if \"More than allowed put data\" in err_msg:\n                raise Error(\"%s - archive too big (issue #1473)!\" % err_msg)\n            else:\n                raise\n        while self.repository.async_response(wait=True) is not None:\n            pass\n        self.manifest.archives[name] = (self.id, metadata.time)\n        self.manifest.write()\n        self.repository.commit(compact=False)\n        self.cache.commit()\n        return metadata\n\n    def calc_stats(self, cache, want_unique=True):\n        if not want_unique:\n            unique_size = 0\n        else:\n\n            def add(id):\n                entry = cache.chunks[id]\n                archive_index.add(id, 1, entry.size)\n\n            archive_index = ChunkIndex()\n            sync = CacheSynchronizer(archive_index)\n            add(self.id)\n            # we must escape any % char in the archive name, because we use it in a format string, see #6500\n            arch_name_escd = self.name.replace(\"%\", \"%%\")\n            pi = ProgressIndicatorPercent(\n                total=len(self.metadata.items),\n                msg=\"Calculating statistics for archive %s ... %%3.0f%%%%\" % arch_name_escd,\n                msgid=\"archive.calc_stats\",\n            )\n            for id, chunk in zip(self.metadata.items, self.repository.get_many(self.metadata.items)):\n                pi.show(increase=1)\n                add(id)\n                _, data = self.repo_objs.parse(id, chunk)\n                sync.feed(data)\n            unique_size = archive_index.stats_against(cache.chunks)[1]\n            pi.finish()\n\n        stats = Statistics(iec=self.iec)\n        stats.usize = unique_size\n        stats.nfiles = self.metadata.nfiles\n        stats.osize = self.metadata.size\n        return stats\n\n    @contextmanager\n    def extract_helper(self, item, path, hlm, *, dry_run=False):\n        hardlink_set = False\n        # Hard link?\n        if \"hlid\" in item:\n            link_target = hlm.retrieve(id=item.hlid)\n            if link_target is not None and has_link:\n                if not dry_run:\n                    # another hardlink to same inode (same hlid) was extracted previously, just link to it\n                    with backup_io(\"link\"):\n                        os.link(link_target, path, follow_symlinks=False)\n                hardlink_set = True\n        yield hardlink_set\n        if not hardlink_set:\n            if \"hlid\" in item and has_link:\n                # Update entry with extracted item path, so that following hardlinks don't extract twice.\n                # We have hardlinking support, so we will hardlink not extract.\n                hlm.remember(id=item.hlid, info=path)\n            else:\n                # Broken platform with no hardlinking support.\n                # In this case, we *want* to extract twice, because there is no other way.\n                pass\n\n    def extract_item(\n        self,\n        item,\n        *,\n        restore_attrs=True,\n        dry_run=False,\n        stdout=False,\n        sparse=False,\n        hlm=None,\n        pi=None,\n        continue_extraction=False,\n    ):\n        \"\"\"\n        Extract archive item.\n\n        :param item: the item to extract\n        :param restore_attrs: restore file attributes\n        :param dry_run: do not write any data\n        :param stdout: write extracted data to stdout\n        :param sparse: write sparse files (chunk-granularity, independent of the original being sparse)\n        :param hlm: maps hlid to link_target for extracting subtrees with hardlinks correctly\n        :param pi: ProgressIndicatorPercent (or similar) for file extraction progress (in bytes)\n        :param continue_extraction: continue a previously interrupted extraction of same archive\n        \"\"\"\n\n        def same_item(item, st):\n            \"\"\"is the archived item the same as the fs item at same path with stat st?\"\"\"\n            if not stat.S_ISREG(st.st_mode):\n                # we only \"optimize\" for regular files.\n                # other file types are less frequent and have no content extraction we could \"optimize away\".\n                return False\n            if item.mode != st.st_mode or item.size != st.st_size:\n                # the size check catches incomplete previous file extraction\n                return False\n            if item.get(\"mtime\") != st.st_mtime_ns:\n                # note: mtime is \"extracted\" late, after xattrs and ACLs, but before flags.\n                return False\n            # this is good enough for the intended use case:\n            # continuing an extraction of same archive that initially started in an empty directory.\n            # there is a very small risk that \"bsdflags\" of one file are wrong:\n            # if a previous extraction was interrupted between setting the mtime and setting non-default flags.\n            return True\n\n        has_damaged_chunks = \"chunks_healthy\" in item\n        if dry_run or stdout:\n            with self.extract_helper(item, \"\", hlm, dry_run=dry_run or stdout) as hardlink_set:\n                if not hardlink_set:\n                    # it does not really set hardlinks due to dry_run, but we need to behave same\n                    # as non-dry_run concerning fetching preloaded chunks from the pipeline or\n                    # it would get stuck.\n                    if \"chunks\" in item:\n                        item_chunks_size = 0\n                        for data in self.pipeline.fetch_many([c.id for c in item.chunks], is_preloaded=True):\n                            if pi:\n                                pi.show(increase=len(data), info=[remove_surrogates(item.path)])\n                            if stdout:\n                                sys.stdout.buffer.write(data)\n                            item_chunks_size += len(data)\n                        if stdout:\n                            sys.stdout.buffer.flush()\n                        if \"size\" in item:\n                            item_size = item.size\n                            if item_size != item_chunks_size:\n                                raise BackupError(\n                                    \"Size inconsistency detected: size {}, chunks size {}\".format(\n                                        item_size, item_chunks_size\n                                    )\n                                )\n            if has_damaged_chunks:\n                raise BackupError(\"File has damaged (all-zero) chunks. Try running borg check --repair.\")\n            return\n\n        dest = self.cwd\n        path = os.path.join(dest, item.path)\n        # Attempt to remove existing files, ignore errors on failure\n        try:\n            st = os.stat(path, follow_symlinks=False)\n            if continue_extraction and same_item(item, st):\n                return  # done! we already have fully extracted this file in a previous run.\n            elif stat.S_ISDIR(st.st_mode):\n                os.rmdir(path)\n            else:\n                os.unlink(path)\n        except UnicodeEncodeError:\n            raise self.IncompatibleFilesystemEncodingError(path, sys.getfilesystemencoding()) from None\n        except OSError:\n            pass\n\n        def make_parent(path):\n            parent_dir = os.path.dirname(path)\n            if not os.path.exists(parent_dir):\n                os.makedirs(parent_dir)\n\n        mode = item.mode\n        if stat.S_ISREG(mode):\n            with backup_io(\"makedirs\"):\n                make_parent(path)\n            with self.extract_helper(item, path, hlm) as hardlink_set:\n                if hardlink_set:\n                    return\n                with backup_io(\"open\"):\n                    fd = open(path, \"wb\")\n                with fd:\n                    ids = [c.id for c in item.chunks]\n                    for data in self.pipeline.fetch_many(ids, is_preloaded=True):\n                        if pi:\n                            pi.show(increase=len(data), info=[remove_surrogates(item.path)])\n                        with backup_io(\"write\"):\n                            if sparse and zeros.startswith(data):\n                                # all-zero chunk: create a hole in a sparse file\n                                fd.seek(len(data), 1)\n                            else:\n                                fd.write(data)\n                    with backup_io(\"truncate_and_attrs\"):\n                        pos = item_chunks_size = fd.tell()\n                        fd.truncate(pos)\n                        fd.flush()\n                        self.restore_attrs(path, item, fd=fd.fileno())\n                if \"size\" in item:\n                    item_size = item.size\n                    if item_size != item_chunks_size:\n                        raise BackupError(\n                            f\"Size inconsistency detected: size {item_size}, chunks size {item_chunks_size}\"\n                        )\n                if has_damaged_chunks:\n                    raise BackupError(\"File has damaged (all-zero) chunks. Try running borg check --repair.\")\n            return\n        with backup_io:\n            # No repository access beyond this point.\n            if stat.S_ISDIR(mode):\n                make_parent(path)\n                if not os.path.exists(path):\n                    os.mkdir(path)\n                if restore_attrs:\n                    self.restore_attrs(path, item)\n            elif stat.S_ISLNK(mode):\n                make_parent(path)\n                with self.extract_helper(item, path, hlm) as hardlink_set:\n                    if hardlink_set:\n                        # unusual, but possible: this is a hardlinked symlink.\n                        return\n                    target = item.target\n                    try:\n                        os.symlink(target, path)\n                    except UnicodeEncodeError:\n                        raise self.IncompatibleFilesystemEncodingError(target, sys.getfilesystemencoding()) from None\n                    self.restore_attrs(path, item, symlink=True)\n            elif stat.S_ISFIFO(mode):\n                make_parent(path)\n                with self.extract_helper(item, path, hlm) as hardlink_set:\n                    if hardlink_set:\n                        return\n                    os.mkfifo(path)\n                    self.restore_attrs(path, item)\n            elif stat.S_ISCHR(mode) or stat.S_ISBLK(mode):\n                make_parent(path)\n                with self.extract_helper(item, path, hlm) as hardlink_set:\n                    if hardlink_set:\n                        return\n                    os.mknod(path, item.mode, item.rdev)\n                    self.restore_attrs(path, item)\n            else:\n                raise Exception(\"Unknown archive item type %r\" % item.mode)\n\n    def restore_attrs(self, path, item, symlink=False, fd=None):\n        \"\"\"\n        Restore filesystem attributes on *path* (*fd*) from *item*.\n\n        Does not access the repository.\n        \"\"\"\n        backup_io.op = \"attrs\"\n        # This code is a bit of a mess due to OS specific differences.\n        if not is_win32:\n            # by using uid_default = -1 and gid_default = -1, they will not be restored if\n            # the archived item has no information about them.\n            uid, gid = get_item_uid_gid(item, numeric=self.numeric_ids, uid_default=-1, gid_default=-1)\n            # if uid and/or gid is -1, chown will keep it as is and not change it.\n            try:\n                if fd:\n                    os.fchown(fd, uid, gid)\n                else:\n                    os.chown(path, uid, gid, follow_symlinks=False)\n            except OSError:\n                pass\n            if fd:\n                os.fchmod(fd, item.mode)\n            else:\n                # To check whether a particular function in the os module accepts False for its\n                # follow_symlinks parameter, the in operator on supports_follow_symlinks should be\n                # used. However, os.chmod is special as some platforms without a working lchmod() do\n                # have fchmodat(), which has a flag that makes it behave like lchmod(). fchmodat()\n                # is ignored when deciding whether or not os.chmod should be set in\n                # os.supports_follow_symlinks. Work around this by using try/except.\n                try:\n                    os.chmod(path, item.mode, follow_symlinks=False)\n                except NotImplementedError:\n                    if not symlink:\n                        os.chmod(path, item.mode)\n            if not self.noacls:\n                acl_set(path, item, self.numeric_ids, fd=fd)\n            if not self.noxattrs and \"xattrs\" in item:\n                # chown removes Linux capabilities, so set the extended attributes at the end, after chown,\n                # since they include the Linux capabilities in the \"security.capability\" attribute.\n                warning = xattr.set_all(fd or path, item.xattrs, follow_symlinks=False)\n                if warning:\n                    set_ec(EXIT_WARNING)\n            # set timestamps rather late\n            mtime = item.mtime\n            atime = item.atime if \"atime\" in item else mtime\n            if \"birthtime\" in item:\n                birthtime = item.birthtime\n                try:\n                    # This should work on FreeBSD, NetBSD, and Darwin and be harmless on other platforms.\n                    # See utimes(2) on either of the BSDs for details.\n                    if fd:\n                        os.utime(fd, None, ns=(atime, birthtime))\n                    else:\n                        os.utime(path, None, ns=(atime, birthtime), follow_symlinks=False)\n                except OSError:\n                    # some systems don't support calling utime on a symlink\n                    pass\n            try:\n                if fd:\n                    os.utime(fd, None, ns=(atime, mtime))\n                else:\n                    os.utime(path, None, ns=(atime, mtime), follow_symlinks=False)\n            except OSError:\n                # some systems don't support calling utime on a symlink\n                pass\n            # bsdflags include the immutable flag and need to be set last:\n            if not self.noflags and \"bsdflags\" in item:\n                try:\n                    set_flags(path, item.bsdflags, fd=fd)\n                except OSError:\n                    pass\n        else:  # win32\n            # set timestamps rather late\n            mtime = item.mtime\n            atime = item.atime if \"atime\" in item else mtime\n            try:\n                # note: no fd support on win32\n                os.utime(path, None, ns=(atime, mtime))\n            except OSError:\n                # some systems don't support calling utime on a symlink\n                pass\n\n    def set_meta(self, key, value):\n        metadata = self._load_meta(self.id)\n        setattr(metadata, key, value)\n        if \"items\" in metadata:\n            del metadata.items\n        data = self.key.pack_and_authenticate_metadata(metadata.as_dict(), context=b\"archive\")\n        new_id = self.key.id_hash(data)\n        self.cache.add_chunk(new_id, {}, data, stats=self.stats)\n        self.manifest.archives[self.name] = (new_id, metadata.time)\n        self.cache.chunk_decref(self.id, self.stats)\n        self.id = new_id\n\n    def rename(self, name):\n        if name in self.manifest.archives:\n            raise self.AlreadyExists(name)\n        oldname = self.name\n        self.name = name\n        self.set_meta(\"name\", name)\n        del self.manifest.archives[oldname]\n\n    def delete(self, stats, progress=False, forced=False):\n        class ChunksIndexError(Error):\n            \"\"\"Chunk ID {} missing from chunks index, corrupted chunks index - aborting transaction.\"\"\"\n\n        exception_ignored = object()\n\n        def fetch_async_response(wait=True):\n            try:\n                return self.repository.async_response(wait=wait)\n            except Repository.ObjectNotFound:\n                nonlocal error\n                # object not in repo - strange, but we wanted to delete it anyway.\n                if forced == 0:\n                    raise\n                error = True\n                return exception_ignored  # must not return None here\n\n        def chunk_decref(id, stats):\n            try:\n                self.cache.chunk_decref(id, stats, wait=False)\n            except KeyError:\n                cid = bin_to_hex(id)\n                raise ChunksIndexError(cid)\n            else:\n                fetch_async_response(wait=False)\n\n        error = False\n        try:\n            unpacker = msgpack.Unpacker(use_list=False)\n            items_ids = self.metadata.items\n            pi = ProgressIndicatorPercent(\n                total=len(items_ids), msg=\"Decrementing references %3.0f%%\", msgid=\"archive.delete\"\n            )\n            for i, (items_id, data) in enumerate(zip(items_ids, self.repository.get_many(items_ids))):\n                if progress:\n                    pi.show(i)\n                _, data = self.repo_objs.parse(items_id, data)\n                unpacker.feed(data)\n                chunk_decref(items_id, stats)\n                try:\n                    for item in unpacker:\n                        item = Item(internal_dict=item)\n                        if \"chunks\" in item:\n                            for chunk_id, size in item.chunks:\n                                chunk_decref(chunk_id, stats)\n                except (TypeError, ValueError):\n                    # if items metadata spans multiple chunks and one chunk got dropped somehow,\n                    # it could be that unpacker yields bad types\n                    if forced == 0:\n                        raise\n                    error = True\n            if progress:\n                pi.finish()\n        except (msgpack.UnpackException, Repository.ObjectNotFound):\n            # items metadata corrupted\n            if forced == 0:\n                raise\n            error = True\n\n        # delete the blocks that store all the references that end up being loaded into metadata.items:\n        for id in self.metadata.item_ptrs:\n            chunk_decref(id, stats)\n\n        # in forced delete mode, we try hard to delete at least the manifest entry,\n        # if possible also the archive superblock, even if processing the items raises\n        # some harmless exception.\n        chunk_decref(self.id, stats)\n        del self.manifest.archives[self.name]\n        while fetch_async_response(wait=True) is not None:\n            # we did async deletes, process outstanding results (== exceptions),\n            # so there is nothing pending when we return and our caller wants to commit.\n            pass\n        if error:\n            logger.warning(\"forced deletion succeeded, but the deleted archive was corrupted.\")\n            logger.warning(\"borg check --repair is required to free all space.\")\n\n    @staticmethod\n    def compare_archives_iter(\n        archive1: \"Archive\", archive2: \"Archive\", matcher=None, can_compare_chunk_ids=False\n    ) -> Iterator[ItemDiff]:\n        \"\"\"\n        Yields an ItemDiff instance describing changes/indicating equality.\n\n        :param matcher: PatternMatcher class to restrict results to only matching paths.\n        :param can_compare_chunk_ids: Whether --chunker-params are the same for both archives.\n        \"\"\"\n\n        def compare_items(path: str, item1: Item, item2: Item):\n            return ItemDiff(\n                path,\n                item1,\n                item2,\n                archive1.pipeline.fetch_many([c.id for c in item1.get(\"chunks\", [])]),\n                archive2.pipeline.fetch_many([c.id for c in item2.get(\"chunks\", [])]),\n                can_compare_chunk_ids=can_compare_chunk_ids,\n            )\n\n        orphans_archive1: OrderedDict[str, Item] = OrderedDict()\n        orphans_archive2: OrderedDict[str, Item] = OrderedDict()\n\n        assert matcher is not None, \"matcher must be set\"\n\n        for item1, item2 in zip_longest(\n            archive1.iter_items(lambda item: matcher.match(item.path)),\n            archive2.iter_items(lambda item: matcher.match(item.path)),\n        ):\n            if item1 and item2 and item1.path == item2.path:\n                yield compare_items(item1.path, item1, item2)\n                continue\n            if item1:\n                matching_orphan = orphans_archive2.pop(item1.path, None)\n                if matching_orphan:\n                    yield compare_items(item1.path, item1, matching_orphan)\n                else:\n                    orphans_archive1[item1.path] = item1\n            if item2:\n                matching_orphan = orphans_archive1.pop(item2.path, None)\n                if matching_orphan:\n                    yield compare_items(matching_orphan.path, matching_orphan, item2)\n                else:\n                    orphans_archive2[item2.path] = item2\n        # At this point orphans_* contain items that had no matching partner in the other archive\n        for added in orphans_archive2.values():\n            path = added.path\n            deleted_item = Item.create_deleted(path)\n            yield compare_items(path, deleted_item, added)\n        for deleted in orphans_archive1.values():\n            path = deleted.path\n            deleted_item = Item.create_deleted(path)\n            yield compare_items(path, deleted, deleted_item)\n\n\nclass MetadataCollector:\n    def __init__(self, *, noatime, noctime, nobirthtime, numeric_ids, noflags, noacls, noxattrs):\n        self.noatime = noatime\n        self.noctime = noctime\n        self.numeric_ids = numeric_ids\n        self.noflags = noflags\n        self.noacls = noacls\n        self.noxattrs = noxattrs\n        self.nobirthtime = nobirthtime\n\n    def stat_simple_attrs(self, st):\n        attrs = {}\n        attrs[\"mode\"] = st.st_mode\n        # borg can work with archives only having mtime (very old borg archives do not have\n        # atime/ctime). it can be useful to omit atime/ctime, if they change without the\n        # file content changing - e.g. to get better metadata deduplication.\n        attrs[\"mtime\"] = safe_ns(st.st_mtime_ns)\n        if not self.noatime:\n            attrs[\"atime\"] = safe_ns(st.st_atime_ns)\n        if not self.noctime:\n            attrs[\"ctime\"] = safe_ns(st.st_ctime_ns)\n        if not self.nobirthtime and hasattr(st, \"st_birthtime\"):\n            # sadly, there's no stat_result.st_birthtime_ns\n            attrs[\"birthtime\"] = safe_ns(int(st.st_birthtime * 10**9))\n        attrs[\"uid\"] = st.st_uid\n        attrs[\"gid\"] = st.st_gid\n        if not self.numeric_ids:\n            user = uid2user(st.st_uid)\n            if user is not None:\n                attrs[\"user\"] = user\n            group = gid2group(st.st_gid)\n            if group is not None:\n                attrs[\"group\"] = group\n        return attrs\n\n    def stat_ext_attrs(self, st, path, fd=None):\n        attrs = {}\n        if not self.noflags:\n            with backup_io(\"extended stat (flags)\"):\n                flags = get_flags(path, st, fd=fd)\n            attrs[\"bsdflags\"] = flags\n        if not self.noxattrs:\n            with backup_io(\"extended stat (xattrs)\"):\n                xattrs = xattr.get_all(fd or path, follow_symlinks=False)\n            attrs[\"xattrs\"] = StableDict(xattrs)\n        if not self.noacls:\n            with backup_io(\"extended stat (ACLs)\"):\n                acl_get(path, attrs, st, self.numeric_ids, fd=fd)\n        return attrs\n\n    def stat_attrs(self, st, path, fd=None):\n        attrs = self.stat_simple_attrs(st)\n        attrs.update(self.stat_ext_attrs(st, path, fd=fd))\n        return attrs\n\n\n# remember a few recently used all-zero chunk hashes in this mapping.\n# (hash_func, chunk_length) -> chunk_hash\n# we play safe and have the hash_func in the mapping key, in case we\n# have different hash_funcs within the same borg run.\nzero_chunk_ids = LRUCache(10)  # type: ignore[var-annotated]\n\n\ndef cached_hash(chunk, id_hash):\n    allocation = chunk.meta[\"allocation\"]\n    if allocation == CH_DATA:\n        data = chunk.data\n        chunk_id = id_hash(data)\n    elif allocation in (CH_HOLE, CH_ALLOC):\n        size = chunk.meta[\"size\"]\n        assert size <= len(zeros)\n        data = memoryview(zeros)[:size]\n        try:\n            chunk_id = zero_chunk_ids[(id_hash, size)]\n        except KeyError:\n            chunk_id = id_hash(data)\n            zero_chunk_ids[(id_hash, size)] = chunk_id\n    else:\n        raise ValueError(\"unexpected allocation type\")\n    return chunk_id, data\n\n\nclass ChunksProcessor:\n    # Processes an iterator of chunks for an Item\n\n    def __init__(\n        self,\n        *,\n        key,\n        cache,\n        add_item,\n        prepare_checkpoint,\n        write_checkpoint,\n        checkpoint_interval,\n        checkpoint_volume,\n        rechunkify,\n    ):\n        self.key = key\n        self.cache = cache\n        self.add_item = add_item\n        self.prepare_checkpoint = prepare_checkpoint\n        self.write_checkpoint = write_checkpoint\n        self.rechunkify = rechunkify\n        # time interval based checkpointing\n        self.checkpoint_interval = checkpoint_interval\n        self.last_checkpoint = time.monotonic()\n        # file content volume based checkpointing\n        self.checkpoint_volume = checkpoint_volume\n        self.current_volume = 0\n        self.last_volume_checkpoint = 0\n\n    def write_part_file(self, item):\n        self.prepare_checkpoint()\n        item = Item(internal_dict=item.as_dict())\n        # for borg recreate, we already have a size member in the source item (giving the total file size),\n        # but we consider only a part of the file here, thus we must recompute the size from the chunks:\n        item.get_size(memorize=True, from_chunks=True)\n        item.path += \".borg_part\"\n        self.add_item(item, show_progress=False)\n        self.write_checkpoint()\n\n    def maybe_checkpoint(self, item):\n        checkpoint_done = False\n        sig_int_triggered = sig_int and sig_int.action_triggered()\n        if (\n            sig_int_triggered\n            or (self.checkpoint_interval and time.monotonic() - self.last_checkpoint > self.checkpoint_interval)\n            or (self.checkpoint_volume and self.current_volume - self.last_volume_checkpoint >= self.checkpoint_volume)\n        ):\n            if sig_int_triggered:\n                logger.info(\"checkpoint requested: starting checkpoint creation...\")\n            self.write_part_file(item)\n            checkpoint_done = True\n            self.last_checkpoint = time.monotonic()\n            self.last_volume_checkpoint = self.current_volume\n            if sig_int_triggered:\n                sig_int.action_completed()\n                logger.info(\"checkpoint requested: finished checkpoint creation!\")\n        return checkpoint_done  # whether a checkpoint archive was created\n\n    def process_file_chunks(self, item, cache, stats, show_progress, chunk_iter, chunk_processor=None):\n        if not chunk_processor:\n\n            def chunk_processor(chunk):\n                started_hashing = time.monotonic()\n                chunk_id, data = cached_hash(chunk, self.key.id_hash)\n                stats.hashing_time += time.monotonic() - started_hashing\n                chunk_entry = cache.add_chunk(chunk_id, {}, data, stats=stats, wait=False)\n                self.cache.repository.async_response(wait=False)\n                return chunk_entry\n\n        item.chunks = []\n        # if we rechunkify, we'll get a fundamentally different chunks list, thus we need\n        # to get rid of .chunks_healthy, as it might not correspond to .chunks any more.\n        if self.rechunkify and \"chunks_healthy\" in item:\n            del item.chunks_healthy\n        for chunk in chunk_iter:\n            chunk_entry = chunk_processor(chunk)\n            item.chunks.append(chunk_entry)\n            self.current_volume += chunk_entry[1]\n            if show_progress:\n                stats.show_progress(item=item, dt=0.2)\n            self.maybe_checkpoint(item)\n\n\nclass FilesystemObjectProcessors:\n    # When ported to threading, then this doesn't need chunker, cache, key any more.\n    # write_checkpoint should then be in the item buffer,\n    # and process_file becomes a callback passed to __init__.\n\n    def __init__(\n        self,\n        *,\n        metadata_collector,\n        cache,\n        key,\n        add_item,\n        process_file_chunks,\n        chunker_params,\n        show_progress,\n        sparse,\n        log_json,\n        iec,\n        file_status_printer=None,\n    ):\n        self.metadata_collector = metadata_collector\n        self.cache = cache\n        self.key = key\n        self.add_item = add_item\n        self.process_file_chunks = process_file_chunks\n        self.show_progress = show_progress\n        self.print_file_status = file_status_printer or (lambda *args: None)\n\n        self.hlm = HardLinkManager(id_type=tuple, info_type=(list, type(None)))  # (dev, ino) -> chunks or None\n        self.stats = Statistics(output_json=log_json, iec=iec)  # threading: done by cache (including progress)\n        self.cwd = os.getcwd()\n        self.chunker = get_chunker(*chunker_params, seed=key.chunk_seed, sparse=sparse)\n\n    @contextmanager\n    def create_helper(self, path, st, status=None, hardlinkable=True):\n        sanitized_path = remove_dotdot_prefixes(path)\n        item = Item(path=sanitized_path)\n        hardlinked = hardlinkable and st.st_nlink > 1\n        hl_chunks = None\n        update_map = False\n        if hardlinked:\n            status = \"h\"  # hardlink\n            nothing = object()\n            chunks = self.hlm.retrieve(id=(st.st_ino, st.st_dev), default=nothing)\n            if chunks is nothing:\n                update_map = True\n            elif chunks is not None:\n                hl_chunks = chunks\n            item.hlid = self.hlm.hardlink_id_from_inode(ino=st.st_ino, dev=st.st_dev)\n        yield item, status, hardlinked, hl_chunks\n        self.add_item(item, stats=self.stats)\n        if update_map:\n            # remember the hlid of this fs object and if the item has chunks,\n            # also remember them, so we do not have to re-chunk a hardlink.\n            chunks = item.chunks if \"chunks\" in item else None\n            self.hlm.remember(id=(st.st_ino, st.st_dev), info=chunks)\n\n    def process_dir_with_fd(self, *, path, fd, st):\n        with self.create_helper(path, st, \"d\", hardlinkable=False) as (item, status, hardlinked, hl_chunks):\n            item.update(self.metadata_collector.stat_attrs(st, path, fd=fd))\n            return status\n\n    def process_dir(self, *, path, parent_fd, name, st):\n        with self.create_helper(path, st, \"d\", hardlinkable=False) as (item, status, hardlinked, hl_chunks):\n            with OsOpen(path=path, parent_fd=parent_fd, name=name, flags=flags_dir, noatime=True, op=\"dir_open\") as fd:\n                # fd is None for directories on windows, in that case a race condition check is not possible.\n                if fd is not None:\n                    with backup_io(\"fstat\"):\n                        st = stat_update_check(st, os.fstat(fd))\n                item.update(self.metadata_collector.stat_attrs(st, path, fd=fd))\n                return status\n\n    def process_fifo(self, *, path, parent_fd, name, st):\n        with self.create_helper(path, st, \"f\") as (item, status, hardlinked, hl_chunks):  # fifo\n            with OsOpen(path=path, parent_fd=parent_fd, name=name, flags=flags_normal, noatime=True) as fd:\n                with backup_io(\"fstat\"):\n                    st = stat_update_check(st, os.fstat(fd))\n                item.update(self.metadata_collector.stat_attrs(st, path, fd=fd))\n                return status\n\n    def process_dev(self, *, path, parent_fd, name, st, dev_type):\n        with self.create_helper(path, st, dev_type) as (item, status, hardlinked, hl_chunks):  # char/block device\n            # looks like we can not work fd-based here without causing issues when trying to open/close the device\n            with backup_io(\"stat\"):\n                st = stat_update_check(st, os_stat(path=path, parent_fd=parent_fd, name=name, follow_symlinks=False))\n            item.rdev = st.st_rdev\n            item.update(self.metadata_collector.stat_attrs(st, path))\n            return status\n\n    def process_symlink(self, *, path, parent_fd, name, st):\n        with self.create_helper(path, st, \"s\", hardlinkable=True) as (item, status, hardlinked, hl_chunks):\n            fname = name if name is not None and parent_fd is not None else path\n            with backup_io(\"readlink\"):\n                target = os.readlink(fname, dir_fd=parent_fd)\n            item.target = target\n            item.update(self.metadata_collector.stat_attrs(st, path))  # can't use FD here?\n            return status\n\n    def process_pipe(self, *, path, cache, fd, mode, user=None, group=None):\n        status = \"i\"  # stdin (or other pipe)\n        self.print_file_status(status, path)\n        status = None  # we already printed the status\n        if user is not None:\n            uid = user2uid(user)\n            if uid is None:\n                raise Error(\"no such user: %s\" % user)\n        else:\n            uid = None\n        if group is not None:\n            gid = group2gid(group)\n            if gid is None:\n                raise Error(\"no such group: %s\" % group)\n        else:\n            gid = None\n        t = int(time.time()) * 1000000000\n        item = Item(path=path, mode=mode & 0o107777 | 0o100000, mtime=t, atime=t, ctime=t)  # forcing regular file mode\n        if user is not None:\n            item.user = user\n        if group is not None:\n            item.group = group\n        if uid is not None:\n            item.uid = uid\n        if gid is not None:\n            item.gid = gid\n        try:\n            self.process_file_chunks(\n                item, cache, self.stats, self.show_progress, backup_io_iter(self.chunker.chunkify(fd))\n            )\n        except BackupOSError:\n            # see comments in process_file's exception handler, same issue here.\n            for chunk in item.get(\"chunks\", []):\n                cache.chunk_decref(chunk.id, self.stats, wait=False)\n            raise\n        else:\n            item.get_size(memorize=True)\n            self.stats.nfiles += 1\n            self.add_item(item, stats=self.stats)\n            return status\n\n    def process_file(self, *, path, parent_fd, name, st, cache, flags=flags_normal, last_try=False):\n        with self.create_helper(path, st, None) as (item, status, hardlinked, hl_chunks):  # no status yet\n            with OsOpen(path=path, parent_fd=parent_fd, name=name, flags=flags, noatime=True) as fd:\n                with backup_io(\"fstat\"):\n                    st = stat_update_check(st, os.fstat(fd))\n                item.update(self.metadata_collector.stat_simple_attrs(st))\n                is_special_file = is_special(st.st_mode)\n                if is_special_file:\n                    # we process a special file like a regular file. reflect that in mode,\n                    # so it can be extracted / accessed in FUSE mount like a regular file.\n                    # this needs to be done early, so that part files also get the patched mode.\n                    item.mode = stat.S_IFREG | stat.S_IMODE(item.mode)\n                # we begin processing chunks now (writing or incref'ing them to the repository),\n                # which might require cleanup (see except-branch):\n                try:\n                    if hl_chunks is not None:  # create_helper gave us chunks from a previous hardlink\n                        item.chunks = []\n                        for chunk_id, chunk_size in hl_chunks:\n                            # process one-by-one, so we will know in item.chunks how far we got\n                            chunk_entry = cache.chunk_incref(chunk_id, self.stats)\n                            item.chunks.append(chunk_entry)\n                    else:  # normal case, no \"2nd+\" hardlink\n                        if not is_special_file:\n                            hashed_path = safe_encode(os.path.join(self.cwd, path))\n                            started_hashing = time.monotonic()\n                            path_hash = self.key.id_hash(hashed_path)\n                            self.stats.hashing_time += time.monotonic() - started_hashing\n                            known, ids = cache.file_known_and_unchanged(hashed_path, path_hash, st)\n                        else:\n                            # in --read-special mode, we may be called for special files.\n                            # there should be no information in the cache about special files processed in\n                            # read-special mode, but we better play safe as this was wrong in the past:\n                            hashed_path = path_hash = None\n                            known, ids = False, None\n                        if ids is not None:\n                            # Make sure all ids are available\n                            for id_ in ids:\n                                if not cache.seen_chunk(id_):\n                                    # cache said it is unmodified, but we lost a chunk: process file like modified\n                                    status = \"M\"\n                                    break\n                            else:\n                                item.chunks = []\n                                for chunk_id in ids:\n                                    # process one-by-one, so we will know in item.chunks how far we got\n                                    chunk_entry = cache.chunk_incref(chunk_id, self.stats)\n                                    item.chunks.append(chunk_entry)\n                                status = \"U\"  # regular file, unchanged\n                        else:\n                            status = \"M\" if known else \"A\"  # regular file, modified or added\n                        self.print_file_status(status, path)\n                        # Only chunkify the file if needed\n                        changed_while_backup = False\n                        if \"chunks\" not in item:\n                            with backup_io(\"read\"):\n                                self.process_file_chunks(\n                                    item,\n                                    cache,\n                                    self.stats,\n                                    self.show_progress,\n                                    backup_io_iter(self.chunker.chunkify(None, fd)),\n                                )\n                                self.stats.chunking_time = self.chunker.chunking_time\n                            if not is_win32:  # TODO for win32\n                                with backup_io(\"fstat2\"):\n                                    st2 = os.fstat(fd)\n                                # special files:\n                                # - fifos change naturally, because they are fed from the other side. no problem.\n                                # - blk/chr devices don't change ctime anyway.\n                                changed_while_backup = not is_special_file and st.st_ctime_ns != st2.st_ctime_ns\n                            if changed_while_backup:\n                                # regular file changed while we backed it up, might be inconsistent/corrupt!\n                                if last_try:\n                                    status = \"C\"  # crap! retries did not help.\n                                else:\n                                    raise BackupError(\"file changed while we read it!\")\n                            if not is_special_file and not changed_while_backup:\n                                # we must not memorize special files, because the contents of e.g. a\n                                # block or char device will change without its mtime/size/inode changing.\n                                # also, we must not memorize a potentially inconsistent/corrupt file that\n                                # changed while we backed it up.\n                                cache.memorize_file(hashed_path, path_hash, st, [c.id for c in item.chunks])\n                        self.stats.files_stats[status] += 1  # must be done late\n                        if not changed_while_backup:\n                            status = None  # we already called print_file_status\n                    self.stats.nfiles += 1\n                    item.update(self.metadata_collector.stat_ext_attrs(st, path, fd=fd))\n                    item.get_size(memorize=True)\n                    return status\n                except BackupOSError:\n                    # Something went wrong and we might need to clean up a bit.\n                    # Maybe we have already incref'ed some file content chunks in the repo -\n                    # but we will not add an item (see add_item in create_helper) and thus\n                    # they would be orphaned chunks in case that we commit the transaction.\n                    for chunk in item.get(\"chunks\", []):\n                        cache.chunk_decref(chunk.id, self.stats, wait=False)\n                    # Now that we have cleaned up the chunk references, we can re-raise the exception.\n                    # This will skip processing of this file, but might retry or continue with the next one.\n                    raise\n\n\nclass TarfileObjectProcessors:\n    def __init__(\n        self,\n        *,\n        cache,\n        key,\n        add_item,\n        process_file_chunks,\n        chunker_params,\n        show_progress,\n        log_json,\n        iec,\n        file_status_printer=None,\n    ):\n        self.cache = cache\n        self.key = key\n        self.add_item = add_item\n        self.process_file_chunks = process_file_chunks\n        self.show_progress = show_progress\n        self.print_file_status = file_status_printer or (lambda *args: None)\n\n        self.stats = Statistics(output_json=log_json, iec=iec)  # threading: done by cache (including progress)\n        self.chunker = get_chunker(*chunker_params, seed=key.chunk_seed, sparse=False)\n        self.hlm = HardLinkManager(id_type=str, info_type=list)  # path -> chunks\n\n    @contextmanager\n    def create_helper(self, tarinfo, status=None, type=None):\n        ph = tarinfo.pax_headers\n        if ph and \"BORG.item.version\" in ph:\n            assert ph[\"BORG.item.version\"] == \"1\"\n            meta_bin = base64.b64decode(ph[\"BORG.item.meta\"])\n            meta_dict = msgpack.unpackb(meta_bin, object_hook=StableDict)\n            item = Item(internal_dict=meta_dict)\n        else:\n\n            def s_to_ns(s):\n                return safe_ns(int(float(s) * 1e9))\n\n            item = Item(\n                path=make_path_safe(tarinfo.name),\n                mode=tarinfo.mode | type,\n                uid=tarinfo.uid,\n                gid=tarinfo.gid,\n                mtime=s_to_ns(tarinfo.mtime),\n            )\n            if tarinfo.uname:\n                item.user = tarinfo.uname\n            if tarinfo.gname:\n                item.group = tarinfo.gname\n            if ph:\n                # note: for mtime this is a bit redundant as it is already done by tarfile module,\n                #       but we just do it in our way to be consistent for sure.\n                for name in \"atime\", \"ctime\", \"mtime\":\n                    if name in ph:\n                        ns = s_to_ns(ph[name])\n                        setattr(item, name, ns)\n        yield item, status\n        # if we get here, \"with\"-block worked ok without error/exception, the item was processed ok...\n        self.add_item(item, stats=self.stats)\n\n    def process_dir(self, *, tarinfo, status, type):\n        with self.create_helper(tarinfo, status, type) as (item, status):\n            return status\n\n    def process_fifo(self, *, tarinfo, status, type):\n        with self.create_helper(tarinfo, status, type) as (item, status):\n            return status\n\n    def process_dev(self, *, tarinfo, status, type):\n        with self.create_helper(tarinfo, status, type) as (item, status):\n            item.rdev = os.makedev(tarinfo.devmajor, tarinfo.devminor)\n            return status\n\n    def process_symlink(self, *, tarinfo, status, type):\n        with self.create_helper(tarinfo, status, type) as (item, status):\n            item.target = tarinfo.linkname\n            return status\n\n    def process_hardlink(self, *, tarinfo, status, type):\n        with self.create_helper(tarinfo, status, type) as (item, status):\n            # create a not hardlinked borg item, reusing the chunks, see HardLinkManager.__doc__\n            chunks = self.hlm.retrieve(tarinfo.linkname)\n            if chunks is not None:\n                item.chunks = chunks\n            item.get_size(memorize=True, from_chunks=True)\n            self.stats.nfiles += 1\n            return status\n\n    def process_file(self, *, tarinfo, status, type, tar):\n        with self.create_helper(tarinfo, status, type) as (item, status):\n            self.print_file_status(status, tarinfo.name)\n            status = None  # we already printed the status\n            try:\n                fd = tar.extractfile(tarinfo)\n                self.process_file_chunks(\n                    item, self.cache, self.stats, self.show_progress, backup_io_iter(self.chunker.chunkify(fd))\n                )\n                item.get_size(memorize=True, from_chunks=True)\n                self.stats.nfiles += 1\n                # we need to remember ALL files, see HardLinkManager.__doc__\n                self.hlm.remember(id=tarinfo.name, info=item.chunks)\n                return status\n            except BackupOSError:\n                # see comment in FilesystemObjectProcessors.process_file, same issue here.\n                for chunk in item.get(\"chunks\", []):\n                    self.cache.chunk_decref(chunk.id, self.stats, wait=False)\n                raise\n\n\ndef valid_msgpacked_dict(d, keys_serialized):\n    \"\"\"check if the data <d> looks like a msgpacked dict\"\"\"\n    d_len = len(d)\n    if d_len == 0:\n        return False\n    if d[0] & 0xF0 == 0x80:  # object is a fixmap (up to 15 elements)\n        offs = 1\n    elif d[0] == 0xDE:  # object is a map16 (up to 2^16-1 elements)\n        offs = 3\n    else:\n        # object is not a map (dict)\n        # note: we must not have dicts with > 2^16-1 elements\n        return False\n    if d_len <= offs:\n        return False\n    # is the first dict key a bytestring?\n    if d[offs] & 0xE0 == 0xA0:  # key is a small bytestring (up to 31 chars)\n        pass\n    elif d[offs] in (0xD9, 0xDA, 0xDB):  # key is a str8, str16 or str32\n        pass\n    else:\n        # key is not a bytestring\n        return False\n    # is the bytestring any of the expected key names?\n    key_serialized = d[offs:]\n    return any(key_serialized.startswith(pattern) for pattern in keys_serialized)\n\n\nclass RobustUnpacker:\n    \"\"\"A restartable/robust version of the streaming msgpack unpacker\"\"\"\n\n    def __init__(self, validator, item_keys):\n        super().__init__()\n        self.item_keys = [msgpack.packb(name) for name in item_keys]\n        self.validator = validator\n        self._buffered_data = []\n        self._resync = False\n        self._unpacker = msgpack.Unpacker(object_hook=StableDict)\n\n    def resync(self):\n        self._buffered_data = []\n        self._resync = True\n\n    def feed(self, data):\n        if self._resync:\n            self._buffered_data.append(data)\n        else:\n            self._unpacker.feed(data)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self._resync:\n            data = b\"\".join(self._buffered_data)\n            while self._resync:\n                if not data:\n                    raise StopIteration\n                # Abort early if the data does not look like a serialized item dict\n                if not valid_msgpacked_dict(data, self.item_keys):\n                    data = data[1:]\n                    continue\n                self._unpacker = msgpack.Unpacker(object_hook=StableDict)\n                self._unpacker.feed(data)\n                try:\n                    item = next(self._unpacker)\n                except (msgpack.UnpackException, StopIteration):\n                    # as long as we are resyncing, we also ignore StopIteration\n                    pass\n                else:\n                    if self.validator(item):\n                        self._resync = False\n                        return item\n                data = data[1:]\n        else:\n            return next(self._unpacker)\n\n\nclass ArchiveChecker:\n    def __init__(self):\n        self.error_found = False\n        self.possibly_superseded = set()\n\n    def check(\n        self,\n        repository,\n        *,\n        verify_data=False,\n        repair=False,\n        match=None,\n        sort_by=\"\",\n        first=0,\n        last=0,\n        older=None,\n        newer=None,\n        oldest=None,\n        newest=None,\n    ):\n        \"\"\"Perform a set of checks on 'repository'\n\n        :param repair: enable repair mode, write updated or corrected data into repository\n        :param first/last/sort_by: only check this number of first/last archives ordered by sort_by\n        :param match: only check archives matching this pattern\n        :param older/newer: only check archives older/newer than timedelta from now\n        :param oldest/newest: only check archives older/newer than timedelta from oldest/newest archive timestamp\n        :param verify_data: integrity verification of data referenced by archives\n        \"\"\"\n        logger.info(\"Starting archive consistency check...\")\n        self.check_all = not any((first, last, match, older, newer, oldest, newest))\n        self.repair = repair\n        self.repository = repository\n        self.init_chunks()\n        if not self.chunks:\n            logger.error(\"Repository contains no apparent data at all, cannot continue check/repair.\")\n            return False\n        self.key = self.make_key(repository)\n        self.repo_objs = RepoObj(self.key)\n        if verify_data:\n            self.verify_data()\n        if Manifest.MANIFEST_ID not in self.chunks:\n            logger.error(\"Repository manifest not found!\")\n            self.error_found = True\n            self.manifest = self.rebuild_manifest()\n        else:\n            try:\n                self.manifest = Manifest.load(repository, (Manifest.Operation.CHECK,), key=self.key)\n            except IntegrityErrorBase as exc:\n                logger.error(\"Repository manifest is corrupted: %s\", exc)\n                self.error_found = True\n                del self.chunks[Manifest.MANIFEST_ID]\n                self.manifest = self.rebuild_manifest()\n        self.rebuild_refcounts(\n            match=match, first=first, last=last, sort_by=sort_by, older=older, oldest=oldest, newer=newer, newest=newest\n        )\n        self.orphan_chunks_check()\n        self.finish()\n        if self.error_found:\n            logger.error(\"Archive consistency check complete, problems found.\")\n        else:\n            logger.info(\"Archive consistency check complete, no problems found.\")\n        return self.repair or not self.error_found\n\n    def init_chunks(self):\n        \"\"\"Fetch a list of all object keys from repository\"\"\"\n        # Explicitly set the initial usable hash table capacity to avoid performance issues\n        # due to hash table \"resonance\".\n        # Since reconstruction of archive items can add some new chunks, add 10 % headroom.\n        self.chunks = ChunkIndex(usable=len(self.repository) * 1.1)\n        marker = None\n        while True:\n            result = self.repository.list(limit=LIST_SCAN_LIMIT, marker=marker)\n            if not result:\n                break\n            marker = result[-1]\n            init_entry = ChunkIndexEntry(refcount=0, size=0)\n            for id_ in result:\n                self.chunks[id_] = init_entry\n\n    def make_key(self, repository):\n        attempt = 0\n        for chunkid, _ in self.chunks.iteritems():\n            attempt += 1\n            if attempt > 999:\n                # we did a lot of attempts, but could not create the key via key_factory, give up.\n                break\n            cdata = repository.get(chunkid)\n            try:\n                return key_factory(repository, cdata)\n            except UnsupportedPayloadError:\n                # we get here, if the cdata we got has a corrupted key type byte\n                pass  # ignore it, just try the next chunk\n        if attempt == 0:\n            msg = \"make_key: repository has no chunks at all!\"\n        else:\n            msg = \"make_key: failed to create the key (tried %d chunks)\" % attempt\n        raise IntegrityError(msg)\n\n    def verify_data(self):\n        logger.info(\"Starting cryptographic data integrity verification...\")\n        chunks_count_index = len(self.chunks)\n        chunks_count_segments = 0\n        errors = 0\n        defect_chunks = []\n        pi = ProgressIndicatorPercent(\n            total=chunks_count_index, msg=\"Verifying data %6.2f%%\", step=0.01, msgid=\"check.verify_data\"\n        )\n        state = None\n        while True:\n            chunk_ids, state = self.repository.scan(limit=100, state=state)\n            if not chunk_ids:\n                break\n            chunks_count_segments += len(chunk_ids)\n            chunk_data_iter = self.repository.get_many(chunk_ids)\n            chunk_ids_revd = list(reversed(chunk_ids))\n            while chunk_ids_revd:\n                pi.show()\n                chunk_id = chunk_ids_revd.pop(-1)  # better efficiency\n                try:\n                    encrypted_data = next(chunk_data_iter)\n                except (Repository.ObjectNotFound, IntegrityErrorBase) as err:\n                    self.error_found = True\n                    errors += 1\n                    logger.error(\"chunk %s: %s\", bin_to_hex(chunk_id), err)\n                    if isinstance(err, IntegrityErrorBase):\n                        defect_chunks.append(chunk_id)\n                    # as the exception killed our generator, make a new one for remaining chunks:\n                    if chunk_ids_revd:\n                        chunk_ids = list(reversed(chunk_ids_revd))\n                        chunk_data_iter = self.repository.get_many(chunk_ids)\n                else:\n                    try:\n                        # we must decompress, so it'll call assert_id() in there:\n                        self.repo_objs.parse(chunk_id, encrypted_data, decompress=True)\n                    except IntegrityErrorBase as integrity_error:\n                        self.error_found = True\n                        errors += 1\n                        logger.error(\"chunk %s, integrity error: %s\", bin_to_hex(chunk_id), integrity_error)\n                        defect_chunks.append(chunk_id)\n        pi.finish()\n        if chunks_count_index != chunks_count_segments:\n            logger.error(\"Repo/Chunks index object count vs. segment files object count mismatch.\")\n            logger.error(\n                \"Repo/Chunks index: %d objects != segment files: %d objects\", chunks_count_index, chunks_count_segments\n            )\n        if defect_chunks:\n            if self.repair:\n                # if we kill the defect chunk here, subsequent actions within this \"borg check\"\n                # run will find missing chunks and replace them with all-zero replacement\n                # chunks and flag the files as \"repaired\".\n                # if another backup is done later and the missing chunks get backed up again,\n                # a \"borg check\" afterwards can heal all files where this chunk was missing.\n                logger.warning(\n                    \"Found defect chunks. They will be deleted now, so affected files can \"\n                    \"get repaired now and maybe healed later.\"\n                )\n                for defect_chunk in defect_chunks:\n                    # remote repo (ssh): retry might help for strange network / NIC / RAM errors\n                    # as the chunk will be retransmitted from remote server.\n                    # local repo (fs): as chunks.iteritems loop usually pumps a lot of data through,\n                    # a defect chunk is likely not in the fs cache any more and really gets re-read\n                    # from the underlying media.\n                    try:\n                        encrypted_data = self.repository.get(defect_chunk)\n                        # we must decompress, so it'll call assert_id() in there:\n                        self.repo_objs.parse(defect_chunk, encrypted_data, decompress=True)\n                    except IntegrityErrorBase:\n                        # failed twice -> get rid of this chunk\n                        del self.chunks[defect_chunk]\n                        self.repository.delete(defect_chunk)\n                        logger.debug(\"chunk %s deleted.\", bin_to_hex(defect_chunk))\n                    else:\n                        logger.warning(\"chunk %s not deleted, did not consistently fail.\", bin_to_hex(defect_chunk))\n            else:\n                logger.warning(\n                    \"Found defect chunks. With --repair, they would get deleted, so affected \"\n                    \"files could get repaired then and maybe healed later.\"\n                )\n                for defect_chunk in defect_chunks:\n                    logger.debug(\"chunk %s is defect.\", bin_to_hex(defect_chunk))\n        log = logger.error if errors else logger.info\n        log(\n            \"Finished cryptographic data integrity verification, verified %d chunks with %d integrity errors.\",\n            chunks_count_segments,\n            errors,\n        )\n\n    def rebuild_manifest(self):\n        \"\"\"Rebuild the manifest object if it is missing\n\n        Iterates through all objects in the repository looking for archive metadata blocks.\n        \"\"\"\n\n        def valid_archive(obj):\n            if not isinstance(obj, dict):\n                return False\n            return REQUIRED_ARCHIVE_KEYS.issubset(obj)\n\n        logger.info(\"Rebuilding missing manifest, this might take some time...\")\n        # as we have lost the manifest, we do not know any more what valid item keys we had.\n        # collecting any key we encounter in a damaged repo seems unwise, thus we just use\n        # the hardcoded list from the source code. thus, it is not recommended to rebuild a\n        # lost manifest on a older borg version than the most recent one that was ever used\n        # within this repository (assuming that newer borg versions support more item keys).\n        manifest = Manifest(self.key, self.repository)\n        archive_keys_serialized = [msgpack.packb(name) for name in ARCHIVE_KEYS]\n        pi = ProgressIndicatorPercent(\n            total=len(self.chunks), msg=\"Rebuilding manifest %6.2f%%\", step=0.01, msgid=\"check.rebuild_manifest\"\n        )\n        for chunk_id, _ in self.chunks.iteritems():\n            pi.show()\n            cdata = self.repository.get(chunk_id)\n            try:\n                _, data = self.repo_objs.parse(chunk_id, cdata)\n            except IntegrityErrorBase as exc:\n                logger.error(\"Skipping corrupted chunk: %s\", exc)\n                self.error_found = True\n                continue\n            if not valid_msgpacked_dict(data, archive_keys_serialized):\n                continue\n            if b\"command_line\" not in data or b\"\\xa7version\\x02\" not in data:\n                continue\n            try:\n                archive = msgpack.unpackb(data)\n            # Ignore exceptions that might be raised when feeding msgpack with invalid data\n            except msgpack.UnpackException:\n                continue\n            if valid_archive(archive):\n                # **after** doing the low-level checks and having a strong indication that we\n                # are likely looking at an archive item here, also check the TAM authentication:\n                try:\n                    archive, verified, _ = self.key.unpack_and_verify_archive(data, force_tam_not_required=False)\n                except IntegrityError:\n                    # TAM issues - do not accept this archive!\n                    # either somebody is trying to attack us with a fake archive data or\n                    # we have an ancient archive made before TAM was a thing (borg < 1.0.9) **and** this repo\n                    # was not correctly upgraded to borg 1.2.5 (see advisory at top of the changelog).\n                    # borg can't tell the difference, so it has to assume this archive might be an attack\n                    # and drops this archive.\n                    continue\n                # note: if we get here and verified is False, a TAM is not required.\n                archive = ArchiveItem(internal_dict=archive)\n                name = archive.name\n                logger.info(\"Found archive %s\", name)\n                if name in manifest.archives:\n                    i = 1\n                    while True:\n                        new_name = \"%s.%d\" % (name, i)\n                        if new_name not in manifest.archives:\n                            break\n                        i += 1\n                    logger.warning(\"Duplicate archive name %s, storing as %s\", name, new_name)\n                    name = new_name\n                manifest.archives[name] = (chunk_id, archive.time)\n        pi.finish()\n        logger.info(\"Manifest rebuild complete.\")\n        return manifest\n\n    def rebuild_refcounts(\n        self, first=0, last=0, sort_by=\"\", match=None, older=None, newer=None, oldest=None, newest=None\n    ):\n        \"\"\"Rebuild object reference counts by walking the metadata\n\n        Missing and/or incorrect data is repaired when detected\n        \"\"\"\n        # Exclude the manifest from chunks (manifest entry might be already deleted from self.chunks)\n        self.chunks.pop(Manifest.MANIFEST_ID, None)\n\n        def mark_as_possibly_superseded(id_):\n            if self.chunks.get(id_, ChunkIndexEntry(0, 0)).refcount == 0:\n                self.possibly_superseded.add(id_)\n\n        def add_callback(chunk):\n            id_ = self.key.id_hash(chunk)\n            cdata = self.repo_objs.format(id_, {}, chunk)\n            add_reference(id_, len(chunk), cdata)\n            return id_\n\n        def add_reference(id_, size, cdata=None):\n            try:\n                self.chunks.incref(id_)\n            except KeyError:\n                assert cdata is not None\n                self.chunks[id_] = ChunkIndexEntry(refcount=1, size=size)\n                if self.repair:\n                    self.repository.put(id_, cdata)\n\n        def verify_file_chunks(archive_name, item):\n            \"\"\"Verifies that all file chunks are present.\n\n            Missing file chunks will be replaced with new chunks of the same length containing all zeros.\n            If a previously missing file chunk re-appears, the replacement chunk is replaced by the correct one.\n            \"\"\"\n\n            def replacement_chunk(size):\n                chunk = Chunk(None, allocation=CH_ALLOC, size=size)\n                chunk_id, data = cached_hash(chunk, self.key.id_hash)\n                cdata = self.repo_objs.format(chunk_id, {}, data)\n                return chunk_id, size, cdata\n\n            offset = 0\n            chunk_list = []\n            chunks_replaced = False\n            has_chunks_healthy = \"chunks_healthy\" in item\n            chunks_current = item.chunks\n            chunks_healthy = item.chunks_healthy if has_chunks_healthy else chunks_current\n            if has_chunks_healthy and len(chunks_current) != len(chunks_healthy):\n                # should never happen, but there was issue #3218.\n                logger.warning(f\"{archive_name}: {item.path}: Invalid chunks_healthy metadata removed!\")\n                del item.chunks_healthy\n                has_chunks_healthy = False\n                chunks_healthy = chunks_current\n            for chunk_current, chunk_healthy in zip(chunks_current, chunks_healthy):\n                chunk_id, size = chunk_healthy\n                if chunk_id not in self.chunks:\n                    # a chunk of the healthy list is missing\n                    if chunk_current == chunk_healthy:\n                        logger.error(\n                            \"{}: {}: New missing file chunk detected (Byte {}-{}, Chunk {}). \"\n                            \"Replacing with all-zero chunk.\".format(\n                                archive_name, item.path, offset, offset + size, bin_to_hex(chunk_id)\n                            )\n                        )\n                        self.error_found = chunks_replaced = True\n                        chunk_id, size, cdata = replacement_chunk(size)\n                        add_reference(chunk_id, size, cdata)\n                    else:\n                        logger.info(\n                            \"{}: {}: Previously missing file chunk is still missing (Byte {}-{}, Chunk {}). \"\n                            \"It has an all-zero replacement chunk already.\".format(\n                                archive_name, item.path, offset, offset + size, bin_to_hex(chunk_id)\n                            )\n                        )\n                        chunk_id, size = chunk_current\n                        if chunk_id in self.chunks:\n                            add_reference(chunk_id, size)\n                        else:\n                            logger.warning(\n                                \"{}: {}: Missing all-zero replacement chunk detected (Byte {}-{}, Chunk {}). \"\n                                \"Generating new replacement chunk.\".format(\n                                    archive_name, item.path, offset, offset + size, bin_to_hex(chunk_id)\n                                )\n                            )\n                            self.error_found = chunks_replaced = True\n                            chunk_id, size, cdata = replacement_chunk(size)\n                            add_reference(chunk_id, size, cdata)\n                else:\n                    if chunk_current == chunk_healthy:\n                        # normal case, all fine.\n                        add_reference(chunk_id, size)\n                    else:\n                        logger.info(\n                            \"{}: {}: Healed previously missing file chunk! (Byte {}-{}, Chunk {}).\".format(\n                                archive_name, item.path, offset, offset + size, bin_to_hex(chunk_id)\n                            )\n                        )\n                        add_reference(chunk_id, size)\n                        mark_as_possibly_superseded(chunk_current[0])  # maybe orphaned the all-zero replacement chunk\n                chunk_list.append([chunk_id, size])  # list-typed element as chunks_healthy is list-of-lists\n                offset += size\n            if chunks_replaced and not has_chunks_healthy:\n                # if this is first repair, remember the correct chunk IDs, so we can maybe heal the file later\n                item.chunks_healthy = item.chunks\n            if has_chunks_healthy and chunk_list == chunks_healthy:\n                logger.info(f\"{archive_name}: {item.path}: Completely healed previously damaged file!\")\n                del item.chunks_healthy\n            item.chunks = chunk_list\n            if \"size\" in item:\n                item_size = item.size\n                item_chunks_size = item.get_size(from_chunks=True)\n                if item_size != item_chunks_size:\n                    # just warn, but keep the inconsistency, so that borg extract can warn about it.\n                    logger.warning(\n                        \"{}: {}: size inconsistency detected: size {}, chunks size {}\".format(\n                            archive_name, item.path, item_size, item_chunks_size\n                        )\n                    )\n\n        def robust_iterator(archive):\n            \"\"\"Iterates through all archive items\n\n            Missing item chunks will be skipped and the msgpack stream will be restarted\n            \"\"\"\n            item_keys = self.manifest.item_keys\n            required_item_keys = REQUIRED_ITEM_KEYS\n            unpacker = RobustUnpacker(\n                lambda item: isinstance(item, StableDict) and \"path\" in item, self.manifest.item_keys\n            )\n            _state = 0\n\n            def missing_chunk_detector(chunk_id):\n                nonlocal _state\n                if _state % 2 != int(chunk_id not in self.chunks):\n                    _state += 1\n                return _state\n\n            def report(msg, chunk_id, chunk_no):\n                cid = bin_to_hex(chunk_id)\n                msg += \" [chunk: %06d_%s]\" % (chunk_no, cid)  # see \"debug dump-archive-items\"\n                self.error_found = True\n                logger.error(msg)\n\n            def list_keys_safe(keys):\n                return \", \".join(k.decode(errors=\"replace\") if isinstance(k, bytes) else str(k) for k in keys)\n\n            def valid_item(obj):\n                if not isinstance(obj, StableDict):\n                    return False, \"not a dictionary\"\n                keys = set(obj)\n                if not required_item_keys.issubset(keys):\n                    return False, \"missing required keys: \" + list_keys_safe(required_item_keys - keys)\n                if not keys.issubset(item_keys):\n                    return False, \"invalid keys: \" + list_keys_safe(keys - item_keys)\n                return True, \"\"\n\n            i = 0\n            archive_items = archive_get_items(archive, repo_objs=self.repo_objs, repository=repository)\n            for state, items in groupby(archive_items, missing_chunk_detector):\n                items = list(items)\n                if state % 2:\n                    for chunk_id in items:\n                        report(\"item metadata chunk missing\", chunk_id, i)\n                        i += 1\n                    continue\n                if state > 0:\n                    unpacker.resync()\n                for chunk_id, cdata in zip(items, repository.get_many(items)):\n                    try:\n                        _, data = self.repo_objs.parse(chunk_id, cdata)\n                        unpacker.feed(data)\n                        for item in unpacker:\n                            valid, reason = valid_item(item)\n                            if valid:\n                                yield Item(internal_dict=item)\n                            else:\n                                report(\n                                    \"Did not get expected metadata dict when unpacking item metadata (%s)\" % reason,\n                                    chunk_id,\n                                    i,\n                                )\n                    except IntegrityError as integrity_error:\n                        # repo_objs.parse() detected integrity issues.\n                        # maybe the repo gave us a valid cdata, but not for the chunk_id we wanted.\n                        # or the authentication of cdata failed, meaning the encrypted data was corrupted.\n                        report(str(integrity_error), chunk_id, i)\n                    except msgpack.UnpackException:\n                        report(\"Unpacker crashed while unpacking item metadata, trying to resync...\", chunk_id, i)\n                        unpacker.resync()\n                    except Exception:\n                        report(\"Exception while decrypting or unpacking item metadata\", chunk_id, i)\n                        raise\n                    i += 1\n\n        sort_by = sort_by.split(\",\")\n        if any((first, last, match, older, newer, newest, oldest)):\n            archive_infos = self.manifest.archives.list(\n                sort_by=sort_by,\n                match=match,\n                first=first,\n                last=last,\n                oldest=oldest,\n                newest=newest,\n                older=older,\n                newer=newer,\n            )\n            if match and not archive_infos:\n                logger.warning(\"--match-archives %s does not match any archives\", match)\n            if first and len(archive_infos) < first:\n                logger.warning(\"--first %d archives: only found %d archives\", first, len(archive_infos))\n            if last and len(archive_infos) < last:\n                logger.warning(\"--last %d archives: only found %d archives\", last, len(archive_infos))\n        else:\n            archive_infos = self.manifest.archives.list(sort_by=sort_by, consider_checkpoints=True)\n        num_archives = len(archive_infos)\n\n        pi = ProgressIndicatorPercent(\n            total=num_archives, msg=\"Checking archives %3.1f%%\", step=0.1, msgid=\"check.rebuild_refcounts\"\n        )\n        with cache_if_remote(self.repository) as repository:\n            for i, info in enumerate(archive_infos):\n                pi.show(i)\n                logger.info(f\"Analyzing archive {info.name} ({i + 1}/{num_archives})\")\n                archive_id = info.id\n                if archive_id not in self.chunks:\n                    logger.error(\"Archive metadata block %s is missing!\", bin_to_hex(archive_id))\n                    self.error_found = True\n                    del self.manifest.archives[info.name]\n                    continue\n                mark_as_possibly_superseded(archive_id)\n                cdata = self.repository.get(archive_id)\n                try:\n                    _, data = self.repo_objs.parse(archive_id, cdata)\n                except IntegrityError as integrity_error:\n                    logger.error(\"Archive metadata block %s is corrupted: %s\", bin_to_hex(archive_id), integrity_error)\n                    self.error_found = True\n                    del self.manifest.archives[info.name]\n                    continue\n                try:\n                    archive, verified, salt = self.key.unpack_and_verify_archive(data, force_tam_not_required=False)\n                except IntegrityError as integrity_error:\n                    # looks like there is a TAM issue with this archive, this might be an attack!\n                    # when upgrading to borg 1.2.5, users are expected to TAM-authenticate all archives they\n                    # trust, so there shouldn't be any without TAM.\n                    logger.error(\"Archive TAM authentication issue for archive %s: %s\", info.name, integrity_error)\n                    self.error_found = True\n                    del self.manifest.archives[info.name]\n                    continue\n                archive = ArchiveItem(internal_dict=archive)\n                if archive.version != 2:\n                    raise Exception(\"Unknown archive metadata version\")\n                items_buffer = ChunkBuffer(self.key)\n                items_buffer.write_chunk = add_callback\n                for item in robust_iterator(archive):\n                    if \"chunks\" in item:\n                        verify_file_chunks(info.name, item)\n                    items_buffer.add(item)\n                items_buffer.flush(flush=True)\n                for previous_item_id in archive_get_items(\n                    archive, repo_objs=self.repo_objs, repository=self.repository\n                ):\n                    mark_as_possibly_superseded(previous_item_id)\n                for previous_item_ptr in archive.item_ptrs:\n                    mark_as_possibly_superseded(previous_item_ptr)\n                archive.item_ptrs = archive_put_items(\n                    items_buffer.chunks, repo_objs=self.repo_objs, add_reference=add_reference\n                )\n                data = self.key.pack_and_authenticate_metadata(archive.as_dict(), context=b\"archive\", salt=salt)\n                new_archive_id = self.key.id_hash(data)\n                cdata = self.repo_objs.format(new_archive_id, {}, data)\n                add_reference(new_archive_id, len(data), cdata)\n                self.manifest.archives[info.name] = (new_archive_id, info.ts)\n            pi.finish()\n\n    def orphan_chunks_check(self):\n        if self.check_all:\n            unused = {id_ for id_, entry in self.chunks.iteritems() if entry.refcount == 0}\n            orphaned = unused - self.possibly_superseded\n            if orphaned:\n                logger.error(f\"{len(orphaned)} orphaned objects found!\")\n                for chunk_id in orphaned:\n                    logger.debug(f\"chunk {bin_to_hex(chunk_id)} is orphaned.\")\n                self.error_found = True\n            if self.repair and unused:\n                logger.info(\n                    \"Deleting %d orphaned and %d superseded objects...\" % (len(orphaned), len(self.possibly_superseded))\n                )\n                for id_ in unused:\n                    self.repository.delete(id_)\n                logger.info(\"Finished deleting orphaned/superseded objects.\")\n        else:\n            logger.info(\"Orphaned objects check skipped (needs all archives checked).\")\n\n    def finish(self):\n        if self.repair:\n            logger.info(\"Writing Manifest.\")\n            self.manifest.write()\n            logger.info(\"Committing repo.\")\n            self.repository.commit(compact=False)\n\n\nclass ArchiveRecreater:\n    class Interrupted(Exception):\n        def __init__(self, metadata=None):\n            self.metadata = metadata or {}\n\n    @staticmethod\n    def is_temporary_archive(archive_name):\n        return archive_name.endswith(\".recreate\")\n\n    def __init__(\n        self,\n        manifest,\n        cache,\n        matcher,\n        exclude_caches=False,\n        exclude_if_present=None,\n        keep_exclude_tags=False,\n        chunker_params=None,\n        compression=None,\n        recompress=False,\n        always_recompress=False,\n        dry_run=False,\n        stats=False,\n        progress=False,\n        file_status_printer=None,\n        timestamp=None,\n        checkpoint_interval=1800,\n        checkpoint_volume=0,\n    ):\n        self.manifest = manifest\n        self.repository = manifest.repository\n        self.key = manifest.key\n        self.repo_objs = manifest.repo_objs\n        self.cache = cache\n\n        self.matcher = matcher\n        self.exclude_caches = exclude_caches\n        self.exclude_if_present = exclude_if_present or []\n        self.keep_exclude_tags = keep_exclude_tags\n\n        self.rechunkify = chunker_params is not None\n        if self.rechunkify:\n            logger.debug(\"Rechunking archives to %s\", chunker_params)\n        self.chunker_params = chunker_params or CHUNKER_PARAMS\n        self.recompress = recompress\n        self.always_recompress = always_recompress\n        self.compression = compression or CompressionSpec(\"none\")\n        self.seen_chunks = set()\n\n        self.timestamp = timestamp\n        self.dry_run = dry_run\n        self.stats = stats\n        self.progress = progress\n        self.print_file_status = file_status_printer or (lambda *args: None)\n        self.checkpoint_interval = None if dry_run else checkpoint_interval\n        self.checkpoint_volume = None if dry_run else checkpoint_volume\n\n    def recreate(self, archive_name, comment=None, target_name=None):\n        assert not self.is_temporary_archive(archive_name)\n        archive = self.open_archive(archive_name)\n        target = self.create_target(archive, target_name)\n        if self.exclude_if_present or self.exclude_caches:\n            self.matcher_add_tagged_dirs(archive)\n        if (\n            self.matcher.empty()\n            and not self.recompress\n            and not target.recreate_rechunkify\n            and comment is None\n            and target_name is None\n        ):\n            # nothing to do\n            return False\n        self.process_items(archive, target)\n        replace_original = target_name is None\n        self.save(archive, target, comment, replace_original=replace_original)\n        return True\n\n    def process_items(self, archive, target):\n        matcher = self.matcher\n\n        for item in archive.iter_items():\n            if not matcher.match(item.path):\n                self.print_file_status(\"-\", item.path)  # excluded (either by \"-\" or by \"!\")\n                continue\n            if self.dry_run:\n                self.print_file_status(\"+\", item.path)  # included\n            else:\n                self.process_item(archive, target, item)\n        if self.progress:\n            target.stats.show_progress(final=True)\n\n    def process_item(self, archive, target, item):\n        status = file_status(item.mode)\n        if \"chunks\" in item:\n            self.print_file_status(status, item.path)\n            status = None\n            self.process_chunks(archive, target, item)\n            target.stats.nfiles += 1\n        target.add_item(item, stats=target.stats)\n        self.print_file_status(status, item.path)\n\n    def process_chunks(self, archive, target, item):\n        if not self.recompress and not target.recreate_rechunkify:\n            for chunk_id, size in item.chunks:\n                self.cache.chunk_incref(chunk_id, target.stats)\n            return item.chunks\n        chunk_iterator = self.iter_chunks(archive, target, list(item.chunks))\n        chunk_processor = partial(self.chunk_processor, target)\n        target.process_file_chunks(item, self.cache, target.stats, self.progress, chunk_iterator, chunk_processor)\n\n    def chunk_processor(self, target, chunk):\n        chunk_id, data = cached_hash(chunk, self.key.id_hash)\n        if chunk_id in self.seen_chunks:\n            return self.cache.chunk_incref(chunk_id, target.stats)\n        overwrite = self.recompress\n        if self.recompress and not self.always_recompress and chunk_id in self.cache.chunks:\n            # Check if this chunk is already compressed the way we want it\n            old_meta = self.repo_objs.parse_meta(chunk_id, self.repository.get(chunk_id, read_data=False))\n            compr_hdr = bytes((old_meta[\"ctype\"], old_meta[\"clevel\"]))\n            compressor_cls, level = Compressor.detect(compr_hdr)\n            if (\n                compressor_cls.name == self.repo_objs.compressor.decide({}, data).name\n                and level == self.repo_objs.compressor.level\n            ):\n                # Stored chunk has the same compression method and level as we wanted\n                overwrite = False\n        chunk_entry = self.cache.add_chunk(chunk_id, {}, data, stats=target.stats, overwrite=overwrite, wait=False)\n        self.cache.repository.async_response(wait=False)\n        self.seen_chunks.add(chunk_entry.id)\n        return chunk_entry\n\n    def iter_chunks(self, archive, target, chunks):\n        chunk_iterator = archive.pipeline.fetch_many([chunk_id for chunk_id, _ in chunks])\n        if target.recreate_rechunkify:\n            # The target.chunker will read the file contents through ChunkIteratorFileWrapper chunk-by-chunk\n            # (does not load the entire file into memory)\n            file = ChunkIteratorFileWrapper(chunk_iterator)\n            yield from target.chunker.chunkify(file)\n        else:\n            for chunk in chunk_iterator:\n                yield Chunk(chunk, size=len(chunk), allocation=CH_DATA)\n\n    def save(self, archive, target, comment=None, replace_original=True):\n        if self.dry_run:\n            return\n        if comment is None:\n            comment = archive.metadata.get(\"comment\", \"\")\n\n        # Keep for the statistics if necessary\n        if self.stats:\n            _start = target.start\n\n        if self.timestamp is None:\n            additional_metadata = {\n                \"time\": archive.metadata.time,\n                \"time_end\": archive.metadata.get(\"time_end\") or archive.metadata.time,\n                \"command_line\": archive.metadata.command_line,\n                # but also remember recreate metadata:\n                \"recreate_command_line\": join_cmd(sys.argv),\n            }\n        else:\n            additional_metadata = {\n                \"command_line\": archive.metadata.command_line,\n                # but also remember recreate metadata:\n                \"recreate_command_line\": join_cmd(sys.argv),\n            }\n\n        target.save(comment=comment, timestamp=self.timestamp, additional_metadata=additional_metadata)\n        if replace_original:\n            archive.delete(Statistics(), progress=self.progress)\n            target.rename(archive.name)\n        if self.stats:\n            target.start = _start\n            target.end = archive_ts_now()\n            log_multi(str(target), str(target.stats))\n\n    def matcher_add_tagged_dirs(self, archive):\n        \"\"\"Add excludes to the matcher created by exclude_cache and exclude_if_present.\"\"\"\n\n        def exclude(dir, tag_item):\n            if self.keep_exclude_tags:\n                tag_files.append(PathPrefixPattern(tag_item.path, recurse_dir=False))\n                tagged_dirs.append(FnmatchPattern(dir + \"/\", recurse_dir=False))\n            else:\n                tagged_dirs.append(PathPrefixPattern(dir, recurse_dir=False))\n\n        matcher = self.matcher\n        tag_files = []\n        tagged_dirs = []\n\n        for item in archive.iter_items(\n            filter=lambda item: os.path.basename(item.path) == CACHE_TAG_NAME or matcher.match(item.path)\n        ):\n            dir, tag_file = os.path.split(item.path)\n            if tag_file in self.exclude_if_present:\n                exclude(dir, item)\n            elif self.exclude_caches and tag_file == CACHE_TAG_NAME and stat.S_ISREG(item.mode):\n                file = open_item(archive, item)\n                if file.read(len(CACHE_TAG_CONTENTS)) == CACHE_TAG_CONTENTS:\n                    exclude(dir, item)\n        matcher.add(tag_files, IECommand.Include)\n        matcher.add(tagged_dirs, IECommand.ExcludeNoRecurse)\n\n    def create_target(self, archive, target_name=None):\n        \"\"\"Create target archive.\"\"\"\n        target_name = target_name or archive.name + \".recreate\"\n        target = self.create_target_archive(target_name)\n        # If the archives use the same chunker params, then don't rechunkify\n        source_chunker_params = tuple(archive.metadata.get(\"chunker_params\", []))\n        if len(source_chunker_params) == 4 and isinstance(source_chunker_params[0], int):\n            # this is a borg < 1.2 chunker_params tuple, no chunker algo specified, but we only had buzhash:\n            source_chunker_params = (CH_BUZHASH,) + source_chunker_params\n        target.recreate_rechunkify = self.rechunkify and source_chunker_params != target.chunker_params\n        if target.recreate_rechunkify:\n            logger.debug(\n                \"Rechunking archive from %s to %s\", source_chunker_params or \"(unknown)\", target.chunker_params\n            )\n        target.process_file_chunks = ChunksProcessor(\n            cache=self.cache,\n            key=self.key,\n            add_item=target.add_item,\n            prepare_checkpoint=target.prepare_checkpoint,\n            write_checkpoint=target.write_checkpoint,\n            checkpoint_interval=self.checkpoint_interval,\n            checkpoint_volume=self.checkpoint_volume,\n            rechunkify=target.recreate_rechunkify,\n        ).process_file_chunks\n        target.chunker = get_chunker(*target.chunker_params, seed=self.key.chunk_seed, sparse=False)\n        return target\n\n    def create_target_archive(self, name):\n        target = Archive(\n            self.manifest,\n            name,\n            create=True,\n            progress=self.progress,\n            chunker_params=self.chunker_params,\n            cache=self.cache,\n        )\n        return target\n\n    def open_archive(self, name, **kwargs):\n        return Archive(self.manifest, name, cache=self.cache, **kwargs)\n", "import configparser\nimport os\nimport shutil\nimport stat\nfrom binascii import unhexlify\nfrom collections import namedtuple\nfrom time import perf_counter\n\nfrom .logger import create_logger\n\nlogger = create_logger()\n\nfiles_cache_logger = create_logger(\"borg.debug.files_cache\")\n\nfrom .constants import CACHE_README, FILES_CACHE_MODE_DISABLED\nfrom .hashindex import ChunkIndex, ChunkIndexEntry, CacheSynchronizer\nfrom .helpers import Location\nfrom .helpers import Error\nfrom .helpers import get_cache_dir, get_security_dir\nfrom .helpers import bin_to_hex, parse_stringified_list\nfrom .helpers import format_file_size\nfrom .helpers import safe_ns\nfrom .helpers import yes\nfrom .helpers import remove_surrogates\nfrom .helpers import ProgressIndicatorPercent, ProgressIndicatorMessage\nfrom .helpers import set_ec, EXIT_WARNING\nfrom .helpers import safe_unlink\nfrom .helpers import msgpack\nfrom .helpers.msgpack import int_to_timestamp, timestamp_to_int\nfrom .item import ArchiveItem, ChunkListEntry\nfrom .crypto.key import PlaintextKey\nfrom .crypto.file_integrity import IntegrityCheckedFile, DetachedIntegrityCheckedFile, FileIntegrityError\nfrom .locking import Lock\nfrom .manifest import Manifest\nfrom .platform import SaveFile\nfrom .remote import cache_if_remote\nfrom .repository import LIST_SCAN_LIMIT\n\n# note: cmtime might me either a ctime or a mtime timestamp\nFileCacheEntry = namedtuple(\"FileCacheEntry\", \"age inode size cmtime chunk_ids\")\n\n\nclass SecurityManager:\n    \"\"\"\n    Tracks repositories. Ensures that nothing bad happens (repository swaps,\n    replay attacks, unknown repositories etc.).\n\n    This is complicated by the Cache being initially used for this, while\n    only some commands actually use the Cache, which meant that other commands\n    did not perform these checks.\n\n    Further complications were created by the Cache being a cache, so it\n    could be legitimately deleted, which is annoying because Borg didn't\n    recognize repositories after that.\n\n    Therefore a second location, the security database (see get_security_dir),\n    was introduced which stores this information. However, this means that\n    the code has to deal with a cache existing but no security DB entry,\n    or inconsistencies between the security DB and the cache which have to\n    be reconciled, and also with no cache existing but a security DB entry.\n    \"\"\"\n\n    def __init__(self, repository):\n        self.repository = repository\n        self.dir = get_security_dir(repository.id_str, legacy=(repository.version == 1))\n        self.cache_dir = cache_dir(repository)\n        self.key_type_file = os.path.join(self.dir, \"key-type\")\n        self.location_file = os.path.join(self.dir, \"location\")\n        self.manifest_ts_file = os.path.join(self.dir, \"manifest-timestamp\")\n\n    @staticmethod\n    def destroy(repository, path=None):\n        \"\"\"destroy the security dir for ``repository`` or at ``path``\"\"\"\n        path = path or get_security_dir(repository.id_str, legacy=(repository.version == 1))\n        if os.path.exists(path):\n            shutil.rmtree(path)\n\n    def known(self):\n        return all(os.path.exists(f) for f in (self.key_type_file, self.location_file, self.manifest_ts_file))\n\n    def key_matches(self, key):\n        if not self.known():\n            return False\n        try:\n            with open(self.key_type_file) as fd:\n                type = fd.read()\n                return type == str(key.TYPE)\n        except OSError as exc:\n            logger.warning(\"Could not read/parse key type file: %s\", exc)\n\n    def save(self, manifest, key):\n        logger.debug(\"security: saving state for %s to %s\", self.repository.id_str, self.dir)\n        current_location = self.repository._location.canonical_path()\n        logger.debug(\"security: current location   %s\", current_location)\n        logger.debug(\"security: key type           %s\", str(key.TYPE))\n        logger.debug(\"security: manifest timestamp %s\", manifest.timestamp)\n        with SaveFile(self.location_file) as fd:\n            fd.write(current_location)\n        with SaveFile(self.key_type_file) as fd:\n            fd.write(str(key.TYPE))\n        with SaveFile(self.manifest_ts_file) as fd:\n            fd.write(manifest.timestamp)\n\n    def assert_location_matches(self, cache_config=None):\n        # Warn user before sending data to a relocated repository\n        try:\n            with open(self.location_file) as fd:\n                previous_location = fd.read()\n            logger.debug(\"security: read previous location %r\", previous_location)\n        except FileNotFoundError:\n            logger.debug(\"security: previous location file %s not found\", self.location_file)\n            previous_location = None\n        except OSError as exc:\n            logger.warning(\"Could not read previous location file: %s\", exc)\n            previous_location = None\n        if cache_config and cache_config.previous_location and previous_location != cache_config.previous_location:\n            # Reconcile cache and security dir; we take the cache location.\n            previous_location = cache_config.previous_location\n            logger.debug(\"security: using previous_location of cache: %r\", previous_location)\n\n        repository_location = self.repository._location.canonical_path()\n        if previous_location and previous_location != repository_location:\n            msg = (\n                \"Warning: The repository at location {} was previously located at {}\\n\".format(\n                    repository_location, previous_location\n                )\n                + \"Do you want to continue? [yN] \"\n            )\n            if not yes(\n                msg,\n                false_msg=\"Aborting.\",\n                invalid_msg=\"Invalid answer, aborting.\",\n                retry=False,\n                env_var_override=\"BORG_RELOCATED_REPO_ACCESS_IS_OK\",\n            ):\n                raise Cache.RepositoryAccessAborted()\n            # adapt on-disk config immediately if the new location was accepted\n            logger.debug(\"security: updating location stored in cache and security dir\")\n            with SaveFile(self.location_file) as fd:\n                fd.write(repository_location)\n            if cache_config:\n                cache_config.save()\n\n    def assert_no_manifest_replay(self, manifest, key, cache_config=None):\n        try:\n            with open(self.manifest_ts_file) as fd:\n                timestamp = fd.read()\n            logger.debug(\"security: read manifest timestamp %r\", timestamp)\n        except FileNotFoundError:\n            logger.debug(\"security: manifest timestamp file %s not found\", self.manifest_ts_file)\n            timestamp = \"\"\n        except OSError as exc:\n            logger.warning(\"Could not read previous location file: %s\", exc)\n            timestamp = \"\"\n        if cache_config:\n            timestamp = max(timestamp, cache_config.timestamp or \"\")\n        logger.debug(\"security: determined newest manifest timestamp as %s\", timestamp)\n        # If repository is older than the cache or security dir something fishy is going on\n        if timestamp and timestamp > manifest.timestamp:\n            if isinstance(key, PlaintextKey):\n                raise Cache.RepositoryIDNotUnique()\n            else:\n                raise Cache.RepositoryReplay()\n\n    def assert_key_type(self, key, cache_config=None):\n        # Make sure an encrypted repository has not been swapped for an unencrypted repository\n        if cache_config and cache_config.key_type is not None and cache_config.key_type != str(key.TYPE):\n            raise Cache.EncryptionMethodMismatch()\n        if self.known() and not self.key_matches(key):\n            raise Cache.EncryptionMethodMismatch()\n\n    def assert_secure(self, manifest, key, *, cache_config=None, warn_if_unencrypted=True, lock_wait=None):\n        # warn_if_unencrypted=False is only used for initializing a new repository.\n        # Thus, avoiding asking about a repository that's currently initializing.\n        self.assert_access_unknown(warn_if_unencrypted, manifest, key)\n        if cache_config:\n            self._assert_secure(manifest, key, cache_config)\n        else:\n            cache_config = CacheConfig(self.repository, lock_wait=lock_wait)\n            if cache_config.exists():\n                with cache_config:\n                    self._assert_secure(manifest, key, cache_config)\n            else:\n                self._assert_secure(manifest, key)\n        logger.debug(\"security: repository checks ok, allowing access\")\n\n    def _assert_secure(self, manifest, key, cache_config=None):\n        self.assert_location_matches(cache_config)\n        self.assert_key_type(key, cache_config)\n        self.assert_no_manifest_replay(manifest, key, cache_config)\n        if not self.known():\n            logger.debug(\"security: remembering previously unknown repository\")\n            self.save(manifest, key)\n\n    def assert_access_unknown(self, warn_if_unencrypted, manifest, key):\n        # warn_if_unencrypted=False is only used for initializing a new repository.\n        # Thus, avoiding asking about a repository that's currently initializing.\n        if not key.logically_encrypted and not self.known():\n            msg = (\n                \"Warning: Attempting to access a previously unknown unencrypted repository!\\n\"\n                + \"Do you want to continue? [yN] \"\n            )\n            allow_access = not warn_if_unencrypted or yes(\n                msg,\n                false_msg=\"Aborting.\",\n                invalid_msg=\"Invalid answer, aborting.\",\n                retry=False,\n                env_var_override=\"BORG_UNKNOWN_UNENCRYPTED_REPO_ACCESS_IS_OK\",\n            )\n            if allow_access:\n                if warn_if_unencrypted:\n                    logger.debug(\"security: remembering unknown unencrypted repository (explicitly allowed)\")\n                else:\n                    logger.debug(\"security: initializing unencrypted repository\")\n                self.save(manifest, key)\n            else:\n                raise Cache.CacheInitAbortedError()\n\n\ndef assert_secure(repository, manifest, lock_wait):\n    sm = SecurityManager(repository)\n    sm.assert_secure(manifest, manifest.key, lock_wait=lock_wait)\n\n\ndef recanonicalize_relative_location(cache_location, repository):\n    # borg < 1.0.8rc1 had different canonicalization for the repo location (see #1655 and #1741).\n    repo_location = repository._location.canonical_path()\n    rl = Location(repo_location)\n    cl = Location(cache_location)\n    if (\n        cl.proto == rl.proto\n        and cl.user == rl.user\n        and cl.host == rl.host\n        and cl.port == rl.port\n        and cl.path\n        and rl.path\n        and cl.path.startswith(\"/~/\")\n        and rl.path.startswith(\"/./\")\n        and cl.path[3:] == rl.path[3:]\n    ):\n        # everything is same except the expected change in relative path canonicalization,\n        # update previous_location to avoid warning / user query about changed location:\n        return repo_location\n    else:\n        return cache_location\n\n\ndef cache_dir(repository, path=None):\n    return path or os.path.join(get_cache_dir(), repository.id_str)\n\n\ndef files_cache_name():\n    suffix = os.environ.get(\"BORG_FILES_CACHE_SUFFIX\", \"\")\n    return \"files.\" + suffix if suffix else \"files\"\n\n\ndef discover_files_cache_name(path):\n    return [fn for fn in os.listdir(path) if fn == \"files\" or fn.startswith(\"files.\")][0]\n\n\nclass CacheConfig:\n    def __init__(self, repository, path=None, lock_wait=None):\n        self.repository = repository\n        self.path = cache_dir(repository, path)\n        logger.debug(\"Using %s as cache\", self.path)\n        self.config_path = os.path.join(self.path, \"config\")\n        self.lock = None\n        self.lock_wait = lock_wait\n\n    def __enter__(self):\n        self.open()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def exists(self):\n        return os.path.exists(self.config_path)\n\n    def create(self):\n        assert not self.exists()\n        config = configparser.ConfigParser(interpolation=None)\n        config.add_section(\"cache\")\n        config.set(\"cache\", \"version\", \"1\")\n        config.set(\"cache\", \"repository\", self.repository.id_str)\n        config.set(\"cache\", \"manifest\", \"\")\n        config.add_section(\"integrity\")\n        config.set(\"integrity\", \"manifest\", \"\")\n        with SaveFile(self.config_path) as fd:\n            config.write(fd)\n\n    def open(self):\n        self.lock = Lock(os.path.join(self.path, \"lock\"), exclusive=True, timeout=self.lock_wait).acquire()\n        self.load()\n\n    def load(self):\n        self._config = configparser.ConfigParser(interpolation=None)\n        with open(self.config_path) as fd:\n            self._config.read_file(fd)\n        self._check_upgrade(self.config_path)\n        self.id = self._config.get(\"cache\", \"repository\")\n        self.manifest_id = unhexlify(self._config.get(\"cache\", \"manifest\"))\n        self.timestamp = self._config.get(\"cache\", \"timestamp\", fallback=None)\n        self.key_type = self._config.get(\"cache\", \"key_type\", fallback=None)\n        self.ignored_features = set(parse_stringified_list(self._config.get(\"cache\", \"ignored_features\", fallback=\"\")))\n        self.mandatory_features = set(\n            parse_stringified_list(self._config.get(\"cache\", \"mandatory_features\", fallback=\"\"))\n        )\n        try:\n            self.integrity = dict(self._config.items(\"integrity\"))\n            if self._config.get(\"cache\", \"manifest\") != self.integrity.pop(\"manifest\"):\n                # The cache config file is updated (parsed with ConfigParser, the state of the ConfigParser\n                # is modified and then written out.), not re-created.\n                # Thus, older versions will leave our [integrity] section alone, making the section's data invalid.\n                # Therefore, we also add the manifest ID to this section and\n                # can discern whether an older version interfered by comparing the manifest IDs of this section\n                # and the main [cache] section.\n                self.integrity = {}\n                logger.warning(\"Cache integrity data not available: old Borg version modified the cache.\")\n        except configparser.NoSectionError:\n            logger.debug(\"Cache integrity: No integrity data found (files, chunks). Cache is from old version.\")\n            self.integrity = {}\n        previous_location = self._config.get(\"cache\", \"previous_location\", fallback=None)\n        if previous_location:\n            self.previous_location = recanonicalize_relative_location(previous_location, self.repository)\n        else:\n            self.previous_location = None\n        self._config.set(\"cache\", \"previous_location\", self.repository._location.canonical_path())\n\n    def save(self, manifest=None, key=None):\n        if manifest:\n            self._config.set(\"cache\", \"manifest\", manifest.id_str)\n            self._config.set(\"cache\", \"timestamp\", manifest.timestamp)\n            self._config.set(\"cache\", \"ignored_features\", \",\".join(self.ignored_features))\n            self._config.set(\"cache\", \"mandatory_features\", \",\".join(self.mandatory_features))\n            if not self._config.has_section(\"integrity\"):\n                self._config.add_section(\"integrity\")\n            for file, integrity_data in self.integrity.items():\n                self._config.set(\"integrity\", file, integrity_data)\n            self._config.set(\"integrity\", \"manifest\", manifest.id_str)\n        if key:\n            self._config.set(\"cache\", \"key_type\", str(key.TYPE))\n        with SaveFile(self.config_path) as fd:\n            self._config.write(fd)\n\n    def close(self):\n        if self.lock is not None:\n            self.lock.release()\n            self.lock = None\n\n    def _check_upgrade(self, config_path):\n        try:\n            cache_version = self._config.getint(\"cache\", \"version\")\n            wanted_version = 1\n            if cache_version != wanted_version:\n                self.close()\n                raise Exception(\n                    \"%s has unexpected cache version %d (wanted: %d).\" % (config_path, cache_version, wanted_version)\n                )\n        except configparser.NoSectionError:\n            self.close()\n            raise Exception(\"%s does not look like a Borg cache.\" % config_path) from None\n\n\nclass Cache:\n    \"\"\"Client Side cache\"\"\"\n\n    class RepositoryIDNotUnique(Error):\n        \"\"\"Cache is newer than repository - do you have multiple, independently updated repos with same ID?\"\"\"\n\n    class RepositoryReplay(Error):\n        \"\"\"Cache, or information obtained from the security directory is newer than repository - this is either an attack or unsafe (multiple repos with same ID)\"\"\"\n\n    class CacheInitAbortedError(Error):\n        \"\"\"Cache initialization aborted\"\"\"\n\n    class RepositoryAccessAborted(Error):\n        \"\"\"Repository access aborted\"\"\"\n\n    class EncryptionMethodMismatch(Error):\n        \"\"\"Repository encryption method changed since last access, refusing to continue\"\"\"\n\n    @staticmethod\n    def break_lock(repository, path=None):\n        path = cache_dir(repository, path)\n        Lock(os.path.join(path, \"lock\"), exclusive=True).break_lock()\n\n    @staticmethod\n    def destroy(repository, path=None):\n        \"\"\"destroy the cache for ``repository`` or at ``path``\"\"\"\n        path = path or os.path.join(get_cache_dir(), repository.id_str)\n        config = os.path.join(path, \"config\")\n        if os.path.exists(config):\n            os.remove(config)  # kill config first\n            shutil.rmtree(path)\n\n    def __new__(\n        cls,\n        repository,\n        manifest,\n        path=None,\n        sync=True,\n        warn_if_unencrypted=True,\n        progress=False,\n        lock_wait=None,\n        permit_adhoc_cache=False,\n        cache_mode=FILES_CACHE_MODE_DISABLED,\n        iec=False,\n    ):\n        def local():\n            return LocalCache(\n                manifest=manifest,\n                path=path,\n                sync=sync,\n                warn_if_unencrypted=warn_if_unencrypted,\n                progress=progress,\n                iec=iec,\n                lock_wait=lock_wait,\n                cache_mode=cache_mode,\n            )\n\n        def adhoc():\n            return AdHocCache(manifest=manifest, lock_wait=lock_wait, iec=iec)\n\n        if not permit_adhoc_cache:\n            return local()\n\n        # ad-hoc cache may be permitted, but if the local cache is in sync it'd be stupid to invalidate\n        # it by needlessly using the ad-hoc cache.\n        # Check if the local cache exists and is in sync.\n\n        cache_config = CacheConfig(repository, path, lock_wait)\n        if cache_config.exists():\n            with cache_config:\n                cache_in_sync = cache_config.manifest_id == manifest.id\n            # Don't nest cache locks\n            if cache_in_sync:\n                # Local cache is in sync, use it\n                logger.debug(\"Cache: choosing local cache (in sync)\")\n                return local()\n        logger.debug(\"Cache: choosing ad-hoc cache (local cache does not exist or is not in sync)\")\n        return adhoc()\n\n\nclass CacheStatsMixin:\n    str_format = \"\"\"\\\nOriginal size: {0.total_size}\nDeduplicated size: {0.unique_size}\nUnique chunks: {0.total_unique_chunks}\nTotal chunks: {0.total_chunks}\n\"\"\"\n\n    def __init__(self, iec=False):\n        self.iec = iec\n\n    def __str__(self):\n        return self.str_format.format(self.format_tuple())\n\n    Summary = namedtuple(\"Summary\", [\"total_size\", \"unique_size\", \"total_unique_chunks\", \"total_chunks\"])\n\n    def stats(self):\n        from .archive import Archive\n\n        # XXX: this should really be moved down to `hashindex.pyx`\n        total_size, unique_size, total_unique_chunks, total_chunks = self.chunks.summarize()\n        # since borg 1.2 we have new archive metadata telling the total size per archive,\n        # so we can just sum up all archives to get the \"all archives\" stats:\n        total_size = 0\n        for archive_name in self.manifest.archives:\n            archive = Archive(self.manifest, archive_name)\n            stats = archive.calc_stats(self, want_unique=False)\n            total_size += stats.osize\n        stats = self.Summary(total_size, unique_size, total_unique_chunks, total_chunks)._asdict()\n        return stats\n\n    def format_tuple(self):\n        stats = self.stats()\n        for field in [\"total_size\", \"unique_size\"]:\n            stats[field] = format_file_size(stats[field], iec=self.iec)\n        return self.Summary(**stats)\n\n\nclass LocalCache(CacheStatsMixin):\n    \"\"\"\n    Persistent, local (client-side) cache.\n    \"\"\"\n\n    def __init__(\n        self,\n        manifest,\n        path=None,\n        sync=True,\n        warn_if_unencrypted=True,\n        progress=False,\n        lock_wait=None,\n        cache_mode=FILES_CACHE_MODE_DISABLED,\n        iec=False,\n    ):\n        \"\"\"\n        :param warn_if_unencrypted: print warning if accessing unknown unencrypted repository\n        :param lock_wait: timeout for lock acquisition (int [s] or None [wait forever])\n        :param sync: do :meth:`.sync`\n        :param cache_mode: what shall be compared in the file stat infos vs. cached stat infos comparison\n        \"\"\"\n        CacheStatsMixin.__init__(self, iec=iec)\n        assert isinstance(manifest, Manifest)\n        self.manifest = manifest\n        self.repository = manifest.repository\n        self.key = manifest.key\n        self.repo_objs = manifest.repo_objs\n        self.progress = progress\n        self.cache_mode = cache_mode\n        self.timestamp = None\n        self.txn_active = False\n\n        self.path = cache_dir(self.repository, path)\n        self.security_manager = SecurityManager(self.repository)\n        self.cache_config = CacheConfig(self.repository, self.path, lock_wait)\n\n        # Warn user before sending data to a never seen before unencrypted repository\n        if not os.path.exists(self.path):\n            self.security_manager.assert_access_unknown(warn_if_unencrypted, manifest, self.key)\n            self.create()\n\n        self.open()\n        try:\n            self.security_manager.assert_secure(manifest, self.key, cache_config=self.cache_config)\n\n            if not self.check_cache_compatibility():\n                self.wipe_cache()\n\n            self.update_compatibility()\n\n            if sync and self.manifest.id != self.cache_config.manifest_id:\n                self.sync()\n                self.commit()\n        except:  # noqa\n            self.close()\n            raise\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def create(self):\n        \"\"\"Create a new empty cache at `self.path`\"\"\"\n        os.makedirs(self.path)\n        with open(os.path.join(self.path, \"README\"), \"w\") as fd:\n            fd.write(CACHE_README)\n        self.cache_config.create()\n        ChunkIndex().write(os.path.join(self.path, \"chunks\"))\n        os.makedirs(os.path.join(self.path, \"chunks.archive.d\"))\n        with SaveFile(os.path.join(self.path, files_cache_name()), binary=True):\n            pass  # empty file\n\n    def _do_open(self):\n        self.cache_config.load()\n        with IntegrityCheckedFile(\n            path=os.path.join(self.path, \"chunks\"),\n            write=False,\n            integrity_data=self.cache_config.integrity.get(\"chunks\"),\n        ) as fd:\n            self.chunks = ChunkIndex.read(fd)\n        if \"d\" in self.cache_mode:  # d(isabled)\n            self.files = None\n        else:\n            self._read_files()\n\n    def open(self):\n        if not os.path.isdir(self.path):\n            raise Exception(\"%s Does not look like a Borg cache\" % self.path)\n        self.cache_config.open()\n        self.rollback()\n\n    def close(self):\n        if self.cache_config is not None:\n            self.cache_config.close()\n            self.cache_config = None\n\n    def _read_files(self):\n        self.files = {}\n        self._newest_cmtime = None\n        logger.debug(\"Reading files cache ...\")\n        files_cache_logger.debug(\"FILES-CACHE-LOAD: starting...\")\n        msg = None\n        try:\n            with IntegrityCheckedFile(\n                path=os.path.join(self.path, files_cache_name()),\n                write=False,\n                integrity_data=self.cache_config.integrity.get(files_cache_name()),\n            ) as fd:\n                u = msgpack.Unpacker(use_list=True)\n                while True:\n                    data = fd.read(64 * 1024)\n                    if not data:\n                        break\n                    u.feed(data)\n                    try:\n                        for path_hash, item in u:\n                            entry = FileCacheEntry(*item)\n                            # in the end, this takes about 240 Bytes per file\n                            self.files[path_hash] = msgpack.packb(entry._replace(age=entry.age + 1))\n                    except (TypeError, ValueError) as exc:\n                        msg = \"The files cache seems invalid. [%s]\" % str(exc)\n                        break\n        except OSError as exc:\n            msg = \"The files cache can't be read. [%s]\" % str(exc)\n        except FileIntegrityError as fie:\n            msg = \"The files cache is corrupted. [%s]\" % str(fie)\n        if msg is not None:\n            logger.warning(msg)\n            logger.warning(\"Continuing without files cache - expect lower performance.\")\n            self.files = {}\n        files_cache_logger.debug(\"FILES-CACHE-LOAD: finished, %d entries loaded.\", len(self.files))\n\n    def begin_txn(self):\n        # Initialize transaction snapshot\n        pi = ProgressIndicatorMessage(msgid=\"cache.begin_transaction\")\n        txn_dir = os.path.join(self.path, \"txn.tmp\")\n        os.mkdir(txn_dir)\n        pi.output(\"Initializing cache transaction: Reading config\")\n        shutil.copy(os.path.join(self.path, \"config\"), txn_dir)\n        pi.output(\"Initializing cache transaction: Reading chunks\")\n        shutil.copy(os.path.join(self.path, \"chunks\"), txn_dir)\n        pi.output(\"Initializing cache transaction: Reading files\")\n        try:\n            shutil.copy(os.path.join(self.path, files_cache_name()), txn_dir)\n        except FileNotFoundError:\n            with SaveFile(os.path.join(txn_dir, files_cache_name()), binary=True):\n                pass  # empty file\n        os.replace(txn_dir, os.path.join(self.path, \"txn.active\"))\n        self.txn_active = True\n        pi.finish()\n\n    def commit(self):\n        \"\"\"Commit transaction\"\"\"\n        if not self.txn_active:\n            return\n        self.security_manager.save(self.manifest, self.key)\n        pi = ProgressIndicatorMessage(msgid=\"cache.commit\")\n        if self.files is not None:\n            if self._newest_cmtime is None:\n                # was never set because no files were modified/added\n                self._newest_cmtime = 2**63 - 1  # nanoseconds, good until y2262\n            ttl = int(os.environ.get(\"BORG_FILES_CACHE_TTL\", 20))\n            pi.output(\"Saving files cache\")\n            files_cache_logger.debug(\"FILES-CACHE-SAVE: starting...\")\n            with IntegrityCheckedFile(path=os.path.join(self.path, files_cache_name()), write=True) as fd:\n                entry_count = 0\n                for path_hash, item in self.files.items():\n                    # Only keep files seen in this backup that are older than newest cmtime seen in this backup -\n                    # this is to avoid issues with filesystem snapshots and cmtime granularity.\n                    # Also keep files from older backups that have not reached BORG_FILES_CACHE_TTL yet.\n                    entry = FileCacheEntry(*msgpack.unpackb(item))\n                    if (\n                        entry.age == 0\n                        and timestamp_to_int(entry.cmtime) < self._newest_cmtime\n                        or entry.age > 0\n                        and entry.age < ttl\n                    ):\n                        msgpack.pack((path_hash, entry), fd)\n                        entry_count += 1\n            files_cache_logger.debug(\"FILES-CACHE-KILL: removed all old entries with age >= TTL [%d]\", ttl)\n            files_cache_logger.debug(\n                \"FILES-CACHE-KILL: removed all current entries with newest cmtime %d\", self._newest_cmtime\n            )\n            files_cache_logger.debug(\"FILES-CACHE-SAVE: finished, %d remaining entries saved.\", entry_count)\n            self.cache_config.integrity[files_cache_name()] = fd.integrity_data\n        pi.output(\"Saving chunks cache\")\n        with IntegrityCheckedFile(path=os.path.join(self.path, \"chunks\"), write=True) as fd:\n            self.chunks.write(fd)\n        self.cache_config.integrity[\"chunks\"] = fd.integrity_data\n        pi.output(\"Saving cache config\")\n        self.cache_config.save(self.manifest, self.key)\n        os.replace(os.path.join(self.path, \"txn.active\"), os.path.join(self.path, \"txn.tmp\"))\n        shutil.rmtree(os.path.join(self.path, \"txn.tmp\"))\n        self.txn_active = False\n        pi.finish()\n\n    def rollback(self):\n        \"\"\"Roll back partial and aborted transactions\"\"\"\n        # Remove partial transaction\n        if os.path.exists(os.path.join(self.path, \"txn.tmp\")):\n            shutil.rmtree(os.path.join(self.path, \"txn.tmp\"))\n        # Roll back active transaction\n        txn_dir = os.path.join(self.path, \"txn.active\")\n        if os.path.exists(txn_dir):\n            shutil.copy(os.path.join(txn_dir, \"config\"), self.path)\n            shutil.copy(os.path.join(txn_dir, \"chunks\"), self.path)\n            shutil.copy(os.path.join(txn_dir, discover_files_cache_name(txn_dir)), self.path)\n            txn_tmp = os.path.join(self.path, \"txn.tmp\")\n            os.replace(txn_dir, txn_tmp)\n            if os.path.exists(txn_tmp):\n                shutil.rmtree(txn_tmp)\n        self.txn_active = False\n        self._do_open()\n\n    def sync(self):\n        \"\"\"Re-synchronize chunks cache with repository.\n\n        Maintains a directory with known backup archive indexes, so it only\n        needs to fetch infos from repo and build a chunk index once per backup\n        archive.\n        If out of sync, missing archive indexes get added, outdated indexes\n        get removed and a new master chunks index is built by merging all\n        archive indexes.\n        \"\"\"\n        archive_path = os.path.join(self.path, \"chunks.archive.d\")\n        # Instrumentation\n        processed_item_metadata_bytes = 0\n        processed_item_metadata_chunks = 0\n        compact_chunks_archive_saved_space = 0\n\n        def mkpath(id, suffix=\"\"):\n            id_hex = bin_to_hex(id)\n            path = os.path.join(archive_path, id_hex + suffix)\n            return path\n\n        def cached_archives():\n            if self.do_cache:\n                fns = os.listdir(archive_path)\n                # filenames with 64 hex digits == 256bit,\n                # or compact indices which are 64 hex digits + \".compact\"\n                return {unhexlify(fn) for fn in fns if len(fn) == 64} | {\n                    unhexlify(fn[:64]) for fn in fns if len(fn) == 72 and fn.endswith(\".compact\")\n                }\n            else:\n                return set()\n\n        def repo_archives():\n            return {info.id for info in self.manifest.archives.list()}\n\n        def cleanup_outdated(ids):\n            for id in ids:\n                cleanup_cached_archive(id)\n\n        def cleanup_cached_archive(id, cleanup_compact=True):\n            try:\n                os.unlink(mkpath(id))\n                os.unlink(mkpath(id) + \".integrity\")\n            except FileNotFoundError:\n                pass\n            if not cleanup_compact:\n                return\n            try:\n                os.unlink(mkpath(id, suffix=\".compact\"))\n                os.unlink(mkpath(id, suffix=\".compact\") + \".integrity\")\n            except FileNotFoundError:\n                pass\n\n        def fetch_and_build_idx(archive_id, decrypted_repository, chunk_idx):\n            nonlocal processed_item_metadata_bytes\n            nonlocal processed_item_metadata_chunks\n            csize, data = decrypted_repository.get(archive_id)\n            chunk_idx.add(archive_id, 1, len(data))\n            archive, verified, _ = self.key.unpack_and_verify_archive(data, force_tam_not_required=True)\n            archive = ArchiveItem(internal_dict=archive)\n            if archive.version not in (1, 2):  # legacy\n                raise Exception(\"Unknown archive metadata version\")\n            if archive.version == 1:\n                items = archive.items\n            elif archive.version == 2:\n                items = []\n                for chunk_id, (csize, data) in zip(archive.item_ptrs, decrypted_repository.get_many(archive.item_ptrs)):\n                    chunk_idx.add(chunk_id, 1, len(data))\n                    ids = msgpack.unpackb(data)\n                    items.extend(ids)\n            sync = CacheSynchronizer(chunk_idx)\n            for item_id, (csize, data) in zip(items, decrypted_repository.get_many(items)):\n                chunk_idx.add(item_id, 1, len(data))\n                processed_item_metadata_bytes += len(data)\n                processed_item_metadata_chunks += 1\n                sync.feed(data)\n            if self.do_cache:\n                write_archive_index(archive_id, chunk_idx)\n\n        def write_archive_index(archive_id, chunk_idx):\n            nonlocal compact_chunks_archive_saved_space\n            compact_chunks_archive_saved_space += chunk_idx.compact()\n            fn = mkpath(archive_id, suffix=\".compact\")\n            fn_tmp = mkpath(archive_id, suffix=\".tmp\")\n            try:\n                with DetachedIntegrityCheckedFile(\n                    path=fn_tmp, write=True, filename=bin_to_hex(archive_id) + \".compact\"\n                ) as fd:\n                    chunk_idx.write(fd)\n            except Exception:\n                safe_unlink(fn_tmp)\n            else:\n                os.replace(fn_tmp, fn)\n\n        def read_archive_index(archive_id, archive_name):\n            archive_chunk_idx_path = mkpath(archive_id)\n            logger.info(\"Reading cached archive chunk index for %s\", archive_name)\n            try:\n                try:\n                    # Attempt to load compact index first\n                    with DetachedIntegrityCheckedFile(path=archive_chunk_idx_path + \".compact\", write=False) as fd:\n                        archive_chunk_idx = ChunkIndex.read(fd, permit_compact=True)\n                    # In case a non-compact index exists, delete it.\n                    cleanup_cached_archive(archive_id, cleanup_compact=False)\n                    # Compact index read - return index, no conversion necessary (below).\n                    return archive_chunk_idx\n                except FileNotFoundError:\n                    # No compact index found, load non-compact index, and convert below.\n                    with DetachedIntegrityCheckedFile(path=archive_chunk_idx_path, write=False) as fd:\n                        archive_chunk_idx = ChunkIndex.read(fd)\n            except FileIntegrityError as fie:\n                logger.error(\"Cached archive chunk index of %s is corrupted: %s\", archive_name, fie)\n                # Delete corrupted index, set warning. A new index must be build.\n                cleanup_cached_archive(archive_id)\n                set_ec(EXIT_WARNING)\n                return None\n\n            # Convert to compact index. Delete the existing index first.\n            logger.debug(\"Found non-compact index for %s, converting to compact.\", archive_name)\n            cleanup_cached_archive(archive_id)\n            write_archive_index(archive_id, archive_chunk_idx)\n            return archive_chunk_idx\n\n        def get_archive_ids_to_names(archive_ids):\n            # Pass once over all archives and build a mapping from ids to names.\n            # The easier approach, doing a similar loop for each archive, has\n            # square complexity and does about a dozen million functions calls\n            # with 1100 archives (which takes 30s CPU seconds _alone_).\n            archive_names = {}\n            for info in self.manifest.archives.list():\n                if info.id in archive_ids:\n                    archive_names[info.id] = info.name\n            assert len(archive_names) == len(archive_ids)\n            return archive_names\n\n        def create_master_idx(chunk_idx):\n            logger.debug(\"Synchronizing chunks index...\")\n            cached_ids = cached_archives()\n            archive_ids = repo_archives()\n            logger.info(\n                \"Cached archive chunk indexes: %d fresh, %d stale, %d need fetching.\",\n                len(archive_ids & cached_ids),\n                len(cached_ids - archive_ids),\n                len(archive_ids - cached_ids),\n            )\n            # deallocates old hashindex, creates empty hashindex:\n            chunk_idx.clear()\n            cleanup_outdated(cached_ids - archive_ids)\n            # Explicitly set the usable initial hash table capacity to avoid performance issues\n            # due to hash table \"resonance\".\n            master_index_capacity = len(self.repository)\n            if archive_ids:\n                chunk_idx = None if not self.do_cache else ChunkIndex(usable=master_index_capacity)\n                pi = ProgressIndicatorPercent(\n                    total=len(archive_ids),\n                    step=0.1,\n                    msg=\"%3.0f%% Syncing chunks index. Processing archive %s.\",\n                    msgid=\"cache.sync\",\n                )\n                archive_ids_to_names = get_archive_ids_to_names(archive_ids)\n                for archive_id, archive_name in archive_ids_to_names.items():\n                    pi.show(info=[remove_surrogates(archive_name)])  # legacy. borg2 always has pure unicode arch names.\n                    if self.do_cache:\n                        if archive_id in cached_ids:\n                            archive_chunk_idx = read_archive_index(archive_id, archive_name)\n                            if archive_chunk_idx is None:\n                                cached_ids.remove(archive_id)\n                        if archive_id not in cached_ids:\n                            # Do not make this an else branch; the FileIntegrityError exception handler\n                            # above can remove *archive_id* from *cached_ids*.\n                            logger.info(\"Fetching and building archive index for %s.\", archive_name)\n                            archive_chunk_idx = ChunkIndex()\n                            fetch_and_build_idx(archive_id, decrypted_repository, archive_chunk_idx)\n                        logger.debug(\"Merging into master chunks index.\")\n                        chunk_idx.merge(archive_chunk_idx)\n                    else:\n                        chunk_idx = chunk_idx or ChunkIndex(usable=master_index_capacity)\n                        logger.info(\"Fetching archive index for %s.\", archive_name)\n                        fetch_and_build_idx(archive_id, decrypted_repository, chunk_idx)\n                pi.finish()\n                logger.debug(\n                    \"Chunks index sync: processed %s (%d chunks) of metadata.\",\n                    format_file_size(processed_item_metadata_bytes),\n                    processed_item_metadata_chunks,\n                )\n                logger.debug(\n                    \"Chunks index sync: compact chunks.archive.d storage saved %s bytes.\",\n                    format_file_size(compact_chunks_archive_saved_space),\n                )\n            logger.debug(\"Chunks index sync done.\")\n            return chunk_idx\n\n        # The cache can be used by a command that e.g. only checks against Manifest.Operation.WRITE,\n        # which does not have to include all flags from Manifest.Operation.READ.\n        # Since the sync will attempt to read archives, check compatibility with Manifest.Operation.READ.\n        self.manifest.check_repository_compatibility((Manifest.Operation.READ,))\n\n        self.begin_txn()\n        with cache_if_remote(self.repository, decrypted_cache=self.repo_objs) as decrypted_repository:\n            # TEMPORARY HACK:\n            # to avoid archive index caching, create a FILE named ~/.cache/borg/REPOID/chunks.archive.d -\n            # this is only recommended if you have a fast, low latency connection to your repo (e.g. if repo is local).\n            self.do_cache = os.path.isdir(archive_path)\n            self.chunks = create_master_idx(self.chunks)\n\n    def check_cache_compatibility(self):\n        my_features = Manifest.SUPPORTED_REPO_FEATURES\n        if self.cache_config.ignored_features & my_features:\n            # The cache might not contain references of chunks that need a feature that is mandatory for some operation\n            # and which this version supports. To avoid corruption while executing that operation force rebuild.\n            return False\n        if not self.cache_config.mandatory_features <= my_features:\n            # The cache was build with consideration to at least one feature that this version does not understand.\n            # This client might misinterpret the cache. Thus force a rebuild.\n            return False\n        return True\n\n    def wipe_cache(self):\n        logger.warning(\"Discarding incompatible cache and forcing a cache rebuild\")\n        archive_path = os.path.join(self.path, \"chunks.archive.d\")\n        if os.path.isdir(archive_path):\n            shutil.rmtree(os.path.join(self.path, \"chunks.archive.d\"))\n            os.makedirs(os.path.join(self.path, \"chunks.archive.d\"))\n        self.chunks = ChunkIndex()\n        with SaveFile(os.path.join(self.path, files_cache_name()), binary=True):\n            pass  # empty file\n        self.cache_config.manifest_id = \"\"\n        self.cache_config._config.set(\"cache\", \"manifest\", \"\")\n\n        self.cache_config.ignored_features = set()\n        self.cache_config.mandatory_features = set()\n\n    def update_compatibility(self):\n        operation_to_features_map = self.manifest.get_all_mandatory_features()\n        my_features = Manifest.SUPPORTED_REPO_FEATURES\n        repo_features = set()\n        for operation, features in operation_to_features_map.items():\n            repo_features.update(features)\n\n        self.cache_config.ignored_features.update(repo_features - my_features)\n        self.cache_config.mandatory_features.update(repo_features & my_features)\n\n    def add_chunk(\n        self, id, meta, data, *, stats, overwrite=False, wait=True, compress=True, size=None, ctype=None, clevel=None\n    ):\n        if not self.txn_active:\n            self.begin_txn()\n        if size is None and compress:\n            size = len(data)  # data is still uncompressed\n        refcount = self.seen_chunk(id, size)\n        if refcount and not overwrite:\n            return self.chunk_incref(id, stats)\n        if size is None:\n            raise ValueError(\"when giving compressed data for a new chunk, the uncompressed size must be given also\")\n        cdata = self.repo_objs.format(id, meta, data, compress=compress, size=size, ctype=ctype, clevel=clevel)\n        self.repository.put(id, cdata, wait=wait)\n        self.chunks.add(id, 1, size)\n        stats.update(size, not refcount)\n        return ChunkListEntry(id, size)\n\n    def seen_chunk(self, id, size=None):\n        refcount, stored_size = self.chunks.get(id, ChunkIndexEntry(0, None))\n        if size is not None and stored_size is not None and size != stored_size:\n            # we already have a chunk with that id, but different size.\n            # this is either a hash collision (unlikely) or corruption or a bug.\n            raise Exception(\n                \"chunk has same id [%r], but different size (stored: %d new: %d)!\" % (id, stored_size, size)\n            )\n        return refcount\n\n    def chunk_incref(self, id, stats, size=None):\n        if not self.txn_active:\n            self.begin_txn()\n        count, _size = self.chunks.incref(id)\n        stats.update(_size, False)\n        return ChunkListEntry(id, _size)\n\n    def chunk_decref(self, id, stats, wait=True):\n        if not self.txn_active:\n            self.begin_txn()\n        count, size = self.chunks.decref(id)\n        if count == 0:\n            del self.chunks[id]\n            self.repository.delete(id, wait=wait)\n            stats.update(-size, True)\n        else:\n            stats.update(-size, False)\n\n    def file_known_and_unchanged(self, hashed_path, path_hash, st):\n        \"\"\"\n        Check if we know the file that has this path_hash (know == it is in our files cache) and\n        whether it is unchanged (the size/inode number/cmtime is same for stuff we check in this cache_mode).\n\n        :param hashed_path: the file's path as we gave it to hash(hashed_path)\n        :param path_hash: hash(hashed_path), to save some memory in the files cache\n        :param st: the file's stat() result\n        :return: known, ids (known is True if we have infos about this file in the cache,\n                             ids is the list of chunk ids IF the file has not changed, otherwise None).\n        \"\"\"\n        if not stat.S_ISREG(st.st_mode):\n            return False, None\n        cache_mode = self.cache_mode\n        if \"d\" in cache_mode:  # d(isabled)\n            files_cache_logger.debug(\"UNKNOWN: files cache disabled\")\n            return False, None\n        # note: r(echunk) does not need the files cache in this method, but the files cache will\n        # be updated and saved to disk to memorize the files. To preserve previous generations in\n        # the cache, this means that it also needs to get loaded from disk first.\n        if \"r\" in cache_mode:  # r(echunk)\n            files_cache_logger.debug(\"UNKNOWN: rechunking enforced\")\n            return False, None\n        entry = self.files.get(path_hash)\n        if not entry:\n            files_cache_logger.debug(\"UNKNOWN: no file metadata in cache for: %r\", hashed_path)\n            return False, None\n        # we know the file!\n        entry = FileCacheEntry(*msgpack.unpackb(entry))\n        if \"s\" in cache_mode and entry.size != st.st_size:\n            files_cache_logger.debug(\"KNOWN-CHANGED: file size has changed: %r\", hashed_path)\n            return True, None\n        if \"i\" in cache_mode and entry.inode != st.st_ino:\n            files_cache_logger.debug(\"KNOWN-CHANGED: file inode number has changed: %r\", hashed_path)\n            return True, None\n        if \"c\" in cache_mode and timestamp_to_int(entry.cmtime) != st.st_ctime_ns:\n            files_cache_logger.debug(\"KNOWN-CHANGED: file ctime has changed: %r\", hashed_path)\n            return True, None\n        elif \"m\" in cache_mode and timestamp_to_int(entry.cmtime) != st.st_mtime_ns:\n            files_cache_logger.debug(\"KNOWN-CHANGED: file mtime has changed: %r\", hashed_path)\n            return True, None\n        # we ignored the inode number in the comparison above or it is still same.\n        # if it is still the same, replacing it in the tuple doesn't change it.\n        # if we ignored it, a reason for doing that is that files were moved to a new\n        # disk / new fs (so a one-time change of inode number is expected) and we wanted\n        # to avoid everything getting chunked again. to be able to re-enable the inode\n        # number comparison in a future backup run (and avoid chunking everything\n        # again at that time), we need to update the inode number in the cache with what\n        # we see in the filesystem.\n        self.files[path_hash] = msgpack.packb(entry._replace(inode=st.st_ino, age=0))\n        return True, entry.chunk_ids\n\n    def memorize_file(self, hashed_path, path_hash, st, ids):\n        if not stat.S_ISREG(st.st_mode):\n            return\n        cache_mode = self.cache_mode\n        # note: r(echunk) modes will update the files cache, d(isabled) mode won't\n        if \"d\" in cache_mode:\n            files_cache_logger.debug(\"FILES-CACHE-NOUPDATE: files cache disabled\")\n            return\n        if \"c\" in cache_mode:\n            cmtime_type = \"ctime\"\n            cmtime_ns = safe_ns(st.st_ctime_ns)\n        elif \"m\" in cache_mode:\n            cmtime_type = \"mtime\"\n            cmtime_ns = safe_ns(st.st_mtime_ns)\n        else:  # neither 'c' nor 'm' in cache_mode, avoid UnboundLocalError\n            cmtime_type = \"ctime\"\n            cmtime_ns = safe_ns(st.st_ctime_ns)\n        entry = FileCacheEntry(\n            age=0, inode=st.st_ino, size=st.st_size, cmtime=int_to_timestamp(cmtime_ns), chunk_ids=ids\n        )\n        self.files[path_hash] = msgpack.packb(entry)\n        self._newest_cmtime = max(self._newest_cmtime or 0, cmtime_ns)\n        files_cache_logger.debug(\n            \"FILES-CACHE-UPDATE: put %r [has %s] <- %r\",\n            entry._replace(chunk_ids=\"[%d entries]\" % len(entry.chunk_ids)),\n            cmtime_type,\n            hashed_path,\n        )\n\n\nclass AdHocCache(CacheStatsMixin):\n    \"\"\"\n    Ad-hoc, non-persistent cache.\n\n    Compared to the standard LocalCache the AdHocCache does not maintain accurate reference count,\n    nor does it provide a files cache (which would require persistence). Chunks that were not added\n    during the current AdHocCache lifetime won't have correct size set (0 bytes) and will\n    have an infinite reference count (MAX_VALUE).\n    \"\"\"\n\n    str_format = \"\"\"\\\nAll archives:                unknown              unknown              unknown\n\n                       Unique chunks         Total chunks\nChunk index:    {0.total_unique_chunks:20d}             unknown\"\"\"\n\n    def __init__(self, manifest, warn_if_unencrypted=True, lock_wait=None, iec=False):\n        CacheStatsMixin.__init__(self, iec=iec)\n        assert isinstance(manifest, Manifest)\n        self.manifest = manifest\n        self.repository = manifest.repository\n        self.key = manifest.key\n        self.repo_objs = manifest.repo_objs\n        self._txn_active = False\n\n        self.security_manager = SecurityManager(self.repository)\n        self.security_manager.assert_secure(manifest, self.key, lock_wait=lock_wait)\n\n        logger.warning(\"Note: --no-cache-sync is an experimental feature.\")\n\n    # Public API\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    files = None  # type: ignore\n    cache_mode = \"d\"\n\n    def file_known_and_unchanged(self, hashed_path, path_hash, st):\n        files_cache_logger.debug(\"UNKNOWN: files cache not implemented\")\n        return False, None\n\n    def memorize_file(self, hashed_path, path_hash, st, ids):\n        pass\n\n    def add_chunk(self, id, meta, data, *, stats, overwrite=False, wait=True, compress=True, size=None):\n        assert not overwrite, \"AdHocCache does not permit overwrites \u2014 trying to use it for recreate?\"\n        if not self._txn_active:\n            self.begin_txn()\n        if size is None and compress:\n            size = len(data)  # data is still uncompressed\n        if size is None:\n            raise ValueError(\"when giving compressed data for a chunk, the uncompressed size must be given also\")\n        refcount = self.seen_chunk(id, size)\n        if refcount:\n            return self.chunk_incref(id, stats, size=size)\n        cdata = self.repo_objs.format(id, meta, data, compress=compress)\n        self.repository.put(id, cdata, wait=wait)\n        self.chunks.add(id, 1, size)\n        stats.update(size, not refcount)\n        return ChunkListEntry(id, size)\n\n    def seen_chunk(self, id, size=None):\n        if not self._txn_active:\n            self.begin_txn()\n        entry = self.chunks.get(id, ChunkIndexEntry(0, None))\n        if entry.refcount and size and not entry.size:\n            # The LocalCache has existing size information and uses *size* to make an effort at detecting collisions.\n            # This is of course not possible for the AdHocCache.\n            # Here *size* is used to update the chunk's size information, which will be zero for existing chunks.\n            self.chunks[id] = entry._replace(size=size)\n        return entry.refcount\n\n    def chunk_incref(self, id, stats, size=None):\n        if not self._txn_active:\n            self.begin_txn()\n        count, _size = self.chunks.incref(id)\n        # When _size is 0 and size is not given, then this chunk has not been locally visited yet (seen_chunk with\n        # size or add_chunk); we can't add references to those (size=0 is invalid) and generally don't try to.\n        size = _size or size\n        assert size\n        stats.update(size, False)\n        return ChunkListEntry(id, size)\n\n    def chunk_decref(self, id, stats, wait=True):\n        if not self._txn_active:\n            self.begin_txn()\n        count, size = self.chunks.decref(id)\n        if count == 0:\n            del self.chunks[id]\n            self.repository.delete(id, wait=wait)\n            stats.update(-size, True)\n        else:\n            stats.update(-size, False)\n\n    def commit(self):\n        if not self._txn_active:\n            return\n        self.security_manager.save(self.manifest, self.key)\n        self._txn_active = False\n\n    def rollback(self):\n        self._txn_active = False\n        del self.chunks\n\n    def begin_txn(self):\n        self._txn_active = True\n        # Explicitly set the initial usable hash table capacity to avoid performance issues\n        # due to hash table \"resonance\".\n        # Since we're creating an archive, add 10 % from the start.\n        num_chunks = len(self.repository)\n        self.chunks = ChunkIndex(usable=num_chunks * 1.1)\n        pi = ProgressIndicatorPercent(\n            total=num_chunks, msg=\"Downloading chunk list... %3.0f%%\", msgid=\"cache.download_chunks\"\n        )\n        t0 = perf_counter()\n        num_requests = 0\n        marker = None\n        while True:\n            result = self.repository.list(limit=LIST_SCAN_LIMIT, marker=marker)\n            num_requests += 1\n            if not result:\n                break\n            pi.show(increase=len(result))\n            marker = result[-1]\n            # All chunks from the repository have a refcount of MAX_VALUE, which is sticky,\n            # therefore we can't/won't delete them. Chunks we added ourselves in this transaction\n            # (e.g. checkpoint archives) are tracked correctly.\n            init_entry = ChunkIndexEntry(refcount=ChunkIndex.MAX_VALUE, size=0)\n            for id_ in result:\n                self.chunks[id_] = init_entry\n        assert len(self.chunks) == num_chunks\n        # LocalCache does not contain the manifest, either.\n        del self.chunks[self.manifest.MANIFEST_ID]\n        duration = perf_counter() - t0 or 0.01\n        pi.finish()\n        logger.debug(\n            \"AdHocCache: downloaded %d chunk IDs in %.2f s (%d requests), ~%s/s\",\n            num_chunks,\n            duration,\n            num_requests,\n            format_file_size(num_chunks * 34 / duration),\n        )\n        # Chunk IDs in a list are encoded in 34 bytes: 1 byte msgpack header, 1 byte length, 32 ID bytes.\n        # Protocol overhead is neglected in this calculation.\n", "import binascii\nimport hmac\nimport os\nimport textwrap\nfrom binascii import a2b_base64, b2a_base64, hexlify\nfrom hashlib import sha256, pbkdf2_hmac\nfrom typing import Literal, Callable, ClassVar\n\nfrom ..logger import create_logger\n\nlogger = create_logger()\n\nimport argon2.low_level\n\nfrom ..constants import *  # NOQA\nfrom ..helpers import StableDict\nfrom ..helpers import Error, IntegrityError\nfrom ..helpers import get_keys_dir, get_security_dir\nfrom ..helpers import get_limited_unpacker\nfrom ..helpers import bin_to_hex\nfrom ..helpers.passphrase import Passphrase, PasswordRetriesExceeded, PassphraseWrong\nfrom ..helpers import msgpack\nfrom ..helpers import workarounds\nfrom ..item import Key, EncryptedKey, want_bytes\nfrom ..manifest import Manifest\nfrom ..platform import SaveFile\nfrom ..repoobj import RepoObj\n\n\nfrom .low_level import AES, bytes_to_int, num_cipher_blocks, hmac_sha256, blake2b_256, hkdf_hmac_sha512\nfrom .low_level import AES256_CTR_HMAC_SHA256, AES256_CTR_BLAKE2b, AES256_OCB, CHACHA20_POLY1305\nfrom . import low_level\n\n# workaround for lost passphrase or key in \"authenticated\" or \"authenticated-blake2\" mode\nAUTHENTICATED_NO_KEY = \"authenticated_no_key\" in workarounds\n\n\nclass UnsupportedPayloadError(Error):\n    \"\"\"Unsupported payload type {}. A newer version is required to access this repository.\"\"\"\n\n\nclass UnsupportedManifestError(Error):\n    \"\"\"Unsupported manifest envelope. A newer version is required to access this repository.\"\"\"\n\n\nclass KeyfileNotFoundError(Error):\n    \"\"\"No key file for repository {} found in {}.\"\"\"\n\n\nclass KeyfileInvalidError(Error):\n    \"\"\"Invalid key file for repository {} found in {}.\"\"\"\n\n\nclass KeyfileMismatchError(Error):\n    \"\"\"Mismatch between repository {} and key file {}.\"\"\"\n\n\nclass RepoKeyNotFoundError(Error):\n    \"\"\"No key entry found in the config of repository {}.\"\"\"\n\n\nclass UnsupportedKeyFormatError(Error):\n    \"\"\"Your borg key is stored in an unsupported format. Try using a newer version of borg.\"\"\"\n\n\nclass TAMRequiredError(IntegrityError):\n    __doc__ = textwrap.dedent(\n        \"\"\"\n    Manifest is unauthenticated, but it is required for this repository. Is somebody attacking you?\n    \"\"\"\n    ).strip()\n    traceback = False\n\n\nclass ArchiveTAMRequiredError(TAMRequiredError):\n    __doc__ = textwrap.dedent(\n        \"\"\"\n    Archive '{}' is unauthenticated, but it is required for this repository.\n    \"\"\"\n    ).strip()\n    traceback = False\n\n\nclass TAMInvalid(IntegrityError):\n    __doc__ = IntegrityError.__doc__\n    traceback = False\n\n    def __init__(self):\n        # Error message becomes: \"Data integrity error: Manifest authentication did not verify\"\n        super().__init__(\"Manifest authentication did not verify\")\n\n\nclass ArchiveTAMInvalid(IntegrityError):\n    __doc__ = IntegrityError.__doc__\n    traceback = False\n\n    def __init__(self):\n        # Error message becomes: \"Data integrity error: Archive authentication did not verify\"\n        super().__init__(\"Archive authentication did not verify\")\n\n\nclass TAMUnsupportedSuiteError(IntegrityError):\n    \"\"\"Could not verify manifest: Unsupported suite {!r}; a newer version is needed.\"\"\"\n\n    traceback = False\n\n\ndef key_creator(repository, args, *, other_key=None):\n    for key in AVAILABLE_KEY_TYPES:\n        if key.ARG_NAME == args.encryption:\n            assert key.ARG_NAME is not None\n            return key.create(repository, args, other_key=other_key)\n    else:\n        raise ValueError('Invalid encryption mode \"%s\"' % args.encryption)\n\n\ndef key_argument_names():\n    return [key.ARG_NAME for key in AVAILABLE_KEY_TYPES if key.ARG_NAME]\n\n\ndef identify_key(manifest_data):\n    key_type = manifest_data[0]\n    if key_type == KeyType.PASSPHRASE:  # legacy, see comment in KeyType class.\n        return RepoKey\n\n    for key in LEGACY_KEY_TYPES + AVAILABLE_KEY_TYPES:\n        if key.TYPE == key_type:\n            return key\n    else:\n        raise UnsupportedPayloadError(key_type)\n\n\ndef key_factory(repository, manifest_chunk, *, ro_cls=RepoObj):\n    manifest_data = ro_cls.extract_crypted_data(manifest_chunk)\n    assert manifest_data, \"manifest data must not be zero bytes long\"\n    return identify_key(manifest_data).detect(repository, manifest_data)\n\n\ndef tam_required_file(repository):\n    security_dir = get_security_dir(bin_to_hex(repository.id), legacy=(repository.version == 1))\n    return os.path.join(security_dir, \"tam_required\")\n\n\ndef tam_required(repository):\n    file = tam_required_file(repository)\n    return os.path.isfile(file)\n\n\ndef uses_same_chunker_secret(other_key, key):\n    \"\"\"is the chunker secret the same?\"\"\"\n    # avoid breaking the deduplication by a different chunker secret\n    same_chunker_secret = other_key.chunk_seed == key.chunk_seed\n    return same_chunker_secret\n\n\ndef uses_same_id_hash(other_key, key):\n    \"\"\"other_key -> key upgrade: is the id hash the same?\"\"\"\n    # avoid breaking the deduplication by changing the id hash\n    old_sha256_ids = (PlaintextKey,)\n    new_sha256_ids = (PlaintextKey,)\n    old_hmac_sha256_ids = (RepoKey, KeyfileKey, AuthenticatedKey)\n    new_hmac_sha256_ids = (AESOCBRepoKey, AESOCBKeyfileKey, CHPORepoKey, CHPOKeyfileKey, AuthenticatedKey)\n    old_blake2_ids = (Blake2RepoKey, Blake2KeyfileKey, Blake2AuthenticatedKey)\n    new_blake2_ids = (\n        Blake2AESOCBRepoKey,\n        Blake2AESOCBKeyfileKey,\n        Blake2CHPORepoKey,\n        Blake2CHPOKeyfileKey,\n        Blake2AuthenticatedKey,\n    )\n    same_ids = (\n        isinstance(other_key, old_hmac_sha256_ids + new_hmac_sha256_ids)\n        and isinstance(key, new_hmac_sha256_ids)\n        or isinstance(other_key, old_blake2_ids + new_blake2_ids)\n        and isinstance(key, new_blake2_ids)\n        or isinstance(other_key, old_sha256_ids + new_sha256_ids)\n        and isinstance(key, new_sha256_ids)\n    )\n    return same_ids\n\n\nclass KeyBase:\n    # Numeric key type ID, must fit in one byte.\n    TYPE: int = None  # override in subclasses\n    # set of key type IDs the class can handle as input\n    TYPES_ACCEPTABLE: set[int] = None  # override in subclasses\n\n    # Human-readable name\n    NAME = \"UNDEFINED\"\n\n    # Name used in command line / API (e.g. borg init --encryption=...)\n    ARG_NAME = \"UNDEFINED\"\n\n    # Storage type (no key blob storage / keyfile / repo)\n    STORAGE: ClassVar[str] = KeyBlobStorage.NO_STORAGE\n\n    # Seed for the buzhash chunker (borg.algorithms.chunker.Chunker)\n    # type is int\n    chunk_seed: int = None\n\n    # Whether this *particular instance* is encrypted from a practical point of view,\n    # i.e. when it's using encryption with a empty passphrase, then\n    # that may be *technically* called encryption, but for all intents and purposes\n    # that's as good as not encrypting in the first place, and this member should be False.\n    #\n    # The empty passphrase is also special because Borg tries it first when no passphrase\n    # was supplied, and if an empty passphrase works, then Borg won't ask for one.\n    logically_encrypted = False\n\n    def __init__(self, repository):\n        self.TYPE_STR = bytes([self.TYPE])\n        self.repository = repository\n        self.target = None  # key location file path / repo obj\n        self.tam_required = True\n        self.copy_crypt_key = False\n\n    def id_hash(self, data):\n        \"\"\"Return HMAC hash using the \"id\" HMAC key\"\"\"\n        raise NotImplementedError\n\n    def encrypt(self, id, data):\n        pass\n\n    def decrypt(self, id, data):\n        pass\n\n    def assert_id(self, id, data):\n        if id and id != Manifest.MANIFEST_ID:\n            id_computed = self.id_hash(data)\n            if not hmac.compare_digest(id_computed, id):\n                raise IntegrityError(\"Chunk %s: id verification failed\" % bin_to_hex(id))\n\n    def assert_type(self, type_byte, id=None):\n        if type_byte not in self.TYPES_ACCEPTABLE:\n            id_str = bin_to_hex(id) if id is not None else \"(unknown)\"\n            raise IntegrityError(f\"Chunk {id_str}: Invalid encryption envelope\")\n\n    def _tam_key(self, salt, context):\n        return hkdf_hmac_sha512(\n            ikm=self.id_key + self.crypt_key,\n            salt=salt,\n            info=b\"borg-metadata-authentication-\" + context,\n            output_length=64,\n        )\n\n    def pack_and_authenticate_metadata(self, metadata_dict, context=b\"manifest\", salt=None):\n        if salt is None:\n            salt = os.urandom(64)\n        metadata_dict = StableDict(metadata_dict)\n        tam = metadata_dict[\"tam\"] = StableDict({\"type\": \"HKDF_HMAC_SHA512\", \"hmac\": bytes(64), \"salt\": salt})\n        packed = msgpack.packb(metadata_dict)\n        tam_key = self._tam_key(salt, context)\n        tam[\"hmac\"] = hmac.digest(tam_key, packed, \"sha512\")\n        return msgpack.packb(metadata_dict)\n\n    def unpack_and_verify_manifest(self, data, force_tam_not_required=False):\n        \"\"\"Unpack msgpacked *data* and return (object, did_verify).\"\"\"\n        if data.startswith(b\"\\xc1\" * 4):\n            # This is a manifest from the future, we can't read it.\n            raise UnsupportedManifestError()\n        tam_required = self.tam_required\n        if force_tam_not_required and tam_required:\n            logger.warning(\"Manifest authentication DISABLED.\")\n            tam_required = False\n        data = bytearray(data)\n        unpacker = get_limited_unpacker(\"manifest\")\n        unpacker.feed(data)\n        unpacked = unpacker.unpack()\n        if AUTHENTICATED_NO_KEY:\n            return unpacked, True  # True is a lie.\n        if \"tam\" not in unpacked:\n            if tam_required:\n                raise TAMRequiredError(self.repository._location.canonical_path())\n            else:\n                logger.debug(\"Manifest TAM not found and not required\")\n                return unpacked, False\n        tam = unpacked.pop(\"tam\", None)\n        if not isinstance(tam, dict):\n            raise TAMInvalid()\n        tam_type = tam.get(\"type\", \"<none>\")\n        if tam_type != \"HKDF_HMAC_SHA512\":\n            if tam_required:\n                raise TAMUnsupportedSuiteError(repr(tam_type))\n            else:\n                logger.debug(\n                    \"Ignoring manifest TAM made with unsupported suite, since TAM is not required: %r\", tam_type\n                )\n                return unpacked, False\n        tam_hmac = tam.get(\"hmac\")\n        tam_salt = tam.get(\"salt\")\n        if not isinstance(tam_salt, (bytes, str)) or not isinstance(tam_hmac, (bytes, str)):\n            raise TAMInvalid()\n        tam_hmac = want_bytes(tam_hmac)  # legacy\n        tam_salt = want_bytes(tam_salt)  # legacy\n        offset = data.index(tam_hmac)\n        data[offset : offset + 64] = bytes(64)\n        tam_key = self._tam_key(tam_salt, context=b\"manifest\")\n        calculated_hmac = hmac.digest(tam_key, data, \"sha512\")\n        if not hmac.compare_digest(calculated_hmac, tam_hmac):\n            raise TAMInvalid()\n        logger.debug(\"TAM-verified manifest\")\n        return unpacked, True\n\n    def unpack_and_verify_archive(self, data, force_tam_not_required=False):\n        \"\"\"Unpack msgpacked *data* and return (object, did_verify).\"\"\"\n        tam_required = self.tam_required\n        if force_tam_not_required and tam_required:\n            # for a long time, borg only checked manifest for \"tam_required\" and\n            # people might have archives without TAM, so don't be too annoyingly loud here:\n            logger.debug(\"Archive authentication DISABLED.\")\n            tam_required = False\n        data = bytearray(data)\n        unpacker = get_limited_unpacker(\"archive\")\n        unpacker.feed(data)\n        unpacked = unpacker.unpack()\n        if \"tam\" not in unpacked:\n            if tam_required:\n                archive_name = unpacked.get(\"name\", \"<unknown>\")\n                raise ArchiveTAMRequiredError(archive_name)\n            else:\n                logger.debug(\"Archive TAM not found and not required\")\n                return unpacked, False, None\n        tam = unpacked.pop(\"tam\", None)\n        if not isinstance(tam, dict):\n            raise ArchiveTAMInvalid()\n        tam_type = tam.get(\"type\", \"<none>\")\n        if tam_type != \"HKDF_HMAC_SHA512\":\n            if tam_required:\n                raise TAMUnsupportedSuiteError(repr(tam_type))\n            else:\n                logger.debug(\n                    \"Ignoring archive TAM made with unsupported suite, since TAM is not required: %r\", tam_type\n                )\n                return unpacked, False, None\n        tam_hmac = tam.get(\"hmac\")\n        tam_salt = tam.get(\"salt\")\n        if not isinstance(tam_salt, (bytes, str)) or not isinstance(tam_hmac, (bytes, str)):\n            raise ArchiveTAMInvalid()\n        tam_hmac = want_bytes(tam_hmac)  # legacy\n        tam_salt = want_bytes(tam_salt)  # legacy\n        offset = data.index(tam_hmac)\n        data[offset : offset + 64] = bytes(64)\n        tam_key = self._tam_key(tam_salt, context=b\"archive\")\n        calculated_hmac = hmac.digest(tam_key, data, \"sha512\")\n        if not hmac.compare_digest(calculated_hmac, tam_hmac):\n            raise ArchiveTAMInvalid()\n        logger.debug(\"TAM-verified archive\")\n        return unpacked, True, tam_salt\n\n\nclass PlaintextKey(KeyBase):\n    TYPE = KeyType.PLAINTEXT\n    TYPES_ACCEPTABLE = {TYPE}\n    NAME = \"plaintext\"\n    ARG_NAME = \"none\"\n\n    chunk_seed = 0\n    logically_encrypted = False\n\n    def __init__(self, repository):\n        super().__init__(repository)\n        self.tam_required = False\n\n    @classmethod\n    def create(cls, repository, args, **kw):\n        logger.info('Encryption NOT enabled.\\nUse the \"--encryption=repokey|keyfile\" to enable encryption.')\n        return cls(repository)\n\n    @classmethod\n    def detect(cls, repository, manifest_data):\n        return cls(repository)\n\n    def id_hash(self, data):\n        return sha256(data).digest()\n\n    def encrypt(self, id, data):\n        return b\"\".join([self.TYPE_STR, data])\n\n    def decrypt(self, id, data):\n        self.assert_type(data[0], id)\n        return memoryview(data)[1:]\n\n    def _tam_key(self, salt, context):\n        return salt + context\n\n\ndef random_blake2b_256_key():\n    # This might look a bit curious, but is the same construction used in the keyed mode of BLAKE2b.\n    # Why limit the key to 64 bytes and pad it with 64 nulls nonetheless? The answer is that BLAKE2b\n    # has a 128 byte block size, but only 64 bytes of internal state (this is also referred to as a\n    # \"local wide pipe\" design, because the compression function transforms (block, state) => state,\n    # and len(block) >= len(state), hence wide.)\n    # In other words, a key longer than 64 bytes would have simply no advantage, since the function\n    # has no way of propagating more than 64 bytes of entropy internally.\n    # It's padded to a full block so that the key is never buffered internally by blake2b_update, ie.\n    # it remains in a single memory location that can be tracked and could be erased securely, if we\n    # wanted to.\n    return os.urandom(64) + bytes(64)\n\n\nclass ID_BLAKE2b_256:\n    \"\"\"\n    Key mix-in class for using BLAKE2b-256 for the id key.\n\n    The id_key length must be 32 bytes.\n    \"\"\"\n\n    def id_hash(self, data):\n        return blake2b_256(self.id_key, data)\n\n    def init_from_random_data(self):\n        super().init_from_random_data()\n        enc_key = os.urandom(32)\n        enc_hmac_key = random_blake2b_256_key()\n        self.crypt_key = enc_key + enc_hmac_key\n        self.id_key = random_blake2b_256_key()\n\n\nclass ID_HMAC_SHA_256:\n    \"\"\"\n    Key mix-in class for using HMAC-SHA-256 for the id key.\n\n    The id_key length must be 32 bytes.\n    \"\"\"\n\n    def id_hash(self, data):\n        return hmac_sha256(self.id_key, data)\n\n\nclass AESKeyBase(KeyBase):\n    \"\"\"\n    Chunks are encrypted using 256bit AES in Counter Mode (CTR)\n\n    Payload layout: TYPE(1) + HMAC(32) + NONCE(8) + CIPHERTEXT\n\n    To reduce payload size only 8 bytes of the 16 bytes nonce is saved\n    in the payload, the first 8 bytes are always zeros. This does not\n    affect security but limits the maximum repository capacity to\n    only 295 exabytes!\n    \"\"\"\n\n    PAYLOAD_OVERHEAD = 1 + 32 + 8  # TYPE + HMAC + NONCE\n\n    CIPHERSUITE: Callable = None  # override in derived class\n\n    logically_encrypted = True\n\n    def encrypt(self, id, data):\n        # legacy, this is only used by the tests.\n        next_iv = self.cipher.next_iv()\n        return self.cipher.encrypt(data, header=self.TYPE_STR, iv=next_iv)\n\n    def decrypt(self, id, data):\n        self.assert_type(data[0], id)\n        try:\n            return self.cipher.decrypt(data)\n        except IntegrityError as e:\n            raise IntegrityError(f\"Chunk {bin_to_hex(id)}: Could not decrypt [{str(e)}]\")\n\n    def init_from_given_data(self, *, crypt_key, id_key, chunk_seed):\n        assert len(crypt_key) in (32 + 32, 32 + 128)\n        assert len(id_key) in (32, 128)\n        assert isinstance(chunk_seed, int)\n        self.crypt_key = crypt_key\n        self.id_key = id_key\n        self.chunk_seed = chunk_seed\n\n    def init_from_random_data(self):\n        data = os.urandom(100)\n        chunk_seed = bytes_to_int(data[96:100])\n        # Convert to signed int32\n        if chunk_seed & 0x80000000:\n            chunk_seed = chunk_seed - 0xFFFFFFFF - 1\n        self.init_from_given_data(crypt_key=data[0:64], id_key=data[64:96], chunk_seed=chunk_seed)\n\n    def init_ciphers(self, manifest_data=None):\n        enc_key, enc_hmac_key = self.crypt_key[0:32], self.crypt_key[32:]\n        self.cipher = self.CIPHERSUITE(mac_key=enc_hmac_key, enc_key=enc_key, header_len=1, aad_offset=1)\n        if manifest_data is None:\n            nonce = 0\n        else:\n            self.assert_type(manifest_data[0])\n            # manifest_blocks is a safe upper bound on the amount of cipher blocks needed\n            # to encrypt the manifest. depending on the ciphersuite and overhead, it might\n            # be a bit too high, but that does not matter.\n            manifest_blocks = num_cipher_blocks(len(manifest_data))\n            nonce = self.cipher.extract_iv(manifest_data) + manifest_blocks\n        self.cipher.set_iv(nonce)\n\n\nclass FlexiKey:\n    FILE_ID = \"BORG_KEY\"\n    STORAGE: ClassVar[str] = KeyBlobStorage.NO_STORAGE  # override in subclass\n\n    @classmethod\n    def detect(cls, repository, manifest_data):\n        key = cls(repository)\n        target = key.find_key()\n        prompt = \"Enter passphrase for key %s: \" % target\n        passphrase = Passphrase.env_passphrase()\n        if passphrase is None:\n            passphrase = Passphrase()\n            if not key.load(target, passphrase):\n                for retry in range(0, 3):\n                    passphrase = Passphrase.getpass(prompt)\n                    if key.load(target, passphrase):\n                        break\n                else:\n                    raise PasswordRetriesExceeded\n        else:\n            if not key.load(target, passphrase):\n                raise PassphraseWrong\n        key.init_ciphers(manifest_data)\n        key._passphrase = passphrase\n        return key\n\n    def _load(self, key_data, passphrase):\n        cdata = a2b_base64(key_data)\n        data = self.decrypt_key_file(cdata, passphrase)\n        if data:\n            data = msgpack.unpackb(data)\n            key = Key(internal_dict=data)\n            if key.version not in (1, 2):  # legacy: item.Key can still process v1 keys\n                raise UnsupportedKeyFormatError()\n            self.repository_id = key.repository_id\n            self.crypt_key = key.crypt_key\n            self.id_key = key.id_key\n            self.chunk_seed = key.chunk_seed\n            self.tam_required = key.get(\"tam_required\", tam_required(self.repository))\n            return True\n        return False\n\n    def decrypt_key_file(self, data, passphrase):\n        unpacker = get_limited_unpacker(\"key\")\n        unpacker.feed(data)\n        data = unpacker.unpack()\n        encrypted_key = EncryptedKey(internal_dict=data)\n        if encrypted_key.version != 1:\n            raise UnsupportedKeyFormatError()\n        else:\n            self._encrypted_key_algorithm = encrypted_key.algorithm\n            if encrypted_key.algorithm == \"sha256\":\n                return self.decrypt_key_file_pbkdf2(encrypted_key, passphrase)\n            elif encrypted_key.algorithm == \"argon2 chacha20-poly1305\":\n                return self.decrypt_key_file_argon2(encrypted_key, passphrase)\n            else:\n                raise UnsupportedKeyFormatError()\n\n    @staticmethod\n    def pbkdf2(passphrase, salt, iterations, output_len_in_bytes):\n        if os.environ.get(\"BORG_TESTONLY_WEAKEN_KDF\") == \"1\":\n            iterations = 1\n        return pbkdf2_hmac(\"sha256\", passphrase.encode(\"utf-8\"), salt, iterations, output_len_in_bytes)\n\n    @staticmethod\n    def argon2(\n        passphrase: str,\n        output_len_in_bytes: int,\n        salt: bytes,\n        time_cost: int,\n        memory_cost: int,\n        parallelism: int,\n        type: Literal[\"i\", \"d\", \"id\"],\n    ) -> bytes:\n        if os.environ.get(\"BORG_TESTONLY_WEAKEN_KDF\") == \"1\":\n            time_cost = 1\n            parallelism = 1\n            # 8 is the smallest value that avoids the \"Memory cost is too small\" exception\n            memory_cost = 8\n        type_map = {\"i\": argon2.low_level.Type.I, \"d\": argon2.low_level.Type.D, \"id\": argon2.low_level.Type.ID}\n        key = argon2.low_level.hash_secret_raw(\n            secret=passphrase.encode(\"utf-8\"),\n            hash_len=output_len_in_bytes,\n            salt=salt,\n            time_cost=time_cost,\n            memory_cost=memory_cost,\n            parallelism=parallelism,\n            type=type_map[type],\n        )\n        return key\n\n    def decrypt_key_file_pbkdf2(self, encrypted_key, passphrase):\n        key = self.pbkdf2(passphrase, encrypted_key.salt, encrypted_key.iterations, 32)\n        data = AES(key, b\"\\0\" * 16).decrypt(encrypted_key.data)\n        if hmac.compare_digest(hmac_sha256(key, data), encrypted_key.hash):\n            return data\n        return None\n\n    def decrypt_key_file_argon2(self, encrypted_key, passphrase):\n        key = self.argon2(\n            passphrase,\n            output_len_in_bytes=32,\n            salt=encrypted_key.salt,\n            time_cost=encrypted_key.argon2_time_cost,\n            memory_cost=encrypted_key.argon2_memory_cost,\n            parallelism=encrypted_key.argon2_parallelism,\n            type=encrypted_key.argon2_type,\n        )\n        ae_cipher = CHACHA20_POLY1305(key=key, iv=0, header_len=0, aad_offset=0)\n        try:\n            return ae_cipher.decrypt(encrypted_key.data)\n        except low_level.IntegrityError:\n            return None\n\n    def encrypt_key_file(self, data, passphrase, algorithm):\n        if algorithm == \"sha256\":\n            return self.encrypt_key_file_pbkdf2(data, passphrase)\n        elif algorithm == \"argon2 chacha20-poly1305\":\n            return self.encrypt_key_file_argon2(data, passphrase)\n        else:\n            raise ValueError(f\"Unexpected algorithm: {algorithm}\")\n\n    def encrypt_key_file_pbkdf2(self, data, passphrase):\n        salt = os.urandom(32)\n        iterations = PBKDF2_ITERATIONS\n        key = self.pbkdf2(passphrase, salt, iterations, 32)\n        hash = hmac_sha256(key, data)\n        cdata = AES(key, b\"\\0\" * 16).encrypt(data)\n        enc_key = EncryptedKey(version=1, salt=salt, iterations=iterations, algorithm=\"sha256\", hash=hash, data=cdata)\n        return msgpack.packb(enc_key.as_dict())\n\n    def encrypt_key_file_argon2(self, data, passphrase):\n        salt = os.urandom(ARGON2_SALT_BYTES)\n        key = self.argon2(passphrase, output_len_in_bytes=32, salt=salt, **ARGON2_ARGS)\n        ae_cipher = CHACHA20_POLY1305(key=key, iv=0, header_len=0, aad_offset=0)\n        encrypted_key = EncryptedKey(\n            version=1,\n            algorithm=\"argon2 chacha20-poly1305\",\n            salt=salt,\n            data=ae_cipher.encrypt(data),\n            **{\"argon2_\" + k: v for k, v in ARGON2_ARGS.items()},\n        )\n        return msgpack.packb(encrypted_key.as_dict())\n\n    def _save(self, passphrase, algorithm):\n        key = Key(\n            version=2,\n            repository_id=self.repository_id,\n            crypt_key=self.crypt_key,\n            id_key=self.id_key,\n            chunk_seed=self.chunk_seed,\n            tam_required=self.tam_required,\n        )\n        data = self.encrypt_key_file(msgpack.packb(key.as_dict()), passphrase, algorithm)\n        key_data = \"\\n\".join(textwrap.wrap(b2a_base64(data).decode(\"ascii\")))\n        return key_data\n\n    def change_passphrase(self, passphrase=None):\n        if passphrase is None:\n            passphrase = Passphrase.new(allow_empty=True)\n        self.save(self.target, passphrase, algorithm=self._encrypted_key_algorithm)\n\n    @classmethod\n    def create(cls, repository, args, *, other_key=None):\n        key = cls(repository)\n        key.repository_id = repository.id\n        if other_key is not None:\n            if isinstance(other_key, PlaintextKey):\n                raise Error(\"Copying key material from an unencrypted repository is not possible.\")\n            if isinstance(key, AESKeyBase):\n                # user must use an AEADKeyBase subclass (AEAD modes with session keys)\n                raise Error(\"Copying key material to an AES-CTR based mode is insecure and unsupported.\")\n            if not uses_same_id_hash(other_key, key):\n                raise Error(\"You must keep the same ID hash (HMAC-SHA256 or BLAKE2b) or deduplication will break.\")\n            if other_key.copy_crypt_key:\n                # give the user the option to use the same authenticated encryption (AE) key\n                crypt_key = other_key.crypt_key\n            else:\n                # borg transfer re-encrypts all data anyway, thus we can default to a new, random AE key\n                crypt_key = os.urandom(64)\n            key.init_from_given_data(crypt_key=crypt_key, id_key=other_key.id_key, chunk_seed=other_key.chunk_seed)\n            passphrase = other_key._passphrase\n        else:\n            key.init_from_random_data()\n            passphrase = Passphrase.new(allow_empty=True)\n        key.init_ciphers()\n        target = key.get_new_target(args)\n        key.save(target, passphrase, create=True, algorithm=KEY_ALGORITHMS[\"argon2\"])\n        logger.info('Key in \"%s\" created.' % target)\n        logger.info(\"Keep this key safe. Your data will be inaccessible without it.\")\n        return key\n\n    def sanity_check(self, filename, id):\n        file_id = self.FILE_ID.encode() + b\" \"\n        repo_id = hexlify(id)\n        with open(filename, \"rb\") as fd:\n            # we do the magic / id check in binary mode to avoid stumbling over\n            # decoding errors if somebody has binary files in the keys dir for some reason.\n            if fd.read(len(file_id)) != file_id:\n                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)\n            if fd.read(len(repo_id)) != repo_id:\n                raise KeyfileMismatchError(self.repository._location.canonical_path(), filename)\n        # we get here if it really looks like a borg key for this repo,\n        # do some more checks that are close to how borg reads/parses the key.\n        with open(filename, \"r\") as fd:\n            lines = fd.readlines()\n            if len(lines) < 2:\n                logger.warning(f\"borg key sanity check: expected 2+ lines total. [{filename}]\")\n                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)\n            if len(lines[0].rstrip()) > len(file_id) + len(repo_id):\n                logger.warning(f\"borg key sanity check: key line 1 seems too long. [{filename}]\")\n                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)\n            key_b64 = \"\".join(lines[1:])\n            try:\n                key = a2b_base64(key_b64)\n            except binascii.Error:\n                logger.warning(f\"borg key sanity check: key line 2+ does not look like base64. [{filename}]\")\n                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)\n            if len(key) < 20:\n                # this is in no way a precise check, usually we have about 400b key data.\n                logger.warning(\n                    f\"borg key sanity check: binary encrypted key data from key line 2+ suspiciously short.\"\n                    f\" [{filename}]\"\n                )\n                raise KeyfileInvalidError(self.repository._location.canonical_path(), filename)\n        # looks good!\n        return filename\n\n    def find_key(self):\n        if self.STORAGE == KeyBlobStorage.KEYFILE:\n            keyfile = self._find_key_file_from_environment()\n            if keyfile is not None:\n                return self.sanity_check(keyfile, self.repository.id)\n            keyfile = self._find_key_in_keys_dir()\n            if keyfile is not None:\n                return keyfile\n            raise KeyfileNotFoundError(self.repository._location.canonical_path(), get_keys_dir())\n        elif self.STORAGE == KeyBlobStorage.REPO:\n            loc = self.repository._location.canonical_path()\n            key = self.repository.load_key()\n            if not key:\n                # if we got an empty key, it means there is no key.\n                raise RepoKeyNotFoundError(loc) from None\n            return loc\n        else:\n            raise TypeError(\"Unsupported borg key storage type\")\n\n    def get_existing_or_new_target(self, args):\n        keyfile = self._find_key_file_from_environment()\n        if keyfile is not None:\n            return keyfile\n        keyfile = self._find_key_in_keys_dir()\n        if keyfile is not None:\n            return keyfile\n        return self._get_new_target_in_keys_dir(args)\n\n    def _find_key_in_keys_dir(self):\n        id = self.repository.id\n        keys_dir = get_keys_dir()\n        for name in os.listdir(keys_dir):\n            filename = os.path.join(keys_dir, name)\n            try:\n                return self.sanity_check(filename, id)\n            except (KeyfileInvalidError, KeyfileMismatchError):\n                pass\n\n    def get_new_target(self, args):\n        if self.STORAGE == KeyBlobStorage.KEYFILE:\n            keyfile = self._find_key_file_from_environment()\n            if keyfile is not None:\n                return keyfile\n            return self._get_new_target_in_keys_dir(args)\n        elif self.STORAGE == KeyBlobStorage.REPO:\n            return self.repository\n        else:\n            raise TypeError(\"Unsupported borg key storage type\")\n\n    def _find_key_file_from_environment(self):\n        keyfile = os.environ.get(\"BORG_KEY_FILE\")\n        if keyfile:\n            return os.path.abspath(keyfile)\n\n    def _get_new_target_in_keys_dir(self, args):\n        filename = args.location.to_key_filename()\n        path = filename\n        i = 1\n        while os.path.exists(path):\n            i += 1\n            path = filename + \".%d\" % i\n        return path\n\n    def load(self, target, passphrase):\n        if self.STORAGE == KeyBlobStorage.KEYFILE:\n            with open(target) as fd:\n                key_data = \"\".join(fd.readlines()[1:])\n        elif self.STORAGE == KeyBlobStorage.REPO:\n            # While the repository is encrypted, we consider a repokey repository with a blank\n            # passphrase an unencrypted repository.\n            self.logically_encrypted = passphrase != \"\"\n\n            # what we get in target is just a repo location, but we already have the repo obj:\n            target = self.repository\n            key_data = target.load_key()\n            if not key_data:\n                # if we got an empty key, it means there is no key.\n                loc = target._location.canonical_path()\n                raise RepoKeyNotFoundError(loc) from None\n            key_data = key_data.decode(\"utf-8\")  # remote repo: msgpack issue #99, getting bytes\n        else:\n            raise TypeError(\"Unsupported borg key storage type\")\n        success = self._load(key_data, passphrase)\n        if success:\n            self.target = target\n        return success\n\n    def save(self, target, passphrase, algorithm, create=False):\n        key_data = self._save(passphrase, algorithm)\n        if self.STORAGE == KeyBlobStorage.KEYFILE:\n            if create and os.path.isfile(target):\n                # if a new keyfile key repository is created, ensure that an existing keyfile of another\n                # keyfile key repo is not accidentally overwritten by careless use of the BORG_KEY_FILE env var.\n                # see issue #6036\n                raise Error('Aborting because key in \"%s\" already exists.' % target)\n            with SaveFile(target) as fd:\n                fd.write(f\"{self.FILE_ID} {bin_to_hex(self.repository_id)}\\n\")\n                fd.write(key_data)\n                fd.write(\"\\n\")\n        elif self.STORAGE == KeyBlobStorage.REPO:\n            self.logically_encrypted = passphrase != \"\"\n            key_data = key_data.encode(\"utf-8\")  # remote repo: msgpack issue #99, giving bytes\n            target.save_key(key_data)\n        else:\n            raise TypeError(\"Unsupported borg key storage type\")\n        self.target = target\n\n    def remove(self, target):\n        if self.STORAGE == KeyBlobStorage.KEYFILE:\n            os.remove(target)\n        elif self.STORAGE == KeyBlobStorage.REPO:\n            target.save_key(b\"\")  # save empty key (no new api at remote repo necessary)\n        else:\n            raise TypeError(\"Unsupported borg key storage type\")\n\n\nclass KeyfileKey(ID_HMAC_SHA_256, AESKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.KEYFILE, KeyType.REPO, KeyType.PASSPHRASE}\n    TYPE = KeyType.KEYFILE\n    NAME = \"key file\"\n    ARG_NAME = \"keyfile\"\n    STORAGE = KeyBlobStorage.KEYFILE\n    CIPHERSUITE = AES256_CTR_HMAC_SHA256\n\n\nclass RepoKey(ID_HMAC_SHA_256, AESKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.KEYFILE, KeyType.REPO, KeyType.PASSPHRASE}\n    TYPE = KeyType.REPO\n    NAME = \"repokey\"\n    ARG_NAME = \"repokey\"\n    STORAGE = KeyBlobStorage.REPO\n    CIPHERSUITE = AES256_CTR_HMAC_SHA256\n\n\nclass Blake2KeyfileKey(ID_BLAKE2b_256, AESKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.BLAKE2KEYFILE, KeyType.BLAKE2REPO}\n    TYPE = KeyType.BLAKE2KEYFILE\n    NAME = \"key file BLAKE2b\"\n    ARG_NAME = \"keyfile-blake2\"\n    STORAGE = KeyBlobStorage.KEYFILE\n    CIPHERSUITE = AES256_CTR_BLAKE2b\n\n\nclass Blake2RepoKey(ID_BLAKE2b_256, AESKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.BLAKE2KEYFILE, KeyType.BLAKE2REPO}\n    TYPE = KeyType.BLAKE2REPO\n    NAME = \"repokey BLAKE2b\"\n    ARG_NAME = \"repokey-blake2\"\n    STORAGE = KeyBlobStorage.REPO\n    CIPHERSUITE = AES256_CTR_BLAKE2b\n\n\nclass AuthenticatedKeyBase(AESKeyBase, FlexiKey):\n    STORAGE = KeyBlobStorage.REPO\n\n    # It's only authenticated, not encrypted.\n    logically_encrypted = False\n\n    def _load(self, key_data, passphrase):\n        if AUTHENTICATED_NO_KEY:\n            # fake _load if we have no key or passphrase\n            NOPE = bytes(32)  # 256 bit all-zero\n            self.repository_id = NOPE\n            self.enc_key = NOPE\n            self.enc_hmac_key = NOPE\n            self.id_key = NOPE\n            self.chunk_seed = 0\n            self.tam_required = False\n            return True\n        return super()._load(key_data, passphrase)\n\n    def load(self, target, passphrase):\n        success = super().load(target, passphrase)\n        self.logically_encrypted = False\n        return success\n\n    def save(self, target, passphrase, algorithm, create=False):\n        super().save(target, passphrase, algorithm, create=create)\n        self.logically_encrypted = False\n\n    def init_ciphers(self, manifest_data=None):\n        if manifest_data is not None:\n            self.assert_type(manifest_data[0])\n\n    def encrypt(self, id, data):\n        return b\"\".join([self.TYPE_STR, data])\n\n    def decrypt(self, id, data):\n        self.assert_type(data[0], id)\n        return memoryview(data)[1:]\n\n\nclass AuthenticatedKey(ID_HMAC_SHA_256, AuthenticatedKeyBase):\n    TYPE = KeyType.AUTHENTICATED\n    TYPES_ACCEPTABLE = {TYPE}\n    NAME = \"authenticated\"\n    ARG_NAME = \"authenticated\"\n\n\nclass Blake2AuthenticatedKey(ID_BLAKE2b_256, AuthenticatedKeyBase):\n    TYPE = KeyType.BLAKE2AUTHENTICATED\n    TYPES_ACCEPTABLE = {TYPE}\n    NAME = \"authenticated BLAKE2b\"\n    ARG_NAME = \"authenticated-blake2\"\n\n\n# ------------ new crypto ------------\n\n\nclass AEADKeyBase(KeyBase):\n    \"\"\"\n    Chunks are encrypted and authenticated using some AEAD ciphersuite\n\n    Layout: suite:4 keytype:4 reserved:8 messageIV:48 sessionID:192 auth_tag:128 payload:... [bits]\n            ^-------------------- AAD ----------------------------^\n    Offsets:0                 1          2            8             32           48 [bytes]\n\n    suite: 1010b for new AEAD crypto, 0000b is old crypto\n    keytype: see constants.KeyType (suite+keytype)\n    reserved: all-zero, for future use\n    messageIV: a counter starting from 0 for all new encrypted messages of one session\n    sessionID: 192bit random, computed once per session (the session key is derived from this)\n    auth_tag: authentication tag output of the AEAD cipher (computed over payload and AAD)\n    payload: encrypted chunk data\n    \"\"\"\n\n    PAYLOAD_OVERHEAD = 1 + 1 + 6 + 24 + 16  # [bytes], see Layout\n\n    CIPHERSUITE: Callable = None  # override in subclass\n\n    logically_encrypted = True\n\n    MAX_IV = 2**48 - 1\n\n    def assert_id(self, id, data):\n        # Comparing the id hash here would not be needed any more for the new AEAD crypto **IF** we\n        # could be sure that chunks were created by normal (not tampered, not evil) borg code:\n        # We put the id into AAD when storing the chunk, so it gets into the authentication tag computation.\n        # when decrypting, we provide the id we **want** as AAD for the auth tag verification, so\n        # decrypting only succeeds if we got the ciphertext we wrote **for that chunk id**.\n        # So, basically the **repository** can not cheat on us by giving us a different chunk.\n        #\n        # **BUT**, if chunks are created by tampered, evil borg code, the borg client code could put\n        # a wrong chunkid into AAD and then AEAD-encrypt-and-auth this and store it into the\n        # repository using this bad chunkid as key (violating the usual chunkid == id_hash(data)).\n        # Later, when reading such a bad chunk, AEAD-auth-and-decrypt would not notice any\n        # issue and decrypt successfully.\n        # Thus, to notice such evil borg activity, we must check for such violations here:\n        if id and id != Manifest.MANIFEST_ID:\n            id_computed = self.id_hash(data)\n            if not hmac.compare_digest(id_computed, id):\n                raise IntegrityError(\"Chunk %s: id verification failed\" % bin_to_hex(id))\n\n    def encrypt(self, id, data):\n        # to encrypt new data in this session we use always self.cipher and self.sessionid\n        reserved = b\"\\0\"\n        iv = self.cipher.next_iv()\n        if iv > self.MAX_IV:  # see the data-structures docs about why the IV range is enough\n            raise IntegrityError(\"IV overflow, should never happen.\")\n        iv_48bit = iv.to_bytes(6, \"big\")\n        header = self.TYPE_STR + reserved + iv_48bit + self.sessionid\n        return self.cipher.encrypt(data, header=header, iv=iv, aad=id)\n\n    def decrypt(self, id, data):\n        # to decrypt existing data, we need to get a cipher configured for the sessionid and iv from header\n        self.assert_type(data[0], id)\n        iv_48bit = data[2:8]\n        sessionid = data[8:32]\n        iv = int.from_bytes(iv_48bit, \"big\")\n        cipher = self._get_cipher(sessionid, iv)\n        try:\n            return cipher.decrypt(data, aad=id)\n        except IntegrityError as e:\n            raise IntegrityError(f\"Chunk {bin_to_hex(id)}: Could not decrypt [{str(e)}]\")\n\n    def init_from_given_data(self, *, crypt_key, id_key, chunk_seed):\n        assert len(crypt_key) in (32 + 32, 32 + 128)\n        assert len(id_key) in (32, 128)\n        assert isinstance(chunk_seed, int)\n        self.crypt_key = crypt_key\n        self.id_key = id_key\n        self.chunk_seed = chunk_seed\n\n    def init_from_random_data(self):\n        data = os.urandom(100)\n        chunk_seed = bytes_to_int(data[96:100])\n        # Convert to signed int32\n        if chunk_seed & 0x80000000:\n            chunk_seed = chunk_seed - 0xFFFFFFFF - 1\n        self.init_from_given_data(crypt_key=data[0:64], id_key=data[64:96], chunk_seed=chunk_seed)\n\n    def _get_session_key(self, sessionid):\n        assert len(sessionid) == 24  # 192bit\n        key = hkdf_hmac_sha512(\n            ikm=self.crypt_key,\n            salt=sessionid,\n            info=b\"borg-session-key-\" + self.CIPHERSUITE.__name__.encode(),\n            output_length=32,\n        )\n        return key\n\n    def _get_cipher(self, sessionid, iv):\n        assert isinstance(iv, int)\n        key = self._get_session_key(sessionid)\n        cipher = self.CIPHERSUITE(key=key, iv=iv, header_len=1 + 1 + 6 + 24, aad_offset=0)\n        return cipher\n\n    def init_ciphers(self, manifest_data=None, iv=0):\n        # in every new session we start with a fresh sessionid and at iv == 0, manifest_data and iv params are ignored\n        self.sessionid = os.urandom(24)\n        self.cipher = self._get_cipher(self.sessionid, iv=0)\n\n\nclass AESOCBKeyfileKey(ID_HMAC_SHA_256, AEADKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.AESOCBKEYFILE, KeyType.AESOCBREPO}\n    TYPE = KeyType.AESOCBKEYFILE\n    NAME = \"key file AES-OCB\"\n    ARG_NAME = \"keyfile-aes-ocb\"\n    STORAGE = KeyBlobStorage.KEYFILE\n    CIPHERSUITE = AES256_OCB\n\n\nclass AESOCBRepoKey(ID_HMAC_SHA_256, AEADKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.AESOCBKEYFILE, KeyType.AESOCBREPO}\n    TYPE = KeyType.AESOCBREPO\n    NAME = \"repokey AES-OCB\"\n    ARG_NAME = \"repokey-aes-ocb\"\n    STORAGE = KeyBlobStorage.REPO\n    CIPHERSUITE = AES256_OCB\n\n\nclass CHPOKeyfileKey(ID_HMAC_SHA_256, AEADKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.CHPOKEYFILE, KeyType.CHPOREPO}\n    TYPE = KeyType.CHPOKEYFILE\n    NAME = \"key file ChaCha20-Poly1305\"\n    ARG_NAME = \"keyfile-chacha20-poly1305\"\n    STORAGE = KeyBlobStorage.KEYFILE\n    CIPHERSUITE = CHACHA20_POLY1305\n\n\nclass CHPORepoKey(ID_HMAC_SHA_256, AEADKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.CHPOKEYFILE, KeyType.CHPOREPO}\n    TYPE = KeyType.CHPOREPO\n    NAME = \"repokey ChaCha20-Poly1305\"\n    ARG_NAME = \"repokey-chacha20-poly1305\"\n    STORAGE = KeyBlobStorage.REPO\n    CIPHERSUITE = CHACHA20_POLY1305\n\n\nclass Blake2AESOCBKeyfileKey(ID_BLAKE2b_256, AEADKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.BLAKE2AESOCBKEYFILE, KeyType.BLAKE2AESOCBREPO}\n    TYPE = KeyType.BLAKE2AESOCBKEYFILE\n    NAME = \"key file BLAKE2b AES-OCB\"\n    ARG_NAME = \"keyfile-blake2-aes-ocb\"\n    STORAGE = KeyBlobStorage.KEYFILE\n    CIPHERSUITE = AES256_OCB\n\n\nclass Blake2AESOCBRepoKey(ID_BLAKE2b_256, AEADKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.BLAKE2AESOCBKEYFILE, KeyType.BLAKE2AESOCBREPO}\n    TYPE = KeyType.BLAKE2AESOCBREPO\n    NAME = \"repokey BLAKE2b AES-OCB\"\n    ARG_NAME = \"repokey-blake2-aes-ocb\"\n    STORAGE = KeyBlobStorage.REPO\n    CIPHERSUITE = AES256_OCB\n\n\nclass Blake2CHPOKeyfileKey(ID_BLAKE2b_256, AEADKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.BLAKE2CHPOKEYFILE, KeyType.BLAKE2CHPOREPO}\n    TYPE = KeyType.BLAKE2CHPOKEYFILE\n    NAME = \"key file BLAKE2b ChaCha20-Poly1305\"\n    ARG_NAME = \"keyfile-blake2-chacha20-poly1305\"\n    STORAGE = KeyBlobStorage.KEYFILE\n    CIPHERSUITE = CHACHA20_POLY1305\n\n\nclass Blake2CHPORepoKey(ID_BLAKE2b_256, AEADKeyBase, FlexiKey):\n    TYPES_ACCEPTABLE = {KeyType.BLAKE2CHPOKEYFILE, KeyType.BLAKE2CHPOREPO}\n    TYPE = KeyType.BLAKE2CHPOREPO\n    NAME = \"repokey BLAKE2b ChaCha20-Poly1305\"\n    ARG_NAME = \"repokey-blake2-chacha20-poly1305\"\n    STORAGE = KeyBlobStorage.REPO\n    CIPHERSUITE = CHACHA20_POLY1305\n\n\nLEGACY_KEY_TYPES = (\n    # legacy (AES-CTR based) crypto\n    KeyfileKey,\n    RepoKey,\n    Blake2KeyfileKey,\n    Blake2RepoKey,\n)\n\nAVAILABLE_KEY_TYPES = (\n    # these are available encryption modes for new repositories\n    # not encrypted modes\n    PlaintextKey,\n    AuthenticatedKey,\n    Blake2AuthenticatedKey,\n    # new crypto\n    AESOCBKeyfileKey,\n    AESOCBRepoKey,\n    CHPOKeyfileKey,\n    CHPORepoKey,\n    Blake2AESOCBKeyfileKey,\n    Blake2AESOCBRepoKey,\n    Blake2CHPOKeyfileKey,\n    Blake2CHPORepoKey,\n)\n", "\"\"\"\nwrapping msgpack\n================\n\nWe wrap msgpack here the way we need it - to avoid having lots of clutter in the calling code.\n\nPacking\n-------\n- use_bin_type = True (used by borg since borg 2.0)\n  This is used to generate output according to new msgpack 2.0 spec.\n  This cleanly keeps bytes and str types apart.\n\n- use_bin_type = False (used by borg < 1.3)\n  This creates output according to the older msgpack spec.\n  BAD: str and bytes were packed into same \"raw\" representation.\n\n- unicode_errors = 'surrogateescape'\n  Guess backup applications are one of the rare cases when this needs to be used.\n  It is needed because borg also needs to deal with data that does not cleanly encode/decode using utf-8.\n  There's a lot of crap out there, e.g. in filenames and as a backup tool, we must keep them as good as possible.\n\nUnpacking\n---------\n- raw = False (used by borg since borg 2.0)\n  We already can use this with borg 2.0 due to the type conversion to the desired type in item.py update_internal\n  methods. This type conversion code can be removed in future, when we do not have to deal with data any more\n  that was packed the old way.\n  It will then unpack according to the msgpack 2.0 spec format and directly output bytes or str.\n\n- raw = True (the old way, used by borg < 1.3)\n\n- unicode_errors = 'surrogateescape' -> see description above (will be used when raw is False).\n\nAs of borg 2.0, we have fixed most of the msgpack str/bytes mess, #968.\nBorg now still needs to **read** old repos, archives, keys, ... so we can not yet fix it completely.\nBut from now on, borg only **writes** new data according to the new msgpack 2.0 spec,\nthus we can remove some legacy support in a later borg release (some places are marked with \"legacy\").\n\ncurrent way in msgpack terms\n----------------------------\n\n- pack with use_bin_type=True (according to msgpack 2.0 spec)\n- packs str -> raw and bytes -> bin\n- unpack with raw=False (according to msgpack 2.0 spec, using unicode_errors='surrogateescape')\n- unpacks bin to bytes and raw to str (thus we need to convert to desired type if we want bytes from \"raw\")\n\"\"\"\n\nfrom .datastruct import StableDict\nfrom ..constants import *  # NOQA\n\nfrom msgpack import Packer as mp_Packer\nfrom msgpack import packb as mp_packb\nfrom msgpack import pack as mp_pack\nfrom msgpack import Unpacker as mp_Unpacker\nfrom msgpack import unpackb as mp_unpackb\nfrom msgpack import unpack as mp_unpack\nfrom msgpack import version as mp_version\n\nfrom msgpack import ExtType, Timestamp\nfrom msgpack import OutOfData\n\n\nversion = mp_version\n\nUSE_BIN_TYPE = True\nRAW = False\nUNICODE_ERRORS = \"surrogateescape\"\n\n\nclass PackException(Exception):\n    \"\"\"Exception while msgpack packing\"\"\"\n\n\nclass UnpackException(Exception):\n    \"\"\"Exception while msgpack unpacking\"\"\"\n\n\nclass Packer(mp_Packer):\n    def __init__(\n        self,\n        *,\n        default=None,\n        unicode_errors=UNICODE_ERRORS,\n        use_single_float=False,\n        autoreset=True,\n        use_bin_type=USE_BIN_TYPE,\n        strict_types=False\n    ):\n        assert unicode_errors == UNICODE_ERRORS\n        super().__init__(\n            default=default,\n            unicode_errors=unicode_errors,\n            use_single_float=use_single_float,\n            autoreset=autoreset,\n            use_bin_type=use_bin_type,\n            strict_types=strict_types,\n        )\n\n    def pack(self, obj):\n        try:\n            return super().pack(obj)\n        except Exception as e:\n            raise PackException(e)\n\n\ndef packb(o, *, use_bin_type=USE_BIN_TYPE, unicode_errors=UNICODE_ERRORS, **kwargs):\n    assert unicode_errors == UNICODE_ERRORS\n    try:\n        return mp_packb(o, use_bin_type=use_bin_type, unicode_errors=unicode_errors, **kwargs)\n    except Exception as e:\n        raise PackException(e)\n\n\ndef pack(o, stream, *, use_bin_type=USE_BIN_TYPE, unicode_errors=UNICODE_ERRORS, **kwargs):\n    assert unicode_errors == UNICODE_ERRORS\n    try:\n        return mp_pack(o, stream, use_bin_type=use_bin_type, unicode_errors=unicode_errors, **kwargs)\n    except Exception as e:\n        raise PackException(e)\n\n\nclass Unpacker(mp_Unpacker):\n    def __init__(\n        self,\n        file_like=None,\n        *,\n        read_size=0,\n        use_list=True,\n        raw=RAW,\n        object_hook=None,\n        object_pairs_hook=None,\n        list_hook=None,\n        unicode_errors=UNICODE_ERRORS,\n        max_buffer_size=0,\n        ext_hook=ExtType,\n        strict_map_key=False\n    ):\n        assert raw == RAW\n        assert unicode_errors == UNICODE_ERRORS\n        kw = dict(\n            file_like=file_like,\n            read_size=read_size,\n            use_list=use_list,\n            raw=raw,\n            object_hook=object_hook,\n            object_pairs_hook=object_pairs_hook,\n            list_hook=list_hook,\n            unicode_errors=unicode_errors,\n            max_buffer_size=max_buffer_size,\n            ext_hook=ext_hook,\n            strict_map_key=strict_map_key,\n        )\n        super().__init__(**kw)\n\n    def unpack(self):\n        try:\n            return super().unpack()\n        except OutOfData:\n            raise\n        except Exception as e:\n            raise UnpackException(e)\n\n    def __next__(self):\n        try:\n            return super().__next__()\n        except StopIteration:\n            raise\n        except Exception as e:\n            raise UnpackException(e)\n\n    next = __next__\n\n\ndef unpackb(packed, *, raw=RAW, unicode_errors=UNICODE_ERRORS, strict_map_key=False, **kwargs):\n    assert raw == RAW\n    assert unicode_errors == UNICODE_ERRORS\n    try:\n        kw = dict(raw=raw, unicode_errors=unicode_errors, strict_map_key=strict_map_key)\n        kw.update(kwargs)\n        return mp_unpackb(packed, **kw)\n    except Exception as e:\n        raise UnpackException(e)\n\n\ndef unpack(stream, *, raw=RAW, unicode_errors=UNICODE_ERRORS, strict_map_key=False, **kwargs):\n    assert raw == RAW\n    assert unicode_errors == UNICODE_ERRORS\n    try:\n        kw = dict(raw=raw, unicode_errors=unicode_errors, strict_map_key=strict_map_key)\n        kw.update(kwargs)\n        return mp_unpack(stream, **kw)\n    except Exception as e:\n        raise UnpackException(e)\n\n\n# msgpacking related utilities -----------------------------------------------\n\n\ndef is_slow_msgpack():\n    import msgpack\n    import msgpack.fallback\n\n    return msgpack.Packer is msgpack.fallback.Packer\n\n\ndef is_supported_msgpack():\n    # DO NOT CHANGE OR REMOVE! See also requirements and comments in setup.cfg.\n    import msgpack\n\n    if msgpack.version in []:  # < add bad releases here to deny list\n        return False\n    return (1, 0, 3) <= msgpack.version <= (1, 0, 5)\n\n\ndef get_limited_unpacker(kind):\n    \"\"\"return a limited Unpacker because we should not trust msgpack data received from remote\"\"\"\n    # Note: msgpack >= 0.6.1 auto-computes DoS-safe max values from len(data) for\n    #       unpack(data) or from max_buffer_size for Unpacker(max_buffer_size=N).\n    args = dict(use_list=False, max_buffer_size=3 * max(BUFSIZE, MAX_OBJECT_SIZE))  # return tuples, not lists\n    if kind in (\"server\", \"client\"):\n        pass  # nothing special\n    elif kind in (\"manifest\", \"archive\", \"key\"):\n        args.update(dict(use_list=True, object_hook=StableDict))  # default value\n    else:\n        raise ValueError('kind must be \"server\", \"client\", \"manifest\", \"archive\" or \"key\"')\n    return Unpacker(**args)\n\n\ndef int_to_timestamp(ns):\n    assert isinstance(ns, int)\n    return Timestamp.from_unix_nano(ns)\n\n\ndef timestamp_to_int(ts):\n    assert isinstance(ts, Timestamp)\n    return ts.to_unix_nano()\n", "import abc\nimport argparse\nimport base64\nimport hashlib\nimport json\nimport os\nimport os.path\nimport re\nimport shlex\nimport stat\nimport uuid\nfrom typing import Dict, Set, Tuple, ClassVar, Any, TYPE_CHECKING, Literal\nfrom binascii import hexlify\nfrom collections import Counter, OrderedDict\nfrom datetime import datetime, timezone\nfrom functools import partial\nfrom string import Formatter\n\nfrom ..logger import create_logger\n\nlogger = create_logger()\n\nfrom .errors import Error\nfrom .fs import get_keys_dir, make_path_safe\nfrom .msgpack import Timestamp\nfrom .time import OutputTimestamp, format_time, safe_timestamp\nfrom .. import __version__ as borg_version\nfrom .. import __version_tuple__ as borg_version_tuple\nfrom ..constants import *  # NOQA\n\nif TYPE_CHECKING:\n    from ..item import ItemDiff\n\n\ndef bin_to_hex(binary):\n    return hexlify(binary).decode(\"ascii\")\n\n\ndef safe_decode(s, coding=\"utf-8\", errors=\"surrogateescape\"):\n    \"\"\"decode bytes to str, with round-tripping \"invalid\" bytes\"\"\"\n    if s is None:\n        return None\n    return s.decode(coding, errors)\n\n\ndef safe_encode(s, coding=\"utf-8\", errors=\"surrogateescape\"):\n    \"\"\"encode str to bytes, with round-tripping \"invalid\" bytes\"\"\"\n    if s is None:\n        return None\n    return s.encode(coding, errors)\n\n\ndef remove_surrogates(s, errors=\"replace\"):\n    \"\"\"Replace surrogates generated by fsdecode with '?'\"\"\"\n    return s.encode(\"utf-8\", errors).decode(\"utf-8\")\n\n\ndef binary_to_json(key, value):\n    assert isinstance(key, str)\n    assert isinstance(value, bytes)\n    return {key + \"_b64\": base64.b64encode(value).decode(\"ascii\")}\n\n\ndef text_to_json(key, value):\n    \"\"\"\n    Return a dict made from key/value that can be fed safely into a JSON encoder.\n\n    JSON can only contain pure, valid unicode (but not: unicode with surrogate escapes).\n\n    But sometimes we have to deal with such values and we do it like this:\n    - <key>: value as pure unicode text (surrogate escapes, if any, replaced by ?)\n    - <key>_b64: value as base64 encoded binary representation (only set if value has surrogate-escapes)\n    \"\"\"\n    coding = \"utf-8\"\n    assert isinstance(key, str)\n    assert isinstance(value, str)  # str might contain surrogate escapes\n    data = {}\n    try:\n        value.encode(coding, errors=\"strict\")  # check if pure unicode\n    except UnicodeEncodeError:\n        # value has surrogate escape sequences\n        data[key] = remove_surrogates(value)\n        value_bytes = value.encode(coding, errors=\"surrogateescape\")\n        data.update(binary_to_json(key, value_bytes))\n    else:\n        # value is pure unicode\n        data[key] = value\n        # we do not give the b64 representation, not needed\n    return data\n\n\ndef join_cmd(argv, rs=False):\n    cmd = shlex.join(argv)\n    return remove_surrogates(cmd) if rs else cmd\n\n\ndef eval_escapes(s):\n    \"\"\"Evaluate literal escape sequences in a string (eg `\\\\n` -> `\\n`).\"\"\"\n    return s.encode(\"ascii\", \"backslashreplace\").decode(\"unicode-escape\")\n\n\ndef decode_dict(d, keys, encoding=\"utf-8\", errors=\"surrogateescape\"):\n    for key in keys:\n        if isinstance(d.get(key), bytes):\n            d[key] = d[key].decode(encoding, errors)\n    return d\n\n\ndef positive_int_validator(value):\n    \"\"\"argparse type for positive integers\"\"\"\n    int_value = int(value)\n    if int_value <= 0:\n        raise argparse.ArgumentTypeError(\"A positive integer is required: %s\" % value)\n    return int_value\n\n\ndef interval(s):\n    \"\"\"Convert a string representing a valid interval to a number of hours.\"\"\"\n    multiplier = {\"H\": 1, \"d\": 24, \"w\": 24 * 7, \"m\": 24 * 31, \"y\": 24 * 365}\n\n    if s.endswith(tuple(multiplier.keys())):\n        number = s[:-1]\n        suffix = s[-1]\n    else:\n        # range suffixes in ascending multiplier order\n        ranges = [k for k, v in sorted(multiplier.items(), key=lambda t: t[1])]\n        raise argparse.ArgumentTypeError(f'Unexpected interval time unit \"{s[-1]}\": expected one of {ranges!r}')\n\n    try:\n        hours = int(number) * multiplier[suffix]\n    except ValueError:\n        hours = -1\n\n    if hours <= 0:\n        raise argparse.ArgumentTypeError('Unexpected interval number \"%s\": expected an integer greater than 0' % number)\n\n    return hours\n\n\ndef ChunkerParams(s):\n    params = s.strip().split(\",\")\n    count = len(params)\n    if count == 0:\n        raise argparse.ArgumentTypeError(\"no chunker params given\")\n    algo = params[0].lower()\n    if algo == CH_FAIL and count == 3:\n        block_size = int(params[1])\n        fail_map = str(params[2])\n        return algo, block_size, fail_map\n    if algo == CH_FIXED and 2 <= count <= 3:  # fixed, block_size[, header_size]\n        block_size = int(params[1])\n        header_size = int(params[2]) if count == 3 else 0\n        if block_size < 64:\n            # we are only disallowing the most extreme cases of abuse here - this does NOT imply\n            # that cutting chunks of the minimum allowed size is efficient concerning storage\n            # or in-memory chunk management.\n            # choose the block (chunk) size wisely: if you have a lot of data and you cut\n            # it into very small chunks, you are asking for trouble!\n            raise argparse.ArgumentTypeError(\"block_size must not be less than 64 Bytes\")\n        if block_size > MAX_DATA_SIZE or header_size > MAX_DATA_SIZE:\n            raise argparse.ArgumentTypeError(\n                \"block_size and header_size must not exceed MAX_DATA_SIZE [%d]\" % MAX_DATA_SIZE\n            )\n        return algo, block_size, header_size\n    if algo == \"default\" and count == 1:  # default\n        return CHUNKER_PARAMS\n    # this must stay last as it deals with old-style compat mode (no algorithm, 4 params, buzhash):\n    if algo == CH_BUZHASH and count == 5 or count == 4:  # [buzhash, ]chunk_min, chunk_max, chunk_mask, window_size\n        chunk_min, chunk_max, chunk_mask, window_size = (int(p) for p in params[count - 4 :])\n        if not (chunk_min <= chunk_mask <= chunk_max):\n            raise argparse.ArgumentTypeError(\"required: chunk_min <= chunk_mask <= chunk_max\")\n        if chunk_min < 6:\n            # see comment in 'fixed' algo check\n            raise argparse.ArgumentTypeError(\n                \"min. chunk size exponent must not be less than 6 (2^6 = 64B min. chunk size)\"\n            )\n        if chunk_max > 23:\n            raise argparse.ArgumentTypeError(\n                \"max. chunk size exponent must not be more than 23 (2^23 = 8MiB max. chunk size)\"\n            )\n        return CH_BUZHASH, chunk_min, chunk_max, chunk_mask, window_size\n    raise argparse.ArgumentTypeError(\"invalid chunker params\")\n\n\ndef FilesCacheMode(s):\n    ENTRIES_MAP = dict(ctime=\"c\", mtime=\"m\", size=\"s\", inode=\"i\", rechunk=\"r\", disabled=\"d\")\n    VALID_MODES = (\"cis\", \"ims\", \"cs\", \"ms\", \"cr\", \"mr\", \"d\", \"s\")  # letters in alpha order\n    entries = set(s.strip().split(\",\"))\n    if not entries <= set(ENTRIES_MAP):\n        raise argparse.ArgumentTypeError(\n            \"cache mode must be a comma-separated list of: %s\" % \",\".join(sorted(ENTRIES_MAP))\n        )\n    short_entries = {ENTRIES_MAP[entry] for entry in entries}\n    mode = \"\".join(sorted(short_entries))\n    if mode not in VALID_MODES:\n        raise argparse.ArgumentTypeError(\"cache mode short must be one of: %s\" % \",\".join(VALID_MODES))\n    return mode\n\n\ndef partial_format(format, mapping):\n    \"\"\"\n    Apply format.format_map(mapping) while preserving unknown keys\n\n    Does not support attribute access, indexing and ![rsa] conversions\n    \"\"\"\n    for key, value in mapping.items():\n        key = re.escape(key)\n        format = re.sub(\n            rf\"(?<!\\{{)((\\{{{key}\\}})|(\\{{{key}:[^\\}}]*\\}}))\", lambda match: match.group(1).format_map(mapping), format\n        )\n    return format\n\n\nclass DatetimeWrapper:\n    def __init__(self, dt):\n        self.dt = dt\n\n    def __format__(self, format_spec):\n        if format_spec == \"\":\n            format_spec = ISO_FORMAT_NO_USECS\n        return self.dt.__format__(format_spec)\n\n\nclass PlaceholderError(Error):\n    \"\"\"Formatting Error: \"{}\".format({}): {}({})\"\"\"\n\n\nclass InvalidPlaceholder(PlaceholderError):\n    \"\"\"Invalid placeholder \"{}\" in string: {}\"\"\"\n\n\ndef format_line(format, data):\n    for _, key, _, conversion in Formatter().parse(format):\n        if not key:\n            continue\n        if conversion or key not in data:\n            raise InvalidPlaceholder(key, format)\n    try:\n        return format.format_map(data)\n    except Exception as e:\n        raise PlaceholderError(format, data, e.__class__.__name__, str(e))\n\n\ndef _replace_placeholders(text, overrides={}):\n    \"\"\"Replace placeholders in text with their values.\"\"\"\n    from ..platform import fqdn, hostname, getosusername\n\n    current_time = datetime.now(timezone.utc)\n    data = {\n        \"pid\": os.getpid(),\n        \"fqdn\": fqdn,\n        \"reverse-fqdn\": \".\".join(reversed(fqdn.split(\".\"))),\n        \"hostname\": hostname,\n        \"now\": DatetimeWrapper(current_time.astimezone()),\n        \"utcnow\": DatetimeWrapper(current_time),\n        \"user\": getosusername(),\n        \"uuid4\": str(uuid.uuid4()),\n        \"borgversion\": borg_version,\n        \"borgmajor\": \"%d\" % borg_version_tuple[:1],\n        \"borgminor\": \"%d.%d\" % borg_version_tuple[:2],\n        \"borgpatch\": \"%d.%d.%d\" % borg_version_tuple[:3],\n        **overrides,\n    }\n    return format_line(text, data)\n\n\nclass PlaceholderReplacer:\n    def __init__(self):\n        self.reset()\n\n    def override(self, key, value):\n        self.overrides[key] = value\n\n    def reset(self):\n        self.overrides = {}\n\n    def __call__(self, text, overrides=None):\n        ovr = {}\n        ovr.update(self.overrides)\n        ovr.update(overrides or {})\n        return _replace_placeholders(text, overrides=ovr)\n\n\nreplace_placeholders = PlaceholderReplacer()\n\n\ndef SortBySpec(text):\n    from ..manifest import AI_HUMAN_SORT_KEYS\n\n    for token in text.split(\",\"):\n        if token not in AI_HUMAN_SORT_KEYS:\n            raise argparse.ArgumentTypeError(\"Invalid sort key: %s\" % token)\n    return text.replace(\"timestamp\", \"ts\")\n\n\ndef format_file_size(v, precision=2, sign=False, iec=False):\n    \"\"\"Format file size into a human friendly format\"\"\"\n    fn = sizeof_fmt_iec if iec else sizeof_fmt_decimal\n    return fn(v, suffix=\"B\", sep=\" \", precision=precision, sign=sign)\n\n\nclass FileSize(int):\n    def __new__(cls, value, iec=False):\n        obj = int.__new__(cls, value)\n        obj.iec = iec\n        return obj\n\n    def __format__(self, format_spec):\n        return format_file_size(int(self), iec=self.iec).__format__(format_spec)\n\n\ndef parse_file_size(s):\n    \"\"\"Return int from file size (1234, 55G, 1.7T).\"\"\"\n    if not s:\n        return int(s)  # will raise\n    suffix = s[-1]\n    power = 1000\n    try:\n        factor = {\"K\": power, \"M\": power**2, \"G\": power**3, \"T\": power**4, \"P\": power**5}[suffix]\n        s = s[:-1]\n    except KeyError:\n        factor = 1\n    return int(float(s) * factor)\n\n\ndef parse_storage_quota(storage_quota):\n    parsed = parse_file_size(storage_quota)\n    if parsed < parse_file_size(\"10M\"):\n        raise argparse.ArgumentTypeError(\"quota is too small (%s). At least 10M are required.\" % storage_quota)\n    return parsed\n\n\ndef sizeof_fmt(num, suffix=\"B\", units=None, power=None, sep=\"\", precision=2, sign=False):\n    sign = \"+\" if sign and num > 0 else \"\"\n    fmt = \"{0:{1}.{2}f}{3}{4}{5}\"\n    prec = 0\n    for unit in units[:-1]:\n        if abs(round(num, precision)) < power:\n            break\n        num /= float(power)\n        prec = precision\n    else:\n        unit = units[-1]\n    return fmt.format(num, sign, prec, sep, unit, suffix)\n\n\ndef sizeof_fmt_iec(num, suffix=\"B\", sep=\"\", precision=2, sign=False):\n    return sizeof_fmt(\n        num,\n        suffix=suffix,\n        sep=sep,\n        precision=precision,\n        sign=sign,\n        units=[\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\", \"Yi\"],\n        power=1024,\n    )\n\n\ndef sizeof_fmt_decimal(num, suffix=\"B\", sep=\"\", precision=2, sign=False):\n    return sizeof_fmt(\n        num,\n        suffix=suffix,\n        sep=sep,\n        precision=precision,\n        sign=sign,\n        units=[\"\", \"k\", \"M\", \"G\", \"T\", \"P\", \"E\", \"Z\", \"Y\"],\n        power=1000,\n    )\n\n\ndef format_archive(archive):\n    return \"%-36s %s [%s]\" % (archive.name, format_time(archive.ts), bin_to_hex(archive.id))\n\n\ndef parse_stringified_list(s):\n    items = re.split(\" *, *\", s)\n    return [item for item in items if item != \"\"]\n\n\nclass Location:\n    \"\"\"Object representing a repository location\"\"\"\n\n    # user must not contain \"@\", \":\" or \"/\".\n    # Quoting adduser error message:\n    # \"To avoid problems, the username should consist only of letters, digits,\n    # underscores, periods, at signs and dashes, and not start with a dash\n    # (as defined by IEEE Std 1003.1-2001).\"\n    # We use \"@\" as separator between username and hostname, so we must\n    # disallow it within the pure username part.\n    optional_user_re = r\"\"\"\n        (?:(?P<user>[^@:/]+)@)?\n    \"\"\"\n\n    # path must not contain :: (it ends at :: or string end), but may contain single colons.\n    # to avoid ambiguities with other regexes, it must also not start with \":\" nor with \"//\" nor with \"ssh://\".\n    local_path_re = r\"\"\"\n        (?!(:|//|ssh://|socket://))                         # not starting with \":\" or // or ssh:// or socket://\n        (?P<path>([^:]|(:(?!:)))+)                          # any chars, but no \"::\"\n        \"\"\"\n\n    # file_path must not contain :: (it ends at :: or string end), but may contain single colons.\n    # it must start with a / and that slash is part of the path.\n    file_path_re = r\"\"\"\n        (?P<path>(([^/]*)/([^:]|(:(?!:)))+))                # start opt. servername, then /, then any chars, but no \"::\"\n        \"\"\"\n\n    # abs_path must not contain :: (it ends at :: or string end), but may contain single colons.\n    # it must start with a / and that slash is part of the path.\n    abs_path_re = r\"\"\"\n        (?P<path>(/([^:]|(:(?!:)))+))                       # start with /, then any chars, but no \"::\"\n        \"\"\"\n\n    # host NAME, or host IP ADDRESS (v4 or v6, v6 must be in square brackets)\n    host_re = r\"\"\"\n        (?P<host>(\n            (?!\\[)[^:/]+(?<!\\])     # hostname or v4 addr, not containing : or / (does not match v6 addr: no brackets!)\n            |\n            \\[[0-9a-fA-F:.]+\\])     # ipv6 address in brackets\n        )\n    \"\"\"\n\n    # regexes for misc. kinds of supported location specifiers:\n    ssh_re = re.compile(\n        r\"\"\"\n        (?P<proto>ssh)://                                       # ssh://\n        \"\"\"\n        + optional_user_re\n        + host_re\n        + r\"\"\"                 # user@  (optional), host name or address\n        (?::(?P<port>\\d+))?                                     # :port (optional)\n        \"\"\"\n        + abs_path_re,\n        re.VERBOSE,\n    )  # path\n\n    socket_re = re.compile(\n        r\"\"\"\n        (?P<proto>socket)://                                    # socket://\n        \"\"\"\n        + abs_path_re,\n        re.VERBOSE,\n    )  # path\n\n    file_re = re.compile(\n        r\"\"\"\n        (?P<proto>file)://                                      # file://\n        \"\"\"\n        + file_path_re,\n        re.VERBOSE,\n    )  # servername/path or path\n\n    local_re = re.compile(local_path_re, re.VERBOSE)  # local path\n\n    win_file_re = re.compile(\n        r\"\"\"\n        (?:file://)?                                        # optional file protocol\n        (?P<path>\n            (?:[a-zA-Z]:)?                                  # Drive letter followed by a colon (optional)\n            (?:[^:]+)                                       # Anything which does not contain a :, at least one char\n        )\n        \"\"\",\n        re.VERBOSE,\n    )\n\n    def __init__(self, text=\"\", overrides={}, other=False):\n        self.repo_env_var = \"BORG_OTHER_REPO\" if other else \"BORG_REPO\"\n        self.valid = False\n        self.proto = None\n        self.user = None\n        self._host = None\n        self.port = None\n        self.path = None\n        self.raw = None\n        self.processed = None\n        self.parse(text, overrides)\n\n    def parse(self, text, overrides={}):\n        if not text:\n            # we did not get a text to parse, so we try to fetch from the environment\n            text = os.environ.get(self.repo_env_var)\n            if text is None:\n                return\n\n        self.raw = text  # as given by user, might contain placeholders\n        self.processed = replace_placeholders(self.raw, overrides)  # after placeholder replacement\n        valid = self._parse(self.processed)\n        if valid:\n            self.valid = True\n        else:\n            raise ValueError('Invalid location format: \"%s\"' % self.processed)\n\n    def _parse(self, text):\n        def normpath_special(p):\n            # avoid that normpath strips away our relative path hack and even makes p absolute\n            relative = p.startswith(\"/./\")\n            p = os.path.normpath(p)\n            return (\"/.\" + p) if relative else p\n\n        m = self.ssh_re.match(text)\n        if m:\n            self.proto = m.group(\"proto\")\n            self.user = m.group(\"user\")\n            self._host = m.group(\"host\")\n            self.port = m.group(\"port\") and int(m.group(\"port\")) or None\n            self.path = normpath_special(m.group(\"path\"))\n            return True\n        m = self.file_re.match(text)\n        if m:\n            self.proto = m.group(\"proto\")\n            self.path = normpath_special(m.group(\"path\"))\n            return True\n        m = self.socket_re.match(text)\n        if m:\n            self.proto = m.group(\"proto\")\n            self.path = normpath_special(m.group(\"path\"))\n            return True\n        m = self.local_re.match(text)\n        if m:\n            self.proto = \"file\"\n            self.path = normpath_special(m.group(\"path\"))\n            return True\n        return False\n\n    def __str__(self):\n        items = [\n            \"proto=%r\" % self.proto,\n            \"user=%r\" % self.user,\n            \"host=%r\" % self.host,\n            \"port=%r\" % self.port,\n            \"path=%r\" % self.path,\n        ]\n        return \", \".join(items)\n\n    def to_key_filename(self):\n        name = re.sub(r\"[^\\w]\", \"_\", self.path).strip(\"_\")\n        if self.proto not in (\"file\", \"socket\"):\n            name = re.sub(r\"[^\\w]\", \"_\", self.host) + \"__\" + name\n        if len(name) > 100:\n            # Limit file names to some reasonable length. Most file systems\n            # limit them to 255 [unit of choice]; due to variations in unicode\n            # handling we truncate to 100 *characters*.\n            name = name[:100]\n        return os.path.join(get_keys_dir(), name)\n\n    def __repr__(self):\n        return \"Location(%s)\" % self\n\n    @property\n    def host(self):\n        # strip square brackets used for IPv6 addrs\n        if self._host is not None:\n            return self._host.lstrip(\"[\").rstrip(\"]\")\n\n    def canonical_path(self):\n        if self.proto in (\"file\", \"socket\"):\n            return self.path\n        else:\n            if self.path and self.path.startswith(\"~\"):\n                path = \"/\" + self.path  # /~/x = path x relative to home dir\n            elif self.path and not self.path.startswith(\"/\"):\n                path = \"/./\" + self.path  # /./x = path x relative to cwd\n            else:\n                path = self.path\n            return \"ssh://{}{}{}{}\".format(\n                f\"{self.user}@\" if self.user else \"\",\n                self._host,  # needed for ipv6 addrs\n                f\":{self.port}\" if self.port else \"\",\n                path,\n            )\n\n    def with_timestamp(self, timestamp):\n        # note: this only affects the repository URL/path, not the archive name!\n        return Location(\n            self.raw,\n            overrides={\n                \"now\": DatetimeWrapper(timestamp),\n                \"utcnow\": DatetimeWrapper(timestamp.astimezone(timezone.utc)),\n            },\n        )\n\n\ndef location_validator(proto=None, other=False):\n    def validator(text):\n        try:\n            loc = Location(text, other=other)\n        except ValueError as err:\n            raise argparse.ArgumentTypeError(str(err)) from None\n        if proto is not None and loc.proto != proto:\n            if proto == \"file\":\n                raise argparse.ArgumentTypeError('\"%s\": Repository must be local' % text)\n            else:\n                raise argparse.ArgumentTypeError('\"%s\": Repository must be remote' % text)\n        return loc\n\n    return validator\n\n\ndef relative_time_marker_validator(text: str):\n    time_marker_regex = r\"^\\d+[md]$\"\n    match = re.compile(time_marker_regex).search(text)\n    if not match:\n        raise argparse.ArgumentTypeError(f\"Invalid relative time marker used: {text}\")\n    else:\n        return text\n\n\ndef text_validator(*, name, max_length, min_length=0, invalid_ctrl_chars=\"\\0\", invalid_chars=\"\", no_blanks=False):\n    def validator(text):\n        assert isinstance(text, str)\n        if len(text) < min_length:\n            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [length < {min_length}]')\n        if len(text) > max_length:\n            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [length > {max_length}]')\n        if invalid_ctrl_chars and re.search(f\"[{re.escape(invalid_ctrl_chars)}]\", text):\n            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [invalid control chars detected]')\n        if invalid_chars and re.search(f\"[{re.escape(invalid_chars)}]\", text):\n            raise argparse.ArgumentTypeError(\n                f'Invalid {name}: \"{text}\" [invalid chars detected matching \"{invalid_chars}\"]'\n            )\n        if no_blanks and (text.startswith(\" \") or text.endswith(\" \")):\n            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [leading or trailing blanks detected]')\n        try:\n            text.encode(\"utf-8\", errors=\"strict\")\n        except UnicodeEncodeError:\n            # looks like text contains surrogate-escapes\n            raise argparse.ArgumentTypeError(f'Invalid {name}: \"{text}\" [contains non-unicode characters]')\n        return text\n\n    return validator\n\n\ncomment_validator = text_validator(name=\"comment\", max_length=10000)\n\n\ndef archivename_validator(text):\n    # we make sure that the archive name can be used as directory name (for borg mount)\n    MAX_PATH = 260  # Windows default. Since Win10, there is a registry setting LongPathsEnabled to get more.\n    MAX_DIRNAME = MAX_PATH - len(\"12345678.123\")\n    SAFETY_MARGIN = 48  # borgfs path: mountpoint / archivename / dir / dir / ... / file\n    MAX_ARCHIVENAME = MAX_DIRNAME - SAFETY_MARGIN\n    invalid_ctrl_chars = \"\".join(chr(i) for i in range(32))\n    # note: \":\" is also an invalid path char on windows, but we can not blacklist it,\n    # because e.g. our {now} placeholder creates ISO-8601 like output like 2022-12-10T20:47:42 .\n    invalid_chars = r\"/\" + r\"\\\"<|>?*\"  # posix + windows\n    validate_text = text_validator(\n        name=\"archive name\",\n        min_length=1,\n        max_length=MAX_ARCHIVENAME,\n        invalid_ctrl_chars=invalid_ctrl_chars,\n        invalid_chars=invalid_chars,\n        no_blanks=True,\n    )\n    return validate_text(text)\n\n\nclass BaseFormatter(metaclass=abc.ABCMeta):\n    format: str\n    static_data: Dict[str, Any]\n    FIXED_KEYS: ClassVar[Dict[str, str]] = {\n        # Formatting aids\n        \"LF\": \"\\n\",\n        \"SPACE\": \" \",\n        \"TAB\": \"\\t\",\n        \"CR\": \"\\r\",\n        \"NUL\": \"\\0\",\n        \"NEWLINE\": \"\\n\",\n        \"NL\": \"\\n\",  # \\n is automatically converted to os.linesep on write\n    }\n    KEY_DESCRIPTIONS: ClassVar[Dict[str, str]] = {\n        \"NEWLINE\": \"OS dependent line separator\",\n        \"NL\": \"alias of NEWLINE\",\n        \"NUL\": \"NUL character for creating print0 / xargs -0 like output\",\n        \"SPACE\": \"space character\",\n        \"TAB\": \"tab character\",\n        \"CR\": \"carriage return character\",\n        \"LF\": \"line feed character\",\n    }\n    KEY_GROUPS: ClassVar[Tuple[Tuple[str, ...], ...]] = ((\"NEWLINE\", \"NL\", \"NUL\", \"SPACE\", \"TAB\", \"CR\", \"LF\"),)\n\n    def __init__(self, format: str, static: Dict[str, Any]) -> None:\n        self.format = partial_format(format, static)\n        self.static_data = static\n\n    @abc.abstractmethod\n    def get_item_data(self, item, jsonline=False) -> dict:\n        raise NotImplementedError\n\n    def format_item(self, item, jsonline=False, sort=False):\n        data = self.get_item_data(item, jsonline)\n        return (\n            f\"{json.dumps(data, cls=BorgJsonEncoder, sort_keys=sort)}\\n\" if jsonline else self.format.format_map(data)\n        )\n\n    @classmethod\n    def keys_help(cls):\n        help = []\n        keys: Set[str] = set()\n        keys.update(cls.KEY_DESCRIPTIONS.keys())\n        keys.update(key for group in cls.KEY_GROUPS for key in group)\n\n        for group in cls.KEY_GROUPS:\n            for key in group:\n                keys.remove(key)\n                text = \"- \" + key\n                if key in cls.KEY_DESCRIPTIONS:\n                    text += \": \" + cls.KEY_DESCRIPTIONS[key]\n                help.append(text)\n            help.append(\"\")\n        assert not keys, str(keys)\n        return \"\\n\".join(help)\n\n\nclass ArchiveFormatter(BaseFormatter):\n    KEY_DESCRIPTIONS = {\n        \"archive\": \"archive name\",\n        \"name\": 'alias of \"archive\"',\n        \"comment\": \"archive comment\",\n        # *start* is the key used by borg-info for this timestamp, this makes the formats more compatible\n        \"start\": \"time (start) of creation of the archive\",\n        \"time\": 'alias of \"start\"',\n        \"end\": \"time (end) of creation of the archive\",\n        \"command_line\": \"command line which was used to create the archive\",\n        \"id\": \"internal ID of the archive\",\n        \"hostname\": \"hostname of host on which this archive was created\",\n        \"username\": \"username of user who created this archive\",\n        \"tam\": \"TAM authentication state of this archive\",\n        \"size\": \"size of this archive (data plus metadata, not considering compression and deduplication)\",\n        \"nfiles\": \"count of files in this archive\",\n    }\n    KEY_GROUPS = (\n        (\"archive\", \"name\", \"comment\", \"id\", \"tam\"),\n        (\"start\", \"time\", \"end\", \"command_line\"),\n        (\"hostname\", \"username\"),\n        (\"size\", \"nfiles\"),\n    )\n\n    def __init__(self, format, repository, manifest, key, *, iec=False):\n        static_data = {}  # here could be stuff on repo level, above archive level\n        static_data.update(self.FIXED_KEYS)\n        super().__init__(format, static_data)\n        self.repository = repository\n        self.manifest = manifest\n        self.key = key\n        self.name = None\n        self.id = None\n        self._archive = None\n        self.iec = iec\n        self.format_keys = {f[1] for f in Formatter().parse(format)}\n        self.call_keys = {\n            \"hostname\": partial(self.get_meta, \"hostname\", \"\"),\n            \"username\": partial(self.get_meta, \"username\", \"\"),\n            \"comment\": partial(self.get_meta, \"comment\", \"\"),\n            \"command_line\": partial(self.get_meta, \"command_line\", \"\"),\n            \"tam\": self.get_tam,\n            \"size\": partial(self.get_meta, \"size\", 0),\n            \"nfiles\": partial(self.get_meta, \"nfiles\", 0),\n            \"end\": self.get_ts_end,\n        }\n        self.used_call_keys = set(self.call_keys) & self.format_keys\n\n    def get_item_data(self, archive_info, jsonline=False):\n        self.name = archive_info.name\n        self.id = archive_info.id\n        item_data = {}\n        item_data.update({} if jsonline else self.static_data)\n        item_data.update(\n            {\n                \"name\": archive_info.name,\n                \"archive\": archive_info.name,\n                \"id\": bin_to_hex(archive_info.id),\n                \"time\": self.format_time(archive_info.ts),\n                \"start\": self.format_time(archive_info.ts),\n            }\n        )\n        for key in self.used_call_keys:\n            item_data[key] = self.call_keys[key]()\n\n        # Note: name and comment are validated, should never contain surrogate escapes.\n        # But unsure whether hostname, username, command_line could contain surrogate escapes, play safe:\n        for key in \"hostname\", \"username\", \"command_line\":\n            if key in item_data:\n                item_data.update(text_to_json(key, item_data[key]))\n        return item_data\n\n    @property\n    def archive(self):\n        \"\"\"lazy load / update loaded archive\"\"\"\n        if self._archive is None or self._archive.id != self.id:\n            from ..archive import Archive\n\n            self._archive = Archive(self.manifest, self.name, iec=self.iec)\n        return self._archive\n\n    def get_meta(self, key, default=None):\n        return self.archive.metadata.get(key, default)\n\n    def get_ts_end(self):\n        return self.format_time(self.archive.ts_end)\n\n    def get_tam(self):\n        return \"verified\" if self.archive.tam_verified else \"none\"\n\n    def format_time(self, ts):\n        return OutputTimestamp(ts)\n\n\nclass ItemFormatter(BaseFormatter):\n    # we provide the hash algos from python stdlib (except shake_*) and additionally xxh64.\n    # shake_* is not provided because it uses an incompatible .digest() method to support variable length.\n    hash_algorithms = set(hashlib.algorithms_guaranteed).union({\"xxh64\"}).difference({\"shake_128\", \"shake_256\"})\n    KEY_DESCRIPTIONS = {\n        \"type\": \"file type (file, dir, symlink, ...)\",\n        \"mode\": \"file mode (as in stat)\",\n        \"uid\": \"user id of file owner\",\n        \"gid\": \"group id of file owner\",\n        \"user\": \"user name of file owner\",\n        \"group\": \"group name of file owner\",\n        \"path\": \"file path\",\n        \"target\": \"link target for symlinks\",\n        \"hlid\": \"hard link identity (same if hardlinking same fs object)\",\n        \"flags\": \"file flags\",\n        \"extra\": 'prepends {target} with \" -> \" for soft links and \" link to \" for hard links',\n        \"size\": \"file size\",\n        \"dsize\": \"deduplicated size\",\n        \"num_chunks\": \"number of chunks in this file\",\n        \"unique_chunks\": \"number of unique chunks in this file\",\n        \"mtime\": \"file modification time\",\n        \"ctime\": \"file change time\",\n        \"atime\": \"file access time\",\n        \"isomtime\": \"file modification time (ISO 8601 format)\",\n        \"isoctime\": \"file change time (ISO 8601 format)\",\n        \"isoatime\": \"file access time (ISO 8601 format)\",\n        \"xxh64\": \"XXH64 checksum of this file (note: this is NOT a cryptographic hash!)\",\n        \"health\": 'either \"healthy\" (file ok) or \"broken\" (if file has all-zero replacement chunks)',\n        \"archiveid\": \"internal ID of the archive\",\n        \"archivename\": \"name of the archive\",\n    }\n    KEY_GROUPS = (\n        (\"type\", \"mode\", \"uid\", \"gid\", \"user\", \"group\", \"path\", \"target\", \"hlid\", \"flags\"),\n        (\"size\", \"dsize\", \"num_chunks\", \"unique_chunks\"),\n        (\"mtime\", \"ctime\", \"atime\", \"isomtime\", \"isoctime\", \"isoatime\"),\n        tuple(sorted(hash_algorithms)),\n        (\"archiveid\", \"archivename\", \"extra\"),\n        (\"health\",),\n    )\n\n    KEYS_REQUIRING_CACHE = (\"dsize\", \"unique_chunks\")\n\n    @classmethod\n    def format_needs_cache(cls, format):\n        format_keys = {f[1] for f in Formatter().parse(format)}\n        return any(key in cls.KEYS_REQUIRING_CACHE for key in format_keys)\n\n    def __init__(self, archive, format):\n        from ..checksums import StreamingXXH64\n\n        static_data = {\"archivename\": archive.name, \"archiveid\": archive.fpr}\n        static_data.update(self.FIXED_KEYS)\n        super().__init__(format, static_data)\n        self.xxh64 = StreamingXXH64\n        self.archive = archive\n        self.format_keys = {f[1] for f in Formatter().parse(format)}\n        self.call_keys = {\n            \"size\": self.calculate_size,\n            \"dsize\": partial(self.sum_unique_chunks_metadata, lambda chunk: chunk.size),\n            \"num_chunks\": self.calculate_num_chunks,\n            \"unique_chunks\": partial(self.sum_unique_chunks_metadata, lambda chunk: 1),\n            \"isomtime\": partial(self.format_iso_time, \"mtime\"),\n            \"isoctime\": partial(self.format_iso_time, \"ctime\"),\n            \"isoatime\": partial(self.format_iso_time, \"atime\"),\n            \"mtime\": partial(self.format_time, \"mtime\"),\n            \"ctime\": partial(self.format_time, \"ctime\"),\n            \"atime\": partial(self.format_time, \"atime\"),\n        }\n        for hash_function in self.hash_algorithms:\n            self.call_keys[hash_function] = partial(self.hash_item, hash_function)\n        self.used_call_keys = set(self.call_keys) & self.format_keys\n\n    def get_item_data(self, item, jsonline=False):\n        item_data = {}\n        item_data.update({} if jsonline else self.static_data)\n\n        item_data.update(text_to_json(\"path\", item.path))\n        target = item.get(\"target\", \"\")\n        item_data.update(text_to_json(\"target\", target))\n        if not jsonline:\n            item_data[\"extra\"] = \"\" if not target else f\" -> {item_data['target']}\"\n\n        hlid = item.get(\"hlid\")\n        hlid = bin_to_hex(hlid) if hlid else \"\"\n        item_data[\"hlid\"] = hlid\n\n        mode = stat.filemode(item.mode)\n        item_type = mode[0]\n        item_data[\"type\"] = item_type\n        item_data[\"mode\"] = mode\n\n        item_data[\"uid\"] = item.get(\"uid\")  # int or None\n        item_data[\"gid\"] = item.get(\"gid\")  # int or None\n        item_data.update(text_to_json(\"user\", item.get(\"user\", str(item_data[\"uid\"]))))\n        item_data.update(text_to_json(\"group\", item.get(\"group\", str(item_data[\"gid\"]))))\n\n        if jsonline:\n            item_data[\"healthy\"] = \"chunks_healthy\" not in item\n        else:\n            item_data[\"health\"] = \"broken\" if \"chunks_healthy\" in item else \"healthy\"\n        item_data[\"flags\"] = item.get(\"bsdflags\")  # int if flags known, else (if flags unknown) None\n        for key in self.used_call_keys:\n            item_data[key] = self.call_keys[key](item)\n        return item_data\n\n    def sum_unique_chunks_metadata(self, metadata_func, item):\n        \"\"\"\n        sum unique chunks metadata, a unique chunk is a chunk which is referenced globally as often as it is in the\n        item\n\n        item: The item to sum its unique chunks' metadata\n        metadata_func: A function that takes a parameter of type ChunkIndexEntry and returns a number, used to return\n        the metadata needed from the chunk\n        \"\"\"\n        chunk_index = self.archive.cache.chunks\n        chunks = item.get(\"chunks\", [])\n        chunks_counter = Counter(c.id for c in chunks)\n        return sum(metadata_func(c) for c in chunks if chunk_index[c.id].refcount == chunks_counter[c.id])\n\n    def calculate_num_chunks(self, item):\n        return len(item.get(\"chunks\", []))\n\n    def calculate_size(self, item):\n        # note: does not support hardlink slaves, they will be size 0\n        return item.get_size()\n\n    def hash_item(self, hash_function, item):\n        if \"chunks\" not in item:\n            return \"\"\n        if hash_function == \"xxh64\":\n            hash = self.xxh64()\n        elif hash_function in self.hash_algorithms:\n            hash = hashlib.new(hash_function)\n        for data in self.archive.pipeline.fetch_many([c.id for c in item.chunks]):\n            hash.update(data)\n        return hash.hexdigest()\n\n    def format_time(self, key, item):\n        return OutputTimestamp(safe_timestamp(item.get(key) or item.mtime))\n\n    def format_iso_time(self, key, item):\n        return self.format_time(key, item).isoformat()\n\n\nclass DiffFormatter(BaseFormatter):\n    KEY_DESCRIPTIONS = {\n        \"path\": \"archived file path\",\n        \"change\": \"all available changes\",\n        \"content\": \"file content change\",\n        \"mode\": \"file mode change\",\n        \"type\": \"file type change\",\n        \"owner\": \"file owner (user/group) change\",\n        \"user\": \"file user change\",\n        \"group\": \"file group change\",\n        \"link\": \"file link change\",\n        \"directory\": \"file directory change\",\n        \"blkdev\": \"file block device change\",\n        \"chrdev\": \"file character device change\",\n        \"fifo\": \"file fifo change\",\n        \"mtime\": \"file modification time change\",\n        \"ctime\": \"file change time change\",\n        \"isomtime\": \"file modification time change (ISO 8601)\",\n        \"isoctime\": \"file creation time change (ISO 8601)\",\n    }\n    KEY_GROUPS = (\n        (\"path\", \"change\"),\n        (\"content\", \"mode\", \"type\", \"owner\", \"group\", \"user\"),\n        (\"link\", \"directory\", \"blkdev\", \"chrdev\", \"fifo\"),\n        (\"mtime\", \"ctime\", \"isomtime\", \"isoctime\"),\n    )\n    METADATA = (\"mode\", \"type\", \"owner\", \"group\", \"user\", \"mtime\", \"ctime\")\n\n    def __init__(self, format, content_only=False):\n        static_data = {}\n        static_data.update(self.FIXED_KEYS)\n        super().__init__(format or \"{content}{link}{directory}{blkdev}{chrdev}{fifo} {path}{NL}\", static_data)\n        self.content_only = content_only\n        self.format_keys = {f[1] for f in Formatter().parse(format)}\n        self.call_keys = {\n            \"content\": self.format_content,\n            \"mode\": self.format_mode,\n            \"type\": partial(self.format_mode, filetype=True),\n            \"owner\": partial(self.format_owner),\n            \"group\": partial(self.format_owner, spec=\"group\"),\n            \"user\": partial(self.format_owner, spec=\"user\"),\n            \"link\": partial(self.format_other, \"link\"),\n            \"directory\": partial(self.format_other, \"directory\"),\n            \"blkdev\": partial(self.format_other, \"blkdev\"),\n            \"chrdev\": partial(self.format_other, \"chrdev\"),\n            \"fifo\": partial(self.format_other, \"fifo\"),\n            \"mtime\": partial(self.format_time, \"mtime\"),\n            \"ctime\": partial(self.format_time, \"ctime\"),\n            \"isomtime\": partial(self.format_iso_time, \"mtime\"),\n            \"isoctime\": partial(self.format_iso_time, \"ctime\"),\n        }\n        self.used_call_keys = set(self.call_keys) & self.format_keys\n        if self.content_only:\n            self.used_call_keys -= set(self.METADATA)\n\n    def get_item_data(self, item: \"ItemDiff\", jsonline=False) -> dict:\n        diff_data = {}\n        for key in self.used_call_keys:\n            diff_data[key] = self.call_keys[key](item)\n\n        change = []\n        for key in self.call_keys:\n            if key in (\"isomtime\", \"isoctime\"):\n                continue\n            if self.content_only and key in self.METADATA:\n                continue\n            change.append(self.call_keys[key](item))\n        diff_data[\"change\"] = \" \".join([v for v in change if v])\n        diff_data[\"path\"] = item.path\n        diff_data.update({} if jsonline else self.static_data)\n        return diff_data\n\n    def format_other(self, key, diff: \"ItemDiff\"):\n        change = diff.changes().get(key)\n        return f\"{change.diff_type}\".ljust(27) if change else \"\"  # 27 is the length of the content change\n\n    def format_mode(self, diff: \"ItemDiff\", filetype=False):\n        change = diff.type() if filetype else diff.mode()\n        return f\"[{change.diff_data['item1']} -> {change.diff_data['item2']}]\" if change else \"\"\n\n    def format_owner(self, diff: \"ItemDiff\", spec: Literal[\"owner\", \"user\", \"group\"] = \"owner\"):\n        if spec == \"user\":\n            change = diff.user()\n            return f\"[{change.diff_data['item1']} -> {change.diff_data['item2']}]\" if change else \"\"\n        if spec == \"group\":\n            change = diff.group()\n            return f\"[{change.diff_data['item1']} -> {change.diff_data['item2']}]\" if change else \"\"\n        if spec != \"owner\":\n            raise ValueError(f\"Invalid owner spec: {spec}\")\n        change = diff.owner()\n        if change:\n            return \"[{}:{} -> {}:{}]\".format(\n                change.diff_data[\"item1\"][0],\n                change.diff_data[\"item1\"][1],\n                change.diff_data[\"item2\"][0],\n                change.diff_data[\"item2\"][1],\n            )\n        return \"\"\n\n    def format_content(self, diff: \"ItemDiff\"):\n        change = diff.content()\n        if change:\n            if change.diff_type == \"added\":\n                return \"{}: {:>20}\".format(change.diff_type, format_file_size(change.diff_data[\"added\"]))\n            if change.diff_type == \"removed\":\n                return \"{}: {:>18}\".format(change.diff_type, format_file_size(change.diff_data[\"removed\"]))\n            if \"added\" not in change.diff_data and \"removed\" not in change.diff_data:\n                return \"modified:  (can't get size)\"\n            return \"{}: {:>8} {:>8}\".format(\n                change.diff_type,\n                format_file_size(change.diff_data[\"added\"], precision=1, sign=True),\n                format_file_size(-change.diff_data[\"removed\"], precision=1, sign=True),\n            )\n        return \"\"\n\n    def format_time(self, key, diff: \"ItemDiff\"):\n        change = diff.changes().get(key)\n        return f\"[{key}: {change.diff_data['item1']} -> {change.diff_data['item2']}]\" if change else \"\"\n\n    def format_iso_time(self, key, diff: \"ItemDiff\"):\n        change = diff.changes().get(key)\n        return (\n            f\"[{key}: {change.diff_data['item1'].isoformat()} -> {change.diff_data['item2'].isoformat()}]\"\n            if change\n            else \"\"\n        )\n\n\ndef file_status(mode):\n    if stat.S_ISREG(mode):\n        return \"A\"\n    elif stat.S_ISDIR(mode):\n        return \"d\"\n    elif stat.S_ISBLK(mode):\n        return \"b\"\n    elif stat.S_ISCHR(mode):\n        return \"c\"\n    elif stat.S_ISLNK(mode):\n        return \"s\"\n    elif stat.S_ISFIFO(mode):\n        return \"f\"\n    return \"?\"\n\n\ndef clean_lines(lines, lstrip=None, rstrip=None, remove_empty=True, remove_comments=True):\n    \"\"\"\n    clean lines (usually read from a config file):\n\n    1. strip whitespace (left and right), 2. remove empty lines, 3. remove comments.\n\n    note: only \"pure comment lines\" are supported, no support for \"trailing comments\".\n\n    :param lines: input line iterator (e.g. list or open text file) that gives unclean input lines\n    :param lstrip: lstrip call arguments or False, if lstripping is not desired\n    :param rstrip: rstrip call arguments or False, if rstripping is not desired\n    :param remove_comments: remove comment lines (lines starting with \"#\")\n    :param remove_empty: remove empty lines\n    :return: yields processed lines\n    \"\"\"\n    for line in lines:\n        if lstrip is not False:\n            line = line.lstrip(lstrip)\n        if rstrip is not False:\n            line = line.rstrip(rstrip)\n        if remove_empty and not line:\n            continue\n        if remove_comments and line.startswith(\"#\"):\n            continue\n        yield line\n\n\ndef swidth_slice(string, max_width):\n    \"\"\"\n    Return a slice of *max_width* cells from *string*.\n\n    Negative *max_width* means from the end of string.\n\n    *max_width* is in units of character cells (or \"columns\").\n    Latin characters are usually one cell wide, many CJK characters are two cells wide.\n    \"\"\"\n    from ..platform import swidth\n\n    reverse = max_width < 0\n    max_width = abs(max_width)\n    if reverse:\n        string = reversed(string)\n    current_swidth = 0\n    result = []\n    for character in string:\n        current_swidth += swidth(character)\n        if current_swidth > max_width:\n            break\n        result.append(character)\n    if reverse:\n        result.reverse()\n    return \"\".join(result)\n\n\ndef ellipsis_truncate(msg, space):\n    \"\"\"\n    shorten a long string by adding ellipsis between it and return it, example:\n    this_is_a_very_long_string -------> this_is..._string\n    \"\"\"\n    from ..platform import swidth\n\n    ellipsis_width = swidth(\"...\")\n    msg_width = swidth(msg)\n    if space < 8:\n        # if there is very little space, just show ...\n        return \"...\" + \" \" * (space - ellipsis_width)\n    if space < ellipsis_width + msg_width:\n        return f\"{swidth_slice(msg, space // 2 - ellipsis_width)}...{swidth_slice(msg, -space // 2)}\"\n    return msg + \" \" * (space - msg_width)\n\n\nclass BorgJsonEncoder(json.JSONEncoder):\n    def default(self, o):\n        from ..repository import Repository\n        from ..remote import RemoteRepository\n        from ..archive import Archive\n        from ..cache import LocalCache, AdHocCache\n\n        if isinstance(o, Repository) or isinstance(o, RemoteRepository):\n            return {\"id\": bin_to_hex(o.id), \"location\": o._location.canonical_path()}\n        if isinstance(o, Archive):\n            return o.info()\n        if isinstance(o, LocalCache):\n            return {\"path\": o.path, \"stats\": o.stats()}\n        if isinstance(o, AdHocCache):\n            return {\"stats\": o.stats()}\n        if callable(getattr(o, \"to_json\", None)):\n            return o.to_json()\n        return super().default(o)\n\n\ndef basic_json_data(manifest, *, cache=None, extra=None):\n    key = manifest.key\n    data = extra or {}\n    data.update({\"repository\": BorgJsonEncoder().default(manifest.repository), \"encryption\": {\"mode\": key.ARG_NAME}})\n    data[\"repository\"][\"last_modified\"] = OutputTimestamp(manifest.last_timestamp)\n    if key.NAME.startswith(\"key file\"):\n        data[\"encryption\"][\"keyfile\"] = key.find_key()\n    if cache:\n        data[\"cache\"] = cache\n    return data\n\n\ndef json_dump(obj):\n    \"\"\"Dump using BorgJSONEncoder.\"\"\"\n    return json.dumps(obj, sort_keys=True, indent=4, cls=BorgJsonEncoder)\n\n\ndef json_print(obj):\n    print(json_dump(obj))\n\n\ndef prepare_dump_dict(d):\n    def decode_bytes(value):\n        # this should somehow be reversible later, but usual strings should\n        # look nice and chunk ids should mostly show in hex. Use a special\n        # inband signaling character (ASCII DEL) to distinguish between\n        # decoded and hex mode.\n        if not value.startswith(b\"\\x7f\"):\n            try:\n                value = value.decode()\n                return value\n            except UnicodeDecodeError:\n                pass\n        return \"\\u007f\" + bin_to_hex(value)\n\n    def decode_tuple(t):\n        res = []\n        for value in t:\n            if isinstance(value, dict):\n                value = decode(value)\n            elif isinstance(value, tuple) or isinstance(value, list):\n                value = decode_tuple(value)\n            elif isinstance(value, bytes):\n                value = decode_bytes(value)\n            res.append(value)\n        return res\n\n    def decode(d):\n        res = OrderedDict()\n        for key, value in d.items():\n            if isinstance(value, dict):\n                value = decode(value)\n            elif isinstance(value, (tuple, list)):\n                value = decode_tuple(value)\n            elif isinstance(value, bytes):\n                value = decode_bytes(value)\n            elif isinstance(value, Timestamp):\n                value = value.to_unix_nano()\n            if isinstance(key, bytes):\n                key = key.decode()\n            res[key] = value\n        return res\n\n    return decode(d)\n\n\nclass Highlander(argparse.Action):\n    \"\"\"make sure some option is only given once\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.__called = False\n        super().__init__(*args, **kwargs)\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        if self.__called:\n            raise argparse.ArgumentError(self, \"There can be only one.\")\n        self.__called = True\n        setattr(namespace, self.dest, values)\n\n\nclass MakePathSafeAction(Highlander):\n    def __call__(self, parser, namespace, path, option_string=None):\n        try:\n            sanitized_path = make_path_safe(path)\n        except ValueError as e:\n            raise argparse.ArgumentError(self, e)\n        if sanitized_path == \".\":\n            raise argparse.ArgumentError(self, f\"{path!r} is not a valid file name\")\n        setattr(namespace, self.dest, sanitized_path)\n", "import shutil\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom ...archive import ChunkBuffer\nfrom ...constants import *  # NOQA\nfrom ...helpers import bin_to_hex\nfrom ...manifest import Manifest\nfrom ...repository import Repository\nfrom . import cmd, src_file, create_src_archive, open_archive, generate_archiver_tests, RK_ENCRYPTION\n\npytest_generate_tests = lambda metafunc: generate_archiver_tests(metafunc, kinds=\"local,remote,binary\")  # NOQA\n\n\ndef check_cmd_setup(archiver):\n    with patch.object(ChunkBuffer, \"BUFFER_SIZE\", 10):\n        cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n        create_src_archive(archiver, \"archive1\")\n        create_src_archive(archiver, \"archive2\")\n\n\ndef test_check_usage(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n\n    output = cmd(archiver, \"check\", \"-v\", \"--progress\", exit_code=0)\n    assert \"Starting repository check\" in output\n    assert \"Starting archive consistency check\" in output\n    assert \"Checking segments\" in output\n\n    output = cmd(archiver, \"check\", \"-v\", \"--repository-only\", exit_code=0)\n    assert \"Starting repository check\" in output\n    assert \"Starting archive consistency check\" not in output\n    assert \"Checking segments\" not in output\n\n    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", exit_code=0)\n    assert \"Starting repository check\" not in output\n    assert \"Starting archive consistency check\" in output\n\n    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--match-archives=archive2\", exit_code=0)\n    assert \"archive1\" not in output\n\n    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--first=1\", exit_code=0)\n    assert \"archive1\" in output\n    assert \"archive2\" not in output\n\n    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--last=1\", exit_code=0)\n    assert \"archive1\" not in output\n    assert \"archive2\" in output\n\n\ndef test_date_matching(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n\n    shutil.rmtree(archiver.repository_path)\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    earliest_ts = \"2022-11-20T23:59:59\"\n    ts_in_between = \"2022-12-18T23:59:59\"\n    create_src_archive(archiver, \"archive1\", ts=earliest_ts)\n    create_src_archive(archiver, \"archive2\", ts=ts_in_between)\n    create_src_archive(archiver, \"archive3\")\n    cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--oldest=23e\", exit_code=2)\n\n    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--oldest=1m\", exit_code=0)\n    assert \"archive1\" in output\n    assert \"archive2\" in output\n    assert \"archive3\" not in output\n\n    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--newest=1m\", exit_code=0)\n    assert \"archive3\" in output\n    assert \"archive2\" not in output\n    assert \"archive1\" not in output\n\n    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--newer=1d\", exit_code=0)\n    assert \"archive3\" in output\n    assert \"archive1\" not in output\n    assert \"archive2\" not in output\n\n    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--older=1d\", exit_code=0)\n    assert \"archive1\" in output\n    assert \"archive2\" in output\n    assert \"archive3\" not in output\n\n    # check for output when timespan older than the earliest archive is given. Issue #1711\n    output = cmd(archiver, \"check\", \"-v\", \"--archives-only\", \"--older=9999m\", exit_code=0)\n    for archive in (\"archive1\", \"archive2\", \"archive3\"):\n        assert archive not in output\n\n\ndef test_missing_file_chunk(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n\n    archive, repository = open_archive(archiver.repository_path, \"archive1\")\n\n    with repository:\n        for item in archive.iter_items():\n            if item.path.endswith(src_file):\n                valid_chunks = item.chunks\n                killed_chunk = valid_chunks[-1]\n                repository.delete(killed_chunk.id)\n                break\n        else:\n            pytest.fail(\"should not happen\")  # convert 'fail'\n        repository.commit(compact=False)\n\n    cmd(archiver, \"check\", exit_code=1)\n    output = cmd(archiver, \"check\", \"--repair\", exit_code=0)\n    assert \"New missing file chunk detected\" in output\n\n    cmd(archiver, \"check\", exit_code=0)\n    output = cmd(archiver, \"list\", \"archive1\", \"--format={health}#{path}{NL}\", exit_code=0)\n    assert \"broken#\" in output\n\n    # check that the file in the old archives has now a different chunk list without the killed chunk\n    for archive_name in (\"archive1\", \"archive2\"):\n        archive, repository = open_archive(archiver.repository_path, archive_name)\n        with repository:\n            for item in archive.iter_items():\n                if item.path.endswith(src_file):\n                    assert valid_chunks != item.chunks\n                    assert killed_chunk not in item.chunks\n                    break\n            else:\n                pytest.fail(\"should not happen\")  # convert 'fail'\n\n    # do a fresh backup (that will include the killed chunk)\n    with patch.object(ChunkBuffer, \"BUFFER_SIZE\", 10):\n        create_src_archive(archiver, \"archive3\")\n\n    # check should be able to heal the file now:\n    output = cmd(archiver, \"check\", \"-v\", \"--repair\", exit_code=0)\n    assert \"Healed previously missing file chunk\" in output\n    assert f\"{src_file}: Completely healed previously damaged file!\" in output\n\n    # check that the file in the old archives has the correct chunks again\n    for archive_name in (\"archive1\", \"archive2\"):\n        archive, repository = open_archive(archiver.repository_path, archive_name)\n        with repository:\n            for item in archive.iter_items():\n                if item.path.endswith(src_file):\n                    assert valid_chunks == item.chunks\n                    break\n            else:\n                pytest.fail(\"should not happen\")\n\n    # list is also all-healthy again\n    output = cmd(archiver, \"list\", \"archive1\", \"--format={health}#{path}{NL}\", exit_code=0)\n    assert \"broken#\" not in output\n\n\ndef test_missing_archive_item_chunk(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n    archive, repository = open_archive(archiver.repository_path, \"archive1\")\n    with repository:\n        repository.delete(archive.metadata.items[0])\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", exit_code=1)\n    cmd(archiver, \"check\", \"--repair\", exit_code=0)\n    cmd(archiver, \"check\", exit_code=0)\n\n\ndef test_missing_archive_metadata(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n    archive, repository = open_archive(archiver.repository_path, \"archive1\")\n    with repository:\n        repository.delete(archive.id)\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", exit_code=1)\n    cmd(archiver, \"check\", \"--repair\", exit_code=0)\n    cmd(archiver, \"check\", exit_code=0)\n\n\ndef test_missing_manifest(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n    archive, repository = open_archive(archiver.repository_path, \"archive1\")\n    with repository:\n        repository.delete(Manifest.MANIFEST_ID)\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", exit_code=1)\n    output = cmd(archiver, \"check\", \"-v\", \"--repair\", exit_code=0)\n    assert \"archive1\" in output\n    assert \"archive2\" in output\n    cmd(archiver, \"check\", exit_code=0)\n\n\ndef test_corrupted_manifest(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n    archive, repository = open_archive(archiver.repository_path, \"archive1\")\n    with repository:\n        manifest = repository.get(Manifest.MANIFEST_ID)\n        corrupted_manifest = manifest + b\"corrupted!\"\n        repository.put(Manifest.MANIFEST_ID, corrupted_manifest)\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", exit_code=1)\n    output = cmd(archiver, \"check\", \"-v\", \"--repair\", exit_code=0)\n    assert \"archive1\" in output\n    assert \"archive2\" in output\n    cmd(archiver, \"check\", exit_code=0)\n\n\ndef test_manifest_rebuild_corrupted_chunk(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n    archive, repository = open_archive(archiver.repository_path, \"archive1\")\n    with repository:\n        manifest = repository.get(Manifest.MANIFEST_ID)\n        corrupted_manifest = manifest + b\"corrupted!\"\n        repository.put(Manifest.MANIFEST_ID, corrupted_manifest)\n        chunk = repository.get(archive.id)\n        corrupted_chunk = chunk + b\"corrupted!\"\n        repository.put(archive.id, corrupted_chunk)\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", exit_code=1)\n    output = cmd(archiver, \"check\", \"-v\", \"--repair\", exit_code=0)\n    assert \"archive2\" in output\n    cmd(archiver, \"check\", exit_code=0)\n\n\ndef test_manifest_rebuild_duplicate_archive(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n    archive, repository = open_archive(archiver.repository_path, \"archive1\")\n    repo_objs = archive.repo_objs\n    with repository:\n        manifest = repository.get(Manifest.MANIFEST_ID)\n        corrupted_manifest = manifest + b\"corrupted!\"\n        repository.put(Manifest.MANIFEST_ID, corrupted_manifest)\n        archive_dict = {\n            \"command_line\": \"\",\n            \"item_ptrs\": [],\n            \"hostname\": \"foo\",\n            \"username\": \"bar\",\n            \"name\": \"archive1\",\n            \"time\": \"2016-12-15T18:49:51.849711\",\n            \"version\": 2,\n        }\n        archive = repo_objs.key.pack_and_authenticate_metadata(archive_dict, context=b\"archive\")\n        archive_id = repo_objs.id_hash(archive)\n        repository.put(archive_id, repo_objs.format(archive_id, {}, archive))\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", exit_code=1)\n    cmd(archiver, \"check\", \"--repair\", exit_code=0)\n    output = cmd(archiver, \"rlist\")\n    assert \"archive1\" in output\n    assert \"archive1.1\" in output\n    assert \"archive2\" in output\n\n\ndef test_extra_chunks(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    if archiver.get_kind() == \"remote\":\n        pytest.skip(\"only works locally\")\n    check_cmd_setup(archiver)\n    cmd(archiver, \"check\", exit_code=0)\n    with Repository(archiver.repository_location, exclusive=True) as repository:\n        repository.put(b\"01234567890123456789012345678901\", b\"xxxx\")\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", exit_code=1)\n    cmd(archiver, \"check\", exit_code=1)\n    cmd(archiver, \"check\", \"--repair\", exit_code=0)\n    cmd(archiver, \"check\", exit_code=0)\n    cmd(archiver, \"extract\", \"archive1\", \"--dry-run\", exit_code=0)\n\n\n@pytest.mark.parametrize(\"init_args\", [[\"--encryption=repokey-aes-ocb\"], [\"--encryption\", \"none\"]])\ndef test_verify_data(archivers, request, init_args):\n    archiver = request.getfixturevalue(archivers)\n    check_cmd_setup(archiver)\n    shutil.rmtree(archiver.repository_path)\n    cmd(archiver, \"rcreate\", *init_args)\n    create_src_archive(archiver, \"archive1\")\n    archive, repository = open_archive(archiver.repository_path, \"archive1\")\n    with repository:\n        for item in archive.iter_items():\n            if item.path.endswith(src_file):\n                chunk = item.chunks[-1]\n                data = repository.get(chunk.id)\n                data = data[0:100] + b\"x\" + data[101:]\n                repository.put(chunk.id, data)\n                break\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", exit_code=0)\n    output = cmd(archiver, \"check\", \"--verify-data\", exit_code=1)\n    assert bin_to_hex(chunk.id) + \", integrity error\" in output\n\n    # repair (heal is tested in another test)\n    output = cmd(archiver, \"check\", \"--repair\", \"--verify-data\", exit_code=0)\n    assert bin_to_hex(chunk.id) + \", integrity error\" in output\n    assert f\"{src_file}: New missing file chunk detected\" in output\n\n\ndef test_empty_repository(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    if archiver.get_kind() == \"remote\":\n        pytest.skip(\"only works locally\")\n    check_cmd_setup(archiver)\n    with Repository(archiver.repository_location, exclusive=True) as repository:\n        for id_ in repository.list():\n            repository.delete(id_)\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", exit_code=1)\n", "import os\nimport shutil\nfrom datetime import datetime, timezone, timedelta\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom ...cache import Cache, LocalCache\nfrom ...constants import *  # NOQA\nfrom ...crypto.key import TAMRequiredError\nfrom ...helpers import Location, get_security_dir, bin_to_hex, archive_ts_now\nfrom ...helpers import EXIT_ERROR\nfrom ...helpers import msgpack\nfrom ...manifest import Manifest, MandatoryFeatureUnsupported\nfrom ...remote import RemoteRepository, PathNotAllowed\nfrom ...repository import Repository\nfrom .. import llfuse\nfrom .. import changedir\nfrom . import cmd, _extract_repository_id, open_repository, check_cache, create_test_files, create_src_archive\nfrom . import _set_repository_id, create_regular_file, assert_creates_file, generate_archiver_tests, RK_ENCRYPTION\n\npytest_generate_tests = lambda metafunc: generate_archiver_tests(metafunc, kinds=\"local,remote\")  # NOQA\n\n\ndef get_security_directory(repo_path):\n    repository_id = bin_to_hex(_extract_repository_id(repo_path))\n    return get_security_dir(repository_id)\n\n\ndef add_unknown_feature(repo_path, operation):\n    with Repository(repo_path, exclusive=True) as repository:\n        manifest = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)\n        manifest.config[\"feature_flags\"] = {operation.value: {\"mandatory\": [\"unknown-feature\"]}}\n        manifest.write()\n        repository.commit(compact=False)\n\n\ndef cmd_raises_unknown_feature(archiver, args):\n    if archiver.FORK_DEFAULT:\n        cmd(archiver, *args, exit_code=EXIT_ERROR)\n    else:\n        with pytest.raises(MandatoryFeatureUnsupported) as excinfo:\n            cmd(archiver, *args)\n        assert excinfo.value.args == ([\"unknown-feature\"],)\n\n\ndef test_repository_swap_detection(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    create_test_files(archiver.input_path)\n    os.environ[\"BORG_PASSPHRASE\"] = \"passphrase\"\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    repository_id = _extract_repository_id(archiver.repository_path)\n    cmd(archiver, \"create\", \"test\", \"input\")\n    shutil.rmtree(archiver.repository_path)\n    cmd(archiver, \"rcreate\", \"--encryption=none\")\n    _set_repository_id(archiver.repository_path, repository_id)\n    assert repository_id == _extract_repository_id(archiver.repository_path)\n    if archiver.FORK_DEFAULT:\n        cmd(archiver, \"create\", \"test.2\", \"input\", exit_code=EXIT_ERROR)\n    else:\n        with pytest.raises(Cache.EncryptionMethodMismatch):\n            cmd(archiver, \"create\", \"test.2\", \"input\")\n\n\ndef test_repository_swap_detection2(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    create_test_files(archiver.input_path)\n    original_location = archiver.repository_location\n    archiver.repository_location = original_location + \"_unencrypted\"\n    cmd(archiver, \"rcreate\", \"--encryption=none\")\n    os.environ[\"BORG_PASSPHRASE\"] = \"passphrase\"\n    archiver.repository_location = original_location + \"_encrypted\"\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    cmd(archiver, \"create\", \"test\", \"input\")\n    shutil.rmtree(archiver.repository_path + \"_encrypted\")\n    os.replace(archiver.repository_path + \"_unencrypted\", archiver.repository_path + \"_encrypted\")\n    if archiver.FORK_DEFAULT:\n        cmd(archiver, \"create\", \"test.2\", \"input\", exit_code=EXIT_ERROR)\n    else:\n        with pytest.raises(Cache.RepositoryAccessAborted):\n            cmd(archiver, \"create\", \"test.2\", \"input\")\n\n\ndef test_repository_swap_detection_no_cache(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    create_test_files(archiver.input_path)\n    os.environ[\"BORG_PASSPHRASE\"] = \"passphrase\"\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    repository_id = _extract_repository_id(archiver.repository_path)\n    cmd(archiver, \"create\", \"test\", \"input\")\n    shutil.rmtree(archiver.repository_path)\n    cmd(archiver, \"rcreate\", \"--encryption=none\")\n    _set_repository_id(archiver.repository_path, repository_id)\n    assert repository_id == _extract_repository_id(archiver.repository_path)\n    cmd(archiver, \"rdelete\", \"--cache-only\")\n    if archiver.FORK_DEFAULT:\n        cmd(archiver, \"create\", \"test.2\", \"input\", exit_code=EXIT_ERROR)\n    else:\n        with pytest.raises(Cache.EncryptionMethodMismatch):\n            cmd(archiver, \"create\", \"test.2\", \"input\")\n\n\ndef test_repository_swap_detection2_no_cache(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    original_location = archiver.repository_location\n    create_test_files(archiver.input_path)\n    archiver.repository_location = original_location + \"_unencrypted\"\n    cmd(archiver, \"rcreate\", \"--encryption=none\")\n    os.environ[\"BORG_PASSPHRASE\"] = \"passphrase\"\n    archiver.repository_location = original_location + \"_encrypted\"\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    cmd(archiver, \"create\", \"test\", \"input\")\n    archiver.repository_location = original_location + \"_unencrypted\"\n    cmd(archiver, \"rdelete\", \"--cache-only\")\n    archiver.repository_location = original_location + \"_encrypted\"\n    cmd(archiver, \"rdelete\", \"--cache-only\")\n    shutil.rmtree(archiver.repository_path + \"_encrypted\")\n    os.replace(archiver.repository_path + \"_unencrypted\", archiver.repository_path + \"_encrypted\")\n    if archiver.FORK_DEFAULT:\n        cmd(archiver, \"create\", \"test.2\", \"input\", exit_code=EXIT_ERROR)\n    else:\n        with pytest.raises(Cache.RepositoryAccessAborted):\n            cmd(archiver, \"create\", \"test.2\", \"input\")\n\n\ndef test_repository_swap_detection_repokey_blank_passphrase(archivers, request, monkeypatch):\n    archiver = request.getfixturevalue(archivers)\n    # Check that a repokey repo with a blank passphrase is considered like a plaintext repo.\n    create_test_files(archiver.input_path)\n    # User initializes her repository with her passphrase\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    cmd(archiver, \"create\", \"test\", \"input\")\n    # Attacker replaces it with her own repository, which is encrypted but has no passphrase set\n    shutil.rmtree(archiver.repository_path)\n\n    monkeypatch.setenv(\"BORG_PASSPHRASE\", \"\")\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    # Delete cache & security database, AKA switch to user perspective\n    cmd(archiver, \"rdelete\", \"--cache-only\")\n    shutil.rmtree(get_security_directory(archiver.repository_path))\n\n    monkeypatch.delenv(\"BORG_PASSPHRASE\")\n    # This is the part were the user would be tricked, e.g. she assumes that BORG_PASSPHRASE\n    # is set, while it isn't. Previously this raised no warning,\n    # since the repository is, technically, encrypted.\n    if archiver.FORK_DEFAULT:\n        cmd(archiver, \"create\", \"test.2\", \"input\", exit_code=EXIT_ERROR)\n    else:\n        with pytest.raises(Cache.CacheInitAbortedError):\n            cmd(archiver, \"create\", \"test.2\", \"input\")\n\n\ndef test_repository_move(archivers, request, monkeypatch):\n    archiver = request.getfixturevalue(archivers)\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    security_dir = get_security_directory(archiver.repository_path)\n    os.replace(archiver.repository_path, archiver.repository_path + \"_new\")\n    archiver.repository_location += \"_new\"\n    monkeypatch.setenv(\"BORG_RELOCATED_REPO_ACCESS_IS_OK\", \"yes\")\n    cmd(archiver, \"rinfo\")\n    monkeypatch.delenv(\"BORG_RELOCATED_REPO_ACCESS_IS_OK\")\n    with open(os.path.join(security_dir, \"location\")) as fd:\n        location = fd.read()\n        assert location == Location(archiver.repository_location).canonical_path()\n    # Needs no confirmation anymore\n    cmd(archiver, \"rinfo\")\n    shutil.rmtree(archiver.cache_path)\n    cmd(archiver, \"rinfo\")\n    shutil.rmtree(security_dir)\n    cmd(archiver, \"rinfo\")\n    for file in (\"location\", \"key-type\", \"manifest-timestamp\"):\n        assert os.path.exists(os.path.join(security_dir, file))\n\n\ndef test_security_dir_compat(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    with open(os.path.join(get_security_directory(archiver.repository_path), \"location\"), \"w\") as fd:\n        fd.write(\"something outdated\")\n    # This is fine, because the cache still has the correct information. security_dir and cache can disagree\n    # if older versions are used to confirm a renamed repository.\n    cmd(archiver, \"rinfo\")\n\n\ndef test_unknown_unencrypted(archivers, request, monkeypatch):\n    archiver = request.getfixturevalue(archivers)\n    cmd(archiver, \"rcreate\", \"--encryption=none\")\n    # Ok: repository is known\n    cmd(archiver, \"rinfo\")\n\n    # Ok: repository is still known (through security_dir)\n    shutil.rmtree(archiver.cache_path)\n    cmd(archiver, \"rinfo\")\n\n    # Needs confirmation: cache and security dir both gone (e.g. another host or rm -rf ~)\n    shutil.rmtree(get_security_directory(archiver.repository_path))\n    if archiver.FORK_DEFAULT:\n        cmd(archiver, \"rinfo\", exit_code=EXIT_ERROR)\n    else:\n        with pytest.raises(Cache.CacheInitAbortedError):\n            cmd(archiver, \"rinfo\")\n    monkeypatch.setenv(\"BORG_UNKNOWN_UNENCRYPTED_REPO_ACCESS_IS_OK\", \"yes\")\n    cmd(archiver, \"rinfo\")\n\n\ndef test_unknown_feature_on_create(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    print(cmd(archiver, \"rcreate\", RK_ENCRYPTION))\n    add_unknown_feature(archiver.repository_path, Manifest.Operation.WRITE)\n    cmd_raises_unknown_feature(archiver, [\"create\", \"test\", \"input\"])\n\n\ndef test_unknown_feature_on_cache_sync(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    cmd(archiver, \"rdelete\", \"--cache-only\")\n    add_unknown_feature(archiver.repository_path, Manifest.Operation.READ)\n    cmd_raises_unknown_feature(archiver, [\"create\", \"test\", \"input\"])\n\n\ndef test_unknown_feature_on_change_passphrase(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    print(cmd(archiver, \"rcreate\", RK_ENCRYPTION))\n    add_unknown_feature(archiver.repository_path, Manifest.Operation.CHECK)\n    cmd_raises_unknown_feature(archiver, [\"key\", \"change-passphrase\"])\n\n\ndef test_unknown_feature_on_read(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    print(cmd(archiver, \"rcreate\", RK_ENCRYPTION))\n    cmd(archiver, \"create\", \"test\", \"input\")\n    add_unknown_feature(archiver.repository_path, Manifest.Operation.READ)\n    with changedir(\"output\"):\n        cmd_raises_unknown_feature(archiver, [\"extract\", \"test\"])\n    cmd_raises_unknown_feature(archiver, [\"rlist\"])\n    cmd_raises_unknown_feature(archiver, [\"info\", \"-a\", \"test\"])\n\n\ndef test_unknown_feature_on_rename(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    print(cmd(archiver, \"rcreate\", RK_ENCRYPTION))\n    cmd(archiver, \"create\", \"test\", \"input\")\n    add_unknown_feature(archiver.repository_path, Manifest.Operation.CHECK)\n    cmd_raises_unknown_feature(archiver, [\"rename\", \"test\", \"other\"])\n\n\ndef test_unknown_feature_on_delete(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    print(cmd(archiver, \"rcreate\", RK_ENCRYPTION))\n    cmd(archiver, \"create\", \"test\", \"input\")\n    add_unknown_feature(archiver.repository_path, Manifest.Operation.DELETE)\n    # delete of an archive raises\n    cmd_raises_unknown_feature(archiver, [\"delete\", \"-a\", \"test\"])\n    cmd_raises_unknown_feature(archiver, [\"prune\", \"--keep-daily=3\"])\n    # delete of the whole repository ignores features\n    cmd(archiver, \"rdelete\")\n\n\n@pytest.mark.skipif(not llfuse, reason=\"llfuse not installed\")\ndef test_unknown_feature_on_mount(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    cmd(archiver, \"create\", \"test\", \"input\")\n    add_unknown_feature(archiver.repository_path, Manifest.Operation.READ)\n    mountpoint = os.path.join(archiver.tmpdir, \"mountpoint\")\n    os.mkdir(mountpoint)\n    # XXX this might hang if it doesn't raise an error\n    archiver.repository_location += \"::test\"\n    cmd_raises_unknown_feature(archiver, [\"mount\", mountpoint])\n\n\n@pytest.mark.allow_cache_wipe\ndef test_unknown_mandatory_feature_in_cache(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    remote_repo = archiver.get_kind() == \"remote\"\n    print(cmd(archiver, \"rcreate\", RK_ENCRYPTION))\n\n    with Repository(archiver.repository_path, exclusive=True) as repository:\n        if remote_repo:\n            repository._location = Location(archiver.repository_location)\n        manifest = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)\n        with Cache(repository, manifest) as cache:\n            cache.begin_txn()\n            cache.cache_config.mandatory_features = {\"unknown-feature\"}\n            cache.commit()\n\n    if archiver.FORK_DEFAULT:\n        cmd(archiver, \"create\", \"test\", \"input\")\n    else:\n        called = False\n        wipe_cache_safe = LocalCache.wipe_cache\n\n        def wipe_wrapper(*args):\n            nonlocal called\n            called = True\n            wipe_cache_safe(*args)\n\n        with patch.object(LocalCache, \"wipe_cache\", wipe_wrapper):\n            cmd(archiver, \"create\", \"test\", \"input\")\n\n        assert called\n\n    with Repository(archiver.repository_path, exclusive=True) as repository:\n        if remote_repo:\n            repository._location = Location(archiver.repository_location)\n        manifest = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)\n        with Cache(repository, manifest) as cache:\n            assert cache.cache_config.mandatory_features == set()\n\n\ndef test_check_cache(archivers, request):\n    archiver = request.getfixturevalue(archivers)\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    cmd(archiver, \"create\", \"test\", \"input\")\n    with open_repository(archiver) as repository:\n        manifest = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)\n        with Cache(repository, manifest, sync=False) as cache:\n            cache.begin_txn()\n            cache.chunks.incref(list(cache.chunks.iteritems())[0][0])\n            cache.commit()\n    with pytest.raises(AssertionError):\n        check_cache(archiver)\n\n\n#  Begin manifest TAM tests\ndef spoof_manifest(repository):\n    with repository:\n        manifest = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)\n        cdata = manifest.repo_objs.format(\n            Manifest.MANIFEST_ID,\n            {},\n            msgpack.packb(\n                {\n                    \"version\": 1,\n                    \"archives\": {},\n                    \"config\": {},\n                    \"timestamp\": (datetime.now(tz=timezone.utc) + timedelta(days=1)).isoformat(timespec=\"microseconds\"),\n                }\n            ),\n        )\n        repository.put(Manifest.MANIFEST_ID, cdata)\n        repository.commit(compact=False)\n\n\ndef test_fresh_init_tam_required(archiver):\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    repository = Repository(archiver.repository_path, exclusive=True)\n    with repository:\n        manifest = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)\n        cdata = manifest.repo_objs.format(\n            Manifest.MANIFEST_ID,\n            {},\n            msgpack.packb(\n                {\n                    \"version\": 1,\n                    \"archives\": {},\n                    \"timestamp\": (datetime.now(tz=timezone.utc) + timedelta(days=1)).isoformat(timespec=\"microseconds\"),\n                }\n            ),\n        )\n        repository.put(Manifest.MANIFEST_ID, cdata)\n        repository.commit(compact=False)\n\n    with pytest.raises(TAMRequiredError):\n        cmd(archiver, \"rlist\")\n\n\ndef test_not_required(archiver):\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    create_src_archive(archiver, \"archive1234\")\n    repository = Repository(archiver.repository_path, exclusive=True)\n    # Manifest must be authenticated now\n    output = cmd(archiver, \"rlist\", \"--debug\")\n    assert \"archive1234\" in output\n    assert \"TAM-verified manifest\" in output\n    # Try to spoof / modify pre-1.0.9\n    spoof_manifest(repository)\n    # Fails\n    with pytest.raises(TAMRequiredError):\n        cmd(archiver, \"rlist\")\n\n\n#  Begin archive TAM tests\ndef write_archive_without_tam(repository, archive_name):\n    manifest = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)\n    archive_data = msgpack.packb(\n        {\n            \"version\": 2,\n            \"name\": archive_name,\n            \"item_ptrs\": [],\n            \"command_line\": \"\",\n            \"hostname\": \"\",\n            \"username\": \"\",\n            \"time\": archive_ts_now().isoformat(timespec=\"microseconds\"),\n            \"size\": 0,\n            \"nfiles\": 0,\n        }\n    )\n    archive_id = manifest.repo_objs.id_hash(archive_data)\n    cdata = manifest.repo_objs.format(archive_id, {}, archive_data)\n    repository.put(archive_id, cdata)\n    manifest.archives[archive_name] = (archive_id, datetime.now())\n    manifest.write()\n    repository.commit(compact=False)\n\n\ndef test_check_rebuild_manifest(archiver):\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    create_src_archive(archiver, \"archive_tam\")\n    repository = Repository(archiver.repository_path, exclusive=True)\n    with repository:\n        write_archive_without_tam(repository, \"archive_no_tam\")\n        repository.delete(Manifest.MANIFEST_ID)  # kill manifest, so check has to rebuild it\n        repository.commit(compact=False)\n    cmd(archiver, \"check\", \"--repair\")\n    output = cmd(archiver, \"rlist\", \"--format='{name} tam:{tam}{NL}'\")\n    assert \"archive_tam tam:verified\" in output  # TAM-verified archive is in rebuilt manifest\n    assert \"archive_no_tam\" not in output  # check got rid of untrusted not TAM-verified archive\n\n\ndef test_check_rebuild_refcounts(archiver):\n    cmd(archiver, \"rcreate\", RK_ENCRYPTION)\n    create_src_archive(archiver, \"archive_tam\")\n    archive_id_pre_check = cmd(archiver, \"rlist\", \"--format='{name} {id}{NL}'\")\n    repository = Repository(archiver.repository_path, exclusive=True)\n    with repository:\n        write_archive_without_tam(repository, \"archive_no_tam\")\n    output = cmd(archiver, \"rlist\", \"--format='{name} tam:{tam}{NL}'\")\n    assert \"archive_tam tam:verified\" in output  # good\n    assert \"archive_no_tam tam:none\" in output  # could be borg < 1.0.9 archive or fake\n    cmd(archiver, \"check\", \"--repair\")\n    output = cmd(archiver, \"rlist\", \"--format='{name} tam:{tam}{NL}'\")\n    assert \"archive_tam tam:verified\" in output  # TAM-verified archive still there\n    assert \"archive_no_tam\" not in output  # check got rid of untrusted not TAM-verified archive\n    archive_id_post_check = cmd(archiver, \"rlist\", \"--format='{name} {id}{NL}'\")\n    assert archive_id_post_check == archive_id_pre_check  # rebuild_refcounts didn't change archive_tam archive id\n\n\n# Begin Remote Tests\ndef test_remote_repo_restrict_to_path(remote_archiver):\n    original_location, repo_path = remote_archiver.repository_location, remote_archiver.repository_path\n    # restricted to repo directory itself:\n    with patch.object(RemoteRepository, \"extra_test_args\", [\"--restrict-to-path\", repo_path]):\n        cmd(remote_archiver, \"rcreate\", RK_ENCRYPTION)\n    # restricted to repo directory itself, fail for other directories with same prefix:\n    with patch.object(RemoteRepository, \"extra_test_args\", [\"--restrict-to-path\", repo_path]):\n        with pytest.raises(PathNotAllowed):\n            remote_archiver.repository_location = original_location + \"_0\"\n            cmd(remote_archiver, \"rcreate\", RK_ENCRYPTION)\n    # restricted to a completely different path:\n    with patch.object(RemoteRepository, \"extra_test_args\", [\"--restrict-to-path\", \"/foo\"]):\n        with pytest.raises(PathNotAllowed):\n            remote_archiver.repository_location = original_location + \"_1\"\n            cmd(remote_archiver, \"rcreate\", RK_ENCRYPTION)\n    path_prefix = os.path.dirname(repo_path)\n    # restrict to repo directory's parent directory:\n    with patch.object(RemoteRepository, \"extra_test_args\", [\"--restrict-to-path\", path_prefix]):\n        remote_archiver.repository_location = original_location + \"_2\"\n        cmd(remote_archiver, \"rcreate\", RK_ENCRYPTION)\n    # restrict to repo directory's parent directory and another directory:\n    with patch.object(\n        RemoteRepository, \"extra_test_args\", [\"--restrict-to-path\", \"/foo\", \"--restrict-to-path\", path_prefix]\n    ):\n        remote_archiver.repository_location = original_location + \"_3\"\n        cmd(remote_archiver, \"rcreate\", RK_ENCRYPTION)\n\n\ndef test_remote_repo_restrict_to_repository(remote_archiver):\n    repo_path = remote_archiver.repository_path\n    # restricted to repo directory itself:\n    with patch.object(RemoteRepository, \"extra_test_args\", [\"--restrict-to-repository\", repo_path]):\n        cmd(remote_archiver, \"rcreate\", RK_ENCRYPTION)\n    parent_path = os.path.join(repo_path, \"..\")\n    with patch.object(RemoteRepository, \"extra_test_args\", [\"--restrict-to-repository\", parent_path]):\n        with pytest.raises(PathNotAllowed):\n            cmd(remote_archiver, \"rcreate\", RK_ENCRYPTION)\n\n\ndef test_remote_repo_strip_components_doesnt_leak(remote_archiver):\n    cmd(remote_archiver, \"rcreate\", RK_ENCRYPTION)\n    create_regular_file(remote_archiver.input_path, \"dir/file\", contents=b\"test file contents 1\")\n    create_regular_file(remote_archiver.input_path, \"dir/file2\", contents=b\"test file contents 2\")\n    create_regular_file(remote_archiver.input_path, \"skipped-file1\", contents=b\"test file contents 3\")\n    create_regular_file(remote_archiver.input_path, \"skipped-file2\", contents=b\"test file contents 4\")\n    create_regular_file(remote_archiver.input_path, \"skipped-file3\", contents=b\"test file contents 5\")\n    cmd(remote_archiver, \"create\", \"test\", \"input\")\n    marker = \"cached responses left in RemoteRepository\"\n    with changedir(\"output\"):\n        res = cmd(remote_archiver, \"extract\", \"test\", \"--debug\", \"--strip-components\", \"3\")\n        assert marker not in res\n        with assert_creates_file(\"file\"):\n            res = cmd(remote_archiver, \"extract\", \"test\", \"--debug\", \"--strip-components\", \"2\")\n            assert marker not in res\n        with assert_creates_file(\"dir/file\"):\n            res = cmd(remote_archiver, \"extract\", \"test\", \"--debug\", \"--strip-components\", \"1\")\n            assert marker not in res\n        with assert_creates_file(\"input/dir/file\"):\n            res = cmd(remote_archiver, \"extract\", \"test\", \"--debug\", \"--strip-components\", \"0\")\n            assert marker not in res\n", "import tempfile\nfrom binascii import hexlify, unhexlify, a2b_base64\nfrom unittest.mock import MagicMock\n\nimport pytest\n\nfrom ..crypto.key import bin_to_hex\nfrom ..crypto.key import PlaintextKey, AuthenticatedKey, Blake2AuthenticatedKey\nfrom ..crypto.key import RepoKey, KeyfileKey, Blake2RepoKey, Blake2KeyfileKey\nfrom ..crypto.key import AEADKeyBase\nfrom ..crypto.key import AESOCBRepoKey, AESOCBKeyfileKey, CHPORepoKey, CHPOKeyfileKey\nfrom ..crypto.key import Blake2AESOCBRepoKey, Blake2AESOCBKeyfileKey, Blake2CHPORepoKey, Blake2CHPOKeyfileKey\nfrom ..crypto.key import ID_HMAC_SHA_256, ID_BLAKE2b_256\nfrom ..crypto.key import TAMRequiredError, TAMInvalid, TAMUnsupportedSuiteError, ArchiveTAMInvalid\nfrom ..crypto.key import UnsupportedManifestError, UnsupportedKeyFormatError\nfrom ..crypto.key import identify_key\nfrom ..crypto.low_level import IntegrityError as IntegrityErrorBase\nfrom ..helpers import IntegrityError\nfrom ..helpers import Location\nfrom ..helpers import StableDict\nfrom ..helpers import msgpack\nfrom ..constants import KEY_ALGORITHMS\n\n\nclass TestKey:\n    class MockArgs:\n        location = Location(tempfile.mkstemp()[1])\n        key_algorithm = \"argon2\"\n\n    keyfile2_key_file = \"\"\"\n        BORG_KEY 0000000000000000000000000000000000000000000000000000000000000000\n        hqlhbGdvcml0aG2mc2hhMjU2pGRhdGHaAN4u2SiN7hqISe3OA8raBWNuvHn1R50ZU7HVCn\n        11vTJNEaj9soxUaIGcW+pAB2N5yYoKMg/sGCMuZa286iJ008DvN99rf/ORfcKrK2GmzslO\n        N3uv9Tk9HtqV/Sq5zgM9xuY9rEeQGDQVQ+AOsFamJqSUrAemGJbJqw9IerXC/jN4XPnX6J\n        pi1cXCFxHfDaEhmWrkdPNoZdirCv/eP/dOVOLmwU58YsS+MvkZNfEa16el/fSb/ENdrwJ/\n        2aYMQrDdk1d5MYzkjotv/KpofNwPXZchu2EwH7OIHWQjEVL1DZWkaGFzaNoAIO/7qn1hr3\n        F84MsMMiqpbz4KVICeBZhfAaTPs4W7BC63qml0ZXJhdGlvbnPOAAGGoKRzYWx02gAgLENQ\n        2uVCoR7EnAoiRzn8J+orbojKtJlNCnQ31SSC8rendmVyc2lvbgE=\"\"\".strip()\n\n    keyfile2_cdata = bytes.fromhex(\n        \"003be7d57280d1a42add9f3f36ea363bbc5e9349ad01ddec0634a54dd02959e70500000000000003ec063d2cbcacba6b\"\n    )\n    keyfile2_id = unhexlify(\"c3fbf14bc001ebcc3cd86e696c13482ed071740927cd7cbe1b01b4bfcee49314\")\n\n    keyfile_blake2_key_file = \"\"\"\n        BORG_KEY 0000000000000000000000000000000000000000000000000000000000000000\n        hqlhbGdvcml0aG2mc2hhMjU2pGRhdGHaAZ7VCsTjbLhC1ipXOyhcGn7YnROEhP24UQvOCi\n        Oar1G+JpwgO9BIYaiCODUpzPuDQEm6WxyTwEneJ3wsuyeqyh7ru2xo9FAUKRf6jcqqZnan\n        ycTfktkUC+CPhKR7W6MTu5fPvy99chyL09/RGdD15aswR5PjNoFu4626sfMrBReyPdlxqt\n        F80m+fbNE/vln2Trqoz9EMHQ3IxjIK4q0m4Aj7TwCu7ZankFtwt898+tYsWE7lb2Ps/gXB\n        F8PM/5wHpYps2AKhDCpwKp5HyqIqlF5IzR2ydL9QP20QBjp/rSi6b+xwrfxNJZfw78f8ef\n        A2Yj7xIsxNQ0kmVmTL/UF6d7+Mw1JfurWrySiDU7QQ+RiZpWUZ0DdReB+e4zn6/KNKC884\n        34SGywADuLIQe2FKU+5jBCbutEyEGILQbAR/cgeLy5+V2XwXMJh4ytwXVIeT6Lk+qhYAdz\n        Klx4ub7XijKcOxJyBE+4k33DAhcfIT2r4/sxgMhXrIOEQPKsMAixzdcqVYkpou+6c4PZeL\n        nr+UjfJwOqK1BlWk1NgwE4GXYIKkaGFzaNoAIAzjUtpBPPh6kItZtHQZvnQG6FpucZNfBC\n        UTHFJg343jqml0ZXJhdGlvbnPOAAGGoKRzYWx02gAgz3YaUZZ/s+UWywj97EY5b4KhtJYi\n        qkPqtDDxs2j/T7+ndmVyc2lvbgE=\"\"\".strip()\n\n    keyfile_blake2_cdata = bytes.fromhex(\n        \"04d6040f5ef80e0a8ac92badcbe3dee83b7a6b53d5c9a58c4eed14964cb10ef591040404040404040d1e65cc1f435027\"\n    )\n    # Verified against b2sum. Entire string passed to BLAKE2, including the padded 64 byte key contained in\n    # keyfile_blake2_key_file above is\n    # 19280471de95185ec27ecb6fc9edbb4f4db26974c315ede1cd505fab4250ce7cd0d081ea66946c\n    # 95f0db934d5f616921efbd869257e8ded2bd9bd93d7f07b1a30000000000000000000000000000\n    # 000000000000000000000000000000000000000000000000000000000000000000000000000000\n    # 00000000000000000000007061796c6f6164\n    #                       p a y l o a d\n    keyfile_blake2_id = bytes.fromhex(\"d8bc68e961c79f99be39061589e5179b2113cd9226e07b08ddd4a1fef7ce93fb\")\n\n    @pytest.fixture\n    def keys_dir(self, request, monkeypatch, tmpdir):\n        monkeypatch.setenv(\"BORG_KEYS_DIR\", str(tmpdir))\n        return tmpdir\n\n    @pytest.fixture(\n        params=(\n            # not encrypted\n            PlaintextKey,\n            AuthenticatedKey,\n            Blake2AuthenticatedKey,\n            # legacy crypto\n            KeyfileKey,\n            Blake2KeyfileKey,\n            RepoKey,\n            Blake2RepoKey,\n            # new crypto\n            AESOCBKeyfileKey,\n            AESOCBRepoKey,\n            Blake2AESOCBKeyfileKey,\n            Blake2AESOCBRepoKey,\n            CHPOKeyfileKey,\n            CHPORepoKey,\n            Blake2CHPOKeyfileKey,\n            Blake2CHPORepoKey,\n        )\n    )\n    def key(self, request, monkeypatch):\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"test\")\n        return request.param.create(self.MockRepository(), self.MockArgs())\n\n    class MockRepository:\n        class _Location:\n            raw = processed = \"/some/place\"\n\n            def canonical_path(self):\n                return self.processed\n\n        _location = _Location()\n        id = bytes(32)\n        id_str = bin_to_hex(id)\n        version = 2\n\n        def save_key(self, data):\n            self.key_data = data\n\n        def load_key(self):\n            return self.key_data\n\n    def test_plaintext(self):\n        key = PlaintextKey.create(None, None)\n        chunk = b\"foo\"\n        id = key.id_hash(chunk)\n        assert hexlify(id) == b\"2c26b46b68ffc68ff99b453c1d30413413422d706483bfa0f98a5e886266e7ae\"\n        assert chunk == key.decrypt(id, key.encrypt(id, chunk))\n\n    def test_keyfile(self, monkeypatch, keys_dir):\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"test\")\n        key = KeyfileKey.create(self.MockRepository(), self.MockArgs())\n        assert key.cipher.next_iv() == 0\n        chunk = b\"ABC\"\n        id = key.id_hash(chunk)\n        manifest = key.encrypt(id, chunk)\n        assert key.cipher.extract_iv(manifest) == 0\n        manifest2 = key.encrypt(id, chunk)\n        assert manifest != manifest2\n        assert key.decrypt(id, manifest) == key.decrypt(id, manifest2)\n        assert key.cipher.extract_iv(manifest2) == 1\n        iv = key.cipher.extract_iv(manifest)\n        key2 = KeyfileKey.detect(self.MockRepository(), manifest)\n        assert key2.cipher.next_iv() >= iv + key2.cipher.block_count(len(manifest) - KeyfileKey.PAYLOAD_OVERHEAD)\n        # Key data sanity check\n        assert len({key2.id_key, key2.crypt_key}) == 2\n        assert key2.chunk_seed != 0\n        chunk = b\"foo\"\n        id = key.id_hash(chunk)\n        assert chunk == key2.decrypt(id, key.encrypt(id, chunk))\n\n    def test_keyfile_kfenv(self, tmpdir, monkeypatch):\n        keyfile = tmpdir.join(\"keyfile\")\n        monkeypatch.setenv(\"BORG_KEY_FILE\", str(keyfile))\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"testkf\")\n        assert not keyfile.exists()\n        key = CHPOKeyfileKey.create(self.MockRepository(), self.MockArgs())\n        assert keyfile.exists()\n        chunk = b\"ABC\"\n        chunk_id = key.id_hash(chunk)\n        chunk_cdata = key.encrypt(chunk_id, chunk)\n        key = CHPOKeyfileKey.detect(self.MockRepository(), chunk_cdata)\n        assert chunk == key.decrypt(chunk_id, chunk_cdata)\n        keyfile.remove()\n        with pytest.raises(FileNotFoundError):\n            CHPOKeyfileKey.detect(self.MockRepository(), chunk_cdata)\n\n    def test_keyfile2(self, monkeypatch, keys_dir):\n        with keys_dir.join(\"keyfile\").open(\"w\") as fd:\n            fd.write(self.keyfile2_key_file)\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"passphrase\")\n        key = KeyfileKey.detect(self.MockRepository(), self.keyfile2_cdata)\n        assert key.decrypt(self.keyfile2_id, self.keyfile2_cdata) == b\"payload\"\n\n    def test_keyfile2_kfenv(self, tmpdir, monkeypatch):\n        keyfile = tmpdir.join(\"keyfile\")\n        with keyfile.open(\"w\") as fd:\n            fd.write(self.keyfile2_key_file)\n        monkeypatch.setenv(\"BORG_KEY_FILE\", str(keyfile))\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"passphrase\")\n        key = KeyfileKey.detect(self.MockRepository(), self.keyfile2_cdata)\n        assert key.decrypt(self.keyfile2_id, self.keyfile2_cdata) == b\"payload\"\n\n    def test_keyfile_blake2(self, monkeypatch, keys_dir):\n        with keys_dir.join(\"keyfile\").open(\"w\") as fd:\n            fd.write(self.keyfile_blake2_key_file)\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"passphrase\")\n        key = Blake2KeyfileKey.detect(self.MockRepository(), self.keyfile_blake2_cdata)\n        assert key.decrypt(self.keyfile_blake2_id, self.keyfile_blake2_cdata) == b\"payload\"\n\n    def _corrupt_byte(self, key, data, offset):\n        data = bytearray(data)\n        # note: we corrupt in a way so that even corruption of the unauthenticated encryption type byte\n        # will trigger an IntegrityError (does not happen while we stay within TYPES_ACCEPTABLE).\n        data[offset] ^= 64\n        with pytest.raises(IntegrityErrorBase):\n            key.decrypt(b\"\", data)\n\n    def test_decrypt_integrity(self, monkeypatch, keys_dir):\n        with keys_dir.join(\"keyfile\").open(\"w\") as fd:\n            fd.write(self.keyfile2_key_file)\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"passphrase\")\n        key = KeyfileKey.detect(self.MockRepository(), self.keyfile2_cdata)\n\n        data = self.keyfile2_cdata\n        for i in range(len(data)):\n            self._corrupt_byte(key, data, i)\n\n        with pytest.raises(IntegrityError):\n            data = bytearray(self.keyfile2_cdata)\n            id = bytearray(key.id_hash(data))  # corrupt chunk id\n            id[12] = 0\n            plaintext = key.decrypt(id, data)\n            key.assert_id(id, plaintext)\n\n    def test_roundtrip(self, key):\n        repository = key.repository\n        plaintext = b\"foo\"\n        id = key.id_hash(plaintext)\n        encrypted = key.encrypt(id, plaintext)\n        identified_key_class = identify_key(encrypted)\n        assert identified_key_class == key.__class__\n        loaded_key = identified_key_class.detect(repository, encrypted)\n        decrypted = loaded_key.decrypt(id, encrypted)\n        assert decrypted == plaintext\n\n    def test_assert_id(self, key):\n        plaintext = b\"123456789\"\n        id = key.id_hash(plaintext)\n        key.assert_id(id, plaintext)\n        id_changed = bytearray(id)\n        id_changed[0] ^= 1\n        if not isinstance(key, AEADKeyBase):\n            with pytest.raises(IntegrityError):\n                key.assert_id(id_changed, plaintext)\n            plaintext_changed = plaintext + b\"1\"\n            with pytest.raises(IntegrityError):\n                key.assert_id(id, plaintext_changed)\n\n    def test_authenticated_encrypt(self, monkeypatch):\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"test\")\n        key = AuthenticatedKey.create(self.MockRepository(), self.MockArgs())\n        assert AuthenticatedKey.id_hash is ID_HMAC_SHA_256.id_hash\n        assert len(key.id_key) == 32\n        plaintext = b\"123456789\"\n        id = key.id_hash(plaintext)\n        authenticated = key.encrypt(id, plaintext)\n        # 0x07 is the key TYPE.\n        assert authenticated == b\"\\x07\" + plaintext\n\n    def test_blake2_authenticated_encrypt(self, monkeypatch):\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"test\")\n        key = Blake2AuthenticatedKey.create(self.MockRepository(), self.MockArgs())\n        assert Blake2AuthenticatedKey.id_hash is ID_BLAKE2b_256.id_hash\n        assert len(key.id_key) == 128\n        plaintext = b\"123456789\"\n        id = key.id_hash(plaintext)\n        authenticated = key.encrypt(id, plaintext)\n        # 0x06 is the key TYPE.\n        assert authenticated == b\"\\x06\" + plaintext\n\n\nclass TestTAM:\n    @pytest.fixture\n    def key(self, monkeypatch):\n        monkeypatch.setenv(\"BORG_PASSPHRASE\", \"test\")\n        return CHPOKeyfileKey.create(TestKey.MockRepository(), TestKey.MockArgs())\n\n    def test_unpack_future(self, key):\n        blob = b\"\\xc1\\xc1\\xc1\\xc1foobar\"\n        with pytest.raises(UnsupportedManifestError):\n            key.unpack_and_verify_manifest(blob)\n\n        blob = b\"\\xc1\\xc1\\xc1\"\n        with pytest.raises(msgpack.UnpackException):\n            key.unpack_and_verify_manifest(blob)\n\n    def test_missing_when_required(self, key):\n        blob = msgpack.packb({})\n        with pytest.raises(TAMRequiredError):\n            key.unpack_and_verify_manifest(blob)\n        with pytest.raises(TAMRequiredError):\n            key.unpack_and_verify_archive(blob)\n\n    def test_missing(self, key):\n        blob = msgpack.packb({})\n        key.tam_required = False\n        unpacked, verified = key.unpack_and_verify_manifest(blob)\n        assert unpacked == {}\n        assert not verified\n        unpacked, verified, _ = key.unpack_and_verify_archive(blob)\n        assert unpacked == {}\n        assert not verified\n\n    def test_unknown_type_when_required(self, key):\n        blob = msgpack.packb({\"tam\": {\"type\": \"HMAC_VOLLBIT\"}})\n        with pytest.raises(TAMUnsupportedSuiteError):\n            key.unpack_and_verify_manifest(blob)\n        with pytest.raises(TAMUnsupportedSuiteError):\n            key.unpack_and_verify_archive(blob)\n\n    def test_unknown_type(self, key):\n        blob = msgpack.packb({\"tam\": {\"type\": \"HMAC_VOLLBIT\"}})\n        key.tam_required = False\n        unpacked, verified = key.unpack_and_verify_manifest(blob)\n        assert unpacked == {}\n        assert not verified\n        unpacked, verified, _ = key.unpack_and_verify_archive(blob)\n        assert unpacked == {}\n        assert not verified\n\n    @pytest.mark.parametrize(\n        \"tam, exc\",\n        (\n            ({}, TAMUnsupportedSuiteError),\n            ({\"type\": b\"\\xff\"}, TAMUnsupportedSuiteError),\n            (None, TAMInvalid),\n            (1234, TAMInvalid),\n        ),\n    )\n    def test_invalid_manifest(self, key, tam, exc):\n        blob = msgpack.packb({\"tam\": tam})\n        with pytest.raises(exc):\n            key.unpack_and_verify_manifest(blob)\n\n    @pytest.mark.parametrize(\n        \"tam, exc\",\n        (\n            ({}, TAMUnsupportedSuiteError),\n            ({\"type\": b\"\\xff\"}, TAMUnsupportedSuiteError),\n            (None, ArchiveTAMInvalid),\n            (1234, ArchiveTAMInvalid),\n        ),\n    )\n    def test_invalid_archive(self, key, tam, exc):\n        blob = msgpack.packb({\"tam\": tam})\n        with pytest.raises(exc):\n            key.unpack_and_verify_archive(blob)\n\n    @pytest.mark.parametrize(\n        \"hmac, salt\",\n        (({}, bytes(64)), (bytes(64), {}), (None, bytes(64)), (bytes(64), None)),\n        ids=[\"ed-b64\", \"b64-ed\", \"n-b64\", \"b64-n\"],\n    )\n    def test_wrong_types(self, key, hmac, salt):\n        data = {\"tam\": {\"type\": \"HKDF_HMAC_SHA512\", \"hmac\": hmac, \"salt\": salt}}\n        tam = data[\"tam\"]\n        if hmac is None:\n            del tam[\"hmac\"]\n        if salt is None:\n            del tam[\"salt\"]\n        blob = msgpack.packb(data)\n        with pytest.raises(TAMInvalid):\n            key.unpack_and_verify_manifest(blob)\n        with pytest.raises(ArchiveTAMInvalid):\n            key.unpack_and_verify_archive(blob)\n\n    def test_round_trip_manifest(self, key):\n        data = {\"foo\": \"bar\"}\n        blob = key.pack_and_authenticate_metadata(data, context=b\"manifest\")\n        assert blob.startswith(b\"\\x82\")\n\n        unpacked = msgpack.unpackb(blob)\n        assert unpacked[\"tam\"][\"type\"] == \"HKDF_HMAC_SHA512\"\n\n        unpacked, verified = key.unpack_and_verify_manifest(blob)\n        assert verified\n        assert unpacked[\"foo\"] == \"bar\"\n        assert \"tam\" not in unpacked\n\n    def test_round_trip_archive(self, key):\n        data = {\"foo\": \"bar\"}\n        blob = key.pack_and_authenticate_metadata(data, context=b\"archive\")\n        assert blob.startswith(b\"\\x82\")\n\n        unpacked = msgpack.unpackb(blob)\n        assert unpacked[\"tam\"][\"type\"] == \"HKDF_HMAC_SHA512\"\n\n        unpacked, verified, _ = key.unpack_and_verify_archive(blob)\n        assert verified\n        assert unpacked[\"foo\"] == \"bar\"\n        assert \"tam\" not in unpacked\n\n    @pytest.mark.parametrize(\"which\", (\"hmac\", \"salt\"))\n    def test_tampered_manifest(self, key, which):\n        data = {\"foo\": \"bar\"}\n        blob = key.pack_and_authenticate_metadata(data, context=b\"manifest\")\n        assert blob.startswith(b\"\\x82\")\n\n        unpacked = msgpack.unpackb(blob, object_hook=StableDict)\n        assert len(unpacked[\"tam\"][which]) == 64\n        unpacked[\"tam\"][which] = unpacked[\"tam\"][which][0:32] + bytes(32)\n        assert len(unpacked[\"tam\"][which]) == 64\n        blob = msgpack.packb(unpacked)\n\n        with pytest.raises(TAMInvalid):\n            key.unpack_and_verify_manifest(blob)\n\n    @pytest.mark.parametrize(\"which\", (\"hmac\", \"salt\"))\n    def test_tampered_archive(self, key, which):\n        data = {\"foo\": \"bar\"}\n        blob = key.pack_and_authenticate_metadata(data, context=b\"archive\")\n        assert blob.startswith(b\"\\x82\")\n\n        unpacked = msgpack.unpackb(blob, object_hook=StableDict)\n        assert len(unpacked[\"tam\"][which]) == 64\n        unpacked[\"tam\"][which] = unpacked[\"tam\"][which][0:32] + bytes(32)\n        assert len(unpacked[\"tam\"][which]) == 64\n        blob = msgpack.packb(unpacked)\n\n        with pytest.raises(ArchiveTAMInvalid):\n            key.unpack_and_verify_archive(blob)\n\n\ndef test_decrypt_key_file_unsupported_algorithm():\n    \"\"\"We will add more algorithms in the future. We should raise a helpful error.\"\"\"\n    key = CHPOKeyfileKey(None)\n    encrypted = msgpack.packb({\"algorithm\": \"THIS ALGORITHM IS NOT SUPPORTED\", \"version\": 1})\n\n    with pytest.raises(UnsupportedKeyFormatError):\n        key.decrypt_key_file(encrypted, \"hello, pass phrase\")\n\n\ndef test_decrypt_key_file_v2_is_unsupported():\n    \"\"\"There may eventually be a version 2 of the format. For now we should raise a helpful error.\"\"\"\n    key = CHPOKeyfileKey(None)\n    encrypted = msgpack.packb({\"version\": 2})\n\n    with pytest.raises(UnsupportedKeyFormatError):\n        key.decrypt_key_file(encrypted, \"hello, pass phrase\")\n\n\ndef test_key_file_roundtrip(monkeypatch):\n    def to_dict(key):\n        extract = \"repository_id\", \"crypt_key\", \"id_key\", \"chunk_seed\"\n        return {a: getattr(key, a) for a in extract}\n\n    repository = MagicMock(id=b\"repository_id\")\n    monkeypatch.setenv(\"BORG_PASSPHRASE\", \"hello, pass phrase\")\n\n    save_me = AESOCBRepoKey.create(repository, args=MagicMock(key_algorithm=\"argon2\"))\n    saved = repository.save_key.call_args.args[0]\n    repository.load_key.return_value = saved\n    load_me = AESOCBRepoKey.detect(repository, manifest_data=None)\n\n    assert to_dict(load_me) == to_dict(save_me)\n    assert msgpack.unpackb(a2b_base64(saved))[\"algorithm\"] == KEY_ALGORITHMS[\"argon2\"]\n"], "filenames": ["docs/changes_1.x.rst", "src/borg/archive.py", "src/borg/cache.py", "src/borg/crypto/key.py", "src/borg/helpers/msgpack.py", "src/borg/helpers/parseformat.py", "src/borg/testsuite/archiver/check_cmd.py", "src/borg/testsuite/archiver/checks.py", "src/borg/testsuite/key.py"], "buggy_code_start_loc": [6, 495, 758, 74, 222, 725, 9, 11, 14], "buggy_code_end_loc": [6, 2271, 759, 280, 226, 796, 247, 380, 361], "fixing_code_start_loc": [7, 496, 758, 75, 222, 726, 8, 11, 14], "fixing_code_end_loc": [75, 2297, 760, 349, 226, 802, 245, 437, 411], "type": "CWE-347", "message": "borgbackup is an opensource, deduplicating archiver with compression and authenticated encryption. A flaw in the cryptographic authentication scheme in borgbackup allowed an attacker to fake archives and potentially indirectly cause backup data loss in the repository. The attack requires an attacker to be able to: 1. insert files (with no additional headers) into backups and 2. gain write access to the repository. This vulnerability does not disclose plaintext to the attacker, nor does it affect the authenticity of existing archives. Creating plausible fake archives may be feasible for empty or small archives, but is unlikely for large archives. The issue has been fixed in borgbackup 1.2.5. Users are advised to upgrade. Additionally to installing the fixed code, users must follow the upgrade procedure as documented in the change log. Data loss after being attacked can be avoided by reviewing the archives (timestamp and contents valid and as expected) after any \"borg check --repair\" and before \"borg prune\". There are no known workarounds for this vulnerability.", "other": {"cve": {"id": "CVE-2023-36811", "sourceIdentifier": "security-advisories@github.com", "published": "2023-08-30T18:15:09.487", "lastModified": "2023-09-15T22:15:13.833", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "borgbackup is an opensource, deduplicating archiver with compression and authenticated encryption. A flaw in the cryptographic authentication scheme in borgbackup allowed an attacker to fake archives and potentially indirectly cause backup data loss in the repository. The attack requires an attacker to be able to: 1. insert files (with no additional headers) into backups and 2. gain write access to the repository. This vulnerability does not disclose plaintext to the attacker, nor does it affect the authenticity of existing archives. Creating plausible fake archives may be feasible for empty or small archives, but is unlikely for large archives. The issue has been fixed in borgbackup 1.2.5. Users are advised to upgrade. Additionally to installing the fixed code, users must follow the upgrade procedure as documented in the change log. Data loss after being attacked can be avoided by reviewing the archives (timestamp and contents valid and as expected) after any \"borg check --repair\" and before \"borg prune\". There are no known workarounds for this vulnerability."}, {"lang": "es", "value": "borgbackup es un archivador de duplicaci\u00f3n de c\u00f3digo abierto con compresi\u00f3n y cifrado autenticado. Una falla en el esquema de autenticaci\u00f3n criptogr\u00e1fica en borgbackup permiti\u00f3 a un atacante falsificar archivos y potencialmente causar indirectamente la p\u00e9rdida de datos de los respaldo en el repositorio. El ataque requiere que un atacante pueda: 1. insertar archivos (sin encabezados adicionales) en las copias de seguridad y 2. obtener acceso de escritura al repositorio. Esta vulnerabilidad no revela texto sin formato al atacante ni afecta la autenticidad de los archivos existentes. La creaci\u00f3n de archivos falsos plausibles puede ser factible para archivos vac\u00edos o peque\u00f1os, pero es poco probable para archivos grandes. El problema se solucion\u00f3 en borgbackup 1.2.5. Se recomienda a los usuarios que actualicen. Adem\u00e1s de instalar el c\u00f3digo fijo, los usuarios deben seguir el procedimiento de actualizaci\u00f3n como se documenta en el registro de cambios. La p\u00e9rdida de datos despu\u00e9s de un ataque se puede evitar revisando los archivos (marca de tiempo y contenido v\u00e1lidos y esperados) despu\u00e9s de cualquier \"borg check --repair\" y antes de \"borg prune\". No se conocen workarounds para esta vulnerabilidad."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:N/I:H/A:N", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 4.7, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.0, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:N/I:H/A:N", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 4.7, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.0, "impactScore": 3.6}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-347"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:borgbackup:borg:*:*:*:*:*:*:*:*", "versionEndExcluding": "1.2.5", "matchCriteriaId": "61C2D872-E840-4067-8CEE-A9DB11B89028"}]}]}], "references": [{"url": "https://github.com/borgbackup/borg/blob/1.2.5-cvedocs/docs/changes.rst#pre-125-archives-spoofing-vulnerability-cve-2023-36811", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/borgbackup/borg/commit/3eb070191da10c2d3f7bc6484cf3d51c3045f884", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/borgbackup/borg/security/advisories/GHSA-8fjr-hghr-4m99", "source": "security-advisories@github.com", "tags": ["Vendor Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/5Q3OHXERTU547SEQ3YREZXHOCYNLVD63/", "source": "security-advisories@github.com"}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/XOZDFIYEBIOKSIEAXUJJJFUJTAJ7TF3C/", "source": "security-advisories@github.com"}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/ZUCQSMAWOJBCRGF6XPKEZ2TPGAPNKIWV/", "source": "security-advisories@github.com"}]}, "github_commit_url": "https://github.com/borgbackup/borg/commit/3eb070191da10c2d3f7bc6484cf3d51c3045f884"}}