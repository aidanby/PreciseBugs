{"buggy_code": ["package config\n\nimport (\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\n\t\"github.com/docker/docker/api/types\"\n)\n\ntype dockerNetworkInterfaces struct {\n\tV4 struct {\n\t\tSubnet  string `default:\"172.18.0.0/16\"`\n\t\tGateway string `default:\"172.18.0.1\"`\n\t}\n\tV6 struct {\n\t\tSubnet  string `default:\"fdba:17c8:6c94::/64\"`\n\t\tGateway string `default:\"fdba:17c8:6c94::1011\"`\n\t}\n}\n\ntype DockerNetworkConfiguration struct {\n\t// The interface that should be used to create the network. Must not conflict\n\t// with any other interfaces in use by Docker or on the system.\n\tInterface string `default:\"172.18.0.1\" json:\"interface\" yaml:\"interface\"`\n\n\t// The DNS settings for containers.\n\tDns []string `default:\"[\\\"1.1.1.1\\\", \\\"1.0.0.1\\\"]\"`\n\n\t// The name of the network to use. If this network already exists it will not\n\t// be created. If it is not found, a new network will be created using the interface\n\t// defined.\n\tName       string                  `default:\"pterodactyl_nw\"`\n\tISPN       bool                    `default:\"false\" yaml:\"ispn\"`\n\tDriver     string                  `default:\"bridge\"`\n\tMode       string                  `default:\"pterodactyl_nw\" yaml:\"network_mode\"`\n\tIsInternal bool                    `default:\"false\" yaml:\"is_internal\"`\n\tEnableICC  bool                    `default:\"true\" yaml:\"enable_icc\"`\n\tInterfaces dockerNetworkInterfaces `yaml:\"interfaces\"`\n}\n\n// DockerConfiguration defines the docker configuration used by the daemon when\n// interacting with containers and networks on the system.\ntype DockerConfiguration struct {\n\t// Network configuration that should be used when creating a new network\n\t// for containers run through the daemon.\n\tNetwork DockerNetworkConfiguration `json:\"network\" yaml:\"network\"`\n\n\t// Domainname is the Docker domainname for all containers.\n\tDomainname string `default:\"\" json:\"domainname\" yaml:\"domainname\"`\n\n\t// Registries .\n\tRegistries map[string]RegistryConfiguration `json:\"registries\" yaml:\"registries\"`\n\n\t// The size of the /tmp directory when mounted into a container. Please be aware that Docker\n\t// utilizes host memory for this value, and that we do not keep track of the space used here\n\t// so avoid allocating too much to a server.\n\tTmpfsSize uint `default:\"100\" json:\"tmpfs_size\" yaml:\"tmpfs_size\"`\n}\n\n// RegistryConfiguration defines the authentication credentials for a given\n// Docker registry.\ntype RegistryConfiguration struct {\n\tUsername string `yaml:\"username\"`\n\tPassword string `yaml:\"password\"`\n}\n\n// Base64 returns the authentication for a given registry as a base64 encoded\n// string value.\nfunc (c RegistryConfiguration) Base64() (string, error) {\n\tb, err := json.Marshal(types.AuthConfig{\n\t\tUsername: c.Username,\n\t\tPassword: c.Password,\n\t})\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn base64.URLEncoding.EncodeToString(b), nil\n}\n", "package docker\n\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"emperror.dev/errors\"\n\t\"github.com/apex/log\"\n\t\"github.com/docker/docker/api/types\"\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/mount\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/docker/docker/daemon/logger/jsonfilelog\"\n\t\"github.com/pterodactyl/wings/config\"\n\t\"github.com/pterodactyl/wings/environment\"\n\t\"github.com/pterodactyl/wings/system\"\n)\n\nvar ErrNotAttached = errors.Sentinel(\"not attached to instance\")\n\n// A custom console writer that allows us to keep a function blocked until the\n// given stream is properly closed. This does nothing special, only exists to\n// make a noop io.Writer.\ntype noopWriter struct{}\n\nvar _ io.Writer = noopWriter{}\n\n// Implement the required Write function to satisfy the io.Writer interface.\nfunc (nw noopWriter) Write(b []byte) (int, error) {\n\treturn len(b), nil\n}\n\n// Attach attaches to the docker container itself and ensures that we can pipe\n// data in and out of the process stream. This should not be used for reading\n// console data as you *will* miss important output at the beginning because of\n// the time delay with attaching to the output.\n//\n// Calling this function will poll resources for the container in the background\n// until the provided context is canceled by the caller. Failure to cancel said\n// context will cause background memory leaks as the goroutine will not exit.\nfunc (e *Environment) Attach() error {\n\tif e.IsAttached() {\n\t\treturn nil\n\t}\n\n\tif err := e.followOutput(); err != nil {\n\t\treturn err\n\t}\n\n\topts := types.ContainerAttachOptions{\n\t\tStdin:  true,\n\t\tStdout: true,\n\t\tStderr: true,\n\t\tStream: true,\n\t}\n\n\t// Set the stream again with the container.\n\tif st, err := e.client.ContainerAttach(context.Background(), e.Id, opts); err != nil {\n\t\treturn err\n\t} else {\n\t\te.SetStream(&st)\n\t}\n\n\tgo func() {\n\t\tctx, cancel := context.WithCancel(context.Background())\n\t\tdefer cancel()\n\t\tdefer e.stream.Close()\n\t\tdefer func() {\n\t\t\te.SetState(environment.ProcessOfflineState)\n\t\t\te.SetStream(nil)\n\t\t}()\n\n\t\tgo func() {\n\t\t\tif err := e.pollResources(ctx); err != nil {\n\t\t\t\tif !errors.Is(err, context.Canceled) {\n\t\t\t\t\te.log().WithField(\"error\", err).Error(\"error during environment resource polling\")\n\t\t\t\t} else {\n\t\t\t\t\te.log().Warn(\"stopping server resource polling: context canceled\")\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\n\t\t// Block the completion of this routine until the container is no longer running. This allows\n\t\t// the pollResources function to run until it needs to be stopped. Because the container\n\t\t// can be polled for resource usage, even when stopped, we need to have this logic present\n\t\t// in order to cancel the context and therefore stop the routine that is spawned.\n\t\t//\n\t\t// For now, DO NOT use client#ContainerWait from the Docker package. There is a nasty\n\t\t// bug causing containers to hang on deletion and cause servers to lock up on the system.\n\t\t//\n\t\t// This weird code isn't intuitive, but it keeps the function from ending until the container\n\t\t// is stopped and therefore the stream reader ends up closed.\n\t\t// @see https://github.com/moby/moby/issues/41827\n\t\tc := new(noopWriter)\n\t\tif _, err := io.Copy(c, e.stream.Reader); err != nil {\n\t\t\te.log().WithField(\"error\", err).Error(\"could not copy from environment stream to noop writer\")\n\t\t}\n\t}()\n\n\treturn nil\n}\n\n// InSituUpdate performs an in-place update of the Docker container's resource\n// limits without actually making any changes to the operational state of the\n// container. This allows memory, cpu, and IO limitations to be adjusted on the\n// fly for individual instances.\nfunc (e *Environment) InSituUpdate() error {\n\tctx, cancel := context.WithTimeout(context.Background(), time.Second*10)\n\tdefer cancel()\n\n\tif _, err := e.client.ContainerInspect(ctx, e.Id); err != nil {\n\t\t// If the container doesn't exist for some reason there really isn't anything\n\t\t// we can do to fix that in this process (it doesn't make sense at least). In those\n\t\t// cases just return without doing anything since we still want to save the configuration\n\t\t// to the disk.\n\t\t//\n\t\t// We'll let a boot process make modifications to the container if needed at this point.\n\t\tif client.IsErrNotFound(err) {\n\t\t\treturn nil\n\t\t}\n\t\treturn errors.Wrap(err, \"environment/docker: could not inspect container\")\n\t}\n\n\t// CPU pinning cannot be removed once it is applied to a container. The same is true\n\t// for removing memory limits, a container must be re-created.\n\t//\n\t// @see https://github.com/moby/moby/issues/41946\n\tif _, err := e.client.ContainerUpdate(ctx, e.Id, container.UpdateConfig{\n\t\tResources: e.resources(),\n\t}); err != nil {\n\t\treturn errors.Wrap(err, \"environment/docker: could not update container\")\n\t}\n\treturn nil\n}\n\n// Create creates a new container for the server using all of the data that is\n// currently available for it. If the container already exists it will be\n// returned.\nfunc (e *Environment) Create() error {\n\t// If the container already exists don't hit the user with an error, just return\n\t// the current information about it which is what we would do when creating the\n\t// container anyways.\n\tif _, err := e.client.ContainerInspect(context.Background(), e.Id); err == nil {\n\t\treturn nil\n\t} else if !client.IsErrNotFound(err) {\n\t\treturn errors.Wrap(err, \"environment/docker: failed to inspect container\")\n\t}\n\n\t// Try to pull the requested image before creating the container.\n\tif err := e.ensureImageExists(e.meta.Image); err != nil {\n\t\treturn errors.WithStackIf(err)\n\t}\n\n\ta := e.Configuration.Allocations()\n\n\tevs := e.Configuration.EnvironmentVariables()\n\tfor i, v := range evs {\n\t\t// Convert 127.0.0.1 to the pterodactyl0 network interface if the environment is Docker\n\t\t// so that the server operates as expected.\n\t\tif v == \"SERVER_IP=127.0.0.1\" {\n\t\t\tevs[i] = \"SERVER_IP=\" + config.Get().Docker.Network.Interface\n\t\t}\n\t}\n\n\tconf := &container.Config{\n\t\tHostname:     e.Id,\n\t\tDomainname:   config.Get().Docker.Domainname,\n\t\tUser:         strconv.Itoa(config.Get().System.User.Uid),\n\t\tAttachStdin:  true,\n\t\tAttachStdout: true,\n\t\tAttachStderr: true,\n\t\tOpenStdin:    true,\n\t\tTty:          true,\n\t\tExposedPorts: a.Exposed(),\n\t\tImage:        strings.TrimPrefix(e.meta.Image, \"~\"),\n\t\tEnv:          e.Configuration.EnvironmentVariables(),\n\t\tLabels: map[string]string{\n\t\t\t\"Service\":       \"Pterodactyl\",\n\t\t\t\"ContainerType\": \"server_process\",\n\t\t},\n\t}\n\n\ttmpfsSize := strconv.Itoa(int(config.Get().Docker.TmpfsSize))\n\n\thostConf := &container.HostConfig{\n\t\tPortBindings: a.DockerBindings(),\n\n\t\t// Configure the mounts for this container. First mount the server data directory\n\t\t// into the container as a r/w bind.\n\t\tMounts: e.convertMounts(),\n\n\t\t// Configure the /tmp folder mapping in containers. This is necessary for some\n\t\t// games that need to make use of it for downloads and other installation processes.\n\t\tTmpfs: map[string]string{\n\t\t\t\"/tmp\": \"rw,exec,nosuid,size=\" + tmpfsSize + \"M\",\n\t\t},\n\n\t\t// Define resource limits for the container based on the data passed through\n\t\t// from the Panel.\n\t\tResources: e.resources(),\n\n\t\tDNS: config.Get().Docker.Network.Dns,\n\n\t\t// Configure logging for the container to make it easier on the Daemon to grab\n\t\t// the server output. Ensure that we don't use too much space on the host machine\n\t\t// since we only need it for the last few hundred lines of output and don't care\n\t\t// about anything else in it.\n\t\tLogConfig: container.LogConfig{\n\t\t\tType: jsonfilelog.Name,\n\t\t\tConfig: map[string]string{\n\t\t\t\t\"max-size\": \"5m\",\n\t\t\t\t\"max-file\": \"1\",\n\t\t\t\t\"compress\": \"false\",\n\t\t\t},\n\t\t},\n\n\t\tSecurityOpt:    []string{\"no-new-privileges\"},\n\t\tReadonlyRootfs: true,\n\t\tCapDrop: []string{\n\t\t\t\"setpcap\", \"mknod\", \"audit_write\", \"net_raw\", \"dac_override\",\n\t\t\t\"fowner\", \"fsetid\", \"net_bind_service\", \"sys_chroot\", \"setfcap\",\n\t\t},\n\t\tNetworkMode: container.NetworkMode(config.Get().Docker.Network.Mode),\n\t}\n\n\tif _, err := e.client.ContainerCreate(context.Background(), conf, hostConf, nil, nil, e.Id); err != nil {\n\t\treturn errors.Wrap(err, \"environment/docker: failed to create container\")\n\t}\n\n\treturn nil\n}\n\n// Destroy will remove the Docker container from the server. If the container\n// is currently running it will be forcibly stopped by Docker.\nfunc (e *Environment) Destroy() error {\n\t// We set it to stopping than offline to prevent crash detection from being triggered.\n\te.SetState(environment.ProcessStoppingState)\n\n\terr := e.client.ContainerRemove(context.Background(), e.Id, types.ContainerRemoveOptions{\n\t\tRemoveVolumes: true,\n\t\tRemoveLinks:   false,\n\t\tForce:         true,\n\t})\n\n\te.SetState(environment.ProcessOfflineState)\n\n\t// Don't trigger a destroy failure if we try to delete a container that does not\n\t// exist on the system. We're just a step ahead of ourselves in that case.\n\t//\n\t// @see https://github.com/pterodactyl/panel/issues/2001\n\tif err != nil && client.IsErrNotFound(err) {\n\t\treturn nil\n\t}\n\n\treturn err\n}\n\n// SendCommand sends the specified command to the stdin of the running container\n// instance. There is no confirmation that this data is sent successfully, only\n// that it gets pushed into the stdin.\nfunc (e *Environment) SendCommand(c string) error {\n\tif !e.IsAttached() {\n\t\treturn errors.Wrap(ErrNotAttached, \"environment/docker: cannot send command to container\")\n\t}\n\n\te.mu.RLock()\n\tdefer e.mu.RUnlock()\n\n\t// If the command being processed is the same as the process stop command then we\n\t// want to mark the server as entering the stopping state otherwise the process will\n\t// stop and Wings will think it has crashed and attempt to restart it.\n\tif e.meta.Stop.Type == \"command\" && c == e.meta.Stop.Value {\n\t\te.SetState(environment.ProcessStoppingState)\n\t}\n\n\t_, err := e.stream.Conn.Write([]byte(c + \"\\n\"))\n\n\treturn errors.Wrap(err, \"environment/docker: could not write to container stream\")\n}\n\n// Readlog reads the log file for the server. This does not care if the server\n// is running or not, it will simply try to read the last X bytes of the file\n// and return them.\nfunc (e *Environment) Readlog(lines int) ([]string, error) {\n\tr, err := e.client.ContainerLogs(context.Background(), e.Id, types.ContainerLogsOptions{\n\t\tShowStdout: true,\n\t\tShowStderr: true,\n\t\tTail:       strconv.Itoa(lines),\n\t})\n\tif err != nil {\n\t\treturn nil, errors.WithStack(err)\n\t}\n\tdefer r.Close()\n\n\tvar out []string\n\tscanner := bufio.NewScanner(r)\n\tfor scanner.Scan() {\n\t\tout = append(out, scanner.Text())\n\t}\n\n\treturn out, nil\n}\n\n// Attaches to the log for the container. This avoids us missing crucial output\n// that happens in the split seconds before the code moves from 'Starting' to\n// 'Attaching' on the process.\nfunc (e *Environment) followOutput() error {\n\tif exists, err := e.Exists(); !exists {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn errors.New(fmt.Sprintf(\"no such container: %s\", e.Id))\n\t}\n\n\topts := types.ContainerLogsOptions{\n\t\tShowStderr: true,\n\t\tShowStdout: true,\n\t\tFollow:     true,\n\t\tSince:      time.Now().Format(time.RFC3339),\n\t}\n\n\treader, err := e.client.ContainerLogs(context.Background(), e.Id, opts)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tgo e.scanOutput(reader)\n\n\treturn nil\n}\n\nfunc (e *Environment) scanOutput(reader io.ReadCloser) {\n\tdefer reader.Close()\n\n\tevents := e.Events()\n\n\terr := system.ScanReader(reader, func(line string) {\n\t\tevents.Publish(environment.ConsoleOutputEvent, line)\n\t})\n\n\tif err != nil && err != io.EOF {\n\t\tlog.WithField(\"error\", err).WithField(\"container_id\", e.Id).Warn(\"error processing scanner line in console output\")\n\t\treturn\n\t}\n\n\t// Return here if the server is offline or currently stopping.\n\tif e.State() == environment.ProcessStoppingState || e.State() == environment.ProcessOfflineState {\n\t\treturn\n\t}\n\n\t// Close the current reader before starting a new one, the defer will still run\n\t// but it will do nothing if we already closed the stream.\n\t_ = reader.Close()\n\n\t// Start following the output of the server again.\n\tgo e.followOutput()\n}\n\ntype imagePullStatus struct {\n\tStatus   string `json:\"status\"`\n\tProgress string `json:\"progress\"`\n}\n\n// Pulls the image from Docker. If there is an error while pulling the image\n// from the source but the image already exists locally, we will report that\n// error to the logger but continue with the process.\n//\n// The reasoning behind this is that Quay has had some serious outages as of\n// late, and we don't need to block all of the servers from booting just because\n// of that. I'd imagine in a lot of cases an outage shouldn't affect users too\n// badly. It'll at least keep existing servers working correctly if anything.\nfunc (e *Environment) ensureImageExists(image string) error {\n\te.Events().Publish(environment.DockerImagePullStarted, \"\")\n\tdefer e.Events().Publish(environment.DockerImagePullCompleted, \"\")\n\n\t// Images prefixed with a ~ are local images that we do not need to try and pull.\n\tif strings.HasPrefix(image, \"~\") {\n\t\treturn nil\n\t}\n\n\t// Give it up to 15 minutes to pull the image. I think this should cover 99.8% of cases where an\n\t// image pull might fail. I can't imagine it will ever take more than 15 minutes to fully pull\n\t// an image. Let me know when I am inevitably wrong here...\n\tctx, cancel := context.WithTimeout(context.Background(), time.Minute*15)\n\tdefer cancel()\n\n\t// Get a registry auth configuration from the config.\n\tvar registryAuth *config.RegistryConfiguration\n\tfor registry, c := range config.Get().Docker.Registries {\n\t\tif !strings.HasPrefix(image, registry) {\n\t\t\tcontinue\n\t\t}\n\n\t\tlog.WithField(\"registry\", registry).Debug(\"using authentication for registry\")\n\t\tregistryAuth = &c\n\t\tbreak\n\t}\n\n\t// Get the ImagePullOptions.\n\timagePullOptions := types.ImagePullOptions{All: false}\n\tif registryAuth != nil {\n\t\tb64, err := registryAuth.Base64()\n\t\tif err != nil {\n\t\t\tlog.WithError(err).Error(\"failed to get registry auth credentials\")\n\t\t}\n\n\t\t// b64 is a string so if there is an error it will just be empty, not nil.\n\t\timagePullOptions.RegistryAuth = b64\n\t}\n\n\tout, err := e.client.ImagePull(ctx, image, imagePullOptions)\n\tif err != nil {\n\t\timages, ierr := e.client.ImageList(ctx, types.ImageListOptions{})\n\t\tif ierr != nil {\n\t\t\t// Well damn, something has gone really wrong here, just go ahead and abort there\n\t\t\t// isn't much anything we can do to try and self-recover from this.\n\t\t\treturn errors.Wrap(ierr, \"environment/docker: failed to list images\")\n\t\t}\n\n\t\tfor _, img := range images {\n\t\t\tfor _, t := range img.RepoTags {\n\t\t\t\tif t != image {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tlog.WithFields(log.Fields{\n\t\t\t\t\t\"image\":        image,\n\t\t\t\t\t\"container_id\": e.Id,\n\t\t\t\t\t\"err\":          err.Error(),\n\t\t\t\t}).Warn(\"unable to pull requested image from remote source, however the image exists locally\")\n\n\t\t\t\t// Okay, we found a matching container image, in that case just go ahead and return\n\t\t\t\t// from this function, since there is nothing else we need to do here.\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\n\t\treturn errors.Wrapf(err, \"environment/docker: failed to pull \\\"%s\\\" image for server\", image)\n\t}\n\tdefer out.Close()\n\n\tlog.WithField(\"image\", image).Debug(\"pulling docker image... this could take a bit of time\")\n\n\t// I'm not sure what the best approach here is, but this will block execution until the image\n\t// is done being pulled, which is what we need.\n\tscanner := bufio.NewScanner(out)\n\n\tfor scanner.Scan() {\n\t\ts := imagePullStatus{}\n\t\tfmt.Println(scanner.Text())\n\n\t\tif err := json.Unmarshal(scanner.Bytes(), &s); err == nil {\n\t\t\te.Events().Publish(environment.DockerImagePullStatus, s.Status+\" \"+s.Progress)\n\t\t}\n\t}\n\n\tif err := scanner.Err(); err != nil {\n\t\treturn err\n\t}\n\n\tlog.WithField(\"image\", image).Debug(\"completed docker image pull\")\n\n\treturn nil\n}\n\nfunc (e *Environment) convertMounts() []mount.Mount {\n\tvar out []mount.Mount\n\n\tfor _, m := range e.Configuration.Mounts() {\n\t\tout = append(out, mount.Mount{\n\t\t\tType:     mount.TypeBind,\n\t\t\tSource:   m.Source,\n\t\t\tTarget:   m.Target,\n\t\t\tReadOnly: m.ReadOnly,\n\t\t})\n\t}\n\n\treturn out\n}\n\nfunc (e *Environment) resources() container.Resources {\n\tl := e.Configuration.Limits()\n\n\treturn container.Resources{\n\t\tMemory:            l.BoundedMemoryLimit(),\n\t\tMemoryReservation: l.MemoryLimit * 1_000_000,\n\t\tMemorySwap:        l.ConvertedSwap(),\n\t\tCPUQuota:          l.ConvertedCpuLimit(),\n\t\tCPUPeriod:         100_000,\n\t\tCPUShares:         1024,\n\t\tBlkioWeight:       l.IoWeight,\n\t\tOomKillDisable:    &l.OOMDisabled,\n\t\tCpusetCpus:        l.Threads,\n\t}\n}\n", "package environment\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"strconv\"\n\n\t\"github.com/apex/log\"\n)\n\ntype Mount struct {\n\t// In Docker environments this makes no difference, however in a non-Docker environment you\n\t// should treat the \"Default\" mount as the root directory for the server. All other mounts\n\t// are just in addition to that one, and generally things like shared maps or timezone data.\n\tDefault bool `json:\"-\"`\n\n\t// The target path on the system. This is \"/home/container\" for all server's Default mount\n\t// but in non-container environments you can likely ignore the target and just work with the\n\t// source.\n\tTarget string `json:\"target\"`\n\n\t// The directory from which the files will be read. In Docker environments this is the directory\n\t// that we're mounting into the container at the Target location.\n\tSource string `json:\"source\"`\n\n\t// Whether or not the directory is being mounted as read-only. It is up to the environment to\n\t// handle this value correctly and ensure security expectations are met with its usage.\n\tReadOnly bool `json:\"read_only\"`\n}\n\n// The build settings for a given server that impact docker container creation and\n// resource limits for a server instance.\ntype Limits struct {\n\t// The total amount of memory in megabytes that this server is allowed to\n\t// use on the host system.\n\tMemoryLimit int64 `json:\"memory_limit\"`\n\n\t// The amount of additional swap space to be provided to a container instance.\n\tSwap int64 `json:\"swap\"`\n\n\t// The relative weight for IO operations in a container. This is relative to other\n\t// containers on the system and should be a value between 10 and 1000.\n\tIoWeight uint16 `json:\"io_weight\"`\n\n\t// The percentage of CPU that this instance is allowed to consume relative to\n\t// the host. A value of 200% represents complete utilization of two cores. This\n\t// should be a value between 1 and THREAD_COUNT * 100.\n\tCpuLimit int64 `json:\"cpu_limit\"`\n\n\t// The amount of disk space in megabytes that a server is allowed to use.\n\tDiskSpace int64 `json:\"disk_space\"`\n\n\t// Sets which CPU threads can be used by the docker instance.\n\tThreads string `json:\"threads\"`\n\n\tOOMDisabled bool `json:\"oom_disabled\"`\n}\n\n// Converts the CPU limit for a server build into a number that can be better understood\n// by the Docker environment. If there is no limit set, return -1 which will indicate to\n// Docker that it has unlimited CPU quota.\nfunc (r *Limits) ConvertedCpuLimit() int64 {\n\tif r.CpuLimit == 0 {\n\t\treturn -1\n\t}\n\n\treturn r.CpuLimit * 1000\n}\n\n// Set the hard limit for memory usage to be 5% more than the amount of memory assigned to\n// the server. If the memory limit for the server is < 4G, use 10%, if less than 2G use\n// 15%. This avoids unexpected crashes from processes like Java which run over the limit.\nfunc (r *Limits) MemoryOverheadMultiplier() float64 {\n\tif r.MemoryLimit <= 2048 {\n\t\treturn 1.15\n\t} else if r.MemoryLimit <= 4096 {\n\t\treturn 1.10\n\t}\n\n\treturn 1.05\n}\n\nfunc (r *Limits) BoundedMemoryLimit() int64 {\n\treturn int64(math.Round(float64(r.MemoryLimit) * r.MemoryOverheadMultiplier() * 1_000_000))\n}\n\n// Returns the amount of swap available as a total in bytes. This is returned as the amount\n// of memory available to the server initially, PLUS the amount of additional swap to include\n// which is the format used by Docker.\nfunc (r *Limits) ConvertedSwap() int64 {\n\tif r.Swap < 0 {\n\t\treturn -1\n\t}\n\n\treturn (r.Swap * 1_000_000) + r.BoundedMemoryLimit()\n}\n\ntype Variables map[string]interface{}\n\n// Ugly hacky function to handle environment variables that get passed through as not-a-string\n// from the Panel. Ideally we'd just say only pass strings, but that is a fragile idea and if a\n// string wasn't passed through you'd cause a crash or the server to become unavailable. For now\n// try to handle the most likely values from the JSON and hope for the best.\nfunc (v Variables) Get(key string) string {\n\tval, ok := v[key]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\n\tswitch val.(type) {\n\tcase int:\n\t\treturn strconv.Itoa(val.(int))\n\tcase int32:\n\t\treturn strconv.FormatInt(val.(int64), 10)\n\tcase int64:\n\t\treturn strconv.FormatInt(val.(int64), 10)\n\tcase float32:\n\t\treturn fmt.Sprintf(\"%f\", val.(float32))\n\tcase float64:\n\t\treturn fmt.Sprintf(\"%f\", val.(float64))\n\tcase bool:\n\t\treturn strconv.FormatBool(val.(bool))\n\tcase string:\n\t\treturn val.(string)\n\t}\n\n\t// TODO: I think we can add a check for val == nil and return an empty string for those\n\t//  and this warning should theoretically never happen?\n\tlog.Warn(fmt.Sprintf(\"failed to marshal environment variable \\\"%s\\\" of type %+v into string\", key, val))\n\n\treturn \"\"\n}\n"], "fixing_code": ["package config\n\nimport (\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\n\t\"github.com/docker/docker/api/types\"\n)\n\ntype dockerNetworkInterfaces struct {\n\tV4 struct {\n\t\tSubnet  string `default:\"172.18.0.0/16\"`\n\t\tGateway string `default:\"172.18.0.1\"`\n\t}\n\tV6 struct {\n\t\tSubnet  string `default:\"fdba:17c8:6c94::/64\"`\n\t\tGateway string `default:\"fdba:17c8:6c94::1011\"`\n\t}\n}\n\ntype DockerNetworkConfiguration struct {\n\t// The interface that should be used to create the network. Must not conflict\n\t// with any other interfaces in use by Docker or on the system.\n\tInterface string `default:\"172.18.0.1\" json:\"interface\" yaml:\"interface\"`\n\n\t// The DNS settings for containers.\n\tDns []string `default:\"[\\\"1.1.1.1\\\", \\\"1.0.0.1\\\"]\"`\n\n\t// The name of the network to use. If this network already exists it will not\n\t// be created. If it is not found, a new network will be created using the interface\n\t// defined.\n\tName       string                  `default:\"pterodactyl_nw\"`\n\tISPN       bool                    `default:\"false\" yaml:\"ispn\"`\n\tDriver     string                  `default:\"bridge\"`\n\tMode       string                  `default:\"pterodactyl_nw\" yaml:\"network_mode\"`\n\tIsInternal bool                    `default:\"false\" yaml:\"is_internal\"`\n\tEnableICC  bool                    `default:\"true\" yaml:\"enable_icc\"`\n\tInterfaces dockerNetworkInterfaces `yaml:\"interfaces\"`\n}\n\n// DockerConfiguration defines the docker configuration used by the daemon when\n// interacting with containers and networks on the system.\ntype DockerConfiguration struct {\n\t// Network configuration that should be used when creating a new network\n\t// for containers run through the daemon.\n\tNetwork DockerNetworkConfiguration `json:\"network\" yaml:\"network\"`\n\n\t// Domainname is the Docker domainname for all containers.\n\tDomainname string `default:\"\" json:\"domainname\" yaml:\"domainname\"`\n\n\t// Registries .\n\tRegistries map[string]RegistryConfiguration `json:\"registries\" yaml:\"registries\"`\n\n\t// The size of the /tmp directory when mounted into a container. Please be aware that Docker\n\t// utilizes host memory for this value, and that we do not keep track of the space used here\n\t// so avoid allocating too much to a server.\n\tTmpfsSize uint `default:\"100\" json:\"tmpfs_size\" yaml:\"tmpfs_size\"`\n\n\t// ContainerPidLimit sets the total number of processes that can be active in a container\n\t// at any given moment. This is a security concern in shared-hosting environments where a\n\t// malicious process could create enough processes to cause the host node to run out of\n\t// available pids and crash.\n\tContainerPidLimit int64 `default:\"256\" json:\"container_pid_limit\" yaml:\"container_pid_limit\"`\n}\n\n// RegistryConfiguration defines the authentication credentials for a given\n// Docker registry.\ntype RegistryConfiguration struct {\n\tUsername string `yaml:\"username\"`\n\tPassword string `yaml:\"password\"`\n}\n\n// Base64 returns the authentication for a given registry as a base64 encoded\n// string value.\nfunc (c RegistryConfiguration) Base64() (string, error) {\n\tb, err := json.Marshal(types.AuthConfig{\n\t\tUsername: c.Username,\n\t\tPassword: c.Password,\n\t})\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn base64.URLEncoding.EncodeToString(b), nil\n}\n", "package docker\n\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"emperror.dev/errors\"\n\t\"github.com/apex/log\"\n\t\"github.com/docker/docker/api/types\"\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/mount\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/docker/docker/daemon/logger/jsonfilelog\"\n\t\"github.com/pterodactyl/wings/config\"\n\t\"github.com/pterodactyl/wings/environment\"\n\t\"github.com/pterodactyl/wings/system\"\n)\n\nvar ErrNotAttached = errors.Sentinel(\"not attached to instance\")\n\n// A custom console writer that allows us to keep a function blocked until the\n// given stream is properly closed. This does nothing special, only exists to\n// make a noop io.Writer.\ntype noopWriter struct{}\n\nvar _ io.Writer = noopWriter{}\n\n// Implement the required Write function to satisfy the io.Writer interface.\nfunc (nw noopWriter) Write(b []byte) (int, error) {\n\treturn len(b), nil\n}\n\n// Attach attaches to the docker container itself and ensures that we can pipe\n// data in and out of the process stream. This should not be used for reading\n// console data as you *will* miss important output at the beginning because of\n// the time delay with attaching to the output.\n//\n// Calling this function will poll resources for the container in the background\n// until the provided context is canceled by the caller. Failure to cancel said\n// context will cause background memory leaks as the goroutine will not exit.\nfunc (e *Environment) Attach() error {\n\tif e.IsAttached() {\n\t\treturn nil\n\t}\n\n\tif err := e.followOutput(); err != nil {\n\t\treturn err\n\t}\n\n\topts := types.ContainerAttachOptions{\n\t\tStdin:  true,\n\t\tStdout: true,\n\t\tStderr: true,\n\t\tStream: true,\n\t}\n\n\t// Set the stream again with the container.\n\tif st, err := e.client.ContainerAttach(context.Background(), e.Id, opts); err != nil {\n\t\treturn err\n\t} else {\n\t\te.SetStream(&st)\n\t}\n\n\tgo func() {\n\t\tctx, cancel := context.WithCancel(context.Background())\n\t\tdefer cancel()\n\t\tdefer e.stream.Close()\n\t\tdefer func() {\n\t\t\te.SetState(environment.ProcessOfflineState)\n\t\t\te.SetStream(nil)\n\t\t}()\n\n\t\tgo func() {\n\t\t\tif err := e.pollResources(ctx); err != nil {\n\t\t\t\tif !errors.Is(err, context.Canceled) {\n\t\t\t\t\te.log().WithField(\"error\", err).Error(\"error during environment resource polling\")\n\t\t\t\t} else {\n\t\t\t\t\te.log().Warn(\"stopping server resource polling: context canceled\")\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\n\t\t// Block the completion of this routine until the container is no longer running. This allows\n\t\t// the pollResources function to run until it needs to be stopped. Because the container\n\t\t// can be polled for resource usage, even when stopped, we need to have this logic present\n\t\t// in order to cancel the context and therefore stop the routine that is spawned.\n\t\t//\n\t\t// For now, DO NOT use client#ContainerWait from the Docker package. There is a nasty\n\t\t// bug causing containers to hang on deletion and cause servers to lock up on the system.\n\t\t//\n\t\t// This weird code isn't intuitive, but it keeps the function from ending until the container\n\t\t// is stopped and therefore the stream reader ends up closed.\n\t\t// @see https://github.com/moby/moby/issues/41827\n\t\tc := new(noopWriter)\n\t\tif _, err := io.Copy(c, e.stream.Reader); err != nil {\n\t\t\te.log().WithField(\"error\", err).Error(\"could not copy from environment stream to noop writer\")\n\t\t}\n\t}()\n\n\treturn nil\n}\n\n// InSituUpdate performs an in-place update of the Docker container's resource\n// limits without actually making any changes to the operational state of the\n// container. This allows memory, cpu, and IO limitations to be adjusted on the\n// fly for individual instances.\nfunc (e *Environment) InSituUpdate() error {\n\tctx, cancel := context.WithTimeout(context.Background(), time.Second*10)\n\tdefer cancel()\n\n\tif _, err := e.client.ContainerInspect(ctx, e.Id); err != nil {\n\t\t// If the container doesn't exist for some reason there really isn't anything\n\t\t// we can do to fix that in this process (it doesn't make sense at least). In those\n\t\t// cases just return without doing anything since we still want to save the configuration\n\t\t// to the disk.\n\t\t//\n\t\t// We'll let a boot process make modifications to the container if needed at this point.\n\t\tif client.IsErrNotFound(err) {\n\t\t\treturn nil\n\t\t}\n\t\treturn errors.Wrap(err, \"environment/docker: could not inspect container\")\n\t}\n\n\t// CPU pinning cannot be removed once it is applied to a container. The same is true\n\t// for removing memory limits, a container must be re-created.\n\t//\n\t// @see https://github.com/moby/moby/issues/41946\n\tif _, err := e.client.ContainerUpdate(ctx, e.Id, container.UpdateConfig{\n\t\tResources: e.resources(),\n\t}); err != nil {\n\t\treturn errors.Wrap(err, \"environment/docker: could not update container\")\n\t}\n\treturn nil\n}\n\n// Create creates a new container for the server using all of the data that is\n// currently available for it. If the container already exists it will be\n// returned.\nfunc (e *Environment) Create() error {\n\t// If the container already exists don't hit the user with an error, just return\n\t// the current information about it which is what we would do when creating the\n\t// container anyways.\n\tif _, err := e.client.ContainerInspect(context.Background(), e.Id); err == nil {\n\t\treturn nil\n\t} else if !client.IsErrNotFound(err) {\n\t\treturn errors.Wrap(err, \"environment/docker: failed to inspect container\")\n\t}\n\n\t// Try to pull the requested image before creating the container.\n\tif err := e.ensureImageExists(e.meta.Image); err != nil {\n\t\treturn errors.WithStackIf(err)\n\t}\n\n\ta := e.Configuration.Allocations()\n\n\tevs := e.Configuration.EnvironmentVariables()\n\tfor i, v := range evs {\n\t\t// Convert 127.0.0.1 to the pterodactyl0 network interface if the environment is Docker\n\t\t// so that the server operates as expected.\n\t\tif v == \"SERVER_IP=127.0.0.1\" {\n\t\t\tevs[i] = \"SERVER_IP=\" + config.Get().Docker.Network.Interface\n\t\t}\n\t}\n\n\tconf := &container.Config{\n\t\tHostname:     e.Id,\n\t\tDomainname:   config.Get().Docker.Domainname,\n\t\tUser:         strconv.Itoa(config.Get().System.User.Uid),\n\t\tAttachStdin:  true,\n\t\tAttachStdout: true,\n\t\tAttachStderr: true,\n\t\tOpenStdin:    true,\n\t\tTty:          true,\n\t\tExposedPorts: a.Exposed(),\n\t\tImage:        strings.TrimPrefix(e.meta.Image, \"~\"),\n\t\tEnv:          e.Configuration.EnvironmentVariables(),\n\t\tLabels: map[string]string{\n\t\t\t\"Service\":       \"Pterodactyl\",\n\t\t\t\"ContainerType\": \"server_process\",\n\t\t},\n\t}\n\n\ttmpfsSize := strconv.Itoa(int(config.Get().Docker.TmpfsSize))\n\n\thostConf := &container.HostConfig{\n\t\tPortBindings: a.DockerBindings(),\n\n\t\t// Configure the mounts for this container. First mount the server data directory\n\t\t// into the container as a r/w bind.\n\t\tMounts: e.convertMounts(),\n\n\t\t// Configure the /tmp folder mapping in containers. This is necessary for some\n\t\t// games that need to make use of it for downloads and other installation processes.\n\t\tTmpfs: map[string]string{\n\t\t\t\"/tmp\": \"rw,exec,nosuid,size=\" + tmpfsSize + \"M\",\n\t\t},\n\n\t\t// Define resource limits for the container based on the data passed through\n\t\t// from the Panel.\n\t\tResources: e.resources(),\n\n\t\tDNS: config.Get().Docker.Network.Dns,\n\n\t\t// Configure logging for the container to make it easier on the Daemon to grab\n\t\t// the server output. Ensure that we don't use too much space on the host machine\n\t\t// since we only need it for the last few hundred lines of output and don't care\n\t\t// about anything else in it.\n\t\tLogConfig: container.LogConfig{\n\t\t\tType: jsonfilelog.Name,\n\t\t\tConfig: map[string]string{\n\t\t\t\t\"max-size\": \"5m\",\n\t\t\t\t\"max-file\": \"1\",\n\t\t\t\t\"compress\": \"false\",\n\t\t\t},\n\t\t},\n\n\t\tSecurityOpt:    []string{\"no-new-privileges\"},\n\t\tReadonlyRootfs: true,\n\t\tCapDrop: []string{\n\t\t\t\"setpcap\", \"mknod\", \"audit_write\", \"net_raw\", \"dac_override\",\n\t\t\t\"fowner\", \"fsetid\", \"net_bind_service\", \"sys_chroot\", \"setfcap\",\n\t\t},\n\t\tNetworkMode: container.NetworkMode(config.Get().Docker.Network.Mode),\n\t}\n\n\tif _, err := e.client.ContainerCreate(context.Background(), conf, hostConf, nil, nil, e.Id); err != nil {\n\t\treturn errors.Wrap(err, \"environment/docker: failed to create container\")\n\t}\n\n\treturn nil\n}\n\n// Destroy will remove the Docker container from the server. If the container\n// is currently running it will be forcibly stopped by Docker.\nfunc (e *Environment) Destroy() error {\n\t// We set it to stopping than offline to prevent crash detection from being triggered.\n\te.SetState(environment.ProcessStoppingState)\n\n\terr := e.client.ContainerRemove(context.Background(), e.Id, types.ContainerRemoveOptions{\n\t\tRemoveVolumes: true,\n\t\tRemoveLinks:   false,\n\t\tForce:         true,\n\t})\n\n\te.SetState(environment.ProcessOfflineState)\n\n\t// Don't trigger a destroy failure if we try to delete a container that does not\n\t// exist on the system. We're just a step ahead of ourselves in that case.\n\t//\n\t// @see https://github.com/pterodactyl/panel/issues/2001\n\tif err != nil && client.IsErrNotFound(err) {\n\t\treturn nil\n\t}\n\n\treturn err\n}\n\n// SendCommand sends the specified command to the stdin of the running container\n// instance. There is no confirmation that this data is sent successfully, only\n// that it gets pushed into the stdin.\nfunc (e *Environment) SendCommand(c string) error {\n\tif !e.IsAttached() {\n\t\treturn errors.Wrap(ErrNotAttached, \"environment/docker: cannot send command to container\")\n\t}\n\n\te.mu.RLock()\n\tdefer e.mu.RUnlock()\n\n\t// If the command being processed is the same as the process stop command then we\n\t// want to mark the server as entering the stopping state otherwise the process will\n\t// stop and Wings will think it has crashed and attempt to restart it.\n\tif e.meta.Stop.Type == \"command\" && c == e.meta.Stop.Value {\n\t\te.SetState(environment.ProcessStoppingState)\n\t}\n\n\t_, err := e.stream.Conn.Write([]byte(c + \"\\n\"))\n\n\treturn errors.Wrap(err, \"environment/docker: could not write to container stream\")\n}\n\n// Readlog reads the log file for the server. This does not care if the server\n// is running or not, it will simply try to read the last X bytes of the file\n// and return them.\nfunc (e *Environment) Readlog(lines int) ([]string, error) {\n\tr, err := e.client.ContainerLogs(context.Background(), e.Id, types.ContainerLogsOptions{\n\t\tShowStdout: true,\n\t\tShowStderr: true,\n\t\tTail:       strconv.Itoa(lines),\n\t})\n\tif err != nil {\n\t\treturn nil, errors.WithStack(err)\n\t}\n\tdefer r.Close()\n\n\tvar out []string\n\tscanner := bufio.NewScanner(r)\n\tfor scanner.Scan() {\n\t\tout = append(out, scanner.Text())\n\t}\n\n\treturn out, nil\n}\n\n// Attaches to the log for the container. This avoids us missing crucial output\n// that happens in the split seconds before the code moves from 'Starting' to\n// 'Attaching' on the process.\nfunc (e *Environment) followOutput() error {\n\tif exists, err := e.Exists(); !exists {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn errors.New(fmt.Sprintf(\"no such container: %s\", e.Id))\n\t}\n\n\topts := types.ContainerLogsOptions{\n\t\tShowStderr: true,\n\t\tShowStdout: true,\n\t\tFollow:     true,\n\t\tSince:      time.Now().Format(time.RFC3339),\n\t}\n\n\treader, err := e.client.ContainerLogs(context.Background(), e.Id, opts)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tgo e.scanOutput(reader)\n\n\treturn nil\n}\n\nfunc (e *Environment) scanOutput(reader io.ReadCloser) {\n\tdefer reader.Close()\n\n\tevents := e.Events()\n\n\terr := system.ScanReader(reader, func(line string) {\n\t\tevents.Publish(environment.ConsoleOutputEvent, line)\n\t})\n\n\tif err != nil && err != io.EOF {\n\t\tlog.WithField(\"error\", err).WithField(\"container_id\", e.Id).Warn(\"error processing scanner line in console output\")\n\t\treturn\n\t}\n\n\t// Return here if the server is offline or currently stopping.\n\tif e.State() == environment.ProcessStoppingState || e.State() == environment.ProcessOfflineState {\n\t\treturn\n\t}\n\n\t// Close the current reader before starting a new one, the defer will still run\n\t// but it will do nothing if we already closed the stream.\n\t_ = reader.Close()\n\n\t// Start following the output of the server again.\n\tgo e.followOutput()\n}\n\ntype imagePullStatus struct {\n\tStatus   string `json:\"status\"`\n\tProgress string `json:\"progress\"`\n}\n\n// Pulls the image from Docker. If there is an error while pulling the image\n// from the source but the image already exists locally, we will report that\n// error to the logger but continue with the process.\n//\n// The reasoning behind this is that Quay has had some serious outages as of\n// late, and we don't need to block all of the servers from booting just because\n// of that. I'd imagine in a lot of cases an outage shouldn't affect users too\n// badly. It'll at least keep existing servers working correctly if anything.\nfunc (e *Environment) ensureImageExists(image string) error {\n\te.Events().Publish(environment.DockerImagePullStarted, \"\")\n\tdefer e.Events().Publish(environment.DockerImagePullCompleted, \"\")\n\n\t// Images prefixed with a ~ are local images that we do not need to try and pull.\n\tif strings.HasPrefix(image, \"~\") {\n\t\treturn nil\n\t}\n\n\t// Give it up to 15 minutes to pull the image. I think this should cover 99.8% of cases where an\n\t// image pull might fail. I can't imagine it will ever take more than 15 minutes to fully pull\n\t// an image. Let me know when I am inevitably wrong here...\n\tctx, cancel := context.WithTimeout(context.Background(), time.Minute*15)\n\tdefer cancel()\n\n\t// Get a registry auth configuration from the config.\n\tvar registryAuth *config.RegistryConfiguration\n\tfor registry, c := range config.Get().Docker.Registries {\n\t\tif !strings.HasPrefix(image, registry) {\n\t\t\tcontinue\n\t\t}\n\n\t\tlog.WithField(\"registry\", registry).Debug(\"using authentication for registry\")\n\t\tregistryAuth = &c\n\t\tbreak\n\t}\n\n\t// Get the ImagePullOptions.\n\timagePullOptions := types.ImagePullOptions{All: false}\n\tif registryAuth != nil {\n\t\tb64, err := registryAuth.Base64()\n\t\tif err != nil {\n\t\t\tlog.WithError(err).Error(\"failed to get registry auth credentials\")\n\t\t}\n\n\t\t// b64 is a string so if there is an error it will just be empty, not nil.\n\t\timagePullOptions.RegistryAuth = b64\n\t}\n\n\tout, err := e.client.ImagePull(ctx, image, imagePullOptions)\n\tif err != nil {\n\t\timages, ierr := e.client.ImageList(ctx, types.ImageListOptions{})\n\t\tif ierr != nil {\n\t\t\t// Well damn, something has gone really wrong here, just go ahead and abort there\n\t\t\t// isn't much anything we can do to try and self-recover from this.\n\t\t\treturn errors.Wrap(ierr, \"environment/docker: failed to list images\")\n\t\t}\n\n\t\tfor _, img := range images {\n\t\t\tfor _, t := range img.RepoTags {\n\t\t\t\tif t != image {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tlog.WithFields(log.Fields{\n\t\t\t\t\t\"image\":        image,\n\t\t\t\t\t\"container_id\": e.Id,\n\t\t\t\t\t\"err\":          err.Error(),\n\t\t\t\t}).Warn(\"unable to pull requested image from remote source, however the image exists locally\")\n\n\t\t\t\t// Okay, we found a matching container image, in that case just go ahead and return\n\t\t\t\t// from this function, since there is nothing else we need to do here.\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\n\t\treturn errors.Wrapf(err, \"environment/docker: failed to pull \\\"%s\\\" image for server\", image)\n\t}\n\tdefer out.Close()\n\n\tlog.WithField(\"image\", image).Debug(\"pulling docker image... this could take a bit of time\")\n\n\t// I'm not sure what the best approach here is, but this will block execution until the image\n\t// is done being pulled, which is what we need.\n\tscanner := bufio.NewScanner(out)\n\n\tfor scanner.Scan() {\n\t\ts := imagePullStatus{}\n\t\tfmt.Println(scanner.Text())\n\n\t\tif err := json.Unmarshal(scanner.Bytes(), &s); err == nil {\n\t\t\te.Events().Publish(environment.DockerImagePullStatus, s.Status+\" \"+s.Progress)\n\t\t}\n\t}\n\n\tif err := scanner.Err(); err != nil {\n\t\treturn err\n\t}\n\n\tlog.WithField(\"image\", image).Debug(\"completed docker image pull\")\n\n\treturn nil\n}\n\nfunc (e *Environment) convertMounts() []mount.Mount {\n\tvar out []mount.Mount\n\n\tfor _, m := range e.Configuration.Mounts() {\n\t\tout = append(out, mount.Mount{\n\t\t\tType:     mount.TypeBind,\n\t\t\tSource:   m.Source,\n\t\t\tTarget:   m.Target,\n\t\t\tReadOnly: m.ReadOnly,\n\t\t})\n\t}\n\n\treturn out\n}\n\nfunc (e *Environment) resources() container.Resources {\n\tl := e.Configuration.Limits()\n\tpids := l.ProcessLimit()\n\n\treturn container.Resources{\n\t\tMemory:            l.BoundedMemoryLimit(),\n\t\tMemoryReservation: l.MemoryLimit * 1_000_000,\n\t\tMemorySwap:        l.ConvertedSwap(),\n\t\tCPUQuota:          l.ConvertedCpuLimit(),\n\t\tCPUPeriod:         100_000,\n\t\tCPUShares:         1024,\n\t\tBlkioWeight:       l.IoWeight,\n\t\tOomKillDisable:    &l.OOMDisabled,\n\t\tCpusetCpus:        l.Threads,\n\t\tPidsLimit:         &pids,\n\t}\n}\n", "package environment\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"strconv\"\n\n\t\"github.com/apex/log\"\n\t\"github.com/pterodactyl/wings/config\"\n)\n\ntype Mount struct {\n\t// In Docker environments this makes no difference, however in a non-Docker environment you\n\t// should treat the \"Default\" mount as the root directory for the server. All other mounts\n\t// are just in addition to that one, and generally things like shared maps or timezone data.\n\tDefault bool `json:\"-\"`\n\n\t// The target path on the system. This is \"/home/container\" for all server's Default mount\n\t// but in non-container environments you can likely ignore the target and just work with the\n\t// source.\n\tTarget string `json:\"target\"`\n\n\t// The directory from which the files will be read. In Docker environments this is the directory\n\t// that we're mounting into the container at the Target location.\n\tSource string `json:\"source\"`\n\n\t// Whether or not the directory is being mounted as read-only. It is up to the environment to\n\t// handle this value correctly and ensure security expectations are met with its usage.\n\tReadOnly bool `json:\"read_only\"`\n}\n\n// Limits is the build settings for a given server that impact docker container\n// creation and resource limits for a server instance.\ntype Limits struct {\n\t// The total amount of memory in megabytes that this server is allowed to\n\t// use on the host system.\n\tMemoryLimit int64 `json:\"memory_limit\"`\n\n\t// The amount of additional swap space to be provided to a container instance.\n\tSwap int64 `json:\"swap\"`\n\n\t// The relative weight for IO operations in a container. This is relative to other\n\t// containers on the system and should be a value between 10 and 1000.\n\tIoWeight uint16 `json:\"io_weight\"`\n\n\t// The percentage of CPU that this instance is allowed to consume relative to\n\t// the host. A value of 200% represents complete utilization of two cores. This\n\t// should be a value between 1 and THREAD_COUNT * 100.\n\tCpuLimit int64 `json:\"cpu_limit\"`\n\n\t// The amount of disk space in megabytes that a server is allowed to use.\n\tDiskSpace int64 `json:\"disk_space\"`\n\n\t// Sets which CPU threads can be used by the docker instance.\n\tThreads string `json:\"threads\"`\n\n\tOOMDisabled bool `json:\"oom_disabled\"`\n}\n\n// ConvertedCpuLimit converts the CPU limit for a server build into a number\n// that can be better understood by the Docker environment. If there is no limit\n// set, return -1 which will indicate to Docker that it has unlimited CPU quota.\nfunc (r *Limits) ConvertedCpuLimit() int64 {\n\tif r.CpuLimit == 0 {\n\t\treturn -1\n\t}\n\n\treturn r.CpuLimit * 1000\n}\n\n// MemoryOverheadMultiplier sets the hard limit for memory usage to be 5% more\n// than the amount of memory assigned to the server. If the memory limit for the\n// server is < 4G, use 10%, if less than 2G use 15%. This avoids unexpected\n// crashes from processes like Java which run over the limit.\nfunc (r *Limits) MemoryOverheadMultiplier() float64 {\n\tif r.MemoryLimit <= 2048 {\n\t\treturn 1.15\n\t} else if r.MemoryLimit <= 4096 {\n\t\treturn 1.10\n\t}\n\n\treturn 1.05\n}\n\nfunc (r *Limits) BoundedMemoryLimit() int64 {\n\treturn int64(math.Round(float64(r.MemoryLimit) * r.MemoryOverheadMultiplier() * 1_000_000))\n}\n\n// ConvertedSwap returns the amount of swap available as a total in bytes. This\n// is returned as the amount of memory available to the server initially, PLUS\n// the amount of additional swap to include which is the format used by Docker.\nfunc (r *Limits) ConvertedSwap() int64 {\n\tif r.Swap < 0 {\n\t\treturn -1\n\t}\n\n\treturn (r.Swap * 1_000_000) + r.BoundedMemoryLimit()\n}\n\n// ProcessLimit returns the process limit for a container. This is currently\n// defined at a system level and not on a per-server basis.\nfunc (r *Limits) ProcessLimit() int64 {\n\treturn config.Get().Docker.ContainerPidLimit\n}\n\ntype Variables map[string]interface{}\n\n// Get is an ugly hacky function to handle environment variables that get passed\n// through as not-a-string from the Panel. Ideally we'd just say only pass\n// strings, but that is a fragile idea and if a string wasn't passed through\n// you'd cause a crash or the server to become unavailable. For now try to\n// handle the most likely values from the JSON and hope for the best.\nfunc (v Variables) Get(key string) string {\n\tval, ok := v[key]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\n\tswitch val.(type) {\n\tcase int:\n\t\treturn strconv.Itoa(val.(int))\n\tcase int32:\n\t\treturn strconv.FormatInt(val.(int64), 10)\n\tcase int64:\n\t\treturn strconv.FormatInt(val.(int64), 10)\n\tcase float32:\n\t\treturn fmt.Sprintf(\"%f\", val.(float32))\n\tcase float64:\n\t\treturn fmt.Sprintf(\"%f\", val.(float64))\n\tcase bool:\n\t\treturn strconv.FormatBool(val.(bool))\n\tcase string:\n\t\treturn val.(string)\n\t}\n\n\t// TODO: I think we can add a check for val == nil and return an empty string for those\n\t//  and this warning should theoretically never happen?\n\tlog.Warn(fmt.Sprintf(\"failed to marshal environment variable \\\"%s\\\" of type %+v into string\", key, val))\n\n\treturn \"\"\n}\n"], "filenames": ["config/config_docker.go", "environment/docker/container.go", "environment/settings.go"], "buggy_code_start_loc": [57, 488, 8], "buggy_code_end_loc": [57, 502, 104], "fixing_code_start_loc": [58, 489, 9], "fixing_code_end_loc": [64, 504, 113], "type": "CWE-770", "message": "Wings is the control plane software for the open source Pterodactyl game management system. All versions of Pterodactyl Wings prior to `1.4.4` are vulnerable to system resource exhaustion due to improper container process limits being defined. A malicious user can consume more resources than intended and cause downstream impacts to other clients on the same hardware, eventually causing the physical server to stop responding. Users should upgrade to `1.4.4` to mitigate the issue. There is no non-code based workaround for impacted versions of the software. Users running customized versions of this software can manually set a PID limit for containers created.", "other": {"cve": {"id": "CVE-2021-32699", "sourceIdentifier": "security-advisories@github.com", "published": "2021-06-22T20:15:08.560", "lastModified": "2022-10-25T15:26:36.537", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Wings is the control plane software for the open source Pterodactyl game management system. All versions of Pterodactyl Wings prior to `1.4.4` are vulnerable to system resource exhaustion due to improper container process limits being defined. A malicious user can consume more resources than intended and cause downstream impacts to other clients on the same hardware, eventually causing the physical server to stop responding. Users should upgrade to `1.4.4` to mitigate the issue. There is no non-code based workaround for impacted versions of the software. Users running customized versions of this software can manually set a PID limit for containers created."}, {"lang": "es", "value": "Wings es el software del plano de control del sistema de administraci\u00f3n de juegos de c\u00f3digo abierto Pterodactyl. Todas las versiones de Pterodactyl Wings anteriores a \"1.4.4\" son vulnerables al agotamiento de los recursos del sistema debido a la definici\u00f3n inapropiada de los l\u00edmites de los procesos de los contenedores. Un usuario malicioso puede consumir m\u00e1s recursos de los previstos y causar impactos posteriores a otros clientes en el mismo hardware, causando eventualmente que el servidor f\u00edsico deje de responder. Los usuarios deben actualizar a la versi\u00f3n \"1.4.4\" para mitigar el problema. No existe ninguna soluci\u00f3n no basada en c\u00f3digo para las versiones afectadas del software. Los usuarios que ejecutan versiones personalizadas de este software pueden establecer manualmente un l\u00edmite de PID para los contenedores creados"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.0, "impactScore": 4.0}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.0, "impactScore": 4.0}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-770"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-400"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:pterodactyl:wings:*:*:*:*:*:*:*:*", "versionEndExcluding": "1.4.4", "matchCriteriaId": "3BEEE05E-C3DE-42BF-89F3-A0629A5847EC"}]}]}], "references": [{"url": "https://github.com/pterodactyl/wings/commit/e0078eee0a71d61573a94c75e6efcad069d78de3", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/pterodactyl/wings/security/advisories/GHSA-jj6m-r8jc-2gp7", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/pterodactyl/wings/commit/e0078eee0a71d61573a94c75e6efcad069d78de3"}}