{"buggy_code": ["/*\n * Copyright \u00a9 2014 Broadcom\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice (including the next\n * paragraph) shall be included in all copies or substantial portions of the\n * Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n * IN THE SOFTWARE.\n */\n\n#include <linux/module.h>\n#include <linux/platform_device.h>\n#include <linux/pm_runtime.h>\n#include <linux/device.h>\n#include <linux/io.h>\n\n#include \"uapi/drm/vc4_drm.h\"\n#include \"vc4_drv.h\"\n#include \"vc4_regs.h\"\n#include \"vc4_trace.h\"\n\nstatic void\nvc4_queue_hangcheck(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\tmod_timer(&vc4->hangcheck.timer,\n\t\t  round_jiffies_up(jiffies + msecs_to_jiffies(100)));\n}\n\nstruct vc4_hang_state {\n\tstruct drm_vc4_get_hang_state user_state;\n\n\tu32 bo_count;\n\tstruct drm_gem_object **bo;\n};\n\nstatic void\nvc4_free_hang_state(struct drm_device *dev, struct vc4_hang_state *state)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < state->user_state.bo_count; i++)\n\t\tdrm_gem_object_unreference_unlocked(state->bo[i]);\n\n\tkfree(state);\n}\n\nint\nvc4_get_hang_state_ioctl(struct drm_device *dev, void *data,\n\t\t\t struct drm_file *file_priv)\n{\n\tstruct drm_vc4_get_hang_state *get_state = data;\n\tstruct drm_vc4_get_hang_state_bo *bo_state;\n\tstruct vc4_hang_state *kernel_state;\n\tstruct drm_vc4_get_hang_state *state;\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tunsigned long irqflags;\n\tu32 i;\n\tint ret = 0;\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\tkernel_state = vc4->hang_state;\n\tif (!kernel_state) {\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\treturn -ENOENT;\n\t}\n\tstate = &kernel_state->user_state;\n\n\t/* If the user's array isn't big enough, just return the\n\t * required array size.\n\t */\n\tif (get_state->bo_count < state->bo_count) {\n\t\tget_state->bo_count = state->bo_count;\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\treturn 0;\n\t}\n\n\tvc4->hang_state = NULL;\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\n\t/* Save the user's BO pointer, so we don't stomp it with the memcpy. */\n\tstate->bo = get_state->bo;\n\tmemcpy(get_state, state, sizeof(*state));\n\n\tbo_state = kcalloc(state->bo_count, sizeof(*bo_state), GFP_KERNEL);\n\tif (!bo_state) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free;\n\t}\n\n\tfor (i = 0; i < state->bo_count; i++) {\n\t\tstruct vc4_bo *vc4_bo = to_vc4_bo(kernel_state->bo[i]);\n\t\tu32 handle;\n\n\t\tret = drm_gem_handle_create(file_priv, kernel_state->bo[i],\n\t\t\t\t\t    &handle);\n\n\t\tif (ret) {\n\t\t\tstate->bo_count = i - 1;\n\t\t\tgoto err;\n\t\t}\n\t\tbo_state[i].handle = handle;\n\t\tbo_state[i].paddr = vc4_bo->base.paddr;\n\t\tbo_state[i].size = vc4_bo->base.base.size;\n\t}\n\n\tif (copy_to_user((void __user *)(uintptr_t)get_state->bo,\n\t\t\t bo_state,\n\t\t\t state->bo_count * sizeof(*bo_state)))\n\t\tret = -EFAULT;\n\n\tkfree(bo_state);\n\nerr_free:\n\n\tvc4_free_hang_state(dev, kernel_state);\n\nerr:\n\treturn ret;\n}\n\nstatic void\nvc4_save_hang_state(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct drm_vc4_get_hang_state *state;\n\tstruct vc4_hang_state *kernel_state;\n\tstruct vc4_exec_info *exec[2];\n\tstruct vc4_bo *bo;\n\tunsigned long irqflags;\n\tunsigned int i, j, unref_list_count, prev_idx;\n\n\tkernel_state = kcalloc(1, sizeof(*kernel_state), GFP_KERNEL);\n\tif (!kernel_state)\n\t\treturn;\n\n\tstate = &kernel_state->user_state;\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\texec[0] = vc4_first_bin_job(vc4);\n\texec[1] = vc4_first_render_job(vc4);\n\tif (!exec[0] && !exec[1]) {\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\treturn;\n\t}\n\n\t/* Get the bos from both binner and renderer into hang state. */\n\tstate->bo_count = 0;\n\tfor (i = 0; i < 2; i++) {\n\t\tif (!exec[i])\n\t\t\tcontinue;\n\n\t\tunref_list_count = 0;\n\t\tlist_for_each_entry(bo, &exec[i]->unref_list, unref_head)\n\t\t\tunref_list_count++;\n\t\tstate->bo_count += exec[i]->bo_count + unref_list_count;\n\t}\n\n\tkernel_state->bo = kcalloc(state->bo_count,\n\t\t\t\t   sizeof(*kernel_state->bo), GFP_ATOMIC);\n\n\tif (!kernel_state->bo) {\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\treturn;\n\t}\n\n\tprev_idx = 0;\n\tfor (i = 0; i < 2; i++) {\n\t\tif (!exec[i])\n\t\t\tcontinue;\n\n\t\tfor (j = 0; j < exec[i]->bo_count; j++) {\n\t\t\tdrm_gem_object_reference(&exec[i]->bo[j]->base);\n\t\t\tkernel_state->bo[j + prev_idx] = &exec[i]->bo[j]->base;\n\t\t}\n\n\t\tlist_for_each_entry(bo, &exec[i]->unref_list, unref_head) {\n\t\t\tdrm_gem_object_reference(&bo->base.base);\n\t\t\tkernel_state->bo[j + prev_idx] = &bo->base.base;\n\t\t\tj++;\n\t\t}\n\t\tprev_idx = j + 1;\n\t}\n\n\tif (exec[0])\n\t\tstate->start_bin = exec[0]->ct0ca;\n\tif (exec[1])\n\t\tstate->start_render = exec[1]->ct1ca;\n\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\n\tstate->ct0ca = V3D_READ(V3D_CTNCA(0));\n\tstate->ct0ea = V3D_READ(V3D_CTNEA(0));\n\n\tstate->ct1ca = V3D_READ(V3D_CTNCA(1));\n\tstate->ct1ea = V3D_READ(V3D_CTNEA(1));\n\n\tstate->ct0cs = V3D_READ(V3D_CTNCS(0));\n\tstate->ct1cs = V3D_READ(V3D_CTNCS(1));\n\n\tstate->ct0ra0 = V3D_READ(V3D_CT00RA0);\n\tstate->ct1ra0 = V3D_READ(V3D_CT01RA0);\n\n\tstate->bpca = V3D_READ(V3D_BPCA);\n\tstate->bpcs = V3D_READ(V3D_BPCS);\n\tstate->bpoa = V3D_READ(V3D_BPOA);\n\tstate->bpos = V3D_READ(V3D_BPOS);\n\n\tstate->vpmbase = V3D_READ(V3D_VPMBASE);\n\n\tstate->dbge = V3D_READ(V3D_DBGE);\n\tstate->fdbgo = V3D_READ(V3D_FDBGO);\n\tstate->fdbgb = V3D_READ(V3D_FDBGB);\n\tstate->fdbgr = V3D_READ(V3D_FDBGR);\n\tstate->fdbgs = V3D_READ(V3D_FDBGS);\n\tstate->errstat = V3D_READ(V3D_ERRSTAT);\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\tif (vc4->hang_state) {\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\tvc4_free_hang_state(dev, kernel_state);\n\t} else {\n\t\tvc4->hang_state = kernel_state;\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t}\n}\n\nstatic void\nvc4_reset(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\tDRM_INFO(\"Resetting GPU.\\n\");\n\n\tmutex_lock(&vc4->power_lock);\n\tif (vc4->power_refcount) {\n\t\t/* Power the device off and back on the by dropping the\n\t\t * reference on runtime PM.\n\t\t */\n\t\tpm_runtime_put_sync_suspend(&vc4->v3d->pdev->dev);\n\t\tpm_runtime_get_sync(&vc4->v3d->pdev->dev);\n\t}\n\tmutex_unlock(&vc4->power_lock);\n\n\tvc4_irq_reset(dev);\n\n\t/* Rearm the hangcheck -- another job might have been waiting\n\t * for our hung one to get kicked off, and vc4_irq_reset()\n\t * would have started it.\n\t */\n\tvc4_queue_hangcheck(dev);\n}\n\nstatic void\nvc4_reset_work(struct work_struct *work)\n{\n\tstruct vc4_dev *vc4 =\n\t\tcontainer_of(work, struct vc4_dev, hangcheck.reset_work);\n\n\tvc4_save_hang_state(vc4->dev);\n\n\tvc4_reset(vc4->dev);\n}\n\nstatic void\nvc4_hangcheck_elapsed(unsigned long data)\n{\n\tstruct drm_device *dev = (struct drm_device *)data;\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tuint32_t ct0ca, ct1ca;\n\tunsigned long irqflags;\n\tstruct vc4_exec_info *bin_exec, *render_exec;\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\n\tbin_exec = vc4_first_bin_job(vc4);\n\trender_exec = vc4_first_render_job(vc4);\n\n\t/* If idle, we can stop watching for hangs. */\n\tif (!bin_exec && !render_exec) {\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\treturn;\n\t}\n\n\tct0ca = V3D_READ(V3D_CTNCA(0));\n\tct1ca = V3D_READ(V3D_CTNCA(1));\n\n\t/* If we've made any progress in execution, rearm the timer\n\t * and wait.\n\t */\n\tif ((bin_exec && ct0ca != bin_exec->last_ct0ca) ||\n\t    (render_exec && ct1ca != render_exec->last_ct1ca)) {\n\t\tif (bin_exec)\n\t\t\tbin_exec->last_ct0ca = ct0ca;\n\t\tif (render_exec)\n\t\t\trender_exec->last_ct1ca = ct1ca;\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\tvc4_queue_hangcheck(dev);\n\t\treturn;\n\t}\n\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\n\t/* We've gone too long with no progress, reset.  This has to\n\t * be done from a work struct, since resetting can sleep and\n\t * this timer hook isn't allowed to.\n\t */\n\tschedule_work(&vc4->hangcheck.reset_work);\n}\n\nstatic void\nsubmit_cl(struct drm_device *dev, uint32_t thread, uint32_t start, uint32_t end)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\t/* Set the current and end address of the control list.\n\t * Writing the end register is what starts the job.\n\t */\n\tV3D_WRITE(V3D_CTNCA(thread), start);\n\tV3D_WRITE(V3D_CTNEA(thread), end);\n}\n\nint\nvc4_wait_for_seqno(struct drm_device *dev, uint64_t seqno, uint64_t timeout_ns,\n\t\t   bool interruptible)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tint ret = 0;\n\tunsigned long timeout_expire;\n\tDEFINE_WAIT(wait);\n\n\tif (vc4->finished_seqno >= seqno)\n\t\treturn 0;\n\n\tif (timeout_ns == 0)\n\t\treturn -ETIME;\n\n\ttimeout_expire = jiffies + nsecs_to_jiffies(timeout_ns);\n\n\ttrace_vc4_wait_for_seqno_begin(dev, seqno, timeout_ns);\n\tfor (;;) {\n\t\tprepare_to_wait(&vc4->job_wait_queue, &wait,\n\t\t\t\tinterruptible ? TASK_INTERRUPTIBLE :\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\n\t\tif (interruptible && signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (vc4->finished_seqno >= seqno)\n\t\t\tbreak;\n\n\t\tif (timeout_ns != ~0ull) {\n\t\t\tif (time_after_eq(jiffies, timeout_expire)) {\n\t\t\t\tret = -ETIME;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tschedule_timeout(timeout_expire - jiffies);\n\t\t} else {\n\t\t\tschedule();\n\t\t}\n\t}\n\n\tfinish_wait(&vc4->job_wait_queue, &wait);\n\ttrace_vc4_wait_for_seqno_end(dev, seqno);\n\n\treturn ret;\n}\n\nstatic void\nvc4_flush_caches(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\t/* Flush the GPU L2 caches.  These caches sit on top of system\n\t * L3 (the 128kb or so shared with the CPU), and are\n\t * non-allocating in the L3.\n\t */\n\tV3D_WRITE(V3D_L2CACTL,\n\t\t  V3D_L2CACTL_L2CCLR);\n\n\tV3D_WRITE(V3D_SLCACTL,\n\t\t  VC4_SET_FIELD(0xf, V3D_SLCACTL_T1CC) |\n\t\t  VC4_SET_FIELD(0xf, V3D_SLCACTL_T0CC) |\n\t\t  VC4_SET_FIELD(0xf, V3D_SLCACTL_UCC) |\n\t\t  VC4_SET_FIELD(0xf, V3D_SLCACTL_ICC));\n}\n\n/* Sets the registers for the next job to be actually be executed in\n * the hardware.\n *\n * The job_lock should be held during this.\n */\nvoid\nvc4_submit_next_bin_job(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct vc4_exec_info *exec;\n\nagain:\n\texec = vc4_first_bin_job(vc4);\n\tif (!exec)\n\t\treturn;\n\n\tvc4_flush_caches(dev);\n\n\t/* Either put the job in the binner if it uses the binner, or\n\t * immediately move it to the to-be-rendered queue.\n\t */\n\tif (exec->ct0ca != exec->ct0ea) {\n\t\tsubmit_cl(dev, 0, exec->ct0ca, exec->ct0ea);\n\t} else {\n\t\tvc4_move_job_to_render(dev, exec);\n\t\tgoto again;\n\t}\n}\n\nvoid\nvc4_submit_next_render_job(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct vc4_exec_info *exec = vc4_first_render_job(vc4);\n\n\tif (!exec)\n\t\treturn;\n\n\tsubmit_cl(dev, 1, exec->ct1ca, exec->ct1ea);\n}\n\nvoid\nvc4_move_job_to_render(struct drm_device *dev, struct vc4_exec_info *exec)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tbool was_empty = list_empty(&vc4->render_job_list);\n\n\tlist_move_tail(&exec->head, &vc4->render_job_list);\n\tif (was_empty)\n\t\tvc4_submit_next_render_job(dev);\n}\n\nstatic void\nvc4_update_bo_seqnos(struct vc4_exec_info *exec, uint64_t seqno)\n{\n\tstruct vc4_bo *bo;\n\tunsigned i;\n\n\tfor (i = 0; i < exec->bo_count; i++) {\n\t\tbo = to_vc4_bo(&exec->bo[i]->base);\n\t\tbo->seqno = seqno;\n\t}\n\n\tlist_for_each_entry(bo, &exec->unref_list, unref_head) {\n\t\tbo->seqno = seqno;\n\t}\n\n\tfor (i = 0; i < exec->rcl_write_bo_count; i++) {\n\t\tbo = to_vc4_bo(&exec->rcl_write_bo[i]->base);\n\t\tbo->write_seqno = seqno;\n\t}\n}\n\n/* Queues a struct vc4_exec_info for execution.  If no job is\n * currently executing, then submits it.\n *\n * Unlike most GPUs, our hardware only handles one command list at a\n * time.  To queue multiple jobs at once, we'd need to edit the\n * previous command list to have a jump to the new one at the end, and\n * then bump the end address.  That's a change for a later date,\n * though.\n */\nstatic void\nvc4_queue_submit(struct drm_device *dev, struct vc4_exec_info *exec)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tuint64_t seqno;\n\tunsigned long irqflags;\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\n\tseqno = ++vc4->emit_seqno;\n\texec->seqno = seqno;\n\tvc4_update_bo_seqnos(exec, seqno);\n\n\tlist_add_tail(&exec->head, &vc4->bin_job_list);\n\n\t/* If no job was executing, kick ours off.  Otherwise, it'll\n\t * get started when the previous job's flush done interrupt\n\t * occurs.\n\t */\n\tif (vc4_first_bin_job(vc4) == exec) {\n\t\tvc4_submit_next_bin_job(dev);\n\t\tvc4_queue_hangcheck(dev);\n\t}\n\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n}\n\n/**\n * Looks up a bunch of GEM handles for BOs and stores the array for\n * use in the command validator that actually writes relocated\n * addresses pointing to them.\n */\nstatic int\nvc4_cl_lookup_bos(struct drm_device *dev,\n\t\t  struct drm_file *file_priv,\n\t\t  struct vc4_exec_info *exec)\n{\n\tstruct drm_vc4_submit_cl *args = exec->args;\n\tuint32_t *handles;\n\tint ret = 0;\n\tint i;\n\n\texec->bo_count = args->bo_handle_count;\n\n\tif (!exec->bo_count) {\n\t\t/* See comment on bo_index for why we have to check\n\t\t * this.\n\t\t */\n\t\tDRM_ERROR(\"Rendering requires BOs to validate\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\texec->bo = drm_calloc_large(exec->bo_count,\n\t\t\t\t    sizeof(struct drm_gem_cma_object *));\n\tif (!exec->bo) {\n\t\tDRM_ERROR(\"Failed to allocate validated BO pointers\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\thandles = drm_malloc_ab(exec->bo_count, sizeof(uint32_t));\n\tif (!handles) {\n\t\tret = -ENOMEM;\n\t\tDRM_ERROR(\"Failed to allocate incoming GEM handles\\n\");\n\t\tgoto fail;\n\t}\n\n\tif (copy_from_user(handles,\n\t\t\t   (void __user *)(uintptr_t)args->bo_handles,\n\t\t\t   exec->bo_count * sizeof(uint32_t))) {\n\t\tret = -EFAULT;\n\t\tDRM_ERROR(\"Failed to copy in GEM handles\\n\");\n\t\tgoto fail;\n\t}\n\n\tspin_lock(&file_priv->table_lock);\n\tfor (i = 0; i < exec->bo_count; i++) {\n\t\tstruct drm_gem_object *bo = idr_find(&file_priv->object_idr,\n\t\t\t\t\t\t     handles[i]);\n\t\tif (!bo) {\n\t\t\tDRM_ERROR(\"Failed to look up GEM BO %d: %d\\n\",\n\t\t\t\t  i, handles[i]);\n\t\t\tret = -EINVAL;\n\t\t\tspin_unlock(&file_priv->table_lock);\n\t\t\tgoto fail;\n\t\t}\n\t\tdrm_gem_object_reference(bo);\n\t\texec->bo[i] = (struct drm_gem_cma_object *)bo;\n\t}\n\tspin_unlock(&file_priv->table_lock);\n\nfail:\n\tdrm_free_large(handles);\n\treturn ret;\n}\n\nstatic int\nvc4_get_bcl(struct drm_device *dev, struct vc4_exec_info *exec)\n{\n\tstruct drm_vc4_submit_cl *args = exec->args;\n\tvoid *temp = NULL;\n\tvoid *bin;\n\tint ret = 0;\n\tuint32_t bin_offset = 0;\n\tuint32_t shader_rec_offset = roundup(bin_offset + args->bin_cl_size,\n\t\t\t\t\t     16);\n\tuint32_t uniforms_offset = shader_rec_offset + args->shader_rec_size;\n\tuint32_t exec_size = uniforms_offset + args->uniforms_size;\n\tuint32_t temp_size = exec_size + (sizeof(struct vc4_shader_state) *\n\t\t\t\t\t  args->shader_rec_count);\n\tstruct vc4_bo *bo;\n\n\tif (uniforms_offset < shader_rec_offset ||\n\t    exec_size < uniforms_offset ||\n\t    args->shader_rec_count >= (UINT_MAX /\n\t\t\t\t\t  sizeof(struct vc4_shader_state)) ||\n\t    temp_size < exec_size) {\n\t\tDRM_ERROR(\"overflow in exec arguments\\n\");\n\t\tgoto fail;\n\t}\n\n\t/* Allocate space where we'll store the copied in user command lists\n\t * and shader records.\n\t *\n\t * We don't just copy directly into the BOs because we need to\n\t * read the contents back for validation, and I think the\n\t * bo->vaddr is uncached access.\n\t */\n\ttemp = drm_malloc_ab(temp_size, 1);\n\tif (!temp) {\n\t\tDRM_ERROR(\"Failed to allocate storage for copying \"\n\t\t\t  \"in bin/render CLs.\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tbin = temp + bin_offset;\n\texec->shader_rec_u = temp + shader_rec_offset;\n\texec->uniforms_u = temp + uniforms_offset;\n\texec->shader_state = temp + exec_size;\n\texec->shader_state_size = args->shader_rec_count;\n\n\tif (copy_from_user(bin,\n\t\t\t   (void __user *)(uintptr_t)args->bin_cl,\n\t\t\t   args->bin_cl_size)) {\n\t\tret = -EFAULT;\n\t\tgoto fail;\n\t}\n\n\tif (copy_from_user(exec->shader_rec_u,\n\t\t\t   (void __user *)(uintptr_t)args->shader_rec,\n\t\t\t   args->shader_rec_size)) {\n\t\tret = -EFAULT;\n\t\tgoto fail;\n\t}\n\n\tif (copy_from_user(exec->uniforms_u,\n\t\t\t   (void __user *)(uintptr_t)args->uniforms,\n\t\t\t   args->uniforms_size)) {\n\t\tret = -EFAULT;\n\t\tgoto fail;\n\t}\n\n\tbo = vc4_bo_create(dev, exec_size, true);\n\tif (IS_ERR(bo)) {\n\t\tDRM_ERROR(\"Couldn't allocate BO for binning\\n\");\n\t\tret = PTR_ERR(bo);\n\t\tgoto fail;\n\t}\n\texec->exec_bo = &bo->base;\n\n\tlist_add_tail(&to_vc4_bo(&exec->exec_bo->base)->unref_head,\n\t\t      &exec->unref_list);\n\n\texec->ct0ca = exec->exec_bo->paddr + bin_offset;\n\n\texec->bin_u = bin;\n\n\texec->shader_rec_v = exec->exec_bo->vaddr + shader_rec_offset;\n\texec->shader_rec_p = exec->exec_bo->paddr + shader_rec_offset;\n\texec->shader_rec_size = args->shader_rec_size;\n\n\texec->uniforms_v = exec->exec_bo->vaddr + uniforms_offset;\n\texec->uniforms_p = exec->exec_bo->paddr + uniforms_offset;\n\texec->uniforms_size = args->uniforms_size;\n\n\tret = vc4_validate_bin_cl(dev,\n\t\t\t\t  exec->exec_bo->vaddr + bin_offset,\n\t\t\t\t  bin,\n\t\t\t\t  exec);\n\tif (ret)\n\t\tgoto fail;\n\n\tret = vc4_validate_shader_recs(dev, exec);\n\tif (ret)\n\t\tgoto fail;\n\n\t/* Block waiting on any previous rendering into the CS's VBO,\n\t * IB, or textures, so that pixels are actually written by the\n\t * time we try to read them.\n\t */\n\tret = vc4_wait_for_seqno(dev, exec->bin_dep_seqno, ~0ull, true);\n\nfail:\n\tdrm_free_large(temp);\n\treturn ret;\n}\n\nstatic void\nvc4_complete_exec(struct drm_device *dev, struct vc4_exec_info *exec)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tunsigned i;\n\n\tif (exec->bo) {\n\t\tfor (i = 0; i < exec->bo_count; i++)\n\t\t\tdrm_gem_object_unreference_unlocked(&exec->bo[i]->base);\n\t\tdrm_free_large(exec->bo);\n\t}\n\n\twhile (!list_empty(&exec->unref_list)) {\n\t\tstruct vc4_bo *bo = list_first_entry(&exec->unref_list,\n\t\t\t\t\t\t     struct vc4_bo, unref_head);\n\t\tlist_del(&bo->unref_head);\n\t\tdrm_gem_object_unreference_unlocked(&bo->base.base);\n\t}\n\n\tmutex_lock(&vc4->power_lock);\n\tif (--vc4->power_refcount == 0) {\n\t\tpm_runtime_mark_last_busy(&vc4->v3d->pdev->dev);\n\t\tpm_runtime_put_autosuspend(&vc4->v3d->pdev->dev);\n\t}\n\tmutex_unlock(&vc4->power_lock);\n\n\tkfree(exec);\n}\n\nvoid\nvc4_job_handle_completed(struct vc4_dev *vc4)\n{\n\tunsigned long irqflags;\n\tstruct vc4_seqno_cb *cb, *cb_temp;\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\twhile (!list_empty(&vc4->job_done_list)) {\n\t\tstruct vc4_exec_info *exec =\n\t\t\tlist_first_entry(&vc4->job_done_list,\n\t\t\t\t\t struct vc4_exec_info, head);\n\t\tlist_del(&exec->head);\n\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\tvc4_complete_exec(vc4->dev, exec);\n\t\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\t}\n\n\tlist_for_each_entry_safe(cb, cb_temp, &vc4->seqno_cb_list, work.entry) {\n\t\tif (cb->seqno <= vc4->finished_seqno) {\n\t\t\tlist_del_init(&cb->work.entry);\n\t\t\tschedule_work(&cb->work);\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n}\n\nstatic void vc4_seqno_cb_work(struct work_struct *work)\n{\n\tstruct vc4_seqno_cb *cb = container_of(work, struct vc4_seqno_cb, work);\n\n\tcb->func(cb);\n}\n\nint vc4_queue_seqno_cb(struct drm_device *dev,\n\t\t       struct vc4_seqno_cb *cb, uint64_t seqno,\n\t\t       void (*func)(struct vc4_seqno_cb *cb))\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tint ret = 0;\n\tunsigned long irqflags;\n\n\tcb->func = func;\n\tINIT_WORK(&cb->work, vc4_seqno_cb_work);\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\tif (seqno > vc4->finished_seqno) {\n\t\tcb->seqno = seqno;\n\t\tlist_add_tail(&cb->work.entry, &vc4->seqno_cb_list);\n\t} else {\n\t\tschedule_work(&cb->work);\n\t}\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\n\treturn ret;\n}\n\n/* Scheduled when any job has been completed, this walks the list of\n * jobs that had completed and unrefs their BOs and frees their exec\n * structs.\n */\nstatic void\nvc4_job_done_work(struct work_struct *work)\n{\n\tstruct vc4_dev *vc4 =\n\t\tcontainer_of(work, struct vc4_dev, job_done_work);\n\n\tvc4_job_handle_completed(vc4);\n}\n\nstatic int\nvc4_wait_for_seqno_ioctl_helper(struct drm_device *dev,\n\t\t\t\tuint64_t seqno,\n\t\t\t\tuint64_t *timeout_ns)\n{\n\tunsigned long start = jiffies;\n\tint ret = vc4_wait_for_seqno(dev, seqno, *timeout_ns, true);\n\n\tif ((ret == -EINTR || ret == -ERESTARTSYS) && *timeout_ns != ~0ull) {\n\t\tuint64_t delta = jiffies_to_nsecs(jiffies - start);\n\n\t\tif (*timeout_ns >= delta)\n\t\t\t*timeout_ns -= delta;\n\t}\n\n\treturn ret;\n}\n\nint\nvc4_wait_seqno_ioctl(struct drm_device *dev, void *data,\n\t\t     struct drm_file *file_priv)\n{\n\tstruct drm_vc4_wait_seqno *args = data;\n\n\treturn vc4_wait_for_seqno_ioctl_helper(dev, args->seqno,\n\t\t\t\t\t       &args->timeout_ns);\n}\n\nint\nvc4_wait_bo_ioctl(struct drm_device *dev, void *data,\n\t\t  struct drm_file *file_priv)\n{\n\tint ret;\n\tstruct drm_vc4_wait_bo *args = data;\n\tstruct drm_gem_object *gem_obj;\n\tstruct vc4_bo *bo;\n\n\tif (args->pad != 0)\n\t\treturn -EINVAL;\n\n\tgem_obj = drm_gem_object_lookup(file_priv, args->handle);\n\tif (!gem_obj) {\n\t\tDRM_ERROR(\"Failed to look up GEM BO %d\\n\", args->handle);\n\t\treturn -EINVAL;\n\t}\n\tbo = to_vc4_bo(gem_obj);\n\n\tret = vc4_wait_for_seqno_ioctl_helper(dev, bo->seqno,\n\t\t\t\t\t      &args->timeout_ns);\n\n\tdrm_gem_object_unreference_unlocked(gem_obj);\n\treturn ret;\n}\n\n/**\n * Submits a command list to the VC4.\n *\n * This is what is called batchbuffer emitting on other hardware.\n */\nint\nvc4_submit_cl_ioctl(struct drm_device *dev, void *data,\n\t\t    struct drm_file *file_priv)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct drm_vc4_submit_cl *args = data;\n\tstruct vc4_exec_info *exec;\n\tint ret = 0;\n\n\tif ((args->flags & ~VC4_SUBMIT_CL_USE_CLEAR_COLOR) != 0) {\n\t\tDRM_ERROR(\"Unknown flags: 0x%02x\\n\", args->flags);\n\t\treturn -EINVAL;\n\t}\n\n\texec = kcalloc(1, sizeof(*exec), GFP_KERNEL);\n\tif (!exec) {\n\t\tDRM_ERROR(\"malloc failure on exec struct\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_lock(&vc4->power_lock);\n\tif (vc4->power_refcount++ == 0)\n\t\tret = pm_runtime_get_sync(&vc4->v3d->pdev->dev);\n\tmutex_unlock(&vc4->power_lock);\n\tif (ret < 0) {\n\t\tkfree(exec);\n\t\treturn ret;\n\t}\n\n\texec->args = args;\n\tINIT_LIST_HEAD(&exec->unref_list);\n\n\tret = vc4_cl_lookup_bos(dev, file_priv, exec);\n\tif (ret)\n\t\tgoto fail;\n\n\tif (exec->args->bin_cl_size != 0) {\n\t\tret = vc4_get_bcl(dev, exec);\n\t\tif (ret)\n\t\t\tgoto fail;\n\t} else {\n\t\texec->ct0ca = 0;\n\t\texec->ct0ea = 0;\n\t}\n\n\tret = vc4_get_rcl(dev, exec);\n\tif (ret)\n\t\tgoto fail;\n\n\t/* Clear this out of the struct we'll be putting in the queue,\n\t * since it's part of our stack.\n\t */\n\texec->args = NULL;\n\n\tvc4_queue_submit(dev, exec);\n\n\t/* Return the seqno for our job. */\n\targs->seqno = vc4->emit_seqno;\n\n\treturn 0;\n\nfail:\n\tvc4_complete_exec(vc4->dev, exec);\n\n\treturn ret;\n}\n\nvoid\nvc4_gem_init(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\tINIT_LIST_HEAD(&vc4->bin_job_list);\n\tINIT_LIST_HEAD(&vc4->render_job_list);\n\tINIT_LIST_HEAD(&vc4->job_done_list);\n\tINIT_LIST_HEAD(&vc4->seqno_cb_list);\n\tspin_lock_init(&vc4->job_lock);\n\n\tINIT_WORK(&vc4->hangcheck.reset_work, vc4_reset_work);\n\tsetup_timer(&vc4->hangcheck.timer,\n\t\t    vc4_hangcheck_elapsed,\n\t\t    (unsigned long)dev);\n\n\tINIT_WORK(&vc4->job_done_work, vc4_job_done_work);\n\n\tmutex_init(&vc4->power_lock);\n}\n\nvoid\nvc4_gem_destroy(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\t/* Waiting for exec to finish would need to be done before\n\t * unregistering V3D.\n\t */\n\tWARN_ON(vc4->emit_seqno != vc4->finished_seqno);\n\n\t/* V3D should already have disabled its interrupt and cleared\n\t * the overflow allocation registers.  Now free the object.\n\t */\n\tif (vc4->overflow_mem) {\n\t\tdrm_gem_object_unreference_unlocked(&vc4->overflow_mem->base.base);\n\t\tvc4->overflow_mem = NULL;\n\t}\n\n\tif (vc4->hang_state)\n\t\tvc4_free_hang_state(dev, vc4->hang_state);\n\n\tvc4_bo_cache_destroy(dev);\n}\n"], "fixing_code": ["/*\n * Copyright \u00a9 2014 Broadcom\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice (including the next\n * paragraph) shall be included in all copies or substantial portions of the\n * Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n * IN THE SOFTWARE.\n */\n\n#include <linux/module.h>\n#include <linux/platform_device.h>\n#include <linux/pm_runtime.h>\n#include <linux/device.h>\n#include <linux/io.h>\n\n#include \"uapi/drm/vc4_drm.h\"\n#include \"vc4_drv.h\"\n#include \"vc4_regs.h\"\n#include \"vc4_trace.h\"\n\nstatic void\nvc4_queue_hangcheck(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\tmod_timer(&vc4->hangcheck.timer,\n\t\t  round_jiffies_up(jiffies + msecs_to_jiffies(100)));\n}\n\nstruct vc4_hang_state {\n\tstruct drm_vc4_get_hang_state user_state;\n\n\tu32 bo_count;\n\tstruct drm_gem_object **bo;\n};\n\nstatic void\nvc4_free_hang_state(struct drm_device *dev, struct vc4_hang_state *state)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < state->user_state.bo_count; i++)\n\t\tdrm_gem_object_unreference_unlocked(state->bo[i]);\n\n\tkfree(state);\n}\n\nint\nvc4_get_hang_state_ioctl(struct drm_device *dev, void *data,\n\t\t\t struct drm_file *file_priv)\n{\n\tstruct drm_vc4_get_hang_state *get_state = data;\n\tstruct drm_vc4_get_hang_state_bo *bo_state;\n\tstruct vc4_hang_state *kernel_state;\n\tstruct drm_vc4_get_hang_state *state;\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tunsigned long irqflags;\n\tu32 i;\n\tint ret = 0;\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\tkernel_state = vc4->hang_state;\n\tif (!kernel_state) {\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\treturn -ENOENT;\n\t}\n\tstate = &kernel_state->user_state;\n\n\t/* If the user's array isn't big enough, just return the\n\t * required array size.\n\t */\n\tif (get_state->bo_count < state->bo_count) {\n\t\tget_state->bo_count = state->bo_count;\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\treturn 0;\n\t}\n\n\tvc4->hang_state = NULL;\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\n\t/* Save the user's BO pointer, so we don't stomp it with the memcpy. */\n\tstate->bo = get_state->bo;\n\tmemcpy(get_state, state, sizeof(*state));\n\n\tbo_state = kcalloc(state->bo_count, sizeof(*bo_state), GFP_KERNEL);\n\tif (!bo_state) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free;\n\t}\n\n\tfor (i = 0; i < state->bo_count; i++) {\n\t\tstruct vc4_bo *vc4_bo = to_vc4_bo(kernel_state->bo[i]);\n\t\tu32 handle;\n\n\t\tret = drm_gem_handle_create(file_priv, kernel_state->bo[i],\n\t\t\t\t\t    &handle);\n\n\t\tif (ret) {\n\t\t\tstate->bo_count = i - 1;\n\t\t\tgoto err;\n\t\t}\n\t\tbo_state[i].handle = handle;\n\t\tbo_state[i].paddr = vc4_bo->base.paddr;\n\t\tbo_state[i].size = vc4_bo->base.base.size;\n\t}\n\n\tif (copy_to_user((void __user *)(uintptr_t)get_state->bo,\n\t\t\t bo_state,\n\t\t\t state->bo_count * sizeof(*bo_state)))\n\t\tret = -EFAULT;\n\n\tkfree(bo_state);\n\nerr_free:\n\n\tvc4_free_hang_state(dev, kernel_state);\n\nerr:\n\treturn ret;\n}\n\nstatic void\nvc4_save_hang_state(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct drm_vc4_get_hang_state *state;\n\tstruct vc4_hang_state *kernel_state;\n\tstruct vc4_exec_info *exec[2];\n\tstruct vc4_bo *bo;\n\tunsigned long irqflags;\n\tunsigned int i, j, unref_list_count, prev_idx;\n\n\tkernel_state = kcalloc(1, sizeof(*kernel_state), GFP_KERNEL);\n\tif (!kernel_state)\n\t\treturn;\n\n\tstate = &kernel_state->user_state;\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\texec[0] = vc4_first_bin_job(vc4);\n\texec[1] = vc4_first_render_job(vc4);\n\tif (!exec[0] && !exec[1]) {\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\treturn;\n\t}\n\n\t/* Get the bos from both binner and renderer into hang state. */\n\tstate->bo_count = 0;\n\tfor (i = 0; i < 2; i++) {\n\t\tif (!exec[i])\n\t\t\tcontinue;\n\n\t\tunref_list_count = 0;\n\t\tlist_for_each_entry(bo, &exec[i]->unref_list, unref_head)\n\t\t\tunref_list_count++;\n\t\tstate->bo_count += exec[i]->bo_count + unref_list_count;\n\t}\n\n\tkernel_state->bo = kcalloc(state->bo_count,\n\t\t\t\t   sizeof(*kernel_state->bo), GFP_ATOMIC);\n\n\tif (!kernel_state->bo) {\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\treturn;\n\t}\n\n\tprev_idx = 0;\n\tfor (i = 0; i < 2; i++) {\n\t\tif (!exec[i])\n\t\t\tcontinue;\n\n\t\tfor (j = 0; j < exec[i]->bo_count; j++) {\n\t\t\tdrm_gem_object_reference(&exec[i]->bo[j]->base);\n\t\t\tkernel_state->bo[j + prev_idx] = &exec[i]->bo[j]->base;\n\t\t}\n\n\t\tlist_for_each_entry(bo, &exec[i]->unref_list, unref_head) {\n\t\t\tdrm_gem_object_reference(&bo->base.base);\n\t\t\tkernel_state->bo[j + prev_idx] = &bo->base.base;\n\t\t\tj++;\n\t\t}\n\t\tprev_idx = j + 1;\n\t}\n\n\tif (exec[0])\n\t\tstate->start_bin = exec[0]->ct0ca;\n\tif (exec[1])\n\t\tstate->start_render = exec[1]->ct1ca;\n\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\n\tstate->ct0ca = V3D_READ(V3D_CTNCA(0));\n\tstate->ct0ea = V3D_READ(V3D_CTNEA(0));\n\n\tstate->ct1ca = V3D_READ(V3D_CTNCA(1));\n\tstate->ct1ea = V3D_READ(V3D_CTNEA(1));\n\n\tstate->ct0cs = V3D_READ(V3D_CTNCS(0));\n\tstate->ct1cs = V3D_READ(V3D_CTNCS(1));\n\n\tstate->ct0ra0 = V3D_READ(V3D_CT00RA0);\n\tstate->ct1ra0 = V3D_READ(V3D_CT01RA0);\n\n\tstate->bpca = V3D_READ(V3D_BPCA);\n\tstate->bpcs = V3D_READ(V3D_BPCS);\n\tstate->bpoa = V3D_READ(V3D_BPOA);\n\tstate->bpos = V3D_READ(V3D_BPOS);\n\n\tstate->vpmbase = V3D_READ(V3D_VPMBASE);\n\n\tstate->dbge = V3D_READ(V3D_DBGE);\n\tstate->fdbgo = V3D_READ(V3D_FDBGO);\n\tstate->fdbgb = V3D_READ(V3D_FDBGB);\n\tstate->fdbgr = V3D_READ(V3D_FDBGR);\n\tstate->fdbgs = V3D_READ(V3D_FDBGS);\n\tstate->errstat = V3D_READ(V3D_ERRSTAT);\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\tif (vc4->hang_state) {\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\tvc4_free_hang_state(dev, kernel_state);\n\t} else {\n\t\tvc4->hang_state = kernel_state;\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t}\n}\n\nstatic void\nvc4_reset(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\tDRM_INFO(\"Resetting GPU.\\n\");\n\n\tmutex_lock(&vc4->power_lock);\n\tif (vc4->power_refcount) {\n\t\t/* Power the device off and back on the by dropping the\n\t\t * reference on runtime PM.\n\t\t */\n\t\tpm_runtime_put_sync_suspend(&vc4->v3d->pdev->dev);\n\t\tpm_runtime_get_sync(&vc4->v3d->pdev->dev);\n\t}\n\tmutex_unlock(&vc4->power_lock);\n\n\tvc4_irq_reset(dev);\n\n\t/* Rearm the hangcheck -- another job might have been waiting\n\t * for our hung one to get kicked off, and vc4_irq_reset()\n\t * would have started it.\n\t */\n\tvc4_queue_hangcheck(dev);\n}\n\nstatic void\nvc4_reset_work(struct work_struct *work)\n{\n\tstruct vc4_dev *vc4 =\n\t\tcontainer_of(work, struct vc4_dev, hangcheck.reset_work);\n\n\tvc4_save_hang_state(vc4->dev);\n\n\tvc4_reset(vc4->dev);\n}\n\nstatic void\nvc4_hangcheck_elapsed(unsigned long data)\n{\n\tstruct drm_device *dev = (struct drm_device *)data;\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tuint32_t ct0ca, ct1ca;\n\tunsigned long irqflags;\n\tstruct vc4_exec_info *bin_exec, *render_exec;\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\n\tbin_exec = vc4_first_bin_job(vc4);\n\trender_exec = vc4_first_render_job(vc4);\n\n\t/* If idle, we can stop watching for hangs. */\n\tif (!bin_exec && !render_exec) {\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\treturn;\n\t}\n\n\tct0ca = V3D_READ(V3D_CTNCA(0));\n\tct1ca = V3D_READ(V3D_CTNCA(1));\n\n\t/* If we've made any progress in execution, rearm the timer\n\t * and wait.\n\t */\n\tif ((bin_exec && ct0ca != bin_exec->last_ct0ca) ||\n\t    (render_exec && ct1ca != render_exec->last_ct1ca)) {\n\t\tif (bin_exec)\n\t\t\tbin_exec->last_ct0ca = ct0ca;\n\t\tif (render_exec)\n\t\t\trender_exec->last_ct1ca = ct1ca;\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\tvc4_queue_hangcheck(dev);\n\t\treturn;\n\t}\n\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\n\t/* We've gone too long with no progress, reset.  This has to\n\t * be done from a work struct, since resetting can sleep and\n\t * this timer hook isn't allowed to.\n\t */\n\tschedule_work(&vc4->hangcheck.reset_work);\n}\n\nstatic void\nsubmit_cl(struct drm_device *dev, uint32_t thread, uint32_t start, uint32_t end)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\t/* Set the current and end address of the control list.\n\t * Writing the end register is what starts the job.\n\t */\n\tV3D_WRITE(V3D_CTNCA(thread), start);\n\tV3D_WRITE(V3D_CTNEA(thread), end);\n}\n\nint\nvc4_wait_for_seqno(struct drm_device *dev, uint64_t seqno, uint64_t timeout_ns,\n\t\t   bool interruptible)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tint ret = 0;\n\tunsigned long timeout_expire;\n\tDEFINE_WAIT(wait);\n\n\tif (vc4->finished_seqno >= seqno)\n\t\treturn 0;\n\n\tif (timeout_ns == 0)\n\t\treturn -ETIME;\n\n\ttimeout_expire = jiffies + nsecs_to_jiffies(timeout_ns);\n\n\ttrace_vc4_wait_for_seqno_begin(dev, seqno, timeout_ns);\n\tfor (;;) {\n\t\tprepare_to_wait(&vc4->job_wait_queue, &wait,\n\t\t\t\tinterruptible ? TASK_INTERRUPTIBLE :\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\n\t\tif (interruptible && signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (vc4->finished_seqno >= seqno)\n\t\t\tbreak;\n\n\t\tif (timeout_ns != ~0ull) {\n\t\t\tif (time_after_eq(jiffies, timeout_expire)) {\n\t\t\t\tret = -ETIME;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tschedule_timeout(timeout_expire - jiffies);\n\t\t} else {\n\t\t\tschedule();\n\t\t}\n\t}\n\n\tfinish_wait(&vc4->job_wait_queue, &wait);\n\ttrace_vc4_wait_for_seqno_end(dev, seqno);\n\n\treturn ret;\n}\n\nstatic void\nvc4_flush_caches(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\t/* Flush the GPU L2 caches.  These caches sit on top of system\n\t * L3 (the 128kb or so shared with the CPU), and are\n\t * non-allocating in the L3.\n\t */\n\tV3D_WRITE(V3D_L2CACTL,\n\t\t  V3D_L2CACTL_L2CCLR);\n\n\tV3D_WRITE(V3D_SLCACTL,\n\t\t  VC4_SET_FIELD(0xf, V3D_SLCACTL_T1CC) |\n\t\t  VC4_SET_FIELD(0xf, V3D_SLCACTL_T0CC) |\n\t\t  VC4_SET_FIELD(0xf, V3D_SLCACTL_UCC) |\n\t\t  VC4_SET_FIELD(0xf, V3D_SLCACTL_ICC));\n}\n\n/* Sets the registers for the next job to be actually be executed in\n * the hardware.\n *\n * The job_lock should be held during this.\n */\nvoid\nvc4_submit_next_bin_job(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct vc4_exec_info *exec;\n\nagain:\n\texec = vc4_first_bin_job(vc4);\n\tif (!exec)\n\t\treturn;\n\n\tvc4_flush_caches(dev);\n\n\t/* Either put the job in the binner if it uses the binner, or\n\t * immediately move it to the to-be-rendered queue.\n\t */\n\tif (exec->ct0ca != exec->ct0ea) {\n\t\tsubmit_cl(dev, 0, exec->ct0ca, exec->ct0ea);\n\t} else {\n\t\tvc4_move_job_to_render(dev, exec);\n\t\tgoto again;\n\t}\n}\n\nvoid\nvc4_submit_next_render_job(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct vc4_exec_info *exec = vc4_first_render_job(vc4);\n\n\tif (!exec)\n\t\treturn;\n\n\tsubmit_cl(dev, 1, exec->ct1ca, exec->ct1ea);\n}\n\nvoid\nvc4_move_job_to_render(struct drm_device *dev, struct vc4_exec_info *exec)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tbool was_empty = list_empty(&vc4->render_job_list);\n\n\tlist_move_tail(&exec->head, &vc4->render_job_list);\n\tif (was_empty)\n\t\tvc4_submit_next_render_job(dev);\n}\n\nstatic void\nvc4_update_bo_seqnos(struct vc4_exec_info *exec, uint64_t seqno)\n{\n\tstruct vc4_bo *bo;\n\tunsigned i;\n\n\tfor (i = 0; i < exec->bo_count; i++) {\n\t\tbo = to_vc4_bo(&exec->bo[i]->base);\n\t\tbo->seqno = seqno;\n\t}\n\n\tlist_for_each_entry(bo, &exec->unref_list, unref_head) {\n\t\tbo->seqno = seqno;\n\t}\n\n\tfor (i = 0; i < exec->rcl_write_bo_count; i++) {\n\t\tbo = to_vc4_bo(&exec->rcl_write_bo[i]->base);\n\t\tbo->write_seqno = seqno;\n\t}\n}\n\n/* Queues a struct vc4_exec_info for execution.  If no job is\n * currently executing, then submits it.\n *\n * Unlike most GPUs, our hardware only handles one command list at a\n * time.  To queue multiple jobs at once, we'd need to edit the\n * previous command list to have a jump to the new one at the end, and\n * then bump the end address.  That's a change for a later date,\n * though.\n */\nstatic void\nvc4_queue_submit(struct drm_device *dev, struct vc4_exec_info *exec)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tuint64_t seqno;\n\tunsigned long irqflags;\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\n\tseqno = ++vc4->emit_seqno;\n\texec->seqno = seqno;\n\tvc4_update_bo_seqnos(exec, seqno);\n\n\tlist_add_tail(&exec->head, &vc4->bin_job_list);\n\n\t/* If no job was executing, kick ours off.  Otherwise, it'll\n\t * get started when the previous job's flush done interrupt\n\t * occurs.\n\t */\n\tif (vc4_first_bin_job(vc4) == exec) {\n\t\tvc4_submit_next_bin_job(dev);\n\t\tvc4_queue_hangcheck(dev);\n\t}\n\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n}\n\n/**\n * Looks up a bunch of GEM handles for BOs and stores the array for\n * use in the command validator that actually writes relocated\n * addresses pointing to them.\n */\nstatic int\nvc4_cl_lookup_bos(struct drm_device *dev,\n\t\t  struct drm_file *file_priv,\n\t\t  struct vc4_exec_info *exec)\n{\n\tstruct drm_vc4_submit_cl *args = exec->args;\n\tuint32_t *handles;\n\tint ret = 0;\n\tint i;\n\n\texec->bo_count = args->bo_handle_count;\n\n\tif (!exec->bo_count) {\n\t\t/* See comment on bo_index for why we have to check\n\t\t * this.\n\t\t */\n\t\tDRM_ERROR(\"Rendering requires BOs to validate\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\texec->bo = drm_calloc_large(exec->bo_count,\n\t\t\t\t    sizeof(struct drm_gem_cma_object *));\n\tif (!exec->bo) {\n\t\tDRM_ERROR(\"Failed to allocate validated BO pointers\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\thandles = drm_malloc_ab(exec->bo_count, sizeof(uint32_t));\n\tif (!handles) {\n\t\tret = -ENOMEM;\n\t\tDRM_ERROR(\"Failed to allocate incoming GEM handles\\n\");\n\t\tgoto fail;\n\t}\n\n\tif (copy_from_user(handles,\n\t\t\t   (void __user *)(uintptr_t)args->bo_handles,\n\t\t\t   exec->bo_count * sizeof(uint32_t))) {\n\t\tret = -EFAULT;\n\t\tDRM_ERROR(\"Failed to copy in GEM handles\\n\");\n\t\tgoto fail;\n\t}\n\n\tspin_lock(&file_priv->table_lock);\n\tfor (i = 0; i < exec->bo_count; i++) {\n\t\tstruct drm_gem_object *bo = idr_find(&file_priv->object_idr,\n\t\t\t\t\t\t     handles[i]);\n\t\tif (!bo) {\n\t\t\tDRM_ERROR(\"Failed to look up GEM BO %d: %d\\n\",\n\t\t\t\t  i, handles[i]);\n\t\t\tret = -EINVAL;\n\t\t\tspin_unlock(&file_priv->table_lock);\n\t\t\tgoto fail;\n\t\t}\n\t\tdrm_gem_object_reference(bo);\n\t\texec->bo[i] = (struct drm_gem_cma_object *)bo;\n\t}\n\tspin_unlock(&file_priv->table_lock);\n\nfail:\n\tdrm_free_large(handles);\n\treturn ret;\n}\n\nstatic int\nvc4_get_bcl(struct drm_device *dev, struct vc4_exec_info *exec)\n{\n\tstruct drm_vc4_submit_cl *args = exec->args;\n\tvoid *temp = NULL;\n\tvoid *bin;\n\tint ret = 0;\n\tuint32_t bin_offset = 0;\n\tuint32_t shader_rec_offset = roundup(bin_offset + args->bin_cl_size,\n\t\t\t\t\t     16);\n\tuint32_t uniforms_offset = shader_rec_offset + args->shader_rec_size;\n\tuint32_t exec_size = uniforms_offset + args->uniforms_size;\n\tuint32_t temp_size = exec_size + (sizeof(struct vc4_shader_state) *\n\t\t\t\t\t  args->shader_rec_count);\n\tstruct vc4_bo *bo;\n\n\tif (shader_rec_offset < args->bin_cl_size ||\n\t    uniforms_offset < shader_rec_offset ||\n\t    exec_size < uniforms_offset ||\n\t    args->shader_rec_count >= (UINT_MAX /\n\t\t\t\t\t  sizeof(struct vc4_shader_state)) ||\n\t    temp_size < exec_size) {\n\t\tDRM_ERROR(\"overflow in exec arguments\\n\");\n\t\tgoto fail;\n\t}\n\n\t/* Allocate space where we'll store the copied in user command lists\n\t * and shader records.\n\t *\n\t * We don't just copy directly into the BOs because we need to\n\t * read the contents back for validation, and I think the\n\t * bo->vaddr is uncached access.\n\t */\n\ttemp = drm_malloc_ab(temp_size, 1);\n\tif (!temp) {\n\t\tDRM_ERROR(\"Failed to allocate storage for copying \"\n\t\t\t  \"in bin/render CLs.\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tbin = temp + bin_offset;\n\texec->shader_rec_u = temp + shader_rec_offset;\n\texec->uniforms_u = temp + uniforms_offset;\n\texec->shader_state = temp + exec_size;\n\texec->shader_state_size = args->shader_rec_count;\n\n\tif (copy_from_user(bin,\n\t\t\t   (void __user *)(uintptr_t)args->bin_cl,\n\t\t\t   args->bin_cl_size)) {\n\t\tret = -EFAULT;\n\t\tgoto fail;\n\t}\n\n\tif (copy_from_user(exec->shader_rec_u,\n\t\t\t   (void __user *)(uintptr_t)args->shader_rec,\n\t\t\t   args->shader_rec_size)) {\n\t\tret = -EFAULT;\n\t\tgoto fail;\n\t}\n\n\tif (copy_from_user(exec->uniforms_u,\n\t\t\t   (void __user *)(uintptr_t)args->uniforms,\n\t\t\t   args->uniforms_size)) {\n\t\tret = -EFAULT;\n\t\tgoto fail;\n\t}\n\n\tbo = vc4_bo_create(dev, exec_size, true);\n\tif (IS_ERR(bo)) {\n\t\tDRM_ERROR(\"Couldn't allocate BO for binning\\n\");\n\t\tret = PTR_ERR(bo);\n\t\tgoto fail;\n\t}\n\texec->exec_bo = &bo->base;\n\n\tlist_add_tail(&to_vc4_bo(&exec->exec_bo->base)->unref_head,\n\t\t      &exec->unref_list);\n\n\texec->ct0ca = exec->exec_bo->paddr + bin_offset;\n\n\texec->bin_u = bin;\n\n\texec->shader_rec_v = exec->exec_bo->vaddr + shader_rec_offset;\n\texec->shader_rec_p = exec->exec_bo->paddr + shader_rec_offset;\n\texec->shader_rec_size = args->shader_rec_size;\n\n\texec->uniforms_v = exec->exec_bo->vaddr + uniforms_offset;\n\texec->uniforms_p = exec->exec_bo->paddr + uniforms_offset;\n\texec->uniforms_size = args->uniforms_size;\n\n\tret = vc4_validate_bin_cl(dev,\n\t\t\t\t  exec->exec_bo->vaddr + bin_offset,\n\t\t\t\t  bin,\n\t\t\t\t  exec);\n\tif (ret)\n\t\tgoto fail;\n\n\tret = vc4_validate_shader_recs(dev, exec);\n\tif (ret)\n\t\tgoto fail;\n\n\t/* Block waiting on any previous rendering into the CS's VBO,\n\t * IB, or textures, so that pixels are actually written by the\n\t * time we try to read them.\n\t */\n\tret = vc4_wait_for_seqno(dev, exec->bin_dep_seqno, ~0ull, true);\n\nfail:\n\tdrm_free_large(temp);\n\treturn ret;\n}\n\nstatic void\nvc4_complete_exec(struct drm_device *dev, struct vc4_exec_info *exec)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tunsigned i;\n\n\tif (exec->bo) {\n\t\tfor (i = 0; i < exec->bo_count; i++)\n\t\t\tdrm_gem_object_unreference_unlocked(&exec->bo[i]->base);\n\t\tdrm_free_large(exec->bo);\n\t}\n\n\twhile (!list_empty(&exec->unref_list)) {\n\t\tstruct vc4_bo *bo = list_first_entry(&exec->unref_list,\n\t\t\t\t\t\t     struct vc4_bo, unref_head);\n\t\tlist_del(&bo->unref_head);\n\t\tdrm_gem_object_unreference_unlocked(&bo->base.base);\n\t}\n\n\tmutex_lock(&vc4->power_lock);\n\tif (--vc4->power_refcount == 0) {\n\t\tpm_runtime_mark_last_busy(&vc4->v3d->pdev->dev);\n\t\tpm_runtime_put_autosuspend(&vc4->v3d->pdev->dev);\n\t}\n\tmutex_unlock(&vc4->power_lock);\n\n\tkfree(exec);\n}\n\nvoid\nvc4_job_handle_completed(struct vc4_dev *vc4)\n{\n\tunsigned long irqflags;\n\tstruct vc4_seqno_cb *cb, *cb_temp;\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\twhile (!list_empty(&vc4->job_done_list)) {\n\t\tstruct vc4_exec_info *exec =\n\t\t\tlist_first_entry(&vc4->job_done_list,\n\t\t\t\t\t struct vc4_exec_info, head);\n\t\tlist_del(&exec->head);\n\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\tvc4_complete_exec(vc4->dev, exec);\n\t\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\t}\n\n\tlist_for_each_entry_safe(cb, cb_temp, &vc4->seqno_cb_list, work.entry) {\n\t\tif (cb->seqno <= vc4->finished_seqno) {\n\t\t\tlist_del_init(&cb->work.entry);\n\t\t\tschedule_work(&cb->work);\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n}\n\nstatic void vc4_seqno_cb_work(struct work_struct *work)\n{\n\tstruct vc4_seqno_cb *cb = container_of(work, struct vc4_seqno_cb, work);\n\n\tcb->func(cb);\n}\n\nint vc4_queue_seqno_cb(struct drm_device *dev,\n\t\t       struct vc4_seqno_cb *cb, uint64_t seqno,\n\t\t       void (*func)(struct vc4_seqno_cb *cb))\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tint ret = 0;\n\tunsigned long irqflags;\n\n\tcb->func = func;\n\tINIT_WORK(&cb->work, vc4_seqno_cb_work);\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\tif (seqno > vc4->finished_seqno) {\n\t\tcb->seqno = seqno;\n\t\tlist_add_tail(&cb->work.entry, &vc4->seqno_cb_list);\n\t} else {\n\t\tschedule_work(&cb->work);\n\t}\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\n\treturn ret;\n}\n\n/* Scheduled when any job has been completed, this walks the list of\n * jobs that had completed and unrefs their BOs and frees their exec\n * structs.\n */\nstatic void\nvc4_job_done_work(struct work_struct *work)\n{\n\tstruct vc4_dev *vc4 =\n\t\tcontainer_of(work, struct vc4_dev, job_done_work);\n\n\tvc4_job_handle_completed(vc4);\n}\n\nstatic int\nvc4_wait_for_seqno_ioctl_helper(struct drm_device *dev,\n\t\t\t\tuint64_t seqno,\n\t\t\t\tuint64_t *timeout_ns)\n{\n\tunsigned long start = jiffies;\n\tint ret = vc4_wait_for_seqno(dev, seqno, *timeout_ns, true);\n\n\tif ((ret == -EINTR || ret == -ERESTARTSYS) && *timeout_ns != ~0ull) {\n\t\tuint64_t delta = jiffies_to_nsecs(jiffies - start);\n\n\t\tif (*timeout_ns >= delta)\n\t\t\t*timeout_ns -= delta;\n\t}\n\n\treturn ret;\n}\n\nint\nvc4_wait_seqno_ioctl(struct drm_device *dev, void *data,\n\t\t     struct drm_file *file_priv)\n{\n\tstruct drm_vc4_wait_seqno *args = data;\n\n\treturn vc4_wait_for_seqno_ioctl_helper(dev, args->seqno,\n\t\t\t\t\t       &args->timeout_ns);\n}\n\nint\nvc4_wait_bo_ioctl(struct drm_device *dev, void *data,\n\t\t  struct drm_file *file_priv)\n{\n\tint ret;\n\tstruct drm_vc4_wait_bo *args = data;\n\tstruct drm_gem_object *gem_obj;\n\tstruct vc4_bo *bo;\n\n\tif (args->pad != 0)\n\t\treturn -EINVAL;\n\n\tgem_obj = drm_gem_object_lookup(file_priv, args->handle);\n\tif (!gem_obj) {\n\t\tDRM_ERROR(\"Failed to look up GEM BO %d\\n\", args->handle);\n\t\treturn -EINVAL;\n\t}\n\tbo = to_vc4_bo(gem_obj);\n\n\tret = vc4_wait_for_seqno_ioctl_helper(dev, bo->seqno,\n\t\t\t\t\t      &args->timeout_ns);\n\n\tdrm_gem_object_unreference_unlocked(gem_obj);\n\treturn ret;\n}\n\n/**\n * Submits a command list to the VC4.\n *\n * This is what is called batchbuffer emitting on other hardware.\n */\nint\nvc4_submit_cl_ioctl(struct drm_device *dev, void *data,\n\t\t    struct drm_file *file_priv)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct drm_vc4_submit_cl *args = data;\n\tstruct vc4_exec_info *exec;\n\tint ret = 0;\n\n\tif ((args->flags & ~VC4_SUBMIT_CL_USE_CLEAR_COLOR) != 0) {\n\t\tDRM_ERROR(\"Unknown flags: 0x%02x\\n\", args->flags);\n\t\treturn -EINVAL;\n\t}\n\n\texec = kcalloc(1, sizeof(*exec), GFP_KERNEL);\n\tif (!exec) {\n\t\tDRM_ERROR(\"malloc failure on exec struct\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_lock(&vc4->power_lock);\n\tif (vc4->power_refcount++ == 0)\n\t\tret = pm_runtime_get_sync(&vc4->v3d->pdev->dev);\n\tmutex_unlock(&vc4->power_lock);\n\tif (ret < 0) {\n\t\tkfree(exec);\n\t\treturn ret;\n\t}\n\n\texec->args = args;\n\tINIT_LIST_HEAD(&exec->unref_list);\n\n\tret = vc4_cl_lookup_bos(dev, file_priv, exec);\n\tif (ret)\n\t\tgoto fail;\n\n\tif (exec->args->bin_cl_size != 0) {\n\t\tret = vc4_get_bcl(dev, exec);\n\t\tif (ret)\n\t\t\tgoto fail;\n\t} else {\n\t\texec->ct0ca = 0;\n\t\texec->ct0ea = 0;\n\t}\n\n\tret = vc4_get_rcl(dev, exec);\n\tif (ret)\n\t\tgoto fail;\n\n\t/* Clear this out of the struct we'll be putting in the queue,\n\t * since it's part of our stack.\n\t */\n\texec->args = NULL;\n\n\tvc4_queue_submit(dev, exec);\n\n\t/* Return the seqno for our job. */\n\targs->seqno = vc4->emit_seqno;\n\n\treturn 0;\n\nfail:\n\tvc4_complete_exec(vc4->dev, exec);\n\n\treturn ret;\n}\n\nvoid\nvc4_gem_init(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\tINIT_LIST_HEAD(&vc4->bin_job_list);\n\tINIT_LIST_HEAD(&vc4->render_job_list);\n\tINIT_LIST_HEAD(&vc4->job_done_list);\n\tINIT_LIST_HEAD(&vc4->seqno_cb_list);\n\tspin_lock_init(&vc4->job_lock);\n\n\tINIT_WORK(&vc4->hangcheck.reset_work, vc4_reset_work);\n\tsetup_timer(&vc4->hangcheck.timer,\n\t\t    vc4_hangcheck_elapsed,\n\t\t    (unsigned long)dev);\n\n\tINIT_WORK(&vc4->job_done_work, vc4_job_done_work);\n\n\tmutex_init(&vc4->power_lock);\n}\n\nvoid\nvc4_gem_destroy(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\t/* Waiting for exec to finish would need to be done before\n\t * unregistering V3D.\n\t */\n\tWARN_ON(vc4->emit_seqno != vc4->finished_seqno);\n\n\t/* V3D should already have disabled its interrupt and cleared\n\t * the overflow allocation registers.  Now free the object.\n\t */\n\tif (vc4->overflow_mem) {\n\t\tdrm_gem_object_unreference_unlocked(&vc4->overflow_mem->base.base);\n\t\tvc4->overflow_mem = NULL;\n\t}\n\n\tif (vc4->hang_state)\n\t\tvc4_free_hang_state(dev, vc4->hang_state);\n\n\tvc4_bo_cache_destroy(dev);\n}\n"], "filenames": ["drivers/gpu/drm/vc4/vc4_gem.c"], "buggy_code_start_loc": [597], "buggy_code_end_loc": [598], "fixing_code_start_loc": [597], "fixing_code_end_loc": [599], "type": "CWE-190", "message": "Integer overflow in the vc4_get_bcl function in drivers/gpu/drm/vc4/vc4_gem.c in the VideoCore DRM driver in the Linux kernel before 4.9.7 allows local users to cause a denial of service or possibly have unspecified other impact via a crafted size value in a VC4_SUBMIT_CL ioctl call.", "other": {"cve": {"id": "CVE-2017-5576", "sourceIdentifier": "cve@mitre.org", "published": "2017-02-06T06:59:00.763", "lastModified": "2023-02-10T00:53:43.570", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Integer overflow in the vc4_get_bcl function in drivers/gpu/drm/vc4/vc4_gem.c in the VideoCore DRM driver in the Linux kernel before 4.9.7 allows local users to cause a denial of service or possibly have unspecified other impact via a crafted size value in a VC4_SUBMIT_CL ioctl call."}, {"lang": "es", "value": "Desbordamiento de enteros en la funci\u00f3n vc4_get_bcl en drivers/gpu/drm/vc4/vc4_gem.c en el controlador de VideoCore DRM en el kernel de Linux en versiones anteriores a 4.9.7 permite a usuarios locales provocar una denegaci\u00f3n de servicio o posiblemente tener otro impacto no especificado a trav\u00e9s de un valor de tama\u00f1o manipulado en una llamada ioctl VC4_SUBMIT_CL."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-190"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.5", "versionEndExcluding": "4.9.7", "matchCriteriaId": "20EE2EFD-24B9-486E-8B08-FB740ABD8585"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=0f2ff82e11c86c05d051cae32b58226392d33bbf", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.9.7", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2017/01/21/7", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/95767", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1416436", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch"]}, {"url": "https://github.com/torvalds/linux/commit/0f2ff82e11c86c05d051cae32b58226392d33bbf", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://lkml.org/lkml/2017/1/17/761", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/0f2ff82e11c86c05d051cae32b58226392d33bbf"}}