{"buggy_code": ["/*\n *  This file contains idle entry/exit functions for POWER7,\n *  POWER8 and POWER9 CPUs.\n *\n *  This program is free software; you can redistribute it and/or\n *  modify it under the terms of the GNU General Public License\n *  as published by the Free Software Foundation; either version\n *  2 of the License, or (at your option) any later version.\n */\n\n#include <linux/threads.h>\n#include <asm/processor.h>\n#include <asm/page.h>\n#include <asm/cputable.h>\n#include <asm/thread_info.h>\n#include <asm/ppc_asm.h>\n#include <asm/asm-offsets.h>\n#include <asm/ppc-opcode.h>\n#include <asm/hw_irq.h>\n#include <asm/kvm_book3s_asm.h>\n#include <asm/opal.h>\n#include <asm/cpuidle.h>\n#include <asm/exception-64s.h>\n#include <asm/book3s/64/mmu-hash.h>\n#include <asm/mmu.h>\n#include <asm/asm-compat.h>\n#include <asm/feature-fixups.h>\n\n#undef DEBUG\n\n/*\n * Use unused space in the interrupt stack to save and restore\n * registers for winkle support.\n */\n#define _MMCR0\tGPR0\n#define _SDR1\tGPR3\n#define _PTCR\tGPR3\n#define _RPR\tGPR4\n#define _SPURR\tGPR5\n#define _PURR\tGPR6\n#define _TSCR\tGPR7\n#define _DSCR\tGPR8\n#define _AMOR\tGPR9\n#define _WORT\tGPR10\n#define _WORC\tGPR11\n#define _LPCR\tGPR12\n\n#define PSSCR_EC_ESL_MASK_SHIFTED          (PSSCR_EC | PSSCR_ESL) >> 16\n\n\t.text\n\n/*\n * Used by threads before entering deep idle states. Saves SPRs\n * in interrupt stack frame\n */\nsave_sprs_to_stack:\n\t/*\n\t * Note all register i.e per-core, per-subcore or per-thread is saved\n\t * here since any thread in the core might wake up first\n\t */\nBEGIN_FTR_SECTION\n\t/*\n\t * Note - SDR1 is dropped in Power ISA v3. Hence not restoring\n\t * SDR1 here\n\t */\n\tmfspr\tr3,SPRN_PTCR\n\tstd\tr3,_PTCR(r1)\n\tmfspr\tr3,SPRN_LPCR\n\tstd\tr3,_LPCR(r1)\nFTR_SECTION_ELSE\n\tmfspr\tr3,SPRN_SDR1\n\tstd\tr3,_SDR1(r1)\nALT_FTR_SECTION_END_IFSET(CPU_FTR_ARCH_300)\n\tmfspr\tr3,SPRN_RPR\n\tstd\tr3,_RPR(r1)\n\tmfspr\tr3,SPRN_SPURR\n\tstd\tr3,_SPURR(r1)\n\tmfspr\tr3,SPRN_PURR\n\tstd\tr3,_PURR(r1)\n\tmfspr\tr3,SPRN_TSCR\n\tstd\tr3,_TSCR(r1)\n\tmfspr\tr3,SPRN_DSCR\n\tstd\tr3,_DSCR(r1)\n\tmfspr\tr3,SPRN_AMOR\n\tstd\tr3,_AMOR(r1)\n\tmfspr\tr3,SPRN_WORT\n\tstd\tr3,_WORT(r1)\n\tmfspr\tr3,SPRN_WORC\n\tstd\tr3,_WORC(r1)\n/*\n * On POWER9, there are idle states such as stop4, invoked via cpuidle,\n * that lose hypervisor resources. In such cases, we need to save\n * additional SPRs before entering those idle states so that they can\n * be restored to their older values on wakeup from the idle state.\n *\n * On POWER8, the only such deep idle state is winkle which is used\n * only in the context of CPU-Hotplug, where these additional SPRs are\n * reinitiazed to a sane value. Hence there is no need to save/restore\n * these SPRs.\n */\nBEGIN_FTR_SECTION\n\tblr\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_300)\n\npower9_save_additional_sprs:\n\tmfspr\tr3, SPRN_PID\n\tmfspr\tr4, SPRN_LDBAR\n\tstd\tr3, STOP_PID(r13)\n\tstd\tr4, STOP_LDBAR(r13)\n\n\tmfspr\tr3, SPRN_FSCR\n\tmfspr\tr4, SPRN_HFSCR\n\tstd\tr3, STOP_FSCR(r13)\n\tstd\tr4, STOP_HFSCR(r13)\n\n\tmfspr\tr3, SPRN_MMCRA\n\tmfspr\tr4, SPRN_MMCR0\n\tstd\tr3, STOP_MMCRA(r13)\n\tstd\tr4, _MMCR0(r1)\n\n\tmfspr\tr3, SPRN_MMCR1\n\tmfspr\tr4, SPRN_MMCR2\n\tstd\tr3, STOP_MMCR1(r13)\n\tstd\tr4, STOP_MMCR2(r13)\n\tblr\n\npower9_restore_additional_sprs:\n\tld\tr3,_LPCR(r1)\n\tld\tr4, STOP_PID(r13)\n\tmtspr\tSPRN_LPCR,r3\n\tmtspr\tSPRN_PID, r4\n\n\tld\tr3, STOP_LDBAR(r13)\n\tld\tr4, STOP_FSCR(r13)\n\tmtspr\tSPRN_LDBAR, r3\n\tmtspr\tSPRN_FSCR, r4\n\n\tld\tr3, STOP_HFSCR(r13)\n\tld\tr4, STOP_MMCRA(r13)\n\tmtspr\tSPRN_HFSCR, r3\n\tmtspr\tSPRN_MMCRA, r4\n\n\tld\tr3, _MMCR0(r1)\n\tld\tr4, STOP_MMCR1(r13)\n\tmtspr\tSPRN_MMCR0, r3\n\tmtspr\tSPRN_MMCR1, r4\n\n\tld\tr3, STOP_MMCR2(r13)\n\tld\tr4, PACA_SPRG_VDSO(r13)\n\tmtspr\tSPRN_MMCR2, r3\n\tmtspr\tSPRN_SPRG3, r4\n\tblr\n\n/*\n * Used by threads when the lock bit of core_idle_state is set.\n * Threads will spin in HMT_LOW until the lock bit is cleared.\n * r14 - pointer to core_idle_state\n * r15 - used to load contents of core_idle_state\n * r9  - used as a temporary variable\n */\n\ncore_idle_lock_held:\n\tHMT_LOW\n3:\tlwz\tr15,0(r14)\n\tandis.\tr15,r15,PNV_CORE_IDLE_LOCK_BIT@h\n\tbne\t3b\n\tHMT_MEDIUM\n\tlwarx\tr15,0,r14\n\tandis.\tr9,r15,PNV_CORE_IDLE_LOCK_BIT@h\n\tbne-\tcore_idle_lock_held\n\tblr\n\n/* Reuse an unused pt_regs slot for IAMR */\n#define PNV_POWERSAVE_IAMR\t_DAR\n\n/*\n * Pass requested state in r3:\n *\tr3 - PNV_THREAD_NAP/SLEEP/WINKLE in POWER8\n *\t   - Requested PSSCR value in POWER9\n *\n * Address of idle handler to branch to in realmode in r4\n */\npnv_powersave_common:\n\t/* Use r3 to pass state nap/sleep/winkle */\n\t/* NAP is a state loss, we create a regs frame on the\n\t * stack, fill it up with the state we care about and\n\t * stick a pointer to it in PACAR1. We really only\n\t * need to save PC, some CR bits and the NV GPRs,\n\t * but for now an interrupt frame will do.\n\t */\n\tmtctr\tr4\n\n\tmflr\tr0\n\tstd\tr0,16(r1)\n\tstdu\tr1,-INT_FRAME_SIZE(r1)\n\tstd\tr0,_LINK(r1)\n\tstd\tr0,_NIP(r1)\n\n\t/* We haven't lost state ... yet */\n\tli\tr0,0\n\tstb\tr0,PACA_NAPSTATELOST(r13)\n\n\t/* Continue saving state */\n\tSAVE_GPR(2, r1)\n\tSAVE_NVGPRS(r1)\n\nBEGIN_FTR_SECTION\n\tmfspr\tr5, SPRN_IAMR\n\tstd\tr5, PNV_POWERSAVE_IAMR(r1)\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\n\tmfcr\tr5\n\tstd\tr5,_CCR(r1)\n\tstd\tr1,PACAR1(r13)\n\nBEGIN_FTR_SECTION\n\t/*\n\t * POWER9 does not require real mode to stop, and presently does not\n\t * set hwthread_state for KVM (threads don't share MMU context), so\n\t * we can remain in virtual mode for this.\n\t */\n\tbctr\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_300)\n\t/*\n\t * POWER8\n\t * Go to real mode to do the nap, as required by the architecture.\n\t * Also, we need to be in real mode before setting hwthread_state,\n\t * because as soon as we do that, another thread can switch\n\t * the MMU context to the guest.\n\t */\n\tLOAD_REG_IMMEDIATE(r7, MSR_IDLE)\n\tmtmsrd\tr7,0\n\tbctr\n\n/*\n * This is the sequence required to execute idle instructions, as\n * specified in ISA v2.07 (and earlier). MSR[IR] and MSR[DR] must be 0.\n */\n#define IDLE_STATE_ENTER_SEQ_NORET(IDLE_INST)\t\t\t\\\n\t/* Magic NAP/SLEEP/WINKLE mode enter sequence */\t\\\n\tstd\tr0,0(r1);\t\t\t\t\t\\\n\tptesync;\t\t\t\t\t\t\\\n\tld\tr0,0(r1);\t\t\t\t\t\\\n236:\tcmpd\tcr0,r0,r0;\t\t\t\t\t\\\n\tbne\t236b;\t\t\t\t\t\t\\\n\tIDLE_INST;\n\n\n\t.globl pnv_enter_arch207_idle_mode\npnv_enter_arch207_idle_mode:\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\n\t/* Tell KVM we're entering idle */\n\tli\tr4,KVM_HWTHREAD_IN_IDLE\n\t/******************************************************/\n\t/*  N O T E   W E L L    ! ! !    N O T E   W E L L   */\n\t/* The following store to HSTATE_HWTHREAD_STATE(r13)  */\n\t/* MUST occur in real mode, i.e. with the MMU off,    */\n\t/* and the MMU must stay off until we clear this flag */\n\t/* and test HSTATE_HWTHREAD_REQ(r13) in               */\n\t/* pnv_powersave_wakeup in this file.                 */\n\t/* The reason is that another thread can switch the   */\n\t/* MMU to a guest context whenever this flag is set   */\n\t/* to KVM_HWTHREAD_IN_IDLE, and if the MMU was on,    */\n\t/* that would potentially cause this thread to start  */\n\t/* executing instructions from guest memory in        */\n\t/* hypervisor mode, leading to a host crash or data   */\n\t/* corruption, or worse.                              */\n\t/******************************************************/\n\tstb\tr4,HSTATE_HWTHREAD_STATE(r13)\n#endif\n\tstb\tr3,PACA_THREAD_IDLE_STATE(r13)\n\tcmpwi\tcr3,r3,PNV_THREAD_SLEEP\n\tbge\tcr3,2f\n\tIDLE_STATE_ENTER_SEQ_NORET(PPC_NAP)\n\t/* No return */\n2:\n\t/* Sleep or winkle */\n\tlbz\tr7,PACA_THREAD_MASK(r13)\n\tld\tr14,PACA_CORE_IDLE_STATE_PTR(r13)\n\tli\tr5,0\n\tbeq\tcr3,3f\n\tlis\tr5,PNV_CORE_IDLE_WINKLE_COUNT@h\n3:\nlwarx_loop1:\n\tlwarx\tr15,0,r14\n\n\tandis.\tr9,r15,PNV_CORE_IDLE_LOCK_BIT@h\n\tbnel-\tcore_idle_lock_held\n\n\tadd\tr15,r15,r5\t\t\t/* Add if winkle */\n\tandc\tr15,r15,r7\t\t\t/* Clear thread bit */\n\n\tandi.\tr9,r15,PNV_CORE_IDLE_THREAD_BITS\n\n/*\n * If cr0 = 0, then current thread is the last thread of the core entering\n * sleep. Last thread needs to execute the hardware bug workaround code if\n * required by the platform.\n * Make the workaround call unconditionally here. The below branch call is\n * patched out when the idle states are discovered if the platform does not\n * require it.\n */\n.global pnv_fastsleep_workaround_at_entry\npnv_fastsleep_workaround_at_entry:\n\tbeq\tfastsleep_workaround_at_entry\n\n\tstwcx.\tr15,0,r14\n\tbne-\tlwarx_loop1\n\tisync\n\ncommon_enter: /* common code for all the threads entering sleep or winkle */\n\tbgt\tcr3,enter_winkle\n\tIDLE_STATE_ENTER_SEQ_NORET(PPC_SLEEP)\n\nfastsleep_workaround_at_entry:\n\toris\tr15,r15,PNV_CORE_IDLE_LOCK_BIT@h\n\tstwcx.\tr15,0,r14\n\tbne-\tlwarx_loop1\n\tisync\n\n\t/* Fast sleep workaround */\n\tli\tr3,1\n\tli\tr4,1\n\tbl\topal_config_cpu_idle_state\n\n\t/* Unlock */\n\txoris\tr15,r15,PNV_CORE_IDLE_LOCK_BIT@h\n\tlwsync\n\tstw\tr15,0(r14)\n\tb\tcommon_enter\n\nenter_winkle:\n\tbl\tsave_sprs_to_stack\n\n\tIDLE_STATE_ENTER_SEQ_NORET(PPC_WINKLE)\n\n/*\n * r3 - PSSCR value corresponding to the requested stop state.\n */\npower_enter_stop:\n/*\n * Check if we are executing the lite variant with ESL=EC=0\n */\n\tandis.   r4,r3,PSSCR_EC_ESL_MASK_SHIFTED\n\tclrldi   r3,r3,60 /* r3 = Bits[60:63] = Requested Level (RL) */\n\tbne\t .Lhandle_esl_ec_set\n\tPPC_STOP\n\tli\tr3,0  /* Since we didn't lose state, return 0 */\n\tstd\tr3, PACA_REQ_PSSCR(r13)\n\n\t/*\n\t * pnv_wakeup_noloss() expects r12 to contain the SRR1 value so\n\t * it can determine if the wakeup reason is an HMI in\n\t * CHECK_HMI_INTERRUPT.\n\t *\n\t * However, when we wakeup with ESL=0, SRR1 will not contain the wakeup\n\t * reason, so there is no point setting r12 to SRR1.\n\t *\n\t * Further, we clear r12 here, so that we don't accidentally enter the\n\t * HMI in pnv_wakeup_noloss() if the value of r12[42:45] == WAKE_HMI.\n\t */\n\tli\tr12, 0\n\tb \tpnv_wakeup_noloss\n\n.Lhandle_esl_ec_set:\nBEGIN_FTR_SECTION\n\t/*\n\t * POWER9 DD2.0 or earlier can incorrectly set PMAO when waking up after\n\t * a state-loss idle. Saving and restoring MMCR0 over idle is a\n\t * workaround.\n\t */\n\tmfspr\tr4,SPRN_MMCR0\n\tstd\tr4,_MMCR0(r1)\nEND_FTR_SECTION_IFCLR(CPU_FTR_POWER9_DD2_1)\n\n/*\n * Check if the requested state is a deep idle state.\n */\n\tLOAD_REG_ADDRBASE(r5,pnv_first_deep_stop_state)\n\tld\tr4,ADDROFF(pnv_first_deep_stop_state)(r5)\n\tcmpd\tr3,r4\n\tbge\t.Lhandle_deep_stop\n\tPPC_STOP\t/* Does not return (system reset interrupt) */\n\n.Lhandle_deep_stop:\n/*\n * Entering deep idle state.\n * Clear thread bit in PACA_CORE_IDLE_STATE, save SPRs to\n * stack and enter stop\n */\n\tlbz     r7,PACA_THREAD_MASK(r13)\n\tld      r14,PACA_CORE_IDLE_STATE_PTR(r13)\n\nlwarx_loop_stop:\n\tlwarx   r15,0,r14\n\tandis.\tr9,r15,PNV_CORE_IDLE_LOCK_BIT@h\n\tbnel-\tcore_idle_lock_held\n\tandc    r15,r15,r7                      /* Clear thread bit */\n\n\tstwcx.  r15,0,r14\n\tbne-    lwarx_loop_stop\n\tisync\n\n\tbl\tsave_sprs_to_stack\n\n\tPPC_STOP\t/* Does not return (system reset interrupt) */\n\n/*\n * Entered with MSR[EE]=0 and no soft-masked interrupts pending.\n * r3 contains desired idle state (PNV_THREAD_NAP/SLEEP/WINKLE).\n */\n_GLOBAL(power7_idle_insn)\n\t/* Now check if user or arch enabled NAP mode */\n\tLOAD_REG_ADDR(r4, pnv_enter_arch207_idle_mode)\n\tb\tpnv_powersave_common\n\n#define CHECK_HMI_INTERRUPT\t\t\t\t\t\t\\\nBEGIN_FTR_SECTION_NESTED(66);\t\t\t\t\t\t\\\n\trlwinm\tr0,r12,45-31,0xf;  /* extract wake reason field (P8) */\t\\\nFTR_SECTION_ELSE_NESTED(66);\t\t\t\t\t\t\\\n\trlwinm\tr0,r12,45-31,0xe;  /* P7 wake reason field is 3 bits */\t\\\nALT_FTR_SECTION_END_NESTED_IFSET(CPU_FTR_ARCH_207S, 66);\t\t\\\n\tcmpwi\tr0,0xa;\t\t\t/* Hypervisor maintenance ? */\t\\\n\tbne+\t20f;\t\t\t\t\t\t\t\\\n\t/* Invoke opal call to handle hmi */\t\t\t\t\\\n\tld\tr2,PACATOC(r13);\t\t\t\t\t\\\n\tld\tr1,PACAR1(r13);\t\t\t\t\t\t\\\n\tstd\tr3,ORIG_GPR3(r1);\t/* Save original r3 */\t\t\\\n\tli\tr3,0;\t\t\t/* NULL argument */\t\t\\\n\tbl\thmi_exception_realmode;\t\t\t\t\t\\\n\tnop;\t\t\t\t\t\t\t\t\\\n\tld\tr3,ORIG_GPR3(r1);\t/* Restore original r3 */\t\\\n20:\tnop;\n\n/*\n * Entered with MSR[EE]=0 and no soft-masked interrupts pending.\n * r3 contains desired PSSCR register value.\n *\n * Offline (CPU unplug) case also must notify KVM that the CPU is\n * idle.\n */\n_GLOBAL(power9_offline_stop)\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\n\t/*\n\t * Tell KVM we're entering idle.\n\t * This does not have to be done in real mode because the P9 MMU\n\t * is independent per-thread. Some steppings share radix/hash mode\n\t * between threads, but in that case KVM has a barrier sync in real\n\t * mode before and after switching between radix and hash.\n\t */\n\tli\tr4,KVM_HWTHREAD_IN_IDLE\n\tstb\tr4,HSTATE_HWTHREAD_STATE(r13)\n#endif\n\t/* fall through */\n\n_GLOBAL(power9_idle_stop)\n\tstd\tr3, PACA_REQ_PSSCR(r13)\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\nBEGIN_FTR_SECTION\n\tsync\n\tlwz\tr5, PACA_DONT_STOP(r13)\n\tcmpwi\tr5, 0\n\tbne\t1f\nEND_FTR_SECTION_IFSET(CPU_FTR_P9_TM_XER_SO_BUG)\n#endif\n\tmtspr \tSPRN_PSSCR,r3\n\tLOAD_REG_ADDR(r4,power_enter_stop)\n\tb\tpnv_powersave_common\n\t/* No return */\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\n1:\n\t/*\n\t * We get here when TM / thread reconfiguration bug workaround\n\t * code wants to get the CPU into SMT4 mode, and therefore\n\t * we are being asked not to stop.\n\t */\n\tli\tr3, 0\n\tstd\tr3, PACA_REQ_PSSCR(r13)\n\tblr\t\t/* return 0 for wakeup cause / SRR1 value */\n#endif\n\n/*\n * Called from machine check handler for powersave wakeups.\n * Low level machine check processing has already been done. Now just\n * go through the wake up path to get everything in order.\n *\n * r3 - The original SRR1 value.\n * Original SRR[01] have been clobbered.\n * MSR_RI is clear.\n */\n.global pnv_powersave_wakeup_mce\npnv_powersave_wakeup_mce:\n\t/* Set cr3 for pnv_powersave_wakeup */\n\trlwinm\tr11,r3,47-31,30,31\n\tcmpwi\tcr3,r11,2\n\n\t/*\n\t * Now put the original SRR1 with SRR1_WAKEMCE_RESVD as the wake\n\t * reason into r12, which allows reuse of the system reset wakeup\n\t * code without being mistaken for another type of wakeup.\n\t */\n\toris\tr12,r3,SRR1_WAKEMCE_RESVD@h\n\n\tb\tpnv_powersave_wakeup\n\n/*\n * Called from reset vector for powersave wakeups.\n * cr3 - set to gt if waking up with partial/complete hypervisor state loss\n * r12 - SRR1\n */\n.global pnv_powersave_wakeup\npnv_powersave_wakeup:\n\tld\tr2, PACATOC(r13)\n\nBEGIN_FTR_SECTION\n\tbl\tpnv_restore_hyp_resource_arch300\nFTR_SECTION_ELSE\n\tbl\tpnv_restore_hyp_resource_arch207\nALT_FTR_SECTION_END_IFSET(CPU_FTR_ARCH_300)\n\n\tli\tr0,PNV_THREAD_RUNNING\n\tstb\tr0,PACA_THREAD_IDLE_STATE(r13)\t/* Clear thread state */\n\n\tmr\tr3,r12\n\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\n\tlbz\tr0,HSTATE_HWTHREAD_STATE(r13)\n\tcmpwi\tr0,KVM_HWTHREAD_IN_KERNEL\n\tbeq\t0f\n\tli\tr0,KVM_HWTHREAD_IN_KERNEL\n\tstb\tr0,HSTATE_HWTHREAD_STATE(r13)\n\t/* Order setting hwthread_state vs. testing hwthread_req */\n\tsync\n0:\tlbz\tr0,HSTATE_HWTHREAD_REQ(r13)\n\tcmpwi\tr0,0\n\tbeq\t1f\n\tb\tkvm_start_guest\n1:\n#endif\n\n\t/* Return SRR1 from power7_nap() */\n\tblt\tcr3,pnv_wakeup_noloss\n\tb\tpnv_wakeup_loss\n\n/*\n * Check whether we have woken up with hypervisor state loss.\n * If yes, restore hypervisor state and return back to link.\n *\n * cr3 - set to gt if waking up with partial/complete hypervisor state loss\n */\npnv_restore_hyp_resource_arch300:\n\t/*\n\t * Workaround for POWER9, if we lost resources, the ERAT\n\t * might have been mixed up and needs flushing. We also need\n\t * to reload MMCR0 (see comment above). We also need to set\n\t * then clear bit 60 in MMCRA to ensure the PMU starts running.\n\t */\n\tblt\tcr3,1f\nBEGIN_FTR_SECTION\n\tPPC_INVALIDATE_ERAT\n\tld\tr1,PACAR1(r13)\n\tld\tr4,_MMCR0(r1)\n\tmtspr\tSPRN_MMCR0,r4\nEND_FTR_SECTION_IFCLR(CPU_FTR_POWER9_DD2_1)\n\tmfspr\tr4,SPRN_MMCRA\n\tori\tr4,r4,(1 << (63-60))\n\tmtspr\tSPRN_MMCRA,r4\n\txori\tr4,r4,(1 << (63-60))\n\tmtspr\tSPRN_MMCRA,r4\n1:\n\t/*\n\t * POWER ISA 3. Use PSSCR to determine if we\n\t * are waking up from deep idle state\n\t */\n\tLOAD_REG_ADDRBASE(r5,pnv_first_deep_stop_state)\n\tld\tr4,ADDROFF(pnv_first_deep_stop_state)(r5)\n\n\t/*\n\t * 0-3 bits correspond to Power-Saving Level Status\n\t * which indicates the idle state we are waking up from\n\t */\n\tmfspr\tr5, SPRN_PSSCR\n\trldicl  r5,r5,4,60\n\tli\tr0, 0\t\t/* clear requested_psscr to say we're awake */\n\tstd\tr0, PACA_REQ_PSSCR(r13)\n\tcmpd\tcr4,r5,r4\n\tbge\tcr4,pnv_wakeup_tb_loss /* returns to caller */\n\n\tblr\t/* Waking up without hypervisor state loss. */\n\n/* Same calling convention as arch300 */\npnv_restore_hyp_resource_arch207:\n\t/*\n\t * POWER ISA 2.07 or less.\n\t * Check if we slept with sleep or winkle.\n\t */\n\tlbz\tr4,PACA_THREAD_IDLE_STATE(r13)\n\tcmpwi\tcr2,r4,PNV_THREAD_NAP\n\tbgt\tcr2,pnv_wakeup_tb_loss\t/* Either sleep or Winkle */\n\n\t/*\n\t * We fall through here if PACA_THREAD_IDLE_STATE shows we are waking\n\t * up from nap. At this stage CR3 shouldn't contains 'gt' since that\n\t * indicates we are waking with hypervisor state loss from nap.\n\t */\n\tbgt\tcr3,.\n\n\tblr\t/* Waking up without hypervisor state loss */\n\n/*\n * Called if waking up from idle state which can cause either partial or\n * complete hyp state loss.\n * In POWER8, called if waking up from fastsleep or winkle\n * In POWER9, called if waking up from stop state >= pnv_first_deep_stop_state\n *\n * r13 - PACA\n * cr3 - gt if waking up with partial/complete hypervisor state loss\n *\n * If ISA300:\n * cr4 - gt or eq if waking up from complete hypervisor state loss.\n *\n * If ISA207:\n * r4 - PACA_THREAD_IDLE_STATE\n */\npnv_wakeup_tb_loss:\n\tld\tr1,PACAR1(r13)\n\t/*\n\t * Before entering any idle state, the NVGPRs are saved in the stack.\n\t * If there was a state loss, or PACA_NAPSTATELOST was set, then the\n\t * NVGPRs are restored. If we are here, it is likely that state is lost,\n\t * but not guaranteed -- neither ISA207 nor ISA300 tests to reach\n\t * here are the same as the test to restore NVGPRS:\n\t * PACA_THREAD_IDLE_STATE test for ISA207, PSSCR test for ISA300,\n\t * and SRR1 test for restoring NVGPRs.\n\t *\n\t * We are about to clobber NVGPRs now, so set NAPSTATELOST to\n\t * guarantee they will always be restored. This might be tightened\n\t * with careful reading of specs (particularly for ISA300) but this\n\t * is already a slow wakeup path and it's simpler to be safe.\n\t */\n\tli\tr0,1\n\tstb\tr0,PACA_NAPSTATELOST(r13)\n\n\t/*\n\t *\n\t * Save SRR1 and LR in NVGPRs as they might be clobbered in\n\t * opal_call() (called in CHECK_HMI_INTERRUPT). SRR1 is required\n\t * to determine the wakeup reason if we branch to kvm_start_guest. LR\n\t * is required to return back to reset vector after hypervisor state\n\t * restore is complete.\n\t */\n\tmr\tr19,r12\n\tmr\tr18,r4\n\tmflr\tr17\nBEGIN_FTR_SECTION\n\tCHECK_HMI_INTERRUPT\nEND_FTR_SECTION_IFSET(CPU_FTR_HVMODE)\n\n\tld\tr14,PACA_CORE_IDLE_STATE_PTR(r13)\n\tlbz\tr7,PACA_THREAD_MASK(r13)\n\n\t/*\n\t * Take the core lock to synchronize against other threads.\n\t *\n\t * Lock bit is set in one of the 2 cases-\n\t * a. In the sleep/winkle enter path, the last thread is executing\n\t * fastsleep workaround code.\n\t * b. In the wake up path, another thread is executing fastsleep\n\t * workaround undo code or resyncing timebase or restoring context\n\t * In either case loop until the lock bit is cleared.\n\t */\n1:\n\tlwarx\tr15,0,r14\n\tandis.\tr9,r15,PNV_CORE_IDLE_LOCK_BIT@h\n\tbnel-\tcore_idle_lock_held\n\toris\tr15,r15,PNV_CORE_IDLE_LOCK_BIT@h\n\tstwcx.\tr15,0,r14\n\tbne-\t1b\n\tisync\n\n\tandi.\tr9,r15,PNV_CORE_IDLE_THREAD_BITS\n\tcmpwi\tcr2,r9,0\n\n\t/*\n\t * At this stage\n\t * cr2 - eq if first thread to wakeup in core\n\t * cr3-  gt if waking up with partial/complete hypervisor state loss\n\t * ISA300:\n\t * cr4 - gt or eq if waking up from complete hypervisor state loss.\n\t */\n\nBEGIN_FTR_SECTION\n\t/*\n\t * Were we in winkle?\n\t * If yes, check if all threads were in winkle, decrement our\n\t * winkle count, set all thread winkle bits if all were in winkle.\n\t * Check if our thread has a winkle bit set, and set cr4 accordingly\n\t * (to match ISA300, above). Pseudo-code for core idle state\n\t * transitions for ISA207 is as follows (everything happens atomically\n\t * due to store conditional and/or lock bit):\n\t *\n\t * nap_idle() { }\n\t * nap_wake() { }\n\t *\n\t * sleep_idle()\n\t * {\n\t *\tcore_idle_state &= ~thread_in_core\n\t * }\n\t *\n\t * sleep_wake()\n\t * {\n\t *     bool first_in_core, first_in_subcore;\n\t *\n\t *     first_in_core = (core_idle_state & IDLE_THREAD_BITS) == 0;\n\t *     first_in_subcore = (core_idle_state & SUBCORE_SIBLING_MASK) == 0;\n\t *\n\t *     core_idle_state |= thread_in_core;\n\t * }\n\t *\n\t * winkle_idle()\n\t * {\n\t *\tcore_idle_state &= ~thread_in_core;\n\t *\tcore_idle_state += 1 << WINKLE_COUNT_SHIFT;\n\t * }\n\t *\n\t * winkle_wake()\n\t * {\n\t *     bool first_in_core, first_in_subcore, winkle_state_lost;\n\t *\n\t *     first_in_core = (core_idle_state & IDLE_THREAD_BITS) == 0;\n\t *     first_in_subcore = (core_idle_state & SUBCORE_SIBLING_MASK) == 0;\n\t *\n\t *     core_idle_state |= thread_in_core;\n\t *\n\t *     if ((core_idle_state & WINKLE_MASK) == (8 << WINKLE_COUNT_SIHFT))\n\t *         core_idle_state |= THREAD_WINKLE_BITS;\n\t *     core_idle_state -= 1 << WINKLE_COUNT_SHIFT;\n\t *\n\t *     winkle_state_lost = core_idle_state &\n\t *\t\t\t\t(thread_in_core << WINKLE_THREAD_SHIFT);\n\t *     core_idle_state &= ~(thread_in_core << WINKLE_THREAD_SHIFT);\n\t * }\n\t *\n\t */\n\tcmpwi\tr18,PNV_THREAD_WINKLE\n\tbne\t2f\n\tandis.\tr9,r15,PNV_CORE_IDLE_WINKLE_COUNT_ALL_BIT@h\n\tsubis\tr15,r15,PNV_CORE_IDLE_WINKLE_COUNT@h\n\tbeq\t2f\n\tori\tr15,r15,PNV_CORE_IDLE_THREAD_WINKLE_BITS /* all were winkle */\n2:\n\t/* Shift thread bit to winkle mask, then test if this thread is set,\n\t * and remove it from the winkle bits */\n\tslwi\tr8,r7,8\n\tand\tr8,r8,r15\n\tandc\tr15,r15,r8\n\tcmpwi\tcr4,r8,1 /* cr4 will be gt if our bit is set, lt if not */\n\n\tlbz\tr4,PACA_SUBCORE_SIBLING_MASK(r13)\n\tand\tr4,r4,r15\n\tcmpwi\tr4,0\t/* Check if first in subcore */\n\n\tor\tr15,r15,r7\t\t/* Set thread bit */\n\tbeq\tfirst_thread_in_subcore\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_300)\n\n\tor\tr15,r15,r7\t\t/* Set thread bit */\n\tbeq\tcr2,first_thread_in_core\n\n\t/* Not first thread in core or subcore to wake up */\n\tb\tclear_lock\n\nfirst_thread_in_subcore:\n\t/*\n\t * If waking up from sleep, subcore state is not lost. Hence\n\t * skip subcore state restore\n\t */\n\tblt\tcr4,subcore_state_restored\n\n\t/* Restore per-subcore state */\n\tld      r4,_SDR1(r1)\n\tmtspr   SPRN_SDR1,r4\n\n\tld      r4,_RPR(r1)\n\tmtspr   SPRN_RPR,r4\n\tld\tr4,_AMOR(r1)\n\tmtspr\tSPRN_AMOR,r4\n\nsubcore_state_restored:\n\t/*\n\t * Check if the thread is also the first thread in the core. If not,\n\t * skip to clear_lock.\n\t */\n\tbne\tcr2,clear_lock\n\nfirst_thread_in_core:\n\n\t/*\n\t * First thread in the core waking up from any state which can cause\n\t * partial or complete hypervisor state loss. It needs to\n\t * call the fastsleep workaround code if the platform requires it.\n\t * Call it unconditionally here. The below branch instruction will\n\t * be patched out if the platform does not have fastsleep or does not\n\t * require the workaround. Patching will be performed during the\n\t * discovery of idle-states.\n\t */\n.global pnv_fastsleep_workaround_at_exit\npnv_fastsleep_workaround_at_exit:\n\tb\tfastsleep_workaround_at_exit\n\ntimebase_resync:\n\t/*\n\t * Use cr3 which indicates that we are waking up with atleast partial\n\t * hypervisor state loss to determine if TIMEBASE RESYNC is needed.\n\t */\n\tble\tcr3,.Ltb_resynced\n\t/* Time base re-sync */\n\tbl\topal_resync_timebase;\n\t/*\n\t * If waking up from sleep (POWER8), per core state\n\t * is not lost, skip to clear_lock.\n\t */\n.Ltb_resynced:\n\tblt\tcr4,clear_lock\n\n\t/*\n\t * First thread in the core to wake up and its waking up with\n\t * complete hypervisor state loss. Restore per core hypervisor\n\t * state.\n\t */\nBEGIN_FTR_SECTION\n\tld\tr4,_PTCR(r1)\n\tmtspr\tSPRN_PTCR,r4\n\tld\tr4,_RPR(r1)\n\tmtspr\tSPRN_RPR,r4\n\tld\tr4,_AMOR(r1)\n\tmtspr\tSPRN_AMOR,r4\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_300)\n\n\tld\tr4,_TSCR(r1)\n\tmtspr\tSPRN_TSCR,r4\n\tld\tr4,_WORC(r1)\n\tmtspr\tSPRN_WORC,r4\n\nclear_lock:\n\txoris\tr15,r15,PNV_CORE_IDLE_LOCK_BIT@h\n\tlwsync\n\tstw\tr15,0(r14)\n\ncommon_exit:\n\t/*\n\t * Common to all threads.\n\t *\n\t * If waking up from sleep, hypervisor state is not lost. Hence\n\t * skip hypervisor state restore.\n\t */\n\tblt\tcr4,hypervisor_state_restored\n\n\t/* Waking up from winkle */\n\nBEGIN_MMU_FTR_SECTION\n\tb\tno_segments\nEND_MMU_FTR_SECTION_IFSET(MMU_FTR_TYPE_RADIX)\n\t/* Restore SLB  from PACA */\n\tld\tr8,PACA_SLBSHADOWPTR(r13)\n\n\t.rept\tSLB_NUM_BOLTED\n\tli\tr3, SLBSHADOW_SAVEAREA\n\tLDX_BE\tr5, r8, r3\n\taddi\tr3, r3, 8\n\tLDX_BE\tr6, r8, r3\n\tandis.\tr7,r5,SLB_ESID_V@h\n\tbeq\t1f\n\tslbmte\tr6,r5\n1:\taddi\tr8,r8,16\n\t.endr\nno_segments:\n\n\t/* Restore per thread state */\n\n\tld\tr4,_SPURR(r1)\n\tmtspr\tSPRN_SPURR,r4\n\tld\tr4,_PURR(r1)\n\tmtspr\tSPRN_PURR,r4\n\tld\tr4,_DSCR(r1)\n\tmtspr\tSPRN_DSCR,r4\n\tld\tr4,_WORT(r1)\n\tmtspr\tSPRN_WORT,r4\n\n\t/* Call cur_cpu_spec->cpu_restore() */\n\tLOAD_REG_ADDR(r4, cur_cpu_spec)\n\tld\tr4,0(r4)\n\tld\tr12,CPU_SPEC_RESTORE(r4)\n#ifdef PPC64_ELF_ABI_v1\n\tld\tr12,0(r12)\n#endif\n\tmtctr\tr12\n\tbctrl\n\n/*\n * On POWER9, we can come here on wakeup from a cpuidle stop state.\n * Hence restore the additional SPRs to the saved value.\n *\n * On POWER8, we come here only on winkle. Since winkle is used\n * only in the case of CPU-Hotplug, we don't need to restore\n * the additional SPRs.\n */\nBEGIN_FTR_SECTION\n\tbl \tpower9_restore_additional_sprs\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_300)\nhypervisor_state_restored:\n\n\tmr\tr12,r19\n\tmtlr\tr17\n\tblr\t\t/* return to pnv_powersave_wakeup */\n\nfastsleep_workaround_at_exit:\n\tli\tr3,1\n\tli\tr4,0\n\tbl\topal_config_cpu_idle_state\n\tb\ttimebase_resync\n\n/*\n * R3 here contains the value that will be returned to the caller\n * of power7_nap.\n * R12 contains SRR1 for CHECK_HMI_INTERRUPT.\n */\n.global pnv_wakeup_loss\npnv_wakeup_loss:\n\tld\tr1,PACAR1(r13)\nBEGIN_FTR_SECTION\n\tCHECK_HMI_INTERRUPT\nEND_FTR_SECTION_IFSET(CPU_FTR_HVMODE)\n\tREST_NVGPRS(r1)\n\tREST_GPR(2, r1)\n\nBEGIN_FTR_SECTION\n\t/* IAMR was saved in pnv_powersave_common() */\n\tld\tr5, PNV_POWERSAVE_IAMR(r1)\n\tmtspr\tSPRN_IAMR, r5\n\t/*\n\t * We don't need an isync here because the upcoming mtmsrd is\n\t * execution synchronizing.\n\t */\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\n\tld\tr4,PACAKMSR(r13)\n\tld\tr5,_LINK(r1)\n\tld\tr6,_CCR(r1)\n\taddi\tr1,r1,INT_FRAME_SIZE\n\tmtlr\tr5\n\tmtcr\tr6\n\tmtmsrd\tr4\n\tblr\n\n/*\n * R3 here contains the value that will be returned to the caller\n * of power7_nap.\n * R12 contains SRR1 for CHECK_HMI_INTERRUPT.\n */\npnv_wakeup_noloss:\n\tlbz\tr0,PACA_NAPSTATELOST(r13)\n\tcmpwi\tr0,0\n\tbne\tpnv_wakeup_loss\n\tld\tr1,PACAR1(r13)\nBEGIN_FTR_SECTION\n\tCHECK_HMI_INTERRUPT\nEND_FTR_SECTION_IFSET(CPU_FTR_HVMODE)\n\tld\tr4,PACAKMSR(r13)\n\tld\tr5,_NIP(r1)\n\tld\tr6,_CCR(r1)\n\taddi\tr1,r1,INT_FRAME_SIZE\n\tmtlr\tr5\n\tmtcr\tr6\n\tmtmsrd\tr4\n\tblr\n"], "fixing_code": ["/*\n *  This file contains idle entry/exit functions for POWER7,\n *  POWER8 and POWER9 CPUs.\n *\n *  This program is free software; you can redistribute it and/or\n *  modify it under the terms of the GNU General Public License\n *  as published by the Free Software Foundation; either version\n *  2 of the License, or (at your option) any later version.\n */\n\n#include <linux/threads.h>\n#include <asm/processor.h>\n#include <asm/page.h>\n#include <asm/cputable.h>\n#include <asm/thread_info.h>\n#include <asm/ppc_asm.h>\n#include <asm/asm-offsets.h>\n#include <asm/ppc-opcode.h>\n#include <asm/hw_irq.h>\n#include <asm/kvm_book3s_asm.h>\n#include <asm/opal.h>\n#include <asm/cpuidle.h>\n#include <asm/exception-64s.h>\n#include <asm/book3s/64/mmu-hash.h>\n#include <asm/mmu.h>\n#include <asm/asm-compat.h>\n#include <asm/feature-fixups.h>\n\n#undef DEBUG\n\n/*\n * Use unused space in the interrupt stack to save and restore\n * registers for winkle support.\n */\n#define _MMCR0\tGPR0\n#define _SDR1\tGPR3\n#define _PTCR\tGPR3\n#define _RPR\tGPR4\n#define _SPURR\tGPR5\n#define _PURR\tGPR6\n#define _TSCR\tGPR7\n#define _DSCR\tGPR8\n#define _AMOR\tGPR9\n#define _WORT\tGPR10\n#define _WORC\tGPR11\n#define _LPCR\tGPR12\n\n#define PSSCR_EC_ESL_MASK_SHIFTED          (PSSCR_EC | PSSCR_ESL) >> 16\n\n\t.text\n\n/*\n * Used by threads before entering deep idle states. Saves SPRs\n * in interrupt stack frame\n */\nsave_sprs_to_stack:\n\t/*\n\t * Note all register i.e per-core, per-subcore or per-thread is saved\n\t * here since any thread in the core might wake up first\n\t */\nBEGIN_FTR_SECTION\n\t/*\n\t * Note - SDR1 is dropped in Power ISA v3. Hence not restoring\n\t * SDR1 here\n\t */\n\tmfspr\tr3,SPRN_PTCR\n\tstd\tr3,_PTCR(r1)\n\tmfspr\tr3,SPRN_LPCR\n\tstd\tr3,_LPCR(r1)\nFTR_SECTION_ELSE\n\tmfspr\tr3,SPRN_SDR1\n\tstd\tr3,_SDR1(r1)\nALT_FTR_SECTION_END_IFSET(CPU_FTR_ARCH_300)\n\tmfspr\tr3,SPRN_RPR\n\tstd\tr3,_RPR(r1)\n\tmfspr\tr3,SPRN_SPURR\n\tstd\tr3,_SPURR(r1)\n\tmfspr\tr3,SPRN_PURR\n\tstd\tr3,_PURR(r1)\n\tmfspr\tr3,SPRN_TSCR\n\tstd\tr3,_TSCR(r1)\n\tmfspr\tr3,SPRN_DSCR\n\tstd\tr3,_DSCR(r1)\n\tmfspr\tr3,SPRN_AMOR\n\tstd\tr3,_AMOR(r1)\n\tmfspr\tr3,SPRN_WORT\n\tstd\tr3,_WORT(r1)\n\tmfspr\tr3,SPRN_WORC\n\tstd\tr3,_WORC(r1)\n/*\n * On POWER9, there are idle states such as stop4, invoked via cpuidle,\n * that lose hypervisor resources. In such cases, we need to save\n * additional SPRs before entering those idle states so that they can\n * be restored to their older values on wakeup from the idle state.\n *\n * On POWER8, the only such deep idle state is winkle which is used\n * only in the context of CPU-Hotplug, where these additional SPRs are\n * reinitiazed to a sane value. Hence there is no need to save/restore\n * these SPRs.\n */\nBEGIN_FTR_SECTION\n\tblr\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_300)\n\npower9_save_additional_sprs:\n\tmfspr\tr3, SPRN_PID\n\tmfspr\tr4, SPRN_LDBAR\n\tstd\tr3, STOP_PID(r13)\n\tstd\tr4, STOP_LDBAR(r13)\n\n\tmfspr\tr3, SPRN_FSCR\n\tmfspr\tr4, SPRN_HFSCR\n\tstd\tr3, STOP_FSCR(r13)\n\tstd\tr4, STOP_HFSCR(r13)\n\n\tmfspr\tr3, SPRN_MMCRA\n\tmfspr\tr4, SPRN_MMCR0\n\tstd\tr3, STOP_MMCRA(r13)\n\tstd\tr4, _MMCR0(r1)\n\n\tmfspr\tr3, SPRN_MMCR1\n\tmfspr\tr4, SPRN_MMCR2\n\tstd\tr3, STOP_MMCR1(r13)\n\tstd\tr4, STOP_MMCR2(r13)\n\tblr\n\npower9_restore_additional_sprs:\n\tld\tr3,_LPCR(r1)\n\tld\tr4, STOP_PID(r13)\n\tmtspr\tSPRN_LPCR,r3\n\tmtspr\tSPRN_PID, r4\n\n\tld\tr3, STOP_LDBAR(r13)\n\tld\tr4, STOP_FSCR(r13)\n\tmtspr\tSPRN_LDBAR, r3\n\tmtspr\tSPRN_FSCR, r4\n\n\tld\tr3, STOP_HFSCR(r13)\n\tld\tr4, STOP_MMCRA(r13)\n\tmtspr\tSPRN_HFSCR, r3\n\tmtspr\tSPRN_MMCRA, r4\n\n\tld\tr3, _MMCR0(r1)\n\tld\tr4, STOP_MMCR1(r13)\n\tmtspr\tSPRN_MMCR0, r3\n\tmtspr\tSPRN_MMCR1, r4\n\n\tld\tr3, STOP_MMCR2(r13)\n\tld\tr4, PACA_SPRG_VDSO(r13)\n\tmtspr\tSPRN_MMCR2, r3\n\tmtspr\tSPRN_SPRG3, r4\n\tblr\n\n/*\n * Used by threads when the lock bit of core_idle_state is set.\n * Threads will spin in HMT_LOW until the lock bit is cleared.\n * r14 - pointer to core_idle_state\n * r15 - used to load contents of core_idle_state\n * r9  - used as a temporary variable\n */\n\ncore_idle_lock_held:\n\tHMT_LOW\n3:\tlwz\tr15,0(r14)\n\tandis.\tr15,r15,PNV_CORE_IDLE_LOCK_BIT@h\n\tbne\t3b\n\tHMT_MEDIUM\n\tlwarx\tr15,0,r14\n\tandis.\tr9,r15,PNV_CORE_IDLE_LOCK_BIT@h\n\tbne-\tcore_idle_lock_held\n\tblr\n\n/* Reuse some unused pt_regs slots for AMR/IAMR/UAMOR/UAMOR */\n#define PNV_POWERSAVE_AMR\t_TRAP\n#define PNV_POWERSAVE_IAMR\t_DAR\n#define PNV_POWERSAVE_UAMOR\t_DSISR\n#define PNV_POWERSAVE_AMOR\tRESULT\n\n/*\n * Pass requested state in r3:\n *\tr3 - PNV_THREAD_NAP/SLEEP/WINKLE in POWER8\n *\t   - Requested PSSCR value in POWER9\n *\n * Address of idle handler to branch to in realmode in r4\n */\npnv_powersave_common:\n\t/* Use r3 to pass state nap/sleep/winkle */\n\t/* NAP is a state loss, we create a regs frame on the\n\t * stack, fill it up with the state we care about and\n\t * stick a pointer to it in PACAR1. We really only\n\t * need to save PC, some CR bits and the NV GPRs,\n\t * but for now an interrupt frame will do.\n\t */\n\tmtctr\tr4\n\n\tmflr\tr0\n\tstd\tr0,16(r1)\n\tstdu\tr1,-INT_FRAME_SIZE(r1)\n\tstd\tr0,_LINK(r1)\n\tstd\tr0,_NIP(r1)\n\n\t/* We haven't lost state ... yet */\n\tli\tr0,0\n\tstb\tr0,PACA_NAPSTATELOST(r13)\n\n\t/* Continue saving state */\n\tSAVE_GPR(2, r1)\n\tSAVE_NVGPRS(r1)\n\nBEGIN_FTR_SECTION\n\tmfspr\tr4, SPRN_AMR\n\tmfspr\tr5, SPRN_IAMR\n\tmfspr\tr6, SPRN_UAMOR\n\tstd\tr4, PNV_POWERSAVE_AMR(r1)\n\tstd\tr5, PNV_POWERSAVE_IAMR(r1)\n\tstd\tr6, PNV_POWERSAVE_UAMOR(r1)\nBEGIN_FTR_SECTION_NESTED(42)\n\tmfspr\tr7, SPRN_AMOR\n\tstd\tr7, PNV_POWERSAVE_AMOR(r1)\nEND_FTR_SECTION_NESTED_IFSET(CPU_FTR_HVMODE, 42)\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\n\tmfcr\tr5\n\tstd\tr5,_CCR(r1)\n\tstd\tr1,PACAR1(r13)\n\nBEGIN_FTR_SECTION\n\t/*\n\t * POWER9 does not require real mode to stop, and presently does not\n\t * set hwthread_state for KVM (threads don't share MMU context), so\n\t * we can remain in virtual mode for this.\n\t */\n\tbctr\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_300)\n\t/*\n\t * POWER8\n\t * Go to real mode to do the nap, as required by the architecture.\n\t * Also, we need to be in real mode before setting hwthread_state,\n\t * because as soon as we do that, another thread can switch\n\t * the MMU context to the guest.\n\t */\n\tLOAD_REG_IMMEDIATE(r7, MSR_IDLE)\n\tmtmsrd\tr7,0\n\tbctr\n\n/*\n * This is the sequence required to execute idle instructions, as\n * specified in ISA v2.07 (and earlier). MSR[IR] and MSR[DR] must be 0.\n */\n#define IDLE_STATE_ENTER_SEQ_NORET(IDLE_INST)\t\t\t\\\n\t/* Magic NAP/SLEEP/WINKLE mode enter sequence */\t\\\n\tstd\tr0,0(r1);\t\t\t\t\t\\\n\tptesync;\t\t\t\t\t\t\\\n\tld\tr0,0(r1);\t\t\t\t\t\\\n236:\tcmpd\tcr0,r0,r0;\t\t\t\t\t\\\n\tbne\t236b;\t\t\t\t\t\t\\\n\tIDLE_INST;\n\n\n\t.globl pnv_enter_arch207_idle_mode\npnv_enter_arch207_idle_mode:\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\n\t/* Tell KVM we're entering idle */\n\tli\tr4,KVM_HWTHREAD_IN_IDLE\n\t/******************************************************/\n\t/*  N O T E   W E L L    ! ! !    N O T E   W E L L   */\n\t/* The following store to HSTATE_HWTHREAD_STATE(r13)  */\n\t/* MUST occur in real mode, i.e. with the MMU off,    */\n\t/* and the MMU must stay off until we clear this flag */\n\t/* and test HSTATE_HWTHREAD_REQ(r13) in               */\n\t/* pnv_powersave_wakeup in this file.                 */\n\t/* The reason is that another thread can switch the   */\n\t/* MMU to a guest context whenever this flag is set   */\n\t/* to KVM_HWTHREAD_IN_IDLE, and if the MMU was on,    */\n\t/* that would potentially cause this thread to start  */\n\t/* executing instructions from guest memory in        */\n\t/* hypervisor mode, leading to a host crash or data   */\n\t/* corruption, or worse.                              */\n\t/******************************************************/\n\tstb\tr4,HSTATE_HWTHREAD_STATE(r13)\n#endif\n\tstb\tr3,PACA_THREAD_IDLE_STATE(r13)\n\tcmpwi\tcr3,r3,PNV_THREAD_SLEEP\n\tbge\tcr3,2f\n\tIDLE_STATE_ENTER_SEQ_NORET(PPC_NAP)\n\t/* No return */\n2:\n\t/* Sleep or winkle */\n\tlbz\tr7,PACA_THREAD_MASK(r13)\n\tld\tr14,PACA_CORE_IDLE_STATE_PTR(r13)\n\tli\tr5,0\n\tbeq\tcr3,3f\n\tlis\tr5,PNV_CORE_IDLE_WINKLE_COUNT@h\n3:\nlwarx_loop1:\n\tlwarx\tr15,0,r14\n\n\tandis.\tr9,r15,PNV_CORE_IDLE_LOCK_BIT@h\n\tbnel-\tcore_idle_lock_held\n\n\tadd\tr15,r15,r5\t\t\t/* Add if winkle */\n\tandc\tr15,r15,r7\t\t\t/* Clear thread bit */\n\n\tandi.\tr9,r15,PNV_CORE_IDLE_THREAD_BITS\n\n/*\n * If cr0 = 0, then current thread is the last thread of the core entering\n * sleep. Last thread needs to execute the hardware bug workaround code if\n * required by the platform.\n * Make the workaround call unconditionally here. The below branch call is\n * patched out when the idle states are discovered if the platform does not\n * require it.\n */\n.global pnv_fastsleep_workaround_at_entry\npnv_fastsleep_workaround_at_entry:\n\tbeq\tfastsleep_workaround_at_entry\n\n\tstwcx.\tr15,0,r14\n\tbne-\tlwarx_loop1\n\tisync\n\ncommon_enter: /* common code for all the threads entering sleep or winkle */\n\tbgt\tcr3,enter_winkle\n\tIDLE_STATE_ENTER_SEQ_NORET(PPC_SLEEP)\n\nfastsleep_workaround_at_entry:\n\toris\tr15,r15,PNV_CORE_IDLE_LOCK_BIT@h\n\tstwcx.\tr15,0,r14\n\tbne-\tlwarx_loop1\n\tisync\n\n\t/* Fast sleep workaround */\n\tli\tr3,1\n\tli\tr4,1\n\tbl\topal_config_cpu_idle_state\n\n\t/* Unlock */\n\txoris\tr15,r15,PNV_CORE_IDLE_LOCK_BIT@h\n\tlwsync\n\tstw\tr15,0(r14)\n\tb\tcommon_enter\n\nenter_winkle:\n\tbl\tsave_sprs_to_stack\n\n\tIDLE_STATE_ENTER_SEQ_NORET(PPC_WINKLE)\n\n/*\n * r3 - PSSCR value corresponding to the requested stop state.\n */\npower_enter_stop:\n/*\n * Check if we are executing the lite variant with ESL=EC=0\n */\n\tandis.   r4,r3,PSSCR_EC_ESL_MASK_SHIFTED\n\tclrldi   r3,r3,60 /* r3 = Bits[60:63] = Requested Level (RL) */\n\tbne\t .Lhandle_esl_ec_set\n\tPPC_STOP\n\tli\tr3,0  /* Since we didn't lose state, return 0 */\n\tstd\tr3, PACA_REQ_PSSCR(r13)\n\n\t/*\n\t * pnv_wakeup_noloss() expects r12 to contain the SRR1 value so\n\t * it can determine if the wakeup reason is an HMI in\n\t * CHECK_HMI_INTERRUPT.\n\t *\n\t * However, when we wakeup with ESL=0, SRR1 will not contain the wakeup\n\t * reason, so there is no point setting r12 to SRR1.\n\t *\n\t * Further, we clear r12 here, so that we don't accidentally enter the\n\t * HMI in pnv_wakeup_noloss() if the value of r12[42:45] == WAKE_HMI.\n\t */\n\tli\tr12, 0\n\tb \tpnv_wakeup_noloss\n\n.Lhandle_esl_ec_set:\nBEGIN_FTR_SECTION\n\t/*\n\t * POWER9 DD2.0 or earlier can incorrectly set PMAO when waking up after\n\t * a state-loss idle. Saving and restoring MMCR0 over idle is a\n\t * workaround.\n\t */\n\tmfspr\tr4,SPRN_MMCR0\n\tstd\tr4,_MMCR0(r1)\nEND_FTR_SECTION_IFCLR(CPU_FTR_POWER9_DD2_1)\n\n/*\n * Check if the requested state is a deep idle state.\n */\n\tLOAD_REG_ADDRBASE(r5,pnv_first_deep_stop_state)\n\tld\tr4,ADDROFF(pnv_first_deep_stop_state)(r5)\n\tcmpd\tr3,r4\n\tbge\t.Lhandle_deep_stop\n\tPPC_STOP\t/* Does not return (system reset interrupt) */\n\n.Lhandle_deep_stop:\n/*\n * Entering deep idle state.\n * Clear thread bit in PACA_CORE_IDLE_STATE, save SPRs to\n * stack and enter stop\n */\n\tlbz     r7,PACA_THREAD_MASK(r13)\n\tld      r14,PACA_CORE_IDLE_STATE_PTR(r13)\n\nlwarx_loop_stop:\n\tlwarx   r15,0,r14\n\tandis.\tr9,r15,PNV_CORE_IDLE_LOCK_BIT@h\n\tbnel-\tcore_idle_lock_held\n\tandc    r15,r15,r7                      /* Clear thread bit */\n\n\tstwcx.  r15,0,r14\n\tbne-    lwarx_loop_stop\n\tisync\n\n\tbl\tsave_sprs_to_stack\n\n\tPPC_STOP\t/* Does not return (system reset interrupt) */\n\n/*\n * Entered with MSR[EE]=0 and no soft-masked interrupts pending.\n * r3 contains desired idle state (PNV_THREAD_NAP/SLEEP/WINKLE).\n */\n_GLOBAL(power7_idle_insn)\n\t/* Now check if user or arch enabled NAP mode */\n\tLOAD_REG_ADDR(r4, pnv_enter_arch207_idle_mode)\n\tb\tpnv_powersave_common\n\n#define CHECK_HMI_INTERRUPT\t\t\t\t\t\t\\\nBEGIN_FTR_SECTION_NESTED(66);\t\t\t\t\t\t\\\n\trlwinm\tr0,r12,45-31,0xf;  /* extract wake reason field (P8) */\t\\\nFTR_SECTION_ELSE_NESTED(66);\t\t\t\t\t\t\\\n\trlwinm\tr0,r12,45-31,0xe;  /* P7 wake reason field is 3 bits */\t\\\nALT_FTR_SECTION_END_NESTED_IFSET(CPU_FTR_ARCH_207S, 66);\t\t\\\n\tcmpwi\tr0,0xa;\t\t\t/* Hypervisor maintenance ? */\t\\\n\tbne+\t20f;\t\t\t\t\t\t\t\\\n\t/* Invoke opal call to handle hmi */\t\t\t\t\\\n\tld\tr2,PACATOC(r13);\t\t\t\t\t\\\n\tld\tr1,PACAR1(r13);\t\t\t\t\t\t\\\n\tstd\tr3,ORIG_GPR3(r1);\t/* Save original r3 */\t\t\\\n\tli\tr3,0;\t\t\t/* NULL argument */\t\t\\\n\tbl\thmi_exception_realmode;\t\t\t\t\t\\\n\tnop;\t\t\t\t\t\t\t\t\\\n\tld\tr3,ORIG_GPR3(r1);\t/* Restore original r3 */\t\\\n20:\tnop;\n\n/*\n * Entered with MSR[EE]=0 and no soft-masked interrupts pending.\n * r3 contains desired PSSCR register value.\n *\n * Offline (CPU unplug) case also must notify KVM that the CPU is\n * idle.\n */\n_GLOBAL(power9_offline_stop)\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\n\t/*\n\t * Tell KVM we're entering idle.\n\t * This does not have to be done in real mode because the P9 MMU\n\t * is independent per-thread. Some steppings share radix/hash mode\n\t * between threads, but in that case KVM has a barrier sync in real\n\t * mode before and after switching between radix and hash.\n\t */\n\tli\tr4,KVM_HWTHREAD_IN_IDLE\n\tstb\tr4,HSTATE_HWTHREAD_STATE(r13)\n#endif\n\t/* fall through */\n\n_GLOBAL(power9_idle_stop)\n\tstd\tr3, PACA_REQ_PSSCR(r13)\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\nBEGIN_FTR_SECTION\n\tsync\n\tlwz\tr5, PACA_DONT_STOP(r13)\n\tcmpwi\tr5, 0\n\tbne\t1f\nEND_FTR_SECTION_IFSET(CPU_FTR_P9_TM_XER_SO_BUG)\n#endif\n\tmtspr \tSPRN_PSSCR,r3\n\tLOAD_REG_ADDR(r4,power_enter_stop)\n\tb\tpnv_powersave_common\n\t/* No return */\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\n1:\n\t/*\n\t * We get here when TM / thread reconfiguration bug workaround\n\t * code wants to get the CPU into SMT4 mode, and therefore\n\t * we are being asked not to stop.\n\t */\n\tli\tr3, 0\n\tstd\tr3, PACA_REQ_PSSCR(r13)\n\tblr\t\t/* return 0 for wakeup cause / SRR1 value */\n#endif\n\n/*\n * Called from machine check handler for powersave wakeups.\n * Low level machine check processing has already been done. Now just\n * go through the wake up path to get everything in order.\n *\n * r3 - The original SRR1 value.\n * Original SRR[01] have been clobbered.\n * MSR_RI is clear.\n */\n.global pnv_powersave_wakeup_mce\npnv_powersave_wakeup_mce:\n\t/* Set cr3 for pnv_powersave_wakeup */\n\trlwinm\tr11,r3,47-31,30,31\n\tcmpwi\tcr3,r11,2\n\n\t/*\n\t * Now put the original SRR1 with SRR1_WAKEMCE_RESVD as the wake\n\t * reason into r12, which allows reuse of the system reset wakeup\n\t * code without being mistaken for another type of wakeup.\n\t */\n\toris\tr12,r3,SRR1_WAKEMCE_RESVD@h\n\n\tb\tpnv_powersave_wakeup\n\n/*\n * Called from reset vector for powersave wakeups.\n * cr3 - set to gt if waking up with partial/complete hypervisor state loss\n * r12 - SRR1\n */\n.global pnv_powersave_wakeup\npnv_powersave_wakeup:\n\tld\tr2, PACATOC(r13)\n\nBEGIN_FTR_SECTION\n\tbl\tpnv_restore_hyp_resource_arch300\nFTR_SECTION_ELSE\n\tbl\tpnv_restore_hyp_resource_arch207\nALT_FTR_SECTION_END_IFSET(CPU_FTR_ARCH_300)\n\n\tli\tr0,PNV_THREAD_RUNNING\n\tstb\tr0,PACA_THREAD_IDLE_STATE(r13)\t/* Clear thread state */\n\n\tmr\tr3,r12\n\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\n\tlbz\tr0,HSTATE_HWTHREAD_STATE(r13)\n\tcmpwi\tr0,KVM_HWTHREAD_IN_KERNEL\n\tbeq\t0f\n\tli\tr0,KVM_HWTHREAD_IN_KERNEL\n\tstb\tr0,HSTATE_HWTHREAD_STATE(r13)\n\t/* Order setting hwthread_state vs. testing hwthread_req */\n\tsync\n0:\tlbz\tr0,HSTATE_HWTHREAD_REQ(r13)\n\tcmpwi\tr0,0\n\tbeq\t1f\n\tb\tkvm_start_guest\n1:\n#endif\n\n\t/* Return SRR1 from power7_nap() */\n\tblt\tcr3,pnv_wakeup_noloss\n\tb\tpnv_wakeup_loss\n\n/*\n * Check whether we have woken up with hypervisor state loss.\n * If yes, restore hypervisor state and return back to link.\n *\n * cr3 - set to gt if waking up with partial/complete hypervisor state loss\n */\npnv_restore_hyp_resource_arch300:\n\t/*\n\t * Workaround for POWER9, if we lost resources, the ERAT\n\t * might have been mixed up and needs flushing. We also need\n\t * to reload MMCR0 (see comment above). We also need to set\n\t * then clear bit 60 in MMCRA to ensure the PMU starts running.\n\t */\n\tblt\tcr3,1f\nBEGIN_FTR_SECTION\n\tPPC_INVALIDATE_ERAT\n\tld\tr1,PACAR1(r13)\n\tld\tr4,_MMCR0(r1)\n\tmtspr\tSPRN_MMCR0,r4\nEND_FTR_SECTION_IFCLR(CPU_FTR_POWER9_DD2_1)\n\tmfspr\tr4,SPRN_MMCRA\n\tori\tr4,r4,(1 << (63-60))\n\tmtspr\tSPRN_MMCRA,r4\n\txori\tr4,r4,(1 << (63-60))\n\tmtspr\tSPRN_MMCRA,r4\n1:\n\t/*\n\t * POWER ISA 3. Use PSSCR to determine if we\n\t * are waking up from deep idle state\n\t */\n\tLOAD_REG_ADDRBASE(r5,pnv_first_deep_stop_state)\n\tld\tr4,ADDROFF(pnv_first_deep_stop_state)(r5)\n\n\t/*\n\t * 0-3 bits correspond to Power-Saving Level Status\n\t * which indicates the idle state we are waking up from\n\t */\n\tmfspr\tr5, SPRN_PSSCR\n\trldicl  r5,r5,4,60\n\tli\tr0, 0\t\t/* clear requested_psscr to say we're awake */\n\tstd\tr0, PACA_REQ_PSSCR(r13)\n\tcmpd\tcr4,r5,r4\n\tbge\tcr4,pnv_wakeup_tb_loss /* returns to caller */\n\n\tblr\t/* Waking up without hypervisor state loss. */\n\n/* Same calling convention as arch300 */\npnv_restore_hyp_resource_arch207:\n\t/*\n\t * POWER ISA 2.07 or less.\n\t * Check if we slept with sleep or winkle.\n\t */\n\tlbz\tr4,PACA_THREAD_IDLE_STATE(r13)\n\tcmpwi\tcr2,r4,PNV_THREAD_NAP\n\tbgt\tcr2,pnv_wakeup_tb_loss\t/* Either sleep or Winkle */\n\n\t/*\n\t * We fall through here if PACA_THREAD_IDLE_STATE shows we are waking\n\t * up from nap. At this stage CR3 shouldn't contains 'gt' since that\n\t * indicates we are waking with hypervisor state loss from nap.\n\t */\n\tbgt\tcr3,.\n\n\tblr\t/* Waking up without hypervisor state loss */\n\n/*\n * Called if waking up from idle state which can cause either partial or\n * complete hyp state loss.\n * In POWER8, called if waking up from fastsleep or winkle\n * In POWER9, called if waking up from stop state >= pnv_first_deep_stop_state\n *\n * r13 - PACA\n * cr3 - gt if waking up with partial/complete hypervisor state loss\n *\n * If ISA300:\n * cr4 - gt or eq if waking up from complete hypervisor state loss.\n *\n * If ISA207:\n * r4 - PACA_THREAD_IDLE_STATE\n */\npnv_wakeup_tb_loss:\n\tld\tr1,PACAR1(r13)\n\t/*\n\t * Before entering any idle state, the NVGPRs are saved in the stack.\n\t * If there was a state loss, or PACA_NAPSTATELOST was set, then the\n\t * NVGPRs are restored. If we are here, it is likely that state is lost,\n\t * but not guaranteed -- neither ISA207 nor ISA300 tests to reach\n\t * here are the same as the test to restore NVGPRS:\n\t * PACA_THREAD_IDLE_STATE test for ISA207, PSSCR test for ISA300,\n\t * and SRR1 test for restoring NVGPRs.\n\t *\n\t * We are about to clobber NVGPRs now, so set NAPSTATELOST to\n\t * guarantee they will always be restored. This might be tightened\n\t * with careful reading of specs (particularly for ISA300) but this\n\t * is already a slow wakeup path and it's simpler to be safe.\n\t */\n\tli\tr0,1\n\tstb\tr0,PACA_NAPSTATELOST(r13)\n\n\t/*\n\t *\n\t * Save SRR1 and LR in NVGPRs as they might be clobbered in\n\t * opal_call() (called in CHECK_HMI_INTERRUPT). SRR1 is required\n\t * to determine the wakeup reason if we branch to kvm_start_guest. LR\n\t * is required to return back to reset vector after hypervisor state\n\t * restore is complete.\n\t */\n\tmr\tr19,r12\n\tmr\tr18,r4\n\tmflr\tr17\nBEGIN_FTR_SECTION\n\tCHECK_HMI_INTERRUPT\nEND_FTR_SECTION_IFSET(CPU_FTR_HVMODE)\n\n\tld\tr14,PACA_CORE_IDLE_STATE_PTR(r13)\n\tlbz\tr7,PACA_THREAD_MASK(r13)\n\n\t/*\n\t * Take the core lock to synchronize against other threads.\n\t *\n\t * Lock bit is set in one of the 2 cases-\n\t * a. In the sleep/winkle enter path, the last thread is executing\n\t * fastsleep workaround code.\n\t * b. In the wake up path, another thread is executing fastsleep\n\t * workaround undo code or resyncing timebase or restoring context\n\t * In either case loop until the lock bit is cleared.\n\t */\n1:\n\tlwarx\tr15,0,r14\n\tandis.\tr9,r15,PNV_CORE_IDLE_LOCK_BIT@h\n\tbnel-\tcore_idle_lock_held\n\toris\tr15,r15,PNV_CORE_IDLE_LOCK_BIT@h\n\tstwcx.\tr15,0,r14\n\tbne-\t1b\n\tisync\n\n\tandi.\tr9,r15,PNV_CORE_IDLE_THREAD_BITS\n\tcmpwi\tcr2,r9,0\n\n\t/*\n\t * At this stage\n\t * cr2 - eq if first thread to wakeup in core\n\t * cr3-  gt if waking up with partial/complete hypervisor state loss\n\t * ISA300:\n\t * cr4 - gt or eq if waking up from complete hypervisor state loss.\n\t */\n\nBEGIN_FTR_SECTION\n\t/*\n\t * Were we in winkle?\n\t * If yes, check if all threads were in winkle, decrement our\n\t * winkle count, set all thread winkle bits if all were in winkle.\n\t * Check if our thread has a winkle bit set, and set cr4 accordingly\n\t * (to match ISA300, above). Pseudo-code for core idle state\n\t * transitions for ISA207 is as follows (everything happens atomically\n\t * due to store conditional and/or lock bit):\n\t *\n\t * nap_idle() { }\n\t * nap_wake() { }\n\t *\n\t * sleep_idle()\n\t * {\n\t *\tcore_idle_state &= ~thread_in_core\n\t * }\n\t *\n\t * sleep_wake()\n\t * {\n\t *     bool first_in_core, first_in_subcore;\n\t *\n\t *     first_in_core = (core_idle_state & IDLE_THREAD_BITS) == 0;\n\t *     first_in_subcore = (core_idle_state & SUBCORE_SIBLING_MASK) == 0;\n\t *\n\t *     core_idle_state |= thread_in_core;\n\t * }\n\t *\n\t * winkle_idle()\n\t * {\n\t *\tcore_idle_state &= ~thread_in_core;\n\t *\tcore_idle_state += 1 << WINKLE_COUNT_SHIFT;\n\t * }\n\t *\n\t * winkle_wake()\n\t * {\n\t *     bool first_in_core, first_in_subcore, winkle_state_lost;\n\t *\n\t *     first_in_core = (core_idle_state & IDLE_THREAD_BITS) == 0;\n\t *     first_in_subcore = (core_idle_state & SUBCORE_SIBLING_MASK) == 0;\n\t *\n\t *     core_idle_state |= thread_in_core;\n\t *\n\t *     if ((core_idle_state & WINKLE_MASK) == (8 << WINKLE_COUNT_SIHFT))\n\t *         core_idle_state |= THREAD_WINKLE_BITS;\n\t *     core_idle_state -= 1 << WINKLE_COUNT_SHIFT;\n\t *\n\t *     winkle_state_lost = core_idle_state &\n\t *\t\t\t\t(thread_in_core << WINKLE_THREAD_SHIFT);\n\t *     core_idle_state &= ~(thread_in_core << WINKLE_THREAD_SHIFT);\n\t * }\n\t *\n\t */\n\tcmpwi\tr18,PNV_THREAD_WINKLE\n\tbne\t2f\n\tandis.\tr9,r15,PNV_CORE_IDLE_WINKLE_COUNT_ALL_BIT@h\n\tsubis\tr15,r15,PNV_CORE_IDLE_WINKLE_COUNT@h\n\tbeq\t2f\n\tori\tr15,r15,PNV_CORE_IDLE_THREAD_WINKLE_BITS /* all were winkle */\n2:\n\t/* Shift thread bit to winkle mask, then test if this thread is set,\n\t * and remove it from the winkle bits */\n\tslwi\tr8,r7,8\n\tand\tr8,r8,r15\n\tandc\tr15,r15,r8\n\tcmpwi\tcr4,r8,1 /* cr4 will be gt if our bit is set, lt if not */\n\n\tlbz\tr4,PACA_SUBCORE_SIBLING_MASK(r13)\n\tand\tr4,r4,r15\n\tcmpwi\tr4,0\t/* Check if first in subcore */\n\n\tor\tr15,r15,r7\t\t/* Set thread bit */\n\tbeq\tfirst_thread_in_subcore\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_300)\n\n\tor\tr15,r15,r7\t\t/* Set thread bit */\n\tbeq\tcr2,first_thread_in_core\n\n\t/* Not first thread in core or subcore to wake up */\n\tb\tclear_lock\n\nfirst_thread_in_subcore:\n\t/*\n\t * If waking up from sleep, subcore state is not lost. Hence\n\t * skip subcore state restore\n\t */\n\tblt\tcr4,subcore_state_restored\n\n\t/* Restore per-subcore state */\n\tld      r4,_SDR1(r1)\n\tmtspr   SPRN_SDR1,r4\n\n\tld      r4,_RPR(r1)\n\tmtspr   SPRN_RPR,r4\n\tld\tr4,_AMOR(r1)\n\tmtspr\tSPRN_AMOR,r4\n\nsubcore_state_restored:\n\t/*\n\t * Check if the thread is also the first thread in the core. If not,\n\t * skip to clear_lock.\n\t */\n\tbne\tcr2,clear_lock\n\nfirst_thread_in_core:\n\n\t/*\n\t * First thread in the core waking up from any state which can cause\n\t * partial or complete hypervisor state loss. It needs to\n\t * call the fastsleep workaround code if the platform requires it.\n\t * Call it unconditionally here. The below branch instruction will\n\t * be patched out if the platform does not have fastsleep or does not\n\t * require the workaround. Patching will be performed during the\n\t * discovery of idle-states.\n\t */\n.global pnv_fastsleep_workaround_at_exit\npnv_fastsleep_workaround_at_exit:\n\tb\tfastsleep_workaround_at_exit\n\ntimebase_resync:\n\t/*\n\t * Use cr3 which indicates that we are waking up with atleast partial\n\t * hypervisor state loss to determine if TIMEBASE RESYNC is needed.\n\t */\n\tble\tcr3,.Ltb_resynced\n\t/* Time base re-sync */\n\tbl\topal_resync_timebase;\n\t/*\n\t * If waking up from sleep (POWER8), per core state\n\t * is not lost, skip to clear_lock.\n\t */\n.Ltb_resynced:\n\tblt\tcr4,clear_lock\n\n\t/*\n\t * First thread in the core to wake up and its waking up with\n\t * complete hypervisor state loss. Restore per core hypervisor\n\t * state.\n\t */\nBEGIN_FTR_SECTION\n\tld\tr4,_PTCR(r1)\n\tmtspr\tSPRN_PTCR,r4\n\tld\tr4,_RPR(r1)\n\tmtspr\tSPRN_RPR,r4\n\tld\tr4,_AMOR(r1)\n\tmtspr\tSPRN_AMOR,r4\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_300)\n\n\tld\tr4,_TSCR(r1)\n\tmtspr\tSPRN_TSCR,r4\n\tld\tr4,_WORC(r1)\n\tmtspr\tSPRN_WORC,r4\n\nclear_lock:\n\txoris\tr15,r15,PNV_CORE_IDLE_LOCK_BIT@h\n\tlwsync\n\tstw\tr15,0(r14)\n\ncommon_exit:\n\t/*\n\t * Common to all threads.\n\t *\n\t * If waking up from sleep, hypervisor state is not lost. Hence\n\t * skip hypervisor state restore.\n\t */\n\tblt\tcr4,hypervisor_state_restored\n\n\t/* Waking up from winkle */\n\nBEGIN_MMU_FTR_SECTION\n\tb\tno_segments\nEND_MMU_FTR_SECTION_IFSET(MMU_FTR_TYPE_RADIX)\n\t/* Restore SLB  from PACA */\n\tld\tr8,PACA_SLBSHADOWPTR(r13)\n\n\t.rept\tSLB_NUM_BOLTED\n\tli\tr3, SLBSHADOW_SAVEAREA\n\tLDX_BE\tr5, r8, r3\n\taddi\tr3, r3, 8\n\tLDX_BE\tr6, r8, r3\n\tandis.\tr7,r5,SLB_ESID_V@h\n\tbeq\t1f\n\tslbmte\tr6,r5\n1:\taddi\tr8,r8,16\n\t.endr\nno_segments:\n\n\t/* Restore per thread state */\n\n\tld\tr4,_SPURR(r1)\n\tmtspr\tSPRN_SPURR,r4\n\tld\tr4,_PURR(r1)\n\tmtspr\tSPRN_PURR,r4\n\tld\tr4,_DSCR(r1)\n\tmtspr\tSPRN_DSCR,r4\n\tld\tr4,_WORT(r1)\n\tmtspr\tSPRN_WORT,r4\n\n\t/* Call cur_cpu_spec->cpu_restore() */\n\tLOAD_REG_ADDR(r4, cur_cpu_spec)\n\tld\tr4,0(r4)\n\tld\tr12,CPU_SPEC_RESTORE(r4)\n#ifdef PPC64_ELF_ABI_v1\n\tld\tr12,0(r12)\n#endif\n\tmtctr\tr12\n\tbctrl\n\n/*\n * On POWER9, we can come here on wakeup from a cpuidle stop state.\n * Hence restore the additional SPRs to the saved value.\n *\n * On POWER8, we come here only on winkle. Since winkle is used\n * only in the case of CPU-Hotplug, we don't need to restore\n * the additional SPRs.\n */\nBEGIN_FTR_SECTION\n\tbl \tpower9_restore_additional_sprs\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_300)\nhypervisor_state_restored:\n\n\tmr\tr12,r19\n\tmtlr\tr17\n\tblr\t\t/* return to pnv_powersave_wakeup */\n\nfastsleep_workaround_at_exit:\n\tli\tr3,1\n\tli\tr4,0\n\tbl\topal_config_cpu_idle_state\n\tb\ttimebase_resync\n\n/*\n * R3 here contains the value that will be returned to the caller\n * of power7_nap.\n * R12 contains SRR1 for CHECK_HMI_INTERRUPT.\n */\n.global pnv_wakeup_loss\npnv_wakeup_loss:\n\tld\tr1,PACAR1(r13)\nBEGIN_FTR_SECTION\n\tCHECK_HMI_INTERRUPT\nEND_FTR_SECTION_IFSET(CPU_FTR_HVMODE)\n\tREST_NVGPRS(r1)\n\tREST_GPR(2, r1)\n\nBEGIN_FTR_SECTION\n\t/* These regs were saved in pnv_powersave_common() */\n\tld\tr4, PNV_POWERSAVE_AMR(r1)\n\tld\tr5, PNV_POWERSAVE_IAMR(r1)\n\tld\tr6, PNV_POWERSAVE_UAMOR(r1)\n\tmtspr\tSPRN_AMR, r4\n\tmtspr\tSPRN_IAMR, r5\n\tmtspr\tSPRN_UAMOR, r6\nBEGIN_FTR_SECTION_NESTED(42)\n\tld\tr7, PNV_POWERSAVE_AMOR(r1)\n\tmtspr\tSPRN_AMOR, r7\nEND_FTR_SECTION_NESTED_IFSET(CPU_FTR_HVMODE, 42)\n\t/*\n\t * We don't need an isync here after restoring IAMR because the upcoming\n\t * mtmsrd is execution synchronizing.\n\t */\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\n\tld\tr4,PACAKMSR(r13)\n\tld\tr5,_LINK(r1)\n\tld\tr6,_CCR(r1)\n\taddi\tr1,r1,INT_FRAME_SIZE\n\tmtlr\tr5\n\tmtcr\tr6\n\tmtmsrd\tr4\n\tblr\n\n/*\n * R3 here contains the value that will be returned to the caller\n * of power7_nap.\n * R12 contains SRR1 for CHECK_HMI_INTERRUPT.\n */\npnv_wakeup_noloss:\n\tlbz\tr0,PACA_NAPSTATELOST(r13)\n\tcmpwi\tr0,0\n\tbne\tpnv_wakeup_loss\n\tld\tr1,PACAR1(r13)\nBEGIN_FTR_SECTION\n\tCHECK_HMI_INTERRUPT\nEND_FTR_SECTION_IFSET(CPU_FTR_HVMODE)\n\tld\tr4,PACAKMSR(r13)\n\tld\tr5,_NIP(r1)\n\tld\tr6,_CCR(r1)\n\taddi\tr1,r1,INT_FRAME_SIZE\n\tmtlr\tr5\n\tmtcr\tr6\n\tmtmsrd\tr4\n\tblr\n"], "filenames": ["arch/powerpc/kernel/idle_book3s.S"], "buggy_code_start_loc": [173], "buggy_code_end_loc": [944], "fixing_code_start_loc": [173], "fixing_code_end_loc": [963], "type": "NVD-CWE-noinfo", "message": "An issue was discovered in the Linux kernel before 5.2 on the powerpc platform. arch/powerpc/kernel/idle_book3s.S does not have save/restore functionality for PNV_POWERSAVE_AMR, PNV_POWERSAVE_UAMOR, and PNV_POWERSAVE_AMOR, aka CID-53a712bae5dd.", "other": {"cve": {"id": "CVE-2020-11669", "sourceIdentifier": "cve@mitre.org", "published": "2020-04-10T15:15:12.630", "lastModified": "2020-05-28T15:15:11.637", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "An issue was discovered in the Linux kernel before 5.2 on the powerpc platform. arch/powerpc/kernel/idle_book3s.S does not have save/restore functionality for PNV_POWERSAVE_AMR, PNV_POWERSAVE_UAMOR, and PNV_POWERSAVE_AMOR, aka CID-53a712bae5dd."}, {"lang": "es", "value": "Se detect\u00f3 un problema en el kernel de Linux versiones anteriores a 5.2, en la plataforma powerpc. El archivo arch/powerpc/kernel/idle_book3s.S no posee la funcionalidad de guardar y restaurar para PNV_POWERSAVE_AMR, PNV_POWERSAVE_UAMOR y PNV_POWERSAVE_AMOR, tambi\u00e9n se conoce como CID-53a712bae5dd."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.2", "matchCriteriaId": "D045CBEB-1802-47E4-AEDB-310B5DF2E8AA"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:leap:15.1:*:*:*:*:*:*:*", "matchCriteriaId": "B620311B-34A3-48A6-82DF-6F078D7A4493"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "142AD0DD-4CF3-4D74-9442-459CE3347E3A"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2020-04/msg00035.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:3517", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.2", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=53a712bae5dd919521a58d7bad773b949358add0", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/53a712bae5dd919521a58d7bad773b949358add0", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://lists.ozlabs.org/pipermail/linuxppc-dev/2020-April/208660.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://lists.ozlabs.org/pipermail/linuxppc-dev/2020-April/208661.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://lists.ozlabs.org/pipermail/linuxppc-dev/2020-April/208663.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20200430-0001/", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4363-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4368-1/", "source": "cve@mitre.org"}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/53a712bae5dd919521a58d7bad773b949358add0"}}