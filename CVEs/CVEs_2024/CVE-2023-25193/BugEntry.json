{"buggy_code": ["/*\n * Copyright \u00a9 2007,2008,2009,2010  Red Hat, Inc.\n * Copyright \u00a9 2010,2012  Google, Inc.\n *\n *  This is part of HarfBuzz, a text shaping library.\n *\n * Permission is hereby granted, without written agreement and without\n * license or royalty fees, to use, copy, modify, and distribute this\n * software and its documentation for any purpose, provided that the\n * above copyright notice and the following two paragraphs appear in\n * all copies of this software.\n *\n * IN NO EVENT SHALL THE COPYRIGHT HOLDER BE LIABLE TO ANY PARTY FOR\n * DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES\n * ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN\n * IF THE COPYRIGHT HOLDER HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH\n * DAMAGE.\n *\n * THE COPYRIGHT HOLDER SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING,\n * BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND\n * FITNESS FOR A PARTICULAR PURPOSE.  THE SOFTWARE PROVIDED HEREUNDER IS\n * ON AN \"AS IS\" BASIS, AND THE COPYRIGHT HOLDER HAS NO OBLIGATION TO\n * PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.\n *\n * Red Hat Author(s): Behdad Esfahbod\n * Google Author(s): Behdad Esfahbod\n */\n\n#ifndef HB_OT_LAYOUT_GSUBGPOS_HH\n#define HB_OT_LAYOUT_GSUBGPOS_HH\n\n#include \"hb.hh\"\n#include \"hb-buffer.hh\"\n#include \"hb-map.hh\"\n#include \"hb-set.hh\"\n#include \"hb-ot-map.hh\"\n#include \"hb-ot-layout-common.hh\"\n#include \"hb-ot-layout-gdef-table.hh\"\n\n\nnamespace OT {\n\n\nstruct hb_intersects_context_t :\n       hb_dispatch_context_t<hb_intersects_context_t, bool>\n{\n  template <typename T>\n  return_t dispatch (const T &obj) { return obj.intersects (this->glyphs); }\n  static return_t default_return_value () { return false; }\n  bool stop_sublookup_iteration (return_t r) const { return r; }\n\n  const hb_set_t *glyphs;\n\n  hb_intersects_context_t (const hb_set_t *glyphs_) :\n                            glyphs (glyphs_) {}\n};\n\nstruct hb_have_non_1to1_context_t :\n       hb_dispatch_context_t<hb_have_non_1to1_context_t, bool>\n{\n  template <typename T>\n  return_t dispatch (const T &obj) { return obj.may_have_non_1to1 (); }\n  static return_t default_return_value () { return false; }\n  bool stop_sublookup_iteration (return_t r) const { return r; }\n};\n\nstruct hb_closure_context_t :\n       hb_dispatch_context_t<hb_closure_context_t>\n{\n  typedef return_t (*recurse_func_t) (hb_closure_context_t *c, unsigned lookup_index, hb_set_t *covered_seq_indicies, unsigned seq_index, unsigned end_index);\n  template <typename T>\n  return_t dispatch (const T &obj) { obj.closure (this); return hb_empty_t (); }\n  static return_t default_return_value () { return hb_empty_t (); }\n  void recurse (unsigned lookup_index, hb_set_t *covered_seq_indicies, unsigned seq_index, unsigned end_index)\n  {\n    if (unlikely (nesting_level_left == 0 || !recurse_func))\n      return;\n\n    nesting_level_left--;\n    recurse_func (this, lookup_index, covered_seq_indicies, seq_index, end_index);\n    nesting_level_left++;\n  }\n\n  void reset_lookup_visit_count ()\n  { lookup_count = 0; }\n\n  bool lookup_limit_exceeded ()\n  { return lookup_count > HB_MAX_LOOKUP_VISIT_COUNT; }\n\n  bool should_visit_lookup (unsigned int lookup_index)\n  {\n    if (lookup_count++ > HB_MAX_LOOKUP_VISIT_COUNT)\n      return false;\n\n    if (is_lookup_done (lookup_index))\n      return false;\n\n    return true;\n  }\n\n  bool is_lookup_done (unsigned int lookup_index)\n  {\n    if (unlikely (done_lookups_glyph_count->in_error () ||\n\t\t  done_lookups_glyph_set->in_error ()))\n      return true;\n\n    /* Have we visited this lookup with the current set of glyphs? */\n    if (done_lookups_glyph_count->get (lookup_index) != glyphs->get_population ())\n    {\n      done_lookups_glyph_count->set (lookup_index, glyphs->get_population ());\n\n      if (!done_lookups_glyph_set->has (lookup_index))\n      {\n\tif (unlikely (!done_lookups_glyph_set->set (lookup_index, hb::unique_ptr<hb_set_t> {hb_set_create ()})))\n\t  return true;\n      }\n\n      done_lookups_glyph_set->get (lookup_index)->clear ();\n    }\n\n    hb_set_t *covered_glyph_set = done_lookups_glyph_set->get (lookup_index);\n    if (unlikely (covered_glyph_set->in_error ()))\n      return true;\n    if (parent_active_glyphs ().is_subset (*covered_glyph_set))\n      return true;\n\n    covered_glyph_set->union_ (parent_active_glyphs ());\n    return false;\n  }\n\n  const hb_set_t& previous_parent_active_glyphs () {\n    if (active_glyphs_stack.length <= 1)\n      return *glyphs;\n\n    return active_glyphs_stack[active_glyphs_stack.length - 2];\n  }\n\n  const hb_set_t& parent_active_glyphs ()\n  {\n    if (!active_glyphs_stack)\n      return *glyphs;\n\n    return active_glyphs_stack.tail ();\n  }\n\n  hb_set_t& push_cur_active_glyphs ()\n  {\n    return *active_glyphs_stack.push ();\n  }\n\n  bool pop_cur_done_glyphs ()\n  {\n    if (!active_glyphs_stack)\n      return false;\n\n    active_glyphs_stack.pop ();\n    return true;\n  }\n\n  hb_face_t *face;\n  hb_set_t *glyphs;\n  hb_set_t output[1];\n  hb_vector_t<hb_set_t> active_glyphs_stack;\n  recurse_func_t recurse_func = nullptr;\n  unsigned int nesting_level_left;\n\n  hb_closure_context_t (hb_face_t *face_,\n\t\t\thb_set_t *glyphs_,\n\t\t\thb_map_t *done_lookups_glyph_count_,\n\t\t\thb_hashmap_t<unsigned, hb::unique_ptr<hb_set_t>> *done_lookups_glyph_set_,\n\t\t\tunsigned int nesting_level_left_ = HB_MAX_NESTING_LEVEL) :\n\t\t\t  face (face_),\n\t\t\t  glyphs (glyphs_),\n\t\t\t  nesting_level_left (nesting_level_left_),\n\t\t\t  done_lookups_glyph_count (done_lookups_glyph_count_),\n\t\t\t  done_lookups_glyph_set (done_lookups_glyph_set_)\n  {}\n\n  ~hb_closure_context_t () { flush (); }\n\n  void set_recurse_func (recurse_func_t func) { recurse_func = func; }\n\n  void flush ()\n  {\n    output->del_range (face->get_num_glyphs (), HB_SET_VALUE_INVALID);\t/* Remove invalid glyphs. */\n    glyphs->union_ (*output);\n    output->clear ();\n    active_glyphs_stack.pop ();\n    active_glyphs_stack.reset ();\n  }\n\n  private:\n  hb_map_t *done_lookups_glyph_count;\n  hb_hashmap_t<unsigned, hb::unique_ptr<hb_set_t>> *done_lookups_glyph_set;\n  unsigned int lookup_count = 0;\n};\n\n\n\nstruct hb_closure_lookups_context_t :\n       hb_dispatch_context_t<hb_closure_lookups_context_t>\n{\n  typedef return_t (*recurse_func_t) (hb_closure_lookups_context_t *c, unsigned lookup_index);\n  template <typename T>\n  return_t dispatch (const T &obj) { obj.closure_lookups (this); return hb_empty_t (); }\n  static return_t default_return_value () { return hb_empty_t (); }\n  void recurse (unsigned lookup_index)\n  {\n    if (unlikely (nesting_level_left == 0 || !recurse_func))\n      return;\n\n    /* Return if new lookup was recursed to before. */\n    if (lookup_limit_exceeded ()\n        || visited_lookups->in_error ()\n        || visited_lookups->has (lookup_index))\n      // Don't increment lookup count here, that will be done in the call to closure_lookups()\n      // made by recurse_func.\n      return;\n\n    nesting_level_left--;\n    recurse_func (this, lookup_index);\n    nesting_level_left++;\n  }\n\n  void set_lookup_visited (unsigned lookup_index)\n  { visited_lookups->add (lookup_index); }\n\n  void set_lookup_inactive (unsigned lookup_index)\n  { inactive_lookups->add (lookup_index); }\n\n  bool lookup_limit_exceeded ()\n  {\n    bool ret = lookup_count > HB_MAX_LOOKUP_VISIT_COUNT;\n    if (ret)\n      DEBUG_MSG (SUBSET, nullptr, \"lookup visit count limit exceeded in lookup closure!\");\n    return ret; }\n\n  bool is_lookup_visited (unsigned lookup_index)\n  {\n    if (unlikely (lookup_count++ > HB_MAX_LOOKUP_VISIT_COUNT))\n    {\n      DEBUG_MSG (SUBSET, nullptr, \"total visited lookup count %u exceeds max limit, lookup %u is dropped.\",\n                 lookup_count, lookup_index);\n      return true;\n    }\n\n    if (unlikely (visited_lookups->in_error ()))\n      return true;\n\n    return visited_lookups->has (lookup_index);\n  }\n\n  hb_face_t *face;\n  const hb_set_t *glyphs;\n  recurse_func_t recurse_func;\n  unsigned int nesting_level_left;\n\n  hb_closure_lookups_context_t (hb_face_t *face_,\n\t\t\t\tconst hb_set_t *glyphs_,\n\t\t\t\thb_set_t *visited_lookups_,\n\t\t\t\thb_set_t *inactive_lookups_,\n\t\t\t\tunsigned nesting_level_left_ = HB_MAX_NESTING_LEVEL) :\n\t\t\t\tface (face_),\n\t\t\t\tglyphs (glyphs_),\n\t\t\t\trecurse_func (nullptr),\n\t\t\t\tnesting_level_left (nesting_level_left_),\n\t\t\t\tvisited_lookups (visited_lookups_),\n\t\t\t\tinactive_lookups (inactive_lookups_),\n\t\t\t\tlookup_count (0) {}\n\n  void set_recurse_func (recurse_func_t func) { recurse_func = func; }\n\n  private:\n  hb_set_t *visited_lookups;\n  hb_set_t *inactive_lookups;\n  unsigned int lookup_count;\n};\n\nstruct hb_would_apply_context_t :\n       hb_dispatch_context_t<hb_would_apply_context_t, bool>\n{\n  template <typename T>\n  return_t dispatch (const T &obj) { return obj.would_apply (this); }\n  static return_t default_return_value () { return false; }\n  bool stop_sublookup_iteration (return_t r) const { return r; }\n\n  hb_face_t *face;\n  const hb_codepoint_t *glyphs;\n  unsigned int len;\n  bool zero_context;\n\n  hb_would_apply_context_t (hb_face_t *face_,\n\t\t\t    const hb_codepoint_t *glyphs_,\n\t\t\t    unsigned int len_,\n\t\t\t    bool zero_context_) :\n\t\t\t      face (face_),\n\t\t\t      glyphs (glyphs_),\n\t\t\t      len (len_),\n\t\t\t      zero_context (zero_context_) {}\n};\n\nstruct hb_collect_glyphs_context_t :\n       hb_dispatch_context_t<hb_collect_glyphs_context_t>\n{\n  typedef return_t (*recurse_func_t) (hb_collect_glyphs_context_t *c, unsigned int lookup_index);\n  template <typename T>\n  return_t dispatch (const T &obj) { obj.collect_glyphs (this); return hb_empty_t (); }\n  static return_t default_return_value () { return hb_empty_t (); }\n  void recurse (unsigned int lookup_index)\n  {\n    if (unlikely (nesting_level_left == 0 || !recurse_func))\n      return;\n\n    /* Note that GPOS sets recurse_func to nullptr already, so it doesn't get\n     * past the previous check.  For GSUB, we only want to collect the output\n     * glyphs in the recursion.  If output is not requested, we can go home now.\n     *\n     * Note further, that the above is not exactly correct.  A recursed lookup\n     * is allowed to match input that is not matched in the context, but that's\n     * not how most fonts are built.  It's possible to relax that and recurse\n     * with all sets here if it proves to be an issue.\n     */\n\n    if (output == hb_set_get_empty ())\n      return;\n\n    /* Return if new lookup was recursed to before. */\n    if (recursed_lookups->has (lookup_index))\n      return;\n\n    hb_set_t *old_before = before;\n    hb_set_t *old_input  = input;\n    hb_set_t *old_after  = after;\n    before = input = after = hb_set_get_empty ();\n\n    nesting_level_left--;\n    recurse_func (this, lookup_index);\n    nesting_level_left++;\n\n    before = old_before;\n    input  = old_input;\n    after  = old_after;\n\n    recursed_lookups->add (lookup_index);\n  }\n\n  hb_face_t *face;\n  hb_set_t *before;\n  hb_set_t *input;\n  hb_set_t *after;\n  hb_set_t *output;\n  recurse_func_t recurse_func;\n  hb_set_t *recursed_lookups;\n  unsigned int nesting_level_left;\n\n  hb_collect_glyphs_context_t (hb_face_t *face_,\n\t\t\t       hb_set_t  *glyphs_before, /* OUT.  May be NULL */\n\t\t\t       hb_set_t  *glyphs_input,  /* OUT.  May be NULL */\n\t\t\t       hb_set_t  *glyphs_after,  /* OUT.  May be NULL */\n\t\t\t       hb_set_t  *glyphs_output, /* OUT.  May be NULL */\n\t\t\t       unsigned int nesting_level_left_ = HB_MAX_NESTING_LEVEL) :\n\t\t\t      face (face_),\n\t\t\t      before (glyphs_before ? glyphs_before : hb_set_get_empty ()),\n\t\t\t      input  (glyphs_input  ? glyphs_input  : hb_set_get_empty ()),\n\t\t\t      after  (glyphs_after  ? glyphs_after  : hb_set_get_empty ()),\n\t\t\t      output (glyphs_output ? glyphs_output : hb_set_get_empty ()),\n\t\t\t      recurse_func (nullptr),\n\t\t\t      recursed_lookups (hb_set_create ()),\n\t\t\t      nesting_level_left (nesting_level_left_) {}\n  ~hb_collect_glyphs_context_t () { hb_set_destroy (recursed_lookups); }\n\n  void set_recurse_func (recurse_func_t func) { recurse_func = func; }\n};\n\n\n\ntemplate <typename set_t>\nstruct hb_collect_coverage_context_t :\n       hb_dispatch_context_t<hb_collect_coverage_context_t<set_t>, const Coverage &>\n{\n  typedef const Coverage &return_t; // Stoopid that we have to dupe this here.\n  template <typename T>\n  return_t dispatch (const T &obj) { return obj.get_coverage (); }\n  static return_t default_return_value () { return Null (Coverage); }\n  bool stop_sublookup_iteration (return_t r) const\n  {\n    r.collect_coverage (set);\n    return false;\n  }\n\n  hb_collect_coverage_context_t (set_t *set_) :\n\t\t\t\t   set (set_) {}\n\n  set_t *set;\n};\n\nstruct hb_ot_apply_context_t :\n       hb_dispatch_context_t<hb_ot_apply_context_t, bool, HB_DEBUG_APPLY>\n{\n  struct matcher_t\n  {\n    matcher_t () :\n\t     lookup_props (0),\n\t     mask (-1),\n\t     ignore_zwnj (false),\n\t     ignore_zwj (false),\n\t     per_syllable (false),\n\t     syllable {0},\n\t     match_func (nullptr),\n\t     match_data (nullptr) {}\n\n    typedef bool (*match_func_t) (hb_glyph_info_t &info, unsigned value, const void *data);\n\n    void set_ignore_zwnj (bool ignore_zwnj_) { ignore_zwnj = ignore_zwnj_; }\n    void set_ignore_zwj (bool ignore_zwj_) { ignore_zwj = ignore_zwj_; }\n    void set_lookup_props (unsigned int lookup_props_) { lookup_props = lookup_props_; }\n    void set_mask (hb_mask_t mask_) { mask = mask_; }\n    void set_per_syllable (bool per_syllable_) { per_syllable = per_syllable_; }\n    void set_syllable (uint8_t syllable_)  { syllable = per_syllable ? syllable_ : 0; }\n    void set_match_func (match_func_t match_func_,\n\t\t\t const void *match_data_)\n    { match_func = match_func_; match_data = match_data_; }\n\n    enum may_match_t {\n      MATCH_NO,\n      MATCH_YES,\n      MATCH_MAYBE\n    };\n\n    may_match_t may_match (hb_glyph_info_t &info,\n\t\t\t   hb_codepoint_t glyph_data) const\n    {\n      if (!(info.mask & mask) ||\n\t  (syllable && syllable != info.syllable ()))\n\treturn MATCH_NO;\n\n      if (match_func)\n\treturn match_func (info, glyph_data, match_data) ? MATCH_YES : MATCH_NO;\n\n      return MATCH_MAYBE;\n    }\n\n    enum may_skip_t {\n      SKIP_NO,\n      SKIP_YES,\n      SKIP_MAYBE\n    };\n\n    may_skip_t may_skip (const hb_ot_apply_context_t *c,\n\t\t\t const hb_glyph_info_t       &info) const\n    {\n      if (!c->check_glyph_property (&info, lookup_props))\n\treturn SKIP_YES;\n\n      if (unlikely (_hb_glyph_info_is_default_ignorable_and_not_hidden (&info) &&\n\t\t    (ignore_zwnj || !_hb_glyph_info_is_zwnj (&info)) &&\n\t\t    (ignore_zwj || !_hb_glyph_info_is_zwj (&info))))\n\treturn SKIP_MAYBE;\n\n      return SKIP_NO;\n    }\n\n    protected:\n    unsigned int lookup_props;\n    hb_mask_t mask;\n    bool ignore_zwnj;\n    bool ignore_zwj;\n    bool per_syllable;\n    uint8_t syllable;\n    match_func_t match_func;\n    const void *match_data;\n  };\n\n  struct skipping_iterator_t\n  {\n    void init (hb_ot_apply_context_t *c_, bool context_match = false)\n    {\n      c = c_;\n      match_glyph_data16 = nullptr;\n#ifndef HB_NO_BEYOND_64K\n      match_glyph_data24 = nullptr;\n#endif\n      matcher.set_match_func (nullptr, nullptr);\n      matcher.set_lookup_props (c->lookup_props);\n      /* Ignore ZWNJ if we are matching GPOS, or matching GSUB context and asked to. */\n      matcher.set_ignore_zwnj (c->table_index == 1 || (context_match && c->auto_zwnj));\n      /* Ignore ZWJ if we are matching context, or asked to. */\n      matcher.set_ignore_zwj  (context_match || c->auto_zwj);\n      matcher.set_mask (context_match ? -1 : c->lookup_mask);\n      matcher.set_per_syllable (c->per_syllable);\n    }\n    void set_lookup_props (unsigned int lookup_props)\n    {\n      matcher.set_lookup_props (lookup_props);\n    }\n    void set_match_func (matcher_t::match_func_t match_func_,\n\t\t\t const void *match_data_)\n    {\n      matcher.set_match_func (match_func_, match_data_);\n    }\n    void set_glyph_data (const HBUINT16 glyph_data[])\n    {\n      match_glyph_data16 = glyph_data;\n#ifndef HB_NO_BEYOND_64K\n      match_glyph_data24 = nullptr;\n#endif\n    }\n#ifndef HB_NO_BEYOND_64K\n    void set_glyph_data (const HBUINT24 glyph_data[])\n    {\n      match_glyph_data16 = nullptr;\n      match_glyph_data24 = glyph_data;\n    }\n#endif\n\n    void reset (unsigned int start_index_,\n\t\tunsigned int num_items_)\n    {\n      idx = start_index_;\n      num_items = num_items_;\n      end = c->buffer->len;\n      matcher.set_syllable (start_index_ == c->buffer->idx ? c->buffer->cur().syllable () : 0);\n    }\n\n    void reject ()\n    {\n      num_items++;\n      backup_glyph_data ();\n    }\n\n    matcher_t::may_skip_t\n    may_skip (const hb_glyph_info_t &info) const\n    { return matcher.may_skip (c, info); }\n\n    bool next (unsigned *unsafe_to = nullptr)\n    {\n      assert (num_items > 0);\n      /* The alternate condition below is faster at string boundaries,\n       * but produces subpar \"unsafe-to-concat\" values. */\n      signed stop = (signed) end - (signed) num_items;\n      if (c->buffer->flags & HB_BUFFER_FLAG_PRODUCE_UNSAFE_TO_CONCAT)\n        stop = (signed) end - 1;\n      while ((signed) idx < stop)\n      {\n\tidx++;\n\thb_glyph_info_t &info = c->buffer->info[idx];\n\n\tmatcher_t::may_skip_t skip = matcher.may_skip (c, info);\n\tif (unlikely (skip == matcher_t::SKIP_YES))\n\t  continue;\n\n\tmatcher_t::may_match_t match = matcher.may_match (info, get_glyph_data ());\n\tif (match == matcher_t::MATCH_YES ||\n\t    (match == matcher_t::MATCH_MAYBE &&\n\t     skip == matcher_t::SKIP_NO))\n\t{\n\t  num_items--;\n\t  advance_glyph_data ();\n\t  return true;\n\t}\n\n\tif (skip == matcher_t::SKIP_NO)\n\t{\n\t  if (unsafe_to)\n\t    *unsafe_to = idx + 1;\n\t  return false;\n\t}\n      }\n      if (unsafe_to)\n        *unsafe_to = end;\n      return false;\n    }\n    bool prev (unsigned *unsafe_from = nullptr)\n    {\n      assert (num_items > 0);\n      /* The alternate condition below is faster at string boundaries,\n       * but produces subpar \"unsafe-to-concat\" values. */\n      unsigned stop = num_items - 1;\n      if (c->buffer->flags & HB_BUFFER_FLAG_PRODUCE_UNSAFE_TO_CONCAT)\n        stop = 1 - 1;\n      while (idx > stop)\n      {\n\tidx--;\n\thb_glyph_info_t &info = c->buffer->out_info[idx];\n\n\tmatcher_t::may_skip_t skip = matcher.may_skip (c, info);\n\tif (unlikely (skip == matcher_t::SKIP_YES))\n\t  continue;\n\n\tmatcher_t::may_match_t match = matcher.may_match (info, get_glyph_data ());\n\tif (match == matcher_t::MATCH_YES ||\n\t    (match == matcher_t::MATCH_MAYBE &&\n\t     skip == matcher_t::SKIP_NO))\n\t{\n\t  num_items--;\n\t  advance_glyph_data ();\n\t  return true;\n\t}\n\n\tif (skip == matcher_t::SKIP_NO)\n\t{\n\t  if (unsafe_from)\n\t    *unsafe_from = hb_max (1u, idx) - 1u;\n\t  return false;\n\t}\n      }\n      if (unsafe_from)\n        *unsafe_from = 0;\n      return false;\n    }\n\n    hb_codepoint_t\n    get_glyph_data ()\n    {\n      if (match_glyph_data16) return *match_glyph_data16;\n#ifndef HB_NO_BEYOND_64K\n      else\n      if (match_glyph_data24) return *match_glyph_data24;\n#endif\n      return 0;\n    }\n    void\n    advance_glyph_data ()\n    {\n      if (match_glyph_data16) match_glyph_data16++;\n#ifndef HB_NO_BEYOND_64K\n      else\n      if (match_glyph_data24) match_glyph_data24++;\n#endif\n    }\n    void\n    backup_glyph_data ()\n    {\n      if (match_glyph_data16) match_glyph_data16--;\n#ifndef HB_NO_BEYOND_64K\n      else\n      if (match_glyph_data24) match_glyph_data24--;\n#endif\n    }\n\n    unsigned int idx;\n    protected:\n    hb_ot_apply_context_t *c;\n    matcher_t matcher;\n    const HBUINT16 *match_glyph_data16;\n#ifndef HB_NO_BEYOND_64K\n    const HBUINT24 *match_glyph_data24;\n#endif\n\n    unsigned int num_items;\n    unsigned int end;\n  };\n\n\n  const char *get_name () { return \"APPLY\"; }\n  typedef return_t (*recurse_func_t) (hb_ot_apply_context_t *c, unsigned int lookup_index);\n  template <typename T>\n  return_t dispatch (const T &obj) { return obj.apply (this); }\n  static return_t default_return_value () { return false; }\n  bool stop_sublookup_iteration (return_t r) const { return r; }\n  return_t recurse (unsigned int sub_lookup_index)\n  {\n    if (unlikely (nesting_level_left == 0 || !recurse_func || buffer->max_ops-- <= 0))\n    {\n      buffer->shaping_failed = true;\n      return default_return_value ();\n    }\n\n    nesting_level_left--;\n    bool ret = recurse_func (this, sub_lookup_index);\n    nesting_level_left++;\n    return ret;\n  }\n\n  skipping_iterator_t iter_input, iter_context;\n\n  unsigned int table_index; /* GSUB/GPOS */\n  hb_font_t *font;\n  hb_face_t *face;\n  hb_buffer_t *buffer;\n  recurse_func_t recurse_func = nullptr;\n  const GDEF &gdef;\n  const VariationStore &var_store;\n  VariationStore::cache_t *var_store_cache;\n  hb_set_digest_t digest;\n\n  hb_direction_t direction;\n  hb_mask_t lookup_mask = 1;\n  unsigned int lookup_index = (unsigned) -1;\n  unsigned int lookup_props = 0;\n  unsigned int nesting_level_left = HB_MAX_NESTING_LEVEL;\n\n  bool has_glyph_classes;\n  bool auto_zwnj = true;\n  bool auto_zwj = true;\n  bool per_syllable = false;\n  bool random = false;\n  uint32_t random_state = 1;\n  unsigned new_syllables = (unsigned) -1;\n\n  hb_ot_apply_context_t (unsigned int table_index_,\n\t\t\t hb_font_t *font_,\n\t\t\t hb_buffer_t *buffer_) :\n\t\t\ttable_index (table_index_),\n\t\t\tfont (font_), face (font->face), buffer (buffer_),\n\t\t\tgdef (\n#ifndef HB_NO_OT_LAYOUT\n\t\t\t      *face->table.GDEF->table\n#else\n\t\t\t      Null (GDEF)\n#endif\n\t\t\t     ),\n\t\t\tvar_store (gdef.get_var_store ()),\n\t\t\tvar_store_cache (\n#ifndef HB_NO_VAR\n\t\t\t\t\t table_index == 1 && font->num_coords ? var_store.create_cache () : nullptr\n#else\n\t\t\t\t\t nullptr\n#endif\n\t\t\t\t\t),\n\t\t\tdigest (buffer_->digest ()),\n\t\t\tdirection (buffer_->props.direction),\n\t\t\thas_glyph_classes (gdef.has_glyph_classes ())\n  { init_iters (); }\n\n  ~hb_ot_apply_context_t ()\n  {\n#ifndef HB_NO_VAR\n    VariationStore::destroy_cache (var_store_cache);\n#endif\n  }\n\n  void init_iters ()\n  {\n    iter_input.init (this, false);\n    iter_context.init (this, true);\n  }\n\n  void set_lookup_mask (hb_mask_t mask) { lookup_mask = mask; init_iters (); }\n  void set_auto_zwj (bool auto_zwj_) { auto_zwj = auto_zwj_; init_iters (); }\n  void set_auto_zwnj (bool auto_zwnj_) { auto_zwnj = auto_zwnj_; init_iters (); }\n  void set_per_syllable (bool per_syllable_) { per_syllable = per_syllable_; init_iters (); }\n  void set_random (bool random_) { random = random_; }\n  void set_recurse_func (recurse_func_t func) { recurse_func = func; }\n  void set_lookup_index (unsigned int lookup_index_) { lookup_index = lookup_index_; }\n  void set_lookup_props (unsigned int lookup_props_) { lookup_props = lookup_props_; init_iters (); }\n\n  uint32_t random_number ()\n  {\n    /* http://www.cplusplus.com/reference/random/minstd_rand/ */\n    random_state = random_state * 48271 % 2147483647;\n    return random_state;\n  }\n\n  bool match_properties_mark (hb_codepoint_t  glyph,\n\t\t\t      unsigned int    glyph_props,\n\t\t\t      unsigned int    match_props) const\n  {\n    /* If using mark filtering sets, the high short of\n     * match_props has the set index.\n     */\n    if (match_props & LookupFlag::UseMarkFilteringSet)\n      return gdef.mark_set_covers (match_props >> 16, glyph);\n\n    /* The second byte of match_props has the meaning\n     * \"ignore marks of attachment type different than\n     * the attachment type specified.\"\n     */\n    if (match_props & LookupFlag::MarkAttachmentType)\n      return (match_props & LookupFlag::MarkAttachmentType) == (glyph_props & LookupFlag::MarkAttachmentType);\n\n    return true;\n  }\n\n  bool check_glyph_property (const hb_glyph_info_t *info,\n\t\t\t     unsigned int  match_props) const\n  {\n    hb_codepoint_t glyph = info->codepoint;\n    unsigned int glyph_props = _hb_glyph_info_get_glyph_props (info);\n\n    /* Not covered, if, for example, glyph class is ligature and\n     * match_props includes LookupFlags::IgnoreLigatures\n     */\n    if (glyph_props & match_props & LookupFlag::IgnoreFlags)\n      return false;\n\n    if (unlikely (glyph_props & HB_OT_LAYOUT_GLYPH_PROPS_MARK))\n      return match_properties_mark (glyph, glyph_props, match_props);\n\n    return true;\n  }\n\n  void _set_glyph_class (hb_codepoint_t glyph_index,\n\t\t\t  unsigned int class_guess = 0,\n\t\t\t  bool ligature = false,\n\t\t\t  bool component = false)\n  {\n    digest.add (glyph_index);\n\n    if (new_syllables != (unsigned) -1)\n      buffer->cur().syllable() = new_syllables;\n\n    unsigned int props = _hb_glyph_info_get_glyph_props (&buffer->cur());\n    props |= HB_OT_LAYOUT_GLYPH_PROPS_SUBSTITUTED;\n    if (ligature)\n    {\n      props |= HB_OT_LAYOUT_GLYPH_PROPS_LIGATED;\n      /* In the only place that the MULTIPLIED bit is used, Uniscribe\n       * seems to only care about the \"last\" transformation between\n       * Ligature and Multiple substitutions.  Ie. if you ligate, expand,\n       * and ligate again, it forgives the multiplication and acts as\n       * if only ligation happened.  As such, clear MULTIPLIED bit.\n       */\n      props &= ~HB_OT_LAYOUT_GLYPH_PROPS_MULTIPLIED;\n    }\n    if (component)\n      props |= HB_OT_LAYOUT_GLYPH_PROPS_MULTIPLIED;\n    if (likely (has_glyph_classes))\n    {\n      props &= HB_OT_LAYOUT_GLYPH_PROPS_PRESERVE;\n      _hb_glyph_info_set_glyph_props (&buffer->cur(), props | gdef.get_glyph_props (glyph_index));\n    }\n    else if (class_guess)\n    {\n      props &= HB_OT_LAYOUT_GLYPH_PROPS_PRESERVE;\n      _hb_glyph_info_set_glyph_props (&buffer->cur(), props | class_guess);\n    }\n    else\n      _hb_glyph_info_set_glyph_props (&buffer->cur(), props);\n  }\n\n  void replace_glyph (hb_codepoint_t glyph_index)\n  {\n    _set_glyph_class (glyph_index);\n    (void) buffer->replace_glyph (glyph_index);\n  }\n  void replace_glyph_inplace (hb_codepoint_t glyph_index)\n  {\n    _set_glyph_class (glyph_index);\n    buffer->cur().codepoint = glyph_index;\n  }\n  void replace_glyph_with_ligature (hb_codepoint_t glyph_index,\n\t\t\t\t    unsigned int class_guess)\n  {\n    _set_glyph_class (glyph_index, class_guess, true);\n    (void) buffer->replace_glyph (glyph_index);\n  }\n  void output_glyph_for_component (hb_codepoint_t glyph_index,\n\t\t\t\t   unsigned int class_guess)\n  {\n    _set_glyph_class (glyph_index, class_guess, false, true);\n    (void) buffer->output_glyph (glyph_index);\n  }\n};\n\n\nstruct hb_accelerate_subtables_context_t :\n       hb_dispatch_context_t<hb_accelerate_subtables_context_t>\n{\n  template <typename Type>\n  static inline bool apply_to (const void *obj, hb_ot_apply_context_t *c)\n  {\n    const Type *typed_obj = (const Type *) obj;\n    return typed_obj->apply (c);\n  }\n\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n  template <typename T>\n  static inline auto apply_cached_ (const T *obj, hb_ot_apply_context_t *c, hb_priority<1>) HB_RETURN (bool, obj->apply (c, true) )\n  template <typename T>\n  static inline auto apply_cached_ (const T *obj, hb_ot_apply_context_t *c, hb_priority<0>) HB_RETURN (bool, obj->apply (c) )\n  template <typename Type>\n  static inline bool apply_cached_to (const void *obj, hb_ot_apply_context_t *c)\n  {\n    const Type *typed_obj = (const Type *) obj;\n    return apply_cached_ (typed_obj, c, hb_prioritize);\n  }\n\n  template <typename T>\n  static inline auto cache_func_ (const T *obj, hb_ot_apply_context_t *c, bool enter, hb_priority<1>) HB_RETURN (bool, obj->cache_func (c, enter) )\n  template <typename T>\n  static inline bool cache_func_ (const T *obj, hb_ot_apply_context_t *c, bool enter, hb_priority<0>) { return false; }\n  template <typename Type>\n  static inline bool cache_func_to (const void *obj, hb_ot_apply_context_t *c, bool enter)\n  {\n    const Type *typed_obj = (const Type *) obj;\n    return cache_func_ (typed_obj, c, enter, hb_prioritize);\n  }\n#endif\n\n  typedef bool (*hb_apply_func_t) (const void *obj, hb_ot_apply_context_t *c);\n  typedef bool (*hb_cache_func_t) (const void *obj, hb_ot_apply_context_t *c, bool enter);\n\n  struct hb_applicable_t\n  {\n    friend struct hb_accelerate_subtables_context_t;\n    friend struct hb_ot_layout_lookup_accelerator_t;\n\n    template <typename T>\n    void init (const T &obj_,\n\t       hb_apply_func_t apply_func_\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n\t       , hb_apply_func_t apply_cached_func_\n\t       , hb_cache_func_t cache_func_\n#endif\n\t\t)\n    {\n      obj = &obj_;\n      apply_func = apply_func_;\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n      apply_cached_func = apply_cached_func_;\n      cache_func = cache_func_;\n#endif\n      digest.init ();\n      obj_.get_coverage ().collect_coverage (&digest);\n    }\n\n    bool apply (hb_ot_apply_context_t *c) const\n    {\n      return digest.may_have (c->buffer->cur().codepoint) && apply_func (obj, c);\n    }\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n    bool apply_cached (hb_ot_apply_context_t *c) const\n    {\n      return digest.may_have (c->buffer->cur().codepoint) &&  apply_cached_func (obj, c);\n    }\n    bool cache_enter (hb_ot_apply_context_t *c) const\n    {\n      return cache_func (obj, c, true);\n    }\n    void cache_leave (hb_ot_apply_context_t *c) const\n    {\n      cache_func (obj, c, false);\n    }\n#endif\n\n    private:\n    const void *obj;\n    hb_apply_func_t apply_func;\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n    hb_apply_func_t apply_cached_func;\n    hb_cache_func_t cache_func;\n#endif\n    hb_set_digest_t digest;\n  };\n\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n  template <typename T>\n  auto cache_cost (const T &obj, hb_priority<1>) HB_AUTO_RETURN ( obj.cache_cost () )\n  template <typename T>\n  auto cache_cost (const T &obj, hb_priority<0>) HB_AUTO_RETURN ( 0u )\n#endif\n\n  /* Dispatch interface. */\n  template <typename T>\n  return_t dispatch (const T &obj)\n  {\n    hb_applicable_t *entry = &array[i++];\n\n    entry->init (obj,\n\t\t apply_to<T>\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n\t\t , apply_cached_to<T>\n\t\t , cache_func_to<T>\n#endif\n\t\t );\n\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n    /* Cache handling\n     *\n     * We allow one subtable from each lookup to use a cache. The assumption\n     * being that multiple subtables of the same lookup cannot use a cache\n     * because the resources they would use will collide.  As such, we ask\n     * each subtable to tell us how much it costs (which a cache would avoid),\n     * and we allocate the cache opportunity to the costliest subtable.\n     */\n    unsigned cost = cache_cost (obj, hb_prioritize);\n    if (cost > cache_user_cost)\n    {\n      cache_user_idx = i - 1;\n      cache_user_cost = cost;\n    }\n#endif\n\n    return hb_empty_t ();\n  }\n  static return_t default_return_value () { return hb_empty_t (); }\n\n  hb_accelerate_subtables_context_t (hb_applicable_t *array_) :\n\t\t\t\t     array (array_) {}\n\n  hb_applicable_t *array;\n  unsigned i = 0;\n\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n  unsigned cache_user_idx = (unsigned) -1;\n  unsigned cache_user_cost = 0;\n#endif\n};\n\n\ntypedef bool (*intersects_func_t) (const hb_set_t *glyphs, unsigned value, const void *data, void *cache);\ntypedef void (*intersected_glyphs_func_t) (const hb_set_t *glyphs, const void *data, unsigned value, hb_set_t *intersected_glyphs, void *cache);\ntypedef void (*collect_glyphs_func_t) (hb_set_t *glyphs, unsigned value, const void *data);\ntypedef bool (*match_func_t) (hb_glyph_info_t &info, unsigned value, const void *data);\n\nstruct ContextClosureFuncs\n{\n  intersects_func_t intersects;\n  intersected_glyphs_func_t intersected_glyphs;\n};\nstruct ContextCollectGlyphsFuncs\n{\n  collect_glyphs_func_t collect;\n};\nstruct ContextApplyFuncs\n{\n  match_func_t match;\n};\nstruct ChainContextApplyFuncs\n{\n  match_func_t match[3];\n};\n\n\nstatic inline bool intersects_glyph (const hb_set_t *glyphs, unsigned value, const void *data HB_UNUSED, void *cache HB_UNUSED)\n{\n  return glyphs->has (value);\n}\nstatic inline bool intersects_class (const hb_set_t *glyphs, unsigned value, const void *data, void *cache)\n{\n  const ClassDef &class_def = *reinterpret_cast<const ClassDef *>(data);\n  hb_map_t *map = (hb_map_t *) cache;\n\n  hb_codepoint_t *cached_v;\n  if (map->has (value, &cached_v))\n    return *cached_v;\n\n  bool v = class_def.intersects_class (glyphs, value);\n  map->set (value, v);\n\n  return v;\n}\nstatic inline bool intersects_coverage (const hb_set_t *glyphs, unsigned value, const void *data, void *cache HB_UNUSED)\n{\n  Offset16To<Coverage> coverage;\n  coverage = value;\n  return (data+coverage).intersects (glyphs);\n}\n\n\nstatic inline void intersected_glyph (const hb_set_t *glyphs HB_UNUSED, const void *data, unsigned value, hb_set_t *intersected_glyphs, HB_UNUSED void *cache)\n{\n  unsigned g = reinterpret_cast<const HBUINT16 *>(data)[value];\n  intersected_glyphs->add (g);\n}\n\nusing intersected_class_cache_t = hb_hashmap_t<unsigned, hb_set_t>;\n\nstatic inline void intersected_class_glyphs (const hb_set_t *glyphs, const void *data, unsigned value, hb_set_t *intersected_glyphs, void *cache)\n{\n  const ClassDef &class_def = *reinterpret_cast<const ClassDef *>(data);\n\n  intersected_class_cache_t *map = (intersected_class_cache_t *) cache;\n\n  hb_set_t *cached_v;\n  if (map->has (value, &cached_v))\n  {\n    intersected_glyphs->union_ (*cached_v);\n    return;\n  }\n\n  hb_set_t v;\n  class_def.intersected_class_glyphs (glyphs, value, &v);\n\n  intersected_glyphs->union_ (v);\n\n  map->set (value, std::move (v));\n}\n\nstatic inline void intersected_coverage_glyphs (const hb_set_t *glyphs, const void *data, unsigned value, hb_set_t *intersected_glyphs, HB_UNUSED void *cache)\n{\n  Offset16To<Coverage> coverage;\n  coverage = value;\n  (data+coverage).intersect_set (*glyphs, *intersected_glyphs);\n}\n\n\ntemplate <typename HBUINT>\nstatic inline bool array_is_subset_of (const hb_set_t *glyphs,\n\t\t\t\t       unsigned int count,\n\t\t\t\t       const HBUINT values[],\n\t\t\t\t       intersects_func_t intersects_func,\n\t\t\t\t       const void *intersects_data,\n\t\t\t\t       void *cache)\n{\n  for (const auto &_ : + hb_iter (values, count))\n    if (!intersects_func (glyphs, _, intersects_data, cache)) return false;\n  return true;\n}\n\n\nstatic inline void collect_glyph (hb_set_t *glyphs, unsigned value, const void *data HB_UNUSED)\n{\n  glyphs->add (value);\n}\nstatic inline void collect_class (hb_set_t *glyphs, unsigned value, const void *data)\n{\n  const ClassDef &class_def = *reinterpret_cast<const ClassDef *>(data);\n  class_def.collect_class (glyphs, value);\n}\nstatic inline void collect_coverage (hb_set_t *glyphs, unsigned value, const void *data)\n{\n  Offset16To<Coverage> coverage;\n  coverage = value;\n  (data+coverage).collect_coverage (glyphs);\n}\ntemplate <typename HBUINT>\nstatic inline void collect_array (hb_collect_glyphs_context_t *c HB_UNUSED,\n\t\t\t\t  hb_set_t *glyphs,\n\t\t\t\t  unsigned int count,\n\t\t\t\t  const HBUINT values[],\n\t\t\t\t  collect_glyphs_func_t collect_func,\n\t\t\t\t  const void *collect_data)\n{\n  return\n  + hb_iter (values, count)\n  | hb_apply ([&] (const HBUINT &_) { collect_func (glyphs, _, collect_data); })\n  ;\n}\n\n\nstatic inline bool match_glyph (hb_glyph_info_t &info, unsigned value, const void *data HB_UNUSED)\n{\n  return info.codepoint == value;\n}\nstatic inline bool match_class (hb_glyph_info_t &info, unsigned value, const void *data)\n{\n  const ClassDef &class_def = *reinterpret_cast<const ClassDef *>(data);\n  return class_def.get_class (info.codepoint) == value;\n}\nstatic inline bool match_class_cached (hb_glyph_info_t &info, unsigned value, const void *data)\n{\n  unsigned klass = info.syllable();\n  if (klass < 255)\n    return klass == value;\n  const ClassDef &class_def = *reinterpret_cast<const ClassDef *>(data);\n  klass = class_def.get_class (info.codepoint);\n  if (likely (klass < 255))\n    info.syllable() = klass;\n  return klass == value;\n}\nstatic inline bool match_coverage (hb_glyph_info_t &info, unsigned value, const void *data)\n{\n  Offset16To<Coverage> coverage;\n  coverage = value;\n  return (data+coverage).get_coverage (info.codepoint) != NOT_COVERED;\n}\n\ntemplate <typename HBUINT>\nstatic inline bool would_match_input (hb_would_apply_context_t *c,\n\t\t\t\t      unsigned int count, /* Including the first glyph (not matched) */\n\t\t\t\t      const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t      match_func_t match_func,\n\t\t\t\t      const void *match_data)\n{\n  if (count != c->len)\n    return false;\n\n  for (unsigned int i = 1; i < count; i++)\n  {\n    hb_glyph_info_t info;\n    info.codepoint = c->glyphs[i];\n    if (likely (!match_func (info, input[i - 1], match_data)))\n      return false;\n  }\n\n  return true;\n}\ntemplate <typename HBUINT>\nstatic inline bool match_input (hb_ot_apply_context_t *c,\n\t\t\t\tunsigned int count, /* Including the first glyph (not matched) */\n\t\t\t\tconst HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\tmatch_func_t match_func,\n\t\t\t\tconst void *match_data,\n\t\t\t\tunsigned int *end_position,\n\t\t\t\tunsigned int match_positions[HB_MAX_CONTEXT_LENGTH],\n\t\t\t\tunsigned int *p_total_component_count = nullptr)\n{\n  TRACE_APPLY (nullptr);\n\n  if (unlikely (count > HB_MAX_CONTEXT_LENGTH)) return_trace (false);\n\n  hb_buffer_t *buffer = c->buffer;\n\n  hb_ot_apply_context_t::skipping_iterator_t &skippy_iter = c->iter_input;\n  skippy_iter.reset (buffer->idx, count - 1);\n  skippy_iter.set_match_func (match_func, match_data);\n  skippy_iter.set_glyph_data (input);\n\n  /*\n   * This is perhaps the trickiest part of OpenType...  Remarks:\n   *\n   * - If all components of the ligature were marks, we call this a mark ligature.\n   *\n   * - If there is no GDEF, and the ligature is NOT a mark ligature, we categorize\n   *   it as a ligature glyph.\n   *\n   * - Ligatures cannot be formed across glyphs attached to different components\n   *   of previous ligatures.  Eg. the sequence is LAM,SHADDA,LAM,FATHA,HEH, and\n   *   LAM,LAM,HEH form a ligature, leaving SHADDA,FATHA next to eachother.\n   *   However, it would be wrong to ligate that SHADDA,FATHA sequence.\n   *   There are a couple of exceptions to this:\n   *\n   *   o If a ligature tries ligating with marks that belong to it itself, go ahead,\n   *     assuming that the font designer knows what they are doing (otherwise it can\n   *     break Indic stuff when a matra wants to ligate with a conjunct,\n   *\n   *   o If two marks want to ligate and they belong to different components of the\n   *     same ligature glyph, and said ligature glyph is to be ignored according to\n   *     mark-filtering rules, then allow.\n   *     https://github.com/harfbuzz/harfbuzz/issues/545\n   */\n\n  unsigned int total_component_count = 0;\n  total_component_count += _hb_glyph_info_get_lig_num_comps (&buffer->cur());\n\n  unsigned int first_lig_id = _hb_glyph_info_get_lig_id (&buffer->cur());\n  unsigned int first_lig_comp = _hb_glyph_info_get_lig_comp (&buffer->cur());\n\n  enum {\n    LIGBASE_NOT_CHECKED,\n    LIGBASE_MAY_NOT_SKIP,\n    LIGBASE_MAY_SKIP\n  } ligbase = LIGBASE_NOT_CHECKED;\n\n  match_positions[0] = buffer->idx;\n  for (unsigned int i = 1; i < count; i++)\n  {\n    unsigned unsafe_to;\n    if (!skippy_iter.next (&unsafe_to))\n    {\n      *end_position = unsafe_to;\n      return_trace (false);\n    }\n\n    match_positions[i] = skippy_iter.idx;\n\n    unsigned int this_lig_id = _hb_glyph_info_get_lig_id (&buffer->info[skippy_iter.idx]);\n    unsigned int this_lig_comp = _hb_glyph_info_get_lig_comp (&buffer->info[skippy_iter.idx]);\n\n    if (first_lig_id && first_lig_comp)\n    {\n      /* If first component was attached to a previous ligature component,\n       * all subsequent components should be attached to the same ligature\n       * component, otherwise we shouldn't ligate them... */\n      if (first_lig_id != this_lig_id || first_lig_comp != this_lig_comp)\n      {\n\t/* ...unless, we are attached to a base ligature and that base\n\t * ligature is ignorable. */\n\tif (ligbase == LIGBASE_NOT_CHECKED)\n\t{\n\t  bool found = false;\n\t  const auto *out = buffer->out_info;\n\t  unsigned int j = buffer->out_len;\n\t  while (j && _hb_glyph_info_get_lig_id (&out[j - 1]) == first_lig_id)\n\t  {\n\t    if (_hb_glyph_info_get_lig_comp (&out[j - 1]) == 0)\n\t    {\n\t      j--;\n\t      found = true;\n\t      break;\n\t    }\n\t    j--;\n\t  }\n\n\t  if (found && skippy_iter.may_skip (out[j]) == hb_ot_apply_context_t::matcher_t::SKIP_YES)\n\t    ligbase = LIGBASE_MAY_SKIP;\n\t  else\n\t    ligbase = LIGBASE_MAY_NOT_SKIP;\n\t}\n\n\tif (ligbase == LIGBASE_MAY_NOT_SKIP)\n\t  return_trace (false);\n      }\n    }\n    else\n    {\n      /* If first component was NOT attached to a previous ligature component,\n       * all subsequent components should also NOT be attached to any ligature\n       * component, unless they are attached to the first component itself! */\n      if (this_lig_id && this_lig_comp && (this_lig_id != first_lig_id))\n\treturn_trace (false);\n    }\n\n    total_component_count += _hb_glyph_info_get_lig_num_comps (&buffer->info[skippy_iter.idx]);\n  }\n\n  *end_position = skippy_iter.idx + 1;\n\n  if (p_total_component_count)\n    *p_total_component_count = total_component_count;\n\n  return_trace (true);\n}\nstatic inline bool ligate_input (hb_ot_apply_context_t *c,\n\t\t\t\t unsigned int count, /* Including the first glyph */\n\t\t\t\t const unsigned int match_positions[HB_MAX_CONTEXT_LENGTH], /* Including the first glyph */\n\t\t\t\t unsigned int match_end,\n\t\t\t\t hb_codepoint_t lig_glyph,\n\t\t\t\t unsigned int total_component_count)\n{\n  TRACE_APPLY (nullptr);\n\n  hb_buffer_t *buffer = c->buffer;\n\n  buffer->merge_clusters (buffer->idx, match_end);\n\n  /* - If a base and one or more marks ligate, consider that as a base, NOT\n   *   ligature, such that all following marks can still attach to it.\n   *   https://github.com/harfbuzz/harfbuzz/issues/1109\n   *\n   * - If all components of the ligature were marks, we call this a mark ligature.\n   *   If it *is* a mark ligature, we don't allocate a new ligature id, and leave\n   *   the ligature to keep its old ligature id.  This will allow it to attach to\n   *   a base ligature in GPOS.  Eg. if the sequence is: LAM,LAM,SHADDA,FATHA,HEH,\n   *   and LAM,LAM,HEH for a ligature, they will leave SHADDA and FATHA with a\n   *   ligature id and component value of 2.  Then if SHADDA,FATHA form a ligature\n   *   later, we don't want them to lose their ligature id/component, otherwise\n   *   GPOS will fail to correctly position the mark ligature on top of the\n   *   LAM,LAM,HEH ligature.  See:\n   *     https://bugzilla.gnome.org/show_bug.cgi?id=676343\n   *\n   * - If a ligature is formed of components that some of which are also ligatures\n   *   themselves, and those ligature components had marks attached to *their*\n   *   components, we have to attach the marks to the new ligature component\n   *   positions!  Now *that*'s tricky!  And these marks may be following the\n   *   last component of the whole sequence, so we should loop forward looking\n   *   for them and update them.\n   *\n   *   Eg. the sequence is LAM,LAM,SHADDA,FATHA,HEH, and the font first forms a\n   *   'calt' ligature of LAM,HEH, leaving the SHADDA and FATHA with a ligature\n   *   id and component == 1.  Now, during 'liga', the LAM and the LAM-HEH ligature\n   *   form a LAM-LAM-HEH ligature.  We need to reassign the SHADDA and FATHA to\n   *   the new ligature with a component value of 2.\n   *\n   *   This in fact happened to a font...  See:\n   *   https://bugzilla.gnome.org/show_bug.cgi?id=437633\n   */\n\n  bool is_base_ligature = _hb_glyph_info_is_base_glyph (&buffer->info[match_positions[0]]);\n  bool is_mark_ligature = _hb_glyph_info_is_mark (&buffer->info[match_positions[0]]);\n  for (unsigned int i = 1; i < count; i++)\n    if (!_hb_glyph_info_is_mark (&buffer->info[match_positions[i]]))\n    {\n      is_base_ligature = false;\n      is_mark_ligature = false;\n      break;\n    }\n  bool is_ligature = !is_base_ligature && !is_mark_ligature;\n\n  unsigned int klass = is_ligature ? HB_OT_LAYOUT_GLYPH_PROPS_LIGATURE : 0;\n  unsigned int lig_id = is_ligature ? _hb_allocate_lig_id (buffer) : 0;\n  unsigned int last_lig_id = _hb_glyph_info_get_lig_id (&buffer->cur());\n  unsigned int last_num_components = _hb_glyph_info_get_lig_num_comps (&buffer->cur());\n  unsigned int components_so_far = last_num_components;\n\n  if (is_ligature)\n  {\n    _hb_glyph_info_set_lig_props_for_ligature (&buffer->cur(), lig_id, total_component_count);\n    if (_hb_glyph_info_get_general_category (&buffer->cur()) == HB_UNICODE_GENERAL_CATEGORY_NON_SPACING_MARK)\n    {\n      _hb_glyph_info_set_general_category (&buffer->cur(), HB_UNICODE_GENERAL_CATEGORY_OTHER_LETTER);\n    }\n  }\n  c->replace_glyph_with_ligature (lig_glyph, klass);\n\n  for (unsigned int i = 1; i < count; i++)\n  {\n    while (buffer->idx < match_positions[i] && buffer->successful)\n    {\n      if (is_ligature)\n      {\n\tunsigned int this_comp = _hb_glyph_info_get_lig_comp (&buffer->cur());\n\tif (this_comp == 0)\n\t  this_comp = last_num_components;\n\tunsigned int new_lig_comp = components_so_far - last_num_components +\n\t\t\t\t    hb_min (this_comp, last_num_components);\n\t  _hb_glyph_info_set_lig_props_for_mark (&buffer->cur(), lig_id, new_lig_comp);\n      }\n      (void) buffer->next_glyph ();\n    }\n\n    last_lig_id = _hb_glyph_info_get_lig_id (&buffer->cur());\n    last_num_components = _hb_glyph_info_get_lig_num_comps (&buffer->cur());\n    components_so_far += last_num_components;\n\n    /* Skip the base glyph */\n    buffer->idx++;\n  }\n\n  if (!is_mark_ligature && last_lig_id)\n  {\n    /* Re-adjust components for any marks following. */\n    for (unsigned i = buffer->idx; i < buffer->len; ++i)\n    {\n      if (last_lig_id != _hb_glyph_info_get_lig_id (&buffer->info[i])) break;\n\n      unsigned this_comp = _hb_glyph_info_get_lig_comp (&buffer->info[i]);\n      if (!this_comp) break;\n\n      unsigned new_lig_comp = components_so_far - last_num_components +\n\t\t\t      hb_min (this_comp, last_num_components);\n      _hb_glyph_info_set_lig_props_for_mark (&buffer->info[i], lig_id, new_lig_comp);\n    }\n  }\n  return_trace (true);\n}\n\ntemplate <typename HBUINT>\nstatic inline bool match_backtrack (hb_ot_apply_context_t *c,\n\t\t\t\t    unsigned int count,\n\t\t\t\t    const HBUINT backtrack[],\n\t\t\t\t    match_func_t match_func,\n\t\t\t\t    const void *match_data,\n\t\t\t\t    unsigned int *match_start)\n{\n  TRACE_APPLY (nullptr);\n\n  hb_ot_apply_context_t::skipping_iterator_t &skippy_iter = c->iter_context;\n  skippy_iter.reset (c->buffer->backtrack_len (), count);\n  skippy_iter.set_match_func (match_func, match_data);\n  skippy_iter.set_glyph_data (backtrack);\n\n  for (unsigned int i = 0; i < count; i++)\n  {\n    unsigned unsafe_from;\n    if (!skippy_iter.prev (&unsafe_from))\n    {\n      *match_start = unsafe_from;\n      return_trace (false);\n    }\n  }\n\n  *match_start = skippy_iter.idx;\n  return_trace (true);\n}\n\ntemplate <typename HBUINT>\nstatic inline bool match_lookahead (hb_ot_apply_context_t *c,\n\t\t\t\t    unsigned int count,\n\t\t\t\t    const HBUINT lookahead[],\n\t\t\t\t    match_func_t match_func,\n\t\t\t\t    const void *match_data,\n\t\t\t\t    unsigned int start_index,\n\t\t\t\t    unsigned int *end_index)\n{\n  TRACE_APPLY (nullptr);\n\n  hb_ot_apply_context_t::skipping_iterator_t &skippy_iter = c->iter_context;\n  skippy_iter.reset (start_index - 1, count);\n  skippy_iter.set_match_func (match_func, match_data);\n  skippy_iter.set_glyph_data (lookahead);\n\n  for (unsigned int i = 0; i < count; i++)\n  {\n    unsigned unsafe_to;\n    if (!skippy_iter.next (&unsafe_to))\n    {\n      *end_index = unsafe_to;\n      return_trace (false);\n    }\n  }\n\n  *end_index = skippy_iter.idx + 1;\n  return_trace (true);\n}\n\n\n\nstruct LookupRecord\n{\n  bool serialize (hb_serialize_context_t *c,\n\t\t  const hb_map_t         *lookup_map) const\n  {\n    TRACE_SERIALIZE (this);\n    auto *out = c->embed (*this);\n    if (unlikely (!out)) return_trace (false);\n\n    return_trace (c->check_assign (out->lookupListIndex, lookup_map->get (lookupListIndex), HB_SERIALIZE_ERROR_INT_OVERFLOW));\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    return_trace (c->check_struct (this));\n  }\n\n  HBUINT16\tsequenceIndex;\t\t/* Index into current glyph\n\t\t\t\t\t * sequence--first glyph = 0 */\n  HBUINT16\tlookupListIndex;\t/* Lookup to apply to that\n\t\t\t\t\t * position--zero--based */\n  public:\n  DEFINE_SIZE_STATIC (4);\n};\n\nstatic unsigned serialize_lookuprecord_array (hb_serialize_context_t *c,\n\t\t\t\t\t      const hb_array_t<const LookupRecord> lookupRecords,\n\t\t\t\t\t      const hb_map_t *lookup_map)\n{\n  unsigned count = 0;\n  for (const LookupRecord& r : lookupRecords)\n  {\n    if (!lookup_map->has (r.lookupListIndex))\n      continue;\n\n    if (!r.serialize (c, lookup_map))\n      return 0;\n\n    count++;\n  }\n  return count;\n}\n\nenum ContextFormat { SimpleContext = 1, ClassBasedContext = 2, CoverageBasedContext = 3 };\n\ntemplate <typename HBUINT>\nstatic void context_closure_recurse_lookups (hb_closure_context_t *c,\n\t\t\t\t\t     unsigned inputCount, const HBUINT input[],\n\t\t\t\t\t     unsigned lookupCount,\n\t\t\t\t\t     const LookupRecord lookupRecord[] /* Array of LookupRecords--in design order */,\n\t\t\t\t\t     unsigned value,\n\t\t\t\t\t     ContextFormat context_format,\n\t\t\t\t\t     const void *data,\n\t\t\t\t\t     intersected_glyphs_func_t intersected_glyphs_func,\n\t\t\t\t\t     void *cache)\n{\n  hb_set_t covered_seq_indicies;\n  hb_set_t pos_glyphs;\n  for (unsigned int i = 0; i < lookupCount; i++)\n  {\n    unsigned seqIndex = lookupRecord[i].sequenceIndex;\n    if (seqIndex >= inputCount) continue;\n\n    bool has_pos_glyphs = false;\n\n    if (!covered_seq_indicies.has (seqIndex))\n    {\n      has_pos_glyphs = true;\n      pos_glyphs.clear ();\n      if (seqIndex == 0)\n      {\n        switch (context_format) {\n        case ContextFormat::SimpleContext:\n          pos_glyphs.add (value);\n          break;\n        case ContextFormat::ClassBasedContext:\n          intersected_glyphs_func (&c->parent_active_glyphs (), data, value, &pos_glyphs, cache);\n          break;\n        case ContextFormat::CoverageBasedContext:\n          pos_glyphs.set (c->parent_active_glyphs ());\n          break;\n        }\n      }\n      else\n      {\n        const void *input_data = input;\n        unsigned input_value = seqIndex - 1;\n        if (context_format != ContextFormat::SimpleContext)\n        {\n          input_data = data;\n          input_value = input[seqIndex - 1];\n        }\n\n        intersected_glyphs_func (c->glyphs, input_data, input_value, &pos_glyphs, cache);\n      }\n    }\n\n    covered_seq_indicies.add (seqIndex);\n    if (has_pos_glyphs) {\n      c->push_cur_active_glyphs () = std::move (pos_glyphs);\n    } else {\n      c->push_cur_active_glyphs ().set (*c->glyphs);\n    }\n\n    unsigned endIndex = inputCount;\n    if (context_format == ContextFormat::CoverageBasedContext)\n      endIndex += 1;\n\n    c->recurse (lookupRecord[i].lookupListIndex, &covered_seq_indicies, seqIndex, endIndex);\n\n    c->pop_cur_done_glyphs ();\n  }\n}\n\ntemplate <typename context_t>\nstatic inline void recurse_lookups (context_t *c,\n                                    unsigned int lookupCount,\n                                    const LookupRecord lookupRecord[] /* Array of LookupRecords--in design order */)\n{\n  for (unsigned int i = 0; i < lookupCount; i++)\n    c->recurse (lookupRecord[i].lookupListIndex);\n}\n\nstatic inline void apply_lookup (hb_ot_apply_context_t *c,\n\t\t\t\t unsigned int count, /* Including the first glyph */\n\t\t\t\t unsigned int match_positions[HB_MAX_CONTEXT_LENGTH], /* Including the first glyph */\n\t\t\t\t unsigned int lookupCount,\n\t\t\t\t const LookupRecord lookupRecord[], /* Array of LookupRecords--in design order */\n\t\t\t\t unsigned int match_end)\n{\n  hb_buffer_t *buffer = c->buffer;\n  int end;\n\n  /* All positions are distance from beginning of *output* buffer.\n   * Adjust. */\n  {\n    unsigned int bl = buffer->backtrack_len ();\n    end = bl + match_end - buffer->idx;\n\n    int delta = bl - buffer->idx;\n    /* Convert positions to new indexing. */\n    for (unsigned int j = 0; j < count; j++)\n      match_positions[j] += delta;\n  }\n\n  for (unsigned int i = 0; i < lookupCount && buffer->successful; i++)\n  {\n    unsigned int idx = lookupRecord[i].sequenceIndex;\n    if (idx >= count)\n      continue;\n\n    unsigned int orig_len = buffer->backtrack_len () + buffer->lookahead_len ();\n\n    /* This can happen if earlier recursed lookups deleted many entries. */\n    if (unlikely (match_positions[idx] >= orig_len))\n      continue;\n\n    if (unlikely (!buffer->move_to (match_positions[idx])))\n      break;\n\n    if (unlikely (buffer->max_ops <= 0))\n      break;\n\n    if (HB_BUFFER_MESSAGE_MORE && c->buffer->messaging ())\n    {\n      if (buffer->have_output)\n        c->buffer->sync_so_far ();\n      c->buffer->message (c->font,\n\t\t\t  \"recursing to lookup %u at %u\",\n\t\t\t  (unsigned) lookupRecord[i].lookupListIndex,\n\t\t\t  buffer->idx);\n    }\n\n    if (!c->recurse (lookupRecord[i].lookupListIndex))\n      continue;\n\n    if (HB_BUFFER_MESSAGE_MORE && c->buffer->messaging ())\n    {\n      if (buffer->have_output)\n        c->buffer->sync_so_far ();\n      c->buffer->message (c->font,\n\t\t\t  \"recursed to lookup %u\",\n\t\t\t  (unsigned) lookupRecord[i].lookupListIndex);\n    }\n\n    unsigned int new_len = buffer->backtrack_len () + buffer->lookahead_len ();\n    int delta = new_len - orig_len;\n\n    if (!delta)\n      continue;\n\n    /* Recursed lookup changed buffer len.  Adjust.\n     *\n     * TODO:\n     *\n     * Right now, if buffer length increased by n, we assume n new glyphs\n     * were added right after the current position, and if buffer length\n     * was decreased by n, we assume n match positions after the current\n     * one where removed.  The former (buffer length increased) case is\n     * fine, but the decrease case can be improved in at least two ways,\n     * both of which are significant:\n     *\n     *   - If recursed-to lookup is MultipleSubst and buffer length\n     *     decreased, then it's current match position that was deleted,\n     *     NOT the one after it.\n     *\n     *   - If buffer length was decreased by n, it does not necessarily\n     *     mean that n match positions where removed, as there recursed-to\n     *     lookup might had a different LookupFlag.  Here's a constructed\n     *     case of that:\n     *     https://github.com/harfbuzz/harfbuzz/discussions/3538\n     *\n     * It should be possible to construct tests for both of these cases.\n     */\n\n    end += delta;\n    if (end < int (match_positions[idx]))\n    {\n      /* End might end up being smaller than match_positions[idx] if the recursed\n       * lookup ended up removing many items.\n       * Just never rewind end beyond start of current position, since that is\n       * not possible in the recursed lookup.  Also adjust delta as such.\n       *\n       * https://bugs.chromium.org/p/chromium/issues/detail?id=659496\n       * https://github.com/harfbuzz/harfbuzz/issues/1611\n       */\n      delta += match_positions[idx] - end;\n      end = match_positions[idx];\n    }\n\n    unsigned int next = idx + 1; /* next now is the position after the recursed lookup. */\n\n    if (delta > 0)\n    {\n      if (unlikely (delta + count > HB_MAX_CONTEXT_LENGTH))\n\tbreak;\n    }\n    else\n    {\n      /* NOTE: delta is non-positive. */\n      delta = hb_max (delta, (int) next - (int) count);\n      next -= delta;\n    }\n\n    /* Shift! */\n    memmove (match_positions + next + delta, match_positions + next,\n\t     (count - next) * sizeof (match_positions[0]));\n    next += delta;\n    count += delta;\n\n    /* Fill in new entries. */\n    for (unsigned int j = idx + 1; j < next; j++)\n      match_positions[j] = match_positions[j - 1] + 1;\n\n    /* And fixup the rest. */\n    for (; next < count; next++)\n      match_positions[next] += delta;\n  }\n\n  (void) buffer->move_to (end);\n}\n\n\n\n/* Contextual lookups */\n\nstruct ContextClosureLookupContext\n{\n  ContextClosureFuncs funcs;\n  ContextFormat context_format;\n  const void *intersects_data;\n  void *intersects_cache;\n  void *intersected_glyphs_cache;\n};\n\nstruct ContextCollectGlyphsLookupContext\n{\n  ContextCollectGlyphsFuncs funcs;\n  const void *collect_data;\n};\n\nstruct ContextApplyLookupContext\n{\n  ContextApplyFuncs funcs;\n  const void *match_data;\n};\n\ntemplate <typename HBUINT>\nstatic inline bool context_intersects (const hb_set_t *glyphs,\n\t\t\t\t       unsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t       const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t       ContextClosureLookupContext &lookup_context)\n{\n  return array_is_subset_of (glyphs,\n\t\t\t     inputCount ? inputCount - 1 : 0, input,\n\t\t\t     lookup_context.funcs.intersects,\n\t\t\t     lookup_context.intersects_data,\n\t\t\t     lookup_context.intersects_cache);\n}\n\ntemplate <typename HBUINT>\nstatic inline void context_closure_lookup (hb_closure_context_t *c,\n\t\t\t\t\t   unsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t\t   const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t\t   unsigned int lookupCount,\n\t\t\t\t\t   const LookupRecord lookupRecord[],\n\t\t\t\t\t   unsigned value, /* Index of first glyph in Coverage or Class value in ClassDef table */\n\t\t\t\t\t   ContextClosureLookupContext &lookup_context)\n{\n  if (context_intersects (c->glyphs,\n\t\t\t  inputCount, input,\n\t\t\t  lookup_context))\n    context_closure_recurse_lookups (c,\n\t\t\t\t     inputCount, input,\n\t\t\t\t     lookupCount, lookupRecord,\n\t\t\t\t     value,\n\t\t\t\t     lookup_context.context_format,\n\t\t\t\t     lookup_context.intersects_data,\n\t\t\t\t     lookup_context.funcs.intersected_glyphs,\n\t\t\t\t     lookup_context.intersected_glyphs_cache);\n}\n\ntemplate <typename HBUINT>\nstatic inline void context_collect_glyphs_lookup (hb_collect_glyphs_context_t *c,\n\t\t\t\t\t\t  unsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t\t\t  const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t\t\t  unsigned int lookupCount,\n\t\t\t\t\t\t  const LookupRecord lookupRecord[],\n\t\t\t\t\t\t  ContextCollectGlyphsLookupContext &lookup_context)\n{\n  collect_array (c, c->input,\n\t\t inputCount ? inputCount - 1 : 0, input,\n\t\t lookup_context.funcs.collect, lookup_context.collect_data);\n  recurse_lookups (c,\n\t\t   lookupCount, lookupRecord);\n}\n\ntemplate <typename HBUINT>\nstatic inline bool context_would_apply_lookup (hb_would_apply_context_t *c,\n\t\t\t\t\t       unsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t\t       const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t\t       unsigned int lookupCount HB_UNUSED,\n\t\t\t\t\t       const LookupRecord lookupRecord[] HB_UNUSED,\n\t\t\t\t\t       const ContextApplyLookupContext &lookup_context)\n{\n  return would_match_input (c,\n\t\t\t    inputCount, input,\n\t\t\t    lookup_context.funcs.match, lookup_context.match_data);\n}\n\ntemplate <typename HBUINT>\nstatic inline bool context_apply_lookup (hb_ot_apply_context_t *c,\n\t\t\t\t\t unsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t\t const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t\t unsigned int lookupCount,\n\t\t\t\t\t const LookupRecord lookupRecord[],\n\t\t\t\t\t const ContextApplyLookupContext &lookup_context)\n{\n  unsigned match_end = 0;\n  unsigned match_positions[HB_MAX_CONTEXT_LENGTH];\n  if (match_input (c,\n\t\t   inputCount, input,\n\t\t   lookup_context.funcs.match, lookup_context.match_data,\n\t\t   &match_end, match_positions))\n  {\n    c->buffer->unsafe_to_break (c->buffer->idx, match_end);\n    apply_lookup (c,\n\t\t  inputCount, match_positions,\n\t\t  lookupCount, lookupRecord,\n\t\t  match_end);\n    return true;\n  }\n  else\n  {\n    c->buffer->unsafe_to_concat (c->buffer->idx, match_end);\n    return false;\n  }\n}\n\ntemplate <typename Types>\nstruct Rule\n{\n  bool intersects (const hb_set_t *glyphs, ContextClosureLookupContext &lookup_context) const\n  {\n    return context_intersects (glyphs,\n\t\t\t       inputCount, inputZ.arrayZ,\n\t\t\t       lookup_context);\n  }\n\n  void closure (hb_closure_context_t *c, unsigned value, ContextClosureLookupContext &lookup_context) const\n  {\n    if (unlikely (c->lookup_limit_exceeded ())) return;\n\n    const auto &lookupRecord = StructAfter<UnsizedArrayOf<LookupRecord>>\n\t\t\t\t\t   (inputZ.as_array ((inputCount ? inputCount - 1 : 0)));\n    context_closure_lookup (c,\n\t\t\t    inputCount, inputZ.arrayZ,\n\t\t\t    lookupCount, lookupRecord.arrayZ,\n\t\t\t    value, lookup_context);\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c,\n                        ContextClosureLookupContext &lookup_context) const\n  {\n    if (unlikely (c->lookup_limit_exceeded ())) return;\n    if (!intersects (c->glyphs, lookup_context)) return;\n\n    const auto &lookupRecord = StructAfter<UnsizedArrayOf<LookupRecord>>\n\t\t\t\t\t   (inputZ.as_array (inputCount ? inputCount - 1 : 0));\n    recurse_lookups (c, lookupCount, lookupRecord.arrayZ);\n  }\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c,\n\t\t       ContextCollectGlyphsLookupContext &lookup_context) const\n  {\n    const auto &lookupRecord = StructAfter<UnsizedArrayOf<LookupRecord>>\n\t\t\t\t\t   (inputZ.as_array (inputCount ? inputCount - 1 : 0));\n    context_collect_glyphs_lookup (c,\n\t\t\t\t   inputCount, inputZ.arrayZ,\n\t\t\t\t   lookupCount, lookupRecord.arrayZ,\n\t\t\t\t   lookup_context);\n  }\n\n  bool would_apply (hb_would_apply_context_t *c,\n\t\t    const ContextApplyLookupContext &lookup_context) const\n  {\n    const auto &lookupRecord = StructAfter<UnsizedArrayOf<LookupRecord>>\n\t\t\t\t\t   (inputZ.as_array (inputCount ? inputCount - 1 : 0));\n    return context_would_apply_lookup (c,\n\t\t\t\t       inputCount, inputZ.arrayZ,\n\t\t\t\t       lookupCount, lookupRecord.arrayZ,\n\t\t\t\t       lookup_context);\n  }\n\n  bool apply (hb_ot_apply_context_t *c,\n\t      const ContextApplyLookupContext &lookup_context) const\n  {\n    TRACE_APPLY (this);\n    const auto &lookupRecord = StructAfter<UnsizedArrayOf<LookupRecord>>\n\t\t\t\t\t   (inputZ.as_array (inputCount ? inputCount - 1 : 0));\n    return_trace (context_apply_lookup (c, inputCount, inputZ.arrayZ, lookupCount, lookupRecord.arrayZ, lookup_context));\n  }\n\n  bool serialize (hb_serialize_context_t *c,\n\t\t  const hb_map_t *input_mapping, /* old->new glyphid or class mapping */\n\t\t  const hb_map_t *lookup_map) const\n  {\n    TRACE_SERIALIZE (this);\n    auto *out = c->start_embed (this);\n    if (unlikely (!c->extend_min (out))) return_trace (false);\n\n    out->inputCount = inputCount;\n    const auto input = inputZ.as_array (inputCount - 1);\n    for (const auto org : input)\n    {\n      HBUINT16 d;\n      d = input_mapping->get (org);\n      c->copy (d);\n    }\n\n    const auto &lookupRecord = StructAfter<UnsizedArrayOf<LookupRecord>>\n\t\t\t\t\t   (inputZ.as_array ((inputCount ? inputCount - 1 : 0)));\n\n    unsigned count = serialize_lookuprecord_array (c, lookupRecord.as_array (lookupCount), lookup_map);\n    return_trace (c->check_assign (out->lookupCount, count, HB_SERIALIZE_ERROR_INT_OVERFLOW));\n  }\n\n  bool subset (hb_subset_context_t *c,\n\t       const hb_map_t *lookup_map,\n\t       const hb_map_t *klass_map = nullptr) const\n  {\n    TRACE_SUBSET (this);\n    if (unlikely (!inputCount)) return_trace (false);\n    const auto input = inputZ.as_array (inputCount - 1);\n\n    const hb_map_t *mapping = klass_map == nullptr ? c->plan->glyph_map : klass_map;\n    if (!hb_all (input, mapping)) return_trace (false);\n    return_trace (serialize (c->serializer, mapping, lookup_map));\n  }\n\n  public:\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    return_trace (inputCount.sanitize (c) &&\n\t\t  lookupCount.sanitize (c) &&\n\t\t  c->check_range (inputZ.arrayZ,\n\t\t\t\t  inputZ.item_size * (inputCount ? inputCount - 1 : 0) +\n\t\t\t\t  LookupRecord::static_size * lookupCount));\n  }\n\n  protected:\n  HBUINT16\tinputCount;\t\t/* Total number of glyphs in input\n\t\t\t\t\t * glyph sequence--includes the first\n\t\t\t\t\t * glyph */\n  HBUINT16\tlookupCount;\t\t/* Number of LookupRecords */\n  UnsizedArrayOf<typename Types::HBUINT>\n\t\tinputZ;\t\t\t/* Array of match inputs--start with\n\t\t\t\t\t * second glyph */\n/*UnsizedArrayOf<LookupRecord>\n\t\tlookupRecordX;*/\t/* Array of LookupRecords--in\n\t\t\t\t\t * design order */\n  public:\n  DEFINE_SIZE_ARRAY (4, inputZ);\n};\n\ntemplate <typename Types>\nstruct RuleSet\n{\n  using Rule = OT::Rule<Types>;\n\n  bool intersects (const hb_set_t *glyphs,\n\t\t   ContextClosureLookupContext &lookup_context) const\n  {\n    return\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_map ([&] (const Rule &_) { return _.intersects (glyphs, lookup_context); })\n    | hb_any\n    ;\n  }\n\n  void closure (hb_closure_context_t *c, unsigned value,\n\t\tContextClosureLookupContext &lookup_context) const\n  {\n    if (unlikely (c->lookup_limit_exceeded ())) return;\n\n    return\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const Rule &_) { _.closure (c, value, lookup_context); })\n    ;\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c,\n                        ContextClosureLookupContext &lookup_context) const\n  {\n    if (unlikely (c->lookup_limit_exceeded ())) return;\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const Rule &_) { _.closure_lookups (c, lookup_context); })\n    ;\n  }\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c,\n\t\t       ContextCollectGlyphsLookupContext &lookup_context) const\n  {\n    return\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const Rule &_) { _.collect_glyphs (c, lookup_context); })\n    ;\n  }\n\n  bool would_apply (hb_would_apply_context_t *c,\n\t\t    const ContextApplyLookupContext &lookup_context) const\n  {\n    return\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_map ([&] (const Rule &_) { return _.would_apply (c, lookup_context); })\n    | hb_any\n    ;\n  }\n\n  bool apply (hb_ot_apply_context_t *c,\n\t      const ContextApplyLookupContext &lookup_context) const\n  {\n    TRACE_APPLY (this);\n    return_trace (\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_map ([&] (const Rule &_) { return _.apply (c, lookup_context); })\n    | hb_any\n    )\n    ;\n  }\n\n  bool subset (hb_subset_context_t *c,\n\t       const hb_map_t *lookup_map,\n\t       const hb_map_t *klass_map = nullptr) const\n  {\n    TRACE_SUBSET (this);\n\n    auto snap = c->serializer->snapshot ();\n    auto *out = c->serializer->start_embed (*this);\n    if (unlikely (!c->serializer->extend_min (out))) return_trace (false);\n\n    for (const Offset16To<Rule>& _ : rule)\n    {\n      if (!_) continue;\n      auto o_snap = c->serializer->snapshot ();\n      auto *o = out->rule.serialize_append (c->serializer);\n      if (unlikely (!o)) continue;\n\n      if (!o->serialize_subset (c, _, this, lookup_map, klass_map))\n      {\n\tout->rule.pop ();\n\tc->serializer->revert (o_snap);\n      }\n    }\n\n    bool ret = bool (out->rule);\n    if (!ret) c->serializer->revert (snap);\n\n    return_trace (ret);\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    return_trace (rule.sanitize (c, this));\n  }\n\n  protected:\n  Array16OfOffset16To<Rule>\n\t\trule;\t\t\t/* Array of Rule tables\n\t\t\t\t\t * ordered by preference */\n  public:\n  DEFINE_SIZE_ARRAY (2, rule);\n};\n\n\ntemplate <typename Types>\nstruct ContextFormat1_4\n{\n  using RuleSet = OT::RuleSet<Types>;\n\n  bool intersects (const hb_set_t *glyphs) const\n  {\n    struct ContextClosureLookupContext lookup_context = {\n      {intersects_glyph, intersected_glyph},\n      ContextFormat::SimpleContext,\n      nullptr\n    };\n\n    return\n    + hb_zip (this+coverage, ruleSet)\n    | hb_filter (*glyphs, hb_first)\n    | hb_map (hb_second)\n    | hb_map (hb_add (this))\n    | hb_map ([&] (const RuleSet &_) { return _.intersects (glyphs, lookup_context); })\n    | hb_any\n    ;\n  }\n\n  bool may_have_non_1to1 () const\n  { return true; }\n\n  void closure (hb_closure_context_t *c) const\n  {\n    hb_set_t& cur_active_glyphs = c->push_cur_active_glyphs ();\n    get_coverage ().intersect_set (c->previous_parent_active_glyphs (), cur_active_glyphs);\n\n    struct ContextClosureLookupContext lookup_context = {\n      {intersects_glyph, intersected_glyph},\n      ContextFormat::SimpleContext,\n      nullptr\n    };\n\n    + hb_zip (this+coverage, hb_range ((unsigned) ruleSet.len))\n    | hb_filter ([&] (hb_codepoint_t _) {\n      return c->previous_parent_active_glyphs ().has (_);\n    }, hb_first)\n    | hb_map ([&](const hb_pair_t<hb_codepoint_t, unsigned> _) { return hb_pair_t<unsigned, const RuleSet&> (_.first, this+ruleSet[_.second]); })\n    | hb_apply ([&] (const hb_pair_t<unsigned, const RuleSet&>& _) { _.second.closure (c, _.first, lookup_context); })\n    ;\n\n    c->pop_cur_done_glyphs ();\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c) const\n  {\n    struct ContextClosureLookupContext lookup_context = {\n      {intersects_glyph, nullptr},\n      ContextFormat::SimpleContext,\n      nullptr\n    };\n\n    + hb_zip (this+coverage, ruleSet)\n    | hb_filter (*c->glyphs, hb_first)\n    | hb_map (hb_second)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const RuleSet &_) { _.closure_lookups (c, lookup_context); })\n    ;\n  }\n\n  void collect_variation_indices (hb_collect_variation_indices_context_t *c) const {}\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c) const\n  {\n    (this+coverage).collect_coverage (c->input);\n\n    struct ContextCollectGlyphsLookupContext lookup_context = {\n      {collect_glyph},\n      nullptr\n    };\n\n    + hb_iter (ruleSet)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const RuleSet &_) { _.collect_glyphs (c, lookup_context); })\n    ;\n  }\n\n  bool would_apply (hb_would_apply_context_t *c) const\n  {\n    const RuleSet &rule_set = this+ruleSet[(this+coverage).get_coverage (c->glyphs[0])];\n    struct ContextApplyLookupContext lookup_context = {\n      {match_glyph},\n      nullptr\n    };\n    return rule_set.would_apply (c, lookup_context);\n  }\n\n  const Coverage &get_coverage () const { return this+coverage; }\n\n  bool apply (hb_ot_apply_context_t *c) const\n  {\n    TRACE_APPLY (this);\n    unsigned int index = (this+coverage).get_coverage (c->buffer->cur().codepoint);\n    if (likely (index == NOT_COVERED))\n      return_trace (false);\n\n    const RuleSet &rule_set = this+ruleSet[index];\n    struct ContextApplyLookupContext lookup_context = {\n      {match_glyph},\n      nullptr\n    };\n    return_trace (rule_set.apply (c, lookup_context));\n  }\n\n  bool subset (hb_subset_context_t *c) const\n  {\n    TRACE_SUBSET (this);\n    const hb_set_t &glyphset = *c->plan->glyphset_gsub ();\n    const hb_map_t &glyph_map = *c->plan->glyph_map;\n\n    auto *out = c->serializer->start_embed (*this);\n    if (unlikely (!c->serializer->extend_min (out))) return_trace (false);\n    out->format = format;\n\n    const hb_map_t *lookup_map = c->table_tag == HB_OT_TAG_GSUB ? &c->plan->gsub_lookups : &c->plan->gpos_lookups;\n    hb_sorted_vector_t<hb_codepoint_t> new_coverage;\n    + hb_zip (this+coverage, ruleSet)\n    | hb_filter (glyphset, hb_first)\n    | hb_filter (subset_offset_array (c, out->ruleSet, this, lookup_map), hb_second)\n    | hb_map (hb_first)\n    | hb_map (glyph_map)\n    | hb_sink (new_coverage)\n    ;\n\n    out->coverage.serialize_serialize (c->serializer, new_coverage.iter ());\n    return_trace (bool (new_coverage));\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    return_trace (coverage.sanitize (c, this) && ruleSet.sanitize (c, this));\n  }\n\n  protected:\n  HBUINT16\tformat;\t\t\t/* Format identifier--format = 1 */\n  typename Types::template OffsetTo<Coverage>\n\t\tcoverage;\t\t/* Offset to Coverage table--from\n\t\t\t\t\t * beginning of table */\n  Array16Of<typename Types::template OffsetTo<RuleSet>>\n\t\truleSet;\t\t/* Array of RuleSet tables\n\t\t\t\t\t * ordered by Coverage Index */\n  public:\n  DEFINE_SIZE_ARRAY (2 + 2 * Types::size, ruleSet);\n};\n\n\ntemplate <typename Types>\nstruct ContextFormat2_5\n{\n  using RuleSet = OT::RuleSet<SmallTypes>;\n\n  bool intersects (const hb_set_t *glyphs) const\n  {\n    if (!(this+coverage).intersects (glyphs))\n      return false;\n\n    const ClassDef &class_def = this+classDef;\n\n    hb_map_t cache;\n    struct ContextClosureLookupContext lookup_context = {\n      {intersects_class, nullptr},\n      ContextFormat::ClassBasedContext,\n      &class_def,\n      &cache\n    };\n\n    hb_set_t retained_coverage_glyphs;\n    (this+coverage).intersect_set (*glyphs, retained_coverage_glyphs);\n\n    hb_set_t coverage_glyph_classes;\n    class_def.intersected_classes (&retained_coverage_glyphs, &coverage_glyph_classes);\n\n\n    return\n    + hb_iter (ruleSet)\n    | hb_map (hb_add (this))\n    | hb_enumerate\n    | hb_map ([&] (const hb_pair_t<unsigned, const RuleSet &> p)\n\t      { return class_def.intersects_class (glyphs, p.first) &&\n\t\t       coverage_glyph_classes.has (p.first) &&\n\t\t       p.second.intersects (glyphs, lookup_context); })\n    | hb_any\n    ;\n  }\n\n  bool may_have_non_1to1 () const\n  { return true; }\n\n  void closure (hb_closure_context_t *c) const\n  {\n    if (!(this+coverage).intersects (c->glyphs))\n      return;\n\n    hb_set_t& cur_active_glyphs = c->push_cur_active_glyphs ();\n    get_coverage ().intersect_set (c->previous_parent_active_glyphs (),\n                                                 cur_active_glyphs);\n\n    const ClassDef &class_def = this+classDef;\n\n    hb_map_t cache;\n    intersected_class_cache_t intersected_cache;\n    struct ContextClosureLookupContext lookup_context = {\n      {intersects_class, intersected_class_glyphs},\n      ContextFormat::ClassBasedContext,\n      &class_def,\n      &cache,\n      &intersected_cache\n    };\n\n    + hb_enumerate (ruleSet)\n    | hb_filter ([&] (unsigned _)\n    { return class_def.intersects_class (&c->parent_active_glyphs (), _); },\n\t\t hb_first)\n    | hb_apply ([&] (const hb_pair_t<unsigned, const typename Types::template OffsetTo<RuleSet>&> _)\n                {\n                  const RuleSet& rule_set = this+_.second;\n                  rule_set.closure (c, _.first, lookup_context);\n                })\n    ;\n\n    c->pop_cur_done_glyphs ();\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c) const\n  {\n    if (!(this+coverage).intersects (c->glyphs))\n      return;\n\n    const ClassDef &class_def = this+classDef;\n\n    hb_map_t cache;\n    struct ContextClosureLookupContext lookup_context = {\n      {intersects_class, nullptr},\n      ContextFormat::ClassBasedContext,\n      &class_def,\n      &cache\n    };\n\n    + hb_iter (ruleSet)\n    | hb_map (hb_add (this))\n    | hb_enumerate\n    | hb_filter ([&] (const hb_pair_t<unsigned, const RuleSet &> p)\n    { return class_def.intersects_class (c->glyphs, p.first); })\n    | hb_map (hb_second)\n    | hb_apply ([&] (const RuleSet & _)\n    { _.closure_lookups (c, lookup_context); });\n  }\n\n  void collect_variation_indices (hb_collect_variation_indices_context_t *c) const {}\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c) const\n  {\n    (this+coverage).collect_coverage (c->input);\n\n    const ClassDef &class_def = this+classDef;\n    struct ContextCollectGlyphsLookupContext lookup_context = {\n      {collect_class},\n      &class_def\n    };\n\n    + hb_iter (ruleSet)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const RuleSet &_) { _.collect_glyphs (c, lookup_context); })\n    ;\n  }\n\n  bool would_apply (hb_would_apply_context_t *c) const\n  {\n    const ClassDef &class_def = this+classDef;\n    unsigned int index = class_def.get_class (c->glyphs[0]);\n    const RuleSet &rule_set = this+ruleSet[index];\n    struct ContextApplyLookupContext lookup_context = {\n      {match_class},\n      &class_def\n    };\n    return rule_set.would_apply (c, lookup_context);\n  }\n\n  const Coverage &get_coverage () const { return this+coverage; }\n\n  unsigned cache_cost () const\n  {\n    unsigned c = (this+classDef).cost () * ruleSet.len;\n    return c >= 4 ? c : 0;\n  }\n  bool cache_func (hb_ot_apply_context_t *c, bool enter) const\n  {\n    if (enter)\n    {\n      if (!HB_BUFFER_TRY_ALLOCATE_VAR (c->buffer, syllable))\n\treturn false;\n      auto &info = c->buffer->info;\n      unsigned count = c->buffer->len;\n      for (unsigned i = 0; i < count; i++)\n\tinfo[i].syllable() = 255;\n      c->new_syllables = 255;\n      return true;\n    }\n    else\n    {\n      c->new_syllables = (unsigned) -1;\n      HB_BUFFER_DEALLOCATE_VAR (c->buffer, syllable);\n      return true;\n    }\n  }\n\n  bool apply (hb_ot_apply_context_t *c, bool cached = false) const\n  {\n    TRACE_APPLY (this);\n    unsigned int index = (this+coverage).get_coverage (c->buffer->cur().codepoint);\n    if (likely (index == NOT_COVERED)) return_trace (false);\n\n    const ClassDef &class_def = this+classDef;\n\n    struct ContextApplyLookupContext lookup_context = {\n      {cached ? match_class_cached : match_class},\n      &class_def\n    };\n\n    if (cached && c->buffer->cur().syllable() < 255)\n      index = c->buffer->cur().syllable ();\n    else\n    {\n      index = class_def.get_class (c->buffer->cur().codepoint);\n      if (cached && index < 255)\n\tc->buffer->cur().syllable() = index;\n    }\n    const RuleSet &rule_set = this+ruleSet[index];\n    return_trace (rule_set.apply (c, lookup_context));\n  }\n\n  bool subset (hb_subset_context_t *c) const\n  {\n    TRACE_SUBSET (this);\n    auto *out = c->serializer->start_embed (*this);\n    if (unlikely (!c->serializer->extend_min (out))) return_trace (false);\n    out->format = format;\n    if (unlikely (!out->coverage.serialize_subset (c, coverage, this)))\n      return_trace (false);\n\n    hb_map_t klass_map;\n    out->classDef.serialize_subset (c, classDef, this, &klass_map);\n\n    const hb_set_t* glyphset = c->plan->glyphset_gsub ();\n    hb_set_t retained_coverage_glyphs;\n    (this+coverage).intersect_set (*glyphset, retained_coverage_glyphs);\n\n    hb_set_t coverage_glyph_classes;\n    (this+classDef).intersected_classes (&retained_coverage_glyphs, &coverage_glyph_classes);\n\n    const hb_map_t *lookup_map = c->table_tag == HB_OT_TAG_GSUB ? &c->plan->gsub_lookups : &c->plan->gpos_lookups;\n    bool ret = true;\n    int non_zero_index = -1, index = 0;\n    auto snapshot = c->serializer->snapshot();\n    for (const auto& _ : + hb_enumerate (ruleSet)\n\t\t\t | hb_filter (klass_map, hb_first))\n    {\n      auto *o = out->ruleSet.serialize_append (c->serializer);\n      if (unlikely (!o))\n      {\n\tret = false;\n\tbreak;\n      }\n\n      if (coverage_glyph_classes.has (_.first) &&\n\t  o->serialize_subset (c, _.second, this, lookup_map, &klass_map)) {\n\tnon_zero_index = index;\n        snapshot = c->serializer->snapshot();\n      }\n\n      index++;\n    }\n\n    if (!ret || non_zero_index == -1) return_trace (false);\n\n    //prune empty trailing ruleSets\n    --index;\n    while (index > non_zero_index)\n    {\n      out->ruleSet.pop ();\n      index--;\n    }\n    c->serializer->revert (snapshot);\n\n    return_trace (bool (out->ruleSet));\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    return_trace (coverage.sanitize (c, this) && classDef.sanitize (c, this) && ruleSet.sanitize (c, this));\n  }\n\n  protected:\n  HBUINT16\tformat;\t\t\t/* Format identifier--format = 2 */\n  typename Types::template OffsetTo<Coverage>\n\t\tcoverage;\t\t/* Offset to Coverage table--from\n\t\t\t\t\t * beginning of table */\n  typename Types::template OffsetTo<ClassDef>\n\t\tclassDef;\t\t/* Offset to glyph ClassDef table--from\n\t\t\t\t\t * beginning of table */\n  Array16Of<typename Types::template OffsetTo<RuleSet>>\n\t\truleSet;\t\t/* Array of RuleSet tables\n\t\t\t\t\t * ordered by class */\n  public:\n  DEFINE_SIZE_ARRAY (4 + 2 * Types::size, ruleSet);\n};\n\n\nstruct ContextFormat3\n{\n  using RuleSet = OT::RuleSet<SmallTypes>;\n\n  bool intersects (const hb_set_t *glyphs) const\n  {\n    if (!(this+coverageZ[0]).intersects (glyphs))\n      return false;\n\n    struct ContextClosureLookupContext lookup_context = {\n      {intersects_coverage, nullptr},\n      ContextFormat::CoverageBasedContext,\n      this\n    };\n    return context_intersects (glyphs,\n\t\t\t       glyphCount, (const HBUINT16 *) (coverageZ.arrayZ + 1),\n\t\t\t       lookup_context);\n  }\n\n  bool may_have_non_1to1 () const\n  { return true; }\n\n  void closure (hb_closure_context_t *c) const\n  {\n    if (!(this+coverageZ[0]).intersects (c->glyphs))\n      return;\n\n    hb_set_t& cur_active_glyphs = c->push_cur_active_glyphs ();\n    get_coverage ().intersect_set (c->previous_parent_active_glyphs (),\n                                                 cur_active_glyphs);\n\n\n    const LookupRecord *lookupRecord = &StructAfter<LookupRecord> (coverageZ.as_array (glyphCount));\n    struct ContextClosureLookupContext lookup_context = {\n      {intersects_coverage, intersected_coverage_glyphs},\n      ContextFormat::CoverageBasedContext,\n      this\n    };\n    context_closure_lookup (c,\n\t\t\t    glyphCount, (const HBUINT16 *) (coverageZ.arrayZ + 1),\n\t\t\t    lookupCount, lookupRecord,\n\t\t\t    0, lookup_context);\n\n    c->pop_cur_done_glyphs ();\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c) const\n  {\n    if (!intersects (c->glyphs))\n      return;\n    const LookupRecord *lookupRecord = &StructAfter<LookupRecord> (coverageZ.as_array (glyphCount));\n    recurse_lookups (c, lookupCount, lookupRecord);\n  }\n\n  void collect_variation_indices (hb_collect_variation_indices_context_t *c) const {}\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c) const\n  {\n    (this+coverageZ[0]).collect_coverage (c->input);\n\n    const LookupRecord *lookupRecord = &StructAfter<LookupRecord> (coverageZ.as_array (glyphCount));\n    struct ContextCollectGlyphsLookupContext lookup_context = {\n      {collect_coverage},\n      this\n    };\n\n    context_collect_glyphs_lookup (c,\n\t\t\t\t   glyphCount, (const HBUINT16 *) (coverageZ.arrayZ + 1),\n\t\t\t\t   lookupCount, lookupRecord,\n\t\t\t\t   lookup_context);\n  }\n\n  bool would_apply (hb_would_apply_context_t *c) const\n  {\n    const LookupRecord *lookupRecord = &StructAfter<LookupRecord> (coverageZ.as_array (glyphCount));\n    struct ContextApplyLookupContext lookup_context = {\n      {match_coverage},\n      this\n    };\n    return context_would_apply_lookup (c,\n\t\t\t\t       glyphCount, (const HBUINT16 *) (coverageZ.arrayZ + 1),\n\t\t\t\t       lookupCount, lookupRecord,\n\t\t\t\t       lookup_context);\n  }\n\n  const Coverage &get_coverage () const { return this+coverageZ[0]; }\n\n  bool apply (hb_ot_apply_context_t *c) const\n  {\n    TRACE_APPLY (this);\n    unsigned int index = (this+coverageZ[0]).get_coverage (c->buffer->cur().codepoint);\n    if (likely (index == NOT_COVERED)) return_trace (false);\n\n    const LookupRecord *lookupRecord = &StructAfter<LookupRecord> (coverageZ.as_array (glyphCount));\n    struct ContextApplyLookupContext lookup_context = {\n      {match_coverage},\n      this\n    };\n    return_trace (context_apply_lookup (c, glyphCount, (const HBUINT16 *) (coverageZ.arrayZ + 1), lookupCount, lookupRecord, lookup_context));\n  }\n\n  bool subset (hb_subset_context_t *c) const\n  {\n    TRACE_SUBSET (this);\n    auto *out = c->serializer->start_embed (this);\n    if (unlikely (!c->serializer->extend_min (out))) return_trace (false);\n\n    out->format = format;\n    out->glyphCount = glyphCount;\n\n    auto coverages = coverageZ.as_array (glyphCount);\n\n    for (const Offset16To<Coverage>& offset : coverages)\n    {\n      /* TODO(subset) This looks like should not be necessary to write this way. */\n      auto *o = c->serializer->allocate_size<Offset16To<Coverage>> (Offset16To<Coverage>::static_size);\n      if (unlikely (!o)) return_trace (false);\n      if (!o->serialize_subset (c, offset, this)) return_trace (false);\n    }\n\n    const auto& lookupRecord = StructAfter<UnsizedArrayOf<LookupRecord>> (coverageZ.as_array (glyphCount));\n    const hb_map_t *lookup_map = c->table_tag == HB_OT_TAG_GSUB ? &c->plan->gsub_lookups : &c->plan->gpos_lookups;\n\n\n    unsigned count = serialize_lookuprecord_array (c->serializer, lookupRecord.as_array (lookupCount), lookup_map);\n    return_trace (c->serializer->check_assign (out->lookupCount, count, HB_SERIALIZE_ERROR_INT_OVERFLOW));\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    if (!c->check_struct (this)) return_trace (false);\n    unsigned int count = glyphCount;\n    if (!count) return_trace (false); /* We want to access coverageZ[0] freely. */\n    if (!c->check_array (coverageZ.arrayZ, count)) return_trace (false);\n    for (unsigned int i = 0; i < count; i++)\n      if (!coverageZ[i].sanitize (c, this)) return_trace (false);\n    const LookupRecord *lookupRecord = &StructAfter<LookupRecord> (coverageZ.as_array (glyphCount));\n    return_trace (c->check_array (lookupRecord, lookupCount));\n  }\n\n  protected:\n  HBUINT16\tformat;\t\t\t/* Format identifier--format = 3 */\n  HBUINT16\tglyphCount;\t\t/* Number of glyphs in the input glyph\n\t\t\t\t\t * sequence */\n  HBUINT16\tlookupCount;\t\t/* Number of LookupRecords */\n  UnsizedArrayOf<Offset16To<Coverage>>\n\t\tcoverageZ;\t\t/* Array of offsets to Coverage\n\t\t\t\t\t * table in glyph sequence order */\n/*UnsizedArrayOf<LookupRecord>\n\t\tlookupRecordX;*/\t/* Array of LookupRecords--in\n\t\t\t\t\t * design order */\n  public:\n  DEFINE_SIZE_ARRAY (6, coverageZ);\n};\n\nstruct Context\n{\n  template <typename context_t, typename ...Ts>\n  typename context_t::return_t dispatch (context_t *c, Ts&&... ds) const\n  {\n    if (unlikely (!c->may_dispatch (this, &u.format))) return c->no_dispatch_return_value ();\n    TRACE_DISPATCH (this, u.format);\n    switch (u.format) {\n    case 1: return_trace (c->dispatch (u.format1, std::forward<Ts> (ds)...));\n    case 2: return_trace (c->dispatch (u.format2, std::forward<Ts> (ds)...));\n    case 3: return_trace (c->dispatch (u.format3, std::forward<Ts> (ds)...));\n#ifndef HB_NO_BEYOND_64K\n    case 4: return_trace (c->dispatch (u.format4, std::forward<Ts> (ds)...));\n    case 5: return_trace (c->dispatch (u.format5, std::forward<Ts> (ds)...));\n#endif\n    default:return_trace (c->default_return_value ());\n    }\n  }\n\n  protected:\n  union {\n  HBUINT16\t\t\tformat;\t\t/* Format identifier */\n  ContextFormat1_4<SmallTypes>\tformat1;\n  ContextFormat2_5<SmallTypes>\tformat2;\n  ContextFormat3\t\tformat3;\n#ifndef HB_NO_BEYOND_64K\n  ContextFormat1_4<MediumTypes>\tformat4;\n  ContextFormat2_5<MediumTypes>\tformat5;\n#endif\n  } u;\n};\n\n\n/* Chaining Contextual lookups */\n\nstruct ChainContextClosureLookupContext\n{\n  ContextClosureFuncs funcs;\n  ContextFormat context_format;\n  const void *intersects_data[3];\n  void *intersects_cache[3];\n  void *intersected_glyphs_cache;\n};\n\nstruct ChainContextCollectGlyphsLookupContext\n{\n  ContextCollectGlyphsFuncs funcs;\n  const void *collect_data[3];\n};\n\nstruct ChainContextApplyLookupContext\n{\n  ChainContextApplyFuncs funcs;\n  const void *match_data[3];\n};\n\ntemplate <typename HBUINT>\nstatic inline bool chain_context_intersects (const hb_set_t *glyphs,\n\t\t\t\t\t     unsigned int backtrackCount,\n\t\t\t\t\t     const HBUINT backtrack[],\n\t\t\t\t\t     unsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t\t     const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t\t     unsigned int lookaheadCount,\n\t\t\t\t\t     const HBUINT lookahead[],\n\t\t\t\t\t     ChainContextClosureLookupContext &lookup_context)\n{\n  return array_is_subset_of (glyphs,\n\t\t\t     backtrackCount, backtrack,\n\t\t\t     lookup_context.funcs.intersects,\n\t\t\t     lookup_context.intersects_data[0],\n\t\t\t     lookup_context.intersects_cache[0])\n      && array_is_subset_of (glyphs,\n\t\t\t     inputCount ? inputCount - 1 : 0, input,\n\t\t\t     lookup_context.funcs.intersects,\n\t\t\t     lookup_context.intersects_data[1],\n\t\t\t     lookup_context.intersects_cache[1])\n      && array_is_subset_of (glyphs,\n\t\t\t     lookaheadCount, lookahead,\n\t\t\t     lookup_context.funcs.intersects,\n\t\t\t     lookup_context.intersects_data[2],\n\t\t\t     lookup_context.intersects_cache[2]);\n}\n\ntemplate <typename HBUINT>\nstatic inline void chain_context_closure_lookup (hb_closure_context_t *c,\n\t\t\t\t\t\t unsigned int backtrackCount,\n\t\t\t\t\t\t const HBUINT backtrack[],\n\t\t\t\t\t\t unsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t\t\t const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t\t\t unsigned int lookaheadCount,\n\t\t\t\t\t\t const HBUINT lookahead[],\n\t\t\t\t\t\t unsigned int lookupCount,\n\t\t\t\t\t\t const LookupRecord lookupRecord[],\n\t\t\t\t\t\t unsigned value,\n\t\t\t\t\t\t ChainContextClosureLookupContext &lookup_context)\n{\n  if (chain_context_intersects (c->glyphs,\n\t\t\t\tbacktrackCount, backtrack,\n\t\t\t\tinputCount, input,\n\t\t\t\tlookaheadCount, lookahead,\n\t\t\t\tlookup_context))\n    context_closure_recurse_lookups (c,\n\t\t     inputCount, input,\n\t\t     lookupCount, lookupRecord,\n\t\t     value,\n\t\t     lookup_context.context_format,\n\t\t     lookup_context.intersects_data[1],\n\t\t     lookup_context.funcs.intersected_glyphs,\n\t\t     lookup_context.intersected_glyphs_cache);\n}\n\ntemplate <typename HBUINT>\nstatic inline void chain_context_collect_glyphs_lookup (hb_collect_glyphs_context_t *c,\n\t\t\t\t\t\t\tunsigned int backtrackCount,\n\t\t\t\t\t\t\tconst HBUINT backtrack[],\n\t\t\t\t\t\t\tunsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t\t\t\tconst HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t\t\t\tunsigned int lookaheadCount,\n\t\t\t\t\t\t\tconst HBUINT lookahead[],\n\t\t\t\t\t\t\tunsigned int lookupCount,\n\t\t\t\t\t\t\tconst LookupRecord lookupRecord[],\n\t\t\t\t\t\t\tChainContextCollectGlyphsLookupContext &lookup_context)\n{\n  collect_array (c, c->before,\n\t\t backtrackCount, backtrack,\n\t\t lookup_context.funcs.collect, lookup_context.collect_data[0]);\n  collect_array (c, c->input,\n\t\t inputCount ? inputCount - 1 : 0, input,\n\t\t lookup_context.funcs.collect, lookup_context.collect_data[1]);\n  collect_array (c, c->after,\n\t\t lookaheadCount, lookahead,\n\t\t lookup_context.funcs.collect, lookup_context.collect_data[2]);\n  recurse_lookups (c,\n\t\t   lookupCount, lookupRecord);\n}\n\ntemplate <typename HBUINT>\nstatic inline bool chain_context_would_apply_lookup (hb_would_apply_context_t *c,\n\t\t\t\t\t\t     unsigned int backtrackCount,\n\t\t\t\t\t\t     const HBUINT backtrack[] HB_UNUSED,\n\t\t\t\t\t\t     unsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t\t\t     const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t\t\t     unsigned int lookaheadCount,\n\t\t\t\t\t\t     const HBUINT lookahead[] HB_UNUSED,\n\t\t\t\t\t\t     unsigned int lookupCount HB_UNUSED,\n\t\t\t\t\t\t     const LookupRecord lookupRecord[] HB_UNUSED,\n\t\t\t\t\t\t     const ChainContextApplyLookupContext &lookup_context)\n{\n  return (c->zero_context ? !backtrackCount && !lookaheadCount : true)\n      && would_match_input (c,\n\t\t\t    inputCount, input,\n\t\t\t    lookup_context.funcs.match[1], lookup_context.match_data[1]);\n}\n\ntemplate <typename HBUINT>\nstatic inline bool chain_context_apply_lookup (hb_ot_apply_context_t *c,\n\t\t\t\t\t       unsigned int backtrackCount,\n\t\t\t\t\t       const HBUINT backtrack[],\n\t\t\t\t\t       unsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t\t       const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t\t       unsigned int lookaheadCount,\n\t\t\t\t\t       const HBUINT lookahead[],\n\t\t\t\t\t       unsigned int lookupCount,\n\t\t\t\t\t       const LookupRecord lookupRecord[],\n\t\t\t\t\t       const ChainContextApplyLookupContext &lookup_context)\n{\n  unsigned end_index = c->buffer->idx;\n  unsigned match_end = 0;\n  unsigned match_positions[HB_MAX_CONTEXT_LENGTH];\n  if (!(match_input (c,\n\t\t     inputCount, input,\n\t\t     lookup_context.funcs.match[1], lookup_context.match_data[1],\n\t\t     &match_end, match_positions) && (end_index = match_end)\n       && match_lookahead (c,\n\t\t\t   lookaheadCount, lookahead,\n\t\t\t   lookup_context.funcs.match[2], lookup_context.match_data[2],\n\t\t\t   match_end, &end_index)))\n  {\n    c->buffer->unsafe_to_concat (c->buffer->idx, end_index);\n    return false;\n  }\n\n  unsigned start_index = c->buffer->out_len;\n  if (!match_backtrack (c,\n\t\t\tbacktrackCount, backtrack,\n\t\t\tlookup_context.funcs.match[0], lookup_context.match_data[0],\n\t\t\t&start_index))\n  {\n    c->buffer->unsafe_to_concat_from_outbuffer (start_index, end_index);\n    return false;\n  }\n\n  c->buffer->unsafe_to_break_from_outbuffer (start_index, end_index);\n  apply_lookup (c,\n\t\tinputCount, match_positions,\n\t\tlookupCount, lookupRecord,\n\t\tmatch_end);\n  return true;\n}\n\ntemplate <typename Types>\nstruct ChainRule\n{\n  bool intersects (const hb_set_t *glyphs, ChainContextClosureLookupContext &lookup_context) const\n  {\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    return chain_context_intersects (glyphs,\n\t\t\t\t     backtrack.len, backtrack.arrayZ,\n\t\t\t\t     input.lenP1, input.arrayZ,\n\t\t\t\t     lookahead.len, lookahead.arrayZ,\n\t\t\t\t     lookup_context);\n  }\n\n  void closure (hb_closure_context_t *c, unsigned value,\n\t\tChainContextClosureLookupContext &lookup_context) const\n  {\n    if (unlikely (c->lookup_limit_exceeded ())) return;\n\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    chain_context_closure_lookup (c,\n\t\t\t\t  backtrack.len, backtrack.arrayZ,\n\t\t\t\t  input.lenP1, input.arrayZ,\n\t\t\t\t  lookahead.len, lookahead.arrayZ,\n\t\t\t\t  lookup.len, lookup.arrayZ,\n\t\t\t\t  value,\n\t\t\t\t  lookup_context);\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c,\n                        ChainContextClosureLookupContext &lookup_context) const\n  {\n    if (unlikely (c->lookup_limit_exceeded ())) return;\n    if (!intersects (c->glyphs, lookup_context)) return;\n\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    recurse_lookups (c, lookup.len, lookup.arrayZ);\n  }\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c,\n\t\t       ChainContextCollectGlyphsLookupContext &lookup_context) const\n  {\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    chain_context_collect_glyphs_lookup (c,\n\t\t\t\t\t backtrack.len, backtrack.arrayZ,\n\t\t\t\t\t input.lenP1, input.arrayZ,\n\t\t\t\t\t lookahead.len, lookahead.arrayZ,\n\t\t\t\t\t lookup.len, lookup.arrayZ,\n\t\t\t\t\t lookup_context);\n  }\n\n  bool would_apply (hb_would_apply_context_t *c,\n\t\t    const ChainContextApplyLookupContext &lookup_context) const\n  {\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    return chain_context_would_apply_lookup (c,\n\t\t\t\t\t     backtrack.len, backtrack.arrayZ,\n\t\t\t\t\t     input.lenP1, input.arrayZ,\n\t\t\t\t\t     lookahead.len, lookahead.arrayZ, lookup.len,\n\t\t\t\t\t     lookup.arrayZ, lookup_context);\n  }\n\n  bool apply (hb_ot_apply_context_t *c,\n\t      const ChainContextApplyLookupContext &lookup_context) const\n  {\n    TRACE_APPLY (this);\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    return_trace (chain_context_apply_lookup (c,\n\t\t\t\t\t      backtrack.len, backtrack.arrayZ,\n\t\t\t\t\t      input.lenP1, input.arrayZ,\n\t\t\t\t\t      lookahead.len, lookahead.arrayZ, lookup.len,\n\t\t\t\t\t      lookup.arrayZ, lookup_context));\n  }\n\n  template<typename Iterator,\n\t   hb_requires (hb_is_iterator (Iterator))>\n  void serialize_array (hb_serialize_context_t *c,\n\t\t\tHBUINT16 len,\n\t\t\tIterator it) const\n  {\n    c->copy (len);\n    for (const auto g : it)\n      c->copy ((HBUINT16) g);\n  }\n\n  bool serialize (hb_serialize_context_t *c,\n\t\t  const hb_map_t *lookup_map,\n\t\t  const hb_map_t *backtrack_map,\n\t\t  const hb_map_t *input_map = nullptr,\n\t\t  const hb_map_t *lookahead_map = nullptr) const\n  {\n    TRACE_SERIALIZE (this);\n    auto *out = c->start_embed (this);\n    if (unlikely (!out)) return_trace (false);\n\n    const hb_map_t *mapping = backtrack_map;\n    serialize_array (c, backtrack.len, + backtrack.iter ()\n\t\t\t\t       | hb_map (mapping));\n\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    if (input_map) mapping = input_map;\n    serialize_array (c, input.lenP1, + input.iter ()\n\t\t\t\t     | hb_map (mapping));\n\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    if (lookahead_map) mapping = lookahead_map;\n    serialize_array (c, lookahead.len, + lookahead.iter ()\n\t\t\t\t       | hb_map (mapping));\n\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n\n    HBUINT16* lookupCount = c->embed (&(lookup.len));\n    if (!lookupCount) return_trace (false);\n\n    unsigned count = serialize_lookuprecord_array (c, lookup.as_array (), lookup_map);\n    return_trace (c->check_assign (*lookupCount, count, HB_SERIALIZE_ERROR_INT_OVERFLOW));\n  }\n\n  bool subset (hb_subset_context_t *c,\n\t       const hb_map_t *lookup_map,\n\t       const hb_map_t *backtrack_map = nullptr,\n\t       const hb_map_t *input_map = nullptr,\n\t       const hb_map_t *lookahead_map = nullptr) const\n  {\n    TRACE_SUBSET (this);\n\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n\n    if (!backtrack_map)\n    {\n      const hb_set_t &glyphset = *c->plan->glyphset_gsub ();\n      if (!hb_all (backtrack, glyphset) ||\n\t  !hb_all (input, glyphset) ||\n\t  !hb_all (lookahead, glyphset))\n\treturn_trace (false);\n\n      serialize (c->serializer, lookup_map, c->plan->glyph_map);\n    }\n    else\n    {\n      if (!hb_all (backtrack, backtrack_map) ||\n\t  !hb_all (input, input_map) ||\n\t  !hb_all (lookahead, lookahead_map))\n\treturn_trace (false);\n\n      serialize (c->serializer, lookup_map, backtrack_map, input_map, lookahead_map);\n    }\n\n    return_trace (true);\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    if (!backtrack.sanitize (c)) return_trace (false);\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    if (!input.sanitize (c)) return_trace (false);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    if (!lookahead.sanitize (c)) return_trace (false);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    return_trace (lookup.sanitize (c));\n  }\n\n  protected:\n  Array16Of<typename Types::HBUINT>\n\t\tbacktrack;\t\t/* Array of backtracking values\n\t\t\t\t\t * (to be matched before the input\n\t\t\t\t\t * sequence) */\n  HeadlessArrayOf<typename Types::HBUINT>\n\t\tinputX;\t\t\t/* Array of input values (start with\n\t\t\t\t\t * second glyph) */\n  Array16Of<typename Types::HBUINT>\n\t\tlookaheadX;\t\t/* Array of lookahead values's (to be\n\t\t\t\t\t * matched after the input sequence) */\n  Array16Of<LookupRecord>\n\t\tlookupX;\t\t/* Array of LookupRecords--in\n\t\t\t\t\t * design order) */\n  public:\n  DEFINE_SIZE_MIN (8);\n};\n\ntemplate <typename Types>\nstruct ChainRuleSet\n{\n  using ChainRule = OT::ChainRule<Types>;\n\n  bool intersects (const hb_set_t *glyphs, ChainContextClosureLookupContext &lookup_context) const\n  {\n    return\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_map ([&] (const ChainRule &_) { return _.intersects (glyphs, lookup_context); })\n    | hb_any\n    ;\n  }\n  void closure (hb_closure_context_t *c, unsigned value, ChainContextClosureLookupContext &lookup_context) const\n  {\n    if (unlikely (c->lookup_limit_exceeded ())) return;\n\n    return\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const ChainRule &_) { _.closure (c, value, lookup_context); })\n    ;\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c,\n                        ChainContextClosureLookupContext &lookup_context) const\n  {\n    if (unlikely (c->lookup_limit_exceeded ())) return;\n\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const ChainRule &_) { _.closure_lookups (c, lookup_context); })\n    ;\n  }\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c, ChainContextCollectGlyphsLookupContext &lookup_context) const\n  {\n    return\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const ChainRule &_) { _.collect_glyphs (c, lookup_context); })\n    ;\n  }\n\n  bool would_apply (hb_would_apply_context_t *c,\n\t\t    const ChainContextApplyLookupContext &lookup_context) const\n  {\n    return\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_map ([&] (const ChainRule &_) { return _.would_apply (c, lookup_context); })\n    | hb_any\n    ;\n  }\n\n  bool apply (hb_ot_apply_context_t *c,\n\t      const ChainContextApplyLookupContext &lookup_context) const\n  {\n    TRACE_APPLY (this);\n    return_trace (\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_map ([&] (const ChainRule &_) { return _.apply (c, lookup_context); })\n    | hb_any\n    )\n    ;\n  }\n\n  bool subset (hb_subset_context_t *c,\n\t       const hb_map_t *lookup_map,\n\t       const hb_map_t *backtrack_klass_map = nullptr,\n\t       const hb_map_t *input_klass_map = nullptr,\n\t       const hb_map_t *lookahead_klass_map = nullptr) const\n  {\n    TRACE_SUBSET (this);\n\n    auto snap = c->serializer->snapshot ();\n    auto *out = c->serializer->start_embed (*this);\n    if (unlikely (!c->serializer->extend_min (out))) return_trace (false);\n\n    for (const Offset16To<ChainRule>& _ : rule)\n    {\n      if (!_) continue;\n      auto o_snap = c->serializer->snapshot ();\n      auto *o = out->rule.serialize_append (c->serializer);\n      if (unlikely (!o)) continue;\n\n      if (!o->serialize_subset (c, _, this,\n\t\t\t\tlookup_map,\n\t\t\t\tbacktrack_klass_map,\n\t\t\t\tinput_klass_map,\n\t\t\t\tlookahead_klass_map))\n      {\n\tout->rule.pop ();\n\tc->serializer->revert (o_snap);\n      }\n    }\n\n    bool ret = bool (out->rule);\n    if (!ret) c->serializer->revert (snap);\n\n    return_trace (ret);\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    return_trace (rule.sanitize (c, this));\n  }\n\n  protected:\n  Array16OfOffset16To<ChainRule>\n\t\trule;\t\t\t/* Array of ChainRule tables\n\t\t\t\t\t * ordered by preference */\n  public:\n  DEFINE_SIZE_ARRAY (2, rule);\n};\n\ntemplate <typename Types>\nstruct ChainContextFormat1_4\n{\n  using ChainRuleSet = OT::ChainRuleSet<Types>;\n\n  bool intersects (const hb_set_t *glyphs) const\n  {\n    struct ChainContextClosureLookupContext lookup_context = {\n      {intersects_glyph, intersected_glyph},\n      ContextFormat::SimpleContext,\n      {nullptr, nullptr, nullptr}\n    };\n\n    return\n    + hb_zip (this+coverage, ruleSet)\n    | hb_filter (*glyphs, hb_first)\n    | hb_map (hb_second)\n    | hb_map (hb_add (this))\n    | hb_map ([&] (const ChainRuleSet &_) { return _.intersects (glyphs, lookup_context); })\n    | hb_any\n    ;\n  }\n\n  bool may_have_non_1to1 () const\n  { return true; }\n\n  void closure (hb_closure_context_t *c) const\n  {\n    hb_set_t& cur_active_glyphs = c->push_cur_active_glyphs ();\n    get_coverage ().intersect_set (c->previous_parent_active_glyphs (),\n                                                 cur_active_glyphs);\n\n    struct ChainContextClosureLookupContext lookup_context = {\n      {intersects_glyph, intersected_glyph},\n      ContextFormat::SimpleContext,\n      {nullptr, nullptr, nullptr}\n    };\n\n    + hb_zip (this+coverage, hb_range ((unsigned) ruleSet.len))\n    | hb_filter ([&] (hb_codepoint_t _) {\n      return c->previous_parent_active_glyphs ().has (_);\n    }, hb_first)\n    | hb_map ([&](const hb_pair_t<hb_codepoint_t, unsigned> _) { return hb_pair_t<unsigned, const ChainRuleSet&> (_.first, this+ruleSet[_.second]); })\n    | hb_apply ([&] (const hb_pair_t<unsigned, const ChainRuleSet&>& _) { _.second.closure (c, _.first, lookup_context); })\n    ;\n\n    c->pop_cur_done_glyphs ();\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c) const\n  {\n    struct ChainContextClosureLookupContext lookup_context = {\n      {intersects_glyph, nullptr},\n      ContextFormat::SimpleContext,\n      {nullptr, nullptr, nullptr}\n    };\n\n    + hb_zip (this+coverage, ruleSet)\n    | hb_filter (*c->glyphs, hb_first)\n    | hb_map (hb_second)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const ChainRuleSet &_) { _.closure_lookups (c, lookup_context); })\n    ;\n  }\n\n  void collect_variation_indices (hb_collect_variation_indices_context_t *c) const {}\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c) const\n  {\n    (this+coverage).collect_coverage (c->input);\n\n    struct ChainContextCollectGlyphsLookupContext lookup_context = {\n      {collect_glyph},\n      {nullptr, nullptr, nullptr}\n    };\n\n    + hb_iter (ruleSet)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const ChainRuleSet &_) { _.collect_glyphs (c, lookup_context); })\n    ;\n  }\n\n  bool would_apply (hb_would_apply_context_t *c) const\n  {\n    const ChainRuleSet &rule_set = this+ruleSet[(this+coverage).get_coverage (c->glyphs[0])];\n    struct ChainContextApplyLookupContext lookup_context = {\n      {{match_glyph, match_glyph, match_glyph}},\n      {nullptr, nullptr, nullptr}\n    };\n    return rule_set.would_apply (c, lookup_context);\n  }\n\n  const Coverage &get_coverage () const { return this+coverage; }\n\n  bool apply (hb_ot_apply_context_t *c) const\n  {\n    TRACE_APPLY (this);\n    unsigned int index = (this+coverage).get_coverage (c->buffer->cur().codepoint);\n    if (likely (index == NOT_COVERED)) return_trace (false);\n\n    const ChainRuleSet &rule_set = this+ruleSet[index];\n    struct ChainContextApplyLookupContext lookup_context = {\n      {{match_glyph, match_glyph, match_glyph}},\n      {nullptr, nullptr, nullptr}\n    };\n    return_trace (rule_set.apply (c, lookup_context));\n  }\n\n  bool subset (hb_subset_context_t *c) const\n  {\n    TRACE_SUBSET (this);\n    const hb_set_t &glyphset = *c->plan->glyphset_gsub ();\n    const hb_map_t &glyph_map = *c->plan->glyph_map;\n\n    auto *out = c->serializer->start_embed (*this);\n    if (unlikely (!c->serializer->extend_min (out))) return_trace (false);\n    out->format = format;\n\n    const hb_map_t *lookup_map = c->table_tag == HB_OT_TAG_GSUB ? &c->plan->gsub_lookups : &c->plan->gpos_lookups;\n    hb_sorted_vector_t<hb_codepoint_t> new_coverage;\n    + hb_zip (this+coverage, ruleSet)\n    | hb_filter (glyphset, hb_first)\n    | hb_filter (subset_offset_array (c, out->ruleSet, this, lookup_map), hb_second)\n    | hb_map (hb_first)\n    | hb_map (glyph_map)\n    | hb_sink (new_coverage)\n    ;\n\n    out->coverage.serialize_serialize (c->serializer, new_coverage.iter ());\n    return_trace (bool (new_coverage));\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    return_trace (coverage.sanitize (c, this) && ruleSet.sanitize (c, this));\n  }\n\n  protected:\n  HBUINT16\tformat;\t\t\t/* Format identifier--format = 1 */\n  typename Types::template OffsetTo<Coverage>\n\t\tcoverage;\t\t/* Offset to Coverage table--from\n\t\t\t\t\t * beginning of table */\n  Array16Of<typename Types::template OffsetTo<ChainRuleSet>>\n\t\truleSet;\t\t/* Array of ChainRuleSet tables\n\t\t\t\t\t * ordered by Coverage Index */\n  public:\n  DEFINE_SIZE_ARRAY (2 + 2 * Types::size, ruleSet);\n};\n\ntemplate <typename Types>\nstruct ChainContextFormat2_5\n{\n  using ChainRuleSet = OT::ChainRuleSet<SmallTypes>;\n\n  bool intersects (const hb_set_t *glyphs) const\n  {\n    if (!(this+coverage).intersects (glyphs))\n      return false;\n\n    const ClassDef &backtrack_class_def = this+backtrackClassDef;\n    const ClassDef &input_class_def = this+inputClassDef;\n    const ClassDef &lookahead_class_def = this+lookaheadClassDef;\n\n    hb_map_t caches[3] = {};\n    struct ChainContextClosureLookupContext lookup_context = {\n      {intersects_class, nullptr},\n      ContextFormat::ClassBasedContext,\n      {&backtrack_class_def,\n       &input_class_def,\n       &lookahead_class_def},\n      {&caches[0], &caches[1], &caches[2]}\n    };\n\n    hb_set_t retained_coverage_glyphs;\n    (this+coverage).intersect_set (*glyphs, retained_coverage_glyphs);\n\n    hb_set_t coverage_glyph_classes;\n    input_class_def.intersected_classes (&retained_coverage_glyphs, &coverage_glyph_classes);\n\n    return\n    + hb_iter (ruleSet)\n    | hb_map (hb_add (this))\n    | hb_enumerate\n    | hb_map ([&] (const hb_pair_t<unsigned, const ChainRuleSet &> p)\n\t      { return input_class_def.intersects_class (glyphs, p.first) &&\n\t\t       coverage_glyph_classes.has (p.first) &&\n\t\t       p.second.intersects (glyphs, lookup_context); })\n    | hb_any\n    ;\n  }\n\n  bool may_have_non_1to1 () const\n  { return true; }\n\n  void closure (hb_closure_context_t *c) const\n  {\n    if (!(this+coverage).intersects (c->glyphs))\n      return;\n\n    hb_set_t& cur_active_glyphs = c->push_cur_active_glyphs ();\n    get_coverage ().intersect_set (c->previous_parent_active_glyphs (),\n                                                 cur_active_glyphs);\n\n\n    const ClassDef &backtrack_class_def = this+backtrackClassDef;\n    const ClassDef &input_class_def = this+inputClassDef;\n    const ClassDef &lookahead_class_def = this+lookaheadClassDef;\n\n    hb_map_t caches[3] = {};\n    intersected_class_cache_t intersected_cache;\n    struct ChainContextClosureLookupContext lookup_context = {\n      {intersects_class, intersected_class_glyphs},\n      ContextFormat::ClassBasedContext,\n      {&backtrack_class_def,\n       &input_class_def,\n       &lookahead_class_def},\n      {&caches[0], &caches[1], &caches[2]},\n      &intersected_cache\n    };\n\n    + hb_enumerate (ruleSet)\n    | hb_filter ([&] (unsigned _)\n    { return input_class_def.intersects_class (&c->parent_active_glyphs (), _); },\n\t\t hb_first)\n    | hb_apply ([&] (const hb_pair_t<unsigned, const typename Types::template OffsetTo<ChainRuleSet>&> _)\n                {\n                  const ChainRuleSet& chainrule_set = this+_.second;\n                  chainrule_set.closure (c, _.first, lookup_context);\n                })\n    ;\n\n    c->pop_cur_done_glyphs ();\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c) const\n  {\n    if (!(this+coverage).intersects (c->glyphs))\n      return;\n\n    const ClassDef &backtrack_class_def = this+backtrackClassDef;\n    const ClassDef &input_class_def = this+inputClassDef;\n    const ClassDef &lookahead_class_def = this+lookaheadClassDef;\n\n    hb_map_t caches[3] = {};\n    struct ChainContextClosureLookupContext lookup_context = {\n      {intersects_class, nullptr},\n      ContextFormat::ClassBasedContext,\n      {&backtrack_class_def,\n       &input_class_def,\n       &lookahead_class_def},\n      {&caches[0], &caches[1], &caches[2]}\n    };\n\n    + hb_iter (ruleSet)\n    | hb_map (hb_add (this))\n    | hb_enumerate\n    | hb_filter([&] (unsigned klass)\n    { return input_class_def.intersects_class (c->glyphs, klass); }, hb_first)\n    | hb_map (hb_second)\n    | hb_apply ([&] (const ChainRuleSet &_)\n    { _.closure_lookups (c, lookup_context); })\n    ;\n  }\n\n  void collect_variation_indices (hb_collect_variation_indices_context_t *c) const {}\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c) const\n  {\n    (this+coverage).collect_coverage (c->input);\n\n    const ClassDef &backtrack_class_def = this+backtrackClassDef;\n    const ClassDef &input_class_def = this+inputClassDef;\n    const ClassDef &lookahead_class_def = this+lookaheadClassDef;\n\n    struct ChainContextCollectGlyphsLookupContext lookup_context = {\n      {collect_class},\n      {&backtrack_class_def,\n       &input_class_def,\n       &lookahead_class_def}\n    };\n\n    + hb_iter (ruleSet)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const ChainRuleSet &_) { _.collect_glyphs (c, lookup_context); })\n    ;\n  }\n\n  bool would_apply (hb_would_apply_context_t *c) const\n  {\n    const ClassDef &backtrack_class_def = this+backtrackClassDef;\n    const ClassDef &input_class_def = this+inputClassDef;\n    const ClassDef &lookahead_class_def = this+lookaheadClassDef;\n\n    unsigned int index = input_class_def.get_class (c->glyphs[0]);\n    const ChainRuleSet &rule_set = this+ruleSet[index];\n    struct ChainContextApplyLookupContext lookup_context = {\n      {{match_class, match_class, match_class}},\n      {&backtrack_class_def,\n       &input_class_def,\n       &lookahead_class_def}\n    };\n    return rule_set.would_apply (c, lookup_context);\n  }\n\n  const Coverage &get_coverage () const { return this+coverage; }\n\n  unsigned cache_cost () const\n  {\n    unsigned c = (this+lookaheadClassDef).cost () * ruleSet.len;\n    return c >= 4 ? c : 0;\n  }\n  bool cache_func (hb_ot_apply_context_t *c, bool enter) const\n  {\n    if (enter)\n    {\n      if (!HB_BUFFER_TRY_ALLOCATE_VAR (c->buffer, syllable))\n\treturn false;\n      auto &info = c->buffer->info;\n      unsigned count = c->buffer->len;\n      for (unsigned i = 0; i < count; i++)\n\tinfo[i].syllable() = 255;\n      c->new_syllables = 255;\n      return true;\n    }\n    else\n    {\n      c->new_syllables = (unsigned) -1;\n      HB_BUFFER_DEALLOCATE_VAR (c->buffer, syllable);\n      return true;\n    }\n  }\n\n  bool apply (hb_ot_apply_context_t *c, bool cached = false) const\n  {\n    TRACE_APPLY (this);\n    unsigned int index = (this+coverage).get_coverage (c->buffer->cur().codepoint);\n    if (likely (index == NOT_COVERED)) return_trace (false);\n\n    const ClassDef &backtrack_class_def = this+backtrackClassDef;\n    const ClassDef &input_class_def = this+inputClassDef;\n    const ClassDef &lookahead_class_def = this+lookaheadClassDef;\n\n    /* For ChainContextFormat2_5 we cache the LookaheadClassDef instead of InputClassDef.\n     * The reason is that most heavy fonts want to identify a glyph in context and apply\n     * a lookup to it. In this scenario, the length of the input sequence is one, whereas\n     * the lookahead / backtrack are typically longer.  The one glyph in input sequence is\n     * looked-up below and no input glyph is looked up in individual rules, whereas the\n     * lookahead and backtrack glyphs are tried.  Since we match lookahead before backtrack,\n     * we should cache lookahead.  This decisions showed a 20% improvement in shaping of\n     * the Gulzar font.\n     */\n\n    struct ChainContextApplyLookupContext lookup_context = {\n      {{cached && &backtrack_class_def == &lookahead_class_def ? match_class_cached : match_class,\n        cached && &input_class_def == &lookahead_class_def ? match_class_cached : match_class,\n        cached ? match_class_cached : match_class}},\n      {&backtrack_class_def,\n       &input_class_def,\n       &lookahead_class_def}\n    };\n\n    index = input_class_def.get_class (c->buffer->cur().codepoint);\n    const ChainRuleSet &rule_set = this+ruleSet[index];\n    return_trace (rule_set.apply (c, lookup_context));\n  }\n\n  bool subset (hb_subset_context_t *c) const\n  {\n    TRACE_SUBSET (this);\n    auto *out = c->serializer->start_embed (*this);\n    if (unlikely (!c->serializer->extend_min (out))) return_trace (false);\n    out->format = format;\n    out->coverage.serialize_subset (c, coverage, this);\n\n    hb_map_t backtrack_klass_map;\n    hb_map_t input_klass_map;\n    hb_map_t lookahead_klass_map;\n\n    out->backtrackClassDef.serialize_subset (c, backtrackClassDef, this, &backtrack_klass_map);\n    // TODO: subset inputClassDef based on glyphs survived in Coverage subsetting\n    out->inputClassDef.serialize_subset (c, inputClassDef, this, &input_klass_map);\n    out->lookaheadClassDef.serialize_subset (c, lookaheadClassDef, this, &lookahead_klass_map);\n\n    if (unlikely (!c->serializer->propagate_error (backtrack_klass_map,\n\t\t\t\t\t\t   input_klass_map,\n\t\t\t\t\t\t   lookahead_klass_map)))\n      return_trace (false);\n\n    const hb_set_t* glyphset = c->plan->glyphset_gsub ();\n    hb_set_t retained_coverage_glyphs;\n    (this+coverage).intersect_set (*glyphset, retained_coverage_glyphs);\n\n    hb_set_t coverage_glyph_classes;\n    (this+inputClassDef).intersected_classes (&retained_coverage_glyphs, &coverage_glyph_classes);\n\n    int non_zero_index = -1, index = 0;\n    bool ret = true;\n    const hb_map_t *lookup_map = c->table_tag == HB_OT_TAG_GSUB ? &c->plan->gsub_lookups : &c->plan->gpos_lookups;\n    auto last_non_zero = c->serializer->snapshot ();\n    for (const auto& _ : + hb_enumerate (ruleSet)\n\t\t\t | hb_filter (input_klass_map, hb_first))\n    {\n      auto *o = out->ruleSet.serialize_append (c->serializer);\n      if (unlikely (!o))\n      {\n\tret = false;\n\tbreak;\n      }\n      if (coverage_glyph_classes.has (_.first) &&\n          o->serialize_subset (c, _.second, this,\n\t\t\t       lookup_map,\n\t\t\t       &backtrack_klass_map,\n\t\t\t       &input_klass_map,\n\t\t\t       &lookahead_klass_map))\n      {\n        last_non_zero = c->serializer->snapshot ();\n\tnon_zero_index = index;\n      }\n\n      index++;\n    }\n\n    if (!ret || non_zero_index == -1) return_trace (false);\n\n    // prune empty trailing ruleSets\n    if (index > non_zero_index) {\n      c->serializer->revert (last_non_zero);\n      out->ruleSet.len = non_zero_index + 1;\n    }\n\n    return_trace (bool (out->ruleSet));\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    return_trace (coverage.sanitize (c, this) &&\n\t\t  backtrackClassDef.sanitize (c, this) &&\n\t\t  inputClassDef.sanitize (c, this) &&\n\t\t  lookaheadClassDef.sanitize (c, this) &&\n\t\t  ruleSet.sanitize (c, this));\n  }\n\n  protected:\n  HBUINT16\tformat;\t\t\t/* Format identifier--format = 2 */\n  typename Types::template OffsetTo<Coverage>\n\t\tcoverage;\t\t/* Offset to Coverage table--from\n\t\t\t\t\t * beginning of table */\n  typename Types::template OffsetTo<ClassDef>\n\t\tbacktrackClassDef;\t/* Offset to glyph ClassDef table\n\t\t\t\t\t * containing backtrack sequence\n\t\t\t\t\t * data--from beginning of table */\n  typename Types::template OffsetTo<ClassDef>\n\t\tinputClassDef;\t\t/* Offset to glyph ClassDef\n\t\t\t\t\t * table containing input sequence\n\t\t\t\t\t * data--from beginning of table */\n  typename Types::template OffsetTo<ClassDef>\n\t\tlookaheadClassDef;\t/* Offset to glyph ClassDef table\n\t\t\t\t\t * containing lookahead sequence\n\t\t\t\t\t * data--from beginning of table */\n  Array16Of<typename Types::template OffsetTo<ChainRuleSet>>\n\t\truleSet;\t\t/* Array of ChainRuleSet tables\n\t\t\t\t\t * ordered by class */\n  public:\n  DEFINE_SIZE_ARRAY (4 + 4 * Types::size, ruleSet);\n};\n\nstruct ChainContextFormat3\n{\n  using RuleSet = OT::RuleSet<SmallTypes>;\n\n  bool intersects (const hb_set_t *glyphs) const\n  {\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n\n    if (!(this+input[0]).intersects (glyphs))\n      return false;\n\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    struct ChainContextClosureLookupContext lookup_context = {\n      {intersects_coverage, nullptr},\n      ContextFormat::CoverageBasedContext,\n      {this, this, this}\n    };\n    return chain_context_intersects (glyphs,\n\t\t\t\t     backtrack.len, (const HBUINT16 *) backtrack.arrayZ,\n\t\t\t\t     input.len, (const HBUINT16 *) input.arrayZ + 1,\n\t\t\t\t     lookahead.len, (const HBUINT16 *) lookahead.arrayZ,\n\t\t\t\t     lookup_context);\n  }\n\n  bool may_have_non_1to1 () const\n  { return true; }\n\n  void closure (hb_closure_context_t *c) const\n  {\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n\n    if (!(this+input[0]).intersects (c->glyphs))\n      return;\n\n    hb_set_t& cur_active_glyphs = c->push_cur_active_glyphs ();\n    get_coverage ().intersect_set (c->previous_parent_active_glyphs (),\n                                                 cur_active_glyphs);\n\n\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    struct ChainContextClosureLookupContext lookup_context = {\n      {intersects_coverage, intersected_coverage_glyphs},\n      ContextFormat::CoverageBasedContext,\n      {this, this, this}\n    };\n    chain_context_closure_lookup (c,\n\t\t\t\t  backtrack.len, (const HBUINT16 *) backtrack.arrayZ,\n\t\t\t\t  input.len, (const HBUINT16 *) input.arrayZ + 1,\n\t\t\t\t  lookahead.len, (const HBUINT16 *) lookahead.arrayZ,\n\t\t\t\t  lookup.len, lookup.arrayZ,\n\t\t\t\t  0, lookup_context);\n\n    c->pop_cur_done_glyphs ();\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c) const\n  {\n    if (!intersects (c->glyphs))\n      return;\n\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    recurse_lookups (c, lookup.len, lookup.arrayZ);\n  }\n\n  void collect_variation_indices (hb_collect_variation_indices_context_t *c) const {}\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c) const\n  {\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n\n    (this+input[0]).collect_coverage (c->input);\n\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n\n    struct ChainContextCollectGlyphsLookupContext lookup_context = {\n      {collect_coverage},\n      {this, this, this}\n    };\n    chain_context_collect_glyphs_lookup (c,\n\t\t\t\t\t backtrack.len, (const HBUINT16 *) backtrack.arrayZ,\n\t\t\t\t\t input.len, (const HBUINT16 *) input.arrayZ + 1,\n\t\t\t\t\t lookahead.len, (const HBUINT16 *) lookahead.arrayZ,\n\t\t\t\t\t lookup.len, lookup.arrayZ,\n\t\t\t\t\t lookup_context);\n  }\n\n  bool would_apply (hb_would_apply_context_t *c) const\n  {\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    struct ChainContextApplyLookupContext lookup_context = {\n      {{match_coverage, match_coverage, match_coverage}},\n      {this, this, this}\n    };\n    return chain_context_would_apply_lookup (c,\n\t\t\t\t\t     backtrack.len, (const HBUINT16 *) backtrack.arrayZ,\n\t\t\t\t\t     input.len, (const HBUINT16 *) input.arrayZ + 1,\n\t\t\t\t\t     lookahead.len, (const HBUINT16 *) lookahead.arrayZ,\n\t\t\t\t\t     lookup.len, lookup.arrayZ, lookup_context);\n  }\n\n  const Coverage &get_coverage () const\n  {\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    return this+input[0];\n  }\n\n  bool apply (hb_ot_apply_context_t *c) const\n  {\n    TRACE_APPLY (this);\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n\n    unsigned int index = (this+input[0]).get_coverage (c->buffer->cur().codepoint);\n    if (likely (index == NOT_COVERED)) return_trace (false);\n\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    struct ChainContextApplyLookupContext lookup_context = {\n      {{match_coverage, match_coverage, match_coverage}},\n      {this, this, this}\n    };\n    return_trace (chain_context_apply_lookup (c,\n\t\t\t\t\t      backtrack.len, (const HBUINT16 *) backtrack.arrayZ,\n\t\t\t\t\t      input.len, (const HBUINT16 *) input.arrayZ + 1,\n\t\t\t\t\t      lookahead.len, (const HBUINT16 *) lookahead.arrayZ,\n\t\t\t\t\t      lookup.len, lookup.arrayZ, lookup_context));\n  }\n\n  template<typename Iterator,\n\t   hb_requires (hb_is_iterator (Iterator))>\n  bool serialize_coverage_offsets (hb_subset_context_t *c, Iterator it, const void* base) const\n  {\n    TRACE_SERIALIZE (this);\n    auto *out = c->serializer->start_embed<Array16OfOffset16To<Coverage>> ();\n\n    if (unlikely (!c->serializer->allocate_size<HBUINT16> (HBUINT16::static_size)))\n      return_trace (false);\n\n    for (auto& offset : it) {\n      auto *o = out->serialize_append (c->serializer);\n      if (unlikely (!o) || !o->serialize_subset (c, offset, base))\n        return_trace (false);\n    }\n\n    return_trace (true);\n  }\n\n  bool subset (hb_subset_context_t *c) const\n  {\n    TRACE_SUBSET (this);\n\n    auto *out = c->serializer->start_embed (this);\n    if (unlikely (!out)) return_trace (false);\n    if (unlikely (!c->serializer->embed (this->format))) return_trace (false);\n\n    if (!serialize_coverage_offsets (c, backtrack.iter (), this))\n      return_trace (false);\n\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    if (!serialize_coverage_offsets (c, input.iter (), this))\n      return_trace (false);\n\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    if (!serialize_coverage_offsets (c, lookahead.iter (), this))\n      return_trace (false);\n\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    const hb_map_t *lookup_map = c->table_tag == HB_OT_TAG_GSUB ? &c->plan->gsub_lookups : &c->plan->gpos_lookups;\n\n    HBUINT16 *lookupCount = c->serializer->copy<HBUINT16> (lookup.len);\n    if (!lookupCount) return_trace (false);\n\n    unsigned count = serialize_lookuprecord_array (c->serializer, lookup.as_array (), lookup_map);\n    return_trace (c->serializer->check_assign (*lookupCount, count, HB_SERIALIZE_ERROR_INT_OVERFLOW));\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    if (!backtrack.sanitize (c, this)) return_trace (false);\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    if (!input.sanitize (c, this)) return_trace (false);\n    if (!input.len) return_trace (false); /* To be consistent with Context. */\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    if (!lookahead.sanitize (c, this)) return_trace (false);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    return_trace (lookup.sanitize (c));\n  }\n\n  protected:\n  HBUINT16\tformat;\t\t\t/* Format identifier--format = 3 */\n  Array16OfOffset16To<Coverage>\n\t\tbacktrack;\t\t/* Array of coverage tables\n\t\t\t\t\t * in backtracking sequence, in  glyph\n\t\t\t\t\t * sequence order */\n  Array16OfOffset16To<Coverage>\n\t\tinputX\t\t;\t/* Array of coverage\n\t\t\t\t\t * tables in input sequence, in glyph\n\t\t\t\t\t * sequence order */\n  Array16OfOffset16To<Coverage>\n\t\tlookaheadX;\t\t/* Array of coverage tables\n\t\t\t\t\t * in lookahead sequence, in glyph\n\t\t\t\t\t * sequence order */\n  Array16Of<LookupRecord>\n\t\tlookupX;\t\t/* Array of LookupRecords--in\n\t\t\t\t\t * design order) */\n  public:\n  DEFINE_SIZE_MIN (10);\n};\n\nstruct ChainContext\n{\n  template <typename context_t, typename ...Ts>\n  typename context_t::return_t dispatch (context_t *c, Ts&&... ds) const\n  {\n    if (unlikely (!c->may_dispatch (this, &u.format))) return c->no_dispatch_return_value ();\n    TRACE_DISPATCH (this, u.format);\n    switch (u.format) {\n    case 1: return_trace (c->dispatch (u.format1, std::forward<Ts> (ds)...));\n    case 2: return_trace (c->dispatch (u.format2, std::forward<Ts> (ds)...));\n    case 3: return_trace (c->dispatch (u.format3, std::forward<Ts> (ds)...));\n#ifndef HB_NO_BEYOND_64K\n    case 4: return_trace (c->dispatch (u.format4, std::forward<Ts> (ds)...));\n    case 5: return_trace (c->dispatch (u.format5, std::forward<Ts> (ds)...));\n#endif\n    default:return_trace (c->default_return_value ());\n    }\n  }\n\n  protected:\n  union {\n  HBUINT16\t\t\t\tformat;\t/* Format identifier */\n  ChainContextFormat1_4<SmallTypes>\tformat1;\n  ChainContextFormat2_5<SmallTypes>\tformat2;\n  ChainContextFormat3\t\t\tformat3;\n#ifndef HB_NO_BEYOND_64K\n  ChainContextFormat1_4<MediumTypes>\tformat4;\n  ChainContextFormat2_5<MediumTypes>\tformat5;\n#endif\n  } u;\n};\n\n\ntemplate <typename T>\nstruct ExtensionFormat1\n{\n  unsigned int get_type () const { return extensionLookupType; }\n\n  template <typename X>\n  const X& get_subtable () const\n  { return this + reinterpret_cast<const Offset32To<typename T::SubTable> &> (extensionOffset); }\n\n  template <typename context_t, typename ...Ts>\n  typename context_t::return_t dispatch (context_t *c, Ts&&... ds) const\n  {\n    if (unlikely (!c->may_dispatch (this, this))) return c->no_dispatch_return_value ();\n    TRACE_DISPATCH (this, format);\n    return_trace (get_subtable<typename T::SubTable> ().dispatch (c, get_type (), std::forward<Ts> (ds)...));\n  }\n\n  void collect_variation_indices (hb_collect_variation_indices_context_t *c) const\n  { dispatch (c); }\n\n  /* This is called from may_dispatch() above with hb_sanitize_context_t. */\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    return_trace (c->check_struct (this) &&\n\t\t  extensionLookupType != T::SubTable::Extension);\n  }\n\n  bool subset (hb_subset_context_t *c) const\n  {\n    TRACE_SUBSET (this);\n\n    auto *out = c->serializer->start_embed (this);\n    if (unlikely (!out || !c->serializer->extend_min (out))) return_trace (false);\n\n    out->format = format;\n    out->extensionLookupType = extensionLookupType;\n\n    const auto& src_offset =\n        reinterpret_cast<const Offset32To<typename T::SubTable> &> (extensionOffset);\n    auto& dest_offset =\n        reinterpret_cast<Offset32To<typename T::SubTable> &> (out->extensionOffset);\n\n    return_trace (dest_offset.serialize_subset (c, src_offset, this, get_type ()));\n  }\n\n  protected:\n  HBUINT16\tformat;\t\t\t/* Format identifier. Set to 1. */\n  HBUINT16\textensionLookupType;\t/* Lookup type of subtable referenced\n\t\t\t\t\t * by ExtensionOffset (i.e. the\n\t\t\t\t\t * extension subtable). */\n  Offset32\textensionOffset;\t/* Offset to the extension subtable,\n\t\t\t\t\t * of lookup type subtable. */\n  public:\n  DEFINE_SIZE_STATIC (8);\n};\n\ntemplate <typename T>\nstruct Extension\n{\n  unsigned int get_type () const\n  {\n    switch (u.format) {\n    case 1: return u.format1.get_type ();\n    default:return 0;\n    }\n  }\n  template <typename X>\n  const X& get_subtable () const\n  {\n    switch (u.format) {\n    case 1: return u.format1.template get_subtable<typename T::SubTable> ();\n    default:return Null (typename T::SubTable);\n    }\n  }\n\n  // Specialization of dispatch for subset. dispatch() normally just\n  // dispatches to the sub table this points too, but for subset\n  // we need to run subset on this subtable too.\n  template <typename ...Ts>\n  typename hb_subset_context_t::return_t dispatch (hb_subset_context_t *c, Ts&&... ds) const\n  {\n    switch (u.format) {\n    case 1: return u.format1.subset (c);\n    default: return c->default_return_value ();\n    }\n  }\n\n  template <typename context_t, typename ...Ts>\n  typename context_t::return_t dispatch (context_t *c, Ts&&... ds) const\n  {\n    if (unlikely (!c->may_dispatch (this, &u.format))) return c->no_dispatch_return_value ();\n    TRACE_DISPATCH (this, u.format);\n    switch (u.format) {\n    case 1: return_trace (u.format1.dispatch (c, std::forward<Ts> (ds)...));\n    default:return_trace (c->default_return_value ());\n    }\n  }\n\n  protected:\n  union {\n  HBUINT16\t\tformat;\t\t/* Format identifier */\n  ExtensionFormat1<T>\tformat1;\n  } u;\n};\n\n\n/*\n * GSUB/GPOS Common\n */\n\nstruct hb_ot_layout_lookup_accelerator_t\n{\n  template <typename TLookup>\n  static hb_ot_layout_lookup_accelerator_t *create (const TLookup &lookup)\n  {\n    unsigned count = lookup.get_subtable_count ();\n\n    unsigned size = sizeof (hb_ot_layout_lookup_accelerator_t) -\n\t\t    HB_VAR_ARRAY * sizeof (hb_accelerate_subtables_context_t::hb_applicable_t) +\n\t\t    count * sizeof (hb_accelerate_subtables_context_t::hb_applicable_t);\n\n    /* The following is a calloc because when we are collecting subtables,\n     * some of them might be invalid and hence not collect; as a result,\n     * we might not fill in all the count entries of the subtables array.\n     * Zeroing it allows the set digest to gatekeep it without having to\n     * initialize it further. */\n    auto *thiz = (hb_ot_layout_lookup_accelerator_t *) hb_calloc (1, size);\n    if (unlikely (!thiz))\n      return nullptr;\n\n    hb_accelerate_subtables_context_t c_accelerate_subtables (thiz->subtables);\n    lookup.dispatch (&c_accelerate_subtables);\n\n    thiz->digest.init ();\n    for (auto& subtable : hb_iter (thiz->subtables, count))\n      thiz->digest.add (subtable.digest);\n\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n    thiz->cache_user_idx = c_accelerate_subtables.cache_user_idx;\n    for (unsigned i = 0; i < count; i++)\n      if (i != thiz->cache_user_idx)\n\tthiz->subtables[i].apply_cached_func = thiz->subtables[i].apply_func;\n#endif\n\n    return thiz;\n  }\n\n  bool may_have (hb_codepoint_t g) const\n  { return digest.may_have (g); }\n\n  bool apply (hb_ot_apply_context_t *c, unsigned subtables_count, bool use_cache) const\n  {\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n    if (use_cache)\n    {\n      return\n      + hb_iter (hb_iter (subtables, subtables_count))\n      | hb_map ([&c] (const hb_accelerate_subtables_context_t::hb_applicable_t &_) { return _.apply_cached (c); })\n      | hb_any\n      ;\n    }\n    else\n#endif\n    {\n      return\n      + hb_iter (hb_iter (subtables, subtables_count))\n      | hb_map ([&c] (const hb_accelerate_subtables_context_t::hb_applicable_t &_) { return _.apply (c); })\n      | hb_any\n      ;\n    }\n    return false;\n  }\n\n  bool cache_enter (hb_ot_apply_context_t *c) const\n  {\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n    return cache_user_idx != (unsigned) -1 &&\n\t   subtables[cache_user_idx].cache_enter (c);\n#else\n    return false;\n#endif\n  }\n  void cache_leave (hb_ot_apply_context_t *c) const\n  {\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n    subtables[cache_user_idx].cache_leave (c);\n#endif\n  }\n\n\n  hb_set_digest_t digest;\n  private:\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n  unsigned cache_user_idx = (unsigned) -1;\n#endif\n  hb_accelerate_subtables_context_t::hb_applicable_t subtables[HB_VAR_ARRAY];\n};\n\ntemplate <typename Types>\nstruct GSUBGPOSVersion1_2\n{\n  friend struct GSUBGPOS;\n\n  protected:\n  FixedVersion<>version;\t/* Version of the GSUB/GPOS table--initially set\n\t\t\t\t * to 0x00010000u */\n  typename Types:: template OffsetTo<ScriptList>\n\t\tscriptList;\t/* ScriptList table */\n  typename Types::template OffsetTo<FeatureList>\n\t\tfeatureList;\t/* FeatureList table */\n  typename Types::template OffsetTo<LookupList<Types>>\n\t\tlookupList;\t/* LookupList table */\n  Offset32To<FeatureVariations>\n\t\tfeatureVars;\t/* Offset to Feature Variations\n\t\t\t\t   table--from beginning of table\n\t\t\t\t * (may be NULL).  Introduced\n\t\t\t\t * in version 0x00010001. */\n  public:\n  DEFINE_SIZE_MIN (4 + 3 * Types::size);\n\n  unsigned int get_size () const\n  {\n    return min_size +\n\t   (version.to_int () >= 0x00010001u ? featureVars.static_size : 0);\n  }\n\n  const typename Types::template OffsetTo<LookupList<Types>>* get_lookup_list_offset () const\n  {\n    return &lookupList;\n  }\n\n  template <typename TLookup>\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    typedef List16OfOffsetTo<TLookup, typename Types::HBUINT> TLookupList;\n    if (unlikely (!(scriptList.sanitize (c, this) &&\n\t\t    featureList.sanitize (c, this) &&\n\t\t    reinterpret_cast<const typename Types::template OffsetTo<TLookupList> &> (lookupList).sanitize (c, this))))\n      return_trace (false);\n\n#ifndef HB_NO_VAR\n    if (unlikely (!(version.to_int () < 0x00010001u || featureVars.sanitize (c, this))))\n      return_trace (false);\n#endif\n\n    return_trace (true);\n  }\n\n  template <typename TLookup>\n  bool subset (hb_subset_layout_context_t *c) const\n  {\n    TRACE_SUBSET (this);\n\n    auto *out = c->subset_context->serializer->start_embed (this);\n    if (unlikely (!c->subset_context->serializer->extend_min (out))) return_trace (false);\n\n    out->version = version;\n\n    typedef LookupOffsetList<TLookup, typename Types::HBUINT> TLookupList;\n    reinterpret_cast<typename Types::template OffsetTo<TLookupList> &> (out->lookupList)\n\t.serialize_subset (c->subset_context,\n\t\t\t   reinterpret_cast<const typename Types::template OffsetTo<TLookupList> &> (lookupList),\n\t\t\t   this,\n\t\t\t   c);\n\n    reinterpret_cast<typename Types::template OffsetTo<RecordListOfFeature> &> (out->featureList)\n\t.serialize_subset (c->subset_context,\n\t\t\t   reinterpret_cast<const typename Types::template OffsetTo<RecordListOfFeature> &> (featureList),\n\t\t\t   this,\n\t\t\t   c);\n\n    out->scriptList.serialize_subset (c->subset_context,\n\t\t\t\t      scriptList,\n\t\t\t\t      this,\n\t\t\t\t      c);\n\n#ifndef HB_NO_VAR\n    if (version.to_int () >= 0x00010001u)\n    {\n      auto snapshot = c->subset_context->serializer->snapshot ();\n      if (!c->subset_context->serializer->extend_min (&out->featureVars))\n        return_trace (false);\n\n      // TODO(qxliu76): the current implementation doesn't correctly handle feature variations\n      //                that are dropped by instancing when the associated conditions don't trigger.\n      //                Since partial instancing isn't yet supported this isn't an issue yet but will\n      //                need to be fixed for partial instancing.\n\n\n\n      // if all axes are pinned all feature vars are dropped.\n      bool ret = !c->subset_context->plan->all_axes_pinned\n                 && out->featureVars.serialize_subset (c->subset_context, featureVars, this, c);\n      if (!ret && version.major == 1)\n      {\n        c->subset_context->serializer->revert (snapshot);\n\tout->version.major = 1;\n\tout->version.minor = 0;\n      }\n    }\n#endif\n\n    return_trace (true);\n  }\n};\n\nstruct GSUBGPOS\n{\n  unsigned int get_size () const\n  {\n    switch (u.version.major) {\n    case 1: return u.version1.get_size ();\n#ifndef HB_NO_BEYOND_64K\n    case 2: return u.version2.get_size ();\n#endif\n    default: return u.version.static_size;\n    }\n  }\n\n  template <typename TLookup>\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    if (unlikely (!u.version.sanitize (c))) return_trace (false);\n    switch (u.version.major) {\n    case 1: return_trace (u.version1.sanitize<TLookup> (c));\n#ifndef HB_NO_BEYOND_64K\n    case 2: return_trace (u.version2.sanitize<TLookup> (c));\n#endif\n    default: return_trace (true);\n    }\n  }\n\n  template <typename TLookup>\n  bool subset (hb_subset_layout_context_t *c) const\n  {\n    switch (u.version.major) {\n    case 1: return u.version1.subset<TLookup> (c);\n#ifndef HB_NO_BEYOND_64K\n    case 2: return u.version2.subset<TLookup> (c);\n#endif\n    default: return false;\n    }\n  }\n\n  const ScriptList &get_script_list () const\n  {\n    switch (u.version.major) {\n    case 1: return this+u.version1.scriptList;\n#ifndef HB_NO_BEYOND_64K\n    case 2: return this+u.version2.scriptList;\n#endif\n    default: return Null (ScriptList);\n    }\n  }\n  const FeatureList &get_feature_list () const\n  {\n    switch (u.version.major) {\n    case 1: return this+u.version1.featureList;\n#ifndef HB_NO_BEYOND_64K\n    case 2: return this+u.version2.featureList;\n#endif\n    default: return Null (FeatureList);\n    }\n  }\n  unsigned int get_lookup_count () const\n  {\n    switch (u.version.major) {\n    case 1: return (this+u.version1.lookupList).len;\n#ifndef HB_NO_BEYOND_64K\n    case 2: return (this+u.version2.lookupList).len;\n#endif\n    default: return 0;\n    }\n  }\n  const Lookup& get_lookup (unsigned int i) const\n  {\n    switch (u.version.major) {\n    case 1: return (this+u.version1.lookupList)[i];\n#ifndef HB_NO_BEYOND_64K\n    case 2: return (this+u.version2.lookupList)[i];\n#endif\n    default: return Null (Lookup);\n    }\n  }\n  const FeatureVariations &get_feature_variations () const\n  {\n    switch (u.version.major) {\n    case 1: return (u.version.to_int () >= 0x00010001u ? this+u.version1.featureVars : Null (FeatureVariations));\n#ifndef HB_NO_BEYOND_64K\n    case 2: return this+u.version2.featureVars;\n#endif\n    default: return Null (FeatureVariations);\n    }\n  }\n\n  bool has_data () const { return u.version.to_int (); }\n  unsigned int get_script_count () const\n  { return get_script_list ().len; }\n  const Tag& get_script_tag (unsigned int i) const\n  { return get_script_list ().get_tag (i); }\n  unsigned int get_script_tags (unsigned int start_offset,\n\t\t\t\tunsigned int *script_count /* IN/OUT */,\n\t\t\t\thb_tag_t     *script_tags /* OUT */) const\n  { return get_script_list ().get_tags (start_offset, script_count, script_tags); }\n  const Script& get_script (unsigned int i) const\n  { return get_script_list ()[i]; }\n  bool find_script_index (hb_tag_t tag, unsigned int *index) const\n  { return get_script_list ().find_index (tag, index); }\n\n  unsigned int get_feature_count () const\n  { return get_feature_list ().len; }\n  hb_tag_t get_feature_tag (unsigned int i) const\n  { return i == Index::NOT_FOUND_INDEX ? HB_TAG_NONE : get_feature_list ().get_tag (i); }\n  unsigned int get_feature_tags (unsigned int start_offset,\n\t\t\t\t unsigned int *feature_count /* IN/OUT */,\n\t\t\t\t hb_tag_t     *feature_tags /* OUT */) const\n  { return get_feature_list ().get_tags (start_offset, feature_count, feature_tags); }\n  const Feature& get_feature (unsigned int i) const\n  { return get_feature_list ()[i]; }\n  bool find_feature_index (hb_tag_t tag, unsigned int *index) const\n  { return get_feature_list ().find_index (tag, index); }\n\n  bool find_variations_index (const int *coords, unsigned int num_coords,\n\t\t\t      unsigned int *index) const\n  {\n#ifdef HB_NO_VAR\n    *index = FeatureVariations::NOT_FOUND_INDEX;\n    return false;\n#endif\n    return get_feature_variations ().find_index (coords, num_coords, index);\n  }\n  const Feature& get_feature_variation (unsigned int feature_index,\n\t\t\t\t\tunsigned int variations_index) const\n  {\n#ifndef HB_NO_VAR\n    if (FeatureVariations::NOT_FOUND_INDEX != variations_index &&\n\tu.version.to_int () >= 0x00010001u)\n    {\n      const Feature *feature = get_feature_variations ().find_substitute (variations_index,\n\t\t\t\t\t\t\t\t\t  feature_index);\n      if (feature)\n\treturn *feature;\n    }\n#endif\n    return get_feature (feature_index);\n  }\n\n  void feature_variation_collect_lookups (const hb_set_t *feature_indexes,\n\t\t\t\t\t  const hb_hashmap_t<unsigned, const Feature*> *feature_substitutes_map,\n\t\t\t\t\t  hb_set_t       *lookup_indexes /* OUT */) const\n  {\n#ifndef HB_NO_VAR\n    get_feature_variations ().collect_lookups (feature_indexes, feature_substitutes_map, lookup_indexes);\n#endif\n  }\n\n#ifndef HB_NO_VAR\n  void collect_feature_substitutes_with_variations (hb_collect_feature_substitutes_with_var_context_t *c) const\n  { get_feature_variations ().collect_feature_substitutes_with_variations (c); }\n#endif\n\n  template <typename TLookup>\n  void closure_lookups (hb_face_t      *face,\n\t\t\tconst hb_set_t *glyphs,\n\t\t\thb_set_t       *lookup_indexes /* IN/OUT */) const\n  {\n    hb_set_t visited_lookups, inactive_lookups;\n    hb_closure_lookups_context_t c (face, glyphs, &visited_lookups, &inactive_lookups);\n\n    c.set_recurse_func (TLookup::template dispatch_recurse_func<hb_closure_lookups_context_t>);\n\n    for (unsigned lookup_index : *lookup_indexes)\n      reinterpret_cast<const TLookup &> (get_lookup (lookup_index)).closure_lookups (&c, lookup_index);\n\n    hb_set_union (lookup_indexes, &visited_lookups);\n    hb_set_subtract (lookup_indexes, &inactive_lookups);\n  }\n\n  void prune_langsys (const hb_map_t *duplicate_feature_map,\n                      const hb_set_t *layout_scripts,\n                      hb_hashmap_t<unsigned, hb::unique_ptr<hb_set_t>> *script_langsys_map,\n                      hb_set_t       *new_feature_indexes /* OUT */) const\n  {\n    hb_prune_langsys_context_t c (this, script_langsys_map, duplicate_feature_map, new_feature_indexes);\n\n    unsigned count = get_script_count ();\n    for (unsigned script_index = 0; script_index < count; script_index++)\n    {\n      const Tag& tag = get_script_tag (script_index);\n      if (!layout_scripts->has (tag)) continue;\n      const Script& s = get_script (script_index);\n      s.prune_langsys (&c, script_index);\n    }\n  }\n\n  void prune_features (const hb_map_t *lookup_indices, /* IN */\n\t\t       const hb_hashmap_t<unsigned, hb::shared_ptr<hb_set_t>> *feature_record_cond_idx_map, /* IN */\n\t\t       const hb_hashmap_t<unsigned, const Feature*> *feature_substitutes_map, /* IN */\n\t\t       hb_set_t       *feature_indices /* IN/OUT */) const\n  {\n#ifndef HB_NO_VAR\n    // This is the set of feature indices which have alternate versions defined\n    // if the FeatureVariation's table and the alternate version(s) intersect the\n    // set of lookup indices.\n    hb_set_t alternate_feature_indices;\n    get_feature_variations ().closure_features (lookup_indices, feature_record_cond_idx_map, &alternate_feature_indices);\n    if (unlikely (alternate_feature_indices.in_error()))\n    {\n      feature_indices->err ();\n      return;\n    }\n#endif\n\n    for (unsigned i : hb_iter (feature_indices))\n    {\n      hb_tag_t tag =  get_feature_tag (i);\n      if (tag == HB_TAG ('p', 'r', 'e', 'f'))\n        // Note: Never ever drop feature 'pref', even if it's empty.\n        // HarfBuzz chooses shaper for Khmer based on presence of this\n        // feature.\tSee thread at:\n\t// http://lists.freedesktop.org/archives/harfbuzz/2012-November/002660.html\n        continue;\n\n\n      const Feature *f = &(get_feature (i));\n      const Feature** p = nullptr;\n      if (feature_substitutes_map->has (i, &p))\n        f = *p;\n\n      if (!f->featureParams.is_null () &&\n          tag == HB_TAG ('s', 'i', 'z', 'e'))\n        continue;\n\n      if (!f->intersects_lookup_indexes (lookup_indices)\n#ifndef HB_NO_VAR\n          && !alternate_feature_indices.has (i)\n#endif\n\t  )\n\tfeature_indices->del (i);\n    }\n  }\n\n  template <typename T>\n  struct accelerator_t\n  {\n    accelerator_t (hb_face_t *face)\n    {\n      this->table = hb_sanitize_context_t ().reference_table<T> (face);\n      if (unlikely (this->table->is_blocklisted (this->table.get_blob (), face)))\n      {\n\thb_blob_destroy (this->table.get_blob ());\n\tthis->table = hb_blob_get_empty ();\n      }\n\n      this->lookup_count = table->get_lookup_count ();\n\n      this->accels = (hb_atomic_ptr_t<hb_ot_layout_lookup_accelerator_t> *) hb_calloc (this->lookup_count, sizeof (*accels));\n      if (unlikely (!this->accels))\n      {\n\tthis->lookup_count = 0;\n\tthis->table.destroy ();\n\tthis->table = hb_blob_get_empty ();\n      }\n    }\n    ~accelerator_t ()\n    {\n      for (unsigned int i = 0; i < this->lookup_count; i++)\n\thb_free (this->accels[i]);\n      hb_free (this->accels);\n      this->table.destroy ();\n    }\n\n    hb_ot_layout_lookup_accelerator_t *get_accel (unsigned lookup_index) const\n    {\n      if (unlikely (lookup_index >= lookup_count)) return nullptr;\n\n    retry:\n      auto *accel = accels[lookup_index].get_acquire ();\n      if (unlikely (!accel))\n      {\n\taccel = hb_ot_layout_lookup_accelerator_t::create (table->get_lookup (lookup_index));\n\tif (unlikely (!accel))\n\t  return nullptr;\n\n\tif (unlikely (!accels[lookup_index].cmpexch (nullptr, accel)))\n\t{\n\t  hb_free (accel);\n\t  goto retry;\n\t}\n      }\n\n      return accel;\n    }\n\n    hb_blob_ptr_t<T> table;\n    unsigned int lookup_count;\n    hb_atomic_ptr_t<hb_ot_layout_lookup_accelerator_t> *accels;\n  };\n\n  protected:\n  union {\n  FixedVersion<>\t\t\tversion;\t/* Version identifier */\n  GSUBGPOSVersion1_2<SmallTypes>\tversion1;\n#ifndef HB_NO_BEYOND_64K\n  GSUBGPOSVersion1_2<MediumTypes>\tversion2;\n#endif\n  } u;\n  public:\n  DEFINE_SIZE_MIN (4);\n};\n\n\n} /* namespace OT */\n\n\n#endif /* HB_OT_LAYOUT_GSUBGPOS_HH */\n"], "fixing_code": ["/*\n * Copyright \u00a9 2007,2008,2009,2010  Red Hat, Inc.\n * Copyright \u00a9 2010,2012  Google, Inc.\n *\n *  This is part of HarfBuzz, a text shaping library.\n *\n * Permission is hereby granted, without written agreement and without\n * license or royalty fees, to use, copy, modify, and distribute this\n * software and its documentation for any purpose, provided that the\n * above copyright notice and the following two paragraphs appear in\n * all copies of this software.\n *\n * IN NO EVENT SHALL THE COPYRIGHT HOLDER BE LIABLE TO ANY PARTY FOR\n * DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES\n * ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN\n * IF THE COPYRIGHT HOLDER HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH\n * DAMAGE.\n *\n * THE COPYRIGHT HOLDER SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING,\n * BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND\n * FITNESS FOR A PARTICULAR PURPOSE.  THE SOFTWARE PROVIDED HEREUNDER IS\n * ON AN \"AS IS\" BASIS, AND THE COPYRIGHT HOLDER HAS NO OBLIGATION TO\n * PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.\n *\n * Red Hat Author(s): Behdad Esfahbod\n * Google Author(s): Behdad Esfahbod\n */\n\n#ifndef HB_OT_LAYOUT_GSUBGPOS_HH\n#define HB_OT_LAYOUT_GSUBGPOS_HH\n\n#include \"hb.hh\"\n#include \"hb-buffer.hh\"\n#include \"hb-map.hh\"\n#include \"hb-set.hh\"\n#include \"hb-ot-map.hh\"\n#include \"hb-ot-layout-common.hh\"\n#include \"hb-ot-layout-gdef-table.hh\"\n\n\nnamespace OT {\n\n\nstruct hb_intersects_context_t :\n       hb_dispatch_context_t<hb_intersects_context_t, bool>\n{\n  template <typename T>\n  return_t dispatch (const T &obj) { return obj.intersects (this->glyphs); }\n  static return_t default_return_value () { return false; }\n  bool stop_sublookup_iteration (return_t r) const { return r; }\n\n  const hb_set_t *glyphs;\n\n  hb_intersects_context_t (const hb_set_t *glyphs_) :\n                            glyphs (glyphs_) {}\n};\n\nstruct hb_have_non_1to1_context_t :\n       hb_dispatch_context_t<hb_have_non_1to1_context_t, bool>\n{\n  template <typename T>\n  return_t dispatch (const T &obj) { return obj.may_have_non_1to1 (); }\n  static return_t default_return_value () { return false; }\n  bool stop_sublookup_iteration (return_t r) const { return r; }\n};\n\nstruct hb_closure_context_t :\n       hb_dispatch_context_t<hb_closure_context_t>\n{\n  typedef return_t (*recurse_func_t) (hb_closure_context_t *c, unsigned lookup_index, hb_set_t *covered_seq_indicies, unsigned seq_index, unsigned end_index);\n  template <typename T>\n  return_t dispatch (const T &obj) { obj.closure (this); return hb_empty_t (); }\n  static return_t default_return_value () { return hb_empty_t (); }\n  void recurse (unsigned lookup_index, hb_set_t *covered_seq_indicies, unsigned seq_index, unsigned end_index)\n  {\n    if (unlikely (nesting_level_left == 0 || !recurse_func))\n      return;\n\n    nesting_level_left--;\n    recurse_func (this, lookup_index, covered_seq_indicies, seq_index, end_index);\n    nesting_level_left++;\n  }\n\n  void reset_lookup_visit_count ()\n  { lookup_count = 0; }\n\n  bool lookup_limit_exceeded ()\n  { return lookup_count > HB_MAX_LOOKUP_VISIT_COUNT; }\n\n  bool should_visit_lookup (unsigned int lookup_index)\n  {\n    if (lookup_count++ > HB_MAX_LOOKUP_VISIT_COUNT)\n      return false;\n\n    if (is_lookup_done (lookup_index))\n      return false;\n\n    return true;\n  }\n\n  bool is_lookup_done (unsigned int lookup_index)\n  {\n    if (unlikely (done_lookups_glyph_count->in_error () ||\n\t\t  done_lookups_glyph_set->in_error ()))\n      return true;\n\n    /* Have we visited this lookup with the current set of glyphs? */\n    if (done_lookups_glyph_count->get (lookup_index) != glyphs->get_population ())\n    {\n      done_lookups_glyph_count->set (lookup_index, glyphs->get_population ());\n\n      if (!done_lookups_glyph_set->has (lookup_index))\n      {\n\tif (unlikely (!done_lookups_glyph_set->set (lookup_index, hb::unique_ptr<hb_set_t> {hb_set_create ()})))\n\t  return true;\n      }\n\n      done_lookups_glyph_set->get (lookup_index)->clear ();\n    }\n\n    hb_set_t *covered_glyph_set = done_lookups_glyph_set->get (lookup_index);\n    if (unlikely (covered_glyph_set->in_error ()))\n      return true;\n    if (parent_active_glyphs ().is_subset (*covered_glyph_set))\n      return true;\n\n    covered_glyph_set->union_ (parent_active_glyphs ());\n    return false;\n  }\n\n  const hb_set_t& previous_parent_active_glyphs () {\n    if (active_glyphs_stack.length <= 1)\n      return *glyphs;\n\n    return active_glyphs_stack[active_glyphs_stack.length - 2];\n  }\n\n  const hb_set_t& parent_active_glyphs ()\n  {\n    if (!active_glyphs_stack)\n      return *glyphs;\n\n    return active_glyphs_stack.tail ();\n  }\n\n  hb_set_t& push_cur_active_glyphs ()\n  {\n    return *active_glyphs_stack.push ();\n  }\n\n  bool pop_cur_done_glyphs ()\n  {\n    if (!active_glyphs_stack)\n      return false;\n\n    active_glyphs_stack.pop ();\n    return true;\n  }\n\n  hb_face_t *face;\n  hb_set_t *glyphs;\n  hb_set_t output[1];\n  hb_vector_t<hb_set_t> active_glyphs_stack;\n  recurse_func_t recurse_func = nullptr;\n  unsigned int nesting_level_left;\n\n  hb_closure_context_t (hb_face_t *face_,\n\t\t\thb_set_t *glyphs_,\n\t\t\thb_map_t *done_lookups_glyph_count_,\n\t\t\thb_hashmap_t<unsigned, hb::unique_ptr<hb_set_t>> *done_lookups_glyph_set_,\n\t\t\tunsigned int nesting_level_left_ = HB_MAX_NESTING_LEVEL) :\n\t\t\t  face (face_),\n\t\t\t  glyphs (glyphs_),\n\t\t\t  nesting_level_left (nesting_level_left_),\n\t\t\t  done_lookups_glyph_count (done_lookups_glyph_count_),\n\t\t\t  done_lookups_glyph_set (done_lookups_glyph_set_)\n  {}\n\n  ~hb_closure_context_t () { flush (); }\n\n  void set_recurse_func (recurse_func_t func) { recurse_func = func; }\n\n  void flush ()\n  {\n    output->del_range (face->get_num_glyphs (), HB_SET_VALUE_INVALID);\t/* Remove invalid glyphs. */\n    glyphs->union_ (*output);\n    output->clear ();\n    active_glyphs_stack.pop ();\n    active_glyphs_stack.reset ();\n  }\n\n  private:\n  hb_map_t *done_lookups_glyph_count;\n  hb_hashmap_t<unsigned, hb::unique_ptr<hb_set_t>> *done_lookups_glyph_set;\n  unsigned int lookup_count = 0;\n};\n\n\n\nstruct hb_closure_lookups_context_t :\n       hb_dispatch_context_t<hb_closure_lookups_context_t>\n{\n  typedef return_t (*recurse_func_t) (hb_closure_lookups_context_t *c, unsigned lookup_index);\n  template <typename T>\n  return_t dispatch (const T &obj) { obj.closure_lookups (this); return hb_empty_t (); }\n  static return_t default_return_value () { return hb_empty_t (); }\n  void recurse (unsigned lookup_index)\n  {\n    if (unlikely (nesting_level_left == 0 || !recurse_func))\n      return;\n\n    /* Return if new lookup was recursed to before. */\n    if (lookup_limit_exceeded ()\n        || visited_lookups->in_error ()\n        || visited_lookups->has (lookup_index))\n      // Don't increment lookup count here, that will be done in the call to closure_lookups()\n      // made by recurse_func.\n      return;\n\n    nesting_level_left--;\n    recurse_func (this, lookup_index);\n    nesting_level_left++;\n  }\n\n  void set_lookup_visited (unsigned lookup_index)\n  { visited_lookups->add (lookup_index); }\n\n  void set_lookup_inactive (unsigned lookup_index)\n  { inactive_lookups->add (lookup_index); }\n\n  bool lookup_limit_exceeded ()\n  {\n    bool ret = lookup_count > HB_MAX_LOOKUP_VISIT_COUNT;\n    if (ret)\n      DEBUG_MSG (SUBSET, nullptr, \"lookup visit count limit exceeded in lookup closure!\");\n    return ret; }\n\n  bool is_lookup_visited (unsigned lookup_index)\n  {\n    if (unlikely (lookup_count++ > HB_MAX_LOOKUP_VISIT_COUNT))\n    {\n      DEBUG_MSG (SUBSET, nullptr, \"total visited lookup count %u exceeds max limit, lookup %u is dropped.\",\n                 lookup_count, lookup_index);\n      return true;\n    }\n\n    if (unlikely (visited_lookups->in_error ()))\n      return true;\n\n    return visited_lookups->has (lookup_index);\n  }\n\n  hb_face_t *face;\n  const hb_set_t *glyphs;\n  recurse_func_t recurse_func;\n  unsigned int nesting_level_left;\n\n  hb_closure_lookups_context_t (hb_face_t *face_,\n\t\t\t\tconst hb_set_t *glyphs_,\n\t\t\t\thb_set_t *visited_lookups_,\n\t\t\t\thb_set_t *inactive_lookups_,\n\t\t\t\tunsigned nesting_level_left_ = HB_MAX_NESTING_LEVEL) :\n\t\t\t\tface (face_),\n\t\t\t\tglyphs (glyphs_),\n\t\t\t\trecurse_func (nullptr),\n\t\t\t\tnesting_level_left (nesting_level_left_),\n\t\t\t\tvisited_lookups (visited_lookups_),\n\t\t\t\tinactive_lookups (inactive_lookups_),\n\t\t\t\tlookup_count (0) {}\n\n  void set_recurse_func (recurse_func_t func) { recurse_func = func; }\n\n  private:\n  hb_set_t *visited_lookups;\n  hb_set_t *inactive_lookups;\n  unsigned int lookup_count;\n};\n\nstruct hb_would_apply_context_t :\n       hb_dispatch_context_t<hb_would_apply_context_t, bool>\n{\n  template <typename T>\n  return_t dispatch (const T &obj) { return obj.would_apply (this); }\n  static return_t default_return_value () { return false; }\n  bool stop_sublookup_iteration (return_t r) const { return r; }\n\n  hb_face_t *face;\n  const hb_codepoint_t *glyphs;\n  unsigned int len;\n  bool zero_context;\n\n  hb_would_apply_context_t (hb_face_t *face_,\n\t\t\t    const hb_codepoint_t *glyphs_,\n\t\t\t    unsigned int len_,\n\t\t\t    bool zero_context_) :\n\t\t\t      face (face_),\n\t\t\t      glyphs (glyphs_),\n\t\t\t      len (len_),\n\t\t\t      zero_context (zero_context_) {}\n};\n\nstruct hb_collect_glyphs_context_t :\n       hb_dispatch_context_t<hb_collect_glyphs_context_t>\n{\n  typedef return_t (*recurse_func_t) (hb_collect_glyphs_context_t *c, unsigned int lookup_index);\n  template <typename T>\n  return_t dispatch (const T &obj) { obj.collect_glyphs (this); return hb_empty_t (); }\n  static return_t default_return_value () { return hb_empty_t (); }\n  void recurse (unsigned int lookup_index)\n  {\n    if (unlikely (nesting_level_left == 0 || !recurse_func))\n      return;\n\n    /* Note that GPOS sets recurse_func to nullptr already, so it doesn't get\n     * past the previous check.  For GSUB, we only want to collect the output\n     * glyphs in the recursion.  If output is not requested, we can go home now.\n     *\n     * Note further, that the above is not exactly correct.  A recursed lookup\n     * is allowed to match input that is not matched in the context, but that's\n     * not how most fonts are built.  It's possible to relax that and recurse\n     * with all sets here if it proves to be an issue.\n     */\n\n    if (output == hb_set_get_empty ())\n      return;\n\n    /* Return if new lookup was recursed to before. */\n    if (recursed_lookups->has (lookup_index))\n      return;\n\n    hb_set_t *old_before = before;\n    hb_set_t *old_input  = input;\n    hb_set_t *old_after  = after;\n    before = input = after = hb_set_get_empty ();\n\n    nesting_level_left--;\n    recurse_func (this, lookup_index);\n    nesting_level_left++;\n\n    before = old_before;\n    input  = old_input;\n    after  = old_after;\n\n    recursed_lookups->add (lookup_index);\n  }\n\n  hb_face_t *face;\n  hb_set_t *before;\n  hb_set_t *input;\n  hb_set_t *after;\n  hb_set_t *output;\n  recurse_func_t recurse_func;\n  hb_set_t *recursed_lookups;\n  unsigned int nesting_level_left;\n\n  hb_collect_glyphs_context_t (hb_face_t *face_,\n\t\t\t       hb_set_t  *glyphs_before, /* OUT.  May be NULL */\n\t\t\t       hb_set_t  *glyphs_input,  /* OUT.  May be NULL */\n\t\t\t       hb_set_t  *glyphs_after,  /* OUT.  May be NULL */\n\t\t\t       hb_set_t  *glyphs_output, /* OUT.  May be NULL */\n\t\t\t       unsigned int nesting_level_left_ = HB_MAX_NESTING_LEVEL) :\n\t\t\t      face (face_),\n\t\t\t      before (glyphs_before ? glyphs_before : hb_set_get_empty ()),\n\t\t\t      input  (glyphs_input  ? glyphs_input  : hb_set_get_empty ()),\n\t\t\t      after  (glyphs_after  ? glyphs_after  : hb_set_get_empty ()),\n\t\t\t      output (glyphs_output ? glyphs_output : hb_set_get_empty ()),\n\t\t\t      recurse_func (nullptr),\n\t\t\t      recursed_lookups (hb_set_create ()),\n\t\t\t      nesting_level_left (nesting_level_left_) {}\n  ~hb_collect_glyphs_context_t () { hb_set_destroy (recursed_lookups); }\n\n  void set_recurse_func (recurse_func_t func) { recurse_func = func; }\n};\n\n\n\ntemplate <typename set_t>\nstruct hb_collect_coverage_context_t :\n       hb_dispatch_context_t<hb_collect_coverage_context_t<set_t>, const Coverage &>\n{\n  typedef const Coverage &return_t; // Stoopid that we have to dupe this here.\n  template <typename T>\n  return_t dispatch (const T &obj) { return obj.get_coverage (); }\n  static return_t default_return_value () { return Null (Coverage); }\n  bool stop_sublookup_iteration (return_t r) const\n  {\n    r.collect_coverage (set);\n    return false;\n  }\n\n  hb_collect_coverage_context_t (set_t *set_) :\n\t\t\t\t   set (set_) {}\n\n  set_t *set;\n};\n\nstruct hb_ot_apply_context_t :\n       hb_dispatch_context_t<hb_ot_apply_context_t, bool, HB_DEBUG_APPLY>\n{\n  struct matcher_t\n  {\n    matcher_t () :\n\t     lookup_props (0),\n\t     mask (-1),\n\t     ignore_zwnj (false),\n\t     ignore_zwj (false),\n\t     per_syllable (false),\n\t     syllable {0},\n\t     match_func (nullptr),\n\t     match_data (nullptr) {}\n\n    typedef bool (*match_func_t) (hb_glyph_info_t &info, unsigned value, const void *data);\n\n    void set_ignore_zwnj (bool ignore_zwnj_) { ignore_zwnj = ignore_zwnj_; }\n    void set_ignore_zwj (bool ignore_zwj_) { ignore_zwj = ignore_zwj_; }\n    void set_lookup_props (unsigned int lookup_props_) { lookup_props = lookup_props_; }\n    void set_mask (hb_mask_t mask_) { mask = mask_; }\n    void set_per_syllable (bool per_syllable_) { per_syllable = per_syllable_; }\n    void set_syllable (uint8_t syllable_)  { syllable = per_syllable ? syllable_ : 0; }\n    void set_match_func (match_func_t match_func_,\n\t\t\t const void *match_data_)\n    { match_func = match_func_; match_data = match_data_; }\n\n    enum may_match_t {\n      MATCH_NO,\n      MATCH_YES,\n      MATCH_MAYBE\n    };\n\n    may_match_t may_match (hb_glyph_info_t &info,\n\t\t\t   hb_codepoint_t glyph_data) const\n    {\n      if (!(info.mask & mask) ||\n\t  (syllable && syllable != info.syllable ()))\n\treturn MATCH_NO;\n\n      if (match_func)\n\treturn match_func (info, glyph_data, match_data) ? MATCH_YES : MATCH_NO;\n\n      return MATCH_MAYBE;\n    }\n\n    enum may_skip_t {\n      SKIP_NO,\n      SKIP_YES,\n      SKIP_MAYBE\n    };\n\n    may_skip_t may_skip (const hb_ot_apply_context_t *c,\n\t\t\t const hb_glyph_info_t       &info) const\n    {\n      if (!c->check_glyph_property (&info, lookup_props))\n\treturn SKIP_YES;\n\n      if (unlikely (_hb_glyph_info_is_default_ignorable_and_not_hidden (&info) &&\n\t\t    (ignore_zwnj || !_hb_glyph_info_is_zwnj (&info)) &&\n\t\t    (ignore_zwj || !_hb_glyph_info_is_zwj (&info))))\n\treturn SKIP_MAYBE;\n\n      return SKIP_NO;\n    }\n\n    protected:\n    unsigned int lookup_props;\n    hb_mask_t mask;\n    bool ignore_zwnj;\n    bool ignore_zwj;\n    bool per_syllable;\n    uint8_t syllable;\n    match_func_t match_func;\n    const void *match_data;\n  };\n\n  struct skipping_iterator_t\n  {\n    void init (hb_ot_apply_context_t *c_, bool context_match = false)\n    {\n      c = c_;\n      match_glyph_data16 = nullptr;\n#ifndef HB_NO_BEYOND_64K\n      match_glyph_data24 = nullptr;\n#endif\n      matcher.set_match_func (nullptr, nullptr);\n      matcher.set_lookup_props (c->lookup_props);\n      /* Ignore ZWNJ if we are matching GPOS, or matching GSUB context and asked to. */\n      matcher.set_ignore_zwnj (c->table_index == 1 || (context_match && c->auto_zwnj));\n      /* Ignore ZWJ if we are matching context, or asked to. */\n      matcher.set_ignore_zwj  (context_match || c->auto_zwj);\n      matcher.set_mask (context_match ? -1 : c->lookup_mask);\n      matcher.set_per_syllable (c->per_syllable);\n    }\n    void set_lookup_props (unsigned int lookup_props)\n    {\n      matcher.set_lookup_props (lookup_props);\n    }\n    void set_match_func (matcher_t::match_func_t match_func_,\n\t\t\t const void *match_data_)\n    {\n      matcher.set_match_func (match_func_, match_data_);\n    }\n    void set_glyph_data (const HBUINT16 glyph_data[])\n    {\n      match_glyph_data16 = glyph_data;\n#ifndef HB_NO_BEYOND_64K\n      match_glyph_data24 = nullptr;\n#endif\n    }\n#ifndef HB_NO_BEYOND_64K\n    void set_glyph_data (const HBUINT24 glyph_data[])\n    {\n      match_glyph_data16 = nullptr;\n      match_glyph_data24 = glyph_data;\n    }\n#endif\n\n    void reset (unsigned int start_index_,\n\t\tunsigned int num_items_)\n    {\n      idx = start_index_;\n      num_items = num_items_;\n      end = c->buffer->len;\n      matcher.set_syllable (start_index_ == c->buffer->idx ? c->buffer->cur().syllable () : 0);\n    }\n\n    void reject ()\n    {\n      num_items++;\n      backup_glyph_data ();\n    }\n\n    matcher_t::may_skip_t\n    may_skip (const hb_glyph_info_t &info) const\n    { return matcher.may_skip (c, info); }\n\n    bool next (unsigned *unsafe_to = nullptr)\n    {\n      assert (num_items > 0);\n      /* The alternate condition below is faster at string boundaries,\n       * but produces subpar \"unsafe-to-concat\" values. */\n      signed stop = (signed) end - (signed) num_items;\n      if (c->buffer->flags & HB_BUFFER_FLAG_PRODUCE_UNSAFE_TO_CONCAT)\n        stop = (signed) end - 1;\n      while ((signed) idx < stop)\n      {\n\tidx++;\n\thb_glyph_info_t &info = c->buffer->info[idx];\n\n\tmatcher_t::may_skip_t skip = matcher.may_skip (c, info);\n\tif (unlikely (skip == matcher_t::SKIP_YES))\n\t  continue;\n\n\tmatcher_t::may_match_t match = matcher.may_match (info, get_glyph_data ());\n\tif (match == matcher_t::MATCH_YES ||\n\t    (match == matcher_t::MATCH_MAYBE &&\n\t     skip == matcher_t::SKIP_NO))\n\t{\n\t  num_items--;\n\t  advance_glyph_data ();\n\t  return true;\n\t}\n\n\tif (skip == matcher_t::SKIP_NO)\n\t{\n\t  if (unsafe_to)\n\t    *unsafe_to = idx + 1;\n\t  return false;\n\t}\n      }\n      if (unsafe_to)\n        *unsafe_to = end;\n      return false;\n    }\n    bool prev (unsigned *unsafe_from = nullptr)\n    {\n      assert (num_items > 0);\n      /* The alternate condition below is faster at string boundaries,\n       * but produces subpar \"unsafe-to-concat\" values. */\n      unsigned stop = num_items - 1;\n      if (c->buffer->flags & HB_BUFFER_FLAG_PRODUCE_UNSAFE_TO_CONCAT)\n        stop = 1 - 1;\n\n      /* When looking back, limit how far we search; this function is mostly\n       * used for looking back for base glyphs when attaching marks. If we\n       * don't limit, we can get O(n^2) behavior where n is the number of\n       * consecutive marks. */\n      stop = (unsigned) hb_max ((int) stop, (int) idx - HB_MAX_CONTEXT_LENGTH);\n\n      while (idx > stop)\n      {\n\tidx--;\n\thb_glyph_info_t &info = c->buffer->out_info[idx];\n\n\tmatcher_t::may_skip_t skip = matcher.may_skip (c, info);\n\tif (unlikely (skip == matcher_t::SKIP_YES))\n\t  continue;\n\n\tmatcher_t::may_match_t match = matcher.may_match (info, get_glyph_data ());\n\tif (match == matcher_t::MATCH_YES ||\n\t    (match == matcher_t::MATCH_MAYBE &&\n\t     skip == matcher_t::SKIP_NO))\n\t{\n\t  num_items--;\n\t  advance_glyph_data ();\n\t  return true;\n\t}\n\n\tif (skip == matcher_t::SKIP_NO)\n\t{\n\t  if (unsafe_from)\n\t    *unsafe_from = hb_max (1u, idx) - 1u;\n\t  return false;\n\t}\n      }\n      if (unsafe_from)\n        *unsafe_from = 0;\n      return false;\n    }\n\n    hb_codepoint_t\n    get_glyph_data ()\n    {\n      if (match_glyph_data16) return *match_glyph_data16;\n#ifndef HB_NO_BEYOND_64K\n      else\n      if (match_glyph_data24) return *match_glyph_data24;\n#endif\n      return 0;\n    }\n    void\n    advance_glyph_data ()\n    {\n      if (match_glyph_data16) match_glyph_data16++;\n#ifndef HB_NO_BEYOND_64K\n      else\n      if (match_glyph_data24) match_glyph_data24++;\n#endif\n    }\n    void\n    backup_glyph_data ()\n    {\n      if (match_glyph_data16) match_glyph_data16--;\n#ifndef HB_NO_BEYOND_64K\n      else\n      if (match_glyph_data24) match_glyph_data24--;\n#endif\n    }\n\n    unsigned int idx;\n    protected:\n    hb_ot_apply_context_t *c;\n    matcher_t matcher;\n    const HBUINT16 *match_glyph_data16;\n#ifndef HB_NO_BEYOND_64K\n    const HBUINT24 *match_glyph_data24;\n#endif\n\n    unsigned int num_items;\n    unsigned int end;\n  };\n\n\n  const char *get_name () { return \"APPLY\"; }\n  typedef return_t (*recurse_func_t) (hb_ot_apply_context_t *c, unsigned int lookup_index);\n  template <typename T>\n  return_t dispatch (const T &obj) { return obj.apply (this); }\n  static return_t default_return_value () { return false; }\n  bool stop_sublookup_iteration (return_t r) const { return r; }\n  return_t recurse (unsigned int sub_lookup_index)\n  {\n    if (unlikely (nesting_level_left == 0 || !recurse_func || buffer->max_ops-- <= 0))\n    {\n      buffer->shaping_failed = true;\n      return default_return_value ();\n    }\n\n    nesting_level_left--;\n    bool ret = recurse_func (this, sub_lookup_index);\n    nesting_level_left++;\n    return ret;\n  }\n\n  skipping_iterator_t iter_input, iter_context;\n\n  unsigned int table_index; /* GSUB/GPOS */\n  hb_font_t *font;\n  hb_face_t *face;\n  hb_buffer_t *buffer;\n  recurse_func_t recurse_func = nullptr;\n  const GDEF &gdef;\n  const VariationStore &var_store;\n  VariationStore::cache_t *var_store_cache;\n  hb_set_digest_t digest;\n\n  hb_direction_t direction;\n  hb_mask_t lookup_mask = 1;\n  unsigned int lookup_index = (unsigned) -1;\n  unsigned int lookup_props = 0;\n  unsigned int nesting_level_left = HB_MAX_NESTING_LEVEL;\n\n  bool has_glyph_classes;\n  bool auto_zwnj = true;\n  bool auto_zwj = true;\n  bool per_syllable = false;\n  bool random = false;\n  uint32_t random_state = 1;\n  unsigned new_syllables = (unsigned) -1;\n\n  hb_ot_apply_context_t (unsigned int table_index_,\n\t\t\t hb_font_t *font_,\n\t\t\t hb_buffer_t *buffer_) :\n\t\t\ttable_index (table_index_),\n\t\t\tfont (font_), face (font->face), buffer (buffer_),\n\t\t\tgdef (\n#ifndef HB_NO_OT_LAYOUT\n\t\t\t      *face->table.GDEF->table\n#else\n\t\t\t      Null (GDEF)\n#endif\n\t\t\t     ),\n\t\t\tvar_store (gdef.get_var_store ()),\n\t\t\tvar_store_cache (\n#ifndef HB_NO_VAR\n\t\t\t\t\t table_index == 1 && font->num_coords ? var_store.create_cache () : nullptr\n#else\n\t\t\t\t\t nullptr\n#endif\n\t\t\t\t\t),\n\t\t\tdigest (buffer_->digest ()),\n\t\t\tdirection (buffer_->props.direction),\n\t\t\thas_glyph_classes (gdef.has_glyph_classes ())\n  { init_iters (); }\n\n  ~hb_ot_apply_context_t ()\n  {\n#ifndef HB_NO_VAR\n    VariationStore::destroy_cache (var_store_cache);\n#endif\n  }\n\n  void init_iters ()\n  {\n    iter_input.init (this, false);\n    iter_context.init (this, true);\n  }\n\n  void set_lookup_mask (hb_mask_t mask) { lookup_mask = mask; init_iters (); }\n  void set_auto_zwj (bool auto_zwj_) { auto_zwj = auto_zwj_; init_iters (); }\n  void set_auto_zwnj (bool auto_zwnj_) { auto_zwnj = auto_zwnj_; init_iters (); }\n  void set_per_syllable (bool per_syllable_) { per_syllable = per_syllable_; init_iters (); }\n  void set_random (bool random_) { random = random_; }\n  void set_recurse_func (recurse_func_t func) { recurse_func = func; }\n  void set_lookup_index (unsigned int lookup_index_) { lookup_index = lookup_index_; }\n  void set_lookup_props (unsigned int lookup_props_) { lookup_props = lookup_props_; init_iters (); }\n\n  uint32_t random_number ()\n  {\n    /* http://www.cplusplus.com/reference/random/minstd_rand/ */\n    random_state = random_state * 48271 % 2147483647;\n    return random_state;\n  }\n\n  bool match_properties_mark (hb_codepoint_t  glyph,\n\t\t\t      unsigned int    glyph_props,\n\t\t\t      unsigned int    match_props) const\n  {\n    /* If using mark filtering sets, the high short of\n     * match_props has the set index.\n     */\n    if (match_props & LookupFlag::UseMarkFilteringSet)\n      return gdef.mark_set_covers (match_props >> 16, glyph);\n\n    /* The second byte of match_props has the meaning\n     * \"ignore marks of attachment type different than\n     * the attachment type specified.\"\n     */\n    if (match_props & LookupFlag::MarkAttachmentType)\n      return (match_props & LookupFlag::MarkAttachmentType) == (glyph_props & LookupFlag::MarkAttachmentType);\n\n    return true;\n  }\n\n  bool check_glyph_property (const hb_glyph_info_t *info,\n\t\t\t     unsigned int  match_props) const\n  {\n    hb_codepoint_t glyph = info->codepoint;\n    unsigned int glyph_props = _hb_glyph_info_get_glyph_props (info);\n\n    /* Not covered, if, for example, glyph class is ligature and\n     * match_props includes LookupFlags::IgnoreLigatures\n     */\n    if (glyph_props & match_props & LookupFlag::IgnoreFlags)\n      return false;\n\n    if (unlikely (glyph_props & HB_OT_LAYOUT_GLYPH_PROPS_MARK))\n      return match_properties_mark (glyph, glyph_props, match_props);\n\n    return true;\n  }\n\n  void _set_glyph_class (hb_codepoint_t glyph_index,\n\t\t\t  unsigned int class_guess = 0,\n\t\t\t  bool ligature = false,\n\t\t\t  bool component = false)\n  {\n    digest.add (glyph_index);\n\n    if (new_syllables != (unsigned) -1)\n      buffer->cur().syllable() = new_syllables;\n\n    unsigned int props = _hb_glyph_info_get_glyph_props (&buffer->cur());\n    props |= HB_OT_LAYOUT_GLYPH_PROPS_SUBSTITUTED;\n    if (ligature)\n    {\n      props |= HB_OT_LAYOUT_GLYPH_PROPS_LIGATED;\n      /* In the only place that the MULTIPLIED bit is used, Uniscribe\n       * seems to only care about the \"last\" transformation between\n       * Ligature and Multiple substitutions.  Ie. if you ligate, expand,\n       * and ligate again, it forgives the multiplication and acts as\n       * if only ligation happened.  As such, clear MULTIPLIED bit.\n       */\n      props &= ~HB_OT_LAYOUT_GLYPH_PROPS_MULTIPLIED;\n    }\n    if (component)\n      props |= HB_OT_LAYOUT_GLYPH_PROPS_MULTIPLIED;\n    if (likely (has_glyph_classes))\n    {\n      props &= HB_OT_LAYOUT_GLYPH_PROPS_PRESERVE;\n      _hb_glyph_info_set_glyph_props (&buffer->cur(), props | gdef.get_glyph_props (glyph_index));\n    }\n    else if (class_guess)\n    {\n      props &= HB_OT_LAYOUT_GLYPH_PROPS_PRESERVE;\n      _hb_glyph_info_set_glyph_props (&buffer->cur(), props | class_guess);\n    }\n    else\n      _hb_glyph_info_set_glyph_props (&buffer->cur(), props);\n  }\n\n  void replace_glyph (hb_codepoint_t glyph_index)\n  {\n    _set_glyph_class (glyph_index);\n    (void) buffer->replace_glyph (glyph_index);\n  }\n  void replace_glyph_inplace (hb_codepoint_t glyph_index)\n  {\n    _set_glyph_class (glyph_index);\n    buffer->cur().codepoint = glyph_index;\n  }\n  void replace_glyph_with_ligature (hb_codepoint_t glyph_index,\n\t\t\t\t    unsigned int class_guess)\n  {\n    _set_glyph_class (glyph_index, class_guess, true);\n    (void) buffer->replace_glyph (glyph_index);\n  }\n  void output_glyph_for_component (hb_codepoint_t glyph_index,\n\t\t\t\t   unsigned int class_guess)\n  {\n    _set_glyph_class (glyph_index, class_guess, false, true);\n    (void) buffer->output_glyph (glyph_index);\n  }\n};\n\n\nstruct hb_accelerate_subtables_context_t :\n       hb_dispatch_context_t<hb_accelerate_subtables_context_t>\n{\n  template <typename Type>\n  static inline bool apply_to (const void *obj, hb_ot_apply_context_t *c)\n  {\n    const Type *typed_obj = (const Type *) obj;\n    return typed_obj->apply (c);\n  }\n\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n  template <typename T>\n  static inline auto apply_cached_ (const T *obj, hb_ot_apply_context_t *c, hb_priority<1>) HB_RETURN (bool, obj->apply (c, true) )\n  template <typename T>\n  static inline auto apply_cached_ (const T *obj, hb_ot_apply_context_t *c, hb_priority<0>) HB_RETURN (bool, obj->apply (c) )\n  template <typename Type>\n  static inline bool apply_cached_to (const void *obj, hb_ot_apply_context_t *c)\n  {\n    const Type *typed_obj = (const Type *) obj;\n    return apply_cached_ (typed_obj, c, hb_prioritize);\n  }\n\n  template <typename T>\n  static inline auto cache_func_ (const T *obj, hb_ot_apply_context_t *c, bool enter, hb_priority<1>) HB_RETURN (bool, obj->cache_func (c, enter) )\n  template <typename T>\n  static inline bool cache_func_ (const T *obj, hb_ot_apply_context_t *c, bool enter, hb_priority<0>) { return false; }\n  template <typename Type>\n  static inline bool cache_func_to (const void *obj, hb_ot_apply_context_t *c, bool enter)\n  {\n    const Type *typed_obj = (const Type *) obj;\n    return cache_func_ (typed_obj, c, enter, hb_prioritize);\n  }\n#endif\n\n  typedef bool (*hb_apply_func_t) (const void *obj, hb_ot_apply_context_t *c);\n  typedef bool (*hb_cache_func_t) (const void *obj, hb_ot_apply_context_t *c, bool enter);\n\n  struct hb_applicable_t\n  {\n    friend struct hb_accelerate_subtables_context_t;\n    friend struct hb_ot_layout_lookup_accelerator_t;\n\n    template <typename T>\n    void init (const T &obj_,\n\t       hb_apply_func_t apply_func_\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n\t       , hb_apply_func_t apply_cached_func_\n\t       , hb_cache_func_t cache_func_\n#endif\n\t\t)\n    {\n      obj = &obj_;\n      apply_func = apply_func_;\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n      apply_cached_func = apply_cached_func_;\n      cache_func = cache_func_;\n#endif\n      digest.init ();\n      obj_.get_coverage ().collect_coverage (&digest);\n    }\n\n    bool apply (hb_ot_apply_context_t *c) const\n    {\n      return digest.may_have (c->buffer->cur().codepoint) && apply_func (obj, c);\n    }\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n    bool apply_cached (hb_ot_apply_context_t *c) const\n    {\n      return digest.may_have (c->buffer->cur().codepoint) &&  apply_cached_func (obj, c);\n    }\n    bool cache_enter (hb_ot_apply_context_t *c) const\n    {\n      return cache_func (obj, c, true);\n    }\n    void cache_leave (hb_ot_apply_context_t *c) const\n    {\n      cache_func (obj, c, false);\n    }\n#endif\n\n    private:\n    const void *obj;\n    hb_apply_func_t apply_func;\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n    hb_apply_func_t apply_cached_func;\n    hb_cache_func_t cache_func;\n#endif\n    hb_set_digest_t digest;\n  };\n\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n  template <typename T>\n  auto cache_cost (const T &obj, hb_priority<1>) HB_AUTO_RETURN ( obj.cache_cost () )\n  template <typename T>\n  auto cache_cost (const T &obj, hb_priority<0>) HB_AUTO_RETURN ( 0u )\n#endif\n\n  /* Dispatch interface. */\n  template <typename T>\n  return_t dispatch (const T &obj)\n  {\n    hb_applicable_t *entry = &array[i++];\n\n    entry->init (obj,\n\t\t apply_to<T>\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n\t\t , apply_cached_to<T>\n\t\t , cache_func_to<T>\n#endif\n\t\t );\n\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n    /* Cache handling\n     *\n     * We allow one subtable from each lookup to use a cache. The assumption\n     * being that multiple subtables of the same lookup cannot use a cache\n     * because the resources they would use will collide.  As such, we ask\n     * each subtable to tell us how much it costs (which a cache would avoid),\n     * and we allocate the cache opportunity to the costliest subtable.\n     */\n    unsigned cost = cache_cost (obj, hb_prioritize);\n    if (cost > cache_user_cost)\n    {\n      cache_user_idx = i - 1;\n      cache_user_cost = cost;\n    }\n#endif\n\n    return hb_empty_t ();\n  }\n  static return_t default_return_value () { return hb_empty_t (); }\n\n  hb_accelerate_subtables_context_t (hb_applicable_t *array_) :\n\t\t\t\t     array (array_) {}\n\n  hb_applicable_t *array;\n  unsigned i = 0;\n\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n  unsigned cache_user_idx = (unsigned) -1;\n  unsigned cache_user_cost = 0;\n#endif\n};\n\n\ntypedef bool (*intersects_func_t) (const hb_set_t *glyphs, unsigned value, const void *data, void *cache);\ntypedef void (*intersected_glyphs_func_t) (const hb_set_t *glyphs, const void *data, unsigned value, hb_set_t *intersected_glyphs, void *cache);\ntypedef void (*collect_glyphs_func_t) (hb_set_t *glyphs, unsigned value, const void *data);\ntypedef bool (*match_func_t) (hb_glyph_info_t &info, unsigned value, const void *data);\n\nstruct ContextClosureFuncs\n{\n  intersects_func_t intersects;\n  intersected_glyphs_func_t intersected_glyphs;\n};\nstruct ContextCollectGlyphsFuncs\n{\n  collect_glyphs_func_t collect;\n};\nstruct ContextApplyFuncs\n{\n  match_func_t match;\n};\nstruct ChainContextApplyFuncs\n{\n  match_func_t match[3];\n};\n\n\nstatic inline bool intersects_glyph (const hb_set_t *glyphs, unsigned value, const void *data HB_UNUSED, void *cache HB_UNUSED)\n{\n  return glyphs->has (value);\n}\nstatic inline bool intersects_class (const hb_set_t *glyphs, unsigned value, const void *data, void *cache)\n{\n  const ClassDef &class_def = *reinterpret_cast<const ClassDef *>(data);\n  hb_map_t *map = (hb_map_t *) cache;\n\n  hb_codepoint_t *cached_v;\n  if (map->has (value, &cached_v))\n    return *cached_v;\n\n  bool v = class_def.intersects_class (glyphs, value);\n  map->set (value, v);\n\n  return v;\n}\nstatic inline bool intersects_coverage (const hb_set_t *glyphs, unsigned value, const void *data, void *cache HB_UNUSED)\n{\n  Offset16To<Coverage> coverage;\n  coverage = value;\n  return (data+coverage).intersects (glyphs);\n}\n\n\nstatic inline void intersected_glyph (const hb_set_t *glyphs HB_UNUSED, const void *data, unsigned value, hb_set_t *intersected_glyphs, HB_UNUSED void *cache)\n{\n  unsigned g = reinterpret_cast<const HBUINT16 *>(data)[value];\n  intersected_glyphs->add (g);\n}\n\nusing intersected_class_cache_t = hb_hashmap_t<unsigned, hb_set_t>;\n\nstatic inline void intersected_class_glyphs (const hb_set_t *glyphs, const void *data, unsigned value, hb_set_t *intersected_glyphs, void *cache)\n{\n  const ClassDef &class_def = *reinterpret_cast<const ClassDef *>(data);\n\n  intersected_class_cache_t *map = (intersected_class_cache_t *) cache;\n\n  hb_set_t *cached_v;\n  if (map->has (value, &cached_v))\n  {\n    intersected_glyphs->union_ (*cached_v);\n    return;\n  }\n\n  hb_set_t v;\n  class_def.intersected_class_glyphs (glyphs, value, &v);\n\n  intersected_glyphs->union_ (v);\n\n  map->set (value, std::move (v));\n}\n\nstatic inline void intersected_coverage_glyphs (const hb_set_t *glyphs, const void *data, unsigned value, hb_set_t *intersected_glyphs, HB_UNUSED void *cache)\n{\n  Offset16To<Coverage> coverage;\n  coverage = value;\n  (data+coverage).intersect_set (*glyphs, *intersected_glyphs);\n}\n\n\ntemplate <typename HBUINT>\nstatic inline bool array_is_subset_of (const hb_set_t *glyphs,\n\t\t\t\t       unsigned int count,\n\t\t\t\t       const HBUINT values[],\n\t\t\t\t       intersects_func_t intersects_func,\n\t\t\t\t       const void *intersects_data,\n\t\t\t\t       void *cache)\n{\n  for (const auto &_ : + hb_iter (values, count))\n    if (!intersects_func (glyphs, _, intersects_data, cache)) return false;\n  return true;\n}\n\n\nstatic inline void collect_glyph (hb_set_t *glyphs, unsigned value, const void *data HB_UNUSED)\n{\n  glyphs->add (value);\n}\nstatic inline void collect_class (hb_set_t *glyphs, unsigned value, const void *data)\n{\n  const ClassDef &class_def = *reinterpret_cast<const ClassDef *>(data);\n  class_def.collect_class (glyphs, value);\n}\nstatic inline void collect_coverage (hb_set_t *glyphs, unsigned value, const void *data)\n{\n  Offset16To<Coverage> coverage;\n  coverage = value;\n  (data+coverage).collect_coverage (glyphs);\n}\ntemplate <typename HBUINT>\nstatic inline void collect_array (hb_collect_glyphs_context_t *c HB_UNUSED,\n\t\t\t\t  hb_set_t *glyphs,\n\t\t\t\t  unsigned int count,\n\t\t\t\t  const HBUINT values[],\n\t\t\t\t  collect_glyphs_func_t collect_func,\n\t\t\t\t  const void *collect_data)\n{\n  return\n  + hb_iter (values, count)\n  | hb_apply ([&] (const HBUINT &_) { collect_func (glyphs, _, collect_data); })\n  ;\n}\n\n\nstatic inline bool match_glyph (hb_glyph_info_t &info, unsigned value, const void *data HB_UNUSED)\n{\n  return info.codepoint == value;\n}\nstatic inline bool match_class (hb_glyph_info_t &info, unsigned value, const void *data)\n{\n  const ClassDef &class_def = *reinterpret_cast<const ClassDef *>(data);\n  return class_def.get_class (info.codepoint) == value;\n}\nstatic inline bool match_class_cached (hb_glyph_info_t &info, unsigned value, const void *data)\n{\n  unsigned klass = info.syllable();\n  if (klass < 255)\n    return klass == value;\n  const ClassDef &class_def = *reinterpret_cast<const ClassDef *>(data);\n  klass = class_def.get_class (info.codepoint);\n  if (likely (klass < 255))\n    info.syllable() = klass;\n  return klass == value;\n}\nstatic inline bool match_coverage (hb_glyph_info_t &info, unsigned value, const void *data)\n{\n  Offset16To<Coverage> coverage;\n  coverage = value;\n  return (data+coverage).get_coverage (info.codepoint) != NOT_COVERED;\n}\n\ntemplate <typename HBUINT>\nstatic inline bool would_match_input (hb_would_apply_context_t *c,\n\t\t\t\t      unsigned int count, /* Including the first glyph (not matched) */\n\t\t\t\t      const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t      match_func_t match_func,\n\t\t\t\t      const void *match_data)\n{\n  if (count != c->len)\n    return false;\n\n  for (unsigned int i = 1; i < count; i++)\n  {\n    hb_glyph_info_t info;\n    info.codepoint = c->glyphs[i];\n    if (likely (!match_func (info, input[i - 1], match_data)))\n      return false;\n  }\n\n  return true;\n}\ntemplate <typename HBUINT>\nstatic inline bool match_input (hb_ot_apply_context_t *c,\n\t\t\t\tunsigned int count, /* Including the first glyph (not matched) */\n\t\t\t\tconst HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\tmatch_func_t match_func,\n\t\t\t\tconst void *match_data,\n\t\t\t\tunsigned int *end_position,\n\t\t\t\tunsigned int match_positions[HB_MAX_CONTEXT_LENGTH],\n\t\t\t\tunsigned int *p_total_component_count = nullptr)\n{\n  TRACE_APPLY (nullptr);\n\n  if (unlikely (count > HB_MAX_CONTEXT_LENGTH)) return_trace (false);\n\n  hb_buffer_t *buffer = c->buffer;\n\n  hb_ot_apply_context_t::skipping_iterator_t &skippy_iter = c->iter_input;\n  skippy_iter.reset (buffer->idx, count - 1);\n  skippy_iter.set_match_func (match_func, match_data);\n  skippy_iter.set_glyph_data (input);\n\n  /*\n   * This is perhaps the trickiest part of OpenType...  Remarks:\n   *\n   * - If all components of the ligature were marks, we call this a mark ligature.\n   *\n   * - If there is no GDEF, and the ligature is NOT a mark ligature, we categorize\n   *   it as a ligature glyph.\n   *\n   * - Ligatures cannot be formed across glyphs attached to different components\n   *   of previous ligatures.  Eg. the sequence is LAM,SHADDA,LAM,FATHA,HEH, and\n   *   LAM,LAM,HEH form a ligature, leaving SHADDA,FATHA next to eachother.\n   *   However, it would be wrong to ligate that SHADDA,FATHA sequence.\n   *   There are a couple of exceptions to this:\n   *\n   *   o If a ligature tries ligating with marks that belong to it itself, go ahead,\n   *     assuming that the font designer knows what they are doing (otherwise it can\n   *     break Indic stuff when a matra wants to ligate with a conjunct,\n   *\n   *   o If two marks want to ligate and they belong to different components of the\n   *     same ligature glyph, and said ligature glyph is to be ignored according to\n   *     mark-filtering rules, then allow.\n   *     https://github.com/harfbuzz/harfbuzz/issues/545\n   */\n\n  unsigned int total_component_count = 0;\n  total_component_count += _hb_glyph_info_get_lig_num_comps (&buffer->cur());\n\n  unsigned int first_lig_id = _hb_glyph_info_get_lig_id (&buffer->cur());\n  unsigned int first_lig_comp = _hb_glyph_info_get_lig_comp (&buffer->cur());\n\n  enum {\n    LIGBASE_NOT_CHECKED,\n    LIGBASE_MAY_NOT_SKIP,\n    LIGBASE_MAY_SKIP\n  } ligbase = LIGBASE_NOT_CHECKED;\n\n  match_positions[0] = buffer->idx;\n  for (unsigned int i = 1; i < count; i++)\n  {\n    unsigned unsafe_to;\n    if (!skippy_iter.next (&unsafe_to))\n    {\n      *end_position = unsafe_to;\n      return_trace (false);\n    }\n\n    match_positions[i] = skippy_iter.idx;\n\n    unsigned int this_lig_id = _hb_glyph_info_get_lig_id (&buffer->info[skippy_iter.idx]);\n    unsigned int this_lig_comp = _hb_glyph_info_get_lig_comp (&buffer->info[skippy_iter.idx]);\n\n    if (first_lig_id && first_lig_comp)\n    {\n      /* If first component was attached to a previous ligature component,\n       * all subsequent components should be attached to the same ligature\n       * component, otherwise we shouldn't ligate them... */\n      if (first_lig_id != this_lig_id || first_lig_comp != this_lig_comp)\n      {\n\t/* ...unless, we are attached to a base ligature and that base\n\t * ligature is ignorable. */\n\tif (ligbase == LIGBASE_NOT_CHECKED)\n\t{\n\t  bool found = false;\n\t  const auto *out = buffer->out_info;\n\t  unsigned int j = buffer->out_len;\n\t  while (j && _hb_glyph_info_get_lig_id (&out[j - 1]) == first_lig_id)\n\t  {\n\t    if (_hb_glyph_info_get_lig_comp (&out[j - 1]) == 0)\n\t    {\n\t      j--;\n\t      found = true;\n\t      break;\n\t    }\n\t    j--;\n\t  }\n\n\t  if (found && skippy_iter.may_skip (out[j]) == hb_ot_apply_context_t::matcher_t::SKIP_YES)\n\t    ligbase = LIGBASE_MAY_SKIP;\n\t  else\n\t    ligbase = LIGBASE_MAY_NOT_SKIP;\n\t}\n\n\tif (ligbase == LIGBASE_MAY_NOT_SKIP)\n\t  return_trace (false);\n      }\n    }\n    else\n    {\n      /* If first component was NOT attached to a previous ligature component,\n       * all subsequent components should also NOT be attached to any ligature\n       * component, unless they are attached to the first component itself! */\n      if (this_lig_id && this_lig_comp && (this_lig_id != first_lig_id))\n\treturn_trace (false);\n    }\n\n    total_component_count += _hb_glyph_info_get_lig_num_comps (&buffer->info[skippy_iter.idx]);\n  }\n\n  *end_position = skippy_iter.idx + 1;\n\n  if (p_total_component_count)\n    *p_total_component_count = total_component_count;\n\n  return_trace (true);\n}\nstatic inline bool ligate_input (hb_ot_apply_context_t *c,\n\t\t\t\t unsigned int count, /* Including the first glyph */\n\t\t\t\t const unsigned int match_positions[HB_MAX_CONTEXT_LENGTH], /* Including the first glyph */\n\t\t\t\t unsigned int match_end,\n\t\t\t\t hb_codepoint_t lig_glyph,\n\t\t\t\t unsigned int total_component_count)\n{\n  TRACE_APPLY (nullptr);\n\n  hb_buffer_t *buffer = c->buffer;\n\n  buffer->merge_clusters (buffer->idx, match_end);\n\n  /* - If a base and one or more marks ligate, consider that as a base, NOT\n   *   ligature, such that all following marks can still attach to it.\n   *   https://github.com/harfbuzz/harfbuzz/issues/1109\n   *\n   * - If all components of the ligature were marks, we call this a mark ligature.\n   *   If it *is* a mark ligature, we don't allocate a new ligature id, and leave\n   *   the ligature to keep its old ligature id.  This will allow it to attach to\n   *   a base ligature in GPOS.  Eg. if the sequence is: LAM,LAM,SHADDA,FATHA,HEH,\n   *   and LAM,LAM,HEH for a ligature, they will leave SHADDA and FATHA with a\n   *   ligature id and component value of 2.  Then if SHADDA,FATHA form a ligature\n   *   later, we don't want them to lose their ligature id/component, otherwise\n   *   GPOS will fail to correctly position the mark ligature on top of the\n   *   LAM,LAM,HEH ligature.  See:\n   *     https://bugzilla.gnome.org/show_bug.cgi?id=676343\n   *\n   * - If a ligature is formed of components that some of which are also ligatures\n   *   themselves, and those ligature components had marks attached to *their*\n   *   components, we have to attach the marks to the new ligature component\n   *   positions!  Now *that*'s tricky!  And these marks may be following the\n   *   last component of the whole sequence, so we should loop forward looking\n   *   for them and update them.\n   *\n   *   Eg. the sequence is LAM,LAM,SHADDA,FATHA,HEH, and the font first forms a\n   *   'calt' ligature of LAM,HEH, leaving the SHADDA and FATHA with a ligature\n   *   id and component == 1.  Now, during 'liga', the LAM and the LAM-HEH ligature\n   *   form a LAM-LAM-HEH ligature.  We need to reassign the SHADDA and FATHA to\n   *   the new ligature with a component value of 2.\n   *\n   *   This in fact happened to a font...  See:\n   *   https://bugzilla.gnome.org/show_bug.cgi?id=437633\n   */\n\n  bool is_base_ligature = _hb_glyph_info_is_base_glyph (&buffer->info[match_positions[0]]);\n  bool is_mark_ligature = _hb_glyph_info_is_mark (&buffer->info[match_positions[0]]);\n  for (unsigned int i = 1; i < count; i++)\n    if (!_hb_glyph_info_is_mark (&buffer->info[match_positions[i]]))\n    {\n      is_base_ligature = false;\n      is_mark_ligature = false;\n      break;\n    }\n  bool is_ligature = !is_base_ligature && !is_mark_ligature;\n\n  unsigned int klass = is_ligature ? HB_OT_LAYOUT_GLYPH_PROPS_LIGATURE : 0;\n  unsigned int lig_id = is_ligature ? _hb_allocate_lig_id (buffer) : 0;\n  unsigned int last_lig_id = _hb_glyph_info_get_lig_id (&buffer->cur());\n  unsigned int last_num_components = _hb_glyph_info_get_lig_num_comps (&buffer->cur());\n  unsigned int components_so_far = last_num_components;\n\n  if (is_ligature)\n  {\n    _hb_glyph_info_set_lig_props_for_ligature (&buffer->cur(), lig_id, total_component_count);\n    if (_hb_glyph_info_get_general_category (&buffer->cur()) == HB_UNICODE_GENERAL_CATEGORY_NON_SPACING_MARK)\n    {\n      _hb_glyph_info_set_general_category (&buffer->cur(), HB_UNICODE_GENERAL_CATEGORY_OTHER_LETTER);\n    }\n  }\n  c->replace_glyph_with_ligature (lig_glyph, klass);\n\n  for (unsigned int i = 1; i < count; i++)\n  {\n    while (buffer->idx < match_positions[i] && buffer->successful)\n    {\n      if (is_ligature)\n      {\n\tunsigned int this_comp = _hb_glyph_info_get_lig_comp (&buffer->cur());\n\tif (this_comp == 0)\n\t  this_comp = last_num_components;\n\tunsigned int new_lig_comp = components_so_far - last_num_components +\n\t\t\t\t    hb_min (this_comp, last_num_components);\n\t  _hb_glyph_info_set_lig_props_for_mark (&buffer->cur(), lig_id, new_lig_comp);\n      }\n      (void) buffer->next_glyph ();\n    }\n\n    last_lig_id = _hb_glyph_info_get_lig_id (&buffer->cur());\n    last_num_components = _hb_glyph_info_get_lig_num_comps (&buffer->cur());\n    components_so_far += last_num_components;\n\n    /* Skip the base glyph */\n    buffer->idx++;\n  }\n\n  if (!is_mark_ligature && last_lig_id)\n  {\n    /* Re-adjust components for any marks following. */\n    for (unsigned i = buffer->idx; i < buffer->len; ++i)\n    {\n      if (last_lig_id != _hb_glyph_info_get_lig_id (&buffer->info[i])) break;\n\n      unsigned this_comp = _hb_glyph_info_get_lig_comp (&buffer->info[i]);\n      if (!this_comp) break;\n\n      unsigned new_lig_comp = components_so_far - last_num_components +\n\t\t\t      hb_min (this_comp, last_num_components);\n      _hb_glyph_info_set_lig_props_for_mark (&buffer->info[i], lig_id, new_lig_comp);\n    }\n  }\n  return_trace (true);\n}\n\ntemplate <typename HBUINT>\nstatic inline bool match_backtrack (hb_ot_apply_context_t *c,\n\t\t\t\t    unsigned int count,\n\t\t\t\t    const HBUINT backtrack[],\n\t\t\t\t    match_func_t match_func,\n\t\t\t\t    const void *match_data,\n\t\t\t\t    unsigned int *match_start)\n{\n  TRACE_APPLY (nullptr);\n\n  hb_ot_apply_context_t::skipping_iterator_t &skippy_iter = c->iter_context;\n  skippy_iter.reset (c->buffer->backtrack_len (), count);\n  skippy_iter.set_match_func (match_func, match_data);\n  skippy_iter.set_glyph_data (backtrack);\n\n  for (unsigned int i = 0; i < count; i++)\n  {\n    unsigned unsafe_from;\n    if (!skippy_iter.prev (&unsafe_from))\n    {\n      *match_start = unsafe_from;\n      return_trace (false);\n    }\n  }\n\n  *match_start = skippy_iter.idx;\n  return_trace (true);\n}\n\ntemplate <typename HBUINT>\nstatic inline bool match_lookahead (hb_ot_apply_context_t *c,\n\t\t\t\t    unsigned int count,\n\t\t\t\t    const HBUINT lookahead[],\n\t\t\t\t    match_func_t match_func,\n\t\t\t\t    const void *match_data,\n\t\t\t\t    unsigned int start_index,\n\t\t\t\t    unsigned int *end_index)\n{\n  TRACE_APPLY (nullptr);\n\n  hb_ot_apply_context_t::skipping_iterator_t &skippy_iter = c->iter_context;\n  skippy_iter.reset (start_index - 1, count);\n  skippy_iter.set_match_func (match_func, match_data);\n  skippy_iter.set_glyph_data (lookahead);\n\n  for (unsigned int i = 0; i < count; i++)\n  {\n    unsigned unsafe_to;\n    if (!skippy_iter.next (&unsafe_to))\n    {\n      *end_index = unsafe_to;\n      return_trace (false);\n    }\n  }\n\n  *end_index = skippy_iter.idx + 1;\n  return_trace (true);\n}\n\n\n\nstruct LookupRecord\n{\n  bool serialize (hb_serialize_context_t *c,\n\t\t  const hb_map_t         *lookup_map) const\n  {\n    TRACE_SERIALIZE (this);\n    auto *out = c->embed (*this);\n    if (unlikely (!out)) return_trace (false);\n\n    return_trace (c->check_assign (out->lookupListIndex, lookup_map->get (lookupListIndex), HB_SERIALIZE_ERROR_INT_OVERFLOW));\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    return_trace (c->check_struct (this));\n  }\n\n  HBUINT16\tsequenceIndex;\t\t/* Index into current glyph\n\t\t\t\t\t * sequence--first glyph = 0 */\n  HBUINT16\tlookupListIndex;\t/* Lookup to apply to that\n\t\t\t\t\t * position--zero--based */\n  public:\n  DEFINE_SIZE_STATIC (4);\n};\n\nstatic unsigned serialize_lookuprecord_array (hb_serialize_context_t *c,\n\t\t\t\t\t      const hb_array_t<const LookupRecord> lookupRecords,\n\t\t\t\t\t      const hb_map_t *lookup_map)\n{\n  unsigned count = 0;\n  for (const LookupRecord& r : lookupRecords)\n  {\n    if (!lookup_map->has (r.lookupListIndex))\n      continue;\n\n    if (!r.serialize (c, lookup_map))\n      return 0;\n\n    count++;\n  }\n  return count;\n}\n\nenum ContextFormat { SimpleContext = 1, ClassBasedContext = 2, CoverageBasedContext = 3 };\n\ntemplate <typename HBUINT>\nstatic void context_closure_recurse_lookups (hb_closure_context_t *c,\n\t\t\t\t\t     unsigned inputCount, const HBUINT input[],\n\t\t\t\t\t     unsigned lookupCount,\n\t\t\t\t\t     const LookupRecord lookupRecord[] /* Array of LookupRecords--in design order */,\n\t\t\t\t\t     unsigned value,\n\t\t\t\t\t     ContextFormat context_format,\n\t\t\t\t\t     const void *data,\n\t\t\t\t\t     intersected_glyphs_func_t intersected_glyphs_func,\n\t\t\t\t\t     void *cache)\n{\n  hb_set_t covered_seq_indicies;\n  hb_set_t pos_glyphs;\n  for (unsigned int i = 0; i < lookupCount; i++)\n  {\n    unsigned seqIndex = lookupRecord[i].sequenceIndex;\n    if (seqIndex >= inputCount) continue;\n\n    bool has_pos_glyphs = false;\n\n    if (!covered_seq_indicies.has (seqIndex))\n    {\n      has_pos_glyphs = true;\n      pos_glyphs.clear ();\n      if (seqIndex == 0)\n      {\n        switch (context_format) {\n        case ContextFormat::SimpleContext:\n          pos_glyphs.add (value);\n          break;\n        case ContextFormat::ClassBasedContext:\n          intersected_glyphs_func (&c->parent_active_glyphs (), data, value, &pos_glyphs, cache);\n          break;\n        case ContextFormat::CoverageBasedContext:\n          pos_glyphs.set (c->parent_active_glyphs ());\n          break;\n        }\n      }\n      else\n      {\n        const void *input_data = input;\n        unsigned input_value = seqIndex - 1;\n        if (context_format != ContextFormat::SimpleContext)\n        {\n          input_data = data;\n          input_value = input[seqIndex - 1];\n        }\n\n        intersected_glyphs_func (c->glyphs, input_data, input_value, &pos_glyphs, cache);\n      }\n    }\n\n    covered_seq_indicies.add (seqIndex);\n    if (has_pos_glyphs) {\n      c->push_cur_active_glyphs () = std::move (pos_glyphs);\n    } else {\n      c->push_cur_active_glyphs ().set (*c->glyphs);\n    }\n\n    unsigned endIndex = inputCount;\n    if (context_format == ContextFormat::CoverageBasedContext)\n      endIndex += 1;\n\n    c->recurse (lookupRecord[i].lookupListIndex, &covered_seq_indicies, seqIndex, endIndex);\n\n    c->pop_cur_done_glyphs ();\n  }\n}\n\ntemplate <typename context_t>\nstatic inline void recurse_lookups (context_t *c,\n                                    unsigned int lookupCount,\n                                    const LookupRecord lookupRecord[] /* Array of LookupRecords--in design order */)\n{\n  for (unsigned int i = 0; i < lookupCount; i++)\n    c->recurse (lookupRecord[i].lookupListIndex);\n}\n\nstatic inline void apply_lookup (hb_ot_apply_context_t *c,\n\t\t\t\t unsigned int count, /* Including the first glyph */\n\t\t\t\t unsigned int match_positions[HB_MAX_CONTEXT_LENGTH], /* Including the first glyph */\n\t\t\t\t unsigned int lookupCount,\n\t\t\t\t const LookupRecord lookupRecord[], /* Array of LookupRecords--in design order */\n\t\t\t\t unsigned int match_end)\n{\n  hb_buffer_t *buffer = c->buffer;\n  int end;\n\n  /* All positions are distance from beginning of *output* buffer.\n   * Adjust. */\n  {\n    unsigned int bl = buffer->backtrack_len ();\n    end = bl + match_end - buffer->idx;\n\n    int delta = bl - buffer->idx;\n    /* Convert positions to new indexing. */\n    for (unsigned int j = 0; j < count; j++)\n      match_positions[j] += delta;\n  }\n\n  for (unsigned int i = 0; i < lookupCount && buffer->successful; i++)\n  {\n    unsigned int idx = lookupRecord[i].sequenceIndex;\n    if (idx >= count)\n      continue;\n\n    unsigned int orig_len = buffer->backtrack_len () + buffer->lookahead_len ();\n\n    /* This can happen if earlier recursed lookups deleted many entries. */\n    if (unlikely (match_positions[idx] >= orig_len))\n      continue;\n\n    if (unlikely (!buffer->move_to (match_positions[idx])))\n      break;\n\n    if (unlikely (buffer->max_ops <= 0))\n      break;\n\n    if (HB_BUFFER_MESSAGE_MORE && c->buffer->messaging ())\n    {\n      if (buffer->have_output)\n        c->buffer->sync_so_far ();\n      c->buffer->message (c->font,\n\t\t\t  \"recursing to lookup %u at %u\",\n\t\t\t  (unsigned) lookupRecord[i].lookupListIndex,\n\t\t\t  buffer->idx);\n    }\n\n    if (!c->recurse (lookupRecord[i].lookupListIndex))\n      continue;\n\n    if (HB_BUFFER_MESSAGE_MORE && c->buffer->messaging ())\n    {\n      if (buffer->have_output)\n        c->buffer->sync_so_far ();\n      c->buffer->message (c->font,\n\t\t\t  \"recursed to lookup %u\",\n\t\t\t  (unsigned) lookupRecord[i].lookupListIndex);\n    }\n\n    unsigned int new_len = buffer->backtrack_len () + buffer->lookahead_len ();\n    int delta = new_len - orig_len;\n\n    if (!delta)\n      continue;\n\n    /* Recursed lookup changed buffer len.  Adjust.\n     *\n     * TODO:\n     *\n     * Right now, if buffer length increased by n, we assume n new glyphs\n     * were added right after the current position, and if buffer length\n     * was decreased by n, we assume n match positions after the current\n     * one where removed.  The former (buffer length increased) case is\n     * fine, but the decrease case can be improved in at least two ways,\n     * both of which are significant:\n     *\n     *   - If recursed-to lookup is MultipleSubst and buffer length\n     *     decreased, then it's current match position that was deleted,\n     *     NOT the one after it.\n     *\n     *   - If buffer length was decreased by n, it does not necessarily\n     *     mean that n match positions where removed, as there recursed-to\n     *     lookup might had a different LookupFlag.  Here's a constructed\n     *     case of that:\n     *     https://github.com/harfbuzz/harfbuzz/discussions/3538\n     *\n     * It should be possible to construct tests for both of these cases.\n     */\n\n    end += delta;\n    if (end < int (match_positions[idx]))\n    {\n      /* End might end up being smaller than match_positions[idx] if the recursed\n       * lookup ended up removing many items.\n       * Just never rewind end beyond start of current position, since that is\n       * not possible in the recursed lookup.  Also adjust delta as such.\n       *\n       * https://bugs.chromium.org/p/chromium/issues/detail?id=659496\n       * https://github.com/harfbuzz/harfbuzz/issues/1611\n       */\n      delta += match_positions[idx] - end;\n      end = match_positions[idx];\n    }\n\n    unsigned int next = idx + 1; /* next now is the position after the recursed lookup. */\n\n    if (delta > 0)\n    {\n      if (unlikely (delta + count > HB_MAX_CONTEXT_LENGTH))\n\tbreak;\n    }\n    else\n    {\n      /* NOTE: delta is non-positive. */\n      delta = hb_max (delta, (int) next - (int) count);\n      next -= delta;\n    }\n\n    /* Shift! */\n    memmove (match_positions + next + delta, match_positions + next,\n\t     (count - next) * sizeof (match_positions[0]));\n    next += delta;\n    count += delta;\n\n    /* Fill in new entries. */\n    for (unsigned int j = idx + 1; j < next; j++)\n      match_positions[j] = match_positions[j - 1] + 1;\n\n    /* And fixup the rest. */\n    for (; next < count; next++)\n      match_positions[next] += delta;\n  }\n\n  (void) buffer->move_to (end);\n}\n\n\n\n/* Contextual lookups */\n\nstruct ContextClosureLookupContext\n{\n  ContextClosureFuncs funcs;\n  ContextFormat context_format;\n  const void *intersects_data;\n  void *intersects_cache;\n  void *intersected_glyphs_cache;\n};\n\nstruct ContextCollectGlyphsLookupContext\n{\n  ContextCollectGlyphsFuncs funcs;\n  const void *collect_data;\n};\n\nstruct ContextApplyLookupContext\n{\n  ContextApplyFuncs funcs;\n  const void *match_data;\n};\n\ntemplate <typename HBUINT>\nstatic inline bool context_intersects (const hb_set_t *glyphs,\n\t\t\t\t       unsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t       const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t       ContextClosureLookupContext &lookup_context)\n{\n  return array_is_subset_of (glyphs,\n\t\t\t     inputCount ? inputCount - 1 : 0, input,\n\t\t\t     lookup_context.funcs.intersects,\n\t\t\t     lookup_context.intersects_data,\n\t\t\t     lookup_context.intersects_cache);\n}\n\ntemplate <typename HBUINT>\nstatic inline void context_closure_lookup (hb_closure_context_t *c,\n\t\t\t\t\t   unsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t\t   const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t\t   unsigned int lookupCount,\n\t\t\t\t\t   const LookupRecord lookupRecord[],\n\t\t\t\t\t   unsigned value, /* Index of first glyph in Coverage or Class value in ClassDef table */\n\t\t\t\t\t   ContextClosureLookupContext &lookup_context)\n{\n  if (context_intersects (c->glyphs,\n\t\t\t  inputCount, input,\n\t\t\t  lookup_context))\n    context_closure_recurse_lookups (c,\n\t\t\t\t     inputCount, input,\n\t\t\t\t     lookupCount, lookupRecord,\n\t\t\t\t     value,\n\t\t\t\t     lookup_context.context_format,\n\t\t\t\t     lookup_context.intersects_data,\n\t\t\t\t     lookup_context.funcs.intersected_glyphs,\n\t\t\t\t     lookup_context.intersected_glyphs_cache);\n}\n\ntemplate <typename HBUINT>\nstatic inline void context_collect_glyphs_lookup (hb_collect_glyphs_context_t *c,\n\t\t\t\t\t\t  unsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t\t\t  const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t\t\t  unsigned int lookupCount,\n\t\t\t\t\t\t  const LookupRecord lookupRecord[],\n\t\t\t\t\t\t  ContextCollectGlyphsLookupContext &lookup_context)\n{\n  collect_array (c, c->input,\n\t\t inputCount ? inputCount - 1 : 0, input,\n\t\t lookup_context.funcs.collect, lookup_context.collect_data);\n  recurse_lookups (c,\n\t\t   lookupCount, lookupRecord);\n}\n\ntemplate <typename HBUINT>\nstatic inline bool context_would_apply_lookup (hb_would_apply_context_t *c,\n\t\t\t\t\t       unsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t\t       const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t\t       unsigned int lookupCount HB_UNUSED,\n\t\t\t\t\t       const LookupRecord lookupRecord[] HB_UNUSED,\n\t\t\t\t\t       const ContextApplyLookupContext &lookup_context)\n{\n  return would_match_input (c,\n\t\t\t    inputCount, input,\n\t\t\t    lookup_context.funcs.match, lookup_context.match_data);\n}\n\ntemplate <typename HBUINT>\nstatic inline bool context_apply_lookup (hb_ot_apply_context_t *c,\n\t\t\t\t\t unsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t\t const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t\t unsigned int lookupCount,\n\t\t\t\t\t const LookupRecord lookupRecord[],\n\t\t\t\t\t const ContextApplyLookupContext &lookup_context)\n{\n  unsigned match_end = 0;\n  unsigned match_positions[HB_MAX_CONTEXT_LENGTH];\n  if (match_input (c,\n\t\t   inputCount, input,\n\t\t   lookup_context.funcs.match, lookup_context.match_data,\n\t\t   &match_end, match_positions))\n  {\n    c->buffer->unsafe_to_break (c->buffer->idx, match_end);\n    apply_lookup (c,\n\t\t  inputCount, match_positions,\n\t\t  lookupCount, lookupRecord,\n\t\t  match_end);\n    return true;\n  }\n  else\n  {\n    c->buffer->unsafe_to_concat (c->buffer->idx, match_end);\n    return false;\n  }\n}\n\ntemplate <typename Types>\nstruct Rule\n{\n  bool intersects (const hb_set_t *glyphs, ContextClosureLookupContext &lookup_context) const\n  {\n    return context_intersects (glyphs,\n\t\t\t       inputCount, inputZ.arrayZ,\n\t\t\t       lookup_context);\n  }\n\n  void closure (hb_closure_context_t *c, unsigned value, ContextClosureLookupContext &lookup_context) const\n  {\n    if (unlikely (c->lookup_limit_exceeded ())) return;\n\n    const auto &lookupRecord = StructAfter<UnsizedArrayOf<LookupRecord>>\n\t\t\t\t\t   (inputZ.as_array ((inputCount ? inputCount - 1 : 0)));\n    context_closure_lookup (c,\n\t\t\t    inputCount, inputZ.arrayZ,\n\t\t\t    lookupCount, lookupRecord.arrayZ,\n\t\t\t    value, lookup_context);\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c,\n                        ContextClosureLookupContext &lookup_context) const\n  {\n    if (unlikely (c->lookup_limit_exceeded ())) return;\n    if (!intersects (c->glyphs, lookup_context)) return;\n\n    const auto &lookupRecord = StructAfter<UnsizedArrayOf<LookupRecord>>\n\t\t\t\t\t   (inputZ.as_array (inputCount ? inputCount - 1 : 0));\n    recurse_lookups (c, lookupCount, lookupRecord.arrayZ);\n  }\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c,\n\t\t       ContextCollectGlyphsLookupContext &lookup_context) const\n  {\n    const auto &lookupRecord = StructAfter<UnsizedArrayOf<LookupRecord>>\n\t\t\t\t\t   (inputZ.as_array (inputCount ? inputCount - 1 : 0));\n    context_collect_glyphs_lookup (c,\n\t\t\t\t   inputCount, inputZ.arrayZ,\n\t\t\t\t   lookupCount, lookupRecord.arrayZ,\n\t\t\t\t   lookup_context);\n  }\n\n  bool would_apply (hb_would_apply_context_t *c,\n\t\t    const ContextApplyLookupContext &lookup_context) const\n  {\n    const auto &lookupRecord = StructAfter<UnsizedArrayOf<LookupRecord>>\n\t\t\t\t\t   (inputZ.as_array (inputCount ? inputCount - 1 : 0));\n    return context_would_apply_lookup (c,\n\t\t\t\t       inputCount, inputZ.arrayZ,\n\t\t\t\t       lookupCount, lookupRecord.arrayZ,\n\t\t\t\t       lookup_context);\n  }\n\n  bool apply (hb_ot_apply_context_t *c,\n\t      const ContextApplyLookupContext &lookup_context) const\n  {\n    TRACE_APPLY (this);\n    const auto &lookupRecord = StructAfter<UnsizedArrayOf<LookupRecord>>\n\t\t\t\t\t   (inputZ.as_array (inputCount ? inputCount - 1 : 0));\n    return_trace (context_apply_lookup (c, inputCount, inputZ.arrayZ, lookupCount, lookupRecord.arrayZ, lookup_context));\n  }\n\n  bool serialize (hb_serialize_context_t *c,\n\t\t  const hb_map_t *input_mapping, /* old->new glyphid or class mapping */\n\t\t  const hb_map_t *lookup_map) const\n  {\n    TRACE_SERIALIZE (this);\n    auto *out = c->start_embed (this);\n    if (unlikely (!c->extend_min (out))) return_trace (false);\n\n    out->inputCount = inputCount;\n    const auto input = inputZ.as_array (inputCount - 1);\n    for (const auto org : input)\n    {\n      HBUINT16 d;\n      d = input_mapping->get (org);\n      c->copy (d);\n    }\n\n    const auto &lookupRecord = StructAfter<UnsizedArrayOf<LookupRecord>>\n\t\t\t\t\t   (inputZ.as_array ((inputCount ? inputCount - 1 : 0)));\n\n    unsigned count = serialize_lookuprecord_array (c, lookupRecord.as_array (lookupCount), lookup_map);\n    return_trace (c->check_assign (out->lookupCount, count, HB_SERIALIZE_ERROR_INT_OVERFLOW));\n  }\n\n  bool subset (hb_subset_context_t *c,\n\t       const hb_map_t *lookup_map,\n\t       const hb_map_t *klass_map = nullptr) const\n  {\n    TRACE_SUBSET (this);\n    if (unlikely (!inputCount)) return_trace (false);\n    const auto input = inputZ.as_array (inputCount - 1);\n\n    const hb_map_t *mapping = klass_map == nullptr ? c->plan->glyph_map : klass_map;\n    if (!hb_all (input, mapping)) return_trace (false);\n    return_trace (serialize (c->serializer, mapping, lookup_map));\n  }\n\n  public:\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    return_trace (inputCount.sanitize (c) &&\n\t\t  lookupCount.sanitize (c) &&\n\t\t  c->check_range (inputZ.arrayZ,\n\t\t\t\t  inputZ.item_size * (inputCount ? inputCount - 1 : 0) +\n\t\t\t\t  LookupRecord::static_size * lookupCount));\n  }\n\n  protected:\n  HBUINT16\tinputCount;\t\t/* Total number of glyphs in input\n\t\t\t\t\t * glyph sequence--includes the first\n\t\t\t\t\t * glyph */\n  HBUINT16\tlookupCount;\t\t/* Number of LookupRecords */\n  UnsizedArrayOf<typename Types::HBUINT>\n\t\tinputZ;\t\t\t/* Array of match inputs--start with\n\t\t\t\t\t * second glyph */\n/*UnsizedArrayOf<LookupRecord>\n\t\tlookupRecordX;*/\t/* Array of LookupRecords--in\n\t\t\t\t\t * design order */\n  public:\n  DEFINE_SIZE_ARRAY (4, inputZ);\n};\n\ntemplate <typename Types>\nstruct RuleSet\n{\n  using Rule = OT::Rule<Types>;\n\n  bool intersects (const hb_set_t *glyphs,\n\t\t   ContextClosureLookupContext &lookup_context) const\n  {\n    return\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_map ([&] (const Rule &_) { return _.intersects (glyphs, lookup_context); })\n    | hb_any\n    ;\n  }\n\n  void closure (hb_closure_context_t *c, unsigned value,\n\t\tContextClosureLookupContext &lookup_context) const\n  {\n    if (unlikely (c->lookup_limit_exceeded ())) return;\n\n    return\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const Rule &_) { _.closure (c, value, lookup_context); })\n    ;\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c,\n                        ContextClosureLookupContext &lookup_context) const\n  {\n    if (unlikely (c->lookup_limit_exceeded ())) return;\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const Rule &_) { _.closure_lookups (c, lookup_context); })\n    ;\n  }\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c,\n\t\t       ContextCollectGlyphsLookupContext &lookup_context) const\n  {\n    return\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const Rule &_) { _.collect_glyphs (c, lookup_context); })\n    ;\n  }\n\n  bool would_apply (hb_would_apply_context_t *c,\n\t\t    const ContextApplyLookupContext &lookup_context) const\n  {\n    return\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_map ([&] (const Rule &_) { return _.would_apply (c, lookup_context); })\n    | hb_any\n    ;\n  }\n\n  bool apply (hb_ot_apply_context_t *c,\n\t      const ContextApplyLookupContext &lookup_context) const\n  {\n    TRACE_APPLY (this);\n    return_trace (\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_map ([&] (const Rule &_) { return _.apply (c, lookup_context); })\n    | hb_any\n    )\n    ;\n  }\n\n  bool subset (hb_subset_context_t *c,\n\t       const hb_map_t *lookup_map,\n\t       const hb_map_t *klass_map = nullptr) const\n  {\n    TRACE_SUBSET (this);\n\n    auto snap = c->serializer->snapshot ();\n    auto *out = c->serializer->start_embed (*this);\n    if (unlikely (!c->serializer->extend_min (out))) return_trace (false);\n\n    for (const Offset16To<Rule>& _ : rule)\n    {\n      if (!_) continue;\n      auto o_snap = c->serializer->snapshot ();\n      auto *o = out->rule.serialize_append (c->serializer);\n      if (unlikely (!o)) continue;\n\n      if (!o->serialize_subset (c, _, this, lookup_map, klass_map))\n      {\n\tout->rule.pop ();\n\tc->serializer->revert (o_snap);\n      }\n    }\n\n    bool ret = bool (out->rule);\n    if (!ret) c->serializer->revert (snap);\n\n    return_trace (ret);\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    return_trace (rule.sanitize (c, this));\n  }\n\n  protected:\n  Array16OfOffset16To<Rule>\n\t\trule;\t\t\t/* Array of Rule tables\n\t\t\t\t\t * ordered by preference */\n  public:\n  DEFINE_SIZE_ARRAY (2, rule);\n};\n\n\ntemplate <typename Types>\nstruct ContextFormat1_4\n{\n  using RuleSet = OT::RuleSet<Types>;\n\n  bool intersects (const hb_set_t *glyphs) const\n  {\n    struct ContextClosureLookupContext lookup_context = {\n      {intersects_glyph, intersected_glyph},\n      ContextFormat::SimpleContext,\n      nullptr\n    };\n\n    return\n    + hb_zip (this+coverage, ruleSet)\n    | hb_filter (*glyphs, hb_first)\n    | hb_map (hb_second)\n    | hb_map (hb_add (this))\n    | hb_map ([&] (const RuleSet &_) { return _.intersects (glyphs, lookup_context); })\n    | hb_any\n    ;\n  }\n\n  bool may_have_non_1to1 () const\n  { return true; }\n\n  void closure (hb_closure_context_t *c) const\n  {\n    hb_set_t& cur_active_glyphs = c->push_cur_active_glyphs ();\n    get_coverage ().intersect_set (c->previous_parent_active_glyphs (), cur_active_glyphs);\n\n    struct ContextClosureLookupContext lookup_context = {\n      {intersects_glyph, intersected_glyph},\n      ContextFormat::SimpleContext,\n      nullptr\n    };\n\n    + hb_zip (this+coverage, hb_range ((unsigned) ruleSet.len))\n    | hb_filter ([&] (hb_codepoint_t _) {\n      return c->previous_parent_active_glyphs ().has (_);\n    }, hb_first)\n    | hb_map ([&](const hb_pair_t<hb_codepoint_t, unsigned> _) { return hb_pair_t<unsigned, const RuleSet&> (_.first, this+ruleSet[_.second]); })\n    | hb_apply ([&] (const hb_pair_t<unsigned, const RuleSet&>& _) { _.second.closure (c, _.first, lookup_context); })\n    ;\n\n    c->pop_cur_done_glyphs ();\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c) const\n  {\n    struct ContextClosureLookupContext lookup_context = {\n      {intersects_glyph, nullptr},\n      ContextFormat::SimpleContext,\n      nullptr\n    };\n\n    + hb_zip (this+coverage, ruleSet)\n    | hb_filter (*c->glyphs, hb_first)\n    | hb_map (hb_second)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const RuleSet &_) { _.closure_lookups (c, lookup_context); })\n    ;\n  }\n\n  void collect_variation_indices (hb_collect_variation_indices_context_t *c) const {}\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c) const\n  {\n    (this+coverage).collect_coverage (c->input);\n\n    struct ContextCollectGlyphsLookupContext lookup_context = {\n      {collect_glyph},\n      nullptr\n    };\n\n    + hb_iter (ruleSet)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const RuleSet &_) { _.collect_glyphs (c, lookup_context); })\n    ;\n  }\n\n  bool would_apply (hb_would_apply_context_t *c) const\n  {\n    const RuleSet &rule_set = this+ruleSet[(this+coverage).get_coverage (c->glyphs[0])];\n    struct ContextApplyLookupContext lookup_context = {\n      {match_glyph},\n      nullptr\n    };\n    return rule_set.would_apply (c, lookup_context);\n  }\n\n  const Coverage &get_coverage () const { return this+coverage; }\n\n  bool apply (hb_ot_apply_context_t *c) const\n  {\n    TRACE_APPLY (this);\n    unsigned int index = (this+coverage).get_coverage (c->buffer->cur().codepoint);\n    if (likely (index == NOT_COVERED))\n      return_trace (false);\n\n    const RuleSet &rule_set = this+ruleSet[index];\n    struct ContextApplyLookupContext lookup_context = {\n      {match_glyph},\n      nullptr\n    };\n    return_trace (rule_set.apply (c, lookup_context));\n  }\n\n  bool subset (hb_subset_context_t *c) const\n  {\n    TRACE_SUBSET (this);\n    const hb_set_t &glyphset = *c->plan->glyphset_gsub ();\n    const hb_map_t &glyph_map = *c->plan->glyph_map;\n\n    auto *out = c->serializer->start_embed (*this);\n    if (unlikely (!c->serializer->extend_min (out))) return_trace (false);\n    out->format = format;\n\n    const hb_map_t *lookup_map = c->table_tag == HB_OT_TAG_GSUB ? &c->plan->gsub_lookups : &c->plan->gpos_lookups;\n    hb_sorted_vector_t<hb_codepoint_t> new_coverage;\n    + hb_zip (this+coverage, ruleSet)\n    | hb_filter (glyphset, hb_first)\n    | hb_filter (subset_offset_array (c, out->ruleSet, this, lookup_map), hb_second)\n    | hb_map (hb_first)\n    | hb_map (glyph_map)\n    | hb_sink (new_coverage)\n    ;\n\n    out->coverage.serialize_serialize (c->serializer, new_coverage.iter ());\n    return_trace (bool (new_coverage));\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    return_trace (coverage.sanitize (c, this) && ruleSet.sanitize (c, this));\n  }\n\n  protected:\n  HBUINT16\tformat;\t\t\t/* Format identifier--format = 1 */\n  typename Types::template OffsetTo<Coverage>\n\t\tcoverage;\t\t/* Offset to Coverage table--from\n\t\t\t\t\t * beginning of table */\n  Array16Of<typename Types::template OffsetTo<RuleSet>>\n\t\truleSet;\t\t/* Array of RuleSet tables\n\t\t\t\t\t * ordered by Coverage Index */\n  public:\n  DEFINE_SIZE_ARRAY (2 + 2 * Types::size, ruleSet);\n};\n\n\ntemplate <typename Types>\nstruct ContextFormat2_5\n{\n  using RuleSet = OT::RuleSet<SmallTypes>;\n\n  bool intersects (const hb_set_t *glyphs) const\n  {\n    if (!(this+coverage).intersects (glyphs))\n      return false;\n\n    const ClassDef &class_def = this+classDef;\n\n    hb_map_t cache;\n    struct ContextClosureLookupContext lookup_context = {\n      {intersects_class, nullptr},\n      ContextFormat::ClassBasedContext,\n      &class_def,\n      &cache\n    };\n\n    hb_set_t retained_coverage_glyphs;\n    (this+coverage).intersect_set (*glyphs, retained_coverage_glyphs);\n\n    hb_set_t coverage_glyph_classes;\n    class_def.intersected_classes (&retained_coverage_glyphs, &coverage_glyph_classes);\n\n\n    return\n    + hb_iter (ruleSet)\n    | hb_map (hb_add (this))\n    | hb_enumerate\n    | hb_map ([&] (const hb_pair_t<unsigned, const RuleSet &> p)\n\t      { return class_def.intersects_class (glyphs, p.first) &&\n\t\t       coverage_glyph_classes.has (p.first) &&\n\t\t       p.second.intersects (glyphs, lookup_context); })\n    | hb_any\n    ;\n  }\n\n  bool may_have_non_1to1 () const\n  { return true; }\n\n  void closure (hb_closure_context_t *c) const\n  {\n    if (!(this+coverage).intersects (c->glyphs))\n      return;\n\n    hb_set_t& cur_active_glyphs = c->push_cur_active_glyphs ();\n    get_coverage ().intersect_set (c->previous_parent_active_glyphs (),\n                                                 cur_active_glyphs);\n\n    const ClassDef &class_def = this+classDef;\n\n    hb_map_t cache;\n    intersected_class_cache_t intersected_cache;\n    struct ContextClosureLookupContext lookup_context = {\n      {intersects_class, intersected_class_glyphs},\n      ContextFormat::ClassBasedContext,\n      &class_def,\n      &cache,\n      &intersected_cache\n    };\n\n    + hb_enumerate (ruleSet)\n    | hb_filter ([&] (unsigned _)\n    { return class_def.intersects_class (&c->parent_active_glyphs (), _); },\n\t\t hb_first)\n    | hb_apply ([&] (const hb_pair_t<unsigned, const typename Types::template OffsetTo<RuleSet>&> _)\n                {\n                  const RuleSet& rule_set = this+_.second;\n                  rule_set.closure (c, _.first, lookup_context);\n                })\n    ;\n\n    c->pop_cur_done_glyphs ();\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c) const\n  {\n    if (!(this+coverage).intersects (c->glyphs))\n      return;\n\n    const ClassDef &class_def = this+classDef;\n\n    hb_map_t cache;\n    struct ContextClosureLookupContext lookup_context = {\n      {intersects_class, nullptr},\n      ContextFormat::ClassBasedContext,\n      &class_def,\n      &cache\n    };\n\n    + hb_iter (ruleSet)\n    | hb_map (hb_add (this))\n    | hb_enumerate\n    | hb_filter ([&] (const hb_pair_t<unsigned, const RuleSet &> p)\n    { return class_def.intersects_class (c->glyphs, p.first); })\n    | hb_map (hb_second)\n    | hb_apply ([&] (const RuleSet & _)\n    { _.closure_lookups (c, lookup_context); });\n  }\n\n  void collect_variation_indices (hb_collect_variation_indices_context_t *c) const {}\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c) const\n  {\n    (this+coverage).collect_coverage (c->input);\n\n    const ClassDef &class_def = this+classDef;\n    struct ContextCollectGlyphsLookupContext lookup_context = {\n      {collect_class},\n      &class_def\n    };\n\n    + hb_iter (ruleSet)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const RuleSet &_) { _.collect_glyphs (c, lookup_context); })\n    ;\n  }\n\n  bool would_apply (hb_would_apply_context_t *c) const\n  {\n    const ClassDef &class_def = this+classDef;\n    unsigned int index = class_def.get_class (c->glyphs[0]);\n    const RuleSet &rule_set = this+ruleSet[index];\n    struct ContextApplyLookupContext lookup_context = {\n      {match_class},\n      &class_def\n    };\n    return rule_set.would_apply (c, lookup_context);\n  }\n\n  const Coverage &get_coverage () const { return this+coverage; }\n\n  unsigned cache_cost () const\n  {\n    unsigned c = (this+classDef).cost () * ruleSet.len;\n    return c >= 4 ? c : 0;\n  }\n  bool cache_func (hb_ot_apply_context_t *c, bool enter) const\n  {\n    if (enter)\n    {\n      if (!HB_BUFFER_TRY_ALLOCATE_VAR (c->buffer, syllable))\n\treturn false;\n      auto &info = c->buffer->info;\n      unsigned count = c->buffer->len;\n      for (unsigned i = 0; i < count; i++)\n\tinfo[i].syllable() = 255;\n      c->new_syllables = 255;\n      return true;\n    }\n    else\n    {\n      c->new_syllables = (unsigned) -1;\n      HB_BUFFER_DEALLOCATE_VAR (c->buffer, syllable);\n      return true;\n    }\n  }\n\n  bool apply (hb_ot_apply_context_t *c, bool cached = false) const\n  {\n    TRACE_APPLY (this);\n    unsigned int index = (this+coverage).get_coverage (c->buffer->cur().codepoint);\n    if (likely (index == NOT_COVERED)) return_trace (false);\n\n    const ClassDef &class_def = this+classDef;\n\n    struct ContextApplyLookupContext lookup_context = {\n      {cached ? match_class_cached : match_class},\n      &class_def\n    };\n\n    if (cached && c->buffer->cur().syllable() < 255)\n      index = c->buffer->cur().syllable ();\n    else\n    {\n      index = class_def.get_class (c->buffer->cur().codepoint);\n      if (cached && index < 255)\n\tc->buffer->cur().syllable() = index;\n    }\n    const RuleSet &rule_set = this+ruleSet[index];\n    return_trace (rule_set.apply (c, lookup_context));\n  }\n\n  bool subset (hb_subset_context_t *c) const\n  {\n    TRACE_SUBSET (this);\n    auto *out = c->serializer->start_embed (*this);\n    if (unlikely (!c->serializer->extend_min (out))) return_trace (false);\n    out->format = format;\n    if (unlikely (!out->coverage.serialize_subset (c, coverage, this)))\n      return_trace (false);\n\n    hb_map_t klass_map;\n    out->classDef.serialize_subset (c, classDef, this, &klass_map);\n\n    const hb_set_t* glyphset = c->plan->glyphset_gsub ();\n    hb_set_t retained_coverage_glyphs;\n    (this+coverage).intersect_set (*glyphset, retained_coverage_glyphs);\n\n    hb_set_t coverage_glyph_classes;\n    (this+classDef).intersected_classes (&retained_coverage_glyphs, &coverage_glyph_classes);\n\n    const hb_map_t *lookup_map = c->table_tag == HB_OT_TAG_GSUB ? &c->plan->gsub_lookups : &c->plan->gpos_lookups;\n    bool ret = true;\n    int non_zero_index = -1, index = 0;\n    auto snapshot = c->serializer->snapshot();\n    for (const auto& _ : + hb_enumerate (ruleSet)\n\t\t\t | hb_filter (klass_map, hb_first))\n    {\n      auto *o = out->ruleSet.serialize_append (c->serializer);\n      if (unlikely (!o))\n      {\n\tret = false;\n\tbreak;\n      }\n\n      if (coverage_glyph_classes.has (_.first) &&\n\t  o->serialize_subset (c, _.second, this, lookup_map, &klass_map)) {\n\tnon_zero_index = index;\n        snapshot = c->serializer->snapshot();\n      }\n\n      index++;\n    }\n\n    if (!ret || non_zero_index == -1) return_trace (false);\n\n    //prune empty trailing ruleSets\n    --index;\n    while (index > non_zero_index)\n    {\n      out->ruleSet.pop ();\n      index--;\n    }\n    c->serializer->revert (snapshot);\n\n    return_trace (bool (out->ruleSet));\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    return_trace (coverage.sanitize (c, this) && classDef.sanitize (c, this) && ruleSet.sanitize (c, this));\n  }\n\n  protected:\n  HBUINT16\tformat;\t\t\t/* Format identifier--format = 2 */\n  typename Types::template OffsetTo<Coverage>\n\t\tcoverage;\t\t/* Offset to Coverage table--from\n\t\t\t\t\t * beginning of table */\n  typename Types::template OffsetTo<ClassDef>\n\t\tclassDef;\t\t/* Offset to glyph ClassDef table--from\n\t\t\t\t\t * beginning of table */\n  Array16Of<typename Types::template OffsetTo<RuleSet>>\n\t\truleSet;\t\t/* Array of RuleSet tables\n\t\t\t\t\t * ordered by class */\n  public:\n  DEFINE_SIZE_ARRAY (4 + 2 * Types::size, ruleSet);\n};\n\n\nstruct ContextFormat3\n{\n  using RuleSet = OT::RuleSet<SmallTypes>;\n\n  bool intersects (const hb_set_t *glyphs) const\n  {\n    if (!(this+coverageZ[0]).intersects (glyphs))\n      return false;\n\n    struct ContextClosureLookupContext lookup_context = {\n      {intersects_coverage, nullptr},\n      ContextFormat::CoverageBasedContext,\n      this\n    };\n    return context_intersects (glyphs,\n\t\t\t       glyphCount, (const HBUINT16 *) (coverageZ.arrayZ + 1),\n\t\t\t       lookup_context);\n  }\n\n  bool may_have_non_1to1 () const\n  { return true; }\n\n  void closure (hb_closure_context_t *c) const\n  {\n    if (!(this+coverageZ[0]).intersects (c->glyphs))\n      return;\n\n    hb_set_t& cur_active_glyphs = c->push_cur_active_glyphs ();\n    get_coverage ().intersect_set (c->previous_parent_active_glyphs (),\n                                                 cur_active_glyphs);\n\n\n    const LookupRecord *lookupRecord = &StructAfter<LookupRecord> (coverageZ.as_array (glyphCount));\n    struct ContextClosureLookupContext lookup_context = {\n      {intersects_coverage, intersected_coverage_glyphs},\n      ContextFormat::CoverageBasedContext,\n      this\n    };\n    context_closure_lookup (c,\n\t\t\t    glyphCount, (const HBUINT16 *) (coverageZ.arrayZ + 1),\n\t\t\t    lookupCount, lookupRecord,\n\t\t\t    0, lookup_context);\n\n    c->pop_cur_done_glyphs ();\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c) const\n  {\n    if (!intersects (c->glyphs))\n      return;\n    const LookupRecord *lookupRecord = &StructAfter<LookupRecord> (coverageZ.as_array (glyphCount));\n    recurse_lookups (c, lookupCount, lookupRecord);\n  }\n\n  void collect_variation_indices (hb_collect_variation_indices_context_t *c) const {}\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c) const\n  {\n    (this+coverageZ[0]).collect_coverage (c->input);\n\n    const LookupRecord *lookupRecord = &StructAfter<LookupRecord> (coverageZ.as_array (glyphCount));\n    struct ContextCollectGlyphsLookupContext lookup_context = {\n      {collect_coverage},\n      this\n    };\n\n    context_collect_glyphs_lookup (c,\n\t\t\t\t   glyphCount, (const HBUINT16 *) (coverageZ.arrayZ + 1),\n\t\t\t\t   lookupCount, lookupRecord,\n\t\t\t\t   lookup_context);\n  }\n\n  bool would_apply (hb_would_apply_context_t *c) const\n  {\n    const LookupRecord *lookupRecord = &StructAfter<LookupRecord> (coverageZ.as_array (glyphCount));\n    struct ContextApplyLookupContext lookup_context = {\n      {match_coverage},\n      this\n    };\n    return context_would_apply_lookup (c,\n\t\t\t\t       glyphCount, (const HBUINT16 *) (coverageZ.arrayZ + 1),\n\t\t\t\t       lookupCount, lookupRecord,\n\t\t\t\t       lookup_context);\n  }\n\n  const Coverage &get_coverage () const { return this+coverageZ[0]; }\n\n  bool apply (hb_ot_apply_context_t *c) const\n  {\n    TRACE_APPLY (this);\n    unsigned int index = (this+coverageZ[0]).get_coverage (c->buffer->cur().codepoint);\n    if (likely (index == NOT_COVERED)) return_trace (false);\n\n    const LookupRecord *lookupRecord = &StructAfter<LookupRecord> (coverageZ.as_array (glyphCount));\n    struct ContextApplyLookupContext lookup_context = {\n      {match_coverage},\n      this\n    };\n    return_trace (context_apply_lookup (c, glyphCount, (const HBUINT16 *) (coverageZ.arrayZ + 1), lookupCount, lookupRecord, lookup_context));\n  }\n\n  bool subset (hb_subset_context_t *c) const\n  {\n    TRACE_SUBSET (this);\n    auto *out = c->serializer->start_embed (this);\n    if (unlikely (!c->serializer->extend_min (out))) return_trace (false);\n\n    out->format = format;\n    out->glyphCount = glyphCount;\n\n    auto coverages = coverageZ.as_array (glyphCount);\n\n    for (const Offset16To<Coverage>& offset : coverages)\n    {\n      /* TODO(subset) This looks like should not be necessary to write this way. */\n      auto *o = c->serializer->allocate_size<Offset16To<Coverage>> (Offset16To<Coverage>::static_size);\n      if (unlikely (!o)) return_trace (false);\n      if (!o->serialize_subset (c, offset, this)) return_trace (false);\n    }\n\n    const auto& lookupRecord = StructAfter<UnsizedArrayOf<LookupRecord>> (coverageZ.as_array (glyphCount));\n    const hb_map_t *lookup_map = c->table_tag == HB_OT_TAG_GSUB ? &c->plan->gsub_lookups : &c->plan->gpos_lookups;\n\n\n    unsigned count = serialize_lookuprecord_array (c->serializer, lookupRecord.as_array (lookupCount), lookup_map);\n    return_trace (c->serializer->check_assign (out->lookupCount, count, HB_SERIALIZE_ERROR_INT_OVERFLOW));\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    if (!c->check_struct (this)) return_trace (false);\n    unsigned int count = glyphCount;\n    if (!count) return_trace (false); /* We want to access coverageZ[0] freely. */\n    if (!c->check_array (coverageZ.arrayZ, count)) return_trace (false);\n    for (unsigned int i = 0; i < count; i++)\n      if (!coverageZ[i].sanitize (c, this)) return_trace (false);\n    const LookupRecord *lookupRecord = &StructAfter<LookupRecord> (coverageZ.as_array (glyphCount));\n    return_trace (c->check_array (lookupRecord, lookupCount));\n  }\n\n  protected:\n  HBUINT16\tformat;\t\t\t/* Format identifier--format = 3 */\n  HBUINT16\tglyphCount;\t\t/* Number of glyphs in the input glyph\n\t\t\t\t\t * sequence */\n  HBUINT16\tlookupCount;\t\t/* Number of LookupRecords */\n  UnsizedArrayOf<Offset16To<Coverage>>\n\t\tcoverageZ;\t\t/* Array of offsets to Coverage\n\t\t\t\t\t * table in glyph sequence order */\n/*UnsizedArrayOf<LookupRecord>\n\t\tlookupRecordX;*/\t/* Array of LookupRecords--in\n\t\t\t\t\t * design order */\n  public:\n  DEFINE_SIZE_ARRAY (6, coverageZ);\n};\n\nstruct Context\n{\n  template <typename context_t, typename ...Ts>\n  typename context_t::return_t dispatch (context_t *c, Ts&&... ds) const\n  {\n    if (unlikely (!c->may_dispatch (this, &u.format))) return c->no_dispatch_return_value ();\n    TRACE_DISPATCH (this, u.format);\n    switch (u.format) {\n    case 1: return_trace (c->dispatch (u.format1, std::forward<Ts> (ds)...));\n    case 2: return_trace (c->dispatch (u.format2, std::forward<Ts> (ds)...));\n    case 3: return_trace (c->dispatch (u.format3, std::forward<Ts> (ds)...));\n#ifndef HB_NO_BEYOND_64K\n    case 4: return_trace (c->dispatch (u.format4, std::forward<Ts> (ds)...));\n    case 5: return_trace (c->dispatch (u.format5, std::forward<Ts> (ds)...));\n#endif\n    default:return_trace (c->default_return_value ());\n    }\n  }\n\n  protected:\n  union {\n  HBUINT16\t\t\tformat;\t\t/* Format identifier */\n  ContextFormat1_4<SmallTypes>\tformat1;\n  ContextFormat2_5<SmallTypes>\tformat2;\n  ContextFormat3\t\tformat3;\n#ifndef HB_NO_BEYOND_64K\n  ContextFormat1_4<MediumTypes>\tformat4;\n  ContextFormat2_5<MediumTypes>\tformat5;\n#endif\n  } u;\n};\n\n\n/* Chaining Contextual lookups */\n\nstruct ChainContextClosureLookupContext\n{\n  ContextClosureFuncs funcs;\n  ContextFormat context_format;\n  const void *intersects_data[3];\n  void *intersects_cache[3];\n  void *intersected_glyphs_cache;\n};\n\nstruct ChainContextCollectGlyphsLookupContext\n{\n  ContextCollectGlyphsFuncs funcs;\n  const void *collect_data[3];\n};\n\nstruct ChainContextApplyLookupContext\n{\n  ChainContextApplyFuncs funcs;\n  const void *match_data[3];\n};\n\ntemplate <typename HBUINT>\nstatic inline bool chain_context_intersects (const hb_set_t *glyphs,\n\t\t\t\t\t     unsigned int backtrackCount,\n\t\t\t\t\t     const HBUINT backtrack[],\n\t\t\t\t\t     unsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t\t     const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t\t     unsigned int lookaheadCount,\n\t\t\t\t\t     const HBUINT lookahead[],\n\t\t\t\t\t     ChainContextClosureLookupContext &lookup_context)\n{\n  return array_is_subset_of (glyphs,\n\t\t\t     backtrackCount, backtrack,\n\t\t\t     lookup_context.funcs.intersects,\n\t\t\t     lookup_context.intersects_data[0],\n\t\t\t     lookup_context.intersects_cache[0])\n      && array_is_subset_of (glyphs,\n\t\t\t     inputCount ? inputCount - 1 : 0, input,\n\t\t\t     lookup_context.funcs.intersects,\n\t\t\t     lookup_context.intersects_data[1],\n\t\t\t     lookup_context.intersects_cache[1])\n      && array_is_subset_of (glyphs,\n\t\t\t     lookaheadCount, lookahead,\n\t\t\t     lookup_context.funcs.intersects,\n\t\t\t     lookup_context.intersects_data[2],\n\t\t\t     lookup_context.intersects_cache[2]);\n}\n\ntemplate <typename HBUINT>\nstatic inline void chain_context_closure_lookup (hb_closure_context_t *c,\n\t\t\t\t\t\t unsigned int backtrackCount,\n\t\t\t\t\t\t const HBUINT backtrack[],\n\t\t\t\t\t\t unsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t\t\t const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t\t\t unsigned int lookaheadCount,\n\t\t\t\t\t\t const HBUINT lookahead[],\n\t\t\t\t\t\t unsigned int lookupCount,\n\t\t\t\t\t\t const LookupRecord lookupRecord[],\n\t\t\t\t\t\t unsigned value,\n\t\t\t\t\t\t ChainContextClosureLookupContext &lookup_context)\n{\n  if (chain_context_intersects (c->glyphs,\n\t\t\t\tbacktrackCount, backtrack,\n\t\t\t\tinputCount, input,\n\t\t\t\tlookaheadCount, lookahead,\n\t\t\t\tlookup_context))\n    context_closure_recurse_lookups (c,\n\t\t     inputCount, input,\n\t\t     lookupCount, lookupRecord,\n\t\t     value,\n\t\t     lookup_context.context_format,\n\t\t     lookup_context.intersects_data[1],\n\t\t     lookup_context.funcs.intersected_glyphs,\n\t\t     lookup_context.intersected_glyphs_cache);\n}\n\ntemplate <typename HBUINT>\nstatic inline void chain_context_collect_glyphs_lookup (hb_collect_glyphs_context_t *c,\n\t\t\t\t\t\t\tunsigned int backtrackCount,\n\t\t\t\t\t\t\tconst HBUINT backtrack[],\n\t\t\t\t\t\t\tunsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t\t\t\tconst HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t\t\t\tunsigned int lookaheadCount,\n\t\t\t\t\t\t\tconst HBUINT lookahead[],\n\t\t\t\t\t\t\tunsigned int lookupCount,\n\t\t\t\t\t\t\tconst LookupRecord lookupRecord[],\n\t\t\t\t\t\t\tChainContextCollectGlyphsLookupContext &lookup_context)\n{\n  collect_array (c, c->before,\n\t\t backtrackCount, backtrack,\n\t\t lookup_context.funcs.collect, lookup_context.collect_data[0]);\n  collect_array (c, c->input,\n\t\t inputCount ? inputCount - 1 : 0, input,\n\t\t lookup_context.funcs.collect, lookup_context.collect_data[1]);\n  collect_array (c, c->after,\n\t\t lookaheadCount, lookahead,\n\t\t lookup_context.funcs.collect, lookup_context.collect_data[2]);\n  recurse_lookups (c,\n\t\t   lookupCount, lookupRecord);\n}\n\ntemplate <typename HBUINT>\nstatic inline bool chain_context_would_apply_lookup (hb_would_apply_context_t *c,\n\t\t\t\t\t\t     unsigned int backtrackCount,\n\t\t\t\t\t\t     const HBUINT backtrack[] HB_UNUSED,\n\t\t\t\t\t\t     unsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t\t\t     const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t\t\t     unsigned int lookaheadCount,\n\t\t\t\t\t\t     const HBUINT lookahead[] HB_UNUSED,\n\t\t\t\t\t\t     unsigned int lookupCount HB_UNUSED,\n\t\t\t\t\t\t     const LookupRecord lookupRecord[] HB_UNUSED,\n\t\t\t\t\t\t     const ChainContextApplyLookupContext &lookup_context)\n{\n  return (c->zero_context ? !backtrackCount && !lookaheadCount : true)\n      && would_match_input (c,\n\t\t\t    inputCount, input,\n\t\t\t    lookup_context.funcs.match[1], lookup_context.match_data[1]);\n}\n\ntemplate <typename HBUINT>\nstatic inline bool chain_context_apply_lookup (hb_ot_apply_context_t *c,\n\t\t\t\t\t       unsigned int backtrackCount,\n\t\t\t\t\t       const HBUINT backtrack[],\n\t\t\t\t\t       unsigned int inputCount, /* Including the first glyph (not matched) */\n\t\t\t\t\t       const HBUINT input[], /* Array of input values--start with second glyph */\n\t\t\t\t\t       unsigned int lookaheadCount,\n\t\t\t\t\t       const HBUINT lookahead[],\n\t\t\t\t\t       unsigned int lookupCount,\n\t\t\t\t\t       const LookupRecord lookupRecord[],\n\t\t\t\t\t       const ChainContextApplyLookupContext &lookup_context)\n{\n  unsigned end_index = c->buffer->idx;\n  unsigned match_end = 0;\n  unsigned match_positions[HB_MAX_CONTEXT_LENGTH];\n  if (!(match_input (c,\n\t\t     inputCount, input,\n\t\t     lookup_context.funcs.match[1], lookup_context.match_data[1],\n\t\t     &match_end, match_positions) && (end_index = match_end)\n       && match_lookahead (c,\n\t\t\t   lookaheadCount, lookahead,\n\t\t\t   lookup_context.funcs.match[2], lookup_context.match_data[2],\n\t\t\t   match_end, &end_index)))\n  {\n    c->buffer->unsafe_to_concat (c->buffer->idx, end_index);\n    return false;\n  }\n\n  unsigned start_index = c->buffer->out_len;\n  if (!match_backtrack (c,\n\t\t\tbacktrackCount, backtrack,\n\t\t\tlookup_context.funcs.match[0], lookup_context.match_data[0],\n\t\t\t&start_index))\n  {\n    c->buffer->unsafe_to_concat_from_outbuffer (start_index, end_index);\n    return false;\n  }\n\n  c->buffer->unsafe_to_break_from_outbuffer (start_index, end_index);\n  apply_lookup (c,\n\t\tinputCount, match_positions,\n\t\tlookupCount, lookupRecord,\n\t\tmatch_end);\n  return true;\n}\n\ntemplate <typename Types>\nstruct ChainRule\n{\n  bool intersects (const hb_set_t *glyphs, ChainContextClosureLookupContext &lookup_context) const\n  {\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    return chain_context_intersects (glyphs,\n\t\t\t\t     backtrack.len, backtrack.arrayZ,\n\t\t\t\t     input.lenP1, input.arrayZ,\n\t\t\t\t     lookahead.len, lookahead.arrayZ,\n\t\t\t\t     lookup_context);\n  }\n\n  void closure (hb_closure_context_t *c, unsigned value,\n\t\tChainContextClosureLookupContext &lookup_context) const\n  {\n    if (unlikely (c->lookup_limit_exceeded ())) return;\n\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    chain_context_closure_lookup (c,\n\t\t\t\t  backtrack.len, backtrack.arrayZ,\n\t\t\t\t  input.lenP1, input.arrayZ,\n\t\t\t\t  lookahead.len, lookahead.arrayZ,\n\t\t\t\t  lookup.len, lookup.arrayZ,\n\t\t\t\t  value,\n\t\t\t\t  lookup_context);\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c,\n                        ChainContextClosureLookupContext &lookup_context) const\n  {\n    if (unlikely (c->lookup_limit_exceeded ())) return;\n    if (!intersects (c->glyphs, lookup_context)) return;\n\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    recurse_lookups (c, lookup.len, lookup.arrayZ);\n  }\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c,\n\t\t       ChainContextCollectGlyphsLookupContext &lookup_context) const\n  {\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    chain_context_collect_glyphs_lookup (c,\n\t\t\t\t\t backtrack.len, backtrack.arrayZ,\n\t\t\t\t\t input.lenP1, input.arrayZ,\n\t\t\t\t\t lookahead.len, lookahead.arrayZ,\n\t\t\t\t\t lookup.len, lookup.arrayZ,\n\t\t\t\t\t lookup_context);\n  }\n\n  bool would_apply (hb_would_apply_context_t *c,\n\t\t    const ChainContextApplyLookupContext &lookup_context) const\n  {\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    return chain_context_would_apply_lookup (c,\n\t\t\t\t\t     backtrack.len, backtrack.arrayZ,\n\t\t\t\t\t     input.lenP1, input.arrayZ,\n\t\t\t\t\t     lookahead.len, lookahead.arrayZ, lookup.len,\n\t\t\t\t\t     lookup.arrayZ, lookup_context);\n  }\n\n  bool apply (hb_ot_apply_context_t *c,\n\t      const ChainContextApplyLookupContext &lookup_context) const\n  {\n    TRACE_APPLY (this);\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    return_trace (chain_context_apply_lookup (c,\n\t\t\t\t\t      backtrack.len, backtrack.arrayZ,\n\t\t\t\t\t      input.lenP1, input.arrayZ,\n\t\t\t\t\t      lookahead.len, lookahead.arrayZ, lookup.len,\n\t\t\t\t\t      lookup.arrayZ, lookup_context));\n  }\n\n  template<typename Iterator,\n\t   hb_requires (hb_is_iterator (Iterator))>\n  void serialize_array (hb_serialize_context_t *c,\n\t\t\tHBUINT16 len,\n\t\t\tIterator it) const\n  {\n    c->copy (len);\n    for (const auto g : it)\n      c->copy ((HBUINT16) g);\n  }\n\n  bool serialize (hb_serialize_context_t *c,\n\t\t  const hb_map_t *lookup_map,\n\t\t  const hb_map_t *backtrack_map,\n\t\t  const hb_map_t *input_map = nullptr,\n\t\t  const hb_map_t *lookahead_map = nullptr) const\n  {\n    TRACE_SERIALIZE (this);\n    auto *out = c->start_embed (this);\n    if (unlikely (!out)) return_trace (false);\n\n    const hb_map_t *mapping = backtrack_map;\n    serialize_array (c, backtrack.len, + backtrack.iter ()\n\t\t\t\t       | hb_map (mapping));\n\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    if (input_map) mapping = input_map;\n    serialize_array (c, input.lenP1, + input.iter ()\n\t\t\t\t     | hb_map (mapping));\n\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    if (lookahead_map) mapping = lookahead_map;\n    serialize_array (c, lookahead.len, + lookahead.iter ()\n\t\t\t\t       | hb_map (mapping));\n\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n\n    HBUINT16* lookupCount = c->embed (&(lookup.len));\n    if (!lookupCount) return_trace (false);\n\n    unsigned count = serialize_lookuprecord_array (c, lookup.as_array (), lookup_map);\n    return_trace (c->check_assign (*lookupCount, count, HB_SERIALIZE_ERROR_INT_OVERFLOW));\n  }\n\n  bool subset (hb_subset_context_t *c,\n\t       const hb_map_t *lookup_map,\n\t       const hb_map_t *backtrack_map = nullptr,\n\t       const hb_map_t *input_map = nullptr,\n\t       const hb_map_t *lookahead_map = nullptr) const\n  {\n    TRACE_SUBSET (this);\n\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n\n    if (!backtrack_map)\n    {\n      const hb_set_t &glyphset = *c->plan->glyphset_gsub ();\n      if (!hb_all (backtrack, glyphset) ||\n\t  !hb_all (input, glyphset) ||\n\t  !hb_all (lookahead, glyphset))\n\treturn_trace (false);\n\n      serialize (c->serializer, lookup_map, c->plan->glyph_map);\n    }\n    else\n    {\n      if (!hb_all (backtrack, backtrack_map) ||\n\t  !hb_all (input, input_map) ||\n\t  !hb_all (lookahead, lookahead_map))\n\treturn_trace (false);\n\n      serialize (c->serializer, lookup_map, backtrack_map, input_map, lookahead_map);\n    }\n\n    return_trace (true);\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    if (!backtrack.sanitize (c)) return_trace (false);\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    if (!input.sanitize (c)) return_trace (false);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    if (!lookahead.sanitize (c)) return_trace (false);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    return_trace (lookup.sanitize (c));\n  }\n\n  protected:\n  Array16Of<typename Types::HBUINT>\n\t\tbacktrack;\t\t/* Array of backtracking values\n\t\t\t\t\t * (to be matched before the input\n\t\t\t\t\t * sequence) */\n  HeadlessArrayOf<typename Types::HBUINT>\n\t\tinputX;\t\t\t/* Array of input values (start with\n\t\t\t\t\t * second glyph) */\n  Array16Of<typename Types::HBUINT>\n\t\tlookaheadX;\t\t/* Array of lookahead values's (to be\n\t\t\t\t\t * matched after the input sequence) */\n  Array16Of<LookupRecord>\n\t\tlookupX;\t\t/* Array of LookupRecords--in\n\t\t\t\t\t * design order) */\n  public:\n  DEFINE_SIZE_MIN (8);\n};\n\ntemplate <typename Types>\nstruct ChainRuleSet\n{\n  using ChainRule = OT::ChainRule<Types>;\n\n  bool intersects (const hb_set_t *glyphs, ChainContextClosureLookupContext &lookup_context) const\n  {\n    return\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_map ([&] (const ChainRule &_) { return _.intersects (glyphs, lookup_context); })\n    | hb_any\n    ;\n  }\n  void closure (hb_closure_context_t *c, unsigned value, ChainContextClosureLookupContext &lookup_context) const\n  {\n    if (unlikely (c->lookup_limit_exceeded ())) return;\n\n    return\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const ChainRule &_) { _.closure (c, value, lookup_context); })\n    ;\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c,\n                        ChainContextClosureLookupContext &lookup_context) const\n  {\n    if (unlikely (c->lookup_limit_exceeded ())) return;\n\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const ChainRule &_) { _.closure_lookups (c, lookup_context); })\n    ;\n  }\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c, ChainContextCollectGlyphsLookupContext &lookup_context) const\n  {\n    return\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const ChainRule &_) { _.collect_glyphs (c, lookup_context); })\n    ;\n  }\n\n  bool would_apply (hb_would_apply_context_t *c,\n\t\t    const ChainContextApplyLookupContext &lookup_context) const\n  {\n    return\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_map ([&] (const ChainRule &_) { return _.would_apply (c, lookup_context); })\n    | hb_any\n    ;\n  }\n\n  bool apply (hb_ot_apply_context_t *c,\n\t      const ChainContextApplyLookupContext &lookup_context) const\n  {\n    TRACE_APPLY (this);\n    return_trace (\n    + hb_iter (rule)\n    | hb_map (hb_add (this))\n    | hb_map ([&] (const ChainRule &_) { return _.apply (c, lookup_context); })\n    | hb_any\n    )\n    ;\n  }\n\n  bool subset (hb_subset_context_t *c,\n\t       const hb_map_t *lookup_map,\n\t       const hb_map_t *backtrack_klass_map = nullptr,\n\t       const hb_map_t *input_klass_map = nullptr,\n\t       const hb_map_t *lookahead_klass_map = nullptr) const\n  {\n    TRACE_SUBSET (this);\n\n    auto snap = c->serializer->snapshot ();\n    auto *out = c->serializer->start_embed (*this);\n    if (unlikely (!c->serializer->extend_min (out))) return_trace (false);\n\n    for (const Offset16To<ChainRule>& _ : rule)\n    {\n      if (!_) continue;\n      auto o_snap = c->serializer->snapshot ();\n      auto *o = out->rule.serialize_append (c->serializer);\n      if (unlikely (!o)) continue;\n\n      if (!o->serialize_subset (c, _, this,\n\t\t\t\tlookup_map,\n\t\t\t\tbacktrack_klass_map,\n\t\t\t\tinput_klass_map,\n\t\t\t\tlookahead_klass_map))\n      {\n\tout->rule.pop ();\n\tc->serializer->revert (o_snap);\n      }\n    }\n\n    bool ret = bool (out->rule);\n    if (!ret) c->serializer->revert (snap);\n\n    return_trace (ret);\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    return_trace (rule.sanitize (c, this));\n  }\n\n  protected:\n  Array16OfOffset16To<ChainRule>\n\t\trule;\t\t\t/* Array of ChainRule tables\n\t\t\t\t\t * ordered by preference */\n  public:\n  DEFINE_SIZE_ARRAY (2, rule);\n};\n\ntemplate <typename Types>\nstruct ChainContextFormat1_4\n{\n  using ChainRuleSet = OT::ChainRuleSet<Types>;\n\n  bool intersects (const hb_set_t *glyphs) const\n  {\n    struct ChainContextClosureLookupContext lookup_context = {\n      {intersects_glyph, intersected_glyph},\n      ContextFormat::SimpleContext,\n      {nullptr, nullptr, nullptr}\n    };\n\n    return\n    + hb_zip (this+coverage, ruleSet)\n    | hb_filter (*glyphs, hb_first)\n    | hb_map (hb_second)\n    | hb_map (hb_add (this))\n    | hb_map ([&] (const ChainRuleSet &_) { return _.intersects (glyphs, lookup_context); })\n    | hb_any\n    ;\n  }\n\n  bool may_have_non_1to1 () const\n  { return true; }\n\n  void closure (hb_closure_context_t *c) const\n  {\n    hb_set_t& cur_active_glyphs = c->push_cur_active_glyphs ();\n    get_coverage ().intersect_set (c->previous_parent_active_glyphs (),\n                                                 cur_active_glyphs);\n\n    struct ChainContextClosureLookupContext lookup_context = {\n      {intersects_glyph, intersected_glyph},\n      ContextFormat::SimpleContext,\n      {nullptr, nullptr, nullptr}\n    };\n\n    + hb_zip (this+coverage, hb_range ((unsigned) ruleSet.len))\n    | hb_filter ([&] (hb_codepoint_t _) {\n      return c->previous_parent_active_glyphs ().has (_);\n    }, hb_first)\n    | hb_map ([&](const hb_pair_t<hb_codepoint_t, unsigned> _) { return hb_pair_t<unsigned, const ChainRuleSet&> (_.first, this+ruleSet[_.second]); })\n    | hb_apply ([&] (const hb_pair_t<unsigned, const ChainRuleSet&>& _) { _.second.closure (c, _.first, lookup_context); })\n    ;\n\n    c->pop_cur_done_glyphs ();\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c) const\n  {\n    struct ChainContextClosureLookupContext lookup_context = {\n      {intersects_glyph, nullptr},\n      ContextFormat::SimpleContext,\n      {nullptr, nullptr, nullptr}\n    };\n\n    + hb_zip (this+coverage, ruleSet)\n    | hb_filter (*c->glyphs, hb_first)\n    | hb_map (hb_second)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const ChainRuleSet &_) { _.closure_lookups (c, lookup_context); })\n    ;\n  }\n\n  void collect_variation_indices (hb_collect_variation_indices_context_t *c) const {}\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c) const\n  {\n    (this+coverage).collect_coverage (c->input);\n\n    struct ChainContextCollectGlyphsLookupContext lookup_context = {\n      {collect_glyph},\n      {nullptr, nullptr, nullptr}\n    };\n\n    + hb_iter (ruleSet)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const ChainRuleSet &_) { _.collect_glyphs (c, lookup_context); })\n    ;\n  }\n\n  bool would_apply (hb_would_apply_context_t *c) const\n  {\n    const ChainRuleSet &rule_set = this+ruleSet[(this+coverage).get_coverage (c->glyphs[0])];\n    struct ChainContextApplyLookupContext lookup_context = {\n      {{match_glyph, match_glyph, match_glyph}},\n      {nullptr, nullptr, nullptr}\n    };\n    return rule_set.would_apply (c, lookup_context);\n  }\n\n  const Coverage &get_coverage () const { return this+coverage; }\n\n  bool apply (hb_ot_apply_context_t *c) const\n  {\n    TRACE_APPLY (this);\n    unsigned int index = (this+coverage).get_coverage (c->buffer->cur().codepoint);\n    if (likely (index == NOT_COVERED)) return_trace (false);\n\n    const ChainRuleSet &rule_set = this+ruleSet[index];\n    struct ChainContextApplyLookupContext lookup_context = {\n      {{match_glyph, match_glyph, match_glyph}},\n      {nullptr, nullptr, nullptr}\n    };\n    return_trace (rule_set.apply (c, lookup_context));\n  }\n\n  bool subset (hb_subset_context_t *c) const\n  {\n    TRACE_SUBSET (this);\n    const hb_set_t &glyphset = *c->plan->glyphset_gsub ();\n    const hb_map_t &glyph_map = *c->plan->glyph_map;\n\n    auto *out = c->serializer->start_embed (*this);\n    if (unlikely (!c->serializer->extend_min (out))) return_trace (false);\n    out->format = format;\n\n    const hb_map_t *lookup_map = c->table_tag == HB_OT_TAG_GSUB ? &c->plan->gsub_lookups : &c->plan->gpos_lookups;\n    hb_sorted_vector_t<hb_codepoint_t> new_coverage;\n    + hb_zip (this+coverage, ruleSet)\n    | hb_filter (glyphset, hb_first)\n    | hb_filter (subset_offset_array (c, out->ruleSet, this, lookup_map), hb_second)\n    | hb_map (hb_first)\n    | hb_map (glyph_map)\n    | hb_sink (new_coverage)\n    ;\n\n    out->coverage.serialize_serialize (c->serializer, new_coverage.iter ());\n    return_trace (bool (new_coverage));\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    return_trace (coverage.sanitize (c, this) && ruleSet.sanitize (c, this));\n  }\n\n  protected:\n  HBUINT16\tformat;\t\t\t/* Format identifier--format = 1 */\n  typename Types::template OffsetTo<Coverage>\n\t\tcoverage;\t\t/* Offset to Coverage table--from\n\t\t\t\t\t * beginning of table */\n  Array16Of<typename Types::template OffsetTo<ChainRuleSet>>\n\t\truleSet;\t\t/* Array of ChainRuleSet tables\n\t\t\t\t\t * ordered by Coverage Index */\n  public:\n  DEFINE_SIZE_ARRAY (2 + 2 * Types::size, ruleSet);\n};\n\ntemplate <typename Types>\nstruct ChainContextFormat2_5\n{\n  using ChainRuleSet = OT::ChainRuleSet<SmallTypes>;\n\n  bool intersects (const hb_set_t *glyphs) const\n  {\n    if (!(this+coverage).intersects (glyphs))\n      return false;\n\n    const ClassDef &backtrack_class_def = this+backtrackClassDef;\n    const ClassDef &input_class_def = this+inputClassDef;\n    const ClassDef &lookahead_class_def = this+lookaheadClassDef;\n\n    hb_map_t caches[3] = {};\n    struct ChainContextClosureLookupContext lookup_context = {\n      {intersects_class, nullptr},\n      ContextFormat::ClassBasedContext,\n      {&backtrack_class_def,\n       &input_class_def,\n       &lookahead_class_def},\n      {&caches[0], &caches[1], &caches[2]}\n    };\n\n    hb_set_t retained_coverage_glyphs;\n    (this+coverage).intersect_set (*glyphs, retained_coverage_glyphs);\n\n    hb_set_t coverage_glyph_classes;\n    input_class_def.intersected_classes (&retained_coverage_glyphs, &coverage_glyph_classes);\n\n    return\n    + hb_iter (ruleSet)\n    | hb_map (hb_add (this))\n    | hb_enumerate\n    | hb_map ([&] (const hb_pair_t<unsigned, const ChainRuleSet &> p)\n\t      { return input_class_def.intersects_class (glyphs, p.first) &&\n\t\t       coverage_glyph_classes.has (p.first) &&\n\t\t       p.second.intersects (glyphs, lookup_context); })\n    | hb_any\n    ;\n  }\n\n  bool may_have_non_1to1 () const\n  { return true; }\n\n  void closure (hb_closure_context_t *c) const\n  {\n    if (!(this+coverage).intersects (c->glyphs))\n      return;\n\n    hb_set_t& cur_active_glyphs = c->push_cur_active_glyphs ();\n    get_coverage ().intersect_set (c->previous_parent_active_glyphs (),\n                                                 cur_active_glyphs);\n\n\n    const ClassDef &backtrack_class_def = this+backtrackClassDef;\n    const ClassDef &input_class_def = this+inputClassDef;\n    const ClassDef &lookahead_class_def = this+lookaheadClassDef;\n\n    hb_map_t caches[3] = {};\n    intersected_class_cache_t intersected_cache;\n    struct ChainContextClosureLookupContext lookup_context = {\n      {intersects_class, intersected_class_glyphs},\n      ContextFormat::ClassBasedContext,\n      {&backtrack_class_def,\n       &input_class_def,\n       &lookahead_class_def},\n      {&caches[0], &caches[1], &caches[2]},\n      &intersected_cache\n    };\n\n    + hb_enumerate (ruleSet)\n    | hb_filter ([&] (unsigned _)\n    { return input_class_def.intersects_class (&c->parent_active_glyphs (), _); },\n\t\t hb_first)\n    | hb_apply ([&] (const hb_pair_t<unsigned, const typename Types::template OffsetTo<ChainRuleSet>&> _)\n                {\n                  const ChainRuleSet& chainrule_set = this+_.second;\n                  chainrule_set.closure (c, _.first, lookup_context);\n                })\n    ;\n\n    c->pop_cur_done_glyphs ();\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c) const\n  {\n    if (!(this+coverage).intersects (c->glyphs))\n      return;\n\n    const ClassDef &backtrack_class_def = this+backtrackClassDef;\n    const ClassDef &input_class_def = this+inputClassDef;\n    const ClassDef &lookahead_class_def = this+lookaheadClassDef;\n\n    hb_map_t caches[3] = {};\n    struct ChainContextClosureLookupContext lookup_context = {\n      {intersects_class, nullptr},\n      ContextFormat::ClassBasedContext,\n      {&backtrack_class_def,\n       &input_class_def,\n       &lookahead_class_def},\n      {&caches[0], &caches[1], &caches[2]}\n    };\n\n    + hb_iter (ruleSet)\n    | hb_map (hb_add (this))\n    | hb_enumerate\n    | hb_filter([&] (unsigned klass)\n    { return input_class_def.intersects_class (c->glyphs, klass); }, hb_first)\n    | hb_map (hb_second)\n    | hb_apply ([&] (const ChainRuleSet &_)\n    { _.closure_lookups (c, lookup_context); })\n    ;\n  }\n\n  void collect_variation_indices (hb_collect_variation_indices_context_t *c) const {}\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c) const\n  {\n    (this+coverage).collect_coverage (c->input);\n\n    const ClassDef &backtrack_class_def = this+backtrackClassDef;\n    const ClassDef &input_class_def = this+inputClassDef;\n    const ClassDef &lookahead_class_def = this+lookaheadClassDef;\n\n    struct ChainContextCollectGlyphsLookupContext lookup_context = {\n      {collect_class},\n      {&backtrack_class_def,\n       &input_class_def,\n       &lookahead_class_def}\n    };\n\n    + hb_iter (ruleSet)\n    | hb_map (hb_add (this))\n    | hb_apply ([&] (const ChainRuleSet &_) { _.collect_glyphs (c, lookup_context); })\n    ;\n  }\n\n  bool would_apply (hb_would_apply_context_t *c) const\n  {\n    const ClassDef &backtrack_class_def = this+backtrackClassDef;\n    const ClassDef &input_class_def = this+inputClassDef;\n    const ClassDef &lookahead_class_def = this+lookaheadClassDef;\n\n    unsigned int index = input_class_def.get_class (c->glyphs[0]);\n    const ChainRuleSet &rule_set = this+ruleSet[index];\n    struct ChainContextApplyLookupContext lookup_context = {\n      {{match_class, match_class, match_class}},\n      {&backtrack_class_def,\n       &input_class_def,\n       &lookahead_class_def}\n    };\n    return rule_set.would_apply (c, lookup_context);\n  }\n\n  const Coverage &get_coverage () const { return this+coverage; }\n\n  unsigned cache_cost () const\n  {\n    unsigned c = (this+lookaheadClassDef).cost () * ruleSet.len;\n    return c >= 4 ? c : 0;\n  }\n  bool cache_func (hb_ot_apply_context_t *c, bool enter) const\n  {\n    if (enter)\n    {\n      if (!HB_BUFFER_TRY_ALLOCATE_VAR (c->buffer, syllable))\n\treturn false;\n      auto &info = c->buffer->info;\n      unsigned count = c->buffer->len;\n      for (unsigned i = 0; i < count; i++)\n\tinfo[i].syllable() = 255;\n      c->new_syllables = 255;\n      return true;\n    }\n    else\n    {\n      c->new_syllables = (unsigned) -1;\n      HB_BUFFER_DEALLOCATE_VAR (c->buffer, syllable);\n      return true;\n    }\n  }\n\n  bool apply (hb_ot_apply_context_t *c, bool cached = false) const\n  {\n    TRACE_APPLY (this);\n    unsigned int index = (this+coverage).get_coverage (c->buffer->cur().codepoint);\n    if (likely (index == NOT_COVERED)) return_trace (false);\n\n    const ClassDef &backtrack_class_def = this+backtrackClassDef;\n    const ClassDef &input_class_def = this+inputClassDef;\n    const ClassDef &lookahead_class_def = this+lookaheadClassDef;\n\n    /* For ChainContextFormat2_5 we cache the LookaheadClassDef instead of InputClassDef.\n     * The reason is that most heavy fonts want to identify a glyph in context and apply\n     * a lookup to it. In this scenario, the length of the input sequence is one, whereas\n     * the lookahead / backtrack are typically longer.  The one glyph in input sequence is\n     * looked-up below and no input glyph is looked up in individual rules, whereas the\n     * lookahead and backtrack glyphs are tried.  Since we match lookahead before backtrack,\n     * we should cache lookahead.  This decisions showed a 20% improvement in shaping of\n     * the Gulzar font.\n     */\n\n    struct ChainContextApplyLookupContext lookup_context = {\n      {{cached && &backtrack_class_def == &lookahead_class_def ? match_class_cached : match_class,\n        cached && &input_class_def == &lookahead_class_def ? match_class_cached : match_class,\n        cached ? match_class_cached : match_class}},\n      {&backtrack_class_def,\n       &input_class_def,\n       &lookahead_class_def}\n    };\n\n    index = input_class_def.get_class (c->buffer->cur().codepoint);\n    const ChainRuleSet &rule_set = this+ruleSet[index];\n    return_trace (rule_set.apply (c, lookup_context));\n  }\n\n  bool subset (hb_subset_context_t *c) const\n  {\n    TRACE_SUBSET (this);\n    auto *out = c->serializer->start_embed (*this);\n    if (unlikely (!c->serializer->extend_min (out))) return_trace (false);\n    out->format = format;\n    out->coverage.serialize_subset (c, coverage, this);\n\n    hb_map_t backtrack_klass_map;\n    hb_map_t input_klass_map;\n    hb_map_t lookahead_klass_map;\n\n    out->backtrackClassDef.serialize_subset (c, backtrackClassDef, this, &backtrack_klass_map);\n    // TODO: subset inputClassDef based on glyphs survived in Coverage subsetting\n    out->inputClassDef.serialize_subset (c, inputClassDef, this, &input_klass_map);\n    out->lookaheadClassDef.serialize_subset (c, lookaheadClassDef, this, &lookahead_klass_map);\n\n    if (unlikely (!c->serializer->propagate_error (backtrack_klass_map,\n\t\t\t\t\t\t   input_klass_map,\n\t\t\t\t\t\t   lookahead_klass_map)))\n      return_trace (false);\n\n    const hb_set_t* glyphset = c->plan->glyphset_gsub ();\n    hb_set_t retained_coverage_glyphs;\n    (this+coverage).intersect_set (*glyphset, retained_coverage_glyphs);\n\n    hb_set_t coverage_glyph_classes;\n    (this+inputClassDef).intersected_classes (&retained_coverage_glyphs, &coverage_glyph_classes);\n\n    int non_zero_index = -1, index = 0;\n    bool ret = true;\n    const hb_map_t *lookup_map = c->table_tag == HB_OT_TAG_GSUB ? &c->plan->gsub_lookups : &c->plan->gpos_lookups;\n    auto last_non_zero = c->serializer->snapshot ();\n    for (const auto& _ : + hb_enumerate (ruleSet)\n\t\t\t | hb_filter (input_klass_map, hb_first))\n    {\n      auto *o = out->ruleSet.serialize_append (c->serializer);\n      if (unlikely (!o))\n      {\n\tret = false;\n\tbreak;\n      }\n      if (coverage_glyph_classes.has (_.first) &&\n          o->serialize_subset (c, _.second, this,\n\t\t\t       lookup_map,\n\t\t\t       &backtrack_klass_map,\n\t\t\t       &input_klass_map,\n\t\t\t       &lookahead_klass_map))\n      {\n        last_non_zero = c->serializer->snapshot ();\n\tnon_zero_index = index;\n      }\n\n      index++;\n    }\n\n    if (!ret || non_zero_index == -1) return_trace (false);\n\n    // prune empty trailing ruleSets\n    if (index > non_zero_index) {\n      c->serializer->revert (last_non_zero);\n      out->ruleSet.len = non_zero_index + 1;\n    }\n\n    return_trace (bool (out->ruleSet));\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    return_trace (coverage.sanitize (c, this) &&\n\t\t  backtrackClassDef.sanitize (c, this) &&\n\t\t  inputClassDef.sanitize (c, this) &&\n\t\t  lookaheadClassDef.sanitize (c, this) &&\n\t\t  ruleSet.sanitize (c, this));\n  }\n\n  protected:\n  HBUINT16\tformat;\t\t\t/* Format identifier--format = 2 */\n  typename Types::template OffsetTo<Coverage>\n\t\tcoverage;\t\t/* Offset to Coverage table--from\n\t\t\t\t\t * beginning of table */\n  typename Types::template OffsetTo<ClassDef>\n\t\tbacktrackClassDef;\t/* Offset to glyph ClassDef table\n\t\t\t\t\t * containing backtrack sequence\n\t\t\t\t\t * data--from beginning of table */\n  typename Types::template OffsetTo<ClassDef>\n\t\tinputClassDef;\t\t/* Offset to glyph ClassDef\n\t\t\t\t\t * table containing input sequence\n\t\t\t\t\t * data--from beginning of table */\n  typename Types::template OffsetTo<ClassDef>\n\t\tlookaheadClassDef;\t/* Offset to glyph ClassDef table\n\t\t\t\t\t * containing lookahead sequence\n\t\t\t\t\t * data--from beginning of table */\n  Array16Of<typename Types::template OffsetTo<ChainRuleSet>>\n\t\truleSet;\t\t/* Array of ChainRuleSet tables\n\t\t\t\t\t * ordered by class */\n  public:\n  DEFINE_SIZE_ARRAY (4 + 4 * Types::size, ruleSet);\n};\n\nstruct ChainContextFormat3\n{\n  using RuleSet = OT::RuleSet<SmallTypes>;\n\n  bool intersects (const hb_set_t *glyphs) const\n  {\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n\n    if (!(this+input[0]).intersects (glyphs))\n      return false;\n\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    struct ChainContextClosureLookupContext lookup_context = {\n      {intersects_coverage, nullptr},\n      ContextFormat::CoverageBasedContext,\n      {this, this, this}\n    };\n    return chain_context_intersects (glyphs,\n\t\t\t\t     backtrack.len, (const HBUINT16 *) backtrack.arrayZ,\n\t\t\t\t     input.len, (const HBUINT16 *) input.arrayZ + 1,\n\t\t\t\t     lookahead.len, (const HBUINT16 *) lookahead.arrayZ,\n\t\t\t\t     lookup_context);\n  }\n\n  bool may_have_non_1to1 () const\n  { return true; }\n\n  void closure (hb_closure_context_t *c) const\n  {\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n\n    if (!(this+input[0]).intersects (c->glyphs))\n      return;\n\n    hb_set_t& cur_active_glyphs = c->push_cur_active_glyphs ();\n    get_coverage ().intersect_set (c->previous_parent_active_glyphs (),\n                                                 cur_active_glyphs);\n\n\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    struct ChainContextClosureLookupContext lookup_context = {\n      {intersects_coverage, intersected_coverage_glyphs},\n      ContextFormat::CoverageBasedContext,\n      {this, this, this}\n    };\n    chain_context_closure_lookup (c,\n\t\t\t\t  backtrack.len, (const HBUINT16 *) backtrack.arrayZ,\n\t\t\t\t  input.len, (const HBUINT16 *) input.arrayZ + 1,\n\t\t\t\t  lookahead.len, (const HBUINT16 *) lookahead.arrayZ,\n\t\t\t\t  lookup.len, lookup.arrayZ,\n\t\t\t\t  0, lookup_context);\n\n    c->pop_cur_done_glyphs ();\n  }\n\n  void closure_lookups (hb_closure_lookups_context_t *c) const\n  {\n    if (!intersects (c->glyphs))\n      return;\n\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    recurse_lookups (c, lookup.len, lookup.arrayZ);\n  }\n\n  void collect_variation_indices (hb_collect_variation_indices_context_t *c) const {}\n\n  void collect_glyphs (hb_collect_glyphs_context_t *c) const\n  {\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n\n    (this+input[0]).collect_coverage (c->input);\n\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n\n    struct ChainContextCollectGlyphsLookupContext lookup_context = {\n      {collect_coverage},\n      {this, this, this}\n    };\n    chain_context_collect_glyphs_lookup (c,\n\t\t\t\t\t backtrack.len, (const HBUINT16 *) backtrack.arrayZ,\n\t\t\t\t\t input.len, (const HBUINT16 *) input.arrayZ + 1,\n\t\t\t\t\t lookahead.len, (const HBUINT16 *) lookahead.arrayZ,\n\t\t\t\t\t lookup.len, lookup.arrayZ,\n\t\t\t\t\t lookup_context);\n  }\n\n  bool would_apply (hb_would_apply_context_t *c) const\n  {\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    struct ChainContextApplyLookupContext lookup_context = {\n      {{match_coverage, match_coverage, match_coverage}},\n      {this, this, this}\n    };\n    return chain_context_would_apply_lookup (c,\n\t\t\t\t\t     backtrack.len, (const HBUINT16 *) backtrack.arrayZ,\n\t\t\t\t\t     input.len, (const HBUINT16 *) input.arrayZ + 1,\n\t\t\t\t\t     lookahead.len, (const HBUINT16 *) lookahead.arrayZ,\n\t\t\t\t\t     lookup.len, lookup.arrayZ, lookup_context);\n  }\n\n  const Coverage &get_coverage () const\n  {\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    return this+input[0];\n  }\n\n  bool apply (hb_ot_apply_context_t *c) const\n  {\n    TRACE_APPLY (this);\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n\n    unsigned int index = (this+input[0]).get_coverage (c->buffer->cur().codepoint);\n    if (likely (index == NOT_COVERED)) return_trace (false);\n\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    struct ChainContextApplyLookupContext lookup_context = {\n      {{match_coverage, match_coverage, match_coverage}},\n      {this, this, this}\n    };\n    return_trace (chain_context_apply_lookup (c,\n\t\t\t\t\t      backtrack.len, (const HBUINT16 *) backtrack.arrayZ,\n\t\t\t\t\t      input.len, (const HBUINT16 *) input.arrayZ + 1,\n\t\t\t\t\t      lookahead.len, (const HBUINT16 *) lookahead.arrayZ,\n\t\t\t\t\t      lookup.len, lookup.arrayZ, lookup_context));\n  }\n\n  template<typename Iterator,\n\t   hb_requires (hb_is_iterator (Iterator))>\n  bool serialize_coverage_offsets (hb_subset_context_t *c, Iterator it, const void* base) const\n  {\n    TRACE_SERIALIZE (this);\n    auto *out = c->serializer->start_embed<Array16OfOffset16To<Coverage>> ();\n\n    if (unlikely (!c->serializer->allocate_size<HBUINT16> (HBUINT16::static_size)))\n      return_trace (false);\n\n    for (auto& offset : it) {\n      auto *o = out->serialize_append (c->serializer);\n      if (unlikely (!o) || !o->serialize_subset (c, offset, base))\n        return_trace (false);\n    }\n\n    return_trace (true);\n  }\n\n  bool subset (hb_subset_context_t *c) const\n  {\n    TRACE_SUBSET (this);\n\n    auto *out = c->serializer->start_embed (this);\n    if (unlikely (!out)) return_trace (false);\n    if (unlikely (!c->serializer->embed (this->format))) return_trace (false);\n\n    if (!serialize_coverage_offsets (c, backtrack.iter (), this))\n      return_trace (false);\n\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    if (!serialize_coverage_offsets (c, input.iter (), this))\n      return_trace (false);\n\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    if (!serialize_coverage_offsets (c, lookahead.iter (), this))\n      return_trace (false);\n\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    const hb_map_t *lookup_map = c->table_tag == HB_OT_TAG_GSUB ? &c->plan->gsub_lookups : &c->plan->gpos_lookups;\n\n    HBUINT16 *lookupCount = c->serializer->copy<HBUINT16> (lookup.len);\n    if (!lookupCount) return_trace (false);\n\n    unsigned count = serialize_lookuprecord_array (c->serializer, lookup.as_array (), lookup_map);\n    return_trace (c->serializer->check_assign (*lookupCount, count, HB_SERIALIZE_ERROR_INT_OVERFLOW));\n  }\n\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    if (!backtrack.sanitize (c, this)) return_trace (false);\n    const auto &input = StructAfter<decltype (inputX)> (backtrack);\n    if (!input.sanitize (c, this)) return_trace (false);\n    if (!input.len) return_trace (false); /* To be consistent with Context. */\n    const auto &lookahead = StructAfter<decltype (lookaheadX)> (input);\n    if (!lookahead.sanitize (c, this)) return_trace (false);\n    const auto &lookup = StructAfter<decltype (lookupX)> (lookahead);\n    return_trace (lookup.sanitize (c));\n  }\n\n  protected:\n  HBUINT16\tformat;\t\t\t/* Format identifier--format = 3 */\n  Array16OfOffset16To<Coverage>\n\t\tbacktrack;\t\t/* Array of coverage tables\n\t\t\t\t\t * in backtracking sequence, in  glyph\n\t\t\t\t\t * sequence order */\n  Array16OfOffset16To<Coverage>\n\t\tinputX\t\t;\t/* Array of coverage\n\t\t\t\t\t * tables in input sequence, in glyph\n\t\t\t\t\t * sequence order */\n  Array16OfOffset16To<Coverage>\n\t\tlookaheadX;\t\t/* Array of coverage tables\n\t\t\t\t\t * in lookahead sequence, in glyph\n\t\t\t\t\t * sequence order */\n  Array16Of<LookupRecord>\n\t\tlookupX;\t\t/* Array of LookupRecords--in\n\t\t\t\t\t * design order) */\n  public:\n  DEFINE_SIZE_MIN (10);\n};\n\nstruct ChainContext\n{\n  template <typename context_t, typename ...Ts>\n  typename context_t::return_t dispatch (context_t *c, Ts&&... ds) const\n  {\n    if (unlikely (!c->may_dispatch (this, &u.format))) return c->no_dispatch_return_value ();\n    TRACE_DISPATCH (this, u.format);\n    switch (u.format) {\n    case 1: return_trace (c->dispatch (u.format1, std::forward<Ts> (ds)...));\n    case 2: return_trace (c->dispatch (u.format2, std::forward<Ts> (ds)...));\n    case 3: return_trace (c->dispatch (u.format3, std::forward<Ts> (ds)...));\n#ifndef HB_NO_BEYOND_64K\n    case 4: return_trace (c->dispatch (u.format4, std::forward<Ts> (ds)...));\n    case 5: return_trace (c->dispatch (u.format5, std::forward<Ts> (ds)...));\n#endif\n    default:return_trace (c->default_return_value ());\n    }\n  }\n\n  protected:\n  union {\n  HBUINT16\t\t\t\tformat;\t/* Format identifier */\n  ChainContextFormat1_4<SmallTypes>\tformat1;\n  ChainContextFormat2_5<SmallTypes>\tformat2;\n  ChainContextFormat3\t\t\tformat3;\n#ifndef HB_NO_BEYOND_64K\n  ChainContextFormat1_4<MediumTypes>\tformat4;\n  ChainContextFormat2_5<MediumTypes>\tformat5;\n#endif\n  } u;\n};\n\n\ntemplate <typename T>\nstruct ExtensionFormat1\n{\n  unsigned int get_type () const { return extensionLookupType; }\n\n  template <typename X>\n  const X& get_subtable () const\n  { return this + reinterpret_cast<const Offset32To<typename T::SubTable> &> (extensionOffset); }\n\n  template <typename context_t, typename ...Ts>\n  typename context_t::return_t dispatch (context_t *c, Ts&&... ds) const\n  {\n    if (unlikely (!c->may_dispatch (this, this))) return c->no_dispatch_return_value ();\n    TRACE_DISPATCH (this, format);\n    return_trace (get_subtable<typename T::SubTable> ().dispatch (c, get_type (), std::forward<Ts> (ds)...));\n  }\n\n  void collect_variation_indices (hb_collect_variation_indices_context_t *c) const\n  { dispatch (c); }\n\n  /* This is called from may_dispatch() above with hb_sanitize_context_t. */\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    return_trace (c->check_struct (this) &&\n\t\t  extensionLookupType != T::SubTable::Extension);\n  }\n\n  bool subset (hb_subset_context_t *c) const\n  {\n    TRACE_SUBSET (this);\n\n    auto *out = c->serializer->start_embed (this);\n    if (unlikely (!out || !c->serializer->extend_min (out))) return_trace (false);\n\n    out->format = format;\n    out->extensionLookupType = extensionLookupType;\n\n    const auto& src_offset =\n        reinterpret_cast<const Offset32To<typename T::SubTable> &> (extensionOffset);\n    auto& dest_offset =\n        reinterpret_cast<Offset32To<typename T::SubTable> &> (out->extensionOffset);\n\n    return_trace (dest_offset.serialize_subset (c, src_offset, this, get_type ()));\n  }\n\n  protected:\n  HBUINT16\tformat;\t\t\t/* Format identifier. Set to 1. */\n  HBUINT16\textensionLookupType;\t/* Lookup type of subtable referenced\n\t\t\t\t\t * by ExtensionOffset (i.e. the\n\t\t\t\t\t * extension subtable). */\n  Offset32\textensionOffset;\t/* Offset to the extension subtable,\n\t\t\t\t\t * of lookup type subtable. */\n  public:\n  DEFINE_SIZE_STATIC (8);\n};\n\ntemplate <typename T>\nstruct Extension\n{\n  unsigned int get_type () const\n  {\n    switch (u.format) {\n    case 1: return u.format1.get_type ();\n    default:return 0;\n    }\n  }\n  template <typename X>\n  const X& get_subtable () const\n  {\n    switch (u.format) {\n    case 1: return u.format1.template get_subtable<typename T::SubTable> ();\n    default:return Null (typename T::SubTable);\n    }\n  }\n\n  // Specialization of dispatch for subset. dispatch() normally just\n  // dispatches to the sub table this points too, but for subset\n  // we need to run subset on this subtable too.\n  template <typename ...Ts>\n  typename hb_subset_context_t::return_t dispatch (hb_subset_context_t *c, Ts&&... ds) const\n  {\n    switch (u.format) {\n    case 1: return u.format1.subset (c);\n    default: return c->default_return_value ();\n    }\n  }\n\n  template <typename context_t, typename ...Ts>\n  typename context_t::return_t dispatch (context_t *c, Ts&&... ds) const\n  {\n    if (unlikely (!c->may_dispatch (this, &u.format))) return c->no_dispatch_return_value ();\n    TRACE_DISPATCH (this, u.format);\n    switch (u.format) {\n    case 1: return_trace (u.format1.dispatch (c, std::forward<Ts> (ds)...));\n    default:return_trace (c->default_return_value ());\n    }\n  }\n\n  protected:\n  union {\n  HBUINT16\t\tformat;\t\t/* Format identifier */\n  ExtensionFormat1<T>\tformat1;\n  } u;\n};\n\n\n/*\n * GSUB/GPOS Common\n */\n\nstruct hb_ot_layout_lookup_accelerator_t\n{\n  template <typename TLookup>\n  static hb_ot_layout_lookup_accelerator_t *create (const TLookup &lookup)\n  {\n    unsigned count = lookup.get_subtable_count ();\n\n    unsigned size = sizeof (hb_ot_layout_lookup_accelerator_t) -\n\t\t    HB_VAR_ARRAY * sizeof (hb_accelerate_subtables_context_t::hb_applicable_t) +\n\t\t    count * sizeof (hb_accelerate_subtables_context_t::hb_applicable_t);\n\n    /* The following is a calloc because when we are collecting subtables,\n     * some of them might be invalid and hence not collect; as a result,\n     * we might not fill in all the count entries of the subtables array.\n     * Zeroing it allows the set digest to gatekeep it without having to\n     * initialize it further. */\n    auto *thiz = (hb_ot_layout_lookup_accelerator_t *) hb_calloc (1, size);\n    if (unlikely (!thiz))\n      return nullptr;\n\n    hb_accelerate_subtables_context_t c_accelerate_subtables (thiz->subtables);\n    lookup.dispatch (&c_accelerate_subtables);\n\n    thiz->digest.init ();\n    for (auto& subtable : hb_iter (thiz->subtables, count))\n      thiz->digest.add (subtable.digest);\n\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n    thiz->cache_user_idx = c_accelerate_subtables.cache_user_idx;\n    for (unsigned i = 0; i < count; i++)\n      if (i != thiz->cache_user_idx)\n\tthiz->subtables[i].apply_cached_func = thiz->subtables[i].apply_func;\n#endif\n\n    return thiz;\n  }\n\n  bool may_have (hb_codepoint_t g) const\n  { return digest.may_have (g); }\n\n  bool apply (hb_ot_apply_context_t *c, unsigned subtables_count, bool use_cache) const\n  {\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n    if (use_cache)\n    {\n      return\n      + hb_iter (hb_iter (subtables, subtables_count))\n      | hb_map ([&c] (const hb_accelerate_subtables_context_t::hb_applicable_t &_) { return _.apply_cached (c); })\n      | hb_any\n      ;\n    }\n    else\n#endif\n    {\n      return\n      + hb_iter (hb_iter (subtables, subtables_count))\n      | hb_map ([&c] (const hb_accelerate_subtables_context_t::hb_applicable_t &_) { return _.apply (c); })\n      | hb_any\n      ;\n    }\n    return false;\n  }\n\n  bool cache_enter (hb_ot_apply_context_t *c) const\n  {\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n    return cache_user_idx != (unsigned) -1 &&\n\t   subtables[cache_user_idx].cache_enter (c);\n#else\n    return false;\n#endif\n  }\n  void cache_leave (hb_ot_apply_context_t *c) const\n  {\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n    subtables[cache_user_idx].cache_leave (c);\n#endif\n  }\n\n\n  hb_set_digest_t digest;\n  private:\n#ifndef HB_NO_OT_LAYOUT_LOOKUP_CACHE\n  unsigned cache_user_idx = (unsigned) -1;\n#endif\n  hb_accelerate_subtables_context_t::hb_applicable_t subtables[HB_VAR_ARRAY];\n};\n\ntemplate <typename Types>\nstruct GSUBGPOSVersion1_2\n{\n  friend struct GSUBGPOS;\n\n  protected:\n  FixedVersion<>version;\t/* Version of the GSUB/GPOS table--initially set\n\t\t\t\t * to 0x00010000u */\n  typename Types:: template OffsetTo<ScriptList>\n\t\tscriptList;\t/* ScriptList table */\n  typename Types::template OffsetTo<FeatureList>\n\t\tfeatureList;\t/* FeatureList table */\n  typename Types::template OffsetTo<LookupList<Types>>\n\t\tlookupList;\t/* LookupList table */\n  Offset32To<FeatureVariations>\n\t\tfeatureVars;\t/* Offset to Feature Variations\n\t\t\t\t   table--from beginning of table\n\t\t\t\t * (may be NULL).  Introduced\n\t\t\t\t * in version 0x00010001. */\n  public:\n  DEFINE_SIZE_MIN (4 + 3 * Types::size);\n\n  unsigned int get_size () const\n  {\n    return min_size +\n\t   (version.to_int () >= 0x00010001u ? featureVars.static_size : 0);\n  }\n\n  const typename Types::template OffsetTo<LookupList<Types>>* get_lookup_list_offset () const\n  {\n    return &lookupList;\n  }\n\n  template <typename TLookup>\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    typedef List16OfOffsetTo<TLookup, typename Types::HBUINT> TLookupList;\n    if (unlikely (!(scriptList.sanitize (c, this) &&\n\t\t    featureList.sanitize (c, this) &&\n\t\t    reinterpret_cast<const typename Types::template OffsetTo<TLookupList> &> (lookupList).sanitize (c, this))))\n      return_trace (false);\n\n#ifndef HB_NO_VAR\n    if (unlikely (!(version.to_int () < 0x00010001u || featureVars.sanitize (c, this))))\n      return_trace (false);\n#endif\n\n    return_trace (true);\n  }\n\n  template <typename TLookup>\n  bool subset (hb_subset_layout_context_t *c) const\n  {\n    TRACE_SUBSET (this);\n\n    auto *out = c->subset_context->serializer->start_embed (this);\n    if (unlikely (!c->subset_context->serializer->extend_min (out))) return_trace (false);\n\n    out->version = version;\n\n    typedef LookupOffsetList<TLookup, typename Types::HBUINT> TLookupList;\n    reinterpret_cast<typename Types::template OffsetTo<TLookupList> &> (out->lookupList)\n\t.serialize_subset (c->subset_context,\n\t\t\t   reinterpret_cast<const typename Types::template OffsetTo<TLookupList> &> (lookupList),\n\t\t\t   this,\n\t\t\t   c);\n\n    reinterpret_cast<typename Types::template OffsetTo<RecordListOfFeature> &> (out->featureList)\n\t.serialize_subset (c->subset_context,\n\t\t\t   reinterpret_cast<const typename Types::template OffsetTo<RecordListOfFeature> &> (featureList),\n\t\t\t   this,\n\t\t\t   c);\n\n    out->scriptList.serialize_subset (c->subset_context,\n\t\t\t\t      scriptList,\n\t\t\t\t      this,\n\t\t\t\t      c);\n\n#ifndef HB_NO_VAR\n    if (version.to_int () >= 0x00010001u)\n    {\n      auto snapshot = c->subset_context->serializer->snapshot ();\n      if (!c->subset_context->serializer->extend_min (&out->featureVars))\n        return_trace (false);\n\n      // TODO(qxliu76): the current implementation doesn't correctly handle feature variations\n      //                that are dropped by instancing when the associated conditions don't trigger.\n      //                Since partial instancing isn't yet supported this isn't an issue yet but will\n      //                need to be fixed for partial instancing.\n\n\n\n      // if all axes are pinned all feature vars are dropped.\n      bool ret = !c->subset_context->plan->all_axes_pinned\n                 && out->featureVars.serialize_subset (c->subset_context, featureVars, this, c);\n      if (!ret && version.major == 1)\n      {\n        c->subset_context->serializer->revert (snapshot);\n\tout->version.major = 1;\n\tout->version.minor = 0;\n      }\n    }\n#endif\n\n    return_trace (true);\n  }\n};\n\nstruct GSUBGPOS\n{\n  unsigned int get_size () const\n  {\n    switch (u.version.major) {\n    case 1: return u.version1.get_size ();\n#ifndef HB_NO_BEYOND_64K\n    case 2: return u.version2.get_size ();\n#endif\n    default: return u.version.static_size;\n    }\n  }\n\n  template <typename TLookup>\n  bool sanitize (hb_sanitize_context_t *c) const\n  {\n    TRACE_SANITIZE (this);\n    if (unlikely (!u.version.sanitize (c))) return_trace (false);\n    switch (u.version.major) {\n    case 1: return_trace (u.version1.sanitize<TLookup> (c));\n#ifndef HB_NO_BEYOND_64K\n    case 2: return_trace (u.version2.sanitize<TLookup> (c));\n#endif\n    default: return_trace (true);\n    }\n  }\n\n  template <typename TLookup>\n  bool subset (hb_subset_layout_context_t *c) const\n  {\n    switch (u.version.major) {\n    case 1: return u.version1.subset<TLookup> (c);\n#ifndef HB_NO_BEYOND_64K\n    case 2: return u.version2.subset<TLookup> (c);\n#endif\n    default: return false;\n    }\n  }\n\n  const ScriptList &get_script_list () const\n  {\n    switch (u.version.major) {\n    case 1: return this+u.version1.scriptList;\n#ifndef HB_NO_BEYOND_64K\n    case 2: return this+u.version2.scriptList;\n#endif\n    default: return Null (ScriptList);\n    }\n  }\n  const FeatureList &get_feature_list () const\n  {\n    switch (u.version.major) {\n    case 1: return this+u.version1.featureList;\n#ifndef HB_NO_BEYOND_64K\n    case 2: return this+u.version2.featureList;\n#endif\n    default: return Null (FeatureList);\n    }\n  }\n  unsigned int get_lookup_count () const\n  {\n    switch (u.version.major) {\n    case 1: return (this+u.version1.lookupList).len;\n#ifndef HB_NO_BEYOND_64K\n    case 2: return (this+u.version2.lookupList).len;\n#endif\n    default: return 0;\n    }\n  }\n  const Lookup& get_lookup (unsigned int i) const\n  {\n    switch (u.version.major) {\n    case 1: return (this+u.version1.lookupList)[i];\n#ifndef HB_NO_BEYOND_64K\n    case 2: return (this+u.version2.lookupList)[i];\n#endif\n    default: return Null (Lookup);\n    }\n  }\n  const FeatureVariations &get_feature_variations () const\n  {\n    switch (u.version.major) {\n    case 1: return (u.version.to_int () >= 0x00010001u ? this+u.version1.featureVars : Null (FeatureVariations));\n#ifndef HB_NO_BEYOND_64K\n    case 2: return this+u.version2.featureVars;\n#endif\n    default: return Null (FeatureVariations);\n    }\n  }\n\n  bool has_data () const { return u.version.to_int (); }\n  unsigned int get_script_count () const\n  { return get_script_list ().len; }\n  const Tag& get_script_tag (unsigned int i) const\n  { return get_script_list ().get_tag (i); }\n  unsigned int get_script_tags (unsigned int start_offset,\n\t\t\t\tunsigned int *script_count /* IN/OUT */,\n\t\t\t\thb_tag_t     *script_tags /* OUT */) const\n  { return get_script_list ().get_tags (start_offset, script_count, script_tags); }\n  const Script& get_script (unsigned int i) const\n  { return get_script_list ()[i]; }\n  bool find_script_index (hb_tag_t tag, unsigned int *index) const\n  { return get_script_list ().find_index (tag, index); }\n\n  unsigned int get_feature_count () const\n  { return get_feature_list ().len; }\n  hb_tag_t get_feature_tag (unsigned int i) const\n  { return i == Index::NOT_FOUND_INDEX ? HB_TAG_NONE : get_feature_list ().get_tag (i); }\n  unsigned int get_feature_tags (unsigned int start_offset,\n\t\t\t\t unsigned int *feature_count /* IN/OUT */,\n\t\t\t\t hb_tag_t     *feature_tags /* OUT */) const\n  { return get_feature_list ().get_tags (start_offset, feature_count, feature_tags); }\n  const Feature& get_feature (unsigned int i) const\n  { return get_feature_list ()[i]; }\n  bool find_feature_index (hb_tag_t tag, unsigned int *index) const\n  { return get_feature_list ().find_index (tag, index); }\n\n  bool find_variations_index (const int *coords, unsigned int num_coords,\n\t\t\t      unsigned int *index) const\n  {\n#ifdef HB_NO_VAR\n    *index = FeatureVariations::NOT_FOUND_INDEX;\n    return false;\n#endif\n    return get_feature_variations ().find_index (coords, num_coords, index);\n  }\n  const Feature& get_feature_variation (unsigned int feature_index,\n\t\t\t\t\tunsigned int variations_index) const\n  {\n#ifndef HB_NO_VAR\n    if (FeatureVariations::NOT_FOUND_INDEX != variations_index &&\n\tu.version.to_int () >= 0x00010001u)\n    {\n      const Feature *feature = get_feature_variations ().find_substitute (variations_index,\n\t\t\t\t\t\t\t\t\t  feature_index);\n      if (feature)\n\treturn *feature;\n    }\n#endif\n    return get_feature (feature_index);\n  }\n\n  void feature_variation_collect_lookups (const hb_set_t *feature_indexes,\n\t\t\t\t\t  const hb_hashmap_t<unsigned, const Feature*> *feature_substitutes_map,\n\t\t\t\t\t  hb_set_t       *lookup_indexes /* OUT */) const\n  {\n#ifndef HB_NO_VAR\n    get_feature_variations ().collect_lookups (feature_indexes, feature_substitutes_map, lookup_indexes);\n#endif\n  }\n\n#ifndef HB_NO_VAR\n  void collect_feature_substitutes_with_variations (hb_collect_feature_substitutes_with_var_context_t *c) const\n  { get_feature_variations ().collect_feature_substitutes_with_variations (c); }\n#endif\n\n  template <typename TLookup>\n  void closure_lookups (hb_face_t      *face,\n\t\t\tconst hb_set_t *glyphs,\n\t\t\thb_set_t       *lookup_indexes /* IN/OUT */) const\n  {\n    hb_set_t visited_lookups, inactive_lookups;\n    hb_closure_lookups_context_t c (face, glyphs, &visited_lookups, &inactive_lookups);\n\n    c.set_recurse_func (TLookup::template dispatch_recurse_func<hb_closure_lookups_context_t>);\n\n    for (unsigned lookup_index : *lookup_indexes)\n      reinterpret_cast<const TLookup &> (get_lookup (lookup_index)).closure_lookups (&c, lookup_index);\n\n    hb_set_union (lookup_indexes, &visited_lookups);\n    hb_set_subtract (lookup_indexes, &inactive_lookups);\n  }\n\n  void prune_langsys (const hb_map_t *duplicate_feature_map,\n                      const hb_set_t *layout_scripts,\n                      hb_hashmap_t<unsigned, hb::unique_ptr<hb_set_t>> *script_langsys_map,\n                      hb_set_t       *new_feature_indexes /* OUT */) const\n  {\n    hb_prune_langsys_context_t c (this, script_langsys_map, duplicate_feature_map, new_feature_indexes);\n\n    unsigned count = get_script_count ();\n    for (unsigned script_index = 0; script_index < count; script_index++)\n    {\n      const Tag& tag = get_script_tag (script_index);\n      if (!layout_scripts->has (tag)) continue;\n      const Script& s = get_script (script_index);\n      s.prune_langsys (&c, script_index);\n    }\n  }\n\n  void prune_features (const hb_map_t *lookup_indices, /* IN */\n\t\t       const hb_hashmap_t<unsigned, hb::shared_ptr<hb_set_t>> *feature_record_cond_idx_map, /* IN */\n\t\t       const hb_hashmap_t<unsigned, const Feature*> *feature_substitutes_map, /* IN */\n\t\t       hb_set_t       *feature_indices /* IN/OUT */) const\n  {\n#ifndef HB_NO_VAR\n    // This is the set of feature indices which have alternate versions defined\n    // if the FeatureVariation's table and the alternate version(s) intersect the\n    // set of lookup indices.\n    hb_set_t alternate_feature_indices;\n    get_feature_variations ().closure_features (lookup_indices, feature_record_cond_idx_map, &alternate_feature_indices);\n    if (unlikely (alternate_feature_indices.in_error()))\n    {\n      feature_indices->err ();\n      return;\n    }\n#endif\n\n    for (unsigned i : hb_iter (feature_indices))\n    {\n      hb_tag_t tag =  get_feature_tag (i);\n      if (tag == HB_TAG ('p', 'r', 'e', 'f'))\n        // Note: Never ever drop feature 'pref', even if it's empty.\n        // HarfBuzz chooses shaper for Khmer based on presence of this\n        // feature.\tSee thread at:\n\t// http://lists.freedesktop.org/archives/harfbuzz/2012-November/002660.html\n        continue;\n\n\n      const Feature *f = &(get_feature (i));\n      const Feature** p = nullptr;\n      if (feature_substitutes_map->has (i, &p))\n        f = *p;\n\n      if (!f->featureParams.is_null () &&\n          tag == HB_TAG ('s', 'i', 'z', 'e'))\n        continue;\n\n      if (!f->intersects_lookup_indexes (lookup_indices)\n#ifndef HB_NO_VAR\n          && !alternate_feature_indices.has (i)\n#endif\n\t  )\n\tfeature_indices->del (i);\n    }\n  }\n\n  template <typename T>\n  struct accelerator_t\n  {\n    accelerator_t (hb_face_t *face)\n    {\n      this->table = hb_sanitize_context_t ().reference_table<T> (face);\n      if (unlikely (this->table->is_blocklisted (this->table.get_blob (), face)))\n      {\n\thb_blob_destroy (this->table.get_blob ());\n\tthis->table = hb_blob_get_empty ();\n      }\n\n      this->lookup_count = table->get_lookup_count ();\n\n      this->accels = (hb_atomic_ptr_t<hb_ot_layout_lookup_accelerator_t> *) hb_calloc (this->lookup_count, sizeof (*accels));\n      if (unlikely (!this->accels))\n      {\n\tthis->lookup_count = 0;\n\tthis->table.destroy ();\n\tthis->table = hb_blob_get_empty ();\n      }\n    }\n    ~accelerator_t ()\n    {\n      for (unsigned int i = 0; i < this->lookup_count; i++)\n\thb_free (this->accels[i]);\n      hb_free (this->accels);\n      this->table.destroy ();\n    }\n\n    hb_ot_layout_lookup_accelerator_t *get_accel (unsigned lookup_index) const\n    {\n      if (unlikely (lookup_index >= lookup_count)) return nullptr;\n\n    retry:\n      auto *accel = accels[lookup_index].get_acquire ();\n      if (unlikely (!accel))\n      {\n\taccel = hb_ot_layout_lookup_accelerator_t::create (table->get_lookup (lookup_index));\n\tif (unlikely (!accel))\n\t  return nullptr;\n\n\tif (unlikely (!accels[lookup_index].cmpexch (nullptr, accel)))\n\t{\n\t  hb_free (accel);\n\t  goto retry;\n\t}\n      }\n\n      return accel;\n    }\n\n    hb_blob_ptr_t<T> table;\n    unsigned int lookup_count;\n    hb_atomic_ptr_t<hb_ot_layout_lookup_accelerator_t> *accels;\n  };\n\n  protected:\n  union {\n  FixedVersion<>\t\t\tversion;\t/* Version identifier */\n  GSUBGPOSVersion1_2<SmallTypes>\tversion1;\n#ifndef HB_NO_BEYOND_64K\n  GSUBGPOSVersion1_2<MediumTypes>\tversion2;\n#endif\n  } u;\n  public:\n  DEFINE_SIZE_MIN (4);\n};\n\n\n} /* namespace OT */\n\n\n#endif /* HB_OT_LAYOUT_GSUBGPOS_HH */\n"], "filenames": ["src/hb-ot-layout-gsubgpos.hh"], "buggy_code_start_loc": [580], "buggy_code_end_loc": [580], "fixing_code_start_loc": [581], "fixing_code_end_loc": [588], "type": "CWE-770", "message": "hb-ot-layout-gsubgpos.hh in HarfBuzz through 6.0.0 allows attackers to trigger O(n^2) growth via consecutive marks during the process of looking back for base glyphs when attaching marks.", "other": {"cve": {"id": "CVE-2023-25193", "sourceIdentifier": "cve@mitre.org", "published": "2023-02-04T20:15:08.027", "lastModified": "2023-03-14T05:15:29.457", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "hb-ot-layout-gsubgpos.hh in HarfBuzz through 6.0.0 allows attackers to trigger O(n^2) growth via consecutive marks during the process of looking back for base glyphs when attaching marks."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-770"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:harfbuzz_project:harfbuzz:*:*:*:*:*:*:*:*", "versionEndIncluding": "6.0.0", "matchCriteriaId": "85CABB33-5FC2-447E-BFD5-70AA07F97359"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:36:*:*:*:*:*:*:*", "matchCriteriaId": "5C675112-476C-4D7C-BCB9-A2FB2D0BC9FD"}]}]}], "references": [{"url": "https://chromium.googlesource.com/chromium/src/+/e1f324aa681af54101c1f2d173d92adb80e37088/DEPS#361", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/harfbuzz/harfbuzz/blob/2822b589bc837fae6f66233e2cf2eef0f6ce8470/src/hb-ot-layout-gsubgpos.hh", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/harfbuzz/harfbuzz/commit/85be877925ddbf34f74a1229f3ca1716bb6170dc", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/KWCHWSICWVZSAXP2YAXM65JC2GR53547/", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/YZ5M2GSAIHFPLHYJXUPQ2QDJCLWXUGO3/", "source": "cve@mitre.org"}]}, "github_commit_url": "https://github.com/harfbuzz/harfbuzz/commit/85be877925ddbf34f74a1229f3ca1716bb6170dc"}}