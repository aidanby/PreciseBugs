{"buggy_code": ["/*\n *\tRoutines having to do with the 'struct sk_buff' memory handlers.\n *\n *\tAuthors:\tAlan Cox <alan@lxorguk.ukuu.org.uk>\n *\t\t\tFlorian La Roche <rzsfl@rz.uni-sb.de>\n *\n *\tFixes:\n *\t\tAlan Cox\t:\tFixed the worst of the load\n *\t\t\t\t\tbalancer bugs.\n *\t\tDave Platt\t:\tInterrupt stacking fix.\n *\tRichard Kooijman\t:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tChanged buffer format.\n *\t\tAlan Cox\t:\tdestructor hook for AF_UNIX etc.\n *\t\tLinus Torvalds\t:\tBetter skb_clone.\n *\t\tAlan Cox\t:\tAdded skb_copy.\n *\t\tAlan Cox\t:\tAdded all the changed routines Linus\n *\t\t\t\t\tonly put in the headers\n *\t\tRay VanTassle\t:\tFixed --skb->lock in free\n *\t\tAlan Cox\t:\tskb_copy copy arp field\n *\t\tAndi Kleen\t:\tslabified it.\n *\t\tRobert Olsson\t:\tRemoved skb_head_pool\n *\n *\tNOTE:\n *\t\tThe __skb_ routines should be called with interrupts\n *\tdisabled, or you better be *real* sure that the operation is atomic\n *\twith respect to whatever list is being frobbed (e.g. via lock_sock()\n *\tor via disabling bottom half handlers, etc).\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n */\n\n/*\n *\tThe functions in this file will not compile correctly with gcc 2.4.x\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/kmemcheck.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/slab.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n#include <linux/netdevice.h>\n#ifdef CONFIG_NET_CLS_ACT\n#include <net/pkt_sched.h>\n#endif\n#include <linux/string.h>\n#include <linux/skbuff.h>\n#include <linux/splice.h>\n#include <linux/cache.h>\n#include <linux/rtnetlink.h>\n#include <linux/init.h>\n#include <linux/scatterlist.h>\n#include <linux/errqueue.h>\n#include <linux/prefetch.h>\n\n#include <net/protocol.h>\n#include <net/dst.h>\n#include <net/sock.h>\n#include <net/checksum.h>\n#include <net/ip6_checksum.h>\n#include <net/xfrm.h>\n\n#include <asm/uaccess.h>\n#include <trace/events/skb.h>\n#include <linux/highmem.h>\n\nstruct kmem_cache *skbuff_head_cache __read_mostly;\nstatic struct kmem_cache *skbuff_fclone_cache __read_mostly;\n\n/**\n *\tskb_panic - private function for out-of-line support\n *\t@skb:\tbuffer\n *\t@sz:\tsize\n *\t@addr:\taddress\n *\t@msg:\tskb_over_panic or skb_under_panic\n *\n *\tOut-of-line support for skb_put() and skb_push().\n *\tCalled via the wrapper skb_over_panic() or skb_under_panic().\n *\tKeep out of line to prevent kernel bloat.\n *\t__builtin_return_address is not used because it is not always reliable.\n */\nstatic void skb_panic(struct sk_buff *skb, unsigned int sz, void *addr,\n\t\t      const char msg[])\n{\n\tpr_emerg(\"%s: text:%p len:%d put:%d head:%p data:%p tail:%#lx end:%#lx dev:%s\\n\",\n\t\t msg, addr, skb->len, sz, skb->head, skb->data,\n\t\t (unsigned long)skb->tail, (unsigned long)skb->end,\n\t\t skb->dev ? skb->dev->name : \"<NULL>\");\n\tBUG();\n}\n\nstatic void skb_over_panic(struct sk_buff *skb, unsigned int sz, void *addr)\n{\n\tskb_panic(skb, sz, addr, __func__);\n}\n\nstatic void skb_under_panic(struct sk_buff *skb, unsigned int sz, void *addr)\n{\n\tskb_panic(skb, sz, addr, __func__);\n}\n\n/*\n * kmalloc_reserve is a wrapper around kmalloc_node_track_caller that tells\n * the caller if emergency pfmemalloc reserves are being used. If it is and\n * the socket is later found to be SOCK_MEMALLOC then PFMEMALLOC reserves\n * may be used. Otherwise, the packet data may be discarded until enough\n * memory is free\n */\n#define kmalloc_reserve(size, gfp, node, pfmemalloc) \\\n\t __kmalloc_reserve(size, gfp, node, _RET_IP_, pfmemalloc)\n\nstatic void *__kmalloc_reserve(size_t size, gfp_t flags, int node,\n\t\t\t       unsigned long ip, bool *pfmemalloc)\n{\n\tvoid *obj;\n\tbool ret_pfmemalloc = false;\n\n\t/*\n\t * Try a regular allocation, when that fails and we're not entitled\n\t * to the reserves, fail.\n\t */\n\tobj = kmalloc_node_track_caller(size,\n\t\t\t\t\tflags | __GFP_NOMEMALLOC | __GFP_NOWARN,\n\t\t\t\t\tnode);\n\tif (obj || !(gfp_pfmemalloc_allowed(flags)))\n\t\tgoto out;\n\n\t/* Try again but now we are using pfmemalloc reserves */\n\tret_pfmemalloc = true;\n\tobj = kmalloc_node_track_caller(size, flags, node);\n\nout:\n\tif (pfmemalloc)\n\t\t*pfmemalloc = ret_pfmemalloc;\n\n\treturn obj;\n}\n\n/* \tAllocate a new skbuff. We do this ourselves so we can fill in a few\n *\t'private' fields and also do memory statistics to find all the\n *\t[BEEP] leaks.\n *\n */\n\nstruct sk_buff *__alloc_skb_head(gfp_t gfp_mask, int node)\n{\n\tstruct sk_buff *skb;\n\n\t/* Get the HEAD */\n\tskb = kmem_cache_alloc_node(skbuff_head_cache,\n\t\t\t\t    gfp_mask & ~__GFP_DMA, node);\n\tif (!skb)\n\t\tgoto out;\n\n\t/*\n\t * Only clear those fields we need to clear, not those that we will\n\t * actually initialise below. Hence, don't put any more fields after\n\t * the tail pointer in struct sk_buff!\n\t */\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\tskb->head = NULL;\n\tskb->truesize = sizeof(struct sk_buff);\n\tatomic_set(&skb->users, 1);\n\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\nout:\n\treturn skb;\n}\n\n/**\n *\t__alloc_skb\t-\tallocate a network buffer\n *\t@size: size to allocate\n *\t@gfp_mask: allocation mask\n *\t@flags: If SKB_ALLOC_FCLONE is set, allocate from fclone cache\n *\t\tinstead of head cache and allocate a cloned (child) skb.\n *\t\tIf SKB_ALLOC_RX is set, __GFP_MEMALLOC will be used for\n *\t\tallocations in case the data is required for writeback\n *\t@node: numa node to allocate memory on\n *\n *\tAllocate a new &sk_buff. The returned buffer has no headroom and a\n *\ttail room of at least size bytes. The object has a reference count\n *\tof one. The return is the buffer. On a failure the return is %NULL.\n *\n *\tBuffers may only be allocated from interrupts using a @gfp_mask of\n *\t%GFP_ATOMIC.\n */\nstruct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,\n\t\t\t    int flags, int node)\n{\n\tstruct kmem_cache *cache;\n\tstruct skb_shared_info *shinfo;\n\tstruct sk_buff *skb;\n\tu8 *data;\n\tbool pfmemalloc;\n\n\tcache = (flags & SKB_ALLOC_FCLONE)\n\t\t? skbuff_fclone_cache : skbuff_head_cache;\n\n\tif (sk_memalloc_socks() && (flags & SKB_ALLOC_RX))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\t/* Get the HEAD */\n\tskb = kmem_cache_alloc_node(cache, gfp_mask & ~__GFP_DMA, node);\n\tif (!skb)\n\t\tgoto out;\n\tprefetchw(skb);\n\n\t/* We do our best to align skb_shared_info on a separate cache\n\t * line. It usually works because kmalloc(X > SMP_CACHE_BYTES) gives\n\t * aligned memory blocks, unless SLUB/SLAB debug is enabled.\n\t * Both skb->head and skb_shared_info are cache line aligned.\n\t */\n\tsize = SKB_DATA_ALIGN(size);\n\tsize += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tdata = kmalloc_reserve(size, gfp_mask, node, &pfmemalloc);\n\tif (!data)\n\t\tgoto nodata;\n\t/* kmalloc(size) might give us more room than requested.\n\t * Put skb_shared_info exactly at the end of allocated zone,\n\t * to allow max possible filling before reallocation.\n\t */\n\tsize = SKB_WITH_OVERHEAD(ksize(data));\n\tprefetchw(data + size);\n\n\t/*\n\t * Only clear those fields we need to clear, not those that we will\n\t * actually initialise below. Hence, don't put any more fields after\n\t * the tail pointer in struct sk_buff!\n\t */\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\t/* Account for allocated memory : skb + skb->head */\n\tskb->truesize = SKB_TRUESIZE(size);\n\tskb->pfmemalloc = pfmemalloc;\n\tatomic_set(&skb->users, 1);\n\tskb->head = data;\n\tskb->data = data;\n\tskb_reset_tail_pointer(skb);\n\tskb->end = skb->tail + size;\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\n\tskb->transport_header = (typeof(skb->transport_header))~0U;\n\n\t/* make sure we initialize shinfo sequentially */\n\tshinfo = skb_shinfo(skb);\n\tmemset(shinfo, 0, offsetof(struct skb_shared_info, dataref));\n\tatomic_set(&shinfo->dataref, 1);\n\tkmemcheck_annotate_variable(shinfo->destructor_arg);\n\n\tif (flags & SKB_ALLOC_FCLONE) {\n\t\tstruct sk_buff *child = skb + 1;\n\t\tatomic_t *fclone_ref = (atomic_t *) (child + 1);\n\n\t\tkmemcheck_annotate_bitfield(child, flags1);\n\t\tkmemcheck_annotate_bitfield(child, flags2);\n\t\tskb->fclone = SKB_FCLONE_ORIG;\n\t\tatomic_set(fclone_ref, 1);\n\n\t\tchild->fclone = SKB_FCLONE_UNAVAILABLE;\n\t\tchild->pfmemalloc = pfmemalloc;\n\t}\nout:\n\treturn skb;\nnodata:\n\tkmem_cache_free(cache, skb);\n\tskb = NULL;\n\tgoto out;\n}\nEXPORT_SYMBOL(__alloc_skb);\n\n/**\n * build_skb - build a network buffer\n * @data: data buffer provided by caller\n * @frag_size: size of fragment, or 0 if head was kmalloced\n *\n * Allocate a new &sk_buff. Caller provides space holding head and\n * skb_shared_info. @data must have been allocated by kmalloc() only if\n * @frag_size is 0, otherwise data should come from the page allocator.\n * The return is the new skb buffer.\n * On a failure the return is %NULL, and @data is not freed.\n * Notes :\n *  Before IO, driver allocates only data buffer where NIC put incoming frame\n *  Driver should add room at head (NET_SKB_PAD) and\n *  MUST add room at tail (SKB_DATA_ALIGN(skb_shared_info))\n *  After IO, driver calls build_skb(), to allocate sk_buff and populate it\n *  before giving packet to stack.\n *  RX rings only contains data buffers, not full skbs.\n */\nstruct sk_buff *build_skb(void *data, unsigned int frag_size)\n{\n\tstruct skb_shared_info *shinfo;\n\tstruct sk_buff *skb;\n\tunsigned int size = frag_size ? : ksize(data);\n\n\tskb = kmem_cache_alloc(skbuff_head_cache, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn NULL;\n\n\tsize -= SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\tskb->truesize = SKB_TRUESIZE(size);\n\tskb->head_frag = frag_size != 0;\n\tatomic_set(&skb->users, 1);\n\tskb->head = data;\n\tskb->data = data;\n\tskb_reset_tail_pointer(skb);\n\tskb->end = skb->tail + size;\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\n\tskb->transport_header = (typeof(skb->transport_header))~0U;\n\n\t/* make sure we initialize shinfo sequentially */\n\tshinfo = skb_shinfo(skb);\n\tmemset(shinfo, 0, offsetof(struct skb_shared_info, dataref));\n\tatomic_set(&shinfo->dataref, 1);\n\tkmemcheck_annotate_variable(shinfo->destructor_arg);\n\n\treturn skb;\n}\nEXPORT_SYMBOL(build_skb);\n\nstruct netdev_alloc_cache {\n\tstruct page_frag\tfrag;\n\t/* we maintain a pagecount bias, so that we dont dirty cache line\n\t * containing page->_count every time we allocate a fragment.\n\t */\n\tunsigned int\t\tpagecnt_bias;\n};\nstatic DEFINE_PER_CPU(struct netdev_alloc_cache, netdev_alloc_cache);\n\nstatic void *__netdev_alloc_frag(unsigned int fragsz, gfp_t gfp_mask)\n{\n\tstruct netdev_alloc_cache *nc;\n\tvoid *data = NULL;\n\tint order;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tnc = &__get_cpu_var(netdev_alloc_cache);\n\tif (unlikely(!nc->frag.page)) {\nrefill:\n\t\tfor (order = NETDEV_FRAG_PAGE_MAX_ORDER; ;) {\n\t\t\tgfp_t gfp = gfp_mask;\n\n\t\t\tif (order)\n\t\t\t\tgfp |= __GFP_COMP | __GFP_NOWARN;\n\t\t\tnc->frag.page = alloc_pages(gfp, order);\n\t\t\tif (likely(nc->frag.page))\n\t\t\t\tbreak;\n\t\t\tif (--order < 0)\n\t\t\t\tgoto end;\n\t\t}\n\t\tnc->frag.size = PAGE_SIZE << order;\nrecycle:\n\t\tatomic_set(&nc->frag.page->_count, NETDEV_PAGECNT_MAX_BIAS);\n\t\tnc->pagecnt_bias = NETDEV_PAGECNT_MAX_BIAS;\n\t\tnc->frag.offset = 0;\n\t}\n\n\tif (nc->frag.offset + fragsz > nc->frag.size) {\n\t\t/* avoid unnecessary locked operations if possible */\n\t\tif ((atomic_read(&nc->frag.page->_count) == nc->pagecnt_bias) ||\n\t\t    atomic_sub_and_test(nc->pagecnt_bias, &nc->frag.page->_count))\n\t\t\tgoto recycle;\n\t\tgoto refill;\n\t}\n\n\tdata = page_address(nc->frag.page) + nc->frag.offset;\n\tnc->frag.offset += fragsz;\n\tnc->pagecnt_bias--;\nend:\n\tlocal_irq_restore(flags);\n\treturn data;\n}\n\n/**\n * netdev_alloc_frag - allocate a page fragment\n * @fragsz: fragment size\n *\n * Allocates a frag from a page for receive buffer.\n * Uses GFP_ATOMIC allocations.\n */\nvoid *netdev_alloc_frag(unsigned int fragsz)\n{\n\treturn __netdev_alloc_frag(fragsz, GFP_ATOMIC | __GFP_COLD);\n}\nEXPORT_SYMBOL(netdev_alloc_frag);\n\n/**\n *\t__netdev_alloc_skb - allocate an skbuff for rx on a specific device\n *\t@dev: network device to receive on\n *\t@length: length to allocate\n *\t@gfp_mask: get_free_pages mask, passed to alloc_skb\n *\n *\tAllocate a new &sk_buff and assign it a usage count of one. The\n *\tbuffer has unspecified headroom built in. Users should allocate\n *\tthe headroom they think they need without accounting for the\n *\tbuilt in space. The built in space is used for optimisations.\n *\n *\t%NULL is returned if there is no free memory.\n */\nstruct sk_buff *__netdev_alloc_skb(struct net_device *dev,\n\t\t\t\t   unsigned int length, gfp_t gfp_mask)\n{\n\tstruct sk_buff *skb = NULL;\n\tunsigned int fragsz = SKB_DATA_ALIGN(length + NET_SKB_PAD) +\n\t\t\t      SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tif (fragsz <= PAGE_SIZE && !(gfp_mask & (__GFP_WAIT | GFP_DMA))) {\n\t\tvoid *data;\n\n\t\tif (sk_memalloc_socks())\n\t\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\t\tdata = __netdev_alloc_frag(fragsz, gfp_mask);\n\n\t\tif (likely(data)) {\n\t\t\tskb = build_skb(data, fragsz);\n\t\t\tif (unlikely(!skb))\n\t\t\t\tput_page(virt_to_head_page(data));\n\t\t}\n\t} else {\n\t\tskb = __alloc_skb(length + NET_SKB_PAD, gfp_mask,\n\t\t\t\t  SKB_ALLOC_RX, NUMA_NO_NODE);\n\t}\n\tif (likely(skb)) {\n\t\tskb_reserve(skb, NET_SKB_PAD);\n\t\tskb->dev = dev;\n\t}\n\treturn skb;\n}\nEXPORT_SYMBOL(__netdev_alloc_skb);\n\nvoid skb_add_rx_frag(struct sk_buff *skb, int i, struct page *page, int off,\n\t\t     int size, unsigned int truesize)\n{\n\tskb_fill_page_desc(skb, i, page, off, size);\n\tskb->len += size;\n\tskb->data_len += size;\n\tskb->truesize += truesize;\n}\nEXPORT_SYMBOL(skb_add_rx_frag);\n\nvoid skb_coalesce_rx_frag(struct sk_buff *skb, int i, int size,\n\t\t\t  unsigned int truesize)\n{\n\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\tskb_frag_size_add(frag, size);\n\tskb->len += size;\n\tskb->data_len += size;\n\tskb->truesize += truesize;\n}\nEXPORT_SYMBOL(skb_coalesce_rx_frag);\n\nstatic void skb_drop_list(struct sk_buff **listp)\n{\n\tkfree_skb_list(*listp);\n\t*listp = NULL;\n}\n\nstatic inline void skb_drop_fraglist(struct sk_buff *skb)\n{\n\tskb_drop_list(&skb_shinfo(skb)->frag_list);\n}\n\nstatic void skb_clone_fraglist(struct sk_buff *skb)\n{\n\tstruct sk_buff *list;\n\n\tskb_walk_frags(skb, list)\n\t\tskb_get(list);\n}\n\nstatic void skb_free_head(struct sk_buff *skb)\n{\n\tif (skb->head_frag)\n\t\tput_page(virt_to_head_page(skb->head));\n\telse\n\t\tkfree(skb->head);\n}\n\nstatic void skb_release_data(struct sk_buff *skb)\n{\n\tif (!skb->cloned ||\n\t    !atomic_sub_return(skb->nohdr ? (1 << SKB_DATAREF_SHIFT) + 1 : 1,\n\t\t\t       &skb_shinfo(skb)->dataref)) {\n\t\tif (skb_shinfo(skb)->nr_frags) {\n\t\t\tint i;\n\t\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\t\t\tskb_frag_unref(skb, i);\n\t\t}\n\n\t\t/*\n\t\t * If skb buf is from userspace, we need to notify the caller\n\t\t * the lower device DMA has done;\n\t\t */\n\t\tif (skb_shinfo(skb)->tx_flags & SKBTX_DEV_ZEROCOPY) {\n\t\t\tstruct ubuf_info *uarg;\n\n\t\t\tuarg = skb_shinfo(skb)->destructor_arg;\n\t\t\tif (uarg->callback)\n\t\t\t\tuarg->callback(uarg, true);\n\t\t}\n\n\t\tif (skb_has_frag_list(skb))\n\t\t\tskb_drop_fraglist(skb);\n\n\t\tskb_free_head(skb);\n\t}\n}\n\n/*\n *\tFree an skbuff by memory without cleaning the state.\n */\nstatic void kfree_skbmem(struct sk_buff *skb)\n{\n\tstruct sk_buff *other;\n\tatomic_t *fclone_ref;\n\n\tswitch (skb->fclone) {\n\tcase SKB_FCLONE_UNAVAILABLE:\n\t\tkmem_cache_free(skbuff_head_cache, skb);\n\t\tbreak;\n\n\tcase SKB_FCLONE_ORIG:\n\t\tfclone_ref = (atomic_t *) (skb + 2);\n\t\tif (atomic_dec_and_test(fclone_ref))\n\t\t\tkmem_cache_free(skbuff_fclone_cache, skb);\n\t\tbreak;\n\n\tcase SKB_FCLONE_CLONE:\n\t\tfclone_ref = (atomic_t *) (skb + 1);\n\t\tother = skb - 1;\n\n\t\t/* The clone portion is available for\n\t\t * fast-cloning again.\n\t\t */\n\t\tskb->fclone = SKB_FCLONE_UNAVAILABLE;\n\n\t\tif (atomic_dec_and_test(fclone_ref))\n\t\t\tkmem_cache_free(skbuff_fclone_cache, other);\n\t\tbreak;\n\t}\n}\n\nstatic void skb_release_head_state(struct sk_buff *skb)\n{\n\tskb_dst_drop(skb);\n#ifdef CONFIG_XFRM\n\tsecpath_put(skb->sp);\n#endif\n\tif (skb->destructor) {\n\t\tWARN_ON(in_irq());\n\t\tskb->destructor(skb);\n\t}\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\tnf_conntrack_put(skb->nfct);\n#endif\n#ifdef CONFIG_BRIDGE_NETFILTER\n\tnf_bridge_put(skb->nf_bridge);\n#endif\n/* XXX: IS this still necessary? - JHS */\n#ifdef CONFIG_NET_SCHED\n\tskb->tc_index = 0;\n#ifdef CONFIG_NET_CLS_ACT\n\tskb->tc_verd = 0;\n#endif\n#endif\n}\n\n/* Free everything but the sk_buff shell. */\nstatic void skb_release_all(struct sk_buff *skb)\n{\n\tskb_release_head_state(skb);\n\tif (likely(skb->head))\n\t\tskb_release_data(skb);\n}\n\n/**\n *\t__kfree_skb - private function\n *\t@skb: buffer\n *\n *\tFree an sk_buff. Release anything attached to the buffer.\n *\tClean the state. This is an internal helper function. Users should\n *\talways call kfree_skb\n */\n\nvoid __kfree_skb(struct sk_buff *skb)\n{\n\tskb_release_all(skb);\n\tkfree_skbmem(skb);\n}\nEXPORT_SYMBOL(__kfree_skb);\n\n/**\n *\tkfree_skb - free an sk_buff\n *\t@skb: buffer to free\n *\n *\tDrop a reference to the buffer and free it if the usage count has\n *\thit zero.\n */\nvoid kfree_skb(struct sk_buff *skb)\n{\n\tif (unlikely(!skb))\n\t\treturn;\n\tif (likely(atomic_read(&skb->users) == 1))\n\t\tsmp_rmb();\n\telse if (likely(!atomic_dec_and_test(&skb->users)))\n\t\treturn;\n\ttrace_kfree_skb(skb, __builtin_return_address(0));\n\t__kfree_skb(skb);\n}\nEXPORT_SYMBOL(kfree_skb);\n\nvoid kfree_skb_list(struct sk_buff *segs)\n{\n\twhile (segs) {\n\t\tstruct sk_buff *next = segs->next;\n\n\t\tkfree_skb(segs);\n\t\tsegs = next;\n\t}\n}\nEXPORT_SYMBOL(kfree_skb_list);\n\n/**\n *\tskb_tx_error - report an sk_buff xmit error\n *\t@skb: buffer that triggered an error\n *\n *\tReport xmit error if a device callback is tracking this skb.\n *\tskb must be freed afterwards.\n */\nvoid skb_tx_error(struct sk_buff *skb)\n{\n\tif (skb_shinfo(skb)->tx_flags & SKBTX_DEV_ZEROCOPY) {\n\t\tstruct ubuf_info *uarg;\n\n\t\tuarg = skb_shinfo(skb)->destructor_arg;\n\t\tif (uarg->callback)\n\t\t\tuarg->callback(uarg, false);\n\t\tskb_shinfo(skb)->tx_flags &= ~SKBTX_DEV_ZEROCOPY;\n\t}\n}\nEXPORT_SYMBOL(skb_tx_error);\n\n/**\n *\tconsume_skb - free an skbuff\n *\t@skb: buffer to free\n *\n *\tDrop a ref to the buffer and free it if the usage count has hit zero\n *\tFunctions identically to kfree_skb, but kfree_skb assumes that the frame\n *\tis being dropped after a failure and notes that\n */\nvoid consume_skb(struct sk_buff *skb)\n{\n\tif (unlikely(!skb))\n\t\treturn;\n\tif (likely(atomic_read(&skb->users) == 1))\n\t\tsmp_rmb();\n\telse if (likely(!atomic_dec_and_test(&skb->users)))\n\t\treturn;\n\ttrace_consume_skb(skb);\n\t__kfree_skb(skb);\n}\nEXPORT_SYMBOL(consume_skb);\n\nstatic void __copy_skb_header(struct sk_buff *new, const struct sk_buff *old)\n{\n\tnew->tstamp\t\t= old->tstamp;\n\tnew->dev\t\t= old->dev;\n\tnew->transport_header\t= old->transport_header;\n\tnew->network_header\t= old->network_header;\n\tnew->mac_header\t\t= old->mac_header;\n\tnew->inner_protocol\t= old->inner_protocol;\n\tnew->inner_transport_header = old->inner_transport_header;\n\tnew->inner_network_header = old->inner_network_header;\n\tnew->inner_mac_header = old->inner_mac_header;\n\tskb_dst_copy(new, old);\n\tskb_copy_hash(new, old);\n\tnew->ooo_okay\t\t= old->ooo_okay;\n\tnew->no_fcs\t\t= old->no_fcs;\n\tnew->encapsulation\t= old->encapsulation;\n#ifdef CONFIG_XFRM\n\tnew->sp\t\t\t= secpath_get(old->sp);\n#endif\n\tmemcpy(new->cb, old->cb, sizeof(old->cb));\n\tnew->csum\t\t= old->csum;\n\tnew->local_df\t\t= old->local_df;\n\tnew->pkt_type\t\t= old->pkt_type;\n\tnew->ip_summed\t\t= old->ip_summed;\n\tskb_copy_queue_mapping(new, old);\n\tnew->priority\t\t= old->priority;\n#if IS_ENABLED(CONFIG_IP_VS)\n\tnew->ipvs_property\t= old->ipvs_property;\n#endif\n\tnew->pfmemalloc\t\t= old->pfmemalloc;\n\tnew->protocol\t\t= old->protocol;\n\tnew->mark\t\t= old->mark;\n\tnew->skb_iif\t\t= old->skb_iif;\n\t__nf_copy(new, old);\n#ifdef CONFIG_NET_SCHED\n\tnew->tc_index\t\t= old->tc_index;\n#ifdef CONFIG_NET_CLS_ACT\n\tnew->tc_verd\t\t= old->tc_verd;\n#endif\n#endif\n\tnew->vlan_proto\t\t= old->vlan_proto;\n\tnew->vlan_tci\t\t= old->vlan_tci;\n\n\tskb_copy_secmark(new, old);\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tnew->napi_id\t= old->napi_id;\n#endif\n}\n\n/*\n * You should not add any new code to this function.  Add it to\n * __copy_skb_header above instead.\n */\nstatic struct sk_buff *__skb_clone(struct sk_buff *n, struct sk_buff *skb)\n{\n#define C(x) n->x = skb->x\n\n\tn->next = n->prev = NULL;\n\tn->sk = NULL;\n\t__copy_skb_header(n, skb);\n\n\tC(len);\n\tC(data_len);\n\tC(mac_len);\n\tn->hdr_len = skb->nohdr ? skb_headroom(skb) : skb->hdr_len;\n\tn->cloned = 1;\n\tn->nohdr = 0;\n\tn->destructor = NULL;\n\tC(tail);\n\tC(end);\n\tC(head);\n\tC(head_frag);\n\tC(data);\n\tC(truesize);\n\tatomic_set(&n->users, 1);\n\n\tatomic_inc(&(skb_shinfo(skb)->dataref));\n\tskb->cloned = 1;\n\n\treturn n;\n#undef C\n}\n\n/**\n *\tskb_morph\t-\tmorph one skb into another\n *\t@dst: the skb to receive the contents\n *\t@src: the skb to supply the contents\n *\n *\tThis is identical to skb_clone except that the target skb is\n *\tsupplied by the user.\n *\n *\tThe target skb is returned upon exit.\n */\nstruct sk_buff *skb_morph(struct sk_buff *dst, struct sk_buff *src)\n{\n\tskb_release_all(dst);\n\treturn __skb_clone(dst, src);\n}\nEXPORT_SYMBOL_GPL(skb_morph);\n\n/**\n *\tskb_copy_ubufs\t-\tcopy userspace skb frags buffers to kernel\n *\t@skb: the skb to modify\n *\t@gfp_mask: allocation priority\n *\n *\tThis must be called on SKBTX_DEV_ZEROCOPY skb.\n *\tIt will copy all frags into kernel and drop the reference\n *\tto userspace pages.\n *\n *\tIf this function is called from an interrupt gfp_mask() must be\n *\t%GFP_ATOMIC.\n *\n *\tReturns 0 on success or a negative error code on failure\n *\tto allocate kernel memory to copy to.\n */\nint skb_copy_ubufs(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tint i;\n\tint num_frags = skb_shinfo(skb)->nr_frags;\n\tstruct page *page, *head = NULL;\n\tstruct ubuf_info *uarg = skb_shinfo(skb)->destructor_arg;\n\n\tfor (i = 0; i < num_frags; i++) {\n\t\tu8 *vaddr;\n\t\tskb_frag_t *f = &skb_shinfo(skb)->frags[i];\n\n\t\tpage = alloc_page(gfp_mask);\n\t\tif (!page) {\n\t\t\twhile (head) {\n\t\t\t\tstruct page *next = (struct page *)page_private(head);\n\t\t\t\tput_page(head);\n\t\t\t\thead = next;\n\t\t\t}\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tvaddr = kmap_atomic(skb_frag_page(f));\n\t\tmemcpy(page_address(page),\n\t\t       vaddr + f->page_offset, skb_frag_size(f));\n\t\tkunmap_atomic(vaddr);\n\t\tset_page_private(page, (unsigned long)head);\n\t\thead = page;\n\t}\n\n\t/* skb frags release userspace buffers */\n\tfor (i = 0; i < num_frags; i++)\n\t\tskb_frag_unref(skb, i);\n\n\tuarg->callback(uarg, false);\n\n\t/* skb frags point to kernel buffers */\n\tfor (i = num_frags - 1; i >= 0; i--) {\n\t\t__skb_fill_page_desc(skb, i, head, 0,\n\t\t\t\t     skb_shinfo(skb)->frags[i].size);\n\t\thead = (struct page *)page_private(head);\n\t}\n\n\tskb_shinfo(skb)->tx_flags &= ~SKBTX_DEV_ZEROCOPY;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_copy_ubufs);\n\n/**\n *\tskb_clone\t-\tduplicate an sk_buff\n *\t@skb: buffer to clone\n *\t@gfp_mask: allocation priority\n *\n *\tDuplicate an &sk_buff. The new one is not owned by a socket. Both\n *\tcopies share the same packet data but not structure. The new\n *\tbuffer has a reference count of 1. If the allocation fails the\n *\tfunction returns %NULL otherwise the new buffer is returned.\n *\n *\tIf this function is called from an interrupt gfp_mask() must be\n *\t%GFP_ATOMIC.\n */\n\nstruct sk_buff *skb_clone(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tstruct sk_buff *n;\n\n\tif (skb_orphan_frags(skb, gfp_mask))\n\t\treturn NULL;\n\n\tn = skb + 1;\n\tif (skb->fclone == SKB_FCLONE_ORIG &&\n\t    n->fclone == SKB_FCLONE_UNAVAILABLE) {\n\t\tatomic_t *fclone_ref = (atomic_t *) (n + 1);\n\t\tn->fclone = SKB_FCLONE_CLONE;\n\t\tatomic_inc(fclone_ref);\n\t} else {\n\t\tif (skb_pfmemalloc(skb))\n\t\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\t\tn = kmem_cache_alloc(skbuff_head_cache, gfp_mask);\n\t\tif (!n)\n\t\t\treturn NULL;\n\n\t\tkmemcheck_annotate_bitfield(n, flags1);\n\t\tkmemcheck_annotate_bitfield(n, flags2);\n\t\tn->fclone = SKB_FCLONE_UNAVAILABLE;\n\t}\n\n\treturn __skb_clone(n, skb);\n}\nEXPORT_SYMBOL(skb_clone);\n\nstatic void skb_headers_offset_update(struct sk_buff *skb, int off)\n{\n\t/* Only adjust this if it actually is csum_start rather than csum */\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tskb->csum_start += off;\n\t/* {transport,network,mac}_header and tail are relative to skb->head */\n\tskb->transport_header += off;\n\tskb->network_header   += off;\n\tif (skb_mac_header_was_set(skb))\n\t\tskb->mac_header += off;\n\tskb->inner_transport_header += off;\n\tskb->inner_network_header += off;\n\tskb->inner_mac_header += off;\n}\n\nstatic void copy_skb_header(struct sk_buff *new, const struct sk_buff *old)\n{\n\t__copy_skb_header(new, old);\n\n\tskb_shinfo(new)->gso_size = skb_shinfo(old)->gso_size;\n\tskb_shinfo(new)->gso_segs = skb_shinfo(old)->gso_segs;\n\tskb_shinfo(new)->gso_type = skb_shinfo(old)->gso_type;\n}\n\nstatic inline int skb_alloc_rx_flag(const struct sk_buff *skb)\n{\n\tif (skb_pfmemalloc(skb))\n\t\treturn SKB_ALLOC_RX;\n\treturn 0;\n}\n\n/**\n *\tskb_copy\t-\tcreate private copy of an sk_buff\n *\t@skb: buffer to copy\n *\t@gfp_mask: allocation priority\n *\n *\tMake a copy of both an &sk_buff and its data. This is used when the\n *\tcaller wishes to modify the data and needs a private copy of the\n *\tdata to alter. Returns %NULL on failure or the pointer to the buffer\n *\ton success. The returned buffer has a reference count of 1.\n *\n *\tAs by-product this function converts non-linear &sk_buff to linear\n *\tone, so that &sk_buff becomes completely private and caller is allowed\n *\tto modify all the data of returned buffer. This means that this\n *\tfunction is not recommended for use in circumstances when only\n *\theader is going to be modified. Use pskb_copy() instead.\n */\n\nstruct sk_buff *skb_copy(const struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tint headerlen = skb_headroom(skb);\n\tunsigned int size = skb_end_offset(skb) + skb->data_len;\n\tstruct sk_buff *n = __alloc_skb(size, gfp_mask,\n\t\t\t\t\tskb_alloc_rx_flag(skb), NUMA_NO_NODE);\n\n\tif (!n)\n\t\treturn NULL;\n\n\t/* Set the data pointer */\n\tskb_reserve(n, headerlen);\n\t/* Set the tail pointer and length */\n\tskb_put(n, skb->len);\n\n\tif (skb_copy_bits(skb, -headerlen, n->head, headerlen + skb->len))\n\t\tBUG();\n\n\tcopy_skb_header(n, skb);\n\treturn n;\n}\nEXPORT_SYMBOL(skb_copy);\n\n/**\n *\t__pskb_copy\t-\tcreate copy of an sk_buff with private head.\n *\t@skb: buffer to copy\n *\t@headroom: headroom of new skb\n *\t@gfp_mask: allocation priority\n *\n *\tMake a copy of both an &sk_buff and part of its data, located\n *\tin header. Fragmented data remain shared. This is used when\n *\tthe caller wishes to modify only header of &sk_buff and needs\n *\tprivate copy of the header to alter. Returns %NULL on failure\n *\tor the pointer to the buffer on success.\n *\tThe returned buffer has a reference count of 1.\n */\n\nstruct sk_buff *__pskb_copy(struct sk_buff *skb, int headroom, gfp_t gfp_mask)\n{\n\tunsigned int size = skb_headlen(skb) + headroom;\n\tstruct sk_buff *n = __alloc_skb(size, gfp_mask,\n\t\t\t\t\tskb_alloc_rx_flag(skb), NUMA_NO_NODE);\n\n\tif (!n)\n\t\tgoto out;\n\n\t/* Set the data pointer */\n\tskb_reserve(n, headroom);\n\t/* Set the tail pointer and length */\n\tskb_put(n, skb_headlen(skb));\n\t/* Copy the bytes */\n\tskb_copy_from_linear_data(skb, n->data, n->len);\n\n\tn->truesize += skb->data_len;\n\tn->data_len  = skb->data_len;\n\tn->len\t     = skb->len;\n\n\tif (skb_shinfo(skb)->nr_frags) {\n\t\tint i;\n\n\t\tif (skb_orphan_frags(skb, gfp_mask)) {\n\t\t\tkfree_skb(n);\n\t\t\tn = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_shinfo(n)->frags[i] = skb_shinfo(skb)->frags[i];\n\t\t\tskb_frag_ref(skb, i);\n\t\t}\n\t\tskb_shinfo(n)->nr_frags = i;\n\t}\n\n\tif (skb_has_frag_list(skb)) {\n\t\tskb_shinfo(n)->frag_list = skb_shinfo(skb)->frag_list;\n\t\tskb_clone_fraglist(n);\n\t}\n\n\tcopy_skb_header(n, skb);\nout:\n\treturn n;\n}\nEXPORT_SYMBOL(__pskb_copy);\n\n/**\n *\tpskb_expand_head - reallocate header of &sk_buff\n *\t@skb: buffer to reallocate\n *\t@nhead: room to add at head\n *\t@ntail: room to add at tail\n *\t@gfp_mask: allocation priority\n *\n *\tExpands (or creates identical copy, if @nhead and @ntail are zero)\n *\theader of @skb. &sk_buff itself is not changed. &sk_buff MUST have\n *\treference count of 1. Returns zero in the case of success or error,\n *\tif expansion failed. In the last case, &sk_buff is not changed.\n *\n *\tAll the pointers pointing into skb header may change and must be\n *\treloaded after call to this function.\n */\n\nint pskb_expand_head(struct sk_buff *skb, int nhead, int ntail,\n\t\t     gfp_t gfp_mask)\n{\n\tint i;\n\tu8 *data;\n\tint size = nhead + skb_end_offset(skb) + ntail;\n\tlong off;\n\n\tBUG_ON(nhead < 0);\n\n\tif (skb_shared(skb))\n\t\tBUG();\n\n\tsize = SKB_DATA_ALIGN(size);\n\n\tif (skb_pfmemalloc(skb))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\tdata = kmalloc_reserve(size + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),\n\t\t\t       gfp_mask, NUMA_NO_NODE, NULL);\n\tif (!data)\n\t\tgoto nodata;\n\tsize = SKB_WITH_OVERHEAD(ksize(data));\n\n\t/* Copy only real data... and, alas, header. This should be\n\t * optimized for the cases when header is void.\n\t */\n\tmemcpy(data + nhead, skb->head, skb_tail_pointer(skb) - skb->head);\n\n\tmemcpy((struct skb_shared_info *)(data + size),\n\t       skb_shinfo(skb),\n\t       offsetof(struct skb_shared_info, frags[skb_shinfo(skb)->nr_frags]));\n\n\t/*\n\t * if shinfo is shared we must drop the old head gracefully, but if it\n\t * is not we can just drop the old head and let the existing refcount\n\t * be since all we did is relocate the values\n\t */\n\tif (skb_cloned(skb)) {\n\t\t/* copy this zero copy skb frags */\n\t\tif (skb_orphan_frags(skb, gfp_mask))\n\t\t\tgoto nofrags;\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\t\tskb_frag_ref(skb, i);\n\n\t\tif (skb_has_frag_list(skb))\n\t\t\tskb_clone_fraglist(skb);\n\n\t\tskb_release_data(skb);\n\t} else {\n\t\tskb_free_head(skb);\n\t}\n\toff = (data + nhead) - skb->head;\n\n\tskb->head     = data;\n\tskb->head_frag = 0;\n\tskb->data    += off;\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\n\tskb->end      = size;\n\toff           = nhead;\n#else\n\tskb->end      = skb->head + size;\n#endif\n\tskb->tail\t      += off;\n\tskb_headers_offset_update(skb, nhead);\n\tskb->cloned   = 0;\n\tskb->hdr_len  = 0;\n\tskb->nohdr    = 0;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1);\n\treturn 0;\n\nnofrags:\n\tkfree(data);\nnodata:\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL(pskb_expand_head);\n\n/* Make private copy of skb with writable head and some headroom */\n\nstruct sk_buff *skb_realloc_headroom(struct sk_buff *skb, unsigned int headroom)\n{\n\tstruct sk_buff *skb2;\n\tint delta = headroom - skb_headroom(skb);\n\n\tif (delta <= 0)\n\t\tskb2 = pskb_copy(skb, GFP_ATOMIC);\n\telse {\n\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\tif (skb2 && pskb_expand_head(skb2, SKB_DATA_ALIGN(delta), 0,\n\t\t\t\t\t     GFP_ATOMIC)) {\n\t\t\tkfree_skb(skb2);\n\t\t\tskb2 = NULL;\n\t\t}\n\t}\n\treturn skb2;\n}\nEXPORT_SYMBOL(skb_realloc_headroom);\n\n/**\n *\tskb_copy_expand\t-\tcopy and expand sk_buff\n *\t@skb: buffer to copy\n *\t@newheadroom: new free bytes at head\n *\t@newtailroom: new free bytes at tail\n *\t@gfp_mask: allocation priority\n *\n *\tMake a copy of both an &sk_buff and its data and while doing so\n *\tallocate additional space.\n *\n *\tThis is used when the caller wishes to modify the data and needs a\n *\tprivate copy of the data to alter as well as more space for new fields.\n *\tReturns %NULL on failure or the pointer to the buffer\n *\ton success. The returned buffer has a reference count of 1.\n *\n *\tYou must pass %GFP_ATOMIC as the allocation priority if this function\n *\tis called from an interrupt.\n */\nstruct sk_buff *skb_copy_expand(const struct sk_buff *skb,\n\t\t\t\tint newheadroom, int newtailroom,\n\t\t\t\tgfp_t gfp_mask)\n{\n\t/*\n\t *\tAllocate the copy buffer\n\t */\n\tstruct sk_buff *n = __alloc_skb(newheadroom + skb->len + newtailroom,\n\t\t\t\t\tgfp_mask, skb_alloc_rx_flag(skb),\n\t\t\t\t\tNUMA_NO_NODE);\n\tint oldheadroom = skb_headroom(skb);\n\tint head_copy_len, head_copy_off;\n\n\tif (!n)\n\t\treturn NULL;\n\n\tskb_reserve(n, newheadroom);\n\n\t/* Set the tail pointer and length */\n\tskb_put(n, skb->len);\n\n\thead_copy_len = oldheadroom;\n\thead_copy_off = 0;\n\tif (newheadroom <= head_copy_len)\n\t\thead_copy_len = newheadroom;\n\telse\n\t\thead_copy_off = newheadroom - head_copy_len;\n\n\t/* Copy the linear header and data. */\n\tif (skb_copy_bits(skb, -head_copy_len, n->head + head_copy_off,\n\t\t\t  skb->len + head_copy_len))\n\t\tBUG();\n\n\tcopy_skb_header(n, skb);\n\n\tskb_headers_offset_update(n, newheadroom - oldheadroom);\n\n\treturn n;\n}\nEXPORT_SYMBOL(skb_copy_expand);\n\n/**\n *\tskb_pad\t\t\t-\tzero pad the tail of an skb\n *\t@skb: buffer to pad\n *\t@pad: space to pad\n *\n *\tEnsure that a buffer is followed by a padding area that is zero\n *\tfilled. Used by network drivers which may DMA or transfer data\n *\tbeyond the buffer end onto the wire.\n *\n *\tMay return error in out of memory cases. The skb is freed on error.\n */\n\nint skb_pad(struct sk_buff *skb, int pad)\n{\n\tint err;\n\tint ntail;\n\n\t/* If the skbuff is non linear tailroom is always zero.. */\n\tif (!skb_cloned(skb) && skb_tailroom(skb) >= pad) {\n\t\tmemset(skb->data+skb->len, 0, pad);\n\t\treturn 0;\n\t}\n\n\tntail = skb->data_len + pad - (skb->end - skb->tail);\n\tif (likely(skb_cloned(skb) || ntail > 0)) {\n\t\terr = pskb_expand_head(skb, 0, ntail, GFP_ATOMIC);\n\t\tif (unlikely(err))\n\t\t\tgoto free_skb;\n\t}\n\n\t/* FIXME: The use of this function with non-linear skb's really needs\n\t * to be audited.\n\t */\n\terr = skb_linearize(skb);\n\tif (unlikely(err))\n\t\tgoto free_skb;\n\n\tmemset(skb->data + skb->len, 0, pad);\n\treturn 0;\n\nfree_skb:\n\tkfree_skb(skb);\n\treturn err;\n}\nEXPORT_SYMBOL(skb_pad);\n\n/**\n *\tpskb_put - add data to the tail of a potentially fragmented buffer\n *\t@skb: start of the buffer to use\n *\t@tail: tail fragment of the buffer to use\n *\t@len: amount of data to add\n *\n *\tThis function extends the used data area of the potentially\n *\tfragmented buffer. @tail must be the last fragment of @skb -- or\n *\t@skb itself. If this would exceed the total buffer size the kernel\n *\twill panic. A pointer to the first byte of the extra data is\n *\treturned.\n */\n\nunsigned char *pskb_put(struct sk_buff *skb, struct sk_buff *tail, int len)\n{\n\tif (tail != skb) {\n\t\tskb->data_len += len;\n\t\tskb->len += len;\n\t}\n\treturn skb_put(tail, len);\n}\nEXPORT_SYMBOL_GPL(pskb_put);\n\n/**\n *\tskb_put - add data to a buffer\n *\t@skb: buffer to use\n *\t@len: amount of data to add\n *\n *\tThis function extends the used data area of the buffer. If this would\n *\texceed the total buffer size the kernel will panic. A pointer to the\n *\tfirst byte of the extra data is returned.\n */\nunsigned char *skb_put(struct sk_buff *skb, unsigned int len)\n{\n\tunsigned char *tmp = skb_tail_pointer(skb);\n\tSKB_LINEAR_ASSERT(skb);\n\tskb->tail += len;\n\tskb->len  += len;\n\tif (unlikely(skb->tail > skb->end))\n\t\tskb_over_panic(skb, len, __builtin_return_address(0));\n\treturn tmp;\n}\nEXPORT_SYMBOL(skb_put);\n\n/**\n *\tskb_push - add data to the start of a buffer\n *\t@skb: buffer to use\n *\t@len: amount of data to add\n *\n *\tThis function extends the used data area of the buffer at the buffer\n *\tstart. If this would exceed the total buffer headroom the kernel will\n *\tpanic. A pointer to the first byte of the extra data is returned.\n */\nunsigned char *skb_push(struct sk_buff *skb, unsigned int len)\n{\n\tskb->data -= len;\n\tskb->len  += len;\n\tif (unlikely(skb->data<skb->head))\n\t\tskb_under_panic(skb, len, __builtin_return_address(0));\n\treturn skb->data;\n}\nEXPORT_SYMBOL(skb_push);\n\n/**\n *\tskb_pull - remove data from the start of a buffer\n *\t@skb: buffer to use\n *\t@len: amount of data to remove\n *\n *\tThis function removes data from the start of a buffer, returning\n *\tthe memory to the headroom. A pointer to the next data in the buffer\n *\tis returned. Once the data has been pulled future pushes will overwrite\n *\tthe old data.\n */\nunsigned char *skb_pull(struct sk_buff *skb, unsigned int len)\n{\n\treturn skb_pull_inline(skb, len);\n}\nEXPORT_SYMBOL(skb_pull);\n\n/**\n *\tskb_trim - remove end from a buffer\n *\t@skb: buffer to alter\n *\t@len: new length\n *\n *\tCut the length of a buffer down by removing data from the tail. If\n *\tthe buffer is already under the length specified it is not modified.\n *\tThe skb must be linear.\n */\nvoid skb_trim(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->len > len)\n\t\t__skb_trim(skb, len);\n}\nEXPORT_SYMBOL(skb_trim);\n\n/* Trims skb to length len. It can change skb pointers.\n */\n\nint ___pskb_trim(struct sk_buff *skb, unsigned int len)\n{\n\tstruct sk_buff **fragp;\n\tstruct sk_buff *frag;\n\tint offset = skb_headlen(skb);\n\tint nfrags = skb_shinfo(skb)->nr_frags;\n\tint i;\n\tint err;\n\n\tif (skb_cloned(skb) &&\n\t    unlikely((err = pskb_expand_head(skb, 0, 0, GFP_ATOMIC))))\n\t\treturn err;\n\n\ti = 0;\n\tif (offset >= len)\n\t\tgoto drop_pages;\n\n\tfor (; i < nfrags; i++) {\n\t\tint end = offset + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (end < len) {\n\t\t\toffset = end;\n\t\t\tcontinue;\n\t\t}\n\n\t\tskb_frag_size_set(&skb_shinfo(skb)->frags[i++], len - offset);\n\ndrop_pages:\n\t\tskb_shinfo(skb)->nr_frags = i;\n\n\t\tfor (; i < nfrags; i++)\n\t\t\tskb_frag_unref(skb, i);\n\n\t\tif (skb_has_frag_list(skb))\n\t\t\tskb_drop_fraglist(skb);\n\t\tgoto done;\n\t}\n\n\tfor (fragp = &skb_shinfo(skb)->frag_list; (frag = *fragp);\n\t     fragp = &frag->next) {\n\t\tint end = offset + frag->len;\n\n\t\tif (skb_shared(frag)) {\n\t\t\tstruct sk_buff *nfrag;\n\n\t\t\tnfrag = skb_clone(frag, GFP_ATOMIC);\n\t\t\tif (unlikely(!nfrag))\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tnfrag->next = frag->next;\n\t\t\tconsume_skb(frag);\n\t\t\tfrag = nfrag;\n\t\t\t*fragp = frag;\n\t\t}\n\n\t\tif (end < len) {\n\t\t\toffset = end;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (end > len &&\n\t\t    unlikely((err = pskb_trim(frag, len - offset))))\n\t\t\treturn err;\n\n\t\tif (frag->next)\n\t\t\tskb_drop_list(&frag->next);\n\t\tbreak;\n\t}\n\ndone:\n\tif (len > skb_headlen(skb)) {\n\t\tskb->data_len -= skb->len - len;\n\t\tskb->len       = len;\n\t} else {\n\t\tskb->len       = len;\n\t\tskb->data_len  = 0;\n\t\tskb_set_tail_pointer(skb, len);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(___pskb_trim);\n\n/**\n *\t__pskb_pull_tail - advance tail of skb header\n *\t@skb: buffer to reallocate\n *\t@delta: number of bytes to advance tail\n *\n *\tThe function makes a sense only on a fragmented &sk_buff,\n *\tit expands header moving its tail forward and copying necessary\n *\tdata from fragmented part.\n *\n *\t&sk_buff MUST have reference count of 1.\n *\n *\tReturns %NULL (and &sk_buff does not change) if pull failed\n *\tor value of new tail of skb in the case of success.\n *\n *\tAll the pointers pointing into skb header may change and must be\n *\treloaded after call to this function.\n */\n\n/* Moves tail of skb head forward, copying data from fragmented part,\n * when it is necessary.\n * 1. It may fail due to malloc failure.\n * 2. It may change skb pointers.\n *\n * It is pretty complicated. Luckily, it is called only in exceptional cases.\n */\nunsigned char *__pskb_pull_tail(struct sk_buff *skb, int delta)\n{\n\t/* If skb has not enough free space at tail, get new one\n\t * plus 128 bytes for future expansions. If we have enough\n\t * room at tail, reallocate without expansion only if skb is cloned.\n\t */\n\tint i, k, eat = (skb->tail + delta) - skb->end;\n\n\tif (eat > 0 || skb_cloned(skb)) {\n\t\tif (pskb_expand_head(skb, 0, eat > 0 ? eat + 128 : 0,\n\t\t\t\t     GFP_ATOMIC))\n\t\t\treturn NULL;\n\t}\n\n\tif (skb_copy_bits(skb, skb_headlen(skb), skb_tail_pointer(skb), delta))\n\t\tBUG();\n\n\t/* Optimization: no fragments, no reasons to preestimate\n\t * size of pulled pages. Superb.\n\t */\n\tif (!skb_has_frag_list(skb))\n\t\tgoto pull_pages;\n\n\t/* Estimate size of pulled pages. */\n\teat = delta;\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (size >= eat)\n\t\t\tgoto pull_pages;\n\t\teat -= size;\n\t}\n\n\t/* If we need update frag list, we are in troubles.\n\t * Certainly, it possible to add an offset to skb data,\n\t * but taking into account that pulling is expected to\n\t * be very rare operation, it is worth to fight against\n\t * further bloating skb head and crucify ourselves here instead.\n\t * Pure masohism, indeed. 8)8)\n\t */\n\tif (eat) {\n\t\tstruct sk_buff *list = skb_shinfo(skb)->frag_list;\n\t\tstruct sk_buff *clone = NULL;\n\t\tstruct sk_buff *insp = NULL;\n\n\t\tdo {\n\t\t\tBUG_ON(!list);\n\n\t\t\tif (list->len <= eat) {\n\t\t\t\t/* Eaten as whole. */\n\t\t\t\teat -= list->len;\n\t\t\t\tlist = list->next;\n\t\t\t\tinsp = list;\n\t\t\t} else {\n\t\t\t\t/* Eaten partially. */\n\n\t\t\t\tif (skb_shared(list)) {\n\t\t\t\t\t/* Sucks! We need to fork list. :-( */\n\t\t\t\t\tclone = skb_clone(list, GFP_ATOMIC);\n\t\t\t\t\tif (!clone)\n\t\t\t\t\t\treturn NULL;\n\t\t\t\t\tinsp = list->next;\n\t\t\t\t\tlist = clone;\n\t\t\t\t} else {\n\t\t\t\t\t/* This may be pulled without\n\t\t\t\t\t * problems. */\n\t\t\t\t\tinsp = list;\n\t\t\t\t}\n\t\t\t\tif (!pskb_pull(list, eat)) {\n\t\t\t\t\tkfree_skb(clone);\n\t\t\t\t\treturn NULL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} while (eat);\n\n\t\t/* Free pulled out fragments. */\n\t\twhile ((list = skb_shinfo(skb)->frag_list) != insp) {\n\t\t\tskb_shinfo(skb)->frag_list = list->next;\n\t\t\tkfree_skb(list);\n\t\t}\n\t\t/* And insert new clone at head. */\n\t\tif (clone) {\n\t\t\tclone->next = list;\n\t\t\tskb_shinfo(skb)->frag_list = clone;\n\t\t}\n\t}\n\t/* Success! Now we may commit changes to skb data. */\n\npull_pages:\n\teat = delta;\n\tk = 0;\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (size <= eat) {\n\t\t\tskb_frag_unref(skb, i);\n\t\t\teat -= size;\n\t\t} else {\n\t\t\tskb_shinfo(skb)->frags[k] = skb_shinfo(skb)->frags[i];\n\t\t\tif (eat) {\n\t\t\t\tskb_shinfo(skb)->frags[k].page_offset += eat;\n\t\t\t\tskb_frag_size_sub(&skb_shinfo(skb)->frags[k], eat);\n\t\t\t\teat = 0;\n\t\t\t}\n\t\t\tk++;\n\t\t}\n\t}\n\tskb_shinfo(skb)->nr_frags = k;\n\n\tskb->tail     += delta;\n\tskb->data_len -= delta;\n\n\treturn skb_tail_pointer(skb);\n}\nEXPORT_SYMBOL(__pskb_pull_tail);\n\n/**\n *\tskb_copy_bits - copy bits from skb to kernel buffer\n *\t@skb: source skb\n *\t@offset: offset in source\n *\t@to: destination buffer\n *\t@len: number of bytes to copy\n *\n *\tCopy the specified number of bytes from the source skb to the\n *\tdestination buffer.\n *\n *\tCAUTION ! :\n *\t\tIf its prototype is ever changed,\n *\t\tcheck arch/{*}/net/{*}.S files,\n *\t\tsince it is called from BPF assembly code.\n */\nint skb_copy_bits(const struct sk_buff *skb, int offset, void *to, int len)\n{\n\tint start = skb_headlen(skb);\n\tstruct sk_buff *frag_iter;\n\tint i, copy;\n\n\tif (offset > (int)skb->len - len)\n\t\tgoto fault;\n\n\t/* Copy header. */\n\tif ((copy = start - offset) > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tskb_copy_from_linear_data_offset(skb, offset, to, copy);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn 0;\n\t\toffset += copy;\n\t\tto     += copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\t\tskb_frag_t *f = &skb_shinfo(skb)->frags[i];\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(f);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\n\t\t\tvaddr = kmap_atomic(skb_frag_page(f));\n\t\t\tmemcpy(to,\n\t\t\t       vaddr + f->page_offset + offset - start,\n\t\t\t       copy);\n\t\t\tkunmap_atomic(vaddr);\n\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tif (skb_copy_bits(frag_iter, offset - start, to, copy))\n\t\t\t\tgoto fault;\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tif (!len)\n\t\treturn 0;\n\nfault:\n\treturn -EFAULT;\n}\nEXPORT_SYMBOL(skb_copy_bits);\n\n/*\n * Callback from splice_to_pipe(), if we need to release some pages\n * at the end of the spd in case we error'ed out in filling the pipe.\n */\nstatic void sock_spd_release(struct splice_pipe_desc *spd, unsigned int i)\n{\n\tput_page(spd->pages[i]);\n}\n\nstatic struct page *linear_to_page(struct page *page, unsigned int *len,\n\t\t\t\t   unsigned int *offset,\n\t\t\t\t   struct sock *sk)\n{\n\tstruct page_frag *pfrag = sk_page_frag(sk);\n\n\tif (!sk_page_frag_refill(sk, pfrag))\n\t\treturn NULL;\n\n\t*len = min_t(unsigned int, *len, pfrag->size - pfrag->offset);\n\n\tmemcpy(page_address(pfrag->page) + pfrag->offset,\n\t       page_address(page) + *offset, *len);\n\t*offset = pfrag->offset;\n\tpfrag->offset += *len;\n\n\treturn pfrag->page;\n}\n\nstatic bool spd_can_coalesce(const struct splice_pipe_desc *spd,\n\t\t\t     struct page *page,\n\t\t\t     unsigned int offset)\n{\n\treturn\tspd->nr_pages &&\n\t\tspd->pages[spd->nr_pages - 1] == page &&\n\t\t(spd->partial[spd->nr_pages - 1].offset +\n\t\t spd->partial[spd->nr_pages - 1].len == offset);\n}\n\n/*\n * Fill page/offset/length into spd, if it can hold more pages.\n */\nstatic bool spd_fill_page(struct splice_pipe_desc *spd,\n\t\t\t  struct pipe_inode_info *pipe, struct page *page,\n\t\t\t  unsigned int *len, unsigned int offset,\n\t\t\t  bool linear,\n\t\t\t  struct sock *sk)\n{\n\tif (unlikely(spd->nr_pages == MAX_SKB_FRAGS))\n\t\treturn true;\n\n\tif (linear) {\n\t\tpage = linear_to_page(page, len, &offset, sk);\n\t\tif (!page)\n\t\t\treturn true;\n\t}\n\tif (spd_can_coalesce(spd, page, offset)) {\n\t\tspd->partial[spd->nr_pages - 1].len += *len;\n\t\treturn false;\n\t}\n\tget_page(page);\n\tspd->pages[spd->nr_pages] = page;\n\tspd->partial[spd->nr_pages].len = *len;\n\tspd->partial[spd->nr_pages].offset = offset;\n\tspd->nr_pages++;\n\n\treturn false;\n}\n\nstatic bool __splice_segment(struct page *page, unsigned int poff,\n\t\t\t     unsigned int plen, unsigned int *off,\n\t\t\t     unsigned int *len,\n\t\t\t     struct splice_pipe_desc *spd, bool linear,\n\t\t\t     struct sock *sk,\n\t\t\t     struct pipe_inode_info *pipe)\n{\n\tif (!*len)\n\t\treturn true;\n\n\t/* skip this segment if already processed */\n\tif (*off >= plen) {\n\t\t*off -= plen;\n\t\treturn false;\n\t}\n\n\t/* ignore any bits we already processed */\n\tpoff += *off;\n\tplen -= *off;\n\t*off = 0;\n\n\tdo {\n\t\tunsigned int flen = min(*len, plen);\n\n\t\tif (spd_fill_page(spd, pipe, page, &flen, poff,\n\t\t\t\t  linear, sk))\n\t\t\treturn true;\n\t\tpoff += flen;\n\t\tplen -= flen;\n\t\t*len -= flen;\n\t} while (*len && plen);\n\n\treturn false;\n}\n\n/*\n * Map linear and fragment data from the skb to spd. It reports true if the\n * pipe is full or if we already spliced the requested length.\n */\nstatic bool __skb_splice_bits(struct sk_buff *skb, struct pipe_inode_info *pipe,\n\t\t\t      unsigned int *offset, unsigned int *len,\n\t\t\t      struct splice_pipe_desc *spd, struct sock *sk)\n{\n\tint seg;\n\n\t/* map the linear part :\n\t * If skb->head_frag is set, this 'linear' part is backed by a\n\t * fragment, and if the head is not shared with any clones then\n\t * we can avoid a copy since we own the head portion of this page.\n\t */\n\tif (__splice_segment(virt_to_page(skb->data),\n\t\t\t     (unsigned long) skb->data & (PAGE_SIZE - 1),\n\t\t\t     skb_headlen(skb),\n\t\t\t     offset, len, spd,\n\t\t\t     skb_head_is_locked(skb),\n\t\t\t     sk, pipe))\n\t\treturn true;\n\n\t/*\n\t * then map the fragments\n\t */\n\tfor (seg = 0; seg < skb_shinfo(skb)->nr_frags; seg++) {\n\t\tconst skb_frag_t *f = &skb_shinfo(skb)->frags[seg];\n\n\t\tif (__splice_segment(skb_frag_page(f),\n\t\t\t\t     f->page_offset, skb_frag_size(f),\n\t\t\t\t     offset, len, spd, false, sk, pipe))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * Map data from the skb to a pipe. Should handle both the linear part,\n * the fragments, and the frag list. It does NOT handle frag lists within\n * the frag list, if such a thing exists. We'd probably need to recurse to\n * handle that cleanly.\n */\nint skb_splice_bits(struct sk_buff *skb, unsigned int offset,\n\t\t    struct pipe_inode_info *pipe, unsigned int tlen,\n\t\t    unsigned int flags)\n{\n\tstruct partial_page partial[MAX_SKB_FRAGS];\n\tstruct page *pages[MAX_SKB_FRAGS];\n\tstruct splice_pipe_desc spd = {\n\t\t.pages = pages,\n\t\t.partial = partial,\n\t\t.nr_pages_max = MAX_SKB_FRAGS,\n\t\t.flags = flags,\n\t\t.ops = &nosteal_pipe_buf_ops,\n\t\t.spd_release = sock_spd_release,\n\t};\n\tstruct sk_buff *frag_iter;\n\tstruct sock *sk = skb->sk;\n\tint ret = 0;\n\n\t/*\n\t * __skb_splice_bits() only fails if the output has no room left,\n\t * so no point in going over the frag_list for the error case.\n\t */\n\tif (__skb_splice_bits(skb, pipe, &offset, &tlen, &spd, sk))\n\t\tgoto done;\n\telse if (!tlen)\n\t\tgoto done;\n\n\t/*\n\t * now see if we have a frag_list to map\n\t */\n\tskb_walk_frags(skb, frag_iter) {\n\t\tif (!tlen)\n\t\t\tbreak;\n\t\tif (__skb_splice_bits(frag_iter, pipe, &offset, &tlen, &spd, sk))\n\t\t\tbreak;\n\t}\n\ndone:\n\tif (spd.nr_pages) {\n\t\t/*\n\t\t * Drop the socket lock, otherwise we have reverse\n\t\t * locking dependencies between sk_lock and i_mutex\n\t\t * here as compared to sendfile(). We enter here\n\t\t * with the socket lock held, and splice_to_pipe() will\n\t\t * grab the pipe inode lock. For sendfile() emulation,\n\t\t * we call into ->sendpage() with the i_mutex lock held\n\t\t * and networking will grab the socket lock.\n\t\t */\n\t\trelease_sock(sk);\n\t\tret = splice_to_pipe(pipe, &spd);\n\t\tlock_sock(sk);\n\t}\n\n\treturn ret;\n}\n\n/**\n *\tskb_store_bits - store bits from kernel buffer to skb\n *\t@skb: destination buffer\n *\t@offset: offset in destination\n *\t@from: source buffer\n *\t@len: number of bytes to copy\n *\n *\tCopy the specified number of bytes from the source buffer to the\n *\tdestination skb.  This function handles all the messy bits of\n *\ttraversing fragment lists and such.\n */\n\nint skb_store_bits(struct sk_buff *skb, int offset, const void *from, int len)\n{\n\tint start = skb_headlen(skb);\n\tstruct sk_buff *frag_iter;\n\tint i, copy;\n\n\tif (offset > (int)skb->len - len)\n\t\tgoto fault;\n\n\tif ((copy = start - offset) > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tskb_copy_to_linear_data_offset(skb, offset, from, copy);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn 0;\n\t\toffset += copy;\n\t\tfrom += copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(frag);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\n\t\t\tvaddr = kmap_atomic(skb_frag_page(frag));\n\t\t\tmemcpy(vaddr + frag->page_offset + offset - start,\n\t\t\t       from, copy);\n\t\t\tkunmap_atomic(vaddr);\n\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tfrom += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tif (skb_store_bits(frag_iter, offset - start,\n\t\t\t\t\t   from, copy))\n\t\t\t\tgoto fault;\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tfrom += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tif (!len)\n\t\treturn 0;\n\nfault:\n\treturn -EFAULT;\n}\nEXPORT_SYMBOL(skb_store_bits);\n\n/* Checksum skb data. */\n__wsum __skb_checksum(const struct sk_buff *skb, int offset, int len,\n\t\t      __wsum csum, const struct skb_checksum_ops *ops)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\tint pos = 0;\n\n\t/* Checksum header. */\n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tcsum = ops->update(skb->data + offset, copy, csum);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn csum;\n\t\toffset += copy;\n\t\tpos\t= copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(frag);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\t__wsum csum2;\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tvaddr = kmap_atomic(skb_frag_page(frag));\n\t\t\tcsum2 = ops->update(vaddr + frag->page_offset +\n\t\t\t\t\t    offset - start, copy, 0);\n\t\t\tkunmap_atomic(vaddr);\n\t\t\tcsum = ops->combine(csum, csum2, pos, copy);\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\t__wsum csum2;\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tcsum2 = __skb_checksum(frag_iter, offset - start,\n\t\t\t\t\t       copy, 0, ops);\n\t\t\tcsum = ops->combine(csum, csum2, pos, copy);\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tBUG_ON(len);\n\n\treturn csum;\n}\nEXPORT_SYMBOL(__skb_checksum);\n\n__wsum skb_checksum(const struct sk_buff *skb, int offset,\n\t\t    int len, __wsum csum)\n{\n\tconst struct skb_checksum_ops ops = {\n\t\t.update  = csum_partial_ext,\n\t\t.combine = csum_block_add_ext,\n\t};\n\n\treturn __skb_checksum(skb, offset, len, csum, &ops);\n}\nEXPORT_SYMBOL(skb_checksum);\n\n/* Both of above in one bottle. */\n\n__wsum skb_copy_and_csum_bits(const struct sk_buff *skb, int offset,\n\t\t\t\t    u8 *to, int len, __wsum csum)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\tint pos = 0;\n\n\t/* Copy header. */\n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tcsum = csum_partial_copy_nocheck(skb->data + offset, to,\n\t\t\t\t\t\t copy, csum);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn csum;\n\t\toffset += copy;\n\t\tto     += copy;\n\t\tpos\t= copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\t__wsum csum2;\n\t\t\tu8 *vaddr;\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tvaddr = kmap_atomic(skb_frag_page(frag));\n\t\t\tcsum2 = csum_partial_copy_nocheck(vaddr +\n\t\t\t\t\t\t\t  frag->page_offset +\n\t\t\t\t\t\t\t  offset - start, to,\n\t\t\t\t\t\t\t  copy, 0);\n\t\t\tkunmap_atomic(vaddr);\n\t\t\tcsum = csum_block_add(csum, csum2, pos);\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\t__wsum csum2;\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tcsum2 = skb_copy_and_csum_bits(frag_iter,\n\t\t\t\t\t\t       offset - start,\n\t\t\t\t\t\t       to, copy, 0);\n\t\t\tcsum = csum_block_add(csum, csum2, pos);\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tBUG_ON(len);\n\treturn csum;\n}\nEXPORT_SYMBOL(skb_copy_and_csum_bits);\n\n /**\n *\tskb_zerocopy_headlen - Calculate headroom needed for skb_zerocopy()\n *\t@from: source buffer\n *\n *\tCalculates the amount of linear headroom needed in the 'to' skb passed\n *\tinto skb_zerocopy().\n */\nunsigned int\nskb_zerocopy_headlen(const struct sk_buff *from)\n{\n\tunsigned int hlen = 0;\n\n\tif (!from->head_frag ||\n\t    skb_headlen(from) < L1_CACHE_BYTES ||\n\t    skb_shinfo(from)->nr_frags >= MAX_SKB_FRAGS)\n\t\thlen = skb_headlen(from);\n\n\tif (skb_has_frag_list(from))\n\t\thlen = from->len;\n\n\treturn hlen;\n}\nEXPORT_SYMBOL_GPL(skb_zerocopy_headlen);\n\n/**\n *\tskb_zerocopy - Zero copy skb to skb\n *\t@to: destination buffer\n *\t@from: source buffer\n *\t@len: number of bytes to copy from source buffer\n *\t@hlen: size of linear headroom in destination buffer\n *\n *\tCopies up to `len` bytes from `from` to `to` by creating references\n *\tto the frags in the source buffer.\n *\n *\tThe `hlen` as calculated by skb_zerocopy_headlen() specifies the\n *\theadroom in the `to` buffer.\n */\nvoid\nskb_zerocopy(struct sk_buff *to, const struct sk_buff *from, int len, int hlen)\n{\n\tint i, j = 0;\n\tint plen = 0; /* length of skb->head fragment */\n\tstruct page *page;\n\tunsigned int offset;\n\n\tBUG_ON(!from->head_frag && !hlen);\n\n\t/* dont bother with small payloads */\n\tif (len <= skb_tailroom(to)) {\n\t\tskb_copy_bits(from, 0, skb_put(to, len), len);\n\t\treturn;\n\t}\n\n\tif (hlen) {\n\t\tskb_copy_bits(from, 0, skb_put(to, hlen), hlen);\n\t\tlen -= hlen;\n\t} else {\n\t\tplen = min_t(int, skb_headlen(from), len);\n\t\tif (plen) {\n\t\t\tpage = virt_to_head_page(from->head);\n\t\t\toffset = from->data - (unsigned char *)page_address(page);\n\t\t\t__skb_fill_page_desc(to, 0, page, offset, plen);\n\t\t\tget_page(page);\n\t\t\tj = 1;\n\t\t\tlen -= plen;\n\t\t}\n\t}\n\n\tto->truesize += len + plen;\n\tto->len += len + plen;\n\tto->data_len += len + plen;\n\n\tfor (i = 0; i < skb_shinfo(from)->nr_frags; i++) {\n\t\tif (!len)\n\t\t\tbreak;\n\t\tskb_shinfo(to)->frags[j] = skb_shinfo(from)->frags[i];\n\t\tskb_shinfo(to)->frags[j].size = min_t(int, skb_shinfo(to)->frags[j].size, len);\n\t\tlen -= skb_shinfo(to)->frags[j].size;\n\t\tskb_frag_ref(to, j);\n\t\tj++;\n\t}\n\tskb_shinfo(to)->nr_frags = j;\n}\nEXPORT_SYMBOL_GPL(skb_zerocopy);\n\nvoid skb_copy_and_csum_dev(const struct sk_buff *skb, u8 *to)\n{\n\t__wsum csum;\n\tlong csstart;\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tcsstart = skb_checksum_start_offset(skb);\n\telse\n\t\tcsstart = skb_headlen(skb);\n\n\tBUG_ON(csstart > skb_headlen(skb));\n\n\tskb_copy_from_linear_data(skb, to, csstart);\n\n\tcsum = 0;\n\tif (csstart != skb->len)\n\t\tcsum = skb_copy_and_csum_bits(skb, csstart, to + csstart,\n\t\t\t\t\t      skb->len - csstart, 0);\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tlong csstuff = csstart + skb->csum_offset;\n\n\t\t*((__sum16 *)(to + csstuff)) = csum_fold(csum);\n\t}\n}\nEXPORT_SYMBOL(skb_copy_and_csum_dev);\n\n/**\n *\tskb_dequeue - remove from the head of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the head of the list. The list lock is taken so the function\n *\tmay be used safely with other locking list functions. The head item is\n *\treturned or %NULL if the list is empty.\n */\n\nstruct sk_buff *skb_dequeue(struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\tstruct sk_buff *result;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tresult = __skb_dequeue(list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n\treturn result;\n}\nEXPORT_SYMBOL(skb_dequeue);\n\n/**\n *\tskb_dequeue_tail - remove from the tail of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the tail of the list. The list lock is taken so the function\n *\tmay be used safely with other locking list functions. The tail item is\n *\treturned or %NULL if the list is empty.\n */\nstruct sk_buff *skb_dequeue_tail(struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\tstruct sk_buff *result;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tresult = __skb_dequeue_tail(list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n\treturn result;\n}\nEXPORT_SYMBOL(skb_dequeue_tail);\n\n/**\n *\tskb_queue_purge - empty a list\n *\t@list: list to empty\n *\n *\tDelete all buffers on an &sk_buff list. Each buffer is removed from\n *\tthe list and one reference dropped. This function takes the list\n *\tlock and is atomic with respect to other list locking functions.\n */\nvoid skb_queue_purge(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb;\n\twhile ((skb = skb_dequeue(list)) != NULL)\n\t\tkfree_skb(skb);\n}\nEXPORT_SYMBOL(skb_queue_purge);\n\n/**\n *\tskb_queue_head - queue a buffer at the list head\n *\t@list: list to use\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer at the start of the list. This function takes the\n *\tlist lock and can be used safely with other locking &sk_buff functions\n *\tsafely.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_queue_head(struct sk_buff_head *list, struct sk_buff *newsk)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_queue_head(list, newsk);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_queue_head);\n\n/**\n *\tskb_queue_tail - queue a buffer at the list tail\n *\t@list: list to use\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer at the tail of the list. This function takes the\n *\tlist lock and can be used safely with other locking &sk_buff functions\n *\tsafely.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_queue_tail(struct sk_buff_head *list, struct sk_buff *newsk)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_queue_tail(list, newsk);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_queue_tail);\n\n/**\n *\tskb_unlink\t-\tremove a buffer from a list\n *\t@skb: buffer to remove\n *\t@list: list to use\n *\n *\tRemove a packet from a list. The list locks are taken and this\n *\tfunction is atomic with respect to other list locked calls\n *\n *\tYou must know what list the SKB is on.\n */\nvoid skb_unlink(struct sk_buff *skb, struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_unlink(skb, list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_unlink);\n\n/**\n *\tskb_append\t-\tappend a buffer\n *\t@old: buffer to insert after\n *\t@newsk: buffer to insert\n *\t@list: list to use\n *\n *\tPlace a packet after a given packet in a list. The list locks are taken\n *\tand this function is atomic with respect to other list locked calls.\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_append(struct sk_buff *old, struct sk_buff *newsk, struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_queue_after(list, old, newsk);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_append);\n\n/**\n *\tskb_insert\t-\tinsert a buffer\n *\t@old: buffer to insert before\n *\t@newsk: buffer to insert\n *\t@list: list to use\n *\n *\tPlace a packet before a given packet in a list. The list locks are\n * \ttaken and this function is atomic with respect to other list locked\n *\tcalls.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_insert(struct sk_buff *old, struct sk_buff *newsk, struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_insert(newsk, old->prev, old, list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_insert);\n\nstatic inline void skb_split_inside_header(struct sk_buff *skb,\n\t\t\t\t\t   struct sk_buff* skb1,\n\t\t\t\t\t   const u32 len, const int pos)\n{\n\tint i;\n\n\tskb_copy_from_linear_data_offset(skb, len, skb_put(skb1, pos - len),\n\t\t\t\t\t pos - len);\n\t/* And move data appendix as is. */\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\tskb_shinfo(skb1)->frags[i] = skb_shinfo(skb)->frags[i];\n\n\tskb_shinfo(skb1)->nr_frags = skb_shinfo(skb)->nr_frags;\n\tskb_shinfo(skb)->nr_frags  = 0;\n\tskb1->data_len\t\t   = skb->data_len;\n\tskb1->len\t\t   += skb1->data_len;\n\tskb->data_len\t\t   = 0;\n\tskb->len\t\t   = len;\n\tskb_set_tail_pointer(skb, len);\n}\n\nstatic inline void skb_split_no_header(struct sk_buff *skb,\n\t\t\t\t       struct sk_buff* skb1,\n\t\t\t\t       const u32 len, int pos)\n{\n\tint i, k = 0;\n\tconst int nfrags = skb_shinfo(skb)->nr_frags;\n\n\tskb_shinfo(skb)->nr_frags = 0;\n\tskb1->len\t\t  = skb1->data_len = skb->len - len;\n\tskb->len\t\t  = len;\n\tskb->data_len\t\t  = len - pos;\n\n\tfor (i = 0; i < nfrags; i++) {\n\t\tint size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (pos + size > len) {\n\t\t\tskb_shinfo(skb1)->frags[k] = skb_shinfo(skb)->frags[i];\n\n\t\t\tif (pos < len) {\n\t\t\t\t/* Split frag.\n\t\t\t\t * We have two variants in this case:\n\t\t\t\t * 1. Move all the frag to the second\n\t\t\t\t *    part, if it is possible. F.e.\n\t\t\t\t *    this approach is mandatory for TUX,\n\t\t\t\t *    where splitting is expensive.\n\t\t\t\t * 2. Split is accurately. We make this.\n\t\t\t\t */\n\t\t\t\tskb_frag_ref(skb, i);\n\t\t\t\tskb_shinfo(skb1)->frags[0].page_offset += len - pos;\n\t\t\t\tskb_frag_size_sub(&skb_shinfo(skb1)->frags[0], len - pos);\n\t\t\t\tskb_frag_size_set(&skb_shinfo(skb)->frags[i], len - pos);\n\t\t\t\tskb_shinfo(skb)->nr_frags++;\n\t\t\t}\n\t\t\tk++;\n\t\t} else\n\t\t\tskb_shinfo(skb)->nr_frags++;\n\t\tpos += size;\n\t}\n\tskb_shinfo(skb1)->nr_frags = k;\n}\n\n/**\n * skb_split - Split fragmented skb to two parts at length len.\n * @skb: the buffer to split\n * @skb1: the buffer to receive the second part\n * @len: new length for skb\n */\nvoid skb_split(struct sk_buff *skb, struct sk_buff *skb1, const u32 len)\n{\n\tint pos = skb_headlen(skb);\n\n\tskb_shinfo(skb1)->tx_flags = skb_shinfo(skb)->tx_flags & SKBTX_SHARED_FRAG;\n\tif (len < pos)\t/* Split line is inside header. */\n\t\tskb_split_inside_header(skb, skb1, len, pos);\n\telse\t\t/* Second chunk has no header, nothing to copy. */\n\t\tskb_split_no_header(skb, skb1, len, pos);\n}\nEXPORT_SYMBOL(skb_split);\n\n/* Shifting from/to a cloned skb is a no-go.\n *\n * Caller cannot keep skb_shinfo related pointers past calling here!\n */\nstatic int skb_prepare_for_shift(struct sk_buff *skb)\n{\n\treturn skb_cloned(skb) && pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n}\n\n/**\n * skb_shift - Shifts paged data partially from skb to another\n * @tgt: buffer into which tail data gets added\n * @skb: buffer from which the paged data comes from\n * @shiftlen: shift up to this many bytes\n *\n * Attempts to shift up to shiftlen worth of bytes, which may be less than\n * the length of the skb, from skb to tgt. Returns number bytes shifted.\n * It's up to caller to free skb if everything was shifted.\n *\n * If @tgt runs out of frags, the whole operation is aborted.\n *\n * Skb cannot include anything else but paged data while tgt is allowed\n * to have non-paged data as well.\n *\n * TODO: full sized shift could be optimized but that would need\n * specialized skb free'er to handle frags without up-to-date nr_frags.\n */\nint skb_shift(struct sk_buff *tgt, struct sk_buff *skb, int shiftlen)\n{\n\tint from, to, merge, todo;\n\tstruct skb_frag_struct *fragfrom, *fragto;\n\n\tBUG_ON(shiftlen > skb->len);\n\tBUG_ON(skb_headlen(skb));\t/* Would corrupt stream */\n\n\ttodo = shiftlen;\n\tfrom = 0;\n\tto = skb_shinfo(tgt)->nr_frags;\n\tfragfrom = &skb_shinfo(skb)->frags[from];\n\n\t/* Actual merge is delayed until the point when we know we can\n\t * commit all, so that we don't have to undo partial changes\n\t */\n\tif (!to ||\n\t    !skb_can_coalesce(tgt, to, skb_frag_page(fragfrom),\n\t\t\t      fragfrom->page_offset)) {\n\t\tmerge = -1;\n\t} else {\n\t\tmerge = to - 1;\n\n\t\ttodo -= skb_frag_size(fragfrom);\n\t\tif (todo < 0) {\n\t\t\tif (skb_prepare_for_shift(skb) ||\n\t\t\t    skb_prepare_for_shift(tgt))\n\t\t\t\treturn 0;\n\n\t\t\t/* All previous frag pointers might be stale! */\n\t\t\tfragfrom = &skb_shinfo(skb)->frags[from];\n\t\t\tfragto = &skb_shinfo(tgt)->frags[merge];\n\n\t\t\tskb_frag_size_add(fragto, shiftlen);\n\t\t\tskb_frag_size_sub(fragfrom, shiftlen);\n\t\t\tfragfrom->page_offset += shiftlen;\n\n\t\t\tgoto onlymerged;\n\t\t}\n\n\t\tfrom++;\n\t}\n\n\t/* Skip full, not-fitting skb to avoid expensive operations */\n\tif ((shiftlen == skb->len) &&\n\t    (skb_shinfo(skb)->nr_frags - from) > (MAX_SKB_FRAGS - to))\n\t\treturn 0;\n\n\tif (skb_prepare_for_shift(skb) || skb_prepare_for_shift(tgt))\n\t\treturn 0;\n\n\twhile ((todo > 0) && (from < skb_shinfo(skb)->nr_frags)) {\n\t\tif (to == MAX_SKB_FRAGS)\n\t\t\treturn 0;\n\n\t\tfragfrom = &skb_shinfo(skb)->frags[from];\n\t\tfragto = &skb_shinfo(tgt)->frags[to];\n\n\t\tif (todo >= skb_frag_size(fragfrom)) {\n\t\t\t*fragto = *fragfrom;\n\t\t\ttodo -= skb_frag_size(fragfrom);\n\t\t\tfrom++;\n\t\t\tto++;\n\n\t\t} else {\n\t\t\t__skb_frag_ref(fragfrom);\n\t\t\tfragto->page = fragfrom->page;\n\t\t\tfragto->page_offset = fragfrom->page_offset;\n\t\t\tskb_frag_size_set(fragto, todo);\n\n\t\t\tfragfrom->page_offset += todo;\n\t\t\tskb_frag_size_sub(fragfrom, todo);\n\t\t\ttodo = 0;\n\n\t\t\tto++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Ready to \"commit\" this state change to tgt */\n\tskb_shinfo(tgt)->nr_frags = to;\n\n\tif (merge >= 0) {\n\t\tfragfrom = &skb_shinfo(skb)->frags[0];\n\t\tfragto = &skb_shinfo(tgt)->frags[merge];\n\n\t\tskb_frag_size_add(fragto, skb_frag_size(fragfrom));\n\t\t__skb_frag_unref(fragfrom);\n\t}\n\n\t/* Reposition in the original skb */\n\tto = 0;\n\twhile (from < skb_shinfo(skb)->nr_frags)\n\t\tskb_shinfo(skb)->frags[to++] = skb_shinfo(skb)->frags[from++];\n\tskb_shinfo(skb)->nr_frags = to;\n\n\tBUG_ON(todo > 0 && !skb_shinfo(skb)->nr_frags);\n\nonlymerged:\n\t/* Most likely the tgt won't ever need its checksum anymore, skb on\n\t * the other hand might need it if it needs to be resent\n\t */\n\ttgt->ip_summed = CHECKSUM_PARTIAL;\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\t/* Yak, is it really working this way? Some helper please? */\n\tskb->len -= shiftlen;\n\tskb->data_len -= shiftlen;\n\tskb->truesize -= shiftlen;\n\ttgt->len += shiftlen;\n\ttgt->data_len += shiftlen;\n\ttgt->truesize += shiftlen;\n\n\treturn shiftlen;\n}\n\n/**\n * skb_prepare_seq_read - Prepare a sequential read of skb data\n * @skb: the buffer to read\n * @from: lower offset of data to be read\n * @to: upper offset of data to be read\n * @st: state variable\n *\n * Initializes the specified state variable. Must be called before\n * invoking skb_seq_read() for the first time.\n */\nvoid skb_prepare_seq_read(struct sk_buff *skb, unsigned int from,\n\t\t\t  unsigned int to, struct skb_seq_state *st)\n{\n\tst->lower_offset = from;\n\tst->upper_offset = to;\n\tst->root_skb = st->cur_skb = skb;\n\tst->frag_idx = st->stepped_offset = 0;\n\tst->frag_data = NULL;\n}\nEXPORT_SYMBOL(skb_prepare_seq_read);\n\n/**\n * skb_seq_read - Sequentially read skb data\n * @consumed: number of bytes consumed by the caller so far\n * @data: destination pointer for data to be returned\n * @st: state variable\n *\n * Reads a block of skb data at @consumed relative to the\n * lower offset specified to skb_prepare_seq_read(). Assigns\n * the head of the data block to @data and returns the length\n * of the block or 0 if the end of the skb data or the upper\n * offset has been reached.\n *\n * The caller is not required to consume all of the data\n * returned, i.e. @consumed is typically set to the number\n * of bytes already consumed and the next call to\n * skb_seq_read() will return the remaining part of the block.\n *\n * Note 1: The size of each block of data returned can be arbitrary,\n *       this limitation is the cost for zerocopy seqeuental\n *       reads of potentially non linear data.\n *\n * Note 2: Fragment lists within fragments are not implemented\n *       at the moment, state->root_skb could be replaced with\n *       a stack for this purpose.\n */\nunsigned int skb_seq_read(unsigned int consumed, const u8 **data,\n\t\t\t  struct skb_seq_state *st)\n{\n\tunsigned int block_limit, abs_offset = consumed + st->lower_offset;\n\tskb_frag_t *frag;\n\n\tif (unlikely(abs_offset >= st->upper_offset)) {\n\t\tif (st->frag_data) {\n\t\t\tkunmap_atomic(st->frag_data);\n\t\t\tst->frag_data = NULL;\n\t\t}\n\t\treturn 0;\n\t}\n\nnext_skb:\n\tblock_limit = skb_headlen(st->cur_skb) + st->stepped_offset;\n\n\tif (abs_offset < block_limit && !st->frag_data) {\n\t\t*data = st->cur_skb->data + (abs_offset - st->stepped_offset);\n\t\treturn block_limit - abs_offset;\n\t}\n\n\tif (st->frag_idx == 0 && !st->frag_data)\n\t\tst->stepped_offset += skb_headlen(st->cur_skb);\n\n\twhile (st->frag_idx < skb_shinfo(st->cur_skb)->nr_frags) {\n\t\tfrag = &skb_shinfo(st->cur_skb)->frags[st->frag_idx];\n\t\tblock_limit = skb_frag_size(frag) + st->stepped_offset;\n\n\t\tif (abs_offset < block_limit) {\n\t\t\tif (!st->frag_data)\n\t\t\t\tst->frag_data = kmap_atomic(skb_frag_page(frag));\n\n\t\t\t*data = (u8 *) st->frag_data + frag->page_offset +\n\t\t\t\t(abs_offset - st->stepped_offset);\n\n\t\t\treturn block_limit - abs_offset;\n\t\t}\n\n\t\tif (st->frag_data) {\n\t\t\tkunmap_atomic(st->frag_data);\n\t\t\tst->frag_data = NULL;\n\t\t}\n\n\t\tst->frag_idx++;\n\t\tst->stepped_offset += skb_frag_size(frag);\n\t}\n\n\tif (st->frag_data) {\n\t\tkunmap_atomic(st->frag_data);\n\t\tst->frag_data = NULL;\n\t}\n\n\tif (st->root_skb == st->cur_skb && skb_has_frag_list(st->root_skb)) {\n\t\tst->cur_skb = skb_shinfo(st->root_skb)->frag_list;\n\t\tst->frag_idx = 0;\n\t\tgoto next_skb;\n\t} else if (st->cur_skb->next) {\n\t\tst->cur_skb = st->cur_skb->next;\n\t\tst->frag_idx = 0;\n\t\tgoto next_skb;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_seq_read);\n\n/**\n * skb_abort_seq_read - Abort a sequential read of skb data\n * @st: state variable\n *\n * Must be called if skb_seq_read() was not called until it\n * returned 0.\n */\nvoid skb_abort_seq_read(struct skb_seq_state *st)\n{\n\tif (st->frag_data)\n\t\tkunmap_atomic(st->frag_data);\n}\nEXPORT_SYMBOL(skb_abort_seq_read);\n\n#define TS_SKB_CB(state)\t((struct skb_seq_state *) &((state)->cb))\n\nstatic unsigned int skb_ts_get_next_block(unsigned int offset, const u8 **text,\n\t\t\t\t\t  struct ts_config *conf,\n\t\t\t\t\t  struct ts_state *state)\n{\n\treturn skb_seq_read(offset, text, TS_SKB_CB(state));\n}\n\nstatic void skb_ts_finish(struct ts_config *conf, struct ts_state *state)\n{\n\tskb_abort_seq_read(TS_SKB_CB(state));\n}\n\n/**\n * skb_find_text - Find a text pattern in skb data\n * @skb: the buffer to look in\n * @from: search offset\n * @to: search limit\n * @config: textsearch configuration\n * @state: uninitialized textsearch state variable\n *\n * Finds a pattern in the skb data according to the specified\n * textsearch configuration. Use textsearch_next() to retrieve\n * subsequent occurrences of the pattern. Returns the offset\n * to the first occurrence or UINT_MAX if no match was found.\n */\nunsigned int skb_find_text(struct sk_buff *skb, unsigned int from,\n\t\t\t   unsigned int to, struct ts_config *config,\n\t\t\t   struct ts_state *state)\n{\n\tunsigned int ret;\n\n\tconfig->get_next_block = skb_ts_get_next_block;\n\tconfig->finish = skb_ts_finish;\n\n\tskb_prepare_seq_read(skb, from, to, TS_SKB_CB(state));\n\n\tret = textsearch_find(config, state);\n\treturn (ret <= to - from ? ret : UINT_MAX);\n}\nEXPORT_SYMBOL(skb_find_text);\n\n/**\n * skb_append_datato_frags - append the user data to a skb\n * @sk: sock  structure\n * @skb: skb structure to be appened with user data.\n * @getfrag: call back function to be used for getting the user data\n * @from: pointer to user message iov\n * @length: length of the iov message\n *\n * Description: This procedure append the user data in the fragment part\n * of the skb if any page alloc fails user this procedure returns  -ENOMEM\n */\nint skb_append_datato_frags(struct sock *sk, struct sk_buff *skb,\n\t\t\tint (*getfrag)(void *from, char *to, int offset,\n\t\t\t\t\tint len, int odd, struct sk_buff *skb),\n\t\t\tvoid *from, int length)\n{\n\tint frg_cnt = skb_shinfo(skb)->nr_frags;\n\tint copy;\n\tint offset = 0;\n\tint ret;\n\tstruct page_frag *pfrag = &current->task_frag;\n\n\tdo {\n\t\t/* Return error if we don't have space for new frag */\n\t\tif (frg_cnt >= MAX_SKB_FRAGS)\n\t\t\treturn -EMSGSIZE;\n\n\t\tif (!sk_page_frag_refill(sk, pfrag))\n\t\t\treturn -ENOMEM;\n\n\t\t/* copy the user data to page */\n\t\tcopy = min_t(int, length, pfrag->size - pfrag->offset);\n\n\t\tret = getfrag(from, page_address(pfrag->page) + pfrag->offset,\n\t\t\t      offset, copy, 0, skb);\n\t\tif (ret < 0)\n\t\t\treturn -EFAULT;\n\n\t\t/* copy was successful so update the size parameters */\n\t\tskb_fill_page_desc(skb, frg_cnt, pfrag->page, pfrag->offset,\n\t\t\t\t   copy);\n\t\tfrg_cnt++;\n\t\tpfrag->offset += copy;\n\t\tget_page(pfrag->page);\n\n\t\tskb->truesize += copy;\n\t\tatomic_add(copy, &sk->sk_wmem_alloc);\n\t\tskb->len += copy;\n\t\tskb->data_len += copy;\n\t\toffset += copy;\n\t\tlength -= copy;\n\n\t} while (length > 0);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_append_datato_frags);\n\n/**\n *\tskb_pull_rcsum - pull skb and update receive checksum\n *\t@skb: buffer to update\n *\t@len: length of data pulled\n *\n *\tThis function performs an skb_pull on the packet and updates\n *\tthe CHECKSUM_COMPLETE checksum.  It should be used on\n *\treceive path processing instead of skb_pull unless you know\n *\tthat the checksum difference is zero (e.g., a valid IP header)\n *\tor you are setting ip_summed to CHECKSUM_NONE.\n */\nunsigned char *skb_pull_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tBUG_ON(len > skb->len);\n\tskb->len -= len;\n\tBUG_ON(skb->len < skb->data_len);\n\tskb_postpull_rcsum(skb, skb->data, len);\n\treturn skb->data += len;\n}\nEXPORT_SYMBOL_GPL(skb_pull_rcsum);\n\n/**\n *\tskb_segment - Perform protocol segmentation on skb.\n *\t@head_skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n *\n *\tThis function performs segmentation on the given skb.  It returns\n *\ta pointer to the first in a list of new skbs for the segments.\n *\tIn case of error it returns ERR_PTR(err).\n */\nstruct sk_buff *skb_segment(struct sk_buff *head_skb,\n\t\t\t    netdev_features_t features)\n{\n\tstruct sk_buff *segs = NULL;\n\tstruct sk_buff *tail = NULL;\n\tstruct sk_buff *list_skb = skb_shinfo(head_skb)->frag_list;\n\tskb_frag_t *frag = skb_shinfo(head_skb)->frags;\n\tunsigned int mss = skb_shinfo(head_skb)->gso_size;\n\tunsigned int doffset = head_skb->data - skb_mac_header(head_skb);\n\tunsigned int offset = doffset;\n\tunsigned int tnl_hlen = skb_tnl_header_len(head_skb);\n\tunsigned int headroom;\n\tunsigned int len;\n\t__be16 proto;\n\tbool csum;\n\tint sg = !!(features & NETIF_F_SG);\n\tint nfrags = skb_shinfo(head_skb)->nr_frags;\n\tint err = -ENOMEM;\n\tint i = 0;\n\tint pos;\n\n\tproto = skb_network_protocol(head_skb);\n\tif (unlikely(!proto))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tcsum = !!can_checksum_protocol(features, proto);\n\t__skb_push(head_skb, doffset);\n\theadroom = skb_headroom(head_skb);\n\tpos = skb_headlen(head_skb);\n\n\tdo {\n\t\tstruct sk_buff *nskb;\n\t\tskb_frag_t *nskb_frag;\n\t\tint hsize;\n\t\tint size;\n\n\t\tlen = head_skb->len - offset;\n\t\tif (len > mss)\n\t\t\tlen = mss;\n\n\t\thsize = skb_headlen(head_skb) - offset;\n\t\tif (hsize < 0)\n\t\t\thsize = 0;\n\t\tif (hsize > len || !sg)\n\t\t\thsize = len;\n\n\t\tif (!hsize && i >= nfrags && skb_headlen(list_skb) &&\n\t\t    (skb_headlen(list_skb) == len || sg)) {\n\t\t\tBUG_ON(skb_headlen(list_skb) > len);\n\n\t\t\ti = 0;\n\t\t\tnfrags = skb_shinfo(list_skb)->nr_frags;\n\t\t\tfrag = skb_shinfo(list_skb)->frags;\n\t\t\tpos += skb_headlen(list_skb);\n\n\t\t\twhile (pos < offset + len) {\n\t\t\t\tBUG_ON(i >= nfrags);\n\n\t\t\t\tsize = skb_frag_size(frag);\n\t\t\t\tif (pos + size > offset + len)\n\t\t\t\t\tbreak;\n\n\t\t\t\ti++;\n\t\t\t\tpos += size;\n\t\t\t\tfrag++;\n\t\t\t}\n\n\t\t\tnskb = skb_clone(list_skb, GFP_ATOMIC);\n\t\t\tlist_skb = list_skb->next;\n\n\t\t\tif (unlikely(!nskb))\n\t\t\t\tgoto err;\n\n\t\t\tif (unlikely(pskb_trim(nskb, len))) {\n\t\t\t\tkfree_skb(nskb);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\thsize = skb_end_offset(nskb);\n\t\t\tif (skb_cow_head(nskb, doffset + headroom)) {\n\t\t\t\tkfree_skb(nskb);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tnskb->truesize += skb_end_offset(nskb) - hsize;\n\t\t\tskb_release_head_state(nskb);\n\t\t\t__skb_push(nskb, doffset);\n\t\t} else {\n\t\t\tnskb = __alloc_skb(hsize + doffset + headroom,\n\t\t\t\t\t   GFP_ATOMIC, skb_alloc_rx_flag(head_skb),\n\t\t\t\t\t   NUMA_NO_NODE);\n\n\t\t\tif (unlikely(!nskb))\n\t\t\t\tgoto err;\n\n\t\t\tskb_reserve(nskb, headroom);\n\t\t\t__skb_put(nskb, doffset);\n\t\t}\n\n\t\tif (segs)\n\t\t\ttail->next = nskb;\n\t\telse\n\t\t\tsegs = nskb;\n\t\ttail = nskb;\n\n\t\t__copy_skb_header(nskb, head_skb);\n\t\tnskb->mac_len = head_skb->mac_len;\n\n\t\tskb_headers_offset_update(nskb, skb_headroom(nskb) - headroom);\n\n\t\tskb_copy_from_linear_data_offset(head_skb, -tnl_hlen,\n\t\t\t\t\t\t nskb->data - tnl_hlen,\n\t\t\t\t\t\t doffset + tnl_hlen);\n\n\t\tif (nskb->len == len + doffset)\n\t\t\tgoto perform_csum_check;\n\n\t\tif (!sg) {\n\t\t\tnskb->ip_summed = CHECKSUM_NONE;\n\t\t\tnskb->csum = skb_copy_and_csum_bits(head_skb, offset,\n\t\t\t\t\t\t\t    skb_put(nskb, len),\n\t\t\t\t\t\t\t    len, 0);\n\t\t\tcontinue;\n\t\t}\n\n\t\tnskb_frag = skb_shinfo(nskb)->frags;\n\n\t\tskb_copy_from_linear_data_offset(head_skb, offset,\n\t\t\t\t\t\t skb_put(nskb, hsize), hsize);\n\n\t\tskb_shinfo(nskb)->tx_flags = skb_shinfo(head_skb)->tx_flags &\n\t\t\tSKBTX_SHARED_FRAG;\n\n\t\twhile (pos < offset + len) {\n\t\t\tif (i >= nfrags) {\n\t\t\t\tBUG_ON(skb_headlen(list_skb));\n\n\t\t\t\ti = 0;\n\t\t\t\tnfrags = skb_shinfo(list_skb)->nr_frags;\n\t\t\t\tfrag = skb_shinfo(list_skb)->frags;\n\n\t\t\t\tBUG_ON(!nfrags);\n\n\t\t\t\tlist_skb = list_skb->next;\n\t\t\t}\n\n\t\t\tif (unlikely(skb_shinfo(nskb)->nr_frags >=\n\t\t\t\t     MAX_SKB_FRAGS)) {\n\t\t\t\tnet_warn_ratelimited(\n\t\t\t\t\t\"skb_segment: too many frags: %u %u\\n\",\n\t\t\t\t\tpos, mss);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\t*nskb_frag = *frag;\n\t\t\t__skb_frag_ref(nskb_frag);\n\t\t\tsize = skb_frag_size(nskb_frag);\n\n\t\t\tif (pos < offset) {\n\t\t\t\tnskb_frag->page_offset += offset - pos;\n\t\t\t\tskb_frag_size_sub(nskb_frag, offset - pos);\n\t\t\t}\n\n\t\t\tskb_shinfo(nskb)->nr_frags++;\n\n\t\t\tif (pos + size <= offset + len) {\n\t\t\t\ti++;\n\t\t\t\tfrag++;\n\t\t\t\tpos += size;\n\t\t\t} else {\n\t\t\t\tskb_frag_size_sub(nskb_frag, pos + size - (offset + len));\n\t\t\t\tgoto skip_fraglist;\n\t\t\t}\n\n\t\t\tnskb_frag++;\n\t\t}\n\nskip_fraglist:\n\t\tnskb->data_len = len - hsize;\n\t\tnskb->len += nskb->data_len;\n\t\tnskb->truesize += nskb->data_len;\n\nperform_csum_check:\n\t\tif (!csum) {\n\t\t\tnskb->csum = skb_checksum(nskb, doffset,\n\t\t\t\t\t\t  nskb->len - doffset, 0);\n\t\t\tnskb->ip_summed = CHECKSUM_NONE;\n\t\t}\n\t} while ((offset += len) < head_skb->len);\n\n\treturn segs;\n\nerr:\n\tkfree_skb_list(segs);\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(skb_segment);\n\nint skb_gro_receive(struct sk_buff **head, struct sk_buff *skb)\n{\n\tstruct skb_shared_info *pinfo, *skbinfo = skb_shinfo(skb);\n\tunsigned int offset = skb_gro_offset(skb);\n\tunsigned int headlen = skb_headlen(skb);\n\tstruct sk_buff *nskb, *lp, *p = *head;\n\tunsigned int len = skb_gro_len(skb);\n\tunsigned int delta_truesize;\n\tunsigned int headroom;\n\n\tif (unlikely(p->len + len >= 65536))\n\t\treturn -E2BIG;\n\n\tlp = NAPI_GRO_CB(p)->last ?: p;\n\tpinfo = skb_shinfo(lp);\n\n\tif (headlen <= offset) {\n\t\tskb_frag_t *frag;\n\t\tskb_frag_t *frag2;\n\t\tint i = skbinfo->nr_frags;\n\t\tint nr_frags = pinfo->nr_frags + i;\n\n\t\tif (nr_frags > MAX_SKB_FRAGS)\n\t\t\tgoto merge;\n\n\t\toffset -= headlen;\n\t\tpinfo->nr_frags = nr_frags;\n\t\tskbinfo->nr_frags = 0;\n\n\t\tfrag = pinfo->frags + nr_frags;\n\t\tfrag2 = skbinfo->frags + i;\n\t\tdo {\n\t\t\t*--frag = *--frag2;\n\t\t} while (--i);\n\n\t\tfrag->page_offset += offset;\n\t\tskb_frag_size_sub(frag, offset);\n\n\t\t/* all fragments truesize : remove (head size + sk_buff) */\n\t\tdelta_truesize = skb->truesize -\n\t\t\t\t SKB_TRUESIZE(skb_end_offset(skb));\n\n\t\tskb->truesize -= skb->data_len;\n\t\tskb->len -= skb->data_len;\n\t\tskb->data_len = 0;\n\n\t\tNAPI_GRO_CB(skb)->free = NAPI_GRO_FREE;\n\t\tgoto done;\n\t} else if (skb->head_frag) {\n\t\tint nr_frags = pinfo->nr_frags;\n\t\tskb_frag_t *frag = pinfo->frags + nr_frags;\n\t\tstruct page *page = virt_to_head_page(skb->head);\n\t\tunsigned int first_size = headlen - offset;\n\t\tunsigned int first_offset;\n\n\t\tif (nr_frags + 1 + skbinfo->nr_frags > MAX_SKB_FRAGS)\n\t\t\tgoto merge;\n\n\t\tfirst_offset = skb->data -\n\t\t\t       (unsigned char *)page_address(page) +\n\t\t\t       offset;\n\n\t\tpinfo->nr_frags = nr_frags + 1 + skbinfo->nr_frags;\n\n\t\tfrag->page.p\t  = page;\n\t\tfrag->page_offset = first_offset;\n\t\tskb_frag_size_set(frag, first_size);\n\n\t\tmemcpy(frag + 1, skbinfo->frags, sizeof(*frag) * skbinfo->nr_frags);\n\t\t/* We dont need to clear skbinfo->nr_frags here */\n\n\t\tdelta_truesize = skb->truesize - SKB_DATA_ALIGN(sizeof(struct sk_buff));\n\t\tNAPI_GRO_CB(skb)->free = NAPI_GRO_FREE_STOLEN_HEAD;\n\t\tgoto done;\n\t}\n\tif (pinfo->frag_list)\n\t\tgoto merge;\n\tif (skb_gro_len(p) != pinfo->gso_size)\n\t\treturn -E2BIG;\n\n\theadroom = skb_headroom(p);\n\tnskb = alloc_skb(headroom + skb_gro_offset(p), GFP_ATOMIC);\n\tif (unlikely(!nskb))\n\t\treturn -ENOMEM;\n\n\t__copy_skb_header(nskb, p);\n\tnskb->mac_len = p->mac_len;\n\n\tskb_reserve(nskb, headroom);\n\t__skb_put(nskb, skb_gro_offset(p));\n\n\tskb_set_mac_header(nskb, skb_mac_header(p) - p->data);\n\tskb_set_network_header(nskb, skb_network_offset(p));\n\tskb_set_transport_header(nskb, skb_transport_offset(p));\n\n\t__skb_pull(p, skb_gro_offset(p));\n\tmemcpy(skb_mac_header(nskb), skb_mac_header(p),\n\t       p->data - skb_mac_header(p));\n\n\tskb_shinfo(nskb)->frag_list = p;\n\tskb_shinfo(nskb)->gso_size = pinfo->gso_size;\n\tpinfo->gso_size = 0;\n\tskb_header_release(p);\n\tNAPI_GRO_CB(nskb)->last = p;\n\n\tnskb->data_len += p->len;\n\tnskb->truesize += p->truesize;\n\tnskb->len += p->len;\n\n\t*head = nskb;\n\tnskb->next = p->next;\n\tp->next = NULL;\n\n\tp = nskb;\n\nmerge:\n\tdelta_truesize = skb->truesize;\n\tif (offset > headlen) {\n\t\tunsigned int eat = offset - headlen;\n\n\t\tskbinfo->frags[0].page_offset += eat;\n\t\tskb_frag_size_sub(&skbinfo->frags[0], eat);\n\t\tskb->data_len -= eat;\n\t\tskb->len -= eat;\n\t\toffset = headlen;\n\t}\n\n\t__skb_pull(skb, offset);\n\n\tif (!NAPI_GRO_CB(p)->last)\n\t\tskb_shinfo(p)->frag_list = skb;\n\telse\n\t\tNAPI_GRO_CB(p)->last->next = skb;\n\tNAPI_GRO_CB(p)->last = skb;\n\tskb_header_release(skb);\n\tlp = p;\n\ndone:\n\tNAPI_GRO_CB(p)->count++;\n\tp->data_len += len;\n\tp->truesize += delta_truesize;\n\tp->len += len;\n\tif (lp != p) {\n\t\tlp->data_len += len;\n\t\tlp->truesize += delta_truesize;\n\t\tlp->len += len;\n\t}\n\tNAPI_GRO_CB(skb)->same_flow = 1;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_gro_receive);\n\nvoid __init skb_init(void)\n{\n\tskbuff_head_cache = kmem_cache_create(\"skbuff_head_cache\",\n\t\t\t\t\t      sizeof(struct sk_buff),\n\t\t\t\t\t      0,\n\t\t\t\t\t      SLAB_HWCACHE_ALIGN|SLAB_PANIC,\n\t\t\t\t\t      NULL);\n\tskbuff_fclone_cache = kmem_cache_create(\"skbuff_fclone_cache\",\n\t\t\t\t\t\t(2*sizeof(struct sk_buff)) +\n\t\t\t\t\t\tsizeof(atomic_t),\n\t\t\t\t\t\t0,\n\t\t\t\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC,\n\t\t\t\t\t\tNULL);\n}\n\n/**\n *\tskb_to_sgvec - Fill a scatter-gather list from a socket buffer\n *\t@skb: Socket buffer containing the buffers to be mapped\n *\t@sg: The scatter-gather list to map into\n *\t@offset: The offset into the buffer's contents to start mapping\n *\t@len: Length of buffer space to be mapped\n *\n *\tFill the specified scatter-gather list with mappings/pointers into a\n *\tregion of the buffer space attached to a socket buffer.\n */\nstatic int\n__skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg, int offset, int len)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\tint elt = 0;\n\n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tsg_set_buf(sg, skb->data + offset, copy);\n\t\telt++;\n\t\tif ((len -= copy) == 0)\n\t\t\treturn elt;\n\t\toffset += copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tsg_set_page(&sg[elt], skb_frag_page(frag), copy,\n\t\t\t\t\tfrag->page_offset+offset-start);\n\t\t\telt++;\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn elt;\n\t\t\toffset += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\telt += __skb_to_sgvec(frag_iter, sg+elt, offset - start,\n\t\t\t\t\t      copy);\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn elt;\n\t\t\toffset += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tBUG_ON(len);\n\treturn elt;\n}\n\nint skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg, int offset, int len)\n{\n\tint nsg = __skb_to_sgvec(skb, sg, offset, len);\n\n\tsg_mark_end(&sg[nsg - 1]);\n\n\treturn nsg;\n}\nEXPORT_SYMBOL_GPL(skb_to_sgvec);\n\n/**\n *\tskb_cow_data - Check that a socket buffer's data buffers are writable\n *\t@skb: The socket buffer to check.\n *\t@tailbits: Amount of trailing space to be added\n *\t@trailer: Returned pointer to the skb where the @tailbits space begins\n *\n *\tMake sure that the data buffers attached to a socket buffer are\n *\twritable. If they are not, private copies are made of the data buffers\n *\tand the socket buffer is set to use these instead.\n *\n *\tIf @tailbits is given, make sure that there is space to write @tailbits\n *\tbytes of data beyond current end of socket buffer.  @trailer will be\n *\tset to point to the skb in which this space begins.\n *\n *\tThe number of scatterlist elements required to completely map the\n *\tCOW'd and extended socket buffer will be returned.\n */\nint skb_cow_data(struct sk_buff *skb, int tailbits, struct sk_buff **trailer)\n{\n\tint copyflag;\n\tint elt;\n\tstruct sk_buff *skb1, **skb_p;\n\n\t/* If skb is cloned or its head is paged, reallocate\n\t * head pulling out all the pages (pages are considered not writable\n\t * at the moment even if they are anonymous).\n\t */\n\tif ((skb_cloned(skb) || skb_shinfo(skb)->nr_frags) &&\n\t    __pskb_pull_tail(skb, skb_pagelen(skb)-skb_headlen(skb)) == NULL)\n\t\treturn -ENOMEM;\n\n\t/* Easy case. Most of packets will go this way. */\n\tif (!skb_has_frag_list(skb)) {\n\t\t/* A little of trouble, not enough of space for trailer.\n\t\t * This should not happen, when stack is tuned to generate\n\t\t * good frames. OK, on miss we reallocate and reserve even more\n\t\t * space, 128 bytes is fair. */\n\n\t\tif (skb_tailroom(skb) < tailbits &&\n\t\t    pskb_expand_head(skb, 0, tailbits-skb_tailroom(skb)+128, GFP_ATOMIC))\n\t\t\treturn -ENOMEM;\n\n\t\t/* Voila! */\n\t\t*trailer = skb;\n\t\treturn 1;\n\t}\n\n\t/* Misery. We are in troubles, going to mincer fragments... */\n\n\telt = 1;\n\tskb_p = &skb_shinfo(skb)->frag_list;\n\tcopyflag = 0;\n\n\twhile ((skb1 = *skb_p) != NULL) {\n\t\tint ntail = 0;\n\n\t\t/* The fragment is partially pulled by someone,\n\t\t * this can happen on input. Copy it and everything\n\t\t * after it. */\n\n\t\tif (skb_shared(skb1))\n\t\t\tcopyflag = 1;\n\n\t\t/* If the skb is the last, worry about trailer. */\n\n\t\tif (skb1->next == NULL && tailbits) {\n\t\t\tif (skb_shinfo(skb1)->nr_frags ||\n\t\t\t    skb_has_frag_list(skb1) ||\n\t\t\t    skb_tailroom(skb1) < tailbits)\n\t\t\t\tntail = tailbits + 128;\n\t\t}\n\n\t\tif (copyflag ||\n\t\t    skb_cloned(skb1) ||\n\t\t    ntail ||\n\t\t    skb_shinfo(skb1)->nr_frags ||\n\t\t    skb_has_frag_list(skb1)) {\n\t\t\tstruct sk_buff *skb2;\n\n\t\t\t/* Fuck, we are miserable poor guys... */\n\t\t\tif (ntail == 0)\n\t\t\t\tskb2 = skb_copy(skb1, GFP_ATOMIC);\n\t\t\telse\n\t\t\t\tskb2 = skb_copy_expand(skb1,\n\t\t\t\t\t\t       skb_headroom(skb1),\n\t\t\t\t\t\t       ntail,\n\t\t\t\t\t\t       GFP_ATOMIC);\n\t\t\tif (unlikely(skb2 == NULL))\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tif (skb1->sk)\n\t\t\t\tskb_set_owner_w(skb2, skb1->sk);\n\n\t\t\t/* Looking around. Are we still alive?\n\t\t\t * OK, link new skb, drop old one */\n\n\t\t\tskb2->next = skb1->next;\n\t\t\t*skb_p = skb2;\n\t\t\tkfree_skb(skb1);\n\t\t\tskb1 = skb2;\n\t\t}\n\t\telt++;\n\t\t*trailer = skb1;\n\t\tskb_p = &skb1->next;\n\t}\n\n\treturn elt;\n}\nEXPORT_SYMBOL_GPL(skb_cow_data);\n\nstatic void sock_rmem_free(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\n\tatomic_sub(skb->truesize, &sk->sk_rmem_alloc);\n}\n\n/*\n * Note: We dont mem charge error packets (no sk_forward_alloc changes)\n */\nint sock_queue_err_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tint len = skb->len;\n\n\tif (atomic_read(&sk->sk_rmem_alloc) + skb->truesize >=\n\t    (unsigned int)sk->sk_rcvbuf)\n\t\treturn -ENOMEM;\n\n\tskb_orphan(skb);\n\tskb->sk = sk;\n\tskb->destructor = sock_rmem_free;\n\tatomic_add(skb->truesize, &sk->sk_rmem_alloc);\n\n\t/* before exiting rcu section, make sure dst is refcounted */\n\tskb_dst_force(skb);\n\n\tskb_queue_tail(&sk->sk_error_queue, skb);\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_data_ready(sk, len);\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_queue_err_skb);\n\nvoid skb_tstamp_tx(struct sk_buff *orig_skb,\n\t\tstruct skb_shared_hwtstamps *hwtstamps)\n{\n\tstruct sock *sk = orig_skb->sk;\n\tstruct sock_exterr_skb *serr;\n\tstruct sk_buff *skb;\n\tint err;\n\n\tif (!sk)\n\t\treturn;\n\n\tif (hwtstamps) {\n\t\t*skb_hwtstamps(orig_skb) =\n\t\t\t*hwtstamps;\n\t} else {\n\t\t/*\n\t\t * no hardware time stamps available,\n\t\t * so keep the shared tx_flags and only\n\t\t * store software time stamp\n\t\t */\n\t\torig_skb->tstamp = ktime_get_real();\n\t}\n\n\tskb = skb_clone(orig_skb, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn;\n\n\tserr = SKB_EXT_ERR(skb);\n\tmemset(serr, 0, sizeof(*serr));\n\tserr->ee.ee_errno = ENOMSG;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_TIMESTAMPING;\n\n\terr = sock_queue_err_skb(sk, skb);\n\n\tif (err)\n\t\tkfree_skb(skb);\n}\nEXPORT_SYMBOL_GPL(skb_tstamp_tx);\n\nvoid skb_complete_wifi_ack(struct sk_buff *skb, bool acked)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct sock_exterr_skb *serr;\n\tint err;\n\n\tskb->wifi_acked_valid = 1;\n\tskb->wifi_acked = acked;\n\n\tserr = SKB_EXT_ERR(skb);\n\tmemset(serr, 0, sizeof(*serr));\n\tserr->ee.ee_errno = ENOMSG;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_TXSTATUS;\n\n\terr = sock_queue_err_skb(sk, skb);\n\tif (err)\n\t\tkfree_skb(skb);\n}\nEXPORT_SYMBOL_GPL(skb_complete_wifi_ack);\n\n\n/**\n * skb_partial_csum_set - set up and verify partial csum values for packet\n * @skb: the skb to set\n * @start: the number of bytes after skb->data to start checksumming.\n * @off: the offset from start to place the checksum.\n *\n * For untrusted partially-checksummed packets, we need to make sure the values\n * for skb->csum_start and skb->csum_offset are valid so we don't oops.\n *\n * This function checks and sets those values and skb->ip_summed: if this\n * returns false you should drop the packet.\n */\nbool skb_partial_csum_set(struct sk_buff *skb, u16 start, u16 off)\n{\n\tif (unlikely(start > skb_headlen(skb)) ||\n\t    unlikely((int)start + off > skb_headlen(skb) - 2)) {\n\t\tnet_warn_ratelimited(\"bad partial csum: csum=%u/%u len=%u\\n\",\n\t\t\t\t     start, off, skb_headlen(skb));\n\t\treturn false;\n\t}\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\tskb->csum_start = skb_headroom(skb) + start;\n\tskb->csum_offset = off;\n\tskb_set_transport_header(skb, start);\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(skb_partial_csum_set);\n\nstatic int skb_maybe_pull_tail(struct sk_buff *skb, unsigned int len,\n\t\t\t       unsigned int max)\n{\n\tif (skb_headlen(skb) >= len)\n\t\treturn 0;\n\n\t/* If we need to pullup then pullup to the max, so we\n\t * won't need to do it again.\n\t */\n\tif (max > skb->len)\n\t\tmax = skb->len;\n\n\tif (__pskb_pull_tail(skb, max - skb_headlen(skb)) == NULL)\n\t\treturn -ENOMEM;\n\n\tif (skb_headlen(skb) < len)\n\t\treturn -EPROTO;\n\n\treturn 0;\n}\n\n/* This value should be large enough to cover a tagged ethernet header plus\n * maximally sized IP and TCP or UDP headers.\n */\n#define MAX_IP_HDR_LEN 128\n\nstatic int skb_checksum_setup_ip(struct sk_buff *skb, bool recalculate)\n{\n\tunsigned int off;\n\tbool fragment;\n\tint err;\n\n\tfragment = false;\n\n\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t  sizeof(struct iphdr),\n\t\t\t\t  MAX_IP_HDR_LEN);\n\tif (err < 0)\n\t\tgoto out;\n\n\tif (ip_hdr(skb)->frag_off & htons(IP_OFFSET | IP_MF))\n\t\tfragment = true;\n\n\toff = ip_hdrlen(skb);\n\n\terr = -EPROTO;\n\n\tif (fragment)\n\t\tgoto out;\n\n\tswitch (ip_hdr(skb)->protocol) {\n\tcase IPPROTO_TCP:\n\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t  off + sizeof(struct tcphdr),\n\t\t\t\t\t  MAX_IP_HDR_LEN);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\n\t\tif (!skb_partial_csum_set(skb, off,\n\t\t\t\t\t  offsetof(struct tcphdr, check))) {\n\t\t\terr = -EPROTO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (recalculate)\n\t\t\ttcp_hdr(skb)->check =\n\t\t\t\t~csum_tcpudp_magic(ip_hdr(skb)->saddr,\n\t\t\t\t\t\t   ip_hdr(skb)->daddr,\n\t\t\t\t\t\t   skb->len - off,\n\t\t\t\t\t\t   IPPROTO_TCP, 0);\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t  off + sizeof(struct udphdr),\n\t\t\t\t\t  MAX_IP_HDR_LEN);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\n\t\tif (!skb_partial_csum_set(skb, off,\n\t\t\t\t\t  offsetof(struct udphdr, check))) {\n\t\t\terr = -EPROTO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (recalculate)\n\t\t\tudp_hdr(skb)->check =\n\t\t\t\t~csum_tcpudp_magic(ip_hdr(skb)->saddr,\n\t\t\t\t\t\t   ip_hdr(skb)->daddr,\n\t\t\t\t\t\t   skb->len - off,\n\t\t\t\t\t\t   IPPROTO_UDP, 0);\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\terr = 0;\n\nout:\n\treturn err;\n}\n\n/* This value should be large enough to cover a tagged ethernet header plus\n * an IPv6 header, all options, and a maximal TCP or UDP header.\n */\n#define MAX_IPV6_HDR_LEN 256\n\n#define OPT_HDR(type, skb, off) \\\n\t(type *)(skb_network_header(skb) + (off))\n\nstatic int skb_checksum_setup_ipv6(struct sk_buff *skb, bool recalculate)\n{\n\tint err;\n\tu8 nexthdr;\n\tunsigned int off;\n\tunsigned int len;\n\tbool fragment;\n\tbool done;\n\n\tfragment = false;\n\tdone = false;\n\n\toff = sizeof(struct ipv6hdr);\n\n\terr = skb_maybe_pull_tail(skb, off, MAX_IPV6_HDR_LEN);\n\tif (err < 0)\n\t\tgoto out;\n\n\tnexthdr = ipv6_hdr(skb)->nexthdr;\n\n\tlen = sizeof(struct ipv6hdr) + ntohs(ipv6_hdr(skb)->payload_len);\n\twhile (off <= len && !done) {\n\t\tswitch (nexthdr) {\n\t\tcase IPPROTO_DSTOPTS:\n\t\tcase IPPROTO_HOPOPTS:\n\t\tcase IPPROTO_ROUTING: {\n\t\t\tstruct ipv6_opt_hdr *hp;\n\n\t\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t\t  off +\n\t\t\t\t\t\t  sizeof(struct ipv6_opt_hdr),\n\t\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\thp = OPT_HDR(struct ipv6_opt_hdr, skb, off);\n\t\t\tnexthdr = hp->nexthdr;\n\t\t\toff += ipv6_optlen(hp);\n\t\t\tbreak;\n\t\t}\n\t\tcase IPPROTO_AH: {\n\t\t\tstruct ip_auth_hdr *hp;\n\n\t\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t\t  off +\n\t\t\t\t\t\t  sizeof(struct ip_auth_hdr),\n\t\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\thp = OPT_HDR(struct ip_auth_hdr, skb, off);\n\t\t\tnexthdr = hp->nexthdr;\n\t\t\toff += ipv6_authlen(hp);\n\t\t\tbreak;\n\t\t}\n\t\tcase IPPROTO_FRAGMENT: {\n\t\t\tstruct frag_hdr *hp;\n\n\t\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t\t  off +\n\t\t\t\t\t\t  sizeof(struct frag_hdr),\n\t\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\thp = OPT_HDR(struct frag_hdr, skb, off);\n\n\t\t\tif (hp->frag_off & htons(IP6_OFFSET | IP6_MF))\n\t\t\t\tfragment = true;\n\n\t\t\tnexthdr = hp->nexthdr;\n\t\t\toff += sizeof(struct frag_hdr);\n\t\t\tbreak;\n\t\t}\n\t\tdefault:\n\t\t\tdone = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\terr = -EPROTO;\n\n\tif (!done || fragment)\n\t\tgoto out;\n\n\tswitch (nexthdr) {\n\tcase IPPROTO_TCP:\n\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t  off + sizeof(struct tcphdr),\n\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\n\t\tif (!skb_partial_csum_set(skb, off,\n\t\t\t\t\t  offsetof(struct tcphdr, check))) {\n\t\t\terr = -EPROTO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (recalculate)\n\t\t\ttcp_hdr(skb)->check =\n\t\t\t\t~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t\t &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t\t skb->len - off,\n\t\t\t\t\t\t IPPROTO_TCP, 0);\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t  off + sizeof(struct udphdr),\n\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\n\t\tif (!skb_partial_csum_set(skb, off,\n\t\t\t\t\t  offsetof(struct udphdr, check))) {\n\t\t\terr = -EPROTO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (recalculate)\n\t\t\tudp_hdr(skb)->check =\n\t\t\t\t~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t\t &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t\t skb->len - off,\n\t\t\t\t\t\t IPPROTO_UDP, 0);\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\terr = 0;\n\nout:\n\treturn err;\n}\n\n/**\n * skb_checksum_setup - set up partial checksum offset\n * @skb: the skb to set up\n * @recalculate: if true the pseudo-header checksum will be recalculated\n */\nint skb_checksum_setup(struct sk_buff *skb, bool recalculate)\n{\n\tint err;\n\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_IP):\n\t\terr = skb_checksum_setup_ip(skb, recalculate);\n\t\tbreak;\n\n\tcase htons(ETH_P_IPV6):\n\t\terr = skb_checksum_setup_ipv6(skb, recalculate);\n\t\tbreak;\n\n\tdefault:\n\t\terr = -EPROTO;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(skb_checksum_setup);\n\nvoid __skb_warn_lro_forwarding(const struct sk_buff *skb)\n{\n\tnet_warn_ratelimited(\"%s: received packets cannot be forwarded while LRO is enabled\\n\",\n\t\t\t     skb->dev->name);\n}\nEXPORT_SYMBOL(__skb_warn_lro_forwarding);\n\nvoid kfree_skb_partial(struct sk_buff *skb, bool head_stolen)\n{\n\tif (head_stolen) {\n\t\tskb_release_head_state(skb);\n\t\tkmem_cache_free(skbuff_head_cache, skb);\n\t} else {\n\t\t__kfree_skb(skb);\n\t}\n}\nEXPORT_SYMBOL(kfree_skb_partial);\n\n/**\n * skb_try_coalesce - try to merge skb to prior one\n * @to: prior buffer\n * @from: buffer to add\n * @fragstolen: pointer to boolean\n * @delta_truesize: how much more was allocated than was requested\n */\nbool skb_try_coalesce(struct sk_buff *to, struct sk_buff *from,\n\t\t      bool *fragstolen, int *delta_truesize)\n{\n\tint i, delta, len = from->len;\n\n\t*fragstolen = false;\n\n\tif (skb_cloned(to))\n\t\treturn false;\n\n\tif (len <= skb_tailroom(to)) {\n\t\tBUG_ON(skb_copy_bits(from, 0, skb_put(to, len), len));\n\t\t*delta_truesize = 0;\n\t\treturn true;\n\t}\n\n\tif (skb_has_frag_list(to) || skb_has_frag_list(from))\n\t\treturn false;\n\n\tif (skb_headlen(from) != 0) {\n\t\tstruct page *page;\n\t\tunsigned int offset;\n\n\t\tif (skb_shinfo(to)->nr_frags +\n\t\t    skb_shinfo(from)->nr_frags >= MAX_SKB_FRAGS)\n\t\t\treturn false;\n\n\t\tif (skb_head_is_locked(from))\n\t\t\treturn false;\n\n\t\tdelta = from->truesize - SKB_DATA_ALIGN(sizeof(struct sk_buff));\n\n\t\tpage = virt_to_head_page(from->head);\n\t\toffset = from->data - (unsigned char *)page_address(page);\n\n\t\tskb_fill_page_desc(to, skb_shinfo(to)->nr_frags,\n\t\t\t\t   page, offset, skb_headlen(from));\n\t\t*fragstolen = true;\n\t} else {\n\t\tif (skb_shinfo(to)->nr_frags +\n\t\t    skb_shinfo(from)->nr_frags > MAX_SKB_FRAGS)\n\t\t\treturn false;\n\n\t\tdelta = from->truesize - SKB_TRUESIZE(skb_end_offset(from));\n\t}\n\n\tWARN_ON_ONCE(delta < len);\n\n\tmemcpy(skb_shinfo(to)->frags + skb_shinfo(to)->nr_frags,\n\t       skb_shinfo(from)->frags,\n\t       skb_shinfo(from)->nr_frags * sizeof(skb_frag_t));\n\tskb_shinfo(to)->nr_frags += skb_shinfo(from)->nr_frags;\n\n\tif (!skb_cloned(from))\n\t\tskb_shinfo(from)->nr_frags = 0;\n\n\t/* if the skb is not cloned this does nothing\n\t * since we set nr_frags to 0.\n\t */\n\tfor (i = 0; i < skb_shinfo(from)->nr_frags; i++)\n\t\tskb_frag_ref(from, i);\n\n\tto->truesize += delta;\n\tto->len += len;\n\tto->data_len += len;\n\n\t*delta_truesize = delta;\n\treturn true;\n}\nEXPORT_SYMBOL(skb_try_coalesce);\n\n/**\n * skb_scrub_packet - scrub an skb\n *\n * @skb: buffer to clean\n * @xnet: packet is crossing netns\n *\n * skb_scrub_packet can be used after encapsulating or decapsulting a packet\n * into/from a tunnel. Some information have to be cleared during these\n * operations.\n * skb_scrub_packet can also be used to clean a skb before injecting it in\n * another namespace (@xnet == true). We have to clear all information in the\n * skb that could impact namespace isolation.\n */\nvoid skb_scrub_packet(struct sk_buff *skb, bool xnet)\n{\n\tif (xnet)\n\t\tskb_orphan(skb);\n\tskb->tstamp.tv64 = 0;\n\tskb->pkt_type = PACKET_HOST;\n\tskb->skb_iif = 0;\n\tskb->local_df = 0;\n\tskb_dst_drop(skb);\n\tskb->mark = 0;\n\tsecpath_reset(skb);\n\tnf_reset(skb);\n\tnf_reset_trace(skb);\n}\nEXPORT_SYMBOL_GPL(skb_scrub_packet);\n\n/**\n * skb_gso_transport_seglen - Return length of individual segments of a gso packet\n *\n * @skb: GSO skb\n *\n * skb_gso_transport_seglen is used to determine the real size of the\n * individual segments, including Layer4 headers (TCP/UDP).\n *\n * The MAC/L2 or network (IP, IPv6) headers are not accounted for.\n */\nunsigned int skb_gso_transport_seglen(const struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\tunsigned int hdr_len;\n\n\tif (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6)))\n\t\thdr_len = tcp_hdrlen(skb);\n\telse\n\t\thdr_len = sizeof(struct udphdr);\n\treturn hdr_len + shinfo->gso_size;\n}\nEXPORT_SYMBOL_GPL(skb_gso_transport_seglen);\n"], "fixing_code": ["/*\n *\tRoutines having to do with the 'struct sk_buff' memory handlers.\n *\n *\tAuthors:\tAlan Cox <alan@lxorguk.ukuu.org.uk>\n *\t\t\tFlorian La Roche <rzsfl@rz.uni-sb.de>\n *\n *\tFixes:\n *\t\tAlan Cox\t:\tFixed the worst of the load\n *\t\t\t\t\tbalancer bugs.\n *\t\tDave Platt\t:\tInterrupt stacking fix.\n *\tRichard Kooijman\t:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tChanged buffer format.\n *\t\tAlan Cox\t:\tdestructor hook for AF_UNIX etc.\n *\t\tLinus Torvalds\t:\tBetter skb_clone.\n *\t\tAlan Cox\t:\tAdded skb_copy.\n *\t\tAlan Cox\t:\tAdded all the changed routines Linus\n *\t\t\t\t\tonly put in the headers\n *\t\tRay VanTassle\t:\tFixed --skb->lock in free\n *\t\tAlan Cox\t:\tskb_copy copy arp field\n *\t\tAndi Kleen\t:\tslabified it.\n *\t\tRobert Olsson\t:\tRemoved skb_head_pool\n *\n *\tNOTE:\n *\t\tThe __skb_ routines should be called with interrupts\n *\tdisabled, or you better be *real* sure that the operation is atomic\n *\twith respect to whatever list is being frobbed (e.g. via lock_sock()\n *\tor via disabling bottom half handlers, etc).\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n */\n\n/*\n *\tThe functions in this file will not compile correctly with gcc 2.4.x\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/kmemcheck.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/slab.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n#include <linux/netdevice.h>\n#ifdef CONFIG_NET_CLS_ACT\n#include <net/pkt_sched.h>\n#endif\n#include <linux/string.h>\n#include <linux/skbuff.h>\n#include <linux/splice.h>\n#include <linux/cache.h>\n#include <linux/rtnetlink.h>\n#include <linux/init.h>\n#include <linux/scatterlist.h>\n#include <linux/errqueue.h>\n#include <linux/prefetch.h>\n\n#include <net/protocol.h>\n#include <net/dst.h>\n#include <net/sock.h>\n#include <net/checksum.h>\n#include <net/ip6_checksum.h>\n#include <net/xfrm.h>\n\n#include <asm/uaccess.h>\n#include <trace/events/skb.h>\n#include <linux/highmem.h>\n\nstruct kmem_cache *skbuff_head_cache __read_mostly;\nstatic struct kmem_cache *skbuff_fclone_cache __read_mostly;\n\n/**\n *\tskb_panic - private function for out-of-line support\n *\t@skb:\tbuffer\n *\t@sz:\tsize\n *\t@addr:\taddress\n *\t@msg:\tskb_over_panic or skb_under_panic\n *\n *\tOut-of-line support for skb_put() and skb_push().\n *\tCalled via the wrapper skb_over_panic() or skb_under_panic().\n *\tKeep out of line to prevent kernel bloat.\n *\t__builtin_return_address is not used because it is not always reliable.\n */\nstatic void skb_panic(struct sk_buff *skb, unsigned int sz, void *addr,\n\t\t      const char msg[])\n{\n\tpr_emerg(\"%s: text:%p len:%d put:%d head:%p data:%p tail:%#lx end:%#lx dev:%s\\n\",\n\t\t msg, addr, skb->len, sz, skb->head, skb->data,\n\t\t (unsigned long)skb->tail, (unsigned long)skb->end,\n\t\t skb->dev ? skb->dev->name : \"<NULL>\");\n\tBUG();\n}\n\nstatic void skb_over_panic(struct sk_buff *skb, unsigned int sz, void *addr)\n{\n\tskb_panic(skb, sz, addr, __func__);\n}\n\nstatic void skb_under_panic(struct sk_buff *skb, unsigned int sz, void *addr)\n{\n\tskb_panic(skb, sz, addr, __func__);\n}\n\n/*\n * kmalloc_reserve is a wrapper around kmalloc_node_track_caller that tells\n * the caller if emergency pfmemalloc reserves are being used. If it is and\n * the socket is later found to be SOCK_MEMALLOC then PFMEMALLOC reserves\n * may be used. Otherwise, the packet data may be discarded until enough\n * memory is free\n */\n#define kmalloc_reserve(size, gfp, node, pfmemalloc) \\\n\t __kmalloc_reserve(size, gfp, node, _RET_IP_, pfmemalloc)\n\nstatic void *__kmalloc_reserve(size_t size, gfp_t flags, int node,\n\t\t\t       unsigned long ip, bool *pfmemalloc)\n{\n\tvoid *obj;\n\tbool ret_pfmemalloc = false;\n\n\t/*\n\t * Try a regular allocation, when that fails and we're not entitled\n\t * to the reserves, fail.\n\t */\n\tobj = kmalloc_node_track_caller(size,\n\t\t\t\t\tflags | __GFP_NOMEMALLOC | __GFP_NOWARN,\n\t\t\t\t\tnode);\n\tif (obj || !(gfp_pfmemalloc_allowed(flags)))\n\t\tgoto out;\n\n\t/* Try again but now we are using pfmemalloc reserves */\n\tret_pfmemalloc = true;\n\tobj = kmalloc_node_track_caller(size, flags, node);\n\nout:\n\tif (pfmemalloc)\n\t\t*pfmemalloc = ret_pfmemalloc;\n\n\treturn obj;\n}\n\n/* \tAllocate a new skbuff. We do this ourselves so we can fill in a few\n *\t'private' fields and also do memory statistics to find all the\n *\t[BEEP] leaks.\n *\n */\n\nstruct sk_buff *__alloc_skb_head(gfp_t gfp_mask, int node)\n{\n\tstruct sk_buff *skb;\n\n\t/* Get the HEAD */\n\tskb = kmem_cache_alloc_node(skbuff_head_cache,\n\t\t\t\t    gfp_mask & ~__GFP_DMA, node);\n\tif (!skb)\n\t\tgoto out;\n\n\t/*\n\t * Only clear those fields we need to clear, not those that we will\n\t * actually initialise below. Hence, don't put any more fields after\n\t * the tail pointer in struct sk_buff!\n\t */\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\tskb->head = NULL;\n\tskb->truesize = sizeof(struct sk_buff);\n\tatomic_set(&skb->users, 1);\n\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\nout:\n\treturn skb;\n}\n\n/**\n *\t__alloc_skb\t-\tallocate a network buffer\n *\t@size: size to allocate\n *\t@gfp_mask: allocation mask\n *\t@flags: If SKB_ALLOC_FCLONE is set, allocate from fclone cache\n *\t\tinstead of head cache and allocate a cloned (child) skb.\n *\t\tIf SKB_ALLOC_RX is set, __GFP_MEMALLOC will be used for\n *\t\tallocations in case the data is required for writeback\n *\t@node: numa node to allocate memory on\n *\n *\tAllocate a new &sk_buff. The returned buffer has no headroom and a\n *\ttail room of at least size bytes. The object has a reference count\n *\tof one. The return is the buffer. On a failure the return is %NULL.\n *\n *\tBuffers may only be allocated from interrupts using a @gfp_mask of\n *\t%GFP_ATOMIC.\n */\nstruct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,\n\t\t\t    int flags, int node)\n{\n\tstruct kmem_cache *cache;\n\tstruct skb_shared_info *shinfo;\n\tstruct sk_buff *skb;\n\tu8 *data;\n\tbool pfmemalloc;\n\n\tcache = (flags & SKB_ALLOC_FCLONE)\n\t\t? skbuff_fclone_cache : skbuff_head_cache;\n\n\tif (sk_memalloc_socks() && (flags & SKB_ALLOC_RX))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\t/* Get the HEAD */\n\tskb = kmem_cache_alloc_node(cache, gfp_mask & ~__GFP_DMA, node);\n\tif (!skb)\n\t\tgoto out;\n\tprefetchw(skb);\n\n\t/* We do our best to align skb_shared_info on a separate cache\n\t * line. It usually works because kmalloc(X > SMP_CACHE_BYTES) gives\n\t * aligned memory blocks, unless SLUB/SLAB debug is enabled.\n\t * Both skb->head and skb_shared_info are cache line aligned.\n\t */\n\tsize = SKB_DATA_ALIGN(size);\n\tsize += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tdata = kmalloc_reserve(size, gfp_mask, node, &pfmemalloc);\n\tif (!data)\n\t\tgoto nodata;\n\t/* kmalloc(size) might give us more room than requested.\n\t * Put skb_shared_info exactly at the end of allocated zone,\n\t * to allow max possible filling before reallocation.\n\t */\n\tsize = SKB_WITH_OVERHEAD(ksize(data));\n\tprefetchw(data + size);\n\n\t/*\n\t * Only clear those fields we need to clear, not those that we will\n\t * actually initialise below. Hence, don't put any more fields after\n\t * the tail pointer in struct sk_buff!\n\t */\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\t/* Account for allocated memory : skb + skb->head */\n\tskb->truesize = SKB_TRUESIZE(size);\n\tskb->pfmemalloc = pfmemalloc;\n\tatomic_set(&skb->users, 1);\n\tskb->head = data;\n\tskb->data = data;\n\tskb_reset_tail_pointer(skb);\n\tskb->end = skb->tail + size;\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\n\tskb->transport_header = (typeof(skb->transport_header))~0U;\n\n\t/* make sure we initialize shinfo sequentially */\n\tshinfo = skb_shinfo(skb);\n\tmemset(shinfo, 0, offsetof(struct skb_shared_info, dataref));\n\tatomic_set(&shinfo->dataref, 1);\n\tkmemcheck_annotate_variable(shinfo->destructor_arg);\n\n\tif (flags & SKB_ALLOC_FCLONE) {\n\t\tstruct sk_buff *child = skb + 1;\n\t\tatomic_t *fclone_ref = (atomic_t *) (child + 1);\n\n\t\tkmemcheck_annotate_bitfield(child, flags1);\n\t\tkmemcheck_annotate_bitfield(child, flags2);\n\t\tskb->fclone = SKB_FCLONE_ORIG;\n\t\tatomic_set(fclone_ref, 1);\n\n\t\tchild->fclone = SKB_FCLONE_UNAVAILABLE;\n\t\tchild->pfmemalloc = pfmemalloc;\n\t}\nout:\n\treturn skb;\nnodata:\n\tkmem_cache_free(cache, skb);\n\tskb = NULL;\n\tgoto out;\n}\nEXPORT_SYMBOL(__alloc_skb);\n\n/**\n * build_skb - build a network buffer\n * @data: data buffer provided by caller\n * @frag_size: size of fragment, or 0 if head was kmalloced\n *\n * Allocate a new &sk_buff. Caller provides space holding head and\n * skb_shared_info. @data must have been allocated by kmalloc() only if\n * @frag_size is 0, otherwise data should come from the page allocator.\n * The return is the new skb buffer.\n * On a failure the return is %NULL, and @data is not freed.\n * Notes :\n *  Before IO, driver allocates only data buffer where NIC put incoming frame\n *  Driver should add room at head (NET_SKB_PAD) and\n *  MUST add room at tail (SKB_DATA_ALIGN(skb_shared_info))\n *  After IO, driver calls build_skb(), to allocate sk_buff and populate it\n *  before giving packet to stack.\n *  RX rings only contains data buffers, not full skbs.\n */\nstruct sk_buff *build_skb(void *data, unsigned int frag_size)\n{\n\tstruct skb_shared_info *shinfo;\n\tstruct sk_buff *skb;\n\tunsigned int size = frag_size ? : ksize(data);\n\n\tskb = kmem_cache_alloc(skbuff_head_cache, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn NULL;\n\n\tsize -= SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\tskb->truesize = SKB_TRUESIZE(size);\n\tskb->head_frag = frag_size != 0;\n\tatomic_set(&skb->users, 1);\n\tskb->head = data;\n\tskb->data = data;\n\tskb_reset_tail_pointer(skb);\n\tskb->end = skb->tail + size;\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\n\tskb->transport_header = (typeof(skb->transport_header))~0U;\n\n\t/* make sure we initialize shinfo sequentially */\n\tshinfo = skb_shinfo(skb);\n\tmemset(shinfo, 0, offsetof(struct skb_shared_info, dataref));\n\tatomic_set(&shinfo->dataref, 1);\n\tkmemcheck_annotate_variable(shinfo->destructor_arg);\n\n\treturn skb;\n}\nEXPORT_SYMBOL(build_skb);\n\nstruct netdev_alloc_cache {\n\tstruct page_frag\tfrag;\n\t/* we maintain a pagecount bias, so that we dont dirty cache line\n\t * containing page->_count every time we allocate a fragment.\n\t */\n\tunsigned int\t\tpagecnt_bias;\n};\nstatic DEFINE_PER_CPU(struct netdev_alloc_cache, netdev_alloc_cache);\n\nstatic void *__netdev_alloc_frag(unsigned int fragsz, gfp_t gfp_mask)\n{\n\tstruct netdev_alloc_cache *nc;\n\tvoid *data = NULL;\n\tint order;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tnc = &__get_cpu_var(netdev_alloc_cache);\n\tif (unlikely(!nc->frag.page)) {\nrefill:\n\t\tfor (order = NETDEV_FRAG_PAGE_MAX_ORDER; ;) {\n\t\t\tgfp_t gfp = gfp_mask;\n\n\t\t\tif (order)\n\t\t\t\tgfp |= __GFP_COMP | __GFP_NOWARN;\n\t\t\tnc->frag.page = alloc_pages(gfp, order);\n\t\t\tif (likely(nc->frag.page))\n\t\t\t\tbreak;\n\t\t\tif (--order < 0)\n\t\t\t\tgoto end;\n\t\t}\n\t\tnc->frag.size = PAGE_SIZE << order;\nrecycle:\n\t\tatomic_set(&nc->frag.page->_count, NETDEV_PAGECNT_MAX_BIAS);\n\t\tnc->pagecnt_bias = NETDEV_PAGECNT_MAX_BIAS;\n\t\tnc->frag.offset = 0;\n\t}\n\n\tif (nc->frag.offset + fragsz > nc->frag.size) {\n\t\t/* avoid unnecessary locked operations if possible */\n\t\tif ((atomic_read(&nc->frag.page->_count) == nc->pagecnt_bias) ||\n\t\t    atomic_sub_and_test(nc->pagecnt_bias, &nc->frag.page->_count))\n\t\t\tgoto recycle;\n\t\tgoto refill;\n\t}\n\n\tdata = page_address(nc->frag.page) + nc->frag.offset;\n\tnc->frag.offset += fragsz;\n\tnc->pagecnt_bias--;\nend:\n\tlocal_irq_restore(flags);\n\treturn data;\n}\n\n/**\n * netdev_alloc_frag - allocate a page fragment\n * @fragsz: fragment size\n *\n * Allocates a frag from a page for receive buffer.\n * Uses GFP_ATOMIC allocations.\n */\nvoid *netdev_alloc_frag(unsigned int fragsz)\n{\n\treturn __netdev_alloc_frag(fragsz, GFP_ATOMIC | __GFP_COLD);\n}\nEXPORT_SYMBOL(netdev_alloc_frag);\n\n/**\n *\t__netdev_alloc_skb - allocate an skbuff for rx on a specific device\n *\t@dev: network device to receive on\n *\t@length: length to allocate\n *\t@gfp_mask: get_free_pages mask, passed to alloc_skb\n *\n *\tAllocate a new &sk_buff and assign it a usage count of one. The\n *\tbuffer has unspecified headroom built in. Users should allocate\n *\tthe headroom they think they need without accounting for the\n *\tbuilt in space. The built in space is used for optimisations.\n *\n *\t%NULL is returned if there is no free memory.\n */\nstruct sk_buff *__netdev_alloc_skb(struct net_device *dev,\n\t\t\t\t   unsigned int length, gfp_t gfp_mask)\n{\n\tstruct sk_buff *skb = NULL;\n\tunsigned int fragsz = SKB_DATA_ALIGN(length + NET_SKB_PAD) +\n\t\t\t      SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tif (fragsz <= PAGE_SIZE && !(gfp_mask & (__GFP_WAIT | GFP_DMA))) {\n\t\tvoid *data;\n\n\t\tif (sk_memalloc_socks())\n\t\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\t\tdata = __netdev_alloc_frag(fragsz, gfp_mask);\n\n\t\tif (likely(data)) {\n\t\t\tskb = build_skb(data, fragsz);\n\t\t\tif (unlikely(!skb))\n\t\t\t\tput_page(virt_to_head_page(data));\n\t\t}\n\t} else {\n\t\tskb = __alloc_skb(length + NET_SKB_PAD, gfp_mask,\n\t\t\t\t  SKB_ALLOC_RX, NUMA_NO_NODE);\n\t}\n\tif (likely(skb)) {\n\t\tskb_reserve(skb, NET_SKB_PAD);\n\t\tskb->dev = dev;\n\t}\n\treturn skb;\n}\nEXPORT_SYMBOL(__netdev_alloc_skb);\n\nvoid skb_add_rx_frag(struct sk_buff *skb, int i, struct page *page, int off,\n\t\t     int size, unsigned int truesize)\n{\n\tskb_fill_page_desc(skb, i, page, off, size);\n\tskb->len += size;\n\tskb->data_len += size;\n\tskb->truesize += truesize;\n}\nEXPORT_SYMBOL(skb_add_rx_frag);\n\nvoid skb_coalesce_rx_frag(struct sk_buff *skb, int i, int size,\n\t\t\t  unsigned int truesize)\n{\n\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\tskb_frag_size_add(frag, size);\n\tskb->len += size;\n\tskb->data_len += size;\n\tskb->truesize += truesize;\n}\nEXPORT_SYMBOL(skb_coalesce_rx_frag);\n\nstatic void skb_drop_list(struct sk_buff **listp)\n{\n\tkfree_skb_list(*listp);\n\t*listp = NULL;\n}\n\nstatic inline void skb_drop_fraglist(struct sk_buff *skb)\n{\n\tskb_drop_list(&skb_shinfo(skb)->frag_list);\n}\n\nstatic void skb_clone_fraglist(struct sk_buff *skb)\n{\n\tstruct sk_buff *list;\n\n\tskb_walk_frags(skb, list)\n\t\tskb_get(list);\n}\n\nstatic void skb_free_head(struct sk_buff *skb)\n{\n\tif (skb->head_frag)\n\t\tput_page(virt_to_head_page(skb->head));\n\telse\n\t\tkfree(skb->head);\n}\n\nstatic void skb_release_data(struct sk_buff *skb)\n{\n\tif (!skb->cloned ||\n\t    !atomic_sub_return(skb->nohdr ? (1 << SKB_DATAREF_SHIFT) + 1 : 1,\n\t\t\t       &skb_shinfo(skb)->dataref)) {\n\t\tif (skb_shinfo(skb)->nr_frags) {\n\t\t\tint i;\n\t\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\t\t\tskb_frag_unref(skb, i);\n\t\t}\n\n\t\t/*\n\t\t * If skb buf is from userspace, we need to notify the caller\n\t\t * the lower device DMA has done;\n\t\t */\n\t\tif (skb_shinfo(skb)->tx_flags & SKBTX_DEV_ZEROCOPY) {\n\t\t\tstruct ubuf_info *uarg;\n\n\t\t\tuarg = skb_shinfo(skb)->destructor_arg;\n\t\t\tif (uarg->callback)\n\t\t\t\tuarg->callback(uarg, true);\n\t\t}\n\n\t\tif (skb_has_frag_list(skb))\n\t\t\tskb_drop_fraglist(skb);\n\n\t\tskb_free_head(skb);\n\t}\n}\n\n/*\n *\tFree an skbuff by memory without cleaning the state.\n */\nstatic void kfree_skbmem(struct sk_buff *skb)\n{\n\tstruct sk_buff *other;\n\tatomic_t *fclone_ref;\n\n\tswitch (skb->fclone) {\n\tcase SKB_FCLONE_UNAVAILABLE:\n\t\tkmem_cache_free(skbuff_head_cache, skb);\n\t\tbreak;\n\n\tcase SKB_FCLONE_ORIG:\n\t\tfclone_ref = (atomic_t *) (skb + 2);\n\t\tif (atomic_dec_and_test(fclone_ref))\n\t\t\tkmem_cache_free(skbuff_fclone_cache, skb);\n\t\tbreak;\n\n\tcase SKB_FCLONE_CLONE:\n\t\tfclone_ref = (atomic_t *) (skb + 1);\n\t\tother = skb - 1;\n\n\t\t/* The clone portion is available for\n\t\t * fast-cloning again.\n\t\t */\n\t\tskb->fclone = SKB_FCLONE_UNAVAILABLE;\n\n\t\tif (atomic_dec_and_test(fclone_ref))\n\t\t\tkmem_cache_free(skbuff_fclone_cache, other);\n\t\tbreak;\n\t}\n}\n\nstatic void skb_release_head_state(struct sk_buff *skb)\n{\n\tskb_dst_drop(skb);\n#ifdef CONFIG_XFRM\n\tsecpath_put(skb->sp);\n#endif\n\tif (skb->destructor) {\n\t\tWARN_ON(in_irq());\n\t\tskb->destructor(skb);\n\t}\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\tnf_conntrack_put(skb->nfct);\n#endif\n#ifdef CONFIG_BRIDGE_NETFILTER\n\tnf_bridge_put(skb->nf_bridge);\n#endif\n/* XXX: IS this still necessary? - JHS */\n#ifdef CONFIG_NET_SCHED\n\tskb->tc_index = 0;\n#ifdef CONFIG_NET_CLS_ACT\n\tskb->tc_verd = 0;\n#endif\n#endif\n}\n\n/* Free everything but the sk_buff shell. */\nstatic void skb_release_all(struct sk_buff *skb)\n{\n\tskb_release_head_state(skb);\n\tif (likely(skb->head))\n\t\tskb_release_data(skb);\n}\n\n/**\n *\t__kfree_skb - private function\n *\t@skb: buffer\n *\n *\tFree an sk_buff. Release anything attached to the buffer.\n *\tClean the state. This is an internal helper function. Users should\n *\talways call kfree_skb\n */\n\nvoid __kfree_skb(struct sk_buff *skb)\n{\n\tskb_release_all(skb);\n\tkfree_skbmem(skb);\n}\nEXPORT_SYMBOL(__kfree_skb);\n\n/**\n *\tkfree_skb - free an sk_buff\n *\t@skb: buffer to free\n *\n *\tDrop a reference to the buffer and free it if the usage count has\n *\thit zero.\n */\nvoid kfree_skb(struct sk_buff *skb)\n{\n\tif (unlikely(!skb))\n\t\treturn;\n\tif (likely(atomic_read(&skb->users) == 1))\n\t\tsmp_rmb();\n\telse if (likely(!atomic_dec_and_test(&skb->users)))\n\t\treturn;\n\ttrace_kfree_skb(skb, __builtin_return_address(0));\n\t__kfree_skb(skb);\n}\nEXPORT_SYMBOL(kfree_skb);\n\nvoid kfree_skb_list(struct sk_buff *segs)\n{\n\twhile (segs) {\n\t\tstruct sk_buff *next = segs->next;\n\n\t\tkfree_skb(segs);\n\t\tsegs = next;\n\t}\n}\nEXPORT_SYMBOL(kfree_skb_list);\n\n/**\n *\tskb_tx_error - report an sk_buff xmit error\n *\t@skb: buffer that triggered an error\n *\n *\tReport xmit error if a device callback is tracking this skb.\n *\tskb must be freed afterwards.\n */\nvoid skb_tx_error(struct sk_buff *skb)\n{\n\tif (skb_shinfo(skb)->tx_flags & SKBTX_DEV_ZEROCOPY) {\n\t\tstruct ubuf_info *uarg;\n\n\t\tuarg = skb_shinfo(skb)->destructor_arg;\n\t\tif (uarg->callback)\n\t\t\tuarg->callback(uarg, false);\n\t\tskb_shinfo(skb)->tx_flags &= ~SKBTX_DEV_ZEROCOPY;\n\t}\n}\nEXPORT_SYMBOL(skb_tx_error);\n\n/**\n *\tconsume_skb - free an skbuff\n *\t@skb: buffer to free\n *\n *\tDrop a ref to the buffer and free it if the usage count has hit zero\n *\tFunctions identically to kfree_skb, but kfree_skb assumes that the frame\n *\tis being dropped after a failure and notes that\n */\nvoid consume_skb(struct sk_buff *skb)\n{\n\tif (unlikely(!skb))\n\t\treturn;\n\tif (likely(atomic_read(&skb->users) == 1))\n\t\tsmp_rmb();\n\telse if (likely(!atomic_dec_and_test(&skb->users)))\n\t\treturn;\n\ttrace_consume_skb(skb);\n\t__kfree_skb(skb);\n}\nEXPORT_SYMBOL(consume_skb);\n\nstatic void __copy_skb_header(struct sk_buff *new, const struct sk_buff *old)\n{\n\tnew->tstamp\t\t= old->tstamp;\n\tnew->dev\t\t= old->dev;\n\tnew->transport_header\t= old->transport_header;\n\tnew->network_header\t= old->network_header;\n\tnew->mac_header\t\t= old->mac_header;\n\tnew->inner_protocol\t= old->inner_protocol;\n\tnew->inner_transport_header = old->inner_transport_header;\n\tnew->inner_network_header = old->inner_network_header;\n\tnew->inner_mac_header = old->inner_mac_header;\n\tskb_dst_copy(new, old);\n\tskb_copy_hash(new, old);\n\tnew->ooo_okay\t\t= old->ooo_okay;\n\tnew->no_fcs\t\t= old->no_fcs;\n\tnew->encapsulation\t= old->encapsulation;\n#ifdef CONFIG_XFRM\n\tnew->sp\t\t\t= secpath_get(old->sp);\n#endif\n\tmemcpy(new->cb, old->cb, sizeof(old->cb));\n\tnew->csum\t\t= old->csum;\n\tnew->local_df\t\t= old->local_df;\n\tnew->pkt_type\t\t= old->pkt_type;\n\tnew->ip_summed\t\t= old->ip_summed;\n\tskb_copy_queue_mapping(new, old);\n\tnew->priority\t\t= old->priority;\n#if IS_ENABLED(CONFIG_IP_VS)\n\tnew->ipvs_property\t= old->ipvs_property;\n#endif\n\tnew->pfmemalloc\t\t= old->pfmemalloc;\n\tnew->protocol\t\t= old->protocol;\n\tnew->mark\t\t= old->mark;\n\tnew->skb_iif\t\t= old->skb_iif;\n\t__nf_copy(new, old);\n#ifdef CONFIG_NET_SCHED\n\tnew->tc_index\t\t= old->tc_index;\n#ifdef CONFIG_NET_CLS_ACT\n\tnew->tc_verd\t\t= old->tc_verd;\n#endif\n#endif\n\tnew->vlan_proto\t\t= old->vlan_proto;\n\tnew->vlan_tci\t\t= old->vlan_tci;\n\n\tskb_copy_secmark(new, old);\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tnew->napi_id\t= old->napi_id;\n#endif\n}\n\n/*\n * You should not add any new code to this function.  Add it to\n * __copy_skb_header above instead.\n */\nstatic struct sk_buff *__skb_clone(struct sk_buff *n, struct sk_buff *skb)\n{\n#define C(x) n->x = skb->x\n\n\tn->next = n->prev = NULL;\n\tn->sk = NULL;\n\t__copy_skb_header(n, skb);\n\n\tC(len);\n\tC(data_len);\n\tC(mac_len);\n\tn->hdr_len = skb->nohdr ? skb_headroom(skb) : skb->hdr_len;\n\tn->cloned = 1;\n\tn->nohdr = 0;\n\tn->destructor = NULL;\n\tC(tail);\n\tC(end);\n\tC(head);\n\tC(head_frag);\n\tC(data);\n\tC(truesize);\n\tatomic_set(&n->users, 1);\n\n\tatomic_inc(&(skb_shinfo(skb)->dataref));\n\tskb->cloned = 1;\n\n\treturn n;\n#undef C\n}\n\n/**\n *\tskb_morph\t-\tmorph one skb into another\n *\t@dst: the skb to receive the contents\n *\t@src: the skb to supply the contents\n *\n *\tThis is identical to skb_clone except that the target skb is\n *\tsupplied by the user.\n *\n *\tThe target skb is returned upon exit.\n */\nstruct sk_buff *skb_morph(struct sk_buff *dst, struct sk_buff *src)\n{\n\tskb_release_all(dst);\n\treturn __skb_clone(dst, src);\n}\nEXPORT_SYMBOL_GPL(skb_morph);\n\n/**\n *\tskb_copy_ubufs\t-\tcopy userspace skb frags buffers to kernel\n *\t@skb: the skb to modify\n *\t@gfp_mask: allocation priority\n *\n *\tThis must be called on SKBTX_DEV_ZEROCOPY skb.\n *\tIt will copy all frags into kernel and drop the reference\n *\tto userspace pages.\n *\n *\tIf this function is called from an interrupt gfp_mask() must be\n *\t%GFP_ATOMIC.\n *\n *\tReturns 0 on success or a negative error code on failure\n *\tto allocate kernel memory to copy to.\n */\nint skb_copy_ubufs(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tint i;\n\tint num_frags = skb_shinfo(skb)->nr_frags;\n\tstruct page *page, *head = NULL;\n\tstruct ubuf_info *uarg = skb_shinfo(skb)->destructor_arg;\n\n\tfor (i = 0; i < num_frags; i++) {\n\t\tu8 *vaddr;\n\t\tskb_frag_t *f = &skb_shinfo(skb)->frags[i];\n\n\t\tpage = alloc_page(gfp_mask);\n\t\tif (!page) {\n\t\t\twhile (head) {\n\t\t\t\tstruct page *next = (struct page *)page_private(head);\n\t\t\t\tput_page(head);\n\t\t\t\thead = next;\n\t\t\t}\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tvaddr = kmap_atomic(skb_frag_page(f));\n\t\tmemcpy(page_address(page),\n\t\t       vaddr + f->page_offset, skb_frag_size(f));\n\t\tkunmap_atomic(vaddr);\n\t\tset_page_private(page, (unsigned long)head);\n\t\thead = page;\n\t}\n\n\t/* skb frags release userspace buffers */\n\tfor (i = 0; i < num_frags; i++)\n\t\tskb_frag_unref(skb, i);\n\n\tuarg->callback(uarg, false);\n\n\t/* skb frags point to kernel buffers */\n\tfor (i = num_frags - 1; i >= 0; i--) {\n\t\t__skb_fill_page_desc(skb, i, head, 0,\n\t\t\t\t     skb_shinfo(skb)->frags[i].size);\n\t\thead = (struct page *)page_private(head);\n\t}\n\n\tskb_shinfo(skb)->tx_flags &= ~SKBTX_DEV_ZEROCOPY;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_copy_ubufs);\n\n/**\n *\tskb_clone\t-\tduplicate an sk_buff\n *\t@skb: buffer to clone\n *\t@gfp_mask: allocation priority\n *\n *\tDuplicate an &sk_buff. The new one is not owned by a socket. Both\n *\tcopies share the same packet data but not structure. The new\n *\tbuffer has a reference count of 1. If the allocation fails the\n *\tfunction returns %NULL otherwise the new buffer is returned.\n *\n *\tIf this function is called from an interrupt gfp_mask() must be\n *\t%GFP_ATOMIC.\n */\n\nstruct sk_buff *skb_clone(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tstruct sk_buff *n;\n\n\tif (skb_orphan_frags(skb, gfp_mask))\n\t\treturn NULL;\n\n\tn = skb + 1;\n\tif (skb->fclone == SKB_FCLONE_ORIG &&\n\t    n->fclone == SKB_FCLONE_UNAVAILABLE) {\n\t\tatomic_t *fclone_ref = (atomic_t *) (n + 1);\n\t\tn->fclone = SKB_FCLONE_CLONE;\n\t\tatomic_inc(fclone_ref);\n\t} else {\n\t\tif (skb_pfmemalloc(skb))\n\t\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\t\tn = kmem_cache_alloc(skbuff_head_cache, gfp_mask);\n\t\tif (!n)\n\t\t\treturn NULL;\n\n\t\tkmemcheck_annotate_bitfield(n, flags1);\n\t\tkmemcheck_annotate_bitfield(n, flags2);\n\t\tn->fclone = SKB_FCLONE_UNAVAILABLE;\n\t}\n\n\treturn __skb_clone(n, skb);\n}\nEXPORT_SYMBOL(skb_clone);\n\nstatic void skb_headers_offset_update(struct sk_buff *skb, int off)\n{\n\t/* Only adjust this if it actually is csum_start rather than csum */\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tskb->csum_start += off;\n\t/* {transport,network,mac}_header and tail are relative to skb->head */\n\tskb->transport_header += off;\n\tskb->network_header   += off;\n\tif (skb_mac_header_was_set(skb))\n\t\tskb->mac_header += off;\n\tskb->inner_transport_header += off;\n\tskb->inner_network_header += off;\n\tskb->inner_mac_header += off;\n}\n\nstatic void copy_skb_header(struct sk_buff *new, const struct sk_buff *old)\n{\n\t__copy_skb_header(new, old);\n\n\tskb_shinfo(new)->gso_size = skb_shinfo(old)->gso_size;\n\tskb_shinfo(new)->gso_segs = skb_shinfo(old)->gso_segs;\n\tskb_shinfo(new)->gso_type = skb_shinfo(old)->gso_type;\n}\n\nstatic inline int skb_alloc_rx_flag(const struct sk_buff *skb)\n{\n\tif (skb_pfmemalloc(skb))\n\t\treturn SKB_ALLOC_RX;\n\treturn 0;\n}\n\n/**\n *\tskb_copy\t-\tcreate private copy of an sk_buff\n *\t@skb: buffer to copy\n *\t@gfp_mask: allocation priority\n *\n *\tMake a copy of both an &sk_buff and its data. This is used when the\n *\tcaller wishes to modify the data and needs a private copy of the\n *\tdata to alter. Returns %NULL on failure or the pointer to the buffer\n *\ton success. The returned buffer has a reference count of 1.\n *\n *\tAs by-product this function converts non-linear &sk_buff to linear\n *\tone, so that &sk_buff becomes completely private and caller is allowed\n *\tto modify all the data of returned buffer. This means that this\n *\tfunction is not recommended for use in circumstances when only\n *\theader is going to be modified. Use pskb_copy() instead.\n */\n\nstruct sk_buff *skb_copy(const struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tint headerlen = skb_headroom(skb);\n\tunsigned int size = skb_end_offset(skb) + skb->data_len;\n\tstruct sk_buff *n = __alloc_skb(size, gfp_mask,\n\t\t\t\t\tskb_alloc_rx_flag(skb), NUMA_NO_NODE);\n\n\tif (!n)\n\t\treturn NULL;\n\n\t/* Set the data pointer */\n\tskb_reserve(n, headerlen);\n\t/* Set the tail pointer and length */\n\tskb_put(n, skb->len);\n\n\tif (skb_copy_bits(skb, -headerlen, n->head, headerlen + skb->len))\n\t\tBUG();\n\n\tcopy_skb_header(n, skb);\n\treturn n;\n}\nEXPORT_SYMBOL(skb_copy);\n\n/**\n *\t__pskb_copy\t-\tcreate copy of an sk_buff with private head.\n *\t@skb: buffer to copy\n *\t@headroom: headroom of new skb\n *\t@gfp_mask: allocation priority\n *\n *\tMake a copy of both an &sk_buff and part of its data, located\n *\tin header. Fragmented data remain shared. This is used when\n *\tthe caller wishes to modify only header of &sk_buff and needs\n *\tprivate copy of the header to alter. Returns %NULL on failure\n *\tor the pointer to the buffer on success.\n *\tThe returned buffer has a reference count of 1.\n */\n\nstruct sk_buff *__pskb_copy(struct sk_buff *skb, int headroom, gfp_t gfp_mask)\n{\n\tunsigned int size = skb_headlen(skb) + headroom;\n\tstruct sk_buff *n = __alloc_skb(size, gfp_mask,\n\t\t\t\t\tskb_alloc_rx_flag(skb), NUMA_NO_NODE);\n\n\tif (!n)\n\t\tgoto out;\n\n\t/* Set the data pointer */\n\tskb_reserve(n, headroom);\n\t/* Set the tail pointer and length */\n\tskb_put(n, skb_headlen(skb));\n\t/* Copy the bytes */\n\tskb_copy_from_linear_data(skb, n->data, n->len);\n\n\tn->truesize += skb->data_len;\n\tn->data_len  = skb->data_len;\n\tn->len\t     = skb->len;\n\n\tif (skb_shinfo(skb)->nr_frags) {\n\t\tint i;\n\n\t\tif (skb_orphan_frags(skb, gfp_mask)) {\n\t\t\tkfree_skb(n);\n\t\t\tn = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_shinfo(n)->frags[i] = skb_shinfo(skb)->frags[i];\n\t\t\tskb_frag_ref(skb, i);\n\t\t}\n\t\tskb_shinfo(n)->nr_frags = i;\n\t}\n\n\tif (skb_has_frag_list(skb)) {\n\t\tskb_shinfo(n)->frag_list = skb_shinfo(skb)->frag_list;\n\t\tskb_clone_fraglist(n);\n\t}\n\n\tcopy_skb_header(n, skb);\nout:\n\treturn n;\n}\nEXPORT_SYMBOL(__pskb_copy);\n\n/**\n *\tpskb_expand_head - reallocate header of &sk_buff\n *\t@skb: buffer to reallocate\n *\t@nhead: room to add at head\n *\t@ntail: room to add at tail\n *\t@gfp_mask: allocation priority\n *\n *\tExpands (or creates identical copy, if @nhead and @ntail are zero)\n *\theader of @skb. &sk_buff itself is not changed. &sk_buff MUST have\n *\treference count of 1. Returns zero in the case of success or error,\n *\tif expansion failed. In the last case, &sk_buff is not changed.\n *\n *\tAll the pointers pointing into skb header may change and must be\n *\treloaded after call to this function.\n */\n\nint pskb_expand_head(struct sk_buff *skb, int nhead, int ntail,\n\t\t     gfp_t gfp_mask)\n{\n\tint i;\n\tu8 *data;\n\tint size = nhead + skb_end_offset(skb) + ntail;\n\tlong off;\n\n\tBUG_ON(nhead < 0);\n\n\tif (skb_shared(skb))\n\t\tBUG();\n\n\tsize = SKB_DATA_ALIGN(size);\n\n\tif (skb_pfmemalloc(skb))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\tdata = kmalloc_reserve(size + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),\n\t\t\t       gfp_mask, NUMA_NO_NODE, NULL);\n\tif (!data)\n\t\tgoto nodata;\n\tsize = SKB_WITH_OVERHEAD(ksize(data));\n\n\t/* Copy only real data... and, alas, header. This should be\n\t * optimized for the cases when header is void.\n\t */\n\tmemcpy(data + nhead, skb->head, skb_tail_pointer(skb) - skb->head);\n\n\tmemcpy((struct skb_shared_info *)(data + size),\n\t       skb_shinfo(skb),\n\t       offsetof(struct skb_shared_info, frags[skb_shinfo(skb)->nr_frags]));\n\n\t/*\n\t * if shinfo is shared we must drop the old head gracefully, but if it\n\t * is not we can just drop the old head and let the existing refcount\n\t * be since all we did is relocate the values\n\t */\n\tif (skb_cloned(skb)) {\n\t\t/* copy this zero copy skb frags */\n\t\tif (skb_orphan_frags(skb, gfp_mask))\n\t\t\tgoto nofrags;\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\t\tskb_frag_ref(skb, i);\n\n\t\tif (skb_has_frag_list(skb))\n\t\t\tskb_clone_fraglist(skb);\n\n\t\tskb_release_data(skb);\n\t} else {\n\t\tskb_free_head(skb);\n\t}\n\toff = (data + nhead) - skb->head;\n\n\tskb->head     = data;\n\tskb->head_frag = 0;\n\tskb->data    += off;\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\n\tskb->end      = size;\n\toff           = nhead;\n#else\n\tskb->end      = skb->head + size;\n#endif\n\tskb->tail\t      += off;\n\tskb_headers_offset_update(skb, nhead);\n\tskb->cloned   = 0;\n\tskb->hdr_len  = 0;\n\tskb->nohdr    = 0;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1);\n\treturn 0;\n\nnofrags:\n\tkfree(data);\nnodata:\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL(pskb_expand_head);\n\n/* Make private copy of skb with writable head and some headroom */\n\nstruct sk_buff *skb_realloc_headroom(struct sk_buff *skb, unsigned int headroom)\n{\n\tstruct sk_buff *skb2;\n\tint delta = headroom - skb_headroom(skb);\n\n\tif (delta <= 0)\n\t\tskb2 = pskb_copy(skb, GFP_ATOMIC);\n\telse {\n\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\tif (skb2 && pskb_expand_head(skb2, SKB_DATA_ALIGN(delta), 0,\n\t\t\t\t\t     GFP_ATOMIC)) {\n\t\t\tkfree_skb(skb2);\n\t\t\tskb2 = NULL;\n\t\t}\n\t}\n\treturn skb2;\n}\nEXPORT_SYMBOL(skb_realloc_headroom);\n\n/**\n *\tskb_copy_expand\t-\tcopy and expand sk_buff\n *\t@skb: buffer to copy\n *\t@newheadroom: new free bytes at head\n *\t@newtailroom: new free bytes at tail\n *\t@gfp_mask: allocation priority\n *\n *\tMake a copy of both an &sk_buff and its data and while doing so\n *\tallocate additional space.\n *\n *\tThis is used when the caller wishes to modify the data and needs a\n *\tprivate copy of the data to alter as well as more space for new fields.\n *\tReturns %NULL on failure or the pointer to the buffer\n *\ton success. The returned buffer has a reference count of 1.\n *\n *\tYou must pass %GFP_ATOMIC as the allocation priority if this function\n *\tis called from an interrupt.\n */\nstruct sk_buff *skb_copy_expand(const struct sk_buff *skb,\n\t\t\t\tint newheadroom, int newtailroom,\n\t\t\t\tgfp_t gfp_mask)\n{\n\t/*\n\t *\tAllocate the copy buffer\n\t */\n\tstruct sk_buff *n = __alloc_skb(newheadroom + skb->len + newtailroom,\n\t\t\t\t\tgfp_mask, skb_alloc_rx_flag(skb),\n\t\t\t\t\tNUMA_NO_NODE);\n\tint oldheadroom = skb_headroom(skb);\n\tint head_copy_len, head_copy_off;\n\n\tif (!n)\n\t\treturn NULL;\n\n\tskb_reserve(n, newheadroom);\n\n\t/* Set the tail pointer and length */\n\tskb_put(n, skb->len);\n\n\thead_copy_len = oldheadroom;\n\thead_copy_off = 0;\n\tif (newheadroom <= head_copy_len)\n\t\thead_copy_len = newheadroom;\n\telse\n\t\thead_copy_off = newheadroom - head_copy_len;\n\n\t/* Copy the linear header and data. */\n\tif (skb_copy_bits(skb, -head_copy_len, n->head + head_copy_off,\n\t\t\t  skb->len + head_copy_len))\n\t\tBUG();\n\n\tcopy_skb_header(n, skb);\n\n\tskb_headers_offset_update(n, newheadroom - oldheadroom);\n\n\treturn n;\n}\nEXPORT_SYMBOL(skb_copy_expand);\n\n/**\n *\tskb_pad\t\t\t-\tzero pad the tail of an skb\n *\t@skb: buffer to pad\n *\t@pad: space to pad\n *\n *\tEnsure that a buffer is followed by a padding area that is zero\n *\tfilled. Used by network drivers which may DMA or transfer data\n *\tbeyond the buffer end onto the wire.\n *\n *\tMay return error in out of memory cases. The skb is freed on error.\n */\n\nint skb_pad(struct sk_buff *skb, int pad)\n{\n\tint err;\n\tint ntail;\n\n\t/* If the skbuff is non linear tailroom is always zero.. */\n\tif (!skb_cloned(skb) && skb_tailroom(skb) >= pad) {\n\t\tmemset(skb->data+skb->len, 0, pad);\n\t\treturn 0;\n\t}\n\n\tntail = skb->data_len + pad - (skb->end - skb->tail);\n\tif (likely(skb_cloned(skb) || ntail > 0)) {\n\t\terr = pskb_expand_head(skb, 0, ntail, GFP_ATOMIC);\n\t\tif (unlikely(err))\n\t\t\tgoto free_skb;\n\t}\n\n\t/* FIXME: The use of this function with non-linear skb's really needs\n\t * to be audited.\n\t */\n\terr = skb_linearize(skb);\n\tif (unlikely(err))\n\t\tgoto free_skb;\n\n\tmemset(skb->data + skb->len, 0, pad);\n\treturn 0;\n\nfree_skb:\n\tkfree_skb(skb);\n\treturn err;\n}\nEXPORT_SYMBOL(skb_pad);\n\n/**\n *\tpskb_put - add data to the tail of a potentially fragmented buffer\n *\t@skb: start of the buffer to use\n *\t@tail: tail fragment of the buffer to use\n *\t@len: amount of data to add\n *\n *\tThis function extends the used data area of the potentially\n *\tfragmented buffer. @tail must be the last fragment of @skb -- or\n *\t@skb itself. If this would exceed the total buffer size the kernel\n *\twill panic. A pointer to the first byte of the extra data is\n *\treturned.\n */\n\nunsigned char *pskb_put(struct sk_buff *skb, struct sk_buff *tail, int len)\n{\n\tif (tail != skb) {\n\t\tskb->data_len += len;\n\t\tskb->len += len;\n\t}\n\treturn skb_put(tail, len);\n}\nEXPORT_SYMBOL_GPL(pskb_put);\n\n/**\n *\tskb_put - add data to a buffer\n *\t@skb: buffer to use\n *\t@len: amount of data to add\n *\n *\tThis function extends the used data area of the buffer. If this would\n *\texceed the total buffer size the kernel will panic. A pointer to the\n *\tfirst byte of the extra data is returned.\n */\nunsigned char *skb_put(struct sk_buff *skb, unsigned int len)\n{\n\tunsigned char *tmp = skb_tail_pointer(skb);\n\tSKB_LINEAR_ASSERT(skb);\n\tskb->tail += len;\n\tskb->len  += len;\n\tif (unlikely(skb->tail > skb->end))\n\t\tskb_over_panic(skb, len, __builtin_return_address(0));\n\treturn tmp;\n}\nEXPORT_SYMBOL(skb_put);\n\n/**\n *\tskb_push - add data to the start of a buffer\n *\t@skb: buffer to use\n *\t@len: amount of data to add\n *\n *\tThis function extends the used data area of the buffer at the buffer\n *\tstart. If this would exceed the total buffer headroom the kernel will\n *\tpanic. A pointer to the first byte of the extra data is returned.\n */\nunsigned char *skb_push(struct sk_buff *skb, unsigned int len)\n{\n\tskb->data -= len;\n\tskb->len  += len;\n\tif (unlikely(skb->data<skb->head))\n\t\tskb_under_panic(skb, len, __builtin_return_address(0));\n\treturn skb->data;\n}\nEXPORT_SYMBOL(skb_push);\n\n/**\n *\tskb_pull - remove data from the start of a buffer\n *\t@skb: buffer to use\n *\t@len: amount of data to remove\n *\n *\tThis function removes data from the start of a buffer, returning\n *\tthe memory to the headroom. A pointer to the next data in the buffer\n *\tis returned. Once the data has been pulled future pushes will overwrite\n *\tthe old data.\n */\nunsigned char *skb_pull(struct sk_buff *skb, unsigned int len)\n{\n\treturn skb_pull_inline(skb, len);\n}\nEXPORT_SYMBOL(skb_pull);\n\n/**\n *\tskb_trim - remove end from a buffer\n *\t@skb: buffer to alter\n *\t@len: new length\n *\n *\tCut the length of a buffer down by removing data from the tail. If\n *\tthe buffer is already under the length specified it is not modified.\n *\tThe skb must be linear.\n */\nvoid skb_trim(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->len > len)\n\t\t__skb_trim(skb, len);\n}\nEXPORT_SYMBOL(skb_trim);\n\n/* Trims skb to length len. It can change skb pointers.\n */\n\nint ___pskb_trim(struct sk_buff *skb, unsigned int len)\n{\n\tstruct sk_buff **fragp;\n\tstruct sk_buff *frag;\n\tint offset = skb_headlen(skb);\n\tint nfrags = skb_shinfo(skb)->nr_frags;\n\tint i;\n\tint err;\n\n\tif (skb_cloned(skb) &&\n\t    unlikely((err = pskb_expand_head(skb, 0, 0, GFP_ATOMIC))))\n\t\treturn err;\n\n\ti = 0;\n\tif (offset >= len)\n\t\tgoto drop_pages;\n\n\tfor (; i < nfrags; i++) {\n\t\tint end = offset + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (end < len) {\n\t\t\toffset = end;\n\t\t\tcontinue;\n\t\t}\n\n\t\tskb_frag_size_set(&skb_shinfo(skb)->frags[i++], len - offset);\n\ndrop_pages:\n\t\tskb_shinfo(skb)->nr_frags = i;\n\n\t\tfor (; i < nfrags; i++)\n\t\t\tskb_frag_unref(skb, i);\n\n\t\tif (skb_has_frag_list(skb))\n\t\t\tskb_drop_fraglist(skb);\n\t\tgoto done;\n\t}\n\n\tfor (fragp = &skb_shinfo(skb)->frag_list; (frag = *fragp);\n\t     fragp = &frag->next) {\n\t\tint end = offset + frag->len;\n\n\t\tif (skb_shared(frag)) {\n\t\t\tstruct sk_buff *nfrag;\n\n\t\t\tnfrag = skb_clone(frag, GFP_ATOMIC);\n\t\t\tif (unlikely(!nfrag))\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tnfrag->next = frag->next;\n\t\t\tconsume_skb(frag);\n\t\t\tfrag = nfrag;\n\t\t\t*fragp = frag;\n\t\t}\n\n\t\tif (end < len) {\n\t\t\toffset = end;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (end > len &&\n\t\t    unlikely((err = pskb_trim(frag, len - offset))))\n\t\t\treturn err;\n\n\t\tif (frag->next)\n\t\t\tskb_drop_list(&frag->next);\n\t\tbreak;\n\t}\n\ndone:\n\tif (len > skb_headlen(skb)) {\n\t\tskb->data_len -= skb->len - len;\n\t\tskb->len       = len;\n\t} else {\n\t\tskb->len       = len;\n\t\tskb->data_len  = 0;\n\t\tskb_set_tail_pointer(skb, len);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(___pskb_trim);\n\n/**\n *\t__pskb_pull_tail - advance tail of skb header\n *\t@skb: buffer to reallocate\n *\t@delta: number of bytes to advance tail\n *\n *\tThe function makes a sense only on a fragmented &sk_buff,\n *\tit expands header moving its tail forward and copying necessary\n *\tdata from fragmented part.\n *\n *\t&sk_buff MUST have reference count of 1.\n *\n *\tReturns %NULL (and &sk_buff does not change) if pull failed\n *\tor value of new tail of skb in the case of success.\n *\n *\tAll the pointers pointing into skb header may change and must be\n *\treloaded after call to this function.\n */\n\n/* Moves tail of skb head forward, copying data from fragmented part,\n * when it is necessary.\n * 1. It may fail due to malloc failure.\n * 2. It may change skb pointers.\n *\n * It is pretty complicated. Luckily, it is called only in exceptional cases.\n */\nunsigned char *__pskb_pull_tail(struct sk_buff *skb, int delta)\n{\n\t/* If skb has not enough free space at tail, get new one\n\t * plus 128 bytes for future expansions. If we have enough\n\t * room at tail, reallocate without expansion only if skb is cloned.\n\t */\n\tint i, k, eat = (skb->tail + delta) - skb->end;\n\n\tif (eat > 0 || skb_cloned(skb)) {\n\t\tif (pskb_expand_head(skb, 0, eat > 0 ? eat + 128 : 0,\n\t\t\t\t     GFP_ATOMIC))\n\t\t\treturn NULL;\n\t}\n\n\tif (skb_copy_bits(skb, skb_headlen(skb), skb_tail_pointer(skb), delta))\n\t\tBUG();\n\n\t/* Optimization: no fragments, no reasons to preestimate\n\t * size of pulled pages. Superb.\n\t */\n\tif (!skb_has_frag_list(skb))\n\t\tgoto pull_pages;\n\n\t/* Estimate size of pulled pages. */\n\teat = delta;\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (size >= eat)\n\t\t\tgoto pull_pages;\n\t\teat -= size;\n\t}\n\n\t/* If we need update frag list, we are in troubles.\n\t * Certainly, it possible to add an offset to skb data,\n\t * but taking into account that pulling is expected to\n\t * be very rare operation, it is worth to fight against\n\t * further bloating skb head and crucify ourselves here instead.\n\t * Pure masohism, indeed. 8)8)\n\t */\n\tif (eat) {\n\t\tstruct sk_buff *list = skb_shinfo(skb)->frag_list;\n\t\tstruct sk_buff *clone = NULL;\n\t\tstruct sk_buff *insp = NULL;\n\n\t\tdo {\n\t\t\tBUG_ON(!list);\n\n\t\t\tif (list->len <= eat) {\n\t\t\t\t/* Eaten as whole. */\n\t\t\t\teat -= list->len;\n\t\t\t\tlist = list->next;\n\t\t\t\tinsp = list;\n\t\t\t} else {\n\t\t\t\t/* Eaten partially. */\n\n\t\t\t\tif (skb_shared(list)) {\n\t\t\t\t\t/* Sucks! We need to fork list. :-( */\n\t\t\t\t\tclone = skb_clone(list, GFP_ATOMIC);\n\t\t\t\t\tif (!clone)\n\t\t\t\t\t\treturn NULL;\n\t\t\t\t\tinsp = list->next;\n\t\t\t\t\tlist = clone;\n\t\t\t\t} else {\n\t\t\t\t\t/* This may be pulled without\n\t\t\t\t\t * problems. */\n\t\t\t\t\tinsp = list;\n\t\t\t\t}\n\t\t\t\tif (!pskb_pull(list, eat)) {\n\t\t\t\t\tkfree_skb(clone);\n\t\t\t\t\treturn NULL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} while (eat);\n\n\t\t/* Free pulled out fragments. */\n\t\twhile ((list = skb_shinfo(skb)->frag_list) != insp) {\n\t\t\tskb_shinfo(skb)->frag_list = list->next;\n\t\t\tkfree_skb(list);\n\t\t}\n\t\t/* And insert new clone at head. */\n\t\tif (clone) {\n\t\t\tclone->next = list;\n\t\t\tskb_shinfo(skb)->frag_list = clone;\n\t\t}\n\t}\n\t/* Success! Now we may commit changes to skb data. */\n\npull_pages:\n\teat = delta;\n\tk = 0;\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (size <= eat) {\n\t\t\tskb_frag_unref(skb, i);\n\t\t\teat -= size;\n\t\t} else {\n\t\t\tskb_shinfo(skb)->frags[k] = skb_shinfo(skb)->frags[i];\n\t\t\tif (eat) {\n\t\t\t\tskb_shinfo(skb)->frags[k].page_offset += eat;\n\t\t\t\tskb_frag_size_sub(&skb_shinfo(skb)->frags[k], eat);\n\t\t\t\teat = 0;\n\t\t\t}\n\t\t\tk++;\n\t\t}\n\t}\n\tskb_shinfo(skb)->nr_frags = k;\n\n\tskb->tail     += delta;\n\tskb->data_len -= delta;\n\n\treturn skb_tail_pointer(skb);\n}\nEXPORT_SYMBOL(__pskb_pull_tail);\n\n/**\n *\tskb_copy_bits - copy bits from skb to kernel buffer\n *\t@skb: source skb\n *\t@offset: offset in source\n *\t@to: destination buffer\n *\t@len: number of bytes to copy\n *\n *\tCopy the specified number of bytes from the source skb to the\n *\tdestination buffer.\n *\n *\tCAUTION ! :\n *\t\tIf its prototype is ever changed,\n *\t\tcheck arch/{*}/net/{*}.S files,\n *\t\tsince it is called from BPF assembly code.\n */\nint skb_copy_bits(const struct sk_buff *skb, int offset, void *to, int len)\n{\n\tint start = skb_headlen(skb);\n\tstruct sk_buff *frag_iter;\n\tint i, copy;\n\n\tif (offset > (int)skb->len - len)\n\t\tgoto fault;\n\n\t/* Copy header. */\n\tif ((copy = start - offset) > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tskb_copy_from_linear_data_offset(skb, offset, to, copy);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn 0;\n\t\toffset += copy;\n\t\tto     += copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\t\tskb_frag_t *f = &skb_shinfo(skb)->frags[i];\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(f);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\n\t\t\tvaddr = kmap_atomic(skb_frag_page(f));\n\t\t\tmemcpy(to,\n\t\t\t       vaddr + f->page_offset + offset - start,\n\t\t\t       copy);\n\t\t\tkunmap_atomic(vaddr);\n\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tif (skb_copy_bits(frag_iter, offset - start, to, copy))\n\t\t\t\tgoto fault;\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tif (!len)\n\t\treturn 0;\n\nfault:\n\treturn -EFAULT;\n}\nEXPORT_SYMBOL(skb_copy_bits);\n\n/*\n * Callback from splice_to_pipe(), if we need to release some pages\n * at the end of the spd in case we error'ed out in filling the pipe.\n */\nstatic void sock_spd_release(struct splice_pipe_desc *spd, unsigned int i)\n{\n\tput_page(spd->pages[i]);\n}\n\nstatic struct page *linear_to_page(struct page *page, unsigned int *len,\n\t\t\t\t   unsigned int *offset,\n\t\t\t\t   struct sock *sk)\n{\n\tstruct page_frag *pfrag = sk_page_frag(sk);\n\n\tif (!sk_page_frag_refill(sk, pfrag))\n\t\treturn NULL;\n\n\t*len = min_t(unsigned int, *len, pfrag->size - pfrag->offset);\n\n\tmemcpy(page_address(pfrag->page) + pfrag->offset,\n\t       page_address(page) + *offset, *len);\n\t*offset = pfrag->offset;\n\tpfrag->offset += *len;\n\n\treturn pfrag->page;\n}\n\nstatic bool spd_can_coalesce(const struct splice_pipe_desc *spd,\n\t\t\t     struct page *page,\n\t\t\t     unsigned int offset)\n{\n\treturn\tspd->nr_pages &&\n\t\tspd->pages[spd->nr_pages - 1] == page &&\n\t\t(spd->partial[spd->nr_pages - 1].offset +\n\t\t spd->partial[spd->nr_pages - 1].len == offset);\n}\n\n/*\n * Fill page/offset/length into spd, if it can hold more pages.\n */\nstatic bool spd_fill_page(struct splice_pipe_desc *spd,\n\t\t\t  struct pipe_inode_info *pipe, struct page *page,\n\t\t\t  unsigned int *len, unsigned int offset,\n\t\t\t  bool linear,\n\t\t\t  struct sock *sk)\n{\n\tif (unlikely(spd->nr_pages == MAX_SKB_FRAGS))\n\t\treturn true;\n\n\tif (linear) {\n\t\tpage = linear_to_page(page, len, &offset, sk);\n\t\tif (!page)\n\t\t\treturn true;\n\t}\n\tif (spd_can_coalesce(spd, page, offset)) {\n\t\tspd->partial[spd->nr_pages - 1].len += *len;\n\t\treturn false;\n\t}\n\tget_page(page);\n\tspd->pages[spd->nr_pages] = page;\n\tspd->partial[spd->nr_pages].len = *len;\n\tspd->partial[spd->nr_pages].offset = offset;\n\tspd->nr_pages++;\n\n\treturn false;\n}\n\nstatic bool __splice_segment(struct page *page, unsigned int poff,\n\t\t\t     unsigned int plen, unsigned int *off,\n\t\t\t     unsigned int *len,\n\t\t\t     struct splice_pipe_desc *spd, bool linear,\n\t\t\t     struct sock *sk,\n\t\t\t     struct pipe_inode_info *pipe)\n{\n\tif (!*len)\n\t\treturn true;\n\n\t/* skip this segment if already processed */\n\tif (*off >= plen) {\n\t\t*off -= plen;\n\t\treturn false;\n\t}\n\n\t/* ignore any bits we already processed */\n\tpoff += *off;\n\tplen -= *off;\n\t*off = 0;\n\n\tdo {\n\t\tunsigned int flen = min(*len, plen);\n\n\t\tif (spd_fill_page(spd, pipe, page, &flen, poff,\n\t\t\t\t  linear, sk))\n\t\t\treturn true;\n\t\tpoff += flen;\n\t\tplen -= flen;\n\t\t*len -= flen;\n\t} while (*len && plen);\n\n\treturn false;\n}\n\n/*\n * Map linear and fragment data from the skb to spd. It reports true if the\n * pipe is full or if we already spliced the requested length.\n */\nstatic bool __skb_splice_bits(struct sk_buff *skb, struct pipe_inode_info *pipe,\n\t\t\t      unsigned int *offset, unsigned int *len,\n\t\t\t      struct splice_pipe_desc *spd, struct sock *sk)\n{\n\tint seg;\n\n\t/* map the linear part :\n\t * If skb->head_frag is set, this 'linear' part is backed by a\n\t * fragment, and if the head is not shared with any clones then\n\t * we can avoid a copy since we own the head portion of this page.\n\t */\n\tif (__splice_segment(virt_to_page(skb->data),\n\t\t\t     (unsigned long) skb->data & (PAGE_SIZE - 1),\n\t\t\t     skb_headlen(skb),\n\t\t\t     offset, len, spd,\n\t\t\t     skb_head_is_locked(skb),\n\t\t\t     sk, pipe))\n\t\treturn true;\n\n\t/*\n\t * then map the fragments\n\t */\n\tfor (seg = 0; seg < skb_shinfo(skb)->nr_frags; seg++) {\n\t\tconst skb_frag_t *f = &skb_shinfo(skb)->frags[seg];\n\n\t\tif (__splice_segment(skb_frag_page(f),\n\t\t\t\t     f->page_offset, skb_frag_size(f),\n\t\t\t\t     offset, len, spd, false, sk, pipe))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * Map data from the skb to a pipe. Should handle both the linear part,\n * the fragments, and the frag list. It does NOT handle frag lists within\n * the frag list, if such a thing exists. We'd probably need to recurse to\n * handle that cleanly.\n */\nint skb_splice_bits(struct sk_buff *skb, unsigned int offset,\n\t\t    struct pipe_inode_info *pipe, unsigned int tlen,\n\t\t    unsigned int flags)\n{\n\tstruct partial_page partial[MAX_SKB_FRAGS];\n\tstruct page *pages[MAX_SKB_FRAGS];\n\tstruct splice_pipe_desc spd = {\n\t\t.pages = pages,\n\t\t.partial = partial,\n\t\t.nr_pages_max = MAX_SKB_FRAGS,\n\t\t.flags = flags,\n\t\t.ops = &nosteal_pipe_buf_ops,\n\t\t.spd_release = sock_spd_release,\n\t};\n\tstruct sk_buff *frag_iter;\n\tstruct sock *sk = skb->sk;\n\tint ret = 0;\n\n\t/*\n\t * __skb_splice_bits() only fails if the output has no room left,\n\t * so no point in going over the frag_list for the error case.\n\t */\n\tif (__skb_splice_bits(skb, pipe, &offset, &tlen, &spd, sk))\n\t\tgoto done;\n\telse if (!tlen)\n\t\tgoto done;\n\n\t/*\n\t * now see if we have a frag_list to map\n\t */\n\tskb_walk_frags(skb, frag_iter) {\n\t\tif (!tlen)\n\t\t\tbreak;\n\t\tif (__skb_splice_bits(frag_iter, pipe, &offset, &tlen, &spd, sk))\n\t\t\tbreak;\n\t}\n\ndone:\n\tif (spd.nr_pages) {\n\t\t/*\n\t\t * Drop the socket lock, otherwise we have reverse\n\t\t * locking dependencies between sk_lock and i_mutex\n\t\t * here as compared to sendfile(). We enter here\n\t\t * with the socket lock held, and splice_to_pipe() will\n\t\t * grab the pipe inode lock. For sendfile() emulation,\n\t\t * we call into ->sendpage() with the i_mutex lock held\n\t\t * and networking will grab the socket lock.\n\t\t */\n\t\trelease_sock(sk);\n\t\tret = splice_to_pipe(pipe, &spd);\n\t\tlock_sock(sk);\n\t}\n\n\treturn ret;\n}\n\n/**\n *\tskb_store_bits - store bits from kernel buffer to skb\n *\t@skb: destination buffer\n *\t@offset: offset in destination\n *\t@from: source buffer\n *\t@len: number of bytes to copy\n *\n *\tCopy the specified number of bytes from the source buffer to the\n *\tdestination skb.  This function handles all the messy bits of\n *\ttraversing fragment lists and such.\n */\n\nint skb_store_bits(struct sk_buff *skb, int offset, const void *from, int len)\n{\n\tint start = skb_headlen(skb);\n\tstruct sk_buff *frag_iter;\n\tint i, copy;\n\n\tif (offset > (int)skb->len - len)\n\t\tgoto fault;\n\n\tif ((copy = start - offset) > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tskb_copy_to_linear_data_offset(skb, offset, from, copy);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn 0;\n\t\toffset += copy;\n\t\tfrom += copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(frag);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\n\t\t\tvaddr = kmap_atomic(skb_frag_page(frag));\n\t\t\tmemcpy(vaddr + frag->page_offset + offset - start,\n\t\t\t       from, copy);\n\t\t\tkunmap_atomic(vaddr);\n\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tfrom += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tif (skb_store_bits(frag_iter, offset - start,\n\t\t\t\t\t   from, copy))\n\t\t\t\tgoto fault;\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tfrom += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tif (!len)\n\t\treturn 0;\n\nfault:\n\treturn -EFAULT;\n}\nEXPORT_SYMBOL(skb_store_bits);\n\n/* Checksum skb data. */\n__wsum __skb_checksum(const struct sk_buff *skb, int offset, int len,\n\t\t      __wsum csum, const struct skb_checksum_ops *ops)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\tint pos = 0;\n\n\t/* Checksum header. */\n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tcsum = ops->update(skb->data + offset, copy, csum);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn csum;\n\t\toffset += copy;\n\t\tpos\t= copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(frag);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\t__wsum csum2;\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tvaddr = kmap_atomic(skb_frag_page(frag));\n\t\t\tcsum2 = ops->update(vaddr + frag->page_offset +\n\t\t\t\t\t    offset - start, copy, 0);\n\t\t\tkunmap_atomic(vaddr);\n\t\t\tcsum = ops->combine(csum, csum2, pos, copy);\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\t__wsum csum2;\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tcsum2 = __skb_checksum(frag_iter, offset - start,\n\t\t\t\t\t       copy, 0, ops);\n\t\t\tcsum = ops->combine(csum, csum2, pos, copy);\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tBUG_ON(len);\n\n\treturn csum;\n}\nEXPORT_SYMBOL(__skb_checksum);\n\n__wsum skb_checksum(const struct sk_buff *skb, int offset,\n\t\t    int len, __wsum csum)\n{\n\tconst struct skb_checksum_ops ops = {\n\t\t.update  = csum_partial_ext,\n\t\t.combine = csum_block_add_ext,\n\t};\n\n\treturn __skb_checksum(skb, offset, len, csum, &ops);\n}\nEXPORT_SYMBOL(skb_checksum);\n\n/* Both of above in one bottle. */\n\n__wsum skb_copy_and_csum_bits(const struct sk_buff *skb, int offset,\n\t\t\t\t    u8 *to, int len, __wsum csum)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\tint pos = 0;\n\n\t/* Copy header. */\n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tcsum = csum_partial_copy_nocheck(skb->data + offset, to,\n\t\t\t\t\t\t copy, csum);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn csum;\n\t\toffset += copy;\n\t\tto     += copy;\n\t\tpos\t= copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\t__wsum csum2;\n\t\t\tu8 *vaddr;\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tvaddr = kmap_atomic(skb_frag_page(frag));\n\t\t\tcsum2 = csum_partial_copy_nocheck(vaddr +\n\t\t\t\t\t\t\t  frag->page_offset +\n\t\t\t\t\t\t\t  offset - start, to,\n\t\t\t\t\t\t\t  copy, 0);\n\t\t\tkunmap_atomic(vaddr);\n\t\t\tcsum = csum_block_add(csum, csum2, pos);\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\t__wsum csum2;\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tcsum2 = skb_copy_and_csum_bits(frag_iter,\n\t\t\t\t\t\t       offset - start,\n\t\t\t\t\t\t       to, copy, 0);\n\t\t\tcsum = csum_block_add(csum, csum2, pos);\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tBUG_ON(len);\n\treturn csum;\n}\nEXPORT_SYMBOL(skb_copy_and_csum_bits);\n\n /**\n *\tskb_zerocopy_headlen - Calculate headroom needed for skb_zerocopy()\n *\t@from: source buffer\n *\n *\tCalculates the amount of linear headroom needed in the 'to' skb passed\n *\tinto skb_zerocopy().\n */\nunsigned int\nskb_zerocopy_headlen(const struct sk_buff *from)\n{\n\tunsigned int hlen = 0;\n\n\tif (!from->head_frag ||\n\t    skb_headlen(from) < L1_CACHE_BYTES ||\n\t    skb_shinfo(from)->nr_frags >= MAX_SKB_FRAGS)\n\t\thlen = skb_headlen(from);\n\n\tif (skb_has_frag_list(from))\n\t\thlen = from->len;\n\n\treturn hlen;\n}\nEXPORT_SYMBOL_GPL(skb_zerocopy_headlen);\n\n/**\n *\tskb_zerocopy - Zero copy skb to skb\n *\t@to: destination buffer\n *\t@from: source buffer\n *\t@len: number of bytes to copy from source buffer\n *\t@hlen: size of linear headroom in destination buffer\n *\n *\tCopies up to `len` bytes from `from` to `to` by creating references\n *\tto the frags in the source buffer.\n *\n *\tThe `hlen` as calculated by skb_zerocopy_headlen() specifies the\n *\theadroom in the `to` buffer.\n */\nvoid\nskb_zerocopy(struct sk_buff *to, const struct sk_buff *from, int len, int hlen)\n{\n\tint i, j = 0;\n\tint plen = 0; /* length of skb->head fragment */\n\tstruct page *page;\n\tunsigned int offset;\n\n\tBUG_ON(!from->head_frag && !hlen);\n\n\t/* dont bother with small payloads */\n\tif (len <= skb_tailroom(to)) {\n\t\tskb_copy_bits(from, 0, skb_put(to, len), len);\n\t\treturn;\n\t}\n\n\tif (hlen) {\n\t\tskb_copy_bits(from, 0, skb_put(to, hlen), hlen);\n\t\tlen -= hlen;\n\t} else {\n\t\tplen = min_t(int, skb_headlen(from), len);\n\t\tif (plen) {\n\t\t\tpage = virt_to_head_page(from->head);\n\t\t\toffset = from->data - (unsigned char *)page_address(page);\n\t\t\t__skb_fill_page_desc(to, 0, page, offset, plen);\n\t\t\tget_page(page);\n\t\t\tj = 1;\n\t\t\tlen -= plen;\n\t\t}\n\t}\n\n\tto->truesize += len + plen;\n\tto->len += len + plen;\n\tto->data_len += len + plen;\n\n\tfor (i = 0; i < skb_shinfo(from)->nr_frags; i++) {\n\t\tif (!len)\n\t\t\tbreak;\n\t\tskb_shinfo(to)->frags[j] = skb_shinfo(from)->frags[i];\n\t\tskb_shinfo(to)->frags[j].size = min_t(int, skb_shinfo(to)->frags[j].size, len);\n\t\tlen -= skb_shinfo(to)->frags[j].size;\n\t\tskb_frag_ref(to, j);\n\t\tj++;\n\t}\n\tskb_shinfo(to)->nr_frags = j;\n}\nEXPORT_SYMBOL_GPL(skb_zerocopy);\n\nvoid skb_copy_and_csum_dev(const struct sk_buff *skb, u8 *to)\n{\n\t__wsum csum;\n\tlong csstart;\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tcsstart = skb_checksum_start_offset(skb);\n\telse\n\t\tcsstart = skb_headlen(skb);\n\n\tBUG_ON(csstart > skb_headlen(skb));\n\n\tskb_copy_from_linear_data(skb, to, csstart);\n\n\tcsum = 0;\n\tif (csstart != skb->len)\n\t\tcsum = skb_copy_and_csum_bits(skb, csstart, to + csstart,\n\t\t\t\t\t      skb->len - csstart, 0);\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tlong csstuff = csstart + skb->csum_offset;\n\n\t\t*((__sum16 *)(to + csstuff)) = csum_fold(csum);\n\t}\n}\nEXPORT_SYMBOL(skb_copy_and_csum_dev);\n\n/**\n *\tskb_dequeue - remove from the head of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the head of the list. The list lock is taken so the function\n *\tmay be used safely with other locking list functions. The head item is\n *\treturned or %NULL if the list is empty.\n */\n\nstruct sk_buff *skb_dequeue(struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\tstruct sk_buff *result;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tresult = __skb_dequeue(list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n\treturn result;\n}\nEXPORT_SYMBOL(skb_dequeue);\n\n/**\n *\tskb_dequeue_tail - remove from the tail of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the tail of the list. The list lock is taken so the function\n *\tmay be used safely with other locking list functions. The tail item is\n *\treturned or %NULL if the list is empty.\n */\nstruct sk_buff *skb_dequeue_tail(struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\tstruct sk_buff *result;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tresult = __skb_dequeue_tail(list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n\treturn result;\n}\nEXPORT_SYMBOL(skb_dequeue_tail);\n\n/**\n *\tskb_queue_purge - empty a list\n *\t@list: list to empty\n *\n *\tDelete all buffers on an &sk_buff list. Each buffer is removed from\n *\tthe list and one reference dropped. This function takes the list\n *\tlock and is atomic with respect to other list locking functions.\n */\nvoid skb_queue_purge(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb;\n\twhile ((skb = skb_dequeue(list)) != NULL)\n\t\tkfree_skb(skb);\n}\nEXPORT_SYMBOL(skb_queue_purge);\n\n/**\n *\tskb_queue_head - queue a buffer at the list head\n *\t@list: list to use\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer at the start of the list. This function takes the\n *\tlist lock and can be used safely with other locking &sk_buff functions\n *\tsafely.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_queue_head(struct sk_buff_head *list, struct sk_buff *newsk)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_queue_head(list, newsk);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_queue_head);\n\n/**\n *\tskb_queue_tail - queue a buffer at the list tail\n *\t@list: list to use\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer at the tail of the list. This function takes the\n *\tlist lock and can be used safely with other locking &sk_buff functions\n *\tsafely.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_queue_tail(struct sk_buff_head *list, struct sk_buff *newsk)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_queue_tail(list, newsk);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_queue_tail);\n\n/**\n *\tskb_unlink\t-\tremove a buffer from a list\n *\t@skb: buffer to remove\n *\t@list: list to use\n *\n *\tRemove a packet from a list. The list locks are taken and this\n *\tfunction is atomic with respect to other list locked calls\n *\n *\tYou must know what list the SKB is on.\n */\nvoid skb_unlink(struct sk_buff *skb, struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_unlink(skb, list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_unlink);\n\n/**\n *\tskb_append\t-\tappend a buffer\n *\t@old: buffer to insert after\n *\t@newsk: buffer to insert\n *\t@list: list to use\n *\n *\tPlace a packet after a given packet in a list. The list locks are taken\n *\tand this function is atomic with respect to other list locked calls.\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_append(struct sk_buff *old, struct sk_buff *newsk, struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_queue_after(list, old, newsk);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_append);\n\n/**\n *\tskb_insert\t-\tinsert a buffer\n *\t@old: buffer to insert before\n *\t@newsk: buffer to insert\n *\t@list: list to use\n *\n *\tPlace a packet before a given packet in a list. The list locks are\n * \ttaken and this function is atomic with respect to other list locked\n *\tcalls.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_insert(struct sk_buff *old, struct sk_buff *newsk, struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_insert(newsk, old->prev, old, list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_insert);\n\nstatic inline void skb_split_inside_header(struct sk_buff *skb,\n\t\t\t\t\t   struct sk_buff* skb1,\n\t\t\t\t\t   const u32 len, const int pos)\n{\n\tint i;\n\n\tskb_copy_from_linear_data_offset(skb, len, skb_put(skb1, pos - len),\n\t\t\t\t\t pos - len);\n\t/* And move data appendix as is. */\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\tskb_shinfo(skb1)->frags[i] = skb_shinfo(skb)->frags[i];\n\n\tskb_shinfo(skb1)->nr_frags = skb_shinfo(skb)->nr_frags;\n\tskb_shinfo(skb)->nr_frags  = 0;\n\tskb1->data_len\t\t   = skb->data_len;\n\tskb1->len\t\t   += skb1->data_len;\n\tskb->data_len\t\t   = 0;\n\tskb->len\t\t   = len;\n\tskb_set_tail_pointer(skb, len);\n}\n\nstatic inline void skb_split_no_header(struct sk_buff *skb,\n\t\t\t\t       struct sk_buff* skb1,\n\t\t\t\t       const u32 len, int pos)\n{\n\tint i, k = 0;\n\tconst int nfrags = skb_shinfo(skb)->nr_frags;\n\n\tskb_shinfo(skb)->nr_frags = 0;\n\tskb1->len\t\t  = skb1->data_len = skb->len - len;\n\tskb->len\t\t  = len;\n\tskb->data_len\t\t  = len - pos;\n\n\tfor (i = 0; i < nfrags; i++) {\n\t\tint size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (pos + size > len) {\n\t\t\tskb_shinfo(skb1)->frags[k] = skb_shinfo(skb)->frags[i];\n\n\t\t\tif (pos < len) {\n\t\t\t\t/* Split frag.\n\t\t\t\t * We have two variants in this case:\n\t\t\t\t * 1. Move all the frag to the second\n\t\t\t\t *    part, if it is possible. F.e.\n\t\t\t\t *    this approach is mandatory for TUX,\n\t\t\t\t *    where splitting is expensive.\n\t\t\t\t * 2. Split is accurately. We make this.\n\t\t\t\t */\n\t\t\t\tskb_frag_ref(skb, i);\n\t\t\t\tskb_shinfo(skb1)->frags[0].page_offset += len - pos;\n\t\t\t\tskb_frag_size_sub(&skb_shinfo(skb1)->frags[0], len - pos);\n\t\t\t\tskb_frag_size_set(&skb_shinfo(skb)->frags[i], len - pos);\n\t\t\t\tskb_shinfo(skb)->nr_frags++;\n\t\t\t}\n\t\t\tk++;\n\t\t} else\n\t\t\tskb_shinfo(skb)->nr_frags++;\n\t\tpos += size;\n\t}\n\tskb_shinfo(skb1)->nr_frags = k;\n}\n\n/**\n * skb_split - Split fragmented skb to two parts at length len.\n * @skb: the buffer to split\n * @skb1: the buffer to receive the second part\n * @len: new length for skb\n */\nvoid skb_split(struct sk_buff *skb, struct sk_buff *skb1, const u32 len)\n{\n\tint pos = skb_headlen(skb);\n\n\tskb_shinfo(skb1)->tx_flags = skb_shinfo(skb)->tx_flags & SKBTX_SHARED_FRAG;\n\tif (len < pos)\t/* Split line is inside header. */\n\t\tskb_split_inside_header(skb, skb1, len, pos);\n\telse\t\t/* Second chunk has no header, nothing to copy. */\n\t\tskb_split_no_header(skb, skb1, len, pos);\n}\nEXPORT_SYMBOL(skb_split);\n\n/* Shifting from/to a cloned skb is a no-go.\n *\n * Caller cannot keep skb_shinfo related pointers past calling here!\n */\nstatic int skb_prepare_for_shift(struct sk_buff *skb)\n{\n\treturn skb_cloned(skb) && pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n}\n\n/**\n * skb_shift - Shifts paged data partially from skb to another\n * @tgt: buffer into which tail data gets added\n * @skb: buffer from which the paged data comes from\n * @shiftlen: shift up to this many bytes\n *\n * Attempts to shift up to shiftlen worth of bytes, which may be less than\n * the length of the skb, from skb to tgt. Returns number bytes shifted.\n * It's up to caller to free skb if everything was shifted.\n *\n * If @tgt runs out of frags, the whole operation is aborted.\n *\n * Skb cannot include anything else but paged data while tgt is allowed\n * to have non-paged data as well.\n *\n * TODO: full sized shift could be optimized but that would need\n * specialized skb free'er to handle frags without up-to-date nr_frags.\n */\nint skb_shift(struct sk_buff *tgt, struct sk_buff *skb, int shiftlen)\n{\n\tint from, to, merge, todo;\n\tstruct skb_frag_struct *fragfrom, *fragto;\n\n\tBUG_ON(shiftlen > skb->len);\n\tBUG_ON(skb_headlen(skb));\t/* Would corrupt stream */\n\n\ttodo = shiftlen;\n\tfrom = 0;\n\tto = skb_shinfo(tgt)->nr_frags;\n\tfragfrom = &skb_shinfo(skb)->frags[from];\n\n\t/* Actual merge is delayed until the point when we know we can\n\t * commit all, so that we don't have to undo partial changes\n\t */\n\tif (!to ||\n\t    !skb_can_coalesce(tgt, to, skb_frag_page(fragfrom),\n\t\t\t      fragfrom->page_offset)) {\n\t\tmerge = -1;\n\t} else {\n\t\tmerge = to - 1;\n\n\t\ttodo -= skb_frag_size(fragfrom);\n\t\tif (todo < 0) {\n\t\t\tif (skb_prepare_for_shift(skb) ||\n\t\t\t    skb_prepare_for_shift(tgt))\n\t\t\t\treturn 0;\n\n\t\t\t/* All previous frag pointers might be stale! */\n\t\t\tfragfrom = &skb_shinfo(skb)->frags[from];\n\t\t\tfragto = &skb_shinfo(tgt)->frags[merge];\n\n\t\t\tskb_frag_size_add(fragto, shiftlen);\n\t\t\tskb_frag_size_sub(fragfrom, shiftlen);\n\t\t\tfragfrom->page_offset += shiftlen;\n\n\t\t\tgoto onlymerged;\n\t\t}\n\n\t\tfrom++;\n\t}\n\n\t/* Skip full, not-fitting skb to avoid expensive operations */\n\tif ((shiftlen == skb->len) &&\n\t    (skb_shinfo(skb)->nr_frags - from) > (MAX_SKB_FRAGS - to))\n\t\treturn 0;\n\n\tif (skb_prepare_for_shift(skb) || skb_prepare_for_shift(tgt))\n\t\treturn 0;\n\n\twhile ((todo > 0) && (from < skb_shinfo(skb)->nr_frags)) {\n\t\tif (to == MAX_SKB_FRAGS)\n\t\t\treturn 0;\n\n\t\tfragfrom = &skb_shinfo(skb)->frags[from];\n\t\tfragto = &skb_shinfo(tgt)->frags[to];\n\n\t\tif (todo >= skb_frag_size(fragfrom)) {\n\t\t\t*fragto = *fragfrom;\n\t\t\ttodo -= skb_frag_size(fragfrom);\n\t\t\tfrom++;\n\t\t\tto++;\n\n\t\t} else {\n\t\t\t__skb_frag_ref(fragfrom);\n\t\t\tfragto->page = fragfrom->page;\n\t\t\tfragto->page_offset = fragfrom->page_offset;\n\t\t\tskb_frag_size_set(fragto, todo);\n\n\t\t\tfragfrom->page_offset += todo;\n\t\t\tskb_frag_size_sub(fragfrom, todo);\n\t\t\ttodo = 0;\n\n\t\t\tto++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Ready to \"commit\" this state change to tgt */\n\tskb_shinfo(tgt)->nr_frags = to;\n\n\tif (merge >= 0) {\n\t\tfragfrom = &skb_shinfo(skb)->frags[0];\n\t\tfragto = &skb_shinfo(tgt)->frags[merge];\n\n\t\tskb_frag_size_add(fragto, skb_frag_size(fragfrom));\n\t\t__skb_frag_unref(fragfrom);\n\t}\n\n\t/* Reposition in the original skb */\n\tto = 0;\n\twhile (from < skb_shinfo(skb)->nr_frags)\n\t\tskb_shinfo(skb)->frags[to++] = skb_shinfo(skb)->frags[from++];\n\tskb_shinfo(skb)->nr_frags = to;\n\n\tBUG_ON(todo > 0 && !skb_shinfo(skb)->nr_frags);\n\nonlymerged:\n\t/* Most likely the tgt won't ever need its checksum anymore, skb on\n\t * the other hand might need it if it needs to be resent\n\t */\n\ttgt->ip_summed = CHECKSUM_PARTIAL;\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\t/* Yak, is it really working this way? Some helper please? */\n\tskb->len -= shiftlen;\n\tskb->data_len -= shiftlen;\n\tskb->truesize -= shiftlen;\n\ttgt->len += shiftlen;\n\ttgt->data_len += shiftlen;\n\ttgt->truesize += shiftlen;\n\n\treturn shiftlen;\n}\n\n/**\n * skb_prepare_seq_read - Prepare a sequential read of skb data\n * @skb: the buffer to read\n * @from: lower offset of data to be read\n * @to: upper offset of data to be read\n * @st: state variable\n *\n * Initializes the specified state variable. Must be called before\n * invoking skb_seq_read() for the first time.\n */\nvoid skb_prepare_seq_read(struct sk_buff *skb, unsigned int from,\n\t\t\t  unsigned int to, struct skb_seq_state *st)\n{\n\tst->lower_offset = from;\n\tst->upper_offset = to;\n\tst->root_skb = st->cur_skb = skb;\n\tst->frag_idx = st->stepped_offset = 0;\n\tst->frag_data = NULL;\n}\nEXPORT_SYMBOL(skb_prepare_seq_read);\n\n/**\n * skb_seq_read - Sequentially read skb data\n * @consumed: number of bytes consumed by the caller so far\n * @data: destination pointer for data to be returned\n * @st: state variable\n *\n * Reads a block of skb data at @consumed relative to the\n * lower offset specified to skb_prepare_seq_read(). Assigns\n * the head of the data block to @data and returns the length\n * of the block or 0 if the end of the skb data or the upper\n * offset has been reached.\n *\n * The caller is not required to consume all of the data\n * returned, i.e. @consumed is typically set to the number\n * of bytes already consumed and the next call to\n * skb_seq_read() will return the remaining part of the block.\n *\n * Note 1: The size of each block of data returned can be arbitrary,\n *       this limitation is the cost for zerocopy seqeuental\n *       reads of potentially non linear data.\n *\n * Note 2: Fragment lists within fragments are not implemented\n *       at the moment, state->root_skb could be replaced with\n *       a stack for this purpose.\n */\nunsigned int skb_seq_read(unsigned int consumed, const u8 **data,\n\t\t\t  struct skb_seq_state *st)\n{\n\tunsigned int block_limit, abs_offset = consumed + st->lower_offset;\n\tskb_frag_t *frag;\n\n\tif (unlikely(abs_offset >= st->upper_offset)) {\n\t\tif (st->frag_data) {\n\t\t\tkunmap_atomic(st->frag_data);\n\t\t\tst->frag_data = NULL;\n\t\t}\n\t\treturn 0;\n\t}\n\nnext_skb:\n\tblock_limit = skb_headlen(st->cur_skb) + st->stepped_offset;\n\n\tif (abs_offset < block_limit && !st->frag_data) {\n\t\t*data = st->cur_skb->data + (abs_offset - st->stepped_offset);\n\t\treturn block_limit - abs_offset;\n\t}\n\n\tif (st->frag_idx == 0 && !st->frag_data)\n\t\tst->stepped_offset += skb_headlen(st->cur_skb);\n\n\twhile (st->frag_idx < skb_shinfo(st->cur_skb)->nr_frags) {\n\t\tfrag = &skb_shinfo(st->cur_skb)->frags[st->frag_idx];\n\t\tblock_limit = skb_frag_size(frag) + st->stepped_offset;\n\n\t\tif (abs_offset < block_limit) {\n\t\t\tif (!st->frag_data)\n\t\t\t\tst->frag_data = kmap_atomic(skb_frag_page(frag));\n\n\t\t\t*data = (u8 *) st->frag_data + frag->page_offset +\n\t\t\t\t(abs_offset - st->stepped_offset);\n\n\t\t\treturn block_limit - abs_offset;\n\t\t}\n\n\t\tif (st->frag_data) {\n\t\t\tkunmap_atomic(st->frag_data);\n\t\t\tst->frag_data = NULL;\n\t\t}\n\n\t\tst->frag_idx++;\n\t\tst->stepped_offset += skb_frag_size(frag);\n\t}\n\n\tif (st->frag_data) {\n\t\tkunmap_atomic(st->frag_data);\n\t\tst->frag_data = NULL;\n\t}\n\n\tif (st->root_skb == st->cur_skb && skb_has_frag_list(st->root_skb)) {\n\t\tst->cur_skb = skb_shinfo(st->root_skb)->frag_list;\n\t\tst->frag_idx = 0;\n\t\tgoto next_skb;\n\t} else if (st->cur_skb->next) {\n\t\tst->cur_skb = st->cur_skb->next;\n\t\tst->frag_idx = 0;\n\t\tgoto next_skb;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_seq_read);\n\n/**\n * skb_abort_seq_read - Abort a sequential read of skb data\n * @st: state variable\n *\n * Must be called if skb_seq_read() was not called until it\n * returned 0.\n */\nvoid skb_abort_seq_read(struct skb_seq_state *st)\n{\n\tif (st->frag_data)\n\t\tkunmap_atomic(st->frag_data);\n}\nEXPORT_SYMBOL(skb_abort_seq_read);\n\n#define TS_SKB_CB(state)\t((struct skb_seq_state *) &((state)->cb))\n\nstatic unsigned int skb_ts_get_next_block(unsigned int offset, const u8 **text,\n\t\t\t\t\t  struct ts_config *conf,\n\t\t\t\t\t  struct ts_state *state)\n{\n\treturn skb_seq_read(offset, text, TS_SKB_CB(state));\n}\n\nstatic void skb_ts_finish(struct ts_config *conf, struct ts_state *state)\n{\n\tskb_abort_seq_read(TS_SKB_CB(state));\n}\n\n/**\n * skb_find_text - Find a text pattern in skb data\n * @skb: the buffer to look in\n * @from: search offset\n * @to: search limit\n * @config: textsearch configuration\n * @state: uninitialized textsearch state variable\n *\n * Finds a pattern in the skb data according to the specified\n * textsearch configuration. Use textsearch_next() to retrieve\n * subsequent occurrences of the pattern. Returns the offset\n * to the first occurrence or UINT_MAX if no match was found.\n */\nunsigned int skb_find_text(struct sk_buff *skb, unsigned int from,\n\t\t\t   unsigned int to, struct ts_config *config,\n\t\t\t   struct ts_state *state)\n{\n\tunsigned int ret;\n\n\tconfig->get_next_block = skb_ts_get_next_block;\n\tconfig->finish = skb_ts_finish;\n\n\tskb_prepare_seq_read(skb, from, to, TS_SKB_CB(state));\n\n\tret = textsearch_find(config, state);\n\treturn (ret <= to - from ? ret : UINT_MAX);\n}\nEXPORT_SYMBOL(skb_find_text);\n\n/**\n * skb_append_datato_frags - append the user data to a skb\n * @sk: sock  structure\n * @skb: skb structure to be appened with user data.\n * @getfrag: call back function to be used for getting the user data\n * @from: pointer to user message iov\n * @length: length of the iov message\n *\n * Description: This procedure append the user data in the fragment part\n * of the skb if any page alloc fails user this procedure returns  -ENOMEM\n */\nint skb_append_datato_frags(struct sock *sk, struct sk_buff *skb,\n\t\t\tint (*getfrag)(void *from, char *to, int offset,\n\t\t\t\t\tint len, int odd, struct sk_buff *skb),\n\t\t\tvoid *from, int length)\n{\n\tint frg_cnt = skb_shinfo(skb)->nr_frags;\n\tint copy;\n\tint offset = 0;\n\tint ret;\n\tstruct page_frag *pfrag = &current->task_frag;\n\n\tdo {\n\t\t/* Return error if we don't have space for new frag */\n\t\tif (frg_cnt >= MAX_SKB_FRAGS)\n\t\t\treturn -EMSGSIZE;\n\n\t\tif (!sk_page_frag_refill(sk, pfrag))\n\t\t\treturn -ENOMEM;\n\n\t\t/* copy the user data to page */\n\t\tcopy = min_t(int, length, pfrag->size - pfrag->offset);\n\n\t\tret = getfrag(from, page_address(pfrag->page) + pfrag->offset,\n\t\t\t      offset, copy, 0, skb);\n\t\tif (ret < 0)\n\t\t\treturn -EFAULT;\n\n\t\t/* copy was successful so update the size parameters */\n\t\tskb_fill_page_desc(skb, frg_cnt, pfrag->page, pfrag->offset,\n\t\t\t\t   copy);\n\t\tfrg_cnt++;\n\t\tpfrag->offset += copy;\n\t\tget_page(pfrag->page);\n\n\t\tskb->truesize += copy;\n\t\tatomic_add(copy, &sk->sk_wmem_alloc);\n\t\tskb->len += copy;\n\t\tskb->data_len += copy;\n\t\toffset += copy;\n\t\tlength -= copy;\n\n\t} while (length > 0);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_append_datato_frags);\n\n/**\n *\tskb_pull_rcsum - pull skb and update receive checksum\n *\t@skb: buffer to update\n *\t@len: length of data pulled\n *\n *\tThis function performs an skb_pull on the packet and updates\n *\tthe CHECKSUM_COMPLETE checksum.  It should be used on\n *\treceive path processing instead of skb_pull unless you know\n *\tthat the checksum difference is zero (e.g., a valid IP header)\n *\tor you are setting ip_summed to CHECKSUM_NONE.\n */\nunsigned char *skb_pull_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tBUG_ON(len > skb->len);\n\tskb->len -= len;\n\tBUG_ON(skb->len < skb->data_len);\n\tskb_postpull_rcsum(skb, skb->data, len);\n\treturn skb->data += len;\n}\nEXPORT_SYMBOL_GPL(skb_pull_rcsum);\n\n/**\n *\tskb_segment - Perform protocol segmentation on skb.\n *\t@head_skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n *\n *\tThis function performs segmentation on the given skb.  It returns\n *\ta pointer to the first in a list of new skbs for the segments.\n *\tIn case of error it returns ERR_PTR(err).\n */\nstruct sk_buff *skb_segment(struct sk_buff *head_skb,\n\t\t\t    netdev_features_t features)\n{\n\tstruct sk_buff *segs = NULL;\n\tstruct sk_buff *tail = NULL;\n\tstruct sk_buff *list_skb = skb_shinfo(head_skb)->frag_list;\n\tskb_frag_t *frag = skb_shinfo(head_skb)->frags;\n\tunsigned int mss = skb_shinfo(head_skb)->gso_size;\n\tunsigned int doffset = head_skb->data - skb_mac_header(head_skb);\n\tstruct sk_buff *frag_skb = head_skb;\n\tunsigned int offset = doffset;\n\tunsigned int tnl_hlen = skb_tnl_header_len(head_skb);\n\tunsigned int headroom;\n\tunsigned int len;\n\t__be16 proto;\n\tbool csum;\n\tint sg = !!(features & NETIF_F_SG);\n\tint nfrags = skb_shinfo(head_skb)->nr_frags;\n\tint err = -ENOMEM;\n\tint i = 0;\n\tint pos;\n\n\tproto = skb_network_protocol(head_skb);\n\tif (unlikely(!proto))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tcsum = !!can_checksum_protocol(features, proto);\n\t__skb_push(head_skb, doffset);\n\theadroom = skb_headroom(head_skb);\n\tpos = skb_headlen(head_skb);\n\n\tdo {\n\t\tstruct sk_buff *nskb;\n\t\tskb_frag_t *nskb_frag;\n\t\tint hsize;\n\t\tint size;\n\n\t\tlen = head_skb->len - offset;\n\t\tif (len > mss)\n\t\t\tlen = mss;\n\n\t\thsize = skb_headlen(head_skb) - offset;\n\t\tif (hsize < 0)\n\t\t\thsize = 0;\n\t\tif (hsize > len || !sg)\n\t\t\thsize = len;\n\n\t\tif (!hsize && i >= nfrags && skb_headlen(list_skb) &&\n\t\t    (skb_headlen(list_skb) == len || sg)) {\n\t\t\tBUG_ON(skb_headlen(list_skb) > len);\n\n\t\t\ti = 0;\n\t\t\tnfrags = skb_shinfo(list_skb)->nr_frags;\n\t\t\tfrag = skb_shinfo(list_skb)->frags;\n\t\t\tfrag_skb = list_skb;\n\t\t\tpos += skb_headlen(list_skb);\n\n\t\t\twhile (pos < offset + len) {\n\t\t\t\tBUG_ON(i >= nfrags);\n\n\t\t\t\tsize = skb_frag_size(frag);\n\t\t\t\tif (pos + size > offset + len)\n\t\t\t\t\tbreak;\n\n\t\t\t\ti++;\n\t\t\t\tpos += size;\n\t\t\t\tfrag++;\n\t\t\t}\n\n\t\t\tnskb = skb_clone(list_skb, GFP_ATOMIC);\n\t\t\tlist_skb = list_skb->next;\n\n\t\t\tif (unlikely(!nskb))\n\t\t\t\tgoto err;\n\n\t\t\tif (unlikely(pskb_trim(nskb, len))) {\n\t\t\t\tkfree_skb(nskb);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\thsize = skb_end_offset(nskb);\n\t\t\tif (skb_cow_head(nskb, doffset + headroom)) {\n\t\t\t\tkfree_skb(nskb);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tnskb->truesize += skb_end_offset(nskb) - hsize;\n\t\t\tskb_release_head_state(nskb);\n\t\t\t__skb_push(nskb, doffset);\n\t\t} else {\n\t\t\tnskb = __alloc_skb(hsize + doffset + headroom,\n\t\t\t\t\t   GFP_ATOMIC, skb_alloc_rx_flag(head_skb),\n\t\t\t\t\t   NUMA_NO_NODE);\n\n\t\t\tif (unlikely(!nskb))\n\t\t\t\tgoto err;\n\n\t\t\tskb_reserve(nskb, headroom);\n\t\t\t__skb_put(nskb, doffset);\n\t\t}\n\n\t\tif (segs)\n\t\t\ttail->next = nskb;\n\t\telse\n\t\t\tsegs = nskb;\n\t\ttail = nskb;\n\n\t\t__copy_skb_header(nskb, head_skb);\n\t\tnskb->mac_len = head_skb->mac_len;\n\n\t\tskb_headers_offset_update(nskb, skb_headroom(nskb) - headroom);\n\n\t\tskb_copy_from_linear_data_offset(head_skb, -tnl_hlen,\n\t\t\t\t\t\t nskb->data - tnl_hlen,\n\t\t\t\t\t\t doffset + tnl_hlen);\n\n\t\tif (nskb->len == len + doffset)\n\t\t\tgoto perform_csum_check;\n\n\t\tif (!sg) {\n\t\t\tnskb->ip_summed = CHECKSUM_NONE;\n\t\t\tnskb->csum = skb_copy_and_csum_bits(head_skb, offset,\n\t\t\t\t\t\t\t    skb_put(nskb, len),\n\t\t\t\t\t\t\t    len, 0);\n\t\t\tcontinue;\n\t\t}\n\n\t\tnskb_frag = skb_shinfo(nskb)->frags;\n\n\t\tskb_copy_from_linear_data_offset(head_skb, offset,\n\t\t\t\t\t\t skb_put(nskb, hsize), hsize);\n\n\t\tskb_shinfo(nskb)->tx_flags = skb_shinfo(head_skb)->tx_flags &\n\t\t\tSKBTX_SHARED_FRAG;\n\n\t\twhile (pos < offset + len) {\n\t\t\tif (i >= nfrags) {\n\t\t\t\tBUG_ON(skb_headlen(list_skb));\n\n\t\t\t\ti = 0;\n\t\t\t\tnfrags = skb_shinfo(list_skb)->nr_frags;\n\t\t\t\tfrag = skb_shinfo(list_skb)->frags;\n\t\t\t\tfrag_skb = list_skb;\n\n\t\t\t\tBUG_ON(!nfrags);\n\n\t\t\t\tlist_skb = list_skb->next;\n\t\t\t}\n\n\t\t\tif (unlikely(skb_shinfo(nskb)->nr_frags >=\n\t\t\t\t     MAX_SKB_FRAGS)) {\n\t\t\t\tnet_warn_ratelimited(\n\t\t\t\t\t\"skb_segment: too many frags: %u %u\\n\",\n\t\t\t\t\tpos, mss);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tif (unlikely(skb_orphan_frags(frag_skb, GFP_ATOMIC)))\n\t\t\t\tgoto err;\n\n\t\t\t*nskb_frag = *frag;\n\t\t\t__skb_frag_ref(nskb_frag);\n\t\t\tsize = skb_frag_size(nskb_frag);\n\n\t\t\tif (pos < offset) {\n\t\t\t\tnskb_frag->page_offset += offset - pos;\n\t\t\t\tskb_frag_size_sub(nskb_frag, offset - pos);\n\t\t\t}\n\n\t\t\tskb_shinfo(nskb)->nr_frags++;\n\n\t\t\tif (pos + size <= offset + len) {\n\t\t\t\ti++;\n\t\t\t\tfrag++;\n\t\t\t\tpos += size;\n\t\t\t} else {\n\t\t\t\tskb_frag_size_sub(nskb_frag, pos + size - (offset + len));\n\t\t\t\tgoto skip_fraglist;\n\t\t\t}\n\n\t\t\tnskb_frag++;\n\t\t}\n\nskip_fraglist:\n\t\tnskb->data_len = len - hsize;\n\t\tnskb->len += nskb->data_len;\n\t\tnskb->truesize += nskb->data_len;\n\nperform_csum_check:\n\t\tif (!csum) {\n\t\t\tnskb->csum = skb_checksum(nskb, doffset,\n\t\t\t\t\t\t  nskb->len - doffset, 0);\n\t\t\tnskb->ip_summed = CHECKSUM_NONE;\n\t\t}\n\t} while ((offset += len) < head_skb->len);\n\n\treturn segs;\n\nerr:\n\tkfree_skb_list(segs);\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(skb_segment);\n\nint skb_gro_receive(struct sk_buff **head, struct sk_buff *skb)\n{\n\tstruct skb_shared_info *pinfo, *skbinfo = skb_shinfo(skb);\n\tunsigned int offset = skb_gro_offset(skb);\n\tunsigned int headlen = skb_headlen(skb);\n\tstruct sk_buff *nskb, *lp, *p = *head;\n\tunsigned int len = skb_gro_len(skb);\n\tunsigned int delta_truesize;\n\tunsigned int headroom;\n\n\tif (unlikely(p->len + len >= 65536))\n\t\treturn -E2BIG;\n\n\tlp = NAPI_GRO_CB(p)->last ?: p;\n\tpinfo = skb_shinfo(lp);\n\n\tif (headlen <= offset) {\n\t\tskb_frag_t *frag;\n\t\tskb_frag_t *frag2;\n\t\tint i = skbinfo->nr_frags;\n\t\tint nr_frags = pinfo->nr_frags + i;\n\n\t\tif (nr_frags > MAX_SKB_FRAGS)\n\t\t\tgoto merge;\n\n\t\toffset -= headlen;\n\t\tpinfo->nr_frags = nr_frags;\n\t\tskbinfo->nr_frags = 0;\n\n\t\tfrag = pinfo->frags + nr_frags;\n\t\tfrag2 = skbinfo->frags + i;\n\t\tdo {\n\t\t\t*--frag = *--frag2;\n\t\t} while (--i);\n\n\t\tfrag->page_offset += offset;\n\t\tskb_frag_size_sub(frag, offset);\n\n\t\t/* all fragments truesize : remove (head size + sk_buff) */\n\t\tdelta_truesize = skb->truesize -\n\t\t\t\t SKB_TRUESIZE(skb_end_offset(skb));\n\n\t\tskb->truesize -= skb->data_len;\n\t\tskb->len -= skb->data_len;\n\t\tskb->data_len = 0;\n\n\t\tNAPI_GRO_CB(skb)->free = NAPI_GRO_FREE;\n\t\tgoto done;\n\t} else if (skb->head_frag) {\n\t\tint nr_frags = pinfo->nr_frags;\n\t\tskb_frag_t *frag = pinfo->frags + nr_frags;\n\t\tstruct page *page = virt_to_head_page(skb->head);\n\t\tunsigned int first_size = headlen - offset;\n\t\tunsigned int first_offset;\n\n\t\tif (nr_frags + 1 + skbinfo->nr_frags > MAX_SKB_FRAGS)\n\t\t\tgoto merge;\n\n\t\tfirst_offset = skb->data -\n\t\t\t       (unsigned char *)page_address(page) +\n\t\t\t       offset;\n\n\t\tpinfo->nr_frags = nr_frags + 1 + skbinfo->nr_frags;\n\n\t\tfrag->page.p\t  = page;\n\t\tfrag->page_offset = first_offset;\n\t\tskb_frag_size_set(frag, first_size);\n\n\t\tmemcpy(frag + 1, skbinfo->frags, sizeof(*frag) * skbinfo->nr_frags);\n\t\t/* We dont need to clear skbinfo->nr_frags here */\n\n\t\tdelta_truesize = skb->truesize - SKB_DATA_ALIGN(sizeof(struct sk_buff));\n\t\tNAPI_GRO_CB(skb)->free = NAPI_GRO_FREE_STOLEN_HEAD;\n\t\tgoto done;\n\t}\n\tif (pinfo->frag_list)\n\t\tgoto merge;\n\tif (skb_gro_len(p) != pinfo->gso_size)\n\t\treturn -E2BIG;\n\n\theadroom = skb_headroom(p);\n\tnskb = alloc_skb(headroom + skb_gro_offset(p), GFP_ATOMIC);\n\tif (unlikely(!nskb))\n\t\treturn -ENOMEM;\n\n\t__copy_skb_header(nskb, p);\n\tnskb->mac_len = p->mac_len;\n\n\tskb_reserve(nskb, headroom);\n\t__skb_put(nskb, skb_gro_offset(p));\n\n\tskb_set_mac_header(nskb, skb_mac_header(p) - p->data);\n\tskb_set_network_header(nskb, skb_network_offset(p));\n\tskb_set_transport_header(nskb, skb_transport_offset(p));\n\n\t__skb_pull(p, skb_gro_offset(p));\n\tmemcpy(skb_mac_header(nskb), skb_mac_header(p),\n\t       p->data - skb_mac_header(p));\n\n\tskb_shinfo(nskb)->frag_list = p;\n\tskb_shinfo(nskb)->gso_size = pinfo->gso_size;\n\tpinfo->gso_size = 0;\n\tskb_header_release(p);\n\tNAPI_GRO_CB(nskb)->last = p;\n\n\tnskb->data_len += p->len;\n\tnskb->truesize += p->truesize;\n\tnskb->len += p->len;\n\n\t*head = nskb;\n\tnskb->next = p->next;\n\tp->next = NULL;\n\n\tp = nskb;\n\nmerge:\n\tdelta_truesize = skb->truesize;\n\tif (offset > headlen) {\n\t\tunsigned int eat = offset - headlen;\n\n\t\tskbinfo->frags[0].page_offset += eat;\n\t\tskb_frag_size_sub(&skbinfo->frags[0], eat);\n\t\tskb->data_len -= eat;\n\t\tskb->len -= eat;\n\t\toffset = headlen;\n\t}\n\n\t__skb_pull(skb, offset);\n\n\tif (!NAPI_GRO_CB(p)->last)\n\t\tskb_shinfo(p)->frag_list = skb;\n\telse\n\t\tNAPI_GRO_CB(p)->last->next = skb;\n\tNAPI_GRO_CB(p)->last = skb;\n\tskb_header_release(skb);\n\tlp = p;\n\ndone:\n\tNAPI_GRO_CB(p)->count++;\n\tp->data_len += len;\n\tp->truesize += delta_truesize;\n\tp->len += len;\n\tif (lp != p) {\n\t\tlp->data_len += len;\n\t\tlp->truesize += delta_truesize;\n\t\tlp->len += len;\n\t}\n\tNAPI_GRO_CB(skb)->same_flow = 1;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_gro_receive);\n\nvoid __init skb_init(void)\n{\n\tskbuff_head_cache = kmem_cache_create(\"skbuff_head_cache\",\n\t\t\t\t\t      sizeof(struct sk_buff),\n\t\t\t\t\t      0,\n\t\t\t\t\t      SLAB_HWCACHE_ALIGN|SLAB_PANIC,\n\t\t\t\t\t      NULL);\n\tskbuff_fclone_cache = kmem_cache_create(\"skbuff_fclone_cache\",\n\t\t\t\t\t\t(2*sizeof(struct sk_buff)) +\n\t\t\t\t\t\tsizeof(atomic_t),\n\t\t\t\t\t\t0,\n\t\t\t\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC,\n\t\t\t\t\t\tNULL);\n}\n\n/**\n *\tskb_to_sgvec - Fill a scatter-gather list from a socket buffer\n *\t@skb: Socket buffer containing the buffers to be mapped\n *\t@sg: The scatter-gather list to map into\n *\t@offset: The offset into the buffer's contents to start mapping\n *\t@len: Length of buffer space to be mapped\n *\n *\tFill the specified scatter-gather list with mappings/pointers into a\n *\tregion of the buffer space attached to a socket buffer.\n */\nstatic int\n__skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg, int offset, int len)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\tint elt = 0;\n\n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tsg_set_buf(sg, skb->data + offset, copy);\n\t\telt++;\n\t\tif ((len -= copy) == 0)\n\t\t\treturn elt;\n\t\toffset += copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tsg_set_page(&sg[elt], skb_frag_page(frag), copy,\n\t\t\t\t\tfrag->page_offset+offset-start);\n\t\t\telt++;\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn elt;\n\t\t\toffset += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\telt += __skb_to_sgvec(frag_iter, sg+elt, offset - start,\n\t\t\t\t\t      copy);\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn elt;\n\t\t\toffset += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tBUG_ON(len);\n\treturn elt;\n}\n\nint skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg, int offset, int len)\n{\n\tint nsg = __skb_to_sgvec(skb, sg, offset, len);\n\n\tsg_mark_end(&sg[nsg - 1]);\n\n\treturn nsg;\n}\nEXPORT_SYMBOL_GPL(skb_to_sgvec);\n\n/**\n *\tskb_cow_data - Check that a socket buffer's data buffers are writable\n *\t@skb: The socket buffer to check.\n *\t@tailbits: Amount of trailing space to be added\n *\t@trailer: Returned pointer to the skb where the @tailbits space begins\n *\n *\tMake sure that the data buffers attached to a socket buffer are\n *\twritable. If they are not, private copies are made of the data buffers\n *\tand the socket buffer is set to use these instead.\n *\n *\tIf @tailbits is given, make sure that there is space to write @tailbits\n *\tbytes of data beyond current end of socket buffer.  @trailer will be\n *\tset to point to the skb in which this space begins.\n *\n *\tThe number of scatterlist elements required to completely map the\n *\tCOW'd and extended socket buffer will be returned.\n */\nint skb_cow_data(struct sk_buff *skb, int tailbits, struct sk_buff **trailer)\n{\n\tint copyflag;\n\tint elt;\n\tstruct sk_buff *skb1, **skb_p;\n\n\t/* If skb is cloned or its head is paged, reallocate\n\t * head pulling out all the pages (pages are considered not writable\n\t * at the moment even if they are anonymous).\n\t */\n\tif ((skb_cloned(skb) || skb_shinfo(skb)->nr_frags) &&\n\t    __pskb_pull_tail(skb, skb_pagelen(skb)-skb_headlen(skb)) == NULL)\n\t\treturn -ENOMEM;\n\n\t/* Easy case. Most of packets will go this way. */\n\tif (!skb_has_frag_list(skb)) {\n\t\t/* A little of trouble, not enough of space for trailer.\n\t\t * This should not happen, when stack is tuned to generate\n\t\t * good frames. OK, on miss we reallocate and reserve even more\n\t\t * space, 128 bytes is fair. */\n\n\t\tif (skb_tailroom(skb) < tailbits &&\n\t\t    pskb_expand_head(skb, 0, tailbits-skb_tailroom(skb)+128, GFP_ATOMIC))\n\t\t\treturn -ENOMEM;\n\n\t\t/* Voila! */\n\t\t*trailer = skb;\n\t\treturn 1;\n\t}\n\n\t/* Misery. We are in troubles, going to mincer fragments... */\n\n\telt = 1;\n\tskb_p = &skb_shinfo(skb)->frag_list;\n\tcopyflag = 0;\n\n\twhile ((skb1 = *skb_p) != NULL) {\n\t\tint ntail = 0;\n\n\t\t/* The fragment is partially pulled by someone,\n\t\t * this can happen on input. Copy it and everything\n\t\t * after it. */\n\n\t\tif (skb_shared(skb1))\n\t\t\tcopyflag = 1;\n\n\t\t/* If the skb is the last, worry about trailer. */\n\n\t\tif (skb1->next == NULL && tailbits) {\n\t\t\tif (skb_shinfo(skb1)->nr_frags ||\n\t\t\t    skb_has_frag_list(skb1) ||\n\t\t\t    skb_tailroom(skb1) < tailbits)\n\t\t\t\tntail = tailbits + 128;\n\t\t}\n\n\t\tif (copyflag ||\n\t\t    skb_cloned(skb1) ||\n\t\t    ntail ||\n\t\t    skb_shinfo(skb1)->nr_frags ||\n\t\t    skb_has_frag_list(skb1)) {\n\t\t\tstruct sk_buff *skb2;\n\n\t\t\t/* Fuck, we are miserable poor guys... */\n\t\t\tif (ntail == 0)\n\t\t\t\tskb2 = skb_copy(skb1, GFP_ATOMIC);\n\t\t\telse\n\t\t\t\tskb2 = skb_copy_expand(skb1,\n\t\t\t\t\t\t       skb_headroom(skb1),\n\t\t\t\t\t\t       ntail,\n\t\t\t\t\t\t       GFP_ATOMIC);\n\t\t\tif (unlikely(skb2 == NULL))\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tif (skb1->sk)\n\t\t\t\tskb_set_owner_w(skb2, skb1->sk);\n\n\t\t\t/* Looking around. Are we still alive?\n\t\t\t * OK, link new skb, drop old one */\n\n\t\t\tskb2->next = skb1->next;\n\t\t\t*skb_p = skb2;\n\t\t\tkfree_skb(skb1);\n\t\t\tskb1 = skb2;\n\t\t}\n\t\telt++;\n\t\t*trailer = skb1;\n\t\tskb_p = &skb1->next;\n\t}\n\n\treturn elt;\n}\nEXPORT_SYMBOL_GPL(skb_cow_data);\n\nstatic void sock_rmem_free(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\n\tatomic_sub(skb->truesize, &sk->sk_rmem_alloc);\n}\n\n/*\n * Note: We dont mem charge error packets (no sk_forward_alloc changes)\n */\nint sock_queue_err_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tint len = skb->len;\n\n\tif (atomic_read(&sk->sk_rmem_alloc) + skb->truesize >=\n\t    (unsigned int)sk->sk_rcvbuf)\n\t\treturn -ENOMEM;\n\n\tskb_orphan(skb);\n\tskb->sk = sk;\n\tskb->destructor = sock_rmem_free;\n\tatomic_add(skb->truesize, &sk->sk_rmem_alloc);\n\n\t/* before exiting rcu section, make sure dst is refcounted */\n\tskb_dst_force(skb);\n\n\tskb_queue_tail(&sk->sk_error_queue, skb);\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_data_ready(sk, len);\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_queue_err_skb);\n\nvoid skb_tstamp_tx(struct sk_buff *orig_skb,\n\t\tstruct skb_shared_hwtstamps *hwtstamps)\n{\n\tstruct sock *sk = orig_skb->sk;\n\tstruct sock_exterr_skb *serr;\n\tstruct sk_buff *skb;\n\tint err;\n\n\tif (!sk)\n\t\treturn;\n\n\tif (hwtstamps) {\n\t\t*skb_hwtstamps(orig_skb) =\n\t\t\t*hwtstamps;\n\t} else {\n\t\t/*\n\t\t * no hardware time stamps available,\n\t\t * so keep the shared tx_flags and only\n\t\t * store software time stamp\n\t\t */\n\t\torig_skb->tstamp = ktime_get_real();\n\t}\n\n\tskb = skb_clone(orig_skb, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn;\n\n\tserr = SKB_EXT_ERR(skb);\n\tmemset(serr, 0, sizeof(*serr));\n\tserr->ee.ee_errno = ENOMSG;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_TIMESTAMPING;\n\n\terr = sock_queue_err_skb(sk, skb);\n\n\tif (err)\n\t\tkfree_skb(skb);\n}\nEXPORT_SYMBOL_GPL(skb_tstamp_tx);\n\nvoid skb_complete_wifi_ack(struct sk_buff *skb, bool acked)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct sock_exterr_skb *serr;\n\tint err;\n\n\tskb->wifi_acked_valid = 1;\n\tskb->wifi_acked = acked;\n\n\tserr = SKB_EXT_ERR(skb);\n\tmemset(serr, 0, sizeof(*serr));\n\tserr->ee.ee_errno = ENOMSG;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_TXSTATUS;\n\n\terr = sock_queue_err_skb(sk, skb);\n\tif (err)\n\t\tkfree_skb(skb);\n}\nEXPORT_SYMBOL_GPL(skb_complete_wifi_ack);\n\n\n/**\n * skb_partial_csum_set - set up and verify partial csum values for packet\n * @skb: the skb to set\n * @start: the number of bytes after skb->data to start checksumming.\n * @off: the offset from start to place the checksum.\n *\n * For untrusted partially-checksummed packets, we need to make sure the values\n * for skb->csum_start and skb->csum_offset are valid so we don't oops.\n *\n * This function checks and sets those values and skb->ip_summed: if this\n * returns false you should drop the packet.\n */\nbool skb_partial_csum_set(struct sk_buff *skb, u16 start, u16 off)\n{\n\tif (unlikely(start > skb_headlen(skb)) ||\n\t    unlikely((int)start + off > skb_headlen(skb) - 2)) {\n\t\tnet_warn_ratelimited(\"bad partial csum: csum=%u/%u len=%u\\n\",\n\t\t\t\t     start, off, skb_headlen(skb));\n\t\treturn false;\n\t}\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\tskb->csum_start = skb_headroom(skb) + start;\n\tskb->csum_offset = off;\n\tskb_set_transport_header(skb, start);\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(skb_partial_csum_set);\n\nstatic int skb_maybe_pull_tail(struct sk_buff *skb, unsigned int len,\n\t\t\t       unsigned int max)\n{\n\tif (skb_headlen(skb) >= len)\n\t\treturn 0;\n\n\t/* If we need to pullup then pullup to the max, so we\n\t * won't need to do it again.\n\t */\n\tif (max > skb->len)\n\t\tmax = skb->len;\n\n\tif (__pskb_pull_tail(skb, max - skb_headlen(skb)) == NULL)\n\t\treturn -ENOMEM;\n\n\tif (skb_headlen(skb) < len)\n\t\treturn -EPROTO;\n\n\treturn 0;\n}\n\n/* This value should be large enough to cover a tagged ethernet header plus\n * maximally sized IP and TCP or UDP headers.\n */\n#define MAX_IP_HDR_LEN 128\n\nstatic int skb_checksum_setup_ip(struct sk_buff *skb, bool recalculate)\n{\n\tunsigned int off;\n\tbool fragment;\n\tint err;\n\n\tfragment = false;\n\n\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t  sizeof(struct iphdr),\n\t\t\t\t  MAX_IP_HDR_LEN);\n\tif (err < 0)\n\t\tgoto out;\n\n\tif (ip_hdr(skb)->frag_off & htons(IP_OFFSET | IP_MF))\n\t\tfragment = true;\n\n\toff = ip_hdrlen(skb);\n\n\terr = -EPROTO;\n\n\tif (fragment)\n\t\tgoto out;\n\n\tswitch (ip_hdr(skb)->protocol) {\n\tcase IPPROTO_TCP:\n\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t  off + sizeof(struct tcphdr),\n\t\t\t\t\t  MAX_IP_HDR_LEN);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\n\t\tif (!skb_partial_csum_set(skb, off,\n\t\t\t\t\t  offsetof(struct tcphdr, check))) {\n\t\t\terr = -EPROTO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (recalculate)\n\t\t\ttcp_hdr(skb)->check =\n\t\t\t\t~csum_tcpudp_magic(ip_hdr(skb)->saddr,\n\t\t\t\t\t\t   ip_hdr(skb)->daddr,\n\t\t\t\t\t\t   skb->len - off,\n\t\t\t\t\t\t   IPPROTO_TCP, 0);\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t  off + sizeof(struct udphdr),\n\t\t\t\t\t  MAX_IP_HDR_LEN);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\n\t\tif (!skb_partial_csum_set(skb, off,\n\t\t\t\t\t  offsetof(struct udphdr, check))) {\n\t\t\terr = -EPROTO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (recalculate)\n\t\t\tudp_hdr(skb)->check =\n\t\t\t\t~csum_tcpudp_magic(ip_hdr(skb)->saddr,\n\t\t\t\t\t\t   ip_hdr(skb)->daddr,\n\t\t\t\t\t\t   skb->len - off,\n\t\t\t\t\t\t   IPPROTO_UDP, 0);\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\terr = 0;\n\nout:\n\treturn err;\n}\n\n/* This value should be large enough to cover a tagged ethernet header plus\n * an IPv6 header, all options, and a maximal TCP or UDP header.\n */\n#define MAX_IPV6_HDR_LEN 256\n\n#define OPT_HDR(type, skb, off) \\\n\t(type *)(skb_network_header(skb) + (off))\n\nstatic int skb_checksum_setup_ipv6(struct sk_buff *skb, bool recalculate)\n{\n\tint err;\n\tu8 nexthdr;\n\tunsigned int off;\n\tunsigned int len;\n\tbool fragment;\n\tbool done;\n\n\tfragment = false;\n\tdone = false;\n\n\toff = sizeof(struct ipv6hdr);\n\n\terr = skb_maybe_pull_tail(skb, off, MAX_IPV6_HDR_LEN);\n\tif (err < 0)\n\t\tgoto out;\n\n\tnexthdr = ipv6_hdr(skb)->nexthdr;\n\n\tlen = sizeof(struct ipv6hdr) + ntohs(ipv6_hdr(skb)->payload_len);\n\twhile (off <= len && !done) {\n\t\tswitch (nexthdr) {\n\t\tcase IPPROTO_DSTOPTS:\n\t\tcase IPPROTO_HOPOPTS:\n\t\tcase IPPROTO_ROUTING: {\n\t\t\tstruct ipv6_opt_hdr *hp;\n\n\t\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t\t  off +\n\t\t\t\t\t\t  sizeof(struct ipv6_opt_hdr),\n\t\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\thp = OPT_HDR(struct ipv6_opt_hdr, skb, off);\n\t\t\tnexthdr = hp->nexthdr;\n\t\t\toff += ipv6_optlen(hp);\n\t\t\tbreak;\n\t\t}\n\t\tcase IPPROTO_AH: {\n\t\t\tstruct ip_auth_hdr *hp;\n\n\t\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t\t  off +\n\t\t\t\t\t\t  sizeof(struct ip_auth_hdr),\n\t\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\thp = OPT_HDR(struct ip_auth_hdr, skb, off);\n\t\t\tnexthdr = hp->nexthdr;\n\t\t\toff += ipv6_authlen(hp);\n\t\t\tbreak;\n\t\t}\n\t\tcase IPPROTO_FRAGMENT: {\n\t\t\tstruct frag_hdr *hp;\n\n\t\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t\t  off +\n\t\t\t\t\t\t  sizeof(struct frag_hdr),\n\t\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\thp = OPT_HDR(struct frag_hdr, skb, off);\n\n\t\t\tif (hp->frag_off & htons(IP6_OFFSET | IP6_MF))\n\t\t\t\tfragment = true;\n\n\t\t\tnexthdr = hp->nexthdr;\n\t\t\toff += sizeof(struct frag_hdr);\n\t\t\tbreak;\n\t\t}\n\t\tdefault:\n\t\t\tdone = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\terr = -EPROTO;\n\n\tif (!done || fragment)\n\t\tgoto out;\n\n\tswitch (nexthdr) {\n\tcase IPPROTO_TCP:\n\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t  off + sizeof(struct tcphdr),\n\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\n\t\tif (!skb_partial_csum_set(skb, off,\n\t\t\t\t\t  offsetof(struct tcphdr, check))) {\n\t\t\terr = -EPROTO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (recalculate)\n\t\t\ttcp_hdr(skb)->check =\n\t\t\t\t~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t\t &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t\t skb->len - off,\n\t\t\t\t\t\t IPPROTO_TCP, 0);\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t  off + sizeof(struct udphdr),\n\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\n\t\tif (!skb_partial_csum_set(skb, off,\n\t\t\t\t\t  offsetof(struct udphdr, check))) {\n\t\t\terr = -EPROTO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (recalculate)\n\t\t\tudp_hdr(skb)->check =\n\t\t\t\t~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t\t &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t\t skb->len - off,\n\t\t\t\t\t\t IPPROTO_UDP, 0);\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\terr = 0;\n\nout:\n\treturn err;\n}\n\n/**\n * skb_checksum_setup - set up partial checksum offset\n * @skb: the skb to set up\n * @recalculate: if true the pseudo-header checksum will be recalculated\n */\nint skb_checksum_setup(struct sk_buff *skb, bool recalculate)\n{\n\tint err;\n\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_IP):\n\t\terr = skb_checksum_setup_ip(skb, recalculate);\n\t\tbreak;\n\n\tcase htons(ETH_P_IPV6):\n\t\terr = skb_checksum_setup_ipv6(skb, recalculate);\n\t\tbreak;\n\n\tdefault:\n\t\terr = -EPROTO;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(skb_checksum_setup);\n\nvoid __skb_warn_lro_forwarding(const struct sk_buff *skb)\n{\n\tnet_warn_ratelimited(\"%s: received packets cannot be forwarded while LRO is enabled\\n\",\n\t\t\t     skb->dev->name);\n}\nEXPORT_SYMBOL(__skb_warn_lro_forwarding);\n\nvoid kfree_skb_partial(struct sk_buff *skb, bool head_stolen)\n{\n\tif (head_stolen) {\n\t\tskb_release_head_state(skb);\n\t\tkmem_cache_free(skbuff_head_cache, skb);\n\t} else {\n\t\t__kfree_skb(skb);\n\t}\n}\nEXPORT_SYMBOL(kfree_skb_partial);\n\n/**\n * skb_try_coalesce - try to merge skb to prior one\n * @to: prior buffer\n * @from: buffer to add\n * @fragstolen: pointer to boolean\n * @delta_truesize: how much more was allocated than was requested\n */\nbool skb_try_coalesce(struct sk_buff *to, struct sk_buff *from,\n\t\t      bool *fragstolen, int *delta_truesize)\n{\n\tint i, delta, len = from->len;\n\n\t*fragstolen = false;\n\n\tif (skb_cloned(to))\n\t\treturn false;\n\n\tif (len <= skb_tailroom(to)) {\n\t\tBUG_ON(skb_copy_bits(from, 0, skb_put(to, len), len));\n\t\t*delta_truesize = 0;\n\t\treturn true;\n\t}\n\n\tif (skb_has_frag_list(to) || skb_has_frag_list(from))\n\t\treturn false;\n\n\tif (skb_headlen(from) != 0) {\n\t\tstruct page *page;\n\t\tunsigned int offset;\n\n\t\tif (skb_shinfo(to)->nr_frags +\n\t\t    skb_shinfo(from)->nr_frags >= MAX_SKB_FRAGS)\n\t\t\treturn false;\n\n\t\tif (skb_head_is_locked(from))\n\t\t\treturn false;\n\n\t\tdelta = from->truesize - SKB_DATA_ALIGN(sizeof(struct sk_buff));\n\n\t\tpage = virt_to_head_page(from->head);\n\t\toffset = from->data - (unsigned char *)page_address(page);\n\n\t\tskb_fill_page_desc(to, skb_shinfo(to)->nr_frags,\n\t\t\t\t   page, offset, skb_headlen(from));\n\t\t*fragstolen = true;\n\t} else {\n\t\tif (skb_shinfo(to)->nr_frags +\n\t\t    skb_shinfo(from)->nr_frags > MAX_SKB_FRAGS)\n\t\t\treturn false;\n\n\t\tdelta = from->truesize - SKB_TRUESIZE(skb_end_offset(from));\n\t}\n\n\tWARN_ON_ONCE(delta < len);\n\n\tmemcpy(skb_shinfo(to)->frags + skb_shinfo(to)->nr_frags,\n\t       skb_shinfo(from)->frags,\n\t       skb_shinfo(from)->nr_frags * sizeof(skb_frag_t));\n\tskb_shinfo(to)->nr_frags += skb_shinfo(from)->nr_frags;\n\n\tif (!skb_cloned(from))\n\t\tskb_shinfo(from)->nr_frags = 0;\n\n\t/* if the skb is not cloned this does nothing\n\t * since we set nr_frags to 0.\n\t */\n\tfor (i = 0; i < skb_shinfo(from)->nr_frags; i++)\n\t\tskb_frag_ref(from, i);\n\n\tto->truesize += delta;\n\tto->len += len;\n\tto->data_len += len;\n\n\t*delta_truesize = delta;\n\treturn true;\n}\nEXPORT_SYMBOL(skb_try_coalesce);\n\n/**\n * skb_scrub_packet - scrub an skb\n *\n * @skb: buffer to clean\n * @xnet: packet is crossing netns\n *\n * skb_scrub_packet can be used after encapsulating or decapsulting a packet\n * into/from a tunnel. Some information have to be cleared during these\n * operations.\n * skb_scrub_packet can also be used to clean a skb before injecting it in\n * another namespace (@xnet == true). We have to clear all information in the\n * skb that could impact namespace isolation.\n */\nvoid skb_scrub_packet(struct sk_buff *skb, bool xnet)\n{\n\tif (xnet)\n\t\tskb_orphan(skb);\n\tskb->tstamp.tv64 = 0;\n\tskb->pkt_type = PACKET_HOST;\n\tskb->skb_iif = 0;\n\tskb->local_df = 0;\n\tskb_dst_drop(skb);\n\tskb->mark = 0;\n\tsecpath_reset(skb);\n\tnf_reset(skb);\n\tnf_reset_trace(skb);\n}\nEXPORT_SYMBOL_GPL(skb_scrub_packet);\n\n/**\n * skb_gso_transport_seglen - Return length of individual segments of a gso packet\n *\n * @skb: GSO skb\n *\n * skb_gso_transport_seglen is used to determine the real size of the\n * individual segments, including Layer4 headers (TCP/UDP).\n *\n * The MAC/L2 or network (IP, IPv6) headers are not accounted for.\n */\nunsigned int skb_gso_transport_seglen(const struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\tunsigned int hdr_len;\n\n\tif (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6)))\n\t\thdr_len = tcp_hdrlen(skb);\n\telse\n\t\thdr_len = sizeof(struct udphdr);\n\treturn hdr_len + shinfo->gso_size;\n}\nEXPORT_SYMBOL_GPL(skb_gso_transport_seglen);\n"], "filenames": ["net/core/skbuff.c"], "buggy_code_start_loc": [2856], "buggy_code_end_loc": [3000], "fixing_code_start_loc": [2857], "fixing_code_end_loc": [3007], "type": "CWE-416", "message": "Use-after-free vulnerability in the skb_segment function in net/core/skbuff.c in the Linux kernel through 3.13.6 allows attackers to obtain sensitive information from kernel memory by leveraging the absence of a certain orphaning operation.", "other": {"cve": {"id": "CVE-2014-0131", "sourceIdentifier": "secalert@redhat.com", "published": "2014-03-24T16:40:48.093", "lastModified": "2023-02-13T00:32:38.900", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Use-after-free vulnerability in the skb_segment function in net/core/skbuff.c in the Linux kernel through 3.13.6 allows attackers to obtain sensitive information from kernel memory by leveraging the absence of a certain orphaning operation."}, {"lang": "es", "value": "Vulnerabilidad de uso despu\u00e9s de liberaci\u00f3n en la funci\u00f3n skb_segment en net/core/skbuff.c en el kernel de Linux hasta 3.13.6 permite a atacantes obtener informaci\u00f3n sensible de la memoria del kernel mediante el aprovechamiento de la ausencia de cierta operaci\u00f3n hu\u00e9rfana."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:A/AC:M/Au:N/C:P/I:N/A:N", "accessVector": "ADJACENT_NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 2.9}, "baseSeverity": "LOW", "exploitabilityScore": 5.5, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.0", "versionEndIncluding": "3.13.6", "matchCriteriaId": "AACBAF47-C734-432C-AB3B-5BD4474107E6"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:evergreen:11.4:*:*:*:*:*:*:*", "matchCriteriaId": "CCE4D64E-8C4B-4F21-A9B0-90637C85C1D0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:suse:linux_enterprise_server:11:sp2:*:*:ltss:*:*:*", "matchCriteriaId": "CB6476C7-03F2-4939-AB85-69AA524516D9"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=1fd819ecb90cc9b822cd84d3056ddba315d3340f", "source": "secalert@redhat.com"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-03/msg00010.html", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-03/msg00025.html", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2014/03/10/4", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.spinics.net/lists/netdev/msg274250.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.spinics.net/lists/netdev/msg274316.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1074589", "source": "secalert@redhat.com", "tags": ["Issue Tracking"]}, {"url": "https://github.com/torvalds/linux/commit/1fd819ecb90cc9b822cd84d3056ddba315d3340f", "source": "secalert@redhat.com", "tags": ["Patch", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/1fd819ecb90cc9b822cd84d3056ddba315d3340f"}}