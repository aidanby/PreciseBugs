{"buggy_code": ["/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com\n * Copyright (c) 2016 Facebook\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of version 2 of the GNU General Public\n * License as published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful, but\n * WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n * General Public License for more details.\n */\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/bpf.h>\n#include <linux/bpf_verifier.h>\n#include <linux/filter.h>\n#include <net/netlink.h>\n#include <linux/file.h>\n#include <linux/vmalloc.h>\n#include <linux/stringify.h>\n\n#include \"disasm.h\"\n\nstatic const struct bpf_verifier_ops * const bpf_verifier_ops[] = {\n#define BPF_PROG_TYPE(_id, _name) \\\n\t[_id] = & _name ## _verifier_ops,\n#define BPF_MAP_TYPE(_id, _ops)\n#include <linux/bpf_types.h>\n#undef BPF_PROG_TYPE\n#undef BPF_MAP_TYPE\n};\n\n/* bpf_check() is a static code analyzer that walks eBPF program\n * instruction by instruction and updates register/stack state.\n * All paths of conditional branches are analyzed until 'bpf_exit' insn.\n *\n * The first pass is depth-first-search to check that the program is a DAG.\n * It rejects the following programs:\n * - larger than BPF_MAXINSNS insns\n * - if loop is present (detected via back-edge)\n * - unreachable insns exist (shouldn't be a forest. program = one function)\n * - out of bounds or malformed jumps\n * The second pass is all possible path descent from the 1st insn.\n * Since it's analyzing all pathes through the program, the length of the\n * analysis is limited to 64k insn, which may be hit even if total number of\n * insn is less then 4K, but there are too many branches that change stack/regs.\n * Number of 'branches to be analyzed' is limited to 1k\n *\n * On entry to each instruction, each register has a type, and the instruction\n * changes the types of the registers depending on instruction semantics.\n * If instruction is BPF_MOV64_REG(BPF_REG_1, BPF_REG_5), then type of R5 is\n * copied to R1.\n *\n * All registers are 64-bit.\n * R0 - return register\n * R1-R5 argument passing registers\n * R6-R9 callee saved registers\n * R10 - frame pointer read-only\n *\n * At the start of BPF program the register R1 contains a pointer to bpf_context\n * and has type PTR_TO_CTX.\n *\n * Verifier tracks arithmetic operations on pointers in case:\n *    BPF_MOV64_REG(BPF_REG_1, BPF_REG_10),\n *    BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, -20),\n * 1st insn copies R10 (which has FRAME_PTR) type into R1\n * and 2nd arithmetic instruction is pattern matched to recognize\n * that it wants to construct a pointer to some element within stack.\n * So after 2nd insn, the register R1 has type PTR_TO_STACK\n * (and -20 constant is saved for further stack bounds checking).\n * Meaning that this reg is a pointer to stack plus known immediate constant.\n *\n * Most of the time the registers have SCALAR_VALUE type, which\n * means the register has some value, but it's not a valid pointer.\n * (like pointer plus pointer becomes SCALAR_VALUE type)\n *\n * When verifier sees load or store instructions the type of base register\n * can be: PTR_TO_MAP_VALUE, PTR_TO_CTX, PTR_TO_STACK. These are three pointer\n * types recognized by check_mem_access() function.\n *\n * PTR_TO_MAP_VALUE means that this register is pointing to 'map element value'\n * and the range of [ptr, ptr + map's value_size) is accessible.\n *\n * registers used to pass values to function calls are checked against\n * function argument constraints.\n *\n * ARG_PTR_TO_MAP_KEY is one of such argument constraints.\n * It means that the register type passed to this function must be\n * PTR_TO_STACK and it will be used inside the function as\n * 'pointer to map element key'\n *\n * For example the argument constraints for bpf_map_lookup_elem():\n *   .ret_type = RET_PTR_TO_MAP_VALUE_OR_NULL,\n *   .arg1_type = ARG_CONST_MAP_PTR,\n *   .arg2_type = ARG_PTR_TO_MAP_KEY,\n *\n * ret_type says that this function returns 'pointer to map elem value or null'\n * function expects 1st argument to be a const pointer to 'struct bpf_map' and\n * 2nd argument should be a pointer to stack, which will be used inside\n * the helper function as a pointer to map element key.\n *\n * On the kernel side the helper function looks like:\n * u64 bpf_map_lookup_elem(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)\n * {\n *    struct bpf_map *map = (struct bpf_map *) (unsigned long) r1;\n *    void *key = (void *) (unsigned long) r2;\n *    void *value;\n *\n *    here kernel can access 'key' and 'map' pointers safely, knowing that\n *    [key, key + map->key_size) bytes are valid and were initialized on\n *    the stack of eBPF program.\n * }\n *\n * Corresponding eBPF program may look like:\n *    BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),  // after this insn R2 type is FRAME_PTR\n *    BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -4), // after this insn R2 type is PTR_TO_STACK\n *    BPF_LD_MAP_FD(BPF_REG_1, map_fd),      // after this insn R1 type is CONST_PTR_TO_MAP\n *    BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_map_lookup_elem),\n * here verifier looks at prototype of map_lookup_elem() and sees:\n * .arg1_type == ARG_CONST_MAP_PTR and R1->type == CONST_PTR_TO_MAP, which is ok,\n * Now verifier knows that this map has key of R1->map_ptr->key_size bytes\n *\n * Then .arg2_type == ARG_PTR_TO_MAP_KEY and R2->type == PTR_TO_STACK, ok so far,\n * Now verifier checks that [R2, R2 + map's key_size) are within stack limits\n * and were initialized prior to this call.\n * If it's ok, then verifier allows this BPF_CALL insn and looks at\n * .ret_type which is RET_PTR_TO_MAP_VALUE_OR_NULL, so it sets\n * R0->type = PTR_TO_MAP_VALUE_OR_NULL which means bpf_map_lookup_elem() function\n * returns ether pointer to map value or NULL.\n *\n * When type PTR_TO_MAP_VALUE_OR_NULL passes through 'if (reg != 0) goto +off'\n * insn, the register holding that pointer in the true branch changes state to\n * PTR_TO_MAP_VALUE and the same register changes state to CONST_IMM in the false\n * branch. See check_cond_jmp_op().\n *\n * After the call R0 is set to return type of the function and registers R1-R5\n * are set to NOT_INIT to indicate that they are no longer readable.\n */\n\n/* verifier_state + insn_idx are pushed to stack when branch is encountered */\nstruct bpf_verifier_stack_elem {\n\t/* verifer state is 'st'\n\t * before processing instruction 'insn_idx'\n\t * and after processing instruction 'prev_insn_idx'\n\t */\n\tstruct bpf_verifier_state st;\n\tint insn_idx;\n\tint prev_insn_idx;\n\tstruct bpf_verifier_stack_elem *next;\n};\n\n#define BPF_COMPLEXITY_LIMIT_INSNS\t131072\n#define BPF_COMPLEXITY_LIMIT_STACK\t1024\n\n#define BPF_MAP_PTR_POISON ((void *)0xeB9F + POISON_POINTER_DELTA)\n\nstruct bpf_call_arg_meta {\n\tstruct bpf_map *map_ptr;\n\tbool raw_mode;\n\tbool pkt_access;\n\tint regno;\n\tint access_size;\n};\n\nstatic DEFINE_MUTEX(bpf_verifier_lock);\n\n/* log_level controls verbosity level of eBPF verifier.\n * verbose() is used to dump the verification trace to the log, so the user\n * can figure out what's wrong with the program\n */\nstatic __printf(2, 3) void verbose(struct bpf_verifier_env *env,\n\t\t\t\t   const char *fmt, ...)\n{\n\tstruct bpf_verifer_log *log = &env->log;\n\tunsigned int n;\n\tva_list args;\n\n\tif (!log->level || !log->ubuf || bpf_verifier_log_full(log))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tn = vscnprintf(log->kbuf, BPF_VERIFIER_TMP_LOG_SIZE, fmt, args);\n\tva_end(args);\n\n\tWARN_ONCE(n >= BPF_VERIFIER_TMP_LOG_SIZE - 1,\n\t\t  \"verifier log line truncated - local buffer too short\\n\");\n\n\tn = min(log->len_total - log->len_used - 1, n);\n\tlog->kbuf[n] = '\\0';\n\n\tif (!copy_to_user(log->ubuf + log->len_used, log->kbuf, n + 1))\n\t\tlog->len_used += n;\n\telse\n\t\tlog->ubuf = NULL;\n}\n\nstatic bool type_is_pkt_pointer(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_PACKET ||\n\t       type == PTR_TO_PACKET_META;\n}\n\n/* string representation of 'enum bpf_reg_type' */\nstatic const char * const reg_type_str[] = {\n\t[NOT_INIT]\t\t= \"?\",\n\t[SCALAR_VALUE]\t\t= \"inv\",\n\t[PTR_TO_CTX]\t\t= \"ctx\",\n\t[CONST_PTR_TO_MAP]\t= \"map_ptr\",\n\t[PTR_TO_MAP_VALUE]\t= \"map_value\",\n\t[PTR_TO_MAP_VALUE_OR_NULL] = \"map_value_or_null\",\n\t[PTR_TO_STACK]\t\t= \"fp\",\n\t[PTR_TO_PACKET]\t\t= \"pkt\",\n\t[PTR_TO_PACKET_META]\t= \"pkt_meta\",\n\t[PTR_TO_PACKET_END]\t= \"pkt_end\",\n};\n\nstatic void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t struct bpf_verifier_state *state)\n{\n\tstruct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d=%s\", i, reg_type_str[t]);\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (t == CONST_PTR_TO_MAP ||\n\t\t\t\t t == PTR_TO_MAP_VALUE ||\n\t\t\t\t t == PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tif (state->stack[i].slot_type[0] == STACK_SPILL)\n\t\t\tverbose(env, \" fp%d=%s\",\n\t\t\t\t-MAX_BPF_STACK + i * BPF_REG_SIZE,\n\t\t\t\treg_type_str[state->stack[i].spilled_ptr.type]);\n\t}\n\tverbose(env, \"\\n\");\n}\n\nstatic int copy_stack_state(struct bpf_verifier_state *dst,\n\t\t\t    const struct bpf_verifier_state *src)\n{\n\tif (!src->stack)\n\t\treturn 0;\n\tif (WARN_ON_ONCE(dst->allocated_stack < src->allocated_stack)) {\n\t\t/* internal bug, make state invalid to reject the program */\n\t\tmemset(dst, 0, sizeof(*dst));\n\t\treturn -EFAULT;\n\t}\n\tmemcpy(dst->stack, src->stack,\n\t       sizeof(*src->stack) * (src->allocated_stack / BPF_REG_SIZE));\n\treturn 0;\n}\n\n/* do_check() starts with zero-sized stack in struct bpf_verifier_state to\n * make it consume minimal amount of memory. check_stack_write() access from\n * the program calls into realloc_verifier_state() to grow the stack size.\n * Note there is a non-zero 'parent' pointer inside bpf_verifier_state\n * which this function copies over. It points to previous bpf_verifier_state\n * which is never reallocated\n */\nstatic int realloc_verifier_state(struct bpf_verifier_state *state, int size,\n\t\t\t\t  bool copy_old)\n{\n\tu32 old_size = state->allocated_stack;\n\tstruct bpf_stack_state *new_stack;\n\tint slot = size / BPF_REG_SIZE;\n\n\tif (size <= old_size || !size) {\n\t\tif (copy_old)\n\t\t\treturn 0;\n\t\tstate->allocated_stack = slot * BPF_REG_SIZE;\n\t\tif (!size && old_size) {\n\t\t\tkfree(state->stack);\n\t\t\tstate->stack = NULL;\n\t\t}\n\t\treturn 0;\n\t}\n\tnew_stack = kmalloc_array(slot, sizeof(struct bpf_stack_state),\n\t\t\t\t  GFP_KERNEL);\n\tif (!new_stack)\n\t\treturn -ENOMEM;\n\tif (copy_old) {\n\t\tif (state->stack)\n\t\t\tmemcpy(new_stack, state->stack,\n\t\t\t       sizeof(*new_stack) * (old_size / BPF_REG_SIZE));\n\t\tmemset(new_stack + old_size / BPF_REG_SIZE, 0,\n\t\t       sizeof(*new_stack) * (size - old_size) / BPF_REG_SIZE);\n\t}\n\tstate->allocated_stack = slot * BPF_REG_SIZE;\n\tkfree(state->stack);\n\tstate->stack = new_stack;\n\treturn 0;\n}\n\nstatic void free_verifier_state(struct bpf_verifier_state *state,\n\t\t\t\tbool free_self)\n{\n\tkfree(state->stack);\n\tif (free_self)\n\t\tkfree(state);\n}\n\n/* copy verifier state from src to dst growing dst stack space\n * when necessary to accommodate larger src stack\n */\nstatic int copy_verifier_state(struct bpf_verifier_state *dst,\n\t\t\t       const struct bpf_verifier_state *src)\n{\n\tint err;\n\n\terr = realloc_verifier_state(dst, src->allocated_stack, false);\n\tif (err)\n\t\treturn err;\n\tmemcpy(dst, src, offsetof(struct bpf_verifier_state, allocated_stack));\n\treturn copy_stack_state(dst, src);\n}\n\nstatic int pop_stack(struct bpf_verifier_env *env, int *prev_insn_idx,\n\t\t     int *insn_idx)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_verifier_stack_elem *elem, *head = env->head;\n\tint err;\n\n\tif (env->head == NULL)\n\t\treturn -ENOENT;\n\n\tif (cur) {\n\t\terr = copy_verifier_state(cur, &head->st);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (insn_idx)\n\t\t*insn_idx = head->insn_idx;\n\tif (prev_insn_idx)\n\t\t*prev_insn_idx = head->prev_insn_idx;\n\telem = head->next;\n\tfree_verifier_state(&head->st, false);\n\tkfree(head);\n\tenv->head = elem;\n\tenv->stack_size--;\n\treturn 0;\n}\n\nstatic struct bpf_verifier_state *push_stack(struct bpf_verifier_env *env,\n\t\t\t\t\t     int insn_idx, int prev_insn_idx)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_verifier_stack_elem *elem;\n\tint err;\n\n\telem = kzalloc(sizeof(struct bpf_verifier_stack_elem), GFP_KERNEL);\n\tif (!elem)\n\t\tgoto err;\n\n\telem->insn_idx = insn_idx;\n\telem->prev_insn_idx = prev_insn_idx;\n\telem->next = env->head;\n\tenv->head = elem;\n\tenv->stack_size++;\n\terr = copy_verifier_state(&elem->st, cur);\n\tif (err)\n\t\tgoto err;\n\tif (env->stack_size > BPF_COMPLEXITY_LIMIT_STACK) {\n\t\tverbose(env, \"BPF program is too complex\\n\");\n\t\tgoto err;\n\t}\n\treturn &elem->st;\nerr:\n\t/* pop all elements and return */\n\twhile (!pop_stack(env, NULL, NULL));\n\treturn NULL;\n}\n\n#define CALLER_SAVED_REGS 6\nstatic const int caller_saved[CALLER_SAVED_REGS] = {\n\tBPF_REG_0, BPF_REG_1, BPF_REG_2, BPF_REG_3, BPF_REG_4, BPF_REG_5\n};\n\nstatic void __mark_reg_not_init(struct bpf_reg_state *reg);\n\n/* Mark the unknown part of a register (variable offset or scalar value) as\n * known to have the value @imm.\n */\nstatic void __mark_reg_known(struct bpf_reg_state *reg, u64 imm)\n{\n\treg->id = 0;\n\treg->var_off = tnum_const(imm);\n\treg->smin_value = (s64)imm;\n\treg->smax_value = (s64)imm;\n\treg->umin_value = imm;\n\treg->umax_value = imm;\n}\n\n/* Mark the 'variable offset' part of a register as zero.  This should be\n * used only on registers holding a pointer type.\n */\nstatic void __mark_reg_known_zero(struct bpf_reg_state *reg)\n{\n\t__mark_reg_known(reg, 0);\n}\n\nstatic void mark_reg_known_zero(struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_known_zero(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs */\n\t\tfor (regno = 0; regno < MAX_BPF_REG; regno++)\n\t\t\t__mark_reg_not_init(regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_known_zero(regs + regno);\n}\n\nstatic bool reg_is_pkt_pointer(const struct bpf_reg_state *reg)\n{\n\treturn type_is_pkt_pointer(reg->type);\n}\n\nstatic bool reg_is_pkt_pointer_any(const struct bpf_reg_state *reg)\n{\n\treturn reg_is_pkt_pointer(reg) ||\n\t       reg->type == PTR_TO_PACKET_END;\n}\n\n/* Unmodified PTR_TO_PACKET[_META,_END] register from ctx access. */\nstatic bool reg_is_init_pkt_pointer(const struct bpf_reg_state *reg,\n\t\t\t\t    enum bpf_reg_type which)\n{\n\t/* The register can already have a range from prior markings.\n\t * This is fine as long as it hasn't been advanced from its\n\t * origin.\n\t */\n\treturn reg->type == which &&\n\t       reg->id == 0 &&\n\t       reg->off == 0 &&\n\t       tnum_equals_const(reg->var_off, 0);\n}\n\n/* Attempts to improve min/max values based on var_off information */\nstatic void __update_reg_bounds(struct bpf_reg_state *reg)\n{\n\t/* min signed is max(sign bit) | min(other bits) */\n\treg->smin_value = max_t(s64, reg->smin_value,\n\t\t\t\treg->var_off.value | (reg->var_off.mask & S64_MIN));\n\t/* max signed is min(sign bit) | max(other bits) */\n\treg->smax_value = min_t(s64, reg->smax_value,\n\t\t\t\treg->var_off.value | (reg->var_off.mask & S64_MAX));\n\treg->umin_value = max(reg->umin_value, reg->var_off.value);\n\treg->umax_value = min(reg->umax_value,\n\t\t\t      reg->var_off.value | reg->var_off.mask);\n}\n\n/* Uses signed min/max values to inform unsigned, and vice-versa */\nstatic void __reg_deduce_bounds(struct bpf_reg_state *reg)\n{\n\t/* Learn sign from signed bounds.\n\t * If we cannot cross the sign boundary, then signed and unsigned bounds\n\t * are the same, so combine.  This works even in the negative case, e.g.\n\t * -3 s<= x s<= -1 implies 0xf...fd u<= x u<= 0xf...ff.\n\t */\n\tif (reg->smin_value >= 0 || reg->smax_value < 0) {\n\t\treg->smin_value = reg->umin_value = max_t(u64, reg->smin_value,\n\t\t\t\t\t\t\t  reg->umin_value);\n\t\treg->smax_value = reg->umax_value = min_t(u64, reg->smax_value,\n\t\t\t\t\t\t\t  reg->umax_value);\n\t\treturn;\n\t}\n\t/* Learn sign from unsigned bounds.  Signed bounds cross the sign\n\t * boundary, so we must be careful.\n\t */\n\tif ((s64)reg->umax_value >= 0) {\n\t\t/* Positive.  We can't learn anything from the smin, but smax\n\t\t * is positive, hence safe.\n\t\t */\n\t\treg->smin_value = reg->umin_value;\n\t\treg->smax_value = reg->umax_value = min_t(u64, reg->smax_value,\n\t\t\t\t\t\t\t  reg->umax_value);\n\t} else if ((s64)reg->umin_value < 0) {\n\t\t/* Negative.  We can't learn anything from the smax, but smin\n\t\t * is negative, hence safe.\n\t\t */\n\t\treg->smin_value = reg->umin_value = max_t(u64, reg->smin_value,\n\t\t\t\t\t\t\t  reg->umin_value);\n\t\treg->smax_value = reg->umax_value;\n\t}\n}\n\n/* Attempts to improve var_off based on unsigned min/max information */\nstatic void __reg_bound_offset(struct bpf_reg_state *reg)\n{\n\treg->var_off = tnum_intersect(reg->var_off,\n\t\t\t\t      tnum_range(reg->umin_value,\n\t\t\t\t\t\t reg->umax_value));\n}\n\n/* Reset the min/max bounds of a register */\nstatic void __mark_reg_unbounded(struct bpf_reg_state *reg)\n{\n\treg->smin_value = S64_MIN;\n\treg->smax_value = S64_MAX;\n\treg->umin_value = 0;\n\treg->umax_value = U64_MAX;\n}\n\n/* Mark a register as having a completely unknown (scalar) value. */\nstatic void __mark_reg_unknown(struct bpf_reg_state *reg)\n{\n\treg->type = SCALAR_VALUE;\n\treg->id = 0;\n\treg->off = 0;\n\treg->var_off = tnum_unknown;\n\t__mark_reg_unbounded(reg);\n}\n\nstatic void mark_reg_unknown(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_unknown(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs */\n\t\tfor (regno = 0; regno < MAX_BPF_REG; regno++)\n\t\t\t__mark_reg_not_init(regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_unknown(regs + regno);\n}\n\nstatic void __mark_reg_not_init(struct bpf_reg_state *reg)\n{\n\t__mark_reg_unknown(reg);\n\treg->type = NOT_INIT;\n}\n\nstatic void mark_reg_not_init(struct bpf_verifier_env *env,\n\t\t\t      struct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_not_init(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs */\n\t\tfor (regno = 0; regno < MAX_BPF_REG; regno++)\n\t\t\t__mark_reg_not_init(regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_not_init(regs + regno);\n}\n\nstatic void init_reg_state(struct bpf_verifier_env *env,\n\t\t\t   struct bpf_reg_state *regs)\n{\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\tmark_reg_not_init(env, regs, i);\n\t\tregs[i].live = REG_LIVE_NONE;\n\t}\n\n\t/* frame pointer */\n\tregs[BPF_REG_FP].type = PTR_TO_STACK;\n\tmark_reg_known_zero(env, regs, BPF_REG_FP);\n\n\t/* 1st arg to a function */\n\tregs[BPF_REG_1].type = PTR_TO_CTX;\n\tmark_reg_known_zero(env, regs, BPF_REG_1);\n}\n\nenum reg_arg_type {\n\tSRC_OP,\t\t/* register is used as source operand */\n\tDST_OP,\t\t/* register is used as destination operand */\n\tDST_OP_NO_MARK\t/* same as above, check only, don't mark */\n};\n\nstatic void mark_reg_read(const struct bpf_verifier_state *state, u32 regno)\n{\n\tstruct bpf_verifier_state *parent = state->parent;\n\n\tif (regno == BPF_REG_FP)\n\t\t/* We don't need to worry about FP liveness because it's read-only */\n\t\treturn;\n\n\twhile (parent) {\n\t\t/* if read wasn't screened by an earlier write ... */\n\t\tif (state->regs[regno].live & REG_LIVE_WRITTEN)\n\t\t\tbreak;\n\t\t/* ... then we depend on parent's value */\n\t\tparent->regs[regno].live |= REG_LIVE_READ;\n\t\tstate = parent;\n\t\tparent = state->parent;\n\t}\n}\n\nstatic int check_reg_arg(struct bpf_verifier_env *env, u32 regno,\n\t\t\t enum reg_arg_type t)\n{\n\tstruct bpf_reg_state *regs = env->cur_state->regs;\n\n\tif (regno >= MAX_BPF_REG) {\n\t\tverbose(env, \"R%d is invalid\\n\", regno);\n\t\treturn -EINVAL;\n\t}\n\n\tif (t == SRC_OP) {\n\t\t/* check whether register used as source operand can be read */\n\t\tif (regs[regno].type == NOT_INIT) {\n\t\t\tverbose(env, \"R%d !read_ok\\n\", regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmark_reg_read(env->cur_state, regno);\n\t} else {\n\t\t/* check whether register used as dest operand can be written to */\n\t\tif (regno == BPF_REG_FP) {\n\t\t\tverbose(env, \"frame pointer is read only\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tregs[regno].live |= REG_LIVE_WRITTEN;\n\t\tif (t == DST_OP)\n\t\t\tmark_reg_unknown(env, regs, regno);\n\t}\n\treturn 0;\n}\n\nstatic bool is_spillable_regtype(enum bpf_reg_type type)\n{\n\tswitch (type) {\n\tcase PTR_TO_MAP_VALUE:\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\tcase PTR_TO_STACK:\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET_END:\n\tcase CONST_PTR_TO_MAP:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* check_stack_read/write functions track spill/fill of registers,\n * stack boundary and alignment are checked in check_mem_access()\n */\nstatic int check_stack_write(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_verifier_state *state, int off,\n\t\t\t     int size, int value_regno)\n{\n\tint i, slot = -off - 1, spi = slot / BPF_REG_SIZE, err;\n\n\terr = realloc_verifier_state(state, round_up(slot + 1, BPF_REG_SIZE),\n\t\t\t\t     true);\n\tif (err)\n\t\treturn err;\n\t/* caller checked that off % size == 0 and -MAX_BPF_STACK <= off < 0,\n\t * so it's aligned access and [off, off + size) are within stack limits\n\t */\n\tif (!env->allow_ptr_leaks &&\n\t    state->stack[spi].slot_type[0] == STACK_SPILL &&\n\t    size != BPF_REG_SIZE) {\n\t\tverbose(env, \"attempt to corrupt spilled pointer on stack\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tif (value_regno >= 0 &&\n\t    is_spillable_regtype(state->regs[value_regno].type)) {\n\n\t\t/* register containing pointer is being spilled into stack */\n\t\tif (size != BPF_REG_SIZE) {\n\t\t\tverbose(env, \"invalid size of register spill\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* save register state */\n\t\tstate->stack[spi].spilled_ptr = state->regs[value_regno];\n\t\tstate->stack[spi].spilled_ptr.live |= REG_LIVE_WRITTEN;\n\n\t\tfor (i = 0; i < BPF_REG_SIZE; i++)\n\t\t\tstate->stack[spi].slot_type[i] = STACK_SPILL;\n\t} else {\n\t\t/* regular write of data into stack */\n\t\tstate->stack[spi].spilled_ptr = (struct bpf_reg_state) {};\n\n\t\tfor (i = 0; i < size; i++)\n\t\t\tstate->stack[spi].slot_type[(slot - i) % BPF_REG_SIZE] =\n\t\t\t\tSTACK_MISC;\n\t}\n\treturn 0;\n}\n\nstatic void mark_stack_slot_read(const struct bpf_verifier_state *state, int slot)\n{\n\tstruct bpf_verifier_state *parent = state->parent;\n\n\twhile (parent) {\n\t\t/* if read wasn't screened by an earlier write ... */\n\t\tif (state->stack[slot].spilled_ptr.live & REG_LIVE_WRITTEN)\n\t\t\tbreak;\n\t\t/* ... then we depend on parent's value */\n\t\tparent->stack[slot].spilled_ptr.live |= REG_LIVE_READ;\n\t\tstate = parent;\n\t\tparent = state->parent;\n\t}\n}\n\nstatic int check_stack_read(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_verifier_state *state, int off, int size,\n\t\t\t    int value_regno)\n{\n\tint i, slot = -off - 1, spi = slot / BPF_REG_SIZE;\n\tu8 *stype;\n\n\tif (state->allocated_stack <= slot) {\n\t\tverbose(env, \"invalid read from stack off %d+0 size %d\\n\",\n\t\t\toff, size);\n\t\treturn -EACCES;\n\t}\n\tstype = state->stack[spi].slot_type;\n\n\tif (stype[0] == STACK_SPILL) {\n\t\tif (size != BPF_REG_SIZE) {\n\t\t\tverbose(env, \"invalid size of register spill\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tfor (i = 1; i < BPF_REG_SIZE; i++) {\n\t\t\tif (stype[(slot - i) % BPF_REG_SIZE] != STACK_SPILL) {\n\t\t\t\tverbose(env, \"corrupted spill memory\\n\");\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\t\t}\n\n\t\tif (value_regno >= 0) {\n\t\t\t/* restore register state from stack */\n\t\t\tstate->regs[value_regno] = state->stack[spi].spilled_ptr;\n\t\t\tmark_stack_slot_read(state, spi);\n\t\t}\n\t\treturn 0;\n\t} else {\n\t\tfor (i = 0; i < size; i++) {\n\t\t\tif (stype[(slot - i) % BPF_REG_SIZE] != STACK_MISC) {\n\t\t\t\tverbose(env, \"invalid read from stack off %d+%d size %d\\n\",\n\t\t\t\t\toff, i, size);\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\t\t}\n\t\tif (value_regno >= 0)\n\t\t\t/* have read misc data from the stack */\n\t\t\tmark_reg_unknown(env, state->regs, value_regno);\n\t\treturn 0;\n\t}\n}\n\n/* check read/write into map element returned by bpf_map_lookup_elem() */\nstatic int __check_map_access(struct bpf_verifier_env *env, u32 regno, int off,\n\t\t\t      int size, bool zero_size_allowed)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_map *map = regs[regno].map_ptr;\n\n\tif (off < 0 || size < 0 || (size == 0 && !zero_size_allowed) ||\n\t    off + size > map->value_size) {\n\t\tverbose(env, \"invalid access to map value, value_size=%d off=%d size=%d\\n\",\n\t\t\tmap->value_size, off, size);\n\t\treturn -EACCES;\n\t}\n\treturn 0;\n}\n\n/* check read/write into a map element with possible variable offset */\nstatic int check_map_access(struct bpf_verifier_env *env, u32 regno,\n\t\t\t    int off, int size, bool zero_size_allowed)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_reg_state *reg = &state->regs[regno];\n\tint err;\n\n\t/* We may have adjusted the register to this map value, so we\n\t * need to try adding each of min_value and max_value to off\n\t * to make sure our theoretical access will be safe.\n\t */\n\tif (env->log.level)\n\t\tprint_verifier_state(env, state);\n\t/* The minimum value is only important with signed\n\t * comparisons where we can't assume the floor of a\n\t * value is 0.  If we are using signed variables for our\n\t * index'es we need to make sure that whatever we use\n\t * will have a set floor within our range.\n\t */\n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_map_access(env, regno, reg->smin_value + off, size,\n\t\t\t\t zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d min value is outside of the array range\\n\",\n\t\t\tregno);\n\t\treturn err;\n\t}\n\n\t/* If we haven't set a max value then we need to bail since we can't be\n\t * sure we won't do bad things.\n\t * If reg->umax_value + off could overflow, treat that as unbounded too.\n\t */\n\tif (reg->umax_value >= BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"R%d unbounded memory access, make sure to bounds check any array access into a map\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_map_access(env, regno, reg->umax_value + off, size,\n\t\t\t\t zero_size_allowed);\n\tif (err)\n\t\tverbose(env, \"R%d max value is outside of the array range\\n\",\n\t\t\tregno);\n\treturn err;\n}\n\n#define MAX_PACKET_OFF 0xffff\n\nstatic bool may_access_direct_pkt_data(struct bpf_verifier_env *env,\n\t\t\t\t       const struct bpf_call_arg_meta *meta,\n\t\t\t\t       enum bpf_access_type t)\n{\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_LWT_IN:\n\tcase BPF_PROG_TYPE_LWT_OUT:\n\t\t/* dst_input() and dst_output() can't write for now */\n\t\tif (t == BPF_WRITE)\n\t\t\treturn false;\n\t\t/* fallthrough */\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\tcase BPF_PROG_TYPE_XDP:\n\tcase BPF_PROG_TYPE_LWT_XMIT:\n\tcase BPF_PROG_TYPE_SK_SKB:\n\t\tif (meta)\n\t\t\treturn meta->pkt_access;\n\n\t\tenv->seen_direct_write = true;\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic int __check_packet_access(struct bpf_verifier_env *env, u32 regno,\n\t\t\t\t int off, int size, bool zero_size_allowed)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\n\tif (off < 0 || size < 0 || (size == 0 && !zero_size_allowed) ||\n\t    (u64)off + size > reg->range) {\n\t\tverbose(env, \"invalid access to packet, off=%d size=%d, R%d(id=%d,off=%d,r=%d)\\n\",\n\t\t\toff, size, regno, reg->id, reg->off, reg->range);\n\t\treturn -EACCES;\n\t}\n\treturn 0;\n}\n\nstatic int check_packet_access(struct bpf_verifier_env *env, u32 regno, int off,\n\t\t\t       int size, bool zero_size_allowed)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\tint err;\n\n\t/* We may have added a variable offset to the packet pointer; but any\n\t * reg->range we have comes after that.  We are only checking the fixed\n\t * offset.\n\t */\n\n\t/* We don't allow negative numbers, because we aren't tracking enough\n\t * detail to prove they're safe.\n\t */\n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_packet_access(env, regno, off, size, zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d offset is outside of the packet\\n\", regno);\n\t\treturn err;\n\t}\n\treturn err;\n}\n\n/* check access to 'struct bpf_context' fields.  Supports fixed offsets only */\nstatic int check_ctx_access(struct bpf_verifier_env *env, int insn_idx, int off, int size,\n\t\t\t    enum bpf_access_type t, enum bpf_reg_type *reg_type)\n{\n\tstruct bpf_insn_access_aux info = {\n\t\t.reg_type = *reg_type,\n\t};\n\n\tif (env->ops->is_valid_access &&\n\t    env->ops->is_valid_access(off, size, t, &info)) {\n\t\t/* A non zero info.ctx_field_size indicates that this field is a\n\t\t * candidate for later verifier transformation to load the whole\n\t\t * field and then apply a mask when accessed with a narrower\n\t\t * access than actual ctx access size. A zero info.ctx_field_size\n\t\t * will only allow for whole field access and rejects any other\n\t\t * type of narrower access.\n\t\t */\n\t\t*reg_type = info.reg_type;\n\n\t\tenv->insn_aux_data[insn_idx].ctx_field_size = info.ctx_field_size;\n\t\t/* remember the offset of last byte accessed in ctx */\n\t\tif (env->prog->aux->max_ctx_offset < off + size)\n\t\t\tenv->prog->aux->max_ctx_offset = off + size;\n\t\treturn 0;\n\t}\n\n\tverbose(env, \"invalid bpf_context access off=%d size=%d\\n\", off, size);\n\treturn -EACCES;\n}\n\nstatic bool __is_pointer_value(bool allow_ptr_leaks,\n\t\t\t       const struct bpf_reg_state *reg)\n{\n\tif (allow_ptr_leaks)\n\t\treturn false;\n\n\treturn reg->type != SCALAR_VALUE;\n}\n\nstatic bool is_pointer_value(struct bpf_verifier_env *env, int regno)\n{\n\treturn __is_pointer_value(env->allow_ptr_leaks, cur_regs(env) + regno);\n}\n\nstatic int check_pkt_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t\t   const struct bpf_reg_state *reg,\n\t\t\t\t   int off, int size, bool strict)\n{\n\tstruct tnum reg_off;\n\tint ip_align;\n\n\t/* Byte size accesses are always allowed. */\n\tif (!strict || size == 1)\n\t\treturn 0;\n\n\t/* For platforms that do not have a Kconfig enabling\n\t * CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS the value of\n\t * NET_IP_ALIGN is universally set to '2'.  And on platforms\n\t * that do set CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS, we get\n\t * to this code only in strict mode where we want to emulate\n\t * the NET_IP_ALIGN==2 checking.  Therefore use an\n\t * unconditional IP align value of '2'.\n\t */\n\tip_align = 2;\n\n\treg_off = tnum_add(reg->var_off, tnum_const(ip_align + reg->off + off));\n\tif (!tnum_is_aligned(reg_off, size)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env,\n\t\t\t\"misaligned packet access off %d+%s+%d+%d size %d\\n\",\n\t\t\tip_align, tn_buf, reg->off, off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_generic_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t\t       const struct bpf_reg_state *reg,\n\t\t\t\t       const char *pointer_desc,\n\t\t\t\t       int off, int size, bool strict)\n{\n\tstruct tnum reg_off;\n\n\t/* Byte size accesses are always allowed. */\n\tif (!strict || size == 1)\n\t\treturn 0;\n\n\treg_off = tnum_add(reg->var_off, tnum_const(reg->off + off));\n\tif (!tnum_is_aligned(reg_off, size)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"misaligned %saccess off %s+%d+%d size %d\\n\",\n\t\t\tpointer_desc, tn_buf, reg->off, off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t       const struct bpf_reg_state *reg,\n\t\t\t       int off, int size)\n{\n\tbool strict = env->strict_alignment;\n\tconst char *pointer_desc = \"\";\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\t/* Special case, because of NET_IP_ALIGN. Given metadata sits\n\t\t * right in front, treat it the very same way.\n\t\t */\n\t\treturn check_pkt_ptr_alignment(env, reg, off, size, strict);\n\tcase PTR_TO_MAP_VALUE:\n\t\tpointer_desc = \"value \";\n\t\tbreak;\n\tcase PTR_TO_CTX:\n\t\tpointer_desc = \"context \";\n\t\tbreak;\n\tcase PTR_TO_STACK:\n\t\tpointer_desc = \"stack \";\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn check_generic_ptr_alignment(env, reg, pointer_desc, off, size,\n\t\t\t\t\t   strict);\n}\n\n/* truncate register to smaller size (in bytes)\n * must be called with size < BPF_REG_SIZE\n */\nstatic void coerce_reg_to_size(struct bpf_reg_state *reg, int size)\n{\n\tu64 mask;\n\n\t/* clear high bits in bit representation */\n\treg->var_off = tnum_cast(reg->var_off, size);\n\n\t/* fix arithmetic bounds */\n\tmask = ((u64)1 << (size * 8)) - 1;\n\tif ((reg->umin_value & ~mask) == (reg->umax_value & ~mask)) {\n\t\treg->umin_value &= mask;\n\t\treg->umax_value &= mask;\n\t} else {\n\t\treg->umin_value = 0;\n\t\treg->umax_value = mask;\n\t}\n\treg->smin_value = reg->umin_value;\n\treg->smax_value = reg->umax_value;\n}\n\n/* check whether memory at (regno + off) is accessible for t = (read | write)\n * if t==write, value_regno is a register which value is stored into memory\n * if t==read, value_regno is a register which will receive the value from memory\n * if t==write && value_regno==-1, some unknown value is stored into memory\n * if t==read && value_regno==-1, don't care what we read from memory\n */\nstatic int check_mem_access(struct bpf_verifier_env *env, int insn_idx, u32 regno, int off,\n\t\t\t    int bpf_size, enum bpf_access_type t,\n\t\t\t    int value_regno)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = regs + regno;\n\tint size, err = 0;\n\n\tsize = bpf_size_to_bytes(bpf_size);\n\tif (size < 0)\n\t\treturn size;\n\n\t/* alignment checks will add in reg->off themselves */\n\terr = check_ptr_alignment(env, reg, off, size);\n\tif (err)\n\t\treturn err;\n\n\t/* for access checks, reg->off is just part of off */\n\toff += reg->off;\n\n\tif (reg->type == PTR_TO_MAP_VALUE) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into map\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_map_access(env, regno, off, size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\n\t} else if (reg->type == PTR_TO_CTX) {\n\t\tenum bpf_reg_type reg_type = SCALAR_VALUE;\n\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into ctx\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* ctx accesses must be at a fixed offset, so that we can\n\t\t * determine what type of data were returned.\n\t\t */\n\t\tif (reg->off) {\n\t\t\tverbose(env,\n\t\t\t\t\"dereference of modified ctx ptr R%d off=%d+%d, ctx+const is allowed, ctx+const+const is not\\n\",\n\t\t\t\tregno, reg->off, off - reg->off);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (!tnum_is_const(reg->var_off) || reg->var_off.value) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env,\n\t\t\t\t\"variable ctx access var_off=%s off=%d size=%d\",\n\t\t\t\ttn_buf, off, size);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_ctx_access(env, insn_idx, off, size, t, &reg_type);\n\t\tif (!err && t == BPF_READ && value_regno >= 0) {\n\t\t\t/* ctx access returns either a scalar, or a\n\t\t\t * PTR_TO_PACKET[_META,_END]. In the latter\n\t\t\t * case, we know the offset is zero.\n\t\t\t */\n\t\t\tif (reg_type == SCALAR_VALUE)\n\t\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t\t\telse\n\t\t\t\tmark_reg_known_zero(env, regs,\n\t\t\t\t\t\t    value_regno);\n\t\t\tregs[value_regno].id = 0;\n\t\t\tregs[value_regno].off = 0;\n\t\t\tregs[value_regno].range = 0;\n\t\t\tregs[value_regno].type = reg_type;\n\t\t}\n\n\t} else if (reg->type == PTR_TO_STACK) {\n\t\t/* stack accesses must be at a fixed offset, so that we can\n\t\t * determine what type of data were returned.\n\t\t * See check_stack_read().\n\t\t */\n\t\tif (!tnum_is_const(reg->var_off)) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"variable stack access var_off=%s off=%d size=%d\",\n\t\t\t\ttn_buf, off, size);\n\t\t\treturn -EACCES;\n\t\t}\n\t\toff += reg->var_off.value;\n\t\tif (off >= 0 || off < -MAX_BPF_STACK) {\n\t\t\tverbose(env, \"invalid stack off=%d size=%d\\n\", off,\n\t\t\t\tsize);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (env->prog->aux->stack_depth < -off)\n\t\t\tenv->prog->aux->stack_depth = -off;\n\n\t\tif (t == BPF_WRITE)\n\t\t\terr = check_stack_write(env, state, off, size,\n\t\t\t\t\t\tvalue_regno);\n\t\telse\n\t\t\terr = check_stack_read(env, state, off, size,\n\t\t\t\t\t       value_regno);\n\t} else if (reg_is_pkt_pointer(reg)) {\n\t\tif (t == BPF_WRITE && !may_access_direct_pkt_data(env, NULL, t)) {\n\t\t\tverbose(env, \"cannot write into packet\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into packet\\n\",\n\t\t\t\tvalue_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_packet_access(env, regno, off, size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else {\n\t\tverbose(env, \"R%d invalid mem access '%s'\\n\", regno,\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!err && size < BPF_REG_SIZE && value_regno >= 0 && t == BPF_READ &&\n\t    regs[value_regno].type == SCALAR_VALUE) {\n\t\t/* b/h/w load zero-extends, mark upper bits as known 0 */\n\t\tcoerce_reg_to_size(&regs[value_regno], size);\n\t}\n\treturn err;\n}\n\nstatic int check_xadd(struct bpf_verifier_env *env, int insn_idx, struct bpf_insn *insn)\n{\n\tint err;\n\n\tif ((BPF_SIZE(insn->code) != BPF_W && BPF_SIZE(insn->code) != BPF_DW) ||\n\t    insn->imm != 0) {\n\t\tverbose(env, \"BPF_XADD uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check src1 operand */\n\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, insn->src_reg)) {\n\t\tverbose(env, \"R%d leaks addr into mem\\n\", insn->src_reg);\n\t\treturn -EACCES;\n\t}\n\n\t/* check whether atomic_add can read the memory */\n\terr = check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t       BPF_SIZE(insn->code), BPF_READ, -1);\n\tif (err)\n\t\treturn err;\n\n\t/* check whether atomic_add can write into the same memory */\n\treturn check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t\tBPF_SIZE(insn->code), BPF_WRITE, -1);\n}\n\n/* Does this register contain a constant zero? */\nstatic bool register_is_null(struct bpf_reg_state reg)\n{\n\treturn reg.type == SCALAR_VALUE && tnum_equals_const(reg.var_off, 0);\n}\n\n/* when register 'regno' is passed into function that will read 'access_size'\n * bytes from that pointer, make sure that it's within stack boundary\n * and all elements of stack are initialized.\n * Unlike most pointer bounds-checking functions, this one doesn't take an\n * 'off' argument, so it has to add in reg->off itself.\n */\nstatic int check_stack_boundary(struct bpf_verifier_env *env, int regno,\n\t\t\t\tint access_size, bool zero_size_allowed,\n\t\t\t\tstruct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_reg_state *regs = state->regs;\n\tint off, i, slot, spi;\n\n\tif (regs[regno].type != PTR_TO_STACK) {\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(regs[regno]))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\t\treg_type_str[regs[regno].type],\n\t\t\treg_type_str[PTR_TO_STACK]);\n\t\treturn -EACCES;\n\t}\n\n\t/* Only allow fixed-offset stack reads */\n\tif (!tnum_is_const(regs[regno].var_off)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), regs[regno].var_off);\n\t\tverbose(env, \"invalid variable stack read R%d var_off=%s\\n\",\n\t\t\tregno, tn_buf);\n\t\treturn -EACCES;\n\t}\n\toff = regs[regno].off + regs[regno].var_off.value;\n\tif (off >= 0 || off < -MAX_BPF_STACK || off + access_size > 0 ||\n\t    access_size < 0 || (access_size == 0 && !zero_size_allowed)) {\n\t\tverbose(env, \"invalid stack type R%d off=%d access_size=%d\\n\",\n\t\t\tregno, off, access_size);\n\t\treturn -EACCES;\n\t}\n\n\tif (env->prog->aux->stack_depth < -off)\n\t\tenv->prog->aux->stack_depth = -off;\n\n\tif (meta && meta->raw_mode) {\n\t\tmeta->access_size = access_size;\n\t\tmeta->regno = regno;\n\t\treturn 0;\n\t}\n\n\tfor (i = 0; i < access_size; i++) {\n\t\tslot = -(off + i) - 1;\n\t\tspi = slot / BPF_REG_SIZE;\n\t\tif (state->allocated_stack <= slot ||\n\t\t    state->stack[spi].slot_type[slot % BPF_REG_SIZE] !=\n\t\t\tSTACK_MISC) {\n\t\t\tverbose(env, \"invalid indirect read from stack off %d+%d size %d\\n\",\n\t\t\t\toff, i, access_size);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_VALUE:\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tdefault: /* scalar_value|ptr_to_stack or invalid ptr */\n\t\treturn check_stack_boundary(env, regno, access_size,\n\t\t\t\t\t    zero_size_allowed, meta);\n\t}\n}\n\nstatic int check_func_arg(struct bpf_verifier_env *env, u32 regno,\n\t\t\t  enum bpf_arg_type arg_type,\n\t\t\t  struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\tenum bpf_reg_type expected_type, type = reg->type;\n\tint err = 0;\n\n\tif (arg_type == ARG_DONTCARE)\n\t\treturn 0;\n\n\terr = check_reg_arg(env, regno, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (arg_type == ARG_ANYTHING) {\n\t\tif (is_pointer_value(env, regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into helper function\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (type_is_pkt_pointer(type) &&\n\t    !may_access_direct_pkt_data(env, meta, BPF_READ)) {\n\t\tverbose(env, \"helper access to the packet is not allowed\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tif (arg_type == ARG_PTR_TO_MAP_KEY ||\n\t    arg_type == ARG_PTR_TO_MAP_VALUE) {\n\t\texpected_type = PTR_TO_STACK;\n\t\tif (!type_is_pkt_pointer(type) &&\n\t\t    type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_CONST_SIZE ||\n\t\t   arg_type == ARG_CONST_SIZE_OR_ZERO) {\n\t\texpected_type = SCALAR_VALUE;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_CONST_MAP_PTR) {\n\t\texpected_type = CONST_PTR_TO_MAP;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_PTR_TO_CTX) {\n\t\texpected_type = PTR_TO_CTX;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_PTR_TO_MEM ||\n\t\t   arg_type == ARG_PTR_TO_MEM_OR_NULL ||\n\t\t   arg_type == ARG_PTR_TO_UNINIT_MEM) {\n\t\texpected_type = PTR_TO_STACK;\n\t\t/* One exception here. In case function allows for NULL to be\n\t\t * passed in as argument, it's a SCALAR_VALUE type. Final test\n\t\t * happens during stack boundary checking.\n\t\t */\n\t\tif (register_is_null(*reg) &&\n\t\t    arg_type == ARG_PTR_TO_MEM_OR_NULL)\n\t\t\t/* final test in check_stack_boundary() */;\n\t\telse if (!type_is_pkt_pointer(type) &&\n\t\t\t type != PTR_TO_MAP_VALUE &&\n\t\t\t type != expected_type)\n\t\t\tgoto err_type;\n\t\tmeta->raw_mode = arg_type == ARG_PTR_TO_UNINIT_MEM;\n\t} else {\n\t\tverbose(env, \"unsupported arg_type %d\\n\", arg_type);\n\t\treturn -EFAULT;\n\t}\n\n\tif (arg_type == ARG_CONST_MAP_PTR) {\n\t\t/* bpf_map_xxx(map_ptr) call: remember that map_ptr */\n\t\tmeta->map_ptr = reg->map_ptr;\n\t} else if (arg_type == ARG_PTR_TO_MAP_KEY) {\n\t\t/* bpf_map_xxx(..., map_ptr, ..., key) call:\n\t\t * check that [key, key + map->key_size) are within\n\t\t * stack limits and initialized\n\t\t */\n\t\tif (!meta->map_ptr) {\n\t\t\t/* in function declaration map_ptr must come before\n\t\t\t * map_key, so that it's verified and known before\n\t\t\t * we have to check map_key here. Otherwise it means\n\t\t\t * that kernel subsystem misconfigured verifier\n\t\t\t */\n\t\t\tverbose(env, \"invalid map_ptr to access map->key\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (type_is_pkt_pointer(type))\n\t\t\terr = check_packet_access(env, regno, reg->off,\n\t\t\t\t\t\t  meta->map_ptr->key_size,\n\t\t\t\t\t\t  false);\n\t\telse\n\t\t\terr = check_stack_boundary(env, regno,\n\t\t\t\t\t\t   meta->map_ptr->key_size,\n\t\t\t\t\t\t   false, NULL);\n\t} else if (arg_type == ARG_PTR_TO_MAP_VALUE) {\n\t\t/* bpf_map_xxx(..., map_ptr, ..., value) call:\n\t\t * check [value, value + map->value_size) validity\n\t\t */\n\t\tif (!meta->map_ptr) {\n\t\t\t/* kernel subsystem misconfigured verifier */\n\t\t\tverbose(env, \"invalid map_ptr to access map->value\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (type_is_pkt_pointer(type))\n\t\t\terr = check_packet_access(env, regno, reg->off,\n\t\t\t\t\t\t  meta->map_ptr->value_size,\n\t\t\t\t\t\t  false);\n\t\telse\n\t\t\terr = check_stack_boundary(env, regno,\n\t\t\t\t\t\t   meta->map_ptr->value_size,\n\t\t\t\t\t\t   false, NULL);\n\t} else if (arg_type == ARG_CONST_SIZE ||\n\t\t   arg_type == ARG_CONST_SIZE_OR_ZERO) {\n\t\tbool zero_size_allowed = (arg_type == ARG_CONST_SIZE_OR_ZERO);\n\n\t\t/* bpf_xxx(..., buf, len) call will access 'len' bytes\n\t\t * from stack pointer 'buf'. Check it\n\t\t * note: regno == len, regno - 1 == buf\n\t\t */\n\t\tif (regno == 0) {\n\t\t\t/* kernel subsystem misconfigured verifier */\n\t\t\tverbose(env,\n\t\t\t\t\"ARG_CONST_SIZE cannot be first argument\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* The register is SCALAR_VALUE; the access check\n\t\t * happens using its boundaries.\n\t\t */\n\n\t\tif (!tnum_is_const(reg->var_off))\n\t\t\t/* For unprivileged variable accesses, disable raw\n\t\t\t * mode so that the program is required to\n\t\t\t * initialize all the memory that the helper could\n\t\t\t * just partially fill up.\n\t\t\t */\n\t\t\tmeta = NULL;\n\n\t\tif (reg->smin_value < 0) {\n\t\t\tverbose(env, \"R%d min value is negative, either use unsigned or 'var &= const'\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (reg->umin_value == 0) {\n\t\t\terr = check_helper_mem_access(env, regno - 1, 0,\n\t\t\t\t\t\t      zero_size_allowed,\n\t\t\t\t\t\t      meta);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tif (reg->umax_value >= BPF_MAX_VAR_SIZ) {\n\t\t\tverbose(env, \"R%d unbounded memory access, use 'var &= const' or 'if (var < const)'\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_helper_mem_access(env, regno - 1,\n\t\t\t\t\t      reg->umax_value,\n\t\t\t\t\t      zero_size_allowed, meta);\n\t}\n\n\treturn err;\nerr_type:\n\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\treg_type_str[type], reg_type_str[expected_type]);\n\treturn -EACCES;\n}\n\nstatic int check_map_func_compatibility(struct bpf_verifier_env *env,\n\t\t\t\t\tstruct bpf_map *map, int func_id)\n{\n\tif (!map)\n\t\treturn 0;\n\n\t/* We need a two way check, first is from map perspective ... */\n\tswitch (map->map_type) {\n\tcase BPF_MAP_TYPE_PROG_ARRAY:\n\t\tif (func_id != BPF_FUNC_tail_call)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_PERF_EVENT_ARRAY:\n\t\tif (func_id != BPF_FUNC_perf_event_read &&\n\t\t    func_id != BPF_FUNC_perf_event_output &&\n\t\t    func_id != BPF_FUNC_perf_event_read_value)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_STACK_TRACE:\n\t\tif (func_id != BPF_FUNC_get_stackid)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_CGROUP_ARRAY:\n\t\tif (func_id != BPF_FUNC_skb_under_cgroup &&\n\t\t    func_id != BPF_FUNC_current_task_under_cgroup)\n\t\t\tgoto error;\n\t\tbreak;\n\t/* devmap returns a pointer to a live net_device ifindex that we cannot\n\t * allow to be modified from bpf side. So do not allow lookup elements\n\t * for now.\n\t */\n\tcase BPF_MAP_TYPE_DEVMAP:\n\t\tif (func_id != BPF_FUNC_redirect_map)\n\t\t\tgoto error;\n\t\tbreak;\n\t/* Restrict bpf side of cpumap, open when use-cases appear */\n\tcase BPF_MAP_TYPE_CPUMAP:\n\t\tif (func_id != BPF_FUNC_redirect_map)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_ARRAY_OF_MAPS:\n\tcase BPF_MAP_TYPE_HASH_OF_MAPS:\n\t\tif (func_id != BPF_FUNC_map_lookup_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_SOCKMAP:\n\t\tif (func_id != BPF_FUNC_sk_redirect_map &&\n\t\t    func_id != BPF_FUNC_sock_map_update &&\n\t\t    func_id != BPF_FUNC_map_delete_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* ... and second from the function itself. */\n\tswitch (func_id) {\n\tcase BPF_FUNC_tail_call:\n\t\tif (map->map_type != BPF_MAP_TYPE_PROG_ARRAY)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_perf_event_read:\n\tcase BPF_FUNC_perf_event_output:\n\tcase BPF_FUNC_perf_event_read_value:\n\t\tif (map->map_type != BPF_MAP_TYPE_PERF_EVENT_ARRAY)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_get_stackid:\n\t\tif (map->map_type != BPF_MAP_TYPE_STACK_TRACE)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_current_task_under_cgroup:\n\tcase BPF_FUNC_skb_under_cgroup:\n\t\tif (map->map_type != BPF_MAP_TYPE_CGROUP_ARRAY)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_redirect_map:\n\t\tif (map->map_type != BPF_MAP_TYPE_DEVMAP &&\n\t\t    map->map_type != BPF_MAP_TYPE_CPUMAP)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_redirect_map:\n\t\tif (map->map_type != BPF_MAP_TYPE_SOCKMAP)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sock_map_update:\n\t\tif (map->map_type != BPF_MAP_TYPE_SOCKMAP)\n\t\t\tgoto error;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\nerror:\n\tverbose(env, \"cannot pass map_type %d into func %s#%d\\n\",\n\t\tmap->map_type, func_id_name(func_id), func_id);\n\treturn -EINVAL;\n}\n\nstatic int check_raw_mode(const struct bpf_func_proto *fn)\n{\n\tint count = 0;\n\n\tif (fn->arg1_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg2_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg3_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg4_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg5_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\n\treturn count > 1 ? -EINVAL : 0;\n}\n\n/* Packet data might have moved, any old PTR_TO_PACKET[_META,_END]\n * are now invalid, so turn them into unknown SCALAR_VALUE.\n */\nstatic void clear_all_pkt_pointers(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (reg_is_pkt_pointer_any(&regs[i]))\n\t\t\tmark_reg_unknown(env, regs, i);\n\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tif (state->stack[i].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\treg = &state->stack[i].spilled_ptr;\n\t\tif (reg_is_pkt_pointer_any(reg))\n\t\t\t__mark_reg_unknown(reg);\n\t}\n}\n\nstatic int check_call(struct bpf_verifier_env *env, int func_id, int insn_idx)\n{\n\tconst struct bpf_func_proto *fn = NULL;\n\tstruct bpf_reg_state *regs;\n\tstruct bpf_call_arg_meta meta;\n\tbool changes_data;\n\tint i, err;\n\n\t/* find function prototype */\n\tif (func_id < 0 || func_id >= __BPF_FUNC_MAX_ID) {\n\t\tverbose(env, \"invalid func %s#%d\\n\", func_id_name(func_id),\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->ops->get_func_proto)\n\t\tfn = env->ops->get_func_proto(func_id);\n\n\tif (!fn) {\n\t\tverbose(env, \"unknown func %s#%d\\n\", func_id_name(func_id),\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\t/* eBPF programs must be GPL compatible to use GPL-ed functions */\n\tif (!env->prog->gpl_compatible && fn->gpl_only) {\n\t\tverbose(env, \"cannot call GPL only function from proprietary program\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* With LD_ABS/IND some JITs save/restore skb from r1. */\n\tchanges_data = bpf_helper_changes_pkt_data(fn->func);\n\tif (changes_data && fn->arg1_type != ARG_PTR_TO_CTX) {\n\t\tverbose(env, \"kernel subsystem misconfigured func %s#%d: r1 != ctx\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn -EINVAL;\n\t}\n\n\tmemset(&meta, 0, sizeof(meta));\n\tmeta.pkt_access = fn->pkt_access;\n\n\t/* We only support one arg being in raw mode at the moment, which\n\t * is sufficient for the helper functions we have right now.\n\t */\n\terr = check_raw_mode(fn);\n\tif (err) {\n\t\tverbose(env, \"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn err;\n\t}\n\n\t/* check args */\n\terr = check_func_arg(env, BPF_REG_1, fn->arg1_type, &meta);\n\tif (err)\n\t\treturn err;\n\terr = check_func_arg(env, BPF_REG_2, fn->arg2_type, &meta);\n\tif (err)\n\t\treturn err;\n\terr = check_func_arg(env, BPF_REG_3, fn->arg3_type, &meta);\n\tif (err)\n\t\treturn err;\n\terr = check_func_arg(env, BPF_REG_4, fn->arg4_type, &meta);\n\tif (err)\n\t\treturn err;\n\terr = check_func_arg(env, BPF_REG_5, fn->arg5_type, &meta);\n\tif (err)\n\t\treturn err;\n\n\t/* Mark slots with STACK_MISC in case of raw mode, stack offset\n\t * is inferred from register state.\n\t */\n\tfor (i = 0; i < meta.access_size; i++) {\n\t\terr = check_mem_access(env, insn_idx, meta.regno, i, BPF_B, BPF_WRITE, -1);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tregs = cur_regs(env);\n\t/* reset caller saved regs */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* update return register (already marked as written above) */\n\tif (fn->ret_type == RET_INTEGER) {\n\t\t/* sets type to SCALAR_VALUE */\n\t\tmark_reg_unknown(env, regs, BPF_REG_0);\n\t} else if (fn->ret_type == RET_VOID) {\n\t\tregs[BPF_REG_0].type = NOT_INIT;\n\t} else if (fn->ret_type == RET_PTR_TO_MAP_VALUE_OR_NULL) {\n\t\tstruct bpf_insn_aux_data *insn_aux;\n\n\t\tregs[BPF_REG_0].type = PTR_TO_MAP_VALUE_OR_NULL;\n\t\t/* There is no offset yet applied, variable or fixed */\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].off = 0;\n\t\t/* remember map_ptr, so that check_map_access()\n\t\t * can check 'value_size' boundary of memory access\n\t\t * to map element returned from bpf_map_lookup_elem()\n\t\t */\n\t\tif (meta.map_ptr == NULL) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured verifier\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tregs[BPF_REG_0].map_ptr = meta.map_ptr;\n\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t\tinsn_aux = &env->insn_aux_data[insn_idx];\n\t\tif (!insn_aux->map_ptr)\n\t\t\tinsn_aux->map_ptr = meta.map_ptr;\n\t\telse if (insn_aux->map_ptr != meta.map_ptr)\n\t\t\tinsn_aux->map_ptr = BPF_MAP_PTR_POISON;\n\t} else {\n\t\tverbose(env, \"unknown return type %d of func %s#%d\\n\",\n\t\t\tfn->ret_type, func_id_name(func_id), func_id);\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_map_func_compatibility(env, meta.map_ptr, func_id);\n\tif (err)\n\t\treturn err;\n\n\tif (changes_data)\n\t\tclear_all_pkt_pointers(env);\n\treturn 0;\n}\n\nstatic bool signed_add_overflows(s64 a, s64 b)\n{\n\t/* Do the add in u64, where overflow is well-defined */\n\ts64 res = (s64)((u64)a + (u64)b);\n\n\tif (b < 0)\n\t\treturn res > a;\n\treturn res < a;\n}\n\nstatic bool signed_sub_overflows(s64 a, s64 b)\n{\n\t/* Do the sub in u64, where overflow is well-defined */\n\ts64 res = (s64)((u64)a - (u64)b);\n\n\tif (b < 0)\n\t\treturn res < a;\n\treturn res > a;\n}\n\n/* Handles arithmetic on a pointer and a scalar: computes new min/max and var_off.\n * Caller should also handle BPF_MOV case separately.\n * If we return -EACCES, caller may want to try again treating pointer as a\n * scalar.  So we only emit a diagnostic if !env->allow_ptr_leaks.\n */\nstatic int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\n\tdst_reg = &regs[dst];\n\n\tif (WARN_ON_ONCE(known && (smin_val != smax_val))) {\n\t\tprint_verifier_state(env, env->cur_state);\n\t\tverbose(env,\n\t\t\t\"verifier internal error: known but bad sbounds\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON_ONCE(known && (umin_val != umax_val))) {\n\t\tprint_verifier_state(env, env->cur_state);\n\t\tverbose(env,\n\t\t\t\"verifier internal error: known but bad ubounds\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (!env->allow_ptr_leaks)\n\t\t\tverbose(env,\n\t\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tif (ptr_reg->type == PTR_TO_MAP_VALUE_OR_NULL) {\n\t\tif (!env->allow_ptr_leaks)\n\t\t\tverbose(env, \"R%d pointer arithmetic on PTR_TO_MAP_VALUE_OR_NULL prohibited, null-check it first\\n\",\n\t\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\tif (ptr_reg->type == CONST_PTR_TO_MAP) {\n\t\tif (!env->allow_ptr_leaks)\n\t\t\tverbose(env, \"R%d pointer arithmetic on CONST_PTR_TO_MAP prohibited\\n\",\n\t\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\tif (ptr_reg->type == PTR_TO_PACKET_END) {\n\t\tif (!env->allow_ptr_leaks)\n\t\t\tverbose(env, \"R%d pointer arithmetic on PTR_TO_PACKET_END prohibited\\n\",\n\t\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->range = ptr_reg->range;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tdst_reg->range = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tif (!env->allow_ptr_leaks)\n\t\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tif (!env->allow_ptr_leaks)\n\t\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->range = ptr_reg->range;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tdst_reg->range = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit for now.\n\t\t * (However, in principle we could allow some cases, e.g.\n\t\t * ptr &= ~3 which would reduce min_value by 3.)\n\t\t */\n\t\tif (!env->allow_ptr_leaks)\n\t\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tif (!env->allow_ptr_leaks)\n\t\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}\n\n/* WARNING: This function does calculations on 64-bit values, but the actual\n * execution may occur on 32-bit values. Therefore, things like bitshifts\n * need extra checks in the 32-bit case.\n */\nstatic int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known, dst_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\n\tsmin_val = src_reg.smin_value;\n\tsmax_val = src_reg.smax_value;\n\tumin_val = src_reg.umin_value;\n\tumax_val = src_reg.umax_value;\n\tsrc_known = tnum_is_const(src_reg.var_off);\n\tdst_known = tnum_is_const(dst_reg->var_off);\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n\t\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value += smin_val;\n\t\t\tdst_reg->smax_value += smax_val;\n\t\t}\n\t\tif (dst_reg->umin_value + umin_val < umin_val ||\n\t\t    dst_reg->umax_value + umax_val < umax_val) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value += umin_val;\n\t\t\tdst_reg->umax_value += umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n\t\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value -= smax_val;\n\t\t\tdst_reg->smax_value -= smin_val;\n\t\t}\n\t\tif (dst_reg->umin_value < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value -= umax_val;\n\t\t\tdst_reg->umax_value -= umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_MUL:\n\t\tdst_reg->var_off = tnum_mul(dst_reg->var_off, src_reg.var_off);\n\t\tif (smin_val < 0 || dst_reg->smin_value < 0) {\n\t\t\t/* Ain't nobody got time to multiply that sign */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* Both values are positive, so we can work with unsigned and\n\t\t * copy the result to signed (unless it exceeds S64_MAX).\n\t\t */\n\t\tif (umax_val > U32_MAX || dst_reg->umax_value > U32_MAX) {\n\t\t\t/* Potential overflow, we know nothing */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t/* (except what we can learn from the var_off) */\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tdst_reg->umin_value *= umin_val;\n\t\tdst_reg->umax_value *= umax_val;\n\t\tif (dst_reg->umax_value > S64_MAX) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value &\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our minimum from the var_off, since that's inherently\n\t\t * bitwise.  Our maximum is the minimum of the operands' maxima.\n\t\t */\n\t\tdst_reg->var_off = tnum_and(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = dst_reg->var_off.value;\n\t\tdst_reg->umax_value = min(dst_reg->umax_value, umax_val);\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ANDing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ANDing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_OR:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value |\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our maximum from the var_off, and our minimum is the\n\t\t * maximum of the operands' minima\n\t\t */\n\t\tdst_reg->var_off = tnum_or(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = max(dst_reg->umin_value, umin_val);\n\t\tdst_reg->umax_value = dst_reg->var_off.value |\n\t\t\t\t      dst_reg->var_off.mask;\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ORing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_LSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* We lose all sign bit information (except what we can pick\n\t\t * up from var_off)\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\t/* If we might shift our top bit out, then we know nothing */\n\t\tif (dst_reg->umax_value > 1ULL << (63 - umax_val)) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value <<= umin_val;\n\t\t\tdst_reg->umax_value <<= umax_val;\n\t\t}\n\t\tif (src_known)\n\t\t\tdst_reg->var_off = tnum_lshift(dst_reg->var_off, umin_val);\n\t\telse\n\t\t\tdst_reg->var_off = tnum_lshift(tnum_unknown, umin_val);\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_RSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* BPF_RSH is an unsigned shift.  If the value in dst_reg might\n\t\t * be negative, then either:\n\t\t * 1) src_reg might be zero, so the sign bit of the result is\n\t\t *    unknown, so we lose our signed bounds\n\t\t * 2) it's known negative, thus the unsigned bounds capture the\n\t\t *    signed bounds\n\t\t * 3) the signed bounds cross zero, so they tell us nothing\n\t\t *    about the result\n\t\t * If the value in dst_reg is known nonnegative, then again the\n\t\t * unsigned bounts capture the signed bounds.\n\t\t * Thus, in all cases it suffices to blow away our signed bounds\n\t\t * and rely on inferring new ones from the unsigned bounds and\n\t\t * var_off of the result.\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\tif (src_known)\n\t\t\tdst_reg->var_off = tnum_rshift(dst_reg->var_off,\n\t\t\t\t\t\t       umin_val);\n\t\telse\n\t\t\tdst_reg->var_off = tnum_rshift(tnum_unknown, umin_val);\n\t\tdst_reg->umin_value >>= umax_val;\n\t\tdst_reg->umax_value >>= umin_val;\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tdefault:\n\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\tbreak;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops are (32,32)->32 */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\tcoerce_reg_to_size(&src_reg, 4);\n\t}\n\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}\n\n/* Handles ALU ops other than BPF_END, BPF_NEG and BPF_MOV: computes new min/max\n * and var_off.\n */\nstatic int adjust_reg_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *dst_reg, *src_reg;\n\tstruct bpf_reg_state *ptr_reg = NULL, off_reg = {0};\n\tu8 opcode = BPF_OP(insn->code);\n\tint rc;\n\n\tdst_reg = &regs[insn->dst_reg];\n\tsrc_reg = NULL;\n\tif (dst_reg->type != SCALAR_VALUE)\n\t\tptr_reg = dst_reg;\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (src_reg->type != SCALAR_VALUE) {\n\t\t\tif (dst_reg->type != SCALAR_VALUE) {\n\t\t\t\t/* Combining two pointers by any ALU op yields\n\t\t\t\t * an arbitrary scalar.\n\t\t\t\t */\n\t\t\t\tif (!env->allow_ptr_leaks) {\n\t\t\t\t\tverbose(env, \"R%d pointer %s pointer prohibited\\n\",\n\t\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\t\tbpf_alu_string[opcode >> 4]);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\treturn 0;\n\t\t\t} else {\n\t\t\t\t/* scalar += pointer\n\t\t\t\t * This is legal, but we have to reverse our\n\t\t\t\t * src/dest handling in computing the range\n\t\t\t\t */\n\t\t\t\trc = adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t\t     src_reg, dst_reg);\n\t\t\t\tif (rc == -EACCES && env->allow_ptr_leaks) {\n\t\t\t\t\t/* scalar += unknown scalar */\n\t\t\t\t\t__mark_reg_unknown(&off_reg);\n\t\t\t\t\treturn adjust_scalar_min_max_vals(\n\t\t\t\t\t\t\tenv, insn,\n\t\t\t\t\t\t\tdst_reg, off_reg);\n\t\t\t\t}\n\t\t\t\treturn rc;\n\t\t\t}\n\t\t} else if (ptr_reg) {\n\t\t\t/* pointer += scalar */\n\t\t\trc = adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t     dst_reg, src_reg);\n\t\t\tif (rc == -EACCES && env->allow_ptr_leaks) {\n\t\t\t\t/* unknown scalar += scalar */\n\t\t\t\t__mark_reg_unknown(dst_reg);\n\t\t\t\treturn adjust_scalar_min_max_vals(\n\t\t\t\t\t\tenv, insn, dst_reg, *src_reg);\n\t\t\t}\n\t\t\treturn rc;\n\t\t}\n\t} else {\n\t\t/* Pretend the src is a reg with a known value, since we only\n\t\t * need to be able to read from this state.\n\t\t */\n\t\toff_reg.type = SCALAR_VALUE;\n\t\t__mark_reg_known(&off_reg, insn->imm);\n\t\tsrc_reg = &off_reg;\n\t\tif (ptr_reg) { /* pointer += K */\n\t\t\trc = adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t     ptr_reg, src_reg);\n\t\t\tif (rc == -EACCES && env->allow_ptr_leaks) {\n\t\t\t\t/* unknown scalar += K */\n\t\t\t\t__mark_reg_unknown(dst_reg);\n\t\t\t\treturn adjust_scalar_min_max_vals(\n\t\t\t\t\t\tenv, insn, dst_reg, off_reg);\n\t\t\t}\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\t/* Got here implies adding two SCALAR_VALUEs */\n\tif (WARN_ON_ONCE(ptr_reg)) {\n\t\tprint_verifier_state(env, env->cur_state);\n\t\tverbose(env, \"verifier internal error: unexpected ptr_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON(!src_reg)) {\n\t\tprint_verifier_state(env, env->cur_state);\n\t\tverbose(env, \"verifier internal error: no src_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n}\n\n/* check validity of 32-bit and 64-bit arithmetic operations */\nstatic int check_alu_op(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode == BPF_END || opcode == BPF_NEG) {\n\t\tif (opcode == BPF_NEG) {\n\t\t\tif (BPF_SRC(insn->code) != 0 ||\n\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t    insn->off != 0 || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_NEG uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0 ||\n\t\t\t    (insn->imm != 16 && insn->imm != 32 && insn->imm != 64) ||\n\t\t\t    BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\tverbose(env, \"BPF_END uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->dst_reg)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic prohibited\\n\",\n\t\t\t\tinsn->dst_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t} else if (opcode == BPF_MOV) {\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t/* case: R1 = R2\n\t\t\t\t * copy register state to dest reg\n\t\t\t\t */\n\t\t\t\tregs[insn->dst_reg] = regs[insn->src_reg];\n\t\t\t\tregs[insn->dst_reg].live |= REG_LIVE_WRITTEN;\n\t\t\t} else {\n\t\t\t\t/* R1 = (u32) R2 */\n\t\t\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"R%d partial copy of pointer\\n\",\n\t\t\t\t\t\tinsn->src_reg);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\tcoerce_reg_to_size(&regs[insn->dst_reg], 4);\n\t\t\t}\n\t\t} else {\n\t\t\t/* case: R = imm\n\t\t\t * remember the value we stored into this reg\n\t\t\t */\n\t\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t insn->imm);\n\t\t\t} else {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t (u32)insn->imm);\n\t\t\t}\n\t\t}\n\n\t} else if (opcode > BPF_END) {\n\t\tverbose(env, \"invalid BPF_ALU opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\n\t} else {\t/* all other ALU ops: and, sub, xor, add, ... */\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src2 operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif ((opcode == BPF_MOD || opcode == BPF_DIV) &&\n\t\t    BPF_SRC(insn->code) == BPF_K && insn->imm == 0) {\n\t\t\tverbose(env, \"div by zero\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((opcode == BPF_LSH || opcode == BPF_RSH ||\n\t\t     opcode == BPF_ARSH) && BPF_SRC(insn->code) == BPF_K) {\n\t\t\tint size = BPF_CLASS(insn->code) == BPF_ALU64 ? 64 : 32;\n\n\t\t\tif (insn->imm < 0 || insn->imm >= size) {\n\t\t\t\tverbose(env, \"invalid shift %d\\n\", insn->imm);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\treturn adjust_reg_min_max_vals(env, insn);\n\t}\n\n\treturn 0;\n}\n\nstatic void find_good_pkt_pointers(struct bpf_verifier_state *state,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   enum bpf_reg_type type,\n\t\t\t\t   bool range_right_open)\n{\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tu16 new_range;\n\tint i;\n\n\tif (dst_reg->off < 0 ||\n\t    (dst_reg->off == 0 && range_right_open))\n\t\t/* This doesn't give us any range */\n\t\treturn;\n\n\tif (dst_reg->umax_value > MAX_PACKET_OFF ||\n\t    dst_reg->umax_value + dst_reg->off > MAX_PACKET_OFF)\n\t\t/* Risk of overflow.  For instance, ptr + (1<<63) may be less\n\t\t * than pkt_end, but that's because it's also less than pkt.\n\t\t */\n\t\treturn;\n\n\tnew_range = dst_reg->off;\n\tif (range_right_open)\n\t\tnew_range--;\n\n\t/* Examples for register markings:\n\t *\n\t * pkt_data in dst register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 > pkt_end) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 < pkt_end) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   Where:\n\t *     r2 == dst_reg, pkt_end == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * pkt_data in src register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end >= r2) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end <= r2) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   Where:\n\t *     pkt_end == dst_reg, r2 == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * Find register r3 and mark its range as r3=pkt(id=n,off=0,r=8)\n\t * or r3=pkt(id=n,off=0,r=8-1), so that range of bytes [r3, r3 + 8)\n\t * and [r3, r3 + 8-1) respectively is safe to access depending on\n\t * the check.\n\t */\n\n\t/* If our ids match, then we must have the same max_value.  And we\n\t * don't care about the other reg's fixed offset, since if it's too big\n\t * the range won't allow anything.\n\t * dst_reg->off is known < MAX_PACKET_OFF, therefore it fits in a u16.\n\t */\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (regs[i].type == type && regs[i].id == dst_reg->id)\n\t\t\t/* keep the maximum range already checked */\n\t\t\tregs[i].range = max(regs[i].range, new_range);\n\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tif (state->stack[i].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\treg = &state->stack[i].spilled_ptr;\n\t\tif (reg->type == type && reg->id == dst_reg->id)\n\t\t\treg->range = max(reg->range, new_range);\n\t}\n}\n\n/* Adjusts the register min/max values in the case that the dst_reg is the\n * variable register that we are working on, and src_reg is a constant or we're\n * simply doing a BPF_K check.\n * In JEQ/JNE cases we also adjust the var_off values.\n */\nstatic void reg_set_min_max(struct bpf_reg_state *true_reg,\n\t\t\t    struct bpf_reg_state *false_reg, u64 val,\n\t\t\t    u8 opcode)\n{\n\t/* If the dst_reg is a pointer, we can't learn anything about its\n\t * variable offset from the compare (unless src_reg were a pointer into\n\t * the same object, but we don't bother with that.\n\t * Since false_reg and true_reg have the same type by construction, we\n\t * only need to check one of them for pointerness.\n\t */\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Same as above, but for the case that dst_reg holds a constant and src_reg is\n * the variable reg.\n */\nstatic void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\n\t\t\t\tstruct bpf_reg_state *false_reg, u64 val,\n\t\t\t\tu8 opcode)\n{\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JGT:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Regs are known to be equal, so intersect their min/max/var_off */\nstatic void __reg_combine_min_max(struct bpf_reg_state *src_reg,\n\t\t\t\t  struct bpf_reg_state *dst_reg)\n{\n\tsrc_reg->umin_value = dst_reg->umin_value = max(src_reg->umin_value,\n\t\t\t\t\t\t\tdst_reg->umin_value);\n\tsrc_reg->umax_value = dst_reg->umax_value = min(src_reg->umax_value,\n\t\t\t\t\t\t\tdst_reg->umax_value);\n\tsrc_reg->smin_value = dst_reg->smin_value = max(src_reg->smin_value,\n\t\t\t\t\t\t\tdst_reg->smin_value);\n\tsrc_reg->smax_value = dst_reg->smax_value = min(src_reg->smax_value,\n\t\t\t\t\t\t\tdst_reg->smax_value);\n\tsrc_reg->var_off = dst_reg->var_off = tnum_intersect(src_reg->var_off,\n\t\t\t\t\t\t\t     dst_reg->var_off);\n\t/* We might have learned new bounds from the var_off. */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n\t/* We might have learned something about the sign bit. */\n\t__reg_deduce_bounds(src_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(src_reg);\n\t__reg_bound_offset(dst_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void reg_combine_min_max(struct bpf_reg_state *true_src,\n\t\t\t\tstruct bpf_reg_state *true_dst,\n\t\t\t\tstruct bpf_reg_state *false_src,\n\t\t\t\tstruct bpf_reg_state *false_dst,\n\t\t\t\tu8 opcode)\n{\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t__reg_combine_min_max(true_src, true_dst);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t__reg_combine_min_max(false_src, false_dst);\n\t\tbreak;\n\t}\n}\n\nstatic void mark_map_reg(struct bpf_reg_state *regs, u32 regno, u32 id,\n\t\t\t bool is_null)\n{\n\tstruct bpf_reg_state *reg = &regs[regno];\n\n\tif (reg->type == PTR_TO_MAP_VALUE_OR_NULL && reg->id == id) {\n\t\t/* Old offset (both fixed and variable parts) should\n\t\t * have been known-zero, because we don't allow pointer\n\t\t * arithmetic on pointers that might be NULL.\n\t\t */\n\t\tif (WARN_ON_ONCE(reg->smin_value || reg->smax_value ||\n\t\t\t\t !tnum_equals_const(reg->var_off, 0) ||\n\t\t\t\t reg->off)) {\n\t\t\t__mark_reg_known_zero(reg);\n\t\t\treg->off = 0;\n\t\t}\n\t\tif (is_null) {\n\t\t\treg->type = SCALAR_VALUE;\n\t\t} else if (reg->map_ptr->inner_map_meta) {\n\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\treg->map_ptr = reg->map_ptr->inner_map_meta;\n\t\t} else {\n\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t}\n\t\t/* We don't need id from this point onwards anymore, thus we\n\t\t * should better reset it, so that state pruning has chances\n\t\t * to take effect.\n\t\t */\n\t\treg->id = 0;\n\t}\n}\n\n/* The logic is similar to find_good_pkt_pointers(), both could eventually\n * be folded together at some point.\n */\nstatic void mark_map_regs(struct bpf_verifier_state *state, u32 regno,\n\t\t\t  bool is_null)\n{\n\tstruct bpf_reg_state *regs = state->regs;\n\tu32 id = regs[regno].id;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tmark_map_reg(regs, i, id, is_null);\n\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tif (state->stack[i].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\tmark_map_reg(&state->stack[i].spilled_ptr, 0, id, is_null);\n\t}\n}\n\nstatic bool try_match_pkt_pointers(const struct bpf_insn *insn,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   struct bpf_reg_state *src_reg,\n\t\t\t\t   struct bpf_verifier_state *this_branch,\n\t\t\t\t   struct bpf_verifier_state *other_branch)\n{\n\tif (BPF_SRC(insn->code) != BPF_X)\n\t\treturn false;\n\n\tswitch (BPF_OP(insn->code)) {\n\tcase BPF_JGT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' > pkt_end, pkt_meta' > pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end > pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' < pkt_end, pkt_meta' < pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end < pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' >= pkt_end, pkt_meta' >= pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end >= pkt_data', pkt_data >= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' <= pkt_end, pkt_meta' <= pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end <= pkt_data', pkt_data <= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *other_branch, *this_branch = env->cur_state;\n\tstruct bpf_reg_state *regs = this_branch->regs, *dst_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\n\t/* detect if R == 0 where R was initialized to zero earlier */\n\tif (BPF_SRC(insn->code) == BPF_K &&\n\t    (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    dst_reg->type == SCALAR_VALUE &&\n\t    tnum_equals_const(dst_reg->var_off, insn->imm)) {\n\t\tif (opcode == BPF_JEQ) {\n\t\t\t/* if (imm == imm) goto pc+off;\n\t\t\t * only follow the goto, ignore fall-through\n\t\t\t */\n\t\t\t*insn_idx += insn->off;\n\t\t\treturn 0;\n\t\t} else {\n\t\t\t/* if (imm != imm) goto pc+off;\n\t\t\t * only follow fall-through branch, since\n\t\t\t * that's where the program will go\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    regs[insn->src_reg].type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(regs[insn->src_reg].var_off))\n\t\t\t\treg_set_min_max(&other_branch->regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg, regs[insn->src_reg].var_off.value,\n\t\t\t\t\t\topcode);\n\t\t\telse if (tnum_is_const(dst_reg->var_off))\n\t\t\t\treg_set_min_max_inv(&other_branch->regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    dst_reg->var_off.value, opcode);\n\t\t\telse if (opcode == BPF_JEQ || opcode == BPF_JNE)\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch->regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch->regs[insn->dst_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->dst_reg], opcode);\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch->regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, opcode);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem() */\n\tif (BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    dst_reg->type == PTR_TO_MAP_VALUE_OR_NULL) {\n\t\t/* Mark all identical map registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_map_regs(this_branch, insn->dst_reg, opcode == BPF_JNE);\n\t\tmark_map_regs(other_branch, insn->dst_reg, opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level)\n\t\tprint_verifier_state(env, this_branch);\n\treturn 0;\n}\n\n/* return the map pointer stored inside BPF_LD_IMM64 instruction */\nstatic struct bpf_map *ld_imm64_to_map_ptr(struct bpf_insn *insn)\n{\n\tu64 imm64 = ((u64) (u32) insn[0].imm) | ((u64) (u32) insn[1].imm) << 32;\n\n\treturn (struct bpf_map *) (unsigned long) imm64;\n}\n\n/* verify BPF_LD_IMM64 instruction */\nstatic int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tint err;\n\n\tif (BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid BPF_LD_IMM insn\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (insn->off != 0) {\n\t\tverbose(env, \"BPF_LD_IMM64 uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (insn->src_reg == 0) {\n\t\tu64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;\n\n\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t__mark_reg_known(&regs[insn->dst_reg], imm);\n\t\treturn 0;\n\t}\n\n\t/* replace_map_fd_with_map_ptr() should have caught bad ld_imm64 */\n\tBUG_ON(insn->src_reg != BPF_PSEUDO_MAP_FD);\n\n\tregs[insn->dst_reg].type = CONST_PTR_TO_MAP;\n\tregs[insn->dst_reg].map_ptr = ld_imm64_to_map_ptr(insn);\n\treturn 0;\n}\n\nstatic bool may_access_skb(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* verify safety of LD_ABS|LD_IND instructions:\n * - they can only appear in the programs where ctx == skb\n * - since they are wrappers of function calls, they scratch R1-R5 registers,\n *   preserve R6-R9, and store return value into R0\n *\n * Implicit input:\n *   ctx == skb == R6 == CTX\n *\n * Explicit input:\n *   SRC == any register\n *   IMM == 32-bit immediate\n *\n * Output:\n *   R0 - 8/16/32-bit skb data converted to cpu endianness\n */\nstatic int check_ld_abs(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 mode = BPF_MODE(insn->code);\n\tint i, err;\n\n\tif (!may_access_skb(env->prog->type)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions not allowed for this program type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\n\t    BPF_SIZE(insn->code) == BPF_DW ||\n\t    (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check whether implicit source operand (register R6) is readable */\n\terr = check_reg_arg(env, BPF_REG_6, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (regs[BPF_REG_6].type != PTR_TO_CTX) {\n\t\tverbose(env,\n\t\t\t\"at the time of BPF_LD_ABS|IND R6 != pointer to skb\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mode == BPF_IND) {\n\t\t/* check explicit source operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* reset caller saved regs to unreadable */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* mark destination R0 register as readable, since it contains\n\t * the value fetched from the packet.\n\t * Already marked as written above.\n\t */\n\tmark_reg_unknown(env, regs, BPF_REG_0);\n\treturn 0;\n}\n\nstatic int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose(env, \"At program exit the register R0 \");\n\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"has value %s\", tn_buf);\n\t\t} else {\n\t\t\tverbose(env, \"has unknown scalar value\");\n\t\t}\n\t\tverbose(env, \" should have been 0 or 1\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/* non-recursive DFS pseudo code\n * 1  procedure DFS-iterative(G,v):\n * 2      label v as discovered\n * 3      let S be a stack\n * 4      S.push(v)\n * 5      while S is not empty\n * 6            t <- S.pop()\n * 7            if t is what we're looking for:\n * 8                return t\n * 9            for all edges e in G.adjacentEdges(t) do\n * 10               if edge e is already labelled\n * 11                   continue with the next edge\n * 12               w <- G.adjacentVertex(t,e)\n * 13               if vertex w is not discovered and not explored\n * 14                   label e as tree-edge\n * 15                   label w as discovered\n * 16                   S.push(w)\n * 17                   continue at 5\n * 18               else if vertex w is discovered\n * 19                   label e as back-edge\n * 20               else\n * 21                   // vertex w is explored\n * 22                   label e as forward- or cross-edge\n * 23           label t as explored\n * 24           S.pop()\n *\n * convention:\n * 0x10 - discovered\n * 0x11 - discovered and fall-through edge labelled\n * 0x12 - discovered and fall-through and branch edges labelled\n * 0x20 - explored\n */\n\nenum {\n\tDISCOVERED = 0x10,\n\tEXPLORED = 0x20,\n\tFALLTHROUGH = 1,\n\tBRANCH = 2,\n};\n\n#define STATE_LIST_MARK ((struct bpf_verifier_state_list *) -1L)\n\nstatic int *insn_stack;\t/* stack of insns to process */\nstatic int cur_stack;\t/* current stack index */\nstatic int *insn_state;\n\n/* t, w, e - match pseudo-code above:\n * t - index of current instruction\n * w - next instruction\n * e - edge\n */\nstatic int push_insn(int t, int w, int e, struct bpf_verifier_env *env)\n{\n\tif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\n\t\treturn 0;\n\n\tif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\n\t\treturn 0;\n\n\tif (w < 0 || w >= env->prog->len) {\n\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e == BRANCH)\n\t\t/* mark branch target for state pruning */\n\t\tenv->explored_states[w] = STATE_LIST_MARK;\n\n\tif (insn_state[w] == 0) {\n\t\t/* tree-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t\tinsn_state[w] = DISCOVERED;\n\t\tif (cur_stack >= env->prog->len)\n\t\t\treturn -E2BIG;\n\t\tinsn_stack[cur_stack++] = w;\n\t\treturn 1;\n\t} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\n\t\tverbose(env, \"back-edge from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t} else if (insn_state[w] == EXPLORED) {\n\t\t/* forward- or cross-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t} else {\n\t\tverbose(env, \"insn state internal bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n/* non-recursive depth-first-search to detect loops in BPF program\n * loop == back-edge in directed graph\n */\nstatic int check_cfg(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint ret = 0;\n\tint i, t;\n\n\tinsn_state = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_state)\n\t\treturn -ENOMEM;\n\n\tinsn_stack = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_stack) {\n\t\tkfree(insn_state);\n\t\treturn -ENOMEM;\n\t}\n\n\tinsn_state[0] = DISCOVERED; /* mark 1st insn as discovered */\n\tinsn_stack[0] = 0; /* 0 is the first instruction */\n\tcur_stack = 1;\n\npeek_stack:\n\tif (cur_stack == 0)\n\t\tgoto check_state;\n\tt = insn_stack[cur_stack - 1];\n\n\tif (BPF_CLASS(insns[t].code) == BPF_JMP) {\n\t\tu8 opcode = BPF_OP(insns[t].code);\n\n\t\tif (opcode == BPF_EXIT) {\n\t\t\tgoto mark_explored;\n\t\t} else if (opcode == BPF_CALL) {\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t} else if (opcode == BPF_JA) {\n\t\t\tif (BPF_SRC(insns[t].code) != BPF_K) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t\t/* unconditional jump with single edge */\n\t\t\tret = push_insn(t, t + insns[t].off + 1,\n\t\t\t\t\tFALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\t/* tell verifier to check for equivalent states\n\t\t\t * after every call and jump\n\t\t\t */\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t} else {\n\t\t\t/* conditional jump with two edges */\n\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\n\t\t\tret = push_insn(t, t + insns[t].off + 1, BRANCH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t}\n\t} else {\n\t\t/* all other non-branch instructions with single\n\t\t * fall-through edge\n\t\t */\n\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\tif (ret == 1)\n\t\t\tgoto peek_stack;\n\t\telse if (ret < 0)\n\t\t\tgoto err_free;\n\t}\n\nmark_explored:\n\tinsn_state[t] = EXPLORED;\n\tif (cur_stack-- <= 0) {\n\t\tverbose(env, \"pop stack internal bug\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free;\n\t}\n\tgoto peek_stack;\n\ncheck_state:\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn_state[i] != EXPLORED) {\n\t\t\tverbose(env, \"unreachable insn %d\\n\", i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t}\n\tret = 0; /* cfg looks good */\n\nerr_free:\n\tkfree(insn_state);\n\tkfree(insn_stack);\n\treturn ret;\n}\n\n/* check %cur's range satisfies %old's */\nstatic bool range_within(struct bpf_reg_state *old,\n\t\t\t struct bpf_reg_state *cur)\n{\n\treturn old->umin_value <= cur->umin_value &&\n\t       old->umax_value >= cur->umax_value &&\n\t       old->smin_value <= cur->smin_value &&\n\t       old->smax_value >= cur->smax_value;\n}\n\n/* Maximum number of register states that can exist at once */\n#define ID_MAP_SIZE\t(MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE)\nstruct idpair {\n\tu32 old;\n\tu32 cur;\n};\n\n/* If in the old state two registers had the same id, then they need to have\n * the same id in the new state as well.  But that id could be different from\n * the old state, so we need to track the mapping from old to new ids.\n * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent\n * regs with old id 5 must also have new id 9 for the new state to be safe.  But\n * regs with a different old id could still have new id 9, we don't care about\n * that.\n * So we look through our idmap to see if this old id has been seen before.  If\n * so, we require the new id to match; otherwise, we add the id pair to the map.\n */\nstatic bool check_ids(u32 old_id, u32 cur_id, struct idpair *idmap)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\n/* Returns true if (rold safe implies rcur safe) */\nstatic bool regsafe(struct bpf_reg_state *rold, struct bpf_reg_state *rcur,\n\t\t    struct idpair *idmap)\n{\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, live)) == 0)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (rold->type) {\n\tcase SCALAR_VALUE:\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* if we knew anything about the old value, we're not\n\t\t\t * equal, because we can't know anything about the\n\t\t\t * scalar value of the pointer in the new value.\n\t\t\t */\n\t\t\treturn rold->umin_value == 0 &&\n\t\t\t       rold->umax_value == U64_MAX &&\n\t\t\t       rold->smin_value == S64_MIN &&\n\t\t\t       rold->smax_value == S64_MAX &&\n\t\t\t       tnum_is_unknown(rold->var_off);\n\t\t}\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * We don't care about the 'id' value, because nothing\n\t\t * uses it for PTR_TO_MAP_VALUE (only for ..._OR_NULL)\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (rcur->type != PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\treturn false;\n\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\treturn false;\n\t\t/* Check our ids match any regs they're supposed to */\n\t\treturn check_ids(rold->id, rcur->id, idmap);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_STACK:\n\tcase PTR_TO_PACKET_END:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\nstatic bool stacksafe(struct bpf_verifier_state *old,\n\t\t      struct bpf_verifier_state *cur,\n\t\t      struct idpair *idmap)\n{\n\tint i, spi;\n\n\t/* if explored stack has more populated slots than current stack\n\t * such stacks are not equivalent\n\t */\n\tif (old->allocated_stack > cur->allocated_stack)\n\t\treturn false;\n\n\t/* walk slots of the explored stack and ignore any additional\n\t * slots in the current stack, since explored(safe) state\n\t * didn't use them\n\t */\n\tfor (i = 0; i < old->allocated_stack; i++) {\n\t\tspi = i / BPF_REG_SIZE;\n\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_INVALID)\n\t\t\tcontinue;\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] !=\n\t\t    cur->stack[spi].slot_type[i % BPF_REG_SIZE])\n\t\t\t/* Ex: old explored (safe) state has STACK_SPILL in\n\t\t\t * this stack slot, but current has has STACK_MISC ->\n\t\t\t * this verifier states are not equivalent,\n\t\t\t * return false to continue verification of this path\n\t\t\t */\n\t\t\treturn false;\n\t\tif (i % BPF_REG_SIZE)\n\t\t\tcontinue;\n\t\tif (old->stack[spi].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\tif (!regsafe(&old->stack[spi].spilled_ptr,\n\t\t\t     &cur->stack[spi].spilled_ptr,\n\t\t\t     idmap))\n\t\t\t/* when explored and current stack slot are both storing\n\t\t\t * spilled registers, check that stored pointers types\n\t\t\t * are the same as well.\n\t\t\t * Ex: explored safe path could have stored\n\t\t\t * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -8}\n\t\t\t * but current path has stored:\n\t\t\t * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -16}\n\t\t\t * such verifier states are not equivalent.\n\t\t\t * return false to continue verification of this path\n\t\t\t */\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\n/* compare two verifier states\n *\n * all states stored in state_list are known to be valid, since\n * verifier reached 'bpf_exit' instruction through them\n *\n * this function is called when verifier exploring different branches of\n * execution popped from the state stack. If it sees an old state that has\n * more strict register state and more strict stack state then this execution\n * branch doesn't need to be explored further, since verifier already\n * concluded that more strict state leads to valid finish.\n *\n * Therefore two states are equivalent if register state is more conservative\n * and explored stack state is more conservative than the current one.\n * Example:\n *       explored                   current\n * (slot1=INV slot2=MISC) == (slot1=MISC slot2=MISC)\n * (slot1=MISC slot2=MISC) != (slot1=INV slot2=MISC)\n *\n * In other words if current stack state (one being explored) has more\n * valid slots than old one that already passed validation, it means\n * the verifier can stop exploring and conclude that current state is valid too\n *\n * Similarly with registers. If explored state has register type as invalid\n * whereas register type in current state is meaningful, it means that\n * the current state will reach 'bpf_exit' instruction safely\n */\nstatic bool states_equal(struct bpf_verifier_env *env,\n\t\t\t struct bpf_verifier_state *old,\n\t\t\t struct bpf_verifier_state *cur)\n{\n\tstruct idpair *idmap;\n\tbool ret = false;\n\tint i;\n\n\tidmap = kcalloc(ID_MAP_SIZE, sizeof(struct idpair), GFP_KERNEL);\n\t/* If we failed to allocate the idmap, just say it's not safe */\n\tif (!idmap)\n\t\treturn false;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\tif (!regsafe(&old->regs[i], &cur->regs[i], idmap))\n\t\t\tgoto out_free;\n\t}\n\n\tif (!stacksafe(old, cur, idmap))\n\t\tgoto out_free;\n\tret = true;\nout_free:\n\tkfree(idmap);\n\treturn ret;\n}\n\n/* A write screens off any subsequent reads; but write marks come from the\n * straight-line code between a state and its parent.  When we arrive at a\n * jump target (in the first iteration of the propagate_liveness() loop),\n * we didn't arrive by the straight-line code, so read marks in state must\n * propagate to parent regardless of state's write marks.\n */\nstatic bool do_propagate_liveness(const struct bpf_verifier_state *state,\n\t\t\t\t  struct bpf_verifier_state *parent)\n{\n\tbool writes = parent == state->parent; /* Observe write marks */\n\tbool touched = false; /* any changes made? */\n\tint i;\n\n\tif (!parent)\n\t\treturn touched;\n\t/* Propagate read liveness of registers... */\n\tBUILD_BUG_ON(BPF_REG_FP + 1 != MAX_BPF_REG);\n\t/* We don't need to worry about FP liveness because it's read-only */\n\tfor (i = 0; i < BPF_REG_FP; i++) {\n\t\tif (parent->regs[i].live & REG_LIVE_READ)\n\t\t\tcontinue;\n\t\tif (writes && (state->regs[i].live & REG_LIVE_WRITTEN))\n\t\t\tcontinue;\n\t\tif (state->regs[i].live & REG_LIVE_READ) {\n\t\t\tparent->regs[i].live |= REG_LIVE_READ;\n\t\t\ttouched = true;\n\t\t}\n\t}\n\t/* ... and stack slots */\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE &&\n\t\t    i < parent->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tif (parent->stack[i].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\tif (state->stack[i].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\tif (parent->stack[i].spilled_ptr.live & REG_LIVE_READ)\n\t\t\tcontinue;\n\t\tif (writes &&\n\t\t    (state->stack[i].spilled_ptr.live & REG_LIVE_WRITTEN))\n\t\t\tcontinue;\n\t\tif (state->stack[i].spilled_ptr.live & REG_LIVE_READ) {\n\t\t\tparent->stack[i].spilled_ptr.live |= REG_LIVE_READ;\n\t\t\ttouched = true;\n\t\t}\n\t}\n\treturn touched;\n}\n\n/* \"parent\" is \"a state from which we reach the current state\", but initially\n * it is not the state->parent (i.e. \"the state whose straight-line code leads\n * to the current state\"), instead it is the state that happened to arrive at\n * a (prunable) equivalent of the current state.  See comment above\n * do_propagate_liveness() for consequences of this.\n * This function is just a more efficient way of calling mark_reg_read() or\n * mark_stack_slot_read() on each reg in \"parent\" that is read in \"state\",\n * though it requires that parent != state->parent in the call arguments.\n */\nstatic void propagate_liveness(const struct bpf_verifier_state *state,\n\t\t\t       struct bpf_verifier_state *parent)\n{\n\twhile (do_propagate_liveness(state, parent)) {\n\t\t/* Something changed, so we need to feed those changes onward */\n\t\tstate = parent;\n\t\tparent = state->parent;\n\t}\n}\n\nstatic int is_state_visited(struct bpf_verifier_env *env, int insn_idx)\n{\n\tstruct bpf_verifier_state_list *new_sl;\n\tstruct bpf_verifier_state_list *sl;\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tint i, err;\n\n\tsl = env->explored_states[insn_idx];\n\tif (!sl)\n\t\t/* this 'insn_idx' instruction wasn't marked, so we will not\n\t\t * be doing state search here\n\t\t */\n\t\treturn 0;\n\n\twhile (sl != STATE_LIST_MARK) {\n\t\tif (states_equal(env, &sl->state, cur)) {\n\t\t\t/* reached equivalent register/stack state,\n\t\t\t * prune the search.\n\t\t\t * Registers read by the continuation are read by us.\n\t\t\t * If we have any write marks in env->cur_state, they\n\t\t\t * will prevent corresponding reads in the continuation\n\t\t\t * from reaching our parent (an explored_state).  Our\n\t\t\t * own state will get the read marks recorded, but\n\t\t\t * they'll be immediately forgotten as we're pruning\n\t\t\t * this state and will pop a new one.\n\t\t\t */\n\t\t\tpropagate_liveness(&sl->state, cur);\n\t\t\treturn 1;\n\t\t}\n\t\tsl = sl->next;\n\t}\n\n\t/* there were no equivalent states, remember current one.\n\t * technically the current state is not proven to be safe yet,\n\t * but it will either reach bpf_exit (which means it's safe) or\n\t * it will be rejected. Since there are no loops, we won't be\n\t * seeing this 'insn_idx' instruction again on the way to bpf_exit\n\t */\n\tnew_sl = kzalloc(sizeof(struct bpf_verifier_state_list), GFP_KERNEL);\n\tif (!new_sl)\n\t\treturn -ENOMEM;\n\n\t/* add new state to the head of linked list */\n\terr = copy_verifier_state(&new_sl->state, cur);\n\tif (err) {\n\t\tfree_verifier_state(&new_sl->state, false);\n\t\tkfree(new_sl);\n\t\treturn err;\n\t}\n\tnew_sl->next = env->explored_states[insn_idx];\n\tenv->explored_states[insn_idx] = new_sl;\n\t/* connect new state to parentage chain */\n\tcur->parent = &new_sl->state;\n\t/* clear write marks in current state: the writes we did are not writes\n\t * our child did, so they don't screen off its reads from us.\n\t * (There are no read marks in current state, because reads always mark\n\t * their parent and current state never has children yet.  Only\n\t * explored_states can get read marks.)\n\t */\n\tfor (i = 0; i < BPF_REG_FP; i++)\n\t\tcur->regs[i].live = REG_LIVE_NONE;\n\tfor (i = 0; i < cur->allocated_stack / BPF_REG_SIZE; i++)\n\t\tif (cur->stack[i].slot_type[0] == STACK_SPILL)\n\t\t\tcur->stack[i].spilled_ptr.live = REG_LIVE_NONE;\n\treturn 0;\n}\n\nstatic int ext_analyzer_insn_hook(struct bpf_verifier_env *env,\n\t\t\t\t  int insn_idx, int prev_insn_idx)\n{\n\tif (env->dev_ops && env->dev_ops->insn_hook)\n\t\treturn env->dev_ops->insn_hook(env, insn_idx, prev_insn_idx);\n\n\treturn 0;\n}\n\nstatic int do_check(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *state;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_reg_state *regs;\n\tint insn_cnt = env->prog->len;\n\tint insn_idx, prev_insn_idx = 0;\n\tint insn_processed = 0;\n\tbool do_print_state = false;\n\n\tstate = kzalloc(sizeof(struct bpf_verifier_state), GFP_KERNEL);\n\tif (!state)\n\t\treturn -ENOMEM;\n\tenv->cur_state = state;\n\tinit_reg_state(env, state->regs);\n\tstate->parent = NULL;\n\tinsn_idx = 0;\n\tfor (;;) {\n\t\tstruct bpf_insn *insn;\n\t\tu8 class;\n\t\tint err;\n\n\t\tif (insn_idx >= insn_cnt) {\n\t\t\tverbose(env, \"invalid insn idx %d insn_cnt %d\\n\",\n\t\t\t\tinsn_idx, insn_cnt);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn = &insns[insn_idx];\n\t\tclass = BPF_CLASS(insn->code);\n\n\t\tif (++insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {\n\t\t\tverbose(env,\n\t\t\t\t\"BPF program is too large. Processed %d insn\\n\",\n\t\t\t\tinsn_processed);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\terr = is_state_visited(env, insn_idx);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (err == 1) {\n\t\t\t/* found equivalent state, can prune the search */\n\t\t\tif (env->log.level) {\n\t\t\t\tif (do_print_state)\n\t\t\t\t\tverbose(env, \"\\nfrom %d to %d: safe\\n\",\n\t\t\t\t\t\tprev_insn_idx, insn_idx);\n\t\t\t\telse\n\t\t\t\t\tverbose(env, \"%d: safe\\n\", insn_idx);\n\t\t\t}\n\t\t\tgoto process_bpf_exit;\n\t\t}\n\n\t\tif (need_resched())\n\t\t\tcond_resched();\n\n\t\tif (env->log.level > 1 || (env->log.level && do_print_state)) {\n\t\t\tif (env->log.level > 1)\n\t\t\t\tverbose(env, \"%d:\", insn_idx);\n\t\t\telse\n\t\t\t\tverbose(env, \"\\nfrom %d to %d:\",\n\t\t\t\t\tprev_insn_idx, insn_idx);\n\t\t\tprint_verifier_state(env, state);\n\t\t\tdo_print_state = false;\n\t\t}\n\n\t\tif (env->log.level) {\n\t\t\tverbose(env, \"%d: \", insn_idx);\n\t\t\tprint_bpf_insn(verbose, env, insn,\n\t\t\t\t       env->allow_ptr_leaks);\n\t\t}\n\n\t\terr = ext_analyzer_insn_hook(env, insn_idx, prev_insn_idx);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tregs = cur_regs(env);\n\t\tenv->insn_aux_data[insn_idx].seen = true;\n\t\tif (class == BPF_ALU || class == BPF_ALU64) {\n\t\t\terr = check_alu_op(env, insn);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_LDX) {\n\t\t\tenum bpf_reg_type *prev_src_type, src_reg_type;\n\n\t\t\t/* check for reserved fields is already done */\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tsrc_reg_type = regs[insn->src_reg].type;\n\n\t\t\t/* check that memory (src_reg + off) is readable,\n\t\t\t * the state of dst_reg will be updated by this func\n\t\t\t */\n\t\t\terr = check_mem_access(env, insn_idx, insn->src_reg, insn->off,\n\t\t\t\t\t       BPF_SIZE(insn->code), BPF_READ,\n\t\t\t\t\t       insn->dst_reg);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_src_type = &env->insn_aux_data[insn_idx].ptr_type;\n\n\t\t\tif (*prev_src_type == NOT_INIT) {\n\t\t\t\t/* saw a valid insn\n\t\t\t\t * dst_reg = *(u32 *)(src_reg + off)\n\t\t\t\t * save type to validate intersecting paths\n\t\t\t\t */\n\t\t\t\t*prev_src_type = src_reg_type;\n\n\t\t\t} else if (src_reg_type != *prev_src_type &&\n\t\t\t\t   (src_reg_type == PTR_TO_CTX ||\n\t\t\t\t    *prev_src_type == PTR_TO_CTX)) {\n\t\t\t\t/* ABuser program is trying to use the same insn\n\t\t\t\t * dst_reg = *(u32*) (src_reg + off)\n\t\t\t\t * with different pointer types:\n\t\t\t\t * src_reg == ctx in one branch and\n\t\t\t\t * src_reg == stack|map in some other branch.\n\t\t\t\t * Reject it.\n\t\t\t\t */\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_STX) {\n\t\t\tenum bpf_reg_type *prev_dst_type, dst_reg_type;\n\n\t\t\tif (BPF_MODE(insn->code) == BPF_XADD) {\n\t\t\t\terr = check_xadd(env, insn_idx, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tinsn_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\t/* check src2 operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tdst_reg_type = regs[insn->dst_reg].type;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t\t\t       BPF_SIZE(insn->code), BPF_WRITE,\n\t\t\t\t\t       insn->src_reg);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_dst_type = &env->insn_aux_data[insn_idx].ptr_type;\n\n\t\t\tif (*prev_dst_type == NOT_INIT) {\n\t\t\t\t*prev_dst_type = dst_reg_type;\n\t\t\t} else if (dst_reg_type != *prev_dst_type &&\n\t\t\t\t   (dst_reg_type == PTR_TO_CTX ||\n\t\t\t\t    *prev_dst_type == PTR_TO_CTX)) {\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_ST) {\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM ||\n\t\t\t    insn->src_reg != BPF_REG_0) {\n\t\t\t\tverbose(env, \"BPF_ST uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t\t\t       BPF_SIZE(insn->code), BPF_WRITE,\n\t\t\t\t\t       -1);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_JMP) {\n\t\t\tu8 opcode = BPF_OP(insn->code);\n\n\t\t\tif (opcode == BPF_CALL) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->off != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(env, \"BPF_CALL uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\terr = check_call(env, insn->imm, insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (opcode == BPF_JA) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(env, \"BPF_JA uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tinsn_idx += insn->off + 1;\n\t\t\t\tcontinue;\n\n\t\t\t} else if (opcode == BPF_EXIT) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(env, \"BPF_EXIT uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\t/* eBPF calling convetion is such that R0 is used\n\t\t\t\t * to return the value from eBPF program.\n\t\t\t\t * Make sure that it's readable at this time\n\t\t\t\t * of bpf_exit, which means that program wrote\n\t\t\t\t * something into it earlier\n\t\t\t\t */\n\t\t\t\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\t\t\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\n\t\t\t\terr = check_return_code(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\nprocess_bpf_exit:\n\t\t\t\terr = pop_stack(env, &prev_insn_idx, &insn_idx);\n\t\t\t\tif (err < 0) {\n\t\t\t\t\tif (err != -ENOENT)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = check_cond_jmp_op(env, insn, &insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t} else if (class == BPF_LD) {\n\t\t\tu8 mode = BPF_MODE(insn->code);\n\n\t\t\tif (mode == BPF_ABS || mode == BPF_IND) {\n\t\t\t\terr = check_ld_abs(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (mode == BPF_IMM) {\n\t\t\t\terr = check_ld_imm(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tinsn_idx++;\n\t\t\t\tenv->insn_aux_data[insn_idx].seen = true;\n\t\t\t} else {\n\t\t\t\tverbose(env, \"invalid BPF_LD mode\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tverbose(env, \"unknown insn class %d\\n\", class);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tinsn_idx++;\n\t}\n\n\tverbose(env, \"processed %d insns, stack depth %d\\n\", insn_processed,\n\t\tenv->prog->aux->stack_depth);\n\treturn 0;\n}\n\nstatic int check_map_prealloc(struct bpf_map *map)\n{\n\treturn (map->map_type != BPF_MAP_TYPE_HASH &&\n\t\tmap->map_type != BPF_MAP_TYPE_PERCPU_HASH &&\n\t\tmap->map_type != BPF_MAP_TYPE_HASH_OF_MAPS) ||\n\t\t!(map->map_flags & BPF_F_NO_PREALLOC);\n}\n\nstatic int check_map_prog_compatibility(struct bpf_verifier_env *env,\n\t\t\t\t\tstruct bpf_map *map,\n\t\t\t\t\tstruct bpf_prog *prog)\n\n{\n\t/* Make sure that BPF_PROG_TYPE_PERF_EVENT programs only use\n\t * preallocated hash maps, since doing memory allocation\n\t * in overflow_handler can crash depending on where nmi got\n\t * triggered.\n\t */\n\tif (prog->type == BPF_PROG_TYPE_PERF_EVENT) {\n\t\tif (!check_map_prealloc(map)) {\n\t\t\tverbose(env, \"perf_event programs can only use preallocated hash map\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (map->inner_map_meta &&\n\t\t    !check_map_prealloc(map->inner_map_meta)) {\n\t\t\tverbose(env, \"perf_event programs can only use preallocated inner hash map\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/* look for pseudo eBPF instructions that access map FDs and\n * replace them with actual map pointers\n */\nstatic int replace_map_fd_with_map_ptr(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint i, j, err;\n\n\terr = bpf_prog_calc_tag(env->prog);\n\tif (err)\n\t\treturn err;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (BPF_CLASS(insn->code) == BPF_LDX &&\n\t\t    (BPF_MODE(insn->code) != BPF_MEM || insn->imm != 0)) {\n\t\t\tverbose(env, \"BPF_LDX uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_STX &&\n\t\t    ((BPF_MODE(insn->code) != BPF_MEM &&\n\t\t      BPF_MODE(insn->code) != BPF_XADD) || insn->imm != 0)) {\n\t\t\tverbose(env, \"BPF_STX uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (insn[0].code == (BPF_LD | BPF_IMM | BPF_DW)) {\n\t\t\tstruct bpf_map *map;\n\t\t\tstruct fd f;\n\n\t\t\tif (i == insn_cnt - 1 || insn[1].code != 0 ||\n\t\t\t    insn[1].dst_reg != 0 || insn[1].src_reg != 0 ||\n\t\t\t    insn[1].off != 0) {\n\t\t\t\tverbose(env, \"invalid bpf_ld_imm64 insn\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (insn->src_reg == 0)\n\t\t\t\t/* valid generic load 64-bit imm */\n\t\t\t\tgoto next_insn;\n\n\t\t\tif (insn->src_reg != BPF_PSEUDO_MAP_FD) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"unrecognized bpf_ld_imm64 insn\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tf = fdget(insn->imm);\n\t\t\tmap = __bpf_map_get(f);\n\t\t\tif (IS_ERR(map)) {\n\t\t\t\tverbose(env, \"fd %d is not pointing to valid bpf_map\\n\",\n\t\t\t\t\tinsn->imm);\n\t\t\t\treturn PTR_ERR(map);\n\t\t\t}\n\n\t\t\terr = check_map_prog_compatibility(env, map, env->prog);\n\t\t\tif (err) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\t/* store map pointer inside BPF_LD_IMM64 instruction */\n\t\t\tinsn[0].imm = (u32) (unsigned long) map;\n\t\t\tinsn[1].imm = ((u64) (unsigned long) map) >> 32;\n\n\t\t\t/* check whether we recorded this map already */\n\t\t\tfor (j = 0; j < env->used_map_cnt; j++)\n\t\t\t\tif (env->used_maps[j] == map) {\n\t\t\t\t\tfdput(f);\n\t\t\t\t\tgoto next_insn;\n\t\t\t\t}\n\n\t\t\tif (env->used_map_cnt >= MAX_USED_MAPS) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn -E2BIG;\n\t\t\t}\n\n\t\t\t/* hold the map. If the program is rejected by verifier,\n\t\t\t * the map will be released by release_maps() or it\n\t\t\t * will be used by the valid program until it's unloaded\n\t\t\t * and all maps are released in free_bpf_prog_info()\n\t\t\t */\n\t\t\tmap = bpf_map_inc(map, false);\n\t\t\tif (IS_ERR(map)) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn PTR_ERR(map);\n\t\t\t}\n\t\t\tenv->used_maps[env->used_map_cnt++] = map;\n\n\t\t\tfdput(f);\nnext_insn:\n\t\t\tinsn++;\n\t\t\ti++;\n\t\t}\n\t}\n\n\t/* now all pseudo BPF_LD_IMM64 instructions load valid\n\t * 'struct bpf_map *' into a register instead of user map_fd.\n\t * These pointers will be used later by verifier to validate map access.\n\t */\n\treturn 0;\n}\n\n/* drop refcnt of maps used by the rejected program */\nstatic void release_maps(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tfor (i = 0; i < env->used_map_cnt; i++)\n\t\tbpf_map_put(env->used_maps[i]);\n}\n\n/* convert pseudo BPF_LD_IMM64 into generic BPF_LD_IMM64 */\nstatic void convert_pseudo_ld_imm64(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint i;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++)\n\t\tif (insn->code == (BPF_LD | BPF_IMM | BPF_DW))\n\t\t\tinsn->src_reg = 0;\n}\n\n/* single env->prog->insni[off] instruction was replaced with the range\n * insni[off, off + cnt).  Adjust corresponding insn_aux_data by copying\n * [0, off) and [off, end) to new locations, so the patched range stays zero\n */\nstatic int adjust_insn_aux_data(struct bpf_verifier_env *env, u32 prog_len,\n\t\t\t\tu32 off, u32 cnt)\n{\n\tstruct bpf_insn_aux_data *new_data, *old_data = env->insn_aux_data;\n\tint i;\n\n\tif (cnt == 1)\n\t\treturn 0;\n\tnew_data = vzalloc(sizeof(struct bpf_insn_aux_data) * prog_len);\n\tif (!new_data)\n\t\treturn -ENOMEM;\n\tmemcpy(new_data, old_data, sizeof(struct bpf_insn_aux_data) * off);\n\tmemcpy(new_data + off + cnt - 1, old_data + off,\n\t       sizeof(struct bpf_insn_aux_data) * (prog_len - off - cnt + 1));\n\tfor (i = off; i < off + cnt - 1; i++)\n\t\tnew_data[i].seen = true;\n\tenv->insn_aux_data = new_data;\n\tvfree(old_data);\n\treturn 0;\n}\n\nstatic struct bpf_prog *bpf_patch_insn_data(struct bpf_verifier_env *env, u32 off,\n\t\t\t\t\t    const struct bpf_insn *patch, u32 len)\n{\n\tstruct bpf_prog *new_prog;\n\n\tnew_prog = bpf_patch_insn_single(env->prog, off, patch, len);\n\tif (!new_prog)\n\t\treturn NULL;\n\tif (adjust_insn_aux_data(env, new_prog->len, off, len))\n\t\treturn NULL;\n\treturn new_prog;\n}\n\n/* The verifier does more data flow analysis than llvm and will not explore\n * branches that are dead at run time. Malicious programs can have dead code\n * too. Therefore replace all dead at-run-time code with nops.\n */\nstatic void sanitize_dead_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn_aux_data *aux_data = env->insn_aux_data;\n\tstruct bpf_insn nop = BPF_MOV64_REG(BPF_REG_0, BPF_REG_0);\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tconst int insn_cnt = env->prog->len;\n\tint i;\n\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (aux_data[i].seen)\n\t\t\tcontinue;\n\t\tmemcpy(insn + i, &nop, sizeof(nop));\n\t}\n}\n\n/* convert load instructions that access fields of 'struct __sk_buff'\n * into sequence of instructions that access fields of 'struct sk_buff'\n */\nstatic int convert_ctx_accesses(struct bpf_verifier_env *env)\n{\n\tconst struct bpf_verifier_ops *ops = env->ops;\n\tint i, cnt, size, ctx_field_size, delta = 0;\n\tconst int insn_cnt = env->prog->len;\n\tstruct bpf_insn insn_buf[16], *insn;\n\tstruct bpf_prog *new_prog;\n\tenum bpf_access_type type;\n\tbool is_narrower_load;\n\tu32 target_size;\n\n\tif (ops->gen_prologue) {\n\t\tcnt = ops->gen_prologue(insn_buf, env->seen_direct_write,\n\t\t\t\t\tenv->prog);\n\t\tif (cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t} else if (cnt) {\n\t\t\tnew_prog = bpf_patch_insn_data(env, 0, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tenv->prog = new_prog;\n\t\t\tdelta += cnt - 1;\n\t\t}\n\t}\n\n\tif (!ops->convert_ctx_access)\n\t\treturn 0;\n\n\tinsn = env->prog->insnsi + delta;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_LDX | BPF_MEM | BPF_B) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_H) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_W) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_DW))\n\t\t\ttype = BPF_READ;\n\t\telse if (insn->code == (BPF_STX | BPF_MEM | BPF_B) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_H) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_W) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_DW))\n\t\t\ttype = BPF_WRITE;\n\t\telse\n\t\t\tcontinue;\n\n\t\tif (env->insn_aux_data[i + delta].ptr_type != PTR_TO_CTX)\n\t\t\tcontinue;\n\n\t\tctx_field_size = env->insn_aux_data[i + delta].ctx_field_size;\n\t\tsize = BPF_LDST_BYTES(insn);\n\n\t\t/* If the read access is a narrower load of the field,\n\t\t * convert to a 4/8-byte load, to minimum program type specific\n\t\t * convert_ctx_access changes. If conversion is successful,\n\t\t * we will apply proper mask to the result.\n\t\t */\n\t\tis_narrower_load = size < ctx_field_size;\n\t\tif (is_narrower_load) {\n\t\t\tu32 off = insn->off;\n\t\t\tu8 size_code;\n\n\t\t\tif (type == BPF_WRITE) {\n\t\t\t\tverbose(env, \"bpf verifier narrow ctx access misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tsize_code = BPF_H;\n\t\t\tif (ctx_field_size == 4)\n\t\t\t\tsize_code = BPF_W;\n\t\t\telse if (ctx_field_size == 8)\n\t\t\t\tsize_code = BPF_DW;\n\n\t\t\tinsn->off = off & ~(ctx_field_size - 1);\n\t\t\tinsn->code = BPF_LDX | BPF_MEM | size_code;\n\t\t}\n\n\t\ttarget_size = 0;\n\t\tcnt = ops->convert_ctx_access(type, insn, insn_buf, env->prog,\n\t\t\t\t\t      &target_size);\n\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf) ||\n\t\t    (ctx_field_size && !target_size)) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (is_narrower_load && size < target_size) {\n\t\t\tif (ctx_field_size <= 4)\n\t\t\t\tinsn_buf[cnt++] = BPF_ALU32_IMM(BPF_AND, insn->dst_reg,\n\t\t\t\t\t\t\t\t(1 << size * 8) - 1);\n\t\t\telse\n\t\t\t\tinsn_buf[cnt++] = BPF_ALU64_IMM(BPF_AND, insn->dst_reg,\n\t\t\t\t\t\t\t\t(1 << size * 8) - 1);\n\t\t}\n\n\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\tif (!new_prog)\n\t\t\treturn -ENOMEM;\n\n\t\tdelta += cnt - 1;\n\n\t\t/* keep walking new program and skip insns we just inserted */\n\t\tenv->prog = new_prog;\n\t\tinsn      = new_prog->insnsi + i + delta;\n\t}\n\n\treturn 0;\n}\n\n/* fixup insn->imm field of bpf_call instructions\n * and inline eligible helpers as explicit sequence of BPF instructions\n *\n * this function is called after eBPF program passed verification\n */\nstatic int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\n\t\tif (insn->imm == BPF_FUNC_get_route_realm)\n\t\t\tprog->dst_needed = 1;\n\t\tif (insn->imm == BPF_FUNC_get_prandom_u32)\n\t\t\tbpf_user_rnd_init_once();\n\t\tif (insn->imm == BPF_FUNC_tail_call) {\n\t\t\t/* If we tail call into other programs, we\n\t\t\t * cannot make any assumptions since they can\n\t\t\t * be replaced dynamically during runtime in\n\t\t\t * the program array.\n\t\t\t */\n\t\t\tprog->cb_access = 1;\n\t\t\tenv->prog->aux->stack_depth = MAX_BPF_STACK;\n\n\t\t\t/* mark bpf_tail_call as different opcode to avoid\n\t\t\t * conditional branch in the interpeter for every normal\n\t\t\t * call and to prevent accidental JITing by JIT compiler\n\t\t\t * that doesn't support bpf_tail_call yet\n\t\t\t */\n\t\t\tinsn->imm = 0;\n\t\t\tinsn->code = BPF_JMP | BPF_TAIL_CALL;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* BPF_EMIT_CALL() assumptions in some of the map_gen_lookup\n\t\t * handlers are currently limited to 64 bit only.\n\t\t */\n\t\tif (ebpf_jit_enabled() && BITS_PER_LONG == 64 &&\n\t\t    insn->imm == BPF_FUNC_map_lookup_elem) {\n\t\t\tmap_ptr = env->insn_aux_data[i + delta].map_ptr;\n\t\t\tif (map_ptr == BPF_MAP_PTR_POISON ||\n\t\t\t    !map_ptr->ops->map_gen_lookup)\n\t\t\t\tgoto patch_call_imm;\n\n\t\t\tcnt = map_ptr->ops->map_gen_lookup(map_ptr, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf,\n\t\t\t\t\t\t       cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta += cnt - 1;\n\n\t\t\t/* keep walking new program and skip insns we just inserted */\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->imm == BPF_FUNC_redirect_map) {\n\t\t\t/* Note, we cannot use prog directly as imm as subsequent\n\t\t\t * rewrites would still change the prog pointer. The only\n\t\t\t * stable address we can use is aux, which also works with\n\t\t\t * prog clones during blinding.\n\t\t\t */\n\t\t\tu64 addr = (unsigned long)prog->aux;\n\t\t\tstruct bpf_insn r4_ld[] = {\n\t\t\t\tBPF_LD_IMM64(BPF_REG_4, addr),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tcnt = ARRAY_SIZE(r4_ld);\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, r4_ld, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t}\npatch_call_imm:\n\t\tfn = env->ops->get_func_proto(insn->imm);\n\t\t/* all functions that have prototype and verifier allowed\n\t\t * programs to call them, must be real in-kernel functions\n\t\t */\n\t\tif (!fn->func) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\t\tfunc_id_name(insn->imm), insn->imm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tinsn->imm = fn->func - __bpf_call_base;\n\t}\n\n\treturn 0;\n}\n\nstatic void free_states(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state_list *sl, *sln;\n\tint i;\n\n\tif (!env->explored_states)\n\t\treturn;\n\n\tfor (i = 0; i < env->prog->len; i++) {\n\t\tsl = env->explored_states[i];\n\n\t\tif (sl)\n\t\t\twhile (sl != STATE_LIST_MARK) {\n\t\t\t\tsln = sl->next;\n\t\t\t\tfree_verifier_state(&sl->state, false);\n\t\t\t\tkfree(sl);\n\t\t\t\tsl = sln;\n\t\t\t}\n\t}\n\n\tkfree(env->explored_states);\n}\n\nint bpf_check(struct bpf_prog **prog, union bpf_attr *attr)\n{\n\tstruct bpf_verifier_env *env;\n\tstruct bpf_verifer_log *log;\n\tint ret = -EINVAL;\n\n\t/* no program is valid */\n\tif (ARRAY_SIZE(bpf_verifier_ops) == 0)\n\t\treturn -EINVAL;\n\n\t/* 'struct bpf_verifier_env' can be global, but since it's not small,\n\t * allocate/free it every time bpf_check() is called\n\t */\n\tenv = kzalloc(sizeof(struct bpf_verifier_env), GFP_KERNEL);\n\tif (!env)\n\t\treturn -ENOMEM;\n\tlog = &env->log;\n\n\tenv->insn_aux_data = vzalloc(sizeof(struct bpf_insn_aux_data) *\n\t\t\t\t     (*prog)->len);\n\tret = -ENOMEM;\n\tif (!env->insn_aux_data)\n\t\tgoto err_free_env;\n\tenv->prog = *prog;\n\tenv->ops = bpf_verifier_ops[env->prog->type];\n\n\t/* grab the mutex to protect few globals used by verifier */\n\tmutex_lock(&bpf_verifier_lock);\n\n\tif (attr->log_level || attr->log_buf || attr->log_size) {\n\t\t/* user requested verbose verifier output\n\t\t * and supplied buffer to store the verification trace\n\t\t */\n\t\tlog->level = attr->log_level;\n\t\tlog->ubuf = (char __user *) (unsigned long) attr->log_buf;\n\t\tlog->len_total = attr->log_size;\n\n\t\tret = -EINVAL;\n\t\t/* log attributes have to be sane */\n\t\tif (log->len_total < 128 || log->len_total > UINT_MAX >> 8 ||\n\t\t    !log->level || !log->ubuf)\n\t\t\tgoto err_unlock;\n\t}\n\n\tenv->strict_alignment = !!(attr->prog_flags & BPF_F_STRICT_ALIGNMENT);\n\tif (!IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS))\n\t\tenv->strict_alignment = true;\n\n\tif (env->prog->aux->offload) {\n\t\tret = bpf_prog_offload_verifier_prep(env);\n\t\tif (ret)\n\t\t\tgoto err_unlock;\n\t}\n\n\tret = replace_map_fd_with_map_ptr(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tenv->explored_states = kcalloc(env->prog->len,\n\t\t\t\t       sizeof(struct bpf_verifier_state_list *),\n\t\t\t\t       GFP_USER);\n\tret = -ENOMEM;\n\tif (!env->explored_states)\n\t\tgoto skip_full_check;\n\n\tret = check_cfg(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tenv->allow_ptr_leaks = capable(CAP_SYS_ADMIN);\n\n\tret = do_check(env);\n\tif (env->cur_state) {\n\t\tfree_verifier_state(env->cur_state, true);\n\t\tenv->cur_state = NULL;\n\t}\n\nskip_full_check:\n\twhile (!pop_stack(env, NULL, NULL));\n\tfree_states(env);\n\n\tif (ret == 0)\n\t\tsanitize_dead_code(env);\n\n\tif (ret == 0)\n\t\t/* program is valid, convert *(u32*)(ctx + off) accesses */\n\t\tret = convert_ctx_accesses(env);\n\n\tif (ret == 0)\n\t\tret = fixup_bpf_calls(env);\n\n\tif (log->level && bpf_verifier_log_full(log))\n\t\tret = -ENOSPC;\n\tif (log->level && !log->ubuf) {\n\t\tret = -EFAULT;\n\t\tgoto err_release_maps;\n\t}\n\n\tif (ret == 0 && env->used_map_cnt) {\n\t\t/* if program passed verifier, update used_maps in bpf_prog_info */\n\t\tenv->prog->aux->used_maps = kmalloc_array(env->used_map_cnt,\n\t\t\t\t\t\t\t  sizeof(env->used_maps[0]),\n\t\t\t\t\t\t\t  GFP_KERNEL);\n\n\t\tif (!env->prog->aux->used_maps) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_release_maps;\n\t\t}\n\n\t\tmemcpy(env->prog->aux->used_maps, env->used_maps,\n\t\t       sizeof(env->used_maps[0]) * env->used_map_cnt);\n\t\tenv->prog->aux->used_map_cnt = env->used_map_cnt;\n\n\t\t/* program is valid. Convert pseudo bpf_ld_imm64 into generic\n\t\t * bpf_ld_imm64 instructions\n\t\t */\n\t\tconvert_pseudo_ld_imm64(env);\n\t}\n\nerr_release_maps:\n\tif (!env->prog->aux->used_maps)\n\t\t/* if we didn't copy map pointers into bpf_prog_info, release\n\t\t * them now. Otherwise free_bpf_prog_info() will release them.\n\t\t */\n\t\trelease_maps(env);\n\t*prog = env->prog;\nerr_unlock:\n\tmutex_unlock(&bpf_verifier_lock);\n\tvfree(env->insn_aux_data);\nerr_free_env:\n\tkfree(env);\n\treturn ret;\n}\n"], "fixing_code": ["/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com\n * Copyright (c) 2016 Facebook\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of version 2 of the GNU General Public\n * License as published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful, but\n * WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n * General Public License for more details.\n */\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/bpf.h>\n#include <linux/bpf_verifier.h>\n#include <linux/filter.h>\n#include <net/netlink.h>\n#include <linux/file.h>\n#include <linux/vmalloc.h>\n#include <linux/stringify.h>\n\n#include \"disasm.h\"\n\nstatic const struct bpf_verifier_ops * const bpf_verifier_ops[] = {\n#define BPF_PROG_TYPE(_id, _name) \\\n\t[_id] = & _name ## _verifier_ops,\n#define BPF_MAP_TYPE(_id, _ops)\n#include <linux/bpf_types.h>\n#undef BPF_PROG_TYPE\n#undef BPF_MAP_TYPE\n};\n\n/* bpf_check() is a static code analyzer that walks eBPF program\n * instruction by instruction and updates register/stack state.\n * All paths of conditional branches are analyzed until 'bpf_exit' insn.\n *\n * The first pass is depth-first-search to check that the program is a DAG.\n * It rejects the following programs:\n * - larger than BPF_MAXINSNS insns\n * - if loop is present (detected via back-edge)\n * - unreachable insns exist (shouldn't be a forest. program = one function)\n * - out of bounds or malformed jumps\n * The second pass is all possible path descent from the 1st insn.\n * Since it's analyzing all pathes through the program, the length of the\n * analysis is limited to 64k insn, which may be hit even if total number of\n * insn is less then 4K, but there are too many branches that change stack/regs.\n * Number of 'branches to be analyzed' is limited to 1k\n *\n * On entry to each instruction, each register has a type, and the instruction\n * changes the types of the registers depending on instruction semantics.\n * If instruction is BPF_MOV64_REG(BPF_REG_1, BPF_REG_5), then type of R5 is\n * copied to R1.\n *\n * All registers are 64-bit.\n * R0 - return register\n * R1-R5 argument passing registers\n * R6-R9 callee saved registers\n * R10 - frame pointer read-only\n *\n * At the start of BPF program the register R1 contains a pointer to bpf_context\n * and has type PTR_TO_CTX.\n *\n * Verifier tracks arithmetic operations on pointers in case:\n *    BPF_MOV64_REG(BPF_REG_1, BPF_REG_10),\n *    BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, -20),\n * 1st insn copies R10 (which has FRAME_PTR) type into R1\n * and 2nd arithmetic instruction is pattern matched to recognize\n * that it wants to construct a pointer to some element within stack.\n * So after 2nd insn, the register R1 has type PTR_TO_STACK\n * (and -20 constant is saved for further stack bounds checking).\n * Meaning that this reg is a pointer to stack plus known immediate constant.\n *\n * Most of the time the registers have SCALAR_VALUE type, which\n * means the register has some value, but it's not a valid pointer.\n * (like pointer plus pointer becomes SCALAR_VALUE type)\n *\n * When verifier sees load or store instructions the type of base register\n * can be: PTR_TO_MAP_VALUE, PTR_TO_CTX, PTR_TO_STACK. These are three pointer\n * types recognized by check_mem_access() function.\n *\n * PTR_TO_MAP_VALUE means that this register is pointing to 'map element value'\n * and the range of [ptr, ptr + map's value_size) is accessible.\n *\n * registers used to pass values to function calls are checked against\n * function argument constraints.\n *\n * ARG_PTR_TO_MAP_KEY is one of such argument constraints.\n * It means that the register type passed to this function must be\n * PTR_TO_STACK and it will be used inside the function as\n * 'pointer to map element key'\n *\n * For example the argument constraints for bpf_map_lookup_elem():\n *   .ret_type = RET_PTR_TO_MAP_VALUE_OR_NULL,\n *   .arg1_type = ARG_CONST_MAP_PTR,\n *   .arg2_type = ARG_PTR_TO_MAP_KEY,\n *\n * ret_type says that this function returns 'pointer to map elem value or null'\n * function expects 1st argument to be a const pointer to 'struct bpf_map' and\n * 2nd argument should be a pointer to stack, which will be used inside\n * the helper function as a pointer to map element key.\n *\n * On the kernel side the helper function looks like:\n * u64 bpf_map_lookup_elem(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)\n * {\n *    struct bpf_map *map = (struct bpf_map *) (unsigned long) r1;\n *    void *key = (void *) (unsigned long) r2;\n *    void *value;\n *\n *    here kernel can access 'key' and 'map' pointers safely, knowing that\n *    [key, key + map->key_size) bytes are valid and were initialized on\n *    the stack of eBPF program.\n * }\n *\n * Corresponding eBPF program may look like:\n *    BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),  // after this insn R2 type is FRAME_PTR\n *    BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -4), // after this insn R2 type is PTR_TO_STACK\n *    BPF_LD_MAP_FD(BPF_REG_1, map_fd),      // after this insn R1 type is CONST_PTR_TO_MAP\n *    BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_map_lookup_elem),\n * here verifier looks at prototype of map_lookup_elem() and sees:\n * .arg1_type == ARG_CONST_MAP_PTR and R1->type == CONST_PTR_TO_MAP, which is ok,\n * Now verifier knows that this map has key of R1->map_ptr->key_size bytes\n *\n * Then .arg2_type == ARG_PTR_TO_MAP_KEY and R2->type == PTR_TO_STACK, ok so far,\n * Now verifier checks that [R2, R2 + map's key_size) are within stack limits\n * and were initialized prior to this call.\n * If it's ok, then verifier allows this BPF_CALL insn and looks at\n * .ret_type which is RET_PTR_TO_MAP_VALUE_OR_NULL, so it sets\n * R0->type = PTR_TO_MAP_VALUE_OR_NULL which means bpf_map_lookup_elem() function\n * returns ether pointer to map value or NULL.\n *\n * When type PTR_TO_MAP_VALUE_OR_NULL passes through 'if (reg != 0) goto +off'\n * insn, the register holding that pointer in the true branch changes state to\n * PTR_TO_MAP_VALUE and the same register changes state to CONST_IMM in the false\n * branch. See check_cond_jmp_op().\n *\n * After the call R0 is set to return type of the function and registers R1-R5\n * are set to NOT_INIT to indicate that they are no longer readable.\n */\n\n/* verifier_state + insn_idx are pushed to stack when branch is encountered */\nstruct bpf_verifier_stack_elem {\n\t/* verifer state is 'st'\n\t * before processing instruction 'insn_idx'\n\t * and after processing instruction 'prev_insn_idx'\n\t */\n\tstruct bpf_verifier_state st;\n\tint insn_idx;\n\tint prev_insn_idx;\n\tstruct bpf_verifier_stack_elem *next;\n};\n\n#define BPF_COMPLEXITY_LIMIT_INSNS\t131072\n#define BPF_COMPLEXITY_LIMIT_STACK\t1024\n\n#define BPF_MAP_PTR_POISON ((void *)0xeB9F + POISON_POINTER_DELTA)\n\nstruct bpf_call_arg_meta {\n\tstruct bpf_map *map_ptr;\n\tbool raw_mode;\n\tbool pkt_access;\n\tint regno;\n\tint access_size;\n};\n\nstatic DEFINE_MUTEX(bpf_verifier_lock);\n\n/* log_level controls verbosity level of eBPF verifier.\n * verbose() is used to dump the verification trace to the log, so the user\n * can figure out what's wrong with the program\n */\nstatic __printf(2, 3) void verbose(struct bpf_verifier_env *env,\n\t\t\t\t   const char *fmt, ...)\n{\n\tstruct bpf_verifer_log *log = &env->log;\n\tunsigned int n;\n\tva_list args;\n\n\tif (!log->level || !log->ubuf || bpf_verifier_log_full(log))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tn = vscnprintf(log->kbuf, BPF_VERIFIER_TMP_LOG_SIZE, fmt, args);\n\tva_end(args);\n\n\tWARN_ONCE(n >= BPF_VERIFIER_TMP_LOG_SIZE - 1,\n\t\t  \"verifier log line truncated - local buffer too short\\n\");\n\n\tn = min(log->len_total - log->len_used - 1, n);\n\tlog->kbuf[n] = '\\0';\n\n\tif (!copy_to_user(log->ubuf + log->len_used, log->kbuf, n + 1))\n\t\tlog->len_used += n;\n\telse\n\t\tlog->ubuf = NULL;\n}\n\nstatic bool type_is_pkt_pointer(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_PACKET ||\n\t       type == PTR_TO_PACKET_META;\n}\n\n/* string representation of 'enum bpf_reg_type' */\nstatic const char * const reg_type_str[] = {\n\t[NOT_INIT]\t\t= \"?\",\n\t[SCALAR_VALUE]\t\t= \"inv\",\n\t[PTR_TO_CTX]\t\t= \"ctx\",\n\t[CONST_PTR_TO_MAP]\t= \"map_ptr\",\n\t[PTR_TO_MAP_VALUE]\t= \"map_value\",\n\t[PTR_TO_MAP_VALUE_OR_NULL] = \"map_value_or_null\",\n\t[PTR_TO_STACK]\t\t= \"fp\",\n\t[PTR_TO_PACKET]\t\t= \"pkt\",\n\t[PTR_TO_PACKET_META]\t= \"pkt_meta\",\n\t[PTR_TO_PACKET_END]\t= \"pkt_end\",\n};\n\nstatic void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t struct bpf_verifier_state *state)\n{\n\tstruct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d=%s\", i, reg_type_str[t]);\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (t == CONST_PTR_TO_MAP ||\n\t\t\t\t t == PTR_TO_MAP_VALUE ||\n\t\t\t\t t == PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tif (state->stack[i].slot_type[0] == STACK_SPILL)\n\t\t\tverbose(env, \" fp%d=%s\",\n\t\t\t\t-MAX_BPF_STACK + i * BPF_REG_SIZE,\n\t\t\t\treg_type_str[state->stack[i].spilled_ptr.type]);\n\t}\n\tverbose(env, \"\\n\");\n}\n\nstatic int copy_stack_state(struct bpf_verifier_state *dst,\n\t\t\t    const struct bpf_verifier_state *src)\n{\n\tif (!src->stack)\n\t\treturn 0;\n\tif (WARN_ON_ONCE(dst->allocated_stack < src->allocated_stack)) {\n\t\t/* internal bug, make state invalid to reject the program */\n\t\tmemset(dst, 0, sizeof(*dst));\n\t\treturn -EFAULT;\n\t}\n\tmemcpy(dst->stack, src->stack,\n\t       sizeof(*src->stack) * (src->allocated_stack / BPF_REG_SIZE));\n\treturn 0;\n}\n\n/* do_check() starts with zero-sized stack in struct bpf_verifier_state to\n * make it consume minimal amount of memory. check_stack_write() access from\n * the program calls into realloc_verifier_state() to grow the stack size.\n * Note there is a non-zero 'parent' pointer inside bpf_verifier_state\n * which this function copies over. It points to previous bpf_verifier_state\n * which is never reallocated\n */\nstatic int realloc_verifier_state(struct bpf_verifier_state *state, int size,\n\t\t\t\t  bool copy_old)\n{\n\tu32 old_size = state->allocated_stack;\n\tstruct bpf_stack_state *new_stack;\n\tint slot = size / BPF_REG_SIZE;\n\n\tif (size <= old_size || !size) {\n\t\tif (copy_old)\n\t\t\treturn 0;\n\t\tstate->allocated_stack = slot * BPF_REG_SIZE;\n\t\tif (!size && old_size) {\n\t\t\tkfree(state->stack);\n\t\t\tstate->stack = NULL;\n\t\t}\n\t\treturn 0;\n\t}\n\tnew_stack = kmalloc_array(slot, sizeof(struct bpf_stack_state),\n\t\t\t\t  GFP_KERNEL);\n\tif (!new_stack)\n\t\treturn -ENOMEM;\n\tif (copy_old) {\n\t\tif (state->stack)\n\t\t\tmemcpy(new_stack, state->stack,\n\t\t\t       sizeof(*new_stack) * (old_size / BPF_REG_SIZE));\n\t\tmemset(new_stack + old_size / BPF_REG_SIZE, 0,\n\t\t       sizeof(*new_stack) * (size - old_size) / BPF_REG_SIZE);\n\t}\n\tstate->allocated_stack = slot * BPF_REG_SIZE;\n\tkfree(state->stack);\n\tstate->stack = new_stack;\n\treturn 0;\n}\n\nstatic void free_verifier_state(struct bpf_verifier_state *state,\n\t\t\t\tbool free_self)\n{\n\tkfree(state->stack);\n\tif (free_self)\n\t\tkfree(state);\n}\n\n/* copy verifier state from src to dst growing dst stack space\n * when necessary to accommodate larger src stack\n */\nstatic int copy_verifier_state(struct bpf_verifier_state *dst,\n\t\t\t       const struct bpf_verifier_state *src)\n{\n\tint err;\n\n\terr = realloc_verifier_state(dst, src->allocated_stack, false);\n\tif (err)\n\t\treturn err;\n\tmemcpy(dst, src, offsetof(struct bpf_verifier_state, allocated_stack));\n\treturn copy_stack_state(dst, src);\n}\n\nstatic int pop_stack(struct bpf_verifier_env *env, int *prev_insn_idx,\n\t\t     int *insn_idx)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_verifier_stack_elem *elem, *head = env->head;\n\tint err;\n\n\tif (env->head == NULL)\n\t\treturn -ENOENT;\n\n\tif (cur) {\n\t\terr = copy_verifier_state(cur, &head->st);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (insn_idx)\n\t\t*insn_idx = head->insn_idx;\n\tif (prev_insn_idx)\n\t\t*prev_insn_idx = head->prev_insn_idx;\n\telem = head->next;\n\tfree_verifier_state(&head->st, false);\n\tkfree(head);\n\tenv->head = elem;\n\tenv->stack_size--;\n\treturn 0;\n}\n\nstatic struct bpf_verifier_state *push_stack(struct bpf_verifier_env *env,\n\t\t\t\t\t     int insn_idx, int prev_insn_idx)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_verifier_stack_elem *elem;\n\tint err;\n\n\telem = kzalloc(sizeof(struct bpf_verifier_stack_elem), GFP_KERNEL);\n\tif (!elem)\n\t\tgoto err;\n\n\telem->insn_idx = insn_idx;\n\telem->prev_insn_idx = prev_insn_idx;\n\telem->next = env->head;\n\tenv->head = elem;\n\tenv->stack_size++;\n\terr = copy_verifier_state(&elem->st, cur);\n\tif (err)\n\t\tgoto err;\n\tif (env->stack_size > BPF_COMPLEXITY_LIMIT_STACK) {\n\t\tverbose(env, \"BPF program is too complex\\n\");\n\t\tgoto err;\n\t}\n\treturn &elem->st;\nerr:\n\t/* pop all elements and return */\n\twhile (!pop_stack(env, NULL, NULL));\n\treturn NULL;\n}\n\n#define CALLER_SAVED_REGS 6\nstatic const int caller_saved[CALLER_SAVED_REGS] = {\n\tBPF_REG_0, BPF_REG_1, BPF_REG_2, BPF_REG_3, BPF_REG_4, BPF_REG_5\n};\n\nstatic void __mark_reg_not_init(struct bpf_reg_state *reg);\n\n/* Mark the unknown part of a register (variable offset or scalar value) as\n * known to have the value @imm.\n */\nstatic void __mark_reg_known(struct bpf_reg_state *reg, u64 imm)\n{\n\treg->id = 0;\n\treg->var_off = tnum_const(imm);\n\treg->smin_value = (s64)imm;\n\treg->smax_value = (s64)imm;\n\treg->umin_value = imm;\n\treg->umax_value = imm;\n}\n\n/* Mark the 'variable offset' part of a register as zero.  This should be\n * used only on registers holding a pointer type.\n */\nstatic void __mark_reg_known_zero(struct bpf_reg_state *reg)\n{\n\t__mark_reg_known(reg, 0);\n}\n\nstatic void mark_reg_known_zero(struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_known_zero(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs */\n\t\tfor (regno = 0; regno < MAX_BPF_REG; regno++)\n\t\t\t__mark_reg_not_init(regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_known_zero(regs + regno);\n}\n\nstatic bool reg_is_pkt_pointer(const struct bpf_reg_state *reg)\n{\n\treturn type_is_pkt_pointer(reg->type);\n}\n\nstatic bool reg_is_pkt_pointer_any(const struct bpf_reg_state *reg)\n{\n\treturn reg_is_pkt_pointer(reg) ||\n\t       reg->type == PTR_TO_PACKET_END;\n}\n\n/* Unmodified PTR_TO_PACKET[_META,_END] register from ctx access. */\nstatic bool reg_is_init_pkt_pointer(const struct bpf_reg_state *reg,\n\t\t\t\t    enum bpf_reg_type which)\n{\n\t/* The register can already have a range from prior markings.\n\t * This is fine as long as it hasn't been advanced from its\n\t * origin.\n\t */\n\treturn reg->type == which &&\n\t       reg->id == 0 &&\n\t       reg->off == 0 &&\n\t       tnum_equals_const(reg->var_off, 0);\n}\n\n/* Attempts to improve min/max values based on var_off information */\nstatic void __update_reg_bounds(struct bpf_reg_state *reg)\n{\n\t/* min signed is max(sign bit) | min(other bits) */\n\treg->smin_value = max_t(s64, reg->smin_value,\n\t\t\t\treg->var_off.value | (reg->var_off.mask & S64_MIN));\n\t/* max signed is min(sign bit) | max(other bits) */\n\treg->smax_value = min_t(s64, reg->smax_value,\n\t\t\t\treg->var_off.value | (reg->var_off.mask & S64_MAX));\n\treg->umin_value = max(reg->umin_value, reg->var_off.value);\n\treg->umax_value = min(reg->umax_value,\n\t\t\t      reg->var_off.value | reg->var_off.mask);\n}\n\n/* Uses signed min/max values to inform unsigned, and vice-versa */\nstatic void __reg_deduce_bounds(struct bpf_reg_state *reg)\n{\n\t/* Learn sign from signed bounds.\n\t * If we cannot cross the sign boundary, then signed and unsigned bounds\n\t * are the same, so combine.  This works even in the negative case, e.g.\n\t * -3 s<= x s<= -1 implies 0xf...fd u<= x u<= 0xf...ff.\n\t */\n\tif (reg->smin_value >= 0 || reg->smax_value < 0) {\n\t\treg->smin_value = reg->umin_value = max_t(u64, reg->smin_value,\n\t\t\t\t\t\t\t  reg->umin_value);\n\t\treg->smax_value = reg->umax_value = min_t(u64, reg->smax_value,\n\t\t\t\t\t\t\t  reg->umax_value);\n\t\treturn;\n\t}\n\t/* Learn sign from unsigned bounds.  Signed bounds cross the sign\n\t * boundary, so we must be careful.\n\t */\n\tif ((s64)reg->umax_value >= 0) {\n\t\t/* Positive.  We can't learn anything from the smin, but smax\n\t\t * is positive, hence safe.\n\t\t */\n\t\treg->smin_value = reg->umin_value;\n\t\treg->smax_value = reg->umax_value = min_t(u64, reg->smax_value,\n\t\t\t\t\t\t\t  reg->umax_value);\n\t} else if ((s64)reg->umin_value < 0) {\n\t\t/* Negative.  We can't learn anything from the smax, but smin\n\t\t * is negative, hence safe.\n\t\t */\n\t\treg->smin_value = reg->umin_value = max_t(u64, reg->smin_value,\n\t\t\t\t\t\t\t  reg->umin_value);\n\t\treg->smax_value = reg->umax_value;\n\t}\n}\n\n/* Attempts to improve var_off based on unsigned min/max information */\nstatic void __reg_bound_offset(struct bpf_reg_state *reg)\n{\n\treg->var_off = tnum_intersect(reg->var_off,\n\t\t\t\t      tnum_range(reg->umin_value,\n\t\t\t\t\t\t reg->umax_value));\n}\n\n/* Reset the min/max bounds of a register */\nstatic void __mark_reg_unbounded(struct bpf_reg_state *reg)\n{\n\treg->smin_value = S64_MIN;\n\treg->smax_value = S64_MAX;\n\treg->umin_value = 0;\n\treg->umax_value = U64_MAX;\n}\n\n/* Mark a register as having a completely unknown (scalar) value. */\nstatic void __mark_reg_unknown(struct bpf_reg_state *reg)\n{\n\treg->type = SCALAR_VALUE;\n\treg->id = 0;\n\treg->off = 0;\n\treg->var_off = tnum_unknown;\n\t__mark_reg_unbounded(reg);\n}\n\nstatic void mark_reg_unknown(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_unknown(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs */\n\t\tfor (regno = 0; regno < MAX_BPF_REG; regno++)\n\t\t\t__mark_reg_not_init(regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_unknown(regs + regno);\n}\n\nstatic void __mark_reg_not_init(struct bpf_reg_state *reg)\n{\n\t__mark_reg_unknown(reg);\n\treg->type = NOT_INIT;\n}\n\nstatic void mark_reg_not_init(struct bpf_verifier_env *env,\n\t\t\t      struct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_not_init(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs */\n\t\tfor (regno = 0; regno < MAX_BPF_REG; regno++)\n\t\t\t__mark_reg_not_init(regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_not_init(regs + regno);\n}\n\nstatic void init_reg_state(struct bpf_verifier_env *env,\n\t\t\t   struct bpf_reg_state *regs)\n{\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\tmark_reg_not_init(env, regs, i);\n\t\tregs[i].live = REG_LIVE_NONE;\n\t}\n\n\t/* frame pointer */\n\tregs[BPF_REG_FP].type = PTR_TO_STACK;\n\tmark_reg_known_zero(env, regs, BPF_REG_FP);\n\n\t/* 1st arg to a function */\n\tregs[BPF_REG_1].type = PTR_TO_CTX;\n\tmark_reg_known_zero(env, regs, BPF_REG_1);\n}\n\nenum reg_arg_type {\n\tSRC_OP,\t\t/* register is used as source operand */\n\tDST_OP,\t\t/* register is used as destination operand */\n\tDST_OP_NO_MARK\t/* same as above, check only, don't mark */\n};\n\nstatic void mark_reg_read(const struct bpf_verifier_state *state, u32 regno)\n{\n\tstruct bpf_verifier_state *parent = state->parent;\n\n\tif (regno == BPF_REG_FP)\n\t\t/* We don't need to worry about FP liveness because it's read-only */\n\t\treturn;\n\n\twhile (parent) {\n\t\t/* if read wasn't screened by an earlier write ... */\n\t\tif (state->regs[regno].live & REG_LIVE_WRITTEN)\n\t\t\tbreak;\n\t\t/* ... then we depend on parent's value */\n\t\tparent->regs[regno].live |= REG_LIVE_READ;\n\t\tstate = parent;\n\t\tparent = state->parent;\n\t}\n}\n\nstatic int check_reg_arg(struct bpf_verifier_env *env, u32 regno,\n\t\t\t enum reg_arg_type t)\n{\n\tstruct bpf_reg_state *regs = env->cur_state->regs;\n\n\tif (regno >= MAX_BPF_REG) {\n\t\tverbose(env, \"R%d is invalid\\n\", regno);\n\t\treturn -EINVAL;\n\t}\n\n\tif (t == SRC_OP) {\n\t\t/* check whether register used as source operand can be read */\n\t\tif (regs[regno].type == NOT_INIT) {\n\t\t\tverbose(env, \"R%d !read_ok\\n\", regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmark_reg_read(env->cur_state, regno);\n\t} else {\n\t\t/* check whether register used as dest operand can be written to */\n\t\tif (regno == BPF_REG_FP) {\n\t\t\tverbose(env, \"frame pointer is read only\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tregs[regno].live |= REG_LIVE_WRITTEN;\n\t\tif (t == DST_OP)\n\t\t\tmark_reg_unknown(env, regs, regno);\n\t}\n\treturn 0;\n}\n\nstatic bool is_spillable_regtype(enum bpf_reg_type type)\n{\n\tswitch (type) {\n\tcase PTR_TO_MAP_VALUE:\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\tcase PTR_TO_STACK:\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET_END:\n\tcase CONST_PTR_TO_MAP:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* check_stack_read/write functions track spill/fill of registers,\n * stack boundary and alignment are checked in check_mem_access()\n */\nstatic int check_stack_write(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_verifier_state *state, int off,\n\t\t\t     int size, int value_regno)\n{\n\tint i, slot = -off - 1, spi = slot / BPF_REG_SIZE, err;\n\n\terr = realloc_verifier_state(state, round_up(slot + 1, BPF_REG_SIZE),\n\t\t\t\t     true);\n\tif (err)\n\t\treturn err;\n\t/* caller checked that off % size == 0 and -MAX_BPF_STACK <= off < 0,\n\t * so it's aligned access and [off, off + size) are within stack limits\n\t */\n\tif (!env->allow_ptr_leaks &&\n\t    state->stack[spi].slot_type[0] == STACK_SPILL &&\n\t    size != BPF_REG_SIZE) {\n\t\tverbose(env, \"attempt to corrupt spilled pointer on stack\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tif (value_regno >= 0 &&\n\t    is_spillable_regtype(state->regs[value_regno].type)) {\n\n\t\t/* register containing pointer is being spilled into stack */\n\t\tif (size != BPF_REG_SIZE) {\n\t\t\tverbose(env, \"invalid size of register spill\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* save register state */\n\t\tstate->stack[spi].spilled_ptr = state->regs[value_regno];\n\t\tstate->stack[spi].spilled_ptr.live |= REG_LIVE_WRITTEN;\n\n\t\tfor (i = 0; i < BPF_REG_SIZE; i++)\n\t\t\tstate->stack[spi].slot_type[i] = STACK_SPILL;\n\t} else {\n\t\t/* regular write of data into stack */\n\t\tstate->stack[spi].spilled_ptr = (struct bpf_reg_state) {};\n\n\t\tfor (i = 0; i < size; i++)\n\t\t\tstate->stack[spi].slot_type[(slot - i) % BPF_REG_SIZE] =\n\t\t\t\tSTACK_MISC;\n\t}\n\treturn 0;\n}\n\nstatic void mark_stack_slot_read(const struct bpf_verifier_state *state, int slot)\n{\n\tstruct bpf_verifier_state *parent = state->parent;\n\n\twhile (parent) {\n\t\t/* if read wasn't screened by an earlier write ... */\n\t\tif (state->stack[slot].spilled_ptr.live & REG_LIVE_WRITTEN)\n\t\t\tbreak;\n\t\t/* ... then we depend on parent's value */\n\t\tparent->stack[slot].spilled_ptr.live |= REG_LIVE_READ;\n\t\tstate = parent;\n\t\tparent = state->parent;\n\t}\n}\n\nstatic int check_stack_read(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_verifier_state *state, int off, int size,\n\t\t\t    int value_regno)\n{\n\tint i, slot = -off - 1, spi = slot / BPF_REG_SIZE;\n\tu8 *stype;\n\n\tif (state->allocated_stack <= slot) {\n\t\tverbose(env, \"invalid read from stack off %d+0 size %d\\n\",\n\t\t\toff, size);\n\t\treturn -EACCES;\n\t}\n\tstype = state->stack[spi].slot_type;\n\n\tif (stype[0] == STACK_SPILL) {\n\t\tif (size != BPF_REG_SIZE) {\n\t\t\tverbose(env, \"invalid size of register spill\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tfor (i = 1; i < BPF_REG_SIZE; i++) {\n\t\t\tif (stype[(slot - i) % BPF_REG_SIZE] != STACK_SPILL) {\n\t\t\t\tverbose(env, \"corrupted spill memory\\n\");\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\t\t}\n\n\t\tif (value_regno >= 0) {\n\t\t\t/* restore register state from stack */\n\t\t\tstate->regs[value_regno] = state->stack[spi].spilled_ptr;\n\t\t\tmark_stack_slot_read(state, spi);\n\t\t}\n\t\treturn 0;\n\t} else {\n\t\tfor (i = 0; i < size; i++) {\n\t\t\tif (stype[(slot - i) % BPF_REG_SIZE] != STACK_MISC) {\n\t\t\t\tverbose(env, \"invalid read from stack off %d+%d size %d\\n\",\n\t\t\t\t\toff, i, size);\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\t\t}\n\t\tif (value_regno >= 0)\n\t\t\t/* have read misc data from the stack */\n\t\t\tmark_reg_unknown(env, state->regs, value_regno);\n\t\treturn 0;\n\t}\n}\n\n/* check read/write into map element returned by bpf_map_lookup_elem() */\nstatic int __check_map_access(struct bpf_verifier_env *env, u32 regno, int off,\n\t\t\t      int size, bool zero_size_allowed)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_map *map = regs[regno].map_ptr;\n\n\tif (off < 0 || size < 0 || (size == 0 && !zero_size_allowed) ||\n\t    off + size > map->value_size) {\n\t\tverbose(env, \"invalid access to map value, value_size=%d off=%d size=%d\\n\",\n\t\t\tmap->value_size, off, size);\n\t\treturn -EACCES;\n\t}\n\treturn 0;\n}\n\n/* check read/write into a map element with possible variable offset */\nstatic int check_map_access(struct bpf_verifier_env *env, u32 regno,\n\t\t\t    int off, int size, bool zero_size_allowed)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_reg_state *reg = &state->regs[regno];\n\tint err;\n\n\t/* We may have adjusted the register to this map value, so we\n\t * need to try adding each of min_value and max_value to off\n\t * to make sure our theoretical access will be safe.\n\t */\n\tif (env->log.level)\n\t\tprint_verifier_state(env, state);\n\t/* The minimum value is only important with signed\n\t * comparisons where we can't assume the floor of a\n\t * value is 0.  If we are using signed variables for our\n\t * index'es we need to make sure that whatever we use\n\t * will have a set floor within our range.\n\t */\n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_map_access(env, regno, reg->smin_value + off, size,\n\t\t\t\t zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d min value is outside of the array range\\n\",\n\t\t\tregno);\n\t\treturn err;\n\t}\n\n\t/* If we haven't set a max value then we need to bail since we can't be\n\t * sure we won't do bad things.\n\t * If reg->umax_value + off could overflow, treat that as unbounded too.\n\t */\n\tif (reg->umax_value >= BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"R%d unbounded memory access, make sure to bounds check any array access into a map\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_map_access(env, regno, reg->umax_value + off, size,\n\t\t\t\t zero_size_allowed);\n\tif (err)\n\t\tverbose(env, \"R%d max value is outside of the array range\\n\",\n\t\t\tregno);\n\treturn err;\n}\n\n#define MAX_PACKET_OFF 0xffff\n\nstatic bool may_access_direct_pkt_data(struct bpf_verifier_env *env,\n\t\t\t\t       const struct bpf_call_arg_meta *meta,\n\t\t\t\t       enum bpf_access_type t)\n{\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_LWT_IN:\n\tcase BPF_PROG_TYPE_LWT_OUT:\n\t\t/* dst_input() and dst_output() can't write for now */\n\t\tif (t == BPF_WRITE)\n\t\t\treturn false;\n\t\t/* fallthrough */\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\tcase BPF_PROG_TYPE_XDP:\n\tcase BPF_PROG_TYPE_LWT_XMIT:\n\tcase BPF_PROG_TYPE_SK_SKB:\n\t\tif (meta)\n\t\t\treturn meta->pkt_access;\n\n\t\tenv->seen_direct_write = true;\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic int __check_packet_access(struct bpf_verifier_env *env, u32 regno,\n\t\t\t\t int off, int size, bool zero_size_allowed)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\n\tif (off < 0 || size < 0 || (size == 0 && !zero_size_allowed) ||\n\t    (u64)off + size > reg->range) {\n\t\tverbose(env, \"invalid access to packet, off=%d size=%d, R%d(id=%d,off=%d,r=%d)\\n\",\n\t\t\toff, size, regno, reg->id, reg->off, reg->range);\n\t\treturn -EACCES;\n\t}\n\treturn 0;\n}\n\nstatic int check_packet_access(struct bpf_verifier_env *env, u32 regno, int off,\n\t\t\t       int size, bool zero_size_allowed)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\tint err;\n\n\t/* We may have added a variable offset to the packet pointer; but any\n\t * reg->range we have comes after that.  We are only checking the fixed\n\t * offset.\n\t */\n\n\t/* We don't allow negative numbers, because we aren't tracking enough\n\t * detail to prove they're safe.\n\t */\n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_packet_access(env, regno, off, size, zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d offset is outside of the packet\\n\", regno);\n\t\treturn err;\n\t}\n\treturn err;\n}\n\n/* check access to 'struct bpf_context' fields.  Supports fixed offsets only */\nstatic int check_ctx_access(struct bpf_verifier_env *env, int insn_idx, int off, int size,\n\t\t\t    enum bpf_access_type t, enum bpf_reg_type *reg_type)\n{\n\tstruct bpf_insn_access_aux info = {\n\t\t.reg_type = *reg_type,\n\t};\n\n\tif (env->ops->is_valid_access &&\n\t    env->ops->is_valid_access(off, size, t, &info)) {\n\t\t/* A non zero info.ctx_field_size indicates that this field is a\n\t\t * candidate for later verifier transformation to load the whole\n\t\t * field and then apply a mask when accessed with a narrower\n\t\t * access than actual ctx access size. A zero info.ctx_field_size\n\t\t * will only allow for whole field access and rejects any other\n\t\t * type of narrower access.\n\t\t */\n\t\t*reg_type = info.reg_type;\n\n\t\tenv->insn_aux_data[insn_idx].ctx_field_size = info.ctx_field_size;\n\t\t/* remember the offset of last byte accessed in ctx */\n\t\tif (env->prog->aux->max_ctx_offset < off + size)\n\t\t\tenv->prog->aux->max_ctx_offset = off + size;\n\t\treturn 0;\n\t}\n\n\tverbose(env, \"invalid bpf_context access off=%d size=%d\\n\", off, size);\n\treturn -EACCES;\n}\n\nstatic bool __is_pointer_value(bool allow_ptr_leaks,\n\t\t\t       const struct bpf_reg_state *reg)\n{\n\tif (allow_ptr_leaks)\n\t\treturn false;\n\n\treturn reg->type != SCALAR_VALUE;\n}\n\nstatic bool is_pointer_value(struct bpf_verifier_env *env, int regno)\n{\n\treturn __is_pointer_value(env->allow_ptr_leaks, cur_regs(env) + regno);\n}\n\nstatic int check_pkt_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t\t   const struct bpf_reg_state *reg,\n\t\t\t\t   int off, int size, bool strict)\n{\n\tstruct tnum reg_off;\n\tint ip_align;\n\n\t/* Byte size accesses are always allowed. */\n\tif (!strict || size == 1)\n\t\treturn 0;\n\n\t/* For platforms that do not have a Kconfig enabling\n\t * CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS the value of\n\t * NET_IP_ALIGN is universally set to '2'.  And on platforms\n\t * that do set CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS, we get\n\t * to this code only in strict mode where we want to emulate\n\t * the NET_IP_ALIGN==2 checking.  Therefore use an\n\t * unconditional IP align value of '2'.\n\t */\n\tip_align = 2;\n\n\treg_off = tnum_add(reg->var_off, tnum_const(ip_align + reg->off + off));\n\tif (!tnum_is_aligned(reg_off, size)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env,\n\t\t\t\"misaligned packet access off %d+%s+%d+%d size %d\\n\",\n\t\t\tip_align, tn_buf, reg->off, off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_generic_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t\t       const struct bpf_reg_state *reg,\n\t\t\t\t       const char *pointer_desc,\n\t\t\t\t       int off, int size, bool strict)\n{\n\tstruct tnum reg_off;\n\n\t/* Byte size accesses are always allowed. */\n\tif (!strict || size == 1)\n\t\treturn 0;\n\n\treg_off = tnum_add(reg->var_off, tnum_const(reg->off + off));\n\tif (!tnum_is_aligned(reg_off, size)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"misaligned %saccess off %s+%d+%d size %d\\n\",\n\t\t\tpointer_desc, tn_buf, reg->off, off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t       const struct bpf_reg_state *reg,\n\t\t\t       int off, int size)\n{\n\tbool strict = env->strict_alignment;\n\tconst char *pointer_desc = \"\";\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\t/* Special case, because of NET_IP_ALIGN. Given metadata sits\n\t\t * right in front, treat it the very same way.\n\t\t */\n\t\treturn check_pkt_ptr_alignment(env, reg, off, size, strict);\n\tcase PTR_TO_MAP_VALUE:\n\t\tpointer_desc = \"value \";\n\t\tbreak;\n\tcase PTR_TO_CTX:\n\t\tpointer_desc = \"context \";\n\t\tbreak;\n\tcase PTR_TO_STACK:\n\t\tpointer_desc = \"stack \";\n\t\t/* The stack spill tracking logic in check_stack_write()\n\t\t * and check_stack_read() relies on stack accesses being\n\t\t * aligned.\n\t\t */\n\t\tstrict = true;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn check_generic_ptr_alignment(env, reg, pointer_desc, off, size,\n\t\t\t\t\t   strict);\n}\n\n/* truncate register to smaller size (in bytes)\n * must be called with size < BPF_REG_SIZE\n */\nstatic void coerce_reg_to_size(struct bpf_reg_state *reg, int size)\n{\n\tu64 mask;\n\n\t/* clear high bits in bit representation */\n\treg->var_off = tnum_cast(reg->var_off, size);\n\n\t/* fix arithmetic bounds */\n\tmask = ((u64)1 << (size * 8)) - 1;\n\tif ((reg->umin_value & ~mask) == (reg->umax_value & ~mask)) {\n\t\treg->umin_value &= mask;\n\t\treg->umax_value &= mask;\n\t} else {\n\t\treg->umin_value = 0;\n\t\treg->umax_value = mask;\n\t}\n\treg->smin_value = reg->umin_value;\n\treg->smax_value = reg->umax_value;\n}\n\n/* check whether memory at (regno + off) is accessible for t = (read | write)\n * if t==write, value_regno is a register which value is stored into memory\n * if t==read, value_regno is a register which will receive the value from memory\n * if t==write && value_regno==-1, some unknown value is stored into memory\n * if t==read && value_regno==-1, don't care what we read from memory\n */\nstatic int check_mem_access(struct bpf_verifier_env *env, int insn_idx, u32 regno, int off,\n\t\t\t    int bpf_size, enum bpf_access_type t,\n\t\t\t    int value_regno)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = regs + regno;\n\tint size, err = 0;\n\n\tsize = bpf_size_to_bytes(bpf_size);\n\tif (size < 0)\n\t\treturn size;\n\n\t/* alignment checks will add in reg->off themselves */\n\terr = check_ptr_alignment(env, reg, off, size);\n\tif (err)\n\t\treturn err;\n\n\t/* for access checks, reg->off is just part of off */\n\toff += reg->off;\n\n\tif (reg->type == PTR_TO_MAP_VALUE) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into map\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_map_access(env, regno, off, size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\n\t} else if (reg->type == PTR_TO_CTX) {\n\t\tenum bpf_reg_type reg_type = SCALAR_VALUE;\n\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into ctx\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* ctx accesses must be at a fixed offset, so that we can\n\t\t * determine what type of data were returned.\n\t\t */\n\t\tif (reg->off) {\n\t\t\tverbose(env,\n\t\t\t\t\"dereference of modified ctx ptr R%d off=%d+%d, ctx+const is allowed, ctx+const+const is not\\n\",\n\t\t\t\tregno, reg->off, off - reg->off);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (!tnum_is_const(reg->var_off) || reg->var_off.value) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env,\n\t\t\t\t\"variable ctx access var_off=%s off=%d size=%d\",\n\t\t\t\ttn_buf, off, size);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_ctx_access(env, insn_idx, off, size, t, &reg_type);\n\t\tif (!err && t == BPF_READ && value_regno >= 0) {\n\t\t\t/* ctx access returns either a scalar, or a\n\t\t\t * PTR_TO_PACKET[_META,_END]. In the latter\n\t\t\t * case, we know the offset is zero.\n\t\t\t */\n\t\t\tif (reg_type == SCALAR_VALUE)\n\t\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t\t\telse\n\t\t\t\tmark_reg_known_zero(env, regs,\n\t\t\t\t\t\t    value_regno);\n\t\t\tregs[value_regno].id = 0;\n\t\t\tregs[value_regno].off = 0;\n\t\t\tregs[value_regno].range = 0;\n\t\t\tregs[value_regno].type = reg_type;\n\t\t}\n\n\t} else if (reg->type == PTR_TO_STACK) {\n\t\t/* stack accesses must be at a fixed offset, so that we can\n\t\t * determine what type of data were returned.\n\t\t * See check_stack_read().\n\t\t */\n\t\tif (!tnum_is_const(reg->var_off)) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"variable stack access var_off=%s off=%d size=%d\",\n\t\t\t\ttn_buf, off, size);\n\t\t\treturn -EACCES;\n\t\t}\n\t\toff += reg->var_off.value;\n\t\tif (off >= 0 || off < -MAX_BPF_STACK) {\n\t\t\tverbose(env, \"invalid stack off=%d size=%d\\n\", off,\n\t\t\t\tsize);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (env->prog->aux->stack_depth < -off)\n\t\t\tenv->prog->aux->stack_depth = -off;\n\n\t\tif (t == BPF_WRITE)\n\t\t\terr = check_stack_write(env, state, off, size,\n\t\t\t\t\t\tvalue_regno);\n\t\telse\n\t\t\terr = check_stack_read(env, state, off, size,\n\t\t\t\t\t       value_regno);\n\t} else if (reg_is_pkt_pointer(reg)) {\n\t\tif (t == BPF_WRITE && !may_access_direct_pkt_data(env, NULL, t)) {\n\t\t\tverbose(env, \"cannot write into packet\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into packet\\n\",\n\t\t\t\tvalue_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_packet_access(env, regno, off, size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else {\n\t\tverbose(env, \"R%d invalid mem access '%s'\\n\", regno,\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!err && size < BPF_REG_SIZE && value_regno >= 0 && t == BPF_READ &&\n\t    regs[value_regno].type == SCALAR_VALUE) {\n\t\t/* b/h/w load zero-extends, mark upper bits as known 0 */\n\t\tcoerce_reg_to_size(&regs[value_regno], size);\n\t}\n\treturn err;\n}\n\nstatic int check_xadd(struct bpf_verifier_env *env, int insn_idx, struct bpf_insn *insn)\n{\n\tint err;\n\n\tif ((BPF_SIZE(insn->code) != BPF_W && BPF_SIZE(insn->code) != BPF_DW) ||\n\t    insn->imm != 0) {\n\t\tverbose(env, \"BPF_XADD uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check src1 operand */\n\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, insn->src_reg)) {\n\t\tverbose(env, \"R%d leaks addr into mem\\n\", insn->src_reg);\n\t\treturn -EACCES;\n\t}\n\n\t/* check whether atomic_add can read the memory */\n\terr = check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t       BPF_SIZE(insn->code), BPF_READ, -1);\n\tif (err)\n\t\treturn err;\n\n\t/* check whether atomic_add can write into the same memory */\n\treturn check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t\tBPF_SIZE(insn->code), BPF_WRITE, -1);\n}\n\n/* Does this register contain a constant zero? */\nstatic bool register_is_null(struct bpf_reg_state reg)\n{\n\treturn reg.type == SCALAR_VALUE && tnum_equals_const(reg.var_off, 0);\n}\n\n/* when register 'regno' is passed into function that will read 'access_size'\n * bytes from that pointer, make sure that it's within stack boundary\n * and all elements of stack are initialized.\n * Unlike most pointer bounds-checking functions, this one doesn't take an\n * 'off' argument, so it has to add in reg->off itself.\n */\nstatic int check_stack_boundary(struct bpf_verifier_env *env, int regno,\n\t\t\t\tint access_size, bool zero_size_allowed,\n\t\t\t\tstruct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_reg_state *regs = state->regs;\n\tint off, i, slot, spi;\n\n\tif (regs[regno].type != PTR_TO_STACK) {\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(regs[regno]))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\t\treg_type_str[regs[regno].type],\n\t\t\treg_type_str[PTR_TO_STACK]);\n\t\treturn -EACCES;\n\t}\n\n\t/* Only allow fixed-offset stack reads */\n\tif (!tnum_is_const(regs[regno].var_off)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), regs[regno].var_off);\n\t\tverbose(env, \"invalid variable stack read R%d var_off=%s\\n\",\n\t\t\tregno, tn_buf);\n\t\treturn -EACCES;\n\t}\n\toff = regs[regno].off + regs[regno].var_off.value;\n\tif (off >= 0 || off < -MAX_BPF_STACK || off + access_size > 0 ||\n\t    access_size < 0 || (access_size == 0 && !zero_size_allowed)) {\n\t\tverbose(env, \"invalid stack type R%d off=%d access_size=%d\\n\",\n\t\t\tregno, off, access_size);\n\t\treturn -EACCES;\n\t}\n\n\tif (env->prog->aux->stack_depth < -off)\n\t\tenv->prog->aux->stack_depth = -off;\n\n\tif (meta && meta->raw_mode) {\n\t\tmeta->access_size = access_size;\n\t\tmeta->regno = regno;\n\t\treturn 0;\n\t}\n\n\tfor (i = 0; i < access_size; i++) {\n\t\tslot = -(off + i) - 1;\n\t\tspi = slot / BPF_REG_SIZE;\n\t\tif (state->allocated_stack <= slot ||\n\t\t    state->stack[spi].slot_type[slot % BPF_REG_SIZE] !=\n\t\t\tSTACK_MISC) {\n\t\t\tverbose(env, \"invalid indirect read from stack off %d+%d size %d\\n\",\n\t\t\t\toff, i, access_size);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_VALUE:\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tdefault: /* scalar_value|ptr_to_stack or invalid ptr */\n\t\treturn check_stack_boundary(env, regno, access_size,\n\t\t\t\t\t    zero_size_allowed, meta);\n\t}\n}\n\nstatic int check_func_arg(struct bpf_verifier_env *env, u32 regno,\n\t\t\t  enum bpf_arg_type arg_type,\n\t\t\t  struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\tenum bpf_reg_type expected_type, type = reg->type;\n\tint err = 0;\n\n\tif (arg_type == ARG_DONTCARE)\n\t\treturn 0;\n\n\terr = check_reg_arg(env, regno, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (arg_type == ARG_ANYTHING) {\n\t\tif (is_pointer_value(env, regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into helper function\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (type_is_pkt_pointer(type) &&\n\t    !may_access_direct_pkt_data(env, meta, BPF_READ)) {\n\t\tverbose(env, \"helper access to the packet is not allowed\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tif (arg_type == ARG_PTR_TO_MAP_KEY ||\n\t    arg_type == ARG_PTR_TO_MAP_VALUE) {\n\t\texpected_type = PTR_TO_STACK;\n\t\tif (!type_is_pkt_pointer(type) &&\n\t\t    type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_CONST_SIZE ||\n\t\t   arg_type == ARG_CONST_SIZE_OR_ZERO) {\n\t\texpected_type = SCALAR_VALUE;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_CONST_MAP_PTR) {\n\t\texpected_type = CONST_PTR_TO_MAP;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_PTR_TO_CTX) {\n\t\texpected_type = PTR_TO_CTX;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_PTR_TO_MEM ||\n\t\t   arg_type == ARG_PTR_TO_MEM_OR_NULL ||\n\t\t   arg_type == ARG_PTR_TO_UNINIT_MEM) {\n\t\texpected_type = PTR_TO_STACK;\n\t\t/* One exception here. In case function allows for NULL to be\n\t\t * passed in as argument, it's a SCALAR_VALUE type. Final test\n\t\t * happens during stack boundary checking.\n\t\t */\n\t\tif (register_is_null(*reg) &&\n\t\t    arg_type == ARG_PTR_TO_MEM_OR_NULL)\n\t\t\t/* final test in check_stack_boundary() */;\n\t\telse if (!type_is_pkt_pointer(type) &&\n\t\t\t type != PTR_TO_MAP_VALUE &&\n\t\t\t type != expected_type)\n\t\t\tgoto err_type;\n\t\tmeta->raw_mode = arg_type == ARG_PTR_TO_UNINIT_MEM;\n\t} else {\n\t\tverbose(env, \"unsupported arg_type %d\\n\", arg_type);\n\t\treturn -EFAULT;\n\t}\n\n\tif (arg_type == ARG_CONST_MAP_PTR) {\n\t\t/* bpf_map_xxx(map_ptr) call: remember that map_ptr */\n\t\tmeta->map_ptr = reg->map_ptr;\n\t} else if (arg_type == ARG_PTR_TO_MAP_KEY) {\n\t\t/* bpf_map_xxx(..., map_ptr, ..., key) call:\n\t\t * check that [key, key + map->key_size) are within\n\t\t * stack limits and initialized\n\t\t */\n\t\tif (!meta->map_ptr) {\n\t\t\t/* in function declaration map_ptr must come before\n\t\t\t * map_key, so that it's verified and known before\n\t\t\t * we have to check map_key here. Otherwise it means\n\t\t\t * that kernel subsystem misconfigured verifier\n\t\t\t */\n\t\t\tverbose(env, \"invalid map_ptr to access map->key\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (type_is_pkt_pointer(type))\n\t\t\terr = check_packet_access(env, regno, reg->off,\n\t\t\t\t\t\t  meta->map_ptr->key_size,\n\t\t\t\t\t\t  false);\n\t\telse\n\t\t\terr = check_stack_boundary(env, regno,\n\t\t\t\t\t\t   meta->map_ptr->key_size,\n\t\t\t\t\t\t   false, NULL);\n\t} else if (arg_type == ARG_PTR_TO_MAP_VALUE) {\n\t\t/* bpf_map_xxx(..., map_ptr, ..., value) call:\n\t\t * check [value, value + map->value_size) validity\n\t\t */\n\t\tif (!meta->map_ptr) {\n\t\t\t/* kernel subsystem misconfigured verifier */\n\t\t\tverbose(env, \"invalid map_ptr to access map->value\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (type_is_pkt_pointer(type))\n\t\t\terr = check_packet_access(env, regno, reg->off,\n\t\t\t\t\t\t  meta->map_ptr->value_size,\n\t\t\t\t\t\t  false);\n\t\telse\n\t\t\terr = check_stack_boundary(env, regno,\n\t\t\t\t\t\t   meta->map_ptr->value_size,\n\t\t\t\t\t\t   false, NULL);\n\t} else if (arg_type == ARG_CONST_SIZE ||\n\t\t   arg_type == ARG_CONST_SIZE_OR_ZERO) {\n\t\tbool zero_size_allowed = (arg_type == ARG_CONST_SIZE_OR_ZERO);\n\n\t\t/* bpf_xxx(..., buf, len) call will access 'len' bytes\n\t\t * from stack pointer 'buf'. Check it\n\t\t * note: regno == len, regno - 1 == buf\n\t\t */\n\t\tif (regno == 0) {\n\t\t\t/* kernel subsystem misconfigured verifier */\n\t\t\tverbose(env,\n\t\t\t\t\"ARG_CONST_SIZE cannot be first argument\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* The register is SCALAR_VALUE; the access check\n\t\t * happens using its boundaries.\n\t\t */\n\n\t\tif (!tnum_is_const(reg->var_off))\n\t\t\t/* For unprivileged variable accesses, disable raw\n\t\t\t * mode so that the program is required to\n\t\t\t * initialize all the memory that the helper could\n\t\t\t * just partially fill up.\n\t\t\t */\n\t\t\tmeta = NULL;\n\n\t\tif (reg->smin_value < 0) {\n\t\t\tverbose(env, \"R%d min value is negative, either use unsigned or 'var &= const'\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (reg->umin_value == 0) {\n\t\t\terr = check_helper_mem_access(env, regno - 1, 0,\n\t\t\t\t\t\t      zero_size_allowed,\n\t\t\t\t\t\t      meta);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tif (reg->umax_value >= BPF_MAX_VAR_SIZ) {\n\t\t\tverbose(env, \"R%d unbounded memory access, use 'var &= const' or 'if (var < const)'\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_helper_mem_access(env, regno - 1,\n\t\t\t\t\t      reg->umax_value,\n\t\t\t\t\t      zero_size_allowed, meta);\n\t}\n\n\treturn err;\nerr_type:\n\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\treg_type_str[type], reg_type_str[expected_type]);\n\treturn -EACCES;\n}\n\nstatic int check_map_func_compatibility(struct bpf_verifier_env *env,\n\t\t\t\t\tstruct bpf_map *map, int func_id)\n{\n\tif (!map)\n\t\treturn 0;\n\n\t/* We need a two way check, first is from map perspective ... */\n\tswitch (map->map_type) {\n\tcase BPF_MAP_TYPE_PROG_ARRAY:\n\t\tif (func_id != BPF_FUNC_tail_call)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_PERF_EVENT_ARRAY:\n\t\tif (func_id != BPF_FUNC_perf_event_read &&\n\t\t    func_id != BPF_FUNC_perf_event_output &&\n\t\t    func_id != BPF_FUNC_perf_event_read_value)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_STACK_TRACE:\n\t\tif (func_id != BPF_FUNC_get_stackid)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_CGROUP_ARRAY:\n\t\tif (func_id != BPF_FUNC_skb_under_cgroup &&\n\t\t    func_id != BPF_FUNC_current_task_under_cgroup)\n\t\t\tgoto error;\n\t\tbreak;\n\t/* devmap returns a pointer to a live net_device ifindex that we cannot\n\t * allow to be modified from bpf side. So do not allow lookup elements\n\t * for now.\n\t */\n\tcase BPF_MAP_TYPE_DEVMAP:\n\t\tif (func_id != BPF_FUNC_redirect_map)\n\t\t\tgoto error;\n\t\tbreak;\n\t/* Restrict bpf side of cpumap, open when use-cases appear */\n\tcase BPF_MAP_TYPE_CPUMAP:\n\t\tif (func_id != BPF_FUNC_redirect_map)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_ARRAY_OF_MAPS:\n\tcase BPF_MAP_TYPE_HASH_OF_MAPS:\n\t\tif (func_id != BPF_FUNC_map_lookup_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_SOCKMAP:\n\t\tif (func_id != BPF_FUNC_sk_redirect_map &&\n\t\t    func_id != BPF_FUNC_sock_map_update &&\n\t\t    func_id != BPF_FUNC_map_delete_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* ... and second from the function itself. */\n\tswitch (func_id) {\n\tcase BPF_FUNC_tail_call:\n\t\tif (map->map_type != BPF_MAP_TYPE_PROG_ARRAY)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_perf_event_read:\n\tcase BPF_FUNC_perf_event_output:\n\tcase BPF_FUNC_perf_event_read_value:\n\t\tif (map->map_type != BPF_MAP_TYPE_PERF_EVENT_ARRAY)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_get_stackid:\n\t\tif (map->map_type != BPF_MAP_TYPE_STACK_TRACE)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_current_task_under_cgroup:\n\tcase BPF_FUNC_skb_under_cgroup:\n\t\tif (map->map_type != BPF_MAP_TYPE_CGROUP_ARRAY)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_redirect_map:\n\t\tif (map->map_type != BPF_MAP_TYPE_DEVMAP &&\n\t\t    map->map_type != BPF_MAP_TYPE_CPUMAP)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_redirect_map:\n\t\tif (map->map_type != BPF_MAP_TYPE_SOCKMAP)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sock_map_update:\n\t\tif (map->map_type != BPF_MAP_TYPE_SOCKMAP)\n\t\t\tgoto error;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\nerror:\n\tverbose(env, \"cannot pass map_type %d into func %s#%d\\n\",\n\t\tmap->map_type, func_id_name(func_id), func_id);\n\treturn -EINVAL;\n}\n\nstatic int check_raw_mode(const struct bpf_func_proto *fn)\n{\n\tint count = 0;\n\n\tif (fn->arg1_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg2_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg3_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg4_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg5_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\n\treturn count > 1 ? -EINVAL : 0;\n}\n\n/* Packet data might have moved, any old PTR_TO_PACKET[_META,_END]\n * are now invalid, so turn them into unknown SCALAR_VALUE.\n */\nstatic void clear_all_pkt_pointers(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (reg_is_pkt_pointer_any(&regs[i]))\n\t\t\tmark_reg_unknown(env, regs, i);\n\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tif (state->stack[i].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\treg = &state->stack[i].spilled_ptr;\n\t\tif (reg_is_pkt_pointer_any(reg))\n\t\t\t__mark_reg_unknown(reg);\n\t}\n}\n\nstatic int check_call(struct bpf_verifier_env *env, int func_id, int insn_idx)\n{\n\tconst struct bpf_func_proto *fn = NULL;\n\tstruct bpf_reg_state *regs;\n\tstruct bpf_call_arg_meta meta;\n\tbool changes_data;\n\tint i, err;\n\n\t/* find function prototype */\n\tif (func_id < 0 || func_id >= __BPF_FUNC_MAX_ID) {\n\t\tverbose(env, \"invalid func %s#%d\\n\", func_id_name(func_id),\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->ops->get_func_proto)\n\t\tfn = env->ops->get_func_proto(func_id);\n\n\tif (!fn) {\n\t\tverbose(env, \"unknown func %s#%d\\n\", func_id_name(func_id),\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\t/* eBPF programs must be GPL compatible to use GPL-ed functions */\n\tif (!env->prog->gpl_compatible && fn->gpl_only) {\n\t\tverbose(env, \"cannot call GPL only function from proprietary program\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* With LD_ABS/IND some JITs save/restore skb from r1. */\n\tchanges_data = bpf_helper_changes_pkt_data(fn->func);\n\tif (changes_data && fn->arg1_type != ARG_PTR_TO_CTX) {\n\t\tverbose(env, \"kernel subsystem misconfigured func %s#%d: r1 != ctx\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn -EINVAL;\n\t}\n\n\tmemset(&meta, 0, sizeof(meta));\n\tmeta.pkt_access = fn->pkt_access;\n\n\t/* We only support one arg being in raw mode at the moment, which\n\t * is sufficient for the helper functions we have right now.\n\t */\n\terr = check_raw_mode(fn);\n\tif (err) {\n\t\tverbose(env, \"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn err;\n\t}\n\n\t/* check args */\n\terr = check_func_arg(env, BPF_REG_1, fn->arg1_type, &meta);\n\tif (err)\n\t\treturn err;\n\terr = check_func_arg(env, BPF_REG_2, fn->arg2_type, &meta);\n\tif (err)\n\t\treturn err;\n\terr = check_func_arg(env, BPF_REG_3, fn->arg3_type, &meta);\n\tif (err)\n\t\treturn err;\n\terr = check_func_arg(env, BPF_REG_4, fn->arg4_type, &meta);\n\tif (err)\n\t\treturn err;\n\terr = check_func_arg(env, BPF_REG_5, fn->arg5_type, &meta);\n\tif (err)\n\t\treturn err;\n\n\t/* Mark slots with STACK_MISC in case of raw mode, stack offset\n\t * is inferred from register state.\n\t */\n\tfor (i = 0; i < meta.access_size; i++) {\n\t\terr = check_mem_access(env, insn_idx, meta.regno, i, BPF_B, BPF_WRITE, -1);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tregs = cur_regs(env);\n\t/* reset caller saved regs */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* update return register (already marked as written above) */\n\tif (fn->ret_type == RET_INTEGER) {\n\t\t/* sets type to SCALAR_VALUE */\n\t\tmark_reg_unknown(env, regs, BPF_REG_0);\n\t} else if (fn->ret_type == RET_VOID) {\n\t\tregs[BPF_REG_0].type = NOT_INIT;\n\t} else if (fn->ret_type == RET_PTR_TO_MAP_VALUE_OR_NULL) {\n\t\tstruct bpf_insn_aux_data *insn_aux;\n\n\t\tregs[BPF_REG_0].type = PTR_TO_MAP_VALUE_OR_NULL;\n\t\t/* There is no offset yet applied, variable or fixed */\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].off = 0;\n\t\t/* remember map_ptr, so that check_map_access()\n\t\t * can check 'value_size' boundary of memory access\n\t\t * to map element returned from bpf_map_lookup_elem()\n\t\t */\n\t\tif (meta.map_ptr == NULL) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured verifier\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tregs[BPF_REG_0].map_ptr = meta.map_ptr;\n\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t\tinsn_aux = &env->insn_aux_data[insn_idx];\n\t\tif (!insn_aux->map_ptr)\n\t\t\tinsn_aux->map_ptr = meta.map_ptr;\n\t\telse if (insn_aux->map_ptr != meta.map_ptr)\n\t\t\tinsn_aux->map_ptr = BPF_MAP_PTR_POISON;\n\t} else {\n\t\tverbose(env, \"unknown return type %d of func %s#%d\\n\",\n\t\t\tfn->ret_type, func_id_name(func_id), func_id);\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_map_func_compatibility(env, meta.map_ptr, func_id);\n\tif (err)\n\t\treturn err;\n\n\tif (changes_data)\n\t\tclear_all_pkt_pointers(env);\n\treturn 0;\n}\n\nstatic bool signed_add_overflows(s64 a, s64 b)\n{\n\t/* Do the add in u64, where overflow is well-defined */\n\ts64 res = (s64)((u64)a + (u64)b);\n\n\tif (b < 0)\n\t\treturn res > a;\n\treturn res < a;\n}\n\nstatic bool signed_sub_overflows(s64 a, s64 b)\n{\n\t/* Do the sub in u64, where overflow is well-defined */\n\ts64 res = (s64)((u64)a - (u64)b);\n\n\tif (b < 0)\n\t\treturn res < a;\n\treturn res > a;\n}\n\n/* Handles arithmetic on a pointer and a scalar: computes new min/max and var_off.\n * Caller should also handle BPF_MOV case separately.\n * If we return -EACCES, caller may want to try again treating pointer as a\n * scalar.  So we only emit a diagnostic if !env->allow_ptr_leaks.\n */\nstatic int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\n\tdst_reg = &regs[dst];\n\n\tif (WARN_ON_ONCE(known && (smin_val != smax_val))) {\n\t\tprint_verifier_state(env, env->cur_state);\n\t\tverbose(env,\n\t\t\t\"verifier internal error: known but bad sbounds\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON_ONCE(known && (umin_val != umax_val))) {\n\t\tprint_verifier_state(env, env->cur_state);\n\t\tverbose(env,\n\t\t\t\"verifier internal error: known but bad ubounds\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (!env->allow_ptr_leaks)\n\t\t\tverbose(env,\n\t\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tif (ptr_reg->type == PTR_TO_MAP_VALUE_OR_NULL) {\n\t\tif (!env->allow_ptr_leaks)\n\t\t\tverbose(env, \"R%d pointer arithmetic on PTR_TO_MAP_VALUE_OR_NULL prohibited, null-check it first\\n\",\n\t\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\tif (ptr_reg->type == CONST_PTR_TO_MAP) {\n\t\tif (!env->allow_ptr_leaks)\n\t\t\tverbose(env, \"R%d pointer arithmetic on CONST_PTR_TO_MAP prohibited\\n\",\n\t\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\tif (ptr_reg->type == PTR_TO_PACKET_END) {\n\t\tif (!env->allow_ptr_leaks)\n\t\t\tverbose(env, \"R%d pointer arithmetic on PTR_TO_PACKET_END prohibited\\n\",\n\t\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->range = ptr_reg->range;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tdst_reg->range = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tif (!env->allow_ptr_leaks)\n\t\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tif (!env->allow_ptr_leaks)\n\t\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->range = ptr_reg->range;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tdst_reg->range = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit for now.\n\t\t * (However, in principle we could allow some cases, e.g.\n\t\t * ptr &= ~3 which would reduce min_value by 3.)\n\t\t */\n\t\tif (!env->allow_ptr_leaks)\n\t\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tif (!env->allow_ptr_leaks)\n\t\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}\n\n/* WARNING: This function does calculations on 64-bit values, but the actual\n * execution may occur on 32-bit values. Therefore, things like bitshifts\n * need extra checks in the 32-bit case.\n */\nstatic int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known, dst_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\n\tsmin_val = src_reg.smin_value;\n\tsmax_val = src_reg.smax_value;\n\tumin_val = src_reg.umin_value;\n\tumax_val = src_reg.umax_value;\n\tsrc_known = tnum_is_const(src_reg.var_off);\n\tdst_known = tnum_is_const(dst_reg->var_off);\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n\t\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value += smin_val;\n\t\t\tdst_reg->smax_value += smax_val;\n\t\t}\n\t\tif (dst_reg->umin_value + umin_val < umin_val ||\n\t\t    dst_reg->umax_value + umax_val < umax_val) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value += umin_val;\n\t\t\tdst_reg->umax_value += umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n\t\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value -= smax_val;\n\t\t\tdst_reg->smax_value -= smin_val;\n\t\t}\n\t\tif (dst_reg->umin_value < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value -= umax_val;\n\t\t\tdst_reg->umax_value -= umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_MUL:\n\t\tdst_reg->var_off = tnum_mul(dst_reg->var_off, src_reg.var_off);\n\t\tif (smin_val < 0 || dst_reg->smin_value < 0) {\n\t\t\t/* Ain't nobody got time to multiply that sign */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* Both values are positive, so we can work with unsigned and\n\t\t * copy the result to signed (unless it exceeds S64_MAX).\n\t\t */\n\t\tif (umax_val > U32_MAX || dst_reg->umax_value > U32_MAX) {\n\t\t\t/* Potential overflow, we know nothing */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t/* (except what we can learn from the var_off) */\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tdst_reg->umin_value *= umin_val;\n\t\tdst_reg->umax_value *= umax_val;\n\t\tif (dst_reg->umax_value > S64_MAX) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value &\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our minimum from the var_off, since that's inherently\n\t\t * bitwise.  Our maximum is the minimum of the operands' maxima.\n\t\t */\n\t\tdst_reg->var_off = tnum_and(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = dst_reg->var_off.value;\n\t\tdst_reg->umax_value = min(dst_reg->umax_value, umax_val);\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ANDing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ANDing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_OR:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value |\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our maximum from the var_off, and our minimum is the\n\t\t * maximum of the operands' minima\n\t\t */\n\t\tdst_reg->var_off = tnum_or(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = max(dst_reg->umin_value, umin_val);\n\t\tdst_reg->umax_value = dst_reg->var_off.value |\n\t\t\t\t      dst_reg->var_off.mask;\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ORing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_LSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* We lose all sign bit information (except what we can pick\n\t\t * up from var_off)\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\t/* If we might shift our top bit out, then we know nothing */\n\t\tif (dst_reg->umax_value > 1ULL << (63 - umax_val)) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value <<= umin_val;\n\t\t\tdst_reg->umax_value <<= umax_val;\n\t\t}\n\t\tif (src_known)\n\t\t\tdst_reg->var_off = tnum_lshift(dst_reg->var_off, umin_val);\n\t\telse\n\t\t\tdst_reg->var_off = tnum_lshift(tnum_unknown, umin_val);\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_RSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* BPF_RSH is an unsigned shift.  If the value in dst_reg might\n\t\t * be negative, then either:\n\t\t * 1) src_reg might be zero, so the sign bit of the result is\n\t\t *    unknown, so we lose our signed bounds\n\t\t * 2) it's known negative, thus the unsigned bounds capture the\n\t\t *    signed bounds\n\t\t * 3) the signed bounds cross zero, so they tell us nothing\n\t\t *    about the result\n\t\t * If the value in dst_reg is known nonnegative, then again the\n\t\t * unsigned bounts capture the signed bounds.\n\t\t * Thus, in all cases it suffices to blow away our signed bounds\n\t\t * and rely on inferring new ones from the unsigned bounds and\n\t\t * var_off of the result.\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\tif (src_known)\n\t\t\tdst_reg->var_off = tnum_rshift(dst_reg->var_off,\n\t\t\t\t\t\t       umin_val);\n\t\telse\n\t\t\tdst_reg->var_off = tnum_rshift(tnum_unknown, umin_val);\n\t\tdst_reg->umin_value >>= umax_val;\n\t\tdst_reg->umax_value >>= umin_val;\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tdefault:\n\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\tbreak;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops are (32,32)->32 */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\tcoerce_reg_to_size(&src_reg, 4);\n\t}\n\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}\n\n/* Handles ALU ops other than BPF_END, BPF_NEG and BPF_MOV: computes new min/max\n * and var_off.\n */\nstatic int adjust_reg_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *dst_reg, *src_reg;\n\tstruct bpf_reg_state *ptr_reg = NULL, off_reg = {0};\n\tu8 opcode = BPF_OP(insn->code);\n\tint rc;\n\n\tdst_reg = &regs[insn->dst_reg];\n\tsrc_reg = NULL;\n\tif (dst_reg->type != SCALAR_VALUE)\n\t\tptr_reg = dst_reg;\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (src_reg->type != SCALAR_VALUE) {\n\t\t\tif (dst_reg->type != SCALAR_VALUE) {\n\t\t\t\t/* Combining two pointers by any ALU op yields\n\t\t\t\t * an arbitrary scalar.\n\t\t\t\t */\n\t\t\t\tif (!env->allow_ptr_leaks) {\n\t\t\t\t\tverbose(env, \"R%d pointer %s pointer prohibited\\n\",\n\t\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\t\tbpf_alu_string[opcode >> 4]);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\treturn 0;\n\t\t\t} else {\n\t\t\t\t/* scalar += pointer\n\t\t\t\t * This is legal, but we have to reverse our\n\t\t\t\t * src/dest handling in computing the range\n\t\t\t\t */\n\t\t\t\trc = adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t\t     src_reg, dst_reg);\n\t\t\t\tif (rc == -EACCES && env->allow_ptr_leaks) {\n\t\t\t\t\t/* scalar += unknown scalar */\n\t\t\t\t\t__mark_reg_unknown(&off_reg);\n\t\t\t\t\treturn adjust_scalar_min_max_vals(\n\t\t\t\t\t\t\tenv, insn,\n\t\t\t\t\t\t\tdst_reg, off_reg);\n\t\t\t\t}\n\t\t\t\treturn rc;\n\t\t\t}\n\t\t} else if (ptr_reg) {\n\t\t\t/* pointer += scalar */\n\t\t\trc = adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t     dst_reg, src_reg);\n\t\t\tif (rc == -EACCES && env->allow_ptr_leaks) {\n\t\t\t\t/* unknown scalar += scalar */\n\t\t\t\t__mark_reg_unknown(dst_reg);\n\t\t\t\treturn adjust_scalar_min_max_vals(\n\t\t\t\t\t\tenv, insn, dst_reg, *src_reg);\n\t\t\t}\n\t\t\treturn rc;\n\t\t}\n\t} else {\n\t\t/* Pretend the src is a reg with a known value, since we only\n\t\t * need to be able to read from this state.\n\t\t */\n\t\toff_reg.type = SCALAR_VALUE;\n\t\t__mark_reg_known(&off_reg, insn->imm);\n\t\tsrc_reg = &off_reg;\n\t\tif (ptr_reg) { /* pointer += K */\n\t\t\trc = adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t     ptr_reg, src_reg);\n\t\t\tif (rc == -EACCES && env->allow_ptr_leaks) {\n\t\t\t\t/* unknown scalar += K */\n\t\t\t\t__mark_reg_unknown(dst_reg);\n\t\t\t\treturn adjust_scalar_min_max_vals(\n\t\t\t\t\t\tenv, insn, dst_reg, off_reg);\n\t\t\t}\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\t/* Got here implies adding two SCALAR_VALUEs */\n\tif (WARN_ON_ONCE(ptr_reg)) {\n\t\tprint_verifier_state(env, env->cur_state);\n\t\tverbose(env, \"verifier internal error: unexpected ptr_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON(!src_reg)) {\n\t\tprint_verifier_state(env, env->cur_state);\n\t\tverbose(env, \"verifier internal error: no src_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n}\n\n/* check validity of 32-bit and 64-bit arithmetic operations */\nstatic int check_alu_op(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode == BPF_END || opcode == BPF_NEG) {\n\t\tif (opcode == BPF_NEG) {\n\t\t\tif (BPF_SRC(insn->code) != 0 ||\n\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t    insn->off != 0 || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_NEG uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0 ||\n\t\t\t    (insn->imm != 16 && insn->imm != 32 && insn->imm != 64) ||\n\t\t\t    BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\tverbose(env, \"BPF_END uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->dst_reg)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic prohibited\\n\",\n\t\t\t\tinsn->dst_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t} else if (opcode == BPF_MOV) {\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t/* case: R1 = R2\n\t\t\t\t * copy register state to dest reg\n\t\t\t\t */\n\t\t\t\tregs[insn->dst_reg] = regs[insn->src_reg];\n\t\t\t\tregs[insn->dst_reg].live |= REG_LIVE_WRITTEN;\n\t\t\t} else {\n\t\t\t\t/* R1 = (u32) R2 */\n\t\t\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"R%d partial copy of pointer\\n\",\n\t\t\t\t\t\tinsn->src_reg);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\tcoerce_reg_to_size(&regs[insn->dst_reg], 4);\n\t\t\t}\n\t\t} else {\n\t\t\t/* case: R = imm\n\t\t\t * remember the value we stored into this reg\n\t\t\t */\n\t\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t insn->imm);\n\t\t\t} else {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t (u32)insn->imm);\n\t\t\t}\n\t\t}\n\n\t} else if (opcode > BPF_END) {\n\t\tverbose(env, \"invalid BPF_ALU opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\n\t} else {\t/* all other ALU ops: and, sub, xor, add, ... */\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src2 operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif ((opcode == BPF_MOD || opcode == BPF_DIV) &&\n\t\t    BPF_SRC(insn->code) == BPF_K && insn->imm == 0) {\n\t\t\tverbose(env, \"div by zero\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((opcode == BPF_LSH || opcode == BPF_RSH ||\n\t\t     opcode == BPF_ARSH) && BPF_SRC(insn->code) == BPF_K) {\n\t\t\tint size = BPF_CLASS(insn->code) == BPF_ALU64 ? 64 : 32;\n\n\t\t\tif (insn->imm < 0 || insn->imm >= size) {\n\t\t\t\tverbose(env, \"invalid shift %d\\n\", insn->imm);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\treturn adjust_reg_min_max_vals(env, insn);\n\t}\n\n\treturn 0;\n}\n\nstatic void find_good_pkt_pointers(struct bpf_verifier_state *state,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   enum bpf_reg_type type,\n\t\t\t\t   bool range_right_open)\n{\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tu16 new_range;\n\tint i;\n\n\tif (dst_reg->off < 0 ||\n\t    (dst_reg->off == 0 && range_right_open))\n\t\t/* This doesn't give us any range */\n\t\treturn;\n\n\tif (dst_reg->umax_value > MAX_PACKET_OFF ||\n\t    dst_reg->umax_value + dst_reg->off > MAX_PACKET_OFF)\n\t\t/* Risk of overflow.  For instance, ptr + (1<<63) may be less\n\t\t * than pkt_end, but that's because it's also less than pkt.\n\t\t */\n\t\treturn;\n\n\tnew_range = dst_reg->off;\n\tif (range_right_open)\n\t\tnew_range--;\n\n\t/* Examples for register markings:\n\t *\n\t * pkt_data in dst register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 > pkt_end) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 < pkt_end) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   Where:\n\t *     r2 == dst_reg, pkt_end == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * pkt_data in src register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end >= r2) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end <= r2) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   Where:\n\t *     pkt_end == dst_reg, r2 == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * Find register r3 and mark its range as r3=pkt(id=n,off=0,r=8)\n\t * or r3=pkt(id=n,off=0,r=8-1), so that range of bytes [r3, r3 + 8)\n\t * and [r3, r3 + 8-1) respectively is safe to access depending on\n\t * the check.\n\t */\n\n\t/* If our ids match, then we must have the same max_value.  And we\n\t * don't care about the other reg's fixed offset, since if it's too big\n\t * the range won't allow anything.\n\t * dst_reg->off is known < MAX_PACKET_OFF, therefore it fits in a u16.\n\t */\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (regs[i].type == type && regs[i].id == dst_reg->id)\n\t\t\t/* keep the maximum range already checked */\n\t\t\tregs[i].range = max(regs[i].range, new_range);\n\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tif (state->stack[i].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\treg = &state->stack[i].spilled_ptr;\n\t\tif (reg->type == type && reg->id == dst_reg->id)\n\t\t\treg->range = max(reg->range, new_range);\n\t}\n}\n\n/* Adjusts the register min/max values in the case that the dst_reg is the\n * variable register that we are working on, and src_reg is a constant or we're\n * simply doing a BPF_K check.\n * In JEQ/JNE cases we also adjust the var_off values.\n */\nstatic void reg_set_min_max(struct bpf_reg_state *true_reg,\n\t\t\t    struct bpf_reg_state *false_reg, u64 val,\n\t\t\t    u8 opcode)\n{\n\t/* If the dst_reg is a pointer, we can't learn anything about its\n\t * variable offset from the compare (unless src_reg were a pointer into\n\t * the same object, but we don't bother with that.\n\t * Since false_reg and true_reg have the same type by construction, we\n\t * only need to check one of them for pointerness.\n\t */\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Same as above, but for the case that dst_reg holds a constant and src_reg is\n * the variable reg.\n */\nstatic void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\n\t\t\t\tstruct bpf_reg_state *false_reg, u64 val,\n\t\t\t\tu8 opcode)\n{\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JGT:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Regs are known to be equal, so intersect their min/max/var_off */\nstatic void __reg_combine_min_max(struct bpf_reg_state *src_reg,\n\t\t\t\t  struct bpf_reg_state *dst_reg)\n{\n\tsrc_reg->umin_value = dst_reg->umin_value = max(src_reg->umin_value,\n\t\t\t\t\t\t\tdst_reg->umin_value);\n\tsrc_reg->umax_value = dst_reg->umax_value = min(src_reg->umax_value,\n\t\t\t\t\t\t\tdst_reg->umax_value);\n\tsrc_reg->smin_value = dst_reg->smin_value = max(src_reg->smin_value,\n\t\t\t\t\t\t\tdst_reg->smin_value);\n\tsrc_reg->smax_value = dst_reg->smax_value = min(src_reg->smax_value,\n\t\t\t\t\t\t\tdst_reg->smax_value);\n\tsrc_reg->var_off = dst_reg->var_off = tnum_intersect(src_reg->var_off,\n\t\t\t\t\t\t\t     dst_reg->var_off);\n\t/* We might have learned new bounds from the var_off. */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n\t/* We might have learned something about the sign bit. */\n\t__reg_deduce_bounds(src_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(src_reg);\n\t__reg_bound_offset(dst_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void reg_combine_min_max(struct bpf_reg_state *true_src,\n\t\t\t\tstruct bpf_reg_state *true_dst,\n\t\t\t\tstruct bpf_reg_state *false_src,\n\t\t\t\tstruct bpf_reg_state *false_dst,\n\t\t\t\tu8 opcode)\n{\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t__reg_combine_min_max(true_src, true_dst);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t__reg_combine_min_max(false_src, false_dst);\n\t\tbreak;\n\t}\n}\n\nstatic void mark_map_reg(struct bpf_reg_state *regs, u32 regno, u32 id,\n\t\t\t bool is_null)\n{\n\tstruct bpf_reg_state *reg = &regs[regno];\n\n\tif (reg->type == PTR_TO_MAP_VALUE_OR_NULL && reg->id == id) {\n\t\t/* Old offset (both fixed and variable parts) should\n\t\t * have been known-zero, because we don't allow pointer\n\t\t * arithmetic on pointers that might be NULL.\n\t\t */\n\t\tif (WARN_ON_ONCE(reg->smin_value || reg->smax_value ||\n\t\t\t\t !tnum_equals_const(reg->var_off, 0) ||\n\t\t\t\t reg->off)) {\n\t\t\t__mark_reg_known_zero(reg);\n\t\t\treg->off = 0;\n\t\t}\n\t\tif (is_null) {\n\t\t\treg->type = SCALAR_VALUE;\n\t\t} else if (reg->map_ptr->inner_map_meta) {\n\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\treg->map_ptr = reg->map_ptr->inner_map_meta;\n\t\t} else {\n\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t}\n\t\t/* We don't need id from this point onwards anymore, thus we\n\t\t * should better reset it, so that state pruning has chances\n\t\t * to take effect.\n\t\t */\n\t\treg->id = 0;\n\t}\n}\n\n/* The logic is similar to find_good_pkt_pointers(), both could eventually\n * be folded together at some point.\n */\nstatic void mark_map_regs(struct bpf_verifier_state *state, u32 regno,\n\t\t\t  bool is_null)\n{\n\tstruct bpf_reg_state *regs = state->regs;\n\tu32 id = regs[regno].id;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tmark_map_reg(regs, i, id, is_null);\n\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tif (state->stack[i].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\tmark_map_reg(&state->stack[i].spilled_ptr, 0, id, is_null);\n\t}\n}\n\nstatic bool try_match_pkt_pointers(const struct bpf_insn *insn,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   struct bpf_reg_state *src_reg,\n\t\t\t\t   struct bpf_verifier_state *this_branch,\n\t\t\t\t   struct bpf_verifier_state *other_branch)\n{\n\tif (BPF_SRC(insn->code) != BPF_X)\n\t\treturn false;\n\n\tswitch (BPF_OP(insn->code)) {\n\tcase BPF_JGT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' > pkt_end, pkt_meta' > pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end > pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' < pkt_end, pkt_meta' < pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end < pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' >= pkt_end, pkt_meta' >= pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end >= pkt_data', pkt_data >= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' <= pkt_end, pkt_meta' <= pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end <= pkt_data', pkt_data <= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *other_branch, *this_branch = env->cur_state;\n\tstruct bpf_reg_state *regs = this_branch->regs, *dst_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\n\t/* detect if R == 0 where R was initialized to zero earlier */\n\tif (BPF_SRC(insn->code) == BPF_K &&\n\t    (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    dst_reg->type == SCALAR_VALUE &&\n\t    tnum_equals_const(dst_reg->var_off, insn->imm)) {\n\t\tif (opcode == BPF_JEQ) {\n\t\t\t/* if (imm == imm) goto pc+off;\n\t\t\t * only follow the goto, ignore fall-through\n\t\t\t */\n\t\t\t*insn_idx += insn->off;\n\t\t\treturn 0;\n\t\t} else {\n\t\t\t/* if (imm != imm) goto pc+off;\n\t\t\t * only follow fall-through branch, since\n\t\t\t * that's where the program will go\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    regs[insn->src_reg].type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(regs[insn->src_reg].var_off))\n\t\t\t\treg_set_min_max(&other_branch->regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg, regs[insn->src_reg].var_off.value,\n\t\t\t\t\t\topcode);\n\t\t\telse if (tnum_is_const(dst_reg->var_off))\n\t\t\t\treg_set_min_max_inv(&other_branch->regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    dst_reg->var_off.value, opcode);\n\t\t\telse if (opcode == BPF_JEQ || opcode == BPF_JNE)\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch->regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch->regs[insn->dst_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->dst_reg], opcode);\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch->regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, opcode);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem() */\n\tif (BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    dst_reg->type == PTR_TO_MAP_VALUE_OR_NULL) {\n\t\t/* Mark all identical map registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_map_regs(this_branch, insn->dst_reg, opcode == BPF_JNE);\n\t\tmark_map_regs(other_branch, insn->dst_reg, opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level)\n\t\tprint_verifier_state(env, this_branch);\n\treturn 0;\n}\n\n/* return the map pointer stored inside BPF_LD_IMM64 instruction */\nstatic struct bpf_map *ld_imm64_to_map_ptr(struct bpf_insn *insn)\n{\n\tu64 imm64 = ((u64) (u32) insn[0].imm) | ((u64) (u32) insn[1].imm) << 32;\n\n\treturn (struct bpf_map *) (unsigned long) imm64;\n}\n\n/* verify BPF_LD_IMM64 instruction */\nstatic int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tint err;\n\n\tif (BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid BPF_LD_IMM insn\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (insn->off != 0) {\n\t\tverbose(env, \"BPF_LD_IMM64 uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (insn->src_reg == 0) {\n\t\tu64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;\n\n\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t__mark_reg_known(&regs[insn->dst_reg], imm);\n\t\treturn 0;\n\t}\n\n\t/* replace_map_fd_with_map_ptr() should have caught bad ld_imm64 */\n\tBUG_ON(insn->src_reg != BPF_PSEUDO_MAP_FD);\n\n\tregs[insn->dst_reg].type = CONST_PTR_TO_MAP;\n\tregs[insn->dst_reg].map_ptr = ld_imm64_to_map_ptr(insn);\n\treturn 0;\n}\n\nstatic bool may_access_skb(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* verify safety of LD_ABS|LD_IND instructions:\n * - they can only appear in the programs where ctx == skb\n * - since they are wrappers of function calls, they scratch R1-R5 registers,\n *   preserve R6-R9, and store return value into R0\n *\n * Implicit input:\n *   ctx == skb == R6 == CTX\n *\n * Explicit input:\n *   SRC == any register\n *   IMM == 32-bit immediate\n *\n * Output:\n *   R0 - 8/16/32-bit skb data converted to cpu endianness\n */\nstatic int check_ld_abs(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 mode = BPF_MODE(insn->code);\n\tint i, err;\n\n\tif (!may_access_skb(env->prog->type)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions not allowed for this program type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\n\t    BPF_SIZE(insn->code) == BPF_DW ||\n\t    (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check whether implicit source operand (register R6) is readable */\n\terr = check_reg_arg(env, BPF_REG_6, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (regs[BPF_REG_6].type != PTR_TO_CTX) {\n\t\tverbose(env,\n\t\t\t\"at the time of BPF_LD_ABS|IND R6 != pointer to skb\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mode == BPF_IND) {\n\t\t/* check explicit source operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* reset caller saved regs to unreadable */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* mark destination R0 register as readable, since it contains\n\t * the value fetched from the packet.\n\t * Already marked as written above.\n\t */\n\tmark_reg_unknown(env, regs, BPF_REG_0);\n\treturn 0;\n}\n\nstatic int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose(env, \"At program exit the register R0 \");\n\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"has value %s\", tn_buf);\n\t\t} else {\n\t\t\tverbose(env, \"has unknown scalar value\");\n\t\t}\n\t\tverbose(env, \" should have been 0 or 1\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/* non-recursive DFS pseudo code\n * 1  procedure DFS-iterative(G,v):\n * 2      label v as discovered\n * 3      let S be a stack\n * 4      S.push(v)\n * 5      while S is not empty\n * 6            t <- S.pop()\n * 7            if t is what we're looking for:\n * 8                return t\n * 9            for all edges e in G.adjacentEdges(t) do\n * 10               if edge e is already labelled\n * 11                   continue with the next edge\n * 12               w <- G.adjacentVertex(t,e)\n * 13               if vertex w is not discovered and not explored\n * 14                   label e as tree-edge\n * 15                   label w as discovered\n * 16                   S.push(w)\n * 17                   continue at 5\n * 18               else if vertex w is discovered\n * 19                   label e as back-edge\n * 20               else\n * 21                   // vertex w is explored\n * 22                   label e as forward- or cross-edge\n * 23           label t as explored\n * 24           S.pop()\n *\n * convention:\n * 0x10 - discovered\n * 0x11 - discovered and fall-through edge labelled\n * 0x12 - discovered and fall-through and branch edges labelled\n * 0x20 - explored\n */\n\nenum {\n\tDISCOVERED = 0x10,\n\tEXPLORED = 0x20,\n\tFALLTHROUGH = 1,\n\tBRANCH = 2,\n};\n\n#define STATE_LIST_MARK ((struct bpf_verifier_state_list *) -1L)\n\nstatic int *insn_stack;\t/* stack of insns to process */\nstatic int cur_stack;\t/* current stack index */\nstatic int *insn_state;\n\n/* t, w, e - match pseudo-code above:\n * t - index of current instruction\n * w - next instruction\n * e - edge\n */\nstatic int push_insn(int t, int w, int e, struct bpf_verifier_env *env)\n{\n\tif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\n\t\treturn 0;\n\n\tif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\n\t\treturn 0;\n\n\tif (w < 0 || w >= env->prog->len) {\n\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e == BRANCH)\n\t\t/* mark branch target for state pruning */\n\t\tenv->explored_states[w] = STATE_LIST_MARK;\n\n\tif (insn_state[w] == 0) {\n\t\t/* tree-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t\tinsn_state[w] = DISCOVERED;\n\t\tif (cur_stack >= env->prog->len)\n\t\t\treturn -E2BIG;\n\t\tinsn_stack[cur_stack++] = w;\n\t\treturn 1;\n\t} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\n\t\tverbose(env, \"back-edge from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t} else if (insn_state[w] == EXPLORED) {\n\t\t/* forward- or cross-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t} else {\n\t\tverbose(env, \"insn state internal bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n/* non-recursive depth-first-search to detect loops in BPF program\n * loop == back-edge in directed graph\n */\nstatic int check_cfg(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint ret = 0;\n\tint i, t;\n\n\tinsn_state = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_state)\n\t\treturn -ENOMEM;\n\n\tinsn_stack = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_stack) {\n\t\tkfree(insn_state);\n\t\treturn -ENOMEM;\n\t}\n\n\tinsn_state[0] = DISCOVERED; /* mark 1st insn as discovered */\n\tinsn_stack[0] = 0; /* 0 is the first instruction */\n\tcur_stack = 1;\n\npeek_stack:\n\tif (cur_stack == 0)\n\t\tgoto check_state;\n\tt = insn_stack[cur_stack - 1];\n\n\tif (BPF_CLASS(insns[t].code) == BPF_JMP) {\n\t\tu8 opcode = BPF_OP(insns[t].code);\n\n\t\tif (opcode == BPF_EXIT) {\n\t\t\tgoto mark_explored;\n\t\t} else if (opcode == BPF_CALL) {\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t} else if (opcode == BPF_JA) {\n\t\t\tif (BPF_SRC(insns[t].code) != BPF_K) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t\t/* unconditional jump with single edge */\n\t\t\tret = push_insn(t, t + insns[t].off + 1,\n\t\t\t\t\tFALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\t/* tell verifier to check for equivalent states\n\t\t\t * after every call and jump\n\t\t\t */\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t} else {\n\t\t\t/* conditional jump with two edges */\n\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\n\t\t\tret = push_insn(t, t + insns[t].off + 1, BRANCH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t}\n\t} else {\n\t\t/* all other non-branch instructions with single\n\t\t * fall-through edge\n\t\t */\n\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\tif (ret == 1)\n\t\t\tgoto peek_stack;\n\t\telse if (ret < 0)\n\t\t\tgoto err_free;\n\t}\n\nmark_explored:\n\tinsn_state[t] = EXPLORED;\n\tif (cur_stack-- <= 0) {\n\t\tverbose(env, \"pop stack internal bug\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free;\n\t}\n\tgoto peek_stack;\n\ncheck_state:\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn_state[i] != EXPLORED) {\n\t\t\tverbose(env, \"unreachable insn %d\\n\", i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t}\n\tret = 0; /* cfg looks good */\n\nerr_free:\n\tkfree(insn_state);\n\tkfree(insn_stack);\n\treturn ret;\n}\n\n/* check %cur's range satisfies %old's */\nstatic bool range_within(struct bpf_reg_state *old,\n\t\t\t struct bpf_reg_state *cur)\n{\n\treturn old->umin_value <= cur->umin_value &&\n\t       old->umax_value >= cur->umax_value &&\n\t       old->smin_value <= cur->smin_value &&\n\t       old->smax_value >= cur->smax_value;\n}\n\n/* Maximum number of register states that can exist at once */\n#define ID_MAP_SIZE\t(MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE)\nstruct idpair {\n\tu32 old;\n\tu32 cur;\n};\n\n/* If in the old state two registers had the same id, then they need to have\n * the same id in the new state as well.  But that id could be different from\n * the old state, so we need to track the mapping from old to new ids.\n * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent\n * regs with old id 5 must also have new id 9 for the new state to be safe.  But\n * regs with a different old id could still have new id 9, we don't care about\n * that.\n * So we look through our idmap to see if this old id has been seen before.  If\n * so, we require the new id to match; otherwise, we add the id pair to the map.\n */\nstatic bool check_ids(u32 old_id, u32 cur_id, struct idpair *idmap)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\n/* Returns true if (rold safe implies rcur safe) */\nstatic bool regsafe(struct bpf_reg_state *rold, struct bpf_reg_state *rcur,\n\t\t    struct idpair *idmap)\n{\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, live)) == 0)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (rold->type) {\n\tcase SCALAR_VALUE:\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* if we knew anything about the old value, we're not\n\t\t\t * equal, because we can't know anything about the\n\t\t\t * scalar value of the pointer in the new value.\n\t\t\t */\n\t\t\treturn rold->umin_value == 0 &&\n\t\t\t       rold->umax_value == U64_MAX &&\n\t\t\t       rold->smin_value == S64_MIN &&\n\t\t\t       rold->smax_value == S64_MAX &&\n\t\t\t       tnum_is_unknown(rold->var_off);\n\t\t}\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * We don't care about the 'id' value, because nothing\n\t\t * uses it for PTR_TO_MAP_VALUE (only for ..._OR_NULL)\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (rcur->type != PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\treturn false;\n\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\treturn false;\n\t\t/* Check our ids match any regs they're supposed to */\n\t\treturn check_ids(rold->id, rcur->id, idmap);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_STACK:\n\tcase PTR_TO_PACKET_END:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\nstatic bool stacksafe(struct bpf_verifier_state *old,\n\t\t      struct bpf_verifier_state *cur,\n\t\t      struct idpair *idmap)\n{\n\tint i, spi;\n\n\t/* if explored stack has more populated slots than current stack\n\t * such stacks are not equivalent\n\t */\n\tif (old->allocated_stack > cur->allocated_stack)\n\t\treturn false;\n\n\t/* walk slots of the explored stack and ignore any additional\n\t * slots in the current stack, since explored(safe) state\n\t * didn't use them\n\t */\n\tfor (i = 0; i < old->allocated_stack; i++) {\n\t\tspi = i / BPF_REG_SIZE;\n\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_INVALID)\n\t\t\tcontinue;\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] !=\n\t\t    cur->stack[spi].slot_type[i % BPF_REG_SIZE])\n\t\t\t/* Ex: old explored (safe) state has STACK_SPILL in\n\t\t\t * this stack slot, but current has has STACK_MISC ->\n\t\t\t * this verifier states are not equivalent,\n\t\t\t * return false to continue verification of this path\n\t\t\t */\n\t\t\treturn false;\n\t\tif (i % BPF_REG_SIZE)\n\t\t\tcontinue;\n\t\tif (old->stack[spi].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\tif (!regsafe(&old->stack[spi].spilled_ptr,\n\t\t\t     &cur->stack[spi].spilled_ptr,\n\t\t\t     idmap))\n\t\t\t/* when explored and current stack slot are both storing\n\t\t\t * spilled registers, check that stored pointers types\n\t\t\t * are the same as well.\n\t\t\t * Ex: explored safe path could have stored\n\t\t\t * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -8}\n\t\t\t * but current path has stored:\n\t\t\t * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -16}\n\t\t\t * such verifier states are not equivalent.\n\t\t\t * return false to continue verification of this path\n\t\t\t */\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\n/* compare two verifier states\n *\n * all states stored in state_list are known to be valid, since\n * verifier reached 'bpf_exit' instruction through them\n *\n * this function is called when verifier exploring different branches of\n * execution popped from the state stack. If it sees an old state that has\n * more strict register state and more strict stack state then this execution\n * branch doesn't need to be explored further, since verifier already\n * concluded that more strict state leads to valid finish.\n *\n * Therefore two states are equivalent if register state is more conservative\n * and explored stack state is more conservative than the current one.\n * Example:\n *       explored                   current\n * (slot1=INV slot2=MISC) == (slot1=MISC slot2=MISC)\n * (slot1=MISC slot2=MISC) != (slot1=INV slot2=MISC)\n *\n * In other words if current stack state (one being explored) has more\n * valid slots than old one that already passed validation, it means\n * the verifier can stop exploring and conclude that current state is valid too\n *\n * Similarly with registers. If explored state has register type as invalid\n * whereas register type in current state is meaningful, it means that\n * the current state will reach 'bpf_exit' instruction safely\n */\nstatic bool states_equal(struct bpf_verifier_env *env,\n\t\t\t struct bpf_verifier_state *old,\n\t\t\t struct bpf_verifier_state *cur)\n{\n\tstruct idpair *idmap;\n\tbool ret = false;\n\tint i;\n\n\tidmap = kcalloc(ID_MAP_SIZE, sizeof(struct idpair), GFP_KERNEL);\n\t/* If we failed to allocate the idmap, just say it's not safe */\n\tif (!idmap)\n\t\treturn false;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\tif (!regsafe(&old->regs[i], &cur->regs[i], idmap))\n\t\t\tgoto out_free;\n\t}\n\n\tif (!stacksafe(old, cur, idmap))\n\t\tgoto out_free;\n\tret = true;\nout_free:\n\tkfree(idmap);\n\treturn ret;\n}\n\n/* A write screens off any subsequent reads; but write marks come from the\n * straight-line code between a state and its parent.  When we arrive at a\n * jump target (in the first iteration of the propagate_liveness() loop),\n * we didn't arrive by the straight-line code, so read marks in state must\n * propagate to parent regardless of state's write marks.\n */\nstatic bool do_propagate_liveness(const struct bpf_verifier_state *state,\n\t\t\t\t  struct bpf_verifier_state *parent)\n{\n\tbool writes = parent == state->parent; /* Observe write marks */\n\tbool touched = false; /* any changes made? */\n\tint i;\n\n\tif (!parent)\n\t\treturn touched;\n\t/* Propagate read liveness of registers... */\n\tBUILD_BUG_ON(BPF_REG_FP + 1 != MAX_BPF_REG);\n\t/* We don't need to worry about FP liveness because it's read-only */\n\tfor (i = 0; i < BPF_REG_FP; i++) {\n\t\tif (parent->regs[i].live & REG_LIVE_READ)\n\t\t\tcontinue;\n\t\tif (writes && (state->regs[i].live & REG_LIVE_WRITTEN))\n\t\t\tcontinue;\n\t\tif (state->regs[i].live & REG_LIVE_READ) {\n\t\t\tparent->regs[i].live |= REG_LIVE_READ;\n\t\t\ttouched = true;\n\t\t}\n\t}\n\t/* ... and stack slots */\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE &&\n\t\t    i < parent->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tif (parent->stack[i].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\tif (state->stack[i].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\tif (parent->stack[i].spilled_ptr.live & REG_LIVE_READ)\n\t\t\tcontinue;\n\t\tif (writes &&\n\t\t    (state->stack[i].spilled_ptr.live & REG_LIVE_WRITTEN))\n\t\t\tcontinue;\n\t\tif (state->stack[i].spilled_ptr.live & REG_LIVE_READ) {\n\t\t\tparent->stack[i].spilled_ptr.live |= REG_LIVE_READ;\n\t\t\ttouched = true;\n\t\t}\n\t}\n\treturn touched;\n}\n\n/* \"parent\" is \"a state from which we reach the current state\", but initially\n * it is not the state->parent (i.e. \"the state whose straight-line code leads\n * to the current state\"), instead it is the state that happened to arrive at\n * a (prunable) equivalent of the current state.  See comment above\n * do_propagate_liveness() for consequences of this.\n * This function is just a more efficient way of calling mark_reg_read() or\n * mark_stack_slot_read() on each reg in \"parent\" that is read in \"state\",\n * though it requires that parent != state->parent in the call arguments.\n */\nstatic void propagate_liveness(const struct bpf_verifier_state *state,\n\t\t\t       struct bpf_verifier_state *parent)\n{\n\twhile (do_propagate_liveness(state, parent)) {\n\t\t/* Something changed, so we need to feed those changes onward */\n\t\tstate = parent;\n\t\tparent = state->parent;\n\t}\n}\n\nstatic int is_state_visited(struct bpf_verifier_env *env, int insn_idx)\n{\n\tstruct bpf_verifier_state_list *new_sl;\n\tstruct bpf_verifier_state_list *sl;\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tint i, err;\n\n\tsl = env->explored_states[insn_idx];\n\tif (!sl)\n\t\t/* this 'insn_idx' instruction wasn't marked, so we will not\n\t\t * be doing state search here\n\t\t */\n\t\treturn 0;\n\n\twhile (sl != STATE_LIST_MARK) {\n\t\tif (states_equal(env, &sl->state, cur)) {\n\t\t\t/* reached equivalent register/stack state,\n\t\t\t * prune the search.\n\t\t\t * Registers read by the continuation are read by us.\n\t\t\t * If we have any write marks in env->cur_state, they\n\t\t\t * will prevent corresponding reads in the continuation\n\t\t\t * from reaching our parent (an explored_state).  Our\n\t\t\t * own state will get the read marks recorded, but\n\t\t\t * they'll be immediately forgotten as we're pruning\n\t\t\t * this state and will pop a new one.\n\t\t\t */\n\t\t\tpropagate_liveness(&sl->state, cur);\n\t\t\treturn 1;\n\t\t}\n\t\tsl = sl->next;\n\t}\n\n\t/* there were no equivalent states, remember current one.\n\t * technically the current state is not proven to be safe yet,\n\t * but it will either reach bpf_exit (which means it's safe) or\n\t * it will be rejected. Since there are no loops, we won't be\n\t * seeing this 'insn_idx' instruction again on the way to bpf_exit\n\t */\n\tnew_sl = kzalloc(sizeof(struct bpf_verifier_state_list), GFP_KERNEL);\n\tif (!new_sl)\n\t\treturn -ENOMEM;\n\n\t/* add new state to the head of linked list */\n\terr = copy_verifier_state(&new_sl->state, cur);\n\tif (err) {\n\t\tfree_verifier_state(&new_sl->state, false);\n\t\tkfree(new_sl);\n\t\treturn err;\n\t}\n\tnew_sl->next = env->explored_states[insn_idx];\n\tenv->explored_states[insn_idx] = new_sl;\n\t/* connect new state to parentage chain */\n\tcur->parent = &new_sl->state;\n\t/* clear write marks in current state: the writes we did are not writes\n\t * our child did, so they don't screen off its reads from us.\n\t * (There are no read marks in current state, because reads always mark\n\t * their parent and current state never has children yet.  Only\n\t * explored_states can get read marks.)\n\t */\n\tfor (i = 0; i < BPF_REG_FP; i++)\n\t\tcur->regs[i].live = REG_LIVE_NONE;\n\tfor (i = 0; i < cur->allocated_stack / BPF_REG_SIZE; i++)\n\t\tif (cur->stack[i].slot_type[0] == STACK_SPILL)\n\t\t\tcur->stack[i].spilled_ptr.live = REG_LIVE_NONE;\n\treturn 0;\n}\n\nstatic int ext_analyzer_insn_hook(struct bpf_verifier_env *env,\n\t\t\t\t  int insn_idx, int prev_insn_idx)\n{\n\tif (env->dev_ops && env->dev_ops->insn_hook)\n\t\treturn env->dev_ops->insn_hook(env, insn_idx, prev_insn_idx);\n\n\treturn 0;\n}\n\nstatic int do_check(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *state;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_reg_state *regs;\n\tint insn_cnt = env->prog->len;\n\tint insn_idx, prev_insn_idx = 0;\n\tint insn_processed = 0;\n\tbool do_print_state = false;\n\n\tstate = kzalloc(sizeof(struct bpf_verifier_state), GFP_KERNEL);\n\tif (!state)\n\t\treturn -ENOMEM;\n\tenv->cur_state = state;\n\tinit_reg_state(env, state->regs);\n\tstate->parent = NULL;\n\tinsn_idx = 0;\n\tfor (;;) {\n\t\tstruct bpf_insn *insn;\n\t\tu8 class;\n\t\tint err;\n\n\t\tif (insn_idx >= insn_cnt) {\n\t\t\tverbose(env, \"invalid insn idx %d insn_cnt %d\\n\",\n\t\t\t\tinsn_idx, insn_cnt);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn = &insns[insn_idx];\n\t\tclass = BPF_CLASS(insn->code);\n\n\t\tif (++insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {\n\t\t\tverbose(env,\n\t\t\t\t\"BPF program is too large. Processed %d insn\\n\",\n\t\t\t\tinsn_processed);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\terr = is_state_visited(env, insn_idx);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (err == 1) {\n\t\t\t/* found equivalent state, can prune the search */\n\t\t\tif (env->log.level) {\n\t\t\t\tif (do_print_state)\n\t\t\t\t\tverbose(env, \"\\nfrom %d to %d: safe\\n\",\n\t\t\t\t\t\tprev_insn_idx, insn_idx);\n\t\t\t\telse\n\t\t\t\t\tverbose(env, \"%d: safe\\n\", insn_idx);\n\t\t\t}\n\t\t\tgoto process_bpf_exit;\n\t\t}\n\n\t\tif (need_resched())\n\t\t\tcond_resched();\n\n\t\tif (env->log.level > 1 || (env->log.level && do_print_state)) {\n\t\t\tif (env->log.level > 1)\n\t\t\t\tverbose(env, \"%d:\", insn_idx);\n\t\t\telse\n\t\t\t\tverbose(env, \"\\nfrom %d to %d:\",\n\t\t\t\t\tprev_insn_idx, insn_idx);\n\t\t\tprint_verifier_state(env, state);\n\t\t\tdo_print_state = false;\n\t\t}\n\n\t\tif (env->log.level) {\n\t\t\tverbose(env, \"%d: \", insn_idx);\n\t\t\tprint_bpf_insn(verbose, env, insn,\n\t\t\t\t       env->allow_ptr_leaks);\n\t\t}\n\n\t\terr = ext_analyzer_insn_hook(env, insn_idx, prev_insn_idx);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tregs = cur_regs(env);\n\t\tenv->insn_aux_data[insn_idx].seen = true;\n\t\tif (class == BPF_ALU || class == BPF_ALU64) {\n\t\t\terr = check_alu_op(env, insn);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_LDX) {\n\t\t\tenum bpf_reg_type *prev_src_type, src_reg_type;\n\n\t\t\t/* check for reserved fields is already done */\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tsrc_reg_type = regs[insn->src_reg].type;\n\n\t\t\t/* check that memory (src_reg + off) is readable,\n\t\t\t * the state of dst_reg will be updated by this func\n\t\t\t */\n\t\t\terr = check_mem_access(env, insn_idx, insn->src_reg, insn->off,\n\t\t\t\t\t       BPF_SIZE(insn->code), BPF_READ,\n\t\t\t\t\t       insn->dst_reg);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_src_type = &env->insn_aux_data[insn_idx].ptr_type;\n\n\t\t\tif (*prev_src_type == NOT_INIT) {\n\t\t\t\t/* saw a valid insn\n\t\t\t\t * dst_reg = *(u32 *)(src_reg + off)\n\t\t\t\t * save type to validate intersecting paths\n\t\t\t\t */\n\t\t\t\t*prev_src_type = src_reg_type;\n\n\t\t\t} else if (src_reg_type != *prev_src_type &&\n\t\t\t\t   (src_reg_type == PTR_TO_CTX ||\n\t\t\t\t    *prev_src_type == PTR_TO_CTX)) {\n\t\t\t\t/* ABuser program is trying to use the same insn\n\t\t\t\t * dst_reg = *(u32*) (src_reg + off)\n\t\t\t\t * with different pointer types:\n\t\t\t\t * src_reg == ctx in one branch and\n\t\t\t\t * src_reg == stack|map in some other branch.\n\t\t\t\t * Reject it.\n\t\t\t\t */\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_STX) {\n\t\t\tenum bpf_reg_type *prev_dst_type, dst_reg_type;\n\n\t\t\tif (BPF_MODE(insn->code) == BPF_XADD) {\n\t\t\t\terr = check_xadd(env, insn_idx, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tinsn_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\t/* check src2 operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tdst_reg_type = regs[insn->dst_reg].type;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t\t\t       BPF_SIZE(insn->code), BPF_WRITE,\n\t\t\t\t\t       insn->src_reg);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_dst_type = &env->insn_aux_data[insn_idx].ptr_type;\n\n\t\t\tif (*prev_dst_type == NOT_INIT) {\n\t\t\t\t*prev_dst_type = dst_reg_type;\n\t\t\t} else if (dst_reg_type != *prev_dst_type &&\n\t\t\t\t   (dst_reg_type == PTR_TO_CTX ||\n\t\t\t\t    *prev_dst_type == PTR_TO_CTX)) {\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_ST) {\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM ||\n\t\t\t    insn->src_reg != BPF_REG_0) {\n\t\t\t\tverbose(env, \"BPF_ST uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t\t\t       BPF_SIZE(insn->code), BPF_WRITE,\n\t\t\t\t\t       -1);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_JMP) {\n\t\t\tu8 opcode = BPF_OP(insn->code);\n\n\t\t\tif (opcode == BPF_CALL) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->off != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(env, \"BPF_CALL uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\terr = check_call(env, insn->imm, insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (opcode == BPF_JA) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(env, \"BPF_JA uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tinsn_idx += insn->off + 1;\n\t\t\t\tcontinue;\n\n\t\t\t} else if (opcode == BPF_EXIT) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(env, \"BPF_EXIT uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\t/* eBPF calling convetion is such that R0 is used\n\t\t\t\t * to return the value from eBPF program.\n\t\t\t\t * Make sure that it's readable at this time\n\t\t\t\t * of bpf_exit, which means that program wrote\n\t\t\t\t * something into it earlier\n\t\t\t\t */\n\t\t\t\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\t\t\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\n\t\t\t\terr = check_return_code(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\nprocess_bpf_exit:\n\t\t\t\terr = pop_stack(env, &prev_insn_idx, &insn_idx);\n\t\t\t\tif (err < 0) {\n\t\t\t\t\tif (err != -ENOENT)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = check_cond_jmp_op(env, insn, &insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t} else if (class == BPF_LD) {\n\t\t\tu8 mode = BPF_MODE(insn->code);\n\n\t\t\tif (mode == BPF_ABS || mode == BPF_IND) {\n\t\t\t\terr = check_ld_abs(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (mode == BPF_IMM) {\n\t\t\t\terr = check_ld_imm(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tinsn_idx++;\n\t\t\t\tenv->insn_aux_data[insn_idx].seen = true;\n\t\t\t} else {\n\t\t\t\tverbose(env, \"invalid BPF_LD mode\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tverbose(env, \"unknown insn class %d\\n\", class);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tinsn_idx++;\n\t}\n\n\tverbose(env, \"processed %d insns, stack depth %d\\n\", insn_processed,\n\t\tenv->prog->aux->stack_depth);\n\treturn 0;\n}\n\nstatic int check_map_prealloc(struct bpf_map *map)\n{\n\treturn (map->map_type != BPF_MAP_TYPE_HASH &&\n\t\tmap->map_type != BPF_MAP_TYPE_PERCPU_HASH &&\n\t\tmap->map_type != BPF_MAP_TYPE_HASH_OF_MAPS) ||\n\t\t!(map->map_flags & BPF_F_NO_PREALLOC);\n}\n\nstatic int check_map_prog_compatibility(struct bpf_verifier_env *env,\n\t\t\t\t\tstruct bpf_map *map,\n\t\t\t\t\tstruct bpf_prog *prog)\n\n{\n\t/* Make sure that BPF_PROG_TYPE_PERF_EVENT programs only use\n\t * preallocated hash maps, since doing memory allocation\n\t * in overflow_handler can crash depending on where nmi got\n\t * triggered.\n\t */\n\tif (prog->type == BPF_PROG_TYPE_PERF_EVENT) {\n\t\tif (!check_map_prealloc(map)) {\n\t\t\tverbose(env, \"perf_event programs can only use preallocated hash map\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (map->inner_map_meta &&\n\t\t    !check_map_prealloc(map->inner_map_meta)) {\n\t\t\tverbose(env, \"perf_event programs can only use preallocated inner hash map\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/* look for pseudo eBPF instructions that access map FDs and\n * replace them with actual map pointers\n */\nstatic int replace_map_fd_with_map_ptr(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint i, j, err;\n\n\terr = bpf_prog_calc_tag(env->prog);\n\tif (err)\n\t\treturn err;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (BPF_CLASS(insn->code) == BPF_LDX &&\n\t\t    (BPF_MODE(insn->code) != BPF_MEM || insn->imm != 0)) {\n\t\t\tverbose(env, \"BPF_LDX uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_STX &&\n\t\t    ((BPF_MODE(insn->code) != BPF_MEM &&\n\t\t      BPF_MODE(insn->code) != BPF_XADD) || insn->imm != 0)) {\n\t\t\tverbose(env, \"BPF_STX uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (insn[0].code == (BPF_LD | BPF_IMM | BPF_DW)) {\n\t\t\tstruct bpf_map *map;\n\t\t\tstruct fd f;\n\n\t\t\tif (i == insn_cnt - 1 || insn[1].code != 0 ||\n\t\t\t    insn[1].dst_reg != 0 || insn[1].src_reg != 0 ||\n\t\t\t    insn[1].off != 0) {\n\t\t\t\tverbose(env, \"invalid bpf_ld_imm64 insn\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (insn->src_reg == 0)\n\t\t\t\t/* valid generic load 64-bit imm */\n\t\t\t\tgoto next_insn;\n\n\t\t\tif (insn->src_reg != BPF_PSEUDO_MAP_FD) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"unrecognized bpf_ld_imm64 insn\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tf = fdget(insn->imm);\n\t\t\tmap = __bpf_map_get(f);\n\t\t\tif (IS_ERR(map)) {\n\t\t\t\tverbose(env, \"fd %d is not pointing to valid bpf_map\\n\",\n\t\t\t\t\tinsn->imm);\n\t\t\t\treturn PTR_ERR(map);\n\t\t\t}\n\n\t\t\terr = check_map_prog_compatibility(env, map, env->prog);\n\t\t\tif (err) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\t/* store map pointer inside BPF_LD_IMM64 instruction */\n\t\t\tinsn[0].imm = (u32) (unsigned long) map;\n\t\t\tinsn[1].imm = ((u64) (unsigned long) map) >> 32;\n\n\t\t\t/* check whether we recorded this map already */\n\t\t\tfor (j = 0; j < env->used_map_cnt; j++)\n\t\t\t\tif (env->used_maps[j] == map) {\n\t\t\t\t\tfdput(f);\n\t\t\t\t\tgoto next_insn;\n\t\t\t\t}\n\n\t\t\tif (env->used_map_cnt >= MAX_USED_MAPS) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn -E2BIG;\n\t\t\t}\n\n\t\t\t/* hold the map. If the program is rejected by verifier,\n\t\t\t * the map will be released by release_maps() or it\n\t\t\t * will be used by the valid program until it's unloaded\n\t\t\t * and all maps are released in free_bpf_prog_info()\n\t\t\t */\n\t\t\tmap = bpf_map_inc(map, false);\n\t\t\tif (IS_ERR(map)) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn PTR_ERR(map);\n\t\t\t}\n\t\t\tenv->used_maps[env->used_map_cnt++] = map;\n\n\t\t\tfdput(f);\nnext_insn:\n\t\t\tinsn++;\n\t\t\ti++;\n\t\t}\n\t}\n\n\t/* now all pseudo BPF_LD_IMM64 instructions load valid\n\t * 'struct bpf_map *' into a register instead of user map_fd.\n\t * These pointers will be used later by verifier to validate map access.\n\t */\n\treturn 0;\n}\n\n/* drop refcnt of maps used by the rejected program */\nstatic void release_maps(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tfor (i = 0; i < env->used_map_cnt; i++)\n\t\tbpf_map_put(env->used_maps[i]);\n}\n\n/* convert pseudo BPF_LD_IMM64 into generic BPF_LD_IMM64 */\nstatic void convert_pseudo_ld_imm64(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint i;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++)\n\t\tif (insn->code == (BPF_LD | BPF_IMM | BPF_DW))\n\t\t\tinsn->src_reg = 0;\n}\n\n/* single env->prog->insni[off] instruction was replaced with the range\n * insni[off, off + cnt).  Adjust corresponding insn_aux_data by copying\n * [0, off) and [off, end) to new locations, so the patched range stays zero\n */\nstatic int adjust_insn_aux_data(struct bpf_verifier_env *env, u32 prog_len,\n\t\t\t\tu32 off, u32 cnt)\n{\n\tstruct bpf_insn_aux_data *new_data, *old_data = env->insn_aux_data;\n\tint i;\n\n\tif (cnt == 1)\n\t\treturn 0;\n\tnew_data = vzalloc(sizeof(struct bpf_insn_aux_data) * prog_len);\n\tif (!new_data)\n\t\treturn -ENOMEM;\n\tmemcpy(new_data, old_data, sizeof(struct bpf_insn_aux_data) * off);\n\tmemcpy(new_data + off + cnt - 1, old_data + off,\n\t       sizeof(struct bpf_insn_aux_data) * (prog_len - off - cnt + 1));\n\tfor (i = off; i < off + cnt - 1; i++)\n\t\tnew_data[i].seen = true;\n\tenv->insn_aux_data = new_data;\n\tvfree(old_data);\n\treturn 0;\n}\n\nstatic struct bpf_prog *bpf_patch_insn_data(struct bpf_verifier_env *env, u32 off,\n\t\t\t\t\t    const struct bpf_insn *patch, u32 len)\n{\n\tstruct bpf_prog *new_prog;\n\n\tnew_prog = bpf_patch_insn_single(env->prog, off, patch, len);\n\tif (!new_prog)\n\t\treturn NULL;\n\tif (adjust_insn_aux_data(env, new_prog->len, off, len))\n\t\treturn NULL;\n\treturn new_prog;\n}\n\n/* The verifier does more data flow analysis than llvm and will not explore\n * branches that are dead at run time. Malicious programs can have dead code\n * too. Therefore replace all dead at-run-time code with nops.\n */\nstatic void sanitize_dead_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn_aux_data *aux_data = env->insn_aux_data;\n\tstruct bpf_insn nop = BPF_MOV64_REG(BPF_REG_0, BPF_REG_0);\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tconst int insn_cnt = env->prog->len;\n\tint i;\n\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (aux_data[i].seen)\n\t\t\tcontinue;\n\t\tmemcpy(insn + i, &nop, sizeof(nop));\n\t}\n}\n\n/* convert load instructions that access fields of 'struct __sk_buff'\n * into sequence of instructions that access fields of 'struct sk_buff'\n */\nstatic int convert_ctx_accesses(struct bpf_verifier_env *env)\n{\n\tconst struct bpf_verifier_ops *ops = env->ops;\n\tint i, cnt, size, ctx_field_size, delta = 0;\n\tconst int insn_cnt = env->prog->len;\n\tstruct bpf_insn insn_buf[16], *insn;\n\tstruct bpf_prog *new_prog;\n\tenum bpf_access_type type;\n\tbool is_narrower_load;\n\tu32 target_size;\n\n\tif (ops->gen_prologue) {\n\t\tcnt = ops->gen_prologue(insn_buf, env->seen_direct_write,\n\t\t\t\t\tenv->prog);\n\t\tif (cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t} else if (cnt) {\n\t\t\tnew_prog = bpf_patch_insn_data(env, 0, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tenv->prog = new_prog;\n\t\t\tdelta += cnt - 1;\n\t\t}\n\t}\n\n\tif (!ops->convert_ctx_access)\n\t\treturn 0;\n\n\tinsn = env->prog->insnsi + delta;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_LDX | BPF_MEM | BPF_B) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_H) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_W) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_DW))\n\t\t\ttype = BPF_READ;\n\t\telse if (insn->code == (BPF_STX | BPF_MEM | BPF_B) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_H) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_W) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_DW))\n\t\t\ttype = BPF_WRITE;\n\t\telse\n\t\t\tcontinue;\n\n\t\tif (env->insn_aux_data[i + delta].ptr_type != PTR_TO_CTX)\n\t\t\tcontinue;\n\n\t\tctx_field_size = env->insn_aux_data[i + delta].ctx_field_size;\n\t\tsize = BPF_LDST_BYTES(insn);\n\n\t\t/* If the read access is a narrower load of the field,\n\t\t * convert to a 4/8-byte load, to minimum program type specific\n\t\t * convert_ctx_access changes. If conversion is successful,\n\t\t * we will apply proper mask to the result.\n\t\t */\n\t\tis_narrower_load = size < ctx_field_size;\n\t\tif (is_narrower_load) {\n\t\t\tu32 off = insn->off;\n\t\t\tu8 size_code;\n\n\t\t\tif (type == BPF_WRITE) {\n\t\t\t\tverbose(env, \"bpf verifier narrow ctx access misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tsize_code = BPF_H;\n\t\t\tif (ctx_field_size == 4)\n\t\t\t\tsize_code = BPF_W;\n\t\t\telse if (ctx_field_size == 8)\n\t\t\t\tsize_code = BPF_DW;\n\n\t\t\tinsn->off = off & ~(ctx_field_size - 1);\n\t\t\tinsn->code = BPF_LDX | BPF_MEM | size_code;\n\t\t}\n\n\t\ttarget_size = 0;\n\t\tcnt = ops->convert_ctx_access(type, insn, insn_buf, env->prog,\n\t\t\t\t\t      &target_size);\n\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf) ||\n\t\t    (ctx_field_size && !target_size)) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (is_narrower_load && size < target_size) {\n\t\t\tif (ctx_field_size <= 4)\n\t\t\t\tinsn_buf[cnt++] = BPF_ALU32_IMM(BPF_AND, insn->dst_reg,\n\t\t\t\t\t\t\t\t(1 << size * 8) - 1);\n\t\t\telse\n\t\t\t\tinsn_buf[cnt++] = BPF_ALU64_IMM(BPF_AND, insn->dst_reg,\n\t\t\t\t\t\t\t\t(1 << size * 8) - 1);\n\t\t}\n\n\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\tif (!new_prog)\n\t\t\treturn -ENOMEM;\n\n\t\tdelta += cnt - 1;\n\n\t\t/* keep walking new program and skip insns we just inserted */\n\t\tenv->prog = new_prog;\n\t\tinsn      = new_prog->insnsi + i + delta;\n\t}\n\n\treturn 0;\n}\n\n/* fixup insn->imm field of bpf_call instructions\n * and inline eligible helpers as explicit sequence of BPF instructions\n *\n * this function is called after eBPF program passed verification\n */\nstatic int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\n\t\tif (insn->imm == BPF_FUNC_get_route_realm)\n\t\t\tprog->dst_needed = 1;\n\t\tif (insn->imm == BPF_FUNC_get_prandom_u32)\n\t\t\tbpf_user_rnd_init_once();\n\t\tif (insn->imm == BPF_FUNC_tail_call) {\n\t\t\t/* If we tail call into other programs, we\n\t\t\t * cannot make any assumptions since they can\n\t\t\t * be replaced dynamically during runtime in\n\t\t\t * the program array.\n\t\t\t */\n\t\t\tprog->cb_access = 1;\n\t\t\tenv->prog->aux->stack_depth = MAX_BPF_STACK;\n\n\t\t\t/* mark bpf_tail_call as different opcode to avoid\n\t\t\t * conditional branch in the interpeter for every normal\n\t\t\t * call and to prevent accidental JITing by JIT compiler\n\t\t\t * that doesn't support bpf_tail_call yet\n\t\t\t */\n\t\t\tinsn->imm = 0;\n\t\t\tinsn->code = BPF_JMP | BPF_TAIL_CALL;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* BPF_EMIT_CALL() assumptions in some of the map_gen_lookup\n\t\t * handlers are currently limited to 64 bit only.\n\t\t */\n\t\tif (ebpf_jit_enabled() && BITS_PER_LONG == 64 &&\n\t\t    insn->imm == BPF_FUNC_map_lookup_elem) {\n\t\t\tmap_ptr = env->insn_aux_data[i + delta].map_ptr;\n\t\t\tif (map_ptr == BPF_MAP_PTR_POISON ||\n\t\t\t    !map_ptr->ops->map_gen_lookup)\n\t\t\t\tgoto patch_call_imm;\n\n\t\t\tcnt = map_ptr->ops->map_gen_lookup(map_ptr, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf,\n\t\t\t\t\t\t       cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta += cnt - 1;\n\n\t\t\t/* keep walking new program and skip insns we just inserted */\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->imm == BPF_FUNC_redirect_map) {\n\t\t\t/* Note, we cannot use prog directly as imm as subsequent\n\t\t\t * rewrites would still change the prog pointer. The only\n\t\t\t * stable address we can use is aux, which also works with\n\t\t\t * prog clones during blinding.\n\t\t\t */\n\t\t\tu64 addr = (unsigned long)prog->aux;\n\t\t\tstruct bpf_insn r4_ld[] = {\n\t\t\t\tBPF_LD_IMM64(BPF_REG_4, addr),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tcnt = ARRAY_SIZE(r4_ld);\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, r4_ld, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t}\npatch_call_imm:\n\t\tfn = env->ops->get_func_proto(insn->imm);\n\t\t/* all functions that have prototype and verifier allowed\n\t\t * programs to call them, must be real in-kernel functions\n\t\t */\n\t\tif (!fn->func) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\t\tfunc_id_name(insn->imm), insn->imm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tinsn->imm = fn->func - __bpf_call_base;\n\t}\n\n\treturn 0;\n}\n\nstatic void free_states(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state_list *sl, *sln;\n\tint i;\n\n\tif (!env->explored_states)\n\t\treturn;\n\n\tfor (i = 0; i < env->prog->len; i++) {\n\t\tsl = env->explored_states[i];\n\n\t\tif (sl)\n\t\t\twhile (sl != STATE_LIST_MARK) {\n\t\t\t\tsln = sl->next;\n\t\t\t\tfree_verifier_state(&sl->state, false);\n\t\t\t\tkfree(sl);\n\t\t\t\tsl = sln;\n\t\t\t}\n\t}\n\n\tkfree(env->explored_states);\n}\n\nint bpf_check(struct bpf_prog **prog, union bpf_attr *attr)\n{\n\tstruct bpf_verifier_env *env;\n\tstruct bpf_verifer_log *log;\n\tint ret = -EINVAL;\n\n\t/* no program is valid */\n\tif (ARRAY_SIZE(bpf_verifier_ops) == 0)\n\t\treturn -EINVAL;\n\n\t/* 'struct bpf_verifier_env' can be global, but since it's not small,\n\t * allocate/free it every time bpf_check() is called\n\t */\n\tenv = kzalloc(sizeof(struct bpf_verifier_env), GFP_KERNEL);\n\tif (!env)\n\t\treturn -ENOMEM;\n\tlog = &env->log;\n\n\tenv->insn_aux_data = vzalloc(sizeof(struct bpf_insn_aux_data) *\n\t\t\t\t     (*prog)->len);\n\tret = -ENOMEM;\n\tif (!env->insn_aux_data)\n\t\tgoto err_free_env;\n\tenv->prog = *prog;\n\tenv->ops = bpf_verifier_ops[env->prog->type];\n\n\t/* grab the mutex to protect few globals used by verifier */\n\tmutex_lock(&bpf_verifier_lock);\n\n\tif (attr->log_level || attr->log_buf || attr->log_size) {\n\t\t/* user requested verbose verifier output\n\t\t * and supplied buffer to store the verification trace\n\t\t */\n\t\tlog->level = attr->log_level;\n\t\tlog->ubuf = (char __user *) (unsigned long) attr->log_buf;\n\t\tlog->len_total = attr->log_size;\n\n\t\tret = -EINVAL;\n\t\t/* log attributes have to be sane */\n\t\tif (log->len_total < 128 || log->len_total > UINT_MAX >> 8 ||\n\t\t    !log->level || !log->ubuf)\n\t\t\tgoto err_unlock;\n\t}\n\n\tenv->strict_alignment = !!(attr->prog_flags & BPF_F_STRICT_ALIGNMENT);\n\tif (!IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS))\n\t\tenv->strict_alignment = true;\n\n\tif (env->prog->aux->offload) {\n\t\tret = bpf_prog_offload_verifier_prep(env);\n\t\tif (ret)\n\t\t\tgoto err_unlock;\n\t}\n\n\tret = replace_map_fd_with_map_ptr(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tenv->explored_states = kcalloc(env->prog->len,\n\t\t\t\t       sizeof(struct bpf_verifier_state_list *),\n\t\t\t\t       GFP_USER);\n\tret = -ENOMEM;\n\tif (!env->explored_states)\n\t\tgoto skip_full_check;\n\n\tret = check_cfg(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tenv->allow_ptr_leaks = capable(CAP_SYS_ADMIN);\n\n\tret = do_check(env);\n\tif (env->cur_state) {\n\t\tfree_verifier_state(env->cur_state, true);\n\t\tenv->cur_state = NULL;\n\t}\n\nskip_full_check:\n\twhile (!pop_stack(env, NULL, NULL));\n\tfree_states(env);\n\n\tif (ret == 0)\n\t\tsanitize_dead_code(env);\n\n\tif (ret == 0)\n\t\t/* program is valid, convert *(u32*)(ctx + off) accesses */\n\t\tret = convert_ctx_accesses(env);\n\n\tif (ret == 0)\n\t\tret = fixup_bpf_calls(env);\n\n\tif (log->level && bpf_verifier_log_full(log))\n\t\tret = -ENOSPC;\n\tif (log->level && !log->ubuf) {\n\t\tret = -EFAULT;\n\t\tgoto err_release_maps;\n\t}\n\n\tif (ret == 0 && env->used_map_cnt) {\n\t\t/* if program passed verifier, update used_maps in bpf_prog_info */\n\t\tenv->prog->aux->used_maps = kmalloc_array(env->used_map_cnt,\n\t\t\t\t\t\t\t  sizeof(env->used_maps[0]),\n\t\t\t\t\t\t\t  GFP_KERNEL);\n\n\t\tif (!env->prog->aux->used_maps) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_release_maps;\n\t\t}\n\n\t\tmemcpy(env->prog->aux->used_maps, env->used_maps,\n\t\t       sizeof(env->used_maps[0]) * env->used_map_cnt);\n\t\tenv->prog->aux->used_map_cnt = env->used_map_cnt;\n\n\t\t/* program is valid. Convert pseudo bpf_ld_imm64 into generic\n\t\t * bpf_ld_imm64 instructions\n\t\t */\n\t\tconvert_pseudo_ld_imm64(env);\n\t}\n\nerr_release_maps:\n\tif (!env->prog->aux->used_maps)\n\t\t/* if we didn't copy map pointers into bpf_prog_info, release\n\t\t * them now. Otherwise free_bpf_prog_info() will release them.\n\t\t */\n\t\trelease_maps(env);\n\t*prog = env->prog;\nerr_unlock:\n\tmutex_unlock(&bpf_verifier_lock);\n\tvfree(env->insn_aux_data);\nerr_free_env:\n\tkfree(env);\n\treturn ret;\n}\n"], "filenames": ["kernel/bpf/verifier.c"], "buggy_code_start_loc": [1061], "buggy_code_end_loc": [1061], "fixing_code_start_loc": [1062], "fixing_code_end_loc": [1067], "type": "CWE-119", "message": "kernel/bpf/verifier.c in the Linux kernel through 4.14.8 allows local users to cause a denial of service (memory corruption) or possibly have unspecified other impact by leveraging the lack of stack-pointer alignment enforcement.", "other": {"cve": {"id": "CVE-2017-17856", "sourceIdentifier": "cve@mitre.org", "published": "2017-12-27T17:08:20.297", "lastModified": "2023-02-07T22:17:37.507", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "kernel/bpf/verifier.c in the Linux kernel through 4.14.8 allows local users to cause a denial of service (memory corruption) or possibly have unspecified other impact by leveraging the lack of stack-pointer alignment enforcement."}, {"lang": "es", "value": "kernel/bpf/verifier.c en el kernel de Linux, en versiones anteriores a la 4.14.8, permite que los usuarios locales provoquen una denegaci\u00f3n de servicio (corrupci\u00f3n de memoria) o, posiblemente, causen otros impactos no especificados aprovechando la falta de aplicaci\u00f3n de la alineaci\u00f3n del puntero de pila."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-119"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.14", "versionEndExcluding": "4.14.9", "matchCriteriaId": "4FBC3D92-C940-441E-AF85-454AB6B8D4D2"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=a5ec6ae161d72f01411169a938fa5f8baea16e8f", "source": "cve@mitre.org", "tags": ["Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2017/12/21/2", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/a5ec6ae161d72f01411169a938fa5f8baea16e8f", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/a5ec6ae161d72f01411169a938fa5f8baea16e8f"}}