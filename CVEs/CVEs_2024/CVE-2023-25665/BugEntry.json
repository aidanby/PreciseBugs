{"buggy_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// SparseSparseBinaryOpShared is the shared code for binary coefficient-wise\n// (cwise) operations of the following form:\n//\n//   sparse_t <binary cwise op> sparse_t -> new sparse_t\n//\n// The output SparseTensor may store up to \"a_nnz + b_nnz\" elements.\n\n// IMPLEMENTATION DETAILS (not part of the interface specification).\n//\n// This kernel implements the \"union\" semantics on the non-zeros: namely, any\n// non-zero from either side participate in the calculations, and any resultant\n// zeros will NOT be excluded from the output storage.\n//\n// (In the future, we could always add a pruning op the prunes away the zeros,\n// if desirable.)\n\n// See docs of all registered ops in ../ops/sparse_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_util.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/cwise_ops.h\"\n#include \"tensorflow/core/kernels/cwise_ops_common.h\"\n#include \"tensorflow/core/util/sparse/sparse_tensor.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\nnamespace {\n// Unions the sparse indices and outputs corresponding values: namely, if a\n// non-zero appear in one side, it will participate in the calculation, where\n// the counterpart on the other side is either a value or an implicit zero.\n//\n// On exit, outputs the augmented values in \"{a,b}_augmented_values\", and fills\n// \"entries_to_copy\" with \"(from_a?, index)\" pairs.  All three vectors have the\n// same size.\n//\n// The input and output sparse tensors are assumed ordered in the canonical\n// row-major order.\ntemplate <typename T>\nvoid UnionSparseIndicesAndValues(\n    typename TTypes<int64_t>::ConstMatrix a_indices_mat,\n    typename TTypes<T>::ConstFlat a_values, int64_t a_nnz,\n    typename TTypes<int64_t>::ConstMatrix b_indices_mat,\n    typename TTypes<T>::ConstFlat b_values, int64_t b_nnz, int num_dims,\n    std::vector<T> *a_augmented_values, std::vector<T> *b_augmented_values,\n    std::vector<std::pair<bool, int64>> *entries_to_copy) {\n  entries_to_copy->reserve(a_nnz + b_nnz);\n  a_augmented_values->reserve(a_nnz);\n  b_augmented_values->reserve(b_nnz);\n\n  int64_t i = 0, j = 0;\n  const T kZero = T(0);\n  while (i < a_nnz && j < b_nnz) {\n    switch (sparse::DimComparator::cmp(a_indices_mat, b_indices_mat, i, j,\n                                       num_dims)) {\n      case -1:\n        entries_to_copy->emplace_back(true, i);\n        a_augmented_values->push_back(a_values(i));\n        b_augmented_values->push_back(kZero);\n        ++i;\n        break;\n      case 0:\n        entries_to_copy->emplace_back(true, i);\n        a_augmented_values->push_back(a_values(i));\n        b_augmented_values->push_back(b_values(j));\n        ++i;\n        ++j;\n        break;\n      case 1:\n        entries_to_copy->emplace_back(false, j);\n        a_augmented_values->push_back(kZero);\n        b_augmented_values->push_back(b_values(j));\n        ++j;\n        break;\n    }\n  }\n  // Handles leftovers; at most one loop runs.\n  while (i < a_nnz) {\n    entries_to_copy->emplace_back(/* is_a */ true, i);\n    a_augmented_values->push_back(a_values(i++));\n    b_augmented_values->push_back(kZero);\n  }\n  while (j < b_nnz) {\n    entries_to_copy->emplace_back(/* is_a */ false, j);\n    a_augmented_values->push_back(kZero);\n    b_augmented_values->push_back(b_values(j++));\n  }\n}\n}  // anonymous namespace\n\n// Device: CPUDevice.  GPU kernel is not supported currently.\n// T: dtype of the SparseTensor's.\n// Functor: binary cwise operation to perform on the corresponding operand\n// values.  See cwise_ops.h for a list of possible functors to register with.\ntemplate <typename Device, typename T, typename Functor>\nclass SparseSparseBinaryOpShared : public OpKernel {\n public:\n  explicit SparseSparseBinaryOpShared(OpKernelConstruction *ctx)\n      : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext *ctx) override {\n    const Tensor *a_indices_t, *a_values_t, *a_shape_t, *b_indices_t,\n        *b_values_t, *b_shape_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_indices\", &a_indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_values\", &a_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_shape\", &a_shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_indices\", &b_indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_values\", &b_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_shape\", &b_shape_t));\n\n    // Validations.\n    OP_REQUIRES(\n        ctx,\n        TensorShapeUtils::IsMatrix(a_indices_t->shape()) &&\n            TensorShapeUtils::IsMatrix(b_indices_t->shape()),\n        errors::InvalidArgument(\"Inputs a_indices and b_indices should be \"\n                                \"matrices but received shapes: \",\n                                a_indices_t->shape().DebugString(), \", \",\n                                b_indices_t->shape().DebugString()));\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(a_values_t->shape()) &&\n                    TensorShapeUtils::IsVector(b_values_t->shape()),\n                errors::InvalidArgument(\n                    \"Inputs a_values and b_values should be vectors \"\n                    \"but received shapes: \",\n                    a_values_t->shape().DebugString(), \" and \",\n                    b_values_t->shape().DebugString()));\n\n    const int64_t a_nnz = a_indices_t->dim_size(0);\n    const int64_t b_nnz = b_indices_t->dim_size(0);\n\n    const auto a_values = a_values_t->vec<T>();\n    const auto b_values = b_values_t->vec<T>();\n\n    OP_REQUIRES(\n        ctx, a_values.size() == a_nnz && b_values.size() == b_nnz,\n        errors::InvalidArgument(\"Expected \", a_nnz, \" and \", b_nnz,\n                                \" non-empty input values, got \",\n                                a_values.size(), \" and \", b_values.size()));\n\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(a_shape_t->shape()) &&\n                    TensorShapeUtils::IsVector(b_shape_t->shape()),\n                errors::InvalidArgument(\n                    \"Input shapes should be a vector but received shapes \",\n                    a_shape_t->shape().DebugString(), \" and \",\n                    b_shape_t->shape().DebugString()));\n    const int num_dims = a_indices_t->dim_size(1);\n    OP_REQUIRES(\n        ctx, a_shape_t->NumElements() == num_dims,\n        errors::InvalidArgument(\"Second dimension of a_indices and length of \"\n                                \"a_shape must match, got \",\n                                num_dims, \" and \", a_shape_t->NumElements()));\n    OP_REQUIRES(ctx, num_dims > 0,\n                errors::InvalidArgument(\"Tensors must not be empty\"));\n    OP_REQUIRES(ctx, a_shape_t->IsSameSize(*b_shape_t),\n                errors::InvalidArgument(\n                    \"Operands do not have the same ranks; got shapes: \",\n                    a_shape_t->SummarizeValue(10), \" and \",\n                    b_shape_t->SummarizeValue(10)));\n    const auto a_shape = a_shape_t->flat<int64_t>();\n    const auto b_shape = b_shape_t->flat<int64_t>();\n    for (int i = 0; i < a_shape_t->NumElements(); ++i) {\n      OP_REQUIRES(ctx, a_shape(i) == b_shape(i),\n                  errors::InvalidArgument(\"Operands' shapes do not match: got \",\n                                          a_shape(i), \" and \", b_shape(i),\n                                          \" for dimension \", i));\n    }\n\n    const auto a_indices_mat = a_indices_t->matrix<int64_t>();\n    const auto b_indices_mat = b_indices_t->matrix<int64_t>();\n    std::vector<T> a_augmented_values, b_augmented_values;\n    std::vector<std::pair<bool, int64>> entries_to_copy;  // from_a?, idx\n    UnionSparseIndicesAndValues(a_indices_mat, a_values, a_nnz, b_indices_mat,\n                                b_values, b_nnz, num_dims, &a_augmented_values,\n                                &b_augmented_values, &entries_to_copy);\n\n    // Allocates and fills output tensors.\n    const int64_t sum_nnz = a_augmented_values.size();\n    Tensor *output_indices_t, *output_values_t;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({sum_nnz, num_dims}),\n                                        &output_indices_t));\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(1, TensorShape({sum_nnz}), &output_values_t));\n    auto output_indices_mat = output_indices_t->matrix<int64_t>();\n\n    for (int64_t i = 0; i < sum_nnz; ++i) {\n      const bool from_a = entries_to_copy[i].first;\n      const int64_t idx = entries_to_copy[i].second;\n      output_indices_mat.chip<0>(i) =\n          from_a ? a_indices_mat.chip<0>(idx) : b_indices_mat.chip<0>(idx);\n    }\n\n    // Performs the functor operation using Eigen.\n    //\n    // Note that the two stack-allocated std::vector's may not be aligned. Using\n    // allocate_temp() would've given us aligned storage, but we do not know\n    // their sizes in advance, so we couldn't use allocate_temp() anyway.\n    //\n    // TODO(zongheng): measure if it's worthwhile to somehow force alignment.\n    using UnalignedTensorMap =\n        Eigen::TensorMap<Eigen::Tensor<const T, 1, Eigen::RowMajor>,\n                         Eigen::Unaligned>;\n    auto a_augmented_values_t =\n        UnalignedTensorMap(a_augmented_values.data(), sum_nnz);\n    auto b_augmented_values_t =\n        UnalignedTensorMap(b_augmented_values.data(), sum_nnz);\n    output_values_t->flat<T>().device(ctx->eigen_device<Device>()) =\n        a_augmented_values_t.binaryExpr(b_augmented_values_t,\n                                        typename Functor::func());\n  }\n};\n\n#define REGISTER_KERNELS(T)                                                  \\\n  REGISTER_KERNEL_BUILDER(                                                   \\\n      Name(\"SparseSparseMinimum\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      SparseSparseBinaryOpShared<CPUDevice, T, functor::minimum<T>>)         \\\n                                                                             \\\n  REGISTER_KERNEL_BUILDER(                                                   \\\n      Name(\"SparseSparseMaximum\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      SparseSparseBinaryOpShared<CPUDevice, T, functor::maximum<T>>)\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for Python ops defined in sparse_ops.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.sparse_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import googletest\nfrom tensorflow.python.platform import test\n\n\n# TODO(zongheng): it'd be great to factor out this function and various random\n# SparseTensor gen funcs.\ndef _sparsify(x, thresh=0.5, index_dtype=np.int64):\n  x[x < thresh] = 0\n\n  non_zero = np.where(x)\n  x_indices = np.vstack(non_zero).astype(index_dtype).T\n  x_values = x[non_zero]\n  x_shape = x.shape\n\n  return sparse_tensor.SparseTensor(\n      indices=x_indices, values=x_values, dense_shape=x_shape), len(x_values)\n\n\nclass SparseToIndicatorTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensor_5x6(self, dtype):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtype),\n        constant_op.constant(shape, dtypes.int64))\n\n  def _SparseTensor_2x3x4(self, dtype):\n    # Includes two entries with the form [1, 1, x] : 150.\n    ind = np.array([[0, 0, 1], [0, 1, 0], [0, 1, 2], [1, 0, 3], [1, 1, 0],\n                    [1, 1, 1], [1, 1, 2], [1, 2, 2]])\n    val = np.array([1, 10, 12, 103, 150, 149, 150, 122])\n    shape = np.array([2, 3, 4])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtype),\n        constant_op.constant(shape, dtypes.int64))\n\n  def testInt32(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6(dtypes.int32)\n      output = sparse_ops.sparse_to_indicator(sp_input, 50)\n\n      expected_output = np.zeros((5, 50), dtype=np.bool_)\n      expected_trues = ((0, 0), (1, 10), (1, 13), (1, 14), (3, 32), (3, 33))\n      for expected_true in expected_trues:\n        expected_output[expected_true] = True\n\n      self.assertAllEqual(output, expected_output)\n\n  def testInt64(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6(dtypes.int64)\n      output = sparse_ops.sparse_to_indicator(sp_input, 50)\n\n      expected_output = np.zeros((5, 50), dtype=np.bool_)\n      expected_trues = [(0, 0), (1, 10), (1, 13), (1, 14), (3, 32), (3, 33)]\n      for expected_true in expected_trues:\n        expected_output[expected_true] = True\n\n      self.assertAllEqual(output, expected_output)\n\n  def testHigherRank(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x3x4(dtypes.int64)\n      output = sparse_ops.sparse_to_indicator(sp_input, 200)\n\n      expected_output = np.zeros((2, 3, 200), dtype=np.bool_)\n      expected_trues = [(0, 0, 1), (0, 1, 10), (0, 1, 12), (1, 0, 103),\n                        (1, 1, 149), (1, 1, 150), (1, 2, 122)]\n      for expected_true in expected_trues:\n        expected_output[expected_true] = True\n\n      self.assertAllEqual(output, expected_output)\n\n\nclass SparseMergeTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensorValue_3x50(self, indices_dtype, values_dtype):\n    # NOTE: This input is intentionally not sorted to validate the\n    # already_sorted flag below.\n    ind = np.array([[0, 0], [1, 0], [1, 2], [2, 0], [2, 1], [1, 1]])\n    # NB: these are not sorted\n    indices = np.array([0, 13, 10, 33, 32, 14])\n    values = np.array([-3, 4, 1, 9, 5, 1])\n    shape = np.array([3, 3])\n    indices = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(indices, indices_dtype), np.array(shape, np.int64))\n    values = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(values, values_dtype), np.array(shape, np.int64))\n    return indices, values\n\n  def _SparseTensor_3x50(self, indices_dtype, values_dtype):\n    indices, values = self._SparseTensorValue_3x50(indices_dtype, values_dtype)\n    return (sparse_tensor.SparseTensor.from_value(indices),\n            sparse_tensor.SparseTensor.from_value(values))\n\n  def _AssertResultsSorted(self, output, vocab_size):\n    self.assertAllEqual(output.indices,\n                        [[0, 0], [1, 10], [1, 13], [1, 14], [2, 32], [2, 33]])\n    self.assertAllEqual(output.values, [-3, 1, 4, 1, 5, 9])\n    self.assertAllEqual(output.dense_shape, [3, vocab_size])\n\n  def _AssertResultsNotSorted(self, output, vocab_size):\n    self.assertAllEqual(output.indices,\n                        [[0, 0], [1, 13], [1, 10], [2, 33], [2, 32], [1, 14]])\n    self.assertAllEqual(output.values, [-3, 4, 1, 9, 5, 1])\n    self.assertAllEqual(output.dense_shape, [3, vocab_size])\n\n  def testInt32AndFloat32(self):\n    vocab_size = 50\n    indices_v, values_v = self._SparseTensorValue_3x50(np.int32, np.float32)\n    with test_util.force_cpu():\n      for indices in (indices_v,\n                      sparse_tensor.SparseTensor.from_value(indices_v)):\n        for values in (values_v,\n                       sparse_tensor.SparseTensor.from_value(values_v)):\n          sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n          output = self.evaluate(sp_output)\n          self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat32(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float32)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat64(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt32AndFloat32NonCanonicalOrder(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int32, np.float32)\n      sp_output = sparse_ops.sparse_merge(\n          indices, values, vocab_size, already_sorted=True)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsNotSorted(output, vocab_size)\n\n  def testInt64AndFloat32NonCanonicalOrder(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float32)\n      sp_output = sparse_ops.sparse_merge(\n          indices, values, vocab_size, already_sorted=True)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsNotSorted(output, vocab_size)\n\n  def testInt64AndFloat64NonCanonicalOrder(self):\n    vocab_size = 50\n    vocab_size_tensor = constant_op.constant(vocab_size, dtypes.int64)\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(\n          indices, values, vocab_size_tensor, already_sorted=True)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsNotSorted(output, vocab_size)\n\n  def testShouldSetLastDimensionInDynamicShape(self):\n    with ops.Graph().as_default():\n      shape = constant_op.constant([2, 2], dtype=dtypes.int64)\n      dynamic_shape = array_ops.placeholder_with_default(shape, shape=[2])\n      ids = sparse_tensor.SparseTensor(\n          indices=[[0, 0], [0, 1]],\n          values=[1, 3],\n          dense_shape=dynamic_shape)\n      values = sparse_tensor.SparseTensor(\n          indices=[[0, 0], [0, 1]],\n          values=[0.4, 0.7],\n          dense_shape=dynamic_shape)\n      merged = sparse_ops.sparse_merge(\n          sp_ids=ids, sp_values=values, vocab_size=5)\n      self.assertEqual(5, merged.get_shape()[1])\n\n\nclass SparseMergeHighDimTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensor_3x50(self, indices_dtype, values_dtype):\n    # NOTE: This input is intentionally not sorted to validate the\n    # already_sorted flag below.\n    ind = np.array([[0, 0], [1, 0], [1, 2], [2, 0], [2, 1], [1, 1]])\n    # NB: these are not sorted\n    indices0 = np.array([0, 13, 10, 33, 32, 14])\n    indices1 = np.array([12, 4, 0, 0, 1, 30])\n    values = np.array([-3, 4, 1, 9, 5, 1])\n    shape = np.array([3, 3])\n    indices0 = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(indices0, indices_dtype), np.array(shape, np.int64))\n    indices1 = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(indices1, indices_dtype), np.array(shape, np.int64))\n    values = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(values, values_dtype), np.array(shape, np.int64))\n    return ([sparse_tensor.SparseTensor.from_value(indices0),\n             sparse_tensor.SparseTensor.from_value(indices1)],\n            sparse_tensor.SparseTensor.from_value(values))\n\n  def _AssertResultsSorted(self, output, vocab_size):\n    self.assertAllEqual(\n        output.indices,\n        [[0, 0, 12], [1, 10, 0], [1, 13, 4], [1, 14, 30], [2, 32, 1],\n         [2, 33, 0]])\n    self.assertAllEqual(output.values, [-3, 1, 4, 1, 5, 9])\n    self.assertAllEqual(output.dense_shape, [3] + vocab_size)\n\n  def testInt64AndFloat32(self):\n    vocab_size = [50, 31]\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float32)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat64(self):\n    vocab_size = [50, 31]\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat64Shape(self):\n    vocab_size = [50, 30]\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n\nclass SparseRetainTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensorValue_5x6(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(val, np.int32), np.array(shape, np.int64))\n\n  def _SparseTensor_5x6(self):\n    return sparse_tensor.SparseTensor.from_value(self._SparseTensorValue_5x6())\n\n  def testBasic(self):\n    with test_util.force_cpu():\n      for sp_input in (self._SparseTensorValue_5x6(), self._SparseTensor_5x6()):\n        to_retain = np.array([1, 0, 0, 1, 1, 0], dtype=np.bool_)\n        sp_output = sparse_ops.sparse_retain(sp_input, to_retain)\n\n        output = self.evaluate(sp_output)\n\n        self.assertAllEqual(output.indices, [[0, 0], [1, 4], [3, 2]])\n        self.assertAllEqual(output.values, [0, 14, 32])\n        self.assertAllEqual(output.dense_shape, [5, 6])\n\n  def testRetainNone(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6()\n      to_retain = np.zeros((6,), dtype=np.bool_)\n      sp_output = sparse_ops.sparse_retain(sp_input, to_retain)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, np.array([]).reshape((0, 2)))\n      self.assertAllEqual(output.values, [])\n      self.assertAllEqual(output.dense_shape, [5, 6])\n\n  def testMismatchedRetainShape(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6()\n      to_retain = np.array([1, 0, 0, 1, 0], dtype=np.bool_)\n      with self.assertRaises(ValueError):\n        sparse_ops.sparse_retain(sp_input, to_retain)\n\n\nclass SparseResetShapeTest(test_util.TensorFlowTestCase):\n\n  _IND_2_5_6 = np.array(\n      [[0, 0, 0], [0, 1, 0], [0, 1, 3], [1, 1, 4], [1, 3, 2], [1, 3, 3]],\n      dtype=np.int64)\n  _VAL_2_5_6 = np.array([0, 10, 13, 14, 32, 33], dtype=np.int32)\n  _SHP_2_5_6 = np.array([2, 5, 6], dtype=np.int64)\n\n  def _SparseTensor_2x5x6(self):\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(self._IND_2_5_6, dtypes.int64),\n        constant_op.constant(self._VAL_2_5_6, dtypes.int32),\n        constant_op.constant(self._SHP_2_5_6, dtypes.int64))\n\n  def _SparseTensor_2x5x6_Empty(self):\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(\n            np.empty(shape=[0, 3], dtype=np.int64), dtypes.int64),\n        constant_op.constant(np.empty(shape=[0], dtype=np.int32), dtypes.int32),\n        constant_op.constant(self._SHP_2_5_6, dtypes.int64))\n\n  def _SparseTensorValue_2x5x6(self):\n    return sparse_tensor.SparseTensorValue(self._IND_2_5_6, self._VAL_2_5_6,\n                                           self._SHP_2_5_6)\n\n  def testStaticShapeInfoPreservedWhenNewShapeIsProvidedAndStatic(self):\n    sp_input = self._SparseTensor_2x5x6()\n    new_shape = np.array([3, 6, 7], dtype=np.int64)\n    sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n    self.assertAllEqual([3, 6, 7], sp_output.get_shape())\n\n  def testBasic(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6()\n      new_shape = np.array([3, 6, 7], dtype=np.int64)\n      sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [3, 6, 7])\n\n  def testInputUnavailableInGraphConstructionOk(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensorValue_2x5x6()\n      new_shape = np.array([3, 6, 7], dtype=np.int64)\n      sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [3, 6, 7])\n\n  @test_util.run_deprecated_v1\n  def testFeedInputUnavailableInGraphConstructionOk(self):\n    with self.session(use_gpu=False) as sess:\n      sp_input = array_ops.sparse_placeholder(dtype=dtypes.int32)\n      new_shape = np.array([3, 6, 7], dtype=np.int64)\n      sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      output = sess.run(sp_output,\n                        feed_dict={sp_input: self._SparseTensorValue_2x5x6()})\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [3, 6, 7])\n\n  def testTightBoundingBox(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6()\n      sp_output = sparse_ops.sparse_reset_shape(sp_input)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [2, 4, 5])\n\n  def testTightBoundingBoxEmpty(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6_Empty()\n      sp_output = sparse_ops.sparse_reset_shape(sp_input)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices.shape, [0, 3])\n      self.assertAllEqual(output.values.shape, [0])\n      self.assertAllEqual(output.dense_shape, [0, 0, 0])\n\n  def testInvalidRank(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6()\n      new_shape = np.array([3, 7], dtype=np.int64)\n\n      with self.assertRaises(ValueError):\n        sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n  @test_util.run_deprecated_v1\n  def testInvalidRankNewShapeUnavailableInGraphConstruction(self):\n    with self.session(use_gpu=False) as sess:\n      new_shape = array_ops.placeholder(dtype=dtypes.int64)\n      sp_input = self._SparseTensor_2x5x6()\n      out = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      with self.assertRaisesOpError(\"x == y did not hold element-wise\"):\n        sess.run(out, feed_dict={new_shape: np.array([3, 7], dtype=np.int64)})\n\n  def testInvalidDimensionSizeStatic(self):\n    sp_input = self._SparseTensor_2x5x6()\n    new_shape = np.array([3, 7, 5], dtype=np.int64)\n\n    with self.assertRaisesRegex(ValueError, \"should have dimension sizes\"):\n      sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n  @test_util.run_deprecated_v1\n  def testInvalidDimensionSizeDynamic(self):\n    with self.session(use_gpu=False) as sess:\n      sp_input = self._SparseTensor_2x5x6()\n      new_shape = array_ops.placeholder(dtype=dtypes.int32)\n      out = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      with self.assertRaisesOpError(\"x <= y did not hold element-wise\"):\n        sess.run(out, feed_dict={new_shape: [3, 7, 5]})\n\n  @test_util.run_deprecated_v1\n  def testInvalidDimensionSizeInputUnavailableInGraphConstruction(self):\n    sp_input = array_ops.sparse_placeholder(dtype=dtypes.int32)\n    with self.session(use_gpu=False) as sess:\n      new_shape = np.array([3, 7, 5], dtype=np.int64)\n      out = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      with self.assertRaisesOpError(\"x <= y did not hold element-wise\"):\n        sess.run(out, feed_dict={sp_input: self._SparseTensorValue_2x5x6()})\n\n\nclass SparseSetShapeTest(test_util.TensorFlowTestCase):\n\n  def testSetShapeEagerValidates(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    sp = sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtypes.int64),\n        constant_op.constant(shape, dtypes.int64))\n\n    self.assertEqual(sp.shape, tensor_shape.TensorShape([5, 6]))\n\n    sp.set_shape(tensor_shape.TensorShape(None))\n    sp.set_shape(tensor_shape.TensorShape([None, None]))\n    sp.set_shape(tensor_shape.TensorShape([5, None]))\n    sp.set_shape(tensor_shape.TensorShape([None, 6]))\n    sp.set_shape(tensor_shape.TensorShape([5, 6]))\n\n    with self.assertRaises(ValueError):\n      sp.set_shape([None, None, None])\n\n    with self.assertRaises(ValueError):\n      sp.set_shape([3, None])\n\n    with self.assertRaises(ValueError):\n      sp.set_shape([None, 7])\n\n    with self.assertRaises(ValueError):\n      sp.set_shape([3, 6])\n\n  def testSetShapeFunctionMerges(self):\n\n    @def_function.function\n    def dynamic_shape_sparse(dense_shape):\n      ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n      val = np.array([0, 10, 13, 14, 32, 33])\n      sp = sparse_tensor.SparseTensor(\n          constant_op.constant(ind, dtypes.int64),\n          constant_op.constant(val, dtypes.int64),\n          dense_shape)\n\n      sp.set_shape(tensor_shape.TensorShape(None))\n      self.assertEqual(sp.shape, tensor_shape.TensorShape(None))\n\n      sp.set_shape(tensor_shape.TensorShape([None, None]))\n      self.assertEqual(sp.shape, tensor_shape.TensorShape([None, None]))\n\n      sp.set_shape(tensor_shape.TensorShape([5, None]))\n      self.assertEqual(sp.shape, tensor_shape.TensorShape([5, None]))\n\n      sp.set_shape(tensor_shape.TensorShape([None, 6]))\n      self.assertEqual(sp.shape, tensor_shape.TensorShape([5, 6]))\n\n      sp.set_shape(tensor_shape.TensorShape([None, None]))\n      self.assertEqual(sp.shape, tensor_shape.TensorShape([5, 6]))\n\n      sp.set_shape(tensor_shape.TensorShape([5, 6]))\n      self.assertEqual(sp.shape, tensor_shape.TensorShape([5, 6]))\n\n      with self.assertRaises(ValueError):\n        sp.set_shape([None, None, None])\n\n      with self.assertRaises(ValueError):\n        sp.set_shape([3, None])\n\n      with self.assertRaises(ValueError):\n        sp.set_shape([None, 7])\n\n      with self.assertRaises(ValueError):\n        sp.set_shape([3, 6])\n\n    dense_shape_spec = tensor_spec.TensorSpec(None, dtypes.int64)\n    _ = dynamic_shape_sparse.get_concrete_function(dense_shape_spec)\n\n\nclass SparseFillEmptyRowsTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensorValue_5x6(self, dtype=np.int32):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64), np.array(val, dtype), np.array(\n            shape, np.int64))\n\n  def _SparseTensor_5x6(self):\n    return sparse_tensor.SparseTensor.from_value(self._SparseTensorValue_5x6())\n\n  def _SparseTensor_String5x6(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtypes.string),\n        constant_op.constant(shape, dtypes.int64))\n\n  def _SparseTensor_2x6(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4]])\n    val = np.array([0, 10, 13, 14])\n    shape = np.array([2, 6])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtypes.int32),\n        constant_op.constant(shape, dtypes.int64))\n\n  def testFillNumber(self):\n    with test_util.use_gpu():\n      for sp_input in (self._SparseTensorValue_5x6(), self._SparseTensor_5x6()):\n        sp_output, empty_row_indicator = (\n            sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n        output, empty_row_indicator_out = self.evaluate(\n            [sp_output, empty_row_indicator])\n\n        self.assertAllEqual(\n            output.indices,\n            [[0, 0], [1, 0], [1, 3], [1, 4], [2, 0], [3, 2], [3, 3], [4, 0]])\n        self.assertAllEqual(output.values, [0, 10, 13, 14, -1, 32, 33, -1])\n        self.assertAllEqual(output.dense_shape, [5, 6])\n        self.assertAllEqual(empty_row_indicator_out,\n                            np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n\n  def testSparseFillEmptyRowsGradEmpty(self):\n    with test_util.use_gpu():\n      grad, _ = self.evaluate(\n          sparse_ops.sparse_fill_empty_rows_grad(\n              reverse_index_map=[], grad_values=[]))\n      self.assertAllEqual(grad, [])\n\n  @test_util.run_deprecated_v1\n  def testFillFloat(self):\n    with self.session():\n      values = constant_op.constant(\n          [0.0, 10.0, 13.0, 14.0, 32.0, 33.0], dtype=dtypes.float64)\n      default_value = constant_op.constant(-1.0, dtype=dtypes.float64)\n      sp_input = sparse_tensor.SparseTensorValue(\n          indices=np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]]),\n          values=values,\n          dense_shape=np.array([5, 6]))\n      sp_output, empty_row_indicator = (sparse_ops.sparse_fill_empty_rows(\n          sp_input, default_value))\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 0], [1, 0], [1, 3], [1, 4],\n                                           [2, 0], [3, 2], [3, 3], [4, 0]])\n      self.assertAllClose(output.values, [0, 10, 13, 14, -1, 32, 33, -1])\n      self.assertAllEqual(output.dense_shape, [5, 6])\n      self.assertAllEqual(empty_row_indicator_out,\n                          np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n\n      values_grad_err = gradient_checker.compute_gradient_error(\n          values, values.shape.as_list(), sp_output.values, [8], delta=1e-8)\n      self.assertGreater(values_grad_err, 0)\n      self.assertLess(values_grad_err, 1e-8)\n\n      default_value_grad_err = gradient_checker.compute_gradient_error(\n          default_value,\n          default_value.shape.as_list(),\n          sp_output.values, [8],\n          delta=1e-8)\n      self.assertGreater(default_value_grad_err, 0)\n      self.assertLess(default_value_grad_err, 1e-8)\n\n  def testFillString(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_String5x6()\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, \"\"))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(\n          output.indices,\n          [[0, 0], [1, 0], [1, 3], [1, 4], [2, 0], [3, 2], [3, 3], [4, 0]])\n      self.assertAllEqual(output.values,\n                          [b\"a\", b\"b\", b\"c\", b\"d\", b\"\", b\"e\", b\"f\", b\"\"])\n      self.assertAllEqual(output.dense_shape, [5, 6])\n      self.assertAllEqual(empty_row_indicator_out,\n                          np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n\n  def testNoEmptyRows(self):\n    with test_util.use_gpu():\n      sp_input = self._SparseTensor_2x6()\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 0], [1, 0], [1, 3], [1, 4]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14])\n      self.assertAllEqual(output.dense_shape, [2, 6])\n      self.assertAllEqual(empty_row_indicator_out, np.zeros(2).astype(np.bool_))\n\n  def testNoEmptyRowsAndUnordered(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.array([[1, 2], [1, 3], [0, 1], [0, 3]]),\n          values=np.array([1, 3, 2, 4]),\n          dense_shape=np.array([2, 5]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 1], [0, 3], [1, 2], [1, 3]])\n      self.assertAllEqual(output.values, [2, 4, 1, 3])\n      self.assertAllEqual(output.dense_shape, [2, 5])\n      self.assertAllEqual(empty_row_indicator_out, np.zeros(2).astype(np.bool_))\n\n  def testUnordered(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.array([[2, 3], [2, 2], [0, 1], [0, 3]]),\n          values=np.array([1, 3, 2, 4]),\n          dense_shape=np.array([3, 5]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices,\n                          [[0, 1], [0, 3], [1, 0], [2, 3], [2, 2]])\n      self.assertAllEqual(output.values, [2, 4, -1, 1, 3])\n      self.assertAllEqual(output.dense_shape, [3, 5])\n      self.assertAllEqual(empty_row_indicator_out, [False, True, False])\n\n  def testEmptyIndicesTensor(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.ones([0, 2]),\n          values=np.ones([0]),\n          dense_shape=np.array([2, 5]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 0], [1, 0]])\n      self.assertAllEqual(output.values, [-1, -1])\n      self.assertAllEqual(output.dense_shape, [2, 5])\n      self.assertAllEqual(empty_row_indicator_out, np.ones(2).astype(np.bool_))\n\n  def testEmptyOutput(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.ones([0, 2]),\n          values=np.ones([0]),\n          dense_shape=np.array([0, 3]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, np.ones([0, 2]))\n      self.assertAllEqual(output.values, np.ones([0]))\n      self.assertAllEqual(output.dense_shape, [0, 3])\n      self.assertAllEqual(empty_row_indicator_out, [])\n\n  def testInvalidIndices(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.array([[1, 2], [1, 3], [99, 1], [99, 3]]),\n          values=np.array([1, 3, 2, 4]),\n          dense_shape=np.array([2, 5]))\n\n      with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                  r\"indices\\(2, 0\\) is invalid\"):\n        self.evaluate(sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n\nclass SparseAddTest(test_util.TensorFlowTestCase):\n\n  def testValuesInVariable(self):\n    indices = constant_op.constant([[0]], dtype=dtypes.int64)\n    values = variables.Variable([1], trainable=False, dtype=dtypes.float32)\n    shape = constant_op.constant([1], dtype=dtypes.int64)\n\n    sp_input = sparse_tensor.SparseTensor(indices, values, shape)\n    sp_output = sparse_ops.sparse_add(sp_input, sp_input)\n\n    with test_util.force_cpu():\n      self.evaluate(variables.global_variables_initializer())\n      output = self.evaluate(sp_output)\n      self.assertAllEqual(output.values, [2])\n\n\nclass SparseReduceTest(test_util.TensorFlowTestCase):\n\n  # [[1, ?, 2]\n  #  [?, 3, ?]]\n  # where ? is implicitly-zero.\n  ind = np.array([[0, 0], [0, 2], [1, 1]]).astype(np.int64)\n  vals = np.array([1, 1, 1]).astype(np.int32)\n  dense_shape = np.array([2, 3]).astype(np.int64)\n\n  def _compare(self, sp_t, reduction_axes, ndims, keep_dims, do_sum):\n    densified = self.evaluate(sparse_ops.sparse_tensor_to_dense(sp_t))\n\n    np_ans = densified\n    if reduction_axes is None:\n      if do_sum:\n        np_ans = np.sum(np_ans, keepdims=keep_dims)\n      else:\n        np_ans = np.max(np_ans, keepdims=keep_dims)\n    else:\n      if not isinstance(reduction_axes, list):  # Single scalar.\n        reduction_axes = [reduction_axes]\n      reduction_axes = np.array(reduction_axes).astype(np.int32)\n      # Handles negative axes.\n      reduction_axes = (reduction_axes + ndims) % ndims\n      # Loop below depends on sorted.\n      reduction_axes.sort()\n      for ra in reduction_axes.ravel()[::-1]:\n        if do_sum:\n          np_ans = np.sum(np_ans, axis=ra, keepdims=keep_dims)\n        else:\n          np_ans = np.max(np_ans, axis=ra, keepdims=keep_dims)\n\n    with self.cached_session():\n      if do_sum:\n        tf_dense_ans = sparse_ops.sparse_reduce_sum(sp_t, reduction_axes,\n                                                    keep_dims)\n      else:\n        tf_dense_ans = sparse_ops.sparse_reduce_max(sp_t, reduction_axes,\n                                                    keep_dims)\n      out_dense = self.evaluate(tf_dense_ans)\n\n      if do_sum:\n        tf_sparse_ans = sparse_ops.sparse_reduce_sum_sparse(sp_t,\n                                                            reduction_axes,\n                                                            keep_dims)\n      else:\n        tf_sparse_ans = sparse_ops.sparse_reduce_max_sparse(sp_t,\n                                                            reduction_axes,\n                                                            keep_dims)\n      # Convert to dense for comparison purposes.\n      out_sparse = sparse_ops.sparse_tensor_to_dense(tf_sparse_ans)\n\n    self.assertAllClose(np_ans, out_dense)\n    self.assertAllClose(np_ans, out_sparse)\n\n  def _compare_all(self, sp_t, reduction_axes, ndims):\n    self._compare(sp_t, reduction_axes, ndims, False, False)\n    self._compare(sp_t, reduction_axes, ndims, False, True)\n    self._compare(sp_t, reduction_axes, ndims, True, False)\n    self._compare(sp_t, reduction_axes, ndims, True, True)\n\n  # (TODO:b/133851381): Re-enable this test.\n  def disabledtestSimpleAndRandomInputs(self):\n    if np.__version__ == \"1.13.0\":\n      self.skipTest(\"numpy 1.13.0 bug\")\n\n    sp_t = sparse_tensor.SparseTensor(self.ind, self.vals, self.dense_shape)\n\n    with test_util.force_cpu():\n      self._compare_all(sp_t, None, ndims=2)\n      self._compare_all(sp_t, 0, ndims=2)\n      self._compare_all(sp_t, [1], ndims=2)\n      self._compare_all(sp_t, [0, 1], ndims=2)\n      self._compare_all(sp_t, [1, 0], ndims=2)\n      self._compare_all(sp_t, [-1], ndims=2)\n      self._compare_all(sp_t, [1, -2], ndims=2)\n\n    np.random.seed(1618)\n    test_dims = [(1618, 1, 11, 7, 1), (1,), (1, 1, 1)]\n    with test_util.force_cpu():\n      for dims in test_dims:\n        sp_t, unused_nnz = _sparsify(np.random.randn(*dims))\n        # reduce all using None\n        self._compare_all(sp_t, None, ndims=len(dims))\n        # reduce random axes from 1D to N-D\n        for d in range(1, len(dims) + 1):\n          axes = np.random.choice(len(dims), size=d, replace=False).tolist()\n          self._compare_all(sp_t, axes, ndims=len(dims))\n\n  def testInvalidAxes(self):\n    sp_t = sparse_tensor.SparseTensor(self.ind, self.vals, self.dense_shape)\n    with test_util.force_cpu():\n      with self.assertRaisesOpError(\"Invalid reduction dimension -3\"):\n        self.evaluate(sparse_ops.sparse_reduce_sum(sp_t, -3))\n      with self.assertRaisesOpError(\"Invalid reduction dimension 2\"):\n        self.evaluate(sparse_ops.sparse_reduce_sum(sp_t, 2))\n      with self.assertRaisesOpError(\"Invalid reduction dimension -3\"):\n        self.evaluate(sparse_ops.sparse_reduce_max(sp_t, -3))\n      with self.assertRaisesOpError(\"Invalid reduction dimension 2\"):\n        self.evaluate(sparse_ops.sparse_reduce_max(sp_t, 2))\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    np.random.seed(8161)\n    test_dims = [(11, 1, 5, 7, 1), (2, 2)]\n    with self.session(use_gpu=False):\n      for dims in test_dims:\n        sp_t, nnz = _sparsify(np.random.randn(*dims))\n        # reduce random axes from 1D to N-D\n        for d in range(1, len(dims) + 1):\n          axes = np.random.choice(len(dims), size=d, replace=False).tolist()\n          reduced = sparse_ops.sparse_reduce_sum(sp_t, axes)\n\n          err = gradient_checker.compute_gradient_error(\n              sp_t.values, (nnz,), reduced,\n              self.evaluate(reduced).shape)\n          self.assertLess(err, 1e-3)\n\n        # Tests for negative axes.\n        reduced = sparse_ops.sparse_reduce_sum(sp_t, -1)\n        err = gradient_checker.compute_gradient_error(\n            sp_t.values, (nnz,), reduced,\n            self.evaluate(reduced).shape)\n        self.assertLess(err, 1e-3)\n\n  def _testSparseReduceShape(self, sp_t, reduction_axes, ndims, keep_dims,\n                             do_sum):\n    densified = self.evaluate(sparse_ops.sparse_tensor_to_dense(sp_t))\n\n    np_op = np.sum\n    tf_op = sparse_ops.sparse_reduce_sum\n    if not do_sum:\n      np_op = np.max\n      tf_op = sparse_ops.sparse_reduce_max\n\n    np_ans = densified\n    if reduction_axes is None:\n      np_ans = np_op(np_ans, keepdims=keep_dims)\n    else:\n      if not isinstance(reduction_axes, list):  # Single scalar.\n        reduction_axes = [reduction_axes]\n      reduction_axes = np.array(reduction_axes).astype(np.int32)\n      # Handles negative axes.\n      reduction_axes = (reduction_axes + ndims) % ndims\n      # Loop below depends on sorted.\n      reduction_axes.sort()\n      for ra in reduction_axes.ravel()[::-1]:\n        np_ans = np_op(np_ans, axis=ra, keepdims=keep_dims)\n\n    tf_ans = tf_op(sp_t, reduction_axes, keep_dims)\n    self.assertAllEqual(np_ans.shape, tf_ans.get_shape().as_list())\n\n  # (TODO:b/133851381): Re-enable this test\n  def disabledtestSparseReduceSumOrMaxShape(self):\n    sp_t = sparse_tensor.SparseTensor(self.ind, self.vals, self.dense_shape)\n\n    with test_util.force_cpu():\n      for do_sum in [True, False]:\n        for keep_dims in [True, False]:\n          self._testSparseReduceShape(sp_t, None, 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, 0, 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [1], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [0, 1], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [1, 0], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [-1], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [1, -2], 2, keep_dims, do_sum)\n\n  def testIntegerOverflow(self):\n    with self.cached_session(use_gpu=False):\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_reduce_max(\n            input_indices=[[1, 2], [3, 4]],\n            input_shape=[2**32, 2**31],\n            input_values=[1, 3],\n            reduction_axes=[0],\n            keep_dims=False,\n            name=None)\n\n        self.evaluate(res)\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_reduce_max_sparse(\n            input_indices=[[1, 2], [3, 4]],\n            input_shape=[2**32, 2**31],\n            input_values=[1, 3],\n            reduction_axes=[0],\n            keep_dims=False,\n            name=None)\n\n        self.evaluate(res)\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_reduce_sum(\n            input_indices=[[1, 2], [3, 4]],\n            input_shape=[2**32, 2**31],\n            input_values=[1, 3],\n            reduction_axes=[0],\n            keep_dims=False,\n            name=None)\n\n        self.evaluate(res)\n\n\nclass SparseMathOpsTest(test_util.TensorFlowTestCase):\n\n  def _check(self, result_tensor, result_np, input_sp_t):\n    self.assertTrue(isinstance(result_tensor, sparse_tensor.SparseTensor))\n    self.assertTrue(isinstance(input_sp_t, sparse_tensor.SparseTensor))\n    self.assertAllCloseAccordingToType(input_sp_t.indices,\n                                       result_tensor.indices)\n    self.assertAllCloseAccordingToType(input_sp_t.dense_shape,\n                                       result_tensor.dense_shape)\n\n    res_densified = sparse_ops.sparse_to_dense(\n        result_tensor.indices, result_tensor.dense_shape, result_tensor.values)\n    self.assertAllCloseAccordingToType(result_np, res_densified)\n\n  @test_util.run_deprecated_v1\n  def testCwiseShapeValidation(self):\n    # Test case for GitHub 24072.\n    with test_util.force_cpu():\n      a = array_ops.ones([3, 4, 1], dtype=dtypes.int32)\n      b = sparse_tensor.SparseTensor([[0, 0, 1, 0], [0, 0, 3, 0]], [10, 20],\n                                     [1, 1, 4, 2])\n      c = a * b\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          \"broadcasts dense to sparse only; got incompatible shapes\"):\n        self.evaluate(c)\n\n  def testCwiseDivAndMul(self):\n    np.random.seed(1618)\n    sp_shapes = [(10, 10, 10), (5, 5), (1618,), (3, 3, 7)]\n    dense_shapes = [(10, 10, 1), (5, 5), (1,), (1, 7)]\n\n    with test_util.force_cpu():\n      for dtype in [np.float32, np.float64, np.int32, np.int64]:\n        for sp_shape, dense_shape in zip(sp_shapes, dense_shapes):\n          sp_vals_np = np.random.rand(*sp_shape).astype(dtype) + 1\n          dense_vals_np = np.random.rand(*dense_shape).astype(dtype) + 1\n          sp_t, unused_nnz = _sparsify(sp_vals_np, thresh=1.5)\n          sp_t_densified = sparse_ops.sparse_tensor_to_dense(sp_t)\n          dense_t = constant_op.constant(dense_vals_np)\n\n          self._check(sp_t / dense_t, sp_t_densified / dense_vals_np, sp_t)\n          # Check commutative.\n          self._check(sp_t * dense_t, sp_t_densified * dense_vals_np, sp_t)\n          self._check(dense_t * sp_t, sp_t_densified * dense_vals_np, sp_t)\n\n          if dtype in [np.int32, np.int64]:\n            res = sp_t / dense_t  # should invoke \"__truediv__\"\n            self.assertEqual(res.values.dtype, np.float64)\n\n  def testCwiseAdd(self):\n    with test_util.force_cpu():\n      # Identity(2) + AllOnes(2,2).  Should be equal to 2 * Identity(2).\n      indices = [[0, 0], [1, 1]]\n      vals = [1, 1]\n      shape = (2, 2)\n\n      sp_t = sparse_tensor.SparseTensor(indices, vals, shape)\n      dense_t = array_ops.ones(shape, dtype=dtypes.int32)\n      self._check(\n          sparse_ops.sparse_dense_cwise_add(sp_t, dense_t),\n          np.identity(2) * 2, sp_t)\n\n      # Variant of above, but broadcasts the dense side.\n      dense_t = array_ops.ones([1], dtype=dtypes.int32)\n      self._check(\n          sparse_ops.sparse_dense_cwise_add(sp_t, dense_t),\n          np.identity(2) * 2, sp_t)\n\n  @test_util.run_deprecated_v1\n  def testGradients(self):\n    np.random.seed(1618)\n    sp_shapes = [(10, 10, 10), (5, 5), (1618,), (3, 3, 7)]\n    dense_shapes = [(10, 10, 1), (5, 5), (1,), (1, 7)]\n\n    with self.session(use_gpu=False):\n      for dtype in [np.float32, np.float64]:\n        for sp_shape, dense_shape in zip(sp_shapes, dense_shapes):\n          sp_vals_np = np.random.rand(*sp_shape).astype(dtype) + 1\n          dense_vals_np = np.random.rand(*dense_shape).astype(dtype) + 1\n          sp_t, nnz = _sparsify(sp_vals_np, thresh=1.5)\n          dense_t = constant_op.constant(dense_vals_np)\n\n          cmul = sp_t * dense_t\n          err = gradient_checker.compute_gradient_error([sp_t.values, dense_t],\n                                                        [(nnz,), dense_shape],\n                                                        cmul.values, (nnz,))\n          self.assertLess(err, 1e-4)\n\n          cdiv = sp_t / dense_t\n          err = gradient_checker.compute_gradient_error(sp_t.values, (nnz,),\n                                                        cdiv.values, (nnz,))\n          self.assertLess(err, 1e-4)\n          err = gradient_checker.compute_gradient_error(\n              dense_t,\n              dense_shape,\n              cdiv.values, (nnz,),\n              x_init_value=dense_vals_np)\n          self.assertLess(err, 2e-4)\n\n\nclass SparseSoftmaxTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testEquivalentToDensified(self):\n    np.random.seed(1618)\n    n, m = np.random.choice(20, size=2)\n\n    for dtype in [np.float16, np.float32, np.float64]:\n      sp_vals_np = np.random.rand(n, m).astype(dtype)\n\n      batched_sp_t, unused_nnz1 = _sparsify(\n          sp_vals_np.reshape((1, n, m)), thresh=0.)  # No masking.\n\n      with test_util.force_cpu():\n        densified = constant_op.constant(sp_vals_np)\n\n        sp_result = self.evaluate(\n            sparse_ops.sparse_softmax(batched_sp_t)).values.reshape((n, m))\n        dense_result = nn_ops.softmax(densified)\n\n        self.assertAllCloseAccordingToType(dense_result, sp_result)\n\n  def testHigherRanks(self):\n    # For the first shape:\n    # First batch:\n    # [?   e.]\n    # [1.  ? ]\n    # Second batch:\n    # [e   ? ]\n    # [e   e ]\n    #\n    # The softmax results should be:\n    # [?   1.]     [1    ?]\n    # [1.  ? ] and [.5  .5]\n    # where ? means implicitly zero.\n    #\n    # The second shape: same input data, but with a higher-rank shape.\n    shapes = [[2, 2, 2], [2, 1, 2, 2]]\n    for shape in shapes:\n      values = np.asarray(\n          [0., np.e, 1., 0., np.e, 0., np.e, np.e]).reshape(shape)\n      sp_t, unused_nnz = _sparsify(values, thresh=1e-2)\n      expected_values = [1., 1., 1., .5, .5]\n\n      with test_util.force_cpu():\n        result = sparse_ops.sparse_softmax(sp_t)\n\n        self.assertAllEqual(expected_values, result.values)\n        self.assertAllEqual(sp_t.indices, result.indices)\n        self.assertAllEqual(shape, result.dense_shape)\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    x_shape = [2, 5, 10]\n    with self.cached_session(use_gpu=False):\n      for dtype in [np.float32, np.float64]:\n        x_np = np.random.randn(*x_shape).astype(dtype)\n        x_tf, nnz = _sparsify(x_np)\n        y_tf = sparse_ops.sparse_softmax(x_tf)\n        err = gradient_checker.compute_gradient_error(x_tf.values, (nnz,),\n                                                      y_tf.values, (nnz,))\n        self.assertLess(err, 1e-4)\n\n  def testIntegerOverflow(self):\n    with self.cached_session(use_gpu=False):\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_softmax(\n            sp_indices=[[1, 1]],\n            sp_values=[2.0],\n            sp_shape=[2**32, 2**31],\n            name=None)\n\n        self.evaluate(res)\n\n  def testReshapeNegativeShape(self):\n    with self.cached_session(use_gpu=False):\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_softmax(\n            sp_indices=[[1, 1]], sp_values=[2.0], sp_shape=[-1, 1], name=None)\n\n        self.evaluate(res)\n\n\nclass SparseMinimumMaximumTest(test_util.TensorFlowTestCase):\n\n  def _assertSparseTensorValueEqual(self, a, b):\n    self.assertAllEqual(a.indices, b.indices)\n    self.assertAllEqual(a.values, b.values)\n    self.assertAllEqual(a.dense_shape, b.dense_shape)\n\n  def testBasic(self):\n    with test_util.force_cpu():\n      # 1-D, values at index 0.\n      sp_zero = sparse_tensor.SparseTensor([[0]], [0], [7])\n      sp_one = sparse_tensor.SparseTensor([[0]], [1], [7])\n      max_tf = sparse_ops.sparse_maximum(sp_zero, sp_one)\n      min_tf = sparse_ops.sparse_minimum(sp_zero, sp_one)\n      self._assertSparseTensorValueEqual(sp_one, max_tf)\n      self._assertSparseTensorValueEqual(sp_zero, min_tf)\n\n      # Values at different indices.\n      sp_zero = sparse_tensor.SparseTensor([[0]], [0], [7])\n      sp_zero_2 = sparse_tensor.SparseTensor([[1]], [0], [7])\n      expected = sparse_tensor.SparseTensor([[0], [1]], [0, 0], [7])\n      max_tf = sparse_ops.sparse_maximum(sp_zero, sp_zero_2)\n      min_tf = sparse_ops.sparse_minimum(sp_zero, sp_zero_2)\n      self._assertSparseTensorValueEqual(expected, max_tf)\n      self._assertSparseTensorValueEqual(expected, min_tf)\n\n  @test_util.run_deprecated_v1\n  def testRandom(self):\n    np.random.seed(1618)\n    shapes = [(13,), (6, 8), (1, 7, 1)]\n    for shape in shapes:\n      for dtype in [np.int32, np.int64, np.float16, np.float32, np.float64]:\n        a_np = np.random.randn(*shape).astype(dtype)\n        b_np = np.random.randn(*shape).astype(dtype)\n        sp_a, unused_a_nnz = _sparsify(a_np, thresh=-.5)\n        sp_b, unused_b_nnz = _sparsify(b_np, thresh=-.5)\n\n        with self.cached_session(use_gpu=False):\n          maximum_tf = sparse_ops.sparse_maximum(sp_a, sp_b)\n          maximum_tf_densified = sparse_ops.sparse_tensor_to_dense(\n              maximum_tf).eval()\n          minimum_tf = sparse_ops.sparse_minimum(sp_a, sp_b)\n          minimum_tf_densified = sparse_ops.sparse_tensor_to_dense(\n              minimum_tf).eval()\n\n          a_densified = sparse_ops.sparse_tensor_to_dense(sp_a).eval()\n          b_densified = sparse_ops.sparse_tensor_to_dense(sp_b).eval()\n\n        self.assertAllEqual(\n            np.maximum(a_densified, b_densified), maximum_tf_densified)\n        self.assertAllEqual(\n            np.minimum(a_densified, b_densified), minimum_tf_densified)\n\n  def testMismatchedShapes(self):\n    with test_util.force_cpu():\n      sp_zero = sparse_tensor.SparseTensor([[0, 0]], [0], [1, 1])\n      sp_one = sparse_tensor.SparseTensor([[0]], [1], [2])\n      with self.assertRaisesOpError(\"Operands do not have the same ranks\"):\n        self.evaluate(sparse_ops.sparse_maximum(sp_zero, sp_one))\n\n      sp_zero = sparse_tensor.SparseTensor([[0]], [0], [1])\n      sp_one = sparse_tensor.SparseTensor([[0]], [1], [2])\n      with self.assertRaisesOpError(\"Operands' shapes do not match\"):\n        self.evaluate(sparse_ops.sparse_maximum(sp_zero, sp_one))\n\n\nclass SparseTransposeTest(test.TestCase):\n\n  def testTranspose(self):\n    if np.__version__ == \"1.13.0\":\n      self.skipTest(\"numpy 1.13.0 bug\")\n\n    with test_util.force_cpu():\n      np.random.seed(1618)\n      shapes = [np.random.randint(1, 10, size=rank) for rank in range(1, 6)]\n      for shape in shapes:\n        for dtype in [np.int32, np.int64, np.float32, np.float64]:\n          dn_input = np.random.randn(*shape).astype(dtype)\n          rank = self.evaluate(array_ops.rank(dn_input))\n          perm = np.random.choice(rank, rank, False)\n          sp_input, unused_a_nnz = _sparsify(dn_input)\n          sp_trans = sparse_ops.sparse_transpose(sp_input, perm=perm)\n          dn_trans = sparse_ops.sparse_tensor_to_dense(sp_trans)\n          expected_trans = array_ops.transpose(dn_input, perm=perm)\n          self.assertAllEqual(expected_trans.shape, sp_trans.get_shape())\n          self.assertAllEqual(dn_trans, expected_trans)\n\n\nclass SparsePlaceholderTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testPlaceholder(self):\n    foo = array_ops.sparse_placeholder(dtypes.float32, shape=(10, 47))\n    self.assertAllEqual([10, 47], foo.get_shape())\n    self.assertAllEqual([None, 2], foo.indices.get_shape().as_list())\n\n  @test_util.run_deprecated_v1\n  def testPartialShapePlaceholder(self):\n    foo = array_ops.sparse_placeholder(dtypes.float32, shape=(None, 47))\n    self.assertAllEqual([None, 47], foo.get_shape().as_list())\n    self.assertAllEqual([None, 2], foo.indices.get_shape().as_list())\n\n  @test_util.run_deprecated_v1\n  def testNoShapePlaceholder(self):\n    foo = array_ops.sparse_placeholder(dtypes.float32, shape=None)\n    self.assertAllEqual(None, foo.get_shape())\n    self.assertAllEqual([None, None], foo.indices.get_shape().as_list())\n\n\nif __name__ == \"__main__\":\n  googletest.main()\n"], "fixing_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// SparseSparseBinaryOpShared is the shared code for binary coefficient-wise\n// (cwise) operations of the following form:\n//\n//   sparse_t <binary cwise op> sparse_t -> new sparse_t\n//\n// The output SparseTensor may store up to \"a_nnz + b_nnz\" elements.\n\n// IMPLEMENTATION DETAILS (not part of the interface specification).\n//\n// This kernel implements the \"union\" semantics on the non-zeros: namely, any\n// non-zero from either side participate in the calculations, and any resultant\n// zeros will NOT be excluded from the output storage.\n//\n// (In the future, we could always add a pruning op the prunes away the zeros,\n// if desirable.)\n\n// See docs of all registered ops in ../ops/sparse_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_util.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/cwise_ops.h\"\n#include \"tensorflow/core/kernels/cwise_ops_common.h\"\n#include \"tensorflow/core/kernels/sparse_utils.h\"\n#include \"tensorflow/core/util/sparse/sparse_tensor.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\nnamespace {\n// Unions the sparse indices and outputs corresponding values: namely, if a\n// non-zero appear in one side, it will participate in the calculation, where\n// the counterpart on the other side is either a value or an implicit zero.\n//\n// On exit, outputs the augmented values in \"{a,b}_augmented_values\", and fills\n// \"entries_to_copy\" with \"(from_a?, index)\" pairs.  All three vectors have the\n// same size.\n//\n// The input and output sparse tensors are assumed ordered in the canonical\n// row-major order.\ntemplate <typename T>\nvoid UnionSparseIndicesAndValues(\n    typename TTypes<int64_t>::ConstMatrix a_indices_mat,\n    typename TTypes<T>::ConstFlat a_values, int64_t a_nnz,\n    typename TTypes<int64_t>::ConstMatrix b_indices_mat,\n    typename TTypes<T>::ConstFlat b_values, int64_t b_nnz, int num_dims,\n    std::vector<T> *a_augmented_values, std::vector<T> *b_augmented_values,\n    std::vector<std::pair<bool, int64>> *entries_to_copy) {\n  entries_to_copy->reserve(a_nnz + b_nnz);\n  a_augmented_values->reserve(a_nnz);\n  b_augmented_values->reserve(b_nnz);\n\n  int64_t i = 0, j = 0;\n  const T kZero = T(0);\n  while (i < a_nnz && j < b_nnz) {\n    switch (sparse::DimComparator::cmp(a_indices_mat, b_indices_mat, i, j,\n                                       num_dims)) {\n      case -1:\n        entries_to_copy->emplace_back(true, i);\n        a_augmented_values->push_back(a_values(i));\n        b_augmented_values->push_back(kZero);\n        ++i;\n        break;\n      case 0:\n        entries_to_copy->emplace_back(true, i);\n        a_augmented_values->push_back(a_values(i));\n        b_augmented_values->push_back(b_values(j));\n        ++i;\n        ++j;\n        break;\n      case 1:\n        entries_to_copy->emplace_back(false, j);\n        a_augmented_values->push_back(kZero);\n        b_augmented_values->push_back(b_values(j));\n        ++j;\n        break;\n    }\n  }\n  // Handles leftovers; at most one loop runs.\n  while (i < a_nnz) {\n    entries_to_copy->emplace_back(/* is_a */ true, i);\n    a_augmented_values->push_back(a_values(i++));\n    b_augmented_values->push_back(kZero);\n  }\n  while (j < b_nnz) {\n    entries_to_copy->emplace_back(/* is_a */ false, j);\n    a_augmented_values->push_back(kZero);\n    b_augmented_values->push_back(b_values(j++));\n  }\n}\n}  // anonymous namespace\n\n// Device: CPUDevice.  GPU kernel is not supported currently.\n// T: dtype of the SparseTensor's.\n// Functor: binary cwise operation to perform on the corresponding operand\n// values.  See cwise_ops.h for a list of possible functors to register with.\ntemplate <typename Device, typename T, typename Functor>\nclass SparseSparseBinaryOpShared : public OpKernel {\n public:\n  explicit SparseSparseBinaryOpShared(OpKernelConstruction *ctx)\n      : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext *ctx) override {\n    const Tensor *a_indices_t, *a_values_t, *a_shape_t, *b_indices_t,\n        *b_values_t, *b_shape_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_indices\", &a_indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_values\", &a_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_shape\", &a_shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_indices\", &b_indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_values\", &b_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_shape\", &b_shape_t));\n\n    // Validations.\n    OP_REQUIRES_OK(ctx, sparse_utils::ValidateSparseTensor<int64_t>(\n                            *a_indices_t, *a_values_t, *a_shape_t,\n                            sparse_utils::IndexValidation::kUnordered));\n    OP_REQUIRES_OK(ctx, sparse_utils::ValidateSparseTensor<int64_t>(\n                            *b_indices_t, *b_values_t, *b_shape_t,\n                            sparse_utils::IndexValidation::kUnordered));\n\n    const int64_t a_nnz = a_indices_t->dim_size(0);\n    const int64_t b_nnz = b_indices_t->dim_size(0);\n\n    const auto a_values = a_values_t->vec<T>();\n    const auto b_values = b_values_t->vec<T>();\n\n    const int num_dims = a_indices_t->dim_size(1);\n    OP_REQUIRES(ctx, num_dims > 0,\n                errors::InvalidArgument(\"Tensors must not be empty\"));\n    OP_REQUIRES(ctx, a_shape_t->IsSameSize(*b_shape_t),\n                errors::InvalidArgument(\n                    \"Operands do not have the same ranks; got shapes: \",\n                    a_shape_t->SummarizeValue(10), \" and \",\n                    b_shape_t->SummarizeValue(10)));\n    const auto a_shape = a_shape_t->flat<int64_t>();\n    const auto b_shape = b_shape_t->flat<int64_t>();\n    for (int i = 0; i < a_shape_t->NumElements(); ++i) {\n      OP_REQUIRES(ctx, a_shape(i) == b_shape(i),\n                  errors::InvalidArgument(\"Operands' shapes do not match: got \",\n                                          a_shape(i), \" and \", b_shape(i),\n                                          \" for dimension \", i));\n    }\n\n    const auto a_indices_mat = a_indices_t->matrix<int64_t>();\n    const auto b_indices_mat = b_indices_t->matrix<int64_t>();\n    std::vector<T> a_augmented_values, b_augmented_values;\n    std::vector<std::pair<bool, int64_t>> entries_to_copy;  // from_a?, idx\n    UnionSparseIndicesAndValues(a_indices_mat, a_values, a_nnz, b_indices_mat,\n                                b_values, b_nnz, num_dims, &a_augmented_values,\n                                &b_augmented_values, &entries_to_copy);\n\n    // Allocates and fills output tensors.\n    const int64_t sum_nnz = a_augmented_values.size();\n    Tensor *output_indices_t, *output_values_t;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({sum_nnz, num_dims}),\n                                        &output_indices_t));\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(1, TensorShape({sum_nnz}), &output_values_t));\n    auto output_indices_mat = output_indices_t->matrix<int64_t>();\n\n    for (int64_t i = 0; i < sum_nnz; ++i) {\n      const bool from_a = entries_to_copy[i].first;\n      const int64_t idx = entries_to_copy[i].second;\n      output_indices_mat.chip<0>(i) =\n          from_a ? a_indices_mat.chip<0>(idx) : b_indices_mat.chip<0>(idx);\n    }\n\n    // Performs the functor operation using Eigen.\n    //\n    // Note that the two stack-allocated std::vector's may not be aligned. Using\n    // allocate_temp() would've given us aligned storage, but we do not know\n    // their sizes in advance, so we couldn't use allocate_temp() anyway.\n    //\n    // TODO(zongheng): measure if it's worthwhile to somehow force alignment.\n    using UnalignedTensorMap =\n        Eigen::TensorMap<Eigen::Tensor<const T, 1, Eigen::RowMajor>,\n                         Eigen::Unaligned>;\n    auto a_augmented_values_t =\n        UnalignedTensorMap(a_augmented_values.data(), sum_nnz);\n    auto b_augmented_values_t =\n        UnalignedTensorMap(b_augmented_values.data(), sum_nnz);\n    output_values_t->flat<T>().device(ctx->eigen_device<Device>()) =\n        a_augmented_values_t.binaryExpr(b_augmented_values_t,\n                                        typename Functor::func());\n  }\n};\n\n#define REGISTER_KERNELS(T)                                                  \\\n  REGISTER_KERNEL_BUILDER(                                                   \\\n      Name(\"SparseSparseMinimum\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      SparseSparseBinaryOpShared<CPUDevice, T, functor::minimum<T>>)         \\\n                                                                             \\\n  REGISTER_KERNEL_BUILDER(                                                   \\\n      Name(\"SparseSparseMaximum\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      SparseSparseBinaryOpShared<CPUDevice, T, functor::maximum<T>>)\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for Python ops defined in sparse_ops.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_sparse_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.sparse_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import googletest\nfrom tensorflow.python.platform import test\n\n\n# TODO(zongheng): it'd be great to factor out this function and various random\n# SparseTensor gen funcs.\ndef _sparsify(x, thresh=0.5, index_dtype=np.int64):\n  x[x < thresh] = 0\n\n  non_zero = np.where(x)\n  x_indices = np.vstack(non_zero).astype(index_dtype).T\n  x_values = x[non_zero]\n  x_shape = x.shape\n\n  return sparse_tensor.SparseTensor(\n      indices=x_indices, values=x_values, dense_shape=x_shape), len(x_values)\n\n\nclass SparseToIndicatorTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensor_5x6(self, dtype):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtype),\n        constant_op.constant(shape, dtypes.int64))\n\n  def _SparseTensor_2x3x4(self, dtype):\n    # Includes two entries with the form [1, 1, x] : 150.\n    ind = np.array([[0, 0, 1], [0, 1, 0], [0, 1, 2], [1, 0, 3], [1, 1, 0],\n                    [1, 1, 1], [1, 1, 2], [1, 2, 2]])\n    val = np.array([1, 10, 12, 103, 150, 149, 150, 122])\n    shape = np.array([2, 3, 4])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtype),\n        constant_op.constant(shape, dtypes.int64))\n\n  def testInt32(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6(dtypes.int32)\n      output = sparse_ops.sparse_to_indicator(sp_input, 50)\n\n      expected_output = np.zeros((5, 50), dtype=np.bool_)\n      expected_trues = ((0, 0), (1, 10), (1, 13), (1, 14), (3, 32), (3, 33))\n      for expected_true in expected_trues:\n        expected_output[expected_true] = True\n\n      self.assertAllEqual(output, expected_output)\n\n  def testInt64(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6(dtypes.int64)\n      output = sparse_ops.sparse_to_indicator(sp_input, 50)\n\n      expected_output = np.zeros((5, 50), dtype=np.bool_)\n      expected_trues = [(0, 0), (1, 10), (1, 13), (1, 14), (3, 32), (3, 33)]\n      for expected_true in expected_trues:\n        expected_output[expected_true] = True\n\n      self.assertAllEqual(output, expected_output)\n\n  def testHigherRank(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x3x4(dtypes.int64)\n      output = sparse_ops.sparse_to_indicator(sp_input, 200)\n\n      expected_output = np.zeros((2, 3, 200), dtype=np.bool_)\n      expected_trues = [(0, 0, 1), (0, 1, 10), (0, 1, 12), (1, 0, 103),\n                        (1, 1, 149), (1, 1, 150), (1, 2, 122)]\n      for expected_true in expected_trues:\n        expected_output[expected_true] = True\n\n      self.assertAllEqual(output, expected_output)\n\n\nclass SparseMergeTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensorValue_3x50(self, indices_dtype, values_dtype):\n    # NOTE: This input is intentionally not sorted to validate the\n    # already_sorted flag below.\n    ind = np.array([[0, 0], [1, 0], [1, 2], [2, 0], [2, 1], [1, 1]])\n    # NB: these are not sorted\n    indices = np.array([0, 13, 10, 33, 32, 14])\n    values = np.array([-3, 4, 1, 9, 5, 1])\n    shape = np.array([3, 3])\n    indices = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(indices, indices_dtype), np.array(shape, np.int64))\n    values = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(values, values_dtype), np.array(shape, np.int64))\n    return indices, values\n\n  def _SparseTensor_3x50(self, indices_dtype, values_dtype):\n    indices, values = self._SparseTensorValue_3x50(indices_dtype, values_dtype)\n    return (sparse_tensor.SparseTensor.from_value(indices),\n            sparse_tensor.SparseTensor.from_value(values))\n\n  def _AssertResultsSorted(self, output, vocab_size):\n    self.assertAllEqual(output.indices,\n                        [[0, 0], [1, 10], [1, 13], [1, 14], [2, 32], [2, 33]])\n    self.assertAllEqual(output.values, [-3, 1, 4, 1, 5, 9])\n    self.assertAllEqual(output.dense_shape, [3, vocab_size])\n\n  def _AssertResultsNotSorted(self, output, vocab_size):\n    self.assertAllEqual(output.indices,\n                        [[0, 0], [1, 13], [1, 10], [2, 33], [2, 32], [1, 14]])\n    self.assertAllEqual(output.values, [-3, 4, 1, 9, 5, 1])\n    self.assertAllEqual(output.dense_shape, [3, vocab_size])\n\n  def testInt32AndFloat32(self):\n    vocab_size = 50\n    indices_v, values_v = self._SparseTensorValue_3x50(np.int32, np.float32)\n    with test_util.force_cpu():\n      for indices in (indices_v,\n                      sparse_tensor.SparseTensor.from_value(indices_v)):\n        for values in (values_v,\n                       sparse_tensor.SparseTensor.from_value(values_v)):\n          sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n          output = self.evaluate(sp_output)\n          self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat32(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float32)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat64(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt32AndFloat32NonCanonicalOrder(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int32, np.float32)\n      sp_output = sparse_ops.sparse_merge(\n          indices, values, vocab_size, already_sorted=True)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsNotSorted(output, vocab_size)\n\n  def testInt64AndFloat32NonCanonicalOrder(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float32)\n      sp_output = sparse_ops.sparse_merge(\n          indices, values, vocab_size, already_sorted=True)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsNotSorted(output, vocab_size)\n\n  def testInt64AndFloat64NonCanonicalOrder(self):\n    vocab_size = 50\n    vocab_size_tensor = constant_op.constant(vocab_size, dtypes.int64)\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(\n          indices, values, vocab_size_tensor, already_sorted=True)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsNotSorted(output, vocab_size)\n\n  def testShouldSetLastDimensionInDynamicShape(self):\n    with ops.Graph().as_default():\n      shape = constant_op.constant([2, 2], dtype=dtypes.int64)\n      dynamic_shape = array_ops.placeholder_with_default(shape, shape=[2])\n      ids = sparse_tensor.SparseTensor(\n          indices=[[0, 0], [0, 1]],\n          values=[1, 3],\n          dense_shape=dynamic_shape)\n      values = sparse_tensor.SparseTensor(\n          indices=[[0, 0], [0, 1]],\n          values=[0.4, 0.7],\n          dense_shape=dynamic_shape)\n      merged = sparse_ops.sparse_merge(\n          sp_ids=ids, sp_values=values, vocab_size=5)\n      self.assertEqual(5, merged.get_shape()[1])\n\n\nclass SparseMergeHighDimTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensor_3x50(self, indices_dtype, values_dtype):\n    # NOTE: This input is intentionally not sorted to validate the\n    # already_sorted flag below.\n    ind = np.array([[0, 0], [1, 0], [1, 2], [2, 0], [2, 1], [1, 1]])\n    # NB: these are not sorted\n    indices0 = np.array([0, 13, 10, 33, 32, 14])\n    indices1 = np.array([12, 4, 0, 0, 1, 30])\n    values = np.array([-3, 4, 1, 9, 5, 1])\n    shape = np.array([3, 3])\n    indices0 = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(indices0, indices_dtype), np.array(shape, np.int64))\n    indices1 = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(indices1, indices_dtype), np.array(shape, np.int64))\n    values = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(values, values_dtype), np.array(shape, np.int64))\n    return ([sparse_tensor.SparseTensor.from_value(indices0),\n             sparse_tensor.SparseTensor.from_value(indices1)],\n            sparse_tensor.SparseTensor.from_value(values))\n\n  def _AssertResultsSorted(self, output, vocab_size):\n    self.assertAllEqual(\n        output.indices,\n        [[0, 0, 12], [1, 10, 0], [1, 13, 4], [1, 14, 30], [2, 32, 1],\n         [2, 33, 0]])\n    self.assertAllEqual(output.values, [-3, 1, 4, 1, 5, 9])\n    self.assertAllEqual(output.dense_shape, [3] + vocab_size)\n\n  def testInt64AndFloat32(self):\n    vocab_size = [50, 31]\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float32)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat64(self):\n    vocab_size = [50, 31]\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat64Shape(self):\n    vocab_size = [50, 30]\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n\nclass SparseRetainTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensorValue_5x6(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(val, np.int32), np.array(shape, np.int64))\n\n  def _SparseTensor_5x6(self):\n    return sparse_tensor.SparseTensor.from_value(self._SparseTensorValue_5x6())\n\n  def testBasic(self):\n    with test_util.force_cpu():\n      for sp_input in (self._SparseTensorValue_5x6(), self._SparseTensor_5x6()):\n        to_retain = np.array([1, 0, 0, 1, 1, 0], dtype=np.bool_)\n        sp_output = sparse_ops.sparse_retain(sp_input, to_retain)\n\n        output = self.evaluate(sp_output)\n\n        self.assertAllEqual(output.indices, [[0, 0], [1, 4], [3, 2]])\n        self.assertAllEqual(output.values, [0, 14, 32])\n        self.assertAllEqual(output.dense_shape, [5, 6])\n\n  def testRetainNone(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6()\n      to_retain = np.zeros((6,), dtype=np.bool_)\n      sp_output = sparse_ops.sparse_retain(sp_input, to_retain)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, np.array([]).reshape((0, 2)))\n      self.assertAllEqual(output.values, [])\n      self.assertAllEqual(output.dense_shape, [5, 6])\n\n  def testMismatchedRetainShape(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6()\n      to_retain = np.array([1, 0, 0, 1, 0], dtype=np.bool_)\n      with self.assertRaises(ValueError):\n        sparse_ops.sparse_retain(sp_input, to_retain)\n\n\nclass SparseResetShapeTest(test_util.TensorFlowTestCase):\n\n  _IND_2_5_6 = np.array(\n      [[0, 0, 0], [0, 1, 0], [0, 1, 3], [1, 1, 4], [1, 3, 2], [1, 3, 3]],\n      dtype=np.int64)\n  _VAL_2_5_6 = np.array([0, 10, 13, 14, 32, 33], dtype=np.int32)\n  _SHP_2_5_6 = np.array([2, 5, 6], dtype=np.int64)\n\n  def _SparseTensor_2x5x6(self):\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(self._IND_2_5_6, dtypes.int64),\n        constant_op.constant(self._VAL_2_5_6, dtypes.int32),\n        constant_op.constant(self._SHP_2_5_6, dtypes.int64))\n\n  def _SparseTensor_2x5x6_Empty(self):\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(\n            np.empty(shape=[0, 3], dtype=np.int64), dtypes.int64),\n        constant_op.constant(np.empty(shape=[0], dtype=np.int32), dtypes.int32),\n        constant_op.constant(self._SHP_2_5_6, dtypes.int64))\n\n  def _SparseTensorValue_2x5x6(self):\n    return sparse_tensor.SparseTensorValue(self._IND_2_5_6, self._VAL_2_5_6,\n                                           self._SHP_2_5_6)\n\n  def testStaticShapeInfoPreservedWhenNewShapeIsProvidedAndStatic(self):\n    sp_input = self._SparseTensor_2x5x6()\n    new_shape = np.array([3, 6, 7], dtype=np.int64)\n    sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n    self.assertAllEqual([3, 6, 7], sp_output.get_shape())\n\n  def testBasic(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6()\n      new_shape = np.array([3, 6, 7], dtype=np.int64)\n      sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [3, 6, 7])\n\n  def testInputUnavailableInGraphConstructionOk(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensorValue_2x5x6()\n      new_shape = np.array([3, 6, 7], dtype=np.int64)\n      sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [3, 6, 7])\n\n  @test_util.run_deprecated_v1\n  def testFeedInputUnavailableInGraphConstructionOk(self):\n    with self.session(use_gpu=False) as sess:\n      sp_input = array_ops.sparse_placeholder(dtype=dtypes.int32)\n      new_shape = np.array([3, 6, 7], dtype=np.int64)\n      sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      output = sess.run(sp_output,\n                        feed_dict={sp_input: self._SparseTensorValue_2x5x6()})\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [3, 6, 7])\n\n  def testTightBoundingBox(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6()\n      sp_output = sparse_ops.sparse_reset_shape(sp_input)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [2, 4, 5])\n\n  def testTightBoundingBoxEmpty(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6_Empty()\n      sp_output = sparse_ops.sparse_reset_shape(sp_input)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices.shape, [0, 3])\n      self.assertAllEqual(output.values.shape, [0])\n      self.assertAllEqual(output.dense_shape, [0, 0, 0])\n\n  def testInvalidRank(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6()\n      new_shape = np.array([3, 7], dtype=np.int64)\n\n      with self.assertRaises(ValueError):\n        sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n  @test_util.run_deprecated_v1\n  def testInvalidRankNewShapeUnavailableInGraphConstruction(self):\n    with self.session(use_gpu=False) as sess:\n      new_shape = array_ops.placeholder(dtype=dtypes.int64)\n      sp_input = self._SparseTensor_2x5x6()\n      out = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      with self.assertRaisesOpError(\"x == y did not hold element-wise\"):\n        sess.run(out, feed_dict={new_shape: np.array([3, 7], dtype=np.int64)})\n\n  def testInvalidDimensionSizeStatic(self):\n    sp_input = self._SparseTensor_2x5x6()\n    new_shape = np.array([3, 7, 5], dtype=np.int64)\n\n    with self.assertRaisesRegex(ValueError, \"should have dimension sizes\"):\n      sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n  @test_util.run_deprecated_v1\n  def testInvalidDimensionSizeDynamic(self):\n    with self.session(use_gpu=False) as sess:\n      sp_input = self._SparseTensor_2x5x6()\n      new_shape = array_ops.placeholder(dtype=dtypes.int32)\n      out = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      with self.assertRaisesOpError(\"x <= y did not hold element-wise\"):\n        sess.run(out, feed_dict={new_shape: [3, 7, 5]})\n\n  @test_util.run_deprecated_v1\n  def testInvalidDimensionSizeInputUnavailableInGraphConstruction(self):\n    sp_input = array_ops.sparse_placeholder(dtype=dtypes.int32)\n    with self.session(use_gpu=False) as sess:\n      new_shape = np.array([3, 7, 5], dtype=np.int64)\n      out = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      with self.assertRaisesOpError(\"x <= y did not hold element-wise\"):\n        sess.run(out, feed_dict={sp_input: self._SparseTensorValue_2x5x6()})\n\n\nclass SparseSetShapeTest(test_util.TensorFlowTestCase):\n\n  def testSetShapeEagerValidates(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    sp = sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtypes.int64),\n        constant_op.constant(shape, dtypes.int64))\n\n    self.assertEqual(sp.shape, tensor_shape.TensorShape([5, 6]))\n\n    sp.set_shape(tensor_shape.TensorShape(None))\n    sp.set_shape(tensor_shape.TensorShape([None, None]))\n    sp.set_shape(tensor_shape.TensorShape([5, None]))\n    sp.set_shape(tensor_shape.TensorShape([None, 6]))\n    sp.set_shape(tensor_shape.TensorShape([5, 6]))\n\n    with self.assertRaises(ValueError):\n      sp.set_shape([None, None, None])\n\n    with self.assertRaises(ValueError):\n      sp.set_shape([3, None])\n\n    with self.assertRaises(ValueError):\n      sp.set_shape([None, 7])\n\n    with self.assertRaises(ValueError):\n      sp.set_shape([3, 6])\n\n  def testSetShapeFunctionMerges(self):\n\n    @def_function.function\n    def dynamic_shape_sparse(dense_shape):\n      ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n      val = np.array([0, 10, 13, 14, 32, 33])\n      sp = sparse_tensor.SparseTensor(\n          constant_op.constant(ind, dtypes.int64),\n          constant_op.constant(val, dtypes.int64),\n          dense_shape)\n\n      sp.set_shape(tensor_shape.TensorShape(None))\n      self.assertEqual(sp.shape, tensor_shape.TensorShape(None))\n\n      sp.set_shape(tensor_shape.TensorShape([None, None]))\n      self.assertEqual(sp.shape, tensor_shape.TensorShape([None, None]))\n\n      sp.set_shape(tensor_shape.TensorShape([5, None]))\n      self.assertEqual(sp.shape, tensor_shape.TensorShape([5, None]))\n\n      sp.set_shape(tensor_shape.TensorShape([None, 6]))\n      self.assertEqual(sp.shape, tensor_shape.TensorShape([5, 6]))\n\n      sp.set_shape(tensor_shape.TensorShape([None, None]))\n      self.assertEqual(sp.shape, tensor_shape.TensorShape([5, 6]))\n\n      sp.set_shape(tensor_shape.TensorShape([5, 6]))\n      self.assertEqual(sp.shape, tensor_shape.TensorShape([5, 6]))\n\n      with self.assertRaises(ValueError):\n        sp.set_shape([None, None, None])\n\n      with self.assertRaises(ValueError):\n        sp.set_shape([3, None])\n\n      with self.assertRaises(ValueError):\n        sp.set_shape([None, 7])\n\n      with self.assertRaises(ValueError):\n        sp.set_shape([3, 6])\n\n    dense_shape_spec = tensor_spec.TensorSpec(None, dtypes.int64)\n    _ = dynamic_shape_sparse.get_concrete_function(dense_shape_spec)\n\n\nclass SparseFillEmptyRowsTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensorValue_5x6(self, dtype=np.int32):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64), np.array(val, dtype), np.array(\n            shape, np.int64))\n\n  def _SparseTensor_5x6(self):\n    return sparse_tensor.SparseTensor.from_value(self._SparseTensorValue_5x6())\n\n  def _SparseTensor_String5x6(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtypes.string),\n        constant_op.constant(shape, dtypes.int64))\n\n  def _SparseTensor_2x6(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4]])\n    val = np.array([0, 10, 13, 14])\n    shape = np.array([2, 6])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtypes.int32),\n        constant_op.constant(shape, dtypes.int64))\n\n  def testFillNumber(self):\n    with test_util.use_gpu():\n      for sp_input in (self._SparseTensorValue_5x6(), self._SparseTensor_5x6()):\n        sp_output, empty_row_indicator = (\n            sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n        output, empty_row_indicator_out = self.evaluate(\n            [sp_output, empty_row_indicator])\n\n        self.assertAllEqual(\n            output.indices,\n            [[0, 0], [1, 0], [1, 3], [1, 4], [2, 0], [3, 2], [3, 3], [4, 0]])\n        self.assertAllEqual(output.values, [0, 10, 13, 14, -1, 32, 33, -1])\n        self.assertAllEqual(output.dense_shape, [5, 6])\n        self.assertAllEqual(empty_row_indicator_out,\n                            np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n\n  def testSparseFillEmptyRowsGradEmpty(self):\n    with test_util.use_gpu():\n      grad, _ = self.evaluate(\n          sparse_ops.sparse_fill_empty_rows_grad(\n              reverse_index_map=[], grad_values=[]))\n      self.assertAllEqual(grad, [])\n\n  @test_util.run_deprecated_v1\n  def testFillFloat(self):\n    with self.session():\n      values = constant_op.constant(\n          [0.0, 10.0, 13.0, 14.0, 32.0, 33.0], dtype=dtypes.float64)\n      default_value = constant_op.constant(-1.0, dtype=dtypes.float64)\n      sp_input = sparse_tensor.SparseTensorValue(\n          indices=np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]]),\n          values=values,\n          dense_shape=np.array([5, 6]))\n      sp_output, empty_row_indicator = (sparse_ops.sparse_fill_empty_rows(\n          sp_input, default_value))\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 0], [1, 0], [1, 3], [1, 4],\n                                           [2, 0], [3, 2], [3, 3], [4, 0]])\n      self.assertAllClose(output.values, [0, 10, 13, 14, -1, 32, 33, -1])\n      self.assertAllEqual(output.dense_shape, [5, 6])\n      self.assertAllEqual(empty_row_indicator_out,\n                          np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n\n      values_grad_err = gradient_checker.compute_gradient_error(\n          values, values.shape.as_list(), sp_output.values, [8], delta=1e-8)\n      self.assertGreater(values_grad_err, 0)\n      self.assertLess(values_grad_err, 1e-8)\n\n      default_value_grad_err = gradient_checker.compute_gradient_error(\n          default_value,\n          default_value.shape.as_list(),\n          sp_output.values, [8],\n          delta=1e-8)\n      self.assertGreater(default_value_grad_err, 0)\n      self.assertLess(default_value_grad_err, 1e-8)\n\n  def testFillString(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_String5x6()\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, \"\"))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(\n          output.indices,\n          [[0, 0], [1, 0], [1, 3], [1, 4], [2, 0], [3, 2], [3, 3], [4, 0]])\n      self.assertAllEqual(output.values,\n                          [b\"a\", b\"b\", b\"c\", b\"d\", b\"\", b\"e\", b\"f\", b\"\"])\n      self.assertAllEqual(output.dense_shape, [5, 6])\n      self.assertAllEqual(empty_row_indicator_out,\n                          np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n\n  def testNoEmptyRows(self):\n    with test_util.use_gpu():\n      sp_input = self._SparseTensor_2x6()\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 0], [1, 0], [1, 3], [1, 4]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14])\n      self.assertAllEqual(output.dense_shape, [2, 6])\n      self.assertAllEqual(empty_row_indicator_out, np.zeros(2).astype(np.bool_))\n\n  def testNoEmptyRowsAndUnordered(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.array([[1, 2], [1, 3], [0, 1], [0, 3]]),\n          values=np.array([1, 3, 2, 4]),\n          dense_shape=np.array([2, 5]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 1], [0, 3], [1, 2], [1, 3]])\n      self.assertAllEqual(output.values, [2, 4, 1, 3])\n      self.assertAllEqual(output.dense_shape, [2, 5])\n      self.assertAllEqual(empty_row_indicator_out, np.zeros(2).astype(np.bool_))\n\n  def testUnordered(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.array([[2, 3], [2, 2], [0, 1], [0, 3]]),\n          values=np.array([1, 3, 2, 4]),\n          dense_shape=np.array([3, 5]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices,\n                          [[0, 1], [0, 3], [1, 0], [2, 3], [2, 2]])\n      self.assertAllEqual(output.values, [2, 4, -1, 1, 3])\n      self.assertAllEqual(output.dense_shape, [3, 5])\n      self.assertAllEqual(empty_row_indicator_out, [False, True, False])\n\n  def testEmptyIndicesTensor(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.ones([0, 2]),\n          values=np.ones([0]),\n          dense_shape=np.array([2, 5]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 0], [1, 0]])\n      self.assertAllEqual(output.values, [-1, -1])\n      self.assertAllEqual(output.dense_shape, [2, 5])\n      self.assertAllEqual(empty_row_indicator_out, np.ones(2).astype(np.bool_))\n\n  def testEmptyOutput(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.ones([0, 2]),\n          values=np.ones([0]),\n          dense_shape=np.array([0, 3]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, np.ones([0, 2]))\n      self.assertAllEqual(output.values, np.ones([0]))\n      self.assertAllEqual(output.dense_shape, [0, 3])\n      self.assertAllEqual(empty_row_indicator_out, [])\n\n  def testInvalidIndices(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.array([[1, 2], [1, 3], [99, 1], [99, 3]]),\n          values=np.array([1, 3, 2, 4]),\n          dense_shape=np.array([2, 5]))\n\n      with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                  r\"indices\\(2, 0\\) is invalid\"):\n        self.evaluate(sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n\nclass SparseAddTest(test_util.TensorFlowTestCase):\n\n  def testValuesInVariable(self):\n    indices = constant_op.constant([[0]], dtype=dtypes.int64)\n    values = variables.Variable([1], trainable=False, dtype=dtypes.float32)\n    shape = constant_op.constant([1], dtype=dtypes.int64)\n\n    sp_input = sparse_tensor.SparseTensor(indices, values, shape)\n    sp_output = sparse_ops.sparse_add(sp_input, sp_input)\n\n    with test_util.force_cpu():\n      self.evaluate(variables.global_variables_initializer())\n      output = self.evaluate(sp_output)\n      self.assertAllEqual(output.values, [2])\n\n\nclass SparseReduceTest(test_util.TensorFlowTestCase):\n\n  # [[1, ?, 2]\n  #  [?, 3, ?]]\n  # where ? is implicitly-zero.\n  ind = np.array([[0, 0], [0, 2], [1, 1]]).astype(np.int64)\n  vals = np.array([1, 1, 1]).astype(np.int32)\n  dense_shape = np.array([2, 3]).astype(np.int64)\n\n  def _compare(self, sp_t, reduction_axes, ndims, keep_dims, do_sum):\n    densified = self.evaluate(sparse_ops.sparse_tensor_to_dense(sp_t))\n\n    np_ans = densified\n    if reduction_axes is None:\n      if do_sum:\n        np_ans = np.sum(np_ans, keepdims=keep_dims)\n      else:\n        np_ans = np.max(np_ans, keepdims=keep_dims)\n    else:\n      if not isinstance(reduction_axes, list):  # Single scalar.\n        reduction_axes = [reduction_axes]\n      reduction_axes = np.array(reduction_axes).astype(np.int32)\n      # Handles negative axes.\n      reduction_axes = (reduction_axes + ndims) % ndims\n      # Loop below depends on sorted.\n      reduction_axes.sort()\n      for ra in reduction_axes.ravel()[::-1]:\n        if do_sum:\n          np_ans = np.sum(np_ans, axis=ra, keepdims=keep_dims)\n        else:\n          np_ans = np.max(np_ans, axis=ra, keepdims=keep_dims)\n\n    with self.cached_session():\n      if do_sum:\n        tf_dense_ans = sparse_ops.sparse_reduce_sum(sp_t, reduction_axes,\n                                                    keep_dims)\n      else:\n        tf_dense_ans = sparse_ops.sparse_reduce_max(sp_t, reduction_axes,\n                                                    keep_dims)\n      out_dense = self.evaluate(tf_dense_ans)\n\n      if do_sum:\n        tf_sparse_ans = sparse_ops.sparse_reduce_sum_sparse(sp_t,\n                                                            reduction_axes,\n                                                            keep_dims)\n      else:\n        tf_sparse_ans = sparse_ops.sparse_reduce_max_sparse(sp_t,\n                                                            reduction_axes,\n                                                            keep_dims)\n      # Convert to dense for comparison purposes.\n      out_sparse = sparse_ops.sparse_tensor_to_dense(tf_sparse_ans)\n\n    self.assertAllClose(np_ans, out_dense)\n    self.assertAllClose(np_ans, out_sparse)\n\n  def _compare_all(self, sp_t, reduction_axes, ndims):\n    self._compare(sp_t, reduction_axes, ndims, False, False)\n    self._compare(sp_t, reduction_axes, ndims, False, True)\n    self._compare(sp_t, reduction_axes, ndims, True, False)\n    self._compare(sp_t, reduction_axes, ndims, True, True)\n\n  # (TODO:b/133851381): Re-enable this test.\n  def disabledtestSimpleAndRandomInputs(self):\n    if np.__version__ == \"1.13.0\":\n      self.skipTest(\"numpy 1.13.0 bug\")\n\n    sp_t = sparse_tensor.SparseTensor(self.ind, self.vals, self.dense_shape)\n\n    with test_util.force_cpu():\n      self._compare_all(sp_t, None, ndims=2)\n      self._compare_all(sp_t, 0, ndims=2)\n      self._compare_all(sp_t, [1], ndims=2)\n      self._compare_all(sp_t, [0, 1], ndims=2)\n      self._compare_all(sp_t, [1, 0], ndims=2)\n      self._compare_all(sp_t, [-1], ndims=2)\n      self._compare_all(sp_t, [1, -2], ndims=2)\n\n    np.random.seed(1618)\n    test_dims = [(1618, 1, 11, 7, 1), (1,), (1, 1, 1)]\n    with test_util.force_cpu():\n      for dims in test_dims:\n        sp_t, unused_nnz = _sparsify(np.random.randn(*dims))\n        # reduce all using None\n        self._compare_all(sp_t, None, ndims=len(dims))\n        # reduce random axes from 1D to N-D\n        for d in range(1, len(dims) + 1):\n          axes = np.random.choice(len(dims), size=d, replace=False).tolist()\n          self._compare_all(sp_t, axes, ndims=len(dims))\n\n  def testInvalidAxes(self):\n    sp_t = sparse_tensor.SparseTensor(self.ind, self.vals, self.dense_shape)\n    with test_util.force_cpu():\n      with self.assertRaisesOpError(\"Invalid reduction dimension -3\"):\n        self.evaluate(sparse_ops.sparse_reduce_sum(sp_t, -3))\n      with self.assertRaisesOpError(\"Invalid reduction dimension 2\"):\n        self.evaluate(sparse_ops.sparse_reduce_sum(sp_t, 2))\n      with self.assertRaisesOpError(\"Invalid reduction dimension -3\"):\n        self.evaluate(sparse_ops.sparse_reduce_max(sp_t, -3))\n      with self.assertRaisesOpError(\"Invalid reduction dimension 2\"):\n        self.evaluate(sparse_ops.sparse_reduce_max(sp_t, 2))\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    np.random.seed(8161)\n    test_dims = [(11, 1, 5, 7, 1), (2, 2)]\n    with self.session(use_gpu=False):\n      for dims in test_dims:\n        sp_t, nnz = _sparsify(np.random.randn(*dims))\n        # reduce random axes from 1D to N-D\n        for d in range(1, len(dims) + 1):\n          axes = np.random.choice(len(dims), size=d, replace=False).tolist()\n          reduced = sparse_ops.sparse_reduce_sum(sp_t, axes)\n\n          err = gradient_checker.compute_gradient_error(\n              sp_t.values, (nnz,), reduced,\n              self.evaluate(reduced).shape)\n          self.assertLess(err, 1e-3)\n\n        # Tests for negative axes.\n        reduced = sparse_ops.sparse_reduce_sum(sp_t, -1)\n        err = gradient_checker.compute_gradient_error(\n            sp_t.values, (nnz,), reduced,\n            self.evaluate(reduced).shape)\n        self.assertLess(err, 1e-3)\n\n  def _testSparseReduceShape(self, sp_t, reduction_axes, ndims, keep_dims,\n                             do_sum):\n    densified = self.evaluate(sparse_ops.sparse_tensor_to_dense(sp_t))\n\n    np_op = np.sum\n    tf_op = sparse_ops.sparse_reduce_sum\n    if not do_sum:\n      np_op = np.max\n      tf_op = sparse_ops.sparse_reduce_max\n\n    np_ans = densified\n    if reduction_axes is None:\n      np_ans = np_op(np_ans, keepdims=keep_dims)\n    else:\n      if not isinstance(reduction_axes, list):  # Single scalar.\n        reduction_axes = [reduction_axes]\n      reduction_axes = np.array(reduction_axes).astype(np.int32)\n      # Handles negative axes.\n      reduction_axes = (reduction_axes + ndims) % ndims\n      # Loop below depends on sorted.\n      reduction_axes.sort()\n      for ra in reduction_axes.ravel()[::-1]:\n        np_ans = np_op(np_ans, axis=ra, keepdims=keep_dims)\n\n    tf_ans = tf_op(sp_t, reduction_axes, keep_dims)\n    self.assertAllEqual(np_ans.shape, tf_ans.get_shape().as_list())\n\n  # (TODO:b/133851381): Re-enable this test\n  def disabledtestSparseReduceSumOrMaxShape(self):\n    sp_t = sparse_tensor.SparseTensor(self.ind, self.vals, self.dense_shape)\n\n    with test_util.force_cpu():\n      for do_sum in [True, False]:\n        for keep_dims in [True, False]:\n          self._testSparseReduceShape(sp_t, None, 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, 0, 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [1], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [0, 1], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [1, 0], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [-1], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [1, -2], 2, keep_dims, do_sum)\n\n  def testIntegerOverflow(self):\n    with self.cached_session(use_gpu=False):\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_reduce_max(\n            input_indices=[[1, 2], [3, 4]],\n            input_shape=[2**32, 2**31],\n            input_values=[1, 3],\n            reduction_axes=[0],\n            keep_dims=False,\n            name=None)\n\n        self.evaluate(res)\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_reduce_max_sparse(\n            input_indices=[[1, 2], [3, 4]],\n            input_shape=[2**32, 2**31],\n            input_values=[1, 3],\n            reduction_axes=[0],\n            keep_dims=False,\n            name=None)\n\n        self.evaluate(res)\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_reduce_sum(\n            input_indices=[[1, 2], [3, 4]],\n            input_shape=[2**32, 2**31],\n            input_values=[1, 3],\n            reduction_axes=[0],\n            keep_dims=False,\n            name=None)\n\n        self.evaluate(res)\n\n\nclass SparseMathOpsTest(test_util.TensorFlowTestCase):\n\n  def _check(self, result_tensor, result_np, input_sp_t):\n    self.assertTrue(isinstance(result_tensor, sparse_tensor.SparseTensor))\n    self.assertTrue(isinstance(input_sp_t, sparse_tensor.SparseTensor))\n    self.assertAllCloseAccordingToType(input_sp_t.indices,\n                                       result_tensor.indices)\n    self.assertAllCloseAccordingToType(input_sp_t.dense_shape,\n                                       result_tensor.dense_shape)\n\n    res_densified = sparse_ops.sparse_to_dense(\n        result_tensor.indices, result_tensor.dense_shape, result_tensor.values)\n    self.assertAllCloseAccordingToType(result_np, res_densified)\n\n  @test_util.run_deprecated_v1\n  def testCwiseShapeValidation(self):\n    # Test case for GitHub 24072.\n    with test_util.force_cpu():\n      a = array_ops.ones([3, 4, 1], dtype=dtypes.int32)\n      b = sparse_tensor.SparseTensor([[0, 0, 1, 0], [0, 0, 3, 0]], [10, 20],\n                                     [1, 1, 4, 2])\n      c = a * b\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          \"broadcasts dense to sparse only; got incompatible shapes\"):\n        self.evaluate(c)\n\n  def testCwiseDivAndMul(self):\n    np.random.seed(1618)\n    sp_shapes = [(10, 10, 10), (5, 5), (1618,), (3, 3, 7)]\n    dense_shapes = [(10, 10, 1), (5, 5), (1,), (1, 7)]\n\n    with test_util.force_cpu():\n      for dtype in [np.float32, np.float64, np.int32, np.int64]:\n        for sp_shape, dense_shape in zip(sp_shapes, dense_shapes):\n          sp_vals_np = np.random.rand(*sp_shape).astype(dtype) + 1\n          dense_vals_np = np.random.rand(*dense_shape).astype(dtype) + 1\n          sp_t, unused_nnz = _sparsify(sp_vals_np, thresh=1.5)\n          sp_t_densified = sparse_ops.sparse_tensor_to_dense(sp_t)\n          dense_t = constant_op.constant(dense_vals_np)\n\n          self._check(sp_t / dense_t, sp_t_densified / dense_vals_np, sp_t)\n          # Check commutative.\n          self._check(sp_t * dense_t, sp_t_densified * dense_vals_np, sp_t)\n          self._check(dense_t * sp_t, sp_t_densified * dense_vals_np, sp_t)\n\n          if dtype in [np.int32, np.int64]:\n            res = sp_t / dense_t  # should invoke \"__truediv__\"\n            self.assertEqual(res.values.dtype, np.float64)\n\n  def testCwiseAdd(self):\n    with test_util.force_cpu():\n      # Identity(2) + AllOnes(2,2).  Should be equal to 2 * Identity(2).\n      indices = [[0, 0], [1, 1]]\n      vals = [1, 1]\n      shape = (2, 2)\n\n      sp_t = sparse_tensor.SparseTensor(indices, vals, shape)\n      dense_t = array_ops.ones(shape, dtype=dtypes.int32)\n      self._check(\n          sparse_ops.sparse_dense_cwise_add(sp_t, dense_t),\n          np.identity(2) * 2, sp_t)\n\n      # Variant of above, but broadcasts the dense side.\n      dense_t = array_ops.ones([1], dtype=dtypes.int32)\n      self._check(\n          sparse_ops.sparse_dense_cwise_add(sp_t, dense_t),\n          np.identity(2) * 2, sp_t)\n\n  @test_util.run_deprecated_v1\n  def testGradients(self):\n    np.random.seed(1618)\n    sp_shapes = [(10, 10, 10), (5, 5), (1618,), (3, 3, 7)]\n    dense_shapes = [(10, 10, 1), (5, 5), (1,), (1, 7)]\n\n    with self.session(use_gpu=False):\n      for dtype in [np.float32, np.float64]:\n        for sp_shape, dense_shape in zip(sp_shapes, dense_shapes):\n          sp_vals_np = np.random.rand(*sp_shape).astype(dtype) + 1\n          dense_vals_np = np.random.rand(*dense_shape).astype(dtype) + 1\n          sp_t, nnz = _sparsify(sp_vals_np, thresh=1.5)\n          dense_t = constant_op.constant(dense_vals_np)\n\n          cmul = sp_t * dense_t\n          err = gradient_checker.compute_gradient_error([sp_t.values, dense_t],\n                                                        [(nnz,), dense_shape],\n                                                        cmul.values, (nnz,))\n          self.assertLess(err, 1e-4)\n\n          cdiv = sp_t / dense_t\n          err = gradient_checker.compute_gradient_error(sp_t.values, (nnz,),\n                                                        cdiv.values, (nnz,))\n          self.assertLess(err, 1e-4)\n          err = gradient_checker.compute_gradient_error(\n              dense_t,\n              dense_shape,\n              cdiv.values, (nnz,),\n              x_init_value=dense_vals_np)\n          self.assertLess(err, 2e-4)\n\n\nclass SparseSoftmaxTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testEquivalentToDensified(self):\n    np.random.seed(1618)\n    n, m = np.random.choice(20, size=2)\n\n    for dtype in [np.float16, np.float32, np.float64]:\n      sp_vals_np = np.random.rand(n, m).astype(dtype)\n\n      batched_sp_t, unused_nnz1 = _sparsify(\n          sp_vals_np.reshape((1, n, m)), thresh=0.)  # No masking.\n\n      with test_util.force_cpu():\n        densified = constant_op.constant(sp_vals_np)\n\n        sp_result = self.evaluate(\n            sparse_ops.sparse_softmax(batched_sp_t)).values.reshape((n, m))\n        dense_result = nn_ops.softmax(densified)\n\n        self.assertAllCloseAccordingToType(dense_result, sp_result)\n\n  def testHigherRanks(self):\n    # For the first shape:\n    # First batch:\n    # [?   e.]\n    # [1.  ? ]\n    # Second batch:\n    # [e   ? ]\n    # [e   e ]\n    #\n    # The softmax results should be:\n    # [?   1.]     [1    ?]\n    # [1.  ? ] and [.5  .5]\n    # where ? means implicitly zero.\n    #\n    # The second shape: same input data, but with a higher-rank shape.\n    shapes = [[2, 2, 2], [2, 1, 2, 2]]\n    for shape in shapes:\n      values = np.asarray(\n          [0., np.e, 1., 0., np.e, 0., np.e, np.e]).reshape(shape)\n      sp_t, unused_nnz = _sparsify(values, thresh=1e-2)\n      expected_values = [1., 1., 1., .5, .5]\n\n      with test_util.force_cpu():\n        result = sparse_ops.sparse_softmax(sp_t)\n\n        self.assertAllEqual(expected_values, result.values)\n        self.assertAllEqual(sp_t.indices, result.indices)\n        self.assertAllEqual(shape, result.dense_shape)\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    x_shape = [2, 5, 10]\n    with self.cached_session(use_gpu=False):\n      for dtype in [np.float32, np.float64]:\n        x_np = np.random.randn(*x_shape).astype(dtype)\n        x_tf, nnz = _sparsify(x_np)\n        y_tf = sparse_ops.sparse_softmax(x_tf)\n        err = gradient_checker.compute_gradient_error(x_tf.values, (nnz,),\n                                                      y_tf.values, (nnz,))\n        self.assertLess(err, 1e-4)\n\n  def testIntegerOverflow(self):\n    with self.cached_session(use_gpu=False):\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_softmax(\n            sp_indices=[[1, 1]],\n            sp_values=[2.0],\n            sp_shape=[2**32, 2**31],\n            name=None)\n\n        self.evaluate(res)\n\n  def testReshapeNegativeShape(self):\n    with self.cached_session(use_gpu=False):\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_softmax(\n            sp_indices=[[1, 1]], sp_values=[2.0], sp_shape=[-1, 1], name=None)\n\n        self.evaluate(res)\n\n\nclass SparseMinimumMaximumTest(test_util.TensorFlowTestCase):\n\n  def _assertSparseTensorValueEqual(self, a, b):\n    self.assertAllEqual(a.indices, b.indices)\n    self.assertAllEqual(a.values, b.values)\n    self.assertAllEqual(a.dense_shape, b.dense_shape)\n\n  def testBasic(self):\n    with test_util.force_cpu():\n      # 1-D, values at index 0.\n      sp_zero = sparse_tensor.SparseTensor([[0]], [0], [7])\n      sp_one = sparse_tensor.SparseTensor([[0]], [1], [7])\n      max_tf = sparse_ops.sparse_maximum(sp_zero, sp_one)\n      min_tf = sparse_ops.sparse_minimum(sp_zero, sp_one)\n      self._assertSparseTensorValueEqual(sp_one, max_tf)\n      self._assertSparseTensorValueEqual(sp_zero, min_tf)\n\n      # Values at different indices.\n      sp_zero = sparse_tensor.SparseTensor([[0]], [0], [7])\n      sp_zero_2 = sparse_tensor.SparseTensor([[1]], [0], [7])\n      expected = sparse_tensor.SparseTensor([[0], [1]], [0, 0], [7])\n      max_tf = sparse_ops.sparse_maximum(sp_zero, sp_zero_2)\n      min_tf = sparse_ops.sparse_minimum(sp_zero, sp_zero_2)\n      self._assertSparseTensorValueEqual(expected, max_tf)\n      self._assertSparseTensorValueEqual(expected, min_tf)\n\n  def testInvalidSparseInputs(self):\n    with test_util.force_cpu():\n      with self.assertRaisesRegex(\n          (ValueError, errors.InvalidArgumentError),\n          \".*Index rank .* and shape rank .* do not match.*\",\n      ):\n        self.evaluate(\n            gen_sparse_ops.sparse_sparse_maximum(\n                [[1]], [0], [2], [[]], [1], [2]\n            )\n        )\n\n  @test_util.run_deprecated_v1\n  def testRandom(self):\n    np.random.seed(1618)\n    shapes = [(13,), (6, 8), (1, 7, 1)]\n    for shape in shapes:\n      for dtype in [np.int32, np.int64, np.float16, np.float32, np.float64]:\n        a_np = np.random.randn(*shape).astype(dtype)\n        b_np = np.random.randn(*shape).astype(dtype)\n        sp_a, unused_a_nnz = _sparsify(a_np, thresh=-.5)\n        sp_b, unused_b_nnz = _sparsify(b_np, thresh=-.5)\n\n        with self.cached_session(use_gpu=False):\n          maximum_tf = sparse_ops.sparse_maximum(sp_a, sp_b)\n          maximum_tf_densified = sparse_ops.sparse_tensor_to_dense(\n              maximum_tf).eval()\n          minimum_tf = sparse_ops.sparse_minimum(sp_a, sp_b)\n          minimum_tf_densified = sparse_ops.sparse_tensor_to_dense(\n              minimum_tf).eval()\n\n          a_densified = sparse_ops.sparse_tensor_to_dense(sp_a).eval()\n          b_densified = sparse_ops.sparse_tensor_to_dense(sp_b).eval()\n\n        self.assertAllEqual(\n            np.maximum(a_densified, b_densified), maximum_tf_densified)\n        self.assertAllEqual(\n            np.minimum(a_densified, b_densified), minimum_tf_densified)\n\n  def testMismatchedShapes(self):\n    with test_util.force_cpu():\n      sp_zero = sparse_tensor.SparseTensor([[0, 0]], [0], [1, 1])\n      sp_one = sparse_tensor.SparseTensor([[0]], [1], [2])\n      with self.assertRaisesOpError(\"Operands do not have the same ranks\"):\n        self.evaluate(sparse_ops.sparse_maximum(sp_zero, sp_one))\n\n      sp_zero = sparse_tensor.SparseTensor([[0]], [0], [1])\n      sp_one = sparse_tensor.SparseTensor([[0]], [1], [2])\n      with self.assertRaisesOpError(\"Operands' shapes do not match\"):\n        self.evaluate(sparse_ops.sparse_maximum(sp_zero, sp_one))\n\n\nclass SparseTransposeTest(test.TestCase):\n\n  def testTranspose(self):\n    if np.__version__ == \"1.13.0\":\n      self.skipTest(\"numpy 1.13.0 bug\")\n\n    with test_util.force_cpu():\n      np.random.seed(1618)\n      shapes = [np.random.randint(1, 10, size=rank) for rank in range(1, 6)]\n      for shape in shapes:\n        for dtype in [np.int32, np.int64, np.float32, np.float64]:\n          dn_input = np.random.randn(*shape).astype(dtype)\n          rank = self.evaluate(array_ops.rank(dn_input))\n          perm = np.random.choice(rank, rank, False)\n          sp_input, unused_a_nnz = _sparsify(dn_input)\n          sp_trans = sparse_ops.sparse_transpose(sp_input, perm=perm)\n          dn_trans = sparse_ops.sparse_tensor_to_dense(sp_trans)\n          expected_trans = array_ops.transpose(dn_input, perm=perm)\n          self.assertAllEqual(expected_trans.shape, sp_trans.get_shape())\n          self.assertAllEqual(dn_trans, expected_trans)\n\n\nclass SparsePlaceholderTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testPlaceholder(self):\n    foo = array_ops.sparse_placeholder(dtypes.float32, shape=(10, 47))\n    self.assertAllEqual([10, 47], foo.get_shape())\n    self.assertAllEqual([None, 2], foo.indices.get_shape().as_list())\n\n  @test_util.run_deprecated_v1\n  def testPartialShapePlaceholder(self):\n    foo = array_ops.sparse_placeholder(dtypes.float32, shape=(None, 47))\n    self.assertAllEqual([None, 47], foo.get_shape().as_list())\n    self.assertAllEqual([None, 2], foo.indices.get_shape().as_list())\n\n  @test_util.run_deprecated_v1\n  def testNoShapePlaceholder(self):\n    foo = array_ops.sparse_placeholder(dtypes.float32, shape=None)\n    self.assertAllEqual(None, foo.get_shape())\n    self.assertAllEqual([None, None], foo.indices.get_shape().as_list())\n\n\nif __name__ == \"__main__\":\n  googletest.main()\n"], "filenames": ["tensorflow/core/kernels/sparse_sparse_binary_op_shared.cc", "tensorflow/python/kernel_tests/sparse_ops/sparse_ops_test.py"], "buggy_code_start_loc": [43, 28], "buggy_code_end_loc": [196, 1177], "fixing_code_start_loc": [44, 29], "fixing_code_end_loc": [169, 1191], "type": "CWE-476", "message": "TensorFlow is an open source platform for machine learning. Prior to versions 2.12.0 and 2.11.1, when `SparseSparseMaximum` is given invalid sparse tensors as inputs, it can give a null pointer error. A fix is included in TensorFlow version 2.12 and version 2.11.1.", "other": {"cve": {"id": "CVE-2023-25665", "sourceIdentifier": "security-advisories@github.com", "published": "2023-03-25T00:15:07.427", "lastModified": "2023-03-31T14:20:43.060", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. Prior to versions 2.12.0 and 2.11.1, when `SparseSparseMaximum` is given invalid sparse tensors as inputs, it can give a null pointer error. A fix is included in TensorFlow version 2.12 and version 2.11.1."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.12.0", "matchCriteriaId": "FAC3DE54-93B4-4D6C-9648-B9D416B9770F"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/5e0ecfb42f5f65629fd7a4edd6c4afe7ff0feb04", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-558h-mq8x-7q9g", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/5e0ecfb42f5f65629fd7a4edd6c4afe7ff0feb04"}}