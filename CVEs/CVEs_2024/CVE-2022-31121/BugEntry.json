{"buggy_code": ["/*\nCopyright IBM Corp. 2017 All Rights Reserved.\n\nSPDX-License-Identifier: Apache-2.0\n*/\n\npackage cluster_test\n\nimport (\n\t\"context\"\n\t\"crypto/rand\"\n\t\"crypto/x509\"\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/golang/protobuf/proto\"\n\t\"github.com/hyperledger/fabric-protos-go/common\"\n\t\"github.com/hyperledger/fabric-protos-go/orderer\"\n\t\"github.com/hyperledger/fabric/common/crypto\"\n\t\"github.com/hyperledger/fabric/common/crypto/tlsgen\"\n\t\"github.com/hyperledger/fabric/common/flogging\"\n\t\"github.com/hyperledger/fabric/common/metrics\"\n\t\"github.com/hyperledger/fabric/common/metrics/disabled\"\n\t\"github.com/hyperledger/fabric/common/metrics/metricsfakes\"\n\tcomm_utils \"github.com/hyperledger/fabric/internal/pkg/comm\"\n\t\"github.com/hyperledger/fabric/orderer/common/cluster\"\n\t\"github.com/hyperledger/fabric/orderer/common/cluster/mocks\"\n\t\"github.com/onsi/gomega\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/stretchr/testify/mock\"\n\t\"github.com/stretchr/testify/require\"\n\t\"go.uber.org/zap\"\n\t\"go.uber.org/zap/zapcore\"\n\t\"google.golang.org/grpc\"\n)\n\nconst (\n\ttestChannel  = \"test\"\n\ttestChannel2 = \"test2\"\n\ttimeout      = time.Second * 10\n)\n\nvar (\n\t// CA that generates TLS key-pairs.\n\t// We use only one CA because the authentication\n\t// is based on TLS pinning\n\tca = createCAOrPanic()\n\n\tlastNodeID uint64\n\n\ttestSubReq = &orderer.SubmitRequest{\n\t\tChannel: \"test\",\n\t}\n\n\ttestReq = &orderer.SubmitRequest{\n\t\tChannel: \"test\",\n\t\tPayload: &common.Envelope{\n\t\t\tPayload: []byte(\"test\"),\n\t\t},\n\t}\n\n\ttestReq2 = &orderer.SubmitRequest{\n\t\tChannel: testChannel2,\n\t\tPayload: &common.Envelope{\n\t\t\tPayload: []byte(testChannel2),\n\t\t},\n\t}\n\n\ttestRes = &orderer.SubmitResponse{\n\t\tInfo: \"test\",\n\t}\n\n\tfooReq = wrapSubmitReq(&orderer.SubmitRequest{\n\t\tChannel: \"foo\",\n\t})\n\n\tbarReq = wrapSubmitReq(&orderer.SubmitRequest{\n\t\tChannel: \"bar\",\n\t})\n\n\ttestConsensusReq = &orderer.StepRequest{\n\t\tPayload: &orderer.StepRequest_ConsensusRequest{\n\t\t\tConsensusRequest: &orderer.ConsensusRequest{\n\t\t\t\tPayload: []byte{1, 2, 3},\n\t\t\t\tChannel: testChannel,\n\t\t\t},\n\t\t},\n\t}\n\n\tchannelExtractor = &mockChannelExtractor{}\n)\n\nfunc nextUnusedID() uint64 {\n\treturn atomic.AddUint64(&lastNodeID, 1)\n}\n\nfunc createCAOrPanic() tlsgen.CA {\n\tca, err := tlsgen.NewCA()\n\tif err != nil {\n\t\tpanic(fmt.Sprintf(\"failed creating CA: %+v\", err))\n\t}\n\treturn ca\n}\n\ntype mockChannelExtractor struct{}\n\nfunc (*mockChannelExtractor) TargetChannel(msg proto.Message) string {\n\tswitch req := msg.(type) {\n\tcase *orderer.ConsensusRequest:\n\t\treturn req.Channel\n\tcase *orderer.SubmitRequest:\n\t\treturn req.Channel\n\tdefault:\n\t\treturn \"\"\n\t}\n}\n\ntype clusterNode struct {\n\tlock         sync.Mutex\n\tfrozen       bool\n\tfreezeCond   sync.Cond\n\tdialer       *cluster.PredicateDialer\n\thandler      *mocks.Handler\n\tnodeInfo     cluster.RemoteNode\n\tsrv          *comm_utils.GRPCServer\n\tbindAddress  string\n\tclientConfig comm_utils.ClientConfig\n\tserverConfig comm_utils.ServerConfig\n\tc            *cluster.Comm\n}\n\nfunc (cn *clusterNode) Step(stream orderer.Cluster_StepServer) error {\n\tcn.waitIfFrozen()\n\treq, err := stream.Recv()\n\tif err != nil {\n\t\treturn err\n\t}\n\tif submitReq := req.GetSubmitRequest(); submitReq != nil {\n\t\treturn cn.c.DispatchSubmit(stream.Context(), submitReq)\n\t}\n\tif err := cn.c.DispatchConsensus(stream.Context(), req.GetConsensusRequest()); err != nil {\n\t\treturn err\n\t}\n\treturn stream.Send(&orderer.StepResponse{})\n}\n\nfunc (cn *clusterNode) waitIfFrozen() {\n\tcn.lock.Lock()\n\t// There is no freeze after an unfreeze so no need\n\t// for a for loop.\n\tif cn.frozen {\n\t\tcn.freezeCond.Wait()\n\t\treturn\n\t}\n\tcn.lock.Unlock()\n}\n\nfunc (cn *clusterNode) freeze() {\n\tcn.lock.Lock()\n\tdefer cn.lock.Unlock()\n\tcn.frozen = true\n}\n\nfunc (cn *clusterNode) unfreeze() {\n\tcn.lock.Lock()\n\tcn.frozen = false\n\tcn.lock.Unlock()\n\tcn.freezeCond.Broadcast()\n}\n\nfunc (cn *clusterNode) resurrect() {\n\tgRPCServer, err := comm_utils.NewGRPCServer(cn.bindAddress, cn.serverConfig)\n\tif err != nil {\n\t\tpanic(fmt.Errorf(\"failed starting gRPC server: %v\", err))\n\t}\n\tcn.srv = gRPCServer\n\torderer.RegisterClusterServer(gRPCServer.Server(), cn)\n\tgo cn.srv.Start()\n}\n\nfunc (cn *clusterNode) stop() {\n\tcn.srv.Stop()\n\tcn.c.Shutdown()\n}\n\nfunc (cn *clusterNode) renewCertificates() {\n\tclientKeyPair, err := ca.NewClientCertKeyPair()\n\tif err != nil {\n\t\tpanic(fmt.Errorf(\"failed creating client certificate %v\", err))\n\t}\n\tserverKeyPair, err := ca.NewServerCertKeyPair(\"127.0.0.1\")\n\tif err != nil {\n\t\tpanic(fmt.Errorf(\"failed creating server certificate %v\", err))\n\t}\n\n\tcn.nodeInfo.ClientTLSCert = clientKeyPair.TLSCert.Raw\n\tcn.nodeInfo.ServerTLSCert = serverKeyPair.TLSCert.Raw\n\n\tcn.serverConfig.SecOpts.Certificate = serverKeyPair.Cert\n\tcn.serverConfig.SecOpts.Key = serverKeyPair.Key\n\n\tcn.dialer.Config.SecOpts.Key = clientKeyPair.Key\n\tcn.dialer.Config.SecOpts.Certificate = clientKeyPair.Cert\n}\n\nfunc newTestNodeWithMetrics(t *testing.T, metrics cluster.MetricsProvider, tlsConnGauge metrics.Gauge) *clusterNode {\n\tserverKeyPair, err := ca.NewServerCertKeyPair(\"127.0.0.1\")\n\trequire.NoError(t, err)\n\n\tclientKeyPair, _ := ca.NewClientCertKeyPair()\n\n\thandler := &mocks.Handler{}\n\tclientConfig := comm_utils.ClientConfig{\n\t\tAsyncConnect: true,\n\t\tDialTimeout:  time.Hour,\n\t\tSecOpts: comm_utils.SecureOptions{\n\t\t\tRequireClientCert: true,\n\t\t\tKey:               clientKeyPair.Key,\n\t\t\tCertificate:       clientKeyPair.Cert,\n\t\t\tServerRootCAs:     [][]byte{ca.CertBytes()},\n\t\t\tUseTLS:            true,\n\t\t\tClientRootCAs:     [][]byte{ca.CertBytes()},\n\t\t},\n\t}\n\n\tdialer := &cluster.PredicateDialer{\n\t\tConfig: clientConfig,\n\t}\n\n\tsrvConfig := comm_utils.ServerConfig{\n\t\tSecOpts: comm_utils.SecureOptions{\n\t\t\tKey:         serverKeyPair.Key,\n\t\t\tCertificate: serverKeyPair.Cert,\n\t\t\tUseTLS:      true,\n\t\t},\n\t}\n\tgRPCServer, err := comm_utils.NewGRPCServer(\"127.0.0.1:\", srvConfig)\n\trequire.NoError(t, err)\n\n\ttstSrv := &clusterNode{\n\t\tdialer:       dialer,\n\t\tclientConfig: clientConfig,\n\t\tserverConfig: srvConfig,\n\t\tbindAddress:  gRPCServer.Address(),\n\t\thandler:      handler,\n\t\tnodeInfo: cluster.RemoteNode{\n\t\t\tEndpoint:      gRPCServer.Address(),\n\t\t\tID:            nextUnusedID(),\n\t\t\tServerTLSCert: serverKeyPair.TLSCert.Raw,\n\t\t\tClientTLSCert: clientKeyPair.TLSCert.Raw,\n\t\t},\n\t\tsrv: gRPCServer,\n\t}\n\n\ttstSrv.freezeCond.L = &tstSrv.lock\n\n\tcompareCert := cluster.CachePublicKeyComparisons(func(a, b []byte) bool {\n\t\treturn crypto.CertificatesWithSamePublicKey(a, b) == nil\n\t})\n\n\ttstSrv.c = &cluster.Comm{\n\t\tCertExpWarningThreshold: time.Hour,\n\t\tSendBufferSize:          1,\n\t\tLogger:                  flogging.MustGetLogger(\"test\"),\n\t\tChan2Members:            make(cluster.MembersByChannel),\n\t\tH:                       handler,\n\t\tChanExt:                 channelExtractor,\n\t\tConnections:             cluster.NewConnectionStore(dialer, tlsConnGauge),\n\t\tMetrics:                 cluster.NewMetrics(metrics),\n\t\tCompareCertificate:      compareCert,\n\t}\n\n\torderer.RegisterClusterServer(gRPCServer.Server(), tstSrv)\n\tgo gRPCServer.Start()\n\treturn tstSrv\n}\n\nfunc newTestNode(t *testing.T) *clusterNode {\n\treturn newTestNodeWithMetrics(t, &disabled.Provider{}, &disabled.Gauge{})\n}\n\nfunc TestSendBigMessage(t *testing.T) {\n\t// Scenario: Basic test that spawns 5 nodes and sends a big message\n\t// from one of the nodes to the others.\n\t// A receiver node's Step() server side method (which calls Recv)\n\t// is frozen until the sender's node Send method returns,\n\t// Hence - the sender node finishes calling Send\n\t// before a receiver node starts calling Recv.\n\t// This ensures that Send is non blocking even with big messages.\n\t// In the test, we send a total of 8MB of random data (2MB to each node).\n\t// The randomness is used so gRPC compression won't compress it to a lower size.\n\n\tnode1 := newTestNode(t)\n\tnode2 := newTestNode(t)\n\tnode3 := newTestNode(t)\n\tnode4 := newTestNode(t)\n\tnode5 := newTestNode(t)\n\n\tfor _, node := range []*clusterNode{node2, node3, node4, node5} {\n\t\tnode.c.SendBufferSize = 1\n\t}\n\n\tdefer node1.stop()\n\tdefer node2.stop()\n\tdefer node3.stop()\n\tdefer node4.stop()\n\tdefer node5.stop()\n\n\tconfig := []cluster.RemoteNode{node1.nodeInfo, node2.nodeInfo, node3.nodeInfo, node4.nodeInfo, node5.nodeInfo}\n\tnode1.c.Configure(testChannel, config)\n\tnode2.c.Configure(testChannel, config)\n\tnode3.c.Configure(testChannel, config)\n\tnode4.c.Configure(testChannel, config)\n\tnode5.c.Configure(testChannel, config)\n\n\tvar messageReceived sync.WaitGroup\n\tmessageReceived.Add(4)\n\n\tmsgSize := 1024 * 1024 * 2\n\tbigMsg := &orderer.ConsensusRequest{\n\t\tChannel: testChannel,\n\t\tPayload: make([]byte, msgSize),\n\t}\n\n\t_, err := rand.Read(bigMsg.Payload)\n\trequire.NoError(t, err)\n\n\twrappedMsg := &orderer.StepRequest{\n\t\tPayload: &orderer.StepRequest_ConsensusRequest{\n\t\t\tConsensusRequest: bigMsg,\n\t\t},\n\t}\n\n\tfor _, node := range []*clusterNode{node2, node3, node4, node5} {\n\t\tnode.handler.On(\"OnConsensus\", testChannel, node1.nodeInfo.ID, mock.Anything).Run(func(args mock.Arguments) {\n\t\t\tmsg := args.Get(2).(*orderer.ConsensusRequest)\n\t\t\trequire.Len(t, msg.Payload, msgSize)\n\t\t\tmessageReceived.Done()\n\t\t}).Return(nil)\n\t}\n\n\tstreams := map[uint64]*cluster.Stream{}\n\n\tfor _, node := range []*clusterNode{node2, node3, node4, node5} {\n\t\t// Freeze the node, in order to block its Recv\n\t\tnode.freeze()\n\t}\n\n\tfor _, node := range []*clusterNode{node2, node3, node4, node5} {\n\t\trm, err := node1.c.Remote(testChannel, node.nodeInfo.ID)\n\t\trequire.NoError(t, err)\n\n\t\tstream := assertEventualEstablishStream(t, rm)\n\t\tstreams[node.nodeInfo.ID] = stream\n\t}\n\n\tt0 := time.Now()\n\tfor _, node := range []*clusterNode{node2, node3, node4, node5} {\n\t\tstream := streams[node.nodeInfo.ID]\n\n\t\tt1 := time.Now()\n\t\terr = stream.Send(wrappedMsg)\n\t\trequire.NoError(t, err)\n\t\tt.Log(\"Sending took\", time.Since(t1))\n\n\t\t// Unfreeze the node. It can now call Recv, and signal the messageReceived waitGroup.\n\t\tnode.unfreeze()\n\t}\n\n\tt.Log(\"Total sending time to all 4 nodes took:\", time.Since(t0))\n\n\tmessageReceived.Wait()\n}\n\nfunc TestBlockingSend(t *testing.T) {\n\t// Scenario: Basic test that spawns 2 nodes and sends from the first node\n\t// to the second node, three SubmitRequests, or three consensus requests.\n\t// SubmitRequests should block, but consensus requests should not.\n\n\tfor _, testCase := range []struct {\n\t\tdescription        string\n\t\tmessageToSend      *orderer.StepRequest\n\t\tstreamUnblocks     bool\n\t\telapsedGreaterThan time.Duration\n\t\toverflowErr        string\n\t}{\n\t\t{\n\t\t\tdescription:        \"SubmitRequest\",\n\t\t\tmessageToSend:      wrapSubmitReq(testReq),\n\t\t\tstreamUnblocks:     true,\n\t\t\telapsedGreaterThan: time.Second / 2,\n\t\t},\n\t\t{\n\t\t\tdescription:   \"ConsensusRequest\",\n\t\t\tmessageToSend: testConsensusReq,\n\t\t\toverflowErr:   \"send queue overflown\",\n\t\t},\n\t} {\n\t\tt.Run(testCase.description, func(t *testing.T) {\n\t\t\tnode1 := newTestNode(t)\n\t\t\tnode2 := newTestNode(t)\n\n\t\t\tnode1.c.SendBufferSize = 1\n\t\t\tnode2.c.SendBufferSize = 1\n\n\t\t\tdefer node1.stop()\n\t\t\tdefer node2.stop()\n\n\t\t\tconfig := []cluster.RemoteNode{node1.nodeInfo, node2.nodeInfo}\n\t\t\tnode1.c.Configure(testChannel, config)\n\t\t\tnode2.c.Configure(testChannel, config)\n\n\t\t\trm, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tclient := &mocks.ClusterClient{}\n\t\t\tfakeStream := &mocks.StepClient{}\n\n\t\t\t// Replace real client with a mock client\n\t\t\trm.Client = client\n\t\t\trm.ProbeConn = func(_ *grpc.ClientConn) error {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\t// Configure client to return the mock stream\n\t\t\tfakeStream.On(\"Context\", mock.Anything).Return(context.Background())\n\t\t\tclient.On(\"Step\", mock.Anything).Return(fakeStream, nil).Once()\n\n\t\t\tunBlock := make(chan struct{})\n\t\t\tvar sendInvoked sync.WaitGroup\n\t\t\tsendInvoked.Add(1)\n\t\t\tvar once sync.Once\n\t\t\tfakeStream.On(\"Send\", mock.Anything).Run(func(_ mock.Arguments) {\n\t\t\t\tonce.Do(sendInvoked.Done)\n\t\t\t\t<-unBlock\n\t\t\t}).Return(errors.New(\"oops\"))\n\n\t\t\tstream, err := rm.NewStream(time.Hour)\n\t\t\trequire.NoError(t, err)\n\n\t\t\t// The first send doesn't block, even though the Send operation blocks.\n\t\t\terr = stream.Send(testCase.messageToSend)\n\t\t\trequire.NoError(t, err)\n\n\t\t\t// The second once doesn't either.\n\t\t\t// After this point, we have 1 goroutine which is blocked on Send(),\n\t\t\t// and one message in the buffer.\n\t\t\tsendInvoked.Wait()\n\t\t\terr = stream.Send(testCase.messageToSend)\n\t\t\trequire.NoError(t, err)\n\n\t\t\t// The third blocks, so we need to unblock it ourselves\n\t\t\t// in order for it to go through, unless the operation\n\t\t\t// is non blocking.\n\t\t\tgo func() {\n\t\t\t\ttime.Sleep(time.Second)\n\t\t\t\tif testCase.streamUnblocks {\n\t\t\t\t\tclose(unBlock)\n\t\t\t\t}\n\t\t\t}()\n\n\t\t\tt1 := time.Now()\n\t\t\terr = stream.Send(testCase.messageToSend)\n\t\t\t// The third send always overflows or blocks.\n\t\t\t// If we expect to receive an overflow error - assert it.\n\t\t\tif testCase.overflowErr != \"\" {\n\t\t\t\trequire.EqualError(t, err, testCase.overflowErr)\n\t\t\t}\n\t\t\telapsed := time.Since(t1)\n\t\t\tt.Log(\"Elapsed time:\", elapsed)\n\t\t\trequire.True(t, elapsed > testCase.elapsedGreaterThan)\n\n\t\t\tif !testCase.streamUnblocks {\n\t\t\t\tclose(unBlock)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestBasic(t *testing.T) {\n\t// Scenario: Basic test that spawns 2 nodes and sends each other\n\t// messages that are expected to be echoed back\n\n\tnode1 := newTestNode(t)\n\tnode2 := newTestNode(t)\n\n\tdefer node1.stop()\n\tdefer node2.stop()\n\n\tconfig := []cluster.RemoteNode{node1.nodeInfo, node2.nodeInfo}\n\tnode1.c.Configure(testChannel, config)\n\tnode2.c.Configure(testChannel, config)\n\n\tassertBiDiCommunication(t, node1, node2, testReq)\n}\n\nfunc TestUnavailableHosts(t *testing.T) {\n\t// Scenario: A node is configured to connect\n\t// to a host that is down\n\tnode1 := newTestNode(t)\n\n\tclientConfig := node1.dialer.Config\n\t// The below timeout makes sure that connection establishment is done\n\t// asynchronously. Had it been synchronous, the Remote() call would be\n\t// blocked for an hour.\n\tclientConfig.DialTimeout = time.Hour\n\tdefer node1.stop()\n\n\tnode2 := newTestNode(t)\n\tnode2.stop()\n\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node2.nodeInfo})\n\tremote, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\trequire.NotNil(t, remote)\n\n\t_, err = remote.NewStream(time.Millisecond * 100)\n\trequire.Contains(t, err.Error(), \"connection\")\n}\n\nfunc TestStreamAbortReportCorrectError(t *testing.T) {\n\t// Scenario: node 1 acquires a stream to node 2 and then the stream\n\t// encounters an error and as a result, the stream is aborted.\n\t// We ensure the error reported is the first error, even after\n\t// multiple attempts of using it.\n\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\n\tnode2 := newTestNode(t)\n\tdefer node2.stop()\n\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node2.nodeInfo})\n\tnode2.c.Configure(testChannel, []cluster.RemoteNode{node1.nodeInfo})\n\n\tnode2.handler.On(\"OnSubmit\", testChannel, node1.nodeInfo.ID, mock.Anything).Return(errors.Errorf(\"whoops\")).Once()\n\n\trm1, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\tvar streamTerminated sync.WaitGroup\n\tstreamTerminated.Add(1)\n\n\tstream := assertEventualEstablishStream(t, rm1)\n\n\tl, err := zap.NewDevelopment()\n\trequire.NoError(t, err)\n\tstream.Logger = flogging.NewFabricLogger(l, zap.Hooks(func(entry zapcore.Entry) error {\n\t\tif strings.Contains(entry.Message, \"Stream 1 to\") && strings.Contains(entry.Message, \"terminated\") {\n\t\t\tstreamTerminated.Done()\n\t\t}\n\t\treturn nil\n\t}))\n\n\t// Probe the stream for the first time\n\terr = stream.Send(wrapSubmitReq(testReq))\n\trequire.NoError(t, err)\n\n\t// We should receive back the crafted error\n\t_, err = stream.Recv()\n\trequire.Contains(t, err.Error(), \"whoops\")\n\n\t// Wait for the stream to be terminated from within the communication infrastructure\n\tstreamTerminated.Wait()\n\n\t// We should still receive the original crafted error despite the stream being terminated\n\terr = stream.Send(wrapSubmitReq(testReq))\n\trequire.Contains(t, err.Error(), \"whoops\")\n}\n\nfunc TestStreamAbort(t *testing.T) {\n\t// Scenarios: node 1 is connected to node 2 in 2 channels,\n\t// and the consumer of the communication calls receive.\n\t// The two sub-scenarios happen:\n\t// 1) The server certificate of node 2 changes in the first channel\n\t// 2) Node 2 is evicted from the membership of the first channel\n\t// In both of the scenarios, the Recv() call should be aborted\n\n\tnode2 := newTestNode(t)\n\tdefer node2.stop()\n\n\tinvalidNodeInfo := cluster.RemoteNode{\n\t\tID:            node2.nodeInfo.ID,\n\t\tServerTLSCert: []byte{1, 2, 3},\n\t\tClientTLSCert: []byte{1, 2, 3},\n\t}\n\n\tfor _, tst := range []struct {\n\t\ttestName      string\n\t\tmembership    []cluster.RemoteNode\n\t\texpectedError string\n\t}{\n\t\t{\n\t\t\ttestName:      \"Evicted from membership\",\n\t\t\tmembership:    nil,\n\t\t\texpectedError: \"rpc error: code = Canceled desc = context canceled\",\n\t\t},\n\t\t{\n\t\t\ttestName:      \"Changed TLS certificate\",\n\t\t\tmembership:    []cluster.RemoteNode{invalidNodeInfo},\n\t\t\texpectedError: \"rpc error: code = Canceled desc = context canceled\",\n\t\t},\n\t} {\n\t\tt.Run(tst.testName, func(t *testing.T) {\n\t\t\ttestStreamAbort(t, node2, tst.membership, tst.expectedError)\n\t\t})\n\t}\n\tnode2.handler.AssertNumberOfCalls(t, \"OnSubmit\", 2)\n}\n\nfunc testStreamAbort(t *testing.T, node2 *clusterNode, newMembership []cluster.RemoteNode, expectedError string) {\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node2.nodeInfo})\n\tnode2.c.Configure(testChannel, []cluster.RemoteNode{node1.nodeInfo})\n\tnode1.c.Configure(testChannel2, []cluster.RemoteNode{node2.nodeInfo})\n\tnode2.c.Configure(testChannel2, []cluster.RemoteNode{node1.nodeInfo})\n\n\tvar streamCreated sync.WaitGroup\n\tstreamCreated.Add(1)\n\n\tstopChan := make(chan struct{})\n\n\tnode2.handler.On(\"OnSubmit\", testChannel, node1.nodeInfo.ID, mock.Anything).Once().Run(func(_ mock.Arguments) {\n\t\t// Notify the stream was created\n\t\tstreamCreated.Done()\n\t\t// Wait for the test to finish\n\t\t<-stopChan\n\t}).Return(nil).Once()\n\n\trm1, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\n\tgo func() {\n\t\tstream := assertEventualEstablishStream(t, rm1)\n\t\t// Signal the reconfiguration\n\t\terr = stream.Send(wrapSubmitReq(testReq))\n\t\trequire.NoError(t, err)\n\t\t_, err := stream.Recv()\n\t\trequire.Contains(t, err.Error(), expectedError)\n\t\tclose(stopChan)\n\t}()\n\n\tgo func() {\n\t\t// Wait for the stream reference to be obtained\n\t\tstreamCreated.Wait()\n\t\t// Reconfigure the channel membership\n\t\tnode1.c.Configure(testChannel, newMembership)\n\t}()\n\n\t<-stopChan\n}\n\nfunc TestDoubleReconfigure(t *testing.T) {\n\t// Scenario: Basic test that spawns 2 nodes\n\t// and configures node 1 twice, and checks that\n\t// the remote stub for node 1 wasn't re-created in the second\n\t// configuration since it already existed\n\n\tnode1 := newTestNode(t)\n\tnode2 := newTestNode(t)\n\n\tdefer node1.stop()\n\tdefer node2.stop()\n\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node2.nodeInfo})\n\trm1, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node2.nodeInfo})\n\trm2, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\t// Ensure the references are equal\n\trequire.True(t, rm1 == rm2)\n}\n\nfunc TestInvalidChannel(t *testing.T) {\n\t// Scenario: node 1 it ordered to send a message on a channel\n\t// that doesn't exist, and also receives a message, but\n\t// the channel cannot be extracted from the message.\n\n\tt.Run(\"channel doesn't exist\", func(t *testing.T) {\n\t\tnode1 := newTestNode(t)\n\t\tdefer node1.stop()\n\n\t\t_, err := node1.c.Remote(testChannel, 0)\n\t\trequire.EqualError(t, err, \"channel test doesn't exist\")\n\t})\n\n\tt.Run(\"channel cannot be extracted\", func(t *testing.T) {\n\t\tnode1 := newTestNode(t)\n\t\tdefer node1.stop()\n\n\t\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node1.nodeInfo})\n\t\tgt := gomega.NewGomegaWithT(t)\n\t\tgt.Eventually(func() (bool, error) {\n\t\t\t_, err := node1.c.Remote(testChannel, node1.nodeInfo.ID)\n\t\t\treturn true, err\n\t\t}, time.Minute).Should(gomega.BeTrue())\n\n\t\tstub, err := node1.c.Remote(testChannel, node1.nodeInfo.ID)\n\t\trequire.NoError(t, err)\n\n\t\tstream := assertEventualEstablishStream(t, stub)\n\n\t\t// An empty SubmitRequest has an empty channel which is invalid\n\t\terr = stream.Send(wrapSubmitReq(&orderer.SubmitRequest{}))\n\t\trequire.NoError(t, err)\n\n\t\t_, err = stream.Recv()\n\t\trequire.EqualError(t, err, \"rpc error: code = Unknown desc = badly formatted message, cannot extract channel\")\n\n\t\t// Test directly without going through the gRPC stream\n\t\terr = node1.c.DispatchSubmit(context.Background(), &orderer.SubmitRequest{})\n\t\trequire.EqualError(t, err, \"badly formatted message, cannot extract channel\")\n\t})\n}\n\nfunc TestAbortRPC(t *testing.T) {\n\t// Scenarios:\n\t// (I) The node calls an RPC, and calls Abort() on the remote context\n\t//  in parallel. The RPC should return even though the server-side call hasn't finished.\n\t// (II) The node calls an RPC, but the server-side processing takes too long,\n\t// and the RPC invocation returns prematurely.\n\n\ttestCases := []struct {\n\t\tname        string\n\t\tabortFunc   func(*cluster.RemoteContext)\n\t\trpcTimeout  time.Duration\n\t\texpectedErr string\n\t}{\n\t\t{\n\t\t\tname:        \"Abort() called\",\n\t\t\texpectedErr: \"rpc error: code = Canceled desc = context canceled\",\n\t\t\trpcTimeout:  time.Hour,\n\t\t\tabortFunc: func(rc *cluster.RemoteContext) {\n\t\t\t\trc.Abort()\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:        \"RPC timeout\",\n\t\t\texpectedErr: \"rpc timeout expired\",\n\t\t\trpcTimeout:  time.Second,\n\t\t\tabortFunc:   func(*cluster.RemoteContext) {},\n\t\t},\n\t}\n\n\tfor _, testCase := range testCases {\n\t\ttestCase := testCase\n\t\tt.Run(testCase.name, func(t *testing.T) {\n\t\t\ttestAbort(t, testCase.abortFunc, testCase.rpcTimeout, testCase.expectedErr)\n\t\t})\n\t}\n}\n\nfunc testAbort(t *testing.T, abortFunc func(*cluster.RemoteContext), rpcTimeout time.Duration, expectedErr string) {\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\n\tnode2 := newTestNode(t)\n\tdefer node2.stop()\n\n\tconfig := []cluster.RemoteNode{node1.nodeInfo, node2.nodeInfo}\n\tnode1.c.Configure(testChannel, config)\n\tnode2.c.Configure(testChannel, config)\n\tvar onStepCalled sync.WaitGroup\n\tonStepCalled.Add(1)\n\n\t// stuckCall ensures the OnStep() call is stuck throughout this test\n\tvar stuckCall sync.WaitGroup\n\tstuckCall.Add(1)\n\t// At the end of the test, release the server-side resources\n\tdefer stuckCall.Done()\n\n\tnode2.handler.On(\"OnSubmit\", testChannel, node1.nodeInfo.ID, mock.Anything).Return(nil).Once().Run(func(_ mock.Arguments) {\n\t\tonStepCalled.Done()\n\t\tstuckCall.Wait()\n\t}).Once()\n\n\trm, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\n\tgo func() {\n\t\tonStepCalled.Wait()\n\t\tabortFunc(rm)\n\t}()\n\n\tvar stream *cluster.Stream\n\tgt := gomega.NewGomegaWithT(t)\n\tgt.Eventually(func() error {\n\t\tstream, err = rm.NewStream(rpcTimeout)\n\t\treturn err\n\t}, time.Second*10, time.Millisecond*10).Should(gomega.Succeed())\n\n\tstream.Send(wrapSubmitReq(testSubReq))\n\t_, err = stream.Recv()\n\n\trequire.EqualError(t, err, expectedErr)\n\n\tnode2.handler.AssertNumberOfCalls(t, \"OnSubmit\", 1)\n}\n\nfunc TestNoTLSCertificate(t *testing.T) {\n\t// Scenario: The node is sent a message by another node that doesn't\n\t// connect with mutual TLS, thus doesn't provide a TLS certificate\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node1.nodeInfo})\n\n\tclientConfig := comm_utils.ClientConfig{\n\t\tAsyncConnect: true,\n\t\tDialTimeout:  time.Millisecond * 100,\n\t\tSecOpts: comm_utils.SecureOptions{\n\t\t\tServerRootCAs: [][]byte{ca.CertBytes()},\n\t\t\tUseTLS:        true,\n\t\t},\n\t}\n\n\tvar conn *grpc.ClientConn\n\tgt := gomega.NewGomegaWithT(t)\n\tgt.Eventually(func() (bool, error) {\n\t\tvar err error\n\t\tconn, err = clientConfig.Dial(node1.srv.Address())\n\t\treturn true, err\n\t}, time.Minute).Should(gomega.BeTrue())\n\n\techoClient := orderer.NewClusterClient(conn)\n\tstream, err := echoClient.Step(context.Background())\n\trequire.NoError(t, err)\n\n\terr = stream.Send(wrapSubmitReq(testSubReq))\n\trequire.NoError(t, err)\n\t_, err = stream.Recv()\n\trequire.EqualError(t, err, \"rpc error: code = Unknown desc = no TLS certificate sent\")\n}\n\nfunc TestReconnect(t *testing.T) {\n\t// Scenario: node 1 and node 2 are connected,\n\t// and node 2 is taken offline.\n\t// Node 1 tries to send a message to node 2 but fails,\n\t// and afterwards node 2 is brought back, after which\n\t// node 1 sends more messages, and it should succeed\n\t// sending a message to node 2 eventually.\n\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\tconf := node1.dialer.Config\n\tconf.DialTimeout = time.Hour\n\n\tnode2 := newTestNode(t)\n\tnode2.handler.On(\"OnSubmit\", testChannel, node1.nodeInfo.ID, mock.Anything).Return(nil)\n\tdefer node2.stop()\n\n\tconfig := []cluster.RemoteNode{node1.nodeInfo, node2.nodeInfo}\n\tnode1.c.Configure(testChannel, config)\n\tnode2.c.Configure(testChannel, config)\n\n\t// Make node 2 be offline by shutting down its gRPC service\n\tnode2.srv.Stop()\n\t// Obtain the stub for node 2.\n\t// Should succeed, because the connection was created at time of configuration\n\tstub, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\n\t// Try to obtain a stream. Should not Succeed.\n\tgt := gomega.NewGomegaWithT(t)\n\tgt.Eventually(func() error {\n\t\t_, err = stub.NewStream(time.Hour)\n\t\treturn err\n\t}).Should(gomega.Not(gomega.Succeed()))\n\n\t// Wait for the port to be released\n\tfor {\n\t\tlsnr, err := net.Listen(\"tcp\", node2.nodeInfo.Endpoint)\n\t\tif err == nil {\n\t\t\tlsnr.Close()\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// Resurrect node 2\n\tnode2.resurrect()\n\t// Send a message from node 1 to node 2.\n\t// Should succeed eventually\n\tassertEventualSendMessage(t, stub, testReq)\n}\n\nfunc TestRenewCertificates(t *testing.T) {\n\t// Scenario: node 1 and node 2 are connected,\n\t// Node 2's certificate is renewed, and\n\t// node 1 is reconfigured with the new\n\t// configuration without being restarted.\n\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\n\tnode2 := newTestNode(t)\n\tdefer node2.stop()\n\n\tnode1.handler.On(\"OnStep\", testChannel, node2.nodeInfo.ID, mock.Anything).Return(testRes, nil)\n\tnode2.handler.On(\"OnStep\", testChannel, node1.nodeInfo.ID, mock.Anything).Return(testRes, nil)\n\n\tconfig := []cluster.RemoteNode{node1.nodeInfo, node2.nodeInfo}\n\tnode1.c.Configure(testChannel, config)\n\tnode2.c.Configure(testChannel, config)\n\n\tassertBiDiCommunication(t, node1, node2, testReq)\n\n\t// Close outgoing connections from node2 to node1\n\tnode2.c.Configure(testChannel, nil)\n\t// Stop the gRPC service of node 2 to replace its certificate\n\tnode2.srv.Stop()\n\n\t// Wait until node 1 detects this\n\tgt := gomega.NewGomegaWithT(t)\n\tgt.Eventually(func() error {\n\t\tremote, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tstream, err := remote.NewStream(time.Hour)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\terr = stream.Send(wrapSubmitReq(testSubReq))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t}).Should(gomega.Not(gomega.Succeed()))\n\n\t// Renew node 2's keys\n\tnode2.renewCertificates()\n\n\t// Resurrect node 2 to make it service connections again\n\tnode2.resurrect()\n\n\t// W.L.O.G, try to send a message from node1 to node2\n\t// It should fail, because node2's server certificate has now changed,\n\t// so it closed the connection to the remote node\n\tinfo2 := node2.nodeInfo\n\tremote, err := node1.c.Remote(testChannel, info2.ID)\n\trequire.NoError(t, err)\n\trequire.NotNil(t, remote)\n\t_, err = remote.NewStream(time.Hour)\n\trequire.Contains(t, err.Error(), info2.Endpoint)\n\n\t// Reconfigure both nodes with the updates keys\n\tconfig = []cluster.RemoteNode{node1.nodeInfo, node2.nodeInfo}\n\tnode1.c.Configure(testChannel, config)\n\tnode2.c.Configure(testChannel, config)\n\n\t// Finally, check that the nodes can communicate once again\n\tassertBiDiCommunication(t, node1, node2, testReq)\n}\n\nfunc TestMembershipReconfiguration(t *testing.T) {\n\t// Scenario: node 1 and node 2 are started up\n\t// and node 2 is configured to know about node 1,\n\t// without node1 knowing about node 2.\n\t// The communication between them should only work\n\t// after node 1 is configured to know about node 2.\n\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\n\tnode2 := newTestNode(t)\n\tdefer node2.stop()\n\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{})\n\tnode2.c.Configure(testChannel, []cluster.RemoteNode{node1.nodeInfo})\n\n\t// Node 1 can't connect to node 2 because it doesn't know its TLS certificate yet\n\t_, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.EqualError(t, err, fmt.Sprintf(\"node %d doesn't exist in channel test's membership\", node2.nodeInfo.ID))\n\t// Node 2 can connect to node 1, but it can't send it messages because node 1 doesn't know node 2 yet.\n\n\tgt := gomega.NewGomegaWithT(t)\n\tgt.Eventually(func() (bool, error) {\n\t\t_, err := node2.c.Remote(testChannel, node1.nodeInfo.ID)\n\t\treturn true, err\n\t}, time.Minute).Should(gomega.BeTrue())\n\n\tstub, err := node2.c.Remote(testChannel, node1.nodeInfo.ID)\n\trequire.NoError(t, err)\n\n\tstream := assertEventualEstablishStream(t, stub)\n\terr = stream.Send(wrapSubmitReq(testSubReq))\n\trequire.NoError(t, err)\n\n\t_, err = stream.Recv()\n\trequire.EqualError(t, err, \"rpc error: code = Unknown desc = certificate extracted from TLS connection isn't authorized\")\n\n\t// Next, configure node 1 to know about node 2\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node2.nodeInfo})\n\n\t// Check that the communication works correctly between both nodes\n\tassertBiDiCommunication(t, node1, node2, testReq)\n\tassertBiDiCommunication(t, node2, node1, testReq)\n\n\t// Reconfigure node 2 to forget about node 1\n\tnode2.c.Configure(testChannel, []cluster.RemoteNode{})\n\t// Node 1 can still connect to node 2\n\tstub, err = node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\t// But can't send a message because node 2 now doesn't authorized node 1\n\tstream = assertEventualEstablishStream(t, stub)\n\tstream.Send(wrapSubmitReq(testSubReq))\n\t_, err = stream.Recv()\n\trequire.EqualError(t, err, \"rpc error: code = Unknown desc = certificate extracted from TLS connection isn't authorized\")\n}\n\nfunc TestShutdown(t *testing.T) {\n\t// Scenario: node 1 is shut down and as a result, can't\n\t// send messages to anyone, nor can it be reconfigured\n\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\n\tnode1.c.Shutdown()\n\n\t// Obtaining a RemoteContext cannot succeed because shutdown was called before\n\t_, err := node1.c.Remote(testChannel, node1.nodeInfo.ID)\n\trequire.EqualError(t, err, \"communication has been shut down\")\n\n\tnode2 := newTestNode(t)\n\tdefer node2.stop()\n\n\tnode2.c.Configure(testChannel, []cluster.RemoteNode{node1.nodeInfo})\n\t// Configuration of node doesn't take place\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node2.nodeInfo})\n\n\tgt := gomega.NewGomegaWithT(t)\n\tgt.Eventually(func() error {\n\t\t_, err := node2.c.Remote(testChannel, node1.nodeInfo.ID)\n\t\treturn err\n\t}, time.Minute).Should(gomega.Succeed())\n\n\tstub, err := node2.c.Remote(testChannel, node1.nodeInfo.ID)\n\trequire.NoError(t, err)\n\n\t// Therefore, sending a message doesn't succeed because node 1 rejected the configuration change\n\tgt.Eventually(func() string {\n\t\tstream, err := stub.NewStream(time.Hour)\n\t\tif err != nil {\n\t\t\treturn err.Error()\n\t\t}\n\t\terr = stream.Send(wrapSubmitReq(testSubReq))\n\t\trequire.NoError(t, err)\n\n\t\t_, err = stream.Recv()\n\t\treturn err.Error()\n\t}, timeout).Should(gomega.ContainSubstring(\"channel test doesn't exist\"))\n}\n\nfunc TestMultiChannelConfig(t *testing.T) {\n\t// Scenario: node 1 is knows node 2 only in channel \"foo\"\n\t// and knows node 3 only in channel \"bar\".\n\t// Messages that are received, are routed according to their corresponding channels\n\t// and when node 2 sends a message for channel \"bar\" to node 1, it is rejected.\n\t// Same thing applies for node 3 that sends a message to node 1 in channel \"foo\".\n\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\n\tnode2 := newTestNode(t)\n\tdefer node2.stop()\n\n\tnode3 := newTestNode(t)\n\tdefer node3.stop()\n\n\tnode1.c.Configure(\"foo\", []cluster.RemoteNode{node2.nodeInfo})\n\tnode1.c.Configure(\"bar\", []cluster.RemoteNode{node3.nodeInfo})\n\tnode2.c.Configure(\"foo\", []cluster.RemoteNode{node1.nodeInfo})\n\tnode3.c.Configure(\"bar\", []cluster.RemoteNode{node1.nodeInfo})\n\n\tt.Run(\"Correct channel\", func(t *testing.T) {\n\t\tvar fromNode2 sync.WaitGroup\n\t\tfromNode2.Add(1)\n\t\tnode1.handler.On(\"OnSubmit\", \"foo\", node2.nodeInfo.ID, mock.Anything).Return(nil).Run(func(_ mock.Arguments) {\n\t\t\tfromNode2.Done()\n\t\t}).Once()\n\n\t\tvar fromNode3 sync.WaitGroup\n\t\tfromNode3.Add(1)\n\t\tnode1.handler.On(\"OnSubmit\", \"bar\", node3.nodeInfo.ID, mock.Anything).Return(nil).Run(func(_ mock.Arguments) {\n\t\t\tfromNode3.Done()\n\t\t}).Once()\n\n\t\tnode2toNode1, err := node2.c.Remote(\"foo\", node1.nodeInfo.ID)\n\t\trequire.NoError(t, err)\n\t\tnode3toNode1, err := node3.c.Remote(\"bar\", node1.nodeInfo.ID)\n\t\trequire.NoError(t, err)\n\n\t\tstream := assertEventualEstablishStream(t, node2toNode1)\n\t\tstream.Send(fooReq)\n\n\t\tfromNode2.Wait()\n\t\tnode1.handler.AssertNumberOfCalls(t, \"OnSubmit\", 1)\n\n\t\tstream = assertEventualEstablishStream(t, node3toNode1)\n\t\tstream.Send(barReq)\n\n\t\tfromNode3.Wait()\n\t\tnode1.handler.AssertNumberOfCalls(t, \"OnSubmit\", 2)\n\t})\n\n\tt.Run(\"Incorrect channel\", func(t *testing.T) {\n\t\tnode1.handler.On(\"OnSubmit\", \"foo\", node2.nodeInfo.ID, mock.Anything).Return(nil)\n\t\tnode1.handler.On(\"OnSubmit\", \"bar\", node3.nodeInfo.ID, mock.Anything).Return(nil)\n\n\t\tnode2toNode1, err := node2.c.Remote(\"foo\", node1.nodeInfo.ID)\n\t\trequire.NoError(t, err)\n\t\tnode3toNode1, err := node3.c.Remote(\"bar\", node1.nodeInfo.ID)\n\t\trequire.NoError(t, err)\n\n\t\tassertEventualSendMessage(t, node2toNode1, &orderer.SubmitRequest{Channel: \"foo\"})\n\t\trequire.NoError(t, err)\n\t\tstream, err := node2toNode1.NewStream(time.Hour)\n\t\trequire.NoError(t, err)\n\t\terr = stream.Send(barReq)\n\t\trequire.NoError(t, err)\n\t\t_, err = stream.Recv()\n\t\trequire.EqualError(t, err, \"rpc error: code = Unknown desc = certificate extracted from TLS connection isn't authorized\")\n\n\t\tassertEventualSendMessage(t, node3toNode1, &orderer.SubmitRequest{Channel: \"bar\"})\n\t\tstream, err = node3toNode1.NewStream(time.Hour)\n\t\trequire.NoError(t, err)\n\t\terr = stream.Send(fooReq)\n\t\trequire.NoError(t, err)\n\t\t_, err = stream.Recv()\n\t\trequire.EqualError(t, err, \"rpc error: code = Unknown desc = certificate extracted from TLS connection isn't authorized\")\n\t})\n}\n\nfunc TestConnectionFailure(t *testing.T) {\n\t// Scenario: node 1 fails to connect to node 2.\n\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\n\tnode2 := newTestNode(t)\n\tdefer node2.stop()\n\n\tdialer := &mocks.SecureDialer{}\n\tdialer.On(\"Dial\", mock.Anything, mock.Anything).Return(nil, errors.New(\"oops\"))\n\tnode1.c.Connections = cluster.NewConnectionStore(dialer, &disabled.Gauge{})\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node2.nodeInfo})\n\n\t_, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.EqualError(t, err, \"oops\")\n}\n\ntype testMetrics struct {\n\tfakeProvider        *mocks.MetricsProvider\n\tegressQueueLength   metricsfakes.Gauge\n\tegressQueueCapacity metricsfakes.Gauge\n\tegressStreamCount   metricsfakes.Gauge\n\tegressTLSConnCount  metricsfakes.Gauge\n\tegressWorkerSize    metricsfakes.Gauge\n\tingressStreamsCount metricsfakes.Gauge\n\tmsgSendTime         metricsfakes.Histogram\n\tmsgDropCount        metricsfakes.Counter\n}\n\nfunc (tm *testMetrics) initialize() {\n\ttm.egressQueueLength.WithReturns(&tm.egressQueueLength)\n\ttm.egressQueueCapacity.WithReturns(&tm.egressQueueCapacity)\n\ttm.egressStreamCount.WithReturns(&tm.egressStreamCount)\n\ttm.egressTLSConnCount.WithReturns(&tm.egressTLSConnCount)\n\ttm.egressWorkerSize.WithReturns(&tm.egressWorkerSize)\n\ttm.ingressStreamsCount.WithReturns(&tm.ingressStreamsCount)\n\ttm.msgSendTime.WithReturns(&tm.msgSendTime)\n\ttm.msgDropCount.WithReturns(&tm.msgDropCount)\n\n\tfakeProvider := tm.fakeProvider\n\tfakeProvider.On(\"NewGauge\", cluster.IngressStreamsCountOpts).Return(&tm.ingressStreamsCount)\n\tfakeProvider.On(\"NewGauge\", cluster.EgressQueueLengthOpts).Return(&tm.egressQueueLength)\n\tfakeProvider.On(\"NewGauge\", cluster.EgressQueueCapacityOpts).Return(&tm.egressQueueCapacity)\n\tfakeProvider.On(\"NewGauge\", cluster.EgressStreamsCountOpts).Return(&tm.egressStreamCount)\n\tfakeProvider.On(\"NewGauge\", cluster.EgressTLSConnectionCountOpts).Return(&tm.egressTLSConnCount)\n\tfakeProvider.On(\"NewGauge\", cluster.EgressWorkersOpts).Return(&tm.egressWorkerSize)\n\tfakeProvider.On(\"NewCounter\", cluster.MessagesDroppedCountOpts).Return(&tm.msgDropCount)\n\tfakeProvider.On(\"NewHistogram\", cluster.MessageSendTimeOpts).Return(&tm.msgSendTime)\n}\n\nfunc TestMetrics(t *testing.T) {\n\tfor _, testCase := range []struct {\n\t\tname        string\n\t\trunTest     func(t *testing.T, node1, node2 *clusterNode, testMetrics *testMetrics)\n\t\ttestMetrics *testMetrics\n\t}{\n\t\t{\n\t\t\tname: \"EgressQueueOccupancy\",\n\t\t\trunTest: func(t *testing.T, node1, node2 *clusterNode, testMetrics *testMetrics) {\n\t\t\t\tassertBiDiCommunication(t, node1, node2, testReq)\n\t\t\t\trequire.Equal(t, []string{\"host\", node2.nodeInfo.Endpoint, \"msg_type\", \"transaction\", \"channel\", testChannel},\n\t\t\t\t\ttestMetrics.egressQueueLength.WithArgsForCall(0))\n\t\t\t\trequire.Equal(t, float64(0), testMetrics.egressQueueLength.SetArgsForCall(0))\n\t\t\t\trequire.Equal(t, float64(1), testMetrics.egressQueueCapacity.SetArgsForCall(0))\n\n\t\t\t\tvar messageReceived sync.WaitGroup\n\t\t\t\tmessageReceived.Add(1)\n\t\t\t\tnode2.handler.On(\"OnConsensus\", testChannel, node1.nodeInfo.ID, mock.Anything).Run(func(args mock.Arguments) {\n\t\t\t\t\tmessageReceived.Done()\n\t\t\t\t}).Return(nil)\n\n\t\t\t\trm, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tstream := assertEventualEstablishStream(t, rm)\n\t\t\t\tstream.Send(testConsensusReq)\n\t\t\t\tmessageReceived.Wait()\n\n\t\t\t\trequire.Equal(t, []string{\"host\", node2.nodeInfo.Endpoint, \"msg_type\", \"consensus\", \"channel\", testChannel},\n\t\t\t\t\ttestMetrics.egressQueueLength.WithArgsForCall(1))\n\t\t\t\trequire.Equal(t, float64(0), testMetrics.egressQueueLength.SetArgsForCall(1))\n\t\t\t\trequire.Equal(t, float64(1), testMetrics.egressQueueCapacity.SetArgsForCall(1))\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"EgressStreamsCount\",\n\t\t\trunTest: func(t *testing.T, node1, node2 *clusterNode, testMetrics *testMetrics) {\n\t\t\t\tassertBiDiCommunication(t, node1, node2, testReq)\n\t\t\t\trequire.Equal(t, 1, testMetrics.egressStreamCount.SetCallCount())\n\t\t\t\trequire.Equal(t, 1, testMetrics.egressStreamCount.WithCallCount())\n\t\t\t\trequire.Equal(t, []string{\"channel\", testChannel}, testMetrics.egressStreamCount.WithArgsForCall(0))\n\n\t\t\t\tassertBiDiCommunicationForChannel(t, node1, node2, testReq2, testChannel2)\n\t\t\t\trequire.Equal(t, 2, testMetrics.egressStreamCount.SetCallCount())\n\t\t\t\trequire.Equal(t, 2, testMetrics.egressStreamCount.WithCallCount())\n\t\t\t\trequire.Equal(t, []string{\"channel\", testChannel2}, testMetrics.egressStreamCount.WithArgsForCall(1))\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"EgressTLSConnCount\",\n\t\t\trunTest: func(t *testing.T, node1, node2 *clusterNode, testMetrics *testMetrics) {\n\t\t\t\tassertBiDiCommunication(t, node1, node2, testReq)\n\t\t\t\trequire.Equal(t, []string{\"channel\", testChannel}, testMetrics.egressStreamCount.WithArgsForCall(0))\n\n\t\t\t\tassertBiDiCommunicationForChannel(t, node1, node2, testReq2, testChannel2)\n\t\t\t\trequire.Equal(t, []string{\"channel\", testChannel2}, testMetrics.egressStreamCount.WithArgsForCall(1))\n\n\t\t\t\t// A single TLS connection despite 2 streams\n\t\t\t\trequire.Equal(t, float64(1), testMetrics.egressTLSConnCount.SetArgsForCall(0))\n\t\t\t\trequire.Equal(t, 1, testMetrics.egressTLSConnCount.SetCallCount())\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"EgressWorkerSize\",\n\t\t\trunTest: func(t *testing.T, node1, node2 *clusterNode, testMetrics *testMetrics) {\n\t\t\t\tassertBiDiCommunication(t, node1, node2, testReq)\n\t\t\t\trequire.Equal(t, []string{\"channel\", testChannel}, testMetrics.egressStreamCount.WithArgsForCall(0))\n\n\t\t\t\tassertBiDiCommunicationForChannel(t, node1, node2, testReq2, testChannel2)\n\t\t\t\trequire.Equal(t, []string{\"channel\", testChannel2}, testMetrics.egressStreamCount.WithArgsForCall(1))\n\n\t\t\t\trequire.Equal(t, float64(1), testMetrics.egressWorkerSize.SetArgsForCall(0))\n\t\t\t\trequire.Equal(t, float64(1), testMetrics.egressWorkerSize.SetArgsForCall(1))\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"MsgSendTime\",\n\t\t\trunTest: func(t *testing.T, node1, node2 *clusterNode, testMetrics *testMetrics) {\n\t\t\t\tassertBiDiCommunication(t, node1, node2, testReq)\n\t\t\t\trequire.Eventually(t, func() bool { return testMetrics.msgSendTime.ObserveCallCount() > 0 }, time.Second, 10*time.Millisecond)\n\t\t\t\trequire.Equal(t, 1, testMetrics.msgSendTime.ObserveCallCount())\n\t\t\t\trequire.Equal(t, []string{\"host\", node2.nodeInfo.Endpoint, \"channel\", testChannel}, testMetrics.msgSendTime.WithArgsForCall(0))\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"MsgDropCount\",\n\t\t\trunTest: func(t *testing.T, node1, node2 *clusterNode, testMetrics *testMetrics) {\n\t\t\t\tblockRecv := make(chan struct{})\n\t\t\t\twasReported := func() bool {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-blockRecv:\n\t\t\t\t\t\treturn true\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn false\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// When the drop count is reported, release the lock on the server side receive operation.\n\t\t\t\ttestMetrics.msgDropCount.AddStub = func(float642 float64) {\n\t\t\t\t\tif !wasReported() {\n\t\t\t\t\t\tclose(blockRecv)\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tnode2.handler.On(\"OnConsensus\", testChannel, node1.nodeInfo.ID, mock.Anything).Run(func(args mock.Arguments) {\n\t\t\t\t\t// Block until the message drop is reported\n\t\t\t\t\t<-blockRecv\n\t\t\t\t}).Return(nil)\n\n\t\t\t\trm, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tstream := assertEventualEstablishStream(t, rm)\n\t\t\t\t// Send too many messages while the server side is not reading from the stream\n\t\t\t\tfor {\n\t\t\t\t\tstream.Send(testConsensusReq)\n\t\t\t\t\tif wasReported() {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\trequire.Equal(t, []string{\"host\", node2.nodeInfo.Endpoint, \"channel\", testChannel},\n\t\t\t\t\ttestMetrics.msgDropCount.WithArgsForCall(0))\n\t\t\t\trequire.Equal(t, 1, testMetrics.msgDropCount.AddCallCount())\n\t\t\t},\n\t\t},\n\t} {\n\t\ttestCase := testCase\n\t\tt.Run(testCase.name, func(t *testing.T) {\n\t\t\tfakeProvider := &mocks.MetricsProvider{}\n\t\t\ttestCase.testMetrics = &testMetrics{\n\t\t\t\tfakeProvider: fakeProvider,\n\t\t\t}\n\n\t\t\ttestCase.testMetrics.initialize()\n\n\t\t\tnode1 := newTestNodeWithMetrics(t, fakeProvider, &testCase.testMetrics.egressTLSConnCount)\n\t\t\tdefer node1.stop()\n\n\t\t\tnode2 := newTestNode(t)\n\t\t\tdefer node2.stop()\n\n\t\t\tconfigForNode1 := []cluster.RemoteNode{node2.nodeInfo}\n\t\t\tconfigForNode2 := []cluster.RemoteNode{node1.nodeInfo}\n\t\t\tnode1.c.Configure(testChannel, configForNode1)\n\t\t\tnode2.c.Configure(testChannel, configForNode2)\n\t\t\tnode1.c.Configure(testChannel2, configForNode1)\n\t\t\tnode2.c.Configure(testChannel2, configForNode2)\n\n\t\t\ttestCase.runTest(t, node1, node2, testCase.testMetrics)\n\t\t})\n\t}\n}\n\nfunc TestCertExpirationWarningEgress(t *testing.T) {\n\t// Scenario: Ensures that when certificates are due to expire,\n\t// a warning is logged to the log.\n\n\tnode1 := newTestNode(t)\n\tnode2 := newTestNode(t)\n\n\tcert, err := x509.ParseCertificate(node2.nodeInfo.ServerTLSCert)\n\trequire.NoError(t, err)\n\trequire.NotNil(t, cert)\n\n\t// Let the NotAfter time of the certificate be T1, the current time be T0.\n\t// So time.Until is (T1 - T0), which means we have (T1 - T0) time left.\n\t// We want to trigger a warning, so we set the warning threshold to be 20 seconds above\n\t// the time left, so the time left would be smaller than the threshold.\n\tnode1.c.CertExpWarningThreshold = time.Until(cert.NotAfter) + time.Second*20\n\t// We only alert once in 3 seconds\n\tnode1.c.MinimumExpirationWarningInterval = time.Second * 3\n\n\tdefer node1.stop()\n\tdefer node2.stop()\n\n\tconfig := []cluster.RemoteNode{node1.nodeInfo, node2.nodeInfo}\n\tnode1.c.Configure(testChannel, config)\n\tnode2.c.Configure(testChannel, config)\n\n\tstub, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\n\tmockgRPC := &mocks.StepClient{}\n\tmockgRPC.On(\"Send\", mock.Anything).Return(nil)\n\tmockgRPC.On(\"Context\").Return(context.Background())\n\tmockClient := &mocks.ClusterClient{}\n\tmockClient.On(\"Step\", mock.Anything).Return(mockgRPC, nil)\n\n\tstub.Client = mockClient\n\n\tstream := assertEventualEstablishStream(t, stub)\n\n\talerts := make(chan struct{}, 100)\n\n\tstream.Logger = stream.Logger.WithOptions(zap.Hooks(func(entry zapcore.Entry) error {\n\t\tif strings.Contains(entry.Message, \"expires in less than\") {\n\t\t\talerts <- struct{}{}\n\t\t}\n\t\treturn nil\n\t}))\n\n\t// Send a message to the node and expert an alert to be logged.\n\tstream.Send(wrapSubmitReq(testReq))\n\tselect {\n\tcase <-alerts:\n\tcase <-time.After(time.Second * 5):\n\t\tt.Fatal(\"Should have logged an alert\")\n\t}\n\t// Send another message, and ensure we don't log anything to the log, because the\n\t// alerts should be suppressed before the minimum interval timeout expires.\n\tstream.Send(wrapSubmitReq(testReq))\n\tselect {\n\tcase <-alerts:\n\t\tt.Fatal(\"Should not have logged an alert\")\n\tcase <-time.After(time.Millisecond * 500):\n\t}\n\t// Wait enough time for the alert interval to clear.\n\ttime.Sleep(node1.c.MinimumExpirationWarningInterval + time.Second)\n\t// Send again a message, and this time it should be logged again.\n\tstream.Send(wrapSubmitReq(testReq))\n\tselect {\n\tcase <-alerts:\n\tcase <-time.After(time.Second * 5):\n\t\tt.Fatal(\"Should have logged an alert\")\n\t}\n}\n\nfunc assertBiDiCommunicationForChannel(t *testing.T, node1, node2 *clusterNode, msgToSend *orderer.SubmitRequest, channel string) {\n\testablish := []struct {\n\t\tlabel    string\n\t\tsender   *clusterNode\n\t\treceiver *clusterNode\n\t\ttarget   uint64\n\t}{\n\t\t{label: \"1->2\", sender: node1, target: node2.nodeInfo.ID, receiver: node2},\n\t\t{label: \"2->1\", sender: node2, target: node1.nodeInfo.ID, receiver: node1},\n\t}\n\tfor _, estab := range establish {\n\t\tstub, err := estab.sender.c.Remote(channel, estab.target)\n\t\trequire.NoError(t, err)\n\n\t\tstream := assertEventualEstablishStream(t, stub)\n\n\t\tvar wg sync.WaitGroup\n\t\twg.Add(1)\n\t\testab.receiver.handler.On(\"OnSubmit\", channel, estab.sender.nodeInfo.ID, mock.Anything).Return(nil).Once().Run(func(args mock.Arguments) {\n\t\t\treq := args.Get(2).(*orderer.SubmitRequest)\n\t\t\trequire.True(t, proto.Equal(req, msgToSend))\n\t\t\tt.Log(estab.label)\n\t\t\twg.Done()\n\t\t})\n\n\t\terr = stream.Send(wrapSubmitReq(msgToSend))\n\t\trequire.NoError(t, err)\n\n\t\twg.Wait()\n\t}\n}\n\nfunc assertBiDiCommunication(t *testing.T, node1, node2 *clusterNode, msgToSend *orderer.SubmitRequest) {\n\tassertBiDiCommunicationForChannel(t, node1, node2, msgToSend, testChannel)\n}\n\nfunc assertEventualEstablishStream(t *testing.T, rpc *cluster.RemoteContext) *cluster.Stream {\n\tvar res *cluster.Stream\n\tgt := gomega.NewGomegaWithT(t)\n\tgt.Eventually(func() error {\n\t\tstream, err := rpc.NewStream(time.Hour)\n\t\tres = stream\n\t\treturn err\n\t}, timeout).Should(gomega.Succeed())\n\treturn res\n}\n\nfunc assertEventualSendMessage(t *testing.T, rpc *cluster.RemoteContext, req *orderer.SubmitRequest) orderer.Cluster_StepClient {\n\tvar res orderer.Cluster_StepClient\n\tgt := gomega.NewGomegaWithT(t)\n\tgt.Eventually(func() error {\n\t\tstream, err := rpc.NewStream(time.Hour)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tres = stream\n\t\treturn stream.Send(wrapSubmitReq(req))\n\t}, timeout).Should(gomega.Succeed())\n\treturn res\n}\n\nfunc wrapSubmitReq(req *orderer.SubmitRequest) *orderer.StepRequest {\n\treturn &orderer.StepRequest{\n\t\tPayload: &orderer.StepRequest_SubmitRequest{\n\t\t\tSubmitRequest: req,\n\t\t},\n\t}\n}\n", "/*\nCopyright IBM Corp. 2017 All Rights Reserved.\n\nSPDX-License-Identifier: Apache-2.0\n*/\n\npackage cluster\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"time\"\n\n\t\"github.com/hyperledger/fabric-protos-go/orderer\"\n\t\"github.com/hyperledger/fabric/common/flogging\"\n\t\"github.com/hyperledger/fabric/common/util\"\n\t\"go.uber.org/zap\"\n\t\"google.golang.org/grpc\"\n)\n\n//go:generate mockery -dir . -name Dispatcher -case underscore -output ./mocks/\n\n// Dispatcher dispatches requests\ntype Dispatcher interface {\n\tDispatchSubmit(ctx context.Context, request *orderer.SubmitRequest) error\n\tDispatchConsensus(ctx context.Context, request *orderer.ConsensusRequest) error\n}\n\n//go:generate mockery -dir . -name StepStream -case underscore -output ./mocks/\n\n// StepStream defines the gRPC stream for sending\n// transactions, and receiving corresponding responses\ntype StepStream interface {\n\tSend(response *orderer.StepResponse) error\n\tRecv() (*orderer.StepRequest, error)\n\tgrpc.ServerStream\n}\n\n// Service defines the raft Service\ntype Service struct {\n\tStreamCountReporter              *StreamCountReporter\n\tDispatcher                       Dispatcher\n\tLogger                           *flogging.FabricLogger\n\tStepLogger                       *flogging.FabricLogger\n\tMinimumExpirationWarningInterval time.Duration\n\tCertExpWarningThreshold          time.Duration\n}\n\n// Step passes an implementation-specific message to another cluster member.\nfunc (s *Service) Step(stream orderer.Cluster_StepServer) error {\n\ts.StreamCountReporter.Increment()\n\tdefer s.StreamCountReporter.Decrement()\n\n\taddr := util.ExtractRemoteAddress(stream.Context())\n\tcommonName := commonNameFromContext(stream.Context())\n\texp := s.initializeExpirationCheck(stream, addr, commonName)\n\ts.Logger.Debugf(\"Connection from %s(%s)\", commonName, addr)\n\tdefer s.Logger.Debugf(\"Closing connection from %s(%s)\", commonName, addr)\n\tfor {\n\t\terr := s.handleMessage(stream, addr, exp)\n\t\tif err == io.EOF {\n\t\t\ts.Logger.Debugf(\"%s(%s) disconnected\", commonName, addr)\n\t\t\treturn nil\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// Else, no error occurred, so we continue to the next iteration\n\t}\n}\n\nfunc (s *Service) handleMessage(stream StepStream, addr string, exp *certificateExpirationCheck) error {\n\trequest, err := stream.Recv()\n\tif err == io.EOF {\n\t\treturn err\n\t}\n\tif err != nil {\n\t\ts.Logger.Warningf(\"Stream read from %s failed: %v\", addr, err)\n\t\treturn err\n\t}\n\n\texp.checkExpiration(time.Now(), extractChannel(request))\n\n\tif s.StepLogger.IsEnabledFor(zap.DebugLevel) {\n\t\tnodeName := commonNameFromContext(stream.Context())\n\t\ts.StepLogger.Debugf(\"Received message from %s(%s): %v\", nodeName, addr, requestAsString(request))\n\t}\n\n\tif submitReq := request.GetSubmitRequest(); submitReq != nil {\n\t\tnodeName := commonNameFromContext(stream.Context())\n\t\ts.Logger.Debugf(\"Received message from %s(%s): %v\", nodeName, addr, requestAsString(request))\n\t\treturn s.handleSubmit(submitReq, stream, addr)\n\t}\n\n\t// Else, it's a consensus message.\n\treturn s.Dispatcher.DispatchConsensus(stream.Context(), request.GetConsensusRequest())\n}\n\nfunc (s *Service) handleSubmit(request *orderer.SubmitRequest, stream StepStream, addr string) error {\n\terr := s.Dispatcher.DispatchSubmit(stream.Context(), request)\n\tif err != nil {\n\t\ts.Logger.Warningf(\"Handling of Submit() from %s failed: %v\", addr, err)\n\t\treturn err\n\t}\n\treturn err\n}\n\nfunc (s *Service) initializeExpirationCheck(stream orderer.Cluster_StepServer, endpoint, nodeName string) *certificateExpirationCheck {\n\treturn &certificateExpirationCheck{\n\t\tminimumExpirationWarningInterval: s.MinimumExpirationWarningInterval,\n\t\texpirationWarningThreshold:       s.CertExpWarningThreshold,\n\t\texpiresAt:                        expiresAt(stream),\n\t\tendpoint:                         endpoint,\n\t\tnodeName:                         nodeName,\n\t\talert: func(template string, args ...interface{}) {\n\t\t\ts.Logger.Warningf(template, args...)\n\t\t},\n\t}\n}\n\nfunc expiresAt(stream orderer.Cluster_StepServer) time.Time {\n\tcert := util.ExtractCertificateFromContext(stream.Context())\n\tif cert == nil {\n\t\treturn time.Time{}\n\t}\n\treturn cert.NotAfter\n}\n\nfunc extractChannel(msg *orderer.StepRequest) string {\n\tif consReq := msg.GetConsensusRequest(); consReq != nil {\n\t\treturn consReq.Channel\n\t}\n\n\tif submitReq := msg.GetSubmitRequest(); submitReq != nil {\n\t\treturn submitReq.Channel\n\t}\n\n\treturn \"\"\n}\n"], "fixing_code": ["/*\nCopyright IBM Corp. 2017 All Rights Reserved.\n\nSPDX-License-Identifier: Apache-2.0\n*/\n\npackage cluster_test\n\nimport (\n\t\"context\"\n\t\"crypto/rand\"\n\t\"crypto/x509\"\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/golang/protobuf/proto\"\n\t\"github.com/hyperledger/fabric-protos-go/common\"\n\t\"github.com/hyperledger/fabric-protos-go/orderer\"\n\t\"github.com/hyperledger/fabric/common/crypto\"\n\t\"github.com/hyperledger/fabric/common/crypto/tlsgen\"\n\t\"github.com/hyperledger/fabric/common/flogging\"\n\t\"github.com/hyperledger/fabric/common/metrics\"\n\t\"github.com/hyperledger/fabric/common/metrics/disabled\"\n\t\"github.com/hyperledger/fabric/common/metrics/metricsfakes\"\n\tcomm_utils \"github.com/hyperledger/fabric/internal/pkg/comm\"\n\t\"github.com/hyperledger/fabric/orderer/common/cluster\"\n\t\"github.com/hyperledger/fabric/orderer/common/cluster/mocks\"\n\t\"github.com/onsi/gomega\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/stretchr/testify/mock\"\n\t\"github.com/stretchr/testify/require\"\n\t\"go.uber.org/zap\"\n\t\"go.uber.org/zap/zapcore\"\n\t\"google.golang.org/grpc\"\n)\n\nconst (\n\ttestChannel  = \"test\"\n\ttestChannel2 = \"test2\"\n\ttimeout      = time.Second * 10\n)\n\nvar (\n\t// CA that generates TLS key-pairs.\n\t// We use only one CA because the authentication\n\t// is based on TLS pinning\n\tca = createCAOrPanic()\n\n\tlastNodeID uint64\n\n\ttestSubReq = &orderer.SubmitRequest{\n\t\tChannel: \"test\",\n\t}\n\n\ttestReq = &orderer.SubmitRequest{\n\t\tChannel: \"test\",\n\t\tPayload: &common.Envelope{\n\t\t\tPayload: []byte(\"test\"),\n\t\t},\n\t}\n\n\ttestReq2 = &orderer.SubmitRequest{\n\t\tChannel: testChannel2,\n\t\tPayload: &common.Envelope{\n\t\t\tPayload: []byte(testChannel2),\n\t\t},\n\t}\n\n\ttestRes = &orderer.SubmitResponse{\n\t\tInfo: \"test\",\n\t}\n\n\tfooReq = wrapSubmitReq(&orderer.SubmitRequest{\n\t\tChannel: \"foo\",\n\t})\n\n\tbarReq = wrapSubmitReq(&orderer.SubmitRequest{\n\t\tChannel: \"bar\",\n\t})\n\n\ttestConsensusReq = &orderer.StepRequest{\n\t\tPayload: &orderer.StepRequest_ConsensusRequest{\n\t\t\tConsensusRequest: &orderer.ConsensusRequest{\n\t\t\t\tPayload: []byte{1, 2, 3},\n\t\t\t\tChannel: testChannel,\n\t\t\t},\n\t\t},\n\t}\n\n\tchannelExtractor = &mockChannelExtractor{}\n)\n\nfunc nextUnusedID() uint64 {\n\treturn atomic.AddUint64(&lastNodeID, 1)\n}\n\nfunc createCAOrPanic() tlsgen.CA {\n\tca, err := tlsgen.NewCA()\n\tif err != nil {\n\t\tpanic(fmt.Sprintf(\"failed creating CA: %+v\", err))\n\t}\n\treturn ca\n}\n\ntype mockChannelExtractor struct{}\n\nfunc (*mockChannelExtractor) TargetChannel(msg proto.Message) string {\n\tswitch req := msg.(type) {\n\tcase *orderer.ConsensusRequest:\n\t\treturn req.Channel\n\tcase *orderer.SubmitRequest:\n\t\treturn req.Channel\n\tdefault:\n\t\treturn \"\"\n\t}\n}\n\ntype clusterServer interface {\n\t// Step passes an implementation-specific message to another cluster member.\n\tStep(server orderer.Cluster_StepServer) error\n}\n\ntype clusterNode struct {\n\tlock         sync.Mutex\n\tfrozen       bool\n\tfreezeCond   sync.Cond\n\tdialer       *cluster.PredicateDialer\n\thandler      *mocks.Handler\n\tnodeInfo     cluster.RemoteNode\n\tsrv          *comm_utils.GRPCServer\n\tbindAddress  string\n\tclientConfig comm_utils.ClientConfig\n\tserverConfig comm_utils.ServerConfig\n\tc            *cluster.Comm\n\tdispatcher   clusterServer\n}\n\nfunc (cn *clusterNode) Step(stream orderer.Cluster_StepServer) error {\n\tcn.waitIfFrozen()\n\treq, err := stream.Recv()\n\tif err != nil {\n\t\treturn err\n\t}\n\tif submitReq := req.GetSubmitRequest(); submitReq != nil {\n\t\treturn cn.c.DispatchSubmit(stream.Context(), submitReq)\n\t}\n\tif err := cn.c.DispatchConsensus(stream.Context(), req.GetConsensusRequest()); err != nil {\n\t\treturn err\n\t}\n\treturn stream.Send(&orderer.StepResponse{})\n}\n\nfunc (cn *clusterNode) waitIfFrozen() {\n\tcn.lock.Lock()\n\t// There is no freeze after an unfreeze so no need\n\t// for a for loop.\n\tif cn.frozen {\n\t\tcn.freezeCond.Wait()\n\t\treturn\n\t}\n\tcn.lock.Unlock()\n}\n\nfunc (cn *clusterNode) freeze() {\n\tcn.lock.Lock()\n\tdefer cn.lock.Unlock()\n\tcn.frozen = true\n}\n\nfunc (cn *clusterNode) unfreeze() {\n\tcn.lock.Lock()\n\tcn.frozen = false\n\tcn.lock.Unlock()\n\tcn.freezeCond.Broadcast()\n}\n\nfunc (cn *clusterNode) resurrect() {\n\tgRPCServer, err := comm_utils.NewGRPCServer(cn.bindAddress, cn.serverConfig)\n\tif err != nil {\n\t\tpanic(fmt.Errorf(\"failed starting gRPC server: %v\", err))\n\t}\n\tcn.srv = gRPCServer\n\torderer.RegisterClusterServer(gRPCServer.Server(), cn.dispatcher)\n\tgo cn.srv.Start()\n}\n\nfunc (cn *clusterNode) stop() {\n\tcn.srv.Stop()\n\tcn.c.Shutdown()\n}\n\nfunc (cn *clusterNode) renewCertificates() {\n\tclientKeyPair, err := ca.NewClientCertKeyPair()\n\tif err != nil {\n\t\tpanic(fmt.Errorf(\"failed creating client certificate %v\", err))\n\t}\n\tserverKeyPair, err := ca.NewServerCertKeyPair(\"127.0.0.1\")\n\tif err != nil {\n\t\tpanic(fmt.Errorf(\"failed creating server certificate %v\", err))\n\t}\n\n\tcn.nodeInfo.ClientTLSCert = clientKeyPair.TLSCert.Raw\n\tcn.nodeInfo.ServerTLSCert = serverKeyPair.TLSCert.Raw\n\n\tcn.serverConfig.SecOpts.Certificate = serverKeyPair.Cert\n\tcn.serverConfig.SecOpts.Key = serverKeyPair.Key\n\n\tcn.dialer.Config.SecOpts.Key = clientKeyPair.Key\n\tcn.dialer.Config.SecOpts.Certificate = clientKeyPair.Cert\n}\n\nfunc newTestNodeWithMetrics(t *testing.T, metrics cluster.MetricsProvider, tlsConnGauge metrics.Gauge) *clusterNode {\n\tserverKeyPair, err := ca.NewServerCertKeyPair(\"127.0.0.1\")\n\trequire.NoError(t, err)\n\n\tclientKeyPair, _ := ca.NewClientCertKeyPair()\n\n\thandler := &mocks.Handler{}\n\tclientConfig := comm_utils.ClientConfig{\n\t\tAsyncConnect: true,\n\t\tDialTimeout:  time.Hour,\n\t\tSecOpts: comm_utils.SecureOptions{\n\t\t\tRequireClientCert: true,\n\t\t\tKey:               clientKeyPair.Key,\n\t\t\tCertificate:       clientKeyPair.Cert,\n\t\t\tServerRootCAs:     [][]byte{ca.CertBytes()},\n\t\t\tUseTLS:            true,\n\t\t\tClientRootCAs:     [][]byte{ca.CertBytes()},\n\t\t},\n\t}\n\n\tdialer := &cluster.PredicateDialer{\n\t\tConfig: clientConfig,\n\t}\n\n\tsrvConfig := comm_utils.ServerConfig{\n\t\tSecOpts: comm_utils.SecureOptions{\n\t\t\tKey:         serverKeyPair.Key,\n\t\t\tCertificate: serverKeyPair.Cert,\n\t\t\tUseTLS:      true,\n\t\t},\n\t}\n\tgRPCServer, err := comm_utils.NewGRPCServer(\"127.0.0.1:\", srvConfig)\n\trequire.NoError(t, err)\n\n\ttstSrv := &clusterNode{\n\t\tdialer:       dialer,\n\t\tclientConfig: clientConfig,\n\t\tserverConfig: srvConfig,\n\t\tbindAddress:  gRPCServer.Address(),\n\t\thandler:      handler,\n\t\tnodeInfo: cluster.RemoteNode{\n\t\t\tEndpoint:      gRPCServer.Address(),\n\t\t\tID:            nextUnusedID(),\n\t\t\tServerTLSCert: serverKeyPair.TLSCert.Raw,\n\t\t\tClientTLSCert: clientKeyPair.TLSCert.Raw,\n\t\t},\n\t\tsrv: gRPCServer,\n\t}\n\n\tif tstSrv.dispatcher == nil {\n\t\ttstSrv.dispatcher = tstSrv\n\t}\n\n\ttstSrv.freezeCond.L = &tstSrv.lock\n\n\tcompareCert := cluster.CachePublicKeyComparisons(func(a, b []byte) bool {\n\t\treturn crypto.CertificatesWithSamePublicKey(a, b) == nil\n\t})\n\n\ttstSrv.c = &cluster.Comm{\n\t\tCertExpWarningThreshold: time.Hour,\n\t\tSendBufferSize:          1,\n\t\tLogger:                  flogging.MustGetLogger(\"test\"),\n\t\tChan2Members:            make(cluster.MembersByChannel),\n\t\tH:                       handler,\n\t\tChanExt:                 channelExtractor,\n\t\tConnections:             cluster.NewConnectionStore(dialer, tlsConnGauge),\n\t\tMetrics:                 cluster.NewMetrics(metrics),\n\t\tCompareCertificate:      compareCert,\n\t}\n\n\torderer.RegisterClusterServer(gRPCServer.Server(), tstSrv.dispatcher)\n\tgo gRPCServer.Start()\n\treturn tstSrv\n}\n\nfunc newTestNode(t *testing.T) *clusterNode {\n\treturn newTestNodeWithMetrics(t, &disabled.Provider{}, &disabled.Gauge{})\n}\n\nfunc TestSendBigMessage(t *testing.T) {\n\t// Scenario: Basic test that spawns 5 nodes and sends a big message\n\t// from one of the nodes to the others.\n\t// A receiver node's Step() server side method (which calls Recv)\n\t// is frozen until the sender's node Send method returns,\n\t// Hence - the sender node finishes calling Send\n\t// before a receiver node starts calling Recv.\n\t// This ensures that Send is non blocking even with big messages.\n\t// In the test, we send a total of 8MB of random data (2MB to each node).\n\t// The randomness is used so gRPC compression won't compress it to a lower size.\n\n\tnode1 := newTestNode(t)\n\tnode2 := newTestNode(t)\n\tnode3 := newTestNode(t)\n\tnode4 := newTestNode(t)\n\tnode5 := newTestNode(t)\n\n\tfor _, node := range []*clusterNode{node2, node3, node4, node5} {\n\t\tnode.c.SendBufferSize = 1\n\t}\n\n\tdefer node1.stop()\n\tdefer node2.stop()\n\tdefer node3.stop()\n\tdefer node4.stop()\n\tdefer node5.stop()\n\n\tconfig := []cluster.RemoteNode{node1.nodeInfo, node2.nodeInfo, node3.nodeInfo, node4.nodeInfo, node5.nodeInfo}\n\tnode1.c.Configure(testChannel, config)\n\tnode2.c.Configure(testChannel, config)\n\tnode3.c.Configure(testChannel, config)\n\tnode4.c.Configure(testChannel, config)\n\tnode5.c.Configure(testChannel, config)\n\n\tvar messageReceived sync.WaitGroup\n\tmessageReceived.Add(4)\n\n\tmsgSize := 1024 * 1024 * 2\n\tbigMsg := &orderer.ConsensusRequest{\n\t\tChannel: testChannel,\n\t\tPayload: make([]byte, msgSize),\n\t}\n\n\t_, err := rand.Read(bigMsg.Payload)\n\trequire.NoError(t, err)\n\n\twrappedMsg := &orderer.StepRequest{\n\t\tPayload: &orderer.StepRequest_ConsensusRequest{\n\t\t\tConsensusRequest: bigMsg,\n\t\t},\n\t}\n\n\tfor _, node := range []*clusterNode{node2, node3, node4, node5} {\n\t\tnode.handler.On(\"OnConsensus\", testChannel, node1.nodeInfo.ID, mock.Anything).Run(func(args mock.Arguments) {\n\t\t\tmsg := args.Get(2).(*orderer.ConsensusRequest)\n\t\t\trequire.Len(t, msg.Payload, msgSize)\n\t\t\tmessageReceived.Done()\n\t\t}).Return(nil)\n\t}\n\n\tstreams := map[uint64]*cluster.Stream{}\n\n\tfor _, node := range []*clusterNode{node2, node3, node4, node5} {\n\t\t// Freeze the node, in order to block its Recv\n\t\tnode.freeze()\n\t}\n\n\tfor _, node := range []*clusterNode{node2, node3, node4, node5} {\n\t\trm, err := node1.c.Remote(testChannel, node.nodeInfo.ID)\n\t\trequire.NoError(t, err)\n\n\t\tstream := assertEventualEstablishStream(t, rm)\n\t\tstreams[node.nodeInfo.ID] = stream\n\t}\n\n\tt0 := time.Now()\n\tfor _, node := range []*clusterNode{node2, node3, node4, node5} {\n\t\tstream := streams[node.nodeInfo.ID]\n\n\t\tt1 := time.Now()\n\t\terr = stream.Send(wrappedMsg)\n\t\trequire.NoError(t, err)\n\t\tt.Log(\"Sending took\", time.Since(t1))\n\n\t\t// Unfreeze the node. It can now call Recv, and signal the messageReceived waitGroup.\n\t\tnode.unfreeze()\n\t}\n\n\tt.Log(\"Total sending time to all 4 nodes took:\", time.Since(t0))\n\n\tmessageReceived.Wait()\n}\n\nfunc TestBlockingSend(t *testing.T) {\n\t// Scenario: Basic test that spawns 2 nodes and sends from the first node\n\t// to the second node, three SubmitRequests, or three consensus requests.\n\t// SubmitRequests should block, but consensus requests should not.\n\n\tfor _, testCase := range []struct {\n\t\tdescription        string\n\t\tmessageToSend      *orderer.StepRequest\n\t\tstreamUnblocks     bool\n\t\telapsedGreaterThan time.Duration\n\t\toverflowErr        string\n\t}{\n\t\t{\n\t\t\tdescription:        \"SubmitRequest\",\n\t\t\tmessageToSend:      wrapSubmitReq(testReq),\n\t\t\tstreamUnblocks:     true,\n\t\t\telapsedGreaterThan: time.Second / 2,\n\t\t},\n\t\t{\n\t\t\tdescription:   \"ConsensusRequest\",\n\t\t\tmessageToSend: testConsensusReq,\n\t\t\toverflowErr:   \"send queue overflown\",\n\t\t},\n\t} {\n\t\tt.Run(testCase.description, func(t *testing.T) {\n\t\t\tnode1 := newTestNode(t)\n\t\t\tnode2 := newTestNode(t)\n\n\t\t\tnode1.c.SendBufferSize = 1\n\t\t\tnode2.c.SendBufferSize = 1\n\n\t\t\tdefer node1.stop()\n\t\t\tdefer node2.stop()\n\n\t\t\tconfig := []cluster.RemoteNode{node1.nodeInfo, node2.nodeInfo}\n\t\t\tnode1.c.Configure(testChannel, config)\n\t\t\tnode2.c.Configure(testChannel, config)\n\n\t\t\trm, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tclient := &mocks.ClusterClient{}\n\t\t\tfakeStream := &mocks.StepClient{}\n\n\t\t\t// Replace real client with a mock client\n\t\t\trm.Client = client\n\t\t\trm.ProbeConn = func(_ *grpc.ClientConn) error {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\t// Configure client to return the mock stream\n\t\t\tfakeStream.On(\"Context\", mock.Anything).Return(context.Background())\n\t\t\tclient.On(\"Step\", mock.Anything).Return(fakeStream, nil).Once()\n\n\t\t\tunBlock := make(chan struct{})\n\t\t\tvar sendInvoked sync.WaitGroup\n\t\t\tsendInvoked.Add(1)\n\t\t\tvar once sync.Once\n\t\t\tfakeStream.On(\"Send\", mock.Anything).Run(func(_ mock.Arguments) {\n\t\t\t\tonce.Do(sendInvoked.Done)\n\t\t\t\t<-unBlock\n\t\t\t}).Return(errors.New(\"oops\"))\n\n\t\t\tstream, err := rm.NewStream(time.Hour)\n\t\t\trequire.NoError(t, err)\n\n\t\t\t// The first send doesn't block, even though the Send operation blocks.\n\t\t\terr = stream.Send(testCase.messageToSend)\n\t\t\trequire.NoError(t, err)\n\n\t\t\t// The second once doesn't either.\n\t\t\t// After this point, we have 1 goroutine which is blocked on Send(),\n\t\t\t// and one message in the buffer.\n\t\t\tsendInvoked.Wait()\n\t\t\terr = stream.Send(testCase.messageToSend)\n\t\t\trequire.NoError(t, err)\n\n\t\t\t// The third blocks, so we need to unblock it ourselves\n\t\t\t// in order for it to go through, unless the operation\n\t\t\t// is non blocking.\n\t\t\tgo func() {\n\t\t\t\ttime.Sleep(time.Second)\n\t\t\t\tif testCase.streamUnblocks {\n\t\t\t\t\tclose(unBlock)\n\t\t\t\t}\n\t\t\t}()\n\n\t\t\tt1 := time.Now()\n\t\t\terr = stream.Send(testCase.messageToSend)\n\t\t\t// The third send always overflows or blocks.\n\t\t\t// If we expect to receive an overflow error - assert it.\n\t\t\tif testCase.overflowErr != \"\" {\n\t\t\t\trequire.EqualError(t, err, testCase.overflowErr)\n\t\t\t}\n\t\t\telapsed := time.Since(t1)\n\t\t\tt.Log(\"Elapsed time:\", elapsed)\n\t\t\trequire.True(t, elapsed > testCase.elapsedGreaterThan)\n\n\t\t\tif !testCase.streamUnblocks {\n\t\t\t\tclose(unBlock)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestEmptyRequest(t *testing.T) {\n\t// Scenario: Ensures empty messages are discarded and an error is returned\n\t// back to the sender.\n\n\tnode1 := newTestNode(t)\n\tnode2 := newTestNode(t)\n\n\tnode2.srv.Stop()\n\tsvc := &cluster.Service{\n\t\tStepLogger: flogging.MustGetLogger(\"test\"),\n\t\tLogger:     flogging.MustGetLogger(\"test\"),\n\t\tStreamCountReporter: &cluster.StreamCountReporter{\n\t\t\tMetrics: cluster.NewMetrics(&disabled.Provider{}),\n\t\t},\n\t\tDispatcher: node2.c,\n\t}\n\tnode2.dispatcher = svc\n\n\t// Sleep to let the gRPC service be closed\n\ttime.Sleep(time.Second)\n\n\t// Resurrect the node with the new dispatcher\n\tnode2.resurrect()\n\n\tdefer node1.stop()\n\tdefer node2.stop()\n\n\tconfig := []cluster.RemoteNode{node1.nodeInfo, node2.nodeInfo}\n\tnode1.c.Configure(testChannel, config)\n\tnode2.c.Configure(testChannel, config)\n\n\tassertBiDiCommunication(t, node1, node2, testReq)\n\n\trm, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\n\tstream, err := rm.NewStream(time.Second * 10)\n\trequire.NoError(t, err)\n\n\terr = stream.Send(&orderer.StepRequest{})\n\trequire.NoError(t, err)\n\n\t_, err = stream.Recv()\n\trequire.Error(t, err, \"message is neither a Submit nor a Consensus request\")\n}\n\nfunc TestBasic(t *testing.T) {\n\t// Scenario: Basic test that spawns 2 nodes and sends each other\n\t// messages that are expected to be echoed back\n\n\tnode1 := newTestNode(t)\n\tnode2 := newTestNode(t)\n\n\tdefer node1.stop()\n\tdefer node2.stop()\n\n\tconfig := []cluster.RemoteNode{node1.nodeInfo, node2.nodeInfo}\n\tnode1.c.Configure(testChannel, config)\n\tnode2.c.Configure(testChannel, config)\n\n\tassertBiDiCommunication(t, node1, node2, testReq)\n}\n\nfunc TestUnavailableHosts(t *testing.T) {\n\t// Scenario: A node is configured to connect\n\t// to a host that is down\n\tnode1 := newTestNode(t)\n\n\tclientConfig := node1.dialer.Config\n\t// The below timeout makes sure that connection establishment is done\n\t// asynchronously. Had it been synchronous, the Remote() call would be\n\t// blocked for an hour.\n\tclientConfig.DialTimeout = time.Hour\n\tdefer node1.stop()\n\n\tnode2 := newTestNode(t)\n\tnode2.stop()\n\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node2.nodeInfo})\n\tremote, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\trequire.NotNil(t, remote)\n\n\t_, err = remote.NewStream(time.Millisecond * 100)\n\trequire.Contains(t, err.Error(), \"connection\")\n}\n\nfunc TestStreamAbortReportCorrectError(t *testing.T) {\n\t// Scenario: node 1 acquires a stream to node 2 and then the stream\n\t// encounters an error and as a result, the stream is aborted.\n\t// We ensure the error reported is the first error, even after\n\t// multiple attempts of using it.\n\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\n\tnode2 := newTestNode(t)\n\tdefer node2.stop()\n\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node2.nodeInfo})\n\tnode2.c.Configure(testChannel, []cluster.RemoteNode{node1.nodeInfo})\n\n\tnode2.handler.On(\"OnSubmit\", testChannel, node1.nodeInfo.ID, mock.Anything).Return(errors.Errorf(\"whoops\")).Once()\n\n\trm1, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\tvar streamTerminated sync.WaitGroup\n\tstreamTerminated.Add(1)\n\n\tstream := assertEventualEstablishStream(t, rm1)\n\n\tl, err := zap.NewDevelopment()\n\trequire.NoError(t, err)\n\tstream.Logger = flogging.NewFabricLogger(l, zap.Hooks(func(entry zapcore.Entry) error {\n\t\tif strings.Contains(entry.Message, \"Stream 1 to\") && strings.Contains(entry.Message, \"terminated\") {\n\t\t\tstreamTerminated.Done()\n\t\t}\n\t\treturn nil\n\t}))\n\n\t// Probe the stream for the first time\n\terr = stream.Send(wrapSubmitReq(testReq))\n\trequire.NoError(t, err)\n\n\t// We should receive back the crafted error\n\t_, err = stream.Recv()\n\trequire.Contains(t, err.Error(), \"whoops\")\n\n\t// Wait for the stream to be terminated from within the communication infrastructure\n\tstreamTerminated.Wait()\n\n\t// We should still receive the original crafted error despite the stream being terminated\n\terr = stream.Send(wrapSubmitReq(testReq))\n\trequire.Contains(t, err.Error(), \"whoops\")\n}\n\nfunc TestStreamAbort(t *testing.T) {\n\t// Scenarios: node 1 is connected to node 2 in 2 channels,\n\t// and the consumer of the communication calls receive.\n\t// The two sub-scenarios happen:\n\t// 1) The server certificate of node 2 changes in the first channel\n\t// 2) Node 2 is evicted from the membership of the first channel\n\t// In both of the scenarios, the Recv() call should be aborted\n\n\tnode2 := newTestNode(t)\n\tdefer node2.stop()\n\n\tinvalidNodeInfo := cluster.RemoteNode{\n\t\tID:            node2.nodeInfo.ID,\n\t\tServerTLSCert: []byte{1, 2, 3},\n\t\tClientTLSCert: []byte{1, 2, 3},\n\t}\n\n\tfor _, tst := range []struct {\n\t\ttestName      string\n\t\tmembership    []cluster.RemoteNode\n\t\texpectedError string\n\t}{\n\t\t{\n\t\t\ttestName:      \"Evicted from membership\",\n\t\t\tmembership:    nil,\n\t\t\texpectedError: \"rpc error: code = Canceled desc = context canceled\",\n\t\t},\n\t\t{\n\t\t\ttestName:      \"Changed TLS certificate\",\n\t\t\tmembership:    []cluster.RemoteNode{invalidNodeInfo},\n\t\t\texpectedError: \"rpc error: code = Canceled desc = context canceled\",\n\t\t},\n\t} {\n\t\tt.Run(tst.testName, func(t *testing.T) {\n\t\t\ttestStreamAbort(t, node2, tst.membership, tst.expectedError)\n\t\t})\n\t}\n\tnode2.handler.AssertNumberOfCalls(t, \"OnSubmit\", 2)\n}\n\nfunc testStreamAbort(t *testing.T, node2 *clusterNode, newMembership []cluster.RemoteNode, expectedError string) {\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node2.nodeInfo})\n\tnode2.c.Configure(testChannel, []cluster.RemoteNode{node1.nodeInfo})\n\tnode1.c.Configure(testChannel2, []cluster.RemoteNode{node2.nodeInfo})\n\tnode2.c.Configure(testChannel2, []cluster.RemoteNode{node1.nodeInfo})\n\n\tvar streamCreated sync.WaitGroup\n\tstreamCreated.Add(1)\n\n\tstopChan := make(chan struct{})\n\n\tnode2.handler.On(\"OnSubmit\", testChannel, node1.nodeInfo.ID, mock.Anything).Once().Run(func(_ mock.Arguments) {\n\t\t// Notify the stream was created\n\t\tstreamCreated.Done()\n\t\t// Wait for the test to finish\n\t\t<-stopChan\n\t}).Return(nil).Once()\n\n\trm1, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\n\tgo func() {\n\t\tstream := assertEventualEstablishStream(t, rm1)\n\t\t// Signal the reconfiguration\n\t\terr = stream.Send(wrapSubmitReq(testReq))\n\t\trequire.NoError(t, err)\n\t\t_, err := stream.Recv()\n\t\trequire.Contains(t, err.Error(), expectedError)\n\t\tclose(stopChan)\n\t}()\n\n\tgo func() {\n\t\t// Wait for the stream reference to be obtained\n\t\tstreamCreated.Wait()\n\t\t// Reconfigure the channel membership\n\t\tnode1.c.Configure(testChannel, newMembership)\n\t}()\n\n\t<-stopChan\n}\n\nfunc TestDoubleReconfigure(t *testing.T) {\n\t// Scenario: Basic test that spawns 2 nodes\n\t// and configures node 1 twice, and checks that\n\t// the remote stub for node 1 wasn't re-created in the second\n\t// configuration since it already existed\n\n\tnode1 := newTestNode(t)\n\tnode2 := newTestNode(t)\n\n\tdefer node1.stop()\n\tdefer node2.stop()\n\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node2.nodeInfo})\n\trm1, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node2.nodeInfo})\n\trm2, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\t// Ensure the references are equal\n\trequire.True(t, rm1 == rm2)\n}\n\nfunc TestInvalidChannel(t *testing.T) {\n\t// Scenario: node 1 it ordered to send a message on a channel\n\t// that doesn't exist, and also receives a message, but\n\t// the channel cannot be extracted from the message.\n\n\tt.Run(\"channel doesn't exist\", func(t *testing.T) {\n\t\tnode1 := newTestNode(t)\n\t\tdefer node1.stop()\n\n\t\t_, err := node1.c.Remote(testChannel, 0)\n\t\trequire.EqualError(t, err, \"channel test doesn't exist\")\n\t})\n\n\tt.Run(\"channel cannot be extracted\", func(t *testing.T) {\n\t\tnode1 := newTestNode(t)\n\t\tdefer node1.stop()\n\n\t\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node1.nodeInfo})\n\t\tgt := gomega.NewGomegaWithT(t)\n\t\tgt.Eventually(func() (bool, error) {\n\t\t\t_, err := node1.c.Remote(testChannel, node1.nodeInfo.ID)\n\t\t\treturn true, err\n\t\t}, time.Minute).Should(gomega.BeTrue())\n\n\t\tstub, err := node1.c.Remote(testChannel, node1.nodeInfo.ID)\n\t\trequire.NoError(t, err)\n\n\t\tstream := assertEventualEstablishStream(t, stub)\n\n\t\t// An empty SubmitRequest has an empty channel which is invalid\n\t\terr = stream.Send(wrapSubmitReq(&orderer.SubmitRequest{}))\n\t\trequire.NoError(t, err)\n\n\t\t_, err = stream.Recv()\n\t\trequire.EqualError(t, err, \"rpc error: code = Unknown desc = badly formatted message, cannot extract channel\")\n\n\t\t// Test directly without going through the gRPC stream\n\t\terr = node1.c.DispatchSubmit(context.Background(), &orderer.SubmitRequest{})\n\t\trequire.EqualError(t, err, \"badly formatted message, cannot extract channel\")\n\t})\n}\n\nfunc TestAbortRPC(t *testing.T) {\n\t// Scenarios:\n\t// (I) The node calls an RPC, and calls Abort() on the remote context\n\t//  in parallel. The RPC should return even though the server-side call hasn't finished.\n\t// (II) The node calls an RPC, but the server-side processing takes too long,\n\t// and the RPC invocation returns prematurely.\n\n\ttestCases := []struct {\n\t\tname        string\n\t\tabortFunc   func(*cluster.RemoteContext)\n\t\trpcTimeout  time.Duration\n\t\texpectedErr string\n\t}{\n\t\t{\n\t\t\tname:        \"Abort() called\",\n\t\t\texpectedErr: \"rpc error: code = Canceled desc = context canceled\",\n\t\t\trpcTimeout:  time.Hour,\n\t\t\tabortFunc: func(rc *cluster.RemoteContext) {\n\t\t\t\trc.Abort()\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:        \"RPC timeout\",\n\t\t\texpectedErr: \"rpc timeout expired\",\n\t\t\trpcTimeout:  time.Second,\n\t\t\tabortFunc:   func(*cluster.RemoteContext) {},\n\t\t},\n\t}\n\n\tfor _, testCase := range testCases {\n\t\ttestCase := testCase\n\t\tt.Run(testCase.name, func(t *testing.T) {\n\t\t\ttestAbort(t, testCase.abortFunc, testCase.rpcTimeout, testCase.expectedErr)\n\t\t})\n\t}\n}\n\nfunc testAbort(t *testing.T, abortFunc func(*cluster.RemoteContext), rpcTimeout time.Duration, expectedErr string) {\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\n\tnode2 := newTestNode(t)\n\tdefer node2.stop()\n\n\tconfig := []cluster.RemoteNode{node1.nodeInfo, node2.nodeInfo}\n\tnode1.c.Configure(testChannel, config)\n\tnode2.c.Configure(testChannel, config)\n\tvar onStepCalled sync.WaitGroup\n\tonStepCalled.Add(1)\n\n\t// stuckCall ensures the OnStep() call is stuck throughout this test\n\tvar stuckCall sync.WaitGroup\n\tstuckCall.Add(1)\n\t// At the end of the test, release the server-side resources\n\tdefer stuckCall.Done()\n\n\tnode2.handler.On(\"OnSubmit\", testChannel, node1.nodeInfo.ID, mock.Anything).Return(nil).Once().Run(func(_ mock.Arguments) {\n\t\tonStepCalled.Done()\n\t\tstuckCall.Wait()\n\t}).Once()\n\n\trm, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\n\tgo func() {\n\t\tonStepCalled.Wait()\n\t\tabortFunc(rm)\n\t}()\n\n\tvar stream *cluster.Stream\n\tgt := gomega.NewGomegaWithT(t)\n\tgt.Eventually(func() error {\n\t\tstream, err = rm.NewStream(rpcTimeout)\n\t\treturn err\n\t}, time.Second*10, time.Millisecond*10).Should(gomega.Succeed())\n\n\tstream.Send(wrapSubmitReq(testSubReq))\n\t_, err = stream.Recv()\n\n\trequire.EqualError(t, err, expectedErr)\n\n\tnode2.handler.AssertNumberOfCalls(t, \"OnSubmit\", 1)\n}\n\nfunc TestNoTLSCertificate(t *testing.T) {\n\t// Scenario: The node is sent a message by another node that doesn't\n\t// connect with mutual TLS, thus doesn't provide a TLS certificate\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node1.nodeInfo})\n\n\tclientConfig := comm_utils.ClientConfig{\n\t\tAsyncConnect: true,\n\t\tDialTimeout:  time.Millisecond * 100,\n\t\tSecOpts: comm_utils.SecureOptions{\n\t\t\tServerRootCAs: [][]byte{ca.CertBytes()},\n\t\t\tUseTLS:        true,\n\t\t},\n\t}\n\n\tvar conn *grpc.ClientConn\n\tgt := gomega.NewGomegaWithT(t)\n\tgt.Eventually(func() (bool, error) {\n\t\tvar err error\n\t\tconn, err = clientConfig.Dial(node1.srv.Address())\n\t\treturn true, err\n\t}, time.Minute).Should(gomega.BeTrue())\n\n\techoClient := orderer.NewClusterClient(conn)\n\tstream, err := echoClient.Step(context.Background())\n\trequire.NoError(t, err)\n\n\terr = stream.Send(wrapSubmitReq(testSubReq))\n\trequire.NoError(t, err)\n\t_, err = stream.Recv()\n\trequire.EqualError(t, err, \"rpc error: code = Unknown desc = no TLS certificate sent\")\n}\n\nfunc TestReconnect(t *testing.T) {\n\t// Scenario: node 1 and node 2 are connected,\n\t// and node 2 is taken offline.\n\t// Node 1 tries to send a message to node 2 but fails,\n\t// and afterwards node 2 is brought back, after which\n\t// node 1 sends more messages, and it should succeed\n\t// sending a message to node 2 eventually.\n\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\tconf := node1.dialer.Config\n\tconf.DialTimeout = time.Hour\n\n\tnode2 := newTestNode(t)\n\tnode2.handler.On(\"OnSubmit\", testChannel, node1.nodeInfo.ID, mock.Anything).Return(nil)\n\tdefer node2.stop()\n\n\tconfig := []cluster.RemoteNode{node1.nodeInfo, node2.nodeInfo}\n\tnode1.c.Configure(testChannel, config)\n\tnode2.c.Configure(testChannel, config)\n\n\t// Make node 2 be offline by shutting down its gRPC service\n\tnode2.srv.Stop()\n\t// Obtain the stub for node 2.\n\t// Should succeed, because the connection was created at time of configuration\n\tstub, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\n\t// Try to obtain a stream. Should not Succeed.\n\tgt := gomega.NewGomegaWithT(t)\n\tgt.Eventually(func() error {\n\t\t_, err = stub.NewStream(time.Hour)\n\t\treturn err\n\t}).Should(gomega.Not(gomega.Succeed()))\n\n\t// Wait for the port to be released\n\tfor {\n\t\tlsnr, err := net.Listen(\"tcp\", node2.nodeInfo.Endpoint)\n\t\tif err == nil {\n\t\t\tlsnr.Close()\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// Resurrect node 2\n\tnode2.resurrect()\n\t// Send a message from node 1 to node 2.\n\t// Should succeed eventually\n\tassertEventualSendMessage(t, stub, testReq)\n}\n\nfunc TestRenewCertificates(t *testing.T) {\n\t// Scenario: node 1 and node 2 are connected,\n\t// Node 2's certificate is renewed, and\n\t// node 1 is reconfigured with the new\n\t// configuration without being restarted.\n\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\n\tnode2 := newTestNode(t)\n\tdefer node2.stop()\n\n\tnode1.handler.On(\"OnStep\", testChannel, node2.nodeInfo.ID, mock.Anything).Return(testRes, nil)\n\tnode2.handler.On(\"OnStep\", testChannel, node1.nodeInfo.ID, mock.Anything).Return(testRes, nil)\n\n\tconfig := []cluster.RemoteNode{node1.nodeInfo, node2.nodeInfo}\n\tnode1.c.Configure(testChannel, config)\n\tnode2.c.Configure(testChannel, config)\n\n\tassertBiDiCommunication(t, node1, node2, testReq)\n\n\t// Close outgoing connections from node2 to node1\n\tnode2.c.Configure(testChannel, nil)\n\t// Stop the gRPC service of node 2 to replace its certificate\n\tnode2.srv.Stop()\n\n\t// Wait until node 1 detects this\n\tgt := gomega.NewGomegaWithT(t)\n\tgt.Eventually(func() error {\n\t\tremote, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tstream, err := remote.NewStream(time.Hour)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\terr = stream.Send(wrapSubmitReq(testSubReq))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t}).Should(gomega.Not(gomega.Succeed()))\n\n\t// Renew node 2's keys\n\tnode2.renewCertificates()\n\n\t// Resurrect node 2 to make it service connections again\n\tnode2.resurrect()\n\n\t// W.L.O.G, try to send a message from node1 to node2\n\t// It should fail, because node2's server certificate has now changed,\n\t// so it closed the connection to the remote node\n\tinfo2 := node2.nodeInfo\n\tremote, err := node1.c.Remote(testChannel, info2.ID)\n\trequire.NoError(t, err)\n\trequire.NotNil(t, remote)\n\t_, err = remote.NewStream(time.Hour)\n\trequire.Contains(t, err.Error(), info2.Endpoint)\n\n\t// Reconfigure both nodes with the updates keys\n\tconfig = []cluster.RemoteNode{node1.nodeInfo, node2.nodeInfo}\n\tnode1.c.Configure(testChannel, config)\n\tnode2.c.Configure(testChannel, config)\n\n\t// Finally, check that the nodes can communicate once again\n\tassertBiDiCommunication(t, node1, node2, testReq)\n}\n\nfunc TestMembershipReconfiguration(t *testing.T) {\n\t// Scenario: node 1 and node 2 are started up\n\t// and node 2 is configured to know about node 1,\n\t// without node1 knowing about node 2.\n\t// The communication between them should only work\n\t// after node 1 is configured to know about node 2.\n\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\n\tnode2 := newTestNode(t)\n\tdefer node2.stop()\n\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{})\n\tnode2.c.Configure(testChannel, []cluster.RemoteNode{node1.nodeInfo})\n\n\t// Node 1 can't connect to node 2 because it doesn't know its TLS certificate yet\n\t_, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.EqualError(t, err, fmt.Sprintf(\"node %d doesn't exist in channel test's membership\", node2.nodeInfo.ID))\n\t// Node 2 can connect to node 1, but it can't send it messages because node 1 doesn't know node 2 yet.\n\n\tgt := gomega.NewGomegaWithT(t)\n\tgt.Eventually(func() (bool, error) {\n\t\t_, err := node2.c.Remote(testChannel, node1.nodeInfo.ID)\n\t\treturn true, err\n\t}, time.Minute).Should(gomega.BeTrue())\n\n\tstub, err := node2.c.Remote(testChannel, node1.nodeInfo.ID)\n\trequire.NoError(t, err)\n\n\tstream := assertEventualEstablishStream(t, stub)\n\terr = stream.Send(wrapSubmitReq(testSubReq))\n\trequire.NoError(t, err)\n\n\t_, err = stream.Recv()\n\trequire.EqualError(t, err, \"rpc error: code = Unknown desc = certificate extracted from TLS connection isn't authorized\")\n\n\t// Next, configure node 1 to know about node 2\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node2.nodeInfo})\n\n\t// Check that the communication works correctly between both nodes\n\tassertBiDiCommunication(t, node1, node2, testReq)\n\tassertBiDiCommunication(t, node2, node1, testReq)\n\n\t// Reconfigure node 2 to forget about node 1\n\tnode2.c.Configure(testChannel, []cluster.RemoteNode{})\n\t// Node 1 can still connect to node 2\n\tstub, err = node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\t// But can't send a message because node 2 now doesn't authorized node 1\n\tstream = assertEventualEstablishStream(t, stub)\n\tstream.Send(wrapSubmitReq(testSubReq))\n\t_, err = stream.Recv()\n\trequire.EqualError(t, err, \"rpc error: code = Unknown desc = certificate extracted from TLS connection isn't authorized\")\n}\n\nfunc TestShutdown(t *testing.T) {\n\t// Scenario: node 1 is shut down and as a result, can't\n\t// send messages to anyone, nor can it be reconfigured\n\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\n\tnode1.c.Shutdown()\n\n\t// Obtaining a RemoteContext cannot succeed because shutdown was called before\n\t_, err := node1.c.Remote(testChannel, node1.nodeInfo.ID)\n\trequire.EqualError(t, err, \"communication has been shut down\")\n\n\tnode2 := newTestNode(t)\n\tdefer node2.stop()\n\n\tnode2.c.Configure(testChannel, []cluster.RemoteNode{node1.nodeInfo})\n\t// Configuration of node doesn't take place\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node2.nodeInfo})\n\n\tgt := gomega.NewGomegaWithT(t)\n\tgt.Eventually(func() error {\n\t\t_, err := node2.c.Remote(testChannel, node1.nodeInfo.ID)\n\t\treturn err\n\t}, time.Minute).Should(gomega.Succeed())\n\n\tstub, err := node2.c.Remote(testChannel, node1.nodeInfo.ID)\n\trequire.NoError(t, err)\n\n\t// Therefore, sending a message doesn't succeed because node 1 rejected the configuration change\n\tgt.Eventually(func() string {\n\t\tstream, err := stub.NewStream(time.Hour)\n\t\tif err != nil {\n\t\t\treturn err.Error()\n\t\t}\n\t\terr = stream.Send(wrapSubmitReq(testSubReq))\n\t\trequire.NoError(t, err)\n\n\t\t_, err = stream.Recv()\n\t\treturn err.Error()\n\t}, timeout).Should(gomega.ContainSubstring(\"channel test doesn't exist\"))\n}\n\nfunc TestMultiChannelConfig(t *testing.T) {\n\t// Scenario: node 1 is knows node 2 only in channel \"foo\"\n\t// and knows node 3 only in channel \"bar\".\n\t// Messages that are received, are routed according to their corresponding channels\n\t// and when node 2 sends a message for channel \"bar\" to node 1, it is rejected.\n\t// Same thing applies for node 3 that sends a message to node 1 in channel \"foo\".\n\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\n\tnode2 := newTestNode(t)\n\tdefer node2.stop()\n\n\tnode3 := newTestNode(t)\n\tdefer node3.stop()\n\n\tnode1.c.Configure(\"foo\", []cluster.RemoteNode{node2.nodeInfo})\n\tnode1.c.Configure(\"bar\", []cluster.RemoteNode{node3.nodeInfo})\n\tnode2.c.Configure(\"foo\", []cluster.RemoteNode{node1.nodeInfo})\n\tnode3.c.Configure(\"bar\", []cluster.RemoteNode{node1.nodeInfo})\n\n\tt.Run(\"Correct channel\", func(t *testing.T) {\n\t\tvar fromNode2 sync.WaitGroup\n\t\tfromNode2.Add(1)\n\t\tnode1.handler.On(\"OnSubmit\", \"foo\", node2.nodeInfo.ID, mock.Anything).Return(nil).Run(func(_ mock.Arguments) {\n\t\t\tfromNode2.Done()\n\t\t}).Once()\n\n\t\tvar fromNode3 sync.WaitGroup\n\t\tfromNode3.Add(1)\n\t\tnode1.handler.On(\"OnSubmit\", \"bar\", node3.nodeInfo.ID, mock.Anything).Return(nil).Run(func(_ mock.Arguments) {\n\t\t\tfromNode3.Done()\n\t\t}).Once()\n\n\t\tnode2toNode1, err := node2.c.Remote(\"foo\", node1.nodeInfo.ID)\n\t\trequire.NoError(t, err)\n\t\tnode3toNode1, err := node3.c.Remote(\"bar\", node1.nodeInfo.ID)\n\t\trequire.NoError(t, err)\n\n\t\tstream := assertEventualEstablishStream(t, node2toNode1)\n\t\tstream.Send(fooReq)\n\n\t\tfromNode2.Wait()\n\t\tnode1.handler.AssertNumberOfCalls(t, \"OnSubmit\", 1)\n\n\t\tstream = assertEventualEstablishStream(t, node3toNode1)\n\t\tstream.Send(barReq)\n\n\t\tfromNode3.Wait()\n\t\tnode1.handler.AssertNumberOfCalls(t, \"OnSubmit\", 2)\n\t})\n\n\tt.Run(\"Incorrect channel\", func(t *testing.T) {\n\t\tnode1.handler.On(\"OnSubmit\", \"foo\", node2.nodeInfo.ID, mock.Anything).Return(nil)\n\t\tnode1.handler.On(\"OnSubmit\", \"bar\", node3.nodeInfo.ID, mock.Anything).Return(nil)\n\n\t\tnode2toNode1, err := node2.c.Remote(\"foo\", node1.nodeInfo.ID)\n\t\trequire.NoError(t, err)\n\t\tnode3toNode1, err := node3.c.Remote(\"bar\", node1.nodeInfo.ID)\n\t\trequire.NoError(t, err)\n\n\t\tassertEventualSendMessage(t, node2toNode1, &orderer.SubmitRequest{Channel: \"foo\"})\n\t\trequire.NoError(t, err)\n\t\tstream, err := node2toNode1.NewStream(time.Hour)\n\t\trequire.NoError(t, err)\n\t\terr = stream.Send(barReq)\n\t\trequire.NoError(t, err)\n\t\t_, err = stream.Recv()\n\t\trequire.EqualError(t, err, \"rpc error: code = Unknown desc = certificate extracted from TLS connection isn't authorized\")\n\n\t\tassertEventualSendMessage(t, node3toNode1, &orderer.SubmitRequest{Channel: \"bar\"})\n\t\tstream, err = node3toNode1.NewStream(time.Hour)\n\t\trequire.NoError(t, err)\n\t\terr = stream.Send(fooReq)\n\t\trequire.NoError(t, err)\n\t\t_, err = stream.Recv()\n\t\trequire.EqualError(t, err, \"rpc error: code = Unknown desc = certificate extracted from TLS connection isn't authorized\")\n\t})\n}\n\nfunc TestConnectionFailure(t *testing.T) {\n\t// Scenario: node 1 fails to connect to node 2.\n\n\tnode1 := newTestNode(t)\n\tdefer node1.stop()\n\n\tnode2 := newTestNode(t)\n\tdefer node2.stop()\n\n\tdialer := &mocks.SecureDialer{}\n\tdialer.On(\"Dial\", mock.Anything, mock.Anything).Return(nil, errors.New(\"oops\"))\n\tnode1.c.Connections = cluster.NewConnectionStore(dialer, &disabled.Gauge{})\n\tnode1.c.Configure(testChannel, []cluster.RemoteNode{node2.nodeInfo})\n\n\t_, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.EqualError(t, err, \"oops\")\n}\n\ntype testMetrics struct {\n\tfakeProvider        *mocks.MetricsProvider\n\tegressQueueLength   metricsfakes.Gauge\n\tegressQueueCapacity metricsfakes.Gauge\n\tegressStreamCount   metricsfakes.Gauge\n\tegressTLSConnCount  metricsfakes.Gauge\n\tegressWorkerSize    metricsfakes.Gauge\n\tingressStreamsCount metricsfakes.Gauge\n\tmsgSendTime         metricsfakes.Histogram\n\tmsgDropCount        metricsfakes.Counter\n}\n\nfunc (tm *testMetrics) initialize() {\n\ttm.egressQueueLength.WithReturns(&tm.egressQueueLength)\n\ttm.egressQueueCapacity.WithReturns(&tm.egressQueueCapacity)\n\ttm.egressStreamCount.WithReturns(&tm.egressStreamCount)\n\ttm.egressTLSConnCount.WithReturns(&tm.egressTLSConnCount)\n\ttm.egressWorkerSize.WithReturns(&tm.egressWorkerSize)\n\ttm.ingressStreamsCount.WithReturns(&tm.ingressStreamsCount)\n\ttm.msgSendTime.WithReturns(&tm.msgSendTime)\n\ttm.msgDropCount.WithReturns(&tm.msgDropCount)\n\n\tfakeProvider := tm.fakeProvider\n\tfakeProvider.On(\"NewGauge\", cluster.IngressStreamsCountOpts).Return(&tm.ingressStreamsCount)\n\tfakeProvider.On(\"NewGauge\", cluster.EgressQueueLengthOpts).Return(&tm.egressQueueLength)\n\tfakeProvider.On(\"NewGauge\", cluster.EgressQueueCapacityOpts).Return(&tm.egressQueueCapacity)\n\tfakeProvider.On(\"NewGauge\", cluster.EgressStreamsCountOpts).Return(&tm.egressStreamCount)\n\tfakeProvider.On(\"NewGauge\", cluster.EgressTLSConnectionCountOpts).Return(&tm.egressTLSConnCount)\n\tfakeProvider.On(\"NewGauge\", cluster.EgressWorkersOpts).Return(&tm.egressWorkerSize)\n\tfakeProvider.On(\"NewCounter\", cluster.MessagesDroppedCountOpts).Return(&tm.msgDropCount)\n\tfakeProvider.On(\"NewHistogram\", cluster.MessageSendTimeOpts).Return(&tm.msgSendTime)\n}\n\nfunc TestMetrics(t *testing.T) {\n\tfor _, testCase := range []struct {\n\t\tname        string\n\t\trunTest     func(t *testing.T, node1, node2 *clusterNode, testMetrics *testMetrics)\n\t\ttestMetrics *testMetrics\n\t}{\n\t\t{\n\t\t\tname: \"EgressQueueOccupancy\",\n\t\t\trunTest: func(t *testing.T, node1, node2 *clusterNode, testMetrics *testMetrics) {\n\t\t\t\tassertBiDiCommunication(t, node1, node2, testReq)\n\t\t\t\trequire.Equal(t, []string{\"host\", node2.nodeInfo.Endpoint, \"msg_type\", \"transaction\", \"channel\", testChannel},\n\t\t\t\t\ttestMetrics.egressQueueLength.WithArgsForCall(0))\n\t\t\t\trequire.Equal(t, float64(0), testMetrics.egressQueueLength.SetArgsForCall(0))\n\t\t\t\trequire.Equal(t, float64(1), testMetrics.egressQueueCapacity.SetArgsForCall(0))\n\n\t\t\t\tvar messageReceived sync.WaitGroup\n\t\t\t\tmessageReceived.Add(1)\n\t\t\t\tnode2.handler.On(\"OnConsensus\", testChannel, node1.nodeInfo.ID, mock.Anything).Run(func(args mock.Arguments) {\n\t\t\t\t\tmessageReceived.Done()\n\t\t\t\t}).Return(nil)\n\n\t\t\t\trm, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tstream := assertEventualEstablishStream(t, rm)\n\t\t\t\tstream.Send(testConsensusReq)\n\t\t\t\tmessageReceived.Wait()\n\n\t\t\t\trequire.Equal(t, []string{\"host\", node2.nodeInfo.Endpoint, \"msg_type\", \"consensus\", \"channel\", testChannel},\n\t\t\t\t\ttestMetrics.egressQueueLength.WithArgsForCall(1))\n\t\t\t\trequire.Equal(t, float64(0), testMetrics.egressQueueLength.SetArgsForCall(1))\n\t\t\t\trequire.Equal(t, float64(1), testMetrics.egressQueueCapacity.SetArgsForCall(1))\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"EgressStreamsCount\",\n\t\t\trunTest: func(t *testing.T, node1, node2 *clusterNode, testMetrics *testMetrics) {\n\t\t\t\tassertBiDiCommunication(t, node1, node2, testReq)\n\t\t\t\trequire.Equal(t, 1, testMetrics.egressStreamCount.SetCallCount())\n\t\t\t\trequire.Equal(t, 1, testMetrics.egressStreamCount.WithCallCount())\n\t\t\t\trequire.Equal(t, []string{\"channel\", testChannel}, testMetrics.egressStreamCount.WithArgsForCall(0))\n\n\t\t\t\tassertBiDiCommunicationForChannel(t, node1, node2, testReq2, testChannel2)\n\t\t\t\trequire.Equal(t, 2, testMetrics.egressStreamCount.SetCallCount())\n\t\t\t\trequire.Equal(t, 2, testMetrics.egressStreamCount.WithCallCount())\n\t\t\t\trequire.Equal(t, []string{\"channel\", testChannel2}, testMetrics.egressStreamCount.WithArgsForCall(1))\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"EgressTLSConnCount\",\n\t\t\trunTest: func(t *testing.T, node1, node2 *clusterNode, testMetrics *testMetrics) {\n\t\t\t\tassertBiDiCommunication(t, node1, node2, testReq)\n\t\t\t\trequire.Equal(t, []string{\"channel\", testChannel}, testMetrics.egressStreamCount.WithArgsForCall(0))\n\n\t\t\t\tassertBiDiCommunicationForChannel(t, node1, node2, testReq2, testChannel2)\n\t\t\t\trequire.Equal(t, []string{\"channel\", testChannel2}, testMetrics.egressStreamCount.WithArgsForCall(1))\n\n\t\t\t\t// A single TLS connection despite 2 streams\n\t\t\t\trequire.Equal(t, float64(1), testMetrics.egressTLSConnCount.SetArgsForCall(0))\n\t\t\t\trequire.Equal(t, 1, testMetrics.egressTLSConnCount.SetCallCount())\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"EgressWorkerSize\",\n\t\t\trunTest: func(t *testing.T, node1, node2 *clusterNode, testMetrics *testMetrics) {\n\t\t\t\tassertBiDiCommunication(t, node1, node2, testReq)\n\t\t\t\trequire.Equal(t, []string{\"channel\", testChannel}, testMetrics.egressStreamCount.WithArgsForCall(0))\n\n\t\t\t\tassertBiDiCommunicationForChannel(t, node1, node2, testReq2, testChannel2)\n\t\t\t\trequire.Equal(t, []string{\"channel\", testChannel2}, testMetrics.egressStreamCount.WithArgsForCall(1))\n\n\t\t\t\trequire.Equal(t, float64(1), testMetrics.egressWorkerSize.SetArgsForCall(0))\n\t\t\t\trequire.Equal(t, float64(1), testMetrics.egressWorkerSize.SetArgsForCall(1))\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"MsgSendTime\",\n\t\t\trunTest: func(t *testing.T, node1, node2 *clusterNode, testMetrics *testMetrics) {\n\t\t\t\tassertBiDiCommunication(t, node1, node2, testReq)\n\t\t\t\trequire.Eventually(t, func() bool { return testMetrics.msgSendTime.ObserveCallCount() > 0 }, time.Second, 10*time.Millisecond)\n\t\t\t\trequire.Equal(t, 1, testMetrics.msgSendTime.ObserveCallCount())\n\t\t\t\trequire.Equal(t, []string{\"host\", node2.nodeInfo.Endpoint, \"channel\", testChannel}, testMetrics.msgSendTime.WithArgsForCall(0))\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"MsgDropCount\",\n\t\t\trunTest: func(t *testing.T, node1, node2 *clusterNode, testMetrics *testMetrics) {\n\t\t\t\tblockRecv := make(chan struct{})\n\t\t\t\twasReported := func() bool {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-blockRecv:\n\t\t\t\t\t\treturn true\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn false\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// When the drop count is reported, release the lock on the server side receive operation.\n\t\t\t\ttestMetrics.msgDropCount.AddStub = func(float642 float64) {\n\t\t\t\t\tif !wasReported() {\n\t\t\t\t\t\tclose(blockRecv)\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tnode2.handler.On(\"OnConsensus\", testChannel, node1.nodeInfo.ID, mock.Anything).Run(func(args mock.Arguments) {\n\t\t\t\t\t// Block until the message drop is reported\n\t\t\t\t\t<-blockRecv\n\t\t\t\t}).Return(nil)\n\n\t\t\t\trm, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tstream := assertEventualEstablishStream(t, rm)\n\t\t\t\t// Send too many messages while the server side is not reading from the stream\n\t\t\t\tfor {\n\t\t\t\t\tstream.Send(testConsensusReq)\n\t\t\t\t\tif wasReported() {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\trequire.Equal(t, []string{\"host\", node2.nodeInfo.Endpoint, \"channel\", testChannel},\n\t\t\t\t\ttestMetrics.msgDropCount.WithArgsForCall(0))\n\t\t\t\trequire.Equal(t, 1, testMetrics.msgDropCount.AddCallCount())\n\t\t\t},\n\t\t},\n\t} {\n\t\ttestCase := testCase\n\t\tt.Run(testCase.name, func(t *testing.T) {\n\t\t\tfakeProvider := &mocks.MetricsProvider{}\n\t\t\ttestCase.testMetrics = &testMetrics{\n\t\t\t\tfakeProvider: fakeProvider,\n\t\t\t}\n\n\t\t\ttestCase.testMetrics.initialize()\n\n\t\t\tnode1 := newTestNodeWithMetrics(t, fakeProvider, &testCase.testMetrics.egressTLSConnCount)\n\t\t\tdefer node1.stop()\n\n\t\t\tnode2 := newTestNode(t)\n\t\t\tdefer node2.stop()\n\n\t\t\tconfigForNode1 := []cluster.RemoteNode{node2.nodeInfo}\n\t\t\tconfigForNode2 := []cluster.RemoteNode{node1.nodeInfo}\n\t\t\tnode1.c.Configure(testChannel, configForNode1)\n\t\t\tnode2.c.Configure(testChannel, configForNode2)\n\t\t\tnode1.c.Configure(testChannel2, configForNode1)\n\t\t\tnode2.c.Configure(testChannel2, configForNode2)\n\n\t\t\ttestCase.runTest(t, node1, node2, testCase.testMetrics)\n\t\t})\n\t}\n}\n\nfunc TestCertExpirationWarningEgress(t *testing.T) {\n\t// Scenario: Ensures that when certificates are due to expire,\n\t// a warning is logged to the log.\n\n\tnode1 := newTestNode(t)\n\tnode2 := newTestNode(t)\n\n\tcert, err := x509.ParseCertificate(node2.nodeInfo.ServerTLSCert)\n\trequire.NoError(t, err)\n\trequire.NotNil(t, cert)\n\n\t// Let the NotAfter time of the certificate be T1, the current time be T0.\n\t// So time.Until is (T1 - T0), which means we have (T1 - T0) time left.\n\t// We want to trigger a warning, so we set the warning threshold to be 20 seconds above\n\t// the time left, so the time left would be smaller than the threshold.\n\tnode1.c.CertExpWarningThreshold = time.Until(cert.NotAfter) + time.Second*20\n\t// We only alert once in 3 seconds\n\tnode1.c.MinimumExpirationWarningInterval = time.Second * 3\n\n\tdefer node1.stop()\n\tdefer node2.stop()\n\n\tconfig := []cluster.RemoteNode{node1.nodeInfo, node2.nodeInfo}\n\tnode1.c.Configure(testChannel, config)\n\tnode2.c.Configure(testChannel, config)\n\n\tstub, err := node1.c.Remote(testChannel, node2.nodeInfo.ID)\n\trequire.NoError(t, err)\n\n\tmockgRPC := &mocks.StepClient{}\n\tmockgRPC.On(\"Send\", mock.Anything).Return(nil)\n\tmockgRPC.On(\"Context\").Return(context.Background())\n\tmockClient := &mocks.ClusterClient{}\n\tmockClient.On(\"Step\", mock.Anything).Return(mockgRPC, nil)\n\n\tstub.Client = mockClient\n\n\tstream := assertEventualEstablishStream(t, stub)\n\n\talerts := make(chan struct{}, 100)\n\n\tstream.Logger = stream.Logger.WithOptions(zap.Hooks(func(entry zapcore.Entry) error {\n\t\tif strings.Contains(entry.Message, \"expires in less than\") {\n\t\t\talerts <- struct{}{}\n\t\t}\n\t\treturn nil\n\t}))\n\n\t// Send a message to the node and expert an alert to be logged.\n\tstream.Send(wrapSubmitReq(testReq))\n\tselect {\n\tcase <-alerts:\n\tcase <-time.After(time.Second * 5):\n\t\tt.Fatal(\"Should have logged an alert\")\n\t}\n\t// Send another message, and ensure we don't log anything to the log, because the\n\t// alerts should be suppressed before the minimum interval timeout expires.\n\tstream.Send(wrapSubmitReq(testReq))\n\tselect {\n\tcase <-alerts:\n\t\tt.Fatal(\"Should not have logged an alert\")\n\tcase <-time.After(time.Millisecond * 500):\n\t}\n\t// Wait enough time for the alert interval to clear.\n\ttime.Sleep(node1.c.MinimumExpirationWarningInterval + time.Second)\n\t// Send again a message, and this time it should be logged again.\n\tstream.Send(wrapSubmitReq(testReq))\n\tselect {\n\tcase <-alerts:\n\tcase <-time.After(time.Second * 5):\n\t\tt.Fatal(\"Should have logged an alert\")\n\t}\n}\n\nfunc assertBiDiCommunicationForChannel(t *testing.T, node1, node2 *clusterNode, msgToSend *orderer.SubmitRequest, channel string) {\n\testablish := []struct {\n\t\tlabel    string\n\t\tsender   *clusterNode\n\t\treceiver *clusterNode\n\t\ttarget   uint64\n\t}{\n\t\t{label: \"1->2\", sender: node1, target: node2.nodeInfo.ID, receiver: node2},\n\t\t{label: \"2->1\", sender: node2, target: node1.nodeInfo.ID, receiver: node1},\n\t}\n\tfor _, estab := range establish {\n\t\tstub, err := estab.sender.c.Remote(channel, estab.target)\n\t\trequire.NoError(t, err)\n\n\t\tstream := assertEventualEstablishStream(t, stub)\n\n\t\tvar wg sync.WaitGroup\n\t\twg.Add(1)\n\t\testab.receiver.handler.On(\"OnSubmit\", channel, estab.sender.nodeInfo.ID, mock.Anything).Return(nil).Once().Run(func(args mock.Arguments) {\n\t\t\treq := args.Get(2).(*orderer.SubmitRequest)\n\t\t\trequire.True(t, proto.Equal(req, msgToSend))\n\t\t\tt.Log(estab.label)\n\t\t\twg.Done()\n\t\t})\n\n\t\terr = stream.Send(wrapSubmitReq(msgToSend))\n\t\trequire.NoError(t, err)\n\n\t\twg.Wait()\n\t}\n}\n\nfunc assertBiDiCommunication(t *testing.T, node1, node2 *clusterNode, msgToSend *orderer.SubmitRequest) {\n\tassertBiDiCommunicationForChannel(t, node1, node2, msgToSend, testChannel)\n}\n\nfunc assertEventualEstablishStream(t *testing.T, rpc *cluster.RemoteContext) *cluster.Stream {\n\tvar res *cluster.Stream\n\tgt := gomega.NewGomegaWithT(t)\n\tgt.Eventually(func() error {\n\t\tstream, err := rpc.NewStream(time.Hour)\n\t\tres = stream\n\t\treturn err\n\t}, timeout).Should(gomega.Succeed())\n\treturn res\n}\n\nfunc assertEventualSendMessage(t *testing.T, rpc *cluster.RemoteContext, req *orderer.SubmitRequest) orderer.Cluster_StepClient {\n\tvar res orderer.Cluster_StepClient\n\tgt := gomega.NewGomegaWithT(t)\n\tgt.Eventually(func() error {\n\t\tstream, err := rpc.NewStream(time.Hour)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tres = stream\n\t\treturn stream.Send(wrapSubmitReq(req))\n\t}, timeout).Should(gomega.Succeed())\n\treturn res\n}\n\nfunc wrapSubmitReq(req *orderer.SubmitRequest) *orderer.StepRequest {\n\treturn &orderer.StepRequest{\n\t\tPayload: &orderer.StepRequest_SubmitRequest{\n\t\t\tSubmitRequest: req,\n\t\t},\n\t}\n}\n", "/*\nCopyright IBM Corp. 2017 All Rights Reserved.\n\nSPDX-License-Identifier: Apache-2.0\n*/\n\npackage cluster\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"time\"\n\n\t\"github.com/hyperledger/fabric-protos-go/orderer\"\n\t\"github.com/hyperledger/fabric/common/flogging\"\n\t\"github.com/hyperledger/fabric/common/util\"\n\t\"github.com/pkg/errors\"\n\t\"go.uber.org/zap\"\n\t\"google.golang.org/grpc\"\n)\n\n//go:generate mockery -dir . -name Dispatcher -case underscore -output ./mocks/\n\n// Dispatcher dispatches requests\ntype Dispatcher interface {\n\tDispatchSubmit(ctx context.Context, request *orderer.SubmitRequest) error\n\tDispatchConsensus(ctx context.Context, request *orderer.ConsensusRequest) error\n}\n\n//go:generate mockery -dir . -name StepStream -case underscore -output ./mocks/\n\n// StepStream defines the gRPC stream for sending\n// transactions, and receiving corresponding responses\ntype StepStream interface {\n\tSend(response *orderer.StepResponse) error\n\tRecv() (*orderer.StepRequest, error)\n\tgrpc.ServerStream\n}\n\n// Service defines the raft Service\ntype Service struct {\n\tStreamCountReporter              *StreamCountReporter\n\tDispatcher                       Dispatcher\n\tLogger                           *flogging.FabricLogger\n\tStepLogger                       *flogging.FabricLogger\n\tMinimumExpirationWarningInterval time.Duration\n\tCertExpWarningThreshold          time.Duration\n}\n\n// Step passes an implementation-specific message to another cluster member.\nfunc (s *Service) Step(stream orderer.Cluster_StepServer) error {\n\ts.StreamCountReporter.Increment()\n\tdefer s.StreamCountReporter.Decrement()\n\n\taddr := util.ExtractRemoteAddress(stream.Context())\n\tcommonName := commonNameFromContext(stream.Context())\n\texp := s.initializeExpirationCheck(stream, addr, commonName)\n\ts.Logger.Debugf(\"Connection from %s(%s)\", commonName, addr)\n\tdefer s.Logger.Debugf(\"Closing connection from %s(%s)\", commonName, addr)\n\tfor {\n\t\terr := s.handleMessage(stream, addr, exp)\n\t\tif err == io.EOF {\n\t\t\ts.Logger.Debugf(\"%s(%s) disconnected\", commonName, addr)\n\t\t\treturn nil\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// Else, no error occurred, so we continue to the next iteration\n\t}\n}\n\nfunc (s *Service) handleMessage(stream StepStream, addr string, exp *certificateExpirationCheck) error {\n\trequest, err := stream.Recv()\n\tif err == io.EOF {\n\t\treturn err\n\t}\n\tif err != nil {\n\t\ts.Logger.Warningf(\"Stream read from %s failed: %v\", addr, err)\n\t\treturn err\n\t}\n\n\texp.checkExpiration(time.Now(), extractChannel(request))\n\n\tif s.StepLogger.IsEnabledFor(zap.DebugLevel) {\n\t\tnodeName := commonNameFromContext(stream.Context())\n\t\ts.StepLogger.Debugf(\"Received message from %s(%s): %v\", nodeName, addr, requestAsString(request))\n\t}\n\n\tif submitReq := request.GetSubmitRequest(); submitReq != nil {\n\t\tnodeName := commonNameFromContext(stream.Context())\n\t\ts.Logger.Debugf(\"Received message from %s(%s): %v\", nodeName, addr, requestAsString(request))\n\t\treturn s.handleSubmit(submitReq, stream, addr)\n\t} else if consensusReq := request.GetConsensusRequest(); consensusReq != nil {\n\t\treturn s.Dispatcher.DispatchConsensus(stream.Context(), request.GetConsensusRequest())\n\t}\n\n\treturn errors.Errorf(\"message is neither a Submit nor a Consensus request\")\n}\n\nfunc (s *Service) handleSubmit(request *orderer.SubmitRequest, stream StepStream, addr string) error {\n\terr := s.Dispatcher.DispatchSubmit(stream.Context(), request)\n\tif err != nil {\n\t\ts.Logger.Warningf(\"Handling of Submit() from %s failed: %v\", addr, err)\n\t\treturn err\n\t}\n\treturn err\n}\n\nfunc (s *Service) initializeExpirationCheck(stream orderer.Cluster_StepServer, endpoint, nodeName string) *certificateExpirationCheck {\n\treturn &certificateExpirationCheck{\n\t\tminimumExpirationWarningInterval: s.MinimumExpirationWarningInterval,\n\t\texpirationWarningThreshold:       s.CertExpWarningThreshold,\n\t\texpiresAt:                        expiresAt(stream),\n\t\tendpoint:                         endpoint,\n\t\tnodeName:                         nodeName,\n\t\talert: func(template string, args ...interface{}) {\n\t\t\ts.Logger.Warningf(template, args...)\n\t\t},\n\t}\n}\n\nfunc expiresAt(stream orderer.Cluster_StepServer) time.Time {\n\tcert := util.ExtractCertificateFromContext(stream.Context())\n\tif cert == nil {\n\t\treturn time.Time{}\n\t}\n\treturn cert.NotAfter\n}\n\nfunc extractChannel(msg *orderer.StepRequest) string {\n\tif consReq := msg.GetConsensusRequest(); consReq != nil {\n\t\treturn consReq.Channel\n\t}\n\n\tif submitReq := msg.GetSubmitRequest(); submitReq != nil {\n\t\treturn submitReq.Channel\n\t}\n\n\treturn \"\"\n}\n"], "filenames": ["orderer/common/cluster/comm_test.go", "orderer/common/cluster/service.go"], "buggy_code_start_loc": [122, 16], "buggy_code_end_loc": [481, 97], "fixing_code_start_loc": [123, 17], "fixing_code_end_loc": [538, 99], "type": "CWE-20", "message": "Hyperledger Fabric is a permissioned distributed ledger framework. In affected versions if a consensus client sends a malformed consensus request to an orderer it may crash the orderer node. A fix has been added in commit 0f1835949 which checks for missing consensus messages and returns an error to the consensus client should the message be missing. Users are advised to upgrade to versions 2.2.7 or v2.4.5. There are no known workarounds for this issue.", "other": {"cve": {"id": "CVE-2022-31121", "sourceIdentifier": "security-advisories@github.com", "published": "2022-07-07T18:15:09.517", "lastModified": "2022-07-15T15:18:48.167", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Hyperledger Fabric is a permissioned distributed ledger framework. In affected versions if a consensus client sends a malformed consensus request to an orderer it may crash the orderer node. A fix has been added in commit 0f1835949 which checks for missing consensus messages and returns an error to the consensus client should the message be missing. Users are advised to upgrade to versions 2.2.7 or v2.4.5. There are no known workarounds for this issue."}, {"lang": "es", "value": "Hyperledger Fabric es un marco de libro mayor distribuido con permisos. En versiones afectadas, si un cliente de consenso env\u00eda una petici\u00f3n de consenso malformada a un ordenante, puede bloquear el nodo ordenante. Ha sido a\u00f1adida una correcci\u00f3n en el commit 0f1835949 que comprueba si faltan mensajes de consenso y devuelve un error al cliente de consenso si falta el mensaje. Es recomendado a usuarios actualizar a versiones 2.2.7 o v2.4.5. No se presentan mitigaciones conocidas para este problema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 5.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:hyperledger:fabric:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.2.7", "matchCriteriaId": "E53C59E5-DF7D-4FD9-9F55-6F4738546A62"}, {"vulnerable": true, "criteria": "cpe:2.3:a:hyperledger:fabric:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.4.5", "matchCriteriaId": "9B13458A-9B90-4D8D-B46C-ABEF1AE7CF81"}]}]}], "references": [{"url": "https://github.com/hyperledger/fabric/commit/0f18359493bcbd5f9f9d1a9b05adabfe5da23b06", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/hyperledger/fabric/releases/tag/v2.2.7", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/hyperledger/fabric/releases/tag/v2.4.5", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/hyperledger/fabric/security/advisories/GHSA-72x4-cq6r-jp4p", "source": "security-advisories@github.com", "tags": ["Issue Tracking", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/hyperledger/fabric/commit/0f18359493bcbd5f9f9d1a9b05adabfe5da23b06"}}