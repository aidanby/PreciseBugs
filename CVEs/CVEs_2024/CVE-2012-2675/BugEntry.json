{"buggy_code": ["/* Alternative malloc implementation for multiple threads without\r\nlock contention based on dlmalloc. (C) 2005-2010 Niall Douglas\r\n\r\nBoost Software License - Version 1.0 - August 17th, 2003\r\n\r\nPermission is hereby granted, free of charge, to any person or organization\r\nobtaining a copy of the software and accompanying documentation covered by\r\nthis license (the \"Software\") to use, reproduce, display, distribute,\r\nexecute, and transmit the Software, and to prepare derivative works of the\r\nSoftware, and to permit third-parties to whom the Software is furnished to\r\ndo so, all subject to the following:\r\n\r\nThe copyright notices in the Software and this entire statement, including\r\nthe above license grant, this restriction and the following disclaimer,\r\nmust be included in all copies of the Software, in whole or in part, and\r\nall derivative works of the Software, unless such copies or derivative\r\nworks are solely in the form of machine-executable object code generated by\r\na source language processor.\r\n\r\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\nFITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT\r\nSHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE\r\nFOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,\r\nARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\r\nDEALINGS IN THE SOFTWARE.\r\n*/\r\n\r\n#if 0 /* Effectively makes nedmalloc = dlmalloc */\r\n#define THREADCACHEMAX 0\r\n#define DEFAULT_GRANULARITY 65536\r\n#define DEFAULTMAXTHREADSINPOOL 1\r\n#endif\r\n\r\n#ifdef _MSC_VER\r\n/* Enable full aliasing on MSVC */\r\n/*#pragma optimize(\"a\", on)*/\r\n/*#pragma optimize(\"g\", off)*/\r\n\r\n#pragma warning(push)\r\n#pragma warning(disable:4100)\t/* unreferenced formal parameter */\r\n#pragma warning(disable:4127)\t/* conditional expression is constant */\r\n#pragma warning(disable:4232)\t/* address of dllimport is not static, identity not guaranteed */\r\n#pragma warning(disable:4706)\t/* assignment within conditional expression */\r\n\r\n#define _CRT_SECURE_NO_WARNINGS 1\t/* Don't care about MSVC warnings on POSIX functions */\r\n#include <stdio.h>\r\n#define fopen(f, m) _fsopen((f), (m), 0x40/*_SH_DENYNO*/)\t/* Have Windows let other programs view the log file as it is written */\r\n#ifndef UNICODE\r\n#define UNICODE\t\t\t\t\t/* Turn on windows unicode support */\r\n#endif\r\n#else\r\n#include <stdio.h>\r\n#endif\r\n\r\n/*#define NEDMALLOC_DEBUG 1*/\r\n/*#define ENABLE_LOGGING 7\r\n#define NEDMALLOC_TESTLOGENTRY(tc, np, type, mspace, size, mem, alignment, flags, returned) (((type)&ENABLE_LOGGING)&&((size)>16*1024))\r\n#define NEDMALLOC_STACKBACKTRACEDEPTH 16*/\r\n/*#define NEDMALLOC_FORCERESERVE(p, mem, size) (((size)>=(256*1024)) ? M2_RESERVE_MULT(8) : 0)*/\r\n/*#define WIN32_DIRECT_USE_FILE_MAPPINGS 0*/\r\n\r\n/*#define NEDMALLOC_DEBUG 1*/\r\n\r\n/*#define FULLSANITYCHECKS*/\r\n\r\n/* There is only support for the user mode page allocator on Windows at present */\r\n#if !defined(ENABLE_USERMODEPAGEALLOCATOR)\r\n#define ENABLE_USERMODEPAGEALLOCATOR 0\r\n#endif\r\n\r\n/* If link time code generation is on, don't force or prevent inlining */\r\n#if defined(_MSC_VER) && defined(NEDMALLOC_DLL_EXPORTS)\r\n#define FORCEINLINE\r\n#define NOINLINE\r\n#endif\r\n\r\n#include \"nedmalloc.h\"\r\n#include <errno.h>\r\n#if defined(WIN32)\r\n #include <malloc.h>\r\n#else\r\n#if defined(__cplusplus)\r\nextern \"C\"\r\n#else\r\nextern\r\n#endif\r\n#if defined(__linux__) || defined(__FreeBSD__)\r\n/* Sadly we can't include <malloc.h> as it causes a redefinition error */\r\nsize_t malloc_usable_size(void *);\r\n#elif defined(__APPLE__)\r\nsize_t malloc_size(const void *ptr);\r\n#else\r\n#error Do not know what to do here\r\n#endif\r\n#endif\r\n\r\n#if USE_ALLOCATOR==1\r\n #define MSPACES 1\r\n #define ONLY_MSPACES 1\r\n#endif\r\n#define USE_DL_PREFIX 1\r\n#ifndef USE_LOCKS\r\n #define USE_LOCKS 1\r\n#endif\r\n#define FOOTERS 1           /* Need to enable footers so frees lock the right mspace */\r\n#ifndef NEDMALLOC_DEBUG\r\n #if defined(DEBUG) || defined(_DEBUG)\r\n  #define NEDMALLOC_DEBUG 1\r\n #else\r\n  #define NEDMALLOC_DEBUG 0\r\n #endif\r\n#endif\r\n/* We need to consistently define DEBUG=0|1, _DEBUG and NDEBUG for dlmalloc */\r\n#if !defined(DEBUG) && !defined(NDEBUG)\r\n #ifdef __GNUC__\r\n  #warning DEBUG may not be defined but without NDEBUG being defined allocator will run with assert checking! Define NDEBUG to run at full speed.\r\n #elif defined(_MSC_VER)\r\n  #pragma message(__FILE__ \": WARNING: DEBUG may not be defined but without NDEBUG being defined allocator will run with assert checking! Define NDEBUG to run at full speed.\")\r\n #endif\r\n#endif\r\n#undef DEBUG\r\n#undef _DEBUG\r\n#if NEDMALLOC_DEBUG\r\n #define _DEBUG\r\n #define DEBUG 1\r\n#else\r\n #define DEBUG 0\r\n#endif\r\n#ifdef NDEBUG               /* Disable assert checking on release builds */\r\n #undef DEBUG\r\n #undef _DEBUG\r\n#endif\r\n/* The default of 64Kb means we spend too much time kernel-side */\r\n#ifndef DEFAULT_GRANULARITY\r\n#define DEFAULT_GRANULARITY (1*1024*1024)\r\n#if DEBUG\r\n#define DEFAULT_GRANULARITY_ALIGNED\r\n#endif\r\n#endif\r\n/*#define USE_SPIN_LOCKS 0*/\r\n\r\n#if ENABLE_USERMODEPAGEALLOCATOR\r\nextern int OSHavePhysicalPageSupport(void);\r\nextern void *userpage_malloc(size_t toallocate, unsigned flags);\r\nextern int userpage_free(void *mem, size_t size);\r\nextern void *userpage_realloc(void *mem, size_t oldsize, size_t newsize, int flags, unsigned flags2);\r\n\r\n#define USERPAGE_TOPDOWN                   (M2_CUSTOM_FLAGS_BEGIN<<0)\r\n#define USERPAGE_NOCOMMIT                  (M2_CUSTOM_FLAGS_BEGIN<<1)\r\n\r\n/* This can provide a very significant speed boost */\r\n#undef MMAP_CLEARS\r\n#define MMAP_CLEARS 0\r\n\r\n#define MUNMAP(h, a, s)                    (!OSHavePhysicalPageSupport() ? MUNMAP_DEFAULT((h), (a), (s)) : userpage_free((a), (s)))\r\n#define MMAP(s, f)                         (!OSHavePhysicalPageSupport() ? MMAP_DEFAULT((s)) : userpage_malloc((s), (f)))\r\n#define MREMAP(addr, osz, nsz, mv)         (!OSHavePhysicalPageSupport() ? MREMAP_DEFAULT((addr), (osz), (nsz), (mv)) : userpage_realloc((addr), (osz), (nsz), (mv), 0))\r\n#define DIRECT_MMAP(h, s, f)               (!OSHavePhysicalPageSupport() ? DIRECT_MMAP_DEFAULT((h), (s), (f)) : userpage_malloc((s), (f)|USERPAGE_TOPDOWN))\r\n#define DIRECT_MREMAP(h, a, os, ns, f, f2) (!OSHavePhysicalPageSupport() ? DIRECT_MREMAP_DEFAULT((h), (a), (os), (ns), (f), (f2)) : userpage_realloc((a), (os), (ns), (f), (f2)|USERPAGE_TOPDOWN))\r\n\r\n/*#undef MREMAP\r\n#define MREMAP(addr, osz, nsz, mv)         (!OSHavePhysicalPageSupport() ? MREMAP_DEFAULT((addr), (osz), (nsz), (mv)) : MFAIL)*/\r\n/*#undef DIRECT_MREMAP\r\n#define DIRECT_MREMAP(h, a, os, ns, f, f2) (!OSHavePhysicalPageSupport() ? DIRECT_MREMAP_DEFAULT((h), (a), (os), (ns), (f), (f2)) : MFAIL)*/\r\n\r\n#endif\r\n#include \"malloc.c.h\"\r\n#ifdef NDEBUG               /* Disable assert checking on release builds */\r\n #undef DEBUG\r\n#endif\r\n\r\n/* The default number of threads allowed into a pool at once */\r\n#ifndef DEFAULTMAXTHREADSINPOOL\r\n#define DEFAULTMAXTHREADSINPOOL 4\r\n#endif\r\n/* The maximum size to be allocated from the thread cache */\r\n#ifndef THREADCACHEMAX\r\n#define THREADCACHEMAX 8192\r\n#elif THREADCACHEMAX && !defined(THREADCACHEMAXBINS)\r\n #ifdef __GNUC__\r\n  #warning If you are changing THREADCACHEMAX, do you also need to change THREADCACHEMAXBINS=(topbitpos(THREADCACHEMAX)-4)?\r\n #elif defined(_MSC_VER)\r\n  #pragma message(__FILE__ \": WARNING: If you are changing THREADCACHEMAX, do you also need to change THREADCACHEMAXBINS=(topbitpos(THREADCACHEMAX)-4)?\")\r\n #endif\r\n#endif\r\n/* The maximum concurrent threads in a pool possible */\r\n#ifndef MAXTHREADSINPOOL\r\n#define MAXTHREADSINPOOL 16\r\n#endif\r\n/* The maximum number of threadcaches which can be allocated */\r\n#ifndef THREADCACHEMAXCACHES\r\n#define THREADCACHEMAXCACHES 256\r\n#endif\r\n#ifndef THREADCACHEMAXBINS\r\n#if 0\r\n/* The number of cache entries for finer grained bins. This is (topbitpos(THREADCACHEMAX)-4)*2 */\r\n#define THREADCACHEMAXBINS ((13-4)*2)\r\n#else\r\n/* The number of cache entries. This is (topbitpos(THREADCACHEMAX)-4) */\r\n#define THREADCACHEMAXBINS (13-4)\r\n#endif\r\n#endif\r\n/* Point at which the free space in a thread cache is garbage collected */\r\n#ifndef THREADCACHEMAXFREESPACE\r\n#define THREADCACHEMAXFREESPACE (1024*1024)\r\n#endif\r\n/* NEDMALLOC_FORCERESERVE is used to force malloc2 flags for normal malloc, calloc et al */\r\n#ifndef NEDMALLOC_FORCERESERVE\r\n#define NEDMALLOC_FORCERESERVE(p, mem, size) 0\r\n#endif\r\n/* ENABLE_LOGGING is a bitmask of what events to log */\r\n#if ENABLE_LOGGING\r\n#ifndef NEDMALLOC_LOGFILE\r\n#define NEDMALLOC_LOGFILE \"nedmalloc.csv\"\r\n#endif\r\n#endif\r\n/* NEDMALLOC_TESTLOGENTRY returns non-zero if the entry should be logged */\r\n#ifndef NEDMALLOC_TESTLOGENTRY\r\n#define NEDMALLOC_TESTLOGENTRY(tc, np, type, mspace, size, mem, alignment, flags, returned) ((type)&ENABLE_LOGGING)\r\n#endif\r\n/* NEDMALLOC_STACKBACKTRACEDEPTH has the logger store a stack backtrace per logged item. Slow! */\r\n#ifndef NEDMALLOC_STACKBACKTRACEDEPTH\r\n#define NEDMALLOC_STACKBACKTRACEDEPTH 0\r\n#endif\r\n#if NEDMALLOC_STACKBACKTRACEDEPTH\r\n#include \"Dbghelp.h\"\r\n#include \"psapi.h\"\r\n#pragma comment(lib, \"dbghelp.lib\")\r\n#pragma comment(lib, \"psapi.lib\")\r\n#endif\r\n#define NM_FLAGS_MASK (M2_FLAGS_MASK&~M2_ZERO_MEMORY)\r\n\r\n#if USE_LOCKS\r\n#ifdef WIN32\r\n #define TLSVAR\t\t\tDWORD\r\n #define TLSALLOC(k)\t(*(k)=TlsAlloc(), TLS_OUT_OF_INDEXES==*(k))\r\n #define TLSFREE(k)\t\t(!TlsFree(k))\r\n #define TLSGET(k)\t\tTlsGetValue(k)\r\n #define TLSSET(k, a)\t(!TlsSetValue(k, a))\r\n #ifdef DEBUG\r\nstatic LPVOID ChkedTlsGetValue(DWORD idx)\r\n{\r\n\tLPVOID ret=TlsGetValue(idx);\r\n\tassert(S_OK==GetLastError());\r\n\treturn ret;\r\n}\r\n  #undef TLSGET\r\n  #define TLSGET(k) ChkedTlsGetValue(k)\r\n #endif\r\n#else\r\n #define TLSVAR\t\t\tpthread_key_t\r\n #define TLSALLOC(k)\tpthread_key_create(k, 0)\r\n #define TLSFREE(k)\t\tpthread_key_delete(k)\r\n #define TLSGET(k)\t\tpthread_getspecific(k)\r\n #define TLSSET(k, a)\tpthread_setspecific(k, a)\r\n#endif\r\n#else /* Probably if you're not using locks then you don't want ANY pthread stuff at all */\r\n #define TLSVAR\t\t\tvoid *\r\n #define TLSALLOC(k)\t(*k=0)\r\n #define TLSFREE(k)\t\t(k=0)\r\n #define TLSGET(k)\t\tk\r\n #define TLSSET(k, a)\t(k=a, 0)\r\n#endif\r\n\r\n#if ENABLE_USERMODEPAGEALLOCATOR\r\n#include \"usermodepageallocator.c\"\r\n#endif\r\n\r\n#if defined(__cplusplus)\r\n#if !defined(NO_NED_NAMESPACE)\r\nnamespace nedalloc {\r\n#else\r\nextern \"C\" {\r\n#endif\r\n#endif\r\n\r\n#if USE_ALLOCATOR==0\r\nstatic void *unsupported_operation(const char *opname) THROWSPEC\r\n{\r\n\tfprintf(stderr, \"nedmalloc: The operation %s is not supported under this build configuration\\n\", opname);\r\n\tabort();\r\n\treturn 0;\r\n}\r\nstatic size_t mspacecounter=(size_t) 0xdeadbeef;\r\n#endif\r\n#ifndef ENABLE_FAST_HEAP_DETECTION\r\nstatic void *RESTRICT leastusedaddress;\r\nstatic size_t largestusedblock;\r\n#endif\r\n/* Used to redirect system allocator ops if needed */\r\nextern void *(*sysmalloc)(size_t);\r\nextern void *(*syscalloc)(size_t, size_t);\r\nextern void *(*sysrealloc)(void *, size_t);\r\nextern void (*sysfree)(void *);\r\nextern size_t (*sysblksize)(void *);\r\n\r\n#if !defined(REPLACE_SYSTEM_ALLOCATOR) || (!defined(_MSC_VER) && !defined(__MINGW32__))\r\nvoid *(*sysmalloc)(size_t)=malloc;\r\nvoid *(*syscalloc)(size_t, size_t)=calloc;\r\nvoid *(*sysrealloc)(void *, size_t)=realloc;\r\nvoid (*sysfree)(void *)=free;\r\nsize_t (*sysblksize)(void *)=\r\n#if defined(_MSC_VER) || defined(__MINGW32__)\r\n\t/* This is the MSVCRT equivalent */\r\n\t_msize;\r\n#elif defined(__linux__) || defined(__FreeBSD__)\r\n\t/* This is the glibc/ptmalloc2/dlmalloc/BSD equivalent.  */\r\n\tmalloc_usable_size;\r\n#elif defined(__APPLE__)\r\n\t/* This is the Apple BSD libc equivalent.  */\r\n\tmalloc_size;\r\n#else\r\n#error Cannot tolerate the memory allocator of an unknown system!\r\n#endif\r\n#else\r\n/* Remove the MSVCRT dependency on the memory functions */\r\nvoid *(*sysmalloc)(size_t);\r\nvoid *(*syscalloc)(size_t, size_t);\r\nvoid *(*sysrealloc)(void *, size_t);\r\nvoid (*sysfree)(void *);\r\nsize_t (*sysblksize)(void *);\r\n#endif\r\n\r\nstatic FORCEINLINE NEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void *CallMalloc(void *RESTRICT mspace, size_t size, size_t alignment, unsigned flags) THROWSPEC\r\n{\r\n\tvoid *RESTRICT ret=0;\r\n#if USE_MAGIC_HEADERS\r\n\tsize_t _alignment=alignment;\r\n\tsize_t *_ret=0;\r\n\tsize+=alignment+3*sizeof(size_t);\r\n\t_alignment=0;\r\n#endif\r\n#if USE_ALLOCATOR==0\r\n\tret=(flags & M2_ZERO_MEMORY) ? syscalloc(1, size) : sysmalloc(size);\t/* magic headers takes care of alignment */\r\n#elif USE_ALLOCATOR==1\r\n\tret=mspace_malloc2((mstate) mspace, size, alignment, flags);\r\n#ifndef ENABLE_FAST_HEAP_DETECTION\r\n\tif(ret)\r\n\t{\r\n\t\tmchunkptr p=mem2chunk(ret);\r\n\t\tsize_t truesize=chunksize(p) - overhead_for(p);\r\n\t\tif(!leastusedaddress || (void *)((mstate) mspace)->least_addr<leastusedaddress) leastusedaddress=(void *)((mstate) mspace)->least_addr;\r\n\t\tif(!largestusedblock || truesize>largestusedblock) largestusedblock=(truesize+mparams.page_size) & ~(mparams.page_size-1);\r\n\t}\r\n#endif\r\n#endif\r\n\tif(!ret) return 0;\r\n#if DEBUG\r\n\tif(flags & M2_ZERO_MEMORY)\r\n\t{\r\n\t\tconst char *RESTRICT n;\r\n\t\tfor(n=(const char *)ret; n<(const char *)ret+size; n++)\r\n\t\t{\r\n\t\t\tassert(!*n);\r\n\t\t}\r\n\t}\r\n#endif\r\n#if USE_MAGIC_HEADERS\r\n\t_ret=(size_t *) ret;\r\n\tret=(void *)(_ret+3);\r\n\tif(alignment) ret=(void *)(((size_t) ret+alignment-1)&~(alignment-1));\r\n\tfor(; _ret<(size_t *)ret-2; _ret++) *_ret=*(size_t *)\"NEDMALOC\";\r\n\t_ret[0]=(size_t) mspace;\r\n\t_ret[1]=size-3*sizeof(size_t);\r\n#endif\r\n\treturn ret;\r\n}\r\n\r\nstatic FORCEINLINE NEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void *CallRealloc(void *RESTRICT mspace, void *RESTRICT mem, int isforeign, size_t oldsize, size_t newsize, size_t alignment, unsigned flags) THROWSPEC\r\n{\r\n\tvoid *RESTRICT ret=0;\r\n#if USE_MAGIC_HEADERS\r\n\tmstate oldmspace=0;\r\n\tsize_t *_ret=0, *_mem=(size_t *) mem-3;\r\n#endif\r\n\tif(isforeign)\r\n\t{\t/* Transfer */\r\n#if USE_MAGIC_HEADERS\r\n\t\tassert(_mem[0]!=*(size_t *) \"NEDMALOC\");\r\n#endif\r\n\t\tif((ret=CallMalloc(mspace, newsize, alignment, flags)))\r\n\t\t{\r\n#if defined(DEBUG)\r\n\t\t\tprintf(\"*** nedmalloc frees system allocated block %p\\n\", mem);\r\n#endif\r\n\t\t\tmemcpy(ret, mem, oldsize<newsize ? oldsize : newsize);\r\n\t\t\tsysfree(mem);\r\n\t\t}\r\n\t\treturn ret;\r\n\t}\r\n#if USE_MAGIC_HEADERS\r\n\tassert(_mem[0]==*(size_t *) \"NEDMALOC\");\r\n\tnewsize+=3*sizeof(size_t);\r\n\toldmspace=(mstate) _mem[1];\r\n\tassert(oldsize>=_mem[2]);\r\n\tfor(; *_mem==*(size_t *) \"NEDMALOC\"; *_mem--=*(size_t *) \"nedmaloc\");\r\n\tmem=(void *)(++_mem);\r\n#endif\r\n#if USE_ALLOCATOR==0\r\n\tret=sysrealloc(mem, newsize);\r\n#elif USE_ALLOCATOR==1\r\n\tret=mspace_realloc2((mstate) mspace, mem, newsize, alignment, flags);\r\n#ifndef ENABLE_FAST_HEAP_DETECTION\r\n\tif(ret)\r\n\t{\r\n\t\tmchunkptr p=mem2chunk(ret);\r\n\t\tsize_t truesize=chunksize(p) - overhead_for(p);\r\n\t\tif(!leastusedaddress || (void *)((mstate) mspace)->least_addr<leastusedaddress) leastusedaddress=(void *)((mstate) mspace)->least_addr;\r\n\t\tif(!largestusedblock || truesize>largestusedblock) largestusedblock=(truesize+mparams.page_size) & ~(mparams.page_size-1);\r\n\t}\r\n#endif\r\n#endif\r\n\tif(!ret)\r\n\t{\t/* Put it back the way it was */\r\n#if USE_MAGIC_HEADERS\r\n\t\tfor(; *_mem==0; *_mem++=*(size_t *) \"NEDMALOC\");\r\n#endif\r\n\t\treturn 0;\r\n\t}\r\n#if USE_MAGIC_HEADERS\r\n\t_ret=(size_t *) ret;\r\n\tret=(void *)(_ret+3);\r\n\tfor(; _ret<(size_t *)ret-2; _ret++) *_ret=*(size_t *) \"NEDMALOC\";\r\n\t_ret[0]=(size_t) mspace;\r\n\t_ret[1]=newsize-3*sizeof(size_t);\r\n#endif\r\n\treturn ret;\r\n}\r\n\r\nstatic FORCEINLINE void CallFree(void *RESTRICT mspace, void *RESTRICT mem, int isforeign) THROWSPEC\r\n{\r\n#if USE_MAGIC_HEADERS\r\n\tmstate oldmspace=0;\r\n\tsize_t *_mem=(size_t *) mem-3, oldsize=0;\r\n#endif\r\n\tif(isforeign)\r\n\t{\r\n#if USE_MAGIC_HEADERS\r\n\t\tassert(_mem[0]!=*(size_t *) \"NEDMALOC\");\r\n#endif\r\n#if defined(DEBUG)\r\n\t\tprintf(\"*** nedmalloc frees system allocated block %p\\n\", mem);\r\n#endif\r\n\t\tsysfree(mem);\r\n\t\treturn;\r\n\t}\r\n#if USE_MAGIC_HEADERS\r\n\tassert(_mem[0]==*(size_t *) \"NEDMALOC\");\r\n\toldmspace=(mstate) _mem[1];\r\n\toldsize=_mem[2];\r\n\tfor(; *_mem==*(size_t *) \"NEDMALOC\"; *_mem--=*(size_t *) \"nedmaloc\");\r\n\tmem=(void *)(++_mem);\r\n#endif\r\n#if USE_ALLOCATOR==0\r\n\tsysfree(mem);\r\n#elif USE_ALLOCATOR==1\r\n\tmspace_free((mstate) mspace, mem);\r\n#endif\r\n}\r\n\r\nstatic NEDMALLOCNOALIASATTR mstate nedblkmstate(void *RESTRICT mem) THROWSPEC\r\n{\r\n\tif(mem)\r\n\t{\r\n#if USE_MAGIC_HEADERS\r\n\t\tsize_t *_mem=(size_t *) mem-3;\r\n\t\tif(_mem[0]==*(size_t *) \"NEDMALOC\")\r\n\t\t{\r\n\t\t\treturn (mstate) _mem[1];\r\n\t\t}\r\n\t\telse return 0;\r\n#else\r\n#if USE_ALLOCATOR==0\r\n\t\t/* Fail everything */\r\n\t\treturn 0;\r\n#elif USE_ALLOCATOR==1\r\n#ifdef ENABLE_FAST_HEAP_DETECTION\r\n#ifdef WIN32\r\n\t\t/*  On Windows for RELEASE both x86 and x64 the NT heap precedes each block with an eight byte header\r\n\t\t\twhich looks like:\r\n\t\t\t\tnormal: 4 bytes of size, 4 bytes of [char < 64, char < 64, char < 64 bit 0 always set, char random ]\r\n\t\t\t\tmmaped: 4 bytes of size  4 bytes of [zero,      zero,      0xb,                        zero        ]\r\n\r\n\t\t\tOn Windows for DEBUG both x86 and x64 the preceding four bytes is always 0xfdfdfdfd (no man's land).\r\n\t\t*/\r\n#pragma pack(push, 1)\r\n\t\tstruct _HEAP_ENTRY\r\n\t\t{\r\n\t\t\tUSHORT Size;\r\n\t\t\tUSHORT PreviousSize;\r\n\t\t\tUCHAR Cookie;\t\t\t/* SegmentIndex */\r\n\t\t\tUCHAR Flags;\t\t\t/* always bit 0 (HEAP_ENTRY_BUSY). bit 1=(HEAP_ENTRY_EXTRA_PRESENT), bit 2=normal block (HEAP_ENTRY_FILL_PATTERN), bit 3=mmap block (HEAP_ENTRY_VIRTUAL_ALLOC). Bit 4 (HEAP_ENTRY_LAST_ENTRY) could be set */\r\n\t\t\tUCHAR UnusedBytes;\r\n\t\t\tUCHAR SmallTagIndex;\t/* fastbin index. Always one of 0x02, 0x03, 0x04 < 0x80 */\r\n\t\t} *RESTRICT he=((struct _HEAP_ENTRY *) mem)-1;\r\n#pragma pack(pop)\r\n\t\tunsigned int header=((unsigned int *)mem)[-1], mask1=0x8080E100, result1, mask2=0xFFFFFF06, result2;\r\n\t\tresult1=header & mask1;\t/* Positive testing for NT heap */\r\n\t\tresult2=header & mask2;\t/* Positive testing for dlmalloc */\r\n\t\tif(result1==0x00000100 && result2!=0x00000102)\r\n\t\t{\t/* This is likely a NT heap block */\r\n\t\t\treturn 0;\r\n\t\t}\r\n#endif\r\n#ifdef __linux__\r\n\t\t/* On Linux glibc uses ptmalloc2 (really dlmalloc) just as we do, but prev_foot contains rubbish\r\n\t\twhen the preceding block is allocated because ptmalloc2 finds the local mstate by rounding the ptr\r\n\t\tdown to the nearest megabyte. It's like dlmalloc with FOOTERS disabled. */\r\n\t\tmchunkptr p=mem2chunk(mem);\r\n\t\tmstate fm=get_mstate_for(p);\r\n\t\t/* If it's a ptmalloc2 block, fm is likely to be some crazy value */\r\n\t\tif(!is_aligned(fm)) return 0;\r\n\t\tif((size_t)mem-(size_t)fm>=(size_t)1<<(SIZE_T_BITSIZE-1)) return 0;\r\n\t\tif(ok_magic(fm))\r\n\t\t\treturn fm;\r\n\t\telse\r\n\t\t\treturn 0;\r\n\t\tif(1) { }\r\n#endif\r\n\t\telse\r\n\t\t{\r\n\t\t\tmchunkptr p=mem2chunk(mem);\r\n\t\t\tmstate fm=get_mstate_for(p);\r\n\t\t\tassert(ok_magic(fm));\t/* If this fails, someone tried to free a block twice */\r\n\t\t\tif(ok_magic(fm))\r\n\t\t\t\treturn fm;\r\n\t\t}\r\n#else\r\n#ifdef WIN32\r\n#ifdef _MSC_VER\r\n\t\t__try\r\n#elif defined(__MINGW32__)\r\n\t\t__try1\r\n#endif\r\n#endif\r\n\t\t{\r\n\t\t\t/* We try to return zero here if it isn't one of our own blocks, however\r\n\t\t\tthe current block annotation scheme used by dlmalloc makes it impossible\r\n\t\t\tto be absolutely sure of avoiding a segfault.\r\n\r\n\t\t\tmchunkptr->prev_foot = mem-(2*size_t) = mstate ^ mparams.magic for PRECEDING block;\r\n\t\t\tmchunkptr->head      = mem-(1*size_t) = 8 multiple size of this block with bottom three bits = FLAG_BITS\r\n\t\t\t    FLAG_BITS = bit 0 is CINUSE (currently in use unless is mmap), bit 1 is PINUSE (previous block currently\r\n\t\t\t\t            in use unless mmap), bit 2 is UNUSED and currently is always zero.\r\n\t\t\t*/\r\n\t\t\tregister void *RESTRICT leastusedaddress_=leastusedaddress;\t\t/* Cache these to avoid register reloading */\r\n\t\t\tregister size_t largestusedblock_=largestusedblock;\r\n\t\t\tif(!is_aligned(mem)) return 0;\t\t/* Would fail very rarely as all allocators return aligned blocks */\r\n\t\t\tif(mem<leastusedaddress_) return 0;\t/* Simple but effective */\r\n\t\t\t{\r\n\t\t\t\tmchunkptr p=mem2chunk(mem);\r\n\t\t\t\tmstate fm=0;\r\n\t\t\t\tint ismmapped=is_mmapped(p);\r\n\t\t\t\tif((!ismmapped && !is_inuse(p)) || (p->head & FLAG4_BIT)) return 0;\r\n\t\t\t\t/* Reduced uncertainty by 0.5^2 = 25.0% */\r\n\t\t\t\t/* size should never exceed largestusedblock */\r\n\t\t\t\tif(chunksize(p)-overhead_for(p)>largestusedblock_) return 0;\r\n\t\t\t\t/* Reduced uncertainty by a minimum of 0.5^3 = 12.5%, maximum 0.5^16 = 0.0015% */\r\n\t\t\t\t/* Having sanity checked prev_foot and head, check next block */\r\n\t\t\t\tif(!ismmapped && (!next_pinuse(p) || (next_chunk(p)->head & FLAG4_BIT))) return 0;\r\n\t\t\t\t/* Reduced uncertainty by 0.5^5 = 3.13% or 0.5^18 = 0.00038% */\r\n#if 0\r\n\t\t\t\t/* If previous block is free, check that its next block pointer equals us */\r\n\t\t\t\tif(!ismmapped && !pinuse(p))\r\n\t\t\t\t\tif(next_chunk(prev_chunk(p))!=p) return 0;\r\n\t\t\t\t/* We could start comparing prev_foot's for similarity but it starts getting slow. */\r\n#endif\r\n\t\t\t\tfm = get_mstate_for(p);\r\n\t\t\t\tif(!is_aligned(fm) || (void *)fm<leastusedaddress_) return 0;\r\n#if 0\r\n\t\t\t\t/* See if mem is lower in memory than mem */\r\n\t\t\t\tif((size_t)mem-(size_t)fm>=(size_t)1<<(SIZE_T_BITSIZE-1)) return 0;\r\n#endif\r\n\t\t\t\tassert(ok_magic(fm));\t/* If this fails, someone tried to free a block twice */\r\n\t\t\t\tif(ok_magic(fm))\r\n\t\t\t\t\treturn fm;\r\n\t\t\t}\r\n\t\t}\r\n#ifdef WIN32\r\n#ifdef _MSC_VER\r\n\t\t__except(1) { }\r\n#elif defined(__MINGW32__)\r\n\t\t__except1(1) { }\r\n#endif\r\n#endif\r\n#endif\r\n#endif\r\n#endif\r\n\t}\r\n\treturn 0;\r\n}\r\nNEDMALLOCNOALIASATTR size_t nedblksize(int *RESTRICT isforeign, void *RESTRICT mem, unsigned flags) THROWSPEC\r\n{\r\n\tif(mem)\r\n\t{\r\n\t\tif(isforeign) *isforeign=1;\r\n#if USE_MAGIC_HEADERS\r\n\t\t{\r\n\t\t\tsize_t *_mem=(size_t *) mem-3;\r\n\t\t\tif(_mem[0]==*(size_t *) \"NEDMALOC\")\r\n\t\t\t{\r\n\t\t\t\tmstate mspace=(mstate) _mem[1];\r\n\t\t\t\tsize_t size=_mem[2];\r\n\t\t\t\tif(isforeign) *isforeign=0;\r\n\t\t\t\treturn size;\r\n\t\t\t}\r\n\t\t}\r\n#elif USE_ALLOCATOR==1\r\n\t\tif((flags & NM_SKIP_TOLERANCE_CHECKS) || nedblkmstate(mem))\r\n\t\t{\r\n\t\t\tmchunkptr p=mem2chunk(mem);\r\n\t\t\tif(isforeign) *isforeign=0;\r\n\t\t\treturn chunksize(p)-overhead_for(p);\r\n\t\t}\r\n#ifdef DEBUG\r\n\t\telse\r\n\t\t{\r\n\t\t\tint a=1; /* Set breakpoints here if needed */\r\n\t\t}\r\n#endif\r\n#endif\r\n#if defined(ENABLE_TOLERANT_NEDMALLOC) || USE_ALLOCATOR==0\r\n\t\treturn sysblksize(mem);\r\n#endif\r\n\t}\r\n\treturn 0;\r\n}\r\nNEDMALLOCNOALIASATTR size_t nedmemsize(void *RESTRICT mem) THROWSPEC { return nedblksize(0, mem, 0); }\r\n\r\nNEDMALLOCNOALIASATTR void nedsetvalue(void *v) THROWSPEC\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{ nedpsetvalue((nedpool *) 0, v); }\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedmalloc(size_t size) THROWSPEC\t\t\t\t\t\t\t\t\t\t\t\t\t{ return nedpmalloc((nedpool *) 0, size); }\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedcalloc(size_t no, size_t size) THROWSPEC\t\t\t\t\t\t\t\t\t\t{ return nedpcalloc((nedpool *) 0, no, size); }\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedrealloc(void *mem, size_t size) THROWSPEC\t\t\t\t\t\t\t\t\t\t{ return nedprealloc((nedpool *) 0, mem, size); }\r\nNEDMALLOCNOALIASATTR void   nedfree(void *mem) THROWSPEC\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{ nedpfree((nedpool *) 0, mem); }\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedmemalign(size_t alignment, size_t bytes) THROWSPEC\t\t\t\t\t\t\t\t{ return nedpmemalign((nedpool *) 0, alignment, bytes); }\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedmalloc2(size_t size, size_t alignment, unsigned flags) THROWSPEC\t\t\t\t{ return nedpmalloc2((nedpool *) 0, size, alignment, flags); }\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedrealloc2(void *mem, size_t size, size_t alignment, unsigned flags) THROWSPEC\t{ return nedprealloc2((nedpool *) 0, mem, size, alignment, flags); }\r\nNEDMALLOCNOALIASATTR void   nedfree2(void *mem, unsigned flags) THROWSPEC\t\t\t\t\t\t\t\t\t\t\t\t\t\t{ nedpfree2((nedpool *) 0, mem, flags); }\r\nNEDMALLOCNOALIASATTR struct nedmallinfo nedmallinfo(void) THROWSPEC\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{ return nedpmallinfo((nedpool *) 0); }\r\nNEDMALLOCNOALIASATTR int    nedmallopt(int parno, int value) THROWSPEC\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{ return nedpmallopt((nedpool *) 0, parno, value); }\r\nNEDMALLOCNOALIASATTR int    nedmalloc_trim(size_t pad) THROWSPEC\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{ return nedpmalloc_trim((nedpool *) 0, pad); }\r\nvoid   nedmalloc_stats() THROWSPEC\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{ nedpmalloc_stats((nedpool *) 0); }\r\nNEDMALLOCNOALIASATTR size_t nedmalloc_footprint() THROWSPEC\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{ return nedpmalloc_footprint((nedpool *) 0); }\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void **nedindependent_calloc(size_t elemsno, size_t elemsize, void **chunks) THROWSPEC\t{ return nedpindependent_calloc((nedpool *) 0, elemsno, elemsize, chunks); }\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void **nedindependent_comalloc(size_t elems, size_t *sizes, void **chunks) THROWSPEC\t\t{ return nedpindependent_comalloc((nedpool *) 0, elems, sizes, chunks); }\r\n\r\n#ifdef WIN32\r\ntypedef unsigned __int64 timeCount;\r\nstatic timeCount GetTimestamp()\r\n{\r\n\tstatic LARGE_INTEGER ticksPerSec;\r\n\tstatic double scalefactor;\r\n\tstatic timeCount baseCount;\r\n\tLARGE_INTEGER val;\r\n\ttimeCount ret;\r\n\tif(!scalefactor)\r\n\t{\r\n\t\tif(QueryPerformanceFrequency(&ticksPerSec))\r\n\t\t\tscalefactor=ticksPerSec.QuadPart/1000000000000.0;\r\n\t\telse\r\n\t\t\tscalefactor=1;\r\n\t}\r\n\tif(!QueryPerformanceCounter(&val))\r\n\t\treturn (timeCount) GetTickCount() * 1000000000;\r\n\tret=(timeCount) (val.QuadPart/scalefactor);\r\n\tif(!baseCount) baseCount=ret;\r\n\treturn ret-baseCount;\r\n}\r\n#else\r\n#include <sys/time.h>\r\n\r\ntypedef unsigned long long timeCount;\r\nstatic timeCount GetTimestamp()\r\n{\r\n\tstatic timeCount baseCount;\r\n\ttimeCount ret;\r\n#ifdef CLOCK_MONOTONIC\r\n\tstruct timespec ts;\r\n\tclock_gettime(CLOCK_MONOTONIC, &ts);\r\n\tret=((timeCount) ts.tv_sec*1000000000000LL)+ts.tv_nsec*1000LL;\r\n#else\r\n\tstruct timeval tv;\r\n\tgettimeofday(&tv, 0);\r\n\tret=((timeCount) tv.tv_sec*1000000000000LL)+tv.tv_usec*1000000LL;\r\n#endif\r\n\tif(!baseCount) baseCount=ret;\r\n\treturn ret-baseCount;\r\n}\r\n#endif\r\n\r\n/* Set ENABLE_LOGGING to an AND mask of which of these you want to\r\nlog, so set it to 0xffffffff for everything */\r\ntypedef enum LogEntryType_t\r\n{\r\n\tLOGENTRY_MALLOC\t\t\t\t\t=(1<<0),\r\n\tLOGENTRY_REALLOC\t\t\t\t=(1<<1),\r\n\tLOGENTRY_FREE\t\t\t\t\t=(1<<2),\r\n\r\n\tLOGENTRY_THREADCACHE_MALLOC\t\t=(1<<3),\r\n\tLOGENTRY_THREADCACHE_FREE\t\t=(1<<4),\r\n\tLOGENTRY_THREADCACHE_CLEAN\t\t=(1<<5),\r\n\r\n\tLOGENTRY_POOL_MALLOC\t\t\t=(1<<6),\r\n\tLOGENTRY_POOL_REALLOC\t\t\t=(1<<7),\r\n\tLOGENTRY_POOL_FREE\t\t\t\t=(1<<8)\r\n} LogEntryType;\r\n#if NEDMALLOC_STACKBACKTRACEDEPTH\r\ntypedef struct StackFrameType_t\r\n{\r\n\tvoid *pc;\r\n\tchar module[64];\r\n\tchar functname[256];\r\n\tchar file[96];\r\n\tint lineno;\r\n} StackFrameType;\r\n#endif\r\ntypedef struct logentry_t\r\n{\r\n\ttimeCount timestamp;\r\n\tnedpool *np;\r\n\tLogEntryType type;\r\n\tint mspace;\r\n\tsize_t size;\r\n\tvoid *mem;\r\n\tsize_t alignment;\r\n\tunsigned flags;\r\n\tvoid *returned;\r\n#if NEDMALLOC_STACKBACKTRACEDEPTH\r\n\tStackFrameType stack[NEDMALLOC_STACKBACKTRACEDEPTH];\r\n#endif\r\n} logentry;\r\nstatic const char *LogEntryTypeStrings[]={\r\n\t\"LOGENTRY_MALLOC\",\r\n\t\"LOGENTRY_REALLOC\",\r\n\t\"LOGENTRY_FREE\",\r\n\r\n\t\"LOGENTRY_THREADCACHE_MALLOC\",\r\n\t\"LOGENTRY_THREADCACHE_FREE\",\r\n\t\"LOGENTRY_THREADCACHE_CLEAN\",\r\n\r\n\t\"LOGENTRY_POOL_MALLOC\",\r\n\t\"LOGENTRY_POOL_REALLOC\",\r\n\t\"LOGENTRY_POOL_FREE\",\r\n\t\"******************\"\r\n};\r\n\r\nstruct threadcacheblk_t;\r\ntypedef struct threadcacheblk_t threadcacheblk;\r\nstruct threadcacheblk_t\r\n{\t/* Keep less than 32 bytes as sizeof(threadcacheblk) is the minimum allocation size */\r\n#ifdef FULLSANITYCHECKS\r\n\tunsigned int magic;\r\n#endif\r\n\tint isforeign;\r\n\tunsigned int lastUsed;\r\n\tsize_t size;\r\n\tthreadcacheblk *RESTRICT next, *RESTRICT prev;\r\n};\r\ntypedef struct threadcache_t\r\n{\r\n#ifdef FULLSANITYCHECKS\r\n\tunsigned int magic1;\r\n#endif\r\n\tint mymspace;\t\t\t\t\t\t/* Last mspace entry this thread used */\r\n\tlong threadid;\r\n\tunsigned int mallocs, frees, successes;\r\n#if ENABLE_LOGGING\r\n\tlogentry *logentries, *logentriesptr, *logentriesend;\r\n#endif\r\n\tsize_t freeInCache;\t\t\t\t\t/* How much free space is stored in this cache */\r\n\tthreadcacheblk *RESTRICT bins[(THREADCACHEMAXBINS+1)*2];\r\n#ifdef FULLSANITYCHECKS\r\n\tunsigned int magic2;\r\n#endif\r\n} threadcache;\r\nstruct nedpool_t\r\n{\r\n#if USE_LOCKS\r\n\tMLOCK_T mutex;\r\n#endif\r\n\tvoid *uservalue;\r\n\tint threads;\t\t\t\t\t\t/* Max entries in m to use */\r\n\tthreadcache *RESTRICT caches[THREADCACHEMAXCACHES];\r\n\tTLSVAR mycache;\t\t\t\t\t\t/* Thread cache for this thread. 0 for unset, negative for use mspace-1 directly, otherwise is cache-1 */\r\n\tmstate m[MAXTHREADSINPOOL+1];\t\t/* mspace entries for this pool */\r\n};\r\nstatic nedpool syspool;\r\n\r\n#if ENABLE_LOGGING\r\n#if NEDMALLOC_STACKBACKTRACEDEPTH\r\n#if defined(WIN32) && defined(_MSC_VER)\r\n#define COPY_STRING(d, s, maxlen) { size_t len=strlen(s); len=(len>maxlen) ? maxlen-1 : len; memcpy(d, s, len); d[len]=0; }\r\n\r\n#pragma optimize(\"g\", off)\r\nstatic int ExceptionFilter(unsigned int code, struct _EXCEPTION_POINTERS *ep, CONTEXT *ct) THROWSPEC\r\n{\r\n\t*ct=*ep->ContextRecord;\r\n\treturn EXCEPTION_EXECUTE_HANDLER;\r\n}\r\n\r\nstatic DWORD64 __stdcall GetModBase(HANDLE hProcess, DWORD64 dwAddr) THROWSPEC\r\n{\r\n\tDWORD64 modulebase;\r\n\t// Try to get the module base if already loaded, otherwise load the module\r\n\tmodulebase=SymGetModuleBase64(hProcess, dwAddr);\r\n\tif(modulebase)\r\n\t\treturn modulebase;\r\n\telse\r\n\t{\r\n\t\tMEMORY_BASIC_INFORMATION stMBI ;\r\n\t\tif ( 0 != VirtualQueryEx ( hProcess, (LPCVOID)(size_t)dwAddr, &stMBI, sizeof(stMBI)))\r\n\t\t{\r\n\t\t\tint n;\r\n\t\t\tDWORD dwPathLen=0, dwNameLen=0 ;\r\n\t\t\tTCHAR szFile[ MAX_PATH ], szModuleName[ MAX_PATH ] ;\r\n\t\t\tMODULEINFO mi={0};\r\n\t\t\tdwPathLen = GetModuleFileName ( (HMODULE) stMBI.AllocationBase , szFile, MAX_PATH );\r\n\t\t\tdwNameLen = GetModuleBaseName (hProcess, (HMODULE) stMBI.AllocationBase , szModuleName, MAX_PATH );\r\n\t\t\tfor(n=dwNameLen; n>0; n--)\r\n\t\t\t{\r\n\t\t\t\tif(szModuleName[n]=='.')\r\n\t\t\t\t{\r\n\t\t\t\t\tszModuleName[n]=0;\r\n\t\t\t\t\tbreak;\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tif(!GetModuleInformation(hProcess, (HMODULE) stMBI.AllocationBase, &mi, sizeof(mi)))\r\n\t\t\t{\r\n\t\t\t\t//fxmessage(\"WARNING: GetModuleInformation() returned error code %d\\n\", GetLastError());\r\n\t\t\t}\r\n\t\t\tif(!SymLoadModule64 ( hProcess, NULL, (PSTR)( (dwPathLen) ? szFile : 0), (PSTR)( (dwNameLen) ? szModuleName : 0),\r\n\t\t\t\t(DWORD64) mi.lpBaseOfDll, mi.SizeOfImage))\r\n\t\t\t{\r\n\t\t\t\t//fxmessage(\"WARNING: SymLoadModule64() returned error code %d\\n\", GetLastError());\r\n\t\t\t}\r\n\t\t\t//fxmessage(\"%s, %p, %x, %x\\n\", szFile, mi.lpBaseOfDll, mi.SizeOfImage, (DWORD) mi.lpBaseOfDll+mi.SizeOfImage);\r\n\t\t\tmodulebase=SymGetModuleBase64(hProcess, dwAddr);\r\n\t\t\treturn modulebase;\r\n\t\t}\r\n\t}\r\n\treturn 0;\r\n}\r\n\r\nextern HANDLE sym_myprocess;\r\nextern VOID (WINAPI *RtlCaptureContextAddr)(PCONTEXT);\r\nextern void DeinitSym(void) THROWSPEC;\r\nstatic void DoStackWalk(logentry *p) THROWSPEC\r\n{\r\n\tint i,i2;\r\n\tHANDLE mythread=(HANDLE) GetCurrentThread();\r\n\tSTACKFRAME64 sf={ 0 };\r\n\tCONTEXT ct={ 0 };\r\n\tif(!sym_myprocess)\r\n\t{\r\n\t\tDWORD symopts;\r\n\t\tDuplicateHandle(GetCurrentProcess(), GetCurrentProcess(), GetCurrentProcess(), &sym_myprocess, 0, FALSE, DUPLICATE_SAME_ACCESS);\r\n\t\tsymopts=SymGetOptions();\r\n\t\tSymSetOptions(symopts /*| SYMOPT_DEFERRED_LOADS*/ | SYMOPT_LOAD_LINES);\r\n\t\tSymInitialize(sym_myprocess, NULL, TRUE);\r\n\t\tatexit(DeinitSym);\r\n\t}\r\n\tct.ContextFlags=CONTEXT_FULL;\r\n\r\n\t// Use RtlCaptureContext() if we have it as it saves an exception throw\r\n\tif((VOID (WINAPI *)(PCONTEXT)) -1==RtlCaptureContextAddr)\r\n\t\tRtlCaptureContextAddr=(VOID (WINAPI *)(PCONTEXT)) GetProcAddress(GetModuleHandle(L\"kernel32\"), \"RtlCaptureContext\");\r\n\tif(RtlCaptureContextAddr)\r\n\t\tRtlCaptureContextAddr(&ct);\r\n\telse\r\n\t{\t// This is nasty, but it works\r\n\t\t__try\r\n\t\t{\r\n\t\t\tint *foo=0;\r\n\t\t\t*foo=78;\r\n\t\t}\r\n\t\t__except (ExceptionFilter(GetExceptionCode(), GetExceptionInformation(), &ct))\r\n\t\t{\r\n\t\t}\r\n\t}\r\n\r\n\tsf.AddrPC.Mode=sf.AddrStack.Mode=sf.AddrFrame.Mode=AddrModeFlat;\r\n#if !(defined(_M_AMD64) || defined(_M_X64))\r\n\tsf.AddrPC.Offset   =ct.Eip;\r\n\tsf.AddrStack.Offset=ct.Esp;\r\n\tsf.AddrFrame.Offset=ct.Ebp;\r\n#else\r\n\tsf.AddrPC.Offset   =ct.Rip;\r\n\tsf.AddrStack.Offset=ct.Rsp;\r\n\tsf.AddrFrame.Offset=ct.Rbp; // maybe Rdi?\r\n#endif\r\n\tfor(i2=0; i2<NEDMALLOC_STACKBACKTRACEDEPTH; i2++)\r\n\t{\r\n\t\tIMAGEHLP_MODULE64 ihm={ sizeof(IMAGEHLP_MODULE64) };\r\n\t\tchar temp[MAX_PATH+sizeof(IMAGEHLP_SYMBOL64)];\r\n\t\tIMAGEHLP_SYMBOL64 *ihs;\r\n\t\tIMAGEHLP_LINE64 ihl={ sizeof(IMAGEHLP_LINE64) };\r\n\t\tDWORD64 offset;\r\n\t\tif(!StackWalk64(\r\n#if !(defined(_M_AMD64) || defined(_M_X64))\r\n\t\t\tIMAGE_FILE_MACHINE_I386,\r\n#else\r\n\t\t\tIMAGE_FILE_MACHINE_AMD64,\r\n#endif\r\n\t\t\tsym_myprocess, mythread, &sf, &ct, NULL, SymFunctionTableAccess64, GetModBase, NULL))\r\n\t\t\tbreak;\r\n\t\tif(0==sf.AddrPC.Offset)\r\n\t\t\tbreak;\r\n\t\ti=i2;\r\n\t\tif(i)\t// Skip first entry relating to this function\r\n\t\t{\r\n\t\t\tDWORD lineoffset=0;\r\n\t\t\tp->stack[i-1].pc=(void *)(size_t) sf.AddrPC.Offset;\r\n\t\t\tif(SymGetModuleInfo64(sym_myprocess, sf.AddrPC.Offset, &ihm))\r\n\t\t\t{\r\n\t\t\t\tchar *leaf;\r\n\t\t\t\tleaf=strrchr(ihm.ImageName, '\\\\');\r\n\t\t\t\tif(!leaf) leaf=ihm.ImageName-1;\r\n\t\t\t\tCOPY_STRING(p->stack[i-1].module, leaf+1, sizeof(p->stack[i-1].module));\r\n\t\t\t}\r\n\t\t\telse strcpy(p->stack[i-1].module, \"<unknown>\");\r\n\t\t\t//fxmessage(\"WARNING: SymGetModuleInfo64() returned error code %d\\n\", GetLastError());\r\n\t\t\tmemset(temp, 0, MAX_PATH+sizeof(IMAGEHLP_SYMBOL64));\r\n\t\t\tihs=(IMAGEHLP_SYMBOL64 *) temp;\r\n\t\t\tihs->SizeOfStruct=sizeof(IMAGEHLP_SYMBOL64);\r\n\t\t\tihs->Address=sf.AddrPC.Offset;\r\n\t\t\tihs->MaxNameLength=MAX_PATH;\r\n\r\n\t\t\tif(SymGetSymFromAddr64(sym_myprocess, sf.AddrPC.Offset, &offset, ihs))\r\n\t\t\t{\r\n\t\t\t\tCOPY_STRING(p->stack[i-1].functname, ihs->Name, sizeof(p->stack[i-1].functname));\r\n\t\t\t\tif(strlen(p->stack[i-1].functname)<sizeof(p->stack[i-1].functname)-8)\r\n\t\t\t\t{\r\n\t\t\t\t\tsprintf(strchr(p->stack[i-1].functname, 0), \" +0x%x\", offset);\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\telse\r\n\t\t\t\tstrcpy(p->stack[i-1].functname, \"<unknown>\");\r\n\t\t\tif(SymGetLineFromAddr64(sym_myprocess, sf.AddrPC.Offset, &lineoffset, &ihl))\r\n\t\t\t{\r\n\t\t\t\tchar *leaf;\r\n\t\t\t\tp->stack[i-1].lineno=ihl.LineNumber;\r\n\r\n\t\t\t\tleaf=strrchr(ihl.FileName, '\\\\');\r\n\t\t\t\tif(!leaf) leaf=ihl.FileName-1;\r\n\t\t\t\tCOPY_STRING(p->stack[i-1].file, leaf+1, sizeof(p->stack[i-1].file));\r\n\t\t\t}\r\n\t\t\telse\r\n\t\t\t\tstrcpy(p->stack[i-1].file, \"<unknown>\");\r\n\t\t}\r\n\t}\r\n}\r\n#pragma optimize(\"g\", on)\r\n#else\r\nstatic void DoStackWalk(logentry *p) THROWSPEC\r\n{\r\n\tvoid *backtr[NEDMALLOC_STACKBACKTRACEDEPTH];\r\n\tsize_t size;\r\n\tchar **strings;\r\n\tsize_t i2;\r\n\tsize=backtrace(backtr, NEDMALLOC_STACKBACKTRACEDEPTH);\r\n\tstrings=backtrace_symbols(backtr, size);\r\n\tfor(i2=0; i2<size; i2++)\r\n\t{\t// Format can be <file path>(<mangled symbol>+0x<offset>) [<pc>]\r\n\t\t// or can be <file path> [<pc>]\r\n\t\tint start=0, end=strlen(strings[i2]), which=0, idx;\r\n\t\tfor(idx=0; idx<end; idx++)\r\n\t\t{\r\n\t\t\tif(0==which && (' '==strings[i2][idx] || '('==strings[i2][idx]))\r\n\t\t\t{\r\n\t\t\t\tint len=FXMIN(idx-start, (int) sizeof(p->stack[i2].file));\r\n\t\t\t\tmemcpy(p->stack[i2].file, strings[i2]+start, len);\r\n\t\t\t\tp->stack[i2].file[len]=0;\r\n\t\t\t\twhich=(' '==strings[i2][idx]) ? 2 : 1;\r\n\t\t\t\tstart=idx+1;\r\n\t\t\t}\r\n\t\t\telse if(1==which && ')'==strings[i2][idx])\r\n\t\t\t{\r\n\t\t\t\tFXString functname(strings[i2]+start, idx-start);\r\n\t\t\t\tFXint offset=functname.rfind(\"+0x\");\r\n\t\t\t\tFXString rawsymbol(functname.left(offset));\r\n\t\t\t\tFXString symbol(rawsymbol.length() ? fxdemanglesymbol(rawsymbol, false) : rawsymbol);\r\n\t\t\t\tsymbol.append(functname.mid(offset));\r\n\t\t\t\tint len=FXMIN(symbol.length(), (int) sizeof(p->stack[i2].functname));\r\n\t\t\t\tmemcpy(p->stack[i2].functname, symbol.text(), len);\r\n\t\t\t\tp->stack[i2].functname[len]=0;\r\n\t\t\t\twhich=2;\r\n\t\t\t}\r\n\t\t\telse if(2==which && '['==strings[i2][idx])\r\n\t\t\t{\r\n\t\t\t\tstart=idx+1;\r\n\t\t\t\twhich=3;\r\n\t\t\t}\r\n\t\t\telse if(3==which && ']'==strings[i2][idx])\r\n\t\t\t{\r\n\t\t\t\tFXString v(strings[i2]+start+2, idx-start-2);\r\n\t\t\t\tp->stack[i2].pc=(void *)(FXuval)v.toULong(0, 16);\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\tfree(strings);\r\n}\r\n#endif\r\n#endif\r\n#endif\r\nstatic FORCEINLINE logentry *LogOperation(threadcache *tc, nedpool *np, LogEntryType type, int mspace, size_t size, void *mem, size_t alignment, unsigned flags, void *returned) THROWSPEC\r\n{\r\n#if ENABLE_LOGGING\r\n\tif(tc->logentries && NEDMALLOC_TESTLOGENTRY(tc, np, type, mspace, size, mem, alignment, flags, returned))\r\n\t{\r\n\t\tlogentry *le;\r\n\t\tif(tc->logentriesptr==tc->logentriesend)\r\n\t\t{\r\n\t\t\tmchunkptr cp=mem2chunk(tc->logentries);\r\n\t\t\tsize_t logentrieslen=chunksize(cp)-overhead_for(cp);\r\n\t\t\tle=(logentry *) CallRealloc(0, tc->logentries, 0, logentrieslen, (logentrieslen*3)/2, 0, M2_ZERO_MEMORY|M2_ALWAYS_MMAP|M2_RESERVE_MULT(8));\r\n\t\t\tif(!le) return 0;\r\n\t\t\ttc->logentriesptr=le+(tc->logentriesptr-tc->logentries);\r\n\t\t\ttc->logentries=le;\r\n\t\t\tcp=mem2chunk(tc->logentries);\r\n\t\t\tlogentrieslen=(chunksize(cp)-overhead_for(cp))/sizeof(logentry);\r\n\t\t\ttc->logentriesend=tc->logentries+logentrieslen;\r\n\t\t}\r\n\t\tle=tc->logentriesptr++;\r\n\t\tassert(le+1<=tc->logentriesend);\r\n\t\tle->timestamp=GetTimestamp();\r\n\t\tle->np=np;\r\n\t\tle->type=type;\r\n\t\tle->mspace=mspace;\r\n\t\tle->size=size;\r\n\t\tle->mem=mem;\r\n\t\tle->alignment=alignment;\r\n\t\tle->flags=flags;\r\n\t\tle->returned=returned;\r\n#if NEDMALLOC_STACKBACKTRACEDEPTH\r\n\t\tDoStackWalk(le);\r\n#endif\r\n\t\treturn le;\r\n\t}\r\n#endif\r\n\treturn 0;\r\n}\r\n\r\nstatic FORCEINLINE NEDMALLOCNOALIASATTR unsigned int size2binidx(size_t _size) THROWSPEC\r\n{\t/* 8=1000\t16=10000\t20=10100\t24=11000\t32=100000\t48=110000\t4096=1000000000000 */\r\n\tunsigned int topbit, size=(unsigned int)(_size>>4);\r\n\t/* 16=1\t\t20=1\t24=1\t32=10\t48=11\t64=100\t96=110\t128=1000\t4096=100000000 */\r\n\r\n#if defined(__GNUC__)\r\n        topbit = sizeof(size)*__CHAR_BIT__ - 1 - __builtin_clz(size);\r\n#elif defined(_MSC_VER) && _MSC_VER>=1300\r\n\t{\r\n            unsigned long bsrTopBit;\r\n\r\n            _BitScanReverse(&bsrTopBit, size);\r\n\r\n            topbit = bsrTopBit;\r\n        }\r\n#else\r\n#if 0\r\n\tunion {\r\n\t\tunsigned asInt[2];\r\n\t\tdouble asDouble;\r\n\t};\r\n\tint n;\r\n\r\n\tasDouble = (double)size + 0.5;\r\n\ttopbit = (asInt[!FOX_BIGENDIAN] >> 20) - 1023;\r\n#else\r\n\t{\r\n\t\tunsigned int x=size;\r\n\t\tx = x | (x >> 1);\r\n\t\tx = x | (x >> 2);\r\n\t\tx = x | (x >> 4);\r\n\t\tx = x | (x >> 8);\r\n\t\tx = x | (x >>16);\r\n\t\tx = ~x;\r\n\t\tx = x - ((x >> 1) & 0x55555555);\r\n\t\tx = (x & 0x33333333) + ((x >> 2) & 0x33333333);\r\n\t\tx = (x + (x >> 4)) & 0x0F0F0F0F;\r\n\t\tx = x + (x << 8);\r\n\t\tx = x + (x << 16);\r\n\t\ttopbit=31 - (x >> 24);\r\n\t}\r\n#endif\r\n#endif\r\n\treturn topbit;\r\n}\r\n\r\n\r\n#ifdef FULLSANITYCHECKS\r\nstatic void tcsanitycheck(threadcacheblk *RESTRICT *RESTRICT ptr) THROWSPEC\r\n{\r\n\tassert((ptr[0] && ptr[1]) || (!ptr[0] && !ptr[1]));\r\n\tif(ptr[0] && ptr[1])\r\n\t{\r\n\t\tassert(ptr[0]->isforeign || nedblkmstate(ptr[0]));\r\n\t\tassert(ptr[1]->isforeign || nedblkmstate(ptr[1]));\r\n\t\tassert(nedblksize(0, ptr[0], 0)>=sizeof(threadcacheblk));\r\n\t\tassert(nedblksize(0, ptr[1], 0)>=sizeof(threadcacheblk));\r\n\t\tassert(*(unsigned int *) \"NEDN\"==ptr[0]->magic);\r\n\t\tassert(*(unsigned int *) \"NEDN\"==ptr[1]->magic);\r\n\t\tassert(!ptr[0]->prev);\r\n\t\tassert(!ptr[1]->next);\r\n\t\tif(ptr[0]==ptr[1])\r\n\t\t{\r\n\t\t\tassert(!ptr[0]->next);\r\n\t\t\tassert(!ptr[1]->prev);\r\n\t\t}\r\n\t}\r\n}\r\nstatic void tcfullsanitycheck(threadcache *tc) THROWSPEC\r\n{\r\n\tthreadcacheblk *RESTRICT *RESTRICT tcbptr=tc->bins;\r\n\tint n;\r\n\tfor(n=0; n<=THREADCACHEMAXBINS; n++, tcbptr+=2)\r\n\t{\r\n\t\tthreadcacheblk *RESTRICT b, *RESTRICT ob=0;\r\n\t\ttcsanitycheck(tcbptr);\r\n\t\tfor(b=tcbptr[0]; b; ob=b, b=b->next)\r\n\t\t{\r\n\t\t\tassert(b->isforeign || nedblkmstate(b));\r\n\t\t\tassert(nedblksize(0, b, 0)>=sizeof(threadcacheblk));\r\n\t\t\tassert(*(unsigned int *) \"NEDN\"==b->magic);\r\n\t\t\tassert(!ob || ob->next==b);\r\n\t\t\tassert(!ob || b->prev==ob);\r\n\t\t}\r\n\t}\r\n}\r\n#endif\r\n\r\nstatic NOINLINE int InitPool(nedpool *RESTRICT p, size_t capacity, int threads) THROWSPEC;\r\nstatic NOINLINE void RemoveCacheEntries(nedpool *RESTRICT p, threadcache *RESTRICT tc, unsigned int age) THROWSPEC\r\n{\r\n#ifdef FULLSANITYCHECKS\r\n\ttcfullsanitycheck(tc);\r\n#endif\r\n\tif(tc->freeInCache)\r\n\t{\r\n\t\tthreadcacheblk *RESTRICT *RESTRICT tcbptr=tc->bins;\r\n\t\tint n;\r\n\t\tfor(n=0; n<=THREADCACHEMAXBINS; n++, tcbptr+=2)\r\n\t\t{\r\n\t\t\tthreadcacheblk *RESTRICT *RESTRICT tcb=tcbptr+1;\t\t/* come from oldest end of list */\r\n\t\t\t/*tcsanitycheck(tcbptr);*/\r\n\t\t\tfor(; *tcb && tc->frees-(*tcb)->lastUsed>=age; )\r\n\t\t\t{\r\n\t\t\t\tthreadcacheblk *RESTRICT f=*tcb;\r\n\t\t\t\tsize_t blksize=f->size; /*nedblksize(f);*/\r\n\t\t\t\tassert(blksize<=nedblksize(0, f, 0));\r\n\t\t\t\tassert(blksize);\r\n#ifdef FULLSANITYCHECKS\r\n\t\t\t\tassert(*(unsigned int *) \"NEDN\"==(*tcb)->magic);\r\n#endif\r\n\t\t\t\t*tcb=(*tcb)->prev;\r\n\t\t\t\tif(*tcb)\r\n\t\t\t\t\t(*tcb)->next=0;\r\n\t\t\t\telse\r\n\t\t\t\t\t*tcbptr=0;\r\n\t\t\t\ttc->freeInCache-=blksize;\r\n\t\t\t\tassert((long) tc->freeInCache>=0);\r\n\t\t\t\tCallFree(0, f, f->isforeign);\r\n\t\t\t\t/*tcsanitycheck(tcbptr);*/\r\n\t\t\t\tLogOperation(tc, p, LOGENTRY_THREADCACHE_CLEAN, age, blksize, f, 0, 0, 0);\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n#ifdef FULLSANITYCHECKS\r\n\ttcfullsanitycheck(tc);\r\n#endif\r\n}\r\nsize_t nedflushlogs(nedpool *p, char *filepath) THROWSPEC\r\n{\r\n\tsize_t count=0;\r\n\tif(!p)\r\n\t{\r\n\t\tp=&syspool;\r\n\t\tif(!syspool.threads) InitPool(&syspool, 0, -1);\r\n\t}\r\n\tif(p->caches)\r\n\t{\r\n\t\tthreadcache *tc;\r\n\t\tint n;\r\n#if ENABLE_LOGGING\r\n\t\tint haslogentries=0;\r\n#endif\r\n\t\tfor(n=0; n<THREADCACHEMAXCACHES; n++)\r\n\t\t{\r\n\t\t\tif((tc=p->caches[n]))\r\n\t\t\t{\r\n\t\t\t\tcount+=tc->freeInCache;\r\n\t\t\t\ttc->frees++;\r\n\t\t\t\tRemoveCacheEntries(p, tc, 0);\r\n\t\t\t\tassert(!tc->freeInCache);\r\n#if ENABLE_LOGGING\r\n\t\t\t\thaslogentries|=!!tc->logentries;\r\n#endif\r\n\t\t\t}\r\n\t\t}\r\n#if ENABLE_LOGGING\r\n\t\tif(haslogentries)\r\n\t\t{\r\n\t\t\tchar buffer[MAX_PATH]=NEDMALLOC_LOGFILE;\r\n\t\t\tFILE *oh;\r\n\t\t\tfpos_t pos1, pos2;\r\n\t\t\tif(!filepath) filepath=buffer;\r\n\t\t\toh=fopen(filepath, \"r+\");\r\n\t\t\twhile(!oh)\r\n\t\t\t{\r\n\t\t\t\tchar *bptr;\r\n\t\t\t\tif((oh=fopen(filepath, \"w\"))) break;\r\n\t\t\t\tif(ENOSPC==errno) break;\r\n\t\t\t\tbptr=strrchr(filepath, '.');\r\n\t\t\t\tif(bptr-filepath>=MAX_PATH-6) abort();\r\n\t\t\t\tmemcpy(bptr, \"!.csv\", 6);\r\n\t\t\t}\r\n\t\t\tif(oh)\r\n\t\t\t{\r\n\t\t\t\tfgetpos(oh, &pos1);\r\n\t\t\t\tfseek(oh, 0, SEEK_END);\r\n\t\t\t\tfgetpos(oh, &pos2);\r\n\t\t\t\tif(pos1==pos2)\r\n\t\t\t\t\tfprintf(oh, \"Timestamp, Pool, Operation, MSpace, Size, Block, Alignment, Flags, Returned,\\\"Stack Backtrace\\\"\\n\");\r\n\t\t\t\tfor(n=0; n<THREADCACHEMAXCACHES; n++)\r\n\t\t\t\t{\r\n\t\t\t\t\tif((tc=p->caches[n]) && tc->logentries)\r\n\t\t\t\t\t{\r\n\t\t\t\t\t\tlogentry *le;\r\n\t\t\t\t\t\tfor(le=tc->logentries; le<tc->logentriesptr; le++)\r\n\t\t\t\t\t\t{\r\n\t\t\t\t\t\t\tconst char *LogEntryTypeString=LogEntryTypeStrings[size2binidx(((size_t)le->type)<<4)];\r\n\t\t\t\t\t\t\tchar stackbacktrace[16384]=\"?\";\r\n#if NEDMALLOC_STACKBACKTRACEDEPTH\r\n\t\t\t\t\t\t\tchar *sbtp=stackbacktrace;\r\n\t\t\t\t\t\t\tint i;\r\n\t\t\t\t\t\t\tfor(i=0; i<NEDMALLOC_STACKBACKTRACEDEPTH && le->stack[i].pc; i++)\r\n\t\t\t\t\t\t\t{\r\n\t\t\t\t\t\t\t\tsbtp+=sprintf(sbtp, \"0x%p:%s:%s (%s:%u),\",\r\n\t\t\t\t\t\t\t\t\tle->stack[i].pc, le->stack[i].module, le->stack[i].functname, le->stack[i].file, le->stack[i].lineno);\r\n\t\t\t\t\t\t\t\tif(sbtp>=stackbacktrace+sizeof(stackbacktrace)) abort();\r\n\t\t\t\t\t\t\t}\r\n\t\t\t\t\t\t\tif(NEDMALLOC_STACKBACKTRACEDEPTH==i)\r\n\t\t\t\t\t\t\t\tstrcpy(sbtp, \"<backtrace may continue ...>\");\r\n\t\t\t\t\t\t\telse\r\n\t\t\t\t\t\t\t\tstrcpy(sbtp, \"<backtrace ends>\");\r\n\t\t\t\t\t\t\tif(strchr(sbtp, 0)>=stackbacktrace+sizeof(stackbacktrace)) abort();\r\n#endif\r\n\t\t\t\t\t\t\tfprintf(oh, \"%llu, 0x%p, %s, %d, %Iu, 0x%p, %Iu, 0x%x, 0x%p,\\\"%s\\\"\\n\",\r\n\t\t\t\t\t\t\t\tle->timestamp, le->np, LogEntryTypeString, le->mspace, le->size, le->mem, le->alignment, le->flags, le->returned, stackbacktrace);\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t\tCallFree(0, tc->logentries, 0);\r\n\t\t\t\t\t\ttc->logentries=tc->logentriesptr=tc->logentriesend=0;\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t\tfclose(oh);\r\n\t\t\t}\r\n\t\t}\r\n#endif\r\n\t}\r\n\treturn count;\r\n}\r\nstatic void DestroyCaches(nedpool *RESTRICT p) THROWSPEC\r\n{\r\n\tif(p->caches)\r\n\t{\r\n\t\tthreadcache *tc;\r\n\t\tint n;\r\n\t\tnedflushlogs(p, 0);\r\n\t\tfor(n=0; n<THREADCACHEMAXCACHES; n++)\r\n\t\t{\r\n\t\t\tif((tc=p->caches[n]))\r\n\t\t\t{\r\n\t\t\t\ttc->mymspace=-1;\r\n\t\t\t\ttc->threadid=0;\r\n\t\t\t\tCallFree(0, tc, 0);\r\n\t\t\t\tp->caches[n]=0;\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\r\n\r\nstatic NOINLINE threadcache *AllocCache(nedpool *RESTRICT p) THROWSPEC\r\n{\r\n\tthreadcache *tc=0;\r\n\tint n, end;\r\n#if USE_LOCKS\r\n\tACQUIRE_LOCK(&p->mutex);\r\n#endif\r\n\tfor(n=0; n<THREADCACHEMAXCACHES && p->caches[n]; n++);\r\n\tif(THREADCACHEMAXCACHES==n)\r\n\t{\t/* List exhausted, so disable for this thread */\r\n#if USE_LOCKS\r\n\t\tRELEASE_LOCK(&p->mutex);\r\n#endif\r\n\t\treturn 0;\r\n\t}\r\n\ttc=p->caches[n]=(threadcache *) CallMalloc(p->m[0], sizeof(threadcache), 0, M2_ZERO_MEMORY);\r\n\tif(!tc)\r\n\t{\r\n#if USE_LOCKS\r\n\t\tRELEASE_LOCK(&p->mutex);\r\n#endif\r\n\t\treturn 0;\r\n\t}\r\n#ifdef FULLSANITYCHECKS\r\n\ttc->magic1=*(unsigned int *)\"NEDMALC1\";\r\n\ttc->magic2=*(unsigned int *)\"NEDMALC2\";\r\n#endif\r\n\ttc->threadid=\r\n#if USE_LOCKS\r\n\t\t(long)(size_t)CURRENT_THREAD;\r\n#else\r\n\t\t1;\r\n#endif\r\n\tfor(end=0; p->m[end]; end++);\r\n\ttc->mymspace=abs(tc->threadid) % end;\r\n#if ENABLE_LOGGING\r\n\t{\r\n\t\tmchunkptr cp;\r\n\t\tsize_t logentrieslen=2048/sizeof(logentry);\t\t/* One page */\r\n\t\ttc->logentries=tc->logentriesptr=(logentry *) CallMalloc(p->m[0], logentrieslen*sizeof(logentry), 0, M2_ZERO_MEMORY|M2_ALWAYS_MMAP|M2_RESERVE_MULT(8));\r\n\t\tif(!tc->logentries)\r\n\t\t{\r\n#if USE_LOCKS\r\n\t\t\tRELEASE_LOCK(&p->mutex);\r\n#endif\r\n\t\t\treturn 0;\r\n\t\t}\r\n\t\tcp=mem2chunk(tc->logentries);\r\n\t\tlogentrieslen=(chunksize(cp)-overhead_for(cp))/sizeof(logentry);\r\n\t\ttc->logentriesend=tc->logentries+logentrieslen;\r\n\t}\r\n#endif\r\n#if USE_LOCKS\r\n\tRELEASE_LOCK(&p->mutex);\r\n#endif\r\n\tif(TLSSET(p->mycache, (void *)(size_t)(n+1))) abort();\r\n\treturn tc;\r\n}\r\n\r\nstatic void *threadcache_malloc(nedpool *RESTRICT p, threadcache *RESTRICT tc, size_t *RESTRICT _size) THROWSPEC\r\n{\r\n\tvoid *RESTRICT ret=0;\r\n\tsize_t size=*_size, blksize=0;\r\n\tunsigned int bestsize;\r\n\tunsigned int idx=size2binidx(size);\r\n\tthreadcacheblk *RESTRICT blk, *RESTRICT *RESTRICT binsptr;\r\n#ifdef FULLSANITYCHECKS\r\n\ttcfullsanitycheck(tc);\r\n#endif\r\n\t/* Calculate best fit bin size */\r\n\tbestsize=1<<(idx+4);\r\n#if 0\r\n\t/* Finer grained bin fit */\r\n\tidx<<=1;\r\n\tif(size>bestsize)\r\n\t{\r\n\t\tidx++;\r\n\t\tbestsize+=bestsize>>1;\r\n\t}\r\n\tif(size>bestsize)\r\n\t{\r\n\t\tidx++;\r\n\t\tbestsize=1<<(4+(idx>>1));\r\n\t}\r\n#else\r\n\tif(size>bestsize)\r\n\t{\r\n\t\tidx++;\r\n\t\tbestsize<<=1;\r\n\t}\r\n#endif\r\n\tassert(bestsize>=size);\r\n\tif(size<bestsize) size=bestsize;\r\n\tassert(size<=THREADCACHEMAX);\r\n\tassert(idx<=THREADCACHEMAXBINS);\r\n\tbinsptr=&tc->bins[idx*2];\r\n\t/* Try to match close, but move up a bin if necessary */\r\n\tblk=*binsptr;\r\n\tif(!blk || blk->size<size)\r\n\t{\t/* Bump it up a bin */\r\n\t\tif(idx<THREADCACHEMAXBINS)\r\n\t\t{\r\n\t\t\tidx++;\r\n\t\t\tbinsptr+=2;\r\n\t\t\tblk=*binsptr;\r\n\t\t}\r\n\t}\r\n\tif(blk)\r\n\t{\r\n\t\tblksize=blk->size; /*nedblksize(blk);*/\r\n\t\tassert(nedblksize(0, blk, 0)>=blksize);\r\n\t\tassert(blksize>=size);\r\n\t\tif(blk->next)\r\n\t\t\tblk->next->prev=0;\r\n\t\t*binsptr=blk->next;\r\n\t\tif(!*binsptr)\r\n\t\t\tbinsptr[1]=0;\r\n#ifdef FULLSANITYCHECKS\r\n\t\tblk->magic=0;\r\n#endif\r\n\t\tassert(binsptr[0]!=blk && binsptr[1]!=blk);\r\n\t\tassert(nedblksize(0, blk, 0)>=sizeof(threadcacheblk) && nedblksize(0, blk, 0)<=THREADCACHEMAX+CHUNK_OVERHEAD);\r\n\t\t/*printf(\"malloc: %p, %p, %p, %lu\\n\", p, tc, blk, (long) _size);*/\r\n\t\tret=(void *) blk;\r\n\t}\r\n\t++tc->mallocs;\r\n\tif(ret)\r\n\t{\r\n\t\tassert(blksize>=size);\r\n\t\t++tc->successes;\r\n\t\ttc->freeInCache-=blksize;\r\n\t\tassert((long) tc->freeInCache>=0);\r\n\t}\r\n#if defined(DEBUG) && 0\r\n\tif(!(tc->mallocs & 0xfff))\r\n\t{\r\n\t\tprintf(\"*** threadcache=%u, mallocs=%u (%f), free=%u (%f), freeInCache=%u\\n\", (unsigned int) tc->threadid, tc->mallocs,\r\n\t\t\t(float) tc->successes/tc->mallocs, tc->frees, (float) tc->successes/tc->frees, (unsigned int) tc->freeInCache);\r\n\t}\r\n#endif\r\n#ifdef FULLSANITYCHECKS\r\n\ttcfullsanitycheck(tc);\r\n#endif\r\n\t*_size=size;\r\n\treturn ret;\r\n}\r\nstatic NOINLINE void ReleaseFreeInCache(nedpool *RESTRICT p, threadcache *RESTRICT tc, int mymspace) THROWSPEC\r\n{\r\n\tunsigned int age=THREADCACHEMAXFREESPACE/8192;\r\n#if USE_LOCKS\r\n\t/*ACQUIRE_LOCK(&p->m[mymspace]->mutex);*/\r\n#endif\r\n\twhile(age && tc->freeInCache>=THREADCACHEMAXFREESPACE)\r\n\t{\r\n\t\tRemoveCacheEntries(p, tc, age);\r\n\t\t/*printf(\"*** Removing cache entries older than %u (%u)\\n\", age, (unsigned int) tc->freeInCache);*/\r\n\t\tage>>=1;\r\n\t}\r\n#if USE_LOCKS\r\n\t/*RELEASE_LOCK(&p->m[mymspace]->mutex);*/\r\n#endif\r\n}\r\nstatic void threadcache_free(nedpool *RESTRICT p, threadcache *RESTRICT tc, int mymspace, void *RESTRICT mem, size_t size, int isforeign) THROWSPEC\r\n{\r\n\tunsigned int bestsize;\r\n\tunsigned int idx=size2binidx(size);\r\n\tthreadcacheblk *RESTRICT *RESTRICT binsptr, *RESTRICT tck=(threadcacheblk *RESTRICT) mem;\r\n\tassert(size>=sizeof(threadcacheblk) && size<=THREADCACHEMAX+CHUNK_OVERHEAD);\r\n#ifdef DEBUG\r\n\t/* Make sure this is a valid memory block */\r\n\tassert(nedblksize(0, mem, 0));\r\n#endif\r\n#ifdef FULLSANITYCHECKS\r\n\ttcfullsanitycheck(tc);\r\n#endif\r\n\t/* Calculate best fit bin size */\r\n\tbestsize=1<<(idx+4);\r\n#if 0\r\n\t/* Finer grained bin fit */\r\n\tidx<<=1;\r\n\tif(size>bestsize)\r\n\t{\r\n\t\tunsigned int biggerbestsize=bestsize+bestsize<<1;\r\n\t\tif(size>=biggerbestsize)\r\n\t\t{\r\n\t\t\tidx++;\r\n\t\t\tbestsize=biggerbestsize;\r\n\t\t}\r\n\t}\r\n#endif\r\n\tif(bestsize!=size)\t/* dlmalloc can round up, so we round down to preserve indexing */\r\n\t\tsize=bestsize;\r\n\tbinsptr=&tc->bins[idx*2];\r\n\tassert(idx<=THREADCACHEMAXBINS);\r\n\tif(tck==*binsptr)\r\n\t{\r\n\t\tfprintf(stderr, \"nedmalloc: Attempt to free already freed memory block %p - aborting!\\n\", tck);\r\n\t\tabort();\r\n\t}\r\n#ifdef FULLSANITYCHECKS\r\n\ttck->magic=*(unsigned int *) \"NEDN\";\r\n#endif\r\n\ttck->isforeign=isforeign;\r\n\ttck->lastUsed=++tc->frees;\r\n\ttck->size=(unsigned int) size;\r\n\ttck->next=*binsptr;\r\n\ttck->prev=0;\r\n\tif(tck->next)\r\n\t\ttck->next->prev=tck;\r\n\telse\r\n\t\tbinsptr[1]=tck;\r\n\tassert(!*binsptr || (*binsptr)->size==tck->size);\r\n\t*binsptr=tck;\r\n\tassert(tck==tc->bins[idx*2]);\r\n\tassert(tc->bins[idx*2+1]==tck || binsptr[0]->next->prev==tck);\r\n\t/*printf(\"free: %p, %p, %p, %lu\\n\", p, tc, mem, (long) size);*/\r\n\ttc->freeInCache+=size;\r\n#ifdef FULLSANITYCHECKS\r\n\ttcfullsanitycheck(tc);\r\n#endif\r\n#if 1\r\n\tif(tc->freeInCache>=THREADCACHEMAXFREESPACE)\r\n\t\tReleaseFreeInCache(p, tc, mymspace);\r\n#endif\r\n}\r\n\r\n\r\n\r\n\r\nstatic NOINLINE int InitPool(nedpool *RESTRICT p, size_t capacity, int threads) THROWSPEC\r\n{\t/* threads is -1 for system pool */\r\n\tensure_initialization();\r\n\tACQUIRE_MALLOC_GLOBAL_LOCK();\r\n\tif(p->threads) goto done;\r\n#if USE_LOCKS\r\n\tif(INITIAL_LOCK(&p->mutex)) goto err;\r\n#endif\r\n\tif(TLSALLOC(&p->mycache)) goto err;\r\n#if USE_ALLOCATOR==0\r\n\tp->m[0]=(mstate) mspacecounter++;\r\n#elif USE_ALLOCATOR==1\r\n\tif(!(p->m[0]=(mstate) create_mspace(capacity, 1))) goto err;\r\n\tp->m[0]->extp=p;\r\n#endif\r\n\tp->threads=(threads>MAXTHREADSINPOOL) ? MAXTHREADSINPOOL : (!threads) ? DEFAULTMAXTHREADSINPOOL : threads;\r\ndone:\r\n\tRELEASE_MALLOC_GLOBAL_LOCK();\r\n\treturn 1;\r\nerr:\r\n\tif(threads<0)\r\n\t\tabort();\t\t\t/* If you can't allocate for system pool, we're screwed */\r\n\tDestroyCaches(p);\r\n\tif(p->m[0])\r\n\t{\r\n#if USE_ALLOCATOR==1\r\n\t\tdestroy_mspace(p->m[0]);\r\n#endif\r\n\t\tp->m[0]=0;\r\n\t}\r\n\tif(p->mycache)\r\n\t{\r\n\t\tif(TLSFREE(p->mycache)) abort();\r\n\t\tp->mycache=0;\r\n\t}\r\n\tRELEASE_MALLOC_GLOBAL_LOCK();\r\n\treturn 0;\r\n}\r\nstatic NOINLINE mstate FindMSpace(nedpool *RESTRICT p, threadcache *RESTRICT tc, int *RESTRICT lastUsed, size_t size) THROWSPEC\r\n{\t/* Gets called when thread's last used mspace is in use. The strategy\r\n\tis to run through the list of all available mspaces looking for an\r\n\tunlocked one and if we fail, we create a new one so long as we don't\r\n\texceed p->threads */\r\n\tint n, end;\r\n\tn=end=*lastUsed+1;\r\n#if USE_LOCKS\r\n\tfor(; p->m[n]; end=++n)\r\n\t{\r\n\t\tif(TRY_LOCK(&p->m[n]->mutex)) goto found;\r\n\t}\r\n\tfor(n=0; n<*lastUsed && p->m[n]; n++)\r\n\t{\r\n\t\tif(TRY_LOCK(&p->m[n]->mutex)) goto found;\r\n\t}\r\n\tif(end<p->threads)\r\n\t{\r\n\t\tmstate temp;\r\n#if USE_ALLOCATOR==0\r\n\t\ttemp=(mstate) mspacecounter++;\r\n#elif USE_ALLOCATOR==1\r\n\t\tif(!(temp=(mstate) create_mspace(size, 1)))\r\n\t\t\tgoto badexit;\r\n#endif\r\n\t\t/* Now we're ready to modify the lists, we lock */\r\n\t\tACQUIRE_LOCK(&p->mutex);\r\n\t\twhile(p->m[end] && end<p->threads)\r\n\t\t\tend++;\r\n\t\tif(end>=p->threads)\r\n\t\t{\t/* Drat, must destroy it now */\r\n\t\t\tRELEASE_LOCK(&p->mutex);\r\n#if USE_ALLOCATOR==1\r\n\t\t\tdestroy_mspace((mstate) temp);\r\n#endif\r\n\t\t\tgoto badexit;\r\n\t\t}\r\n\t\t/* We really want to make sure this goes into memory now but we\r\n\t\thave to be careful of breaking aliasing rules, so write it twice */\r\n\t\t{\r\n\t\t\tvolatile struct malloc_state **_m=(volatile struct malloc_state **) &p->m[end];\r\n\t\t\t*_m=(p->m[end]=temp);\r\n\t\t}\r\n\t\tACQUIRE_LOCK(&p->m[end]->mutex);\r\n\t\t/*printf(\"Created mspace idx %d\\n\", end);*/\r\n\t\tRELEASE_LOCK(&p->mutex);\r\n\t\tn=end;\r\n\t\tgoto found;\r\n\t}\r\n\t/* Let it lock on the last one it used */\r\nbadexit:\r\n\tACQUIRE_LOCK(&p->m[*lastUsed]->mutex);\r\n\treturn p->m[*lastUsed];\r\n#endif\r\nfound:\r\n\t*lastUsed=n;\r\n\tif(tc)\r\n\t\ttc->mymspace=n;\r\n\telse\r\n\t{\r\n\t\tif(TLSSET(p->mycache, (void *)(size_t)(-(n+1)))) abort();\r\n\t}\r\n\treturn p->m[n];\r\n}\r\n\r\ntypedef struct PoolList_t\r\n{\r\n\tsize_t size;\t\t\t/* Size of list */\r\n\tsize_t length;\t\t\t/* Actual entries in list */\r\n#ifdef DEBUG\r\n\tnedpool *list[1];\t\t/* Force testing of list expansion */\r\n#else\r\n\tnedpool *list[16];\r\n#endif\r\n} PoolList;\r\n#if USE_LOCKS\r\nstatic MLOCK_T poollistlock;\r\n#endif\r\nstatic PoolList *poollist;\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR nedpool *nedcreatepool(size_t capacity, int threads) THROWSPEC\r\n{\r\n\tnedpool *ret=0;\r\n\tif(!poollist)\r\n\t{\r\n\t\tPoolList *newpoollist=0;\r\n\t\tif(!(newpoollist=(PoolList *) nedpcalloc(0, 1, sizeof(PoolList)+sizeof(nedpool *)))) return 0;\r\n#if USE_LOCKS\r\n\t\tINITIAL_LOCK(&poollistlock);\r\n\t\tACQUIRE_LOCK(&poollistlock);\r\n#endif\r\n\t\tpoollist=newpoollist;\r\n\t\tpoollist->size=sizeof(poollist->list)/sizeof(nedpool *);\r\n\t}\r\n#if USE_LOCKS\r\n\telse\r\n\t\tACQUIRE_LOCK(&poollistlock);\r\n#endif\r\n\tif(poollist->length==poollist->size)\r\n\t{\r\n\t\tPoolList *newpoollist=0;\r\n\t\tsize_t newsize=0;\r\n\t\tnewsize=sizeof(PoolList)+(poollist->size+1)*sizeof(nedpool *);\r\n\t\tif(!(newpoollist=(PoolList *) nedprealloc(0, poollist, newsize))) goto badexit;\r\n\t\tpoollist=newpoollist;\r\n\t\tmemset(&poollist->list[poollist->size], 0, newsize-((size_t)&poollist->list[poollist->size]-(size_t)&poollist->list[0]));\r\n\t\tpoollist->size=((newsize-((char *)&poollist->list[0]-(char *)poollist))/sizeof(nedpool *))-1;\r\n\t\tassert(poollist->size>poollist->length);\r\n\t}\r\n\tif(!(ret=(nedpool *) nedpcalloc(0, 1, sizeof(nedpool)))) goto badexit;\r\n\tif(!InitPool(ret, capacity, threads))\r\n\t{\r\n\t\tnedpfree(0, ret);\r\n\t\tgoto badexit;\r\n\t}\r\n\tpoollist->list[poollist->length++]=ret;\r\nbadexit:\r\n\t{\r\n#if USE_LOCKS\r\n\t\tRELEASE_LOCK(&poollistlock);\r\n#endif\r\n\t}\r\n\treturn ret;\r\n}\r\nvoid neddestroypool(nedpool *p) THROWSPEC\r\n{\r\n\tunsigned int n;\r\n#if USE_LOCKS\r\n\tACQUIRE_LOCK(&p->mutex);\r\n#endif\r\n\tDestroyCaches(p);\r\n\tfor(n=0; p->m[n]; n++)\r\n\t{\r\n#if USE_ALLOCATOR==1\r\n\t\tdestroy_mspace(p->m[n]);\r\n#endif\r\n\t\tp->m[n]=0;\r\n\t}\r\n#if USE_LOCKS\r\n\tRELEASE_LOCK(&p->mutex);\r\n\tDESTROY_LOCK(&p->mutex);\r\n#endif\r\n\tif(TLSFREE(p->mycache)) abort();\r\n\tnedpfree(0, p);\r\n#if USE_LOCKS\r\n\tACQUIRE_LOCK(&poollistlock);\r\n#endif\r\n\tassert(poollist);\r\n\tfor(n=0; n<poollist->length && poollist->list[n]!=p; n++);\r\n\tassert(n!=poollist->length);\r\n\tmemmove(&poollist->list[n], &poollist->list[n+1], (size_t)&poollist->list[poollist->length]-(size_t)&poollist->list[n]);\r\n\tif(!--poollist->length)\r\n\t{\r\n\t\tassert(!poollist->list[0]);\r\n\t\tnedpfree(0, poollist);\r\n\t\tpoollist=0;\r\n\t}\r\n#if USE_LOCKS\r\n\tRELEASE_LOCK(&poollistlock);\r\n#endif\r\n}\r\nvoid neddestroysyspool() THROWSPEC\r\n{\r\n\tnedpool *p=&syspool;\r\n\tint n;\r\n#if USE_LOCKS\r\n\tACQUIRE_LOCK(&p->mutex);\r\n#endif\r\n\tDestroyCaches(p);\r\n\tfor(n=0; p->m[n]; n++)\r\n\t{\r\n#if USE_ALLOCATOR==1\r\n\t\tdestroy_mspace(p->m[n]);\r\n#endif\r\n\t\tp->m[n]=0;\r\n\t}\r\n\t/* Render syspool unusable */\r\n\tfor(n=0; n<THREADCACHEMAXCACHES; n++)\r\n\t\tp->caches[n]=(threadcache *)(size_t)(sizeof(size_t)>4 ? 0xdeadbeefdeadbeefULL : 0xdeadbeefUL);\r\n\tfor(n=0; n<MAXTHREADSINPOOL+1; n++)\r\n\t\tp->m[n]=(mstate)(size_t)(sizeof(size_t)>4 ? 0xdeadbeefdeadbeefULL : 0xdeadbeefUL);\r\n\tif(TLSFREE(p->mycache)) abort();\r\n#if USE_LOCKS\r\n\tRELEASE_LOCK(&p->mutex);\r\n\tDESTROY_LOCK(&p->mutex);\r\n#endif\r\n}\r\nnedpool **nedpoollist() THROWSPEC\r\n{\r\n\tnedpool **ret=0;\r\n\tif(poollist)\r\n\t{\r\n#if USE_LOCKS\r\n\t\tACQUIRE_LOCK(&poollistlock);\r\n#endif\r\n\t\tif(!(ret=(nedpool **) nedmalloc((poollist->length+1)*sizeof(nedpool *)))) goto badexit;\r\n\t\tmemcpy(ret, poollist->list, (poollist->length+1)*sizeof(nedpool *));\r\nbadexit:\r\n\t\t{\r\n#if USE_LOCKS\r\n\t\t\tRELEASE_LOCK(&poollistlock);\r\n#endif\r\n\t\t}\r\n\t}\r\n\treturn ret;\r\n}\r\n\r\nvoid nedpsetvalue(nedpool *p, void *v) THROWSPEC\r\n{\r\n\tif(!p) { p=&syspool; if(!syspool.threads) InitPool(&syspool, 0, -1); }\r\n\tp->uservalue=v;\r\n}\r\nvoid *nedgetvalue(nedpool **p, void *mem) THROWSPEC\r\n{\r\n\tnedpool *np=0;\r\n\tmstate fm=nedblkmstate(mem);\r\n\tif(!fm || !fm->extp) return 0;\r\n\tnp=(nedpool *) fm->extp;\r\n\tif(p) *p=np;\r\n\treturn np->uservalue;\r\n}\r\n\r\nvoid nedtrimthreadcache(nedpool *p, int disable) THROWSPEC\r\n{\r\n\tint mycache;\r\n\tif(!p)\r\n\t{\r\n\t\tp=&syspool;\r\n\t\tif(!syspool.threads) InitPool(&syspool, 0, -1);\r\n\t}\r\n\tmycache=(int)(size_t) TLSGET(p->mycache);\r\n\tif(!mycache)\r\n\t{\t/* Set to mspace 0 */\r\n\t\tif(disable && TLSSET(p->mycache, (void *)(size_t)-1)) abort();\r\n\t}\r\n\telse if(mycache>0)\r\n\t{\t/* Set to last used mspace */\r\n\t\tthreadcache *tc=p->caches[mycache-1];\r\n#if defined(DEBUG)\r\n\t\tprintf(\"Threadcache utilisation: %lf%% in cache with %lf%% lost to other threads\\n\",\r\n\t\t\t100.0*tc->successes/tc->mallocs, 100.0*((double) tc->mallocs-tc->frees)/tc->mallocs);\r\n#endif\r\n\t\tif(disable && TLSSET(p->mycache, (void *)(size_t)(-tc->mymspace))) abort();\r\n\t\ttc->frees++;\r\n\t\tRemoveCacheEntries(p, tc, 0);\r\n\t\tassert(!tc->freeInCache);\r\n\t\tif(disable)\r\n\t\t{\r\n\t\t\ttc->mymspace=-1;\r\n\t\t\ttc->threadid=0;\r\n\t\t\tCallFree(0, p->caches[mycache-1], 0);\r\n\t\t\tp->caches[mycache-1]=0;\r\n\t\t}\r\n\t}\r\n}\r\nvoid neddisablethreadcache(nedpool *p) THROWSPEC\r\n{\r\n\tnedtrimthreadcache(p, 1);\r\n}\r\n\r\n#if USE_LOCKS && USE_ALLOCATOR==1\r\n#define GETMSPACE(m,p,tc,ms,s,action)                 \\\r\n  do                                                  \\\r\n  {                                                   \\\r\n    mstate m = GetMSpace((p),(tc),(ms),(s));          \\\r\n    action;                                           \\\r\n\tRELEASE_LOCK(&m->mutex);                          \\\r\n  } while (0)\r\n#else\r\n#define GETMSPACE(m,p,tc,ms,s,action)                 \\\r\n  do                                                  \\\r\n  {                                                   \\\r\n    mstate m = GetMSpace((p),(tc),(ms),(s));          \\\r\n    action;                                           \\\r\n  } while (0)\r\n#endif\r\n\r\nstatic FORCEINLINE mstate GetMSpace(nedpool *RESTRICT p, threadcache *RESTRICT tc, int mymspace, size_t size) THROWSPEC\r\n{\t/* Returns a locked and ready for use mspace */\r\n\tmstate m=p->m[mymspace];\r\n\tassert(m);\r\n#if USE_LOCKS && USE_ALLOCATOR==1\r\n\tif(!TRY_LOCK(&p->m[mymspace]->mutex)) m=FindMSpace(p, tc, &mymspace, size);\r\n\t/*assert(IS_LOCKED(&p->m[mymspace]->mutex));*/\r\n#endif\r\n\treturn m;\r\n}\r\nstatic NOINLINE void GetThreadCache_cold1(nedpool *RESTRICT *RESTRICT p) THROWSPEC\r\n{\r\n\t*p=&syspool;\r\n\tif(!syspool.threads) InitPool(&syspool, 0, -1);\r\n}\r\nstatic NOINLINE void GetThreadCache_cold2(nedpool *RESTRICT *RESTRICT p, threadcache *RESTRICT *RESTRICT tc, int *RESTRICT mymspace, int mycache) THROWSPEC\r\n{\r\n\tif(!mycache)\r\n\t{\t/* Need to allocate a new cache */\r\n\t\t*tc=AllocCache(*p);\r\n\t\tif(!*tc)\r\n\t\t{\t/* Disable */\r\n\t\t\tif(TLSSET((*p)->mycache, (void *)(size_t)-1)) abort();\r\n\t\t\t*mymspace=0;\r\n\t\t}\r\n\t\telse\r\n\t\t\t*mymspace=(*tc)->mymspace;\r\n\t}\r\n\telse\r\n\t{\t/* Cache disabled, but we do have an assigned thread pool */\r\n\t\t*tc=0;\r\n\t\t*mymspace=-mycache-1;\r\n\t}\r\n}\r\nstatic FORCEINLINE void GetThreadCache(nedpool *RESTRICT *RESTRICT p, threadcache *RESTRICT *RESTRICT tc, int *RESTRICT mymspace, size_t *RESTRICT size) THROWSPEC\r\n{\r\n\tint mycache;\r\n#if THREADCACHEMAX\r\n\tif(size && *size<sizeof(threadcacheblk)) *size=sizeof(threadcacheblk);\r\n#endif\r\n\tif(!*p)\r\n\t\tGetThreadCache_cold1(p);\r\n\tmycache=(int)(size_t) TLSGET((*p)->mycache);\r\n\tif(mycache>0)\r\n\t{\t/* Already have a cache */\r\n\t\t*tc=(*p)->caches[mycache-1];\r\n\t\t*mymspace=(*tc)->mymspace;\r\n\t}\r\n\telse GetThreadCache_cold2(p, tc, mymspace, mycache);\r\n\tassert(*mymspace>=0);\r\n#if USE_LOCKS\r\n\tassert(!(*tc) || (long)(size_t)CURRENT_THREAD==(*tc)->threadid);\r\n#endif\r\n#ifdef FULLSANITYCHECKS\r\n\tif(*tc)\r\n\t{\r\n\t\tif(*(unsigned int *)\"NEDMALC1\"!=(*tc)->magic1 || *(unsigned int *)\"NEDMALC2\"!=(*tc)->magic2)\r\n\t\t{\r\n\t\t\tabort();\r\n\t\t}\r\n\t}\r\n#endif\r\n}\r\n\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedpmalloc2(nedpool *p, size_t size, size_t alignment, unsigned flags) THROWSPEC\r\n{\r\n\tvoid *ret=0;\r\n\tthreadcache *tc;\r\n\tint mymspace;\r\n\tGetThreadCache(&p, &tc, &mymspace, &size);\r\n#if THREADCACHEMAX\r\n\tif(alignment<=MALLOC_ALIGNMENT && !(flags & NM_FLAGS_MASK) && tc && size<=THREADCACHEMAX)\r\n\t{\t/* Use the thread cache */\r\n\t\tif((ret=threadcache_malloc(p, tc, &size)))\r\n\t\t{\r\n\t\t\tif((flags & M2_ZERO_MEMORY))\r\n\t\t\t\tmemset(ret, 0, size);\r\n\t\t\tLogOperation(tc, p, LOGENTRY_THREADCACHE_MALLOC, mymspace, size, 0, alignment, flags, ret);\r\n\t\t}\r\n\t}\r\n#endif\r\n\tif(!ret)\r\n\t{\t/* Use this thread's mspace */\r\n        GETMSPACE(m, p, tc, mymspace, size,\r\n                  ret=CallMalloc(m, size, alignment, flags));\r\n\t\tif(ret)\r\n\t\t\tLogOperation(tc, p, LOGENTRY_POOL_MALLOC, mymspace, size, 0, alignment, flags, ret);\r\n\t}\r\n\tLogOperation(tc, p, LOGENTRY_MALLOC, mymspace, size, 0, alignment, flags, ret);\r\n\treturn ret;\r\n}\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedprealloc2(nedpool *p, void *mem, size_t size, size_t alignment, unsigned flags) THROWSPEC\r\n{\r\n\tvoid *ret=0;\r\n\tthreadcache *tc;\r\n\tint mymspace, isforeign=1;\r\n\tsize_t memsize;\r\n\tif(!mem) return nedpmalloc2(p, size, alignment, flags);\r\n#if REALLOC_ZERO_BYTES_FREES\r\n\tif(!size)\r\n\t{\r\n\t\tnedpfree2(p, mem, flags);\r\n\t\treturn 0;\r\n\t}\r\n#endif\r\n\tmemsize=nedblksize(&isforeign, mem, flags);\r\n\tassert(memsize);\r\n\tif(!memsize)\r\n\t{\r\n\t\tfprintf(stderr, \"nedmalloc: nedprealloc() called with a block not created by nedmalloc!\\n\");\r\n\t\tabort();\r\n\t}\r\n\telse if(size<=memsize && memsize-size<\r\n#ifdef DEBUG\r\n\t\t32\r\n#else\r\n\t\t1024\r\n#endif\r\n\t\t)\t\t/* If realloc size is within 1Kb smaller than existing, noop it */\r\n\t\treturn mem;\r\n\tGetThreadCache(&p, &tc, &mymspace, &size);\r\n#if THREADCACHEMAX\r\n\tif(alignment<=MALLOC_ALIGNMENT && !(flags & NM_FLAGS_MASK) && tc && size && size<=THREADCACHEMAX)\r\n\t{\t/* Use the thread cache */\r\n\t\tif((ret=threadcache_malloc(p, tc, &size)))\r\n\t\t{\r\n\t\t\tsize_t tocopy=memsize<size ? memsize : size;\r\n\t\t\tmemcpy(ret, mem, tocopy);\r\n\t\t\tif((flags & M2_ZERO_MEMORY) && size>memsize)\r\n\t\t\t\tmemset((void *)((size_t)ret+memsize), 0, size-memsize);\r\n\t\t\tLogOperation(tc, p, LOGENTRY_THREADCACHE_MALLOC, mymspace, size, mem, alignment, flags, ret);\r\n\t\t\tif(!isforeign && memsize>=sizeof(threadcacheblk) && memsize<=(THREADCACHEMAX+CHUNK_OVERHEAD))\r\n\t\t\t{\r\n\t\t\t\tthreadcache_free(p, tc, mymspace, mem, memsize, isforeign);\r\n\t\t\t\tLogOperation(tc, p, LOGENTRY_THREADCACHE_FREE, mymspace, memsize, mem, 0, 0, 0);\r\n\t\t\t}\r\n\t\t\telse\r\n\t\t\t{\r\n\t\t\t\tCallFree(0, mem, isforeign);\r\n\t\t\t\tLogOperation(tc, p, LOGENTRY_POOL_FREE, mymspace, memsize, mem, 0, 0, 0);\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n#endif\r\n\tif(!ret)\r\n\t{\t/* Reallocs always happen in the mspace they happened in, so skip\r\n\t\tlocking the preferred mspace for this thread */\r\n\t\tret=CallRealloc(p->m[mymspace], mem, isforeign, memsize, size, alignment, flags);\r\n\t\tif(ret)\r\n\t\t\tLogOperation(tc, p, LOGENTRY_POOL_REALLOC, mymspace, size, mem, alignment, flags, ret);\r\n\t}\r\n\tLogOperation(tc, p, LOGENTRY_REALLOC, mymspace, size, mem, alignment, flags, ret);\r\n\treturn ret;\r\n}\r\nNEDMALLOCNOALIASATTR void   nedpfree2(nedpool *p, void *mem, unsigned flags) THROWSPEC\r\n{\t/* Frees always happen in the mspace they happened in, so skip\r\n\tlocking the preferred mspace for this thread */\r\n\tthreadcache *tc;\r\n\tint mymspace, isforeign=1;\r\n\tsize_t memsize;\r\n\tif(!mem)\r\n\t{\t/* If you tried this on FreeBSD you'd be sorry! */\r\n#ifdef DEBUG\r\n\t\tfprintf(stderr, \"nedmalloc: WARNING nedpfree() called with zero. This is not portable behaviour!\\n\");\r\n#endif\r\n\t\treturn;\r\n\t}\r\n\tmemsize=nedblksize(&isforeign, mem, flags);\r\n\tassert(memsize);\r\n\tif(!memsize)\r\n\t{\r\n\t\tfprintf(stderr, \"nedmalloc: nedpfree() called with a block not created by nedmalloc!\\n\");\r\n\t\tabort();\r\n\t}\r\n\tGetThreadCache(&p, &tc, &mymspace, 0);\r\n#if THREADCACHEMAX\r\n\tif(mem && tc && !isforeign && memsize>=sizeof(threadcacheblk) && memsize<=(THREADCACHEMAX+CHUNK_OVERHEAD))\r\n\t{\r\n\t\tthreadcache_free(p, tc, mymspace, mem, memsize, isforeign);\r\n\t\tLogOperation(tc, p, LOGENTRY_THREADCACHE_FREE, mymspace, memsize, mem, 0, 0, 0);\r\n\t}\r\n\telse\r\n#endif\r\n\t{\r\n\t\tCallFree(0, mem, isforeign);\r\n\t\tLogOperation(tc, p, LOGENTRY_POOL_FREE, mymspace, memsize, mem, 0, 0, 0);\r\n\t}\r\n\tLogOperation(tc, p, LOGENTRY_FREE, mymspace, memsize, mem, 0, 0, 0);\r\n}\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedpmalloc(nedpool *p, size_t size) THROWSPEC\r\n{\r\n\tunsigned flags=NEDMALLOC_FORCERESERVE(p, 0, size);\r\n\treturn nedpmalloc2(p, size, 0, flags);\r\n}\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedpcalloc(nedpool *p, size_t no, size_t size) THROWSPEC\r\n{\r\n\tsize_t bytes=no*size;\r\n\t/* Avoid multiplication overflow. */\r\n\tif(size && no!=bytes/size)\r\n\t\treturn 0;\r\n\tunsigned flags=NEDMALLOC_FORCERESERVE(p, 0, bytes);\r\n\treturn nedpmalloc2(p, bytes, 0, M2_ZERO_MEMORY|flags);\r\n}\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedprealloc(nedpool *p, void *mem, size_t size) THROWSPEC\r\n{\r\n\tunsigned flags=NEDMALLOC_FORCERESERVE(p, mem, size);\r\n#if ENABLE_USERMODEPAGEALLOCATOR\r\n\t/* If the user mode page allocator is turned on in a 32 bit process,\r\n\tdon't automatically reserve eight times the address space. */\r\n\tif(8==sizeof(size_t) || !OSHavePhysicalPageSupport())\r\n#endif\r\n\t{\t/* If he reallocs even once, it's probably wise to turn on address space reservation.\r\n\t\tIf the size is larger than mmap_threshold then it'll set the reserve. */\r\n\t\tif(!(flags & M2_RESERVE_MASK)) flags=M2_RESERVE_MULT(8);\r\n\t}\r\n\treturn nedprealloc2(p, mem, size, 0, flags);\r\n}\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedpmemalign(nedpool *p, size_t alignment, size_t bytes) THROWSPEC\r\n{\r\n\tunsigned flags=NEDMALLOC_FORCERESERVE(p, 0, bytes);\r\n\treturn nedpmalloc2(p, bytes, alignment, flags);\r\n}\r\nNEDMALLOCNOALIASATTR void   nedpfree(nedpool *p, void *mem) THROWSPEC\r\n{\r\n  nedpfree2(p, mem, 0);\r\n}\r\n\r\nstruct nedmallinfo nedpmallinfo(nedpool *p) THROWSPEC\r\n{\r\n\tint n;\r\n\tstruct nedmallinfo ret={0};\r\n\tif(!p) { p=&syspool; if(!syspool.threads) InitPool(&syspool, 0, -1); }\r\n\tfor(n=0; p->m[n]; n++)\r\n\t{\r\n#if USE_ALLOCATOR==1 && !NO_MALLINFO\r\n\t\tstruct mallinfo t=mspace_mallinfo(p->m[n]);\r\n\t\tret.arena+=t.arena;\r\n\t\tret.ordblks+=t.ordblks;\r\n\t\tret.hblkhd+=t.hblkhd;\r\n\t\tret.usmblks+=t.usmblks;\r\n\t\tret.uordblks+=t.uordblks;\r\n\t\tret.fordblks+=t.fordblks;\r\n\t\tret.keepcost+=t.keepcost;\r\n#endif\r\n\t}\r\n\treturn ret;\r\n}\r\nint    nedpmallopt(nedpool *p, int parno, int value) THROWSPEC\r\n{\r\n#if USE_ALLOCATOR==1\r\n\treturn mspace_mallopt(parno, value);\r\n#else\r\n\treturn 0;\r\n#endif\r\n}\r\nNEDMALLOCNOALIASATTR void*  nedmalloc_internals(size_t *granularity, size_t *magic) THROWSPEC\r\n{\r\n#if USE_ALLOCATOR==1\r\n\tif(granularity) *granularity=mparams.granularity;\r\n\tif(magic) *magic=mparams.magic;\r\n\treturn (void *) &syspool;\r\n#else\r\n\tif(granularity) *granularity=0;\r\n\tif(magic) *magic=0;\r\n\treturn 0;\r\n#endif\r\n}\r\nint    nedpmalloc_trim(nedpool *p, size_t pad) THROWSPEC\r\n{\r\n\tint n, ret=0;\r\n\tif(!p) { p=&syspool; if(!syspool.threads) InitPool(&syspool, 0, -1); }\r\n\tfor(n=0; p->m[n]; n++)\r\n\t{\r\n#if USE_ALLOCATOR==1\r\n\t\tret+=mspace_trim(p->m[n], pad);\r\n#endif\r\n\t}\r\n\treturn ret;\r\n}\r\nvoid   nedpmalloc_stats(nedpool *p) THROWSPEC\r\n{\r\n\tint n;\r\n\tif(!p) { p=&syspool; if(!syspool.threads) InitPool(&syspool, 0, -1); }\r\n\tfor(n=0; p->m[n]; n++)\r\n\t{\r\n#if USE_ALLOCATOR==1\r\n\t\tmspace_malloc_stats(p->m[n]);\r\n#endif\r\n\t}\r\n}\r\nsize_t nedpmalloc_footprint(nedpool *p) THROWSPEC\r\n{\r\n\tsize_t ret=0;\r\n\tint n;\r\n\tif(!p) { p=&syspool; if(!syspool.threads) InitPool(&syspool, 0, -1); }\r\n\tfor(n=0; p->m[n]; n++)\r\n\t{\r\n#if USE_ALLOCATOR==1\r\n\t\tret+=mspace_footprint(p->m[n]);\r\n#endif\r\n\t}\r\n\treturn ret;\r\n}\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void **nedpindependent_calloc(nedpool *p, size_t elemsno, size_t elemsize, void **chunks) THROWSPEC\r\n{\r\n\tvoid **ret;\r\n\tthreadcache *tc;\r\n\tint mymspace;\r\n\tGetThreadCache(&p, &tc, &mymspace, &elemsize);\r\n#if USE_ALLOCATOR==0\r\n    GETMSPACE(m, p, tc, mymspace, elemsno*elemsize,\r\n              ret=unsupported_operation(\"independent_calloc\"));\r\n#elif USE_ALLOCATOR==1\r\n    GETMSPACE(m, p, tc, mymspace, elemsno*elemsize,\r\n              ret=mspace_independent_calloc(m, elemsno, elemsize, chunks));\r\n#endif\r\n#if ENABLE_LOGGING\r\n\tif(ret && (ENABLE_LOGGING & LOGENTRY_POOL_MALLOC))\r\n\t{\r\n\t\tsize_t n;\r\n\t\tfor(n=0; n<elemsno; n++)\r\n\t\t{\r\n\t\t\tLogOperation(tc, p, LOGENTRY_POOL_MALLOC, mymspace, elemsize, 0, 0, M2_ZERO_MEMORY, ret[n]);\r\n\t\t}\r\n\t}\r\n#endif\r\n\treturn ret;\r\n}\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void **nedpindependent_comalloc(nedpool *p, size_t elems, size_t *sizes, void **chunks) THROWSPEC\r\n{\r\n\tvoid **ret;\r\n\tthreadcache *tc;\r\n\tint mymspace;\r\n    size_t i, *adjustedsizes=(size_t *) alloca(elems*sizeof(size_t));\r\n    if(!adjustedsizes) return 0;\r\n    for(i=0; i<elems; i++)\r\n        adjustedsizes[i]=sizes[i]<sizeof(threadcacheblk) ? sizeof(threadcacheblk) : sizes[i];\r\n\tGetThreadCache(&p, &tc, &mymspace, 0);\r\n#if USE_ALLOCATOR==0\r\n\tGETMSPACE(m, p, tc, mymspace, 0,\r\n              ret=unsupported_operation(\"independent_comalloc\"));\r\n#elif USE_ALLOCATOR==1\r\n\tGETMSPACE(m, p, tc, mymspace, 0,\r\n              ret=mspace_independent_comalloc(m, elems, adjustedsizes, chunks));\r\n#endif\r\n#if ENABLE_LOGGING\r\n\tif(ret && (ENABLE_LOGGING & LOGENTRY_POOL_MALLOC))\r\n\t{\r\n\t\tsize_t n;\r\n\t\tfor(n=0; n<elems; n++)\r\n\t\t{\r\n\t\t\tLogOperation(tc, p, LOGENTRY_POOL_MALLOC, mymspace, sizes[n], 0, 0, 0, ret[n]);\r\n\t\t}\r\n\t}\r\n#endif\r\n\treturn ret;\r\n}\r\n\r\n#if defined(__cplusplus)\r\n}\r\n#endif\r\n\r\n#ifdef _MSC_VER\r\n#pragma warning(pop)\r\n#endif\r\n"], "fixing_code": ["/* Alternative malloc implementation for multiple threads without\r\nlock contention based on dlmalloc. (C) 2005-2010 Niall Douglas\r\n\r\nBoost Software License - Version 1.0 - August 17th, 2003\r\n\r\nPermission is hereby granted, free of charge, to any person or organization\r\nobtaining a copy of the software and accompanying documentation covered by\r\nthis license (the \"Software\") to use, reproduce, display, distribute,\r\nexecute, and transmit the Software, and to prepare derivative works of the\r\nSoftware, and to permit third-parties to whom the Software is furnished to\r\ndo so, all subject to the following:\r\n\r\nThe copyright notices in the Software and this entire statement, including\r\nthe above license grant, this restriction and the following disclaimer,\r\nmust be included in all copies of the Software, in whole or in part, and\r\nall derivative works of the Software, unless such copies or derivative\r\nworks are solely in the form of machine-executable object code generated by\r\na source language processor.\r\n\r\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\nFITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT\r\nSHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE\r\nFOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,\r\nARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\r\nDEALINGS IN THE SOFTWARE.\r\n*/\r\n\r\n#if 0 /* Effectively makes nedmalloc = dlmalloc */\r\n#define THREADCACHEMAX 0\r\n#define DEFAULT_GRANULARITY 65536\r\n#define DEFAULTMAXTHREADSINPOOL 1\r\n#endif\r\n\r\n#ifdef _MSC_VER\r\n/* Enable full aliasing on MSVC */\r\n/*#pragma optimize(\"a\", on)*/\r\n/*#pragma optimize(\"g\", off)*/\r\n\r\n#pragma warning(push)\r\n#pragma warning(disable:4100)\t/* unreferenced formal parameter */\r\n#pragma warning(disable:4127)\t/* conditional expression is constant */\r\n#pragma warning(disable:4232)\t/* address of dllimport is not static, identity not guaranteed */\r\n#pragma warning(disable:4706)\t/* assignment within conditional expression */\r\n\r\n#define _CRT_SECURE_NO_WARNINGS 1\t/* Don't care about MSVC warnings on POSIX functions */\r\n#include <stdio.h>\r\n#define fopen(f, m) _fsopen((f), (m), 0x40/*_SH_DENYNO*/)\t/* Have Windows let other programs view the log file as it is written */\r\n#ifndef UNICODE\r\n#define UNICODE\t\t\t\t\t/* Turn on windows unicode support */\r\n#endif\r\n#else\r\n#include <stdio.h>\r\n#endif\r\n\r\n/*#define NEDMALLOC_DEBUG 1*/\r\n/*#define ENABLE_LOGGING 7\r\n#define NEDMALLOC_TESTLOGENTRY(tc, np, type, mspace, size, mem, alignment, flags, returned) (((type)&ENABLE_LOGGING)&&((size)>16*1024))\r\n#define NEDMALLOC_STACKBACKTRACEDEPTH 16*/\r\n/*#define NEDMALLOC_FORCERESERVE(p, mem, size) (((size)>=(256*1024)) ? M2_RESERVE_MULT(8) : 0)*/\r\n/*#define WIN32_DIRECT_USE_FILE_MAPPINGS 0*/\r\n\r\n/*#define NEDMALLOC_DEBUG 1*/\r\n\r\n/*#define FULLSANITYCHECKS*/\r\n\r\n/* There is only support for the user mode page allocator on Windows at present */\r\n#if !defined(ENABLE_USERMODEPAGEALLOCATOR)\r\n#define ENABLE_USERMODEPAGEALLOCATOR 0\r\n#endif\r\n\r\n/* If link time code generation is on, don't force or prevent inlining */\r\n#if defined(_MSC_VER) && defined(NEDMALLOC_DLL_EXPORTS)\r\n#define FORCEINLINE\r\n#define NOINLINE\r\n#endif\r\n\r\n#include \"nedmalloc.h\"\r\n#include <errno.h>\r\n#if defined(WIN32)\r\n #include <malloc.h>\r\n#else\r\n#if defined(__cplusplus)\r\nextern \"C\"\r\n#else\r\nextern\r\n#endif\r\n#if defined(__linux__) || defined(__FreeBSD__)\r\n/* Sadly we can't include <malloc.h> as it causes a redefinition error */\r\nsize_t malloc_usable_size(void *);\r\n#elif defined(__APPLE__)\r\nsize_t malloc_size(const void *ptr);\r\n#else\r\n#error Do not know what to do here\r\n#endif\r\n#endif\r\n\r\n#if USE_ALLOCATOR==1\r\n #define MSPACES 1\r\n #define ONLY_MSPACES 1\r\n#endif\r\n#define USE_DL_PREFIX 1\r\n#ifndef USE_LOCKS\r\n #define USE_LOCKS 1\r\n#endif\r\n#define FOOTERS 1           /* Need to enable footers so frees lock the right mspace */\r\n#ifndef NEDMALLOC_DEBUG\r\n #if defined(DEBUG) || defined(_DEBUG)\r\n  #define NEDMALLOC_DEBUG 1\r\n #else\r\n  #define NEDMALLOC_DEBUG 0\r\n #endif\r\n#endif\r\n/* We need to consistently define DEBUG=0|1, _DEBUG and NDEBUG for dlmalloc */\r\n#if !defined(DEBUG) && !defined(NDEBUG)\r\n #ifdef __GNUC__\r\n  #warning DEBUG may not be defined but without NDEBUG being defined allocator will run with assert checking! Define NDEBUG to run at full speed.\r\n #elif defined(_MSC_VER)\r\n  #pragma message(__FILE__ \": WARNING: DEBUG may not be defined but without NDEBUG being defined allocator will run with assert checking! Define NDEBUG to run at full speed.\")\r\n #endif\r\n#endif\r\n#undef DEBUG\r\n#undef _DEBUG\r\n#if NEDMALLOC_DEBUG\r\n #define _DEBUG\r\n #define DEBUG 1\r\n#else\r\n #define DEBUG 0\r\n#endif\r\n#ifdef NDEBUG               /* Disable assert checking on release builds */\r\n #undef DEBUG\r\n #undef _DEBUG\r\n#endif\r\n/* The default of 64Kb means we spend too much time kernel-side */\r\n#ifndef DEFAULT_GRANULARITY\r\n#define DEFAULT_GRANULARITY (1*1024*1024)\r\n#if DEBUG\r\n#define DEFAULT_GRANULARITY_ALIGNED\r\n#endif\r\n#endif\r\n/*#define USE_SPIN_LOCKS 0*/\r\n\r\n#if ENABLE_USERMODEPAGEALLOCATOR\r\nextern int OSHavePhysicalPageSupport(void);\r\nextern void *userpage_malloc(size_t toallocate, unsigned flags);\r\nextern int userpage_free(void *mem, size_t size);\r\nextern void *userpage_realloc(void *mem, size_t oldsize, size_t newsize, int flags, unsigned flags2);\r\n\r\n#define USERPAGE_TOPDOWN                   (M2_CUSTOM_FLAGS_BEGIN<<0)\r\n#define USERPAGE_NOCOMMIT                  (M2_CUSTOM_FLAGS_BEGIN<<1)\r\n\r\n/* This can provide a very significant speed boost */\r\n#undef MMAP_CLEARS\r\n#define MMAP_CLEARS 0\r\n\r\n#define MUNMAP(h, a, s)                    (!OSHavePhysicalPageSupport() ? MUNMAP_DEFAULT((h), (a), (s)) : userpage_free((a), (s)))\r\n#define MMAP(s, f)                         (!OSHavePhysicalPageSupport() ? MMAP_DEFAULT((s)) : userpage_malloc((s), (f)))\r\n#define MREMAP(addr, osz, nsz, mv)         (!OSHavePhysicalPageSupport() ? MREMAP_DEFAULT((addr), (osz), (nsz), (mv)) : userpage_realloc((addr), (osz), (nsz), (mv), 0))\r\n#define DIRECT_MMAP(h, s, f)               (!OSHavePhysicalPageSupport() ? DIRECT_MMAP_DEFAULT((h), (s), (f)) : userpage_malloc((s), (f)|USERPAGE_TOPDOWN))\r\n#define DIRECT_MREMAP(h, a, os, ns, f, f2) (!OSHavePhysicalPageSupport() ? DIRECT_MREMAP_DEFAULT((h), (a), (os), (ns), (f), (f2)) : userpage_realloc((a), (os), (ns), (f), (f2)|USERPAGE_TOPDOWN))\r\n\r\n/*#undef MREMAP\r\n#define MREMAP(addr, osz, nsz, mv)         (!OSHavePhysicalPageSupport() ? MREMAP_DEFAULT((addr), (osz), (nsz), (mv)) : MFAIL)*/\r\n/*#undef DIRECT_MREMAP\r\n#define DIRECT_MREMAP(h, a, os, ns, f, f2) (!OSHavePhysicalPageSupport() ? DIRECT_MREMAP_DEFAULT((h), (a), (os), (ns), (f), (f2)) : MFAIL)*/\r\n\r\n#endif\r\n#include \"malloc.c.h\"\r\n#ifdef NDEBUG               /* Disable assert checking on release builds */\r\n #undef DEBUG\r\n#endif\r\n\r\n/* The default number of threads allowed into a pool at once */\r\n#ifndef DEFAULTMAXTHREADSINPOOL\r\n#define DEFAULTMAXTHREADSINPOOL 4\r\n#endif\r\n/* The maximum size to be allocated from the thread cache */\r\n#ifndef THREADCACHEMAX\r\n#define THREADCACHEMAX 8192\r\n#elif THREADCACHEMAX && !defined(THREADCACHEMAXBINS)\r\n #ifdef __GNUC__\r\n  #warning If you are changing THREADCACHEMAX, do you also need to change THREADCACHEMAXBINS=(topbitpos(THREADCACHEMAX)-4)?\r\n #elif defined(_MSC_VER)\r\n  #pragma message(__FILE__ \": WARNING: If you are changing THREADCACHEMAX, do you also need to change THREADCACHEMAXBINS=(topbitpos(THREADCACHEMAX)-4)?\")\r\n #endif\r\n#endif\r\n/* The maximum concurrent threads in a pool possible */\r\n#ifndef MAXTHREADSINPOOL\r\n#define MAXTHREADSINPOOL 16\r\n#endif\r\n/* The maximum number of threadcaches which can be allocated */\r\n#ifndef THREADCACHEMAXCACHES\r\n#define THREADCACHEMAXCACHES 256\r\n#endif\r\n#ifndef THREADCACHEMAXBINS\r\n#if 0\r\n/* The number of cache entries for finer grained bins. This is (topbitpos(THREADCACHEMAX)-4)*2 */\r\n#define THREADCACHEMAXBINS ((13-4)*2)\r\n#else\r\n/* The number of cache entries. This is (topbitpos(THREADCACHEMAX)-4) */\r\n#define THREADCACHEMAXBINS (13-4)\r\n#endif\r\n#endif\r\n/* Point at which the free space in a thread cache is garbage collected */\r\n#ifndef THREADCACHEMAXFREESPACE\r\n#define THREADCACHEMAXFREESPACE (1024*1024)\r\n#endif\r\n/* NEDMALLOC_FORCERESERVE is used to force malloc2 flags for normal malloc, calloc et al */\r\n#ifndef NEDMALLOC_FORCERESERVE\r\n#define NEDMALLOC_FORCERESERVE(p, mem, size) 0\r\n#endif\r\n/* ENABLE_LOGGING is a bitmask of what events to log */\r\n#if ENABLE_LOGGING\r\n#ifndef NEDMALLOC_LOGFILE\r\n#define NEDMALLOC_LOGFILE \"nedmalloc.csv\"\r\n#endif\r\n#endif\r\n/* NEDMALLOC_TESTLOGENTRY returns non-zero if the entry should be logged */\r\n#ifndef NEDMALLOC_TESTLOGENTRY\r\n#define NEDMALLOC_TESTLOGENTRY(tc, np, type, mspace, size, mem, alignment, flags, returned) ((type)&ENABLE_LOGGING)\r\n#endif\r\n/* NEDMALLOC_STACKBACKTRACEDEPTH has the logger store a stack backtrace per logged item. Slow! */\r\n#ifndef NEDMALLOC_STACKBACKTRACEDEPTH\r\n#define NEDMALLOC_STACKBACKTRACEDEPTH 0\r\n#endif\r\n#if NEDMALLOC_STACKBACKTRACEDEPTH\r\n#include \"Dbghelp.h\"\r\n#include \"psapi.h\"\r\n#pragma comment(lib, \"dbghelp.lib\")\r\n#pragma comment(lib, \"psapi.lib\")\r\n#endif\r\n#define NM_FLAGS_MASK (M2_FLAGS_MASK&~M2_ZERO_MEMORY)\r\n\r\n#if USE_LOCKS\r\n#ifdef WIN32\r\n #define TLSVAR\t\t\tDWORD\r\n #define TLSALLOC(k)\t(*(k)=TlsAlloc(), TLS_OUT_OF_INDEXES==*(k))\r\n #define TLSFREE(k)\t\t(!TlsFree(k))\r\n #define TLSGET(k)\t\tTlsGetValue(k)\r\n #define TLSSET(k, a)\t(!TlsSetValue(k, a))\r\n #ifdef DEBUG\r\nstatic LPVOID ChkedTlsGetValue(DWORD idx)\r\n{\r\n\tLPVOID ret=TlsGetValue(idx);\r\n\tassert(S_OK==GetLastError());\r\n\treturn ret;\r\n}\r\n  #undef TLSGET\r\n  #define TLSGET(k) ChkedTlsGetValue(k)\r\n #endif\r\n#else\r\n #define TLSVAR\t\t\tpthread_key_t\r\n #define TLSALLOC(k)\tpthread_key_create(k, 0)\r\n #define TLSFREE(k)\t\tpthread_key_delete(k)\r\n #define TLSGET(k)\t\tpthread_getspecific(k)\r\n #define TLSSET(k, a)\tpthread_setspecific(k, a)\r\n#endif\r\n#else /* Probably if you're not using locks then you don't want ANY pthread stuff at all */\r\n #define TLSVAR\t\t\tvoid *\r\n #define TLSALLOC(k)\t(*k=0)\r\n #define TLSFREE(k)\t\t(k=0)\r\n #define TLSGET(k)\t\tk\r\n #define TLSSET(k, a)\t(k=a, 0)\r\n#endif\r\n\r\n#if ENABLE_USERMODEPAGEALLOCATOR\r\n#include \"usermodepageallocator.c\"\r\n#endif\r\n\r\n#if defined(__cplusplus)\r\n#if !defined(NO_NED_NAMESPACE)\r\nnamespace nedalloc {\r\n#else\r\nextern \"C\" {\r\n#endif\r\n#endif\r\n\r\n#if USE_ALLOCATOR==0\r\nstatic void *unsupported_operation(const char *opname) THROWSPEC\r\n{\r\n\tfprintf(stderr, \"nedmalloc: The operation %s is not supported under this build configuration\\n\", opname);\r\n\tabort();\r\n\treturn 0;\r\n}\r\nstatic size_t mspacecounter=(size_t) 0xdeadbeef;\r\n#endif\r\n#ifndef ENABLE_FAST_HEAP_DETECTION\r\nstatic void *RESTRICT leastusedaddress;\r\nstatic size_t largestusedblock;\r\n#endif\r\n/* Used to redirect system allocator ops if needed */\r\nextern void *(*sysmalloc)(size_t);\r\nextern void *(*syscalloc)(size_t, size_t);\r\nextern void *(*sysrealloc)(void *, size_t);\r\nextern void (*sysfree)(void *);\r\nextern size_t (*sysblksize)(void *);\r\n\r\n#if !defined(REPLACE_SYSTEM_ALLOCATOR) || (!defined(_MSC_VER) && !defined(__MINGW32__))\r\nvoid *(*sysmalloc)(size_t)=malloc;\r\nvoid *(*syscalloc)(size_t, size_t)=calloc;\r\nvoid *(*sysrealloc)(void *, size_t)=realloc;\r\nvoid (*sysfree)(void *)=free;\r\nsize_t (*sysblksize)(void *)=\r\n#if defined(_MSC_VER) || defined(__MINGW32__)\r\n\t/* This is the MSVCRT equivalent */\r\n\t_msize;\r\n#elif defined(__linux__) || defined(__FreeBSD__)\r\n\t/* This is the glibc/ptmalloc2/dlmalloc/BSD equivalent.  */\r\n\tmalloc_usable_size;\r\n#elif defined(__APPLE__)\r\n\t/* This is the Apple BSD libc equivalent.  */\r\n\tmalloc_size;\r\n#else\r\n#error Cannot tolerate the memory allocator of an unknown system!\r\n#endif\r\n#else\r\n/* Remove the MSVCRT dependency on the memory functions */\r\nvoid *(*sysmalloc)(size_t);\r\nvoid *(*syscalloc)(size_t, size_t);\r\nvoid *(*sysrealloc)(void *, size_t);\r\nvoid (*sysfree)(void *);\r\nsize_t (*sysblksize)(void *);\r\n#endif\r\n\r\nstatic FORCEINLINE NEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void *CallMalloc(void *RESTRICT mspace, size_t size, size_t alignment, unsigned flags) THROWSPEC\r\n{\r\n\tvoid *RESTRICT ret=0;\r\n#if USE_MAGIC_HEADERS\r\n\tsize_t _alignment=alignment;\r\n\tsize_t *_ret=0;\r\n\tsize_t bytes=size+alignment+3*sizeof(size_t);\r\n\t/* Avoid addition overflow. */\r\n\tif(bytes<size)\r\n\t\treturn 0;\r\n\tsize=bytes;\r\n\t_alignment=0;\r\n#endif\r\n#if USE_ALLOCATOR==0\r\n\tret=(flags & M2_ZERO_MEMORY) ? syscalloc(1, size) : sysmalloc(size);\t/* magic headers takes care of alignment */\r\n#elif USE_ALLOCATOR==1\r\n\tret=mspace_malloc2((mstate) mspace, size, alignment, flags);\r\n#ifndef ENABLE_FAST_HEAP_DETECTION\r\n\tif(ret)\r\n\t{\r\n\t\tmchunkptr p=mem2chunk(ret);\r\n\t\tsize_t truesize=chunksize(p) - overhead_for(p);\r\n\t\tif(!leastusedaddress || (void *)((mstate) mspace)->least_addr<leastusedaddress) leastusedaddress=(void *)((mstate) mspace)->least_addr;\r\n\t\tif(!largestusedblock || truesize>largestusedblock) largestusedblock=(truesize+mparams.page_size) & ~(mparams.page_size-1);\r\n\t}\r\n#endif\r\n#endif\r\n\tif(!ret) return 0;\r\n#if DEBUG\r\n\tif(flags & M2_ZERO_MEMORY)\r\n\t{\r\n\t\tconst char *RESTRICT n;\r\n\t\tfor(n=(const char *)ret; n<(const char *)ret+size; n++)\r\n\t\t{\r\n\t\t\tassert(!*n);\r\n\t\t}\r\n\t}\r\n#endif\r\n#if USE_MAGIC_HEADERS\r\n\t_ret=(size_t *) ret;\r\n\tret=(void *)(_ret+3);\r\n\tif(alignment) ret=(void *)(((size_t) ret+alignment-1)&~(alignment-1));\r\n\tfor(; _ret<(size_t *)ret-2; _ret++) *_ret=*(size_t *)\"NEDMALOC\";\r\n\t_ret[0]=(size_t) mspace;\r\n\t_ret[1]=size-3*sizeof(size_t);\r\n#endif\r\n\treturn ret;\r\n}\r\n\r\nstatic FORCEINLINE NEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void *CallRealloc(void *RESTRICT mspace, void *RESTRICT mem, int isforeign, size_t oldsize, size_t newsize, size_t alignment, unsigned flags) THROWSPEC\r\n{\r\n\tvoid *RESTRICT ret=0;\r\n#if USE_MAGIC_HEADERS\r\n\tmstate oldmspace=0;\r\n\tsize_t *_ret=0, *_mem=(size_t *) mem-3;\r\n#endif\r\n\tif(isforeign)\r\n\t{\t/* Transfer */\r\n#if USE_MAGIC_HEADERS\r\n\t\tassert(_mem[0]!=*(size_t *) \"NEDMALOC\");\r\n#endif\r\n\t\tif((ret=CallMalloc(mspace, newsize, alignment, flags)))\r\n\t\t{\r\n#if defined(DEBUG)\r\n\t\t\tprintf(\"*** nedmalloc frees system allocated block %p\\n\", mem);\r\n#endif\r\n\t\t\tmemcpy(ret, mem, oldsize<newsize ? oldsize : newsize);\r\n\t\t\tsysfree(mem);\r\n\t\t}\r\n\t\treturn ret;\r\n\t}\r\n#if USE_MAGIC_HEADERS\r\n\tassert(_mem[0]==*(size_t *) \"NEDMALOC\");\r\n\tnewsize+=3*sizeof(size_t);\r\n\toldmspace=(mstate) _mem[1];\r\n\tassert(oldsize>=_mem[2]);\r\n\tfor(; *_mem==*(size_t *) \"NEDMALOC\"; *_mem--=*(size_t *) \"nedmaloc\");\r\n\tmem=(void *)(++_mem);\r\n#endif\r\n#if USE_ALLOCATOR==0\r\n\tret=sysrealloc(mem, newsize);\r\n#elif USE_ALLOCATOR==1\r\n\tret=mspace_realloc2((mstate) mspace, mem, newsize, alignment, flags);\r\n#ifndef ENABLE_FAST_HEAP_DETECTION\r\n\tif(ret)\r\n\t{\r\n\t\tmchunkptr p=mem2chunk(ret);\r\n\t\tsize_t truesize=chunksize(p) - overhead_for(p);\r\n\t\tif(!leastusedaddress || (void *)((mstate) mspace)->least_addr<leastusedaddress) leastusedaddress=(void *)((mstate) mspace)->least_addr;\r\n\t\tif(!largestusedblock || truesize>largestusedblock) largestusedblock=(truesize+mparams.page_size) & ~(mparams.page_size-1);\r\n\t}\r\n#endif\r\n#endif\r\n\tif(!ret)\r\n\t{\t/* Put it back the way it was */\r\n#if USE_MAGIC_HEADERS\r\n\t\tfor(; *_mem==0; *_mem++=*(size_t *) \"NEDMALOC\");\r\n#endif\r\n\t\treturn 0;\r\n\t}\r\n#if USE_MAGIC_HEADERS\r\n\t_ret=(size_t *) ret;\r\n\tret=(void *)(_ret+3);\r\n\tfor(; _ret<(size_t *)ret-2; _ret++) *_ret=*(size_t *) \"NEDMALOC\";\r\n\t_ret[0]=(size_t) mspace;\r\n\t_ret[1]=newsize-3*sizeof(size_t);\r\n#endif\r\n\treturn ret;\r\n}\r\n\r\nstatic FORCEINLINE void CallFree(void *RESTRICT mspace, void *RESTRICT mem, int isforeign) THROWSPEC\r\n{\r\n#if USE_MAGIC_HEADERS\r\n\tmstate oldmspace=0;\r\n\tsize_t *_mem=(size_t *) mem-3, oldsize=0;\r\n#endif\r\n\tif(isforeign)\r\n\t{\r\n#if USE_MAGIC_HEADERS\r\n\t\tassert(_mem[0]!=*(size_t *) \"NEDMALOC\");\r\n#endif\r\n#if defined(DEBUG)\r\n\t\tprintf(\"*** nedmalloc frees system allocated block %p\\n\", mem);\r\n#endif\r\n\t\tsysfree(mem);\r\n\t\treturn;\r\n\t}\r\n#if USE_MAGIC_HEADERS\r\n\tassert(_mem[0]==*(size_t *) \"NEDMALOC\");\r\n\toldmspace=(mstate) _mem[1];\r\n\toldsize=_mem[2];\r\n\tfor(; *_mem==*(size_t *) \"NEDMALOC\"; *_mem--=*(size_t *) \"nedmaloc\");\r\n\tmem=(void *)(++_mem);\r\n#endif\r\n#if USE_ALLOCATOR==0\r\n\tsysfree(mem);\r\n#elif USE_ALLOCATOR==1\r\n\tmspace_free((mstate) mspace, mem);\r\n#endif\r\n}\r\n\r\nstatic NEDMALLOCNOALIASATTR mstate nedblkmstate(void *RESTRICT mem) THROWSPEC\r\n{\r\n\tif(mem)\r\n\t{\r\n#if USE_MAGIC_HEADERS\r\n\t\tsize_t *_mem=(size_t *) mem-3;\r\n\t\tif(_mem[0]==*(size_t *) \"NEDMALOC\")\r\n\t\t{\r\n\t\t\treturn (mstate) _mem[1];\r\n\t\t}\r\n\t\telse return 0;\r\n#else\r\n#if USE_ALLOCATOR==0\r\n\t\t/* Fail everything */\r\n\t\treturn 0;\r\n#elif USE_ALLOCATOR==1\r\n#ifdef ENABLE_FAST_HEAP_DETECTION\r\n#ifdef WIN32\r\n\t\t/*  On Windows for RELEASE both x86 and x64 the NT heap precedes each block with an eight byte header\r\n\t\t\twhich looks like:\r\n\t\t\t\tnormal: 4 bytes of size, 4 bytes of [char < 64, char < 64, char < 64 bit 0 always set, char random ]\r\n\t\t\t\tmmaped: 4 bytes of size  4 bytes of [zero,      zero,      0xb,                        zero        ]\r\n\r\n\t\t\tOn Windows for DEBUG both x86 and x64 the preceding four bytes is always 0xfdfdfdfd (no man's land).\r\n\t\t*/\r\n#pragma pack(push, 1)\r\n\t\tstruct _HEAP_ENTRY\r\n\t\t{\r\n\t\t\tUSHORT Size;\r\n\t\t\tUSHORT PreviousSize;\r\n\t\t\tUCHAR Cookie;\t\t\t/* SegmentIndex */\r\n\t\t\tUCHAR Flags;\t\t\t/* always bit 0 (HEAP_ENTRY_BUSY). bit 1=(HEAP_ENTRY_EXTRA_PRESENT), bit 2=normal block (HEAP_ENTRY_FILL_PATTERN), bit 3=mmap block (HEAP_ENTRY_VIRTUAL_ALLOC). Bit 4 (HEAP_ENTRY_LAST_ENTRY) could be set */\r\n\t\t\tUCHAR UnusedBytes;\r\n\t\t\tUCHAR SmallTagIndex;\t/* fastbin index. Always one of 0x02, 0x03, 0x04 < 0x80 */\r\n\t\t} *RESTRICT he=((struct _HEAP_ENTRY *) mem)-1;\r\n#pragma pack(pop)\r\n\t\tunsigned int header=((unsigned int *)mem)[-1], mask1=0x8080E100, result1, mask2=0xFFFFFF06, result2;\r\n\t\tresult1=header & mask1;\t/* Positive testing for NT heap */\r\n\t\tresult2=header & mask2;\t/* Positive testing for dlmalloc */\r\n\t\tif(result1==0x00000100 && result2!=0x00000102)\r\n\t\t{\t/* This is likely a NT heap block */\r\n\t\t\treturn 0;\r\n\t\t}\r\n#endif\r\n#ifdef __linux__\r\n\t\t/* On Linux glibc uses ptmalloc2 (really dlmalloc) just as we do, but prev_foot contains rubbish\r\n\t\twhen the preceding block is allocated because ptmalloc2 finds the local mstate by rounding the ptr\r\n\t\tdown to the nearest megabyte. It's like dlmalloc with FOOTERS disabled. */\r\n\t\tmchunkptr p=mem2chunk(mem);\r\n\t\tmstate fm=get_mstate_for(p);\r\n\t\t/* If it's a ptmalloc2 block, fm is likely to be some crazy value */\r\n\t\tif(!is_aligned(fm)) return 0;\r\n\t\tif((size_t)mem-(size_t)fm>=(size_t)1<<(SIZE_T_BITSIZE-1)) return 0;\r\n\t\tif(ok_magic(fm))\r\n\t\t\treturn fm;\r\n\t\telse\r\n\t\t\treturn 0;\r\n\t\tif(1) { }\r\n#endif\r\n\t\telse\r\n\t\t{\r\n\t\t\tmchunkptr p=mem2chunk(mem);\r\n\t\t\tmstate fm=get_mstate_for(p);\r\n\t\t\tassert(ok_magic(fm));\t/* If this fails, someone tried to free a block twice */\r\n\t\t\tif(ok_magic(fm))\r\n\t\t\t\treturn fm;\r\n\t\t}\r\n#else\r\n#ifdef WIN32\r\n#ifdef _MSC_VER\r\n\t\t__try\r\n#elif defined(__MINGW32__)\r\n\t\t__try1\r\n#endif\r\n#endif\r\n\t\t{\r\n\t\t\t/* We try to return zero here if it isn't one of our own blocks, however\r\n\t\t\tthe current block annotation scheme used by dlmalloc makes it impossible\r\n\t\t\tto be absolutely sure of avoiding a segfault.\r\n\r\n\t\t\tmchunkptr->prev_foot = mem-(2*size_t) = mstate ^ mparams.magic for PRECEDING block;\r\n\t\t\tmchunkptr->head      = mem-(1*size_t) = 8 multiple size of this block with bottom three bits = FLAG_BITS\r\n\t\t\t    FLAG_BITS = bit 0 is CINUSE (currently in use unless is mmap), bit 1 is PINUSE (previous block currently\r\n\t\t\t\t            in use unless mmap), bit 2 is UNUSED and currently is always zero.\r\n\t\t\t*/\r\n\t\t\tregister void *RESTRICT leastusedaddress_=leastusedaddress;\t\t/* Cache these to avoid register reloading */\r\n\t\t\tregister size_t largestusedblock_=largestusedblock;\r\n\t\t\tif(!is_aligned(mem)) return 0;\t\t/* Would fail very rarely as all allocators return aligned blocks */\r\n\t\t\tif(mem<leastusedaddress_) return 0;\t/* Simple but effective */\r\n\t\t\t{\r\n\t\t\t\tmchunkptr p=mem2chunk(mem);\r\n\t\t\t\tmstate fm=0;\r\n\t\t\t\tint ismmapped=is_mmapped(p);\r\n\t\t\t\tif((!ismmapped && !is_inuse(p)) || (p->head & FLAG4_BIT)) return 0;\r\n\t\t\t\t/* Reduced uncertainty by 0.5^2 = 25.0% */\r\n\t\t\t\t/* size should never exceed largestusedblock */\r\n\t\t\t\tif(chunksize(p)-overhead_for(p)>largestusedblock_) return 0;\r\n\t\t\t\t/* Reduced uncertainty by a minimum of 0.5^3 = 12.5%, maximum 0.5^16 = 0.0015% */\r\n\t\t\t\t/* Having sanity checked prev_foot and head, check next block */\r\n\t\t\t\tif(!ismmapped && (!next_pinuse(p) || (next_chunk(p)->head & FLAG4_BIT))) return 0;\r\n\t\t\t\t/* Reduced uncertainty by 0.5^5 = 3.13% or 0.5^18 = 0.00038% */\r\n#if 0\r\n\t\t\t\t/* If previous block is free, check that its next block pointer equals us */\r\n\t\t\t\tif(!ismmapped && !pinuse(p))\r\n\t\t\t\t\tif(next_chunk(prev_chunk(p))!=p) return 0;\r\n\t\t\t\t/* We could start comparing prev_foot's for similarity but it starts getting slow. */\r\n#endif\r\n\t\t\t\tfm = get_mstate_for(p);\r\n\t\t\t\tif(!is_aligned(fm) || (void *)fm<leastusedaddress_) return 0;\r\n#if 0\r\n\t\t\t\t/* See if mem is lower in memory than mem */\r\n\t\t\t\tif((size_t)mem-(size_t)fm>=(size_t)1<<(SIZE_T_BITSIZE-1)) return 0;\r\n#endif\r\n\t\t\t\tassert(ok_magic(fm));\t/* If this fails, someone tried to free a block twice */\r\n\t\t\t\tif(ok_magic(fm))\r\n\t\t\t\t\treturn fm;\r\n\t\t\t}\r\n\t\t}\r\n#ifdef WIN32\r\n#ifdef _MSC_VER\r\n\t\t__except(1) { }\r\n#elif defined(__MINGW32__)\r\n\t\t__except1(1) { }\r\n#endif\r\n#endif\r\n#endif\r\n#endif\r\n#endif\r\n\t}\r\n\treturn 0;\r\n}\r\nNEDMALLOCNOALIASATTR size_t nedblksize(int *RESTRICT isforeign, void *RESTRICT mem, unsigned flags) THROWSPEC\r\n{\r\n\tif(mem)\r\n\t{\r\n\t\tif(isforeign) *isforeign=1;\r\n#if USE_MAGIC_HEADERS\r\n\t\t{\r\n\t\t\tsize_t *_mem=(size_t *) mem-3;\r\n\t\t\tif(_mem[0]==*(size_t *) \"NEDMALOC\")\r\n\t\t\t{\r\n\t\t\t\tmstate mspace=(mstate) _mem[1];\r\n\t\t\t\tsize_t size=_mem[2];\r\n\t\t\t\tif(isforeign) *isforeign=0;\r\n\t\t\t\treturn size;\r\n\t\t\t}\r\n\t\t}\r\n#elif USE_ALLOCATOR==1\r\n\t\tif((flags & NM_SKIP_TOLERANCE_CHECKS) || nedblkmstate(mem))\r\n\t\t{\r\n\t\t\tmchunkptr p=mem2chunk(mem);\r\n\t\t\tif(isforeign) *isforeign=0;\r\n\t\t\treturn chunksize(p)-overhead_for(p);\r\n\t\t}\r\n#ifdef DEBUG\r\n\t\telse\r\n\t\t{\r\n\t\t\tint a=1; /* Set breakpoints here if needed */\r\n\t\t}\r\n#endif\r\n#endif\r\n#if defined(ENABLE_TOLERANT_NEDMALLOC) || USE_ALLOCATOR==0\r\n\t\treturn sysblksize(mem);\r\n#endif\r\n\t}\r\n\treturn 0;\r\n}\r\nNEDMALLOCNOALIASATTR size_t nedmemsize(void *RESTRICT mem) THROWSPEC { return nedblksize(0, mem, 0); }\r\n\r\nNEDMALLOCNOALIASATTR void nedsetvalue(void *v) THROWSPEC\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{ nedpsetvalue((nedpool *) 0, v); }\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedmalloc(size_t size) THROWSPEC\t\t\t\t\t\t\t\t\t\t\t\t\t{ return nedpmalloc((nedpool *) 0, size); }\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedcalloc(size_t no, size_t size) THROWSPEC\t\t\t\t\t\t\t\t\t\t{ return nedpcalloc((nedpool *) 0, no, size); }\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedrealloc(void *mem, size_t size) THROWSPEC\t\t\t\t\t\t\t\t\t\t{ return nedprealloc((nedpool *) 0, mem, size); }\r\nNEDMALLOCNOALIASATTR void   nedfree(void *mem) THROWSPEC\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{ nedpfree((nedpool *) 0, mem); }\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedmemalign(size_t alignment, size_t bytes) THROWSPEC\t\t\t\t\t\t\t\t{ return nedpmemalign((nedpool *) 0, alignment, bytes); }\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedmalloc2(size_t size, size_t alignment, unsigned flags) THROWSPEC\t\t\t\t{ return nedpmalloc2((nedpool *) 0, size, alignment, flags); }\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedrealloc2(void *mem, size_t size, size_t alignment, unsigned flags) THROWSPEC\t{ return nedprealloc2((nedpool *) 0, mem, size, alignment, flags); }\r\nNEDMALLOCNOALIASATTR void   nedfree2(void *mem, unsigned flags) THROWSPEC\t\t\t\t\t\t\t\t\t\t\t\t\t\t{ nedpfree2((nedpool *) 0, mem, flags); }\r\nNEDMALLOCNOALIASATTR struct nedmallinfo nedmallinfo(void) THROWSPEC\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{ return nedpmallinfo((nedpool *) 0); }\r\nNEDMALLOCNOALIASATTR int    nedmallopt(int parno, int value) THROWSPEC\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{ return nedpmallopt((nedpool *) 0, parno, value); }\r\nNEDMALLOCNOALIASATTR int    nedmalloc_trim(size_t pad) THROWSPEC\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{ return nedpmalloc_trim((nedpool *) 0, pad); }\r\nvoid   nedmalloc_stats() THROWSPEC\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{ nedpmalloc_stats((nedpool *) 0); }\r\nNEDMALLOCNOALIASATTR size_t nedmalloc_footprint() THROWSPEC\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{ return nedpmalloc_footprint((nedpool *) 0); }\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void **nedindependent_calloc(size_t elemsno, size_t elemsize, void **chunks) THROWSPEC\t{ return nedpindependent_calloc((nedpool *) 0, elemsno, elemsize, chunks); }\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void **nedindependent_comalloc(size_t elems, size_t *sizes, void **chunks) THROWSPEC\t\t{ return nedpindependent_comalloc((nedpool *) 0, elems, sizes, chunks); }\r\n\r\n#ifdef WIN32\r\ntypedef unsigned __int64 timeCount;\r\nstatic timeCount GetTimestamp()\r\n{\r\n\tstatic LARGE_INTEGER ticksPerSec;\r\n\tstatic double scalefactor;\r\n\tstatic timeCount baseCount;\r\n\tLARGE_INTEGER val;\r\n\ttimeCount ret;\r\n\tif(!scalefactor)\r\n\t{\r\n\t\tif(QueryPerformanceFrequency(&ticksPerSec))\r\n\t\t\tscalefactor=ticksPerSec.QuadPart/1000000000000.0;\r\n\t\telse\r\n\t\t\tscalefactor=1;\r\n\t}\r\n\tif(!QueryPerformanceCounter(&val))\r\n\t\treturn (timeCount) GetTickCount() * 1000000000;\r\n\tret=(timeCount) (val.QuadPart/scalefactor);\r\n\tif(!baseCount) baseCount=ret;\r\n\treturn ret-baseCount;\r\n}\r\n#else\r\n#include <sys/time.h>\r\n\r\ntypedef unsigned long long timeCount;\r\nstatic timeCount GetTimestamp()\r\n{\r\n\tstatic timeCount baseCount;\r\n\ttimeCount ret;\r\n#ifdef CLOCK_MONOTONIC\r\n\tstruct timespec ts;\r\n\tclock_gettime(CLOCK_MONOTONIC, &ts);\r\n\tret=((timeCount) ts.tv_sec*1000000000000LL)+ts.tv_nsec*1000LL;\r\n#else\r\n\tstruct timeval tv;\r\n\tgettimeofday(&tv, 0);\r\n\tret=((timeCount) tv.tv_sec*1000000000000LL)+tv.tv_usec*1000000LL;\r\n#endif\r\n\tif(!baseCount) baseCount=ret;\r\n\treturn ret-baseCount;\r\n}\r\n#endif\r\n\r\n/* Set ENABLE_LOGGING to an AND mask of which of these you want to\r\nlog, so set it to 0xffffffff for everything */\r\ntypedef enum LogEntryType_t\r\n{\r\n\tLOGENTRY_MALLOC\t\t\t\t\t=(1<<0),\r\n\tLOGENTRY_REALLOC\t\t\t\t=(1<<1),\r\n\tLOGENTRY_FREE\t\t\t\t\t=(1<<2),\r\n\r\n\tLOGENTRY_THREADCACHE_MALLOC\t\t=(1<<3),\r\n\tLOGENTRY_THREADCACHE_FREE\t\t=(1<<4),\r\n\tLOGENTRY_THREADCACHE_CLEAN\t\t=(1<<5),\r\n\r\n\tLOGENTRY_POOL_MALLOC\t\t\t=(1<<6),\r\n\tLOGENTRY_POOL_REALLOC\t\t\t=(1<<7),\r\n\tLOGENTRY_POOL_FREE\t\t\t\t=(1<<8)\r\n} LogEntryType;\r\n#if NEDMALLOC_STACKBACKTRACEDEPTH\r\ntypedef struct StackFrameType_t\r\n{\r\n\tvoid *pc;\r\n\tchar module[64];\r\n\tchar functname[256];\r\n\tchar file[96];\r\n\tint lineno;\r\n} StackFrameType;\r\n#endif\r\ntypedef struct logentry_t\r\n{\r\n\ttimeCount timestamp;\r\n\tnedpool *np;\r\n\tLogEntryType type;\r\n\tint mspace;\r\n\tsize_t size;\r\n\tvoid *mem;\r\n\tsize_t alignment;\r\n\tunsigned flags;\r\n\tvoid *returned;\r\n#if NEDMALLOC_STACKBACKTRACEDEPTH\r\n\tStackFrameType stack[NEDMALLOC_STACKBACKTRACEDEPTH];\r\n#endif\r\n} logentry;\r\nstatic const char *LogEntryTypeStrings[]={\r\n\t\"LOGENTRY_MALLOC\",\r\n\t\"LOGENTRY_REALLOC\",\r\n\t\"LOGENTRY_FREE\",\r\n\r\n\t\"LOGENTRY_THREADCACHE_MALLOC\",\r\n\t\"LOGENTRY_THREADCACHE_FREE\",\r\n\t\"LOGENTRY_THREADCACHE_CLEAN\",\r\n\r\n\t\"LOGENTRY_POOL_MALLOC\",\r\n\t\"LOGENTRY_POOL_REALLOC\",\r\n\t\"LOGENTRY_POOL_FREE\",\r\n\t\"******************\"\r\n};\r\n\r\nstruct threadcacheblk_t;\r\ntypedef struct threadcacheblk_t threadcacheblk;\r\nstruct threadcacheblk_t\r\n{\t/* Keep less than 32 bytes as sizeof(threadcacheblk) is the minimum allocation size */\r\n#ifdef FULLSANITYCHECKS\r\n\tunsigned int magic;\r\n#endif\r\n\tint isforeign;\r\n\tunsigned int lastUsed;\r\n\tsize_t size;\r\n\tthreadcacheblk *RESTRICT next, *RESTRICT prev;\r\n};\r\ntypedef struct threadcache_t\r\n{\r\n#ifdef FULLSANITYCHECKS\r\n\tunsigned int magic1;\r\n#endif\r\n\tint mymspace;\t\t\t\t\t\t/* Last mspace entry this thread used */\r\n\tlong threadid;\r\n\tunsigned int mallocs, frees, successes;\r\n#if ENABLE_LOGGING\r\n\tlogentry *logentries, *logentriesptr, *logentriesend;\r\n#endif\r\n\tsize_t freeInCache;\t\t\t\t\t/* How much free space is stored in this cache */\r\n\tthreadcacheblk *RESTRICT bins[(THREADCACHEMAXBINS+1)*2];\r\n#ifdef FULLSANITYCHECKS\r\n\tunsigned int magic2;\r\n#endif\r\n} threadcache;\r\nstruct nedpool_t\r\n{\r\n#if USE_LOCKS\r\n\tMLOCK_T mutex;\r\n#endif\r\n\tvoid *uservalue;\r\n\tint threads;\t\t\t\t\t\t/* Max entries in m to use */\r\n\tthreadcache *RESTRICT caches[THREADCACHEMAXCACHES];\r\n\tTLSVAR mycache;\t\t\t\t\t\t/* Thread cache for this thread. 0 for unset, negative for use mspace-1 directly, otherwise is cache-1 */\r\n\tmstate m[MAXTHREADSINPOOL+1];\t\t/* mspace entries for this pool */\r\n};\r\nstatic nedpool syspool;\r\n\r\n#if ENABLE_LOGGING\r\n#if NEDMALLOC_STACKBACKTRACEDEPTH\r\n#if defined(WIN32) && defined(_MSC_VER)\r\n#define COPY_STRING(d, s, maxlen) { size_t len=strlen(s); len=(len>maxlen) ? maxlen-1 : len; memcpy(d, s, len); d[len]=0; }\r\n\r\n#pragma optimize(\"g\", off)\r\nstatic int ExceptionFilter(unsigned int code, struct _EXCEPTION_POINTERS *ep, CONTEXT *ct) THROWSPEC\r\n{\r\n\t*ct=*ep->ContextRecord;\r\n\treturn EXCEPTION_EXECUTE_HANDLER;\r\n}\r\n\r\nstatic DWORD64 __stdcall GetModBase(HANDLE hProcess, DWORD64 dwAddr) THROWSPEC\r\n{\r\n\tDWORD64 modulebase;\r\n\t// Try to get the module base if already loaded, otherwise load the module\r\n\tmodulebase=SymGetModuleBase64(hProcess, dwAddr);\r\n\tif(modulebase)\r\n\t\treturn modulebase;\r\n\telse\r\n\t{\r\n\t\tMEMORY_BASIC_INFORMATION stMBI ;\r\n\t\tif ( 0 != VirtualQueryEx ( hProcess, (LPCVOID)(size_t)dwAddr, &stMBI, sizeof(stMBI)))\r\n\t\t{\r\n\t\t\tint n;\r\n\t\t\tDWORD dwPathLen=0, dwNameLen=0 ;\r\n\t\t\tTCHAR szFile[ MAX_PATH ], szModuleName[ MAX_PATH ] ;\r\n\t\t\tMODULEINFO mi={0};\r\n\t\t\tdwPathLen = GetModuleFileName ( (HMODULE) stMBI.AllocationBase , szFile, MAX_PATH );\r\n\t\t\tdwNameLen = GetModuleBaseName (hProcess, (HMODULE) stMBI.AllocationBase , szModuleName, MAX_PATH );\r\n\t\t\tfor(n=dwNameLen; n>0; n--)\r\n\t\t\t{\r\n\t\t\t\tif(szModuleName[n]=='.')\r\n\t\t\t\t{\r\n\t\t\t\t\tszModuleName[n]=0;\r\n\t\t\t\t\tbreak;\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tif(!GetModuleInformation(hProcess, (HMODULE) stMBI.AllocationBase, &mi, sizeof(mi)))\r\n\t\t\t{\r\n\t\t\t\t//fxmessage(\"WARNING: GetModuleInformation() returned error code %d\\n\", GetLastError());\r\n\t\t\t}\r\n\t\t\tif(!SymLoadModule64 ( hProcess, NULL, (PSTR)( (dwPathLen) ? szFile : 0), (PSTR)( (dwNameLen) ? szModuleName : 0),\r\n\t\t\t\t(DWORD64) mi.lpBaseOfDll, mi.SizeOfImage))\r\n\t\t\t{\r\n\t\t\t\t//fxmessage(\"WARNING: SymLoadModule64() returned error code %d\\n\", GetLastError());\r\n\t\t\t}\r\n\t\t\t//fxmessage(\"%s, %p, %x, %x\\n\", szFile, mi.lpBaseOfDll, mi.SizeOfImage, (DWORD) mi.lpBaseOfDll+mi.SizeOfImage);\r\n\t\t\tmodulebase=SymGetModuleBase64(hProcess, dwAddr);\r\n\t\t\treturn modulebase;\r\n\t\t}\r\n\t}\r\n\treturn 0;\r\n}\r\n\r\nextern HANDLE sym_myprocess;\r\nextern VOID (WINAPI *RtlCaptureContextAddr)(PCONTEXT);\r\nextern void DeinitSym(void) THROWSPEC;\r\nstatic void DoStackWalk(logentry *p) THROWSPEC\r\n{\r\n\tint i,i2;\r\n\tHANDLE mythread=(HANDLE) GetCurrentThread();\r\n\tSTACKFRAME64 sf={ 0 };\r\n\tCONTEXT ct={ 0 };\r\n\tif(!sym_myprocess)\r\n\t{\r\n\t\tDWORD symopts;\r\n\t\tDuplicateHandle(GetCurrentProcess(), GetCurrentProcess(), GetCurrentProcess(), &sym_myprocess, 0, FALSE, DUPLICATE_SAME_ACCESS);\r\n\t\tsymopts=SymGetOptions();\r\n\t\tSymSetOptions(symopts /*| SYMOPT_DEFERRED_LOADS*/ | SYMOPT_LOAD_LINES);\r\n\t\tSymInitialize(sym_myprocess, NULL, TRUE);\r\n\t\tatexit(DeinitSym);\r\n\t}\r\n\tct.ContextFlags=CONTEXT_FULL;\r\n\r\n\t// Use RtlCaptureContext() if we have it as it saves an exception throw\r\n\tif((VOID (WINAPI *)(PCONTEXT)) -1==RtlCaptureContextAddr)\r\n\t\tRtlCaptureContextAddr=(VOID (WINAPI *)(PCONTEXT)) GetProcAddress(GetModuleHandle(L\"kernel32\"), \"RtlCaptureContext\");\r\n\tif(RtlCaptureContextAddr)\r\n\t\tRtlCaptureContextAddr(&ct);\r\n\telse\r\n\t{\t// This is nasty, but it works\r\n\t\t__try\r\n\t\t{\r\n\t\t\tint *foo=0;\r\n\t\t\t*foo=78;\r\n\t\t}\r\n\t\t__except (ExceptionFilter(GetExceptionCode(), GetExceptionInformation(), &ct))\r\n\t\t{\r\n\t\t}\r\n\t}\r\n\r\n\tsf.AddrPC.Mode=sf.AddrStack.Mode=sf.AddrFrame.Mode=AddrModeFlat;\r\n#if !(defined(_M_AMD64) || defined(_M_X64))\r\n\tsf.AddrPC.Offset   =ct.Eip;\r\n\tsf.AddrStack.Offset=ct.Esp;\r\n\tsf.AddrFrame.Offset=ct.Ebp;\r\n#else\r\n\tsf.AddrPC.Offset   =ct.Rip;\r\n\tsf.AddrStack.Offset=ct.Rsp;\r\n\tsf.AddrFrame.Offset=ct.Rbp; // maybe Rdi?\r\n#endif\r\n\tfor(i2=0; i2<NEDMALLOC_STACKBACKTRACEDEPTH; i2++)\r\n\t{\r\n\t\tIMAGEHLP_MODULE64 ihm={ sizeof(IMAGEHLP_MODULE64) };\r\n\t\tchar temp[MAX_PATH+sizeof(IMAGEHLP_SYMBOL64)];\r\n\t\tIMAGEHLP_SYMBOL64 *ihs;\r\n\t\tIMAGEHLP_LINE64 ihl={ sizeof(IMAGEHLP_LINE64) };\r\n\t\tDWORD64 offset;\r\n\t\tif(!StackWalk64(\r\n#if !(defined(_M_AMD64) || defined(_M_X64))\r\n\t\t\tIMAGE_FILE_MACHINE_I386,\r\n#else\r\n\t\t\tIMAGE_FILE_MACHINE_AMD64,\r\n#endif\r\n\t\t\tsym_myprocess, mythread, &sf, &ct, NULL, SymFunctionTableAccess64, GetModBase, NULL))\r\n\t\t\tbreak;\r\n\t\tif(0==sf.AddrPC.Offset)\r\n\t\t\tbreak;\r\n\t\ti=i2;\r\n\t\tif(i)\t// Skip first entry relating to this function\r\n\t\t{\r\n\t\t\tDWORD lineoffset=0;\r\n\t\t\tp->stack[i-1].pc=(void *)(size_t) sf.AddrPC.Offset;\r\n\t\t\tif(SymGetModuleInfo64(sym_myprocess, sf.AddrPC.Offset, &ihm))\r\n\t\t\t{\r\n\t\t\t\tchar *leaf;\r\n\t\t\t\tleaf=strrchr(ihm.ImageName, '\\\\');\r\n\t\t\t\tif(!leaf) leaf=ihm.ImageName-1;\r\n\t\t\t\tCOPY_STRING(p->stack[i-1].module, leaf+1, sizeof(p->stack[i-1].module));\r\n\t\t\t}\r\n\t\t\telse strcpy(p->stack[i-1].module, \"<unknown>\");\r\n\t\t\t//fxmessage(\"WARNING: SymGetModuleInfo64() returned error code %d\\n\", GetLastError());\r\n\t\t\tmemset(temp, 0, MAX_PATH+sizeof(IMAGEHLP_SYMBOL64));\r\n\t\t\tihs=(IMAGEHLP_SYMBOL64 *) temp;\r\n\t\t\tihs->SizeOfStruct=sizeof(IMAGEHLP_SYMBOL64);\r\n\t\t\tihs->Address=sf.AddrPC.Offset;\r\n\t\t\tihs->MaxNameLength=MAX_PATH;\r\n\r\n\t\t\tif(SymGetSymFromAddr64(sym_myprocess, sf.AddrPC.Offset, &offset, ihs))\r\n\t\t\t{\r\n\t\t\t\tCOPY_STRING(p->stack[i-1].functname, ihs->Name, sizeof(p->stack[i-1].functname));\r\n\t\t\t\tif(strlen(p->stack[i-1].functname)<sizeof(p->stack[i-1].functname)-8)\r\n\t\t\t\t{\r\n\t\t\t\t\tsprintf(strchr(p->stack[i-1].functname, 0), \" +0x%x\", offset);\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\telse\r\n\t\t\t\tstrcpy(p->stack[i-1].functname, \"<unknown>\");\r\n\t\t\tif(SymGetLineFromAddr64(sym_myprocess, sf.AddrPC.Offset, &lineoffset, &ihl))\r\n\t\t\t{\r\n\t\t\t\tchar *leaf;\r\n\t\t\t\tp->stack[i-1].lineno=ihl.LineNumber;\r\n\r\n\t\t\t\tleaf=strrchr(ihl.FileName, '\\\\');\r\n\t\t\t\tif(!leaf) leaf=ihl.FileName-1;\r\n\t\t\t\tCOPY_STRING(p->stack[i-1].file, leaf+1, sizeof(p->stack[i-1].file));\r\n\t\t\t}\r\n\t\t\telse\r\n\t\t\t\tstrcpy(p->stack[i-1].file, \"<unknown>\");\r\n\t\t}\r\n\t}\r\n}\r\n#pragma optimize(\"g\", on)\r\n#else\r\nstatic void DoStackWalk(logentry *p) THROWSPEC\r\n{\r\n\tvoid *backtr[NEDMALLOC_STACKBACKTRACEDEPTH];\r\n\tsize_t size;\r\n\tchar **strings;\r\n\tsize_t i2;\r\n\tsize=backtrace(backtr, NEDMALLOC_STACKBACKTRACEDEPTH);\r\n\tstrings=backtrace_symbols(backtr, size);\r\n\tfor(i2=0; i2<size; i2++)\r\n\t{\t// Format can be <file path>(<mangled symbol>+0x<offset>) [<pc>]\r\n\t\t// or can be <file path> [<pc>]\r\n\t\tint start=0, end=strlen(strings[i2]), which=0, idx;\r\n\t\tfor(idx=0; idx<end; idx++)\r\n\t\t{\r\n\t\t\tif(0==which && (' '==strings[i2][idx] || '('==strings[i2][idx]))\r\n\t\t\t{\r\n\t\t\t\tint len=FXMIN(idx-start, (int) sizeof(p->stack[i2].file));\r\n\t\t\t\tmemcpy(p->stack[i2].file, strings[i2]+start, len);\r\n\t\t\t\tp->stack[i2].file[len]=0;\r\n\t\t\t\twhich=(' '==strings[i2][idx]) ? 2 : 1;\r\n\t\t\t\tstart=idx+1;\r\n\t\t\t}\r\n\t\t\telse if(1==which && ')'==strings[i2][idx])\r\n\t\t\t{\r\n\t\t\t\tFXString functname(strings[i2]+start, idx-start);\r\n\t\t\t\tFXint offset=functname.rfind(\"+0x\");\r\n\t\t\t\tFXString rawsymbol(functname.left(offset));\r\n\t\t\t\tFXString symbol(rawsymbol.length() ? fxdemanglesymbol(rawsymbol, false) : rawsymbol);\r\n\t\t\t\tsymbol.append(functname.mid(offset));\r\n\t\t\t\tint len=FXMIN(symbol.length(), (int) sizeof(p->stack[i2].functname));\r\n\t\t\t\tmemcpy(p->stack[i2].functname, symbol.text(), len);\r\n\t\t\t\tp->stack[i2].functname[len]=0;\r\n\t\t\t\twhich=2;\r\n\t\t\t}\r\n\t\t\telse if(2==which && '['==strings[i2][idx])\r\n\t\t\t{\r\n\t\t\t\tstart=idx+1;\r\n\t\t\t\twhich=3;\r\n\t\t\t}\r\n\t\t\telse if(3==which && ']'==strings[i2][idx])\r\n\t\t\t{\r\n\t\t\t\tFXString v(strings[i2]+start+2, idx-start-2);\r\n\t\t\t\tp->stack[i2].pc=(void *)(FXuval)v.toULong(0, 16);\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\tfree(strings);\r\n}\r\n#endif\r\n#endif\r\n#endif\r\nstatic FORCEINLINE logentry *LogOperation(threadcache *tc, nedpool *np, LogEntryType type, int mspace, size_t size, void *mem, size_t alignment, unsigned flags, void *returned) THROWSPEC\r\n{\r\n#if ENABLE_LOGGING\r\n\tif(tc->logentries && NEDMALLOC_TESTLOGENTRY(tc, np, type, mspace, size, mem, alignment, flags, returned))\r\n\t{\r\n\t\tlogentry *le;\r\n\t\tif(tc->logentriesptr==tc->logentriesend)\r\n\t\t{\r\n\t\t\tmchunkptr cp=mem2chunk(tc->logentries);\r\n\t\t\tsize_t logentrieslen=chunksize(cp)-overhead_for(cp);\r\n\t\t\tle=(logentry *) CallRealloc(0, tc->logentries, 0, logentrieslen, (logentrieslen*3)/2, 0, M2_ZERO_MEMORY|M2_ALWAYS_MMAP|M2_RESERVE_MULT(8));\r\n\t\t\tif(!le) return 0;\r\n\t\t\ttc->logentriesptr=le+(tc->logentriesptr-tc->logentries);\r\n\t\t\ttc->logentries=le;\r\n\t\t\tcp=mem2chunk(tc->logentries);\r\n\t\t\tlogentrieslen=(chunksize(cp)-overhead_for(cp))/sizeof(logentry);\r\n\t\t\ttc->logentriesend=tc->logentries+logentrieslen;\r\n\t\t}\r\n\t\tle=tc->logentriesptr++;\r\n\t\tassert(le+1<=tc->logentriesend);\r\n\t\tle->timestamp=GetTimestamp();\r\n\t\tle->np=np;\r\n\t\tle->type=type;\r\n\t\tle->mspace=mspace;\r\n\t\tle->size=size;\r\n\t\tle->mem=mem;\r\n\t\tle->alignment=alignment;\r\n\t\tle->flags=flags;\r\n\t\tle->returned=returned;\r\n#if NEDMALLOC_STACKBACKTRACEDEPTH\r\n\t\tDoStackWalk(le);\r\n#endif\r\n\t\treturn le;\r\n\t}\r\n#endif\r\n\treturn 0;\r\n}\r\n\r\nstatic FORCEINLINE NEDMALLOCNOALIASATTR unsigned int size2binidx(size_t _size) THROWSPEC\r\n{\t/* 8=1000\t16=10000\t20=10100\t24=11000\t32=100000\t48=110000\t4096=1000000000000 */\r\n\tunsigned int topbit, size=(unsigned int)(_size>>4);\r\n\t/* 16=1\t\t20=1\t24=1\t32=10\t48=11\t64=100\t96=110\t128=1000\t4096=100000000 */\r\n\r\n#if defined(__GNUC__)\r\n        topbit = sizeof(size)*__CHAR_BIT__ - 1 - __builtin_clz(size);\r\n#elif defined(_MSC_VER) && _MSC_VER>=1300\r\n\t{\r\n            unsigned long bsrTopBit;\r\n\r\n            _BitScanReverse(&bsrTopBit, size);\r\n\r\n            topbit = bsrTopBit;\r\n        }\r\n#else\r\n#if 0\r\n\tunion {\r\n\t\tunsigned asInt[2];\r\n\t\tdouble asDouble;\r\n\t};\r\n\tint n;\r\n\r\n\tasDouble = (double)size + 0.5;\r\n\ttopbit = (asInt[!FOX_BIGENDIAN] >> 20) - 1023;\r\n#else\r\n\t{\r\n\t\tunsigned int x=size;\r\n\t\tx = x | (x >> 1);\r\n\t\tx = x | (x >> 2);\r\n\t\tx = x | (x >> 4);\r\n\t\tx = x | (x >> 8);\r\n\t\tx = x | (x >>16);\r\n\t\tx = ~x;\r\n\t\tx = x - ((x >> 1) & 0x55555555);\r\n\t\tx = (x & 0x33333333) + ((x >> 2) & 0x33333333);\r\n\t\tx = (x + (x >> 4)) & 0x0F0F0F0F;\r\n\t\tx = x + (x << 8);\r\n\t\tx = x + (x << 16);\r\n\t\ttopbit=31 - (x >> 24);\r\n\t}\r\n#endif\r\n#endif\r\n\treturn topbit;\r\n}\r\n\r\n\r\n#ifdef FULLSANITYCHECKS\r\nstatic void tcsanitycheck(threadcacheblk *RESTRICT *RESTRICT ptr) THROWSPEC\r\n{\r\n\tassert((ptr[0] && ptr[1]) || (!ptr[0] && !ptr[1]));\r\n\tif(ptr[0] && ptr[1])\r\n\t{\r\n\t\tassert(ptr[0]->isforeign || nedblkmstate(ptr[0]));\r\n\t\tassert(ptr[1]->isforeign || nedblkmstate(ptr[1]));\r\n\t\tassert(nedblksize(0, ptr[0], 0)>=sizeof(threadcacheblk));\r\n\t\tassert(nedblksize(0, ptr[1], 0)>=sizeof(threadcacheblk));\r\n\t\tassert(*(unsigned int *) \"NEDN\"==ptr[0]->magic);\r\n\t\tassert(*(unsigned int *) \"NEDN\"==ptr[1]->magic);\r\n\t\tassert(!ptr[0]->prev);\r\n\t\tassert(!ptr[1]->next);\r\n\t\tif(ptr[0]==ptr[1])\r\n\t\t{\r\n\t\t\tassert(!ptr[0]->next);\r\n\t\t\tassert(!ptr[1]->prev);\r\n\t\t}\r\n\t}\r\n}\r\nstatic void tcfullsanitycheck(threadcache *tc) THROWSPEC\r\n{\r\n\tthreadcacheblk *RESTRICT *RESTRICT tcbptr=tc->bins;\r\n\tint n;\r\n\tfor(n=0; n<=THREADCACHEMAXBINS; n++, tcbptr+=2)\r\n\t{\r\n\t\tthreadcacheblk *RESTRICT b, *RESTRICT ob=0;\r\n\t\ttcsanitycheck(tcbptr);\r\n\t\tfor(b=tcbptr[0]; b; ob=b, b=b->next)\r\n\t\t{\r\n\t\t\tassert(b->isforeign || nedblkmstate(b));\r\n\t\t\tassert(nedblksize(0, b, 0)>=sizeof(threadcacheblk));\r\n\t\t\tassert(*(unsigned int *) \"NEDN\"==b->magic);\r\n\t\t\tassert(!ob || ob->next==b);\r\n\t\t\tassert(!ob || b->prev==ob);\r\n\t\t}\r\n\t}\r\n}\r\n#endif\r\n\r\nstatic NOINLINE int InitPool(nedpool *RESTRICT p, size_t capacity, int threads) THROWSPEC;\r\nstatic NOINLINE void RemoveCacheEntries(nedpool *RESTRICT p, threadcache *RESTRICT tc, unsigned int age) THROWSPEC\r\n{\r\n#ifdef FULLSANITYCHECKS\r\n\ttcfullsanitycheck(tc);\r\n#endif\r\n\tif(tc->freeInCache)\r\n\t{\r\n\t\tthreadcacheblk *RESTRICT *RESTRICT tcbptr=tc->bins;\r\n\t\tint n;\r\n\t\tfor(n=0; n<=THREADCACHEMAXBINS; n++, tcbptr+=2)\r\n\t\t{\r\n\t\t\tthreadcacheblk *RESTRICT *RESTRICT tcb=tcbptr+1;\t\t/* come from oldest end of list */\r\n\t\t\t/*tcsanitycheck(tcbptr);*/\r\n\t\t\tfor(; *tcb && tc->frees-(*tcb)->lastUsed>=age; )\r\n\t\t\t{\r\n\t\t\t\tthreadcacheblk *RESTRICT f=*tcb;\r\n\t\t\t\tsize_t blksize=f->size; /*nedblksize(f);*/\r\n\t\t\t\tassert(blksize<=nedblksize(0, f, 0));\r\n\t\t\t\tassert(blksize);\r\n#ifdef FULLSANITYCHECKS\r\n\t\t\t\tassert(*(unsigned int *) \"NEDN\"==(*tcb)->magic);\r\n#endif\r\n\t\t\t\t*tcb=(*tcb)->prev;\r\n\t\t\t\tif(*tcb)\r\n\t\t\t\t\t(*tcb)->next=0;\r\n\t\t\t\telse\r\n\t\t\t\t\t*tcbptr=0;\r\n\t\t\t\ttc->freeInCache-=blksize;\r\n\t\t\t\tassert((long) tc->freeInCache>=0);\r\n\t\t\t\tCallFree(0, f, f->isforeign);\r\n\t\t\t\t/*tcsanitycheck(tcbptr);*/\r\n\t\t\t\tLogOperation(tc, p, LOGENTRY_THREADCACHE_CLEAN, age, blksize, f, 0, 0, 0);\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n#ifdef FULLSANITYCHECKS\r\n\ttcfullsanitycheck(tc);\r\n#endif\r\n}\r\nsize_t nedflushlogs(nedpool *p, char *filepath) THROWSPEC\r\n{\r\n\tsize_t count=0;\r\n\tif(!p)\r\n\t{\r\n\t\tp=&syspool;\r\n\t\tif(!syspool.threads) InitPool(&syspool, 0, -1);\r\n\t}\r\n\tif(p->caches)\r\n\t{\r\n\t\tthreadcache *tc;\r\n\t\tint n;\r\n#if ENABLE_LOGGING\r\n\t\tint haslogentries=0;\r\n#endif\r\n\t\tfor(n=0; n<THREADCACHEMAXCACHES; n++)\r\n\t\t{\r\n\t\t\tif((tc=p->caches[n]))\r\n\t\t\t{\r\n\t\t\t\tcount+=tc->freeInCache;\r\n\t\t\t\ttc->frees++;\r\n\t\t\t\tRemoveCacheEntries(p, tc, 0);\r\n\t\t\t\tassert(!tc->freeInCache);\r\n#if ENABLE_LOGGING\r\n\t\t\t\thaslogentries|=!!tc->logentries;\r\n#endif\r\n\t\t\t}\r\n\t\t}\r\n#if ENABLE_LOGGING\r\n\t\tif(haslogentries)\r\n\t\t{\r\n\t\t\tchar buffer[MAX_PATH]=NEDMALLOC_LOGFILE;\r\n\t\t\tFILE *oh;\r\n\t\t\tfpos_t pos1, pos2;\r\n\t\t\tif(!filepath) filepath=buffer;\r\n\t\t\toh=fopen(filepath, \"r+\");\r\n\t\t\twhile(!oh)\r\n\t\t\t{\r\n\t\t\t\tchar *bptr;\r\n\t\t\t\tif((oh=fopen(filepath, \"w\"))) break;\r\n\t\t\t\tif(ENOSPC==errno) break;\r\n\t\t\t\tbptr=strrchr(filepath, '.');\r\n\t\t\t\tif(bptr-filepath>=MAX_PATH-6) abort();\r\n\t\t\t\tmemcpy(bptr, \"!.csv\", 6);\r\n\t\t\t}\r\n\t\t\tif(oh)\r\n\t\t\t{\r\n\t\t\t\tfgetpos(oh, &pos1);\r\n\t\t\t\tfseek(oh, 0, SEEK_END);\r\n\t\t\t\tfgetpos(oh, &pos2);\r\n\t\t\t\tif(pos1==pos2)\r\n\t\t\t\t\tfprintf(oh, \"Timestamp, Pool, Operation, MSpace, Size, Block, Alignment, Flags, Returned,\\\"Stack Backtrace\\\"\\n\");\r\n\t\t\t\tfor(n=0; n<THREADCACHEMAXCACHES; n++)\r\n\t\t\t\t{\r\n\t\t\t\t\tif((tc=p->caches[n]) && tc->logentries)\r\n\t\t\t\t\t{\r\n\t\t\t\t\t\tlogentry *le;\r\n\t\t\t\t\t\tfor(le=tc->logentries; le<tc->logentriesptr; le++)\r\n\t\t\t\t\t\t{\r\n\t\t\t\t\t\t\tconst char *LogEntryTypeString=LogEntryTypeStrings[size2binidx(((size_t)le->type)<<4)];\r\n\t\t\t\t\t\t\tchar stackbacktrace[16384]=\"?\";\r\n#if NEDMALLOC_STACKBACKTRACEDEPTH\r\n\t\t\t\t\t\t\tchar *sbtp=stackbacktrace;\r\n\t\t\t\t\t\t\tint i;\r\n\t\t\t\t\t\t\tfor(i=0; i<NEDMALLOC_STACKBACKTRACEDEPTH && le->stack[i].pc; i++)\r\n\t\t\t\t\t\t\t{\r\n\t\t\t\t\t\t\t\tsbtp+=sprintf(sbtp, \"0x%p:%s:%s (%s:%u),\",\r\n\t\t\t\t\t\t\t\t\tle->stack[i].pc, le->stack[i].module, le->stack[i].functname, le->stack[i].file, le->stack[i].lineno);\r\n\t\t\t\t\t\t\t\tif(sbtp>=stackbacktrace+sizeof(stackbacktrace)) abort();\r\n\t\t\t\t\t\t\t}\r\n\t\t\t\t\t\t\tif(NEDMALLOC_STACKBACKTRACEDEPTH==i)\r\n\t\t\t\t\t\t\t\tstrcpy(sbtp, \"<backtrace may continue ...>\");\r\n\t\t\t\t\t\t\telse\r\n\t\t\t\t\t\t\t\tstrcpy(sbtp, \"<backtrace ends>\");\r\n\t\t\t\t\t\t\tif(strchr(sbtp, 0)>=stackbacktrace+sizeof(stackbacktrace)) abort();\r\n#endif\r\n\t\t\t\t\t\t\tfprintf(oh, \"%llu, 0x%p, %s, %d, %Iu, 0x%p, %Iu, 0x%x, 0x%p,\\\"%s\\\"\\n\",\r\n\t\t\t\t\t\t\t\tle->timestamp, le->np, LogEntryTypeString, le->mspace, le->size, le->mem, le->alignment, le->flags, le->returned, stackbacktrace);\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t\tCallFree(0, tc->logentries, 0);\r\n\t\t\t\t\t\ttc->logentries=tc->logentriesptr=tc->logentriesend=0;\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t\tfclose(oh);\r\n\t\t\t}\r\n\t\t}\r\n#endif\r\n\t}\r\n\treturn count;\r\n}\r\nstatic void DestroyCaches(nedpool *RESTRICT p) THROWSPEC\r\n{\r\n\tif(p->caches)\r\n\t{\r\n\t\tthreadcache *tc;\r\n\t\tint n;\r\n\t\tnedflushlogs(p, 0);\r\n\t\tfor(n=0; n<THREADCACHEMAXCACHES; n++)\r\n\t\t{\r\n\t\t\tif((tc=p->caches[n]))\r\n\t\t\t{\r\n\t\t\t\ttc->mymspace=-1;\r\n\t\t\t\ttc->threadid=0;\r\n\t\t\t\tCallFree(0, tc, 0);\r\n\t\t\t\tp->caches[n]=0;\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\r\n\r\nstatic NOINLINE threadcache *AllocCache(nedpool *RESTRICT p) THROWSPEC\r\n{\r\n\tthreadcache *tc=0;\r\n\tint n, end;\r\n#if USE_LOCKS\r\n\tACQUIRE_LOCK(&p->mutex);\r\n#endif\r\n\tfor(n=0; n<THREADCACHEMAXCACHES && p->caches[n]; n++);\r\n\tif(THREADCACHEMAXCACHES==n)\r\n\t{\t/* List exhausted, so disable for this thread */\r\n#if USE_LOCKS\r\n\t\tRELEASE_LOCK(&p->mutex);\r\n#endif\r\n\t\treturn 0;\r\n\t}\r\n\ttc=p->caches[n]=(threadcache *) CallMalloc(p->m[0], sizeof(threadcache), 0, M2_ZERO_MEMORY);\r\n\tif(!tc)\r\n\t{\r\n#if USE_LOCKS\r\n\t\tRELEASE_LOCK(&p->mutex);\r\n#endif\r\n\t\treturn 0;\r\n\t}\r\n#ifdef FULLSANITYCHECKS\r\n\ttc->magic1=*(unsigned int *)\"NEDMALC1\";\r\n\ttc->magic2=*(unsigned int *)\"NEDMALC2\";\r\n#endif\r\n\ttc->threadid=\r\n#if USE_LOCKS\r\n\t\t(long)(size_t)CURRENT_THREAD;\r\n#else\r\n\t\t1;\r\n#endif\r\n\tfor(end=0; p->m[end]; end++);\r\n\ttc->mymspace=abs(tc->threadid) % end;\r\n#if ENABLE_LOGGING\r\n\t{\r\n\t\tmchunkptr cp;\r\n\t\tsize_t logentrieslen=2048/sizeof(logentry);\t\t/* One page */\r\n\t\ttc->logentries=tc->logentriesptr=(logentry *) CallMalloc(p->m[0], logentrieslen*sizeof(logentry), 0, M2_ZERO_MEMORY|M2_ALWAYS_MMAP|M2_RESERVE_MULT(8));\r\n\t\tif(!tc->logentries)\r\n\t\t{\r\n#if USE_LOCKS\r\n\t\t\tRELEASE_LOCK(&p->mutex);\r\n#endif\r\n\t\t\treturn 0;\r\n\t\t}\r\n\t\tcp=mem2chunk(tc->logentries);\r\n\t\tlogentrieslen=(chunksize(cp)-overhead_for(cp))/sizeof(logentry);\r\n\t\ttc->logentriesend=tc->logentries+logentrieslen;\r\n\t}\r\n#endif\r\n#if USE_LOCKS\r\n\tRELEASE_LOCK(&p->mutex);\r\n#endif\r\n\tif(TLSSET(p->mycache, (void *)(size_t)(n+1))) abort();\r\n\treturn tc;\r\n}\r\n\r\nstatic void *threadcache_malloc(nedpool *RESTRICT p, threadcache *RESTRICT tc, size_t *RESTRICT _size) THROWSPEC\r\n{\r\n\tvoid *RESTRICT ret=0;\r\n\tsize_t size=*_size, blksize=0;\r\n\tunsigned int bestsize;\r\n\tunsigned int idx=size2binidx(size);\r\n\tthreadcacheblk *RESTRICT blk, *RESTRICT *RESTRICT binsptr;\r\n#ifdef FULLSANITYCHECKS\r\n\ttcfullsanitycheck(tc);\r\n#endif\r\n\t/* Calculate best fit bin size */\r\n\tbestsize=1<<(idx+4);\r\n#if 0\r\n\t/* Finer grained bin fit */\r\n\tidx<<=1;\r\n\tif(size>bestsize)\r\n\t{\r\n\t\tidx++;\r\n\t\tbestsize+=bestsize>>1;\r\n\t}\r\n\tif(size>bestsize)\r\n\t{\r\n\t\tidx++;\r\n\t\tbestsize=1<<(4+(idx>>1));\r\n\t}\r\n#else\r\n\tif(size>bestsize)\r\n\t{\r\n\t\tidx++;\r\n\t\tbestsize<<=1;\r\n\t}\r\n#endif\r\n\tassert(bestsize>=size);\r\n\tif(size<bestsize) size=bestsize;\r\n\tassert(size<=THREADCACHEMAX);\r\n\tassert(idx<=THREADCACHEMAXBINS);\r\n\tbinsptr=&tc->bins[idx*2];\r\n\t/* Try to match close, but move up a bin if necessary */\r\n\tblk=*binsptr;\r\n\tif(!blk || blk->size<size)\r\n\t{\t/* Bump it up a bin */\r\n\t\tif(idx<THREADCACHEMAXBINS)\r\n\t\t{\r\n\t\t\tidx++;\r\n\t\t\tbinsptr+=2;\r\n\t\t\tblk=*binsptr;\r\n\t\t}\r\n\t}\r\n\tif(blk)\r\n\t{\r\n\t\tblksize=blk->size; /*nedblksize(blk);*/\r\n\t\tassert(nedblksize(0, blk, 0)>=blksize);\r\n\t\tassert(blksize>=size);\r\n\t\tif(blk->next)\r\n\t\t\tblk->next->prev=0;\r\n\t\t*binsptr=blk->next;\r\n\t\tif(!*binsptr)\r\n\t\t\tbinsptr[1]=0;\r\n#ifdef FULLSANITYCHECKS\r\n\t\tblk->magic=0;\r\n#endif\r\n\t\tassert(binsptr[0]!=blk && binsptr[1]!=blk);\r\n\t\tassert(nedblksize(0, blk, 0)>=sizeof(threadcacheblk) && nedblksize(0, blk, 0)<=THREADCACHEMAX+CHUNK_OVERHEAD);\r\n\t\t/*printf(\"malloc: %p, %p, %p, %lu\\n\", p, tc, blk, (long) _size);*/\r\n\t\tret=(void *) blk;\r\n\t}\r\n\t++tc->mallocs;\r\n\tif(ret)\r\n\t{\r\n\t\tassert(blksize>=size);\r\n\t\t++tc->successes;\r\n\t\ttc->freeInCache-=blksize;\r\n\t\tassert((long) tc->freeInCache>=0);\r\n\t}\r\n#if defined(DEBUG) && 0\r\n\tif(!(tc->mallocs & 0xfff))\r\n\t{\r\n\t\tprintf(\"*** threadcache=%u, mallocs=%u (%f), free=%u (%f), freeInCache=%u\\n\", (unsigned int) tc->threadid, tc->mallocs,\r\n\t\t\t(float) tc->successes/tc->mallocs, tc->frees, (float) tc->successes/tc->frees, (unsigned int) tc->freeInCache);\r\n\t}\r\n#endif\r\n#ifdef FULLSANITYCHECKS\r\n\ttcfullsanitycheck(tc);\r\n#endif\r\n\t*_size=size;\r\n\treturn ret;\r\n}\r\nstatic NOINLINE void ReleaseFreeInCache(nedpool *RESTRICT p, threadcache *RESTRICT tc, int mymspace) THROWSPEC\r\n{\r\n\tunsigned int age=THREADCACHEMAXFREESPACE/8192;\r\n#if USE_LOCKS\r\n\t/*ACQUIRE_LOCK(&p->m[mymspace]->mutex);*/\r\n#endif\r\n\twhile(age && tc->freeInCache>=THREADCACHEMAXFREESPACE)\r\n\t{\r\n\t\tRemoveCacheEntries(p, tc, age);\r\n\t\t/*printf(\"*** Removing cache entries older than %u (%u)\\n\", age, (unsigned int) tc->freeInCache);*/\r\n\t\tage>>=1;\r\n\t}\r\n#if USE_LOCKS\r\n\t/*RELEASE_LOCK(&p->m[mymspace]->mutex);*/\r\n#endif\r\n}\r\nstatic void threadcache_free(nedpool *RESTRICT p, threadcache *RESTRICT tc, int mymspace, void *RESTRICT mem, size_t size, int isforeign) THROWSPEC\r\n{\r\n\tunsigned int bestsize;\r\n\tunsigned int idx=size2binidx(size);\r\n\tthreadcacheblk *RESTRICT *RESTRICT binsptr, *RESTRICT tck=(threadcacheblk *RESTRICT) mem;\r\n\tassert(size>=sizeof(threadcacheblk) && size<=THREADCACHEMAX+CHUNK_OVERHEAD);\r\n#ifdef DEBUG\r\n\t/* Make sure this is a valid memory block */\r\n\tassert(nedblksize(0, mem, 0));\r\n#endif\r\n#ifdef FULLSANITYCHECKS\r\n\ttcfullsanitycheck(tc);\r\n#endif\r\n\t/* Calculate best fit bin size */\r\n\tbestsize=1<<(idx+4);\r\n#if 0\r\n\t/* Finer grained bin fit */\r\n\tidx<<=1;\r\n\tif(size>bestsize)\r\n\t{\r\n\t\tunsigned int biggerbestsize=bestsize+bestsize<<1;\r\n\t\tif(size>=biggerbestsize)\r\n\t\t{\r\n\t\t\tidx++;\r\n\t\t\tbestsize=biggerbestsize;\r\n\t\t}\r\n\t}\r\n#endif\r\n\tif(bestsize!=size)\t/* dlmalloc can round up, so we round down to preserve indexing */\r\n\t\tsize=bestsize;\r\n\tbinsptr=&tc->bins[idx*2];\r\n\tassert(idx<=THREADCACHEMAXBINS);\r\n\tif(tck==*binsptr)\r\n\t{\r\n\t\tfprintf(stderr, \"nedmalloc: Attempt to free already freed memory block %p - aborting!\\n\", tck);\r\n\t\tabort();\r\n\t}\r\n#ifdef FULLSANITYCHECKS\r\n\ttck->magic=*(unsigned int *) \"NEDN\";\r\n#endif\r\n\ttck->isforeign=isforeign;\r\n\ttck->lastUsed=++tc->frees;\r\n\ttck->size=(unsigned int) size;\r\n\ttck->next=*binsptr;\r\n\ttck->prev=0;\r\n\tif(tck->next)\r\n\t\ttck->next->prev=tck;\r\n\telse\r\n\t\tbinsptr[1]=tck;\r\n\tassert(!*binsptr || (*binsptr)->size==tck->size);\r\n\t*binsptr=tck;\r\n\tassert(tck==tc->bins[idx*2]);\r\n\tassert(tc->bins[idx*2+1]==tck || binsptr[0]->next->prev==tck);\r\n\t/*printf(\"free: %p, %p, %p, %lu\\n\", p, tc, mem, (long) size);*/\r\n\ttc->freeInCache+=size;\r\n#ifdef FULLSANITYCHECKS\r\n\ttcfullsanitycheck(tc);\r\n#endif\r\n#if 1\r\n\tif(tc->freeInCache>=THREADCACHEMAXFREESPACE)\r\n\t\tReleaseFreeInCache(p, tc, mymspace);\r\n#endif\r\n}\r\n\r\n\r\n\r\n\r\nstatic NOINLINE int InitPool(nedpool *RESTRICT p, size_t capacity, int threads) THROWSPEC\r\n{\t/* threads is -1 for system pool */\r\n\tensure_initialization();\r\n\tACQUIRE_MALLOC_GLOBAL_LOCK();\r\n\tif(p->threads) goto done;\r\n#if USE_LOCKS\r\n\tif(INITIAL_LOCK(&p->mutex)) goto err;\r\n#endif\r\n\tif(TLSALLOC(&p->mycache)) goto err;\r\n#if USE_ALLOCATOR==0\r\n\tp->m[0]=(mstate) mspacecounter++;\r\n#elif USE_ALLOCATOR==1\r\n\tif(!(p->m[0]=(mstate) create_mspace(capacity, 1))) goto err;\r\n\tp->m[0]->extp=p;\r\n#endif\r\n\tp->threads=(threads>MAXTHREADSINPOOL) ? MAXTHREADSINPOOL : (!threads) ? DEFAULTMAXTHREADSINPOOL : threads;\r\ndone:\r\n\tRELEASE_MALLOC_GLOBAL_LOCK();\r\n\treturn 1;\r\nerr:\r\n\tif(threads<0)\r\n\t\tabort();\t\t\t/* If you can't allocate for system pool, we're screwed */\r\n\tDestroyCaches(p);\r\n\tif(p->m[0])\r\n\t{\r\n#if USE_ALLOCATOR==1\r\n\t\tdestroy_mspace(p->m[0]);\r\n#endif\r\n\t\tp->m[0]=0;\r\n\t}\r\n\tif(p->mycache)\r\n\t{\r\n\t\tif(TLSFREE(p->mycache)) abort();\r\n\t\tp->mycache=0;\r\n\t}\r\n\tRELEASE_MALLOC_GLOBAL_LOCK();\r\n\treturn 0;\r\n}\r\nstatic NOINLINE mstate FindMSpace(nedpool *RESTRICT p, threadcache *RESTRICT tc, int *RESTRICT lastUsed, size_t size) THROWSPEC\r\n{\t/* Gets called when thread's last used mspace is in use. The strategy\r\n\tis to run through the list of all available mspaces looking for an\r\n\tunlocked one and if we fail, we create a new one so long as we don't\r\n\texceed p->threads */\r\n\tint n, end;\r\n\tn=end=*lastUsed+1;\r\n#if USE_LOCKS\r\n\tfor(; p->m[n]; end=++n)\r\n\t{\r\n\t\tif(TRY_LOCK(&p->m[n]->mutex)) goto found;\r\n\t}\r\n\tfor(n=0; n<*lastUsed && p->m[n]; n++)\r\n\t{\r\n\t\tif(TRY_LOCK(&p->m[n]->mutex)) goto found;\r\n\t}\r\n\tif(end<p->threads)\r\n\t{\r\n\t\tmstate temp;\r\n#if USE_ALLOCATOR==0\r\n\t\ttemp=(mstate) mspacecounter++;\r\n#elif USE_ALLOCATOR==1\r\n\t\tif(!(temp=(mstate) create_mspace(size, 1)))\r\n\t\t\tgoto badexit;\r\n#endif\r\n\t\t/* Now we're ready to modify the lists, we lock */\r\n\t\tACQUIRE_LOCK(&p->mutex);\r\n\t\twhile(p->m[end] && end<p->threads)\r\n\t\t\tend++;\r\n\t\tif(end>=p->threads)\r\n\t\t{\t/* Drat, must destroy it now */\r\n\t\t\tRELEASE_LOCK(&p->mutex);\r\n#if USE_ALLOCATOR==1\r\n\t\t\tdestroy_mspace((mstate) temp);\r\n#endif\r\n\t\t\tgoto badexit;\r\n\t\t}\r\n\t\t/* We really want to make sure this goes into memory now but we\r\n\t\thave to be careful of breaking aliasing rules, so write it twice */\r\n\t\t{\r\n\t\t\tvolatile struct malloc_state **_m=(volatile struct malloc_state **) &p->m[end];\r\n\t\t\t*_m=(p->m[end]=temp);\r\n\t\t}\r\n\t\tACQUIRE_LOCK(&p->m[end]->mutex);\r\n\t\t/*printf(\"Created mspace idx %d\\n\", end);*/\r\n\t\tRELEASE_LOCK(&p->mutex);\r\n\t\tn=end;\r\n\t\tgoto found;\r\n\t}\r\n\t/* Let it lock on the last one it used */\r\nbadexit:\r\n\tACQUIRE_LOCK(&p->m[*lastUsed]->mutex);\r\n\treturn p->m[*lastUsed];\r\n#endif\r\nfound:\r\n\t*lastUsed=n;\r\n\tif(tc)\r\n\t\ttc->mymspace=n;\r\n\telse\r\n\t{\r\n\t\tif(TLSSET(p->mycache, (void *)(size_t)(-(n+1)))) abort();\r\n\t}\r\n\treturn p->m[n];\r\n}\r\n\r\ntypedef struct PoolList_t\r\n{\r\n\tsize_t size;\t\t\t/* Size of list */\r\n\tsize_t length;\t\t\t/* Actual entries in list */\r\n#ifdef DEBUG\r\n\tnedpool *list[1];\t\t/* Force testing of list expansion */\r\n#else\r\n\tnedpool *list[16];\r\n#endif\r\n} PoolList;\r\n#if USE_LOCKS\r\nstatic MLOCK_T poollistlock;\r\n#endif\r\nstatic PoolList *poollist;\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR nedpool *nedcreatepool(size_t capacity, int threads) THROWSPEC\r\n{\r\n\tnedpool *ret=0;\r\n\tif(!poollist)\r\n\t{\r\n\t\tPoolList *newpoollist=0;\r\n\t\tif(!(newpoollist=(PoolList *) nedpcalloc(0, 1, sizeof(PoolList)+sizeof(nedpool *)))) return 0;\r\n#if USE_LOCKS\r\n\t\tINITIAL_LOCK(&poollistlock);\r\n\t\tACQUIRE_LOCK(&poollistlock);\r\n#endif\r\n\t\tpoollist=newpoollist;\r\n\t\tpoollist->size=sizeof(poollist->list)/sizeof(nedpool *);\r\n\t}\r\n#if USE_LOCKS\r\n\telse\r\n\t\tACQUIRE_LOCK(&poollistlock);\r\n#endif\r\n\tif(poollist->length==poollist->size)\r\n\t{\r\n\t\tPoolList *newpoollist=0;\r\n\t\tsize_t newsize=0;\r\n\t\tnewsize=sizeof(PoolList)+(poollist->size+1)*sizeof(nedpool *);\r\n\t\tif(!(newpoollist=(PoolList *) nedprealloc(0, poollist, newsize))) goto badexit;\r\n\t\tpoollist=newpoollist;\r\n\t\tmemset(&poollist->list[poollist->size], 0, newsize-((size_t)&poollist->list[poollist->size]-(size_t)&poollist->list[0]));\r\n\t\tpoollist->size=((newsize-((char *)&poollist->list[0]-(char *)poollist))/sizeof(nedpool *))-1;\r\n\t\tassert(poollist->size>poollist->length);\r\n\t}\r\n\tif(!(ret=(nedpool *) nedpcalloc(0, 1, sizeof(nedpool)))) goto badexit;\r\n\tif(!InitPool(ret, capacity, threads))\r\n\t{\r\n\t\tnedpfree(0, ret);\r\n\t\tgoto badexit;\r\n\t}\r\n\tpoollist->list[poollist->length++]=ret;\r\nbadexit:\r\n\t{\r\n#if USE_LOCKS\r\n\t\tRELEASE_LOCK(&poollistlock);\r\n#endif\r\n\t}\r\n\treturn ret;\r\n}\r\nvoid neddestroypool(nedpool *p) THROWSPEC\r\n{\r\n\tunsigned int n;\r\n#if USE_LOCKS\r\n\tACQUIRE_LOCK(&p->mutex);\r\n#endif\r\n\tDestroyCaches(p);\r\n\tfor(n=0; p->m[n]; n++)\r\n\t{\r\n#if USE_ALLOCATOR==1\r\n\t\tdestroy_mspace(p->m[n]);\r\n#endif\r\n\t\tp->m[n]=0;\r\n\t}\r\n#if USE_LOCKS\r\n\tRELEASE_LOCK(&p->mutex);\r\n\tDESTROY_LOCK(&p->mutex);\r\n#endif\r\n\tif(TLSFREE(p->mycache)) abort();\r\n\tnedpfree(0, p);\r\n#if USE_LOCKS\r\n\tACQUIRE_LOCK(&poollistlock);\r\n#endif\r\n\tassert(poollist);\r\n\tfor(n=0; n<poollist->length && poollist->list[n]!=p; n++);\r\n\tassert(n!=poollist->length);\r\n\tmemmove(&poollist->list[n], &poollist->list[n+1], (size_t)&poollist->list[poollist->length]-(size_t)&poollist->list[n]);\r\n\tif(!--poollist->length)\r\n\t{\r\n\t\tassert(!poollist->list[0]);\r\n\t\tnedpfree(0, poollist);\r\n\t\tpoollist=0;\r\n\t}\r\n#if USE_LOCKS\r\n\tRELEASE_LOCK(&poollistlock);\r\n#endif\r\n}\r\nvoid neddestroysyspool() THROWSPEC\r\n{\r\n\tnedpool *p=&syspool;\r\n\tint n;\r\n#if USE_LOCKS\r\n\tACQUIRE_LOCK(&p->mutex);\r\n#endif\r\n\tDestroyCaches(p);\r\n\tfor(n=0; p->m[n]; n++)\r\n\t{\r\n#if USE_ALLOCATOR==1\r\n\t\tdestroy_mspace(p->m[n]);\r\n#endif\r\n\t\tp->m[n]=0;\r\n\t}\r\n\t/* Render syspool unusable */\r\n\tfor(n=0; n<THREADCACHEMAXCACHES; n++)\r\n\t\tp->caches[n]=(threadcache *)(size_t)(sizeof(size_t)>4 ? 0xdeadbeefdeadbeefULL : 0xdeadbeefUL);\r\n\tfor(n=0; n<MAXTHREADSINPOOL+1; n++)\r\n\t\tp->m[n]=(mstate)(size_t)(sizeof(size_t)>4 ? 0xdeadbeefdeadbeefULL : 0xdeadbeefUL);\r\n\tif(TLSFREE(p->mycache)) abort();\r\n#if USE_LOCKS\r\n\tRELEASE_LOCK(&p->mutex);\r\n\tDESTROY_LOCK(&p->mutex);\r\n#endif\r\n}\r\nnedpool **nedpoollist() THROWSPEC\r\n{\r\n\tnedpool **ret=0;\r\n\tif(poollist)\r\n\t{\r\n#if USE_LOCKS\r\n\t\tACQUIRE_LOCK(&poollistlock);\r\n#endif\r\n\t\tif(!(ret=(nedpool **) nedmalloc((poollist->length+1)*sizeof(nedpool *)))) goto badexit;\r\n\t\tmemcpy(ret, poollist->list, (poollist->length+1)*sizeof(nedpool *));\r\nbadexit:\r\n\t\t{\r\n#if USE_LOCKS\r\n\t\t\tRELEASE_LOCK(&poollistlock);\r\n#endif\r\n\t\t}\r\n\t}\r\n\treturn ret;\r\n}\r\n\r\nvoid nedpsetvalue(nedpool *p, void *v) THROWSPEC\r\n{\r\n\tif(!p) { p=&syspool; if(!syspool.threads) InitPool(&syspool, 0, -1); }\r\n\tp->uservalue=v;\r\n}\r\nvoid *nedgetvalue(nedpool **p, void *mem) THROWSPEC\r\n{\r\n\tnedpool *np=0;\r\n\tmstate fm=nedblkmstate(mem);\r\n\tif(!fm || !fm->extp) return 0;\r\n\tnp=(nedpool *) fm->extp;\r\n\tif(p) *p=np;\r\n\treturn np->uservalue;\r\n}\r\n\r\nvoid nedtrimthreadcache(nedpool *p, int disable) THROWSPEC\r\n{\r\n\tint mycache;\r\n\tif(!p)\r\n\t{\r\n\t\tp=&syspool;\r\n\t\tif(!syspool.threads) InitPool(&syspool, 0, -1);\r\n\t}\r\n\tmycache=(int)(size_t) TLSGET(p->mycache);\r\n\tif(!mycache)\r\n\t{\t/* Set to mspace 0 */\r\n\t\tif(disable && TLSSET(p->mycache, (void *)(size_t)-1)) abort();\r\n\t}\r\n\telse if(mycache>0)\r\n\t{\t/* Set to last used mspace */\r\n\t\tthreadcache *tc=p->caches[mycache-1];\r\n#if defined(DEBUG)\r\n\t\tprintf(\"Threadcache utilisation: %lf%% in cache with %lf%% lost to other threads\\n\",\r\n\t\t\t100.0*tc->successes/tc->mallocs, 100.0*((double) tc->mallocs-tc->frees)/tc->mallocs);\r\n#endif\r\n\t\tif(disable && TLSSET(p->mycache, (void *)(size_t)(-tc->mymspace))) abort();\r\n\t\ttc->frees++;\r\n\t\tRemoveCacheEntries(p, tc, 0);\r\n\t\tassert(!tc->freeInCache);\r\n\t\tif(disable)\r\n\t\t{\r\n\t\t\ttc->mymspace=-1;\r\n\t\t\ttc->threadid=0;\r\n\t\t\tCallFree(0, p->caches[mycache-1], 0);\r\n\t\t\tp->caches[mycache-1]=0;\r\n\t\t}\r\n\t}\r\n}\r\nvoid neddisablethreadcache(nedpool *p) THROWSPEC\r\n{\r\n\tnedtrimthreadcache(p, 1);\r\n}\r\n\r\n#if USE_LOCKS && USE_ALLOCATOR==1\r\n#define GETMSPACE(m,p,tc,ms,s,action)                 \\\r\n  do                                                  \\\r\n  {                                                   \\\r\n    mstate m = GetMSpace((p),(tc),(ms),(s));          \\\r\n    action;                                           \\\r\n\tRELEASE_LOCK(&m->mutex);                          \\\r\n  } while (0)\r\n#else\r\n#define GETMSPACE(m,p,tc,ms,s,action)                 \\\r\n  do                                                  \\\r\n  {                                                   \\\r\n    mstate m = GetMSpace((p),(tc),(ms),(s));          \\\r\n    action;                                           \\\r\n  } while (0)\r\n#endif\r\n\r\nstatic FORCEINLINE mstate GetMSpace(nedpool *RESTRICT p, threadcache *RESTRICT tc, int mymspace, size_t size) THROWSPEC\r\n{\t/* Returns a locked and ready for use mspace */\r\n\tmstate m=p->m[mymspace];\r\n\tassert(m);\r\n#if USE_LOCKS && USE_ALLOCATOR==1\r\n\tif(!TRY_LOCK(&p->m[mymspace]->mutex)) m=FindMSpace(p, tc, &mymspace, size);\r\n\t/*assert(IS_LOCKED(&p->m[mymspace]->mutex));*/\r\n#endif\r\n\treturn m;\r\n}\r\nstatic NOINLINE void GetThreadCache_cold1(nedpool *RESTRICT *RESTRICT p) THROWSPEC\r\n{\r\n\t*p=&syspool;\r\n\tif(!syspool.threads) InitPool(&syspool, 0, -1);\r\n}\r\nstatic NOINLINE void GetThreadCache_cold2(nedpool *RESTRICT *RESTRICT p, threadcache *RESTRICT *RESTRICT tc, int *RESTRICT mymspace, int mycache) THROWSPEC\r\n{\r\n\tif(!mycache)\r\n\t{\t/* Need to allocate a new cache */\r\n\t\t*tc=AllocCache(*p);\r\n\t\tif(!*tc)\r\n\t\t{\t/* Disable */\r\n\t\t\tif(TLSSET((*p)->mycache, (void *)(size_t)-1)) abort();\r\n\t\t\t*mymspace=0;\r\n\t\t}\r\n\t\telse\r\n\t\t\t*mymspace=(*tc)->mymspace;\r\n\t}\r\n\telse\r\n\t{\t/* Cache disabled, but we do have an assigned thread pool */\r\n\t\t*tc=0;\r\n\t\t*mymspace=-mycache-1;\r\n\t}\r\n}\r\nstatic FORCEINLINE void GetThreadCache(nedpool *RESTRICT *RESTRICT p, threadcache *RESTRICT *RESTRICT tc, int *RESTRICT mymspace, size_t *RESTRICT size) THROWSPEC\r\n{\r\n\tint mycache;\r\n#if THREADCACHEMAX\r\n\tif(size && *size<sizeof(threadcacheblk)) *size=sizeof(threadcacheblk);\r\n#endif\r\n\tif(!*p)\r\n\t\tGetThreadCache_cold1(p);\r\n\tmycache=(int)(size_t) TLSGET((*p)->mycache);\r\n\tif(mycache>0)\r\n\t{\t/* Already have a cache */\r\n\t\t*tc=(*p)->caches[mycache-1];\r\n\t\t*mymspace=(*tc)->mymspace;\r\n\t}\r\n\telse GetThreadCache_cold2(p, tc, mymspace, mycache);\r\n\tassert(*mymspace>=0);\r\n#if USE_LOCKS\r\n\tassert(!(*tc) || (long)(size_t)CURRENT_THREAD==(*tc)->threadid);\r\n#endif\r\n#ifdef FULLSANITYCHECKS\r\n\tif(*tc)\r\n\t{\r\n\t\tif(*(unsigned int *)\"NEDMALC1\"!=(*tc)->magic1 || *(unsigned int *)\"NEDMALC2\"!=(*tc)->magic2)\r\n\t\t{\r\n\t\t\tabort();\r\n\t\t}\r\n\t}\r\n#endif\r\n}\r\n\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedpmalloc2(nedpool *p, size_t size, size_t alignment, unsigned flags) THROWSPEC\r\n{\r\n\tvoid *ret=0;\r\n\tthreadcache *tc;\r\n\tint mymspace;\r\n\tGetThreadCache(&p, &tc, &mymspace, &size);\r\n#if THREADCACHEMAX\r\n\tif(alignment<=MALLOC_ALIGNMENT && !(flags & NM_FLAGS_MASK) && tc && size<=THREADCACHEMAX)\r\n\t{\t/* Use the thread cache */\r\n\t\tif((ret=threadcache_malloc(p, tc, &size)))\r\n\t\t{\r\n\t\t\tif((flags & M2_ZERO_MEMORY))\r\n\t\t\t\tmemset(ret, 0, size);\r\n\t\t\tLogOperation(tc, p, LOGENTRY_THREADCACHE_MALLOC, mymspace, size, 0, alignment, flags, ret);\r\n\t\t}\r\n\t}\r\n#endif\r\n\tif(!ret)\r\n\t{\t/* Use this thread's mspace */\r\n        GETMSPACE(m, p, tc, mymspace, size,\r\n                  ret=CallMalloc(m, size, alignment, flags));\r\n\t\tif(ret)\r\n\t\t\tLogOperation(tc, p, LOGENTRY_POOL_MALLOC, mymspace, size, 0, alignment, flags, ret);\r\n\t}\r\n\tLogOperation(tc, p, LOGENTRY_MALLOC, mymspace, size, 0, alignment, flags, ret);\r\n\treturn ret;\r\n}\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedprealloc2(nedpool *p, void *mem, size_t size, size_t alignment, unsigned flags) THROWSPEC\r\n{\r\n\tvoid *ret=0;\r\n\tthreadcache *tc;\r\n\tint mymspace, isforeign=1;\r\n\tsize_t memsize;\r\n\tif(!mem) return nedpmalloc2(p, size, alignment, flags);\r\n#if REALLOC_ZERO_BYTES_FREES\r\n\tif(!size)\r\n\t{\r\n\t\tnedpfree2(p, mem, flags);\r\n\t\treturn 0;\r\n\t}\r\n#endif\r\n\tmemsize=nedblksize(&isforeign, mem, flags);\r\n\tassert(memsize);\r\n\tif(!memsize)\r\n\t{\r\n\t\tfprintf(stderr, \"nedmalloc: nedprealloc() called with a block not created by nedmalloc!\\n\");\r\n\t\tabort();\r\n\t}\r\n\telse if(size<=memsize && memsize-size<\r\n#ifdef DEBUG\r\n\t\t32\r\n#else\r\n\t\t1024\r\n#endif\r\n\t\t)\t\t/* If realloc size is within 1Kb smaller than existing, noop it */\r\n\t\treturn mem;\r\n\tGetThreadCache(&p, &tc, &mymspace, &size);\r\n#if THREADCACHEMAX\r\n\tif(alignment<=MALLOC_ALIGNMENT && !(flags & NM_FLAGS_MASK) && tc && size && size<=THREADCACHEMAX)\r\n\t{\t/* Use the thread cache */\r\n\t\tif((ret=threadcache_malloc(p, tc, &size)))\r\n\t\t{\r\n\t\t\tsize_t tocopy=memsize<size ? memsize : size;\r\n\t\t\tmemcpy(ret, mem, tocopy);\r\n\t\t\tif((flags & M2_ZERO_MEMORY) && size>memsize)\r\n\t\t\t\tmemset((void *)((size_t)ret+memsize), 0, size-memsize);\r\n\t\t\tLogOperation(tc, p, LOGENTRY_THREADCACHE_MALLOC, mymspace, size, mem, alignment, flags, ret);\r\n\t\t\tif(!isforeign && memsize>=sizeof(threadcacheblk) && memsize<=(THREADCACHEMAX+CHUNK_OVERHEAD))\r\n\t\t\t{\r\n\t\t\t\tthreadcache_free(p, tc, mymspace, mem, memsize, isforeign);\r\n\t\t\t\tLogOperation(tc, p, LOGENTRY_THREADCACHE_FREE, mymspace, memsize, mem, 0, 0, 0);\r\n\t\t\t}\r\n\t\t\telse\r\n\t\t\t{\r\n\t\t\t\tCallFree(0, mem, isforeign);\r\n\t\t\t\tLogOperation(tc, p, LOGENTRY_POOL_FREE, mymspace, memsize, mem, 0, 0, 0);\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n#endif\r\n\tif(!ret)\r\n\t{\t/* Reallocs always happen in the mspace they happened in, so skip\r\n\t\tlocking the preferred mspace for this thread */\r\n\t\tret=CallRealloc(p->m[mymspace], mem, isforeign, memsize, size, alignment, flags);\r\n\t\tif(ret)\r\n\t\t\tLogOperation(tc, p, LOGENTRY_POOL_REALLOC, mymspace, size, mem, alignment, flags, ret);\r\n\t}\r\n\tLogOperation(tc, p, LOGENTRY_REALLOC, mymspace, size, mem, alignment, flags, ret);\r\n\treturn ret;\r\n}\r\nNEDMALLOCNOALIASATTR void   nedpfree2(nedpool *p, void *mem, unsigned flags) THROWSPEC\r\n{\t/* Frees always happen in the mspace they happened in, so skip\r\n\tlocking the preferred mspace for this thread */\r\n\tthreadcache *tc;\r\n\tint mymspace, isforeign=1;\r\n\tsize_t memsize;\r\n\tif(!mem)\r\n\t{\t/* If you tried this on FreeBSD you'd be sorry! */\r\n#ifdef DEBUG\r\n\t\tfprintf(stderr, \"nedmalloc: WARNING nedpfree() called with zero. This is not portable behaviour!\\n\");\r\n#endif\r\n\t\treturn;\r\n\t}\r\n\tmemsize=nedblksize(&isforeign, mem, flags);\r\n\tassert(memsize);\r\n\tif(!memsize)\r\n\t{\r\n\t\tfprintf(stderr, \"nedmalloc: nedpfree() called with a block not created by nedmalloc!\\n\");\r\n\t\tabort();\r\n\t}\r\n\tGetThreadCache(&p, &tc, &mymspace, 0);\r\n#if THREADCACHEMAX\r\n\tif(mem && tc && !isforeign && memsize>=sizeof(threadcacheblk) && memsize<=(THREADCACHEMAX+CHUNK_OVERHEAD))\r\n\t{\r\n\t\tthreadcache_free(p, tc, mymspace, mem, memsize, isforeign);\r\n\t\tLogOperation(tc, p, LOGENTRY_THREADCACHE_FREE, mymspace, memsize, mem, 0, 0, 0);\r\n\t}\r\n\telse\r\n#endif\r\n\t{\r\n\t\tCallFree(0, mem, isforeign);\r\n\t\tLogOperation(tc, p, LOGENTRY_POOL_FREE, mymspace, memsize, mem, 0, 0, 0);\r\n\t}\r\n\tLogOperation(tc, p, LOGENTRY_FREE, mymspace, memsize, mem, 0, 0, 0);\r\n}\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedpmalloc(nedpool *p, size_t size) THROWSPEC\r\n{\r\n\tunsigned flags=NEDMALLOC_FORCERESERVE(p, 0, size);\r\n\treturn nedpmalloc2(p, size, 0, flags);\r\n}\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedpcalloc(nedpool *p, size_t no, size_t size) THROWSPEC\r\n{\r\n\tsize_t bytes=no*size;\r\n\t/* Avoid multiplication overflow. */\r\n\tif(size && no!=bytes/size)\r\n\t\treturn 0;\r\n\tunsigned flags=NEDMALLOC_FORCERESERVE(p, 0, bytes);\r\n\treturn nedpmalloc2(p, bytes, 0, M2_ZERO_MEMORY|flags);\r\n}\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedprealloc(nedpool *p, void *mem, size_t size) THROWSPEC\r\n{\r\n\tunsigned flags=NEDMALLOC_FORCERESERVE(p, mem, size);\r\n#if ENABLE_USERMODEPAGEALLOCATOR\r\n\t/* If the user mode page allocator is turned on in a 32 bit process,\r\n\tdon't automatically reserve eight times the address space. */\r\n\tif(8==sizeof(size_t) || !OSHavePhysicalPageSupport())\r\n#endif\r\n\t{\t/* If he reallocs even once, it's probably wise to turn on address space reservation.\r\n\t\tIf the size is larger than mmap_threshold then it'll set the reserve. */\r\n\t\tif(!(flags & M2_RESERVE_MASK)) flags=M2_RESERVE_MULT(8);\r\n\t}\r\n\treturn nedprealloc2(p, mem, size, 0, flags);\r\n}\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void * nedpmemalign(nedpool *p, size_t alignment, size_t bytes) THROWSPEC\r\n{\r\n\tunsigned flags=NEDMALLOC_FORCERESERVE(p, 0, bytes);\r\n\treturn nedpmalloc2(p, bytes, alignment, flags);\r\n}\r\nNEDMALLOCNOALIASATTR void   nedpfree(nedpool *p, void *mem) THROWSPEC\r\n{\r\n  nedpfree2(p, mem, 0);\r\n}\r\n\r\nstruct nedmallinfo nedpmallinfo(nedpool *p) THROWSPEC\r\n{\r\n\tint n;\r\n\tstruct nedmallinfo ret={0};\r\n\tif(!p) { p=&syspool; if(!syspool.threads) InitPool(&syspool, 0, -1); }\r\n\tfor(n=0; p->m[n]; n++)\r\n\t{\r\n#if USE_ALLOCATOR==1 && !NO_MALLINFO\r\n\t\tstruct mallinfo t=mspace_mallinfo(p->m[n]);\r\n\t\tret.arena+=t.arena;\r\n\t\tret.ordblks+=t.ordblks;\r\n\t\tret.hblkhd+=t.hblkhd;\r\n\t\tret.usmblks+=t.usmblks;\r\n\t\tret.uordblks+=t.uordblks;\r\n\t\tret.fordblks+=t.fordblks;\r\n\t\tret.keepcost+=t.keepcost;\r\n#endif\r\n\t}\r\n\treturn ret;\r\n}\r\nint    nedpmallopt(nedpool *p, int parno, int value) THROWSPEC\r\n{\r\n#if USE_ALLOCATOR==1\r\n\treturn mspace_mallopt(parno, value);\r\n#else\r\n\treturn 0;\r\n#endif\r\n}\r\nNEDMALLOCNOALIASATTR void*  nedmalloc_internals(size_t *granularity, size_t *magic) THROWSPEC\r\n{\r\n#if USE_ALLOCATOR==1\r\n\tif(granularity) *granularity=mparams.granularity;\r\n\tif(magic) *magic=mparams.magic;\r\n\treturn (void *) &syspool;\r\n#else\r\n\tif(granularity) *granularity=0;\r\n\tif(magic) *magic=0;\r\n\treturn 0;\r\n#endif\r\n}\r\nint    nedpmalloc_trim(nedpool *p, size_t pad) THROWSPEC\r\n{\r\n\tint n, ret=0;\r\n\tif(!p) { p=&syspool; if(!syspool.threads) InitPool(&syspool, 0, -1); }\r\n\tfor(n=0; p->m[n]; n++)\r\n\t{\r\n#if USE_ALLOCATOR==1\r\n\t\tret+=mspace_trim(p->m[n], pad);\r\n#endif\r\n\t}\r\n\treturn ret;\r\n}\r\nvoid   nedpmalloc_stats(nedpool *p) THROWSPEC\r\n{\r\n\tint n;\r\n\tif(!p) { p=&syspool; if(!syspool.threads) InitPool(&syspool, 0, -1); }\r\n\tfor(n=0; p->m[n]; n++)\r\n\t{\r\n#if USE_ALLOCATOR==1\r\n\t\tmspace_malloc_stats(p->m[n]);\r\n#endif\r\n\t}\r\n}\r\nsize_t nedpmalloc_footprint(nedpool *p) THROWSPEC\r\n{\r\n\tsize_t ret=0;\r\n\tint n;\r\n\tif(!p) { p=&syspool; if(!syspool.threads) InitPool(&syspool, 0, -1); }\r\n\tfor(n=0; p->m[n]; n++)\r\n\t{\r\n#if USE_ALLOCATOR==1\r\n\t\tret+=mspace_footprint(p->m[n]);\r\n#endif\r\n\t}\r\n\treturn ret;\r\n}\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void **nedpindependent_calloc(nedpool *p, size_t elemsno, size_t elemsize, void **chunks) THROWSPEC\r\n{\r\n\tvoid **ret;\r\n\tthreadcache *tc;\r\n\tint mymspace;\r\n\tGetThreadCache(&p, &tc, &mymspace, &elemsize);\r\n#if USE_ALLOCATOR==0\r\n    GETMSPACE(m, p, tc, mymspace, elemsno*elemsize,\r\n              ret=unsupported_operation(\"independent_calloc\"));\r\n#elif USE_ALLOCATOR==1\r\n    GETMSPACE(m, p, tc, mymspace, elemsno*elemsize,\r\n              ret=mspace_independent_calloc(m, elemsno, elemsize, chunks));\r\n#endif\r\n#if ENABLE_LOGGING\r\n\tif(ret && (ENABLE_LOGGING & LOGENTRY_POOL_MALLOC))\r\n\t{\r\n\t\tsize_t n;\r\n\t\tfor(n=0; n<elemsno; n++)\r\n\t\t{\r\n\t\t\tLogOperation(tc, p, LOGENTRY_POOL_MALLOC, mymspace, elemsize, 0, 0, M2_ZERO_MEMORY, ret[n]);\r\n\t\t}\r\n\t}\r\n#endif\r\n\treturn ret;\r\n}\r\nNEDMALLOCNOALIASATTR NEDMALLOCPTRATTR void **nedpindependent_comalloc(nedpool *p, size_t elems, size_t *sizes, void **chunks) THROWSPEC\r\n{\r\n\tvoid **ret;\r\n\tthreadcache *tc;\r\n\tint mymspace;\r\n    size_t i, *adjustedsizes=(size_t *) alloca(elems*sizeof(size_t));\r\n    if(!adjustedsizes) return 0;\r\n    for(i=0; i<elems; i++)\r\n        adjustedsizes[i]=sizes[i]<sizeof(threadcacheblk) ? sizeof(threadcacheblk) : sizes[i];\r\n\tGetThreadCache(&p, &tc, &mymspace, 0);\r\n#if USE_ALLOCATOR==0\r\n\tGETMSPACE(m, p, tc, mymspace, 0,\r\n              ret=unsupported_operation(\"independent_comalloc\"));\r\n#elif USE_ALLOCATOR==1\r\n\tGETMSPACE(m, p, tc, mymspace, 0,\r\n              ret=mspace_independent_comalloc(m, elems, adjustedsizes, chunks));\r\n#endif\r\n#if ENABLE_LOGGING\r\n\tif(ret && (ENABLE_LOGGING & LOGENTRY_POOL_MALLOC))\r\n\t{\r\n\t\tsize_t n;\r\n\t\tfor(n=0; n<elems; n++)\r\n\t\t{\r\n\t\t\tLogOperation(tc, p, LOGENTRY_POOL_MALLOC, mymspace, sizes[n], 0, 0, 0, ret[n]);\r\n\t\t}\r\n\t}\r\n#endif\r\n\treturn ret;\r\n}\r\n\r\n#if defined(__cplusplus)\r\n}\r\n#endif\r\n\r\n#ifdef _MSC_VER\r\n#pragma warning(pop)\r\n#endif\r\n"], "filenames": ["nedmalloc.c"], "buggy_code_start_loc": [331], "buggy_code_end_loc": [332], "fixing_code_start_loc": [331], "fixing_code_end_loc": [336], "type": "CWE-189", "message": "Multiple integer overflows in the (1) CallMalloc (malloc) and (2) nedpcalloc (calloc) functions in nedmalloc (nedmalloc.c) before 1.10 beta2 make it easier for context-dependent attackers to perform memory-related attacks such as buffer overflows via a large size value, which causes less memory to be allocated than expected.", "other": {"cve": {"id": "CVE-2012-2675", "sourceIdentifier": "secalert@redhat.com", "published": "2012-07-25T19:55:02.867", "lastModified": "2012-07-30T04:00:00.000", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Multiple integer overflows in the (1) CallMalloc (malloc) and (2) nedpcalloc (calloc) functions in nedmalloc (nedmalloc.c) before 1.10 beta2 make it easier for context-dependent attackers to perform memory-related attacks such as buffer overflows via a large size value, which causes less memory to be allocated than expected."}, {"lang": "es", "value": "M\u00faltiples desbordamientos de enteros en el (1) CallMalloc (malloc) y (2) nedpcalloc (calloc) funciones en nedmalloc (nedmalloc.c) anterior a v1.10 beta2 hacen m\u00e1s f\u00e1cil para los atacantes dependientes de contexto para llevar a cabo los ataques relacionados con la memoria tales como desbordamientos de b\u00fafer a trav\u00e9s de un valor de tama\u00f1o grande, lo que provoca menos memoria que se asignar\u00e1n de lo esperado."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:N/I:P/A:N", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "PARTIAL", "availabilityImpact": "NONE", "baseScore": 4.3}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.6, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-189"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:nedprod:nedmalloc:*:beta1:*:*:*:*:*:*", "versionEndIncluding": "1.10", "matchCriteriaId": "30A22E9F-0282-4078-80F5-68C814BFF1C6"}]}]}], "references": [{"url": "http://kqueue.org/blog/2012/03/05/memory-allocator-security-revisited/", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2012/06/05/1", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2012/06/07/13", "source": "secalert@redhat.com"}, {"url": "https://github.com/ned14/nedmalloc/blob/master/Readme.html", "source": "secalert@redhat.com"}, {"url": "https://github.com/ned14/nedmalloc/commit/1a759756639ab7543b650a10c2d77a0ffc7a2000", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch"]}, {"url": "https://github.com/ned14/nedmalloc/commit/2965eca30c408c13473c4146a9d47d547d288db1", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch"]}]}, "github_commit_url": "https://github.com/ned14/nedmalloc/commit/1a759756639ab7543b650a10c2d77a0ffc7a2000"}}