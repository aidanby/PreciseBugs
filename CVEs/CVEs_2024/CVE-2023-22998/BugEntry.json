{"buggy_code": ["/*\n * Copyright (C) 2015 Red Hat, Inc.\n * All Rights Reserved.\n *\n * Permission is hereby granted, free of charge, to any person obtaining\n * a copy of this software and associated documentation files (the\n * \"Software\"), to deal in the Software without restriction, including\n * without limitation the rights to use, copy, modify, merge, publish,\n * distribute, sublicense, and/or sell copies of the Software, and to\n * permit persons to whom the Software is furnished to do so, subject to\n * the following conditions:\n *\n * The above copyright notice and this permission notice (including the\n * next paragraph) shall be included in all copies or substantial\n * portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n * IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE\n * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n */\n\n#include <linux/dma-mapping.h>\n#include <linux/moduleparam.h>\n\n#include \"virtgpu_drv.h\"\n\nstatic int virtio_gpu_virglrenderer_workaround = 1;\nmodule_param_named(virglhack, virtio_gpu_virglrenderer_workaround, int, 0400);\n\nint virtio_gpu_resource_id_get(struct virtio_gpu_device *vgdev, uint32_t *resid)\n{\n\tif (virtio_gpu_virglrenderer_workaround) {\n\t\t/*\n\t\t * Hack to avoid re-using resource IDs.\n\t\t *\n\t\t * virglrenderer versions up to (and including) 0.7.0\n\t\t * can't deal with that.  virglrenderer commit\n\t\t * \"f91a9dd35715 Fix unlinking resources from hash\n\t\t * table.\" (Feb 2019) fixes the bug.\n\t\t */\n\t\tstatic atomic_t seqno = ATOMIC_INIT(0);\n\t\tint handle = atomic_inc_return(&seqno);\n\t\t*resid = handle + 1;\n\t} else {\n\t\tint handle = ida_alloc(&vgdev->resource_ida, GFP_KERNEL);\n\t\tif (handle < 0)\n\t\t\treturn handle;\n\t\t*resid = handle + 1;\n\t}\n\treturn 0;\n}\n\nstatic void virtio_gpu_resource_id_put(struct virtio_gpu_device *vgdev, uint32_t id)\n{\n\tif (!virtio_gpu_virglrenderer_workaround) {\n\t\tida_free(&vgdev->resource_ida, id - 1);\n\t}\n}\n\nvoid virtio_gpu_cleanup_object(struct virtio_gpu_object *bo)\n{\n\tstruct virtio_gpu_device *vgdev = bo->base.base.dev->dev_private;\n\n\tvirtio_gpu_resource_id_put(vgdev, bo->hw_res_handle);\n\tif (virtio_gpu_is_shmem(bo)) {\n\t\tstruct virtio_gpu_object_shmem *shmem = to_virtio_gpu_shmem(bo);\n\n\t\tif (shmem->pages) {\n\t\t\tif (shmem->mapped) {\n\t\t\t\tdma_unmap_sgtable(vgdev->vdev->dev.parent,\n\t\t\t\t\t     shmem->pages, DMA_TO_DEVICE, 0);\n\t\t\t\tshmem->mapped = 0;\n\t\t\t}\n\n\t\t\tsg_free_table(shmem->pages);\n\t\t\tkfree(shmem->pages);\n\t\t\tshmem->pages = NULL;\n\t\t\tdrm_gem_shmem_unpin(&bo->base);\n\t\t}\n\n\t\tdrm_gem_shmem_free(&bo->base);\n\t} else if (virtio_gpu_is_vram(bo)) {\n\t\tstruct virtio_gpu_object_vram *vram = to_virtio_gpu_vram(bo);\n\n\t\tspin_lock(&vgdev->host_visible_lock);\n\t\tif (drm_mm_node_allocated(&vram->vram_node))\n\t\t\tdrm_mm_remove_node(&vram->vram_node);\n\n\t\tspin_unlock(&vgdev->host_visible_lock);\n\n\t\tdrm_gem_free_mmap_offset(&vram->base.base.base);\n\t\tdrm_gem_object_release(&vram->base.base.base);\n\t\tkfree(vram);\n\t}\n}\n\nstatic void virtio_gpu_free_object(struct drm_gem_object *obj)\n{\n\tstruct virtio_gpu_object *bo = gem_to_virtio_gpu_obj(obj);\n\tstruct virtio_gpu_device *vgdev = bo->base.base.dev->dev_private;\n\n\tif (bo->created) {\n\t\tvirtio_gpu_cmd_unref_resource(vgdev, bo);\n\t\tvirtio_gpu_notify(vgdev);\n\t\t/* completion handler calls virtio_gpu_cleanup_object() */\n\t\treturn;\n\t}\n\tvirtio_gpu_cleanup_object(bo);\n}\n\nstatic const struct drm_gem_object_funcs virtio_gpu_shmem_funcs = {\n\t.free = virtio_gpu_free_object,\n\t.open = virtio_gpu_gem_object_open,\n\t.close = virtio_gpu_gem_object_close,\n\t.print_info = drm_gem_shmem_object_print_info,\n\t.export = virtgpu_gem_prime_export,\n\t.pin = drm_gem_shmem_object_pin,\n\t.unpin = drm_gem_shmem_object_unpin,\n\t.get_sg_table = drm_gem_shmem_object_get_sg_table,\n\t.vmap = drm_gem_shmem_object_vmap,\n\t.vunmap = drm_gem_shmem_object_vunmap,\n\t.mmap = drm_gem_shmem_object_mmap,\n\t.vm_ops = &drm_gem_shmem_vm_ops,\n};\n\nbool virtio_gpu_is_shmem(struct virtio_gpu_object *bo)\n{\n\treturn bo->base.base.funcs == &virtio_gpu_shmem_funcs;\n}\n\nstruct drm_gem_object *virtio_gpu_create_object(struct drm_device *dev,\n\t\t\t\t\t\tsize_t size)\n{\n\tstruct virtio_gpu_object_shmem *shmem;\n\tstruct drm_gem_shmem_object *dshmem;\n\n\tshmem = kzalloc(sizeof(*shmem), GFP_KERNEL);\n\tif (!shmem)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tdshmem = &shmem->base.base;\n\tdshmem->base.funcs = &virtio_gpu_shmem_funcs;\n\treturn &dshmem->base;\n}\n\nstatic int virtio_gpu_object_shmem_init(struct virtio_gpu_device *vgdev,\n\t\t\t\t\tstruct virtio_gpu_object *bo,\n\t\t\t\t\tstruct virtio_gpu_mem_entry **ents,\n\t\t\t\t\tunsigned int *nents)\n{\n\tbool use_dma_api = !virtio_has_dma_quirk(vgdev->vdev);\n\tstruct virtio_gpu_object_shmem *shmem = to_virtio_gpu_shmem(bo);\n\tstruct scatterlist *sg;\n\tint si, ret;\n\n\tret = drm_gem_shmem_pin(&bo->base);\n\tif (ret < 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * virtio_gpu uses drm_gem_shmem_get_sg_table instead of\n\t * drm_gem_shmem_get_pages_sgt because virtio has it's own set of\n\t * dma-ops. This is discouraged for other drivers, but should be fine\n\t * since virtio_gpu doesn't support dma-buf import from other devices.\n\t */\n\tshmem->pages = drm_gem_shmem_get_sg_table(&bo->base);\n\tif (!shmem->pages) {\n\t\tdrm_gem_shmem_unpin(&bo->base);\n\t\treturn -EINVAL;\n\t}\n\n\tif (use_dma_api) {\n\t\tret = dma_map_sgtable(vgdev->vdev->dev.parent,\n\t\t\t\t      shmem->pages, DMA_TO_DEVICE, 0);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\t*nents = shmem->mapped = shmem->pages->nents;\n\t} else {\n\t\t*nents = shmem->pages->orig_nents;\n\t}\n\n\t*ents = kvmalloc_array(*nents,\n\t\t\t       sizeof(struct virtio_gpu_mem_entry),\n\t\t\t       GFP_KERNEL);\n\tif (!(*ents)) {\n\t\tDRM_ERROR(\"failed to allocate ent list\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (use_dma_api) {\n\t\tfor_each_sgtable_dma_sg(shmem->pages, sg, si) {\n\t\t\t(*ents)[si].addr = cpu_to_le64(sg_dma_address(sg));\n\t\t\t(*ents)[si].length = cpu_to_le32(sg_dma_len(sg));\n\t\t\t(*ents)[si].padding = 0;\n\t\t}\n\t} else {\n\t\tfor_each_sgtable_sg(shmem->pages, sg, si) {\n\t\t\t(*ents)[si].addr = cpu_to_le64(sg_phys(sg));\n\t\t\t(*ents)[si].length = cpu_to_le32(sg->length);\n\t\t\t(*ents)[si].padding = 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint virtio_gpu_object_create(struct virtio_gpu_device *vgdev,\n\t\t\t     struct virtio_gpu_object_params *params,\n\t\t\t     struct virtio_gpu_object **bo_ptr,\n\t\t\t     struct virtio_gpu_fence *fence)\n{\n\tstruct virtio_gpu_object_array *objs = NULL;\n\tstruct drm_gem_shmem_object *shmem_obj;\n\tstruct virtio_gpu_object *bo;\n\tstruct virtio_gpu_mem_entry *ents;\n\tunsigned int nents;\n\tint ret;\n\n\t*bo_ptr = NULL;\n\n\tparams->size = roundup(params->size, PAGE_SIZE);\n\tshmem_obj = drm_gem_shmem_create(vgdev->ddev, params->size);\n\tif (IS_ERR(shmem_obj))\n\t\treturn PTR_ERR(shmem_obj);\n\tbo = gem_to_virtio_gpu_obj(&shmem_obj->base);\n\n\tret = virtio_gpu_resource_id_get(vgdev, &bo->hw_res_handle);\n\tif (ret < 0)\n\t\tgoto err_free_gem;\n\n\tbo->dumb = params->dumb;\n\n\tif (fence) {\n\t\tret = -ENOMEM;\n\t\tobjs = virtio_gpu_array_alloc(1);\n\t\tif (!objs)\n\t\t\tgoto err_put_id;\n\t\tvirtio_gpu_array_add_obj(objs, &bo->base.base);\n\n\t\tret = virtio_gpu_array_lock_resv(objs);\n\t\tif (ret != 0)\n\t\t\tgoto err_put_objs;\n\t}\n\n\tret = virtio_gpu_object_shmem_init(vgdev, bo, &ents, &nents);\n\tif (ret != 0) {\n\t\tvirtio_gpu_array_put_free(objs);\n\t\tvirtio_gpu_free_object(&shmem_obj->base);\n\t\treturn ret;\n\t}\n\n\tif (params->blob) {\n\t\tif (params->blob_mem == VIRTGPU_BLOB_MEM_GUEST)\n\t\t\tbo->guest_blob = true;\n\n\t\tvirtio_gpu_cmd_resource_create_blob(vgdev, bo, params,\n\t\t\t\t\t\t    ents, nents);\n\t} else if (params->virgl) {\n\t\tvirtio_gpu_cmd_resource_create_3d(vgdev, bo, params,\n\t\t\t\t\t\t  objs, fence);\n\t\tvirtio_gpu_object_attach(vgdev, bo, ents, nents);\n\t} else {\n\t\tvirtio_gpu_cmd_create_resource(vgdev, bo, params,\n\t\t\t\t\t       objs, fence);\n\t\tvirtio_gpu_object_attach(vgdev, bo, ents, nents);\n\t}\n\n\t*bo_ptr = bo;\n\treturn 0;\n\nerr_put_objs:\n\tvirtio_gpu_array_put_free(objs);\nerr_put_id:\n\tvirtio_gpu_resource_id_put(vgdev, bo->hw_res_handle);\nerr_free_gem:\n\tdrm_gem_shmem_free(shmem_obj);\n\treturn ret;\n}\n"], "fixing_code": ["/*\n * Copyright (C) 2015 Red Hat, Inc.\n * All Rights Reserved.\n *\n * Permission is hereby granted, free of charge, to any person obtaining\n * a copy of this software and associated documentation files (the\n * \"Software\"), to deal in the Software without restriction, including\n * without limitation the rights to use, copy, modify, merge, publish,\n * distribute, sublicense, and/or sell copies of the Software, and to\n * permit persons to whom the Software is furnished to do so, subject to\n * the following conditions:\n *\n * The above copyright notice and this permission notice (including the\n * next paragraph) shall be included in all copies or substantial\n * portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n * IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE\n * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n */\n\n#include <linux/dma-mapping.h>\n#include <linux/moduleparam.h>\n\n#include \"virtgpu_drv.h\"\n\nstatic int virtio_gpu_virglrenderer_workaround = 1;\nmodule_param_named(virglhack, virtio_gpu_virglrenderer_workaround, int, 0400);\n\nint virtio_gpu_resource_id_get(struct virtio_gpu_device *vgdev, uint32_t *resid)\n{\n\tif (virtio_gpu_virglrenderer_workaround) {\n\t\t/*\n\t\t * Hack to avoid re-using resource IDs.\n\t\t *\n\t\t * virglrenderer versions up to (and including) 0.7.0\n\t\t * can't deal with that.  virglrenderer commit\n\t\t * \"f91a9dd35715 Fix unlinking resources from hash\n\t\t * table.\" (Feb 2019) fixes the bug.\n\t\t */\n\t\tstatic atomic_t seqno = ATOMIC_INIT(0);\n\t\tint handle = atomic_inc_return(&seqno);\n\t\t*resid = handle + 1;\n\t} else {\n\t\tint handle = ida_alloc(&vgdev->resource_ida, GFP_KERNEL);\n\t\tif (handle < 0)\n\t\t\treturn handle;\n\t\t*resid = handle + 1;\n\t}\n\treturn 0;\n}\n\nstatic void virtio_gpu_resource_id_put(struct virtio_gpu_device *vgdev, uint32_t id)\n{\n\tif (!virtio_gpu_virglrenderer_workaround) {\n\t\tida_free(&vgdev->resource_ida, id - 1);\n\t}\n}\n\nvoid virtio_gpu_cleanup_object(struct virtio_gpu_object *bo)\n{\n\tstruct virtio_gpu_device *vgdev = bo->base.base.dev->dev_private;\n\n\tvirtio_gpu_resource_id_put(vgdev, bo->hw_res_handle);\n\tif (virtio_gpu_is_shmem(bo)) {\n\t\tstruct virtio_gpu_object_shmem *shmem = to_virtio_gpu_shmem(bo);\n\n\t\tif (shmem->pages) {\n\t\t\tif (shmem->mapped) {\n\t\t\t\tdma_unmap_sgtable(vgdev->vdev->dev.parent,\n\t\t\t\t\t     shmem->pages, DMA_TO_DEVICE, 0);\n\t\t\t\tshmem->mapped = 0;\n\t\t\t}\n\n\t\t\tsg_free_table(shmem->pages);\n\t\t\tkfree(shmem->pages);\n\t\t\tshmem->pages = NULL;\n\t\t\tdrm_gem_shmem_unpin(&bo->base);\n\t\t}\n\n\t\tdrm_gem_shmem_free(&bo->base);\n\t} else if (virtio_gpu_is_vram(bo)) {\n\t\tstruct virtio_gpu_object_vram *vram = to_virtio_gpu_vram(bo);\n\n\t\tspin_lock(&vgdev->host_visible_lock);\n\t\tif (drm_mm_node_allocated(&vram->vram_node))\n\t\t\tdrm_mm_remove_node(&vram->vram_node);\n\n\t\tspin_unlock(&vgdev->host_visible_lock);\n\n\t\tdrm_gem_free_mmap_offset(&vram->base.base.base);\n\t\tdrm_gem_object_release(&vram->base.base.base);\n\t\tkfree(vram);\n\t}\n}\n\nstatic void virtio_gpu_free_object(struct drm_gem_object *obj)\n{\n\tstruct virtio_gpu_object *bo = gem_to_virtio_gpu_obj(obj);\n\tstruct virtio_gpu_device *vgdev = bo->base.base.dev->dev_private;\n\n\tif (bo->created) {\n\t\tvirtio_gpu_cmd_unref_resource(vgdev, bo);\n\t\tvirtio_gpu_notify(vgdev);\n\t\t/* completion handler calls virtio_gpu_cleanup_object() */\n\t\treturn;\n\t}\n\tvirtio_gpu_cleanup_object(bo);\n}\n\nstatic const struct drm_gem_object_funcs virtio_gpu_shmem_funcs = {\n\t.free = virtio_gpu_free_object,\n\t.open = virtio_gpu_gem_object_open,\n\t.close = virtio_gpu_gem_object_close,\n\t.print_info = drm_gem_shmem_object_print_info,\n\t.export = virtgpu_gem_prime_export,\n\t.pin = drm_gem_shmem_object_pin,\n\t.unpin = drm_gem_shmem_object_unpin,\n\t.get_sg_table = drm_gem_shmem_object_get_sg_table,\n\t.vmap = drm_gem_shmem_object_vmap,\n\t.vunmap = drm_gem_shmem_object_vunmap,\n\t.mmap = drm_gem_shmem_object_mmap,\n\t.vm_ops = &drm_gem_shmem_vm_ops,\n};\n\nbool virtio_gpu_is_shmem(struct virtio_gpu_object *bo)\n{\n\treturn bo->base.base.funcs == &virtio_gpu_shmem_funcs;\n}\n\nstruct drm_gem_object *virtio_gpu_create_object(struct drm_device *dev,\n\t\t\t\t\t\tsize_t size)\n{\n\tstruct virtio_gpu_object_shmem *shmem;\n\tstruct drm_gem_shmem_object *dshmem;\n\n\tshmem = kzalloc(sizeof(*shmem), GFP_KERNEL);\n\tif (!shmem)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tdshmem = &shmem->base.base;\n\tdshmem->base.funcs = &virtio_gpu_shmem_funcs;\n\treturn &dshmem->base;\n}\n\nstatic int virtio_gpu_object_shmem_init(struct virtio_gpu_device *vgdev,\n\t\t\t\t\tstruct virtio_gpu_object *bo,\n\t\t\t\t\tstruct virtio_gpu_mem_entry **ents,\n\t\t\t\t\tunsigned int *nents)\n{\n\tbool use_dma_api = !virtio_has_dma_quirk(vgdev->vdev);\n\tstruct virtio_gpu_object_shmem *shmem = to_virtio_gpu_shmem(bo);\n\tstruct scatterlist *sg;\n\tint si, ret;\n\n\tret = drm_gem_shmem_pin(&bo->base);\n\tif (ret < 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * virtio_gpu uses drm_gem_shmem_get_sg_table instead of\n\t * drm_gem_shmem_get_pages_sgt because virtio has it's own set of\n\t * dma-ops. This is discouraged for other drivers, but should be fine\n\t * since virtio_gpu doesn't support dma-buf import from other devices.\n\t */\n\tshmem->pages = drm_gem_shmem_get_sg_table(&bo->base);\n\tif (IS_ERR(shmem->pages)) {\n\t\tdrm_gem_shmem_unpin(&bo->base);\n\t\treturn PTR_ERR(shmem->pages);\n\t}\n\n\tif (use_dma_api) {\n\t\tret = dma_map_sgtable(vgdev->vdev->dev.parent,\n\t\t\t\t      shmem->pages, DMA_TO_DEVICE, 0);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\t*nents = shmem->mapped = shmem->pages->nents;\n\t} else {\n\t\t*nents = shmem->pages->orig_nents;\n\t}\n\n\t*ents = kvmalloc_array(*nents,\n\t\t\t       sizeof(struct virtio_gpu_mem_entry),\n\t\t\t       GFP_KERNEL);\n\tif (!(*ents)) {\n\t\tDRM_ERROR(\"failed to allocate ent list\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (use_dma_api) {\n\t\tfor_each_sgtable_dma_sg(shmem->pages, sg, si) {\n\t\t\t(*ents)[si].addr = cpu_to_le64(sg_dma_address(sg));\n\t\t\t(*ents)[si].length = cpu_to_le32(sg_dma_len(sg));\n\t\t\t(*ents)[si].padding = 0;\n\t\t}\n\t} else {\n\t\tfor_each_sgtable_sg(shmem->pages, sg, si) {\n\t\t\t(*ents)[si].addr = cpu_to_le64(sg_phys(sg));\n\t\t\t(*ents)[si].length = cpu_to_le32(sg->length);\n\t\t\t(*ents)[si].padding = 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint virtio_gpu_object_create(struct virtio_gpu_device *vgdev,\n\t\t\t     struct virtio_gpu_object_params *params,\n\t\t\t     struct virtio_gpu_object **bo_ptr,\n\t\t\t     struct virtio_gpu_fence *fence)\n{\n\tstruct virtio_gpu_object_array *objs = NULL;\n\tstruct drm_gem_shmem_object *shmem_obj;\n\tstruct virtio_gpu_object *bo;\n\tstruct virtio_gpu_mem_entry *ents;\n\tunsigned int nents;\n\tint ret;\n\n\t*bo_ptr = NULL;\n\n\tparams->size = roundup(params->size, PAGE_SIZE);\n\tshmem_obj = drm_gem_shmem_create(vgdev->ddev, params->size);\n\tif (IS_ERR(shmem_obj))\n\t\treturn PTR_ERR(shmem_obj);\n\tbo = gem_to_virtio_gpu_obj(&shmem_obj->base);\n\n\tret = virtio_gpu_resource_id_get(vgdev, &bo->hw_res_handle);\n\tif (ret < 0)\n\t\tgoto err_free_gem;\n\n\tbo->dumb = params->dumb;\n\n\tif (fence) {\n\t\tret = -ENOMEM;\n\t\tobjs = virtio_gpu_array_alloc(1);\n\t\tif (!objs)\n\t\t\tgoto err_put_id;\n\t\tvirtio_gpu_array_add_obj(objs, &bo->base.base);\n\n\t\tret = virtio_gpu_array_lock_resv(objs);\n\t\tif (ret != 0)\n\t\t\tgoto err_put_objs;\n\t}\n\n\tret = virtio_gpu_object_shmem_init(vgdev, bo, &ents, &nents);\n\tif (ret != 0) {\n\t\tvirtio_gpu_array_put_free(objs);\n\t\tvirtio_gpu_free_object(&shmem_obj->base);\n\t\treturn ret;\n\t}\n\n\tif (params->blob) {\n\t\tif (params->blob_mem == VIRTGPU_BLOB_MEM_GUEST)\n\t\t\tbo->guest_blob = true;\n\n\t\tvirtio_gpu_cmd_resource_create_blob(vgdev, bo, params,\n\t\t\t\t\t\t    ents, nents);\n\t} else if (params->virgl) {\n\t\tvirtio_gpu_cmd_resource_create_3d(vgdev, bo, params,\n\t\t\t\t\t\t  objs, fence);\n\t\tvirtio_gpu_object_attach(vgdev, bo, ents, nents);\n\t} else {\n\t\tvirtio_gpu_cmd_create_resource(vgdev, bo, params,\n\t\t\t\t\t       objs, fence);\n\t\tvirtio_gpu_object_attach(vgdev, bo, ents, nents);\n\t}\n\n\t*bo_ptr = bo;\n\treturn 0;\n\nerr_put_objs:\n\tvirtio_gpu_array_put_free(objs);\nerr_put_id:\n\tvirtio_gpu_resource_id_put(vgdev, bo->hw_res_handle);\nerr_free_gem:\n\tdrm_gem_shmem_free(shmem_obj);\n\treturn ret;\n}\n"], "filenames": ["drivers/gpu/drm/virtio/virtgpu_object.c"], "buggy_code_start_loc": [171], "buggy_code_end_loc": [174], "fixing_code_start_loc": [171], "fixing_code_end_loc": [174], "type": "CWE-436", "message": "In the Linux kernel before 6.0.3, drivers/gpu/drm/virtio/virtgpu_object.c misinterprets the drm_gem_shmem_get_sg_table return value (expects it to be NULL in the error case, whereas it is actually an error pointer).", "other": {"cve": {"id": "CVE-2023-22998", "sourceIdentifier": "cve@mitre.org", "published": "2023-02-28T21:15:12.180", "lastModified": "2023-05-03T01:15:13.093", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "In the Linux kernel before 6.0.3, drivers/gpu/drm/virtio/virtgpu_object.c misinterprets the drm_gem_shmem_get_sg_table return value (expects it to be NULL in the error case, whereas it is actually an error pointer)."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-436"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "6.0.3", "matchCriteriaId": "D609BC07-3DEC-40DC-8C58-47EA751E5D46"}]}]}], "references": [{"url": "https://cdn.kernel.org/pub/linux/kernel/v6.x/ChangeLog-6.0.3", "source": "cve@mitre.org", "tags": ["Release Notes"]}, {"url": "https://github.com/torvalds/linux/commit/c24968734abfed81c8f93dc5f44a7b7a9aecadfa", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "https://lists.debian.org/debian-lts-announce/2023/05/msg00005.html", "source": "cve@mitre.org"}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/c24968734abfed81c8f93dc5f44a7b7a9aecadfa"}}