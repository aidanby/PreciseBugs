{"buggy_code": ["/*\n *  linux/fs/namespace.c\n *\n * (C) Copyright Al Viro 2000, 2001\n *\tReleased under GPL v2.\n *\n * Based on code from fs/super.c, copyright Linus Torvalds and others.\n * Heavily rewritten.\n */\n\n#include <linux/syscalls.h>\n#include <linux/export.h>\n#include <linux/capability.h>\n#include <linux/mnt_namespace.h>\n#include <linux/user_namespace.h>\n#include <linux/namei.h>\n#include <linux/security.h>\n#include <linux/idr.h>\n#include <linux/acct.h>\t\t/* acct_auto_close_mnt */\n#include <linux/init.h>\t\t/* init_rootfs */\n#include <linux/fs_struct.h>\t/* get_fs_root et.al. */\n#include <linux/fsnotify.h>\t/* fsnotify_vfsmount_delete */\n#include <linux/uaccess.h>\n#include <linux/proc_ns.h>\n#include <linux/magic.h>\n#include <linux/bootmem.h>\n#include \"pnode.h\"\n#include \"internal.h\"\n\nstatic unsigned int m_hash_mask __read_mostly;\nstatic unsigned int m_hash_shift __read_mostly;\nstatic unsigned int mp_hash_mask __read_mostly;\nstatic unsigned int mp_hash_shift __read_mostly;\n\nstatic __initdata unsigned long mhash_entries;\nstatic int __init set_mhash_entries(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\tmhash_entries = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"mhash_entries=\", set_mhash_entries);\n\nstatic __initdata unsigned long mphash_entries;\nstatic int __init set_mphash_entries(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\tmphash_entries = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"mphash_entries=\", set_mphash_entries);\n\nstatic u64 event;\nstatic DEFINE_IDA(mnt_id_ida);\nstatic DEFINE_IDA(mnt_group_ida);\nstatic DEFINE_SPINLOCK(mnt_id_lock);\nstatic int mnt_id_start = 0;\nstatic int mnt_group_start = 1;\n\nstatic struct hlist_head *mount_hashtable __read_mostly;\nstatic struct hlist_head *mountpoint_hashtable __read_mostly;\nstatic struct kmem_cache *mnt_cache __read_mostly;\nstatic DECLARE_RWSEM(namespace_sem);\n\n/* /sys/fs */\nstruct kobject *fs_kobj;\nEXPORT_SYMBOL_GPL(fs_kobj);\n\n/*\n * vfsmount lock may be taken for read to prevent changes to the\n * vfsmount hash, ie. during mountpoint lookups or walking back\n * up the tree.\n *\n * It should be taken for write in all cases where the vfsmount\n * tree or hash is modified or when a vfsmount structure is modified.\n */\n__cacheline_aligned_in_smp DEFINE_SEQLOCK(mount_lock);\n\nstatic inline struct hlist_head *m_hash(struct vfsmount *mnt, struct dentry *dentry)\n{\n\tunsigned long tmp = ((unsigned long)mnt / L1_CACHE_BYTES);\n\ttmp += ((unsigned long)dentry / L1_CACHE_BYTES);\n\ttmp = tmp + (tmp >> m_hash_shift);\n\treturn &mount_hashtable[tmp & m_hash_mask];\n}\n\nstatic inline struct hlist_head *mp_hash(struct dentry *dentry)\n{\n\tunsigned long tmp = ((unsigned long)dentry / L1_CACHE_BYTES);\n\ttmp = tmp + (tmp >> mp_hash_shift);\n\treturn &mountpoint_hashtable[tmp & mp_hash_mask];\n}\n\n/*\n * allocation is serialized by namespace_sem, but we need the spinlock to\n * serialize with freeing.\n */\nstatic int mnt_alloc_id(struct mount *mnt)\n{\n\tint res;\n\nretry:\n\tida_pre_get(&mnt_id_ida, GFP_KERNEL);\n\tspin_lock(&mnt_id_lock);\n\tres = ida_get_new_above(&mnt_id_ida, mnt_id_start, &mnt->mnt_id);\n\tif (!res)\n\t\tmnt_id_start = mnt->mnt_id + 1;\n\tspin_unlock(&mnt_id_lock);\n\tif (res == -EAGAIN)\n\t\tgoto retry;\n\n\treturn res;\n}\n\nstatic void mnt_free_id(struct mount *mnt)\n{\n\tint id = mnt->mnt_id;\n\tspin_lock(&mnt_id_lock);\n\tida_remove(&mnt_id_ida, id);\n\tif (mnt_id_start > id)\n\t\tmnt_id_start = id;\n\tspin_unlock(&mnt_id_lock);\n}\n\n/*\n * Allocate a new peer group ID\n *\n * mnt_group_ida is protected by namespace_sem\n */\nstatic int mnt_alloc_group_id(struct mount *mnt)\n{\n\tint res;\n\n\tif (!ida_pre_get(&mnt_group_ida, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tres = ida_get_new_above(&mnt_group_ida,\n\t\t\t\tmnt_group_start,\n\t\t\t\t&mnt->mnt_group_id);\n\tif (!res)\n\t\tmnt_group_start = mnt->mnt_group_id + 1;\n\n\treturn res;\n}\n\n/*\n * Release a peer group ID\n */\nvoid mnt_release_group_id(struct mount *mnt)\n{\n\tint id = mnt->mnt_group_id;\n\tida_remove(&mnt_group_ida, id);\n\tif (mnt_group_start > id)\n\t\tmnt_group_start = id;\n\tmnt->mnt_group_id = 0;\n}\n\n/*\n * vfsmount lock must be held for read\n */\nstatic inline void mnt_add_count(struct mount *mnt, int n)\n{\n#ifdef CONFIG_SMP\n\tthis_cpu_add(mnt->mnt_pcp->mnt_count, n);\n#else\n\tpreempt_disable();\n\tmnt->mnt_count += n;\n\tpreempt_enable();\n#endif\n}\n\n/*\n * vfsmount lock must be held for write\n */\nunsigned int mnt_get_count(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tunsigned int count = 0;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcount += per_cpu_ptr(mnt->mnt_pcp, cpu)->mnt_count;\n\t}\n\n\treturn count;\n#else\n\treturn mnt->mnt_count;\n#endif\n}\n\nstatic struct mount *alloc_vfsmnt(const char *name)\n{\n\tstruct mount *mnt = kmem_cache_zalloc(mnt_cache, GFP_KERNEL);\n\tif (mnt) {\n\t\tint err;\n\n\t\terr = mnt_alloc_id(mnt);\n\t\tif (err)\n\t\t\tgoto out_free_cache;\n\n\t\tif (name) {\n\t\t\tmnt->mnt_devname = kstrdup(name, GFP_KERNEL);\n\t\t\tif (!mnt->mnt_devname)\n\t\t\t\tgoto out_free_id;\n\t\t}\n\n#ifdef CONFIG_SMP\n\t\tmnt->mnt_pcp = alloc_percpu(struct mnt_pcp);\n\t\tif (!mnt->mnt_pcp)\n\t\t\tgoto out_free_devname;\n\n\t\tthis_cpu_add(mnt->mnt_pcp->mnt_count, 1);\n#else\n\t\tmnt->mnt_count = 1;\n\t\tmnt->mnt_writers = 0;\n#endif\n\n\t\tINIT_HLIST_NODE(&mnt->mnt_hash);\n\t\tINIT_LIST_HEAD(&mnt->mnt_child);\n\t\tINIT_LIST_HEAD(&mnt->mnt_mounts);\n\t\tINIT_LIST_HEAD(&mnt->mnt_list);\n\t\tINIT_LIST_HEAD(&mnt->mnt_expire);\n\t\tINIT_LIST_HEAD(&mnt->mnt_share);\n\t\tINIT_LIST_HEAD(&mnt->mnt_slave_list);\n\t\tINIT_LIST_HEAD(&mnt->mnt_slave);\n#ifdef CONFIG_FSNOTIFY\n\t\tINIT_HLIST_HEAD(&mnt->mnt_fsnotify_marks);\n#endif\n\t}\n\treturn mnt;\n\n#ifdef CONFIG_SMP\nout_free_devname:\n\tkfree(mnt->mnt_devname);\n#endif\nout_free_id:\n\tmnt_free_id(mnt);\nout_free_cache:\n\tkmem_cache_free(mnt_cache, mnt);\n\treturn NULL;\n}\n\n/*\n * Most r/o checks on a fs are for operations that take\n * discrete amounts of time, like a write() or unlink().\n * We must keep track of when those operations start\n * (for permission checks) and when they end, so that\n * we can determine when writes are able to occur to\n * a filesystem.\n */\n/*\n * __mnt_is_readonly: check whether a mount is read-only\n * @mnt: the mount to check for its write status\n *\n * This shouldn't be used directly ouside of the VFS.\n * It does not guarantee that the filesystem will stay\n * r/w, just that it is right *now*.  This can not and\n * should not be used in place of IS_RDONLY(inode).\n * mnt_want/drop_write() will _keep_ the filesystem\n * r/w.\n */\nint __mnt_is_readonly(struct vfsmount *mnt)\n{\n\tif (mnt->mnt_flags & MNT_READONLY)\n\t\treturn 1;\n\tif (mnt->mnt_sb->s_flags & MS_RDONLY)\n\t\treturn 1;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__mnt_is_readonly);\n\nstatic inline void mnt_inc_writers(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tthis_cpu_inc(mnt->mnt_pcp->mnt_writers);\n#else\n\tmnt->mnt_writers++;\n#endif\n}\n\nstatic inline void mnt_dec_writers(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tthis_cpu_dec(mnt->mnt_pcp->mnt_writers);\n#else\n\tmnt->mnt_writers--;\n#endif\n}\n\nstatic unsigned int mnt_get_writers(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tunsigned int count = 0;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcount += per_cpu_ptr(mnt->mnt_pcp, cpu)->mnt_writers;\n\t}\n\n\treturn count;\n#else\n\treturn mnt->mnt_writers;\n#endif\n}\n\nstatic int mnt_is_readonly(struct vfsmount *mnt)\n{\n\tif (mnt->mnt_sb->s_readonly_remount)\n\t\treturn 1;\n\t/* Order wrt setting s_flags/s_readonly_remount in do_remount() */\n\tsmp_rmb();\n\treturn __mnt_is_readonly(mnt);\n}\n\n/*\n * Most r/o & frozen checks on a fs are for operations that take discrete\n * amounts of time, like a write() or unlink().  We must keep track of when\n * those operations start (for permission checks) and when they end, so that we\n * can determine when writes are able to occur to a filesystem.\n */\n/**\n * __mnt_want_write - get write access to a mount without freeze protection\n * @m: the mount on which to take a write\n *\n * This tells the low-level filesystem that a write is about to be performed to\n * it, and makes sure that writes are allowed (mnt it read-write) before\n * returning success. This operation does not protect against filesystem being\n * frozen. When the write operation is finished, __mnt_drop_write() must be\n * called. This is effectively a refcount.\n */\nint __mnt_want_write(struct vfsmount *m)\n{\n\tstruct mount *mnt = real_mount(m);\n\tint ret = 0;\n\n\tpreempt_disable();\n\tmnt_inc_writers(mnt);\n\t/*\n\t * The store to mnt_inc_writers must be visible before we pass\n\t * MNT_WRITE_HOLD loop below, so that the slowpath can see our\n\t * incremented count after it has set MNT_WRITE_HOLD.\n\t */\n\tsmp_mb();\n\twhile (ACCESS_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD)\n\t\tcpu_relax();\n\t/*\n\t * After the slowpath clears MNT_WRITE_HOLD, mnt_is_readonly will\n\t * be set to match its requirements. So we must not load that until\n\t * MNT_WRITE_HOLD is cleared.\n\t */\n\tsmp_rmb();\n\tif (mnt_is_readonly(m)) {\n\t\tmnt_dec_writers(mnt);\n\t\tret = -EROFS;\n\t}\n\tpreempt_enable();\n\n\treturn ret;\n}\n\n/**\n * mnt_want_write - get write access to a mount\n * @m: the mount on which to take a write\n *\n * This tells the low-level filesystem that a write is about to be performed to\n * it, and makes sure that writes are allowed (mount is read-write, filesystem\n * is not frozen) before returning success.  When the write operation is\n * finished, mnt_drop_write() must be called.  This is effectively a refcount.\n */\nint mnt_want_write(struct vfsmount *m)\n{\n\tint ret;\n\n\tsb_start_write(m->mnt_sb);\n\tret = __mnt_want_write(m);\n\tif (ret)\n\t\tsb_end_write(m->mnt_sb);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mnt_want_write);\n\n/**\n * mnt_clone_write - get write access to a mount\n * @mnt: the mount on which to take a write\n *\n * This is effectively like mnt_want_write, except\n * it must only be used to take an extra write reference\n * on a mountpoint that we already know has a write reference\n * on it. This allows some optimisation.\n *\n * After finished, mnt_drop_write must be called as usual to\n * drop the reference.\n */\nint mnt_clone_write(struct vfsmount *mnt)\n{\n\t/* superblock may be r/o */\n\tif (__mnt_is_readonly(mnt))\n\t\treturn -EROFS;\n\tpreempt_disable();\n\tmnt_inc_writers(real_mount(mnt));\n\tpreempt_enable();\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(mnt_clone_write);\n\n/**\n * __mnt_want_write_file - get write access to a file's mount\n * @file: the file who's mount on which to take a write\n *\n * This is like __mnt_want_write, but it takes a file and can\n * do some optimisations if the file is open for write already\n */\nint __mnt_want_write_file(struct file *file)\n{\n\tif (!(file->f_mode & FMODE_WRITER))\n\t\treturn __mnt_want_write(file->f_path.mnt);\n\telse\n\t\treturn mnt_clone_write(file->f_path.mnt);\n}\n\n/**\n * mnt_want_write_file - get write access to a file's mount\n * @file: the file who's mount on which to take a write\n *\n * This is like mnt_want_write, but it takes a file and can\n * do some optimisations if the file is open for write already\n */\nint mnt_want_write_file(struct file *file)\n{\n\tint ret;\n\n\tsb_start_write(file->f_path.mnt->mnt_sb);\n\tret = __mnt_want_write_file(file);\n\tif (ret)\n\t\tsb_end_write(file->f_path.mnt->mnt_sb);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mnt_want_write_file);\n\n/**\n * __mnt_drop_write - give up write access to a mount\n * @mnt: the mount on which to give up write access\n *\n * Tells the low-level filesystem that we are done\n * performing writes to it.  Must be matched with\n * __mnt_want_write() call above.\n */\nvoid __mnt_drop_write(struct vfsmount *mnt)\n{\n\tpreempt_disable();\n\tmnt_dec_writers(real_mount(mnt));\n\tpreempt_enable();\n}\n\n/**\n * mnt_drop_write - give up write access to a mount\n * @mnt: the mount on which to give up write access\n *\n * Tells the low-level filesystem that we are done performing writes to it and\n * also allows filesystem to be frozen again.  Must be matched with\n * mnt_want_write() call above.\n */\nvoid mnt_drop_write(struct vfsmount *mnt)\n{\n\t__mnt_drop_write(mnt);\n\tsb_end_write(mnt->mnt_sb);\n}\nEXPORT_SYMBOL_GPL(mnt_drop_write);\n\nvoid __mnt_drop_write_file(struct file *file)\n{\n\t__mnt_drop_write(file->f_path.mnt);\n}\n\nvoid mnt_drop_write_file(struct file *file)\n{\n\tmnt_drop_write(file->f_path.mnt);\n}\nEXPORT_SYMBOL(mnt_drop_write_file);\n\nstatic int mnt_make_readonly(struct mount *mnt)\n{\n\tint ret = 0;\n\n\tlock_mount_hash();\n\tmnt->mnt.mnt_flags |= MNT_WRITE_HOLD;\n\t/*\n\t * After storing MNT_WRITE_HOLD, we'll read the counters. This store\n\t * should be visible before we do.\n\t */\n\tsmp_mb();\n\n\t/*\n\t * With writers on hold, if this value is zero, then there are\n\t * definitely no active writers (although held writers may subsequently\n\t * increment the count, they'll have to wait, and decrement it after\n\t * seeing MNT_READONLY).\n\t *\n\t * It is OK to have counter incremented on one CPU and decremented on\n\t * another: the sum will add up correctly. The danger would be when we\n\t * sum up each counter, if we read a counter before it is incremented,\n\t * but then read another CPU's count which it has been subsequently\n\t * decremented from -- we would see more decrements than we should.\n\t * MNT_WRITE_HOLD protects against this scenario, because\n\t * mnt_want_write first increments count, then smp_mb, then spins on\n\t * MNT_WRITE_HOLD, so it can't be decremented by another CPU while\n\t * we're counting up here.\n\t */\n\tif (mnt_get_writers(mnt) > 0)\n\t\tret = -EBUSY;\n\telse\n\t\tmnt->mnt.mnt_flags |= MNT_READONLY;\n\t/*\n\t * MNT_READONLY must become visible before ~MNT_WRITE_HOLD, so writers\n\t * that become unheld will see MNT_READONLY.\n\t */\n\tsmp_wmb();\n\tmnt->mnt.mnt_flags &= ~MNT_WRITE_HOLD;\n\tunlock_mount_hash();\n\treturn ret;\n}\n\nstatic void __mnt_unmake_readonly(struct mount *mnt)\n{\n\tlock_mount_hash();\n\tmnt->mnt.mnt_flags &= ~MNT_READONLY;\n\tunlock_mount_hash();\n}\n\nint sb_prepare_remount_readonly(struct super_block *sb)\n{\n\tstruct mount *mnt;\n\tint err = 0;\n\n\t/* Racy optimization.  Recheck the counter under MNT_WRITE_HOLD */\n\tif (atomic_long_read(&sb->s_remove_count))\n\t\treturn -EBUSY;\n\n\tlock_mount_hash();\n\tlist_for_each_entry(mnt, &sb->s_mounts, mnt_instance) {\n\t\tif (!(mnt->mnt.mnt_flags & MNT_READONLY)) {\n\t\t\tmnt->mnt.mnt_flags |= MNT_WRITE_HOLD;\n\t\t\tsmp_mb();\n\t\t\tif (mnt_get_writers(mnt) > 0) {\n\t\t\t\terr = -EBUSY;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tif (!err && atomic_long_read(&sb->s_remove_count))\n\t\terr = -EBUSY;\n\n\tif (!err) {\n\t\tsb->s_readonly_remount = 1;\n\t\tsmp_wmb();\n\t}\n\tlist_for_each_entry(mnt, &sb->s_mounts, mnt_instance) {\n\t\tif (mnt->mnt.mnt_flags & MNT_WRITE_HOLD)\n\t\t\tmnt->mnt.mnt_flags &= ~MNT_WRITE_HOLD;\n\t}\n\tunlock_mount_hash();\n\n\treturn err;\n}\n\nstatic void free_vfsmnt(struct mount *mnt)\n{\n\tkfree(mnt->mnt_devname);\n#ifdef CONFIG_SMP\n\tfree_percpu(mnt->mnt_pcp);\n#endif\n\tkmem_cache_free(mnt_cache, mnt);\n}\n\nstatic void delayed_free_vfsmnt(struct rcu_head *head)\n{\n\tfree_vfsmnt(container_of(head, struct mount, mnt_rcu));\n}\n\n/* call under rcu_read_lock */\nbool legitimize_mnt(struct vfsmount *bastard, unsigned seq)\n{\n\tstruct mount *mnt;\n\tif (read_seqretry(&mount_lock, seq))\n\t\treturn false;\n\tif (bastard == NULL)\n\t\treturn true;\n\tmnt = real_mount(bastard);\n\tmnt_add_count(mnt, 1);\n\tif (likely(!read_seqretry(&mount_lock, seq)))\n\t\treturn true;\n\tif (bastard->mnt_flags & MNT_SYNC_UMOUNT) {\n\t\tmnt_add_count(mnt, -1);\n\t\treturn false;\n\t}\n\trcu_read_unlock();\n\tmntput(bastard);\n\trcu_read_lock();\n\treturn false;\n}\n\n/*\n * find the first mount at @dentry on vfsmount @mnt.\n * call under rcu_read_lock()\n */\nstruct mount *__lookup_mnt(struct vfsmount *mnt, struct dentry *dentry)\n{\n\tstruct hlist_head *head = m_hash(mnt, dentry);\n\tstruct mount *p;\n\n\thlist_for_each_entry_rcu(p, head, mnt_hash)\n\t\tif (&p->mnt_parent->mnt == mnt && p->mnt_mountpoint == dentry)\n\t\t\treturn p;\n\treturn NULL;\n}\n\n/*\n * find the last mount at @dentry on vfsmount @mnt.\n * mount_lock must be held.\n */\nstruct mount *__lookup_mnt_last(struct vfsmount *mnt, struct dentry *dentry)\n{\n\tstruct mount *p, *res;\n\tres = p = __lookup_mnt(mnt, dentry);\n\tif (!p)\n\t\tgoto out;\n\thlist_for_each_entry_continue(p, mnt_hash) {\n\t\tif (&p->mnt_parent->mnt != mnt || p->mnt_mountpoint != dentry)\n\t\t\tbreak;\n\t\tres = p;\n\t}\nout:\n\treturn res;\n}\n\n/*\n * lookup_mnt - Return the first child mount mounted at path\n *\n * \"First\" means first mounted chronologically.  If you create the\n * following mounts:\n *\n * mount /dev/sda1 /mnt\n * mount /dev/sda2 /mnt\n * mount /dev/sda3 /mnt\n *\n * Then lookup_mnt() on the base /mnt dentry in the root mount will\n * return successively the root dentry and vfsmount of /dev/sda1, then\n * /dev/sda2, then /dev/sda3, then NULL.\n *\n * lookup_mnt takes a reference to the found vfsmount.\n */\nstruct vfsmount *lookup_mnt(struct path *path)\n{\n\tstruct mount *child_mnt;\n\tstruct vfsmount *m;\n\tunsigned seq;\n\n\trcu_read_lock();\n\tdo {\n\t\tseq = read_seqbegin(&mount_lock);\n\t\tchild_mnt = __lookup_mnt(path->mnt, path->dentry);\n\t\tm = child_mnt ? &child_mnt->mnt : NULL;\n\t} while (!legitimize_mnt(m, seq));\n\trcu_read_unlock();\n\treturn m;\n}\n\nstatic struct mountpoint *new_mountpoint(struct dentry *dentry)\n{\n\tstruct hlist_head *chain = mp_hash(dentry);\n\tstruct mountpoint *mp;\n\tint ret;\n\n\thlist_for_each_entry(mp, chain, m_hash) {\n\t\tif (mp->m_dentry == dentry) {\n\t\t\t/* might be worth a WARN_ON() */\n\t\t\tif (d_unlinked(dentry))\n\t\t\t\treturn ERR_PTR(-ENOENT);\n\t\t\tmp->m_count++;\n\t\t\treturn mp;\n\t\t}\n\t}\n\n\tmp = kmalloc(sizeof(struct mountpoint), GFP_KERNEL);\n\tif (!mp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = d_set_mounted(dentry);\n\tif (ret) {\n\t\tkfree(mp);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tmp->m_dentry = dentry;\n\tmp->m_count = 1;\n\thlist_add_head(&mp->m_hash, chain);\n\treturn mp;\n}\n\nstatic void put_mountpoint(struct mountpoint *mp)\n{\n\tif (!--mp->m_count) {\n\t\tstruct dentry *dentry = mp->m_dentry;\n\t\tspin_lock(&dentry->d_lock);\n\t\tdentry->d_flags &= ~DCACHE_MOUNTED;\n\t\tspin_unlock(&dentry->d_lock);\n\t\thlist_del(&mp->m_hash);\n\t\tkfree(mp);\n\t}\n}\n\nstatic inline int check_mnt(struct mount *mnt)\n{\n\treturn mnt->mnt_ns == current->nsproxy->mnt_ns;\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void touch_mnt_namespace(struct mnt_namespace *ns)\n{\n\tif (ns) {\n\t\tns->event = ++event;\n\t\twake_up_interruptible(&ns->poll);\n\t}\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void __touch_mnt_namespace(struct mnt_namespace *ns)\n{\n\tif (ns && ns->event != event) {\n\t\tns->event = event;\n\t\twake_up_interruptible(&ns->poll);\n\t}\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void detach_mnt(struct mount *mnt, struct path *old_path)\n{\n\told_path->dentry = mnt->mnt_mountpoint;\n\told_path->mnt = &mnt->mnt_parent->mnt;\n\tmnt->mnt_parent = mnt;\n\tmnt->mnt_mountpoint = mnt->mnt.mnt_root;\n\tlist_del_init(&mnt->mnt_child);\n\thlist_del_init_rcu(&mnt->mnt_hash);\n\tput_mountpoint(mnt->mnt_mp);\n\tmnt->mnt_mp = NULL;\n}\n\n/*\n * vfsmount lock must be held for write\n */\nvoid mnt_set_mountpoint(struct mount *mnt,\n\t\t\tstruct mountpoint *mp,\n\t\t\tstruct mount *child_mnt)\n{\n\tmp->m_count++;\n\tmnt_add_count(mnt, 1);\t/* essentially, that's mntget */\n\tchild_mnt->mnt_mountpoint = dget(mp->m_dentry);\n\tchild_mnt->mnt_parent = mnt;\n\tchild_mnt->mnt_mp = mp;\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void attach_mnt(struct mount *mnt,\n\t\t\tstruct mount *parent,\n\t\t\tstruct mountpoint *mp)\n{\n\tmnt_set_mountpoint(parent, mp, mnt);\n\thlist_add_head_rcu(&mnt->mnt_hash, m_hash(&parent->mnt, mp->m_dentry));\n\tlist_add_tail(&mnt->mnt_child, &parent->mnt_mounts);\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void commit_tree(struct mount *mnt, struct mount *shadows)\n{\n\tstruct mount *parent = mnt->mnt_parent;\n\tstruct mount *m;\n\tLIST_HEAD(head);\n\tstruct mnt_namespace *n = parent->mnt_ns;\n\n\tBUG_ON(parent == mnt);\n\n\tlist_add_tail(&head, &mnt->mnt_list);\n\tlist_for_each_entry(m, &head, mnt_list)\n\t\tm->mnt_ns = n;\n\n\tlist_splice(&head, n->list.prev);\n\n\tif (shadows)\n\t\thlist_add_after_rcu(&shadows->mnt_hash, &mnt->mnt_hash);\n\telse\n\t\thlist_add_head_rcu(&mnt->mnt_hash,\n\t\t\t\tm_hash(&parent->mnt, mnt->mnt_mountpoint));\n\tlist_add_tail(&mnt->mnt_child, &parent->mnt_mounts);\n\ttouch_mnt_namespace(n);\n}\n\nstatic struct mount *next_mnt(struct mount *p, struct mount *root)\n{\n\tstruct list_head *next = p->mnt_mounts.next;\n\tif (next == &p->mnt_mounts) {\n\t\twhile (1) {\n\t\t\tif (p == root)\n\t\t\t\treturn NULL;\n\t\t\tnext = p->mnt_child.next;\n\t\t\tif (next != &p->mnt_parent->mnt_mounts)\n\t\t\t\tbreak;\n\t\t\tp = p->mnt_parent;\n\t\t}\n\t}\n\treturn list_entry(next, struct mount, mnt_child);\n}\n\nstatic struct mount *skip_mnt_tree(struct mount *p)\n{\n\tstruct list_head *prev = p->mnt_mounts.prev;\n\twhile (prev != &p->mnt_mounts) {\n\t\tp = list_entry(prev, struct mount, mnt_child);\n\t\tprev = p->mnt_mounts.prev;\n\t}\n\treturn p;\n}\n\nstruct vfsmount *\nvfs_kern_mount(struct file_system_type *type, int flags, const char *name, void *data)\n{\n\tstruct mount *mnt;\n\tstruct dentry *root;\n\n\tif (!type)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tmnt = alloc_vfsmnt(name);\n\tif (!mnt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (flags & MS_KERNMOUNT)\n\t\tmnt->mnt.mnt_flags = MNT_INTERNAL;\n\n\troot = mount_fs(type, flags, name, data);\n\tif (IS_ERR(root)) {\n\t\tmnt_free_id(mnt);\n\t\tfree_vfsmnt(mnt);\n\t\treturn ERR_CAST(root);\n\t}\n\n\tmnt->mnt.mnt_root = root;\n\tmnt->mnt.mnt_sb = root->d_sb;\n\tmnt->mnt_mountpoint = mnt->mnt.mnt_root;\n\tmnt->mnt_parent = mnt;\n\tlock_mount_hash();\n\tlist_add_tail(&mnt->mnt_instance, &root->d_sb->s_mounts);\n\tunlock_mount_hash();\n\treturn &mnt->mnt;\n}\nEXPORT_SYMBOL_GPL(vfs_kern_mount);\n\nstatic struct mount *clone_mnt(struct mount *old, struct dentry *root,\n\t\t\t\t\tint flag)\n{\n\tstruct super_block *sb = old->mnt.mnt_sb;\n\tstruct mount *mnt;\n\tint err;\n\n\tmnt = alloc_vfsmnt(old->mnt_devname);\n\tif (!mnt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (flag & (CL_SLAVE | CL_PRIVATE | CL_SHARED_TO_SLAVE))\n\t\tmnt->mnt_group_id = 0; /* not a peer of original */\n\telse\n\t\tmnt->mnt_group_id = old->mnt_group_id;\n\n\tif ((flag & CL_MAKE_SHARED) && !mnt->mnt_group_id) {\n\t\terr = mnt_alloc_group_id(mnt);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t}\n\n\tmnt->mnt.mnt_flags = old->mnt.mnt_flags & ~(MNT_WRITE_HOLD|MNT_MARKED);\n\t/* Don't allow unprivileged users to change mount flags */\n\tif ((flag & CL_UNPRIVILEGED) && (mnt->mnt.mnt_flags & MNT_READONLY))\n\t\tmnt->mnt.mnt_flags |= MNT_LOCK_READONLY;\n\n\t/* Don't allow unprivileged users to reveal what is under a mount */\n\tif ((flag & CL_UNPRIVILEGED) && list_empty(&old->mnt_expire))\n\t\tmnt->mnt.mnt_flags |= MNT_LOCKED;\n\n\tatomic_inc(&sb->s_active);\n\tmnt->mnt.mnt_sb = sb;\n\tmnt->mnt.mnt_root = dget(root);\n\tmnt->mnt_mountpoint = mnt->mnt.mnt_root;\n\tmnt->mnt_parent = mnt;\n\tlock_mount_hash();\n\tlist_add_tail(&mnt->mnt_instance, &sb->s_mounts);\n\tunlock_mount_hash();\n\n\tif ((flag & CL_SLAVE) ||\n\t    ((flag & CL_SHARED_TO_SLAVE) && IS_MNT_SHARED(old))) {\n\t\tlist_add(&mnt->mnt_slave, &old->mnt_slave_list);\n\t\tmnt->mnt_master = old;\n\t\tCLEAR_MNT_SHARED(mnt);\n\t} else if (!(flag & CL_PRIVATE)) {\n\t\tif ((flag & CL_MAKE_SHARED) || IS_MNT_SHARED(old))\n\t\t\tlist_add(&mnt->mnt_share, &old->mnt_share);\n\t\tif (IS_MNT_SLAVE(old))\n\t\t\tlist_add(&mnt->mnt_slave, &old->mnt_slave);\n\t\tmnt->mnt_master = old->mnt_master;\n\t}\n\tif (flag & CL_MAKE_SHARED)\n\t\tset_mnt_shared(mnt);\n\n\t/* stick the duplicate mount on the same expiry list\n\t * as the original if that was on one */\n\tif (flag & CL_EXPIRE) {\n\t\tif (!list_empty(&old->mnt_expire))\n\t\t\tlist_add(&mnt->mnt_expire, &old->mnt_expire);\n\t}\n\n\treturn mnt;\n\n out_free:\n\tmnt_free_id(mnt);\n\tfree_vfsmnt(mnt);\n\treturn ERR_PTR(err);\n}\n\nstatic void mntput_no_expire(struct mount *mnt)\n{\nput_again:\n\trcu_read_lock();\n\tmnt_add_count(mnt, -1);\n\tif (likely(mnt->mnt_ns)) { /* shouldn't be the last one */\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tlock_mount_hash();\n\tif (mnt_get_count(mnt)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tif (unlikely(mnt->mnt_pinned)) {\n\t\tmnt_add_count(mnt, mnt->mnt_pinned + 1);\n\t\tmnt->mnt_pinned = 0;\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\tacct_auto_close_mnt(&mnt->mnt);\n\t\tgoto put_again;\n\t}\n\tif (unlikely(mnt->mnt.mnt_flags & MNT_DOOMED)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tmnt->mnt.mnt_flags |= MNT_DOOMED;\n\trcu_read_unlock();\n\n\tlist_del(&mnt->mnt_instance);\n\tunlock_mount_hash();\n\n\t/*\n\t * This probably indicates that somebody messed\n\t * up a mnt_want/drop_write() pair.  If this\n\t * happens, the filesystem was probably unable\n\t * to make r/w->r/o transitions.\n\t */\n\t/*\n\t * The locking used to deal with mnt_count decrement provides barriers,\n\t * so mnt_get_writers() below is safe.\n\t */\n\tWARN_ON(mnt_get_writers(mnt));\n\tfsnotify_vfsmount_delete(&mnt->mnt);\n\tdput(mnt->mnt.mnt_root);\n\tdeactivate_super(mnt->mnt.mnt_sb);\n\tmnt_free_id(mnt);\n\tcall_rcu(&mnt->mnt_rcu, delayed_free_vfsmnt);\n}\n\nvoid mntput(struct vfsmount *mnt)\n{\n\tif (mnt) {\n\t\tstruct mount *m = real_mount(mnt);\n\t\t/* avoid cacheline pingpong, hope gcc doesn't get \"smart\" */\n\t\tif (unlikely(m->mnt_expiry_mark))\n\t\t\tm->mnt_expiry_mark = 0;\n\t\tmntput_no_expire(m);\n\t}\n}\nEXPORT_SYMBOL(mntput);\n\nstruct vfsmount *mntget(struct vfsmount *mnt)\n{\n\tif (mnt)\n\t\tmnt_add_count(real_mount(mnt), 1);\n\treturn mnt;\n}\nEXPORT_SYMBOL(mntget);\n\nvoid mnt_pin(struct vfsmount *mnt)\n{\n\tlock_mount_hash();\n\treal_mount(mnt)->mnt_pinned++;\n\tunlock_mount_hash();\n}\nEXPORT_SYMBOL(mnt_pin);\n\nvoid mnt_unpin(struct vfsmount *m)\n{\n\tstruct mount *mnt = real_mount(m);\n\tlock_mount_hash();\n\tif (mnt->mnt_pinned) {\n\t\tmnt_add_count(mnt, 1);\n\t\tmnt->mnt_pinned--;\n\t}\n\tunlock_mount_hash();\n}\nEXPORT_SYMBOL(mnt_unpin);\n\nstatic inline void mangle(struct seq_file *m, const char *s)\n{\n\tseq_escape(m, s, \" \\t\\n\\\\\");\n}\n\n/*\n * Simple .show_options callback for filesystems which don't want to\n * implement more complex mount option showing.\n *\n * See also save_mount_options().\n */\nint generic_show_options(struct seq_file *m, struct dentry *root)\n{\n\tconst char *options;\n\n\trcu_read_lock();\n\toptions = rcu_dereference(root->d_sb->s_options);\n\n\tif (options != NULL && options[0]) {\n\t\tseq_putc(m, ',');\n\t\tmangle(m, options);\n\t}\n\trcu_read_unlock();\n\n\treturn 0;\n}\nEXPORT_SYMBOL(generic_show_options);\n\n/*\n * If filesystem uses generic_show_options(), this function should be\n * called from the fill_super() callback.\n *\n * The .remount_fs callback usually needs to be handled in a special\n * way, to make sure, that previous options are not overwritten if the\n * remount fails.\n *\n * Also note, that if the filesystem's .remount_fs function doesn't\n * reset all options to their default value, but changes only newly\n * given options, then the displayed options will not reflect reality\n * any more.\n */\nvoid save_mount_options(struct super_block *sb, char *options)\n{\n\tBUG_ON(sb->s_options);\n\trcu_assign_pointer(sb->s_options, kstrdup(options, GFP_KERNEL));\n}\nEXPORT_SYMBOL(save_mount_options);\n\nvoid replace_mount_options(struct super_block *sb, char *options)\n{\n\tchar *old = sb->s_options;\n\trcu_assign_pointer(sb->s_options, options);\n\tif (old) {\n\t\tsynchronize_rcu();\n\t\tkfree(old);\n\t}\n}\nEXPORT_SYMBOL(replace_mount_options);\n\n#ifdef CONFIG_PROC_FS\n/* iterator; we want it to have access to namespace_sem, thus here... */\nstatic void *m_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct proc_mounts *p = proc_mounts(m);\n\n\tdown_read(&namespace_sem);\n\tif (p->cached_event == p->ns->event) {\n\t\tvoid *v = p->cached_mount;\n\t\tif (*pos == p->cached_index)\n\t\t\treturn v;\n\t\tif (*pos == p->cached_index + 1) {\n\t\t\tv = seq_list_next(v, &p->ns->list, &p->cached_index);\n\t\t\treturn p->cached_mount = v;\n\t\t}\n\t}\n\n\tp->cached_event = p->ns->event;\n\tp->cached_mount = seq_list_start(&p->ns->list, *pos);\n\tp->cached_index = *pos;\n\treturn p->cached_mount;\n}\n\nstatic void *m_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct proc_mounts *p = proc_mounts(m);\n\n\tp->cached_mount = seq_list_next(v, &p->ns->list, pos);\n\tp->cached_index = *pos;\n\treturn p->cached_mount;\n}\n\nstatic void m_stop(struct seq_file *m, void *v)\n{\n\tup_read(&namespace_sem);\n}\n\nstatic int m_show(struct seq_file *m, void *v)\n{\n\tstruct proc_mounts *p = proc_mounts(m);\n\tstruct mount *r = list_entry(v, struct mount, mnt_list);\n\treturn p->show(m, &r->mnt);\n}\n\nconst struct seq_operations mounts_op = {\n\t.start\t= m_start,\n\t.next\t= m_next,\n\t.stop\t= m_stop,\n\t.show\t= m_show,\n};\n#endif  /* CONFIG_PROC_FS */\n\n/**\n * may_umount_tree - check if a mount tree is busy\n * @mnt: root of mount tree\n *\n * This is called to check if a tree of mounts has any\n * open files, pwds, chroots or sub mounts that are\n * busy.\n */\nint may_umount_tree(struct vfsmount *m)\n{\n\tstruct mount *mnt = real_mount(m);\n\tint actual_refs = 0;\n\tint minimum_refs = 0;\n\tstruct mount *p;\n\tBUG_ON(!m);\n\n\t/* write lock needed for mnt_get_count */\n\tlock_mount_hash();\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tactual_refs += mnt_get_count(p);\n\t\tminimum_refs += 2;\n\t}\n\tunlock_mount_hash();\n\n\tif (actual_refs > minimum_refs)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nEXPORT_SYMBOL(may_umount_tree);\n\n/**\n * may_umount - check if a mount point is busy\n * @mnt: root of mount\n *\n * This is called to check if a mount point has any\n * open files, pwds, chroots or sub mounts. If the\n * mount has sub mounts this will return busy\n * regardless of whether the sub mounts are busy.\n *\n * Doesn't take quota and stuff into account. IOW, in some cases it will\n * give false negatives. The main reason why it's here is that we need\n * a non-destructive way to look for easily umountable filesystems.\n */\nint may_umount(struct vfsmount *mnt)\n{\n\tint ret = 1;\n\tdown_read(&namespace_sem);\n\tlock_mount_hash();\n\tif (propagate_mount_busy(real_mount(mnt), 2))\n\t\tret = 0;\n\tunlock_mount_hash();\n\tup_read(&namespace_sem);\n\treturn ret;\n}\n\nEXPORT_SYMBOL(may_umount);\n\nstatic HLIST_HEAD(unmounted);\t/* protected by namespace_sem */\n\nstatic void namespace_unlock(void)\n{\n\tstruct mount *mnt;\n\tstruct hlist_head head = unmounted;\n\n\tif (likely(hlist_empty(&head))) {\n\t\tup_write(&namespace_sem);\n\t\treturn;\n\t}\n\n\thead.first->pprev = &head.first;\n\tINIT_HLIST_HEAD(&unmounted);\n\n\tup_write(&namespace_sem);\n\n\tsynchronize_rcu();\n\n\twhile (!hlist_empty(&head)) {\n\t\tmnt = hlist_entry(head.first, struct mount, mnt_hash);\n\t\thlist_del_init(&mnt->mnt_hash);\n\t\tif (mnt->mnt_ex_mountpoint.mnt)\n\t\t\tpath_put(&mnt->mnt_ex_mountpoint);\n\t\tmntput(&mnt->mnt);\n\t}\n}\n\nstatic inline void namespace_lock(void)\n{\n\tdown_write(&namespace_sem);\n}\n\n/*\n * mount_lock must be held\n * namespace_sem must be held for write\n * how = 0 => just this tree, don't propagate\n * how = 1 => propagate; we know that nobody else has reference to any victims\n * how = 2 => lazy umount\n */\nvoid umount_tree(struct mount *mnt, int how)\n{\n\tHLIST_HEAD(tmp_list);\n\tstruct mount *p;\n\tstruct mount *last = NULL;\n\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\thlist_del_init_rcu(&p->mnt_hash);\n\t\thlist_add_head(&p->mnt_hash, &tmp_list);\n\t}\n\n\tif (how)\n\t\tpropagate_umount(&tmp_list);\n\n\thlist_for_each_entry(p, &tmp_list, mnt_hash) {\n\t\tlist_del_init(&p->mnt_expire);\n\t\tlist_del_init(&p->mnt_list);\n\t\t__touch_mnt_namespace(p->mnt_ns);\n\t\tp->mnt_ns = NULL;\n\t\tif (how < 2)\n\t\t\tp->mnt.mnt_flags |= MNT_SYNC_UMOUNT;\n\t\tlist_del_init(&p->mnt_child);\n\t\tif (mnt_has_parent(p)) {\n\t\t\tput_mountpoint(p->mnt_mp);\n\t\t\t/* move the reference to mountpoint into ->mnt_ex_mountpoint */\n\t\t\tp->mnt_ex_mountpoint.dentry = p->mnt_mountpoint;\n\t\t\tp->mnt_ex_mountpoint.mnt = &p->mnt_parent->mnt;\n\t\t\tp->mnt_mountpoint = p->mnt.mnt_root;\n\t\t\tp->mnt_parent = p;\n\t\t\tp->mnt_mp = NULL;\n\t\t}\n\t\tchange_mnt_propagation(p, MS_PRIVATE);\n\t\tlast = p;\n\t}\n\tif (last) {\n\t\tlast->mnt_hash.next = unmounted.first;\n\t\tunmounted.first = tmp_list.first;\n\t\tunmounted.first->pprev = &unmounted.first;\n\t}\n}\n\nstatic void shrink_submounts(struct mount *mnt);\n\nstatic int do_umount(struct mount *mnt, int flags)\n{\n\tstruct super_block *sb = mnt->mnt.mnt_sb;\n\tint retval;\n\n\tretval = security_sb_umount(&mnt->mnt, flags);\n\tif (retval)\n\t\treturn retval;\n\n\t/*\n\t * Allow userspace to request a mountpoint be expired rather than\n\t * unmounting unconditionally. Unmount only happens if:\n\t *  (1) the mark is already set (the mark is cleared by mntput())\n\t *  (2) the usage count == 1 [parent vfsmount] + 1 [sys_umount]\n\t */\n\tif (flags & MNT_EXPIRE) {\n\t\tif (&mnt->mnt == current->fs->root.mnt ||\n\t\t    flags & (MNT_FORCE | MNT_DETACH))\n\t\t\treturn -EINVAL;\n\n\t\t/*\n\t\t * probably don't strictly need the lock here if we examined\n\t\t * all race cases, but it's a slowpath.\n\t\t */\n\t\tlock_mount_hash();\n\t\tif (mnt_get_count(mnt) != 2) {\n\t\t\tunlock_mount_hash();\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tunlock_mount_hash();\n\n\t\tif (!xchg(&mnt->mnt_expiry_mark, 1))\n\t\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * If we may have to abort operations to get out of this\n\t * mount, and they will themselves hold resources we must\n\t * allow the fs to do things. In the Unix tradition of\n\t * 'Gee thats tricky lets do it in userspace' the umount_begin\n\t * might fail to complete on the first run through as other tasks\n\t * must return, and the like. Thats for the mount program to worry\n\t * about for the moment.\n\t */\n\n\tif (flags & MNT_FORCE && sb->s_op->umount_begin) {\n\t\tsb->s_op->umount_begin(sb);\n\t}\n\n\t/*\n\t * No sense to grab the lock for this test, but test itself looks\n\t * somewhat bogus. Suggestions for better replacement?\n\t * Ho-hum... In principle, we might treat that as umount + switch\n\t * to rootfs. GC would eventually take care of the old vfsmount.\n\t * Actually it makes sense, especially if rootfs would contain a\n\t * /reboot - static binary that would close all descriptors and\n\t * call reboot(9). Then init(8) could umount root and exec /reboot.\n\t */\n\tif (&mnt->mnt == current->fs->root.mnt && !(flags & MNT_DETACH)) {\n\t\t/*\n\t\t * Special case for \"unmounting\" root ...\n\t\t * we just try to remount it readonly.\n\t\t */\n\t\tdown_write(&sb->s_umount);\n\t\tif (!(sb->s_flags & MS_RDONLY))\n\t\t\tretval = do_remount_sb(sb, MS_RDONLY, NULL, 0);\n\t\tup_write(&sb->s_umount);\n\t\treturn retval;\n\t}\n\n\tnamespace_lock();\n\tlock_mount_hash();\n\tevent++;\n\n\tif (flags & MNT_DETACH) {\n\t\tif (!list_empty(&mnt->mnt_list))\n\t\t\tumount_tree(mnt, 2);\n\t\tretval = 0;\n\t} else {\n\t\tshrink_submounts(mnt);\n\t\tretval = -EBUSY;\n\t\tif (!propagate_mount_busy(mnt, 2)) {\n\t\t\tif (!list_empty(&mnt->mnt_list))\n\t\t\t\tumount_tree(mnt, 1);\n\t\t\tretval = 0;\n\t\t}\n\t}\n\tunlock_mount_hash();\n\tnamespace_unlock();\n\treturn retval;\n}\n\n/* \n * Is the caller allowed to modify his namespace?\n */\nstatic inline bool may_mount(void)\n{\n\treturn ns_capable(current->nsproxy->mnt_ns->user_ns, CAP_SYS_ADMIN);\n}\n\n/*\n * Now umount can handle mount points as well as block devices.\n * This is important for filesystems which use unnamed block devices.\n *\n * We now support a flag for forced unmount like the other 'big iron'\n * unixes. Our API is identical to OSF/1 to avoid making a mess of AMD\n */\n\nSYSCALL_DEFINE2(umount, char __user *, name, int, flags)\n{\n\tstruct path path;\n\tstruct mount *mnt;\n\tint retval;\n\tint lookup_flags = 0;\n\n\tif (flags & ~(MNT_FORCE | MNT_DETACH | MNT_EXPIRE | UMOUNT_NOFOLLOW))\n\t\treturn -EINVAL;\n\n\tif (!may_mount())\n\t\treturn -EPERM;\n\n\tif (!(flags & UMOUNT_NOFOLLOW))\n\t\tlookup_flags |= LOOKUP_FOLLOW;\n\n\tretval = user_path_mountpoint_at(AT_FDCWD, name, lookup_flags, &path);\n\tif (retval)\n\t\tgoto out;\n\tmnt = real_mount(path.mnt);\n\tretval = -EINVAL;\n\tif (path.dentry != path.mnt->mnt_root)\n\t\tgoto dput_and_out;\n\tif (!check_mnt(mnt))\n\t\tgoto dput_and_out;\n\tif (mnt->mnt.mnt_flags & MNT_LOCKED)\n\t\tgoto dput_and_out;\n\n\tretval = do_umount(mnt, flags);\ndput_and_out:\n\t/* we mustn't call path_put() as that would clear mnt_expiry_mark */\n\tdput(path.dentry);\n\tmntput_no_expire(mnt);\nout:\n\treturn retval;\n}\n\n#ifdef __ARCH_WANT_SYS_OLDUMOUNT\n\n/*\n *\tThe 2.0 compatible umount. No flags.\n */\nSYSCALL_DEFINE1(oldumount, char __user *, name)\n{\n\treturn sys_umount(name, 0);\n}\n\n#endif\n\nstatic bool is_mnt_ns_file(struct dentry *dentry)\n{\n\t/* Is this a proxy for a mount namespace? */\n\tstruct inode *inode = dentry->d_inode;\n\tstruct proc_ns *ei;\n\n\tif (!proc_ns_inode(inode))\n\t\treturn false;\n\n\tei = get_proc_ns(inode);\n\tif (ei->ns_ops != &mntns_operations)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool mnt_ns_loop(struct dentry *dentry)\n{\n\t/* Could bind mounting the mount namespace inode cause a\n\t * mount namespace loop?\n\t */\n\tstruct mnt_namespace *mnt_ns;\n\tif (!is_mnt_ns_file(dentry))\n\t\treturn false;\n\n\tmnt_ns = get_proc_ns(dentry->d_inode)->ns;\n\treturn current->nsproxy->mnt_ns->seq >= mnt_ns->seq;\n}\n\nstruct mount *copy_tree(struct mount *mnt, struct dentry *dentry,\n\t\t\t\t\tint flag)\n{\n\tstruct mount *res, *p, *q, *r, *parent;\n\n\tif (!(flag & CL_COPY_UNBINDABLE) && IS_MNT_UNBINDABLE(mnt))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!(flag & CL_COPY_MNT_NS_FILE) && is_mnt_ns_file(dentry))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tres = q = clone_mnt(mnt, dentry, flag);\n\tif (IS_ERR(q))\n\t\treturn q;\n\n\tq->mnt.mnt_flags &= ~MNT_LOCKED;\n\tq->mnt_mountpoint = mnt->mnt_mountpoint;\n\n\tp = mnt;\n\tlist_for_each_entry(r, &mnt->mnt_mounts, mnt_child) {\n\t\tstruct mount *s;\n\t\tif (!is_subdir(r->mnt_mountpoint, dentry))\n\t\t\tcontinue;\n\n\t\tfor (s = r; s; s = next_mnt(s, r)) {\n\t\t\tif (!(flag & CL_COPY_UNBINDABLE) &&\n\t\t\t    IS_MNT_UNBINDABLE(s)) {\n\t\t\t\ts = skip_mnt_tree(s);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!(flag & CL_COPY_MNT_NS_FILE) &&\n\t\t\t    is_mnt_ns_file(s->mnt.mnt_root)) {\n\t\t\t\ts = skip_mnt_tree(s);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\twhile (p != s->mnt_parent) {\n\t\t\t\tp = p->mnt_parent;\n\t\t\t\tq = q->mnt_parent;\n\t\t\t}\n\t\t\tp = s;\n\t\t\tparent = q;\n\t\t\tq = clone_mnt(p, p->mnt.mnt_root, flag);\n\t\t\tif (IS_ERR(q))\n\t\t\t\tgoto out;\n\t\t\tlock_mount_hash();\n\t\t\tlist_add_tail(&q->mnt_list, &res->mnt_list);\n\t\t\tattach_mnt(q, parent, p->mnt_mp);\n\t\t\tunlock_mount_hash();\n\t\t}\n\t}\n\treturn res;\nout:\n\tif (res) {\n\t\tlock_mount_hash();\n\t\tumount_tree(res, 0);\n\t\tunlock_mount_hash();\n\t}\n\treturn q;\n}\n\n/* Caller should check returned pointer for errors */\n\nstruct vfsmount *collect_mounts(struct path *path)\n{\n\tstruct mount *tree;\n\tnamespace_lock();\n\ttree = copy_tree(real_mount(path->mnt), path->dentry,\n\t\t\t CL_COPY_ALL | CL_PRIVATE);\n\tnamespace_unlock();\n\tif (IS_ERR(tree))\n\t\treturn ERR_CAST(tree);\n\treturn &tree->mnt;\n}\n\nvoid drop_collected_mounts(struct vfsmount *mnt)\n{\n\tnamespace_lock();\n\tlock_mount_hash();\n\tumount_tree(real_mount(mnt), 0);\n\tunlock_mount_hash();\n\tnamespace_unlock();\n}\n\nint iterate_mounts(int (*f)(struct vfsmount *, void *), void *arg,\n\t\t   struct vfsmount *root)\n{\n\tstruct mount *mnt;\n\tint res = f(root, arg);\n\tif (res)\n\t\treturn res;\n\tlist_for_each_entry(mnt, &real_mount(root)->mnt_list, mnt_list) {\n\t\tres = f(&mnt->mnt, arg);\n\t\tif (res)\n\t\t\treturn res;\n\t}\n\treturn 0;\n}\n\nstatic void cleanup_group_ids(struct mount *mnt, struct mount *end)\n{\n\tstruct mount *p;\n\n\tfor (p = mnt; p != end; p = next_mnt(p, mnt)) {\n\t\tif (p->mnt_group_id && !IS_MNT_SHARED(p))\n\t\t\tmnt_release_group_id(p);\n\t}\n}\n\nstatic int invent_group_ids(struct mount *mnt, bool recurse)\n{\n\tstruct mount *p;\n\n\tfor (p = mnt; p; p = recurse ? next_mnt(p, mnt) : NULL) {\n\t\tif (!p->mnt_group_id && !IS_MNT_SHARED(p)) {\n\t\t\tint err = mnt_alloc_group_id(p);\n\t\t\tif (err) {\n\t\t\t\tcleanup_group_ids(mnt, p);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n *  @source_mnt : mount tree to be attached\n *  @nd         : place the mount tree @source_mnt is attached\n *  @parent_nd  : if non-null, detach the source_mnt from its parent and\n *  \t\t   store the parent mount and mountpoint dentry.\n *  \t\t   (done when source_mnt is moved)\n *\n *  NOTE: in the table below explains the semantics when a source mount\n *  of a given type is attached to a destination mount of a given type.\n * ---------------------------------------------------------------------------\n * |         BIND MOUNT OPERATION                                            |\n * |**************************************************************************\n * | source-->| shared        |       private  |       slave    | unbindable |\n * | dest     |               |                |                |            |\n * |   |      |               |                |                |            |\n * |   v      |               |                |                |            |\n * |**************************************************************************\n * |  shared  | shared (++)   |     shared (+) |     shared(+++)|  invalid   |\n * |          |               |                |                |            |\n * |non-shared| shared (+)    |      private   |      slave (*) |  invalid   |\n * ***************************************************************************\n * A bind operation clones the source mount and mounts the clone on the\n * destination mount.\n *\n * (++)  the cloned mount is propagated to all the mounts in the propagation\n * \t tree of the destination mount and the cloned mount is added to\n * \t the peer group of the source mount.\n * (+)   the cloned mount is created under the destination mount and is marked\n *       as shared. The cloned mount is added to the peer group of the source\n *       mount.\n * (+++) the mount is propagated to all the mounts in the propagation tree\n *       of the destination mount and the cloned mount is made slave\n *       of the same master as that of the source mount. The cloned mount\n *       is marked as 'shared and slave'.\n * (*)   the cloned mount is made a slave of the same master as that of the\n * \t source mount.\n *\n * ---------------------------------------------------------------------------\n * |         \t\tMOVE MOUNT OPERATION                                 |\n * |**************************************************************************\n * | source-->| shared        |       private  |       slave    | unbindable |\n * | dest     |               |                |                |            |\n * |   |      |               |                |                |            |\n * |   v      |               |                |                |            |\n * |**************************************************************************\n * |  shared  | shared (+)    |     shared (+) |    shared(+++) |  invalid   |\n * |          |               |                |                |            |\n * |non-shared| shared (+*)   |      private   |    slave (*)   | unbindable |\n * ***************************************************************************\n *\n * (+)  the mount is moved to the destination. And is then propagated to\n * \tall the mounts in the propagation tree of the destination mount.\n * (+*)  the mount is moved to the destination.\n * (+++)  the mount is moved to the destination and is then propagated to\n * \tall the mounts belonging to the destination mount's propagation tree.\n * \tthe mount is marked as 'shared and slave'.\n * (*)\tthe mount continues to be a slave at the new location.\n *\n * if the source mount is a tree, the operations explained above is\n * applied to each mount in the tree.\n * Must be called without spinlocks held, since this function can sleep\n * in allocations.\n */\nstatic int attach_recursive_mnt(struct mount *source_mnt,\n\t\t\tstruct mount *dest_mnt,\n\t\t\tstruct mountpoint *dest_mp,\n\t\t\tstruct path *parent_path)\n{\n\tHLIST_HEAD(tree_list);\n\tstruct mount *child, *p;\n\tstruct hlist_node *n;\n\tint err;\n\n\tif (IS_MNT_SHARED(dest_mnt)) {\n\t\terr = invent_group_ids(source_mnt, true);\n\t\tif (err)\n\t\t\tgoto out;\n\t\terr = propagate_mnt(dest_mnt, dest_mp, source_mnt, &tree_list);\n\t\tlock_mount_hash();\n\t\tif (err)\n\t\t\tgoto out_cleanup_ids;\n\t\tfor (p = source_mnt; p; p = next_mnt(p, source_mnt))\n\t\t\tset_mnt_shared(p);\n\t} else {\n\t\tlock_mount_hash();\n\t}\n\tif (parent_path) {\n\t\tdetach_mnt(source_mnt, parent_path);\n\t\tattach_mnt(source_mnt, dest_mnt, dest_mp);\n\t\ttouch_mnt_namespace(source_mnt->mnt_ns);\n\t} else {\n\t\tmnt_set_mountpoint(dest_mnt, dest_mp, source_mnt);\n\t\tcommit_tree(source_mnt, NULL);\n\t}\n\n\thlist_for_each_entry_safe(child, n, &tree_list, mnt_hash) {\n\t\tstruct mount *q;\n\t\thlist_del_init(&child->mnt_hash);\n\t\tq = __lookup_mnt_last(&child->mnt_parent->mnt,\n\t\t\t\t      child->mnt_mountpoint);\n\t\tcommit_tree(child, q);\n\t}\n\tunlock_mount_hash();\n\n\treturn 0;\n\n out_cleanup_ids:\n\twhile (!hlist_empty(&tree_list)) {\n\t\tchild = hlist_entry(tree_list.first, struct mount, mnt_hash);\n\t\tumount_tree(child, 0);\n\t}\n\tunlock_mount_hash();\n\tcleanup_group_ids(source_mnt, NULL);\n out:\n\treturn err;\n}\n\nstatic struct mountpoint *lock_mount(struct path *path)\n{\n\tstruct vfsmount *mnt;\n\tstruct dentry *dentry = path->dentry;\nretry:\n\tmutex_lock(&dentry->d_inode->i_mutex);\n\tif (unlikely(cant_mount(dentry))) {\n\t\tmutex_unlock(&dentry->d_inode->i_mutex);\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\tnamespace_lock();\n\tmnt = lookup_mnt(path);\n\tif (likely(!mnt)) {\n\t\tstruct mountpoint *mp = new_mountpoint(dentry);\n\t\tif (IS_ERR(mp)) {\n\t\t\tnamespace_unlock();\n\t\t\tmutex_unlock(&dentry->d_inode->i_mutex);\n\t\t\treturn mp;\n\t\t}\n\t\treturn mp;\n\t}\n\tnamespace_unlock();\n\tmutex_unlock(&path->dentry->d_inode->i_mutex);\n\tpath_put(path);\n\tpath->mnt = mnt;\n\tdentry = path->dentry = dget(mnt->mnt_root);\n\tgoto retry;\n}\n\nstatic void unlock_mount(struct mountpoint *where)\n{\n\tstruct dentry *dentry = where->m_dentry;\n\tput_mountpoint(where);\n\tnamespace_unlock();\n\tmutex_unlock(&dentry->d_inode->i_mutex);\n}\n\nstatic int graft_tree(struct mount *mnt, struct mount *p, struct mountpoint *mp)\n{\n\tif (mnt->mnt.mnt_sb->s_flags & MS_NOUSER)\n\t\treturn -EINVAL;\n\n\tif (S_ISDIR(mp->m_dentry->d_inode->i_mode) !=\n\t      S_ISDIR(mnt->mnt.mnt_root->d_inode->i_mode))\n\t\treturn -ENOTDIR;\n\n\treturn attach_recursive_mnt(mnt, p, mp, NULL);\n}\n\n/*\n * Sanity check the flags to change_mnt_propagation.\n */\n\nstatic int flags_to_propagation_type(int flags)\n{\n\tint type = flags & ~(MS_REC | MS_SILENT);\n\n\t/* Fail if any non-propagation flags are set */\n\tif (type & ~(MS_SHARED | MS_PRIVATE | MS_SLAVE | MS_UNBINDABLE))\n\t\treturn 0;\n\t/* Only one propagation flag should be set */\n\tif (!is_power_of_2(type))\n\t\treturn 0;\n\treturn type;\n}\n\n/*\n * recursively change the type of the mountpoint.\n */\nstatic int do_change_type(struct path *path, int flag)\n{\n\tstruct mount *m;\n\tstruct mount *mnt = real_mount(path->mnt);\n\tint recurse = flag & MS_REC;\n\tint type;\n\tint err = 0;\n\n\tif (path->dentry != path->mnt->mnt_root)\n\t\treturn -EINVAL;\n\n\ttype = flags_to_propagation_type(flag);\n\tif (!type)\n\t\treturn -EINVAL;\n\n\tnamespace_lock();\n\tif (type == MS_SHARED) {\n\t\terr = invent_group_ids(mnt, recurse);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t}\n\n\tlock_mount_hash();\n\tfor (m = mnt; m; m = (recurse ? next_mnt(m, mnt) : NULL))\n\t\tchange_mnt_propagation(m, type);\n\tunlock_mount_hash();\n\n out_unlock:\n\tnamespace_unlock();\n\treturn err;\n}\n\nstatic bool has_locked_children(struct mount *mnt, struct dentry *dentry)\n{\n\tstruct mount *child;\n\tlist_for_each_entry(child, &mnt->mnt_mounts, mnt_child) {\n\t\tif (!is_subdir(child->mnt_mountpoint, dentry))\n\t\t\tcontinue;\n\n\t\tif (child->mnt.mnt_flags & MNT_LOCKED)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n/*\n * do loopback mount.\n */\nstatic int do_loopback(struct path *path, const char *old_name,\n\t\t\t\tint recurse)\n{\n\tstruct path old_path;\n\tstruct mount *mnt = NULL, *old, *parent;\n\tstruct mountpoint *mp;\n\tint err;\n\tif (!old_name || !*old_name)\n\t\treturn -EINVAL;\n\terr = kern_path(old_name, LOOKUP_FOLLOW|LOOKUP_AUTOMOUNT, &old_path);\n\tif (err)\n\t\treturn err;\n\n\terr = -EINVAL;\n\tif (mnt_ns_loop(old_path.dentry))\n\t\tgoto out; \n\n\tmp = lock_mount(path);\n\terr = PTR_ERR(mp);\n\tif (IS_ERR(mp))\n\t\tgoto out;\n\n\told = real_mount(old_path.mnt);\n\tparent = real_mount(path->mnt);\n\n\terr = -EINVAL;\n\tif (IS_MNT_UNBINDABLE(old))\n\t\tgoto out2;\n\n\tif (!check_mnt(parent) || !check_mnt(old))\n\t\tgoto out2;\n\n\tif (!recurse && has_locked_children(old, old_path.dentry))\n\t\tgoto out2;\n\n\tif (recurse)\n\t\tmnt = copy_tree(old, old_path.dentry, CL_COPY_MNT_NS_FILE);\n\telse\n\t\tmnt = clone_mnt(old, old_path.dentry, 0);\n\n\tif (IS_ERR(mnt)) {\n\t\terr = PTR_ERR(mnt);\n\t\tgoto out2;\n\t}\n\n\tmnt->mnt.mnt_flags &= ~MNT_LOCKED;\n\n\terr = graft_tree(mnt, parent, mp);\n\tif (err) {\n\t\tlock_mount_hash();\n\t\tumount_tree(mnt, 0);\n\t\tunlock_mount_hash();\n\t}\nout2:\n\tunlock_mount(mp);\nout:\n\tpath_put(&old_path);\n\treturn err;\n}\n\nstatic int change_mount_flags(struct vfsmount *mnt, int ms_flags)\n{\n\tint error = 0;\n\tint readonly_request = 0;\n\n\tif (ms_flags & MS_RDONLY)\n\t\treadonly_request = 1;\n\tif (readonly_request == __mnt_is_readonly(mnt))\n\t\treturn 0;\n\n\tif (readonly_request)\n\t\terror = mnt_make_readonly(real_mount(mnt));\n\telse\n\t\t__mnt_unmake_readonly(real_mount(mnt));\n\treturn error;\n}\n\n/*\n * change filesystem flags. dir should be a physical root of filesystem.\n * If you've mounted a non-root directory somewhere and want to do remount\n * on it - tough luck.\n */\nstatic int do_remount(struct path *path, int flags, int mnt_flags,\n\t\t      void *data)\n{\n\tint err;\n\tstruct super_block *sb = path->mnt->mnt_sb;\n\tstruct mount *mnt = real_mount(path->mnt);\n\n\tif (!check_mnt(mnt))\n\t\treturn -EINVAL;\n\n\tif (path->dentry != path->mnt->mnt_root)\n\t\treturn -EINVAL;\n\n\t/* Don't allow changing of locked mnt flags.\n\t *\n\t * No locks need to be held here while testing the various\n\t * MNT_LOCK flags because those flags can never be cleared\n\t * once they are set.\n\t */\n\tif ((mnt->mnt.mnt_flags & MNT_LOCK_READONLY) &&\n\t    !(mnt_flags & MNT_READONLY)) {\n\t\treturn -EPERM;\n\t}\n\terr = security_sb_remount(sb, data);\n\tif (err)\n\t\treturn err;\n\n\tdown_write(&sb->s_umount);\n\tif (flags & MS_BIND)\n\t\terr = change_mount_flags(path->mnt, flags);\n\telse if (!capable(CAP_SYS_ADMIN))\n\t\terr = -EPERM;\n\telse\n\t\terr = do_remount_sb(sb, flags, data, 0);\n\tif (!err) {\n\t\tlock_mount_hash();\n\t\tmnt_flags |= mnt->mnt.mnt_flags & ~MNT_USER_SETTABLE_MASK;\n\t\tmnt->mnt.mnt_flags = mnt_flags;\n\t\ttouch_mnt_namespace(mnt->mnt_ns);\n\t\tunlock_mount_hash();\n\t}\n\tup_write(&sb->s_umount);\n\treturn err;\n}\n\nstatic inline int tree_contains_unbindable(struct mount *mnt)\n{\n\tstruct mount *p;\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tif (IS_MNT_UNBINDABLE(p))\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int do_move_mount(struct path *path, const char *old_name)\n{\n\tstruct path old_path, parent_path;\n\tstruct mount *p;\n\tstruct mount *old;\n\tstruct mountpoint *mp;\n\tint err;\n\tif (!old_name || !*old_name)\n\t\treturn -EINVAL;\n\terr = kern_path(old_name, LOOKUP_FOLLOW, &old_path);\n\tif (err)\n\t\treturn err;\n\n\tmp = lock_mount(path);\n\terr = PTR_ERR(mp);\n\tif (IS_ERR(mp))\n\t\tgoto out;\n\n\told = real_mount(old_path.mnt);\n\tp = real_mount(path->mnt);\n\n\terr = -EINVAL;\n\tif (!check_mnt(p) || !check_mnt(old))\n\t\tgoto out1;\n\n\tif (old->mnt.mnt_flags & MNT_LOCKED)\n\t\tgoto out1;\n\n\terr = -EINVAL;\n\tif (old_path.dentry != old_path.mnt->mnt_root)\n\t\tgoto out1;\n\n\tif (!mnt_has_parent(old))\n\t\tgoto out1;\n\n\tif (S_ISDIR(path->dentry->d_inode->i_mode) !=\n\t      S_ISDIR(old_path.dentry->d_inode->i_mode))\n\t\tgoto out1;\n\t/*\n\t * Don't move a mount residing in a shared parent.\n\t */\n\tif (IS_MNT_SHARED(old->mnt_parent))\n\t\tgoto out1;\n\t/*\n\t * Don't move a mount tree containing unbindable mounts to a destination\n\t * mount which is shared.\n\t */\n\tif (IS_MNT_SHARED(p) && tree_contains_unbindable(old))\n\t\tgoto out1;\n\terr = -ELOOP;\n\tfor (; mnt_has_parent(p); p = p->mnt_parent)\n\t\tif (p == old)\n\t\t\tgoto out1;\n\n\terr = attach_recursive_mnt(old, real_mount(path->mnt), mp, &parent_path);\n\tif (err)\n\t\tgoto out1;\n\n\t/* if the mount is moved, it should no longer be expire\n\t * automatically */\n\tlist_del_init(&old->mnt_expire);\nout1:\n\tunlock_mount(mp);\nout:\n\tif (!err)\n\t\tpath_put(&parent_path);\n\tpath_put(&old_path);\n\treturn err;\n}\n\nstatic struct vfsmount *fs_set_subtype(struct vfsmount *mnt, const char *fstype)\n{\n\tint err;\n\tconst char *subtype = strchr(fstype, '.');\n\tif (subtype) {\n\t\tsubtype++;\n\t\terr = -EINVAL;\n\t\tif (!subtype[0])\n\t\t\tgoto err;\n\t} else\n\t\tsubtype = \"\";\n\n\tmnt->mnt_sb->s_subtype = kstrdup(subtype, GFP_KERNEL);\n\terr = -ENOMEM;\n\tif (!mnt->mnt_sb->s_subtype)\n\t\tgoto err;\n\treturn mnt;\n\n err:\n\tmntput(mnt);\n\treturn ERR_PTR(err);\n}\n\n/*\n * add a mount into a namespace's mount tree\n */\nstatic int do_add_mount(struct mount *newmnt, struct path *path, int mnt_flags)\n{\n\tstruct mountpoint *mp;\n\tstruct mount *parent;\n\tint err;\n\n\tmnt_flags &= ~MNT_INTERNAL_FLAGS;\n\n\tmp = lock_mount(path);\n\tif (IS_ERR(mp))\n\t\treturn PTR_ERR(mp);\n\n\tparent = real_mount(path->mnt);\n\terr = -EINVAL;\n\tif (unlikely(!check_mnt(parent))) {\n\t\t/* that's acceptable only for automounts done in private ns */\n\t\tif (!(mnt_flags & MNT_SHRINKABLE))\n\t\t\tgoto unlock;\n\t\t/* ... and for those we'd better have mountpoint still alive */\n\t\tif (!parent->mnt_ns)\n\t\t\tgoto unlock;\n\t}\n\n\t/* Refuse the same filesystem on the same mount point */\n\terr = -EBUSY;\n\tif (path->mnt->mnt_sb == newmnt->mnt.mnt_sb &&\n\t    path->mnt->mnt_root == path->dentry)\n\t\tgoto unlock;\n\n\terr = -EINVAL;\n\tif (S_ISLNK(newmnt->mnt.mnt_root->d_inode->i_mode))\n\t\tgoto unlock;\n\n\tnewmnt->mnt.mnt_flags = mnt_flags;\n\terr = graft_tree(newmnt, parent, mp);\n\nunlock:\n\tunlock_mount(mp);\n\treturn err;\n}\n\n/*\n * create a new mount for userspace and request it to be added into the\n * namespace's tree\n */\nstatic int do_new_mount(struct path *path, const char *fstype, int flags,\n\t\t\tint mnt_flags, const char *name, void *data)\n{\n\tstruct file_system_type *type;\n\tstruct user_namespace *user_ns = current->nsproxy->mnt_ns->user_ns;\n\tstruct vfsmount *mnt;\n\tint err;\n\n\tif (!fstype)\n\t\treturn -EINVAL;\n\n\ttype = get_fs_type(fstype);\n\tif (!type)\n\t\treturn -ENODEV;\n\n\tif (user_ns != &init_user_ns) {\n\t\tif (!(type->fs_flags & FS_USERNS_MOUNT)) {\n\t\t\tput_filesystem(type);\n\t\t\treturn -EPERM;\n\t\t}\n\t\t/* Only in special cases allow devices from mounts\n\t\t * created outside the initial user namespace.\n\t\t */\n\t\tif (!(type->fs_flags & FS_USERNS_DEV_MOUNT)) {\n\t\t\tflags |= MS_NODEV;\n\t\t\tmnt_flags |= MNT_NODEV;\n\t\t}\n\t}\n\n\tmnt = vfs_kern_mount(type, flags, name, data);\n\tif (!IS_ERR(mnt) && (type->fs_flags & FS_HAS_SUBTYPE) &&\n\t    !mnt->mnt_sb->s_subtype)\n\t\tmnt = fs_set_subtype(mnt, fstype);\n\n\tput_filesystem(type);\n\tif (IS_ERR(mnt))\n\t\treturn PTR_ERR(mnt);\n\n\terr = do_add_mount(real_mount(mnt), path, mnt_flags);\n\tif (err)\n\t\tmntput(mnt);\n\treturn err;\n}\n\nint finish_automount(struct vfsmount *m, struct path *path)\n{\n\tstruct mount *mnt = real_mount(m);\n\tint err;\n\t/* The new mount record should have at least 2 refs to prevent it being\n\t * expired before we get a chance to add it\n\t */\n\tBUG_ON(mnt_get_count(mnt) < 2);\n\n\tif (m->mnt_sb == path->mnt->mnt_sb &&\n\t    m->mnt_root == path->dentry) {\n\t\terr = -ELOOP;\n\t\tgoto fail;\n\t}\n\n\terr = do_add_mount(mnt, path, path->mnt->mnt_flags | MNT_SHRINKABLE);\n\tif (!err)\n\t\treturn 0;\nfail:\n\t/* remove m from any expiration list it may be on */\n\tif (!list_empty(&mnt->mnt_expire)) {\n\t\tnamespace_lock();\n\t\tlist_del_init(&mnt->mnt_expire);\n\t\tnamespace_unlock();\n\t}\n\tmntput(m);\n\tmntput(m);\n\treturn err;\n}\n\n/**\n * mnt_set_expiry - Put a mount on an expiration list\n * @mnt: The mount to list.\n * @expiry_list: The list to add the mount to.\n */\nvoid mnt_set_expiry(struct vfsmount *mnt, struct list_head *expiry_list)\n{\n\tnamespace_lock();\n\n\tlist_add_tail(&real_mount(mnt)->mnt_expire, expiry_list);\n\n\tnamespace_unlock();\n}\nEXPORT_SYMBOL(mnt_set_expiry);\n\n/*\n * process a list of expirable mountpoints with the intent of discarding any\n * mountpoints that aren't in use and haven't been touched since last we came\n * here\n */\nvoid mark_mounts_for_expiry(struct list_head *mounts)\n{\n\tstruct mount *mnt, *next;\n\tLIST_HEAD(graveyard);\n\n\tif (list_empty(mounts))\n\t\treturn;\n\n\tnamespace_lock();\n\tlock_mount_hash();\n\n\t/* extract from the expiration list every vfsmount that matches the\n\t * following criteria:\n\t * - only referenced by its parent vfsmount\n\t * - still marked for expiry (marked on the last call here; marks are\n\t *   cleared by mntput())\n\t */\n\tlist_for_each_entry_safe(mnt, next, mounts, mnt_expire) {\n\t\tif (!xchg(&mnt->mnt_expiry_mark, 1) ||\n\t\t\tpropagate_mount_busy(mnt, 1))\n\t\t\tcontinue;\n\t\tlist_move(&mnt->mnt_expire, &graveyard);\n\t}\n\twhile (!list_empty(&graveyard)) {\n\t\tmnt = list_first_entry(&graveyard, struct mount, mnt_expire);\n\t\ttouch_mnt_namespace(mnt->mnt_ns);\n\t\tumount_tree(mnt, 1);\n\t}\n\tunlock_mount_hash();\n\tnamespace_unlock();\n}\n\nEXPORT_SYMBOL_GPL(mark_mounts_for_expiry);\n\n/*\n * Ripoff of 'select_parent()'\n *\n * search the list of submounts for a given mountpoint, and move any\n * shrinkable submounts to the 'graveyard' list.\n */\nstatic int select_submounts(struct mount *parent, struct list_head *graveyard)\n{\n\tstruct mount *this_parent = parent;\n\tstruct list_head *next;\n\tint found = 0;\n\nrepeat:\n\tnext = this_parent->mnt_mounts.next;\nresume:\n\twhile (next != &this_parent->mnt_mounts) {\n\t\tstruct list_head *tmp = next;\n\t\tstruct mount *mnt = list_entry(tmp, struct mount, mnt_child);\n\n\t\tnext = tmp->next;\n\t\tif (!(mnt->mnt.mnt_flags & MNT_SHRINKABLE))\n\t\t\tcontinue;\n\t\t/*\n\t\t * Descend a level if the d_mounts list is non-empty.\n\t\t */\n\t\tif (!list_empty(&mnt->mnt_mounts)) {\n\t\t\tthis_parent = mnt;\n\t\t\tgoto repeat;\n\t\t}\n\n\t\tif (!propagate_mount_busy(mnt, 1)) {\n\t\t\tlist_move_tail(&mnt->mnt_expire, graveyard);\n\t\t\tfound++;\n\t\t}\n\t}\n\t/*\n\t * All done at this level ... ascend and resume the search\n\t */\n\tif (this_parent != parent) {\n\t\tnext = this_parent->mnt_child.next;\n\t\tthis_parent = this_parent->mnt_parent;\n\t\tgoto resume;\n\t}\n\treturn found;\n}\n\n/*\n * process a list of expirable mountpoints with the intent of discarding any\n * submounts of a specific parent mountpoint\n *\n * mount_lock must be held for write\n */\nstatic void shrink_submounts(struct mount *mnt)\n{\n\tLIST_HEAD(graveyard);\n\tstruct mount *m;\n\n\t/* extract submounts of 'mountpoint' from the expiration list */\n\twhile (select_submounts(mnt, &graveyard)) {\n\t\twhile (!list_empty(&graveyard)) {\n\t\t\tm = list_first_entry(&graveyard, struct mount,\n\t\t\t\t\t\tmnt_expire);\n\t\t\ttouch_mnt_namespace(m->mnt_ns);\n\t\t\tumount_tree(m, 1);\n\t\t}\n\t}\n}\n\n/*\n * Some copy_from_user() implementations do not return the exact number of\n * bytes remaining to copy on a fault.  But copy_mount_options() requires that.\n * Note that this function differs from copy_from_user() in that it will oops\n * on bad values of `to', rather than returning a short copy.\n */\nstatic long exact_copy_from_user(void *to, const void __user * from,\n\t\t\t\t unsigned long n)\n{\n\tchar *t = to;\n\tconst char __user *f = from;\n\tchar c;\n\n\tif (!access_ok(VERIFY_READ, from, n))\n\t\treturn n;\n\n\twhile (n) {\n\t\tif (__get_user(c, f)) {\n\t\t\tmemset(t, 0, n);\n\t\t\tbreak;\n\t\t}\n\t\t*t++ = c;\n\t\tf++;\n\t\tn--;\n\t}\n\treturn n;\n}\n\nint copy_mount_options(const void __user * data, unsigned long *where)\n{\n\tint i;\n\tunsigned long page;\n\tunsigned long size;\n\n\t*where = 0;\n\tif (!data)\n\t\treturn 0;\n\n\tif (!(page = __get_free_page(GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\n\t/* We only care that *some* data at the address the user\n\t * gave us is valid.  Just in case, we'll zero\n\t * the remainder of the page.\n\t */\n\t/* copy_from_user cannot cross TASK_SIZE ! */\n\tsize = TASK_SIZE - (unsigned long)data;\n\tif (size > PAGE_SIZE)\n\t\tsize = PAGE_SIZE;\n\n\ti = size - exact_copy_from_user((void *)page, data, size);\n\tif (!i) {\n\t\tfree_page(page);\n\t\treturn -EFAULT;\n\t}\n\tif (i != PAGE_SIZE)\n\t\tmemset((char *)page + i, 0, PAGE_SIZE - i);\n\t*where = page;\n\treturn 0;\n}\n\nint copy_mount_string(const void __user *data, char **where)\n{\n\tchar *tmp;\n\n\tif (!data) {\n\t\t*where = NULL;\n\t\treturn 0;\n\t}\n\n\ttmp = strndup_user(data, PAGE_SIZE);\n\tif (IS_ERR(tmp))\n\t\treturn PTR_ERR(tmp);\n\n\t*where = tmp;\n\treturn 0;\n}\n\n/*\n * Flags is a 32-bit value that allows up to 31 non-fs dependent flags to\n * be given to the mount() call (ie: read-only, no-dev, no-suid etc).\n *\n * data is a (void *) that can point to any structure up to\n * PAGE_SIZE-1 bytes, which can contain arbitrary fs-dependent\n * information (or be NULL).\n *\n * Pre-0.97 versions of mount() didn't have a flags word.\n * When the flags word was introduced its top half was required\n * to have the magic value 0xC0ED, and this remained so until 2.4.0-test9.\n * Therefore, if this magic number is present, it carries no information\n * and must be discarded.\n */\nlong do_mount(const char *dev_name, const char *dir_name,\n\t\tconst char *type_page, unsigned long flags, void *data_page)\n{\n\tstruct path path;\n\tint retval = 0;\n\tint mnt_flags = 0;\n\n\t/* Discard magic */\n\tif ((flags & MS_MGC_MSK) == MS_MGC_VAL)\n\t\tflags &= ~MS_MGC_MSK;\n\n\t/* Basic sanity checks */\n\n\tif (!dir_name || !*dir_name || !memchr(dir_name, 0, PAGE_SIZE))\n\t\treturn -EINVAL;\n\n\tif (data_page)\n\t\t((char *)data_page)[PAGE_SIZE - 1] = 0;\n\n\t/* ... and get the mountpoint */\n\tretval = kern_path(dir_name, LOOKUP_FOLLOW, &path);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = security_sb_mount(dev_name, &path,\n\t\t\t\t   type_page, flags, data_page);\n\tif (!retval && !may_mount())\n\t\tretval = -EPERM;\n\tif (retval)\n\t\tgoto dput_out;\n\n\t/* Default to relatime unless overriden */\n\tif (!(flags & MS_NOATIME))\n\t\tmnt_flags |= MNT_RELATIME;\n\n\t/* Separate the per-mountpoint flags */\n\tif (flags & MS_NOSUID)\n\t\tmnt_flags |= MNT_NOSUID;\n\tif (flags & MS_NODEV)\n\t\tmnt_flags |= MNT_NODEV;\n\tif (flags & MS_NOEXEC)\n\t\tmnt_flags |= MNT_NOEXEC;\n\tif (flags & MS_NOATIME)\n\t\tmnt_flags |= MNT_NOATIME;\n\tif (flags & MS_NODIRATIME)\n\t\tmnt_flags |= MNT_NODIRATIME;\n\tif (flags & MS_STRICTATIME)\n\t\tmnt_flags &= ~(MNT_RELATIME | MNT_NOATIME);\n\tif (flags & MS_RDONLY)\n\t\tmnt_flags |= MNT_READONLY;\n\n\tflags &= ~(MS_NOSUID | MS_NOEXEC | MS_NODEV | MS_ACTIVE | MS_BORN |\n\t\t   MS_NOATIME | MS_NODIRATIME | MS_RELATIME| MS_KERNMOUNT |\n\t\t   MS_STRICTATIME);\n\n\tif (flags & MS_REMOUNT)\n\t\tretval = do_remount(&path, flags & ~MS_REMOUNT, mnt_flags,\n\t\t\t\t    data_page);\n\telse if (flags & MS_BIND)\n\t\tretval = do_loopback(&path, dev_name, flags & MS_REC);\n\telse if (flags & (MS_SHARED | MS_PRIVATE | MS_SLAVE | MS_UNBINDABLE))\n\t\tretval = do_change_type(&path, flags);\n\telse if (flags & MS_MOVE)\n\t\tretval = do_move_mount(&path, dev_name);\n\telse\n\t\tretval = do_new_mount(&path, type_page, flags, mnt_flags,\n\t\t\t\t      dev_name, data_page);\ndput_out:\n\tpath_put(&path);\n\treturn retval;\n}\n\nstatic void free_mnt_ns(struct mnt_namespace *ns)\n{\n\tproc_free_inum(ns->proc_inum);\n\tput_user_ns(ns->user_ns);\n\tkfree(ns);\n}\n\n/*\n * Assign a sequence number so we can detect when we attempt to bind\n * mount a reference to an older mount namespace into the current\n * mount namespace, preventing reference counting loops.  A 64bit\n * number incrementing at 10Ghz will take 12,427 years to wrap which\n * is effectively never, so we can ignore the possibility.\n */\nstatic atomic64_t mnt_ns_seq = ATOMIC64_INIT(1);\n\nstatic struct mnt_namespace *alloc_mnt_ns(struct user_namespace *user_ns)\n{\n\tstruct mnt_namespace *new_ns;\n\tint ret;\n\n\tnew_ns = kmalloc(sizeof(struct mnt_namespace), GFP_KERNEL);\n\tif (!new_ns)\n\t\treturn ERR_PTR(-ENOMEM);\n\tret = proc_alloc_inum(&new_ns->proc_inum);\n\tif (ret) {\n\t\tkfree(new_ns);\n\t\treturn ERR_PTR(ret);\n\t}\n\tnew_ns->seq = atomic64_add_return(1, &mnt_ns_seq);\n\tatomic_set(&new_ns->count, 1);\n\tnew_ns->root = NULL;\n\tINIT_LIST_HEAD(&new_ns->list);\n\tinit_waitqueue_head(&new_ns->poll);\n\tnew_ns->event = 0;\n\tnew_ns->user_ns = get_user_ns(user_ns);\n\treturn new_ns;\n}\n\nstruct mnt_namespace *copy_mnt_ns(unsigned long flags, struct mnt_namespace *ns,\n\t\tstruct user_namespace *user_ns, struct fs_struct *new_fs)\n{\n\tstruct mnt_namespace *new_ns;\n\tstruct vfsmount *rootmnt = NULL, *pwdmnt = NULL;\n\tstruct mount *p, *q;\n\tstruct mount *old;\n\tstruct mount *new;\n\tint copy_flags;\n\n\tBUG_ON(!ns);\n\n\tif (likely(!(flags & CLONE_NEWNS))) {\n\t\tget_mnt_ns(ns);\n\t\treturn ns;\n\t}\n\n\told = ns->root;\n\n\tnew_ns = alloc_mnt_ns(user_ns);\n\tif (IS_ERR(new_ns))\n\t\treturn new_ns;\n\n\tnamespace_lock();\n\t/* First pass: copy the tree topology */\n\tcopy_flags = CL_COPY_UNBINDABLE | CL_EXPIRE;\n\tif (user_ns != ns->user_ns)\n\t\tcopy_flags |= CL_SHARED_TO_SLAVE | CL_UNPRIVILEGED;\n\tnew = copy_tree(old, old->mnt.mnt_root, copy_flags);\n\tif (IS_ERR(new)) {\n\t\tnamespace_unlock();\n\t\tfree_mnt_ns(new_ns);\n\t\treturn ERR_CAST(new);\n\t}\n\tnew_ns->root = new;\n\tlist_add_tail(&new_ns->list, &new->mnt_list);\n\n\t/*\n\t * Second pass: switch the tsk->fs->* elements and mark new vfsmounts\n\t * as belonging to new namespace.  We have already acquired a private\n\t * fs_struct, so tsk->fs->lock is not needed.\n\t */\n\tp = old;\n\tq = new;\n\twhile (p) {\n\t\tq->mnt_ns = new_ns;\n\t\tif (new_fs) {\n\t\t\tif (&p->mnt == new_fs->root.mnt) {\n\t\t\t\tnew_fs->root.mnt = mntget(&q->mnt);\n\t\t\t\trootmnt = &p->mnt;\n\t\t\t}\n\t\t\tif (&p->mnt == new_fs->pwd.mnt) {\n\t\t\t\tnew_fs->pwd.mnt = mntget(&q->mnt);\n\t\t\t\tpwdmnt = &p->mnt;\n\t\t\t}\n\t\t}\n\t\tp = next_mnt(p, old);\n\t\tq = next_mnt(q, new);\n\t\tif (!q)\n\t\t\tbreak;\n\t\twhile (p->mnt.mnt_root != q->mnt.mnt_root)\n\t\t\tp = next_mnt(p, old);\n\t}\n\tnamespace_unlock();\n\n\tif (rootmnt)\n\t\tmntput(rootmnt);\n\tif (pwdmnt)\n\t\tmntput(pwdmnt);\n\n\treturn new_ns;\n}\n\n/**\n * create_mnt_ns - creates a private namespace and adds a root filesystem\n * @mnt: pointer to the new root filesystem mountpoint\n */\nstatic struct mnt_namespace *create_mnt_ns(struct vfsmount *m)\n{\n\tstruct mnt_namespace *new_ns = alloc_mnt_ns(&init_user_ns);\n\tif (!IS_ERR(new_ns)) {\n\t\tstruct mount *mnt = real_mount(m);\n\t\tmnt->mnt_ns = new_ns;\n\t\tnew_ns->root = mnt;\n\t\tlist_add(&mnt->mnt_list, &new_ns->list);\n\t} else {\n\t\tmntput(m);\n\t}\n\treturn new_ns;\n}\n\nstruct dentry *mount_subtree(struct vfsmount *mnt, const char *name)\n{\n\tstruct mnt_namespace *ns;\n\tstruct super_block *s;\n\tstruct path path;\n\tint err;\n\n\tns = create_mnt_ns(mnt);\n\tif (IS_ERR(ns))\n\t\treturn ERR_CAST(ns);\n\n\terr = vfs_path_lookup(mnt->mnt_root, mnt,\n\t\t\tname, LOOKUP_FOLLOW|LOOKUP_AUTOMOUNT, &path);\n\n\tput_mnt_ns(ns);\n\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\t/* trade a vfsmount reference for active sb one */\n\ts = path.mnt->mnt_sb;\n\tatomic_inc(&s->s_active);\n\tmntput(path.mnt);\n\t/* lock the sucker */\n\tdown_write(&s->s_umount);\n\t/* ... and return the root of (sub)tree on it */\n\treturn path.dentry;\n}\nEXPORT_SYMBOL(mount_subtree);\n\nSYSCALL_DEFINE5(mount, char __user *, dev_name, char __user *, dir_name,\n\t\tchar __user *, type, unsigned long, flags, void __user *, data)\n{\n\tint ret;\n\tchar *kernel_type;\n\tstruct filename *kernel_dir;\n\tchar *kernel_dev;\n\tunsigned long data_page;\n\n\tret = copy_mount_string(type, &kernel_type);\n\tif (ret < 0)\n\t\tgoto out_type;\n\n\tkernel_dir = getname(dir_name);\n\tif (IS_ERR(kernel_dir)) {\n\t\tret = PTR_ERR(kernel_dir);\n\t\tgoto out_dir;\n\t}\n\n\tret = copy_mount_string(dev_name, &kernel_dev);\n\tif (ret < 0)\n\t\tgoto out_dev;\n\n\tret = copy_mount_options(data, &data_page);\n\tif (ret < 0)\n\t\tgoto out_data;\n\n\tret = do_mount(kernel_dev, kernel_dir->name, kernel_type, flags,\n\t\t(void *) data_page);\n\n\tfree_page(data_page);\nout_data:\n\tkfree(kernel_dev);\nout_dev:\n\tputname(kernel_dir);\nout_dir:\n\tkfree(kernel_type);\nout_type:\n\treturn ret;\n}\n\n/*\n * Return true if path is reachable from root\n *\n * namespace_sem or mount_lock is held\n */\nbool is_path_reachable(struct mount *mnt, struct dentry *dentry,\n\t\t\t const struct path *root)\n{\n\twhile (&mnt->mnt != root->mnt && mnt_has_parent(mnt)) {\n\t\tdentry = mnt->mnt_mountpoint;\n\t\tmnt = mnt->mnt_parent;\n\t}\n\treturn &mnt->mnt == root->mnt && is_subdir(dentry, root->dentry);\n}\n\nint path_is_under(struct path *path1, struct path *path2)\n{\n\tint res;\n\tread_seqlock_excl(&mount_lock);\n\tres = is_path_reachable(real_mount(path1->mnt), path1->dentry, path2);\n\tread_sequnlock_excl(&mount_lock);\n\treturn res;\n}\nEXPORT_SYMBOL(path_is_under);\n\n/*\n * pivot_root Semantics:\n * Moves the root file system of the current process to the directory put_old,\n * makes new_root as the new root file system of the current process, and sets\n * root/cwd of all processes which had them on the current root to new_root.\n *\n * Restrictions:\n * The new_root and put_old must be directories, and  must not be on the\n * same file  system as the current process root. The put_old  must  be\n * underneath new_root,  i.e. adding a non-zero number of /.. to the string\n * pointed to by put_old must yield the same directory as new_root. No other\n * file system may be mounted on put_old. After all, new_root is a mountpoint.\n *\n * Also, the current root cannot be on the 'rootfs' (initial ramfs) filesystem.\n * See Documentation/filesystems/ramfs-rootfs-initramfs.txt for alternatives\n * in this situation.\n *\n * Notes:\n *  - we don't move root/cwd if they are not at the root (reason: if something\n *    cared enough to change them, it's probably wrong to force them elsewhere)\n *  - it's okay to pick a root that isn't the root of a file system, e.g.\n *    /nfs/my_root where /nfs is the mount point. It must be a mountpoint,\n *    though, so you may need to say mount --bind /nfs/my_root /nfs/my_root\n *    first.\n */\nSYSCALL_DEFINE2(pivot_root, const char __user *, new_root,\n\t\tconst char __user *, put_old)\n{\n\tstruct path new, old, parent_path, root_parent, root;\n\tstruct mount *new_mnt, *root_mnt, *old_mnt;\n\tstruct mountpoint *old_mp, *root_mp;\n\tint error;\n\n\tif (!may_mount())\n\t\treturn -EPERM;\n\n\terror = user_path_dir(new_root, &new);\n\tif (error)\n\t\tgoto out0;\n\n\terror = user_path_dir(put_old, &old);\n\tif (error)\n\t\tgoto out1;\n\n\terror = security_sb_pivotroot(&old, &new);\n\tif (error)\n\t\tgoto out2;\n\n\tget_fs_root(current->fs, &root);\n\told_mp = lock_mount(&old);\n\terror = PTR_ERR(old_mp);\n\tif (IS_ERR(old_mp))\n\t\tgoto out3;\n\n\terror = -EINVAL;\n\tnew_mnt = real_mount(new.mnt);\n\troot_mnt = real_mount(root.mnt);\n\told_mnt = real_mount(old.mnt);\n\tif (IS_MNT_SHARED(old_mnt) ||\n\t\tIS_MNT_SHARED(new_mnt->mnt_parent) ||\n\t\tIS_MNT_SHARED(root_mnt->mnt_parent))\n\t\tgoto out4;\n\tif (!check_mnt(root_mnt) || !check_mnt(new_mnt))\n\t\tgoto out4;\n\tif (new_mnt->mnt.mnt_flags & MNT_LOCKED)\n\t\tgoto out4;\n\terror = -ENOENT;\n\tif (d_unlinked(new.dentry))\n\t\tgoto out4;\n\terror = -EBUSY;\n\tif (new_mnt == root_mnt || old_mnt == root_mnt)\n\t\tgoto out4; /* loop, on the same file system  */\n\terror = -EINVAL;\n\tif (root.mnt->mnt_root != root.dentry)\n\t\tgoto out4; /* not a mountpoint */\n\tif (!mnt_has_parent(root_mnt))\n\t\tgoto out4; /* not attached */\n\troot_mp = root_mnt->mnt_mp;\n\tif (new.mnt->mnt_root != new.dentry)\n\t\tgoto out4; /* not a mountpoint */\n\tif (!mnt_has_parent(new_mnt))\n\t\tgoto out4; /* not attached */\n\t/* make sure we can reach put_old from new_root */\n\tif (!is_path_reachable(old_mnt, old.dentry, &new))\n\t\tgoto out4;\n\troot_mp->m_count++; /* pin it so it won't go away */\n\tlock_mount_hash();\n\tdetach_mnt(new_mnt, &parent_path);\n\tdetach_mnt(root_mnt, &root_parent);\n\tif (root_mnt->mnt.mnt_flags & MNT_LOCKED) {\n\t\tnew_mnt->mnt.mnt_flags |= MNT_LOCKED;\n\t\troot_mnt->mnt.mnt_flags &= ~MNT_LOCKED;\n\t}\n\t/* mount old root on put_old */\n\tattach_mnt(root_mnt, old_mnt, old_mp);\n\t/* mount new_root on / */\n\tattach_mnt(new_mnt, real_mount(root_parent.mnt), root_mp);\n\ttouch_mnt_namespace(current->nsproxy->mnt_ns);\n\tunlock_mount_hash();\n\tchroot_fs_refs(&root, &new);\n\tput_mountpoint(root_mp);\n\terror = 0;\nout4:\n\tunlock_mount(old_mp);\n\tif (!error) {\n\t\tpath_put(&root_parent);\n\t\tpath_put(&parent_path);\n\t}\nout3:\n\tpath_put(&root);\nout2:\n\tpath_put(&old);\nout1:\n\tpath_put(&new);\nout0:\n\treturn error;\n}\n\nstatic void __init init_mount_tree(void)\n{\n\tstruct vfsmount *mnt;\n\tstruct mnt_namespace *ns;\n\tstruct path root;\n\tstruct file_system_type *type;\n\n\ttype = get_fs_type(\"rootfs\");\n\tif (!type)\n\t\tpanic(\"Can't find rootfs type\");\n\tmnt = vfs_kern_mount(type, 0, \"rootfs\", NULL);\n\tput_filesystem(type);\n\tif (IS_ERR(mnt))\n\t\tpanic(\"Can't create rootfs\");\n\n\tns = create_mnt_ns(mnt);\n\tif (IS_ERR(ns))\n\t\tpanic(\"Can't allocate initial namespace\");\n\n\tinit_task.nsproxy->mnt_ns = ns;\n\tget_mnt_ns(ns);\n\n\troot.mnt = mnt;\n\troot.dentry = mnt->mnt_root;\n\n\tset_fs_pwd(current->fs, &root);\n\tset_fs_root(current->fs, &root);\n}\n\nvoid __init mnt_init(void)\n{\n\tunsigned u;\n\tint err;\n\n\tmnt_cache = kmem_cache_create(\"mnt_cache\", sizeof(struct mount),\n\t\t\t0, SLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL);\n\n\tmount_hashtable = alloc_large_system_hash(\"Mount-cache\",\n\t\t\t\tsizeof(struct hlist_head),\n\t\t\t\tmhash_entries, 19,\n\t\t\t\t0,\n\t\t\t\t&m_hash_shift, &m_hash_mask, 0, 0);\n\tmountpoint_hashtable = alloc_large_system_hash(\"Mountpoint-cache\",\n\t\t\t\tsizeof(struct hlist_head),\n\t\t\t\tmphash_entries, 19,\n\t\t\t\t0,\n\t\t\t\t&mp_hash_shift, &mp_hash_mask, 0, 0);\n\n\tif (!mount_hashtable || !mountpoint_hashtable)\n\t\tpanic(\"Failed to allocate mount hash table\\n\");\n\n\tfor (u = 0; u <= m_hash_mask; u++)\n\t\tINIT_HLIST_HEAD(&mount_hashtable[u]);\n\tfor (u = 0; u <= mp_hash_mask; u++)\n\t\tINIT_HLIST_HEAD(&mountpoint_hashtable[u]);\n\n\tkernfs_init();\n\n\terr = sysfs_init();\n\tif (err)\n\t\tprintk(KERN_WARNING \"%s: sysfs_init error: %d\\n\",\n\t\t\t__func__, err);\n\tfs_kobj = kobject_create_and_add(\"fs\", NULL);\n\tif (!fs_kobj)\n\t\tprintk(KERN_WARNING \"%s: kobj create error\\n\", __func__);\n\tinit_rootfs();\n\tinit_mount_tree();\n}\n\nvoid put_mnt_ns(struct mnt_namespace *ns)\n{\n\tif (!atomic_dec_and_test(&ns->count))\n\t\treturn;\n\tdrop_collected_mounts(&ns->root->mnt);\n\tfree_mnt_ns(ns);\n}\n\nstruct vfsmount *kern_mount_data(struct file_system_type *type, void *data)\n{\n\tstruct vfsmount *mnt;\n\tmnt = vfs_kern_mount(type, MS_KERNMOUNT, type->name, data);\n\tif (!IS_ERR(mnt)) {\n\t\t/*\n\t\t * it is a longterm mount, don't release mnt until\n\t\t * we unmount before file sys is unregistered\n\t\t*/\n\t\treal_mount(mnt)->mnt_ns = MNT_NS_INTERNAL;\n\t}\n\treturn mnt;\n}\nEXPORT_SYMBOL_GPL(kern_mount_data);\n\nvoid kern_unmount(struct vfsmount *mnt)\n{\n\t/* release long term mount so mount point can be released */\n\tif (!IS_ERR_OR_NULL(mnt)) {\n\t\treal_mount(mnt)->mnt_ns = NULL;\n\t\tsynchronize_rcu();\t/* yecchhh... */\n\t\tmntput(mnt);\n\t}\n}\nEXPORT_SYMBOL(kern_unmount);\n\nbool our_mnt(struct vfsmount *mnt)\n{\n\treturn check_mnt(real_mount(mnt));\n}\n\nbool current_chrooted(void)\n{\n\t/* Does the current process have a non-standard root */\n\tstruct path ns_root;\n\tstruct path fs_root;\n\tbool chrooted;\n\n\t/* Find the namespace root */\n\tns_root.mnt = &current->nsproxy->mnt_ns->root->mnt;\n\tns_root.dentry = ns_root.mnt->mnt_root;\n\tpath_get(&ns_root);\n\twhile (d_mountpoint(ns_root.dentry) && follow_down_one(&ns_root))\n\t\t;\n\n\tget_fs_root(current->fs, &fs_root);\n\n\tchrooted = !path_equal(&fs_root, &ns_root);\n\n\tpath_put(&fs_root);\n\tpath_put(&ns_root);\n\n\treturn chrooted;\n}\n\nbool fs_fully_visible(struct file_system_type *type)\n{\n\tstruct mnt_namespace *ns = current->nsproxy->mnt_ns;\n\tstruct mount *mnt;\n\tbool visible = false;\n\n\tif (unlikely(!ns))\n\t\treturn false;\n\n\tdown_read(&namespace_sem);\n\tlist_for_each_entry(mnt, &ns->list, mnt_list) {\n\t\tstruct mount *child;\n\t\tif (mnt->mnt.mnt_sb->s_type != type)\n\t\t\tcontinue;\n\n\t\t/* This mount is not fully visible if there are any child mounts\n\t\t * that cover anything except for empty directories.\n\t\t */\n\t\tlist_for_each_entry(child, &mnt->mnt_mounts, mnt_child) {\n\t\t\tstruct inode *inode = child->mnt_mountpoint->d_inode;\n\t\t\tif (!S_ISDIR(inode->i_mode))\n\t\t\t\tgoto next;\n\t\t\tif (inode->i_nlink > 2)\n\t\t\t\tgoto next;\n\t\t}\n\t\tvisible = true;\n\t\tgoto found;\n\tnext:\t;\n\t}\nfound:\n\tup_read(&namespace_sem);\n\treturn visible;\n}\n\nstatic void *mntns_get(struct task_struct *task)\n{\n\tstruct mnt_namespace *ns = NULL;\n\tstruct nsproxy *nsproxy;\n\n\ttask_lock(task);\n\tnsproxy = task->nsproxy;\n\tif (nsproxy) {\n\t\tns = nsproxy->mnt_ns;\n\t\tget_mnt_ns(ns);\n\t}\n\ttask_unlock(task);\n\n\treturn ns;\n}\n\nstatic void mntns_put(void *ns)\n{\n\tput_mnt_ns(ns);\n}\n\nstatic int mntns_install(struct nsproxy *nsproxy, void *ns)\n{\n\tstruct fs_struct *fs = current->fs;\n\tstruct mnt_namespace *mnt_ns = ns;\n\tstruct path root;\n\n\tif (!ns_capable(mnt_ns->user_ns, CAP_SYS_ADMIN) ||\n\t    !ns_capable(current_user_ns(), CAP_SYS_CHROOT) ||\n\t    !ns_capable(current_user_ns(), CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (fs->users != 1)\n\t\treturn -EINVAL;\n\n\tget_mnt_ns(mnt_ns);\n\tput_mnt_ns(nsproxy->mnt_ns);\n\tnsproxy->mnt_ns = mnt_ns;\n\n\t/* Find the root */\n\troot.mnt    = &mnt_ns->root->mnt;\n\troot.dentry = mnt_ns->root->mnt.mnt_root;\n\tpath_get(&root);\n\twhile(d_mountpoint(root.dentry) && follow_down_one(&root))\n\t\t;\n\n\t/* Update the pwd and root */\n\tset_fs_pwd(fs, &root);\n\tset_fs_root(fs, &root);\n\n\tpath_put(&root);\n\treturn 0;\n}\n\nstatic unsigned int mntns_inum(void *ns)\n{\n\tstruct mnt_namespace *mnt_ns = ns;\n\treturn mnt_ns->proc_inum;\n}\n\nconst struct proc_ns_operations mntns_operations = {\n\t.name\t\t= \"mnt\",\n\t.type\t\t= CLONE_NEWNS,\n\t.get\t\t= mntns_get,\n\t.put\t\t= mntns_put,\n\t.install\t= mntns_install,\n\t.inum\t\t= mntns_inum,\n};\n", "/*\n *\n * Definitions for mount interface. This describes the in the kernel build \n * linkedlist with mounted filesystems.\n *\n * Author:  Marco van Wieringen <mvw@planets.elm.net>\n *\n */\n#ifndef _LINUX_MOUNT_H\n#define _LINUX_MOUNT_H\n\n#include <linux/types.h>\n#include <linux/list.h>\n#include <linux/nodemask.h>\n#include <linux/spinlock.h>\n#include <linux/seqlock.h>\n#include <linux/atomic.h>\n\nstruct super_block;\nstruct vfsmount;\nstruct dentry;\nstruct mnt_namespace;\n\n#define MNT_NOSUID\t0x01\n#define MNT_NODEV\t0x02\n#define MNT_NOEXEC\t0x04\n#define MNT_NOATIME\t0x08\n#define MNT_NODIRATIME\t0x10\n#define MNT_RELATIME\t0x20\n#define MNT_READONLY\t0x40\t/* does the user want this to be r/o? */\n\n#define MNT_SHRINKABLE\t0x100\n#define MNT_WRITE_HOLD\t0x200\n\n#define MNT_SHARED\t0x1000\t/* if the vfsmount is a shared mount */\n#define MNT_UNBINDABLE\t0x2000\t/* if the vfsmount is a unbindable mount */\n/*\n * MNT_SHARED_MASK is the set of flags that should be cleared when a\n * mount becomes shared.  Currently, this is only the flag that says a\n * mount cannot be bind mounted, since this is how we create a mount\n * that shares events with another mount.  If you add a new MNT_*\n * flag, consider how it interacts with shared mounts.\n */\n#define MNT_SHARED_MASK\t(MNT_UNBINDABLE)\n#define MNT_USER_SETTABLE_MASK  (MNT_NOSUID | MNT_NODEV | MNT_NOEXEC \\\n\t\t\t\t | MNT_NOATIME | MNT_NODIRATIME | MNT_RELATIME \\\n\t\t\t\t | MNT_READONLY)\n\n#define MNT_INTERNAL_FLAGS (MNT_SHARED | MNT_WRITE_HOLD | MNT_INTERNAL | \\\n\t\t\t    MNT_DOOMED | MNT_SYNC_UMOUNT | MNT_MARKED)\n\n#define MNT_INTERNAL\t0x4000\n\n#define MNT_LOCK_READONLY\t0x400000\n#define MNT_LOCKED\t\t0x800000\n#define MNT_DOOMED\t\t0x1000000\n#define MNT_SYNC_UMOUNT\t\t0x2000000\n#define MNT_MARKED\t\t0x4000000\n\nstruct vfsmount {\n\tstruct dentry *mnt_root;\t/* root of the mounted tree */\n\tstruct super_block *mnt_sb;\t/* pointer to superblock */\n\tint mnt_flags;\n};\n\nstruct file; /* forward dec */\n\nextern int mnt_want_write(struct vfsmount *mnt);\nextern int mnt_want_write_file(struct file *file);\nextern int mnt_clone_write(struct vfsmount *mnt);\nextern void mnt_drop_write(struct vfsmount *mnt);\nextern void mnt_drop_write_file(struct file *file);\nextern void mntput(struct vfsmount *mnt);\nextern struct vfsmount *mntget(struct vfsmount *mnt);\nextern void mnt_pin(struct vfsmount *mnt);\nextern void mnt_unpin(struct vfsmount *mnt);\nextern int __mnt_is_readonly(struct vfsmount *mnt);\n\nstruct file_system_type;\nextern struct vfsmount *vfs_kern_mount(struct file_system_type *type,\n\t\t\t\t      int flags, const char *name,\n\t\t\t\t      void *data);\n\nextern void mnt_set_expiry(struct vfsmount *mnt, struct list_head *expiry_list);\nextern void mark_mounts_for_expiry(struct list_head *mounts);\n\nextern dev_t name_to_dev_t(char *name);\n\n#endif /* _LINUX_MOUNT_H */\n"], "fixing_code": ["/*\n *  linux/fs/namespace.c\n *\n * (C) Copyright Al Viro 2000, 2001\n *\tReleased under GPL v2.\n *\n * Based on code from fs/super.c, copyright Linus Torvalds and others.\n * Heavily rewritten.\n */\n\n#include <linux/syscalls.h>\n#include <linux/export.h>\n#include <linux/capability.h>\n#include <linux/mnt_namespace.h>\n#include <linux/user_namespace.h>\n#include <linux/namei.h>\n#include <linux/security.h>\n#include <linux/idr.h>\n#include <linux/acct.h>\t\t/* acct_auto_close_mnt */\n#include <linux/init.h>\t\t/* init_rootfs */\n#include <linux/fs_struct.h>\t/* get_fs_root et.al. */\n#include <linux/fsnotify.h>\t/* fsnotify_vfsmount_delete */\n#include <linux/uaccess.h>\n#include <linux/proc_ns.h>\n#include <linux/magic.h>\n#include <linux/bootmem.h>\n#include \"pnode.h\"\n#include \"internal.h\"\n\nstatic unsigned int m_hash_mask __read_mostly;\nstatic unsigned int m_hash_shift __read_mostly;\nstatic unsigned int mp_hash_mask __read_mostly;\nstatic unsigned int mp_hash_shift __read_mostly;\n\nstatic __initdata unsigned long mhash_entries;\nstatic int __init set_mhash_entries(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\tmhash_entries = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"mhash_entries=\", set_mhash_entries);\n\nstatic __initdata unsigned long mphash_entries;\nstatic int __init set_mphash_entries(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\tmphash_entries = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"mphash_entries=\", set_mphash_entries);\n\nstatic u64 event;\nstatic DEFINE_IDA(mnt_id_ida);\nstatic DEFINE_IDA(mnt_group_ida);\nstatic DEFINE_SPINLOCK(mnt_id_lock);\nstatic int mnt_id_start = 0;\nstatic int mnt_group_start = 1;\n\nstatic struct hlist_head *mount_hashtable __read_mostly;\nstatic struct hlist_head *mountpoint_hashtable __read_mostly;\nstatic struct kmem_cache *mnt_cache __read_mostly;\nstatic DECLARE_RWSEM(namespace_sem);\n\n/* /sys/fs */\nstruct kobject *fs_kobj;\nEXPORT_SYMBOL_GPL(fs_kobj);\n\n/*\n * vfsmount lock may be taken for read to prevent changes to the\n * vfsmount hash, ie. during mountpoint lookups or walking back\n * up the tree.\n *\n * It should be taken for write in all cases where the vfsmount\n * tree or hash is modified or when a vfsmount structure is modified.\n */\n__cacheline_aligned_in_smp DEFINE_SEQLOCK(mount_lock);\n\nstatic inline struct hlist_head *m_hash(struct vfsmount *mnt, struct dentry *dentry)\n{\n\tunsigned long tmp = ((unsigned long)mnt / L1_CACHE_BYTES);\n\ttmp += ((unsigned long)dentry / L1_CACHE_BYTES);\n\ttmp = tmp + (tmp >> m_hash_shift);\n\treturn &mount_hashtable[tmp & m_hash_mask];\n}\n\nstatic inline struct hlist_head *mp_hash(struct dentry *dentry)\n{\n\tunsigned long tmp = ((unsigned long)dentry / L1_CACHE_BYTES);\n\ttmp = tmp + (tmp >> mp_hash_shift);\n\treturn &mountpoint_hashtable[tmp & mp_hash_mask];\n}\n\n/*\n * allocation is serialized by namespace_sem, but we need the spinlock to\n * serialize with freeing.\n */\nstatic int mnt_alloc_id(struct mount *mnt)\n{\n\tint res;\n\nretry:\n\tida_pre_get(&mnt_id_ida, GFP_KERNEL);\n\tspin_lock(&mnt_id_lock);\n\tres = ida_get_new_above(&mnt_id_ida, mnt_id_start, &mnt->mnt_id);\n\tif (!res)\n\t\tmnt_id_start = mnt->mnt_id + 1;\n\tspin_unlock(&mnt_id_lock);\n\tif (res == -EAGAIN)\n\t\tgoto retry;\n\n\treturn res;\n}\n\nstatic void mnt_free_id(struct mount *mnt)\n{\n\tint id = mnt->mnt_id;\n\tspin_lock(&mnt_id_lock);\n\tida_remove(&mnt_id_ida, id);\n\tif (mnt_id_start > id)\n\t\tmnt_id_start = id;\n\tspin_unlock(&mnt_id_lock);\n}\n\n/*\n * Allocate a new peer group ID\n *\n * mnt_group_ida is protected by namespace_sem\n */\nstatic int mnt_alloc_group_id(struct mount *mnt)\n{\n\tint res;\n\n\tif (!ida_pre_get(&mnt_group_ida, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tres = ida_get_new_above(&mnt_group_ida,\n\t\t\t\tmnt_group_start,\n\t\t\t\t&mnt->mnt_group_id);\n\tif (!res)\n\t\tmnt_group_start = mnt->mnt_group_id + 1;\n\n\treturn res;\n}\n\n/*\n * Release a peer group ID\n */\nvoid mnt_release_group_id(struct mount *mnt)\n{\n\tint id = mnt->mnt_group_id;\n\tida_remove(&mnt_group_ida, id);\n\tif (mnt_group_start > id)\n\t\tmnt_group_start = id;\n\tmnt->mnt_group_id = 0;\n}\n\n/*\n * vfsmount lock must be held for read\n */\nstatic inline void mnt_add_count(struct mount *mnt, int n)\n{\n#ifdef CONFIG_SMP\n\tthis_cpu_add(mnt->mnt_pcp->mnt_count, n);\n#else\n\tpreempt_disable();\n\tmnt->mnt_count += n;\n\tpreempt_enable();\n#endif\n}\n\n/*\n * vfsmount lock must be held for write\n */\nunsigned int mnt_get_count(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tunsigned int count = 0;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcount += per_cpu_ptr(mnt->mnt_pcp, cpu)->mnt_count;\n\t}\n\n\treturn count;\n#else\n\treturn mnt->mnt_count;\n#endif\n}\n\nstatic struct mount *alloc_vfsmnt(const char *name)\n{\n\tstruct mount *mnt = kmem_cache_zalloc(mnt_cache, GFP_KERNEL);\n\tif (mnt) {\n\t\tint err;\n\n\t\terr = mnt_alloc_id(mnt);\n\t\tif (err)\n\t\t\tgoto out_free_cache;\n\n\t\tif (name) {\n\t\t\tmnt->mnt_devname = kstrdup(name, GFP_KERNEL);\n\t\t\tif (!mnt->mnt_devname)\n\t\t\t\tgoto out_free_id;\n\t\t}\n\n#ifdef CONFIG_SMP\n\t\tmnt->mnt_pcp = alloc_percpu(struct mnt_pcp);\n\t\tif (!mnt->mnt_pcp)\n\t\t\tgoto out_free_devname;\n\n\t\tthis_cpu_add(mnt->mnt_pcp->mnt_count, 1);\n#else\n\t\tmnt->mnt_count = 1;\n\t\tmnt->mnt_writers = 0;\n#endif\n\n\t\tINIT_HLIST_NODE(&mnt->mnt_hash);\n\t\tINIT_LIST_HEAD(&mnt->mnt_child);\n\t\tINIT_LIST_HEAD(&mnt->mnt_mounts);\n\t\tINIT_LIST_HEAD(&mnt->mnt_list);\n\t\tINIT_LIST_HEAD(&mnt->mnt_expire);\n\t\tINIT_LIST_HEAD(&mnt->mnt_share);\n\t\tINIT_LIST_HEAD(&mnt->mnt_slave_list);\n\t\tINIT_LIST_HEAD(&mnt->mnt_slave);\n#ifdef CONFIG_FSNOTIFY\n\t\tINIT_HLIST_HEAD(&mnt->mnt_fsnotify_marks);\n#endif\n\t}\n\treturn mnt;\n\n#ifdef CONFIG_SMP\nout_free_devname:\n\tkfree(mnt->mnt_devname);\n#endif\nout_free_id:\n\tmnt_free_id(mnt);\nout_free_cache:\n\tkmem_cache_free(mnt_cache, mnt);\n\treturn NULL;\n}\n\n/*\n * Most r/o checks on a fs are for operations that take\n * discrete amounts of time, like a write() or unlink().\n * We must keep track of when those operations start\n * (for permission checks) and when they end, so that\n * we can determine when writes are able to occur to\n * a filesystem.\n */\n/*\n * __mnt_is_readonly: check whether a mount is read-only\n * @mnt: the mount to check for its write status\n *\n * This shouldn't be used directly ouside of the VFS.\n * It does not guarantee that the filesystem will stay\n * r/w, just that it is right *now*.  This can not and\n * should not be used in place of IS_RDONLY(inode).\n * mnt_want/drop_write() will _keep_ the filesystem\n * r/w.\n */\nint __mnt_is_readonly(struct vfsmount *mnt)\n{\n\tif (mnt->mnt_flags & MNT_READONLY)\n\t\treturn 1;\n\tif (mnt->mnt_sb->s_flags & MS_RDONLY)\n\t\treturn 1;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__mnt_is_readonly);\n\nstatic inline void mnt_inc_writers(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tthis_cpu_inc(mnt->mnt_pcp->mnt_writers);\n#else\n\tmnt->mnt_writers++;\n#endif\n}\n\nstatic inline void mnt_dec_writers(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tthis_cpu_dec(mnt->mnt_pcp->mnt_writers);\n#else\n\tmnt->mnt_writers--;\n#endif\n}\n\nstatic unsigned int mnt_get_writers(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tunsigned int count = 0;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcount += per_cpu_ptr(mnt->mnt_pcp, cpu)->mnt_writers;\n\t}\n\n\treturn count;\n#else\n\treturn mnt->mnt_writers;\n#endif\n}\n\nstatic int mnt_is_readonly(struct vfsmount *mnt)\n{\n\tif (mnt->mnt_sb->s_readonly_remount)\n\t\treturn 1;\n\t/* Order wrt setting s_flags/s_readonly_remount in do_remount() */\n\tsmp_rmb();\n\treturn __mnt_is_readonly(mnt);\n}\n\n/*\n * Most r/o & frozen checks on a fs are for operations that take discrete\n * amounts of time, like a write() or unlink().  We must keep track of when\n * those operations start (for permission checks) and when they end, so that we\n * can determine when writes are able to occur to a filesystem.\n */\n/**\n * __mnt_want_write - get write access to a mount without freeze protection\n * @m: the mount on which to take a write\n *\n * This tells the low-level filesystem that a write is about to be performed to\n * it, and makes sure that writes are allowed (mnt it read-write) before\n * returning success. This operation does not protect against filesystem being\n * frozen. When the write operation is finished, __mnt_drop_write() must be\n * called. This is effectively a refcount.\n */\nint __mnt_want_write(struct vfsmount *m)\n{\n\tstruct mount *mnt = real_mount(m);\n\tint ret = 0;\n\n\tpreempt_disable();\n\tmnt_inc_writers(mnt);\n\t/*\n\t * The store to mnt_inc_writers must be visible before we pass\n\t * MNT_WRITE_HOLD loop below, so that the slowpath can see our\n\t * incremented count after it has set MNT_WRITE_HOLD.\n\t */\n\tsmp_mb();\n\twhile (ACCESS_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD)\n\t\tcpu_relax();\n\t/*\n\t * After the slowpath clears MNT_WRITE_HOLD, mnt_is_readonly will\n\t * be set to match its requirements. So we must not load that until\n\t * MNT_WRITE_HOLD is cleared.\n\t */\n\tsmp_rmb();\n\tif (mnt_is_readonly(m)) {\n\t\tmnt_dec_writers(mnt);\n\t\tret = -EROFS;\n\t}\n\tpreempt_enable();\n\n\treturn ret;\n}\n\n/**\n * mnt_want_write - get write access to a mount\n * @m: the mount on which to take a write\n *\n * This tells the low-level filesystem that a write is about to be performed to\n * it, and makes sure that writes are allowed (mount is read-write, filesystem\n * is not frozen) before returning success.  When the write operation is\n * finished, mnt_drop_write() must be called.  This is effectively a refcount.\n */\nint mnt_want_write(struct vfsmount *m)\n{\n\tint ret;\n\n\tsb_start_write(m->mnt_sb);\n\tret = __mnt_want_write(m);\n\tif (ret)\n\t\tsb_end_write(m->mnt_sb);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mnt_want_write);\n\n/**\n * mnt_clone_write - get write access to a mount\n * @mnt: the mount on which to take a write\n *\n * This is effectively like mnt_want_write, except\n * it must only be used to take an extra write reference\n * on a mountpoint that we already know has a write reference\n * on it. This allows some optimisation.\n *\n * After finished, mnt_drop_write must be called as usual to\n * drop the reference.\n */\nint mnt_clone_write(struct vfsmount *mnt)\n{\n\t/* superblock may be r/o */\n\tif (__mnt_is_readonly(mnt))\n\t\treturn -EROFS;\n\tpreempt_disable();\n\tmnt_inc_writers(real_mount(mnt));\n\tpreempt_enable();\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(mnt_clone_write);\n\n/**\n * __mnt_want_write_file - get write access to a file's mount\n * @file: the file who's mount on which to take a write\n *\n * This is like __mnt_want_write, but it takes a file and can\n * do some optimisations if the file is open for write already\n */\nint __mnt_want_write_file(struct file *file)\n{\n\tif (!(file->f_mode & FMODE_WRITER))\n\t\treturn __mnt_want_write(file->f_path.mnt);\n\telse\n\t\treturn mnt_clone_write(file->f_path.mnt);\n}\n\n/**\n * mnt_want_write_file - get write access to a file's mount\n * @file: the file who's mount on which to take a write\n *\n * This is like mnt_want_write, but it takes a file and can\n * do some optimisations if the file is open for write already\n */\nint mnt_want_write_file(struct file *file)\n{\n\tint ret;\n\n\tsb_start_write(file->f_path.mnt->mnt_sb);\n\tret = __mnt_want_write_file(file);\n\tif (ret)\n\t\tsb_end_write(file->f_path.mnt->mnt_sb);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mnt_want_write_file);\n\n/**\n * __mnt_drop_write - give up write access to a mount\n * @mnt: the mount on which to give up write access\n *\n * Tells the low-level filesystem that we are done\n * performing writes to it.  Must be matched with\n * __mnt_want_write() call above.\n */\nvoid __mnt_drop_write(struct vfsmount *mnt)\n{\n\tpreempt_disable();\n\tmnt_dec_writers(real_mount(mnt));\n\tpreempt_enable();\n}\n\n/**\n * mnt_drop_write - give up write access to a mount\n * @mnt: the mount on which to give up write access\n *\n * Tells the low-level filesystem that we are done performing writes to it and\n * also allows filesystem to be frozen again.  Must be matched with\n * mnt_want_write() call above.\n */\nvoid mnt_drop_write(struct vfsmount *mnt)\n{\n\t__mnt_drop_write(mnt);\n\tsb_end_write(mnt->mnt_sb);\n}\nEXPORT_SYMBOL_GPL(mnt_drop_write);\n\nvoid __mnt_drop_write_file(struct file *file)\n{\n\t__mnt_drop_write(file->f_path.mnt);\n}\n\nvoid mnt_drop_write_file(struct file *file)\n{\n\tmnt_drop_write(file->f_path.mnt);\n}\nEXPORT_SYMBOL(mnt_drop_write_file);\n\nstatic int mnt_make_readonly(struct mount *mnt)\n{\n\tint ret = 0;\n\n\tlock_mount_hash();\n\tmnt->mnt.mnt_flags |= MNT_WRITE_HOLD;\n\t/*\n\t * After storing MNT_WRITE_HOLD, we'll read the counters. This store\n\t * should be visible before we do.\n\t */\n\tsmp_mb();\n\n\t/*\n\t * With writers on hold, if this value is zero, then there are\n\t * definitely no active writers (although held writers may subsequently\n\t * increment the count, they'll have to wait, and decrement it after\n\t * seeing MNT_READONLY).\n\t *\n\t * It is OK to have counter incremented on one CPU and decremented on\n\t * another: the sum will add up correctly. The danger would be when we\n\t * sum up each counter, if we read a counter before it is incremented,\n\t * but then read another CPU's count which it has been subsequently\n\t * decremented from -- we would see more decrements than we should.\n\t * MNT_WRITE_HOLD protects against this scenario, because\n\t * mnt_want_write first increments count, then smp_mb, then spins on\n\t * MNT_WRITE_HOLD, so it can't be decremented by another CPU while\n\t * we're counting up here.\n\t */\n\tif (mnt_get_writers(mnt) > 0)\n\t\tret = -EBUSY;\n\telse\n\t\tmnt->mnt.mnt_flags |= MNT_READONLY;\n\t/*\n\t * MNT_READONLY must become visible before ~MNT_WRITE_HOLD, so writers\n\t * that become unheld will see MNT_READONLY.\n\t */\n\tsmp_wmb();\n\tmnt->mnt.mnt_flags &= ~MNT_WRITE_HOLD;\n\tunlock_mount_hash();\n\treturn ret;\n}\n\nstatic void __mnt_unmake_readonly(struct mount *mnt)\n{\n\tlock_mount_hash();\n\tmnt->mnt.mnt_flags &= ~MNT_READONLY;\n\tunlock_mount_hash();\n}\n\nint sb_prepare_remount_readonly(struct super_block *sb)\n{\n\tstruct mount *mnt;\n\tint err = 0;\n\n\t/* Racy optimization.  Recheck the counter under MNT_WRITE_HOLD */\n\tif (atomic_long_read(&sb->s_remove_count))\n\t\treturn -EBUSY;\n\n\tlock_mount_hash();\n\tlist_for_each_entry(mnt, &sb->s_mounts, mnt_instance) {\n\t\tif (!(mnt->mnt.mnt_flags & MNT_READONLY)) {\n\t\t\tmnt->mnt.mnt_flags |= MNT_WRITE_HOLD;\n\t\t\tsmp_mb();\n\t\t\tif (mnt_get_writers(mnt) > 0) {\n\t\t\t\terr = -EBUSY;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tif (!err && atomic_long_read(&sb->s_remove_count))\n\t\terr = -EBUSY;\n\n\tif (!err) {\n\t\tsb->s_readonly_remount = 1;\n\t\tsmp_wmb();\n\t}\n\tlist_for_each_entry(mnt, &sb->s_mounts, mnt_instance) {\n\t\tif (mnt->mnt.mnt_flags & MNT_WRITE_HOLD)\n\t\t\tmnt->mnt.mnt_flags &= ~MNT_WRITE_HOLD;\n\t}\n\tunlock_mount_hash();\n\n\treturn err;\n}\n\nstatic void free_vfsmnt(struct mount *mnt)\n{\n\tkfree(mnt->mnt_devname);\n#ifdef CONFIG_SMP\n\tfree_percpu(mnt->mnt_pcp);\n#endif\n\tkmem_cache_free(mnt_cache, mnt);\n}\n\nstatic void delayed_free_vfsmnt(struct rcu_head *head)\n{\n\tfree_vfsmnt(container_of(head, struct mount, mnt_rcu));\n}\n\n/* call under rcu_read_lock */\nbool legitimize_mnt(struct vfsmount *bastard, unsigned seq)\n{\n\tstruct mount *mnt;\n\tif (read_seqretry(&mount_lock, seq))\n\t\treturn false;\n\tif (bastard == NULL)\n\t\treturn true;\n\tmnt = real_mount(bastard);\n\tmnt_add_count(mnt, 1);\n\tif (likely(!read_seqretry(&mount_lock, seq)))\n\t\treturn true;\n\tif (bastard->mnt_flags & MNT_SYNC_UMOUNT) {\n\t\tmnt_add_count(mnt, -1);\n\t\treturn false;\n\t}\n\trcu_read_unlock();\n\tmntput(bastard);\n\trcu_read_lock();\n\treturn false;\n}\n\n/*\n * find the first mount at @dentry on vfsmount @mnt.\n * call under rcu_read_lock()\n */\nstruct mount *__lookup_mnt(struct vfsmount *mnt, struct dentry *dentry)\n{\n\tstruct hlist_head *head = m_hash(mnt, dentry);\n\tstruct mount *p;\n\n\thlist_for_each_entry_rcu(p, head, mnt_hash)\n\t\tif (&p->mnt_parent->mnt == mnt && p->mnt_mountpoint == dentry)\n\t\t\treturn p;\n\treturn NULL;\n}\n\n/*\n * find the last mount at @dentry on vfsmount @mnt.\n * mount_lock must be held.\n */\nstruct mount *__lookup_mnt_last(struct vfsmount *mnt, struct dentry *dentry)\n{\n\tstruct mount *p, *res;\n\tres = p = __lookup_mnt(mnt, dentry);\n\tif (!p)\n\t\tgoto out;\n\thlist_for_each_entry_continue(p, mnt_hash) {\n\t\tif (&p->mnt_parent->mnt != mnt || p->mnt_mountpoint != dentry)\n\t\t\tbreak;\n\t\tres = p;\n\t}\nout:\n\treturn res;\n}\n\n/*\n * lookup_mnt - Return the first child mount mounted at path\n *\n * \"First\" means first mounted chronologically.  If you create the\n * following mounts:\n *\n * mount /dev/sda1 /mnt\n * mount /dev/sda2 /mnt\n * mount /dev/sda3 /mnt\n *\n * Then lookup_mnt() on the base /mnt dentry in the root mount will\n * return successively the root dentry and vfsmount of /dev/sda1, then\n * /dev/sda2, then /dev/sda3, then NULL.\n *\n * lookup_mnt takes a reference to the found vfsmount.\n */\nstruct vfsmount *lookup_mnt(struct path *path)\n{\n\tstruct mount *child_mnt;\n\tstruct vfsmount *m;\n\tunsigned seq;\n\n\trcu_read_lock();\n\tdo {\n\t\tseq = read_seqbegin(&mount_lock);\n\t\tchild_mnt = __lookup_mnt(path->mnt, path->dentry);\n\t\tm = child_mnt ? &child_mnt->mnt : NULL;\n\t} while (!legitimize_mnt(m, seq));\n\trcu_read_unlock();\n\treturn m;\n}\n\nstatic struct mountpoint *new_mountpoint(struct dentry *dentry)\n{\n\tstruct hlist_head *chain = mp_hash(dentry);\n\tstruct mountpoint *mp;\n\tint ret;\n\n\thlist_for_each_entry(mp, chain, m_hash) {\n\t\tif (mp->m_dentry == dentry) {\n\t\t\t/* might be worth a WARN_ON() */\n\t\t\tif (d_unlinked(dentry))\n\t\t\t\treturn ERR_PTR(-ENOENT);\n\t\t\tmp->m_count++;\n\t\t\treturn mp;\n\t\t}\n\t}\n\n\tmp = kmalloc(sizeof(struct mountpoint), GFP_KERNEL);\n\tif (!mp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = d_set_mounted(dentry);\n\tif (ret) {\n\t\tkfree(mp);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tmp->m_dentry = dentry;\n\tmp->m_count = 1;\n\thlist_add_head(&mp->m_hash, chain);\n\treturn mp;\n}\n\nstatic void put_mountpoint(struct mountpoint *mp)\n{\n\tif (!--mp->m_count) {\n\t\tstruct dentry *dentry = mp->m_dentry;\n\t\tspin_lock(&dentry->d_lock);\n\t\tdentry->d_flags &= ~DCACHE_MOUNTED;\n\t\tspin_unlock(&dentry->d_lock);\n\t\thlist_del(&mp->m_hash);\n\t\tkfree(mp);\n\t}\n}\n\nstatic inline int check_mnt(struct mount *mnt)\n{\n\treturn mnt->mnt_ns == current->nsproxy->mnt_ns;\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void touch_mnt_namespace(struct mnt_namespace *ns)\n{\n\tif (ns) {\n\t\tns->event = ++event;\n\t\twake_up_interruptible(&ns->poll);\n\t}\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void __touch_mnt_namespace(struct mnt_namespace *ns)\n{\n\tif (ns && ns->event != event) {\n\t\tns->event = event;\n\t\twake_up_interruptible(&ns->poll);\n\t}\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void detach_mnt(struct mount *mnt, struct path *old_path)\n{\n\told_path->dentry = mnt->mnt_mountpoint;\n\told_path->mnt = &mnt->mnt_parent->mnt;\n\tmnt->mnt_parent = mnt;\n\tmnt->mnt_mountpoint = mnt->mnt.mnt_root;\n\tlist_del_init(&mnt->mnt_child);\n\thlist_del_init_rcu(&mnt->mnt_hash);\n\tput_mountpoint(mnt->mnt_mp);\n\tmnt->mnt_mp = NULL;\n}\n\n/*\n * vfsmount lock must be held for write\n */\nvoid mnt_set_mountpoint(struct mount *mnt,\n\t\t\tstruct mountpoint *mp,\n\t\t\tstruct mount *child_mnt)\n{\n\tmp->m_count++;\n\tmnt_add_count(mnt, 1);\t/* essentially, that's mntget */\n\tchild_mnt->mnt_mountpoint = dget(mp->m_dentry);\n\tchild_mnt->mnt_parent = mnt;\n\tchild_mnt->mnt_mp = mp;\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void attach_mnt(struct mount *mnt,\n\t\t\tstruct mount *parent,\n\t\t\tstruct mountpoint *mp)\n{\n\tmnt_set_mountpoint(parent, mp, mnt);\n\thlist_add_head_rcu(&mnt->mnt_hash, m_hash(&parent->mnt, mp->m_dentry));\n\tlist_add_tail(&mnt->mnt_child, &parent->mnt_mounts);\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void commit_tree(struct mount *mnt, struct mount *shadows)\n{\n\tstruct mount *parent = mnt->mnt_parent;\n\tstruct mount *m;\n\tLIST_HEAD(head);\n\tstruct mnt_namespace *n = parent->mnt_ns;\n\n\tBUG_ON(parent == mnt);\n\n\tlist_add_tail(&head, &mnt->mnt_list);\n\tlist_for_each_entry(m, &head, mnt_list)\n\t\tm->mnt_ns = n;\n\n\tlist_splice(&head, n->list.prev);\n\n\tif (shadows)\n\t\thlist_add_after_rcu(&shadows->mnt_hash, &mnt->mnt_hash);\n\telse\n\t\thlist_add_head_rcu(&mnt->mnt_hash,\n\t\t\t\tm_hash(&parent->mnt, mnt->mnt_mountpoint));\n\tlist_add_tail(&mnt->mnt_child, &parent->mnt_mounts);\n\ttouch_mnt_namespace(n);\n}\n\nstatic struct mount *next_mnt(struct mount *p, struct mount *root)\n{\n\tstruct list_head *next = p->mnt_mounts.next;\n\tif (next == &p->mnt_mounts) {\n\t\twhile (1) {\n\t\t\tif (p == root)\n\t\t\t\treturn NULL;\n\t\t\tnext = p->mnt_child.next;\n\t\t\tif (next != &p->mnt_parent->mnt_mounts)\n\t\t\t\tbreak;\n\t\t\tp = p->mnt_parent;\n\t\t}\n\t}\n\treturn list_entry(next, struct mount, mnt_child);\n}\n\nstatic struct mount *skip_mnt_tree(struct mount *p)\n{\n\tstruct list_head *prev = p->mnt_mounts.prev;\n\twhile (prev != &p->mnt_mounts) {\n\t\tp = list_entry(prev, struct mount, mnt_child);\n\t\tprev = p->mnt_mounts.prev;\n\t}\n\treturn p;\n}\n\nstruct vfsmount *\nvfs_kern_mount(struct file_system_type *type, int flags, const char *name, void *data)\n{\n\tstruct mount *mnt;\n\tstruct dentry *root;\n\n\tif (!type)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tmnt = alloc_vfsmnt(name);\n\tif (!mnt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (flags & MS_KERNMOUNT)\n\t\tmnt->mnt.mnt_flags = MNT_INTERNAL;\n\n\troot = mount_fs(type, flags, name, data);\n\tif (IS_ERR(root)) {\n\t\tmnt_free_id(mnt);\n\t\tfree_vfsmnt(mnt);\n\t\treturn ERR_CAST(root);\n\t}\n\n\tmnt->mnt.mnt_root = root;\n\tmnt->mnt.mnt_sb = root->d_sb;\n\tmnt->mnt_mountpoint = mnt->mnt.mnt_root;\n\tmnt->mnt_parent = mnt;\n\tlock_mount_hash();\n\tlist_add_tail(&mnt->mnt_instance, &root->d_sb->s_mounts);\n\tunlock_mount_hash();\n\treturn &mnt->mnt;\n}\nEXPORT_SYMBOL_GPL(vfs_kern_mount);\n\nstatic struct mount *clone_mnt(struct mount *old, struct dentry *root,\n\t\t\t\t\tint flag)\n{\n\tstruct super_block *sb = old->mnt.mnt_sb;\n\tstruct mount *mnt;\n\tint err;\n\n\tmnt = alloc_vfsmnt(old->mnt_devname);\n\tif (!mnt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (flag & (CL_SLAVE | CL_PRIVATE | CL_SHARED_TO_SLAVE))\n\t\tmnt->mnt_group_id = 0; /* not a peer of original */\n\telse\n\t\tmnt->mnt_group_id = old->mnt_group_id;\n\n\tif ((flag & CL_MAKE_SHARED) && !mnt->mnt_group_id) {\n\t\terr = mnt_alloc_group_id(mnt);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t}\n\n\tmnt->mnt.mnt_flags = old->mnt.mnt_flags & ~(MNT_WRITE_HOLD|MNT_MARKED);\n\t/* Don't allow unprivileged users to change mount flags */\n\tif (flag & CL_UNPRIVILEGED) {\n\t\tmnt->mnt.mnt_flags |= MNT_LOCK_ATIME;\n\n\t\tif (mnt->mnt.mnt_flags & MNT_READONLY)\n\t\t\tmnt->mnt.mnt_flags |= MNT_LOCK_READONLY;\n\n\t\tif (mnt->mnt.mnt_flags & MNT_NODEV)\n\t\t\tmnt->mnt.mnt_flags |= MNT_LOCK_NODEV;\n\n\t\tif (mnt->mnt.mnt_flags & MNT_NOSUID)\n\t\t\tmnt->mnt.mnt_flags |= MNT_LOCK_NOSUID;\n\n\t\tif (mnt->mnt.mnt_flags & MNT_NOEXEC)\n\t\t\tmnt->mnt.mnt_flags |= MNT_LOCK_NOEXEC;\n\t}\n\n\t/* Don't allow unprivileged users to reveal what is under a mount */\n\tif ((flag & CL_UNPRIVILEGED) && list_empty(&old->mnt_expire))\n\t\tmnt->mnt.mnt_flags |= MNT_LOCKED;\n\n\tatomic_inc(&sb->s_active);\n\tmnt->mnt.mnt_sb = sb;\n\tmnt->mnt.mnt_root = dget(root);\n\tmnt->mnt_mountpoint = mnt->mnt.mnt_root;\n\tmnt->mnt_parent = mnt;\n\tlock_mount_hash();\n\tlist_add_tail(&mnt->mnt_instance, &sb->s_mounts);\n\tunlock_mount_hash();\n\n\tif ((flag & CL_SLAVE) ||\n\t    ((flag & CL_SHARED_TO_SLAVE) && IS_MNT_SHARED(old))) {\n\t\tlist_add(&mnt->mnt_slave, &old->mnt_slave_list);\n\t\tmnt->mnt_master = old;\n\t\tCLEAR_MNT_SHARED(mnt);\n\t} else if (!(flag & CL_PRIVATE)) {\n\t\tif ((flag & CL_MAKE_SHARED) || IS_MNT_SHARED(old))\n\t\t\tlist_add(&mnt->mnt_share, &old->mnt_share);\n\t\tif (IS_MNT_SLAVE(old))\n\t\t\tlist_add(&mnt->mnt_slave, &old->mnt_slave);\n\t\tmnt->mnt_master = old->mnt_master;\n\t}\n\tif (flag & CL_MAKE_SHARED)\n\t\tset_mnt_shared(mnt);\n\n\t/* stick the duplicate mount on the same expiry list\n\t * as the original if that was on one */\n\tif (flag & CL_EXPIRE) {\n\t\tif (!list_empty(&old->mnt_expire))\n\t\t\tlist_add(&mnt->mnt_expire, &old->mnt_expire);\n\t}\n\n\treturn mnt;\n\n out_free:\n\tmnt_free_id(mnt);\n\tfree_vfsmnt(mnt);\n\treturn ERR_PTR(err);\n}\n\nstatic void mntput_no_expire(struct mount *mnt)\n{\nput_again:\n\trcu_read_lock();\n\tmnt_add_count(mnt, -1);\n\tif (likely(mnt->mnt_ns)) { /* shouldn't be the last one */\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tlock_mount_hash();\n\tif (mnt_get_count(mnt)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tif (unlikely(mnt->mnt_pinned)) {\n\t\tmnt_add_count(mnt, mnt->mnt_pinned + 1);\n\t\tmnt->mnt_pinned = 0;\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\tacct_auto_close_mnt(&mnt->mnt);\n\t\tgoto put_again;\n\t}\n\tif (unlikely(mnt->mnt.mnt_flags & MNT_DOOMED)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tmnt->mnt.mnt_flags |= MNT_DOOMED;\n\trcu_read_unlock();\n\n\tlist_del(&mnt->mnt_instance);\n\tunlock_mount_hash();\n\n\t/*\n\t * This probably indicates that somebody messed\n\t * up a mnt_want/drop_write() pair.  If this\n\t * happens, the filesystem was probably unable\n\t * to make r/w->r/o transitions.\n\t */\n\t/*\n\t * The locking used to deal with mnt_count decrement provides barriers,\n\t * so mnt_get_writers() below is safe.\n\t */\n\tWARN_ON(mnt_get_writers(mnt));\n\tfsnotify_vfsmount_delete(&mnt->mnt);\n\tdput(mnt->mnt.mnt_root);\n\tdeactivate_super(mnt->mnt.mnt_sb);\n\tmnt_free_id(mnt);\n\tcall_rcu(&mnt->mnt_rcu, delayed_free_vfsmnt);\n}\n\nvoid mntput(struct vfsmount *mnt)\n{\n\tif (mnt) {\n\t\tstruct mount *m = real_mount(mnt);\n\t\t/* avoid cacheline pingpong, hope gcc doesn't get \"smart\" */\n\t\tif (unlikely(m->mnt_expiry_mark))\n\t\t\tm->mnt_expiry_mark = 0;\n\t\tmntput_no_expire(m);\n\t}\n}\nEXPORT_SYMBOL(mntput);\n\nstruct vfsmount *mntget(struct vfsmount *mnt)\n{\n\tif (mnt)\n\t\tmnt_add_count(real_mount(mnt), 1);\n\treturn mnt;\n}\nEXPORT_SYMBOL(mntget);\n\nvoid mnt_pin(struct vfsmount *mnt)\n{\n\tlock_mount_hash();\n\treal_mount(mnt)->mnt_pinned++;\n\tunlock_mount_hash();\n}\nEXPORT_SYMBOL(mnt_pin);\n\nvoid mnt_unpin(struct vfsmount *m)\n{\n\tstruct mount *mnt = real_mount(m);\n\tlock_mount_hash();\n\tif (mnt->mnt_pinned) {\n\t\tmnt_add_count(mnt, 1);\n\t\tmnt->mnt_pinned--;\n\t}\n\tunlock_mount_hash();\n}\nEXPORT_SYMBOL(mnt_unpin);\n\nstatic inline void mangle(struct seq_file *m, const char *s)\n{\n\tseq_escape(m, s, \" \\t\\n\\\\\");\n}\n\n/*\n * Simple .show_options callback for filesystems which don't want to\n * implement more complex mount option showing.\n *\n * See also save_mount_options().\n */\nint generic_show_options(struct seq_file *m, struct dentry *root)\n{\n\tconst char *options;\n\n\trcu_read_lock();\n\toptions = rcu_dereference(root->d_sb->s_options);\n\n\tif (options != NULL && options[0]) {\n\t\tseq_putc(m, ',');\n\t\tmangle(m, options);\n\t}\n\trcu_read_unlock();\n\n\treturn 0;\n}\nEXPORT_SYMBOL(generic_show_options);\n\n/*\n * If filesystem uses generic_show_options(), this function should be\n * called from the fill_super() callback.\n *\n * The .remount_fs callback usually needs to be handled in a special\n * way, to make sure, that previous options are not overwritten if the\n * remount fails.\n *\n * Also note, that if the filesystem's .remount_fs function doesn't\n * reset all options to their default value, but changes only newly\n * given options, then the displayed options will not reflect reality\n * any more.\n */\nvoid save_mount_options(struct super_block *sb, char *options)\n{\n\tBUG_ON(sb->s_options);\n\trcu_assign_pointer(sb->s_options, kstrdup(options, GFP_KERNEL));\n}\nEXPORT_SYMBOL(save_mount_options);\n\nvoid replace_mount_options(struct super_block *sb, char *options)\n{\n\tchar *old = sb->s_options;\n\trcu_assign_pointer(sb->s_options, options);\n\tif (old) {\n\t\tsynchronize_rcu();\n\t\tkfree(old);\n\t}\n}\nEXPORT_SYMBOL(replace_mount_options);\n\n#ifdef CONFIG_PROC_FS\n/* iterator; we want it to have access to namespace_sem, thus here... */\nstatic void *m_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct proc_mounts *p = proc_mounts(m);\n\n\tdown_read(&namespace_sem);\n\tif (p->cached_event == p->ns->event) {\n\t\tvoid *v = p->cached_mount;\n\t\tif (*pos == p->cached_index)\n\t\t\treturn v;\n\t\tif (*pos == p->cached_index + 1) {\n\t\t\tv = seq_list_next(v, &p->ns->list, &p->cached_index);\n\t\t\treturn p->cached_mount = v;\n\t\t}\n\t}\n\n\tp->cached_event = p->ns->event;\n\tp->cached_mount = seq_list_start(&p->ns->list, *pos);\n\tp->cached_index = *pos;\n\treturn p->cached_mount;\n}\n\nstatic void *m_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct proc_mounts *p = proc_mounts(m);\n\n\tp->cached_mount = seq_list_next(v, &p->ns->list, pos);\n\tp->cached_index = *pos;\n\treturn p->cached_mount;\n}\n\nstatic void m_stop(struct seq_file *m, void *v)\n{\n\tup_read(&namespace_sem);\n}\n\nstatic int m_show(struct seq_file *m, void *v)\n{\n\tstruct proc_mounts *p = proc_mounts(m);\n\tstruct mount *r = list_entry(v, struct mount, mnt_list);\n\treturn p->show(m, &r->mnt);\n}\n\nconst struct seq_operations mounts_op = {\n\t.start\t= m_start,\n\t.next\t= m_next,\n\t.stop\t= m_stop,\n\t.show\t= m_show,\n};\n#endif  /* CONFIG_PROC_FS */\n\n/**\n * may_umount_tree - check if a mount tree is busy\n * @mnt: root of mount tree\n *\n * This is called to check if a tree of mounts has any\n * open files, pwds, chroots or sub mounts that are\n * busy.\n */\nint may_umount_tree(struct vfsmount *m)\n{\n\tstruct mount *mnt = real_mount(m);\n\tint actual_refs = 0;\n\tint minimum_refs = 0;\n\tstruct mount *p;\n\tBUG_ON(!m);\n\n\t/* write lock needed for mnt_get_count */\n\tlock_mount_hash();\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tactual_refs += mnt_get_count(p);\n\t\tminimum_refs += 2;\n\t}\n\tunlock_mount_hash();\n\n\tif (actual_refs > minimum_refs)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nEXPORT_SYMBOL(may_umount_tree);\n\n/**\n * may_umount - check if a mount point is busy\n * @mnt: root of mount\n *\n * This is called to check if a mount point has any\n * open files, pwds, chroots or sub mounts. If the\n * mount has sub mounts this will return busy\n * regardless of whether the sub mounts are busy.\n *\n * Doesn't take quota and stuff into account. IOW, in some cases it will\n * give false negatives. The main reason why it's here is that we need\n * a non-destructive way to look for easily umountable filesystems.\n */\nint may_umount(struct vfsmount *mnt)\n{\n\tint ret = 1;\n\tdown_read(&namespace_sem);\n\tlock_mount_hash();\n\tif (propagate_mount_busy(real_mount(mnt), 2))\n\t\tret = 0;\n\tunlock_mount_hash();\n\tup_read(&namespace_sem);\n\treturn ret;\n}\n\nEXPORT_SYMBOL(may_umount);\n\nstatic HLIST_HEAD(unmounted);\t/* protected by namespace_sem */\n\nstatic void namespace_unlock(void)\n{\n\tstruct mount *mnt;\n\tstruct hlist_head head = unmounted;\n\n\tif (likely(hlist_empty(&head))) {\n\t\tup_write(&namespace_sem);\n\t\treturn;\n\t}\n\n\thead.first->pprev = &head.first;\n\tINIT_HLIST_HEAD(&unmounted);\n\n\tup_write(&namespace_sem);\n\n\tsynchronize_rcu();\n\n\twhile (!hlist_empty(&head)) {\n\t\tmnt = hlist_entry(head.first, struct mount, mnt_hash);\n\t\thlist_del_init(&mnt->mnt_hash);\n\t\tif (mnt->mnt_ex_mountpoint.mnt)\n\t\t\tpath_put(&mnt->mnt_ex_mountpoint);\n\t\tmntput(&mnt->mnt);\n\t}\n}\n\nstatic inline void namespace_lock(void)\n{\n\tdown_write(&namespace_sem);\n}\n\n/*\n * mount_lock must be held\n * namespace_sem must be held for write\n * how = 0 => just this tree, don't propagate\n * how = 1 => propagate; we know that nobody else has reference to any victims\n * how = 2 => lazy umount\n */\nvoid umount_tree(struct mount *mnt, int how)\n{\n\tHLIST_HEAD(tmp_list);\n\tstruct mount *p;\n\tstruct mount *last = NULL;\n\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\thlist_del_init_rcu(&p->mnt_hash);\n\t\thlist_add_head(&p->mnt_hash, &tmp_list);\n\t}\n\n\tif (how)\n\t\tpropagate_umount(&tmp_list);\n\n\thlist_for_each_entry(p, &tmp_list, mnt_hash) {\n\t\tlist_del_init(&p->mnt_expire);\n\t\tlist_del_init(&p->mnt_list);\n\t\t__touch_mnt_namespace(p->mnt_ns);\n\t\tp->mnt_ns = NULL;\n\t\tif (how < 2)\n\t\t\tp->mnt.mnt_flags |= MNT_SYNC_UMOUNT;\n\t\tlist_del_init(&p->mnt_child);\n\t\tif (mnt_has_parent(p)) {\n\t\t\tput_mountpoint(p->mnt_mp);\n\t\t\t/* move the reference to mountpoint into ->mnt_ex_mountpoint */\n\t\t\tp->mnt_ex_mountpoint.dentry = p->mnt_mountpoint;\n\t\t\tp->mnt_ex_mountpoint.mnt = &p->mnt_parent->mnt;\n\t\t\tp->mnt_mountpoint = p->mnt.mnt_root;\n\t\t\tp->mnt_parent = p;\n\t\t\tp->mnt_mp = NULL;\n\t\t}\n\t\tchange_mnt_propagation(p, MS_PRIVATE);\n\t\tlast = p;\n\t}\n\tif (last) {\n\t\tlast->mnt_hash.next = unmounted.first;\n\t\tunmounted.first = tmp_list.first;\n\t\tunmounted.first->pprev = &unmounted.first;\n\t}\n}\n\nstatic void shrink_submounts(struct mount *mnt);\n\nstatic int do_umount(struct mount *mnt, int flags)\n{\n\tstruct super_block *sb = mnt->mnt.mnt_sb;\n\tint retval;\n\n\tretval = security_sb_umount(&mnt->mnt, flags);\n\tif (retval)\n\t\treturn retval;\n\n\t/*\n\t * Allow userspace to request a mountpoint be expired rather than\n\t * unmounting unconditionally. Unmount only happens if:\n\t *  (1) the mark is already set (the mark is cleared by mntput())\n\t *  (2) the usage count == 1 [parent vfsmount] + 1 [sys_umount]\n\t */\n\tif (flags & MNT_EXPIRE) {\n\t\tif (&mnt->mnt == current->fs->root.mnt ||\n\t\t    flags & (MNT_FORCE | MNT_DETACH))\n\t\t\treturn -EINVAL;\n\n\t\t/*\n\t\t * probably don't strictly need the lock here if we examined\n\t\t * all race cases, but it's a slowpath.\n\t\t */\n\t\tlock_mount_hash();\n\t\tif (mnt_get_count(mnt) != 2) {\n\t\t\tunlock_mount_hash();\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tunlock_mount_hash();\n\n\t\tif (!xchg(&mnt->mnt_expiry_mark, 1))\n\t\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * If we may have to abort operations to get out of this\n\t * mount, and they will themselves hold resources we must\n\t * allow the fs to do things. In the Unix tradition of\n\t * 'Gee thats tricky lets do it in userspace' the umount_begin\n\t * might fail to complete on the first run through as other tasks\n\t * must return, and the like. Thats for the mount program to worry\n\t * about for the moment.\n\t */\n\n\tif (flags & MNT_FORCE && sb->s_op->umount_begin) {\n\t\tsb->s_op->umount_begin(sb);\n\t}\n\n\t/*\n\t * No sense to grab the lock for this test, but test itself looks\n\t * somewhat bogus. Suggestions for better replacement?\n\t * Ho-hum... In principle, we might treat that as umount + switch\n\t * to rootfs. GC would eventually take care of the old vfsmount.\n\t * Actually it makes sense, especially if rootfs would contain a\n\t * /reboot - static binary that would close all descriptors and\n\t * call reboot(9). Then init(8) could umount root and exec /reboot.\n\t */\n\tif (&mnt->mnt == current->fs->root.mnt && !(flags & MNT_DETACH)) {\n\t\t/*\n\t\t * Special case for \"unmounting\" root ...\n\t\t * we just try to remount it readonly.\n\t\t */\n\t\tdown_write(&sb->s_umount);\n\t\tif (!(sb->s_flags & MS_RDONLY))\n\t\t\tretval = do_remount_sb(sb, MS_RDONLY, NULL, 0);\n\t\tup_write(&sb->s_umount);\n\t\treturn retval;\n\t}\n\n\tnamespace_lock();\n\tlock_mount_hash();\n\tevent++;\n\n\tif (flags & MNT_DETACH) {\n\t\tif (!list_empty(&mnt->mnt_list))\n\t\t\tumount_tree(mnt, 2);\n\t\tretval = 0;\n\t} else {\n\t\tshrink_submounts(mnt);\n\t\tretval = -EBUSY;\n\t\tif (!propagate_mount_busy(mnt, 2)) {\n\t\t\tif (!list_empty(&mnt->mnt_list))\n\t\t\t\tumount_tree(mnt, 1);\n\t\t\tretval = 0;\n\t\t}\n\t}\n\tunlock_mount_hash();\n\tnamespace_unlock();\n\treturn retval;\n}\n\n/* \n * Is the caller allowed to modify his namespace?\n */\nstatic inline bool may_mount(void)\n{\n\treturn ns_capable(current->nsproxy->mnt_ns->user_ns, CAP_SYS_ADMIN);\n}\n\n/*\n * Now umount can handle mount points as well as block devices.\n * This is important for filesystems which use unnamed block devices.\n *\n * We now support a flag for forced unmount like the other 'big iron'\n * unixes. Our API is identical to OSF/1 to avoid making a mess of AMD\n */\n\nSYSCALL_DEFINE2(umount, char __user *, name, int, flags)\n{\n\tstruct path path;\n\tstruct mount *mnt;\n\tint retval;\n\tint lookup_flags = 0;\n\n\tif (flags & ~(MNT_FORCE | MNT_DETACH | MNT_EXPIRE | UMOUNT_NOFOLLOW))\n\t\treturn -EINVAL;\n\n\tif (!may_mount())\n\t\treturn -EPERM;\n\n\tif (!(flags & UMOUNT_NOFOLLOW))\n\t\tlookup_flags |= LOOKUP_FOLLOW;\n\n\tretval = user_path_mountpoint_at(AT_FDCWD, name, lookup_flags, &path);\n\tif (retval)\n\t\tgoto out;\n\tmnt = real_mount(path.mnt);\n\tretval = -EINVAL;\n\tif (path.dentry != path.mnt->mnt_root)\n\t\tgoto dput_and_out;\n\tif (!check_mnt(mnt))\n\t\tgoto dput_and_out;\n\tif (mnt->mnt.mnt_flags & MNT_LOCKED)\n\t\tgoto dput_and_out;\n\n\tretval = do_umount(mnt, flags);\ndput_and_out:\n\t/* we mustn't call path_put() as that would clear mnt_expiry_mark */\n\tdput(path.dentry);\n\tmntput_no_expire(mnt);\nout:\n\treturn retval;\n}\n\n#ifdef __ARCH_WANT_SYS_OLDUMOUNT\n\n/*\n *\tThe 2.0 compatible umount. No flags.\n */\nSYSCALL_DEFINE1(oldumount, char __user *, name)\n{\n\treturn sys_umount(name, 0);\n}\n\n#endif\n\nstatic bool is_mnt_ns_file(struct dentry *dentry)\n{\n\t/* Is this a proxy for a mount namespace? */\n\tstruct inode *inode = dentry->d_inode;\n\tstruct proc_ns *ei;\n\n\tif (!proc_ns_inode(inode))\n\t\treturn false;\n\n\tei = get_proc_ns(inode);\n\tif (ei->ns_ops != &mntns_operations)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool mnt_ns_loop(struct dentry *dentry)\n{\n\t/* Could bind mounting the mount namespace inode cause a\n\t * mount namespace loop?\n\t */\n\tstruct mnt_namespace *mnt_ns;\n\tif (!is_mnt_ns_file(dentry))\n\t\treturn false;\n\n\tmnt_ns = get_proc_ns(dentry->d_inode)->ns;\n\treturn current->nsproxy->mnt_ns->seq >= mnt_ns->seq;\n}\n\nstruct mount *copy_tree(struct mount *mnt, struct dentry *dentry,\n\t\t\t\t\tint flag)\n{\n\tstruct mount *res, *p, *q, *r, *parent;\n\n\tif (!(flag & CL_COPY_UNBINDABLE) && IS_MNT_UNBINDABLE(mnt))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!(flag & CL_COPY_MNT_NS_FILE) && is_mnt_ns_file(dentry))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tres = q = clone_mnt(mnt, dentry, flag);\n\tif (IS_ERR(q))\n\t\treturn q;\n\n\tq->mnt.mnt_flags &= ~MNT_LOCKED;\n\tq->mnt_mountpoint = mnt->mnt_mountpoint;\n\n\tp = mnt;\n\tlist_for_each_entry(r, &mnt->mnt_mounts, mnt_child) {\n\t\tstruct mount *s;\n\t\tif (!is_subdir(r->mnt_mountpoint, dentry))\n\t\t\tcontinue;\n\n\t\tfor (s = r; s; s = next_mnt(s, r)) {\n\t\t\tif (!(flag & CL_COPY_UNBINDABLE) &&\n\t\t\t    IS_MNT_UNBINDABLE(s)) {\n\t\t\t\ts = skip_mnt_tree(s);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!(flag & CL_COPY_MNT_NS_FILE) &&\n\t\t\t    is_mnt_ns_file(s->mnt.mnt_root)) {\n\t\t\t\ts = skip_mnt_tree(s);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\twhile (p != s->mnt_parent) {\n\t\t\t\tp = p->mnt_parent;\n\t\t\t\tq = q->mnt_parent;\n\t\t\t}\n\t\t\tp = s;\n\t\t\tparent = q;\n\t\t\tq = clone_mnt(p, p->mnt.mnt_root, flag);\n\t\t\tif (IS_ERR(q))\n\t\t\t\tgoto out;\n\t\t\tlock_mount_hash();\n\t\t\tlist_add_tail(&q->mnt_list, &res->mnt_list);\n\t\t\tattach_mnt(q, parent, p->mnt_mp);\n\t\t\tunlock_mount_hash();\n\t\t}\n\t}\n\treturn res;\nout:\n\tif (res) {\n\t\tlock_mount_hash();\n\t\tumount_tree(res, 0);\n\t\tunlock_mount_hash();\n\t}\n\treturn q;\n}\n\n/* Caller should check returned pointer for errors */\n\nstruct vfsmount *collect_mounts(struct path *path)\n{\n\tstruct mount *tree;\n\tnamespace_lock();\n\ttree = copy_tree(real_mount(path->mnt), path->dentry,\n\t\t\t CL_COPY_ALL | CL_PRIVATE);\n\tnamespace_unlock();\n\tif (IS_ERR(tree))\n\t\treturn ERR_CAST(tree);\n\treturn &tree->mnt;\n}\n\nvoid drop_collected_mounts(struct vfsmount *mnt)\n{\n\tnamespace_lock();\n\tlock_mount_hash();\n\tumount_tree(real_mount(mnt), 0);\n\tunlock_mount_hash();\n\tnamespace_unlock();\n}\n\nint iterate_mounts(int (*f)(struct vfsmount *, void *), void *arg,\n\t\t   struct vfsmount *root)\n{\n\tstruct mount *mnt;\n\tint res = f(root, arg);\n\tif (res)\n\t\treturn res;\n\tlist_for_each_entry(mnt, &real_mount(root)->mnt_list, mnt_list) {\n\t\tres = f(&mnt->mnt, arg);\n\t\tif (res)\n\t\t\treturn res;\n\t}\n\treturn 0;\n}\n\nstatic void cleanup_group_ids(struct mount *mnt, struct mount *end)\n{\n\tstruct mount *p;\n\n\tfor (p = mnt; p != end; p = next_mnt(p, mnt)) {\n\t\tif (p->mnt_group_id && !IS_MNT_SHARED(p))\n\t\t\tmnt_release_group_id(p);\n\t}\n}\n\nstatic int invent_group_ids(struct mount *mnt, bool recurse)\n{\n\tstruct mount *p;\n\n\tfor (p = mnt; p; p = recurse ? next_mnt(p, mnt) : NULL) {\n\t\tif (!p->mnt_group_id && !IS_MNT_SHARED(p)) {\n\t\t\tint err = mnt_alloc_group_id(p);\n\t\t\tif (err) {\n\t\t\t\tcleanup_group_ids(mnt, p);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n *  @source_mnt : mount tree to be attached\n *  @nd         : place the mount tree @source_mnt is attached\n *  @parent_nd  : if non-null, detach the source_mnt from its parent and\n *  \t\t   store the parent mount and mountpoint dentry.\n *  \t\t   (done when source_mnt is moved)\n *\n *  NOTE: in the table below explains the semantics when a source mount\n *  of a given type is attached to a destination mount of a given type.\n * ---------------------------------------------------------------------------\n * |         BIND MOUNT OPERATION                                            |\n * |**************************************************************************\n * | source-->| shared        |       private  |       slave    | unbindable |\n * | dest     |               |                |                |            |\n * |   |      |               |                |                |            |\n * |   v      |               |                |                |            |\n * |**************************************************************************\n * |  shared  | shared (++)   |     shared (+) |     shared(+++)|  invalid   |\n * |          |               |                |                |            |\n * |non-shared| shared (+)    |      private   |      slave (*) |  invalid   |\n * ***************************************************************************\n * A bind operation clones the source mount and mounts the clone on the\n * destination mount.\n *\n * (++)  the cloned mount is propagated to all the mounts in the propagation\n * \t tree of the destination mount and the cloned mount is added to\n * \t the peer group of the source mount.\n * (+)   the cloned mount is created under the destination mount and is marked\n *       as shared. The cloned mount is added to the peer group of the source\n *       mount.\n * (+++) the mount is propagated to all the mounts in the propagation tree\n *       of the destination mount and the cloned mount is made slave\n *       of the same master as that of the source mount. The cloned mount\n *       is marked as 'shared and slave'.\n * (*)   the cloned mount is made a slave of the same master as that of the\n * \t source mount.\n *\n * ---------------------------------------------------------------------------\n * |         \t\tMOVE MOUNT OPERATION                                 |\n * |**************************************************************************\n * | source-->| shared        |       private  |       slave    | unbindable |\n * | dest     |               |                |                |            |\n * |   |      |               |                |                |            |\n * |   v      |               |                |                |            |\n * |**************************************************************************\n * |  shared  | shared (+)    |     shared (+) |    shared(+++) |  invalid   |\n * |          |               |                |                |            |\n * |non-shared| shared (+*)   |      private   |    slave (*)   | unbindable |\n * ***************************************************************************\n *\n * (+)  the mount is moved to the destination. And is then propagated to\n * \tall the mounts in the propagation tree of the destination mount.\n * (+*)  the mount is moved to the destination.\n * (+++)  the mount is moved to the destination and is then propagated to\n * \tall the mounts belonging to the destination mount's propagation tree.\n * \tthe mount is marked as 'shared and slave'.\n * (*)\tthe mount continues to be a slave at the new location.\n *\n * if the source mount is a tree, the operations explained above is\n * applied to each mount in the tree.\n * Must be called without spinlocks held, since this function can sleep\n * in allocations.\n */\nstatic int attach_recursive_mnt(struct mount *source_mnt,\n\t\t\tstruct mount *dest_mnt,\n\t\t\tstruct mountpoint *dest_mp,\n\t\t\tstruct path *parent_path)\n{\n\tHLIST_HEAD(tree_list);\n\tstruct mount *child, *p;\n\tstruct hlist_node *n;\n\tint err;\n\n\tif (IS_MNT_SHARED(dest_mnt)) {\n\t\terr = invent_group_ids(source_mnt, true);\n\t\tif (err)\n\t\t\tgoto out;\n\t\terr = propagate_mnt(dest_mnt, dest_mp, source_mnt, &tree_list);\n\t\tlock_mount_hash();\n\t\tif (err)\n\t\t\tgoto out_cleanup_ids;\n\t\tfor (p = source_mnt; p; p = next_mnt(p, source_mnt))\n\t\t\tset_mnt_shared(p);\n\t} else {\n\t\tlock_mount_hash();\n\t}\n\tif (parent_path) {\n\t\tdetach_mnt(source_mnt, parent_path);\n\t\tattach_mnt(source_mnt, dest_mnt, dest_mp);\n\t\ttouch_mnt_namespace(source_mnt->mnt_ns);\n\t} else {\n\t\tmnt_set_mountpoint(dest_mnt, dest_mp, source_mnt);\n\t\tcommit_tree(source_mnt, NULL);\n\t}\n\n\thlist_for_each_entry_safe(child, n, &tree_list, mnt_hash) {\n\t\tstruct mount *q;\n\t\thlist_del_init(&child->mnt_hash);\n\t\tq = __lookup_mnt_last(&child->mnt_parent->mnt,\n\t\t\t\t      child->mnt_mountpoint);\n\t\tcommit_tree(child, q);\n\t}\n\tunlock_mount_hash();\n\n\treturn 0;\n\n out_cleanup_ids:\n\twhile (!hlist_empty(&tree_list)) {\n\t\tchild = hlist_entry(tree_list.first, struct mount, mnt_hash);\n\t\tumount_tree(child, 0);\n\t}\n\tunlock_mount_hash();\n\tcleanup_group_ids(source_mnt, NULL);\n out:\n\treturn err;\n}\n\nstatic struct mountpoint *lock_mount(struct path *path)\n{\n\tstruct vfsmount *mnt;\n\tstruct dentry *dentry = path->dentry;\nretry:\n\tmutex_lock(&dentry->d_inode->i_mutex);\n\tif (unlikely(cant_mount(dentry))) {\n\t\tmutex_unlock(&dentry->d_inode->i_mutex);\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\tnamespace_lock();\n\tmnt = lookup_mnt(path);\n\tif (likely(!mnt)) {\n\t\tstruct mountpoint *mp = new_mountpoint(dentry);\n\t\tif (IS_ERR(mp)) {\n\t\t\tnamespace_unlock();\n\t\t\tmutex_unlock(&dentry->d_inode->i_mutex);\n\t\t\treturn mp;\n\t\t}\n\t\treturn mp;\n\t}\n\tnamespace_unlock();\n\tmutex_unlock(&path->dentry->d_inode->i_mutex);\n\tpath_put(path);\n\tpath->mnt = mnt;\n\tdentry = path->dentry = dget(mnt->mnt_root);\n\tgoto retry;\n}\n\nstatic void unlock_mount(struct mountpoint *where)\n{\n\tstruct dentry *dentry = where->m_dentry;\n\tput_mountpoint(where);\n\tnamespace_unlock();\n\tmutex_unlock(&dentry->d_inode->i_mutex);\n}\n\nstatic int graft_tree(struct mount *mnt, struct mount *p, struct mountpoint *mp)\n{\n\tif (mnt->mnt.mnt_sb->s_flags & MS_NOUSER)\n\t\treturn -EINVAL;\n\n\tif (S_ISDIR(mp->m_dentry->d_inode->i_mode) !=\n\t      S_ISDIR(mnt->mnt.mnt_root->d_inode->i_mode))\n\t\treturn -ENOTDIR;\n\n\treturn attach_recursive_mnt(mnt, p, mp, NULL);\n}\n\n/*\n * Sanity check the flags to change_mnt_propagation.\n */\n\nstatic int flags_to_propagation_type(int flags)\n{\n\tint type = flags & ~(MS_REC | MS_SILENT);\n\n\t/* Fail if any non-propagation flags are set */\n\tif (type & ~(MS_SHARED | MS_PRIVATE | MS_SLAVE | MS_UNBINDABLE))\n\t\treturn 0;\n\t/* Only one propagation flag should be set */\n\tif (!is_power_of_2(type))\n\t\treturn 0;\n\treturn type;\n}\n\n/*\n * recursively change the type of the mountpoint.\n */\nstatic int do_change_type(struct path *path, int flag)\n{\n\tstruct mount *m;\n\tstruct mount *mnt = real_mount(path->mnt);\n\tint recurse = flag & MS_REC;\n\tint type;\n\tint err = 0;\n\n\tif (path->dentry != path->mnt->mnt_root)\n\t\treturn -EINVAL;\n\n\ttype = flags_to_propagation_type(flag);\n\tif (!type)\n\t\treturn -EINVAL;\n\n\tnamespace_lock();\n\tif (type == MS_SHARED) {\n\t\terr = invent_group_ids(mnt, recurse);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t}\n\n\tlock_mount_hash();\n\tfor (m = mnt; m; m = (recurse ? next_mnt(m, mnt) : NULL))\n\t\tchange_mnt_propagation(m, type);\n\tunlock_mount_hash();\n\n out_unlock:\n\tnamespace_unlock();\n\treturn err;\n}\n\nstatic bool has_locked_children(struct mount *mnt, struct dentry *dentry)\n{\n\tstruct mount *child;\n\tlist_for_each_entry(child, &mnt->mnt_mounts, mnt_child) {\n\t\tif (!is_subdir(child->mnt_mountpoint, dentry))\n\t\t\tcontinue;\n\n\t\tif (child->mnt.mnt_flags & MNT_LOCKED)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n/*\n * do loopback mount.\n */\nstatic int do_loopback(struct path *path, const char *old_name,\n\t\t\t\tint recurse)\n{\n\tstruct path old_path;\n\tstruct mount *mnt = NULL, *old, *parent;\n\tstruct mountpoint *mp;\n\tint err;\n\tif (!old_name || !*old_name)\n\t\treturn -EINVAL;\n\terr = kern_path(old_name, LOOKUP_FOLLOW|LOOKUP_AUTOMOUNT, &old_path);\n\tif (err)\n\t\treturn err;\n\n\terr = -EINVAL;\n\tif (mnt_ns_loop(old_path.dentry))\n\t\tgoto out; \n\n\tmp = lock_mount(path);\n\terr = PTR_ERR(mp);\n\tif (IS_ERR(mp))\n\t\tgoto out;\n\n\told = real_mount(old_path.mnt);\n\tparent = real_mount(path->mnt);\n\n\terr = -EINVAL;\n\tif (IS_MNT_UNBINDABLE(old))\n\t\tgoto out2;\n\n\tif (!check_mnt(parent) || !check_mnt(old))\n\t\tgoto out2;\n\n\tif (!recurse && has_locked_children(old, old_path.dentry))\n\t\tgoto out2;\n\n\tif (recurse)\n\t\tmnt = copy_tree(old, old_path.dentry, CL_COPY_MNT_NS_FILE);\n\telse\n\t\tmnt = clone_mnt(old, old_path.dentry, 0);\n\n\tif (IS_ERR(mnt)) {\n\t\terr = PTR_ERR(mnt);\n\t\tgoto out2;\n\t}\n\n\tmnt->mnt.mnt_flags &= ~MNT_LOCKED;\n\n\terr = graft_tree(mnt, parent, mp);\n\tif (err) {\n\t\tlock_mount_hash();\n\t\tumount_tree(mnt, 0);\n\t\tunlock_mount_hash();\n\t}\nout2:\n\tunlock_mount(mp);\nout:\n\tpath_put(&old_path);\n\treturn err;\n}\n\nstatic int change_mount_flags(struct vfsmount *mnt, int ms_flags)\n{\n\tint error = 0;\n\tint readonly_request = 0;\n\n\tif (ms_flags & MS_RDONLY)\n\t\treadonly_request = 1;\n\tif (readonly_request == __mnt_is_readonly(mnt))\n\t\treturn 0;\n\n\tif (readonly_request)\n\t\terror = mnt_make_readonly(real_mount(mnt));\n\telse\n\t\t__mnt_unmake_readonly(real_mount(mnt));\n\treturn error;\n}\n\n/*\n * change filesystem flags. dir should be a physical root of filesystem.\n * If you've mounted a non-root directory somewhere and want to do remount\n * on it - tough luck.\n */\nstatic int do_remount(struct path *path, int flags, int mnt_flags,\n\t\t      void *data)\n{\n\tint err;\n\tstruct super_block *sb = path->mnt->mnt_sb;\n\tstruct mount *mnt = real_mount(path->mnt);\n\n\tif (!check_mnt(mnt))\n\t\treturn -EINVAL;\n\n\tif (path->dentry != path->mnt->mnt_root)\n\t\treturn -EINVAL;\n\n\t/* Don't allow changing of locked mnt flags.\n\t *\n\t * No locks need to be held here while testing the various\n\t * MNT_LOCK flags because those flags can never be cleared\n\t * once they are set.\n\t */\n\tif ((mnt->mnt.mnt_flags & MNT_LOCK_READONLY) &&\n\t    !(mnt_flags & MNT_READONLY)) {\n\t\treturn -EPERM;\n\t}\n\tif ((mnt->mnt.mnt_flags & MNT_LOCK_NODEV) &&\n\t    !(mnt_flags & MNT_NODEV)) {\n\t\treturn -EPERM;\n\t}\n\tif ((mnt->mnt.mnt_flags & MNT_LOCK_NOSUID) &&\n\t    !(mnt_flags & MNT_NOSUID)) {\n\t\treturn -EPERM;\n\t}\n\tif ((mnt->mnt.mnt_flags & MNT_LOCK_NOEXEC) &&\n\t    !(mnt_flags & MNT_NOEXEC)) {\n\t\treturn -EPERM;\n\t}\n\tif ((mnt->mnt.mnt_flags & MNT_LOCK_ATIME) &&\n\t    ((mnt->mnt.mnt_flags & MNT_ATIME_MASK) != (mnt_flags & MNT_ATIME_MASK))) {\n\t\treturn -EPERM;\n\t}\n\n\terr = security_sb_remount(sb, data);\n\tif (err)\n\t\treturn err;\n\n\tdown_write(&sb->s_umount);\n\tif (flags & MS_BIND)\n\t\terr = change_mount_flags(path->mnt, flags);\n\telse if (!capable(CAP_SYS_ADMIN))\n\t\terr = -EPERM;\n\telse\n\t\terr = do_remount_sb(sb, flags, data, 0);\n\tif (!err) {\n\t\tlock_mount_hash();\n\t\tmnt_flags |= mnt->mnt.mnt_flags & ~MNT_USER_SETTABLE_MASK;\n\t\tmnt->mnt.mnt_flags = mnt_flags;\n\t\ttouch_mnt_namespace(mnt->mnt_ns);\n\t\tunlock_mount_hash();\n\t}\n\tup_write(&sb->s_umount);\n\treturn err;\n}\n\nstatic inline int tree_contains_unbindable(struct mount *mnt)\n{\n\tstruct mount *p;\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tif (IS_MNT_UNBINDABLE(p))\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int do_move_mount(struct path *path, const char *old_name)\n{\n\tstruct path old_path, parent_path;\n\tstruct mount *p;\n\tstruct mount *old;\n\tstruct mountpoint *mp;\n\tint err;\n\tif (!old_name || !*old_name)\n\t\treturn -EINVAL;\n\terr = kern_path(old_name, LOOKUP_FOLLOW, &old_path);\n\tif (err)\n\t\treturn err;\n\n\tmp = lock_mount(path);\n\terr = PTR_ERR(mp);\n\tif (IS_ERR(mp))\n\t\tgoto out;\n\n\told = real_mount(old_path.mnt);\n\tp = real_mount(path->mnt);\n\n\terr = -EINVAL;\n\tif (!check_mnt(p) || !check_mnt(old))\n\t\tgoto out1;\n\n\tif (old->mnt.mnt_flags & MNT_LOCKED)\n\t\tgoto out1;\n\n\terr = -EINVAL;\n\tif (old_path.dentry != old_path.mnt->mnt_root)\n\t\tgoto out1;\n\n\tif (!mnt_has_parent(old))\n\t\tgoto out1;\n\n\tif (S_ISDIR(path->dentry->d_inode->i_mode) !=\n\t      S_ISDIR(old_path.dentry->d_inode->i_mode))\n\t\tgoto out1;\n\t/*\n\t * Don't move a mount residing in a shared parent.\n\t */\n\tif (IS_MNT_SHARED(old->mnt_parent))\n\t\tgoto out1;\n\t/*\n\t * Don't move a mount tree containing unbindable mounts to a destination\n\t * mount which is shared.\n\t */\n\tif (IS_MNT_SHARED(p) && tree_contains_unbindable(old))\n\t\tgoto out1;\n\terr = -ELOOP;\n\tfor (; mnt_has_parent(p); p = p->mnt_parent)\n\t\tif (p == old)\n\t\t\tgoto out1;\n\n\terr = attach_recursive_mnt(old, real_mount(path->mnt), mp, &parent_path);\n\tif (err)\n\t\tgoto out1;\n\n\t/* if the mount is moved, it should no longer be expire\n\t * automatically */\n\tlist_del_init(&old->mnt_expire);\nout1:\n\tunlock_mount(mp);\nout:\n\tif (!err)\n\t\tpath_put(&parent_path);\n\tpath_put(&old_path);\n\treturn err;\n}\n\nstatic struct vfsmount *fs_set_subtype(struct vfsmount *mnt, const char *fstype)\n{\n\tint err;\n\tconst char *subtype = strchr(fstype, '.');\n\tif (subtype) {\n\t\tsubtype++;\n\t\terr = -EINVAL;\n\t\tif (!subtype[0])\n\t\t\tgoto err;\n\t} else\n\t\tsubtype = \"\";\n\n\tmnt->mnt_sb->s_subtype = kstrdup(subtype, GFP_KERNEL);\n\terr = -ENOMEM;\n\tif (!mnt->mnt_sb->s_subtype)\n\t\tgoto err;\n\treturn mnt;\n\n err:\n\tmntput(mnt);\n\treturn ERR_PTR(err);\n}\n\n/*\n * add a mount into a namespace's mount tree\n */\nstatic int do_add_mount(struct mount *newmnt, struct path *path, int mnt_flags)\n{\n\tstruct mountpoint *mp;\n\tstruct mount *parent;\n\tint err;\n\n\tmnt_flags &= ~MNT_INTERNAL_FLAGS;\n\n\tmp = lock_mount(path);\n\tif (IS_ERR(mp))\n\t\treturn PTR_ERR(mp);\n\n\tparent = real_mount(path->mnt);\n\terr = -EINVAL;\n\tif (unlikely(!check_mnt(parent))) {\n\t\t/* that's acceptable only for automounts done in private ns */\n\t\tif (!(mnt_flags & MNT_SHRINKABLE))\n\t\t\tgoto unlock;\n\t\t/* ... and for those we'd better have mountpoint still alive */\n\t\tif (!parent->mnt_ns)\n\t\t\tgoto unlock;\n\t}\n\n\t/* Refuse the same filesystem on the same mount point */\n\terr = -EBUSY;\n\tif (path->mnt->mnt_sb == newmnt->mnt.mnt_sb &&\n\t    path->mnt->mnt_root == path->dentry)\n\t\tgoto unlock;\n\n\terr = -EINVAL;\n\tif (S_ISLNK(newmnt->mnt.mnt_root->d_inode->i_mode))\n\t\tgoto unlock;\n\n\tnewmnt->mnt.mnt_flags = mnt_flags;\n\terr = graft_tree(newmnt, parent, mp);\n\nunlock:\n\tunlock_mount(mp);\n\treturn err;\n}\n\n/*\n * create a new mount for userspace and request it to be added into the\n * namespace's tree\n */\nstatic int do_new_mount(struct path *path, const char *fstype, int flags,\n\t\t\tint mnt_flags, const char *name, void *data)\n{\n\tstruct file_system_type *type;\n\tstruct user_namespace *user_ns = current->nsproxy->mnt_ns->user_ns;\n\tstruct vfsmount *mnt;\n\tint err;\n\n\tif (!fstype)\n\t\treturn -EINVAL;\n\n\ttype = get_fs_type(fstype);\n\tif (!type)\n\t\treturn -ENODEV;\n\n\tif (user_ns != &init_user_ns) {\n\t\tif (!(type->fs_flags & FS_USERNS_MOUNT)) {\n\t\t\tput_filesystem(type);\n\t\t\treturn -EPERM;\n\t\t}\n\t\t/* Only in special cases allow devices from mounts\n\t\t * created outside the initial user namespace.\n\t\t */\n\t\tif (!(type->fs_flags & FS_USERNS_DEV_MOUNT)) {\n\t\t\tflags |= MS_NODEV;\n\t\t\tmnt_flags |= MNT_NODEV | MNT_LOCK_NODEV;\n\t\t}\n\t}\n\n\tmnt = vfs_kern_mount(type, flags, name, data);\n\tif (!IS_ERR(mnt) && (type->fs_flags & FS_HAS_SUBTYPE) &&\n\t    !mnt->mnt_sb->s_subtype)\n\t\tmnt = fs_set_subtype(mnt, fstype);\n\n\tput_filesystem(type);\n\tif (IS_ERR(mnt))\n\t\treturn PTR_ERR(mnt);\n\n\terr = do_add_mount(real_mount(mnt), path, mnt_flags);\n\tif (err)\n\t\tmntput(mnt);\n\treturn err;\n}\n\nint finish_automount(struct vfsmount *m, struct path *path)\n{\n\tstruct mount *mnt = real_mount(m);\n\tint err;\n\t/* The new mount record should have at least 2 refs to prevent it being\n\t * expired before we get a chance to add it\n\t */\n\tBUG_ON(mnt_get_count(mnt) < 2);\n\n\tif (m->mnt_sb == path->mnt->mnt_sb &&\n\t    m->mnt_root == path->dentry) {\n\t\terr = -ELOOP;\n\t\tgoto fail;\n\t}\n\n\terr = do_add_mount(mnt, path, path->mnt->mnt_flags | MNT_SHRINKABLE);\n\tif (!err)\n\t\treturn 0;\nfail:\n\t/* remove m from any expiration list it may be on */\n\tif (!list_empty(&mnt->mnt_expire)) {\n\t\tnamespace_lock();\n\t\tlist_del_init(&mnt->mnt_expire);\n\t\tnamespace_unlock();\n\t}\n\tmntput(m);\n\tmntput(m);\n\treturn err;\n}\n\n/**\n * mnt_set_expiry - Put a mount on an expiration list\n * @mnt: The mount to list.\n * @expiry_list: The list to add the mount to.\n */\nvoid mnt_set_expiry(struct vfsmount *mnt, struct list_head *expiry_list)\n{\n\tnamespace_lock();\n\n\tlist_add_tail(&real_mount(mnt)->mnt_expire, expiry_list);\n\n\tnamespace_unlock();\n}\nEXPORT_SYMBOL(mnt_set_expiry);\n\n/*\n * process a list of expirable mountpoints with the intent of discarding any\n * mountpoints that aren't in use and haven't been touched since last we came\n * here\n */\nvoid mark_mounts_for_expiry(struct list_head *mounts)\n{\n\tstruct mount *mnt, *next;\n\tLIST_HEAD(graveyard);\n\n\tif (list_empty(mounts))\n\t\treturn;\n\n\tnamespace_lock();\n\tlock_mount_hash();\n\n\t/* extract from the expiration list every vfsmount that matches the\n\t * following criteria:\n\t * - only referenced by its parent vfsmount\n\t * - still marked for expiry (marked on the last call here; marks are\n\t *   cleared by mntput())\n\t */\n\tlist_for_each_entry_safe(mnt, next, mounts, mnt_expire) {\n\t\tif (!xchg(&mnt->mnt_expiry_mark, 1) ||\n\t\t\tpropagate_mount_busy(mnt, 1))\n\t\t\tcontinue;\n\t\tlist_move(&mnt->mnt_expire, &graveyard);\n\t}\n\twhile (!list_empty(&graveyard)) {\n\t\tmnt = list_first_entry(&graveyard, struct mount, mnt_expire);\n\t\ttouch_mnt_namespace(mnt->mnt_ns);\n\t\tumount_tree(mnt, 1);\n\t}\n\tunlock_mount_hash();\n\tnamespace_unlock();\n}\n\nEXPORT_SYMBOL_GPL(mark_mounts_for_expiry);\n\n/*\n * Ripoff of 'select_parent()'\n *\n * search the list of submounts for a given mountpoint, and move any\n * shrinkable submounts to the 'graveyard' list.\n */\nstatic int select_submounts(struct mount *parent, struct list_head *graveyard)\n{\n\tstruct mount *this_parent = parent;\n\tstruct list_head *next;\n\tint found = 0;\n\nrepeat:\n\tnext = this_parent->mnt_mounts.next;\nresume:\n\twhile (next != &this_parent->mnt_mounts) {\n\t\tstruct list_head *tmp = next;\n\t\tstruct mount *mnt = list_entry(tmp, struct mount, mnt_child);\n\n\t\tnext = tmp->next;\n\t\tif (!(mnt->mnt.mnt_flags & MNT_SHRINKABLE))\n\t\t\tcontinue;\n\t\t/*\n\t\t * Descend a level if the d_mounts list is non-empty.\n\t\t */\n\t\tif (!list_empty(&mnt->mnt_mounts)) {\n\t\t\tthis_parent = mnt;\n\t\t\tgoto repeat;\n\t\t}\n\n\t\tif (!propagate_mount_busy(mnt, 1)) {\n\t\t\tlist_move_tail(&mnt->mnt_expire, graveyard);\n\t\t\tfound++;\n\t\t}\n\t}\n\t/*\n\t * All done at this level ... ascend and resume the search\n\t */\n\tif (this_parent != parent) {\n\t\tnext = this_parent->mnt_child.next;\n\t\tthis_parent = this_parent->mnt_parent;\n\t\tgoto resume;\n\t}\n\treturn found;\n}\n\n/*\n * process a list of expirable mountpoints with the intent of discarding any\n * submounts of a specific parent mountpoint\n *\n * mount_lock must be held for write\n */\nstatic void shrink_submounts(struct mount *mnt)\n{\n\tLIST_HEAD(graveyard);\n\tstruct mount *m;\n\n\t/* extract submounts of 'mountpoint' from the expiration list */\n\twhile (select_submounts(mnt, &graveyard)) {\n\t\twhile (!list_empty(&graveyard)) {\n\t\t\tm = list_first_entry(&graveyard, struct mount,\n\t\t\t\t\t\tmnt_expire);\n\t\t\ttouch_mnt_namespace(m->mnt_ns);\n\t\t\tumount_tree(m, 1);\n\t\t}\n\t}\n}\n\n/*\n * Some copy_from_user() implementations do not return the exact number of\n * bytes remaining to copy on a fault.  But copy_mount_options() requires that.\n * Note that this function differs from copy_from_user() in that it will oops\n * on bad values of `to', rather than returning a short copy.\n */\nstatic long exact_copy_from_user(void *to, const void __user * from,\n\t\t\t\t unsigned long n)\n{\n\tchar *t = to;\n\tconst char __user *f = from;\n\tchar c;\n\n\tif (!access_ok(VERIFY_READ, from, n))\n\t\treturn n;\n\n\twhile (n) {\n\t\tif (__get_user(c, f)) {\n\t\t\tmemset(t, 0, n);\n\t\t\tbreak;\n\t\t}\n\t\t*t++ = c;\n\t\tf++;\n\t\tn--;\n\t}\n\treturn n;\n}\n\nint copy_mount_options(const void __user * data, unsigned long *where)\n{\n\tint i;\n\tunsigned long page;\n\tunsigned long size;\n\n\t*where = 0;\n\tif (!data)\n\t\treturn 0;\n\n\tif (!(page = __get_free_page(GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\n\t/* We only care that *some* data at the address the user\n\t * gave us is valid.  Just in case, we'll zero\n\t * the remainder of the page.\n\t */\n\t/* copy_from_user cannot cross TASK_SIZE ! */\n\tsize = TASK_SIZE - (unsigned long)data;\n\tif (size > PAGE_SIZE)\n\t\tsize = PAGE_SIZE;\n\n\ti = size - exact_copy_from_user((void *)page, data, size);\n\tif (!i) {\n\t\tfree_page(page);\n\t\treturn -EFAULT;\n\t}\n\tif (i != PAGE_SIZE)\n\t\tmemset((char *)page + i, 0, PAGE_SIZE - i);\n\t*where = page;\n\treturn 0;\n}\n\nint copy_mount_string(const void __user *data, char **where)\n{\n\tchar *tmp;\n\n\tif (!data) {\n\t\t*where = NULL;\n\t\treturn 0;\n\t}\n\n\ttmp = strndup_user(data, PAGE_SIZE);\n\tif (IS_ERR(tmp))\n\t\treturn PTR_ERR(tmp);\n\n\t*where = tmp;\n\treturn 0;\n}\n\n/*\n * Flags is a 32-bit value that allows up to 31 non-fs dependent flags to\n * be given to the mount() call (ie: read-only, no-dev, no-suid etc).\n *\n * data is a (void *) that can point to any structure up to\n * PAGE_SIZE-1 bytes, which can contain arbitrary fs-dependent\n * information (or be NULL).\n *\n * Pre-0.97 versions of mount() didn't have a flags word.\n * When the flags word was introduced its top half was required\n * to have the magic value 0xC0ED, and this remained so until 2.4.0-test9.\n * Therefore, if this magic number is present, it carries no information\n * and must be discarded.\n */\nlong do_mount(const char *dev_name, const char *dir_name,\n\t\tconst char *type_page, unsigned long flags, void *data_page)\n{\n\tstruct path path;\n\tint retval = 0;\n\tint mnt_flags = 0;\n\n\t/* Discard magic */\n\tif ((flags & MS_MGC_MSK) == MS_MGC_VAL)\n\t\tflags &= ~MS_MGC_MSK;\n\n\t/* Basic sanity checks */\n\n\tif (!dir_name || !*dir_name || !memchr(dir_name, 0, PAGE_SIZE))\n\t\treturn -EINVAL;\n\n\tif (data_page)\n\t\t((char *)data_page)[PAGE_SIZE - 1] = 0;\n\n\t/* ... and get the mountpoint */\n\tretval = kern_path(dir_name, LOOKUP_FOLLOW, &path);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = security_sb_mount(dev_name, &path,\n\t\t\t\t   type_page, flags, data_page);\n\tif (!retval && !may_mount())\n\t\tretval = -EPERM;\n\tif (retval)\n\t\tgoto dput_out;\n\n\t/* Default to relatime unless overriden */\n\tif (!(flags & MS_NOATIME))\n\t\tmnt_flags |= MNT_RELATIME;\n\n\t/* Separate the per-mountpoint flags */\n\tif (flags & MS_NOSUID)\n\t\tmnt_flags |= MNT_NOSUID;\n\tif (flags & MS_NODEV)\n\t\tmnt_flags |= MNT_NODEV;\n\tif (flags & MS_NOEXEC)\n\t\tmnt_flags |= MNT_NOEXEC;\n\tif (flags & MS_NOATIME)\n\t\tmnt_flags |= MNT_NOATIME;\n\tif (flags & MS_NODIRATIME)\n\t\tmnt_flags |= MNT_NODIRATIME;\n\tif (flags & MS_STRICTATIME)\n\t\tmnt_flags &= ~(MNT_RELATIME | MNT_NOATIME);\n\tif (flags & MS_RDONLY)\n\t\tmnt_flags |= MNT_READONLY;\n\n\tflags &= ~(MS_NOSUID | MS_NOEXEC | MS_NODEV | MS_ACTIVE | MS_BORN |\n\t\t   MS_NOATIME | MS_NODIRATIME | MS_RELATIME| MS_KERNMOUNT |\n\t\t   MS_STRICTATIME);\n\n\tif (flags & MS_REMOUNT)\n\t\tretval = do_remount(&path, flags & ~MS_REMOUNT, mnt_flags,\n\t\t\t\t    data_page);\n\telse if (flags & MS_BIND)\n\t\tretval = do_loopback(&path, dev_name, flags & MS_REC);\n\telse if (flags & (MS_SHARED | MS_PRIVATE | MS_SLAVE | MS_UNBINDABLE))\n\t\tretval = do_change_type(&path, flags);\n\telse if (flags & MS_MOVE)\n\t\tretval = do_move_mount(&path, dev_name);\n\telse\n\t\tretval = do_new_mount(&path, type_page, flags, mnt_flags,\n\t\t\t\t      dev_name, data_page);\ndput_out:\n\tpath_put(&path);\n\treturn retval;\n}\n\nstatic void free_mnt_ns(struct mnt_namespace *ns)\n{\n\tproc_free_inum(ns->proc_inum);\n\tput_user_ns(ns->user_ns);\n\tkfree(ns);\n}\n\n/*\n * Assign a sequence number so we can detect when we attempt to bind\n * mount a reference to an older mount namespace into the current\n * mount namespace, preventing reference counting loops.  A 64bit\n * number incrementing at 10Ghz will take 12,427 years to wrap which\n * is effectively never, so we can ignore the possibility.\n */\nstatic atomic64_t mnt_ns_seq = ATOMIC64_INIT(1);\n\nstatic struct mnt_namespace *alloc_mnt_ns(struct user_namespace *user_ns)\n{\n\tstruct mnt_namespace *new_ns;\n\tint ret;\n\n\tnew_ns = kmalloc(sizeof(struct mnt_namespace), GFP_KERNEL);\n\tif (!new_ns)\n\t\treturn ERR_PTR(-ENOMEM);\n\tret = proc_alloc_inum(&new_ns->proc_inum);\n\tif (ret) {\n\t\tkfree(new_ns);\n\t\treturn ERR_PTR(ret);\n\t}\n\tnew_ns->seq = atomic64_add_return(1, &mnt_ns_seq);\n\tatomic_set(&new_ns->count, 1);\n\tnew_ns->root = NULL;\n\tINIT_LIST_HEAD(&new_ns->list);\n\tinit_waitqueue_head(&new_ns->poll);\n\tnew_ns->event = 0;\n\tnew_ns->user_ns = get_user_ns(user_ns);\n\treturn new_ns;\n}\n\nstruct mnt_namespace *copy_mnt_ns(unsigned long flags, struct mnt_namespace *ns,\n\t\tstruct user_namespace *user_ns, struct fs_struct *new_fs)\n{\n\tstruct mnt_namespace *new_ns;\n\tstruct vfsmount *rootmnt = NULL, *pwdmnt = NULL;\n\tstruct mount *p, *q;\n\tstruct mount *old;\n\tstruct mount *new;\n\tint copy_flags;\n\n\tBUG_ON(!ns);\n\n\tif (likely(!(flags & CLONE_NEWNS))) {\n\t\tget_mnt_ns(ns);\n\t\treturn ns;\n\t}\n\n\told = ns->root;\n\n\tnew_ns = alloc_mnt_ns(user_ns);\n\tif (IS_ERR(new_ns))\n\t\treturn new_ns;\n\n\tnamespace_lock();\n\t/* First pass: copy the tree topology */\n\tcopy_flags = CL_COPY_UNBINDABLE | CL_EXPIRE;\n\tif (user_ns != ns->user_ns)\n\t\tcopy_flags |= CL_SHARED_TO_SLAVE | CL_UNPRIVILEGED;\n\tnew = copy_tree(old, old->mnt.mnt_root, copy_flags);\n\tif (IS_ERR(new)) {\n\t\tnamespace_unlock();\n\t\tfree_mnt_ns(new_ns);\n\t\treturn ERR_CAST(new);\n\t}\n\tnew_ns->root = new;\n\tlist_add_tail(&new_ns->list, &new->mnt_list);\n\n\t/*\n\t * Second pass: switch the tsk->fs->* elements and mark new vfsmounts\n\t * as belonging to new namespace.  We have already acquired a private\n\t * fs_struct, so tsk->fs->lock is not needed.\n\t */\n\tp = old;\n\tq = new;\n\twhile (p) {\n\t\tq->mnt_ns = new_ns;\n\t\tif (new_fs) {\n\t\t\tif (&p->mnt == new_fs->root.mnt) {\n\t\t\t\tnew_fs->root.mnt = mntget(&q->mnt);\n\t\t\t\trootmnt = &p->mnt;\n\t\t\t}\n\t\t\tif (&p->mnt == new_fs->pwd.mnt) {\n\t\t\t\tnew_fs->pwd.mnt = mntget(&q->mnt);\n\t\t\t\tpwdmnt = &p->mnt;\n\t\t\t}\n\t\t}\n\t\tp = next_mnt(p, old);\n\t\tq = next_mnt(q, new);\n\t\tif (!q)\n\t\t\tbreak;\n\t\twhile (p->mnt.mnt_root != q->mnt.mnt_root)\n\t\t\tp = next_mnt(p, old);\n\t}\n\tnamespace_unlock();\n\n\tif (rootmnt)\n\t\tmntput(rootmnt);\n\tif (pwdmnt)\n\t\tmntput(pwdmnt);\n\n\treturn new_ns;\n}\n\n/**\n * create_mnt_ns - creates a private namespace and adds a root filesystem\n * @mnt: pointer to the new root filesystem mountpoint\n */\nstatic struct mnt_namespace *create_mnt_ns(struct vfsmount *m)\n{\n\tstruct mnt_namespace *new_ns = alloc_mnt_ns(&init_user_ns);\n\tif (!IS_ERR(new_ns)) {\n\t\tstruct mount *mnt = real_mount(m);\n\t\tmnt->mnt_ns = new_ns;\n\t\tnew_ns->root = mnt;\n\t\tlist_add(&mnt->mnt_list, &new_ns->list);\n\t} else {\n\t\tmntput(m);\n\t}\n\treturn new_ns;\n}\n\nstruct dentry *mount_subtree(struct vfsmount *mnt, const char *name)\n{\n\tstruct mnt_namespace *ns;\n\tstruct super_block *s;\n\tstruct path path;\n\tint err;\n\n\tns = create_mnt_ns(mnt);\n\tif (IS_ERR(ns))\n\t\treturn ERR_CAST(ns);\n\n\terr = vfs_path_lookup(mnt->mnt_root, mnt,\n\t\t\tname, LOOKUP_FOLLOW|LOOKUP_AUTOMOUNT, &path);\n\n\tput_mnt_ns(ns);\n\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\t/* trade a vfsmount reference for active sb one */\n\ts = path.mnt->mnt_sb;\n\tatomic_inc(&s->s_active);\n\tmntput(path.mnt);\n\t/* lock the sucker */\n\tdown_write(&s->s_umount);\n\t/* ... and return the root of (sub)tree on it */\n\treturn path.dentry;\n}\nEXPORT_SYMBOL(mount_subtree);\n\nSYSCALL_DEFINE5(mount, char __user *, dev_name, char __user *, dir_name,\n\t\tchar __user *, type, unsigned long, flags, void __user *, data)\n{\n\tint ret;\n\tchar *kernel_type;\n\tstruct filename *kernel_dir;\n\tchar *kernel_dev;\n\tunsigned long data_page;\n\n\tret = copy_mount_string(type, &kernel_type);\n\tif (ret < 0)\n\t\tgoto out_type;\n\n\tkernel_dir = getname(dir_name);\n\tif (IS_ERR(kernel_dir)) {\n\t\tret = PTR_ERR(kernel_dir);\n\t\tgoto out_dir;\n\t}\n\n\tret = copy_mount_string(dev_name, &kernel_dev);\n\tif (ret < 0)\n\t\tgoto out_dev;\n\n\tret = copy_mount_options(data, &data_page);\n\tif (ret < 0)\n\t\tgoto out_data;\n\n\tret = do_mount(kernel_dev, kernel_dir->name, kernel_type, flags,\n\t\t(void *) data_page);\n\n\tfree_page(data_page);\nout_data:\n\tkfree(kernel_dev);\nout_dev:\n\tputname(kernel_dir);\nout_dir:\n\tkfree(kernel_type);\nout_type:\n\treturn ret;\n}\n\n/*\n * Return true if path is reachable from root\n *\n * namespace_sem or mount_lock is held\n */\nbool is_path_reachable(struct mount *mnt, struct dentry *dentry,\n\t\t\t const struct path *root)\n{\n\twhile (&mnt->mnt != root->mnt && mnt_has_parent(mnt)) {\n\t\tdentry = mnt->mnt_mountpoint;\n\t\tmnt = mnt->mnt_parent;\n\t}\n\treturn &mnt->mnt == root->mnt && is_subdir(dentry, root->dentry);\n}\n\nint path_is_under(struct path *path1, struct path *path2)\n{\n\tint res;\n\tread_seqlock_excl(&mount_lock);\n\tres = is_path_reachable(real_mount(path1->mnt), path1->dentry, path2);\n\tread_sequnlock_excl(&mount_lock);\n\treturn res;\n}\nEXPORT_SYMBOL(path_is_under);\n\n/*\n * pivot_root Semantics:\n * Moves the root file system of the current process to the directory put_old,\n * makes new_root as the new root file system of the current process, and sets\n * root/cwd of all processes which had them on the current root to new_root.\n *\n * Restrictions:\n * The new_root and put_old must be directories, and  must not be on the\n * same file  system as the current process root. The put_old  must  be\n * underneath new_root,  i.e. adding a non-zero number of /.. to the string\n * pointed to by put_old must yield the same directory as new_root. No other\n * file system may be mounted on put_old. After all, new_root is a mountpoint.\n *\n * Also, the current root cannot be on the 'rootfs' (initial ramfs) filesystem.\n * See Documentation/filesystems/ramfs-rootfs-initramfs.txt for alternatives\n * in this situation.\n *\n * Notes:\n *  - we don't move root/cwd if they are not at the root (reason: if something\n *    cared enough to change them, it's probably wrong to force them elsewhere)\n *  - it's okay to pick a root that isn't the root of a file system, e.g.\n *    /nfs/my_root where /nfs is the mount point. It must be a mountpoint,\n *    though, so you may need to say mount --bind /nfs/my_root /nfs/my_root\n *    first.\n */\nSYSCALL_DEFINE2(pivot_root, const char __user *, new_root,\n\t\tconst char __user *, put_old)\n{\n\tstruct path new, old, parent_path, root_parent, root;\n\tstruct mount *new_mnt, *root_mnt, *old_mnt;\n\tstruct mountpoint *old_mp, *root_mp;\n\tint error;\n\n\tif (!may_mount())\n\t\treturn -EPERM;\n\n\terror = user_path_dir(new_root, &new);\n\tif (error)\n\t\tgoto out0;\n\n\terror = user_path_dir(put_old, &old);\n\tif (error)\n\t\tgoto out1;\n\n\terror = security_sb_pivotroot(&old, &new);\n\tif (error)\n\t\tgoto out2;\n\n\tget_fs_root(current->fs, &root);\n\told_mp = lock_mount(&old);\n\terror = PTR_ERR(old_mp);\n\tif (IS_ERR(old_mp))\n\t\tgoto out3;\n\n\terror = -EINVAL;\n\tnew_mnt = real_mount(new.mnt);\n\troot_mnt = real_mount(root.mnt);\n\told_mnt = real_mount(old.mnt);\n\tif (IS_MNT_SHARED(old_mnt) ||\n\t\tIS_MNT_SHARED(new_mnt->mnt_parent) ||\n\t\tIS_MNT_SHARED(root_mnt->mnt_parent))\n\t\tgoto out4;\n\tif (!check_mnt(root_mnt) || !check_mnt(new_mnt))\n\t\tgoto out4;\n\tif (new_mnt->mnt.mnt_flags & MNT_LOCKED)\n\t\tgoto out4;\n\terror = -ENOENT;\n\tif (d_unlinked(new.dentry))\n\t\tgoto out4;\n\terror = -EBUSY;\n\tif (new_mnt == root_mnt || old_mnt == root_mnt)\n\t\tgoto out4; /* loop, on the same file system  */\n\terror = -EINVAL;\n\tif (root.mnt->mnt_root != root.dentry)\n\t\tgoto out4; /* not a mountpoint */\n\tif (!mnt_has_parent(root_mnt))\n\t\tgoto out4; /* not attached */\n\troot_mp = root_mnt->mnt_mp;\n\tif (new.mnt->mnt_root != new.dentry)\n\t\tgoto out4; /* not a mountpoint */\n\tif (!mnt_has_parent(new_mnt))\n\t\tgoto out4; /* not attached */\n\t/* make sure we can reach put_old from new_root */\n\tif (!is_path_reachable(old_mnt, old.dentry, &new))\n\t\tgoto out4;\n\troot_mp->m_count++; /* pin it so it won't go away */\n\tlock_mount_hash();\n\tdetach_mnt(new_mnt, &parent_path);\n\tdetach_mnt(root_mnt, &root_parent);\n\tif (root_mnt->mnt.mnt_flags & MNT_LOCKED) {\n\t\tnew_mnt->mnt.mnt_flags |= MNT_LOCKED;\n\t\troot_mnt->mnt.mnt_flags &= ~MNT_LOCKED;\n\t}\n\t/* mount old root on put_old */\n\tattach_mnt(root_mnt, old_mnt, old_mp);\n\t/* mount new_root on / */\n\tattach_mnt(new_mnt, real_mount(root_parent.mnt), root_mp);\n\ttouch_mnt_namespace(current->nsproxy->mnt_ns);\n\tunlock_mount_hash();\n\tchroot_fs_refs(&root, &new);\n\tput_mountpoint(root_mp);\n\terror = 0;\nout4:\n\tunlock_mount(old_mp);\n\tif (!error) {\n\t\tpath_put(&root_parent);\n\t\tpath_put(&parent_path);\n\t}\nout3:\n\tpath_put(&root);\nout2:\n\tpath_put(&old);\nout1:\n\tpath_put(&new);\nout0:\n\treturn error;\n}\n\nstatic void __init init_mount_tree(void)\n{\n\tstruct vfsmount *mnt;\n\tstruct mnt_namespace *ns;\n\tstruct path root;\n\tstruct file_system_type *type;\n\n\ttype = get_fs_type(\"rootfs\");\n\tif (!type)\n\t\tpanic(\"Can't find rootfs type\");\n\tmnt = vfs_kern_mount(type, 0, \"rootfs\", NULL);\n\tput_filesystem(type);\n\tif (IS_ERR(mnt))\n\t\tpanic(\"Can't create rootfs\");\n\n\tns = create_mnt_ns(mnt);\n\tif (IS_ERR(ns))\n\t\tpanic(\"Can't allocate initial namespace\");\n\n\tinit_task.nsproxy->mnt_ns = ns;\n\tget_mnt_ns(ns);\n\n\troot.mnt = mnt;\n\troot.dentry = mnt->mnt_root;\n\n\tset_fs_pwd(current->fs, &root);\n\tset_fs_root(current->fs, &root);\n}\n\nvoid __init mnt_init(void)\n{\n\tunsigned u;\n\tint err;\n\n\tmnt_cache = kmem_cache_create(\"mnt_cache\", sizeof(struct mount),\n\t\t\t0, SLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL);\n\n\tmount_hashtable = alloc_large_system_hash(\"Mount-cache\",\n\t\t\t\tsizeof(struct hlist_head),\n\t\t\t\tmhash_entries, 19,\n\t\t\t\t0,\n\t\t\t\t&m_hash_shift, &m_hash_mask, 0, 0);\n\tmountpoint_hashtable = alloc_large_system_hash(\"Mountpoint-cache\",\n\t\t\t\tsizeof(struct hlist_head),\n\t\t\t\tmphash_entries, 19,\n\t\t\t\t0,\n\t\t\t\t&mp_hash_shift, &mp_hash_mask, 0, 0);\n\n\tif (!mount_hashtable || !mountpoint_hashtable)\n\t\tpanic(\"Failed to allocate mount hash table\\n\");\n\n\tfor (u = 0; u <= m_hash_mask; u++)\n\t\tINIT_HLIST_HEAD(&mount_hashtable[u]);\n\tfor (u = 0; u <= mp_hash_mask; u++)\n\t\tINIT_HLIST_HEAD(&mountpoint_hashtable[u]);\n\n\tkernfs_init();\n\n\terr = sysfs_init();\n\tif (err)\n\t\tprintk(KERN_WARNING \"%s: sysfs_init error: %d\\n\",\n\t\t\t__func__, err);\n\tfs_kobj = kobject_create_and_add(\"fs\", NULL);\n\tif (!fs_kobj)\n\t\tprintk(KERN_WARNING \"%s: kobj create error\\n\", __func__);\n\tinit_rootfs();\n\tinit_mount_tree();\n}\n\nvoid put_mnt_ns(struct mnt_namespace *ns)\n{\n\tif (!atomic_dec_and_test(&ns->count))\n\t\treturn;\n\tdrop_collected_mounts(&ns->root->mnt);\n\tfree_mnt_ns(ns);\n}\n\nstruct vfsmount *kern_mount_data(struct file_system_type *type, void *data)\n{\n\tstruct vfsmount *mnt;\n\tmnt = vfs_kern_mount(type, MS_KERNMOUNT, type->name, data);\n\tif (!IS_ERR(mnt)) {\n\t\t/*\n\t\t * it is a longterm mount, don't release mnt until\n\t\t * we unmount before file sys is unregistered\n\t\t*/\n\t\treal_mount(mnt)->mnt_ns = MNT_NS_INTERNAL;\n\t}\n\treturn mnt;\n}\nEXPORT_SYMBOL_GPL(kern_mount_data);\n\nvoid kern_unmount(struct vfsmount *mnt)\n{\n\t/* release long term mount so mount point can be released */\n\tif (!IS_ERR_OR_NULL(mnt)) {\n\t\treal_mount(mnt)->mnt_ns = NULL;\n\t\tsynchronize_rcu();\t/* yecchhh... */\n\t\tmntput(mnt);\n\t}\n}\nEXPORT_SYMBOL(kern_unmount);\n\nbool our_mnt(struct vfsmount *mnt)\n{\n\treturn check_mnt(real_mount(mnt));\n}\n\nbool current_chrooted(void)\n{\n\t/* Does the current process have a non-standard root */\n\tstruct path ns_root;\n\tstruct path fs_root;\n\tbool chrooted;\n\n\t/* Find the namespace root */\n\tns_root.mnt = &current->nsproxy->mnt_ns->root->mnt;\n\tns_root.dentry = ns_root.mnt->mnt_root;\n\tpath_get(&ns_root);\n\twhile (d_mountpoint(ns_root.dentry) && follow_down_one(&ns_root))\n\t\t;\n\n\tget_fs_root(current->fs, &fs_root);\n\n\tchrooted = !path_equal(&fs_root, &ns_root);\n\n\tpath_put(&fs_root);\n\tpath_put(&ns_root);\n\n\treturn chrooted;\n}\n\nbool fs_fully_visible(struct file_system_type *type)\n{\n\tstruct mnt_namespace *ns = current->nsproxy->mnt_ns;\n\tstruct mount *mnt;\n\tbool visible = false;\n\n\tif (unlikely(!ns))\n\t\treturn false;\n\n\tdown_read(&namespace_sem);\n\tlist_for_each_entry(mnt, &ns->list, mnt_list) {\n\t\tstruct mount *child;\n\t\tif (mnt->mnt.mnt_sb->s_type != type)\n\t\t\tcontinue;\n\n\t\t/* This mount is not fully visible if there are any child mounts\n\t\t * that cover anything except for empty directories.\n\t\t */\n\t\tlist_for_each_entry(child, &mnt->mnt_mounts, mnt_child) {\n\t\t\tstruct inode *inode = child->mnt_mountpoint->d_inode;\n\t\t\tif (!S_ISDIR(inode->i_mode))\n\t\t\t\tgoto next;\n\t\t\tif (inode->i_nlink > 2)\n\t\t\t\tgoto next;\n\t\t}\n\t\tvisible = true;\n\t\tgoto found;\n\tnext:\t;\n\t}\nfound:\n\tup_read(&namespace_sem);\n\treturn visible;\n}\n\nstatic void *mntns_get(struct task_struct *task)\n{\n\tstruct mnt_namespace *ns = NULL;\n\tstruct nsproxy *nsproxy;\n\n\ttask_lock(task);\n\tnsproxy = task->nsproxy;\n\tif (nsproxy) {\n\t\tns = nsproxy->mnt_ns;\n\t\tget_mnt_ns(ns);\n\t}\n\ttask_unlock(task);\n\n\treturn ns;\n}\n\nstatic void mntns_put(void *ns)\n{\n\tput_mnt_ns(ns);\n}\n\nstatic int mntns_install(struct nsproxy *nsproxy, void *ns)\n{\n\tstruct fs_struct *fs = current->fs;\n\tstruct mnt_namespace *mnt_ns = ns;\n\tstruct path root;\n\n\tif (!ns_capable(mnt_ns->user_ns, CAP_SYS_ADMIN) ||\n\t    !ns_capable(current_user_ns(), CAP_SYS_CHROOT) ||\n\t    !ns_capable(current_user_ns(), CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (fs->users != 1)\n\t\treturn -EINVAL;\n\n\tget_mnt_ns(mnt_ns);\n\tput_mnt_ns(nsproxy->mnt_ns);\n\tnsproxy->mnt_ns = mnt_ns;\n\n\t/* Find the root */\n\troot.mnt    = &mnt_ns->root->mnt;\n\troot.dentry = mnt_ns->root->mnt.mnt_root;\n\tpath_get(&root);\n\twhile(d_mountpoint(root.dentry) && follow_down_one(&root))\n\t\t;\n\n\t/* Update the pwd and root */\n\tset_fs_pwd(fs, &root);\n\tset_fs_root(fs, &root);\n\n\tpath_put(&root);\n\treturn 0;\n}\n\nstatic unsigned int mntns_inum(void *ns)\n{\n\tstruct mnt_namespace *mnt_ns = ns;\n\treturn mnt_ns->proc_inum;\n}\n\nconst struct proc_ns_operations mntns_operations = {\n\t.name\t\t= \"mnt\",\n\t.type\t\t= CLONE_NEWNS,\n\t.get\t\t= mntns_get,\n\t.put\t\t= mntns_put,\n\t.install\t= mntns_install,\n\t.inum\t\t= mntns_inum,\n};\n", "/*\n *\n * Definitions for mount interface. This describes the in the kernel build \n * linkedlist with mounted filesystems.\n *\n * Author:  Marco van Wieringen <mvw@planets.elm.net>\n *\n */\n#ifndef _LINUX_MOUNT_H\n#define _LINUX_MOUNT_H\n\n#include <linux/types.h>\n#include <linux/list.h>\n#include <linux/nodemask.h>\n#include <linux/spinlock.h>\n#include <linux/seqlock.h>\n#include <linux/atomic.h>\n\nstruct super_block;\nstruct vfsmount;\nstruct dentry;\nstruct mnt_namespace;\n\n#define MNT_NOSUID\t0x01\n#define MNT_NODEV\t0x02\n#define MNT_NOEXEC\t0x04\n#define MNT_NOATIME\t0x08\n#define MNT_NODIRATIME\t0x10\n#define MNT_RELATIME\t0x20\n#define MNT_READONLY\t0x40\t/* does the user want this to be r/o? */\n\n#define MNT_SHRINKABLE\t0x100\n#define MNT_WRITE_HOLD\t0x200\n\n#define MNT_SHARED\t0x1000\t/* if the vfsmount is a shared mount */\n#define MNT_UNBINDABLE\t0x2000\t/* if the vfsmount is a unbindable mount */\n/*\n * MNT_SHARED_MASK is the set of flags that should be cleared when a\n * mount becomes shared.  Currently, this is only the flag that says a\n * mount cannot be bind mounted, since this is how we create a mount\n * that shares events with another mount.  If you add a new MNT_*\n * flag, consider how it interacts with shared mounts.\n */\n#define MNT_SHARED_MASK\t(MNT_UNBINDABLE)\n#define MNT_USER_SETTABLE_MASK  (MNT_NOSUID | MNT_NODEV | MNT_NOEXEC \\\n\t\t\t\t | MNT_NOATIME | MNT_NODIRATIME | MNT_RELATIME \\\n\t\t\t\t | MNT_READONLY)\n#define MNT_ATIME_MASK (MNT_NOATIME | MNT_NODIRATIME | MNT_RELATIME )\n\n#define MNT_INTERNAL_FLAGS (MNT_SHARED | MNT_WRITE_HOLD | MNT_INTERNAL | \\\n\t\t\t    MNT_DOOMED | MNT_SYNC_UMOUNT | MNT_MARKED)\n\n#define MNT_INTERNAL\t0x4000\n\n#define MNT_LOCK_ATIME\t\t0x040000\n#define MNT_LOCK_NOEXEC\t\t0x080000\n#define MNT_LOCK_NOSUID\t\t0x100000\n#define MNT_LOCK_NODEV\t\t0x200000\n#define MNT_LOCK_READONLY\t0x400000\n#define MNT_LOCKED\t\t0x800000\n#define MNT_DOOMED\t\t0x1000000\n#define MNT_SYNC_UMOUNT\t\t0x2000000\n#define MNT_MARKED\t\t0x4000000\n\nstruct vfsmount {\n\tstruct dentry *mnt_root;\t/* root of the mounted tree */\n\tstruct super_block *mnt_sb;\t/* pointer to superblock */\n\tint mnt_flags;\n};\n\nstruct file; /* forward dec */\n\nextern int mnt_want_write(struct vfsmount *mnt);\nextern int mnt_want_write_file(struct file *file);\nextern int mnt_clone_write(struct vfsmount *mnt);\nextern void mnt_drop_write(struct vfsmount *mnt);\nextern void mnt_drop_write_file(struct file *file);\nextern void mntput(struct vfsmount *mnt);\nextern struct vfsmount *mntget(struct vfsmount *mnt);\nextern void mnt_pin(struct vfsmount *mnt);\nextern void mnt_unpin(struct vfsmount *mnt);\nextern int __mnt_is_readonly(struct vfsmount *mnt);\n\nstruct file_system_type;\nextern struct vfsmount *vfs_kern_mount(struct file_system_type *type,\n\t\t\t\t      int flags, const char *name,\n\t\t\t\t      void *data);\n\nextern void mnt_set_expiry(struct vfsmount *mnt, struct list_head *expiry_list);\nextern void mark_mounts_for_expiry(struct list_head *mounts);\n\nextern dev_t name_to_dev_t(char *name);\n\n#endif /* _LINUX_MOUNT_H */\n"], "filenames": ["fs/namespace.c", "include/linux/mount.h"], "buggy_code_start_loc": [893, 47], "buggy_code_end_loc": [2133, 53], "fixing_code_start_loc": [893, 48], "fixing_code_end_loc": [2163, 59], "type": "CWE-269", "message": "fs/namespace.c in the Linux kernel through 3.16.1 does not properly restrict clearing MNT_NODEV, MNT_NOSUID, and MNT_NOEXEC and changing MNT_ATIME_MASK during a remount of a bind mount, which allows local users to gain privileges, interfere with backups and auditing on systems that had atime enabled, or cause a denial of service (excessive filesystem updating) on systems that had atime disabled via a \"mount -o remount\" command within a user namespace.", "other": {"cve": {"id": "CVE-2014-5207", "sourceIdentifier": "cve@mitre.org", "published": "2014-08-18T11:15:27.153", "lastModified": "2020-08-14T18:14:17.550", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "fs/namespace.c in the Linux kernel through 3.16.1 does not properly restrict clearing MNT_NODEV, MNT_NOSUID, and MNT_NOEXEC and changing MNT_ATIME_MASK during a remount of a bind mount, which allows local users to gain privileges, interfere with backups and auditing on systems that had atime enabled, or cause a denial of service (excessive filesystem updating) on systems that had atime disabled via a \"mount -o remount\" command within a user namespace."}, {"lang": "es", "value": "fs/namespace.c en el kernel de Linux hasta 3.16.1 no restringe debidamente la limpieza MNT_NODEV, MNT_NOSUID, y MNT_NOEXEC y el cambio MNT_ATIME_MASK durante un remontaje de un montaje bind, lo que permite a usuarios locales ganar privilegios, interferir con copias de seguridad y auditoria en sistemas que ten\u00edan atime habilitado, o causar una denegaci\u00f3n de servicio (la actualizaci\u00f3n excesiva de sistemas de ficheros) en sistemas que ten\u00edan atime deshabilitado a trav\u00e9s de un comando 'mount -o remount' dentro de un espacio para el nombre del usuario."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:H/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "HIGH", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 6.2}, "baseSeverity": "MEDIUM", "exploitabilityScore": 1.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-269"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.16.1", "matchCriteriaId": "8452407A-5074-4385-B9A1-9E49042CCAEB"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:12.04:*:*:*:esm:*:*:*", "matchCriteriaId": "8D305F7A-D159-4716-AB26-5E38BB5CD991"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:esm:*:*:*", "matchCriteriaId": "815D70A8-47D3-459C-A32C-9FEACA0659D1"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commit;h=9566d6742852c527bf5af38af5cbb878dad75705", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://packetstormsecurity.com/files/128595/Linux-Kernel-3.16.1-FUSE-Privilege-Escalation.html", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory", "VDB Entry"]}, {"url": "http://seclists.org/oss-sec/2014/q3/352", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.exploit-db.com/exploits/34923", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory", "VDB Entry"]}, {"url": "http://www.openwall.com/lists/oss-security/2014/08/13/4", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/69216", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.ubuntu.com/usn/USN-2317-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2318-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1129662", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://exchange.xforce.ibmcloud.com/vulnerabilities/95266", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://github.com/torvalds/linux/commit/9566d6742852c527bf5af38af5cbb878dad75705", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/9566d6742852c527bf5af38af5cbb878dad75705"}}