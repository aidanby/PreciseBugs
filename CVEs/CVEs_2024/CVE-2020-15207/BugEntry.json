{"buggy_code": ["/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_REDUCE_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_REDUCE_H_\n\n#include \"ruy/profiler/instrumentation.h\"  // from @ruy\n#include \"tensorflow/lite/kernels/internal/common.h\"\n#include \"tensorflow/lite/kernels/internal/cppmath.h\"\n#include \"tensorflow/lite/kernels/internal/max.h\"\n#include \"tensorflow/lite/kernels/internal/min.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n\nnamespace tflite {\n\nnamespace reference_ops {\n\n// A generic reduce method that can be used for reduce_sum, reduce_mean, etc.\n// This method iterates through input data and reduce elements along the\n// dimensions given in axis.\ntemplate <typename In, typename Out>\ninline bool Reduce(const In* input_data, const int* input_dims,\n                   const int* output_dims, const int input_num_dims,\n                   const int output_num_dims, const int* axis,\n                   const int num_axis, int* input_iter,\n                   Out reducer(const Out current, const In in),\n                   Out* output_data) {\n  // Reset input iterator.\n  for (int idx = 0; idx < input_num_dims; ++idx) {\n    input_iter[idx] = 0;\n  }\n  // Iterate through input_data.\n  do {\n    size_t input_offset =\n        ReducedOutputOffset(input_num_dims, input_dims, input_iter, 0, nullptr);\n    size_t output_offset = ReducedOutputOffset(input_num_dims, input_dims,\n                                               input_iter, num_axis, axis);\n    output_data[output_offset] =\n        reducer(output_data[output_offset], input_data[input_offset]);\n  } while (NextIndex(input_num_dims, input_dims, input_iter));\n  return true;\n}\n\n// This method parses the input 'axis' to remove duplicates and handle negative\n// values, and returns a valid 'out_axis'\ninline bool ResolveAxis(const int num_dims, const int* axis,\n                        const int64_t num_axis, int* out_axis,\n                        int* out_num_axis) {\n  *out_num_axis = 0;  // Just in case.\n  // Short-circuit axis resolution for scalars; the axis will go unused.\n  if (num_dims == 0) {\n    return true;\n  }\n  // o(n^2) is fine since out_num_axis should be really small, mostly <= 4\n  for (int64_t idx = 0; idx < num_axis; ++idx) {\n    // Handle negative index. A positive index 'p_idx' can be represented as a\n    // negative index 'n_idx' as: n_idx = p_idx-num_dims\n    // eg: For num_dims=3, [0, 1, 2] is the same as [-3, -2, -1]  */\n    int current = axis[idx] < 0 ? (axis[idx] + num_dims) : axis[idx];\n    TFLITE_DCHECK(current >= 0 && current < num_dims);\n    bool is_dup = false;\n    for (int j = 0; j < *out_num_axis; ++j) {\n      if (out_axis[j] == current) {\n        is_dup = true;\n        break;\n      }\n    }\n    if (!is_dup) {\n      out_axis[*out_num_axis] = current;\n      *out_num_axis += 1;\n    }\n  }\n  return true;\n}\n\n// This method expects that output_data has been initialized.\ntemplate <typename In, typename Out>\ninline bool ReduceSumImpl(const In* input_data, const int* input_dims,\n                          const int* output_dims, const int input_num_dims,\n                          const int output_num_dims, const int* axis,\n                          const int num_axis, int* input_iter,\n                          Out* output_data) {\n  auto reducer = [](const Out current, const In in) -> Out {\n    const Out actual_in = static_cast<Out>(in);\n    return current + actual_in;\n  };\n  return Reduce<In, Out>(input_data, input_dims, output_dims, input_num_dims,\n                         output_num_dims, axis, num_axis, input_iter, reducer,\n                         output_data);\n}\n\ntemplate <typename T>\ninline bool InitTensorDataForReduce(const int* dims, const int num_dims,\n                                    const T init_value, T* data) {\n  size_t num_elements = 1;\n  for (int idx = 0; idx < num_dims; ++idx) {\n    size_t current = static_cast<size_t>(dims[idx]);\n    // Overflow prevention.\n    if (num_elements > std::numeric_limits<size_t>::max() / current) {\n      return false;\n    }\n    num_elements *= current;\n  }\n  for (size_t idx = 0; idx < num_elements; ++idx) {\n    data[idx] = init_value;\n  }\n  return true;\n}\n\n// Computes the generic value (i.e., sum/max/min/prod) of elements across\n// dimensions given in axis. It needs to pass in init_value and reducer.\ntemplate <typename T>\ninline bool ReduceGeneric(const T* input_data, const int* input_dims,\n                          const int input_num_dims, T* output_data,\n                          const int* output_dims, const int output_num_dims,\n                          const int* axis, const int64_t num_axis_dimensions,\n                          bool keep_dims, int* temp_index, int* resolved_axis,\n                          T init_value,\n                          T reducer(const T current, const T in)) {\n  // Reset output data.\n  if (!InitTensorDataForReduce(output_dims, output_num_dims, init_value,\n                               output_data)) {\n    return false;\n  }\n\n  // Resolve axis.\n  int num_resolved_axis = 0;\n  if (!ResolveAxis(input_num_dims, axis, num_axis_dimensions, resolved_axis,\n                   &num_resolved_axis)) {\n    return false;\n  }\n\n  return Reduce<T, T>(input_data, input_dims, output_dims, input_num_dims,\n                      output_num_dims, resolved_axis, num_resolved_axis,\n                      temp_index, reducer, output_data);\n}\n\n// Computes the mean of elements across dimensions given in axis.\n// It does so in two stages, first calculates the sum of elements along the axis\n// then divides it by the number of element in axis.\ntemplate <typename T, typename U>\ninline bool Mean(const T* input_data, const int* input_dims,\n                 const int input_num_dims, T* output_data,\n                 const int* output_dims, const int output_num_dims,\n                 const int* axis, const int num_axis_dimensions, bool keep_dims,\n                 int* temp_index, int* resolved_axis, U* temp_sum) {\n  ruy::profiler::ScopeLabel label(\"Mean\");\n  // Reset output data.\n  size_t num_outputs = 1;\n  for (int idx = 0; idx < output_num_dims; ++idx) {\n    size_t current = static_cast<size_t>(output_dims[idx]);\n    // Overflow prevention.\n    if (num_outputs > std::numeric_limits<size_t>::max() / current) {\n      return false;\n    }\n    num_outputs *= current;\n  }\n  for (size_t idx = 0; idx < num_outputs; ++idx) {\n    output_data[idx] = T();\n    temp_sum[idx] = U();\n  }\n\n  // Resolve axis.\n  int num_resolved_axis = 0;\n  if (!ResolveAxis(input_num_dims, axis, num_axis_dimensions, resolved_axis,\n                   &num_resolved_axis)) {\n    return false;\n  }\n\n  if (!ReduceSumImpl<T, U>(input_data, input_dims, output_dims, input_num_dims,\n                           output_num_dims, resolved_axis, num_resolved_axis,\n                           temp_index, temp_sum)) {\n    return false;\n  }\n\n  // Calculate mean by dividing output_data by num of aggregated element.\n  size_t num_elements_in_axis = 1;\n  for (int idx = 0; idx < num_resolved_axis; ++idx) {\n    size_t current = static_cast<size_t>(input_dims[resolved_axis[idx]]);\n    // Overflow prevention.\n    if (current > (std::numeric_limits<size_t>::max() / num_elements_in_axis)) {\n      return false;\n    }\n    num_elements_in_axis *= current;\n  }\n\n  if (num_elements_in_axis > 0) {\n    for (size_t idx = 0; idx < num_outputs; ++idx) {\n      output_data[idx] =\n          static_cast<T>(temp_sum[idx] / static_cast<U>(num_elements_in_axis));\n    }\n  }\n  return true;\n}\n\ntemplate <typename T>\ninline void Mean(const tflite::MeanParams& op_params,\n                 const RuntimeShape& unextended_input_shape,\n                 const T* input_data,\n                 const RuntimeShape& unextended_output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Mean4D\");\n\n  // Current implementation only supports dimension equals 4 and simultaneous\n  // reduction over width and height.\n  TFLITE_CHECK_EQ(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_CHECK_LE(unextended_output_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape output_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_shape);\n\n  const int output_batch = output_shape.Dims(0);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int output_depth = output_shape.Dims(3);\n\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n\n  TFLITE_CHECK_EQ(op_params.axis_count, 2);\n  TFLITE_CHECK((op_params.axis[0] == 1 && op_params.axis[1] == 2) ||\n               (op_params.axis[0] == 2 && op_params.axis[1] == 1));\n  TFLITE_CHECK_EQ(output_height, 1);\n  TFLITE_CHECK_EQ(output_width, 1);\n\n  for (int out_b = 0; out_b < output_batch; ++out_b) {\n    for (int out_d = 0; out_d < output_depth; ++out_d) {\n      float value = 0;\n      for (int in_h = 0; in_h < input_height; ++in_h) {\n        for (int in_w = 0; in_w < input_width; ++in_w) {\n          value += input_data[Offset(input_shape, out_b, in_h, in_w, out_d)];\n        }\n      }\n      output_data[Offset(output_shape, out_b, 0, 0, out_d)] =\n          value / (input_width * input_height);\n    }\n  }\n}\n\ninline void Mean(const tflite::MeanParams& op_params,\n                 const RuntimeShape& unextended_input_shape,\n                 const uint8_t* input_data, int32_t input_zero_point,\n                 float input_scale, const RuntimeShape& unextended_output_shape,\n                 uint8_t* output_data, int32_t output_zero_point,\n                 float output_scale) {\n  ruy::profiler::ScopeLabel label(\"Mean4D/Uint8\");\n\n  // Current implementation only supports dimension equals 4 and simultaneous\n  // reduction over width and height.\n  TFLITE_CHECK_EQ(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_CHECK_LE(unextended_output_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape output_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_shape);\n  const int output_batch = output_shape.Dims(0);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int output_depth = output_shape.Dims(3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const float num_elements_in_axis = input_width * input_height;\n\n  TFLITE_CHECK_EQ(op_params.axis_count, 2);\n  TFLITE_CHECK((op_params.axis[0] == 1 && op_params.axis[1] == 2) ||\n               (op_params.axis[0] == 2 && op_params.axis[1] == 1));\n  TFLITE_CHECK_EQ(output_height, 1);\n  TFLITE_CHECK_EQ(output_width, 1);\n\n  constexpr int32_t kMinValue = std::numeric_limits<uint8_t>::min();\n  constexpr int32_t kMaxValue = std::numeric_limits<uint8_t>::max();\n\n  int32_t bias =\n      output_zero_point -\n      static_cast<int32_t>(input_zero_point * input_scale / output_scale);\n  double real_scale =\n      static_cast<double>(input_scale / (num_elements_in_axis * output_scale));\n\n  int32_t multiplier;\n  int shift;\n  QuantizeMultiplier(real_scale, &multiplier, &shift);\n  for (int out_b = 0; out_b < output_batch; ++out_b) {\n    for (int out_d = 0; out_d < output_depth; ++out_d) {\n      int32_t acc = 0;\n      for (int in_h = 0; in_h < input_height; ++in_h) {\n        for (int in_w = 0; in_w < input_width; ++in_w) {\n          acc += input_data[Offset(input_shape, out_b, in_h, in_w, out_d)];\n        }\n      }\n      acc = MultiplyByQuantizedMultiplier(acc, multiplier, shift);\n      acc += bias;\n      acc = std::min(std::max(acc, kMinValue), kMaxValue);\n      output_data[Offset(output_shape, out_b, 0, 0, out_d)] =\n          static_cast<uint8_t>(acc);\n    }\n  }\n}\n\n// Computes the mean of elements across dimensions given in axis.\n// It does so in two stages, first calculates the sum of elements along the axis\n// then divides it by the number of element in axis for quantized values.\ntemplate <typename T, typename U>\ninline bool QuantizedMeanOrSum(const T* input_data, int32_t input_zero_point,\n                               float input_scale, const int* input_dims,\n                               const int input_num_dims, T* output_data,\n                               int32_t output_zero_point, float output_scale,\n                               const int* output_dims,\n                               const int output_num_dims, const int* axis,\n                               const int num_axis_dimensions, bool keep_dims,\n                               int* temp_index, int* resolved_axis, U* temp_sum,\n                               bool compute_sum) {\n  const bool uint8_case = std::is_same<T, uint8_t>::value;\n  const bool int16_case = std::is_same<T, int16_t>::value;\n  if (uint8_case) {\n    ruy::profiler::ScopeLabel label(compute_sum ? \"Sum/Uint8\" : \"Mean/Uint8\");\n  } else if (int16_case) {\n    ruy::profiler::ScopeLabel label(compute_sum ? \"Sum/Int16\" : \"Mean/Int16\");\n  } else {\n    ruy::profiler::ScopeLabel label(compute_sum ? \"Sum/Int8\" : \"Mean/Int8\");\n  }\n  // Reset output data.\n  size_t num_outputs = 1;\n  for (int idx = 0; idx < output_num_dims; ++idx) {\n    size_t current = static_cast<size_t>(output_dims[idx]);\n    // Overflow prevention.\n    if (num_outputs > std::numeric_limits<size_t>::max() / current) {\n      return false;\n    }\n    num_outputs *= current;\n  }\n  for (size_t idx = 0; idx < num_outputs; ++idx) {\n    output_data[idx] = T();\n    temp_sum[idx] = U();\n  }\n\n  // Resolve axis.\n  int num_resolved_axis = 0;\n  if (!ResolveAxis(input_num_dims, axis, num_axis_dimensions, resolved_axis,\n                   &num_resolved_axis)) {\n    return false;\n  }\n\n  if (!ReduceSumImpl<T, U>(input_data, input_dims, output_dims, input_num_dims,\n                           output_num_dims, resolved_axis, num_resolved_axis,\n                           temp_index, temp_sum)) {\n    return false;\n  }\n\n  // Calculate mean by dividing output_data by num of aggregated element.\n  size_t num_elements_in_axis = 1;\n  for (int idx = 0; idx < num_resolved_axis; ++idx) {\n    size_t current = static_cast<size_t>(input_dims[resolved_axis[idx]]);\n    // Overflow prevention.\n    if (current > (std::numeric_limits<size_t>::max() / num_elements_in_axis)) {\n      return false;\n    }\n    num_elements_in_axis *= current;\n  }\n\n  if (num_elements_in_axis > 0) {\n    const float scale = input_scale / output_scale;\n    if (compute_sum) {\n      // TODO(b/116341117): Eliminate float and do this completely in 8bit.\n      const float bias =\n          -input_zero_point * scale * num_elements_in_axis + 0.5f;\n      for (size_t idx = 0; idx < num_outputs; ++idx) {\n        const U value =\n            static_cast<U>(TfLiteRound(temp_sum[idx] * scale + bias)) +\n            output_zero_point;\n        output_data[idx] = static_cast<T>(value);\n      }\n    } else {\n      const float bias = -input_zero_point * scale + 0.5f;\n      for (size_t idx = 0; idx < num_outputs; ++idx) {\n        float float_mean = static_cast<float>(temp_sum[idx]) /\n                           static_cast<float>(num_elements_in_axis);\n        float result = TfLiteMin(\n            TfLiteRound(float_mean * scale + bias) + output_zero_point,\n            static_cast<float>(std::numeric_limits<T>::max()));\n        result = TfLiteMax(result,\n                           static_cast<float>(std::numeric_limits<T>::min()));\n        output_data[idx] = static_cast<T>(result);\n      }\n    }\n  }\n  return true;\n}\n\n}  // namespace reference_ops\n\n}  // namespace tflite\n\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_REDUCE_H_\n"], "fixing_code": ["/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_REDUCE_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_REDUCE_H_\n\n#include \"ruy/profiler/instrumentation.h\"  // from @ruy\n#include \"tensorflow/lite/kernels/internal/common.h\"\n#include \"tensorflow/lite/kernels/internal/cppmath.h\"\n#include \"tensorflow/lite/kernels/internal/max.h\"\n#include \"tensorflow/lite/kernels/internal/min.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n\nnamespace tflite {\n\nnamespace reference_ops {\n\n// A generic reduce method that can be used for reduce_sum, reduce_mean, etc.\n// This method iterates through input data and reduce elements along the\n// dimensions given in axis.\ntemplate <typename In, typename Out>\ninline bool Reduce(const In* input_data, const int* input_dims,\n                   const int* output_dims, const int input_num_dims,\n                   const int output_num_dims, const int* axis,\n                   const int num_axis, int* input_iter,\n                   Out reducer(const Out current, const In in),\n                   Out* output_data) {\n  // Reset input iterator.\n  for (int idx = 0; idx < input_num_dims; ++idx) {\n    input_iter[idx] = 0;\n  }\n  // Iterate through input_data.\n  do {\n    size_t input_offset =\n        ReducedOutputOffset(input_num_dims, input_dims, input_iter, 0, nullptr);\n    size_t output_offset = ReducedOutputOffset(input_num_dims, input_dims,\n                                               input_iter, num_axis, axis);\n    output_data[output_offset] =\n        reducer(output_data[output_offset], input_data[input_offset]);\n  } while (NextIndex(input_num_dims, input_dims, input_iter));\n  return true;\n}\n\n// This method parses the input 'axis' to remove duplicates and handle negative\n// values, and returns a valid 'out_axis'\ninline bool ResolveAxis(const int num_dims, const int* axis,\n                        const int64_t num_axis, int* out_axis,\n                        int* out_num_axis) {\n  *out_num_axis = 0;  // Just in case.\n  // Short-circuit axis resolution for scalars; the axis will go unused.\n  if (num_dims == 0) {\n    return true;\n  }\n  // o(n^2) is fine since out_num_axis should be really small, mostly <= 4\n  for (int64_t idx = 0; idx < num_axis; ++idx) {\n    // Handle negative index. A positive index 'p_idx' can be represented as a\n    // negative index 'n_idx' as: n_idx = p_idx-num_dims\n    // eg: For num_dims=3, [0, 1, 2] is the same as [-3, -2, -1]  */\n    int current = axis[idx] < 0 ? (axis[idx] + num_dims) : axis[idx];\n    TFLITE_DCHECK(current >= 0 && current < num_dims);\n    if (current < 0 || current >= num_dims) {\n      return false;\n    }\n    bool is_dup = false;\n    for (int j = 0; j < *out_num_axis; ++j) {\n      if (out_axis[j] == current) {\n        is_dup = true;\n        break;\n      }\n    }\n    if (!is_dup) {\n      out_axis[*out_num_axis] = current;\n      *out_num_axis += 1;\n    }\n  }\n  return true;\n}\n\n// This method expects that output_data has been initialized.\ntemplate <typename In, typename Out>\ninline bool ReduceSumImpl(const In* input_data, const int* input_dims,\n                          const int* output_dims, const int input_num_dims,\n                          const int output_num_dims, const int* axis,\n                          const int num_axis, int* input_iter,\n                          Out* output_data) {\n  auto reducer = [](const Out current, const In in) -> Out {\n    const Out actual_in = static_cast<Out>(in);\n    return current + actual_in;\n  };\n  return Reduce<In, Out>(input_data, input_dims, output_dims, input_num_dims,\n                         output_num_dims, axis, num_axis, input_iter, reducer,\n                         output_data);\n}\n\ntemplate <typename T>\ninline bool InitTensorDataForReduce(const int* dims, const int num_dims,\n                                    const T init_value, T* data) {\n  size_t num_elements = 1;\n  for (int idx = 0; idx < num_dims; ++idx) {\n    size_t current = static_cast<size_t>(dims[idx]);\n    // Overflow prevention.\n    if (num_elements > std::numeric_limits<size_t>::max() / current) {\n      return false;\n    }\n    num_elements *= current;\n  }\n  for (size_t idx = 0; idx < num_elements; ++idx) {\n    data[idx] = init_value;\n  }\n  return true;\n}\n\n// Computes the generic value (i.e., sum/max/min/prod) of elements across\n// dimensions given in axis. It needs to pass in init_value and reducer.\ntemplate <typename T>\ninline bool ReduceGeneric(const T* input_data, const int* input_dims,\n                          const int input_num_dims, T* output_data,\n                          const int* output_dims, const int output_num_dims,\n                          const int* axis, const int64_t num_axis_dimensions,\n                          bool keep_dims, int* temp_index, int* resolved_axis,\n                          T init_value,\n                          T reducer(const T current, const T in)) {\n  // Reset output data.\n  if (!InitTensorDataForReduce(output_dims, output_num_dims, init_value,\n                               output_data)) {\n    return false;\n  }\n\n  // Resolve axis.\n  int num_resolved_axis = 0;\n  if (!ResolveAxis(input_num_dims, axis, num_axis_dimensions, resolved_axis,\n                   &num_resolved_axis)) {\n    return false;\n  }\n\n  return Reduce<T, T>(input_data, input_dims, output_dims, input_num_dims,\n                      output_num_dims, resolved_axis, num_resolved_axis,\n                      temp_index, reducer, output_data);\n}\n\n// Computes the mean of elements across dimensions given in axis.\n// It does so in two stages, first calculates the sum of elements along the axis\n// then divides it by the number of element in axis.\ntemplate <typename T, typename U>\ninline bool Mean(const T* input_data, const int* input_dims,\n                 const int input_num_dims, T* output_data,\n                 const int* output_dims, const int output_num_dims,\n                 const int* axis, const int num_axis_dimensions, bool keep_dims,\n                 int* temp_index, int* resolved_axis, U* temp_sum) {\n  ruy::profiler::ScopeLabel label(\"Mean\");\n  // Reset output data.\n  size_t num_outputs = 1;\n  for (int idx = 0; idx < output_num_dims; ++idx) {\n    size_t current = static_cast<size_t>(output_dims[idx]);\n    // Overflow prevention.\n    if (num_outputs > std::numeric_limits<size_t>::max() / current) {\n      return false;\n    }\n    num_outputs *= current;\n  }\n  for (size_t idx = 0; idx < num_outputs; ++idx) {\n    output_data[idx] = T();\n    temp_sum[idx] = U();\n  }\n\n  // Resolve axis.\n  int num_resolved_axis = 0;\n  if (!ResolveAxis(input_num_dims, axis, num_axis_dimensions, resolved_axis,\n                   &num_resolved_axis)) {\n    return false;\n  }\n\n  if (!ReduceSumImpl<T, U>(input_data, input_dims, output_dims, input_num_dims,\n                           output_num_dims, resolved_axis, num_resolved_axis,\n                           temp_index, temp_sum)) {\n    return false;\n  }\n\n  // Calculate mean by dividing output_data by num of aggregated element.\n  size_t num_elements_in_axis = 1;\n  for (int idx = 0; idx < num_resolved_axis; ++idx) {\n    size_t current = static_cast<size_t>(input_dims[resolved_axis[idx]]);\n    // Overflow prevention.\n    if (current > (std::numeric_limits<size_t>::max() / num_elements_in_axis)) {\n      return false;\n    }\n    num_elements_in_axis *= current;\n  }\n\n  if (num_elements_in_axis > 0) {\n    for (size_t idx = 0; idx < num_outputs; ++idx) {\n      output_data[idx] =\n          static_cast<T>(temp_sum[idx] / static_cast<U>(num_elements_in_axis));\n    }\n  }\n  return true;\n}\n\ntemplate <typename T>\ninline void Mean(const tflite::MeanParams& op_params,\n                 const RuntimeShape& unextended_input_shape,\n                 const T* input_data,\n                 const RuntimeShape& unextended_output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Mean4D\");\n\n  // Current implementation only supports dimension equals 4 and simultaneous\n  // reduction over width and height.\n  TFLITE_CHECK_EQ(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_CHECK_LE(unextended_output_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape output_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_shape);\n\n  const int output_batch = output_shape.Dims(0);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int output_depth = output_shape.Dims(3);\n\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n\n  TFLITE_CHECK_EQ(op_params.axis_count, 2);\n  TFLITE_CHECK((op_params.axis[0] == 1 && op_params.axis[1] == 2) ||\n               (op_params.axis[0] == 2 && op_params.axis[1] == 1));\n  TFLITE_CHECK_EQ(output_height, 1);\n  TFLITE_CHECK_EQ(output_width, 1);\n\n  for (int out_b = 0; out_b < output_batch; ++out_b) {\n    for (int out_d = 0; out_d < output_depth; ++out_d) {\n      float value = 0;\n      for (int in_h = 0; in_h < input_height; ++in_h) {\n        for (int in_w = 0; in_w < input_width; ++in_w) {\n          value += input_data[Offset(input_shape, out_b, in_h, in_w, out_d)];\n        }\n      }\n      output_data[Offset(output_shape, out_b, 0, 0, out_d)] =\n          value / (input_width * input_height);\n    }\n  }\n}\n\ninline void Mean(const tflite::MeanParams& op_params,\n                 const RuntimeShape& unextended_input_shape,\n                 const uint8_t* input_data, int32_t input_zero_point,\n                 float input_scale, const RuntimeShape& unextended_output_shape,\n                 uint8_t* output_data, int32_t output_zero_point,\n                 float output_scale) {\n  ruy::profiler::ScopeLabel label(\"Mean4D/Uint8\");\n\n  // Current implementation only supports dimension equals 4 and simultaneous\n  // reduction over width and height.\n  TFLITE_CHECK_EQ(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_CHECK_LE(unextended_output_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape output_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_shape);\n  const int output_batch = output_shape.Dims(0);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n  const int output_depth = output_shape.Dims(3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const float num_elements_in_axis = input_width * input_height;\n\n  TFLITE_CHECK_EQ(op_params.axis_count, 2);\n  TFLITE_CHECK((op_params.axis[0] == 1 && op_params.axis[1] == 2) ||\n               (op_params.axis[0] == 2 && op_params.axis[1] == 1));\n  TFLITE_CHECK_EQ(output_height, 1);\n  TFLITE_CHECK_EQ(output_width, 1);\n\n  constexpr int32_t kMinValue = std::numeric_limits<uint8_t>::min();\n  constexpr int32_t kMaxValue = std::numeric_limits<uint8_t>::max();\n\n  int32_t bias =\n      output_zero_point -\n      static_cast<int32_t>(input_zero_point * input_scale / output_scale);\n  double real_scale =\n      static_cast<double>(input_scale / (num_elements_in_axis * output_scale));\n\n  int32_t multiplier;\n  int shift;\n  QuantizeMultiplier(real_scale, &multiplier, &shift);\n  for (int out_b = 0; out_b < output_batch; ++out_b) {\n    for (int out_d = 0; out_d < output_depth; ++out_d) {\n      int32_t acc = 0;\n      for (int in_h = 0; in_h < input_height; ++in_h) {\n        for (int in_w = 0; in_w < input_width; ++in_w) {\n          acc += input_data[Offset(input_shape, out_b, in_h, in_w, out_d)];\n        }\n      }\n      acc = MultiplyByQuantizedMultiplier(acc, multiplier, shift);\n      acc += bias;\n      acc = std::min(std::max(acc, kMinValue), kMaxValue);\n      output_data[Offset(output_shape, out_b, 0, 0, out_d)] =\n          static_cast<uint8_t>(acc);\n    }\n  }\n}\n\n// Computes the mean of elements across dimensions given in axis.\n// It does so in two stages, first calculates the sum of elements along the axis\n// then divides it by the number of element in axis for quantized values.\ntemplate <typename T, typename U>\ninline bool QuantizedMeanOrSum(const T* input_data, int32_t input_zero_point,\n                               float input_scale, const int* input_dims,\n                               const int input_num_dims, T* output_data,\n                               int32_t output_zero_point, float output_scale,\n                               const int* output_dims,\n                               const int output_num_dims, const int* axis,\n                               const int num_axis_dimensions, bool keep_dims,\n                               int* temp_index, int* resolved_axis, U* temp_sum,\n                               bool compute_sum) {\n  const bool uint8_case = std::is_same<T, uint8_t>::value;\n  const bool int16_case = std::is_same<T, int16_t>::value;\n  if (uint8_case) {\n    ruy::profiler::ScopeLabel label(compute_sum ? \"Sum/Uint8\" : \"Mean/Uint8\");\n  } else if (int16_case) {\n    ruy::profiler::ScopeLabel label(compute_sum ? \"Sum/Int16\" : \"Mean/Int16\");\n  } else {\n    ruy::profiler::ScopeLabel label(compute_sum ? \"Sum/Int8\" : \"Mean/Int8\");\n  }\n  // Reset output data.\n  size_t num_outputs = 1;\n  for (int idx = 0; idx < output_num_dims; ++idx) {\n    size_t current = static_cast<size_t>(output_dims[idx]);\n    // Overflow prevention.\n    if (num_outputs > std::numeric_limits<size_t>::max() / current) {\n      return false;\n    }\n    num_outputs *= current;\n  }\n  for (size_t idx = 0; idx < num_outputs; ++idx) {\n    output_data[idx] = T();\n    temp_sum[idx] = U();\n  }\n\n  // Resolve axis.\n  int num_resolved_axis = 0;\n  if (!ResolveAxis(input_num_dims, axis, num_axis_dimensions, resolved_axis,\n                   &num_resolved_axis)) {\n    return false;\n  }\n\n  if (!ReduceSumImpl<T, U>(input_data, input_dims, output_dims, input_num_dims,\n                           output_num_dims, resolved_axis, num_resolved_axis,\n                           temp_index, temp_sum)) {\n    return false;\n  }\n\n  // Calculate mean by dividing output_data by num of aggregated element.\n  size_t num_elements_in_axis = 1;\n  for (int idx = 0; idx < num_resolved_axis; ++idx) {\n    size_t current = static_cast<size_t>(input_dims[resolved_axis[idx]]);\n    // Overflow prevention.\n    if (current > (std::numeric_limits<size_t>::max() / num_elements_in_axis)) {\n      return false;\n    }\n    num_elements_in_axis *= current;\n  }\n\n  if (num_elements_in_axis > 0) {\n    const float scale = input_scale / output_scale;\n    if (compute_sum) {\n      // TODO(b/116341117): Eliminate float and do this completely in 8bit.\n      const float bias =\n          -input_zero_point * scale * num_elements_in_axis + 0.5f;\n      for (size_t idx = 0; idx < num_outputs; ++idx) {\n        const U value =\n            static_cast<U>(TfLiteRound(temp_sum[idx] * scale + bias)) +\n            output_zero_point;\n        output_data[idx] = static_cast<T>(value);\n      }\n    } else {\n      const float bias = -input_zero_point * scale + 0.5f;\n      for (size_t idx = 0; idx < num_outputs; ++idx) {\n        float float_mean = static_cast<float>(temp_sum[idx]) /\n                           static_cast<float>(num_elements_in_axis);\n        float result = TfLiteMin(\n            TfLiteRound(float_mean * scale + bias) + output_zero_point,\n            static_cast<float>(std::numeric_limits<T>::max()));\n        result = TfLiteMax(result,\n                           static_cast<float>(std::numeric_limits<T>::min()));\n        output_data[idx] = static_cast<T>(result);\n      }\n    }\n  }\n  return true;\n}\n\n}  // namespace reference_ops\n\n}  // namespace tflite\n\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_REDUCE_H_\n"], "filenames": ["tensorflow/lite/kernels/internal/reference/reduce.h"], "buggy_code_start_loc": [72], "buggy_code_end_loc": [72], "fixing_code_start_loc": [73], "fixing_code_end_loc": [76], "type": "CWE-787", "message": "In tensorflow-lite before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, to mimic Python's indexing with negative values, TFLite uses `ResolveAxis` to convert negative values to positive indices. However, the only check that the converted index is now valid is only present in debug builds. If the `DCHECK` does not trigger, then code execution moves ahead with a negative index. This, in turn, results in accessing data out of bounds which results in segfaults and/or data corruption. The issue is patched in commit 2d88f470dea2671b430884260f3626b1fe99830a, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.", "other": {"cve": {"id": "CVE-2020-15207", "sourceIdentifier": "security-advisories@github.com", "published": "2020-09-25T19:15:15.993", "lastModified": "2021-11-18T17:27:27.907", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In tensorflow-lite before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, to mimic Python's indexing with negative values, TFLite uses `ResolveAxis` to convert negative values to positive indices. However, the only check that the converted index is now valid is only present in debug builds. If the `DCHECK` does not trigger, then code execution moves ahead with a negative index. This, in turn, results in accessing data out of bounds which results in segfaults and/or data corruption. The issue is patched in commit 2d88f470dea2671b430884260f3626b1fe99830a, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1."}, {"lang": "es", "value": "En tensorflow-lite versiones anteriores a 1.15.4, 2.0.3, 2.1.2, 2.2.1 y 2.3.1, para imitar la indexaci\u00f3n de Python con valores negativos, TFLite usa \"ResolveAxis\" para convertir valores negativos en \u00edndices positivos.&#xa0;Sin embargo, la \u00fanica comprobaci\u00f3n de que el \u00edndice convertido ahora es v\u00e1lido solo est\u00e1 presente en las compilaciones de depuraci\u00f3n.&#xa0;Si el \"DCHECK\" no se activa, entonces la ejecuci\u00f3n de c\u00f3digo avanza con un \u00edndice negativo.&#xa0;Esto, a su vez, resulta en el acceso a los datos fuera de l\u00edmites, resultando en segmentaciones y/o una corrupci\u00f3n de lo datos.&#xa0;El problema es parcheado en el commit 2d88f470dea2671b430884260f3626b1fe99830a, y es publicado en TensorFlow versiones 1.15.4, 2.0.3, 2.1.2, 2.2.1 o 2.3.1"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:C/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.0, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 2.2, "impactScore": 6.0}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:C/C:N/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.7, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.2, "impactScore": 5.8}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:P/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 6.8}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.6, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-787"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-119"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:lite:*:*:*", "versionEndExcluding": "1.15.4", "matchCriteriaId": "7A5421A9-693F-472A-9A21-43950C884C77"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:lite:*:*:*", "versionStartIncluding": "2.0.0", "versionEndExcluding": "2.0.3", "matchCriteriaId": "B0FEB74E-5E54-4A2F-910C-FA1812C73DB2"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:lite:*:*:*", "versionStartIncluding": "2.1.0", "versionEndExcluding": "2.1.2", "matchCriteriaId": "47D83682-6615-49BC-8043-F36B9D017578"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:lite:*:*:*", "versionStartIncluding": "2.2.0", "versionEndExcluding": "2.2.1", "matchCriteriaId": "323B716A-E8F7-4CDA-B8FD-A56977D59C02"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:lite:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.1", "matchCriteriaId": "C09502A8-B667-4867-BEBD-40333E98A601"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:leap:15.2:*:*:*:*:*:*:*", "matchCriteriaId": "B009C22E-30A4-4288-BCF6-C3E81DEAF45A"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2020-10/msg00065.html", "source": "security-advisories@github.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/2d88f470dea2671b430884260f3626b1fe99830a", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.3.1", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-q4qf-3fc6-8x34", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/2d88f470dea2671b430884260f3626b1fe99830a"}}