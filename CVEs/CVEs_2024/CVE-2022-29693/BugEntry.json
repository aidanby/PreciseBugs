{"buggy_code": ["/* Unicorn Emulator Engine */\n/* By Nguyen Anh Quynh <aquynh@gmail.com>, 2015 */\n/* Modified for Unicorn Engine by Chen Huitao<chenhuitao@hfmrit.com>, 2020 */\n\n#if defined(UNICORN_HAS_OSXKERNEL)\n#include <libkern/libkern.h>\n#else\n#include <stddef.h>\n#include <stdio.h>\n#include <stdlib.h>\n#endif\n\n#include <time.h> // nanosleep\n#include <string.h>\n\n#include \"uc_priv.h\"\n\n// target specific headers\n#include \"qemu/target/m68k/unicorn.h\"\n#include \"qemu/target/i386/unicorn.h\"\n#include \"qemu/target/arm/unicorn.h\"\n#include \"qemu/target/mips/unicorn.h\"\n#include \"qemu/target/sparc/unicorn.h\"\n#include \"qemu/target/ppc/unicorn.h\"\n#include \"qemu/target/riscv/unicorn.h\"\n#include \"qemu/target/s390x/unicorn.h\"\n\n#include \"qemu/include/qemu/queue.h\"\n#include \"qemu-common.h\"\n\nstatic void clear_deleted_hooks(uc_engine *uc);\n\nstatic void *hook_insert(struct list *l, struct hook *h)\n{\n    void *item = list_insert(l, (void *)h);\n    if (item) {\n        h->refs++;\n    }\n    return item;\n}\n\nstatic void *hook_append(struct list *l, struct hook *h)\n{\n    void *item = list_append(l, (void *)h);\n    if (item) {\n        h->refs++;\n    }\n    return item;\n}\n\nstatic void hook_delete(void *data)\n{\n    struct hook *h = (struct hook *)data;\n\n    h->refs--;\n\n    if (h->refs == 0) {\n        free(h);\n    }\n}\n\nUNICORN_EXPORT\nunsigned int uc_version(unsigned int *major, unsigned int *minor)\n{\n    if (major != NULL && minor != NULL) {\n        *major = UC_API_MAJOR;\n        *minor = UC_API_MINOR;\n    }\n\n    return (UC_API_MAJOR << 24) + (UC_API_MINOR << 16) + (UC_API_PATCH << 8) +\n           UC_API_EXTRA;\n}\n\nUNICORN_EXPORT\nuc_err uc_errno(uc_engine *uc)\n{\n    return uc->errnum;\n}\n\nUNICORN_EXPORT\nconst char *uc_strerror(uc_err code)\n{\n    switch (code) {\n    default:\n        return \"Unknown error code\";\n    case UC_ERR_OK:\n        return \"OK (UC_ERR_OK)\";\n    case UC_ERR_NOMEM:\n        return \"No memory available or memory not present (UC_ERR_NOMEM)\";\n    case UC_ERR_ARCH:\n        return \"Invalid/unsupported architecture (UC_ERR_ARCH)\";\n    case UC_ERR_HANDLE:\n        return \"Invalid handle (UC_ERR_HANDLE)\";\n    case UC_ERR_MODE:\n        return \"Invalid mode (UC_ERR_MODE)\";\n    case UC_ERR_VERSION:\n        return \"Different API version between core & binding (UC_ERR_VERSION)\";\n    case UC_ERR_READ_UNMAPPED:\n        return \"Invalid memory read (UC_ERR_READ_UNMAPPED)\";\n    case UC_ERR_WRITE_UNMAPPED:\n        return \"Invalid memory write (UC_ERR_WRITE_UNMAPPED)\";\n    case UC_ERR_FETCH_UNMAPPED:\n        return \"Invalid memory fetch (UC_ERR_FETCH_UNMAPPED)\";\n    case UC_ERR_HOOK:\n        return \"Invalid hook type (UC_ERR_HOOK)\";\n    case UC_ERR_INSN_INVALID:\n        return \"Invalid instruction (UC_ERR_INSN_INVALID)\";\n    case UC_ERR_MAP:\n        return \"Invalid memory mapping (UC_ERR_MAP)\";\n    case UC_ERR_WRITE_PROT:\n        return \"Write to write-protected memory (UC_ERR_WRITE_PROT)\";\n    case UC_ERR_READ_PROT:\n        return \"Read from non-readable memory (UC_ERR_READ_PROT)\";\n    case UC_ERR_FETCH_PROT:\n        return \"Fetch from non-executable memory (UC_ERR_FETCH_PROT)\";\n    case UC_ERR_ARG:\n        return \"Invalid argument (UC_ERR_ARG)\";\n    case UC_ERR_READ_UNALIGNED:\n        return \"Read from unaligned memory (UC_ERR_READ_UNALIGNED)\";\n    case UC_ERR_WRITE_UNALIGNED:\n        return \"Write to unaligned memory (UC_ERR_WRITE_UNALIGNED)\";\n    case UC_ERR_FETCH_UNALIGNED:\n        return \"Fetch from unaligned memory (UC_ERR_FETCH_UNALIGNED)\";\n    case UC_ERR_RESOURCE:\n        return \"Insufficient resource (UC_ERR_RESOURCE)\";\n    case UC_ERR_EXCEPTION:\n        return \"Unhandled CPU exception (UC_ERR_EXCEPTION)\";\n    }\n}\n\nUNICORN_EXPORT\nbool uc_arch_supported(uc_arch arch)\n{\n    switch (arch) {\n#ifdef UNICORN_HAS_ARM\n    case UC_ARCH_ARM:\n        return true;\n#endif\n#ifdef UNICORN_HAS_ARM64\n    case UC_ARCH_ARM64:\n        return true;\n#endif\n#ifdef UNICORN_HAS_M68K\n    case UC_ARCH_M68K:\n        return true;\n#endif\n#ifdef UNICORN_HAS_MIPS\n    case UC_ARCH_MIPS:\n        return true;\n#endif\n#ifdef UNICORN_HAS_PPC\n    case UC_ARCH_PPC:\n        return true;\n#endif\n#ifdef UNICORN_HAS_SPARC\n    case UC_ARCH_SPARC:\n        return true;\n#endif\n#ifdef UNICORN_HAS_X86\n    case UC_ARCH_X86:\n        return true;\n#endif\n#ifdef UNICORN_HAS_RISCV\n    case UC_ARCH_RISCV:\n        return true;\n#endif\n#ifdef UNICORN_HAS_S390X\n    case UC_ARCH_S390X:\n        return true;\n#endif\n    /* Invalid or disabled arch */\n    default:\n        return false;\n    }\n}\n\n#define UC_INIT(uc)                                                            \\\n    if (unlikely(!(uc)->init_done)) {                                          \\\n        int __init_ret = uc_init(uc);                                          \\\n        if (unlikely(__init_ret != UC_ERR_OK)) {                               \\\n            return __init_ret;                                                 \\\n        }                                                                      \\\n    }\n\nstatic gint uc_exits_cmp(gconstpointer a, gconstpointer b, gpointer user_data)\n{\n    uint64_t lhs = *((uint64_t *)a);\n    uint64_t rhs = *((uint64_t *)b);\n\n    if (lhs < rhs) {\n        return -1;\n    } else if (lhs == rhs) {\n        return 0;\n    } else {\n        return 1;\n    }\n}\n\nstatic uc_err uc_init(uc_engine *uc)\n{\n\n    if (uc->init_done) {\n        return UC_ERR_HANDLE;\n    }\n\n    uc->hooks_to_del.delete_fn = hook_delete;\n\n    for (int i = 0; i < UC_HOOK_MAX; i++) {\n        uc->hook[i].delete_fn = hook_delete;\n    }\n\n    uc->ctl_exits = g_tree_new_full(uc_exits_cmp, NULL, g_free, NULL);\n\n    if (machine_initialize(uc)) {\n        return UC_ERR_RESOURCE;\n    }\n\n    // init fpu softfloat\n    uc->softfloat_initialize();\n\n    if (uc->reg_reset) {\n        uc->reg_reset(uc);\n    }\n\n    uc->init_done = true;\n\n    return UC_ERR_OK;\n}\n\nUNICORN_EXPORT\nuc_err uc_open(uc_arch arch, uc_mode mode, uc_engine **result)\n{\n    struct uc_struct *uc;\n\n    if (arch < UC_ARCH_MAX) {\n        uc = calloc(1, sizeof(*uc));\n        if (!uc) {\n            // memory insufficient\n            return UC_ERR_NOMEM;\n        }\n\n        /* qemu/exec.c: phys_map_node_reserve() */\n        uc->alloc_hint = 16;\n        uc->errnum = UC_ERR_OK;\n        uc->arch = arch;\n        uc->mode = mode;\n\n        // uc->ram_list = { .blocks = QLIST_HEAD_INITIALIZER(ram_list.blocks) };\n        QLIST_INIT(&uc->ram_list.blocks);\n\n        QTAILQ_INIT(&uc->memory_listeners);\n\n        QTAILQ_INIT(&uc->address_spaces);\n\n        switch (arch) {\n        default:\n            break;\n#ifdef UNICORN_HAS_M68K\n        case UC_ARCH_M68K:\n            if ((mode & ~UC_MODE_M68K_MASK) || !(mode & UC_MODE_BIG_ENDIAN)) {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            uc->init_arch = m68k_uc_init;\n            break;\n#endif\n#ifdef UNICORN_HAS_X86\n        case UC_ARCH_X86:\n            if ((mode & ~UC_MODE_X86_MASK) || (mode & UC_MODE_BIG_ENDIAN) ||\n                !(mode & (UC_MODE_16 | UC_MODE_32 | UC_MODE_64))) {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            uc->init_arch = x86_uc_init;\n            break;\n#endif\n#ifdef UNICORN_HAS_ARM\n        case UC_ARCH_ARM:\n            if ((mode & ~UC_MODE_ARM_MASK)) {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            uc->init_arch = arm_uc_init;\n\n            if (mode & UC_MODE_THUMB) {\n                uc->thumb = 1;\n            }\n            break;\n#endif\n#ifdef UNICORN_HAS_ARM64\n        case UC_ARCH_ARM64:\n            if (mode & ~UC_MODE_ARM_MASK) {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            uc->init_arch = arm64_uc_init;\n            break;\n#endif\n\n#if defined(UNICORN_HAS_MIPS) || defined(UNICORN_HAS_MIPSEL) ||                \\\n    defined(UNICORN_HAS_MIPS64) || defined(UNICORN_HAS_MIPS64EL)\n        case UC_ARCH_MIPS:\n            if ((mode & ~UC_MODE_MIPS_MASK) ||\n                !(mode & (UC_MODE_MIPS32 | UC_MODE_MIPS64))) {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            if (mode & UC_MODE_BIG_ENDIAN) {\n#ifdef UNICORN_HAS_MIPS\n                if (mode & UC_MODE_MIPS32) {\n                    uc->init_arch = mips_uc_init;\n                }\n#endif\n#ifdef UNICORN_HAS_MIPS64\n                if (mode & UC_MODE_MIPS64) {\n                    uc->init_arch = mips64_uc_init;\n                }\n#endif\n            } else { // little endian\n#ifdef UNICORN_HAS_MIPSEL\n                if (mode & UC_MODE_MIPS32) {\n                    uc->init_arch = mipsel_uc_init;\n                }\n#endif\n#ifdef UNICORN_HAS_MIPS64EL\n                if (mode & UC_MODE_MIPS64) {\n                    uc->init_arch = mips64el_uc_init;\n                }\n#endif\n            }\n            break;\n#endif\n\n#ifdef UNICORN_HAS_SPARC\n        case UC_ARCH_SPARC:\n            if ((mode & ~UC_MODE_SPARC_MASK) || !(mode & UC_MODE_BIG_ENDIAN) ||\n                !(mode & (UC_MODE_SPARC32 | UC_MODE_SPARC64))) {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            if (mode & UC_MODE_SPARC64) {\n                uc->init_arch = sparc64_uc_init;\n            } else {\n                uc->init_arch = sparc_uc_init;\n            }\n            break;\n#endif\n#ifdef UNICORN_HAS_PPC\n        case UC_ARCH_PPC:\n            if ((mode & ~UC_MODE_PPC_MASK) || !(mode & UC_MODE_BIG_ENDIAN) ||\n                !(mode & (UC_MODE_PPC32 | UC_MODE_PPC64))) {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            if (mode & UC_MODE_PPC64) {\n                uc->init_arch = ppc64_uc_init;\n            } else {\n                uc->init_arch = ppc_uc_init;\n            }\n            break;\n#endif\n#ifdef UNICORN_HAS_RISCV\n        case UC_ARCH_RISCV:\n            if ((mode & ~UC_MODE_RISCV_MASK) ||\n                !(mode & (UC_MODE_RISCV32 | UC_MODE_RISCV64))) {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            if (mode & UC_MODE_RISCV32) {\n                uc->init_arch = riscv32_uc_init;\n            } else if (mode & UC_MODE_RISCV64) {\n                uc->init_arch = riscv64_uc_init;\n            } else {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            break;\n#endif\n#ifdef UNICORN_HAS_S390X\n        case UC_ARCH_S390X:\n            if ((mode & ~UC_MODE_S390X_MASK) || !(mode & UC_MODE_BIG_ENDIAN)) {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            uc->init_arch = s390_uc_init;\n            break;\n#endif\n        }\n\n        if (uc->init_arch == NULL) {\n            return UC_ERR_ARCH;\n        }\n\n        uc->init_done = false;\n        uc->cpu_model = INT_MAX; // INT_MAX means the default cpu model.\n\n        *result = uc;\n\n        return UC_ERR_OK;\n    } else {\n        return UC_ERR_ARCH;\n    }\n}\n\nUNICORN_EXPORT\nuc_err uc_close(uc_engine *uc)\n{\n    int i;\n    MemoryRegion *mr;\n\n    if (!uc->init_done) {\n        free(uc);\n        return UC_ERR_OK;\n    }\n\n    // Cleanup internally.\n    if (uc->release) {\n        uc->release(uc->tcg_ctx);\n    }\n    g_free(uc->tcg_ctx);\n\n    // Cleanup CPU.\n    g_free(uc->cpu->cpu_ases);\n    g_free(uc->cpu->thread);\n\n    /* cpu */\n    free(uc->cpu);\n\n    /* flatviews */\n    g_hash_table_destroy(uc->flat_views);\n\n    // During flatviews destruction, we may still access memory regions.\n    // So we free them afterwards.\n    /* memory */\n    mr = &uc->io_mem_unassigned;\n    mr->destructor(mr);\n    mr = uc->system_io;\n    mr->destructor(mr);\n    mr = uc->system_memory;\n    mr->destructor(mr);\n    g_free(uc->system_memory);\n    g_free(uc->system_io);\n\n    // Thread relateds.\n    if (uc->qemu_thread_data) {\n        g_free(uc->qemu_thread_data);\n    }\n\n    /* free */\n    g_free(uc->init_target_page);\n\n    // Other auxilaries.\n    g_free(uc->l1_map);\n\n    if (uc->bounce.buffer) {\n        free(uc->bounce.buffer);\n    }\n\n    // free hooks and hook lists\n    clear_deleted_hooks(uc);\n\n    for (i = 0; i < UC_HOOK_MAX; i++) {\n        list_clear(&uc->hook[i]);\n    }\n\n    free(uc->mapped_blocks);\n\n    g_tree_destroy(uc->ctl_exits);\n\n    // finally, free uc itself.\n    memset(uc, 0, sizeof(*uc));\n    free(uc);\n\n    return UC_ERR_OK;\n}\n\nUNICORN_EXPORT\nuc_err uc_reg_read_batch(uc_engine *uc, int *ids, void **vals, int count)\n{\n    int ret = UC_ERR_OK;\n\n    UC_INIT(uc);\n\n    if (uc->reg_read) {\n        ret = uc->reg_read(uc, (unsigned int *)ids, vals, count);\n    } else {\n        return UC_ERR_HANDLE;\n    }\n\n    return ret;\n}\n\nUNICORN_EXPORT\nuc_err uc_reg_write_batch(uc_engine *uc, int *ids, void *const *vals, int count)\n{\n    int ret = UC_ERR_OK;\n\n    UC_INIT(uc);\n\n    if (uc->reg_write) {\n        ret = uc->reg_write(uc, (unsigned int *)ids, vals, count);\n    } else {\n        return UC_ERR_HANDLE;\n    }\n\n    return ret;\n}\n\nUNICORN_EXPORT\nuc_err uc_reg_read(uc_engine *uc, int regid, void *value)\n{\n    UC_INIT(uc);\n    return uc_reg_read_batch(uc, &regid, &value, 1);\n}\n\nUNICORN_EXPORT\nuc_err uc_reg_write(uc_engine *uc, int regid, const void *value)\n{\n    UC_INIT(uc);\n    return uc_reg_write_batch(uc, &regid, (void *const *)&value, 1);\n}\n\n// check if a memory area is mapped\n// this is complicated because an area can overlap adjacent blocks\nstatic bool check_mem_area(uc_engine *uc, uint64_t address, size_t size)\n{\n    size_t count = 0, len;\n\n    while (count < size) {\n        MemoryRegion *mr = memory_mapping(uc, address);\n        if (mr) {\n            len = (size_t)MIN(size - count, mr->end - address);\n            count += len;\n            address += len;\n        } else { // this address is not mapped in yet\n            break;\n        }\n    }\n\n    return (count == size);\n}\n\nUNICORN_EXPORT\nuc_err uc_mem_read(uc_engine *uc, uint64_t address, void *_bytes, size_t size)\n{\n    size_t count = 0, len;\n    uint8_t *bytes = _bytes;\n\n    UC_INIT(uc);\n\n    // qemu cpu_physical_memory_rw() size is an int\n    if (size > INT_MAX)\n        return UC_ERR_ARG;\n\n    if (uc->mem_redirect) {\n        address = uc->mem_redirect(address);\n    }\n\n    if (!check_mem_area(uc, address, size)) {\n        return UC_ERR_READ_UNMAPPED;\n    }\n\n    // memory area can overlap adjacent memory blocks\n    while (count < size) {\n        MemoryRegion *mr = memory_mapping(uc, address);\n        if (mr) {\n            len = (size_t)MIN(size - count, mr->end - address);\n            if (uc->read_mem(&uc->address_space_memory, address, bytes, len) ==\n                false) {\n                break;\n            }\n            count += len;\n            address += len;\n            bytes += len;\n        } else { // this address is not mapped in yet\n            break;\n        }\n    }\n\n    if (count == size) {\n        return UC_ERR_OK;\n    } else {\n        return UC_ERR_READ_UNMAPPED;\n    }\n}\n\nUNICORN_EXPORT\nuc_err uc_mem_write(uc_engine *uc, uint64_t address, const void *_bytes,\n                    size_t size)\n{\n    size_t count = 0, len;\n    const uint8_t *bytes = _bytes;\n\n    UC_INIT(uc);\n\n    // qemu cpu_physical_memory_rw() size is an int\n    if (size > INT_MAX)\n        return UC_ERR_ARG;\n\n    if (uc->mem_redirect) {\n        address = uc->mem_redirect(address);\n    }\n\n    if (!check_mem_area(uc, address, size)) {\n        return UC_ERR_WRITE_UNMAPPED;\n    }\n\n    // memory area can overlap adjacent memory blocks\n    while (count < size) {\n        MemoryRegion *mr = memory_mapping(uc, address);\n        if (mr) {\n            uint32_t operms = mr->perms;\n            if (!(operms & UC_PROT_WRITE)) { // write protected\n                // but this is not the program accessing memory, so temporarily\n                // mark writable\n                uc->readonly_mem(mr, false);\n            }\n\n            len = (size_t)MIN(size - count, mr->end - address);\n            if (uc->write_mem(&uc->address_space_memory, address, bytes, len) ==\n                false) {\n                break;\n            }\n\n            if (!(operms & UC_PROT_WRITE)) { // write protected\n                // now write protect it again\n                uc->readonly_mem(mr, true);\n            }\n\n            count += len;\n            address += len;\n            bytes += len;\n        } else { // this address is not mapped in yet\n            break;\n        }\n    }\n\n    if (count == size) {\n        return UC_ERR_OK;\n    } else {\n        return UC_ERR_WRITE_UNMAPPED;\n    }\n}\n\n#define TIMEOUT_STEP 2 // microseconds\nstatic void *_timeout_fn(void *arg)\n{\n    struct uc_struct *uc = arg;\n    int64_t current_time = get_clock();\n\n    do {\n        usleep(TIMEOUT_STEP);\n        // perhaps emulation is even done before timeout?\n        if (uc->emulation_done) {\n            break;\n        }\n    } while ((uint64_t)(get_clock() - current_time) < uc->timeout);\n\n    // timeout before emulation is done?\n    if (!uc->emulation_done) {\n        uc->timed_out = true;\n        // force emulation to stop\n        uc_emu_stop(uc);\n    }\n\n    return NULL;\n}\n\nstatic void enable_emu_timer(uc_engine *uc, uint64_t timeout)\n{\n    uc->timeout = timeout;\n    qemu_thread_create(uc, &uc->timer, \"timeout\", _timeout_fn, uc,\n                       QEMU_THREAD_JOINABLE);\n}\n\nstatic void hook_count_cb(struct uc_struct *uc, uint64_t address, uint32_t size,\n                          void *user_data)\n{\n    // count this instruction. ah ah ah.\n    uc->emu_counter++;\n    // printf(\":: emu counter = %u, at %lx\\n\", uc->emu_counter, address);\n\n    if (uc->emu_counter > uc->emu_count) {\n        // printf(\":: emu counter = %u, stop emulation\\n\", uc->emu_counter);\n        uc_emu_stop(uc);\n    }\n}\n\nstatic void clear_deleted_hooks(uc_engine *uc)\n{\n    struct list_item *cur;\n    struct hook *hook;\n    int i;\n\n    for (cur = uc->hooks_to_del.head;\n         cur != NULL && (hook = (struct hook *)cur->data); cur = cur->next) {\n        assert(hook->to_delete);\n        for (i = 0; i < UC_HOOK_MAX; i++) {\n            if (list_remove(&uc->hook[i], (void *)hook)) {\n                break;\n            }\n        }\n    }\n\n    list_clear(&uc->hooks_to_del);\n}\n\nUNICORN_EXPORT\nuc_err uc_emu_start(uc_engine *uc, uint64_t begin, uint64_t until,\n                    uint64_t timeout, size_t count)\n{\n    uc_err err;\n\n    // reset the counter\n    uc->emu_counter = 0;\n    uc->invalid_error = UC_ERR_OK;\n    uc->emulation_done = false;\n    uc->size_recur_mem = 0;\n    uc->timed_out = false;\n    uc->first_tb = true;\n\n    UC_INIT(uc);\n\n    // Advance the nested levels. We must decrease the level count by one when\n    // we return from uc_emu_start.\n    if (uc->nested_level >= UC_MAX_NESTED_LEVEL) {\n        // We can't support so many nested levels.\n        return UC_ERR_RESOURCE;\n    }\n    uc->nested_level++;\n\n    switch (uc->arch) {\n    default:\n        break;\n#ifdef UNICORN_HAS_M68K\n    case UC_ARCH_M68K:\n        uc_reg_write(uc, UC_M68K_REG_PC, &begin);\n        break;\n#endif\n#ifdef UNICORN_HAS_X86\n    case UC_ARCH_X86:\n        switch (uc->mode) {\n        default:\n            break;\n        case UC_MODE_16: {\n            uint64_t ip;\n            uint16_t cs;\n\n            uc_reg_read(uc, UC_X86_REG_CS, &cs);\n            // compensate for later adding up IP & CS\n            ip = begin - cs * 16;\n            uc_reg_write(uc, UC_X86_REG_IP, &ip);\n            break;\n        }\n        case UC_MODE_32:\n            uc_reg_write(uc, UC_X86_REG_EIP, &begin);\n            break;\n        case UC_MODE_64:\n            uc_reg_write(uc, UC_X86_REG_RIP, &begin);\n            break;\n        }\n        break;\n#endif\n#ifdef UNICORN_HAS_ARM\n    case UC_ARCH_ARM:\n        uc_reg_write(uc, UC_ARM_REG_R15, &begin);\n        break;\n#endif\n#ifdef UNICORN_HAS_ARM64\n    case UC_ARCH_ARM64:\n        uc_reg_write(uc, UC_ARM64_REG_PC, &begin);\n        break;\n#endif\n#ifdef UNICORN_HAS_MIPS\n    case UC_ARCH_MIPS:\n        // TODO: MIPS32/MIPS64/BIGENDIAN etc\n        uc_reg_write(uc, UC_MIPS_REG_PC, &begin);\n        break;\n#endif\n#ifdef UNICORN_HAS_SPARC\n    case UC_ARCH_SPARC:\n        // TODO: Sparc/Sparc64\n        uc_reg_write(uc, UC_SPARC_REG_PC, &begin);\n        break;\n#endif\n#ifdef UNICORN_HAS_PPC\n    case UC_ARCH_PPC:\n        uc_reg_write(uc, UC_PPC_REG_PC, &begin);\n        break;\n#endif\n#ifdef UNICORN_HAS_RISCV\n    case UC_ARCH_RISCV:\n        uc_reg_write(uc, UC_RISCV_REG_PC, &begin);\n        break;\n#endif\n#ifdef UNICORN_HAS_S390X\n    case UC_ARCH_S390X:\n        uc_reg_write(uc, UC_S390X_REG_PC, &begin);\n        break;\n#endif\n    }\n\n    uc->stop_request = false;\n\n    uc->emu_count = count;\n    // remove count hook if counting isn't necessary\n    if (count <= 0 && uc->count_hook != 0) {\n        uc_hook_del(uc, uc->count_hook);\n        uc->count_hook = 0;\n    }\n    // set up count hook to count instructions.\n    if (count > 0 && uc->count_hook == 0) {\n        uc_err err;\n        // callback to count instructions must be run before everything else,\n        // so instead of appending, we must insert the hook at the begin\n        // of the hook list\n        uc->hook_insert = 1;\n        err = uc_hook_add(uc, &uc->count_hook, UC_HOOK_CODE, hook_count_cb,\n                          NULL, 1, 0);\n        // restore to append mode for uc_hook_add()\n        uc->hook_insert = 0;\n        if (err != UC_ERR_OK) {\n            uc->nested_level--;\n            return err;\n        }\n    }\n\n    // If UC_CTL_UC_USE_EXITS is set, then the @until param won't have any\n    // effect. This is designed for the backward compatibility.\n    if (!uc->use_exits) {\n        uc->exits[uc->nested_level - 1] = until;\n    }\n\n    if (timeout) {\n        enable_emu_timer(uc, timeout * 1000); // microseconds -> nanoseconds\n    }\n\n    uc->vm_start(uc);\n\n    uc->nested_level--;\n\n    // emulation is done if and only if we exit the outer uc_emu_start\n    // or we may lost uc_emu_stop\n    if (uc->nested_level == 0) {\n        uc->emulation_done = true;\n    }\n\n    // remove hooks to delete\n    clear_deleted_hooks(uc);\n\n    if (timeout) {\n        // wait for the timer to finish\n        qemu_thread_join(&uc->timer);\n    }\n\n    // We may be in a nested uc_emu_start and thus clear invalid_error\n    // once we are done.\n    err = uc->invalid_error;\n    uc->invalid_error = 0;\n    return err;\n}\n\nUNICORN_EXPORT\nuc_err uc_emu_stop(uc_engine *uc)\n{\n    UC_INIT(uc);\n\n    if (uc->emulation_done) {\n        return UC_ERR_OK;\n    }\n\n    uc->stop_request = true;\n    // TODO: make this atomic somehow?\n    if (uc->cpu) {\n        // exit the current TB\n        cpu_exit(uc->cpu);\n    }\n\n    return UC_ERR_OK;\n}\n\n// return target index where a memory region at the address exists, or could be\n// inserted\n//\n// address either is inside the mapping at the returned index, or is in free\n// space before the next mapping.\n//\n// if there is overlap, between regions, ending address will be higher than the\n// starting address of the mapping at returned index\nstatic int bsearch_mapped_blocks(const uc_engine *uc, uint64_t address)\n{\n    int left, right, mid;\n    MemoryRegion *mapping;\n\n    left = 0;\n    right = uc->mapped_block_count;\n\n    while (left < right) {\n        mid = left + (right - left) / 2;\n\n        mapping = uc->mapped_blocks[mid];\n\n        if (mapping->end - 1 < address) {\n            left = mid + 1;\n        } else if (mapping->addr > address) {\n            right = mid;\n        } else {\n            return mid;\n        }\n    }\n\n    return left;\n}\n\n// find if a memory range overlaps with existing mapped regions\nstatic bool memory_overlap(struct uc_struct *uc, uint64_t begin, size_t size)\n{\n    unsigned int i;\n    uint64_t end = begin + size - 1;\n\n    i = bsearch_mapped_blocks(uc, begin);\n\n    // is this the highest region with no possible overlap?\n    if (i >= uc->mapped_block_count)\n        return false;\n\n    // end address overlaps this region?\n    if (end >= uc->mapped_blocks[i]->addr)\n        return true;\n\n    // not found\n    return false;\n}\n\n// common setup/error checking shared between uc_mem_map and uc_mem_map_ptr\nstatic uc_err mem_map(uc_engine *uc, uint64_t address, size_t size,\n                      uint32_t perms, MemoryRegion *block)\n{\n    MemoryRegion **regions;\n    int pos;\n\n    if (block == NULL) {\n        return UC_ERR_NOMEM;\n    }\n\n    if ((uc->mapped_block_count & (MEM_BLOCK_INCR - 1)) == 0) { // time to grow\n        regions = (MemoryRegion **)g_realloc(\n            uc->mapped_blocks,\n            sizeof(MemoryRegion *) * (uc->mapped_block_count + MEM_BLOCK_INCR));\n        if (regions == NULL) {\n            return UC_ERR_NOMEM;\n        }\n        uc->mapped_blocks = regions;\n    }\n\n    pos = bsearch_mapped_blocks(uc, block->addr);\n\n    // shift the array right to give space for the new pointer\n    memmove(&uc->mapped_blocks[pos + 1], &uc->mapped_blocks[pos],\n            sizeof(MemoryRegion *) * (uc->mapped_block_count - pos));\n\n    uc->mapped_blocks[pos] = block;\n    uc->mapped_block_count++;\n\n    return UC_ERR_OK;\n}\n\nstatic uc_err mem_map_check(uc_engine *uc, uint64_t address, size_t size,\n                            uint32_t perms)\n{\n    if (size == 0) {\n        // invalid memory mapping\n        return UC_ERR_ARG;\n    }\n\n    // address cannot wrapp around\n    if (address + size - 1 < address) {\n        return UC_ERR_ARG;\n    }\n\n    // address must be aligned to uc->target_page_size\n    if ((address & uc->target_page_align) != 0) {\n        return UC_ERR_ARG;\n    }\n\n    // size must be multiple of uc->target_page_size\n    if ((size & uc->target_page_align) != 0) {\n        return UC_ERR_ARG;\n    }\n\n    // check for only valid permissions\n    if ((perms & ~UC_PROT_ALL) != 0) {\n        return UC_ERR_ARG;\n    }\n\n    // this area overlaps existing mapped regions?\n    if (memory_overlap(uc, address, size)) {\n        return UC_ERR_MAP;\n    }\n\n    return UC_ERR_OK;\n}\n\nUNICORN_EXPORT\nuc_err uc_mem_map(uc_engine *uc, uint64_t address, size_t size, uint32_t perms)\n{\n    uc_err res;\n\n    UC_INIT(uc);\n\n    if (uc->mem_redirect) {\n        address = uc->mem_redirect(address);\n    }\n\n    res = mem_map_check(uc, address, size, perms);\n    if (res) {\n        return res;\n    }\n\n    return mem_map(uc, address, size, perms,\n                   uc->memory_map(uc, address, size, perms));\n}\n\nUNICORN_EXPORT\nuc_err uc_mem_map_ptr(uc_engine *uc, uint64_t address, size_t size,\n                      uint32_t perms, void *ptr)\n{\n    uc_err res;\n\n    UC_INIT(uc);\n\n    if (ptr == NULL) {\n        return UC_ERR_ARG;\n    }\n\n    if (uc->mem_redirect) {\n        address = uc->mem_redirect(address);\n    }\n\n    res = mem_map_check(uc, address, size, perms);\n    if (res) {\n        return res;\n    }\n\n    return mem_map(uc, address, size, UC_PROT_ALL,\n                   uc->memory_map_ptr(uc, address, size, perms, ptr));\n}\n\nUNICORN_EXPORT\nuc_err uc_mmio_map(uc_engine *uc, uint64_t address, size_t size,\n                   uc_cb_mmio_read_t read_cb, void *user_data_read,\n                   uc_cb_mmio_write_t write_cb, void *user_data_write)\n{\n    uc_err res;\n\n    UC_INIT(uc);\n\n    if (uc->mem_redirect) {\n        address = uc->mem_redirect(address);\n    }\n\n    res = mem_map_check(uc, address, size, UC_PROT_ALL);\n    if (res)\n        return res;\n\n    // The callbacks do not need to be checked for NULL here, as their presence\n    // (or lack thereof) will determine the permissions used.\n    return mem_map(uc, address, size, UC_PROT_NONE,\n                   uc->memory_map_io(uc, address, size, read_cb, write_cb,\n                                     user_data_read, user_data_write));\n}\n\n// Create a backup copy of the indicated MemoryRegion.\n// Generally used in prepartion for splitting a MemoryRegion.\nstatic uint8_t *copy_region(struct uc_struct *uc, MemoryRegion *mr)\n{\n    uint8_t *block = (uint8_t *)g_malloc0((size_t)int128_get64(mr->size));\n    if (block != NULL) {\n        uc_err err =\n            uc_mem_read(uc, mr->addr, block, (size_t)int128_get64(mr->size));\n        if (err != UC_ERR_OK) {\n            free(block);\n            block = NULL;\n        }\n    }\n\n    return block;\n}\n\n/*\n    This function is similar to split_region, but for MMIO memory.\n\n    This function would delete the region unconditionally.\n\n    Note this function may be called recursively.\n*/\nstatic bool split_mmio_region(struct uc_struct *uc, MemoryRegion *mr,\n                              uint64_t address, size_t size)\n{\n    uint64_t begin, end, chunk_end;\n    size_t l_size, r_size;\n    mmio_cbs backup;\n\n    chunk_end = address + size;\n\n    // This branch also break recursion.\n    if (address <= mr->addr && chunk_end >= mr->end) {\n        return true;\n    }\n\n    if (size == 0) {\n        return false;\n    }\n\n    begin = mr->addr;\n    end = mr->end;\n\n    memcpy(&backup, mr->opaque, sizeof(mmio_cbs));\n\n    /* overlapping cases\n     *               |------mr------|\n     * case 1    |---size--|            // Is it possible???\n     * case 2           |--size--|\n     * case 3                  |---size--|\n     */\n\n    // unmap this region first, then do split it later\n    if (uc_mem_unmap(uc, mr->addr, (size_t)int128_get64(mr->size)) !=\n        UC_ERR_OK) {\n        return false;\n    }\n\n    // adjust some things\n    if (address < begin) {\n        address = begin;\n    }\n    if (chunk_end > end) {\n        chunk_end = end;\n    }\n\n    // compute sub region sizes\n    l_size = (size_t)(address - begin);\n    r_size = (size_t)(end - chunk_end);\n\n    if (l_size > 0) {\n        if (uc_mmio_map(uc, begin, l_size, backup.read, backup.user_data_read,\n                        backup.write, backup.user_data_write) != UC_ERR_OK) {\n            return false;\n        }\n    }\n\n    if (r_size > 0) {\n        if (uc_mmio_map(uc, chunk_end, r_size, backup.read,\n                        backup.user_data_read, backup.write,\n                        backup.user_data_write) != UC_ERR_OK) {\n            return false;\n        }\n    }\n\n    return true;\n}\n\n/*\n   Split the given MemoryRegion at the indicated address for the indicated size\n   this may result in the create of up to 3 spanning sections. If the delete\n   parameter is true, the no new section will be created to replace the indicate\n   range. This functions exists to support uc_mem_protect and uc_mem_unmap.\n\n   This is a static function and callers have already done some preliminary\n   parameter validation.\n\n   The do_delete argument indicates that we are being called to support\n   uc_mem_unmap. In this case we save some time by choosing NOT to remap\n   the areas that are intended to get unmapped\n */\n// TODO: investigate whether qemu region manipulation functions already offered\n// this capability\nstatic bool split_region(struct uc_struct *uc, MemoryRegion *mr,\n                         uint64_t address, size_t size, bool do_delete)\n{\n    uint8_t *backup;\n    uint32_t perms;\n    uint64_t begin, end, chunk_end;\n    size_t l_size, m_size, r_size;\n    RAMBlock *block = NULL;\n    bool prealloc = false;\n\n    chunk_end = address + size;\n\n    // if this region belongs to area [address, address+size],\n    // then there is no work to do.\n    if (address <= mr->addr && chunk_end >= mr->end) {\n        return true;\n    }\n\n    if (size == 0) {\n        // trivial case\n        return true;\n    }\n\n    if (address >= mr->end || chunk_end <= mr->addr) {\n        // impossible case\n        return false;\n    }\n\n    // Find the correct and large enough (which contains our target mr)\n    // to create the content backup.\n    QLIST_FOREACH(block, &uc->ram_list.blocks, next)\n    {\n        // block->offset is the offset within ram_addr_t, not GPA\n        if (block->mr->addr <= mr->addr &&\n            block->used_length + block->mr->addr >= mr->end) {\n            break;\n        }\n    }\n\n    if (block == NULL) {\n        return false;\n    }\n\n    // RAM_PREALLOC is not defined outside exec.c and I didn't feel like\n    // moving it\n    prealloc = !!(block->flags & 1);\n\n    if (block->flags & 1) {\n        backup = block->host;\n    } else {\n        backup = copy_region(uc, mr);\n        if (backup == NULL) {\n            return false;\n        }\n    }\n\n    // save the essential information required for the split before mr gets\n    // deleted\n    perms = mr->perms;\n    begin = mr->addr;\n    end = mr->end;\n\n    // unmap this region first, then do split it later\n    if (uc_mem_unmap(uc, mr->addr, (size_t)int128_get64(mr->size)) !=\n        UC_ERR_OK) {\n        goto error;\n    }\n\n    /* overlapping cases\n     *               |------mr------|\n     * case 1    |---size--|\n     * case 2           |--size--|\n     * case 3                  |---size--|\n     */\n\n    // adjust some things\n    if (address < begin) {\n        address = begin;\n    }\n    if (chunk_end > end) {\n        chunk_end = end;\n    }\n\n    // compute sub region sizes\n    l_size = (size_t)(address - begin);\n    r_size = (size_t)(end - chunk_end);\n    m_size = (size_t)(chunk_end - address);\n\n    // If there are error in any of the below operations, things are too far\n    // gone at that point to recover. Could try to remap orignal region, but\n    // these smaller allocation just failed so no guarantee that we can recover\n    // the original allocation at this point\n    if (l_size > 0) {\n        if (!prealloc) {\n            if (uc_mem_map(uc, begin, l_size, perms) != UC_ERR_OK) {\n                goto error;\n            }\n            if (uc_mem_write(uc, begin, backup, l_size) != UC_ERR_OK) {\n                goto error;\n            }\n        } else {\n            if (uc_mem_map_ptr(uc, begin, l_size, perms, backup) != UC_ERR_OK) {\n                goto error;\n            }\n        }\n    }\n\n    if (m_size > 0 && !do_delete) {\n        if (!prealloc) {\n            if (uc_mem_map(uc, address, m_size, perms) != UC_ERR_OK) {\n                goto error;\n            }\n            if (uc_mem_write(uc, address, backup + l_size, m_size) !=\n                UC_ERR_OK) {\n                goto error;\n            }\n        } else {\n            if (uc_mem_map_ptr(uc, address, m_size, perms, backup + l_size) !=\n                UC_ERR_OK) {\n                goto error;\n            }\n        }\n    }\n\n    if (r_size > 0) {\n        if (!prealloc) {\n            if (uc_mem_map(uc, chunk_end, r_size, perms) != UC_ERR_OK) {\n                goto error;\n            }\n            if (uc_mem_write(uc, chunk_end, backup + l_size + m_size, r_size) !=\n                UC_ERR_OK) {\n                goto error;\n            }\n        } else {\n            if (uc_mem_map_ptr(uc, chunk_end, r_size, perms,\n                               backup + l_size + m_size) != UC_ERR_OK) {\n                goto error;\n            }\n        }\n    }\n\n    if (!prealloc) {\n        free(backup);\n    }\n    return true;\n\nerror:\n    if (!prealloc) {\n        free(backup);\n    }\n    return false;\n}\n\nUNICORN_EXPORT\nuc_err uc_mem_protect(struct uc_struct *uc, uint64_t address, size_t size,\n                      uint32_t perms)\n{\n    MemoryRegion *mr;\n    uint64_t addr = address;\n    size_t count, len;\n    bool remove_exec = false;\n\n    UC_INIT(uc);\n\n    if (size == 0) {\n        // trivial case, no change\n        return UC_ERR_OK;\n    }\n\n    // address must be aligned to uc->target_page_size\n    if ((address & uc->target_page_align) != 0) {\n        return UC_ERR_ARG;\n    }\n\n    // size must be multiple of uc->target_page_size\n    if ((size & uc->target_page_align) != 0) {\n        return UC_ERR_ARG;\n    }\n\n    // check for only valid permissions\n    if ((perms & ~UC_PROT_ALL) != 0) {\n        return UC_ERR_ARG;\n    }\n\n    if (uc->mem_redirect) {\n        address = uc->mem_redirect(address);\n    }\n\n    // check that user's entire requested block is mapped\n    if (!check_mem_area(uc, address, size)) {\n        return UC_ERR_NOMEM;\n    }\n\n    // Now we know entire region is mapped, so change permissions\n    // We may need to split regions if this area spans adjacent regions\n    addr = address;\n    count = 0;\n    while (count < size) {\n        mr = memory_mapping(uc, addr);\n        len = (size_t)MIN(size - count, mr->end - addr);\n        if (!split_region(uc, mr, addr, len, false)) {\n            return UC_ERR_NOMEM;\n        }\n\n        mr = memory_mapping(uc, addr);\n        // will this remove EXEC permission?\n        if (((mr->perms & UC_PROT_EXEC) != 0) &&\n            ((perms & UC_PROT_EXEC) == 0)) {\n            remove_exec = true;\n        }\n        mr->perms = perms;\n        uc->readonly_mem(mr, (perms & UC_PROT_WRITE) == 0);\n\n        count += len;\n        addr += len;\n    }\n\n    // if EXEC permission is removed, then quit TB and continue at the same\n    // place\n    if (remove_exec) {\n        uc->quit_request = true;\n        uc_emu_stop(uc);\n    }\n\n    return UC_ERR_OK;\n}\n\nUNICORN_EXPORT\nuc_err uc_mem_unmap(struct uc_struct *uc, uint64_t address, size_t size)\n{\n    MemoryRegion *mr;\n    uint64_t addr;\n    size_t count, len;\n\n    UC_INIT(uc);\n\n    if (size == 0) {\n        // nothing to unmap\n        return UC_ERR_OK;\n    }\n\n    // address must be aligned to uc->target_page_size\n    if ((address & uc->target_page_align) != 0) {\n        return UC_ERR_ARG;\n    }\n\n    // size must be multiple of uc->target_page_size\n    if ((size & uc->target_page_align) != 0) {\n        return UC_ERR_ARG;\n    }\n\n    if (uc->mem_redirect) {\n        address = uc->mem_redirect(address);\n    }\n\n    // check that user's entire requested block is mapped\n    if (!check_mem_area(uc, address, size)) {\n        return UC_ERR_NOMEM;\n    }\n\n    // Now we know entire region is mapped, so do the unmap\n    // We may need to split regions if this area spans adjacent regions\n    addr = address;\n    count = 0;\n    while (count < size) {\n        mr = memory_mapping(uc, addr);\n        len = (size_t)MIN(size - count, mr->end - addr);\n        if (!mr->ram) {\n            if (!split_mmio_region(uc, mr, addr, len)) {\n                return UC_ERR_NOMEM;\n            }\n        } else {\n            if (!split_region(uc, mr, addr, len, true)) {\n                return UC_ERR_NOMEM;\n            }\n        }\n\n        // if we can retrieve the mapping, then no splitting took place\n        // so unmap here\n        mr = memory_mapping(uc, addr);\n        if (mr != NULL) {\n            uc->memory_unmap(uc, mr);\n        }\n        count += len;\n        addr += len;\n    }\n\n    return UC_ERR_OK;\n}\n\n// find the memory region of this address\nMemoryRegion *memory_mapping(struct uc_struct *uc, uint64_t address)\n{\n    unsigned int i;\n\n    if (uc->mapped_block_count == 0) {\n        return NULL;\n    }\n\n    if (uc->mem_redirect) {\n        address = uc->mem_redirect(address);\n    }\n\n    // try with the cache index first\n    i = uc->mapped_block_cache_index;\n\n    if (i < uc->mapped_block_count && address >= uc->mapped_blocks[i]->addr &&\n        address < uc->mapped_blocks[i]->end) {\n        return uc->mapped_blocks[i];\n    }\n\n    i = bsearch_mapped_blocks(uc, address);\n\n    if (i < uc->mapped_block_count && address >= uc->mapped_blocks[i]->addr &&\n        address <= uc->mapped_blocks[i]->end - 1)\n        return uc->mapped_blocks[i];\n\n    // not found\n    return NULL;\n}\n\nUNICORN_EXPORT\nuc_err uc_hook_add(uc_engine *uc, uc_hook *hh, int type, void *callback,\n                   void *user_data, uint64_t begin, uint64_t end, ...)\n{\n    int ret = UC_ERR_OK;\n    int i = 0;\n\n    UC_INIT(uc);\n\n    struct hook *hook = calloc(1, sizeof(struct hook));\n    if (hook == NULL) {\n        return UC_ERR_NOMEM;\n    }\n\n    hook->begin = begin;\n    hook->end = end;\n    hook->type = type;\n    hook->callback = callback;\n    hook->user_data = user_data;\n    hook->refs = 0;\n    hook->to_delete = false;\n    *hh = (uc_hook)hook;\n\n    // UC_HOOK_INSN has an extra argument for instruction ID\n    if (type & UC_HOOK_INSN) {\n        va_list valist;\n\n        va_start(valist, end);\n        hook->insn = va_arg(valist, int);\n        va_end(valist);\n\n        if (uc->insn_hook_validate) {\n            if (!uc->insn_hook_validate(hook->insn)) {\n                free(hook);\n                return UC_ERR_HOOK;\n            }\n        }\n\n        if (uc->hook_insert) {\n            if (hook_insert(&uc->hook[UC_HOOK_INSN_IDX], hook) == NULL) {\n                free(hook);\n                return UC_ERR_NOMEM;\n            }\n        } else {\n            if (hook_append(&uc->hook[UC_HOOK_INSN_IDX], hook) == NULL) {\n                free(hook);\n                return UC_ERR_NOMEM;\n            }\n        }\n\n        uc->hooks_count[UC_HOOK_INSN_IDX]++;\n        return UC_ERR_OK;\n    }\n\n    if (type & UC_HOOK_TCG_OPCODE) {\n        va_list valist;\n\n        va_start(valist, end);\n        hook->op = va_arg(valist, int);\n        hook->op_flags = va_arg(valist, int);\n        va_end(valist);\n\n        if (uc->opcode_hook_invalidate) {\n            if (!uc->opcode_hook_invalidate(hook->op, hook->op_flags)) {\n                free(hook);\n                return UC_ERR_HOOK;\n            }\n        }\n\n        if (uc->hook_insert) {\n            if (hook_insert(&uc->hook[UC_HOOK_TCG_OPCODE_IDX], hook) == NULL) {\n                free(hook);\n                return UC_ERR_NOMEM;\n            }\n        } else {\n            if (hook_append(&uc->hook[UC_HOOK_TCG_OPCODE_IDX], hook) == NULL) {\n                free(hook);\n                return UC_ERR_NOMEM;\n            }\n        }\n\n        uc->hooks_count[UC_HOOK_TCG_OPCODE_IDX]++;\n        return UC_ERR_OK;\n    }\n\n    while ((type >> i) > 0) {\n        if ((type >> i) & 1) {\n            // TODO: invalid hook error?\n            if (i < UC_HOOK_MAX) {\n                if (uc->hook_insert) {\n                    if (hook_insert(&uc->hook[i], hook) == NULL) {\n                        free(hook);\n                        return UC_ERR_NOMEM;\n                    }\n                } else {\n                    if (hook_append(&uc->hook[i], hook) == NULL) {\n                        free(hook);\n                        return UC_ERR_NOMEM;\n                    }\n                }\n                uc->hooks_count[i]++;\n            }\n        }\n        i++;\n    }\n\n    // we didn't use the hook\n    // TODO: return an error?\n    if (hook->refs == 0) {\n        free(hook);\n    }\n\n    return ret;\n}\n\nUNICORN_EXPORT\nuc_err uc_hook_del(uc_engine *uc, uc_hook hh)\n{\n    int i;\n    struct hook *hook = (struct hook *)hh;\n\n    UC_INIT(uc);\n\n    // we can't dereference hook->type if hook is invalid\n    // so for now we need to iterate over all possible types to remove the hook\n    // which is less efficient\n    // an optimization would be to align the hook pointer\n    // and store the type mask in the hook pointer.\n    for (i = 0; i < UC_HOOK_MAX; i++) {\n        if (list_exists(&uc->hook[i], (void *)hook)) {\n            hook->to_delete = true;\n            uc->hooks_count[i]--;\n            hook_append(&uc->hooks_to_del, hook);\n        }\n    }\n\n    return UC_ERR_OK;\n}\n\n// TCG helper\n// 2 arguments are enough for most opcodes. Load/Store needs 3 arguments but we\n// have memory hooks already. We may exceed the maximum arguments of a tcg\n// helper but that's easy to extend.\nvoid helper_uc_traceopcode(struct hook *hook, uint64_t arg1, uint64_t arg2,\n                           uint32_t size, void *handle, uint64_t address);\nvoid helper_uc_traceopcode(struct hook *hook, uint64_t arg1, uint64_t arg2,\n                           uint32_t size, void *handle, uint64_t address)\n{\n    struct uc_struct *uc = handle;\n\n    if (unlikely(uc->stop_request)) {\n        return;\n    }\n\n    if (unlikely(hook->to_delete)) {\n        return;\n    }\n\n    // We did all checks in translation time.\n    //\n    // This could optimize the case that we have multiple hooks with different\n    // opcodes and have one callback per opcode. Note that the assumption don't\n    // hold in most cases for uc_tracecode.\n    //\n    // TODO: Shall we have a flag to allow users to control whether updating PC?\n    ((uc_hook_tcg_op_2)hook->callback)(uc, address, arg1, arg2, size,\n                                       hook->user_data);\n\n    if (unlikely(uc->stop_request)) {\n        return;\n    }\n}\n\nvoid helper_uc_tracecode(int32_t size, uc_hook_idx index, void *handle,\n                         int64_t address);\nvoid helper_uc_tracecode(int32_t size, uc_hook_idx index, void *handle,\n                         int64_t address)\n{\n    struct uc_struct *uc = handle;\n    struct list_item *cur;\n    struct hook *hook;\n    int hook_flags =\n        index &\n        UC_HOOK_FLAG_MASK; // The index here may contain additional flags. See\n                           // the comments of uc_hook_idx for details.\n\n    index = index & UC_HOOK_IDX_MASK;\n\n    // This has been done in tcg code.\n    // sync PC in CPUArchState with address\n    // if (uc->set_pc) {\n    //     uc->set_pc(uc, address);\n    // }\n\n    // the last callback may already asked to stop emulation\n    if (uc->stop_request && !(hook_flags & UC_HOOK_FLAG_NO_STOP)) {\n        return;\n    }\n\n    for (cur = uc->hook[index].head;\n         cur != NULL && (hook = (struct hook *)cur->data); cur = cur->next) {\n        if (hook->to_delete) {\n            continue;\n        }\n\n        // on invalid block/instruction, call instruction counter (if enable),\n        // then quit\n        if (size == 0) {\n            if (index == UC_HOOK_CODE_IDX && uc->count_hook) {\n                // this is the instruction counter (first hook in the list)\n                ((uc_cb_hookcode_t)hook->callback)(uc, address, size,\n                                                   hook->user_data);\n            }\n\n            return;\n        }\n\n        if (HOOK_BOUND_CHECK(hook, (uint64_t)address)) {\n            ((uc_cb_hookcode_t)hook->callback)(uc, address, size,\n                                               hook->user_data);\n        }\n\n        // the last callback may already asked to stop emulation\n        // Unicorn:\n        //   In an ARM IT block, we behave like the emulation continues\n        //   normally. No check_exit_request is generated and the hooks are\n        //   triggered normally. In other words, the whole IT block is treated\n        //   as a single instruction.\n        if (uc->stop_request && !(hook_flags & UC_HOOK_FLAG_NO_STOP)) {\n            break;\n        }\n    }\n}\n\nUNICORN_EXPORT\nuc_err uc_mem_regions(uc_engine *uc, uc_mem_region **regions, uint32_t *count)\n{\n    uint32_t i;\n    uc_mem_region *r = NULL;\n\n    UC_INIT(uc);\n\n    *count = uc->mapped_block_count;\n\n    if (*count) {\n        r = g_malloc0(*count * sizeof(uc_mem_region));\n        if (r == NULL) {\n            // out of memory\n            return UC_ERR_NOMEM;\n        }\n    }\n\n    for (i = 0; i < *count; i++) {\n        r[i].begin = uc->mapped_blocks[i]->addr;\n        r[i].end = uc->mapped_blocks[i]->end - 1;\n        r[i].perms = uc->mapped_blocks[i]->perms;\n    }\n\n    *regions = r;\n\n    return UC_ERR_OK;\n}\n\nUNICORN_EXPORT\nuc_err uc_query(uc_engine *uc, uc_query_type type, size_t *result)\n{\n    UC_INIT(uc);\n\n    switch (type) {\n    default:\n        return UC_ERR_ARG;\n\n    case UC_QUERY_PAGE_SIZE:\n        *result = uc->target_page_size;\n        break;\n\n    case UC_QUERY_ARCH:\n        *result = uc->arch;\n        break;\n\n    case UC_QUERY_MODE:\n#ifdef UNICORN_HAS_ARM\n        if (uc->arch == UC_ARCH_ARM) {\n            return uc->query(uc, type, result);\n        }\n#endif\n        *result = uc->mode;\n        break;\n\n    case UC_QUERY_TIMEOUT:\n        *result = uc->timed_out;\n        break;\n    }\n\n    return UC_ERR_OK;\n}\n\nUNICORN_EXPORT\nuc_err uc_context_alloc(uc_engine *uc, uc_context **context)\n{\n    struct uc_context **_context = context;\n    size_t size = uc_context_size(uc);\n\n    UC_INIT(uc);\n\n    *_context = g_malloc(size);\n    if (*_context) {\n        (*_context)->context_size = uc->cpu_context_size;\n        (*_context)->arch = uc->arch;\n        (*_context)->mode = uc->mode;\n        return UC_ERR_OK;\n    } else {\n        return UC_ERR_NOMEM;\n    }\n}\n\nUNICORN_EXPORT\nuc_err uc_free(void *mem)\n{\n    g_free(mem);\n    return UC_ERR_OK;\n}\n\nUNICORN_EXPORT\nsize_t uc_context_size(uc_engine *uc)\n{\n    UC_INIT(uc);\n    // return the total size of struct uc_context\n    return sizeof(uc_context) + uc->cpu_context_size;\n}\n\nUNICORN_EXPORT\nuc_err uc_context_save(uc_engine *uc, uc_context *context)\n{\n    UC_INIT(uc);\n\n    memcpy(context->data, uc->cpu->env_ptr, context->context_size);\n\n    return UC_ERR_OK;\n}\n\nUNICORN_EXPORT\nuc_err uc_context_reg_write(uc_context *ctx, int regid, const void *value)\n{\n    return uc_context_reg_write_batch(ctx, &regid, (void *const *)&value, 1);\n}\n\nUNICORN_EXPORT\nuc_err uc_context_reg_read(uc_context *ctx, int regid, void *value)\n{\n    return uc_context_reg_read_batch(ctx, &regid, &value, 1);\n}\n\n// Keep in mind that we don't a uc_engine when r/w the registers of a context.\nstatic void find_context_reg_rw_function(uc_arch arch, uc_mode mode,\n                                         context_reg_rw_t *rw)\n{\n    // We believe that the arch/mode pair is correct.\n    switch (arch) {\n    default:\n        rw->context_reg_read = NULL;\n        rw->context_reg_write = NULL;\n        break;\n#ifdef UNICORN_HAS_M68K\n    case UC_ARCH_M68K:\n        rw->context_reg_read = m68k_context_reg_read;\n        rw->context_reg_write = m68k_context_reg_write;\n        break;\n#endif\n#ifdef UNICORN_HAS_X86\n    case UC_ARCH_X86:\n        rw->context_reg_read = x86_context_reg_read;\n        rw->context_reg_write = x86_context_reg_write;\n        break;\n#endif\n#ifdef UNICORN_HAS_ARM\n    case UC_ARCH_ARM:\n        rw->context_reg_read = arm_context_reg_read;\n        rw->context_reg_write = arm_context_reg_write;\n        break;\n#endif\n#ifdef UNICORN_HAS_ARM64\n    case UC_ARCH_ARM64:\n        rw->context_reg_read = arm64_context_reg_read;\n        rw->context_reg_write = arm64_context_reg_write;\n        break;\n#endif\n\n#if defined(UNICORN_HAS_MIPS) || defined(UNICORN_HAS_MIPSEL) ||                \\\n    defined(UNICORN_HAS_MIPS64) || defined(UNICORN_HAS_MIPS64EL)\n    case UC_ARCH_MIPS:\n        if (mode & UC_MODE_BIG_ENDIAN) {\n#ifdef UNICORN_HAS_MIPS\n            if (mode & UC_MODE_MIPS32) {\n                rw->context_reg_read = mips_context_reg_read;\n                rw->context_reg_write = mips_context_reg_write;\n            }\n#endif\n#ifdef UNICORN_HAS_MIPS64\n            if (mode & UC_MODE_MIPS64) {\n                rw->context_reg_read = mips64_context_reg_read;\n                rw->context_reg_write = mips64_context_reg_write;\n            }\n#endif\n        } else { // little endian\n#ifdef UNICORN_HAS_MIPSEL\n            if (mode & UC_MODE_MIPS32) {\n                rw->context_reg_read = mipsel_context_reg_read;\n                rw->context_reg_write = mipsel_context_reg_write;\n            }\n#endif\n#ifdef UNICORN_HAS_MIPS64EL\n            if (mode & UC_MODE_MIPS64) {\n                rw->context_reg_read = mips64el_context_reg_read;\n                rw->context_reg_write = mips64el_context_reg_write;\n            }\n#endif\n        }\n        break;\n#endif\n\n#ifdef UNICORN_HAS_SPARC\n    case UC_ARCH_SPARC:\n        if (mode & UC_MODE_SPARC64) {\n            rw->context_reg_read = sparc64_context_reg_read;\n            rw->context_reg_write = sparc64_context_reg_write;\n        } else {\n            rw->context_reg_read = sparc_context_reg_read;\n            rw->context_reg_write = sparc_context_reg_write;\n        }\n        break;\n#endif\n#ifdef UNICORN_HAS_PPC\n    case UC_ARCH_PPC:\n        if (mode & UC_MODE_PPC64) {\n            rw->context_reg_read = ppc64_context_reg_read;\n            rw->context_reg_write = ppc64_context_reg_write;\n        } else {\n            rw->context_reg_read = ppc_context_reg_read;\n            rw->context_reg_write = ppc_context_reg_write;\n        }\n        break;\n#endif\n#ifdef UNICORN_HAS_RISCV\n    case UC_ARCH_RISCV:\n        if (mode & UC_MODE_RISCV32) {\n            rw->context_reg_read = riscv32_context_reg_read;\n            rw->context_reg_write = riscv32_context_reg_write;\n        } else if (mode & UC_MODE_RISCV64) {\n            rw->context_reg_read = riscv64_context_reg_read;\n            rw->context_reg_write = riscv64_context_reg_write;\n        }\n        break;\n#endif\n#ifdef UNICORN_HAS_S390X\n    case UC_ARCH_S390X:\n        rw->context_reg_read = s390_context_reg_read;\n        rw->context_reg_write = s390_context_reg_write;\n        break;\n#endif\n    }\n\n    return;\n}\n\nUNICORN_EXPORT\nuc_err uc_context_reg_write_batch(uc_context *ctx, int *ids, void *const *vals,\n                                  int count)\n{\n    int ret = UC_ERR_OK;\n    context_reg_rw_t rw;\n\n    find_context_reg_rw_function(ctx->arch, ctx->mode, &rw);\n    if (rw.context_reg_write) {\n        ret = rw.context_reg_write(ctx, (unsigned int *)ids, vals, count);\n    } else {\n        return UC_ERR_HANDLE;\n    }\n\n    return ret;\n}\n\nUNICORN_EXPORT\nuc_err uc_context_reg_read_batch(uc_context *ctx, int *ids, void **vals,\n                                 int count)\n{\n    int ret = UC_ERR_OK;\n    context_reg_rw_t rw;\n\n    find_context_reg_rw_function(ctx->arch, ctx->mode, &rw);\n    if (rw.context_reg_read) {\n        ret = rw.context_reg_read(ctx, (unsigned int *)ids, vals, count);\n    } else {\n        return UC_ERR_HANDLE;\n    }\n\n    return ret;\n}\n\nUNICORN_EXPORT\nuc_err uc_context_restore(uc_engine *uc, uc_context *context)\n{\n    UC_INIT(uc);\n\n    memcpy(uc->cpu->env_ptr, context->data, context->context_size);\n\n    return UC_ERR_OK;\n}\n\nUNICORN_EXPORT\nuc_err uc_context_free(uc_context *context)\n{\n\n    return uc_free(context);\n}\n\ntypedef struct _uc_ctl_exit_request {\n    uint64_t *array;\n    size_t len;\n} uc_ctl_exit_request;\n\nstatic inline gboolean uc_read_exit_iter(gpointer key, gpointer val,\n                                         gpointer data)\n{\n    uc_ctl_exit_request *req = (uc_ctl_exit_request *)data;\n\n    req->array[req->len++] = *(uint64_t *)key;\n\n    return false;\n}\n\nUNICORN_EXPORT\nuc_err uc_ctl(uc_engine *uc, uc_control_type control, ...)\n{\n    int rw, type;\n    uc_err err = UC_ERR_OK;\n    va_list args;\n\n    // MSVC Would do signed shift on signed integers.\n    rw = (uint32_t)control >> 30;\n    type = (control & ((1 << 16) - 1));\n    va_start(args, control);\n\n    switch (type) {\n    case UC_CTL_UC_MODE: {\n        if (rw == UC_CTL_IO_READ) {\n            int *pmode = va_arg(args, int *);\n            *pmode = uc->mode;\n        } else {\n            err = UC_ERR_ARG;\n        }\n        break;\n    }\n\n    case UC_CTL_UC_ARCH: {\n        if (rw == UC_CTL_IO_READ) {\n            int *arch = va_arg(args, int *);\n            *arch = uc->arch;\n        } else {\n            err = UC_ERR_ARG;\n        }\n        break;\n    }\n\n    case UC_CTL_UC_TIMEOUT: {\n        if (rw == UC_CTL_IO_READ) {\n            uint64_t *arch = va_arg(args, uint64_t *);\n            *arch = uc->timeout;\n        } else {\n            err = UC_ERR_ARG;\n        }\n        break;\n    }\n\n    case UC_CTL_UC_PAGE_SIZE: {\n        if (rw == UC_CTL_IO_READ) {\n\n            UC_INIT(uc);\n\n            uint32_t *page_size = va_arg(args, uint32_t *);\n            *page_size = uc->target_page_size;\n        } else {\n            uint32_t page_size = va_arg(args, uint32_t);\n            int bits = 0;\n\n            if (uc->init_done) {\n                err = UC_ERR_ARG;\n                break;\n            }\n\n            if (uc->arch != UC_ARCH_ARM) {\n                err = UC_ERR_ARG;\n                break;\n            }\n\n            if ((page_size & (page_size - 1))) {\n                err = UC_ERR_ARG;\n                break;\n            }\n\n            while (page_size) {\n                bits++;\n                page_size >>= 1;\n            }\n\n            uc->target_bits = bits;\n\n            err = UC_ERR_OK;\n        }\n        break;\n    }\n\n    case UC_CTL_UC_USE_EXITS: {\n        if (rw == UC_CTL_IO_WRITE) {\n            int use_exits = va_arg(args, int);\n            uc->use_exits = use_exits;\n        } else {\n            err = UC_ERR_ARG;\n        }\n        break;\n    }\n\n    case UC_CTL_UC_EXITS_CNT: {\n\n        UC_INIT(uc);\n\n        if (!uc->use_exits) {\n            err = UC_ERR_ARG;\n        } else if (rw == UC_CTL_IO_READ) {\n            size_t *exits_cnt = va_arg(args, size_t *);\n            *exits_cnt = g_tree_nnodes(uc->ctl_exits);\n        } else {\n            err = UC_ERR_ARG;\n        }\n        break;\n    }\n\n    case UC_CTL_UC_EXITS: {\n\n        UC_INIT(uc);\n\n        if (!uc->use_exits) {\n            err = UC_ERR_ARG;\n        } else if (rw == UC_CTL_IO_READ) {\n            uint64_t *exits = va_arg(args, uint64_t *);\n            size_t cnt = va_arg(args, size_t);\n            if (cnt < g_tree_nnodes(uc->ctl_exits)) {\n                err = UC_ERR_ARG;\n            } else {\n                uc_ctl_exit_request req;\n                req.array = exits;\n                req.len = 0;\n\n                g_tree_foreach(uc->ctl_exits, uc_read_exit_iter, (void *)&req);\n            }\n        } else if (rw == UC_CTL_IO_WRITE) {\n            uint64_t *exits = va_arg(args, uint64_t *);\n            size_t cnt = va_arg(args, size_t);\n\n            g_tree_remove_all(uc->ctl_exits);\n\n            for (size_t i = 0; i < cnt; i++) {\n                uc_add_exit(uc, exits[i]);\n            }\n        } else {\n            err = UC_ERR_ARG;\n        }\n        break;\n    }\n\n    case UC_CTL_CPU_MODEL: {\n        if (rw == UC_CTL_IO_READ) {\n\n            UC_INIT(uc);\n\n            int *model = va_arg(args, int *);\n            *model = uc->cpu_model;\n        } else {\n            int model = va_arg(args, int);\n\n            if (uc->init_done) {\n                err = UC_ERR_ARG;\n                break;\n            }\n\n            if (uc->arch == UC_ARCH_ARM) {\n                if (uc->mode & UC_MODE_BIG_ENDIAN) {\n                    // These cpu models don't support big endian code access.\n                    if (model <= UC_CPU_ARM_CORTEX_A15 &&\n                        model >= UC_CPU_ARM_CORTEX_A7) {\n                        err = UC_ERR_ARG;\n                        break;\n                    }\n                }\n            }\n\n            uc->cpu_model = model;\n\n            err = UC_ERR_OK;\n        }\n        break;\n    }\n\n    case UC_CTL_TB_REQUEST_CACHE: {\n\n        UC_INIT(uc);\n\n        if (rw == UC_CTL_IO_READ_WRITE) {\n            uint64_t addr = va_arg(args, uint64_t);\n            uc_tb *tb = va_arg(args, uc_tb *);\n            err = uc->uc_gen_tb(uc, addr, tb);\n        } else {\n            err = UC_ERR_ARG;\n        }\n        break;\n    }\n\n    case UC_CTL_TB_REMOVE_CACHE: {\n\n        UC_INIT(uc);\n\n        if (rw == UC_CTL_IO_WRITE) {\n            uint64_t addr = va_arg(args, uint64_t);\n            uint64_t end = va_arg(args, uint64_t);\n            if (end <= addr) {\n                err = UC_ERR_ARG;\n            } else {\n                uc->uc_invalidate_tb(uc, addr, end - addr);\n            }\n        } else {\n            err = UC_ERR_ARG;\n        }\n        break;\n    }\n\n    default:\n        err = UC_ERR_ARG;\n        break;\n    }\n\n    va_end(args);\n\n    return err;\n}\n\n#ifdef UNICORN_TRACER\nuc_tracer *get_tracer()\n{\n    static uc_tracer tracer;\n    return &tracer;\n}\n\nvoid trace_start(uc_tracer *tracer, trace_loc loc)\n{\n    tracer->starts[loc] = get_clock();\n}\n\nvoid trace_end(uc_tracer *tracer, trace_loc loc, const char *fmt, ...)\n{\n    va_list args;\n    int64_t end = get_clock();\n\n    va_start(args, fmt);\n\n    vfprintf(stderr, fmt, args);\n\n    va_end(args);\n\n    fprintf(stderr, \"%.6fus\\n\",\n            (double)(end - tracer->starts[loc]) / (double)(1000));\n}\n#endif"], "fixing_code": ["/* Unicorn Emulator Engine */\n/* By Nguyen Anh Quynh <aquynh@gmail.com>, 2015 */\n/* Modified for Unicorn Engine by Chen Huitao<chenhuitao@hfmrit.com>, 2020 */\n\n#if defined(UNICORN_HAS_OSXKERNEL)\n#include <libkern/libkern.h>\n#else\n#include <stddef.h>\n#include <stdio.h>\n#include <stdlib.h>\n#endif\n\n#include <time.h> // nanosleep\n#include <string.h>\n\n#include \"uc_priv.h\"\n\n// target specific headers\n#include \"qemu/target/m68k/unicorn.h\"\n#include \"qemu/target/i386/unicorn.h\"\n#include \"qemu/target/arm/unicorn.h\"\n#include \"qemu/target/mips/unicorn.h\"\n#include \"qemu/target/sparc/unicorn.h\"\n#include \"qemu/target/ppc/unicorn.h\"\n#include \"qemu/target/riscv/unicorn.h\"\n#include \"qemu/target/s390x/unicorn.h\"\n\n#include \"qemu/include/qemu/queue.h\"\n#include \"qemu-common.h\"\n\nstatic void clear_deleted_hooks(uc_engine *uc);\n\nstatic void *hook_insert(struct list *l, struct hook *h)\n{\n    void *item = list_insert(l, (void *)h);\n    if (item) {\n        h->refs++;\n    }\n    return item;\n}\n\nstatic void *hook_append(struct list *l, struct hook *h)\n{\n    void *item = list_append(l, (void *)h);\n    if (item) {\n        h->refs++;\n    }\n    return item;\n}\n\nstatic void hook_delete(void *data)\n{\n    struct hook *h = (struct hook *)data;\n\n    h->refs--;\n\n    if (h->refs == 0) {\n        free(h);\n    }\n}\n\nUNICORN_EXPORT\nunsigned int uc_version(unsigned int *major, unsigned int *minor)\n{\n    if (major != NULL && minor != NULL) {\n        *major = UC_API_MAJOR;\n        *minor = UC_API_MINOR;\n    }\n\n    return (UC_API_MAJOR << 24) + (UC_API_MINOR << 16) + (UC_API_PATCH << 8) +\n           UC_API_EXTRA;\n}\n\nUNICORN_EXPORT\nuc_err uc_errno(uc_engine *uc)\n{\n    return uc->errnum;\n}\n\nUNICORN_EXPORT\nconst char *uc_strerror(uc_err code)\n{\n    switch (code) {\n    default:\n        return \"Unknown error code\";\n    case UC_ERR_OK:\n        return \"OK (UC_ERR_OK)\";\n    case UC_ERR_NOMEM:\n        return \"No memory available or memory not present (UC_ERR_NOMEM)\";\n    case UC_ERR_ARCH:\n        return \"Invalid/unsupported architecture (UC_ERR_ARCH)\";\n    case UC_ERR_HANDLE:\n        return \"Invalid handle (UC_ERR_HANDLE)\";\n    case UC_ERR_MODE:\n        return \"Invalid mode (UC_ERR_MODE)\";\n    case UC_ERR_VERSION:\n        return \"Different API version between core & binding (UC_ERR_VERSION)\";\n    case UC_ERR_READ_UNMAPPED:\n        return \"Invalid memory read (UC_ERR_READ_UNMAPPED)\";\n    case UC_ERR_WRITE_UNMAPPED:\n        return \"Invalid memory write (UC_ERR_WRITE_UNMAPPED)\";\n    case UC_ERR_FETCH_UNMAPPED:\n        return \"Invalid memory fetch (UC_ERR_FETCH_UNMAPPED)\";\n    case UC_ERR_HOOK:\n        return \"Invalid hook type (UC_ERR_HOOK)\";\n    case UC_ERR_INSN_INVALID:\n        return \"Invalid instruction (UC_ERR_INSN_INVALID)\";\n    case UC_ERR_MAP:\n        return \"Invalid memory mapping (UC_ERR_MAP)\";\n    case UC_ERR_WRITE_PROT:\n        return \"Write to write-protected memory (UC_ERR_WRITE_PROT)\";\n    case UC_ERR_READ_PROT:\n        return \"Read from non-readable memory (UC_ERR_READ_PROT)\";\n    case UC_ERR_FETCH_PROT:\n        return \"Fetch from non-executable memory (UC_ERR_FETCH_PROT)\";\n    case UC_ERR_ARG:\n        return \"Invalid argument (UC_ERR_ARG)\";\n    case UC_ERR_READ_UNALIGNED:\n        return \"Read from unaligned memory (UC_ERR_READ_UNALIGNED)\";\n    case UC_ERR_WRITE_UNALIGNED:\n        return \"Write to unaligned memory (UC_ERR_WRITE_UNALIGNED)\";\n    case UC_ERR_FETCH_UNALIGNED:\n        return \"Fetch from unaligned memory (UC_ERR_FETCH_UNALIGNED)\";\n    case UC_ERR_RESOURCE:\n        return \"Insufficient resource (UC_ERR_RESOURCE)\";\n    case UC_ERR_EXCEPTION:\n        return \"Unhandled CPU exception (UC_ERR_EXCEPTION)\";\n    }\n}\n\nUNICORN_EXPORT\nbool uc_arch_supported(uc_arch arch)\n{\n    switch (arch) {\n#ifdef UNICORN_HAS_ARM\n    case UC_ARCH_ARM:\n        return true;\n#endif\n#ifdef UNICORN_HAS_ARM64\n    case UC_ARCH_ARM64:\n        return true;\n#endif\n#ifdef UNICORN_HAS_M68K\n    case UC_ARCH_M68K:\n        return true;\n#endif\n#ifdef UNICORN_HAS_MIPS\n    case UC_ARCH_MIPS:\n        return true;\n#endif\n#ifdef UNICORN_HAS_PPC\n    case UC_ARCH_PPC:\n        return true;\n#endif\n#ifdef UNICORN_HAS_SPARC\n    case UC_ARCH_SPARC:\n        return true;\n#endif\n#ifdef UNICORN_HAS_X86\n    case UC_ARCH_X86:\n        return true;\n#endif\n#ifdef UNICORN_HAS_RISCV\n    case UC_ARCH_RISCV:\n        return true;\n#endif\n#ifdef UNICORN_HAS_S390X\n    case UC_ARCH_S390X:\n        return true;\n#endif\n    /* Invalid or disabled arch */\n    default:\n        return false;\n    }\n}\n\n#define UC_INIT(uc)                                                            \\\n    if (unlikely(!(uc)->init_done)) {                                          \\\n        int __init_ret = uc_init(uc);                                          \\\n        if (unlikely(__init_ret != UC_ERR_OK)) {                               \\\n            return __init_ret;                                                 \\\n        }                                                                      \\\n    }\n\nstatic gint uc_exits_cmp(gconstpointer a, gconstpointer b, gpointer user_data)\n{\n    uint64_t lhs = *((uint64_t *)a);\n    uint64_t rhs = *((uint64_t *)b);\n\n    if (lhs < rhs) {\n        return -1;\n    } else if (lhs == rhs) {\n        return 0;\n    } else {\n        return 1;\n    }\n}\n\nstatic uc_err uc_init(uc_engine *uc)\n{\n\n    if (uc->init_done) {\n        return UC_ERR_HANDLE;\n    }\n\n    uc->hooks_to_del.delete_fn = hook_delete;\n\n    for (int i = 0; i < UC_HOOK_MAX; i++) {\n        uc->hook[i].delete_fn = hook_delete;\n    }\n\n    uc->ctl_exits = g_tree_new_full(uc_exits_cmp, NULL, g_free, NULL);\n\n    if (machine_initialize(uc)) {\n        return UC_ERR_RESOURCE;\n    }\n\n    // init fpu softfloat\n    uc->softfloat_initialize();\n\n    if (uc->reg_reset) {\n        uc->reg_reset(uc);\n    }\n\n    uc->init_done = true;\n\n    return UC_ERR_OK;\n}\n\nUNICORN_EXPORT\nuc_err uc_open(uc_arch arch, uc_mode mode, uc_engine **result)\n{\n    struct uc_struct *uc;\n\n    if (arch < UC_ARCH_MAX) {\n        uc = calloc(1, sizeof(*uc));\n        if (!uc) {\n            // memory insufficient\n            return UC_ERR_NOMEM;\n        }\n\n        /* qemu/exec.c: phys_map_node_reserve() */\n        uc->alloc_hint = 16;\n        uc->errnum = UC_ERR_OK;\n        uc->arch = arch;\n        uc->mode = mode;\n\n        // uc->ram_list = { .blocks = QLIST_HEAD_INITIALIZER(ram_list.blocks) };\n        QLIST_INIT(&uc->ram_list.blocks);\n\n        QTAILQ_INIT(&uc->memory_listeners);\n\n        QTAILQ_INIT(&uc->address_spaces);\n\n        switch (arch) {\n        default:\n            break;\n#ifdef UNICORN_HAS_M68K\n        case UC_ARCH_M68K:\n            if ((mode & ~UC_MODE_M68K_MASK) || !(mode & UC_MODE_BIG_ENDIAN)) {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            uc->init_arch = m68k_uc_init;\n            break;\n#endif\n#ifdef UNICORN_HAS_X86\n        case UC_ARCH_X86:\n            if ((mode & ~UC_MODE_X86_MASK) || (mode & UC_MODE_BIG_ENDIAN) ||\n                !(mode & (UC_MODE_16 | UC_MODE_32 | UC_MODE_64))) {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            uc->init_arch = x86_uc_init;\n            break;\n#endif\n#ifdef UNICORN_HAS_ARM\n        case UC_ARCH_ARM:\n            if ((mode & ~UC_MODE_ARM_MASK)) {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            uc->init_arch = arm_uc_init;\n\n            if (mode & UC_MODE_THUMB) {\n                uc->thumb = 1;\n            }\n            break;\n#endif\n#ifdef UNICORN_HAS_ARM64\n        case UC_ARCH_ARM64:\n            if (mode & ~UC_MODE_ARM_MASK) {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            uc->init_arch = arm64_uc_init;\n            break;\n#endif\n\n#if defined(UNICORN_HAS_MIPS) || defined(UNICORN_HAS_MIPSEL) ||                \\\n    defined(UNICORN_HAS_MIPS64) || defined(UNICORN_HAS_MIPS64EL)\n        case UC_ARCH_MIPS:\n            if ((mode & ~UC_MODE_MIPS_MASK) ||\n                !(mode & (UC_MODE_MIPS32 | UC_MODE_MIPS64))) {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            if (mode & UC_MODE_BIG_ENDIAN) {\n#ifdef UNICORN_HAS_MIPS\n                if (mode & UC_MODE_MIPS32) {\n                    uc->init_arch = mips_uc_init;\n                }\n#endif\n#ifdef UNICORN_HAS_MIPS64\n                if (mode & UC_MODE_MIPS64) {\n                    uc->init_arch = mips64_uc_init;\n                }\n#endif\n            } else { // little endian\n#ifdef UNICORN_HAS_MIPSEL\n                if (mode & UC_MODE_MIPS32) {\n                    uc->init_arch = mipsel_uc_init;\n                }\n#endif\n#ifdef UNICORN_HAS_MIPS64EL\n                if (mode & UC_MODE_MIPS64) {\n                    uc->init_arch = mips64el_uc_init;\n                }\n#endif\n            }\n            break;\n#endif\n\n#ifdef UNICORN_HAS_SPARC\n        case UC_ARCH_SPARC:\n            if ((mode & ~UC_MODE_SPARC_MASK) || !(mode & UC_MODE_BIG_ENDIAN) ||\n                !(mode & (UC_MODE_SPARC32 | UC_MODE_SPARC64))) {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            if (mode & UC_MODE_SPARC64) {\n                uc->init_arch = sparc64_uc_init;\n            } else {\n                uc->init_arch = sparc_uc_init;\n            }\n            break;\n#endif\n#ifdef UNICORN_HAS_PPC\n        case UC_ARCH_PPC:\n            if ((mode & ~UC_MODE_PPC_MASK) || !(mode & UC_MODE_BIG_ENDIAN) ||\n                !(mode & (UC_MODE_PPC32 | UC_MODE_PPC64))) {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            if (mode & UC_MODE_PPC64) {\n                uc->init_arch = ppc64_uc_init;\n            } else {\n                uc->init_arch = ppc_uc_init;\n            }\n            break;\n#endif\n#ifdef UNICORN_HAS_RISCV\n        case UC_ARCH_RISCV:\n            if ((mode & ~UC_MODE_RISCV_MASK) ||\n                !(mode & (UC_MODE_RISCV32 | UC_MODE_RISCV64))) {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            if (mode & UC_MODE_RISCV32) {\n                uc->init_arch = riscv32_uc_init;\n            } else if (mode & UC_MODE_RISCV64) {\n                uc->init_arch = riscv64_uc_init;\n            } else {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            break;\n#endif\n#ifdef UNICORN_HAS_S390X\n        case UC_ARCH_S390X:\n            if ((mode & ~UC_MODE_S390X_MASK) || !(mode & UC_MODE_BIG_ENDIAN)) {\n                free(uc);\n                return UC_ERR_MODE;\n            }\n            uc->init_arch = s390_uc_init;\n            break;\n#endif\n        }\n\n        if (uc->init_arch == NULL) {\n            free(uc);\n            return UC_ERR_ARCH;\n        }\n\n        uc->init_done = false;\n        uc->cpu_model = INT_MAX; // INT_MAX means the default cpu model.\n\n        *result = uc;\n\n        return UC_ERR_OK;\n    } else {\n        return UC_ERR_ARCH;\n    }\n}\n\nUNICORN_EXPORT\nuc_err uc_close(uc_engine *uc)\n{\n    int i;\n    MemoryRegion *mr;\n\n    if (!uc->init_done) {\n        free(uc);\n        return UC_ERR_OK;\n    }\n\n    // Cleanup internally.\n    if (uc->release) {\n        uc->release(uc->tcg_ctx);\n    }\n    g_free(uc->tcg_ctx);\n\n    // Cleanup CPU.\n    g_free(uc->cpu->cpu_ases);\n    g_free(uc->cpu->thread);\n\n    /* cpu */\n    free(uc->cpu);\n\n    /* flatviews */\n    g_hash_table_destroy(uc->flat_views);\n\n    // During flatviews destruction, we may still access memory regions.\n    // So we free them afterwards.\n    /* memory */\n    mr = &uc->io_mem_unassigned;\n    mr->destructor(mr);\n    mr = uc->system_io;\n    mr->destructor(mr);\n    mr = uc->system_memory;\n    mr->destructor(mr);\n    g_free(uc->system_memory);\n    g_free(uc->system_io);\n\n    // Thread relateds.\n    if (uc->qemu_thread_data) {\n        g_free(uc->qemu_thread_data);\n    }\n\n    /* free */\n    g_free(uc->init_target_page);\n\n    // Other auxilaries.\n    g_free(uc->l1_map);\n\n    if (uc->bounce.buffer) {\n        free(uc->bounce.buffer);\n    }\n\n    // free hooks and hook lists\n    clear_deleted_hooks(uc);\n\n    for (i = 0; i < UC_HOOK_MAX; i++) {\n        list_clear(&uc->hook[i]);\n    }\n\n    free(uc->mapped_blocks);\n\n    g_tree_destroy(uc->ctl_exits);\n\n    // finally, free uc itself.\n    memset(uc, 0, sizeof(*uc));\n    free(uc);\n\n    return UC_ERR_OK;\n}\n\nUNICORN_EXPORT\nuc_err uc_reg_read_batch(uc_engine *uc, int *ids, void **vals, int count)\n{\n    int ret = UC_ERR_OK;\n\n    UC_INIT(uc);\n\n    if (uc->reg_read) {\n        ret = uc->reg_read(uc, (unsigned int *)ids, vals, count);\n    } else {\n        return UC_ERR_HANDLE;\n    }\n\n    return ret;\n}\n\nUNICORN_EXPORT\nuc_err uc_reg_write_batch(uc_engine *uc, int *ids, void *const *vals, int count)\n{\n    int ret = UC_ERR_OK;\n\n    UC_INIT(uc);\n\n    if (uc->reg_write) {\n        ret = uc->reg_write(uc, (unsigned int *)ids, vals, count);\n    } else {\n        return UC_ERR_HANDLE;\n    }\n\n    return ret;\n}\n\nUNICORN_EXPORT\nuc_err uc_reg_read(uc_engine *uc, int regid, void *value)\n{\n    UC_INIT(uc);\n    return uc_reg_read_batch(uc, &regid, &value, 1);\n}\n\nUNICORN_EXPORT\nuc_err uc_reg_write(uc_engine *uc, int regid, const void *value)\n{\n    UC_INIT(uc);\n    return uc_reg_write_batch(uc, &regid, (void *const *)&value, 1);\n}\n\n// check if a memory area is mapped\n// this is complicated because an area can overlap adjacent blocks\nstatic bool check_mem_area(uc_engine *uc, uint64_t address, size_t size)\n{\n    size_t count = 0, len;\n\n    while (count < size) {\n        MemoryRegion *mr = memory_mapping(uc, address);\n        if (mr) {\n            len = (size_t)MIN(size - count, mr->end - address);\n            count += len;\n            address += len;\n        } else { // this address is not mapped in yet\n            break;\n        }\n    }\n\n    return (count == size);\n}\n\nUNICORN_EXPORT\nuc_err uc_mem_read(uc_engine *uc, uint64_t address, void *_bytes, size_t size)\n{\n    size_t count = 0, len;\n    uint8_t *bytes = _bytes;\n\n    UC_INIT(uc);\n\n    // qemu cpu_physical_memory_rw() size is an int\n    if (size > INT_MAX)\n        return UC_ERR_ARG;\n\n    if (uc->mem_redirect) {\n        address = uc->mem_redirect(address);\n    }\n\n    if (!check_mem_area(uc, address, size)) {\n        return UC_ERR_READ_UNMAPPED;\n    }\n\n    // memory area can overlap adjacent memory blocks\n    while (count < size) {\n        MemoryRegion *mr = memory_mapping(uc, address);\n        if (mr) {\n            len = (size_t)MIN(size - count, mr->end - address);\n            if (uc->read_mem(&uc->address_space_memory, address, bytes, len) ==\n                false) {\n                break;\n            }\n            count += len;\n            address += len;\n            bytes += len;\n        } else { // this address is not mapped in yet\n            break;\n        }\n    }\n\n    if (count == size) {\n        return UC_ERR_OK;\n    } else {\n        return UC_ERR_READ_UNMAPPED;\n    }\n}\n\nUNICORN_EXPORT\nuc_err uc_mem_write(uc_engine *uc, uint64_t address, const void *_bytes,\n                    size_t size)\n{\n    size_t count = 0, len;\n    const uint8_t *bytes = _bytes;\n\n    UC_INIT(uc);\n\n    // qemu cpu_physical_memory_rw() size is an int\n    if (size > INT_MAX)\n        return UC_ERR_ARG;\n\n    if (uc->mem_redirect) {\n        address = uc->mem_redirect(address);\n    }\n\n    if (!check_mem_area(uc, address, size)) {\n        return UC_ERR_WRITE_UNMAPPED;\n    }\n\n    // memory area can overlap adjacent memory blocks\n    while (count < size) {\n        MemoryRegion *mr = memory_mapping(uc, address);\n        if (mr) {\n            uint32_t operms = mr->perms;\n            if (!(operms & UC_PROT_WRITE)) { // write protected\n                // but this is not the program accessing memory, so temporarily\n                // mark writable\n                uc->readonly_mem(mr, false);\n            }\n\n            len = (size_t)MIN(size - count, mr->end - address);\n            if (uc->write_mem(&uc->address_space_memory, address, bytes, len) ==\n                false) {\n                break;\n            }\n\n            if (!(operms & UC_PROT_WRITE)) { // write protected\n                // now write protect it again\n                uc->readonly_mem(mr, true);\n            }\n\n            count += len;\n            address += len;\n            bytes += len;\n        } else { // this address is not mapped in yet\n            break;\n        }\n    }\n\n    if (count == size) {\n        return UC_ERR_OK;\n    } else {\n        return UC_ERR_WRITE_UNMAPPED;\n    }\n}\n\n#define TIMEOUT_STEP 2 // microseconds\nstatic void *_timeout_fn(void *arg)\n{\n    struct uc_struct *uc = arg;\n    int64_t current_time = get_clock();\n\n    do {\n        usleep(TIMEOUT_STEP);\n        // perhaps emulation is even done before timeout?\n        if (uc->emulation_done) {\n            break;\n        }\n    } while ((uint64_t)(get_clock() - current_time) < uc->timeout);\n\n    // timeout before emulation is done?\n    if (!uc->emulation_done) {\n        uc->timed_out = true;\n        // force emulation to stop\n        uc_emu_stop(uc);\n    }\n\n    return NULL;\n}\n\nstatic void enable_emu_timer(uc_engine *uc, uint64_t timeout)\n{\n    uc->timeout = timeout;\n    qemu_thread_create(uc, &uc->timer, \"timeout\", _timeout_fn, uc,\n                       QEMU_THREAD_JOINABLE);\n}\n\nstatic void hook_count_cb(struct uc_struct *uc, uint64_t address, uint32_t size,\n                          void *user_data)\n{\n    // count this instruction. ah ah ah.\n    uc->emu_counter++;\n    // printf(\":: emu counter = %u, at %lx\\n\", uc->emu_counter, address);\n\n    if (uc->emu_counter > uc->emu_count) {\n        // printf(\":: emu counter = %u, stop emulation\\n\", uc->emu_counter);\n        uc_emu_stop(uc);\n    }\n}\n\nstatic void clear_deleted_hooks(uc_engine *uc)\n{\n    struct list_item *cur;\n    struct hook *hook;\n    int i;\n\n    for (cur = uc->hooks_to_del.head;\n         cur != NULL && (hook = (struct hook *)cur->data); cur = cur->next) {\n        assert(hook->to_delete);\n        for (i = 0; i < UC_HOOK_MAX; i++) {\n            if (list_remove(&uc->hook[i], (void *)hook)) {\n                break;\n            }\n        }\n    }\n\n    list_clear(&uc->hooks_to_del);\n}\n\nUNICORN_EXPORT\nuc_err uc_emu_start(uc_engine *uc, uint64_t begin, uint64_t until,\n                    uint64_t timeout, size_t count)\n{\n    uc_err err;\n\n    // reset the counter\n    uc->emu_counter = 0;\n    uc->invalid_error = UC_ERR_OK;\n    uc->emulation_done = false;\n    uc->size_recur_mem = 0;\n    uc->timed_out = false;\n    uc->first_tb = true;\n\n    UC_INIT(uc);\n\n    // Advance the nested levels. We must decrease the level count by one when\n    // we return from uc_emu_start.\n    if (uc->nested_level >= UC_MAX_NESTED_LEVEL) {\n        // We can't support so many nested levels.\n        return UC_ERR_RESOURCE;\n    }\n    uc->nested_level++;\n\n    switch (uc->arch) {\n    default:\n        break;\n#ifdef UNICORN_HAS_M68K\n    case UC_ARCH_M68K:\n        uc_reg_write(uc, UC_M68K_REG_PC, &begin);\n        break;\n#endif\n#ifdef UNICORN_HAS_X86\n    case UC_ARCH_X86:\n        switch (uc->mode) {\n        default:\n            break;\n        case UC_MODE_16: {\n            uint64_t ip;\n            uint16_t cs;\n\n            uc_reg_read(uc, UC_X86_REG_CS, &cs);\n            // compensate for later adding up IP & CS\n            ip = begin - cs * 16;\n            uc_reg_write(uc, UC_X86_REG_IP, &ip);\n            break;\n        }\n        case UC_MODE_32:\n            uc_reg_write(uc, UC_X86_REG_EIP, &begin);\n            break;\n        case UC_MODE_64:\n            uc_reg_write(uc, UC_X86_REG_RIP, &begin);\n            break;\n        }\n        break;\n#endif\n#ifdef UNICORN_HAS_ARM\n    case UC_ARCH_ARM:\n        uc_reg_write(uc, UC_ARM_REG_R15, &begin);\n        break;\n#endif\n#ifdef UNICORN_HAS_ARM64\n    case UC_ARCH_ARM64:\n        uc_reg_write(uc, UC_ARM64_REG_PC, &begin);\n        break;\n#endif\n#ifdef UNICORN_HAS_MIPS\n    case UC_ARCH_MIPS:\n        // TODO: MIPS32/MIPS64/BIGENDIAN etc\n        uc_reg_write(uc, UC_MIPS_REG_PC, &begin);\n        break;\n#endif\n#ifdef UNICORN_HAS_SPARC\n    case UC_ARCH_SPARC:\n        // TODO: Sparc/Sparc64\n        uc_reg_write(uc, UC_SPARC_REG_PC, &begin);\n        break;\n#endif\n#ifdef UNICORN_HAS_PPC\n    case UC_ARCH_PPC:\n        uc_reg_write(uc, UC_PPC_REG_PC, &begin);\n        break;\n#endif\n#ifdef UNICORN_HAS_RISCV\n    case UC_ARCH_RISCV:\n        uc_reg_write(uc, UC_RISCV_REG_PC, &begin);\n        break;\n#endif\n#ifdef UNICORN_HAS_S390X\n    case UC_ARCH_S390X:\n        uc_reg_write(uc, UC_S390X_REG_PC, &begin);\n        break;\n#endif\n    }\n\n    uc->stop_request = false;\n\n    uc->emu_count = count;\n    // remove count hook if counting isn't necessary\n    if (count <= 0 && uc->count_hook != 0) {\n        uc_hook_del(uc, uc->count_hook);\n        uc->count_hook = 0;\n    }\n    // set up count hook to count instructions.\n    if (count > 0 && uc->count_hook == 0) {\n        uc_err err;\n        // callback to count instructions must be run before everything else,\n        // so instead of appending, we must insert the hook at the begin\n        // of the hook list\n        uc->hook_insert = 1;\n        err = uc_hook_add(uc, &uc->count_hook, UC_HOOK_CODE, hook_count_cb,\n                          NULL, 1, 0);\n        // restore to append mode for uc_hook_add()\n        uc->hook_insert = 0;\n        if (err != UC_ERR_OK) {\n            uc->nested_level--;\n            return err;\n        }\n    }\n\n    // If UC_CTL_UC_USE_EXITS is set, then the @until param won't have any\n    // effect. This is designed for the backward compatibility.\n    if (!uc->use_exits) {\n        uc->exits[uc->nested_level - 1] = until;\n    }\n\n    if (timeout) {\n        enable_emu_timer(uc, timeout * 1000); // microseconds -> nanoseconds\n    }\n\n    uc->vm_start(uc);\n\n    uc->nested_level--;\n\n    // emulation is done if and only if we exit the outer uc_emu_start\n    // or we may lost uc_emu_stop\n    if (uc->nested_level == 0) {\n        uc->emulation_done = true;\n    }\n\n    // remove hooks to delete\n    clear_deleted_hooks(uc);\n\n    if (timeout) {\n        // wait for the timer to finish\n        qemu_thread_join(&uc->timer);\n    }\n\n    // We may be in a nested uc_emu_start and thus clear invalid_error\n    // once we are done.\n    err = uc->invalid_error;\n    uc->invalid_error = 0;\n    return err;\n}\n\nUNICORN_EXPORT\nuc_err uc_emu_stop(uc_engine *uc)\n{\n    UC_INIT(uc);\n\n    if (uc->emulation_done) {\n        return UC_ERR_OK;\n    }\n\n    uc->stop_request = true;\n    // TODO: make this atomic somehow?\n    if (uc->cpu) {\n        // exit the current TB\n        cpu_exit(uc->cpu);\n    }\n\n    return UC_ERR_OK;\n}\n\n// return target index where a memory region at the address exists, or could be\n// inserted\n//\n// address either is inside the mapping at the returned index, or is in free\n// space before the next mapping.\n//\n// if there is overlap, between regions, ending address will be higher than the\n// starting address of the mapping at returned index\nstatic int bsearch_mapped_blocks(const uc_engine *uc, uint64_t address)\n{\n    int left, right, mid;\n    MemoryRegion *mapping;\n\n    left = 0;\n    right = uc->mapped_block_count;\n\n    while (left < right) {\n        mid = left + (right - left) / 2;\n\n        mapping = uc->mapped_blocks[mid];\n\n        if (mapping->end - 1 < address) {\n            left = mid + 1;\n        } else if (mapping->addr > address) {\n            right = mid;\n        } else {\n            return mid;\n        }\n    }\n\n    return left;\n}\n\n// find if a memory range overlaps with existing mapped regions\nstatic bool memory_overlap(struct uc_struct *uc, uint64_t begin, size_t size)\n{\n    unsigned int i;\n    uint64_t end = begin + size - 1;\n\n    i = bsearch_mapped_blocks(uc, begin);\n\n    // is this the highest region with no possible overlap?\n    if (i >= uc->mapped_block_count)\n        return false;\n\n    // end address overlaps this region?\n    if (end >= uc->mapped_blocks[i]->addr)\n        return true;\n\n    // not found\n    return false;\n}\n\n// common setup/error checking shared between uc_mem_map and uc_mem_map_ptr\nstatic uc_err mem_map(uc_engine *uc, uint64_t address, size_t size,\n                      uint32_t perms, MemoryRegion *block)\n{\n    MemoryRegion **regions;\n    int pos;\n\n    if (block == NULL) {\n        return UC_ERR_NOMEM;\n    }\n\n    if ((uc->mapped_block_count & (MEM_BLOCK_INCR - 1)) == 0) { // time to grow\n        regions = (MemoryRegion **)g_realloc(\n            uc->mapped_blocks,\n            sizeof(MemoryRegion *) * (uc->mapped_block_count + MEM_BLOCK_INCR));\n        if (regions == NULL) {\n            return UC_ERR_NOMEM;\n        }\n        uc->mapped_blocks = regions;\n    }\n\n    pos = bsearch_mapped_blocks(uc, block->addr);\n\n    // shift the array right to give space for the new pointer\n    memmove(&uc->mapped_blocks[pos + 1], &uc->mapped_blocks[pos],\n            sizeof(MemoryRegion *) * (uc->mapped_block_count - pos));\n\n    uc->mapped_blocks[pos] = block;\n    uc->mapped_block_count++;\n\n    return UC_ERR_OK;\n}\n\nstatic uc_err mem_map_check(uc_engine *uc, uint64_t address, size_t size,\n                            uint32_t perms)\n{\n    if (size == 0) {\n        // invalid memory mapping\n        return UC_ERR_ARG;\n    }\n\n    // address cannot wrapp around\n    if (address + size - 1 < address) {\n        return UC_ERR_ARG;\n    }\n\n    // address must be aligned to uc->target_page_size\n    if ((address & uc->target_page_align) != 0) {\n        return UC_ERR_ARG;\n    }\n\n    // size must be multiple of uc->target_page_size\n    if ((size & uc->target_page_align) != 0) {\n        return UC_ERR_ARG;\n    }\n\n    // check for only valid permissions\n    if ((perms & ~UC_PROT_ALL) != 0) {\n        return UC_ERR_ARG;\n    }\n\n    // this area overlaps existing mapped regions?\n    if (memory_overlap(uc, address, size)) {\n        return UC_ERR_MAP;\n    }\n\n    return UC_ERR_OK;\n}\n\nUNICORN_EXPORT\nuc_err uc_mem_map(uc_engine *uc, uint64_t address, size_t size, uint32_t perms)\n{\n    uc_err res;\n\n    UC_INIT(uc);\n\n    if (uc->mem_redirect) {\n        address = uc->mem_redirect(address);\n    }\n\n    res = mem_map_check(uc, address, size, perms);\n    if (res) {\n        return res;\n    }\n\n    return mem_map(uc, address, size, perms,\n                   uc->memory_map(uc, address, size, perms));\n}\n\nUNICORN_EXPORT\nuc_err uc_mem_map_ptr(uc_engine *uc, uint64_t address, size_t size,\n                      uint32_t perms, void *ptr)\n{\n    uc_err res;\n\n    UC_INIT(uc);\n\n    if (ptr == NULL) {\n        return UC_ERR_ARG;\n    }\n\n    if (uc->mem_redirect) {\n        address = uc->mem_redirect(address);\n    }\n\n    res = mem_map_check(uc, address, size, perms);\n    if (res) {\n        return res;\n    }\n\n    return mem_map(uc, address, size, UC_PROT_ALL,\n                   uc->memory_map_ptr(uc, address, size, perms, ptr));\n}\n\nUNICORN_EXPORT\nuc_err uc_mmio_map(uc_engine *uc, uint64_t address, size_t size,\n                   uc_cb_mmio_read_t read_cb, void *user_data_read,\n                   uc_cb_mmio_write_t write_cb, void *user_data_write)\n{\n    uc_err res;\n\n    UC_INIT(uc);\n\n    if (uc->mem_redirect) {\n        address = uc->mem_redirect(address);\n    }\n\n    res = mem_map_check(uc, address, size, UC_PROT_ALL);\n    if (res)\n        return res;\n\n    // The callbacks do not need to be checked for NULL here, as their presence\n    // (or lack thereof) will determine the permissions used.\n    return mem_map(uc, address, size, UC_PROT_NONE,\n                   uc->memory_map_io(uc, address, size, read_cb, write_cb,\n                                     user_data_read, user_data_write));\n}\n\n// Create a backup copy of the indicated MemoryRegion.\n// Generally used in prepartion for splitting a MemoryRegion.\nstatic uint8_t *copy_region(struct uc_struct *uc, MemoryRegion *mr)\n{\n    uint8_t *block = (uint8_t *)g_malloc0((size_t)int128_get64(mr->size));\n    if (block != NULL) {\n        uc_err err =\n            uc_mem_read(uc, mr->addr, block, (size_t)int128_get64(mr->size));\n        if (err != UC_ERR_OK) {\n            free(block);\n            block = NULL;\n        }\n    }\n\n    return block;\n}\n\n/*\n    This function is similar to split_region, but for MMIO memory.\n\n    This function would delete the region unconditionally.\n\n    Note this function may be called recursively.\n*/\nstatic bool split_mmio_region(struct uc_struct *uc, MemoryRegion *mr,\n                              uint64_t address, size_t size)\n{\n    uint64_t begin, end, chunk_end;\n    size_t l_size, r_size;\n    mmio_cbs backup;\n\n    chunk_end = address + size;\n\n    // This branch also break recursion.\n    if (address <= mr->addr && chunk_end >= mr->end) {\n        return true;\n    }\n\n    if (size == 0) {\n        return false;\n    }\n\n    begin = mr->addr;\n    end = mr->end;\n\n    memcpy(&backup, mr->opaque, sizeof(mmio_cbs));\n\n    /* overlapping cases\n     *               |------mr------|\n     * case 1    |---size--|            // Is it possible???\n     * case 2           |--size--|\n     * case 3                  |---size--|\n     */\n\n    // unmap this region first, then do split it later\n    if (uc_mem_unmap(uc, mr->addr, (size_t)int128_get64(mr->size)) !=\n        UC_ERR_OK) {\n        return false;\n    }\n\n    // adjust some things\n    if (address < begin) {\n        address = begin;\n    }\n    if (chunk_end > end) {\n        chunk_end = end;\n    }\n\n    // compute sub region sizes\n    l_size = (size_t)(address - begin);\n    r_size = (size_t)(end - chunk_end);\n\n    if (l_size > 0) {\n        if (uc_mmio_map(uc, begin, l_size, backup.read, backup.user_data_read,\n                        backup.write, backup.user_data_write) != UC_ERR_OK) {\n            return false;\n        }\n    }\n\n    if (r_size > 0) {\n        if (uc_mmio_map(uc, chunk_end, r_size, backup.read,\n                        backup.user_data_read, backup.write,\n                        backup.user_data_write) != UC_ERR_OK) {\n            return false;\n        }\n    }\n\n    return true;\n}\n\n/*\n   Split the given MemoryRegion at the indicated address for the indicated size\n   this may result in the create of up to 3 spanning sections. If the delete\n   parameter is true, the no new section will be created to replace the indicate\n   range. This functions exists to support uc_mem_protect and uc_mem_unmap.\n\n   This is a static function and callers have already done some preliminary\n   parameter validation.\n\n   The do_delete argument indicates that we are being called to support\n   uc_mem_unmap. In this case we save some time by choosing NOT to remap\n   the areas that are intended to get unmapped\n */\n// TODO: investigate whether qemu region manipulation functions already offered\n// this capability\nstatic bool split_region(struct uc_struct *uc, MemoryRegion *mr,\n                         uint64_t address, size_t size, bool do_delete)\n{\n    uint8_t *backup;\n    uint32_t perms;\n    uint64_t begin, end, chunk_end;\n    size_t l_size, m_size, r_size;\n    RAMBlock *block = NULL;\n    bool prealloc = false;\n\n    chunk_end = address + size;\n\n    // if this region belongs to area [address, address+size],\n    // then there is no work to do.\n    if (address <= mr->addr && chunk_end >= mr->end) {\n        return true;\n    }\n\n    if (size == 0) {\n        // trivial case\n        return true;\n    }\n\n    if (address >= mr->end || chunk_end <= mr->addr) {\n        // impossible case\n        return false;\n    }\n\n    // Find the correct and large enough (which contains our target mr)\n    // to create the content backup.\n    QLIST_FOREACH(block, &uc->ram_list.blocks, next)\n    {\n        // block->offset is the offset within ram_addr_t, not GPA\n        if (block->mr->addr <= mr->addr &&\n            block->used_length + block->mr->addr >= mr->end) {\n            break;\n        }\n    }\n\n    if (block == NULL) {\n        return false;\n    }\n\n    // RAM_PREALLOC is not defined outside exec.c and I didn't feel like\n    // moving it\n    prealloc = !!(block->flags & 1);\n\n    if (block->flags & 1) {\n        backup = block->host;\n    } else {\n        backup = copy_region(uc, mr);\n        if (backup == NULL) {\n            return false;\n        }\n    }\n\n    // save the essential information required for the split before mr gets\n    // deleted\n    perms = mr->perms;\n    begin = mr->addr;\n    end = mr->end;\n\n    // unmap this region first, then do split it later\n    if (uc_mem_unmap(uc, mr->addr, (size_t)int128_get64(mr->size)) !=\n        UC_ERR_OK) {\n        goto error;\n    }\n\n    /* overlapping cases\n     *               |------mr------|\n     * case 1    |---size--|\n     * case 2           |--size--|\n     * case 3                  |---size--|\n     */\n\n    // adjust some things\n    if (address < begin) {\n        address = begin;\n    }\n    if (chunk_end > end) {\n        chunk_end = end;\n    }\n\n    // compute sub region sizes\n    l_size = (size_t)(address - begin);\n    r_size = (size_t)(end - chunk_end);\n    m_size = (size_t)(chunk_end - address);\n\n    // If there are error in any of the below operations, things are too far\n    // gone at that point to recover. Could try to remap orignal region, but\n    // these smaller allocation just failed so no guarantee that we can recover\n    // the original allocation at this point\n    if (l_size > 0) {\n        if (!prealloc) {\n            if (uc_mem_map(uc, begin, l_size, perms) != UC_ERR_OK) {\n                goto error;\n            }\n            if (uc_mem_write(uc, begin, backup, l_size) != UC_ERR_OK) {\n                goto error;\n            }\n        } else {\n            if (uc_mem_map_ptr(uc, begin, l_size, perms, backup) != UC_ERR_OK) {\n                goto error;\n            }\n        }\n    }\n\n    if (m_size > 0 && !do_delete) {\n        if (!prealloc) {\n            if (uc_mem_map(uc, address, m_size, perms) != UC_ERR_OK) {\n                goto error;\n            }\n            if (uc_mem_write(uc, address, backup + l_size, m_size) !=\n                UC_ERR_OK) {\n                goto error;\n            }\n        } else {\n            if (uc_mem_map_ptr(uc, address, m_size, perms, backup + l_size) !=\n                UC_ERR_OK) {\n                goto error;\n            }\n        }\n    }\n\n    if (r_size > 0) {\n        if (!prealloc) {\n            if (uc_mem_map(uc, chunk_end, r_size, perms) != UC_ERR_OK) {\n                goto error;\n            }\n            if (uc_mem_write(uc, chunk_end, backup + l_size + m_size, r_size) !=\n                UC_ERR_OK) {\n                goto error;\n            }\n        } else {\n            if (uc_mem_map_ptr(uc, chunk_end, r_size, perms,\n                               backup + l_size + m_size) != UC_ERR_OK) {\n                goto error;\n            }\n        }\n    }\n\n    if (!prealloc) {\n        free(backup);\n    }\n    return true;\n\nerror:\n    if (!prealloc) {\n        free(backup);\n    }\n    return false;\n}\n\nUNICORN_EXPORT\nuc_err uc_mem_protect(struct uc_struct *uc, uint64_t address, size_t size,\n                      uint32_t perms)\n{\n    MemoryRegion *mr;\n    uint64_t addr = address;\n    size_t count, len;\n    bool remove_exec = false;\n\n    UC_INIT(uc);\n\n    if (size == 0) {\n        // trivial case, no change\n        return UC_ERR_OK;\n    }\n\n    // address must be aligned to uc->target_page_size\n    if ((address & uc->target_page_align) != 0) {\n        return UC_ERR_ARG;\n    }\n\n    // size must be multiple of uc->target_page_size\n    if ((size & uc->target_page_align) != 0) {\n        return UC_ERR_ARG;\n    }\n\n    // check for only valid permissions\n    if ((perms & ~UC_PROT_ALL) != 0) {\n        return UC_ERR_ARG;\n    }\n\n    if (uc->mem_redirect) {\n        address = uc->mem_redirect(address);\n    }\n\n    // check that user's entire requested block is mapped\n    if (!check_mem_area(uc, address, size)) {\n        return UC_ERR_NOMEM;\n    }\n\n    // Now we know entire region is mapped, so change permissions\n    // We may need to split regions if this area spans adjacent regions\n    addr = address;\n    count = 0;\n    while (count < size) {\n        mr = memory_mapping(uc, addr);\n        len = (size_t)MIN(size - count, mr->end - addr);\n        if (!split_region(uc, mr, addr, len, false)) {\n            return UC_ERR_NOMEM;\n        }\n\n        mr = memory_mapping(uc, addr);\n        // will this remove EXEC permission?\n        if (((mr->perms & UC_PROT_EXEC) != 0) &&\n            ((perms & UC_PROT_EXEC) == 0)) {\n            remove_exec = true;\n        }\n        mr->perms = perms;\n        uc->readonly_mem(mr, (perms & UC_PROT_WRITE) == 0);\n\n        count += len;\n        addr += len;\n    }\n\n    // if EXEC permission is removed, then quit TB and continue at the same\n    // place\n    if (remove_exec) {\n        uc->quit_request = true;\n        uc_emu_stop(uc);\n    }\n\n    return UC_ERR_OK;\n}\n\nUNICORN_EXPORT\nuc_err uc_mem_unmap(struct uc_struct *uc, uint64_t address, size_t size)\n{\n    MemoryRegion *mr;\n    uint64_t addr;\n    size_t count, len;\n\n    UC_INIT(uc);\n\n    if (size == 0) {\n        // nothing to unmap\n        return UC_ERR_OK;\n    }\n\n    // address must be aligned to uc->target_page_size\n    if ((address & uc->target_page_align) != 0) {\n        return UC_ERR_ARG;\n    }\n\n    // size must be multiple of uc->target_page_size\n    if ((size & uc->target_page_align) != 0) {\n        return UC_ERR_ARG;\n    }\n\n    if (uc->mem_redirect) {\n        address = uc->mem_redirect(address);\n    }\n\n    // check that user's entire requested block is mapped\n    if (!check_mem_area(uc, address, size)) {\n        return UC_ERR_NOMEM;\n    }\n\n    // Now we know entire region is mapped, so do the unmap\n    // We may need to split regions if this area spans adjacent regions\n    addr = address;\n    count = 0;\n    while (count < size) {\n        mr = memory_mapping(uc, addr);\n        len = (size_t)MIN(size - count, mr->end - addr);\n        if (!mr->ram) {\n            if (!split_mmio_region(uc, mr, addr, len)) {\n                return UC_ERR_NOMEM;\n            }\n        } else {\n            if (!split_region(uc, mr, addr, len, true)) {\n                return UC_ERR_NOMEM;\n            }\n        }\n\n        // if we can retrieve the mapping, then no splitting took place\n        // so unmap here\n        mr = memory_mapping(uc, addr);\n        if (mr != NULL) {\n            uc->memory_unmap(uc, mr);\n        }\n        count += len;\n        addr += len;\n    }\n\n    return UC_ERR_OK;\n}\n\n// find the memory region of this address\nMemoryRegion *memory_mapping(struct uc_struct *uc, uint64_t address)\n{\n    unsigned int i;\n\n    if (uc->mapped_block_count == 0) {\n        return NULL;\n    }\n\n    if (uc->mem_redirect) {\n        address = uc->mem_redirect(address);\n    }\n\n    // try with the cache index first\n    i = uc->mapped_block_cache_index;\n\n    if (i < uc->mapped_block_count && address >= uc->mapped_blocks[i]->addr &&\n        address < uc->mapped_blocks[i]->end) {\n        return uc->mapped_blocks[i];\n    }\n\n    i = bsearch_mapped_blocks(uc, address);\n\n    if (i < uc->mapped_block_count && address >= uc->mapped_blocks[i]->addr &&\n        address <= uc->mapped_blocks[i]->end - 1)\n        return uc->mapped_blocks[i];\n\n    // not found\n    return NULL;\n}\n\nUNICORN_EXPORT\nuc_err uc_hook_add(uc_engine *uc, uc_hook *hh, int type, void *callback,\n                   void *user_data, uint64_t begin, uint64_t end, ...)\n{\n    int ret = UC_ERR_OK;\n    int i = 0;\n\n    UC_INIT(uc);\n\n    struct hook *hook = calloc(1, sizeof(struct hook));\n    if (hook == NULL) {\n        return UC_ERR_NOMEM;\n    }\n\n    hook->begin = begin;\n    hook->end = end;\n    hook->type = type;\n    hook->callback = callback;\n    hook->user_data = user_data;\n    hook->refs = 0;\n    hook->to_delete = false;\n    *hh = (uc_hook)hook;\n\n    // UC_HOOK_INSN has an extra argument for instruction ID\n    if (type & UC_HOOK_INSN) {\n        va_list valist;\n\n        va_start(valist, end);\n        hook->insn = va_arg(valist, int);\n        va_end(valist);\n\n        if (uc->insn_hook_validate) {\n            if (!uc->insn_hook_validate(hook->insn)) {\n                free(hook);\n                return UC_ERR_HOOK;\n            }\n        }\n\n        if (uc->hook_insert) {\n            if (hook_insert(&uc->hook[UC_HOOK_INSN_IDX], hook) == NULL) {\n                free(hook);\n                return UC_ERR_NOMEM;\n            }\n        } else {\n            if (hook_append(&uc->hook[UC_HOOK_INSN_IDX], hook) == NULL) {\n                free(hook);\n                return UC_ERR_NOMEM;\n            }\n        }\n\n        uc->hooks_count[UC_HOOK_INSN_IDX]++;\n        return UC_ERR_OK;\n    }\n\n    if (type & UC_HOOK_TCG_OPCODE) {\n        va_list valist;\n\n        va_start(valist, end);\n        hook->op = va_arg(valist, int);\n        hook->op_flags = va_arg(valist, int);\n        va_end(valist);\n\n        if (uc->opcode_hook_invalidate) {\n            if (!uc->opcode_hook_invalidate(hook->op, hook->op_flags)) {\n                free(hook);\n                return UC_ERR_HOOK;\n            }\n        }\n\n        if (uc->hook_insert) {\n            if (hook_insert(&uc->hook[UC_HOOK_TCG_OPCODE_IDX], hook) == NULL) {\n                free(hook);\n                return UC_ERR_NOMEM;\n            }\n        } else {\n            if (hook_append(&uc->hook[UC_HOOK_TCG_OPCODE_IDX], hook) == NULL) {\n                free(hook);\n                return UC_ERR_NOMEM;\n            }\n        }\n\n        uc->hooks_count[UC_HOOK_TCG_OPCODE_IDX]++;\n        return UC_ERR_OK;\n    }\n\n    while ((type >> i) > 0) {\n        if ((type >> i) & 1) {\n            // TODO: invalid hook error?\n            if (i < UC_HOOK_MAX) {\n                if (uc->hook_insert) {\n                    if (hook_insert(&uc->hook[i], hook) == NULL) {\n                        free(hook);\n                        return UC_ERR_NOMEM;\n                    }\n                } else {\n                    if (hook_append(&uc->hook[i], hook) == NULL) {\n                        free(hook);\n                        return UC_ERR_NOMEM;\n                    }\n                }\n                uc->hooks_count[i]++;\n            }\n        }\n        i++;\n    }\n\n    // we didn't use the hook\n    // TODO: return an error?\n    if (hook->refs == 0) {\n        free(hook);\n    }\n\n    return ret;\n}\n\nUNICORN_EXPORT\nuc_err uc_hook_del(uc_engine *uc, uc_hook hh)\n{\n    int i;\n    struct hook *hook = (struct hook *)hh;\n\n    UC_INIT(uc);\n\n    // we can't dereference hook->type if hook is invalid\n    // so for now we need to iterate over all possible types to remove the hook\n    // which is less efficient\n    // an optimization would be to align the hook pointer\n    // and store the type mask in the hook pointer.\n    for (i = 0; i < UC_HOOK_MAX; i++) {\n        if (list_exists(&uc->hook[i], (void *)hook)) {\n            hook->to_delete = true;\n            uc->hooks_count[i]--;\n            hook_append(&uc->hooks_to_del, hook);\n        }\n    }\n\n    return UC_ERR_OK;\n}\n\n// TCG helper\n// 2 arguments are enough for most opcodes. Load/Store needs 3 arguments but we\n// have memory hooks already. We may exceed the maximum arguments of a tcg\n// helper but that's easy to extend.\nvoid helper_uc_traceopcode(struct hook *hook, uint64_t arg1, uint64_t arg2,\n                           uint32_t size, void *handle, uint64_t address);\nvoid helper_uc_traceopcode(struct hook *hook, uint64_t arg1, uint64_t arg2,\n                           uint32_t size, void *handle, uint64_t address)\n{\n    struct uc_struct *uc = handle;\n\n    if (unlikely(uc->stop_request)) {\n        return;\n    }\n\n    if (unlikely(hook->to_delete)) {\n        return;\n    }\n\n    // We did all checks in translation time.\n    //\n    // This could optimize the case that we have multiple hooks with different\n    // opcodes and have one callback per opcode. Note that the assumption don't\n    // hold in most cases for uc_tracecode.\n    //\n    // TODO: Shall we have a flag to allow users to control whether updating PC?\n    ((uc_hook_tcg_op_2)hook->callback)(uc, address, arg1, arg2, size,\n                                       hook->user_data);\n\n    if (unlikely(uc->stop_request)) {\n        return;\n    }\n}\n\nvoid helper_uc_tracecode(int32_t size, uc_hook_idx index, void *handle,\n                         int64_t address);\nvoid helper_uc_tracecode(int32_t size, uc_hook_idx index, void *handle,\n                         int64_t address)\n{\n    struct uc_struct *uc = handle;\n    struct list_item *cur;\n    struct hook *hook;\n    int hook_flags =\n        index &\n        UC_HOOK_FLAG_MASK; // The index here may contain additional flags. See\n                           // the comments of uc_hook_idx for details.\n\n    index = index & UC_HOOK_IDX_MASK;\n\n    // This has been done in tcg code.\n    // sync PC in CPUArchState with address\n    // if (uc->set_pc) {\n    //     uc->set_pc(uc, address);\n    // }\n\n    // the last callback may already asked to stop emulation\n    if (uc->stop_request && !(hook_flags & UC_HOOK_FLAG_NO_STOP)) {\n        return;\n    }\n\n    for (cur = uc->hook[index].head;\n         cur != NULL && (hook = (struct hook *)cur->data); cur = cur->next) {\n        if (hook->to_delete) {\n            continue;\n        }\n\n        // on invalid block/instruction, call instruction counter (if enable),\n        // then quit\n        if (size == 0) {\n            if (index == UC_HOOK_CODE_IDX && uc->count_hook) {\n                // this is the instruction counter (first hook in the list)\n                ((uc_cb_hookcode_t)hook->callback)(uc, address, size,\n                                                   hook->user_data);\n            }\n\n            return;\n        }\n\n        if (HOOK_BOUND_CHECK(hook, (uint64_t)address)) {\n            ((uc_cb_hookcode_t)hook->callback)(uc, address, size,\n                                               hook->user_data);\n        }\n\n        // the last callback may already asked to stop emulation\n        // Unicorn:\n        //   In an ARM IT block, we behave like the emulation continues\n        //   normally. No check_exit_request is generated and the hooks are\n        //   triggered normally. In other words, the whole IT block is treated\n        //   as a single instruction.\n        if (uc->stop_request && !(hook_flags & UC_HOOK_FLAG_NO_STOP)) {\n            break;\n        }\n    }\n}\n\nUNICORN_EXPORT\nuc_err uc_mem_regions(uc_engine *uc, uc_mem_region **regions, uint32_t *count)\n{\n    uint32_t i;\n    uc_mem_region *r = NULL;\n\n    UC_INIT(uc);\n\n    *count = uc->mapped_block_count;\n\n    if (*count) {\n        r = g_malloc0(*count * sizeof(uc_mem_region));\n        if (r == NULL) {\n            // out of memory\n            return UC_ERR_NOMEM;\n        }\n    }\n\n    for (i = 0; i < *count; i++) {\n        r[i].begin = uc->mapped_blocks[i]->addr;\n        r[i].end = uc->mapped_blocks[i]->end - 1;\n        r[i].perms = uc->mapped_blocks[i]->perms;\n    }\n\n    *regions = r;\n\n    return UC_ERR_OK;\n}\n\nUNICORN_EXPORT\nuc_err uc_query(uc_engine *uc, uc_query_type type, size_t *result)\n{\n    UC_INIT(uc);\n\n    switch (type) {\n    default:\n        return UC_ERR_ARG;\n\n    case UC_QUERY_PAGE_SIZE:\n        *result = uc->target_page_size;\n        break;\n\n    case UC_QUERY_ARCH:\n        *result = uc->arch;\n        break;\n\n    case UC_QUERY_MODE:\n#ifdef UNICORN_HAS_ARM\n        if (uc->arch == UC_ARCH_ARM) {\n            return uc->query(uc, type, result);\n        }\n#endif\n        *result = uc->mode;\n        break;\n\n    case UC_QUERY_TIMEOUT:\n        *result = uc->timed_out;\n        break;\n    }\n\n    return UC_ERR_OK;\n}\n\nUNICORN_EXPORT\nuc_err uc_context_alloc(uc_engine *uc, uc_context **context)\n{\n    struct uc_context **_context = context;\n    size_t size = uc_context_size(uc);\n\n    UC_INIT(uc);\n\n    *_context = g_malloc(size);\n    if (*_context) {\n        (*_context)->context_size = uc->cpu_context_size;\n        (*_context)->arch = uc->arch;\n        (*_context)->mode = uc->mode;\n        return UC_ERR_OK;\n    } else {\n        return UC_ERR_NOMEM;\n    }\n}\n\nUNICORN_EXPORT\nuc_err uc_free(void *mem)\n{\n    g_free(mem);\n    return UC_ERR_OK;\n}\n\nUNICORN_EXPORT\nsize_t uc_context_size(uc_engine *uc)\n{\n    UC_INIT(uc);\n    // return the total size of struct uc_context\n    return sizeof(uc_context) + uc->cpu_context_size;\n}\n\nUNICORN_EXPORT\nuc_err uc_context_save(uc_engine *uc, uc_context *context)\n{\n    UC_INIT(uc);\n\n    memcpy(context->data, uc->cpu->env_ptr, context->context_size);\n\n    return UC_ERR_OK;\n}\n\nUNICORN_EXPORT\nuc_err uc_context_reg_write(uc_context *ctx, int regid, const void *value)\n{\n    return uc_context_reg_write_batch(ctx, &regid, (void *const *)&value, 1);\n}\n\nUNICORN_EXPORT\nuc_err uc_context_reg_read(uc_context *ctx, int regid, void *value)\n{\n    return uc_context_reg_read_batch(ctx, &regid, &value, 1);\n}\n\n// Keep in mind that we don't a uc_engine when r/w the registers of a context.\nstatic void find_context_reg_rw_function(uc_arch arch, uc_mode mode,\n                                         context_reg_rw_t *rw)\n{\n    // We believe that the arch/mode pair is correct.\n    switch (arch) {\n    default:\n        rw->context_reg_read = NULL;\n        rw->context_reg_write = NULL;\n        break;\n#ifdef UNICORN_HAS_M68K\n    case UC_ARCH_M68K:\n        rw->context_reg_read = m68k_context_reg_read;\n        rw->context_reg_write = m68k_context_reg_write;\n        break;\n#endif\n#ifdef UNICORN_HAS_X86\n    case UC_ARCH_X86:\n        rw->context_reg_read = x86_context_reg_read;\n        rw->context_reg_write = x86_context_reg_write;\n        break;\n#endif\n#ifdef UNICORN_HAS_ARM\n    case UC_ARCH_ARM:\n        rw->context_reg_read = arm_context_reg_read;\n        rw->context_reg_write = arm_context_reg_write;\n        break;\n#endif\n#ifdef UNICORN_HAS_ARM64\n    case UC_ARCH_ARM64:\n        rw->context_reg_read = arm64_context_reg_read;\n        rw->context_reg_write = arm64_context_reg_write;\n        break;\n#endif\n\n#if defined(UNICORN_HAS_MIPS) || defined(UNICORN_HAS_MIPSEL) ||                \\\n    defined(UNICORN_HAS_MIPS64) || defined(UNICORN_HAS_MIPS64EL)\n    case UC_ARCH_MIPS:\n        if (mode & UC_MODE_BIG_ENDIAN) {\n#ifdef UNICORN_HAS_MIPS\n            if (mode & UC_MODE_MIPS32) {\n                rw->context_reg_read = mips_context_reg_read;\n                rw->context_reg_write = mips_context_reg_write;\n            }\n#endif\n#ifdef UNICORN_HAS_MIPS64\n            if (mode & UC_MODE_MIPS64) {\n                rw->context_reg_read = mips64_context_reg_read;\n                rw->context_reg_write = mips64_context_reg_write;\n            }\n#endif\n        } else { // little endian\n#ifdef UNICORN_HAS_MIPSEL\n            if (mode & UC_MODE_MIPS32) {\n                rw->context_reg_read = mipsel_context_reg_read;\n                rw->context_reg_write = mipsel_context_reg_write;\n            }\n#endif\n#ifdef UNICORN_HAS_MIPS64EL\n            if (mode & UC_MODE_MIPS64) {\n                rw->context_reg_read = mips64el_context_reg_read;\n                rw->context_reg_write = mips64el_context_reg_write;\n            }\n#endif\n        }\n        break;\n#endif\n\n#ifdef UNICORN_HAS_SPARC\n    case UC_ARCH_SPARC:\n        if (mode & UC_MODE_SPARC64) {\n            rw->context_reg_read = sparc64_context_reg_read;\n            rw->context_reg_write = sparc64_context_reg_write;\n        } else {\n            rw->context_reg_read = sparc_context_reg_read;\n            rw->context_reg_write = sparc_context_reg_write;\n        }\n        break;\n#endif\n#ifdef UNICORN_HAS_PPC\n    case UC_ARCH_PPC:\n        if (mode & UC_MODE_PPC64) {\n            rw->context_reg_read = ppc64_context_reg_read;\n            rw->context_reg_write = ppc64_context_reg_write;\n        } else {\n            rw->context_reg_read = ppc_context_reg_read;\n            rw->context_reg_write = ppc_context_reg_write;\n        }\n        break;\n#endif\n#ifdef UNICORN_HAS_RISCV\n    case UC_ARCH_RISCV:\n        if (mode & UC_MODE_RISCV32) {\n            rw->context_reg_read = riscv32_context_reg_read;\n            rw->context_reg_write = riscv32_context_reg_write;\n        } else if (mode & UC_MODE_RISCV64) {\n            rw->context_reg_read = riscv64_context_reg_read;\n            rw->context_reg_write = riscv64_context_reg_write;\n        }\n        break;\n#endif\n#ifdef UNICORN_HAS_S390X\n    case UC_ARCH_S390X:\n        rw->context_reg_read = s390_context_reg_read;\n        rw->context_reg_write = s390_context_reg_write;\n        break;\n#endif\n    }\n\n    return;\n}\n\nUNICORN_EXPORT\nuc_err uc_context_reg_write_batch(uc_context *ctx, int *ids, void *const *vals,\n                                  int count)\n{\n    int ret = UC_ERR_OK;\n    context_reg_rw_t rw;\n\n    find_context_reg_rw_function(ctx->arch, ctx->mode, &rw);\n    if (rw.context_reg_write) {\n        ret = rw.context_reg_write(ctx, (unsigned int *)ids, vals, count);\n    } else {\n        return UC_ERR_HANDLE;\n    }\n\n    return ret;\n}\n\nUNICORN_EXPORT\nuc_err uc_context_reg_read_batch(uc_context *ctx, int *ids, void **vals,\n                                 int count)\n{\n    int ret = UC_ERR_OK;\n    context_reg_rw_t rw;\n\n    find_context_reg_rw_function(ctx->arch, ctx->mode, &rw);\n    if (rw.context_reg_read) {\n        ret = rw.context_reg_read(ctx, (unsigned int *)ids, vals, count);\n    } else {\n        return UC_ERR_HANDLE;\n    }\n\n    return ret;\n}\n\nUNICORN_EXPORT\nuc_err uc_context_restore(uc_engine *uc, uc_context *context)\n{\n    UC_INIT(uc);\n\n    memcpy(uc->cpu->env_ptr, context->data, context->context_size);\n\n    return UC_ERR_OK;\n}\n\nUNICORN_EXPORT\nuc_err uc_context_free(uc_context *context)\n{\n\n    return uc_free(context);\n}\n\ntypedef struct _uc_ctl_exit_request {\n    uint64_t *array;\n    size_t len;\n} uc_ctl_exit_request;\n\nstatic inline gboolean uc_read_exit_iter(gpointer key, gpointer val,\n                                         gpointer data)\n{\n    uc_ctl_exit_request *req = (uc_ctl_exit_request *)data;\n\n    req->array[req->len++] = *(uint64_t *)key;\n\n    return false;\n}\n\nUNICORN_EXPORT\nuc_err uc_ctl(uc_engine *uc, uc_control_type control, ...)\n{\n    int rw, type;\n    uc_err err = UC_ERR_OK;\n    va_list args;\n\n    // MSVC Would do signed shift on signed integers.\n    rw = (uint32_t)control >> 30;\n    type = (control & ((1 << 16) - 1));\n    va_start(args, control);\n\n    switch (type) {\n    case UC_CTL_UC_MODE: {\n        if (rw == UC_CTL_IO_READ) {\n            int *pmode = va_arg(args, int *);\n            *pmode = uc->mode;\n        } else {\n            err = UC_ERR_ARG;\n        }\n        break;\n    }\n\n    case UC_CTL_UC_ARCH: {\n        if (rw == UC_CTL_IO_READ) {\n            int *arch = va_arg(args, int *);\n            *arch = uc->arch;\n        } else {\n            err = UC_ERR_ARG;\n        }\n        break;\n    }\n\n    case UC_CTL_UC_TIMEOUT: {\n        if (rw == UC_CTL_IO_READ) {\n            uint64_t *arch = va_arg(args, uint64_t *);\n            *arch = uc->timeout;\n        } else {\n            err = UC_ERR_ARG;\n        }\n        break;\n    }\n\n    case UC_CTL_UC_PAGE_SIZE: {\n        if (rw == UC_CTL_IO_READ) {\n\n            UC_INIT(uc);\n\n            uint32_t *page_size = va_arg(args, uint32_t *);\n            *page_size = uc->target_page_size;\n        } else {\n            uint32_t page_size = va_arg(args, uint32_t);\n            int bits = 0;\n\n            if (uc->init_done) {\n                err = UC_ERR_ARG;\n                break;\n            }\n\n            if (uc->arch != UC_ARCH_ARM) {\n                err = UC_ERR_ARG;\n                break;\n            }\n\n            if ((page_size & (page_size - 1))) {\n                err = UC_ERR_ARG;\n                break;\n            }\n\n            while (page_size) {\n                bits++;\n                page_size >>= 1;\n            }\n\n            uc->target_bits = bits;\n\n            err = UC_ERR_OK;\n        }\n        break;\n    }\n\n    case UC_CTL_UC_USE_EXITS: {\n        if (rw == UC_CTL_IO_WRITE) {\n            int use_exits = va_arg(args, int);\n            uc->use_exits = use_exits;\n        } else {\n            err = UC_ERR_ARG;\n        }\n        break;\n    }\n\n    case UC_CTL_UC_EXITS_CNT: {\n\n        UC_INIT(uc);\n\n        if (!uc->use_exits) {\n            err = UC_ERR_ARG;\n        } else if (rw == UC_CTL_IO_READ) {\n            size_t *exits_cnt = va_arg(args, size_t *);\n            *exits_cnt = g_tree_nnodes(uc->ctl_exits);\n        } else {\n            err = UC_ERR_ARG;\n        }\n        break;\n    }\n\n    case UC_CTL_UC_EXITS: {\n\n        UC_INIT(uc);\n\n        if (!uc->use_exits) {\n            err = UC_ERR_ARG;\n        } else if (rw == UC_CTL_IO_READ) {\n            uint64_t *exits = va_arg(args, uint64_t *);\n            size_t cnt = va_arg(args, size_t);\n            if (cnt < g_tree_nnodes(uc->ctl_exits)) {\n                err = UC_ERR_ARG;\n            } else {\n                uc_ctl_exit_request req;\n                req.array = exits;\n                req.len = 0;\n\n                g_tree_foreach(uc->ctl_exits, uc_read_exit_iter, (void *)&req);\n            }\n        } else if (rw == UC_CTL_IO_WRITE) {\n            uint64_t *exits = va_arg(args, uint64_t *);\n            size_t cnt = va_arg(args, size_t);\n\n            g_tree_remove_all(uc->ctl_exits);\n\n            for (size_t i = 0; i < cnt; i++) {\n                uc_add_exit(uc, exits[i]);\n            }\n        } else {\n            err = UC_ERR_ARG;\n        }\n        break;\n    }\n\n    case UC_CTL_CPU_MODEL: {\n        if (rw == UC_CTL_IO_READ) {\n\n            UC_INIT(uc);\n\n            int *model = va_arg(args, int *);\n            *model = uc->cpu_model;\n        } else {\n            int model = va_arg(args, int);\n\n            if (uc->init_done) {\n                err = UC_ERR_ARG;\n                break;\n            }\n\n            if (uc->arch == UC_ARCH_ARM) {\n                if (uc->mode & UC_MODE_BIG_ENDIAN) {\n                    // These cpu models don't support big endian code access.\n                    if (model <= UC_CPU_ARM_CORTEX_A15 &&\n                        model >= UC_CPU_ARM_CORTEX_A7) {\n                        err = UC_ERR_ARG;\n                        break;\n                    }\n                }\n            }\n\n            uc->cpu_model = model;\n\n            err = UC_ERR_OK;\n        }\n        break;\n    }\n\n    case UC_CTL_TB_REQUEST_CACHE: {\n\n        UC_INIT(uc);\n\n        if (rw == UC_CTL_IO_READ_WRITE) {\n            uint64_t addr = va_arg(args, uint64_t);\n            uc_tb *tb = va_arg(args, uc_tb *);\n            err = uc->uc_gen_tb(uc, addr, tb);\n        } else {\n            err = UC_ERR_ARG;\n        }\n        break;\n    }\n\n    case UC_CTL_TB_REMOVE_CACHE: {\n\n        UC_INIT(uc);\n\n        if (rw == UC_CTL_IO_WRITE) {\n            uint64_t addr = va_arg(args, uint64_t);\n            uint64_t end = va_arg(args, uint64_t);\n            if (end <= addr) {\n                err = UC_ERR_ARG;\n            } else {\n                uc->uc_invalidate_tb(uc, addr, end - addr);\n            }\n        } else {\n            err = UC_ERR_ARG;\n        }\n        break;\n    }\n\n    default:\n        err = UC_ERR_ARG;\n        break;\n    }\n\n    va_end(args);\n\n    return err;\n}\n\n#ifdef UNICORN_TRACER\nuc_tracer *get_tracer()\n{\n    static uc_tracer tracer;\n    return &tracer;\n}\n\nvoid trace_start(uc_tracer *tracer, trace_loc loc)\n{\n    tracer->starts[loc] = get_clock();\n}\n\nvoid trace_end(uc_tracer *tracer, trace_loc loc, const char *fmt, ...)\n{\n    va_list args;\n    int64_t end = get_clock();\n\n    va_start(args, fmt);\n\n    vfprintf(stderr, fmt, args);\n\n    va_end(args);\n\n    fprintf(stderr, \"%.6fus\\n\",\n            (double)(end - tracer->starts[loc]) / (double)(1000));\n}\n#endif"], "filenames": ["uc.c"], "buggy_code_start_loc": [390], "buggy_code_end_loc": [390], "fixing_code_start_loc": [391], "fixing_code_end_loc": [392], "type": "CWE-401", "message": "Unicorn Engine v2.0.0-rc7 and below was discovered to contain a memory leak via the function uc_close at /my/unicorn/uc.c.", "other": {"cve": {"id": "CVE-2022-29693", "sourceIdentifier": "cve@mitre.org", "published": "2022-06-02T14:15:50.513", "lastModified": "2022-06-09T20:42:00.747", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Unicorn Engine v2.0.0-rc7 and below was discovered to contain a memory leak via the function uc_close at /my/unicorn/uc.c."}, {"lang": "es", "value": "Se ha detectado que Unicorn Engine versiones v2.0.0-rc7 y anteriores ,conten\u00edan una p\u00e9rdida de memoria por medio de la funci\u00f3n uc_close en el archivo /my/unicorn/uc.c"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 5.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-401"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:unicorn-engine:unicorn_engine:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.0.0", "matchCriteriaId": "9D2ABAA2-6A02-4D61-8F92-77EC8211ACC2"}, {"vulnerable": true, "criteria": "cpe:2.3:a:unicorn-engine:unicorn_engine:2.0.0:-:*:*:*:*:*:*", "matchCriteriaId": "7BBDE074-9876-4FF1-8664-6141001A6245"}, {"vulnerable": true, "criteria": "cpe:2.3:a:unicorn-engine:unicorn_engine:2.0.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "54D5A177-A072-4A7F-9C0A-57545CC04FDD"}, {"vulnerable": true, "criteria": "cpe:2.3:a:unicorn-engine:unicorn_engine:2.0.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "B3D6EED4-B4EE-4486-A0FA-6932A27179F9"}, {"vulnerable": true, "criteria": "cpe:2.3:a:unicorn-engine:unicorn_engine:2.0.0:rc3:*:*:*:*:*:*", "matchCriteriaId": "2401D8B7-59B6-4FEC-8E1C-4D61FC5861C6"}, {"vulnerable": true, "criteria": "cpe:2.3:a:unicorn-engine:unicorn_engine:2.0.0:rc4:*:*:*:*:*:*", "matchCriteriaId": "564B499B-2038-4FBE-B503-1200E10D2DAC"}, {"vulnerable": true, "criteria": "cpe:2.3:a:unicorn-engine:unicorn_engine:2.0.0:rc5:*:*:*:*:*:*", "matchCriteriaId": "C819005A-3A48-4C3F-96FB-ED72F33B05CE"}, {"vulnerable": true, "criteria": "cpe:2.3:a:unicorn-engine:unicorn_engine:2.0.0:rc6:*:*:*:*:*:*", "matchCriteriaId": "77AFCCB5-87B5-4E1A-9850-2E218D9074CD"}]}]}], "references": [{"url": "https://github.com/unicorn-engine/unicorn/commit/469fc4c35a0cfabdbefb158e22d145f4ee6f77b9", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/unicorn-engine/unicorn/issues/1586", "source": "cve@mitre.org", "tags": ["Exploit", "Issue Tracking", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/unicorn-engine/unicorn/commit/469fc4c35a0cfabdbefb158e22d145f4ee6f77b9"}}