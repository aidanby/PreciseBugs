{"buggy_code": ["package record\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"fmt\"\n\t\"sync\"\n\n\t\"github.com/libp2p/go-libp2p/core/crypto\"\n\t\"github.com/libp2p/go-libp2p/core/internal/catch\"\n\t\"github.com/libp2p/go-libp2p/core/record/pb\"\n\n\tpool \"github.com/libp2p/go-buffer-pool\"\n\n\t\"github.com/multiformats/go-varint\"\n\t\"google.golang.org/protobuf/proto\"\n)\n\n//go:generate protoc --proto_path=$PWD:$PWD/../.. --go_out=. --go_opt=Mpb/envelope.proto=./pb pb/envelope.proto\n\n// Envelope contains an arbitrary []byte payload, signed by a libp2p peer.\n//\n// Envelopes are signed in the context of a particular \"domain\", which is a\n// string specified when creating and verifying the envelope. You must know the\n// domain string used to produce the envelope in order to verify the signature\n// and access the payload.\ntype Envelope struct {\n\t// The public key that can be used to verify the signature and derive the peer id of the signer.\n\tPublicKey crypto.PubKey\n\n\t// A binary identifier that indicates what kind of data is contained in the payload.\n\t// TODO(yusef): enforce multicodec prefix\n\tPayloadType []byte\n\n\t// The envelope payload.\n\tRawPayload []byte\n\n\t// The signature of the domain string :: type hint :: payload.\n\tsignature []byte\n\n\t// the unmarshalled payload as a Record, cached on first access via the Record accessor method\n\tcached         Record\n\tunmarshalError error\n\tunmarshalOnce  sync.Once\n}\n\nvar ErrEmptyDomain = errors.New(\"envelope domain must not be empty\")\nvar ErrEmptyPayloadType = errors.New(\"payloadType must not be empty\")\nvar ErrInvalidSignature = errors.New(\"invalid signature or incorrect domain\")\n\n// Seal marshals the given Record, places the marshaled bytes inside an Envelope,\n// and signs with the given private key.\nfunc Seal(rec Record, privateKey crypto.PrivKey) (*Envelope, error) {\n\tpayload, err := rec.MarshalRecord()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error marshaling record: %v\", err)\n\t}\n\n\tdomain := rec.Domain()\n\tpayloadType := rec.Codec()\n\tif domain == \"\" {\n\t\treturn nil, ErrEmptyDomain\n\t}\n\n\tif len(payloadType) == 0 {\n\t\treturn nil, ErrEmptyPayloadType\n\t}\n\n\tunsigned, err := makeUnsigned(domain, payloadType, payload)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer pool.Put(unsigned)\n\n\tsig, err := privateKey.Sign(unsigned)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &Envelope{\n\t\tPublicKey:   privateKey.GetPublic(),\n\t\tPayloadType: payloadType,\n\t\tRawPayload:  payload,\n\t\tsignature:   sig,\n\t}, nil\n}\n\n// ConsumeEnvelope unmarshals a serialized Envelope and validates its\n// signature using the provided 'domain' string. If validation fails, an error\n// is returned, along with the unmarshalled envelope so it can be inspected.\n//\n// On success, ConsumeEnvelope returns the Envelope itself, as well as the inner payload,\n// unmarshalled into a concrete Record type. The actual type of the returned Record depends\n// on what has been registered for the Envelope's PayloadType (see RegisterType for details).\n//\n// You can type assert on the returned Record to convert it to an instance of the concrete\n// Record type:\n//\n//\tenvelope, rec, err := ConsumeEnvelope(envelopeBytes, peer.PeerRecordEnvelopeDomain)\n//\tif err != nil {\n//\t  handleError(envelope, err)  // envelope may be non-nil, even if errors occur!\n//\t  return\n//\t}\n//\tpeerRec, ok := rec.(*peer.PeerRecord)\n//\tif ok {\n//\t  doSomethingWithPeerRecord(peerRec)\n//\t}\n//\n// Important: you MUST check the error value before using the returned Envelope. In some error\n// cases, including when the envelope signature is invalid, both the Envelope and an error will\n// be returned. This allows you to inspect the unmarshalled but invalid Envelope. As a result,\n// you must not assume that any non-nil Envelope returned from this function is valid.\n//\n// If the Envelope signature is valid, but no Record type is registered for the Envelope's\n// PayloadType, ErrPayloadTypeNotRegistered will be returned, along with the Envelope and\n// a nil Record.\nfunc ConsumeEnvelope(data []byte, domain string) (envelope *Envelope, rec Record, err error) {\n\te, err := UnmarshalEnvelope(data)\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"failed when unmarshalling the envelope: %w\", err)\n\t}\n\n\terr = e.validate(domain)\n\tif err != nil {\n\t\treturn e, nil, fmt.Errorf(\"failed to validate envelope: %w\", err)\n\t}\n\n\trec, err = e.Record()\n\tif err != nil {\n\t\treturn e, nil, fmt.Errorf(\"failed to unmarshal envelope payload: %w\", err)\n\t}\n\treturn e, rec, nil\n}\n\n// ConsumeTypedEnvelope unmarshals a serialized Envelope and validates its\n// signature. If validation fails, an error is returned, along with the unmarshalled\n// envelope so it can be inspected.\n//\n// Unlike ConsumeEnvelope, ConsumeTypedEnvelope does not try to automatically determine\n// the type of Record to unmarshal the Envelope's payload into. Instead, the caller provides\n// a destination Record instance, which will unmarshal the Envelope payload. It is the caller's\n// responsibility to determine whether the given Record type is able to unmarshal the payload\n// correctly.\n//\n//\trec := &MyRecordType{}\n//\tenvelope, err := ConsumeTypedEnvelope(envelopeBytes, rec)\n//\tif err != nil {\n//\t  handleError(envelope, err)\n//\t}\n//\tdoSomethingWithRecord(rec)\n//\n// Important: you MUST check the error value before using the returned Envelope. In some error\n// cases, including when the envelope signature is invalid, both the Envelope and an error will\n// be returned. This allows you to inspect the unmarshalled but invalid Envelope. As a result,\n// you must not assume that any non-nil Envelope returned from this function is valid.\nfunc ConsumeTypedEnvelope(data []byte, destRecord Record) (envelope *Envelope, err error) {\n\te, err := UnmarshalEnvelope(data)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed when unmarshalling the envelope: %w\", err)\n\t}\n\n\terr = e.validate(destRecord.Domain())\n\tif err != nil {\n\t\treturn e, fmt.Errorf(\"failed to validate envelope: %w\", err)\n\t}\n\n\terr = destRecord.UnmarshalRecord(e.RawPayload)\n\tif err != nil {\n\t\treturn e, fmt.Errorf(\"failed to unmarshal envelope payload: %w\", err)\n\t}\n\te.cached = destRecord\n\treturn e, nil\n}\n\n// UnmarshalEnvelope unmarshals a serialized Envelope protobuf message,\n// without validating its contents. Most users should use ConsumeEnvelope.\nfunc UnmarshalEnvelope(data []byte) (*Envelope, error) {\n\tvar e pb.Envelope\n\tif err := proto.Unmarshal(data, &e); err != nil {\n\t\treturn nil, err\n\t}\n\n\tkey, err := crypto.PublicKeyFromProto(e.PublicKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &Envelope{\n\t\tPublicKey:   key,\n\t\tPayloadType: e.PayloadType,\n\t\tRawPayload:  e.Payload,\n\t\tsignature:   e.Signature,\n\t}, nil\n}\n\n// Marshal returns a byte slice containing a serialized protobuf representation\n// of a Envelope.\nfunc (e *Envelope) Marshal() (res []byte, err error) {\n\tdefer func() { catch.HandlePanic(recover(), &err, \"libp2p envelope marshal\") }()\n\tkey, err := crypto.PublicKeyToProto(e.PublicKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tmsg := pb.Envelope{\n\t\tPublicKey:   key,\n\t\tPayloadType: e.PayloadType,\n\t\tPayload:     e.RawPayload,\n\t\tSignature:   e.signature,\n\t}\n\treturn proto.Marshal(&msg)\n}\n\n// Equal returns true if the other Envelope has the same public key,\n// payload, payload type, and signature. This implies that they were also\n// created with the same domain string.\nfunc (e *Envelope) Equal(other *Envelope) bool {\n\tif other == nil {\n\t\treturn e == nil\n\t}\n\treturn e.PublicKey.Equals(other.PublicKey) &&\n\t\tbytes.Equal(e.PayloadType, other.PayloadType) &&\n\t\tbytes.Equal(e.signature, other.signature) &&\n\t\tbytes.Equal(e.RawPayload, other.RawPayload)\n}\n\n// Record returns the Envelope's payload unmarshalled as a Record.\n// The concrete type of the returned Record depends on which Record\n// type was registered for the Envelope's PayloadType - see record.RegisterType.\n//\n// Once unmarshalled, the Record is cached for future access.\nfunc (e *Envelope) Record() (Record, error) {\n\te.unmarshalOnce.Do(func() {\n\t\tif e.cached != nil {\n\t\t\treturn\n\t\t}\n\t\te.cached, e.unmarshalError = unmarshalRecordPayload(e.PayloadType, e.RawPayload)\n\t})\n\treturn e.cached, e.unmarshalError\n}\n\n// TypedRecord unmarshals the Envelope's payload to the given Record instance.\n// It is the caller's responsibility to ensure that the Record type is capable\n// of unmarshalling the Envelope payload. Callers can inspect the Envelope's\n// PayloadType field to determine the correct type of Record to use.\n//\n// This method will always unmarshal the Envelope payload even if a cached record\n// exists.\nfunc (e *Envelope) TypedRecord(dest Record) error {\n\treturn dest.UnmarshalRecord(e.RawPayload)\n}\n\n// validate returns nil if the envelope signature is valid for the given 'domain',\n// or an error if signature validation fails.\nfunc (e *Envelope) validate(domain string) error {\n\tunsigned, err := makeUnsigned(domain, e.PayloadType, e.RawPayload)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer pool.Put(unsigned)\n\n\tvalid, err := e.PublicKey.Verify(unsigned, e.signature)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed while verifying signature: %w\", err)\n\t}\n\tif !valid {\n\t\treturn ErrInvalidSignature\n\t}\n\treturn nil\n}\n\n// makeUnsigned is a helper function that prepares a buffer to sign or verify.\n// It returns a byte slice from a pool. The caller MUST return this slice to the\n// pool.\nfunc makeUnsigned(domain string, payloadType []byte, payload []byte) ([]byte, error) {\n\tvar (\n\t\tfields = [][]byte{[]byte(domain), payloadType, payload}\n\n\t\t// fields are prefixed with their length as an unsigned varint. we\n\t\t// compute the lengths before allocating the sig buffer so we know how\n\t\t// much space to add for the lengths\n\t\tflen = make([][]byte, len(fields))\n\t\tsize = 0\n\t)\n\n\tfor i, f := range fields {\n\t\tl := len(f)\n\t\tflen[i] = varint.ToUvarint(uint64(l))\n\t\tsize += l + len(flen[i])\n\t}\n\n\tb := pool.Get(size)\n\n\tvar s int\n\tfor i, f := range fields {\n\t\ts += copy(b[s:], flen[i])\n\t\ts += copy(b[s:], f)\n\t}\n\n\treturn b[:s], nil\n}\n", "package identify\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/exp/slices\"\n\n\t\"github.com/libp2p/go-libp2p/core/crypto\"\n\t\"github.com/libp2p/go-libp2p/core/event\"\n\t\"github.com/libp2p/go-libp2p/core/host\"\n\t\"github.com/libp2p/go-libp2p/core/network\"\n\t\"github.com/libp2p/go-libp2p/core/peer\"\n\t\"github.com/libp2p/go-libp2p/core/peerstore\"\n\t\"github.com/libp2p/go-libp2p/core/protocol\"\n\t\"github.com/libp2p/go-libp2p/core/record\"\n\t\"github.com/libp2p/go-libp2p/p2p/host/eventbus\"\n\t\"github.com/libp2p/go-libp2p/p2p/protocol/identify/pb\"\n\n\tlogging \"github.com/ipfs/go-log/v2\"\n\t\"github.com/libp2p/go-msgio/pbio\"\n\tma \"github.com/multiformats/go-multiaddr\"\n\tmanet \"github.com/multiformats/go-multiaddr/net\"\n\tmsmux \"github.com/multiformats/go-multistream\"\n\t\"google.golang.org/protobuf/proto\"\n)\n\n//go:generate protoc --proto_path=$PWD:$PWD/../../.. --go_out=. --go_opt=Mpb/identify.proto=./pb pb/identify.proto\n\nvar log = logging.Logger(\"net/identify\")\n\nconst (\n\t// ID is the protocol.ID of version 1.0.0 of the identify service.\n\tID = \"/ipfs/id/1.0.0\"\n\t// IDPush is the protocol.ID of the Identify push protocol.\n\t// It sends full identify messages containing the current state of the peer.\n\tIDPush = \"/ipfs/id/push/1.0.0\"\n)\n\nconst DefaultProtocolVersion = \"ipfs/0.1.0\"\n\nconst ServiceName = \"libp2p.identify\"\n\nconst maxPushConcurrency = 32\n\n// StreamReadTimeout is the read timeout on all incoming Identify family streams.\nvar StreamReadTimeout = 60 * time.Second\n\nconst (\n\tlegacyIDSize = 2 * 1024 // 2k Bytes\n\tsignedIDSize = 8 * 1024 // 8K\n\tmaxMessages  = 10\n)\n\nvar defaultUserAgent = \"github.com/libp2p/go-libp2p\"\n\ntype identifySnapshot struct {\n\tseq       uint64\n\tprotocols []protocol.ID\n\taddrs     []ma.Multiaddr\n\trecord    *record.Envelope\n}\n\n// Equal says if two snapshots are identical.\n// It does NOT compare the sequence number.\nfunc (s identifySnapshot) Equal(other *identifySnapshot) bool {\n\thasRecord := s.record != nil\n\totherHasRecord := other.record != nil\n\tif hasRecord != otherHasRecord {\n\t\treturn false\n\t}\n\tif hasRecord && !s.record.Equal(other.record) {\n\t\treturn false\n\t}\n\tif !slices.Equal(s.protocols, other.protocols) {\n\t\treturn false\n\t}\n\tif len(s.addrs) != len(other.addrs) {\n\t\treturn false\n\t}\n\tfor i, a := range s.addrs {\n\t\tif !a.Equal(other.addrs[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\ntype IDService interface {\n\t// IdentifyConn synchronously triggers an identify request on the connection and\n\t// waits for it to complete. If the connection is being identified by another\n\t// caller, this call will wait. If the connection has already been identified,\n\t// it will return immediately.\n\tIdentifyConn(network.Conn)\n\t// IdentifyWait triggers an identify (if the connection has not already been\n\t// identified) and returns a channel that is closed when the identify protocol\n\t// completes.\n\tIdentifyWait(network.Conn) <-chan struct{}\n\t// OwnObservedAddrs returns the addresses peers have reported we've dialed from\n\tOwnObservedAddrs() []ma.Multiaddr\n\t// ObservedAddrsFor returns the addresses peers have reported we've dialed from,\n\t// for a specific local address.\n\tObservedAddrsFor(local ma.Multiaddr) []ma.Multiaddr\n\tStart()\n\tio.Closer\n}\n\ntype identifyPushSupport uint8\n\nconst (\n\tidentifyPushSupportUnknown identifyPushSupport = iota\n\tidentifyPushSupported\n\tidentifyPushUnsupported\n)\n\ntype entry struct {\n\t// The IdentifyWaitChan is created when IdentifyWait is called for the first time.\n\t// IdentifyWait closes this channel when the Identify request completes, or when it fails.\n\tIdentifyWaitChan chan struct{}\n\n\t// PushSupport saves our knowledge about the peer's support of the Identify Push protocol.\n\t// Before the identify request returns, we don't know yet if the peer supports Identify Push.\n\tPushSupport identifyPushSupport\n\t// Sequence is the sequence number of the last snapshot we sent to this peer.\n\tSequence uint64\n}\n\n// idService is a structure that implements ProtocolIdentify.\n// It is a trivial service that gives the other peer some\n// useful information about the local peer. A sort of hello.\n//\n// The idService sends:\n//   - Our libp2p Protocol Version\n//   - Our libp2p Agent Version\n//   - Our public Listen Addresses\ntype idService struct {\n\tHost            host.Host\n\tUserAgent       string\n\tProtocolVersion string\n\n\tmetricsTracer MetricsTracer\n\n\tsetupCompleted chan struct{} // is closed when Start has finished setting up\n\tctx            context.Context\n\tctxCancel      context.CancelFunc\n\t// track resources that need to be shut down before we shut down\n\trefCount sync.WaitGroup\n\n\tdisableSignedPeerRecord bool\n\n\tconnsMu sync.RWMutex\n\t// The conns map contains all connections we're currently handling.\n\t// Connections are inserted as soon as they're available in the swarm\n\t// Connections are removed from the map when the connection disconnects.\n\tconns map[network.Conn]entry\n\n\taddrMu sync.Mutex\n\n\t// our own observed addresses.\n\tobservedAddrs *ObservedAddrManager\n\n\temitters struct {\n\t\tevtPeerProtocolsUpdated        event.Emitter\n\t\tevtPeerIdentificationCompleted event.Emitter\n\t\tevtPeerIdentificationFailed    event.Emitter\n\t}\n\n\tcurrentSnapshot struct {\n\t\tsync.Mutex\n\t\tsnapshot identifySnapshot\n\t}\n}\n\n// NewIDService constructs a new *idService and activates it by\n// attaching its stream handler to the given host.Host.\nfunc NewIDService(h host.Host, opts ...Option) (*idService, error) {\n\tvar cfg config\n\tfor _, opt := range opts {\n\t\topt(&cfg)\n\t}\n\n\tuserAgent := defaultUserAgent\n\tif cfg.userAgent != \"\" {\n\t\tuserAgent = cfg.userAgent\n\t}\n\n\tprotocolVersion := DefaultProtocolVersion\n\tif cfg.protocolVersion != \"\" {\n\t\tprotocolVersion = cfg.protocolVersion\n\t}\n\n\tctx, cancel := context.WithCancel(context.Background())\n\ts := &idService{\n\t\tHost:                    h,\n\t\tUserAgent:               userAgent,\n\t\tProtocolVersion:         protocolVersion,\n\t\tctx:                     ctx,\n\t\tctxCancel:               cancel,\n\t\tconns:                   make(map[network.Conn]entry),\n\t\tdisableSignedPeerRecord: cfg.disableSignedPeerRecord,\n\t\tsetupCompleted:          make(chan struct{}),\n\t\tmetricsTracer:           cfg.metricsTracer,\n\t}\n\n\tobservedAddrs, err := NewObservedAddrManager(h)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create observed address manager: %s\", err)\n\t}\n\ts.observedAddrs = observedAddrs\n\n\ts.emitters.evtPeerProtocolsUpdated, err = h.EventBus().Emitter(&event.EvtPeerProtocolsUpdated{})\n\tif err != nil {\n\t\tlog.Warnf(\"identify service not emitting peer protocol updates; err: %s\", err)\n\t}\n\ts.emitters.evtPeerIdentificationCompleted, err = h.EventBus().Emitter(&event.EvtPeerIdentificationCompleted{})\n\tif err != nil {\n\t\tlog.Warnf(\"identify service not emitting identification completed events; err: %s\", err)\n\t}\n\ts.emitters.evtPeerIdentificationFailed, err = h.EventBus().Emitter(&event.EvtPeerIdentificationFailed{})\n\tif err != nil {\n\t\tlog.Warnf(\"identify service not emitting identification failed events; err: %s\", err)\n\t}\n\treturn s, nil\n}\n\nfunc (ids *idService) Start() {\n\tids.Host.Network().Notify((*netNotifiee)(ids))\n\tids.Host.SetStreamHandler(ID, ids.handleIdentifyRequest)\n\tids.Host.SetStreamHandler(IDPush, ids.handlePush)\n\tids.updateSnapshot()\n\tclose(ids.setupCompleted)\n\n\tids.refCount.Add(1)\n\tgo ids.loop(ids.ctx)\n}\n\nfunc (ids *idService) loop(ctx context.Context) {\n\tdefer ids.refCount.Done()\n\n\tsub, err := ids.Host.EventBus().Subscribe(\n\t\t[]any{&event.EvtLocalProtocolsUpdated{}, &event.EvtLocalAddressesUpdated{}},\n\t\teventbus.BufSize(256),\n\t\teventbus.Name(\"identify (loop)\"),\n\t)\n\tif err != nil {\n\t\tlog.Errorf(\"failed to subscribe to events on the bus, err=%s\", err)\n\t\treturn\n\t}\n\tdefer sub.Close()\n\n\t// Send pushes from a separate Go routine.\n\t// That way, we can end up with\n\t// * this Go routine busy looping over all peers in sendPushes\n\t// * another push being queued in the triggerPush channel\n\ttriggerPush := make(chan struct{}, 1)\n\tids.refCount.Add(1)\n\tgo func() {\n\t\tdefer ids.refCount.Done()\n\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\tcase <-triggerPush:\n\t\t\t\tids.sendPushes(ctx)\n\t\t\t}\n\t\t}\n\t}()\n\n\tfor {\n\t\tselect {\n\t\tcase e, ok := <-sub.Out():\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif updated := ids.updateSnapshot(); !updated {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif ids.metricsTracer != nil {\n\t\t\t\tids.metricsTracer.TriggeredPushes(e)\n\t\t\t}\n\t\t\tselect {\n\t\t\tcase triggerPush <- struct{}{}:\n\t\t\tdefault: // we already have one more push queued, no need to queue another one\n\t\t\t}\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (ids *idService) sendPushes(ctx context.Context) {\n\tids.connsMu.RLock()\n\tconns := make([]network.Conn, 0, len(ids.conns))\n\tfor c, e := range ids.conns {\n\t\t// Push even if we don't know if push is supported.\n\t\t// This will be only the case while the IdentifyWaitChan call is in flight.\n\t\tif e.PushSupport == identifyPushSupported || e.PushSupport == identifyPushSupportUnknown {\n\t\t\tconns = append(conns, c)\n\t\t}\n\t}\n\tids.connsMu.RUnlock()\n\n\tsem := make(chan struct{}, maxPushConcurrency)\n\tvar wg sync.WaitGroup\n\tfor _, c := range conns {\n\t\t// check if the connection is still alive\n\t\tids.connsMu.RLock()\n\t\te, ok := ids.conns[c]\n\t\tids.connsMu.RUnlock()\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\t// check if we already sent the current snapshot to this peer\n\t\tids.currentSnapshot.Lock()\n\t\tsnapshot := ids.currentSnapshot.snapshot\n\t\tids.currentSnapshot.Unlock()\n\t\tif e.Sequence >= snapshot.seq {\n\t\t\tlog.Debugw(\"already sent this snapshot to peer\", \"peer\", c.RemotePeer(), \"seq\", snapshot.seq)\n\t\t\tcontinue\n\t\t}\n\t\t// we haven't, send it now\n\t\tsem <- struct{}{}\n\t\twg.Add(1)\n\t\tgo func(c network.Conn) {\n\t\t\tdefer wg.Done()\n\t\t\tdefer func() { <-sem }()\n\t\t\tctx, cancel := context.WithTimeout(ctx, 5*time.Second)\n\t\t\tdefer cancel()\n\t\t\tstr, err := ids.Host.NewStream(ctx, c.RemotePeer(), IDPush)\n\t\t\tif err != nil { // connection might have been closed recently\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// TODO: find out if the peer supports push if we didn't have any information about push support\n\t\t\tif err := ids.sendIdentifyResp(str, true); err != nil {\n\t\t\t\tlog.Debugw(\"failed to send identify push\", \"peer\", c.RemotePeer(), \"error\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}(c)\n\t}\n\twg.Wait()\n}\n\n// Close shuts down the idService\nfunc (ids *idService) Close() error {\n\tids.ctxCancel()\n\tids.observedAddrs.Close()\n\tids.refCount.Wait()\n\treturn nil\n}\n\nfunc (ids *idService) OwnObservedAddrs() []ma.Multiaddr {\n\treturn ids.observedAddrs.Addrs()\n}\n\nfunc (ids *idService) ObservedAddrsFor(local ma.Multiaddr) []ma.Multiaddr {\n\treturn ids.observedAddrs.AddrsFor(local)\n}\n\n// IdentifyConn runs the Identify protocol on a connection.\n// It returns when we've received the peer's Identify message (or the request fails).\n// If successful, the peer store will contain the peer's addresses and supported protocols.\nfunc (ids *idService) IdentifyConn(c network.Conn) {\n\t<-ids.IdentifyWait(c)\n}\n\n// IdentifyWait runs the Identify protocol on a connection.\n// It doesn't block and returns a channel that is closed when we receive\n// the peer's Identify message (or the request fails).\n// If successful, the peer store will contain the peer's addresses and supported protocols.\nfunc (ids *idService) IdentifyWait(c network.Conn) <-chan struct{} {\n\tids.connsMu.Lock()\n\tdefer ids.connsMu.Unlock()\n\n\te, found := ids.conns[c]\n\tif !found {\n\t\t// No entry found. We may have gotten an out of order notification. Check it we should have this conn (because we're still connected)\n\t\t// We hold the ids.connsMu lock so this is safe since a disconnect event will be processed later if we are connected.\n\t\tif c.IsClosed() {\n\t\t\tlog.Debugw(\"connection not found in identify service\", \"peer\", c.RemotePeer())\n\t\t\tch := make(chan struct{})\n\t\t\tclose(ch)\n\t\t\treturn ch\n\t\t} else {\n\t\t\tids.addConnWithLock(c)\n\t\t}\n\t}\n\n\tif e.IdentifyWaitChan != nil {\n\t\treturn e.IdentifyWaitChan\n\t}\n\t// First call to IdentifyWait for this connection. Create the channel.\n\te.IdentifyWaitChan = make(chan struct{})\n\tids.conns[c] = e\n\n\t// Spawn an identify. The connection may actually be closed\n\t// already, but that doesn't really matter. We'll fail to open a\n\t// stream then forget the connection.\n\tgo func() {\n\t\tdefer close(e.IdentifyWaitChan)\n\t\tif err := ids.identifyConn(c); err != nil {\n\t\t\tlog.Warnf(\"failed to identify %s: %s\", c.RemotePeer(), err)\n\t\t\tids.emitters.evtPeerIdentificationFailed.Emit(event.EvtPeerIdentificationFailed{Peer: c.RemotePeer(), Reason: err})\n\t\t\treturn\n\t\t}\n\n\t\tids.emitters.evtPeerIdentificationCompleted.Emit(event.EvtPeerIdentificationCompleted{Peer: c.RemotePeer()})\n\t}()\n\n\treturn e.IdentifyWaitChan\n}\n\nfunc (ids *idService) identifyConn(c network.Conn) error {\n\ts, err := c.NewStream(network.WithUseTransient(context.TODO(), \"identify\"))\n\tif err != nil {\n\t\tlog.Debugw(\"error opening identify stream\", \"peer\", c.RemotePeer(), \"error\", err)\n\t\treturn err\n\t}\n\n\tif err := s.SetProtocol(ID); err != nil {\n\t\tlog.Warnf(\"error setting identify protocol for stream: %s\", err)\n\t\ts.Reset()\n\t}\n\n\t// ok give the response to our handler.\n\tif err := msmux.SelectProtoOrFail(ID, s); err != nil {\n\t\tlog.Infow(\"failed negotiate identify protocol with peer\", \"peer\", c.RemotePeer(), \"error\", err)\n\t\ts.Reset()\n\t\treturn err\n\t}\n\n\treturn ids.handleIdentifyResponse(s, false)\n}\n\n// handlePush handles incoming identify push streams\nfunc (ids *idService) handlePush(s network.Stream) {\n\tids.handleIdentifyResponse(s, true)\n}\n\nfunc (ids *idService) handleIdentifyRequest(s network.Stream) {\n\t_ = ids.sendIdentifyResp(s, false)\n}\n\nfunc (ids *idService) sendIdentifyResp(s network.Stream, isPush bool) error {\n\tif err := s.Scope().SetService(ServiceName); err != nil {\n\t\ts.Reset()\n\t\treturn fmt.Errorf(\"failed to attaching stream to identify service: %w\", err)\n\t}\n\tdefer s.Close()\n\n\tids.currentSnapshot.Lock()\n\tsnapshot := ids.currentSnapshot.snapshot\n\tids.currentSnapshot.Unlock()\n\n\tlog.Debugw(\"sending snapshot\", \"seq\", snapshot.seq, \"protocols\", snapshot.protocols, \"addrs\", snapshot.addrs)\n\n\tmes := ids.createBaseIdentifyResponse(s.Conn(), &snapshot)\n\tmes.SignedPeerRecord = ids.getSignedRecord(&snapshot)\n\n\tlog.Debugf(\"%s sending message to %s %s\", ID, s.Conn().RemotePeer(), s.Conn().RemoteMultiaddr())\n\tif err := ids.writeChunkedIdentifyMsg(s, mes); err != nil {\n\t\treturn err\n\t}\n\n\tif ids.metricsTracer != nil {\n\t\tids.metricsTracer.IdentifySent(isPush, len(mes.Protocols), len(mes.ListenAddrs))\n\t}\n\n\tids.connsMu.Lock()\n\tdefer ids.connsMu.Unlock()\n\te, ok := ids.conns[s.Conn()]\n\t// The connection might already have been closed.\n\t// We *should* receive the Connected notification from the swarm before we're able to accept the peer's\n\t// Identify stream, but if that for some reason doesn't work, we also wouldn't have a map entry here.\n\t// The only consequence would be that we send a spurious Push to that peer later.\n\tif !ok {\n\t\treturn nil\n\t}\n\te.Sequence = snapshot.seq\n\tids.conns[s.Conn()] = e\n\treturn nil\n}\n\nfunc (ids *idService) handleIdentifyResponse(s network.Stream, isPush bool) error {\n\tif err := s.Scope().SetService(ServiceName); err != nil {\n\t\tlog.Warnf(\"error attaching stream to identify service: %s\", err)\n\t\ts.Reset()\n\t\treturn err\n\t}\n\n\tif err := s.Scope().ReserveMemory(signedIDSize, network.ReservationPriorityAlways); err != nil {\n\t\tlog.Warnf(\"error reserving memory for identify stream: %s\", err)\n\t\ts.Reset()\n\t\treturn err\n\t}\n\tdefer s.Scope().ReleaseMemory(signedIDSize)\n\n\t_ = s.SetReadDeadline(time.Now().Add(StreamReadTimeout))\n\n\tc := s.Conn()\n\n\tr := pbio.NewDelimitedReader(s, signedIDSize)\n\tmes := &pb.Identify{}\n\n\tif err := readAllIDMessages(r, mes); err != nil {\n\t\tlog.Warn(\"error reading identify message: \", err)\n\t\ts.Reset()\n\t\treturn err\n\t}\n\n\tdefer s.Close()\n\n\tlog.Debugf(\"%s received message from %s %s\", s.Protocol(), c.RemotePeer(), c.RemoteMultiaddr())\n\n\tids.consumeMessage(mes, c, isPush)\n\n\tif ids.metricsTracer != nil {\n\t\tids.metricsTracer.IdentifyReceived(isPush, len(mes.Protocols), len(mes.ListenAddrs))\n\t}\n\n\tids.connsMu.Lock()\n\tdefer ids.connsMu.Unlock()\n\te, ok := ids.conns[c]\n\tif !ok { // might already have disconnected\n\t\treturn nil\n\t}\n\tsup, err := ids.Host.Peerstore().SupportsProtocols(c.RemotePeer(), IDPush)\n\tif supportsIdentifyPush := err == nil && len(sup) > 0; supportsIdentifyPush {\n\t\te.PushSupport = identifyPushSupported\n\t} else {\n\t\te.PushSupport = identifyPushUnsupported\n\t}\n\n\tif ids.metricsTracer != nil {\n\t\tids.metricsTracer.ConnPushSupport(e.PushSupport)\n\t}\n\n\tids.conns[c] = e\n\treturn nil\n}\n\nfunc readAllIDMessages(r pbio.Reader, finalMsg proto.Message) error {\n\tmes := &pb.Identify{}\n\tfor i := 0; i < maxMessages; i++ {\n\t\tswitch err := r.ReadMsg(mes); err {\n\t\tcase io.EOF:\n\t\t\treturn nil\n\t\tcase nil:\n\t\t\tproto.Merge(finalMsg, mes)\n\t\tdefault:\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn fmt.Errorf(\"too many parts\")\n}\n\nfunc (ids *idService) updateSnapshot() (updated bool) {\n\taddrs := ids.Host.Addrs()\n\tsort.Slice(addrs, func(i, j int) bool { return bytes.Compare(addrs[i].Bytes(), addrs[j].Bytes()) == -1 })\n\tprotos := ids.Host.Mux().Protocols()\n\tsort.Slice(protos, func(i, j int) bool { return protos[i] < protos[j] })\n\tsnapshot := identifySnapshot{\n\t\taddrs:     addrs,\n\t\tprotocols: protos,\n\t}\n\n\tif !ids.disableSignedPeerRecord {\n\t\tif cab, ok := peerstore.GetCertifiedAddrBook(ids.Host.Peerstore()); ok {\n\t\t\tsnapshot.record = cab.GetPeerRecord(ids.Host.ID())\n\t\t}\n\t}\n\n\tids.currentSnapshot.Lock()\n\tdefer ids.currentSnapshot.Unlock()\n\n\tif ids.currentSnapshot.snapshot.Equal(&snapshot) {\n\t\treturn false\n\t}\n\n\tsnapshot.seq = ids.currentSnapshot.snapshot.seq + 1\n\tids.currentSnapshot.snapshot = snapshot\n\n\tlog.Debugw(\"updating snapshot\", \"seq\", snapshot.seq, \"addrs\", snapshot.addrs)\n\treturn true\n}\n\nfunc (ids *idService) writeChunkedIdentifyMsg(s network.Stream, mes *pb.Identify) error {\n\twriter := pbio.NewDelimitedWriter(s)\n\n\tif mes.SignedPeerRecord == nil || proto.Size(mes) <= legacyIDSize {\n\t\treturn writer.WriteMsg(mes)\n\t}\n\n\tsr := mes.SignedPeerRecord\n\tmes.SignedPeerRecord = nil\n\tif err := writer.WriteMsg(mes); err != nil {\n\t\treturn err\n\t}\n\t// then write just the signed record\n\treturn writer.WriteMsg(&pb.Identify{SignedPeerRecord: sr})\n}\n\nfunc (ids *idService) createBaseIdentifyResponse(conn network.Conn, snapshot *identifySnapshot) *pb.Identify {\n\tmes := &pb.Identify{}\n\n\tremoteAddr := conn.RemoteMultiaddr()\n\tlocalAddr := conn.LocalMultiaddr()\n\n\t// set protocols this node is currently handling\n\tmes.Protocols = protocol.ConvertToStrings(snapshot.protocols)\n\n\t// observed address so other side is informed of their\n\t// \"public\" address, at least in relation to us.\n\tmes.ObservedAddr = remoteAddr.Bytes()\n\n\t// populate unsigned addresses.\n\t// peers that do not yet support signed addresses will need this.\n\t// Note: LocalMultiaddr is sometimes 0.0.0.0\n\tviaLoopback := manet.IsIPLoopback(localAddr) || manet.IsIPLoopback(remoteAddr)\n\tmes.ListenAddrs = make([][]byte, 0, len(snapshot.addrs))\n\tfor _, addr := range snapshot.addrs {\n\t\tif !viaLoopback && manet.IsIPLoopback(addr) {\n\t\t\tcontinue\n\t\t}\n\t\tmes.ListenAddrs = append(mes.ListenAddrs, addr.Bytes())\n\t}\n\t// set our public key\n\townKey := ids.Host.Peerstore().PubKey(ids.Host.ID())\n\n\t// check if we even have a public key.\n\tif ownKey == nil {\n\t\t// public key is nil. We are either using insecure transport or something erratic happened.\n\t\t// check if we're even operating in \"secure mode\"\n\t\tif ids.Host.Peerstore().PrivKey(ids.Host.ID()) != nil {\n\t\t\t// private key is present. But NO public key. Something bad happened.\n\t\t\tlog.Errorf(\"did not have own public key in Peerstore\")\n\t\t}\n\t\t// if neither of the key is present it is safe to assume that we are using an insecure transport.\n\t} else {\n\t\t// public key is present. Safe to proceed.\n\t\tif kb, err := crypto.MarshalPublicKey(ownKey); err != nil {\n\t\t\tlog.Errorf(\"failed to convert key to bytes\")\n\t\t} else {\n\t\t\tmes.PublicKey = kb\n\t\t}\n\t}\n\n\t// set protocol versions\n\tmes.ProtocolVersion = &ids.ProtocolVersion\n\tmes.AgentVersion = &ids.UserAgent\n\n\treturn mes\n}\n\nfunc (ids *idService) getSignedRecord(snapshot *identifySnapshot) []byte {\n\tif ids.disableSignedPeerRecord || snapshot.record == nil {\n\t\treturn nil\n\t}\n\n\trecBytes, err := snapshot.record.Marshal()\n\tif err != nil {\n\t\tlog.Errorw(\"failed to marshal signed record\", \"err\", err)\n\t\treturn nil\n\t}\n\n\treturn recBytes\n}\n\n// diff takes two slices of strings (a and b) and computes which elements were added and removed in b\nfunc diff(a, b []protocol.ID) (added, removed []protocol.ID) {\n\t// This is O(n^2), but it's fine because the slices are small.\n\tfor _, x := range b {\n\t\tvar found bool\n\t\tfor _, y := range a {\n\t\t\tif x == y {\n\t\t\t\tfound = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !found {\n\t\t\tadded = append(added, x)\n\t\t}\n\t}\n\tfor _, x := range a {\n\t\tvar found bool\n\t\tfor _, y := range b {\n\t\t\tif x == y {\n\t\t\t\tfound = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !found {\n\t\t\tremoved = append(removed, x)\n\t\t}\n\t}\n\treturn\n}\n\nfunc (ids *idService) consumeMessage(mes *pb.Identify, c network.Conn, isPush bool) {\n\tp := c.RemotePeer()\n\n\tsupported, _ := ids.Host.Peerstore().GetProtocols(p)\n\tmesProtocols := protocol.ConvertFromStrings(mes.Protocols)\n\tadded, removed := diff(supported, mesProtocols)\n\tids.Host.Peerstore().SetProtocols(p, mesProtocols...)\n\tif isPush {\n\t\tids.emitters.evtPeerProtocolsUpdated.Emit(event.EvtPeerProtocolsUpdated{\n\t\t\tPeer:    p,\n\t\t\tAdded:   added,\n\t\t\tRemoved: removed,\n\t\t})\n\t}\n\n\t// mes.ObservedAddr\n\tids.consumeObservedAddress(mes.GetObservedAddr(), c)\n\n\t// mes.ListenAddrs\n\tladdrs := mes.GetListenAddrs()\n\tlmaddrs := make([]ma.Multiaddr, 0, len(laddrs))\n\tfor _, addr := range laddrs {\n\t\tmaddr, err := ma.NewMultiaddrBytes(addr)\n\t\tif err != nil {\n\t\t\tlog.Debugf(\"%s failed to parse multiaddr from %s %s\", ID,\n\t\t\t\tp, c.RemoteMultiaddr())\n\t\t\tcontinue\n\t\t}\n\t\tlmaddrs = append(lmaddrs, maddr)\n\t}\n\n\t// NOTE: Do not add `c.RemoteMultiaddr()` to the peerstore if the remote\n\t// peer doesn't tell us to do so. Otherwise, we'll advertise it.\n\t//\n\t// This can cause an \"addr-splosion\" issue where the network will slowly\n\t// gossip and collect observed but unadvertised addresses. Given a NAT\n\t// that picks random source ports, this can cause DHT nodes to collect\n\t// many undialable addresses for other peers.\n\n\t// add certified addresses for the peer, if they sent us a signed peer record\n\t// otherwise use the unsigned addresses.\n\tsignedPeerRecord, err := signedPeerRecordFromMessage(mes)\n\tif err != nil {\n\t\tlog.Errorf(\"error getting peer record from Identify message: %v\", err)\n\t}\n\n\t// Extend the TTLs on the known (probably) good addresses.\n\t// Taking the lock ensures that we don't concurrently process a disconnect.\n\tids.addrMu.Lock()\n\tttl := peerstore.RecentlyConnectedAddrTTL\n\tif ids.Host.Network().Connectedness(p) == network.Connected {\n\t\tttl = peerstore.ConnectedAddrTTL\n\t}\n\n\t// Downgrade connected and recently connected addrs to a temporary TTL.\n\tfor _, ttl := range []time.Duration{\n\t\tpeerstore.RecentlyConnectedAddrTTL,\n\t\tpeerstore.ConnectedAddrTTL,\n\t} {\n\t\tids.Host.Peerstore().UpdateAddrs(p, ttl, peerstore.TempAddrTTL)\n\t}\n\n\t// add signed addrs if we have them and the peerstore supports them\n\tcab, ok := peerstore.GetCertifiedAddrBook(ids.Host.Peerstore())\n\tif ok && signedPeerRecord != nil {\n\t\t_, addErr := cab.ConsumePeerRecord(signedPeerRecord, ttl)\n\t\tif addErr != nil {\n\t\t\tlog.Debugf(\"error adding signed addrs to peerstore: %v\", addErr)\n\t\t}\n\t} else {\n\t\tids.Host.Peerstore().AddAddrs(p, lmaddrs, ttl)\n\t}\n\n\t// Finally, expire all temporary addrs.\n\tids.Host.Peerstore().UpdateAddrs(p, peerstore.TempAddrTTL, 0)\n\tids.addrMu.Unlock()\n\n\tlog.Debugf(\"%s received listen addrs for %s: %s\", c.LocalPeer(), c.RemotePeer(), lmaddrs)\n\n\t// get protocol versions\n\tpv := mes.GetProtocolVersion()\n\tav := mes.GetAgentVersion()\n\n\tids.Host.Peerstore().Put(p, \"ProtocolVersion\", pv)\n\tids.Host.Peerstore().Put(p, \"AgentVersion\", av)\n\n\t// get the key from the other side. we may not have it (no-auth transport)\n\tids.consumeReceivedPubKey(c, mes.PublicKey)\n}\n\nfunc (ids *idService) consumeReceivedPubKey(c network.Conn, kb []byte) {\n\tlp := c.LocalPeer()\n\trp := c.RemotePeer()\n\n\tif kb == nil {\n\t\tlog.Debugf(\"%s did not receive public key for remote peer: %s\", lp, rp)\n\t\treturn\n\t}\n\n\tnewKey, err := crypto.UnmarshalPublicKey(kb)\n\tif err != nil {\n\t\tlog.Warnf(\"%s cannot unmarshal key from remote peer: %s, %s\", lp, rp, err)\n\t\treturn\n\t}\n\n\t// verify key matches peer.ID\n\tnp, err := peer.IDFromPublicKey(newKey)\n\tif err != nil {\n\t\tlog.Debugf(\"%s cannot get peer.ID from key of remote peer: %s, %s\", lp, rp, err)\n\t\treturn\n\t}\n\n\tif np != rp {\n\t\t// if the newKey's peer.ID does not match known peer.ID...\n\n\t\tif rp == \"\" && np != \"\" {\n\t\t\t// if local peerid is empty, then use the new, sent key.\n\t\t\terr := ids.Host.Peerstore().AddPubKey(rp, newKey)\n\t\t\tif err != nil {\n\t\t\t\tlog.Debugf(\"%s could not add key for %s to peerstore: %s\", lp, rp, err)\n\t\t\t}\n\n\t\t} else {\n\t\t\t// we have a local peer.ID and it does not match the sent key... error.\n\t\t\tlog.Errorf(\"%s received key for remote peer %s mismatch: %s\", lp, rp, np)\n\t\t}\n\t\treturn\n\t}\n\n\tcurrKey := ids.Host.Peerstore().PubKey(rp)\n\tif currKey == nil {\n\t\t// no key? no auth transport. set this one.\n\t\terr := ids.Host.Peerstore().AddPubKey(rp, newKey)\n\t\tif err != nil {\n\t\t\tlog.Debugf(\"%s could not add key for %s to peerstore: %s\", lp, rp, err)\n\t\t}\n\t\treturn\n\t}\n\n\t// ok, we have a local key, we should verify they match.\n\tif currKey.Equals(newKey) {\n\t\treturn // ok great. we're done.\n\t}\n\n\t// weird, got a different key... but the different key MATCHES the peer.ID.\n\t// this odd. let's log error and investigate. this should basically never happen\n\t// and it means we have something funky going on and possibly a bug.\n\tlog.Errorf(\"%s identify got a different key for: %s\", lp, rp)\n\n\t// okay... does ours NOT match the remote peer.ID?\n\tcp, err := peer.IDFromPublicKey(currKey)\n\tif err != nil {\n\t\tlog.Errorf(\"%s cannot get peer.ID from local key of remote peer: %s, %s\", lp, rp, err)\n\t\treturn\n\t}\n\tif cp != rp {\n\t\tlog.Errorf(\"%s local key for remote peer %s yields different peer.ID: %s\", lp, rp, cp)\n\t\treturn\n\t}\n\n\t// okay... curr key DOES NOT match new key. both match peer.ID. wat?\n\tlog.Errorf(\"%s local key and received key for %s do not match, but match peer.ID\", lp, rp)\n}\n\n// HasConsistentTransport returns true if the address 'a' shares a\n// protocol set with any address in the green set. This is used\n// to check if a given address might be one of the addresses a peer is\n// listening on.\nfunc HasConsistentTransport(a ma.Multiaddr, green []ma.Multiaddr) bool {\n\tprotosMatch := func(a, b []ma.Protocol) bool {\n\t\tif len(a) != len(b) {\n\t\t\treturn false\n\t\t}\n\n\t\tfor i, p := range a {\n\t\t\tif b[i].Code != p.Code {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\t}\n\n\tprotos := a.Protocols()\n\n\tfor _, ga := range green {\n\t\tif protosMatch(protos, ga.Protocols()) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\nfunc (ids *idService) consumeObservedAddress(observed []byte, c network.Conn) {\n\tif observed == nil {\n\t\treturn\n\t}\n\n\tmaddr, err := ma.NewMultiaddrBytes(observed)\n\tif err != nil {\n\t\tlog.Debugf(\"error parsing received observed addr for %s: %s\", c, err)\n\t\treturn\n\t}\n\n\tids.observedAddrs.Record(c, maddr)\n}\n\n// addConnWithLock assuems caller holds the connsMu lock\nfunc (ids *idService) addConnWithLock(c network.Conn) {\n\t_, found := ids.conns[c]\n\tif !found {\n\t\t<-ids.setupCompleted\n\t\tids.conns[c] = entry{}\n\t}\n}\n\nfunc signedPeerRecordFromMessage(msg *pb.Identify) (*record.Envelope, error) {\n\tif msg.SignedPeerRecord == nil || len(msg.SignedPeerRecord) == 0 {\n\t\treturn nil, nil\n\t}\n\tenv, _, err := record.ConsumeEnvelope(msg.SignedPeerRecord, peer.PeerRecordEnvelopeDomain)\n\treturn env, err\n}\n\n// netNotifiee defines methods to be used with the swarm\ntype netNotifiee idService\n\nfunc (nn *netNotifiee) IDService() *idService {\n\treturn (*idService)(nn)\n}\n\nfunc (nn *netNotifiee) Connected(_ network.Network, c network.Conn) {\n\tids := nn.IDService()\n\n\tids.connsMu.Lock()\n\tids.addConnWithLock(c)\n\tids.connsMu.Unlock()\n\n\tnn.IDService().IdentifyWait(c)\n}\n\nfunc (nn *netNotifiee) Disconnected(_ network.Network, c network.Conn) {\n\tids := nn.IDService()\n\n\t// Stop tracking the connection.\n\tids.connsMu.Lock()\n\tdelete(ids.conns, c)\n\tids.connsMu.Unlock()\n\n\tif ids.Host.Network().Connectedness(c.RemotePeer()) != network.Connected {\n\t\t// Last disconnect.\n\t\t// Undo the setting of addresses to peer.ConnectedAddrTTL we did\n\t\tids.addrMu.Lock()\n\t\tdefer ids.addrMu.Unlock()\n\t\tids.Host.Peerstore().UpdateAddrs(c.RemotePeer(), peerstore.ConnectedAddrTTL, peerstore.RecentlyConnectedAddrTTL)\n\t}\n}\n\nfunc (nn *netNotifiee) Listen(n network.Network, a ma.Multiaddr)      {}\nfunc (nn *netNotifiee) ListenClose(n network.Network, a ma.Multiaddr) {}\n", "package identify\n\nimport (\n\t\"context\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/libp2p/go-libp2p/core/network\"\n\t\"github.com/libp2p/go-libp2p/core/peer\"\n\tblhost \"github.com/libp2p/go-libp2p/p2p/host/blank\"\n\tswarmt \"github.com/libp2p/go-libp2p/p2p/net/swarm/testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestFastDisconnect(t *testing.T) {\n\t// This test checks to see if we correctly abort sending an identify\n\t// response if the peer disconnects before we handle the request.\n\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n\tdefer cancel()\n\n\ttarget := blhost.NewBlankHost(swarmt.GenSwarm(t))\n\tdefer target.Close()\n\tids, err := NewIDService(target)\n\trequire.NoError(t, err)\n\tdefer ids.Close()\n\tids.Start()\n\n\tsync := make(chan struct{})\n\ttarget.SetStreamHandler(ID, func(s network.Stream) {\n\t\t// Wait till the stream is set up on both sides.\n\t\tselect {\n\t\tcase <-sync:\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\t}\n\n\t\t// Kill the connection, and make sure we're completely disconnected.\n\t\tassert.Eventually(t,\n\t\t\tfunc() bool {\n\t\t\t\tfor _, conn := range target.Network().ConnsToPeer(s.Conn().RemotePeer()) {\n\t\t\t\t\tconn.Close()\n\t\t\t\t}\n\t\t\t\treturn target.Network().Connectedness(s.Conn().RemotePeer()) != network.Connected\n\t\t\t},\n\t\t\t2*time.Second,\n\t\t\ttime.Millisecond,\n\t\t)\n\t\t// Now try to handle the response.\n\t\t// This should not block indefinitely, or panic, or anything like that.\n\t\t//\n\t\t// However, if we have a bug, that _could_ happen.\n\t\tids.handleIdentifyRequest(s)\n\n\t\t// Ok, allow the outer test to continue.\n\t\tselect {\n\t\tcase <-sync:\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\t}\n\t})\n\n\tsource := blhost.NewBlankHost(swarmt.GenSwarm(t))\n\tdefer source.Close()\n\n\t// only connect to the first address, to make sure we only end up with one connection\n\trequire.NoError(t, source.Connect(ctx, peer.AddrInfo{ID: target.ID(), Addrs: target.Addrs()}))\n\ts, err := source.NewStream(ctx, target.ID(), ID)\n\trequire.NoError(t, err)\n\tselect {\n\tcase sync <- struct{}{}:\n\tcase <-ctx.Done():\n\t\tt.Fatal(ctx.Err())\n\t}\n\ts.Reset()\n\tselect {\n\tcase sync <- struct{}{}:\n\tcase <-ctx.Done():\n\t\tt.Fatal(ctx.Err())\n\t}\n\t// double-check to make sure we didn't actually timeout somewhere.\n\trequire.NoError(t, ctx.Err())\n}\n"], "fixing_code": ["package record\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"fmt\"\n\t\"sync\"\n\n\t\"github.com/libp2p/go-libp2p/core/crypto\"\n\t\"github.com/libp2p/go-libp2p/core/internal/catch\"\n\t\"github.com/libp2p/go-libp2p/core/record/pb\"\n\n\tpool \"github.com/libp2p/go-buffer-pool\"\n\n\t\"github.com/multiformats/go-varint\"\n\t\"google.golang.org/protobuf/proto\"\n)\n\n//go:generate protoc --proto_path=$PWD:$PWD/../.. --go_out=. --go_opt=Mpb/envelope.proto=./pb pb/envelope.proto\n\n// Envelope contains an arbitrary []byte payload, signed by a libp2p peer.\n//\n// Envelopes are signed in the context of a particular \"domain\", which is a\n// string specified when creating and verifying the envelope. You must know the\n// domain string used to produce the envelope in order to verify the signature\n// and access the payload.\ntype Envelope struct {\n\t// The public key that can be used to verify the signature and derive the peer id of the signer.\n\tPublicKey crypto.PubKey\n\n\t// A binary identifier that indicates what kind of data is contained in the payload.\n\t// TODO(yusef): enforce multicodec prefix\n\tPayloadType []byte\n\n\t// The envelope payload.\n\tRawPayload []byte\n\n\t// The signature of the domain string :: type hint :: payload.\n\tsignature []byte\n\n\t// the unmarshalled payload as a Record, cached on first access via the Record accessor method\n\tcached         Record\n\tunmarshalError error\n\tunmarshalOnce  sync.Once\n}\n\nvar ErrEmptyDomain = errors.New(\"envelope domain must not be empty\")\nvar ErrEmptyPayloadType = errors.New(\"payloadType must not be empty\")\nvar ErrInvalidSignature = errors.New(\"invalid signature or incorrect domain\")\n\n// Seal marshals the given Record, places the marshaled bytes inside an Envelope,\n// and signs with the given private key.\nfunc Seal(rec Record, privateKey crypto.PrivKey) (*Envelope, error) {\n\tpayload, err := rec.MarshalRecord()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error marshaling record: %v\", err)\n\t}\n\n\tdomain := rec.Domain()\n\tpayloadType := rec.Codec()\n\tif domain == \"\" {\n\t\treturn nil, ErrEmptyDomain\n\t}\n\n\tif len(payloadType) == 0 {\n\t\treturn nil, ErrEmptyPayloadType\n\t}\n\n\tunsigned, err := makeUnsigned(domain, payloadType, payload)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer pool.Put(unsigned)\n\n\tsig, err := privateKey.Sign(unsigned)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &Envelope{\n\t\tPublicKey:   privateKey.GetPublic(),\n\t\tPayloadType: payloadType,\n\t\tRawPayload:  payload,\n\t\tsignature:   sig,\n\t}, nil\n}\n\n// ConsumeEnvelope unmarshals a serialized Envelope and validates its\n// signature using the provided 'domain' string. If validation fails, an error\n// is returned, along with the unmarshalled envelope so it can be inspected.\n//\n// On success, ConsumeEnvelope returns the Envelope itself, as well as the inner payload,\n// unmarshalled into a concrete Record type. The actual type of the returned Record depends\n// on what has been registered for the Envelope's PayloadType (see RegisterType for details).\n//\n// You can type assert on the returned Record to convert it to an instance of the concrete\n// Record type:\n//\n//\tenvelope, rec, err := ConsumeEnvelope(envelopeBytes, peer.PeerRecordEnvelopeDomain)\n//\tif err != nil {\n//\t  handleError(envelope, err)  // envelope may be non-nil, even if errors occur!\n//\t  return\n//\t}\n//\tpeerRec, ok := rec.(*peer.PeerRecord)\n//\tif ok {\n//\t  doSomethingWithPeerRecord(peerRec)\n//\t}\n//\n// If the Envelope signature is valid, but no Record type is registered for the Envelope's\n// PayloadType, ErrPayloadTypeNotRegistered will be returned, along with the Envelope and\n// a nil Record.\nfunc ConsumeEnvelope(data []byte, domain string) (envelope *Envelope, rec Record, err error) {\n\te, err := UnmarshalEnvelope(data)\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"failed when unmarshalling the envelope: %w\", err)\n\t}\n\n\terr = e.validate(domain)\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"failed to validate envelope: %w\", err)\n\t}\n\n\trec, err = e.Record()\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"failed to unmarshal envelope payload: %w\", err)\n\t}\n\treturn e, rec, nil\n}\n\n// ConsumeTypedEnvelope unmarshals a serialized Envelope and validates its\n// signature. If validation fails, an error is returned, along with the unmarshalled\n// envelope so it can be inspected.\n//\n// Unlike ConsumeEnvelope, ConsumeTypedEnvelope does not try to automatically determine\n// the type of Record to unmarshal the Envelope's payload into. Instead, the caller provides\n// a destination Record instance, which will unmarshal the Envelope payload. It is the caller's\n// responsibility to determine whether the given Record type is able to unmarshal the payload\n// correctly.\n//\n//\trec := &MyRecordType{}\n//\tenvelope, err := ConsumeTypedEnvelope(envelopeBytes, rec)\n//\tif err != nil {\n//\t  handleError(envelope, err)\n//\t}\n//\tdoSomethingWithRecord(rec)\n//\n// Important: you MUST check the error value before using the returned Envelope. In some error\n// cases, including when the envelope signature is invalid, both the Envelope and an error will\n// be returned. This allows you to inspect the unmarshalled but invalid Envelope. As a result,\n// you must not assume that any non-nil Envelope returned from this function is valid.\nfunc ConsumeTypedEnvelope(data []byte, destRecord Record) (envelope *Envelope, err error) {\n\te, err := UnmarshalEnvelope(data)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed when unmarshalling the envelope: %w\", err)\n\t}\n\n\terr = e.validate(destRecord.Domain())\n\tif err != nil {\n\t\treturn e, fmt.Errorf(\"failed to validate envelope: %w\", err)\n\t}\n\n\terr = destRecord.UnmarshalRecord(e.RawPayload)\n\tif err != nil {\n\t\treturn e, fmt.Errorf(\"failed to unmarshal envelope payload: %w\", err)\n\t}\n\te.cached = destRecord\n\treturn e, nil\n}\n\n// UnmarshalEnvelope unmarshals a serialized Envelope protobuf message,\n// without validating its contents. Most users should use ConsumeEnvelope.\nfunc UnmarshalEnvelope(data []byte) (*Envelope, error) {\n\tvar e pb.Envelope\n\tif err := proto.Unmarshal(data, &e); err != nil {\n\t\treturn nil, err\n\t}\n\n\tkey, err := crypto.PublicKeyFromProto(e.PublicKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &Envelope{\n\t\tPublicKey:   key,\n\t\tPayloadType: e.PayloadType,\n\t\tRawPayload:  e.Payload,\n\t\tsignature:   e.Signature,\n\t}, nil\n}\n\n// Marshal returns a byte slice containing a serialized protobuf representation\n// of a Envelope.\nfunc (e *Envelope) Marshal() (res []byte, err error) {\n\tdefer func() { catch.HandlePanic(recover(), &err, \"libp2p envelope marshal\") }()\n\tkey, err := crypto.PublicKeyToProto(e.PublicKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tmsg := pb.Envelope{\n\t\tPublicKey:   key,\n\t\tPayloadType: e.PayloadType,\n\t\tPayload:     e.RawPayload,\n\t\tSignature:   e.signature,\n\t}\n\treturn proto.Marshal(&msg)\n}\n\n// Equal returns true if the other Envelope has the same public key,\n// payload, payload type, and signature. This implies that they were also\n// created with the same domain string.\nfunc (e *Envelope) Equal(other *Envelope) bool {\n\tif other == nil {\n\t\treturn e == nil\n\t}\n\treturn e.PublicKey.Equals(other.PublicKey) &&\n\t\tbytes.Equal(e.PayloadType, other.PayloadType) &&\n\t\tbytes.Equal(e.signature, other.signature) &&\n\t\tbytes.Equal(e.RawPayload, other.RawPayload)\n}\n\n// Record returns the Envelope's payload unmarshalled as a Record.\n// The concrete type of the returned Record depends on which Record\n// type was registered for the Envelope's PayloadType - see record.RegisterType.\n//\n// Once unmarshalled, the Record is cached for future access.\nfunc (e *Envelope) Record() (Record, error) {\n\te.unmarshalOnce.Do(func() {\n\t\tif e.cached != nil {\n\t\t\treturn\n\t\t}\n\t\te.cached, e.unmarshalError = unmarshalRecordPayload(e.PayloadType, e.RawPayload)\n\t})\n\treturn e.cached, e.unmarshalError\n}\n\n// TypedRecord unmarshals the Envelope's payload to the given Record instance.\n// It is the caller's responsibility to ensure that the Record type is capable\n// of unmarshalling the Envelope payload. Callers can inspect the Envelope's\n// PayloadType field to determine the correct type of Record to use.\n//\n// This method will always unmarshal the Envelope payload even if a cached record\n// exists.\nfunc (e *Envelope) TypedRecord(dest Record) error {\n\treturn dest.UnmarshalRecord(e.RawPayload)\n}\n\n// validate returns nil if the envelope signature is valid for the given 'domain',\n// or an error if signature validation fails.\nfunc (e *Envelope) validate(domain string) error {\n\tunsigned, err := makeUnsigned(domain, e.PayloadType, e.RawPayload)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer pool.Put(unsigned)\n\n\tvalid, err := e.PublicKey.Verify(unsigned, e.signature)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed while verifying signature: %w\", err)\n\t}\n\tif !valid {\n\t\treturn ErrInvalidSignature\n\t}\n\treturn nil\n}\n\n// makeUnsigned is a helper function that prepares a buffer to sign or verify.\n// It returns a byte slice from a pool. The caller MUST return this slice to the\n// pool.\nfunc makeUnsigned(domain string, payloadType []byte, payload []byte) ([]byte, error) {\n\tvar (\n\t\tfields = [][]byte{[]byte(domain), payloadType, payload}\n\n\t\t// fields are prefixed with their length as an unsigned varint. we\n\t\t// compute the lengths before allocating the sig buffer so we know how\n\t\t// much space to add for the lengths\n\t\tflen = make([][]byte, len(fields))\n\t\tsize = 0\n\t)\n\n\tfor i, f := range fields {\n\t\tl := len(f)\n\t\tflen[i] = varint.ToUvarint(uint64(l))\n\t\tsize += l + len(flen[i])\n\t}\n\n\tb := pool.Get(size)\n\n\tvar s int\n\tfor i, f := range fields {\n\t\ts += copy(b[s:], flen[i])\n\t\ts += copy(b[s:], f)\n\t}\n\n\treturn b[:s], nil\n}\n", "package identify\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/exp/slices\"\n\n\t\"github.com/libp2p/go-libp2p/core/crypto\"\n\t\"github.com/libp2p/go-libp2p/core/event\"\n\t\"github.com/libp2p/go-libp2p/core/host\"\n\t\"github.com/libp2p/go-libp2p/core/network\"\n\t\"github.com/libp2p/go-libp2p/core/peer\"\n\t\"github.com/libp2p/go-libp2p/core/peerstore\"\n\t\"github.com/libp2p/go-libp2p/core/protocol\"\n\t\"github.com/libp2p/go-libp2p/core/record\"\n\t\"github.com/libp2p/go-libp2p/p2p/host/eventbus\"\n\t\"github.com/libp2p/go-libp2p/p2p/protocol/identify/pb\"\n\n\tlogging \"github.com/ipfs/go-log/v2\"\n\t\"github.com/libp2p/go-msgio/pbio\"\n\tma \"github.com/multiformats/go-multiaddr\"\n\tmanet \"github.com/multiformats/go-multiaddr/net\"\n\tmsmux \"github.com/multiformats/go-multistream\"\n\t\"google.golang.org/protobuf/proto\"\n)\n\n//go:generate protoc --proto_path=$PWD:$PWD/../../.. --go_out=. --go_opt=Mpb/identify.proto=./pb pb/identify.proto\n\nvar log = logging.Logger(\"net/identify\")\n\nconst (\n\t// ID is the protocol.ID of version 1.0.0 of the identify service.\n\tID = \"/ipfs/id/1.0.0\"\n\t// IDPush is the protocol.ID of the Identify push protocol.\n\t// It sends full identify messages containing the current state of the peer.\n\tIDPush = \"/ipfs/id/push/1.0.0\"\n)\n\nconst DefaultProtocolVersion = \"ipfs/0.1.0\"\n\nconst ServiceName = \"libp2p.identify\"\n\nconst maxPushConcurrency = 32\n\n// StreamReadTimeout is the read timeout on all incoming Identify family streams.\nvar StreamReadTimeout = 60 * time.Second\n\nconst (\n\tlegacyIDSize = 2 * 1024 // 2k Bytes\n\tsignedIDSize = 8 * 1024 // 8K\n\tmaxMessages  = 10\n)\n\nvar defaultUserAgent = \"github.com/libp2p/go-libp2p\"\n\ntype identifySnapshot struct {\n\tseq       uint64\n\tprotocols []protocol.ID\n\taddrs     []ma.Multiaddr\n\trecord    *record.Envelope\n}\n\n// Equal says if two snapshots are identical.\n// It does NOT compare the sequence number.\nfunc (s identifySnapshot) Equal(other *identifySnapshot) bool {\n\thasRecord := s.record != nil\n\totherHasRecord := other.record != nil\n\tif hasRecord != otherHasRecord {\n\t\treturn false\n\t}\n\tif hasRecord && !s.record.Equal(other.record) {\n\t\treturn false\n\t}\n\tif !slices.Equal(s.protocols, other.protocols) {\n\t\treturn false\n\t}\n\tif len(s.addrs) != len(other.addrs) {\n\t\treturn false\n\t}\n\tfor i, a := range s.addrs {\n\t\tif !a.Equal(other.addrs[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\ntype IDService interface {\n\t// IdentifyConn synchronously triggers an identify request on the connection and\n\t// waits for it to complete. If the connection is being identified by another\n\t// caller, this call will wait. If the connection has already been identified,\n\t// it will return immediately.\n\tIdentifyConn(network.Conn)\n\t// IdentifyWait triggers an identify (if the connection has not already been\n\t// identified) and returns a channel that is closed when the identify protocol\n\t// completes.\n\tIdentifyWait(network.Conn) <-chan struct{}\n\t// OwnObservedAddrs returns the addresses peers have reported we've dialed from\n\tOwnObservedAddrs() []ma.Multiaddr\n\t// ObservedAddrsFor returns the addresses peers have reported we've dialed from,\n\t// for a specific local address.\n\tObservedAddrsFor(local ma.Multiaddr) []ma.Multiaddr\n\tStart()\n\tio.Closer\n}\n\ntype identifyPushSupport uint8\n\nconst (\n\tidentifyPushSupportUnknown identifyPushSupport = iota\n\tidentifyPushSupported\n\tidentifyPushUnsupported\n)\n\ntype entry struct {\n\t// The IdentifyWaitChan is created when IdentifyWait is called for the first time.\n\t// IdentifyWait closes this channel when the Identify request completes, or when it fails.\n\tIdentifyWaitChan chan struct{}\n\n\t// PushSupport saves our knowledge about the peer's support of the Identify Push protocol.\n\t// Before the identify request returns, we don't know yet if the peer supports Identify Push.\n\tPushSupport identifyPushSupport\n\t// Sequence is the sequence number of the last snapshot we sent to this peer.\n\tSequence uint64\n}\n\n// idService is a structure that implements ProtocolIdentify.\n// It is a trivial service that gives the other peer some\n// useful information about the local peer. A sort of hello.\n//\n// The idService sends:\n//   - Our libp2p Protocol Version\n//   - Our libp2p Agent Version\n//   - Our public Listen Addresses\ntype idService struct {\n\tHost            host.Host\n\tUserAgent       string\n\tProtocolVersion string\n\n\tmetricsTracer MetricsTracer\n\n\tsetupCompleted chan struct{} // is closed when Start has finished setting up\n\tctx            context.Context\n\tctxCancel      context.CancelFunc\n\t// track resources that need to be shut down before we shut down\n\trefCount sync.WaitGroup\n\n\tdisableSignedPeerRecord bool\n\n\tconnsMu sync.RWMutex\n\t// The conns map contains all connections we're currently handling.\n\t// Connections are inserted as soon as they're available in the swarm\n\t// Connections are removed from the map when the connection disconnects.\n\tconns map[network.Conn]entry\n\n\taddrMu sync.Mutex\n\n\t// our own observed addresses.\n\tobservedAddrs *ObservedAddrManager\n\n\temitters struct {\n\t\tevtPeerProtocolsUpdated        event.Emitter\n\t\tevtPeerIdentificationCompleted event.Emitter\n\t\tevtPeerIdentificationFailed    event.Emitter\n\t}\n\n\tcurrentSnapshot struct {\n\t\tsync.Mutex\n\t\tsnapshot identifySnapshot\n\t}\n}\n\n// NewIDService constructs a new *idService and activates it by\n// attaching its stream handler to the given host.Host.\nfunc NewIDService(h host.Host, opts ...Option) (*idService, error) {\n\tvar cfg config\n\tfor _, opt := range opts {\n\t\topt(&cfg)\n\t}\n\n\tuserAgent := defaultUserAgent\n\tif cfg.userAgent != \"\" {\n\t\tuserAgent = cfg.userAgent\n\t}\n\n\tprotocolVersion := DefaultProtocolVersion\n\tif cfg.protocolVersion != \"\" {\n\t\tprotocolVersion = cfg.protocolVersion\n\t}\n\n\tctx, cancel := context.WithCancel(context.Background())\n\ts := &idService{\n\t\tHost:                    h,\n\t\tUserAgent:               userAgent,\n\t\tProtocolVersion:         protocolVersion,\n\t\tctx:                     ctx,\n\t\tctxCancel:               cancel,\n\t\tconns:                   make(map[network.Conn]entry),\n\t\tdisableSignedPeerRecord: cfg.disableSignedPeerRecord,\n\t\tsetupCompleted:          make(chan struct{}),\n\t\tmetricsTracer:           cfg.metricsTracer,\n\t}\n\n\tobservedAddrs, err := NewObservedAddrManager(h)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create observed address manager: %s\", err)\n\t}\n\ts.observedAddrs = observedAddrs\n\n\ts.emitters.evtPeerProtocolsUpdated, err = h.EventBus().Emitter(&event.EvtPeerProtocolsUpdated{})\n\tif err != nil {\n\t\tlog.Warnf(\"identify service not emitting peer protocol updates; err: %s\", err)\n\t}\n\ts.emitters.evtPeerIdentificationCompleted, err = h.EventBus().Emitter(&event.EvtPeerIdentificationCompleted{})\n\tif err != nil {\n\t\tlog.Warnf(\"identify service not emitting identification completed events; err: %s\", err)\n\t}\n\ts.emitters.evtPeerIdentificationFailed, err = h.EventBus().Emitter(&event.EvtPeerIdentificationFailed{})\n\tif err != nil {\n\t\tlog.Warnf(\"identify service not emitting identification failed events; err: %s\", err)\n\t}\n\treturn s, nil\n}\n\nfunc (ids *idService) Start() {\n\tids.Host.Network().Notify((*netNotifiee)(ids))\n\tids.Host.SetStreamHandler(ID, ids.handleIdentifyRequest)\n\tids.Host.SetStreamHandler(IDPush, ids.handlePush)\n\tids.updateSnapshot()\n\tclose(ids.setupCompleted)\n\n\tids.refCount.Add(1)\n\tgo ids.loop(ids.ctx)\n}\n\nfunc (ids *idService) loop(ctx context.Context) {\n\tdefer ids.refCount.Done()\n\n\tsub, err := ids.Host.EventBus().Subscribe(\n\t\t[]any{&event.EvtLocalProtocolsUpdated{}, &event.EvtLocalAddressesUpdated{}},\n\t\teventbus.BufSize(256),\n\t\teventbus.Name(\"identify (loop)\"),\n\t)\n\tif err != nil {\n\t\tlog.Errorf(\"failed to subscribe to events on the bus, err=%s\", err)\n\t\treturn\n\t}\n\tdefer sub.Close()\n\n\t// Send pushes from a separate Go routine.\n\t// That way, we can end up with\n\t// * this Go routine busy looping over all peers in sendPushes\n\t// * another push being queued in the triggerPush channel\n\ttriggerPush := make(chan struct{}, 1)\n\tids.refCount.Add(1)\n\tgo func() {\n\t\tdefer ids.refCount.Done()\n\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\tcase <-triggerPush:\n\t\t\t\tids.sendPushes(ctx)\n\t\t\t}\n\t\t}\n\t}()\n\n\tfor {\n\t\tselect {\n\t\tcase e, ok := <-sub.Out():\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif updated := ids.updateSnapshot(); !updated {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif ids.metricsTracer != nil {\n\t\t\t\tids.metricsTracer.TriggeredPushes(e)\n\t\t\t}\n\t\t\tselect {\n\t\t\tcase triggerPush <- struct{}{}:\n\t\t\tdefault: // we already have one more push queued, no need to queue another one\n\t\t\t}\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (ids *idService) sendPushes(ctx context.Context) {\n\tids.connsMu.RLock()\n\tconns := make([]network.Conn, 0, len(ids.conns))\n\tfor c, e := range ids.conns {\n\t\t// Push even if we don't know if push is supported.\n\t\t// This will be only the case while the IdentifyWaitChan call is in flight.\n\t\tif e.PushSupport == identifyPushSupported || e.PushSupport == identifyPushSupportUnknown {\n\t\t\tconns = append(conns, c)\n\t\t}\n\t}\n\tids.connsMu.RUnlock()\n\n\tsem := make(chan struct{}, maxPushConcurrency)\n\tvar wg sync.WaitGroup\n\tfor _, c := range conns {\n\t\t// check if the connection is still alive\n\t\tids.connsMu.RLock()\n\t\te, ok := ids.conns[c]\n\t\tids.connsMu.RUnlock()\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\t// check if we already sent the current snapshot to this peer\n\t\tids.currentSnapshot.Lock()\n\t\tsnapshot := ids.currentSnapshot.snapshot\n\t\tids.currentSnapshot.Unlock()\n\t\tif e.Sequence >= snapshot.seq {\n\t\t\tlog.Debugw(\"already sent this snapshot to peer\", \"peer\", c.RemotePeer(), \"seq\", snapshot.seq)\n\t\t\tcontinue\n\t\t}\n\t\t// we haven't, send it now\n\t\tsem <- struct{}{}\n\t\twg.Add(1)\n\t\tgo func(c network.Conn) {\n\t\t\tdefer wg.Done()\n\t\t\tdefer func() { <-sem }()\n\t\t\tctx, cancel := context.WithTimeout(ctx, 5*time.Second)\n\t\t\tdefer cancel()\n\t\t\tstr, err := ids.Host.NewStream(ctx, c.RemotePeer(), IDPush)\n\t\t\tif err != nil { // connection might have been closed recently\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// TODO: find out if the peer supports push if we didn't have any information about push support\n\t\t\tif err := ids.sendIdentifyResp(str, true); err != nil {\n\t\t\t\tlog.Debugw(\"failed to send identify push\", \"peer\", c.RemotePeer(), \"error\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}(c)\n\t}\n\twg.Wait()\n}\n\n// Close shuts down the idService\nfunc (ids *idService) Close() error {\n\tids.ctxCancel()\n\tids.observedAddrs.Close()\n\tids.refCount.Wait()\n\treturn nil\n}\n\nfunc (ids *idService) OwnObservedAddrs() []ma.Multiaddr {\n\treturn ids.observedAddrs.Addrs()\n}\n\nfunc (ids *idService) ObservedAddrsFor(local ma.Multiaddr) []ma.Multiaddr {\n\treturn ids.observedAddrs.AddrsFor(local)\n}\n\n// IdentifyConn runs the Identify protocol on a connection.\n// It returns when we've received the peer's Identify message (or the request fails).\n// If successful, the peer store will contain the peer's addresses and supported protocols.\nfunc (ids *idService) IdentifyConn(c network.Conn) {\n\t<-ids.IdentifyWait(c)\n}\n\n// IdentifyWait runs the Identify protocol on a connection.\n// It doesn't block and returns a channel that is closed when we receive\n// the peer's Identify message (or the request fails).\n// If successful, the peer store will contain the peer's addresses and supported protocols.\nfunc (ids *idService) IdentifyWait(c network.Conn) <-chan struct{} {\n\tids.connsMu.Lock()\n\tdefer ids.connsMu.Unlock()\n\n\te, found := ids.conns[c]\n\tif !found {\n\t\t// No entry found. We may have gotten an out of order notification. Check it we should have this conn (because we're still connected)\n\t\t// We hold the ids.connsMu lock so this is safe since a disconnect event will be processed later if we are connected.\n\t\tif c.IsClosed() {\n\t\t\tlog.Debugw(\"connection not found in identify service\", \"peer\", c.RemotePeer())\n\t\t\tch := make(chan struct{})\n\t\t\tclose(ch)\n\t\t\treturn ch\n\t\t} else {\n\t\t\tids.addConnWithLock(c)\n\t\t}\n\t}\n\n\tif e.IdentifyWaitChan != nil {\n\t\treturn e.IdentifyWaitChan\n\t}\n\t// First call to IdentifyWait for this connection. Create the channel.\n\te.IdentifyWaitChan = make(chan struct{})\n\tids.conns[c] = e\n\n\t// Spawn an identify. The connection may actually be closed\n\t// already, but that doesn't really matter. We'll fail to open a\n\t// stream then forget the connection.\n\tgo func() {\n\t\tdefer close(e.IdentifyWaitChan)\n\t\tif err := ids.identifyConn(c); err != nil {\n\t\t\tlog.Warnf(\"failed to identify %s: %s\", c.RemotePeer(), err)\n\t\t\tids.emitters.evtPeerIdentificationFailed.Emit(event.EvtPeerIdentificationFailed{Peer: c.RemotePeer(), Reason: err})\n\t\t\treturn\n\t\t}\n\n\t\tids.emitters.evtPeerIdentificationCompleted.Emit(event.EvtPeerIdentificationCompleted{Peer: c.RemotePeer()})\n\t}()\n\n\treturn e.IdentifyWaitChan\n}\n\nfunc (ids *idService) identifyConn(c network.Conn) error {\n\ts, err := c.NewStream(network.WithUseTransient(context.TODO(), \"identify\"))\n\tif err != nil {\n\t\tlog.Debugw(\"error opening identify stream\", \"peer\", c.RemotePeer(), \"error\", err)\n\t\treturn err\n\t}\n\n\tif err := s.SetProtocol(ID); err != nil {\n\t\tlog.Warnf(\"error setting identify protocol for stream: %s\", err)\n\t\ts.Reset()\n\t}\n\n\t// ok give the response to our handler.\n\tif err := msmux.SelectProtoOrFail(ID, s); err != nil {\n\t\tlog.Infow(\"failed negotiate identify protocol with peer\", \"peer\", c.RemotePeer(), \"error\", err)\n\t\ts.Reset()\n\t\treturn err\n\t}\n\n\treturn ids.handleIdentifyResponse(s, false)\n}\n\n// handlePush handles incoming identify push streams\nfunc (ids *idService) handlePush(s network.Stream) {\n\tids.handleIdentifyResponse(s, true)\n}\n\nfunc (ids *idService) handleIdentifyRequest(s network.Stream) {\n\t_ = ids.sendIdentifyResp(s, false)\n}\n\nfunc (ids *idService) sendIdentifyResp(s network.Stream, isPush bool) error {\n\tif err := s.Scope().SetService(ServiceName); err != nil {\n\t\ts.Reset()\n\t\treturn fmt.Errorf(\"failed to attaching stream to identify service: %w\", err)\n\t}\n\tdefer s.Close()\n\n\tids.currentSnapshot.Lock()\n\tsnapshot := ids.currentSnapshot.snapshot\n\tids.currentSnapshot.Unlock()\n\n\tlog.Debugw(\"sending snapshot\", \"seq\", snapshot.seq, \"protocols\", snapshot.protocols, \"addrs\", snapshot.addrs)\n\n\tmes := ids.createBaseIdentifyResponse(s.Conn(), &snapshot)\n\tmes.SignedPeerRecord = ids.getSignedRecord(&snapshot)\n\n\tlog.Debugf(\"%s sending message to %s %s\", ID, s.Conn().RemotePeer(), s.Conn().RemoteMultiaddr())\n\tif err := ids.writeChunkedIdentifyMsg(s, mes); err != nil {\n\t\treturn err\n\t}\n\n\tif ids.metricsTracer != nil {\n\t\tids.metricsTracer.IdentifySent(isPush, len(mes.Protocols), len(mes.ListenAddrs))\n\t}\n\n\tids.connsMu.Lock()\n\tdefer ids.connsMu.Unlock()\n\te, ok := ids.conns[s.Conn()]\n\t// The connection might already have been closed.\n\t// We *should* receive the Connected notification from the swarm before we're able to accept the peer's\n\t// Identify stream, but if that for some reason doesn't work, we also wouldn't have a map entry here.\n\t// The only consequence would be that we send a spurious Push to that peer later.\n\tif !ok {\n\t\treturn nil\n\t}\n\te.Sequence = snapshot.seq\n\tids.conns[s.Conn()] = e\n\treturn nil\n}\n\nfunc (ids *idService) handleIdentifyResponse(s network.Stream, isPush bool) error {\n\tif err := s.Scope().SetService(ServiceName); err != nil {\n\t\tlog.Warnf(\"error attaching stream to identify service: %s\", err)\n\t\ts.Reset()\n\t\treturn err\n\t}\n\n\tif err := s.Scope().ReserveMemory(signedIDSize, network.ReservationPriorityAlways); err != nil {\n\t\tlog.Warnf(\"error reserving memory for identify stream: %s\", err)\n\t\ts.Reset()\n\t\treturn err\n\t}\n\tdefer s.Scope().ReleaseMemory(signedIDSize)\n\n\t_ = s.SetReadDeadline(time.Now().Add(StreamReadTimeout))\n\n\tc := s.Conn()\n\n\tr := pbio.NewDelimitedReader(s, signedIDSize)\n\tmes := &pb.Identify{}\n\n\tif err := readAllIDMessages(r, mes); err != nil {\n\t\tlog.Warn(\"error reading identify message: \", err)\n\t\ts.Reset()\n\t\treturn err\n\t}\n\n\tdefer s.Close()\n\n\tlog.Debugf(\"%s received message from %s %s\", s.Protocol(), c.RemotePeer(), c.RemoteMultiaddr())\n\n\tids.consumeMessage(mes, c, isPush)\n\n\tif ids.metricsTracer != nil {\n\t\tids.metricsTracer.IdentifyReceived(isPush, len(mes.Protocols), len(mes.ListenAddrs))\n\t}\n\n\tids.connsMu.Lock()\n\tdefer ids.connsMu.Unlock()\n\te, ok := ids.conns[c]\n\tif !ok { // might already have disconnected\n\t\treturn nil\n\t}\n\tsup, err := ids.Host.Peerstore().SupportsProtocols(c.RemotePeer(), IDPush)\n\tif supportsIdentifyPush := err == nil && len(sup) > 0; supportsIdentifyPush {\n\t\te.PushSupport = identifyPushSupported\n\t} else {\n\t\te.PushSupport = identifyPushUnsupported\n\t}\n\n\tif ids.metricsTracer != nil {\n\t\tids.metricsTracer.ConnPushSupport(e.PushSupport)\n\t}\n\n\tids.conns[c] = e\n\treturn nil\n}\n\nfunc readAllIDMessages(r pbio.Reader, finalMsg proto.Message) error {\n\tmes := &pb.Identify{}\n\tfor i := 0; i < maxMessages; i++ {\n\t\tswitch err := r.ReadMsg(mes); err {\n\t\tcase io.EOF:\n\t\t\treturn nil\n\t\tcase nil:\n\t\t\tproto.Merge(finalMsg, mes)\n\t\tdefault:\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn fmt.Errorf(\"too many parts\")\n}\n\nfunc (ids *idService) updateSnapshot() (updated bool) {\n\taddrs := ids.Host.Addrs()\n\tsort.Slice(addrs, func(i, j int) bool { return bytes.Compare(addrs[i].Bytes(), addrs[j].Bytes()) == -1 })\n\tprotos := ids.Host.Mux().Protocols()\n\tsort.Slice(protos, func(i, j int) bool { return protos[i] < protos[j] })\n\tsnapshot := identifySnapshot{\n\t\taddrs:     addrs,\n\t\tprotocols: protos,\n\t}\n\n\tif !ids.disableSignedPeerRecord {\n\t\tif cab, ok := peerstore.GetCertifiedAddrBook(ids.Host.Peerstore()); ok {\n\t\t\tsnapshot.record = cab.GetPeerRecord(ids.Host.ID())\n\t\t}\n\t}\n\n\tids.currentSnapshot.Lock()\n\tdefer ids.currentSnapshot.Unlock()\n\n\tif ids.currentSnapshot.snapshot.Equal(&snapshot) {\n\t\treturn false\n\t}\n\n\tsnapshot.seq = ids.currentSnapshot.snapshot.seq + 1\n\tids.currentSnapshot.snapshot = snapshot\n\n\tlog.Debugw(\"updating snapshot\", \"seq\", snapshot.seq, \"addrs\", snapshot.addrs)\n\treturn true\n}\n\nfunc (ids *idService) writeChunkedIdentifyMsg(s network.Stream, mes *pb.Identify) error {\n\twriter := pbio.NewDelimitedWriter(s)\n\n\tif mes.SignedPeerRecord == nil || proto.Size(mes) <= legacyIDSize {\n\t\treturn writer.WriteMsg(mes)\n\t}\n\n\tsr := mes.SignedPeerRecord\n\tmes.SignedPeerRecord = nil\n\tif err := writer.WriteMsg(mes); err != nil {\n\t\treturn err\n\t}\n\t// then write just the signed record\n\treturn writer.WriteMsg(&pb.Identify{SignedPeerRecord: sr})\n}\n\nfunc (ids *idService) createBaseIdentifyResponse(conn network.Conn, snapshot *identifySnapshot) *pb.Identify {\n\tmes := &pb.Identify{}\n\n\tremoteAddr := conn.RemoteMultiaddr()\n\tlocalAddr := conn.LocalMultiaddr()\n\n\t// set protocols this node is currently handling\n\tmes.Protocols = protocol.ConvertToStrings(snapshot.protocols)\n\n\t// observed address so other side is informed of their\n\t// \"public\" address, at least in relation to us.\n\tmes.ObservedAddr = remoteAddr.Bytes()\n\n\t// populate unsigned addresses.\n\t// peers that do not yet support signed addresses will need this.\n\t// Note: LocalMultiaddr is sometimes 0.0.0.0\n\tviaLoopback := manet.IsIPLoopback(localAddr) || manet.IsIPLoopback(remoteAddr)\n\tmes.ListenAddrs = make([][]byte, 0, len(snapshot.addrs))\n\tfor _, addr := range snapshot.addrs {\n\t\tif !viaLoopback && manet.IsIPLoopback(addr) {\n\t\t\tcontinue\n\t\t}\n\t\tmes.ListenAddrs = append(mes.ListenAddrs, addr.Bytes())\n\t}\n\t// set our public key\n\townKey := ids.Host.Peerstore().PubKey(ids.Host.ID())\n\n\t// check if we even have a public key.\n\tif ownKey == nil {\n\t\t// public key is nil. We are either using insecure transport or something erratic happened.\n\t\t// check if we're even operating in \"secure mode\"\n\t\tif ids.Host.Peerstore().PrivKey(ids.Host.ID()) != nil {\n\t\t\t// private key is present. But NO public key. Something bad happened.\n\t\t\tlog.Errorf(\"did not have own public key in Peerstore\")\n\t\t}\n\t\t// if neither of the key is present it is safe to assume that we are using an insecure transport.\n\t} else {\n\t\t// public key is present. Safe to proceed.\n\t\tif kb, err := crypto.MarshalPublicKey(ownKey); err != nil {\n\t\t\tlog.Errorf(\"failed to convert key to bytes\")\n\t\t} else {\n\t\t\tmes.PublicKey = kb\n\t\t}\n\t}\n\n\t// set protocol versions\n\tmes.ProtocolVersion = &ids.ProtocolVersion\n\tmes.AgentVersion = &ids.UserAgent\n\n\treturn mes\n}\n\nfunc (ids *idService) getSignedRecord(snapshot *identifySnapshot) []byte {\n\tif ids.disableSignedPeerRecord || snapshot.record == nil {\n\t\treturn nil\n\t}\n\n\trecBytes, err := snapshot.record.Marshal()\n\tif err != nil {\n\t\tlog.Errorw(\"failed to marshal signed record\", \"err\", err)\n\t\treturn nil\n\t}\n\n\treturn recBytes\n}\n\n// diff takes two slices of strings (a and b) and computes which elements were added and removed in b\nfunc diff(a, b []protocol.ID) (added, removed []protocol.ID) {\n\t// This is O(n^2), but it's fine because the slices are small.\n\tfor _, x := range b {\n\t\tvar found bool\n\t\tfor _, y := range a {\n\t\t\tif x == y {\n\t\t\t\tfound = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !found {\n\t\t\tadded = append(added, x)\n\t\t}\n\t}\n\tfor _, x := range a {\n\t\tvar found bool\n\t\tfor _, y := range b {\n\t\t\tif x == y {\n\t\t\t\tfound = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !found {\n\t\t\tremoved = append(removed, x)\n\t\t}\n\t}\n\treturn\n}\n\nfunc (ids *idService) consumeMessage(mes *pb.Identify, c network.Conn, isPush bool) {\n\tp := c.RemotePeer()\n\n\tsupported, _ := ids.Host.Peerstore().GetProtocols(p)\n\tmesProtocols := protocol.ConvertFromStrings(mes.Protocols)\n\tadded, removed := diff(supported, mesProtocols)\n\tids.Host.Peerstore().SetProtocols(p, mesProtocols...)\n\tif isPush {\n\t\tids.emitters.evtPeerProtocolsUpdated.Emit(event.EvtPeerProtocolsUpdated{\n\t\t\tPeer:    p,\n\t\t\tAdded:   added,\n\t\t\tRemoved: removed,\n\t\t})\n\t}\n\n\t// mes.ObservedAddr\n\tids.consumeObservedAddress(mes.GetObservedAddr(), c)\n\n\t// mes.ListenAddrs\n\tladdrs := mes.GetListenAddrs()\n\tlmaddrs := make([]ma.Multiaddr, 0, len(laddrs))\n\tfor _, addr := range laddrs {\n\t\tmaddr, err := ma.NewMultiaddrBytes(addr)\n\t\tif err != nil {\n\t\t\tlog.Debugf(\"%s failed to parse multiaddr from %s %s\", ID,\n\t\t\t\tp, c.RemoteMultiaddr())\n\t\t\tcontinue\n\t\t}\n\t\tlmaddrs = append(lmaddrs, maddr)\n\t}\n\n\t// NOTE: Do not add `c.RemoteMultiaddr()` to the peerstore if the remote\n\t// peer doesn't tell us to do so. Otherwise, we'll advertise it.\n\t//\n\t// This can cause an \"addr-splosion\" issue where the network will slowly\n\t// gossip and collect observed but unadvertised addresses. Given a NAT\n\t// that picks random source ports, this can cause DHT nodes to collect\n\t// many undialable addresses for other peers.\n\n\t// add certified addresses for the peer, if they sent us a signed peer record\n\t// otherwise use the unsigned addresses.\n\tsignedPeerRecord, err := signedPeerRecordFromMessage(mes)\n\tif err != nil {\n\t\tlog.Errorf(\"error getting peer record from Identify message: %v\", err)\n\t}\n\n\t// Extend the TTLs on the known (probably) good addresses.\n\t// Taking the lock ensures that we don't concurrently process a disconnect.\n\tids.addrMu.Lock()\n\tttl := peerstore.RecentlyConnectedAddrTTL\n\tif ids.Host.Network().Connectedness(p) == network.Connected {\n\t\tttl = peerstore.ConnectedAddrTTL\n\t}\n\n\t// Downgrade connected and recently connected addrs to a temporary TTL.\n\tfor _, ttl := range []time.Duration{\n\t\tpeerstore.RecentlyConnectedAddrTTL,\n\t\tpeerstore.ConnectedAddrTTL,\n\t} {\n\t\tids.Host.Peerstore().UpdateAddrs(p, ttl, peerstore.TempAddrTTL)\n\t}\n\n\t// add signed addrs if we have them and the peerstore supports them\n\tcab, ok := peerstore.GetCertifiedAddrBook(ids.Host.Peerstore())\n\tif ok && signedPeerRecord != nil && signedPeerRecord.PublicKey != nil {\n\t\tid, err := peer.IDFromPublicKey(signedPeerRecord.PublicKey)\n\t\tif err != nil {\n\t\t\tlog.Debugf(\"failed to derive peer ID from peer record: %s\", err)\n\t\t} else if id != c.RemotePeer() {\n\t\t\tlog.Debugf(\"received signed peer record for unexpected peer ID. expected %s, got %s\", c.RemotePeer(), id)\n\t\t} else if _, err := cab.ConsumePeerRecord(signedPeerRecord, ttl); err != nil {\n\t\t\tlog.Debugf(\"error adding signed addrs to peerstore: %v\", err)\n\t\t}\n\t} else {\n\t\tids.Host.Peerstore().AddAddrs(p, lmaddrs, ttl)\n\t}\n\n\t// Finally, expire all temporary addrs.\n\tids.Host.Peerstore().UpdateAddrs(p, peerstore.TempAddrTTL, 0)\n\tids.addrMu.Unlock()\n\n\tlog.Debugf(\"%s received listen addrs for %s: %s\", c.LocalPeer(), c.RemotePeer(), lmaddrs)\n\n\t// get protocol versions\n\tpv := mes.GetProtocolVersion()\n\tav := mes.GetAgentVersion()\n\n\tids.Host.Peerstore().Put(p, \"ProtocolVersion\", pv)\n\tids.Host.Peerstore().Put(p, \"AgentVersion\", av)\n\n\t// get the key from the other side. we may not have it (no-auth transport)\n\tids.consumeReceivedPubKey(c, mes.PublicKey)\n}\n\nfunc (ids *idService) consumeReceivedPubKey(c network.Conn, kb []byte) {\n\tlp := c.LocalPeer()\n\trp := c.RemotePeer()\n\n\tif kb == nil {\n\t\tlog.Debugf(\"%s did not receive public key for remote peer: %s\", lp, rp)\n\t\treturn\n\t}\n\n\tnewKey, err := crypto.UnmarshalPublicKey(kb)\n\tif err != nil {\n\t\tlog.Warnf(\"%s cannot unmarshal key from remote peer: %s, %s\", lp, rp, err)\n\t\treturn\n\t}\n\n\t// verify key matches peer.ID\n\tnp, err := peer.IDFromPublicKey(newKey)\n\tif err != nil {\n\t\tlog.Debugf(\"%s cannot get peer.ID from key of remote peer: %s, %s\", lp, rp, err)\n\t\treturn\n\t}\n\n\tif np != rp {\n\t\t// if the newKey's peer.ID does not match known peer.ID...\n\n\t\tif rp == \"\" && np != \"\" {\n\t\t\t// if local peerid is empty, then use the new, sent key.\n\t\t\terr := ids.Host.Peerstore().AddPubKey(rp, newKey)\n\t\t\tif err != nil {\n\t\t\t\tlog.Debugf(\"%s could not add key for %s to peerstore: %s\", lp, rp, err)\n\t\t\t}\n\n\t\t} else {\n\t\t\t// we have a local peer.ID and it does not match the sent key... error.\n\t\t\tlog.Errorf(\"%s received key for remote peer %s mismatch: %s\", lp, rp, np)\n\t\t}\n\t\treturn\n\t}\n\n\tcurrKey := ids.Host.Peerstore().PubKey(rp)\n\tif currKey == nil {\n\t\t// no key? no auth transport. set this one.\n\t\terr := ids.Host.Peerstore().AddPubKey(rp, newKey)\n\t\tif err != nil {\n\t\t\tlog.Debugf(\"%s could not add key for %s to peerstore: %s\", lp, rp, err)\n\t\t}\n\t\treturn\n\t}\n\n\t// ok, we have a local key, we should verify they match.\n\tif currKey.Equals(newKey) {\n\t\treturn // ok great. we're done.\n\t}\n\n\t// weird, got a different key... but the different key MATCHES the peer.ID.\n\t// this odd. let's log error and investigate. this should basically never happen\n\t// and it means we have something funky going on and possibly a bug.\n\tlog.Errorf(\"%s identify got a different key for: %s\", lp, rp)\n\n\t// okay... does ours NOT match the remote peer.ID?\n\tcp, err := peer.IDFromPublicKey(currKey)\n\tif err != nil {\n\t\tlog.Errorf(\"%s cannot get peer.ID from local key of remote peer: %s, %s\", lp, rp, err)\n\t\treturn\n\t}\n\tif cp != rp {\n\t\tlog.Errorf(\"%s local key for remote peer %s yields different peer.ID: %s\", lp, rp, cp)\n\t\treturn\n\t}\n\n\t// okay... curr key DOES NOT match new key. both match peer.ID. wat?\n\tlog.Errorf(\"%s local key and received key for %s do not match, but match peer.ID\", lp, rp)\n}\n\n// HasConsistentTransport returns true if the address 'a' shares a\n// protocol set with any address in the green set. This is used\n// to check if a given address might be one of the addresses a peer is\n// listening on.\nfunc HasConsistentTransport(a ma.Multiaddr, green []ma.Multiaddr) bool {\n\tprotosMatch := func(a, b []ma.Protocol) bool {\n\t\tif len(a) != len(b) {\n\t\t\treturn false\n\t\t}\n\n\t\tfor i, p := range a {\n\t\t\tif b[i].Code != p.Code {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\t}\n\n\tprotos := a.Protocols()\n\n\tfor _, ga := range green {\n\t\tif protosMatch(protos, ga.Protocols()) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\nfunc (ids *idService) consumeObservedAddress(observed []byte, c network.Conn) {\n\tif observed == nil {\n\t\treturn\n\t}\n\n\tmaddr, err := ma.NewMultiaddrBytes(observed)\n\tif err != nil {\n\t\tlog.Debugf(\"error parsing received observed addr for %s: %s\", c, err)\n\t\treturn\n\t}\n\n\tids.observedAddrs.Record(c, maddr)\n}\n\n// addConnWithLock assuems caller holds the connsMu lock\nfunc (ids *idService) addConnWithLock(c network.Conn) {\n\t_, found := ids.conns[c]\n\tif !found {\n\t\t<-ids.setupCompleted\n\t\tids.conns[c] = entry{}\n\t}\n}\n\nfunc signedPeerRecordFromMessage(msg *pb.Identify) (*record.Envelope, error) {\n\tif msg.SignedPeerRecord == nil || len(msg.SignedPeerRecord) == 0 {\n\t\treturn nil, nil\n\t}\n\tenv, _, err := record.ConsumeEnvelope(msg.SignedPeerRecord, peer.PeerRecordEnvelopeDomain)\n\treturn env, err\n}\n\n// netNotifiee defines methods to be used with the swarm\ntype netNotifiee idService\n\nfunc (nn *netNotifiee) IDService() *idService {\n\treturn (*idService)(nn)\n}\n\nfunc (nn *netNotifiee) Connected(_ network.Network, c network.Conn) {\n\tids := nn.IDService()\n\n\tids.connsMu.Lock()\n\tids.addConnWithLock(c)\n\tids.connsMu.Unlock()\n\n\tnn.IDService().IdentifyWait(c)\n}\n\nfunc (nn *netNotifiee) Disconnected(_ network.Network, c network.Conn) {\n\tids := nn.IDService()\n\n\t// Stop tracking the connection.\n\tids.connsMu.Lock()\n\tdelete(ids.conns, c)\n\tids.connsMu.Unlock()\n\n\tif ids.Host.Network().Connectedness(c.RemotePeer()) != network.Connected {\n\t\t// Last disconnect.\n\t\t// Undo the setting of addresses to peer.ConnectedAddrTTL we did\n\t\tids.addrMu.Lock()\n\t\tdefer ids.addrMu.Unlock()\n\t\tids.Host.Peerstore().UpdateAddrs(c.RemotePeer(), peerstore.ConnectedAddrTTL, peerstore.RecentlyConnectedAddrTTL)\n\t}\n}\n\nfunc (nn *netNotifiee) Listen(n network.Network, a ma.Multiaddr)      {}\nfunc (nn *netNotifiee) ListenClose(n network.Network, a ma.Multiaddr) {}\n", "package identify\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/libp2p/go-libp2p/core/network\"\n\t\"github.com/libp2p/go-libp2p/core/peer\"\n\t\"github.com/libp2p/go-libp2p/core/peerstore\"\n\trecordPb \"github.com/libp2p/go-libp2p/core/record/pb\"\n\tblhost \"github.com/libp2p/go-libp2p/p2p/host/blank\"\n\tswarmt \"github.com/libp2p/go-libp2p/p2p/net/swarm/testing\"\n\t\"google.golang.org/protobuf/proto\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestFastDisconnect(t *testing.T) {\n\t// This test checks to see if we correctly abort sending an identify\n\t// response if the peer disconnects before we handle the request.\n\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n\tdefer cancel()\n\n\ttarget := blhost.NewBlankHost(swarmt.GenSwarm(t))\n\tdefer target.Close()\n\tids, err := NewIDService(target)\n\trequire.NoError(t, err)\n\tdefer ids.Close()\n\tids.Start()\n\n\tsync := make(chan struct{})\n\ttarget.SetStreamHandler(ID, func(s network.Stream) {\n\t\t// Wait till the stream is set up on both sides.\n\t\tselect {\n\t\tcase <-sync:\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\t}\n\n\t\t// Kill the connection, and make sure we're completely disconnected.\n\t\tassert.Eventually(t,\n\t\t\tfunc() bool {\n\t\t\t\tfor _, conn := range target.Network().ConnsToPeer(s.Conn().RemotePeer()) {\n\t\t\t\t\tconn.Close()\n\t\t\t\t}\n\t\t\t\treturn target.Network().Connectedness(s.Conn().RemotePeer()) != network.Connected\n\t\t\t},\n\t\t\t2*time.Second,\n\t\t\ttime.Millisecond,\n\t\t)\n\t\t// Now try to handle the response.\n\t\t// This should not block indefinitely, or panic, or anything like that.\n\t\t//\n\t\t// However, if we have a bug, that _could_ happen.\n\t\tids.handleIdentifyRequest(s)\n\n\t\t// Ok, allow the outer test to continue.\n\t\tselect {\n\t\tcase <-sync:\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\t}\n\t})\n\n\tsource := blhost.NewBlankHost(swarmt.GenSwarm(t))\n\tdefer source.Close()\n\n\t// only connect to the first address, to make sure we only end up with one connection\n\trequire.NoError(t, source.Connect(ctx, peer.AddrInfo{ID: target.ID(), Addrs: target.Addrs()}))\n\ts, err := source.NewStream(ctx, target.ID(), ID)\n\trequire.NoError(t, err)\n\tselect {\n\tcase sync <- struct{}{}:\n\tcase <-ctx.Done():\n\t\tt.Fatal(ctx.Err())\n\t}\n\ts.Reset()\n\tselect {\n\tcase sync <- struct{}{}:\n\tcase <-ctx.Done():\n\t\tt.Fatal(ctx.Err())\n\t}\n\t// double-check to make sure we didn't actually timeout somewhere.\n\trequire.NoError(t, ctx.Err())\n}\n\nfunc TestWrongSignedPeerRecord(t *testing.T) {\n\th1 := blhost.NewBlankHost(swarmt.GenSwarm(t))\n\tdefer h1.Close()\n\tids, err := NewIDService(h1)\n\trequire.NoError(t, err)\n\tids.Start()\n\tdefer ids.Close()\n\n\th2 := blhost.NewBlankHost(swarmt.GenSwarm(t))\n\tdefer h2.Close()\n\tids2, err := NewIDService(h2)\n\trequire.NoError(t, err)\n\tids2.Start()\n\tdefer ids2.Close()\n\n\th3 := blhost.NewBlankHost(swarmt.GenSwarm(t))\n\tdefer h2.Close()\n\tids3, err := NewIDService(h3)\n\trequire.NoError(t, err)\n\tids3.Start()\n\tdefer ids3.Close()\n\n\th2.Connect(context.Background(), peer.AddrInfo{ID: h1.ID(), Addrs: h1.Addrs()})\n\ts, err := h2.NewStream(context.Background(), h1.ID(), IDPush)\n\trequire.NoError(t, err)\n\n\terr = ids3.sendIdentifyResp(s, true)\n\t// This should fail because the peer record is signed by h3, not h2\n\trequire.NoError(t, err)\n\ttime.Sleep(time.Second)\n\n\trequire.Empty(t, h1.Peerstore().Addrs(h3.ID()), \"h1 should not know about h3 since it was relayed over h2\")\n}\n\nfunc TestInvalidSignedPeerRecord(t *testing.T) {\n\th1 := blhost.NewBlankHost(swarmt.GenSwarm(t))\n\tdefer h1.Close()\n\tids, err := NewIDService(h1)\n\trequire.NoError(t, err)\n\tids.Start()\n\tdefer ids.Close()\n\n\th2 := blhost.NewBlankHost(swarmt.GenSwarm(t))\n\tdefer h2.Close()\n\tids2, err := NewIDService(h2)\n\trequire.NoError(t, err)\n\t// We don't want to start the identify service, we'll manage the messages h2\n\t// sends manually so we can tweak it\n\t// ids2.Start()\n\n\th2.Connect(context.Background(), peer.AddrInfo{ID: h1.ID(), Addrs: h1.Addrs()})\n\trequire.Empty(t, h1.Peerstore().Addrs(h2.ID()))\n\n\ts, err := h2.NewStream(context.Background(), h1.ID(), IDPush)\n\trequire.NoError(t, err)\n\n\tids2.updateSnapshot()\n\tids2.currentSnapshot.Lock()\n\tsnapshot := ids2.currentSnapshot.snapshot\n\tids2.currentSnapshot.Unlock()\n\tmes := ids2.createBaseIdentifyResponse(s.Conn(), &snapshot)\n\tfmt.Println(\"Signed record is\", snapshot.record)\n\tmarshalled, err := snapshot.record.Marshal()\n\trequire.NoError(t, err)\n\n\tvar envPb recordPb.Envelope\n\terr = proto.Unmarshal(marshalled, &envPb)\n\trequire.NoError(t, err)\n\n\tenvPb.Signature = []byte(\"invalid\")\n\n\tmes.SignedPeerRecord, err = proto.Marshal(&envPb)\n\trequire.NoError(t, err)\n\n\terr = ids2.writeChunkedIdentifyMsg(s, mes)\n\trequire.NoError(t, err)\n\tfmt.Println(\"Done sending msg\")\n\ts.Close()\n\n\t// Wait a bit for h1 to process the message\n\ttime.Sleep(1 * time.Second)\n\n\tcab, ok := h1.Peerstore().(peerstore.CertifiedAddrBook)\n\trequire.True(t, ok)\n\trequire.Nil(t, cab.GetPeerRecord(h2.ID()))\n}\n"], "filenames": ["core/record/envelope.go", "p2p/protocol/identify/id.go", "p2p/protocol/identify/id_glass_test.go"], "buggy_code_start_loc": [109, 769, 4], "buggy_code_end_loc": [131, 773, 84], "fixing_code_start_loc": [108, 769, 5], "fixing_code_end_loc": [126, 777, 176], "type": "NVD-CWE-noinfo", "message": "libp2p is a networking stack and library modularized out of The IPFS Project, and bundled separately for other tools to use. In go-libp2p, by using signed peer records a malicious actor can store an arbitrary amount of data in a remote node\u2019s memory. This memory does not get garbage collected and so the victim can run out of memory and crash. If users of go-libp2p in production are not monitoring memory consumption over time, it could be a silent attack i.e. the attacker could bring down nodes over a period of time (how long depends on the node resources i.e. a go-libp2p node on a virtual server with 4 gb of memory takes about 90 sec to bring down; on a larger server, it might take a bit longer.) This issue was patched in version 0.27.4.", "other": {"cve": {"id": "CVE-2023-40583", "sourceIdentifier": "security-advisories@github.com", "published": "2023-08-25T21:15:09.000", "lastModified": "2023-09-01T13:10:55.577", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "libp2p is a networking stack and library modularized out of The IPFS Project, and bundled separately for other tools to use. In go-libp2p, by using signed peer records a malicious actor can store an arbitrary amount of data in a remote node\u2019s memory. This memory does not get garbage collected and so the victim can run out of memory and crash. If users of go-libp2p in production are not monitoring memory consumption over time, it could be a silent attack i.e. the attacker could bring down nodes over a period of time (how long depends on the node resources i.e. a go-libp2p node on a virtual server with 4 gb of memory takes about 90 sec to bring down; on a larger server, it might take a bit longer.) This issue was patched in version 0.27.4."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-400"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:protocol:libp2p:*:*:*:*:*:go:*:*", "versionEndExcluding": "0.27.4", "matchCriteriaId": "C5F4582C-9B6B-41FB-99E1-C3B2CB0E19AD"}]}]}], "references": [{"url": "https://github.com/libp2p/go-libp2p/commit/45d3c6fff662ddd6938982e7e9309ad5fa2ad8dd", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/libp2p/go-libp2p/releases/tag/v0.27.4", "source": "security-advisories@github.com", "tags": ["Release Notes"]}, {"url": "https://github.com/libp2p/go-libp2p/releases/tag/v0.27.7", "source": "security-advisories@github.com", "tags": ["Release Notes"]}, {"url": "https://github.com/libp2p/go-libp2p/security/advisories/GHSA-gcq9-qqwx-rgj3", "source": "security-advisories@github.com", "tags": ["Vendor Advisory"]}]}, "github_commit_url": "https://github.com/libp2p/go-libp2p/commit/45d3c6fff662ddd6938982e7e9309ad5fa2ad8dd"}}