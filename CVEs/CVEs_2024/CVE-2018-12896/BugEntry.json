{"buggy_code": ["/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _linux_POSIX_TIMERS_H\n#define _linux_POSIX_TIMERS_H\n\n#include <linux/spinlock.h>\n#include <linux/list.h>\n#include <linux/sched.h>\n#include <linux/timex.h>\n#include <linux/alarmtimer.h>\n\nstruct siginfo;\n\nstruct cpu_timer_list {\n\tstruct list_head entry;\n\tu64 expires, incr;\n\tstruct task_struct *task;\n\tint firing;\n};\n\n/*\n * Bit fields within a clockid:\n *\n * The most significant 29 bits hold either a pid or a file descriptor.\n *\n * Bit 2 indicates whether a cpu clock refers to a thread or a process.\n *\n * Bits 1 and 0 give the type: PROF=0, VIRT=1, SCHED=2, or FD=3.\n *\n * A clockid is invalid if bits 2, 1, and 0 are all set.\n */\n#define CPUCLOCK_PID(clock)\t\t((pid_t) ~((clock) >> 3))\n#define CPUCLOCK_PERTHREAD(clock) \\\n\t(((clock) & (clockid_t) CPUCLOCK_PERTHREAD_MASK) != 0)\n\n#define CPUCLOCK_PERTHREAD_MASK\t4\n#define CPUCLOCK_WHICH(clock)\t((clock) & (clockid_t) CPUCLOCK_CLOCK_MASK)\n#define CPUCLOCK_CLOCK_MASK\t3\n#define CPUCLOCK_PROF\t\t0\n#define CPUCLOCK_VIRT\t\t1\n#define CPUCLOCK_SCHED\t\t2\n#define CPUCLOCK_MAX\t\t3\n#define CLOCKFD\t\t\tCPUCLOCK_MAX\n#define CLOCKFD_MASK\t\t(CPUCLOCK_PERTHREAD_MASK|CPUCLOCK_CLOCK_MASK)\n\nstatic inline clockid_t make_process_cpuclock(const unsigned int pid,\n\t\tconst clockid_t clock)\n{\n\treturn ((~pid) << 3) | clock;\n}\nstatic inline clockid_t make_thread_cpuclock(const unsigned int tid,\n\t\tconst clockid_t clock)\n{\n\treturn make_process_cpuclock(tid, clock | CPUCLOCK_PERTHREAD_MASK);\n}\n\nstatic inline clockid_t fd_to_clockid(const int fd)\n{\n\treturn make_process_cpuclock((unsigned int) fd, CLOCKFD);\n}\n\nstatic inline int clockid_to_fd(const clockid_t clk)\n{\n\treturn ~(clk >> 3);\n}\n\n#define REQUEUE_PENDING 1\n\n/**\n * struct k_itimer - POSIX.1b interval timer structure.\n * @list:\t\tList head for binding the timer to signals->posix_timers\n * @t_hash:\t\tEntry in the posix timer hash table\n * @it_lock:\t\tLock protecting the timer\n * @kclock:\t\tPointer to the k_clock struct handling this timer\n * @it_clock:\t\tThe posix timer clock id\n * @it_id:\t\tThe posix timer id for identifying the timer\n * @it_active:\t\tMarker that timer is active\n * @it_overrun:\t\tThe overrun counter for pending signals\n * @it_overrun_last:\tThe overrun at the time of the last delivered signal\n * @it_requeue_pending:\tIndicator that timer waits for being requeued on\n *\t\t\tsignal delivery\n * @it_sigev_notify:\tThe notify word of sigevent struct for signal delivery\n * @it_interval:\tThe interval for periodic timers\n * @it_signal:\t\tPointer to the creators signal struct\n * @it_pid:\t\tThe pid of the process/task targeted by the signal\n * @it_process:\t\tThe task to wakeup on clock_nanosleep (CPU timers)\n * @sigq:\t\tPointer to preallocated sigqueue\n * @it:\t\t\tUnion representing the various posix timer type\n *\t\t\tinternals. Also used for rcu freeing the timer.\n */\nstruct k_itimer {\n\tstruct list_head\tlist;\n\tstruct hlist_node\tt_hash;\n\tspinlock_t\t\tit_lock;\n\tconst struct k_clock\t*kclock;\n\tclockid_t\t\tit_clock;\n\ttimer_t\t\t\tit_id;\n\tint\t\t\tit_active;\n\tint\t\t\tit_overrun;\n\tint\t\t\tit_overrun_last;\n\tint\t\t\tit_requeue_pending;\n\tint\t\t\tit_sigev_notify;\n\tktime_t\t\t\tit_interval;\n\tstruct signal_struct\t*it_signal;\n\tunion {\n\t\tstruct pid\t\t*it_pid;\n\t\tstruct task_struct\t*it_process;\n\t};\n\tstruct sigqueue\t\t*sigq;\n\tunion {\n\t\tstruct {\n\t\t\tstruct hrtimer\ttimer;\n\t\t} real;\n\t\tstruct cpu_timer_list\tcpu;\n\t\tstruct {\n\t\t\tstruct alarm\talarmtimer;\n\t\t} alarm;\n\t\tstruct rcu_head\t\trcu;\n\t} it;\n};\n\nvoid run_posix_cpu_timers(struct task_struct *task);\nvoid posix_cpu_timers_exit(struct task_struct *task);\nvoid posix_cpu_timers_exit_group(struct task_struct *task);\nvoid set_process_cpu_timer(struct task_struct *task, unsigned int clock_idx,\n\t\t\t   u64 *newval, u64 *oldval);\n\nvoid update_rlimit_cpu(struct task_struct *task, unsigned long rlim_new);\n\nvoid posixtimer_rearm(struct siginfo *info);\n#endif\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * Implement CPU time clocks for the POSIX clock interface.\n */\n\n#include <linux/sched/signal.h>\n#include <linux/sched/cputime.h>\n#include <linux/posix-timers.h>\n#include <linux/errno.h>\n#include <linux/math64.h>\n#include <linux/uaccess.h>\n#include <linux/kernel_stat.h>\n#include <trace/events/timer.h>\n#include <linux/tick.h>\n#include <linux/workqueue.h>\n#include <linux/compat.h>\n#include <linux/sched/deadline.h>\n\n#include \"posix-timers.h\"\n\nstatic void posix_cpu_timer_rearm(struct k_itimer *timer);\n\n/*\n * Called after updating RLIMIT_CPU to run cpu timer and update\n * tsk->signal->cputime_expires expiration cache if necessary. Needs\n * siglock protection since other code may update expiration cache as\n * well.\n */\nvoid update_rlimit_cpu(struct task_struct *task, unsigned long rlim_new)\n{\n\tu64 nsecs = rlim_new * NSEC_PER_SEC;\n\n\tspin_lock_irq(&task->sighand->siglock);\n\tset_process_cpu_timer(task, CPUCLOCK_PROF, &nsecs, NULL);\n\tspin_unlock_irq(&task->sighand->siglock);\n}\n\nstatic int check_clock(const clockid_t which_clock)\n{\n\tint error = 0;\n\tstruct task_struct *p;\n\tconst pid_t pid = CPUCLOCK_PID(which_clock);\n\n\tif (CPUCLOCK_WHICH(which_clock) >= CPUCLOCK_MAX)\n\t\treturn -EINVAL;\n\n\tif (pid == 0)\n\t\treturn 0;\n\n\trcu_read_lock();\n\tp = find_task_by_vpid(pid);\n\tif (!p || !(CPUCLOCK_PERTHREAD(which_clock) ?\n\t\t   same_thread_group(p, current) : has_group_leader_pid(p))) {\n\t\terror = -EINVAL;\n\t}\n\trcu_read_unlock();\n\n\treturn error;\n}\n\n/*\n * Update expiry time from increment, and increase overrun count,\n * given the current clock sample.\n */\nstatic void bump_cpu_timer(struct k_itimer *timer, u64 now)\n{\n\tint i;\n\tu64 delta, incr;\n\n\tif (timer->it.cpu.incr == 0)\n\t\treturn;\n\n\tif (now < timer->it.cpu.expires)\n\t\treturn;\n\n\tincr = timer->it.cpu.incr;\n\tdelta = now + incr - timer->it.cpu.expires;\n\n\t/* Don't use (incr*2 < delta), incr*2 might overflow. */\n\tfor (i = 0; incr < delta - incr; i++)\n\t\tincr = incr << 1;\n\n\tfor (; i >= 0; incr >>= 1, i--) {\n\t\tif (delta < incr)\n\t\t\tcontinue;\n\n\t\ttimer->it.cpu.expires += incr;\n\t\ttimer->it_overrun += 1 << i;\n\t\tdelta -= incr;\n\t}\n}\n\n/**\n * task_cputime_zero - Check a task_cputime struct for all zero fields.\n *\n * @cputime:\tThe struct to compare.\n *\n * Checks @cputime to see if all fields are zero.  Returns true if all fields\n * are zero, false if any field is nonzero.\n */\nstatic inline int task_cputime_zero(const struct task_cputime *cputime)\n{\n\tif (!cputime->utime && !cputime->stime && !cputime->sum_exec_runtime)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic inline u64 prof_ticks(struct task_struct *p)\n{\n\tu64 utime, stime;\n\n\ttask_cputime(p, &utime, &stime);\n\n\treturn utime + stime;\n}\nstatic inline u64 virt_ticks(struct task_struct *p)\n{\n\tu64 utime, stime;\n\n\ttask_cputime(p, &utime, &stime);\n\n\treturn utime;\n}\n\nstatic int\nposix_cpu_clock_getres(const clockid_t which_clock, struct timespec64 *tp)\n{\n\tint error = check_clock(which_clock);\n\tif (!error) {\n\t\ttp->tv_sec = 0;\n\t\ttp->tv_nsec = ((NSEC_PER_SEC + HZ - 1) / HZ);\n\t\tif (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED) {\n\t\t\t/*\n\t\t\t * If sched_clock is using a cycle counter, we\n\t\t\t * don't have any idea of its true resolution\n\t\t\t * exported, but it is much more than 1s/HZ.\n\t\t\t */\n\t\t\ttp->tv_nsec = 1;\n\t\t}\n\t}\n\treturn error;\n}\n\nstatic int\nposix_cpu_clock_set(const clockid_t which_clock, const struct timespec64 *tp)\n{\n\t/*\n\t * You can never reset a CPU clock, but we check for other errors\n\t * in the call before failing with EPERM.\n\t */\n\tint error = check_clock(which_clock);\n\tif (error == 0) {\n\t\terror = -EPERM;\n\t}\n\treturn error;\n}\n\n\n/*\n * Sample a per-thread clock for the given task.\n */\nstatic int cpu_clock_sample(const clockid_t which_clock,\n\t\t\t    struct task_struct *p, u64 *sample)\n{\n\tswitch (CPUCLOCK_WHICH(which_clock)) {\n\tdefault:\n\t\treturn -EINVAL;\n\tcase CPUCLOCK_PROF:\n\t\t*sample = prof_ticks(p);\n\t\tbreak;\n\tcase CPUCLOCK_VIRT:\n\t\t*sample = virt_ticks(p);\n\t\tbreak;\n\tcase CPUCLOCK_SCHED:\n\t\t*sample = task_sched_runtime(p);\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\n/*\n * Set cputime to sum_cputime if sum_cputime > cputime. Use cmpxchg\n * to avoid race conditions with concurrent updates to cputime.\n */\nstatic inline void __update_gt_cputime(atomic64_t *cputime, u64 sum_cputime)\n{\n\tu64 curr_cputime;\nretry:\n\tcurr_cputime = atomic64_read(cputime);\n\tif (sum_cputime > curr_cputime) {\n\t\tif (atomic64_cmpxchg(cputime, curr_cputime, sum_cputime) != curr_cputime)\n\t\t\tgoto retry;\n\t}\n}\n\nstatic void update_gt_cputime(struct task_cputime_atomic *cputime_atomic, struct task_cputime *sum)\n{\n\t__update_gt_cputime(&cputime_atomic->utime, sum->utime);\n\t__update_gt_cputime(&cputime_atomic->stime, sum->stime);\n\t__update_gt_cputime(&cputime_atomic->sum_exec_runtime, sum->sum_exec_runtime);\n}\n\n/* Sample task_cputime_atomic values in \"atomic_timers\", store results in \"times\". */\nstatic inline void sample_cputime_atomic(struct task_cputime *times,\n\t\t\t\t\t struct task_cputime_atomic *atomic_times)\n{\n\ttimes->utime = atomic64_read(&atomic_times->utime);\n\ttimes->stime = atomic64_read(&atomic_times->stime);\n\ttimes->sum_exec_runtime = atomic64_read(&atomic_times->sum_exec_runtime);\n}\n\nvoid thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times)\n{\n\tstruct thread_group_cputimer *cputimer = &tsk->signal->cputimer;\n\tstruct task_cputime sum;\n\n\t/* Check if cputimer isn't running. This is accessed without locking. */\n\tif (!READ_ONCE(cputimer->running)) {\n\t\t/*\n\t\t * The POSIX timer interface allows for absolute time expiry\n\t\t * values through the TIMER_ABSTIME flag, therefore we have\n\t\t * to synchronize the timer to the clock every time we start it.\n\t\t */\n\t\tthread_group_cputime(tsk, &sum);\n\t\tupdate_gt_cputime(&cputimer->cputime_atomic, &sum);\n\n\t\t/*\n\t\t * We're setting cputimer->running without a lock. Ensure\n\t\t * this only gets written to in one operation. We set\n\t\t * running after update_gt_cputime() as a small optimization,\n\t\t * but barriers are not required because update_gt_cputime()\n\t\t * can handle concurrent updates.\n\t\t */\n\t\tWRITE_ONCE(cputimer->running, true);\n\t}\n\tsample_cputime_atomic(times, &cputimer->cputime_atomic);\n}\n\n/*\n * Sample a process (thread group) clock for the given group_leader task.\n * Must be called with task sighand lock held for safe while_each_thread()\n * traversal.\n */\nstatic int cpu_clock_sample_group(const clockid_t which_clock,\n\t\t\t\t  struct task_struct *p,\n\t\t\t\t  u64 *sample)\n{\n\tstruct task_cputime cputime;\n\n\tswitch (CPUCLOCK_WHICH(which_clock)) {\n\tdefault:\n\t\treturn -EINVAL;\n\tcase CPUCLOCK_PROF:\n\t\tthread_group_cputime(p, &cputime);\n\t\t*sample = cputime.utime + cputime.stime;\n\t\tbreak;\n\tcase CPUCLOCK_VIRT:\n\t\tthread_group_cputime(p, &cputime);\n\t\t*sample = cputime.utime;\n\t\tbreak;\n\tcase CPUCLOCK_SCHED:\n\t\tthread_group_cputime(p, &cputime);\n\t\t*sample = cputime.sum_exec_runtime;\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int posix_cpu_clock_get_task(struct task_struct *tsk,\n\t\t\t\t    const clockid_t which_clock,\n\t\t\t\t    struct timespec64 *tp)\n{\n\tint err = -EINVAL;\n\tu64 rtn;\n\n\tif (CPUCLOCK_PERTHREAD(which_clock)) {\n\t\tif (same_thread_group(tsk, current))\n\t\t\terr = cpu_clock_sample(which_clock, tsk, &rtn);\n\t} else {\n\t\tif (tsk == current || thread_group_leader(tsk))\n\t\t\terr = cpu_clock_sample_group(which_clock, tsk, &rtn);\n\t}\n\n\tif (!err)\n\t\t*tp = ns_to_timespec64(rtn);\n\n\treturn err;\n}\n\n\nstatic int posix_cpu_clock_get(const clockid_t which_clock, struct timespec64 *tp)\n{\n\tconst pid_t pid = CPUCLOCK_PID(which_clock);\n\tint err = -EINVAL;\n\n\tif (pid == 0) {\n\t\t/*\n\t\t * Special case constant value for our own clocks.\n\t\t * We don't have to do any lookup to find ourselves.\n\t\t */\n\t\terr = posix_cpu_clock_get_task(current, which_clock, tp);\n\t} else {\n\t\t/*\n\t\t * Find the given PID, and validate that the caller\n\t\t * should be able to see it.\n\t\t */\n\t\tstruct task_struct *p;\n\t\trcu_read_lock();\n\t\tp = find_task_by_vpid(pid);\n\t\tif (p)\n\t\t\terr = posix_cpu_clock_get_task(p, which_clock, tp);\n\t\trcu_read_unlock();\n\t}\n\n\treturn err;\n}\n\n/*\n * Validate the clockid_t for a new CPU-clock timer, and initialize the timer.\n * This is called from sys_timer_create() and do_cpu_nanosleep() with the\n * new timer already all-zeros initialized.\n */\nstatic int posix_cpu_timer_create(struct k_itimer *new_timer)\n{\n\tint ret = 0;\n\tconst pid_t pid = CPUCLOCK_PID(new_timer->it_clock);\n\tstruct task_struct *p;\n\n\tif (CPUCLOCK_WHICH(new_timer->it_clock) >= CPUCLOCK_MAX)\n\t\treturn -EINVAL;\n\n\tnew_timer->kclock = &clock_posix_cpu;\n\n\tINIT_LIST_HEAD(&new_timer->it.cpu.entry);\n\n\trcu_read_lock();\n\tif (CPUCLOCK_PERTHREAD(new_timer->it_clock)) {\n\t\tif (pid == 0) {\n\t\t\tp = current;\n\t\t} else {\n\t\t\tp = find_task_by_vpid(pid);\n\t\t\tif (p && !same_thread_group(p, current))\n\t\t\t\tp = NULL;\n\t\t}\n\t} else {\n\t\tif (pid == 0) {\n\t\t\tp = current->group_leader;\n\t\t} else {\n\t\t\tp = find_task_by_vpid(pid);\n\t\t\tif (p && !has_group_leader_pid(p))\n\t\t\t\tp = NULL;\n\t\t}\n\t}\n\tnew_timer->it.cpu.task = p;\n\tif (p) {\n\t\tget_task_struct(p);\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n/*\n * Clean up a CPU-clock timer that is about to be destroyed.\n * This is called from timer deletion with the timer already locked.\n * If we return TIMER_RETRY, it's necessary to release the timer's lock\n * and try again.  (This happens when the timer is in the middle of firing.)\n */\nstatic int posix_cpu_timer_del(struct k_itimer *timer)\n{\n\tint ret = 0;\n\tunsigned long flags;\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *p = timer->it.cpu.task;\n\n\tWARN_ON_ONCE(p == NULL);\n\n\t/*\n\t * Protect against sighand release/switch in exit/exec and process/\n\t * thread timer list entry concurrent read/writes.\n\t */\n\tsighand = lock_task_sighand(p, &flags);\n\tif (unlikely(sighand == NULL)) {\n\t\t/*\n\t\t * We raced with the reaping of the task.\n\t\t * The deletion should have cleared us off the list.\n\t\t */\n\t\tWARN_ON_ONCE(!list_empty(&timer->it.cpu.entry));\n\t} else {\n\t\tif (timer->it.cpu.firing)\n\t\t\tret = TIMER_RETRY;\n\t\telse\n\t\t\tlist_del(&timer->it.cpu.entry);\n\n\t\tunlock_task_sighand(p, &flags);\n\t}\n\n\tif (!ret)\n\t\tput_task_struct(p);\n\n\treturn ret;\n}\n\nstatic void cleanup_timers_list(struct list_head *head)\n{\n\tstruct cpu_timer_list *timer, *next;\n\n\tlist_for_each_entry_safe(timer, next, head, entry)\n\t\tlist_del_init(&timer->entry);\n}\n\n/*\n * Clean out CPU timers still ticking when a thread exited.  The task\n * pointer is cleared, and the expiry time is replaced with the residual\n * time for later timer_gettime calls to return.\n * This must be called with the siglock held.\n */\nstatic void cleanup_timers(struct list_head *head)\n{\n\tcleanup_timers_list(head);\n\tcleanup_timers_list(++head);\n\tcleanup_timers_list(++head);\n}\n\n/*\n * These are both called with the siglock held, when the current thread\n * is being reaped.  When the final (leader) thread in the group is reaped,\n * posix_cpu_timers_exit_group will be called after posix_cpu_timers_exit.\n */\nvoid posix_cpu_timers_exit(struct task_struct *tsk)\n{\n\tcleanup_timers(tsk->cpu_timers);\n}\nvoid posix_cpu_timers_exit_group(struct task_struct *tsk)\n{\n\tcleanup_timers(tsk->signal->cpu_timers);\n}\n\nstatic inline int expires_gt(u64 expires, u64 new_exp)\n{\n\treturn expires == 0 || expires > new_exp;\n}\n\n/*\n * Insert the timer on the appropriate list before any timers that\n * expire later.  This must be called with the sighand lock held.\n */\nstatic void arm_timer(struct k_itimer *timer)\n{\n\tstruct task_struct *p = timer->it.cpu.task;\n\tstruct list_head *head, *listpos;\n\tstruct task_cputime *cputime_expires;\n\tstruct cpu_timer_list *const nt = &timer->it.cpu;\n\tstruct cpu_timer_list *next;\n\n\tif (CPUCLOCK_PERTHREAD(timer->it_clock)) {\n\t\thead = p->cpu_timers;\n\t\tcputime_expires = &p->cputime_expires;\n\t} else {\n\t\thead = p->signal->cpu_timers;\n\t\tcputime_expires = &p->signal->cputime_expires;\n\t}\n\thead += CPUCLOCK_WHICH(timer->it_clock);\n\n\tlistpos = head;\n\tlist_for_each_entry(next, head, entry) {\n\t\tif (nt->expires < next->expires)\n\t\t\tbreak;\n\t\tlistpos = &next->entry;\n\t}\n\tlist_add(&nt->entry, listpos);\n\n\tif (listpos == head) {\n\t\tu64 exp = nt->expires;\n\n\t\t/*\n\t\t * We are the new earliest-expiring POSIX 1.b timer, hence\n\t\t * need to update expiration cache. Take into account that\n\t\t * for process timers we share expiration cache with itimers\n\t\t * and RLIMIT_CPU and for thread timers with RLIMIT_RTTIME.\n\t\t */\n\n\t\tswitch (CPUCLOCK_WHICH(timer->it_clock)) {\n\t\tcase CPUCLOCK_PROF:\n\t\t\tif (expires_gt(cputime_expires->prof_exp, exp))\n\t\t\t\tcputime_expires->prof_exp = exp;\n\t\t\tbreak;\n\t\tcase CPUCLOCK_VIRT:\n\t\t\tif (expires_gt(cputime_expires->virt_exp, exp))\n\t\t\t\tcputime_expires->virt_exp = exp;\n\t\t\tbreak;\n\t\tcase CPUCLOCK_SCHED:\n\t\t\tif (expires_gt(cputime_expires->sched_exp, exp))\n\t\t\t\tcputime_expires->sched_exp = exp;\n\t\t\tbreak;\n\t\t}\n\t\tif (CPUCLOCK_PERTHREAD(timer->it_clock))\n\t\t\ttick_dep_set_task(p, TICK_DEP_BIT_POSIX_TIMER);\n\t\telse\n\t\t\ttick_dep_set_signal(p->signal, TICK_DEP_BIT_POSIX_TIMER);\n\t}\n}\n\n/*\n * The timer is locked, fire it and arrange for its reload.\n */\nstatic void cpu_timer_fire(struct k_itimer *timer)\n{\n\tif ((timer->it_sigev_notify & ~SIGEV_THREAD_ID) == SIGEV_NONE) {\n\t\t/*\n\t\t * User don't want any signal.\n\t\t */\n\t\ttimer->it.cpu.expires = 0;\n\t} else if (unlikely(timer->sigq == NULL)) {\n\t\t/*\n\t\t * This a special case for clock_nanosleep,\n\t\t * not a normal timer from sys_timer_create.\n\t\t */\n\t\twake_up_process(timer->it_process);\n\t\ttimer->it.cpu.expires = 0;\n\t} else if (timer->it.cpu.incr == 0) {\n\t\t/*\n\t\t * One-shot timer.  Clear it as soon as it's fired.\n\t\t */\n\t\tposix_timer_event(timer, 0);\n\t\ttimer->it.cpu.expires = 0;\n\t} else if (posix_timer_event(timer, ++timer->it_requeue_pending)) {\n\t\t/*\n\t\t * The signal did not get queued because the signal\n\t\t * was ignored, so we won't get any callback to\n\t\t * reload the timer.  But we need to keep it\n\t\t * ticking in case the signal is deliverable next time.\n\t\t */\n\t\tposix_cpu_timer_rearm(timer);\n\t\t++timer->it_requeue_pending;\n\t}\n}\n\n/*\n * Sample a process (thread group) timer for the given group_leader task.\n * Must be called with task sighand lock held for safe while_each_thread()\n * traversal.\n */\nstatic int cpu_timer_sample_group(const clockid_t which_clock,\n\t\t\t\t  struct task_struct *p, u64 *sample)\n{\n\tstruct task_cputime cputime;\n\n\tthread_group_cputimer(p, &cputime);\n\tswitch (CPUCLOCK_WHICH(which_clock)) {\n\tdefault:\n\t\treturn -EINVAL;\n\tcase CPUCLOCK_PROF:\n\t\t*sample = cputime.utime + cputime.stime;\n\t\tbreak;\n\tcase CPUCLOCK_VIRT:\n\t\t*sample = cputime.utime;\n\t\tbreak;\n\tcase CPUCLOCK_SCHED:\n\t\t*sample = cputime.sum_exec_runtime;\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\n/*\n * Guts of sys_timer_settime for CPU timers.\n * This is called with the timer locked and interrupts disabled.\n * If we return TIMER_RETRY, it's necessary to release the timer's lock\n * and try again.  (This happens when the timer is in the middle of firing.)\n */\nstatic int posix_cpu_timer_set(struct k_itimer *timer, int timer_flags,\n\t\t\t       struct itimerspec64 *new, struct itimerspec64 *old)\n{\n\tunsigned long flags;\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *p = timer->it.cpu.task;\n\tu64 old_expires, new_expires, old_incr, val;\n\tint ret;\n\n\tWARN_ON_ONCE(p == NULL);\n\n\t/*\n\t * Use the to_ktime conversion because that clamps the maximum\n\t * value to KTIME_MAX and avoid multiplication overflows.\n\t */\n\tnew_expires = ktime_to_ns(timespec64_to_ktime(new->it_value));\n\n\t/*\n\t * Protect against sighand release/switch in exit/exec and p->cpu_timers\n\t * and p->signal->cpu_timers read/write in arm_timer()\n\t */\n\tsighand = lock_task_sighand(p, &flags);\n\t/*\n\t * If p has just been reaped, we can no\n\t * longer get any information about it at all.\n\t */\n\tif (unlikely(sighand == NULL)) {\n\t\treturn -ESRCH;\n\t}\n\n\t/*\n\t * Disarm any old timer after extracting its expiry time.\n\t */\n\tlockdep_assert_irqs_disabled();\n\n\tret = 0;\n\told_incr = timer->it.cpu.incr;\n\told_expires = timer->it.cpu.expires;\n\tif (unlikely(timer->it.cpu.firing)) {\n\t\ttimer->it.cpu.firing = -1;\n\t\tret = TIMER_RETRY;\n\t} else\n\t\tlist_del_init(&timer->it.cpu.entry);\n\n\t/*\n\t * We need to sample the current value to convert the new\n\t * value from to relative and absolute, and to convert the\n\t * old value from absolute to relative.  To set a process\n\t * timer, we need a sample to balance the thread expiry\n\t * times (in arm_timer).  With an absolute time, we must\n\t * check if it's already passed.  In short, we need a sample.\n\t */\n\tif (CPUCLOCK_PERTHREAD(timer->it_clock)) {\n\t\tcpu_clock_sample(timer->it_clock, p, &val);\n\t} else {\n\t\tcpu_timer_sample_group(timer->it_clock, p, &val);\n\t}\n\n\tif (old) {\n\t\tif (old_expires == 0) {\n\t\t\told->it_value.tv_sec = 0;\n\t\t\told->it_value.tv_nsec = 0;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Update the timer in case it has\n\t\t\t * overrun already.  If it has,\n\t\t\t * we'll report it as having overrun\n\t\t\t * and with the next reloaded timer\n\t\t\t * already ticking, though we are\n\t\t\t * swallowing that pending\n\t\t\t * notification here to install the\n\t\t\t * new setting.\n\t\t\t */\n\t\t\tbump_cpu_timer(timer, val);\n\t\t\tif (val < timer->it.cpu.expires) {\n\t\t\t\told_expires = timer->it.cpu.expires - val;\n\t\t\t\told->it_value = ns_to_timespec64(old_expires);\n\t\t\t} else {\n\t\t\t\told->it_value.tv_nsec = 1;\n\t\t\t\told->it_value.tv_sec = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (unlikely(ret)) {\n\t\t/*\n\t\t * We are colliding with the timer actually firing.\n\t\t * Punt after filling in the timer's old value, and\n\t\t * disable this firing since we are already reporting\n\t\t * it as an overrun (thanks to bump_cpu_timer above).\n\t\t */\n\t\tunlock_task_sighand(p, &flags);\n\t\tgoto out;\n\t}\n\n\tif (new_expires != 0 && !(timer_flags & TIMER_ABSTIME)) {\n\t\tnew_expires += val;\n\t}\n\n\t/*\n\t * Install the new expiry time (or zero).\n\t * For a timer with no notification action, we don't actually\n\t * arm the timer (we'll just fake it for timer_gettime).\n\t */\n\ttimer->it.cpu.expires = new_expires;\n\tif (new_expires != 0 && val < new_expires) {\n\t\tarm_timer(timer);\n\t}\n\n\tunlock_task_sighand(p, &flags);\n\t/*\n\t * Install the new reload setting, and\n\t * set up the signal and overrun bookkeeping.\n\t */\n\ttimer->it.cpu.incr = timespec64_to_ns(&new->it_interval);\n\n\t/*\n\t * This acts as a modification timestamp for the timer,\n\t * so any automatic reload attempt will punt on seeing\n\t * that we have reset the timer manually.\n\t */\n\ttimer->it_requeue_pending = (timer->it_requeue_pending + 2) &\n\t\t~REQUEUE_PENDING;\n\ttimer->it_overrun_last = 0;\n\ttimer->it_overrun = -1;\n\n\tif (new_expires != 0 && !(val < new_expires)) {\n\t\t/*\n\t\t * The designated time already passed, so we notify\n\t\t * immediately, even if the thread never runs to\n\t\t * accumulate more time on this clock.\n\t\t */\n\t\tcpu_timer_fire(timer);\n\t}\n\n\tret = 0;\n out:\n\tif (old)\n\t\told->it_interval = ns_to_timespec64(old_incr);\n\n\treturn ret;\n}\n\nstatic void posix_cpu_timer_get(struct k_itimer *timer, struct itimerspec64 *itp)\n{\n\tu64 now;\n\tstruct task_struct *p = timer->it.cpu.task;\n\n\tWARN_ON_ONCE(p == NULL);\n\n\t/*\n\t * Easy part: convert the reload time.\n\t */\n\titp->it_interval = ns_to_timespec64(timer->it.cpu.incr);\n\n\tif (!timer->it.cpu.expires)\n\t\treturn;\n\n\t/*\n\t * Sample the clock to take the difference with the expiry time.\n\t */\n\tif (CPUCLOCK_PERTHREAD(timer->it_clock)) {\n\t\tcpu_clock_sample(timer->it_clock, p, &now);\n\t} else {\n\t\tstruct sighand_struct *sighand;\n\t\tunsigned long flags;\n\n\t\t/*\n\t\t * Protect against sighand release/switch in exit/exec and\n\t\t * also make timer sampling safe if it ends up calling\n\t\t * thread_group_cputime().\n\t\t */\n\t\tsighand = lock_task_sighand(p, &flags);\n\t\tif (unlikely(sighand == NULL)) {\n\t\t\t/*\n\t\t\t * The process has been reaped.\n\t\t\t * We can't even collect a sample any more.\n\t\t\t * Call the timer disarmed, nothing else to do.\n\t\t\t */\n\t\t\ttimer->it.cpu.expires = 0;\n\t\t\treturn;\n\t\t} else {\n\t\t\tcpu_timer_sample_group(timer->it_clock, p, &now);\n\t\t\tunlock_task_sighand(p, &flags);\n\t\t}\n\t}\n\n\tif (now < timer->it.cpu.expires) {\n\t\titp->it_value = ns_to_timespec64(timer->it.cpu.expires - now);\n\t} else {\n\t\t/*\n\t\t * The timer should have expired already, but the firing\n\t\t * hasn't taken place yet.  Say it's just about to expire.\n\t\t */\n\t\titp->it_value.tv_nsec = 1;\n\t\titp->it_value.tv_sec = 0;\n\t}\n}\n\nstatic unsigned long long\ncheck_timers_list(struct list_head *timers,\n\t\t  struct list_head *firing,\n\t\t  unsigned long long curr)\n{\n\tint maxfire = 20;\n\n\twhile (!list_empty(timers)) {\n\t\tstruct cpu_timer_list *t;\n\n\t\tt = list_first_entry(timers, struct cpu_timer_list, entry);\n\n\t\tif (!--maxfire || curr < t->expires)\n\t\t\treturn t->expires;\n\n\t\tt->firing = 1;\n\t\tlist_move_tail(&t->entry, firing);\n\t}\n\n\treturn 0;\n}\n\nstatic inline void check_dl_overrun(struct task_struct *tsk)\n{\n\tif (tsk->dl.dl_overrun) {\n\t\ttsk->dl.dl_overrun = 0;\n\t\t__group_send_sig_info(SIGXCPU, SEND_SIG_PRIV, tsk);\n\t}\n}\n\n/*\n * Check for any per-thread CPU timers that have fired and move them off\n * the tsk->cpu_timers[N] list onto the firing list.  Here we update the\n * tsk->it_*_expires values to reflect the remaining thread CPU timers.\n */\nstatic void check_thread_timers(struct task_struct *tsk,\n\t\t\t\tstruct list_head *firing)\n{\n\tstruct list_head *timers = tsk->cpu_timers;\n\tstruct task_cputime *tsk_expires = &tsk->cputime_expires;\n\tu64 expires;\n\tunsigned long soft;\n\n\tif (dl_task(tsk))\n\t\tcheck_dl_overrun(tsk);\n\n\t/*\n\t * If cputime_expires is zero, then there are no active\n\t * per thread CPU timers.\n\t */\n\tif (task_cputime_zero(&tsk->cputime_expires))\n\t\treturn;\n\n\texpires = check_timers_list(timers, firing, prof_ticks(tsk));\n\ttsk_expires->prof_exp = expires;\n\n\texpires = check_timers_list(++timers, firing, virt_ticks(tsk));\n\ttsk_expires->virt_exp = expires;\n\n\ttsk_expires->sched_exp = check_timers_list(++timers, firing,\n\t\t\t\t\t\t   tsk->se.sum_exec_runtime);\n\n\t/*\n\t * Check for the special case thread timers.\n\t */\n\tsoft = task_rlimit(tsk, RLIMIT_RTTIME);\n\tif (soft != RLIM_INFINITY) {\n\t\tunsigned long hard = task_rlimit_max(tsk, RLIMIT_RTTIME);\n\n\t\tif (hard != RLIM_INFINITY &&\n\t\t    tsk->rt.timeout > DIV_ROUND_UP(hard, USEC_PER_SEC/HZ)) {\n\t\t\t/*\n\t\t\t * At the hard limit, we just die.\n\t\t\t * No need to calculate anything else now.\n\t\t\t */\n\t\t\tif (print_fatal_signals) {\n\t\t\t\tpr_info(\"CPU Watchdog Timeout (hard): %s[%d]\\n\",\n\t\t\t\t\ttsk->comm, task_pid_nr(tsk));\n\t\t\t}\n\t\t\t__group_send_sig_info(SIGKILL, SEND_SIG_PRIV, tsk);\n\t\t\treturn;\n\t\t}\n\t\tif (tsk->rt.timeout > DIV_ROUND_UP(soft, USEC_PER_SEC/HZ)) {\n\t\t\t/*\n\t\t\t * At the soft limit, send a SIGXCPU every second.\n\t\t\t */\n\t\t\tif (soft < hard) {\n\t\t\t\tsoft += USEC_PER_SEC;\n\t\t\t\ttsk->signal->rlim[RLIMIT_RTTIME].rlim_cur =\n\t\t\t\t\tsoft;\n\t\t\t}\n\t\t\tif (print_fatal_signals) {\n\t\t\t\tpr_info(\"RT Watchdog Timeout (soft): %s[%d]\\n\",\n\t\t\t\t\ttsk->comm, task_pid_nr(tsk));\n\t\t\t}\n\t\t\t__group_send_sig_info(SIGXCPU, SEND_SIG_PRIV, tsk);\n\t\t}\n\t}\n\tif (task_cputime_zero(tsk_expires))\n\t\ttick_dep_clear_task(tsk, TICK_DEP_BIT_POSIX_TIMER);\n}\n\nstatic inline void stop_process_timers(struct signal_struct *sig)\n{\n\tstruct thread_group_cputimer *cputimer = &sig->cputimer;\n\n\t/* Turn off cputimer->running. This is done without locking. */\n\tWRITE_ONCE(cputimer->running, false);\n\ttick_dep_clear_signal(sig, TICK_DEP_BIT_POSIX_TIMER);\n}\n\nstatic void check_cpu_itimer(struct task_struct *tsk, struct cpu_itimer *it,\n\t\t\t     u64 *expires, u64 cur_time, int signo)\n{\n\tif (!it->expires)\n\t\treturn;\n\n\tif (cur_time >= it->expires) {\n\t\tif (it->incr)\n\t\t\tit->expires += it->incr;\n\t\telse\n\t\t\tit->expires = 0;\n\n\t\ttrace_itimer_expire(signo == SIGPROF ?\n\t\t\t\t    ITIMER_PROF : ITIMER_VIRTUAL,\n\t\t\t\t    tsk->signal->leader_pid, cur_time);\n\t\t__group_send_sig_info(signo, SEND_SIG_PRIV, tsk);\n\t}\n\n\tif (it->expires && (!*expires || it->expires < *expires))\n\t\t*expires = it->expires;\n}\n\n/*\n * Check for any per-thread CPU timers that have fired and move them\n * off the tsk->*_timers list onto the firing list.  Per-thread timers\n * have already been taken off.\n */\nstatic void check_process_timers(struct task_struct *tsk,\n\t\t\t\t struct list_head *firing)\n{\n\tstruct signal_struct *const sig = tsk->signal;\n\tu64 utime, ptime, virt_expires, prof_expires;\n\tu64 sum_sched_runtime, sched_expires;\n\tstruct list_head *timers = sig->cpu_timers;\n\tstruct task_cputime cputime;\n\tunsigned long soft;\n\n\tif (dl_task(tsk))\n\t\tcheck_dl_overrun(tsk);\n\n\t/*\n\t * If cputimer is not running, then there are no active\n\t * process wide timers (POSIX 1.b, itimers, RLIMIT_CPU).\n\t */\n\tif (!READ_ONCE(tsk->signal->cputimer.running))\n\t\treturn;\n\n        /*\n\t * Signify that a thread is checking for process timers.\n\t * Write access to this field is protected by the sighand lock.\n\t */\n\tsig->cputimer.checking_timer = true;\n\n\t/*\n\t * Collect the current process totals.\n\t */\n\tthread_group_cputimer(tsk, &cputime);\n\tutime = cputime.utime;\n\tptime = utime + cputime.stime;\n\tsum_sched_runtime = cputime.sum_exec_runtime;\n\n\tprof_expires = check_timers_list(timers, firing, ptime);\n\tvirt_expires = check_timers_list(++timers, firing, utime);\n\tsched_expires = check_timers_list(++timers, firing, sum_sched_runtime);\n\n\t/*\n\t * Check for the special case process timers.\n\t */\n\tcheck_cpu_itimer(tsk, &sig->it[CPUCLOCK_PROF], &prof_expires, ptime,\n\t\t\t SIGPROF);\n\tcheck_cpu_itimer(tsk, &sig->it[CPUCLOCK_VIRT], &virt_expires, utime,\n\t\t\t SIGVTALRM);\n\tsoft = task_rlimit(tsk, RLIMIT_CPU);\n\tif (soft != RLIM_INFINITY) {\n\t\tunsigned long psecs = div_u64(ptime, NSEC_PER_SEC);\n\t\tunsigned long hard = task_rlimit_max(tsk, RLIMIT_CPU);\n\t\tu64 x;\n\t\tif (psecs >= hard) {\n\t\t\t/*\n\t\t\t * At the hard limit, we just die.\n\t\t\t * No need to calculate anything else now.\n\t\t\t */\n\t\t\tif (print_fatal_signals) {\n\t\t\t\tpr_info(\"RT Watchdog Timeout (hard): %s[%d]\\n\",\n\t\t\t\t\ttsk->comm, task_pid_nr(tsk));\n\t\t\t}\n\t\t\t__group_send_sig_info(SIGKILL, SEND_SIG_PRIV, tsk);\n\t\t\treturn;\n\t\t}\n\t\tif (psecs >= soft) {\n\t\t\t/*\n\t\t\t * At the soft limit, send a SIGXCPU every second.\n\t\t\t */\n\t\t\tif (print_fatal_signals) {\n\t\t\t\tpr_info(\"CPU Watchdog Timeout (soft): %s[%d]\\n\",\n\t\t\t\t\ttsk->comm, task_pid_nr(tsk));\n\t\t\t}\n\t\t\t__group_send_sig_info(SIGXCPU, SEND_SIG_PRIV, tsk);\n\t\t\tif (soft < hard) {\n\t\t\t\tsoft++;\n\t\t\t\tsig->rlim[RLIMIT_CPU].rlim_cur = soft;\n\t\t\t}\n\t\t}\n\t\tx = soft * NSEC_PER_SEC;\n\t\tif (!prof_expires || x < prof_expires)\n\t\t\tprof_expires = x;\n\t}\n\n\tsig->cputime_expires.prof_exp = prof_expires;\n\tsig->cputime_expires.virt_exp = virt_expires;\n\tsig->cputime_expires.sched_exp = sched_expires;\n\tif (task_cputime_zero(&sig->cputime_expires))\n\t\tstop_process_timers(sig);\n\n\tsig->cputimer.checking_timer = false;\n}\n\n/*\n * This is called from the signal code (via posixtimer_rearm)\n * when the last timer signal was delivered and we have to reload the timer.\n */\nstatic void posix_cpu_timer_rearm(struct k_itimer *timer)\n{\n\tstruct sighand_struct *sighand;\n\tunsigned long flags;\n\tstruct task_struct *p = timer->it.cpu.task;\n\tu64 now;\n\n\tWARN_ON_ONCE(p == NULL);\n\n\t/*\n\t * Fetch the current sample and update the timer's expiry time.\n\t */\n\tif (CPUCLOCK_PERTHREAD(timer->it_clock)) {\n\t\tcpu_clock_sample(timer->it_clock, p, &now);\n\t\tbump_cpu_timer(timer, now);\n\t\tif (unlikely(p->exit_state))\n\t\t\treturn;\n\n\t\t/* Protect timer list r/w in arm_timer() */\n\t\tsighand = lock_task_sighand(p, &flags);\n\t\tif (!sighand)\n\t\t\treturn;\n\t} else {\n\t\t/*\n\t\t * Protect arm_timer() and timer sampling in case of call to\n\t\t * thread_group_cputime().\n\t\t */\n\t\tsighand = lock_task_sighand(p, &flags);\n\t\tif (unlikely(sighand == NULL)) {\n\t\t\t/*\n\t\t\t * The process has been reaped.\n\t\t\t * We can't even collect a sample any more.\n\t\t\t */\n\t\t\ttimer->it.cpu.expires = 0;\n\t\t\treturn;\n\t\t} else if (unlikely(p->exit_state) && thread_group_empty(p)) {\n\t\t\t/* If the process is dying, no need to rearm */\n\t\t\tgoto unlock;\n\t\t}\n\t\tcpu_timer_sample_group(timer->it_clock, p, &now);\n\t\tbump_cpu_timer(timer, now);\n\t\t/* Leave the sighand locked for the call below.  */\n\t}\n\n\t/*\n\t * Now re-arm for the new expiry time.\n\t */\n\tlockdep_assert_irqs_disabled();\n\tarm_timer(timer);\nunlock:\n\tunlock_task_sighand(p, &flags);\n}\n\n/**\n * task_cputime_expired - Compare two task_cputime entities.\n *\n * @sample:\tThe task_cputime structure to be checked for expiration.\n * @expires:\tExpiration times, against which @sample will be checked.\n *\n * Checks @sample against @expires to see if any field of @sample has expired.\n * Returns true if any field of the former is greater than the corresponding\n * field of the latter if the latter field is set.  Otherwise returns false.\n */\nstatic inline int task_cputime_expired(const struct task_cputime *sample,\n\t\t\t\t\tconst struct task_cputime *expires)\n{\n\tif (expires->utime && sample->utime >= expires->utime)\n\t\treturn 1;\n\tif (expires->stime && sample->utime + sample->stime >= expires->stime)\n\t\treturn 1;\n\tif (expires->sum_exec_runtime != 0 &&\n\t    sample->sum_exec_runtime >= expires->sum_exec_runtime)\n\t\treturn 1;\n\treturn 0;\n}\n\n/**\n * fastpath_timer_check - POSIX CPU timers fast path.\n *\n * @tsk:\tThe task (thread) being checked.\n *\n * Check the task and thread group timers.  If both are zero (there are no\n * timers set) return false.  Otherwise snapshot the task and thread group\n * timers and compare them with the corresponding expiration times.  Return\n * true if a timer has expired, else return false.\n */\nstatic inline int fastpath_timer_check(struct task_struct *tsk)\n{\n\tstruct signal_struct *sig;\n\n\tif (!task_cputime_zero(&tsk->cputime_expires)) {\n\t\tstruct task_cputime task_sample;\n\n\t\ttask_cputime(tsk, &task_sample.utime, &task_sample.stime);\n\t\ttask_sample.sum_exec_runtime = tsk->se.sum_exec_runtime;\n\t\tif (task_cputime_expired(&task_sample, &tsk->cputime_expires))\n\t\t\treturn 1;\n\t}\n\n\tsig = tsk->signal;\n\t/*\n\t * Check if thread group timers expired when the cputimer is\n\t * running and no other thread in the group is already checking\n\t * for thread group cputimers. These fields are read without the\n\t * sighand lock. However, this is fine because this is meant to\n\t * be a fastpath heuristic to determine whether we should try to\n\t * acquire the sighand lock to check/handle timers.\n\t *\n\t * In the worst case scenario, if 'running' or 'checking_timer' gets\n\t * set but the current thread doesn't see the change yet, we'll wait\n\t * until the next thread in the group gets a scheduler interrupt to\n\t * handle the timer. This isn't an issue in practice because these\n\t * types of delays with signals actually getting sent are expected.\n\t */\n\tif (READ_ONCE(sig->cputimer.running) &&\n\t    !READ_ONCE(sig->cputimer.checking_timer)) {\n\t\tstruct task_cputime group_sample;\n\n\t\tsample_cputime_atomic(&group_sample, &sig->cputimer.cputime_atomic);\n\n\t\tif (task_cputime_expired(&group_sample, &sig->cputime_expires))\n\t\t\treturn 1;\n\t}\n\n\tif (dl_task(tsk) && tsk->dl.dl_overrun)\n\t\treturn 1;\n\n\treturn 0;\n}\n\n/*\n * This is called from the timer interrupt handler.  The irq handler has\n * already updated our counts.  We need to check if any timers fire now.\n * Interrupts are disabled.\n */\nvoid run_posix_cpu_timers(struct task_struct *tsk)\n{\n\tLIST_HEAD(firing);\n\tstruct k_itimer *timer, *next;\n\tunsigned long flags;\n\n\tlockdep_assert_irqs_disabled();\n\n\t/*\n\t * The fast path checks that there are no expired thread or thread\n\t * group timers.  If that's so, just return.\n\t */\n\tif (!fastpath_timer_check(tsk))\n\t\treturn;\n\n\tif (!lock_task_sighand(tsk, &flags))\n\t\treturn;\n\t/*\n\t * Here we take off tsk->signal->cpu_timers[N] and\n\t * tsk->cpu_timers[N] all the timers that are firing, and\n\t * put them on the firing list.\n\t */\n\tcheck_thread_timers(tsk, &firing);\n\n\tcheck_process_timers(tsk, &firing);\n\n\t/*\n\t * We must release these locks before taking any timer's lock.\n\t * There is a potential race with timer deletion here, as the\n\t * siglock now protects our private firing list.  We have set\n\t * the firing flag in each timer, so that a deletion attempt\n\t * that gets the timer lock before we do will give it up and\n\t * spin until we've taken care of that timer below.\n\t */\n\tunlock_task_sighand(tsk, &flags);\n\n\t/*\n\t * Now that all the timers on our list have the firing flag,\n\t * no one will touch their list entries but us.  We'll take\n\t * each timer's lock before clearing its firing flag, so no\n\t * timer call will interfere.\n\t */\n\tlist_for_each_entry_safe(timer, next, &firing, it.cpu.entry) {\n\t\tint cpu_firing;\n\n\t\tspin_lock(&timer->it_lock);\n\t\tlist_del_init(&timer->it.cpu.entry);\n\t\tcpu_firing = timer->it.cpu.firing;\n\t\ttimer->it.cpu.firing = 0;\n\t\t/*\n\t\t * The firing flag is -1 if we collided with a reset\n\t\t * of the timer, which already reported this\n\t\t * almost-firing as an overrun.  So don't generate an event.\n\t\t */\n\t\tif (likely(cpu_firing >= 0))\n\t\t\tcpu_timer_fire(timer);\n\t\tspin_unlock(&timer->it_lock);\n\t}\n}\n\n/*\n * Set one of the process-wide special case CPU timers or RLIMIT_CPU.\n * The tsk->sighand->siglock must be held by the caller.\n */\nvoid set_process_cpu_timer(struct task_struct *tsk, unsigned int clock_idx,\n\t\t\t   u64 *newval, u64 *oldval)\n{\n\tu64 now;\n\tint ret;\n\n\tWARN_ON_ONCE(clock_idx == CPUCLOCK_SCHED);\n\tret = cpu_timer_sample_group(clock_idx, tsk, &now);\n\n\tif (oldval && ret != -EINVAL) {\n\t\t/*\n\t\t * We are setting itimer. The *oldval is absolute and we update\n\t\t * it to be relative, *newval argument is relative and we update\n\t\t * it to be absolute.\n\t\t */\n\t\tif (*oldval) {\n\t\t\tif (*oldval <= now) {\n\t\t\t\t/* Just about to fire. */\n\t\t\t\t*oldval = TICK_NSEC;\n\t\t\t} else {\n\t\t\t\t*oldval -= now;\n\t\t\t}\n\t\t}\n\n\t\tif (!*newval)\n\t\t\treturn;\n\t\t*newval += now;\n\t}\n\n\t/*\n\t * Update expiration cache if we are the earliest timer, or eventually\n\t * RLIMIT_CPU limit is earlier than prof_exp cpu timer expire.\n\t */\n\tswitch (clock_idx) {\n\tcase CPUCLOCK_PROF:\n\t\tif (expires_gt(tsk->signal->cputime_expires.prof_exp, *newval))\n\t\t\ttsk->signal->cputime_expires.prof_exp = *newval;\n\t\tbreak;\n\tcase CPUCLOCK_VIRT:\n\t\tif (expires_gt(tsk->signal->cputime_expires.virt_exp, *newval))\n\t\t\ttsk->signal->cputime_expires.virt_exp = *newval;\n\t\tbreak;\n\t}\n\n\ttick_dep_set_signal(tsk->signal, TICK_DEP_BIT_POSIX_TIMER);\n}\n\nstatic int do_cpu_nanosleep(const clockid_t which_clock, int flags,\n\t\t\t    const struct timespec64 *rqtp)\n{\n\tstruct itimerspec64 it;\n\tstruct k_itimer timer;\n\tu64 expires;\n\tint error;\n\n\t/*\n\t * Set up a temporary timer and then wait for it to go off.\n\t */\n\tmemset(&timer, 0, sizeof timer);\n\tspin_lock_init(&timer.it_lock);\n\ttimer.it_clock = which_clock;\n\ttimer.it_overrun = -1;\n\terror = posix_cpu_timer_create(&timer);\n\ttimer.it_process = current;\n\tif (!error) {\n\t\tstatic struct itimerspec64 zero_it;\n\t\tstruct restart_block *restart;\n\n\t\tmemset(&it, 0, sizeof(it));\n\t\tit.it_value = *rqtp;\n\n\t\tspin_lock_irq(&timer.it_lock);\n\t\terror = posix_cpu_timer_set(&timer, flags, &it, NULL);\n\t\tif (error) {\n\t\t\tspin_unlock_irq(&timer.it_lock);\n\t\t\treturn error;\n\t\t}\n\n\t\twhile (!signal_pending(current)) {\n\t\t\tif (timer.it.cpu.expires == 0) {\n\t\t\t\t/*\n\t\t\t\t * Our timer fired and was reset, below\n\t\t\t\t * deletion can not fail.\n\t\t\t\t */\n\t\t\t\tposix_cpu_timer_del(&timer);\n\t\t\t\tspin_unlock_irq(&timer.it_lock);\n\t\t\t\treturn 0;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Block until cpu_timer_fire (or a signal) wakes us.\n\t\t\t */\n\t\t\t__set_current_state(TASK_INTERRUPTIBLE);\n\t\t\tspin_unlock_irq(&timer.it_lock);\n\t\t\tschedule();\n\t\t\tspin_lock_irq(&timer.it_lock);\n\t\t}\n\n\t\t/*\n\t\t * We were interrupted by a signal.\n\t\t */\n\t\texpires = timer.it.cpu.expires;\n\t\terror = posix_cpu_timer_set(&timer, 0, &zero_it, &it);\n\t\tif (!error) {\n\t\t\t/*\n\t\t\t * Timer is now unarmed, deletion can not fail.\n\t\t\t */\n\t\t\tposix_cpu_timer_del(&timer);\n\t\t}\n\t\tspin_unlock_irq(&timer.it_lock);\n\n\t\twhile (error == TIMER_RETRY) {\n\t\t\t/*\n\t\t\t * We need to handle case when timer was or is in the\n\t\t\t * middle of firing. In other cases we already freed\n\t\t\t * resources.\n\t\t\t */\n\t\t\tspin_lock_irq(&timer.it_lock);\n\t\t\terror = posix_cpu_timer_del(&timer);\n\t\t\tspin_unlock_irq(&timer.it_lock);\n\t\t}\n\n\t\tif ((it.it_value.tv_sec | it.it_value.tv_nsec) == 0) {\n\t\t\t/*\n\t\t\t * It actually did fire already.\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\n\t\terror = -ERESTART_RESTARTBLOCK;\n\t\t/*\n\t\t * Report back to the user the time still remaining.\n\t\t */\n\t\trestart = &current->restart_block;\n\t\trestart->nanosleep.expires = expires;\n\t\tif (restart->nanosleep.type != TT_NONE)\n\t\t\terror = nanosleep_copyout(restart, &it.it_value);\n\t}\n\n\treturn error;\n}\n\nstatic long posix_cpu_nsleep_restart(struct restart_block *restart_block);\n\nstatic int posix_cpu_nsleep(const clockid_t which_clock, int flags,\n\t\t\t    const struct timespec64 *rqtp)\n{\n\tstruct restart_block *restart_block = &current->restart_block;\n\tint error;\n\n\t/*\n\t * Diagnose required errors first.\n\t */\n\tif (CPUCLOCK_PERTHREAD(which_clock) &&\n\t    (CPUCLOCK_PID(which_clock) == 0 ||\n\t     CPUCLOCK_PID(which_clock) == task_pid_vnr(current)))\n\t\treturn -EINVAL;\n\n\terror = do_cpu_nanosleep(which_clock, flags, rqtp);\n\n\tif (error == -ERESTART_RESTARTBLOCK) {\n\n\t\tif (flags & TIMER_ABSTIME)\n\t\t\treturn -ERESTARTNOHAND;\n\n\t\trestart_block->fn = posix_cpu_nsleep_restart;\n\t\trestart_block->nanosleep.clockid = which_clock;\n\t}\n\treturn error;\n}\n\nstatic long posix_cpu_nsleep_restart(struct restart_block *restart_block)\n{\n\tclockid_t which_clock = restart_block->nanosleep.clockid;\n\tstruct timespec64 t;\n\n\tt = ns_to_timespec64(restart_block->nanosleep.expires);\n\n\treturn do_cpu_nanosleep(which_clock, TIMER_ABSTIME, &t);\n}\n\n#define PROCESS_CLOCK\tmake_process_cpuclock(0, CPUCLOCK_SCHED)\n#define THREAD_CLOCK\tmake_thread_cpuclock(0, CPUCLOCK_SCHED)\n\nstatic int process_cpu_clock_getres(const clockid_t which_clock,\n\t\t\t\t    struct timespec64 *tp)\n{\n\treturn posix_cpu_clock_getres(PROCESS_CLOCK, tp);\n}\nstatic int process_cpu_clock_get(const clockid_t which_clock,\n\t\t\t\t struct timespec64 *tp)\n{\n\treturn posix_cpu_clock_get(PROCESS_CLOCK, tp);\n}\nstatic int process_cpu_timer_create(struct k_itimer *timer)\n{\n\ttimer->it_clock = PROCESS_CLOCK;\n\treturn posix_cpu_timer_create(timer);\n}\nstatic int process_cpu_nsleep(const clockid_t which_clock, int flags,\n\t\t\t      const struct timespec64 *rqtp)\n{\n\treturn posix_cpu_nsleep(PROCESS_CLOCK, flags, rqtp);\n}\nstatic int thread_cpu_clock_getres(const clockid_t which_clock,\n\t\t\t\t   struct timespec64 *tp)\n{\n\treturn posix_cpu_clock_getres(THREAD_CLOCK, tp);\n}\nstatic int thread_cpu_clock_get(const clockid_t which_clock,\n\t\t\t\tstruct timespec64 *tp)\n{\n\treturn posix_cpu_clock_get(THREAD_CLOCK, tp);\n}\nstatic int thread_cpu_timer_create(struct k_itimer *timer)\n{\n\ttimer->it_clock = THREAD_CLOCK;\n\treturn posix_cpu_timer_create(timer);\n}\n\nconst struct k_clock clock_posix_cpu = {\n\t.clock_getres\t= posix_cpu_clock_getres,\n\t.clock_set\t= posix_cpu_clock_set,\n\t.clock_get\t= posix_cpu_clock_get,\n\t.timer_create\t= posix_cpu_timer_create,\n\t.nsleep\t\t= posix_cpu_nsleep,\n\t.timer_set\t= posix_cpu_timer_set,\n\t.timer_del\t= posix_cpu_timer_del,\n\t.timer_get\t= posix_cpu_timer_get,\n\t.timer_rearm\t= posix_cpu_timer_rearm,\n};\n\nconst struct k_clock clock_process = {\n\t.clock_getres\t= process_cpu_clock_getres,\n\t.clock_get\t= process_cpu_clock_get,\n\t.timer_create\t= process_cpu_timer_create,\n\t.nsleep\t\t= process_cpu_nsleep,\n};\n\nconst struct k_clock clock_thread = {\n\t.clock_getres\t= thread_cpu_clock_getres,\n\t.clock_get\t= thread_cpu_clock_get,\n\t.timer_create\t= thread_cpu_timer_create,\n};\n", "/*\n * linux/kernel/posix-timers.c\n *\n *\n * 2002-10-15  Posix Clocks & timers\n *                           by George Anzinger george@mvista.com\n *\n *\t\t\t     Copyright (C) 2002 2003 by MontaVista Software.\n *\n * 2004-06-01  Fix CLOCK_REALTIME clock/timer TIMER_ABSTIME bug.\n *\t\t\t     Copyright (C) 2004 Boris Hu\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or (at\n * your option) any later version.\n *\n * This program is distributed in the hope that it will be useful, but\n * WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n * General Public License for more details.\n\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.\n *\n * MontaVista Software | 1237 East Arques Avenue | Sunnyvale | CA 94085 | USA\n */\n\n/* These are all the functions necessary to implement\n * POSIX clocks & timers\n */\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/slab.h>\n#include <linux/time.h>\n#include <linux/mutex.h>\n#include <linux/sched/task.h>\n\n#include <linux/uaccess.h>\n#include <linux/list.h>\n#include <linux/init.h>\n#include <linux/compiler.h>\n#include <linux/hash.h>\n#include <linux/posix-clock.h>\n#include <linux/posix-timers.h>\n#include <linux/syscalls.h>\n#include <linux/wait.h>\n#include <linux/workqueue.h>\n#include <linux/export.h>\n#include <linux/hashtable.h>\n#include <linux/compat.h>\n#include <linux/nospec.h>\n\n#include \"timekeeping.h\"\n#include \"posix-timers.h\"\n\n/*\n * Management arrays for POSIX timers. Timers are now kept in static hash table\n * with 512 entries.\n * Timer ids are allocated by local routine, which selects proper hash head by\n * key, constructed from current->signal address and per signal struct counter.\n * This keeps timer ids unique per process, but now they can intersect between\n * processes.\n */\n\n/*\n * Lets keep our timers in a slab cache :-)\n */\nstatic struct kmem_cache *posix_timers_cache;\n\nstatic DEFINE_HASHTABLE(posix_timers_hashtable, 9);\nstatic DEFINE_SPINLOCK(hash_lock);\n\nstatic const struct k_clock * const posix_clocks[];\nstatic const struct k_clock *clockid_to_kclock(const clockid_t id);\nstatic const struct k_clock clock_realtime, clock_monotonic;\n\n/*\n * we assume that the new SIGEV_THREAD_ID shares no bits with the other\n * SIGEV values.  Here we put out an error if this assumption fails.\n */\n#if SIGEV_THREAD_ID != (SIGEV_THREAD_ID & \\\n                       ~(SIGEV_SIGNAL | SIGEV_NONE | SIGEV_THREAD))\n#error \"SIGEV_THREAD_ID must not share bit with other SIGEV values!\"\n#endif\n\n/*\n * parisc wants ENOTSUP instead of EOPNOTSUPP\n */\n#ifndef ENOTSUP\n# define ENANOSLEEP_NOTSUP EOPNOTSUPP\n#else\n# define ENANOSLEEP_NOTSUP ENOTSUP\n#endif\n\n/*\n * The timer ID is turned into a timer address by idr_find().\n * Verifying a valid ID consists of:\n *\n * a) checking that idr_find() returns other than -1.\n * b) checking that the timer id matches the one in the timer itself.\n * c) that the timer owner is in the callers thread group.\n */\n\n/*\n * CLOCKs: The POSIX standard calls for a couple of clocks and allows us\n *\t    to implement others.  This structure defines the various\n *\t    clocks.\n *\n * RESOLUTION: Clock resolution is used to round up timer and interval\n *\t    times, NOT to report clock times, which are reported with as\n *\t    much resolution as the system can muster.  In some cases this\n *\t    resolution may depend on the underlying clock hardware and\n *\t    may not be quantifiable until run time, and only then is the\n *\t    necessary code is written.\tThe standard says we should say\n *\t    something about this issue in the documentation...\n *\n * FUNCTIONS: The CLOCKs structure defines possible functions to\n *\t    handle various clock functions.\n *\n *\t    The standard POSIX timer management code assumes the\n *\t    following: 1.) The k_itimer struct (sched.h) is used for\n *\t    the timer.  2.) The list, it_lock, it_clock, it_id and\n *\t    it_pid fields are not modified by timer code.\n *\n * Permissions: It is assumed that the clock_settime() function defined\n *\t    for each clock will take care of permission checks.\t Some\n *\t    clocks may be set able by any user (i.e. local process\n *\t    clocks) others not.\t Currently the only set able clock we\n *\t    have is CLOCK_REALTIME and its high res counter part, both of\n *\t    which we beg off on and pass to do_sys_settimeofday().\n */\nstatic struct k_itimer *__lock_timer(timer_t timer_id, unsigned long *flags);\n\n#define lock_timer(tid, flags)\t\t\t\t\t\t   \\\n({\tstruct k_itimer *__timr;\t\t\t\t\t   \\\n\t__cond_lock(&__timr->it_lock, __timr = __lock_timer(tid, flags));  \\\n\t__timr;\t\t\t\t\t\t\t\t   \\\n})\n\nstatic int hash(struct signal_struct *sig, unsigned int nr)\n{\n\treturn hash_32(hash32_ptr(sig) ^ nr, HASH_BITS(posix_timers_hashtable));\n}\n\nstatic struct k_itimer *__posix_timers_find(struct hlist_head *head,\n\t\t\t\t\t    struct signal_struct *sig,\n\t\t\t\t\t    timer_t id)\n{\n\tstruct k_itimer *timer;\n\n\thlist_for_each_entry_rcu(timer, head, t_hash) {\n\t\tif ((timer->it_signal == sig) && (timer->it_id == id))\n\t\t\treturn timer;\n\t}\n\treturn NULL;\n}\n\nstatic struct k_itimer *posix_timer_by_id(timer_t id)\n{\n\tstruct signal_struct *sig = current->signal;\n\tstruct hlist_head *head = &posix_timers_hashtable[hash(sig, id)];\n\n\treturn __posix_timers_find(head, sig, id);\n}\n\nstatic int posix_timer_add(struct k_itimer *timer)\n{\n\tstruct signal_struct *sig = current->signal;\n\tint first_free_id = sig->posix_timer_id;\n\tstruct hlist_head *head;\n\tint ret = -ENOENT;\n\n\tdo {\n\t\tspin_lock(&hash_lock);\n\t\thead = &posix_timers_hashtable[hash(sig, sig->posix_timer_id)];\n\t\tif (!__posix_timers_find(head, sig, sig->posix_timer_id)) {\n\t\t\thlist_add_head_rcu(&timer->t_hash, head);\n\t\t\tret = sig->posix_timer_id;\n\t\t}\n\t\tif (++sig->posix_timer_id < 0)\n\t\t\tsig->posix_timer_id = 0;\n\t\tif ((sig->posix_timer_id == first_free_id) && (ret == -ENOENT))\n\t\t\t/* Loop over all possible ids completed */\n\t\t\tret = -EAGAIN;\n\t\tspin_unlock(&hash_lock);\n\t} while (ret == -ENOENT);\n\treturn ret;\n}\n\nstatic inline void unlock_timer(struct k_itimer *timr, unsigned long flags)\n{\n\tspin_unlock_irqrestore(&timr->it_lock, flags);\n}\n\n/* Get clock_realtime */\nstatic int posix_clock_realtime_get(clockid_t which_clock, struct timespec64 *tp)\n{\n\tktime_get_real_ts64(tp);\n\treturn 0;\n}\n\n/* Set clock_realtime */\nstatic int posix_clock_realtime_set(const clockid_t which_clock,\n\t\t\t\t    const struct timespec64 *tp)\n{\n\treturn do_sys_settimeofday64(tp, NULL);\n}\n\nstatic int posix_clock_realtime_adj(const clockid_t which_clock,\n\t\t\t\t    struct timex *t)\n{\n\treturn do_adjtimex(t);\n}\n\n/*\n * Get monotonic time for posix timers\n */\nstatic int posix_ktime_get_ts(clockid_t which_clock, struct timespec64 *tp)\n{\n\tktime_get_ts64(tp);\n\treturn 0;\n}\n\n/*\n * Get monotonic-raw time for posix timers\n */\nstatic int posix_get_monotonic_raw(clockid_t which_clock, struct timespec64 *tp)\n{\n\tktime_get_raw_ts64(tp);\n\treturn 0;\n}\n\n\nstatic int posix_get_realtime_coarse(clockid_t which_clock, struct timespec64 *tp)\n{\n\tktime_get_coarse_real_ts64(tp);\n\treturn 0;\n}\n\nstatic int posix_get_monotonic_coarse(clockid_t which_clock,\n\t\t\t\t\t\tstruct timespec64 *tp)\n{\n\tktime_get_coarse_ts64(tp);\n\treturn 0;\n}\n\nstatic int posix_get_coarse_res(const clockid_t which_clock, struct timespec64 *tp)\n{\n\t*tp = ktime_to_timespec64(KTIME_LOW_RES);\n\treturn 0;\n}\n\nstatic int posix_get_boottime(const clockid_t which_clock, struct timespec64 *tp)\n{\n\tktime_get_boottime_ts64(tp);\n\treturn 0;\n}\n\nstatic int posix_get_tai(clockid_t which_clock, struct timespec64 *tp)\n{\n\tktime_get_clocktai_ts64(tp);\n\treturn 0;\n}\n\nstatic int posix_get_hrtimer_res(clockid_t which_clock, struct timespec64 *tp)\n{\n\ttp->tv_sec = 0;\n\ttp->tv_nsec = hrtimer_resolution;\n\treturn 0;\n}\n\n/*\n * Initialize everything, well, just everything in Posix clocks/timers ;)\n */\nstatic __init int init_posix_timers(void)\n{\n\tposix_timers_cache = kmem_cache_create(\"posix_timers_cache\",\n\t\t\t\t\tsizeof (struct k_itimer), 0, SLAB_PANIC,\n\t\t\t\t\tNULL);\n\treturn 0;\n}\n__initcall(init_posix_timers);\n\nstatic void common_hrtimer_rearm(struct k_itimer *timr)\n{\n\tstruct hrtimer *timer = &timr->it.real.timer;\n\n\tif (!timr->it_interval)\n\t\treturn;\n\n\ttimr->it_overrun += (unsigned int) hrtimer_forward(timer,\n\t\t\t\t\t\ttimer->base->get_time(),\n\t\t\t\t\t\ttimr->it_interval);\n\thrtimer_restart(timer);\n}\n\n/*\n * This function is exported for use by the signal deliver code.  It is\n * called just prior to the info block being released and passes that\n * block to us.  It's function is to update the overrun entry AND to\n * restart the timer.  It should only be called if the timer is to be\n * restarted (i.e. we have flagged this in the sys_private entry of the\n * info block).\n *\n * To protect against the timer going away while the interrupt is queued,\n * we require that the it_requeue_pending flag be set.\n */\nvoid posixtimer_rearm(struct siginfo *info)\n{\n\tstruct k_itimer *timr;\n\tunsigned long flags;\n\n\ttimr = lock_timer(info->si_tid, &flags);\n\tif (!timr)\n\t\treturn;\n\n\tif (timr->it_requeue_pending == info->si_sys_private) {\n\t\ttimr->kclock->timer_rearm(timr);\n\n\t\ttimr->it_active = 1;\n\t\ttimr->it_overrun_last = timr->it_overrun;\n\t\ttimr->it_overrun = -1;\n\t\t++timr->it_requeue_pending;\n\n\t\tinfo->si_overrun += timr->it_overrun_last;\n\t}\n\n\tunlock_timer(timr, flags);\n}\n\nint posix_timer_event(struct k_itimer *timr, int si_private)\n{\n\tstruct task_struct *task;\n\tint shared, ret = -1;\n\t/*\n\t * FIXME: if ->sigq is queued we can race with\n\t * dequeue_signal()->posixtimer_rearm().\n\t *\n\t * If dequeue_signal() sees the \"right\" value of\n\t * si_sys_private it calls posixtimer_rearm().\n\t * We re-queue ->sigq and drop ->it_lock().\n\t * posixtimer_rearm() locks the timer\n\t * and re-schedules it while ->sigq is pending.\n\t * Not really bad, but not that we want.\n\t */\n\ttimr->sigq->info.si_sys_private = si_private;\n\n\trcu_read_lock();\n\ttask = pid_task(timr->it_pid, PIDTYPE_PID);\n\tif (task) {\n\t\tshared = !(timr->it_sigev_notify & SIGEV_THREAD_ID);\n\t\tret = send_sigqueue(timr->sigq, task, shared);\n\t}\n\trcu_read_unlock();\n\t/* If we failed to send the signal the timer stops. */\n\treturn ret > 0;\n}\n\n/*\n * This function gets called when a POSIX.1b interval timer expires.  It\n * is used as a callback from the kernel internal timer.  The\n * run_timer_list code ALWAYS calls with interrupts on.\n\n * This code is for CLOCK_REALTIME* and CLOCK_MONOTONIC* timers.\n */\nstatic enum hrtimer_restart posix_timer_fn(struct hrtimer *timer)\n{\n\tstruct k_itimer *timr;\n\tunsigned long flags;\n\tint si_private = 0;\n\tenum hrtimer_restart ret = HRTIMER_NORESTART;\n\n\ttimr = container_of(timer, struct k_itimer, it.real.timer);\n\tspin_lock_irqsave(&timr->it_lock, flags);\n\n\ttimr->it_active = 0;\n\tif (timr->it_interval != 0)\n\t\tsi_private = ++timr->it_requeue_pending;\n\n\tif (posix_timer_event(timr, si_private)) {\n\t\t/*\n\t\t * signal was not sent because of sig_ignor\n\t\t * we will not get a call back to restart it AND\n\t\t * it should be restarted.\n\t\t */\n\t\tif (timr->it_interval != 0) {\n\t\t\tktime_t now = hrtimer_cb_get_time(timer);\n\n\t\t\t/*\n\t\t\t * FIXME: What we really want, is to stop this\n\t\t\t * timer completely and restart it in case the\n\t\t\t * SIG_IGN is removed. This is a non trivial\n\t\t\t * change which involves sighand locking\n\t\t\t * (sigh !), which we don't want to do late in\n\t\t\t * the release cycle.\n\t\t\t *\n\t\t\t * For now we just let timers with an interval\n\t\t\t * less than a jiffie expire every jiffie to\n\t\t\t * avoid softirq starvation in case of SIG_IGN\n\t\t\t * and a very small interval, which would put\n\t\t\t * the timer right back on the softirq pending\n\t\t\t * list. By moving now ahead of time we trick\n\t\t\t * hrtimer_forward() to expire the timer\n\t\t\t * later, while we still maintain the overrun\n\t\t\t * accuracy, but have some inconsistency in\n\t\t\t * the timer_gettime() case. This is at least\n\t\t\t * better than a starved softirq. A more\n\t\t\t * complex fix which solves also another related\n\t\t\t * inconsistency is already in the pipeline.\n\t\t\t */\n#ifdef CONFIG_HIGH_RES_TIMERS\n\t\t\t{\n\t\t\t\tktime_t kj = NSEC_PER_SEC / HZ;\n\n\t\t\t\tif (timr->it_interval < kj)\n\t\t\t\t\tnow = ktime_add(now, kj);\n\t\t\t}\n#endif\n\t\t\ttimr->it_overrun += (unsigned int)\n\t\t\t\thrtimer_forward(timer, now,\n\t\t\t\t\t\ttimr->it_interval);\n\t\t\tret = HRTIMER_RESTART;\n\t\t\t++timr->it_requeue_pending;\n\t\t\ttimr->it_active = 1;\n\t\t}\n\t}\n\n\tunlock_timer(timr, flags);\n\treturn ret;\n}\n\nstatic struct pid *good_sigevent(sigevent_t * event)\n{\n\tstruct task_struct *rtn = current->group_leader;\n\n\tswitch (event->sigev_notify) {\n\tcase SIGEV_SIGNAL | SIGEV_THREAD_ID:\n\t\trtn = find_task_by_vpid(event->sigev_notify_thread_id);\n\t\tif (!rtn || !same_thread_group(rtn, current))\n\t\t\treturn NULL;\n\t\t/* FALLTHRU */\n\tcase SIGEV_SIGNAL:\n\tcase SIGEV_THREAD:\n\t\tif (event->sigev_signo <= 0 || event->sigev_signo > SIGRTMAX)\n\t\t\treturn NULL;\n\t\t/* FALLTHRU */\n\tcase SIGEV_NONE:\n\t\treturn task_pid(rtn);\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic struct k_itimer * alloc_posix_timer(void)\n{\n\tstruct k_itimer *tmr;\n\ttmr = kmem_cache_zalloc(posix_timers_cache, GFP_KERNEL);\n\tif (!tmr)\n\t\treturn tmr;\n\tif (unlikely(!(tmr->sigq = sigqueue_alloc()))) {\n\t\tkmem_cache_free(posix_timers_cache, tmr);\n\t\treturn NULL;\n\t}\n\tclear_siginfo(&tmr->sigq->info);\n\treturn tmr;\n}\n\nstatic void k_itimer_rcu_free(struct rcu_head *head)\n{\n\tstruct k_itimer *tmr = container_of(head, struct k_itimer, it.rcu);\n\n\tkmem_cache_free(posix_timers_cache, tmr);\n}\n\n#define IT_ID_SET\t1\n#define IT_ID_NOT_SET\t0\nstatic void release_posix_timer(struct k_itimer *tmr, int it_id_set)\n{\n\tif (it_id_set) {\n\t\tunsigned long flags;\n\t\tspin_lock_irqsave(&hash_lock, flags);\n\t\thlist_del_rcu(&tmr->t_hash);\n\t\tspin_unlock_irqrestore(&hash_lock, flags);\n\t}\n\tput_pid(tmr->it_pid);\n\tsigqueue_free(tmr->sigq);\n\tcall_rcu(&tmr->it.rcu, k_itimer_rcu_free);\n}\n\nstatic int common_timer_create(struct k_itimer *new_timer)\n{\n\thrtimer_init(&new_timer->it.real.timer, new_timer->it_clock, 0);\n\treturn 0;\n}\n\n/* Create a POSIX.1b interval timer. */\nstatic int do_timer_create(clockid_t which_clock, struct sigevent *event,\n\t\t\t   timer_t __user *created_timer_id)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct k_itimer *new_timer;\n\tint error, new_timer_id;\n\tint it_id_set = IT_ID_NOT_SET;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->timer_create)\n\t\treturn -EOPNOTSUPP;\n\n\tnew_timer = alloc_posix_timer();\n\tif (unlikely(!new_timer))\n\t\treturn -EAGAIN;\n\n\tspin_lock_init(&new_timer->it_lock);\n\tnew_timer_id = posix_timer_add(new_timer);\n\tif (new_timer_id < 0) {\n\t\terror = new_timer_id;\n\t\tgoto out;\n\t}\n\n\tit_id_set = IT_ID_SET;\n\tnew_timer->it_id = (timer_t) new_timer_id;\n\tnew_timer->it_clock = which_clock;\n\tnew_timer->kclock = kc;\n\tnew_timer->it_overrun = -1;\n\n\tif (event) {\n\t\trcu_read_lock();\n\t\tnew_timer->it_pid = get_pid(good_sigevent(event));\n\t\trcu_read_unlock();\n\t\tif (!new_timer->it_pid) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tnew_timer->it_sigev_notify     = event->sigev_notify;\n\t\tnew_timer->sigq->info.si_signo = event->sigev_signo;\n\t\tnew_timer->sigq->info.si_value = event->sigev_value;\n\t} else {\n\t\tnew_timer->it_sigev_notify     = SIGEV_SIGNAL;\n\t\tnew_timer->sigq->info.si_signo = SIGALRM;\n\t\tmemset(&new_timer->sigq->info.si_value, 0, sizeof(sigval_t));\n\t\tnew_timer->sigq->info.si_value.sival_int = new_timer->it_id;\n\t\tnew_timer->it_pid = get_pid(task_tgid(current));\n\t}\n\n\tnew_timer->sigq->info.si_tid   = new_timer->it_id;\n\tnew_timer->sigq->info.si_code  = SI_TIMER;\n\n\tif (copy_to_user(created_timer_id,\n\t\t\t &new_timer_id, sizeof (new_timer_id))) {\n\t\terror = -EFAULT;\n\t\tgoto out;\n\t}\n\n\terror = kc->timer_create(new_timer);\n\tif (error)\n\t\tgoto out;\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tnew_timer->it_signal = current->signal;\n\tlist_add(&new_timer->list, &current->signal->posix_timers);\n\tspin_unlock_irq(&current->sighand->siglock);\n\n\treturn 0;\n\t/*\n\t * In the case of the timer belonging to another task, after\n\t * the task is unlocked, the timer is owned by the other task\n\t * and may cease to exist at any time.  Don't use or modify\n\t * new_timer after the unlock call.\n\t */\nout:\n\trelease_posix_timer(new_timer, it_id_set);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(timer_create, const clockid_t, which_clock,\n\t\tstruct sigevent __user *, timer_event_spec,\n\t\ttimer_t __user *, created_timer_id)\n{\n\tif (timer_event_spec) {\n\t\tsigevent_t event;\n\n\t\tif (copy_from_user(&event, timer_event_spec, sizeof (event)))\n\t\t\treturn -EFAULT;\n\t\treturn do_timer_create(which_clock, &event, created_timer_id);\n\t}\n\treturn do_timer_create(which_clock, NULL, created_timer_id);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE3(timer_create, clockid_t, which_clock,\n\t\t       struct compat_sigevent __user *, timer_event_spec,\n\t\t       timer_t __user *, created_timer_id)\n{\n\tif (timer_event_spec) {\n\t\tsigevent_t event;\n\n\t\tif (get_compat_sigevent(&event, timer_event_spec))\n\t\t\treturn -EFAULT;\n\t\treturn do_timer_create(which_clock, &event, created_timer_id);\n\t}\n\treturn do_timer_create(which_clock, NULL, created_timer_id);\n}\n#endif\n\n/*\n * Locking issues: We need to protect the result of the id look up until\n * we get the timer locked down so it is not deleted under us.  The\n * removal is done under the idr spinlock so we use that here to bridge\n * the find to the timer lock.  To avoid a dead lock, the timer id MUST\n * be release with out holding the timer lock.\n */\nstatic struct k_itimer *__lock_timer(timer_t timer_id, unsigned long *flags)\n{\n\tstruct k_itimer *timr;\n\n\t/*\n\t * timer_t could be any type >= int and we want to make sure any\n\t * @timer_id outside positive int range fails lookup.\n\t */\n\tif ((unsigned long long)timer_id > INT_MAX)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\ttimr = posix_timer_by_id(timer_id);\n\tif (timr) {\n\t\tspin_lock_irqsave(&timr->it_lock, *flags);\n\t\tif (timr->it_signal == current->signal) {\n\t\t\trcu_read_unlock();\n\t\t\treturn timr;\n\t\t}\n\t\tspin_unlock_irqrestore(&timr->it_lock, *flags);\n\t}\n\trcu_read_unlock();\n\n\treturn NULL;\n}\n\nstatic ktime_t common_hrtimer_remaining(struct k_itimer *timr, ktime_t now)\n{\n\tstruct hrtimer *timer = &timr->it.real.timer;\n\n\treturn __hrtimer_expires_remaining_adjusted(timer, now);\n}\n\nstatic s64 common_hrtimer_forward(struct k_itimer *timr, ktime_t now)\n{\n\tstruct hrtimer *timer = &timr->it.real.timer;\n\n\treturn hrtimer_forward(timer, now, timr->it_interval);\n}\n\n/*\n * Get the time remaining on a POSIX.1b interval timer.  This function\n * is ALWAYS called with spin_lock_irq on the timer, thus it must not\n * mess with irq.\n *\n * We have a couple of messes to clean up here.  First there is the case\n * of a timer that has a requeue pending.  These timers should appear to\n * be in the timer list with an expiry as if we were to requeue them\n * now.\n *\n * The second issue is the SIGEV_NONE timer which may be active but is\n * not really ever put in the timer list (to save system resources).\n * This timer may be expired, and if so, we will do it here.  Otherwise\n * it is the same as a requeue pending timer WRT to what we should\n * report.\n */\nvoid common_timer_get(struct k_itimer *timr, struct itimerspec64 *cur_setting)\n{\n\tconst struct k_clock *kc = timr->kclock;\n\tktime_t now, remaining, iv;\n\tstruct timespec64 ts64;\n\tbool sig_none;\n\n\tsig_none = timr->it_sigev_notify == SIGEV_NONE;\n\tiv = timr->it_interval;\n\n\t/* interval timer ? */\n\tif (iv) {\n\t\tcur_setting->it_interval = ktime_to_timespec64(iv);\n\t} else if (!timr->it_active) {\n\t\t/*\n\t\t * SIGEV_NONE oneshot timers are never queued. Check them\n\t\t * below.\n\t\t */\n\t\tif (!sig_none)\n\t\t\treturn;\n\t}\n\n\t/*\n\t * The timespec64 based conversion is suboptimal, but it's not\n\t * worth to implement yet another callback.\n\t */\n\tkc->clock_get(timr->it_clock, &ts64);\n\tnow = timespec64_to_ktime(ts64);\n\n\t/*\n\t * When a requeue is pending or this is a SIGEV_NONE timer move the\n\t * expiry time forward by intervals, so expiry is > now.\n\t */\n\tif (iv && (timr->it_requeue_pending & REQUEUE_PENDING || sig_none))\n\t\ttimr->it_overrun += (int)kc->timer_forward(timr, now);\n\n\tremaining = kc->timer_remaining(timr, now);\n\t/* Return 0 only, when the timer is expired and not pending */\n\tif (remaining <= 0) {\n\t\t/*\n\t\t * A single shot SIGEV_NONE timer must return 0, when\n\t\t * it is expired !\n\t\t */\n\t\tif (!sig_none)\n\t\t\tcur_setting->it_value.tv_nsec = 1;\n\t} else {\n\t\tcur_setting->it_value = ktime_to_timespec64(remaining);\n\t}\n}\n\n/* Get the time remaining on a POSIX.1b interval timer. */\nstatic int do_timer_gettime(timer_t timer_id,  struct itimerspec64 *setting)\n{\n\tstruct k_itimer *timr;\n\tconst struct k_clock *kc;\n\tunsigned long flags;\n\tint ret = 0;\n\n\ttimr = lock_timer(timer_id, &flags);\n\tif (!timr)\n\t\treturn -EINVAL;\n\n\tmemset(setting, 0, sizeof(*setting));\n\tkc = timr->kclock;\n\tif (WARN_ON_ONCE(!kc || !kc->timer_get))\n\t\tret = -EINVAL;\n\telse\n\t\tkc->timer_get(timr, setting);\n\n\tunlock_timer(timr, flags);\n\treturn ret;\n}\n\n/* Get the time remaining on a POSIX.1b interval timer. */\nSYSCALL_DEFINE2(timer_gettime, timer_t, timer_id,\n\t\tstruct __kernel_itimerspec __user *, setting)\n{\n\tstruct itimerspec64 cur_setting;\n\n\tint ret = do_timer_gettime(timer_id, &cur_setting);\n\tif (!ret) {\n\t\tif (put_itimerspec64(&cur_setting, setting))\n\t\t\tret = -EFAULT;\n\t}\n\treturn ret;\n}\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\n\nCOMPAT_SYSCALL_DEFINE2(timer_gettime, timer_t, timer_id,\n\t\t       struct compat_itimerspec __user *, setting)\n{\n\tstruct itimerspec64 cur_setting;\n\n\tint ret = do_timer_gettime(timer_id, &cur_setting);\n\tif (!ret) {\n\t\tif (put_compat_itimerspec64(&cur_setting, setting))\n\t\t\tret = -EFAULT;\n\t}\n\treturn ret;\n}\n\n#endif\n\n/*\n * Get the number of overruns of a POSIX.1b interval timer.  This is to\n * be the overrun of the timer last delivered.  At the same time we are\n * accumulating overruns on the next timer.  The overrun is frozen when\n * the signal is delivered, either at the notify time (if the info block\n * is not queued) or at the actual delivery time (as we are informed by\n * the call back to posixtimer_rearm().  So all we need to do is\n * to pick up the frozen overrun.\n */\nSYSCALL_DEFINE1(timer_getoverrun, timer_t, timer_id)\n{\n\tstruct k_itimer *timr;\n\tint overrun;\n\tunsigned long flags;\n\n\ttimr = lock_timer(timer_id, &flags);\n\tif (!timr)\n\t\treturn -EINVAL;\n\n\toverrun = timr->it_overrun_last;\n\tunlock_timer(timr, flags);\n\n\treturn overrun;\n}\n\nstatic void common_hrtimer_arm(struct k_itimer *timr, ktime_t expires,\n\t\t\t       bool absolute, bool sigev_none)\n{\n\tstruct hrtimer *timer = &timr->it.real.timer;\n\tenum hrtimer_mode mode;\n\n\tmode = absolute ? HRTIMER_MODE_ABS : HRTIMER_MODE_REL;\n\t/*\n\t * Posix magic: Relative CLOCK_REALTIME timers are not affected by\n\t * clock modifications, so they become CLOCK_MONOTONIC based under the\n\t * hood. See hrtimer_init(). Update timr->kclock, so the generic\n\t * functions which use timr->kclock->clock_get() work.\n\t *\n\t * Note: it_clock stays unmodified, because the next timer_set() might\n\t * use ABSTIME, so it needs to switch back.\n\t */\n\tif (timr->it_clock == CLOCK_REALTIME)\n\t\ttimr->kclock = absolute ? &clock_realtime : &clock_monotonic;\n\n\thrtimer_init(&timr->it.real.timer, timr->it_clock, mode);\n\ttimr->it.real.timer.function = posix_timer_fn;\n\n\tif (!absolute)\n\t\texpires = ktime_add_safe(expires, timer->base->get_time());\n\thrtimer_set_expires(timer, expires);\n\n\tif (!sigev_none)\n\t\thrtimer_start_expires(timer, HRTIMER_MODE_ABS);\n}\n\nstatic int common_hrtimer_try_to_cancel(struct k_itimer *timr)\n{\n\treturn hrtimer_try_to_cancel(&timr->it.real.timer);\n}\n\n/* Set a POSIX.1b interval timer. */\nint common_timer_set(struct k_itimer *timr, int flags,\n\t\t     struct itimerspec64 *new_setting,\n\t\t     struct itimerspec64 *old_setting)\n{\n\tconst struct k_clock *kc = timr->kclock;\n\tbool sigev_none;\n\tktime_t expires;\n\n\tif (old_setting)\n\t\tcommon_timer_get(timr, old_setting);\n\n\t/* Prevent rearming by clearing the interval */\n\ttimr->it_interval = 0;\n\t/*\n\t * Careful here. On SMP systems the timer expiry function could be\n\t * active and spinning on timr->it_lock.\n\t */\n\tif (kc->timer_try_to_cancel(timr) < 0)\n\t\treturn TIMER_RETRY;\n\n\ttimr->it_active = 0;\n\ttimr->it_requeue_pending = (timr->it_requeue_pending + 2) &\n\t\t~REQUEUE_PENDING;\n\ttimr->it_overrun_last = 0;\n\n\t/* Switch off the timer when it_value is zero */\n\tif (!new_setting->it_value.tv_sec && !new_setting->it_value.tv_nsec)\n\t\treturn 0;\n\n\ttimr->it_interval = timespec64_to_ktime(new_setting->it_interval);\n\texpires = timespec64_to_ktime(new_setting->it_value);\n\tsigev_none = timr->it_sigev_notify == SIGEV_NONE;\n\n\tkc->timer_arm(timr, expires, flags & TIMER_ABSTIME, sigev_none);\n\ttimr->it_active = !sigev_none;\n\treturn 0;\n}\n\nstatic int do_timer_settime(timer_t timer_id, int flags,\n\t\t\t    struct itimerspec64 *new_spec64,\n\t\t\t    struct itimerspec64 *old_spec64)\n{\n\tconst struct k_clock *kc;\n\tstruct k_itimer *timr;\n\tunsigned long flag;\n\tint error = 0;\n\n\tif (!timespec64_valid(&new_spec64->it_interval) ||\n\t    !timespec64_valid(&new_spec64->it_value))\n\t\treturn -EINVAL;\n\n\tif (old_spec64)\n\t\tmemset(old_spec64, 0, sizeof(*old_spec64));\nretry:\n\ttimr = lock_timer(timer_id, &flag);\n\tif (!timr)\n\t\treturn -EINVAL;\n\n\tkc = timr->kclock;\n\tif (WARN_ON_ONCE(!kc || !kc->timer_set))\n\t\terror = -EINVAL;\n\telse\n\t\terror = kc->timer_set(timr, flags, new_spec64, old_spec64);\n\n\tunlock_timer(timr, flag);\n\tif (error == TIMER_RETRY) {\n\t\told_spec64 = NULL;\t// We already got the old time...\n\t\tgoto retry;\n\t}\n\n\treturn error;\n}\n\n/* Set a POSIX.1b interval timer */\nSYSCALL_DEFINE4(timer_settime, timer_t, timer_id, int, flags,\n\t\tconst struct __kernel_itimerspec __user *, new_setting,\n\t\tstruct __kernel_itimerspec __user *, old_setting)\n{\n\tstruct itimerspec64 new_spec, old_spec;\n\tstruct itimerspec64 *rtn = old_setting ? &old_spec : NULL;\n\tint error = 0;\n\n\tif (!new_setting)\n\t\treturn -EINVAL;\n\n\tif (get_itimerspec64(&new_spec, new_setting))\n\t\treturn -EFAULT;\n\n\terror = do_timer_settime(timer_id, flags, &new_spec, rtn);\n\tif (!error && old_setting) {\n\t\tif (put_itimerspec64(&old_spec, old_setting))\n\t\t\terror = -EFAULT;\n\t}\n\treturn error;\n}\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\nCOMPAT_SYSCALL_DEFINE4(timer_settime, timer_t, timer_id, int, flags,\n\t\t       struct compat_itimerspec __user *, new,\n\t\t       struct compat_itimerspec __user *, old)\n{\n\tstruct itimerspec64 new_spec, old_spec;\n\tstruct itimerspec64 *rtn = old ? &old_spec : NULL;\n\tint error = 0;\n\n\tif (!new)\n\t\treturn -EINVAL;\n\tif (get_compat_itimerspec64(&new_spec, new))\n\t\treturn -EFAULT;\n\n\terror = do_timer_settime(timer_id, flags, &new_spec, rtn);\n\tif (!error && old) {\n\t\tif (put_compat_itimerspec64(&old_spec, old))\n\t\t\terror = -EFAULT;\n\t}\n\treturn error;\n}\n#endif\n\nint common_timer_del(struct k_itimer *timer)\n{\n\tconst struct k_clock *kc = timer->kclock;\n\n\ttimer->it_interval = 0;\n\tif (kc->timer_try_to_cancel(timer) < 0)\n\t\treturn TIMER_RETRY;\n\ttimer->it_active = 0;\n\treturn 0;\n}\n\nstatic inline int timer_delete_hook(struct k_itimer *timer)\n{\n\tconst struct k_clock *kc = timer->kclock;\n\n\tif (WARN_ON_ONCE(!kc || !kc->timer_del))\n\t\treturn -EINVAL;\n\treturn kc->timer_del(timer);\n}\n\n/* Delete a POSIX.1b interval timer. */\nSYSCALL_DEFINE1(timer_delete, timer_t, timer_id)\n{\n\tstruct k_itimer *timer;\n\tunsigned long flags;\n\nretry_delete:\n\ttimer = lock_timer(timer_id, &flags);\n\tif (!timer)\n\t\treturn -EINVAL;\n\n\tif (timer_delete_hook(timer) == TIMER_RETRY) {\n\t\tunlock_timer(timer, flags);\n\t\tgoto retry_delete;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\tlist_del(&timer->list);\n\tspin_unlock(&current->sighand->siglock);\n\t/*\n\t * This keeps any tasks waiting on the spin lock from thinking\n\t * they got something (see the lock code above).\n\t */\n\ttimer->it_signal = NULL;\n\n\tunlock_timer(timer, flags);\n\trelease_posix_timer(timer, IT_ID_SET);\n\treturn 0;\n}\n\n/*\n * return timer owned by the process, used by exit_itimers\n */\nstatic void itimer_delete(struct k_itimer *timer)\n{\n\tunsigned long flags;\n\nretry_delete:\n\tspin_lock_irqsave(&timer->it_lock, flags);\n\n\tif (timer_delete_hook(timer) == TIMER_RETRY) {\n\t\tunlock_timer(timer, flags);\n\t\tgoto retry_delete;\n\t}\n\tlist_del(&timer->list);\n\t/*\n\t * This keeps any tasks waiting on the spin lock from thinking\n\t * they got something (see the lock code above).\n\t */\n\ttimer->it_signal = NULL;\n\n\tunlock_timer(timer, flags);\n\trelease_posix_timer(timer, IT_ID_SET);\n}\n\n/*\n * This is called by do_exit or de_thread, only when there are no more\n * references to the shared signal_struct.\n */\nvoid exit_itimers(struct signal_struct *sig)\n{\n\tstruct k_itimer *tmr;\n\n\twhile (!list_empty(&sig->posix_timers)) {\n\t\ttmr = list_entry(sig->posix_timers.next, struct k_itimer, list);\n\t\titimer_delete(tmr);\n\t}\n}\n\nSYSCALL_DEFINE2(clock_settime, const clockid_t, which_clock,\n\t\tconst struct __kernel_timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 new_tp;\n\n\tif (!kc || !kc->clock_set)\n\t\treturn -EINVAL;\n\n\tif (get_timespec64(&new_tp, tp))\n\t\treturn -EFAULT;\n\n\treturn kc->clock_set(which_clock, &new_tp);\n}\n\nSYSCALL_DEFINE2(clock_gettime, const clockid_t, which_clock,\n\t\tstruct __kernel_timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 kernel_tp;\n\tint error;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\n\terror = kc->clock_get(which_clock, &kernel_tp);\n\n\tif (!error && put_timespec64(&kernel_tp, tp))\n\t\terror = -EFAULT;\n\n\treturn error;\n}\n\nSYSCALL_DEFINE2(clock_adjtime, const clockid_t, which_clock,\n\t\tstruct timex __user *, utx)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timex ktx;\n\tint err;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->clock_adj)\n\t\treturn -EOPNOTSUPP;\n\n\tif (copy_from_user(&ktx, utx, sizeof(ktx)))\n\t\treturn -EFAULT;\n\n\terr = kc->clock_adj(which_clock, &ktx);\n\n\tif (err >= 0 && copy_to_user(utx, &ktx, sizeof(ktx)))\n\t\treturn -EFAULT;\n\n\treturn err;\n}\n\nSYSCALL_DEFINE2(clock_getres, const clockid_t, which_clock,\n\t\tstruct __kernel_timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 rtn_tp;\n\tint error;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\n\terror = kc->clock_getres(which_clock, &rtn_tp);\n\n\tif (!error && tp && put_timespec64(&rtn_tp, tp))\n\t\terror = -EFAULT;\n\n\treturn error;\n}\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\n\nCOMPAT_SYSCALL_DEFINE2(clock_settime, clockid_t, which_clock,\n\t\t       struct compat_timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 ts;\n\n\tif (!kc || !kc->clock_set)\n\t\treturn -EINVAL;\n\n\tif (compat_get_timespec64(&ts, tp))\n\t\treturn -EFAULT;\n\n\treturn kc->clock_set(which_clock, &ts);\n}\n\nCOMPAT_SYSCALL_DEFINE2(clock_gettime, clockid_t, which_clock,\n\t\t       struct compat_timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 ts;\n\tint err;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\n\terr = kc->clock_get(which_clock, &ts);\n\n\tif (!err && compat_put_timespec64(&ts, tp))\n\t\terr = -EFAULT;\n\n\treturn err;\n}\n\n#endif\n\n#ifdef CONFIG_COMPAT\n\nCOMPAT_SYSCALL_DEFINE2(clock_adjtime, clockid_t, which_clock,\n\t\t       struct compat_timex __user *, utp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timex ktx;\n\tint err;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->clock_adj)\n\t\treturn -EOPNOTSUPP;\n\n\terr = compat_get_timex(&ktx, utp);\n\tif (err)\n\t\treturn err;\n\n\terr = kc->clock_adj(which_clock, &ktx);\n\n\tif (err >= 0)\n\t\terr = compat_put_timex(utp, &ktx);\n\n\treturn err;\n}\n\n#endif\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\n\nCOMPAT_SYSCALL_DEFINE2(clock_getres, clockid_t, which_clock,\n\t\t       struct compat_timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 ts;\n\tint err;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\n\terr = kc->clock_getres(which_clock, &ts);\n\tif (!err && tp && compat_put_timespec64(&ts, tp))\n\t\treturn -EFAULT;\n\n\treturn err;\n}\n\n#endif\n\n/*\n * nanosleep for monotonic and realtime clocks\n */\nstatic int common_nsleep(const clockid_t which_clock, int flags,\n\t\t\t const struct timespec64 *rqtp)\n{\n\treturn hrtimer_nanosleep(rqtp, flags & TIMER_ABSTIME ?\n\t\t\t\t HRTIMER_MODE_ABS : HRTIMER_MODE_REL,\n\t\t\t\t which_clock);\n}\n\nSYSCALL_DEFINE4(clock_nanosleep, const clockid_t, which_clock, int, flags,\n\t\tconst struct __kernel_timespec __user *, rqtp,\n\t\tstruct __kernel_timespec __user *, rmtp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 t;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->nsleep)\n\t\treturn -ENANOSLEEP_NOTSUP;\n\n\tif (get_timespec64(&t, rqtp))\n\t\treturn -EFAULT;\n\n\tif (!timespec64_valid(&t))\n\t\treturn -EINVAL;\n\tif (flags & TIMER_ABSTIME)\n\t\trmtp = NULL;\n\tcurrent->restart_block.nanosleep.type = rmtp ? TT_NATIVE : TT_NONE;\n\tcurrent->restart_block.nanosleep.rmtp = rmtp;\n\n\treturn kc->nsleep(which_clock, flags, &t);\n}\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\n\nCOMPAT_SYSCALL_DEFINE4(clock_nanosleep, clockid_t, which_clock, int, flags,\n\t\t       struct compat_timespec __user *, rqtp,\n\t\t       struct compat_timespec __user *, rmtp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 t;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->nsleep)\n\t\treturn -ENANOSLEEP_NOTSUP;\n\n\tif (compat_get_timespec64(&t, rqtp))\n\t\treturn -EFAULT;\n\n\tif (!timespec64_valid(&t))\n\t\treturn -EINVAL;\n\tif (flags & TIMER_ABSTIME)\n\t\trmtp = NULL;\n\tcurrent->restart_block.nanosleep.type = rmtp ? TT_COMPAT : TT_NONE;\n\tcurrent->restart_block.nanosleep.compat_rmtp = rmtp;\n\n\treturn kc->nsleep(which_clock, flags, &t);\n}\n\n#endif\n\nstatic const struct k_clock clock_realtime = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_clock_realtime_get,\n\t.clock_set\t\t= posix_clock_realtime_set,\n\t.clock_adj\t\t= posix_clock_realtime_adj,\n\t.nsleep\t\t\t= common_nsleep,\n\t.timer_create\t\t= common_timer_create,\n\t.timer_set\t\t= common_timer_set,\n\t.timer_get\t\t= common_timer_get,\n\t.timer_del\t\t= common_timer_del,\n\t.timer_rearm\t\t= common_hrtimer_rearm,\n\t.timer_forward\t\t= common_hrtimer_forward,\n\t.timer_remaining\t= common_hrtimer_remaining,\n\t.timer_try_to_cancel\t= common_hrtimer_try_to_cancel,\n\t.timer_arm\t\t= common_hrtimer_arm,\n};\n\nstatic const struct k_clock clock_monotonic = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_ktime_get_ts,\n\t.nsleep\t\t\t= common_nsleep,\n\t.timer_create\t\t= common_timer_create,\n\t.timer_set\t\t= common_timer_set,\n\t.timer_get\t\t= common_timer_get,\n\t.timer_del\t\t= common_timer_del,\n\t.timer_rearm\t\t= common_hrtimer_rearm,\n\t.timer_forward\t\t= common_hrtimer_forward,\n\t.timer_remaining\t= common_hrtimer_remaining,\n\t.timer_try_to_cancel\t= common_hrtimer_try_to_cancel,\n\t.timer_arm\t\t= common_hrtimer_arm,\n};\n\nstatic const struct k_clock clock_monotonic_raw = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_get_monotonic_raw,\n};\n\nstatic const struct k_clock clock_realtime_coarse = {\n\t.clock_getres\t\t= posix_get_coarse_res,\n\t.clock_get\t\t= posix_get_realtime_coarse,\n};\n\nstatic const struct k_clock clock_monotonic_coarse = {\n\t.clock_getres\t\t= posix_get_coarse_res,\n\t.clock_get\t\t= posix_get_monotonic_coarse,\n};\n\nstatic const struct k_clock clock_tai = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_get_tai,\n\t.nsleep\t\t\t= common_nsleep,\n\t.timer_create\t\t= common_timer_create,\n\t.timer_set\t\t= common_timer_set,\n\t.timer_get\t\t= common_timer_get,\n\t.timer_del\t\t= common_timer_del,\n\t.timer_rearm\t\t= common_hrtimer_rearm,\n\t.timer_forward\t\t= common_hrtimer_forward,\n\t.timer_remaining\t= common_hrtimer_remaining,\n\t.timer_try_to_cancel\t= common_hrtimer_try_to_cancel,\n\t.timer_arm\t\t= common_hrtimer_arm,\n};\n\nstatic const struct k_clock clock_boottime = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_get_boottime,\n\t.nsleep\t\t\t= common_nsleep,\n\t.timer_create\t\t= common_timer_create,\n\t.timer_set\t\t= common_timer_set,\n\t.timer_get\t\t= common_timer_get,\n\t.timer_del\t\t= common_timer_del,\n\t.timer_rearm\t\t= common_hrtimer_rearm,\n\t.timer_forward\t\t= common_hrtimer_forward,\n\t.timer_remaining\t= common_hrtimer_remaining,\n\t.timer_try_to_cancel\t= common_hrtimer_try_to_cancel,\n\t.timer_arm\t\t= common_hrtimer_arm,\n};\n\nstatic const struct k_clock * const posix_clocks[] = {\n\t[CLOCK_REALTIME]\t\t= &clock_realtime,\n\t[CLOCK_MONOTONIC]\t\t= &clock_monotonic,\n\t[CLOCK_PROCESS_CPUTIME_ID]\t= &clock_process,\n\t[CLOCK_THREAD_CPUTIME_ID]\t= &clock_thread,\n\t[CLOCK_MONOTONIC_RAW]\t\t= &clock_monotonic_raw,\n\t[CLOCK_REALTIME_COARSE]\t\t= &clock_realtime_coarse,\n\t[CLOCK_MONOTONIC_COARSE]\t= &clock_monotonic_coarse,\n\t[CLOCK_BOOTTIME]\t\t= &clock_boottime,\n\t[CLOCK_REALTIME_ALARM]\t\t= &alarm_clock,\n\t[CLOCK_BOOTTIME_ALARM]\t\t= &alarm_clock,\n\t[CLOCK_TAI]\t\t\t= &clock_tai,\n};\n\nstatic const struct k_clock *clockid_to_kclock(const clockid_t id)\n{\n\tclockid_t idx = id;\n\n\tif (id < 0) {\n\t\treturn (id & CLOCKFD_MASK) == CLOCKFD ?\n\t\t\t&clock_posix_dynamic : &clock_posix_cpu;\n\t}\n\n\tif (id >= ARRAY_SIZE(posix_clocks))\n\t\treturn NULL;\n\n\treturn posix_clocks[array_index_nospec(idx, ARRAY_SIZE(posix_clocks))];\n}\n"], "fixing_code": ["/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _linux_POSIX_TIMERS_H\n#define _linux_POSIX_TIMERS_H\n\n#include <linux/spinlock.h>\n#include <linux/list.h>\n#include <linux/sched.h>\n#include <linux/timex.h>\n#include <linux/alarmtimer.h>\n\nstruct siginfo;\n\nstruct cpu_timer_list {\n\tstruct list_head entry;\n\tu64 expires, incr;\n\tstruct task_struct *task;\n\tint firing;\n};\n\n/*\n * Bit fields within a clockid:\n *\n * The most significant 29 bits hold either a pid or a file descriptor.\n *\n * Bit 2 indicates whether a cpu clock refers to a thread or a process.\n *\n * Bits 1 and 0 give the type: PROF=0, VIRT=1, SCHED=2, or FD=3.\n *\n * A clockid is invalid if bits 2, 1, and 0 are all set.\n */\n#define CPUCLOCK_PID(clock)\t\t((pid_t) ~((clock) >> 3))\n#define CPUCLOCK_PERTHREAD(clock) \\\n\t(((clock) & (clockid_t) CPUCLOCK_PERTHREAD_MASK) != 0)\n\n#define CPUCLOCK_PERTHREAD_MASK\t4\n#define CPUCLOCK_WHICH(clock)\t((clock) & (clockid_t) CPUCLOCK_CLOCK_MASK)\n#define CPUCLOCK_CLOCK_MASK\t3\n#define CPUCLOCK_PROF\t\t0\n#define CPUCLOCK_VIRT\t\t1\n#define CPUCLOCK_SCHED\t\t2\n#define CPUCLOCK_MAX\t\t3\n#define CLOCKFD\t\t\tCPUCLOCK_MAX\n#define CLOCKFD_MASK\t\t(CPUCLOCK_PERTHREAD_MASK|CPUCLOCK_CLOCK_MASK)\n\nstatic inline clockid_t make_process_cpuclock(const unsigned int pid,\n\t\tconst clockid_t clock)\n{\n\treturn ((~pid) << 3) | clock;\n}\nstatic inline clockid_t make_thread_cpuclock(const unsigned int tid,\n\t\tconst clockid_t clock)\n{\n\treturn make_process_cpuclock(tid, clock | CPUCLOCK_PERTHREAD_MASK);\n}\n\nstatic inline clockid_t fd_to_clockid(const int fd)\n{\n\treturn make_process_cpuclock((unsigned int) fd, CLOCKFD);\n}\n\nstatic inline int clockid_to_fd(const clockid_t clk)\n{\n\treturn ~(clk >> 3);\n}\n\n#define REQUEUE_PENDING 1\n\n/**\n * struct k_itimer - POSIX.1b interval timer structure.\n * @list:\t\tList head for binding the timer to signals->posix_timers\n * @t_hash:\t\tEntry in the posix timer hash table\n * @it_lock:\t\tLock protecting the timer\n * @kclock:\t\tPointer to the k_clock struct handling this timer\n * @it_clock:\t\tThe posix timer clock id\n * @it_id:\t\tThe posix timer id for identifying the timer\n * @it_active:\t\tMarker that timer is active\n * @it_overrun:\t\tThe overrun counter for pending signals\n * @it_overrun_last:\tThe overrun at the time of the last delivered signal\n * @it_requeue_pending:\tIndicator that timer waits for being requeued on\n *\t\t\tsignal delivery\n * @it_sigev_notify:\tThe notify word of sigevent struct for signal delivery\n * @it_interval:\tThe interval for periodic timers\n * @it_signal:\t\tPointer to the creators signal struct\n * @it_pid:\t\tThe pid of the process/task targeted by the signal\n * @it_process:\t\tThe task to wakeup on clock_nanosleep (CPU timers)\n * @sigq:\t\tPointer to preallocated sigqueue\n * @it:\t\t\tUnion representing the various posix timer type\n *\t\t\tinternals. Also used for rcu freeing the timer.\n */\nstruct k_itimer {\n\tstruct list_head\tlist;\n\tstruct hlist_node\tt_hash;\n\tspinlock_t\t\tit_lock;\n\tconst struct k_clock\t*kclock;\n\tclockid_t\t\tit_clock;\n\ttimer_t\t\t\tit_id;\n\tint\t\t\tit_active;\n\ts64\t\t\tit_overrun;\n\ts64\t\t\tit_overrun_last;\n\tint\t\t\tit_requeue_pending;\n\tint\t\t\tit_sigev_notify;\n\tktime_t\t\t\tit_interval;\n\tstruct signal_struct\t*it_signal;\n\tunion {\n\t\tstruct pid\t\t*it_pid;\n\t\tstruct task_struct\t*it_process;\n\t};\n\tstruct sigqueue\t\t*sigq;\n\tunion {\n\t\tstruct {\n\t\t\tstruct hrtimer\ttimer;\n\t\t} real;\n\t\tstruct cpu_timer_list\tcpu;\n\t\tstruct {\n\t\t\tstruct alarm\talarmtimer;\n\t\t} alarm;\n\t\tstruct rcu_head\t\trcu;\n\t} it;\n};\n\nvoid run_posix_cpu_timers(struct task_struct *task);\nvoid posix_cpu_timers_exit(struct task_struct *task);\nvoid posix_cpu_timers_exit_group(struct task_struct *task);\nvoid set_process_cpu_timer(struct task_struct *task, unsigned int clock_idx,\n\t\t\t   u64 *newval, u64 *oldval);\n\nvoid update_rlimit_cpu(struct task_struct *task, unsigned long rlim_new);\n\nvoid posixtimer_rearm(struct siginfo *info);\n#endif\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * Implement CPU time clocks for the POSIX clock interface.\n */\n\n#include <linux/sched/signal.h>\n#include <linux/sched/cputime.h>\n#include <linux/posix-timers.h>\n#include <linux/errno.h>\n#include <linux/math64.h>\n#include <linux/uaccess.h>\n#include <linux/kernel_stat.h>\n#include <trace/events/timer.h>\n#include <linux/tick.h>\n#include <linux/workqueue.h>\n#include <linux/compat.h>\n#include <linux/sched/deadline.h>\n\n#include \"posix-timers.h\"\n\nstatic void posix_cpu_timer_rearm(struct k_itimer *timer);\n\n/*\n * Called after updating RLIMIT_CPU to run cpu timer and update\n * tsk->signal->cputime_expires expiration cache if necessary. Needs\n * siglock protection since other code may update expiration cache as\n * well.\n */\nvoid update_rlimit_cpu(struct task_struct *task, unsigned long rlim_new)\n{\n\tu64 nsecs = rlim_new * NSEC_PER_SEC;\n\n\tspin_lock_irq(&task->sighand->siglock);\n\tset_process_cpu_timer(task, CPUCLOCK_PROF, &nsecs, NULL);\n\tspin_unlock_irq(&task->sighand->siglock);\n}\n\nstatic int check_clock(const clockid_t which_clock)\n{\n\tint error = 0;\n\tstruct task_struct *p;\n\tconst pid_t pid = CPUCLOCK_PID(which_clock);\n\n\tif (CPUCLOCK_WHICH(which_clock) >= CPUCLOCK_MAX)\n\t\treturn -EINVAL;\n\n\tif (pid == 0)\n\t\treturn 0;\n\n\trcu_read_lock();\n\tp = find_task_by_vpid(pid);\n\tif (!p || !(CPUCLOCK_PERTHREAD(which_clock) ?\n\t\t   same_thread_group(p, current) : has_group_leader_pid(p))) {\n\t\terror = -EINVAL;\n\t}\n\trcu_read_unlock();\n\n\treturn error;\n}\n\n/*\n * Update expiry time from increment, and increase overrun count,\n * given the current clock sample.\n */\nstatic void bump_cpu_timer(struct k_itimer *timer, u64 now)\n{\n\tint i;\n\tu64 delta, incr;\n\n\tif (timer->it.cpu.incr == 0)\n\t\treturn;\n\n\tif (now < timer->it.cpu.expires)\n\t\treturn;\n\n\tincr = timer->it.cpu.incr;\n\tdelta = now + incr - timer->it.cpu.expires;\n\n\t/* Don't use (incr*2 < delta), incr*2 might overflow. */\n\tfor (i = 0; incr < delta - incr; i++)\n\t\tincr = incr << 1;\n\n\tfor (; i >= 0; incr >>= 1, i--) {\n\t\tif (delta < incr)\n\t\t\tcontinue;\n\n\t\ttimer->it.cpu.expires += incr;\n\t\ttimer->it_overrun += 1LL << i;\n\t\tdelta -= incr;\n\t}\n}\n\n/**\n * task_cputime_zero - Check a task_cputime struct for all zero fields.\n *\n * @cputime:\tThe struct to compare.\n *\n * Checks @cputime to see if all fields are zero.  Returns true if all fields\n * are zero, false if any field is nonzero.\n */\nstatic inline int task_cputime_zero(const struct task_cputime *cputime)\n{\n\tif (!cputime->utime && !cputime->stime && !cputime->sum_exec_runtime)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic inline u64 prof_ticks(struct task_struct *p)\n{\n\tu64 utime, stime;\n\n\ttask_cputime(p, &utime, &stime);\n\n\treturn utime + stime;\n}\nstatic inline u64 virt_ticks(struct task_struct *p)\n{\n\tu64 utime, stime;\n\n\ttask_cputime(p, &utime, &stime);\n\n\treturn utime;\n}\n\nstatic int\nposix_cpu_clock_getres(const clockid_t which_clock, struct timespec64 *tp)\n{\n\tint error = check_clock(which_clock);\n\tif (!error) {\n\t\ttp->tv_sec = 0;\n\t\ttp->tv_nsec = ((NSEC_PER_SEC + HZ - 1) / HZ);\n\t\tif (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED) {\n\t\t\t/*\n\t\t\t * If sched_clock is using a cycle counter, we\n\t\t\t * don't have any idea of its true resolution\n\t\t\t * exported, but it is much more than 1s/HZ.\n\t\t\t */\n\t\t\ttp->tv_nsec = 1;\n\t\t}\n\t}\n\treturn error;\n}\n\nstatic int\nposix_cpu_clock_set(const clockid_t which_clock, const struct timespec64 *tp)\n{\n\t/*\n\t * You can never reset a CPU clock, but we check for other errors\n\t * in the call before failing with EPERM.\n\t */\n\tint error = check_clock(which_clock);\n\tif (error == 0) {\n\t\terror = -EPERM;\n\t}\n\treturn error;\n}\n\n\n/*\n * Sample a per-thread clock for the given task.\n */\nstatic int cpu_clock_sample(const clockid_t which_clock,\n\t\t\t    struct task_struct *p, u64 *sample)\n{\n\tswitch (CPUCLOCK_WHICH(which_clock)) {\n\tdefault:\n\t\treturn -EINVAL;\n\tcase CPUCLOCK_PROF:\n\t\t*sample = prof_ticks(p);\n\t\tbreak;\n\tcase CPUCLOCK_VIRT:\n\t\t*sample = virt_ticks(p);\n\t\tbreak;\n\tcase CPUCLOCK_SCHED:\n\t\t*sample = task_sched_runtime(p);\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\n/*\n * Set cputime to sum_cputime if sum_cputime > cputime. Use cmpxchg\n * to avoid race conditions with concurrent updates to cputime.\n */\nstatic inline void __update_gt_cputime(atomic64_t *cputime, u64 sum_cputime)\n{\n\tu64 curr_cputime;\nretry:\n\tcurr_cputime = atomic64_read(cputime);\n\tif (sum_cputime > curr_cputime) {\n\t\tif (atomic64_cmpxchg(cputime, curr_cputime, sum_cputime) != curr_cputime)\n\t\t\tgoto retry;\n\t}\n}\n\nstatic void update_gt_cputime(struct task_cputime_atomic *cputime_atomic, struct task_cputime *sum)\n{\n\t__update_gt_cputime(&cputime_atomic->utime, sum->utime);\n\t__update_gt_cputime(&cputime_atomic->stime, sum->stime);\n\t__update_gt_cputime(&cputime_atomic->sum_exec_runtime, sum->sum_exec_runtime);\n}\n\n/* Sample task_cputime_atomic values in \"atomic_timers\", store results in \"times\". */\nstatic inline void sample_cputime_atomic(struct task_cputime *times,\n\t\t\t\t\t struct task_cputime_atomic *atomic_times)\n{\n\ttimes->utime = atomic64_read(&atomic_times->utime);\n\ttimes->stime = atomic64_read(&atomic_times->stime);\n\ttimes->sum_exec_runtime = atomic64_read(&atomic_times->sum_exec_runtime);\n}\n\nvoid thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times)\n{\n\tstruct thread_group_cputimer *cputimer = &tsk->signal->cputimer;\n\tstruct task_cputime sum;\n\n\t/* Check if cputimer isn't running. This is accessed without locking. */\n\tif (!READ_ONCE(cputimer->running)) {\n\t\t/*\n\t\t * The POSIX timer interface allows for absolute time expiry\n\t\t * values through the TIMER_ABSTIME flag, therefore we have\n\t\t * to synchronize the timer to the clock every time we start it.\n\t\t */\n\t\tthread_group_cputime(tsk, &sum);\n\t\tupdate_gt_cputime(&cputimer->cputime_atomic, &sum);\n\n\t\t/*\n\t\t * We're setting cputimer->running without a lock. Ensure\n\t\t * this only gets written to in one operation. We set\n\t\t * running after update_gt_cputime() as a small optimization,\n\t\t * but barriers are not required because update_gt_cputime()\n\t\t * can handle concurrent updates.\n\t\t */\n\t\tWRITE_ONCE(cputimer->running, true);\n\t}\n\tsample_cputime_atomic(times, &cputimer->cputime_atomic);\n}\n\n/*\n * Sample a process (thread group) clock for the given group_leader task.\n * Must be called with task sighand lock held for safe while_each_thread()\n * traversal.\n */\nstatic int cpu_clock_sample_group(const clockid_t which_clock,\n\t\t\t\t  struct task_struct *p,\n\t\t\t\t  u64 *sample)\n{\n\tstruct task_cputime cputime;\n\n\tswitch (CPUCLOCK_WHICH(which_clock)) {\n\tdefault:\n\t\treturn -EINVAL;\n\tcase CPUCLOCK_PROF:\n\t\tthread_group_cputime(p, &cputime);\n\t\t*sample = cputime.utime + cputime.stime;\n\t\tbreak;\n\tcase CPUCLOCK_VIRT:\n\t\tthread_group_cputime(p, &cputime);\n\t\t*sample = cputime.utime;\n\t\tbreak;\n\tcase CPUCLOCK_SCHED:\n\t\tthread_group_cputime(p, &cputime);\n\t\t*sample = cputime.sum_exec_runtime;\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int posix_cpu_clock_get_task(struct task_struct *tsk,\n\t\t\t\t    const clockid_t which_clock,\n\t\t\t\t    struct timespec64 *tp)\n{\n\tint err = -EINVAL;\n\tu64 rtn;\n\n\tif (CPUCLOCK_PERTHREAD(which_clock)) {\n\t\tif (same_thread_group(tsk, current))\n\t\t\terr = cpu_clock_sample(which_clock, tsk, &rtn);\n\t} else {\n\t\tif (tsk == current || thread_group_leader(tsk))\n\t\t\terr = cpu_clock_sample_group(which_clock, tsk, &rtn);\n\t}\n\n\tif (!err)\n\t\t*tp = ns_to_timespec64(rtn);\n\n\treturn err;\n}\n\n\nstatic int posix_cpu_clock_get(const clockid_t which_clock, struct timespec64 *tp)\n{\n\tconst pid_t pid = CPUCLOCK_PID(which_clock);\n\tint err = -EINVAL;\n\n\tif (pid == 0) {\n\t\t/*\n\t\t * Special case constant value for our own clocks.\n\t\t * We don't have to do any lookup to find ourselves.\n\t\t */\n\t\terr = posix_cpu_clock_get_task(current, which_clock, tp);\n\t} else {\n\t\t/*\n\t\t * Find the given PID, and validate that the caller\n\t\t * should be able to see it.\n\t\t */\n\t\tstruct task_struct *p;\n\t\trcu_read_lock();\n\t\tp = find_task_by_vpid(pid);\n\t\tif (p)\n\t\t\terr = posix_cpu_clock_get_task(p, which_clock, tp);\n\t\trcu_read_unlock();\n\t}\n\n\treturn err;\n}\n\n/*\n * Validate the clockid_t for a new CPU-clock timer, and initialize the timer.\n * This is called from sys_timer_create() and do_cpu_nanosleep() with the\n * new timer already all-zeros initialized.\n */\nstatic int posix_cpu_timer_create(struct k_itimer *new_timer)\n{\n\tint ret = 0;\n\tconst pid_t pid = CPUCLOCK_PID(new_timer->it_clock);\n\tstruct task_struct *p;\n\n\tif (CPUCLOCK_WHICH(new_timer->it_clock) >= CPUCLOCK_MAX)\n\t\treturn -EINVAL;\n\n\tnew_timer->kclock = &clock_posix_cpu;\n\n\tINIT_LIST_HEAD(&new_timer->it.cpu.entry);\n\n\trcu_read_lock();\n\tif (CPUCLOCK_PERTHREAD(new_timer->it_clock)) {\n\t\tif (pid == 0) {\n\t\t\tp = current;\n\t\t} else {\n\t\t\tp = find_task_by_vpid(pid);\n\t\t\tif (p && !same_thread_group(p, current))\n\t\t\t\tp = NULL;\n\t\t}\n\t} else {\n\t\tif (pid == 0) {\n\t\t\tp = current->group_leader;\n\t\t} else {\n\t\t\tp = find_task_by_vpid(pid);\n\t\t\tif (p && !has_group_leader_pid(p))\n\t\t\t\tp = NULL;\n\t\t}\n\t}\n\tnew_timer->it.cpu.task = p;\n\tif (p) {\n\t\tget_task_struct(p);\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n/*\n * Clean up a CPU-clock timer that is about to be destroyed.\n * This is called from timer deletion with the timer already locked.\n * If we return TIMER_RETRY, it's necessary to release the timer's lock\n * and try again.  (This happens when the timer is in the middle of firing.)\n */\nstatic int posix_cpu_timer_del(struct k_itimer *timer)\n{\n\tint ret = 0;\n\tunsigned long flags;\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *p = timer->it.cpu.task;\n\n\tWARN_ON_ONCE(p == NULL);\n\n\t/*\n\t * Protect against sighand release/switch in exit/exec and process/\n\t * thread timer list entry concurrent read/writes.\n\t */\n\tsighand = lock_task_sighand(p, &flags);\n\tif (unlikely(sighand == NULL)) {\n\t\t/*\n\t\t * We raced with the reaping of the task.\n\t\t * The deletion should have cleared us off the list.\n\t\t */\n\t\tWARN_ON_ONCE(!list_empty(&timer->it.cpu.entry));\n\t} else {\n\t\tif (timer->it.cpu.firing)\n\t\t\tret = TIMER_RETRY;\n\t\telse\n\t\t\tlist_del(&timer->it.cpu.entry);\n\n\t\tunlock_task_sighand(p, &flags);\n\t}\n\n\tif (!ret)\n\t\tput_task_struct(p);\n\n\treturn ret;\n}\n\nstatic void cleanup_timers_list(struct list_head *head)\n{\n\tstruct cpu_timer_list *timer, *next;\n\n\tlist_for_each_entry_safe(timer, next, head, entry)\n\t\tlist_del_init(&timer->entry);\n}\n\n/*\n * Clean out CPU timers still ticking when a thread exited.  The task\n * pointer is cleared, and the expiry time is replaced with the residual\n * time for later timer_gettime calls to return.\n * This must be called with the siglock held.\n */\nstatic void cleanup_timers(struct list_head *head)\n{\n\tcleanup_timers_list(head);\n\tcleanup_timers_list(++head);\n\tcleanup_timers_list(++head);\n}\n\n/*\n * These are both called with the siglock held, when the current thread\n * is being reaped.  When the final (leader) thread in the group is reaped,\n * posix_cpu_timers_exit_group will be called after posix_cpu_timers_exit.\n */\nvoid posix_cpu_timers_exit(struct task_struct *tsk)\n{\n\tcleanup_timers(tsk->cpu_timers);\n}\nvoid posix_cpu_timers_exit_group(struct task_struct *tsk)\n{\n\tcleanup_timers(tsk->signal->cpu_timers);\n}\n\nstatic inline int expires_gt(u64 expires, u64 new_exp)\n{\n\treturn expires == 0 || expires > new_exp;\n}\n\n/*\n * Insert the timer on the appropriate list before any timers that\n * expire later.  This must be called with the sighand lock held.\n */\nstatic void arm_timer(struct k_itimer *timer)\n{\n\tstruct task_struct *p = timer->it.cpu.task;\n\tstruct list_head *head, *listpos;\n\tstruct task_cputime *cputime_expires;\n\tstruct cpu_timer_list *const nt = &timer->it.cpu;\n\tstruct cpu_timer_list *next;\n\n\tif (CPUCLOCK_PERTHREAD(timer->it_clock)) {\n\t\thead = p->cpu_timers;\n\t\tcputime_expires = &p->cputime_expires;\n\t} else {\n\t\thead = p->signal->cpu_timers;\n\t\tcputime_expires = &p->signal->cputime_expires;\n\t}\n\thead += CPUCLOCK_WHICH(timer->it_clock);\n\n\tlistpos = head;\n\tlist_for_each_entry(next, head, entry) {\n\t\tif (nt->expires < next->expires)\n\t\t\tbreak;\n\t\tlistpos = &next->entry;\n\t}\n\tlist_add(&nt->entry, listpos);\n\n\tif (listpos == head) {\n\t\tu64 exp = nt->expires;\n\n\t\t/*\n\t\t * We are the new earliest-expiring POSIX 1.b timer, hence\n\t\t * need to update expiration cache. Take into account that\n\t\t * for process timers we share expiration cache with itimers\n\t\t * and RLIMIT_CPU and for thread timers with RLIMIT_RTTIME.\n\t\t */\n\n\t\tswitch (CPUCLOCK_WHICH(timer->it_clock)) {\n\t\tcase CPUCLOCK_PROF:\n\t\t\tif (expires_gt(cputime_expires->prof_exp, exp))\n\t\t\t\tcputime_expires->prof_exp = exp;\n\t\t\tbreak;\n\t\tcase CPUCLOCK_VIRT:\n\t\t\tif (expires_gt(cputime_expires->virt_exp, exp))\n\t\t\t\tcputime_expires->virt_exp = exp;\n\t\t\tbreak;\n\t\tcase CPUCLOCK_SCHED:\n\t\t\tif (expires_gt(cputime_expires->sched_exp, exp))\n\t\t\t\tcputime_expires->sched_exp = exp;\n\t\t\tbreak;\n\t\t}\n\t\tif (CPUCLOCK_PERTHREAD(timer->it_clock))\n\t\t\ttick_dep_set_task(p, TICK_DEP_BIT_POSIX_TIMER);\n\t\telse\n\t\t\ttick_dep_set_signal(p->signal, TICK_DEP_BIT_POSIX_TIMER);\n\t}\n}\n\n/*\n * The timer is locked, fire it and arrange for its reload.\n */\nstatic void cpu_timer_fire(struct k_itimer *timer)\n{\n\tif ((timer->it_sigev_notify & ~SIGEV_THREAD_ID) == SIGEV_NONE) {\n\t\t/*\n\t\t * User don't want any signal.\n\t\t */\n\t\ttimer->it.cpu.expires = 0;\n\t} else if (unlikely(timer->sigq == NULL)) {\n\t\t/*\n\t\t * This a special case for clock_nanosleep,\n\t\t * not a normal timer from sys_timer_create.\n\t\t */\n\t\twake_up_process(timer->it_process);\n\t\ttimer->it.cpu.expires = 0;\n\t} else if (timer->it.cpu.incr == 0) {\n\t\t/*\n\t\t * One-shot timer.  Clear it as soon as it's fired.\n\t\t */\n\t\tposix_timer_event(timer, 0);\n\t\ttimer->it.cpu.expires = 0;\n\t} else if (posix_timer_event(timer, ++timer->it_requeue_pending)) {\n\t\t/*\n\t\t * The signal did not get queued because the signal\n\t\t * was ignored, so we won't get any callback to\n\t\t * reload the timer.  But we need to keep it\n\t\t * ticking in case the signal is deliverable next time.\n\t\t */\n\t\tposix_cpu_timer_rearm(timer);\n\t\t++timer->it_requeue_pending;\n\t}\n}\n\n/*\n * Sample a process (thread group) timer for the given group_leader task.\n * Must be called with task sighand lock held for safe while_each_thread()\n * traversal.\n */\nstatic int cpu_timer_sample_group(const clockid_t which_clock,\n\t\t\t\t  struct task_struct *p, u64 *sample)\n{\n\tstruct task_cputime cputime;\n\n\tthread_group_cputimer(p, &cputime);\n\tswitch (CPUCLOCK_WHICH(which_clock)) {\n\tdefault:\n\t\treturn -EINVAL;\n\tcase CPUCLOCK_PROF:\n\t\t*sample = cputime.utime + cputime.stime;\n\t\tbreak;\n\tcase CPUCLOCK_VIRT:\n\t\t*sample = cputime.utime;\n\t\tbreak;\n\tcase CPUCLOCK_SCHED:\n\t\t*sample = cputime.sum_exec_runtime;\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\n/*\n * Guts of sys_timer_settime for CPU timers.\n * This is called with the timer locked and interrupts disabled.\n * If we return TIMER_RETRY, it's necessary to release the timer's lock\n * and try again.  (This happens when the timer is in the middle of firing.)\n */\nstatic int posix_cpu_timer_set(struct k_itimer *timer, int timer_flags,\n\t\t\t       struct itimerspec64 *new, struct itimerspec64 *old)\n{\n\tunsigned long flags;\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *p = timer->it.cpu.task;\n\tu64 old_expires, new_expires, old_incr, val;\n\tint ret;\n\n\tWARN_ON_ONCE(p == NULL);\n\n\t/*\n\t * Use the to_ktime conversion because that clamps the maximum\n\t * value to KTIME_MAX and avoid multiplication overflows.\n\t */\n\tnew_expires = ktime_to_ns(timespec64_to_ktime(new->it_value));\n\n\t/*\n\t * Protect against sighand release/switch in exit/exec and p->cpu_timers\n\t * and p->signal->cpu_timers read/write in arm_timer()\n\t */\n\tsighand = lock_task_sighand(p, &flags);\n\t/*\n\t * If p has just been reaped, we can no\n\t * longer get any information about it at all.\n\t */\n\tif (unlikely(sighand == NULL)) {\n\t\treturn -ESRCH;\n\t}\n\n\t/*\n\t * Disarm any old timer after extracting its expiry time.\n\t */\n\tlockdep_assert_irqs_disabled();\n\n\tret = 0;\n\told_incr = timer->it.cpu.incr;\n\told_expires = timer->it.cpu.expires;\n\tif (unlikely(timer->it.cpu.firing)) {\n\t\ttimer->it.cpu.firing = -1;\n\t\tret = TIMER_RETRY;\n\t} else\n\t\tlist_del_init(&timer->it.cpu.entry);\n\n\t/*\n\t * We need to sample the current value to convert the new\n\t * value from to relative and absolute, and to convert the\n\t * old value from absolute to relative.  To set a process\n\t * timer, we need a sample to balance the thread expiry\n\t * times (in arm_timer).  With an absolute time, we must\n\t * check if it's already passed.  In short, we need a sample.\n\t */\n\tif (CPUCLOCK_PERTHREAD(timer->it_clock)) {\n\t\tcpu_clock_sample(timer->it_clock, p, &val);\n\t} else {\n\t\tcpu_timer_sample_group(timer->it_clock, p, &val);\n\t}\n\n\tif (old) {\n\t\tif (old_expires == 0) {\n\t\t\told->it_value.tv_sec = 0;\n\t\t\told->it_value.tv_nsec = 0;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Update the timer in case it has\n\t\t\t * overrun already.  If it has,\n\t\t\t * we'll report it as having overrun\n\t\t\t * and with the next reloaded timer\n\t\t\t * already ticking, though we are\n\t\t\t * swallowing that pending\n\t\t\t * notification here to install the\n\t\t\t * new setting.\n\t\t\t */\n\t\t\tbump_cpu_timer(timer, val);\n\t\t\tif (val < timer->it.cpu.expires) {\n\t\t\t\told_expires = timer->it.cpu.expires - val;\n\t\t\t\told->it_value = ns_to_timespec64(old_expires);\n\t\t\t} else {\n\t\t\t\told->it_value.tv_nsec = 1;\n\t\t\t\told->it_value.tv_sec = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (unlikely(ret)) {\n\t\t/*\n\t\t * We are colliding with the timer actually firing.\n\t\t * Punt after filling in the timer's old value, and\n\t\t * disable this firing since we are already reporting\n\t\t * it as an overrun (thanks to bump_cpu_timer above).\n\t\t */\n\t\tunlock_task_sighand(p, &flags);\n\t\tgoto out;\n\t}\n\n\tif (new_expires != 0 && !(timer_flags & TIMER_ABSTIME)) {\n\t\tnew_expires += val;\n\t}\n\n\t/*\n\t * Install the new expiry time (or zero).\n\t * For a timer with no notification action, we don't actually\n\t * arm the timer (we'll just fake it for timer_gettime).\n\t */\n\ttimer->it.cpu.expires = new_expires;\n\tif (new_expires != 0 && val < new_expires) {\n\t\tarm_timer(timer);\n\t}\n\n\tunlock_task_sighand(p, &flags);\n\t/*\n\t * Install the new reload setting, and\n\t * set up the signal and overrun bookkeeping.\n\t */\n\ttimer->it.cpu.incr = timespec64_to_ns(&new->it_interval);\n\n\t/*\n\t * This acts as a modification timestamp for the timer,\n\t * so any automatic reload attempt will punt on seeing\n\t * that we have reset the timer manually.\n\t */\n\ttimer->it_requeue_pending = (timer->it_requeue_pending + 2) &\n\t\t~REQUEUE_PENDING;\n\ttimer->it_overrun_last = 0;\n\ttimer->it_overrun = -1;\n\n\tif (new_expires != 0 && !(val < new_expires)) {\n\t\t/*\n\t\t * The designated time already passed, so we notify\n\t\t * immediately, even if the thread never runs to\n\t\t * accumulate more time on this clock.\n\t\t */\n\t\tcpu_timer_fire(timer);\n\t}\n\n\tret = 0;\n out:\n\tif (old)\n\t\told->it_interval = ns_to_timespec64(old_incr);\n\n\treturn ret;\n}\n\nstatic void posix_cpu_timer_get(struct k_itimer *timer, struct itimerspec64 *itp)\n{\n\tu64 now;\n\tstruct task_struct *p = timer->it.cpu.task;\n\n\tWARN_ON_ONCE(p == NULL);\n\n\t/*\n\t * Easy part: convert the reload time.\n\t */\n\titp->it_interval = ns_to_timespec64(timer->it.cpu.incr);\n\n\tif (!timer->it.cpu.expires)\n\t\treturn;\n\n\t/*\n\t * Sample the clock to take the difference with the expiry time.\n\t */\n\tif (CPUCLOCK_PERTHREAD(timer->it_clock)) {\n\t\tcpu_clock_sample(timer->it_clock, p, &now);\n\t} else {\n\t\tstruct sighand_struct *sighand;\n\t\tunsigned long flags;\n\n\t\t/*\n\t\t * Protect against sighand release/switch in exit/exec and\n\t\t * also make timer sampling safe if it ends up calling\n\t\t * thread_group_cputime().\n\t\t */\n\t\tsighand = lock_task_sighand(p, &flags);\n\t\tif (unlikely(sighand == NULL)) {\n\t\t\t/*\n\t\t\t * The process has been reaped.\n\t\t\t * We can't even collect a sample any more.\n\t\t\t * Call the timer disarmed, nothing else to do.\n\t\t\t */\n\t\t\ttimer->it.cpu.expires = 0;\n\t\t\treturn;\n\t\t} else {\n\t\t\tcpu_timer_sample_group(timer->it_clock, p, &now);\n\t\t\tunlock_task_sighand(p, &flags);\n\t\t}\n\t}\n\n\tif (now < timer->it.cpu.expires) {\n\t\titp->it_value = ns_to_timespec64(timer->it.cpu.expires - now);\n\t} else {\n\t\t/*\n\t\t * The timer should have expired already, but the firing\n\t\t * hasn't taken place yet.  Say it's just about to expire.\n\t\t */\n\t\titp->it_value.tv_nsec = 1;\n\t\titp->it_value.tv_sec = 0;\n\t}\n}\n\nstatic unsigned long long\ncheck_timers_list(struct list_head *timers,\n\t\t  struct list_head *firing,\n\t\t  unsigned long long curr)\n{\n\tint maxfire = 20;\n\n\twhile (!list_empty(timers)) {\n\t\tstruct cpu_timer_list *t;\n\n\t\tt = list_first_entry(timers, struct cpu_timer_list, entry);\n\n\t\tif (!--maxfire || curr < t->expires)\n\t\t\treturn t->expires;\n\n\t\tt->firing = 1;\n\t\tlist_move_tail(&t->entry, firing);\n\t}\n\n\treturn 0;\n}\n\nstatic inline void check_dl_overrun(struct task_struct *tsk)\n{\n\tif (tsk->dl.dl_overrun) {\n\t\ttsk->dl.dl_overrun = 0;\n\t\t__group_send_sig_info(SIGXCPU, SEND_SIG_PRIV, tsk);\n\t}\n}\n\n/*\n * Check for any per-thread CPU timers that have fired and move them off\n * the tsk->cpu_timers[N] list onto the firing list.  Here we update the\n * tsk->it_*_expires values to reflect the remaining thread CPU timers.\n */\nstatic void check_thread_timers(struct task_struct *tsk,\n\t\t\t\tstruct list_head *firing)\n{\n\tstruct list_head *timers = tsk->cpu_timers;\n\tstruct task_cputime *tsk_expires = &tsk->cputime_expires;\n\tu64 expires;\n\tunsigned long soft;\n\n\tif (dl_task(tsk))\n\t\tcheck_dl_overrun(tsk);\n\n\t/*\n\t * If cputime_expires is zero, then there are no active\n\t * per thread CPU timers.\n\t */\n\tif (task_cputime_zero(&tsk->cputime_expires))\n\t\treturn;\n\n\texpires = check_timers_list(timers, firing, prof_ticks(tsk));\n\ttsk_expires->prof_exp = expires;\n\n\texpires = check_timers_list(++timers, firing, virt_ticks(tsk));\n\ttsk_expires->virt_exp = expires;\n\n\ttsk_expires->sched_exp = check_timers_list(++timers, firing,\n\t\t\t\t\t\t   tsk->se.sum_exec_runtime);\n\n\t/*\n\t * Check for the special case thread timers.\n\t */\n\tsoft = task_rlimit(tsk, RLIMIT_RTTIME);\n\tif (soft != RLIM_INFINITY) {\n\t\tunsigned long hard = task_rlimit_max(tsk, RLIMIT_RTTIME);\n\n\t\tif (hard != RLIM_INFINITY &&\n\t\t    tsk->rt.timeout > DIV_ROUND_UP(hard, USEC_PER_SEC/HZ)) {\n\t\t\t/*\n\t\t\t * At the hard limit, we just die.\n\t\t\t * No need to calculate anything else now.\n\t\t\t */\n\t\t\tif (print_fatal_signals) {\n\t\t\t\tpr_info(\"CPU Watchdog Timeout (hard): %s[%d]\\n\",\n\t\t\t\t\ttsk->comm, task_pid_nr(tsk));\n\t\t\t}\n\t\t\t__group_send_sig_info(SIGKILL, SEND_SIG_PRIV, tsk);\n\t\t\treturn;\n\t\t}\n\t\tif (tsk->rt.timeout > DIV_ROUND_UP(soft, USEC_PER_SEC/HZ)) {\n\t\t\t/*\n\t\t\t * At the soft limit, send a SIGXCPU every second.\n\t\t\t */\n\t\t\tif (soft < hard) {\n\t\t\t\tsoft += USEC_PER_SEC;\n\t\t\t\ttsk->signal->rlim[RLIMIT_RTTIME].rlim_cur =\n\t\t\t\t\tsoft;\n\t\t\t}\n\t\t\tif (print_fatal_signals) {\n\t\t\t\tpr_info(\"RT Watchdog Timeout (soft): %s[%d]\\n\",\n\t\t\t\t\ttsk->comm, task_pid_nr(tsk));\n\t\t\t}\n\t\t\t__group_send_sig_info(SIGXCPU, SEND_SIG_PRIV, tsk);\n\t\t}\n\t}\n\tif (task_cputime_zero(tsk_expires))\n\t\ttick_dep_clear_task(tsk, TICK_DEP_BIT_POSIX_TIMER);\n}\n\nstatic inline void stop_process_timers(struct signal_struct *sig)\n{\n\tstruct thread_group_cputimer *cputimer = &sig->cputimer;\n\n\t/* Turn off cputimer->running. This is done without locking. */\n\tWRITE_ONCE(cputimer->running, false);\n\ttick_dep_clear_signal(sig, TICK_DEP_BIT_POSIX_TIMER);\n}\n\nstatic void check_cpu_itimer(struct task_struct *tsk, struct cpu_itimer *it,\n\t\t\t     u64 *expires, u64 cur_time, int signo)\n{\n\tif (!it->expires)\n\t\treturn;\n\n\tif (cur_time >= it->expires) {\n\t\tif (it->incr)\n\t\t\tit->expires += it->incr;\n\t\telse\n\t\t\tit->expires = 0;\n\n\t\ttrace_itimer_expire(signo == SIGPROF ?\n\t\t\t\t    ITIMER_PROF : ITIMER_VIRTUAL,\n\t\t\t\t    tsk->signal->leader_pid, cur_time);\n\t\t__group_send_sig_info(signo, SEND_SIG_PRIV, tsk);\n\t}\n\n\tif (it->expires && (!*expires || it->expires < *expires))\n\t\t*expires = it->expires;\n}\n\n/*\n * Check for any per-thread CPU timers that have fired and move them\n * off the tsk->*_timers list onto the firing list.  Per-thread timers\n * have already been taken off.\n */\nstatic void check_process_timers(struct task_struct *tsk,\n\t\t\t\t struct list_head *firing)\n{\n\tstruct signal_struct *const sig = tsk->signal;\n\tu64 utime, ptime, virt_expires, prof_expires;\n\tu64 sum_sched_runtime, sched_expires;\n\tstruct list_head *timers = sig->cpu_timers;\n\tstruct task_cputime cputime;\n\tunsigned long soft;\n\n\tif (dl_task(tsk))\n\t\tcheck_dl_overrun(tsk);\n\n\t/*\n\t * If cputimer is not running, then there are no active\n\t * process wide timers (POSIX 1.b, itimers, RLIMIT_CPU).\n\t */\n\tif (!READ_ONCE(tsk->signal->cputimer.running))\n\t\treturn;\n\n        /*\n\t * Signify that a thread is checking for process timers.\n\t * Write access to this field is protected by the sighand lock.\n\t */\n\tsig->cputimer.checking_timer = true;\n\n\t/*\n\t * Collect the current process totals.\n\t */\n\tthread_group_cputimer(tsk, &cputime);\n\tutime = cputime.utime;\n\tptime = utime + cputime.stime;\n\tsum_sched_runtime = cputime.sum_exec_runtime;\n\n\tprof_expires = check_timers_list(timers, firing, ptime);\n\tvirt_expires = check_timers_list(++timers, firing, utime);\n\tsched_expires = check_timers_list(++timers, firing, sum_sched_runtime);\n\n\t/*\n\t * Check for the special case process timers.\n\t */\n\tcheck_cpu_itimer(tsk, &sig->it[CPUCLOCK_PROF], &prof_expires, ptime,\n\t\t\t SIGPROF);\n\tcheck_cpu_itimer(tsk, &sig->it[CPUCLOCK_VIRT], &virt_expires, utime,\n\t\t\t SIGVTALRM);\n\tsoft = task_rlimit(tsk, RLIMIT_CPU);\n\tif (soft != RLIM_INFINITY) {\n\t\tunsigned long psecs = div_u64(ptime, NSEC_PER_SEC);\n\t\tunsigned long hard = task_rlimit_max(tsk, RLIMIT_CPU);\n\t\tu64 x;\n\t\tif (psecs >= hard) {\n\t\t\t/*\n\t\t\t * At the hard limit, we just die.\n\t\t\t * No need to calculate anything else now.\n\t\t\t */\n\t\t\tif (print_fatal_signals) {\n\t\t\t\tpr_info(\"RT Watchdog Timeout (hard): %s[%d]\\n\",\n\t\t\t\t\ttsk->comm, task_pid_nr(tsk));\n\t\t\t}\n\t\t\t__group_send_sig_info(SIGKILL, SEND_SIG_PRIV, tsk);\n\t\t\treturn;\n\t\t}\n\t\tif (psecs >= soft) {\n\t\t\t/*\n\t\t\t * At the soft limit, send a SIGXCPU every second.\n\t\t\t */\n\t\t\tif (print_fatal_signals) {\n\t\t\t\tpr_info(\"CPU Watchdog Timeout (soft): %s[%d]\\n\",\n\t\t\t\t\ttsk->comm, task_pid_nr(tsk));\n\t\t\t}\n\t\t\t__group_send_sig_info(SIGXCPU, SEND_SIG_PRIV, tsk);\n\t\t\tif (soft < hard) {\n\t\t\t\tsoft++;\n\t\t\t\tsig->rlim[RLIMIT_CPU].rlim_cur = soft;\n\t\t\t}\n\t\t}\n\t\tx = soft * NSEC_PER_SEC;\n\t\tif (!prof_expires || x < prof_expires)\n\t\t\tprof_expires = x;\n\t}\n\n\tsig->cputime_expires.prof_exp = prof_expires;\n\tsig->cputime_expires.virt_exp = virt_expires;\n\tsig->cputime_expires.sched_exp = sched_expires;\n\tif (task_cputime_zero(&sig->cputime_expires))\n\t\tstop_process_timers(sig);\n\n\tsig->cputimer.checking_timer = false;\n}\n\n/*\n * This is called from the signal code (via posixtimer_rearm)\n * when the last timer signal was delivered and we have to reload the timer.\n */\nstatic void posix_cpu_timer_rearm(struct k_itimer *timer)\n{\n\tstruct sighand_struct *sighand;\n\tunsigned long flags;\n\tstruct task_struct *p = timer->it.cpu.task;\n\tu64 now;\n\n\tWARN_ON_ONCE(p == NULL);\n\n\t/*\n\t * Fetch the current sample and update the timer's expiry time.\n\t */\n\tif (CPUCLOCK_PERTHREAD(timer->it_clock)) {\n\t\tcpu_clock_sample(timer->it_clock, p, &now);\n\t\tbump_cpu_timer(timer, now);\n\t\tif (unlikely(p->exit_state))\n\t\t\treturn;\n\n\t\t/* Protect timer list r/w in arm_timer() */\n\t\tsighand = lock_task_sighand(p, &flags);\n\t\tif (!sighand)\n\t\t\treturn;\n\t} else {\n\t\t/*\n\t\t * Protect arm_timer() and timer sampling in case of call to\n\t\t * thread_group_cputime().\n\t\t */\n\t\tsighand = lock_task_sighand(p, &flags);\n\t\tif (unlikely(sighand == NULL)) {\n\t\t\t/*\n\t\t\t * The process has been reaped.\n\t\t\t * We can't even collect a sample any more.\n\t\t\t */\n\t\t\ttimer->it.cpu.expires = 0;\n\t\t\treturn;\n\t\t} else if (unlikely(p->exit_state) && thread_group_empty(p)) {\n\t\t\t/* If the process is dying, no need to rearm */\n\t\t\tgoto unlock;\n\t\t}\n\t\tcpu_timer_sample_group(timer->it_clock, p, &now);\n\t\tbump_cpu_timer(timer, now);\n\t\t/* Leave the sighand locked for the call below.  */\n\t}\n\n\t/*\n\t * Now re-arm for the new expiry time.\n\t */\n\tlockdep_assert_irqs_disabled();\n\tarm_timer(timer);\nunlock:\n\tunlock_task_sighand(p, &flags);\n}\n\n/**\n * task_cputime_expired - Compare two task_cputime entities.\n *\n * @sample:\tThe task_cputime structure to be checked for expiration.\n * @expires:\tExpiration times, against which @sample will be checked.\n *\n * Checks @sample against @expires to see if any field of @sample has expired.\n * Returns true if any field of the former is greater than the corresponding\n * field of the latter if the latter field is set.  Otherwise returns false.\n */\nstatic inline int task_cputime_expired(const struct task_cputime *sample,\n\t\t\t\t\tconst struct task_cputime *expires)\n{\n\tif (expires->utime && sample->utime >= expires->utime)\n\t\treturn 1;\n\tif (expires->stime && sample->utime + sample->stime >= expires->stime)\n\t\treturn 1;\n\tif (expires->sum_exec_runtime != 0 &&\n\t    sample->sum_exec_runtime >= expires->sum_exec_runtime)\n\t\treturn 1;\n\treturn 0;\n}\n\n/**\n * fastpath_timer_check - POSIX CPU timers fast path.\n *\n * @tsk:\tThe task (thread) being checked.\n *\n * Check the task and thread group timers.  If both are zero (there are no\n * timers set) return false.  Otherwise snapshot the task and thread group\n * timers and compare them with the corresponding expiration times.  Return\n * true if a timer has expired, else return false.\n */\nstatic inline int fastpath_timer_check(struct task_struct *tsk)\n{\n\tstruct signal_struct *sig;\n\n\tif (!task_cputime_zero(&tsk->cputime_expires)) {\n\t\tstruct task_cputime task_sample;\n\n\t\ttask_cputime(tsk, &task_sample.utime, &task_sample.stime);\n\t\ttask_sample.sum_exec_runtime = tsk->se.sum_exec_runtime;\n\t\tif (task_cputime_expired(&task_sample, &tsk->cputime_expires))\n\t\t\treturn 1;\n\t}\n\n\tsig = tsk->signal;\n\t/*\n\t * Check if thread group timers expired when the cputimer is\n\t * running and no other thread in the group is already checking\n\t * for thread group cputimers. These fields are read without the\n\t * sighand lock. However, this is fine because this is meant to\n\t * be a fastpath heuristic to determine whether we should try to\n\t * acquire the sighand lock to check/handle timers.\n\t *\n\t * In the worst case scenario, if 'running' or 'checking_timer' gets\n\t * set but the current thread doesn't see the change yet, we'll wait\n\t * until the next thread in the group gets a scheduler interrupt to\n\t * handle the timer. This isn't an issue in practice because these\n\t * types of delays with signals actually getting sent are expected.\n\t */\n\tif (READ_ONCE(sig->cputimer.running) &&\n\t    !READ_ONCE(sig->cputimer.checking_timer)) {\n\t\tstruct task_cputime group_sample;\n\n\t\tsample_cputime_atomic(&group_sample, &sig->cputimer.cputime_atomic);\n\n\t\tif (task_cputime_expired(&group_sample, &sig->cputime_expires))\n\t\t\treturn 1;\n\t}\n\n\tif (dl_task(tsk) && tsk->dl.dl_overrun)\n\t\treturn 1;\n\n\treturn 0;\n}\n\n/*\n * This is called from the timer interrupt handler.  The irq handler has\n * already updated our counts.  We need to check if any timers fire now.\n * Interrupts are disabled.\n */\nvoid run_posix_cpu_timers(struct task_struct *tsk)\n{\n\tLIST_HEAD(firing);\n\tstruct k_itimer *timer, *next;\n\tunsigned long flags;\n\n\tlockdep_assert_irqs_disabled();\n\n\t/*\n\t * The fast path checks that there are no expired thread or thread\n\t * group timers.  If that's so, just return.\n\t */\n\tif (!fastpath_timer_check(tsk))\n\t\treturn;\n\n\tif (!lock_task_sighand(tsk, &flags))\n\t\treturn;\n\t/*\n\t * Here we take off tsk->signal->cpu_timers[N] and\n\t * tsk->cpu_timers[N] all the timers that are firing, and\n\t * put them on the firing list.\n\t */\n\tcheck_thread_timers(tsk, &firing);\n\n\tcheck_process_timers(tsk, &firing);\n\n\t/*\n\t * We must release these locks before taking any timer's lock.\n\t * There is a potential race with timer deletion here, as the\n\t * siglock now protects our private firing list.  We have set\n\t * the firing flag in each timer, so that a deletion attempt\n\t * that gets the timer lock before we do will give it up and\n\t * spin until we've taken care of that timer below.\n\t */\n\tunlock_task_sighand(tsk, &flags);\n\n\t/*\n\t * Now that all the timers on our list have the firing flag,\n\t * no one will touch their list entries but us.  We'll take\n\t * each timer's lock before clearing its firing flag, so no\n\t * timer call will interfere.\n\t */\n\tlist_for_each_entry_safe(timer, next, &firing, it.cpu.entry) {\n\t\tint cpu_firing;\n\n\t\tspin_lock(&timer->it_lock);\n\t\tlist_del_init(&timer->it.cpu.entry);\n\t\tcpu_firing = timer->it.cpu.firing;\n\t\ttimer->it.cpu.firing = 0;\n\t\t/*\n\t\t * The firing flag is -1 if we collided with a reset\n\t\t * of the timer, which already reported this\n\t\t * almost-firing as an overrun.  So don't generate an event.\n\t\t */\n\t\tif (likely(cpu_firing >= 0))\n\t\t\tcpu_timer_fire(timer);\n\t\tspin_unlock(&timer->it_lock);\n\t}\n}\n\n/*\n * Set one of the process-wide special case CPU timers or RLIMIT_CPU.\n * The tsk->sighand->siglock must be held by the caller.\n */\nvoid set_process_cpu_timer(struct task_struct *tsk, unsigned int clock_idx,\n\t\t\t   u64 *newval, u64 *oldval)\n{\n\tu64 now;\n\tint ret;\n\n\tWARN_ON_ONCE(clock_idx == CPUCLOCK_SCHED);\n\tret = cpu_timer_sample_group(clock_idx, tsk, &now);\n\n\tif (oldval && ret != -EINVAL) {\n\t\t/*\n\t\t * We are setting itimer. The *oldval is absolute and we update\n\t\t * it to be relative, *newval argument is relative and we update\n\t\t * it to be absolute.\n\t\t */\n\t\tif (*oldval) {\n\t\t\tif (*oldval <= now) {\n\t\t\t\t/* Just about to fire. */\n\t\t\t\t*oldval = TICK_NSEC;\n\t\t\t} else {\n\t\t\t\t*oldval -= now;\n\t\t\t}\n\t\t}\n\n\t\tif (!*newval)\n\t\t\treturn;\n\t\t*newval += now;\n\t}\n\n\t/*\n\t * Update expiration cache if we are the earliest timer, or eventually\n\t * RLIMIT_CPU limit is earlier than prof_exp cpu timer expire.\n\t */\n\tswitch (clock_idx) {\n\tcase CPUCLOCK_PROF:\n\t\tif (expires_gt(tsk->signal->cputime_expires.prof_exp, *newval))\n\t\t\ttsk->signal->cputime_expires.prof_exp = *newval;\n\t\tbreak;\n\tcase CPUCLOCK_VIRT:\n\t\tif (expires_gt(tsk->signal->cputime_expires.virt_exp, *newval))\n\t\t\ttsk->signal->cputime_expires.virt_exp = *newval;\n\t\tbreak;\n\t}\n\n\ttick_dep_set_signal(tsk->signal, TICK_DEP_BIT_POSIX_TIMER);\n}\n\nstatic int do_cpu_nanosleep(const clockid_t which_clock, int flags,\n\t\t\t    const struct timespec64 *rqtp)\n{\n\tstruct itimerspec64 it;\n\tstruct k_itimer timer;\n\tu64 expires;\n\tint error;\n\n\t/*\n\t * Set up a temporary timer and then wait for it to go off.\n\t */\n\tmemset(&timer, 0, sizeof timer);\n\tspin_lock_init(&timer.it_lock);\n\ttimer.it_clock = which_clock;\n\ttimer.it_overrun = -1;\n\terror = posix_cpu_timer_create(&timer);\n\ttimer.it_process = current;\n\tif (!error) {\n\t\tstatic struct itimerspec64 zero_it;\n\t\tstruct restart_block *restart;\n\n\t\tmemset(&it, 0, sizeof(it));\n\t\tit.it_value = *rqtp;\n\n\t\tspin_lock_irq(&timer.it_lock);\n\t\terror = posix_cpu_timer_set(&timer, flags, &it, NULL);\n\t\tif (error) {\n\t\t\tspin_unlock_irq(&timer.it_lock);\n\t\t\treturn error;\n\t\t}\n\n\t\twhile (!signal_pending(current)) {\n\t\t\tif (timer.it.cpu.expires == 0) {\n\t\t\t\t/*\n\t\t\t\t * Our timer fired and was reset, below\n\t\t\t\t * deletion can not fail.\n\t\t\t\t */\n\t\t\t\tposix_cpu_timer_del(&timer);\n\t\t\t\tspin_unlock_irq(&timer.it_lock);\n\t\t\t\treturn 0;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Block until cpu_timer_fire (or a signal) wakes us.\n\t\t\t */\n\t\t\t__set_current_state(TASK_INTERRUPTIBLE);\n\t\t\tspin_unlock_irq(&timer.it_lock);\n\t\t\tschedule();\n\t\t\tspin_lock_irq(&timer.it_lock);\n\t\t}\n\n\t\t/*\n\t\t * We were interrupted by a signal.\n\t\t */\n\t\texpires = timer.it.cpu.expires;\n\t\terror = posix_cpu_timer_set(&timer, 0, &zero_it, &it);\n\t\tif (!error) {\n\t\t\t/*\n\t\t\t * Timer is now unarmed, deletion can not fail.\n\t\t\t */\n\t\t\tposix_cpu_timer_del(&timer);\n\t\t}\n\t\tspin_unlock_irq(&timer.it_lock);\n\n\t\twhile (error == TIMER_RETRY) {\n\t\t\t/*\n\t\t\t * We need to handle case when timer was or is in the\n\t\t\t * middle of firing. In other cases we already freed\n\t\t\t * resources.\n\t\t\t */\n\t\t\tspin_lock_irq(&timer.it_lock);\n\t\t\terror = posix_cpu_timer_del(&timer);\n\t\t\tspin_unlock_irq(&timer.it_lock);\n\t\t}\n\n\t\tif ((it.it_value.tv_sec | it.it_value.tv_nsec) == 0) {\n\t\t\t/*\n\t\t\t * It actually did fire already.\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\n\t\terror = -ERESTART_RESTARTBLOCK;\n\t\t/*\n\t\t * Report back to the user the time still remaining.\n\t\t */\n\t\trestart = &current->restart_block;\n\t\trestart->nanosleep.expires = expires;\n\t\tif (restart->nanosleep.type != TT_NONE)\n\t\t\terror = nanosleep_copyout(restart, &it.it_value);\n\t}\n\n\treturn error;\n}\n\nstatic long posix_cpu_nsleep_restart(struct restart_block *restart_block);\n\nstatic int posix_cpu_nsleep(const clockid_t which_clock, int flags,\n\t\t\t    const struct timespec64 *rqtp)\n{\n\tstruct restart_block *restart_block = &current->restart_block;\n\tint error;\n\n\t/*\n\t * Diagnose required errors first.\n\t */\n\tif (CPUCLOCK_PERTHREAD(which_clock) &&\n\t    (CPUCLOCK_PID(which_clock) == 0 ||\n\t     CPUCLOCK_PID(which_clock) == task_pid_vnr(current)))\n\t\treturn -EINVAL;\n\n\terror = do_cpu_nanosleep(which_clock, flags, rqtp);\n\n\tif (error == -ERESTART_RESTARTBLOCK) {\n\n\t\tif (flags & TIMER_ABSTIME)\n\t\t\treturn -ERESTARTNOHAND;\n\n\t\trestart_block->fn = posix_cpu_nsleep_restart;\n\t\trestart_block->nanosleep.clockid = which_clock;\n\t}\n\treturn error;\n}\n\nstatic long posix_cpu_nsleep_restart(struct restart_block *restart_block)\n{\n\tclockid_t which_clock = restart_block->nanosleep.clockid;\n\tstruct timespec64 t;\n\n\tt = ns_to_timespec64(restart_block->nanosleep.expires);\n\n\treturn do_cpu_nanosleep(which_clock, TIMER_ABSTIME, &t);\n}\n\n#define PROCESS_CLOCK\tmake_process_cpuclock(0, CPUCLOCK_SCHED)\n#define THREAD_CLOCK\tmake_thread_cpuclock(0, CPUCLOCK_SCHED)\n\nstatic int process_cpu_clock_getres(const clockid_t which_clock,\n\t\t\t\t    struct timespec64 *tp)\n{\n\treturn posix_cpu_clock_getres(PROCESS_CLOCK, tp);\n}\nstatic int process_cpu_clock_get(const clockid_t which_clock,\n\t\t\t\t struct timespec64 *tp)\n{\n\treturn posix_cpu_clock_get(PROCESS_CLOCK, tp);\n}\nstatic int process_cpu_timer_create(struct k_itimer *timer)\n{\n\ttimer->it_clock = PROCESS_CLOCK;\n\treturn posix_cpu_timer_create(timer);\n}\nstatic int process_cpu_nsleep(const clockid_t which_clock, int flags,\n\t\t\t      const struct timespec64 *rqtp)\n{\n\treturn posix_cpu_nsleep(PROCESS_CLOCK, flags, rqtp);\n}\nstatic int thread_cpu_clock_getres(const clockid_t which_clock,\n\t\t\t\t   struct timespec64 *tp)\n{\n\treturn posix_cpu_clock_getres(THREAD_CLOCK, tp);\n}\nstatic int thread_cpu_clock_get(const clockid_t which_clock,\n\t\t\t\tstruct timespec64 *tp)\n{\n\treturn posix_cpu_clock_get(THREAD_CLOCK, tp);\n}\nstatic int thread_cpu_timer_create(struct k_itimer *timer)\n{\n\ttimer->it_clock = THREAD_CLOCK;\n\treturn posix_cpu_timer_create(timer);\n}\n\nconst struct k_clock clock_posix_cpu = {\n\t.clock_getres\t= posix_cpu_clock_getres,\n\t.clock_set\t= posix_cpu_clock_set,\n\t.clock_get\t= posix_cpu_clock_get,\n\t.timer_create\t= posix_cpu_timer_create,\n\t.nsleep\t\t= posix_cpu_nsleep,\n\t.timer_set\t= posix_cpu_timer_set,\n\t.timer_del\t= posix_cpu_timer_del,\n\t.timer_get\t= posix_cpu_timer_get,\n\t.timer_rearm\t= posix_cpu_timer_rearm,\n};\n\nconst struct k_clock clock_process = {\n\t.clock_getres\t= process_cpu_clock_getres,\n\t.clock_get\t= process_cpu_clock_get,\n\t.timer_create\t= process_cpu_timer_create,\n\t.nsleep\t\t= process_cpu_nsleep,\n};\n\nconst struct k_clock clock_thread = {\n\t.clock_getres\t= thread_cpu_clock_getres,\n\t.clock_get\t= thread_cpu_clock_get,\n\t.timer_create\t= thread_cpu_timer_create,\n};\n", "/*\n * linux/kernel/posix-timers.c\n *\n *\n * 2002-10-15  Posix Clocks & timers\n *                           by George Anzinger george@mvista.com\n *\n *\t\t\t     Copyright (C) 2002 2003 by MontaVista Software.\n *\n * 2004-06-01  Fix CLOCK_REALTIME clock/timer TIMER_ABSTIME bug.\n *\t\t\t     Copyright (C) 2004 Boris Hu\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or (at\n * your option) any later version.\n *\n * This program is distributed in the hope that it will be useful, but\n * WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n * General Public License for more details.\n\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.\n *\n * MontaVista Software | 1237 East Arques Avenue | Sunnyvale | CA 94085 | USA\n */\n\n/* These are all the functions necessary to implement\n * POSIX clocks & timers\n */\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/slab.h>\n#include <linux/time.h>\n#include <linux/mutex.h>\n#include <linux/sched/task.h>\n\n#include <linux/uaccess.h>\n#include <linux/list.h>\n#include <linux/init.h>\n#include <linux/compiler.h>\n#include <linux/hash.h>\n#include <linux/posix-clock.h>\n#include <linux/posix-timers.h>\n#include <linux/syscalls.h>\n#include <linux/wait.h>\n#include <linux/workqueue.h>\n#include <linux/export.h>\n#include <linux/hashtable.h>\n#include <linux/compat.h>\n#include <linux/nospec.h>\n\n#include \"timekeeping.h\"\n#include \"posix-timers.h\"\n\n/*\n * Management arrays for POSIX timers. Timers are now kept in static hash table\n * with 512 entries.\n * Timer ids are allocated by local routine, which selects proper hash head by\n * key, constructed from current->signal address and per signal struct counter.\n * This keeps timer ids unique per process, but now they can intersect between\n * processes.\n */\n\n/*\n * Lets keep our timers in a slab cache :-)\n */\nstatic struct kmem_cache *posix_timers_cache;\n\nstatic DEFINE_HASHTABLE(posix_timers_hashtable, 9);\nstatic DEFINE_SPINLOCK(hash_lock);\n\nstatic const struct k_clock * const posix_clocks[];\nstatic const struct k_clock *clockid_to_kclock(const clockid_t id);\nstatic const struct k_clock clock_realtime, clock_monotonic;\n\n/*\n * we assume that the new SIGEV_THREAD_ID shares no bits with the other\n * SIGEV values.  Here we put out an error if this assumption fails.\n */\n#if SIGEV_THREAD_ID != (SIGEV_THREAD_ID & \\\n                       ~(SIGEV_SIGNAL | SIGEV_NONE | SIGEV_THREAD))\n#error \"SIGEV_THREAD_ID must not share bit with other SIGEV values!\"\n#endif\n\n/*\n * parisc wants ENOTSUP instead of EOPNOTSUPP\n */\n#ifndef ENOTSUP\n# define ENANOSLEEP_NOTSUP EOPNOTSUPP\n#else\n# define ENANOSLEEP_NOTSUP ENOTSUP\n#endif\n\n/*\n * The timer ID is turned into a timer address by idr_find().\n * Verifying a valid ID consists of:\n *\n * a) checking that idr_find() returns other than -1.\n * b) checking that the timer id matches the one in the timer itself.\n * c) that the timer owner is in the callers thread group.\n */\n\n/*\n * CLOCKs: The POSIX standard calls for a couple of clocks and allows us\n *\t    to implement others.  This structure defines the various\n *\t    clocks.\n *\n * RESOLUTION: Clock resolution is used to round up timer and interval\n *\t    times, NOT to report clock times, which are reported with as\n *\t    much resolution as the system can muster.  In some cases this\n *\t    resolution may depend on the underlying clock hardware and\n *\t    may not be quantifiable until run time, and only then is the\n *\t    necessary code is written.\tThe standard says we should say\n *\t    something about this issue in the documentation...\n *\n * FUNCTIONS: The CLOCKs structure defines possible functions to\n *\t    handle various clock functions.\n *\n *\t    The standard POSIX timer management code assumes the\n *\t    following: 1.) The k_itimer struct (sched.h) is used for\n *\t    the timer.  2.) The list, it_lock, it_clock, it_id and\n *\t    it_pid fields are not modified by timer code.\n *\n * Permissions: It is assumed that the clock_settime() function defined\n *\t    for each clock will take care of permission checks.\t Some\n *\t    clocks may be set able by any user (i.e. local process\n *\t    clocks) others not.\t Currently the only set able clock we\n *\t    have is CLOCK_REALTIME and its high res counter part, both of\n *\t    which we beg off on and pass to do_sys_settimeofday().\n */\nstatic struct k_itimer *__lock_timer(timer_t timer_id, unsigned long *flags);\n\n#define lock_timer(tid, flags)\t\t\t\t\t\t   \\\n({\tstruct k_itimer *__timr;\t\t\t\t\t   \\\n\t__cond_lock(&__timr->it_lock, __timr = __lock_timer(tid, flags));  \\\n\t__timr;\t\t\t\t\t\t\t\t   \\\n})\n\nstatic int hash(struct signal_struct *sig, unsigned int nr)\n{\n\treturn hash_32(hash32_ptr(sig) ^ nr, HASH_BITS(posix_timers_hashtable));\n}\n\nstatic struct k_itimer *__posix_timers_find(struct hlist_head *head,\n\t\t\t\t\t    struct signal_struct *sig,\n\t\t\t\t\t    timer_t id)\n{\n\tstruct k_itimer *timer;\n\n\thlist_for_each_entry_rcu(timer, head, t_hash) {\n\t\tif ((timer->it_signal == sig) && (timer->it_id == id))\n\t\t\treturn timer;\n\t}\n\treturn NULL;\n}\n\nstatic struct k_itimer *posix_timer_by_id(timer_t id)\n{\n\tstruct signal_struct *sig = current->signal;\n\tstruct hlist_head *head = &posix_timers_hashtable[hash(sig, id)];\n\n\treturn __posix_timers_find(head, sig, id);\n}\n\nstatic int posix_timer_add(struct k_itimer *timer)\n{\n\tstruct signal_struct *sig = current->signal;\n\tint first_free_id = sig->posix_timer_id;\n\tstruct hlist_head *head;\n\tint ret = -ENOENT;\n\n\tdo {\n\t\tspin_lock(&hash_lock);\n\t\thead = &posix_timers_hashtable[hash(sig, sig->posix_timer_id)];\n\t\tif (!__posix_timers_find(head, sig, sig->posix_timer_id)) {\n\t\t\thlist_add_head_rcu(&timer->t_hash, head);\n\t\t\tret = sig->posix_timer_id;\n\t\t}\n\t\tif (++sig->posix_timer_id < 0)\n\t\t\tsig->posix_timer_id = 0;\n\t\tif ((sig->posix_timer_id == first_free_id) && (ret == -ENOENT))\n\t\t\t/* Loop over all possible ids completed */\n\t\t\tret = -EAGAIN;\n\t\tspin_unlock(&hash_lock);\n\t} while (ret == -ENOENT);\n\treturn ret;\n}\n\nstatic inline void unlock_timer(struct k_itimer *timr, unsigned long flags)\n{\n\tspin_unlock_irqrestore(&timr->it_lock, flags);\n}\n\n/* Get clock_realtime */\nstatic int posix_clock_realtime_get(clockid_t which_clock, struct timespec64 *tp)\n{\n\tktime_get_real_ts64(tp);\n\treturn 0;\n}\n\n/* Set clock_realtime */\nstatic int posix_clock_realtime_set(const clockid_t which_clock,\n\t\t\t\t    const struct timespec64 *tp)\n{\n\treturn do_sys_settimeofday64(tp, NULL);\n}\n\nstatic int posix_clock_realtime_adj(const clockid_t which_clock,\n\t\t\t\t    struct timex *t)\n{\n\treturn do_adjtimex(t);\n}\n\n/*\n * Get monotonic time for posix timers\n */\nstatic int posix_ktime_get_ts(clockid_t which_clock, struct timespec64 *tp)\n{\n\tktime_get_ts64(tp);\n\treturn 0;\n}\n\n/*\n * Get monotonic-raw time for posix timers\n */\nstatic int posix_get_monotonic_raw(clockid_t which_clock, struct timespec64 *tp)\n{\n\tktime_get_raw_ts64(tp);\n\treturn 0;\n}\n\n\nstatic int posix_get_realtime_coarse(clockid_t which_clock, struct timespec64 *tp)\n{\n\tktime_get_coarse_real_ts64(tp);\n\treturn 0;\n}\n\nstatic int posix_get_monotonic_coarse(clockid_t which_clock,\n\t\t\t\t\t\tstruct timespec64 *tp)\n{\n\tktime_get_coarse_ts64(tp);\n\treturn 0;\n}\n\nstatic int posix_get_coarse_res(const clockid_t which_clock, struct timespec64 *tp)\n{\n\t*tp = ktime_to_timespec64(KTIME_LOW_RES);\n\treturn 0;\n}\n\nstatic int posix_get_boottime(const clockid_t which_clock, struct timespec64 *tp)\n{\n\tktime_get_boottime_ts64(tp);\n\treturn 0;\n}\n\nstatic int posix_get_tai(clockid_t which_clock, struct timespec64 *tp)\n{\n\tktime_get_clocktai_ts64(tp);\n\treturn 0;\n}\n\nstatic int posix_get_hrtimer_res(clockid_t which_clock, struct timespec64 *tp)\n{\n\ttp->tv_sec = 0;\n\ttp->tv_nsec = hrtimer_resolution;\n\treturn 0;\n}\n\n/*\n * Initialize everything, well, just everything in Posix clocks/timers ;)\n */\nstatic __init int init_posix_timers(void)\n{\n\tposix_timers_cache = kmem_cache_create(\"posix_timers_cache\",\n\t\t\t\t\tsizeof (struct k_itimer), 0, SLAB_PANIC,\n\t\t\t\t\tNULL);\n\treturn 0;\n}\n__initcall(init_posix_timers);\n\n/*\n * The siginfo si_overrun field and the return value of timer_getoverrun(2)\n * are of type int. Clamp the overrun value to INT_MAX\n */\nstatic inline int timer_overrun_to_int(struct k_itimer *timr, int baseval)\n{\n\ts64 sum = timr->it_overrun_last + (s64)baseval;\n\n\treturn sum > (s64)INT_MAX ? INT_MAX : (int)sum;\n}\n\nstatic void common_hrtimer_rearm(struct k_itimer *timr)\n{\n\tstruct hrtimer *timer = &timr->it.real.timer;\n\n\tif (!timr->it_interval)\n\t\treturn;\n\n\ttimr->it_overrun += hrtimer_forward(timer, timer->base->get_time(),\n\t\t\t\t\t    timr->it_interval);\n\thrtimer_restart(timer);\n}\n\n/*\n * This function is exported for use by the signal deliver code.  It is\n * called just prior to the info block being released and passes that\n * block to us.  It's function is to update the overrun entry AND to\n * restart the timer.  It should only be called if the timer is to be\n * restarted (i.e. we have flagged this in the sys_private entry of the\n * info block).\n *\n * To protect against the timer going away while the interrupt is queued,\n * we require that the it_requeue_pending flag be set.\n */\nvoid posixtimer_rearm(struct siginfo *info)\n{\n\tstruct k_itimer *timr;\n\tunsigned long flags;\n\n\ttimr = lock_timer(info->si_tid, &flags);\n\tif (!timr)\n\t\treturn;\n\n\tif (timr->it_requeue_pending == info->si_sys_private) {\n\t\ttimr->kclock->timer_rearm(timr);\n\n\t\ttimr->it_active = 1;\n\t\ttimr->it_overrun_last = timr->it_overrun;\n\t\ttimr->it_overrun = -1LL;\n\t\t++timr->it_requeue_pending;\n\n\t\tinfo->si_overrun = timer_overrun_to_int(timr, info->si_overrun);\n\t}\n\n\tunlock_timer(timr, flags);\n}\n\nint posix_timer_event(struct k_itimer *timr, int si_private)\n{\n\tstruct task_struct *task;\n\tint shared, ret = -1;\n\t/*\n\t * FIXME: if ->sigq is queued we can race with\n\t * dequeue_signal()->posixtimer_rearm().\n\t *\n\t * If dequeue_signal() sees the \"right\" value of\n\t * si_sys_private it calls posixtimer_rearm().\n\t * We re-queue ->sigq and drop ->it_lock().\n\t * posixtimer_rearm() locks the timer\n\t * and re-schedules it while ->sigq is pending.\n\t * Not really bad, but not that we want.\n\t */\n\ttimr->sigq->info.si_sys_private = si_private;\n\n\trcu_read_lock();\n\ttask = pid_task(timr->it_pid, PIDTYPE_PID);\n\tif (task) {\n\t\tshared = !(timr->it_sigev_notify & SIGEV_THREAD_ID);\n\t\tret = send_sigqueue(timr->sigq, task, shared);\n\t}\n\trcu_read_unlock();\n\t/* If we failed to send the signal the timer stops. */\n\treturn ret > 0;\n}\n\n/*\n * This function gets called when a POSIX.1b interval timer expires.  It\n * is used as a callback from the kernel internal timer.  The\n * run_timer_list code ALWAYS calls with interrupts on.\n\n * This code is for CLOCK_REALTIME* and CLOCK_MONOTONIC* timers.\n */\nstatic enum hrtimer_restart posix_timer_fn(struct hrtimer *timer)\n{\n\tstruct k_itimer *timr;\n\tunsigned long flags;\n\tint si_private = 0;\n\tenum hrtimer_restart ret = HRTIMER_NORESTART;\n\n\ttimr = container_of(timer, struct k_itimer, it.real.timer);\n\tspin_lock_irqsave(&timr->it_lock, flags);\n\n\ttimr->it_active = 0;\n\tif (timr->it_interval != 0)\n\t\tsi_private = ++timr->it_requeue_pending;\n\n\tif (posix_timer_event(timr, si_private)) {\n\t\t/*\n\t\t * signal was not sent because of sig_ignor\n\t\t * we will not get a call back to restart it AND\n\t\t * it should be restarted.\n\t\t */\n\t\tif (timr->it_interval != 0) {\n\t\t\tktime_t now = hrtimer_cb_get_time(timer);\n\n\t\t\t/*\n\t\t\t * FIXME: What we really want, is to stop this\n\t\t\t * timer completely and restart it in case the\n\t\t\t * SIG_IGN is removed. This is a non trivial\n\t\t\t * change which involves sighand locking\n\t\t\t * (sigh !), which we don't want to do late in\n\t\t\t * the release cycle.\n\t\t\t *\n\t\t\t * For now we just let timers with an interval\n\t\t\t * less than a jiffie expire every jiffie to\n\t\t\t * avoid softirq starvation in case of SIG_IGN\n\t\t\t * and a very small interval, which would put\n\t\t\t * the timer right back on the softirq pending\n\t\t\t * list. By moving now ahead of time we trick\n\t\t\t * hrtimer_forward() to expire the timer\n\t\t\t * later, while we still maintain the overrun\n\t\t\t * accuracy, but have some inconsistency in\n\t\t\t * the timer_gettime() case. This is at least\n\t\t\t * better than a starved softirq. A more\n\t\t\t * complex fix which solves also another related\n\t\t\t * inconsistency is already in the pipeline.\n\t\t\t */\n#ifdef CONFIG_HIGH_RES_TIMERS\n\t\t\t{\n\t\t\t\tktime_t kj = NSEC_PER_SEC / HZ;\n\n\t\t\t\tif (timr->it_interval < kj)\n\t\t\t\t\tnow = ktime_add(now, kj);\n\t\t\t}\n#endif\n\t\t\ttimr->it_overrun += hrtimer_forward(timer, now,\n\t\t\t\t\t\t\t    timr->it_interval);\n\t\t\tret = HRTIMER_RESTART;\n\t\t\t++timr->it_requeue_pending;\n\t\t\ttimr->it_active = 1;\n\t\t}\n\t}\n\n\tunlock_timer(timr, flags);\n\treturn ret;\n}\n\nstatic struct pid *good_sigevent(sigevent_t * event)\n{\n\tstruct task_struct *rtn = current->group_leader;\n\n\tswitch (event->sigev_notify) {\n\tcase SIGEV_SIGNAL | SIGEV_THREAD_ID:\n\t\trtn = find_task_by_vpid(event->sigev_notify_thread_id);\n\t\tif (!rtn || !same_thread_group(rtn, current))\n\t\t\treturn NULL;\n\t\t/* FALLTHRU */\n\tcase SIGEV_SIGNAL:\n\tcase SIGEV_THREAD:\n\t\tif (event->sigev_signo <= 0 || event->sigev_signo > SIGRTMAX)\n\t\t\treturn NULL;\n\t\t/* FALLTHRU */\n\tcase SIGEV_NONE:\n\t\treturn task_pid(rtn);\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic struct k_itimer * alloc_posix_timer(void)\n{\n\tstruct k_itimer *tmr;\n\ttmr = kmem_cache_zalloc(posix_timers_cache, GFP_KERNEL);\n\tif (!tmr)\n\t\treturn tmr;\n\tif (unlikely(!(tmr->sigq = sigqueue_alloc()))) {\n\t\tkmem_cache_free(posix_timers_cache, tmr);\n\t\treturn NULL;\n\t}\n\tclear_siginfo(&tmr->sigq->info);\n\treturn tmr;\n}\n\nstatic void k_itimer_rcu_free(struct rcu_head *head)\n{\n\tstruct k_itimer *tmr = container_of(head, struct k_itimer, it.rcu);\n\n\tkmem_cache_free(posix_timers_cache, tmr);\n}\n\n#define IT_ID_SET\t1\n#define IT_ID_NOT_SET\t0\nstatic void release_posix_timer(struct k_itimer *tmr, int it_id_set)\n{\n\tif (it_id_set) {\n\t\tunsigned long flags;\n\t\tspin_lock_irqsave(&hash_lock, flags);\n\t\thlist_del_rcu(&tmr->t_hash);\n\t\tspin_unlock_irqrestore(&hash_lock, flags);\n\t}\n\tput_pid(tmr->it_pid);\n\tsigqueue_free(tmr->sigq);\n\tcall_rcu(&tmr->it.rcu, k_itimer_rcu_free);\n}\n\nstatic int common_timer_create(struct k_itimer *new_timer)\n{\n\thrtimer_init(&new_timer->it.real.timer, new_timer->it_clock, 0);\n\treturn 0;\n}\n\n/* Create a POSIX.1b interval timer. */\nstatic int do_timer_create(clockid_t which_clock, struct sigevent *event,\n\t\t\t   timer_t __user *created_timer_id)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct k_itimer *new_timer;\n\tint error, new_timer_id;\n\tint it_id_set = IT_ID_NOT_SET;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->timer_create)\n\t\treturn -EOPNOTSUPP;\n\n\tnew_timer = alloc_posix_timer();\n\tif (unlikely(!new_timer))\n\t\treturn -EAGAIN;\n\n\tspin_lock_init(&new_timer->it_lock);\n\tnew_timer_id = posix_timer_add(new_timer);\n\tif (new_timer_id < 0) {\n\t\terror = new_timer_id;\n\t\tgoto out;\n\t}\n\n\tit_id_set = IT_ID_SET;\n\tnew_timer->it_id = (timer_t) new_timer_id;\n\tnew_timer->it_clock = which_clock;\n\tnew_timer->kclock = kc;\n\tnew_timer->it_overrun = -1LL;\n\n\tif (event) {\n\t\trcu_read_lock();\n\t\tnew_timer->it_pid = get_pid(good_sigevent(event));\n\t\trcu_read_unlock();\n\t\tif (!new_timer->it_pid) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tnew_timer->it_sigev_notify     = event->sigev_notify;\n\t\tnew_timer->sigq->info.si_signo = event->sigev_signo;\n\t\tnew_timer->sigq->info.si_value = event->sigev_value;\n\t} else {\n\t\tnew_timer->it_sigev_notify     = SIGEV_SIGNAL;\n\t\tnew_timer->sigq->info.si_signo = SIGALRM;\n\t\tmemset(&new_timer->sigq->info.si_value, 0, sizeof(sigval_t));\n\t\tnew_timer->sigq->info.si_value.sival_int = new_timer->it_id;\n\t\tnew_timer->it_pid = get_pid(task_tgid(current));\n\t}\n\n\tnew_timer->sigq->info.si_tid   = new_timer->it_id;\n\tnew_timer->sigq->info.si_code  = SI_TIMER;\n\n\tif (copy_to_user(created_timer_id,\n\t\t\t &new_timer_id, sizeof (new_timer_id))) {\n\t\terror = -EFAULT;\n\t\tgoto out;\n\t}\n\n\terror = kc->timer_create(new_timer);\n\tif (error)\n\t\tgoto out;\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tnew_timer->it_signal = current->signal;\n\tlist_add(&new_timer->list, &current->signal->posix_timers);\n\tspin_unlock_irq(&current->sighand->siglock);\n\n\treturn 0;\n\t/*\n\t * In the case of the timer belonging to another task, after\n\t * the task is unlocked, the timer is owned by the other task\n\t * and may cease to exist at any time.  Don't use or modify\n\t * new_timer after the unlock call.\n\t */\nout:\n\trelease_posix_timer(new_timer, it_id_set);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(timer_create, const clockid_t, which_clock,\n\t\tstruct sigevent __user *, timer_event_spec,\n\t\ttimer_t __user *, created_timer_id)\n{\n\tif (timer_event_spec) {\n\t\tsigevent_t event;\n\n\t\tif (copy_from_user(&event, timer_event_spec, sizeof (event)))\n\t\t\treturn -EFAULT;\n\t\treturn do_timer_create(which_clock, &event, created_timer_id);\n\t}\n\treturn do_timer_create(which_clock, NULL, created_timer_id);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE3(timer_create, clockid_t, which_clock,\n\t\t       struct compat_sigevent __user *, timer_event_spec,\n\t\t       timer_t __user *, created_timer_id)\n{\n\tif (timer_event_spec) {\n\t\tsigevent_t event;\n\n\t\tif (get_compat_sigevent(&event, timer_event_spec))\n\t\t\treturn -EFAULT;\n\t\treturn do_timer_create(which_clock, &event, created_timer_id);\n\t}\n\treturn do_timer_create(which_clock, NULL, created_timer_id);\n}\n#endif\n\n/*\n * Locking issues: We need to protect the result of the id look up until\n * we get the timer locked down so it is not deleted under us.  The\n * removal is done under the idr spinlock so we use that here to bridge\n * the find to the timer lock.  To avoid a dead lock, the timer id MUST\n * be release with out holding the timer lock.\n */\nstatic struct k_itimer *__lock_timer(timer_t timer_id, unsigned long *flags)\n{\n\tstruct k_itimer *timr;\n\n\t/*\n\t * timer_t could be any type >= int and we want to make sure any\n\t * @timer_id outside positive int range fails lookup.\n\t */\n\tif ((unsigned long long)timer_id > INT_MAX)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\ttimr = posix_timer_by_id(timer_id);\n\tif (timr) {\n\t\tspin_lock_irqsave(&timr->it_lock, *flags);\n\t\tif (timr->it_signal == current->signal) {\n\t\t\trcu_read_unlock();\n\t\t\treturn timr;\n\t\t}\n\t\tspin_unlock_irqrestore(&timr->it_lock, *flags);\n\t}\n\trcu_read_unlock();\n\n\treturn NULL;\n}\n\nstatic ktime_t common_hrtimer_remaining(struct k_itimer *timr, ktime_t now)\n{\n\tstruct hrtimer *timer = &timr->it.real.timer;\n\n\treturn __hrtimer_expires_remaining_adjusted(timer, now);\n}\n\nstatic s64 common_hrtimer_forward(struct k_itimer *timr, ktime_t now)\n{\n\tstruct hrtimer *timer = &timr->it.real.timer;\n\n\treturn hrtimer_forward(timer, now, timr->it_interval);\n}\n\n/*\n * Get the time remaining on a POSIX.1b interval timer.  This function\n * is ALWAYS called with spin_lock_irq on the timer, thus it must not\n * mess with irq.\n *\n * We have a couple of messes to clean up here.  First there is the case\n * of a timer that has a requeue pending.  These timers should appear to\n * be in the timer list with an expiry as if we were to requeue them\n * now.\n *\n * The second issue is the SIGEV_NONE timer which may be active but is\n * not really ever put in the timer list (to save system resources).\n * This timer may be expired, and if so, we will do it here.  Otherwise\n * it is the same as a requeue pending timer WRT to what we should\n * report.\n */\nvoid common_timer_get(struct k_itimer *timr, struct itimerspec64 *cur_setting)\n{\n\tconst struct k_clock *kc = timr->kclock;\n\tktime_t now, remaining, iv;\n\tstruct timespec64 ts64;\n\tbool sig_none;\n\n\tsig_none = timr->it_sigev_notify == SIGEV_NONE;\n\tiv = timr->it_interval;\n\n\t/* interval timer ? */\n\tif (iv) {\n\t\tcur_setting->it_interval = ktime_to_timespec64(iv);\n\t} else if (!timr->it_active) {\n\t\t/*\n\t\t * SIGEV_NONE oneshot timers are never queued. Check them\n\t\t * below.\n\t\t */\n\t\tif (!sig_none)\n\t\t\treturn;\n\t}\n\n\t/*\n\t * The timespec64 based conversion is suboptimal, but it's not\n\t * worth to implement yet another callback.\n\t */\n\tkc->clock_get(timr->it_clock, &ts64);\n\tnow = timespec64_to_ktime(ts64);\n\n\t/*\n\t * When a requeue is pending or this is a SIGEV_NONE timer move the\n\t * expiry time forward by intervals, so expiry is > now.\n\t */\n\tif (iv && (timr->it_requeue_pending & REQUEUE_PENDING || sig_none))\n\t\ttimr->it_overrun += kc->timer_forward(timr, now);\n\n\tremaining = kc->timer_remaining(timr, now);\n\t/* Return 0 only, when the timer is expired and not pending */\n\tif (remaining <= 0) {\n\t\t/*\n\t\t * A single shot SIGEV_NONE timer must return 0, when\n\t\t * it is expired !\n\t\t */\n\t\tif (!sig_none)\n\t\t\tcur_setting->it_value.tv_nsec = 1;\n\t} else {\n\t\tcur_setting->it_value = ktime_to_timespec64(remaining);\n\t}\n}\n\n/* Get the time remaining on a POSIX.1b interval timer. */\nstatic int do_timer_gettime(timer_t timer_id,  struct itimerspec64 *setting)\n{\n\tstruct k_itimer *timr;\n\tconst struct k_clock *kc;\n\tunsigned long flags;\n\tint ret = 0;\n\n\ttimr = lock_timer(timer_id, &flags);\n\tif (!timr)\n\t\treturn -EINVAL;\n\n\tmemset(setting, 0, sizeof(*setting));\n\tkc = timr->kclock;\n\tif (WARN_ON_ONCE(!kc || !kc->timer_get))\n\t\tret = -EINVAL;\n\telse\n\t\tkc->timer_get(timr, setting);\n\n\tunlock_timer(timr, flags);\n\treturn ret;\n}\n\n/* Get the time remaining on a POSIX.1b interval timer. */\nSYSCALL_DEFINE2(timer_gettime, timer_t, timer_id,\n\t\tstruct __kernel_itimerspec __user *, setting)\n{\n\tstruct itimerspec64 cur_setting;\n\n\tint ret = do_timer_gettime(timer_id, &cur_setting);\n\tif (!ret) {\n\t\tif (put_itimerspec64(&cur_setting, setting))\n\t\t\tret = -EFAULT;\n\t}\n\treturn ret;\n}\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\n\nCOMPAT_SYSCALL_DEFINE2(timer_gettime, timer_t, timer_id,\n\t\t       struct compat_itimerspec __user *, setting)\n{\n\tstruct itimerspec64 cur_setting;\n\n\tint ret = do_timer_gettime(timer_id, &cur_setting);\n\tif (!ret) {\n\t\tif (put_compat_itimerspec64(&cur_setting, setting))\n\t\t\tret = -EFAULT;\n\t}\n\treturn ret;\n}\n\n#endif\n\n/*\n * Get the number of overruns of a POSIX.1b interval timer.  This is to\n * be the overrun of the timer last delivered.  At the same time we are\n * accumulating overruns on the next timer.  The overrun is frozen when\n * the signal is delivered, either at the notify time (if the info block\n * is not queued) or at the actual delivery time (as we are informed by\n * the call back to posixtimer_rearm().  So all we need to do is\n * to pick up the frozen overrun.\n */\nSYSCALL_DEFINE1(timer_getoverrun, timer_t, timer_id)\n{\n\tstruct k_itimer *timr;\n\tint overrun;\n\tunsigned long flags;\n\n\ttimr = lock_timer(timer_id, &flags);\n\tif (!timr)\n\t\treturn -EINVAL;\n\n\toverrun = timer_overrun_to_int(timr, 0);\n\tunlock_timer(timr, flags);\n\n\treturn overrun;\n}\n\nstatic void common_hrtimer_arm(struct k_itimer *timr, ktime_t expires,\n\t\t\t       bool absolute, bool sigev_none)\n{\n\tstruct hrtimer *timer = &timr->it.real.timer;\n\tenum hrtimer_mode mode;\n\n\tmode = absolute ? HRTIMER_MODE_ABS : HRTIMER_MODE_REL;\n\t/*\n\t * Posix magic: Relative CLOCK_REALTIME timers are not affected by\n\t * clock modifications, so they become CLOCK_MONOTONIC based under the\n\t * hood. See hrtimer_init(). Update timr->kclock, so the generic\n\t * functions which use timr->kclock->clock_get() work.\n\t *\n\t * Note: it_clock stays unmodified, because the next timer_set() might\n\t * use ABSTIME, so it needs to switch back.\n\t */\n\tif (timr->it_clock == CLOCK_REALTIME)\n\t\ttimr->kclock = absolute ? &clock_realtime : &clock_monotonic;\n\n\thrtimer_init(&timr->it.real.timer, timr->it_clock, mode);\n\ttimr->it.real.timer.function = posix_timer_fn;\n\n\tif (!absolute)\n\t\texpires = ktime_add_safe(expires, timer->base->get_time());\n\thrtimer_set_expires(timer, expires);\n\n\tif (!sigev_none)\n\t\thrtimer_start_expires(timer, HRTIMER_MODE_ABS);\n}\n\nstatic int common_hrtimer_try_to_cancel(struct k_itimer *timr)\n{\n\treturn hrtimer_try_to_cancel(&timr->it.real.timer);\n}\n\n/* Set a POSIX.1b interval timer. */\nint common_timer_set(struct k_itimer *timr, int flags,\n\t\t     struct itimerspec64 *new_setting,\n\t\t     struct itimerspec64 *old_setting)\n{\n\tconst struct k_clock *kc = timr->kclock;\n\tbool sigev_none;\n\tktime_t expires;\n\n\tif (old_setting)\n\t\tcommon_timer_get(timr, old_setting);\n\n\t/* Prevent rearming by clearing the interval */\n\ttimr->it_interval = 0;\n\t/*\n\t * Careful here. On SMP systems the timer expiry function could be\n\t * active and spinning on timr->it_lock.\n\t */\n\tif (kc->timer_try_to_cancel(timr) < 0)\n\t\treturn TIMER_RETRY;\n\n\ttimr->it_active = 0;\n\ttimr->it_requeue_pending = (timr->it_requeue_pending + 2) &\n\t\t~REQUEUE_PENDING;\n\ttimr->it_overrun_last = 0;\n\n\t/* Switch off the timer when it_value is zero */\n\tif (!new_setting->it_value.tv_sec && !new_setting->it_value.tv_nsec)\n\t\treturn 0;\n\n\ttimr->it_interval = timespec64_to_ktime(new_setting->it_interval);\n\texpires = timespec64_to_ktime(new_setting->it_value);\n\tsigev_none = timr->it_sigev_notify == SIGEV_NONE;\n\n\tkc->timer_arm(timr, expires, flags & TIMER_ABSTIME, sigev_none);\n\ttimr->it_active = !sigev_none;\n\treturn 0;\n}\n\nstatic int do_timer_settime(timer_t timer_id, int flags,\n\t\t\t    struct itimerspec64 *new_spec64,\n\t\t\t    struct itimerspec64 *old_spec64)\n{\n\tconst struct k_clock *kc;\n\tstruct k_itimer *timr;\n\tunsigned long flag;\n\tint error = 0;\n\n\tif (!timespec64_valid(&new_spec64->it_interval) ||\n\t    !timespec64_valid(&new_spec64->it_value))\n\t\treturn -EINVAL;\n\n\tif (old_spec64)\n\t\tmemset(old_spec64, 0, sizeof(*old_spec64));\nretry:\n\ttimr = lock_timer(timer_id, &flag);\n\tif (!timr)\n\t\treturn -EINVAL;\n\n\tkc = timr->kclock;\n\tif (WARN_ON_ONCE(!kc || !kc->timer_set))\n\t\terror = -EINVAL;\n\telse\n\t\terror = kc->timer_set(timr, flags, new_spec64, old_spec64);\n\n\tunlock_timer(timr, flag);\n\tif (error == TIMER_RETRY) {\n\t\told_spec64 = NULL;\t// We already got the old time...\n\t\tgoto retry;\n\t}\n\n\treturn error;\n}\n\n/* Set a POSIX.1b interval timer */\nSYSCALL_DEFINE4(timer_settime, timer_t, timer_id, int, flags,\n\t\tconst struct __kernel_itimerspec __user *, new_setting,\n\t\tstruct __kernel_itimerspec __user *, old_setting)\n{\n\tstruct itimerspec64 new_spec, old_spec;\n\tstruct itimerspec64 *rtn = old_setting ? &old_spec : NULL;\n\tint error = 0;\n\n\tif (!new_setting)\n\t\treturn -EINVAL;\n\n\tif (get_itimerspec64(&new_spec, new_setting))\n\t\treturn -EFAULT;\n\n\terror = do_timer_settime(timer_id, flags, &new_spec, rtn);\n\tif (!error && old_setting) {\n\t\tif (put_itimerspec64(&old_spec, old_setting))\n\t\t\terror = -EFAULT;\n\t}\n\treturn error;\n}\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\nCOMPAT_SYSCALL_DEFINE4(timer_settime, timer_t, timer_id, int, flags,\n\t\t       struct compat_itimerspec __user *, new,\n\t\t       struct compat_itimerspec __user *, old)\n{\n\tstruct itimerspec64 new_spec, old_spec;\n\tstruct itimerspec64 *rtn = old ? &old_spec : NULL;\n\tint error = 0;\n\n\tif (!new)\n\t\treturn -EINVAL;\n\tif (get_compat_itimerspec64(&new_spec, new))\n\t\treturn -EFAULT;\n\n\terror = do_timer_settime(timer_id, flags, &new_spec, rtn);\n\tif (!error && old) {\n\t\tif (put_compat_itimerspec64(&old_spec, old))\n\t\t\terror = -EFAULT;\n\t}\n\treturn error;\n}\n#endif\n\nint common_timer_del(struct k_itimer *timer)\n{\n\tconst struct k_clock *kc = timer->kclock;\n\n\ttimer->it_interval = 0;\n\tif (kc->timer_try_to_cancel(timer) < 0)\n\t\treturn TIMER_RETRY;\n\ttimer->it_active = 0;\n\treturn 0;\n}\n\nstatic inline int timer_delete_hook(struct k_itimer *timer)\n{\n\tconst struct k_clock *kc = timer->kclock;\n\n\tif (WARN_ON_ONCE(!kc || !kc->timer_del))\n\t\treturn -EINVAL;\n\treturn kc->timer_del(timer);\n}\n\n/* Delete a POSIX.1b interval timer. */\nSYSCALL_DEFINE1(timer_delete, timer_t, timer_id)\n{\n\tstruct k_itimer *timer;\n\tunsigned long flags;\n\nretry_delete:\n\ttimer = lock_timer(timer_id, &flags);\n\tif (!timer)\n\t\treturn -EINVAL;\n\n\tif (timer_delete_hook(timer) == TIMER_RETRY) {\n\t\tunlock_timer(timer, flags);\n\t\tgoto retry_delete;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\tlist_del(&timer->list);\n\tspin_unlock(&current->sighand->siglock);\n\t/*\n\t * This keeps any tasks waiting on the spin lock from thinking\n\t * they got something (see the lock code above).\n\t */\n\ttimer->it_signal = NULL;\n\n\tunlock_timer(timer, flags);\n\trelease_posix_timer(timer, IT_ID_SET);\n\treturn 0;\n}\n\n/*\n * return timer owned by the process, used by exit_itimers\n */\nstatic void itimer_delete(struct k_itimer *timer)\n{\n\tunsigned long flags;\n\nretry_delete:\n\tspin_lock_irqsave(&timer->it_lock, flags);\n\n\tif (timer_delete_hook(timer) == TIMER_RETRY) {\n\t\tunlock_timer(timer, flags);\n\t\tgoto retry_delete;\n\t}\n\tlist_del(&timer->list);\n\t/*\n\t * This keeps any tasks waiting on the spin lock from thinking\n\t * they got something (see the lock code above).\n\t */\n\ttimer->it_signal = NULL;\n\n\tunlock_timer(timer, flags);\n\trelease_posix_timer(timer, IT_ID_SET);\n}\n\n/*\n * This is called by do_exit or de_thread, only when there are no more\n * references to the shared signal_struct.\n */\nvoid exit_itimers(struct signal_struct *sig)\n{\n\tstruct k_itimer *tmr;\n\n\twhile (!list_empty(&sig->posix_timers)) {\n\t\ttmr = list_entry(sig->posix_timers.next, struct k_itimer, list);\n\t\titimer_delete(tmr);\n\t}\n}\n\nSYSCALL_DEFINE2(clock_settime, const clockid_t, which_clock,\n\t\tconst struct __kernel_timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 new_tp;\n\n\tif (!kc || !kc->clock_set)\n\t\treturn -EINVAL;\n\n\tif (get_timespec64(&new_tp, tp))\n\t\treturn -EFAULT;\n\n\treturn kc->clock_set(which_clock, &new_tp);\n}\n\nSYSCALL_DEFINE2(clock_gettime, const clockid_t, which_clock,\n\t\tstruct __kernel_timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 kernel_tp;\n\tint error;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\n\terror = kc->clock_get(which_clock, &kernel_tp);\n\n\tif (!error && put_timespec64(&kernel_tp, tp))\n\t\terror = -EFAULT;\n\n\treturn error;\n}\n\nSYSCALL_DEFINE2(clock_adjtime, const clockid_t, which_clock,\n\t\tstruct timex __user *, utx)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timex ktx;\n\tint err;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->clock_adj)\n\t\treturn -EOPNOTSUPP;\n\n\tif (copy_from_user(&ktx, utx, sizeof(ktx)))\n\t\treturn -EFAULT;\n\n\terr = kc->clock_adj(which_clock, &ktx);\n\n\tif (err >= 0 && copy_to_user(utx, &ktx, sizeof(ktx)))\n\t\treturn -EFAULT;\n\n\treturn err;\n}\n\nSYSCALL_DEFINE2(clock_getres, const clockid_t, which_clock,\n\t\tstruct __kernel_timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 rtn_tp;\n\tint error;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\n\terror = kc->clock_getres(which_clock, &rtn_tp);\n\n\tif (!error && tp && put_timespec64(&rtn_tp, tp))\n\t\terror = -EFAULT;\n\n\treturn error;\n}\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\n\nCOMPAT_SYSCALL_DEFINE2(clock_settime, clockid_t, which_clock,\n\t\t       struct compat_timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 ts;\n\n\tif (!kc || !kc->clock_set)\n\t\treturn -EINVAL;\n\n\tif (compat_get_timespec64(&ts, tp))\n\t\treturn -EFAULT;\n\n\treturn kc->clock_set(which_clock, &ts);\n}\n\nCOMPAT_SYSCALL_DEFINE2(clock_gettime, clockid_t, which_clock,\n\t\t       struct compat_timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 ts;\n\tint err;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\n\terr = kc->clock_get(which_clock, &ts);\n\n\tif (!err && compat_put_timespec64(&ts, tp))\n\t\terr = -EFAULT;\n\n\treturn err;\n}\n\n#endif\n\n#ifdef CONFIG_COMPAT\n\nCOMPAT_SYSCALL_DEFINE2(clock_adjtime, clockid_t, which_clock,\n\t\t       struct compat_timex __user *, utp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timex ktx;\n\tint err;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->clock_adj)\n\t\treturn -EOPNOTSUPP;\n\n\terr = compat_get_timex(&ktx, utp);\n\tif (err)\n\t\treturn err;\n\n\terr = kc->clock_adj(which_clock, &ktx);\n\n\tif (err >= 0)\n\t\terr = compat_put_timex(utp, &ktx);\n\n\treturn err;\n}\n\n#endif\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\n\nCOMPAT_SYSCALL_DEFINE2(clock_getres, clockid_t, which_clock,\n\t\t       struct compat_timespec __user *, tp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 ts;\n\tint err;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\n\terr = kc->clock_getres(which_clock, &ts);\n\tif (!err && tp && compat_put_timespec64(&ts, tp))\n\t\treturn -EFAULT;\n\n\treturn err;\n}\n\n#endif\n\n/*\n * nanosleep for monotonic and realtime clocks\n */\nstatic int common_nsleep(const clockid_t which_clock, int flags,\n\t\t\t const struct timespec64 *rqtp)\n{\n\treturn hrtimer_nanosleep(rqtp, flags & TIMER_ABSTIME ?\n\t\t\t\t HRTIMER_MODE_ABS : HRTIMER_MODE_REL,\n\t\t\t\t which_clock);\n}\n\nSYSCALL_DEFINE4(clock_nanosleep, const clockid_t, which_clock, int, flags,\n\t\tconst struct __kernel_timespec __user *, rqtp,\n\t\tstruct __kernel_timespec __user *, rmtp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 t;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->nsleep)\n\t\treturn -ENANOSLEEP_NOTSUP;\n\n\tif (get_timespec64(&t, rqtp))\n\t\treturn -EFAULT;\n\n\tif (!timespec64_valid(&t))\n\t\treturn -EINVAL;\n\tif (flags & TIMER_ABSTIME)\n\t\trmtp = NULL;\n\tcurrent->restart_block.nanosleep.type = rmtp ? TT_NATIVE : TT_NONE;\n\tcurrent->restart_block.nanosleep.rmtp = rmtp;\n\n\treturn kc->nsleep(which_clock, flags, &t);\n}\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\n\nCOMPAT_SYSCALL_DEFINE4(clock_nanosleep, clockid_t, which_clock, int, flags,\n\t\t       struct compat_timespec __user *, rqtp,\n\t\t       struct compat_timespec __user *, rmtp)\n{\n\tconst struct k_clock *kc = clockid_to_kclock(which_clock);\n\tstruct timespec64 t;\n\n\tif (!kc)\n\t\treturn -EINVAL;\n\tif (!kc->nsleep)\n\t\treturn -ENANOSLEEP_NOTSUP;\n\n\tif (compat_get_timespec64(&t, rqtp))\n\t\treturn -EFAULT;\n\n\tif (!timespec64_valid(&t))\n\t\treturn -EINVAL;\n\tif (flags & TIMER_ABSTIME)\n\t\trmtp = NULL;\n\tcurrent->restart_block.nanosleep.type = rmtp ? TT_COMPAT : TT_NONE;\n\tcurrent->restart_block.nanosleep.compat_rmtp = rmtp;\n\n\treturn kc->nsleep(which_clock, flags, &t);\n}\n\n#endif\n\nstatic const struct k_clock clock_realtime = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_clock_realtime_get,\n\t.clock_set\t\t= posix_clock_realtime_set,\n\t.clock_adj\t\t= posix_clock_realtime_adj,\n\t.nsleep\t\t\t= common_nsleep,\n\t.timer_create\t\t= common_timer_create,\n\t.timer_set\t\t= common_timer_set,\n\t.timer_get\t\t= common_timer_get,\n\t.timer_del\t\t= common_timer_del,\n\t.timer_rearm\t\t= common_hrtimer_rearm,\n\t.timer_forward\t\t= common_hrtimer_forward,\n\t.timer_remaining\t= common_hrtimer_remaining,\n\t.timer_try_to_cancel\t= common_hrtimer_try_to_cancel,\n\t.timer_arm\t\t= common_hrtimer_arm,\n};\n\nstatic const struct k_clock clock_monotonic = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_ktime_get_ts,\n\t.nsleep\t\t\t= common_nsleep,\n\t.timer_create\t\t= common_timer_create,\n\t.timer_set\t\t= common_timer_set,\n\t.timer_get\t\t= common_timer_get,\n\t.timer_del\t\t= common_timer_del,\n\t.timer_rearm\t\t= common_hrtimer_rearm,\n\t.timer_forward\t\t= common_hrtimer_forward,\n\t.timer_remaining\t= common_hrtimer_remaining,\n\t.timer_try_to_cancel\t= common_hrtimer_try_to_cancel,\n\t.timer_arm\t\t= common_hrtimer_arm,\n};\n\nstatic const struct k_clock clock_monotonic_raw = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_get_monotonic_raw,\n};\n\nstatic const struct k_clock clock_realtime_coarse = {\n\t.clock_getres\t\t= posix_get_coarse_res,\n\t.clock_get\t\t= posix_get_realtime_coarse,\n};\n\nstatic const struct k_clock clock_monotonic_coarse = {\n\t.clock_getres\t\t= posix_get_coarse_res,\n\t.clock_get\t\t= posix_get_monotonic_coarse,\n};\n\nstatic const struct k_clock clock_tai = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_get_tai,\n\t.nsleep\t\t\t= common_nsleep,\n\t.timer_create\t\t= common_timer_create,\n\t.timer_set\t\t= common_timer_set,\n\t.timer_get\t\t= common_timer_get,\n\t.timer_del\t\t= common_timer_del,\n\t.timer_rearm\t\t= common_hrtimer_rearm,\n\t.timer_forward\t\t= common_hrtimer_forward,\n\t.timer_remaining\t= common_hrtimer_remaining,\n\t.timer_try_to_cancel\t= common_hrtimer_try_to_cancel,\n\t.timer_arm\t\t= common_hrtimer_arm,\n};\n\nstatic const struct k_clock clock_boottime = {\n\t.clock_getres\t\t= posix_get_hrtimer_res,\n\t.clock_get\t\t= posix_get_boottime,\n\t.nsleep\t\t\t= common_nsleep,\n\t.timer_create\t\t= common_timer_create,\n\t.timer_set\t\t= common_timer_set,\n\t.timer_get\t\t= common_timer_get,\n\t.timer_del\t\t= common_timer_del,\n\t.timer_rearm\t\t= common_hrtimer_rearm,\n\t.timer_forward\t\t= common_hrtimer_forward,\n\t.timer_remaining\t= common_hrtimer_remaining,\n\t.timer_try_to_cancel\t= common_hrtimer_try_to_cancel,\n\t.timer_arm\t\t= common_hrtimer_arm,\n};\n\nstatic const struct k_clock * const posix_clocks[] = {\n\t[CLOCK_REALTIME]\t\t= &clock_realtime,\n\t[CLOCK_MONOTONIC]\t\t= &clock_monotonic,\n\t[CLOCK_PROCESS_CPUTIME_ID]\t= &clock_process,\n\t[CLOCK_THREAD_CPUTIME_ID]\t= &clock_thread,\n\t[CLOCK_MONOTONIC_RAW]\t\t= &clock_monotonic_raw,\n\t[CLOCK_REALTIME_COARSE]\t\t= &clock_realtime_coarse,\n\t[CLOCK_MONOTONIC_COARSE]\t= &clock_monotonic_coarse,\n\t[CLOCK_BOOTTIME]\t\t= &clock_boottime,\n\t[CLOCK_REALTIME_ALARM]\t\t= &alarm_clock,\n\t[CLOCK_BOOTTIME_ALARM]\t\t= &alarm_clock,\n\t[CLOCK_TAI]\t\t\t= &clock_tai,\n};\n\nstatic const struct k_clock *clockid_to_kclock(const clockid_t id)\n{\n\tclockid_t idx = id;\n\n\tif (id < 0) {\n\t\treturn (id & CLOCKFD_MASK) == CLOCKFD ?\n\t\t\t&clock_posix_dynamic : &clock_posix_cpu;\n\t}\n\n\tif (id >= ARRAY_SIZE(posix_clocks))\n\t\treturn NULL;\n\n\treturn posix_clocks[array_index_nospec(idx, ARRAY_SIZE(posix_clocks))];\n}\n"], "filenames": ["include/linux/posix-timers.h", "kernel/time/posix-cpu-timers.c", "kernel/time/posix-timers.c"], "buggy_code_start_loc": [98, 88, 285], "buggy_code_end_loc": [100, 89, 795], "fixing_code_start_loc": [98, 88, 286], "fixing_code_end_loc": [100, 89, 804], "type": "CWE-190", "message": "An issue was discovered in the Linux kernel through 4.17.3. An Integer Overflow in kernel/time/posix-timers.c in the POSIX timer code is caused by the way the overrun accounting works. Depending on interval and expiry time values, the overrun can be larger than INT_MAX, but the accounting is int based. This basically makes the accounting values, which are visible to user space via timer_getoverrun(2) and siginfo::si_overrun, random. For example, a local user can cause a denial of service (signed integer overflow) via crafted mmap, futex, timer_create, and timer_settime system calls.", "other": {"cve": {"id": "CVE-2018-12896", "sourceIdentifier": "cve@mitre.org", "published": "2018-07-02T17:29:00.660", "lastModified": "2019-04-03T12:04:40.763", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "An issue was discovered in the Linux kernel through 4.17.3. An Integer Overflow in kernel/time/posix-timers.c in the POSIX timer code is caused by the way the overrun accounting works. Depending on interval and expiry time values, the overrun can be larger than INT_MAX, but the accounting is int based. This basically makes the accounting values, which are visible to user space via timer_getoverrun(2) and siginfo::si_overrun, random. For example, a local user can cause a denial of service (signed integer overflow) via crafted mmap, futex, timer_create, and timer_settime system calls."}, {"lang": "es", "value": "Se ha descubierto un problema en el kernel de Linux hasta la versi\u00f3n 4.17.3. Un desbordamiento de enteros en kernel/time/posix-timers.c en el c\u00f3digo temporizador POSIX es causado por la forma en que funciona la contabilidad de desbordamiento. Dependiendo de los valores del intervalo y del tiempo de expiraci\u00f3n, el desbordamiento puede ser mayor que INT_MAX, pero la contabilidad est\u00e1 basada en int. Esto b\u00e1sicamente hace que los valores contables, que son visibles para el espacio de usuario a trav\u00e9s de timer_getoverrun(2) y siginfo::si_overrun, sean aleatorios. Por ejemplo, un usuario local puede causar una denegaci\u00f3n de servicio (desbordamiento de enteros firmados) a trav\u00e9s de llamadas al sistema mmap, futex, timer_create y timer_settime manipuladas."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-190"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.17.3", "matchCriteriaId": "C6ECF5D0-AFCC-470A-9D01-4A372C09556D"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:12.04:*:*:*:esm:*:*:*", "matchCriteriaId": "8D305F7A-D159-4716-AB26-5E38BB5CD991"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:lts:*:*:*", "matchCriteriaId": "B5A6F2F3-4894-4392-8296-3B8DD2679084"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:lts:*:*:*", "matchCriteriaId": "F7016A2A-8365-4F1A-89A2-7A19F2BCAE5B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:lts:*:*:*", "matchCriteriaId": "23A7C53F-B80F-4E6A-AFA9-58EEA84BE11D"}]}]}], "references": [{"url": "https://bugzilla.kernel.org/show_bug.cgi?id=200189", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Vendor Advisory"]}, {"url": "https://github.com/lcytxw/bug_repro/tree/master/bug_200189", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/78c9c4dfbf8c04883941445a195276bb4bb92c76", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2019/03/msg00017.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2019/03/msg00034.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2019/04/msg00004.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3847-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3847-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3847-3/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3848-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3848-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3849-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3849-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/78c9c4dfbf8c04883941445a195276bb4bb92c76"}}