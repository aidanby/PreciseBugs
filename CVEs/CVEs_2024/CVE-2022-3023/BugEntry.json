{"buggy_code": ["// Copyright 2019 PingCAP, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage checkpoints\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"path\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/joho/sqltocsv\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/checkpoints/checkpointspb\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/common\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/config\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/log\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/mydump\"\n\tverify \"github.com/pingcap/tidb/br/pkg/lightning/verification\"\n\t\"github.com/pingcap/tidb/br/pkg/storage\"\n\t\"github.com/pingcap/tidb/br/pkg/version/build\"\n\t\"github.com/pingcap/tidb/util/mathutil\"\n\t\"go.uber.org/zap\"\n\t\"golang.org/x/exp/slices\"\n)\n\ntype CheckpointStatus uint8\n\nconst (\n\tCheckpointStatusMissing         CheckpointStatus = 0\n\tCheckpointStatusMaxInvalid      CheckpointStatus = 25\n\tCheckpointStatusLoaded          CheckpointStatus = 30\n\tCheckpointStatusAllWritten      CheckpointStatus = 60\n\tCheckpointStatusClosed          CheckpointStatus = 90\n\tCheckpointStatusImported        CheckpointStatus = 120\n\tCheckpointStatusIndexImported   CheckpointStatus = 140\n\tCheckpointStatusAlteredAutoInc  CheckpointStatus = 150\n\tCheckpointStatusChecksumSkipped CheckpointStatus = 170\n\tCheckpointStatusChecksummed     CheckpointStatus = 180\n\tCheckpointStatusAnalyzeSkipped  CheckpointStatus = 200\n\tCheckpointStatusAnalyzed        CheckpointStatus = 210\n)\n\nconst WholeTableEngineID = math.MaxInt32\n\nconst (\n\t// the table names to store each kind of checkpoint in the checkpoint database\n\t// remember to increase the version number in case of incompatible change.\n\tCheckpointTableNameTask   = \"task_v2\"\n\tCheckpointTableNameTable  = \"table_v7\"\n\tCheckpointTableNameEngine = \"engine_v5\"\n\tCheckpointTableNameChunk  = \"chunk_v5\"\n\n\t// Some frequently used table name or constants.\n\tallTables       = \"all\"\n\tstringLitAll    = \"'all'\"\n\tcolumnTableName = \"table_name\"\n)\n\nconst (\n\t// shared by MySQLCheckpointsDB and GlueCheckpointsDB\n\tCreateDBTemplate        = \"CREATE DATABASE IF NOT EXISTS %s;\"\n\tCreateTaskTableTemplate = `\n\t\tCREATE TABLE IF NOT EXISTS %s.%s (\n\t\t\tid tinyint(1) PRIMARY KEY,\n\t\t\ttask_id bigint NOT NULL,\n\t\t\tsource_dir varchar(256) NOT NULL,\n\t\t\tbackend varchar(16) NOT NULL,\n\t\t\timporter_addr varchar(256),\n\t\t\ttidb_host varchar(128) NOT NULL,\n\t\t\ttidb_port int NOT NULL,\n\t\t\tpd_addr varchar(128) NOT NULL,\n\t\t\tsorted_kv_dir varchar(256) NOT NULL,\n\t\t\tlightning_ver varchar(48) NOT NULL\n\t\t);`\n\tCreateTableTableTemplate = `\n\t\tCREATE TABLE IF NOT EXISTS %s.%s (\n\t\t\ttask_id bigint NOT NULL,\n\t\t\ttable_name varchar(261) NOT NULL PRIMARY KEY,\n\t\t\thash binary(32) NOT NULL,\n\t\t\tstatus tinyint unsigned DEFAULT 30,\n\t\t\talloc_base bigint NOT NULL DEFAULT 0,\n\t\t\ttable_id bigint NOT NULL DEFAULT 0,\n\t\t\tcreate_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\n\t\t\tupdate_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n\t\t\tkv_bytes bigint unsigned NOT NULL DEFAULT 0,\n\t\t\tkv_kvs bigint unsigned NOT NULL DEFAULT 0,\n\t\t\tkv_checksum bigint unsigned NOT NULL DEFAULT 0,\n\t\t\tINDEX(task_id)\n\t\t);`\n\tCreateEngineTableTemplate = `\n\t\tCREATE TABLE IF NOT EXISTS %s.%s (\n\t\t\ttable_name varchar(261) NOT NULL,\n\t\t\tengine_id int NOT NULL,\n\t\t\tstatus tinyint unsigned DEFAULT 30,\n\t\t\tcreate_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\n\t\t\tupdate_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n\t\t\tPRIMARY KEY(table_name, engine_id DESC)\n\t\t);`\n\tCreateChunkTableTemplate = `\n\t\tCREATE TABLE IF NOT EXISTS %s.%s (\n\t\t\ttable_name varchar(261) NOT NULL,\n\t\t\tengine_id int unsigned NOT NULL,\n\t\t\tpath varchar(2048) NOT NULL,\n\t\t\toffset bigint NOT NULL,\n\t\t\ttype int NOT NULL,\n\t\t\tcompression int NOT NULL,\n\t\t\tsort_key varchar(256) NOT NULL,\n\t\t\tfile_size bigint NOT NULL,\n\t\t\tcolumns text NULL,\n\t\t\tshould_include_row_id BOOL NOT NULL,\n\t\t\tend_offset bigint NOT NULL,\n\t\t\tpos bigint NOT NULL,\n\t\t\tprev_rowid_max bigint NOT NULL,\n\t\t\trowid_max bigint NOT NULL,\n\t\t\tkvc_bytes bigint unsigned NOT NULL DEFAULT 0,\n\t\t\tkvc_kvs bigint unsigned NOT NULL DEFAULT 0,\n\t\t\tkvc_checksum bigint unsigned NOT NULL DEFAULT 0,\n\t\t\tcreate_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\n\t\t\tupdate_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n\t\t\tPRIMARY KEY(table_name, engine_id, path(500), offset)\n\t\t);`\n\tInitTaskTemplate = `\n\t\tREPLACE INTO %s.%s (id, task_id, source_dir, backend, importer_addr, tidb_host, tidb_port, pd_addr, sorted_kv_dir, lightning_ver)\n\t\t\tVALUES (1, ?, ?, ?, ?, ?, ?, ?, ?, ?);`\n\tInitTableTemplate = `\n\t\tINSERT INTO %s.%s (task_id, table_name, hash, table_id) VALUES (?, ?, ?, ?)\n\t\t\tON DUPLICATE KEY UPDATE task_id = CASE\n\t\t\t\tWHEN hash = VALUES(hash)\n\t\t\t\tTHEN VALUES(task_id)\n\t\t\tEND;`\n\tReadTaskTemplate = `\n\t\tSELECT task_id, source_dir, backend, importer_addr, tidb_host, tidb_port, pd_addr, sorted_kv_dir, lightning_ver FROM %s.%s WHERE id = 1;`\n\tReadEngineTemplate = `\n\t\tSELECT engine_id, status FROM %s.%s WHERE table_name = ? ORDER BY engine_id DESC;`\n\tReadChunkTemplate = `\n\t\tSELECT\n\t\t\tengine_id, path, offset, type, compression, sort_key, file_size, columns,\n\t\t\tpos, end_offset, prev_rowid_max, rowid_max,\n\t\t\tkvc_bytes, kvc_kvs, kvc_checksum, unix_timestamp(create_time)\n\t\tFROM %s.%s WHERE table_name = ?\n\t\tORDER BY engine_id, path, offset;`\n\tReadTableRemainTemplate = `\n\t\tSELECT status, alloc_base, table_id, kv_bytes, kv_kvs, kv_checksum FROM %s.%s WHERE table_name = ?;`\n\tReplaceEngineTemplate = `\n\t\tREPLACE INTO %s.%s (table_name, engine_id, status) VALUES (?, ?, ?);`\n\tReplaceChunkTemplate = `\n\t\tREPLACE INTO %s.%s (\n\t\t\t\ttable_name, engine_id,\n\t\t\t\tpath, offset, type, compression, sort_key, file_size, columns, should_include_row_id,\n\t\t\t\tpos, end_offset, prev_rowid_max, rowid_max,\n\t\t\t\tkvc_bytes, kvc_kvs, kvc_checksum, create_time\n\t\t\t) VALUES (\n\t\t\t\t?, ?,\n\t\t\t\t?, ?, ?, ?, ?, ?, ?, FALSE,\n\t\t\t\t?, ?, ?, ?,\n\t\t\t\t0, 0, 0, from_unixtime(?)\n\t\t\t);`\n\tUpdateChunkTemplate = `\n\t\tUPDATE %s.%s SET pos = ?, prev_rowid_max = ?, kvc_bytes = ?, kvc_kvs = ?, kvc_checksum = ?, columns = ?\n\t\tWHERE (table_name, engine_id, path, offset) = (?, ?, ?, ?);`\n\tUpdateTableRebaseTemplate = `\n\t\tUPDATE %s.%s SET alloc_base = GREATEST(?, alloc_base) WHERE table_name = ?;`\n\tUpdateTableStatusTemplate = `\n\t\tUPDATE %s.%s SET status = ? WHERE table_name = ?;`\n\tUpdateTableChecksumTemplate = `UPDATE %s.%s SET kv_bytes = ?, kv_kvs = ?, kv_checksum = ? WHERE table_name = ?;`\n\tUpdateEngineTemplate        = `\n\t\tUPDATE %s.%s SET status = ? WHERE (table_name, engine_id) = (?, ?);`\n\tDeleteCheckpointRecordTemplate = \"DELETE FROM %s.%s WHERE table_name = ?;\"\n)\n\nfunc IsCheckpointTable(name string) bool {\n\tswitch name {\n\tcase CheckpointTableNameTask, CheckpointTableNameTable, CheckpointTableNameEngine, CheckpointTableNameChunk:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\nfunc (status CheckpointStatus) MetricName() string {\n\tswitch status {\n\tcase CheckpointStatusLoaded:\n\t\treturn \"pending\"\n\tcase CheckpointStatusAllWritten:\n\t\treturn \"written\"\n\tcase CheckpointStatusClosed:\n\t\treturn \"closed\"\n\tcase CheckpointStatusImported:\n\t\treturn \"imported\"\n\tcase CheckpointStatusIndexImported:\n\t\treturn \"index_imported\"\n\tcase CheckpointStatusAlteredAutoInc:\n\t\treturn \"altered_auto_inc\"\n\tcase CheckpointStatusChecksummed, CheckpointStatusChecksumSkipped:\n\t\treturn \"checksum\"\n\tcase CheckpointStatusAnalyzed, CheckpointStatusAnalyzeSkipped:\n\t\treturn \"analyzed\"\n\tcase CheckpointStatusMissing:\n\t\treturn \"missing\"\n\tdefault:\n\t\treturn \"invalid\"\n\t}\n}\n\ntype ChunkCheckpointKey struct {\n\tPath   string\n\tOffset int64\n}\n\nfunc (key *ChunkCheckpointKey) String() string {\n\treturn fmt.Sprintf(\"%s:%d\", key.Path, key.Offset)\n}\n\nfunc (key *ChunkCheckpointKey) less(other *ChunkCheckpointKey) bool {\n\tswitch {\n\tcase key.Path < other.Path:\n\t\treturn true\n\tcase key.Path > other.Path:\n\t\treturn false\n\tdefault:\n\t\treturn key.Offset < other.Offset\n\t}\n}\n\ntype ChunkCheckpoint struct {\n\tKey               ChunkCheckpointKey\n\tFileMeta          mydump.SourceFileMeta\n\tColumnPermutation []int\n\tChunk             mydump.Chunk\n\tChecksum          verify.KVChecksum\n\tTimestamp         int64\n}\n\nfunc (ccp *ChunkCheckpoint) DeepCopy() *ChunkCheckpoint {\n\tcolPerm := make([]int, 0, len(ccp.ColumnPermutation))\n\tcolPerm = append(colPerm, ccp.ColumnPermutation...)\n\treturn &ChunkCheckpoint{\n\t\tKey:               ccp.Key,\n\t\tFileMeta:          ccp.FileMeta,\n\t\tColumnPermutation: colPerm,\n\t\tChunk:             ccp.Chunk,\n\t\tChecksum:          ccp.Checksum,\n\t\tTimestamp:         ccp.Timestamp,\n\t}\n}\n\ntype EngineCheckpoint struct {\n\tStatus CheckpointStatus\n\tChunks []*ChunkCheckpoint // a sorted array\n}\n\nfunc (engine *EngineCheckpoint) DeepCopy() *EngineCheckpoint {\n\tchunks := make([]*ChunkCheckpoint, 0, len(engine.Chunks))\n\tfor _, chunk := range engine.Chunks {\n\t\tchunks = append(chunks, chunk.DeepCopy())\n\t}\n\treturn &EngineCheckpoint{\n\t\tStatus: engine.Status,\n\t\tChunks: chunks,\n\t}\n}\n\ntype TableCheckpoint struct {\n\tStatus    CheckpointStatus\n\tAllocBase int64\n\tEngines   map[int32]*EngineCheckpoint\n\tTableID   int64\n\t// remote checksum before restore\n\tChecksum verify.KVChecksum\n}\n\nfunc (cp *TableCheckpoint) DeepCopy() *TableCheckpoint {\n\tengines := make(map[int32]*EngineCheckpoint, len(cp.Engines))\n\tfor engineID, engine := range cp.Engines {\n\t\tengines[engineID] = engine.DeepCopy()\n\t}\n\treturn &TableCheckpoint{\n\t\tStatus:    cp.Status,\n\t\tAllocBase: cp.AllocBase,\n\t\tEngines:   engines,\n\t\tTableID:   cp.TableID,\n\t\tChecksum:  cp.Checksum,\n\t}\n}\n\nfunc (cp *TableCheckpoint) CountChunks() int {\n\tresult := 0\n\tfor _, engine := range cp.Engines {\n\t\tresult += len(engine.Chunks)\n\t}\n\treturn result\n}\n\ntype chunkCheckpointDiff struct {\n\tpos               int64\n\trowID             int64\n\tchecksum          verify.KVChecksum\n\tcolumnPermutation []int\n}\n\ntype engineCheckpointDiff struct {\n\thasStatus bool\n\tstatus    CheckpointStatus\n\tchunks    map[ChunkCheckpointKey]chunkCheckpointDiff\n}\n\ntype TableCheckpointDiff struct {\n\thasStatus   bool\n\thasRebase   bool\n\thasChecksum bool\n\tstatus      CheckpointStatus\n\tallocBase   int64\n\tengines     map[int32]engineCheckpointDiff\n\tchecksum    verify.KVChecksum\n}\n\nfunc NewTableCheckpointDiff() *TableCheckpointDiff {\n\treturn &TableCheckpointDiff{\n\t\tengines: make(map[int32]engineCheckpointDiff),\n\t}\n}\n\nfunc (cpd *TableCheckpointDiff) insertEngineCheckpointDiff(engineID int32, newDiff engineCheckpointDiff) {\n\tif oldDiff, ok := cpd.engines[engineID]; ok {\n\t\tif newDiff.hasStatus {\n\t\t\toldDiff.hasStatus = true\n\t\t\toldDiff.status = newDiff.status\n\t\t}\n\t\tfor key, chunkDiff := range newDiff.chunks {\n\t\t\toldDiff.chunks[key] = chunkDiff\n\t\t}\n\t\tnewDiff = oldDiff\n\t}\n\tcpd.engines[engineID] = newDiff\n}\n\nfunc (cpd *TableCheckpointDiff) String() string {\n\treturn fmt.Sprintf(\n\t\t\"{hasStatus:%v, hasRebase:%v, status:%d, allocBase:%d, engines:[%d]}\",\n\t\tcpd.hasStatus, cpd.hasRebase, cpd.status, cpd.allocBase, len(cpd.engines),\n\t)\n}\n\n// Apply the diff to the existing chunk and engine checkpoints in `cp`.\nfunc (cp *TableCheckpoint) Apply(cpd *TableCheckpointDiff) {\n\tif cpd.hasStatus {\n\t\tcp.Status = cpd.status\n\t}\n\tif cpd.hasRebase {\n\t\tcp.AllocBase = cpd.allocBase\n\t}\n\tfor engineID, engineDiff := range cpd.engines {\n\t\tengine := cp.Engines[engineID]\n\t\tif engine == nil {\n\t\t\tcontinue\n\t\t}\n\t\tif engineDiff.hasStatus {\n\t\t\tengine.Status = engineDiff.status\n\t\t}\n\t\tfor key, diff := range engineDiff.chunks {\n\t\t\tcheckpointKey := key\n\t\t\tindex := sort.Search(len(engine.Chunks), func(i int) bool {\n\t\t\t\treturn !engine.Chunks[i].Key.less(&checkpointKey)\n\t\t\t})\n\t\t\tif index >= len(engine.Chunks) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tchunk := engine.Chunks[index]\n\t\t\tif chunk.Key != checkpointKey {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tchunk.Chunk.Offset = diff.pos\n\t\t\tchunk.Chunk.PrevRowIDMax = diff.rowID\n\t\t\tchunk.Checksum = diff.checksum\n\t\t}\n\t}\n}\n\ntype TableCheckpointMerger interface {\n\t// MergeInto the table checkpoint diff from a status update or chunk update.\n\t// If there are multiple updates to the same table, only the last one will\n\t// take effect. Therefore, the caller must ensure events for the same table\n\t// are properly ordered by the global time (an old event must be merged\n\t// before the new one).\n\tMergeInto(cpd *TableCheckpointDiff)\n}\n\ntype StatusCheckpointMerger struct {\n\tEngineID int32 // WholeTableEngineID == apply to whole table.\n\tStatus   CheckpointStatus\n}\n\nfunc (merger *StatusCheckpointMerger) SetInvalid() {\n\tmerger.Status /= 10\n}\n\nfunc (merger *StatusCheckpointMerger) MergeInto(cpd *TableCheckpointDiff) {\n\tif merger.EngineID == WholeTableEngineID || merger.Status <= CheckpointStatusMaxInvalid {\n\t\tcpd.status = merger.Status\n\t\tcpd.hasStatus = true\n\t}\n\tif merger.EngineID != WholeTableEngineID {\n\t\tcpd.insertEngineCheckpointDiff(merger.EngineID, engineCheckpointDiff{\n\t\t\thasStatus: true,\n\t\t\tstatus:    merger.Status,\n\t\t\tchunks:    make(map[ChunkCheckpointKey]chunkCheckpointDiff),\n\t\t})\n\t}\n}\n\ntype ChunkCheckpointMerger struct {\n\tEngineID          int32\n\tKey               ChunkCheckpointKey\n\tChecksum          verify.KVChecksum\n\tPos               int64\n\tRowID             int64\n\tColumnPermutation []int\n\tEndOffset         int64 // For test only.\n}\n\nfunc (merger *ChunkCheckpointMerger) MergeInto(cpd *TableCheckpointDiff) {\n\tcpd.insertEngineCheckpointDiff(merger.EngineID, engineCheckpointDiff{\n\t\tchunks: map[ChunkCheckpointKey]chunkCheckpointDiff{\n\t\t\tmerger.Key: {\n\t\t\t\tpos:               merger.Pos,\n\t\t\t\trowID:             merger.RowID,\n\t\t\t\tchecksum:          merger.Checksum,\n\t\t\t\tcolumnPermutation: merger.ColumnPermutation,\n\t\t\t},\n\t\t},\n\t})\n}\n\ntype TableChecksumMerger struct {\n\tChecksum verify.KVChecksum\n}\n\nfunc (m *TableChecksumMerger) MergeInto(cpd *TableCheckpointDiff) {\n\tcpd.hasChecksum = true\n\tcpd.checksum = m.Checksum\n}\n\ntype RebaseCheckpointMerger struct {\n\tAllocBase int64\n}\n\nfunc (merger *RebaseCheckpointMerger) MergeInto(cpd *TableCheckpointDiff) {\n\tcpd.hasRebase = true\n\tcpd.allocBase = mathutil.Max(cpd.allocBase, merger.AllocBase)\n}\n\ntype DestroyedTableCheckpoint struct {\n\tTableName   string\n\tMinEngineID int32\n\tMaxEngineID int32\n}\n\ntype TaskCheckpoint struct {\n\tTaskID       int64\n\tSourceDir    string\n\tBackend      string\n\tImporterAddr string\n\tTiDBHost     string\n\tTiDBPort     int\n\tPdAddr       string\n\tSortedKVDir  string\n\tLightningVer string\n}\n\ntype DB interface {\n\tInitialize(ctx context.Context, cfg *config.Config, dbInfo map[string]*TidbDBInfo) error\n\tTaskCheckpoint(ctx context.Context) (*TaskCheckpoint, error)\n\tGet(ctx context.Context, tableName string) (*TableCheckpoint, error)\n\tClose() error\n\t// InsertEngineCheckpoints initializes the checkpoints related to a table.\n\t// It assumes the entire table has not been imported before and will fill in\n\t// default values for the column permutations and checksums.\n\tInsertEngineCheckpoints(ctx context.Context, tableName string, checkpoints map[int32]*EngineCheckpoint) error\n\tUpdate(taskCtx context.Context, checkpointDiffs map[string]*TableCheckpointDiff) error\n\n\tRemoveCheckpoint(ctx context.Context, tableName string) error\n\t// MoveCheckpoints renames the checkpoint schema to include a suffix\n\t// including the taskID (e.g. `tidb_lightning_checkpoints.1234567890.bak`).\n\tMoveCheckpoints(ctx context.Context, taskID int64) error\n\t// GetLocalStoringTables returns a map containing tables have engine files stored on local disk.\n\t// currently only meaningful for local backend\n\tGetLocalStoringTables(ctx context.Context) (map[string][]int32, error)\n\tIgnoreErrorCheckpoint(ctx context.Context, tableName string) error\n\tDestroyErrorCheckpoint(ctx context.Context, tableName string) ([]DestroyedTableCheckpoint, error)\n\tDumpTables(ctx context.Context, csv io.Writer) error\n\tDumpEngines(ctx context.Context, csv io.Writer) error\n\tDumpChunks(ctx context.Context, csv io.Writer) error\n}\n\nfunc OpenCheckpointsDB(ctx context.Context, cfg *config.Config) (DB, error) {\n\tif !cfg.Checkpoint.Enable {\n\t\treturn NewNullCheckpointsDB(), nil\n\t}\n\n\tswitch cfg.Checkpoint.Driver {\n\tcase config.CheckpointDriverMySQL:\n\t\tdb, err := common.ConnectMySQL(cfg.Checkpoint.DSN)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\t\tcpdb, err := NewMySQLCheckpointsDB(ctx, db, cfg.Checkpoint.Schema)\n\t\tif err != nil {\n\t\t\t_ = db.Close()\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\t\treturn cpdb, nil\n\n\tcase config.CheckpointDriverFile:\n\t\tcpdb, err := NewFileCheckpointsDB(ctx, cfg.Checkpoint.DSN)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\t\treturn cpdb, nil\n\n\tdefault:\n\t\treturn nil, common.ErrUnknownCheckpointDriver.GenWithStackByArgs(cfg.Checkpoint.Driver)\n\t}\n}\n\nfunc IsCheckpointsDBExists(ctx context.Context, cfg *config.Config) (bool, error) {\n\tif !cfg.Checkpoint.Enable {\n\t\treturn false, nil\n\t}\n\tswitch cfg.Checkpoint.Driver {\n\tcase config.CheckpointDriverMySQL:\n\t\tdb, err := sql.Open(\"mysql\", cfg.Checkpoint.DSN)\n\t\tif err != nil {\n\t\t\treturn false, errors.Trace(err)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer db.Close()\n\t\tcheckSQL := \"SHOW DATABASES WHERE `DATABASE` = ?\"\n\t\trows, err := db.QueryContext(ctx, checkSQL, cfg.Checkpoint.Schema)\n\t\tif err != nil {\n\t\t\treturn false, errors.Trace(err)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer rows.Close()\n\t\tresult := rows.Next()\n\t\tif err := rows.Err(); err != nil {\n\t\t\treturn false, errors.Trace(err)\n\t\t}\n\t\treturn result, nil\n\n\tcase config.CheckpointDriverFile:\n\t\ts, fileName, err := createExstorageByCompletePath(ctx, cfg.Checkpoint.DSN)\n\t\tif err != nil {\n\t\t\treturn false, errors.Trace(err)\n\t\t}\n\t\tresult, err := s.FileExists(ctx, fileName)\n\t\tif err != nil {\n\t\t\treturn false, errors.Trace(err)\n\t\t}\n\t\treturn result, nil\n\n\tdefault:\n\t\treturn false, common.ErrUnknownCheckpointDriver.GenWithStackByArgs(cfg.Checkpoint.Driver)\n\t}\n}\n\n// NullCheckpointsDB is a checkpoints database with no checkpoints.\ntype NullCheckpointsDB struct{}\n\nfunc NewNullCheckpointsDB() *NullCheckpointsDB {\n\treturn &NullCheckpointsDB{}\n}\n\nfunc (*NullCheckpointsDB) Initialize(context.Context, *config.Config, map[string]*TidbDBInfo) error {\n\treturn nil\n}\n\nfunc (*NullCheckpointsDB) TaskCheckpoint(ctx context.Context) (*TaskCheckpoint, error) {\n\treturn nil, nil\n}\n\nfunc (*NullCheckpointsDB) Close() error {\n\treturn nil\n}\n\nfunc (*NullCheckpointsDB) Get(_ context.Context, _ string) (*TableCheckpoint, error) {\n\treturn &TableCheckpoint{\n\t\tStatus:  CheckpointStatusLoaded,\n\t\tEngines: map[int32]*EngineCheckpoint{},\n\t}, nil\n}\n\nfunc (*NullCheckpointsDB) InsertEngineCheckpoints(_ context.Context, _ string, _ map[int32]*EngineCheckpoint) error {\n\treturn nil\n}\n\nfunc (*NullCheckpointsDB) Update(context.Context, map[string]*TableCheckpointDiff) error {\n\treturn nil\n}\n\ntype MySQLCheckpointsDB struct {\n\tdb     *sql.DB\n\tschema string\n}\n\nfunc NewMySQLCheckpointsDB(ctx context.Context, db *sql.DB, schemaName string) (*MySQLCheckpointsDB, error) {\n\tschema := common.EscapeIdentifier(schemaName)\n\tsql := common.SQLWithRetry{\n\t\tDB:           db,\n\t\tLogger:       log.FromContext(ctx).With(zap.String(\"schema\", schemaName)),\n\t\tHideQueryLog: true,\n\t}\n\terr := sql.Exec(ctx, \"create checkpoints database\", fmt.Sprintf(CreateDBTemplate, schema))\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\terr = sql.Exec(ctx, \"create task checkpoints table\", fmt.Sprintf(CreateTaskTableTemplate, schema, CheckpointTableNameTask))\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\terr = sql.Exec(ctx, \"create table checkpoints table\", fmt.Sprintf(CreateTableTableTemplate, schema, CheckpointTableNameTable))\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\terr = sql.Exec(ctx, \"create engine checkpoints table\", fmt.Sprintf(CreateEngineTableTemplate, schema, CheckpointTableNameEngine))\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\terr = sql.Exec(ctx, \"create chunks checkpoints table\", fmt.Sprintf(CreateChunkTableTemplate, schema, CheckpointTableNameChunk))\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\treturn &MySQLCheckpointsDB{\n\t\tdb:     db,\n\t\tschema: schema,\n\t}, nil\n}\n\nfunc (cpdb *MySQLCheckpointsDB) Initialize(ctx context.Context, cfg *config.Config, dbInfo map[string]*TidbDBInfo) error {\n\t// We can have at most 65535 placeholders https://stackoverflow.com/q/4922345/\n\t// Since this step is not performance critical, we just insert the rows one-by-one.\n\ts := common.SQLWithRetry{DB: cpdb.db, Logger: log.FromContext(ctx)}\n\terr := s.Transact(ctx, \"insert checkpoints\", func(c context.Context, tx *sql.Tx) error {\n\t\ttaskStmt, err := tx.PrepareContext(c, fmt.Sprintf(InitTaskTemplate, cpdb.schema, CheckpointTableNameTask))\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer taskStmt.Close()\n\t\t_, err = taskStmt.ExecContext(ctx, cfg.TaskID, cfg.Mydumper.SourceDir, cfg.TikvImporter.Backend,\n\t\t\tcfg.TikvImporter.Addr, cfg.TiDB.Host, cfg.TiDB.Port, cfg.TiDB.PdAddr, cfg.TikvImporter.SortedKVDir,\n\t\t\tbuild.ReleaseVersion)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\n\t\t// If `hash` is not the same but the `table_name` duplicates,\n\t\t// the CASE expression will return NULL, which can be used to violate\n\t\t// the NOT NULL requirement of `task_id` column, and caused this INSERT\n\t\t// statement to fail with an irrecoverable error.\n\t\t// We do need to capture the error is display a user friendly message\n\t\t// (multiple nodes cannot import the same table) though.\n\t\tstmt, err := tx.PrepareContext(c, fmt.Sprintf(InitTableTemplate, cpdb.schema, CheckpointTableNameTable))\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer stmt.Close()\n\n\t\tfor _, db := range dbInfo {\n\t\t\tfor _, table := range db.Tables {\n\t\t\t\ttableName := common.UniqueTable(db.Name, table.Name)\n\t\t\t\t_, err = stmt.ExecContext(c, cfg.TaskID, tableName, CheckpointStatusLoaded, table.ID)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Trace(err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\treturn nil\n}\n\nfunc (cpdb *MySQLCheckpointsDB) TaskCheckpoint(ctx context.Context) (*TaskCheckpoint, error) {\n\ts := common.SQLWithRetry{\n\t\tDB:     cpdb.db,\n\t\tLogger: log.FromContext(ctx),\n\t}\n\n\ttaskQuery := fmt.Sprintf(ReadTaskTemplate, cpdb.schema, CheckpointTableNameTask)\n\ttaskCp := &TaskCheckpoint{}\n\terr := s.QueryRow(ctx, \"fetch task checkpoint\", taskQuery, &taskCp.TaskID, &taskCp.SourceDir, &taskCp.Backend,\n\t\t&taskCp.ImporterAddr, &taskCp.TiDBHost, &taskCp.TiDBPort, &taskCp.PdAddr, &taskCp.SortedKVDir, &taskCp.LightningVer)\n\tif err != nil {\n\t\t// if task checkpoint is empty, return nil\n\t\tif errors.Cause(err) == sql.ErrNoRows {\n\t\t\treturn nil, nil\n\t\t}\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\treturn taskCp, nil\n}\n\nfunc (cpdb *MySQLCheckpointsDB) Close() error {\n\treturn errors.Trace(cpdb.db.Close())\n}\n\nfunc (cpdb *MySQLCheckpointsDB) Get(ctx context.Context, tableName string) (*TableCheckpoint, error) {\n\tcp := &TableCheckpoint{\n\t\tEngines: map[int32]*EngineCheckpoint{},\n\t}\n\n\ts := common.SQLWithRetry{\n\t\tDB:     cpdb.db,\n\t\tLogger: log.FromContext(ctx).With(zap.String(\"table\", tableName)),\n\t}\n\terr := s.Transact(ctx, \"read checkpoint\", func(c context.Context, tx *sql.Tx) error {\n\t\t// 1. Populate the engines.\n\n\t\tengineQuery := fmt.Sprintf(ReadEngineTemplate, cpdb.schema, CheckpointTableNameEngine)\n\t\tengineRows, err := tx.QueryContext(c, engineQuery, tableName)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer engineRows.Close()\n\t\tfor engineRows.Next() {\n\t\t\tvar (\n\t\t\t\tengineID int32\n\t\t\t\tstatus   uint8\n\t\t\t)\n\t\t\tif err := engineRows.Scan(&engineID, &status); err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\t\t\tcp.Engines[engineID] = &EngineCheckpoint{\n\t\t\t\tStatus: CheckpointStatus(status),\n\t\t\t}\n\t\t}\n\t\tif err := engineRows.Err(); err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\n\t\t// 2. Populate the chunks.\n\n\t\tchunkQuery := fmt.Sprintf(ReadChunkTemplate, cpdb.schema, CheckpointTableNameChunk)\n\t\tchunkRows, err := tx.QueryContext(c, chunkQuery, tableName)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer chunkRows.Close()\n\t\tfor chunkRows.Next() {\n\t\t\tvar (\n\t\t\t\tvalue       = &ChunkCheckpoint{}\n\t\t\t\tcolPerm     []byte\n\t\t\t\tengineID    int32\n\t\t\t\tkvcBytes    uint64\n\t\t\t\tkvcKVs      uint64\n\t\t\t\tkvcChecksum uint64\n\t\t\t)\n\t\t\tif err := chunkRows.Scan(\n\t\t\t\t&engineID, &value.Key.Path, &value.Key.Offset, &value.FileMeta.Type, &value.FileMeta.Compression,\n\t\t\t\t&value.FileMeta.SortKey, &value.FileMeta.FileSize, &colPerm, &value.Chunk.Offset, &value.Chunk.EndOffset,\n\t\t\t\t&value.Chunk.PrevRowIDMax, &value.Chunk.RowIDMax, &kvcBytes, &kvcKVs, &kvcChecksum,\n\t\t\t\t&value.Timestamp,\n\t\t\t); err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\t\t\tvalue.FileMeta.Path = value.Key.Path\n\t\t\tvalue.Checksum = verify.MakeKVChecksum(kvcBytes, kvcKVs, kvcChecksum)\n\t\t\tif err := json.Unmarshal(colPerm, &value.ColumnPermutation); err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\t\t\tcp.Engines[engineID].Chunks = append(cp.Engines[engineID].Chunks, value)\n\t\t}\n\t\tif err := chunkRows.Err(); err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\n\t\t// 3. Fill in the remaining table info\n\n\t\ttableQuery := fmt.Sprintf(ReadTableRemainTemplate, cpdb.schema, CheckpointTableNameTable)\n\t\ttableRow := tx.QueryRowContext(c, tableQuery, tableName)\n\n\t\tvar status uint8\n\t\tvar kvs, bytes, checksum uint64\n\t\tif err := tableRow.Scan(&status, &cp.AllocBase, &cp.TableID, &bytes, &kvs, &checksum); err != nil {\n\t\t\tif err == sql.ErrNoRows {\n\t\t\t\treturn errors.NotFoundf(\"checkpoint for table %s\", tableName)\n\t\t\t}\n\t\t}\n\t\tcp.Checksum = verify.MakeKVChecksum(bytes, kvs, checksum)\n\t\tcp.Status = CheckpointStatus(status)\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\treturn cp, nil\n}\n\nfunc (cpdb *MySQLCheckpointsDB) InsertEngineCheckpoints(ctx context.Context, tableName string, checkpoints map[int32]*EngineCheckpoint) error {\n\ts := common.SQLWithRetry{\n\t\tDB:     cpdb.db,\n\t\tLogger: log.FromContext(ctx).With(zap.String(\"table\", tableName)),\n\t}\n\terr := s.Transact(ctx, \"update engine checkpoints\", func(c context.Context, tx *sql.Tx) error {\n\t\tengineStmt, err := tx.PrepareContext(c, fmt.Sprintf(ReplaceEngineTemplate, cpdb.schema, CheckpointTableNameEngine))\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer engineStmt.Close()\n\n\t\tchunkStmt, err := tx.PrepareContext(c, fmt.Sprintf(ReplaceChunkTemplate, cpdb.schema, CheckpointTableNameChunk))\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer chunkStmt.Close()\n\n\t\tfor engineID, engine := range checkpoints {\n\t\t\t_, err = engineStmt.ExecContext(c, tableName, engineID, engine.Status)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\t\t\tfor _, value := range engine.Chunks {\n\t\t\t\tcolumnPerm, err := json.Marshal(value.ColumnPermutation)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Trace(err)\n\t\t\t\t}\n\t\t\t\t_, err = chunkStmt.ExecContext(\n\t\t\t\t\tc, tableName, engineID,\n\t\t\t\t\tvalue.Key.Path, value.Key.Offset, value.FileMeta.Type, value.FileMeta.Compression,\n\t\t\t\t\tvalue.FileMeta.SortKey, value.FileMeta.FileSize, columnPerm, value.Chunk.Offset, value.Chunk.EndOffset,\n\t\t\t\t\tvalue.Chunk.PrevRowIDMax, value.Chunk.RowIDMax, value.Timestamp,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Trace(err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\treturn nil\n}\n\nfunc (cpdb *MySQLCheckpointsDB) Update(taskCtx context.Context, checkpointDiffs map[string]*TableCheckpointDiff) error {\n\tchunkQuery := fmt.Sprintf(UpdateChunkTemplate, cpdb.schema, CheckpointTableNameChunk)\n\trebaseQuery := fmt.Sprintf(UpdateTableRebaseTemplate, cpdb.schema, CheckpointTableNameTable)\n\ttableStatusQuery := fmt.Sprintf(UpdateTableStatusTemplate, cpdb.schema, CheckpointTableNameTable)\n\ttableChecksumQuery := fmt.Sprintf(UpdateTableChecksumTemplate, cpdb.schema, CheckpointTableNameTable)\n\tengineStatusQuery := fmt.Sprintf(UpdateEngineTemplate, cpdb.schema, CheckpointTableNameEngine)\n\n\ts := common.SQLWithRetry{DB: cpdb.db, Logger: log.FromContext(taskCtx)}\n\treturn s.Transact(taskCtx, \"update checkpoints\", func(c context.Context, tx *sql.Tx) error {\n\t\tchunkStmt, e := tx.PrepareContext(c, chunkQuery)\n\t\tif e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer chunkStmt.Close()\n\t\trebaseStmt, e := tx.PrepareContext(c, rebaseQuery)\n\t\tif e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer rebaseStmt.Close()\n\t\ttableStatusStmt, e := tx.PrepareContext(c, tableStatusQuery)\n\t\tif e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer tableStatusStmt.Close()\n\t\ttableChecksumStmt, e := tx.PrepareContext(c, tableChecksumQuery)\n\t\tif e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer tableChecksumStmt.Close()\n\t\tengineStatusStmt, e := tx.PrepareContext(c, engineStatusQuery)\n\t\tif e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer engineStatusStmt.Close()\n\t\tfor tableName, cpd := range checkpointDiffs {\n\t\t\tif cpd.hasStatus {\n\t\t\t\tif _, e := tableStatusStmt.ExecContext(c, cpd.status, tableName); e != nil {\n\t\t\t\t\treturn errors.Trace(e)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif cpd.hasRebase {\n\t\t\t\tif _, e := rebaseStmt.ExecContext(c, cpd.allocBase, tableName); e != nil {\n\t\t\t\t\treturn errors.Trace(e)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif cpd.hasChecksum {\n\t\t\t\tif _, e := tableChecksumStmt.ExecContext(c, cpd.checksum.SumSize(), cpd.checksum.SumKVS(), cpd.checksum.Sum(), tableName); e != nil {\n\t\t\t\t\treturn errors.Trace(e)\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor engineID, engineDiff := range cpd.engines {\n\t\t\t\tif engineDiff.hasStatus {\n\t\t\t\t\tif _, e := engineStatusStmt.ExecContext(c, engineDiff.status, tableName, engineID); e != nil {\n\t\t\t\t\t\treturn errors.Trace(e)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfor key, diff := range engineDiff.chunks {\n\t\t\t\t\tcolumnPerm, err := json.Marshal(diff.columnPermutation)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn errors.Trace(err)\n\t\t\t\t\t}\n\t\t\t\t\tif _, e := chunkStmt.ExecContext(\n\t\t\t\t\t\tc,\n\t\t\t\t\t\tdiff.pos, diff.rowID, diff.checksum.SumSize(), diff.checksum.SumKVS(), diff.checksum.Sum(),\n\t\t\t\t\t\tcolumnPerm, tableName, engineID, key.Path, key.Offset,\n\t\t\t\t\t); e != nil {\n\t\t\t\t\t\treturn errors.Trace(e)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n}\n\ntype FileCheckpointsDB struct {\n\tlock        sync.Mutex // we need to ensure only a thread can access to `checkpoints` at a time\n\tcheckpoints checkpointspb.CheckpointsModel\n\tctx         context.Context\n\tpath        string\n\tfileName    string\n\texStorage   storage.ExternalStorage\n}\n\nfunc newFileCheckpointsDB(\n\tctx context.Context,\n\tpath string,\n\texStorage storage.ExternalStorage,\n\tfileName string,\n) (*FileCheckpointsDB, error) {\n\tcpdb := &FileCheckpointsDB{\n\t\tcheckpoints: checkpointspb.CheckpointsModel{\n\t\t\tTaskCheckpoint: &checkpointspb.TaskCheckpointModel{},\n\t\t\tCheckpoints:    map[string]*checkpointspb.TableCheckpointModel{},\n\t\t},\n\t\tctx:       ctx,\n\t\tpath:      path,\n\t\tfileName:  fileName,\n\t\texStorage: exStorage,\n\t}\n\n\tif cpdb.fileName == \"\" {\n\t\treturn nil, errors.Errorf(\"the checkpoint DSN '%s' must not be a directory\", path)\n\t}\n\n\texist, err := cpdb.exStorage.FileExists(ctx, cpdb.fileName)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tif !exist {\n\t\tlog.FromContext(ctx).Info(\"open checkpoint file failed, going to create a new one\",\n\t\t\tzap.String(\"path\", path),\n\t\t\tlog.ShortError(err),\n\t\t)\n\t\treturn cpdb, nil\n\t}\n\tcontent, err := cpdb.exStorage.ReadFile(ctx, cpdb.fileName)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\terr = cpdb.checkpoints.Unmarshal(content)\n\tif err != nil {\n\t\tlog.FromContext(ctx).Error(\"checkpoint file is broken\", zap.String(\"path\", path), zap.Error(err))\n\t}\n\t// FIXME: patch for empty map may need initialize manually, because currently\n\t// FIXME: a map of zero size -> marshall -> unmarshall -> become nil, see checkpoint_test.go\n\tif cpdb.checkpoints.Checkpoints == nil {\n\t\tcpdb.checkpoints.Checkpoints = map[string]*checkpointspb.TableCheckpointModel{}\n\t}\n\tfor _, table := range cpdb.checkpoints.Checkpoints {\n\t\tif table.Engines == nil {\n\t\t\ttable.Engines = map[int32]*checkpointspb.EngineCheckpointModel{}\n\t\t}\n\t\tfor _, engine := range table.Engines {\n\t\t\tif engine.Chunks == nil {\n\t\t\t\tengine.Chunks = map[string]*checkpointspb.ChunkCheckpointModel{}\n\t\t\t}\n\t\t}\n\t}\n\treturn cpdb, nil\n}\n\nfunc NewFileCheckpointsDB(ctx context.Context, path string) (*FileCheckpointsDB, error) {\n\t// init ExternalStorage\n\ts, fileName, err := createExstorageByCompletePath(ctx, path)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\treturn newFileCheckpointsDB(ctx, path, s, fileName)\n}\n\nfunc NewFileCheckpointsDBWithExstorageFileName(\n\tctx context.Context,\n\tpath string,\n\ts storage.ExternalStorage,\n\tfileName string,\n) (*FileCheckpointsDB, error) {\n\treturn newFileCheckpointsDB(ctx, path, s, fileName)\n}\n\n// createExstorageByCompletePath create ExternalStorage by completePath and return fileName.\nfunc createExstorageByCompletePath(ctx context.Context, completePath string) (storage.ExternalStorage, string, error) {\n\tif completePath == \"\" {\n\t\treturn nil, \"\", nil\n\t}\n\tfileName, newPath, err := separateCompletePath(completePath)\n\tif err != nil {\n\t\treturn nil, \"\", errors.Trace(err)\n\t}\n\tu, err := storage.ParseBackend(newPath, nil)\n\tif err != nil {\n\t\treturn nil, \"\", errors.Trace(err)\n\t}\n\ts, err := storage.New(ctx, u, &storage.ExternalStorageOptions{})\n\tif err != nil {\n\t\treturn nil, \"\", errors.Trace(err)\n\t}\n\treturn s, fileName, nil\n}\n\n// separateCompletePath separates fileName from completePath, returns fileName and newPath.\nfunc separateCompletePath(completePath string) (string, string, error) {\n\tif completePath == \"\" {\n\t\treturn \"\", \"\", nil\n\t}\n\tvar fileName, newPath string\n\tpurl, err := storage.ParseRawURL(completePath)\n\tif err != nil {\n\t\treturn \"\", \"\", errors.Trace(err)\n\t}\n\t// not url format, we don't use url library to avoid being escaped or unescaped\n\tif purl.Scheme == \"\" {\n\t\t// no fileName, just path\n\t\tif strings.HasSuffix(completePath, \"/\") {\n\t\t\treturn \"\", completePath, nil\n\t\t}\n\t\tfileName = path.Base(completePath)\n\t\tnewPath = path.Dir(completePath)\n\t} else {\n\t\tif strings.HasSuffix(purl.Path, \"/\") {\n\t\t\treturn \"\", completePath, nil\n\t\t}\n\t\tfileName = path.Base(purl.Path)\n\t\tpurl.Path = path.Dir(purl.Path)\n\t\tnewPath = purl.String()\n\t}\n\treturn fileName, newPath, nil\n}\n\nfunc (cpdb *FileCheckpointsDB) save() error {\n\tserialized, err := cpdb.checkpoints.Marshal()\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\treturn cpdb.exStorage.WriteFile(cpdb.ctx, cpdb.fileName, serialized)\n}\n\nfunc (cpdb *FileCheckpointsDB) Initialize(ctx context.Context, cfg *config.Config, dbInfo map[string]*TidbDBInfo) error {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\tcpdb.checkpoints.TaskCheckpoint = &checkpointspb.TaskCheckpointModel{\n\t\tTaskId:       cfg.TaskID,\n\t\tSourceDir:    cfg.Mydumper.SourceDir,\n\t\tBackend:      cfg.TikvImporter.Backend,\n\t\tImporterAddr: cfg.TikvImporter.Addr,\n\t\tTidbHost:     cfg.TiDB.Host,\n\t\tTidbPort:     int32(cfg.TiDB.Port),\n\t\tPdAddr:       cfg.TiDB.PdAddr,\n\t\tSortedKvDir:  cfg.TikvImporter.SortedKVDir,\n\t\tLightningVer: build.ReleaseVersion,\n\t}\n\n\tif cpdb.checkpoints.Checkpoints == nil {\n\t\tcpdb.checkpoints.Checkpoints = make(map[string]*checkpointspb.TableCheckpointModel)\n\t}\n\n\tfor _, db := range dbInfo {\n\t\tfor _, table := range db.Tables {\n\t\t\ttableName := common.UniqueTable(db.Name, table.Name)\n\t\t\tif _, ok := cpdb.checkpoints.Checkpoints[tableName]; !ok {\n\t\t\t\tcpdb.checkpoints.Checkpoints[tableName] = &checkpointspb.TableCheckpointModel{\n\t\t\t\t\tStatus:  uint32(CheckpointStatusLoaded),\n\t\t\t\t\tEngines: map[int32]*checkpointspb.EngineCheckpointModel{},\n\t\t\t\t\tTableID: table.ID,\n\t\t\t\t}\n\t\t\t}\n\t\t\t// TODO check if hash matches\n\t\t}\n\t}\n\n\treturn errors.Trace(cpdb.save())\n}\n\nfunc (cpdb *FileCheckpointsDB) TaskCheckpoint(_ context.Context) (*TaskCheckpoint, error) {\n\t// this method is always called in lock\n\tcp := cpdb.checkpoints.TaskCheckpoint\n\tif cp == nil || cp.TaskId == 0 {\n\t\treturn nil, nil\n\t}\n\n\treturn &TaskCheckpoint{\n\t\tTaskID:       cp.TaskId,\n\t\tSourceDir:    cp.SourceDir,\n\t\tBackend:      cp.Backend,\n\t\tImporterAddr: cp.ImporterAddr,\n\t\tTiDBHost:     cp.TidbHost,\n\t\tTiDBPort:     int(cp.TidbPort),\n\t\tPdAddr:       cp.PdAddr,\n\t\tSortedKVDir:  cp.SortedKvDir,\n\t\tLightningVer: cp.LightningVer,\n\t}, nil\n}\n\nfunc (cpdb *FileCheckpointsDB) Close() error {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\treturn errors.Trace(cpdb.save())\n}\n\nfunc (cpdb *FileCheckpointsDB) Get(_ context.Context, tableName string) (*TableCheckpoint, error) {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\ttableModel, ok := cpdb.checkpoints.Checkpoints[tableName]\n\tif !ok {\n\t\treturn nil, errors.NotFoundf(\"checkpoint for table %s\", tableName)\n\t}\n\n\tcp := &TableCheckpoint{\n\t\tStatus:    CheckpointStatus(tableModel.Status),\n\t\tAllocBase: tableModel.AllocBase,\n\t\tEngines:   make(map[int32]*EngineCheckpoint, len(tableModel.Engines)),\n\t\tTableID:   tableModel.TableID,\n\t\tChecksum:  verify.MakeKVChecksum(tableModel.KvBytes, tableModel.KvKvs, tableModel.KvChecksum),\n\t}\n\n\tfor engineID, engineModel := range tableModel.Engines {\n\t\tengine := &EngineCheckpoint{\n\t\t\tStatus: CheckpointStatus(engineModel.Status),\n\t\t\tChunks: make([]*ChunkCheckpoint, 0, len(engineModel.Chunks)),\n\t\t}\n\n\t\tfor _, chunkModel := range engineModel.Chunks {\n\t\t\tcolPerm := make([]int, 0, len(chunkModel.ColumnPermutation))\n\t\t\tfor _, c := range chunkModel.ColumnPermutation {\n\t\t\t\tcolPerm = append(colPerm, int(c))\n\t\t\t}\n\t\t\tengine.Chunks = append(engine.Chunks, &ChunkCheckpoint{\n\t\t\t\tKey: ChunkCheckpointKey{\n\t\t\t\t\tPath:   chunkModel.Path,\n\t\t\t\t\tOffset: chunkModel.Offset,\n\t\t\t\t},\n\t\t\t\tFileMeta: mydump.SourceFileMeta{\n\t\t\t\t\tPath:        chunkModel.Path,\n\t\t\t\t\tType:        mydump.SourceType(chunkModel.Type),\n\t\t\t\t\tCompression: mydump.Compression(chunkModel.Compression),\n\t\t\t\t\tSortKey:     chunkModel.SortKey,\n\t\t\t\t\tFileSize:    chunkModel.FileSize,\n\t\t\t\t},\n\t\t\t\tColumnPermutation: colPerm,\n\t\t\t\tChunk: mydump.Chunk{\n\t\t\t\t\tOffset:       chunkModel.Pos,\n\t\t\t\t\tEndOffset:    chunkModel.EndOffset,\n\t\t\t\t\tPrevRowIDMax: chunkModel.PrevRowidMax,\n\t\t\t\t\tRowIDMax:     chunkModel.RowidMax,\n\t\t\t\t},\n\t\t\t\tChecksum:  verify.MakeKVChecksum(chunkModel.KvcBytes, chunkModel.KvcKvs, chunkModel.KvcChecksum),\n\t\t\t\tTimestamp: chunkModel.Timestamp,\n\t\t\t})\n\t\t}\n\n\t\tslices.SortFunc(engine.Chunks, func(i, j *ChunkCheckpoint) bool {\n\t\t\treturn i.Key.less(&j.Key)\n\t\t})\n\n\t\tcp.Engines[engineID] = engine\n\t}\n\n\treturn cp, nil\n}\n\nfunc (cpdb *FileCheckpointsDB) InsertEngineCheckpoints(_ context.Context, tableName string, checkpoints map[int32]*EngineCheckpoint) error {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\ttableModel := cpdb.checkpoints.Checkpoints[tableName]\n\tfor engineID, engine := range checkpoints {\n\t\tengineModel := &checkpointspb.EngineCheckpointModel{\n\t\t\tStatus: uint32(CheckpointStatusLoaded),\n\t\t\tChunks: make(map[string]*checkpointspb.ChunkCheckpointModel),\n\t\t}\n\t\tfor _, value := range engine.Chunks {\n\t\t\tkey := value.Key.String()\n\t\t\tchunk, ok := engineModel.Chunks[key]\n\t\t\tif !ok {\n\t\t\t\tchunk = &checkpointspb.ChunkCheckpointModel{\n\t\t\t\t\tPath:   value.Key.Path,\n\t\t\t\t\tOffset: value.Key.Offset,\n\t\t\t\t}\n\t\t\t\tengineModel.Chunks[key] = chunk\n\t\t\t}\n\t\t\tchunk.Type = int32(value.FileMeta.Type)\n\t\t\tchunk.Compression = int32(value.FileMeta.Compression)\n\t\t\tchunk.SortKey = value.FileMeta.SortKey\n\t\t\tchunk.FileSize = value.FileMeta.FileSize\n\t\t\tchunk.Pos = value.Chunk.Offset\n\t\t\tchunk.EndOffset = value.Chunk.EndOffset\n\t\t\tchunk.PrevRowidMax = value.Chunk.PrevRowIDMax\n\t\t\tchunk.RowidMax = value.Chunk.RowIDMax\n\t\t\tchunk.Timestamp = value.Timestamp\n\t\t\tif len(value.ColumnPermutation) > 0 {\n\t\t\t\tchunk.ColumnPermutation = intSlice2Int32Slice(value.ColumnPermutation)\n\t\t\t}\n\t\t}\n\t\ttableModel.Engines[engineID] = engineModel\n\t}\n\n\treturn errors.Trace(cpdb.save())\n}\n\nfunc (cpdb *FileCheckpointsDB) Update(_ context.Context, checkpointDiffs map[string]*TableCheckpointDiff) error {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\tfor tableName, cpd := range checkpointDiffs {\n\t\ttableModel := cpdb.checkpoints.Checkpoints[tableName]\n\t\tif cpd.hasStatus {\n\t\t\ttableModel.Status = uint32(cpd.status)\n\t\t}\n\t\tif cpd.hasRebase {\n\t\t\ttableModel.AllocBase = cpd.allocBase\n\t\t}\n\t\tif cpd.hasChecksum {\n\t\t\ttableModel.KvBytes = cpd.checksum.SumSize()\n\t\t\ttableModel.KvKvs = cpd.checksum.SumKVS()\n\t\t\ttableModel.KvChecksum = cpd.checksum.Sum()\n\t\t}\n\t\tfor engineID, engineDiff := range cpd.engines {\n\t\t\tengineModel := tableModel.Engines[engineID]\n\t\t\tif engineDiff.hasStatus {\n\t\t\t\tengineModel.Status = uint32(engineDiff.status)\n\t\t\t}\n\n\t\t\tfor key, diff := range engineDiff.chunks {\n\t\t\t\tchunkModel := engineModel.Chunks[key.String()]\n\t\t\t\tchunkModel.Pos = diff.pos\n\t\t\t\tchunkModel.PrevRowidMax = diff.rowID\n\t\t\t\tchunkModel.KvcBytes = diff.checksum.SumSize()\n\t\t\t\tchunkModel.KvcKvs = diff.checksum.SumKVS()\n\t\t\t\tchunkModel.KvcChecksum = diff.checksum.Sum()\n\t\t\t\tchunkModel.ColumnPermutation = intSlice2Int32Slice(diff.columnPermutation)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn cpdb.save()\n}\n\n// Management functions ----------------------------------------------------------------------------\n\nvar errCannotManageNullDB = errors.New(\"cannot perform this function while checkpoints is disabled\")\n\nfunc (*NullCheckpointsDB) RemoveCheckpoint(context.Context, string) error {\n\treturn errors.Trace(errCannotManageNullDB)\n}\n\nfunc (*NullCheckpointsDB) MoveCheckpoints(context.Context, int64) error {\n\treturn errors.Trace(errCannotManageNullDB)\n}\n\nfunc (*NullCheckpointsDB) GetLocalStoringTables(context.Context) (map[string][]int32, error) {\n\treturn nil, nil\n}\n\nfunc (*NullCheckpointsDB) IgnoreErrorCheckpoint(context.Context, string) error {\n\treturn errors.Trace(errCannotManageNullDB)\n}\n\nfunc (*NullCheckpointsDB) DestroyErrorCheckpoint(context.Context, string) ([]DestroyedTableCheckpoint, error) {\n\treturn nil, errors.Trace(errCannotManageNullDB)\n}\n\nfunc (*NullCheckpointsDB) DumpTables(context.Context, io.Writer) error {\n\treturn errors.Trace(errCannotManageNullDB)\n}\n\nfunc (*NullCheckpointsDB) DumpEngines(context.Context, io.Writer) error {\n\treturn errors.Trace(errCannotManageNullDB)\n}\n\nfunc (*NullCheckpointsDB) DumpChunks(context.Context, io.Writer) error {\n\treturn errors.Trace(errCannotManageNullDB)\n}\n\nfunc (cpdb *MySQLCheckpointsDB) RemoveCheckpoint(ctx context.Context, tableName string) error {\n\ts := common.SQLWithRetry{\n\t\tDB:     cpdb.db,\n\t\tLogger: log.FromContext(ctx).With(zap.String(\"table\", tableName)),\n\t}\n\n\tif tableName == allTables {\n\t\treturn s.Exec(ctx, \"remove all checkpoints\", \"DROP SCHEMA \"+cpdb.schema)\n\t}\n\n\tdeleteChunkQuery := fmt.Sprintf(DeleteCheckpointRecordTemplate, cpdb.schema, CheckpointTableNameChunk)\n\tdeleteEngineQuery := fmt.Sprintf(DeleteCheckpointRecordTemplate, cpdb.schema, CheckpointTableNameEngine)\n\tdeleteTableQuery := fmt.Sprintf(DeleteCheckpointRecordTemplate, cpdb.schema, CheckpointTableNameTable)\n\n\treturn s.Transact(ctx, \"remove checkpoints\", func(c context.Context, tx *sql.Tx) error {\n\t\tif _, e := tx.ExecContext(c, deleteChunkQuery, tableName); e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\tif _, e := tx.ExecContext(c, deleteEngineQuery, tableName); e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\tif _, e := tx.ExecContext(c, deleteTableQuery, tableName); e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\treturn nil\n\t})\n}\n\nfunc (cpdb *MySQLCheckpointsDB) MoveCheckpoints(ctx context.Context, taskID int64) error {\n\t// The \"cpdb.schema\" is an escaped schema name of the form \"`foo`\".\n\t// We use \"x[1:len(x)-1]\" instead of unescaping it to keep the\n\t// double-backquotes (if any) intact.\n\tnewSchema := fmt.Sprintf(\"`%s.%d.bak`\", cpdb.schema[1:len(cpdb.schema)-1], taskID)\n\ts := common.SQLWithRetry{\n\t\tDB:     cpdb.db,\n\t\tLogger: log.FromContext(ctx).With(zap.Int64(\"taskID\", taskID)),\n\t}\n\n\tcreateSchemaQuery := \"CREATE SCHEMA IF NOT EXISTS \" + newSchema\n\tif e := s.Exec(ctx, \"create backup checkpoints schema\", createSchemaQuery); e != nil {\n\t\treturn e\n\t}\n\tfor _, tbl := range []string{\n\t\tCheckpointTableNameChunk, CheckpointTableNameEngine,\n\t\tCheckpointTableNameTable, CheckpointTableNameTask,\n\t} {\n\t\tquery := fmt.Sprintf(\"RENAME TABLE %[1]s.%[3]s TO %[2]s.%[3]s\", cpdb.schema, newSchema, tbl)\n\t\tif e := s.Exec(ctx, fmt.Sprintf(\"move %s checkpoints table\", tbl), query); e != nil {\n\t\t\treturn e\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (cpdb *MySQLCheckpointsDB) GetLocalStoringTables(ctx context.Context) (map[string][]int32, error) {\n\tvar targetTables map[string][]int32\n\n\t// lightning didn't check CheckpointStatusMaxInvalid before this function is called, so we skip invalid ones\n\t// engines should exist if\n\t// 1. table status is earlier than CheckpointStatusIndexImported, and\n\t// 2. engine status is earlier than CheckpointStatusImported, and\n\t// 3. chunk has been read\n\n\tquery := fmt.Sprintf(`\n\t\tSELECT DISTINCT t.table_name, c.engine_id\n\t\tFROM %s.%s t, %s.%s c, %s.%s e\n\t\tWHERE t.table_name = c.table_name AND t.table_name = e.table_name AND c.engine_id = e.engine_id\n\t\t\tAND %d < t.status AND t.status < %d\n\t\t\tAND %d < e.status AND e.status < %d\n\t\t\tAND c.pos > c.offset;`,\n\t\tcpdb.schema, CheckpointTableNameTable, cpdb.schema, CheckpointTableNameChunk, cpdb.schema, CheckpointTableNameEngine,\n\t\tCheckpointStatusMaxInvalid, CheckpointStatusIndexImported,\n\t\tCheckpointStatusMaxInvalid, CheckpointStatusImported)\n\n\terr := common.Retry(\"get local storing tables\", log.FromContext(ctx), func() error {\n\t\ttargetTables = make(map[string][]int32)\n\t\trows, err := cpdb.db.QueryContext(ctx, query) // #nosec G201\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer rows.Close()\n\t\tfor rows.Next() {\n\t\t\tvar (\n\t\t\t\ttableName string\n\t\t\t\tengineID  int32\n\t\t\t)\n\t\t\tif err := rows.Scan(&tableName, &engineID); err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\t\t\ttargetTables[tableName] = append(targetTables[tableName], engineID)\n\t\t}\n\t\tif err := rows.Err(); err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\treturn targetTables, err\n}\n\nfunc (cpdb *MySQLCheckpointsDB) IgnoreErrorCheckpoint(ctx context.Context, tableName string) error {\n\tvar colName string\n\tif tableName == allTables {\n\t\t// This will expand to `WHERE 'all' = 'all'` and effectively allowing\n\t\t// all tables to be included.\n\t\tcolName = stringLitAll\n\t} else {\n\t\tcolName = columnTableName\n\t}\n\n\t// nolint:gosec\n\tengineQuery := fmt.Sprintf(`\n\t\tUPDATE %s.%s SET status = %d WHERE %s = ? AND status <= %d;\n\t`, cpdb.schema, CheckpointTableNameEngine, CheckpointStatusLoaded, colName, CheckpointStatusMaxInvalid)\n\n\t// nolint:gosec\n\ttableQuery := fmt.Sprintf(`\n\t\tUPDATE %s.%s SET status = %d WHERE %s = ? AND status <= %d;\n\t`, cpdb.schema, CheckpointTableNameTable, CheckpointStatusLoaded, colName, CheckpointStatusMaxInvalid)\n\n\ts := common.SQLWithRetry{\n\t\tDB:     cpdb.db,\n\t\tLogger: log.FromContext(ctx).With(zap.String(\"table\", tableName)),\n\t}\n\terr := s.Transact(ctx, \"ignore error checkpoints\", func(c context.Context, tx *sql.Tx) error {\n\t\tif _, e := tx.ExecContext(c, engineQuery, tableName); e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\tif _, e := tx.ExecContext(c, tableQuery, tableName); e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\treturn nil\n\t})\n\treturn errors.Trace(err)\n}\n\nfunc (cpdb *MySQLCheckpointsDB) DestroyErrorCheckpoint(ctx context.Context, tableName string) ([]DestroyedTableCheckpoint, error) {\n\tvar colName, aliasedColName string\n\n\tif tableName == allTables {\n\t\t// These will expand to `WHERE 'all' = 'all'` and effectively allowing\n\t\t// all tables to be included.\n\t\tcolName = stringLitAll\n\t\taliasedColName = stringLitAll\n\t} else {\n\t\tcolName = columnTableName\n\t\taliasedColName = \"t.table_name\"\n\t}\n\n\tselectQuery := fmt.Sprintf(`\n\t\tSELECT\n\t\t\tt.table_name,\n\t\t\tCOALESCE(MIN(e.engine_id), 0),\n\t\t\tCOALESCE(MAX(e.engine_id), -1)\n\t\tFROM %[1]s.%[4]s t\n\t\tLEFT JOIN %[1]s.%[5]s e ON t.table_name = e.table_name\n\t\tWHERE %[2]s = ? AND t.status <= %[3]d\n\t\tGROUP BY t.table_name;\n\t`, cpdb.schema, aliasedColName, CheckpointStatusMaxInvalid, CheckpointTableNameTable, CheckpointTableNameEngine)\n\n\t// nolint:gosec\n\tdeleteChunkQuery := fmt.Sprintf(`\n\t\tDELETE FROM %[1]s.%[4]s WHERE table_name IN (SELECT table_name FROM %[1]s.%[5]s WHERE %[2]s = ? AND status <= %[3]d)\n\t`, cpdb.schema, colName, CheckpointStatusMaxInvalid, CheckpointTableNameChunk, CheckpointTableNameTable)\n\n\t// nolint:gosec\n\tdeleteEngineQuery := fmt.Sprintf(`\n\t\tDELETE FROM %[1]s.%[4]s WHERE table_name IN (SELECT table_name FROM %[1]s.%[5]s WHERE %[2]s = ? AND status <= %[3]d)\n\t`, cpdb.schema, colName, CheckpointStatusMaxInvalid, CheckpointTableNameEngine, CheckpointTableNameTable)\n\n\t// nolint:gosec\n\tdeleteTableQuery := fmt.Sprintf(`\n\t\tDELETE FROM %s.%s WHERE %s = ? AND status <= %d\n\t`, cpdb.schema, CheckpointTableNameTable, colName, CheckpointStatusMaxInvalid)\n\n\tvar targetTables []DestroyedTableCheckpoint\n\n\ts := common.SQLWithRetry{\n\t\tDB:     cpdb.db,\n\t\tLogger: log.FromContext(ctx).With(zap.String(\"table\", tableName)),\n\t}\n\terr := s.Transact(ctx, \"destroy error checkpoints\", func(c context.Context, tx *sql.Tx) error {\n\t\t// Obtain the list of tables\n\t\ttargetTables = nil\n\t\trows, e := tx.QueryContext(c, selectQuery, tableName) // #nosec G201\n\t\tif e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer rows.Close()\n\t\tfor rows.Next() {\n\t\t\tvar dtc DestroyedTableCheckpoint\n\t\t\tif e := rows.Scan(&dtc.TableName, &dtc.MinEngineID, &dtc.MaxEngineID); e != nil {\n\t\t\t\treturn errors.Trace(e)\n\t\t\t}\n\t\t\ttargetTables = append(targetTables, dtc)\n\t\t}\n\t\tif e := rows.Err(); e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\n\t\t// Delete the checkpoints\n\t\tif _, e := tx.ExecContext(c, deleteChunkQuery, tableName); e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\tif _, e := tx.ExecContext(c, deleteEngineQuery, tableName); e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\tif _, e := tx.ExecContext(c, deleteTableQuery, tableName); e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\treturn targetTables, nil\n}\n\n//nolint:rowserrcheck // sqltocsv.Write will check this.\nfunc (cpdb *MySQLCheckpointsDB) DumpTables(ctx context.Context, writer io.Writer) error {\n\t//nolint: rowserrcheck\n\trows, err := cpdb.db.QueryContext(ctx, fmt.Sprintf(`\n\t\tSELECT\n\t\t\ttask_id,\n\t\t\ttable_name,\n\t\t\thex(hash) AS hash,\n\t\t\tstatus,\n\t\t\talloc_base,\n\t\t\tcreate_time,\n\t\t\tupdate_time\n\t\tFROM %s.%s;\n\t`, cpdb.schema, CheckpointTableNameTable))\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\t//nolint: errcheck\n\tdefer rows.Close()\n\n\treturn errors.Trace(sqltocsv.Write(writer, rows))\n}\n\n//nolint:rowserrcheck // sqltocsv.Write will check this.\nfunc (cpdb *MySQLCheckpointsDB) DumpEngines(ctx context.Context, writer io.Writer) error {\n\t//nolint: rowserrcheck\n\trows, err := cpdb.db.QueryContext(ctx, fmt.Sprintf(`\n\t\tSELECT\n\t\t\ttable_name,\n\t\t\tengine_id,\n\t\t\tstatus,\n\t\t\tcreate_time,\n\t\t\tupdate_time\n\t\tFROM %s.%s;\n\t`, cpdb.schema, CheckpointTableNameEngine))\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\t//nolint: errcheck\n\tdefer rows.Close()\n\n\treturn errors.Trace(sqltocsv.Write(writer, rows))\n}\n\n//nolint:rowserrcheck // sqltocsv.Write will check this.\nfunc (cpdb *MySQLCheckpointsDB) DumpChunks(ctx context.Context, writer io.Writer) error {\n\t//nolint: rowserrcheck\n\trows, err := cpdb.db.QueryContext(ctx, fmt.Sprintf(`\n\t\tSELECT\n\t\t\ttable_name,\n\t\t\tpath,\n\t\t\toffset,\n\t\t\ttype,\n\t\t\tcompression,\n\t\t\tsort_key,\n\t\t\tfile_size,\n\t\t\tcolumns,\n\t\t\tpos,\n\t\t\tend_offset,\n\t\t\tprev_rowid_max,\n\t\t\trowid_max,\n\t\t\tkvc_bytes,\n\t\t\tkvc_kvs,\n\t\t\tkvc_checksum,\n\t\t\tcreate_time,\n\t\t\tupdate_time\n\t\tFROM %s.%s;\n\t`, cpdb.schema, CheckpointTableNameChunk))\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\t//nolint: errcheck\n\tdefer rows.Close()\n\n\treturn errors.Trace(sqltocsv.Write(writer, rows))\n}\n\nfunc (cpdb *FileCheckpointsDB) RemoveCheckpoint(_ context.Context, tableName string) error {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\tif tableName == allTables {\n\t\tcpdb.checkpoints.Reset()\n\t\treturn errors.Trace(cpdb.exStorage.DeleteFile(cpdb.ctx, cpdb.fileName))\n\t}\n\n\tdelete(cpdb.checkpoints.Checkpoints, tableName)\n\treturn errors.Trace(cpdb.save())\n}\n\nfunc (cpdb *FileCheckpointsDB) MoveCheckpoints(ctx context.Context, taskID int64) error {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\tnewFileName := fmt.Sprintf(\"%s.%d.bak\", cpdb.fileName, taskID)\n\treturn cpdb.exStorage.Rename(cpdb.ctx, cpdb.fileName, newFileName)\n}\n\nfunc (cpdb *FileCheckpointsDB) GetLocalStoringTables(_ context.Context) (map[string][]int32, error) {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\ttargetTables := make(map[string][]int32)\n\n\tfor tableName, tableModel := range cpdb.checkpoints.Checkpoints {\n\t\tif tableModel.Status <= uint32(CheckpointStatusMaxInvalid) ||\n\t\t\ttableModel.Status >= uint32(CheckpointStatusIndexImported) {\n\t\t\tcontinue\n\t\t}\n\t\tfor engineID, engineModel := range tableModel.Engines {\n\t\t\tif engineModel.Status <= uint32(CheckpointStatusMaxInvalid) ||\n\t\t\t\tengineModel.Status >= uint32(CheckpointStatusImported) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tfor _, chunkModel := range engineModel.Chunks {\n\t\t\t\tif chunkModel.Pos > chunkModel.Offset {\n\t\t\t\t\ttargetTables[tableName] = append(targetTables[tableName], engineID)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn targetTables, nil\n}\n\nfunc (cpdb *FileCheckpointsDB) IgnoreErrorCheckpoint(_ context.Context, targetTableName string) error {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\tfor tableName, tableModel := range cpdb.checkpoints.Checkpoints {\n\t\tif !(targetTableName == allTables || targetTableName == tableName) {\n\t\t\tcontinue\n\t\t}\n\t\tif tableModel.Status <= uint32(CheckpointStatusMaxInvalid) {\n\t\t\ttableModel.Status = uint32(CheckpointStatusLoaded)\n\t\t}\n\t\tfor _, engineModel := range tableModel.Engines {\n\t\t\tif engineModel.Status <= uint32(CheckpointStatusMaxInvalid) {\n\t\t\t\tengineModel.Status = uint32(CheckpointStatusLoaded)\n\t\t\t}\n\t\t}\n\t}\n\treturn errors.Trace(cpdb.save())\n}\n\nfunc (cpdb *FileCheckpointsDB) DestroyErrorCheckpoint(_ context.Context, targetTableName string) ([]DestroyedTableCheckpoint, error) {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\tvar targetTables []DestroyedTableCheckpoint\n\n\tfor tableName, tableModel := range cpdb.checkpoints.Checkpoints {\n\t\t// Obtain the list of tables\n\t\tif !(targetTableName == allTables || targetTableName == tableName) {\n\t\t\tcontinue\n\t\t}\n\t\tif tableModel.Status <= uint32(CheckpointStatusMaxInvalid) {\n\t\t\tvar minEngineID, maxEngineID int32 = math.MaxInt32, math.MinInt32\n\t\t\tfor engineID := range tableModel.Engines {\n\t\t\t\tif engineID < minEngineID {\n\t\t\t\t\tminEngineID = engineID\n\t\t\t\t}\n\t\t\t\tif engineID > maxEngineID {\n\t\t\t\t\tmaxEngineID = engineID\n\t\t\t\t}\n\t\t\t}\n\n\t\t\ttargetTables = append(targetTables, DestroyedTableCheckpoint{\n\t\t\t\tTableName:   tableName,\n\t\t\t\tMinEngineID: minEngineID,\n\t\t\t\tMaxEngineID: maxEngineID,\n\t\t\t})\n\t\t}\n\t}\n\n\t// Delete the checkpoints\n\tfor _, dtcp := range targetTables {\n\t\tdelete(cpdb.checkpoints.Checkpoints, dtcp.TableName)\n\t}\n\tif err := cpdb.save(); err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\treturn targetTables, nil\n}\n\nfunc (cpdb *FileCheckpointsDB) DumpTables(context.Context, io.Writer) error {\n\treturn errors.Errorf(\"dumping file checkpoint into CSV not unsupported, you may copy %s instead\", cpdb.path)\n}\n\nfunc (cpdb *FileCheckpointsDB) DumpEngines(context.Context, io.Writer) error {\n\treturn errors.Errorf(\"dumping file checkpoint into CSV not unsupported, you may copy %s instead\", cpdb.path)\n}\n\nfunc (cpdb *FileCheckpointsDB) DumpChunks(context.Context, io.Writer) error {\n\treturn errors.Errorf(\"dumping file checkpoint into CSV not unsupported, you may copy %s instead\", cpdb.path)\n}\n\nfunc intSlice2Int32Slice(s []int) []int32 {\n\tres := make([]int32, 0, len(s))\n\tfor _, i := range s {\n\t\tres = append(res, int32(i))\n\t}\n\treturn res\n}\n", "// Copyright 2019 PingCAP, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage common\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"os\"\n\t\"strconv\"\n\t\"strings\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/go-sql-driver/mysql\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/failpoint\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/log\"\n\t\"github.com/pingcap/tidb/br/pkg/utils\"\n\ttmysql \"github.com/pingcap/tidb/errno\"\n\t\"github.com/pingcap/tidb/parser/model\"\n\t\"go.uber.org/zap\"\n)\n\nconst (\n\tretryTimeout = 3 * time.Second\n\n\tdefaultMaxRetry = 3\n)\n\n// MySQLConnectParam records the parameters needed to connect to a MySQL database.\ntype MySQLConnectParam struct {\n\tHost             string\n\tPort             int\n\tUser             string\n\tPassword         string\n\tSQLMode          string\n\tMaxAllowedPacket uint64\n\tTLS              string\n\tVars             map[string]string\n}\n\nfunc (param *MySQLConnectParam) ToDSN() string {\n\thostPort := net.JoinHostPort(param.Host, strconv.Itoa(param.Port))\n\tdsn := fmt.Sprintf(\"%s:%s@tcp(%s)/?charset=utf8mb4&sql_mode='%s'&maxAllowedPacket=%d&tls=%s\",\n\t\tparam.User, param.Password, hostPort,\n\t\tparam.SQLMode, param.MaxAllowedPacket, param.TLS)\n\n\tfor k, v := range param.Vars {\n\t\tdsn += fmt.Sprintf(\"&%s='%s'\", k, url.QueryEscape(v))\n\t}\n\n\treturn dsn\n}\n\nfunc tryConnectMySQL(dsn string) (*sql.DB, error) {\n\tdriverName := \"mysql\"\n\tfailpoint.Inject(\"MockMySQLDriver\", func(val failpoint.Value) {\n\t\tdriverName = val.(string)\n\t})\n\tdb, err := sql.Open(driverName, dsn)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tif err = db.Ping(); err != nil {\n\t\t_ = db.Close()\n\t\treturn nil, errors.Trace(err)\n\t}\n\treturn db, nil\n}\n\n// ConnectMySQL connects MySQL with the dsn. If access is denied and the password is a valid base64 encoding,\n// we will try to connect MySQL with the base64 decoding of the password.\nfunc ConnectMySQL(dsn string) (*sql.DB, error) {\n\tcfg, err := mysql.ParseDSN(dsn)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\t// Try plain password first.\n\tdb, firstErr := tryConnectMySQL(dsn)\n\tif firstErr == nil {\n\t\treturn db, nil\n\t}\n\t// If access is denied and password is encoded by base64, try the decoded string as well.\n\tif mysqlErr, ok := errors.Cause(firstErr).(*mysql.MySQLError); ok && mysqlErr.Number == tmysql.ErrAccessDenied {\n\t\t// If password is encoded by base64, try the decoded string as well.\n\t\tif password, decodeErr := base64.StdEncoding.DecodeString(cfg.Passwd); decodeErr == nil && string(password) != cfg.Passwd {\n\t\t\tcfg.Passwd = string(password)\n\t\t\tdb, err = tryConnectMySQL(cfg.FormatDSN())\n\t\t\tif err == nil {\n\t\t\t\treturn db, nil\n\t\t\t}\n\t\t}\n\t}\n\t// If we can't connect successfully, return the first error.\n\treturn nil, errors.Trace(firstErr)\n}\n\nfunc (param *MySQLConnectParam) Connect() (*sql.DB, error) {\n\tdb, err := ConnectMySQL(param.ToDSN())\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\treturn db, nil\n}\n\n// IsDirExists checks if dir exists.\nfunc IsDirExists(name string) bool {\n\tf, err := os.Stat(name)\n\tif err != nil {\n\t\treturn false\n\t}\n\treturn f != nil && f.IsDir()\n}\n\n// IsEmptyDir checks if dir is empty.\nfunc IsEmptyDir(name string) bool {\n\tentries, err := os.ReadDir(name)\n\tif err != nil {\n\t\treturn false\n\t}\n\treturn len(entries) == 0\n}\n\n// SQLWithRetry constructs a retryable transaction.\ntype SQLWithRetry struct {\n\t// either *sql.DB or *sql.Conn\n\tDB           utils.DBExecutor\n\tLogger       log.Logger\n\tHideQueryLog bool\n}\n\nfunc (t SQLWithRetry) perform(_ context.Context, parentLogger log.Logger, purpose string, action func() error) error {\n\treturn Retry(purpose, parentLogger, action)\n}\n\n// Retry is shared by SQLWithRetry.perform, implementation of GlueCheckpointsDB and TiDB's glue implementation\nfunc Retry(purpose string, parentLogger log.Logger, action func() error) error {\n\tvar err error\noutside:\n\tfor i := 0; i < defaultMaxRetry; i++ {\n\t\tlogger := parentLogger.With(zap.Int(\"retryCnt\", i))\n\n\t\tif i > 0 {\n\t\t\tlogger.Warn(purpose + \" retry start\")\n\t\t\ttime.Sleep(retryTimeout)\n\t\t}\n\n\t\terr = action()\n\t\tswitch {\n\t\tcase err == nil:\n\t\t\treturn nil\n\t\t// do not retry NotFound error\n\t\tcase errors.IsNotFound(err):\n\t\t\tbreak outside\n\t\tcase IsRetryableError(err):\n\t\t\tlogger.Warn(purpose+\" failed but going to try again\", log.ShortError(err))\n\t\t\tcontinue\n\t\tdefault:\n\t\t\tlogger.Warn(purpose+\" failed with no retry\", log.ShortError(err))\n\t\t\tbreak outside\n\t\t}\n\t}\n\n\treturn errors.Annotatef(err, \"%s failed\", purpose)\n}\n\nfunc (t SQLWithRetry) QueryRow(ctx context.Context, purpose string, query string, dest ...interface{}) error {\n\tlogger := t.Logger\n\tif !t.HideQueryLog {\n\t\tlogger = logger.With(zap.String(\"query\", query))\n\t}\n\treturn t.perform(ctx, logger, purpose, func() error {\n\t\treturn t.DB.QueryRowContext(ctx, query).Scan(dest...)\n\t})\n}\n\n// Transact executes an action in a transaction, and retry if the\n// action failed with a retryable error.\nfunc (t SQLWithRetry) Transact(ctx context.Context, purpose string, action func(context.Context, *sql.Tx) error) error {\n\treturn t.perform(ctx, t.Logger, purpose, func() error {\n\t\ttxn, err := t.DB.BeginTx(ctx, nil)\n\t\tif err != nil {\n\t\t\treturn errors.Annotate(err, \"begin transaction failed\")\n\t\t}\n\n\t\terr = action(ctx, txn)\n\t\tif err != nil {\n\t\t\trerr := txn.Rollback()\n\t\t\tif rerr != nil {\n\t\t\t\tt.Logger.Error(purpose+\" rollback transaction failed\", log.ShortError(rerr))\n\t\t\t}\n\t\t\t// we should return the exec err, instead of the rollback rerr.\n\t\t\t// no need to errors.Trace() it, as the error comes from user code anyway.\n\t\t\treturn err\n\t\t}\n\n\t\terr = txn.Commit()\n\t\tif err != nil {\n\t\t\treturn errors.Annotate(err, \"commit transaction failed\")\n\t\t}\n\n\t\treturn nil\n\t})\n}\n\n// Exec executes a single SQL with optional retry.\nfunc (t SQLWithRetry) Exec(ctx context.Context, purpose string, query string, args ...interface{}) error {\n\tlogger := t.Logger\n\tif !t.HideQueryLog {\n\t\tlogger = logger.With(zap.String(\"query\", query), zap.Reflect(\"args\", args))\n\t}\n\treturn t.perform(ctx, logger, purpose, func() error {\n\t\t_, err := t.DB.ExecContext(ctx, query, args...)\n\t\treturn errors.Trace(err)\n\t})\n}\n\n// IsContextCanceledError returns whether the error is caused by context\n// cancellation. This function should only be used when the code logic is\n// affected by whether the error is canceling or not.\n//\n// This function returns `false` (not a context-canceled error) if `err == nil`.\nfunc IsContextCanceledError(err error) bool {\n\treturn log.IsContextCanceledError(err)\n}\n\n// UniqueTable returns an unique table name.\nfunc UniqueTable(schema string, table string) string {\n\tvar builder strings.Builder\n\tWriteMySQLIdentifier(&builder, schema)\n\tbuilder.WriteByte('.')\n\tWriteMySQLIdentifier(&builder, table)\n\treturn builder.String()\n}\n\n// EscapeIdentifier quote and escape an sql identifier\nfunc EscapeIdentifier(identifier string) string {\n\tvar builder strings.Builder\n\tWriteMySQLIdentifier(&builder, identifier)\n\treturn builder.String()\n}\n\n// Writes a MySQL identifier into the string builder.\n// The identifier is always escaped into the form \"`foo`\".\nfunc WriteMySQLIdentifier(builder *strings.Builder, identifier string) {\n\tbuilder.Grow(len(identifier) + 2)\n\tbuilder.WriteByte('`')\n\n\t// use a C-style loop instead of range loop to avoid UTF-8 decoding\n\tfor i := 0; i < len(identifier); i++ {\n\t\tb := identifier[i]\n\t\tif b == '`' {\n\t\t\tbuilder.WriteString(\"``\")\n\t\t} else {\n\t\t\tbuilder.WriteByte(b)\n\t\t}\n\t}\n\n\tbuilder.WriteByte('`')\n}\n\nfunc InterpolateMySQLString(s string) string {\n\tvar builder strings.Builder\n\tbuilder.Grow(len(s) + 2)\n\tbuilder.WriteByte('\\'')\n\tfor i := 0; i < len(s); i++ {\n\t\tb := s[i]\n\t\tif b == '\\'' {\n\t\t\tbuilder.WriteString(\"''\")\n\t\t} else {\n\t\t\tbuilder.WriteByte(b)\n\t\t}\n\t}\n\tbuilder.WriteByte('\\'')\n\treturn builder.String()\n}\n\n// TableExists return whether table with specified name exists in target db\nfunc TableExists(ctx context.Context, db utils.QueryExecutor, schema, table string) (bool, error) {\n\tquery := \"SELECT 1 from INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = ? AND TABLE_NAME = ?\"\n\tvar exist string\n\terr := db.QueryRowContext(ctx, query, schema, table).Scan(&exist)\n\tswitch {\n\tcase err == nil:\n\t\treturn true, nil\n\tcase err == sql.ErrNoRows:\n\t\treturn false, nil\n\tdefault:\n\t\treturn false, errors.Annotatef(err, \"check table exists failed\")\n\t}\n}\n\n// SchemaExists return whether schema with specified name exists.\nfunc SchemaExists(ctx context.Context, db utils.QueryExecutor, schema string) (bool, error) {\n\tquery := \"SELECT 1 from INFORMATION_SCHEMA.SCHEMATA WHERE SCHEMA_NAME = ?\"\n\tvar exist string\n\terr := db.QueryRowContext(ctx, query, schema).Scan(&exist)\n\tswitch {\n\tcase err == nil:\n\t\treturn true, nil\n\tcase err == sql.ErrNoRows:\n\t\treturn false, nil\n\tdefault:\n\t\treturn false, errors.Annotatef(err, \"check schema exists failed\")\n\t}\n}\n\n// GetJSON fetches a page and parses it as JSON. The parsed result will be\n// stored into the `v`. The variable `v` must be a pointer to a type that can be\n// unmarshalled from JSON.\n//\n// Example:\n//\n//\tclient := &http.Client{}\n//\tvar resp struct { IP string }\n//\tif err := util.GetJSON(client, \"http://api.ipify.org/?format=json\", &resp); err != nil {\n//\t\treturn errors.Trace(err)\n//\t}\n//\tfmt.Println(resp.IP)\nfunc GetJSON(ctx context.Context, client *http.Client, url string, v interface{}) error {\n\treq, err := http.NewRequestWithContext(ctx, \"GET\", url, nil)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tresp, err := client.Do(req)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tdefer resp.Body.Close()\n\n\tif resp.StatusCode != http.StatusOK {\n\t\tbody, err := io.ReadAll(resp.Body)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\treturn errors.Errorf(\"get %s http status code != 200, message %s\", url, string(body))\n\t}\n\n\treturn errors.Trace(json.NewDecoder(resp.Body).Decode(v))\n}\n\n// KillMySelf sends sigint to current process, used in integration test only\n//\n// Only works on Unix. Signaling on Windows is not supported.\nfunc KillMySelf() error {\n\tproc, err := os.FindProcess(os.Getpid())\n\tif err == nil {\n\t\terr = proc.Signal(syscall.SIGINT)\n\t}\n\treturn errors.Trace(err)\n}\n\n// KvPair is a pair of key and value.\ntype KvPair struct {\n\t// Key is the key of the KV pair\n\tKey []byte\n\t// Val is the value of the KV pair\n\tVal []byte\n\t// RowID is the row id of the KV pair.\n\tRowID int64\n}\n\n// TableHasAutoRowID return whether table has auto generated row id\nfunc TableHasAutoRowID(info *model.TableInfo) bool {\n\treturn !info.PKIsHandle && !info.IsCommonHandle\n}\n\n// TableHasAutoID return whether table has auto generated id.\nfunc TableHasAutoID(info *model.TableInfo) bool {\n\treturn TableHasAutoRowID(info) || info.GetAutoIncrementColInfo() != nil || info.ContainsAutoRandomBits()\n}\n\n// StringSliceEqual checks if two string slices are equal.\nfunc StringSliceEqual(a, b []string) bool {\n\tif len(a) != len(b) {\n\t\treturn false\n\t}\n\tfor i, v := range a {\n\t\tif v != b[i] {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n", "// Copyright 2019 PingCAP, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage common_test\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"database/sql/driver\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"math/rand\"\n\t\"net/http\"\n\t\"net/http/httptest\"\n\t\"strconv\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/DATA-DOG/go-sqlmock\"\n\t\"github.com/go-sql-driver/mysql\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/failpoint\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/common\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/log\"\n\ttmysql \"github.com/pingcap/tidb/errno\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestDirNotExist(t *testing.T) {\n\trequire.True(t, common.IsDirExists(\".\"))\n\trequire.False(t, common.IsDirExists(\"not-exists\"))\n}\n\nfunc TestGetJSON(t *testing.T) {\n\ttype TestPayload struct {\n\t\tUsername string `json:\"username\"`\n\t\tPassword string `json:\"password\"`\n\t}\n\trequest := TestPayload{\n\t\tUsername: \"lightning\",\n\t\tPassword: \"lightning-ctl\",\n\t}\n\n\tctx := context.Background()\n\t// Mock success response\n\thandle := func(res http.ResponseWriter, _ *http.Request) {\n\t\tres.WriteHeader(http.StatusOK)\n\t\terr := json.NewEncoder(res).Encode(request)\n\t\trequire.NoError(t, err)\n\t}\n\ttestServer := httptest.NewServer(http.HandlerFunc(func(res http.ResponseWriter, req *http.Request) {\n\t\thandle(res, req)\n\t}))\n\tdefer testServer.Close()\n\n\tclient := &http.Client{Timeout: time.Second}\n\n\tresponse := TestPayload{}\n\terr := common.GetJSON(ctx, client, \"http://not-exists\", &response)\n\trequire.Error(t, err)\n\terr = common.GetJSON(ctx, client, testServer.URL, &response)\n\trequire.NoError(t, err)\n\trequire.Equal(t, request, response)\n\n\t// Mock `StatusNoContent` response\n\thandle = func(res http.ResponseWriter, _ *http.Request) {\n\t\tres.WriteHeader(http.StatusNoContent)\n\t}\n\terr = common.GetJSON(ctx, client, testServer.URL, &response)\n\trequire.Error(t, err)\n\trequire.Regexp(t, \".*http status code != 200.*\", err.Error())\n}\n\nfunc TestToDSN(t *testing.T) {\n\tparam := common.MySQLConnectParam{\n\t\tHost:             \"127.0.0.1\",\n\t\tPort:             4000,\n\t\tUser:             \"root\",\n\t\tPassword:         \"123456\",\n\t\tSQLMode:          \"strict\",\n\t\tMaxAllowedPacket: 1234,\n\t\tTLS:              \"cluster\",\n\t\tVars: map[string]string{\n\t\t\t\"tidb_distsql_scan_concurrency\": \"1\",\n\t\t},\n\t}\n\trequire.Equal(t, \"root:123456@tcp(127.0.0.1:4000)/?charset=utf8mb4&sql_mode='strict'&maxAllowedPacket=1234&tls=cluster&tidb_distsql_scan_concurrency='1'\", param.ToDSN())\n\n\tparam.Host = \"::1\"\n\trequire.Equal(t, \"root:123456@tcp([::1]:4000)/?charset=utf8mb4&sql_mode='strict'&maxAllowedPacket=1234&tls=cluster&tidb_distsql_scan_concurrency='1'\", param.ToDSN())\n}\n\ntype mockDriver struct {\n\tdriver.Driver\n\tplainPsw string\n}\n\nfunc (m *mockDriver) Open(dsn string) (driver.Conn, error) {\n\tcfg, err := mysql.ParseDSN(dsn)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\taccessDenied := cfg.Passwd != m.plainPsw\n\treturn &mockConn{accessDenied: accessDenied}, nil\n}\n\ntype mockConn struct {\n\tdriver.Conn\n\tdriver.Pinger\n\taccessDenied bool\n}\n\nfunc (c *mockConn) Ping(ctx context.Context) error {\n\tif c.accessDenied {\n\t\treturn &mysql.MySQLError{Number: tmysql.ErrAccessDenied, Message: \"access denied\"}\n\t}\n\treturn nil\n}\n\nfunc (c *mockConn) Close() error {\n\treturn nil\n}\n\nfunc TestConnect(t *testing.T) {\n\tplainPsw := \"dQAUoDiyb1ucWZk7\"\n\tdriverName := \"mysql-mock-\" + strconv.Itoa(rand.Int())\n\tsql.Register(driverName, &mockDriver{plainPsw: plainPsw})\n\n\trequire.NoError(t, failpoint.Enable(\n\t\t\"github.com/pingcap/tidb/br/pkg/lightning/common/MockMySQLDriver\",\n\t\tfmt.Sprintf(\"return(\\\"%s\\\")\", driverName)))\n\tdefer func() {\n\t\trequire.NoError(t, failpoint.Disable(\"github.com/pingcap/tidb/br/pkg/lightning/common/MockMySQLDriver\"))\n\t}()\n\n\tparam := common.MySQLConnectParam{\n\t\tHost:             \"127.0.0.1\",\n\t\tPort:             4000,\n\t\tUser:             \"root\",\n\t\tPassword:         plainPsw,\n\t\tSQLMode:          \"strict\",\n\t\tMaxAllowedPacket: 1234,\n\t}\n\tdb, err := param.Connect()\n\trequire.NoError(t, err)\n\trequire.NoError(t, db.Close())\n\tparam.Password = base64.StdEncoding.EncodeToString([]byte(plainPsw))\n\tdb, err = param.Connect()\n\trequire.NoError(t, err)\n\trequire.NoError(t, db.Close())\n}\n\nfunc TestIsContextCanceledError(t *testing.T) {\n\trequire.True(t, common.IsContextCanceledError(context.Canceled))\n\trequire.False(t, common.IsContextCanceledError(io.EOF))\n}\n\nfunc TestUniqueTable(t *testing.T) {\n\ttableName := common.UniqueTable(\"test\", \"t1\")\n\trequire.Equal(t, \"`test`.`t1`\", tableName)\n\n\ttableName = common.UniqueTable(\"test\", \"t`1\")\n\trequire.Equal(t, \"`test`.`t``1`\", tableName)\n}\n\nfunc TestSQLWithRetry(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer db.Close()\n\n\tsqlWithRetry := &common.SQLWithRetry{\n\t\tDB:     db,\n\t\tLogger: log.L(),\n\t}\n\taValue := new(int)\n\n\t// retry defaultMaxRetry times and still failed\n\tfor i := 0; i < 3; i++ {\n\t\tmock.ExpectQuery(\"select a from test.t1\").WillReturnError(errors.Annotate(mysql.ErrInvalidConn, \"mock error\"))\n\t}\n\terr = sqlWithRetry.QueryRow(context.Background(), \"\", \"select a from test.t1\", aValue)\n\trequire.Regexp(t, \".*mock error\", err.Error())\n\n\t// meet unretryable error and will return directly\n\tmock.ExpectQuery(\"select a from test.t1\").WillReturnError(context.Canceled)\n\terr = sqlWithRetry.QueryRow(context.Background(), \"\", \"select a from test.t1\", aValue)\n\trequire.Regexp(t, \".*context canceled\", err.Error())\n\n\t// query success\n\trows := sqlmock.NewRows([]string{\"a\"}).AddRow(\"1\")\n\tmock.ExpectQuery(\"select a from test.t1\").WillReturnRows(rows)\n\n\terr = sqlWithRetry.QueryRow(context.Background(), \"\", \"select a from test.t1\", aValue)\n\trequire.NoError(t, err)\n\trequire.Equal(t, 1, *aValue)\n\n\t// test Exec\n\tmock.ExpectExec(\"delete from\").WillReturnError(context.Canceled)\n\terr = sqlWithRetry.Exec(context.Background(), \"\", \"delete from test.t1 where id = ?\", 2)\n\trequire.Regexp(t, \".*context canceled\", err.Error())\n\n\tmock.ExpectExec(\"delete from\").WillReturnResult(sqlmock.NewResult(0, 1))\n\terr = sqlWithRetry.Exec(context.Background(), \"\", \"delete from test.t1 where id = ?\", 2)\n\trequire.NoError(t, err)\n\n\trequire.Nil(t, mock.ExpectationsWereMet())\n}\n\nfunc TestStringSliceEqual(t *testing.T) {\n\tassert.True(t, common.StringSliceEqual(nil, nil))\n\tassert.True(t, common.StringSliceEqual(nil, []string{}))\n\tassert.False(t, common.StringSliceEqual(nil, []string{\"a\"}))\n\tassert.False(t, common.StringSliceEqual([]string{\"a\"}, nil))\n\tassert.True(t, common.StringSliceEqual([]string{\"a\"}, []string{\"a\"}))\n\tassert.False(t, common.StringSliceEqual([]string{\"a\"}, []string{\"b\"}))\n\tassert.True(t, common.StringSliceEqual([]string{\"a\", \"b\", \"c\"}, []string{\"a\", \"b\", \"c\"}))\n\tassert.False(t, common.StringSliceEqual([]string{\"a\"}, []string{\"a\", \"b\", \"c\"}))\n\tassert.False(t, common.StringSliceEqual([]string{\"a\", \"b\", \"c\"}, []string{\"a\", \"b\"}))\n\tassert.False(t, common.StringSliceEqual([]string{\"a\", \"x\", \"y\"}, []string{\"a\", \"y\", \"x\"}))\n}\n\nfunc TestInterpolateMySQLString(t *testing.T) {\n\tassert.Equal(t, \"'123'\", common.InterpolateMySQLString(\"123\"))\n\tassert.Equal(t, \"'1''23'\", common.InterpolateMySQLString(\"1'23\"))\n\tassert.Equal(t, \"'1''2''''3'\", common.InterpolateMySQLString(\"1'2''3\"))\n}\n", "// Copyright 2019 PingCAP, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage config\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"math\"\n\t\"net\"\n\t\"net/url\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\t\"unicode/utf8\"\n\n\t\"github.com/BurntSushi/toml\"\n\t\"github.com/docker/go-units\"\n\tgomysql \"github.com/go-sql-driver/mysql\"\n\t\"github.com/google/uuid\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/common\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/log\"\n\ttidbcfg \"github.com/pingcap/tidb/config\"\n\t\"github.com/pingcap/tidb/parser/mysql\"\n\t\"github.com/pingcap/tidb/util\"\n\tfilter \"github.com/pingcap/tidb/util/table-filter\"\n\trouter \"github.com/pingcap/tidb/util/table-router\"\n\t\"go.uber.org/atomic\"\n\t\"go.uber.org/zap\"\n)\n\nconst (\n\t// ImportMode defines mode of import for tikv.\n\tImportMode = \"import\"\n\t// NormalMode defines mode of normal for tikv.\n\tNormalMode = \"normal\"\n\n\t// BackendTiDB is a constant for choosing the \"TiDB\" backend in the configuration.\n\tBackendTiDB = \"tidb\"\n\t// BackendLocal is a constant for choosing the \"Local\" backup in the configuration.\n\t// In this mode, we write & sort kv pairs with local storage and directly write them to tikv.\n\tBackendLocal = \"local\"\n\n\t// CheckpointDriverMySQL is a constant for choosing the \"MySQL\" checkpoint driver in the configuration.\n\tCheckpointDriverMySQL = \"mysql\"\n\t// CheckpointDriverFile is a constant for choosing the \"File\" checkpoint driver in the configuration.\n\tCheckpointDriverFile = \"file\"\n\n\t// ReplaceOnDup indicates using REPLACE INTO to insert data\n\tReplaceOnDup = \"replace\"\n\t// IgnoreOnDup indicates using INSERT IGNORE INTO to insert data\n\tIgnoreOnDup = \"ignore\"\n\t// ErrorOnDup indicates using INSERT INTO to insert data, which would violate PK or UNIQUE constraint\n\tErrorOnDup = \"error\"\n\n\tdefaultDistSQLScanConcurrency     = 15\n\tdefaultBuildStatsConcurrency      = 20\n\tdefaultIndexSerialScanConcurrency = 20\n\tdefaultChecksumTableConcurrency   = 2\n\tdefaultTableConcurrency           = 6\n\tdefaultIndexConcurrency           = 2\n\n\t// defaultMetaSchemaName is the default database name used to store lightning metadata\n\tdefaultMetaSchemaName     = \"lightning_metadata\"\n\tdefaultTaskInfoSchemaName = \"lightning_task_info\"\n\n\t// autoDiskQuotaLocalReservedSpeed is the estimated size increase per\n\t// millisecond per write thread the local backend may gain on all engines.\n\t// This is used to compute the maximum size overshoot between two disk quota\n\t// checks, if the first one has barely passed.\n\t//\n\t// With cron.check-disk-quota = 1m, region-concurrency = 40, this should\n\t// contribute 2.3 GiB to the reserved size.\n\t// autoDiskQuotaLocalReservedSpeed uint64 = 1 * units.KiB\n\tdefaultEngineMemCacheSize      = 512 * units.MiB\n\tdefaultLocalWriterMemCacheSize = 128 * units.MiB\n\n\tdefaultCSVDataCharacterSet       = \"binary\"\n\tdefaultCSVDataInvalidCharReplace = utf8.RuneError\n)\n\nvar (\n\tsupportedStorageTypes = []string{\"file\", \"local\", \"s3\", \"noop\", \"gcs\", \"gs\"}\n\n\tdefaultFilter = []string{\n\t\t\"*.*\",\n\t\t\"!mysql.*\",\n\t\t\"!sys.*\",\n\t\t\"!INFORMATION_SCHEMA.*\",\n\t\t\"!PERFORMANCE_SCHEMA.*\",\n\t\t\"!METRICS_SCHEMA.*\",\n\t\t\"!INSPECTION_SCHEMA.*\",\n\t}\n)\n\n// GetDefaultFilter gets the default table filter used in Lightning.\n// It clones the original default filter,\n// so that the original value won't be changed when the returned slice's element is changed.\nfunc GetDefaultFilter() []string {\n\treturn append([]string{}, defaultFilter...)\n}\n\ntype DBStore struct {\n\tHost       string    `toml:\"host\" json:\"host\"`\n\tPort       int       `toml:\"port\" json:\"port\"`\n\tUser       string    `toml:\"user\" json:\"user\"`\n\tPsw        string    `toml:\"password\" json:\"-\"`\n\tStatusPort int       `toml:\"status-port\" json:\"status-port\"`\n\tPdAddr     string    `toml:\"pd-addr\" json:\"pd-addr\"`\n\tStrSQLMode string    `toml:\"sql-mode\" json:\"sql-mode\"`\n\tTLS        string    `toml:\"tls\" json:\"tls\"`\n\tSecurity   *Security `toml:\"security\" json:\"security\"`\n\n\tSQLMode          mysql.SQLMode `toml:\"-\" json:\"-\"`\n\tMaxAllowedPacket uint64        `toml:\"max-allowed-packet\" json:\"max-allowed-packet\"`\n\n\tDistSQLScanConcurrency     int               `toml:\"distsql-scan-concurrency\" json:\"distsql-scan-concurrency\"`\n\tBuildStatsConcurrency      int               `toml:\"build-stats-concurrency\" json:\"build-stats-concurrency\"`\n\tIndexSerialScanConcurrency int               `toml:\"index-serial-scan-concurrency\" json:\"index-serial-scan-concurrency\"`\n\tChecksumTableConcurrency   int               `toml:\"checksum-table-concurrency\" json:\"checksum-table-concurrency\"`\n\tVars                       map[string]string `toml:\"-\" json:\"vars\"`\n}\n\ntype Config struct {\n\tTaskID int64 `toml:\"-\" json:\"id\"`\n\n\tApp  Lightning `toml:\"lightning\" json:\"lightning\"`\n\tTiDB DBStore   `toml:\"tidb\" json:\"tidb\"`\n\n\tCheckpoint   Checkpoint          `toml:\"checkpoint\" json:\"checkpoint\"`\n\tMydumper     MydumperRuntime     `toml:\"mydumper\" json:\"mydumper\"`\n\tTikvImporter TikvImporter        `toml:\"tikv-importer\" json:\"tikv-importer\"`\n\tPostRestore  PostRestore         `toml:\"post-restore\" json:\"post-restore\"`\n\tCron         Cron                `toml:\"cron\" json:\"cron\"`\n\tRoutes       []*router.TableRule `toml:\"routes\" json:\"routes\"`\n\tSecurity     Security            `toml:\"security\" json:\"security\"`\n\n\tBWList filter.MySQLReplicationRules `toml:\"black-white-list\" json:\"black-white-list\"`\n}\n\nfunc (cfg *Config) String() string {\n\tbytes, err := json.Marshal(cfg)\n\tif err != nil {\n\t\tlog.L().Error(\"marshal config to json error\", log.ShortError(err))\n\t}\n\treturn string(bytes)\n}\n\nfunc (cfg *Config) ToTLS() (*common.TLS, error) {\n\thostPort := net.JoinHostPort(cfg.TiDB.Host, strconv.Itoa(cfg.TiDB.StatusPort))\n\treturn common.NewTLS(\n\t\tcfg.Security.CAPath,\n\t\tcfg.Security.CertPath,\n\t\tcfg.Security.KeyPath,\n\t\thostPort,\n\t\tcfg.Security.CABytes,\n\t\tcfg.Security.CertBytes,\n\t\tcfg.Security.KeyBytes,\n\t)\n}\n\ntype Lightning struct {\n\tTableConcurrency  int    `toml:\"table-concurrency\" json:\"table-concurrency\"`\n\tIndexConcurrency  int    `toml:\"index-concurrency\" json:\"index-concurrency\"`\n\tRegionConcurrency int    `toml:\"region-concurrency\" json:\"region-concurrency\"`\n\tIOConcurrency     int    `toml:\"io-concurrency\" json:\"io-concurrency\"`\n\tCheckRequirements bool   `toml:\"check-requirements\" json:\"check-requirements\"`\n\tMetaSchemaName    string `toml:\"meta-schema-name\" json:\"meta-schema-name\"`\n\n\tMaxError           MaxError `toml:\"max-error\" json:\"max-error\"`\n\tTaskInfoSchemaName string   `toml:\"task-info-schema-name\" json:\"task-info-schema-name\"`\n}\n\ntype PostOpLevel int\n\nconst (\n\tOpLevelOff PostOpLevel = iota\n\tOpLevelOptional\n\tOpLevelRequired\n)\n\nfunc (t *PostOpLevel) UnmarshalTOML(v interface{}) error {\n\tswitch val := v.(type) {\n\tcase bool:\n\t\tif val {\n\t\t\t*t = OpLevelRequired\n\t\t} else {\n\t\t\t*t = OpLevelOff\n\t\t}\n\tcase string:\n\t\treturn t.FromStringValue(val)\n\tdefault:\n\t\treturn errors.Errorf(\"invalid op level '%v', please choose valid option between ['off', 'optional', 'required']\", v)\n\t}\n\treturn nil\n}\n\nfunc (t PostOpLevel) MarshalText() ([]byte, error) {\n\treturn []byte(t.String()), nil\n}\n\n// parser command line parameter\nfunc (t *PostOpLevel) FromStringValue(s string) error {\n\tswitch strings.ToLower(s) {\n\t//nolint:goconst // This 'false' and other 'false's aren't the same.\n\tcase \"off\", \"false\":\n\t\t*t = OpLevelOff\n\tcase \"required\", \"true\":\n\t\t*t = OpLevelRequired\n\tcase \"optional\":\n\t\t*t = OpLevelOptional\n\tdefault:\n\t\treturn errors.Errorf(\"invalid op level '%s', please choose valid option between ['off', 'optional', 'required']\", s)\n\t}\n\treturn nil\n}\n\nfunc (t *PostOpLevel) MarshalJSON() ([]byte, error) {\n\treturn []byte(`\"` + t.String() + `\"`), nil\n}\n\nfunc (t *PostOpLevel) UnmarshalJSON(data []byte) error {\n\treturn t.FromStringValue(strings.Trim(string(data), `\"`))\n}\n\nfunc (t PostOpLevel) String() string {\n\tswitch t {\n\tcase OpLevelOff:\n\t\treturn \"off\"\n\tcase OpLevelOptional:\n\t\treturn \"optional\"\n\tcase OpLevelRequired:\n\t\treturn \"required\"\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"invalid post process type '%d'\", t))\n\t}\n}\n\ntype CheckpointKeepStrategy int\n\nconst (\n\t// remove checkpoint data\n\tCheckpointRemove CheckpointKeepStrategy = iota\n\t// keep by rename checkpoint file/db according to task id\n\tCheckpointRename\n\t// keep checkpoint data unchanged\n\tCheckpointOrigin\n)\n\nfunc (t *CheckpointKeepStrategy) UnmarshalTOML(v interface{}) error {\n\tswitch val := v.(type) {\n\tcase bool:\n\t\tif val {\n\t\t\t*t = CheckpointRename\n\t\t} else {\n\t\t\t*t = CheckpointRemove\n\t\t}\n\tcase string:\n\t\treturn t.FromStringValue(val)\n\tdefault:\n\t\treturn errors.Errorf(\"invalid checkpoint keep strategy '%v', please choose valid option between ['remove', 'rename', 'origin']\", v)\n\t}\n\treturn nil\n}\n\nfunc (t CheckpointKeepStrategy) MarshalText() ([]byte, error) {\n\treturn []byte(t.String()), nil\n}\n\n// parser command line parameter\nfunc (t *CheckpointKeepStrategy) FromStringValue(s string) error {\n\tswitch strings.ToLower(s) {\n\t//nolint:goconst // This 'false' and other 'false's aren't the same.\n\tcase \"remove\", \"false\":\n\t\t*t = CheckpointRemove\n\tcase \"rename\", \"true\":\n\t\t*t = CheckpointRename\n\tcase \"origin\":\n\t\t*t = CheckpointOrigin\n\tdefault:\n\t\treturn errors.Errorf(\"invalid checkpoint keep strategy '%s', please choose valid option between ['remove', 'rename', 'origin']\", s)\n\t}\n\treturn nil\n}\n\nfunc (t *CheckpointKeepStrategy) MarshalJSON() ([]byte, error) {\n\treturn []byte(`\"` + t.String() + `\"`), nil\n}\n\nfunc (t *CheckpointKeepStrategy) UnmarshalJSON(data []byte) error {\n\treturn t.FromStringValue(strings.Trim(string(data), `\"`))\n}\n\nfunc (t CheckpointKeepStrategy) String() string {\n\tswitch t {\n\tcase CheckpointRemove:\n\t\treturn \"remove\"\n\tcase CheckpointRename:\n\t\treturn \"rename\"\n\tcase CheckpointOrigin:\n\t\treturn \"origin\"\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"invalid post process type '%d'\", t))\n\t}\n}\n\n// MaxError configures the maximum number of acceptable errors per kind.\ntype MaxError struct {\n\t// Syntax is the maximum number of syntax errors accepted.\n\t// When tolerated, the file chunk causing syntax error will be skipped, and adds 1 to the counter.\n\t// TODO Currently this is hard-coded to zero.\n\tSyntax atomic.Int64 `toml:\"syntax\" json:\"-\"`\n\n\t// Charset is the maximum number of character-set conversion errors accepted.\n\t// When tolerated, and `data-invalid-char-replace` is not changed from \"\\ufffd\",\n\t// every invalid byte in the source file will be converted to U+FFFD and adds 1 to the counter.\n\t// Note that a failed conversion a column's character set (e.g. UTF8-to-GBK conversion)\n\t// is counted as a type error, not a charset error.\n\t// TODO character-set conversion is not yet implemented.\n\tCharset atomic.Int64 `toml:\"charset\" json:\"-\"`\n\n\t// Type is the maximum number of type errors accepted.\n\t// This includes strict-mode errors such as zero in dates, integer overflow, character string too long, etc.\n\t// In TiDB backend, this also includes all possible SQL errors raised from INSERT,\n\t// such as unique key conflict when `on-duplicate` is set to `error`.\n\t// When tolerated, the row causing the error will be skipped, and adds 1 to the counter.\n\tType atomic.Int64 `toml:\"type\" json:\"type\"`\n\n\t// Conflict is the maximum number of unique key conflicts in local backend accepted.\n\t// When tolerated, every pair of conflict adds 1 to the counter.\n\t// Those pairs will NOT be deleted from the target. Conflict resolution is performed separately.\n\t// TODO Currently this is hard-coded to infinity.\n\tConflict atomic.Int64 `toml:\"conflict\" json:\"-\"`\n}\n\nfunc (cfg *MaxError) UnmarshalTOML(v interface{}) error {\n\tswitch val := v.(type) {\n\tcase int64:\n\t\t// ignore val that is smaller than 0\n\t\tif val < 0 {\n\t\t\tval = 0\n\t\t}\n\t\tcfg.Syntax.Store(0)\n\t\tcfg.Charset.Store(math.MaxInt64)\n\t\tcfg.Type.Store(val)\n\t\tcfg.Conflict.Store(math.MaxInt64)\n\t\treturn nil\n\tcase map[string]interface{}:\n\t\t// TODO support stuff like `max-error = { charset = 1000, type = 1000 }` if proved useful.\n\tdefault:\n\t}\n\treturn errors.Errorf(\"invalid max-error '%v', should be an integer\", v)\n}\n\n// DuplicateResolutionAlgorithm is the config type of how to resolve duplicates.\ntype DuplicateResolutionAlgorithm int\n\nconst (\n\t// DupeResAlgNone doesn't detect duplicate.\n\tDupeResAlgNone DuplicateResolutionAlgorithm = iota\n\n\t// DupeResAlgRecord only records duplicate records to `lightning_task_info.conflict_error_v1` table on the target TiDB.\n\tDupeResAlgRecord\n\n\t// DupeResAlgRemove records all duplicate records like the 'record' algorithm and remove all information related to the\n\t// duplicated rows. Users need to analyze the lightning_task_info.conflict_error_v1 table to add back the correct rows.\n\tDupeResAlgRemove\n)\n\nfunc (dra *DuplicateResolutionAlgorithm) UnmarshalTOML(v interface{}) error {\n\tif val, ok := v.(string); ok {\n\t\treturn dra.FromStringValue(val)\n\t}\n\treturn errors.Errorf(\"invalid duplicate-resolution '%v', please choose valid option between ['record', 'none', 'remove']\", v)\n}\n\nfunc (dra DuplicateResolutionAlgorithm) MarshalText() ([]byte, error) {\n\treturn []byte(dra.String()), nil\n}\n\nfunc (dra *DuplicateResolutionAlgorithm) FromStringValue(s string) error {\n\tswitch strings.ToLower(s) {\n\tcase \"record\":\n\t\t*dra = DupeResAlgRecord\n\tcase \"none\":\n\t\t*dra = DupeResAlgNone\n\tcase \"remove\":\n\t\t*dra = DupeResAlgRemove\n\tdefault:\n\t\treturn errors.Errorf(\"invalid duplicate-resolution '%s', please choose valid option between ['record', 'none', 'remove']\", s)\n\t}\n\treturn nil\n}\n\nfunc (dra *DuplicateResolutionAlgorithm) MarshalJSON() ([]byte, error) {\n\treturn []byte(`\"` + dra.String() + `\"`), nil\n}\n\nfunc (dra *DuplicateResolutionAlgorithm) UnmarshalJSON(data []byte) error {\n\treturn dra.FromStringValue(strings.Trim(string(data), `\"`))\n}\n\nfunc (dra DuplicateResolutionAlgorithm) String() string {\n\tswitch dra {\n\tcase DupeResAlgRecord:\n\t\treturn \"record\"\n\tcase DupeResAlgNone:\n\t\treturn \"none\"\n\tcase DupeResAlgRemove:\n\t\treturn \"remove\"\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"invalid duplicate-resolution type '%d'\", dra))\n\t}\n}\n\n// PostRestore has some options which will be executed after kv restored.\ntype PostRestore struct {\n\tChecksum          PostOpLevel `toml:\"checksum\" json:\"checksum\"`\n\tAnalyze           PostOpLevel `toml:\"analyze\" json:\"analyze\"`\n\tLevel1Compact     bool        `toml:\"level-1-compact\" json:\"level-1-compact\"`\n\tPostProcessAtLast bool        `toml:\"post-process-at-last\" json:\"post-process-at-last\"`\n\tCompact           bool        `toml:\"compact\" json:\"compact\"`\n}\n\ntype CSVConfig struct {\n\t// Separator, Delimiter and Terminator should all be in utf8mb4 encoding.\n\tSeparator       string `toml:\"separator\" json:\"separator\"`\n\tDelimiter       string `toml:\"delimiter\" json:\"delimiter\"`\n\tTerminator      string `toml:\"terminator\" json:\"terminator\"`\n\tNull            string `toml:\"null\" json:\"null\"`\n\tHeader          bool   `toml:\"header\" json:\"header\"`\n\tTrimLastSep     bool   `toml:\"trim-last-separator\" json:\"trim-last-separator\"`\n\tNotNull         bool   `toml:\"not-null\" json:\"not-null\"`\n\tBackslashEscape bool   `toml:\"backslash-escape\" json:\"backslash-escape\"`\n}\n\ntype MydumperRuntime struct {\n\tReadBlockSize    ByteSize         `toml:\"read-block-size\" json:\"read-block-size\"`\n\tBatchSize        ByteSize         `toml:\"batch-size\" json:\"batch-size\"`\n\tBatchImportRatio float64          `toml:\"batch-import-ratio\" json:\"batch-import-ratio\"`\n\tSourceDir        string           `toml:\"data-source-dir\" json:\"data-source-dir\"`\n\tCharacterSet     string           `toml:\"character-set\" json:\"character-set\"`\n\tCSV              CSVConfig        `toml:\"csv\" json:\"csv\"`\n\tMaxRegionSize    ByteSize         `toml:\"max-region-size\" json:\"max-region-size\"`\n\tFilter           []string         `toml:\"filter\" json:\"filter\"`\n\tFileRouters      []*FileRouteRule `toml:\"files\" json:\"files\"`\n\t// Deprecated: only used to keep the compatibility.\n\tNoSchema         bool             `toml:\"no-schema\" json:\"no-schema\"`\n\tCaseSensitive    bool             `toml:\"case-sensitive\" json:\"case-sensitive\"`\n\tStrictFormat     bool             `toml:\"strict-format\" json:\"strict-format\"`\n\tDefaultFileRules bool             `toml:\"default-file-rules\" json:\"default-file-rules\"`\n\tIgnoreColumns    AllIgnoreColumns `toml:\"ignore-data-columns\" json:\"ignore-data-columns\"`\n\t// DataCharacterSet is the character set of the source file. Only CSV files are supported now. The following options are supported.\n\t//   - utf8mb4\n\t//   - GB18030\n\t//   - GBK: an extension of the GB2312 character set and is also known as Code Page 936.\n\t//   - binary: no attempt to convert the encoding.\n\t// Leave DataCharacterSet empty will make it use `binary` by default.\n\tDataCharacterSet string `toml:\"data-character-set\" json:\"data-character-set\"`\n\t// DataInvalidCharReplace is the replacement characters for non-compatible characters, which shouldn't duplicate with the separators or line breaks.\n\t// Changing the default value will result in increased parsing time. Non-compatible characters do not cause an increase in error.\n\tDataInvalidCharReplace string `toml:\"data-invalid-char-replace\" json:\"data-invalid-char-replace\"`\n}\n\ntype AllIgnoreColumns []*IgnoreColumns\n\ntype IgnoreColumns struct {\n\tDB          string   `toml:\"db\" json:\"db\"`\n\tTable       string   `toml:\"table\" json:\"table\"`\n\tTableFilter []string `toml:\"table-filter\" json:\"table-filter\"`\n\tColumns     []string `toml:\"columns\" json:\"columns\"`\n}\n\nfunc (ic *IgnoreColumns) ColumnsMap() map[string]struct{} {\n\tcolumnMap := make(map[string]struct{}, len(ic.Columns))\n\tfor _, c := range ic.Columns {\n\t\tcolumnMap[c] = struct{}{}\n\t}\n\treturn columnMap\n}\n\n// GetIgnoreColumns gets Ignore config by schema name/regex and table name/regex.\nfunc (igCols AllIgnoreColumns) GetIgnoreColumns(db string, table string, caseSensitive bool) (*IgnoreColumns, error) {\n\tif !caseSensitive {\n\t\tdb = strings.ToLower(db)\n\t\ttable = strings.ToLower(table)\n\t}\n\tfor i, ig := range igCols {\n\t\tif ig.DB == db && ig.Table == table {\n\t\t\treturn igCols[i], nil\n\t\t}\n\t\tf, err := filter.Parse(ig.TableFilter)\n\t\tif err != nil {\n\t\t\treturn nil, common.ErrInvalidConfig.GenWithStack(\"invalid table filter %s in ignore columns\", strings.Join(ig.TableFilter, \",\"))\n\t\t}\n\t\tif f.MatchTable(db, table) {\n\t\t\treturn igCols[i], nil\n\t\t}\n\t}\n\treturn &IgnoreColumns{Columns: make([]string, 0)}, nil\n}\n\ntype FileRouteRule struct {\n\tPattern     string `json:\"pattern\" toml:\"pattern\" yaml:\"pattern\"`\n\tPath        string `json:\"path\" toml:\"path\" yaml:\"path\"`\n\tSchema      string `json:\"schema\" toml:\"schema\" yaml:\"schema\"`\n\tTable       string `json:\"table\" toml:\"table\" yaml:\"table\"`\n\tType        string `json:\"type\" toml:\"type\" yaml:\"type\"`\n\tKey         string `json:\"key\" toml:\"key\" yaml:\"key\"`\n\tCompression string `json:\"compression\" toml:\"compression\" yaml:\"compression\"`\n\t// unescape the schema/table name only used in lightning's internal logic now.\n\tUnescape bool `json:\"-\" toml:\"-\" yaml:\"-\"`\n\t// TODO: DataCharacterSet here can override the same field in [mydumper.csv] with a higher level.\n\t// This could provide users a more flexible usage to configure different files with\n\t// different data charsets.\n\t// DataCharacterSet string `toml:\"data-character-set\" json:\"data-character-set\"`\n}\n\ntype TikvImporter struct {\n\t// Deprecated: only used to keep the compatibility.\n\tAddr                string                       `toml:\"addr\" json:\"addr\"`\n\tBackend             string                       `toml:\"backend\" json:\"backend\"`\n\tOnDuplicate         string                       `toml:\"on-duplicate\" json:\"on-duplicate\"`\n\tMaxKVPairs          int                          `toml:\"max-kv-pairs\" json:\"max-kv-pairs\"`\n\tSendKVPairs         int                          `toml:\"send-kv-pairs\" json:\"send-kv-pairs\"`\n\tRegionSplitSize     ByteSize                     `toml:\"region-split-size\" json:\"region-split-size\"`\n\tRegionSplitKeys     int                          `toml:\"region-split-keys\" json:\"region-split-keys\"`\n\tSortedKVDir         string                       `toml:\"sorted-kv-dir\" json:\"sorted-kv-dir\"`\n\tDiskQuota           ByteSize                     `toml:\"disk-quota\" json:\"disk-quota\"`\n\tRangeConcurrency    int                          `toml:\"range-concurrency\" json:\"range-concurrency\"`\n\tDuplicateResolution DuplicateResolutionAlgorithm `toml:\"duplicate-resolution\" json:\"duplicate-resolution\"`\n\tIncrementalImport   bool                         `toml:\"incremental-import\" json:\"incremental-import\"`\n\n\tEngineMemCacheSize      ByteSize `toml:\"engine-mem-cache-size\" json:\"engine-mem-cache-size\"`\n\tLocalWriterMemCacheSize ByteSize `toml:\"local-writer-mem-cache-size\" json:\"local-writer-mem-cache-size\"`\n\tStoreWriteBWLimit       ByteSize `toml:\"store-write-bwlimit\" json:\"store-write-bwlimit\"`\n}\n\ntype Checkpoint struct {\n\tSchema           string                 `toml:\"schema\" json:\"schema\"`\n\tDSN              string                 `toml:\"dsn\" json:\"-\"` // DSN may contain password, don't expose this to JSON.\n\tDriver           string                 `toml:\"driver\" json:\"driver\"`\n\tEnable           bool                   `toml:\"enable\" json:\"enable\"`\n\tKeepAfterSuccess CheckpointKeepStrategy `toml:\"keep-after-success\" json:\"keep-after-success\"`\n}\n\ntype Cron struct {\n\tSwitchMode     Duration `toml:\"switch-mode\" json:\"switch-mode\"`\n\tLogProgress    Duration `toml:\"log-progress\" json:\"log-progress\"`\n\tCheckDiskQuota Duration `toml:\"check-disk-quota\" json:\"check-disk-quota\"`\n}\n\ntype Security struct {\n\tCAPath   string `toml:\"ca-path\" json:\"ca-path\"`\n\tCertPath string `toml:\"cert-path\" json:\"cert-path\"`\n\tKeyPath  string `toml:\"key-path\" json:\"key-path\"`\n\t// RedactInfoLog indicates that whether enabling redact log\n\tRedactInfoLog bool `toml:\"redact-info-log\" json:\"redact-info-log\"`\n\n\t// TLSConfigName is used to set tls config for lightning in DM, so we don't expose this field to user\n\t// DM may running many lightning instances at same time, so we need to set different tls config name for each lightning\n\tTLSConfigName string `toml:\"-\" json:\"-\"`\n\n\t// When DM/engine uses lightning as a library, it can directly pass in the content\n\tCABytes   []byte `toml:\"-\" json:\"-\"`\n\tCertBytes []byte `toml:\"-\" json:\"-\"`\n\tKeyBytes  []byte `toml:\"-\" json:\"-\"`\n}\n\n// RegisterMySQL registers the TLS config with name \"cluster\" or security.TLSConfigName\n// for use in `sql.Open()`. This method is goroutine-safe.\nfunc (sec *Security) RegisterMySQL() error {\n\tif sec == nil {\n\t\treturn nil\n\t}\n\n\ttlsConfig, err := util.NewTLSConfig(\n\t\tutil.WithCAPath(sec.CAPath),\n\t\tutil.WithCertAndKeyPath(sec.CertPath, sec.KeyPath),\n\t\tutil.WithCAContent(sec.CABytes),\n\t\tutil.WithCertAndKeyContent(sec.CertBytes, sec.KeyBytes),\n\t)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tif tlsConfig != nil {\n\t\t// error happens only when the key coincides with the built-in names.\n\t\t_ = gomysql.RegisterTLSConfig(sec.TLSConfigName, tlsConfig)\n\t}\n\treturn nil\n}\n\n// DeregisterMySQL deregisters the TLS config with security.TLSConfigName\nfunc (sec *Security) DeregisterMySQL() {\n\tif sec == nil || len(sec.CAPath) == 0 {\n\t\treturn\n\t}\n\tgomysql.DeregisterTLSConfig(sec.TLSConfigName)\n}\n\n// A duration which can be deserialized from a TOML string.\n// Implemented as https://github.com/BurntSushi/toml#using-the-encodingtextunmarshaler-interface\ntype Duration struct {\n\ttime.Duration\n}\n\nfunc (d *Duration) UnmarshalText(text []byte) error {\n\tvar err error\n\td.Duration, err = time.ParseDuration(string(text))\n\treturn errors.Trace(err)\n}\n\nfunc (d Duration) MarshalText() ([]byte, error) {\n\treturn []byte(d.String()), nil\n}\n\nfunc (d *Duration) MarshalJSON() ([]byte, error) {\n\treturn []byte(fmt.Sprintf(`\"%s\"`, d.Duration)), nil\n}\n\n// Charset defines character set\ntype Charset int\n\nconst (\n\tBinary Charset = iota\n\tUTF8MB4\n\tGB18030\n\tGBK\n)\n\n// String return the string value of charset\nfunc (c Charset) String() string {\n\tswitch c {\n\tcase Binary:\n\t\treturn \"binary\"\n\tcase UTF8MB4:\n\t\treturn \"utf8mb4\"\n\tcase GB18030:\n\t\treturn \"gb18030\"\n\tcase GBK:\n\t\treturn \"gbk\"\n\tdefault:\n\t\treturn \"unknown_charset\"\n\t}\n}\n\n// ParseCharset parser character set for string\nfunc ParseCharset(dataCharacterSet string) (Charset, error) {\n\tswitch strings.ToLower(dataCharacterSet) {\n\tcase \"\", \"binary\":\n\t\treturn Binary, nil\n\tcase \"utf8mb4\":\n\t\treturn UTF8MB4, nil\n\tcase \"gb18030\":\n\t\treturn GB18030, nil\n\tcase \"gbk\":\n\t\treturn GBK, nil\n\tdefault:\n\t\treturn Binary, errors.Errorf(\"found unsupported data-character-set: %s\", dataCharacterSet)\n\t}\n}\n\nfunc NewConfig() *Config {\n\treturn &Config{\n\t\tApp: Lightning{\n\t\t\tRegionConcurrency: runtime.NumCPU(),\n\t\t\tTableConcurrency:  0,\n\t\t\tIndexConcurrency:  0,\n\t\t\tIOConcurrency:     5,\n\t\t\tCheckRequirements: true,\n\t\t\tMaxError: MaxError{\n\t\t\t\tCharset:  *atomic.NewInt64(math.MaxInt64),\n\t\t\t\tConflict: *atomic.NewInt64(math.MaxInt64),\n\t\t\t},\n\t\t\tTaskInfoSchemaName: defaultTaskInfoSchemaName,\n\t\t},\n\t\tCheckpoint: Checkpoint{\n\t\t\tEnable: true,\n\t\t},\n\t\tTiDB: DBStore{\n\t\t\tHost:                       \"127.0.0.1\",\n\t\t\tUser:                       \"root\",\n\t\t\tStatusPort:                 10080,\n\t\t\tStrSQLMode:                 \"ONLY_FULL_GROUP_BY,NO_AUTO_CREATE_USER\",\n\t\t\tMaxAllowedPacket:           defaultMaxAllowedPacket,\n\t\t\tBuildStatsConcurrency:      defaultBuildStatsConcurrency,\n\t\t\tDistSQLScanConcurrency:     defaultDistSQLScanConcurrency,\n\t\t\tIndexSerialScanConcurrency: defaultIndexSerialScanConcurrency,\n\t\t\tChecksumTableConcurrency:   defaultChecksumTableConcurrency,\n\t\t},\n\t\tCron: Cron{\n\t\t\tSwitchMode:     Duration{Duration: 5 * time.Minute},\n\t\t\tLogProgress:    Duration{Duration: 5 * time.Minute},\n\t\t\tCheckDiskQuota: Duration{Duration: 1 * time.Minute},\n\t\t},\n\t\tMydumper: MydumperRuntime{\n\t\t\tReadBlockSize: ReadBlockSize,\n\t\t\tCSV: CSVConfig{\n\t\t\t\tSeparator:       \",\",\n\t\t\t\tDelimiter:       `\"`,\n\t\t\t\tHeader:          true,\n\t\t\t\tNotNull:         false,\n\t\t\t\tNull:            `\\N`,\n\t\t\t\tBackslashEscape: true,\n\t\t\t\tTrimLastSep:     false,\n\t\t\t},\n\t\t\tStrictFormat:           false,\n\t\t\tMaxRegionSize:          MaxRegionSize,\n\t\t\tFilter:                 GetDefaultFilter(),\n\t\t\tDataCharacterSet:       defaultCSVDataCharacterSet,\n\t\t\tDataInvalidCharReplace: string(defaultCSVDataInvalidCharReplace),\n\t\t},\n\t\tTikvImporter: TikvImporter{\n\t\t\tBackend:             \"\",\n\t\t\tOnDuplicate:         ReplaceOnDup,\n\t\t\tMaxKVPairs:          4096,\n\t\t\tSendKVPairs:         32768,\n\t\t\tRegionSplitSize:     0,\n\t\t\tDiskQuota:           ByteSize(math.MaxInt64),\n\t\t\tDuplicateResolution: DupeResAlgNone,\n\t\t},\n\t\tPostRestore: PostRestore{\n\t\t\tChecksum:          OpLevelRequired,\n\t\t\tAnalyze:           OpLevelOptional,\n\t\t\tPostProcessAtLast: true,\n\t\t},\n\t}\n}\n\n// LoadFromGlobal resets the current configuration to the global settings.\nfunc (cfg *Config) LoadFromGlobal(global *GlobalConfig) error {\n\tif err := cfg.LoadFromTOML(global.ConfigFileContent); err != nil {\n\t\treturn err\n\t}\n\n\tcfg.TiDB.Host = global.TiDB.Host\n\tcfg.TiDB.Port = global.TiDB.Port\n\tcfg.TiDB.User = global.TiDB.User\n\tcfg.TiDB.Psw = global.TiDB.Psw\n\tcfg.TiDB.StatusPort = global.TiDB.StatusPort\n\tcfg.TiDB.PdAddr = global.TiDB.PdAddr\n\tcfg.Mydumper.NoSchema = global.Mydumper.NoSchema\n\tcfg.Mydumper.SourceDir = global.Mydumper.SourceDir\n\tcfg.Mydumper.Filter = global.Mydumper.Filter\n\tcfg.TikvImporter.Backend = global.TikvImporter.Backend\n\tcfg.TikvImporter.SortedKVDir = global.TikvImporter.SortedKVDir\n\tcfg.Checkpoint.Enable = global.Checkpoint.Enable\n\tcfg.PostRestore.Checksum = global.PostRestore.Checksum\n\tcfg.PostRestore.Analyze = global.PostRestore.Analyze\n\tcfg.App.CheckRequirements = global.App.CheckRequirements\n\tcfg.Security = global.Security\n\tcfg.Mydumper.IgnoreColumns = global.Mydumper.IgnoreColumns\n\treturn nil\n}\n\n// LoadFromTOML overwrites the current configuration by the TOML data\n// If data contains toml items not in Config and GlobalConfig, return an error\n// If data contains toml items not in Config, thus won't take effect, warn user\nfunc (cfg *Config) LoadFromTOML(data []byte) error {\n\t// bothUnused saves toml items not belong to Config nor GlobalConfig\n\tvar bothUnused []string\n\t// warnItems saves legal toml items but won't effect\n\tvar warnItems []string\n\n\tdataStr := string(data)\n\n\t// Here we load toml into cfg, and rest logic is check unused keys\n\tmetaData, err := toml.Decode(dataStr, cfg)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tunusedConfigKeys := metaData.Undecoded()\n\tif len(unusedConfigKeys) == 0 {\n\t\treturn nil\n\t}\n\n\t// Now we deal with potential both-unused keys of Config and GlobalConfig struct\n\n\tmetaDataGlobal, err := toml.Decode(dataStr, &GlobalConfig{})\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\t// Key type returned by metadata.Undecoded doesn't have a equality comparison,\n\t// we convert them to string type instead, and this conversion is identical\n\tunusedGlobalKeys := metaDataGlobal.Undecoded()\n\tunusedGlobalKeyStrs := make(map[string]struct{})\n\tfor _, key := range unusedGlobalKeys {\n\t\tunusedGlobalKeyStrs[key.String()] = struct{}{}\n\t}\n\n\tfor _, key := range unusedConfigKeys {\n\t\tkeyStr := key.String()\n\t\tif _, found := unusedGlobalKeyStrs[keyStr]; found {\n\t\t\tbothUnused = append(bothUnused, keyStr)\n\t\t} else {\n\t\t\twarnItems = append(warnItems, keyStr)\n\t\t}\n\t}\n\n\tif len(bothUnused) > 0 {\n\t\treturn errors.Errorf(\"config file contained unknown configuration options: %s\",\n\t\t\tstrings.Join(bothUnused, \", \"))\n\t}\n\n\t// Warn that some legal field of config file won't be overwritten, such as lightning.file\n\tif len(warnItems) > 0 {\n\t\tlog.L().Warn(\"currently only per-task configuration can be applied, global configuration changes can only be made on startup\",\n\t\t\tzap.Strings(\"global config changes\", warnItems))\n\t}\n\n\treturn nil\n}\n\n// Adjust fixes the invalid or unspecified settings to reasonable valid values.\nfunc (cfg *Config) Adjust(ctx context.Context) error {\n\t// Reject problematic CSV configurations.\n\tcsv := &cfg.Mydumper.CSV\n\tif len(csv.Separator) == 0 {\n\t\treturn common.ErrInvalidConfig.GenWithStack(\"`mydumper.csv.separator` must not be empty\")\n\t}\n\n\tif len(csv.Delimiter) > 0 && (strings.HasPrefix(csv.Separator, csv.Delimiter) || strings.HasPrefix(csv.Delimiter, csv.Separator)) {\n\t\treturn common.ErrInvalidConfig.GenWithStack(\"`mydumper.csv.separator` and `mydumper.csv.delimiter` must not be prefix of each other\")\n\t}\n\n\tif csv.BackslashEscape {\n\t\tif csv.Separator == `\\` {\n\t\t\treturn common.ErrInvalidConfig.GenWithStack(\"cannot use '\\\\' as CSV separator when `mydumper.csv.backslash-escape` is true\")\n\t\t}\n\t\tif csv.Delimiter == `\\` {\n\t\t\treturn common.ErrInvalidConfig.GenWithStack(\"cannot use '\\\\' as CSV delimiter when `mydumper.csv.backslash-escape` is true\")\n\t\t}\n\t\tif csv.Terminator == `\\` {\n\t\t\treturn common.ErrInvalidConfig.GenWithStack(\"cannot use '\\\\' as CSV terminator when `mydumper.csv.backslash-escape` is true\")\n\t\t}\n\t}\n\n\t// adjust file routing\n\tfor _, rule := range cfg.Mydumper.FileRouters {\n\t\tif filepath.IsAbs(rule.Path) {\n\t\t\trelPath, err := filepath.Rel(cfg.Mydumper.SourceDir, rule.Path)\n\t\t\tif err != nil {\n\t\t\t\treturn common.ErrInvalidConfig.Wrap(err).\n\t\t\t\t\tGenWithStack(\"cannot find relative path for file route path %s\", rule.Path)\n\t\t\t}\n\t\t\t// \"..\" means that this path is not in source dir, so we should return an error\n\t\t\tif strings.HasPrefix(relPath, \"..\") {\n\t\t\t\treturn common.ErrInvalidConfig.GenWithStack(\n\t\t\t\t\t\"file route path '%s' is not in source dir '%s'\", rule.Path, cfg.Mydumper.SourceDir)\n\t\t\t}\n\t\t\trule.Path = relPath\n\t\t}\n\t}\n\n\t// enable default file route rule if no rules are set\n\tif len(cfg.Mydumper.FileRouters) == 0 {\n\t\tcfg.Mydumper.DefaultFileRules = true\n\t}\n\n\tif len(cfg.Mydumper.DataCharacterSet) == 0 {\n\t\tcfg.Mydumper.DataCharacterSet = defaultCSVDataCharacterSet\n\t}\n\tcharset, err1 := ParseCharset(cfg.Mydumper.DataCharacterSet)\n\tif err1 != nil {\n\t\treturn common.ErrInvalidConfig.Wrap(err1).GenWithStack(\"invalid `mydumper.data-character-set`\")\n\t}\n\tif charset == GBK || charset == GB18030 {\n\t\tlog.L().Warn(\n\t\t\t\"incompatible strings may be encountered during the transcoding process and will be replaced, please be aware of the risk of not being able to retain the original information\",\n\t\t\tzap.String(\"source-character-set\", charset.String()),\n\t\t\tzap.ByteString(\"invalid-char-replacement\", []byte(cfg.Mydumper.DataInvalidCharReplace)))\n\t}\n\n\tmustHaveInternalConnections, err := cfg.AdjustCommon()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// mydumper.filter and black-white-list cannot co-exist.\n\tif cfg.HasLegacyBlackWhiteList() {\n\t\tlog.L().Warn(\"the config `black-white-list` has been deprecated, please replace with `mydumper.filter`\")\n\t\tif !common.StringSliceEqual(cfg.Mydumper.Filter, defaultFilter) {\n\t\t\treturn common.ErrInvalidConfig.GenWithStack(\"`mydumper.filter` and `black-white-list` cannot be simultaneously defined\")\n\t\t}\n\t}\n\n\tfor _, rule := range cfg.Routes {\n\t\tif !cfg.Mydumper.CaseSensitive {\n\t\t\trule.ToLower()\n\t\t}\n\t\tif err := rule.Valid(); err != nil {\n\t\t\treturn common.ErrInvalidConfig.Wrap(err).GenWithStack(\"file route rule is invalid\")\n\t\t}\n\t}\n\n\tif err := cfg.CheckAndAdjustTiDBPort(ctx, mustHaveInternalConnections); err != nil {\n\t\treturn err\n\t}\n\tcfg.AdjustMydumper()\n\tcfg.AdjustCheckPoint()\n\treturn cfg.CheckAndAdjustFilePath()\n}\n\nfunc (cfg *Config) AdjustCommon() (bool, error) {\n\tif cfg.TikvImporter.Backend == \"\" {\n\t\treturn false, common.ErrInvalidConfig.GenWithStack(\"tikv-importer.backend must not be empty!\")\n\t}\n\tcfg.TikvImporter.Backend = strings.ToLower(cfg.TikvImporter.Backend)\n\tmustHaveInternalConnections := true\n\tswitch cfg.TikvImporter.Backend {\n\tcase BackendTiDB:\n\t\tcfg.DefaultVarsForTiDBBackend()\n\t\tmustHaveInternalConnections = false\n\t\tcfg.PostRestore.Checksum = OpLevelOff\n\t\tcfg.PostRestore.Analyze = OpLevelOff\n\t\tcfg.PostRestore.Compact = false\n\tcase BackendLocal:\n\t\t// RegionConcurrency > NumCPU is meaningless.\n\t\tcpuCount := runtime.NumCPU()\n\t\tif cfg.App.RegionConcurrency > cpuCount {\n\t\t\tcfg.App.RegionConcurrency = cpuCount\n\t\t}\n\t\tcfg.DefaultVarsForImporterAndLocalBackend()\n\tdefault:\n\t\treturn mustHaveInternalConnections, common.ErrInvalidConfig.GenWithStack(\"unsupported `tikv-importer.backend` (%s)\", cfg.TikvImporter.Backend)\n\t}\n\n\t// TODO calculate these from the machine's free memory.\n\tif cfg.TikvImporter.EngineMemCacheSize == 0 {\n\t\tcfg.TikvImporter.EngineMemCacheSize = defaultEngineMemCacheSize\n\t}\n\tif cfg.TikvImporter.LocalWriterMemCacheSize == 0 {\n\t\tcfg.TikvImporter.LocalWriterMemCacheSize = defaultLocalWriterMemCacheSize\n\t}\n\n\tif cfg.TikvImporter.Backend == BackendLocal {\n\t\tif err := cfg.CheckAndAdjustForLocalBackend(); err != nil {\n\t\t\treturn mustHaveInternalConnections, err\n\t\t}\n\t} else {\n\t\tcfg.TikvImporter.DuplicateResolution = DupeResAlgNone\n\t}\n\n\tif cfg.TikvImporter.Backend == BackendTiDB {\n\t\tcfg.TikvImporter.OnDuplicate = strings.ToLower(cfg.TikvImporter.OnDuplicate)\n\t\tswitch cfg.TikvImporter.OnDuplicate {\n\t\tcase ReplaceOnDup, IgnoreOnDup, ErrorOnDup:\n\t\tdefault:\n\t\t\treturn mustHaveInternalConnections, common.ErrInvalidConfig.GenWithStack(\n\t\t\t\t\"unsupported `tikv-importer.on-duplicate` (%s)\", cfg.TikvImporter.OnDuplicate)\n\t\t}\n\t}\n\n\tvar err error\n\tcfg.TiDB.SQLMode, err = mysql.GetSQLMode(cfg.TiDB.StrSQLMode)\n\tif err != nil {\n\t\treturn mustHaveInternalConnections, common.ErrInvalidConfig.Wrap(err).GenWithStack(\"`mydumper.tidb.sql_mode` must be a valid SQL_MODE\")\n\t}\n\n\tif err := cfg.CheckAndAdjustSecurity(); err != nil {\n\t\treturn mustHaveInternalConnections, err\n\t}\n\treturn mustHaveInternalConnections, err\n}\n\nfunc (cfg *Config) CheckAndAdjustForLocalBackend() error {\n\tif len(cfg.TikvImporter.SortedKVDir) == 0 {\n\t\treturn common.ErrInvalidConfig.GenWithStack(\"tikv-importer.sorted-kv-dir must not be empty!\")\n\t}\n\n\tstorageSizeDir := filepath.Clean(cfg.TikvImporter.SortedKVDir)\n\tsortedKVDirInfo, err := os.Stat(storageSizeDir)\n\n\tswitch {\n\tcase os.IsNotExist(err):\n\t\treturn nil\n\tcase err == nil:\n\t\tif !sortedKVDirInfo.IsDir() {\n\t\t\treturn common.ErrInvalidConfig.\n\t\t\t\tGenWithStack(\"tikv-importer.sorted-kv-dir ('%s') is not a directory\", storageSizeDir)\n\t\t}\n\tdefault:\n\t\treturn common.ErrInvalidConfig.Wrap(err).GenWithStack(\"invalid tikv-importer.sorted-kv-dir\")\n\t}\n\n\treturn nil\n}\n\nfunc (cfg *Config) DefaultVarsForTiDBBackend() {\n\tif cfg.App.TableConcurrency == 0 {\n\t\tcfg.App.TableConcurrency = cfg.App.RegionConcurrency\n\t}\n\tif cfg.App.IndexConcurrency == 0 {\n\t\tcfg.App.IndexConcurrency = cfg.App.RegionConcurrency\n\t}\n}\n\nfunc (cfg *Config) DefaultVarsForImporterAndLocalBackend() {\n\tif cfg.App.IndexConcurrency == 0 {\n\t\tcfg.App.IndexConcurrency = defaultIndexConcurrency\n\t}\n\tif cfg.App.TableConcurrency == 0 {\n\t\tcfg.App.TableConcurrency = defaultTableConcurrency\n\t}\n\n\tif len(cfg.App.MetaSchemaName) == 0 {\n\t\tcfg.App.MetaSchemaName = defaultMetaSchemaName\n\t}\n\tif cfg.TikvImporter.RangeConcurrency == 0 {\n\t\tcfg.TikvImporter.RangeConcurrency = 16\n\t}\n\tif cfg.TiDB.BuildStatsConcurrency == 0 {\n\t\tcfg.TiDB.BuildStatsConcurrency = defaultBuildStatsConcurrency\n\t}\n\tif cfg.TiDB.IndexSerialScanConcurrency == 0 {\n\t\tcfg.TiDB.IndexSerialScanConcurrency = defaultIndexSerialScanConcurrency\n\t}\n\tif cfg.TiDB.ChecksumTableConcurrency == 0 {\n\t\tcfg.TiDB.ChecksumTableConcurrency = defaultChecksumTableConcurrency\n\t}\n}\n\nfunc (cfg *Config) CheckAndAdjustTiDBPort(ctx context.Context, mustHaveInternalConnections bool) error {\n\t// automatically determine the TiDB port & PD address from TiDB settings\n\tif mustHaveInternalConnections && (cfg.TiDB.Port <= 0 || len(cfg.TiDB.PdAddr) == 0) {\n\t\ttls, err := cfg.ToTLS()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tvar settings tidbcfg.Config\n\t\terr = tls.GetJSON(ctx, \"/settings\", &settings)\n\t\tif err != nil {\n\t\t\treturn common.ErrInvalidConfig.Wrap(err).GenWithStack(\"cannot fetch settings from TiDB, please manually fill in `tidb.port` and `tidb.pd-addr`\")\n\t\t}\n\t\tif cfg.TiDB.Port <= 0 {\n\t\t\tcfg.TiDB.Port = int(settings.Port)\n\t\t}\n\t\tif len(cfg.TiDB.PdAddr) == 0 {\n\t\t\tpdAddrs := strings.Split(settings.Path, \",\")\n\t\t\tcfg.TiDB.PdAddr = pdAddrs[0] // FIXME support multiple PDs once importer can.\n\t\t}\n\t}\n\n\tif cfg.TiDB.Port <= 0 {\n\t\treturn common.ErrInvalidConfig.GenWithStack(\"invalid `tidb.port` setting\")\n\t}\n\n\tif mustHaveInternalConnections && len(cfg.TiDB.PdAddr) == 0 {\n\t\treturn common.ErrInvalidConfig.GenWithStack(\"invalid `tidb.pd-addr` setting\")\n\t}\n\treturn nil\n}\n\nfunc (cfg *Config) CheckAndAdjustFilePath() error {\n\tvar u *url.URL\n\n\t// An absolute Windows path like \"C:\\Users\\XYZ\" would be interpreted as\n\t// an URL with scheme \"C\" and opaque data \"\\Users\\XYZ\".\n\t// Therefore, we only perform URL parsing if we are sure the path is not\n\t// an absolute Windows path.\n\t// Here we use the `filepath.VolumeName` which can identify the \"C:\" part\n\t// out of the path. On Linux this method always return an empty string.\n\t// On Windows, the drive letter can only be single letters from \"A:\" to \"Z:\",\n\t// so this won't mistake \"S3:\" as a Windows path.\n\tif len(filepath.VolumeName(cfg.Mydumper.SourceDir)) == 0 {\n\t\tvar err error\n\t\tu, err = url.Parse(cfg.Mydumper.SourceDir)\n\t\tif err != nil {\n\t\t\treturn common.ErrInvalidConfig.Wrap(err).GenWithStack(\"cannot parse `mydumper.data-source-dir` %s\", cfg.Mydumper.SourceDir)\n\t\t}\n\t} else {\n\t\tu = &url.URL{}\n\t}\n\n\t// convert path and relative path to a valid file url\n\tif u.Scheme == \"\" {\n\t\tif cfg.Mydumper.SourceDir == \"\" {\n\t\t\treturn common.ErrInvalidConfig.GenWithStack(\"`mydumper.data-source-dir` is not set\")\n\t\t}\n\t\tif !common.IsDirExists(cfg.Mydumper.SourceDir) {\n\t\t\treturn common.ErrInvalidConfig.GenWithStack(\"'%s': `mydumper.data-source-dir` does not exist\", cfg.Mydumper.SourceDir)\n\t\t}\n\t\tabsPath, err := filepath.Abs(cfg.Mydumper.SourceDir)\n\t\tif err != nil {\n\t\t\treturn common.ErrInvalidConfig.Wrap(err).GenWithStack(\"covert data-source-dir '%s' to absolute path failed\", cfg.Mydumper.SourceDir)\n\t\t}\n\t\tu.Path = filepath.ToSlash(absPath)\n\t\tu.Scheme = \"file\"\n\t\tcfg.Mydumper.SourceDir = u.String()\n\t}\n\n\tfound := false\n\tfor _, t := range supportedStorageTypes {\n\t\tif u.Scheme == t {\n\t\t\tfound = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !found {\n\t\treturn common.ErrInvalidConfig.GenWithStack(\n\t\t\t\"unsupported data-source-dir url '%s', supported storage types are %s\",\n\t\t\tcfg.Mydumper.SourceDir, strings.Join(supportedStorageTypes, \",\"))\n\t}\n\treturn nil\n}\n\nfunc (cfg *Config) AdjustCheckPoint() {\n\tif len(cfg.Checkpoint.Schema) == 0 {\n\t\tcfg.Checkpoint.Schema = \"tidb_lightning_checkpoint\"\n\t}\n\tif len(cfg.Checkpoint.Driver) == 0 {\n\t\tcfg.Checkpoint.Driver = CheckpointDriverFile\n\t}\n\tif len(cfg.Checkpoint.DSN) == 0 {\n\t\tswitch cfg.Checkpoint.Driver {\n\t\tcase CheckpointDriverMySQL:\n\t\t\tparam := common.MySQLConnectParam{\n\t\t\t\tHost:             cfg.TiDB.Host,\n\t\t\t\tPort:             cfg.TiDB.Port,\n\t\t\t\tUser:             cfg.TiDB.User,\n\t\t\t\tPassword:         cfg.TiDB.Psw,\n\t\t\t\tSQLMode:          mysql.DefaultSQLMode,\n\t\t\t\tMaxAllowedPacket: defaultMaxAllowedPacket,\n\t\t\t\tTLS:              cfg.TiDB.TLS,\n\t\t\t}\n\t\t\tcfg.Checkpoint.DSN = param.ToDSN()\n\t\tcase CheckpointDriverFile:\n\t\t\tcfg.Checkpoint.DSN = \"/tmp/\" + cfg.Checkpoint.Schema + \".pb\"\n\t\t}\n\t}\n}\n\nfunc (cfg *Config) AdjustMydumper() {\n\tif cfg.Mydumper.BatchImportRatio < 0.0 || cfg.Mydumper.BatchImportRatio >= 1.0 {\n\t\tcfg.Mydumper.BatchImportRatio = 0.75\n\t}\n\tif cfg.Mydumper.ReadBlockSize <= 0 {\n\t\tcfg.Mydumper.ReadBlockSize = ReadBlockSize\n\t}\n\tif len(cfg.Mydumper.CharacterSet) == 0 {\n\t\tcfg.Mydumper.CharacterSet = \"auto\"\n\t}\n\n\tif len(cfg.Mydumper.IgnoreColumns) != 0 {\n\t\t// Tolower columns cause we use Name.L to compare column in tidb.\n\t\tfor _, ig := range cfg.Mydumper.IgnoreColumns {\n\t\t\tcols := make([]string, len(ig.Columns))\n\t\t\tfor i, col := range ig.Columns {\n\t\t\t\tcols[i] = strings.ToLower(col)\n\t\t\t}\n\t\t\tig.Columns = cols\n\t\t}\n\t}\n}\n\nfunc (cfg *Config) CheckAndAdjustSecurity() error {\n\tif cfg.TiDB.Security == nil {\n\t\tcfg.TiDB.Security = &cfg.Security\n\t}\n\n\tswitch cfg.TiDB.TLS {\n\tcase \"\":\n\t\tif len(cfg.TiDB.Security.CAPath) > 0 || len(cfg.TiDB.Security.CABytes) > 0 ||\n\t\t\tlen(cfg.TiDB.Security.CertPath) > 0 || len(cfg.TiDB.Security.CertBytes) > 0 ||\n\t\t\tlen(cfg.TiDB.Security.KeyPath) > 0 || len(cfg.TiDB.Security.KeyBytes) > 0 {\n\t\t\tif cfg.TiDB.Security.TLSConfigName == \"\" {\n\t\t\t\tcfg.TiDB.Security.TLSConfigName = uuid.NewString() // adjust this the default value\n\t\t\t}\n\t\t\tcfg.TiDB.TLS = cfg.TiDB.Security.TLSConfigName\n\t\t} else {\n\t\t\tcfg.TiDB.TLS = \"false\"\n\t\t}\n\tcase \"cluster\":\n\t\tif len(cfg.Security.CAPath) == 0 {\n\t\t\treturn common.ErrInvalidConfig.GenWithStack(\"cannot set `tidb.tls` to 'cluster' without a [security] section\")\n\t\t}\n\tcase \"false\", \"skip-verify\", \"preferred\":\n\t\treturn nil\n\tdefault:\n\t\treturn common.ErrInvalidConfig.GenWithStack(\"unsupported `tidb.tls` config %s\", cfg.TiDB.TLS)\n\t}\n\treturn nil\n}\n\n// HasLegacyBlackWhiteList checks whether the deprecated [black-white-list] section\n// was defined.\nfunc (cfg *Config) HasLegacyBlackWhiteList() bool {\n\treturn len(cfg.BWList.DoTables) != 0 || len(cfg.BWList.DoDBs) != 0 || len(cfg.BWList.IgnoreTables) != 0 || len(cfg.BWList.IgnoreDBs) != 0\n}\n", "// Copyright 2019 PingCAP, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage config_test\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"flag\"\n\t\"fmt\"\n\t\"net\"\n\t\"net/http\"\n\t\"net/http/httptest\"\n\t\"net/url\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/BurntSushi/toml\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/common\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/config\"\n\t\"github.com/pingcap/tidb/parser/mysql\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc startMockServer(t *testing.T, statusCode int, content string) (*httptest.Server, string, int) {\n\tts := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tw.WriteHeader(statusCode)\n\t\t_, _ = fmt.Fprint(w, content)\n\t}))\n\n\turl, err := url.Parse(ts.URL)\n\trequire.NoError(t, err)\n\thost, portString, err := net.SplitHostPort(url.Host)\n\trequire.NoError(t, err)\n\tport, err := strconv.Atoi(portString)\n\trequire.NoError(t, err)\n\n\treturn ts, host, port\n}\n\nfunc assignMinimalLegalValue(cfg *config.Config) {\n\tcfg.TiDB.Host = \"123.45.67.89\"\n\tcfg.TiDB.Port = 4567\n\tcfg.TiDB.StatusPort = 8901\n\tcfg.TiDB.PdAddr = \"234.56.78.90:12345\"\n\tcfg.Mydumper.SourceDir = \"file://.\"\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.SortedKVDir = \".\"\n\tcfg.TikvImporter.DiskQuota = 1\n}\n\nfunc TestAdjustPdAddrAndPort(t *testing.T) {\n\tts, host, port := startMockServer(t, http.StatusOK,\n\t\t`{\"port\":4444,\"advertise-address\":\"\",\"path\":\"123.45.67.89:1234,56.78.90.12:3456\"}`,\n\t)\n\tdefer ts.Close()\n\n\tcfg := config.NewConfig()\n\tcfg.TiDB.Host = host\n\tcfg.TiDB.StatusPort = port\n\tcfg.Mydumper.SourceDir = \".\"\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.SortedKVDir = \".\"\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\n\terr := cfg.Adjust(context.Background())\n\trequire.NoError(t, err)\n\trequire.Equal(t, 4444, cfg.TiDB.Port)\n\trequire.Equal(t, \"123.45.67.89:1234\", cfg.TiDB.PdAddr)\n}\n\nfunc TestAdjustPdAddrAndPortViaAdvertiseAddr(t *testing.T) {\n\tts, host, port := startMockServer(t, http.StatusOK,\n\t\t`{\"port\":6666,\"advertise-address\":\"121.212.121.212:5555\",\"path\":\"34.34.34.34:3434\"}`,\n\t)\n\tdefer ts.Close()\n\n\tcfg := config.NewConfig()\n\tcfg.TiDB.Host = host\n\tcfg.TiDB.StatusPort = port\n\tcfg.Mydumper.SourceDir = \".\"\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.SortedKVDir = \".\"\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\n\terr := cfg.Adjust(context.Background())\n\trequire.NoError(t, err)\n\trequire.Equal(t, 6666, cfg.TiDB.Port)\n\trequire.Equal(t, \"34.34.34.34:3434\", cfg.TiDB.PdAddr)\n}\n\nfunc TestAdjustPageNotFound(t *testing.T) {\n\tts, host, port := startMockServer(t, http.StatusNotFound, \"{}\")\n\tdefer ts.Close()\n\n\tcfg := config.NewConfig()\n\tcfg.TiDB.Host = host\n\tcfg.TiDB.StatusPort = port\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.SortedKVDir = \".\"\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\n\terr := cfg.Adjust(context.Background())\n\trequire.Error(t, err)\n\trequire.Regexp(t, \"cannot fetch settings from TiDB.*\", err.Error())\n}\n\nfunc TestAdjustConnectRefused(t *testing.T) {\n\tts, host, port := startMockServer(t, http.StatusOK, \"{}\")\n\n\tcfg := config.NewConfig()\n\tcfg.TiDB.Host = host\n\tcfg.TiDB.StatusPort = port\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.SortedKVDir = \".\"\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\n\tts.Close() // immediately close to ensure connection refused.\n\n\terr := cfg.Adjust(context.Background())\n\trequire.Error(t, err)\n\trequire.Regexp(t, \"cannot fetch settings from TiDB.*\", err.Error())\n}\n\nfunc TestAdjustBackendNotSet(t *testing.T) {\n\tcfg := config.NewConfig()\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\terr := cfg.Adjust(context.Background())\n\trequire.EqualError(t, err, \"[Lightning:Config:ErrInvalidConfig]tikv-importer.backend must not be empty!\")\n}\n\nfunc TestAdjustInvalidBackend(t *testing.T) {\n\tcfg := config.NewConfig()\n\tcfg.TikvImporter.Backend = \"no_such_backend\"\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\terr := cfg.Adjust(context.Background())\n\trequire.EqualError(t, err, \"[Lightning:Config:ErrInvalidConfig]unsupported `tikv-importer.backend` (no_such_backend)\")\n}\n\nfunc TestCheckAndAdjustFilePath(t *testing.T) {\n\ttmpDir := t.TempDir()\n\t// use slashPath in url to be compatible with windows\n\tslashPath := filepath.ToSlash(tmpDir)\n\tpwd, err := os.Getwd()\n\trequire.NoError(t, err)\n\tspecialDir, err := os.MkdirTemp(tmpDir, \"abc??bcd\")\n\trequire.NoError(t, err)\n\tspecialDir1, err := os.MkdirTemp(tmpDir, \"abc%3F%3F%3Fbcd\")\n\trequire.NoError(t, err)\n\n\tcfg := config.NewConfig()\n\n\tcases := []struct {\n\t\ttest   string\n\t\texpect string\n\t}{\n\t\t{tmpDir, tmpDir},\n\t\t{\".\", filepath.ToSlash(pwd)},\n\t\t{specialDir, specialDir},\n\t\t{specialDir1, specialDir1},\n\t\t{\"file://\" + slashPath, slashPath},\n\t\t{\"local://\" + slashPath, slashPath},\n\t\t{\"s3://bucket_name\", \"\"},\n\t\t{\"s3://bucket_name/path/to/dir\", \"/path/to/dir\"},\n\t\t{\"gcs://bucketname/path/to/dir\", \"/path/to/dir\"},\n\t\t{\"gs://bucketname/path/to/dir\", \"/path/to/dir\"},\n\t\t{\"noop:///\", \"/\"},\n\t}\n\tfor _, testCase := range cases {\n\t\tcfg.Mydumper.SourceDir = testCase.test\n\t\terr = cfg.CheckAndAdjustFilePath()\n\t\trequire.NoError(t, err)\n\t\tu, err := url.Parse(cfg.Mydumper.SourceDir)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, testCase.expect, u.Path)\n\t}\n}\n\nfunc TestAdjustFileRoutePath(t *testing.T) {\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\n\tctx := context.Background()\n\ttmpDir := t.TempDir()\n\tcfg.Mydumper.SourceDir = tmpDir\n\tinvalidPath := filepath.Join(tmpDir, \"../test123/1.sql\")\n\trule := &config.FileRouteRule{Path: invalidPath, Type: \"sql\", Schema: \"test\", Table: \"tbl\"}\n\tcfg.Mydumper.FileRouters = []*config.FileRouteRule{rule}\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\terr := cfg.Adjust(ctx)\n\trequire.Error(t, err)\n\trequire.Regexp(t, fmt.Sprintf(\"\\\\Qfile route path '%s' is not in source dir '%s'\\\\E\", invalidPath, tmpDir), err.Error())\n\n\trelPath := filepath.FromSlash(\"test_dir/1.sql\")\n\trule.Path = filepath.Join(tmpDir, relPath)\n\terr = cfg.Adjust(ctx)\n\trequire.NoError(t, err)\n\trequire.Equal(t, relPath, cfg.Mydumper.FileRouters[0].Path)\n}\n\nfunc TestDecodeError(t *testing.T) {\n\tts, host, port := startMockServer(t, http.StatusOK, \"invalid-string\")\n\tdefer ts.Close()\n\n\tcfg := config.NewConfig()\n\tcfg.TiDB.Host = host\n\tcfg.TiDB.StatusPort = port\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.SortedKVDir = \".\"\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\n\terr := cfg.Adjust(context.Background())\n\trequire.Error(t, err)\n\trequire.Regexp(t, \"cannot fetch settings from TiDB.*\", err.Error())\n}\n\nfunc TestInvalidSetting(t *testing.T) {\n\tts, host, port := startMockServer(t, http.StatusOK, `{\"port\": 0}`)\n\tdefer ts.Close()\n\n\tcfg := config.NewConfig()\n\tcfg.TiDB.Host = host\n\tcfg.TiDB.StatusPort = port\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.SortedKVDir = \".\"\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\n\terr := cfg.Adjust(context.Background())\n\trequire.EqualError(t, err, \"[Lightning:Config:ErrInvalidConfig]invalid `tidb.port` setting\")\n}\n\nfunc TestInvalidPDAddr(t *testing.T) {\n\tts, host, port := startMockServer(t, http.StatusOK, `{\"port\": 1234, \"path\": \",,\"}`)\n\tdefer ts.Close()\n\n\tcfg := config.NewConfig()\n\tcfg.TiDB.Host = host\n\tcfg.TiDB.StatusPort = port\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.SortedKVDir = \".\"\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\n\terr := cfg.Adjust(context.Background())\n\trequire.EqualError(t, err, \"[Lightning:Config:ErrInvalidConfig]invalid `tidb.pd-addr` setting\")\n}\n\nfunc TestAdjustWillNotContactServerIfEverythingIsDefined(t *testing.T) {\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\n\terr := cfg.Adjust(context.Background())\n\trequire.NoError(t, err)\n\trequire.Equal(t, 4567, cfg.TiDB.Port)\n\trequire.Equal(t, \"234.56.78.90:12345\", cfg.TiDB.PdAddr)\n}\n\nfunc TestAdjustWillBatchImportRatioInvalid(t *testing.T) {\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\tcfg.Mydumper.BatchImportRatio = -1\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\terr := cfg.Adjust(context.Background())\n\trequire.NoError(t, err)\n\trequire.Equal(t, 0.75, cfg.Mydumper.BatchImportRatio)\n}\n\nfunc TestAdjustSecuritySection(t *testing.T) {\n\tuuidHolder := \"<uuid>\"\n\ttestCases := []struct {\n\t\tinput       string\n\t\texpectedCA  string\n\t\texpectedTLS string\n\t}{\n\t\t{\n\t\t\tinput:       ``,\n\t\t\texpectedCA:  \"\",\n\t\t\texpectedTLS: \"false\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[security]\n\t\t\t`,\n\t\t\texpectedCA:  \"\",\n\t\t\texpectedTLS: \"false\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[security]\n\t\t\t\tca-path = \"/path/to/ca.pem\"\n\t\t\t`,\n\t\t\texpectedCA:  \"/path/to/ca.pem\",\n\t\t\texpectedTLS: uuidHolder,\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[security]\n\t\t\t\tca-path = \"/path/to/ca.pem\"\n\t\t\t\t[tidb.security]\n\t\t\t`,\n\t\t\texpectedCA:  \"\",\n\t\t\texpectedTLS: \"false\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[security]\n\t\t\t\tca-path = \"/path/to/ca.pem\"\n\t\t\t\t[tidb.security]\n\t\t\t\tca-path = \"/path/to/ca2.pem\"\n\t\t\t`,\n\t\t\texpectedCA:  \"/path/to/ca2.pem\",\n\t\t\texpectedTLS: uuidHolder,\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[security]\n\t\t\t\t[tidb.security]\n\t\t\t\tca-path = \"/path/to/ca2.pem\"\n\t\t\t`,\n\t\t\texpectedCA:  \"/path/to/ca2.pem\",\n\t\t\texpectedTLS: uuidHolder,\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[security]\n\t\t\t\t[tidb]\n\t\t\t\ttls = \"skip-verify\"\n\t\t\t\t[tidb.security]\n\t\t\t`,\n\t\t\texpectedCA:  \"\",\n\t\t\texpectedTLS: \"skip-verify\",\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tcomment := fmt.Sprintf(\"input = %s\", tc.input)\n\n\t\tcfg := config.NewConfig()\n\t\tassignMinimalLegalValue(cfg)\n\t\tcfg.TiDB.DistSQLScanConcurrency = 1\n\t\terr := cfg.LoadFromTOML([]byte(tc.input))\n\t\trequire.NoError(t, err, comment)\n\n\t\terr = cfg.Adjust(context.Background())\n\t\trequire.NoError(t, err, comment)\n\t\trequire.Equal(t, tc.expectedCA, cfg.TiDB.Security.CAPath, comment)\n\t\tif tc.expectedTLS == uuidHolder {\n\t\t\trequire.NotEmpty(t, cfg.TiDB.TLS, comment)\n\t\t} else {\n\t\t\trequire.Equal(t, tc.expectedTLS, cfg.TiDB.TLS, comment)\n\t\t}\n\t}\n\t// test different tls config name\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\tcfg.Security.CAPath = \"/path/to/ca.pem\"\n\tcfg.Security.TLSConfigName = \"tidb-tls\"\n\trequire.NoError(t, cfg.Adjust(context.Background()))\n\trequire.Equal(t, cfg.TiDB.TLS, cfg.TiDB.Security.TLSConfigName)\n}\n\nfunc TestInvalidCSV(t *testing.T) {\n\ttestCases := []struct {\n\t\tinput string\n\t\terr   string\n\t}{\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tseparator = ''\n\t\t\t`,\n\t\t\terr: \"[Lightning:Config:ErrInvalidConfig]`mydumper.csv.separator` must not be empty\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tseparator = 'hello'\n\t\t\t\tdelimiter = 'hel'\n\t\t\t`,\n\t\t\terr: \"[Lightning:Config:ErrInvalidConfig]`mydumper.csv.separator` and `mydumper.csv.delimiter` must not be prefix of each other\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tseparator = 'hel'\n\t\t\t\tdelimiter = 'hello'\n\t\t\t`,\n\t\t\terr: \"[Lightning:Config:ErrInvalidConfig]`mydumper.csv.separator` and `mydumper.csv.delimiter` must not be prefix of each other\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tseparator = '\\'\n\t\t\t\tbackslash-escape = false\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tseparator = '\uff0c'\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tdelimiter = ''\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tdelimiter = 'hello'\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tdelimiter = '\\'\n\t\t\t\tbackslash-escape = false\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tseparator = '\\s'\n\t\t\t\tdelimiter = '\\d'\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tseparator = '|'\n\t\t\t\tdelimiter = '|'\n\t\t\t`,\n\t\t\terr: \"[Lightning:Config:ErrInvalidConfig]`mydumper.csv.separator` and `mydumper.csv.delimiter` must not be prefix of each other\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tseparator = '\\'\n\t\t\t\tbackslash-escape = true\n\t\t\t`,\n\t\t\terr: \"[Lightning:Config:ErrInvalidConfig]cannot use '\\\\' as CSV separator when `mydumper.csv.backslash-escape` is true\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tdelimiter = '\\'\n\t\t\t\tbackslash-escape = true\n\t\t\t`,\n\t\t\terr: \"[Lightning:Config:ErrInvalidConfig]cannot use '\\\\' as CSV delimiter when `mydumper.csv.backslash-escape` is true\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[tidb]\n\t\t\t\tsql-mode = \"invalid-sql-mode\"\n\t\t\t`,\n\t\t\terr: \"[Lightning:Config:ErrInvalidConfig]`mydumper.tidb.sql_mode` must be a valid SQL_MODE: ERROR 1231 (42000): Variable 'sql_mode' can't be set to the value of 'invalid-sql-mode'\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[[routes]]\n\t\t\t\tschema-pattern = \"\"\n\t\t\t\ttable-pattern = \"shard_table_*\"\n\t\t\t`,\n\t\t\terr: \"[Lightning:Config:ErrInvalidConfig]file route rule is invalid: schema pattern of table route rule should not be empty\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[[routes]]\n\t\t\t\tschema-pattern = \"schema_*\"\n\t\t\t\ttable-pattern = \"\"\n\t\t\t`,\n\t\t\terr: \"[Lightning:Config:ErrInvalidConfig]file route rule is invalid: target schema of table route rule should not be empty\",\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tcomment := fmt.Sprintf(\"input = %s\", tc.input)\n\t\tcfg := config.NewConfig()\n\t\tcfg.Mydumper.SourceDir = \"file://.\"\n\t\tcfg.TiDB.Port = 4000\n\t\tcfg.TiDB.PdAddr = \"test.invalid:2379\"\n\t\tcfg.TikvImporter.Backend = config.BackendLocal\n\t\tcfg.TikvImporter.SortedKVDir = \".\"\n\t\tcfg.TiDB.DistSQLScanConcurrency = 1\n\t\terr := cfg.LoadFromTOML([]byte(tc.input))\n\t\trequire.NoError(t, err)\n\n\t\terr = cfg.Adjust(context.Background())\n\t\tif tc.err != \"\" {\n\t\t\trequire.EqualError(t, err, tc.err, comment)\n\t\t} else {\n\t\t\trequire.NoError(t, err)\n\t\t}\n\t}\n}\n\nfunc TestInvalidTOML(t *testing.T) {\n\tcfg := &config.Config{}\n\terr := cfg.LoadFromTOML([]byte(`\n\t\tinvalid[mydumper.csv]\n\t\tdelimiter = '\\'\n\t\tbackslash-escape = true\n\t`))\n\trequire.EqualError(t, err, \"toml: line 2: expected '.' or '=', but got '[' instead\")\n}\n\nfunc TestTOMLUnusedKeys(t *testing.T) {\n\tcfg := &config.Config{}\n\terr := cfg.LoadFromTOML([]byte(`\n\t\t[lightning]\n\t\ttypo = 123\n\t`))\n\trequire.EqualError(t, err, \"config file contained unknown configuration options: lightning.typo\")\n}\n\nfunc TestDurationUnmarshal(t *testing.T) {\n\tduration := config.Duration{}\n\terr := duration.UnmarshalText([]byte(\"13m20s\"))\n\trequire.NoError(t, err)\n\trequire.Equal(t, 13*60+20.0, duration.Duration.Seconds())\n\terr = duration.UnmarshalText([]byte(\"13x20s\"))\n\trequire.Error(t, err)\n\trequire.Regexp(t, \"time: unknown unit .?x.? in duration .?13x20s.?\", err.Error())\n}\n\nfunc TestDurationMarshalJSON(t *testing.T) {\n\tduration := config.Duration{}\n\terr := duration.UnmarshalText([]byte(\"13m20s\"))\n\trequire.NoError(t, err)\n\trequire.Equal(t, 13*60+20.0, duration.Duration.Seconds())\n\tresult, err := duration.MarshalJSON()\n\trequire.NoError(t, err)\n\trequire.Equal(t, `\"13m20s\"`, string(result))\n}\n\nfunc TestDuplicateResolutionAlgorithm(t *testing.T) {\n\tvar dra config.DuplicateResolutionAlgorithm\n\trequire.NoError(t, dra.FromStringValue(\"record\"))\n\trequire.Equal(t, config.DupeResAlgRecord, dra)\n\trequire.NoError(t, dra.FromStringValue(\"none\"))\n\trequire.Equal(t, config.DupeResAlgNone, dra)\n\trequire.NoError(t, dra.FromStringValue(\"remove\"))\n\trequire.Equal(t, config.DupeResAlgRemove, dra)\n\n\trequire.Equal(t, \"record\", config.DupeResAlgRecord.String())\n\trequire.Equal(t, \"none\", config.DupeResAlgNone.String())\n\trequire.Equal(t, \"remove\", config.DupeResAlgRemove.String())\n}\n\nfunc TestLoadConfig(t *testing.T) {\n\tcfg, err := config.LoadGlobalConfig([]string{\"-tidb-port\", \"sss\"}, nil)\n\trequire.EqualError(t, err, `[Lightning:Common:ErrInvalidArgument]invalid argument: invalid value \"sss\" for flag -tidb-port: parse error`)\n\trequire.Nil(t, cfg)\n\n\tcfg, err = config.LoadGlobalConfig([]string{\"-V\"}, nil)\n\trequire.Equal(t, flag.ErrHelp, err)\n\trequire.Nil(t, cfg)\n\n\tcfg, err = config.LoadGlobalConfig([]string{\"-config\", \"not-exists\"}, nil)\n\trequire.Error(t, err)\n\trequire.Regexp(t, \".*(no such file or directory|The system cannot find the file specified).*\", err.Error())\n\trequire.Nil(t, cfg)\n\n\tcfg, err = config.LoadGlobalConfig([]string{\"--server-mode\"}, nil)\n\trequire.EqualError(t, err, \"[Lightning:Config:ErrInvalidConfig]If server-mode is enabled, the status-addr must be a valid listen address\")\n\trequire.Nil(t, cfg)\n\n\tpath, _ := filepath.Abs(\".\")\n\tcfg, err = config.LoadGlobalConfig([]string{\n\t\t\"-L\", \"debug\",\n\t\t\"-log-file\", \"/path/to/file.log\",\n\t\t\"-tidb-host\", \"172.16.30.11\",\n\t\t\"-tidb-port\", \"4001\",\n\t\t\"-tidb-user\", \"guest\",\n\t\t\"-tidb-password\", \"12345\",\n\t\t\"-pd-urls\", \"172.16.30.11:2379,172.16.30.12:2379\",\n\t\t\"-d\", path,\n\t\t\"-backend\", config.BackendLocal,\n\t\t\"-sorted-kv-dir\", \".\",\n\t\t\"-checksum=false\",\n\t}, nil)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"debug\", cfg.App.Config.Level)\n\trequire.Equal(t, \"/path/to/file.log\", cfg.App.Config.File)\n\trequire.Equal(t, \"172.16.30.11\", cfg.TiDB.Host)\n\trequire.Equal(t, 4001, cfg.TiDB.Port)\n\trequire.Equal(t, \"guest\", cfg.TiDB.User)\n\trequire.Equal(t, \"12345\", cfg.TiDB.Psw)\n\trequire.Equal(t, \"172.16.30.11:2379,172.16.30.12:2379\", cfg.TiDB.PdAddr)\n\trequire.Equal(t, path, cfg.Mydumper.SourceDir)\n\trequire.Equal(t, config.BackendLocal, cfg.TikvImporter.Backend)\n\trequire.Equal(t, \".\", cfg.TikvImporter.SortedKVDir)\n\trequire.Equal(t, config.OpLevelOff, cfg.PostRestore.Checksum)\n\trequire.Equal(t, config.OpLevelOptional, cfg.PostRestore.Analyze)\n\n\ttaskCfg := config.NewConfig()\n\terr = taskCfg.LoadFromGlobal(cfg)\n\trequire.NoError(t, err)\n\trequire.Equal(t, config.OpLevelOff, taskCfg.PostRestore.Checksum)\n\trequire.Equal(t, config.OpLevelOptional, taskCfg.PostRestore.Analyze)\n\n\ttaskCfg.Checkpoint.DSN = \"\"\n\ttaskCfg.Checkpoint.Driver = config.CheckpointDriverMySQL\n\ttaskCfg.TiDB.DistSQLScanConcurrency = 1\n\terr = taskCfg.Adjust(context.Background())\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"guest:12345@tcp(172.16.30.11:4001)/?charset=utf8mb4&sql_mode='\"+mysql.DefaultSQLMode+\"'&maxAllowedPacket=67108864&tls=false\", taskCfg.Checkpoint.DSN)\n\n\tresult := taskCfg.String()\n\trequire.Regexp(t, `.*\"pd-addr\":\"172.16.30.11:2379,172.16.30.12:2379\".*`, result)\n\n\tcfg, err = config.LoadGlobalConfig([]string{}, nil)\n\trequire.NoError(t, err)\n\trequire.Regexp(t, \".*lightning.log.*\", cfg.App.Config.File)\n\tcfg, err = config.LoadGlobalConfig([]string{\"--log-file\", \"-\"}, nil)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"-\", cfg.App.Config.File)\n}\n\nfunc TestDefaultImporterBackendValue(t *testing.T) {\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\tcfg.TikvImporter.Backend = \"local\"\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\terr := cfg.Adjust(context.Background())\n\trequire.NoError(t, err)\n\trequire.Equal(t, 2, cfg.App.IndexConcurrency)\n\trequire.Equal(t, 6, cfg.App.TableConcurrency)\n}\n\nfunc TestDefaultTidbBackendValue(t *testing.T) {\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\tcfg.TikvImporter.Backend = \"tidb\"\n\tcfg.App.RegionConcurrency = 123\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\terr := cfg.Adjust(context.Background())\n\trequire.NoError(t, err)\n\trequire.Equal(t, 123, cfg.App.TableConcurrency)\n}\n\nfunc TestDefaultCouldBeOverwritten(t *testing.T) {\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\tcfg.TikvImporter.Backend = \"local\"\n\tcfg.App.IndexConcurrency = 20\n\tcfg.App.TableConcurrency = 60\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\terr := cfg.Adjust(context.Background())\n\trequire.NoError(t, err)\n\trequire.Equal(t, 20, cfg.App.IndexConcurrency)\n\trequire.Equal(t, 60, cfg.App.TableConcurrency)\n}\n\nfunc TestLoadFromInvalidConfig(t *testing.T) {\n\ttaskCfg := config.NewConfig()\n\terr := taskCfg.LoadFromGlobal(&config.GlobalConfig{\n\t\tConfigFileContent: []byte(\"invalid toml\"),\n\t})\n\trequire.Error(t, err)\n\trequire.Regexp(t, \"line 1.*\", err.Error())\n}\n\nfunc TestTomlPostRestore(t *testing.T) {\n\tcfg := &config.Config{}\n\terr := cfg.LoadFromTOML([]byte(`\n\t\t[post-restore]\n\t\tchecksum = \"req\"\n\t`))\n\trequire.EqualError(t, err, \"invalid op level 'req', please choose valid option between ['off', 'optional', 'required']\")\n\n\terr = cfg.LoadFromTOML([]byte(`\n\t\t[post-restore]\n\t\tanalyze = 123\n\t`))\n\trequire.EqualError(t, err, \"invalid op level '123', please choose valid option between ['off', 'optional', 'required']\")\n\n\tkvMap := map[string]config.PostOpLevel{\n\t\t`\"off\"`:      config.OpLevelOff,\n\t\t`\"required\"`: config.OpLevelRequired,\n\t\t`\"optional\"`: config.OpLevelOptional,\n\t\t\"true\":       config.OpLevelRequired,\n\t\t\"false\":      config.OpLevelOff,\n\t}\n\n\tvar b bytes.Buffer\n\tenc := toml.NewEncoder(&b)\n\n\tfor k, v := range kvMap {\n\t\tcfg := &config.Config{}\n\t\tconfStr := fmt.Sprintf(\"[post-restore]\\r\\nchecksum= %s\\r\\n\", k)\n\t\terr := cfg.LoadFromTOML([]byte(confStr))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, v, cfg.PostRestore.Checksum)\n\n\t\tb.Reset()\n\t\trequire.NoError(t, enc.Encode(cfg.PostRestore))\n\t\trequire.Regexp(t, fmt.Sprintf(`(?s).*checksum = \"\\Q%s\\E\".*`, v), &b)\n\t}\n\n\tfor k, v := range kvMap {\n\t\tcfg := &config.Config{}\n\t\tconfStr := fmt.Sprintf(\"[post-restore]\\r\\nanalyze= %s\\r\\n\", k)\n\t\terr := cfg.LoadFromTOML([]byte(confStr))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, v, cfg.PostRestore.Analyze)\n\n\t\tb.Reset()\n\t\trequire.NoError(t, enc.Encode(cfg.PostRestore))\n\t\trequire.Regexp(t, fmt.Sprintf(`(?s).*analyze = \"\\Q%s\\E\".*`, v), &b)\n\t}\n}\n\nfunc TestCronEncodeDecode(t *testing.T) {\n\tcfg := &config.Config{}\n\tcfg.Cron.SwitchMode.Duration = 1 * time.Minute\n\tcfg.Cron.LogProgress.Duration = 2 * time.Minute\n\tcfg.Cron.CheckDiskQuota.Duration = 3 * time.Second\n\tvar b bytes.Buffer\n\trequire.NoError(t, toml.NewEncoder(&b).Encode(cfg.Cron))\n\trequire.Equal(t, \"switch-mode = \\\"1m0s\\\"\\nlog-progress = \\\"2m0s\\\"\\ncheck-disk-quota = \\\"3s\\\"\\n\", b.String())\n\n\tconfStr := \"[cron]\\r\\n\" + b.String()\n\tcfg2 := &config.Config{}\n\trequire.NoError(t, cfg2.LoadFromTOML([]byte(confStr)))\n\trequire.Equal(t, cfg.Cron, cfg2.Cron)\n}\n\nfunc TestAdjustWithLegacyBlackWhiteList(t *testing.T) {\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\trequire.Equal(t, config.GetDefaultFilter(), cfg.Mydumper.Filter)\n\trequire.False(t, cfg.HasLegacyBlackWhiteList())\n\n\tctx := context.Background()\n\tcfg.Mydumper.Filter = []string{\"test.*\"}\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\trequire.NoError(t, cfg.Adjust(ctx))\n\trequire.False(t, cfg.HasLegacyBlackWhiteList())\n\n\tcfg.BWList.DoDBs = []string{\"test\"}\n\trequire.EqualError(t, cfg.Adjust(ctx), \"[Lightning:Config:ErrInvalidConfig]`mydumper.filter` and `black-white-list` cannot be simultaneously defined\")\n\n\tcfg.Mydumper.Filter = config.GetDefaultFilter()\n\trequire.NoError(t, cfg.Adjust(ctx))\n\trequire.True(t, cfg.HasLegacyBlackWhiteList())\n}\n\nfunc TestAdjustDiskQuota(t *testing.T) {\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\n\tbase := t.TempDir()\n\tctx := context.Background()\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.DiskQuota = 0\n\tcfg.TikvImporter.SortedKVDir = base\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\trequire.NoError(t, cfg.Adjust(ctx))\n\trequire.Equal(t, int64(0), int64(cfg.TikvImporter.DiskQuota))\n}\n\nfunc TestDataCharacterSet(t *testing.T) {\n\ttestCases := []struct {\n\t\tinput string\n\t\terr   string\n\t}{\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper]\n\t\t\t\tdata-character-set = 'binary'\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper]\n\t\t\t\tdata-character-set = 'utf8mb4'\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper]\n\t\t\t\tdata-character-set = 'gb18030'\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper]\n\t\t\t\tdata-invalid-char-replace = \"\\u2323\"\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper]\n\t\t\t\tdata-invalid-char-replace = \"a\"\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper]\n\t\t\t\tdata-invalid-char-replace = \"INV\"\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper]\n\t\t\t\tdata-invalid-char-replace = \"\ud83d\ude0a\"\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper]\n\t\t\t\tdata-invalid-char-replace = \"\ud83d\ude0a\ud83d\ude2d\ud83d\ude05\ud83d\ude04\"\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tcomment := fmt.Sprintf(\"input = %s\", tc.input)\n\t\tcfg := config.NewConfig()\n\t\tcfg.Mydumper.SourceDir = \"file://.\"\n\t\tcfg.TiDB.Port = 4000\n\t\tcfg.TiDB.PdAddr = \"test.invalid:2379\"\n\t\tcfg.TikvImporter.Backend = config.BackendLocal\n\t\tcfg.TikvImporter.SortedKVDir = \".\"\n\t\tcfg.TiDB.DistSQLScanConcurrency = 1\n\t\terr := cfg.LoadFromTOML([]byte(tc.input))\n\t\trequire.NoError(t, err)\n\t\terr = cfg.Adjust(context.Background())\n\t\tif tc.err != \"\" {\n\t\t\trequire.EqualError(t, err, tc.err, comment)\n\t\t} else {\n\t\t\trequire.NoError(t, err, comment)\n\t\t}\n\t}\n}\n\nfunc TestCheckpointKeepStrategy(t *testing.T) {\n\ttomlCases := map[interface{}]config.CheckpointKeepStrategy{\n\t\ttrue:     config.CheckpointRename,\n\t\tfalse:    config.CheckpointRemove,\n\t\t\"remove\": config.CheckpointRemove,\n\t\t\"rename\": config.CheckpointRename,\n\t\t\"origin\": config.CheckpointOrigin,\n\t}\n\tvar cp config.CheckpointKeepStrategy\n\tfor key, strategy := range tomlCases {\n\t\terr := cp.UnmarshalTOML(key)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, strategy, cp)\n\t}\n\n\tdefaultCp := \"enable = true\\r\\n\"\n\tcpCfg := &config.Checkpoint{}\n\t_, err := toml.Decode(defaultCp, cpCfg)\n\trequire.NoError(t, err)\n\trequire.Equal(t, config.CheckpointRemove, cpCfg.KeepAfterSuccess)\n\n\tcpFmt := \"keep-after-success = %v\\r\\n\"\n\tfor key, strategy := range tomlCases {\n\t\tcpValue := key\n\t\tif strVal, ok := key.(string); ok {\n\t\t\tcpValue = `\"` + strVal + `\"`\n\t\t}\n\t\ttomlStr := fmt.Sprintf(cpFmt, cpValue)\n\t\tcpCfg := &config.Checkpoint{}\n\t\t_, err := toml.Decode(tomlStr, cpCfg)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, strategy, cpCfg.KeepAfterSuccess)\n\t}\n\n\tmarshalTextCases := map[config.CheckpointKeepStrategy]string{\n\t\tconfig.CheckpointRemove: \"remove\",\n\t\tconfig.CheckpointRename: \"rename\",\n\t\tconfig.CheckpointOrigin: \"origin\",\n\t}\n\tfor strategy, value := range marshalTextCases {\n\t\tres, err := strategy.MarshalText()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []byte(value), res)\n\t}\n}\n\nfunc TestLoadCharsetFromConfig(t *testing.T) {\n\tcases := map[string]config.Charset{\n\t\t\"binary\":  config.Binary,\n\t\t\"BINARY\":  config.Binary,\n\t\t\"GBK\":     config.GBK,\n\t\t\"gbk\":     config.GBK,\n\t\t\"Gbk\":     config.GBK,\n\t\t\"gB18030\": config.GB18030,\n\t\t\"GB18030\": config.GB18030,\n\t}\n\tfor k, v := range cases {\n\t\tcharset, err := config.ParseCharset(k)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, v, charset)\n\t}\n\n\t_, err := config.ParseCharset(\"Unknown\")\n\trequire.EqualError(t, err, \"found unsupported data-character-set: Unknown\")\n}\n\nfunc TestCheckAndAdjustForLocalBackend(t *testing.T) {\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.SortedKVDir = \"\"\n\trequire.EqualError(t, cfg.CheckAndAdjustForLocalBackend(), \"[Lightning:Config:ErrInvalidConfig]tikv-importer.sorted-kv-dir must not be empty!\")\n\n\t// non exists dir is legal\n\tcfg.TikvImporter.SortedKVDir = \"./not-exists\"\n\trequire.NoError(t, cfg.CheckAndAdjustForLocalBackend())\n\n\tbase := t.TempDir()\n\t// create empty file\n\tfile := filepath.Join(base, \"file\")\n\trequire.NoError(t, os.WriteFile(file, []byte(\"\"), 0644))\n\tcfg.TikvImporter.SortedKVDir = file\n\terr := cfg.CheckAndAdjustForLocalBackend()\n\trequire.Error(t, err)\n\trequire.Regexp(t, \"tikv-importer.sorted-kv-dir (.*) is not a directory\", err.Error())\n\n\t// legal dir\n\tcfg.TikvImporter.SortedKVDir = base\n\trequire.NoError(t, cfg.CheckAndAdjustForLocalBackend())\n}\n\nfunc TestCreateSeveralConfigsWithDifferentFilters(t *testing.T) {\n\toriginalDefaultCfg := append([]string{}, config.GetDefaultFilter()...)\n\tcfg1 := config.NewConfig()\n\trequire.NoError(t, cfg1.LoadFromTOML([]byte(`\n\t\t[mydumper]\n\t\tfilter = [\"db1.tbl1\", \"db2.*\", \"!db2.tbl1\"]\n\t`)))\n\trequire.Equal(t, 3, len(cfg1.Mydumper.Filter))\n\trequire.True(t, common.StringSliceEqual(\n\t\tcfg1.Mydumper.Filter,\n\t\t[]string{\"db1.tbl1\", \"db2.*\", \"!db2.tbl1\"},\n\t))\n\trequire.True(t, common.StringSliceEqual(config.GetDefaultFilter(), originalDefaultCfg))\n\n\tcfg2 := config.NewConfig()\n\trequire.True(t, common.StringSliceEqual(\n\t\tcfg2.Mydumper.Filter,\n\t\toriginalDefaultCfg,\n\t))\n\trequire.True(t, common.StringSliceEqual(config.GetDefaultFilter(), originalDefaultCfg))\n\n\tgCfg1, err := config.LoadGlobalConfig([]string{\"-f\", \"db1.tbl1\", \"-f\", \"db2.*\", \"-f\", \"!db2.tbl1\"}, nil)\n\trequire.NoError(t, err)\n\trequire.True(t, common.StringSliceEqual(\n\t\tgCfg1.Mydumper.Filter,\n\t\t[]string{\"db1.tbl1\", \"db2.*\", \"!db2.tbl1\"},\n\t))\n\trequire.True(t, common.StringSliceEqual(config.GetDefaultFilter(), originalDefaultCfg))\n\n\tgCfg2, err := config.LoadGlobalConfig([]string{}, nil)\n\trequire.NoError(t, err)\n\trequire.True(t, common.StringSliceEqual(\n\t\tgCfg2.Mydumper.Filter,\n\t\toriginalDefaultCfg,\n\t))\n\trequire.True(t, common.StringSliceEqual(config.GetDefaultFilter(), originalDefaultCfg))\n}\n", "// Copyright 2016 PingCAP, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage main\n\nimport (\n\t\"database/sql\"\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n\t\"strconv\"\n\t\"strings\"\n\n\t_ \"github.com/go-sql-driver/mysql\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/log\"\n\t\"github.com/pingcap/tidb/parser/mysql\"\n\t\"go.uber.org/zap\"\n)\n\nfunc intRangeValue(column *column, min int64, max int64) (int64, int64) {\n\tvar err error\n\tif len(column.min) > 0 {\n\t\tmin, err = strconv.ParseInt(column.min, 10, 64)\n\t\tif err != nil {\n\t\t\tlog.Fatal(err.Error())\n\t\t}\n\n\t\tif len(column.max) > 0 {\n\t\t\tmax, err = strconv.ParseInt(column.max, 10, 64)\n\t\t\tif err != nil {\n\t\t\t\tlog.Fatal(err.Error())\n\t\t\t}\n\t\t}\n\t}\n\n\treturn min, max\n}\n\nfunc randStringValue(column *column, n int) string {\n\tif column.hist != nil {\n\t\tif column.hist.avgLen == 0 {\n\t\t\tcolumn.hist.avgLen = column.hist.getAvgLen(n)\n\t\t}\n\t\treturn column.hist.randString()\n\t}\n\tif len(column.set) > 0 {\n\t\tidx := randInt(0, len(column.set)-1)\n\t\treturn column.set[idx]\n\t}\n\treturn randString(randInt(1, n))\n}\n\nfunc randInt64Value(column *column, min int64, max int64) int64 {\n\tif column.hist != nil {\n\t\treturn column.hist.randInt()\n\t}\n\tif len(column.set) > 0 {\n\t\tidx := randInt(0, len(column.set)-1)\n\t\tdata, err := strconv.ParseInt(column.set[idx], 10, 64)\n\t\tif err != nil {\n\t\t\tlog.Warn(\"rand int64 failed\", zap.Error(err))\n\t\t}\n\t\treturn data\n\t}\n\n\tmin, max = intRangeValue(column, min, max)\n\treturn randInt64(min, max)\n}\n\nfunc nextInt64Value(column *column, min int64, max int64) int64 {\n\tmin, max = intRangeValue(column, min, max)\n\tcolumn.data.setInitInt64Value(min, max)\n\treturn column.data.nextInt64()\n}\n\nfunc intToDecimalString(intValue int64, decimal int) string {\n\tdata := strconv.FormatInt(intValue, 10)\n\n\t// add leading zero\n\tif len(data) < decimal {\n\t\tdata = strings.Repeat(\"0\", decimal-len(data)) + data\n\t}\n\n\tdec := data[len(data)-decimal:]\n\tif data = data[:len(data)-decimal]; data == \"\" {\n\t\tdata = \"0\"\n\t}\n\tif dec != \"\" {\n\t\tdata = data + \".\" + dec\n\t}\n\treturn data\n}\n\nfunc genRowDatas(table *table, count int) ([]string, error) {\n\tdatas := make([]string, 0, count)\n\tfor i := 0; i < count; i++ {\n\t\tdata, err := genRowData(table)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\t\tdatas = append(datas, data)\n\t}\n\n\treturn datas, nil\n}\n\nfunc genRowData(table *table) (string, error) {\n\tvar values []byte //nolint: prealloc\n\tfor _, column := range table.columns {\n\t\tdata, err := genColumnData(table, column)\n\t\tif err != nil {\n\t\t\treturn \"\", errors.Trace(err)\n\t\t}\n\t\tvalues = append(values, []byte(data)...)\n\t\tvalues = append(values, ',')\n\t}\n\n\tvalues = values[:len(values)-1]\n\tsql := fmt.Sprintf(\"insert into %s (%s) values (%s);\", table.name, table.columnList, string(values))\n\treturn sql, nil\n}\n\n// #nosec G404\nfunc genColumnData(table *table, column *column) (string, error) {\n\ttp := column.tp\n\tincremental := column.incremental\n\tif incremental {\n\t\tincremental = uint32(rand.Int31n(100))+1 <= column.data.probability\n\t\t// If incremental, there is only one worker, so it is safe to directly access datum.\n\t\tif !incremental && column.data.remains > 0 {\n\t\t\tcolumn.data.remains--\n\t\t}\n\t}\n\tif _, ok := table.uniqIndices[column.name]; ok {\n\t\tincremental = true\n\t}\n\tisUnsigned := mysql.HasUnsignedFlag(tp.GetFlag())\n\n\tswitch tp.GetType() {\n\tcase mysql.TypeTiny:\n\t\tvar data int64\n\t\tif incremental {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = nextInt64Value(column, 0, math.MaxUint8)\n\t\t\t} else {\n\t\t\t\tdata = nextInt64Value(column, math.MinInt8, math.MaxInt8)\n\t\t\t}\n\t\t} else {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = randInt64Value(column, 0, math.MaxUint8)\n\t\t\t} else {\n\t\t\t\tdata = randInt64Value(column, math.MinInt8, math.MaxInt8)\n\t\t\t}\n\t\t}\n\t\treturn strconv.FormatInt(data, 10), nil\n\tcase mysql.TypeShort:\n\t\tvar data int64\n\t\tif incremental {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = nextInt64Value(column, 0, math.MaxUint16)\n\t\t\t} else {\n\t\t\t\tdata = nextInt64Value(column, math.MinInt16, math.MaxInt16)\n\t\t\t}\n\t\t} else {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = randInt64Value(column, 0, math.MaxUint16)\n\t\t\t} else {\n\t\t\t\tdata = randInt64Value(column, math.MinInt16, math.MaxInt16)\n\t\t\t}\n\t\t}\n\t\treturn strconv.FormatInt(data, 10), nil\n\tcase mysql.TypeLong:\n\t\tvar data int64\n\t\tif incremental {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = nextInt64Value(column, 0, math.MaxUint32)\n\t\t\t} else {\n\t\t\t\tdata = nextInt64Value(column, math.MinInt32, math.MaxInt32)\n\t\t\t}\n\t\t} else {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = randInt64Value(column, 0, math.MaxUint32)\n\t\t\t} else {\n\t\t\t\tdata = randInt64Value(column, math.MinInt32, math.MaxInt32)\n\t\t\t}\n\t\t}\n\t\treturn strconv.FormatInt(data, 10), nil\n\tcase mysql.TypeLonglong:\n\t\tvar data int64\n\t\tif incremental {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = nextInt64Value(column, 0, math.MaxInt64-1)\n\t\t\t} else {\n\t\t\t\tdata = nextInt64Value(column, math.MinInt32, math.MaxInt32)\n\t\t\t}\n\t\t} else {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = randInt64Value(column, 0, math.MaxInt64-1)\n\t\t\t} else {\n\t\t\t\tdata = randInt64Value(column, math.MinInt32, math.MaxInt32)\n\t\t\t}\n\t\t}\n\t\treturn strconv.FormatInt(data, 10), nil\n\tcase mysql.TypeVarchar, mysql.TypeString, mysql.TypeTinyBlob, mysql.TypeBlob, mysql.TypeMediumBlob, mysql.TypeLongBlob:\n\t\tdata := []byte{'\\''}\n\t\tif incremental {\n\t\t\tdata = append(data, []byte(column.data.nextString(tp.GetFlen()))...)\n\t\t} else {\n\t\t\tdata = append(data, []byte(randStringValue(column, tp.GetFlen()))...)\n\t\t}\n\n\t\tdata = append(data, '\\'')\n\t\treturn string(data), nil\n\tcase mysql.TypeFloat, mysql.TypeDouble:\n\t\tvar data float64\n\t\tif incremental {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = float64(nextInt64Value(column, 0, math.MaxInt64-1))\n\t\t\t} else {\n\t\t\t\tdata = float64(nextInt64Value(column, math.MinInt32, math.MaxInt32))\n\t\t\t}\n\t\t} else {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = float64(randInt64Value(column, 0, math.MaxInt64-1))\n\t\t\t} else {\n\t\t\t\tdata = float64(randInt64Value(column, math.MinInt32, math.MaxInt32))\n\t\t\t}\n\t\t}\n\t\treturn strconv.FormatFloat(data, 'f', -1, 64), nil\n\tcase mysql.TypeDate:\n\t\tdata := []byte{'\\''}\n\t\tif incremental {\n\t\t\tdata = append(data, []byte(column.data.nextDate())...)\n\t\t} else {\n\t\t\tdata = append(data, []byte(randDate(column))...)\n\t\t}\n\n\t\tdata = append(data, '\\'')\n\t\treturn string(data), nil\n\tcase mysql.TypeDatetime, mysql.TypeTimestamp:\n\t\tdata := []byte{'\\''}\n\t\tif incremental {\n\t\t\tdata = append(data, []byte(column.data.nextTimestamp())...)\n\t\t} else {\n\t\t\tdata = append(data, []byte(randTimestamp(column))...)\n\t\t}\n\n\t\tdata = append(data, '\\'')\n\t\treturn string(data), nil\n\tcase mysql.TypeDuration:\n\t\tdata := []byte{'\\''}\n\t\tif incremental {\n\t\t\tdata = append(data, []byte(column.data.nextTime())...)\n\t\t} else {\n\t\t\tdata = append(data, []byte(randTime(column))...)\n\t\t}\n\n\t\tdata = append(data, '\\'')\n\t\treturn string(data), nil\n\tcase mysql.TypeYear:\n\t\tdata := []byte{'\\''}\n\t\tif incremental {\n\t\t\tdata = append(data, []byte(column.data.nextYear())...)\n\t\t} else {\n\t\t\tdata = append(data, []byte(randYear(column))...)\n\t\t}\n\n\t\tdata = append(data, '\\'')\n\t\treturn string(data), nil\n\tcase mysql.TypeNewDecimal:\n\t\tvar limit = int64(math.Pow10(tp.GetFlen()))\n\t\tvar intVal int64\n\t\tif limit < 0 {\n\t\t\tlimit = math.MaxInt64\n\t\t}\n\t\tif incremental {\n\t\t\tif isUnsigned {\n\t\t\t\tintVal = nextInt64Value(column, 0, limit-1)\n\t\t\t} else {\n\t\t\t\tintVal = nextInt64Value(column, (-limit+1)/2, (limit-1)/2)\n\t\t\t}\n\t\t} else {\n\t\t\tif isUnsigned {\n\t\t\t\tintVal = randInt64Value(column, 0, limit-1)\n\t\t\t} else {\n\t\t\t\tintVal = randInt64Value(column, (-limit+1)/2, (limit-1)/2)\n\t\t\t}\n\t\t}\n\t\treturn intToDecimalString(intVal, tp.GetDecimal()), nil\n\tdefault:\n\t\treturn \"\", errors.Errorf(\"unsupported column type - %v\", column)\n\t}\n}\n\nfunc execSQL(db *sql.DB, sql string) error {\n\tif sql == \"\" {\n\t\treturn nil\n\t}\n\n\t_, err := db.Exec(sql)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\treturn nil\n}\n\nfunc createDB(cfg DBConfig) (*sql.DB, error) {\n\tdbDSN := fmt.Sprintf(\"%s:%s@tcp(%s:%d)/%s?charset=utf8\", cfg.User, cfg.Password, cfg.Host, cfg.Port, cfg.Name)\n\tdb, err := sql.Open(\"mysql\", dbDSN)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\treturn db, nil\n}\n\nfunc closeDB(db *sql.DB) error {\n\treturn errors.Trace(db.Close())\n}\n\nfunc createDBs(cfg DBConfig, count int) ([]*sql.DB, error) {\n\tdbs := make([]*sql.DB, 0, count)\n\tfor i := 0; i < count; i++ {\n\t\tdb, err := createDB(cfg)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\n\t\tdbs = append(dbs, db)\n\t}\n\n\treturn dbs, nil\n}\n\nfunc closeDBs(dbs []*sql.DB) {\n\tfor _, db := range dbs {\n\t\terr := closeDB(db)\n\t\tif err != nil {\n\t\t\tlog.Error(\"close DB failed\", zap.Error(err))\n\t\t}\n\t}\n}\n", "// Copyright 2020 PingCAP, Inc. Licensed under Apache-2.0.\n\npackage export\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"net\"\n\t\"strconv\"\n\t\"strings\"\n\t\"text/template\"\n\t\"time\"\n\n\t\"github.com/coreos/go-semver/semver\"\n\t\"github.com/docker/go-units\"\n\t\"github.com/go-sql-driver/mysql\"\n\t\"github.com/google/uuid\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/tidb/br/pkg/storage\"\n\t\"github.com/pingcap/tidb/br/pkg/version\"\n\t\"github.com/pingcap/tidb/util\"\n\t\"github.com/pingcap/tidb/util/promutil\"\n\tfilter \"github.com/pingcap/tidb/util/table-filter\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/spf13/pflag\"\n\t\"go.uber.org/zap\"\n)\n\nconst (\n\tflagDatabase                 = \"database\"\n\tflagTablesList               = \"tables-list\"\n\tflagHost                     = \"host\"\n\tflagUser                     = \"user\"\n\tflagPort                     = \"port\"\n\tflagPassword                 = \"password\"\n\tflagAllowCleartextPasswords  = \"allow-cleartext-passwords\"\n\tflagThreads                  = \"threads\"\n\tflagFilesize                 = \"filesize\"\n\tflagStatementSize            = \"statement-size\"\n\tflagOutput                   = \"output\"\n\tflagLoglevel                 = \"loglevel\"\n\tflagLogfile                  = \"logfile\"\n\tflagLogfmt                   = \"logfmt\"\n\tflagConsistency              = \"consistency\"\n\tflagSnapshot                 = \"snapshot\"\n\tflagNoViews                  = \"no-views\"\n\tflagNoSequences              = \"no-sequences\"\n\tflagSortByPk                 = \"order-by-primary-key\"\n\tflagStatusAddr               = \"status-addr\"\n\tflagRows                     = \"rows\"\n\tflagWhere                    = \"where\"\n\tflagEscapeBackslash          = \"escape-backslash\"\n\tflagFiletype                 = \"filetype\"\n\tflagNoHeader                 = \"no-header\"\n\tflagNoSchemas                = \"no-schemas\"\n\tflagNoData                   = \"no-data\"\n\tflagCsvNullValue             = \"csv-null-value\"\n\tflagSQL                      = \"sql\"\n\tflagFilter                   = \"filter\"\n\tflagCaseSensitive            = \"case-sensitive\"\n\tflagDumpEmptyDatabase        = \"dump-empty-database\"\n\tflagTidbMemQuotaQuery        = \"tidb-mem-quota-query\"\n\tflagCA                       = \"ca\"\n\tflagCert                     = \"cert\"\n\tflagKey                      = \"key\"\n\tflagCsvSeparator             = \"csv-separator\"\n\tflagCsvDelimiter             = \"csv-delimiter\"\n\tflagOutputFilenameTemplate   = \"output-filename-template\"\n\tflagCompleteInsert           = \"complete-insert\"\n\tflagParams                   = \"params\"\n\tflagReadTimeout              = \"read-timeout\"\n\tflagTransactionalConsistency = \"transactional-consistency\"\n\tflagCompress                 = \"compress\"\n\n\t// FlagHelp represents the help flag\n\tFlagHelp = \"help\"\n)\n\n// Config is the dump config for dumpling\ntype Config struct {\n\tstorage.BackendOptions\n\n\tspecifiedTables          bool\n\tAllowCleartextPasswords  bool\n\tSortByPk                 bool\n\tNoViews                  bool\n\tNoSequences              bool\n\tNoHeader                 bool\n\tNoSchemas                bool\n\tNoData                   bool\n\tCompleteInsert           bool\n\tTransactionalConsistency bool\n\tEscapeBackslash          bool\n\tDumpEmptyDatabase        bool\n\tPosAfterConnect          bool\n\tCompressType             storage.CompressType\n\n\tHost     string\n\tPort     int\n\tThreads  int\n\tUser     string\n\tPassword string `json:\"-\"`\n\tSecurity struct {\n\t\tDriveTLSName string `json:\"-\"`\n\t\tCAPath       string\n\t\tCertPath     string\n\t\tKeyPath      string\n\t\tSSLCABytes   []byte `json:\"-\"`\n\t\tSSLCertBytes []byte `json:\"-\"`\n\t\tSSLKeyBytes  []byte `json:\"-\"`\n\t}\n\n\tLogLevel      string\n\tLogFile       string\n\tLogFormat     string\n\tOutputDirPath string\n\tStatusAddr    string\n\tSnapshot      string\n\tConsistency   string\n\tCsvNullValue  string\n\tSQL           string\n\tCsvSeparator  string\n\tCsvDelimiter  string\n\tDatabases     []string\n\n\tTableFilter         filter.Filter `json:\"-\"`\n\tWhere               string\n\tFileType            string\n\tServerInfo          version.ServerInfo\n\tLogger              *zap.Logger        `json:\"-\"`\n\tOutputFileTemplate  *template.Template `json:\"-\"`\n\tRows                uint64\n\tReadTimeout         time.Duration\n\tTiDBMemQuotaQuery   uint64\n\tFileSize            uint64\n\tStatementSize       uint64\n\tSessionParams       map[string]interface{}\n\tTables              DatabaseTables\n\tCollationCompatible string\n\n\tLabels       prometheus.Labels       `json:\"-\"`\n\tPromFactory  promutil.Factory        `json:\"-\"`\n\tPromRegistry promutil.Registry       `json:\"-\"`\n\tExtStorage   storage.ExternalStorage `json:\"-\"`\n}\n\n// ServerInfoUnknown is the unknown database type to dumpling\nvar ServerInfoUnknown = version.ServerInfo{\n\tServerType:    version.ServerTypeUnknown,\n\tServerVersion: nil,\n}\n\n// DefaultConfig returns the default export Config for dumpling\nfunc DefaultConfig() *Config {\n\tallFilter, _ := filter.Parse([]string{\"*.*\"})\n\treturn &Config{\n\t\tDatabases:           nil,\n\t\tHost:                \"127.0.0.1\",\n\t\tUser:                \"root\",\n\t\tPort:                3306,\n\t\tPassword:            \"\",\n\t\tThreads:             4,\n\t\tLogger:              nil,\n\t\tStatusAddr:          \":8281\",\n\t\tFileSize:            UnspecifiedSize,\n\t\tStatementSize:       DefaultStatementSize,\n\t\tOutputDirPath:       \".\",\n\t\tServerInfo:          ServerInfoUnknown,\n\t\tSortByPk:            true,\n\t\tTables:              nil,\n\t\tSnapshot:            \"\",\n\t\tConsistency:         ConsistencyTypeAuto,\n\t\tNoViews:             true,\n\t\tNoSequences:         true,\n\t\tRows:                UnspecifiedSize,\n\t\tWhere:               \"\",\n\t\tFileType:            \"\",\n\t\tNoHeader:            false,\n\t\tNoSchemas:           false,\n\t\tNoData:              false,\n\t\tCsvNullValue:        \"\\\\N\",\n\t\tSQL:                 \"\",\n\t\tTableFilter:         allFilter,\n\t\tDumpEmptyDatabase:   true,\n\t\tSessionParams:       make(map[string]interface{}),\n\t\tOutputFileTemplate:  DefaultOutputFileTemplate,\n\t\tPosAfterConnect:     false,\n\t\tCollationCompatible: LooseCollationCompatible,\n\t\tspecifiedTables:     false,\n\t\tPromFactory:         promutil.NewDefaultFactory(),\n\t\tPromRegistry:        promutil.NewDefaultRegistry(),\n\t}\n}\n\n// String returns dumpling's config in json format\nfunc (conf *Config) String() string {\n\tcfg, err := json.Marshal(conf)\n\tif err != nil && conf.Logger != nil {\n\t\tconf.Logger.Error(\"fail to marshal config to json\", zap.Error(err))\n\t}\n\treturn string(cfg)\n}\n\n// GetDSN generates DSN from Config\nfunc (conf *Config) GetDSN(db string) string {\n\t// maxAllowedPacket=0 can be used to automatically fetch the max_allowed_packet variable from server on every connection.\n\t// https://github.com/go-sql-driver/mysql#maxallowedpacket\n\thostPort := net.JoinHostPort(conf.Host, strconv.Itoa(conf.Port))\n\tdsn := fmt.Sprintf(\"%s:%s@tcp(%s)/%s?collation=utf8mb4_general_ci&readTimeout=%s&writeTimeout=30s&interpolateParams=true&maxAllowedPacket=0\",\n\t\tconf.User, conf.Password, hostPort, db, conf.ReadTimeout)\n\tif conf.Security.DriveTLSName != \"\" {\n\t\tdsn += \"&tls=\" + conf.Security.DriveTLSName\n\t}\n\tif conf.AllowCleartextPasswords {\n\t\tdsn += \"&allowCleartextPasswords=1\"\n\t}\n\treturn dsn\n}\n\nfunc timestampDirName() string {\n\treturn fmt.Sprintf(\"./export-%s\", time.Now().Format(time.RFC3339))\n}\n\n// DefineFlags defines flags of dumpling's configuration\nfunc (*Config) DefineFlags(flags *pflag.FlagSet) {\n\tstorage.DefineFlags(flags)\n\tflags.StringSliceP(flagDatabase, \"B\", nil, \"Databases to dump\")\n\tflags.StringSliceP(flagTablesList, \"T\", nil, \"Comma delimited table list to dump; must be qualified table names\")\n\tflags.StringP(flagHost, \"h\", \"127.0.0.1\", \"The host to connect to\")\n\tflags.StringP(flagUser, \"u\", \"root\", \"Username with privileges to run the dump\")\n\tflags.IntP(flagPort, \"P\", 4000, \"TCP/IP port to connect to\")\n\tflags.StringP(flagPassword, \"p\", \"\", \"User password\")\n\tflags.Bool(flagAllowCleartextPasswords, false, \"Allow passwords to be sent in cleartext (warning: don't use without TLS)\")\n\tflags.IntP(flagThreads, \"t\", 4, \"Number of goroutines to use, default 4\")\n\tflags.StringP(flagFilesize, \"F\", \"\", \"The approximate size of output file\")\n\tflags.Uint64P(flagStatementSize, \"s\", DefaultStatementSize, \"Attempted size of INSERT statement in bytes\")\n\tflags.StringP(flagOutput, \"o\", timestampDirName(), \"Output directory\")\n\tflags.String(flagLoglevel, \"info\", \"Log level: {debug|info|warn|error|dpanic|panic|fatal}\")\n\tflags.StringP(flagLogfile, \"L\", \"\", \"Log file `path`, leave empty to write to console\")\n\tflags.String(flagLogfmt, \"text\", \"Log `format`: {text|json}\")\n\tflags.String(flagConsistency, ConsistencyTypeAuto, \"Consistency level during dumping: {auto|none|flush|lock|snapshot}\")\n\tflags.String(flagSnapshot, \"\", \"Snapshot position (uint64 or MySQL style string timestamp). Valid only when consistency=snapshot\")\n\tflags.BoolP(flagNoViews, \"W\", true, \"Do not dump views\")\n\tflags.Bool(flagNoSequences, true, \"Do not dump sequences\")\n\tflags.Bool(flagSortByPk, true, \"Sort dump results by primary key through order by sql\")\n\tflags.String(flagStatusAddr, \":8281\", \"dumpling API server and pprof addr\")\n\tflags.Uint64P(flagRows, \"r\", UnspecifiedSize, \"If specified, dumpling will split table into chunks and concurrently dump them to different files to improve efficiency. For TiDB v3.0+, specify this will make dumpling split table with each file one TiDB region(no matter how many rows is).\\n\"+\n\t\t\"If not specified, dumpling will dump table without inner-concurrency which could be relatively slow. default unlimited\")\n\tflags.String(flagWhere, \"\", \"Dump only selected records\")\n\tflags.Bool(flagEscapeBackslash, true, \"use backslash to escape special characters\")\n\tflags.String(flagFiletype, \"\", \"The type of export file (sql/csv)\")\n\tflags.Bool(flagNoHeader, false, \"whether not to dump CSV table header\")\n\tflags.BoolP(flagNoSchemas, \"m\", false, \"Do not dump table schemas with the data\")\n\tflags.BoolP(flagNoData, \"d\", false, \"Do not dump table data\")\n\tflags.String(flagCsvNullValue, \"\\\\N\", \"The null value used when export to csv\")\n\tflags.StringP(flagSQL, \"S\", \"\", \"Dump data with given sql. This argument doesn't support concurrent dump\")\n\t_ = flags.MarkHidden(flagSQL)\n\tflags.StringSliceP(flagFilter, \"f\", []string{\"*.*\", DefaultTableFilter}, \"filter to select which tables to dump\")\n\tflags.Bool(flagCaseSensitive, false, \"whether the filter should be case-sensitive\")\n\tflags.Bool(flagDumpEmptyDatabase, true, \"whether to dump empty database\")\n\tflags.Uint64(flagTidbMemQuotaQuery, UnspecifiedSize, \"The maximum memory limit for a single SQL statement, in bytes.\")\n\tflags.String(flagCA, \"\", \"The path name to the certificate authority file for TLS connection\")\n\tflags.String(flagCert, \"\", \"The path name to the client certificate file for TLS connection\")\n\tflags.String(flagKey, \"\", \"The path name to the client private key file for TLS connection\")\n\tflags.String(flagCsvSeparator, \",\", \"The separator for csv files, default ','\")\n\tflags.String(flagCsvDelimiter, \"\\\"\", \"The delimiter for values in csv files, default '\\\"'\")\n\tflags.String(flagOutputFilenameTemplate, \"\", \"The output filename template (without file extension)\")\n\tflags.Bool(flagCompleteInsert, false, \"Use complete INSERT statements that include column names\")\n\tflags.StringToString(flagParams, nil, `Extra session variables used while dumping, accepted format: --params \"character_set_client=latin1,character_set_connection=latin1\"`)\n\tflags.Bool(FlagHelp, false, \"Print help message and quit\")\n\tflags.Duration(flagReadTimeout, 15*time.Minute, \"I/O read timeout for db connection.\")\n\t_ = flags.MarkHidden(flagReadTimeout)\n\tflags.Bool(flagTransactionalConsistency, true, \"Only support transactional consistency\")\n\t_ = flags.MarkHidden(flagTransactionalConsistency)\n\tflags.StringP(flagCompress, \"c\", \"\", \"Compress output file type, support 'gzip', 'no-compression' now\")\n}\n\n// ParseFromFlags parses dumpling's export.Config from flags\n// nolint: gocyclo\nfunc (conf *Config) ParseFromFlags(flags *pflag.FlagSet) error {\n\tvar err error\n\tconf.Databases, err = flags.GetStringSlice(flagDatabase)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Host, err = flags.GetString(flagHost)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.User, err = flags.GetString(flagUser)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Port, err = flags.GetInt(flagPort)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Password, err = flags.GetString(flagPassword)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.AllowCleartextPasswords, err = flags.GetBool(flagAllowCleartextPasswords)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Threads, err = flags.GetInt(flagThreads)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.StatementSize, err = flags.GetUint64(flagStatementSize)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.OutputDirPath, err = flags.GetString(flagOutput)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.LogLevel, err = flags.GetString(flagLoglevel)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.LogFile, err = flags.GetString(flagLogfile)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.LogFormat, err = flags.GetString(flagLogfmt)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Consistency, err = flags.GetString(flagConsistency)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Snapshot, err = flags.GetString(flagSnapshot)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.NoViews, err = flags.GetBool(flagNoViews)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.NoSequences, err = flags.GetBool(flagNoSequences)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.SortByPk, err = flags.GetBool(flagSortByPk)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.StatusAddr, err = flags.GetString(flagStatusAddr)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Rows, err = flags.GetUint64(flagRows)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Where, err = flags.GetString(flagWhere)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.EscapeBackslash, err = flags.GetBool(flagEscapeBackslash)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.FileType, err = flags.GetString(flagFiletype)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.NoHeader, err = flags.GetBool(flagNoHeader)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.NoSchemas, err = flags.GetBool(flagNoSchemas)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.NoData, err = flags.GetBool(flagNoData)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.CsvNullValue, err = flags.GetString(flagCsvNullValue)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.SQL, err = flags.GetString(flagSQL)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.DumpEmptyDatabase, err = flags.GetBool(flagDumpEmptyDatabase)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Security.CAPath, err = flags.GetString(flagCA)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Security.CertPath, err = flags.GetString(flagCert)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Security.KeyPath, err = flags.GetString(flagKey)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.CsvSeparator, err = flags.GetString(flagCsvSeparator)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.CsvDelimiter, err = flags.GetString(flagCsvDelimiter)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.CompleteInsert, err = flags.GetBool(flagCompleteInsert)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.ReadTimeout, err = flags.GetDuration(flagReadTimeout)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.TransactionalConsistency, err = flags.GetBool(flagTransactionalConsistency)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.TiDBMemQuotaQuery, err = flags.GetUint64(flagTidbMemQuotaQuery)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tif conf.Threads <= 0 {\n\t\treturn errors.Errorf(\"--threads is set to %d. It should be greater than 0\", conf.Threads)\n\t}\n\tif len(conf.CsvSeparator) == 0 {\n\t\treturn errors.New(\"--csv-separator is set to \\\"\\\". It must not be an empty string\")\n\t}\n\n\tif conf.SessionParams == nil {\n\t\tconf.SessionParams = make(map[string]interface{})\n\t}\n\n\ttablesList, err := flags.GetStringSlice(flagTablesList)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tfileSizeStr, err := flags.GetString(flagFilesize)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tfilters, err := flags.GetStringSlice(flagFilter)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tcaseSensitive, err := flags.GetBool(flagCaseSensitive)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\toutputFilenameFormat, err := flags.GetString(flagOutputFilenameTemplate)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tparams, err := flags.GetStringToString(flagParams)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tconf.specifiedTables = len(tablesList) > 0\n\tconf.Tables, err = GetConfTables(tablesList)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tconf.TableFilter, err = ParseTableFilter(tablesList, filters)\n\tif err != nil {\n\t\treturn errors.Errorf(\"failed to parse filter: %s\", err)\n\t}\n\n\tif !caseSensitive {\n\t\tconf.TableFilter = filter.CaseInsensitive(conf.TableFilter)\n\t}\n\n\tconf.FileSize, err = ParseFileSize(fileSizeStr)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tif outputFilenameFormat == \"\" && conf.SQL != \"\" {\n\t\toutputFilenameFormat = DefaultAnonymousOutputFileTemplateText\n\t}\n\ttmpl, err := ParseOutputFileTemplate(outputFilenameFormat)\n\tif err != nil {\n\t\treturn errors.Errorf(\"failed to parse output filename template (--output-filename-template '%s')\", outputFilenameFormat)\n\t}\n\tconf.OutputFileTemplate = tmpl\n\n\tcompressType, err := flags.GetString(flagCompress)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.CompressType, err = ParseCompressType(compressType)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tfor k, v := range params {\n\t\tconf.SessionParams[k] = v\n\t}\n\n\terr = conf.BackendOptions.ParseFromFlags(pflag.CommandLine)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\treturn nil\n}\n\n// ParseFileSize parses file size from tables-list and filter arguments\nfunc ParseFileSize(fileSizeStr string) (uint64, error) {\n\tif len(fileSizeStr) == 0 {\n\t\treturn UnspecifiedSize, nil\n\t} else if fileSizeMB, err := strconv.ParseUint(fileSizeStr, 10, 64); err == nil {\n\t\tfmt.Printf(\"Warning: -F without unit is not recommended, try using `-F '%dMiB'` in the future\\n\", fileSizeMB)\n\t\treturn fileSizeMB * units.MiB, nil\n\t} else if size, err := units.RAMInBytes(fileSizeStr); err == nil {\n\t\treturn uint64(size), nil\n\t}\n\treturn 0, errors.Errorf(\"failed to parse filesize (-F '%s')\", fileSizeStr)\n}\n\n// ParseTableFilter parses table filter from tables-list and filter arguments\nfunc ParseTableFilter(tablesList, filters []string) (filter.Filter, error) {\n\tif len(tablesList) == 0 {\n\t\treturn filter.Parse(filters)\n\t}\n\n\t// only parse -T when -f is default value. otherwise bail out.\n\tif !sameStringArray(filters, []string{\"*.*\", DefaultTableFilter}) {\n\t\treturn nil, errors.New(\"cannot pass --tables-list and --filter together\")\n\t}\n\n\ttableNames := make([]filter.Table, 0, len(tablesList))\n\tfor _, table := range tablesList {\n\t\tparts := strings.SplitN(table, \".\", 2)\n\t\tif len(parts) < 2 {\n\t\t\treturn nil, errors.Errorf(\"--tables-list only accepts qualified table names, but `%s` lacks a dot\", table)\n\t\t}\n\t\ttableNames = append(tableNames, filter.Table{Schema: parts[0], Name: parts[1]})\n\t}\n\n\treturn filter.NewTablesFilter(tableNames...), nil\n}\n\n// GetConfTables parses tables from tables-list and filter arguments\nfunc GetConfTables(tablesList []string) (DatabaseTables, error) {\n\tdbTables := DatabaseTables{}\n\tvar (\n\t\ttablename    string\n\t\tavgRowLength uint64\n\t)\n\tavgRowLength = 0\n\tfor _, tablename = range tablesList {\n\t\tparts := strings.SplitN(tablename, \".\", 2)\n\t\tif len(parts) < 2 {\n\t\t\treturn nil, errors.Errorf(\"--tables-list only accepts qualified table names, but `%s` lacks a dot\", tablename)\n\t\t}\n\t\tdbName := parts[0]\n\t\ttbName := parts[1]\n\t\tdbTables[dbName] = append(dbTables[dbName], &TableInfo{tbName, avgRowLength, TableTypeBase})\n\t}\n\treturn dbTables, nil\n}\n\n// ParseCompressType parses compressType string to storage.CompressType\nfunc ParseCompressType(compressType string) (storage.CompressType, error) {\n\tswitch compressType {\n\tcase \"\", \"no-compression\":\n\t\treturn storage.NoCompression, nil\n\tcase \"gzip\", \"gz\":\n\t\treturn storage.Gzip, nil\n\tdefault:\n\t\treturn storage.NoCompression, errors.Errorf(\"unknown compress type %s\", compressType)\n\t}\n}\n\nfunc (conf *Config) createExternalStorage(ctx context.Context) (storage.ExternalStorage, error) {\n\tif conf.ExtStorage != nil {\n\t\treturn conf.ExtStorage, nil\n\t}\n\tb, err := storage.ParseBackend(conf.OutputDirPath, &conf.BackendOptions)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\t// TODO: support setting httpClient with certification later\n\treturn storage.New(ctx, b, &storage.ExternalStorageOptions{})\n}\n\nconst (\n\t// UnspecifiedSize means the filesize/statement-size is unspecified\n\tUnspecifiedSize = 0\n\t// DefaultStatementSize is the default statement size\n\tDefaultStatementSize = 1000000\n\t// TiDBMemQuotaQueryName is the session variable TiDBMemQuotaQuery's name in TiDB\n\tTiDBMemQuotaQueryName = \"tidb_mem_quota_query\"\n\t// DefaultTableFilter is the default exclude table filter. It will exclude all system databases\n\tDefaultTableFilter = \"!/^(mysql|sys|INFORMATION_SCHEMA|PERFORMANCE_SCHEMA|METRICS_SCHEMA|INSPECTION_SCHEMA)$/.*\"\n\n\tdefaultDumpThreads        = 128\n\tdefaultDumpGCSafePointTTL = 5 * 60\n\tdefaultEtcdDialTimeOut    = 3 * time.Second\n\n\t// LooseCollationCompatible is used in DM, represents a collation setting for best compatibility.\n\tLooseCollationCompatible = \"loose\"\n\t// StrictCollationCompatible is used in DM, represents a collation setting for correctness.\n\tStrictCollationCompatible = \"strict\"\n\n\tdumplingServiceSafePointPrefix = \"dumpling\"\n)\n\nvar (\n\tdecodeRegionVersion = semver.New(\"3.0.0\")\n\tgcSafePointVersion  = semver.New(\"4.0.0\")\n\ttableSampleVersion  = semver.New(\"5.0.0-nightly\")\n)\n\nfunc adjustConfig(conf *Config, fns ...func(*Config) error) error {\n\tfor _, f := range fns {\n\t\terr := f(conf)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc registerTLSConfig(conf *Config) error {\n\ttlsConfig, err := util.NewTLSConfig(\n\t\tutil.WithCAPath(conf.Security.CAPath),\n\t\tutil.WithCertAndKeyPath(conf.Security.CertPath, conf.Security.KeyPath),\n\t\tutil.WithCAContent(conf.Security.SSLCABytes),\n\t\tutil.WithCertAndKeyContent(conf.Security.SSLCertBytes, conf.Security.SSLKeyBytes),\n\t)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tif tlsConfig == nil {\n\t\treturn nil\n\t}\n\n\tconf.Security.DriveTLSName = \"dumpling\" + uuid.NewString()\n\terr = mysql.RegisterTLSConfig(conf.Security.DriveTLSName, tlsConfig)\n\treturn errors.Trace(err)\n}\n\nfunc validateSpecifiedSQL(conf *Config) error {\n\tif conf.SQL != \"\" && conf.Where != \"\" {\n\t\treturn errors.New(\"can't specify both --sql and --where at the same time. Please try to combine them into --sql\")\n\t}\n\treturn nil\n}\n\nfunc adjustFileFormat(conf *Config) error {\n\tconf.FileType = strings.ToLower(conf.FileType)\n\tswitch conf.FileType {\n\tcase \"\":\n\t\tif conf.SQL != \"\" {\n\t\t\tconf.FileType = FileFormatCSVString\n\t\t} else {\n\t\t\tconf.FileType = FileFormatSQLTextString\n\t\t}\n\tcase FileFormatSQLTextString:\n\t\tif conf.SQL != \"\" {\n\t\t\treturn errors.Errorf(\"unsupported config.FileType '%s' when we specify --sql, please unset --filetype or set it to 'csv'\", conf.FileType)\n\t\t}\n\tcase FileFormatCSVString:\n\tdefault:\n\t\treturn errors.Errorf(\"unknown config.FileType '%s'\", conf.FileType)\n\t}\n\treturn nil\n}\n\nfunc matchMysqlBugversion(info version.ServerInfo) bool {\n\t// if 8.0.3 <= mysql8 version < 8.0.23\n\t// FLUSH TABLES WITH READ LOCK could block other sessions from executing SHOW TABLE STATUS.\n\t// see more in https://dev.mysql.com/doc/relnotes/mysql/8.0/en/news-8-0-23.html\n\tif info.ServerType != version.ServerTypeMySQL {\n\t\treturn false\n\t}\n\tcurrentVersion := info.ServerVersion\n\tbugVersionStart := semver.New(\"8.0.2\")\n\tbugVersionEnd := semver.New(\"8.0.23\")\n\treturn bugVersionStart.LessThan(*currentVersion) && currentVersion.LessThan(*bugVersionEnd)\n}\n", "// Copyright 2020 PingCAP, Inc. Licensed under Apache-2.0.\n\npackage export\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"math/big\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t// import mysql driver\n\t\"github.com/go-sql-driver/mysql\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/failpoint\"\n\tpclog \"github.com/pingcap/log\"\n\t\"github.com/pingcap/tidb/br/pkg/storage\"\n\t\"github.com/pingcap/tidb/br/pkg/summary\"\n\t\"github.com/pingcap/tidb/br/pkg/version\"\n\t\"github.com/pingcap/tidb/dumpling/cli\"\n\ttcontext \"github.com/pingcap/tidb/dumpling/context\"\n\t\"github.com/pingcap/tidb/dumpling/log\"\n\t\"github.com/pingcap/tidb/parser\"\n\t\"github.com/pingcap/tidb/parser/ast\"\n\t\"github.com/pingcap/tidb/parser/format\"\n\t\"github.com/pingcap/tidb/store/helper\"\n\t\"github.com/pingcap/tidb/tablecodec\"\n\t\"github.com/pingcap/tidb/util/codec\"\n\tpd \"github.com/tikv/pd/client\"\n\t\"go.uber.org/zap\"\n\t\"golang.org/x/exp/slices\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\nvar openDBFunc = sql.Open\n\nvar errEmptyHandleVals = errors.New(\"empty handleVals for TiDB table\")\n\n// Dumper is the dump progress structure\ntype Dumper struct {\n\ttctx      *tcontext.Context\n\tcancelCtx context.CancelFunc\n\tconf      *Config\n\tmetrics   *metrics\n\n\textStore storage.ExternalStorage\n\tdbHandle *sql.DB\n\n\ttidbPDClientForGC             pd.Client\n\tselectTiDBTableRegionFunc     func(tctx *tcontext.Context, conn *BaseConn, meta TableMeta) (pkFields []string, pkVals [][]string, err error)\n\ttotalTables                   int64\n\tcharsetAndDefaultCollationMap map[string]string\n\n\tspeedRecorder *SpeedRecorder\n}\n\n// NewDumper returns a new Dumper\nfunc NewDumper(ctx context.Context, conf *Config) (*Dumper, error) {\n\tfailpoint.Inject(\"setExtStorage\", func(val failpoint.Value) {\n\t\tpath := val.(string)\n\t\tb, err := storage.ParseBackend(path, nil)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\ts, err := storage.New(context.Background(), b, &storage.ExternalStorageOptions{})\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tconf.ExtStorage = s\n\t})\n\n\ttctx, cancelFn := tcontext.Background().WithContext(ctx).WithCancel()\n\td := &Dumper{\n\t\ttctx:                      tctx,\n\t\tconf:                      conf,\n\t\tcancelCtx:                 cancelFn,\n\t\tselectTiDBTableRegionFunc: selectTiDBTableRegion,\n\t\tspeedRecorder:             NewSpeedRecorder(),\n\t}\n\n\tvar err error\n\n\td.metrics = newMetrics(conf.PromFactory, conf.Labels)\n\td.metrics.registerTo(conf.PromRegistry)\n\tdefer func() {\n\t\tif err != nil {\n\t\t\td.metrics.unregisterFrom(conf.PromRegistry)\n\t\t}\n\t}()\n\n\terr = adjustConfig(conf,\n\t\tregisterTLSConfig,\n\t\tvalidateSpecifiedSQL,\n\t\tadjustFileFormat)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\terr = runSteps(d,\n\t\tinitLogger,\n\t\tcreateExternalStore,\n\t\tstartHTTPService,\n\t\topenSQLDB,\n\t\tdetectServerInfo,\n\t\tresolveAutoConsistency,\n\n\t\tvalidateResolveAutoConsistency,\n\t\ttidbSetPDClientForGC,\n\t\ttidbGetSnapshot,\n\t\ttidbStartGCSavepointUpdateService,\n\n\t\tsetSessionParam)\n\treturn d, err\n}\n\n// Dump dumps table from database\n// nolint: gocyclo\nfunc (d *Dumper) Dump() (dumpErr error) {\n\tinitColTypeRowReceiverMap()\n\tvar (\n\t\tconn    *sql.Conn\n\t\terr     error\n\t\tconCtrl ConsistencyController\n\t)\n\ttctx, conf, pool := d.tctx, d.conf, d.dbHandle\n\ttctx.L().Info(\"begin to run Dump\", zap.Stringer(\"conf\", conf))\n\tm := newGlobalMetadata(tctx, d.extStore, conf.Snapshot)\n\trepeatableRead := needRepeatableRead(conf.ServerInfo.ServerType, conf.Consistency)\n\tdefer func() {\n\t\tif dumpErr == nil {\n\t\t\t_ = m.writeGlobalMetaData()\n\t\t}\n\t}()\n\n\t// for consistency lock, we should get table list at first to generate the lock tables SQL\n\tif conf.Consistency == ConsistencyTypeLock {\n\t\tconn, err = createConnWithConsistency(tctx, pool, repeatableRead)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tif err = prepareTableListToDump(tctx, conf, conn); err != nil {\n\t\t\t_ = conn.Close()\n\t\t\treturn err\n\t\t}\n\t\t_ = conn.Close()\n\t}\n\n\tconCtrl, err = NewConsistencyController(tctx, conf, pool)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = conCtrl.Setup(tctx); err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\t// To avoid lock is not released\n\tdefer func() {\n\t\terr = conCtrl.TearDown(tctx)\n\t\tif err != nil {\n\t\t\ttctx.L().Warn(\"fail to tear down consistency controller\", zap.Error(err))\n\t\t}\n\t}()\n\n\tmetaConn, err := createConnWithConsistency(tctx, pool, repeatableRead)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer func() {\n\t\t_ = metaConn.Close()\n\t}()\n\tm.recordStartTime(time.Now())\n\t// for consistency lock, we can write snapshot info after all tables are locked.\n\t// the binlog pos may changed because there is still possible write between we lock tables and write master status.\n\t// but for the locked tables doing replication that starts from metadata is safe.\n\t// for consistency flush, record snapshot after whole tables are locked. The recorded meta info is exactly the locked snapshot.\n\t// for consistency snapshot, we should use the snapshot that we get/set at first in metadata. TiDB will assure the snapshot of TSO.\n\t// for consistency none, the binlog pos in metadata might be earlier than dumped data. We need to enable safe-mode to assure data safety.\n\terr = m.recordGlobalMetaData(metaConn, conf.ServerInfo.ServerType, false)\n\tif err != nil {\n\t\ttctx.L().Info(\"get global metadata failed\", log.ShortError(err))\n\t}\n\n\tif d.conf.CollationCompatible == StrictCollationCompatible {\n\t\t//init charset and default collation map\n\t\td.charsetAndDefaultCollationMap, err = GetCharsetAndDefaultCollation(tctx.Context, metaConn)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// for other consistencies, we should get table list after consistency is set up and GlobalMetaData is cached\n\tif conf.Consistency != ConsistencyTypeLock {\n\t\tif err = prepareTableListToDump(tctx, conf, metaConn); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif err = d.renewSelectTableRegionFuncForLowerTiDB(tctx); err != nil {\n\t\ttctx.L().Info(\"cannot update select table region info for TiDB\", log.ShortError(err))\n\t}\n\n\tatomic.StoreInt64(&d.totalTables, int64(calculateTableCount(conf.Tables)))\n\n\trebuildConn := func(conn *sql.Conn, updateMeta bool) (*sql.Conn, error) {\n\t\t// make sure that the lock connection is still alive\n\t\terr1 := conCtrl.PingContext(tctx)\n\t\tif err1 != nil {\n\t\t\treturn conn, errors.Trace(err1)\n\t\t}\n\t\t// give up the last broken connection\n\t\t_ = conn.Close()\n\t\tnewConn, err1 := createConnWithConsistency(tctx, pool, repeatableRead)\n\t\tif err1 != nil {\n\t\t\treturn conn, errors.Trace(err1)\n\t\t}\n\t\tconn = newConn\n\t\t// renew the master status after connection. dm can't close safe-mode until dm reaches current pos\n\t\tif updateMeta && conf.PosAfterConnect {\n\t\t\terr1 = m.recordGlobalMetaData(conn, conf.ServerInfo.ServerType, true)\n\t\t\tif err1 != nil {\n\t\t\t\treturn conn, errors.Trace(err1)\n\t\t\t}\n\t\t}\n\t\treturn conn, nil\n\t}\n\n\ttaskChan := make(chan Task, defaultDumpThreads)\n\tAddGauge(d.metrics.taskChannelCapacity, defaultDumpThreads)\n\twg, writingCtx := errgroup.WithContext(tctx)\n\twriterCtx := tctx.WithContext(writingCtx)\n\twriters, tearDownWriters, err := d.startWriters(writerCtx, wg, taskChan, rebuildConn)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer tearDownWriters()\n\n\tif conf.TransactionalConsistency {\n\t\tif conf.Consistency == ConsistencyTypeFlush || conf.Consistency == ConsistencyTypeLock {\n\t\t\ttctx.L().Info(\"All the dumping transactions have started. Start to unlock tables\")\n\t\t}\n\t\tif err = conCtrl.TearDown(tctx); err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t}\n\t// Inject consistency failpoint test after we release the table lock\n\tfailpoint.Inject(\"ConsistencyCheck\", nil)\n\n\tif conf.PosAfterConnect {\n\t\t// record again, to provide a location to exit safe mode for DM\n\t\terr = m.recordGlobalMetaData(metaConn, conf.ServerInfo.ServerType, true)\n\t\tif err != nil {\n\t\t\ttctx.L().Info(\"get global metadata (after connection pool established) failed\", log.ShortError(err))\n\t\t}\n\t}\n\n\tsummary.SetLogCollector(summary.NewLogCollector(tctx.L().Info))\n\tsummary.SetUnit(summary.BackupUnit)\n\tdefer summary.Summary(summary.BackupUnit)\n\n\tlogProgressCtx, logProgressCancel := tctx.WithCancel()\n\tgo d.runLogProgress(logProgressCtx)\n\tdefer logProgressCancel()\n\n\ttableDataStartTime := time.Now()\n\n\tfailpoint.Inject(\"PrintTiDBMemQuotaQuery\", func(_ failpoint.Value) {\n\t\trow := d.dbHandle.QueryRowContext(tctx, \"select @@tidb_mem_quota_query;\")\n\t\tvar s string\n\t\terr = row.Scan(&s)\n\t\tif err != nil {\n\t\t\tfmt.Println(errors.Trace(err))\n\t\t} else {\n\t\t\tfmt.Printf(\"tidb_mem_quota_query == %s\\n\", s)\n\t\t}\n\t})\n\tbaseConn := newBaseConn(metaConn, canRebuildConn(conf.Consistency, conf.TransactionalConsistency), rebuildConn)\n\n\tif conf.SQL == \"\" {\n\t\tif err = d.dumpDatabases(writerCtx, baseConn, taskChan); err != nil && !errors.ErrorEqual(err, context.Canceled) {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\td.dumpSQL(writerCtx, baseConn, taskChan)\n\t}\n\tclose(taskChan)\n\t_ = baseConn.DBConn.Close()\n\tif err := wg.Wait(); err != nil {\n\t\tsummary.CollectFailureUnit(\"dump table data\", err)\n\t\treturn errors.Trace(err)\n\t}\n\tsummary.CollectSuccessUnit(\"dump cost\", countTotalTask(writers), time.Since(tableDataStartTime))\n\n\tsummary.SetSuccessStatus(true)\n\tm.recordFinishTime(time.Now())\n\treturn nil\n}\n\nfunc (d *Dumper) startWriters(tctx *tcontext.Context, wg *errgroup.Group, taskChan <-chan Task,\n\trebuildConnFn func(*sql.Conn, bool) (*sql.Conn, error)) ([]*Writer, func(), error) {\n\tconf, pool := d.conf, d.dbHandle\n\twriters := make([]*Writer, conf.Threads)\n\tfor i := 0; i < conf.Threads; i++ {\n\t\tconn, err := createConnWithConsistency(tctx, pool, needRepeatableRead(conf.ServerInfo.ServerType, conf.Consistency))\n\t\tif err != nil {\n\t\t\treturn nil, func() {}, err\n\t\t}\n\t\twriter := NewWriter(tctx, int64(i), conf, conn, d.extStore, d.metrics)\n\t\twriter.rebuildConnFn = rebuildConnFn\n\t\twriter.setFinishTableCallBack(func(task Task) {\n\t\t\tif _, ok := task.(*TaskTableData); ok {\n\t\t\t\tIncCounter(d.metrics.finishedTablesCounter)\n\t\t\t\t// FIXME: actually finishing the last chunk doesn't means this table is 'finished'.\n\t\t\t\t//  We can call this table is 'finished' if all its chunks are finished.\n\t\t\t\t//  Comment this log now to avoid ambiguity.\n\t\t\t\t// tctx.L().Debug(\"finished dumping table data\",\n\t\t\t\t//\tzap.String(\"database\", td.Meta.DatabaseName()),\n\t\t\t\t//\tzap.String(\"table\", td.Meta.TableName()))\n\t\t\t}\n\t\t})\n\t\twriter.setFinishTaskCallBack(func(task Task) {\n\t\t\tIncGauge(d.metrics.taskChannelCapacity)\n\t\t\tif td, ok := task.(*TaskTableData); ok {\n\t\t\t\ttctx.L().Debug(\"finish dumping table data task\",\n\t\t\t\t\tzap.String(\"database\", td.Meta.DatabaseName()),\n\t\t\t\t\tzap.String(\"table\", td.Meta.TableName()),\n\t\t\t\t\tzap.Int(\"chunkIdx\", td.ChunkIndex))\n\t\t\t}\n\t\t})\n\t\twg.Go(func() error {\n\t\t\treturn writer.run(taskChan)\n\t\t})\n\t\twriters[i] = writer\n\t}\n\ttearDown := func() {\n\t\tfor _, w := range writers {\n\t\t\t_ = w.conn.Close()\n\t\t}\n\t}\n\treturn writers, tearDown, nil\n}\n\nfunc (d *Dumper) dumpDatabases(tctx *tcontext.Context, metaConn *BaseConn, taskChan chan<- Task) error {\n\tconf := d.conf\n\tallTables := conf.Tables\n\n\t// policy should be created before database\n\t// placement policy in other server type can be different, so we only handle the tidb server\n\tif conf.ServerInfo.ServerType == version.ServerTypeTiDB {\n\t\tpolicyNames, err := ListAllPlacementPolicyNames(tctx, metaConn)\n\t\tif err != nil {\n\t\t\terrCause := errors.Cause(err)\n\t\t\tif mysqlErr, ok := errCause.(*mysql.MySQLError); ok && mysqlErr.Number == ErrNoSuchTable {\n\t\t\t\t// some old tidb version and other server type doesn't support placement rules, we can skip it.\n\t\t\t\ttctx.L().Debug(\"cannot dump placement policy, maybe the server doesn't support it\", log.ShortError(err))\n\t\t\t} else {\n\t\t\t\ttctx.L().Warn(\"fail to dump placement policy: \", log.ShortError(err))\n\t\t\t}\n\t\t}\n\t\tfor _, policy := range policyNames {\n\t\t\tcreatePolicySQL, err := ShowCreatePlacementPolicy(tctx, metaConn, policy)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\t\t\twrappedCreatePolicySQL := fmt.Sprintf(\"/*T![placement] %s */\", createPolicySQL)\n\t\t\ttask := NewTaskPolicyMeta(policy, wrappedCreatePolicySQL)\n\t\t\tctxDone := d.sendTaskToChan(tctx, task, taskChan)\n\t\t\tif ctxDone {\n\t\t\t\treturn tctx.Err()\n\t\t\t}\n\t\t}\n\t}\n\n\tparser1 := parser.New()\n\tfor dbName, tables := range allTables {\n\t\tif !conf.NoSchemas {\n\t\t\tcreateDatabaseSQL, err := ShowCreateDatabase(tctx, metaConn, dbName)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\n\t\t\t// adjust db collation\n\t\t\tcreateDatabaseSQL, err = adjustDatabaseCollation(tctx, d.conf.CollationCompatible, parser1, createDatabaseSQL, d.charsetAndDefaultCollationMap)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\n\t\t\ttask := NewTaskDatabaseMeta(dbName, createDatabaseSQL)\n\t\t\tctxDone := d.sendTaskToChan(tctx, task, taskChan)\n\t\t\tif ctxDone {\n\t\t\t\treturn tctx.Err()\n\t\t\t}\n\t\t}\n\n\t\tfor _, table := range tables {\n\t\t\ttctx.L().Debug(\"start dumping table...\", zap.String(\"database\", dbName),\n\t\t\t\tzap.String(\"table\", table.Name))\n\t\t\tmeta, err := dumpTableMeta(tctx, conf, metaConn, dbName, table)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\n\t\t\tif !conf.NoSchemas {\n\t\t\t\tswitch table.Type {\n\t\t\t\tcase TableTypeView:\n\t\t\t\t\ttask := NewTaskViewMeta(dbName, table.Name, meta.ShowCreateTable(), meta.ShowCreateView())\n\t\t\t\t\tctxDone := d.sendTaskToChan(tctx, task, taskChan)\n\t\t\t\t\tif ctxDone {\n\t\t\t\t\t\treturn tctx.Err()\n\t\t\t\t\t}\n\t\t\t\tcase TableTypeSequence:\n\t\t\t\t\ttask := NewTaskSequenceMeta(dbName, table.Name, meta.ShowCreateTable())\n\t\t\t\t\tctxDone := d.sendTaskToChan(tctx, task, taskChan)\n\t\t\t\t\tif ctxDone {\n\t\t\t\t\t\treturn tctx.Err()\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\t// adjust table collation\n\t\t\t\t\tnewCreateSQL, err := adjustTableCollation(tctx, d.conf.CollationCompatible, parser1, meta.ShowCreateTable(), d.charsetAndDefaultCollationMap)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn errors.Trace(err)\n\t\t\t\t\t}\n\t\t\t\t\tmeta.(*tableMeta).showCreateTable = newCreateSQL\n\n\t\t\t\t\ttask := NewTaskTableMeta(dbName, table.Name, meta.ShowCreateTable())\n\t\t\t\t\tctxDone := d.sendTaskToChan(tctx, task, taskChan)\n\t\t\t\t\tif ctxDone {\n\t\t\t\t\t\treturn tctx.Err()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif table.Type == TableTypeBase {\n\t\t\t\terr = d.dumpTableData(tctx, metaConn, meta, taskChan)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Trace(err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// adjustDatabaseCollation adjusts db collation and return new create sql and collation\nfunc adjustDatabaseCollation(tctx *tcontext.Context, collationCompatible string, parser *parser.Parser, originSQL string, charsetAndDefaultCollationMap map[string]string) (string, error) {\n\tif collationCompatible != StrictCollationCompatible {\n\t\treturn originSQL, nil\n\t}\n\tstmt, err := parser.ParseOneStmt(originSQL, \"\", \"\")\n\tif err != nil {\n\t\ttctx.L().Warn(\"parse create database error, maybe tidb parser doesn't support it\", zap.String(\"originSQL\", originSQL), log.ShortError(err))\n\t\treturn originSQL, nil\n\t}\n\tcreateStmt, ok := stmt.(*ast.CreateDatabaseStmt)\n\tif !ok {\n\t\treturn originSQL, nil\n\t}\n\tvar charset string\n\tfor _, createOption := range createStmt.Options {\n\t\t// already have 'Collation'\n\t\tif createOption.Tp == ast.DatabaseOptionCollate {\n\t\t\treturn originSQL, nil\n\t\t}\n\t\tif createOption.Tp == ast.DatabaseOptionCharset {\n\t\t\tcharset = createOption.Value\n\t\t}\n\t}\n\t// get db collation\n\tcollation, ok := charsetAndDefaultCollationMap[strings.ToLower(charset)]\n\tif !ok {\n\t\ttctx.L().Warn(\"not found database charset default collation.\", zap.String(\"originSQL\", originSQL), zap.String(\"charset\", strings.ToLower(charset)))\n\t\treturn originSQL, nil\n\t}\n\t// add collation\n\tcreateStmt.Options = append(createStmt.Options, &ast.DatabaseOption{Tp: ast.DatabaseOptionCollate, Value: collation})\n\t// rewrite sql\n\tvar b []byte\n\tbf := bytes.NewBuffer(b)\n\terr = createStmt.Restore(&format.RestoreCtx{\n\t\tFlags: format.DefaultRestoreFlags | format.RestoreTiDBSpecialComment,\n\t\tIn:    bf,\n\t})\n\tif err != nil {\n\t\treturn \"\", errors.Trace(err)\n\t}\n\treturn bf.String(), nil\n}\n\n// adjustTableCollation adjusts table collation\nfunc adjustTableCollation(tctx *tcontext.Context, collationCompatible string, parser *parser.Parser, originSQL string, charsetAndDefaultCollationMap map[string]string) (string, error) {\n\tif collationCompatible != StrictCollationCompatible {\n\t\treturn originSQL, nil\n\t}\n\tstmt, err := parser.ParseOneStmt(originSQL, \"\", \"\")\n\tif err != nil {\n\t\ttctx.L().Warn(\"parse create table error, maybe tidb parser doesn't support it\", zap.String(\"originSQL\", originSQL), log.ShortError(err))\n\t\treturn originSQL, nil\n\t}\n\tcreateStmt, ok := stmt.(*ast.CreateTableStmt)\n\tif !ok {\n\t\treturn originSQL, nil\n\t}\n\tvar charset string\n\tvar collation string\n\tfor _, createOption := range createStmt.Options {\n\t\t// already have 'Collation'\n\t\tif createOption.Tp == ast.TableOptionCollate {\n\t\t\tcollation = createOption.StrValue\n\t\t\tbreak\n\t\t}\n\t\tif createOption.Tp == ast.TableOptionCharset {\n\t\t\tcharset = createOption.StrValue\n\t\t}\n\t}\n\n\tif collation == \"\" && charset != \"\" {\n\t\tcollation, ok := charsetAndDefaultCollationMap[strings.ToLower(charset)]\n\t\tif !ok {\n\t\t\ttctx.L().Warn(\"not found table charset default collation.\", zap.String(\"originSQL\", originSQL), zap.String(\"charset\", strings.ToLower(charset)))\n\t\t\treturn originSQL, nil\n\t\t}\n\n\t\t// add collation\n\t\tcreateStmt.Options = append(createStmt.Options, &ast.TableOption{Tp: ast.TableOptionCollate, StrValue: collation})\n\t}\n\n\t// adjust columns collation\n\tadjustColumnsCollation(tctx, createStmt, charsetAndDefaultCollationMap)\n\n\t// rewrite sql\n\tvar b []byte\n\tbf := bytes.NewBuffer(b)\n\terr = createStmt.Restore(&format.RestoreCtx{\n\t\tFlags: format.DefaultRestoreFlags | format.RestoreTiDBSpecialComment,\n\t\tIn:    bf,\n\t})\n\tif err != nil {\n\t\treturn \"\", errors.Trace(err)\n\t}\n\treturn bf.String(), nil\n}\n\n// adjustColumnsCollation adds column's collation.\nfunc adjustColumnsCollation(tctx *tcontext.Context, createStmt *ast.CreateTableStmt, charsetAndDefaultCollationMap map[string]string) {\nColumnLoop:\n\tfor _, col := range createStmt.Cols {\n\t\tfor _, options := range col.Options {\n\t\t\t// already have 'Collation'\n\t\t\tif options.Tp == ast.ColumnOptionCollate {\n\t\t\t\tcontinue ColumnLoop\n\t\t\t}\n\t\t}\n\t\tfieldType := col.Tp\n\t\tif fieldType.GetCollate() != \"\" {\n\t\t\tcontinue\n\t\t}\n\t\tif fieldType.GetCharset() != \"\" {\n\t\t\t// just have charset\n\t\t\tcollation, ok := charsetAndDefaultCollationMap[strings.ToLower(fieldType.GetCharset())]\n\t\t\tif !ok {\n\t\t\t\ttctx.L().Warn(\"not found charset default collation for column.\", zap.String(\"table\", createStmt.Table.Name.String()), zap.String(\"column\", col.Name.String()), zap.String(\"charset\", strings.ToLower(fieldType.GetCharset())))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfieldType.SetCollate(collation)\n\t\t}\n\t}\n}\n\nfunc (d *Dumper) dumpTableData(tctx *tcontext.Context, conn *BaseConn, meta TableMeta, taskChan chan<- Task) error {\n\tconf := d.conf\n\tif conf.NoData {\n\t\treturn nil\n\t}\n\n\t// Update total rows\n\tfieldName, _ := pickupPossibleField(tctx, meta, conn)\n\tc := estimateCount(tctx, meta.DatabaseName(), meta.TableName(), conn, fieldName, conf)\n\tAddCounter(d.metrics.estimateTotalRowsCounter, float64(c))\n\n\tif conf.Rows == UnspecifiedSize {\n\t\treturn d.sequentialDumpTable(tctx, conn, meta, taskChan)\n\t}\n\treturn d.concurrentDumpTable(tctx, conn, meta, taskChan)\n}\n\nfunc (d *Dumper) buildConcatTask(tctx *tcontext.Context, conn *BaseConn, meta TableMeta) (*TaskTableData, error) {\n\ttableChan := make(chan Task, 128)\n\terrCh := make(chan error, 1)\n\tgo func() {\n\t\t// adjust rows to suitable rows for this table\n\t\td.conf.Rows = GetSuitableRows(meta.AvgRowLength())\n\t\terr := d.concurrentDumpTable(tctx, conn, meta, tableChan)\n\t\td.conf.Rows = UnspecifiedSize\n\t\tif err != nil {\n\t\t\terrCh <- err\n\t\t} else {\n\t\t\tclose(errCh)\n\t\t}\n\t}()\n\ttableDataArr := make([]*tableData, 0)\n\thandleSubTask := func(task Task) {\n\t\ttableTask, ok := task.(*TaskTableData)\n\t\tif !ok {\n\t\t\ttctx.L().Warn(\"unexpected task when splitting table chunks\", zap.String(\"task\", tableTask.Brief()))\n\t\t\treturn\n\t\t}\n\t\ttableDataInst, ok := tableTask.Data.(*tableData)\n\t\tif !ok {\n\t\t\ttctx.L().Warn(\"unexpected task.Data when splitting table chunks\", zap.String(\"task\", tableTask.Brief()))\n\t\t\treturn\n\t\t}\n\t\ttableDataArr = append(tableDataArr, tableDataInst)\n\t}\n\tfor {\n\t\tselect {\n\t\tcase err, ok := <-errCh:\n\t\t\tif !ok {\n\t\t\t\t// make sure all the subtasks in tableChan are handled\n\t\t\t\tfor len(tableChan) > 0 {\n\t\t\t\t\ttask := <-tableChan\n\t\t\t\t\thandleSubTask(task)\n\t\t\t\t}\n\t\t\t\tif len(tableDataArr) <= 1 {\n\t\t\t\t\treturn nil, nil\n\t\t\t\t}\n\t\t\t\tqueries := make([]string, 0, len(tableDataArr))\n\t\t\t\tcolLen := tableDataArr[0].colLen\n\t\t\t\tfor _, tableDataInst := range tableDataArr {\n\t\t\t\t\tqueries = append(queries, tableDataInst.query)\n\t\t\t\t\tif colLen != tableDataInst.colLen {\n\t\t\t\t\t\ttctx.L().Warn(\"colLen varies for same table\",\n\t\t\t\t\t\t\tzap.Int(\"oldColLen\", colLen),\n\t\t\t\t\t\t\tzap.String(\"oldQuery\", queries[0]),\n\t\t\t\t\t\t\tzap.Int(\"newColLen\", tableDataInst.colLen),\n\t\t\t\t\t\t\tzap.String(\"newQuery\", tableDataInst.query))\n\t\t\t\t\t\treturn nil, nil\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn NewTaskTableData(meta, newMultiQueriesChunk(queries, colLen), 0, 1), nil\n\t\t\t}\n\t\t\treturn nil, err\n\t\tcase task := <-tableChan:\n\t\t\thandleSubTask(task)\n\t\t}\n\t}\n}\n\nfunc (d *Dumper) dumpWholeTableDirectly(tctx *tcontext.Context, meta TableMeta, taskChan chan<- Task, partition, orderByClause string, currentChunk, totalChunks int) error {\n\tconf := d.conf\n\ttableIR := SelectAllFromTable(conf, meta, partition, orderByClause)\n\ttask := NewTaskTableData(meta, tableIR, currentChunk, totalChunks)\n\tctxDone := d.sendTaskToChan(tctx, task, taskChan)\n\tif ctxDone {\n\t\treturn tctx.Err()\n\t}\n\treturn nil\n}\n\nfunc (d *Dumper) sequentialDumpTable(tctx *tcontext.Context, conn *BaseConn, meta TableMeta, taskChan chan<- Task) error {\n\tconf := d.conf\n\tif conf.ServerInfo.ServerType == version.ServerTypeTiDB {\n\t\ttask, err := d.buildConcatTask(tctx, conn, meta)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tif task != nil {\n\t\t\tctxDone := d.sendTaskToChan(tctx, task, taskChan)\n\t\t\tif ctxDone {\n\t\t\t\treturn tctx.Err()\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t\ttctx.L().Info(\"didn't build tidb concat sqls, will select all from table now\",\n\t\t\tzap.String(\"database\", meta.DatabaseName()),\n\t\t\tzap.String(\"table\", meta.TableName()))\n\t}\n\torderByClause, err := buildOrderByClause(tctx, conf, conn, meta.DatabaseName(), meta.TableName(), meta.HasImplicitRowID())\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn d.dumpWholeTableDirectly(tctx, meta, taskChan, \"\", orderByClause, 0, 1)\n}\n\n// concurrentDumpTable tries to split table into several chunks to dump\nfunc (d *Dumper) concurrentDumpTable(tctx *tcontext.Context, conn *BaseConn, meta TableMeta, taskChan chan<- Task) error {\n\tconf := d.conf\n\tdb, tbl := meta.DatabaseName(), meta.TableName()\n\tif conf.ServerInfo.ServerType == version.ServerTypeTiDB &&\n\t\tconf.ServerInfo.ServerVersion != nil &&\n\t\t(conf.ServerInfo.ServerVersion.Compare(*tableSampleVersion) >= 0 ||\n\t\t\t(conf.ServerInfo.HasTiKV && conf.ServerInfo.ServerVersion.Compare(*decodeRegionVersion) >= 0)) {\n\t\terr := d.concurrentDumpTiDBTables(tctx, conn, meta, taskChan)\n\t\t// don't retry on context error and successful tasks\n\t\tif err2 := errors.Cause(err); err2 == nil || err2 == context.DeadlineExceeded || err2 == context.Canceled {\n\t\t\treturn err\n\t\t} else if err2 != errEmptyHandleVals {\n\t\t\ttctx.L().Info(\"fallback to concurrent dump tables using rows due to some problem. This won't influence the whole dump process\",\n\t\t\t\tzap.String(\"database\", db), zap.String(\"table\", tbl), log.ShortError(err))\n\t\t}\n\t}\n\n\torderByClause, err := buildOrderByClause(tctx, conf, conn, db, tbl, meta.HasImplicitRowID())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfield, err := pickupPossibleField(tctx, meta, conn)\n\tif err != nil || field == \"\" {\n\t\t// skip split chunk logic if not found proper field\n\t\ttctx.L().Info(\"fallback to sequential dump due to no proper field. This won't influence the whole dump process\",\n\t\t\tzap.String(\"database\", db), zap.String(\"table\", tbl), log.ShortError(err))\n\t\treturn d.dumpWholeTableDirectly(tctx, meta, taskChan, \"\", orderByClause, 0, 1)\n\t}\n\n\tcount := estimateCount(d.tctx, db, tbl, conn, field, conf)\n\ttctx.L().Info(\"get estimated rows count\",\n\t\tzap.String(\"database\", db),\n\t\tzap.String(\"table\", tbl),\n\t\tzap.Uint64(\"estimateCount\", count))\n\tif count < conf.Rows {\n\t\t// skip chunk logic if estimates are low\n\t\ttctx.L().Info(\"fallback to sequential dump due to estimate count < rows. This won't influence the whole dump process\",\n\t\t\tzap.Uint64(\"estimate count\", count),\n\t\t\tzap.Uint64(\"conf.rows\", conf.Rows),\n\t\t\tzap.String(\"database\", db),\n\t\t\tzap.String(\"table\", tbl))\n\t\treturn d.dumpWholeTableDirectly(tctx, meta, taskChan, \"\", orderByClause, 0, 1)\n\t}\n\n\tmin, max, err := d.selectMinAndMaxIntValue(tctx, conn, db, tbl, field)\n\tif err != nil {\n\t\ttctx.L().Info(\"fallback to sequential dump due to cannot get bounding values. This won't influence the whole dump process\",\n\t\t\tlog.ShortError(err))\n\t\treturn d.dumpWholeTableDirectly(tctx, meta, taskChan, \"\", orderByClause, 0, 1)\n\t}\n\ttctx.L().Debug(\"get int bounding values\",\n\t\tzap.String(\"lower\", min.String()),\n\t\tzap.String(\"upper\", max.String()))\n\n\t// every chunk would have eventual adjustments\n\testimatedChunks := count / conf.Rows\n\testimatedStep := new(big.Int).Sub(max, min).Uint64()/estimatedChunks + 1\n\tbigEstimatedStep := new(big.Int).SetUint64(estimatedStep)\n\tcutoff := new(big.Int).Set(min)\n\ttotalChunks := estimatedChunks\n\tif estimatedStep == 1 {\n\t\ttotalChunks = new(big.Int).Sub(max, min).Uint64() + 1\n\t}\n\n\tselectField, selectLen := meta.SelectedField(), meta.SelectedLen()\n\n\tchunkIndex := 0\n\tnullValueCondition := \"\"\n\tif conf.Where == \"\" {\n\t\tnullValueCondition = fmt.Sprintf(\"`%s` IS NULL OR \", escapeString(field))\n\t}\n\tfor max.Cmp(cutoff) >= 0 {\n\t\tnextCutOff := new(big.Int).Add(cutoff, bigEstimatedStep)\n\t\twhere := fmt.Sprintf(\"%s(`%s` >= %d AND `%s` < %d)\", nullValueCondition, escapeString(field), cutoff, escapeString(field), nextCutOff)\n\t\tquery := buildSelectQuery(db, tbl, selectField, \"\", buildWhereCondition(conf, where), orderByClause)\n\t\tif len(nullValueCondition) > 0 {\n\t\t\tnullValueCondition = \"\"\n\t\t}\n\t\ttask := NewTaskTableData(meta, newTableData(query, selectLen, false), chunkIndex, int(totalChunks))\n\t\tctxDone := d.sendTaskToChan(tctx, task, taskChan)\n\t\tif ctxDone {\n\t\t\treturn tctx.Err()\n\t\t}\n\t\tcutoff = nextCutOff\n\t\tchunkIndex++\n\t}\n\treturn nil\n}\n\nfunc (d *Dumper) sendTaskToChan(tctx *tcontext.Context, task Task, taskChan chan<- Task) (ctxDone bool) {\n\tselect {\n\tcase <-tctx.Done():\n\t\treturn true\n\tcase taskChan <- task:\n\t\ttctx.L().Debug(\"send task to writer\",\n\t\t\tzap.String(\"task\", task.Brief()))\n\t\tDecGauge(d.metrics.taskChannelCapacity)\n\t\treturn false\n\t}\n}\n\nfunc (d *Dumper) selectMinAndMaxIntValue(tctx *tcontext.Context, conn *BaseConn, db, tbl, field string) (*big.Int, *big.Int, error) {\n\tconf, zero := d.conf, &big.Int{}\n\tquery := fmt.Sprintf(\"SELECT MIN(`%s`),MAX(`%s`) FROM `%s`.`%s`\",\n\t\tescapeString(field), escapeString(field), escapeString(db), escapeString(tbl))\n\tif conf.Where != \"\" {\n\t\tquery = fmt.Sprintf(\"%s WHERE %s\", query, conf.Where)\n\t}\n\ttctx.L().Debug(\"split chunks\", zap.String(\"query\", query))\n\n\tvar smin sql.NullString\n\tvar smax sql.NullString\n\terr := conn.QuerySQL(tctx, func(rows *sql.Rows) error {\n\t\terr := rows.Scan(&smin, &smax)\n\t\trows.Close()\n\t\treturn err\n\t}, func() {}, query)\n\tif err != nil {\n\t\treturn zero, zero, errors.Annotatef(err, \"can't get min/max values to split chunks, query: %s\", query)\n\t}\n\tif !smax.Valid || !smin.Valid {\n\t\t// found no data\n\t\treturn zero, zero, errors.Errorf(\"no invalid min/max value found in query %s\", query)\n\t}\n\n\tmax := new(big.Int)\n\tmin := new(big.Int)\n\tvar ok bool\n\tif max, ok = max.SetString(smax.String, 10); !ok {\n\t\treturn zero, zero, errors.Errorf(\"fail to convert max value %s in query %s\", smax.String, query)\n\t}\n\tif min, ok = min.SetString(smin.String, 10); !ok {\n\t\treturn zero, zero, errors.Errorf(\"fail to convert min value %s in query %s\", smin.String, query)\n\t}\n\treturn min, max, nil\n}\n\nfunc (d *Dumper) concurrentDumpTiDBTables(tctx *tcontext.Context, conn *BaseConn, meta TableMeta, taskChan chan<- Task) error {\n\tdb, tbl := meta.DatabaseName(), meta.TableName()\n\n\tvar (\n\t\thandleColNames []string\n\t\thandleVals     [][]string\n\t\terr            error\n\t)\n\t// for TiDB v5.0+, we can use table sample directly\n\tif d.conf.ServerInfo.ServerVersion.Compare(*tableSampleVersion) >= 0 {\n\t\ttctx.L().Debug(\"dumping TiDB tables with TABLESAMPLE\",\n\t\t\tzap.String(\"database\", db), zap.String(\"table\", tbl))\n\t\thandleColNames, handleVals, err = selectTiDBTableSample(tctx, conn, meta)\n\t} else {\n\t\t// for TiDB v3.0+, we can use table region decode in TiDB directly\n\t\ttctx.L().Debug(\"dumping TiDB tables with TABLE REGIONS\",\n\t\t\tzap.String(\"database\", db), zap.String(\"table\", tbl))\n\t\tvar partitions []string\n\t\tif d.conf.ServerInfo.ServerVersion.Compare(*gcSafePointVersion) >= 0 {\n\t\t\tpartitions, err = GetPartitionNames(tctx, conn, db, tbl)\n\t\t}\n\t\tif err == nil {\n\t\t\tif len(partitions) == 0 {\n\t\t\t\thandleColNames, handleVals, err = d.selectTiDBTableRegionFunc(tctx, conn, meta)\n\t\t\t} else {\n\t\t\t\treturn d.concurrentDumpTiDBPartitionTables(tctx, conn, meta, taskChan, partitions)\n\t\t\t}\n\t\t}\n\t}\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn d.sendConcurrentDumpTiDBTasks(tctx, meta, taskChan, handleColNames, handleVals, \"\", 0, len(handleVals)+1)\n}\n\nfunc (d *Dumper) concurrentDumpTiDBPartitionTables(tctx *tcontext.Context, conn *BaseConn, meta TableMeta, taskChan chan<- Task, partitions []string) error {\n\tdb, tbl := meta.DatabaseName(), meta.TableName()\n\ttctx.L().Debug(\"dumping TiDB tables with TABLE REGIONS for partition table\",\n\t\tzap.String(\"database\", db), zap.String(\"table\", tbl), zap.Strings(\"partitions\", partitions))\n\n\tstartChunkIdx := 0\n\ttotalChunk := 0\n\tcachedHandleVals := make([][][]string, len(partitions))\n\n\thandleColNames, _, err := selectTiDBRowKeyFields(tctx, conn, meta, checkTiDBTableRegionPkFields)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// cache handleVals here to calculate the total chunks\n\tfor i, partition := range partitions {\n\t\thandleVals, err := selectTiDBPartitionRegion(tctx, conn, db, tbl, partition)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ttotalChunk += len(handleVals) + 1\n\t\tcachedHandleVals[i] = handleVals\n\t}\n\tfor i, partition := range partitions {\n\t\terr := d.sendConcurrentDumpTiDBTasks(tctx, meta, taskChan, handleColNames, cachedHandleVals[i], partition, startChunkIdx, totalChunk)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tstartChunkIdx += len(cachedHandleVals[i]) + 1\n\t}\n\treturn nil\n}\n\nfunc (d *Dumper) sendConcurrentDumpTiDBTasks(tctx *tcontext.Context,\n\tmeta TableMeta, taskChan chan<- Task,\n\thandleColNames []string, handleVals [][]string, partition string, startChunkIdx, totalChunk int) error {\n\tdb, tbl := meta.DatabaseName(), meta.TableName()\n\tif len(handleVals) == 0 {\n\t\tif partition == \"\" {\n\t\t\t// return error to make outside function try using rows method to dump data\n\t\t\treturn errors.Annotatef(errEmptyHandleVals, \"table: `%s`.`%s`\", escapeString(db), escapeString(tbl))\n\t\t}\n\t\treturn d.dumpWholeTableDirectly(tctx, meta, taskChan, partition, buildOrderByClauseString(handleColNames), startChunkIdx, totalChunk)\n\t}\n\tconf := d.conf\n\tselectField, selectLen := meta.SelectedField(), meta.SelectedLen()\n\twhere := buildWhereClauses(handleColNames, handleVals)\n\torderByClause := buildOrderByClauseString(handleColNames)\n\n\tfor i, w := range where {\n\t\tquery := buildSelectQuery(db, tbl, selectField, partition, buildWhereCondition(conf, w), orderByClause)\n\t\ttask := NewTaskTableData(meta, newTableData(query, selectLen, false), i+startChunkIdx, totalChunk)\n\t\tctxDone := d.sendTaskToChan(tctx, task, taskChan)\n\t\tif ctxDone {\n\t\t\treturn tctx.Err()\n\t\t}\n\t}\n\treturn nil\n}\n\n// L returns real logger\nfunc (d *Dumper) L() log.Logger {\n\treturn d.tctx.L()\n}\n\nfunc selectTiDBTableSample(tctx *tcontext.Context, conn *BaseConn, meta TableMeta) (pkFields []string, pkVals [][]string, err error) {\n\tpkFields, pkColTypes, err := selectTiDBRowKeyFields(tctx, conn, meta, nil)\n\tif err != nil {\n\t\treturn nil, nil, errors.Trace(err)\n\t}\n\n\tquery := buildTiDBTableSampleQuery(pkFields, meta.DatabaseName(), meta.TableName())\n\tpkValNum := len(pkFields)\n\tvar iter SQLRowIter\n\trowRec := MakeRowReceiver(pkColTypes)\n\tbuf := new(bytes.Buffer)\n\n\terr = conn.QuerySQL(tctx, func(rows *sql.Rows) error {\n\t\tif iter == nil {\n\t\t\titer = &rowIter{\n\t\t\t\trows: rows,\n\t\t\t\targs: make([]interface{}, pkValNum),\n\t\t\t}\n\t\t}\n\t\terr = iter.Decode(rowRec)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tpkValRow := make([]string, 0, pkValNum)\n\t\tfor _, rec := range rowRec.receivers {\n\t\t\trec.WriteToBuffer(buf, true)\n\t\t\tpkValRow = append(pkValRow, buf.String())\n\t\t\tbuf.Reset()\n\t\t}\n\t\tpkVals = append(pkVals, pkValRow)\n\t\treturn nil\n\t}, func() {\n\t\tif iter != nil {\n\t\t\t_ = iter.Close()\n\t\t\titer = nil\n\t\t}\n\t\trowRec = MakeRowReceiver(pkColTypes)\n\t\tpkVals = pkVals[:0]\n\t\tbuf.Reset()\n\t}, query)\n\tif err == nil && iter != nil && iter.Error() != nil {\n\t\terr = iter.Error()\n\t}\n\n\treturn pkFields, pkVals, err\n}\n\nfunc buildTiDBTableSampleQuery(pkFields []string, dbName, tblName string) string {\n\ttemplate := \"SELECT %s FROM `%s`.`%s` TABLESAMPLE REGIONS() ORDER BY %s\"\n\tquotaPk := make([]string, len(pkFields))\n\tfor i, s := range pkFields {\n\t\tquotaPk[i] = fmt.Sprintf(\"`%s`\", escapeString(s))\n\t}\n\tpks := strings.Join(quotaPk, \",\")\n\treturn fmt.Sprintf(template, pks, escapeString(dbName), escapeString(tblName), pks)\n}\n\nfunc selectTiDBRowKeyFields(tctx *tcontext.Context, conn *BaseConn, meta TableMeta, checkPkFields func([]string, []string) error) (pkFields, pkColTypes []string, err error) {\n\tif meta.HasImplicitRowID() {\n\t\tpkFields, pkColTypes = []string{\"_tidb_rowid\"}, []string{\"BIGINT\"}\n\t} else {\n\t\tpkFields, pkColTypes, err = GetPrimaryKeyAndColumnTypes(tctx, conn, meta)\n\t\tif err == nil {\n\t\t\tif checkPkFields != nil {\n\t\t\t\terr = checkPkFields(pkFields, pkColTypes)\n\t\t\t}\n\t\t}\n\t}\n\treturn\n}\n\nfunc checkTiDBTableRegionPkFields(pkFields, pkColTypes []string) (err error) {\n\tif len(pkFields) != 1 || len(pkColTypes) != 1 {\n\t\terr = errors.Errorf(\"unsupported primary key for selectTableRegion. pkFields: [%s], pkColTypes: [%s]\", strings.Join(pkFields, \", \"), strings.Join(pkColTypes, \", \"))\n\t\treturn\n\t}\n\tif _, ok := dataTypeInt[pkColTypes[0]]; !ok {\n\t\terr = errors.Errorf(\"unsupported primary key type for selectTableRegion. pkFields: [%s], pkColTypes: [%s]\", strings.Join(pkFields, \", \"), strings.Join(pkColTypes, \", \"))\n\t}\n\treturn\n}\n\nfunc selectTiDBTableRegion(tctx *tcontext.Context, conn *BaseConn, meta TableMeta) (pkFields []string, pkVals [][]string, err error) {\n\tpkFields, _, err = selectTiDBRowKeyFields(tctx, conn, meta, checkTiDBTableRegionPkFields)\n\tif err != nil {\n\t\treturn\n\t}\n\n\tvar (\n\t\tstartKey, decodedKey sql.NullString\n\t\trowID                = -1\n\t)\n\tconst (\n\t\ttableRegionSQL = \"SELECT START_KEY,tidb_decode_key(START_KEY) from INFORMATION_SCHEMA.TIKV_REGION_STATUS s WHERE s.DB_NAME = ? AND s.TABLE_NAME = ? AND IS_INDEX = 0 ORDER BY START_KEY;\"\n\t\ttidbRowID      = \"_tidb_rowid=\"\n\t)\n\tdbName, tableName := meta.DatabaseName(), meta.TableName()\n\tlogger := tctx.L().With(zap.String(\"database\", dbName), zap.String(\"table\", tableName))\n\terr = conn.QuerySQL(tctx, func(rows *sql.Rows) error {\n\t\trowID++\n\t\terr = rows.Scan(&startKey, &decodedKey)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\t// first region's start key has no use. It may come from another table or might be invalid\n\t\tif rowID == 0 {\n\t\t\treturn nil\n\t\t}\n\t\tif !startKey.Valid {\n\t\t\tlogger.Debug(\"meet invalid start key\", zap.Int(\"rowID\", rowID))\n\t\t\treturn nil\n\t\t}\n\t\tif !decodedKey.Valid {\n\t\t\tlogger.Debug(\"meet invalid decoded start key\", zap.Int(\"rowID\", rowID), zap.String(\"startKey\", startKey.String))\n\t\t\treturn nil\n\t\t}\n\t\tpkVal, err2 := extractTiDBRowIDFromDecodedKey(tidbRowID, decodedKey.String)\n\t\tif err2 != nil {\n\t\t\tlogger.Debug(\"cannot extract pkVal from decoded start key\",\n\t\t\t\tzap.Int(\"rowID\", rowID), zap.String(\"startKey\", startKey.String), zap.String(\"decodedKey\", decodedKey.String), log.ShortError(err2))\n\t\t} else {\n\t\t\tpkVals = append(pkVals, []string{pkVal})\n\t\t}\n\t\treturn nil\n\t}, func() {\n\t\tpkFields = pkFields[:0]\n\t\tpkVals = pkVals[:0]\n\t}, tableRegionSQL, dbName, tableName)\n\n\treturn pkFields, pkVals, errors.Trace(err)\n}\n\nfunc selectTiDBPartitionRegion(tctx *tcontext.Context, conn *BaseConn, dbName, tableName, partition string) (pkVals [][]string, err error) {\n\tvar startKeys [][]string\n\tconst (\n\t\tpartitionRegionSQL = \"SHOW TABLE `%s`.`%s` PARTITION(`%s`) REGIONS\"\n\t\tregionRowKey       = \"r_\"\n\t)\n\tlogger := tctx.L().With(zap.String(\"database\", dbName), zap.String(\"table\", tableName), zap.String(\"partition\", partition))\n\tstartKeys, err = conn.QuerySQLWithColumns(tctx, []string{\"START_KEY\"}, fmt.Sprintf(partitionRegionSQL, escapeString(dbName), escapeString(tableName), escapeString(partition)))\n\tif err != nil {\n\t\treturn\n\t}\n\tfor rowID, startKey := range startKeys {\n\t\tif rowID == 0 || len(startKey) != 1 {\n\t\t\tcontinue\n\t\t}\n\t\tpkVal, err2 := extractTiDBRowIDFromDecodedKey(regionRowKey, startKey[0])\n\t\tif err2 != nil {\n\t\t\tlogger.Debug(\"show table region start key doesn't have rowID\",\n\t\t\t\tzap.Int(\"rowID\", rowID), zap.String(\"startKey\", startKey[0]), zap.Error(err2))\n\t\t} else {\n\t\t\tpkVals = append(pkVals, []string{pkVal})\n\t\t}\n\t}\n\n\treturn pkVals, nil\n}\n\nfunc extractTiDBRowIDFromDecodedKey(indexField, key string) (string, error) {\n\tif p := strings.Index(key, indexField); p != -1 {\n\t\tp += len(indexField)\n\t\treturn key[p:], nil\n\t}\n\treturn \"\", errors.Errorf(\"decoded key %s doesn't have %s field\", key, indexField)\n}\n\nfunc getListTableTypeByConf(conf *Config) listTableType {\n\t// use listTableByShowTableStatus by default because it has better performance\n\tlistType := listTableByShowTableStatus\n\tif conf.Consistency == ConsistencyTypeLock {\n\t\t// for consistency lock, we need to build the tables to dump as soon as possible\n\t\tlistType = listTableByInfoSchema\n\t} else if conf.Consistency == ConsistencyTypeFlush && matchMysqlBugversion(conf.ServerInfo) {\n\t\t// For some buggy versions of mysql, we need a workaround to get a list of table names.\n\t\tlistType = listTableByShowFullTables\n\t}\n\treturn listType\n}\n\nfunc prepareTableListToDump(tctx *tcontext.Context, conf *Config, db *sql.Conn) error {\n\tif conf.specifiedTables {\n\t\treturn nil\n\t}\n\tdatabases, err := prepareDumpingDatabases(tctx, conf, db)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ttableTypes := []TableType{TableTypeBase}\n\tif !conf.NoViews {\n\t\ttableTypes = append(tableTypes, TableTypeView)\n\t}\n\tif !conf.NoSequences {\n\t\ttableTypes = append(tableTypes, TableTypeSequence)\n\t}\n\n\tifSeqExists, err := CheckIfSeqExists(db)\n\tif err != nil {\n\t\treturn err\n\t}\n\tvar listType listTableType\n\tif ifSeqExists {\n\t\tlistType = listTableByShowFullTables\n\t} else {\n\t\tlistType = getListTableTypeByConf(conf)\n\t}\n\n\tconf.Tables, err = ListAllDatabasesTables(tctx, db, databases, listType, tableTypes...)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfilterTables(tctx, conf)\n\treturn nil\n}\n\nfunc dumpTableMeta(tctx *tcontext.Context, conf *Config, conn *BaseConn, db string, table *TableInfo) (TableMeta, error) {\n\ttbl := table.Name\n\tselectField, selectLen, err := buildSelectField(tctx, conn, db, tbl, conf.CompleteInsert)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar (\n\t\tcolTypes         []*sql.ColumnType\n\t\thasImplicitRowID bool\n\t)\n\tif conf.ServerInfo.ServerType == version.ServerTypeTiDB {\n\t\thasImplicitRowID, err = SelectTiDBRowID(tctx, conn, db, tbl)\n\t\tif err != nil {\n\t\t\ttctx.L().Info(\"check implicit rowID failed\", zap.String(\"database\", db), zap.String(\"table\", tbl), log.ShortError(err))\n\t\t}\n\t}\n\n\t// If all columns are generated\n\tif table.Type == TableTypeBase {\n\t\tif selectField == \"\" {\n\t\t\tcolTypes, err = GetColumnTypes(tctx, conn, \"*\", db, tbl)\n\t\t} else {\n\t\t\tcolTypes, err = GetColumnTypes(tctx, conn, selectField, db, tbl)\n\t\t}\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tmeta := &tableMeta{\n\t\tavgRowLength:     table.AvgRowLength,\n\t\tdatabase:         db,\n\t\ttable:            tbl,\n\t\tcolTypes:         colTypes,\n\t\tselectedField:    selectField,\n\t\tselectedLen:      selectLen,\n\t\thasImplicitRowID: hasImplicitRowID,\n\t\tspecCmts: []string{\n\t\t\t\"/*!40101 SET NAMES binary*/;\",\n\t\t},\n\t}\n\n\tif conf.NoSchemas {\n\t\treturn meta, nil\n\t}\n\tswitch table.Type {\n\tcase TableTypeView:\n\t\tviewName := table.Name\n\t\tcreateTableSQL, createViewSQL, err1 := ShowCreateView(tctx, conn, db, viewName)\n\t\tif err1 != nil {\n\t\t\treturn meta, err1\n\t\t}\n\t\tmeta.showCreateTable = createTableSQL\n\t\tmeta.showCreateView = createViewSQL\n\t\treturn meta, nil\n\tcase TableTypeSequence:\n\t\tsequenceName := table.Name\n\t\tcreateSequenceSQL, err2 := ShowCreateSequence(tctx, conn, db, sequenceName, conf)\n\t\tif err2 != nil {\n\t\t\treturn meta, err2\n\t\t}\n\t\tmeta.showCreateTable = createSequenceSQL\n\t\treturn meta, nil\n\t}\n\n\tcreateTableSQL, err := ShowCreateTable(tctx, conn, db, tbl)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tmeta.showCreateTable = createTableSQL\n\treturn meta, nil\n}\n\nfunc (d *Dumper) dumpSQL(tctx *tcontext.Context, metaConn *BaseConn, taskChan chan<- Task) {\n\tconf := d.conf\n\tmeta := &tableMeta{}\n\tdata := newTableData(conf.SQL, 0, true)\n\ttask := NewTaskTableData(meta, data, 0, 1)\n\tc := detectEstimateRows(tctx, metaConn, fmt.Sprintf(\"EXPLAIN %s\", conf.SQL), []string{\"rows\", \"estRows\", \"count\"})\n\tAddCounter(d.metrics.estimateTotalRowsCounter, float64(c))\n\tatomic.StoreInt64(&d.totalTables, int64(1))\n\td.sendTaskToChan(tctx, task, taskChan)\n}\n\nfunc canRebuildConn(consistency string, trxConsistencyOnly bool) bool {\n\tswitch consistency {\n\tcase ConsistencyTypeLock, ConsistencyTypeFlush:\n\t\treturn !trxConsistencyOnly\n\tcase ConsistencyTypeSnapshot, ConsistencyTypeNone:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// Close closes a Dumper and stop dumping immediately\nfunc (d *Dumper) Close() error {\n\td.cancelCtx()\n\td.metrics.unregisterFrom(d.conf.PromRegistry)\n\tif d.dbHandle != nil {\n\t\treturn d.dbHandle.Close()\n\t}\n\tif d.conf.Security.DriveTLSName != \"\" {\n\t\tmysql.DeregisterTLSConfig(d.conf.Security.DriveTLSName)\n\t}\n\treturn nil\n}\n\nfunc runSteps(d *Dumper, steps ...func(*Dumper) error) error {\n\tfor _, st := range steps {\n\t\terr := st(d)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc initLogger(d *Dumper) error {\n\tconf := d.conf\n\tvar (\n\t\tlogger log.Logger\n\t\terr    error\n\t\tprops  *pclog.ZapProperties\n\t)\n\t// conf.Logger != nil means dumpling is used as a library\n\tif conf.Logger != nil {\n\t\tlogger = log.NewAppLogger(conf.Logger)\n\t} else {\n\t\tlogger, props, err = log.InitAppLogger(&log.Config{\n\t\t\tLevel:  conf.LogLevel,\n\t\t\tFile:   conf.LogFile,\n\t\t\tFormat: conf.LogFormat,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tpclog.ReplaceGlobals(logger.Logger, props)\n\t\tcli.LogLongVersion(logger)\n\t}\n\td.tctx = d.tctx.WithLogger(logger)\n\treturn nil\n}\n\n// createExternalStore is an initialization step of Dumper.\nfunc createExternalStore(d *Dumper) error {\n\ttctx, conf := d.tctx, d.conf\n\textStore, err := conf.createExternalStorage(tctx)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\td.extStore = extStore\n\treturn nil\n}\n\n// startHTTPService is an initialization step of Dumper.\nfunc startHTTPService(d *Dumper) error {\n\tconf := d.conf\n\tif conf.StatusAddr != \"\" {\n\t\tgo func() {\n\t\t\terr := startDumplingService(d.tctx, conf.StatusAddr)\n\t\t\tif err != nil {\n\t\t\t\td.L().Info(\"meet error when stopping dumpling http service\", log.ShortError(err))\n\t\t\t}\n\t\t}()\n\t}\n\treturn nil\n}\n\n// openSQLDB is an initialization step of Dumper.\nfunc openSQLDB(d *Dumper) error {\n\tconf := d.conf\n\tpool, err := sql.Open(\"mysql\", conf.GetDSN(\"\"))\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\td.dbHandle = pool\n\treturn nil\n}\n\n// detectServerInfo is an initialization step of Dumper.\nfunc detectServerInfo(d *Dumper) error {\n\tdb, conf := d.dbHandle, d.conf\n\tversionStr, err := version.FetchVersion(d.tctx.Context, db)\n\tif err != nil {\n\t\tconf.ServerInfo = ServerInfoUnknown\n\t\treturn err\n\t}\n\tconf.ServerInfo = version.ParseServerInfo(versionStr)\n\treturn nil\n}\n\n// resolveAutoConsistency is an initialization step of Dumper.\nfunc resolveAutoConsistency(d *Dumper) error {\n\tconf := d.conf\n\tif conf.Consistency != ConsistencyTypeAuto {\n\t\treturn nil\n\t}\n\tswitch conf.ServerInfo.ServerType {\n\tcase version.ServerTypeTiDB:\n\t\tconf.Consistency = ConsistencyTypeSnapshot\n\tcase version.ServerTypeMySQL, version.ServerTypeMariaDB:\n\t\tconf.Consistency = ConsistencyTypeFlush\n\tdefault:\n\t\tconf.Consistency = ConsistencyTypeNone\n\t}\n\n\tif conf.Consistency == ConsistencyTypeFlush {\n\t\ttimeout := time.Second * 5\n\t\tctx, cancel := context.WithTimeout(d.tctx.Context, timeout)\n\t\tdefer cancel()\n\n\t\t// probe if upstream has enough privilege to FLUSH TABLE WITH READ LOCK\n\t\tconn, err := d.dbHandle.Conn(ctx)\n\t\tif err != nil {\n\t\t\treturn errors.New(\"failed to get connection from db pool after 5 seconds\")\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer conn.Close()\n\n\t\terr = FlushTableWithReadLock(d.tctx, conn)\n\t\t//nolint: errcheck\n\t\tdefer UnlockTables(d.tctx, conn)\n\t\tif err != nil {\n\t\t\t// fallback to ConsistencyTypeLock\n\t\t\td.tctx.L().Warn(\"error when use FLUSH TABLE WITH READ LOCK, fallback to LOCK TABLES\",\n\t\t\t\tzap.Error(err))\n\t\t\tconf.Consistency = ConsistencyTypeLock\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc validateResolveAutoConsistency(d *Dumper) error {\n\tconf := d.conf\n\tif conf.Consistency != ConsistencyTypeSnapshot && conf.Snapshot != \"\" {\n\t\treturn errors.Errorf(\"can't specify --snapshot when --consistency isn't snapshot, resolved consistency: %s\", conf.Consistency)\n\t}\n\treturn nil\n}\n\n// tidbSetPDClientForGC is an initialization step of Dumper.\nfunc tidbSetPDClientForGC(d *Dumper) error {\n\ttctx, si, pool := d.tctx, d.conf.ServerInfo, d.dbHandle\n\tif si.ServerType != version.ServerTypeTiDB ||\n\t\tsi.ServerVersion == nil ||\n\t\tsi.ServerVersion.Compare(*gcSafePointVersion) < 0 {\n\t\treturn nil\n\t}\n\tpdAddrs, err := GetPdAddrs(tctx, pool)\n\tif err != nil {\n\t\ttctx.L().Info(\"meet some problem while fetching pd addrs. This won't affect dump process\", log.ShortError(err))\n\t\treturn nil\n\t}\n\tif len(pdAddrs) > 0 {\n\t\tdoPdGC, err := checkSameCluster(tctx, pool, pdAddrs)\n\t\tif err != nil {\n\t\t\ttctx.L().Info(\"meet error while check whether fetched pd addr and TiDB belong to one cluster. This won't affect dump process\", log.ShortError(err), zap.Strings(\"pdAddrs\", pdAddrs))\n\t\t} else if doPdGC {\n\t\t\tpdClient, err := pd.NewClientWithContext(tctx, pdAddrs, pd.SecurityOption{})\n\t\t\tif err != nil {\n\t\t\t\ttctx.L().Info(\"create pd client to control GC failed. This won't affect dump process\", log.ShortError(err), zap.Strings(\"pdAddrs\", pdAddrs))\n\t\t\t}\n\t\t\td.tidbPDClientForGC = pdClient\n\t\t}\n\t}\n\treturn nil\n}\n\n// tidbGetSnapshot is an initialization step of Dumper.\nfunc tidbGetSnapshot(d *Dumper) error {\n\tconf, doPdGC := d.conf, d.tidbPDClientForGC != nil\n\tconsistency := conf.Consistency\n\tpool, tctx := d.dbHandle, d.tctx\n\tsnapshotConsistency := consistency == \"snapshot\"\n\tif conf.Snapshot == \"\" && (doPdGC || snapshotConsistency) {\n\t\tconn, err := pool.Conn(tctx)\n\t\tif err != nil {\n\t\t\ttctx.L().Warn(\"fail to open connection to get snapshot from TiDB\", log.ShortError(err))\n\t\t\t// for consistency snapshot, we must get a snapshot here, or we will dump inconsistent data, but for other consistency we can ignore this error.\n\t\t\tif !snapshotConsistency {\n\t\t\t\terr = nil\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\tsnapshot, err := getSnapshot(conn)\n\t\t_ = conn.Close()\n\t\tif err != nil {\n\t\t\ttctx.L().Warn(\"fail to get snapshot from TiDB\", log.ShortError(err))\n\t\t\t// for consistency snapshot, we must get a snapshot here, or we will dump inconsistent data, but for other consistency we can ignore this error.\n\t\t\tif !snapshotConsistency {\n\t\t\t\terr = nil\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\tconf.Snapshot = snapshot\n\t}\n\treturn nil\n}\n\n// tidbStartGCSavepointUpdateService is an initialization step of Dumper.\nfunc tidbStartGCSavepointUpdateService(d *Dumper) error {\n\ttctx, pool, conf := d.tctx, d.dbHandle, d.conf\n\tsnapshot, si := conf.Snapshot, conf.ServerInfo\n\tif d.tidbPDClientForGC != nil {\n\t\tsnapshotTS, err := parseSnapshotToTSO(pool, snapshot)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgo updateServiceSafePoint(tctx, d.tidbPDClientForGC, defaultDumpGCSafePointTTL, snapshotTS)\n\t} else if si.ServerType == version.ServerTypeTiDB {\n\t\ttctx.L().Warn(\"If the amount of data to dump is large, criteria: (data more than 60GB or dumped time more than 10 minutes)\\n\" +\n\t\t\t\"you'd better adjust the tikv_gc_life_time to avoid export failure due to TiDB GC during the dump process.\\n\" +\n\t\t\t\"Before dumping: run sql `update mysql.tidb set VARIABLE_VALUE = '720h' where VARIABLE_NAME = 'tikv_gc_life_time';` in tidb.\\n\" +\n\t\t\t\"After dumping: run sql `update mysql.tidb set VARIABLE_VALUE = '10m' where VARIABLE_NAME = 'tikv_gc_life_time';` in tidb.\\n\")\n\t}\n\treturn nil\n}\n\nfunc updateServiceSafePoint(tctx *tcontext.Context, pdClient pd.Client, ttl int64, snapshotTS uint64) {\n\tupdateInterval := time.Duration(ttl/2) * time.Second\n\ttick := time.NewTicker(updateInterval)\n\tdumplingServiceSafePointID := fmt.Sprintf(\"%s_%d\", dumplingServiceSafePointPrefix, time.Now().UnixNano())\n\ttctx.L().Info(\"generate dumpling gc safePoint id\", zap.String(\"id\", dumplingServiceSafePointID))\n\n\tfor {\n\t\ttctx.L().Debug(\"update PD safePoint limit with ttl\",\n\t\t\tzap.Uint64(\"safePoint\", snapshotTS),\n\t\t\tzap.Int64(\"ttl\", ttl))\n\t\tfor retryCnt := 0; retryCnt <= 10; retryCnt++ {\n\t\t\t_, err := pdClient.UpdateServiceGCSafePoint(tctx, dumplingServiceSafePointID, ttl, snapshotTS)\n\t\t\tif err == nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\ttctx.L().Debug(\"update PD safePoint failed\", log.ShortError(err), zap.Int(\"retryTime\", retryCnt))\n\t\t\tselect {\n\t\t\tcase <-tctx.Done():\n\t\t\t\treturn\n\t\t\tcase <-time.After(time.Second):\n\t\t\t}\n\t\t}\n\t\tselect {\n\t\tcase <-tctx.Done():\n\t\t\treturn\n\t\tcase <-tick.C:\n\t\t}\n\t}\n}\n\n// setSessionParam is an initialization step of Dumper.\nfunc setSessionParam(d *Dumper) error {\n\tconf, pool := d.conf, d.dbHandle\n\tsi := conf.ServerInfo\n\tconsistency, snapshot := conf.Consistency, conf.Snapshot\n\tsessionParam := conf.SessionParams\n\tif si.ServerType == version.ServerTypeTiDB && conf.TiDBMemQuotaQuery != UnspecifiedSize {\n\t\tsessionParam[TiDBMemQuotaQueryName] = conf.TiDBMemQuotaQuery\n\t}\n\tvar err error\n\tif snapshot != \"\" {\n\t\tif si.ServerType != version.ServerTypeTiDB {\n\t\t\treturn errors.New(\"snapshot consistency is not supported for this server\")\n\t\t}\n\t\tif consistency == ConsistencyTypeSnapshot {\n\t\t\tconf.ServerInfo.HasTiKV, err = CheckTiDBWithTiKV(pool)\n\t\t\tif err != nil {\n\t\t\t\td.L().Info(\"cannot check whether TiDB has TiKV, will apply tidb_snapshot by default. This won't affect dump process\", log.ShortError(err))\n\t\t\t}\n\t\t\tif conf.ServerInfo.HasTiKV {\n\t\t\t\tsessionParam[\"tidb_snapshot\"] = snapshot\n\t\t\t}\n\t\t}\n\t}\n\tif d.dbHandle, err = resetDBWithSessionParams(d.tctx, pool, conf.GetDSN(\"\"), conf.SessionParams); err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\treturn nil\n}\n\nfunc (d *Dumper) renewSelectTableRegionFuncForLowerTiDB(tctx *tcontext.Context) error {\n\tconf := d.conf\n\tif !(conf.ServerInfo.ServerType == version.ServerTypeTiDB && conf.ServerInfo.ServerVersion != nil && conf.ServerInfo.HasTiKV &&\n\t\tconf.ServerInfo.ServerVersion.Compare(*decodeRegionVersion) >= 0 &&\n\t\tconf.ServerInfo.ServerVersion.Compare(*gcSafePointVersion) < 0) {\n\t\ttctx.L().Debug(\"no need to build region info because database is not TiDB 3.x\")\n\t\treturn nil\n\t}\n\t// for TiDB v3.0+, the original selectTiDBTableRegionFunc will always fail,\n\t// because TiDB v3.0 doesn't have `tidb_decode_key` function nor `DB_NAME`,`TABLE_NAME` columns in `INFORMATION_SCHEMA.TIKV_REGION_STATUS`.\n\t// reference: https://github.com/pingcap/tidb/blob/c497d5c/dumpling/export/dump.go#L775\n\t// To avoid this function continuously returning errors and confusing users because we fail to init this function at first,\n\t// selectTiDBTableRegionFunc is set to always return an ignorable error at first.\n\td.selectTiDBTableRegionFunc = func(_ *tcontext.Context, _ *BaseConn, meta TableMeta) (pkFields []string, pkVals [][]string, err error) {\n\t\treturn nil, nil, errors.Annotatef(errEmptyHandleVals, \"table: `%s`.`%s`\", escapeString(meta.DatabaseName()), escapeString(meta.TableName()))\n\t}\n\tdbHandle, err := openDBFunc(\"mysql\", conf.GetDSN(\"\"))\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tdefer func() {\n\t\t_ = dbHandle.Close()\n\t}()\n\tconn, err := dbHandle.Conn(tctx)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tdefer func() {\n\t\t_ = conn.Close()\n\t}()\n\tdbInfos, err := GetDBInfo(conn, DatabaseTablesToMap(conf.Tables))\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tregionsInfo, err := GetRegionInfos(conn)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\ttikvHelper := &helper.Helper{}\n\ttableInfos := tikvHelper.GetRegionsTableInfo(regionsInfo, dbInfos)\n\n\ttableInfoMap := make(map[string]map[string][]int64, len(conf.Tables))\n\tfor _, region := range regionsInfo.Regions {\n\t\ttableList := tableInfos[region.ID]\n\t\tfor _, table := range tableList {\n\t\t\tdb, tbl := table.DB.Name.O, table.Table.Name.O\n\t\t\tif _, ok := tableInfoMap[db]; !ok {\n\t\t\t\ttableInfoMap[db] = make(map[string][]int64, len(conf.Tables[db]))\n\t\t\t}\n\n\t\t\tkey, err := hex.DecodeString(region.StartKey)\n\t\t\tif err != nil {\n\t\t\t\td.L().Debug(\"invalid region start key\", log.ShortError(err), zap.String(\"key\", region.StartKey))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Auto decode byte if needed.\n\t\t\t_, bs, err := codec.DecodeBytes(key, nil)\n\t\t\tif err == nil {\n\t\t\t\tkey = bs\n\t\t\t}\n\t\t\t// Try to decode it as a record key.\n\t\t\ttableID, handle, err := tablecodec.DecodeRecordKey(key)\n\t\t\tif err != nil {\n\t\t\t\td.L().Debug(\"cannot decode region start key\", log.ShortError(err), zap.String(\"key\", region.StartKey), zap.Int64(\"tableID\", tableID))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif handle.IsInt() {\n\t\t\t\ttableInfoMap[db][tbl] = append(tableInfoMap[db][tbl], handle.IntValue())\n\t\t\t} else {\n\t\t\t\td.L().Debug(\"not an int handle\", log.ShortError(err), zap.Stringer(\"handle\", handle))\n\t\t\t}\n\t\t}\n\t}\n\tfor _, tbInfos := range tableInfoMap {\n\t\tfor _, tbInfoLoop := range tbInfos {\n\t\t\t// make sure tbInfo is only used in this loop\n\t\t\ttbInfo := tbInfoLoop\n\t\t\tslices.Sort(tbInfo)\n\t\t}\n\t}\n\n\td.selectTiDBTableRegionFunc = func(tctx *tcontext.Context, conn *BaseConn, meta TableMeta) (pkFields []string, pkVals [][]string, err error) {\n\t\tpkFields, _, err = selectTiDBRowKeyFields(tctx, conn, meta, checkTiDBTableRegionPkFields)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tdbName, tableName := meta.DatabaseName(), meta.TableName()\n\t\tif tbInfos, ok := tableInfoMap[dbName]; ok {\n\t\t\tif tbInfo, ok := tbInfos[tableName]; ok {\n\t\t\t\tpkVals = make([][]string, len(tbInfo))\n\t\t\t\tfor i, val := range tbInfo {\n\t\t\t\t\tpkVals[i] = []string{strconv.FormatInt(val, 10)}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn\n\t}\n\n\treturn nil\n}\n", "// Copyright 2020 PingCAP, Inc. Licensed under Apache-2.0.\n\npackage export\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"net/url\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/go-sql-driver/mysql\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/failpoint\"\n\t\"github.com/pingcap/tidb/br/pkg/version\"\n\tdbconfig \"github.com/pingcap/tidb/config\"\n\ttcontext \"github.com/pingcap/tidb/dumpling/context\"\n\t\"github.com/pingcap/tidb/dumpling/log\"\n\t\"github.com/pingcap/tidb/errno\"\n\t\"github.com/pingcap/tidb/parser/model\"\n\t\"github.com/pingcap/tidb/store/helper\"\n\t\"go.uber.org/multierr\"\n\t\"go.uber.org/zap\"\n)\n\nconst (\n\torderByTiDBRowID = \"ORDER BY `_tidb_rowid`\"\n)\n\ntype listTableType int\n\nconst (\n\tlistTableByInfoSchema listTableType = iota\n\tlistTableByShowFullTables\n\tlistTableByShowTableStatus\n)\n\n// ShowDatabases shows the databases of a database server.\nfunc ShowDatabases(db *sql.Conn) ([]string, error) {\n\tvar res oneStrColumnTable\n\tif err := simpleQuery(db, \"SHOW DATABASES\", res.handleOneRow); err != nil {\n\t\treturn nil, err\n\t}\n\treturn res.data, nil\n}\n\n// ShowTables shows the tables of a database, the caller should use the correct database.\nfunc ShowTables(db *sql.Conn) ([]string, error) {\n\tvar res oneStrColumnTable\n\tif err := simpleQuery(db, \"SHOW TABLES\", res.handleOneRow); err != nil {\n\t\treturn nil, err\n\t}\n\treturn res.data, nil\n}\n\n// ShowCreateDatabase constructs the create database SQL for a specified database\n// returns (createDatabaseSQL, error)\nfunc ShowCreateDatabase(tctx *tcontext.Context, db *BaseConn, database string) (string, error) {\n\tvar oneRow [2]string\n\thandleOneRow := func(rows *sql.Rows) error {\n\t\treturn rows.Scan(&oneRow[0], &oneRow[1])\n\t}\n\tquery := fmt.Sprintf(\"SHOW CREATE DATABASE `%s`\", escapeString(database))\n\terr := db.QuerySQL(tctx, handleOneRow, func() {\n\t\toneRow[0], oneRow[1] = \"\", \"\"\n\t}, query)\n\tif multiErrs := multierr.Errors(err); len(multiErrs) > 0 {\n\t\tfor _, multiErr := range multiErrs {\n\t\t\tif mysqlErr, ok := errors.Cause(multiErr).(*mysql.MySQLError); ok {\n\t\t\t\t// Falling back to simple create statement for MemSQL/SingleStore, because of this:\n\t\t\t\t// ERROR 1706 (HY000): Feature 'SHOW CREATE DATABASE' is not supported by MemSQL.\n\t\t\t\tif strings.Contains(mysqlErr.Error(), \"SHOW CREATE DATABASE\") {\n\t\t\t\t\treturn fmt.Sprintf(\"CREATE DATABASE `%s`\", escapeString(database)), nil\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn oneRow[1], err\n}\n\n// ShowCreateTable constructs the create table SQL for a specified table\n// returns (createTableSQL, error)\nfunc ShowCreateTable(tctx *tcontext.Context, db *BaseConn, database, table string) (string, error) {\n\tvar oneRow [2]string\n\thandleOneRow := func(rows *sql.Rows) error {\n\t\treturn rows.Scan(&oneRow[0], &oneRow[1])\n\t}\n\tquery := fmt.Sprintf(\"SHOW CREATE TABLE `%s`.`%s`\", escapeString(database), escapeString(table))\n\terr := db.QuerySQL(tctx, handleOneRow, func() {\n\t\toneRow[0], oneRow[1] = \"\", \"\"\n\t}, query)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn oneRow[1], nil\n}\n\n// ShowCreatePlacementPolicy constructs the create policy SQL for a specified table\n// returns (createPolicySQL, error)\nfunc ShowCreatePlacementPolicy(tctx *tcontext.Context, db *BaseConn, policy string) (string, error) {\n\tvar oneRow [2]string\n\thandleOneRow := func(rows *sql.Rows) error {\n\t\treturn rows.Scan(&oneRow[0], &oneRow[1])\n\t}\n\tquery := fmt.Sprintf(\"SHOW CREATE PLACEMENT POLICY `%s`\", escapeString(policy))\n\terr := db.QuerySQL(tctx, handleOneRow, func() {\n\t\toneRow[0], oneRow[1] = \"\", \"\"\n\t}, query)\n\treturn oneRow[1], err\n}\n\n// ShowCreateView constructs the create view SQL for a specified view\n// returns (createFakeTableSQL, createViewSQL, error)\nfunc ShowCreateView(tctx *tcontext.Context, db *BaseConn, database, view string) (createFakeTableSQL string, createRealViewSQL string, err error) {\n\tvar fieldNames []string\n\thandleFieldRow := func(rows *sql.Rows) error {\n\t\tvar oneRow [6]sql.NullString\n\t\tscanErr := rows.Scan(&oneRow[0], &oneRow[1], &oneRow[2], &oneRow[3], &oneRow[4], &oneRow[5])\n\t\tif scanErr != nil {\n\t\t\treturn errors.Trace(scanErr)\n\t\t}\n\t\tif oneRow[0].Valid {\n\t\t\tfieldNames = append(fieldNames, fmt.Sprintf(\"`%s` int\", escapeString(oneRow[0].String)))\n\t\t}\n\t\treturn nil\n\t}\n\tvar oneRow [4]string\n\thandleOneRow := func(rows *sql.Rows) error {\n\t\treturn rows.Scan(&oneRow[0], &oneRow[1], &oneRow[2], &oneRow[3])\n\t}\n\tvar createTableSQL, createViewSQL strings.Builder\n\n\t// Build createTableSQL\n\tquery := fmt.Sprintf(\"SHOW FIELDS FROM `%s`.`%s`\", escapeString(database), escapeString(view))\n\terr = db.QuerySQL(tctx, handleFieldRow, func() {\n\t\tfieldNames = []string{}\n\t}, query)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\tfmt.Fprintf(&createTableSQL, \"CREATE TABLE `%s`(\\n\", escapeString(view))\n\tcreateTableSQL.WriteString(strings.Join(fieldNames, \",\\n\"))\n\tcreateTableSQL.WriteString(\"\\n)ENGINE=MyISAM;\\n\")\n\n\t// Build createViewSQL\n\tfmt.Fprintf(&createViewSQL, \"DROP TABLE IF EXISTS `%s`;\\n\", escapeString(view))\n\tfmt.Fprintf(&createViewSQL, \"DROP VIEW IF EXISTS `%s`;\\n\", escapeString(view))\n\tquery = fmt.Sprintf(\"SHOW CREATE VIEW `%s`.`%s`\", escapeString(database), escapeString(view))\n\terr = db.QuerySQL(tctx, handleOneRow, func() {\n\t\tfor i := range oneRow {\n\t\t\toneRow[i] = \"\"\n\t\t}\n\t}, query)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\t// The result for `show create view` SQL\n\t// mysql> show create view v1;\n\t// +------+-------------------------------------------------------------------------------------------------------------------------------------+----------------------+----------------------+\n\t// | View | Create View                                                                                                                         | character_set_client | collation_connection |\n\t// +------+-------------------------------------------------------------------------------------------------------------------------------------+----------------------+----------------------+\n\t// | v1   | CREATE ALGORITHM=UNDEFINED DEFINER=`root`@`localhost` SQL SECURITY DEFINER VIEW `v1` (`a`) AS SELECT `t`.`a` AS `a` FROM `test`.`t` | utf8                 | utf8_general_ci      |\n\t// +------+-------------------------------------------------------------------------------------------------------------------------------------+----------------------+----------------------+\n\tSetCharset(&createViewSQL, oneRow[2], oneRow[3])\n\tcreateViewSQL.WriteString(oneRow[1])\n\tcreateViewSQL.WriteString(\";\\n\")\n\tRestoreCharset(&createViewSQL)\n\n\treturn createTableSQL.String(), createViewSQL.String(), nil\n}\n\n// ShowCreateSequence constructs the create sequence SQL for a specified sequence\n// returns (createSequenceSQL, error)\nfunc ShowCreateSequence(tctx *tcontext.Context, db *BaseConn, database, sequence string, conf *Config) (string, error) {\n\tvar oneRow [2]string\n\thandleOneRow := func(rows *sql.Rows) error {\n\t\treturn rows.Scan(&oneRow[0], &oneRow[1])\n\t}\n\tvar (\n\t\tcreateSequenceSQL  strings.Builder\n\t\tnextNotCachedValue int64\n\t)\n\tquery := fmt.Sprintf(\"SHOW CREATE SEQUENCE `%s`.`%s`\", escapeString(database), escapeString(sequence))\n\terr := db.QuerySQL(tctx, handleOneRow, func() {\n\t\toneRow[0], oneRow[1] = \"\", \"\"\n\t}, query)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tcreateSequenceSQL.WriteString(oneRow[1])\n\tcreateSequenceSQL.WriteString(\";\\n\")\n\n\tswitch conf.ServerInfo.ServerType {\n\tcase version.ServerTypeTiDB:\n\t\t// Get next not allocated auto increment id of the whole cluster\n\t\tquery := fmt.Sprintf(\"SHOW TABLE `%s`.`%s` NEXT_ROW_ID\", escapeString(database), escapeString(sequence))\n\t\tresults, err := db.QuerySQLWithColumns(tctx, []string{\"NEXT_GLOBAL_ROW_ID\", \"ID_TYPE\"}, query)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tfor _, oneRow := range results {\n\t\t\tnextGlobalRowID, idType := oneRow[0], oneRow[1]\n\t\t\tif idType == \"SEQUENCE\" {\n\t\t\t\tnextNotCachedValue, _ = strconv.ParseInt(nextGlobalRowID, 10, 64)\n\t\t\t}\n\t\t}\n\t\tfmt.Fprintf(&createSequenceSQL, \"SELECT SETVAL(`%s`,%d);\\n\", escapeString(sequence), nextNotCachedValue)\n\tcase version.ServerTypeMariaDB:\n\t\tvar oneRow1 string\n\t\thandleOneRow1 := func(rows *sql.Rows) error {\n\t\t\treturn rows.Scan(&oneRow1)\n\t\t}\n\t\tquery := fmt.Sprintf(\"SELECT NEXT_NOT_CACHED_VALUE FROM `%s`.`%s`\", escapeString(database), escapeString(sequence))\n\t\terr := db.QuerySQL(tctx, handleOneRow1, func() {\n\t\t\toneRow1 = \"\"\n\t\t}, query)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tnextNotCachedValue, _ = strconv.ParseInt(oneRow1, 10, 64)\n\t\tfmt.Fprintf(&createSequenceSQL, \"SELECT SETVAL(`%s`,%d);\\n\", escapeString(sequence), nextNotCachedValue)\n\t}\n\treturn createSequenceSQL.String(), nil\n}\n\n// SetCharset builds the set charset SQLs\nfunc SetCharset(w *strings.Builder, characterSet, collationConnection string) {\n\tw.WriteString(\"SET @PREV_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT;\\n\")\n\tw.WriteString(\"SET @PREV_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS;\\n\")\n\tw.WriteString(\"SET @PREV_COLLATION_CONNECTION=@@COLLATION_CONNECTION;\\n\")\n\n\tfmt.Fprintf(w, \"SET character_set_client = %s;\\n\", characterSet)\n\tfmt.Fprintf(w, \"SET character_set_results = %s;\\n\", characterSet)\n\tfmt.Fprintf(w, \"SET collation_connection = %s;\\n\", collationConnection)\n}\n\n// RestoreCharset builds the restore charset SQLs\nfunc RestoreCharset(w io.StringWriter) {\n\t_, _ = w.WriteString(\"SET character_set_client = @PREV_CHARACTER_SET_CLIENT;\\n\")\n\t_, _ = w.WriteString(\"SET character_set_results = @PREV_CHARACTER_SET_RESULTS;\\n\")\n\t_, _ = w.WriteString(\"SET collation_connection = @PREV_COLLATION_CONNECTION;\\n\")\n}\n\n// ListAllDatabasesTables lists all the databases and tables from the database\n// listTableByInfoSchema list tables by table information_schema in MySQL\n// listTableByShowTableStatus has better performance than listTableByInfoSchema\n// listTableByShowFullTables is used in mysql8 version [8.0.3,8.0.23), more details can be found in the comments of func matchMysqlBugversion\nfunc ListAllDatabasesTables(tctx *tcontext.Context, db *sql.Conn, databaseNames []string,\n\tlistType listTableType, tableTypes ...TableType) (DatabaseTables, error) { // revive:disable-line:flag-parameter\n\tdbTables := DatabaseTables{}\n\tvar (\n\t\tschema, table, tableTypeStr string\n\t\ttableType                   TableType\n\t\tavgRowLength                uint64\n\t\terr                         error\n\t)\n\n\ttableTypeConditions := make([]string, len(tableTypes))\n\tfor i, tableType := range tableTypes {\n\t\ttableTypeConditions[i] = fmt.Sprintf(\"TABLE_TYPE='%s'\", tableType)\n\t}\n\tswitch listType {\n\tcase listTableByInfoSchema:\n\t\tquery := fmt.Sprintf(\"SELECT TABLE_SCHEMA,TABLE_NAME,TABLE_TYPE,AVG_ROW_LENGTH FROM INFORMATION_SCHEMA.TABLES WHERE %s\", strings.Join(tableTypeConditions, \" OR \"))\n\t\tfor _, schema := range databaseNames {\n\t\t\tdbTables[schema] = make([]*TableInfo, 0)\n\t\t}\n\t\tif err = simpleQueryWithArgs(tctx, db, func(rows *sql.Rows) error {\n\t\t\tvar (\n\t\t\t\tsqlAvgRowLength sql.NullInt64\n\t\t\t\terr2            error\n\t\t\t)\n\t\t\tif err2 = rows.Scan(&schema, &table, &tableTypeStr, &sqlAvgRowLength); err != nil {\n\t\t\t\treturn errors.Trace(err2)\n\t\t\t}\n\t\t\ttableType, err2 = ParseTableType(tableTypeStr)\n\t\t\tif err2 != nil {\n\t\t\t\treturn errors.Trace(err2)\n\t\t\t}\n\n\t\t\tif sqlAvgRowLength.Valid {\n\t\t\t\tavgRowLength = uint64(sqlAvgRowLength.Int64)\n\t\t\t} else {\n\t\t\t\tavgRowLength = 0\n\t\t\t}\n\t\t\t// only append tables to schemas in databaseNames\n\t\t\tif _, ok := dbTables[schema]; ok {\n\t\t\t\tdbTables[schema] = append(dbTables[schema], &TableInfo{table, avgRowLength, tableType})\n\t\t\t}\n\t\t\treturn nil\n\t\t}, query); err != nil {\n\t\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t\t}\n\tcase listTableByShowFullTables:\n\t\tfor _, schema = range databaseNames {\n\t\t\tdbTables[schema] = make([]*TableInfo, 0)\n\t\t\tquery := fmt.Sprintf(\"SHOW FULL TABLES FROM `%s` WHERE %s\",\n\t\t\t\tescapeString(schema), strings.Join(tableTypeConditions, \" OR \"))\n\t\t\tif err = simpleQueryWithArgs(tctx, db, func(rows *sql.Rows) error {\n\t\t\t\tvar err2 error\n\t\t\t\tif err2 = rows.Scan(&table, &tableTypeStr); err != nil {\n\t\t\t\t\treturn errors.Trace(err2)\n\t\t\t\t}\n\t\t\t\ttableType, err2 = ParseTableType(tableTypeStr)\n\t\t\t\tif err2 != nil {\n\t\t\t\t\treturn errors.Trace(err2)\n\t\t\t\t}\n\t\t\t\tavgRowLength = 0 // can't get avgRowLength from the result of `show full tables` so hardcode to 0 here\n\t\t\t\tdbTables[schema] = append(dbTables[schema], &TableInfo{table, avgRowLength, tableType})\n\t\t\t\treturn nil\n\t\t\t}, query); err != nil {\n\t\t\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t\t\t}\n\t\t}\n\tdefault:\n\t\tconst queryTemplate = \"SHOW TABLE STATUS FROM `%s`\"\n\t\tselectedTableType := make(map[TableType]struct{})\n\t\tfor _, tableType = range tableTypes {\n\t\t\tselectedTableType[tableType] = struct{}{}\n\t\t}\n\t\tfor _, schema = range databaseNames {\n\t\t\tdbTables[schema] = make([]*TableInfo, 0)\n\t\t\tquery := fmt.Sprintf(queryTemplate, escapeString(schema))\n\t\t\trows, err := db.QueryContext(tctx, query)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t\t\t}\n\t\t\tresults, err := GetSpecifiedColumnValuesAndClose(rows, \"NAME\", \"ENGINE\", \"AVG_ROW_LENGTH\", \"COMMENT\")\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t\t\t}\n\t\t\tfor _, oneRow := range results {\n\t\t\t\ttable, engine, avgRowLengthStr, comment := oneRow[0], oneRow[1], oneRow[2], oneRow[3]\n\t\t\t\tif avgRowLengthStr != \"\" {\n\t\t\t\t\tavgRowLength, err = strconv.ParseUint(avgRowLengthStr, 10, 64)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tavgRowLength = 0\n\t\t\t\t}\n\t\t\t\ttableType = TableTypeBase\n\t\t\t\tif engine == \"\" && (comment == \"\" || comment == TableTypeViewStr) {\n\t\t\t\t\ttableType = TableTypeView\n\t\t\t\t} else if engine == \"\" {\n\t\t\t\t\ttctx.L().Warn(\"invalid table without engine found\", zap.String(\"database\", schema), zap.String(\"table\", table))\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif _, ok := selectedTableType[tableType]; !ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tdbTables[schema] = append(dbTables[schema], &TableInfo{table, avgRowLength, tableType})\n\t\t\t}\n\t\t}\n\t}\n\treturn dbTables, nil\n}\n\n// ListAllPlacementPolicyNames returns all placement policy names.\nfunc ListAllPlacementPolicyNames(tctx *tcontext.Context, db *BaseConn) ([]string, error) {\n\tvar policyList []string\n\tvar policy string\n\tconst query = \"select distinct policy_name from information_schema.placement_policies where policy_name is not null;\"\n\terr := db.QuerySQL(tctx, func(rows *sql.Rows) error {\n\t\terr := rows.Scan(&policy)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tpolicyList = append(policyList, policy)\n\t\treturn nil\n\t}, func() {\n\t\tpolicyList = policyList[:0]\n\t}, query)\n\treturn policyList, errors.Annotatef(err, \"sql: %s\", query)\n}\n\n// SelectVersion gets the version information from the database server\nfunc SelectVersion(db *sql.DB) (string, error) {\n\tvar versionInfo string\n\tconst query = \"SELECT version()\"\n\trow := db.QueryRow(query)\n\terr := row.Scan(&versionInfo)\n\tif err != nil {\n\t\treturn \"\", errors.Annotatef(err, \"sql: %s\", query)\n\t}\n\treturn versionInfo, nil\n}\n\n// SelectAllFromTable dumps data serialized from a specified table\nfunc SelectAllFromTable(conf *Config, meta TableMeta, partition, orderByClause string) TableDataIR {\n\tdatabase, table := meta.DatabaseName(), meta.TableName()\n\tselectedField, selectLen := meta.SelectedField(), meta.SelectedLen()\n\tquery := buildSelectQuery(database, table, selectedField, partition, buildWhereCondition(conf, \"\"), orderByClause)\n\n\treturn &tableData{\n\t\tquery:  query,\n\t\tcolLen: selectLen,\n\t}\n}\n\nfunc buildSelectQuery(database, table, fields, partition, where, orderByClause string) string {\n\tvar query strings.Builder\n\tquery.WriteString(\"SELECT \")\n\tif fields == \"\" {\n\t\t// If all of the columns are generated,\n\t\t// we need to make sure the query is valid.\n\t\tfields = \"''\"\n\t}\n\tquery.WriteString(fields)\n\tquery.WriteString(\" FROM `\")\n\tquery.WriteString(escapeString(database))\n\tquery.WriteString(\"`.`\")\n\tquery.WriteString(escapeString(table))\n\tquery.WriteByte('`')\n\tif partition != \"\" {\n\t\tquery.WriteString(\" PARTITION(`\")\n\t\tquery.WriteString(escapeString(partition))\n\t\tquery.WriteString(\"`)\")\n\t}\n\n\tif where != \"\" {\n\t\tquery.WriteString(\" \")\n\t\tquery.WriteString(where)\n\t}\n\n\tif orderByClause != \"\" {\n\t\tquery.WriteString(\" \")\n\t\tquery.WriteString(orderByClause)\n\t}\n\n\treturn query.String()\n}\n\nfunc buildOrderByClause(tctx *tcontext.Context, conf *Config, db *BaseConn, database, table string, hasImplicitRowID bool) (string, error) { // revive:disable-line:flag-parameter\n\tif !conf.SortByPk {\n\t\treturn \"\", nil\n\t}\n\tif hasImplicitRowID {\n\t\treturn orderByTiDBRowID, nil\n\t}\n\tcols, err := GetPrimaryKeyColumns(tctx, db, database, table)\n\tif err != nil {\n\t\treturn \"\", errors.Trace(err)\n\t}\n\treturn buildOrderByClauseString(cols), nil\n}\n\n// SelectTiDBRowID checks whether this table has _tidb_rowid column\nfunc SelectTiDBRowID(tctx *tcontext.Context, db *BaseConn, database, table string) (bool, error) {\n\ttiDBRowIDQuery := fmt.Sprintf(\"SELECT _tidb_rowid from `%s`.`%s` LIMIT 1\", escapeString(database), escapeString(table))\n\thasImplictRowID := false\n\terr := db.ExecSQL(tctx, func(_ sql.Result, err error) error {\n\t\tif err != nil {\n\t\t\thasImplictRowID = false\n\t\t\terrMsg := strings.ToLower(err.Error())\n\t\t\tif strings.Contains(errMsg, fmt.Sprintf(\"%d\", errno.ErrBadField)) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn errors.Annotatef(err, \"sql: %s\", tiDBRowIDQuery)\n\t\t}\n\t\thasImplictRowID = true\n\t\treturn nil\n\t}, tiDBRowIDQuery)\n\treturn hasImplictRowID, err\n}\n\n// GetSuitableRows gets suitable rows for each table\nfunc GetSuitableRows(avgRowLength uint64) uint64 {\n\tconst (\n\t\tdefaultRows  = 200000\n\t\tmaxRows      = 1000000\n\t\tbytesPerFile = 128 * 1024 * 1024 // 128MB per file by default\n\t)\n\tif avgRowLength == 0 {\n\t\treturn defaultRows\n\t}\n\testimateRows := bytesPerFile / avgRowLength\n\tif estimateRows > maxRows {\n\t\treturn maxRows\n\t}\n\treturn estimateRows\n}\n\n// GetColumnTypes gets *sql.ColumnTypes from a specified table\nfunc GetColumnTypes(tctx *tcontext.Context, db *BaseConn, fields, database, table string) ([]*sql.ColumnType, error) {\n\tquery := fmt.Sprintf(\"SELECT %s FROM `%s`.`%s` LIMIT 1\", fields, escapeString(database), escapeString(table))\n\tvar colTypes []*sql.ColumnType\n\terr := db.QuerySQL(tctx, func(rows *sql.Rows) error {\n\t\tvar err error\n\t\tcolTypes, err = rows.ColumnTypes()\n\t\tif err == nil {\n\t\t\terr = rows.Close()\n\t\t}\n\t\tfailpoint.Inject(\"ChaosBrokenMetaConn\", func(_ failpoint.Value) {\n\t\t\tfailpoint.Return(errors.New(\"connection is closed\"))\n\t\t})\n\t\treturn errors.Annotatef(err, \"sql: %s\", query)\n\t}, func() {\n\t\tcolTypes = nil\n\t}, query)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn colTypes, nil\n}\n\n// GetPrimaryKeyAndColumnTypes gets all primary columns and their types in ordinal order\nfunc GetPrimaryKeyAndColumnTypes(tctx *tcontext.Context, conn *BaseConn, meta TableMeta) ([]string, []string, error) {\n\tvar (\n\t\tcolNames, colTypes []string\n\t\terr                error\n\t)\n\tcolNames, err = GetPrimaryKeyColumns(tctx, conn, meta.DatabaseName(), meta.TableName())\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tcolName2Type := string2Map(meta.ColumnNames(), meta.ColumnTypes())\n\tcolTypes = make([]string, len(colNames))\n\tfor i, colName := range colNames {\n\t\tcolTypes[i] = colName2Type[colName]\n\t}\n\treturn colNames, colTypes, nil\n}\n\n// GetPrimaryKeyColumns gets all primary columns in ordinal order\nfunc GetPrimaryKeyColumns(tctx *tcontext.Context, db *BaseConn, database, table string) ([]string, error) {\n\tpriKeyColsQuery := fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", escapeString(database), escapeString(table))\n\tresults, err := db.QuerySQLWithColumns(tctx, []string{\"KEY_NAME\", \"COLUMN_NAME\"}, priKeyColsQuery)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcols := make([]string, 0, len(results))\n\tfor _, oneRow := range results {\n\t\tkeyName, columnName := oneRow[0], oneRow[1]\n\t\tif keyName == \"PRIMARY\" {\n\t\t\tcols = append(cols, columnName)\n\t\t}\n\t}\n\treturn cols, nil\n}\n\n// getNumericIndex picks up indices according to the following priority:\n// primary key > unique key with the smallest count > key with the max cardinality\n// primary key with multi cols is before unique key with single col because we will sort result by primary keys\nfunc getNumericIndex(tctx *tcontext.Context, db *BaseConn, meta TableMeta) (string, error) {\n\tdatabase, table := meta.DatabaseName(), meta.TableName()\n\tcolName2Type := string2Map(meta.ColumnNames(), meta.ColumnTypes())\n\tkeyQuery := fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", escapeString(database), escapeString(table))\n\tresults, err := db.QuerySQLWithColumns(tctx, []string{\"NON_UNIQUE\", \"SEQ_IN_INDEX\", \"KEY_NAME\", \"COLUMN_NAME\", \"CARDINALITY\"}, keyQuery)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\ttype keyColumnPair struct {\n\t\tcolName string\n\t\tcount   uint64\n\t}\n\tvar (\n\t\tuniqueKeyMap   = map[string]keyColumnPair{} // unique key name -> key column name, unique key columns count\n\t\tkeyColumn      string\n\t\tmaxCardinality int64 = -1\n\t)\n\n\t// check primary key first, then unique key\n\tfor _, oneRow := range results {\n\t\tnonUnique, seqInIndex, keyName, colName, cardinality := oneRow[0], oneRow[1], oneRow[2], oneRow[3], oneRow[4]\n\t\t// only try pick the first column, because the second column of pk/uk in where condition will trigger a full table scan\n\t\tif seqInIndex != \"1\" {\n\t\t\tif pair, ok := uniqueKeyMap[keyName]; ok {\n\t\t\t\tseqInIndexInt, err := strconv.ParseUint(seqInIndex, 10, 64)\n\t\t\t\tif err == nil && seqInIndexInt > pair.count {\n\t\t\t\t\tuniqueKeyMap[keyName] = keyColumnPair{pair.colName, seqInIndexInt}\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\t_, numberColumn := dataTypeInt[colName2Type[colName]]\n\t\tif numberColumn {\n\t\t\tswitch {\n\t\t\tcase keyName == \"PRIMARY\":\n\t\t\t\treturn colName, nil\n\t\t\tcase nonUnique == \"0\":\n\t\t\t\tuniqueKeyMap[keyName] = keyColumnPair{colName, 1}\n\t\t\t// pick index column with max cardinality when there is no unique index\n\t\t\tcase len(uniqueKeyMap) == 0:\n\t\t\t\tcardinalityInt, err := strconv.ParseInt(cardinality, 10, 64)\n\t\t\t\tif err == nil && cardinalityInt > maxCardinality {\n\t\t\t\t\tkeyColumn = colName\n\t\t\t\t\tmaxCardinality = cardinalityInt\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif len(uniqueKeyMap) > 0 {\n\t\tvar (\n\t\t\tminCols         uint64 = math.MaxUint64\n\t\t\tuniqueKeyColumn string\n\t\t)\n\t\tfor _, pair := range uniqueKeyMap {\n\t\t\tif pair.count < minCols {\n\t\t\t\tuniqueKeyColumn = pair.colName\n\t\t\t\tminCols = pair.count\n\t\t\t}\n\t\t}\n\t\treturn uniqueKeyColumn, nil\n\t}\n\treturn keyColumn, nil\n}\n\n// FlushTableWithReadLock flush tables with read lock\nfunc FlushTableWithReadLock(ctx context.Context, db *sql.Conn) error {\n\tconst ftwrlQuery = \"FLUSH TABLES WITH READ LOCK\"\n\t_, err := db.ExecContext(ctx, ftwrlQuery)\n\treturn errors.Annotatef(err, \"sql: %s\", ftwrlQuery)\n}\n\n// LockTables locks table with read lock\nfunc LockTables(ctx context.Context, db *sql.Conn, database, table string) error {\n\tlockTableQuery := fmt.Sprintf(\"LOCK TABLES `%s`.`%s` READ\", escapeString(database), escapeString(table))\n\t_, err := db.ExecContext(ctx, lockTableQuery)\n\treturn errors.Annotatef(err, \"sql: %s\", lockTableQuery)\n}\n\n// UnlockTables unlocks all tables' lock\nfunc UnlockTables(ctx context.Context, db *sql.Conn) error {\n\tconst unlockTableQuery = \"UNLOCK TABLES\"\n\t_, err := db.ExecContext(ctx, unlockTableQuery)\n\treturn errors.Annotatef(err, \"sql: %s\", unlockTableQuery)\n}\n\n// ShowMasterStatus get SHOW MASTER STATUS result from database\nfunc ShowMasterStatus(db *sql.Conn) ([]string, error) {\n\tvar oneRow []string\n\thandleOneRow := func(rows *sql.Rows) error {\n\t\tcols, err := rows.Columns()\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tfieldNum := len(cols)\n\t\toneRow = make([]string, fieldNum)\n\t\taddr := make([]interface{}, fieldNum)\n\t\tfor i := range oneRow {\n\t\t\taddr[i] = &oneRow[i]\n\t\t}\n\t\treturn rows.Scan(addr...)\n\t}\n\tconst showMasterStatusQuery = \"SHOW MASTER STATUS\"\n\terr := simpleQuery(db, showMasterStatusQuery, handleOneRow)\n\tif err != nil {\n\t\treturn nil, errors.Annotatef(err, \"sql: %s\", showMasterStatusQuery)\n\t}\n\treturn oneRow, nil\n}\n\n// GetSpecifiedColumnValueAndClose get columns' values whose name is equal to columnName and close the given rows\nfunc GetSpecifiedColumnValueAndClose(rows *sql.Rows, columnName string) ([]string, error) {\n\tif rows == nil {\n\t\treturn []string{}, nil\n\t}\n\tdefer rows.Close()\n\tvar strs []string\n\tcolumns, _ := rows.Columns()\n\taddr := make([]interface{}, len(columns))\n\toneRow := make([]sql.NullString, len(columns))\n\tfieldIndex := -1\n\tfor i, col := range columns {\n\t\tif strings.EqualFold(col, columnName) {\n\t\t\tfieldIndex = i\n\t\t}\n\t\taddr[i] = &oneRow[i]\n\t}\n\tif fieldIndex == -1 {\n\t\treturn strs, nil\n\t}\n\tfor rows.Next() {\n\t\terr := rows.Scan(addr...)\n\t\tif err != nil {\n\t\t\treturn strs, errors.Trace(err)\n\t\t}\n\t\tif oneRow[fieldIndex].Valid {\n\t\t\tstrs = append(strs, oneRow[fieldIndex].String)\n\t\t}\n\t}\n\treturn strs, errors.Trace(rows.Err())\n}\n\n// GetSpecifiedColumnValuesAndClose get columns' values whose name is equal to columnName\nfunc GetSpecifiedColumnValuesAndClose(rows *sql.Rows, columnName ...string) ([][]string, error) {\n\tif rows == nil {\n\t\treturn [][]string{}, nil\n\t}\n\tdefer rows.Close()\n\tvar strs [][]string\n\tcolumns, err := rows.Columns()\n\tif err != nil {\n\t\treturn strs, errors.Trace(err)\n\t}\n\taddr := make([]interface{}, len(columns))\n\toneRow := make([]sql.NullString, len(columns))\n\tfieldIndexMp := make(map[int]int)\n\tfor i, col := range columns {\n\t\taddr[i] = &oneRow[i]\n\t\tfor j, name := range columnName {\n\t\t\tif strings.EqualFold(col, name) {\n\t\t\t\tfieldIndexMp[i] = j\n\t\t\t}\n\t\t}\n\t}\n\tif len(fieldIndexMp) == 0 {\n\t\treturn strs, nil\n\t}\n\tfor rows.Next() {\n\t\terr := rows.Scan(addr...)\n\t\tif err != nil {\n\t\t\treturn strs, errors.Trace(err)\n\t\t}\n\t\twritten := false\n\t\ttmpStr := make([]string, len(columnName))\n\t\tfor colPos, namePos := range fieldIndexMp {\n\t\t\tif oneRow[colPos].Valid {\n\t\t\t\twritten = true\n\t\t\t\ttmpStr[namePos] = oneRow[colPos].String\n\t\t\t}\n\t\t}\n\t\tif written {\n\t\t\tstrs = append(strs, tmpStr)\n\t\t}\n\t}\n\treturn strs, errors.Trace(rows.Err())\n}\n\n// GetPdAddrs gets PD address from TiDB\nfunc GetPdAddrs(tctx *tcontext.Context, db *sql.DB) ([]string, error) {\n\tconst query = \"SELECT * FROM information_schema.cluster_info where type = 'pd';\"\n\trows, err := db.QueryContext(tctx, query)\n\tif err != nil {\n\t\treturn []string{}, errors.Annotatef(err, \"sql: %s\", query)\n\t}\n\tpdAddrs, err := GetSpecifiedColumnValueAndClose(rows, \"STATUS_ADDRESS\")\n\treturn pdAddrs, errors.Annotatef(err, \"sql: %s\", query)\n}\n\n// GetTiDBDDLIDs gets DDL IDs from TiDB\nfunc GetTiDBDDLIDs(tctx *tcontext.Context, db *sql.DB) ([]string, error) {\n\tconst query = \"SELECT * FROM information_schema.tidb_servers_info;\"\n\trows, err := db.QueryContext(tctx, query)\n\tif err != nil {\n\t\treturn []string{}, errors.Annotatef(err, \"sql: %s\", query)\n\t}\n\tddlIDs, err := GetSpecifiedColumnValueAndClose(rows, \"DDL_ID\")\n\treturn ddlIDs, errors.Annotatef(err, \"sql: %s\", query)\n}\n\n// getTiDBConfig gets tidb config from TiDB server\n// @@tidb_config details doc https://docs.pingcap.com/tidb/stable/system-variables#tidb_config\n// this variable exists at least from v2.0.0, so this works in most existing tidb instances\nfunc getTiDBConfig(db *sql.Conn) (dbconfig.Config, error) {\n\tconst query = \"SELECT @@tidb_config;\"\n\tvar (\n\t\ttidbConfig      dbconfig.Config\n\t\ttidbConfigBytes []byte\n\t)\n\trow := db.QueryRowContext(context.Background(), query)\n\terr := row.Scan(&tidbConfigBytes)\n\tif err != nil {\n\t\treturn tidbConfig, errors.Annotatef(err, \"sql: %s\", query)\n\t}\n\terr = json.Unmarshal(tidbConfigBytes, &tidbConfig)\n\treturn tidbConfig, errors.Annotatef(err, \"sql: %s\", query)\n}\n\n// CheckTiDBWithTiKV use sql to check whether current TiDB has TiKV\nfunc CheckTiDBWithTiKV(db *sql.DB) (bool, error) {\n\tconn, err := db.Conn(context.Background())\n\tif err == nil {\n\t\tdefer func() {\n\t\t\t_ = conn.Close()\n\t\t}()\n\t\ttidbConfig, err := getTiDBConfig(conn)\n\t\tif err == nil {\n\t\t\treturn tidbConfig.Store == \"tikv\", nil\n\t\t}\n\t}\n\tvar count int\n\tconst query = \"SELECT COUNT(1) as c FROM MYSQL.TiDB WHERE VARIABLE_NAME='tikv_gc_safe_point'\"\n\trow := db.QueryRow(query)\n\terr = row.Scan(&count)\n\tif err != nil {\n\t\t// still return true here. Because sometimes users may not have privileges for MySQL.TiDB database\n\t\t// In most production cases TiDB has TiKV\n\t\treturn true, errors.Annotatef(err, \"sql: %s\", query)\n\t}\n\treturn count > 0, nil\n}\n\n// CheckIfSeqExists use sql to check whether sequence exists\nfunc CheckIfSeqExists(db *sql.Conn) (bool, error) {\n\tvar count int\n\tconst query = \"SELECT COUNT(1) as c FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE='SEQUENCE'\"\n\trow := db.QueryRowContext(context.Background(), query)\n\terr := row.Scan(&count)\n\tif err != nil {\n\t\treturn false, errors.Annotatef(err, \"sql: %s\", query)\n\t}\n\n\treturn count > 0, nil\n}\n\n// CheckTiDBEnableTableLock use sql variable to check whether current TiDB has TiKV\nfunc CheckTiDBEnableTableLock(db *sql.Conn) (bool, error) {\n\ttidbConfig, err := getTiDBConfig(db)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\treturn tidbConfig.EnableTableLock, nil\n}\n\nfunc getSnapshot(db *sql.Conn) (string, error) {\n\tstr, err := ShowMasterStatus(db)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn str[snapshotFieldIndex], nil\n}\n\nfunc isUnknownSystemVariableErr(err error) bool {\n\treturn strings.Contains(err.Error(), \"Unknown system variable\")\n}\n\n// resetDBWithSessionParams will return a new sql.DB as a replacement for input `db` with new session parameters.\n// If returned error is nil, the input `db` will be closed.\nfunc resetDBWithSessionParams(tctx *tcontext.Context, db *sql.DB, dsn string, params map[string]interface{}) (*sql.DB, error) {\n\tsupport := make(map[string]interface{})\n\tfor k, v := range params {\n\t\tvar pv interface{}\n\t\tif str, ok := v.(string); ok {\n\t\t\tif pvi, err := strconv.ParseInt(str, 10, 64); err == nil {\n\t\t\t\tpv = pvi\n\t\t\t} else if pvf, err := strconv.ParseFloat(str, 64); err == nil {\n\t\t\t\tpv = pvf\n\t\t\t} else {\n\t\t\t\tpv = str\n\t\t\t}\n\t\t} else {\n\t\t\tpv = v\n\t\t}\n\t\ts := fmt.Sprintf(\"SET SESSION %s = ?\", k)\n\t\t_, err := db.ExecContext(tctx, s, pv)\n\t\tif err != nil {\n\t\t\tif isUnknownSystemVariableErr(err) {\n\t\t\t\ttctx.L().Info(\"session variable is not supported by db\", zap.String(\"variable\", k), zap.Reflect(\"value\", v))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\n\t\tsupport[k] = pv\n\t}\n\n\tfor k, v := range support {\n\t\tvar s string\n\t\t// Wrap string with quote to handle string with space. For example, '2020-10-20 13:41:40'\n\t\t// For --params argument, quote doesn't matter because it doesn't affect the actual value\n\t\tif str, ok := v.(string); ok {\n\t\t\ts = wrapStringWith(str, \"'\")\n\t\t} else {\n\t\t\ts = fmt.Sprintf(\"%v\", v)\n\t\t}\n\t\tdsn += fmt.Sprintf(\"&%s=%s\", k, url.QueryEscape(s))\n\t}\n\n\tdb.Close()\n\tnewDB, err := sql.Open(\"mysql\", dsn)\n\tif err == nil {\n\t\t// ping to make sure all session parameters are set correctly\n\t\terr = newDB.PingContext(tctx)\n\t\tif err != nil {\n\t\t\tnewDB.Close()\n\t\t}\n\t}\n\treturn newDB, errors.Trace(err)\n}\n\nfunc createConnWithConsistency(ctx context.Context, db *sql.DB, repeatableRead bool) (*sql.Conn, error) {\n\tconn, err := db.Conn(ctx)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tvar query string\n\tif repeatableRead {\n\t\tquery = \"SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ\"\n\t\t_, err = conn.ExecContext(ctx, query)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t\t}\n\t}\n\tquery = \"START TRANSACTION /*!40108 WITH CONSISTENT SNAPSHOT */\"\n\t_, err = conn.ExecContext(ctx, query)\n\tif err != nil {\n\t\t// Some MySQL Compatible databases like Vitess and MemSQL/SingleStore\n\t\t// are newer than 4.1.8 (the version comment) but don't actually support\n\t\t// `WITH CONSISTENT SNAPSHOT`. So retry without that if the statement fails.\n\t\tquery = \"START TRANSACTION\"\n\t\t_, err = conn.ExecContext(ctx, query)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t\t}\n\t}\n\treturn conn, nil\n}\n\n// buildSelectField returns the selecting fields' string(joined by comma(`,`)),\n// and the number of writable fields.\nfunc buildSelectField(tctx *tcontext.Context, db *BaseConn, dbName, tableName string, completeInsert bool) (string, int, error) { // revive:disable-line:flag-parameter\n\tquery := fmt.Sprintf(\"SHOW COLUMNS FROM `%s`.`%s`\", escapeString(dbName), escapeString(tableName))\n\tresults, err := db.QuerySQLWithColumns(tctx, []string{\"FIELD\", \"EXTRA\"}, query)\n\tif err != nil {\n\t\treturn \"\", 0, err\n\t}\n\tavailableFields := make([]string, 0)\n\thasGenerateColumn := false\n\tfor _, oneRow := range results {\n\t\tfieldName, extra := oneRow[0], oneRow[1]\n\t\tswitch extra {\n\t\tcase \"STORED GENERATED\", \"VIRTUAL GENERATED\":\n\t\t\thasGenerateColumn = true\n\t\t\tcontinue\n\t\t}\n\t\tavailableFields = append(availableFields, wrapBackTicks(escapeString(fieldName)))\n\t}\n\tif completeInsert || hasGenerateColumn {\n\t\treturn strings.Join(availableFields, \",\"), len(availableFields), nil\n\t}\n\treturn \"*\", len(availableFields), nil\n}\n\nfunc buildWhereClauses(handleColNames []string, handleVals [][]string) []string {\n\tif len(handleColNames) == 0 || len(handleVals) == 0 {\n\t\treturn nil\n\t}\n\tquotaCols := make([]string, len(handleColNames))\n\tfor i, s := range handleColNames {\n\t\tquotaCols[i] = fmt.Sprintf(\"`%s`\", escapeString(s))\n\t}\n\twhere := make([]string, 0, len(handleVals)+1)\n\tbuf := &bytes.Buffer{}\n\tbuildCompareClause(buf, quotaCols, handleVals[0], less, false)\n\twhere = append(where, buf.String())\n\tbuf.Reset()\n\tfor i := 1; i < len(handleVals); i++ {\n\t\tlow, up := handleVals[i-1], handleVals[i]\n\t\tbuildBetweenClause(buf, quotaCols, low, up)\n\t\twhere = append(where, buf.String())\n\t\tbuf.Reset()\n\t}\n\tbuildCompareClause(buf, quotaCols, handleVals[len(handleVals)-1], greater, true)\n\twhere = append(where, buf.String())\n\tbuf.Reset()\n\treturn where\n}\n\n// return greater than TableRangeScan where clause\n// the result doesn't contain brackets\nconst (\n\tgreater = '>'\n\tless    = '<'\n\tequal   = '='\n)\n\n// buildCompareClause build clause with specified bounds. Usually we will use the following two conditions:\n// (compare, writeEqual) == (less, false), return quotaCols < bound clause. In other words, (-inf, bound)\n// (compare, writeEqual) == (greater, true), return quotaCols >= bound clause. In other words, [bound, +inf)\nfunc buildCompareClause(buf *bytes.Buffer, quotaCols []string, bound []string, compare byte, writeEqual bool) { // revive:disable-line:flag-parameter\n\tfor i, col := range quotaCols {\n\t\tif i > 0 {\n\t\t\tbuf.WriteString(\"or(\")\n\t\t}\n\t\tfor j := 0; j < i; j++ {\n\t\t\tbuf.WriteString(quotaCols[j])\n\t\t\tbuf.WriteByte(equal)\n\t\t\tbuf.WriteString(bound[j])\n\t\t\tbuf.WriteString(\" and \")\n\t\t}\n\t\tbuf.WriteString(col)\n\t\tbuf.WriteByte(compare)\n\t\tif writeEqual && i == len(quotaCols)-1 {\n\t\t\tbuf.WriteByte(equal)\n\t\t}\n\t\tbuf.WriteString(bound[i])\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(')')\n\t\t} else if i != len(quotaCols)-1 {\n\t\t\tbuf.WriteByte(' ')\n\t\t}\n\t}\n}\n\n// getCommonLength returns the common length of low and up\nfunc getCommonLength(low []string, up []string) int {\n\tfor i := range low {\n\t\tif low[i] != up[i] {\n\t\t\treturn i\n\t\t}\n\t}\n\treturn len(low)\n}\n\n// buildBetweenClause build clause in a specified table range.\n// the result where clause will be low <= quotaCols < up. In other words, [low, up)\nfunc buildBetweenClause(buf *bytes.Buffer, quotaCols []string, low []string, up []string) {\n\tsingleBetween := func(writeEqual bool) {\n\t\tbuf.WriteString(quotaCols[0])\n\t\tbuf.WriteByte(greater)\n\t\tif writeEqual {\n\t\t\tbuf.WriteByte(equal)\n\t\t}\n\t\tbuf.WriteString(low[0])\n\t\tbuf.WriteString(\" and \")\n\t\tbuf.WriteString(quotaCols[0])\n\t\tbuf.WriteByte(less)\n\t\tbuf.WriteString(up[0])\n\t}\n\t// handle special cases with common prefix\n\tcommonLen := getCommonLength(low, up)\n\tif commonLen > 0 {\n\t\t// unexpected case for low == up, return empty result\n\t\tif commonLen == len(low) {\n\t\t\tbuf.WriteString(\"false\")\n\t\t\treturn\n\t\t}\n\t\tfor i := 0; i < commonLen; i++ {\n\t\t\tif i > 0 {\n\t\t\t\tbuf.WriteString(\" and \")\n\t\t\t}\n\t\t\tbuf.WriteString(quotaCols[i])\n\t\t\tbuf.WriteByte(equal)\n\t\t\tbuf.WriteString(low[i])\n\t\t}\n\t\tbuf.WriteString(\" and(\")\n\t\tdefer buf.WriteByte(')')\n\t\tquotaCols = quotaCols[commonLen:]\n\t\tlow = low[commonLen:]\n\t\tup = up[commonLen:]\n\t}\n\n\t// handle special cases with only one column\n\tif len(quotaCols) == 1 {\n\t\tsingleBetween(true)\n\t\treturn\n\t}\n\tbuf.WriteByte('(')\n\tsingleBetween(false)\n\tbuf.WriteString(\")or(\")\n\tbuf.WriteString(quotaCols[0])\n\tbuf.WriteByte(equal)\n\tbuf.WriteString(low[0])\n\tbuf.WriteString(\" and(\")\n\tbuildCompareClause(buf, quotaCols[1:], low[1:], greater, true)\n\tbuf.WriteString(\"))or(\")\n\tbuf.WriteString(quotaCols[0])\n\tbuf.WriteByte(equal)\n\tbuf.WriteString(up[0])\n\tbuf.WriteString(\" and(\")\n\tbuildCompareClause(buf, quotaCols[1:], up[1:], less, false)\n\tbuf.WriteString(\"))\")\n}\n\nfunc buildOrderByClauseString(handleColNames []string) string {\n\tif len(handleColNames) == 0 {\n\t\treturn \"\"\n\t}\n\tseparator := \",\"\n\tquotaCols := make([]string, len(handleColNames))\n\tfor i, col := range handleColNames {\n\t\tquotaCols[i] = fmt.Sprintf(\"`%s`\", escapeString(col))\n\t}\n\treturn fmt.Sprintf(\"ORDER BY %s\", strings.Join(quotaCols, separator))\n}\n\nfunc buildLockTablesSQL(allTables DatabaseTables, blockList map[string]map[string]interface{}) string {\n\t// ,``.`` READ has 11 bytes, \"LOCK TABLE\" has 10 bytes\n\testimatedCap := len(allTables)*11 + 10\n\ts := bytes.NewBuffer(make([]byte, 0, estimatedCap))\n\tn := false\n\tfor dbName, tables := range allTables {\n\t\tescapedDBName := escapeString(dbName)\n\t\tfor _, table := range tables {\n\t\t\t// Lock views will lock related tables. However, we won't dump data only the create sql of view, so we needn't lock view here.\n\t\t\t// Besides, mydumper also only lock base table here. https://github.com/maxbube/mydumper/blob/1fabdf87e3007e5934227b504ad673ba3697946c/mydumper.c#L1568\n\t\t\tif table.Type != TableTypeBase {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif blockTable, ok := blockList[dbName]; ok {\n\t\t\t\tif _, ok := blockTable[table.Name]; ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !n {\n\t\t\t\tfmt.Fprintf(s, \"LOCK TABLES `%s`.`%s` READ\", escapedDBName, escapeString(table.Name))\n\t\t\t\tn = true\n\t\t\t} else {\n\t\t\t\tfmt.Fprintf(s, \",`%s`.`%s` READ\", escapedDBName, escapeString(table.Name))\n\t\t\t}\n\t\t}\n\t}\n\treturn s.String()\n}\n\ntype oneStrColumnTable struct {\n\tdata []string\n}\n\nfunc (o *oneStrColumnTable) handleOneRow(rows *sql.Rows) error {\n\tvar str string\n\tif err := rows.Scan(&str); err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\to.data = append(o.data, str)\n\treturn nil\n}\n\nfunc simpleQuery(conn *sql.Conn, query string, handleOneRow func(*sql.Rows) error) error {\n\treturn simpleQueryWithArgs(context.Background(), conn, handleOneRow, query)\n}\n\nfunc simpleQueryWithArgs(ctx context.Context, conn *sql.Conn, handleOneRow func(*sql.Rows) error, query string, args ...interface{}) error {\n\tvar (\n\t\trows *sql.Rows\n\t\terr  error\n\t)\n\tif len(args) > 0 {\n\t\trows, err = conn.QueryContext(ctx, query, args...)\n\t} else {\n\t\trows, err = conn.QueryContext(ctx, query)\n\t}\n\tif err != nil {\n\t\treturn errors.Annotatef(err, \"sql: %s, args: %s\", query, args)\n\t}\n\tdefer rows.Close()\n\n\tfor rows.Next() {\n\t\tif err := handleOneRow(rows); err != nil {\n\t\t\trows.Close()\n\t\t\treturn errors.Annotatef(err, \"sql: %s, args: %s\", query, args)\n\t\t}\n\t}\n\treturn errors.Annotatef(rows.Err(), \"sql: %s, args: %s\", query, args)\n}\n\nfunc pickupPossibleField(tctx *tcontext.Context, meta TableMeta, db *BaseConn) (string, error) {\n\t// try using _tidb_rowid first\n\tif meta.HasImplicitRowID() {\n\t\treturn \"_tidb_rowid\", nil\n\t}\n\t// try to use pk or uk\n\tfieldName, err := getNumericIndex(tctx, db, meta)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// if fieldName == \"\", there is no proper index\n\treturn fieldName, nil\n}\n\nfunc estimateCount(tctx *tcontext.Context, dbName, tableName string, db *BaseConn, field string, conf *Config) uint64 {\n\tvar query string\n\tif strings.TrimSpace(field) == \"*\" || strings.TrimSpace(field) == \"\" {\n\t\tquery = fmt.Sprintf(\"EXPLAIN SELECT * FROM `%s`.`%s`\", escapeString(dbName), escapeString(tableName))\n\t} else {\n\t\tquery = fmt.Sprintf(\"EXPLAIN SELECT `%s` FROM `%s`.`%s`\", escapeString(field), escapeString(dbName), escapeString(tableName))\n\t}\n\n\tif conf.Where != \"\" {\n\t\tquery += \" WHERE \"\n\t\tquery += conf.Where\n\t}\n\n\testRows := detectEstimateRows(tctx, db, query, []string{\"rows\", \"estRows\", \"count\"})\n\t/* tidb results field name is estRows (before 4.0.0-beta.2: count)\n\t\t+-----------------------+----------+-----------+---------------------------------------------------------+\n\t\t| id                    | estRows  | task      | access object | operator info                           |\n\t\t+-----------------------+----------+-----------+---------------------------------------------------------+\n\t\t| tablereader_5         | 10000.00 | root      |               | data:tablefullscan_4                    |\n\t\t| \u2514\u2500tablefullscan_4     | 10000.00 | cop[tikv] | table:a       | table:a, keep order:false, stats:pseudo |\n\t\t+-----------------------+----------+-----------+----------------------------------------------------------\n\n\tmariadb result field name is rows\n\t\t+------+-------------+---------+-------+---------------+------+---------+------+----------+-------------+\n\t\t| id   | select_type | table   | type  | possible_keys | key  | key_len | ref  | rows     | Extra       |\n\t\t+------+-------------+---------+-------+---------------+------+---------+------+----------+-------------+\n\t\t|    1 | SIMPLE      | sbtest1 | index | NULL          | k_1  | 4       | NULL | 15000049 | Using index |\n\t\t+------+-------------+---------+-------+---------------+------+---------+------+----------+-------------+\n\n\tmysql result field name is rows\n\t\t+----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+-------------+\n\t\t| id | select_type | table | partitions | type  | possible_keys | key       | key_len | ref  | rows | filtered | Extra       |\n\t\t+----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+-------------+\n\t\t|  1 | SIMPLE      | t1    | NULL       | index | NULL          | multi_col | 10      | NULL |    5 |   100.00 | Using index |\n\t\t+----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+-------------+\n\t*/\n\tif estRows > 0 {\n\t\treturn estRows\n\t}\n\treturn 0\n}\n\nfunc detectEstimateRows(tctx *tcontext.Context, db *BaseConn, query string, fieldNames []string) uint64 {\n\tvar (\n\t\tfieldIndex int\n\t\toneRow     []sql.NullString\n\t)\n\terr := db.QuerySQL(tctx, func(rows *sql.Rows) error {\n\t\tcolumns, err := rows.Columns()\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\taddr := make([]interface{}, len(columns))\n\t\toneRow = make([]sql.NullString, len(columns))\n\t\tfieldIndex = -1\n\tfound:\n\t\tfor i := range oneRow {\n\t\t\tfor _, fieldName := range fieldNames {\n\t\t\t\tif strings.EqualFold(columns[i], fieldName) {\n\t\t\t\t\tfieldIndex = i\n\t\t\t\t\tbreak found\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif fieldIndex == -1 {\n\t\t\trows.Close()\n\t\t\treturn nil\n\t\t}\n\n\t\tfor i := range oneRow {\n\t\t\taddr[i] = &oneRow[i]\n\t\t}\n\t\treturn rows.Scan(addr...)\n\t}, func() {}, query)\n\tif err != nil || fieldIndex == -1 {\n\t\ttctx.L().Info(\"can't estimate rows from db\",\n\t\t\tzap.String(\"query\", query), zap.Int(\"fieldIndex\", fieldIndex), log.ShortError(err))\n\t\treturn 0\n\t}\n\n\testRows, err := strconv.ParseFloat(oneRow[fieldIndex].String, 64)\n\tif err != nil {\n\t\ttctx.L().Info(\"can't get parse estimate rows from db\",\n\t\t\tzap.String(\"query\", query), zap.String(\"estRows\", oneRow[fieldIndex].String), log.ShortError(err))\n\t\treturn 0\n\t}\n\treturn uint64(estRows)\n}\n\nfunc parseSnapshotToTSO(pool *sql.DB, snapshot string) (uint64, error) {\n\tsnapshotTS, err := strconv.ParseUint(snapshot, 10, 64)\n\tif err == nil {\n\t\treturn snapshotTS, nil\n\t}\n\tvar tso sql.NullInt64\n\tquery := \"SELECT unix_timestamp(?)\"\n\trow := pool.QueryRow(query, snapshot)\n\terr = row.Scan(&tso)\n\tif err != nil {\n\t\treturn 0, errors.Annotatef(err, \"sql: %s\", strings.ReplaceAll(query, \"?\", fmt.Sprintf(`\"%s\"`, snapshot)))\n\t}\n\tif !tso.Valid {\n\t\treturn 0, errors.Errorf(\"snapshot %s format not supported. please use tso or '2006-01-02 15:04:05' format time\", snapshot)\n\t}\n\treturn (uint64(tso.Int64) << 18) * 1000, nil\n}\n\nfunc buildWhereCondition(conf *Config, where string) string {\n\tvar query strings.Builder\n\tseparator := \"WHERE\"\n\tleftBracket := \" \"\n\trightBracket := \" \"\n\tif conf.Where != \"\" && where != \"\" {\n\t\tleftBracket = \" (\"\n\t\trightBracket = \") \"\n\t}\n\tif conf.Where != \"\" {\n\t\tquery.WriteString(separator)\n\t\tquery.WriteString(leftBracket)\n\t\tquery.WriteString(conf.Where)\n\t\tquery.WriteString(rightBracket)\n\t\tseparator = \"AND\"\n\t}\n\tif where != \"\" {\n\t\tquery.WriteString(separator)\n\t\tquery.WriteString(leftBracket)\n\t\tquery.WriteString(where)\n\t\tquery.WriteString(rightBracket)\n\t}\n\treturn query.String()\n}\n\nfunc escapeString(s string) string {\n\treturn strings.ReplaceAll(s, \"`\", \"``\")\n}\n\n// GetPartitionNames get partition names from a specified table\nfunc GetPartitionNames(tctx *tcontext.Context, db *BaseConn, schema, table string) (partitions []string, err error) {\n\tpartitions = make([]string, 0)\n\tvar partitionName sql.NullString\n\terr = db.QuerySQL(tctx, func(rows *sql.Rows) error {\n\t\terr := rows.Scan(&partitionName)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tif partitionName.Valid {\n\t\t\tpartitions = append(partitions, partitionName.String)\n\t\t}\n\t\treturn nil\n\t}, func() {\n\t\tpartitions = partitions[:0]\n\t}, \"SELECT PARTITION_NAME from INFORMATION_SCHEMA.PARTITIONS WHERE TABLE_SCHEMA = ? AND TABLE_NAME = ?\", schema, table)\n\treturn\n}\n\n// GetPartitionTableIDs get partition tableIDs through histograms.\n// SHOW STATS_HISTOGRAMS  has db_name,table_name,partition_name but doesn't have partition id\n// mysql.stats_histograms has partition_id but doesn't have db_name,table_name,partition_name\n// So we combine the results from these two sqls to get partition ids for each table\n// If UPDATE_TIME,DISTINCT_COUNT are equal, we assume these two records can represent one line.\n// If histograms are not accurate or (UPDATE_TIME,DISTINCT_COUNT) has duplicate data, it's still fine.\n// Because the possibility is low and the effect is that we will select more than one regions in one time,\n// this will not affect the correctness of the dumping data and will not affect the memory usage much.\n// This method is tricky, but no better way is found.\n// Because TiDB v3.0.0's information_schema.partition table doesn't have partition name or partition id info\n// return (dbName -> tbName -> partitionName -> partitionID, error)\nfunc GetPartitionTableIDs(db *sql.Conn, tables map[string]map[string]struct{}) (map[string]map[string]map[string]int64, error) {\n\tconst (\n\t\tshowStatsHistogramsSQL   = \"SHOW STATS_HISTOGRAMS\"\n\t\tselectStatsHistogramsSQL = \"SELECT TABLE_ID,FROM_UNIXTIME(VERSION DIV 262144 DIV 1000,'%Y-%m-%d %H:%i:%s') AS UPDATE_TIME,DISTINCT_COUNT FROM mysql.stats_histograms\"\n\t)\n\tpartitionIDs := make(map[string]map[string]map[string]int64, len(tables))\n\trows, err := db.QueryContext(context.Background(), showStatsHistogramsSQL)\n\tif err != nil {\n\t\treturn nil, errors.Annotatef(err, \"sql: %s\", showStatsHistogramsSQL)\n\t}\n\tresults, err := GetSpecifiedColumnValuesAndClose(rows, \"DB_NAME\", \"TABLE_NAME\", \"PARTITION_NAME\", \"UPDATE_TIME\", \"DISTINCT_COUNT\")\n\tif err != nil {\n\t\treturn nil, errors.Annotatef(err, \"sql: %s\", showStatsHistogramsSQL)\n\t}\n\ttype partitionInfo struct {\n\t\tdbName, tbName, partitionName string\n\t}\n\tsaveMap := make(map[string]map[string]partitionInfo)\n\tfor _, oneRow := range results {\n\t\tdbName, tbName, partitionName, updateTime, distinctCount := oneRow[0], oneRow[1], oneRow[2], oneRow[3], oneRow[4]\n\t\tif len(partitionName) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tif tbm, ok := tables[dbName]; ok {\n\t\t\tif _, ok = tbm[tbName]; ok {\n\t\t\t\tif _, ok = saveMap[updateTime]; !ok {\n\t\t\t\t\tsaveMap[updateTime] = make(map[string]partitionInfo)\n\t\t\t\t}\n\t\t\t\tsaveMap[updateTime][distinctCount] = partitionInfo{\n\t\t\t\t\tdbName:        dbName,\n\t\t\t\t\ttbName:        tbName,\n\t\t\t\t\tpartitionName: partitionName,\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif len(saveMap) == 0 {\n\t\treturn map[string]map[string]map[string]int64{}, nil\n\t}\n\terr = simpleQuery(db, selectStatsHistogramsSQL, func(rows *sql.Rows) error {\n\t\tvar (\n\t\t\ttableID                   int64\n\t\t\tupdateTime, distinctCount string\n\t\t)\n\t\terr2 := rows.Scan(&tableID, &updateTime, &distinctCount)\n\t\tif err2 != nil {\n\t\t\treturn errors.Trace(err2)\n\t\t}\n\t\tif mpt, ok := saveMap[updateTime]; ok {\n\t\t\tif partition, ok := mpt[distinctCount]; ok {\n\t\t\t\tdbName, tbName, partitionName := partition.dbName, partition.tbName, partition.partitionName\n\t\t\t\tif _, ok := partitionIDs[dbName]; !ok {\n\t\t\t\t\tpartitionIDs[dbName] = make(map[string]map[string]int64)\n\t\t\t\t}\n\t\t\t\tif _, ok := partitionIDs[dbName][tbName]; !ok {\n\t\t\t\t\tpartitionIDs[dbName][tbName] = make(map[string]int64)\n\t\t\t\t}\n\t\t\t\tpartitionIDs[dbName][tbName][partitionName] = tableID\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\treturn partitionIDs, err\n}\n\n// GetDBInfo get model.DBInfos from database sql interface.\n// We need table_id to check whether a region belongs to this table\nfunc GetDBInfo(db *sql.Conn, tables map[string]map[string]struct{}) ([]*model.DBInfo, error) {\n\tconst tableIDSQL = \"SELECT TABLE_SCHEMA,TABLE_NAME,TIDB_TABLE_ID FROM INFORMATION_SCHEMA.TABLES ORDER BY TABLE_SCHEMA\"\n\n\tschemas := make([]*model.DBInfo, 0, len(tables))\n\tvar (\n\t\ttableSchema, tableName string\n\t\ttidbTableID            int64\n\t)\n\tpartitionIDs, err := GetPartitionTableIDs(db, tables)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\terr = simpleQuery(db, tableIDSQL, func(rows *sql.Rows) error {\n\t\terr2 := rows.Scan(&tableSchema, &tableName, &tidbTableID)\n\t\tif err2 != nil {\n\t\t\treturn errors.Trace(err2)\n\t\t}\n\t\tif tbm, ok := tables[tableSchema]; !ok {\n\t\t\treturn nil\n\t\t} else if _, ok = tbm[tableName]; !ok {\n\t\t\treturn nil\n\t\t}\n\t\tlast := len(schemas) - 1\n\t\tif last < 0 || schemas[last].Name.O != tableSchema {\n\t\t\tschemas = append(schemas, &model.DBInfo{\n\t\t\t\tName:   model.CIStr{O: tableSchema},\n\t\t\t\tTables: make([]*model.TableInfo, 0, len(tables[tableSchema])),\n\t\t\t})\n\t\t\tlast++\n\t\t}\n\t\tvar partition *model.PartitionInfo\n\t\tif tbm, ok := partitionIDs[tableSchema]; ok {\n\t\t\tif ptm, ok := tbm[tableName]; ok {\n\t\t\t\tpartition = &model.PartitionInfo{Definitions: make([]model.PartitionDefinition, 0, len(ptm))}\n\t\t\t\tfor partitionName, partitionID := range ptm {\n\t\t\t\t\tpartition.Definitions = append(partition.Definitions, model.PartitionDefinition{\n\t\t\t\t\t\tID:   partitionID,\n\t\t\t\t\t\tName: model.CIStr{O: partitionName},\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tschemas[last].Tables = append(schemas[last].Tables, &model.TableInfo{\n\t\t\tID:        tidbTableID,\n\t\t\tName:      model.CIStr{O: tableName},\n\t\t\tPartition: partition,\n\t\t})\n\t\treturn nil\n\t})\n\treturn schemas, err\n}\n\n// GetRegionInfos get region info including regionID, start key, end key from database sql interface.\n// start key, end key includes information to help split table\nfunc GetRegionInfos(db *sql.Conn) (*helper.RegionsInfo, error) {\n\tconst tableRegionSQL = \"SELECT REGION_ID,START_KEY,END_KEY FROM INFORMATION_SCHEMA.TIKV_REGION_STATUS ORDER BY START_KEY;\"\n\tvar (\n\t\tregionID         int64\n\t\tstartKey, endKey string\n\t)\n\tregionsInfo := &helper.RegionsInfo{Regions: make([]helper.RegionInfo, 0)}\n\terr := simpleQuery(db, tableRegionSQL, func(rows *sql.Rows) error {\n\t\terr := rows.Scan(&regionID, &startKey, &endKey)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tregionsInfo.Regions = append(regionsInfo.Regions, helper.RegionInfo{\n\t\t\tID:       regionID,\n\t\t\tStartKey: startKey,\n\t\t\tEndKey:   endKey,\n\t\t})\n\t\treturn nil\n\t})\n\treturn regionsInfo, err\n}\n\n// GetCharsetAndDefaultCollation gets charset and default collation map.\nfunc GetCharsetAndDefaultCollation(ctx context.Context, db *sql.Conn) (map[string]string, error) {\n\tcharsetAndDefaultCollation := make(map[string]string)\n\tquery := \"SHOW CHARACTER SET\"\n\n\t// Show an example.\n\t/*\n\t\tmysql> SHOW CHARACTER SET;\n\t\t+----------+---------------------------------+---------------------+--------+\n\t\t| Charset  | Description                     | Default collation   | Maxlen |\n\t\t+----------+---------------------------------+---------------------+--------+\n\t\t| armscii8 | ARMSCII-8 Armenian              | armscii8_general_ci |      1 |\n\t\t| ascii    | US ASCII                        | ascii_general_ci    |      1 |\n\t\t| big5     | Big5 Traditional Chinese        | big5_chinese_ci     |      2 |\n\t\t| binary   | Binary pseudo charset           | binary              |      1 |\n\t\t| cp1250   | Windows Central European        | cp1250_general_ci   |      1 |\n\t\t| cp1251   | Windows Cyrillic                | cp1251_general_ci   |      1 |\n\t\t+----------+---------------------------------+---------------------+--------+\n\t*/\n\n\trows, err := db.QueryContext(ctx, query)\n\tif err != nil {\n\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t}\n\n\tdefer rows.Close()\n\tfor rows.Next() {\n\t\tvar charset, description, collation string\n\t\tvar maxlen int\n\t\tif scanErr := rows.Scan(&charset, &description, &collation, &maxlen); scanErr != nil {\n\t\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t\t}\n\t\tcharsetAndDefaultCollation[strings.ToLower(charset)] = collation\n\t}\n\tif err = rows.Close(); err != nil {\n\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t}\n\tif err = rows.Err(); err != nil {\n\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t}\n\treturn charsetAndDefaultCollation, err\n}\n", "// Copyright 2020 PingCAP, Inc. Licensed under Apache-2.0.\n\npackage export\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"database/sql/driver\"\n\t\"encoding/csv\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/DATA-DOG/go-sqlmock\"\n\t\"github.com/go-sql-driver/mysql\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/tidb/br/pkg/version\"\n\tdbconfig \"github.com/pingcap/tidb/config\"\n\ttcontext \"github.com/pingcap/tidb/dumpling/context\"\n\t\"github.com/pingcap/tidb/util/promutil\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nvar showIndexHeaders = []string{\n\t\"Table\",\n\t\"Non_unique\",\n\t\"Key_name\",\n\t\"Seq_in_index\",\n\t\"Column_name\",\n\t\"Collation\",\n\t\"Cardinality\",\n\t\"Sub_part\",\n\t\"Packed\",\n\t\"Null\",\n\t\"Index_type\",\n\t\"Comment\",\n\t\"Index_comment\",\n}\n\nconst (\n\tdatabase = \"foo\"\n\ttable    = \"bar\"\n)\n\nfunc TestBuildSelectAllQuery(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\n\tmockConf := defaultConfigForTest(t)\n\tmockConf.SortByPk = true\n\n\t// Test TiDB server.\n\tmockConf.ServerInfo.ServerType = version.ServerTypeTiDB\n\ttctx := tcontext.Background().WithLogger(appLogger)\n\tbaseConn := newBaseConn(conn, true, nil)\n\n\torderByClause, err := buildOrderByClause(tctx, mockConf, baseConn, database, table, true)\n\trequire.NoError(t, err)\n\n\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\tAddRow(\"id\", \"int(11)\", \"NO\", \"PRI\", nil, \"\"))\n\n\tselectedField, _, err := buildSelectField(tctx, baseConn, database, table, false)\n\trequire.NoError(t, err)\n\n\tq := buildSelectQuery(database, table, selectedField, \"\", \"\", orderByClause)\n\trequire.Equal(t, fmt.Sprintf(\"SELECT * FROM `%s`.`%s` ORDER BY `_tidb_rowid`\", database, table), q)\n\n\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).\n\t\tWillReturnRows(sqlmock.NewRows(showIndexHeaders).\n\t\t\tAddRow(table, 0, \"PRIMARY\", 1, \"id\", \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\"))\n\n\torderByClause, err = buildOrderByClause(tctx, mockConf, baseConn, database, table, false)\n\trequire.NoError(t, err)\n\n\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\tAddRow(\"id\", \"int(11)\", \"NO\", \"PRI\", nil, \"\"))\n\n\tselectedField, _, err = buildSelectField(tctx, baseConn, database, table, false)\n\trequire.NoError(t, err)\n\n\tq = buildSelectQuery(database, table, selectedField, \"\", \"\", orderByClause)\n\trequire.Equal(t, fmt.Sprintf(\"SELECT * FROM `%s`.`%s` ORDER BY `id`\", database, table), q)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\t// Test other servers.\n\totherServers := []version.ServerType{version.ServerTypeUnknown, version.ServerTypeMySQL, version.ServerTypeMariaDB}\n\n\t// Test table with primary key.\n\tfor _, serverTp := range otherServers {\n\t\tmockConf.ServerInfo.ServerType = serverTp\n\t\tcomment := fmt.Sprintf(\"server type: %s\", serverTp)\n\n\t\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).\n\t\t\tWillReturnRows(sqlmock.NewRows(showIndexHeaders).\n\t\t\t\tAddRow(table, 0, \"PRIMARY\", 1, \"id\", \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\"))\n\t\torderByClause, err := buildOrderByClause(tctx, mockConf, baseConn, database, table, false)\n\t\trequire.NoError(t, err, comment)\n\n\t\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").\n\t\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\t\tAddRow(\"id\", \"int(11)\", \"NO\", \"PRI\", nil, \"\"))\n\n\t\tselectedField, _, err = buildSelectField(tctx, baseConn, database, table, false)\n\t\trequire.NoError(t, err, comment)\n\n\t\tq = buildSelectQuery(database, table, selectedField, \"\", \"\", orderByClause)\n\t\trequire.Equal(t, fmt.Sprintf(\"SELECT * FROM `%s`.`%s` ORDER BY `id`\", database, table), q, comment)\n\n\t\terr = mock.ExpectationsWereMet()\n\t\trequire.NoError(t, err, comment)\n\t\trequire.NoError(t, mock.ExpectationsWereMet(), comment)\n\t}\n\n\t// Test table without primary key.\n\tfor _, serverTp := range otherServers {\n\t\tmockConf.ServerInfo.ServerType = serverTp\n\t\tcomment := fmt.Sprintf(\"server type: %s\", serverTp)\n\n\t\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).\n\t\t\tWillReturnRows(sqlmock.NewRows(showIndexHeaders))\n\n\t\torderByClause, err := buildOrderByClause(tctx, mockConf, baseConn, database, table, false)\n\t\trequire.NoError(t, err, comment)\n\n\t\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").\n\t\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\t\tAddRow(\"id\", \"int(11)\", \"NO\", \"PRI\", nil, \"\"))\n\n\t\tselectedField, _, err = buildSelectField(tctx, baseConn, \"test\", \"t\", false)\n\t\trequire.NoError(t, err, comment)\n\n\t\tq := buildSelectQuery(database, table, selectedField, \"\", \"\", orderByClause)\n\t\trequire.Equal(t, fmt.Sprintf(\"SELECT * FROM `%s`.`%s`\", database, table), q, comment)\n\n\t\terr = mock.ExpectationsWereMet()\n\t\trequire.NoError(t, err, comment)\n\t\trequire.NoError(t, mock.ExpectationsWereMet(), comment)\n\t}\n\n\t// Test when config.SortByPk is disabled.\n\tmockConf.SortByPk = false\n\tfor tp := version.ServerTypeUnknown; tp < version.ServerTypeAll; tp++ {\n\t\tmockConf.ServerInfo.ServerType = version.ServerType(tp)\n\t\tcomment := fmt.Sprintf(\"current server type: %v\", tp)\n\n\t\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").\n\t\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\t\tAddRow(\"id\", \"int(11)\", \"NO\", \"PRI\", nil, \"\"))\n\n\t\tselectedField, _, err := buildSelectField(tctx, baseConn, \"test\", \"t\", false)\n\t\trequire.NoError(t, err, comment)\n\n\t\tq := buildSelectQuery(database, table, selectedField, \"\", \"\", \"\")\n\t\trequire.Equal(t, fmt.Sprintf(\"SELECT * FROM `%s`.`%s`\", database, table), q, comment)\n\t\trequire.NoError(t, mock.ExpectationsWereMet(), comment)\n\t}\n}\n\nfunc TestBuildOrderByClause(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\ttctx := tcontext.Background().WithLogger(appLogger)\n\tbaseConn := newBaseConn(conn, true, nil)\n\n\tmockConf := defaultConfigForTest(t)\n\tmockConf.SortByPk = true\n\n\t// Test TiDB server.\n\tmockConf.ServerInfo.ServerType = version.ServerTypeTiDB\n\n\torderByClause, err := buildOrderByClause(tctx, mockConf, baseConn, database, table, true)\n\trequire.NoError(t, err)\n\trequire.Equal(t, orderByTiDBRowID, orderByClause)\n\n\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).\n\t\tWillReturnRows(sqlmock.NewRows(showIndexHeaders).AddRow(table, 0, \"PRIMARY\", 1, \"id\", \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\"))\n\n\torderByClause, err = buildOrderByClause(tctx, mockConf, baseConn, database, table, false)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"ORDER BY `id`\", orderByClause)\n\n\t// Test table with primary key.\n\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).\n\t\tWillReturnRows(sqlmock.NewRows(showIndexHeaders).AddRow(table, 0, \"PRIMARY\", 1, \"id\", \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\"))\n\torderByClause, err = buildOrderByClause(tctx, mockConf, baseConn, database, table, false)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"ORDER BY `id`\", orderByClause)\n\n\t// Test table with joint primary key.\n\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).\n\t\tWillReturnRows(sqlmock.NewRows(showIndexHeaders).\n\t\t\tAddRow(table, 0, \"PRIMARY\", 1, \"id\", \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\").\n\t\t\tAddRow(table, 0, \"PRIMARY\", 2, \"name\", \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\"))\n\torderByClause, err = buildOrderByClause(tctx, mockConf, baseConn, database, table, false)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"ORDER BY `id`,`name`\", orderByClause)\n\n\t// Test table without primary key.\n\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).\n\t\tWillReturnRows(sqlmock.NewRows(showIndexHeaders))\n\n\torderByClause, err = buildOrderByClause(tctx, mockConf, baseConn, database, table, false)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"\", orderByClause)\n\n\t// Test when config.SortByPk is disabled.\n\tmockConf.SortByPk = false\n\tfor _, hasImplicitRowID := range []bool{false, true} {\n\t\tcomment := fmt.Sprintf(\"current hasImplicitRowID: %v\", hasImplicitRowID)\n\n\t\torderByClause, err := buildOrderByClause(tctx, mockConf, baseConn, database, table, hasImplicitRowID)\n\t\trequire.NoError(t, err, comment)\n\t\trequire.Equal(t, \"\", orderByClause, comment)\n\t}\n\n\t// Test build OrderByClause with retry\n\tbaseConn = newBaseConn(conn, true, func(conn *sql.Conn, b bool) (*sql.Conn, error) {\n\t\treturn conn, nil\n\t})\n\tquery := fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)\n\tmock.ExpectQuery(query).WillReturnError(errors.New(\"invalid connection\"))\n\tmock.ExpectQuery(query).WillReturnError(errors.New(\"invalid connection\"))\n\tmock.ExpectQuery(query).WillReturnRows(sqlmock.NewRows(showIndexHeaders).AddRow(table, 0, \"PRIMARY\", 1, \"id\", \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\"))\n\tmockConf.SortByPk = true\n\torderByClause, err = buildOrderByClause(tctx, mockConf, baseConn, database, table, false)\n\trequire.NoError(t, err)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n\trequire.Equal(t, \"ORDER BY `id`\", orderByClause)\n}\n\nfunc TestBuildSelectField(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\ttctx := tcontext.Background().WithLogger(appLogger)\n\tbaseConn := newBaseConn(conn, true, nil)\n\n\t// generate columns not found\n\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\tAddRow(\"id\", \"int(11)\", \"NO\", \"PRI\", nil, \"\"))\n\n\tselectedField, _, err := buildSelectField(tctx, baseConn, \"test\", \"t\", false)\n\trequire.Equal(t, \"*\", selectedField)\n\trequire.NoError(t, err)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\t// user assigns completeInsert\n\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\tAddRow(\"id\", \"int(11)\", \"NO\", \"PRI\", nil, \"\").\n\t\t\tAddRow(\"name\", \"varchar(12)\", \"NO\", \"\", nil, \"\").\n\t\t\tAddRow(\"quo`te\", \"varchar(12)\", \"NO\", \"UNI\", nil, \"\"))\n\n\tselectedField, _, err = buildSelectField(tctx, baseConn, \"test\", \"t\", true)\n\trequire.Equal(t, \"`id`,`name`,`quo``te`\", selectedField)\n\trequire.NoError(t, err)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\t// found generate columns, rest columns is `id`,`name`\n\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\tAddRow(\"id\", \"int(11)\", \"NO\", \"PRI\", nil, \"\").\n\t\t\tAddRow(\"name\", \"varchar(12)\", \"NO\", \"\", nil, \"\").\n\t\t\tAddRow(\"quo`te\", \"varchar(12)\", \"NO\", \"UNI\", nil, \"\").\n\t\t\tAddRow(\"generated\", \"varchar(12)\", \"NO\", \"\", nil, \"VIRTUAL GENERATED\"))\n\n\tselectedField, _, err = buildSelectField(tctx, baseConn, \"test\", \"t\", false)\n\trequire.Equal(t, \"`id`,`name`,`quo``te`\", selectedField)\n\trequire.NoError(t, err)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\t// Test build SelectField with retry\n\tbaseConn = newBaseConn(conn, true, func(conn *sql.Conn, b bool) (*sql.Conn, error) {\n\t\treturn conn, nil\n\t})\n\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").WillReturnError(errors.New(\"invalid connection\"))\n\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").WillReturnError(errors.New(\"invalid connection\"))\n\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\tAddRow(\"id\", \"int(11)\", \"NO\", \"PRI\", nil, \"\"))\n\n\tselectedField, _, err = buildSelectField(tctx, baseConn, \"test\", \"t\", false)\n\trequire.Equal(t, \"*\", selectedField)\n\trequire.NoError(t, err)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n}\n\nfunc TestParseSnapshotToTSO(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\t_ = db.Close()\n\t}()\n\n\tsnapshot := \"2020/07/18 20:31:50\"\n\tvar unixTimeStamp uint64 = 1595075510\n\t// generate columns valid snapshot\n\tmock.ExpectQuery(`SELECT unix_timestamp(?)`).\n\t\tWithArgs(sqlmock.AnyArg()).\n\t\tWillReturnRows(sqlmock.NewRows([]string{`unix_timestamp(\"2020/07/18 20:31:50\")`}).AddRow(1595075510))\n\ttso, err := parseSnapshotToTSO(db, snapshot)\n\trequire.NoError(t, err)\n\trequire.Equal(t, (unixTimeStamp<<18)*1000, tso)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\t// generate columns not valid snapshot\n\tmock.ExpectQuery(`SELECT unix_timestamp(?)`).\n\t\tWithArgs(sqlmock.AnyArg()).\n\t\tWillReturnRows(sqlmock.NewRows([]string{`unix_timestamp(\"XXYYZZ\")`}).AddRow(nil))\n\ttso, err = parseSnapshotToTSO(db, \"XXYYZZ\")\n\trequire.EqualError(t, err, \"snapshot XXYYZZ format not supported. please use tso or '2006-01-02 15:04:05' format time\")\n\trequire.Equal(t, uint64(0), tso)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n}\n\nfunc TestShowCreateView(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\ttctx := tcontext.Background().WithLogger(appLogger)\n\tbaseConn := newBaseConn(conn, true, nil)\n\n\tmock.ExpectQuery(\"SHOW FIELDS FROM `test`.`v`\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\tAddRow(\"a\", \"int(11)\", \"YES\", nil, \"NULL\", nil))\n\n\tmock.ExpectQuery(\"SHOW CREATE VIEW `test`.`v`\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"View\", \"Create View\", \"character_set_client\", \"collation_connection\"}).\n\t\t\tAddRow(\"v\", \"CREATE ALGORITHM=UNDEFINED DEFINER=`root`@`localhost` SQL SECURITY DEFINER VIEW `v` (`a`) AS SELECT `t`.`a` AS `a` FROM `test`.`t`\", \"utf8\", \"utf8_general_ci\"))\n\n\tcreateTableSQL, createViewSQL, err := ShowCreateView(tctx, baseConn, \"test\", \"v\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"CREATE TABLE `v`(\\n`a` int\\n)ENGINE=MyISAM;\\n\", createTableSQL)\n\trequire.Equal(t, \"DROP TABLE IF EXISTS `v`;\\nDROP VIEW IF EXISTS `v`;\\nSET @PREV_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT;\\nSET @PREV_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS;\\nSET @PREV_COLLATION_CONNECTION=@@COLLATION_CONNECTION;\\nSET character_set_client = utf8;\\nSET character_set_results = utf8;\\nSET collation_connection = utf8_general_ci;\\nCREATE ALGORITHM=UNDEFINED DEFINER=`root`@`localhost` SQL SECURITY DEFINER VIEW `v` (`a`) AS SELECT `t`.`a` AS `a` FROM `test`.`t`;\\nSET character_set_client = @PREV_CHARACTER_SET_CLIENT;\\nSET character_set_results = @PREV_CHARACTER_SET_RESULTS;\\nSET collation_connection = @PREV_COLLATION_CONNECTION;\\n\", createViewSQL)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n}\n\nfunc TestShowCreateSequence(t *testing.T) {\n\tconf := defaultConfigForTest(t)\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\ttctx := tcontext.Background().WithLogger(appLogger)\n\tbaseConn := newBaseConn(conn, true, nil)\n\n\tconf.ServerInfo.ServerType = version.ServerTypeTiDB\n\tmock.ExpectQuery(\"SHOW CREATE SEQUENCE `test`.`s`\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Sequence\", \"Create Sequence\"}).\n\t\t\tAddRow(\"s\", \"CREATE SEQUENCE `s` start with 1 minvalue 1 maxvalue 9223372036854775806 increment by 1 cache 1000 nocycle ENGINE=InnoDB\"))\n\tmock.ExpectQuery(\"SHOW TABLE `test`.`s` NEXT_ROW_ID\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"DB_NAME\", \"TABLE_NAME\", \"COLUMN_NAME\", \"NEXT_GLOBAL_ROW_ID\", \"ID_TYPE\"}).\n\t\t\tAddRow(\"test\", \"s\", nil, 1001, \"SEQUENCE\"))\n\n\tcreateSequenceSQL, err := ShowCreateSequence(tctx, baseConn, \"test\", \"s\", conf)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"CREATE SEQUENCE `s` start with 1 minvalue 1 maxvalue 9223372036854775806 increment by 1 cache 1000 nocycle ENGINE=InnoDB;\\nSELECT SETVAL(`s`,1001);\\n\", createSequenceSQL)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\tconf.ServerInfo.ServerType = version.ServerTypeMariaDB\n\tmock.ExpectQuery(\"SHOW CREATE SEQUENCE `test`.`s`\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Table\", \"Create Table\"}).\n\t\t\tAddRow(\"s\", \"CREATE SEQUENCE `s` start with 1 minvalue 1 maxvalue 9223372036854775806 increment by 1 cache 1000 nocycle ENGINE=InnoDB\"))\n\tmock.ExpectQuery(\"SELECT NEXT_NOT_CACHED_VALUE FROM `test`.`s`\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"next_not_cached_value\"}).\n\t\t\tAddRow(1001))\n\n\tcreateSequenceSQL, err = ShowCreateSequence(tctx, baseConn, \"test\", \"s\", conf)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"CREATE SEQUENCE `s` start with 1 minvalue 1 maxvalue 9223372036854775806 increment by 1 cache 1000 nocycle ENGINE=InnoDB;\\nSELECT SETVAL(`s`,1001);\\n\", createSequenceSQL)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n}\n\nfunc TestShowCreatePolicy(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\ttctx := tcontext.Background().WithLogger(appLogger)\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\tbaseConn := newBaseConn(conn, true, nil)\n\n\tmock.ExpectQuery(\"SHOW CREATE PLACEMENT POLICY `policy_x`\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Policy\", \"Create Policy\"}).\n\t\t\tAddRow(\"policy_x\", \"CREATE PLACEMENT POLICY `policy_x` LEARNERS=1\"))\n\n\tcreatePolicySQL, err := ShowCreatePlacementPolicy(tctx, baseConn, \"policy_x\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"CREATE PLACEMENT POLICY `policy_x` LEARNERS=1\", createPolicySQL)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n}\n\nfunc TestListPolicyNames(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\ttctx := tcontext.Background().WithLogger(appLogger)\n\tconn, err := db.Conn(context.Background())\n\tbaseConn := newBaseConn(conn, true, nil)\n\trequire.NoError(t, err)\n\n\tmock.ExpectQuery(\"select distinct policy_name from information_schema.placement_policies where policy_name is not null;\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"policy_name\"}).\n\t\t\tAddRow(\"policy_x\"))\n\tpolicies, err := ListAllPlacementPolicyNames(tctx, baseConn)\n\trequire.NoError(t, err)\n\trequire.Equal(t, []string{\"policy_x\"}, policies)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\t// some old tidb version doesn't support placement rules returns error\n\texpectedErr := &mysql.MySQLError{Number: ErrNoSuchTable, Message: \"Table 'information_schema.placement_policies' doesn't exist\"}\n\tmock.ExpectExec(\"select distinct policy_name from information_schema.placement_policies where policy_name is not null;\").\n\t\tWillReturnError(expectedErr)\n\t_, err = ListAllPlacementPolicyNames(tctx, baseConn)\n\tif mysqlErr, ok := err.(*mysql.MySQLError); ok {\n\t\trequire.Equal(t, mysqlErr.Number, ErrNoSuchTable)\n\t}\n}\n\nfunc TestGetSuitableRows(t *testing.T) {\n\ttestCases := []struct {\n\t\tavgRowLength uint64\n\t\texpectedRows uint64\n\t}{\n\t\t{\n\t\t\t0,\n\t\t\t200000,\n\t\t},\n\t\t{\n\t\t\t32,\n\t\t\t1000000,\n\t\t},\n\t\t{\n\t\t\t1024,\n\t\t\t131072,\n\t\t},\n\t\t{\n\t\t\t4096,\n\t\t\t32768,\n\t\t},\n\t}\n\tfor _, testCase := range testCases {\n\t\trows := GetSuitableRows(testCase.avgRowLength)\n\t\trequire.Equal(t, testCase.expectedRows, rows)\n\t}\n}\n\nfunc TestSelectTiDBRowID(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\ttctx := tcontext.Background().WithLogger(appLogger)\n\tbaseConn := newBaseConn(conn, true, nil)\n\n\tdatabase, table := \"test\", \"t\"\n\n\t// _tidb_rowid is unavailable, or PKIsHandle.\n\tmock.ExpectExec(\"SELECT _tidb_rowid from `test`.`t`\").\n\t\tWillReturnError(errors.New(`1054, \"Unknown column '_tidb_rowid' in 'field list'\"`))\n\thasImplicitRowID, err := SelectTiDBRowID(tctx, baseConn, database, table)\n\trequire.NoError(t, err)\n\trequire.False(t, hasImplicitRowID)\n\n\t// _tidb_rowid is available.\n\tmock.ExpectExec(\"SELECT _tidb_rowid from `test`.`t`\").\n\t\tWillReturnResult(sqlmock.NewResult(0, 0))\n\thasImplicitRowID, err = SelectTiDBRowID(tctx, baseConn, database, table)\n\trequire.NoError(t, err)\n\trequire.True(t, hasImplicitRowID)\n\n\t// _tidb_rowid returns error\n\texpectedErr := errors.New(\"mock error\")\n\tmock.ExpectExec(\"SELECT _tidb_rowid from `test`.`t`\").\n\t\tWillReturnError(expectedErr)\n\thasImplicitRowID, err = SelectTiDBRowID(tctx, baseConn, database, table)\n\trequire.ErrorIs(t, errors.Cause(err), expectedErr)\n\trequire.False(t, hasImplicitRowID)\n}\n\nfunc TestBuildTableSampleQueries(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\tbaseConn := newBaseConn(conn, true, nil)\n\ttctx, cancel := tcontext.Background().WithLogger(appLogger).WithCancel()\n\tmetrics := newMetrics(promutil.NewDefaultFactory(), nil)\n\n\td := &Dumper{\n\t\ttctx:                      tctx,\n\t\tconf:                      DefaultConfig(),\n\t\tcancelCtx:                 cancel,\n\t\tmetrics:                   metrics,\n\t\tselectTiDBTableRegionFunc: selectTiDBTableRegion,\n\t}\n\td.conf.ServerInfo = version.ServerInfo{\n\t\tHasTiKV:       true,\n\t\tServerType:    version.ServerTypeTiDB,\n\t\tServerVersion: tableSampleVersion,\n\t}\n\n\ttestCases := []struct {\n\t\thandleColNames       []string\n\t\thandleColTypes       []string\n\t\thandleVals           [][]driver.Value\n\t\texpectedWhereClauses []string\n\t\thasTiDBRowID         bool\n\t}{\n\t\t{\n\t\t\t[]string{},\n\t\t\t[]string{},\n\t\t\t[][]driver.Value{},\n\t\t\tnil,\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[]string{\"a\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[][]driver.Value{{1}},\n\t\t\t[]string{\"`a`<1\", \"`a`>=1\"},\n\t\t\tfalse,\n\t\t},\n\t\t// check whether dumpling can turn to dump whole table\n\t\t{\n\t\t\t[]string{\"a\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[][]driver.Value{},\n\t\t\tnil,\n\t\t\tfalse,\n\t\t},\n\t\t// check whether dumpling can turn to dump whole table\n\t\t{\n\t\t\t[]string{\"_tidb_rowid\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[][]driver.Value{},\n\t\t\tnil,\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\t[]string{\"_tidb_rowid\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[][]driver.Value{{1}},\n\t\t\t[]string{\"`_tidb_rowid`<1\", \"`_tidb_rowid`>=1\"},\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\t[]string{\"a\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[][]driver.Value{\n\t\t\t\t{1},\n\t\t\t\t{2},\n\t\t\t\t{3},\n\t\t\t},\n\t\t\t[]string{\"`a`<1\", \"`a`>=1 and `a`<2\", \"`a`>=2 and `a`<3\", \"`a`>=3\"},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[]string{\"a\", \"b\"},\n\t\t\t[]string{\"BIGINT\", \"BIGINT\"},\n\t\t\t[][]driver.Value{{1, 2}},\n\t\t\t[]string{\"`a`<1 or(`a`=1 and `b`<2)\", \"`a`>1 or(`a`=1 and `b`>=2)\"},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[]string{\"a\", \"b\"},\n\t\t\t[]string{\"BIGINT\", \"BIGINT\"},\n\t\t\t[][]driver.Value{\n\t\t\t\t{1, 2},\n\t\t\t\t{3, 4},\n\t\t\t\t{5, 6},\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"`a`<1 or(`a`=1 and `b`<2)\",\n\t\t\t\t\"(`a`>1 and `a`<3)or(`a`=1 and(`b`>=2))or(`a`=3 and(`b`<4))\",\n\t\t\t\t\"(`a`>3 and `a`<5)or(`a`=3 and(`b`>=4))or(`a`=5 and(`b`<6))\",\n\t\t\t\t\"`a`>5 or(`a`=5 and `b`>=6)\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[]string{\"a\", \"b\", \"c\"},\n\t\t\t[]string{\"BIGINT\", \"BIGINT\", \"BIGINT\"},\n\t\t\t[][]driver.Value{\n\t\t\t\t{1, 2, 3},\n\t\t\t\t{4, 5, 6},\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"`a`<1 or(`a`=1 and `b`<2)or(`a`=1 and `b`=2 and `c`<3)\",\n\t\t\t\t\"(`a`>1 and `a`<4)or(`a`=1 and(`b`>2 or(`b`=2 and `c`>=3)))or(`a`=4 and(`b`<5 or(`b`=5 and `c`<6)))\",\n\t\t\t\t\"`a`>4 or(`a`=4 and `b`>5)or(`a`=4 and `b`=5 and `c`>=6)\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[]string{\"a\", \"b\", \"c\"},\n\t\t\t[]string{\"BIGINT\", \"BIGINT\", \"BIGINT\"},\n\t\t\t[][]driver.Value{\n\t\t\t\t{1, 2, 3},\n\t\t\t\t{1, 4, 5},\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"`a`<1 or(`a`=1 and `b`<2)or(`a`=1 and `b`=2 and `c`<3)\",\n\t\t\t\t\"`a`=1 and((`b`>2 and `b`<4)or(`b`=2 and(`c`>=3))or(`b`=4 and(`c`<5)))\",\n\t\t\t\t\"`a`>1 or(`a`=1 and `b`>4)or(`a`=1 and `b`=4 and `c`>=5)\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[]string{\"a\", \"b\", \"c\"},\n\t\t\t[]string{\"BIGINT\", \"BIGINT\", \"BIGINT\"},\n\t\t\t[][]driver.Value{\n\t\t\t\t{1, 2, 3},\n\t\t\t\t{1, 2, 8},\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"`a`<1 or(`a`=1 and `b`<2)or(`a`=1 and `b`=2 and `c`<3)\",\n\t\t\t\t\"`a`=1 and `b`=2 and(`c`>=3 and `c`<8)\",\n\t\t\t\t\"`a`>1 or(`a`=1 and `b`>2)or(`a`=1 and `b`=2 and `c`>=8)\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t// special case: avoid return same samples\n\t\t{\n\t\t\t[]string{\"a\", \"b\", \"c\"},\n\t\t\t[]string{\"BIGINT\", \"BIGINT\", \"BIGINT\"},\n\t\t\t[][]driver.Value{\n\t\t\t\t{1, 2, 3},\n\t\t\t\t{1, 2, 3},\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"`a`<1 or(`a`=1 and `b`<2)or(`a`=1 and `b`=2 and `c`<3)\",\n\t\t\t\t\"false\",\n\t\t\t\t\"`a`>1 or(`a`=1 and `b`>2)or(`a`=1 and `b`=2 and `c`>=3)\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t// special case: numbers has bigger lexicographically order but lower number\n\t\t{\n\t\t\t[]string{\"a\", \"b\", \"c\"},\n\t\t\t[]string{\"BIGINT\", \"BIGINT\", \"BIGINT\"},\n\t\t\t[][]driver.Value{\n\t\t\t\t{12, 2, 3},\n\t\t\t\t{111, 4, 5},\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"`a`<12 or(`a`=12 and `b`<2)or(`a`=12 and `b`=2 and `c`<3)\",\n\t\t\t\t\"(`a`>12 and `a`<111)or(`a`=12 and(`b`>2 or(`b`=2 and `c`>=3)))or(`a`=111 and(`b`<4 or(`b`=4 and `c`<5)))\", // should return sql correctly\n\t\t\t\t\"`a`>111 or(`a`=111 and `b`>4)or(`a`=111 and `b`=4 and `c`>=5)\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t// test string fields\n\t\t{\n\t\t\t[]string{\"a\", \"b\", \"c\"},\n\t\t\t[]string{\"BIGINT\", \"BIGINT\", \"varchar\"},\n\t\t\t[][]driver.Value{\n\t\t\t\t{1, 2, \"3\"},\n\t\t\t\t{1, 4, \"5\"},\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"`a`<1 or(`a`=1 and `b`<2)or(`a`=1 and `b`=2 and `c`<'3')\",\n\t\t\t\t\"`a`=1 and((`b`>2 and `b`<4)or(`b`=2 and(`c`>='3'))or(`b`=4 and(`c`<'5')))\",\n\t\t\t\t\"`a`>1 or(`a`=1 and `b`>4)or(`a`=1 and `b`=4 and `c`>='5')\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[]string{\"a\", \"b\", \"c\", \"d\"},\n\t\t\t[]string{\"BIGINT\", \"BIGINT\", \"BIGINT\", \"BIGINT\"},\n\t\t\t[][]driver.Value{\n\t\t\t\t{1, 2, 3, 4},\n\t\t\t\t{5, 6, 7, 8},\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"`a`<1 or(`a`=1 and `b`<2)or(`a`=1 and `b`=2 and `c`<3)or(`a`=1 and `b`=2 and `c`=3 and `d`<4)\",\n\t\t\t\t\"(`a`>1 and `a`<5)or(`a`=1 and(`b`>2 or(`b`=2 and `c`>3)or(`b`=2 and `c`=3 and `d`>=4)))or(`a`=5 and(`b`<6 or(`b`=6 and `c`<7)or(`b`=6 and `c`=7 and `d`<8)))\",\n\t\t\t\t\"`a`>5 or(`a`=5 and `b`>6)or(`a`=5 and `b`=6 and `c`>7)or(`a`=5 and `b`=6 and `c`=7 and `d`>=8)\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t}\n\ttransferHandleValStrings := func(handleColTypes []string, handleVals [][]driver.Value) [][]string {\n\t\thandleValStrings := make([][]string, 0, len(handleVals))\n\t\tfor _, handleVal := range handleVals {\n\t\t\thandleValString := make([]string, 0, len(handleVal))\n\t\t\tfor i, val := range handleVal {\n\t\t\t\trec := colTypeRowReceiverMap[strings.ToUpper(handleColTypes[i])]()\n\t\t\t\tvar valStr string\n\t\t\t\tswitch rec.(type) {\n\t\t\t\tcase *SQLTypeString:\n\t\t\t\t\tvalStr = fmt.Sprintf(\"'%s'\", val)\n\t\t\t\tcase *SQLTypeBytes:\n\t\t\t\t\tvalStr = fmt.Sprintf(\"x'%x'\", val)\n\t\t\t\tcase *SQLTypeNumber:\n\t\t\t\t\tvalStr = fmt.Sprintf(\"%d\", val)\n\t\t\t\t}\n\t\t\t\thandleValString = append(handleValString, valStr)\n\t\t\t}\n\t\t\thandleValStrings = append(handleValStrings, handleValString)\n\t\t}\n\t\treturn handleValStrings\n\t}\n\n\tfor caseID, testCase := range testCases {\n\t\tt.Logf(\"case #%d\", caseID)\n\t\thandleColNames := testCase.handleColNames\n\t\thandleColTypes := testCase.handleColTypes\n\t\thandleVals := testCase.handleVals\n\t\thandleValStrings := transferHandleValStrings(handleColTypes, handleVals)\n\n\t\t// Test build whereClauses\n\t\twhereClauses := buildWhereClauses(handleColNames, handleValStrings)\n\t\trequire.Equal(t, testCase.expectedWhereClauses, whereClauses)\n\n\t\t// Test build tasks through table sample\n\t\tif len(handleColNames) > 0 {\n\t\t\ttaskChan := make(chan Task, 128)\n\t\t\tquotaCols := make([]string, 0, len(handleColNames))\n\t\t\tfor _, col := range handleColNames {\n\t\t\t\tquotaCols = append(quotaCols, wrapBackTicks(col))\n\t\t\t}\n\t\t\tselectFields := strings.Join(quotaCols, \",\")\n\t\t\tmeta := &mockTableIR{\n\t\t\t\tdbName:           database,\n\t\t\t\ttblName:          table,\n\t\t\t\tselectedField:    selectFields,\n\t\t\t\thasImplicitRowID: testCase.hasTiDBRowID,\n\t\t\t\tcolTypes:         handleColTypes,\n\t\t\t\tcolNames:         handleColNames,\n\t\t\t\tspecCmt: []string{\n\t\t\t\t\t\"/*!40101 SET NAMES binary*/;\",\n\t\t\t\t},\n\t\t\t}\n\n\t\t\tif !testCase.hasTiDBRowID {\n\t\t\t\trows := sqlmock.NewRows(showIndexHeaders)\n\t\t\t\tfor i, handleColName := range handleColNames {\n\t\t\t\t\trows.AddRow(table, 0, \"PRIMARY\", i, handleColName, \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\")\n\t\t\t\t}\n\t\t\t\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).WillReturnRows(rows)\n\t\t\t}\n\n\t\t\trows := sqlmock.NewRows(handleColNames)\n\t\t\tfor _, handleVal := range handleVals {\n\t\t\t\trows.AddRow(handleVal...)\n\t\t\t}\n\t\t\tmock.ExpectQuery(fmt.Sprintf(\"SELECT .* FROM `%s`.`%s` TABLESAMPLE REGIONS\", database, table)).WillReturnRows(rows)\n\t\t\t// special case, no enough value to split chunks\n\t\t\tif len(handleVals) == 0 {\n\t\t\t\tif !testCase.hasTiDBRowID {\n\t\t\t\t\trows = sqlmock.NewRows(showIndexHeaders)\n\t\t\t\t\tfor i, handleColName := range handleColNames {\n\t\t\t\t\t\trows.AddRow(table, 0, \"PRIMARY\", i, handleColName, \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\")\n\t\t\t\t\t}\n\t\t\t\t\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).WillReturnRows(rows)\n\t\t\t\t\tmock.ExpectQuery(\"SHOW INDEX FROM\").WillReturnRows(sqlmock.NewRows(showIndexHeaders))\n\t\t\t\t} else {\n\t\t\t\t\td.conf.Rows = 200000\n\t\t\t\t\tmock.ExpectQuery(\"EXPLAIN SELECT `_tidb_rowid`\").\n\t\t\t\t\t\tWillReturnRows(sqlmock.NewRows([]string{\"id\", \"count\", \"task\", \"operator info\"}).\n\t\t\t\t\t\t\tAddRow(\"IndexReader_5\", \"0.00\", \"root\", \"index:IndexScan_4\"))\n\t\t\t\t}\n\t\t\t}\n\n\t\t\trequire.NoError(t, d.concurrentDumpTable(tctx, baseConn, meta, taskChan))\n\t\t\trequire.NoError(t, mock.ExpectationsWereMet())\n\t\t\torderByClause := buildOrderByClauseString(handleColNames)\n\n\t\t\tcheckQuery := func(i int, query string) {\n\t\t\t\ttask := <-taskChan\n\t\t\t\ttaskTableData, ok := task.(*TaskTableData)\n\t\t\t\trequire.True(t, ok)\n\t\t\t\trequire.Equal(t, i, taskTableData.ChunkIndex)\n\n\t\t\t\tdata, ok := taskTableData.Data.(*tableData)\n\t\t\t\trequire.True(t, ok)\n\t\t\t\trequire.Equal(t, query, data.query)\n\t\t\t}\n\n\t\t\t// special case, no value found\n\t\t\tif len(handleVals) == 0 {\n\t\t\t\tquery := buildSelectQuery(database, table, selectFields, \"\", \"\", orderByClause)\n\t\t\t\tcheckQuery(0, query)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tfor i, w := range testCase.expectedWhereClauses {\n\t\t\t\tquery := buildSelectQuery(database, table, selectFields, \"\", buildWhereCondition(d.conf, w), orderByClause)\n\t\t\t\tcheckQuery(i, query)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestBuildPartitionClauses(t *testing.T) {\n\tconst (\n\t\tdbName        = \"test\"\n\t\ttbName        = \"t\"\n\t\tfields        = \"*\"\n\t\tpartition     = \"p0\"\n\t\twhere         = \"WHERE a > 10\"\n\t\torderByClause = \"ORDER BY a\"\n\t)\n\ttestCases := []struct {\n\t\tpartition     string\n\t\twhere         string\n\t\torderByClause string\n\t\texpectedQuery string\n\t}{\n\t\t{\n\t\t\t\"\",\n\t\t\t\"\",\n\t\t\t\"\",\n\t\t\t\"SELECT * FROM `test`.`t`\",\n\t\t},\n\t\t{\n\t\t\tpartition,\n\t\t\t\"\",\n\t\t\t\"\",\n\t\t\t\"SELECT * FROM `test`.`t` PARTITION(`p0`)\",\n\t\t},\n\t\t{\n\t\t\tpartition,\n\t\t\twhere,\n\t\t\t\"\",\n\t\t\t\"SELECT * FROM `test`.`t` PARTITION(`p0`) WHERE a > 10\",\n\t\t},\n\t\t{\n\t\t\tpartition,\n\t\t\t\"\",\n\t\t\torderByClause,\n\t\t\t\"SELECT * FROM `test`.`t` PARTITION(`p0`) ORDER BY a\",\n\t\t},\n\t\t{\n\t\t\tpartition,\n\t\t\twhere,\n\t\t\torderByClause,\n\t\t\t\"SELECT * FROM `test`.`t` PARTITION(`p0`) WHERE a > 10 ORDER BY a\",\n\t\t},\n\t\t{\n\t\t\t\"\",\n\t\t\twhere,\n\t\t\torderByClause,\n\t\t\t\"SELECT * FROM `test`.`t` WHERE a > 10 ORDER BY a\",\n\t\t},\n\t}\n\tfor _, testCase := range testCases {\n\t\tquery := buildSelectQuery(dbName, tbName, fields, testCase.partition, testCase.where, testCase.orderByClause)\n\t\trequire.Equal(t, testCase.expectedQuery, query)\n\t}\n}\n\nfunc TestBuildWhereCondition(t *testing.T) {\n\tconf := DefaultConfig()\n\ttestCases := []struct {\n\t\tconfWhere     string\n\t\tchunkWhere    string\n\t\texpectedWhere string\n\t}{\n\t\t{\n\t\t\t\"\",\n\t\t\t\"\",\n\t\t\t\"\",\n\t\t},\n\t\t{\n\t\t\t\"a >= 1000000 and a <= 2000000\",\n\t\t\t\"\",\n\t\t\t\"WHERE a >= 1000000 and a <= 2000000 \",\n\t\t},\n\t\t{\n\t\t\t\"\",\n\t\t\t\"(`a`>1 and `a`<3)or(`a`=1 and(`b`>=2))or(`a`=3 and(`b`<4))\",\n\t\t\t\"WHERE (`a`>1 and `a`<3)or(`a`=1 and(`b`>=2))or(`a`=3 and(`b`<4)) \",\n\t\t},\n\t\t{\n\t\t\t\"a >= 1000000 and a <= 2000000\",\n\t\t\t\"(`a`>1 and `a`<3)or(`a`=1 and(`b`>=2))or(`a`=3 and(`b`<4))\",\n\t\t\t\"WHERE (a >= 1000000 and a <= 2000000) AND ((`a`>1 and `a`<3)or(`a`=1 and(`b`>=2))or(`a`=3 and(`b`<4))) \",\n\t\t},\n\t}\n\tfor _, testCase := range testCases {\n\t\tconf.Where = testCase.confWhere\n\t\twhere := buildWhereCondition(conf, testCase.chunkWhere)\n\t\trequire.Equal(t, testCase.expectedWhere, where)\n\t}\n}\n\nfunc TestBuildRegionQueriesWithoutPartition(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\tbaseConn := newBaseConn(conn, true, nil)\n\ttctx, cancel := tcontext.Background().WithLogger(appLogger).WithCancel()\n\tmetrics := newMetrics(promutil.NewDefaultFactory(), nil)\n\n\td := &Dumper{\n\t\ttctx:                      tctx,\n\t\tconf:                      DefaultConfig(),\n\t\tcancelCtx:                 cancel,\n\t\tmetrics:                   metrics,\n\t\tselectTiDBTableRegionFunc: selectTiDBTableRegion,\n\t}\n\td.conf.ServerInfo = version.ServerInfo{\n\t\tHasTiKV:       true,\n\t\tServerType:    version.ServerTypeTiDB,\n\t\tServerVersion: gcSafePointVersion,\n\t}\n\td.conf.Rows = 200000\n\tdatabase := \"foo\"\n\ttable := \"bar\"\n\n\ttestCases := []struct {\n\t\tregionResults        [][]driver.Value\n\t\thandleColNames       []string\n\t\thandleColTypes       []string\n\t\texpectedWhereClauses []string\n\t\thasTiDBRowID         bool\n\t}{\n\t\t{\n\t\t\t[][]driver.Value{\n\t\t\t\t{\"7480000000000000FF3300000000000000F8\", \"7480000000000000FF3300000000000000F8\"},\n\t\t\t},\n\t\t\t[]string{\"a\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[]string{\n\t\t\t\t\"\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[][]driver.Value{\n\t\t\t\t{\"7480000000000000FF3300000000000000F8\", \"7480000000000000FF3300000000000000F8\"},\n\t\t\t},\n\t\t\t[]string{\"_tidb_rowid\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[]string{\n\t\t\t\t\"\",\n\t\t\t},\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\t[][]driver.Value{\n\t\t\t\t{\"7480000000000000FF3300000000000000F8\", \"7480000000000000FF3300000000000000F8\"},\n\t\t\t\t{\"7480000000000000FF335F728000000000FF0EA6010000000000FA\", \"tableID=51, _tidb_rowid=960001\"},\n\t\t\t\t{\"7480000000000000FF335F728000000000FF1D4C010000000000FA\", \"tableID=51, _tidb_rowid=1920001\"},\n\t\t\t\t{\"7480000000000000FF335F728000000000FF2BF2010000000000FA\", \"tableID=51, _tidb_rowid=2880001\"},\n\t\t\t},\n\t\t\t[]string{\"a\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[]string{\n\t\t\t\t\"`a`<960001\",\n\t\t\t\t\"`a`>=960001 and `a`<1920001\",\n\t\t\t\t\"`a`>=1920001 and `a`<2880001\",\n\t\t\t\t\"`a`>=2880001\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[][]driver.Value{\n\t\t\t\t{\"7480000000000000FF3300000000000000F8\", \"7480000000000000FF3300000000000000F8\"},\n\t\t\t\t{\"7480000000000000FF335F728000000000FF0EA6010000000000FA\", \"tableID=51, _tidb_rowid=960001\"},\n\t\t\t\t// one invalid key\n\t\t\t\t{\"7520000000000000FF335F728000000000FF0EA6010000000000FA\", \"7520000000000000FF335F728000000000FF0EA6010000000000FA\"},\n\t\t\t\t{\"7480000000000000FF335F728000000000FF1D4C010000000000FA\", \"tableID=51, _tidb_rowid=1920001\"},\n\t\t\t\t{\"7480000000000000FF335F728000000000FF2BF2010000000000FA\", \"tableID=51, _tidb_rowid=2880001\"},\n\t\t\t},\n\t\t\t[]string{\"_tidb_rowid\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[]string{\n\t\t\t\t\"`_tidb_rowid`<960001\",\n\t\t\t\t\"`_tidb_rowid`>=960001 and `_tidb_rowid`<1920001\",\n\t\t\t\t\"`_tidb_rowid`>=1920001 and `_tidb_rowid`<2880001\",\n\t\t\t\t\"`_tidb_rowid`>=2880001\",\n\t\t\t},\n\t\t\ttrue,\n\t\t},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tt.Logf(\"case #%d\", i)\n\t\thandleColNames := testCase.handleColNames\n\t\thandleColTypes := testCase.handleColTypes\n\t\tregionResults := testCase.regionResults\n\n\t\t// Test build tasks through table region\n\t\ttaskChan := make(chan Task, 128)\n\t\tmeta := &mockTableIR{\n\t\t\tdbName:           database,\n\t\t\ttblName:          table,\n\t\t\tselectedField:    \"*\",\n\t\t\tselectedLen:      len(handleColNames),\n\t\t\thasImplicitRowID: testCase.hasTiDBRowID,\n\t\t\tcolTypes:         handleColTypes,\n\t\t\tcolNames:         handleColNames,\n\t\t\tspecCmt: []string{\n\t\t\t\t\"/*!40101 SET NAMES binary*/;\",\n\t\t\t},\n\t\t}\n\n\t\tmock.ExpectQuery(\"SELECT PARTITION_NAME from INFORMATION_SCHEMA.PARTITIONS\").\n\t\t\tWithArgs(database, table).WillReturnRows(sqlmock.NewRows([]string{\"PARTITION_NAME\"}).AddRow(nil))\n\n\t\tif !testCase.hasTiDBRowID {\n\t\t\trows := sqlmock.NewRows(showIndexHeaders)\n\t\t\tfor i, handleColName := range handleColNames {\n\t\t\t\trows.AddRow(table, 0, \"PRIMARY\", i, handleColName, \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\")\n\t\t\t}\n\t\t\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).WillReturnRows(rows)\n\t\t}\n\n\t\trows := sqlmock.NewRows([]string{\"START_KEY\", \"tidb_decode_key(START_KEY)\"})\n\t\tfor _, regionResult := range regionResults {\n\t\t\trows.AddRow(regionResult...)\n\t\t}\n\t\tmock.ExpectQuery(\"SELECT START_KEY,tidb_decode_key\\\\(START_KEY\\\\) from INFORMATION_SCHEMA.TIKV_REGION_STATUS\").\n\t\t\tWithArgs(database, table).WillReturnRows(rows)\n\n\t\torderByClause := buildOrderByClauseString(handleColNames)\n\t\t// special case, no enough value to split chunks\n\t\tif !testCase.hasTiDBRowID && len(regionResults) <= 1 {\n\t\t\trows = sqlmock.NewRows(showIndexHeaders)\n\t\t\tfor i, handleColName := range handleColNames {\n\t\t\t\trows.AddRow(table, 0, \"PRIMARY\", i, handleColName, \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\")\n\t\t\t}\n\t\t\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).WillReturnRows(rows)\n\t\t\tmock.ExpectQuery(\"SHOW INDEX FROM\").WillReturnRows(sqlmock.NewRows(showIndexHeaders))\n\t\t}\n\t\trequire.NoError(t, d.concurrentDumpTable(tctx, baseConn, meta, taskChan))\n\t\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\t\tfor i, w := range testCase.expectedWhereClauses {\n\t\t\tquery := buildSelectQuery(database, table, \"*\", \"\", buildWhereCondition(d.conf, w), orderByClause)\n\t\t\ttask := <-taskChan\n\t\t\ttaskTableData, ok := task.(*TaskTableData)\n\t\t\trequire.True(t, ok)\n\t\t\trequire.Equal(t, i, taskTableData.ChunkIndex)\n\t\t\tdata, ok := taskTableData.Data.(*tableData)\n\t\t\trequire.True(t, ok)\n\t\t\trequire.Equal(t, query, data.query)\n\t\t}\n\t}\n}\n\nfunc TestBuildRegionQueriesWithPartitions(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\tbaseConn := newBaseConn(conn, true, nil)\n\ttctx, cancel := tcontext.Background().WithLogger(appLogger).WithCancel()\n\tmetrics := newMetrics(promutil.NewDefaultFactory(), nil)\n\n\td := &Dumper{\n\t\ttctx:                      tctx,\n\t\tconf:                      DefaultConfig(),\n\t\tcancelCtx:                 cancel,\n\t\tmetrics:                   metrics,\n\t\tselectTiDBTableRegionFunc: selectTiDBTableRegion,\n\t}\n\td.conf.ServerInfo = version.ServerInfo{\n\t\tHasTiKV:       true,\n\t\tServerType:    version.ServerTypeTiDB,\n\t\tServerVersion: gcSafePointVersion,\n\t}\n\tpartitions := []string{\"p0\", \"p1\", \"p2\"}\n\n\ttestCases := []struct {\n\t\tregionResults        [][][]driver.Value\n\t\thandleColNames       []string\n\t\thandleColTypes       []string\n\t\texpectedWhereClauses [][]string\n\t\thasTiDBRowID         bool\n\t\tdumpWholeTable       bool\n\t}{\n\t\t{\n\t\t\t[][][]driver.Value{\n\t\t\t\t{\n\t\t\t\t\t{6009, \"t_121_i_1_0380000000000ea6010380000000000ea601\", \"t_121_\", 6010, 1, 6010, 0, 0, 0, 74, 1052002},\n\t\t\t\t\t{6011, \"t_121_\", \"t_121_i_1_0380000000000ea6010380000000000ea601\", 6012, 1, 6012, 0, 0, 0, 68, 972177},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t{6015, \"t_122_i_1_0380000000002d2a810380000000002d2a81\", \"t_122_\", 6016, 1, 6016, 0, 0, 0, 77, 1092962},\n\t\t\t\t\t{6017, \"t_122_\", \"t_122_i_1_0380000000002d2a810380000000002d2a81\", 6018, 1, 6018, 0, 0, 0, 66, 939975},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t{6021, \"t_123_i_1_0380000000004baf010380000000004baf01\", \"t_123_\", 6022, 1, 6022, 0, 0, 0, 85, 1206726},\n\t\t\t\t\t{6023, \"t_123_\", \"t_123_i_1_0380000000004baf010380000000004baf01\", 6024, 1, 6024, 0, 0, 0, 65, 927576},\n\t\t\t\t},\n\t\t\t},\n\t\t\t[]string{\"_tidb_rowid\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[][]string{\n\t\t\t\t{\"\"}, {\"\"}, {\"\"},\n\t\t\t},\n\t\t\ttrue,\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\t[][][]driver.Value{\n\t\t\t\t{\n\t\t\t\t\t{6009, \"t_121_i_1_0380000000000ea6010380000000000ea601\", \"t_121_r_10001\", 6010, 1, 6010, 0, 0, 0, 74, 1052002},\n\t\t\t\t\t{6013, \"t_121_r_10001\", \"t_121_r_970001\", 6014, 1, 6014, 0, 0, 0, 75, 975908},\n\t\t\t\t\t{6003, \"t_121_r_970001\", \"t_122_\", 6004, 1, 6004, 0, 0, 0, 79, 1022285},\n\t\t\t\t\t{6011, \"t_121_\", \"t_121_i_1_0380000000000ea6010380000000000ea601\", 6012, 1, 6012, 0, 0, 0, 68, 972177},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t{6015, \"t_122_i_1_0380000000002d2a810380000000002d2a81\", \"t_122_r_2070760\", 6016, 1, 6016, 0, 0, 0, 77, 1092962},\n\t\t\t\t\t{6019, \"t_122_r_2070760\", \"t_122_r_3047115\", 6020, 1, 6020, 0, 0, 0, 75, 959650},\n\t\t\t\t\t{6005, \"t_122_r_3047115\", \"t_123_\", 6006, 1, 6006, 0, 0, 0, 77, 992339},\n\t\t\t\t\t{6017, \"t_122_\", \"t_122_i_1_0380000000002d2a810380000000002d2a81\", 6018, 1, 6018, 0, 0, 0, 66, 939975},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t{6021, \"t_123_i_1_0380000000004baf010380000000004baf01\", \"t_123_r_4186953\", 6022, 1, 6022, 0, 0, 0, 85, 1206726},\n\t\t\t\t\t{6025, \"t_123_r_4186953\", \"t_123_r_5165682\", 6026, 1, 6026, 0, 0, 0, 74, 951379},\n\t\t\t\t\t{6007, \"t_123_r_5165682\", \"t_124_\", 6008, 1, 6008, 0, 0, 0, 71, 918488},\n\t\t\t\t\t{6023, \"t_123_\", \"t_123_i_1_0380000000004baf010380000000004baf01\", 6024, 1, 6024, 0, 0, 0, 65, 927576},\n\t\t\t\t},\n\t\t\t},\n\t\t\t[]string{\"_tidb_rowid\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[][]string{\n\t\t\t\t{\n\t\t\t\t\t\"`_tidb_rowid`<10001\",\n\t\t\t\t\t\"`_tidb_rowid`>=10001 and `_tidb_rowid`<970001\",\n\t\t\t\t\t\"`_tidb_rowid`>=970001\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"`_tidb_rowid`<2070760\",\n\t\t\t\t\t\"`_tidb_rowid`>=2070760 and `_tidb_rowid`<3047115\",\n\t\t\t\t\t\"`_tidb_rowid`>=3047115\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"`_tidb_rowid`<4186953\",\n\t\t\t\t\t\"`_tidb_rowid`>=4186953 and `_tidb_rowid`<5165682\",\n\t\t\t\t\t\"`_tidb_rowid`>=5165682\",\n\t\t\t\t},\n\t\t\t},\n\t\t\ttrue,\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[][][]driver.Value{\n\t\t\t\t{\n\t\t\t\t\t{6041, \"t_134_\", \"t_134_r_960001\", 6042, 1, 6042, 0, 0, 0, 69, 964987},\n\t\t\t\t\t{6035, \"t_134_r_960001\", \"t_135_\", 6036, 1, 6036, 0, 0, 0, 75, 1052130},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t{6043, \"t_135_\", \"t_135_r_2960001\", 6044, 1, 6044, 0, 0, 0, 69, 969576},\n\t\t\t\t\t{6037, \"t_135_r_2960001\", \"t_136_\", 6038, 1, 6038, 0, 0, 0, 72, 1014464},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t{6045, \"t_136_\", \"t_136_r_4960001\", 6046, 1, 6046, 0, 0, 0, 68, 957557},\n\t\t\t\t\t{6039, \"t_136_r_4960001\", \"t_137_\", 6040, 1, 6040, 0, 0, 0, 75, 1051579},\n\t\t\t\t},\n\t\t\t},\n\t\t\t[]string{\"a\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[][]string{\n\n\t\t\t\t{\n\t\t\t\t\t\"`a`<960001\",\n\t\t\t\t\t\"`a`>=960001\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"`a`<2960001\",\n\t\t\t\t\t\"`a`>=2960001\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"`a`<4960001\",\n\t\t\t\t\t\"`a`>=4960001\",\n\t\t\t\t},\n\t\t\t},\n\t\t\tfalse,\n\t\t\tfalse,\n\t\t},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tt.Logf(\"case #%d\", i)\n\t\thandleColNames := testCase.handleColNames\n\t\thandleColTypes := testCase.handleColTypes\n\t\tregionResults := testCase.regionResults\n\n\t\t// Test build tasks through table region\n\t\ttaskChan := make(chan Task, 128)\n\t\tmeta := &mockTableIR{\n\t\t\tdbName:           database,\n\t\t\ttblName:          table,\n\t\t\tselectedField:    \"*\",\n\t\t\tselectedLen:      len(handleColNames),\n\t\t\thasImplicitRowID: testCase.hasTiDBRowID,\n\t\t\tcolTypes:         handleColTypes,\n\t\t\tcolNames:         handleColNames,\n\t\t\tspecCmt: []string{\n\t\t\t\t\"/*!40101 SET NAMES binary*/;\",\n\t\t\t},\n\t\t}\n\n\t\trows := sqlmock.NewRows([]string{\"PARTITION_NAME\"})\n\t\tfor _, partition := range partitions {\n\t\t\trows.AddRow(partition)\n\t\t}\n\t\tmock.ExpectQuery(\"SELECT PARTITION_NAME from INFORMATION_SCHEMA.PARTITIONS\").\n\t\t\tWithArgs(database, table).WillReturnRows(rows)\n\n\t\tif !testCase.hasTiDBRowID {\n\t\t\trows = sqlmock.NewRows(showIndexHeaders)\n\t\t\tfor i, handleColName := range handleColNames {\n\t\t\t\trows.AddRow(table, 0, \"PRIMARY\", i, handleColName, \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\")\n\t\t\t}\n\t\t\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).WillReturnRows(rows)\n\t\t}\n\n\t\tfor i, partition := range partitions {\n\t\t\trows = sqlmock.NewRows([]string{\"REGION_ID\", \"START_KEY\", \"END_KEY\", \"LEADER_ID\", \"LEADER_STORE_ID\", \"PEERS\", \"SCATTERING\", \"WRITTEN_BYTES\", \"READ_BYTES\", \"APPROXIMATE_SIZE(MB)\", \"APPROXIMATE_KEYS\"})\n\t\t\tfor _, regionResult := range regionResults[i] {\n\t\t\t\trows.AddRow(regionResult...)\n\t\t\t}\n\t\t\tmock.ExpectQuery(fmt.Sprintf(\"SHOW TABLE `%s`.`%s` PARTITION\\\\(`%s`\\\\) REGIONS\", escapeString(database), escapeString(table), escapeString(partition))).\n\t\t\t\tWillReturnRows(rows)\n\t\t}\n\n\t\torderByClause := buildOrderByClauseString(handleColNames)\n\t\trequire.NoError(t, d.concurrentDumpTable(tctx, baseConn, meta, taskChan))\n\t\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\t\tchunkIdx := 0\n\t\tfor i, partition := range partitions {\n\t\t\tfor _, w := range testCase.expectedWhereClauses[i] {\n\t\t\t\tquery := buildSelectQuery(database, table, \"*\", partition, buildWhereCondition(d.conf, w), orderByClause)\n\t\t\t\ttask := <-taskChan\n\t\t\t\ttaskTableData, ok := task.(*TaskTableData)\n\t\t\t\trequire.True(t, ok)\n\t\t\t\trequire.Equal(t, chunkIdx, taskTableData.ChunkIndex)\n\t\t\t\tdata, ok := taskTableData.Data.(*tableData)\n\t\t\t\trequire.True(t, ok)\n\t\t\t\trequire.Equal(t, query, data.query)\n\t\t\t\tchunkIdx++\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc buildMockNewRows(mock sqlmock.Sqlmock, columns []string, driverValues [][]driver.Value) *sqlmock.Rows {\n\trows := mock.NewRows(columns)\n\tfor _, driverValue := range driverValues {\n\t\trows.AddRow(driverValue...)\n\t}\n\treturn rows\n}\n\nfunc readRegionCsvDriverValues(t *testing.T) [][]driver.Value {\n\tcsvFilename := \"region_results.csv\"\n\tfile, err := os.Open(csvFilename)\n\trequire.NoError(t, err)\n\tcsvReader := csv.NewReader(file)\n\tvalues := make([][]driver.Value, 0, 990)\n\tfor {\n\t\tresults, err := csvReader.Read()\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t}\n\t\trequire.NoError(t, err)\n\t\tif len(results) != 3 {\n\t\t\tcontinue\n\t\t}\n\t\tregionID, err := strconv.Atoi(results[0])\n\t\trequire.NoError(t, err)\n\t\tstartKey, endKey := results[1], results[2]\n\t\tvalues = append(values, []driver.Value{regionID, startKey, endKey})\n\t}\n\treturn values\n}\n\nfunc TestBuildVersion3RegionQueries(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\tbaseConn := newBaseConn(conn, true, nil)\n\ttctx, cancel := tcontext.Background().WithLogger(appLogger).WithCancel()\n\toldOpenFunc := openDBFunc\n\tdefer func() {\n\t\topenDBFunc = oldOpenFunc\n\t}()\n\topenDBFunc = func(_, _ string) (*sql.DB, error) {\n\t\treturn db, nil\n\t}\n\n\tconf := DefaultConfig()\n\tconf.ServerInfo = version.ServerInfo{\n\t\tHasTiKV:       true,\n\t\tServerType:    version.ServerTypeTiDB,\n\t\tServerVersion: decodeRegionVersion,\n\t}\n\tdatabase := \"test\"\n\tconf.Tables = DatabaseTables{\n\t\tdatabase: []*TableInfo{\n\t\t\t{\"t1\", 0, TableTypeBase},\n\t\t\t{\"t2\", 0, TableTypeBase},\n\t\t\t{\"t3\", 0, TableTypeBase},\n\t\t\t{\"t4\", 0, TableTypeBase},\n\t\t},\n\t}\n\tmetrics := newMetrics(promutil.NewDefaultFactory(), nil)\n\n\td := &Dumper{\n\t\ttctx:                      tctx,\n\t\tconf:                      conf,\n\t\tcancelCtx:                 cancel,\n\t\tmetrics:                   metrics,\n\t\tselectTiDBTableRegionFunc: selectTiDBTableRegion,\n\t}\n\tshowStatsHistograms := buildMockNewRows(mock, []string{\"Db_name\", \"Table_name\", \"Partition_name\", \"Column_name\", \"Is_index\", \"Update_time\", \"Distinct_count\", \"Null_count\", \"Avg_col_size\", \"Correlation\"},\n\t\t[][]driver.Value{\n\t\t\t{\"test\", \"t2\", \"p0\", \"a\", 0, \"2021-06-27 17:43:51\", 1999999, 0, 8, 0},\n\t\t\t{\"test\", \"t2\", \"p1\", \"a\", 0, \"2021-06-22 20:30:16\", 1260000, 0, 8, 0},\n\t\t\t{\"test\", \"t2\", \"p2\", \"a\", 0, \"2021-06-22 20:32:16\", 1230000, 0, 8, 0},\n\t\t\t{\"test\", \"t2\", \"p3\", \"a\", 0, \"2021-06-22 20:36:19\", 2000000, 0, 8, 0},\n\t\t\t{\"test\", \"t1\", \"\", \"a\", 0, \"2021-04-22 15:23:58\", 7100000, 0, 8, 0},\n\t\t\t{\"test\", \"t3\", \"\", \"PRIMARY\", 1, \"2021-06-27 22:08:43\", 4980000, 0, 0, 0},\n\t\t\t{\"test\", \"t4\", \"p0\", \"PRIMARY\", 1, \"2021-06-28 10:54:06\", 2000000, 0, 0, 0},\n\t\t\t{\"test\", \"t4\", \"p1\", \"PRIMARY\", 1, \"2021-06-28 10:55:04\", 1300000, 0, 0, 0},\n\t\t\t{\"test\", \"t4\", \"p2\", \"PRIMARY\", 1, \"2021-06-28 10:57:05\", 1830000, 0, 0, 0},\n\t\t\t{\"test\", \"t4\", \"p3\", \"PRIMARY\", 1, \"2021-06-28 10:59:04\", 2000000, 0, 0, 0},\n\t\t\t{\"mysql\", \"global_priv\", \"\", \"PRIMARY\", 1, \"2021-06-04 20:39:44\", 0, 0, 0, 0},\n\t\t})\n\tselectMySQLStatsHistograms := buildMockNewRows(mock, []string{\"TABLE_ID\", \"VERSION\", \"DISTINCT_COUNT\"},\n\t\t[][]driver.Value{\n\t\t\t{15, \"1970-01-01 08:00:00\", 0},\n\t\t\t{15, \"1970-01-01 08:00:00\", 0},\n\t\t\t{15, \"1970-01-01 08:00:00\", 0},\n\t\t\t{41, \"2021-04-22 15:23:58\", 7100000},\n\t\t\t{41, \"2021-04-22 15:23:59\", 7100000},\n\t\t\t{41, \"2021-04-22 15:23:59\", 7100000},\n\t\t\t{41, \"2021-04-22 15:23:59\", 7100000},\n\t\t\t{27, \"1970-01-01 08:00:00\", 0},\n\t\t\t{27, \"1970-01-01 08:00:00\", 0},\n\t\t\t{25, \"1970-01-01 08:00:00\", 0},\n\t\t\t{25, \"1970-01-01 08:00:00\", 0},\n\t\t\t{2098, \"2021-06-04 20:39:41\", 0},\n\t\t\t{2101, \"2021-06-04 20:39:44\", 0},\n\t\t\t{2101, \"2021-06-04 20:39:44\", 0},\n\t\t\t{2101, \"2021-06-04 20:39:44\", 0},\n\t\t\t{2101, \"2021-06-04 20:39:44\", 0},\n\t\t\t{2128, \"2021-06-22 20:29:19\", 1991680},\n\t\t\t{2128, \"2021-06-22 20:29:19\", 1991680},\n\t\t\t{2128, \"2021-06-22 20:29:19\", 1991680},\n\t\t\t{2129, \"2021-06-22 20:30:16\", 1260000},\n\t\t\t{2129, \"2021-06-22 20:30:16\", 1237120},\n\t\t\t{2129, \"2021-06-22 20:30:16\", 1237120},\n\t\t\t{2129, \"2021-06-22 20:30:16\", 1237120},\n\t\t\t{2130, \"2021-06-22 20:32:16\", 1230000},\n\t\t\t{2130, \"2021-06-22 20:32:16\", 1216128},\n\t\t\t{2130, \"2021-06-22 20:32:17\", 1216128},\n\t\t\t{2130, \"2021-06-22 20:32:17\", 1216128},\n\t\t\t{2131, \"2021-06-22 20:36:19\", 2000000},\n\t\t\t{2131, \"2021-06-22 20:36:19\", 1959424},\n\t\t\t{2131, \"2021-06-22 20:36:19\", 1959424},\n\t\t\t{2131, \"2021-06-22 20:36:19\", 1959424},\n\t\t\t{2128, \"2021-06-27 17:43:51\", 1999999},\n\t\t\t{2136, \"2021-06-27 22:08:38\", 4860000},\n\t\t\t{2136, \"2021-06-27 22:08:38\", 4860000},\n\t\t\t{2136, \"2021-06-27 22:08:38\", 4860000},\n\t\t\t{2136, \"2021-06-27 22:08:38\", 4860000},\n\t\t\t{2136, \"2021-06-27 22:08:43\", 4980000},\n\t\t\t{2139, \"2021-06-28 10:54:05\", 1991680},\n\t\t\t{2139, \"2021-06-28 10:54:05\", 1991680},\n\t\t\t{2139, \"2021-06-28 10:54:05\", 1991680},\n\t\t\t{2139, \"2021-06-28 10:54:05\", 1991680},\n\t\t\t{2139, \"2021-06-28 10:54:06\", 2000000},\n\t\t\t{2140, \"2021-06-28 10:55:02\", 1246336},\n\t\t\t{2140, \"2021-06-28 10:55:02\", 1246336},\n\t\t\t{2140, \"2021-06-28 10:55:02\", 1246336},\n\t\t\t{2140, \"2021-06-28 10:55:03\", 1246336},\n\t\t\t{2140, \"2021-06-28 10:55:04\", 1300000},\n\t\t\t{2141, \"2021-06-28 10:57:03\", 1780000},\n\t\t\t{2141, \"2021-06-28 10:57:03\", 1780000},\n\t\t\t{2141, \"2021-06-28 10:57:03\", 1780000},\n\t\t\t{2141, \"2021-06-28 10:57:03\", 1780000},\n\t\t\t{2141, \"2021-06-28 10:57:05\", 1830000},\n\t\t\t{2142, \"2021-06-28 10:59:03\", 1959424},\n\t\t\t{2142, \"2021-06-28 10:59:03\", 1959424},\n\t\t\t{2142, \"2021-06-28 10:59:03\", 1959424},\n\t\t\t{2142, \"2021-06-28 10:59:03\", 1959424},\n\t\t\t{2142, \"2021-06-28 10:59:04\", 2000000},\n\t\t})\n\tselectRegionStatusHistograms := buildMockNewRows(mock, []string{\"REGION_ID\", \"START_KEY\", \"END_KEY\"}, readRegionCsvDriverValues(t))\n\tselectInformationSchemaTables := buildMockNewRows(mock, []string{\"TABLE_SCHEMA\", \"TABLE_NAME\", \"TIDB_TABLE_ID\"},\n\t\t[][]driver.Value{\n\t\t\t{\"mysql\", \"expr_pushdown_blacklist\", 39},\n\t\t\t{\"mysql\", \"user\", 5},\n\t\t\t{\"mysql\", \"db\", 7},\n\t\t\t{\"mysql\", \"tables_priv\", 9},\n\t\t\t{\"mysql\", \"stats_top_n\", 37},\n\t\t\t{\"mysql\", \"columns_priv\", 11},\n\t\t\t{\"mysql\", \"bind_info\", 35},\n\t\t\t{\"mysql\", \"default_roles\", 33},\n\t\t\t{\"mysql\", \"role_edges\", 31},\n\t\t\t{\"mysql\", \"stats_feedback\", 29},\n\t\t\t{\"mysql\", \"gc_delete_range_done\", 27},\n\t\t\t{\"mysql\", \"gc_delete_range\", 25},\n\t\t\t{\"mysql\", \"help_topic\", 17},\n\t\t\t{\"mysql\", \"global_priv\", 2101},\n\t\t\t{\"mysql\", \"stats_histograms\", 21},\n\t\t\t{\"mysql\", \"opt_rule_blacklist\", 2098},\n\t\t\t{\"mysql\", \"stats_meta\", 19},\n\t\t\t{\"mysql\", \"stats_buckets\", 23},\n\t\t\t{\"mysql\", \"tidb\", 15},\n\t\t\t{\"mysql\", \"GLOBAL_VARIABLES\", 13},\n\t\t\t{\"test\", \"t2\", 2127},\n\t\t\t{\"test\", \"t1\", 41},\n\t\t\t{\"test\", \"t3\", 2136},\n\t\t\t{\"test\", \"t4\", 2138},\n\t\t})\n\tmock.ExpectQuery(\"SHOW STATS_HISTOGRAMS\").\n\t\tWillReturnRows(showStatsHistograms)\n\tmock.ExpectQuery(\"SELECT TABLE_ID,FROM_UNIXTIME\").\n\t\tWillReturnRows(selectMySQLStatsHistograms)\n\tmock.ExpectQuery(\"SELECT TABLE_SCHEMA,TABLE_NAME,TIDB_TABLE_ID FROM INFORMATION_SCHEMA.TABLES ORDER BY TABLE_SCHEMA\").\n\t\tWillReturnRows(selectInformationSchemaTables)\n\tmock.ExpectQuery(\"SELECT REGION_ID,START_KEY,END_KEY FROM INFORMATION_SCHEMA.TIKV_REGION_STATUS ORDER BY START_KEY;\").\n\t\tWillReturnRows(selectRegionStatusHistograms)\n\n\trequire.NoError(t, d.renewSelectTableRegionFuncForLowerTiDB(tctx))\n\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\ttestCases := []struct {\n\t\ttableName            string\n\t\thandleColNames       []string\n\t\thandleColTypes       []string\n\t\texpectedWhereClauses []string\n\t\thasTiDBRowID         bool\n\t}{\n\t\t{\n\t\t\t\"t1\",\n\t\t\t[]string{\"a\"},\n\t\t\t[]string{\"INT\"},\n\t\t\t[]string{\n\t\t\t\t\"`a`<960001\",\n\t\t\t\t\"`a`>=960001 and `a`<1920001\",\n\t\t\t\t\"`a`>=1920001 and `a`<2880001\",\n\t\t\t\t\"`a`>=2880001 and `a`<3840001\",\n\t\t\t\t\"`a`>=3840001 and `a`<4800001\",\n\t\t\t\t\"`a`>=4800001 and `a`<5760001\",\n\t\t\t\t\"`a`>=5760001 and `a`<6720001\",\n\t\t\t\t\"`a`>=6720001\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t\"t2\",\n\t\t\t[]string{\"a\"},\n\t\t\t[]string{\"INT\"},\n\t\t\t[]string{\n\t\t\t\t\"`a`<960001\",\n\t\t\t\t\"`a`>=960001 and `a`<2960001\",\n\t\t\t\t\"`a`>=2960001 and `a`<4960001\",\n\t\t\t\t\"`a`>=4960001 and `a`<6960001\",\n\t\t\t\t\"`a`>=6960001\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t\"t3\",\n\t\t\t[]string{\"_tidb_rowid\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[]string{\n\t\t\t\t\"`_tidb_rowid`<81584\",\n\t\t\t\t\"`_tidb_rowid`>=81584 and `_tidb_rowid`<1041584\",\n\t\t\t\t\"`_tidb_rowid`>=1041584 and `_tidb_rowid`<2001584\",\n\t\t\t\t\"`_tidb_rowid`>=2001584 and `_tidb_rowid`<2961584\",\n\t\t\t\t\"`_tidb_rowid`>=2961584 and `_tidb_rowid`<3921584\",\n\t\t\t\t\"`_tidb_rowid`>=3921584 and `_tidb_rowid`<4881584\",\n\t\t\t\t\"`_tidb_rowid`>=4881584 and `_tidb_rowid`<5841584\",\n\t\t\t\t\"`_tidb_rowid`>=5841584 and `_tidb_rowid`<6801584\",\n\t\t\t\t\"`_tidb_rowid`>=6801584\",\n\t\t\t},\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\t\"t4\",\n\t\t\t[]string{\"_tidb_rowid\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[]string{\n\t\t\t\t\"`_tidb_rowid`<180001\",\n\t\t\t\t\"`_tidb_rowid`>=180001 and `_tidb_rowid`<1140001\",\n\t\t\t\t\"`_tidb_rowid`>=1140001 and `_tidb_rowid`<2200001\",\n\t\t\t\t\"`_tidb_rowid`>=2200001 and `_tidb_rowid`<3160001\",\n\t\t\t\t\"`_tidb_rowid`>=3160001 and `_tidb_rowid`<4160001\",\n\t\t\t\t\"`_tidb_rowid`>=4160001 and `_tidb_rowid`<5120001\",\n\t\t\t\t\"`_tidb_rowid`>=5120001 and `_tidb_rowid`<6170001\",\n\t\t\t\t\"`_tidb_rowid`>=6170001 and `_tidb_rowid`<7130001\",\n\t\t\t\t\"`_tidb_rowid`>=7130001\",\n\t\t\t},\n\t\t\ttrue,\n\t\t},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tt.Logf(\"case #%d\", i)\n\t\ttable := testCase.tableName\n\t\thandleColNames := testCase.handleColNames\n\t\thandleColTypes := testCase.handleColTypes\n\n\t\t// Test build tasks through table region\n\t\ttaskChan := make(chan Task, 128)\n\t\tmeta := &mockTableIR{\n\t\t\tdbName:           database,\n\t\t\ttblName:          table,\n\t\t\tselectedField:    \"*\",\n\t\t\thasImplicitRowID: testCase.hasTiDBRowID,\n\t\t\tcolNames:         handleColNames,\n\t\t\tcolTypes:         handleColTypes,\n\t\t\tspecCmt: []string{\n\t\t\t\t\"/*!40101 SET NAMES binary*/;\",\n\t\t\t},\n\t\t}\n\n\t\tif !testCase.hasTiDBRowID {\n\t\t\trows := sqlmock.NewRows(showIndexHeaders)\n\t\t\tfor i, handleColName := range handleColNames {\n\t\t\t\trows.AddRow(table, 0, \"PRIMARY\", i, handleColName, \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\")\n\t\t\t}\n\t\t\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).WillReturnRows(rows)\n\t\t}\n\n\t\torderByClause := buildOrderByClauseString(handleColNames)\n\t\trequire.NoError(t, d.concurrentDumpTable(tctx, baseConn, meta, taskChan))\n\t\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\t\tchunkIdx := 0\n\t\tfor _, w := range testCase.expectedWhereClauses {\n\t\t\tquery := buildSelectQuery(database, table, \"*\", \"\", buildWhereCondition(d.conf, w), orderByClause)\n\t\t\ttask := <-taskChan\n\t\t\ttaskTableData, ok := task.(*TaskTableData)\n\t\t\trequire.True(t, ok)\n\t\t\trequire.Equal(t, chunkIdx, taskTableData.ChunkIndex)\n\n\t\t\tdata, ok := taskTableData.Data.(*tableData)\n\t\t\trequire.True(t, ok)\n\t\t\trequire.Equal(t, query, data.query)\n\n\t\t\tchunkIdx++\n\t\t}\n\t}\n}\n\nfunc TestCheckTiDBWithTiKV(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\t_ = db.Close()\n\t}()\n\n\ttidbConf := dbconfig.NewConfig()\n\tstores := []string{\"unistore\", \"mocktikv\", \"tikv\"}\n\tfor _, store := range stores {\n\t\ttidbConf.Store = store\n\t\ttidbConfBytes, err := json.Marshal(tidbConf)\n\t\trequire.NoError(t, err)\n\t\tmock.ExpectQuery(\"SELECT @@tidb_config\").WillReturnRows(\n\t\t\tsqlmock.NewRows([]string{\"@@tidb_config\"}).AddRow(string(tidbConfBytes)))\n\t\thasTiKV, err := CheckTiDBWithTiKV(db)\n\t\trequire.NoError(t, err)\n\t\tif store == \"tikv\" {\n\t\t\trequire.True(t, hasTiKV)\n\t\t} else {\n\t\t\trequire.False(t, hasTiKV)\n\t\t}\n\t\trequire.NoError(t, mock.ExpectationsWereMet())\n\t}\n\n\terrLackPrivilege := errors.New(\"ERROR 1142 (42000): SELECT command denied to user 'test'@'%' for table 'tidb'\")\n\texpectedResults := []interface{}{errLackPrivilege, 1, 0}\n\tfor i, res := range expectedResults {\n\t\tt.Logf(\"case #%d\", i)\n\t\tmock.ExpectQuery(\"SELECT @@tidb_config\").WillReturnError(errLackPrivilege)\n\t\texpectedErr, ok := res.(error)\n\t\tif ok {\n\t\t\tmock.ExpectQuery(\"SELECT COUNT\").WillReturnError(expectedErr)\n\t\t\thasTiKV, err := CheckTiDBWithTiKV(db)\n\t\t\trequire.ErrorIs(t, err, expectedErr)\n\t\t\trequire.True(t, hasTiKV)\n\t\t} else if cnt, ok := res.(int); ok {\n\t\t\tmock.ExpectQuery(\"SELECT COUNT\").WillReturnRows(\n\t\t\t\tsqlmock.NewRows([]string{\"c\"}).AddRow(cnt))\n\t\t\thasTiKV, err := CheckTiDBWithTiKV(db)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, cnt > 0, hasTiKV)\n\t\t}\n\t\trequire.NoError(t, mock.ExpectationsWereMet())\n\t}\n}\n\nfunc TestPickupPossibleField(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\ttctx := tcontext.Background().WithLogger(appLogger)\n\tbaseConn := newBaseConn(conn, true, nil)\n\n\tmeta := &mockTableIR{\n\t\tdbName:   database,\n\t\ttblName:  table,\n\t\tcolNames: []string{\"string1\", \"int1\", \"int2\", \"float1\", \"bin1\", \"int3\", \"bool1\", \"int4\"},\n\t\tcolTypes: []string{\"VARCHAR\", \"INT\", \"BIGINT\", \"FLOAT\", \"BINARY\", \"MEDIUMINT\", \"BOOL\", \"TINYINT\"},\n\t\tspecCmt: []string{\n\t\t\t\"/*!40101 SET NAMES binary*/;\",\n\t\t},\n\t}\n\n\ttestCases := []struct {\n\t\texpectedErr      error\n\t\texpectedField    string\n\t\thasImplicitRowID bool\n\t\tshowIndexResults [][]driver.Value\n\t}{\n\t\t{\n\t\t\terrors.New(\"show index error\"),\n\t\t\t\"\",\n\t\t\tfalse,\n\t\t\tnil,\n\t\t}, {\n\t\t\tnil,\n\t\t\t\"_tidb_rowid\",\n\t\t\ttrue,\n\t\t\tnil,\n\t\t}, // both primary and unique key columns are integers, use primary key first\n\t\t{\n\t\t\tnil,\n\t\t\t\"int1\",\n\t\t\tfalse,\n\t\t\t[][]driver.Value{\n\t\t\t\t{table, 0, \"PRIMARY\", 1, \"int1\", \"A\", 2, nil, nil, \"\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"PRIMARY\", 2, \"float1\", \"A\", 2, nil, nil, \"\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"int2\", 1, \"int2\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"string1\", 1, \"string1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"int3\", 1, \"int3\", \"A\", 20, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t},\n\t\t}, // primary key doesn't have integer at seq 1, use unique key with integer\n\t\t{\n\t\t\tnil,\n\t\t\t\"int2\",\n\t\t\tfalse,\n\t\t\t[][]driver.Value{\n\t\t\t\t{table, 0, \"PRIMARY\", 1, \"float1\", \"A\", 2, nil, nil, \"\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"PRIMARY\", 2, \"int1\", \"A\", 2, nil, nil, \"\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"int2\", 1, \"int2\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"string1\", 1, \"string1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"int3\", 1, \"int3\", \"A\", 20, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t},\n\t\t}, // several unique keys, use unique key who has a integer in seq 1\n\t\t{\n\t\t\tnil,\n\t\t\t\"int1\",\n\t\t\tfalse,\n\t\t\t[][]driver.Value{\n\t\t\t\t{table, 0, \"u1\", 1, \"int1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u1\", 2, \"string1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u1\", 3, \"bin1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u2\", 1, \"float1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u2\", 2, \"int2\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t},\n\t\t}, // several unique keys and ordinary keys, use unique key who has a integer in seq 1\n\t\t{\n\t\t\tnil,\n\t\t\t\"int1\",\n\t\t\tfalse,\n\t\t\t[][]driver.Value{\n\t\t\t\t{table, 0, \"u1\", 1, \"float1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u1\", 2, \"int2\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u2\", 1, \"int1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u2\", 2, \"string1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u2\", 3, \"bin1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"int3\", 1, \"int3\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t},\n\t\t}, // several unique keys and ordinary keys, use unique key who has less columns\n\t\t{\n\t\t\tnil,\n\t\t\t\"int2\",\n\t\t\tfalse,\n\t\t\t[][]driver.Value{\n\t\t\t\t{table, 0, \"u1\", 1, \"int1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u1\", 2, \"string1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u1\", 3, \"bin1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u2\", 1, \"int2\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u2\", 2, \"string1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"int3\", 1, \"int3\", \"A\", 20, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t},\n\t\t}, // several unique keys and ordinary keys, use key who has max cardinality\n\t\t{\n\t\t\tnil,\n\t\t\t\"int2\",\n\t\t\tfalse,\n\t\t\t[][]driver.Value{\n\t\t\t\t{table, 0, \"PRIMARY\", 1, \"string1\", \"A\", 2, nil, nil, \"\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u1\", 1, \"float1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u1\", 2, \"int3\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"i1\", 1, \"int1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"i2\", 1, \"int2\", \"A\", 5, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"i2\", 2, \"bool1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"i3\", 1, \"bin1\", \"A\", 10, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"i3\", 2, \"int4\", \"A\", 10, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t},\n\t\t},\n\t}\n\n\tquery := fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)\n\tfor i, testCase := range testCases {\n\t\tt.Logf(\"case #%d\", i)\n\n\t\tmeta.hasImplicitRowID = testCase.hasImplicitRowID\n\t\texpectedErr := testCase.expectedErr\n\t\tif expectedErr != nil {\n\t\t\tmock.ExpectQuery(query).WillReturnError(expectedErr)\n\t\t} else if !testCase.hasImplicitRowID {\n\t\t\trows := sqlmock.NewRows(showIndexHeaders)\n\t\t\tfor _, showIndexResult := range testCase.showIndexResults {\n\t\t\t\trows.AddRow(showIndexResult...)\n\t\t\t}\n\t\t\tmock.ExpectQuery(query).WillReturnRows(rows)\n\t\t}\n\n\t\tfield, err := pickupPossibleField(tctx, meta, baseConn)\n\t\tif expectedErr != nil {\n\t\t\trequire.ErrorIs(t, err, expectedErr)\n\t\t} else {\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, testCase.expectedField, field)\n\t\t}\n\t\trequire.NoError(t, mock.ExpectationsWereMet())\n\t}\n}\n\nfunc TestCheckIfSeqExists(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\n\tmock.ExpectQuery(\"SELECT COUNT\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"c\"}).\n\t\t\tAddRow(\"1\"))\n\n\texists, err := CheckIfSeqExists(conn)\n\trequire.NoError(t, err)\n\trequire.Equal(t, true, exists)\n\n\tmock.ExpectQuery(\"SELECT COUNT\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"c\"}).\n\t\t\tAddRow(\"0\"))\n\n\texists, err = CheckIfSeqExists(conn)\n\trequire.NoError(t, err)\n\trequire.Equal(t, false, exists)\n}\n\nfunc TestGetCharsetAndDefaultCollation(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\tctx := context.Background()\n\tconn, err := db.Conn(ctx)\n\trequire.NoError(t, err)\n\n\tmock.ExpectQuery(\"SHOW CHARACTER SET\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Charset\", \"Description\", \"Default collation\", \"Maxlen\"}).\n\t\t\tAddRow(\"utf8mb4\", \"UTF-8 Unicode\", \"utf8mb4_0900_ai_ci\", 4).\n\t\t\tAddRow(\"latin1\", \"cp1252 West European\", \"latin1_swedish_ci\", 1))\n\n\tcharsetAndDefaultCollation, err := GetCharsetAndDefaultCollation(ctx, conn)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"utf8mb4_0900_ai_ci\", charsetAndDefaultCollation[\"utf8mb4\"])\n\trequire.Equal(t, \"latin1_swedish_ci\", charsetAndDefaultCollation[\"latin1\"])\n}\n\nfunc TestGetSpecifiedColumnValueAndClose(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\tctx := context.Background()\n\tconn, err := db.Conn(ctx)\n\trequire.NoError(t, err)\n\n\tmock.ExpectQuery(\"SHOW BINARY LOGS\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Log_name\", \"File_size\"}).\n\t\t\tAddRow(\"mysql-bin.000001\", 52119).\n\t\t\tAddRow(\"mysql-bin.000002\", 114))\n\n\tquery := \"SHOW BINARY LOGS\"\n\trows, err := conn.QueryContext(ctx, query)\n\trequire.NoError(t, err)\n\tdefer rows.Close()\n\tvar rowsResult []string\n\trowsResult, err = GetSpecifiedColumnValueAndClose(rows, \"Log_name\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, 2, len(rowsResult))\n\trequire.Equal(t, \"mysql-bin.000001\", rowsResult[0])\n\trequire.Equal(t, \"mysql-bin.000002\", rowsResult[1])\n\n\terr = mock.ExpectationsWereMet()\n\trequire.NoError(t, err)\n}\n\nfunc TestGetSpecifiedColumnValuesAndClose(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\tctx := context.Background()\n\tconn, err := db.Conn(ctx)\n\trequire.NoError(t, err)\n\n\tmock.ExpectQuery(\"SHOW BINARY LOGS\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Log_name\", \"File_size\"}).\n\t\t\tAddRow(\"mysql-bin.000001\", 52119).\n\t\t\tAddRow(\"mysql-bin.000002\", 114))\n\n\tquery := \"SHOW BINARY LOGS\"\n\trows, err := conn.QueryContext(ctx, query)\n\trequire.NoError(t, err)\n\tdefer rows.Close()\n\tvar rowsResult [][]string\n\trowsResult, err = GetSpecifiedColumnValuesAndClose(rows, \"Log_name\", \"File_size\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, 2, len(rowsResult))\n\trequire.Equal(t, 2, len(rowsResult[0]))\n\trequire.Equal(t, \"mysql-bin.000001\", rowsResult[0][0])\n\trequire.Equal(t, \"52119\", rowsResult[0][1])\n\trequire.Equal(t, \"mysql-bin.000002\", rowsResult[1][0])\n\trequire.Equal(t, \"114\", rowsResult[1][1])\n\n\tmock.ExpectQuery(\"SHOW BINARY LOGS\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Log_name\", \"File_size\", \"Encrypted\"}).\n\t\t\tAddRow(\"mysql-bin.000001\", 52119, \"No\").\n\t\t\tAddRow(\"mysql-bin.000002\", 114, \"No\"))\n\n\trows2, err := conn.QueryContext(ctx, query)\n\trequire.NoError(t, err)\n\tdefer rows2.Close()\n\tvar rowsResult2 [][]string\n\trowsResult2, err = GetSpecifiedColumnValuesAndClose(rows2, \"Log_name\", \"File_size\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, 2, len(rowsResult2))\n\trequire.Equal(t, 2, len(rowsResult2[0]))\n\trequire.Equal(t, \"mysql-bin.000001\", rowsResult2[0][0])\n\trequire.Equal(t, \"52119\", rowsResult2[0][1])\n\trequire.Equal(t, \"mysql-bin.000002\", rowsResult2[1][0])\n\trequire.Equal(t, \"114\", rowsResult2[1][1])\n\n\terr = mock.ExpectationsWereMet()\n\trequire.NoError(t, err)\n}\n", "// Copyright 2021 PingCAP, Inc. Licensed under Apache-2.0.\n\npackage main\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"fmt\"\n\t\"os\"\n\n\t_ \"github.com/go-sql-driver/mysql\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/spf13/cobra\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\nconst (\n\tflagDatabase = \"database\"\n\tflagTable    = \"table\"\n\tflagPort     = \"port\"\n\tflagWorker   = \"worker\"\n)\n\nvar rootCmd *cobra.Command\n\nfunc main() {\n\trootCmd = &cobra.Command{}\n\trootCmd.Flags().StringP(flagDatabase, \"B\", \"s3\", \"Database to import\")\n\trootCmd.Flags().StringP(flagTable, \"T\", \"t\", \"Table to import\")\n\trootCmd.Flags().IntP(flagPort, \"P\", 4000, \"TCP/IP port to connect to\")\n\trootCmd.Flags().IntP(flagWorker, \"w\", 16, \"Workers to import synchronously\")\n\n\trootCmd.RunE = func(cmd *cobra.Command, args []string) error {\n\t\tdatabase, err := cmd.Flags().GetString(flagDatabase)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\ttable, err := cmd.Flags().GetString(flagTable)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tport, err := cmd.Flags().GetInt(flagPort)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tworker, err := cmd.Flags().GetInt(flagWorker)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\n\t\tdsn := fmt.Sprintf(\"%s:%s@tcp(%s:%d)/%s?charset=utf8mb4\", \"root\", \"\", \"127.0.0.1\", port, database)\n\t\tdb, err := sql.Open(\"mysql\", dsn)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\n\t\ttableTemp := `CREATE TABLE IF NOT EXISTS %s (\n\t   a VARCHAR(11)\n)`\n\t\t_, err = db.Exec(fmt.Sprintf(tableTemp, table))\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\n\t\tquery := fmt.Sprintf(\"insert into %s values('aaaaaaaaaa')\", table) // nolint:gosec\n\t\tfor i := 1; i < 10000; i++ {\n\t\t\tquery += \",('aaaaaaaaaa')\"\n\t\t}\n\t\tch := make(chan struct{}, worker)\n\t\tfor i := 0; i < worker; i++ {\n\t\t\tch <- struct{}{}\n\t\t}\n\t\tvar eg *errgroup.Group\n\t\tctx, cancel := context.WithCancel(context.Background())\n\t\tdefer cancel()\n\t\teg, ctx = errgroup.WithContext(ctx)\n\t\tfor i := 0; i < 500; i++ {\n\t\t\tif ctx.Err() != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\t<-ch\n\t\t\teg.Go(func() error {\n\t\t\t\t_, err := db.ExecContext(ctx, query)\n\t\t\t\tif err != nil {\n\t\t\t\t\tcancel()\n\t\t\t\t\treturn errors.Trace(err)\n\t\t\t\t}\n\t\t\t\tch <- struct{}{}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t}\n\t\treturn eg.Wait()\n\t}\n\n\tif err := rootCmd.Execute(); err != nil {\n\t\tfmt.Printf(\"fail to import data, err: %v\", err)\n\t\tos.Exit(2)\n\t}\n}\n", "// Copyright 2022 PingCAP, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage dbutil\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"net/url\"\n\t\"os\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/go-sql-driver/mysql\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/log\"\n\t\"github.com/pingcap/tidb/infoschema\"\n\t\"github.com/pingcap/tidb/parser\"\n\t\"github.com/pingcap/tidb/parser/model\"\n\ttmysql \"github.com/pingcap/tidb/parser/mysql\"\n\t\"github.com/pingcap/tidb/sessionctx/stmtctx\"\n\t\"github.com/pingcap/tidb/types\"\n\t\"github.com/pingcap/tidb/util\"\n\t\"github.com/pingcap/tidb/util/dbterror\"\n\t\"go.uber.org/zap\"\n)\n\nconst (\n\t// DefaultRetryTime is the default retry time to execute sql\n\tDefaultRetryTime = 10\n\n\t// DefaultTimeout is the default timeout for execute sql\n\tDefaultTimeout time.Duration = 10 * time.Second\n\n\t// SlowLogThreshold defines the duration to log debug log of sql when exec time greater than\n\tSlowLogThreshold = 200 * time.Millisecond\n\n\t// DefaultDeleteRowsNum is the default rows num for delete one time\n\tDefaultDeleteRowsNum int64 = 100000\n)\n\nvar (\n\t// ErrVersionNotFound means can't get the database's version\n\tErrVersionNotFound = errors.New(\"can't get the database's version\")\n\n\t// ErrNoData means no data in table\n\tErrNoData = errors.New(\"no data found in table\")\n)\n\n// DBConfig is database configuration.\ntype DBConfig struct {\n\tHost     string `toml:\"host\" json:\"host\"`\n\tUser     string `toml:\"user\" json:\"user\"`\n\tPassword string `toml:\"password\" json:\"-\"`\n\tSchema   string `toml:\"schema\" json:\"schema\"`\n\tSnapshot string `toml:\"snapshot\" json:\"snapshot\"`\n\tPort     int    `toml:\"port\" json:\"port\"`\n}\n\n// String returns native format of database configuration\nfunc (c *DBConfig) String() string {\n\tcfg, err := json.Marshal(c)\n\tif err != nil {\n\t\treturn \"<nil>\"\n\t}\n\treturn string(cfg)\n}\n\n// GetDBConfigFromEnv returns DBConfig from environment\nfunc GetDBConfigFromEnv(schema string) DBConfig {\n\thost := os.Getenv(\"MYSQL_HOST\")\n\tif host == \"\" {\n\t\thost = \"127.0.0.1\"\n\t}\n\tport, _ := strconv.Atoi(os.Getenv(\"MYSQL_PORT\"))\n\tif port == 0 {\n\t\tport = 3306\n\t}\n\tuser := os.Getenv(\"MYSQL_USER\")\n\tif user == \"\" {\n\t\tuser = \"root\"\n\t}\n\tpswd := os.Getenv(\"MYSQL_PSWD\")\n\n\treturn DBConfig{\n\t\tHost:     host,\n\t\tPort:     port,\n\t\tUser:     user,\n\t\tPassword: pswd,\n\t\tSchema:   schema,\n\t}\n}\n\n// OpenDB opens a mysql connection FD\nfunc OpenDB(cfg DBConfig, vars map[string]string) (*sql.DB, error) {\n\tvar dbDSN string\n\tif len(cfg.Snapshot) != 0 {\n\t\tlog.Info(\"create connection with snapshot\", zap.String(\"snapshot\", cfg.Snapshot))\n\t\tdbDSN = fmt.Sprintf(\"%s:%s@tcp(%s:%d)/?charset=utf8mb4&tidb_snapshot=%s\", cfg.User, cfg.Password, cfg.Host, cfg.Port, cfg.Snapshot)\n\t} else {\n\t\tdbDSN = fmt.Sprintf(\"%s:%s@tcp(%s:%d)/?charset=utf8mb4\", cfg.User, cfg.Password, cfg.Host, cfg.Port)\n\t}\n\n\tfor key, val := range vars {\n\t\t// key='val'. add single quote for better compatibility.\n\t\tdbDSN += fmt.Sprintf(\"&%s=%%27%s%%27\", key, url.QueryEscape(val))\n\t}\n\n\tdbConn, err := sql.Open(\"mysql\", dbDSN)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\terr = dbConn.Ping()\n\treturn dbConn, errors.Trace(err)\n}\n\n// CloseDB closes the mysql fd\nfunc CloseDB(db *sql.DB) error {\n\tif db == nil {\n\t\treturn nil\n\t}\n\n\treturn errors.Trace(db.Close())\n}\n\n// GetCreateTableSQL returns the create table statement.\nfunc GetCreateTableSQL(ctx context.Context, db QueryExecutor, schemaName string, tableName string) (string, error) {\n\t/*\n\t\tshow create table example result:\n\t\tmysql> SHOW CREATE TABLE `test`.`itest`;\n\t\t+-------+--------------------------------------------------------------------+\n\t\t| Table | Create Table                                                                                                                              |\n\t\t+-------+--------------------------------------------------------------------+\n\t\t| itest | CREATE TABLE `itest` (\n\t\t\t`id` int(11) DEFAULT NULL,\n\t\t  \t`name` varchar(24) DEFAULT NULL\n\t\t\t) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin |\n\t\t+-------+--------------------------------------------------------------------+\n\t*/\n\tquery := fmt.Sprintf(\"SHOW CREATE TABLE %s\", TableName(schemaName, tableName))\n\n\tvar tbl, createTable sql.NullString\n\terr := db.QueryRowContext(ctx, query).Scan(&tbl, &createTable)\n\tif err != nil {\n\t\treturn \"\", errors.Trace(err)\n\t}\n\tif !tbl.Valid || !createTable.Valid {\n\t\treturn \"\", errors.NotFoundf(\"table %s\", tableName)\n\t}\n\n\treturn createTable.String, nil\n}\n\n// GetRowCount returns row count of the table.\n// if not specify where condition, return total row count of the table.\nfunc GetRowCount(ctx context.Context, db QueryExecutor, schemaName string, tableName string, where string, args []interface{}) (int64, error) {\n\t/*\n\t\tselect count example result:\n\t\tmysql> SELECT count(1) cnt from `test`.`itest` where id > 0;\n\t\t+------+\n\t\t| cnt  |\n\t\t+------+\n\t\t|  100 |\n\t\t+------+\n\t*/\n\n\tquery := fmt.Sprintf(\"SELECT COUNT(1) cnt FROM %s\", TableName(schemaName, tableName))\n\tif len(where) > 0 {\n\t\tquery += fmt.Sprintf(\" WHERE %s\", where)\n\t}\n\tlog.Debug(\"get row count\", zap.String(\"sql\", query), zap.Reflect(\"args\", args))\n\n\tvar cnt sql.NullInt64\n\terr := db.QueryRowContext(ctx, query, args...).Scan(&cnt)\n\tif err != nil {\n\t\treturn 0, errors.Trace(err)\n\t}\n\tif !cnt.Valid {\n\t\treturn 0, errors.NotFoundf(\"table `%s`.`%s`\", schemaName, tableName)\n\t}\n\n\treturn cnt.Int64, nil\n}\n\n// GetRandomValues returns some random value. Tips: limitArgs is the value in limitRange.\nfunc GetRandomValues(ctx context.Context, db QueryExecutor, schemaName, table, column string, num int, limitRange string, limitArgs []interface{}, collation string) ([]string, error) {\n\t/*\n\t\texample:\n\t\tmysql> SELECT `id` FROM (SELECT `id`, rand() rand_value FROM `test`.`test`  WHERE `id` COLLATE \"latin1_bin\" > 0 AND `id` COLLATE \"latin1_bin\" < 100 ORDER BY rand_value LIMIT 5) rand_tmp ORDER BY `id` COLLATE \"latin1_bin\";\n\t\t+------+\n\t\t| id   |\n\t\t+------+\n\t\t|    1 |\n\t\t|    2 |\n\t\t|    3 |\n\t\t+------+\n\t*/\n\n\tif limitRange == \"\" {\n\t\tlimitRange = \"TRUE\"\n\t}\n\n\tif collation != \"\" {\n\t\tcollation = fmt.Sprintf(\" COLLATE \\\"%s\\\"\", collation)\n\t}\n\n\tquery := fmt.Sprintf(\"SELECT %[1]s FROM (SELECT %[1]s, rand() rand_value FROM %[2]s WHERE %[3]s ORDER BY rand_value LIMIT %[4]d)rand_tmp ORDER BY %[1]s%[5]s\",\n\t\tColumnName(column), TableName(schemaName, table), limitRange, num, collation)\n\tlog.Debug(\"get random values\", zap.String(\"sql\", query), zap.Reflect(\"args\", limitArgs))\n\n\trows, err := db.QueryContext(ctx, query, limitArgs...)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tdefer rows.Close()\n\n\trandomValue := make([]string, 0, num)\n\tfor rows.Next() {\n\t\tvar value sql.NullString\n\t\terr = rows.Scan(&value)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\t\tif value.Valid {\n\t\t\trandomValue = append(randomValue, value.String)\n\t\t}\n\t}\n\n\treturn randomValue, errors.Trace(rows.Err())\n}\n\n// GetMinMaxValue return min and max value of given column by specified limitRange condition.\nfunc GetMinMaxValue(ctx context.Context, db QueryExecutor, schema, table, column string, limitRange string, limitArgs []interface{}, collation string) (minStr string, maxStr string, err error) {\n\t/*\n\t\texample:\n\t\tmysql> SELECT MIN(`id`) as MIN, MAX(`id`) as MAX FROM `test`.`testa` WHERE id > 0 AND id < 10;\n\t\t+------+------+\n\t\t| MIN  | MAX  |\n\t\t+------+------+\n\t\t|    1 |    2 |\n\t\t+------+------+\n\t*/\n\n\tif limitRange == \"\" {\n\t\tlimitRange = \"TRUE\"\n\t}\n\n\tif collation != \"\" {\n\t\tcollation = fmt.Sprintf(\" COLLATE \\\"%s\\\"\", collation)\n\t}\n\n\tquery := fmt.Sprintf(\"SELECT /*!40001 SQL_NO_CACHE */ MIN(%s%s) as MIN, MAX(%s%s) as MAX FROM %s WHERE %s\",\n\t\tColumnName(column), collation, ColumnName(column), collation, TableName(schema, table), limitRange)\n\tlog.Debug(\"GetMinMaxValue\", zap.String(\"sql\", query), zap.Reflect(\"args\", limitArgs))\n\n\tvar min, max sql.NullString\n\trows, err := db.QueryContext(ctx, query, limitArgs...)\n\tif err != nil {\n\t\treturn \"\", \"\", errors.Trace(err)\n\t}\n\tdefer rows.Close()\n\n\tfor rows.Next() {\n\t\terr = rows.Scan(&min, &max)\n\t\tif err != nil {\n\t\t\treturn \"\", \"\", errors.Trace(err)\n\t\t}\n\t}\n\n\tif !min.Valid || !max.Valid {\n\t\t// don't have any data\n\t\treturn \"\", \"\", ErrNoData\n\t}\n\n\treturn min.String, max.String, errors.Trace(rows.Err())\n}\n\n// GetTimeZoneOffset is to get offset of timezone.\nfunc GetTimeZoneOffset(ctx context.Context, db QueryExecutor) (time.Duration, error) {\n\tvar timeStr string\n\terr := db.QueryRowContext(ctx, \"SELECT cast(TIMEDIFF(NOW(6), UTC_TIMESTAMP(6)) as time);\").Scan(&timeStr)\n\tif err != nil {\n\t\treturn 0, errors.Trace(err)\n\t}\n\tfactor := time.Duration(1)\n\tif timeStr[0] == '-' || timeStr[0] == '+' {\n\t\tif timeStr[0] == '-' {\n\t\t\tfactor *= -1\n\t\t}\n\t\ttimeStr = timeStr[1:]\n\t}\n\tt, err := time.Parse(\"15:04:05\", timeStr)\n\tif err != nil {\n\t\treturn 0, errors.Trace(err)\n\t}\n\n\tif t.IsZero() {\n\t\treturn 0, nil\n\t}\n\n\thour, minute, second := t.Clock()\n\t//nolint:durationcheck\n\treturn time.Duration(hour*3600+minute*60+second) * time.Second * factor, nil\n}\n\n// FormatTimeZoneOffset is to format offset of timezone.\nfunc FormatTimeZoneOffset(offset time.Duration) string {\n\tprefix := \"+\"\n\tif offset < 0 {\n\t\tprefix = \"-\"\n\t\toffset *= -1\n\t}\n\thours := offset / time.Hour\n\tminutes := (offset % time.Hour) / time.Minute\n\n\treturn fmt.Sprintf(\"%s%02d:%02d\", prefix, hours, minutes)\n}\n\nfunc queryTables(ctx context.Context, db QueryExecutor, q string) (tables []string, err error) {\n\tlog.Debug(\"query tables\", zap.String(\"query\", q))\n\trows, err := db.QueryContext(ctx, q)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tdefer rows.Close()\n\n\ttables = make([]string, 0, 8)\n\tfor rows.Next() {\n\t\tvar table, tType sql.NullString\n\t\terr = rows.Scan(&table, &tType)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\n\t\tif !table.Valid || !tType.Valid {\n\t\t\tcontinue\n\t\t}\n\n\t\ttables = append(tables, table.String)\n\t}\n\n\treturn tables, errors.Trace(rows.Err())\n}\n\n// GetTables returns name of all tables in the specified schema\nfunc GetTables(ctx context.Context, db QueryExecutor, schemaName string) (tables []string, err error) {\n\t/*\n\t\tshow tables without view: https://dev.mysql.com/doc/refman/5.7/en/show-tables.html\n\n\t\texample:\n\t\tmysql> show full tables in test where Table_Type != 'VIEW';\n\t\t+----------------+------------+\n\t\t| Tables_in_test | Table_type |\n\t\t+----------------+------------+\n\t\t| NTEST          | BASE TABLE |\n\t\t+----------------+------------+\n\t*/\n\tquery := fmt.Sprintf(\"SHOW FULL TABLES IN `%s` WHERE Table_Type != 'VIEW';\", escapeName(schemaName))\n\treturn queryTables(ctx, db, query)\n}\n\n// GetViews returns names of all views in the specified schema\nfunc GetViews(ctx context.Context, db QueryExecutor, schemaName string) (tables []string, err error) {\n\tquery := fmt.Sprintf(\"SHOW FULL TABLES IN `%s` WHERE Table_Type = 'VIEW';\", escapeName(schemaName))\n\treturn queryTables(ctx, db, query)\n}\n\n// GetSchemas returns name of all schemas\nfunc GetSchemas(ctx context.Context, db QueryExecutor) ([]string, error) {\n\tquery := \"SHOW DATABASES\"\n\trows, err := db.QueryContext(ctx, query)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tdefer rows.Close()\n\n\t// show an example.\n\t/*\n\t\tmysql> SHOW DATABASES;\n\t\t+--------------------+\n\t\t| Database           |\n\t\t+--------------------+\n\t\t| information_schema |\n\t\t| mysql              |\n\t\t| performance_schema |\n\t\t| sys                |\n\t\t| test_db            |\n\t\t+--------------------+\n\t*/\n\tschemas := make([]string, 0, 10)\n\tfor rows.Next() {\n\t\tvar schema string\n\t\terr = rows.Scan(&schema)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\t\tschemas = append(schemas, schema)\n\t}\n\treturn schemas, errors.Trace(rows.Err())\n}\n\n// GetCRC32Checksum returns checksum code of some data by given condition\nfunc GetCRC32Checksum(ctx context.Context, db QueryExecutor, schemaName, tableName string, tbInfo *model.TableInfo, limitRange string, args []interface{}) (int64, error) {\n\t/*\n\t\tcalculate CRC32 checksum example:\n\t\tmysql> SELECT BIT_XOR(CAST(CRC32(CONCAT_WS(',', id, name, age, CONCAT(ISNULL(id), ISNULL(name), ISNULL(age))))AS UNSIGNED)) AS checksum FROM test.test WHERE id > 0 AND id < 10;\n\t\t+------------+\n\t\t| checksum   |\n\t\t+------------+\n\t\t| 1466098199 |\n\t\t+------------+\n\t*/\n\tcolumnNames := make([]string, 0, len(tbInfo.Columns))\n\tcolumnIsNull := make([]string, 0, len(tbInfo.Columns))\n\tfor _, col := range tbInfo.Columns {\n\t\tcolumnNames = append(columnNames, ColumnName(col.Name.O))\n\t\tcolumnIsNull = append(columnIsNull, fmt.Sprintf(\"ISNULL(%s)\", ColumnName(col.Name.O)))\n\t}\n\n\tquery := fmt.Sprintf(\"SELECT BIT_XOR(CAST(CRC32(CONCAT_WS(',', %s, CONCAT(%s)))AS UNSIGNED)) AS checksum FROM %s WHERE %s;\",\n\t\tstrings.Join(columnNames, \", \"), strings.Join(columnIsNull, \", \"), TableName(schemaName, tableName), limitRange)\n\tlog.Debug(\"checksum\", zap.String(\"sql\", query), zap.Reflect(\"args\", args))\n\n\tvar checksum sql.NullInt64\n\terr := db.QueryRowContext(ctx, query, args...).Scan(&checksum)\n\tif err != nil {\n\t\treturn -1, errors.Trace(err)\n\t}\n\tif !checksum.Valid {\n\t\t// if don't have any data, the checksum will be `NULL`\n\t\tlog.Warn(\"get empty checksum\", zap.String(\"sql\", query), zap.Reflect(\"args\", args))\n\t\treturn 0, nil\n\t}\n\n\treturn checksum.Int64, nil\n}\n\n// Bucket saves the bucket information from TiDB.\ntype Bucket struct {\n\tLowerBound string\n\tUpperBound string\n\tCount      int64\n}\n\n// GetBucketsInfo SHOW STATS_BUCKETS in TiDB.\nfunc GetBucketsInfo(ctx context.Context, db QueryExecutor, schema, table string, tableInfo *model.TableInfo) (map[string][]Bucket, error) {\n\t/*\n\t\texample in tidb:\n\t\tmysql> SHOW STATS_BUCKETS WHERE db_name= \"test\" AND table_name=\"testa\";\n\t\t+---------+------------+----------------+-------------+----------+-----------+-------+---------+---------------------+---------------------+\n\t\t| Db_name | Table_name | Partition_name | Column_name | Is_index | Bucket_id | Count | Repeats | Lower_Bound         | Upper_Bound         |\n\t\t+---------+------------+----------------+-------------+----------+-----------+-------+---------+---------------------+---------------------+\n\t\t| test    | testa      |                | PRIMARY     |        1 |         0 |    64 |       1 | 1846693550524203008 | 1846838686059069440 |\n\t\t| test    | testa      |                | PRIMARY     |        1 |         1 |   128 |       1 | 1846840885082324992 | 1847056389361369088 |\n\t\t+---------+------------+----------------+-------------+----------+-----------+-------+---------+---------------------+---------------------+\n\t*/\n\tbuckets := make(map[string][]Bucket)\n\tquery := \"SHOW STATS_BUCKETS WHERE db_name= ? AND table_name= ?;\"\n\tlog.Debug(\"GetBucketsInfo\", zap.String(\"sql\", query), zap.String(\"schema\", schema), zap.String(\"table\", table))\n\n\trows, err := db.QueryContext(ctx, query, schema, table)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tdefer rows.Close()\n\n\tcols, err := rows.Columns()\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\tfor rows.Next() {\n\t\tvar dbName, tableName, partitionName, columnName, lowerBound, upperBound sql.NullString\n\t\tvar isIndex, bucketID, count, repeats, ndv sql.NullInt64\n\n\t\t// add partiton_name in new version\n\t\tswitch len(cols) {\n\t\tcase 9:\n\t\t\terr = rows.Scan(&dbName, &tableName, &columnName, &isIndex, &bucketID, &count, &repeats, &lowerBound, &upperBound)\n\t\tcase 10:\n\t\t\terr = rows.Scan(&dbName, &tableName, &partitionName, &columnName, &isIndex, &bucketID, &count, &repeats, &lowerBound, &upperBound)\n\t\tcase 11:\n\t\t\terr = rows.Scan(&dbName, &tableName, &partitionName, &columnName, &isIndex, &bucketID, &count, &repeats, &lowerBound, &upperBound, &ndv)\n\t\tdefault:\n\t\t\treturn nil, errors.New(\"Unknown struct for buckets info\")\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\n\t\tif _, ok := buckets[columnName.String]; !ok {\n\t\t\tbuckets[columnName.String] = make([]Bucket, 0, 100)\n\t\t}\n\t\tbuckets[columnName.String] = append(buckets[columnName.String], Bucket{\n\t\t\tCount:      count.Int64,\n\t\t\tLowerBound: lowerBound.String,\n\t\t\tUpperBound: upperBound.String,\n\t\t})\n\t}\n\n\t// when primary key is int type, the columnName will be column's name, not `PRIMARY`, check and transform here.\n\tindices := FindAllIndex(tableInfo)\n\tfor _, index := range indices {\n\t\tif index.Name.O != \"PRIMARY\" {\n\t\t\tcontinue\n\t\t}\n\t\t_, ok := buckets[index.Name.O]\n\t\tif !ok && len(index.Columns) == 1 {\n\t\t\tif _, ok := buckets[index.Columns[0].Name.O]; !ok {\n\t\t\t\treturn nil, errors.NotFoundf(\"primary key on %s in buckets info\", index.Columns[0].Name.O)\n\t\t\t}\n\t\t\tbuckets[index.Name.O] = buckets[index.Columns[0].Name.O]\n\t\t\tdelete(buckets, index.Columns[0].Name.O)\n\t\t}\n\t}\n\n\treturn buckets, errors.Trace(rows.Err())\n}\n\n// AnalyzeValuesFromBuckets analyze upperBound or lowerBound to string for each column.\n// upperBound and lowerBound are looks like '(123, abc)' for multiple fields, or '123' for one field.\nfunc AnalyzeValuesFromBuckets(valueString string, cols []*model.ColumnInfo) ([]string, error) {\n\t// FIXME: maybe some values contains '(', ')' or ', '\n\tvStr := strings.Trim(valueString, \"()\")\n\tvalues := strings.Split(vStr, \", \")\n\tif len(values) != len(cols) {\n\t\treturn nil, errors.Errorf(\"analyze value %s failed\", valueString)\n\t}\n\n\tfor i, col := range cols {\n\t\tif IsTimeTypeAndNeedDecode(col.GetType()) {\n\t\t\t// check if values[i] is already a time string\n\t\t\tsc := &stmtctx.StatementContext{TimeZone: time.UTC}\n\t\t\t_, err := types.ParseTime(sc, values[i], col.GetType(), types.MinFsp)\n\t\t\tif err == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tvalue, err := DecodeTimeInBucket(values[i])\n\t\t\tif err != nil {\n\t\t\t\tlog.Error(\"analyze values from buckets\", zap.String(\"column\", col.Name.O), zap.String(\"value\", values[i]), zap.Error(err))\n\t\t\t\treturn nil, errors.Trace(err)\n\t\t\t}\n\n\t\t\tvalues[i] = value\n\t\t}\n\t}\n\n\treturn values, nil\n}\n\n// DecodeTimeInBucket decodes Time from a packed uint64 value.\nfunc DecodeTimeInBucket(packedStr string) (string, error) {\n\tpacked, err := strconv.ParseUint(packedStr, 10, 64)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tif packed == 0 {\n\t\treturn \"\", nil\n\t}\n\n\tt := new(types.Time)\n\terr = t.FromPackedUint(packed)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\treturn t.String(), nil\n}\n\n// GetTidbLatestTSO returns tidb's current TSO.\nfunc GetTidbLatestTSO(ctx context.Context, db QueryExecutor) (int64, error) {\n\t/*\n\t\texample in tidb:\n\t\tmysql> SHOW MASTER STATUS;\n\t\t+-------------+--------------------+--------------+------------------+-------------------+\n\t\t| File        | Position           | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |\n\t\t+-------------+--------------------+--------------+------------------+-------------------+\n\t\t| tidb-binlog | 400718757701615617 |              |                  |                   |\n\t\t+-------------+--------------------+--------------+------------------+-------------------+\n\t*/\n\trows, err := db.QueryContext(ctx, \"SHOW MASTER STATUS\")\n\tif err != nil {\n\t\treturn 0, errors.Trace(err)\n\t}\n\tdefer rows.Close()\n\n\tfor rows.Next() {\n\t\tfields, err1 := ScanRow(rows)\n\t\tif err1 != nil {\n\t\t\treturn 0, errors.Trace(err1)\n\t\t}\n\n\t\tts, err1 := strconv.ParseInt(string(fields[\"Position\"].Data), 10, 64)\n\t\tif err1 != nil {\n\t\t\treturn 0, errors.Trace(err1)\n\t\t}\n\t\treturn ts, nil\n\t}\n\treturn 0, errors.New(\"get secondary cluster's ts failed\")\n}\n\n// GetDBVersion returns the database's version\nfunc GetDBVersion(ctx context.Context, db QueryExecutor) (string, error) {\n\t/*\n\t\texample in TiDB:\n\t\tmysql> select version();\n\t\t+--------------------------------------+\n\t\t| version()                            |\n\t\t+--------------------------------------+\n\t\t| 5.7.10-TiDB-v2.1.0-beta-173-g7e48ab1 |\n\t\t+--------------------------------------+\n\n\t\texample in MySQL:\n\t\tmysql> select version();\n\t\t+-----------+\n\t\t| version() |\n\t\t+-----------+\n\t\t| 5.7.21    |\n\t\t+-----------+\n\t*/\n\tquery := \"SELECT version()\"\n\tresult, err := db.QueryContext(ctx, query) //nolint:rowserrcheck\n\tif err != nil {\n\t\treturn \"\", errors.Trace(err)\n\t}\n\tdefer result.Close()\n\n\tvar version sql.NullString\n\tfor result.Next() {\n\t\terr := result.Scan(&version)\n\t\tif err != nil {\n\t\t\treturn \"\", errors.Trace(err)\n\t\t}\n\t\tbreak\n\t}\n\n\tif version.Valid {\n\t\treturn version.String, nil\n\t}\n\n\treturn \"\", ErrVersionNotFound\n}\n\n// GetSessionVariable gets server's session variable, although argument is QueryExecutor, (session) system variables may be\n// set through DSN\nfunc GetSessionVariable(ctx context.Context, db QueryExecutor, variable string) (value string, err error) {\n\tquery := fmt.Sprintf(\"SHOW VARIABLES LIKE '%s'\", variable)\n\trows, err := db.QueryContext(ctx, query)\n\n\tif err != nil {\n\t\treturn \"\", errors.Trace(err)\n\t}\n\tdefer rows.Close()\n\n\t// Show an example.\n\t/*\n\t\tmysql> SHOW VARIABLES LIKE \"binlog_format\";\n\t\t+---------------+-------+\n\t\t| Variable_name | Value |\n\t\t+---------------+-------+\n\t\t| binlog_format | ROW   |\n\t\t+---------------+-------+\n\t*/\n\n\tfor rows.Next() {\n\t\tif err = rows.Scan(&variable, &value); err != nil {\n\t\t\treturn \"\", errors.Trace(err)\n\t\t}\n\t}\n\n\tif err := rows.Err(); err != nil {\n\t\treturn \"\", errors.Trace(err)\n\t}\n\n\treturn value, nil\n}\n\n// GetSQLMode returns sql_mode.\nfunc GetSQLMode(ctx context.Context, db QueryExecutor) (tmysql.SQLMode, error) {\n\tsqlMode, err := GetSessionVariable(ctx, db, \"sql_mode\")\n\tif err != nil {\n\t\treturn tmysql.ModeNone, err\n\t}\n\n\tmode, err := tmysql.GetSQLMode(sqlMode)\n\treturn mode, errors.Trace(err)\n}\n\n// IsTiDB returns true if this database is tidb\nfunc IsTiDB(ctx context.Context, db QueryExecutor) (bool, error) {\n\tversion, err := GetDBVersion(ctx, db)\n\tif err != nil {\n\t\tlog.Error(\"get database's version failed\", zap.Error(err))\n\t\treturn false, errors.Trace(err)\n\t}\n\n\treturn strings.Contains(strings.ToLower(version), \"tidb\"), nil\n}\n\n// TableName returns `schema`.`table`\nfunc TableName(schema, table string) string {\n\treturn fmt.Sprintf(\"`%s`.`%s`\", escapeName(schema), escapeName(table))\n}\n\n// ColumnName returns `column`\nfunc ColumnName(column string) string {\n\treturn fmt.Sprintf(\"`%s`\", escapeName(column))\n}\n\nfunc escapeName(name string) string {\n\treturn strings.Replace(name, \"`\", \"``\", -1)\n}\n\n// ReplacePlaceholder will use args to replace '?', used for log.\n// tips: make sure the num of \"?\" is same with len(args)\nfunc ReplacePlaceholder(str string, args []string) string {\n\t/*\n\t\tfor example:\n\t\tstr is \"a > ? AND a < ?\", args is {'1', '2'},\n\t\tthis function will return \"a > '1' AND a < '2'\"\n\t*/\n\tnewStr := strings.Replace(str, \"?\", \"'%s'\", -1)\n\treturn fmt.Sprintf(newStr, util.StringsToInterfaces(args)...)\n}\n\n// ExecSQLWithRetry executes sql with retry\nfunc ExecSQLWithRetry(ctx context.Context, db DBExecutor, sql string, args ...interface{}) (err error) {\n\tfor i := 0; i < DefaultRetryTime; i++ {\n\t\tstartTime := time.Now()\n\t\t_, err = db.ExecContext(ctx, sql, args...)\n\t\ttakeDuration := time.Since(startTime)\n\t\tif takeDuration > SlowLogThreshold {\n\t\t\tlog.Debug(\"exec sql slow\", zap.String(\"sql\", sql), zap.Reflect(\"args\", args), zap.Duration(\"take\", takeDuration))\n\t\t}\n\t\tif err == nil {\n\t\t\treturn nil\n\t\t}\n\n\t\tif ignoreError(err) {\n\t\t\tlog.Warn(\"ignore execute sql error\", zap.Error(err))\n\t\t\treturn nil\n\t\t}\n\n\t\tif !IsRetryableError(err) {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\n\t\tlog.Warn(\"exe sql failed, will try again\", zap.String(\"sql\", sql), zap.Reflect(\"args\", args), zap.Error(err))\n\n\t\tif i == DefaultRetryTime-1 {\n\t\t\tbreak\n\t\t}\n\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn errors.Trace(ctx.Err())\n\t\tcase <-time.After(10 * time.Millisecond):\n\t\t}\n\t}\n\n\treturn errors.Trace(err)\n}\n\n// ExecuteSQLs executes some sqls in one transaction\nfunc ExecuteSQLs(ctx context.Context, db DBExecutor, sqls []string, args [][]interface{}) error {\n\ttxn, err := db.BeginTx(ctx, nil)\n\tif err != nil {\n\t\tlog.Error(\"exec sqls begin\", zap.Error(err))\n\t\treturn errors.Trace(err)\n\t}\n\n\tfor i := range sqls {\n\t\tstartTime := time.Now()\n\n\t\t_, err = txn.ExecContext(ctx, sqls[i], args[i]...)\n\t\tif err != nil {\n\t\t\tlog.Error(\"exec sql\", zap.String(\"sql\", sqls[i]), zap.Reflect(\"args\", args[i]), zap.Error(err))\n\t\t\trerr := txn.Rollback()\n\t\t\tif rerr != nil {\n\t\t\t\tlog.Error(\"rollback\", zap.Error(err))\n\t\t\t}\n\t\t\treturn errors.Trace(err)\n\t\t}\n\n\t\ttakeDuration := time.Since(startTime)\n\t\tif takeDuration > SlowLogThreshold {\n\t\t\tlog.Debug(\"exec sql slow\", zap.String(\"sql\", sqls[i]), zap.Reflect(\"args\", args[i]), zap.Duration(\"take\", takeDuration))\n\t\t}\n\t}\n\n\terr = txn.Commit()\n\tif err != nil {\n\t\tlog.Error(\"exec sqls commit\", zap.Error(err))\n\t\treturn errors.Trace(err)\n\t}\n\n\treturn nil\n}\n\nfunc ignoreError(err error) bool {\n\t// TODO: now only ignore some ddl error, add some dml error later\n\treturn ignoreDDLError(err)\n}\n\nfunc ignoreDDLError(err error) bool {\n\terr = errors.Cause(err)\n\tmysqlErr, ok := err.(*mysql.MySQLError)\n\tif !ok {\n\t\treturn false\n\t}\n\n\terrCode := errors.ErrCode(mysqlErr.Number)\n\tswitch errCode {\n\tcase infoschema.ErrDatabaseExists.Code(), infoschema.ErrDatabaseDropExists.Code(),\n\t\tinfoschema.ErrTableExists.Code(), infoschema.ErrTableDropExists.Code(),\n\t\tinfoschema.ErrColumnExists.Code(), infoschema.ErrIndexExists.Code():\n\t\treturn true\n\tcase dbterror.ErrDupKeyName.Code():\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// DeleteRows delete rows in several times. Only can delete less than 300,000 one time in TiDB.\nfunc DeleteRows(ctx context.Context, db DBExecutor, schemaName string, tableName string, where string, args []interface{}) error {\n\tdeleteSQL := fmt.Sprintf(\"DELETE FROM %s WHERE %s limit %d;\", TableName(schemaName, tableName), where, DefaultDeleteRowsNum)\n\tresult, err := db.ExecContext(ctx, deleteSQL, args...)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\trows, err := result.RowsAffected()\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tif rows < DefaultDeleteRowsNum {\n\t\treturn nil\n\t}\n\n\treturn DeleteRows(ctx, db, schemaName, tableName, where, args)\n}\n\n// getParser gets parser according to sql mode\nfunc getParser(sqlModeStr string) (*parser.Parser, error) {\n\tif len(sqlModeStr) == 0 {\n\t\treturn parser.New(), nil\n\t}\n\n\tsqlMode, err := tmysql.GetSQLMode(tmysql.FormatSQLModeStr(sqlModeStr))\n\tif err != nil {\n\t\treturn nil, errors.Annotatef(err, \"invalid sql mode %s\", sqlModeStr)\n\t}\n\tparser2 := parser.New()\n\tparser2.SetSQLMode(sqlMode)\n\treturn parser2, nil\n}\n\n// GetParserForDB discovers ANSI_QUOTES in db's session variables and returns a proper parser\nfunc GetParserForDB(ctx context.Context, db QueryExecutor) (*parser.Parser, error) {\n\tmode, err := GetSQLMode(ctx, db)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tparser2 := parser.New()\n\tparser2.SetSQLMode(mode)\n\treturn parser2, nil\n}\n"], "fixing_code": ["// Copyright 2019 PingCAP, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage checkpoints\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"path\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/joho/sqltocsv\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/checkpoints/checkpointspb\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/common\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/config\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/log\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/mydump\"\n\tverify \"github.com/pingcap/tidb/br/pkg/lightning/verification\"\n\t\"github.com/pingcap/tidb/br/pkg/storage\"\n\t\"github.com/pingcap/tidb/br/pkg/version/build\"\n\t\"github.com/pingcap/tidb/util/mathutil\"\n\t\"go.uber.org/zap\"\n\t\"golang.org/x/exp/slices\"\n)\n\ntype CheckpointStatus uint8\n\nconst (\n\tCheckpointStatusMissing         CheckpointStatus = 0\n\tCheckpointStatusMaxInvalid      CheckpointStatus = 25\n\tCheckpointStatusLoaded          CheckpointStatus = 30\n\tCheckpointStatusAllWritten      CheckpointStatus = 60\n\tCheckpointStatusClosed          CheckpointStatus = 90\n\tCheckpointStatusImported        CheckpointStatus = 120\n\tCheckpointStatusIndexImported   CheckpointStatus = 140\n\tCheckpointStatusAlteredAutoInc  CheckpointStatus = 150\n\tCheckpointStatusChecksumSkipped CheckpointStatus = 170\n\tCheckpointStatusChecksummed     CheckpointStatus = 180\n\tCheckpointStatusAnalyzeSkipped  CheckpointStatus = 200\n\tCheckpointStatusAnalyzed        CheckpointStatus = 210\n)\n\nconst WholeTableEngineID = math.MaxInt32\n\nconst (\n\t// the table names to store each kind of checkpoint in the checkpoint database\n\t// remember to increase the version number in case of incompatible change.\n\tCheckpointTableNameTask   = \"task_v2\"\n\tCheckpointTableNameTable  = \"table_v7\"\n\tCheckpointTableNameEngine = \"engine_v5\"\n\tCheckpointTableNameChunk  = \"chunk_v5\"\n\n\t// Some frequently used table name or constants.\n\tallTables       = \"all\"\n\tstringLitAll    = \"'all'\"\n\tcolumnTableName = \"table_name\"\n)\n\nconst (\n\t// shared by MySQLCheckpointsDB and GlueCheckpointsDB\n\tCreateDBTemplate        = \"CREATE DATABASE IF NOT EXISTS %s;\"\n\tCreateTaskTableTemplate = `\n\t\tCREATE TABLE IF NOT EXISTS %s.%s (\n\t\t\tid tinyint(1) PRIMARY KEY,\n\t\t\ttask_id bigint NOT NULL,\n\t\t\tsource_dir varchar(256) NOT NULL,\n\t\t\tbackend varchar(16) NOT NULL,\n\t\t\timporter_addr varchar(256),\n\t\t\ttidb_host varchar(128) NOT NULL,\n\t\t\ttidb_port int NOT NULL,\n\t\t\tpd_addr varchar(128) NOT NULL,\n\t\t\tsorted_kv_dir varchar(256) NOT NULL,\n\t\t\tlightning_ver varchar(48) NOT NULL\n\t\t);`\n\tCreateTableTableTemplate = `\n\t\tCREATE TABLE IF NOT EXISTS %s.%s (\n\t\t\ttask_id bigint NOT NULL,\n\t\t\ttable_name varchar(261) NOT NULL PRIMARY KEY,\n\t\t\thash binary(32) NOT NULL,\n\t\t\tstatus tinyint unsigned DEFAULT 30,\n\t\t\talloc_base bigint NOT NULL DEFAULT 0,\n\t\t\ttable_id bigint NOT NULL DEFAULT 0,\n\t\t\tcreate_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\n\t\t\tupdate_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n\t\t\tkv_bytes bigint unsigned NOT NULL DEFAULT 0,\n\t\t\tkv_kvs bigint unsigned NOT NULL DEFAULT 0,\n\t\t\tkv_checksum bigint unsigned NOT NULL DEFAULT 0,\n\t\t\tINDEX(task_id)\n\t\t);`\n\tCreateEngineTableTemplate = `\n\t\tCREATE TABLE IF NOT EXISTS %s.%s (\n\t\t\ttable_name varchar(261) NOT NULL,\n\t\t\tengine_id int NOT NULL,\n\t\t\tstatus tinyint unsigned DEFAULT 30,\n\t\t\tcreate_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\n\t\t\tupdate_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n\t\t\tPRIMARY KEY(table_name, engine_id DESC)\n\t\t);`\n\tCreateChunkTableTemplate = `\n\t\tCREATE TABLE IF NOT EXISTS %s.%s (\n\t\t\ttable_name varchar(261) NOT NULL,\n\t\t\tengine_id int unsigned NOT NULL,\n\t\t\tpath varchar(2048) NOT NULL,\n\t\t\toffset bigint NOT NULL,\n\t\t\ttype int NOT NULL,\n\t\t\tcompression int NOT NULL,\n\t\t\tsort_key varchar(256) NOT NULL,\n\t\t\tfile_size bigint NOT NULL,\n\t\t\tcolumns text NULL,\n\t\t\tshould_include_row_id BOOL NOT NULL,\n\t\t\tend_offset bigint NOT NULL,\n\t\t\tpos bigint NOT NULL,\n\t\t\tprev_rowid_max bigint NOT NULL,\n\t\t\trowid_max bigint NOT NULL,\n\t\t\tkvc_bytes bigint unsigned NOT NULL DEFAULT 0,\n\t\t\tkvc_kvs bigint unsigned NOT NULL DEFAULT 0,\n\t\t\tkvc_checksum bigint unsigned NOT NULL DEFAULT 0,\n\t\t\tcreate_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\n\t\t\tupdate_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n\t\t\tPRIMARY KEY(table_name, engine_id, path(500), offset)\n\t\t);`\n\tInitTaskTemplate = `\n\t\tREPLACE INTO %s.%s (id, task_id, source_dir, backend, importer_addr, tidb_host, tidb_port, pd_addr, sorted_kv_dir, lightning_ver)\n\t\t\tVALUES (1, ?, ?, ?, ?, ?, ?, ?, ?, ?);`\n\tInitTableTemplate = `\n\t\tINSERT INTO %s.%s (task_id, table_name, hash, table_id) VALUES (?, ?, ?, ?)\n\t\t\tON DUPLICATE KEY UPDATE task_id = CASE\n\t\t\t\tWHEN hash = VALUES(hash)\n\t\t\t\tTHEN VALUES(task_id)\n\t\t\tEND;`\n\tReadTaskTemplate = `\n\t\tSELECT task_id, source_dir, backend, importer_addr, tidb_host, tidb_port, pd_addr, sorted_kv_dir, lightning_ver FROM %s.%s WHERE id = 1;`\n\tReadEngineTemplate = `\n\t\tSELECT engine_id, status FROM %s.%s WHERE table_name = ? ORDER BY engine_id DESC;`\n\tReadChunkTemplate = `\n\t\tSELECT\n\t\t\tengine_id, path, offset, type, compression, sort_key, file_size, columns,\n\t\t\tpos, end_offset, prev_rowid_max, rowid_max,\n\t\t\tkvc_bytes, kvc_kvs, kvc_checksum, unix_timestamp(create_time)\n\t\tFROM %s.%s WHERE table_name = ?\n\t\tORDER BY engine_id, path, offset;`\n\tReadTableRemainTemplate = `\n\t\tSELECT status, alloc_base, table_id, kv_bytes, kv_kvs, kv_checksum FROM %s.%s WHERE table_name = ?;`\n\tReplaceEngineTemplate = `\n\t\tREPLACE INTO %s.%s (table_name, engine_id, status) VALUES (?, ?, ?);`\n\tReplaceChunkTemplate = `\n\t\tREPLACE INTO %s.%s (\n\t\t\t\ttable_name, engine_id,\n\t\t\t\tpath, offset, type, compression, sort_key, file_size, columns, should_include_row_id,\n\t\t\t\tpos, end_offset, prev_rowid_max, rowid_max,\n\t\t\t\tkvc_bytes, kvc_kvs, kvc_checksum, create_time\n\t\t\t) VALUES (\n\t\t\t\t?, ?,\n\t\t\t\t?, ?, ?, ?, ?, ?, ?, FALSE,\n\t\t\t\t?, ?, ?, ?,\n\t\t\t\t0, 0, 0, from_unixtime(?)\n\t\t\t);`\n\tUpdateChunkTemplate = `\n\t\tUPDATE %s.%s SET pos = ?, prev_rowid_max = ?, kvc_bytes = ?, kvc_kvs = ?, kvc_checksum = ?, columns = ?\n\t\tWHERE (table_name, engine_id, path, offset) = (?, ?, ?, ?);`\n\tUpdateTableRebaseTemplate = `\n\t\tUPDATE %s.%s SET alloc_base = GREATEST(?, alloc_base) WHERE table_name = ?;`\n\tUpdateTableStatusTemplate = `\n\t\tUPDATE %s.%s SET status = ? WHERE table_name = ?;`\n\tUpdateTableChecksumTemplate = `UPDATE %s.%s SET kv_bytes = ?, kv_kvs = ?, kv_checksum = ? WHERE table_name = ?;`\n\tUpdateEngineTemplate        = `\n\t\tUPDATE %s.%s SET status = ? WHERE (table_name, engine_id) = (?, ?);`\n\tDeleteCheckpointRecordTemplate = \"DELETE FROM %s.%s WHERE table_name = ?;\"\n)\n\nfunc IsCheckpointTable(name string) bool {\n\tswitch name {\n\tcase CheckpointTableNameTask, CheckpointTableNameTable, CheckpointTableNameEngine, CheckpointTableNameChunk:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\nfunc (status CheckpointStatus) MetricName() string {\n\tswitch status {\n\tcase CheckpointStatusLoaded:\n\t\treturn \"pending\"\n\tcase CheckpointStatusAllWritten:\n\t\treturn \"written\"\n\tcase CheckpointStatusClosed:\n\t\treturn \"closed\"\n\tcase CheckpointStatusImported:\n\t\treturn \"imported\"\n\tcase CheckpointStatusIndexImported:\n\t\treturn \"index_imported\"\n\tcase CheckpointStatusAlteredAutoInc:\n\t\treturn \"altered_auto_inc\"\n\tcase CheckpointStatusChecksummed, CheckpointStatusChecksumSkipped:\n\t\treturn \"checksum\"\n\tcase CheckpointStatusAnalyzed, CheckpointStatusAnalyzeSkipped:\n\t\treturn \"analyzed\"\n\tcase CheckpointStatusMissing:\n\t\treturn \"missing\"\n\tdefault:\n\t\treturn \"invalid\"\n\t}\n}\n\ntype ChunkCheckpointKey struct {\n\tPath   string\n\tOffset int64\n}\n\nfunc (key *ChunkCheckpointKey) String() string {\n\treturn fmt.Sprintf(\"%s:%d\", key.Path, key.Offset)\n}\n\nfunc (key *ChunkCheckpointKey) less(other *ChunkCheckpointKey) bool {\n\tswitch {\n\tcase key.Path < other.Path:\n\t\treturn true\n\tcase key.Path > other.Path:\n\t\treturn false\n\tdefault:\n\t\treturn key.Offset < other.Offset\n\t}\n}\n\ntype ChunkCheckpoint struct {\n\tKey               ChunkCheckpointKey\n\tFileMeta          mydump.SourceFileMeta\n\tColumnPermutation []int\n\tChunk             mydump.Chunk\n\tChecksum          verify.KVChecksum\n\tTimestamp         int64\n}\n\nfunc (ccp *ChunkCheckpoint) DeepCopy() *ChunkCheckpoint {\n\tcolPerm := make([]int, 0, len(ccp.ColumnPermutation))\n\tcolPerm = append(colPerm, ccp.ColumnPermutation...)\n\treturn &ChunkCheckpoint{\n\t\tKey:               ccp.Key,\n\t\tFileMeta:          ccp.FileMeta,\n\t\tColumnPermutation: colPerm,\n\t\tChunk:             ccp.Chunk,\n\t\tChecksum:          ccp.Checksum,\n\t\tTimestamp:         ccp.Timestamp,\n\t}\n}\n\ntype EngineCheckpoint struct {\n\tStatus CheckpointStatus\n\tChunks []*ChunkCheckpoint // a sorted array\n}\n\nfunc (engine *EngineCheckpoint) DeepCopy() *EngineCheckpoint {\n\tchunks := make([]*ChunkCheckpoint, 0, len(engine.Chunks))\n\tfor _, chunk := range engine.Chunks {\n\t\tchunks = append(chunks, chunk.DeepCopy())\n\t}\n\treturn &EngineCheckpoint{\n\t\tStatus: engine.Status,\n\t\tChunks: chunks,\n\t}\n}\n\ntype TableCheckpoint struct {\n\tStatus    CheckpointStatus\n\tAllocBase int64\n\tEngines   map[int32]*EngineCheckpoint\n\tTableID   int64\n\t// remote checksum before restore\n\tChecksum verify.KVChecksum\n}\n\nfunc (cp *TableCheckpoint) DeepCopy() *TableCheckpoint {\n\tengines := make(map[int32]*EngineCheckpoint, len(cp.Engines))\n\tfor engineID, engine := range cp.Engines {\n\t\tengines[engineID] = engine.DeepCopy()\n\t}\n\treturn &TableCheckpoint{\n\t\tStatus:    cp.Status,\n\t\tAllocBase: cp.AllocBase,\n\t\tEngines:   engines,\n\t\tTableID:   cp.TableID,\n\t\tChecksum:  cp.Checksum,\n\t}\n}\n\nfunc (cp *TableCheckpoint) CountChunks() int {\n\tresult := 0\n\tfor _, engine := range cp.Engines {\n\t\tresult += len(engine.Chunks)\n\t}\n\treturn result\n}\n\ntype chunkCheckpointDiff struct {\n\tpos               int64\n\trowID             int64\n\tchecksum          verify.KVChecksum\n\tcolumnPermutation []int\n}\n\ntype engineCheckpointDiff struct {\n\thasStatus bool\n\tstatus    CheckpointStatus\n\tchunks    map[ChunkCheckpointKey]chunkCheckpointDiff\n}\n\ntype TableCheckpointDiff struct {\n\thasStatus   bool\n\thasRebase   bool\n\thasChecksum bool\n\tstatus      CheckpointStatus\n\tallocBase   int64\n\tengines     map[int32]engineCheckpointDiff\n\tchecksum    verify.KVChecksum\n}\n\nfunc NewTableCheckpointDiff() *TableCheckpointDiff {\n\treturn &TableCheckpointDiff{\n\t\tengines: make(map[int32]engineCheckpointDiff),\n\t}\n}\n\nfunc (cpd *TableCheckpointDiff) insertEngineCheckpointDiff(engineID int32, newDiff engineCheckpointDiff) {\n\tif oldDiff, ok := cpd.engines[engineID]; ok {\n\t\tif newDiff.hasStatus {\n\t\t\toldDiff.hasStatus = true\n\t\t\toldDiff.status = newDiff.status\n\t\t}\n\t\tfor key, chunkDiff := range newDiff.chunks {\n\t\t\toldDiff.chunks[key] = chunkDiff\n\t\t}\n\t\tnewDiff = oldDiff\n\t}\n\tcpd.engines[engineID] = newDiff\n}\n\nfunc (cpd *TableCheckpointDiff) String() string {\n\treturn fmt.Sprintf(\n\t\t\"{hasStatus:%v, hasRebase:%v, status:%d, allocBase:%d, engines:[%d]}\",\n\t\tcpd.hasStatus, cpd.hasRebase, cpd.status, cpd.allocBase, len(cpd.engines),\n\t)\n}\n\n// Apply the diff to the existing chunk and engine checkpoints in `cp`.\nfunc (cp *TableCheckpoint) Apply(cpd *TableCheckpointDiff) {\n\tif cpd.hasStatus {\n\t\tcp.Status = cpd.status\n\t}\n\tif cpd.hasRebase {\n\t\tcp.AllocBase = cpd.allocBase\n\t}\n\tfor engineID, engineDiff := range cpd.engines {\n\t\tengine := cp.Engines[engineID]\n\t\tif engine == nil {\n\t\t\tcontinue\n\t\t}\n\t\tif engineDiff.hasStatus {\n\t\t\tengine.Status = engineDiff.status\n\t\t}\n\t\tfor key, diff := range engineDiff.chunks {\n\t\t\tcheckpointKey := key\n\t\t\tindex := sort.Search(len(engine.Chunks), func(i int) bool {\n\t\t\t\treturn !engine.Chunks[i].Key.less(&checkpointKey)\n\t\t\t})\n\t\t\tif index >= len(engine.Chunks) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tchunk := engine.Chunks[index]\n\t\t\tif chunk.Key != checkpointKey {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tchunk.Chunk.Offset = diff.pos\n\t\t\tchunk.Chunk.PrevRowIDMax = diff.rowID\n\t\t\tchunk.Checksum = diff.checksum\n\t\t}\n\t}\n}\n\ntype TableCheckpointMerger interface {\n\t// MergeInto the table checkpoint diff from a status update or chunk update.\n\t// If there are multiple updates to the same table, only the last one will\n\t// take effect. Therefore, the caller must ensure events for the same table\n\t// are properly ordered by the global time (an old event must be merged\n\t// before the new one).\n\tMergeInto(cpd *TableCheckpointDiff)\n}\n\ntype StatusCheckpointMerger struct {\n\tEngineID int32 // WholeTableEngineID == apply to whole table.\n\tStatus   CheckpointStatus\n}\n\nfunc (merger *StatusCheckpointMerger) SetInvalid() {\n\tmerger.Status /= 10\n}\n\nfunc (merger *StatusCheckpointMerger) MergeInto(cpd *TableCheckpointDiff) {\n\tif merger.EngineID == WholeTableEngineID || merger.Status <= CheckpointStatusMaxInvalid {\n\t\tcpd.status = merger.Status\n\t\tcpd.hasStatus = true\n\t}\n\tif merger.EngineID != WholeTableEngineID {\n\t\tcpd.insertEngineCheckpointDiff(merger.EngineID, engineCheckpointDiff{\n\t\t\thasStatus: true,\n\t\t\tstatus:    merger.Status,\n\t\t\tchunks:    make(map[ChunkCheckpointKey]chunkCheckpointDiff),\n\t\t})\n\t}\n}\n\ntype ChunkCheckpointMerger struct {\n\tEngineID          int32\n\tKey               ChunkCheckpointKey\n\tChecksum          verify.KVChecksum\n\tPos               int64\n\tRowID             int64\n\tColumnPermutation []int\n\tEndOffset         int64 // For test only.\n}\n\nfunc (merger *ChunkCheckpointMerger) MergeInto(cpd *TableCheckpointDiff) {\n\tcpd.insertEngineCheckpointDiff(merger.EngineID, engineCheckpointDiff{\n\t\tchunks: map[ChunkCheckpointKey]chunkCheckpointDiff{\n\t\t\tmerger.Key: {\n\t\t\t\tpos:               merger.Pos,\n\t\t\t\trowID:             merger.RowID,\n\t\t\t\tchecksum:          merger.Checksum,\n\t\t\t\tcolumnPermutation: merger.ColumnPermutation,\n\t\t\t},\n\t\t},\n\t})\n}\n\ntype TableChecksumMerger struct {\n\tChecksum verify.KVChecksum\n}\n\nfunc (m *TableChecksumMerger) MergeInto(cpd *TableCheckpointDiff) {\n\tcpd.hasChecksum = true\n\tcpd.checksum = m.Checksum\n}\n\ntype RebaseCheckpointMerger struct {\n\tAllocBase int64\n}\n\nfunc (merger *RebaseCheckpointMerger) MergeInto(cpd *TableCheckpointDiff) {\n\tcpd.hasRebase = true\n\tcpd.allocBase = mathutil.Max(cpd.allocBase, merger.AllocBase)\n}\n\ntype DestroyedTableCheckpoint struct {\n\tTableName   string\n\tMinEngineID int32\n\tMaxEngineID int32\n}\n\ntype TaskCheckpoint struct {\n\tTaskID       int64\n\tSourceDir    string\n\tBackend      string\n\tImporterAddr string\n\tTiDBHost     string\n\tTiDBPort     int\n\tPdAddr       string\n\tSortedKVDir  string\n\tLightningVer string\n}\n\ntype DB interface {\n\tInitialize(ctx context.Context, cfg *config.Config, dbInfo map[string]*TidbDBInfo) error\n\tTaskCheckpoint(ctx context.Context) (*TaskCheckpoint, error)\n\tGet(ctx context.Context, tableName string) (*TableCheckpoint, error)\n\tClose() error\n\t// InsertEngineCheckpoints initializes the checkpoints related to a table.\n\t// It assumes the entire table has not been imported before and will fill in\n\t// default values for the column permutations and checksums.\n\tInsertEngineCheckpoints(ctx context.Context, tableName string, checkpoints map[int32]*EngineCheckpoint) error\n\tUpdate(taskCtx context.Context, checkpointDiffs map[string]*TableCheckpointDiff) error\n\n\tRemoveCheckpoint(ctx context.Context, tableName string) error\n\t// MoveCheckpoints renames the checkpoint schema to include a suffix\n\t// including the taskID (e.g. `tidb_lightning_checkpoints.1234567890.bak`).\n\tMoveCheckpoints(ctx context.Context, taskID int64) error\n\t// GetLocalStoringTables returns a map containing tables have engine files stored on local disk.\n\t// currently only meaningful for local backend\n\tGetLocalStoringTables(ctx context.Context) (map[string][]int32, error)\n\tIgnoreErrorCheckpoint(ctx context.Context, tableName string) error\n\tDestroyErrorCheckpoint(ctx context.Context, tableName string) ([]DestroyedTableCheckpoint, error)\n\tDumpTables(ctx context.Context, csv io.Writer) error\n\tDumpEngines(ctx context.Context, csv io.Writer) error\n\tDumpChunks(ctx context.Context, csv io.Writer) error\n}\n\nfunc OpenCheckpointsDB(ctx context.Context, cfg *config.Config) (DB, error) {\n\tif !cfg.Checkpoint.Enable {\n\t\treturn NewNullCheckpointsDB(), nil\n\t}\n\n\tswitch cfg.Checkpoint.Driver {\n\tcase config.CheckpointDriverMySQL:\n\t\tvar (\n\t\t\tdb  *sql.DB\n\t\t\terr error\n\t\t)\n\t\tif cfg.Checkpoint.MySQLParam != nil {\n\t\t\tdb, err = cfg.Checkpoint.MySQLParam.Connect()\n\t\t} else {\n\t\t\tdb, err = sql.Open(\"mysql\", cfg.Checkpoint.DSN)\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\t\tcpdb, err := NewMySQLCheckpointsDB(ctx, db, cfg.Checkpoint.Schema)\n\t\tif err != nil {\n\t\t\t_ = db.Close()\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\t\treturn cpdb, nil\n\n\tcase config.CheckpointDriverFile:\n\t\tcpdb, err := NewFileCheckpointsDB(ctx, cfg.Checkpoint.DSN)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\t\treturn cpdb, nil\n\n\tdefault:\n\t\treturn nil, common.ErrUnknownCheckpointDriver.GenWithStackByArgs(cfg.Checkpoint.Driver)\n\t}\n}\n\nfunc IsCheckpointsDBExists(ctx context.Context, cfg *config.Config) (bool, error) {\n\tif !cfg.Checkpoint.Enable {\n\t\treturn false, nil\n\t}\n\tswitch cfg.Checkpoint.Driver {\n\tcase config.CheckpointDriverMySQL:\n\t\tvar (\n\t\t\tdb  *sql.DB\n\t\t\terr error\n\t\t)\n\t\tif cfg.Checkpoint.MySQLParam != nil {\n\t\t\tdb, err = cfg.Checkpoint.MySQLParam.Connect()\n\t\t} else {\n\t\t\tdb, err = sql.Open(\"mysql\", cfg.Checkpoint.DSN)\n\t\t}\n\t\tif err != nil {\n\t\t\treturn false, errors.Trace(err)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer db.Close()\n\t\tcheckSQL := \"SHOW DATABASES WHERE `DATABASE` = ?\"\n\t\trows, err := db.QueryContext(ctx, checkSQL, cfg.Checkpoint.Schema)\n\t\tif err != nil {\n\t\t\treturn false, errors.Trace(err)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer rows.Close()\n\t\tresult := rows.Next()\n\t\tif err := rows.Err(); err != nil {\n\t\t\treturn false, errors.Trace(err)\n\t\t}\n\t\treturn result, nil\n\n\tcase config.CheckpointDriverFile:\n\t\ts, fileName, err := createExstorageByCompletePath(ctx, cfg.Checkpoint.DSN)\n\t\tif err != nil {\n\t\t\treturn false, errors.Trace(err)\n\t\t}\n\t\tresult, err := s.FileExists(ctx, fileName)\n\t\tif err != nil {\n\t\t\treturn false, errors.Trace(err)\n\t\t}\n\t\treturn result, nil\n\n\tdefault:\n\t\treturn false, common.ErrUnknownCheckpointDriver.GenWithStackByArgs(cfg.Checkpoint.Driver)\n\t}\n}\n\n// NullCheckpointsDB is a checkpoints database with no checkpoints.\ntype NullCheckpointsDB struct{}\n\nfunc NewNullCheckpointsDB() *NullCheckpointsDB {\n\treturn &NullCheckpointsDB{}\n}\n\nfunc (*NullCheckpointsDB) Initialize(context.Context, *config.Config, map[string]*TidbDBInfo) error {\n\treturn nil\n}\n\nfunc (*NullCheckpointsDB) TaskCheckpoint(ctx context.Context) (*TaskCheckpoint, error) {\n\treturn nil, nil\n}\n\nfunc (*NullCheckpointsDB) Close() error {\n\treturn nil\n}\n\nfunc (*NullCheckpointsDB) Get(_ context.Context, _ string) (*TableCheckpoint, error) {\n\treturn &TableCheckpoint{\n\t\tStatus:  CheckpointStatusLoaded,\n\t\tEngines: map[int32]*EngineCheckpoint{},\n\t}, nil\n}\n\nfunc (*NullCheckpointsDB) InsertEngineCheckpoints(_ context.Context, _ string, _ map[int32]*EngineCheckpoint) error {\n\treturn nil\n}\n\nfunc (*NullCheckpointsDB) Update(context.Context, map[string]*TableCheckpointDiff) error {\n\treturn nil\n}\n\ntype MySQLCheckpointsDB struct {\n\tdb     *sql.DB\n\tschema string\n}\n\nfunc NewMySQLCheckpointsDB(ctx context.Context, db *sql.DB, schemaName string) (*MySQLCheckpointsDB, error) {\n\tschema := common.EscapeIdentifier(schemaName)\n\tsql := common.SQLWithRetry{\n\t\tDB:           db,\n\t\tLogger:       log.FromContext(ctx).With(zap.String(\"schema\", schemaName)),\n\t\tHideQueryLog: true,\n\t}\n\terr := sql.Exec(ctx, \"create checkpoints database\", fmt.Sprintf(CreateDBTemplate, schema))\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\terr = sql.Exec(ctx, \"create task checkpoints table\", fmt.Sprintf(CreateTaskTableTemplate, schema, CheckpointTableNameTask))\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\terr = sql.Exec(ctx, \"create table checkpoints table\", fmt.Sprintf(CreateTableTableTemplate, schema, CheckpointTableNameTable))\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\terr = sql.Exec(ctx, \"create engine checkpoints table\", fmt.Sprintf(CreateEngineTableTemplate, schema, CheckpointTableNameEngine))\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\terr = sql.Exec(ctx, \"create chunks checkpoints table\", fmt.Sprintf(CreateChunkTableTemplate, schema, CheckpointTableNameChunk))\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\treturn &MySQLCheckpointsDB{\n\t\tdb:     db,\n\t\tschema: schema,\n\t}, nil\n}\n\nfunc (cpdb *MySQLCheckpointsDB) Initialize(ctx context.Context, cfg *config.Config, dbInfo map[string]*TidbDBInfo) error {\n\t// We can have at most 65535 placeholders https://stackoverflow.com/q/4922345/\n\t// Since this step is not performance critical, we just insert the rows one-by-one.\n\ts := common.SQLWithRetry{DB: cpdb.db, Logger: log.FromContext(ctx)}\n\terr := s.Transact(ctx, \"insert checkpoints\", func(c context.Context, tx *sql.Tx) error {\n\t\ttaskStmt, err := tx.PrepareContext(c, fmt.Sprintf(InitTaskTemplate, cpdb.schema, CheckpointTableNameTask))\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer taskStmt.Close()\n\t\t_, err = taskStmt.ExecContext(ctx, cfg.TaskID, cfg.Mydumper.SourceDir, cfg.TikvImporter.Backend,\n\t\t\tcfg.TikvImporter.Addr, cfg.TiDB.Host, cfg.TiDB.Port, cfg.TiDB.PdAddr, cfg.TikvImporter.SortedKVDir,\n\t\t\tbuild.ReleaseVersion)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\n\t\t// If `hash` is not the same but the `table_name` duplicates,\n\t\t// the CASE expression will return NULL, which can be used to violate\n\t\t// the NOT NULL requirement of `task_id` column, and caused this INSERT\n\t\t// statement to fail with an irrecoverable error.\n\t\t// We do need to capture the error is display a user friendly message\n\t\t// (multiple nodes cannot import the same table) though.\n\t\tstmt, err := tx.PrepareContext(c, fmt.Sprintf(InitTableTemplate, cpdb.schema, CheckpointTableNameTable))\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer stmt.Close()\n\n\t\tfor _, db := range dbInfo {\n\t\t\tfor _, table := range db.Tables {\n\t\t\t\ttableName := common.UniqueTable(db.Name, table.Name)\n\t\t\t\t_, err = stmt.ExecContext(c, cfg.TaskID, tableName, CheckpointStatusLoaded, table.ID)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Trace(err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\treturn nil\n}\n\nfunc (cpdb *MySQLCheckpointsDB) TaskCheckpoint(ctx context.Context) (*TaskCheckpoint, error) {\n\ts := common.SQLWithRetry{\n\t\tDB:     cpdb.db,\n\t\tLogger: log.FromContext(ctx),\n\t}\n\n\ttaskQuery := fmt.Sprintf(ReadTaskTemplate, cpdb.schema, CheckpointTableNameTask)\n\ttaskCp := &TaskCheckpoint{}\n\terr := s.QueryRow(ctx, \"fetch task checkpoint\", taskQuery, &taskCp.TaskID, &taskCp.SourceDir, &taskCp.Backend,\n\t\t&taskCp.ImporterAddr, &taskCp.TiDBHost, &taskCp.TiDBPort, &taskCp.PdAddr, &taskCp.SortedKVDir, &taskCp.LightningVer)\n\tif err != nil {\n\t\t// if task checkpoint is empty, return nil\n\t\tif errors.Cause(err) == sql.ErrNoRows {\n\t\t\treturn nil, nil\n\t\t}\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\treturn taskCp, nil\n}\n\nfunc (cpdb *MySQLCheckpointsDB) Close() error {\n\treturn errors.Trace(cpdb.db.Close())\n}\n\nfunc (cpdb *MySQLCheckpointsDB) Get(ctx context.Context, tableName string) (*TableCheckpoint, error) {\n\tcp := &TableCheckpoint{\n\t\tEngines: map[int32]*EngineCheckpoint{},\n\t}\n\n\ts := common.SQLWithRetry{\n\t\tDB:     cpdb.db,\n\t\tLogger: log.FromContext(ctx).With(zap.String(\"table\", tableName)),\n\t}\n\terr := s.Transact(ctx, \"read checkpoint\", func(c context.Context, tx *sql.Tx) error {\n\t\t// 1. Populate the engines.\n\n\t\tengineQuery := fmt.Sprintf(ReadEngineTemplate, cpdb.schema, CheckpointTableNameEngine)\n\t\tengineRows, err := tx.QueryContext(c, engineQuery, tableName)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer engineRows.Close()\n\t\tfor engineRows.Next() {\n\t\t\tvar (\n\t\t\t\tengineID int32\n\t\t\t\tstatus   uint8\n\t\t\t)\n\t\t\tif err := engineRows.Scan(&engineID, &status); err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\t\t\tcp.Engines[engineID] = &EngineCheckpoint{\n\t\t\t\tStatus: CheckpointStatus(status),\n\t\t\t}\n\t\t}\n\t\tif err := engineRows.Err(); err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\n\t\t// 2. Populate the chunks.\n\n\t\tchunkQuery := fmt.Sprintf(ReadChunkTemplate, cpdb.schema, CheckpointTableNameChunk)\n\t\tchunkRows, err := tx.QueryContext(c, chunkQuery, tableName)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer chunkRows.Close()\n\t\tfor chunkRows.Next() {\n\t\t\tvar (\n\t\t\t\tvalue       = &ChunkCheckpoint{}\n\t\t\t\tcolPerm     []byte\n\t\t\t\tengineID    int32\n\t\t\t\tkvcBytes    uint64\n\t\t\t\tkvcKVs      uint64\n\t\t\t\tkvcChecksum uint64\n\t\t\t)\n\t\t\tif err := chunkRows.Scan(\n\t\t\t\t&engineID, &value.Key.Path, &value.Key.Offset, &value.FileMeta.Type, &value.FileMeta.Compression,\n\t\t\t\t&value.FileMeta.SortKey, &value.FileMeta.FileSize, &colPerm, &value.Chunk.Offset, &value.Chunk.EndOffset,\n\t\t\t\t&value.Chunk.PrevRowIDMax, &value.Chunk.RowIDMax, &kvcBytes, &kvcKVs, &kvcChecksum,\n\t\t\t\t&value.Timestamp,\n\t\t\t); err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\t\t\tvalue.FileMeta.Path = value.Key.Path\n\t\t\tvalue.Checksum = verify.MakeKVChecksum(kvcBytes, kvcKVs, kvcChecksum)\n\t\t\tif err := json.Unmarshal(colPerm, &value.ColumnPermutation); err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\t\t\tcp.Engines[engineID].Chunks = append(cp.Engines[engineID].Chunks, value)\n\t\t}\n\t\tif err := chunkRows.Err(); err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\n\t\t// 3. Fill in the remaining table info\n\n\t\ttableQuery := fmt.Sprintf(ReadTableRemainTemplate, cpdb.schema, CheckpointTableNameTable)\n\t\ttableRow := tx.QueryRowContext(c, tableQuery, tableName)\n\n\t\tvar status uint8\n\t\tvar kvs, bytes, checksum uint64\n\t\tif err := tableRow.Scan(&status, &cp.AllocBase, &cp.TableID, &bytes, &kvs, &checksum); err != nil {\n\t\t\tif err == sql.ErrNoRows {\n\t\t\t\treturn errors.NotFoundf(\"checkpoint for table %s\", tableName)\n\t\t\t}\n\t\t}\n\t\tcp.Checksum = verify.MakeKVChecksum(bytes, kvs, checksum)\n\t\tcp.Status = CheckpointStatus(status)\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\treturn cp, nil\n}\n\nfunc (cpdb *MySQLCheckpointsDB) InsertEngineCheckpoints(ctx context.Context, tableName string, checkpoints map[int32]*EngineCheckpoint) error {\n\ts := common.SQLWithRetry{\n\t\tDB:     cpdb.db,\n\t\tLogger: log.FromContext(ctx).With(zap.String(\"table\", tableName)),\n\t}\n\terr := s.Transact(ctx, \"update engine checkpoints\", func(c context.Context, tx *sql.Tx) error {\n\t\tengineStmt, err := tx.PrepareContext(c, fmt.Sprintf(ReplaceEngineTemplate, cpdb.schema, CheckpointTableNameEngine))\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer engineStmt.Close()\n\n\t\tchunkStmt, err := tx.PrepareContext(c, fmt.Sprintf(ReplaceChunkTemplate, cpdb.schema, CheckpointTableNameChunk))\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer chunkStmt.Close()\n\n\t\tfor engineID, engine := range checkpoints {\n\t\t\t_, err = engineStmt.ExecContext(c, tableName, engineID, engine.Status)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\t\t\tfor _, value := range engine.Chunks {\n\t\t\t\tcolumnPerm, err := json.Marshal(value.ColumnPermutation)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Trace(err)\n\t\t\t\t}\n\t\t\t\t_, err = chunkStmt.ExecContext(\n\t\t\t\t\tc, tableName, engineID,\n\t\t\t\t\tvalue.Key.Path, value.Key.Offset, value.FileMeta.Type, value.FileMeta.Compression,\n\t\t\t\t\tvalue.FileMeta.SortKey, value.FileMeta.FileSize, columnPerm, value.Chunk.Offset, value.Chunk.EndOffset,\n\t\t\t\t\tvalue.Chunk.PrevRowIDMax, value.Chunk.RowIDMax, value.Timestamp,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Trace(err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\treturn nil\n}\n\nfunc (cpdb *MySQLCheckpointsDB) Update(taskCtx context.Context, checkpointDiffs map[string]*TableCheckpointDiff) error {\n\tchunkQuery := fmt.Sprintf(UpdateChunkTemplate, cpdb.schema, CheckpointTableNameChunk)\n\trebaseQuery := fmt.Sprintf(UpdateTableRebaseTemplate, cpdb.schema, CheckpointTableNameTable)\n\ttableStatusQuery := fmt.Sprintf(UpdateTableStatusTemplate, cpdb.schema, CheckpointTableNameTable)\n\ttableChecksumQuery := fmt.Sprintf(UpdateTableChecksumTemplate, cpdb.schema, CheckpointTableNameTable)\n\tengineStatusQuery := fmt.Sprintf(UpdateEngineTemplate, cpdb.schema, CheckpointTableNameEngine)\n\n\ts := common.SQLWithRetry{DB: cpdb.db, Logger: log.FromContext(taskCtx)}\n\treturn s.Transact(taskCtx, \"update checkpoints\", func(c context.Context, tx *sql.Tx) error {\n\t\tchunkStmt, e := tx.PrepareContext(c, chunkQuery)\n\t\tif e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer chunkStmt.Close()\n\t\trebaseStmt, e := tx.PrepareContext(c, rebaseQuery)\n\t\tif e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer rebaseStmt.Close()\n\t\ttableStatusStmt, e := tx.PrepareContext(c, tableStatusQuery)\n\t\tif e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer tableStatusStmt.Close()\n\t\ttableChecksumStmt, e := tx.PrepareContext(c, tableChecksumQuery)\n\t\tif e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer tableChecksumStmt.Close()\n\t\tengineStatusStmt, e := tx.PrepareContext(c, engineStatusQuery)\n\t\tif e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer engineStatusStmt.Close()\n\t\tfor tableName, cpd := range checkpointDiffs {\n\t\t\tif cpd.hasStatus {\n\t\t\t\tif _, e := tableStatusStmt.ExecContext(c, cpd.status, tableName); e != nil {\n\t\t\t\t\treturn errors.Trace(e)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif cpd.hasRebase {\n\t\t\t\tif _, e := rebaseStmt.ExecContext(c, cpd.allocBase, tableName); e != nil {\n\t\t\t\t\treturn errors.Trace(e)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif cpd.hasChecksum {\n\t\t\t\tif _, e := tableChecksumStmt.ExecContext(c, cpd.checksum.SumSize(), cpd.checksum.SumKVS(), cpd.checksum.Sum(), tableName); e != nil {\n\t\t\t\t\treturn errors.Trace(e)\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor engineID, engineDiff := range cpd.engines {\n\t\t\t\tif engineDiff.hasStatus {\n\t\t\t\t\tif _, e := engineStatusStmt.ExecContext(c, engineDiff.status, tableName, engineID); e != nil {\n\t\t\t\t\t\treturn errors.Trace(e)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfor key, diff := range engineDiff.chunks {\n\t\t\t\t\tcolumnPerm, err := json.Marshal(diff.columnPermutation)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn errors.Trace(err)\n\t\t\t\t\t}\n\t\t\t\t\tif _, e := chunkStmt.ExecContext(\n\t\t\t\t\t\tc,\n\t\t\t\t\t\tdiff.pos, diff.rowID, diff.checksum.SumSize(), diff.checksum.SumKVS(), diff.checksum.Sum(),\n\t\t\t\t\t\tcolumnPerm, tableName, engineID, key.Path, key.Offset,\n\t\t\t\t\t); e != nil {\n\t\t\t\t\t\treturn errors.Trace(e)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n}\n\ntype FileCheckpointsDB struct {\n\tlock        sync.Mutex // we need to ensure only a thread can access to `checkpoints` at a time\n\tcheckpoints checkpointspb.CheckpointsModel\n\tctx         context.Context\n\tpath        string\n\tfileName    string\n\texStorage   storage.ExternalStorage\n}\n\nfunc newFileCheckpointsDB(\n\tctx context.Context,\n\tpath string,\n\texStorage storage.ExternalStorage,\n\tfileName string,\n) (*FileCheckpointsDB, error) {\n\tcpdb := &FileCheckpointsDB{\n\t\tcheckpoints: checkpointspb.CheckpointsModel{\n\t\t\tTaskCheckpoint: &checkpointspb.TaskCheckpointModel{},\n\t\t\tCheckpoints:    map[string]*checkpointspb.TableCheckpointModel{},\n\t\t},\n\t\tctx:       ctx,\n\t\tpath:      path,\n\t\tfileName:  fileName,\n\t\texStorage: exStorage,\n\t}\n\n\tif cpdb.fileName == \"\" {\n\t\treturn nil, errors.Errorf(\"the checkpoint DSN '%s' must not be a directory\", path)\n\t}\n\n\texist, err := cpdb.exStorage.FileExists(ctx, cpdb.fileName)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tif !exist {\n\t\tlog.FromContext(ctx).Info(\"open checkpoint file failed, going to create a new one\",\n\t\t\tzap.String(\"path\", path),\n\t\t\tlog.ShortError(err),\n\t\t)\n\t\treturn cpdb, nil\n\t}\n\tcontent, err := cpdb.exStorage.ReadFile(ctx, cpdb.fileName)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\terr = cpdb.checkpoints.Unmarshal(content)\n\tif err != nil {\n\t\tlog.FromContext(ctx).Error(\"checkpoint file is broken\", zap.String(\"path\", path), zap.Error(err))\n\t}\n\t// FIXME: patch for empty map may need initialize manually, because currently\n\t// FIXME: a map of zero size -> marshall -> unmarshall -> become nil, see checkpoint_test.go\n\tif cpdb.checkpoints.Checkpoints == nil {\n\t\tcpdb.checkpoints.Checkpoints = map[string]*checkpointspb.TableCheckpointModel{}\n\t}\n\tfor _, table := range cpdb.checkpoints.Checkpoints {\n\t\tif table.Engines == nil {\n\t\t\ttable.Engines = map[int32]*checkpointspb.EngineCheckpointModel{}\n\t\t}\n\t\tfor _, engine := range table.Engines {\n\t\t\tif engine.Chunks == nil {\n\t\t\t\tengine.Chunks = map[string]*checkpointspb.ChunkCheckpointModel{}\n\t\t\t}\n\t\t}\n\t}\n\treturn cpdb, nil\n}\n\nfunc NewFileCheckpointsDB(ctx context.Context, path string) (*FileCheckpointsDB, error) {\n\t// init ExternalStorage\n\ts, fileName, err := createExstorageByCompletePath(ctx, path)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\treturn newFileCheckpointsDB(ctx, path, s, fileName)\n}\n\nfunc NewFileCheckpointsDBWithExstorageFileName(\n\tctx context.Context,\n\tpath string,\n\ts storage.ExternalStorage,\n\tfileName string,\n) (*FileCheckpointsDB, error) {\n\treturn newFileCheckpointsDB(ctx, path, s, fileName)\n}\n\n// createExstorageByCompletePath create ExternalStorage by completePath and return fileName.\nfunc createExstorageByCompletePath(ctx context.Context, completePath string) (storage.ExternalStorage, string, error) {\n\tif completePath == \"\" {\n\t\treturn nil, \"\", nil\n\t}\n\tfileName, newPath, err := separateCompletePath(completePath)\n\tif err != nil {\n\t\treturn nil, \"\", errors.Trace(err)\n\t}\n\tu, err := storage.ParseBackend(newPath, nil)\n\tif err != nil {\n\t\treturn nil, \"\", errors.Trace(err)\n\t}\n\ts, err := storage.New(ctx, u, &storage.ExternalStorageOptions{})\n\tif err != nil {\n\t\treturn nil, \"\", errors.Trace(err)\n\t}\n\treturn s, fileName, nil\n}\n\n// separateCompletePath separates fileName from completePath, returns fileName and newPath.\nfunc separateCompletePath(completePath string) (string, string, error) {\n\tif completePath == \"\" {\n\t\treturn \"\", \"\", nil\n\t}\n\tvar fileName, newPath string\n\tpurl, err := storage.ParseRawURL(completePath)\n\tif err != nil {\n\t\treturn \"\", \"\", errors.Trace(err)\n\t}\n\t// not url format, we don't use url library to avoid being escaped or unescaped\n\tif purl.Scheme == \"\" {\n\t\t// no fileName, just path\n\t\tif strings.HasSuffix(completePath, \"/\") {\n\t\t\treturn \"\", completePath, nil\n\t\t}\n\t\tfileName = path.Base(completePath)\n\t\tnewPath = path.Dir(completePath)\n\t} else {\n\t\tif strings.HasSuffix(purl.Path, \"/\") {\n\t\t\treturn \"\", completePath, nil\n\t\t}\n\t\tfileName = path.Base(purl.Path)\n\t\tpurl.Path = path.Dir(purl.Path)\n\t\tnewPath = purl.String()\n\t}\n\treturn fileName, newPath, nil\n}\n\nfunc (cpdb *FileCheckpointsDB) save() error {\n\tserialized, err := cpdb.checkpoints.Marshal()\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\treturn cpdb.exStorage.WriteFile(cpdb.ctx, cpdb.fileName, serialized)\n}\n\nfunc (cpdb *FileCheckpointsDB) Initialize(ctx context.Context, cfg *config.Config, dbInfo map[string]*TidbDBInfo) error {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\tcpdb.checkpoints.TaskCheckpoint = &checkpointspb.TaskCheckpointModel{\n\t\tTaskId:       cfg.TaskID,\n\t\tSourceDir:    cfg.Mydumper.SourceDir,\n\t\tBackend:      cfg.TikvImporter.Backend,\n\t\tImporterAddr: cfg.TikvImporter.Addr,\n\t\tTidbHost:     cfg.TiDB.Host,\n\t\tTidbPort:     int32(cfg.TiDB.Port),\n\t\tPdAddr:       cfg.TiDB.PdAddr,\n\t\tSortedKvDir:  cfg.TikvImporter.SortedKVDir,\n\t\tLightningVer: build.ReleaseVersion,\n\t}\n\n\tif cpdb.checkpoints.Checkpoints == nil {\n\t\tcpdb.checkpoints.Checkpoints = make(map[string]*checkpointspb.TableCheckpointModel)\n\t}\n\n\tfor _, db := range dbInfo {\n\t\tfor _, table := range db.Tables {\n\t\t\ttableName := common.UniqueTable(db.Name, table.Name)\n\t\t\tif _, ok := cpdb.checkpoints.Checkpoints[tableName]; !ok {\n\t\t\t\tcpdb.checkpoints.Checkpoints[tableName] = &checkpointspb.TableCheckpointModel{\n\t\t\t\t\tStatus:  uint32(CheckpointStatusLoaded),\n\t\t\t\t\tEngines: map[int32]*checkpointspb.EngineCheckpointModel{},\n\t\t\t\t\tTableID: table.ID,\n\t\t\t\t}\n\t\t\t}\n\t\t\t// TODO check if hash matches\n\t\t}\n\t}\n\n\treturn errors.Trace(cpdb.save())\n}\n\nfunc (cpdb *FileCheckpointsDB) TaskCheckpoint(_ context.Context) (*TaskCheckpoint, error) {\n\t// this method is always called in lock\n\tcp := cpdb.checkpoints.TaskCheckpoint\n\tif cp == nil || cp.TaskId == 0 {\n\t\treturn nil, nil\n\t}\n\n\treturn &TaskCheckpoint{\n\t\tTaskID:       cp.TaskId,\n\t\tSourceDir:    cp.SourceDir,\n\t\tBackend:      cp.Backend,\n\t\tImporterAddr: cp.ImporterAddr,\n\t\tTiDBHost:     cp.TidbHost,\n\t\tTiDBPort:     int(cp.TidbPort),\n\t\tPdAddr:       cp.PdAddr,\n\t\tSortedKVDir:  cp.SortedKvDir,\n\t\tLightningVer: cp.LightningVer,\n\t}, nil\n}\n\nfunc (cpdb *FileCheckpointsDB) Close() error {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\treturn errors.Trace(cpdb.save())\n}\n\nfunc (cpdb *FileCheckpointsDB) Get(_ context.Context, tableName string) (*TableCheckpoint, error) {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\ttableModel, ok := cpdb.checkpoints.Checkpoints[tableName]\n\tif !ok {\n\t\treturn nil, errors.NotFoundf(\"checkpoint for table %s\", tableName)\n\t}\n\n\tcp := &TableCheckpoint{\n\t\tStatus:    CheckpointStatus(tableModel.Status),\n\t\tAllocBase: tableModel.AllocBase,\n\t\tEngines:   make(map[int32]*EngineCheckpoint, len(tableModel.Engines)),\n\t\tTableID:   tableModel.TableID,\n\t\tChecksum:  verify.MakeKVChecksum(tableModel.KvBytes, tableModel.KvKvs, tableModel.KvChecksum),\n\t}\n\n\tfor engineID, engineModel := range tableModel.Engines {\n\t\tengine := &EngineCheckpoint{\n\t\t\tStatus: CheckpointStatus(engineModel.Status),\n\t\t\tChunks: make([]*ChunkCheckpoint, 0, len(engineModel.Chunks)),\n\t\t}\n\n\t\tfor _, chunkModel := range engineModel.Chunks {\n\t\t\tcolPerm := make([]int, 0, len(chunkModel.ColumnPermutation))\n\t\t\tfor _, c := range chunkModel.ColumnPermutation {\n\t\t\t\tcolPerm = append(colPerm, int(c))\n\t\t\t}\n\t\t\tengine.Chunks = append(engine.Chunks, &ChunkCheckpoint{\n\t\t\t\tKey: ChunkCheckpointKey{\n\t\t\t\t\tPath:   chunkModel.Path,\n\t\t\t\t\tOffset: chunkModel.Offset,\n\t\t\t\t},\n\t\t\t\tFileMeta: mydump.SourceFileMeta{\n\t\t\t\t\tPath:        chunkModel.Path,\n\t\t\t\t\tType:        mydump.SourceType(chunkModel.Type),\n\t\t\t\t\tCompression: mydump.Compression(chunkModel.Compression),\n\t\t\t\t\tSortKey:     chunkModel.SortKey,\n\t\t\t\t\tFileSize:    chunkModel.FileSize,\n\t\t\t\t},\n\t\t\t\tColumnPermutation: colPerm,\n\t\t\t\tChunk: mydump.Chunk{\n\t\t\t\t\tOffset:       chunkModel.Pos,\n\t\t\t\t\tEndOffset:    chunkModel.EndOffset,\n\t\t\t\t\tPrevRowIDMax: chunkModel.PrevRowidMax,\n\t\t\t\t\tRowIDMax:     chunkModel.RowidMax,\n\t\t\t\t},\n\t\t\t\tChecksum:  verify.MakeKVChecksum(chunkModel.KvcBytes, chunkModel.KvcKvs, chunkModel.KvcChecksum),\n\t\t\t\tTimestamp: chunkModel.Timestamp,\n\t\t\t})\n\t\t}\n\n\t\tslices.SortFunc(engine.Chunks, func(i, j *ChunkCheckpoint) bool {\n\t\t\treturn i.Key.less(&j.Key)\n\t\t})\n\n\t\tcp.Engines[engineID] = engine\n\t}\n\n\treturn cp, nil\n}\n\nfunc (cpdb *FileCheckpointsDB) InsertEngineCheckpoints(_ context.Context, tableName string, checkpoints map[int32]*EngineCheckpoint) error {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\ttableModel := cpdb.checkpoints.Checkpoints[tableName]\n\tfor engineID, engine := range checkpoints {\n\t\tengineModel := &checkpointspb.EngineCheckpointModel{\n\t\t\tStatus: uint32(CheckpointStatusLoaded),\n\t\t\tChunks: make(map[string]*checkpointspb.ChunkCheckpointModel),\n\t\t}\n\t\tfor _, value := range engine.Chunks {\n\t\t\tkey := value.Key.String()\n\t\t\tchunk, ok := engineModel.Chunks[key]\n\t\t\tif !ok {\n\t\t\t\tchunk = &checkpointspb.ChunkCheckpointModel{\n\t\t\t\t\tPath:   value.Key.Path,\n\t\t\t\t\tOffset: value.Key.Offset,\n\t\t\t\t}\n\t\t\t\tengineModel.Chunks[key] = chunk\n\t\t\t}\n\t\t\tchunk.Type = int32(value.FileMeta.Type)\n\t\t\tchunk.Compression = int32(value.FileMeta.Compression)\n\t\t\tchunk.SortKey = value.FileMeta.SortKey\n\t\t\tchunk.FileSize = value.FileMeta.FileSize\n\t\t\tchunk.Pos = value.Chunk.Offset\n\t\t\tchunk.EndOffset = value.Chunk.EndOffset\n\t\t\tchunk.PrevRowidMax = value.Chunk.PrevRowIDMax\n\t\t\tchunk.RowidMax = value.Chunk.RowIDMax\n\t\t\tchunk.Timestamp = value.Timestamp\n\t\t\tif len(value.ColumnPermutation) > 0 {\n\t\t\t\tchunk.ColumnPermutation = intSlice2Int32Slice(value.ColumnPermutation)\n\t\t\t}\n\t\t}\n\t\ttableModel.Engines[engineID] = engineModel\n\t}\n\n\treturn errors.Trace(cpdb.save())\n}\n\nfunc (cpdb *FileCheckpointsDB) Update(_ context.Context, checkpointDiffs map[string]*TableCheckpointDiff) error {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\tfor tableName, cpd := range checkpointDiffs {\n\t\ttableModel := cpdb.checkpoints.Checkpoints[tableName]\n\t\tif cpd.hasStatus {\n\t\t\ttableModel.Status = uint32(cpd.status)\n\t\t}\n\t\tif cpd.hasRebase {\n\t\t\ttableModel.AllocBase = cpd.allocBase\n\t\t}\n\t\tif cpd.hasChecksum {\n\t\t\ttableModel.KvBytes = cpd.checksum.SumSize()\n\t\t\ttableModel.KvKvs = cpd.checksum.SumKVS()\n\t\t\ttableModel.KvChecksum = cpd.checksum.Sum()\n\t\t}\n\t\tfor engineID, engineDiff := range cpd.engines {\n\t\t\tengineModel := tableModel.Engines[engineID]\n\t\t\tif engineDiff.hasStatus {\n\t\t\t\tengineModel.Status = uint32(engineDiff.status)\n\t\t\t}\n\n\t\t\tfor key, diff := range engineDiff.chunks {\n\t\t\t\tchunkModel := engineModel.Chunks[key.String()]\n\t\t\t\tchunkModel.Pos = diff.pos\n\t\t\t\tchunkModel.PrevRowidMax = diff.rowID\n\t\t\t\tchunkModel.KvcBytes = diff.checksum.SumSize()\n\t\t\t\tchunkModel.KvcKvs = diff.checksum.SumKVS()\n\t\t\t\tchunkModel.KvcChecksum = diff.checksum.Sum()\n\t\t\t\tchunkModel.ColumnPermutation = intSlice2Int32Slice(diff.columnPermutation)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn cpdb.save()\n}\n\n// Management functions ----------------------------------------------------------------------------\n\nvar errCannotManageNullDB = errors.New(\"cannot perform this function while checkpoints is disabled\")\n\nfunc (*NullCheckpointsDB) RemoveCheckpoint(context.Context, string) error {\n\treturn errors.Trace(errCannotManageNullDB)\n}\n\nfunc (*NullCheckpointsDB) MoveCheckpoints(context.Context, int64) error {\n\treturn errors.Trace(errCannotManageNullDB)\n}\n\nfunc (*NullCheckpointsDB) GetLocalStoringTables(context.Context) (map[string][]int32, error) {\n\treturn nil, nil\n}\n\nfunc (*NullCheckpointsDB) IgnoreErrorCheckpoint(context.Context, string) error {\n\treturn errors.Trace(errCannotManageNullDB)\n}\n\nfunc (*NullCheckpointsDB) DestroyErrorCheckpoint(context.Context, string) ([]DestroyedTableCheckpoint, error) {\n\treturn nil, errors.Trace(errCannotManageNullDB)\n}\n\nfunc (*NullCheckpointsDB) DumpTables(context.Context, io.Writer) error {\n\treturn errors.Trace(errCannotManageNullDB)\n}\n\nfunc (*NullCheckpointsDB) DumpEngines(context.Context, io.Writer) error {\n\treturn errors.Trace(errCannotManageNullDB)\n}\n\nfunc (*NullCheckpointsDB) DumpChunks(context.Context, io.Writer) error {\n\treturn errors.Trace(errCannotManageNullDB)\n}\n\nfunc (cpdb *MySQLCheckpointsDB) RemoveCheckpoint(ctx context.Context, tableName string) error {\n\ts := common.SQLWithRetry{\n\t\tDB:     cpdb.db,\n\t\tLogger: log.FromContext(ctx).With(zap.String(\"table\", tableName)),\n\t}\n\n\tif tableName == allTables {\n\t\treturn s.Exec(ctx, \"remove all checkpoints\", \"DROP SCHEMA \"+cpdb.schema)\n\t}\n\n\tdeleteChunkQuery := fmt.Sprintf(DeleteCheckpointRecordTemplate, cpdb.schema, CheckpointTableNameChunk)\n\tdeleteEngineQuery := fmt.Sprintf(DeleteCheckpointRecordTemplate, cpdb.schema, CheckpointTableNameEngine)\n\tdeleteTableQuery := fmt.Sprintf(DeleteCheckpointRecordTemplate, cpdb.schema, CheckpointTableNameTable)\n\n\treturn s.Transact(ctx, \"remove checkpoints\", func(c context.Context, tx *sql.Tx) error {\n\t\tif _, e := tx.ExecContext(c, deleteChunkQuery, tableName); e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\tif _, e := tx.ExecContext(c, deleteEngineQuery, tableName); e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\tif _, e := tx.ExecContext(c, deleteTableQuery, tableName); e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\treturn nil\n\t})\n}\n\nfunc (cpdb *MySQLCheckpointsDB) MoveCheckpoints(ctx context.Context, taskID int64) error {\n\t// The \"cpdb.schema\" is an escaped schema name of the form \"`foo`\".\n\t// We use \"x[1:len(x)-1]\" instead of unescaping it to keep the\n\t// double-backquotes (if any) intact.\n\tnewSchema := fmt.Sprintf(\"`%s.%d.bak`\", cpdb.schema[1:len(cpdb.schema)-1], taskID)\n\ts := common.SQLWithRetry{\n\t\tDB:     cpdb.db,\n\t\tLogger: log.FromContext(ctx).With(zap.Int64(\"taskID\", taskID)),\n\t}\n\n\tcreateSchemaQuery := \"CREATE SCHEMA IF NOT EXISTS \" + newSchema\n\tif e := s.Exec(ctx, \"create backup checkpoints schema\", createSchemaQuery); e != nil {\n\t\treturn e\n\t}\n\tfor _, tbl := range []string{\n\t\tCheckpointTableNameChunk, CheckpointTableNameEngine,\n\t\tCheckpointTableNameTable, CheckpointTableNameTask,\n\t} {\n\t\tquery := fmt.Sprintf(\"RENAME TABLE %[1]s.%[3]s TO %[2]s.%[3]s\", cpdb.schema, newSchema, tbl)\n\t\tif e := s.Exec(ctx, fmt.Sprintf(\"move %s checkpoints table\", tbl), query); e != nil {\n\t\t\treturn e\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (cpdb *MySQLCheckpointsDB) GetLocalStoringTables(ctx context.Context) (map[string][]int32, error) {\n\tvar targetTables map[string][]int32\n\n\t// lightning didn't check CheckpointStatusMaxInvalid before this function is called, so we skip invalid ones\n\t// engines should exist if\n\t// 1. table status is earlier than CheckpointStatusIndexImported, and\n\t// 2. engine status is earlier than CheckpointStatusImported, and\n\t// 3. chunk has been read\n\n\tquery := fmt.Sprintf(`\n\t\tSELECT DISTINCT t.table_name, c.engine_id\n\t\tFROM %s.%s t, %s.%s c, %s.%s e\n\t\tWHERE t.table_name = c.table_name AND t.table_name = e.table_name AND c.engine_id = e.engine_id\n\t\t\tAND %d < t.status AND t.status < %d\n\t\t\tAND %d < e.status AND e.status < %d\n\t\t\tAND c.pos > c.offset;`,\n\t\tcpdb.schema, CheckpointTableNameTable, cpdb.schema, CheckpointTableNameChunk, cpdb.schema, CheckpointTableNameEngine,\n\t\tCheckpointStatusMaxInvalid, CheckpointStatusIndexImported,\n\t\tCheckpointStatusMaxInvalid, CheckpointStatusImported)\n\n\terr := common.Retry(\"get local storing tables\", log.FromContext(ctx), func() error {\n\t\ttargetTables = make(map[string][]int32)\n\t\trows, err := cpdb.db.QueryContext(ctx, query) // #nosec G201\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer rows.Close()\n\t\tfor rows.Next() {\n\t\t\tvar (\n\t\t\t\ttableName string\n\t\t\t\tengineID  int32\n\t\t\t)\n\t\t\tif err := rows.Scan(&tableName, &engineID); err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\t\t\ttargetTables[tableName] = append(targetTables[tableName], engineID)\n\t\t}\n\t\tif err := rows.Err(); err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\treturn targetTables, err\n}\n\nfunc (cpdb *MySQLCheckpointsDB) IgnoreErrorCheckpoint(ctx context.Context, tableName string) error {\n\tvar colName string\n\tif tableName == allTables {\n\t\t// This will expand to `WHERE 'all' = 'all'` and effectively allowing\n\t\t// all tables to be included.\n\t\tcolName = stringLitAll\n\t} else {\n\t\tcolName = columnTableName\n\t}\n\n\t// nolint:gosec\n\tengineQuery := fmt.Sprintf(`\n\t\tUPDATE %s.%s SET status = %d WHERE %s = ? AND status <= %d;\n\t`, cpdb.schema, CheckpointTableNameEngine, CheckpointStatusLoaded, colName, CheckpointStatusMaxInvalid)\n\n\t// nolint:gosec\n\ttableQuery := fmt.Sprintf(`\n\t\tUPDATE %s.%s SET status = %d WHERE %s = ? AND status <= %d;\n\t`, cpdb.schema, CheckpointTableNameTable, CheckpointStatusLoaded, colName, CheckpointStatusMaxInvalid)\n\n\ts := common.SQLWithRetry{\n\t\tDB:     cpdb.db,\n\t\tLogger: log.FromContext(ctx).With(zap.String(\"table\", tableName)),\n\t}\n\terr := s.Transact(ctx, \"ignore error checkpoints\", func(c context.Context, tx *sql.Tx) error {\n\t\tif _, e := tx.ExecContext(c, engineQuery, tableName); e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\tif _, e := tx.ExecContext(c, tableQuery, tableName); e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\treturn nil\n\t})\n\treturn errors.Trace(err)\n}\n\nfunc (cpdb *MySQLCheckpointsDB) DestroyErrorCheckpoint(ctx context.Context, tableName string) ([]DestroyedTableCheckpoint, error) {\n\tvar colName, aliasedColName string\n\n\tif tableName == allTables {\n\t\t// These will expand to `WHERE 'all' = 'all'` and effectively allowing\n\t\t// all tables to be included.\n\t\tcolName = stringLitAll\n\t\taliasedColName = stringLitAll\n\t} else {\n\t\tcolName = columnTableName\n\t\taliasedColName = \"t.table_name\"\n\t}\n\n\tselectQuery := fmt.Sprintf(`\n\t\tSELECT\n\t\t\tt.table_name,\n\t\t\tCOALESCE(MIN(e.engine_id), 0),\n\t\t\tCOALESCE(MAX(e.engine_id), -1)\n\t\tFROM %[1]s.%[4]s t\n\t\tLEFT JOIN %[1]s.%[5]s e ON t.table_name = e.table_name\n\t\tWHERE %[2]s = ? AND t.status <= %[3]d\n\t\tGROUP BY t.table_name;\n\t`, cpdb.schema, aliasedColName, CheckpointStatusMaxInvalid, CheckpointTableNameTable, CheckpointTableNameEngine)\n\n\t// nolint:gosec\n\tdeleteChunkQuery := fmt.Sprintf(`\n\t\tDELETE FROM %[1]s.%[4]s WHERE table_name IN (SELECT table_name FROM %[1]s.%[5]s WHERE %[2]s = ? AND status <= %[3]d)\n\t`, cpdb.schema, colName, CheckpointStatusMaxInvalid, CheckpointTableNameChunk, CheckpointTableNameTable)\n\n\t// nolint:gosec\n\tdeleteEngineQuery := fmt.Sprintf(`\n\t\tDELETE FROM %[1]s.%[4]s WHERE table_name IN (SELECT table_name FROM %[1]s.%[5]s WHERE %[2]s = ? AND status <= %[3]d)\n\t`, cpdb.schema, colName, CheckpointStatusMaxInvalid, CheckpointTableNameEngine, CheckpointTableNameTable)\n\n\t// nolint:gosec\n\tdeleteTableQuery := fmt.Sprintf(`\n\t\tDELETE FROM %s.%s WHERE %s = ? AND status <= %d\n\t`, cpdb.schema, CheckpointTableNameTable, colName, CheckpointStatusMaxInvalid)\n\n\tvar targetTables []DestroyedTableCheckpoint\n\n\ts := common.SQLWithRetry{\n\t\tDB:     cpdb.db,\n\t\tLogger: log.FromContext(ctx).With(zap.String(\"table\", tableName)),\n\t}\n\terr := s.Transact(ctx, \"destroy error checkpoints\", func(c context.Context, tx *sql.Tx) error {\n\t\t// Obtain the list of tables\n\t\ttargetTables = nil\n\t\trows, e := tx.QueryContext(c, selectQuery, tableName) // #nosec G201\n\t\tif e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer rows.Close()\n\t\tfor rows.Next() {\n\t\t\tvar dtc DestroyedTableCheckpoint\n\t\t\tif e := rows.Scan(&dtc.TableName, &dtc.MinEngineID, &dtc.MaxEngineID); e != nil {\n\t\t\t\treturn errors.Trace(e)\n\t\t\t}\n\t\t\ttargetTables = append(targetTables, dtc)\n\t\t}\n\t\tif e := rows.Err(); e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\n\t\t// Delete the checkpoints\n\t\tif _, e := tx.ExecContext(c, deleteChunkQuery, tableName); e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\tif _, e := tx.ExecContext(c, deleteEngineQuery, tableName); e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\tif _, e := tx.ExecContext(c, deleteTableQuery, tableName); e != nil {\n\t\t\treturn errors.Trace(e)\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\treturn targetTables, nil\n}\n\n//nolint:rowserrcheck // sqltocsv.Write will check this.\nfunc (cpdb *MySQLCheckpointsDB) DumpTables(ctx context.Context, writer io.Writer) error {\n\t//nolint: rowserrcheck\n\trows, err := cpdb.db.QueryContext(ctx, fmt.Sprintf(`\n\t\tSELECT\n\t\t\ttask_id,\n\t\t\ttable_name,\n\t\t\thex(hash) AS hash,\n\t\t\tstatus,\n\t\t\talloc_base,\n\t\t\tcreate_time,\n\t\t\tupdate_time\n\t\tFROM %s.%s;\n\t`, cpdb.schema, CheckpointTableNameTable))\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\t//nolint: errcheck\n\tdefer rows.Close()\n\n\treturn errors.Trace(sqltocsv.Write(writer, rows))\n}\n\n//nolint:rowserrcheck // sqltocsv.Write will check this.\nfunc (cpdb *MySQLCheckpointsDB) DumpEngines(ctx context.Context, writer io.Writer) error {\n\t//nolint: rowserrcheck\n\trows, err := cpdb.db.QueryContext(ctx, fmt.Sprintf(`\n\t\tSELECT\n\t\t\ttable_name,\n\t\t\tengine_id,\n\t\t\tstatus,\n\t\t\tcreate_time,\n\t\t\tupdate_time\n\t\tFROM %s.%s;\n\t`, cpdb.schema, CheckpointTableNameEngine))\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\t//nolint: errcheck\n\tdefer rows.Close()\n\n\treturn errors.Trace(sqltocsv.Write(writer, rows))\n}\n\n//nolint:rowserrcheck // sqltocsv.Write will check this.\nfunc (cpdb *MySQLCheckpointsDB) DumpChunks(ctx context.Context, writer io.Writer) error {\n\t//nolint: rowserrcheck\n\trows, err := cpdb.db.QueryContext(ctx, fmt.Sprintf(`\n\t\tSELECT\n\t\t\ttable_name,\n\t\t\tpath,\n\t\t\toffset,\n\t\t\ttype,\n\t\t\tcompression,\n\t\t\tsort_key,\n\t\t\tfile_size,\n\t\t\tcolumns,\n\t\t\tpos,\n\t\t\tend_offset,\n\t\t\tprev_rowid_max,\n\t\t\trowid_max,\n\t\t\tkvc_bytes,\n\t\t\tkvc_kvs,\n\t\t\tkvc_checksum,\n\t\t\tcreate_time,\n\t\t\tupdate_time\n\t\tFROM %s.%s;\n\t`, cpdb.schema, CheckpointTableNameChunk))\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\t//nolint: errcheck\n\tdefer rows.Close()\n\n\treturn errors.Trace(sqltocsv.Write(writer, rows))\n}\n\nfunc (cpdb *FileCheckpointsDB) RemoveCheckpoint(_ context.Context, tableName string) error {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\tif tableName == allTables {\n\t\tcpdb.checkpoints.Reset()\n\t\treturn errors.Trace(cpdb.exStorage.DeleteFile(cpdb.ctx, cpdb.fileName))\n\t}\n\n\tdelete(cpdb.checkpoints.Checkpoints, tableName)\n\treturn errors.Trace(cpdb.save())\n}\n\nfunc (cpdb *FileCheckpointsDB) MoveCheckpoints(ctx context.Context, taskID int64) error {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\tnewFileName := fmt.Sprintf(\"%s.%d.bak\", cpdb.fileName, taskID)\n\treturn cpdb.exStorage.Rename(cpdb.ctx, cpdb.fileName, newFileName)\n}\n\nfunc (cpdb *FileCheckpointsDB) GetLocalStoringTables(_ context.Context) (map[string][]int32, error) {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\ttargetTables := make(map[string][]int32)\n\n\tfor tableName, tableModel := range cpdb.checkpoints.Checkpoints {\n\t\tif tableModel.Status <= uint32(CheckpointStatusMaxInvalid) ||\n\t\t\ttableModel.Status >= uint32(CheckpointStatusIndexImported) {\n\t\t\tcontinue\n\t\t}\n\t\tfor engineID, engineModel := range tableModel.Engines {\n\t\t\tif engineModel.Status <= uint32(CheckpointStatusMaxInvalid) ||\n\t\t\t\tengineModel.Status >= uint32(CheckpointStatusImported) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tfor _, chunkModel := range engineModel.Chunks {\n\t\t\t\tif chunkModel.Pos > chunkModel.Offset {\n\t\t\t\t\ttargetTables[tableName] = append(targetTables[tableName], engineID)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn targetTables, nil\n}\n\nfunc (cpdb *FileCheckpointsDB) IgnoreErrorCheckpoint(_ context.Context, targetTableName string) error {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\tfor tableName, tableModel := range cpdb.checkpoints.Checkpoints {\n\t\tif !(targetTableName == allTables || targetTableName == tableName) {\n\t\t\tcontinue\n\t\t}\n\t\tif tableModel.Status <= uint32(CheckpointStatusMaxInvalid) {\n\t\t\ttableModel.Status = uint32(CheckpointStatusLoaded)\n\t\t}\n\t\tfor _, engineModel := range tableModel.Engines {\n\t\t\tif engineModel.Status <= uint32(CheckpointStatusMaxInvalid) {\n\t\t\t\tengineModel.Status = uint32(CheckpointStatusLoaded)\n\t\t\t}\n\t\t}\n\t}\n\treturn errors.Trace(cpdb.save())\n}\n\nfunc (cpdb *FileCheckpointsDB) DestroyErrorCheckpoint(_ context.Context, targetTableName string) ([]DestroyedTableCheckpoint, error) {\n\tcpdb.lock.Lock()\n\tdefer cpdb.lock.Unlock()\n\n\tvar targetTables []DestroyedTableCheckpoint\n\n\tfor tableName, tableModel := range cpdb.checkpoints.Checkpoints {\n\t\t// Obtain the list of tables\n\t\tif !(targetTableName == allTables || targetTableName == tableName) {\n\t\t\tcontinue\n\t\t}\n\t\tif tableModel.Status <= uint32(CheckpointStatusMaxInvalid) {\n\t\t\tvar minEngineID, maxEngineID int32 = math.MaxInt32, math.MinInt32\n\t\t\tfor engineID := range tableModel.Engines {\n\t\t\t\tif engineID < minEngineID {\n\t\t\t\t\tminEngineID = engineID\n\t\t\t\t}\n\t\t\t\tif engineID > maxEngineID {\n\t\t\t\t\tmaxEngineID = engineID\n\t\t\t\t}\n\t\t\t}\n\n\t\t\ttargetTables = append(targetTables, DestroyedTableCheckpoint{\n\t\t\t\tTableName:   tableName,\n\t\t\t\tMinEngineID: minEngineID,\n\t\t\t\tMaxEngineID: maxEngineID,\n\t\t\t})\n\t\t}\n\t}\n\n\t// Delete the checkpoints\n\tfor _, dtcp := range targetTables {\n\t\tdelete(cpdb.checkpoints.Checkpoints, dtcp.TableName)\n\t}\n\tif err := cpdb.save(); err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\treturn targetTables, nil\n}\n\nfunc (cpdb *FileCheckpointsDB) DumpTables(context.Context, io.Writer) error {\n\treturn errors.Errorf(\"dumping file checkpoint into CSV not unsupported, you may copy %s instead\", cpdb.path)\n}\n\nfunc (cpdb *FileCheckpointsDB) DumpEngines(context.Context, io.Writer) error {\n\treturn errors.Errorf(\"dumping file checkpoint into CSV not unsupported, you may copy %s instead\", cpdb.path)\n}\n\nfunc (cpdb *FileCheckpointsDB) DumpChunks(context.Context, io.Writer) error {\n\treturn errors.Errorf(\"dumping file checkpoint into CSV not unsupported, you may copy %s instead\", cpdb.path)\n}\n\nfunc intSlice2Int32Slice(s []int) []int32 {\n\tres := make([]int32, 0, len(s))\n\tfor _, i := range s {\n\t\tres = append(res, int32(i))\n\t}\n\treturn res\n}\n", "// Copyright 2019 PingCAP, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage common\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"net/http\"\n\t\"os\"\n\t\"strconv\"\n\t\"strings\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/go-sql-driver/mysql\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/failpoint\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/log\"\n\t\"github.com/pingcap/tidb/br/pkg/utils\"\n\ttmysql \"github.com/pingcap/tidb/errno\"\n\t\"github.com/pingcap/tidb/parser/model\"\n\t\"go.uber.org/zap\"\n)\n\nconst (\n\tretryTimeout = 3 * time.Second\n\n\tdefaultMaxRetry = 3\n)\n\n// MySQLConnectParam records the parameters needed to connect to a MySQL database.\ntype MySQLConnectParam struct {\n\tHost             string\n\tPort             int\n\tUser             string\n\tPassword         string\n\tSQLMode          string\n\tMaxAllowedPacket uint64\n\tTLS              string\n\tVars             map[string]string\n}\n\nfunc (param *MySQLConnectParam) ToDriverConfig() *mysql.Config {\n\tcfg := mysql.NewConfig()\n\tcfg.Params = make(map[string]string)\n\n\tcfg.User = param.User\n\tcfg.Passwd = param.Password\n\tcfg.Net = \"tcp\"\n\tcfg.Addr = net.JoinHostPort(param.Host, strconv.Itoa(param.Port))\n\tcfg.Params[\"charset\"] = \"utf8mb4\"\n\tcfg.Params[\"sql_mode\"] = fmt.Sprintf(\"'%s'\", param.SQLMode)\n\tcfg.MaxAllowedPacket = int(param.MaxAllowedPacket)\n\tcfg.TLSConfig = param.TLS\n\n\tfor k, v := range param.Vars {\n\t\tcfg.Params[k] = fmt.Sprintf(\"'%s'\", v)\n\t}\n\treturn cfg\n}\n\nfunc tryConnectMySQL(cfg *mysql.Config) (*sql.DB, error) {\n\tfailpoint.Inject(\"MustMySQLPassword\", func(val failpoint.Value) {\n\t\tpwd := val.(string)\n\t\tif cfg.Passwd != pwd {\n\t\t\tfailpoint.Return(nil, &mysql.MySQLError{Number: tmysql.ErrAccessDenied, Message: \"access denied\"})\n\t\t}\n\t\tfailpoint.Return(nil, nil)\n\t})\n\tc, err := mysql.NewConnector(cfg)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tdb := sql.OpenDB(c)\n\tif err = db.Ping(); err != nil {\n\t\t_ = db.Close()\n\t\treturn nil, errors.Trace(err)\n\t}\n\treturn db, nil\n}\n\n// ConnectMySQL connects MySQL with the dsn. If access is denied and the password is a valid base64 encoding,\n// we will try to connect MySQL with the base64 decoding of the password.\nfunc ConnectMySQL(cfg *mysql.Config) (*sql.DB, error) {\n\t// Try plain password first.\n\tdb, firstErr := tryConnectMySQL(cfg)\n\tif firstErr == nil {\n\t\treturn db, nil\n\t}\n\t// If access is denied and password is encoded by base64, try the decoded string as well.\n\tif mysqlErr, ok := errors.Cause(firstErr).(*mysql.MySQLError); ok && mysqlErr.Number == tmysql.ErrAccessDenied {\n\t\t// If password is encoded by base64, try the decoded string as well.\n\t\tif password, decodeErr := base64.StdEncoding.DecodeString(cfg.Passwd); decodeErr == nil && string(password) != cfg.Passwd {\n\t\t\tcfg.Passwd = string(password)\n\t\t\tdb2, err := tryConnectMySQL(cfg)\n\t\t\tif err == nil {\n\t\t\t\treturn db2, nil\n\t\t\t}\n\t\t}\n\t}\n\t// If we can't connect successfully, return the first error.\n\treturn nil, errors.Trace(firstErr)\n}\n\nfunc (param *MySQLConnectParam) Connect() (*sql.DB, error) {\n\tdb, err := ConnectMySQL(param.ToDriverConfig())\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\treturn db, nil\n}\n\n// IsDirExists checks if dir exists.\nfunc IsDirExists(name string) bool {\n\tf, err := os.Stat(name)\n\tif err != nil {\n\t\treturn false\n\t}\n\treturn f != nil && f.IsDir()\n}\n\n// IsEmptyDir checks if dir is empty.\nfunc IsEmptyDir(name string) bool {\n\tentries, err := os.ReadDir(name)\n\tif err != nil {\n\t\treturn false\n\t}\n\treturn len(entries) == 0\n}\n\n// SQLWithRetry constructs a retryable transaction.\ntype SQLWithRetry struct {\n\t// either *sql.DB or *sql.Conn\n\tDB           utils.DBExecutor\n\tLogger       log.Logger\n\tHideQueryLog bool\n}\n\nfunc (t SQLWithRetry) perform(_ context.Context, parentLogger log.Logger, purpose string, action func() error) error {\n\treturn Retry(purpose, parentLogger, action)\n}\n\n// Retry is shared by SQLWithRetry.perform, implementation of GlueCheckpointsDB and TiDB's glue implementation\nfunc Retry(purpose string, parentLogger log.Logger, action func() error) error {\n\tvar err error\noutside:\n\tfor i := 0; i < defaultMaxRetry; i++ {\n\t\tlogger := parentLogger.With(zap.Int(\"retryCnt\", i))\n\n\t\tif i > 0 {\n\t\t\tlogger.Warn(purpose + \" retry start\")\n\t\t\ttime.Sleep(retryTimeout)\n\t\t}\n\n\t\terr = action()\n\t\tswitch {\n\t\tcase err == nil:\n\t\t\treturn nil\n\t\t// do not retry NotFound error\n\t\tcase errors.IsNotFound(err):\n\t\t\tbreak outside\n\t\tcase IsRetryableError(err):\n\t\t\tlogger.Warn(purpose+\" failed but going to try again\", log.ShortError(err))\n\t\t\tcontinue\n\t\tdefault:\n\t\t\tlogger.Warn(purpose+\" failed with no retry\", log.ShortError(err))\n\t\t\tbreak outside\n\t\t}\n\t}\n\n\treturn errors.Annotatef(err, \"%s failed\", purpose)\n}\n\nfunc (t SQLWithRetry) QueryRow(ctx context.Context, purpose string, query string, dest ...interface{}) error {\n\tlogger := t.Logger\n\tif !t.HideQueryLog {\n\t\tlogger = logger.With(zap.String(\"query\", query))\n\t}\n\treturn t.perform(ctx, logger, purpose, func() error {\n\t\treturn t.DB.QueryRowContext(ctx, query).Scan(dest...)\n\t})\n}\n\n// Transact executes an action in a transaction, and retry if the\n// action failed with a retryable error.\nfunc (t SQLWithRetry) Transact(ctx context.Context, purpose string, action func(context.Context, *sql.Tx) error) error {\n\treturn t.perform(ctx, t.Logger, purpose, func() error {\n\t\ttxn, err := t.DB.BeginTx(ctx, nil)\n\t\tif err != nil {\n\t\t\treturn errors.Annotate(err, \"begin transaction failed\")\n\t\t}\n\n\t\terr = action(ctx, txn)\n\t\tif err != nil {\n\t\t\trerr := txn.Rollback()\n\t\t\tif rerr != nil {\n\t\t\t\tt.Logger.Error(purpose+\" rollback transaction failed\", log.ShortError(rerr))\n\t\t\t}\n\t\t\t// we should return the exec err, instead of the rollback rerr.\n\t\t\t// no need to errors.Trace() it, as the error comes from user code anyway.\n\t\t\treturn err\n\t\t}\n\n\t\terr = txn.Commit()\n\t\tif err != nil {\n\t\t\treturn errors.Annotate(err, \"commit transaction failed\")\n\t\t}\n\n\t\treturn nil\n\t})\n}\n\n// Exec executes a single SQL with optional retry.\nfunc (t SQLWithRetry) Exec(ctx context.Context, purpose string, query string, args ...interface{}) error {\n\tlogger := t.Logger\n\tif !t.HideQueryLog {\n\t\tlogger = logger.With(zap.String(\"query\", query), zap.Reflect(\"args\", args))\n\t}\n\treturn t.perform(ctx, logger, purpose, func() error {\n\t\t_, err := t.DB.ExecContext(ctx, query, args...)\n\t\treturn errors.Trace(err)\n\t})\n}\n\n// IsContextCanceledError returns whether the error is caused by context\n// cancellation. This function should only be used when the code logic is\n// affected by whether the error is canceling or not.\n//\n// This function returns `false` (not a context-canceled error) if `err == nil`.\nfunc IsContextCanceledError(err error) bool {\n\treturn log.IsContextCanceledError(err)\n}\n\n// UniqueTable returns an unique table name.\nfunc UniqueTable(schema string, table string) string {\n\tvar builder strings.Builder\n\tWriteMySQLIdentifier(&builder, schema)\n\tbuilder.WriteByte('.')\n\tWriteMySQLIdentifier(&builder, table)\n\treturn builder.String()\n}\n\n// EscapeIdentifier quote and escape an sql identifier\nfunc EscapeIdentifier(identifier string) string {\n\tvar builder strings.Builder\n\tWriteMySQLIdentifier(&builder, identifier)\n\treturn builder.String()\n}\n\n// Writes a MySQL identifier into the string builder.\n// The identifier is always escaped into the form \"`foo`\".\nfunc WriteMySQLIdentifier(builder *strings.Builder, identifier string) {\n\tbuilder.Grow(len(identifier) + 2)\n\tbuilder.WriteByte('`')\n\n\t// use a C-style loop instead of range loop to avoid UTF-8 decoding\n\tfor i := 0; i < len(identifier); i++ {\n\t\tb := identifier[i]\n\t\tif b == '`' {\n\t\t\tbuilder.WriteString(\"``\")\n\t\t} else {\n\t\t\tbuilder.WriteByte(b)\n\t\t}\n\t}\n\n\tbuilder.WriteByte('`')\n}\n\nfunc InterpolateMySQLString(s string) string {\n\tvar builder strings.Builder\n\tbuilder.Grow(len(s) + 2)\n\tbuilder.WriteByte('\\'')\n\tfor i := 0; i < len(s); i++ {\n\t\tb := s[i]\n\t\tif b == '\\'' {\n\t\t\tbuilder.WriteString(\"''\")\n\t\t} else {\n\t\t\tbuilder.WriteByte(b)\n\t\t}\n\t}\n\tbuilder.WriteByte('\\'')\n\treturn builder.String()\n}\n\n// TableExists return whether table with specified name exists in target db\nfunc TableExists(ctx context.Context, db utils.QueryExecutor, schema, table string) (bool, error) {\n\tquery := \"SELECT 1 from INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = ? AND TABLE_NAME = ?\"\n\tvar exist string\n\terr := db.QueryRowContext(ctx, query, schema, table).Scan(&exist)\n\tswitch {\n\tcase err == nil:\n\t\treturn true, nil\n\tcase err == sql.ErrNoRows:\n\t\treturn false, nil\n\tdefault:\n\t\treturn false, errors.Annotatef(err, \"check table exists failed\")\n\t}\n}\n\n// SchemaExists return whether schema with specified name exists.\nfunc SchemaExists(ctx context.Context, db utils.QueryExecutor, schema string) (bool, error) {\n\tquery := \"SELECT 1 from INFORMATION_SCHEMA.SCHEMATA WHERE SCHEMA_NAME = ?\"\n\tvar exist string\n\terr := db.QueryRowContext(ctx, query, schema).Scan(&exist)\n\tswitch {\n\tcase err == nil:\n\t\treturn true, nil\n\tcase err == sql.ErrNoRows:\n\t\treturn false, nil\n\tdefault:\n\t\treturn false, errors.Annotatef(err, \"check schema exists failed\")\n\t}\n}\n\n// GetJSON fetches a page and parses it as JSON. The parsed result will be\n// stored into the `v`. The variable `v` must be a pointer to a type that can be\n// unmarshalled from JSON.\n//\n// Example:\n//\n//\tclient := &http.Client{}\n//\tvar resp struct { IP string }\n//\tif err := util.GetJSON(client, \"http://api.ipify.org/?format=json\", &resp); err != nil {\n//\t\treturn errors.Trace(err)\n//\t}\n//\tfmt.Println(resp.IP)\nfunc GetJSON(ctx context.Context, client *http.Client, url string, v interface{}) error {\n\treq, err := http.NewRequestWithContext(ctx, \"GET\", url, nil)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tresp, err := client.Do(req)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tdefer resp.Body.Close()\n\n\tif resp.StatusCode != http.StatusOK {\n\t\tbody, err := io.ReadAll(resp.Body)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\treturn errors.Errorf(\"get %s http status code != 200, message %s\", url, string(body))\n\t}\n\n\treturn errors.Trace(json.NewDecoder(resp.Body).Decode(v))\n}\n\n// KillMySelf sends sigint to current process, used in integration test only\n//\n// Only works on Unix. Signaling on Windows is not supported.\nfunc KillMySelf() error {\n\tproc, err := os.FindProcess(os.Getpid())\n\tif err == nil {\n\t\terr = proc.Signal(syscall.SIGINT)\n\t}\n\treturn errors.Trace(err)\n}\n\n// KvPair is a pair of key and value.\ntype KvPair struct {\n\t// Key is the key of the KV pair\n\tKey []byte\n\t// Val is the value of the KV pair\n\tVal []byte\n\t// RowID is the row id of the KV pair.\n\tRowID int64\n}\n\n// TableHasAutoRowID return whether table has auto generated row id\nfunc TableHasAutoRowID(info *model.TableInfo) bool {\n\treturn !info.PKIsHandle && !info.IsCommonHandle\n}\n\n// TableHasAutoID return whether table has auto generated id.\nfunc TableHasAutoID(info *model.TableInfo) bool {\n\treturn TableHasAutoRowID(info) || info.GetAutoIncrementColInfo() != nil || info.ContainsAutoRandomBits()\n}\n\n// StringSliceEqual checks if two string slices are equal.\nfunc StringSliceEqual(a, b []string) bool {\n\tif len(a) != len(b) {\n\t\treturn false\n\t}\n\tfor i, v := range a {\n\t\tif v != b[i] {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n", "// Copyright 2019 PingCAP, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage common_test\n\nimport (\n\t\"context\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/http/httptest\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/DATA-DOG/go-sqlmock\"\n\t\"github.com/go-sql-driver/mysql\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/failpoint\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/common\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/log\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestDirNotExist(t *testing.T) {\n\trequire.True(t, common.IsDirExists(\".\"))\n\trequire.False(t, common.IsDirExists(\"not-exists\"))\n}\n\nfunc TestGetJSON(t *testing.T) {\n\ttype TestPayload struct {\n\t\tUsername string `json:\"username\"`\n\t\tPassword string `json:\"password\"`\n\t}\n\trequest := TestPayload{\n\t\tUsername: \"lightning\",\n\t\tPassword: \"lightning-ctl\",\n\t}\n\n\tctx := context.Background()\n\t// Mock success response\n\thandle := func(res http.ResponseWriter, _ *http.Request) {\n\t\tres.WriteHeader(http.StatusOK)\n\t\terr := json.NewEncoder(res).Encode(request)\n\t\trequire.NoError(t, err)\n\t}\n\ttestServer := httptest.NewServer(http.HandlerFunc(func(res http.ResponseWriter, req *http.Request) {\n\t\thandle(res, req)\n\t}))\n\tdefer testServer.Close()\n\n\tclient := &http.Client{Timeout: time.Second}\n\n\tresponse := TestPayload{}\n\terr := common.GetJSON(ctx, client, \"http://not-exists\", &response)\n\trequire.Error(t, err)\n\terr = common.GetJSON(ctx, client, testServer.URL, &response)\n\trequire.NoError(t, err)\n\trequire.Equal(t, request, response)\n\n\t// Mock `StatusNoContent` response\n\thandle = func(res http.ResponseWriter, _ *http.Request) {\n\t\tres.WriteHeader(http.StatusNoContent)\n\t}\n\terr = common.GetJSON(ctx, client, testServer.URL, &response)\n\trequire.Error(t, err)\n\trequire.Regexp(t, \".*http status code != 200.*\", err.Error())\n}\n\nfunc TestConnect(t *testing.T) {\n\tplainPsw := \"dQAUoDiyb1ucWZk7\"\n\n\trequire.NoError(t, failpoint.Enable(\n\t\t\"github.com/pingcap/tidb/br/pkg/lightning/common/MustMySQLPassword\",\n\t\tfmt.Sprintf(\"return(\\\"%s\\\")\", plainPsw)))\n\tdefer func() {\n\t\trequire.NoError(t, failpoint.Disable(\"github.com/pingcap/tidb/br/pkg/lightning/common/MustMySQLPassword\"))\n\t}()\n\n\tparam := common.MySQLConnectParam{\n\t\tHost:             \"127.0.0.1\",\n\t\tPort:             4000,\n\t\tUser:             \"root\",\n\t\tPassword:         plainPsw,\n\t\tSQLMode:          \"strict\",\n\t\tMaxAllowedPacket: 1234,\n\t}\n\t_, err := param.Connect()\n\trequire.NoError(t, err)\n\tparam.Password = base64.StdEncoding.EncodeToString([]byte(plainPsw))\n\t_, err = param.Connect()\n\trequire.NoError(t, err)\n}\n\nfunc TestIsContextCanceledError(t *testing.T) {\n\trequire.True(t, common.IsContextCanceledError(context.Canceled))\n\trequire.False(t, common.IsContextCanceledError(io.EOF))\n}\n\nfunc TestUniqueTable(t *testing.T) {\n\ttableName := common.UniqueTable(\"test\", \"t1\")\n\trequire.Equal(t, \"`test`.`t1`\", tableName)\n\n\ttableName = common.UniqueTable(\"test\", \"t`1\")\n\trequire.Equal(t, \"`test`.`t``1`\", tableName)\n}\n\nfunc TestSQLWithRetry(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer db.Close()\n\n\tsqlWithRetry := &common.SQLWithRetry{\n\t\tDB:     db,\n\t\tLogger: log.L(),\n\t}\n\taValue := new(int)\n\n\t// retry defaultMaxRetry times and still failed\n\tfor i := 0; i < 3; i++ {\n\t\tmock.ExpectQuery(\"select a from test.t1\").WillReturnError(errors.Annotate(mysql.ErrInvalidConn, \"mock error\"))\n\t}\n\terr = sqlWithRetry.QueryRow(context.Background(), \"\", \"select a from test.t1\", aValue)\n\trequire.Regexp(t, \".*mock error\", err.Error())\n\n\t// meet unretryable error and will return directly\n\tmock.ExpectQuery(\"select a from test.t1\").WillReturnError(context.Canceled)\n\terr = sqlWithRetry.QueryRow(context.Background(), \"\", \"select a from test.t1\", aValue)\n\trequire.Regexp(t, \".*context canceled\", err.Error())\n\n\t// query success\n\trows := sqlmock.NewRows([]string{\"a\"}).AddRow(\"1\")\n\tmock.ExpectQuery(\"select a from test.t1\").WillReturnRows(rows)\n\n\terr = sqlWithRetry.QueryRow(context.Background(), \"\", \"select a from test.t1\", aValue)\n\trequire.NoError(t, err)\n\trequire.Equal(t, 1, *aValue)\n\n\t// test Exec\n\tmock.ExpectExec(\"delete from\").WillReturnError(context.Canceled)\n\terr = sqlWithRetry.Exec(context.Background(), \"\", \"delete from test.t1 where id = ?\", 2)\n\trequire.Regexp(t, \".*context canceled\", err.Error())\n\n\tmock.ExpectExec(\"delete from\").WillReturnResult(sqlmock.NewResult(0, 1))\n\terr = sqlWithRetry.Exec(context.Background(), \"\", \"delete from test.t1 where id = ?\", 2)\n\trequire.NoError(t, err)\n\n\trequire.Nil(t, mock.ExpectationsWereMet())\n}\n\nfunc TestStringSliceEqual(t *testing.T) {\n\tassert.True(t, common.StringSliceEqual(nil, nil))\n\tassert.True(t, common.StringSliceEqual(nil, []string{}))\n\tassert.False(t, common.StringSliceEqual(nil, []string{\"a\"}))\n\tassert.False(t, common.StringSliceEqual([]string{\"a\"}, nil))\n\tassert.True(t, common.StringSliceEqual([]string{\"a\"}, []string{\"a\"}))\n\tassert.False(t, common.StringSliceEqual([]string{\"a\"}, []string{\"b\"}))\n\tassert.True(t, common.StringSliceEqual([]string{\"a\", \"b\", \"c\"}, []string{\"a\", \"b\", \"c\"}))\n\tassert.False(t, common.StringSliceEqual([]string{\"a\"}, []string{\"a\", \"b\", \"c\"}))\n\tassert.False(t, common.StringSliceEqual([]string{\"a\", \"b\", \"c\"}, []string{\"a\", \"b\"}))\n\tassert.False(t, common.StringSliceEqual([]string{\"a\", \"x\", \"y\"}, []string{\"a\", \"y\", \"x\"}))\n}\n\nfunc TestInterpolateMySQLString(t *testing.T) {\n\tassert.Equal(t, \"'123'\", common.InterpolateMySQLString(\"123\"))\n\tassert.Equal(t, \"'1''23'\", common.InterpolateMySQLString(\"1'23\"))\n\tassert.Equal(t, \"'1''2''''3'\", common.InterpolateMySQLString(\"1'2''3\"))\n}\n", "// Copyright 2019 PingCAP, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage config\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"math\"\n\t\"net\"\n\t\"net/url\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\t\"unicode/utf8\"\n\n\t\"github.com/BurntSushi/toml\"\n\t\"github.com/docker/go-units\"\n\tgomysql \"github.com/go-sql-driver/mysql\"\n\t\"github.com/google/uuid\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/common\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/log\"\n\ttidbcfg \"github.com/pingcap/tidb/config\"\n\t\"github.com/pingcap/tidb/parser/mysql\"\n\t\"github.com/pingcap/tidb/util\"\n\tfilter \"github.com/pingcap/tidb/util/table-filter\"\n\trouter \"github.com/pingcap/tidb/util/table-router\"\n\t\"go.uber.org/atomic\"\n\t\"go.uber.org/zap\"\n)\n\nconst (\n\t// ImportMode defines mode of import for tikv.\n\tImportMode = \"import\"\n\t// NormalMode defines mode of normal for tikv.\n\tNormalMode = \"normal\"\n\n\t// BackendTiDB is a constant for choosing the \"TiDB\" backend in the configuration.\n\tBackendTiDB = \"tidb\"\n\t// BackendLocal is a constant for choosing the \"Local\" backup in the configuration.\n\t// In this mode, we write & sort kv pairs with local storage and directly write them to tikv.\n\tBackendLocal = \"local\"\n\n\t// CheckpointDriverMySQL is a constant for choosing the \"MySQL\" checkpoint driver in the configuration.\n\tCheckpointDriverMySQL = \"mysql\"\n\t// CheckpointDriverFile is a constant for choosing the \"File\" checkpoint driver in the configuration.\n\tCheckpointDriverFile = \"file\"\n\n\t// ReplaceOnDup indicates using REPLACE INTO to insert data\n\tReplaceOnDup = \"replace\"\n\t// IgnoreOnDup indicates using INSERT IGNORE INTO to insert data\n\tIgnoreOnDup = \"ignore\"\n\t// ErrorOnDup indicates using INSERT INTO to insert data, which would violate PK or UNIQUE constraint\n\tErrorOnDup = \"error\"\n\n\tdefaultDistSQLScanConcurrency     = 15\n\tdefaultBuildStatsConcurrency      = 20\n\tdefaultIndexSerialScanConcurrency = 20\n\tdefaultChecksumTableConcurrency   = 2\n\tdefaultTableConcurrency           = 6\n\tdefaultIndexConcurrency           = 2\n\n\t// defaultMetaSchemaName is the default database name used to store lightning metadata\n\tdefaultMetaSchemaName     = \"lightning_metadata\"\n\tdefaultTaskInfoSchemaName = \"lightning_task_info\"\n\n\t// autoDiskQuotaLocalReservedSpeed is the estimated size increase per\n\t// millisecond per write thread the local backend may gain on all engines.\n\t// This is used to compute the maximum size overshoot between two disk quota\n\t// checks, if the first one has barely passed.\n\t//\n\t// With cron.check-disk-quota = 1m, region-concurrency = 40, this should\n\t// contribute 2.3 GiB to the reserved size.\n\t// autoDiskQuotaLocalReservedSpeed uint64 = 1 * units.KiB\n\tdefaultEngineMemCacheSize      = 512 * units.MiB\n\tdefaultLocalWriterMemCacheSize = 128 * units.MiB\n\n\tdefaultCSVDataCharacterSet       = \"binary\"\n\tdefaultCSVDataInvalidCharReplace = utf8.RuneError\n)\n\nvar (\n\tsupportedStorageTypes = []string{\"file\", \"local\", \"s3\", \"noop\", \"gcs\", \"gs\"}\n\n\tdefaultFilter = []string{\n\t\t\"*.*\",\n\t\t\"!mysql.*\",\n\t\t\"!sys.*\",\n\t\t\"!INFORMATION_SCHEMA.*\",\n\t\t\"!PERFORMANCE_SCHEMA.*\",\n\t\t\"!METRICS_SCHEMA.*\",\n\t\t\"!INSPECTION_SCHEMA.*\",\n\t}\n)\n\n// GetDefaultFilter gets the default table filter used in Lightning.\n// It clones the original default filter,\n// so that the original value won't be changed when the returned slice's element is changed.\nfunc GetDefaultFilter() []string {\n\treturn append([]string{}, defaultFilter...)\n}\n\ntype DBStore struct {\n\tHost       string    `toml:\"host\" json:\"host\"`\n\tPort       int       `toml:\"port\" json:\"port\"`\n\tUser       string    `toml:\"user\" json:\"user\"`\n\tPsw        string    `toml:\"password\" json:\"-\"`\n\tStatusPort int       `toml:\"status-port\" json:\"status-port\"`\n\tPdAddr     string    `toml:\"pd-addr\" json:\"pd-addr\"`\n\tStrSQLMode string    `toml:\"sql-mode\" json:\"sql-mode\"`\n\tTLS        string    `toml:\"tls\" json:\"tls\"`\n\tSecurity   *Security `toml:\"security\" json:\"security\"`\n\n\tSQLMode          mysql.SQLMode `toml:\"-\" json:\"-\"`\n\tMaxAllowedPacket uint64        `toml:\"max-allowed-packet\" json:\"max-allowed-packet\"`\n\n\tDistSQLScanConcurrency     int               `toml:\"distsql-scan-concurrency\" json:\"distsql-scan-concurrency\"`\n\tBuildStatsConcurrency      int               `toml:\"build-stats-concurrency\" json:\"build-stats-concurrency\"`\n\tIndexSerialScanConcurrency int               `toml:\"index-serial-scan-concurrency\" json:\"index-serial-scan-concurrency\"`\n\tChecksumTableConcurrency   int               `toml:\"checksum-table-concurrency\" json:\"checksum-table-concurrency\"`\n\tVars                       map[string]string `toml:\"-\" json:\"vars\"`\n}\n\ntype Config struct {\n\tTaskID int64 `toml:\"-\" json:\"id\"`\n\n\tApp  Lightning `toml:\"lightning\" json:\"lightning\"`\n\tTiDB DBStore   `toml:\"tidb\" json:\"tidb\"`\n\n\tCheckpoint   Checkpoint          `toml:\"checkpoint\" json:\"checkpoint\"`\n\tMydumper     MydumperRuntime     `toml:\"mydumper\" json:\"mydumper\"`\n\tTikvImporter TikvImporter        `toml:\"tikv-importer\" json:\"tikv-importer\"`\n\tPostRestore  PostRestore         `toml:\"post-restore\" json:\"post-restore\"`\n\tCron         Cron                `toml:\"cron\" json:\"cron\"`\n\tRoutes       []*router.TableRule `toml:\"routes\" json:\"routes\"`\n\tSecurity     Security            `toml:\"security\" json:\"security\"`\n\n\tBWList filter.MySQLReplicationRules `toml:\"black-white-list\" json:\"black-white-list\"`\n}\n\nfunc (cfg *Config) String() string {\n\tbytes, err := json.Marshal(cfg)\n\tif err != nil {\n\t\tlog.L().Error(\"marshal config to json error\", log.ShortError(err))\n\t}\n\treturn string(bytes)\n}\n\nfunc (cfg *Config) ToTLS() (*common.TLS, error) {\n\thostPort := net.JoinHostPort(cfg.TiDB.Host, strconv.Itoa(cfg.TiDB.StatusPort))\n\treturn common.NewTLS(\n\t\tcfg.Security.CAPath,\n\t\tcfg.Security.CertPath,\n\t\tcfg.Security.KeyPath,\n\t\thostPort,\n\t\tcfg.Security.CABytes,\n\t\tcfg.Security.CertBytes,\n\t\tcfg.Security.KeyBytes,\n\t)\n}\n\ntype Lightning struct {\n\tTableConcurrency  int    `toml:\"table-concurrency\" json:\"table-concurrency\"`\n\tIndexConcurrency  int    `toml:\"index-concurrency\" json:\"index-concurrency\"`\n\tRegionConcurrency int    `toml:\"region-concurrency\" json:\"region-concurrency\"`\n\tIOConcurrency     int    `toml:\"io-concurrency\" json:\"io-concurrency\"`\n\tCheckRequirements bool   `toml:\"check-requirements\" json:\"check-requirements\"`\n\tMetaSchemaName    string `toml:\"meta-schema-name\" json:\"meta-schema-name\"`\n\n\tMaxError           MaxError `toml:\"max-error\" json:\"max-error\"`\n\tTaskInfoSchemaName string   `toml:\"task-info-schema-name\" json:\"task-info-schema-name\"`\n}\n\ntype PostOpLevel int\n\nconst (\n\tOpLevelOff PostOpLevel = iota\n\tOpLevelOptional\n\tOpLevelRequired\n)\n\nfunc (t *PostOpLevel) UnmarshalTOML(v interface{}) error {\n\tswitch val := v.(type) {\n\tcase bool:\n\t\tif val {\n\t\t\t*t = OpLevelRequired\n\t\t} else {\n\t\t\t*t = OpLevelOff\n\t\t}\n\tcase string:\n\t\treturn t.FromStringValue(val)\n\tdefault:\n\t\treturn errors.Errorf(\"invalid op level '%v', please choose valid option between ['off', 'optional', 'required']\", v)\n\t}\n\treturn nil\n}\n\nfunc (t PostOpLevel) MarshalText() ([]byte, error) {\n\treturn []byte(t.String()), nil\n}\n\n// parser command line parameter\nfunc (t *PostOpLevel) FromStringValue(s string) error {\n\tswitch strings.ToLower(s) {\n\t//nolint:goconst // This 'false' and other 'false's aren't the same.\n\tcase \"off\", \"false\":\n\t\t*t = OpLevelOff\n\tcase \"required\", \"true\":\n\t\t*t = OpLevelRequired\n\tcase \"optional\":\n\t\t*t = OpLevelOptional\n\tdefault:\n\t\treturn errors.Errorf(\"invalid op level '%s', please choose valid option between ['off', 'optional', 'required']\", s)\n\t}\n\treturn nil\n}\n\nfunc (t *PostOpLevel) MarshalJSON() ([]byte, error) {\n\treturn []byte(`\"` + t.String() + `\"`), nil\n}\n\nfunc (t *PostOpLevel) UnmarshalJSON(data []byte) error {\n\treturn t.FromStringValue(strings.Trim(string(data), `\"`))\n}\n\nfunc (t PostOpLevel) String() string {\n\tswitch t {\n\tcase OpLevelOff:\n\t\treturn \"off\"\n\tcase OpLevelOptional:\n\t\treturn \"optional\"\n\tcase OpLevelRequired:\n\t\treturn \"required\"\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"invalid post process type '%d'\", t))\n\t}\n}\n\ntype CheckpointKeepStrategy int\n\nconst (\n\t// remove checkpoint data\n\tCheckpointRemove CheckpointKeepStrategy = iota\n\t// keep by rename checkpoint file/db according to task id\n\tCheckpointRename\n\t// keep checkpoint data unchanged\n\tCheckpointOrigin\n)\n\nfunc (t *CheckpointKeepStrategy) UnmarshalTOML(v interface{}) error {\n\tswitch val := v.(type) {\n\tcase bool:\n\t\tif val {\n\t\t\t*t = CheckpointRename\n\t\t} else {\n\t\t\t*t = CheckpointRemove\n\t\t}\n\tcase string:\n\t\treturn t.FromStringValue(val)\n\tdefault:\n\t\treturn errors.Errorf(\"invalid checkpoint keep strategy '%v', please choose valid option between ['remove', 'rename', 'origin']\", v)\n\t}\n\treturn nil\n}\n\nfunc (t CheckpointKeepStrategy) MarshalText() ([]byte, error) {\n\treturn []byte(t.String()), nil\n}\n\n// parser command line parameter\nfunc (t *CheckpointKeepStrategy) FromStringValue(s string) error {\n\tswitch strings.ToLower(s) {\n\t//nolint:goconst // This 'false' and other 'false's aren't the same.\n\tcase \"remove\", \"false\":\n\t\t*t = CheckpointRemove\n\tcase \"rename\", \"true\":\n\t\t*t = CheckpointRename\n\tcase \"origin\":\n\t\t*t = CheckpointOrigin\n\tdefault:\n\t\treturn errors.Errorf(\"invalid checkpoint keep strategy '%s', please choose valid option between ['remove', 'rename', 'origin']\", s)\n\t}\n\treturn nil\n}\n\nfunc (t *CheckpointKeepStrategy) MarshalJSON() ([]byte, error) {\n\treturn []byte(`\"` + t.String() + `\"`), nil\n}\n\nfunc (t *CheckpointKeepStrategy) UnmarshalJSON(data []byte) error {\n\treturn t.FromStringValue(strings.Trim(string(data), `\"`))\n}\n\nfunc (t CheckpointKeepStrategy) String() string {\n\tswitch t {\n\tcase CheckpointRemove:\n\t\treturn \"remove\"\n\tcase CheckpointRename:\n\t\treturn \"rename\"\n\tcase CheckpointOrigin:\n\t\treturn \"origin\"\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"invalid post process type '%d'\", t))\n\t}\n}\n\n// MaxError configures the maximum number of acceptable errors per kind.\ntype MaxError struct {\n\t// Syntax is the maximum number of syntax errors accepted.\n\t// When tolerated, the file chunk causing syntax error will be skipped, and adds 1 to the counter.\n\t// TODO Currently this is hard-coded to zero.\n\tSyntax atomic.Int64 `toml:\"syntax\" json:\"-\"`\n\n\t// Charset is the maximum number of character-set conversion errors accepted.\n\t// When tolerated, and `data-invalid-char-replace` is not changed from \"\\ufffd\",\n\t// every invalid byte in the source file will be converted to U+FFFD and adds 1 to the counter.\n\t// Note that a failed conversion a column's character set (e.g. UTF8-to-GBK conversion)\n\t// is counted as a type error, not a charset error.\n\t// TODO character-set conversion is not yet implemented.\n\tCharset atomic.Int64 `toml:\"charset\" json:\"-\"`\n\n\t// Type is the maximum number of type errors accepted.\n\t// This includes strict-mode errors such as zero in dates, integer overflow, character string too long, etc.\n\t// In TiDB backend, this also includes all possible SQL errors raised from INSERT,\n\t// such as unique key conflict when `on-duplicate` is set to `error`.\n\t// When tolerated, the row causing the error will be skipped, and adds 1 to the counter.\n\tType atomic.Int64 `toml:\"type\" json:\"type\"`\n\n\t// Conflict is the maximum number of unique key conflicts in local backend accepted.\n\t// When tolerated, every pair of conflict adds 1 to the counter.\n\t// Those pairs will NOT be deleted from the target. Conflict resolution is performed separately.\n\t// TODO Currently this is hard-coded to infinity.\n\tConflict atomic.Int64 `toml:\"conflict\" json:\"-\"`\n}\n\nfunc (cfg *MaxError) UnmarshalTOML(v interface{}) error {\n\tswitch val := v.(type) {\n\tcase int64:\n\t\t// ignore val that is smaller than 0\n\t\tif val < 0 {\n\t\t\tval = 0\n\t\t}\n\t\tcfg.Syntax.Store(0)\n\t\tcfg.Charset.Store(math.MaxInt64)\n\t\tcfg.Type.Store(val)\n\t\tcfg.Conflict.Store(math.MaxInt64)\n\t\treturn nil\n\tcase map[string]interface{}:\n\t\t// TODO support stuff like `max-error = { charset = 1000, type = 1000 }` if proved useful.\n\tdefault:\n\t}\n\treturn errors.Errorf(\"invalid max-error '%v', should be an integer\", v)\n}\n\n// DuplicateResolutionAlgorithm is the config type of how to resolve duplicates.\ntype DuplicateResolutionAlgorithm int\n\nconst (\n\t// DupeResAlgNone doesn't detect duplicate.\n\tDupeResAlgNone DuplicateResolutionAlgorithm = iota\n\n\t// DupeResAlgRecord only records duplicate records to `lightning_task_info.conflict_error_v1` table on the target TiDB.\n\tDupeResAlgRecord\n\n\t// DupeResAlgRemove records all duplicate records like the 'record' algorithm and remove all information related to the\n\t// duplicated rows. Users need to analyze the lightning_task_info.conflict_error_v1 table to add back the correct rows.\n\tDupeResAlgRemove\n)\n\nfunc (dra *DuplicateResolutionAlgorithm) UnmarshalTOML(v interface{}) error {\n\tif val, ok := v.(string); ok {\n\t\treturn dra.FromStringValue(val)\n\t}\n\treturn errors.Errorf(\"invalid duplicate-resolution '%v', please choose valid option between ['record', 'none', 'remove']\", v)\n}\n\nfunc (dra DuplicateResolutionAlgorithm) MarshalText() ([]byte, error) {\n\treturn []byte(dra.String()), nil\n}\n\nfunc (dra *DuplicateResolutionAlgorithm) FromStringValue(s string) error {\n\tswitch strings.ToLower(s) {\n\tcase \"record\":\n\t\t*dra = DupeResAlgRecord\n\tcase \"none\":\n\t\t*dra = DupeResAlgNone\n\tcase \"remove\":\n\t\t*dra = DupeResAlgRemove\n\tdefault:\n\t\treturn errors.Errorf(\"invalid duplicate-resolution '%s', please choose valid option between ['record', 'none', 'remove']\", s)\n\t}\n\treturn nil\n}\n\nfunc (dra *DuplicateResolutionAlgorithm) MarshalJSON() ([]byte, error) {\n\treturn []byte(`\"` + dra.String() + `\"`), nil\n}\n\nfunc (dra *DuplicateResolutionAlgorithm) UnmarshalJSON(data []byte) error {\n\treturn dra.FromStringValue(strings.Trim(string(data), `\"`))\n}\n\nfunc (dra DuplicateResolutionAlgorithm) String() string {\n\tswitch dra {\n\tcase DupeResAlgRecord:\n\t\treturn \"record\"\n\tcase DupeResAlgNone:\n\t\treturn \"none\"\n\tcase DupeResAlgRemove:\n\t\treturn \"remove\"\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"invalid duplicate-resolution type '%d'\", dra))\n\t}\n}\n\n// PostRestore has some options which will be executed after kv restored.\ntype PostRestore struct {\n\tChecksum          PostOpLevel `toml:\"checksum\" json:\"checksum\"`\n\tAnalyze           PostOpLevel `toml:\"analyze\" json:\"analyze\"`\n\tLevel1Compact     bool        `toml:\"level-1-compact\" json:\"level-1-compact\"`\n\tPostProcessAtLast bool        `toml:\"post-process-at-last\" json:\"post-process-at-last\"`\n\tCompact           bool        `toml:\"compact\" json:\"compact\"`\n}\n\ntype CSVConfig struct {\n\t// Separator, Delimiter and Terminator should all be in utf8mb4 encoding.\n\tSeparator       string `toml:\"separator\" json:\"separator\"`\n\tDelimiter       string `toml:\"delimiter\" json:\"delimiter\"`\n\tTerminator      string `toml:\"terminator\" json:\"terminator\"`\n\tNull            string `toml:\"null\" json:\"null\"`\n\tHeader          bool   `toml:\"header\" json:\"header\"`\n\tTrimLastSep     bool   `toml:\"trim-last-separator\" json:\"trim-last-separator\"`\n\tNotNull         bool   `toml:\"not-null\" json:\"not-null\"`\n\tBackslashEscape bool   `toml:\"backslash-escape\" json:\"backslash-escape\"`\n}\n\ntype MydumperRuntime struct {\n\tReadBlockSize    ByteSize         `toml:\"read-block-size\" json:\"read-block-size\"`\n\tBatchSize        ByteSize         `toml:\"batch-size\" json:\"batch-size\"`\n\tBatchImportRatio float64          `toml:\"batch-import-ratio\" json:\"batch-import-ratio\"`\n\tSourceDir        string           `toml:\"data-source-dir\" json:\"data-source-dir\"`\n\tCharacterSet     string           `toml:\"character-set\" json:\"character-set\"`\n\tCSV              CSVConfig        `toml:\"csv\" json:\"csv\"`\n\tMaxRegionSize    ByteSize         `toml:\"max-region-size\" json:\"max-region-size\"`\n\tFilter           []string         `toml:\"filter\" json:\"filter\"`\n\tFileRouters      []*FileRouteRule `toml:\"files\" json:\"files\"`\n\t// Deprecated: only used to keep the compatibility.\n\tNoSchema         bool             `toml:\"no-schema\" json:\"no-schema\"`\n\tCaseSensitive    bool             `toml:\"case-sensitive\" json:\"case-sensitive\"`\n\tStrictFormat     bool             `toml:\"strict-format\" json:\"strict-format\"`\n\tDefaultFileRules bool             `toml:\"default-file-rules\" json:\"default-file-rules\"`\n\tIgnoreColumns    AllIgnoreColumns `toml:\"ignore-data-columns\" json:\"ignore-data-columns\"`\n\t// DataCharacterSet is the character set of the source file. Only CSV files are supported now. The following options are supported.\n\t//   - utf8mb4\n\t//   - GB18030\n\t//   - GBK: an extension of the GB2312 character set and is also known as Code Page 936.\n\t//   - binary: no attempt to convert the encoding.\n\t// Leave DataCharacterSet empty will make it use `binary` by default.\n\tDataCharacterSet string `toml:\"data-character-set\" json:\"data-character-set\"`\n\t// DataInvalidCharReplace is the replacement characters for non-compatible characters, which shouldn't duplicate with the separators or line breaks.\n\t// Changing the default value will result in increased parsing time. Non-compatible characters do not cause an increase in error.\n\tDataInvalidCharReplace string `toml:\"data-invalid-char-replace\" json:\"data-invalid-char-replace\"`\n}\n\ntype AllIgnoreColumns []*IgnoreColumns\n\ntype IgnoreColumns struct {\n\tDB          string   `toml:\"db\" json:\"db\"`\n\tTable       string   `toml:\"table\" json:\"table\"`\n\tTableFilter []string `toml:\"table-filter\" json:\"table-filter\"`\n\tColumns     []string `toml:\"columns\" json:\"columns\"`\n}\n\nfunc (ic *IgnoreColumns) ColumnsMap() map[string]struct{} {\n\tcolumnMap := make(map[string]struct{}, len(ic.Columns))\n\tfor _, c := range ic.Columns {\n\t\tcolumnMap[c] = struct{}{}\n\t}\n\treturn columnMap\n}\n\n// GetIgnoreColumns gets Ignore config by schema name/regex and table name/regex.\nfunc (igCols AllIgnoreColumns) GetIgnoreColumns(db string, table string, caseSensitive bool) (*IgnoreColumns, error) {\n\tif !caseSensitive {\n\t\tdb = strings.ToLower(db)\n\t\ttable = strings.ToLower(table)\n\t}\n\tfor i, ig := range igCols {\n\t\tif ig.DB == db && ig.Table == table {\n\t\t\treturn igCols[i], nil\n\t\t}\n\t\tf, err := filter.Parse(ig.TableFilter)\n\t\tif err != nil {\n\t\t\treturn nil, common.ErrInvalidConfig.GenWithStack(\"invalid table filter %s in ignore columns\", strings.Join(ig.TableFilter, \",\"))\n\t\t}\n\t\tif f.MatchTable(db, table) {\n\t\t\treturn igCols[i], nil\n\t\t}\n\t}\n\treturn &IgnoreColumns{Columns: make([]string, 0)}, nil\n}\n\ntype FileRouteRule struct {\n\tPattern     string `json:\"pattern\" toml:\"pattern\" yaml:\"pattern\"`\n\tPath        string `json:\"path\" toml:\"path\" yaml:\"path\"`\n\tSchema      string `json:\"schema\" toml:\"schema\" yaml:\"schema\"`\n\tTable       string `json:\"table\" toml:\"table\" yaml:\"table\"`\n\tType        string `json:\"type\" toml:\"type\" yaml:\"type\"`\n\tKey         string `json:\"key\" toml:\"key\" yaml:\"key\"`\n\tCompression string `json:\"compression\" toml:\"compression\" yaml:\"compression\"`\n\t// unescape the schema/table name only used in lightning's internal logic now.\n\tUnescape bool `json:\"-\" toml:\"-\" yaml:\"-\"`\n\t// TODO: DataCharacterSet here can override the same field in [mydumper.csv] with a higher level.\n\t// This could provide users a more flexible usage to configure different files with\n\t// different data charsets.\n\t// DataCharacterSet string `toml:\"data-character-set\" json:\"data-character-set\"`\n}\n\ntype TikvImporter struct {\n\t// Deprecated: only used to keep the compatibility.\n\tAddr                string                       `toml:\"addr\" json:\"addr\"`\n\tBackend             string                       `toml:\"backend\" json:\"backend\"`\n\tOnDuplicate         string                       `toml:\"on-duplicate\" json:\"on-duplicate\"`\n\tMaxKVPairs          int                          `toml:\"max-kv-pairs\" json:\"max-kv-pairs\"`\n\tSendKVPairs         int                          `toml:\"send-kv-pairs\" json:\"send-kv-pairs\"`\n\tRegionSplitSize     ByteSize                     `toml:\"region-split-size\" json:\"region-split-size\"`\n\tRegionSplitKeys     int                          `toml:\"region-split-keys\" json:\"region-split-keys\"`\n\tSortedKVDir         string                       `toml:\"sorted-kv-dir\" json:\"sorted-kv-dir\"`\n\tDiskQuota           ByteSize                     `toml:\"disk-quota\" json:\"disk-quota\"`\n\tRangeConcurrency    int                          `toml:\"range-concurrency\" json:\"range-concurrency\"`\n\tDuplicateResolution DuplicateResolutionAlgorithm `toml:\"duplicate-resolution\" json:\"duplicate-resolution\"`\n\tIncrementalImport   bool                         `toml:\"incremental-import\" json:\"incremental-import\"`\n\n\tEngineMemCacheSize      ByteSize `toml:\"engine-mem-cache-size\" json:\"engine-mem-cache-size\"`\n\tLocalWriterMemCacheSize ByteSize `toml:\"local-writer-mem-cache-size\" json:\"local-writer-mem-cache-size\"`\n\tStoreWriteBWLimit       ByteSize `toml:\"store-write-bwlimit\" json:\"store-write-bwlimit\"`\n}\n\ntype Checkpoint struct {\n\tSchema           string                    `toml:\"schema\" json:\"schema\"`\n\tDSN              string                    `toml:\"dsn\" json:\"-\"` // DSN may contain password, don't expose this to JSON.\n\tMySQLParam       *common.MySQLConnectParam `toml:\"-\" json:\"-\"`   // For some security reason, we use MySQLParam instead of DSN.\n\tDriver           string                    `toml:\"driver\" json:\"driver\"`\n\tEnable           bool                      `toml:\"enable\" json:\"enable\"`\n\tKeepAfterSuccess CheckpointKeepStrategy    `toml:\"keep-after-success\" json:\"keep-after-success\"`\n}\n\ntype Cron struct {\n\tSwitchMode     Duration `toml:\"switch-mode\" json:\"switch-mode\"`\n\tLogProgress    Duration `toml:\"log-progress\" json:\"log-progress\"`\n\tCheckDiskQuota Duration `toml:\"check-disk-quota\" json:\"check-disk-quota\"`\n}\n\ntype Security struct {\n\tCAPath   string `toml:\"ca-path\" json:\"ca-path\"`\n\tCertPath string `toml:\"cert-path\" json:\"cert-path\"`\n\tKeyPath  string `toml:\"key-path\" json:\"key-path\"`\n\t// RedactInfoLog indicates that whether enabling redact log\n\tRedactInfoLog bool `toml:\"redact-info-log\" json:\"redact-info-log\"`\n\n\t// TLSConfigName is used to set tls config for lightning in DM, so we don't expose this field to user\n\t// DM may running many lightning instances at same time, so we need to set different tls config name for each lightning\n\tTLSConfigName string `toml:\"-\" json:\"-\"`\n\n\t// When DM/engine uses lightning as a library, it can directly pass in the content\n\tCABytes   []byte `toml:\"-\" json:\"-\"`\n\tCertBytes []byte `toml:\"-\" json:\"-\"`\n\tKeyBytes  []byte `toml:\"-\" json:\"-\"`\n}\n\n// RegisterMySQL registers the TLS config with name \"cluster\" or security.TLSConfigName\n// for use in `sql.Open()`. This method is goroutine-safe.\nfunc (sec *Security) RegisterMySQL() error {\n\tif sec == nil {\n\t\treturn nil\n\t}\n\n\ttlsConfig, err := util.NewTLSConfig(\n\t\tutil.WithCAPath(sec.CAPath),\n\t\tutil.WithCertAndKeyPath(sec.CertPath, sec.KeyPath),\n\t\tutil.WithCAContent(sec.CABytes),\n\t\tutil.WithCertAndKeyContent(sec.CertBytes, sec.KeyBytes),\n\t)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tif tlsConfig != nil {\n\t\t// error happens only when the key coincides with the built-in names.\n\t\t_ = gomysql.RegisterTLSConfig(sec.TLSConfigName, tlsConfig)\n\t}\n\treturn nil\n}\n\n// DeregisterMySQL deregisters the TLS config with security.TLSConfigName\nfunc (sec *Security) DeregisterMySQL() {\n\tif sec == nil || len(sec.CAPath) == 0 {\n\t\treturn\n\t}\n\tgomysql.DeregisterTLSConfig(sec.TLSConfigName)\n}\n\n// A duration which can be deserialized from a TOML string.\n// Implemented as https://github.com/BurntSushi/toml#using-the-encodingtextunmarshaler-interface\ntype Duration struct {\n\ttime.Duration\n}\n\nfunc (d *Duration) UnmarshalText(text []byte) error {\n\tvar err error\n\td.Duration, err = time.ParseDuration(string(text))\n\treturn errors.Trace(err)\n}\n\nfunc (d Duration) MarshalText() ([]byte, error) {\n\treturn []byte(d.String()), nil\n}\n\nfunc (d *Duration) MarshalJSON() ([]byte, error) {\n\treturn []byte(fmt.Sprintf(`\"%s\"`, d.Duration)), nil\n}\n\n// Charset defines character set\ntype Charset int\n\nconst (\n\tBinary Charset = iota\n\tUTF8MB4\n\tGB18030\n\tGBK\n)\n\n// String return the string value of charset\nfunc (c Charset) String() string {\n\tswitch c {\n\tcase Binary:\n\t\treturn \"binary\"\n\tcase UTF8MB4:\n\t\treturn \"utf8mb4\"\n\tcase GB18030:\n\t\treturn \"gb18030\"\n\tcase GBK:\n\t\treturn \"gbk\"\n\tdefault:\n\t\treturn \"unknown_charset\"\n\t}\n}\n\n// ParseCharset parser character set for string\nfunc ParseCharset(dataCharacterSet string) (Charset, error) {\n\tswitch strings.ToLower(dataCharacterSet) {\n\tcase \"\", \"binary\":\n\t\treturn Binary, nil\n\tcase \"utf8mb4\":\n\t\treturn UTF8MB4, nil\n\tcase \"gb18030\":\n\t\treturn GB18030, nil\n\tcase \"gbk\":\n\t\treturn GBK, nil\n\tdefault:\n\t\treturn Binary, errors.Errorf(\"found unsupported data-character-set: %s\", dataCharacterSet)\n\t}\n}\n\nfunc NewConfig() *Config {\n\treturn &Config{\n\t\tApp: Lightning{\n\t\t\tRegionConcurrency: runtime.NumCPU(),\n\t\t\tTableConcurrency:  0,\n\t\t\tIndexConcurrency:  0,\n\t\t\tIOConcurrency:     5,\n\t\t\tCheckRequirements: true,\n\t\t\tMaxError: MaxError{\n\t\t\t\tCharset:  *atomic.NewInt64(math.MaxInt64),\n\t\t\t\tConflict: *atomic.NewInt64(math.MaxInt64),\n\t\t\t},\n\t\t\tTaskInfoSchemaName: defaultTaskInfoSchemaName,\n\t\t},\n\t\tCheckpoint: Checkpoint{\n\t\t\tEnable: true,\n\t\t},\n\t\tTiDB: DBStore{\n\t\t\tHost:                       \"127.0.0.1\",\n\t\t\tUser:                       \"root\",\n\t\t\tStatusPort:                 10080,\n\t\t\tStrSQLMode:                 \"ONLY_FULL_GROUP_BY,NO_AUTO_CREATE_USER\",\n\t\t\tMaxAllowedPacket:           defaultMaxAllowedPacket,\n\t\t\tBuildStatsConcurrency:      defaultBuildStatsConcurrency,\n\t\t\tDistSQLScanConcurrency:     defaultDistSQLScanConcurrency,\n\t\t\tIndexSerialScanConcurrency: defaultIndexSerialScanConcurrency,\n\t\t\tChecksumTableConcurrency:   defaultChecksumTableConcurrency,\n\t\t},\n\t\tCron: Cron{\n\t\t\tSwitchMode:     Duration{Duration: 5 * time.Minute},\n\t\t\tLogProgress:    Duration{Duration: 5 * time.Minute},\n\t\t\tCheckDiskQuota: Duration{Duration: 1 * time.Minute},\n\t\t},\n\t\tMydumper: MydumperRuntime{\n\t\t\tReadBlockSize: ReadBlockSize,\n\t\t\tCSV: CSVConfig{\n\t\t\t\tSeparator:       \",\",\n\t\t\t\tDelimiter:       `\"`,\n\t\t\t\tHeader:          true,\n\t\t\t\tNotNull:         false,\n\t\t\t\tNull:            `\\N`,\n\t\t\t\tBackslashEscape: true,\n\t\t\t\tTrimLastSep:     false,\n\t\t\t},\n\t\t\tStrictFormat:           false,\n\t\t\tMaxRegionSize:          MaxRegionSize,\n\t\t\tFilter:                 GetDefaultFilter(),\n\t\t\tDataCharacterSet:       defaultCSVDataCharacterSet,\n\t\t\tDataInvalidCharReplace: string(defaultCSVDataInvalidCharReplace),\n\t\t},\n\t\tTikvImporter: TikvImporter{\n\t\t\tBackend:             \"\",\n\t\t\tOnDuplicate:         ReplaceOnDup,\n\t\t\tMaxKVPairs:          4096,\n\t\t\tSendKVPairs:         32768,\n\t\t\tRegionSplitSize:     0,\n\t\t\tDiskQuota:           ByteSize(math.MaxInt64),\n\t\t\tDuplicateResolution: DupeResAlgNone,\n\t\t},\n\t\tPostRestore: PostRestore{\n\t\t\tChecksum:          OpLevelRequired,\n\t\t\tAnalyze:           OpLevelOptional,\n\t\t\tPostProcessAtLast: true,\n\t\t},\n\t}\n}\n\n// LoadFromGlobal resets the current configuration to the global settings.\nfunc (cfg *Config) LoadFromGlobal(global *GlobalConfig) error {\n\tif err := cfg.LoadFromTOML(global.ConfigFileContent); err != nil {\n\t\treturn err\n\t}\n\n\tcfg.TiDB.Host = global.TiDB.Host\n\tcfg.TiDB.Port = global.TiDB.Port\n\tcfg.TiDB.User = global.TiDB.User\n\tcfg.TiDB.Psw = global.TiDB.Psw\n\tcfg.TiDB.StatusPort = global.TiDB.StatusPort\n\tcfg.TiDB.PdAddr = global.TiDB.PdAddr\n\tcfg.Mydumper.NoSchema = global.Mydumper.NoSchema\n\tcfg.Mydumper.SourceDir = global.Mydumper.SourceDir\n\tcfg.Mydumper.Filter = global.Mydumper.Filter\n\tcfg.TikvImporter.Backend = global.TikvImporter.Backend\n\tcfg.TikvImporter.SortedKVDir = global.TikvImporter.SortedKVDir\n\tcfg.Checkpoint.Enable = global.Checkpoint.Enable\n\tcfg.PostRestore.Checksum = global.PostRestore.Checksum\n\tcfg.PostRestore.Analyze = global.PostRestore.Analyze\n\tcfg.App.CheckRequirements = global.App.CheckRequirements\n\tcfg.Security = global.Security\n\tcfg.Mydumper.IgnoreColumns = global.Mydumper.IgnoreColumns\n\treturn nil\n}\n\n// LoadFromTOML overwrites the current configuration by the TOML data\n// If data contains toml items not in Config and GlobalConfig, return an error\n// If data contains toml items not in Config, thus won't take effect, warn user\nfunc (cfg *Config) LoadFromTOML(data []byte) error {\n\t// bothUnused saves toml items not belong to Config nor GlobalConfig\n\tvar bothUnused []string\n\t// warnItems saves legal toml items but won't effect\n\tvar warnItems []string\n\n\tdataStr := string(data)\n\n\t// Here we load toml into cfg, and rest logic is check unused keys\n\tmetaData, err := toml.Decode(dataStr, cfg)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tunusedConfigKeys := metaData.Undecoded()\n\tif len(unusedConfigKeys) == 0 {\n\t\treturn nil\n\t}\n\n\t// Now we deal with potential both-unused keys of Config and GlobalConfig struct\n\n\tmetaDataGlobal, err := toml.Decode(dataStr, &GlobalConfig{})\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\t// Key type returned by metadata.Undecoded doesn't have a equality comparison,\n\t// we convert them to string type instead, and this conversion is identical\n\tunusedGlobalKeys := metaDataGlobal.Undecoded()\n\tunusedGlobalKeyStrs := make(map[string]struct{})\n\tfor _, key := range unusedGlobalKeys {\n\t\tunusedGlobalKeyStrs[key.String()] = struct{}{}\n\t}\n\n\tfor _, key := range unusedConfigKeys {\n\t\tkeyStr := key.String()\n\t\tif _, found := unusedGlobalKeyStrs[keyStr]; found {\n\t\t\tbothUnused = append(bothUnused, keyStr)\n\t\t} else {\n\t\t\twarnItems = append(warnItems, keyStr)\n\t\t}\n\t}\n\n\tif len(bothUnused) > 0 {\n\t\treturn errors.Errorf(\"config file contained unknown configuration options: %s\",\n\t\t\tstrings.Join(bothUnused, \", \"))\n\t}\n\n\t// Warn that some legal field of config file won't be overwritten, such as lightning.file\n\tif len(warnItems) > 0 {\n\t\tlog.L().Warn(\"currently only per-task configuration can be applied, global configuration changes can only be made on startup\",\n\t\t\tzap.Strings(\"global config changes\", warnItems))\n\t}\n\n\treturn nil\n}\n\n// Adjust fixes the invalid or unspecified settings to reasonable valid values.\nfunc (cfg *Config) Adjust(ctx context.Context) error {\n\t// Reject problematic CSV configurations.\n\tcsv := &cfg.Mydumper.CSV\n\tif len(csv.Separator) == 0 {\n\t\treturn common.ErrInvalidConfig.GenWithStack(\"`mydumper.csv.separator` must not be empty\")\n\t}\n\n\tif len(csv.Delimiter) > 0 && (strings.HasPrefix(csv.Separator, csv.Delimiter) || strings.HasPrefix(csv.Delimiter, csv.Separator)) {\n\t\treturn common.ErrInvalidConfig.GenWithStack(\"`mydumper.csv.separator` and `mydumper.csv.delimiter` must not be prefix of each other\")\n\t}\n\n\tif csv.BackslashEscape {\n\t\tif csv.Separator == `\\` {\n\t\t\treturn common.ErrInvalidConfig.GenWithStack(\"cannot use '\\\\' as CSV separator when `mydumper.csv.backslash-escape` is true\")\n\t\t}\n\t\tif csv.Delimiter == `\\` {\n\t\t\treturn common.ErrInvalidConfig.GenWithStack(\"cannot use '\\\\' as CSV delimiter when `mydumper.csv.backslash-escape` is true\")\n\t\t}\n\t\tif csv.Terminator == `\\` {\n\t\t\treturn common.ErrInvalidConfig.GenWithStack(\"cannot use '\\\\' as CSV terminator when `mydumper.csv.backslash-escape` is true\")\n\t\t}\n\t}\n\n\t// adjust file routing\n\tfor _, rule := range cfg.Mydumper.FileRouters {\n\t\tif filepath.IsAbs(rule.Path) {\n\t\t\trelPath, err := filepath.Rel(cfg.Mydumper.SourceDir, rule.Path)\n\t\t\tif err != nil {\n\t\t\t\treturn common.ErrInvalidConfig.Wrap(err).\n\t\t\t\t\tGenWithStack(\"cannot find relative path for file route path %s\", rule.Path)\n\t\t\t}\n\t\t\t// \"..\" means that this path is not in source dir, so we should return an error\n\t\t\tif strings.HasPrefix(relPath, \"..\") {\n\t\t\t\treturn common.ErrInvalidConfig.GenWithStack(\n\t\t\t\t\t\"file route path '%s' is not in source dir '%s'\", rule.Path, cfg.Mydumper.SourceDir)\n\t\t\t}\n\t\t\trule.Path = relPath\n\t\t}\n\t}\n\n\t// enable default file route rule if no rules are set\n\tif len(cfg.Mydumper.FileRouters) == 0 {\n\t\tcfg.Mydumper.DefaultFileRules = true\n\t}\n\n\tif len(cfg.Mydumper.DataCharacterSet) == 0 {\n\t\tcfg.Mydumper.DataCharacterSet = defaultCSVDataCharacterSet\n\t}\n\tcharset, err1 := ParseCharset(cfg.Mydumper.DataCharacterSet)\n\tif err1 != nil {\n\t\treturn common.ErrInvalidConfig.Wrap(err1).GenWithStack(\"invalid `mydumper.data-character-set`\")\n\t}\n\tif charset == GBK || charset == GB18030 {\n\t\tlog.L().Warn(\n\t\t\t\"incompatible strings may be encountered during the transcoding process and will be replaced, please be aware of the risk of not being able to retain the original information\",\n\t\t\tzap.String(\"source-character-set\", charset.String()),\n\t\t\tzap.ByteString(\"invalid-char-replacement\", []byte(cfg.Mydumper.DataInvalidCharReplace)))\n\t}\n\n\tmustHaveInternalConnections, err := cfg.AdjustCommon()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// mydumper.filter and black-white-list cannot co-exist.\n\tif cfg.HasLegacyBlackWhiteList() {\n\t\tlog.L().Warn(\"the config `black-white-list` has been deprecated, please replace with `mydumper.filter`\")\n\t\tif !common.StringSliceEqual(cfg.Mydumper.Filter, defaultFilter) {\n\t\t\treturn common.ErrInvalidConfig.GenWithStack(\"`mydumper.filter` and `black-white-list` cannot be simultaneously defined\")\n\t\t}\n\t}\n\n\tfor _, rule := range cfg.Routes {\n\t\tif !cfg.Mydumper.CaseSensitive {\n\t\t\trule.ToLower()\n\t\t}\n\t\tif err := rule.Valid(); err != nil {\n\t\t\treturn common.ErrInvalidConfig.Wrap(err).GenWithStack(\"file route rule is invalid\")\n\t\t}\n\t}\n\n\tif err := cfg.CheckAndAdjustTiDBPort(ctx, mustHaveInternalConnections); err != nil {\n\t\treturn err\n\t}\n\tcfg.AdjustMydumper()\n\tcfg.AdjustCheckPoint()\n\treturn cfg.CheckAndAdjustFilePath()\n}\n\nfunc (cfg *Config) AdjustCommon() (bool, error) {\n\tif cfg.TikvImporter.Backend == \"\" {\n\t\treturn false, common.ErrInvalidConfig.GenWithStack(\"tikv-importer.backend must not be empty!\")\n\t}\n\tcfg.TikvImporter.Backend = strings.ToLower(cfg.TikvImporter.Backend)\n\tmustHaveInternalConnections := true\n\tswitch cfg.TikvImporter.Backend {\n\tcase BackendTiDB:\n\t\tcfg.DefaultVarsForTiDBBackend()\n\t\tmustHaveInternalConnections = false\n\t\tcfg.PostRestore.Checksum = OpLevelOff\n\t\tcfg.PostRestore.Analyze = OpLevelOff\n\t\tcfg.PostRestore.Compact = false\n\tcase BackendLocal:\n\t\t// RegionConcurrency > NumCPU is meaningless.\n\t\tcpuCount := runtime.NumCPU()\n\t\tif cfg.App.RegionConcurrency > cpuCount {\n\t\t\tcfg.App.RegionConcurrency = cpuCount\n\t\t}\n\t\tcfg.DefaultVarsForImporterAndLocalBackend()\n\tdefault:\n\t\treturn mustHaveInternalConnections, common.ErrInvalidConfig.GenWithStack(\"unsupported `tikv-importer.backend` (%s)\", cfg.TikvImporter.Backend)\n\t}\n\n\t// TODO calculate these from the machine's free memory.\n\tif cfg.TikvImporter.EngineMemCacheSize == 0 {\n\t\tcfg.TikvImporter.EngineMemCacheSize = defaultEngineMemCacheSize\n\t}\n\tif cfg.TikvImporter.LocalWriterMemCacheSize == 0 {\n\t\tcfg.TikvImporter.LocalWriterMemCacheSize = defaultLocalWriterMemCacheSize\n\t}\n\n\tif cfg.TikvImporter.Backend == BackendLocal {\n\t\tif err := cfg.CheckAndAdjustForLocalBackend(); err != nil {\n\t\t\treturn mustHaveInternalConnections, err\n\t\t}\n\t} else {\n\t\tcfg.TikvImporter.DuplicateResolution = DupeResAlgNone\n\t}\n\n\tif cfg.TikvImporter.Backend == BackendTiDB {\n\t\tcfg.TikvImporter.OnDuplicate = strings.ToLower(cfg.TikvImporter.OnDuplicate)\n\t\tswitch cfg.TikvImporter.OnDuplicate {\n\t\tcase ReplaceOnDup, IgnoreOnDup, ErrorOnDup:\n\t\tdefault:\n\t\t\treturn mustHaveInternalConnections, common.ErrInvalidConfig.GenWithStack(\n\t\t\t\t\"unsupported `tikv-importer.on-duplicate` (%s)\", cfg.TikvImporter.OnDuplicate)\n\t\t}\n\t}\n\n\tvar err error\n\tcfg.TiDB.SQLMode, err = mysql.GetSQLMode(cfg.TiDB.StrSQLMode)\n\tif err != nil {\n\t\treturn mustHaveInternalConnections, common.ErrInvalidConfig.Wrap(err).GenWithStack(\"`mydumper.tidb.sql_mode` must be a valid SQL_MODE\")\n\t}\n\n\tif err := cfg.CheckAndAdjustSecurity(); err != nil {\n\t\treturn mustHaveInternalConnections, err\n\t}\n\treturn mustHaveInternalConnections, err\n}\n\nfunc (cfg *Config) CheckAndAdjustForLocalBackend() error {\n\tif len(cfg.TikvImporter.SortedKVDir) == 0 {\n\t\treturn common.ErrInvalidConfig.GenWithStack(\"tikv-importer.sorted-kv-dir must not be empty!\")\n\t}\n\n\tstorageSizeDir := filepath.Clean(cfg.TikvImporter.SortedKVDir)\n\tsortedKVDirInfo, err := os.Stat(storageSizeDir)\n\n\tswitch {\n\tcase os.IsNotExist(err):\n\t\treturn nil\n\tcase err == nil:\n\t\tif !sortedKVDirInfo.IsDir() {\n\t\t\treturn common.ErrInvalidConfig.\n\t\t\t\tGenWithStack(\"tikv-importer.sorted-kv-dir ('%s') is not a directory\", storageSizeDir)\n\t\t}\n\tdefault:\n\t\treturn common.ErrInvalidConfig.Wrap(err).GenWithStack(\"invalid tikv-importer.sorted-kv-dir\")\n\t}\n\n\treturn nil\n}\n\nfunc (cfg *Config) DefaultVarsForTiDBBackend() {\n\tif cfg.App.TableConcurrency == 0 {\n\t\tcfg.App.TableConcurrency = cfg.App.RegionConcurrency\n\t}\n\tif cfg.App.IndexConcurrency == 0 {\n\t\tcfg.App.IndexConcurrency = cfg.App.RegionConcurrency\n\t}\n}\n\nfunc (cfg *Config) DefaultVarsForImporterAndLocalBackend() {\n\tif cfg.App.IndexConcurrency == 0 {\n\t\tcfg.App.IndexConcurrency = defaultIndexConcurrency\n\t}\n\tif cfg.App.TableConcurrency == 0 {\n\t\tcfg.App.TableConcurrency = defaultTableConcurrency\n\t}\n\n\tif len(cfg.App.MetaSchemaName) == 0 {\n\t\tcfg.App.MetaSchemaName = defaultMetaSchemaName\n\t}\n\tif cfg.TikvImporter.RangeConcurrency == 0 {\n\t\tcfg.TikvImporter.RangeConcurrency = 16\n\t}\n\tif cfg.TiDB.BuildStatsConcurrency == 0 {\n\t\tcfg.TiDB.BuildStatsConcurrency = defaultBuildStatsConcurrency\n\t}\n\tif cfg.TiDB.IndexSerialScanConcurrency == 0 {\n\t\tcfg.TiDB.IndexSerialScanConcurrency = defaultIndexSerialScanConcurrency\n\t}\n\tif cfg.TiDB.ChecksumTableConcurrency == 0 {\n\t\tcfg.TiDB.ChecksumTableConcurrency = defaultChecksumTableConcurrency\n\t}\n}\n\nfunc (cfg *Config) CheckAndAdjustTiDBPort(ctx context.Context, mustHaveInternalConnections bool) error {\n\t// automatically determine the TiDB port & PD address from TiDB settings\n\tif mustHaveInternalConnections && (cfg.TiDB.Port <= 0 || len(cfg.TiDB.PdAddr) == 0) {\n\t\ttls, err := cfg.ToTLS()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tvar settings tidbcfg.Config\n\t\terr = tls.GetJSON(ctx, \"/settings\", &settings)\n\t\tif err != nil {\n\t\t\treturn common.ErrInvalidConfig.Wrap(err).GenWithStack(\"cannot fetch settings from TiDB, please manually fill in `tidb.port` and `tidb.pd-addr`\")\n\t\t}\n\t\tif cfg.TiDB.Port <= 0 {\n\t\t\tcfg.TiDB.Port = int(settings.Port)\n\t\t}\n\t\tif len(cfg.TiDB.PdAddr) == 0 {\n\t\t\tpdAddrs := strings.Split(settings.Path, \",\")\n\t\t\tcfg.TiDB.PdAddr = pdAddrs[0] // FIXME support multiple PDs once importer can.\n\t\t}\n\t}\n\n\tif cfg.TiDB.Port <= 0 {\n\t\treturn common.ErrInvalidConfig.GenWithStack(\"invalid `tidb.port` setting\")\n\t}\n\n\tif mustHaveInternalConnections && len(cfg.TiDB.PdAddr) == 0 {\n\t\treturn common.ErrInvalidConfig.GenWithStack(\"invalid `tidb.pd-addr` setting\")\n\t}\n\treturn nil\n}\n\nfunc (cfg *Config) CheckAndAdjustFilePath() error {\n\tvar u *url.URL\n\n\t// An absolute Windows path like \"C:\\Users\\XYZ\" would be interpreted as\n\t// an URL with scheme \"C\" and opaque data \"\\Users\\XYZ\".\n\t// Therefore, we only perform URL parsing if we are sure the path is not\n\t// an absolute Windows path.\n\t// Here we use the `filepath.VolumeName` which can identify the \"C:\" part\n\t// out of the path. On Linux this method always return an empty string.\n\t// On Windows, the drive letter can only be single letters from \"A:\" to \"Z:\",\n\t// so this won't mistake \"S3:\" as a Windows path.\n\tif len(filepath.VolumeName(cfg.Mydumper.SourceDir)) == 0 {\n\t\tvar err error\n\t\tu, err = url.Parse(cfg.Mydumper.SourceDir)\n\t\tif err != nil {\n\t\t\treturn common.ErrInvalidConfig.Wrap(err).GenWithStack(\"cannot parse `mydumper.data-source-dir` %s\", cfg.Mydumper.SourceDir)\n\t\t}\n\t} else {\n\t\tu = &url.URL{}\n\t}\n\n\t// convert path and relative path to a valid file url\n\tif u.Scheme == \"\" {\n\t\tif cfg.Mydumper.SourceDir == \"\" {\n\t\t\treturn common.ErrInvalidConfig.GenWithStack(\"`mydumper.data-source-dir` is not set\")\n\t\t}\n\t\tif !common.IsDirExists(cfg.Mydumper.SourceDir) {\n\t\t\treturn common.ErrInvalidConfig.GenWithStack(\"'%s': `mydumper.data-source-dir` does not exist\", cfg.Mydumper.SourceDir)\n\t\t}\n\t\tabsPath, err := filepath.Abs(cfg.Mydumper.SourceDir)\n\t\tif err != nil {\n\t\t\treturn common.ErrInvalidConfig.Wrap(err).GenWithStack(\"covert data-source-dir '%s' to absolute path failed\", cfg.Mydumper.SourceDir)\n\t\t}\n\t\tu.Path = filepath.ToSlash(absPath)\n\t\tu.Scheme = \"file\"\n\t\tcfg.Mydumper.SourceDir = u.String()\n\t}\n\n\tfound := false\n\tfor _, t := range supportedStorageTypes {\n\t\tif u.Scheme == t {\n\t\t\tfound = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !found {\n\t\treturn common.ErrInvalidConfig.GenWithStack(\n\t\t\t\"unsupported data-source-dir url '%s', supported storage types are %s\",\n\t\t\tcfg.Mydumper.SourceDir, strings.Join(supportedStorageTypes, \",\"))\n\t}\n\treturn nil\n}\n\nfunc (cfg *Config) AdjustCheckPoint() {\n\tif len(cfg.Checkpoint.Schema) == 0 {\n\t\tcfg.Checkpoint.Schema = \"tidb_lightning_checkpoint\"\n\t}\n\tif len(cfg.Checkpoint.Driver) == 0 {\n\t\tcfg.Checkpoint.Driver = CheckpointDriverFile\n\t}\n\tif len(cfg.Checkpoint.DSN) == 0 {\n\t\tswitch cfg.Checkpoint.Driver {\n\t\tcase CheckpointDriverMySQL:\n\t\t\tparam := common.MySQLConnectParam{\n\t\t\t\tHost:             cfg.TiDB.Host,\n\t\t\t\tPort:             cfg.TiDB.Port,\n\t\t\t\tUser:             cfg.TiDB.User,\n\t\t\t\tPassword:         cfg.TiDB.Psw,\n\t\t\t\tSQLMode:          mysql.DefaultSQLMode,\n\t\t\t\tMaxAllowedPacket: defaultMaxAllowedPacket,\n\t\t\t\tTLS:              cfg.TiDB.TLS,\n\t\t\t}\n\t\t\tcfg.Checkpoint.MySQLParam = &param\n\t\tcase CheckpointDriverFile:\n\t\t\tcfg.Checkpoint.DSN = \"/tmp/\" + cfg.Checkpoint.Schema + \".pb\"\n\t\t}\n\t}\n}\n\nfunc (cfg *Config) AdjustMydumper() {\n\tif cfg.Mydumper.BatchImportRatio < 0.0 || cfg.Mydumper.BatchImportRatio >= 1.0 {\n\t\tcfg.Mydumper.BatchImportRatio = 0.75\n\t}\n\tif cfg.Mydumper.ReadBlockSize <= 0 {\n\t\tcfg.Mydumper.ReadBlockSize = ReadBlockSize\n\t}\n\tif len(cfg.Mydumper.CharacterSet) == 0 {\n\t\tcfg.Mydumper.CharacterSet = \"auto\"\n\t}\n\n\tif len(cfg.Mydumper.IgnoreColumns) != 0 {\n\t\t// Tolower columns cause we use Name.L to compare column in tidb.\n\t\tfor _, ig := range cfg.Mydumper.IgnoreColumns {\n\t\t\tcols := make([]string, len(ig.Columns))\n\t\t\tfor i, col := range ig.Columns {\n\t\t\t\tcols[i] = strings.ToLower(col)\n\t\t\t}\n\t\t\tig.Columns = cols\n\t\t}\n\t}\n}\n\nfunc (cfg *Config) CheckAndAdjustSecurity() error {\n\tif cfg.TiDB.Security == nil {\n\t\tcfg.TiDB.Security = &cfg.Security\n\t}\n\n\tswitch cfg.TiDB.TLS {\n\tcase \"\":\n\t\tif len(cfg.TiDB.Security.CAPath) > 0 || len(cfg.TiDB.Security.CABytes) > 0 ||\n\t\t\tlen(cfg.TiDB.Security.CertPath) > 0 || len(cfg.TiDB.Security.CertBytes) > 0 ||\n\t\t\tlen(cfg.TiDB.Security.KeyPath) > 0 || len(cfg.TiDB.Security.KeyBytes) > 0 {\n\t\t\tif cfg.TiDB.Security.TLSConfigName == \"\" {\n\t\t\t\tcfg.TiDB.Security.TLSConfigName = uuid.NewString() // adjust this the default value\n\t\t\t}\n\t\t\tcfg.TiDB.TLS = cfg.TiDB.Security.TLSConfigName\n\t\t} else {\n\t\t\tcfg.TiDB.TLS = \"false\"\n\t\t}\n\tcase \"cluster\":\n\t\tif len(cfg.Security.CAPath) == 0 {\n\t\t\treturn common.ErrInvalidConfig.GenWithStack(\"cannot set `tidb.tls` to 'cluster' without a [security] section\")\n\t\t}\n\tcase \"false\", \"skip-verify\", \"preferred\":\n\t\treturn nil\n\tdefault:\n\t\treturn common.ErrInvalidConfig.GenWithStack(\"unsupported `tidb.tls` config %s\", cfg.TiDB.TLS)\n\t}\n\treturn nil\n}\n\n// HasLegacyBlackWhiteList checks whether the deprecated [black-white-list] section\n// was defined.\nfunc (cfg *Config) HasLegacyBlackWhiteList() bool {\n\treturn len(cfg.BWList.DoTables) != 0 || len(cfg.BWList.DoDBs) != 0 || len(cfg.BWList.IgnoreTables) != 0 || len(cfg.BWList.IgnoreDBs) != 0\n}\n", "// Copyright 2019 PingCAP, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage config_test\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"flag\"\n\t\"fmt\"\n\t\"net\"\n\t\"net/http\"\n\t\"net/http/httptest\"\n\t\"net/url\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/BurntSushi/toml\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/common\"\n\t\"github.com/pingcap/tidb/br/pkg/lightning/config\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc startMockServer(t *testing.T, statusCode int, content string) (*httptest.Server, string, int) {\n\tts := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tw.WriteHeader(statusCode)\n\t\t_, _ = fmt.Fprint(w, content)\n\t}))\n\n\turl, err := url.Parse(ts.URL)\n\trequire.NoError(t, err)\n\thost, portString, err := net.SplitHostPort(url.Host)\n\trequire.NoError(t, err)\n\tport, err := strconv.Atoi(portString)\n\trequire.NoError(t, err)\n\n\treturn ts, host, port\n}\n\nfunc assignMinimalLegalValue(cfg *config.Config) {\n\tcfg.TiDB.Host = \"123.45.67.89\"\n\tcfg.TiDB.Port = 4567\n\tcfg.TiDB.StatusPort = 8901\n\tcfg.TiDB.PdAddr = \"234.56.78.90:12345\"\n\tcfg.Mydumper.SourceDir = \"file://.\"\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.SortedKVDir = \".\"\n\tcfg.TikvImporter.DiskQuota = 1\n}\n\nfunc TestAdjustPdAddrAndPort(t *testing.T) {\n\tts, host, port := startMockServer(t, http.StatusOK,\n\t\t`{\"port\":4444,\"advertise-address\":\"\",\"path\":\"123.45.67.89:1234,56.78.90.12:3456\"}`,\n\t)\n\tdefer ts.Close()\n\n\tcfg := config.NewConfig()\n\tcfg.TiDB.Host = host\n\tcfg.TiDB.StatusPort = port\n\tcfg.Mydumper.SourceDir = \".\"\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.SortedKVDir = \".\"\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\n\terr := cfg.Adjust(context.Background())\n\trequire.NoError(t, err)\n\trequire.Equal(t, 4444, cfg.TiDB.Port)\n\trequire.Equal(t, \"123.45.67.89:1234\", cfg.TiDB.PdAddr)\n}\n\nfunc TestAdjustPdAddrAndPortViaAdvertiseAddr(t *testing.T) {\n\tts, host, port := startMockServer(t, http.StatusOK,\n\t\t`{\"port\":6666,\"advertise-address\":\"121.212.121.212:5555\",\"path\":\"34.34.34.34:3434\"}`,\n\t)\n\tdefer ts.Close()\n\n\tcfg := config.NewConfig()\n\tcfg.TiDB.Host = host\n\tcfg.TiDB.StatusPort = port\n\tcfg.Mydumper.SourceDir = \".\"\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.SortedKVDir = \".\"\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\n\terr := cfg.Adjust(context.Background())\n\trequire.NoError(t, err)\n\trequire.Equal(t, 6666, cfg.TiDB.Port)\n\trequire.Equal(t, \"34.34.34.34:3434\", cfg.TiDB.PdAddr)\n}\n\nfunc TestAdjustPageNotFound(t *testing.T) {\n\tts, host, port := startMockServer(t, http.StatusNotFound, \"{}\")\n\tdefer ts.Close()\n\n\tcfg := config.NewConfig()\n\tcfg.TiDB.Host = host\n\tcfg.TiDB.StatusPort = port\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.SortedKVDir = \".\"\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\n\terr := cfg.Adjust(context.Background())\n\trequire.Error(t, err)\n\trequire.Regexp(t, \"cannot fetch settings from TiDB.*\", err.Error())\n}\n\nfunc TestAdjustConnectRefused(t *testing.T) {\n\tts, host, port := startMockServer(t, http.StatusOK, \"{}\")\n\n\tcfg := config.NewConfig()\n\tcfg.TiDB.Host = host\n\tcfg.TiDB.StatusPort = port\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.SortedKVDir = \".\"\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\n\tts.Close() // immediately close to ensure connection refused.\n\n\terr := cfg.Adjust(context.Background())\n\trequire.Error(t, err)\n\trequire.Regexp(t, \"cannot fetch settings from TiDB.*\", err.Error())\n}\n\nfunc TestAdjustBackendNotSet(t *testing.T) {\n\tcfg := config.NewConfig()\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\terr := cfg.Adjust(context.Background())\n\trequire.EqualError(t, err, \"[Lightning:Config:ErrInvalidConfig]tikv-importer.backend must not be empty!\")\n}\n\nfunc TestAdjustInvalidBackend(t *testing.T) {\n\tcfg := config.NewConfig()\n\tcfg.TikvImporter.Backend = \"no_such_backend\"\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\terr := cfg.Adjust(context.Background())\n\trequire.EqualError(t, err, \"[Lightning:Config:ErrInvalidConfig]unsupported `tikv-importer.backend` (no_such_backend)\")\n}\n\nfunc TestCheckAndAdjustFilePath(t *testing.T) {\n\ttmpDir := t.TempDir()\n\t// use slashPath in url to be compatible with windows\n\tslashPath := filepath.ToSlash(tmpDir)\n\tpwd, err := os.Getwd()\n\trequire.NoError(t, err)\n\tspecialDir, err := os.MkdirTemp(tmpDir, \"abc??bcd\")\n\trequire.NoError(t, err)\n\tspecialDir1, err := os.MkdirTemp(tmpDir, \"abc%3F%3F%3Fbcd\")\n\trequire.NoError(t, err)\n\n\tcfg := config.NewConfig()\n\n\tcases := []struct {\n\t\ttest   string\n\t\texpect string\n\t}{\n\t\t{tmpDir, tmpDir},\n\t\t{\".\", filepath.ToSlash(pwd)},\n\t\t{specialDir, specialDir},\n\t\t{specialDir1, specialDir1},\n\t\t{\"file://\" + slashPath, slashPath},\n\t\t{\"local://\" + slashPath, slashPath},\n\t\t{\"s3://bucket_name\", \"\"},\n\t\t{\"s3://bucket_name/path/to/dir\", \"/path/to/dir\"},\n\t\t{\"gcs://bucketname/path/to/dir\", \"/path/to/dir\"},\n\t\t{\"gs://bucketname/path/to/dir\", \"/path/to/dir\"},\n\t\t{\"noop:///\", \"/\"},\n\t}\n\tfor _, testCase := range cases {\n\t\tcfg.Mydumper.SourceDir = testCase.test\n\t\terr = cfg.CheckAndAdjustFilePath()\n\t\trequire.NoError(t, err)\n\t\tu, err := url.Parse(cfg.Mydumper.SourceDir)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, testCase.expect, u.Path)\n\t}\n}\n\nfunc TestAdjustFileRoutePath(t *testing.T) {\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\n\tctx := context.Background()\n\ttmpDir := t.TempDir()\n\tcfg.Mydumper.SourceDir = tmpDir\n\tinvalidPath := filepath.Join(tmpDir, \"../test123/1.sql\")\n\trule := &config.FileRouteRule{Path: invalidPath, Type: \"sql\", Schema: \"test\", Table: \"tbl\"}\n\tcfg.Mydumper.FileRouters = []*config.FileRouteRule{rule}\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\terr := cfg.Adjust(ctx)\n\trequire.Error(t, err)\n\trequire.Regexp(t, fmt.Sprintf(\"\\\\Qfile route path '%s' is not in source dir '%s'\\\\E\", invalidPath, tmpDir), err.Error())\n\n\trelPath := filepath.FromSlash(\"test_dir/1.sql\")\n\trule.Path = filepath.Join(tmpDir, relPath)\n\terr = cfg.Adjust(ctx)\n\trequire.NoError(t, err)\n\trequire.Equal(t, relPath, cfg.Mydumper.FileRouters[0].Path)\n}\n\nfunc TestDecodeError(t *testing.T) {\n\tts, host, port := startMockServer(t, http.StatusOK, \"invalid-string\")\n\tdefer ts.Close()\n\n\tcfg := config.NewConfig()\n\tcfg.TiDB.Host = host\n\tcfg.TiDB.StatusPort = port\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.SortedKVDir = \".\"\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\n\terr := cfg.Adjust(context.Background())\n\trequire.Error(t, err)\n\trequire.Regexp(t, \"cannot fetch settings from TiDB.*\", err.Error())\n}\n\nfunc TestInvalidSetting(t *testing.T) {\n\tts, host, port := startMockServer(t, http.StatusOK, `{\"port\": 0}`)\n\tdefer ts.Close()\n\n\tcfg := config.NewConfig()\n\tcfg.TiDB.Host = host\n\tcfg.TiDB.StatusPort = port\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.SortedKVDir = \".\"\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\n\terr := cfg.Adjust(context.Background())\n\trequire.EqualError(t, err, \"[Lightning:Config:ErrInvalidConfig]invalid `tidb.port` setting\")\n}\n\nfunc TestInvalidPDAddr(t *testing.T) {\n\tts, host, port := startMockServer(t, http.StatusOK, `{\"port\": 1234, \"path\": \",,\"}`)\n\tdefer ts.Close()\n\n\tcfg := config.NewConfig()\n\tcfg.TiDB.Host = host\n\tcfg.TiDB.StatusPort = port\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.SortedKVDir = \".\"\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\n\terr := cfg.Adjust(context.Background())\n\trequire.EqualError(t, err, \"[Lightning:Config:ErrInvalidConfig]invalid `tidb.pd-addr` setting\")\n}\n\nfunc TestAdjustWillNotContactServerIfEverythingIsDefined(t *testing.T) {\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\n\terr := cfg.Adjust(context.Background())\n\trequire.NoError(t, err)\n\trequire.Equal(t, 4567, cfg.TiDB.Port)\n\trequire.Equal(t, \"234.56.78.90:12345\", cfg.TiDB.PdAddr)\n}\n\nfunc TestAdjustWillBatchImportRatioInvalid(t *testing.T) {\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\tcfg.Mydumper.BatchImportRatio = -1\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\terr := cfg.Adjust(context.Background())\n\trequire.NoError(t, err)\n\trequire.Equal(t, 0.75, cfg.Mydumper.BatchImportRatio)\n}\n\nfunc TestAdjustSecuritySection(t *testing.T) {\n\tuuidHolder := \"<uuid>\"\n\ttestCases := []struct {\n\t\tinput       string\n\t\texpectedCA  string\n\t\texpectedTLS string\n\t}{\n\t\t{\n\t\t\tinput:       ``,\n\t\t\texpectedCA:  \"\",\n\t\t\texpectedTLS: \"false\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[security]\n\t\t\t`,\n\t\t\texpectedCA:  \"\",\n\t\t\texpectedTLS: \"false\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[security]\n\t\t\t\tca-path = \"/path/to/ca.pem\"\n\t\t\t`,\n\t\t\texpectedCA:  \"/path/to/ca.pem\",\n\t\t\texpectedTLS: uuidHolder,\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[security]\n\t\t\t\tca-path = \"/path/to/ca.pem\"\n\t\t\t\t[tidb.security]\n\t\t\t`,\n\t\t\texpectedCA:  \"\",\n\t\t\texpectedTLS: \"false\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[security]\n\t\t\t\tca-path = \"/path/to/ca.pem\"\n\t\t\t\t[tidb.security]\n\t\t\t\tca-path = \"/path/to/ca2.pem\"\n\t\t\t`,\n\t\t\texpectedCA:  \"/path/to/ca2.pem\",\n\t\t\texpectedTLS: uuidHolder,\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[security]\n\t\t\t\t[tidb.security]\n\t\t\t\tca-path = \"/path/to/ca2.pem\"\n\t\t\t`,\n\t\t\texpectedCA:  \"/path/to/ca2.pem\",\n\t\t\texpectedTLS: uuidHolder,\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[security]\n\t\t\t\t[tidb]\n\t\t\t\ttls = \"skip-verify\"\n\t\t\t\t[tidb.security]\n\t\t\t`,\n\t\t\texpectedCA:  \"\",\n\t\t\texpectedTLS: \"skip-verify\",\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tcomment := fmt.Sprintf(\"input = %s\", tc.input)\n\n\t\tcfg := config.NewConfig()\n\t\tassignMinimalLegalValue(cfg)\n\t\tcfg.TiDB.DistSQLScanConcurrency = 1\n\t\terr := cfg.LoadFromTOML([]byte(tc.input))\n\t\trequire.NoError(t, err, comment)\n\n\t\terr = cfg.Adjust(context.Background())\n\t\trequire.NoError(t, err, comment)\n\t\trequire.Equal(t, tc.expectedCA, cfg.TiDB.Security.CAPath, comment)\n\t\tif tc.expectedTLS == uuidHolder {\n\t\t\trequire.NotEmpty(t, cfg.TiDB.TLS, comment)\n\t\t} else {\n\t\t\trequire.Equal(t, tc.expectedTLS, cfg.TiDB.TLS, comment)\n\t\t}\n\t}\n\t// test different tls config name\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\tcfg.Security.CAPath = \"/path/to/ca.pem\"\n\tcfg.Security.TLSConfigName = \"tidb-tls\"\n\trequire.NoError(t, cfg.Adjust(context.Background()))\n\trequire.Equal(t, cfg.TiDB.TLS, cfg.TiDB.Security.TLSConfigName)\n}\n\nfunc TestInvalidCSV(t *testing.T) {\n\ttestCases := []struct {\n\t\tinput string\n\t\terr   string\n\t}{\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tseparator = ''\n\t\t\t`,\n\t\t\terr: \"[Lightning:Config:ErrInvalidConfig]`mydumper.csv.separator` must not be empty\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tseparator = 'hello'\n\t\t\t\tdelimiter = 'hel'\n\t\t\t`,\n\t\t\terr: \"[Lightning:Config:ErrInvalidConfig]`mydumper.csv.separator` and `mydumper.csv.delimiter` must not be prefix of each other\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tseparator = 'hel'\n\t\t\t\tdelimiter = 'hello'\n\t\t\t`,\n\t\t\terr: \"[Lightning:Config:ErrInvalidConfig]`mydumper.csv.separator` and `mydumper.csv.delimiter` must not be prefix of each other\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tseparator = '\\'\n\t\t\t\tbackslash-escape = false\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tseparator = '\uff0c'\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tdelimiter = ''\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tdelimiter = 'hello'\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tdelimiter = '\\'\n\t\t\t\tbackslash-escape = false\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tseparator = '\\s'\n\t\t\t\tdelimiter = '\\d'\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tseparator = '|'\n\t\t\t\tdelimiter = '|'\n\t\t\t`,\n\t\t\terr: \"[Lightning:Config:ErrInvalidConfig]`mydumper.csv.separator` and `mydumper.csv.delimiter` must not be prefix of each other\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tseparator = '\\'\n\t\t\t\tbackslash-escape = true\n\t\t\t`,\n\t\t\terr: \"[Lightning:Config:ErrInvalidConfig]cannot use '\\\\' as CSV separator when `mydumper.csv.backslash-escape` is true\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper.csv]\n\t\t\t\tdelimiter = '\\'\n\t\t\t\tbackslash-escape = true\n\t\t\t`,\n\t\t\terr: \"[Lightning:Config:ErrInvalidConfig]cannot use '\\\\' as CSV delimiter when `mydumper.csv.backslash-escape` is true\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[tidb]\n\t\t\t\tsql-mode = \"invalid-sql-mode\"\n\t\t\t`,\n\t\t\terr: \"[Lightning:Config:ErrInvalidConfig]`mydumper.tidb.sql_mode` must be a valid SQL_MODE: ERROR 1231 (42000): Variable 'sql_mode' can't be set to the value of 'invalid-sql-mode'\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[[routes]]\n\t\t\t\tschema-pattern = \"\"\n\t\t\t\ttable-pattern = \"shard_table_*\"\n\t\t\t`,\n\t\t\terr: \"[Lightning:Config:ErrInvalidConfig]file route rule is invalid: schema pattern of table route rule should not be empty\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[[routes]]\n\t\t\t\tschema-pattern = \"schema_*\"\n\t\t\t\ttable-pattern = \"\"\n\t\t\t`,\n\t\t\terr: \"[Lightning:Config:ErrInvalidConfig]file route rule is invalid: target schema of table route rule should not be empty\",\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tcomment := fmt.Sprintf(\"input = %s\", tc.input)\n\t\tcfg := config.NewConfig()\n\t\tcfg.Mydumper.SourceDir = \"file://.\"\n\t\tcfg.TiDB.Port = 4000\n\t\tcfg.TiDB.PdAddr = \"test.invalid:2379\"\n\t\tcfg.TikvImporter.Backend = config.BackendLocal\n\t\tcfg.TikvImporter.SortedKVDir = \".\"\n\t\tcfg.TiDB.DistSQLScanConcurrency = 1\n\t\terr := cfg.LoadFromTOML([]byte(tc.input))\n\t\trequire.NoError(t, err)\n\n\t\terr = cfg.Adjust(context.Background())\n\t\tif tc.err != \"\" {\n\t\t\trequire.EqualError(t, err, tc.err, comment)\n\t\t} else {\n\t\t\trequire.NoError(t, err)\n\t\t}\n\t}\n}\n\nfunc TestInvalidTOML(t *testing.T) {\n\tcfg := &config.Config{}\n\terr := cfg.LoadFromTOML([]byte(`\n\t\tinvalid[mydumper.csv]\n\t\tdelimiter = '\\'\n\t\tbackslash-escape = true\n\t`))\n\trequire.EqualError(t, err, \"toml: line 2: expected '.' or '=', but got '[' instead\")\n}\n\nfunc TestTOMLUnusedKeys(t *testing.T) {\n\tcfg := &config.Config{}\n\terr := cfg.LoadFromTOML([]byte(`\n\t\t[lightning]\n\t\ttypo = 123\n\t`))\n\trequire.EqualError(t, err, \"config file contained unknown configuration options: lightning.typo\")\n}\n\nfunc TestDurationUnmarshal(t *testing.T) {\n\tduration := config.Duration{}\n\terr := duration.UnmarshalText([]byte(\"13m20s\"))\n\trequire.NoError(t, err)\n\trequire.Equal(t, 13*60+20.0, duration.Duration.Seconds())\n\terr = duration.UnmarshalText([]byte(\"13x20s\"))\n\trequire.Error(t, err)\n\trequire.Regexp(t, \"time: unknown unit .?x.? in duration .?13x20s.?\", err.Error())\n}\n\nfunc TestDurationMarshalJSON(t *testing.T) {\n\tduration := config.Duration{}\n\terr := duration.UnmarshalText([]byte(\"13m20s\"))\n\trequire.NoError(t, err)\n\trequire.Equal(t, 13*60+20.0, duration.Duration.Seconds())\n\tresult, err := duration.MarshalJSON()\n\trequire.NoError(t, err)\n\trequire.Equal(t, `\"13m20s\"`, string(result))\n}\n\nfunc TestDuplicateResolutionAlgorithm(t *testing.T) {\n\tvar dra config.DuplicateResolutionAlgorithm\n\trequire.NoError(t, dra.FromStringValue(\"record\"))\n\trequire.Equal(t, config.DupeResAlgRecord, dra)\n\trequire.NoError(t, dra.FromStringValue(\"none\"))\n\trequire.Equal(t, config.DupeResAlgNone, dra)\n\trequire.NoError(t, dra.FromStringValue(\"remove\"))\n\trequire.Equal(t, config.DupeResAlgRemove, dra)\n\n\trequire.Equal(t, \"record\", config.DupeResAlgRecord.String())\n\trequire.Equal(t, \"none\", config.DupeResAlgNone.String())\n\trequire.Equal(t, \"remove\", config.DupeResAlgRemove.String())\n}\n\nfunc TestLoadConfig(t *testing.T) {\n\tcfg, err := config.LoadGlobalConfig([]string{\"-tidb-port\", \"sss\"}, nil)\n\trequire.EqualError(t, err, `[Lightning:Common:ErrInvalidArgument]invalid argument: invalid value \"sss\" for flag -tidb-port: parse error`)\n\trequire.Nil(t, cfg)\n\n\tcfg, err = config.LoadGlobalConfig([]string{\"-V\"}, nil)\n\trequire.Equal(t, flag.ErrHelp, err)\n\trequire.Nil(t, cfg)\n\n\tcfg, err = config.LoadGlobalConfig([]string{\"-config\", \"not-exists\"}, nil)\n\trequire.Error(t, err)\n\trequire.Regexp(t, \".*(no such file or directory|The system cannot find the file specified).*\", err.Error())\n\trequire.Nil(t, cfg)\n\n\tcfg, err = config.LoadGlobalConfig([]string{\"--server-mode\"}, nil)\n\trequire.EqualError(t, err, \"[Lightning:Config:ErrInvalidConfig]If server-mode is enabled, the status-addr must be a valid listen address\")\n\trequire.Nil(t, cfg)\n\n\tpath, _ := filepath.Abs(\".\")\n\tcfg, err = config.LoadGlobalConfig([]string{\n\t\t\"-L\", \"debug\",\n\t\t\"-log-file\", \"/path/to/file.log\",\n\t\t\"-tidb-host\", \"172.16.30.11\",\n\t\t\"-tidb-port\", \"4001\",\n\t\t\"-tidb-user\", \"guest\",\n\t\t\"-tidb-password\", \"12345\",\n\t\t\"-pd-urls\", \"172.16.30.11:2379,172.16.30.12:2379\",\n\t\t\"-d\", path,\n\t\t\"-backend\", config.BackendLocal,\n\t\t\"-sorted-kv-dir\", \".\",\n\t\t\"-checksum=false\",\n\t}, nil)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"debug\", cfg.App.Config.Level)\n\trequire.Equal(t, \"/path/to/file.log\", cfg.App.Config.File)\n\trequire.Equal(t, \"172.16.30.11\", cfg.TiDB.Host)\n\trequire.Equal(t, 4001, cfg.TiDB.Port)\n\trequire.Equal(t, \"guest\", cfg.TiDB.User)\n\trequire.Equal(t, \"12345\", cfg.TiDB.Psw)\n\trequire.Equal(t, \"172.16.30.11:2379,172.16.30.12:2379\", cfg.TiDB.PdAddr)\n\trequire.Equal(t, path, cfg.Mydumper.SourceDir)\n\trequire.Equal(t, config.BackendLocal, cfg.TikvImporter.Backend)\n\trequire.Equal(t, \".\", cfg.TikvImporter.SortedKVDir)\n\trequire.Equal(t, config.OpLevelOff, cfg.PostRestore.Checksum)\n\trequire.Equal(t, config.OpLevelOptional, cfg.PostRestore.Analyze)\n\n\ttaskCfg := config.NewConfig()\n\terr = taskCfg.LoadFromGlobal(cfg)\n\trequire.NoError(t, err)\n\trequire.Equal(t, config.OpLevelOff, taskCfg.PostRestore.Checksum)\n\trequire.Equal(t, config.OpLevelOptional, taskCfg.PostRestore.Analyze)\n\n\ttaskCfg.Checkpoint.DSN = \"\"\n\ttaskCfg.Checkpoint.Driver = config.CheckpointDriverMySQL\n\ttaskCfg.TiDB.DistSQLScanConcurrency = 1\n\terr = taskCfg.Adjust(context.Background())\n\trequire.NoError(t, err)\n\tequivalentDSN := taskCfg.Checkpoint.MySQLParam.ToDriverConfig().FormatDSN()\n\texpectedDSN := \"guest:12345@tcp(172.16.30.11:4001)/?tls=false&maxAllowedPacket=67108864&charset=utf8mb4&sql_mode=%27ONLY_FULL_GROUP_BY%2CSTRICT_TRANS_TABLES%2CNO_ZERO_IN_DATE%2CNO_ZERO_DATE%2CERROR_FOR_DIVISION_BY_ZERO%2CNO_AUTO_CREATE_USER%2CNO_ENGINE_SUBSTITUTION%27\"\n\trequire.Equal(t, expectedDSN, equivalentDSN)\n\n\tresult := taskCfg.String()\n\trequire.Regexp(t, `.*\"pd-addr\":\"172.16.30.11:2379,172.16.30.12:2379\".*`, result)\n\n\tcfg, err = config.LoadGlobalConfig([]string{}, nil)\n\trequire.NoError(t, err)\n\trequire.Regexp(t, \".*lightning.log.*\", cfg.App.Config.File)\n\tcfg, err = config.LoadGlobalConfig([]string{\"--log-file\", \"-\"}, nil)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"-\", cfg.App.Config.File)\n}\n\nfunc TestDefaultImporterBackendValue(t *testing.T) {\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\tcfg.TikvImporter.Backend = \"local\"\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\terr := cfg.Adjust(context.Background())\n\trequire.NoError(t, err)\n\trequire.Equal(t, 2, cfg.App.IndexConcurrency)\n\trequire.Equal(t, 6, cfg.App.TableConcurrency)\n}\n\nfunc TestDefaultTidbBackendValue(t *testing.T) {\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\tcfg.TikvImporter.Backend = \"tidb\"\n\tcfg.App.RegionConcurrency = 123\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\terr := cfg.Adjust(context.Background())\n\trequire.NoError(t, err)\n\trequire.Equal(t, 123, cfg.App.TableConcurrency)\n}\n\nfunc TestDefaultCouldBeOverwritten(t *testing.T) {\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\tcfg.TikvImporter.Backend = \"local\"\n\tcfg.App.IndexConcurrency = 20\n\tcfg.App.TableConcurrency = 60\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\terr := cfg.Adjust(context.Background())\n\trequire.NoError(t, err)\n\trequire.Equal(t, 20, cfg.App.IndexConcurrency)\n\trequire.Equal(t, 60, cfg.App.TableConcurrency)\n}\n\nfunc TestLoadFromInvalidConfig(t *testing.T) {\n\ttaskCfg := config.NewConfig()\n\terr := taskCfg.LoadFromGlobal(&config.GlobalConfig{\n\t\tConfigFileContent: []byte(\"invalid toml\"),\n\t})\n\trequire.Error(t, err)\n\trequire.Regexp(t, \"line 1.*\", err.Error())\n}\n\nfunc TestTomlPostRestore(t *testing.T) {\n\tcfg := &config.Config{}\n\terr := cfg.LoadFromTOML([]byte(`\n\t\t[post-restore]\n\t\tchecksum = \"req\"\n\t`))\n\trequire.EqualError(t, err, \"invalid op level 'req', please choose valid option between ['off', 'optional', 'required']\")\n\n\terr = cfg.LoadFromTOML([]byte(`\n\t\t[post-restore]\n\t\tanalyze = 123\n\t`))\n\trequire.EqualError(t, err, \"invalid op level '123', please choose valid option between ['off', 'optional', 'required']\")\n\n\tkvMap := map[string]config.PostOpLevel{\n\t\t`\"off\"`:      config.OpLevelOff,\n\t\t`\"required\"`: config.OpLevelRequired,\n\t\t`\"optional\"`: config.OpLevelOptional,\n\t\t\"true\":       config.OpLevelRequired,\n\t\t\"false\":      config.OpLevelOff,\n\t}\n\n\tvar b bytes.Buffer\n\tenc := toml.NewEncoder(&b)\n\n\tfor k, v := range kvMap {\n\t\tcfg := &config.Config{}\n\t\tconfStr := fmt.Sprintf(\"[post-restore]\\r\\nchecksum= %s\\r\\n\", k)\n\t\terr := cfg.LoadFromTOML([]byte(confStr))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, v, cfg.PostRestore.Checksum)\n\n\t\tb.Reset()\n\t\trequire.NoError(t, enc.Encode(cfg.PostRestore))\n\t\trequire.Regexp(t, fmt.Sprintf(`(?s).*checksum = \"\\Q%s\\E\".*`, v), &b)\n\t}\n\n\tfor k, v := range kvMap {\n\t\tcfg := &config.Config{}\n\t\tconfStr := fmt.Sprintf(\"[post-restore]\\r\\nanalyze= %s\\r\\n\", k)\n\t\terr := cfg.LoadFromTOML([]byte(confStr))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, v, cfg.PostRestore.Analyze)\n\n\t\tb.Reset()\n\t\trequire.NoError(t, enc.Encode(cfg.PostRestore))\n\t\trequire.Regexp(t, fmt.Sprintf(`(?s).*analyze = \"\\Q%s\\E\".*`, v), &b)\n\t}\n}\n\nfunc TestCronEncodeDecode(t *testing.T) {\n\tcfg := &config.Config{}\n\tcfg.Cron.SwitchMode.Duration = 1 * time.Minute\n\tcfg.Cron.LogProgress.Duration = 2 * time.Minute\n\tcfg.Cron.CheckDiskQuota.Duration = 3 * time.Second\n\tvar b bytes.Buffer\n\trequire.NoError(t, toml.NewEncoder(&b).Encode(cfg.Cron))\n\trequire.Equal(t, \"switch-mode = \\\"1m0s\\\"\\nlog-progress = \\\"2m0s\\\"\\ncheck-disk-quota = \\\"3s\\\"\\n\", b.String())\n\n\tconfStr := \"[cron]\\r\\n\" + b.String()\n\tcfg2 := &config.Config{}\n\trequire.NoError(t, cfg2.LoadFromTOML([]byte(confStr)))\n\trequire.Equal(t, cfg.Cron, cfg2.Cron)\n}\n\nfunc TestAdjustWithLegacyBlackWhiteList(t *testing.T) {\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\trequire.Equal(t, config.GetDefaultFilter(), cfg.Mydumper.Filter)\n\trequire.False(t, cfg.HasLegacyBlackWhiteList())\n\n\tctx := context.Background()\n\tcfg.Mydumper.Filter = []string{\"test.*\"}\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\trequire.NoError(t, cfg.Adjust(ctx))\n\trequire.False(t, cfg.HasLegacyBlackWhiteList())\n\n\tcfg.BWList.DoDBs = []string{\"test\"}\n\trequire.EqualError(t, cfg.Adjust(ctx), \"[Lightning:Config:ErrInvalidConfig]`mydumper.filter` and `black-white-list` cannot be simultaneously defined\")\n\n\tcfg.Mydumper.Filter = config.GetDefaultFilter()\n\trequire.NoError(t, cfg.Adjust(ctx))\n\trequire.True(t, cfg.HasLegacyBlackWhiteList())\n}\n\nfunc TestAdjustDiskQuota(t *testing.T) {\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\n\tbase := t.TempDir()\n\tctx := context.Background()\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.DiskQuota = 0\n\tcfg.TikvImporter.SortedKVDir = base\n\tcfg.TiDB.DistSQLScanConcurrency = 1\n\trequire.NoError(t, cfg.Adjust(ctx))\n\trequire.Equal(t, int64(0), int64(cfg.TikvImporter.DiskQuota))\n}\n\nfunc TestDataCharacterSet(t *testing.T) {\n\ttestCases := []struct {\n\t\tinput string\n\t\terr   string\n\t}{\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper]\n\t\t\t\tdata-character-set = 'binary'\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper]\n\t\t\t\tdata-character-set = 'utf8mb4'\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper]\n\t\t\t\tdata-character-set = 'gb18030'\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper]\n\t\t\t\tdata-invalid-char-replace = \"\\u2323\"\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper]\n\t\t\t\tdata-invalid-char-replace = \"a\"\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper]\n\t\t\t\tdata-invalid-char-replace = \"INV\"\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper]\n\t\t\t\tdata-invalid-char-replace = \"\ud83d\ude0a\"\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t\t{\n\t\t\tinput: `\n\t\t\t\t[mydumper]\n\t\t\t\tdata-invalid-char-replace = \"\ud83d\ude0a\ud83d\ude2d\ud83d\ude05\ud83d\ude04\"\n\t\t\t`,\n\t\t\terr: \"\",\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tcomment := fmt.Sprintf(\"input = %s\", tc.input)\n\t\tcfg := config.NewConfig()\n\t\tcfg.Mydumper.SourceDir = \"file://.\"\n\t\tcfg.TiDB.Port = 4000\n\t\tcfg.TiDB.PdAddr = \"test.invalid:2379\"\n\t\tcfg.TikvImporter.Backend = config.BackendLocal\n\t\tcfg.TikvImporter.SortedKVDir = \".\"\n\t\tcfg.TiDB.DistSQLScanConcurrency = 1\n\t\terr := cfg.LoadFromTOML([]byte(tc.input))\n\t\trequire.NoError(t, err)\n\t\terr = cfg.Adjust(context.Background())\n\t\tif tc.err != \"\" {\n\t\t\trequire.EqualError(t, err, tc.err, comment)\n\t\t} else {\n\t\t\trequire.NoError(t, err, comment)\n\t\t}\n\t}\n}\n\nfunc TestCheckpointKeepStrategy(t *testing.T) {\n\ttomlCases := map[interface{}]config.CheckpointKeepStrategy{\n\t\ttrue:     config.CheckpointRename,\n\t\tfalse:    config.CheckpointRemove,\n\t\t\"remove\": config.CheckpointRemove,\n\t\t\"rename\": config.CheckpointRename,\n\t\t\"origin\": config.CheckpointOrigin,\n\t}\n\tvar cp config.CheckpointKeepStrategy\n\tfor key, strategy := range tomlCases {\n\t\terr := cp.UnmarshalTOML(key)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, strategy, cp)\n\t}\n\n\tdefaultCp := \"enable = true\\r\\n\"\n\tcpCfg := &config.Checkpoint{}\n\t_, err := toml.Decode(defaultCp, cpCfg)\n\trequire.NoError(t, err)\n\trequire.Equal(t, config.CheckpointRemove, cpCfg.KeepAfterSuccess)\n\n\tcpFmt := \"keep-after-success = %v\\r\\n\"\n\tfor key, strategy := range tomlCases {\n\t\tcpValue := key\n\t\tif strVal, ok := key.(string); ok {\n\t\t\tcpValue = `\"` + strVal + `\"`\n\t\t}\n\t\ttomlStr := fmt.Sprintf(cpFmt, cpValue)\n\t\tcpCfg := &config.Checkpoint{}\n\t\t_, err := toml.Decode(tomlStr, cpCfg)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, strategy, cpCfg.KeepAfterSuccess)\n\t}\n\n\tmarshalTextCases := map[config.CheckpointKeepStrategy]string{\n\t\tconfig.CheckpointRemove: \"remove\",\n\t\tconfig.CheckpointRename: \"rename\",\n\t\tconfig.CheckpointOrigin: \"origin\",\n\t}\n\tfor strategy, value := range marshalTextCases {\n\t\tres, err := strategy.MarshalText()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []byte(value), res)\n\t}\n}\n\nfunc TestLoadCharsetFromConfig(t *testing.T) {\n\tcases := map[string]config.Charset{\n\t\t\"binary\":  config.Binary,\n\t\t\"BINARY\":  config.Binary,\n\t\t\"GBK\":     config.GBK,\n\t\t\"gbk\":     config.GBK,\n\t\t\"Gbk\":     config.GBK,\n\t\t\"gB18030\": config.GB18030,\n\t\t\"GB18030\": config.GB18030,\n\t}\n\tfor k, v := range cases {\n\t\tcharset, err := config.ParseCharset(k)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, v, charset)\n\t}\n\n\t_, err := config.ParseCharset(\"Unknown\")\n\trequire.EqualError(t, err, \"found unsupported data-character-set: Unknown\")\n}\n\nfunc TestCheckAndAdjustForLocalBackend(t *testing.T) {\n\tcfg := config.NewConfig()\n\tassignMinimalLegalValue(cfg)\n\n\tcfg.TikvImporter.Backend = config.BackendLocal\n\tcfg.TikvImporter.SortedKVDir = \"\"\n\trequire.EqualError(t, cfg.CheckAndAdjustForLocalBackend(), \"[Lightning:Config:ErrInvalidConfig]tikv-importer.sorted-kv-dir must not be empty!\")\n\n\t// non exists dir is legal\n\tcfg.TikvImporter.SortedKVDir = \"./not-exists\"\n\trequire.NoError(t, cfg.CheckAndAdjustForLocalBackend())\n\n\tbase := t.TempDir()\n\t// create empty file\n\tfile := filepath.Join(base, \"file\")\n\trequire.NoError(t, os.WriteFile(file, []byte(\"\"), 0644))\n\tcfg.TikvImporter.SortedKVDir = file\n\terr := cfg.CheckAndAdjustForLocalBackend()\n\trequire.Error(t, err)\n\trequire.Regexp(t, \"tikv-importer.sorted-kv-dir (.*) is not a directory\", err.Error())\n\n\t// legal dir\n\tcfg.TikvImporter.SortedKVDir = base\n\trequire.NoError(t, cfg.CheckAndAdjustForLocalBackend())\n}\n\nfunc TestCreateSeveralConfigsWithDifferentFilters(t *testing.T) {\n\toriginalDefaultCfg := append([]string{}, config.GetDefaultFilter()...)\n\tcfg1 := config.NewConfig()\n\trequire.NoError(t, cfg1.LoadFromTOML([]byte(`\n\t\t[mydumper]\n\t\tfilter = [\"db1.tbl1\", \"db2.*\", \"!db2.tbl1\"]\n\t`)))\n\trequire.Equal(t, 3, len(cfg1.Mydumper.Filter))\n\trequire.True(t, common.StringSliceEqual(\n\t\tcfg1.Mydumper.Filter,\n\t\t[]string{\"db1.tbl1\", \"db2.*\", \"!db2.tbl1\"},\n\t))\n\trequire.True(t, common.StringSliceEqual(config.GetDefaultFilter(), originalDefaultCfg))\n\n\tcfg2 := config.NewConfig()\n\trequire.True(t, common.StringSliceEqual(\n\t\tcfg2.Mydumper.Filter,\n\t\toriginalDefaultCfg,\n\t))\n\trequire.True(t, common.StringSliceEqual(config.GetDefaultFilter(), originalDefaultCfg))\n\n\tgCfg1, err := config.LoadGlobalConfig([]string{\"-f\", \"db1.tbl1\", \"-f\", \"db2.*\", \"-f\", \"!db2.tbl1\"}, nil)\n\trequire.NoError(t, err)\n\trequire.True(t, common.StringSliceEqual(\n\t\tgCfg1.Mydumper.Filter,\n\t\t[]string{\"db1.tbl1\", \"db2.*\", \"!db2.tbl1\"},\n\t))\n\trequire.True(t, common.StringSliceEqual(config.GetDefaultFilter(), originalDefaultCfg))\n\n\tgCfg2, err := config.LoadGlobalConfig([]string{}, nil)\n\trequire.NoError(t, err)\n\trequire.True(t, common.StringSliceEqual(\n\t\tgCfg2.Mydumper.Filter,\n\t\toriginalDefaultCfg,\n\t))\n\trequire.True(t, common.StringSliceEqual(config.GetDefaultFilter(), originalDefaultCfg))\n}\n", "// Copyright 2016 PingCAP, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage main\n\nimport (\n\t\"database/sql\"\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n\t\"strconv\"\n\t\"strings\"\n\n\tmysql2 \"github.com/go-sql-driver/mysql\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/log\"\n\t\"github.com/pingcap/tidb/parser/mysql\"\n\t\"go.uber.org/zap\"\n)\n\nfunc intRangeValue(column *column, min int64, max int64) (int64, int64) {\n\tvar err error\n\tif len(column.min) > 0 {\n\t\tmin, err = strconv.ParseInt(column.min, 10, 64)\n\t\tif err != nil {\n\t\t\tlog.Fatal(err.Error())\n\t\t}\n\n\t\tif len(column.max) > 0 {\n\t\t\tmax, err = strconv.ParseInt(column.max, 10, 64)\n\t\t\tif err != nil {\n\t\t\t\tlog.Fatal(err.Error())\n\t\t\t}\n\t\t}\n\t}\n\n\treturn min, max\n}\n\nfunc randStringValue(column *column, n int) string {\n\tif column.hist != nil {\n\t\tif column.hist.avgLen == 0 {\n\t\t\tcolumn.hist.avgLen = column.hist.getAvgLen(n)\n\t\t}\n\t\treturn column.hist.randString()\n\t}\n\tif len(column.set) > 0 {\n\t\tidx := randInt(0, len(column.set)-1)\n\t\treturn column.set[idx]\n\t}\n\treturn randString(randInt(1, n))\n}\n\nfunc randInt64Value(column *column, min int64, max int64) int64 {\n\tif column.hist != nil {\n\t\treturn column.hist.randInt()\n\t}\n\tif len(column.set) > 0 {\n\t\tidx := randInt(0, len(column.set)-1)\n\t\tdata, err := strconv.ParseInt(column.set[idx], 10, 64)\n\t\tif err != nil {\n\t\t\tlog.Warn(\"rand int64 failed\", zap.Error(err))\n\t\t}\n\t\treturn data\n\t}\n\n\tmin, max = intRangeValue(column, min, max)\n\treturn randInt64(min, max)\n}\n\nfunc nextInt64Value(column *column, min int64, max int64) int64 {\n\tmin, max = intRangeValue(column, min, max)\n\tcolumn.data.setInitInt64Value(min, max)\n\treturn column.data.nextInt64()\n}\n\nfunc intToDecimalString(intValue int64, decimal int) string {\n\tdata := strconv.FormatInt(intValue, 10)\n\n\t// add leading zero\n\tif len(data) < decimal {\n\t\tdata = strings.Repeat(\"0\", decimal-len(data)) + data\n\t}\n\n\tdec := data[len(data)-decimal:]\n\tif data = data[:len(data)-decimal]; data == \"\" {\n\t\tdata = \"0\"\n\t}\n\tif dec != \"\" {\n\t\tdata = data + \".\" + dec\n\t}\n\treturn data\n}\n\nfunc genRowDatas(table *table, count int) ([]string, error) {\n\tdatas := make([]string, 0, count)\n\tfor i := 0; i < count; i++ {\n\t\tdata, err := genRowData(table)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\t\tdatas = append(datas, data)\n\t}\n\n\treturn datas, nil\n}\n\nfunc genRowData(table *table) (string, error) {\n\tvar values []byte //nolint: prealloc\n\tfor _, column := range table.columns {\n\t\tdata, err := genColumnData(table, column)\n\t\tif err != nil {\n\t\t\treturn \"\", errors.Trace(err)\n\t\t}\n\t\tvalues = append(values, []byte(data)...)\n\t\tvalues = append(values, ',')\n\t}\n\n\tvalues = values[:len(values)-1]\n\tsql := fmt.Sprintf(\"insert into %s (%s) values (%s);\", table.name, table.columnList, string(values))\n\treturn sql, nil\n}\n\n// #nosec G404\nfunc genColumnData(table *table, column *column) (string, error) {\n\ttp := column.tp\n\tincremental := column.incremental\n\tif incremental {\n\t\tincremental = uint32(rand.Int31n(100))+1 <= column.data.probability\n\t\t// If incremental, there is only one worker, so it is safe to directly access datum.\n\t\tif !incremental && column.data.remains > 0 {\n\t\t\tcolumn.data.remains--\n\t\t}\n\t}\n\tif _, ok := table.uniqIndices[column.name]; ok {\n\t\tincremental = true\n\t}\n\tisUnsigned := mysql.HasUnsignedFlag(tp.GetFlag())\n\n\tswitch tp.GetType() {\n\tcase mysql.TypeTiny:\n\t\tvar data int64\n\t\tif incremental {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = nextInt64Value(column, 0, math.MaxUint8)\n\t\t\t} else {\n\t\t\t\tdata = nextInt64Value(column, math.MinInt8, math.MaxInt8)\n\t\t\t}\n\t\t} else {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = randInt64Value(column, 0, math.MaxUint8)\n\t\t\t} else {\n\t\t\t\tdata = randInt64Value(column, math.MinInt8, math.MaxInt8)\n\t\t\t}\n\t\t}\n\t\treturn strconv.FormatInt(data, 10), nil\n\tcase mysql.TypeShort:\n\t\tvar data int64\n\t\tif incremental {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = nextInt64Value(column, 0, math.MaxUint16)\n\t\t\t} else {\n\t\t\t\tdata = nextInt64Value(column, math.MinInt16, math.MaxInt16)\n\t\t\t}\n\t\t} else {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = randInt64Value(column, 0, math.MaxUint16)\n\t\t\t} else {\n\t\t\t\tdata = randInt64Value(column, math.MinInt16, math.MaxInt16)\n\t\t\t}\n\t\t}\n\t\treturn strconv.FormatInt(data, 10), nil\n\tcase mysql.TypeLong:\n\t\tvar data int64\n\t\tif incremental {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = nextInt64Value(column, 0, math.MaxUint32)\n\t\t\t} else {\n\t\t\t\tdata = nextInt64Value(column, math.MinInt32, math.MaxInt32)\n\t\t\t}\n\t\t} else {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = randInt64Value(column, 0, math.MaxUint32)\n\t\t\t} else {\n\t\t\t\tdata = randInt64Value(column, math.MinInt32, math.MaxInt32)\n\t\t\t}\n\t\t}\n\t\treturn strconv.FormatInt(data, 10), nil\n\tcase mysql.TypeLonglong:\n\t\tvar data int64\n\t\tif incremental {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = nextInt64Value(column, 0, math.MaxInt64-1)\n\t\t\t} else {\n\t\t\t\tdata = nextInt64Value(column, math.MinInt32, math.MaxInt32)\n\t\t\t}\n\t\t} else {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = randInt64Value(column, 0, math.MaxInt64-1)\n\t\t\t} else {\n\t\t\t\tdata = randInt64Value(column, math.MinInt32, math.MaxInt32)\n\t\t\t}\n\t\t}\n\t\treturn strconv.FormatInt(data, 10), nil\n\tcase mysql.TypeVarchar, mysql.TypeString, mysql.TypeTinyBlob, mysql.TypeBlob, mysql.TypeMediumBlob, mysql.TypeLongBlob:\n\t\tdata := []byte{'\\''}\n\t\tif incremental {\n\t\t\tdata = append(data, []byte(column.data.nextString(tp.GetFlen()))...)\n\t\t} else {\n\t\t\tdata = append(data, []byte(randStringValue(column, tp.GetFlen()))...)\n\t\t}\n\n\t\tdata = append(data, '\\'')\n\t\treturn string(data), nil\n\tcase mysql.TypeFloat, mysql.TypeDouble:\n\t\tvar data float64\n\t\tif incremental {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = float64(nextInt64Value(column, 0, math.MaxInt64-1))\n\t\t\t} else {\n\t\t\t\tdata = float64(nextInt64Value(column, math.MinInt32, math.MaxInt32))\n\t\t\t}\n\t\t} else {\n\t\t\tif isUnsigned {\n\t\t\t\tdata = float64(randInt64Value(column, 0, math.MaxInt64-1))\n\t\t\t} else {\n\t\t\t\tdata = float64(randInt64Value(column, math.MinInt32, math.MaxInt32))\n\t\t\t}\n\t\t}\n\t\treturn strconv.FormatFloat(data, 'f', -1, 64), nil\n\tcase mysql.TypeDate:\n\t\tdata := []byte{'\\''}\n\t\tif incremental {\n\t\t\tdata = append(data, []byte(column.data.nextDate())...)\n\t\t} else {\n\t\t\tdata = append(data, []byte(randDate(column))...)\n\t\t}\n\n\t\tdata = append(data, '\\'')\n\t\treturn string(data), nil\n\tcase mysql.TypeDatetime, mysql.TypeTimestamp:\n\t\tdata := []byte{'\\''}\n\t\tif incremental {\n\t\t\tdata = append(data, []byte(column.data.nextTimestamp())...)\n\t\t} else {\n\t\t\tdata = append(data, []byte(randTimestamp(column))...)\n\t\t}\n\n\t\tdata = append(data, '\\'')\n\t\treturn string(data), nil\n\tcase mysql.TypeDuration:\n\t\tdata := []byte{'\\''}\n\t\tif incremental {\n\t\t\tdata = append(data, []byte(column.data.nextTime())...)\n\t\t} else {\n\t\t\tdata = append(data, []byte(randTime(column))...)\n\t\t}\n\n\t\tdata = append(data, '\\'')\n\t\treturn string(data), nil\n\tcase mysql.TypeYear:\n\t\tdata := []byte{'\\''}\n\t\tif incremental {\n\t\t\tdata = append(data, []byte(column.data.nextYear())...)\n\t\t} else {\n\t\t\tdata = append(data, []byte(randYear(column))...)\n\t\t}\n\n\t\tdata = append(data, '\\'')\n\t\treturn string(data), nil\n\tcase mysql.TypeNewDecimal:\n\t\tvar limit = int64(math.Pow10(tp.GetFlen()))\n\t\tvar intVal int64\n\t\tif limit < 0 {\n\t\t\tlimit = math.MaxInt64\n\t\t}\n\t\tif incremental {\n\t\t\tif isUnsigned {\n\t\t\t\tintVal = nextInt64Value(column, 0, limit-1)\n\t\t\t} else {\n\t\t\t\tintVal = nextInt64Value(column, (-limit+1)/2, (limit-1)/2)\n\t\t\t}\n\t\t} else {\n\t\t\tif isUnsigned {\n\t\t\t\tintVal = randInt64Value(column, 0, limit-1)\n\t\t\t} else {\n\t\t\t\tintVal = randInt64Value(column, (-limit+1)/2, (limit-1)/2)\n\t\t\t}\n\t\t}\n\t\treturn intToDecimalString(intVal, tp.GetDecimal()), nil\n\tdefault:\n\t\treturn \"\", errors.Errorf(\"unsupported column type - %v\", column)\n\t}\n}\n\nfunc execSQL(db *sql.DB, sql string) error {\n\tif sql == \"\" {\n\t\treturn nil\n\t}\n\n\t_, err := db.Exec(sql)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\treturn nil\n}\n\nfunc createDB(cfg DBConfig) (*sql.DB, error) {\n\tdriverCfg := mysql2.NewConfig()\n\tdriverCfg.User = cfg.User\n\tdriverCfg.Passwd = cfg.Password\n\tdriverCfg.Net = \"tcp\"\n\tdriverCfg.Addr = cfg.Host + \":\" + strconv.Itoa(cfg.Port)\n\tdriverCfg.DBName = cfg.Name\n\n\tc, err := mysql2.NewConnector(driverCfg)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\treturn sql.OpenDB(c), nil\n}\n\nfunc closeDB(db *sql.DB) error {\n\treturn errors.Trace(db.Close())\n}\n\nfunc createDBs(cfg DBConfig, count int) ([]*sql.DB, error) {\n\tdbs := make([]*sql.DB, 0, count)\n\tfor i := 0; i < count; i++ {\n\t\tdb, err := createDB(cfg)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\n\t\tdbs = append(dbs, db)\n\t}\n\n\treturn dbs, nil\n}\n\nfunc closeDBs(dbs []*sql.DB) {\n\tfor _, db := range dbs {\n\t\terr := closeDB(db)\n\t\tif err != nil {\n\t\t\tlog.Error(\"close DB failed\", zap.Error(err))\n\t\t}\n\t}\n}\n", "// Copyright 2020 PingCAP, Inc. Licensed under Apache-2.0.\n\npackage export\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"net\"\n\t\"strconv\"\n\t\"strings\"\n\t\"text/template\"\n\t\"time\"\n\n\t\"github.com/coreos/go-semver/semver\"\n\t\"github.com/docker/go-units\"\n\t\"github.com/go-sql-driver/mysql\"\n\t\"github.com/google/uuid\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/tidb/br/pkg/storage\"\n\t\"github.com/pingcap/tidb/br/pkg/version\"\n\t\"github.com/pingcap/tidb/util\"\n\t\"github.com/pingcap/tidb/util/promutil\"\n\tfilter \"github.com/pingcap/tidb/util/table-filter\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/spf13/pflag\"\n\t\"go.uber.org/zap\"\n)\n\nconst (\n\tflagDatabase                 = \"database\"\n\tflagTablesList               = \"tables-list\"\n\tflagHost                     = \"host\"\n\tflagUser                     = \"user\"\n\tflagPort                     = \"port\"\n\tflagPassword                 = \"password\"\n\tflagAllowCleartextPasswords  = \"allow-cleartext-passwords\"\n\tflagThreads                  = \"threads\"\n\tflagFilesize                 = \"filesize\"\n\tflagStatementSize            = \"statement-size\"\n\tflagOutput                   = \"output\"\n\tflagLoglevel                 = \"loglevel\"\n\tflagLogfile                  = \"logfile\"\n\tflagLogfmt                   = \"logfmt\"\n\tflagConsistency              = \"consistency\"\n\tflagSnapshot                 = \"snapshot\"\n\tflagNoViews                  = \"no-views\"\n\tflagNoSequences              = \"no-sequences\"\n\tflagSortByPk                 = \"order-by-primary-key\"\n\tflagStatusAddr               = \"status-addr\"\n\tflagRows                     = \"rows\"\n\tflagWhere                    = \"where\"\n\tflagEscapeBackslash          = \"escape-backslash\"\n\tflagFiletype                 = \"filetype\"\n\tflagNoHeader                 = \"no-header\"\n\tflagNoSchemas                = \"no-schemas\"\n\tflagNoData                   = \"no-data\"\n\tflagCsvNullValue             = \"csv-null-value\"\n\tflagSQL                      = \"sql\"\n\tflagFilter                   = \"filter\"\n\tflagCaseSensitive            = \"case-sensitive\"\n\tflagDumpEmptyDatabase        = \"dump-empty-database\"\n\tflagTidbMemQuotaQuery        = \"tidb-mem-quota-query\"\n\tflagCA                       = \"ca\"\n\tflagCert                     = \"cert\"\n\tflagKey                      = \"key\"\n\tflagCsvSeparator             = \"csv-separator\"\n\tflagCsvDelimiter             = \"csv-delimiter\"\n\tflagOutputFilenameTemplate   = \"output-filename-template\"\n\tflagCompleteInsert           = \"complete-insert\"\n\tflagParams                   = \"params\"\n\tflagReadTimeout              = \"read-timeout\"\n\tflagTransactionalConsistency = \"transactional-consistency\"\n\tflagCompress                 = \"compress\"\n\n\t// FlagHelp represents the help flag\n\tFlagHelp = \"help\"\n)\n\n// Config is the dump config for dumpling\ntype Config struct {\n\tstorage.BackendOptions\n\n\tspecifiedTables          bool\n\tAllowCleartextPasswords  bool\n\tSortByPk                 bool\n\tNoViews                  bool\n\tNoSequences              bool\n\tNoHeader                 bool\n\tNoSchemas                bool\n\tNoData                   bool\n\tCompleteInsert           bool\n\tTransactionalConsistency bool\n\tEscapeBackslash          bool\n\tDumpEmptyDatabase        bool\n\tPosAfterConnect          bool\n\tCompressType             storage.CompressType\n\n\tHost     string\n\tPort     int\n\tThreads  int\n\tUser     string\n\tPassword string `json:\"-\"`\n\tSecurity struct {\n\t\tDriveTLSName string `json:\"-\"`\n\t\tCAPath       string\n\t\tCertPath     string\n\t\tKeyPath      string\n\t\tSSLCABytes   []byte `json:\"-\"`\n\t\tSSLCertBytes []byte `json:\"-\"`\n\t\tSSLKeyBytes  []byte `json:\"-\"`\n\t}\n\n\tLogLevel      string\n\tLogFile       string\n\tLogFormat     string\n\tOutputDirPath string\n\tStatusAddr    string\n\tSnapshot      string\n\tConsistency   string\n\tCsvNullValue  string\n\tSQL           string\n\tCsvSeparator  string\n\tCsvDelimiter  string\n\tDatabases     []string\n\n\tTableFilter         filter.Filter `json:\"-\"`\n\tWhere               string\n\tFileType            string\n\tServerInfo          version.ServerInfo\n\tLogger              *zap.Logger        `json:\"-\"`\n\tOutputFileTemplate  *template.Template `json:\"-\"`\n\tRows                uint64\n\tReadTimeout         time.Duration\n\tTiDBMemQuotaQuery   uint64\n\tFileSize            uint64\n\tStatementSize       uint64\n\tSessionParams       map[string]interface{}\n\tTables              DatabaseTables\n\tCollationCompatible string\n\n\tLabels       prometheus.Labels       `json:\"-\"`\n\tPromFactory  promutil.Factory        `json:\"-\"`\n\tPromRegistry promutil.Registry       `json:\"-\"`\n\tExtStorage   storage.ExternalStorage `json:\"-\"`\n}\n\n// ServerInfoUnknown is the unknown database type to dumpling\nvar ServerInfoUnknown = version.ServerInfo{\n\tServerType:    version.ServerTypeUnknown,\n\tServerVersion: nil,\n}\n\n// DefaultConfig returns the default export Config for dumpling\nfunc DefaultConfig() *Config {\n\tallFilter, _ := filter.Parse([]string{\"*.*\"})\n\treturn &Config{\n\t\tDatabases:           nil,\n\t\tHost:                \"127.0.0.1\",\n\t\tUser:                \"root\",\n\t\tPort:                3306,\n\t\tPassword:            \"\",\n\t\tThreads:             4,\n\t\tLogger:              nil,\n\t\tStatusAddr:          \":8281\",\n\t\tFileSize:            UnspecifiedSize,\n\t\tStatementSize:       DefaultStatementSize,\n\t\tOutputDirPath:       \".\",\n\t\tServerInfo:          ServerInfoUnknown,\n\t\tSortByPk:            true,\n\t\tTables:              nil,\n\t\tSnapshot:            \"\",\n\t\tConsistency:         ConsistencyTypeAuto,\n\t\tNoViews:             true,\n\t\tNoSequences:         true,\n\t\tRows:                UnspecifiedSize,\n\t\tWhere:               \"\",\n\t\tFileType:            \"\",\n\t\tNoHeader:            false,\n\t\tNoSchemas:           false,\n\t\tNoData:              false,\n\t\tCsvNullValue:        \"\\\\N\",\n\t\tSQL:                 \"\",\n\t\tTableFilter:         allFilter,\n\t\tDumpEmptyDatabase:   true,\n\t\tSessionParams:       make(map[string]interface{}),\n\t\tOutputFileTemplate:  DefaultOutputFileTemplate,\n\t\tPosAfterConnect:     false,\n\t\tCollationCompatible: LooseCollationCompatible,\n\t\tspecifiedTables:     false,\n\t\tPromFactory:         promutil.NewDefaultFactory(),\n\t\tPromRegistry:        promutil.NewDefaultRegistry(),\n\t}\n}\n\n// String returns dumpling's config in json format\nfunc (conf *Config) String() string {\n\tcfg, err := json.Marshal(conf)\n\tif err != nil && conf.Logger != nil {\n\t\tconf.Logger.Error(\"fail to marshal config to json\", zap.Error(err))\n\t}\n\treturn string(cfg)\n}\n\n// GetDSN generates DSN from Config\nfunc (conf *Config) GetDSN(db string) string {\n\t// maxAllowedPacket=0 can be used to automatically fetch the max_allowed_packet variable from server on every connection.\n\t// https://github.com/go-sql-driver/mysql#maxallowedpacket\n\thostPort := net.JoinHostPort(conf.Host, strconv.Itoa(conf.Port))\n\tdsn := fmt.Sprintf(\"%s:%s@tcp(%s)/%s?collation=utf8mb4_general_ci&readTimeout=%s&writeTimeout=30s&interpolateParams=true&maxAllowedPacket=0\",\n\t\tconf.User, conf.Password, hostPort, db, conf.ReadTimeout)\n\tif conf.Security.DriveTLSName != \"\" {\n\t\tdsn += \"&tls=\" + conf.Security.DriveTLSName\n\t}\n\tif conf.AllowCleartextPasswords {\n\t\tdsn += \"&allowCleartextPasswords=1\"\n\t}\n\treturn dsn\n}\n\n// GetDriverConfig returns the MySQL driver config from Config.\nfunc (conf *Config) GetDriverConfig(db string) *mysql.Config {\n\tdriverCfg := mysql.NewConfig()\n\t// maxAllowedPacket=0 can be used to automatically fetch the max_allowed_packet variable from server on every connection.\n\t// https://github.com/go-sql-driver/mysql#maxallowedpacket\n\thostPort := net.JoinHostPort(conf.Host, strconv.Itoa(conf.Port))\n\tdriverCfg.User = conf.User\n\tdriverCfg.Passwd = conf.Password\n\tdriverCfg.Net = \"tcp\"\n\tdriverCfg.Addr = hostPort\n\tdriverCfg.DBName = db\n\tdriverCfg.Collation = \"utf8mb4_general_ci\"\n\tdriverCfg.ReadTimeout = conf.ReadTimeout\n\tdriverCfg.WriteTimeout = 30 * time.Second\n\tdriverCfg.InterpolateParams = true\n\tdriverCfg.MaxAllowedPacket = 0\n\tif conf.Security.DriveTLSName != \"\" {\n\t\tdriverCfg.TLSConfig = conf.Security.DriveTLSName\n\t}\n\tif conf.AllowCleartextPasswords {\n\t\tdriverCfg.AllowCleartextPasswords = true\n\t}\n\treturn driverCfg\n}\n\nfunc timestampDirName() string {\n\treturn fmt.Sprintf(\"./export-%s\", time.Now().Format(time.RFC3339))\n}\n\n// DefineFlags defines flags of dumpling's configuration\nfunc (*Config) DefineFlags(flags *pflag.FlagSet) {\n\tstorage.DefineFlags(flags)\n\tflags.StringSliceP(flagDatabase, \"B\", nil, \"Databases to dump\")\n\tflags.StringSliceP(flagTablesList, \"T\", nil, \"Comma delimited table list to dump; must be qualified table names\")\n\tflags.StringP(flagHost, \"h\", \"127.0.0.1\", \"The host to connect to\")\n\tflags.StringP(flagUser, \"u\", \"root\", \"Username with privileges to run the dump\")\n\tflags.IntP(flagPort, \"P\", 4000, \"TCP/IP port to connect to\")\n\tflags.StringP(flagPassword, \"p\", \"\", \"User password\")\n\tflags.Bool(flagAllowCleartextPasswords, false, \"Allow passwords to be sent in cleartext (warning: don't use without TLS)\")\n\tflags.IntP(flagThreads, \"t\", 4, \"Number of goroutines to use, default 4\")\n\tflags.StringP(flagFilesize, \"F\", \"\", \"The approximate size of output file\")\n\tflags.Uint64P(flagStatementSize, \"s\", DefaultStatementSize, \"Attempted size of INSERT statement in bytes\")\n\tflags.StringP(flagOutput, \"o\", timestampDirName(), \"Output directory\")\n\tflags.String(flagLoglevel, \"info\", \"Log level: {debug|info|warn|error|dpanic|panic|fatal}\")\n\tflags.StringP(flagLogfile, \"L\", \"\", \"Log file `path`, leave empty to write to console\")\n\tflags.String(flagLogfmt, \"text\", \"Log `format`: {text|json}\")\n\tflags.String(flagConsistency, ConsistencyTypeAuto, \"Consistency level during dumping: {auto|none|flush|lock|snapshot}\")\n\tflags.String(flagSnapshot, \"\", \"Snapshot position (uint64 or MySQL style string timestamp). Valid only when consistency=snapshot\")\n\tflags.BoolP(flagNoViews, \"W\", true, \"Do not dump views\")\n\tflags.Bool(flagNoSequences, true, \"Do not dump sequences\")\n\tflags.Bool(flagSortByPk, true, \"Sort dump results by primary key through order by sql\")\n\tflags.String(flagStatusAddr, \":8281\", \"dumpling API server and pprof addr\")\n\tflags.Uint64P(flagRows, \"r\", UnspecifiedSize, \"If specified, dumpling will split table into chunks and concurrently dump them to different files to improve efficiency. For TiDB v3.0+, specify this will make dumpling split table with each file one TiDB region(no matter how many rows is).\\n\"+\n\t\t\"If not specified, dumpling will dump table without inner-concurrency which could be relatively slow. default unlimited\")\n\tflags.String(flagWhere, \"\", \"Dump only selected records\")\n\tflags.Bool(flagEscapeBackslash, true, \"use backslash to escape special characters\")\n\tflags.String(flagFiletype, \"\", \"The type of export file (sql/csv)\")\n\tflags.Bool(flagNoHeader, false, \"whether not to dump CSV table header\")\n\tflags.BoolP(flagNoSchemas, \"m\", false, \"Do not dump table schemas with the data\")\n\tflags.BoolP(flagNoData, \"d\", false, \"Do not dump table data\")\n\tflags.String(flagCsvNullValue, \"\\\\N\", \"The null value used when export to csv\")\n\tflags.StringP(flagSQL, \"S\", \"\", \"Dump data with given sql. This argument doesn't support concurrent dump\")\n\t_ = flags.MarkHidden(flagSQL)\n\tflags.StringSliceP(flagFilter, \"f\", []string{\"*.*\", DefaultTableFilter}, \"filter to select which tables to dump\")\n\tflags.Bool(flagCaseSensitive, false, \"whether the filter should be case-sensitive\")\n\tflags.Bool(flagDumpEmptyDatabase, true, \"whether to dump empty database\")\n\tflags.Uint64(flagTidbMemQuotaQuery, UnspecifiedSize, \"The maximum memory limit for a single SQL statement, in bytes.\")\n\tflags.String(flagCA, \"\", \"The path name to the certificate authority file for TLS connection\")\n\tflags.String(flagCert, \"\", \"The path name to the client certificate file for TLS connection\")\n\tflags.String(flagKey, \"\", \"The path name to the client private key file for TLS connection\")\n\tflags.String(flagCsvSeparator, \",\", \"The separator for csv files, default ','\")\n\tflags.String(flagCsvDelimiter, \"\\\"\", \"The delimiter for values in csv files, default '\\\"'\")\n\tflags.String(flagOutputFilenameTemplate, \"\", \"The output filename template (without file extension)\")\n\tflags.Bool(flagCompleteInsert, false, \"Use complete INSERT statements that include column names\")\n\tflags.StringToString(flagParams, nil, `Extra session variables used while dumping, accepted format: --params \"character_set_client=latin1,character_set_connection=latin1\"`)\n\tflags.Bool(FlagHelp, false, \"Print help message and quit\")\n\tflags.Duration(flagReadTimeout, 15*time.Minute, \"I/O read timeout for db connection.\")\n\t_ = flags.MarkHidden(flagReadTimeout)\n\tflags.Bool(flagTransactionalConsistency, true, \"Only support transactional consistency\")\n\t_ = flags.MarkHidden(flagTransactionalConsistency)\n\tflags.StringP(flagCompress, \"c\", \"\", \"Compress output file type, support 'gzip', 'no-compression' now\")\n}\n\n// ParseFromFlags parses dumpling's export.Config from flags\n// nolint: gocyclo\nfunc (conf *Config) ParseFromFlags(flags *pflag.FlagSet) error {\n\tvar err error\n\tconf.Databases, err = flags.GetStringSlice(flagDatabase)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Host, err = flags.GetString(flagHost)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.User, err = flags.GetString(flagUser)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Port, err = flags.GetInt(flagPort)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Password, err = flags.GetString(flagPassword)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.AllowCleartextPasswords, err = flags.GetBool(flagAllowCleartextPasswords)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Threads, err = flags.GetInt(flagThreads)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.StatementSize, err = flags.GetUint64(flagStatementSize)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.OutputDirPath, err = flags.GetString(flagOutput)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.LogLevel, err = flags.GetString(flagLoglevel)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.LogFile, err = flags.GetString(flagLogfile)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.LogFormat, err = flags.GetString(flagLogfmt)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Consistency, err = flags.GetString(flagConsistency)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Snapshot, err = flags.GetString(flagSnapshot)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.NoViews, err = flags.GetBool(flagNoViews)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.NoSequences, err = flags.GetBool(flagNoSequences)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.SortByPk, err = flags.GetBool(flagSortByPk)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.StatusAddr, err = flags.GetString(flagStatusAddr)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Rows, err = flags.GetUint64(flagRows)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Where, err = flags.GetString(flagWhere)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.EscapeBackslash, err = flags.GetBool(flagEscapeBackslash)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.FileType, err = flags.GetString(flagFiletype)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.NoHeader, err = flags.GetBool(flagNoHeader)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.NoSchemas, err = flags.GetBool(flagNoSchemas)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.NoData, err = flags.GetBool(flagNoData)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.CsvNullValue, err = flags.GetString(flagCsvNullValue)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.SQL, err = flags.GetString(flagSQL)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.DumpEmptyDatabase, err = flags.GetBool(flagDumpEmptyDatabase)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Security.CAPath, err = flags.GetString(flagCA)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Security.CertPath, err = flags.GetString(flagCert)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.Security.KeyPath, err = flags.GetString(flagKey)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.CsvSeparator, err = flags.GetString(flagCsvSeparator)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.CsvDelimiter, err = flags.GetString(flagCsvDelimiter)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.CompleteInsert, err = flags.GetBool(flagCompleteInsert)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.ReadTimeout, err = flags.GetDuration(flagReadTimeout)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.TransactionalConsistency, err = flags.GetBool(flagTransactionalConsistency)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.TiDBMemQuotaQuery, err = flags.GetUint64(flagTidbMemQuotaQuery)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tif conf.Threads <= 0 {\n\t\treturn errors.Errorf(\"--threads is set to %d. It should be greater than 0\", conf.Threads)\n\t}\n\tif len(conf.CsvSeparator) == 0 {\n\t\treturn errors.New(\"--csv-separator is set to \\\"\\\". It must not be an empty string\")\n\t}\n\n\tif conf.SessionParams == nil {\n\t\tconf.SessionParams = make(map[string]interface{})\n\t}\n\n\ttablesList, err := flags.GetStringSlice(flagTablesList)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tfileSizeStr, err := flags.GetString(flagFilesize)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tfilters, err := flags.GetStringSlice(flagFilter)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tcaseSensitive, err := flags.GetBool(flagCaseSensitive)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\toutputFilenameFormat, err := flags.GetString(flagOutputFilenameTemplate)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tparams, err := flags.GetStringToString(flagParams)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tconf.specifiedTables = len(tablesList) > 0\n\tconf.Tables, err = GetConfTables(tablesList)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tconf.TableFilter, err = ParseTableFilter(tablesList, filters)\n\tif err != nil {\n\t\treturn errors.Errorf(\"failed to parse filter: %s\", err)\n\t}\n\n\tif !caseSensitive {\n\t\tconf.TableFilter = filter.CaseInsensitive(conf.TableFilter)\n\t}\n\n\tconf.FileSize, err = ParseFileSize(fileSizeStr)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tif outputFilenameFormat == \"\" && conf.SQL != \"\" {\n\t\toutputFilenameFormat = DefaultAnonymousOutputFileTemplateText\n\t}\n\ttmpl, err := ParseOutputFileTemplate(outputFilenameFormat)\n\tif err != nil {\n\t\treturn errors.Errorf(\"failed to parse output filename template (--output-filename-template '%s')\", outputFilenameFormat)\n\t}\n\tconf.OutputFileTemplate = tmpl\n\n\tcompressType, err := flags.GetString(flagCompress)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tconf.CompressType, err = ParseCompressType(compressType)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tfor k, v := range params {\n\t\tconf.SessionParams[k] = v\n\t}\n\n\terr = conf.BackendOptions.ParseFromFlags(pflag.CommandLine)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\treturn nil\n}\n\n// ParseFileSize parses file size from tables-list and filter arguments\nfunc ParseFileSize(fileSizeStr string) (uint64, error) {\n\tif len(fileSizeStr) == 0 {\n\t\treturn UnspecifiedSize, nil\n\t} else if fileSizeMB, err := strconv.ParseUint(fileSizeStr, 10, 64); err == nil {\n\t\tfmt.Printf(\"Warning: -F without unit is not recommended, try using `-F '%dMiB'` in the future\\n\", fileSizeMB)\n\t\treturn fileSizeMB * units.MiB, nil\n\t} else if size, err := units.RAMInBytes(fileSizeStr); err == nil {\n\t\treturn uint64(size), nil\n\t}\n\treturn 0, errors.Errorf(\"failed to parse filesize (-F '%s')\", fileSizeStr)\n}\n\n// ParseTableFilter parses table filter from tables-list and filter arguments\nfunc ParseTableFilter(tablesList, filters []string) (filter.Filter, error) {\n\tif len(tablesList) == 0 {\n\t\treturn filter.Parse(filters)\n\t}\n\n\t// only parse -T when -f is default value. otherwise bail out.\n\tif !sameStringArray(filters, []string{\"*.*\", DefaultTableFilter}) {\n\t\treturn nil, errors.New(\"cannot pass --tables-list and --filter together\")\n\t}\n\n\ttableNames := make([]filter.Table, 0, len(tablesList))\n\tfor _, table := range tablesList {\n\t\tparts := strings.SplitN(table, \".\", 2)\n\t\tif len(parts) < 2 {\n\t\t\treturn nil, errors.Errorf(\"--tables-list only accepts qualified table names, but `%s` lacks a dot\", table)\n\t\t}\n\t\ttableNames = append(tableNames, filter.Table{Schema: parts[0], Name: parts[1]})\n\t}\n\n\treturn filter.NewTablesFilter(tableNames...), nil\n}\n\n// GetConfTables parses tables from tables-list and filter arguments\nfunc GetConfTables(tablesList []string) (DatabaseTables, error) {\n\tdbTables := DatabaseTables{}\n\tvar (\n\t\ttablename    string\n\t\tavgRowLength uint64\n\t)\n\tavgRowLength = 0\n\tfor _, tablename = range tablesList {\n\t\tparts := strings.SplitN(tablename, \".\", 2)\n\t\tif len(parts) < 2 {\n\t\t\treturn nil, errors.Errorf(\"--tables-list only accepts qualified table names, but `%s` lacks a dot\", tablename)\n\t\t}\n\t\tdbName := parts[0]\n\t\ttbName := parts[1]\n\t\tdbTables[dbName] = append(dbTables[dbName], &TableInfo{tbName, avgRowLength, TableTypeBase})\n\t}\n\treturn dbTables, nil\n}\n\n// ParseCompressType parses compressType string to storage.CompressType\nfunc ParseCompressType(compressType string) (storage.CompressType, error) {\n\tswitch compressType {\n\tcase \"\", \"no-compression\":\n\t\treturn storage.NoCompression, nil\n\tcase \"gzip\", \"gz\":\n\t\treturn storage.Gzip, nil\n\tdefault:\n\t\treturn storage.NoCompression, errors.Errorf(\"unknown compress type %s\", compressType)\n\t}\n}\n\nfunc (conf *Config) createExternalStorage(ctx context.Context) (storage.ExternalStorage, error) {\n\tif conf.ExtStorage != nil {\n\t\treturn conf.ExtStorage, nil\n\t}\n\tb, err := storage.ParseBackend(conf.OutputDirPath, &conf.BackendOptions)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\t// TODO: support setting httpClient with certification later\n\treturn storage.New(ctx, b, &storage.ExternalStorageOptions{})\n}\n\nconst (\n\t// UnspecifiedSize means the filesize/statement-size is unspecified\n\tUnspecifiedSize = 0\n\t// DefaultStatementSize is the default statement size\n\tDefaultStatementSize = 1000000\n\t// TiDBMemQuotaQueryName is the session variable TiDBMemQuotaQuery's name in TiDB\n\tTiDBMemQuotaQueryName = \"tidb_mem_quota_query\"\n\t// DefaultTableFilter is the default exclude table filter. It will exclude all system databases\n\tDefaultTableFilter = \"!/^(mysql|sys|INFORMATION_SCHEMA|PERFORMANCE_SCHEMA|METRICS_SCHEMA|INSPECTION_SCHEMA)$/.*\"\n\n\tdefaultDumpThreads        = 128\n\tdefaultDumpGCSafePointTTL = 5 * 60\n\tdefaultEtcdDialTimeOut    = 3 * time.Second\n\n\t// LooseCollationCompatible is used in DM, represents a collation setting for best compatibility.\n\tLooseCollationCompatible = \"loose\"\n\t// StrictCollationCompatible is used in DM, represents a collation setting for correctness.\n\tStrictCollationCompatible = \"strict\"\n\n\tdumplingServiceSafePointPrefix = \"dumpling\"\n)\n\nvar (\n\tdecodeRegionVersion = semver.New(\"3.0.0\")\n\tgcSafePointVersion  = semver.New(\"4.0.0\")\n\ttableSampleVersion  = semver.New(\"5.0.0-nightly\")\n)\n\nfunc adjustConfig(conf *Config, fns ...func(*Config) error) error {\n\tfor _, f := range fns {\n\t\terr := f(conf)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc registerTLSConfig(conf *Config) error {\n\ttlsConfig, err := util.NewTLSConfig(\n\t\tutil.WithCAPath(conf.Security.CAPath),\n\t\tutil.WithCertAndKeyPath(conf.Security.CertPath, conf.Security.KeyPath),\n\t\tutil.WithCAContent(conf.Security.SSLCABytes),\n\t\tutil.WithCertAndKeyContent(conf.Security.SSLCertBytes, conf.Security.SSLKeyBytes),\n\t)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tif tlsConfig == nil {\n\t\treturn nil\n\t}\n\n\tconf.Security.DriveTLSName = \"dumpling\" + uuid.NewString()\n\terr = mysql.RegisterTLSConfig(conf.Security.DriveTLSName, tlsConfig)\n\treturn errors.Trace(err)\n}\n\nfunc validateSpecifiedSQL(conf *Config) error {\n\tif conf.SQL != \"\" && conf.Where != \"\" {\n\t\treturn errors.New(\"can't specify both --sql and --where at the same time. Please try to combine them into --sql\")\n\t}\n\treturn nil\n}\n\nfunc adjustFileFormat(conf *Config) error {\n\tconf.FileType = strings.ToLower(conf.FileType)\n\tswitch conf.FileType {\n\tcase \"\":\n\t\tif conf.SQL != \"\" {\n\t\t\tconf.FileType = FileFormatCSVString\n\t\t} else {\n\t\t\tconf.FileType = FileFormatSQLTextString\n\t\t}\n\tcase FileFormatSQLTextString:\n\t\tif conf.SQL != \"\" {\n\t\t\treturn errors.Errorf(\"unsupported config.FileType '%s' when we specify --sql, please unset --filetype or set it to 'csv'\", conf.FileType)\n\t\t}\n\tcase FileFormatCSVString:\n\tdefault:\n\t\treturn errors.Errorf(\"unknown config.FileType '%s'\", conf.FileType)\n\t}\n\treturn nil\n}\n\nfunc matchMysqlBugversion(info version.ServerInfo) bool {\n\t// if 8.0.3 <= mysql8 version < 8.0.23\n\t// FLUSH TABLES WITH READ LOCK could block other sessions from executing SHOW TABLE STATUS.\n\t// see more in https://dev.mysql.com/doc/relnotes/mysql/8.0/en/news-8-0-23.html\n\tif info.ServerType != version.ServerTypeMySQL {\n\t\treturn false\n\t}\n\tcurrentVersion := info.ServerVersion\n\tbugVersionStart := semver.New(\"8.0.2\")\n\tbugVersionEnd := semver.New(\"8.0.23\")\n\treturn bugVersionStart.LessThan(*currentVersion) && currentVersion.LessThan(*bugVersionEnd)\n}\n", "// Copyright 2020 PingCAP, Inc. Licensed under Apache-2.0.\n\npackage export\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"math/big\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t// import mysql driver\n\t\"github.com/go-sql-driver/mysql\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/failpoint\"\n\tpclog \"github.com/pingcap/log\"\n\t\"github.com/pingcap/tidb/br/pkg/storage\"\n\t\"github.com/pingcap/tidb/br/pkg/summary\"\n\t\"github.com/pingcap/tidb/br/pkg/version\"\n\t\"github.com/pingcap/tidb/dumpling/cli\"\n\ttcontext \"github.com/pingcap/tidb/dumpling/context\"\n\t\"github.com/pingcap/tidb/dumpling/log\"\n\t\"github.com/pingcap/tidb/parser\"\n\t\"github.com/pingcap/tidb/parser/ast\"\n\t\"github.com/pingcap/tidb/parser/format\"\n\t\"github.com/pingcap/tidb/store/helper\"\n\t\"github.com/pingcap/tidb/tablecodec\"\n\t\"github.com/pingcap/tidb/util/codec\"\n\tpd \"github.com/tikv/pd/client\"\n\t\"go.uber.org/zap\"\n\t\"golang.org/x/exp/slices\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\nvar openDBFunc = openDB\n\nvar errEmptyHandleVals = errors.New(\"empty handleVals for TiDB table\")\n\n// Dumper is the dump progress structure\ntype Dumper struct {\n\ttctx      *tcontext.Context\n\tcancelCtx context.CancelFunc\n\tconf      *Config\n\tmetrics   *metrics\n\n\textStore storage.ExternalStorage\n\tdbHandle *sql.DB\n\n\ttidbPDClientForGC             pd.Client\n\tselectTiDBTableRegionFunc     func(tctx *tcontext.Context, conn *BaseConn, meta TableMeta) (pkFields []string, pkVals [][]string, err error)\n\ttotalTables                   int64\n\tcharsetAndDefaultCollationMap map[string]string\n\n\tspeedRecorder *SpeedRecorder\n}\n\n// NewDumper returns a new Dumper\nfunc NewDumper(ctx context.Context, conf *Config) (*Dumper, error) {\n\tfailpoint.Inject(\"setExtStorage\", func(val failpoint.Value) {\n\t\tpath := val.(string)\n\t\tb, err := storage.ParseBackend(path, nil)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\ts, err := storage.New(context.Background(), b, &storage.ExternalStorageOptions{})\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tconf.ExtStorage = s\n\t})\n\n\ttctx, cancelFn := tcontext.Background().WithContext(ctx).WithCancel()\n\td := &Dumper{\n\t\ttctx:                      tctx,\n\t\tconf:                      conf,\n\t\tcancelCtx:                 cancelFn,\n\t\tselectTiDBTableRegionFunc: selectTiDBTableRegion,\n\t\tspeedRecorder:             NewSpeedRecorder(),\n\t}\n\n\tvar err error\n\n\td.metrics = newMetrics(conf.PromFactory, conf.Labels)\n\td.metrics.registerTo(conf.PromRegistry)\n\tdefer func() {\n\t\tif err != nil {\n\t\t\td.metrics.unregisterFrom(conf.PromRegistry)\n\t\t}\n\t}()\n\n\terr = adjustConfig(conf,\n\t\tregisterTLSConfig,\n\t\tvalidateSpecifiedSQL,\n\t\tadjustFileFormat)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\terr = runSteps(d,\n\t\tinitLogger,\n\t\tcreateExternalStore,\n\t\tstartHTTPService,\n\t\topenSQLDB,\n\t\tdetectServerInfo,\n\t\tresolveAutoConsistency,\n\n\t\tvalidateResolveAutoConsistency,\n\t\ttidbSetPDClientForGC,\n\t\ttidbGetSnapshot,\n\t\ttidbStartGCSavepointUpdateService,\n\n\t\tsetSessionParam)\n\treturn d, err\n}\n\n// Dump dumps table from database\n// nolint: gocyclo\nfunc (d *Dumper) Dump() (dumpErr error) {\n\tinitColTypeRowReceiverMap()\n\tvar (\n\t\tconn    *sql.Conn\n\t\terr     error\n\t\tconCtrl ConsistencyController\n\t)\n\ttctx, conf, pool := d.tctx, d.conf, d.dbHandle\n\ttctx.L().Info(\"begin to run Dump\", zap.Stringer(\"conf\", conf))\n\tm := newGlobalMetadata(tctx, d.extStore, conf.Snapshot)\n\trepeatableRead := needRepeatableRead(conf.ServerInfo.ServerType, conf.Consistency)\n\tdefer func() {\n\t\tif dumpErr == nil {\n\t\t\t_ = m.writeGlobalMetaData()\n\t\t}\n\t}()\n\n\t// for consistency lock, we should get table list at first to generate the lock tables SQL\n\tif conf.Consistency == ConsistencyTypeLock {\n\t\tconn, err = createConnWithConsistency(tctx, pool, repeatableRead)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tif err = prepareTableListToDump(tctx, conf, conn); err != nil {\n\t\t\t_ = conn.Close()\n\t\t\treturn err\n\t\t}\n\t\t_ = conn.Close()\n\t}\n\n\tconCtrl, err = NewConsistencyController(tctx, conf, pool)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = conCtrl.Setup(tctx); err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\t// To avoid lock is not released\n\tdefer func() {\n\t\terr = conCtrl.TearDown(tctx)\n\t\tif err != nil {\n\t\t\ttctx.L().Warn(\"fail to tear down consistency controller\", zap.Error(err))\n\t\t}\n\t}()\n\n\tmetaConn, err := createConnWithConsistency(tctx, pool, repeatableRead)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer func() {\n\t\t_ = metaConn.Close()\n\t}()\n\tm.recordStartTime(time.Now())\n\t// for consistency lock, we can write snapshot info after all tables are locked.\n\t// the binlog pos may changed because there is still possible write between we lock tables and write master status.\n\t// but for the locked tables doing replication that starts from metadata is safe.\n\t// for consistency flush, record snapshot after whole tables are locked. The recorded meta info is exactly the locked snapshot.\n\t// for consistency snapshot, we should use the snapshot that we get/set at first in metadata. TiDB will assure the snapshot of TSO.\n\t// for consistency none, the binlog pos in metadata might be earlier than dumped data. We need to enable safe-mode to assure data safety.\n\terr = m.recordGlobalMetaData(metaConn, conf.ServerInfo.ServerType, false)\n\tif err != nil {\n\t\ttctx.L().Info(\"get global metadata failed\", log.ShortError(err))\n\t}\n\n\tif d.conf.CollationCompatible == StrictCollationCompatible {\n\t\t//init charset and default collation map\n\t\td.charsetAndDefaultCollationMap, err = GetCharsetAndDefaultCollation(tctx.Context, metaConn)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// for other consistencies, we should get table list after consistency is set up and GlobalMetaData is cached\n\tif conf.Consistency != ConsistencyTypeLock {\n\t\tif err = prepareTableListToDump(tctx, conf, metaConn); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif err = d.renewSelectTableRegionFuncForLowerTiDB(tctx); err != nil {\n\t\ttctx.L().Info(\"cannot update select table region info for TiDB\", log.ShortError(err))\n\t}\n\n\tatomic.StoreInt64(&d.totalTables, int64(calculateTableCount(conf.Tables)))\n\n\trebuildConn := func(conn *sql.Conn, updateMeta bool) (*sql.Conn, error) {\n\t\t// make sure that the lock connection is still alive\n\t\terr1 := conCtrl.PingContext(tctx)\n\t\tif err1 != nil {\n\t\t\treturn conn, errors.Trace(err1)\n\t\t}\n\t\t// give up the last broken connection\n\t\t_ = conn.Close()\n\t\tnewConn, err1 := createConnWithConsistency(tctx, pool, repeatableRead)\n\t\tif err1 != nil {\n\t\t\treturn conn, errors.Trace(err1)\n\t\t}\n\t\tconn = newConn\n\t\t// renew the master status after connection. dm can't close safe-mode until dm reaches current pos\n\t\tif updateMeta && conf.PosAfterConnect {\n\t\t\terr1 = m.recordGlobalMetaData(conn, conf.ServerInfo.ServerType, true)\n\t\t\tif err1 != nil {\n\t\t\t\treturn conn, errors.Trace(err1)\n\t\t\t}\n\t\t}\n\t\treturn conn, nil\n\t}\n\n\ttaskChan := make(chan Task, defaultDumpThreads)\n\tAddGauge(d.metrics.taskChannelCapacity, defaultDumpThreads)\n\twg, writingCtx := errgroup.WithContext(tctx)\n\twriterCtx := tctx.WithContext(writingCtx)\n\twriters, tearDownWriters, err := d.startWriters(writerCtx, wg, taskChan, rebuildConn)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer tearDownWriters()\n\n\tif conf.TransactionalConsistency {\n\t\tif conf.Consistency == ConsistencyTypeFlush || conf.Consistency == ConsistencyTypeLock {\n\t\t\ttctx.L().Info(\"All the dumping transactions have started. Start to unlock tables\")\n\t\t}\n\t\tif err = conCtrl.TearDown(tctx); err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t}\n\t// Inject consistency failpoint test after we release the table lock\n\tfailpoint.Inject(\"ConsistencyCheck\", nil)\n\n\tif conf.PosAfterConnect {\n\t\t// record again, to provide a location to exit safe mode for DM\n\t\terr = m.recordGlobalMetaData(metaConn, conf.ServerInfo.ServerType, true)\n\t\tif err != nil {\n\t\t\ttctx.L().Info(\"get global metadata (after connection pool established) failed\", log.ShortError(err))\n\t\t}\n\t}\n\n\tsummary.SetLogCollector(summary.NewLogCollector(tctx.L().Info))\n\tsummary.SetUnit(summary.BackupUnit)\n\tdefer summary.Summary(summary.BackupUnit)\n\n\tlogProgressCtx, logProgressCancel := tctx.WithCancel()\n\tgo d.runLogProgress(logProgressCtx)\n\tdefer logProgressCancel()\n\n\ttableDataStartTime := time.Now()\n\n\tfailpoint.Inject(\"PrintTiDBMemQuotaQuery\", func(_ failpoint.Value) {\n\t\trow := d.dbHandle.QueryRowContext(tctx, \"select @@tidb_mem_quota_query;\")\n\t\tvar s string\n\t\terr = row.Scan(&s)\n\t\tif err != nil {\n\t\t\tfmt.Println(errors.Trace(err))\n\t\t} else {\n\t\t\tfmt.Printf(\"tidb_mem_quota_query == %s\\n\", s)\n\t\t}\n\t})\n\tbaseConn := newBaseConn(metaConn, canRebuildConn(conf.Consistency, conf.TransactionalConsistency), rebuildConn)\n\n\tif conf.SQL == \"\" {\n\t\tif err = d.dumpDatabases(writerCtx, baseConn, taskChan); err != nil && !errors.ErrorEqual(err, context.Canceled) {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\td.dumpSQL(writerCtx, baseConn, taskChan)\n\t}\n\tclose(taskChan)\n\t_ = baseConn.DBConn.Close()\n\tif err := wg.Wait(); err != nil {\n\t\tsummary.CollectFailureUnit(\"dump table data\", err)\n\t\treturn errors.Trace(err)\n\t}\n\tsummary.CollectSuccessUnit(\"dump cost\", countTotalTask(writers), time.Since(tableDataStartTime))\n\n\tsummary.SetSuccessStatus(true)\n\tm.recordFinishTime(time.Now())\n\treturn nil\n}\n\nfunc (d *Dumper) startWriters(tctx *tcontext.Context, wg *errgroup.Group, taskChan <-chan Task,\n\trebuildConnFn func(*sql.Conn, bool) (*sql.Conn, error)) ([]*Writer, func(), error) {\n\tconf, pool := d.conf, d.dbHandle\n\twriters := make([]*Writer, conf.Threads)\n\tfor i := 0; i < conf.Threads; i++ {\n\t\tconn, err := createConnWithConsistency(tctx, pool, needRepeatableRead(conf.ServerInfo.ServerType, conf.Consistency))\n\t\tif err != nil {\n\t\t\treturn nil, func() {}, err\n\t\t}\n\t\twriter := NewWriter(tctx, int64(i), conf, conn, d.extStore, d.metrics)\n\t\twriter.rebuildConnFn = rebuildConnFn\n\t\twriter.setFinishTableCallBack(func(task Task) {\n\t\t\tif _, ok := task.(*TaskTableData); ok {\n\t\t\t\tIncCounter(d.metrics.finishedTablesCounter)\n\t\t\t\t// FIXME: actually finishing the last chunk doesn't means this table is 'finished'.\n\t\t\t\t//  We can call this table is 'finished' if all its chunks are finished.\n\t\t\t\t//  Comment this log now to avoid ambiguity.\n\t\t\t\t// tctx.L().Debug(\"finished dumping table data\",\n\t\t\t\t//\tzap.String(\"database\", td.Meta.DatabaseName()),\n\t\t\t\t//\tzap.String(\"table\", td.Meta.TableName()))\n\t\t\t}\n\t\t})\n\t\twriter.setFinishTaskCallBack(func(task Task) {\n\t\t\tIncGauge(d.metrics.taskChannelCapacity)\n\t\t\tif td, ok := task.(*TaskTableData); ok {\n\t\t\t\ttctx.L().Debug(\"finish dumping table data task\",\n\t\t\t\t\tzap.String(\"database\", td.Meta.DatabaseName()),\n\t\t\t\t\tzap.String(\"table\", td.Meta.TableName()),\n\t\t\t\t\tzap.Int(\"chunkIdx\", td.ChunkIndex))\n\t\t\t}\n\t\t})\n\t\twg.Go(func() error {\n\t\t\treturn writer.run(taskChan)\n\t\t})\n\t\twriters[i] = writer\n\t}\n\ttearDown := func() {\n\t\tfor _, w := range writers {\n\t\t\t_ = w.conn.Close()\n\t\t}\n\t}\n\treturn writers, tearDown, nil\n}\n\nfunc (d *Dumper) dumpDatabases(tctx *tcontext.Context, metaConn *BaseConn, taskChan chan<- Task) error {\n\tconf := d.conf\n\tallTables := conf.Tables\n\n\t// policy should be created before database\n\t// placement policy in other server type can be different, so we only handle the tidb server\n\tif conf.ServerInfo.ServerType == version.ServerTypeTiDB {\n\t\tpolicyNames, err := ListAllPlacementPolicyNames(tctx, metaConn)\n\t\tif err != nil {\n\t\t\terrCause := errors.Cause(err)\n\t\t\tif mysqlErr, ok := errCause.(*mysql.MySQLError); ok && mysqlErr.Number == ErrNoSuchTable {\n\t\t\t\t// some old tidb version and other server type doesn't support placement rules, we can skip it.\n\t\t\t\ttctx.L().Debug(\"cannot dump placement policy, maybe the server doesn't support it\", log.ShortError(err))\n\t\t\t} else {\n\t\t\t\ttctx.L().Warn(\"fail to dump placement policy: \", log.ShortError(err))\n\t\t\t}\n\t\t}\n\t\tfor _, policy := range policyNames {\n\t\t\tcreatePolicySQL, err := ShowCreatePlacementPolicy(tctx, metaConn, policy)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\t\t\twrappedCreatePolicySQL := fmt.Sprintf(\"/*T![placement] %s */\", createPolicySQL)\n\t\t\ttask := NewTaskPolicyMeta(policy, wrappedCreatePolicySQL)\n\t\t\tctxDone := d.sendTaskToChan(tctx, task, taskChan)\n\t\t\tif ctxDone {\n\t\t\t\treturn tctx.Err()\n\t\t\t}\n\t\t}\n\t}\n\n\tparser1 := parser.New()\n\tfor dbName, tables := range allTables {\n\t\tif !conf.NoSchemas {\n\t\t\tcreateDatabaseSQL, err := ShowCreateDatabase(tctx, metaConn, dbName)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\n\t\t\t// adjust db collation\n\t\t\tcreateDatabaseSQL, err = adjustDatabaseCollation(tctx, d.conf.CollationCompatible, parser1, createDatabaseSQL, d.charsetAndDefaultCollationMap)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\n\t\t\ttask := NewTaskDatabaseMeta(dbName, createDatabaseSQL)\n\t\t\tctxDone := d.sendTaskToChan(tctx, task, taskChan)\n\t\t\tif ctxDone {\n\t\t\t\treturn tctx.Err()\n\t\t\t}\n\t\t}\n\n\t\tfor _, table := range tables {\n\t\t\ttctx.L().Debug(\"start dumping table...\", zap.String(\"database\", dbName),\n\t\t\t\tzap.String(\"table\", table.Name))\n\t\t\tmeta, err := dumpTableMeta(tctx, conf, metaConn, dbName, table)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\n\t\t\tif !conf.NoSchemas {\n\t\t\t\tswitch table.Type {\n\t\t\t\tcase TableTypeView:\n\t\t\t\t\ttask := NewTaskViewMeta(dbName, table.Name, meta.ShowCreateTable(), meta.ShowCreateView())\n\t\t\t\t\tctxDone := d.sendTaskToChan(tctx, task, taskChan)\n\t\t\t\t\tif ctxDone {\n\t\t\t\t\t\treturn tctx.Err()\n\t\t\t\t\t}\n\t\t\t\tcase TableTypeSequence:\n\t\t\t\t\ttask := NewTaskSequenceMeta(dbName, table.Name, meta.ShowCreateTable())\n\t\t\t\t\tctxDone := d.sendTaskToChan(tctx, task, taskChan)\n\t\t\t\t\tif ctxDone {\n\t\t\t\t\t\treturn tctx.Err()\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\t// adjust table collation\n\t\t\t\t\tnewCreateSQL, err := adjustTableCollation(tctx, d.conf.CollationCompatible, parser1, meta.ShowCreateTable(), d.charsetAndDefaultCollationMap)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn errors.Trace(err)\n\t\t\t\t\t}\n\t\t\t\t\tmeta.(*tableMeta).showCreateTable = newCreateSQL\n\n\t\t\t\t\ttask := NewTaskTableMeta(dbName, table.Name, meta.ShowCreateTable())\n\t\t\t\t\tctxDone := d.sendTaskToChan(tctx, task, taskChan)\n\t\t\t\t\tif ctxDone {\n\t\t\t\t\t\treturn tctx.Err()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif table.Type == TableTypeBase {\n\t\t\t\terr = d.dumpTableData(tctx, metaConn, meta, taskChan)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Trace(err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// adjustDatabaseCollation adjusts db collation and return new create sql and collation\nfunc adjustDatabaseCollation(tctx *tcontext.Context, collationCompatible string, parser *parser.Parser, originSQL string, charsetAndDefaultCollationMap map[string]string) (string, error) {\n\tif collationCompatible != StrictCollationCompatible {\n\t\treturn originSQL, nil\n\t}\n\tstmt, err := parser.ParseOneStmt(originSQL, \"\", \"\")\n\tif err != nil {\n\t\ttctx.L().Warn(\"parse create database error, maybe tidb parser doesn't support it\", zap.String(\"originSQL\", originSQL), log.ShortError(err))\n\t\treturn originSQL, nil\n\t}\n\tcreateStmt, ok := stmt.(*ast.CreateDatabaseStmt)\n\tif !ok {\n\t\treturn originSQL, nil\n\t}\n\tvar charset string\n\tfor _, createOption := range createStmt.Options {\n\t\t// already have 'Collation'\n\t\tif createOption.Tp == ast.DatabaseOptionCollate {\n\t\t\treturn originSQL, nil\n\t\t}\n\t\tif createOption.Tp == ast.DatabaseOptionCharset {\n\t\t\tcharset = createOption.Value\n\t\t}\n\t}\n\t// get db collation\n\tcollation, ok := charsetAndDefaultCollationMap[strings.ToLower(charset)]\n\tif !ok {\n\t\ttctx.L().Warn(\"not found database charset default collation.\", zap.String(\"originSQL\", originSQL), zap.String(\"charset\", strings.ToLower(charset)))\n\t\treturn originSQL, nil\n\t}\n\t// add collation\n\tcreateStmt.Options = append(createStmt.Options, &ast.DatabaseOption{Tp: ast.DatabaseOptionCollate, Value: collation})\n\t// rewrite sql\n\tvar b []byte\n\tbf := bytes.NewBuffer(b)\n\terr = createStmt.Restore(&format.RestoreCtx{\n\t\tFlags: format.DefaultRestoreFlags | format.RestoreTiDBSpecialComment,\n\t\tIn:    bf,\n\t})\n\tif err != nil {\n\t\treturn \"\", errors.Trace(err)\n\t}\n\treturn bf.String(), nil\n}\n\n// adjustTableCollation adjusts table collation\nfunc adjustTableCollation(tctx *tcontext.Context, collationCompatible string, parser *parser.Parser, originSQL string, charsetAndDefaultCollationMap map[string]string) (string, error) {\n\tif collationCompatible != StrictCollationCompatible {\n\t\treturn originSQL, nil\n\t}\n\tstmt, err := parser.ParseOneStmt(originSQL, \"\", \"\")\n\tif err != nil {\n\t\ttctx.L().Warn(\"parse create table error, maybe tidb parser doesn't support it\", zap.String(\"originSQL\", originSQL), log.ShortError(err))\n\t\treturn originSQL, nil\n\t}\n\tcreateStmt, ok := stmt.(*ast.CreateTableStmt)\n\tif !ok {\n\t\treturn originSQL, nil\n\t}\n\tvar charset string\n\tvar collation string\n\tfor _, createOption := range createStmt.Options {\n\t\t// already have 'Collation'\n\t\tif createOption.Tp == ast.TableOptionCollate {\n\t\t\tcollation = createOption.StrValue\n\t\t\tbreak\n\t\t}\n\t\tif createOption.Tp == ast.TableOptionCharset {\n\t\t\tcharset = createOption.StrValue\n\t\t}\n\t}\n\n\tif collation == \"\" && charset != \"\" {\n\t\tcollation, ok := charsetAndDefaultCollationMap[strings.ToLower(charset)]\n\t\tif !ok {\n\t\t\ttctx.L().Warn(\"not found table charset default collation.\", zap.String(\"originSQL\", originSQL), zap.String(\"charset\", strings.ToLower(charset)))\n\t\t\treturn originSQL, nil\n\t\t}\n\n\t\t// add collation\n\t\tcreateStmt.Options = append(createStmt.Options, &ast.TableOption{Tp: ast.TableOptionCollate, StrValue: collation})\n\t}\n\n\t// adjust columns collation\n\tadjustColumnsCollation(tctx, createStmt, charsetAndDefaultCollationMap)\n\n\t// rewrite sql\n\tvar b []byte\n\tbf := bytes.NewBuffer(b)\n\terr = createStmt.Restore(&format.RestoreCtx{\n\t\tFlags: format.DefaultRestoreFlags | format.RestoreTiDBSpecialComment,\n\t\tIn:    bf,\n\t})\n\tif err != nil {\n\t\treturn \"\", errors.Trace(err)\n\t}\n\treturn bf.String(), nil\n}\n\n// adjustColumnsCollation adds column's collation.\nfunc adjustColumnsCollation(tctx *tcontext.Context, createStmt *ast.CreateTableStmt, charsetAndDefaultCollationMap map[string]string) {\nColumnLoop:\n\tfor _, col := range createStmt.Cols {\n\t\tfor _, options := range col.Options {\n\t\t\t// already have 'Collation'\n\t\t\tif options.Tp == ast.ColumnOptionCollate {\n\t\t\t\tcontinue ColumnLoop\n\t\t\t}\n\t\t}\n\t\tfieldType := col.Tp\n\t\tif fieldType.GetCollate() != \"\" {\n\t\t\tcontinue\n\t\t}\n\t\tif fieldType.GetCharset() != \"\" {\n\t\t\t// just have charset\n\t\t\tcollation, ok := charsetAndDefaultCollationMap[strings.ToLower(fieldType.GetCharset())]\n\t\t\tif !ok {\n\t\t\t\ttctx.L().Warn(\"not found charset default collation for column.\", zap.String(\"table\", createStmt.Table.Name.String()), zap.String(\"column\", col.Name.String()), zap.String(\"charset\", strings.ToLower(fieldType.GetCharset())))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfieldType.SetCollate(collation)\n\t\t}\n\t}\n}\n\nfunc (d *Dumper) dumpTableData(tctx *tcontext.Context, conn *BaseConn, meta TableMeta, taskChan chan<- Task) error {\n\tconf := d.conf\n\tif conf.NoData {\n\t\treturn nil\n\t}\n\n\t// Update total rows\n\tfieldName, _ := pickupPossibleField(tctx, meta, conn)\n\tc := estimateCount(tctx, meta.DatabaseName(), meta.TableName(), conn, fieldName, conf)\n\tAddCounter(d.metrics.estimateTotalRowsCounter, float64(c))\n\n\tif conf.Rows == UnspecifiedSize {\n\t\treturn d.sequentialDumpTable(tctx, conn, meta, taskChan)\n\t}\n\treturn d.concurrentDumpTable(tctx, conn, meta, taskChan)\n}\n\nfunc (d *Dumper) buildConcatTask(tctx *tcontext.Context, conn *BaseConn, meta TableMeta) (*TaskTableData, error) {\n\ttableChan := make(chan Task, 128)\n\terrCh := make(chan error, 1)\n\tgo func() {\n\t\t// adjust rows to suitable rows for this table\n\t\td.conf.Rows = GetSuitableRows(meta.AvgRowLength())\n\t\terr := d.concurrentDumpTable(tctx, conn, meta, tableChan)\n\t\td.conf.Rows = UnspecifiedSize\n\t\tif err != nil {\n\t\t\terrCh <- err\n\t\t} else {\n\t\t\tclose(errCh)\n\t\t}\n\t}()\n\ttableDataArr := make([]*tableData, 0)\n\thandleSubTask := func(task Task) {\n\t\ttableTask, ok := task.(*TaskTableData)\n\t\tif !ok {\n\t\t\ttctx.L().Warn(\"unexpected task when splitting table chunks\", zap.String(\"task\", tableTask.Brief()))\n\t\t\treturn\n\t\t}\n\t\ttableDataInst, ok := tableTask.Data.(*tableData)\n\t\tif !ok {\n\t\t\ttctx.L().Warn(\"unexpected task.Data when splitting table chunks\", zap.String(\"task\", tableTask.Brief()))\n\t\t\treturn\n\t\t}\n\t\ttableDataArr = append(tableDataArr, tableDataInst)\n\t}\n\tfor {\n\t\tselect {\n\t\tcase err, ok := <-errCh:\n\t\t\tif !ok {\n\t\t\t\t// make sure all the subtasks in tableChan are handled\n\t\t\t\tfor len(tableChan) > 0 {\n\t\t\t\t\ttask := <-tableChan\n\t\t\t\t\thandleSubTask(task)\n\t\t\t\t}\n\t\t\t\tif len(tableDataArr) <= 1 {\n\t\t\t\t\treturn nil, nil\n\t\t\t\t}\n\t\t\t\tqueries := make([]string, 0, len(tableDataArr))\n\t\t\t\tcolLen := tableDataArr[0].colLen\n\t\t\t\tfor _, tableDataInst := range tableDataArr {\n\t\t\t\t\tqueries = append(queries, tableDataInst.query)\n\t\t\t\t\tif colLen != tableDataInst.colLen {\n\t\t\t\t\t\ttctx.L().Warn(\"colLen varies for same table\",\n\t\t\t\t\t\t\tzap.Int(\"oldColLen\", colLen),\n\t\t\t\t\t\t\tzap.String(\"oldQuery\", queries[0]),\n\t\t\t\t\t\t\tzap.Int(\"newColLen\", tableDataInst.colLen),\n\t\t\t\t\t\t\tzap.String(\"newQuery\", tableDataInst.query))\n\t\t\t\t\t\treturn nil, nil\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn NewTaskTableData(meta, newMultiQueriesChunk(queries, colLen), 0, 1), nil\n\t\t\t}\n\t\t\treturn nil, err\n\t\tcase task := <-tableChan:\n\t\t\thandleSubTask(task)\n\t\t}\n\t}\n}\n\nfunc (d *Dumper) dumpWholeTableDirectly(tctx *tcontext.Context, meta TableMeta, taskChan chan<- Task, partition, orderByClause string, currentChunk, totalChunks int) error {\n\tconf := d.conf\n\ttableIR := SelectAllFromTable(conf, meta, partition, orderByClause)\n\ttask := NewTaskTableData(meta, tableIR, currentChunk, totalChunks)\n\tctxDone := d.sendTaskToChan(tctx, task, taskChan)\n\tif ctxDone {\n\t\treturn tctx.Err()\n\t}\n\treturn nil\n}\n\nfunc (d *Dumper) sequentialDumpTable(tctx *tcontext.Context, conn *BaseConn, meta TableMeta, taskChan chan<- Task) error {\n\tconf := d.conf\n\tif conf.ServerInfo.ServerType == version.ServerTypeTiDB {\n\t\ttask, err := d.buildConcatTask(tctx, conn, meta)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tif task != nil {\n\t\t\tctxDone := d.sendTaskToChan(tctx, task, taskChan)\n\t\t\tif ctxDone {\n\t\t\t\treturn tctx.Err()\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t\ttctx.L().Info(\"didn't build tidb concat sqls, will select all from table now\",\n\t\t\tzap.String(\"database\", meta.DatabaseName()),\n\t\t\tzap.String(\"table\", meta.TableName()))\n\t}\n\torderByClause, err := buildOrderByClause(tctx, conf, conn, meta.DatabaseName(), meta.TableName(), meta.HasImplicitRowID())\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn d.dumpWholeTableDirectly(tctx, meta, taskChan, \"\", orderByClause, 0, 1)\n}\n\n// concurrentDumpTable tries to split table into several chunks to dump\nfunc (d *Dumper) concurrentDumpTable(tctx *tcontext.Context, conn *BaseConn, meta TableMeta, taskChan chan<- Task) error {\n\tconf := d.conf\n\tdb, tbl := meta.DatabaseName(), meta.TableName()\n\tif conf.ServerInfo.ServerType == version.ServerTypeTiDB &&\n\t\tconf.ServerInfo.ServerVersion != nil &&\n\t\t(conf.ServerInfo.ServerVersion.Compare(*tableSampleVersion) >= 0 ||\n\t\t\t(conf.ServerInfo.HasTiKV && conf.ServerInfo.ServerVersion.Compare(*decodeRegionVersion) >= 0)) {\n\t\terr := d.concurrentDumpTiDBTables(tctx, conn, meta, taskChan)\n\t\t// don't retry on context error and successful tasks\n\t\tif err2 := errors.Cause(err); err2 == nil || err2 == context.DeadlineExceeded || err2 == context.Canceled {\n\t\t\treturn err\n\t\t} else if err2 != errEmptyHandleVals {\n\t\t\ttctx.L().Info(\"fallback to concurrent dump tables using rows due to some problem. This won't influence the whole dump process\",\n\t\t\t\tzap.String(\"database\", db), zap.String(\"table\", tbl), log.ShortError(err))\n\t\t}\n\t}\n\n\torderByClause, err := buildOrderByClause(tctx, conf, conn, db, tbl, meta.HasImplicitRowID())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfield, err := pickupPossibleField(tctx, meta, conn)\n\tif err != nil || field == \"\" {\n\t\t// skip split chunk logic if not found proper field\n\t\ttctx.L().Info(\"fallback to sequential dump due to no proper field. This won't influence the whole dump process\",\n\t\t\tzap.String(\"database\", db), zap.String(\"table\", tbl), log.ShortError(err))\n\t\treturn d.dumpWholeTableDirectly(tctx, meta, taskChan, \"\", orderByClause, 0, 1)\n\t}\n\n\tcount := estimateCount(d.tctx, db, tbl, conn, field, conf)\n\ttctx.L().Info(\"get estimated rows count\",\n\t\tzap.String(\"database\", db),\n\t\tzap.String(\"table\", tbl),\n\t\tzap.Uint64(\"estimateCount\", count))\n\tif count < conf.Rows {\n\t\t// skip chunk logic if estimates are low\n\t\ttctx.L().Info(\"fallback to sequential dump due to estimate count < rows. This won't influence the whole dump process\",\n\t\t\tzap.Uint64(\"estimate count\", count),\n\t\t\tzap.Uint64(\"conf.rows\", conf.Rows),\n\t\t\tzap.String(\"database\", db),\n\t\t\tzap.String(\"table\", tbl))\n\t\treturn d.dumpWholeTableDirectly(tctx, meta, taskChan, \"\", orderByClause, 0, 1)\n\t}\n\n\tmin, max, err := d.selectMinAndMaxIntValue(tctx, conn, db, tbl, field)\n\tif err != nil {\n\t\ttctx.L().Info(\"fallback to sequential dump due to cannot get bounding values. This won't influence the whole dump process\",\n\t\t\tlog.ShortError(err))\n\t\treturn d.dumpWholeTableDirectly(tctx, meta, taskChan, \"\", orderByClause, 0, 1)\n\t}\n\ttctx.L().Debug(\"get int bounding values\",\n\t\tzap.String(\"lower\", min.String()),\n\t\tzap.String(\"upper\", max.String()))\n\n\t// every chunk would have eventual adjustments\n\testimatedChunks := count / conf.Rows\n\testimatedStep := new(big.Int).Sub(max, min).Uint64()/estimatedChunks + 1\n\tbigEstimatedStep := new(big.Int).SetUint64(estimatedStep)\n\tcutoff := new(big.Int).Set(min)\n\ttotalChunks := estimatedChunks\n\tif estimatedStep == 1 {\n\t\ttotalChunks = new(big.Int).Sub(max, min).Uint64() + 1\n\t}\n\n\tselectField, selectLen := meta.SelectedField(), meta.SelectedLen()\n\n\tchunkIndex := 0\n\tnullValueCondition := \"\"\n\tif conf.Where == \"\" {\n\t\tnullValueCondition = fmt.Sprintf(\"`%s` IS NULL OR \", escapeString(field))\n\t}\n\tfor max.Cmp(cutoff) >= 0 {\n\t\tnextCutOff := new(big.Int).Add(cutoff, bigEstimatedStep)\n\t\twhere := fmt.Sprintf(\"%s(`%s` >= %d AND `%s` < %d)\", nullValueCondition, escapeString(field), cutoff, escapeString(field), nextCutOff)\n\t\tquery := buildSelectQuery(db, tbl, selectField, \"\", buildWhereCondition(conf, where), orderByClause)\n\t\tif len(nullValueCondition) > 0 {\n\t\t\tnullValueCondition = \"\"\n\t\t}\n\t\ttask := NewTaskTableData(meta, newTableData(query, selectLen, false), chunkIndex, int(totalChunks))\n\t\tctxDone := d.sendTaskToChan(tctx, task, taskChan)\n\t\tif ctxDone {\n\t\t\treturn tctx.Err()\n\t\t}\n\t\tcutoff = nextCutOff\n\t\tchunkIndex++\n\t}\n\treturn nil\n}\n\nfunc (d *Dumper) sendTaskToChan(tctx *tcontext.Context, task Task, taskChan chan<- Task) (ctxDone bool) {\n\tselect {\n\tcase <-tctx.Done():\n\t\treturn true\n\tcase taskChan <- task:\n\t\ttctx.L().Debug(\"send task to writer\",\n\t\t\tzap.String(\"task\", task.Brief()))\n\t\tDecGauge(d.metrics.taskChannelCapacity)\n\t\treturn false\n\t}\n}\n\nfunc (d *Dumper) selectMinAndMaxIntValue(tctx *tcontext.Context, conn *BaseConn, db, tbl, field string) (*big.Int, *big.Int, error) {\n\tconf, zero := d.conf, &big.Int{}\n\tquery := fmt.Sprintf(\"SELECT MIN(`%s`),MAX(`%s`) FROM `%s`.`%s`\",\n\t\tescapeString(field), escapeString(field), escapeString(db), escapeString(tbl))\n\tif conf.Where != \"\" {\n\t\tquery = fmt.Sprintf(\"%s WHERE %s\", query, conf.Where)\n\t}\n\ttctx.L().Debug(\"split chunks\", zap.String(\"query\", query))\n\n\tvar smin sql.NullString\n\tvar smax sql.NullString\n\terr := conn.QuerySQL(tctx, func(rows *sql.Rows) error {\n\t\terr := rows.Scan(&smin, &smax)\n\t\trows.Close()\n\t\treturn err\n\t}, func() {}, query)\n\tif err != nil {\n\t\treturn zero, zero, errors.Annotatef(err, \"can't get min/max values to split chunks, query: %s\", query)\n\t}\n\tif !smax.Valid || !smin.Valid {\n\t\t// found no data\n\t\treturn zero, zero, errors.Errorf(\"no invalid min/max value found in query %s\", query)\n\t}\n\n\tmax := new(big.Int)\n\tmin := new(big.Int)\n\tvar ok bool\n\tif max, ok = max.SetString(smax.String, 10); !ok {\n\t\treturn zero, zero, errors.Errorf(\"fail to convert max value %s in query %s\", smax.String, query)\n\t}\n\tif min, ok = min.SetString(smin.String, 10); !ok {\n\t\treturn zero, zero, errors.Errorf(\"fail to convert min value %s in query %s\", smin.String, query)\n\t}\n\treturn min, max, nil\n}\n\nfunc (d *Dumper) concurrentDumpTiDBTables(tctx *tcontext.Context, conn *BaseConn, meta TableMeta, taskChan chan<- Task) error {\n\tdb, tbl := meta.DatabaseName(), meta.TableName()\n\n\tvar (\n\t\thandleColNames []string\n\t\thandleVals     [][]string\n\t\terr            error\n\t)\n\t// for TiDB v5.0+, we can use table sample directly\n\tif d.conf.ServerInfo.ServerVersion.Compare(*tableSampleVersion) >= 0 {\n\t\ttctx.L().Debug(\"dumping TiDB tables with TABLESAMPLE\",\n\t\t\tzap.String(\"database\", db), zap.String(\"table\", tbl))\n\t\thandleColNames, handleVals, err = selectTiDBTableSample(tctx, conn, meta)\n\t} else {\n\t\t// for TiDB v3.0+, we can use table region decode in TiDB directly\n\t\ttctx.L().Debug(\"dumping TiDB tables with TABLE REGIONS\",\n\t\t\tzap.String(\"database\", db), zap.String(\"table\", tbl))\n\t\tvar partitions []string\n\t\tif d.conf.ServerInfo.ServerVersion.Compare(*gcSafePointVersion) >= 0 {\n\t\t\tpartitions, err = GetPartitionNames(tctx, conn, db, tbl)\n\t\t}\n\t\tif err == nil {\n\t\t\tif len(partitions) == 0 {\n\t\t\t\thandleColNames, handleVals, err = d.selectTiDBTableRegionFunc(tctx, conn, meta)\n\t\t\t} else {\n\t\t\t\treturn d.concurrentDumpTiDBPartitionTables(tctx, conn, meta, taskChan, partitions)\n\t\t\t}\n\t\t}\n\t}\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn d.sendConcurrentDumpTiDBTasks(tctx, meta, taskChan, handleColNames, handleVals, \"\", 0, len(handleVals)+1)\n}\n\nfunc (d *Dumper) concurrentDumpTiDBPartitionTables(tctx *tcontext.Context, conn *BaseConn, meta TableMeta, taskChan chan<- Task, partitions []string) error {\n\tdb, tbl := meta.DatabaseName(), meta.TableName()\n\ttctx.L().Debug(\"dumping TiDB tables with TABLE REGIONS for partition table\",\n\t\tzap.String(\"database\", db), zap.String(\"table\", tbl), zap.Strings(\"partitions\", partitions))\n\n\tstartChunkIdx := 0\n\ttotalChunk := 0\n\tcachedHandleVals := make([][][]string, len(partitions))\n\n\thandleColNames, _, err := selectTiDBRowKeyFields(tctx, conn, meta, checkTiDBTableRegionPkFields)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// cache handleVals here to calculate the total chunks\n\tfor i, partition := range partitions {\n\t\thandleVals, err := selectTiDBPartitionRegion(tctx, conn, db, tbl, partition)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ttotalChunk += len(handleVals) + 1\n\t\tcachedHandleVals[i] = handleVals\n\t}\n\tfor i, partition := range partitions {\n\t\terr := d.sendConcurrentDumpTiDBTasks(tctx, meta, taskChan, handleColNames, cachedHandleVals[i], partition, startChunkIdx, totalChunk)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tstartChunkIdx += len(cachedHandleVals[i]) + 1\n\t}\n\treturn nil\n}\n\nfunc (d *Dumper) sendConcurrentDumpTiDBTasks(tctx *tcontext.Context,\n\tmeta TableMeta, taskChan chan<- Task,\n\thandleColNames []string, handleVals [][]string, partition string, startChunkIdx, totalChunk int) error {\n\tdb, tbl := meta.DatabaseName(), meta.TableName()\n\tif len(handleVals) == 0 {\n\t\tif partition == \"\" {\n\t\t\t// return error to make outside function try using rows method to dump data\n\t\t\treturn errors.Annotatef(errEmptyHandleVals, \"table: `%s`.`%s`\", escapeString(db), escapeString(tbl))\n\t\t}\n\t\treturn d.dumpWholeTableDirectly(tctx, meta, taskChan, partition, buildOrderByClauseString(handleColNames), startChunkIdx, totalChunk)\n\t}\n\tconf := d.conf\n\tselectField, selectLen := meta.SelectedField(), meta.SelectedLen()\n\twhere := buildWhereClauses(handleColNames, handleVals)\n\torderByClause := buildOrderByClauseString(handleColNames)\n\n\tfor i, w := range where {\n\t\tquery := buildSelectQuery(db, tbl, selectField, partition, buildWhereCondition(conf, w), orderByClause)\n\t\ttask := NewTaskTableData(meta, newTableData(query, selectLen, false), i+startChunkIdx, totalChunk)\n\t\tctxDone := d.sendTaskToChan(tctx, task, taskChan)\n\t\tif ctxDone {\n\t\t\treturn tctx.Err()\n\t\t}\n\t}\n\treturn nil\n}\n\n// L returns real logger\nfunc (d *Dumper) L() log.Logger {\n\treturn d.tctx.L()\n}\n\nfunc selectTiDBTableSample(tctx *tcontext.Context, conn *BaseConn, meta TableMeta) (pkFields []string, pkVals [][]string, err error) {\n\tpkFields, pkColTypes, err := selectTiDBRowKeyFields(tctx, conn, meta, nil)\n\tif err != nil {\n\t\treturn nil, nil, errors.Trace(err)\n\t}\n\n\tquery := buildTiDBTableSampleQuery(pkFields, meta.DatabaseName(), meta.TableName())\n\tpkValNum := len(pkFields)\n\tvar iter SQLRowIter\n\trowRec := MakeRowReceiver(pkColTypes)\n\tbuf := new(bytes.Buffer)\n\n\terr = conn.QuerySQL(tctx, func(rows *sql.Rows) error {\n\t\tif iter == nil {\n\t\t\titer = &rowIter{\n\t\t\t\trows: rows,\n\t\t\t\targs: make([]interface{}, pkValNum),\n\t\t\t}\n\t\t}\n\t\terr = iter.Decode(rowRec)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tpkValRow := make([]string, 0, pkValNum)\n\t\tfor _, rec := range rowRec.receivers {\n\t\t\trec.WriteToBuffer(buf, true)\n\t\t\tpkValRow = append(pkValRow, buf.String())\n\t\t\tbuf.Reset()\n\t\t}\n\t\tpkVals = append(pkVals, pkValRow)\n\t\treturn nil\n\t}, func() {\n\t\tif iter != nil {\n\t\t\t_ = iter.Close()\n\t\t\titer = nil\n\t\t}\n\t\trowRec = MakeRowReceiver(pkColTypes)\n\t\tpkVals = pkVals[:0]\n\t\tbuf.Reset()\n\t}, query)\n\tif err == nil && iter != nil && iter.Error() != nil {\n\t\terr = iter.Error()\n\t}\n\n\treturn pkFields, pkVals, err\n}\n\nfunc buildTiDBTableSampleQuery(pkFields []string, dbName, tblName string) string {\n\ttemplate := \"SELECT %s FROM `%s`.`%s` TABLESAMPLE REGIONS() ORDER BY %s\"\n\tquotaPk := make([]string, len(pkFields))\n\tfor i, s := range pkFields {\n\t\tquotaPk[i] = fmt.Sprintf(\"`%s`\", escapeString(s))\n\t}\n\tpks := strings.Join(quotaPk, \",\")\n\treturn fmt.Sprintf(template, pks, escapeString(dbName), escapeString(tblName), pks)\n}\n\nfunc selectTiDBRowKeyFields(tctx *tcontext.Context, conn *BaseConn, meta TableMeta, checkPkFields func([]string, []string) error) (pkFields, pkColTypes []string, err error) {\n\tif meta.HasImplicitRowID() {\n\t\tpkFields, pkColTypes = []string{\"_tidb_rowid\"}, []string{\"BIGINT\"}\n\t} else {\n\t\tpkFields, pkColTypes, err = GetPrimaryKeyAndColumnTypes(tctx, conn, meta)\n\t\tif err == nil {\n\t\t\tif checkPkFields != nil {\n\t\t\t\terr = checkPkFields(pkFields, pkColTypes)\n\t\t\t}\n\t\t}\n\t}\n\treturn\n}\n\nfunc checkTiDBTableRegionPkFields(pkFields, pkColTypes []string) (err error) {\n\tif len(pkFields) != 1 || len(pkColTypes) != 1 {\n\t\terr = errors.Errorf(\"unsupported primary key for selectTableRegion. pkFields: [%s], pkColTypes: [%s]\", strings.Join(pkFields, \", \"), strings.Join(pkColTypes, \", \"))\n\t\treturn\n\t}\n\tif _, ok := dataTypeInt[pkColTypes[0]]; !ok {\n\t\terr = errors.Errorf(\"unsupported primary key type for selectTableRegion. pkFields: [%s], pkColTypes: [%s]\", strings.Join(pkFields, \", \"), strings.Join(pkColTypes, \", \"))\n\t}\n\treturn\n}\n\nfunc selectTiDBTableRegion(tctx *tcontext.Context, conn *BaseConn, meta TableMeta) (pkFields []string, pkVals [][]string, err error) {\n\tpkFields, _, err = selectTiDBRowKeyFields(tctx, conn, meta, checkTiDBTableRegionPkFields)\n\tif err != nil {\n\t\treturn\n\t}\n\n\tvar (\n\t\tstartKey, decodedKey sql.NullString\n\t\trowID                = -1\n\t)\n\tconst (\n\t\ttableRegionSQL = \"SELECT START_KEY,tidb_decode_key(START_KEY) from INFORMATION_SCHEMA.TIKV_REGION_STATUS s WHERE s.DB_NAME = ? AND s.TABLE_NAME = ? AND IS_INDEX = 0 ORDER BY START_KEY;\"\n\t\ttidbRowID      = \"_tidb_rowid=\"\n\t)\n\tdbName, tableName := meta.DatabaseName(), meta.TableName()\n\tlogger := tctx.L().With(zap.String(\"database\", dbName), zap.String(\"table\", tableName))\n\terr = conn.QuerySQL(tctx, func(rows *sql.Rows) error {\n\t\trowID++\n\t\terr = rows.Scan(&startKey, &decodedKey)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\t// first region's start key has no use. It may come from another table or might be invalid\n\t\tif rowID == 0 {\n\t\t\treturn nil\n\t\t}\n\t\tif !startKey.Valid {\n\t\t\tlogger.Debug(\"meet invalid start key\", zap.Int(\"rowID\", rowID))\n\t\t\treturn nil\n\t\t}\n\t\tif !decodedKey.Valid {\n\t\t\tlogger.Debug(\"meet invalid decoded start key\", zap.Int(\"rowID\", rowID), zap.String(\"startKey\", startKey.String))\n\t\t\treturn nil\n\t\t}\n\t\tpkVal, err2 := extractTiDBRowIDFromDecodedKey(tidbRowID, decodedKey.String)\n\t\tif err2 != nil {\n\t\t\tlogger.Debug(\"cannot extract pkVal from decoded start key\",\n\t\t\t\tzap.Int(\"rowID\", rowID), zap.String(\"startKey\", startKey.String), zap.String(\"decodedKey\", decodedKey.String), log.ShortError(err2))\n\t\t} else {\n\t\t\tpkVals = append(pkVals, []string{pkVal})\n\t\t}\n\t\treturn nil\n\t}, func() {\n\t\tpkFields = pkFields[:0]\n\t\tpkVals = pkVals[:0]\n\t}, tableRegionSQL, dbName, tableName)\n\n\treturn pkFields, pkVals, errors.Trace(err)\n}\n\nfunc selectTiDBPartitionRegion(tctx *tcontext.Context, conn *BaseConn, dbName, tableName, partition string) (pkVals [][]string, err error) {\n\tvar startKeys [][]string\n\tconst (\n\t\tpartitionRegionSQL = \"SHOW TABLE `%s`.`%s` PARTITION(`%s`) REGIONS\"\n\t\tregionRowKey       = \"r_\"\n\t)\n\tlogger := tctx.L().With(zap.String(\"database\", dbName), zap.String(\"table\", tableName), zap.String(\"partition\", partition))\n\tstartKeys, err = conn.QuerySQLWithColumns(tctx, []string{\"START_KEY\"}, fmt.Sprintf(partitionRegionSQL, escapeString(dbName), escapeString(tableName), escapeString(partition)))\n\tif err != nil {\n\t\treturn\n\t}\n\tfor rowID, startKey := range startKeys {\n\t\tif rowID == 0 || len(startKey) != 1 {\n\t\t\tcontinue\n\t\t}\n\t\tpkVal, err2 := extractTiDBRowIDFromDecodedKey(regionRowKey, startKey[0])\n\t\tif err2 != nil {\n\t\t\tlogger.Debug(\"show table region start key doesn't have rowID\",\n\t\t\t\tzap.Int(\"rowID\", rowID), zap.String(\"startKey\", startKey[0]), zap.Error(err2))\n\t\t} else {\n\t\t\tpkVals = append(pkVals, []string{pkVal})\n\t\t}\n\t}\n\n\treturn pkVals, nil\n}\n\nfunc extractTiDBRowIDFromDecodedKey(indexField, key string) (string, error) {\n\tif p := strings.Index(key, indexField); p != -1 {\n\t\tp += len(indexField)\n\t\treturn key[p:], nil\n\t}\n\treturn \"\", errors.Errorf(\"decoded key %s doesn't have %s field\", key, indexField)\n}\n\nfunc getListTableTypeByConf(conf *Config) listTableType {\n\t// use listTableByShowTableStatus by default because it has better performance\n\tlistType := listTableByShowTableStatus\n\tif conf.Consistency == ConsistencyTypeLock {\n\t\t// for consistency lock, we need to build the tables to dump as soon as possible\n\t\tlistType = listTableByInfoSchema\n\t} else if conf.Consistency == ConsistencyTypeFlush && matchMysqlBugversion(conf.ServerInfo) {\n\t\t// For some buggy versions of mysql, we need a workaround to get a list of table names.\n\t\tlistType = listTableByShowFullTables\n\t}\n\treturn listType\n}\n\nfunc prepareTableListToDump(tctx *tcontext.Context, conf *Config, db *sql.Conn) error {\n\tif conf.specifiedTables {\n\t\treturn nil\n\t}\n\tdatabases, err := prepareDumpingDatabases(tctx, conf, db)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ttableTypes := []TableType{TableTypeBase}\n\tif !conf.NoViews {\n\t\ttableTypes = append(tableTypes, TableTypeView)\n\t}\n\tif !conf.NoSequences {\n\t\ttableTypes = append(tableTypes, TableTypeSequence)\n\t}\n\n\tifSeqExists, err := CheckIfSeqExists(db)\n\tif err != nil {\n\t\treturn err\n\t}\n\tvar listType listTableType\n\tif ifSeqExists {\n\t\tlistType = listTableByShowFullTables\n\t} else {\n\t\tlistType = getListTableTypeByConf(conf)\n\t}\n\n\tconf.Tables, err = ListAllDatabasesTables(tctx, db, databases, listType, tableTypes...)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfilterTables(tctx, conf)\n\treturn nil\n}\n\nfunc dumpTableMeta(tctx *tcontext.Context, conf *Config, conn *BaseConn, db string, table *TableInfo) (TableMeta, error) {\n\ttbl := table.Name\n\tselectField, selectLen, err := buildSelectField(tctx, conn, db, tbl, conf.CompleteInsert)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar (\n\t\tcolTypes         []*sql.ColumnType\n\t\thasImplicitRowID bool\n\t)\n\tif conf.ServerInfo.ServerType == version.ServerTypeTiDB {\n\t\thasImplicitRowID, err = SelectTiDBRowID(tctx, conn, db, tbl)\n\t\tif err != nil {\n\t\t\ttctx.L().Info(\"check implicit rowID failed\", zap.String(\"database\", db), zap.String(\"table\", tbl), log.ShortError(err))\n\t\t}\n\t}\n\n\t// If all columns are generated\n\tif table.Type == TableTypeBase {\n\t\tif selectField == \"\" {\n\t\t\tcolTypes, err = GetColumnTypes(tctx, conn, \"*\", db, tbl)\n\t\t} else {\n\t\t\tcolTypes, err = GetColumnTypes(tctx, conn, selectField, db, tbl)\n\t\t}\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tmeta := &tableMeta{\n\t\tavgRowLength:     table.AvgRowLength,\n\t\tdatabase:         db,\n\t\ttable:            tbl,\n\t\tcolTypes:         colTypes,\n\t\tselectedField:    selectField,\n\t\tselectedLen:      selectLen,\n\t\thasImplicitRowID: hasImplicitRowID,\n\t\tspecCmts: []string{\n\t\t\t\"/*!40101 SET NAMES binary*/;\",\n\t\t},\n\t}\n\n\tif conf.NoSchemas {\n\t\treturn meta, nil\n\t}\n\tswitch table.Type {\n\tcase TableTypeView:\n\t\tviewName := table.Name\n\t\tcreateTableSQL, createViewSQL, err1 := ShowCreateView(tctx, conn, db, viewName)\n\t\tif err1 != nil {\n\t\t\treturn meta, err1\n\t\t}\n\t\tmeta.showCreateTable = createTableSQL\n\t\tmeta.showCreateView = createViewSQL\n\t\treturn meta, nil\n\tcase TableTypeSequence:\n\t\tsequenceName := table.Name\n\t\tcreateSequenceSQL, err2 := ShowCreateSequence(tctx, conn, db, sequenceName, conf)\n\t\tif err2 != nil {\n\t\t\treturn meta, err2\n\t\t}\n\t\tmeta.showCreateTable = createSequenceSQL\n\t\treturn meta, nil\n\t}\n\n\tcreateTableSQL, err := ShowCreateTable(tctx, conn, db, tbl)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tmeta.showCreateTable = createTableSQL\n\treturn meta, nil\n}\n\nfunc (d *Dumper) dumpSQL(tctx *tcontext.Context, metaConn *BaseConn, taskChan chan<- Task) {\n\tconf := d.conf\n\tmeta := &tableMeta{}\n\tdata := newTableData(conf.SQL, 0, true)\n\ttask := NewTaskTableData(meta, data, 0, 1)\n\tc := detectEstimateRows(tctx, metaConn, fmt.Sprintf(\"EXPLAIN %s\", conf.SQL), []string{\"rows\", \"estRows\", \"count\"})\n\tAddCounter(d.metrics.estimateTotalRowsCounter, float64(c))\n\tatomic.StoreInt64(&d.totalTables, int64(1))\n\td.sendTaskToChan(tctx, task, taskChan)\n}\n\nfunc canRebuildConn(consistency string, trxConsistencyOnly bool) bool {\n\tswitch consistency {\n\tcase ConsistencyTypeLock, ConsistencyTypeFlush:\n\t\treturn !trxConsistencyOnly\n\tcase ConsistencyTypeSnapshot, ConsistencyTypeNone:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// Close closes a Dumper and stop dumping immediately\nfunc (d *Dumper) Close() error {\n\td.cancelCtx()\n\td.metrics.unregisterFrom(d.conf.PromRegistry)\n\tif d.dbHandle != nil {\n\t\treturn d.dbHandle.Close()\n\t}\n\tif d.conf.Security.DriveTLSName != \"\" {\n\t\tmysql.DeregisterTLSConfig(d.conf.Security.DriveTLSName)\n\t}\n\treturn nil\n}\n\nfunc runSteps(d *Dumper, steps ...func(*Dumper) error) error {\n\tfor _, st := range steps {\n\t\terr := st(d)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc initLogger(d *Dumper) error {\n\tconf := d.conf\n\tvar (\n\t\tlogger log.Logger\n\t\terr    error\n\t\tprops  *pclog.ZapProperties\n\t)\n\t// conf.Logger != nil means dumpling is used as a library\n\tif conf.Logger != nil {\n\t\tlogger = log.NewAppLogger(conf.Logger)\n\t} else {\n\t\tlogger, props, err = log.InitAppLogger(&log.Config{\n\t\t\tLevel:  conf.LogLevel,\n\t\t\tFile:   conf.LogFile,\n\t\t\tFormat: conf.LogFormat,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tpclog.ReplaceGlobals(logger.Logger, props)\n\t\tcli.LogLongVersion(logger)\n\t}\n\td.tctx = d.tctx.WithLogger(logger)\n\treturn nil\n}\n\n// createExternalStore is an initialization step of Dumper.\nfunc createExternalStore(d *Dumper) error {\n\ttctx, conf := d.tctx, d.conf\n\textStore, err := conf.createExternalStorage(tctx)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\td.extStore = extStore\n\treturn nil\n}\n\n// startHTTPService is an initialization step of Dumper.\nfunc startHTTPService(d *Dumper) error {\n\tconf := d.conf\n\tif conf.StatusAddr != \"\" {\n\t\tgo func() {\n\t\t\terr := startDumplingService(d.tctx, conf.StatusAddr)\n\t\t\tif err != nil {\n\t\t\t\td.L().Info(\"meet error when stopping dumpling http service\", log.ShortError(err))\n\t\t\t}\n\t\t}()\n\t}\n\treturn nil\n}\n\n// openSQLDB is an initialization step of Dumper.\nfunc openSQLDB(d *Dumper) error {\n\tconf := d.conf\n\tc, err := mysql.NewConnector(conf.GetDriverConfig(\"\"))\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\td.dbHandle = sql.OpenDB(c)\n\treturn nil\n}\n\n// detectServerInfo is an initialization step of Dumper.\nfunc detectServerInfo(d *Dumper) error {\n\tdb, conf := d.dbHandle, d.conf\n\tversionStr, err := version.FetchVersion(d.tctx.Context, db)\n\tif err != nil {\n\t\tconf.ServerInfo = ServerInfoUnknown\n\t\treturn err\n\t}\n\tconf.ServerInfo = version.ParseServerInfo(versionStr)\n\treturn nil\n}\n\n// resolveAutoConsistency is an initialization step of Dumper.\nfunc resolveAutoConsistency(d *Dumper) error {\n\tconf := d.conf\n\tif conf.Consistency != ConsistencyTypeAuto {\n\t\treturn nil\n\t}\n\tswitch conf.ServerInfo.ServerType {\n\tcase version.ServerTypeTiDB:\n\t\tconf.Consistency = ConsistencyTypeSnapshot\n\tcase version.ServerTypeMySQL, version.ServerTypeMariaDB:\n\t\tconf.Consistency = ConsistencyTypeFlush\n\tdefault:\n\t\tconf.Consistency = ConsistencyTypeNone\n\t}\n\n\tif conf.Consistency == ConsistencyTypeFlush {\n\t\ttimeout := time.Second * 5\n\t\tctx, cancel := context.WithTimeout(d.tctx.Context, timeout)\n\t\tdefer cancel()\n\n\t\t// probe if upstream has enough privilege to FLUSH TABLE WITH READ LOCK\n\t\tconn, err := d.dbHandle.Conn(ctx)\n\t\tif err != nil {\n\t\t\treturn errors.New(\"failed to get connection from db pool after 5 seconds\")\n\t\t}\n\t\t//nolint: errcheck\n\t\tdefer conn.Close()\n\n\t\terr = FlushTableWithReadLock(d.tctx, conn)\n\t\t//nolint: errcheck\n\t\tdefer UnlockTables(d.tctx, conn)\n\t\tif err != nil {\n\t\t\t// fallback to ConsistencyTypeLock\n\t\t\td.tctx.L().Warn(\"error when use FLUSH TABLE WITH READ LOCK, fallback to LOCK TABLES\",\n\t\t\t\tzap.Error(err))\n\t\t\tconf.Consistency = ConsistencyTypeLock\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc validateResolveAutoConsistency(d *Dumper) error {\n\tconf := d.conf\n\tif conf.Consistency != ConsistencyTypeSnapshot && conf.Snapshot != \"\" {\n\t\treturn errors.Errorf(\"can't specify --snapshot when --consistency isn't snapshot, resolved consistency: %s\", conf.Consistency)\n\t}\n\treturn nil\n}\n\n// tidbSetPDClientForGC is an initialization step of Dumper.\nfunc tidbSetPDClientForGC(d *Dumper) error {\n\ttctx, si, pool := d.tctx, d.conf.ServerInfo, d.dbHandle\n\tif si.ServerType != version.ServerTypeTiDB ||\n\t\tsi.ServerVersion == nil ||\n\t\tsi.ServerVersion.Compare(*gcSafePointVersion) < 0 {\n\t\treturn nil\n\t}\n\tpdAddrs, err := GetPdAddrs(tctx, pool)\n\tif err != nil {\n\t\ttctx.L().Info(\"meet some problem while fetching pd addrs. This won't affect dump process\", log.ShortError(err))\n\t\treturn nil\n\t}\n\tif len(pdAddrs) > 0 {\n\t\tdoPdGC, err := checkSameCluster(tctx, pool, pdAddrs)\n\t\tif err != nil {\n\t\t\ttctx.L().Info(\"meet error while check whether fetched pd addr and TiDB belong to one cluster. This won't affect dump process\", log.ShortError(err), zap.Strings(\"pdAddrs\", pdAddrs))\n\t\t} else if doPdGC {\n\t\t\tpdClient, err := pd.NewClientWithContext(tctx, pdAddrs, pd.SecurityOption{})\n\t\t\tif err != nil {\n\t\t\t\ttctx.L().Info(\"create pd client to control GC failed. This won't affect dump process\", log.ShortError(err), zap.Strings(\"pdAddrs\", pdAddrs))\n\t\t\t}\n\t\t\td.tidbPDClientForGC = pdClient\n\t\t}\n\t}\n\treturn nil\n}\n\n// tidbGetSnapshot is an initialization step of Dumper.\nfunc tidbGetSnapshot(d *Dumper) error {\n\tconf, doPdGC := d.conf, d.tidbPDClientForGC != nil\n\tconsistency := conf.Consistency\n\tpool, tctx := d.dbHandle, d.tctx\n\tsnapshotConsistency := consistency == \"snapshot\"\n\tif conf.Snapshot == \"\" && (doPdGC || snapshotConsistency) {\n\t\tconn, err := pool.Conn(tctx)\n\t\tif err != nil {\n\t\t\ttctx.L().Warn(\"fail to open connection to get snapshot from TiDB\", log.ShortError(err))\n\t\t\t// for consistency snapshot, we must get a snapshot here, or we will dump inconsistent data, but for other consistency we can ignore this error.\n\t\t\tif !snapshotConsistency {\n\t\t\t\terr = nil\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\tsnapshot, err := getSnapshot(conn)\n\t\t_ = conn.Close()\n\t\tif err != nil {\n\t\t\ttctx.L().Warn(\"fail to get snapshot from TiDB\", log.ShortError(err))\n\t\t\t// for consistency snapshot, we must get a snapshot here, or we will dump inconsistent data, but for other consistency we can ignore this error.\n\t\t\tif !snapshotConsistency {\n\t\t\t\terr = nil\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\tconf.Snapshot = snapshot\n\t}\n\treturn nil\n}\n\n// tidbStartGCSavepointUpdateService is an initialization step of Dumper.\nfunc tidbStartGCSavepointUpdateService(d *Dumper) error {\n\ttctx, pool, conf := d.tctx, d.dbHandle, d.conf\n\tsnapshot, si := conf.Snapshot, conf.ServerInfo\n\tif d.tidbPDClientForGC != nil {\n\t\tsnapshotTS, err := parseSnapshotToTSO(pool, snapshot)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgo updateServiceSafePoint(tctx, d.tidbPDClientForGC, defaultDumpGCSafePointTTL, snapshotTS)\n\t} else if si.ServerType == version.ServerTypeTiDB {\n\t\ttctx.L().Warn(\"If the amount of data to dump is large, criteria: (data more than 60GB or dumped time more than 10 minutes)\\n\" +\n\t\t\t\"you'd better adjust the tikv_gc_life_time to avoid export failure due to TiDB GC during the dump process.\\n\" +\n\t\t\t\"Before dumping: run sql `update mysql.tidb set VARIABLE_VALUE = '720h' where VARIABLE_NAME = 'tikv_gc_life_time';` in tidb.\\n\" +\n\t\t\t\"After dumping: run sql `update mysql.tidb set VARIABLE_VALUE = '10m' where VARIABLE_NAME = 'tikv_gc_life_time';` in tidb.\\n\")\n\t}\n\treturn nil\n}\n\nfunc updateServiceSafePoint(tctx *tcontext.Context, pdClient pd.Client, ttl int64, snapshotTS uint64) {\n\tupdateInterval := time.Duration(ttl/2) * time.Second\n\ttick := time.NewTicker(updateInterval)\n\tdumplingServiceSafePointID := fmt.Sprintf(\"%s_%d\", dumplingServiceSafePointPrefix, time.Now().UnixNano())\n\ttctx.L().Info(\"generate dumpling gc safePoint id\", zap.String(\"id\", dumplingServiceSafePointID))\n\n\tfor {\n\t\ttctx.L().Debug(\"update PD safePoint limit with ttl\",\n\t\t\tzap.Uint64(\"safePoint\", snapshotTS),\n\t\t\tzap.Int64(\"ttl\", ttl))\n\t\tfor retryCnt := 0; retryCnt <= 10; retryCnt++ {\n\t\t\t_, err := pdClient.UpdateServiceGCSafePoint(tctx, dumplingServiceSafePointID, ttl, snapshotTS)\n\t\t\tif err == nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\ttctx.L().Debug(\"update PD safePoint failed\", log.ShortError(err), zap.Int(\"retryTime\", retryCnt))\n\t\t\tselect {\n\t\t\tcase <-tctx.Done():\n\t\t\t\treturn\n\t\t\tcase <-time.After(time.Second):\n\t\t\t}\n\t\t}\n\t\tselect {\n\t\tcase <-tctx.Done():\n\t\t\treturn\n\t\tcase <-tick.C:\n\t\t}\n\t}\n}\n\n// setSessionParam is an initialization step of Dumper.\nfunc setSessionParam(d *Dumper) error {\n\tconf, pool := d.conf, d.dbHandle\n\tsi := conf.ServerInfo\n\tconsistency, snapshot := conf.Consistency, conf.Snapshot\n\tsessionParam := conf.SessionParams\n\tif si.ServerType == version.ServerTypeTiDB && conf.TiDBMemQuotaQuery != UnspecifiedSize {\n\t\tsessionParam[TiDBMemQuotaQueryName] = conf.TiDBMemQuotaQuery\n\t}\n\tvar err error\n\tif snapshot != \"\" {\n\t\tif si.ServerType != version.ServerTypeTiDB {\n\t\t\treturn errors.New(\"snapshot consistency is not supported for this server\")\n\t\t}\n\t\tif consistency == ConsistencyTypeSnapshot {\n\t\t\tconf.ServerInfo.HasTiKV, err = CheckTiDBWithTiKV(pool)\n\t\t\tif err != nil {\n\t\t\t\td.L().Info(\"cannot check whether TiDB has TiKV, will apply tidb_snapshot by default. This won't affect dump process\", log.ShortError(err))\n\t\t\t}\n\t\t\tif conf.ServerInfo.HasTiKV {\n\t\t\t\tsessionParam[\"tidb_snapshot\"] = snapshot\n\t\t\t}\n\t\t}\n\t}\n\tif d.dbHandle, err = resetDBWithSessionParams(d.tctx, pool, conf.GetDriverConfig(\"\"), conf.SessionParams); err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\treturn nil\n}\n\nfunc openDB(cfg *mysql.Config) (*sql.DB, error) {\n\tc, err := mysql.NewConnector(cfg)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\treturn sql.OpenDB(c), nil\n}\n\nfunc (d *Dumper) renewSelectTableRegionFuncForLowerTiDB(tctx *tcontext.Context) error {\n\tconf := d.conf\n\tif !(conf.ServerInfo.ServerType == version.ServerTypeTiDB && conf.ServerInfo.ServerVersion != nil && conf.ServerInfo.HasTiKV &&\n\t\tconf.ServerInfo.ServerVersion.Compare(*decodeRegionVersion) >= 0 &&\n\t\tconf.ServerInfo.ServerVersion.Compare(*gcSafePointVersion) < 0) {\n\t\ttctx.L().Debug(\"no need to build region info because database is not TiDB 3.x\")\n\t\treturn nil\n\t}\n\t// for TiDB v3.0+, the original selectTiDBTableRegionFunc will always fail,\n\t// because TiDB v3.0 doesn't have `tidb_decode_key` function nor `DB_NAME`,`TABLE_NAME` columns in `INFORMATION_SCHEMA.TIKV_REGION_STATUS`.\n\t// reference: https://github.com/pingcap/tidb/blob/c497d5c/dumpling/export/dump.go#L775\n\t// To avoid this function continuously returning errors and confusing users because we fail to init this function at first,\n\t// selectTiDBTableRegionFunc is set to always return an ignorable error at first.\n\td.selectTiDBTableRegionFunc = func(_ *tcontext.Context, _ *BaseConn, meta TableMeta) (pkFields []string, pkVals [][]string, err error) {\n\t\treturn nil, nil, errors.Annotatef(errEmptyHandleVals, \"table: `%s`.`%s`\", escapeString(meta.DatabaseName()), escapeString(meta.TableName()))\n\t}\n\tdbHandle, err := openDBFunc(conf.GetDriverConfig(\"\"))\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tdefer func() {\n\t\t_ = dbHandle.Close()\n\t}()\n\tconn, err := dbHandle.Conn(tctx)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tdefer func() {\n\t\t_ = conn.Close()\n\t}()\n\tdbInfos, err := GetDBInfo(conn, DatabaseTablesToMap(conf.Tables))\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tregionsInfo, err := GetRegionInfos(conn)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\ttikvHelper := &helper.Helper{}\n\ttableInfos := tikvHelper.GetRegionsTableInfo(regionsInfo, dbInfos)\n\n\ttableInfoMap := make(map[string]map[string][]int64, len(conf.Tables))\n\tfor _, region := range regionsInfo.Regions {\n\t\ttableList := tableInfos[region.ID]\n\t\tfor _, table := range tableList {\n\t\t\tdb, tbl := table.DB.Name.O, table.Table.Name.O\n\t\t\tif _, ok := tableInfoMap[db]; !ok {\n\t\t\t\ttableInfoMap[db] = make(map[string][]int64, len(conf.Tables[db]))\n\t\t\t}\n\n\t\t\tkey, err := hex.DecodeString(region.StartKey)\n\t\t\tif err != nil {\n\t\t\t\td.L().Debug(\"invalid region start key\", log.ShortError(err), zap.String(\"key\", region.StartKey))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Auto decode byte if needed.\n\t\t\t_, bs, err := codec.DecodeBytes(key, nil)\n\t\t\tif err == nil {\n\t\t\t\tkey = bs\n\t\t\t}\n\t\t\t// Try to decode it as a record key.\n\t\t\ttableID, handle, err := tablecodec.DecodeRecordKey(key)\n\t\t\tif err != nil {\n\t\t\t\td.L().Debug(\"cannot decode region start key\", log.ShortError(err), zap.String(\"key\", region.StartKey), zap.Int64(\"tableID\", tableID))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif handle.IsInt() {\n\t\t\t\ttableInfoMap[db][tbl] = append(tableInfoMap[db][tbl], handle.IntValue())\n\t\t\t} else {\n\t\t\t\td.L().Debug(\"not an int handle\", log.ShortError(err), zap.Stringer(\"handle\", handle))\n\t\t\t}\n\t\t}\n\t}\n\tfor _, tbInfos := range tableInfoMap {\n\t\tfor _, tbInfoLoop := range tbInfos {\n\t\t\t// make sure tbInfo is only used in this loop\n\t\t\ttbInfo := tbInfoLoop\n\t\t\tslices.Sort(tbInfo)\n\t\t}\n\t}\n\n\td.selectTiDBTableRegionFunc = func(tctx *tcontext.Context, conn *BaseConn, meta TableMeta) (pkFields []string, pkVals [][]string, err error) {\n\t\tpkFields, _, err = selectTiDBRowKeyFields(tctx, conn, meta, checkTiDBTableRegionPkFields)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tdbName, tableName := meta.DatabaseName(), meta.TableName()\n\t\tif tbInfos, ok := tableInfoMap[dbName]; ok {\n\t\t\tif tbInfo, ok := tbInfos[tableName]; ok {\n\t\t\t\tpkVals = make([][]string, len(tbInfo))\n\t\t\t\tfor i, val := range tbInfo {\n\t\t\t\t\tpkVals[i] = []string{strconv.FormatInt(val, 10)}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn\n\t}\n\n\treturn nil\n}\n", "// Copyright 2020 PingCAP, Inc. Licensed under Apache-2.0.\n\npackage export\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/go-sql-driver/mysql\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/failpoint\"\n\t\"github.com/pingcap/tidb/br/pkg/version\"\n\tdbconfig \"github.com/pingcap/tidb/config\"\n\ttcontext \"github.com/pingcap/tidb/dumpling/context\"\n\t\"github.com/pingcap/tidb/dumpling/log\"\n\t\"github.com/pingcap/tidb/errno\"\n\t\"github.com/pingcap/tidb/parser/model\"\n\t\"github.com/pingcap/tidb/store/helper\"\n\t\"go.uber.org/multierr\"\n\t\"go.uber.org/zap\"\n)\n\nconst (\n\torderByTiDBRowID = \"ORDER BY `_tidb_rowid`\"\n)\n\ntype listTableType int\n\nconst (\n\tlistTableByInfoSchema listTableType = iota\n\tlistTableByShowFullTables\n\tlistTableByShowTableStatus\n)\n\n// ShowDatabases shows the databases of a database server.\nfunc ShowDatabases(db *sql.Conn) ([]string, error) {\n\tvar res oneStrColumnTable\n\tif err := simpleQuery(db, \"SHOW DATABASES\", res.handleOneRow); err != nil {\n\t\treturn nil, err\n\t}\n\treturn res.data, nil\n}\n\n// ShowTables shows the tables of a database, the caller should use the correct database.\nfunc ShowTables(db *sql.Conn) ([]string, error) {\n\tvar res oneStrColumnTable\n\tif err := simpleQuery(db, \"SHOW TABLES\", res.handleOneRow); err != nil {\n\t\treturn nil, err\n\t}\n\treturn res.data, nil\n}\n\n// ShowCreateDatabase constructs the create database SQL for a specified database\n// returns (createDatabaseSQL, error)\nfunc ShowCreateDatabase(tctx *tcontext.Context, db *BaseConn, database string) (string, error) {\n\tvar oneRow [2]string\n\thandleOneRow := func(rows *sql.Rows) error {\n\t\treturn rows.Scan(&oneRow[0], &oneRow[1])\n\t}\n\tquery := fmt.Sprintf(\"SHOW CREATE DATABASE `%s`\", escapeString(database))\n\terr := db.QuerySQL(tctx, handleOneRow, func() {\n\t\toneRow[0], oneRow[1] = \"\", \"\"\n\t}, query)\n\tif multiErrs := multierr.Errors(err); len(multiErrs) > 0 {\n\t\tfor _, multiErr := range multiErrs {\n\t\t\tif mysqlErr, ok := errors.Cause(multiErr).(*mysql.MySQLError); ok {\n\t\t\t\t// Falling back to simple create statement for MemSQL/SingleStore, because of this:\n\t\t\t\t// ERROR 1706 (HY000): Feature 'SHOW CREATE DATABASE' is not supported by MemSQL.\n\t\t\t\tif strings.Contains(mysqlErr.Error(), \"SHOW CREATE DATABASE\") {\n\t\t\t\t\treturn fmt.Sprintf(\"CREATE DATABASE `%s`\", escapeString(database)), nil\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn oneRow[1], err\n}\n\n// ShowCreateTable constructs the create table SQL for a specified table\n// returns (createTableSQL, error)\nfunc ShowCreateTable(tctx *tcontext.Context, db *BaseConn, database, table string) (string, error) {\n\tvar oneRow [2]string\n\thandleOneRow := func(rows *sql.Rows) error {\n\t\treturn rows.Scan(&oneRow[0], &oneRow[1])\n\t}\n\tquery := fmt.Sprintf(\"SHOW CREATE TABLE `%s`.`%s`\", escapeString(database), escapeString(table))\n\terr := db.QuerySQL(tctx, handleOneRow, func() {\n\t\toneRow[0], oneRow[1] = \"\", \"\"\n\t}, query)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn oneRow[1], nil\n}\n\n// ShowCreatePlacementPolicy constructs the create policy SQL for a specified table\n// returns (createPolicySQL, error)\nfunc ShowCreatePlacementPolicy(tctx *tcontext.Context, db *BaseConn, policy string) (string, error) {\n\tvar oneRow [2]string\n\thandleOneRow := func(rows *sql.Rows) error {\n\t\treturn rows.Scan(&oneRow[0], &oneRow[1])\n\t}\n\tquery := fmt.Sprintf(\"SHOW CREATE PLACEMENT POLICY `%s`\", escapeString(policy))\n\terr := db.QuerySQL(tctx, handleOneRow, func() {\n\t\toneRow[0], oneRow[1] = \"\", \"\"\n\t}, query)\n\treturn oneRow[1], err\n}\n\n// ShowCreateView constructs the create view SQL for a specified view\n// returns (createFakeTableSQL, createViewSQL, error)\nfunc ShowCreateView(tctx *tcontext.Context, db *BaseConn, database, view string) (createFakeTableSQL string, createRealViewSQL string, err error) {\n\tvar fieldNames []string\n\thandleFieldRow := func(rows *sql.Rows) error {\n\t\tvar oneRow [6]sql.NullString\n\t\tscanErr := rows.Scan(&oneRow[0], &oneRow[1], &oneRow[2], &oneRow[3], &oneRow[4], &oneRow[5])\n\t\tif scanErr != nil {\n\t\t\treturn errors.Trace(scanErr)\n\t\t}\n\t\tif oneRow[0].Valid {\n\t\t\tfieldNames = append(fieldNames, fmt.Sprintf(\"`%s` int\", escapeString(oneRow[0].String)))\n\t\t}\n\t\treturn nil\n\t}\n\tvar oneRow [4]string\n\thandleOneRow := func(rows *sql.Rows) error {\n\t\treturn rows.Scan(&oneRow[0], &oneRow[1], &oneRow[2], &oneRow[3])\n\t}\n\tvar createTableSQL, createViewSQL strings.Builder\n\n\t// Build createTableSQL\n\tquery := fmt.Sprintf(\"SHOW FIELDS FROM `%s`.`%s`\", escapeString(database), escapeString(view))\n\terr = db.QuerySQL(tctx, handleFieldRow, func() {\n\t\tfieldNames = []string{}\n\t}, query)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\tfmt.Fprintf(&createTableSQL, \"CREATE TABLE `%s`(\\n\", escapeString(view))\n\tcreateTableSQL.WriteString(strings.Join(fieldNames, \",\\n\"))\n\tcreateTableSQL.WriteString(\"\\n)ENGINE=MyISAM;\\n\")\n\n\t// Build createViewSQL\n\tfmt.Fprintf(&createViewSQL, \"DROP TABLE IF EXISTS `%s`;\\n\", escapeString(view))\n\tfmt.Fprintf(&createViewSQL, \"DROP VIEW IF EXISTS `%s`;\\n\", escapeString(view))\n\tquery = fmt.Sprintf(\"SHOW CREATE VIEW `%s`.`%s`\", escapeString(database), escapeString(view))\n\terr = db.QuerySQL(tctx, handleOneRow, func() {\n\t\tfor i := range oneRow {\n\t\t\toneRow[i] = \"\"\n\t\t}\n\t}, query)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\t// The result for `show create view` SQL\n\t// mysql> show create view v1;\n\t// +------+-------------------------------------------------------------------------------------------------------------------------------------+----------------------+----------------------+\n\t// | View | Create View                                                                                                                         | character_set_client | collation_connection |\n\t// +------+-------------------------------------------------------------------------------------------------------------------------------------+----------------------+----------------------+\n\t// | v1   | CREATE ALGORITHM=UNDEFINED DEFINER=`root`@`localhost` SQL SECURITY DEFINER VIEW `v1` (`a`) AS SELECT `t`.`a` AS `a` FROM `test`.`t` | utf8                 | utf8_general_ci      |\n\t// +------+-------------------------------------------------------------------------------------------------------------------------------------+----------------------+----------------------+\n\tSetCharset(&createViewSQL, oneRow[2], oneRow[3])\n\tcreateViewSQL.WriteString(oneRow[1])\n\tcreateViewSQL.WriteString(\";\\n\")\n\tRestoreCharset(&createViewSQL)\n\n\treturn createTableSQL.String(), createViewSQL.String(), nil\n}\n\n// ShowCreateSequence constructs the create sequence SQL for a specified sequence\n// returns (createSequenceSQL, error)\nfunc ShowCreateSequence(tctx *tcontext.Context, db *BaseConn, database, sequence string, conf *Config) (string, error) {\n\tvar oneRow [2]string\n\thandleOneRow := func(rows *sql.Rows) error {\n\t\treturn rows.Scan(&oneRow[0], &oneRow[1])\n\t}\n\tvar (\n\t\tcreateSequenceSQL  strings.Builder\n\t\tnextNotCachedValue int64\n\t)\n\tquery := fmt.Sprintf(\"SHOW CREATE SEQUENCE `%s`.`%s`\", escapeString(database), escapeString(sequence))\n\terr := db.QuerySQL(tctx, handleOneRow, func() {\n\t\toneRow[0], oneRow[1] = \"\", \"\"\n\t}, query)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tcreateSequenceSQL.WriteString(oneRow[1])\n\tcreateSequenceSQL.WriteString(\";\\n\")\n\n\tswitch conf.ServerInfo.ServerType {\n\tcase version.ServerTypeTiDB:\n\t\t// Get next not allocated auto increment id of the whole cluster\n\t\tquery := fmt.Sprintf(\"SHOW TABLE `%s`.`%s` NEXT_ROW_ID\", escapeString(database), escapeString(sequence))\n\t\tresults, err := db.QuerySQLWithColumns(tctx, []string{\"NEXT_GLOBAL_ROW_ID\", \"ID_TYPE\"}, query)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tfor _, oneRow := range results {\n\t\t\tnextGlobalRowID, idType := oneRow[0], oneRow[1]\n\t\t\tif idType == \"SEQUENCE\" {\n\t\t\t\tnextNotCachedValue, _ = strconv.ParseInt(nextGlobalRowID, 10, 64)\n\t\t\t}\n\t\t}\n\t\tfmt.Fprintf(&createSequenceSQL, \"SELECT SETVAL(`%s`,%d);\\n\", escapeString(sequence), nextNotCachedValue)\n\tcase version.ServerTypeMariaDB:\n\t\tvar oneRow1 string\n\t\thandleOneRow1 := func(rows *sql.Rows) error {\n\t\t\treturn rows.Scan(&oneRow1)\n\t\t}\n\t\tquery := fmt.Sprintf(\"SELECT NEXT_NOT_CACHED_VALUE FROM `%s`.`%s`\", escapeString(database), escapeString(sequence))\n\t\terr := db.QuerySQL(tctx, handleOneRow1, func() {\n\t\t\toneRow1 = \"\"\n\t\t}, query)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tnextNotCachedValue, _ = strconv.ParseInt(oneRow1, 10, 64)\n\t\tfmt.Fprintf(&createSequenceSQL, \"SELECT SETVAL(`%s`,%d);\\n\", escapeString(sequence), nextNotCachedValue)\n\t}\n\treturn createSequenceSQL.String(), nil\n}\n\n// SetCharset builds the set charset SQLs\nfunc SetCharset(w *strings.Builder, characterSet, collationConnection string) {\n\tw.WriteString(\"SET @PREV_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT;\\n\")\n\tw.WriteString(\"SET @PREV_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS;\\n\")\n\tw.WriteString(\"SET @PREV_COLLATION_CONNECTION=@@COLLATION_CONNECTION;\\n\")\n\n\tfmt.Fprintf(w, \"SET character_set_client = %s;\\n\", characterSet)\n\tfmt.Fprintf(w, \"SET character_set_results = %s;\\n\", characterSet)\n\tfmt.Fprintf(w, \"SET collation_connection = %s;\\n\", collationConnection)\n}\n\n// RestoreCharset builds the restore charset SQLs\nfunc RestoreCharset(w io.StringWriter) {\n\t_, _ = w.WriteString(\"SET character_set_client = @PREV_CHARACTER_SET_CLIENT;\\n\")\n\t_, _ = w.WriteString(\"SET character_set_results = @PREV_CHARACTER_SET_RESULTS;\\n\")\n\t_, _ = w.WriteString(\"SET collation_connection = @PREV_COLLATION_CONNECTION;\\n\")\n}\n\n// ListAllDatabasesTables lists all the databases and tables from the database\n// listTableByInfoSchema list tables by table information_schema in MySQL\n// listTableByShowTableStatus has better performance than listTableByInfoSchema\n// listTableByShowFullTables is used in mysql8 version [8.0.3,8.0.23), more details can be found in the comments of func matchMysqlBugversion\nfunc ListAllDatabasesTables(tctx *tcontext.Context, db *sql.Conn, databaseNames []string,\n\tlistType listTableType, tableTypes ...TableType) (DatabaseTables, error) { // revive:disable-line:flag-parameter\n\tdbTables := DatabaseTables{}\n\tvar (\n\t\tschema, table, tableTypeStr string\n\t\ttableType                   TableType\n\t\tavgRowLength                uint64\n\t\terr                         error\n\t)\n\n\ttableTypeConditions := make([]string, len(tableTypes))\n\tfor i, tableType := range tableTypes {\n\t\ttableTypeConditions[i] = fmt.Sprintf(\"TABLE_TYPE='%s'\", tableType)\n\t}\n\tswitch listType {\n\tcase listTableByInfoSchema:\n\t\tquery := fmt.Sprintf(\"SELECT TABLE_SCHEMA,TABLE_NAME,TABLE_TYPE,AVG_ROW_LENGTH FROM INFORMATION_SCHEMA.TABLES WHERE %s\", strings.Join(tableTypeConditions, \" OR \"))\n\t\tfor _, schema := range databaseNames {\n\t\t\tdbTables[schema] = make([]*TableInfo, 0)\n\t\t}\n\t\tif err = simpleQueryWithArgs(tctx, db, func(rows *sql.Rows) error {\n\t\t\tvar (\n\t\t\t\tsqlAvgRowLength sql.NullInt64\n\t\t\t\terr2            error\n\t\t\t)\n\t\t\tif err2 = rows.Scan(&schema, &table, &tableTypeStr, &sqlAvgRowLength); err != nil {\n\t\t\t\treturn errors.Trace(err2)\n\t\t\t}\n\t\t\ttableType, err2 = ParseTableType(tableTypeStr)\n\t\t\tif err2 != nil {\n\t\t\t\treturn errors.Trace(err2)\n\t\t\t}\n\n\t\t\tif sqlAvgRowLength.Valid {\n\t\t\t\tavgRowLength = uint64(sqlAvgRowLength.Int64)\n\t\t\t} else {\n\t\t\t\tavgRowLength = 0\n\t\t\t}\n\t\t\t// only append tables to schemas in databaseNames\n\t\t\tif _, ok := dbTables[schema]; ok {\n\t\t\t\tdbTables[schema] = append(dbTables[schema], &TableInfo{table, avgRowLength, tableType})\n\t\t\t}\n\t\t\treturn nil\n\t\t}, query); err != nil {\n\t\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t\t}\n\tcase listTableByShowFullTables:\n\t\tfor _, schema = range databaseNames {\n\t\t\tdbTables[schema] = make([]*TableInfo, 0)\n\t\t\tquery := fmt.Sprintf(\"SHOW FULL TABLES FROM `%s` WHERE %s\",\n\t\t\t\tescapeString(schema), strings.Join(tableTypeConditions, \" OR \"))\n\t\t\tif err = simpleQueryWithArgs(tctx, db, func(rows *sql.Rows) error {\n\t\t\t\tvar err2 error\n\t\t\t\tif err2 = rows.Scan(&table, &tableTypeStr); err != nil {\n\t\t\t\t\treturn errors.Trace(err2)\n\t\t\t\t}\n\t\t\t\ttableType, err2 = ParseTableType(tableTypeStr)\n\t\t\t\tif err2 != nil {\n\t\t\t\t\treturn errors.Trace(err2)\n\t\t\t\t}\n\t\t\t\tavgRowLength = 0 // can't get avgRowLength from the result of `show full tables` so hardcode to 0 here\n\t\t\t\tdbTables[schema] = append(dbTables[schema], &TableInfo{table, avgRowLength, tableType})\n\t\t\t\treturn nil\n\t\t\t}, query); err != nil {\n\t\t\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t\t\t}\n\t\t}\n\tdefault:\n\t\tconst queryTemplate = \"SHOW TABLE STATUS FROM `%s`\"\n\t\tselectedTableType := make(map[TableType]struct{})\n\t\tfor _, tableType = range tableTypes {\n\t\t\tselectedTableType[tableType] = struct{}{}\n\t\t}\n\t\tfor _, schema = range databaseNames {\n\t\t\tdbTables[schema] = make([]*TableInfo, 0)\n\t\t\tquery := fmt.Sprintf(queryTemplate, escapeString(schema))\n\t\t\trows, err := db.QueryContext(tctx, query)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t\t\t}\n\t\t\tresults, err := GetSpecifiedColumnValuesAndClose(rows, \"NAME\", \"ENGINE\", \"AVG_ROW_LENGTH\", \"COMMENT\")\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t\t\t}\n\t\t\tfor _, oneRow := range results {\n\t\t\t\ttable, engine, avgRowLengthStr, comment := oneRow[0], oneRow[1], oneRow[2], oneRow[3]\n\t\t\t\tif avgRowLengthStr != \"\" {\n\t\t\t\t\tavgRowLength, err = strconv.ParseUint(avgRowLengthStr, 10, 64)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tavgRowLength = 0\n\t\t\t\t}\n\t\t\t\ttableType = TableTypeBase\n\t\t\t\tif engine == \"\" && (comment == \"\" || comment == TableTypeViewStr) {\n\t\t\t\t\ttableType = TableTypeView\n\t\t\t\t} else if engine == \"\" {\n\t\t\t\t\ttctx.L().Warn(\"invalid table without engine found\", zap.String(\"database\", schema), zap.String(\"table\", table))\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif _, ok := selectedTableType[tableType]; !ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tdbTables[schema] = append(dbTables[schema], &TableInfo{table, avgRowLength, tableType})\n\t\t\t}\n\t\t}\n\t}\n\treturn dbTables, nil\n}\n\n// ListAllPlacementPolicyNames returns all placement policy names.\nfunc ListAllPlacementPolicyNames(tctx *tcontext.Context, db *BaseConn) ([]string, error) {\n\tvar policyList []string\n\tvar policy string\n\tconst query = \"select distinct policy_name from information_schema.placement_policies where policy_name is not null;\"\n\terr := db.QuerySQL(tctx, func(rows *sql.Rows) error {\n\t\terr := rows.Scan(&policy)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tpolicyList = append(policyList, policy)\n\t\treturn nil\n\t}, func() {\n\t\tpolicyList = policyList[:0]\n\t}, query)\n\treturn policyList, errors.Annotatef(err, \"sql: %s\", query)\n}\n\n// SelectVersion gets the version information from the database server\nfunc SelectVersion(db *sql.DB) (string, error) {\n\tvar versionInfo string\n\tconst query = \"SELECT version()\"\n\trow := db.QueryRow(query)\n\terr := row.Scan(&versionInfo)\n\tif err != nil {\n\t\treturn \"\", errors.Annotatef(err, \"sql: %s\", query)\n\t}\n\treturn versionInfo, nil\n}\n\n// SelectAllFromTable dumps data serialized from a specified table\nfunc SelectAllFromTable(conf *Config, meta TableMeta, partition, orderByClause string) TableDataIR {\n\tdatabase, table := meta.DatabaseName(), meta.TableName()\n\tselectedField, selectLen := meta.SelectedField(), meta.SelectedLen()\n\tquery := buildSelectQuery(database, table, selectedField, partition, buildWhereCondition(conf, \"\"), orderByClause)\n\n\treturn &tableData{\n\t\tquery:  query,\n\t\tcolLen: selectLen,\n\t}\n}\n\nfunc buildSelectQuery(database, table, fields, partition, where, orderByClause string) string {\n\tvar query strings.Builder\n\tquery.WriteString(\"SELECT \")\n\tif fields == \"\" {\n\t\t// If all of the columns are generated,\n\t\t// we need to make sure the query is valid.\n\t\tfields = \"''\"\n\t}\n\tquery.WriteString(fields)\n\tquery.WriteString(\" FROM `\")\n\tquery.WriteString(escapeString(database))\n\tquery.WriteString(\"`.`\")\n\tquery.WriteString(escapeString(table))\n\tquery.WriteByte('`')\n\tif partition != \"\" {\n\t\tquery.WriteString(\" PARTITION(`\")\n\t\tquery.WriteString(escapeString(partition))\n\t\tquery.WriteString(\"`)\")\n\t}\n\n\tif where != \"\" {\n\t\tquery.WriteString(\" \")\n\t\tquery.WriteString(where)\n\t}\n\n\tif orderByClause != \"\" {\n\t\tquery.WriteString(\" \")\n\t\tquery.WriteString(orderByClause)\n\t}\n\n\treturn query.String()\n}\n\nfunc buildOrderByClause(tctx *tcontext.Context, conf *Config, db *BaseConn, database, table string, hasImplicitRowID bool) (string, error) { // revive:disable-line:flag-parameter\n\tif !conf.SortByPk {\n\t\treturn \"\", nil\n\t}\n\tif hasImplicitRowID {\n\t\treturn orderByTiDBRowID, nil\n\t}\n\tcols, err := GetPrimaryKeyColumns(tctx, db, database, table)\n\tif err != nil {\n\t\treturn \"\", errors.Trace(err)\n\t}\n\treturn buildOrderByClauseString(cols), nil\n}\n\n// SelectTiDBRowID checks whether this table has _tidb_rowid column\nfunc SelectTiDBRowID(tctx *tcontext.Context, db *BaseConn, database, table string) (bool, error) {\n\ttiDBRowIDQuery := fmt.Sprintf(\"SELECT _tidb_rowid from `%s`.`%s` LIMIT 1\", escapeString(database), escapeString(table))\n\thasImplictRowID := false\n\terr := db.ExecSQL(tctx, func(_ sql.Result, err error) error {\n\t\tif err != nil {\n\t\t\thasImplictRowID = false\n\t\t\terrMsg := strings.ToLower(err.Error())\n\t\t\tif strings.Contains(errMsg, fmt.Sprintf(\"%d\", errno.ErrBadField)) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn errors.Annotatef(err, \"sql: %s\", tiDBRowIDQuery)\n\t\t}\n\t\thasImplictRowID = true\n\t\treturn nil\n\t}, tiDBRowIDQuery)\n\treturn hasImplictRowID, err\n}\n\n// GetSuitableRows gets suitable rows for each table\nfunc GetSuitableRows(avgRowLength uint64) uint64 {\n\tconst (\n\t\tdefaultRows  = 200000\n\t\tmaxRows      = 1000000\n\t\tbytesPerFile = 128 * 1024 * 1024 // 128MB per file by default\n\t)\n\tif avgRowLength == 0 {\n\t\treturn defaultRows\n\t}\n\testimateRows := bytesPerFile / avgRowLength\n\tif estimateRows > maxRows {\n\t\treturn maxRows\n\t}\n\treturn estimateRows\n}\n\n// GetColumnTypes gets *sql.ColumnTypes from a specified table\nfunc GetColumnTypes(tctx *tcontext.Context, db *BaseConn, fields, database, table string) ([]*sql.ColumnType, error) {\n\tquery := fmt.Sprintf(\"SELECT %s FROM `%s`.`%s` LIMIT 1\", fields, escapeString(database), escapeString(table))\n\tvar colTypes []*sql.ColumnType\n\terr := db.QuerySQL(tctx, func(rows *sql.Rows) error {\n\t\tvar err error\n\t\tcolTypes, err = rows.ColumnTypes()\n\t\tif err == nil {\n\t\t\terr = rows.Close()\n\t\t}\n\t\tfailpoint.Inject(\"ChaosBrokenMetaConn\", func(_ failpoint.Value) {\n\t\t\tfailpoint.Return(errors.New(\"connection is closed\"))\n\t\t})\n\t\treturn errors.Annotatef(err, \"sql: %s\", query)\n\t}, func() {\n\t\tcolTypes = nil\n\t}, query)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn colTypes, nil\n}\n\n// GetPrimaryKeyAndColumnTypes gets all primary columns and their types in ordinal order\nfunc GetPrimaryKeyAndColumnTypes(tctx *tcontext.Context, conn *BaseConn, meta TableMeta) ([]string, []string, error) {\n\tvar (\n\t\tcolNames, colTypes []string\n\t\terr                error\n\t)\n\tcolNames, err = GetPrimaryKeyColumns(tctx, conn, meta.DatabaseName(), meta.TableName())\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tcolName2Type := string2Map(meta.ColumnNames(), meta.ColumnTypes())\n\tcolTypes = make([]string, len(colNames))\n\tfor i, colName := range colNames {\n\t\tcolTypes[i] = colName2Type[colName]\n\t}\n\treturn colNames, colTypes, nil\n}\n\n// GetPrimaryKeyColumns gets all primary columns in ordinal order\nfunc GetPrimaryKeyColumns(tctx *tcontext.Context, db *BaseConn, database, table string) ([]string, error) {\n\tpriKeyColsQuery := fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", escapeString(database), escapeString(table))\n\tresults, err := db.QuerySQLWithColumns(tctx, []string{\"KEY_NAME\", \"COLUMN_NAME\"}, priKeyColsQuery)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcols := make([]string, 0, len(results))\n\tfor _, oneRow := range results {\n\t\tkeyName, columnName := oneRow[0], oneRow[1]\n\t\tif keyName == \"PRIMARY\" {\n\t\t\tcols = append(cols, columnName)\n\t\t}\n\t}\n\treturn cols, nil\n}\n\n// getNumericIndex picks up indices according to the following priority:\n// primary key > unique key with the smallest count > key with the max cardinality\n// primary key with multi cols is before unique key with single col because we will sort result by primary keys\nfunc getNumericIndex(tctx *tcontext.Context, db *BaseConn, meta TableMeta) (string, error) {\n\tdatabase, table := meta.DatabaseName(), meta.TableName()\n\tcolName2Type := string2Map(meta.ColumnNames(), meta.ColumnTypes())\n\tkeyQuery := fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", escapeString(database), escapeString(table))\n\tresults, err := db.QuerySQLWithColumns(tctx, []string{\"NON_UNIQUE\", \"SEQ_IN_INDEX\", \"KEY_NAME\", \"COLUMN_NAME\", \"CARDINALITY\"}, keyQuery)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\ttype keyColumnPair struct {\n\t\tcolName string\n\t\tcount   uint64\n\t}\n\tvar (\n\t\tuniqueKeyMap   = map[string]keyColumnPair{} // unique key name -> key column name, unique key columns count\n\t\tkeyColumn      string\n\t\tmaxCardinality int64 = -1\n\t)\n\n\t// check primary key first, then unique key\n\tfor _, oneRow := range results {\n\t\tnonUnique, seqInIndex, keyName, colName, cardinality := oneRow[0], oneRow[1], oneRow[2], oneRow[3], oneRow[4]\n\t\t// only try pick the first column, because the second column of pk/uk in where condition will trigger a full table scan\n\t\tif seqInIndex != \"1\" {\n\t\t\tif pair, ok := uniqueKeyMap[keyName]; ok {\n\t\t\t\tseqInIndexInt, err := strconv.ParseUint(seqInIndex, 10, 64)\n\t\t\t\tif err == nil && seqInIndexInt > pair.count {\n\t\t\t\t\tuniqueKeyMap[keyName] = keyColumnPair{pair.colName, seqInIndexInt}\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\t_, numberColumn := dataTypeInt[colName2Type[colName]]\n\t\tif numberColumn {\n\t\t\tswitch {\n\t\t\tcase keyName == \"PRIMARY\":\n\t\t\t\treturn colName, nil\n\t\t\tcase nonUnique == \"0\":\n\t\t\t\tuniqueKeyMap[keyName] = keyColumnPair{colName, 1}\n\t\t\t// pick index column with max cardinality when there is no unique index\n\t\t\tcase len(uniqueKeyMap) == 0:\n\t\t\t\tcardinalityInt, err := strconv.ParseInt(cardinality, 10, 64)\n\t\t\t\tif err == nil && cardinalityInt > maxCardinality {\n\t\t\t\t\tkeyColumn = colName\n\t\t\t\t\tmaxCardinality = cardinalityInt\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif len(uniqueKeyMap) > 0 {\n\t\tvar (\n\t\t\tminCols         uint64 = math.MaxUint64\n\t\t\tuniqueKeyColumn string\n\t\t)\n\t\tfor _, pair := range uniqueKeyMap {\n\t\t\tif pair.count < minCols {\n\t\t\t\tuniqueKeyColumn = pair.colName\n\t\t\t\tminCols = pair.count\n\t\t\t}\n\t\t}\n\t\treturn uniqueKeyColumn, nil\n\t}\n\treturn keyColumn, nil\n}\n\n// FlushTableWithReadLock flush tables with read lock\nfunc FlushTableWithReadLock(ctx context.Context, db *sql.Conn) error {\n\tconst ftwrlQuery = \"FLUSH TABLES WITH READ LOCK\"\n\t_, err := db.ExecContext(ctx, ftwrlQuery)\n\treturn errors.Annotatef(err, \"sql: %s\", ftwrlQuery)\n}\n\n// LockTables locks table with read lock\nfunc LockTables(ctx context.Context, db *sql.Conn, database, table string) error {\n\tlockTableQuery := fmt.Sprintf(\"LOCK TABLES `%s`.`%s` READ\", escapeString(database), escapeString(table))\n\t_, err := db.ExecContext(ctx, lockTableQuery)\n\treturn errors.Annotatef(err, \"sql: %s\", lockTableQuery)\n}\n\n// UnlockTables unlocks all tables' lock\nfunc UnlockTables(ctx context.Context, db *sql.Conn) error {\n\tconst unlockTableQuery = \"UNLOCK TABLES\"\n\t_, err := db.ExecContext(ctx, unlockTableQuery)\n\treturn errors.Annotatef(err, \"sql: %s\", unlockTableQuery)\n}\n\n// ShowMasterStatus get SHOW MASTER STATUS result from database\nfunc ShowMasterStatus(db *sql.Conn) ([]string, error) {\n\tvar oneRow []string\n\thandleOneRow := func(rows *sql.Rows) error {\n\t\tcols, err := rows.Columns()\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tfieldNum := len(cols)\n\t\toneRow = make([]string, fieldNum)\n\t\taddr := make([]interface{}, fieldNum)\n\t\tfor i := range oneRow {\n\t\t\taddr[i] = &oneRow[i]\n\t\t}\n\t\treturn rows.Scan(addr...)\n\t}\n\tconst showMasterStatusQuery = \"SHOW MASTER STATUS\"\n\terr := simpleQuery(db, showMasterStatusQuery, handleOneRow)\n\tif err != nil {\n\t\treturn nil, errors.Annotatef(err, \"sql: %s\", showMasterStatusQuery)\n\t}\n\treturn oneRow, nil\n}\n\n// GetSpecifiedColumnValueAndClose get columns' values whose name is equal to columnName and close the given rows\nfunc GetSpecifiedColumnValueAndClose(rows *sql.Rows, columnName string) ([]string, error) {\n\tif rows == nil {\n\t\treturn []string{}, nil\n\t}\n\tdefer rows.Close()\n\tvar strs []string\n\tcolumns, _ := rows.Columns()\n\taddr := make([]interface{}, len(columns))\n\toneRow := make([]sql.NullString, len(columns))\n\tfieldIndex := -1\n\tfor i, col := range columns {\n\t\tif strings.EqualFold(col, columnName) {\n\t\t\tfieldIndex = i\n\t\t}\n\t\taddr[i] = &oneRow[i]\n\t}\n\tif fieldIndex == -1 {\n\t\treturn strs, nil\n\t}\n\tfor rows.Next() {\n\t\terr := rows.Scan(addr...)\n\t\tif err != nil {\n\t\t\treturn strs, errors.Trace(err)\n\t\t}\n\t\tif oneRow[fieldIndex].Valid {\n\t\t\tstrs = append(strs, oneRow[fieldIndex].String)\n\t\t}\n\t}\n\treturn strs, errors.Trace(rows.Err())\n}\n\n// GetSpecifiedColumnValuesAndClose get columns' values whose name is equal to columnName\nfunc GetSpecifiedColumnValuesAndClose(rows *sql.Rows, columnName ...string) ([][]string, error) {\n\tif rows == nil {\n\t\treturn [][]string{}, nil\n\t}\n\tdefer rows.Close()\n\tvar strs [][]string\n\tcolumns, err := rows.Columns()\n\tif err != nil {\n\t\treturn strs, errors.Trace(err)\n\t}\n\taddr := make([]interface{}, len(columns))\n\toneRow := make([]sql.NullString, len(columns))\n\tfieldIndexMp := make(map[int]int)\n\tfor i, col := range columns {\n\t\taddr[i] = &oneRow[i]\n\t\tfor j, name := range columnName {\n\t\t\tif strings.EqualFold(col, name) {\n\t\t\t\tfieldIndexMp[i] = j\n\t\t\t}\n\t\t}\n\t}\n\tif len(fieldIndexMp) == 0 {\n\t\treturn strs, nil\n\t}\n\tfor rows.Next() {\n\t\terr := rows.Scan(addr...)\n\t\tif err != nil {\n\t\t\treturn strs, errors.Trace(err)\n\t\t}\n\t\twritten := false\n\t\ttmpStr := make([]string, len(columnName))\n\t\tfor colPos, namePos := range fieldIndexMp {\n\t\t\tif oneRow[colPos].Valid {\n\t\t\t\twritten = true\n\t\t\t\ttmpStr[namePos] = oneRow[colPos].String\n\t\t\t}\n\t\t}\n\t\tif written {\n\t\t\tstrs = append(strs, tmpStr)\n\t\t}\n\t}\n\treturn strs, errors.Trace(rows.Err())\n}\n\n// GetPdAddrs gets PD address from TiDB\nfunc GetPdAddrs(tctx *tcontext.Context, db *sql.DB) ([]string, error) {\n\tconst query = \"SELECT * FROM information_schema.cluster_info where type = 'pd';\"\n\trows, err := db.QueryContext(tctx, query)\n\tif err != nil {\n\t\treturn []string{}, errors.Annotatef(err, \"sql: %s\", query)\n\t}\n\tpdAddrs, err := GetSpecifiedColumnValueAndClose(rows, \"STATUS_ADDRESS\")\n\treturn pdAddrs, errors.Annotatef(err, \"sql: %s\", query)\n}\n\n// GetTiDBDDLIDs gets DDL IDs from TiDB\nfunc GetTiDBDDLIDs(tctx *tcontext.Context, db *sql.DB) ([]string, error) {\n\tconst query = \"SELECT * FROM information_schema.tidb_servers_info;\"\n\trows, err := db.QueryContext(tctx, query)\n\tif err != nil {\n\t\treturn []string{}, errors.Annotatef(err, \"sql: %s\", query)\n\t}\n\tddlIDs, err := GetSpecifiedColumnValueAndClose(rows, \"DDL_ID\")\n\treturn ddlIDs, errors.Annotatef(err, \"sql: %s\", query)\n}\n\n// getTiDBConfig gets tidb config from TiDB server\n// @@tidb_config details doc https://docs.pingcap.com/tidb/stable/system-variables#tidb_config\n// this variable exists at least from v2.0.0, so this works in most existing tidb instances\nfunc getTiDBConfig(db *sql.Conn) (dbconfig.Config, error) {\n\tconst query = \"SELECT @@tidb_config;\"\n\tvar (\n\t\ttidbConfig      dbconfig.Config\n\t\ttidbConfigBytes []byte\n\t)\n\trow := db.QueryRowContext(context.Background(), query)\n\terr := row.Scan(&tidbConfigBytes)\n\tif err != nil {\n\t\treturn tidbConfig, errors.Annotatef(err, \"sql: %s\", query)\n\t}\n\terr = json.Unmarshal(tidbConfigBytes, &tidbConfig)\n\treturn tidbConfig, errors.Annotatef(err, \"sql: %s\", query)\n}\n\n// CheckTiDBWithTiKV use sql to check whether current TiDB has TiKV\nfunc CheckTiDBWithTiKV(db *sql.DB) (bool, error) {\n\tconn, err := db.Conn(context.Background())\n\tif err == nil {\n\t\tdefer func() {\n\t\t\t_ = conn.Close()\n\t\t}()\n\t\ttidbConfig, err := getTiDBConfig(conn)\n\t\tif err == nil {\n\t\t\treturn tidbConfig.Store == \"tikv\", nil\n\t\t}\n\t}\n\tvar count int\n\tconst query = \"SELECT COUNT(1) as c FROM MYSQL.TiDB WHERE VARIABLE_NAME='tikv_gc_safe_point'\"\n\trow := db.QueryRow(query)\n\terr = row.Scan(&count)\n\tif err != nil {\n\t\t// still return true here. Because sometimes users may not have privileges for MySQL.TiDB database\n\t\t// In most production cases TiDB has TiKV\n\t\treturn true, errors.Annotatef(err, \"sql: %s\", query)\n\t}\n\treturn count > 0, nil\n}\n\n// CheckIfSeqExists use sql to check whether sequence exists\nfunc CheckIfSeqExists(db *sql.Conn) (bool, error) {\n\tvar count int\n\tconst query = \"SELECT COUNT(1) as c FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE='SEQUENCE'\"\n\trow := db.QueryRowContext(context.Background(), query)\n\terr := row.Scan(&count)\n\tif err != nil {\n\t\treturn false, errors.Annotatef(err, \"sql: %s\", query)\n\t}\n\n\treturn count > 0, nil\n}\n\n// CheckTiDBEnableTableLock use sql variable to check whether current TiDB has TiKV\nfunc CheckTiDBEnableTableLock(db *sql.Conn) (bool, error) {\n\ttidbConfig, err := getTiDBConfig(db)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\treturn tidbConfig.EnableTableLock, nil\n}\n\nfunc getSnapshot(db *sql.Conn) (string, error) {\n\tstr, err := ShowMasterStatus(db)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn str[snapshotFieldIndex], nil\n}\n\nfunc isUnknownSystemVariableErr(err error) bool {\n\treturn strings.Contains(err.Error(), \"Unknown system variable\")\n}\n\n// resetDBWithSessionParams will return a new sql.DB as a replacement for input `db` with new session parameters.\n// If returned error is nil, the input `db` will be closed.\nfunc resetDBWithSessionParams(tctx *tcontext.Context, db *sql.DB, cfg *mysql.Config, params map[string]interface{}) (*sql.DB, error) {\n\tsupport := make(map[string]interface{})\n\tfor k, v := range params {\n\t\tvar pv interface{}\n\t\tif str, ok := v.(string); ok {\n\t\t\tif pvi, err := strconv.ParseInt(str, 10, 64); err == nil {\n\t\t\t\tpv = pvi\n\t\t\t} else if pvf, err := strconv.ParseFloat(str, 64); err == nil {\n\t\t\t\tpv = pvf\n\t\t\t} else {\n\t\t\t\tpv = str\n\t\t\t}\n\t\t} else {\n\t\t\tpv = v\n\t\t}\n\t\ts := fmt.Sprintf(\"SET SESSION %s = ?\", k)\n\t\t_, err := db.ExecContext(tctx, s, pv)\n\t\tif err != nil {\n\t\t\tif isUnknownSystemVariableErr(err) {\n\t\t\t\ttctx.L().Info(\"session variable is not supported by db\", zap.String(\"variable\", k), zap.Reflect(\"value\", v))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\n\t\tsupport[k] = pv\n\t}\n\n\tif cfg.Params == nil {\n\t\tcfg.Params = make(map[string]string)\n\t}\n\n\tfor k, v := range support {\n\t\tvar s string\n\t\t// Wrap string with quote to handle string with space. For example, '2020-10-20 13:41:40'\n\t\t// For --params argument, quote doesn't matter because it doesn't affect the actual value\n\t\tif str, ok := v.(string); ok {\n\t\t\ts = wrapStringWith(str, \"'\")\n\t\t} else {\n\t\t\ts = fmt.Sprintf(\"%v\", v)\n\t\t}\n\t\tcfg.Params[k] = s\n\t}\n\n\tdb.Close()\n\tc, err := mysql.NewConnector(cfg)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tnewDB := sql.OpenDB(c)\n\t// ping to make sure all session parameters are set correctly\n\terr = newDB.PingContext(tctx)\n\tif err != nil {\n\t\tnewDB.Close()\n\t}\n\treturn newDB, nil\n}\n\nfunc createConnWithConsistency(ctx context.Context, db *sql.DB, repeatableRead bool) (*sql.Conn, error) {\n\tconn, err := db.Conn(ctx)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tvar query string\n\tif repeatableRead {\n\t\tquery = \"SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ\"\n\t\t_, err = conn.ExecContext(ctx, query)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t\t}\n\t}\n\tquery = \"START TRANSACTION /*!40108 WITH CONSISTENT SNAPSHOT */\"\n\t_, err = conn.ExecContext(ctx, query)\n\tif err != nil {\n\t\t// Some MySQL Compatible databases like Vitess and MemSQL/SingleStore\n\t\t// are newer than 4.1.8 (the version comment) but don't actually support\n\t\t// `WITH CONSISTENT SNAPSHOT`. So retry without that if the statement fails.\n\t\tquery = \"START TRANSACTION\"\n\t\t_, err = conn.ExecContext(ctx, query)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t\t}\n\t}\n\treturn conn, nil\n}\n\n// buildSelectField returns the selecting fields' string(joined by comma(`,`)),\n// and the number of writable fields.\nfunc buildSelectField(tctx *tcontext.Context, db *BaseConn, dbName, tableName string, completeInsert bool) (string, int, error) { // revive:disable-line:flag-parameter\n\tquery := fmt.Sprintf(\"SHOW COLUMNS FROM `%s`.`%s`\", escapeString(dbName), escapeString(tableName))\n\tresults, err := db.QuerySQLWithColumns(tctx, []string{\"FIELD\", \"EXTRA\"}, query)\n\tif err != nil {\n\t\treturn \"\", 0, err\n\t}\n\tavailableFields := make([]string, 0)\n\thasGenerateColumn := false\n\tfor _, oneRow := range results {\n\t\tfieldName, extra := oneRow[0], oneRow[1]\n\t\tswitch extra {\n\t\tcase \"STORED GENERATED\", \"VIRTUAL GENERATED\":\n\t\t\thasGenerateColumn = true\n\t\t\tcontinue\n\t\t}\n\t\tavailableFields = append(availableFields, wrapBackTicks(escapeString(fieldName)))\n\t}\n\tif completeInsert || hasGenerateColumn {\n\t\treturn strings.Join(availableFields, \",\"), len(availableFields), nil\n\t}\n\treturn \"*\", len(availableFields), nil\n}\n\nfunc buildWhereClauses(handleColNames []string, handleVals [][]string) []string {\n\tif len(handleColNames) == 0 || len(handleVals) == 0 {\n\t\treturn nil\n\t}\n\tquotaCols := make([]string, len(handleColNames))\n\tfor i, s := range handleColNames {\n\t\tquotaCols[i] = fmt.Sprintf(\"`%s`\", escapeString(s))\n\t}\n\twhere := make([]string, 0, len(handleVals)+1)\n\tbuf := &bytes.Buffer{}\n\tbuildCompareClause(buf, quotaCols, handleVals[0], less, false)\n\twhere = append(where, buf.String())\n\tbuf.Reset()\n\tfor i := 1; i < len(handleVals); i++ {\n\t\tlow, up := handleVals[i-1], handleVals[i]\n\t\tbuildBetweenClause(buf, quotaCols, low, up)\n\t\twhere = append(where, buf.String())\n\t\tbuf.Reset()\n\t}\n\tbuildCompareClause(buf, quotaCols, handleVals[len(handleVals)-1], greater, true)\n\twhere = append(where, buf.String())\n\tbuf.Reset()\n\treturn where\n}\n\n// return greater than TableRangeScan where clause\n// the result doesn't contain brackets\nconst (\n\tgreater = '>'\n\tless    = '<'\n\tequal   = '='\n)\n\n// buildCompareClause build clause with specified bounds. Usually we will use the following two conditions:\n// (compare, writeEqual) == (less, false), return quotaCols < bound clause. In other words, (-inf, bound)\n// (compare, writeEqual) == (greater, true), return quotaCols >= bound clause. In other words, [bound, +inf)\nfunc buildCompareClause(buf *bytes.Buffer, quotaCols []string, bound []string, compare byte, writeEqual bool) { // revive:disable-line:flag-parameter\n\tfor i, col := range quotaCols {\n\t\tif i > 0 {\n\t\t\tbuf.WriteString(\"or(\")\n\t\t}\n\t\tfor j := 0; j < i; j++ {\n\t\t\tbuf.WriteString(quotaCols[j])\n\t\t\tbuf.WriteByte(equal)\n\t\t\tbuf.WriteString(bound[j])\n\t\t\tbuf.WriteString(\" and \")\n\t\t}\n\t\tbuf.WriteString(col)\n\t\tbuf.WriteByte(compare)\n\t\tif writeEqual && i == len(quotaCols)-1 {\n\t\t\tbuf.WriteByte(equal)\n\t\t}\n\t\tbuf.WriteString(bound[i])\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(')')\n\t\t} else if i != len(quotaCols)-1 {\n\t\t\tbuf.WriteByte(' ')\n\t\t}\n\t}\n}\n\n// getCommonLength returns the common length of low and up\nfunc getCommonLength(low []string, up []string) int {\n\tfor i := range low {\n\t\tif low[i] != up[i] {\n\t\t\treturn i\n\t\t}\n\t}\n\treturn len(low)\n}\n\n// buildBetweenClause build clause in a specified table range.\n// the result where clause will be low <= quotaCols < up. In other words, [low, up)\nfunc buildBetweenClause(buf *bytes.Buffer, quotaCols []string, low []string, up []string) {\n\tsingleBetween := func(writeEqual bool) {\n\t\tbuf.WriteString(quotaCols[0])\n\t\tbuf.WriteByte(greater)\n\t\tif writeEqual {\n\t\t\tbuf.WriteByte(equal)\n\t\t}\n\t\tbuf.WriteString(low[0])\n\t\tbuf.WriteString(\" and \")\n\t\tbuf.WriteString(quotaCols[0])\n\t\tbuf.WriteByte(less)\n\t\tbuf.WriteString(up[0])\n\t}\n\t// handle special cases with common prefix\n\tcommonLen := getCommonLength(low, up)\n\tif commonLen > 0 {\n\t\t// unexpected case for low == up, return empty result\n\t\tif commonLen == len(low) {\n\t\t\tbuf.WriteString(\"false\")\n\t\t\treturn\n\t\t}\n\t\tfor i := 0; i < commonLen; i++ {\n\t\t\tif i > 0 {\n\t\t\t\tbuf.WriteString(\" and \")\n\t\t\t}\n\t\t\tbuf.WriteString(quotaCols[i])\n\t\t\tbuf.WriteByte(equal)\n\t\t\tbuf.WriteString(low[i])\n\t\t}\n\t\tbuf.WriteString(\" and(\")\n\t\tdefer buf.WriteByte(')')\n\t\tquotaCols = quotaCols[commonLen:]\n\t\tlow = low[commonLen:]\n\t\tup = up[commonLen:]\n\t}\n\n\t// handle special cases with only one column\n\tif len(quotaCols) == 1 {\n\t\tsingleBetween(true)\n\t\treturn\n\t}\n\tbuf.WriteByte('(')\n\tsingleBetween(false)\n\tbuf.WriteString(\")or(\")\n\tbuf.WriteString(quotaCols[0])\n\tbuf.WriteByte(equal)\n\tbuf.WriteString(low[0])\n\tbuf.WriteString(\" and(\")\n\tbuildCompareClause(buf, quotaCols[1:], low[1:], greater, true)\n\tbuf.WriteString(\"))or(\")\n\tbuf.WriteString(quotaCols[0])\n\tbuf.WriteByte(equal)\n\tbuf.WriteString(up[0])\n\tbuf.WriteString(\" and(\")\n\tbuildCompareClause(buf, quotaCols[1:], up[1:], less, false)\n\tbuf.WriteString(\"))\")\n}\n\nfunc buildOrderByClauseString(handleColNames []string) string {\n\tif len(handleColNames) == 0 {\n\t\treturn \"\"\n\t}\n\tseparator := \",\"\n\tquotaCols := make([]string, len(handleColNames))\n\tfor i, col := range handleColNames {\n\t\tquotaCols[i] = fmt.Sprintf(\"`%s`\", escapeString(col))\n\t}\n\treturn fmt.Sprintf(\"ORDER BY %s\", strings.Join(quotaCols, separator))\n}\n\nfunc buildLockTablesSQL(allTables DatabaseTables, blockList map[string]map[string]interface{}) string {\n\t// ,``.`` READ has 11 bytes, \"LOCK TABLE\" has 10 bytes\n\testimatedCap := len(allTables)*11 + 10\n\ts := bytes.NewBuffer(make([]byte, 0, estimatedCap))\n\tn := false\n\tfor dbName, tables := range allTables {\n\t\tescapedDBName := escapeString(dbName)\n\t\tfor _, table := range tables {\n\t\t\t// Lock views will lock related tables. However, we won't dump data only the create sql of view, so we needn't lock view here.\n\t\t\t// Besides, mydumper also only lock base table here. https://github.com/maxbube/mydumper/blob/1fabdf87e3007e5934227b504ad673ba3697946c/mydumper.c#L1568\n\t\t\tif table.Type != TableTypeBase {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif blockTable, ok := blockList[dbName]; ok {\n\t\t\t\tif _, ok := blockTable[table.Name]; ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !n {\n\t\t\t\tfmt.Fprintf(s, \"LOCK TABLES `%s`.`%s` READ\", escapedDBName, escapeString(table.Name))\n\t\t\t\tn = true\n\t\t\t} else {\n\t\t\t\tfmt.Fprintf(s, \",`%s`.`%s` READ\", escapedDBName, escapeString(table.Name))\n\t\t\t}\n\t\t}\n\t}\n\treturn s.String()\n}\n\ntype oneStrColumnTable struct {\n\tdata []string\n}\n\nfunc (o *oneStrColumnTable) handleOneRow(rows *sql.Rows) error {\n\tvar str string\n\tif err := rows.Scan(&str); err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\to.data = append(o.data, str)\n\treturn nil\n}\n\nfunc simpleQuery(conn *sql.Conn, query string, handleOneRow func(*sql.Rows) error) error {\n\treturn simpleQueryWithArgs(context.Background(), conn, handleOneRow, query)\n}\n\nfunc simpleQueryWithArgs(ctx context.Context, conn *sql.Conn, handleOneRow func(*sql.Rows) error, query string, args ...interface{}) error {\n\tvar (\n\t\trows *sql.Rows\n\t\terr  error\n\t)\n\tif len(args) > 0 {\n\t\trows, err = conn.QueryContext(ctx, query, args...)\n\t} else {\n\t\trows, err = conn.QueryContext(ctx, query)\n\t}\n\tif err != nil {\n\t\treturn errors.Annotatef(err, \"sql: %s, args: %s\", query, args)\n\t}\n\tdefer rows.Close()\n\n\tfor rows.Next() {\n\t\tif err := handleOneRow(rows); err != nil {\n\t\t\trows.Close()\n\t\t\treturn errors.Annotatef(err, \"sql: %s, args: %s\", query, args)\n\t\t}\n\t}\n\treturn errors.Annotatef(rows.Err(), \"sql: %s, args: %s\", query, args)\n}\n\nfunc pickupPossibleField(tctx *tcontext.Context, meta TableMeta, db *BaseConn) (string, error) {\n\t// try using _tidb_rowid first\n\tif meta.HasImplicitRowID() {\n\t\treturn \"_tidb_rowid\", nil\n\t}\n\t// try to use pk or uk\n\tfieldName, err := getNumericIndex(tctx, db, meta)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// if fieldName == \"\", there is no proper index\n\treturn fieldName, nil\n}\n\nfunc estimateCount(tctx *tcontext.Context, dbName, tableName string, db *BaseConn, field string, conf *Config) uint64 {\n\tvar query string\n\tif strings.TrimSpace(field) == \"*\" || strings.TrimSpace(field) == \"\" {\n\t\tquery = fmt.Sprintf(\"EXPLAIN SELECT * FROM `%s`.`%s`\", escapeString(dbName), escapeString(tableName))\n\t} else {\n\t\tquery = fmt.Sprintf(\"EXPLAIN SELECT `%s` FROM `%s`.`%s`\", escapeString(field), escapeString(dbName), escapeString(tableName))\n\t}\n\n\tif conf.Where != \"\" {\n\t\tquery += \" WHERE \"\n\t\tquery += conf.Where\n\t}\n\n\testRows := detectEstimateRows(tctx, db, query, []string{\"rows\", \"estRows\", \"count\"})\n\t/* tidb results field name is estRows (before 4.0.0-beta.2: count)\n\t\t+-----------------------+----------+-----------+---------------------------------------------------------+\n\t\t| id                    | estRows  | task      | access object | operator info                           |\n\t\t+-----------------------+----------+-----------+---------------------------------------------------------+\n\t\t| tablereader_5         | 10000.00 | root      |               | data:tablefullscan_4                    |\n\t\t| \u2514\u2500tablefullscan_4     | 10000.00 | cop[tikv] | table:a       | table:a, keep order:false, stats:pseudo |\n\t\t+-----------------------+----------+-----------+----------------------------------------------------------\n\n\tmariadb result field name is rows\n\t\t+------+-------------+---------+-------+---------------+------+---------+------+----------+-------------+\n\t\t| id   | select_type | table   | type  | possible_keys | key  | key_len | ref  | rows     | Extra       |\n\t\t+------+-------------+---------+-------+---------------+------+---------+------+----------+-------------+\n\t\t|    1 | SIMPLE      | sbtest1 | index | NULL          | k_1  | 4       | NULL | 15000049 | Using index |\n\t\t+------+-------------+---------+-------+---------------+------+---------+------+----------+-------------+\n\n\tmysql result field name is rows\n\t\t+----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+-------------+\n\t\t| id | select_type | table | partitions | type  | possible_keys | key       | key_len | ref  | rows | filtered | Extra       |\n\t\t+----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+-------------+\n\t\t|  1 | SIMPLE      | t1    | NULL       | index | NULL          | multi_col | 10      | NULL |    5 |   100.00 | Using index |\n\t\t+----+-------------+-------+------------+-------+---------------+-----------+---------+------+------+----------+-------------+\n\t*/\n\tif estRows > 0 {\n\t\treturn estRows\n\t}\n\treturn 0\n}\n\nfunc detectEstimateRows(tctx *tcontext.Context, db *BaseConn, query string, fieldNames []string) uint64 {\n\tvar (\n\t\tfieldIndex int\n\t\toneRow     []sql.NullString\n\t)\n\terr := db.QuerySQL(tctx, func(rows *sql.Rows) error {\n\t\tcolumns, err := rows.Columns()\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\taddr := make([]interface{}, len(columns))\n\t\toneRow = make([]sql.NullString, len(columns))\n\t\tfieldIndex = -1\n\tfound:\n\t\tfor i := range oneRow {\n\t\t\tfor _, fieldName := range fieldNames {\n\t\t\t\tif strings.EqualFold(columns[i], fieldName) {\n\t\t\t\t\tfieldIndex = i\n\t\t\t\t\tbreak found\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif fieldIndex == -1 {\n\t\t\trows.Close()\n\t\t\treturn nil\n\t\t}\n\n\t\tfor i := range oneRow {\n\t\t\taddr[i] = &oneRow[i]\n\t\t}\n\t\treturn rows.Scan(addr...)\n\t}, func() {}, query)\n\tif err != nil || fieldIndex == -1 {\n\t\ttctx.L().Info(\"can't estimate rows from db\",\n\t\t\tzap.String(\"query\", query), zap.Int(\"fieldIndex\", fieldIndex), log.ShortError(err))\n\t\treturn 0\n\t}\n\n\testRows, err := strconv.ParseFloat(oneRow[fieldIndex].String, 64)\n\tif err != nil {\n\t\ttctx.L().Info(\"can't get parse estimate rows from db\",\n\t\t\tzap.String(\"query\", query), zap.String(\"estRows\", oneRow[fieldIndex].String), log.ShortError(err))\n\t\treturn 0\n\t}\n\treturn uint64(estRows)\n}\n\nfunc parseSnapshotToTSO(pool *sql.DB, snapshot string) (uint64, error) {\n\tsnapshotTS, err := strconv.ParseUint(snapshot, 10, 64)\n\tif err == nil {\n\t\treturn snapshotTS, nil\n\t}\n\tvar tso sql.NullInt64\n\tquery := \"SELECT unix_timestamp(?)\"\n\trow := pool.QueryRow(query, snapshot)\n\terr = row.Scan(&tso)\n\tif err != nil {\n\t\treturn 0, errors.Annotatef(err, \"sql: %s\", strings.ReplaceAll(query, \"?\", fmt.Sprintf(`\"%s\"`, snapshot)))\n\t}\n\tif !tso.Valid {\n\t\treturn 0, errors.Errorf(\"snapshot %s format not supported. please use tso or '2006-01-02 15:04:05' format time\", snapshot)\n\t}\n\treturn (uint64(tso.Int64) << 18) * 1000, nil\n}\n\nfunc buildWhereCondition(conf *Config, where string) string {\n\tvar query strings.Builder\n\tseparator := \"WHERE\"\n\tleftBracket := \" \"\n\trightBracket := \" \"\n\tif conf.Where != \"\" && where != \"\" {\n\t\tleftBracket = \" (\"\n\t\trightBracket = \") \"\n\t}\n\tif conf.Where != \"\" {\n\t\tquery.WriteString(separator)\n\t\tquery.WriteString(leftBracket)\n\t\tquery.WriteString(conf.Where)\n\t\tquery.WriteString(rightBracket)\n\t\tseparator = \"AND\"\n\t}\n\tif where != \"\" {\n\t\tquery.WriteString(separator)\n\t\tquery.WriteString(leftBracket)\n\t\tquery.WriteString(where)\n\t\tquery.WriteString(rightBracket)\n\t}\n\treturn query.String()\n}\n\nfunc escapeString(s string) string {\n\treturn strings.ReplaceAll(s, \"`\", \"``\")\n}\n\n// GetPartitionNames get partition names from a specified table\nfunc GetPartitionNames(tctx *tcontext.Context, db *BaseConn, schema, table string) (partitions []string, err error) {\n\tpartitions = make([]string, 0)\n\tvar partitionName sql.NullString\n\terr = db.QuerySQL(tctx, func(rows *sql.Rows) error {\n\t\terr := rows.Scan(&partitionName)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tif partitionName.Valid {\n\t\t\tpartitions = append(partitions, partitionName.String)\n\t\t}\n\t\treturn nil\n\t}, func() {\n\t\tpartitions = partitions[:0]\n\t}, \"SELECT PARTITION_NAME from INFORMATION_SCHEMA.PARTITIONS WHERE TABLE_SCHEMA = ? AND TABLE_NAME = ?\", schema, table)\n\treturn\n}\n\n// GetPartitionTableIDs get partition tableIDs through histograms.\n// SHOW STATS_HISTOGRAMS  has db_name,table_name,partition_name but doesn't have partition id\n// mysql.stats_histograms has partition_id but doesn't have db_name,table_name,partition_name\n// So we combine the results from these two sqls to get partition ids for each table\n// If UPDATE_TIME,DISTINCT_COUNT are equal, we assume these two records can represent one line.\n// If histograms are not accurate or (UPDATE_TIME,DISTINCT_COUNT) has duplicate data, it's still fine.\n// Because the possibility is low and the effect is that we will select more than one regions in one time,\n// this will not affect the correctness of the dumping data and will not affect the memory usage much.\n// This method is tricky, but no better way is found.\n// Because TiDB v3.0.0's information_schema.partition table doesn't have partition name or partition id info\n// return (dbName -> tbName -> partitionName -> partitionID, error)\nfunc GetPartitionTableIDs(db *sql.Conn, tables map[string]map[string]struct{}) (map[string]map[string]map[string]int64, error) {\n\tconst (\n\t\tshowStatsHistogramsSQL   = \"SHOW STATS_HISTOGRAMS\"\n\t\tselectStatsHistogramsSQL = \"SELECT TABLE_ID,FROM_UNIXTIME(VERSION DIV 262144 DIV 1000,'%Y-%m-%d %H:%i:%s') AS UPDATE_TIME,DISTINCT_COUNT FROM mysql.stats_histograms\"\n\t)\n\tpartitionIDs := make(map[string]map[string]map[string]int64, len(tables))\n\trows, err := db.QueryContext(context.Background(), showStatsHistogramsSQL)\n\tif err != nil {\n\t\treturn nil, errors.Annotatef(err, \"sql: %s\", showStatsHistogramsSQL)\n\t}\n\tresults, err := GetSpecifiedColumnValuesAndClose(rows, \"DB_NAME\", \"TABLE_NAME\", \"PARTITION_NAME\", \"UPDATE_TIME\", \"DISTINCT_COUNT\")\n\tif err != nil {\n\t\treturn nil, errors.Annotatef(err, \"sql: %s\", showStatsHistogramsSQL)\n\t}\n\ttype partitionInfo struct {\n\t\tdbName, tbName, partitionName string\n\t}\n\tsaveMap := make(map[string]map[string]partitionInfo)\n\tfor _, oneRow := range results {\n\t\tdbName, tbName, partitionName, updateTime, distinctCount := oneRow[0], oneRow[1], oneRow[2], oneRow[3], oneRow[4]\n\t\tif len(partitionName) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tif tbm, ok := tables[dbName]; ok {\n\t\t\tif _, ok = tbm[tbName]; ok {\n\t\t\t\tif _, ok = saveMap[updateTime]; !ok {\n\t\t\t\t\tsaveMap[updateTime] = make(map[string]partitionInfo)\n\t\t\t\t}\n\t\t\t\tsaveMap[updateTime][distinctCount] = partitionInfo{\n\t\t\t\t\tdbName:        dbName,\n\t\t\t\t\ttbName:        tbName,\n\t\t\t\t\tpartitionName: partitionName,\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif len(saveMap) == 0 {\n\t\treturn map[string]map[string]map[string]int64{}, nil\n\t}\n\terr = simpleQuery(db, selectStatsHistogramsSQL, func(rows *sql.Rows) error {\n\t\tvar (\n\t\t\ttableID                   int64\n\t\t\tupdateTime, distinctCount string\n\t\t)\n\t\terr2 := rows.Scan(&tableID, &updateTime, &distinctCount)\n\t\tif err2 != nil {\n\t\t\treturn errors.Trace(err2)\n\t\t}\n\t\tif mpt, ok := saveMap[updateTime]; ok {\n\t\t\tif partition, ok := mpt[distinctCount]; ok {\n\t\t\t\tdbName, tbName, partitionName := partition.dbName, partition.tbName, partition.partitionName\n\t\t\t\tif _, ok := partitionIDs[dbName]; !ok {\n\t\t\t\t\tpartitionIDs[dbName] = make(map[string]map[string]int64)\n\t\t\t\t}\n\t\t\t\tif _, ok := partitionIDs[dbName][tbName]; !ok {\n\t\t\t\t\tpartitionIDs[dbName][tbName] = make(map[string]int64)\n\t\t\t\t}\n\t\t\t\tpartitionIDs[dbName][tbName][partitionName] = tableID\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\treturn partitionIDs, err\n}\n\n// GetDBInfo get model.DBInfos from database sql interface.\n// We need table_id to check whether a region belongs to this table\nfunc GetDBInfo(db *sql.Conn, tables map[string]map[string]struct{}) ([]*model.DBInfo, error) {\n\tconst tableIDSQL = \"SELECT TABLE_SCHEMA,TABLE_NAME,TIDB_TABLE_ID FROM INFORMATION_SCHEMA.TABLES ORDER BY TABLE_SCHEMA\"\n\n\tschemas := make([]*model.DBInfo, 0, len(tables))\n\tvar (\n\t\ttableSchema, tableName string\n\t\ttidbTableID            int64\n\t)\n\tpartitionIDs, err := GetPartitionTableIDs(db, tables)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\terr = simpleQuery(db, tableIDSQL, func(rows *sql.Rows) error {\n\t\terr2 := rows.Scan(&tableSchema, &tableName, &tidbTableID)\n\t\tif err2 != nil {\n\t\t\treturn errors.Trace(err2)\n\t\t}\n\t\tif tbm, ok := tables[tableSchema]; !ok {\n\t\t\treturn nil\n\t\t} else if _, ok = tbm[tableName]; !ok {\n\t\t\treturn nil\n\t\t}\n\t\tlast := len(schemas) - 1\n\t\tif last < 0 || schemas[last].Name.O != tableSchema {\n\t\t\tschemas = append(schemas, &model.DBInfo{\n\t\t\t\tName:   model.CIStr{O: tableSchema},\n\t\t\t\tTables: make([]*model.TableInfo, 0, len(tables[tableSchema])),\n\t\t\t})\n\t\t\tlast++\n\t\t}\n\t\tvar partition *model.PartitionInfo\n\t\tif tbm, ok := partitionIDs[tableSchema]; ok {\n\t\t\tif ptm, ok := tbm[tableName]; ok {\n\t\t\t\tpartition = &model.PartitionInfo{Definitions: make([]model.PartitionDefinition, 0, len(ptm))}\n\t\t\t\tfor partitionName, partitionID := range ptm {\n\t\t\t\t\tpartition.Definitions = append(partition.Definitions, model.PartitionDefinition{\n\t\t\t\t\t\tID:   partitionID,\n\t\t\t\t\t\tName: model.CIStr{O: partitionName},\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tschemas[last].Tables = append(schemas[last].Tables, &model.TableInfo{\n\t\t\tID:        tidbTableID,\n\t\t\tName:      model.CIStr{O: tableName},\n\t\t\tPartition: partition,\n\t\t})\n\t\treturn nil\n\t})\n\treturn schemas, err\n}\n\n// GetRegionInfos get region info including regionID, start key, end key from database sql interface.\n// start key, end key includes information to help split table\nfunc GetRegionInfos(db *sql.Conn) (*helper.RegionsInfo, error) {\n\tconst tableRegionSQL = \"SELECT REGION_ID,START_KEY,END_KEY FROM INFORMATION_SCHEMA.TIKV_REGION_STATUS ORDER BY START_KEY;\"\n\tvar (\n\t\tregionID         int64\n\t\tstartKey, endKey string\n\t)\n\tregionsInfo := &helper.RegionsInfo{Regions: make([]helper.RegionInfo, 0)}\n\terr := simpleQuery(db, tableRegionSQL, func(rows *sql.Rows) error {\n\t\terr := rows.Scan(&regionID, &startKey, &endKey)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tregionsInfo.Regions = append(regionsInfo.Regions, helper.RegionInfo{\n\t\t\tID:       regionID,\n\t\t\tStartKey: startKey,\n\t\t\tEndKey:   endKey,\n\t\t})\n\t\treturn nil\n\t})\n\treturn regionsInfo, err\n}\n\n// GetCharsetAndDefaultCollation gets charset and default collation map.\nfunc GetCharsetAndDefaultCollation(ctx context.Context, db *sql.Conn) (map[string]string, error) {\n\tcharsetAndDefaultCollation := make(map[string]string)\n\tquery := \"SHOW CHARACTER SET\"\n\n\t// Show an example.\n\t/*\n\t\tmysql> SHOW CHARACTER SET;\n\t\t+----------+---------------------------------+---------------------+--------+\n\t\t| Charset  | Description                     | Default collation   | Maxlen |\n\t\t+----------+---------------------------------+---------------------+--------+\n\t\t| armscii8 | ARMSCII-8 Armenian              | armscii8_general_ci |      1 |\n\t\t| ascii    | US ASCII                        | ascii_general_ci    |      1 |\n\t\t| big5     | Big5 Traditional Chinese        | big5_chinese_ci     |      2 |\n\t\t| binary   | Binary pseudo charset           | binary              |      1 |\n\t\t| cp1250   | Windows Central European        | cp1250_general_ci   |      1 |\n\t\t| cp1251   | Windows Cyrillic                | cp1251_general_ci   |      1 |\n\t\t+----------+---------------------------------+---------------------+--------+\n\t*/\n\n\trows, err := db.QueryContext(ctx, query)\n\tif err != nil {\n\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t}\n\n\tdefer rows.Close()\n\tfor rows.Next() {\n\t\tvar charset, description, collation string\n\t\tvar maxlen int\n\t\tif scanErr := rows.Scan(&charset, &description, &collation, &maxlen); scanErr != nil {\n\t\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t\t}\n\t\tcharsetAndDefaultCollation[strings.ToLower(charset)] = collation\n\t}\n\tif err = rows.Close(); err != nil {\n\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t}\n\tif err = rows.Err(); err != nil {\n\t\treturn nil, errors.Annotatef(err, \"sql: %s\", query)\n\t}\n\treturn charsetAndDefaultCollation, err\n}\n", "// Copyright 2020 PingCAP, Inc. Licensed under Apache-2.0.\n\npackage export\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"database/sql/driver\"\n\t\"encoding/csv\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/DATA-DOG/go-sqlmock\"\n\t\"github.com/go-sql-driver/mysql\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/tidb/br/pkg/version\"\n\tdbconfig \"github.com/pingcap/tidb/config\"\n\ttcontext \"github.com/pingcap/tidb/dumpling/context\"\n\t\"github.com/pingcap/tidb/util/promutil\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nvar showIndexHeaders = []string{\n\t\"Table\",\n\t\"Non_unique\",\n\t\"Key_name\",\n\t\"Seq_in_index\",\n\t\"Column_name\",\n\t\"Collation\",\n\t\"Cardinality\",\n\t\"Sub_part\",\n\t\"Packed\",\n\t\"Null\",\n\t\"Index_type\",\n\t\"Comment\",\n\t\"Index_comment\",\n}\n\nconst (\n\tdatabase = \"foo\"\n\ttable    = \"bar\"\n)\n\nfunc TestBuildSelectAllQuery(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\n\tmockConf := defaultConfigForTest(t)\n\tmockConf.SortByPk = true\n\n\t// Test TiDB server.\n\tmockConf.ServerInfo.ServerType = version.ServerTypeTiDB\n\ttctx := tcontext.Background().WithLogger(appLogger)\n\tbaseConn := newBaseConn(conn, true, nil)\n\n\torderByClause, err := buildOrderByClause(tctx, mockConf, baseConn, database, table, true)\n\trequire.NoError(t, err)\n\n\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\tAddRow(\"id\", \"int(11)\", \"NO\", \"PRI\", nil, \"\"))\n\n\tselectedField, _, err := buildSelectField(tctx, baseConn, database, table, false)\n\trequire.NoError(t, err)\n\n\tq := buildSelectQuery(database, table, selectedField, \"\", \"\", orderByClause)\n\trequire.Equal(t, fmt.Sprintf(\"SELECT * FROM `%s`.`%s` ORDER BY `_tidb_rowid`\", database, table), q)\n\n\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).\n\t\tWillReturnRows(sqlmock.NewRows(showIndexHeaders).\n\t\t\tAddRow(table, 0, \"PRIMARY\", 1, \"id\", \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\"))\n\n\torderByClause, err = buildOrderByClause(tctx, mockConf, baseConn, database, table, false)\n\trequire.NoError(t, err)\n\n\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\tAddRow(\"id\", \"int(11)\", \"NO\", \"PRI\", nil, \"\"))\n\n\tselectedField, _, err = buildSelectField(tctx, baseConn, database, table, false)\n\trequire.NoError(t, err)\n\n\tq = buildSelectQuery(database, table, selectedField, \"\", \"\", orderByClause)\n\trequire.Equal(t, fmt.Sprintf(\"SELECT * FROM `%s`.`%s` ORDER BY `id`\", database, table), q)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\t// Test other servers.\n\totherServers := []version.ServerType{version.ServerTypeUnknown, version.ServerTypeMySQL, version.ServerTypeMariaDB}\n\n\t// Test table with primary key.\n\tfor _, serverTp := range otherServers {\n\t\tmockConf.ServerInfo.ServerType = serverTp\n\t\tcomment := fmt.Sprintf(\"server type: %s\", serverTp)\n\n\t\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).\n\t\t\tWillReturnRows(sqlmock.NewRows(showIndexHeaders).\n\t\t\t\tAddRow(table, 0, \"PRIMARY\", 1, \"id\", \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\"))\n\t\torderByClause, err := buildOrderByClause(tctx, mockConf, baseConn, database, table, false)\n\t\trequire.NoError(t, err, comment)\n\n\t\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").\n\t\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\t\tAddRow(\"id\", \"int(11)\", \"NO\", \"PRI\", nil, \"\"))\n\n\t\tselectedField, _, err = buildSelectField(tctx, baseConn, database, table, false)\n\t\trequire.NoError(t, err, comment)\n\n\t\tq = buildSelectQuery(database, table, selectedField, \"\", \"\", orderByClause)\n\t\trequire.Equal(t, fmt.Sprintf(\"SELECT * FROM `%s`.`%s` ORDER BY `id`\", database, table), q, comment)\n\n\t\terr = mock.ExpectationsWereMet()\n\t\trequire.NoError(t, err, comment)\n\t\trequire.NoError(t, mock.ExpectationsWereMet(), comment)\n\t}\n\n\t// Test table without primary key.\n\tfor _, serverTp := range otherServers {\n\t\tmockConf.ServerInfo.ServerType = serverTp\n\t\tcomment := fmt.Sprintf(\"server type: %s\", serverTp)\n\n\t\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).\n\t\t\tWillReturnRows(sqlmock.NewRows(showIndexHeaders))\n\n\t\torderByClause, err := buildOrderByClause(tctx, mockConf, baseConn, database, table, false)\n\t\trequire.NoError(t, err, comment)\n\n\t\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").\n\t\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\t\tAddRow(\"id\", \"int(11)\", \"NO\", \"PRI\", nil, \"\"))\n\n\t\tselectedField, _, err = buildSelectField(tctx, baseConn, \"test\", \"t\", false)\n\t\trequire.NoError(t, err, comment)\n\n\t\tq := buildSelectQuery(database, table, selectedField, \"\", \"\", orderByClause)\n\t\trequire.Equal(t, fmt.Sprintf(\"SELECT * FROM `%s`.`%s`\", database, table), q, comment)\n\n\t\terr = mock.ExpectationsWereMet()\n\t\trequire.NoError(t, err, comment)\n\t\trequire.NoError(t, mock.ExpectationsWereMet(), comment)\n\t}\n\n\t// Test when config.SortByPk is disabled.\n\tmockConf.SortByPk = false\n\tfor tp := version.ServerTypeUnknown; tp < version.ServerTypeAll; tp++ {\n\t\tmockConf.ServerInfo.ServerType = version.ServerType(tp)\n\t\tcomment := fmt.Sprintf(\"current server type: %v\", tp)\n\n\t\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").\n\t\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\t\tAddRow(\"id\", \"int(11)\", \"NO\", \"PRI\", nil, \"\"))\n\n\t\tselectedField, _, err := buildSelectField(tctx, baseConn, \"test\", \"t\", false)\n\t\trequire.NoError(t, err, comment)\n\n\t\tq := buildSelectQuery(database, table, selectedField, \"\", \"\", \"\")\n\t\trequire.Equal(t, fmt.Sprintf(\"SELECT * FROM `%s`.`%s`\", database, table), q, comment)\n\t\trequire.NoError(t, mock.ExpectationsWereMet(), comment)\n\t}\n}\n\nfunc TestBuildOrderByClause(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\ttctx := tcontext.Background().WithLogger(appLogger)\n\tbaseConn := newBaseConn(conn, true, nil)\n\n\tmockConf := defaultConfigForTest(t)\n\tmockConf.SortByPk = true\n\n\t// Test TiDB server.\n\tmockConf.ServerInfo.ServerType = version.ServerTypeTiDB\n\n\torderByClause, err := buildOrderByClause(tctx, mockConf, baseConn, database, table, true)\n\trequire.NoError(t, err)\n\trequire.Equal(t, orderByTiDBRowID, orderByClause)\n\n\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).\n\t\tWillReturnRows(sqlmock.NewRows(showIndexHeaders).AddRow(table, 0, \"PRIMARY\", 1, \"id\", \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\"))\n\n\torderByClause, err = buildOrderByClause(tctx, mockConf, baseConn, database, table, false)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"ORDER BY `id`\", orderByClause)\n\n\t// Test table with primary key.\n\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).\n\t\tWillReturnRows(sqlmock.NewRows(showIndexHeaders).AddRow(table, 0, \"PRIMARY\", 1, \"id\", \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\"))\n\torderByClause, err = buildOrderByClause(tctx, mockConf, baseConn, database, table, false)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"ORDER BY `id`\", orderByClause)\n\n\t// Test table with joint primary key.\n\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).\n\t\tWillReturnRows(sqlmock.NewRows(showIndexHeaders).\n\t\t\tAddRow(table, 0, \"PRIMARY\", 1, \"id\", \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\").\n\t\t\tAddRow(table, 0, \"PRIMARY\", 2, \"name\", \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\"))\n\torderByClause, err = buildOrderByClause(tctx, mockConf, baseConn, database, table, false)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"ORDER BY `id`,`name`\", orderByClause)\n\n\t// Test table without primary key.\n\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).\n\t\tWillReturnRows(sqlmock.NewRows(showIndexHeaders))\n\n\torderByClause, err = buildOrderByClause(tctx, mockConf, baseConn, database, table, false)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"\", orderByClause)\n\n\t// Test when config.SortByPk is disabled.\n\tmockConf.SortByPk = false\n\tfor _, hasImplicitRowID := range []bool{false, true} {\n\t\tcomment := fmt.Sprintf(\"current hasImplicitRowID: %v\", hasImplicitRowID)\n\n\t\torderByClause, err := buildOrderByClause(tctx, mockConf, baseConn, database, table, hasImplicitRowID)\n\t\trequire.NoError(t, err, comment)\n\t\trequire.Equal(t, \"\", orderByClause, comment)\n\t}\n\n\t// Test build OrderByClause with retry\n\tbaseConn = newBaseConn(conn, true, func(conn *sql.Conn, b bool) (*sql.Conn, error) {\n\t\treturn conn, nil\n\t})\n\tquery := fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)\n\tmock.ExpectQuery(query).WillReturnError(errors.New(\"invalid connection\"))\n\tmock.ExpectQuery(query).WillReturnError(errors.New(\"invalid connection\"))\n\tmock.ExpectQuery(query).WillReturnRows(sqlmock.NewRows(showIndexHeaders).AddRow(table, 0, \"PRIMARY\", 1, \"id\", \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\"))\n\tmockConf.SortByPk = true\n\torderByClause, err = buildOrderByClause(tctx, mockConf, baseConn, database, table, false)\n\trequire.NoError(t, err)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n\trequire.Equal(t, \"ORDER BY `id`\", orderByClause)\n}\n\nfunc TestBuildSelectField(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\ttctx := tcontext.Background().WithLogger(appLogger)\n\tbaseConn := newBaseConn(conn, true, nil)\n\n\t// generate columns not found\n\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\tAddRow(\"id\", \"int(11)\", \"NO\", \"PRI\", nil, \"\"))\n\n\tselectedField, _, err := buildSelectField(tctx, baseConn, \"test\", \"t\", false)\n\trequire.Equal(t, \"*\", selectedField)\n\trequire.NoError(t, err)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\t// user assigns completeInsert\n\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\tAddRow(\"id\", \"int(11)\", \"NO\", \"PRI\", nil, \"\").\n\t\t\tAddRow(\"name\", \"varchar(12)\", \"NO\", \"\", nil, \"\").\n\t\t\tAddRow(\"quo`te\", \"varchar(12)\", \"NO\", \"UNI\", nil, \"\"))\n\n\tselectedField, _, err = buildSelectField(tctx, baseConn, \"test\", \"t\", true)\n\trequire.Equal(t, \"`id`,`name`,`quo``te`\", selectedField)\n\trequire.NoError(t, err)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\t// found generate columns, rest columns is `id`,`name`\n\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\tAddRow(\"id\", \"int(11)\", \"NO\", \"PRI\", nil, \"\").\n\t\t\tAddRow(\"name\", \"varchar(12)\", \"NO\", \"\", nil, \"\").\n\t\t\tAddRow(\"quo`te\", \"varchar(12)\", \"NO\", \"UNI\", nil, \"\").\n\t\t\tAddRow(\"generated\", \"varchar(12)\", \"NO\", \"\", nil, \"VIRTUAL GENERATED\"))\n\n\tselectedField, _, err = buildSelectField(tctx, baseConn, \"test\", \"t\", false)\n\trequire.Equal(t, \"`id`,`name`,`quo``te`\", selectedField)\n\trequire.NoError(t, err)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\t// Test build SelectField with retry\n\tbaseConn = newBaseConn(conn, true, func(conn *sql.Conn, b bool) (*sql.Conn, error) {\n\t\treturn conn, nil\n\t})\n\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").WillReturnError(errors.New(\"invalid connection\"))\n\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").WillReturnError(errors.New(\"invalid connection\"))\n\tmock.ExpectQuery(\"SHOW COLUMNS FROM\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\tAddRow(\"id\", \"int(11)\", \"NO\", \"PRI\", nil, \"\"))\n\n\tselectedField, _, err = buildSelectField(tctx, baseConn, \"test\", \"t\", false)\n\trequire.Equal(t, \"*\", selectedField)\n\trequire.NoError(t, err)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n}\n\nfunc TestParseSnapshotToTSO(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\t_ = db.Close()\n\t}()\n\n\tsnapshot := \"2020/07/18 20:31:50\"\n\tvar unixTimeStamp uint64 = 1595075510\n\t// generate columns valid snapshot\n\tmock.ExpectQuery(`SELECT unix_timestamp(?)`).\n\t\tWithArgs(sqlmock.AnyArg()).\n\t\tWillReturnRows(sqlmock.NewRows([]string{`unix_timestamp(\"2020/07/18 20:31:50\")`}).AddRow(1595075510))\n\ttso, err := parseSnapshotToTSO(db, snapshot)\n\trequire.NoError(t, err)\n\trequire.Equal(t, (unixTimeStamp<<18)*1000, tso)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\t// generate columns not valid snapshot\n\tmock.ExpectQuery(`SELECT unix_timestamp(?)`).\n\t\tWithArgs(sqlmock.AnyArg()).\n\t\tWillReturnRows(sqlmock.NewRows([]string{`unix_timestamp(\"XXYYZZ\")`}).AddRow(nil))\n\ttso, err = parseSnapshotToTSO(db, \"XXYYZZ\")\n\trequire.EqualError(t, err, \"snapshot XXYYZZ format not supported. please use tso or '2006-01-02 15:04:05' format time\")\n\trequire.Equal(t, uint64(0), tso)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n}\n\nfunc TestShowCreateView(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\ttctx := tcontext.Background().WithLogger(appLogger)\n\tbaseConn := newBaseConn(conn, true, nil)\n\n\tmock.ExpectQuery(\"SHOW FIELDS FROM `test`.`v`\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Field\", \"Type\", \"Null\", \"Key\", \"Default\", \"Extra\"}).\n\t\t\tAddRow(\"a\", \"int(11)\", \"YES\", nil, \"NULL\", nil))\n\n\tmock.ExpectQuery(\"SHOW CREATE VIEW `test`.`v`\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"View\", \"Create View\", \"character_set_client\", \"collation_connection\"}).\n\t\t\tAddRow(\"v\", \"CREATE ALGORITHM=UNDEFINED DEFINER=`root`@`localhost` SQL SECURITY DEFINER VIEW `v` (`a`) AS SELECT `t`.`a` AS `a` FROM `test`.`t`\", \"utf8\", \"utf8_general_ci\"))\n\n\tcreateTableSQL, createViewSQL, err := ShowCreateView(tctx, baseConn, \"test\", \"v\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"CREATE TABLE `v`(\\n`a` int\\n)ENGINE=MyISAM;\\n\", createTableSQL)\n\trequire.Equal(t, \"DROP TABLE IF EXISTS `v`;\\nDROP VIEW IF EXISTS `v`;\\nSET @PREV_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT;\\nSET @PREV_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS;\\nSET @PREV_COLLATION_CONNECTION=@@COLLATION_CONNECTION;\\nSET character_set_client = utf8;\\nSET character_set_results = utf8;\\nSET collation_connection = utf8_general_ci;\\nCREATE ALGORITHM=UNDEFINED DEFINER=`root`@`localhost` SQL SECURITY DEFINER VIEW `v` (`a`) AS SELECT `t`.`a` AS `a` FROM `test`.`t`;\\nSET character_set_client = @PREV_CHARACTER_SET_CLIENT;\\nSET character_set_results = @PREV_CHARACTER_SET_RESULTS;\\nSET collation_connection = @PREV_COLLATION_CONNECTION;\\n\", createViewSQL)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n}\n\nfunc TestShowCreateSequence(t *testing.T) {\n\tconf := defaultConfigForTest(t)\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\ttctx := tcontext.Background().WithLogger(appLogger)\n\tbaseConn := newBaseConn(conn, true, nil)\n\n\tconf.ServerInfo.ServerType = version.ServerTypeTiDB\n\tmock.ExpectQuery(\"SHOW CREATE SEQUENCE `test`.`s`\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Sequence\", \"Create Sequence\"}).\n\t\t\tAddRow(\"s\", \"CREATE SEQUENCE `s` start with 1 minvalue 1 maxvalue 9223372036854775806 increment by 1 cache 1000 nocycle ENGINE=InnoDB\"))\n\tmock.ExpectQuery(\"SHOW TABLE `test`.`s` NEXT_ROW_ID\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"DB_NAME\", \"TABLE_NAME\", \"COLUMN_NAME\", \"NEXT_GLOBAL_ROW_ID\", \"ID_TYPE\"}).\n\t\t\tAddRow(\"test\", \"s\", nil, 1001, \"SEQUENCE\"))\n\n\tcreateSequenceSQL, err := ShowCreateSequence(tctx, baseConn, \"test\", \"s\", conf)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"CREATE SEQUENCE `s` start with 1 minvalue 1 maxvalue 9223372036854775806 increment by 1 cache 1000 nocycle ENGINE=InnoDB;\\nSELECT SETVAL(`s`,1001);\\n\", createSequenceSQL)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\tconf.ServerInfo.ServerType = version.ServerTypeMariaDB\n\tmock.ExpectQuery(\"SHOW CREATE SEQUENCE `test`.`s`\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Table\", \"Create Table\"}).\n\t\t\tAddRow(\"s\", \"CREATE SEQUENCE `s` start with 1 minvalue 1 maxvalue 9223372036854775806 increment by 1 cache 1000 nocycle ENGINE=InnoDB\"))\n\tmock.ExpectQuery(\"SELECT NEXT_NOT_CACHED_VALUE FROM `test`.`s`\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"next_not_cached_value\"}).\n\t\t\tAddRow(1001))\n\n\tcreateSequenceSQL, err = ShowCreateSequence(tctx, baseConn, \"test\", \"s\", conf)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"CREATE SEQUENCE `s` start with 1 minvalue 1 maxvalue 9223372036854775806 increment by 1 cache 1000 nocycle ENGINE=InnoDB;\\nSELECT SETVAL(`s`,1001);\\n\", createSequenceSQL)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n}\n\nfunc TestShowCreatePolicy(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\ttctx := tcontext.Background().WithLogger(appLogger)\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\tbaseConn := newBaseConn(conn, true, nil)\n\n\tmock.ExpectQuery(\"SHOW CREATE PLACEMENT POLICY `policy_x`\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Policy\", \"Create Policy\"}).\n\t\t\tAddRow(\"policy_x\", \"CREATE PLACEMENT POLICY `policy_x` LEARNERS=1\"))\n\n\tcreatePolicySQL, err := ShowCreatePlacementPolicy(tctx, baseConn, \"policy_x\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"CREATE PLACEMENT POLICY `policy_x` LEARNERS=1\", createPolicySQL)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n}\n\nfunc TestListPolicyNames(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\ttctx := tcontext.Background().WithLogger(appLogger)\n\tconn, err := db.Conn(context.Background())\n\tbaseConn := newBaseConn(conn, true, nil)\n\trequire.NoError(t, err)\n\n\tmock.ExpectQuery(\"select distinct policy_name from information_schema.placement_policies where policy_name is not null;\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"policy_name\"}).\n\t\t\tAddRow(\"policy_x\"))\n\tpolicies, err := ListAllPlacementPolicyNames(tctx, baseConn)\n\trequire.NoError(t, err)\n\trequire.Equal(t, []string{\"policy_x\"}, policies)\n\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\t// some old tidb version doesn't support placement rules returns error\n\texpectedErr := &mysql.MySQLError{Number: ErrNoSuchTable, Message: \"Table 'information_schema.placement_policies' doesn't exist\"}\n\tmock.ExpectExec(\"select distinct policy_name from information_schema.placement_policies where policy_name is not null;\").\n\t\tWillReturnError(expectedErr)\n\t_, err = ListAllPlacementPolicyNames(tctx, baseConn)\n\tif mysqlErr, ok := err.(*mysql.MySQLError); ok {\n\t\trequire.Equal(t, mysqlErr.Number, ErrNoSuchTable)\n\t}\n}\n\nfunc TestGetSuitableRows(t *testing.T) {\n\ttestCases := []struct {\n\t\tavgRowLength uint64\n\t\texpectedRows uint64\n\t}{\n\t\t{\n\t\t\t0,\n\t\t\t200000,\n\t\t},\n\t\t{\n\t\t\t32,\n\t\t\t1000000,\n\t\t},\n\t\t{\n\t\t\t1024,\n\t\t\t131072,\n\t\t},\n\t\t{\n\t\t\t4096,\n\t\t\t32768,\n\t\t},\n\t}\n\tfor _, testCase := range testCases {\n\t\trows := GetSuitableRows(testCase.avgRowLength)\n\t\trequire.Equal(t, testCase.expectedRows, rows)\n\t}\n}\n\nfunc TestSelectTiDBRowID(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\ttctx := tcontext.Background().WithLogger(appLogger)\n\tbaseConn := newBaseConn(conn, true, nil)\n\n\tdatabase, table := \"test\", \"t\"\n\n\t// _tidb_rowid is unavailable, or PKIsHandle.\n\tmock.ExpectExec(\"SELECT _tidb_rowid from `test`.`t`\").\n\t\tWillReturnError(errors.New(`1054, \"Unknown column '_tidb_rowid' in 'field list'\"`))\n\thasImplicitRowID, err := SelectTiDBRowID(tctx, baseConn, database, table)\n\trequire.NoError(t, err)\n\trequire.False(t, hasImplicitRowID)\n\n\t// _tidb_rowid is available.\n\tmock.ExpectExec(\"SELECT _tidb_rowid from `test`.`t`\").\n\t\tWillReturnResult(sqlmock.NewResult(0, 0))\n\thasImplicitRowID, err = SelectTiDBRowID(tctx, baseConn, database, table)\n\trequire.NoError(t, err)\n\trequire.True(t, hasImplicitRowID)\n\n\t// _tidb_rowid returns error\n\texpectedErr := errors.New(\"mock error\")\n\tmock.ExpectExec(\"SELECT _tidb_rowid from `test`.`t`\").\n\t\tWillReturnError(expectedErr)\n\thasImplicitRowID, err = SelectTiDBRowID(tctx, baseConn, database, table)\n\trequire.ErrorIs(t, errors.Cause(err), expectedErr)\n\trequire.False(t, hasImplicitRowID)\n}\n\nfunc TestBuildTableSampleQueries(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\tbaseConn := newBaseConn(conn, true, nil)\n\ttctx, cancel := tcontext.Background().WithLogger(appLogger).WithCancel()\n\tmetrics := newMetrics(promutil.NewDefaultFactory(), nil)\n\n\td := &Dumper{\n\t\ttctx:                      tctx,\n\t\tconf:                      DefaultConfig(),\n\t\tcancelCtx:                 cancel,\n\t\tmetrics:                   metrics,\n\t\tselectTiDBTableRegionFunc: selectTiDBTableRegion,\n\t}\n\td.conf.ServerInfo = version.ServerInfo{\n\t\tHasTiKV:       true,\n\t\tServerType:    version.ServerTypeTiDB,\n\t\tServerVersion: tableSampleVersion,\n\t}\n\n\ttestCases := []struct {\n\t\thandleColNames       []string\n\t\thandleColTypes       []string\n\t\thandleVals           [][]driver.Value\n\t\texpectedWhereClauses []string\n\t\thasTiDBRowID         bool\n\t}{\n\t\t{\n\t\t\t[]string{},\n\t\t\t[]string{},\n\t\t\t[][]driver.Value{},\n\t\t\tnil,\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[]string{\"a\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[][]driver.Value{{1}},\n\t\t\t[]string{\"`a`<1\", \"`a`>=1\"},\n\t\t\tfalse,\n\t\t},\n\t\t// check whether dumpling can turn to dump whole table\n\t\t{\n\t\t\t[]string{\"a\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[][]driver.Value{},\n\t\t\tnil,\n\t\t\tfalse,\n\t\t},\n\t\t// check whether dumpling can turn to dump whole table\n\t\t{\n\t\t\t[]string{\"_tidb_rowid\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[][]driver.Value{},\n\t\t\tnil,\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\t[]string{\"_tidb_rowid\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[][]driver.Value{{1}},\n\t\t\t[]string{\"`_tidb_rowid`<1\", \"`_tidb_rowid`>=1\"},\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\t[]string{\"a\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[][]driver.Value{\n\t\t\t\t{1},\n\t\t\t\t{2},\n\t\t\t\t{3},\n\t\t\t},\n\t\t\t[]string{\"`a`<1\", \"`a`>=1 and `a`<2\", \"`a`>=2 and `a`<3\", \"`a`>=3\"},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[]string{\"a\", \"b\"},\n\t\t\t[]string{\"BIGINT\", \"BIGINT\"},\n\t\t\t[][]driver.Value{{1, 2}},\n\t\t\t[]string{\"`a`<1 or(`a`=1 and `b`<2)\", \"`a`>1 or(`a`=1 and `b`>=2)\"},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[]string{\"a\", \"b\"},\n\t\t\t[]string{\"BIGINT\", \"BIGINT\"},\n\t\t\t[][]driver.Value{\n\t\t\t\t{1, 2},\n\t\t\t\t{3, 4},\n\t\t\t\t{5, 6},\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"`a`<1 or(`a`=1 and `b`<2)\",\n\t\t\t\t\"(`a`>1 and `a`<3)or(`a`=1 and(`b`>=2))or(`a`=3 and(`b`<4))\",\n\t\t\t\t\"(`a`>3 and `a`<5)or(`a`=3 and(`b`>=4))or(`a`=5 and(`b`<6))\",\n\t\t\t\t\"`a`>5 or(`a`=5 and `b`>=6)\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[]string{\"a\", \"b\", \"c\"},\n\t\t\t[]string{\"BIGINT\", \"BIGINT\", \"BIGINT\"},\n\t\t\t[][]driver.Value{\n\t\t\t\t{1, 2, 3},\n\t\t\t\t{4, 5, 6},\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"`a`<1 or(`a`=1 and `b`<2)or(`a`=1 and `b`=2 and `c`<3)\",\n\t\t\t\t\"(`a`>1 and `a`<4)or(`a`=1 and(`b`>2 or(`b`=2 and `c`>=3)))or(`a`=4 and(`b`<5 or(`b`=5 and `c`<6)))\",\n\t\t\t\t\"`a`>4 or(`a`=4 and `b`>5)or(`a`=4 and `b`=5 and `c`>=6)\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[]string{\"a\", \"b\", \"c\"},\n\t\t\t[]string{\"BIGINT\", \"BIGINT\", \"BIGINT\"},\n\t\t\t[][]driver.Value{\n\t\t\t\t{1, 2, 3},\n\t\t\t\t{1, 4, 5},\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"`a`<1 or(`a`=1 and `b`<2)or(`a`=1 and `b`=2 and `c`<3)\",\n\t\t\t\t\"`a`=1 and((`b`>2 and `b`<4)or(`b`=2 and(`c`>=3))or(`b`=4 and(`c`<5)))\",\n\t\t\t\t\"`a`>1 or(`a`=1 and `b`>4)or(`a`=1 and `b`=4 and `c`>=5)\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[]string{\"a\", \"b\", \"c\"},\n\t\t\t[]string{\"BIGINT\", \"BIGINT\", \"BIGINT\"},\n\t\t\t[][]driver.Value{\n\t\t\t\t{1, 2, 3},\n\t\t\t\t{1, 2, 8},\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"`a`<1 or(`a`=1 and `b`<2)or(`a`=1 and `b`=2 and `c`<3)\",\n\t\t\t\t\"`a`=1 and `b`=2 and(`c`>=3 and `c`<8)\",\n\t\t\t\t\"`a`>1 or(`a`=1 and `b`>2)or(`a`=1 and `b`=2 and `c`>=8)\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t// special case: avoid return same samples\n\t\t{\n\t\t\t[]string{\"a\", \"b\", \"c\"},\n\t\t\t[]string{\"BIGINT\", \"BIGINT\", \"BIGINT\"},\n\t\t\t[][]driver.Value{\n\t\t\t\t{1, 2, 3},\n\t\t\t\t{1, 2, 3},\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"`a`<1 or(`a`=1 and `b`<2)or(`a`=1 and `b`=2 and `c`<3)\",\n\t\t\t\t\"false\",\n\t\t\t\t\"`a`>1 or(`a`=1 and `b`>2)or(`a`=1 and `b`=2 and `c`>=3)\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t// special case: numbers has bigger lexicographically order but lower number\n\t\t{\n\t\t\t[]string{\"a\", \"b\", \"c\"},\n\t\t\t[]string{\"BIGINT\", \"BIGINT\", \"BIGINT\"},\n\t\t\t[][]driver.Value{\n\t\t\t\t{12, 2, 3},\n\t\t\t\t{111, 4, 5},\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"`a`<12 or(`a`=12 and `b`<2)or(`a`=12 and `b`=2 and `c`<3)\",\n\t\t\t\t\"(`a`>12 and `a`<111)or(`a`=12 and(`b`>2 or(`b`=2 and `c`>=3)))or(`a`=111 and(`b`<4 or(`b`=4 and `c`<5)))\", // should return sql correctly\n\t\t\t\t\"`a`>111 or(`a`=111 and `b`>4)or(`a`=111 and `b`=4 and `c`>=5)\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t// test string fields\n\t\t{\n\t\t\t[]string{\"a\", \"b\", \"c\"},\n\t\t\t[]string{\"BIGINT\", \"BIGINT\", \"varchar\"},\n\t\t\t[][]driver.Value{\n\t\t\t\t{1, 2, \"3\"},\n\t\t\t\t{1, 4, \"5\"},\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"`a`<1 or(`a`=1 and `b`<2)or(`a`=1 and `b`=2 and `c`<'3')\",\n\t\t\t\t\"`a`=1 and((`b`>2 and `b`<4)or(`b`=2 and(`c`>='3'))or(`b`=4 and(`c`<'5')))\",\n\t\t\t\t\"`a`>1 or(`a`=1 and `b`>4)or(`a`=1 and `b`=4 and `c`>='5')\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[]string{\"a\", \"b\", \"c\", \"d\"},\n\t\t\t[]string{\"BIGINT\", \"BIGINT\", \"BIGINT\", \"BIGINT\"},\n\t\t\t[][]driver.Value{\n\t\t\t\t{1, 2, 3, 4},\n\t\t\t\t{5, 6, 7, 8},\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"`a`<1 or(`a`=1 and `b`<2)or(`a`=1 and `b`=2 and `c`<3)or(`a`=1 and `b`=2 and `c`=3 and `d`<4)\",\n\t\t\t\t\"(`a`>1 and `a`<5)or(`a`=1 and(`b`>2 or(`b`=2 and `c`>3)or(`b`=2 and `c`=3 and `d`>=4)))or(`a`=5 and(`b`<6 or(`b`=6 and `c`<7)or(`b`=6 and `c`=7 and `d`<8)))\",\n\t\t\t\t\"`a`>5 or(`a`=5 and `b`>6)or(`a`=5 and `b`=6 and `c`>7)or(`a`=5 and `b`=6 and `c`=7 and `d`>=8)\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t}\n\ttransferHandleValStrings := func(handleColTypes []string, handleVals [][]driver.Value) [][]string {\n\t\thandleValStrings := make([][]string, 0, len(handleVals))\n\t\tfor _, handleVal := range handleVals {\n\t\t\thandleValString := make([]string, 0, len(handleVal))\n\t\t\tfor i, val := range handleVal {\n\t\t\t\trec := colTypeRowReceiverMap[strings.ToUpper(handleColTypes[i])]()\n\t\t\t\tvar valStr string\n\t\t\t\tswitch rec.(type) {\n\t\t\t\tcase *SQLTypeString:\n\t\t\t\t\tvalStr = fmt.Sprintf(\"'%s'\", val)\n\t\t\t\tcase *SQLTypeBytes:\n\t\t\t\t\tvalStr = fmt.Sprintf(\"x'%x'\", val)\n\t\t\t\tcase *SQLTypeNumber:\n\t\t\t\t\tvalStr = fmt.Sprintf(\"%d\", val)\n\t\t\t\t}\n\t\t\t\thandleValString = append(handleValString, valStr)\n\t\t\t}\n\t\t\thandleValStrings = append(handleValStrings, handleValString)\n\t\t}\n\t\treturn handleValStrings\n\t}\n\n\tfor caseID, testCase := range testCases {\n\t\tt.Logf(\"case #%d\", caseID)\n\t\thandleColNames := testCase.handleColNames\n\t\thandleColTypes := testCase.handleColTypes\n\t\thandleVals := testCase.handleVals\n\t\thandleValStrings := transferHandleValStrings(handleColTypes, handleVals)\n\n\t\t// Test build whereClauses\n\t\twhereClauses := buildWhereClauses(handleColNames, handleValStrings)\n\t\trequire.Equal(t, testCase.expectedWhereClauses, whereClauses)\n\n\t\t// Test build tasks through table sample\n\t\tif len(handleColNames) > 0 {\n\t\t\ttaskChan := make(chan Task, 128)\n\t\t\tquotaCols := make([]string, 0, len(handleColNames))\n\t\t\tfor _, col := range handleColNames {\n\t\t\t\tquotaCols = append(quotaCols, wrapBackTicks(col))\n\t\t\t}\n\t\t\tselectFields := strings.Join(quotaCols, \",\")\n\t\t\tmeta := &mockTableIR{\n\t\t\t\tdbName:           database,\n\t\t\t\ttblName:          table,\n\t\t\t\tselectedField:    selectFields,\n\t\t\t\thasImplicitRowID: testCase.hasTiDBRowID,\n\t\t\t\tcolTypes:         handleColTypes,\n\t\t\t\tcolNames:         handleColNames,\n\t\t\t\tspecCmt: []string{\n\t\t\t\t\t\"/*!40101 SET NAMES binary*/;\",\n\t\t\t\t},\n\t\t\t}\n\n\t\t\tif !testCase.hasTiDBRowID {\n\t\t\t\trows := sqlmock.NewRows(showIndexHeaders)\n\t\t\t\tfor i, handleColName := range handleColNames {\n\t\t\t\t\trows.AddRow(table, 0, \"PRIMARY\", i, handleColName, \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\")\n\t\t\t\t}\n\t\t\t\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).WillReturnRows(rows)\n\t\t\t}\n\n\t\t\trows := sqlmock.NewRows(handleColNames)\n\t\t\tfor _, handleVal := range handleVals {\n\t\t\t\trows.AddRow(handleVal...)\n\t\t\t}\n\t\t\tmock.ExpectQuery(fmt.Sprintf(\"SELECT .* FROM `%s`.`%s` TABLESAMPLE REGIONS\", database, table)).WillReturnRows(rows)\n\t\t\t// special case, no enough value to split chunks\n\t\t\tif len(handleVals) == 0 {\n\t\t\t\tif !testCase.hasTiDBRowID {\n\t\t\t\t\trows = sqlmock.NewRows(showIndexHeaders)\n\t\t\t\t\tfor i, handleColName := range handleColNames {\n\t\t\t\t\t\trows.AddRow(table, 0, \"PRIMARY\", i, handleColName, \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\")\n\t\t\t\t\t}\n\t\t\t\t\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).WillReturnRows(rows)\n\t\t\t\t\tmock.ExpectQuery(\"SHOW INDEX FROM\").WillReturnRows(sqlmock.NewRows(showIndexHeaders))\n\t\t\t\t} else {\n\t\t\t\t\td.conf.Rows = 200000\n\t\t\t\t\tmock.ExpectQuery(\"EXPLAIN SELECT `_tidb_rowid`\").\n\t\t\t\t\t\tWillReturnRows(sqlmock.NewRows([]string{\"id\", \"count\", \"task\", \"operator info\"}).\n\t\t\t\t\t\t\tAddRow(\"IndexReader_5\", \"0.00\", \"root\", \"index:IndexScan_4\"))\n\t\t\t\t}\n\t\t\t}\n\n\t\t\trequire.NoError(t, d.concurrentDumpTable(tctx, baseConn, meta, taskChan))\n\t\t\trequire.NoError(t, mock.ExpectationsWereMet())\n\t\t\torderByClause := buildOrderByClauseString(handleColNames)\n\n\t\t\tcheckQuery := func(i int, query string) {\n\t\t\t\ttask := <-taskChan\n\t\t\t\ttaskTableData, ok := task.(*TaskTableData)\n\t\t\t\trequire.True(t, ok)\n\t\t\t\trequire.Equal(t, i, taskTableData.ChunkIndex)\n\n\t\t\t\tdata, ok := taskTableData.Data.(*tableData)\n\t\t\t\trequire.True(t, ok)\n\t\t\t\trequire.Equal(t, query, data.query)\n\t\t\t}\n\n\t\t\t// special case, no value found\n\t\t\tif len(handleVals) == 0 {\n\t\t\t\tquery := buildSelectQuery(database, table, selectFields, \"\", \"\", orderByClause)\n\t\t\t\tcheckQuery(0, query)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tfor i, w := range testCase.expectedWhereClauses {\n\t\t\t\tquery := buildSelectQuery(database, table, selectFields, \"\", buildWhereCondition(d.conf, w), orderByClause)\n\t\t\t\tcheckQuery(i, query)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestBuildPartitionClauses(t *testing.T) {\n\tconst (\n\t\tdbName        = \"test\"\n\t\ttbName        = \"t\"\n\t\tfields        = \"*\"\n\t\tpartition     = \"p0\"\n\t\twhere         = \"WHERE a > 10\"\n\t\torderByClause = \"ORDER BY a\"\n\t)\n\ttestCases := []struct {\n\t\tpartition     string\n\t\twhere         string\n\t\torderByClause string\n\t\texpectedQuery string\n\t}{\n\t\t{\n\t\t\t\"\",\n\t\t\t\"\",\n\t\t\t\"\",\n\t\t\t\"SELECT * FROM `test`.`t`\",\n\t\t},\n\t\t{\n\t\t\tpartition,\n\t\t\t\"\",\n\t\t\t\"\",\n\t\t\t\"SELECT * FROM `test`.`t` PARTITION(`p0`)\",\n\t\t},\n\t\t{\n\t\t\tpartition,\n\t\t\twhere,\n\t\t\t\"\",\n\t\t\t\"SELECT * FROM `test`.`t` PARTITION(`p0`) WHERE a > 10\",\n\t\t},\n\t\t{\n\t\t\tpartition,\n\t\t\t\"\",\n\t\t\torderByClause,\n\t\t\t\"SELECT * FROM `test`.`t` PARTITION(`p0`) ORDER BY a\",\n\t\t},\n\t\t{\n\t\t\tpartition,\n\t\t\twhere,\n\t\t\torderByClause,\n\t\t\t\"SELECT * FROM `test`.`t` PARTITION(`p0`) WHERE a > 10 ORDER BY a\",\n\t\t},\n\t\t{\n\t\t\t\"\",\n\t\t\twhere,\n\t\t\torderByClause,\n\t\t\t\"SELECT * FROM `test`.`t` WHERE a > 10 ORDER BY a\",\n\t\t},\n\t}\n\tfor _, testCase := range testCases {\n\t\tquery := buildSelectQuery(dbName, tbName, fields, testCase.partition, testCase.where, testCase.orderByClause)\n\t\trequire.Equal(t, testCase.expectedQuery, query)\n\t}\n}\n\nfunc TestBuildWhereCondition(t *testing.T) {\n\tconf := DefaultConfig()\n\ttestCases := []struct {\n\t\tconfWhere     string\n\t\tchunkWhere    string\n\t\texpectedWhere string\n\t}{\n\t\t{\n\t\t\t\"\",\n\t\t\t\"\",\n\t\t\t\"\",\n\t\t},\n\t\t{\n\t\t\t\"a >= 1000000 and a <= 2000000\",\n\t\t\t\"\",\n\t\t\t\"WHERE a >= 1000000 and a <= 2000000 \",\n\t\t},\n\t\t{\n\t\t\t\"\",\n\t\t\t\"(`a`>1 and `a`<3)or(`a`=1 and(`b`>=2))or(`a`=3 and(`b`<4))\",\n\t\t\t\"WHERE (`a`>1 and `a`<3)or(`a`=1 and(`b`>=2))or(`a`=3 and(`b`<4)) \",\n\t\t},\n\t\t{\n\t\t\t\"a >= 1000000 and a <= 2000000\",\n\t\t\t\"(`a`>1 and `a`<3)or(`a`=1 and(`b`>=2))or(`a`=3 and(`b`<4))\",\n\t\t\t\"WHERE (a >= 1000000 and a <= 2000000) AND ((`a`>1 and `a`<3)or(`a`=1 and(`b`>=2))or(`a`=3 and(`b`<4))) \",\n\t\t},\n\t}\n\tfor _, testCase := range testCases {\n\t\tconf.Where = testCase.confWhere\n\t\twhere := buildWhereCondition(conf, testCase.chunkWhere)\n\t\trequire.Equal(t, testCase.expectedWhere, where)\n\t}\n}\n\nfunc TestBuildRegionQueriesWithoutPartition(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\tbaseConn := newBaseConn(conn, true, nil)\n\ttctx, cancel := tcontext.Background().WithLogger(appLogger).WithCancel()\n\tmetrics := newMetrics(promutil.NewDefaultFactory(), nil)\n\n\td := &Dumper{\n\t\ttctx:                      tctx,\n\t\tconf:                      DefaultConfig(),\n\t\tcancelCtx:                 cancel,\n\t\tmetrics:                   metrics,\n\t\tselectTiDBTableRegionFunc: selectTiDBTableRegion,\n\t}\n\td.conf.ServerInfo = version.ServerInfo{\n\t\tHasTiKV:       true,\n\t\tServerType:    version.ServerTypeTiDB,\n\t\tServerVersion: gcSafePointVersion,\n\t}\n\td.conf.Rows = 200000\n\tdatabase := \"foo\"\n\ttable := \"bar\"\n\n\ttestCases := []struct {\n\t\tregionResults        [][]driver.Value\n\t\thandleColNames       []string\n\t\thandleColTypes       []string\n\t\texpectedWhereClauses []string\n\t\thasTiDBRowID         bool\n\t}{\n\t\t{\n\t\t\t[][]driver.Value{\n\t\t\t\t{\"7480000000000000FF3300000000000000F8\", \"7480000000000000FF3300000000000000F8\"},\n\t\t\t},\n\t\t\t[]string{\"a\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[]string{\n\t\t\t\t\"\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[][]driver.Value{\n\t\t\t\t{\"7480000000000000FF3300000000000000F8\", \"7480000000000000FF3300000000000000F8\"},\n\t\t\t},\n\t\t\t[]string{\"_tidb_rowid\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[]string{\n\t\t\t\t\"\",\n\t\t\t},\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\t[][]driver.Value{\n\t\t\t\t{\"7480000000000000FF3300000000000000F8\", \"7480000000000000FF3300000000000000F8\"},\n\t\t\t\t{\"7480000000000000FF335F728000000000FF0EA6010000000000FA\", \"tableID=51, _tidb_rowid=960001\"},\n\t\t\t\t{\"7480000000000000FF335F728000000000FF1D4C010000000000FA\", \"tableID=51, _tidb_rowid=1920001\"},\n\t\t\t\t{\"7480000000000000FF335F728000000000FF2BF2010000000000FA\", \"tableID=51, _tidb_rowid=2880001\"},\n\t\t\t},\n\t\t\t[]string{\"a\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[]string{\n\t\t\t\t\"`a`<960001\",\n\t\t\t\t\"`a`>=960001 and `a`<1920001\",\n\t\t\t\t\"`a`>=1920001 and `a`<2880001\",\n\t\t\t\t\"`a`>=2880001\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[][]driver.Value{\n\t\t\t\t{\"7480000000000000FF3300000000000000F8\", \"7480000000000000FF3300000000000000F8\"},\n\t\t\t\t{\"7480000000000000FF335F728000000000FF0EA6010000000000FA\", \"tableID=51, _tidb_rowid=960001\"},\n\t\t\t\t// one invalid key\n\t\t\t\t{\"7520000000000000FF335F728000000000FF0EA6010000000000FA\", \"7520000000000000FF335F728000000000FF0EA6010000000000FA\"},\n\t\t\t\t{\"7480000000000000FF335F728000000000FF1D4C010000000000FA\", \"tableID=51, _tidb_rowid=1920001\"},\n\t\t\t\t{\"7480000000000000FF335F728000000000FF2BF2010000000000FA\", \"tableID=51, _tidb_rowid=2880001\"},\n\t\t\t},\n\t\t\t[]string{\"_tidb_rowid\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[]string{\n\t\t\t\t\"`_tidb_rowid`<960001\",\n\t\t\t\t\"`_tidb_rowid`>=960001 and `_tidb_rowid`<1920001\",\n\t\t\t\t\"`_tidb_rowid`>=1920001 and `_tidb_rowid`<2880001\",\n\t\t\t\t\"`_tidb_rowid`>=2880001\",\n\t\t\t},\n\t\t\ttrue,\n\t\t},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tt.Logf(\"case #%d\", i)\n\t\thandleColNames := testCase.handleColNames\n\t\thandleColTypes := testCase.handleColTypes\n\t\tregionResults := testCase.regionResults\n\n\t\t// Test build tasks through table region\n\t\ttaskChan := make(chan Task, 128)\n\t\tmeta := &mockTableIR{\n\t\t\tdbName:           database,\n\t\t\ttblName:          table,\n\t\t\tselectedField:    \"*\",\n\t\t\tselectedLen:      len(handleColNames),\n\t\t\thasImplicitRowID: testCase.hasTiDBRowID,\n\t\t\tcolTypes:         handleColTypes,\n\t\t\tcolNames:         handleColNames,\n\t\t\tspecCmt: []string{\n\t\t\t\t\"/*!40101 SET NAMES binary*/;\",\n\t\t\t},\n\t\t}\n\n\t\tmock.ExpectQuery(\"SELECT PARTITION_NAME from INFORMATION_SCHEMA.PARTITIONS\").\n\t\t\tWithArgs(database, table).WillReturnRows(sqlmock.NewRows([]string{\"PARTITION_NAME\"}).AddRow(nil))\n\n\t\tif !testCase.hasTiDBRowID {\n\t\t\trows := sqlmock.NewRows(showIndexHeaders)\n\t\t\tfor i, handleColName := range handleColNames {\n\t\t\t\trows.AddRow(table, 0, \"PRIMARY\", i, handleColName, \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\")\n\t\t\t}\n\t\t\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).WillReturnRows(rows)\n\t\t}\n\n\t\trows := sqlmock.NewRows([]string{\"START_KEY\", \"tidb_decode_key(START_KEY)\"})\n\t\tfor _, regionResult := range regionResults {\n\t\t\trows.AddRow(regionResult...)\n\t\t}\n\t\tmock.ExpectQuery(\"SELECT START_KEY,tidb_decode_key\\\\(START_KEY\\\\) from INFORMATION_SCHEMA.TIKV_REGION_STATUS\").\n\t\t\tWithArgs(database, table).WillReturnRows(rows)\n\n\t\torderByClause := buildOrderByClauseString(handleColNames)\n\t\t// special case, no enough value to split chunks\n\t\tif !testCase.hasTiDBRowID && len(regionResults) <= 1 {\n\t\t\trows = sqlmock.NewRows(showIndexHeaders)\n\t\t\tfor i, handleColName := range handleColNames {\n\t\t\t\trows.AddRow(table, 0, \"PRIMARY\", i, handleColName, \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\")\n\t\t\t}\n\t\t\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).WillReturnRows(rows)\n\t\t\tmock.ExpectQuery(\"SHOW INDEX FROM\").WillReturnRows(sqlmock.NewRows(showIndexHeaders))\n\t\t}\n\t\trequire.NoError(t, d.concurrentDumpTable(tctx, baseConn, meta, taskChan))\n\t\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\t\tfor i, w := range testCase.expectedWhereClauses {\n\t\t\tquery := buildSelectQuery(database, table, \"*\", \"\", buildWhereCondition(d.conf, w), orderByClause)\n\t\t\ttask := <-taskChan\n\t\t\ttaskTableData, ok := task.(*TaskTableData)\n\t\t\trequire.True(t, ok)\n\t\t\trequire.Equal(t, i, taskTableData.ChunkIndex)\n\t\t\tdata, ok := taskTableData.Data.(*tableData)\n\t\t\trequire.True(t, ok)\n\t\t\trequire.Equal(t, query, data.query)\n\t\t}\n\t}\n}\n\nfunc TestBuildRegionQueriesWithPartitions(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\tbaseConn := newBaseConn(conn, true, nil)\n\ttctx, cancel := tcontext.Background().WithLogger(appLogger).WithCancel()\n\tmetrics := newMetrics(promutil.NewDefaultFactory(), nil)\n\n\td := &Dumper{\n\t\ttctx:                      tctx,\n\t\tconf:                      DefaultConfig(),\n\t\tcancelCtx:                 cancel,\n\t\tmetrics:                   metrics,\n\t\tselectTiDBTableRegionFunc: selectTiDBTableRegion,\n\t}\n\td.conf.ServerInfo = version.ServerInfo{\n\t\tHasTiKV:       true,\n\t\tServerType:    version.ServerTypeTiDB,\n\t\tServerVersion: gcSafePointVersion,\n\t}\n\tpartitions := []string{\"p0\", \"p1\", \"p2\"}\n\n\ttestCases := []struct {\n\t\tregionResults        [][][]driver.Value\n\t\thandleColNames       []string\n\t\thandleColTypes       []string\n\t\texpectedWhereClauses [][]string\n\t\thasTiDBRowID         bool\n\t\tdumpWholeTable       bool\n\t}{\n\t\t{\n\t\t\t[][][]driver.Value{\n\t\t\t\t{\n\t\t\t\t\t{6009, \"t_121_i_1_0380000000000ea6010380000000000ea601\", \"t_121_\", 6010, 1, 6010, 0, 0, 0, 74, 1052002},\n\t\t\t\t\t{6011, \"t_121_\", \"t_121_i_1_0380000000000ea6010380000000000ea601\", 6012, 1, 6012, 0, 0, 0, 68, 972177},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t{6015, \"t_122_i_1_0380000000002d2a810380000000002d2a81\", \"t_122_\", 6016, 1, 6016, 0, 0, 0, 77, 1092962},\n\t\t\t\t\t{6017, \"t_122_\", \"t_122_i_1_0380000000002d2a810380000000002d2a81\", 6018, 1, 6018, 0, 0, 0, 66, 939975},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t{6021, \"t_123_i_1_0380000000004baf010380000000004baf01\", \"t_123_\", 6022, 1, 6022, 0, 0, 0, 85, 1206726},\n\t\t\t\t\t{6023, \"t_123_\", \"t_123_i_1_0380000000004baf010380000000004baf01\", 6024, 1, 6024, 0, 0, 0, 65, 927576},\n\t\t\t\t},\n\t\t\t},\n\t\t\t[]string{\"_tidb_rowid\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[][]string{\n\t\t\t\t{\"\"}, {\"\"}, {\"\"},\n\t\t\t},\n\t\t\ttrue,\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\t[][][]driver.Value{\n\t\t\t\t{\n\t\t\t\t\t{6009, \"t_121_i_1_0380000000000ea6010380000000000ea601\", \"t_121_r_10001\", 6010, 1, 6010, 0, 0, 0, 74, 1052002},\n\t\t\t\t\t{6013, \"t_121_r_10001\", \"t_121_r_970001\", 6014, 1, 6014, 0, 0, 0, 75, 975908},\n\t\t\t\t\t{6003, \"t_121_r_970001\", \"t_122_\", 6004, 1, 6004, 0, 0, 0, 79, 1022285},\n\t\t\t\t\t{6011, \"t_121_\", \"t_121_i_1_0380000000000ea6010380000000000ea601\", 6012, 1, 6012, 0, 0, 0, 68, 972177},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t{6015, \"t_122_i_1_0380000000002d2a810380000000002d2a81\", \"t_122_r_2070760\", 6016, 1, 6016, 0, 0, 0, 77, 1092962},\n\t\t\t\t\t{6019, \"t_122_r_2070760\", \"t_122_r_3047115\", 6020, 1, 6020, 0, 0, 0, 75, 959650},\n\t\t\t\t\t{6005, \"t_122_r_3047115\", \"t_123_\", 6006, 1, 6006, 0, 0, 0, 77, 992339},\n\t\t\t\t\t{6017, \"t_122_\", \"t_122_i_1_0380000000002d2a810380000000002d2a81\", 6018, 1, 6018, 0, 0, 0, 66, 939975},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t{6021, \"t_123_i_1_0380000000004baf010380000000004baf01\", \"t_123_r_4186953\", 6022, 1, 6022, 0, 0, 0, 85, 1206726},\n\t\t\t\t\t{6025, \"t_123_r_4186953\", \"t_123_r_5165682\", 6026, 1, 6026, 0, 0, 0, 74, 951379},\n\t\t\t\t\t{6007, \"t_123_r_5165682\", \"t_124_\", 6008, 1, 6008, 0, 0, 0, 71, 918488},\n\t\t\t\t\t{6023, \"t_123_\", \"t_123_i_1_0380000000004baf010380000000004baf01\", 6024, 1, 6024, 0, 0, 0, 65, 927576},\n\t\t\t\t},\n\t\t\t},\n\t\t\t[]string{\"_tidb_rowid\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[][]string{\n\t\t\t\t{\n\t\t\t\t\t\"`_tidb_rowid`<10001\",\n\t\t\t\t\t\"`_tidb_rowid`>=10001 and `_tidb_rowid`<970001\",\n\t\t\t\t\t\"`_tidb_rowid`>=970001\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"`_tidb_rowid`<2070760\",\n\t\t\t\t\t\"`_tidb_rowid`>=2070760 and `_tidb_rowid`<3047115\",\n\t\t\t\t\t\"`_tidb_rowid`>=3047115\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"`_tidb_rowid`<4186953\",\n\t\t\t\t\t\"`_tidb_rowid`>=4186953 and `_tidb_rowid`<5165682\",\n\t\t\t\t\t\"`_tidb_rowid`>=5165682\",\n\t\t\t\t},\n\t\t\t},\n\t\t\ttrue,\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t[][][]driver.Value{\n\t\t\t\t{\n\t\t\t\t\t{6041, \"t_134_\", \"t_134_r_960001\", 6042, 1, 6042, 0, 0, 0, 69, 964987},\n\t\t\t\t\t{6035, \"t_134_r_960001\", \"t_135_\", 6036, 1, 6036, 0, 0, 0, 75, 1052130},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t{6043, \"t_135_\", \"t_135_r_2960001\", 6044, 1, 6044, 0, 0, 0, 69, 969576},\n\t\t\t\t\t{6037, \"t_135_r_2960001\", \"t_136_\", 6038, 1, 6038, 0, 0, 0, 72, 1014464},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t{6045, \"t_136_\", \"t_136_r_4960001\", 6046, 1, 6046, 0, 0, 0, 68, 957557},\n\t\t\t\t\t{6039, \"t_136_r_4960001\", \"t_137_\", 6040, 1, 6040, 0, 0, 0, 75, 1051579},\n\t\t\t\t},\n\t\t\t},\n\t\t\t[]string{\"a\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[][]string{\n\n\t\t\t\t{\n\t\t\t\t\t\"`a`<960001\",\n\t\t\t\t\t\"`a`>=960001\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"`a`<2960001\",\n\t\t\t\t\t\"`a`>=2960001\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"`a`<4960001\",\n\t\t\t\t\t\"`a`>=4960001\",\n\t\t\t\t},\n\t\t\t},\n\t\t\tfalse,\n\t\t\tfalse,\n\t\t},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tt.Logf(\"case #%d\", i)\n\t\thandleColNames := testCase.handleColNames\n\t\thandleColTypes := testCase.handleColTypes\n\t\tregionResults := testCase.regionResults\n\n\t\t// Test build tasks through table region\n\t\ttaskChan := make(chan Task, 128)\n\t\tmeta := &mockTableIR{\n\t\t\tdbName:           database,\n\t\t\ttblName:          table,\n\t\t\tselectedField:    \"*\",\n\t\t\tselectedLen:      len(handleColNames),\n\t\t\thasImplicitRowID: testCase.hasTiDBRowID,\n\t\t\tcolTypes:         handleColTypes,\n\t\t\tcolNames:         handleColNames,\n\t\t\tspecCmt: []string{\n\t\t\t\t\"/*!40101 SET NAMES binary*/;\",\n\t\t\t},\n\t\t}\n\n\t\trows := sqlmock.NewRows([]string{\"PARTITION_NAME\"})\n\t\tfor _, partition := range partitions {\n\t\t\trows.AddRow(partition)\n\t\t}\n\t\tmock.ExpectQuery(\"SELECT PARTITION_NAME from INFORMATION_SCHEMA.PARTITIONS\").\n\t\t\tWithArgs(database, table).WillReturnRows(rows)\n\n\t\tif !testCase.hasTiDBRowID {\n\t\t\trows = sqlmock.NewRows(showIndexHeaders)\n\t\t\tfor i, handleColName := range handleColNames {\n\t\t\t\trows.AddRow(table, 0, \"PRIMARY\", i, handleColName, \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\")\n\t\t\t}\n\t\t\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).WillReturnRows(rows)\n\t\t}\n\n\t\tfor i, partition := range partitions {\n\t\t\trows = sqlmock.NewRows([]string{\"REGION_ID\", \"START_KEY\", \"END_KEY\", \"LEADER_ID\", \"LEADER_STORE_ID\", \"PEERS\", \"SCATTERING\", \"WRITTEN_BYTES\", \"READ_BYTES\", \"APPROXIMATE_SIZE(MB)\", \"APPROXIMATE_KEYS\"})\n\t\t\tfor _, regionResult := range regionResults[i] {\n\t\t\t\trows.AddRow(regionResult...)\n\t\t\t}\n\t\t\tmock.ExpectQuery(fmt.Sprintf(\"SHOW TABLE `%s`.`%s` PARTITION\\\\(`%s`\\\\) REGIONS\", escapeString(database), escapeString(table), escapeString(partition))).\n\t\t\t\tWillReturnRows(rows)\n\t\t}\n\n\t\torderByClause := buildOrderByClauseString(handleColNames)\n\t\trequire.NoError(t, d.concurrentDumpTable(tctx, baseConn, meta, taskChan))\n\t\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\t\tchunkIdx := 0\n\t\tfor i, partition := range partitions {\n\t\t\tfor _, w := range testCase.expectedWhereClauses[i] {\n\t\t\t\tquery := buildSelectQuery(database, table, \"*\", partition, buildWhereCondition(d.conf, w), orderByClause)\n\t\t\t\ttask := <-taskChan\n\t\t\t\ttaskTableData, ok := task.(*TaskTableData)\n\t\t\t\trequire.True(t, ok)\n\t\t\t\trequire.Equal(t, chunkIdx, taskTableData.ChunkIndex)\n\t\t\t\tdata, ok := taskTableData.Data.(*tableData)\n\t\t\t\trequire.True(t, ok)\n\t\t\t\trequire.Equal(t, query, data.query)\n\t\t\t\tchunkIdx++\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc buildMockNewRows(mock sqlmock.Sqlmock, columns []string, driverValues [][]driver.Value) *sqlmock.Rows {\n\trows := mock.NewRows(columns)\n\tfor _, driverValue := range driverValues {\n\t\trows.AddRow(driverValue...)\n\t}\n\treturn rows\n}\n\nfunc readRegionCsvDriverValues(t *testing.T) [][]driver.Value {\n\tcsvFilename := \"region_results.csv\"\n\tfile, err := os.Open(csvFilename)\n\trequire.NoError(t, err)\n\tcsvReader := csv.NewReader(file)\n\tvalues := make([][]driver.Value, 0, 990)\n\tfor {\n\t\tresults, err := csvReader.Read()\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t}\n\t\trequire.NoError(t, err)\n\t\tif len(results) != 3 {\n\t\t\tcontinue\n\t\t}\n\t\tregionID, err := strconv.Atoi(results[0])\n\t\trequire.NoError(t, err)\n\t\tstartKey, endKey := results[1], results[2]\n\t\tvalues = append(values, []driver.Value{regionID, startKey, endKey})\n\t}\n\treturn values\n}\n\nfunc TestBuildVersion3RegionQueries(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\tbaseConn := newBaseConn(conn, true, nil)\n\ttctx, cancel := tcontext.Background().WithLogger(appLogger).WithCancel()\n\toldOpenFunc := openDBFunc\n\tdefer func() {\n\t\topenDBFunc = oldOpenFunc\n\t}()\n\topenDBFunc = func(*mysql.Config) (*sql.DB, error) {\n\t\treturn db, nil\n\t}\n\n\tconf := DefaultConfig()\n\tconf.ServerInfo = version.ServerInfo{\n\t\tHasTiKV:       true,\n\t\tServerType:    version.ServerTypeTiDB,\n\t\tServerVersion: decodeRegionVersion,\n\t}\n\tdatabase := \"test\"\n\tconf.Tables = DatabaseTables{\n\t\tdatabase: []*TableInfo{\n\t\t\t{\"t1\", 0, TableTypeBase},\n\t\t\t{\"t2\", 0, TableTypeBase},\n\t\t\t{\"t3\", 0, TableTypeBase},\n\t\t\t{\"t4\", 0, TableTypeBase},\n\t\t},\n\t}\n\tmetrics := newMetrics(promutil.NewDefaultFactory(), nil)\n\n\td := &Dumper{\n\t\ttctx:                      tctx,\n\t\tconf:                      conf,\n\t\tcancelCtx:                 cancel,\n\t\tmetrics:                   metrics,\n\t\tselectTiDBTableRegionFunc: selectTiDBTableRegion,\n\t}\n\tshowStatsHistograms := buildMockNewRows(mock, []string{\"Db_name\", \"Table_name\", \"Partition_name\", \"Column_name\", \"Is_index\", \"Update_time\", \"Distinct_count\", \"Null_count\", \"Avg_col_size\", \"Correlation\"},\n\t\t[][]driver.Value{\n\t\t\t{\"test\", \"t2\", \"p0\", \"a\", 0, \"2021-06-27 17:43:51\", 1999999, 0, 8, 0},\n\t\t\t{\"test\", \"t2\", \"p1\", \"a\", 0, \"2021-06-22 20:30:16\", 1260000, 0, 8, 0},\n\t\t\t{\"test\", \"t2\", \"p2\", \"a\", 0, \"2021-06-22 20:32:16\", 1230000, 0, 8, 0},\n\t\t\t{\"test\", \"t2\", \"p3\", \"a\", 0, \"2021-06-22 20:36:19\", 2000000, 0, 8, 0},\n\t\t\t{\"test\", \"t1\", \"\", \"a\", 0, \"2021-04-22 15:23:58\", 7100000, 0, 8, 0},\n\t\t\t{\"test\", \"t3\", \"\", \"PRIMARY\", 1, \"2021-06-27 22:08:43\", 4980000, 0, 0, 0},\n\t\t\t{\"test\", \"t4\", \"p0\", \"PRIMARY\", 1, \"2021-06-28 10:54:06\", 2000000, 0, 0, 0},\n\t\t\t{\"test\", \"t4\", \"p1\", \"PRIMARY\", 1, \"2021-06-28 10:55:04\", 1300000, 0, 0, 0},\n\t\t\t{\"test\", \"t4\", \"p2\", \"PRIMARY\", 1, \"2021-06-28 10:57:05\", 1830000, 0, 0, 0},\n\t\t\t{\"test\", \"t4\", \"p3\", \"PRIMARY\", 1, \"2021-06-28 10:59:04\", 2000000, 0, 0, 0},\n\t\t\t{\"mysql\", \"global_priv\", \"\", \"PRIMARY\", 1, \"2021-06-04 20:39:44\", 0, 0, 0, 0},\n\t\t})\n\tselectMySQLStatsHistograms := buildMockNewRows(mock, []string{\"TABLE_ID\", \"VERSION\", \"DISTINCT_COUNT\"},\n\t\t[][]driver.Value{\n\t\t\t{15, \"1970-01-01 08:00:00\", 0},\n\t\t\t{15, \"1970-01-01 08:00:00\", 0},\n\t\t\t{15, \"1970-01-01 08:00:00\", 0},\n\t\t\t{41, \"2021-04-22 15:23:58\", 7100000},\n\t\t\t{41, \"2021-04-22 15:23:59\", 7100000},\n\t\t\t{41, \"2021-04-22 15:23:59\", 7100000},\n\t\t\t{41, \"2021-04-22 15:23:59\", 7100000},\n\t\t\t{27, \"1970-01-01 08:00:00\", 0},\n\t\t\t{27, \"1970-01-01 08:00:00\", 0},\n\t\t\t{25, \"1970-01-01 08:00:00\", 0},\n\t\t\t{25, \"1970-01-01 08:00:00\", 0},\n\t\t\t{2098, \"2021-06-04 20:39:41\", 0},\n\t\t\t{2101, \"2021-06-04 20:39:44\", 0},\n\t\t\t{2101, \"2021-06-04 20:39:44\", 0},\n\t\t\t{2101, \"2021-06-04 20:39:44\", 0},\n\t\t\t{2101, \"2021-06-04 20:39:44\", 0},\n\t\t\t{2128, \"2021-06-22 20:29:19\", 1991680},\n\t\t\t{2128, \"2021-06-22 20:29:19\", 1991680},\n\t\t\t{2128, \"2021-06-22 20:29:19\", 1991680},\n\t\t\t{2129, \"2021-06-22 20:30:16\", 1260000},\n\t\t\t{2129, \"2021-06-22 20:30:16\", 1237120},\n\t\t\t{2129, \"2021-06-22 20:30:16\", 1237120},\n\t\t\t{2129, \"2021-06-22 20:30:16\", 1237120},\n\t\t\t{2130, \"2021-06-22 20:32:16\", 1230000},\n\t\t\t{2130, \"2021-06-22 20:32:16\", 1216128},\n\t\t\t{2130, \"2021-06-22 20:32:17\", 1216128},\n\t\t\t{2130, \"2021-06-22 20:32:17\", 1216128},\n\t\t\t{2131, \"2021-06-22 20:36:19\", 2000000},\n\t\t\t{2131, \"2021-06-22 20:36:19\", 1959424},\n\t\t\t{2131, \"2021-06-22 20:36:19\", 1959424},\n\t\t\t{2131, \"2021-06-22 20:36:19\", 1959424},\n\t\t\t{2128, \"2021-06-27 17:43:51\", 1999999},\n\t\t\t{2136, \"2021-06-27 22:08:38\", 4860000},\n\t\t\t{2136, \"2021-06-27 22:08:38\", 4860000},\n\t\t\t{2136, \"2021-06-27 22:08:38\", 4860000},\n\t\t\t{2136, \"2021-06-27 22:08:38\", 4860000},\n\t\t\t{2136, \"2021-06-27 22:08:43\", 4980000},\n\t\t\t{2139, \"2021-06-28 10:54:05\", 1991680},\n\t\t\t{2139, \"2021-06-28 10:54:05\", 1991680},\n\t\t\t{2139, \"2021-06-28 10:54:05\", 1991680},\n\t\t\t{2139, \"2021-06-28 10:54:05\", 1991680},\n\t\t\t{2139, \"2021-06-28 10:54:06\", 2000000},\n\t\t\t{2140, \"2021-06-28 10:55:02\", 1246336},\n\t\t\t{2140, \"2021-06-28 10:55:02\", 1246336},\n\t\t\t{2140, \"2021-06-28 10:55:02\", 1246336},\n\t\t\t{2140, \"2021-06-28 10:55:03\", 1246336},\n\t\t\t{2140, \"2021-06-28 10:55:04\", 1300000},\n\t\t\t{2141, \"2021-06-28 10:57:03\", 1780000},\n\t\t\t{2141, \"2021-06-28 10:57:03\", 1780000},\n\t\t\t{2141, \"2021-06-28 10:57:03\", 1780000},\n\t\t\t{2141, \"2021-06-28 10:57:03\", 1780000},\n\t\t\t{2141, \"2021-06-28 10:57:05\", 1830000},\n\t\t\t{2142, \"2021-06-28 10:59:03\", 1959424},\n\t\t\t{2142, \"2021-06-28 10:59:03\", 1959424},\n\t\t\t{2142, \"2021-06-28 10:59:03\", 1959424},\n\t\t\t{2142, \"2021-06-28 10:59:03\", 1959424},\n\t\t\t{2142, \"2021-06-28 10:59:04\", 2000000},\n\t\t})\n\tselectRegionStatusHistograms := buildMockNewRows(mock, []string{\"REGION_ID\", \"START_KEY\", \"END_KEY\"}, readRegionCsvDriverValues(t))\n\tselectInformationSchemaTables := buildMockNewRows(mock, []string{\"TABLE_SCHEMA\", \"TABLE_NAME\", \"TIDB_TABLE_ID\"},\n\t\t[][]driver.Value{\n\t\t\t{\"mysql\", \"expr_pushdown_blacklist\", 39},\n\t\t\t{\"mysql\", \"user\", 5},\n\t\t\t{\"mysql\", \"db\", 7},\n\t\t\t{\"mysql\", \"tables_priv\", 9},\n\t\t\t{\"mysql\", \"stats_top_n\", 37},\n\t\t\t{\"mysql\", \"columns_priv\", 11},\n\t\t\t{\"mysql\", \"bind_info\", 35},\n\t\t\t{\"mysql\", \"default_roles\", 33},\n\t\t\t{\"mysql\", \"role_edges\", 31},\n\t\t\t{\"mysql\", \"stats_feedback\", 29},\n\t\t\t{\"mysql\", \"gc_delete_range_done\", 27},\n\t\t\t{\"mysql\", \"gc_delete_range\", 25},\n\t\t\t{\"mysql\", \"help_topic\", 17},\n\t\t\t{\"mysql\", \"global_priv\", 2101},\n\t\t\t{\"mysql\", \"stats_histograms\", 21},\n\t\t\t{\"mysql\", \"opt_rule_blacklist\", 2098},\n\t\t\t{\"mysql\", \"stats_meta\", 19},\n\t\t\t{\"mysql\", \"stats_buckets\", 23},\n\t\t\t{\"mysql\", \"tidb\", 15},\n\t\t\t{\"mysql\", \"GLOBAL_VARIABLES\", 13},\n\t\t\t{\"test\", \"t2\", 2127},\n\t\t\t{\"test\", \"t1\", 41},\n\t\t\t{\"test\", \"t3\", 2136},\n\t\t\t{\"test\", \"t4\", 2138},\n\t\t})\n\tmock.ExpectQuery(\"SHOW STATS_HISTOGRAMS\").\n\t\tWillReturnRows(showStatsHistograms)\n\tmock.ExpectQuery(\"SELECT TABLE_ID,FROM_UNIXTIME\").\n\t\tWillReturnRows(selectMySQLStatsHistograms)\n\tmock.ExpectQuery(\"SELECT TABLE_SCHEMA,TABLE_NAME,TIDB_TABLE_ID FROM INFORMATION_SCHEMA.TABLES ORDER BY TABLE_SCHEMA\").\n\t\tWillReturnRows(selectInformationSchemaTables)\n\tmock.ExpectQuery(\"SELECT REGION_ID,START_KEY,END_KEY FROM INFORMATION_SCHEMA.TIKV_REGION_STATUS ORDER BY START_KEY;\").\n\t\tWillReturnRows(selectRegionStatusHistograms)\n\n\trequire.NoError(t, d.renewSelectTableRegionFuncForLowerTiDB(tctx))\n\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\ttestCases := []struct {\n\t\ttableName            string\n\t\thandleColNames       []string\n\t\thandleColTypes       []string\n\t\texpectedWhereClauses []string\n\t\thasTiDBRowID         bool\n\t}{\n\t\t{\n\t\t\t\"t1\",\n\t\t\t[]string{\"a\"},\n\t\t\t[]string{\"INT\"},\n\t\t\t[]string{\n\t\t\t\t\"`a`<960001\",\n\t\t\t\t\"`a`>=960001 and `a`<1920001\",\n\t\t\t\t\"`a`>=1920001 and `a`<2880001\",\n\t\t\t\t\"`a`>=2880001 and `a`<3840001\",\n\t\t\t\t\"`a`>=3840001 and `a`<4800001\",\n\t\t\t\t\"`a`>=4800001 and `a`<5760001\",\n\t\t\t\t\"`a`>=5760001 and `a`<6720001\",\n\t\t\t\t\"`a`>=6720001\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t\"t2\",\n\t\t\t[]string{\"a\"},\n\t\t\t[]string{\"INT\"},\n\t\t\t[]string{\n\t\t\t\t\"`a`<960001\",\n\t\t\t\t\"`a`>=960001 and `a`<2960001\",\n\t\t\t\t\"`a`>=2960001 and `a`<4960001\",\n\t\t\t\t\"`a`>=4960001 and `a`<6960001\",\n\t\t\t\t\"`a`>=6960001\",\n\t\t\t},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t\"t3\",\n\t\t\t[]string{\"_tidb_rowid\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[]string{\n\t\t\t\t\"`_tidb_rowid`<81584\",\n\t\t\t\t\"`_tidb_rowid`>=81584 and `_tidb_rowid`<1041584\",\n\t\t\t\t\"`_tidb_rowid`>=1041584 and `_tidb_rowid`<2001584\",\n\t\t\t\t\"`_tidb_rowid`>=2001584 and `_tidb_rowid`<2961584\",\n\t\t\t\t\"`_tidb_rowid`>=2961584 and `_tidb_rowid`<3921584\",\n\t\t\t\t\"`_tidb_rowid`>=3921584 and `_tidb_rowid`<4881584\",\n\t\t\t\t\"`_tidb_rowid`>=4881584 and `_tidb_rowid`<5841584\",\n\t\t\t\t\"`_tidb_rowid`>=5841584 and `_tidb_rowid`<6801584\",\n\t\t\t\t\"`_tidb_rowid`>=6801584\",\n\t\t\t},\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\t\"t4\",\n\t\t\t[]string{\"_tidb_rowid\"},\n\t\t\t[]string{\"BIGINT\"},\n\t\t\t[]string{\n\t\t\t\t\"`_tidb_rowid`<180001\",\n\t\t\t\t\"`_tidb_rowid`>=180001 and `_tidb_rowid`<1140001\",\n\t\t\t\t\"`_tidb_rowid`>=1140001 and `_tidb_rowid`<2200001\",\n\t\t\t\t\"`_tidb_rowid`>=2200001 and `_tidb_rowid`<3160001\",\n\t\t\t\t\"`_tidb_rowid`>=3160001 and `_tidb_rowid`<4160001\",\n\t\t\t\t\"`_tidb_rowid`>=4160001 and `_tidb_rowid`<5120001\",\n\t\t\t\t\"`_tidb_rowid`>=5120001 and `_tidb_rowid`<6170001\",\n\t\t\t\t\"`_tidb_rowid`>=6170001 and `_tidb_rowid`<7130001\",\n\t\t\t\t\"`_tidb_rowid`>=7130001\",\n\t\t\t},\n\t\t\ttrue,\n\t\t},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tt.Logf(\"case #%d\", i)\n\t\ttable := testCase.tableName\n\t\thandleColNames := testCase.handleColNames\n\t\thandleColTypes := testCase.handleColTypes\n\n\t\t// Test build tasks through table region\n\t\ttaskChan := make(chan Task, 128)\n\t\tmeta := &mockTableIR{\n\t\t\tdbName:           database,\n\t\t\ttblName:          table,\n\t\t\tselectedField:    \"*\",\n\t\t\thasImplicitRowID: testCase.hasTiDBRowID,\n\t\t\tcolNames:         handleColNames,\n\t\t\tcolTypes:         handleColTypes,\n\t\t\tspecCmt: []string{\n\t\t\t\t\"/*!40101 SET NAMES binary*/;\",\n\t\t\t},\n\t\t}\n\n\t\tif !testCase.hasTiDBRowID {\n\t\t\trows := sqlmock.NewRows(showIndexHeaders)\n\t\t\tfor i, handleColName := range handleColNames {\n\t\t\t\trows.AddRow(table, 0, \"PRIMARY\", i, handleColName, \"A\", 0, nil, nil, \"\", \"BTREE\", \"\", \"\")\n\t\t\t}\n\t\t\tmock.ExpectQuery(fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)).WillReturnRows(rows)\n\t\t}\n\n\t\torderByClause := buildOrderByClauseString(handleColNames)\n\t\trequire.NoError(t, d.concurrentDumpTable(tctx, baseConn, meta, taskChan))\n\t\trequire.NoError(t, mock.ExpectationsWereMet())\n\n\t\tchunkIdx := 0\n\t\tfor _, w := range testCase.expectedWhereClauses {\n\t\t\tquery := buildSelectQuery(database, table, \"*\", \"\", buildWhereCondition(d.conf, w), orderByClause)\n\t\t\ttask := <-taskChan\n\t\t\ttaskTableData, ok := task.(*TaskTableData)\n\t\t\trequire.True(t, ok)\n\t\t\trequire.Equal(t, chunkIdx, taskTableData.ChunkIndex)\n\n\t\t\tdata, ok := taskTableData.Data.(*tableData)\n\t\t\trequire.True(t, ok)\n\t\t\trequire.Equal(t, query, data.query)\n\n\t\t\tchunkIdx++\n\t\t}\n\t}\n}\n\nfunc TestCheckTiDBWithTiKV(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\t_ = db.Close()\n\t}()\n\n\ttidbConf := dbconfig.NewConfig()\n\tstores := []string{\"unistore\", \"mocktikv\", \"tikv\"}\n\tfor _, store := range stores {\n\t\ttidbConf.Store = store\n\t\ttidbConfBytes, err := json.Marshal(tidbConf)\n\t\trequire.NoError(t, err)\n\t\tmock.ExpectQuery(\"SELECT @@tidb_config\").WillReturnRows(\n\t\t\tsqlmock.NewRows([]string{\"@@tidb_config\"}).AddRow(string(tidbConfBytes)))\n\t\thasTiKV, err := CheckTiDBWithTiKV(db)\n\t\trequire.NoError(t, err)\n\t\tif store == \"tikv\" {\n\t\t\trequire.True(t, hasTiKV)\n\t\t} else {\n\t\t\trequire.False(t, hasTiKV)\n\t\t}\n\t\trequire.NoError(t, mock.ExpectationsWereMet())\n\t}\n\n\terrLackPrivilege := errors.New(\"ERROR 1142 (42000): SELECT command denied to user 'test'@'%' for table 'tidb'\")\n\texpectedResults := []interface{}{errLackPrivilege, 1, 0}\n\tfor i, res := range expectedResults {\n\t\tt.Logf(\"case #%d\", i)\n\t\tmock.ExpectQuery(\"SELECT @@tidb_config\").WillReturnError(errLackPrivilege)\n\t\texpectedErr, ok := res.(error)\n\t\tif ok {\n\t\t\tmock.ExpectQuery(\"SELECT COUNT\").WillReturnError(expectedErr)\n\t\t\thasTiKV, err := CheckTiDBWithTiKV(db)\n\t\t\trequire.ErrorIs(t, err, expectedErr)\n\t\t\trequire.True(t, hasTiKV)\n\t\t} else if cnt, ok := res.(int); ok {\n\t\t\tmock.ExpectQuery(\"SELECT COUNT\").WillReturnRows(\n\t\t\t\tsqlmock.NewRows([]string{\"c\"}).AddRow(cnt))\n\t\t\thasTiKV, err := CheckTiDBWithTiKV(db)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, cnt > 0, hasTiKV)\n\t\t}\n\t\trequire.NoError(t, mock.ExpectationsWereMet())\n\t}\n}\n\nfunc TestPickupPossibleField(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\ttctx := tcontext.Background().WithLogger(appLogger)\n\tbaseConn := newBaseConn(conn, true, nil)\n\n\tmeta := &mockTableIR{\n\t\tdbName:   database,\n\t\ttblName:  table,\n\t\tcolNames: []string{\"string1\", \"int1\", \"int2\", \"float1\", \"bin1\", \"int3\", \"bool1\", \"int4\"},\n\t\tcolTypes: []string{\"VARCHAR\", \"INT\", \"BIGINT\", \"FLOAT\", \"BINARY\", \"MEDIUMINT\", \"BOOL\", \"TINYINT\"},\n\t\tspecCmt: []string{\n\t\t\t\"/*!40101 SET NAMES binary*/;\",\n\t\t},\n\t}\n\n\ttestCases := []struct {\n\t\texpectedErr      error\n\t\texpectedField    string\n\t\thasImplicitRowID bool\n\t\tshowIndexResults [][]driver.Value\n\t}{\n\t\t{\n\t\t\terrors.New(\"show index error\"),\n\t\t\t\"\",\n\t\t\tfalse,\n\t\t\tnil,\n\t\t}, {\n\t\t\tnil,\n\t\t\t\"_tidb_rowid\",\n\t\t\ttrue,\n\t\t\tnil,\n\t\t}, // both primary and unique key columns are integers, use primary key first\n\t\t{\n\t\t\tnil,\n\t\t\t\"int1\",\n\t\t\tfalse,\n\t\t\t[][]driver.Value{\n\t\t\t\t{table, 0, \"PRIMARY\", 1, \"int1\", \"A\", 2, nil, nil, \"\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"PRIMARY\", 2, \"float1\", \"A\", 2, nil, nil, \"\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"int2\", 1, \"int2\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"string1\", 1, \"string1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"int3\", 1, \"int3\", \"A\", 20, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t},\n\t\t}, // primary key doesn't have integer at seq 1, use unique key with integer\n\t\t{\n\t\t\tnil,\n\t\t\t\"int2\",\n\t\t\tfalse,\n\t\t\t[][]driver.Value{\n\t\t\t\t{table, 0, \"PRIMARY\", 1, \"float1\", \"A\", 2, nil, nil, \"\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"PRIMARY\", 2, \"int1\", \"A\", 2, nil, nil, \"\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"int2\", 1, \"int2\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"string1\", 1, \"string1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"int3\", 1, \"int3\", \"A\", 20, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t},\n\t\t}, // several unique keys, use unique key who has a integer in seq 1\n\t\t{\n\t\t\tnil,\n\t\t\t\"int1\",\n\t\t\tfalse,\n\t\t\t[][]driver.Value{\n\t\t\t\t{table, 0, \"u1\", 1, \"int1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u1\", 2, \"string1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u1\", 3, \"bin1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u2\", 1, \"float1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u2\", 2, \"int2\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t},\n\t\t}, // several unique keys and ordinary keys, use unique key who has a integer in seq 1\n\t\t{\n\t\t\tnil,\n\t\t\t\"int1\",\n\t\t\tfalse,\n\t\t\t[][]driver.Value{\n\t\t\t\t{table, 0, \"u1\", 1, \"float1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u1\", 2, \"int2\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u2\", 1, \"int1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u2\", 2, \"string1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u2\", 3, \"bin1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"int3\", 1, \"int3\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t},\n\t\t}, // several unique keys and ordinary keys, use unique key who has less columns\n\t\t{\n\t\t\tnil,\n\t\t\t\"int2\",\n\t\t\tfalse,\n\t\t\t[][]driver.Value{\n\t\t\t\t{table, 0, \"u1\", 1, \"int1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u1\", 2, \"string1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u1\", 3, \"bin1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u2\", 1, \"int2\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u2\", 2, \"string1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"int3\", 1, \"int3\", \"A\", 20, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t},\n\t\t}, // several unique keys and ordinary keys, use key who has max cardinality\n\t\t{\n\t\t\tnil,\n\t\t\t\"int2\",\n\t\t\tfalse,\n\t\t\t[][]driver.Value{\n\t\t\t\t{table, 0, \"PRIMARY\", 1, \"string1\", \"A\", 2, nil, nil, \"\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u1\", 1, \"float1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 0, \"u1\", 2, \"int3\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"i1\", 1, \"int1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"i2\", 1, \"int2\", \"A\", 5, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"i2\", 2, \"bool1\", \"A\", 2, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"i3\", 1, \"bin1\", \"A\", 10, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t\t{table, 1, \"i3\", 2, \"int4\", \"A\", 10, nil, nil, \"YES\", \"BTREE\", \"\", \"\"},\n\t\t\t},\n\t\t},\n\t}\n\n\tquery := fmt.Sprintf(\"SHOW INDEX FROM `%s`.`%s`\", database, table)\n\tfor i, testCase := range testCases {\n\t\tt.Logf(\"case #%d\", i)\n\n\t\tmeta.hasImplicitRowID = testCase.hasImplicitRowID\n\t\texpectedErr := testCase.expectedErr\n\t\tif expectedErr != nil {\n\t\t\tmock.ExpectQuery(query).WillReturnError(expectedErr)\n\t\t} else if !testCase.hasImplicitRowID {\n\t\t\trows := sqlmock.NewRows(showIndexHeaders)\n\t\t\tfor _, showIndexResult := range testCase.showIndexResults {\n\t\t\t\trows.AddRow(showIndexResult...)\n\t\t\t}\n\t\t\tmock.ExpectQuery(query).WillReturnRows(rows)\n\t\t}\n\n\t\tfield, err := pickupPossibleField(tctx, meta, baseConn)\n\t\tif expectedErr != nil {\n\t\t\trequire.ErrorIs(t, err, expectedErr)\n\t\t} else {\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, testCase.expectedField, field)\n\t\t}\n\t\trequire.NoError(t, mock.ExpectationsWereMet())\n\t}\n}\n\nfunc TestCheckIfSeqExists(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\n\tconn, err := db.Conn(context.Background())\n\trequire.NoError(t, err)\n\n\tmock.ExpectQuery(\"SELECT COUNT\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"c\"}).\n\t\t\tAddRow(\"1\"))\n\n\texists, err := CheckIfSeqExists(conn)\n\trequire.NoError(t, err)\n\trequire.Equal(t, true, exists)\n\n\tmock.ExpectQuery(\"SELECT COUNT\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"c\"}).\n\t\t\tAddRow(\"0\"))\n\n\texists, err = CheckIfSeqExists(conn)\n\trequire.NoError(t, err)\n\trequire.Equal(t, false, exists)\n}\n\nfunc TestGetCharsetAndDefaultCollation(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\tctx := context.Background()\n\tconn, err := db.Conn(ctx)\n\trequire.NoError(t, err)\n\n\tmock.ExpectQuery(\"SHOW CHARACTER SET\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Charset\", \"Description\", \"Default collation\", \"Maxlen\"}).\n\t\t\tAddRow(\"utf8mb4\", \"UTF-8 Unicode\", \"utf8mb4_0900_ai_ci\", 4).\n\t\t\tAddRow(\"latin1\", \"cp1252 West European\", \"latin1_swedish_ci\", 1))\n\n\tcharsetAndDefaultCollation, err := GetCharsetAndDefaultCollation(ctx, conn)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"utf8mb4_0900_ai_ci\", charsetAndDefaultCollation[\"utf8mb4\"])\n\trequire.Equal(t, \"latin1_swedish_ci\", charsetAndDefaultCollation[\"latin1\"])\n}\n\nfunc TestGetSpecifiedColumnValueAndClose(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\tctx := context.Background()\n\tconn, err := db.Conn(ctx)\n\trequire.NoError(t, err)\n\n\tmock.ExpectQuery(\"SHOW BINARY LOGS\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Log_name\", \"File_size\"}).\n\t\t\tAddRow(\"mysql-bin.000001\", 52119).\n\t\t\tAddRow(\"mysql-bin.000002\", 114))\n\n\tquery := \"SHOW BINARY LOGS\"\n\trows, err := conn.QueryContext(ctx, query)\n\trequire.NoError(t, err)\n\tdefer rows.Close()\n\tvar rowsResult []string\n\trowsResult, err = GetSpecifiedColumnValueAndClose(rows, \"Log_name\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, 2, len(rowsResult))\n\trequire.Equal(t, \"mysql-bin.000001\", rowsResult[0])\n\trequire.Equal(t, \"mysql-bin.000002\", rowsResult[1])\n\n\terr = mock.ExpectationsWereMet()\n\trequire.NoError(t, err)\n}\n\nfunc TestGetSpecifiedColumnValuesAndClose(t *testing.T) {\n\tdb, mock, err := sqlmock.New()\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db.Close())\n\t}()\n\tctx := context.Background()\n\tconn, err := db.Conn(ctx)\n\trequire.NoError(t, err)\n\n\tmock.ExpectQuery(\"SHOW BINARY LOGS\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Log_name\", \"File_size\"}).\n\t\t\tAddRow(\"mysql-bin.000001\", 52119).\n\t\t\tAddRow(\"mysql-bin.000002\", 114))\n\n\tquery := \"SHOW BINARY LOGS\"\n\trows, err := conn.QueryContext(ctx, query)\n\trequire.NoError(t, err)\n\tdefer rows.Close()\n\tvar rowsResult [][]string\n\trowsResult, err = GetSpecifiedColumnValuesAndClose(rows, \"Log_name\", \"File_size\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, 2, len(rowsResult))\n\trequire.Equal(t, 2, len(rowsResult[0]))\n\trequire.Equal(t, \"mysql-bin.000001\", rowsResult[0][0])\n\trequire.Equal(t, \"52119\", rowsResult[0][1])\n\trequire.Equal(t, \"mysql-bin.000002\", rowsResult[1][0])\n\trequire.Equal(t, \"114\", rowsResult[1][1])\n\n\tmock.ExpectQuery(\"SHOW BINARY LOGS\").\n\t\tWillReturnRows(sqlmock.NewRows([]string{\"Log_name\", \"File_size\", \"Encrypted\"}).\n\t\t\tAddRow(\"mysql-bin.000001\", 52119, \"No\").\n\t\t\tAddRow(\"mysql-bin.000002\", 114, \"No\"))\n\n\trows2, err := conn.QueryContext(ctx, query)\n\trequire.NoError(t, err)\n\tdefer rows2.Close()\n\tvar rowsResult2 [][]string\n\trowsResult2, err = GetSpecifiedColumnValuesAndClose(rows2, \"Log_name\", \"File_size\")\n\trequire.NoError(t, err)\n\trequire.Equal(t, 2, len(rowsResult2))\n\trequire.Equal(t, 2, len(rowsResult2[0]))\n\trequire.Equal(t, \"mysql-bin.000001\", rowsResult2[0][0])\n\trequire.Equal(t, \"52119\", rowsResult2[0][1])\n\trequire.Equal(t, \"mysql-bin.000002\", rowsResult2[1][0])\n\trequire.Equal(t, \"114\", rowsResult2[1][1])\n\n\terr = mock.ExpectationsWereMet()\n\trequire.NoError(t, err)\n}\n", "// Copyright 2021 PingCAP, Inc. Licensed under Apache-2.0.\n\npackage main\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"fmt\"\n\t\"net\"\n\t\"os\"\n\t\"strconv\"\n\n\t_ \"github.com/go-sql-driver/mysql\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/spf13/cobra\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\nconst (\n\tflagDatabase = \"database\"\n\tflagTable    = \"table\"\n\tflagPort     = \"port\"\n\tflagWorker   = \"worker\"\n)\n\nvar rootCmd *cobra.Command\n\nfunc main() {\n\trootCmd = &cobra.Command{}\n\trootCmd.Flags().StringP(flagDatabase, \"B\", \"s3\", \"Database to import\")\n\trootCmd.Flags().StringP(flagTable, \"T\", \"t\", \"Table to import\")\n\trootCmd.Flags().IntP(flagPort, \"P\", 4000, \"TCP/IP port to connect to\")\n\trootCmd.Flags().IntP(flagWorker, \"w\", 16, \"Workers to import synchronously\")\n\n\trootCmd.RunE = func(cmd *cobra.Command, args []string) error {\n\t\tdatabase, err := cmd.Flags().GetString(flagDatabase)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\ttable, err := cmd.Flags().GetString(flagTable)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tport, err := cmd.Flags().GetInt(flagPort)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tworker, err := cmd.Flags().GetInt(flagWorker)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\n\t\tdsn := fmt.Sprintf(\"%s:%s@tcp(%s)/%s?charset=utf8mb4\", \"root\", \"\", net.JoinHostPort(\"127.0.0.1\", strconv.Itoa(port)), database)\n\t\tdb, err := sql.Open(\"mysql\", dsn)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\n\t\ttableTemp := `CREATE TABLE IF NOT EXISTS %s (\n\t   a VARCHAR(11)\n)`\n\t\t_, err = db.Exec(fmt.Sprintf(tableTemp, table))\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\n\t\tquery := fmt.Sprintf(\"insert into %s values('aaaaaaaaaa')\", table) // nolint:gosec\n\t\tfor i := 1; i < 10000; i++ {\n\t\t\tquery += \",('aaaaaaaaaa')\"\n\t\t}\n\t\tch := make(chan struct{}, worker)\n\t\tfor i := 0; i < worker; i++ {\n\t\t\tch <- struct{}{}\n\t\t}\n\t\tvar eg *errgroup.Group\n\t\tctx, cancel := context.WithCancel(context.Background())\n\t\tdefer cancel()\n\t\teg, ctx = errgroup.WithContext(ctx)\n\t\tfor i := 0; i < 500; i++ {\n\t\t\tif ctx.Err() != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\t<-ch\n\t\t\teg.Go(func() error {\n\t\t\t\t_, err := db.ExecContext(ctx, query)\n\t\t\t\tif err != nil {\n\t\t\t\t\tcancel()\n\t\t\t\t\treturn errors.Trace(err)\n\t\t\t\t}\n\t\t\t\tch <- struct{}{}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t}\n\t\treturn eg.Wait()\n\t}\n\n\tif err := rootCmd.Execute(); err != nil {\n\t\tfmt.Printf(\"fail to import data, err: %v\", err)\n\t\tos.Exit(2)\n\t}\n}\n", "// Copyright 2022 PingCAP, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage dbutil\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"net\"\n\t\"os\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/go-sql-driver/mysql\"\n\t\"github.com/pingcap/errors\"\n\t\"github.com/pingcap/log\"\n\t\"github.com/pingcap/tidb/infoschema\"\n\t\"github.com/pingcap/tidb/parser\"\n\t\"github.com/pingcap/tidb/parser/model\"\n\ttmysql \"github.com/pingcap/tidb/parser/mysql\"\n\t\"github.com/pingcap/tidb/sessionctx/stmtctx\"\n\t\"github.com/pingcap/tidb/types\"\n\t\"github.com/pingcap/tidb/util\"\n\t\"github.com/pingcap/tidb/util/dbterror\"\n\t\"go.uber.org/zap\"\n)\n\nconst (\n\t// DefaultRetryTime is the default retry time to execute sql\n\tDefaultRetryTime = 10\n\n\t// DefaultTimeout is the default timeout for execute sql\n\tDefaultTimeout time.Duration = 10 * time.Second\n\n\t// SlowLogThreshold defines the duration to log debug log of sql when exec time greater than\n\tSlowLogThreshold = 200 * time.Millisecond\n\n\t// DefaultDeleteRowsNum is the default rows num for delete one time\n\tDefaultDeleteRowsNum int64 = 100000\n)\n\nvar (\n\t// ErrVersionNotFound means can't get the database's version\n\tErrVersionNotFound = errors.New(\"can't get the database's version\")\n\n\t// ErrNoData means no data in table\n\tErrNoData = errors.New(\"no data found in table\")\n)\n\n// DBConfig is database configuration.\ntype DBConfig struct {\n\tHost     string `toml:\"host\" json:\"host\"`\n\tUser     string `toml:\"user\" json:\"user\"`\n\tPassword string `toml:\"password\" json:\"-\"`\n\tSchema   string `toml:\"schema\" json:\"schema\"`\n\tSnapshot string `toml:\"snapshot\" json:\"snapshot\"`\n\tPort     int    `toml:\"port\" json:\"port\"`\n}\n\n// String returns native format of database configuration\nfunc (c *DBConfig) String() string {\n\tcfg, err := json.Marshal(c)\n\tif err != nil {\n\t\treturn \"<nil>\"\n\t}\n\treturn string(cfg)\n}\n\n// GetDBConfigFromEnv returns DBConfig from environment\nfunc GetDBConfigFromEnv(schema string) DBConfig {\n\thost := os.Getenv(\"MYSQL_HOST\")\n\tif host == \"\" {\n\t\thost = \"127.0.0.1\"\n\t}\n\tport, _ := strconv.Atoi(os.Getenv(\"MYSQL_PORT\"))\n\tif port == 0 {\n\t\tport = 3306\n\t}\n\tuser := os.Getenv(\"MYSQL_USER\")\n\tif user == \"\" {\n\t\tuser = \"root\"\n\t}\n\tpswd := os.Getenv(\"MYSQL_PSWD\")\n\n\treturn DBConfig{\n\t\tHost:     host,\n\t\tPort:     port,\n\t\tUser:     user,\n\t\tPassword: pswd,\n\t\tSchema:   schema,\n\t}\n}\n\n// OpenDB opens a mysql connection FD\nfunc OpenDB(cfg DBConfig, vars map[string]string) (*sql.DB, error) {\n\tdriverCfg := mysql.NewConfig()\n\tdriverCfg.Params = make(map[string]string)\n\tdriverCfg.User = cfg.User\n\tdriverCfg.Passwd = cfg.Password\n\tdriverCfg.Net = \"tcp\"\n\tdriverCfg.Addr = net.JoinHostPort(cfg.Host, strconv.Itoa(cfg.Port))\n\tdriverCfg.Params[\"charset\"] = \"utf8mb4\"\n\n\tif len(cfg.Snapshot) != 0 {\n\t\tlog.Info(\"create connection with snapshot\", zap.String(\"snapshot\", cfg.Snapshot))\n\t\tdriverCfg.Params[\"tidb_snapshot\"] = cfg.Snapshot\n\t}\n\n\tfor key, val := range vars {\n\t\t// key='val'. add single quote for better compatibility.\n\t\tdriverCfg.Params[key] = fmt.Sprintf(\"'%s'\", val)\n\t}\n\n\tc, err := mysql.NewConnector(driverCfg)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tdb := sql.OpenDB(c)\n\terr = db.Ping()\n\treturn db, errors.Trace(err)\n}\n\n// CloseDB closes the mysql fd\nfunc CloseDB(db *sql.DB) error {\n\tif db == nil {\n\t\treturn nil\n\t}\n\n\treturn errors.Trace(db.Close())\n}\n\n// GetCreateTableSQL returns the create table statement.\nfunc GetCreateTableSQL(ctx context.Context, db QueryExecutor, schemaName string, tableName string) (string, error) {\n\t/*\n\t\tshow create table example result:\n\t\tmysql> SHOW CREATE TABLE `test`.`itest`;\n\t\t+-------+--------------------------------------------------------------------+\n\t\t| Table | Create Table                                                                                                                              |\n\t\t+-------+--------------------------------------------------------------------+\n\t\t| itest | CREATE TABLE `itest` (\n\t\t\t`id` int(11) DEFAULT NULL,\n\t\t  \t`name` varchar(24) DEFAULT NULL\n\t\t\t) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin |\n\t\t+-------+--------------------------------------------------------------------+\n\t*/\n\tquery := fmt.Sprintf(\"SHOW CREATE TABLE %s\", TableName(schemaName, tableName))\n\n\tvar tbl, createTable sql.NullString\n\terr := db.QueryRowContext(ctx, query).Scan(&tbl, &createTable)\n\tif err != nil {\n\t\treturn \"\", errors.Trace(err)\n\t}\n\tif !tbl.Valid || !createTable.Valid {\n\t\treturn \"\", errors.NotFoundf(\"table %s\", tableName)\n\t}\n\n\treturn createTable.String, nil\n}\n\n// GetRowCount returns row count of the table.\n// if not specify where condition, return total row count of the table.\nfunc GetRowCount(ctx context.Context, db QueryExecutor, schemaName string, tableName string, where string, args []interface{}) (int64, error) {\n\t/*\n\t\tselect count example result:\n\t\tmysql> SELECT count(1) cnt from `test`.`itest` where id > 0;\n\t\t+------+\n\t\t| cnt  |\n\t\t+------+\n\t\t|  100 |\n\t\t+------+\n\t*/\n\n\tquery := fmt.Sprintf(\"SELECT COUNT(1) cnt FROM %s\", TableName(schemaName, tableName))\n\tif len(where) > 0 {\n\t\tquery += fmt.Sprintf(\" WHERE %s\", where)\n\t}\n\tlog.Debug(\"get row count\", zap.String(\"sql\", query), zap.Reflect(\"args\", args))\n\n\tvar cnt sql.NullInt64\n\terr := db.QueryRowContext(ctx, query, args...).Scan(&cnt)\n\tif err != nil {\n\t\treturn 0, errors.Trace(err)\n\t}\n\tif !cnt.Valid {\n\t\treturn 0, errors.NotFoundf(\"table `%s`.`%s`\", schemaName, tableName)\n\t}\n\n\treturn cnt.Int64, nil\n}\n\n// GetRandomValues returns some random value. Tips: limitArgs is the value in limitRange.\nfunc GetRandomValues(ctx context.Context, db QueryExecutor, schemaName, table, column string, num int, limitRange string, limitArgs []interface{}, collation string) ([]string, error) {\n\t/*\n\t\texample:\n\t\tmysql> SELECT `id` FROM (SELECT `id`, rand() rand_value FROM `test`.`test`  WHERE `id` COLLATE \"latin1_bin\" > 0 AND `id` COLLATE \"latin1_bin\" < 100 ORDER BY rand_value LIMIT 5) rand_tmp ORDER BY `id` COLLATE \"latin1_bin\";\n\t\t+------+\n\t\t| id   |\n\t\t+------+\n\t\t|    1 |\n\t\t|    2 |\n\t\t|    3 |\n\t\t+------+\n\t*/\n\n\tif limitRange == \"\" {\n\t\tlimitRange = \"TRUE\"\n\t}\n\n\tif collation != \"\" {\n\t\tcollation = fmt.Sprintf(\" COLLATE \\\"%s\\\"\", collation)\n\t}\n\n\tquery := fmt.Sprintf(\"SELECT %[1]s FROM (SELECT %[1]s, rand() rand_value FROM %[2]s WHERE %[3]s ORDER BY rand_value LIMIT %[4]d)rand_tmp ORDER BY %[1]s%[5]s\",\n\t\tColumnName(column), TableName(schemaName, table), limitRange, num, collation)\n\tlog.Debug(\"get random values\", zap.String(\"sql\", query), zap.Reflect(\"args\", limitArgs))\n\n\trows, err := db.QueryContext(ctx, query, limitArgs...)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tdefer rows.Close()\n\n\trandomValue := make([]string, 0, num)\n\tfor rows.Next() {\n\t\tvar value sql.NullString\n\t\terr = rows.Scan(&value)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\t\tif value.Valid {\n\t\t\trandomValue = append(randomValue, value.String)\n\t\t}\n\t}\n\n\treturn randomValue, errors.Trace(rows.Err())\n}\n\n// GetMinMaxValue return min and max value of given column by specified limitRange condition.\nfunc GetMinMaxValue(ctx context.Context, db QueryExecutor, schema, table, column string, limitRange string, limitArgs []interface{}, collation string) (minStr string, maxStr string, err error) {\n\t/*\n\t\texample:\n\t\tmysql> SELECT MIN(`id`) as MIN, MAX(`id`) as MAX FROM `test`.`testa` WHERE id > 0 AND id < 10;\n\t\t+------+------+\n\t\t| MIN  | MAX  |\n\t\t+------+------+\n\t\t|    1 |    2 |\n\t\t+------+------+\n\t*/\n\n\tif limitRange == \"\" {\n\t\tlimitRange = \"TRUE\"\n\t}\n\n\tif collation != \"\" {\n\t\tcollation = fmt.Sprintf(\" COLLATE \\\"%s\\\"\", collation)\n\t}\n\n\tquery := fmt.Sprintf(\"SELECT /*!40001 SQL_NO_CACHE */ MIN(%s%s) as MIN, MAX(%s%s) as MAX FROM %s WHERE %s\",\n\t\tColumnName(column), collation, ColumnName(column), collation, TableName(schema, table), limitRange)\n\tlog.Debug(\"GetMinMaxValue\", zap.String(\"sql\", query), zap.Reflect(\"args\", limitArgs))\n\n\tvar min, max sql.NullString\n\trows, err := db.QueryContext(ctx, query, limitArgs...)\n\tif err != nil {\n\t\treturn \"\", \"\", errors.Trace(err)\n\t}\n\tdefer rows.Close()\n\n\tfor rows.Next() {\n\t\terr = rows.Scan(&min, &max)\n\t\tif err != nil {\n\t\t\treturn \"\", \"\", errors.Trace(err)\n\t\t}\n\t}\n\n\tif !min.Valid || !max.Valid {\n\t\t// don't have any data\n\t\treturn \"\", \"\", ErrNoData\n\t}\n\n\treturn min.String, max.String, errors.Trace(rows.Err())\n}\n\n// GetTimeZoneOffset is to get offset of timezone.\nfunc GetTimeZoneOffset(ctx context.Context, db QueryExecutor) (time.Duration, error) {\n\tvar timeStr string\n\terr := db.QueryRowContext(ctx, \"SELECT cast(TIMEDIFF(NOW(6), UTC_TIMESTAMP(6)) as time);\").Scan(&timeStr)\n\tif err != nil {\n\t\treturn 0, errors.Trace(err)\n\t}\n\tfactor := time.Duration(1)\n\tif timeStr[0] == '-' || timeStr[0] == '+' {\n\t\tif timeStr[0] == '-' {\n\t\t\tfactor *= -1\n\t\t}\n\t\ttimeStr = timeStr[1:]\n\t}\n\tt, err := time.Parse(\"15:04:05\", timeStr)\n\tif err != nil {\n\t\treturn 0, errors.Trace(err)\n\t}\n\n\tif t.IsZero() {\n\t\treturn 0, nil\n\t}\n\n\thour, minute, second := t.Clock()\n\t//nolint:durationcheck\n\treturn time.Duration(hour*3600+minute*60+second) * time.Second * factor, nil\n}\n\n// FormatTimeZoneOffset is to format offset of timezone.\nfunc FormatTimeZoneOffset(offset time.Duration) string {\n\tprefix := \"+\"\n\tif offset < 0 {\n\t\tprefix = \"-\"\n\t\toffset *= -1\n\t}\n\thours := offset / time.Hour\n\tminutes := (offset % time.Hour) / time.Minute\n\n\treturn fmt.Sprintf(\"%s%02d:%02d\", prefix, hours, minutes)\n}\n\nfunc queryTables(ctx context.Context, db QueryExecutor, q string) (tables []string, err error) {\n\tlog.Debug(\"query tables\", zap.String(\"query\", q))\n\trows, err := db.QueryContext(ctx, q)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tdefer rows.Close()\n\n\ttables = make([]string, 0, 8)\n\tfor rows.Next() {\n\t\tvar table, tType sql.NullString\n\t\terr = rows.Scan(&table, &tType)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\n\t\tif !table.Valid || !tType.Valid {\n\t\t\tcontinue\n\t\t}\n\n\t\ttables = append(tables, table.String)\n\t}\n\n\treturn tables, errors.Trace(rows.Err())\n}\n\n// GetTables returns name of all tables in the specified schema\nfunc GetTables(ctx context.Context, db QueryExecutor, schemaName string) (tables []string, err error) {\n\t/*\n\t\tshow tables without view: https://dev.mysql.com/doc/refman/5.7/en/show-tables.html\n\n\t\texample:\n\t\tmysql> show full tables in test where Table_Type != 'VIEW';\n\t\t+----------------+------------+\n\t\t| Tables_in_test | Table_type |\n\t\t+----------------+------------+\n\t\t| NTEST          | BASE TABLE |\n\t\t+----------------+------------+\n\t*/\n\tquery := fmt.Sprintf(\"SHOW FULL TABLES IN `%s` WHERE Table_Type != 'VIEW';\", escapeName(schemaName))\n\treturn queryTables(ctx, db, query)\n}\n\n// GetViews returns names of all views in the specified schema\nfunc GetViews(ctx context.Context, db QueryExecutor, schemaName string) (tables []string, err error) {\n\tquery := fmt.Sprintf(\"SHOW FULL TABLES IN `%s` WHERE Table_Type = 'VIEW';\", escapeName(schemaName))\n\treturn queryTables(ctx, db, query)\n}\n\n// GetSchemas returns name of all schemas\nfunc GetSchemas(ctx context.Context, db QueryExecutor) ([]string, error) {\n\tquery := \"SHOW DATABASES\"\n\trows, err := db.QueryContext(ctx, query)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tdefer rows.Close()\n\n\t// show an example.\n\t/*\n\t\tmysql> SHOW DATABASES;\n\t\t+--------------------+\n\t\t| Database           |\n\t\t+--------------------+\n\t\t| information_schema |\n\t\t| mysql              |\n\t\t| performance_schema |\n\t\t| sys                |\n\t\t| test_db            |\n\t\t+--------------------+\n\t*/\n\tschemas := make([]string, 0, 10)\n\tfor rows.Next() {\n\t\tvar schema string\n\t\terr = rows.Scan(&schema)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\t\tschemas = append(schemas, schema)\n\t}\n\treturn schemas, errors.Trace(rows.Err())\n}\n\n// GetCRC32Checksum returns checksum code of some data by given condition\nfunc GetCRC32Checksum(ctx context.Context, db QueryExecutor, schemaName, tableName string, tbInfo *model.TableInfo, limitRange string, args []interface{}) (int64, error) {\n\t/*\n\t\tcalculate CRC32 checksum example:\n\t\tmysql> SELECT BIT_XOR(CAST(CRC32(CONCAT_WS(',', id, name, age, CONCAT(ISNULL(id), ISNULL(name), ISNULL(age))))AS UNSIGNED)) AS checksum FROM test.test WHERE id > 0 AND id < 10;\n\t\t+------------+\n\t\t| checksum   |\n\t\t+------------+\n\t\t| 1466098199 |\n\t\t+------------+\n\t*/\n\tcolumnNames := make([]string, 0, len(tbInfo.Columns))\n\tcolumnIsNull := make([]string, 0, len(tbInfo.Columns))\n\tfor _, col := range tbInfo.Columns {\n\t\tcolumnNames = append(columnNames, ColumnName(col.Name.O))\n\t\tcolumnIsNull = append(columnIsNull, fmt.Sprintf(\"ISNULL(%s)\", ColumnName(col.Name.O)))\n\t}\n\n\tquery := fmt.Sprintf(\"SELECT BIT_XOR(CAST(CRC32(CONCAT_WS(',', %s, CONCAT(%s)))AS UNSIGNED)) AS checksum FROM %s WHERE %s;\",\n\t\tstrings.Join(columnNames, \", \"), strings.Join(columnIsNull, \", \"), TableName(schemaName, tableName), limitRange)\n\tlog.Debug(\"checksum\", zap.String(\"sql\", query), zap.Reflect(\"args\", args))\n\n\tvar checksum sql.NullInt64\n\terr := db.QueryRowContext(ctx, query, args...).Scan(&checksum)\n\tif err != nil {\n\t\treturn -1, errors.Trace(err)\n\t}\n\tif !checksum.Valid {\n\t\t// if don't have any data, the checksum will be `NULL`\n\t\tlog.Warn(\"get empty checksum\", zap.String(\"sql\", query), zap.Reflect(\"args\", args))\n\t\treturn 0, nil\n\t}\n\n\treturn checksum.Int64, nil\n}\n\n// Bucket saves the bucket information from TiDB.\ntype Bucket struct {\n\tLowerBound string\n\tUpperBound string\n\tCount      int64\n}\n\n// GetBucketsInfo SHOW STATS_BUCKETS in TiDB.\nfunc GetBucketsInfo(ctx context.Context, db QueryExecutor, schema, table string, tableInfo *model.TableInfo) (map[string][]Bucket, error) {\n\t/*\n\t\texample in tidb:\n\t\tmysql> SHOW STATS_BUCKETS WHERE db_name= \"test\" AND table_name=\"testa\";\n\t\t+---------+------------+----------------+-------------+----------+-----------+-------+---------+---------------------+---------------------+\n\t\t| Db_name | Table_name | Partition_name | Column_name | Is_index | Bucket_id | Count | Repeats | Lower_Bound         | Upper_Bound         |\n\t\t+---------+------------+----------------+-------------+----------+-----------+-------+---------+---------------------+---------------------+\n\t\t| test    | testa      |                | PRIMARY     |        1 |         0 |    64 |       1 | 1846693550524203008 | 1846838686059069440 |\n\t\t| test    | testa      |                | PRIMARY     |        1 |         1 |   128 |       1 | 1846840885082324992 | 1847056389361369088 |\n\t\t+---------+------------+----------------+-------------+----------+-----------+-------+---------+---------------------+---------------------+\n\t*/\n\tbuckets := make(map[string][]Bucket)\n\tquery := \"SHOW STATS_BUCKETS WHERE db_name= ? AND table_name= ?;\"\n\tlog.Debug(\"GetBucketsInfo\", zap.String(\"sql\", query), zap.String(\"schema\", schema), zap.String(\"table\", table))\n\n\trows, err := db.QueryContext(ctx, query, schema, table)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tdefer rows.Close()\n\n\tcols, err := rows.Columns()\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\tfor rows.Next() {\n\t\tvar dbName, tableName, partitionName, columnName, lowerBound, upperBound sql.NullString\n\t\tvar isIndex, bucketID, count, repeats, ndv sql.NullInt64\n\n\t\t// add partiton_name in new version\n\t\tswitch len(cols) {\n\t\tcase 9:\n\t\t\terr = rows.Scan(&dbName, &tableName, &columnName, &isIndex, &bucketID, &count, &repeats, &lowerBound, &upperBound)\n\t\tcase 10:\n\t\t\terr = rows.Scan(&dbName, &tableName, &partitionName, &columnName, &isIndex, &bucketID, &count, &repeats, &lowerBound, &upperBound)\n\t\tcase 11:\n\t\t\terr = rows.Scan(&dbName, &tableName, &partitionName, &columnName, &isIndex, &bucketID, &count, &repeats, &lowerBound, &upperBound, &ndv)\n\t\tdefault:\n\t\t\treturn nil, errors.New(\"Unknown struct for buckets info\")\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\n\t\tif _, ok := buckets[columnName.String]; !ok {\n\t\t\tbuckets[columnName.String] = make([]Bucket, 0, 100)\n\t\t}\n\t\tbuckets[columnName.String] = append(buckets[columnName.String], Bucket{\n\t\t\tCount:      count.Int64,\n\t\t\tLowerBound: lowerBound.String,\n\t\t\tUpperBound: upperBound.String,\n\t\t})\n\t}\n\n\t// when primary key is int type, the columnName will be column's name, not `PRIMARY`, check and transform here.\n\tindices := FindAllIndex(tableInfo)\n\tfor _, index := range indices {\n\t\tif index.Name.O != \"PRIMARY\" {\n\t\t\tcontinue\n\t\t}\n\t\t_, ok := buckets[index.Name.O]\n\t\tif !ok && len(index.Columns) == 1 {\n\t\t\tif _, ok := buckets[index.Columns[0].Name.O]; !ok {\n\t\t\t\treturn nil, errors.NotFoundf(\"primary key on %s in buckets info\", index.Columns[0].Name.O)\n\t\t\t}\n\t\t\tbuckets[index.Name.O] = buckets[index.Columns[0].Name.O]\n\t\t\tdelete(buckets, index.Columns[0].Name.O)\n\t\t}\n\t}\n\n\treturn buckets, errors.Trace(rows.Err())\n}\n\n// AnalyzeValuesFromBuckets analyze upperBound or lowerBound to string for each column.\n// upperBound and lowerBound are looks like '(123, abc)' for multiple fields, or '123' for one field.\nfunc AnalyzeValuesFromBuckets(valueString string, cols []*model.ColumnInfo) ([]string, error) {\n\t// FIXME: maybe some values contains '(', ')' or ', '\n\tvStr := strings.Trim(valueString, \"()\")\n\tvalues := strings.Split(vStr, \", \")\n\tif len(values) != len(cols) {\n\t\treturn nil, errors.Errorf(\"analyze value %s failed\", valueString)\n\t}\n\n\tfor i, col := range cols {\n\t\tif IsTimeTypeAndNeedDecode(col.GetType()) {\n\t\t\t// check if values[i] is already a time string\n\t\t\tsc := &stmtctx.StatementContext{TimeZone: time.UTC}\n\t\t\t_, err := types.ParseTime(sc, values[i], col.GetType(), types.MinFsp)\n\t\t\tif err == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tvalue, err := DecodeTimeInBucket(values[i])\n\t\t\tif err != nil {\n\t\t\t\tlog.Error(\"analyze values from buckets\", zap.String(\"column\", col.Name.O), zap.String(\"value\", values[i]), zap.Error(err))\n\t\t\t\treturn nil, errors.Trace(err)\n\t\t\t}\n\n\t\t\tvalues[i] = value\n\t\t}\n\t}\n\n\treturn values, nil\n}\n\n// DecodeTimeInBucket decodes Time from a packed uint64 value.\nfunc DecodeTimeInBucket(packedStr string) (string, error) {\n\tpacked, err := strconv.ParseUint(packedStr, 10, 64)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tif packed == 0 {\n\t\treturn \"\", nil\n\t}\n\n\tt := new(types.Time)\n\terr = t.FromPackedUint(packed)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\treturn t.String(), nil\n}\n\n// GetTidbLatestTSO returns tidb's current TSO.\nfunc GetTidbLatestTSO(ctx context.Context, db QueryExecutor) (int64, error) {\n\t/*\n\t\texample in tidb:\n\t\tmysql> SHOW MASTER STATUS;\n\t\t+-------------+--------------------+--------------+------------------+-------------------+\n\t\t| File        | Position           | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |\n\t\t+-------------+--------------------+--------------+------------------+-------------------+\n\t\t| tidb-binlog | 400718757701615617 |              |                  |                   |\n\t\t+-------------+--------------------+--------------+------------------+-------------------+\n\t*/\n\trows, err := db.QueryContext(ctx, \"SHOW MASTER STATUS\")\n\tif err != nil {\n\t\treturn 0, errors.Trace(err)\n\t}\n\tdefer rows.Close()\n\n\tfor rows.Next() {\n\t\tfields, err1 := ScanRow(rows)\n\t\tif err1 != nil {\n\t\t\treturn 0, errors.Trace(err1)\n\t\t}\n\n\t\tts, err1 := strconv.ParseInt(string(fields[\"Position\"].Data), 10, 64)\n\t\tif err1 != nil {\n\t\t\treturn 0, errors.Trace(err1)\n\t\t}\n\t\treturn ts, nil\n\t}\n\treturn 0, errors.New(\"get secondary cluster's ts failed\")\n}\n\n// GetDBVersion returns the database's version\nfunc GetDBVersion(ctx context.Context, db QueryExecutor) (string, error) {\n\t/*\n\t\texample in TiDB:\n\t\tmysql> select version();\n\t\t+--------------------------------------+\n\t\t| version()                            |\n\t\t+--------------------------------------+\n\t\t| 5.7.10-TiDB-v2.1.0-beta-173-g7e48ab1 |\n\t\t+--------------------------------------+\n\n\t\texample in MySQL:\n\t\tmysql> select version();\n\t\t+-----------+\n\t\t| version() |\n\t\t+-----------+\n\t\t| 5.7.21    |\n\t\t+-----------+\n\t*/\n\tquery := \"SELECT version()\"\n\tresult, err := db.QueryContext(ctx, query) //nolint:rowserrcheck\n\tif err != nil {\n\t\treturn \"\", errors.Trace(err)\n\t}\n\tdefer result.Close()\n\n\tvar version sql.NullString\n\tfor result.Next() {\n\t\terr := result.Scan(&version)\n\t\tif err != nil {\n\t\t\treturn \"\", errors.Trace(err)\n\t\t}\n\t\tbreak\n\t}\n\n\tif version.Valid {\n\t\treturn version.String, nil\n\t}\n\n\treturn \"\", ErrVersionNotFound\n}\n\n// GetSessionVariable gets server's session variable, although argument is QueryExecutor, (session) system variables may be\n// set through DSN\nfunc GetSessionVariable(ctx context.Context, db QueryExecutor, variable string) (value string, err error) {\n\tquery := fmt.Sprintf(\"SHOW VARIABLES LIKE '%s'\", variable)\n\trows, err := db.QueryContext(ctx, query)\n\n\tif err != nil {\n\t\treturn \"\", errors.Trace(err)\n\t}\n\tdefer rows.Close()\n\n\t// Show an example.\n\t/*\n\t\tmysql> SHOW VARIABLES LIKE \"binlog_format\";\n\t\t+---------------+-------+\n\t\t| Variable_name | Value |\n\t\t+---------------+-------+\n\t\t| binlog_format | ROW   |\n\t\t+---------------+-------+\n\t*/\n\n\tfor rows.Next() {\n\t\tif err = rows.Scan(&variable, &value); err != nil {\n\t\t\treturn \"\", errors.Trace(err)\n\t\t}\n\t}\n\n\tif err := rows.Err(); err != nil {\n\t\treturn \"\", errors.Trace(err)\n\t}\n\n\treturn value, nil\n}\n\n// GetSQLMode returns sql_mode.\nfunc GetSQLMode(ctx context.Context, db QueryExecutor) (tmysql.SQLMode, error) {\n\tsqlMode, err := GetSessionVariable(ctx, db, \"sql_mode\")\n\tif err != nil {\n\t\treturn tmysql.ModeNone, err\n\t}\n\n\tmode, err := tmysql.GetSQLMode(sqlMode)\n\treturn mode, errors.Trace(err)\n}\n\n// IsTiDB returns true if this database is tidb\nfunc IsTiDB(ctx context.Context, db QueryExecutor) (bool, error) {\n\tversion, err := GetDBVersion(ctx, db)\n\tif err != nil {\n\t\tlog.Error(\"get database's version failed\", zap.Error(err))\n\t\treturn false, errors.Trace(err)\n\t}\n\n\treturn strings.Contains(strings.ToLower(version), \"tidb\"), nil\n}\n\n// TableName returns `schema`.`table`\nfunc TableName(schema, table string) string {\n\treturn fmt.Sprintf(\"`%s`.`%s`\", escapeName(schema), escapeName(table))\n}\n\n// ColumnName returns `column`\nfunc ColumnName(column string) string {\n\treturn fmt.Sprintf(\"`%s`\", escapeName(column))\n}\n\nfunc escapeName(name string) string {\n\treturn strings.Replace(name, \"`\", \"``\", -1)\n}\n\n// ReplacePlaceholder will use args to replace '?', used for log.\n// tips: make sure the num of \"?\" is same with len(args)\nfunc ReplacePlaceholder(str string, args []string) string {\n\t/*\n\t\tfor example:\n\t\tstr is \"a > ? AND a < ?\", args is {'1', '2'},\n\t\tthis function will return \"a > '1' AND a < '2'\"\n\t*/\n\tnewStr := strings.Replace(str, \"?\", \"'%s'\", -1)\n\treturn fmt.Sprintf(newStr, util.StringsToInterfaces(args)...)\n}\n\n// ExecSQLWithRetry executes sql with retry\nfunc ExecSQLWithRetry(ctx context.Context, db DBExecutor, sql string, args ...interface{}) (err error) {\n\tfor i := 0; i < DefaultRetryTime; i++ {\n\t\tstartTime := time.Now()\n\t\t_, err = db.ExecContext(ctx, sql, args...)\n\t\ttakeDuration := time.Since(startTime)\n\t\tif takeDuration > SlowLogThreshold {\n\t\t\tlog.Debug(\"exec sql slow\", zap.String(\"sql\", sql), zap.Reflect(\"args\", args), zap.Duration(\"take\", takeDuration))\n\t\t}\n\t\tif err == nil {\n\t\t\treturn nil\n\t\t}\n\n\t\tif ignoreError(err) {\n\t\t\tlog.Warn(\"ignore execute sql error\", zap.Error(err))\n\t\t\treturn nil\n\t\t}\n\n\t\tif !IsRetryableError(err) {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\n\t\tlog.Warn(\"exe sql failed, will try again\", zap.String(\"sql\", sql), zap.Reflect(\"args\", args), zap.Error(err))\n\n\t\tif i == DefaultRetryTime-1 {\n\t\t\tbreak\n\t\t}\n\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn errors.Trace(ctx.Err())\n\t\tcase <-time.After(10 * time.Millisecond):\n\t\t}\n\t}\n\n\treturn errors.Trace(err)\n}\n\n// ExecuteSQLs executes some sqls in one transaction\nfunc ExecuteSQLs(ctx context.Context, db DBExecutor, sqls []string, args [][]interface{}) error {\n\ttxn, err := db.BeginTx(ctx, nil)\n\tif err != nil {\n\t\tlog.Error(\"exec sqls begin\", zap.Error(err))\n\t\treturn errors.Trace(err)\n\t}\n\n\tfor i := range sqls {\n\t\tstartTime := time.Now()\n\n\t\t_, err = txn.ExecContext(ctx, sqls[i], args[i]...)\n\t\tif err != nil {\n\t\t\tlog.Error(\"exec sql\", zap.String(\"sql\", sqls[i]), zap.Reflect(\"args\", args[i]), zap.Error(err))\n\t\t\trerr := txn.Rollback()\n\t\t\tif rerr != nil {\n\t\t\t\tlog.Error(\"rollback\", zap.Error(err))\n\t\t\t}\n\t\t\treturn errors.Trace(err)\n\t\t}\n\n\t\ttakeDuration := time.Since(startTime)\n\t\tif takeDuration > SlowLogThreshold {\n\t\t\tlog.Debug(\"exec sql slow\", zap.String(\"sql\", sqls[i]), zap.Reflect(\"args\", args[i]), zap.Duration(\"take\", takeDuration))\n\t\t}\n\t}\n\n\terr = txn.Commit()\n\tif err != nil {\n\t\tlog.Error(\"exec sqls commit\", zap.Error(err))\n\t\treturn errors.Trace(err)\n\t}\n\n\treturn nil\n}\n\nfunc ignoreError(err error) bool {\n\t// TODO: now only ignore some ddl error, add some dml error later\n\treturn ignoreDDLError(err)\n}\n\nfunc ignoreDDLError(err error) bool {\n\terr = errors.Cause(err)\n\tmysqlErr, ok := err.(*mysql.MySQLError)\n\tif !ok {\n\t\treturn false\n\t}\n\n\terrCode := errors.ErrCode(mysqlErr.Number)\n\tswitch errCode {\n\tcase infoschema.ErrDatabaseExists.Code(), infoschema.ErrDatabaseDropExists.Code(),\n\t\tinfoschema.ErrTableExists.Code(), infoschema.ErrTableDropExists.Code(),\n\t\tinfoschema.ErrColumnExists.Code(), infoschema.ErrIndexExists.Code():\n\t\treturn true\n\tcase dbterror.ErrDupKeyName.Code():\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// DeleteRows delete rows in several times. Only can delete less than 300,000 one time in TiDB.\nfunc DeleteRows(ctx context.Context, db DBExecutor, schemaName string, tableName string, where string, args []interface{}) error {\n\tdeleteSQL := fmt.Sprintf(\"DELETE FROM %s WHERE %s limit %d;\", TableName(schemaName, tableName), where, DefaultDeleteRowsNum)\n\tresult, err := db.ExecContext(ctx, deleteSQL, args...)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\trows, err := result.RowsAffected()\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tif rows < DefaultDeleteRowsNum {\n\t\treturn nil\n\t}\n\n\treturn DeleteRows(ctx, db, schemaName, tableName, where, args)\n}\n\n// getParser gets parser according to sql mode\nfunc getParser(sqlModeStr string) (*parser.Parser, error) {\n\tif len(sqlModeStr) == 0 {\n\t\treturn parser.New(), nil\n\t}\n\n\tsqlMode, err := tmysql.GetSQLMode(tmysql.FormatSQLModeStr(sqlModeStr))\n\tif err != nil {\n\t\treturn nil, errors.Annotatef(err, \"invalid sql mode %s\", sqlModeStr)\n\t}\n\tparser2 := parser.New()\n\tparser2.SetSQLMode(sqlMode)\n\treturn parser2, nil\n}\n\n// GetParserForDB discovers ANSI_QUOTES in db's session variables and returns a proper parser\nfunc GetParserForDB(ctx context.Context, db QueryExecutor) (*parser.Parser, error) {\n\tmode, err := GetSQLMode(ctx, db)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tparser2 := parser.New()\n\tparser2.SetSQLMode(mode)\n\treturn parser2, nil\n}\n"], "filenames": ["br/pkg/lightning/checkpoints/checkpoints.go", "br/pkg/lightning/common/util.go", "br/pkg/lightning/common/util_test.go", "br/pkg/lightning/config/config.go", "br/pkg/lightning/config/config_test.go", "cmd/importer/db.go", "dumpling/export/config.go", "dumpling/export/dump.go", "dumpling/export/sql.go", "dumpling/export/sql_test.go", "dumpling/tests/s3/import.go", "util/dbutil/common.go"], "buggy_code_start_loc": [520, 26, 19, 556, 35, 25, 218, 40, 13, 1348, 8, 22], "buggy_code_end_loc": [550, 119, 165, 1146, 630, 328, 218, 1536, 887, 1349, 52, 130], "fixing_code_start_loc": [520, 25, 18, 556, 34, 25, 219, 40, 12, 1348, 9, 22], "fixing_code_end_loc": [566, 124, 105, 1147, 631, 333, 244, 1544, 892, 1349, 54, 135], "type": "CWE-134", "message": "Use of Externally-Controlled Format String in GitHub repository pingcap/tidb prior to 6.4.0, 6.1.3.", "other": {"cve": {"id": "CVE-2022-3023", "sourceIdentifier": "security@huntr.dev", "published": "2022-11-04T12:15:14.127", "lastModified": "2022-11-05T02:02:23.467", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Use of Externally-Controlled Format String in GitHub repository pingcap/tidb prior to 6.4.0, 6.1.3."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}], "cvssMetricV30": [{"source": "security@huntr.dev", "type": "Secondary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:H/UI:R/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "HIGH", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 4.2, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 0.6, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-134"}]}, {"source": "security@huntr.dev", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-134"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:pingcap:tidb:*:*:*:*:*:*:*:*", "versionEndExcluding": "6.4.0", "matchCriteriaId": "4AD17F0B-7427-4CCD-BB5D-03ADDDD84912"}, {"vulnerable": true, "criteria": "cpe:2.3:a:pingcap:tidb:*:*:*:*:*:*:*:*", "versionStartIncluding": "6.1.0", "versionEndExcluding": "6.1.3", "matchCriteriaId": "9FA47A3B-7148-46EC-ACA9-E9A2FB74308A"}]}]}], "references": [{"url": "https://github.com/pingcap/tidb/commit/d0376379d615cc8f263a0b17c031ce403c8dcbfb", "source": "security@huntr.dev", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://huntr.dev/bounties/120f1346-e958-49d0-b66c-0f889a469540", "source": "security@huntr.dev", "tags": ["Permissions Required", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/pingcap/tidb/commit/d0376379d615cc8f263a0b17c031ce403c8dcbfb"}}