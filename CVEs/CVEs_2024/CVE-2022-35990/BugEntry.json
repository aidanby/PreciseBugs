{"buggy_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#if (defined(GOOGLE_CUDA) && GOOGLE_CUDA) || \\\n    (defined(TENSORFLOW_USE_ROCM) && TENSORFLOW_USE_ROCM)\n#define EIGEN_USE_GPU\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#include \"tensorflow/core/kernels/fake_quant_ops_functor.h\"\n// Above is the related header but clang tidy doesn't recognize it.\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/monitoring/gauge.h\"\n#include \"tensorflow/core/platform/protobuf.h\"\n#include \"tensorflow/core/util/determinism.h\"\n\nusing tensorflow::BinaryElementWiseOp;\nusing tensorflow::DEVICE_CPU;\n#if (defined(GOOGLE_CUDA) && GOOGLE_CUDA) || \\\n    (defined(TENSORFLOW_USE_ROCM) && TENSORFLOW_USE_ROCM)\nusing tensorflow::DEVICE_GPU;\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nusing tensorflow::OpKernel;\nusing tensorflow::OpKernelConstruction;\nusing tensorflow::OpKernelContext;\nusing tensorflow::Tensor;\nusing tensorflow::TensorShape;\nusing tensorflow::TTypes;  // NOLINT This is needed in CUDA mode, do not remove.\nusing tensorflow::UnaryElementWiseOp;\nusing tensorflow::errors::InvalidArgument;\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\nauto* using_fake_quant = monitoring::Gauge<bool, 0>::New(\n    \"/tensorflow/api/op/using_fake_quantization\",\n    \"True if a fake_quant op is created.\");\n\n#define SET_USING_FAKE_QUANT() using_fake_quant->GetCell()->Set(true)\n\nnamespace {\nbool IsNumBitsValid(int num_bits) { return num_bits >= 2 && num_bits <= 16; }\n}  // namespace\n\n// -----------------------------------------------------------------------------\n// Implementation of FakeQuantWithMinMaxArgsOp, see its documentation in\n// core/ops/array_ops.cc.\ntemplate <typename Device>\nclass FakeQuantWithMinMaxArgsOp\n    : public UnaryElementWiseOp<float, FakeQuantWithMinMaxArgsOp<Device>> {\n public:\n  typedef UnaryElementWiseOp<float, FakeQuantWithMinMaxArgsOp<Device>> Base;\n  explicit FakeQuantWithMinMaxArgsOp(OpKernelConstruction* context)\n      : Base::UnaryElementWiseOp(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"min\", &min_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"max\", &max_));\n    OP_REQUIRES(context, min_ < max_,\n                InvalidArgument(\"min has to be smaller than max, was: \", min_,\n                                \" >= \", max_));\n    int num_bits;\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_bits\", &num_bits));\n    OP_REQUIRES(\n        context, IsNumBitsValid(num_bits),\n        InvalidArgument(\"num_bits must be between 2 and 16, inclusive\"));\n    bool narrow_range;\n    OP_REQUIRES_OK(context, context->GetAttr(\"narrow_range\", &narrow_range));\n    quant_min_ = narrow_range ? 1 : 0;\n    quant_max_ = (1 << num_bits) - 1;\n    SET_USING_FAKE_QUANT();\n  }\n\n  void Operate(OpKernelContext* context, const Tensor& input, Tensor* output) {\n    FakeQuantWithMinMaxArgsFunctor<Device> functor;\n    functor(context->eigen_device<Device>(), input.flat<float>(), min_, max_,\n            quant_min_, quant_max_, output->flat<float>());\n  }\n\n private:\n  float min_;\n  float max_;\n  int quant_min_;\n  int quant_max_;\n};\n\n// Implementation of FakeQuantWithMinMaxArgsGradientOp, see its documentation in\n// core/ops/array_ops.cc.\ntemplate <typename Device>\nclass FakeQuantWithMinMaxArgsGradientOp\n    : public BinaryElementWiseOp<float,\n                                 FakeQuantWithMinMaxArgsGradientOp<Device>> {\n public:\n  typedef BinaryElementWiseOp<float, FakeQuantWithMinMaxArgsGradientOp<Device>>\n      Base;\n  explicit FakeQuantWithMinMaxArgsGradientOp(OpKernelConstruction* context)\n      : Base::BinaryElementWiseOp(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"min\", &min_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"max\", &max_));\n    OP_REQUIRES(context, min_ < max_,\n                InvalidArgument(\"min has to be smaller than max, was: \", min_,\n                                \" >= \", max_));\n    int num_bits;\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_bits\", &num_bits));\n    OP_REQUIRES(\n        context, IsNumBitsValid(num_bits),\n        InvalidArgument(\"num_bits must be between 2 and 16, inclusive\"));\n    bool narrow_range;\n    OP_REQUIRES_OK(context, context->GetAttr(\"narrow_range\", &narrow_range));\n    quant_min_ = narrow_range ? 1 : 0;\n    quant_max_ = (1 << num_bits) - 1;\n  }\n\n  template <int NDIMS>\n  void Operate(OpKernelContext* context, const Tensor& gradient,\n               const Tensor& input, Tensor* output) {\n    OperateNoTemplate(context, gradient, input, output);\n  }\n\n  void OperateNoTemplate(OpKernelContext* context, const Tensor& gradient,\n                         const Tensor& input, Tensor* output) {\n    OP_REQUIRES(context, input.IsSameSize(gradient),\n                InvalidArgument(\"gradient and input must be the same size\"));\n    FakeQuantWithMinMaxArgsGradientFunctor<Device> functor;\n    functor(context->eigen_device<Device>(), gradient.flat<float>(),\n            input.flat<float>(), min_, max_, quant_min_, quant_max_,\n            output->flat<float>());\n  }\n\n private:\n  float min_;\n  float max_;\n  int quant_min_;\n  int quant_max_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"FakeQuantWithMinMaxArgs\").Device(DEVICE_CPU),\n                        FakeQuantWithMinMaxArgsOp<CPUDevice>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"FakeQuantWithMinMaxArgsGradient\").Device(DEVICE_CPU),\n    FakeQuantWithMinMaxArgsGradientOp<CPUDevice>);\n\n#if (defined(GOOGLE_CUDA) && GOOGLE_CUDA) || \\\n    (defined(TENSORFLOW_USE_ROCM) && TENSORFLOW_USE_ROCM)\ntypedef Eigen::GpuDevice GPUDevice;\n\n// Forward declarations for functor specializations for GPU.\ntemplate <>\nvoid FakeQuantWithMinMaxArgsFunctor<GPUDevice>::operator()(\n    const GPUDevice& d, typename TTypes<float>::ConstFlat inputs,\n    const float min, const float max, const int quant_min, const int quant_max,\n    typename TTypes<float>::Flat outputs);\nextern template struct FakeQuantWithMinMaxArgsFunctor<GPUDevice>;\nREGISTER_KERNEL_BUILDER(Name(\"FakeQuantWithMinMaxArgs\").Device(DEVICE_GPU),\n                        FakeQuantWithMinMaxArgsOp<GPUDevice>);\n\ntemplate <>\nvoid FakeQuantWithMinMaxArgsGradientFunctor<GPUDevice>::operator()(\n    const GPUDevice& d, typename TTypes<float>::ConstFlat gradients,\n    typename TTypes<float>::ConstFlat inputs, const float min, const float max,\n    const int quant_min, const int quant_max,\n    typename TTypes<float>::Flat backprops);\nREGISTER_KERNEL_BUILDER(\n    Name(\"FakeQuantWithMinMaxArgsGradient\").Device(DEVICE_GPU),\n    FakeQuantWithMinMaxArgsGradientOp<GPUDevice>);\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// -----------------------------------------------------------------------------\n// Implementation of FakeQuantWithMinMaxVarsOp, see its documentation in\n// core/ops/array_ops.cc.\ntemplate <typename Device>\nclass FakeQuantWithMinMaxVarsOp : public OpKernel {\n public:\n  explicit FakeQuantWithMinMaxVarsOp(OpKernelConstruction* context)\n      : OpKernel::OpKernel(context) {\n    int num_bits;\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_bits\", &num_bits));\n    OP_REQUIRES(\n        context, IsNumBitsValid(num_bits),\n        InvalidArgument(\"num_bits must be between 2 and 16, inclusive\"));\n    bool narrow_range;\n    OP_REQUIRES_OK(context, context->GetAttr(\"narrow_range\", &narrow_range));\n    quant_min_ = narrow_range ? 1 : 0;\n    quant_max_ = (1 << num_bits) - 1;\n    SET_USING_FAKE_QUANT();\n  }\n\n  void Compute(OpKernelContext* context) override {\n    CHECK_EQ(3, context->num_inputs());\n    const Tensor& input = context->input(0);\n    const Tensor& min = context->input(1);\n    const Tensor& max = context->input(2);\n\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(min.shape()),\n        InvalidArgument(\"`min` must be rank 0 but is rank \", min.dims()));\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(max.shape()),\n        InvalidArgument(\"`max` must be rank 0 but is rank \", max.dims()));\n\n    Tensor* output;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, input.shape(), &output));\n\n    FakeQuantWithMinMaxVarsFunctor<Device> functor;\n    functor(context->eigen_device<Device>(), input.flat<float>(),\n            min.scalar<float>(), max.scalar<float>(), quant_min_, quant_max_,\n            output->flat<float>());\n  }\n\n private:\n  int quant_min_;\n  int quant_max_;\n};\n\n// Implementation of FakeQuantWithMinMaxVarsGradientOp, see its documentation in\n// core/ops/array_ops.cc.\ntemplate <typename Device>\nclass FakeQuantWithMinMaxVarsGradientOp : public OpKernel {\n public:\n  explicit FakeQuantWithMinMaxVarsGradientOp(OpKernelConstruction* context)\n      : OpKernel::OpKernel(context) {\n    int num_bits;\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_bits\", &num_bits));\n    OP_REQUIRES(\n        context, IsNumBitsValid(num_bits),\n        InvalidArgument(\"num_bits must be between 2 and 16, inclusive\"));\n    bool narrow_range;\n    OP_REQUIRES_OK(context, context->GetAttr(\"narrow_range\", &narrow_range));\n    quant_min_ = narrow_range ? 1 : 0;\n    quant_max_ = (1 << num_bits) - 1;\n    if (std::is_same<Device, Eigen::GpuDevice>::value) {\n      OP_REQUIRES(\n          context, !OpDeterminismRequired(),\n          errors::Unimplemented(\n              \"Determinism is not yet supported in GPU implementation of \"\n              \"FakeQuantWithMinMaxVarsGradient.\"));\n    }\n  }\n\n  void Compute(OpKernelContext* context) override {\n    CHECK_EQ(4, context->num_inputs());\n    const Tensor& gradient = context->input(0);\n    const Tensor& input = context->input(1);\n    OP_REQUIRES(context, input.IsSameSize(gradient),\n                InvalidArgument(\"gradient and input must be the same size\"));\n    const Tensor& min = context->input(2);\n    const Tensor& max = context->input(3);\n\n    Tensor* grad_wrt_input;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, input.shape(), &grad_wrt_input));\n\n    TensorShape scalar_shape;\n    Tensor* grad_wrt_min;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(1, scalar_shape, &grad_wrt_min));\n\n    Tensor* grad_wrt_max;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(2, scalar_shape, &grad_wrt_max));\n\n    FakeQuantWithMinMaxVarsGradientFunctor<Device> functor;\n    functor(context->eigen_device<Device>(), gradient.flat<float>(),\n            input.flat<float>(), min.scalar<float>(), max.scalar<float>(),\n            quant_min_, quant_max_, grad_wrt_input->flat<float>(),\n            grad_wrt_min->scalar<float>(), grad_wrt_max->scalar<float>());\n  }\n\n private:\n  int quant_min_;\n  int quant_max_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"FakeQuantWithMinMaxVars\").Device(DEVICE_CPU),\n                        FakeQuantWithMinMaxVarsOp<CPUDevice>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"FakeQuantWithMinMaxVarsGradient\").Device(DEVICE_CPU),\n    FakeQuantWithMinMaxVarsGradientOp<CPUDevice>);\n\n#if (defined(GOOGLE_CUDA) && GOOGLE_CUDA) || \\\n    (defined(TENSORFLOW_USE_ROCM) && TENSORFLOW_USE_ROCM)\ntemplate <>\nvoid FakeQuantWithMinMaxVarsFunctor<GPUDevice>::operator()(\n    const GPUDevice& d, typename TTypes<float>::ConstFlat inputs,\n    typename TTypes<float>::ConstScalar min,\n    typename TTypes<float>::ConstScalar max, const int quant_min,\n    const int quant_max, typename TTypes<float>::Flat output);\nextern template struct FakeQuantWithMinMaxVarsFunctor<GPUDevice>;\nREGISTER_KERNEL_BUILDER(Name(\"FakeQuantWithMinMaxVars\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"min\")\n                            .HostMemory(\"max\"),\n                        FakeQuantWithMinMaxVarsOp<GPUDevice>);\n\ntemplate <>\nvoid FakeQuantWithMinMaxVarsGradientFunctor<GPUDevice>::operator()(\n    const GPUDevice& d, typename TTypes<float>::ConstFlat gradients,\n    typename TTypes<float>::ConstFlat inputs,\n    typename TTypes<float>::ConstScalar min,\n    typename TTypes<float>::ConstScalar max, const int quant_min,\n    const int quant_max, typename TTypes<float>::Flat backprops_wrt_input,\n    typename TTypes<float>::Scalar backprop_wrt_min,\n    typename TTypes<float>::Scalar backprop_wrt_max);\nextern template struct FakeQuantWithMinMaxVarsGradientFunctor<GPUDevice>;\nREGISTER_KERNEL_BUILDER(Name(\"FakeQuantWithMinMaxVarsGradient\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"min\")\n                            .HostMemory(\"max\"),\n                        FakeQuantWithMinMaxVarsGradientOp<GPUDevice>);\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// -----------------------------------------------------------------------------\n// Implementation of FakeQuantWithMinMaxVarsPerChannelOp, see its documentation\n// in core/ops/array_ops.cc.\ntemplate <typename Device>\nclass FakeQuantWithMinMaxVarsPerChannelOp : public OpKernel {\n public:\n  explicit FakeQuantWithMinMaxVarsPerChannelOp(OpKernelConstruction* context)\n      : OpKernel::OpKernel(context) {\n    int num_bits;\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_bits\", &num_bits));\n    OP_REQUIRES(\n        context, IsNumBitsValid(num_bits),\n        InvalidArgument(\"num_bits must be between 2 and 16, inclusive\"));\n    bool narrow_range;\n    OP_REQUIRES_OK(context, context->GetAttr(\"narrow_range\", &narrow_range));\n    quant_min_ = narrow_range ? 1 : 0;\n    quant_max_ = (1 << num_bits) - 1;\n    SET_USING_FAKE_QUANT();\n  }\n\n  void Compute(OpKernelContext* context) override {\n    CHECK_EQ(3, context->num_inputs());\n    const Tensor& input = context->input(0);\n    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.\n    const Tensor& min = context->input(1);\n    const Tensor& max = context->input(2);\n\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(min.shape()),\n        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));\n    OP_REQUIRES(context, min.dim_size(0) == depth,\n                InvalidArgument(\"min has incorrect size, expected \", depth,\n                                \" was \", min.dim_size(0)));\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(max.shape()),\n        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));\n    OP_REQUIRES(context, max.dim_size(0) == depth,\n                InvalidArgument(\"max has incorrect size, expected \", depth,\n                                \" was \", max.dim_size(0)));\n\n    Tensor* output;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, input.shape(), &output));\n\n    FakeQuantWithMinMaxVarsPerChannelFunctor<Device> functor;\n    functor(context->eigen_device<Device>(), input.flat_inner_dims<float, 2>(),\n            min.vec<float>(), max.vec<float>(), quant_min_, quant_max_,\n            output->flat_inner_dims<float, 2>());\n  }\n\n private:\n  int quant_min_;\n  int quant_max_;\n};\n\n// Implementation of FakeQuantWithMinMaxVarsPerChannelGradientOp, see its\n// documentation in core/ops/array_ops.cc.\ntemplate <typename Device>\nclass FakeQuantWithMinMaxVarsPerChannelGradientOp : public OpKernel {\n public:\n  explicit FakeQuantWithMinMaxVarsPerChannelGradientOp(\n      OpKernelConstruction* context)\n      : OpKernel::OpKernel(context) {\n    int num_bits;\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_bits\", &num_bits));\n    OP_REQUIRES(\n        context, IsNumBitsValid(num_bits),\n        InvalidArgument(\"num_bits must be between 2 and 16, inclusive\"));\n    bool narrow_range;\n    OP_REQUIRES_OK(context, context->GetAttr(\"narrow_range\", &narrow_range));\n    quant_min_ = narrow_range ? 1 : 0;\n    quant_max_ = (1 << num_bits) - 1;\n    if (std::is_same<Device, Eigen::GpuDevice>::value) {\n      OP_REQUIRES(\n          context, !OpDeterminismRequired(),\n          errors::Unimplemented(\n              \"Determinism is not yet supported in GPU implementation of \"\n              \"FakeQuantWithMinMaxVarsPerChannelGradient.\"));\n    }\n  }\n\n  void Compute(OpKernelContext* context) override {\n    CHECK_EQ(4, context->num_inputs());\n    const Tensor& gradient = context->input(0);\n    const Tensor& input = context->input(1);\n    OP_REQUIRES(context, input.IsSameSize(gradient),\n                InvalidArgument(\"gradient and input must be the same size\"));\n    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.\n    const Tensor& min = context->input(2);\n    OP_REQUIRES(context, min.dim_size(0) == depth,\n                InvalidArgument(\"min has incorrect size, expected \", depth,\n                                \" was \", min.dim_size(0)));\n    const Tensor& max = context->input(3);\n    OP_REQUIRES(context, max.dim_size(0) == depth,\n                InvalidArgument(\"max has incorrect size, expected \", depth,\n                                \" was \", max.dim_size(0)));\n\n    Tensor* grad_wrt_input;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, input.shape(), &grad_wrt_input));\n\n    TensorShape min_max_shape({input.dim_size(input.dims() - 1)});\n    Tensor* grad_wrt_min;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(1, min_max_shape, &grad_wrt_min));\n\n    Tensor* grad_wrt_max;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(2, min_max_shape, &grad_wrt_max));\n\n    FakeQuantWithMinMaxVarsPerChannelGradientFunctor<Device> functor;\n    functor(\n        context->eigen_device<Device>(), gradient.flat_inner_dims<float, 2>(),\n        input.flat_inner_dims<float, 2>(), min.vec<float>(), max.vec<float>(),\n        quant_min_, quant_max_, grad_wrt_input->flat_inner_dims<float, 2>(),\n        grad_wrt_min->vec<float>(), grad_wrt_max->vec<float>());\n  }\n\n private:\n  int quant_min_;\n  int quant_max_;\n};\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"FakeQuantWithMinMaxVarsPerChannel\").Device(DEVICE_CPU),\n    FakeQuantWithMinMaxVarsPerChannelOp<CPUDevice>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"FakeQuantWithMinMaxVarsPerChannelGradient\").Device(DEVICE_CPU),\n    FakeQuantWithMinMaxVarsPerChannelGradientOp<CPUDevice>);\n\n#if (defined(GOOGLE_CUDA) && GOOGLE_CUDA) || \\\n    (defined(TENSORFLOW_USE_ROCM) && TENSORFLOW_USE_ROCM)\ntemplate <>\nvoid FakeQuantWithMinMaxVarsPerChannelFunctor<GPUDevice>::operator()(\n    const GPUDevice& d, typename TTypes<float>::ConstMatrix inputs,\n    typename TTypes<float>::ConstFlat min,\n    typename TTypes<float>::ConstFlat max, const int quant_min,\n    const int quant_max, typename TTypes<float>::Matrix outputs);\nextern template struct FakeQuantWithMinMaxVarsPerChannelFunctor<GPUDevice>;\n\nREGISTER_KERNEL_BUILDER(Name(\"FakeQuantWithMinMaxVarsPerChannel\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"min\")\n                            .HostMemory(\"max\"),\n                        FakeQuantWithMinMaxVarsPerChannelOp<GPUDevice>);\n\ntemplate <>\nvoid FakeQuantWithMinMaxVarsPerChannelGradientFunctor<GPUDevice>::operator()(\n    const GPUDevice& d, typename TTypes<float>::ConstMatrix gradients,\n    typename TTypes<float>::ConstMatrix inputs,\n    typename TTypes<float>::ConstVec min, typename TTypes<float>::ConstVec max,\n    const int quant_min, const int quant_max,\n    typename TTypes<float>::Matrix backprops_wrt_input,\n    typename TTypes<float>::Vec backprop_wrt_min,\n    typename TTypes<float>::Vec backprop_wrt_max);\nextern template struct FakeQuantWithMinMaxVarsPerChannelGradientFunctor<\n    GPUDevice>;\n\nREGISTER_KERNEL_BUILDER(Name(\"FakeQuantWithMinMaxVarsPerChannelGradient\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"min\")\n                            .HostMemory(\"max\"),\n                        FakeQuantWithMinMaxVarsPerChannelGradientOp<GPUDevice>);\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tf.quantize ops.\"\"\"\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.platform import googletest\n\n\nclass FakeQuantWithMinMaxVarsOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars(\n              inputs=inputs, min=0.0, max=[[1.0], [2.0], [4.0]]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars(\n              inputs=inputs, min=[[1.0], [2.0], [4.0]], max=1.0))\n\n\nclass FakeQuantWithMinMaxVarsPerChannelOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 1\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel(\n              inputs=inputs, min=[[0.0]], max=[1.0]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Dimensions must be equal|incorrect size\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel(\n              inputs=inputs, min=[0.0, 0.1], max=[1.0]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 1\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel(\n              inputs=inputs, min=[1.0], max=[[1.0]]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Dimensions must be equal|incorrect size\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel(\n              inputs=inputs, min=[0.0], max=[1.0, 1.1]))\n\n\nclass QuantizedBiasedAddTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.qint8)\n    bias = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.qint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_bias_add(\n              input=inputs,\n              bias=bias,\n              min_input=[],\n              max_input=1.0,\n              min_bias=0.0,\n              max_bias=1.0,\n              out_type=dtypes.qint32))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_bias_add(\n              input=inputs,\n              bias=bias,\n              min_input=0.0,\n              max_input=[],\n              min_bias=0.0,\n              max_bias=1.0,\n              out_type=dtypes.qint32))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_bias_add(\n              input=inputs,\n              bias=bias,\n              min_input=0.0,\n              max_input=1.0,\n              min_bias=[],\n              max_bias=1.0,\n              out_type=dtypes.qint32))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_bias_add(\n              input=inputs,\n              bias=bias,\n              min_input=0.0,\n              max_input=1.0,\n              min_bias=0.0,\n              max_bias=[],\n              out_type=dtypes.qint32))\n\n\nclass QuantizedInstanceNormOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          array_ops.quantized_instance_norm(\n              x=inputs, x_min=0.0, x_max=[[1.0], [2.0], [4.0]]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          array_ops.quantized_instance_norm(\n              x=inputs, x_min=[[1.0], [2.0], [4.0]], x_max=1.0))\n\n\nclass QuantizedAvgPoolingOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n    ksize = [1, 1, 1, 1]\n    strides = [1, 1, 1, 1]\n    padding = \"SAME\"\n\n    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),\n                                \"must be.* rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_avg_pool(\n              input=inputs,\n              min_input=[],\n              max_input=1.0,\n              ksize=ksize,\n              strides=strides,\n              padding=padding))\n\n    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),\n                                \"must be.* rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_avg_pool(\n              input=inputs,\n              min_input=0.0,\n              max_input=[],\n              ksize=ksize,\n              strides=strides,\n              padding=padding))\n\n\nclass QuantizedMaxPoolingOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n    ksize = [1, 1, 1, 1]\n    strides = [1, 1, 1, 1]\n    padding = \"SAME\"\n\n    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),\n                                \"must be.* rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_max_pool(\n              input=inputs,\n              min_input=[],\n              max_input=1.0,\n              ksize=ksize,\n              strides=strides,\n              padding=padding))\n\n    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),\n                                \"must be.* rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_max_pool(\n              input=inputs,\n              min_input=0.0,\n              max_input=[],\n              ksize=ksize,\n              strides=strides,\n              padding=padding))\n\n\nclass RequantizeOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.requantize(\n              input=inputs,\n              input_min=[],\n              input_max=1.0,\n              requested_output_min=0.0,\n              requested_output_max=1.0,\n              out_type=dtypes.qint8))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.requantize(\n              input=inputs,\n              input_min=0.0,\n              input_max=[],\n              requested_output_min=0.0,\n              requested_output_max=1.0,\n              out_type=dtypes.qint8))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.requantize(\n              input=inputs,\n              input_min=0.0,\n              input_max=1.0,\n              requested_output_min=[],\n              requested_output_max=1.0,\n              out_type=dtypes.qint8))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.requantize(\n              input=inputs,\n              input_min=0.0,\n              input_max=1.0,\n              requested_output_min=0.0,\n              requested_output_max=[],\n              out_type=dtypes.qint8))\n\n\nclass QuantizedAddOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    x = constant_op.constant(\n        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n    y = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.quint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.quantized_add(\n              x=x,\n              y=y,\n              min_x=[],\n              max_x=1.0,\n              min_y=0.0,\n              max_y=1.0,\n              Toutput=dtypes.qint32))\n\n\nclass QuantizedReluOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_relu(\n              features=inputs,\n              min_features=[],\n              max_features=127.0,\n              out_type=dtypes.quint8))\n\n\nclass QuantizedRelu6OpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_relu6(\n              features=inputs,\n              min_features=[],\n              max_features=127.0,\n              out_type=dtypes.quint8))\n\n\nclass QuantizeDownAndShrinkRangeOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.quantize_down_and_shrink_range(input=inputs,\n                                                  input_min=[],\n                                                  input_max=4.0,\n                                                  out_type=dtypes.quint8))\n\n\nif __name__ == \"__main__\":\n  googletest.main()\n"], "fixing_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#if (defined(GOOGLE_CUDA) && GOOGLE_CUDA) || \\\n    (defined(TENSORFLOW_USE_ROCM) && TENSORFLOW_USE_ROCM)\n#define EIGEN_USE_GPU\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#include \"tensorflow/core/kernels/fake_quant_ops_functor.h\"\n// Above is the related header but clang tidy doesn't recognize it.\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/monitoring/gauge.h\"\n#include \"tensorflow/core/platform/protobuf.h\"\n#include \"tensorflow/core/util/determinism.h\"\n\nusing tensorflow::BinaryElementWiseOp;\nusing tensorflow::DEVICE_CPU;\n#if (defined(GOOGLE_CUDA) && GOOGLE_CUDA) || \\\n    (defined(TENSORFLOW_USE_ROCM) && TENSORFLOW_USE_ROCM)\nusing tensorflow::DEVICE_GPU;\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nusing tensorflow::OpKernel;\nusing tensorflow::OpKernelConstruction;\nusing tensorflow::OpKernelContext;\nusing tensorflow::Tensor;\nusing tensorflow::TensorShape;\nusing tensorflow::TTypes;  // NOLINT This is needed in CUDA mode, do not remove.\nusing tensorflow::UnaryElementWiseOp;\nusing tensorflow::errors::InvalidArgument;\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\nauto* using_fake_quant = monitoring::Gauge<bool, 0>::New(\n    \"/tensorflow/api/op/using_fake_quantization\",\n    \"True if a fake_quant op is created.\");\n\n#define SET_USING_FAKE_QUANT() using_fake_quant->GetCell()->Set(true)\n\nnamespace {\nbool IsNumBitsValid(int num_bits) { return num_bits >= 2 && num_bits <= 16; }\n}  // namespace\n\n// -----------------------------------------------------------------------------\n// Implementation of FakeQuantWithMinMaxArgsOp, see its documentation in\n// core/ops/array_ops.cc.\ntemplate <typename Device>\nclass FakeQuantWithMinMaxArgsOp\n    : public UnaryElementWiseOp<float, FakeQuantWithMinMaxArgsOp<Device>> {\n public:\n  typedef UnaryElementWiseOp<float, FakeQuantWithMinMaxArgsOp<Device>> Base;\n  explicit FakeQuantWithMinMaxArgsOp(OpKernelConstruction* context)\n      : Base::UnaryElementWiseOp(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"min\", &min_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"max\", &max_));\n    OP_REQUIRES(context, min_ < max_,\n                InvalidArgument(\"min has to be smaller than max, was: \", min_,\n                                \" >= \", max_));\n    int num_bits;\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_bits\", &num_bits));\n    OP_REQUIRES(\n        context, IsNumBitsValid(num_bits),\n        InvalidArgument(\"num_bits must be between 2 and 16, inclusive\"));\n    bool narrow_range;\n    OP_REQUIRES_OK(context, context->GetAttr(\"narrow_range\", &narrow_range));\n    quant_min_ = narrow_range ? 1 : 0;\n    quant_max_ = (1 << num_bits) - 1;\n    SET_USING_FAKE_QUANT();\n  }\n\n  void Operate(OpKernelContext* context, const Tensor& input, Tensor* output) {\n    FakeQuantWithMinMaxArgsFunctor<Device> functor;\n    functor(context->eigen_device<Device>(), input.flat<float>(), min_, max_,\n            quant_min_, quant_max_, output->flat<float>());\n  }\n\n private:\n  float min_;\n  float max_;\n  int quant_min_;\n  int quant_max_;\n};\n\n// Implementation of FakeQuantWithMinMaxArgsGradientOp, see its documentation in\n// core/ops/array_ops.cc.\ntemplate <typename Device>\nclass FakeQuantWithMinMaxArgsGradientOp\n    : public BinaryElementWiseOp<float,\n                                 FakeQuantWithMinMaxArgsGradientOp<Device>> {\n public:\n  typedef BinaryElementWiseOp<float, FakeQuantWithMinMaxArgsGradientOp<Device>>\n      Base;\n  explicit FakeQuantWithMinMaxArgsGradientOp(OpKernelConstruction* context)\n      : Base::BinaryElementWiseOp(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"min\", &min_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"max\", &max_));\n    OP_REQUIRES(context, min_ < max_,\n                InvalidArgument(\"min has to be smaller than max, was: \", min_,\n                                \" >= \", max_));\n    int num_bits;\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_bits\", &num_bits));\n    OP_REQUIRES(\n        context, IsNumBitsValid(num_bits),\n        InvalidArgument(\"num_bits must be between 2 and 16, inclusive\"));\n    bool narrow_range;\n    OP_REQUIRES_OK(context, context->GetAttr(\"narrow_range\", &narrow_range));\n    quant_min_ = narrow_range ? 1 : 0;\n    quant_max_ = (1 << num_bits) - 1;\n  }\n\n  template <int NDIMS>\n  void Operate(OpKernelContext* context, const Tensor& gradient,\n               const Tensor& input, Tensor* output) {\n    OperateNoTemplate(context, gradient, input, output);\n  }\n\n  void OperateNoTemplate(OpKernelContext* context, const Tensor& gradient,\n                         const Tensor& input, Tensor* output) {\n    OP_REQUIRES(context, input.IsSameSize(gradient),\n                InvalidArgument(\"gradient and input must be the same size\"));\n    FakeQuantWithMinMaxArgsGradientFunctor<Device> functor;\n    functor(context->eigen_device<Device>(), gradient.flat<float>(),\n            input.flat<float>(), min_, max_, quant_min_, quant_max_,\n            output->flat<float>());\n  }\n\n private:\n  float min_;\n  float max_;\n  int quant_min_;\n  int quant_max_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"FakeQuantWithMinMaxArgs\").Device(DEVICE_CPU),\n                        FakeQuantWithMinMaxArgsOp<CPUDevice>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"FakeQuantWithMinMaxArgsGradient\").Device(DEVICE_CPU),\n    FakeQuantWithMinMaxArgsGradientOp<CPUDevice>);\n\n#if (defined(GOOGLE_CUDA) && GOOGLE_CUDA) || \\\n    (defined(TENSORFLOW_USE_ROCM) && TENSORFLOW_USE_ROCM)\ntypedef Eigen::GpuDevice GPUDevice;\n\n// Forward declarations for functor specializations for GPU.\ntemplate <>\nvoid FakeQuantWithMinMaxArgsFunctor<GPUDevice>::operator()(\n    const GPUDevice& d, typename TTypes<float>::ConstFlat inputs,\n    const float min, const float max, const int quant_min, const int quant_max,\n    typename TTypes<float>::Flat outputs);\nextern template struct FakeQuantWithMinMaxArgsFunctor<GPUDevice>;\nREGISTER_KERNEL_BUILDER(Name(\"FakeQuantWithMinMaxArgs\").Device(DEVICE_GPU),\n                        FakeQuantWithMinMaxArgsOp<GPUDevice>);\n\ntemplate <>\nvoid FakeQuantWithMinMaxArgsGradientFunctor<GPUDevice>::operator()(\n    const GPUDevice& d, typename TTypes<float>::ConstFlat gradients,\n    typename TTypes<float>::ConstFlat inputs, const float min, const float max,\n    const int quant_min, const int quant_max,\n    typename TTypes<float>::Flat backprops);\nREGISTER_KERNEL_BUILDER(\n    Name(\"FakeQuantWithMinMaxArgsGradient\").Device(DEVICE_GPU),\n    FakeQuantWithMinMaxArgsGradientOp<GPUDevice>);\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// -----------------------------------------------------------------------------\n// Implementation of FakeQuantWithMinMaxVarsOp, see its documentation in\n// core/ops/array_ops.cc.\ntemplate <typename Device>\nclass FakeQuantWithMinMaxVarsOp : public OpKernel {\n public:\n  explicit FakeQuantWithMinMaxVarsOp(OpKernelConstruction* context)\n      : OpKernel::OpKernel(context) {\n    int num_bits;\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_bits\", &num_bits));\n    OP_REQUIRES(\n        context, IsNumBitsValid(num_bits),\n        InvalidArgument(\"num_bits must be between 2 and 16, inclusive\"));\n    bool narrow_range;\n    OP_REQUIRES_OK(context, context->GetAttr(\"narrow_range\", &narrow_range));\n    quant_min_ = narrow_range ? 1 : 0;\n    quant_max_ = (1 << num_bits) - 1;\n    SET_USING_FAKE_QUANT();\n  }\n\n  void Compute(OpKernelContext* context) override {\n    CHECK_EQ(3, context->num_inputs());\n    const Tensor& input = context->input(0);\n    const Tensor& min = context->input(1);\n    const Tensor& max = context->input(2);\n\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(min.shape()),\n        InvalidArgument(\"`min` must be rank 0 but is rank \", min.dims()));\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(max.shape()),\n        InvalidArgument(\"`max` must be rank 0 but is rank \", max.dims()));\n\n    Tensor* output;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, input.shape(), &output));\n\n    FakeQuantWithMinMaxVarsFunctor<Device> functor;\n    functor(context->eigen_device<Device>(), input.flat<float>(),\n            min.scalar<float>(), max.scalar<float>(), quant_min_, quant_max_,\n            output->flat<float>());\n  }\n\n private:\n  int quant_min_;\n  int quant_max_;\n};\n\n// Implementation of FakeQuantWithMinMaxVarsGradientOp, see its documentation in\n// core/ops/array_ops.cc.\ntemplate <typename Device>\nclass FakeQuantWithMinMaxVarsGradientOp : public OpKernel {\n public:\n  explicit FakeQuantWithMinMaxVarsGradientOp(OpKernelConstruction* context)\n      : OpKernel::OpKernel(context) {\n    int num_bits;\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_bits\", &num_bits));\n    OP_REQUIRES(\n        context, IsNumBitsValid(num_bits),\n        InvalidArgument(\"num_bits must be between 2 and 16, inclusive\"));\n    bool narrow_range;\n    OP_REQUIRES_OK(context, context->GetAttr(\"narrow_range\", &narrow_range));\n    quant_min_ = narrow_range ? 1 : 0;\n    quant_max_ = (1 << num_bits) - 1;\n    if (std::is_same<Device, Eigen::GpuDevice>::value) {\n      OP_REQUIRES(\n          context, !OpDeterminismRequired(),\n          errors::Unimplemented(\n              \"Determinism is not yet supported in GPU implementation of \"\n              \"FakeQuantWithMinMaxVarsGradient.\"));\n    }\n  }\n\n  void Compute(OpKernelContext* context) override {\n    CHECK_EQ(4, context->num_inputs());\n    const Tensor& gradient = context->input(0);\n    const Tensor& input = context->input(1);\n    OP_REQUIRES(context, input.IsSameSize(gradient),\n                InvalidArgument(\"gradient and input must be the same size\"));\n    const Tensor& min = context->input(2);\n    const Tensor& max = context->input(3);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(min.shape()),\n        InvalidArgument(\"`min` must be rank 0 but is rank \", min.dims()));\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(max.shape()),\n        InvalidArgument(\"`max` must be rank 0 but is rank \", max.dims()));\n\n    Tensor* grad_wrt_input;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, input.shape(), &grad_wrt_input));\n\n    TensorShape scalar_shape;\n    Tensor* grad_wrt_min;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(1, scalar_shape, &grad_wrt_min));\n\n    Tensor* grad_wrt_max;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(2, scalar_shape, &grad_wrt_max));\n\n    FakeQuantWithMinMaxVarsGradientFunctor<Device> functor;\n    functor(context->eigen_device<Device>(), gradient.flat<float>(),\n            input.flat<float>(), min.scalar<float>(), max.scalar<float>(),\n            quant_min_, quant_max_, grad_wrt_input->flat<float>(),\n            grad_wrt_min->scalar<float>(), grad_wrt_max->scalar<float>());\n  }\n\n private:\n  int quant_min_;\n  int quant_max_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"FakeQuantWithMinMaxVars\").Device(DEVICE_CPU),\n                        FakeQuantWithMinMaxVarsOp<CPUDevice>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"FakeQuantWithMinMaxVarsGradient\").Device(DEVICE_CPU),\n    FakeQuantWithMinMaxVarsGradientOp<CPUDevice>);\n\n#if (defined(GOOGLE_CUDA) && GOOGLE_CUDA) || \\\n    (defined(TENSORFLOW_USE_ROCM) && TENSORFLOW_USE_ROCM)\ntemplate <>\nvoid FakeQuantWithMinMaxVarsFunctor<GPUDevice>::operator()(\n    const GPUDevice& d, typename TTypes<float>::ConstFlat inputs,\n    typename TTypes<float>::ConstScalar min,\n    typename TTypes<float>::ConstScalar max, const int quant_min,\n    const int quant_max, typename TTypes<float>::Flat output);\nextern template struct FakeQuantWithMinMaxVarsFunctor<GPUDevice>;\nREGISTER_KERNEL_BUILDER(Name(\"FakeQuantWithMinMaxVars\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"min\")\n                            .HostMemory(\"max\"),\n                        FakeQuantWithMinMaxVarsOp<GPUDevice>);\n\ntemplate <>\nvoid FakeQuantWithMinMaxVarsGradientFunctor<GPUDevice>::operator()(\n    const GPUDevice& d, typename TTypes<float>::ConstFlat gradients,\n    typename TTypes<float>::ConstFlat inputs,\n    typename TTypes<float>::ConstScalar min,\n    typename TTypes<float>::ConstScalar max, const int quant_min,\n    const int quant_max, typename TTypes<float>::Flat backprops_wrt_input,\n    typename TTypes<float>::Scalar backprop_wrt_min,\n    typename TTypes<float>::Scalar backprop_wrt_max);\nextern template struct FakeQuantWithMinMaxVarsGradientFunctor<GPUDevice>;\nREGISTER_KERNEL_BUILDER(Name(\"FakeQuantWithMinMaxVarsGradient\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"min\")\n                            .HostMemory(\"max\"),\n                        FakeQuantWithMinMaxVarsGradientOp<GPUDevice>);\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// -----------------------------------------------------------------------------\n// Implementation of FakeQuantWithMinMaxVarsPerChannelOp, see its documentation\n// in core/ops/array_ops.cc.\ntemplate <typename Device>\nclass FakeQuantWithMinMaxVarsPerChannelOp : public OpKernel {\n public:\n  explicit FakeQuantWithMinMaxVarsPerChannelOp(OpKernelConstruction* context)\n      : OpKernel::OpKernel(context) {\n    int num_bits;\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_bits\", &num_bits));\n    OP_REQUIRES(\n        context, IsNumBitsValid(num_bits),\n        InvalidArgument(\"num_bits must be between 2 and 16, inclusive\"));\n    bool narrow_range;\n    OP_REQUIRES_OK(context, context->GetAttr(\"narrow_range\", &narrow_range));\n    quant_min_ = narrow_range ? 1 : 0;\n    quant_max_ = (1 << num_bits) - 1;\n    SET_USING_FAKE_QUANT();\n  }\n\n  void Compute(OpKernelContext* context) override {\n    CHECK_EQ(3, context->num_inputs());\n    const Tensor& input = context->input(0);\n    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.\n    const Tensor& min = context->input(1);\n    const Tensor& max = context->input(2);\n\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(min.shape()),\n        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));\n    OP_REQUIRES(context, min.dim_size(0) == depth,\n                InvalidArgument(\"min has incorrect size, expected \", depth,\n                                \" was \", min.dim_size(0)));\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(max.shape()),\n        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));\n    OP_REQUIRES(context, max.dim_size(0) == depth,\n                InvalidArgument(\"max has incorrect size, expected \", depth,\n                                \" was \", max.dim_size(0)));\n\n    Tensor* output;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, input.shape(), &output));\n\n    FakeQuantWithMinMaxVarsPerChannelFunctor<Device> functor;\n    functor(context->eigen_device<Device>(), input.flat_inner_dims<float, 2>(),\n            min.vec<float>(), max.vec<float>(), quant_min_, quant_max_,\n            output->flat_inner_dims<float, 2>());\n  }\n\n private:\n  int quant_min_;\n  int quant_max_;\n};\n\n// Implementation of FakeQuantWithMinMaxVarsPerChannelGradientOp, see its\n// documentation in core/ops/array_ops.cc.\ntemplate <typename Device>\nclass FakeQuantWithMinMaxVarsPerChannelGradientOp : public OpKernel {\n public:\n  explicit FakeQuantWithMinMaxVarsPerChannelGradientOp(\n      OpKernelConstruction* context)\n      : OpKernel::OpKernel(context) {\n    int num_bits;\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_bits\", &num_bits));\n    OP_REQUIRES(\n        context, IsNumBitsValid(num_bits),\n        InvalidArgument(\"num_bits must be between 2 and 16, inclusive\"));\n    bool narrow_range;\n    OP_REQUIRES_OK(context, context->GetAttr(\"narrow_range\", &narrow_range));\n    quant_min_ = narrow_range ? 1 : 0;\n    quant_max_ = (1 << num_bits) - 1;\n    if (std::is_same<Device, Eigen::GpuDevice>::value) {\n      OP_REQUIRES(\n          context, !OpDeterminismRequired(),\n          errors::Unimplemented(\n              \"Determinism is not yet supported in GPU implementation of \"\n              \"FakeQuantWithMinMaxVarsPerChannelGradient.\"));\n    }\n  }\n\n  void Compute(OpKernelContext* context) override {\n    CHECK_EQ(4, context->num_inputs());\n    const Tensor& gradient = context->input(0);\n    const Tensor& input = context->input(1);\n    OP_REQUIRES(context, input.IsSameSize(gradient),\n                InvalidArgument(\"gradient and input must be the same size\"));\n    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.\n    const Tensor& min = context->input(2);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(min.shape()),\n        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));\n    OP_REQUIRES(context, min.dim_size(0) == depth,\n                InvalidArgument(\"min has incorrect size, expected \", depth,\n                                \" was \", min.dim_size(0)));\n    const Tensor& max = context->input(3);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(max.shape()),\n        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));\n    OP_REQUIRES(context, max.dim_size(0) == depth,\n                InvalidArgument(\"max has incorrect size, expected \", depth,\n                                \" was \", max.dim_size(0)));\n\n    Tensor* grad_wrt_input;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, input.shape(), &grad_wrt_input));\n\n    TensorShape min_max_shape({input.dim_size(input.dims() - 1)});\n    Tensor* grad_wrt_min;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(1, min_max_shape, &grad_wrt_min));\n\n    Tensor* grad_wrt_max;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(2, min_max_shape, &grad_wrt_max));\n\n    FakeQuantWithMinMaxVarsPerChannelGradientFunctor<Device> functor;\n    functor(\n        context->eigen_device<Device>(), gradient.flat_inner_dims<float, 2>(),\n        input.flat_inner_dims<float, 2>(), min.vec<float>(), max.vec<float>(),\n        quant_min_, quant_max_, grad_wrt_input->flat_inner_dims<float, 2>(),\n        grad_wrt_min->vec<float>(), grad_wrt_max->vec<float>());\n  }\n\n private:\n  int quant_min_;\n  int quant_max_;\n};\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"FakeQuantWithMinMaxVarsPerChannel\").Device(DEVICE_CPU),\n    FakeQuantWithMinMaxVarsPerChannelOp<CPUDevice>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"FakeQuantWithMinMaxVarsPerChannelGradient\").Device(DEVICE_CPU),\n    FakeQuantWithMinMaxVarsPerChannelGradientOp<CPUDevice>);\n\n#if (defined(GOOGLE_CUDA) && GOOGLE_CUDA) || \\\n    (defined(TENSORFLOW_USE_ROCM) && TENSORFLOW_USE_ROCM)\ntemplate <>\nvoid FakeQuantWithMinMaxVarsPerChannelFunctor<GPUDevice>::operator()(\n    const GPUDevice& d, typename TTypes<float>::ConstMatrix inputs,\n    typename TTypes<float>::ConstFlat min,\n    typename TTypes<float>::ConstFlat max, const int quant_min,\n    const int quant_max, typename TTypes<float>::Matrix outputs);\nextern template struct FakeQuantWithMinMaxVarsPerChannelFunctor<GPUDevice>;\n\nREGISTER_KERNEL_BUILDER(Name(\"FakeQuantWithMinMaxVarsPerChannel\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"min\")\n                            .HostMemory(\"max\"),\n                        FakeQuantWithMinMaxVarsPerChannelOp<GPUDevice>);\n\ntemplate <>\nvoid FakeQuantWithMinMaxVarsPerChannelGradientFunctor<GPUDevice>::operator()(\n    const GPUDevice& d, typename TTypes<float>::ConstMatrix gradients,\n    typename TTypes<float>::ConstMatrix inputs,\n    typename TTypes<float>::ConstVec min, typename TTypes<float>::ConstVec max,\n    const int quant_min, const int quant_max,\n    typename TTypes<float>::Matrix backprops_wrt_input,\n    typename TTypes<float>::Vec backprop_wrt_min,\n    typename TTypes<float>::Vec backprop_wrt_max);\nextern template struct FakeQuantWithMinMaxVarsPerChannelGradientFunctor<\n    GPUDevice>;\n\nREGISTER_KERNEL_BUILDER(Name(\"FakeQuantWithMinMaxVarsPerChannelGradient\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"min\")\n                            .HostMemory(\"max\"),\n                        FakeQuantWithMinMaxVarsPerChannelGradientOp<GPUDevice>);\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tf.quantize ops.\"\"\"\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.platform import googletest\n\n\nclass FakeQuantWithMinMaxVarsOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars(\n              inputs=inputs, min=0.0, max=[[1.0], [2.0], [4.0]]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars(\n              inputs=inputs, min=[[1.0], [2.0], [4.0]], max=1.0))\n\n\nclass FakeQuantWithMinMaxVarsPerChannelOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 1\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel(\n              inputs=inputs, min=[[0.0]], max=[1.0]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Dimensions must be equal|incorrect size\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel(\n              inputs=inputs, min=[0.0, 0.1], max=[1.0]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 1\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel(\n              inputs=inputs, min=[1.0], max=[[1.0]]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Dimensions must be equal|incorrect size\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel(\n              inputs=inputs, min=[0.0], max=[1.0, 1.1]))\n\n\nclass FakeQuantWithMinMaxVarsGradientOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    gradients = constant_op.constant(\n        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n    inputs = constant_op.constant(\n        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be equal rank|must be rank 0\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_gradient(\n              gradients=gradients,\n              inputs=inputs,\n              min=0.0,\n              max=[[1.0], [2.0], [4.0]]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_gradient(\n              gradients=gradients,\n              inputs=inputs,\n              min=[[1.0], [2.0], [4.0]],\n              max=[[1.0], [2.0], [4.0]]))\n\n\nclass FakeQuantWithMinMaxVarsPerChannelGradientOpTest(\n    test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    gradients = constant_op.constant(\n        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n    inputs = constant_op.constant(\n        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Shapes must be equal rank|must be rank 1\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel_gradient(\n              gradients=gradients, inputs=inputs, min=[[0.0]], max=[1.0]))\n\n    with self.assertRaisesRegex(\n        (ValueError, errors.InvalidArgumentError),\n        \"Dimension 0 in both shapes must be equal|incorrect size\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel_gradient(\n              gradients=gradients, inputs=inputs, min=[0.0, 0.1], max=[1.0]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Shapes must be equal rank|must be rank 1\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel_gradient(\n              gradients=gradients, inputs=inputs, min=[1.0], max=[[1.0]]))\n\n    with self.assertRaisesRegex(\n        (ValueError, errors.InvalidArgumentError),\n        \"Dimension 0 in both shapes must be equal|incorrect size\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel_gradient(\n              gradients=gradients, inputs=inputs, min=[0.0], max=[1.0, 1.1]))\n\n\nclass QuantizedBiasedAddTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.qint8)\n    bias = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.qint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_bias_add(\n              input=inputs,\n              bias=bias,\n              min_input=[],\n              max_input=1.0,\n              min_bias=0.0,\n              max_bias=1.0,\n              out_type=dtypes.qint32))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_bias_add(\n              input=inputs,\n              bias=bias,\n              min_input=0.0,\n              max_input=[],\n              min_bias=0.0,\n              max_bias=1.0,\n              out_type=dtypes.qint32))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_bias_add(\n              input=inputs,\n              bias=bias,\n              min_input=0.0,\n              max_input=1.0,\n              min_bias=[],\n              max_bias=1.0,\n              out_type=dtypes.qint32))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_bias_add(\n              input=inputs,\n              bias=bias,\n              min_input=0.0,\n              max_input=1.0,\n              min_bias=0.0,\n              max_bias=[],\n              out_type=dtypes.qint32))\n\n\nclass QuantizedInstanceNormOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          array_ops.quantized_instance_norm(\n              x=inputs, x_min=0.0, x_max=[[1.0], [2.0], [4.0]]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          array_ops.quantized_instance_norm(\n              x=inputs, x_min=[[1.0], [2.0], [4.0]], x_max=1.0))\n\n\nclass QuantizedAvgPoolingOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n    ksize = [1, 1, 1, 1]\n    strides = [1, 1, 1, 1]\n    padding = \"SAME\"\n\n    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),\n                                \"must be.* rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_avg_pool(\n              input=inputs,\n              min_input=[],\n              max_input=1.0,\n              ksize=ksize,\n              strides=strides,\n              padding=padding))\n\n    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),\n                                \"must be.* rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_avg_pool(\n              input=inputs,\n              min_input=0.0,\n              max_input=[],\n              ksize=ksize,\n              strides=strides,\n              padding=padding))\n\n\nclass QuantizedMaxPoolingOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n    ksize = [1, 1, 1, 1]\n    strides = [1, 1, 1, 1]\n    padding = \"SAME\"\n\n    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),\n                                \"must be.* rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_max_pool(\n              input=inputs,\n              min_input=[],\n              max_input=1.0,\n              ksize=ksize,\n              strides=strides,\n              padding=padding))\n\n    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),\n                                \"must be.* rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_max_pool(\n              input=inputs,\n              min_input=0.0,\n              max_input=[],\n              ksize=ksize,\n              strides=strides,\n              padding=padding))\n\n\nclass RequantizeOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.requantize(\n              input=inputs,\n              input_min=[],\n              input_max=1.0,\n              requested_output_min=0.0,\n              requested_output_max=1.0,\n              out_type=dtypes.qint8))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.requantize(\n              input=inputs,\n              input_min=0.0,\n              input_max=[],\n              requested_output_min=0.0,\n              requested_output_max=1.0,\n              out_type=dtypes.qint8))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.requantize(\n              input=inputs,\n              input_min=0.0,\n              input_max=1.0,\n              requested_output_min=[],\n              requested_output_max=1.0,\n              out_type=dtypes.qint8))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.requantize(\n              input=inputs,\n              input_min=0.0,\n              input_max=1.0,\n              requested_output_min=0.0,\n              requested_output_max=[],\n              out_type=dtypes.qint8))\n\n\nclass QuantizedAddOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    x = constant_op.constant(\n        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n    y = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.quint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.quantized_add(\n              x=x,\n              y=y,\n              min_x=[],\n              max_x=1.0,\n              min_y=0.0,\n              max_y=1.0,\n              Toutput=dtypes.qint32))\n\n\nclass QuantizedReluOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_relu(\n              features=inputs,\n              min_features=[],\n              max_features=127.0,\n              out_type=dtypes.quint8))\n\n\nclass QuantizedRelu6OpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_relu6(\n              features=inputs,\n              min_features=[],\n              max_features=127.0,\n              out_type=dtypes.quint8))\n\n\nclass QuantizeDownAndShrinkRangeOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.quantize_down_and_shrink_range(\n              input=inputs, input_min=[], input_max=4.0,\n              out_type=dtypes.quint8))\n\n\nif __name__ == \"__main__\":\n  googletest.main()\n"], "filenames": ["tensorflow/core/kernels/fake_quant_ops.cc", "tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.py"], "buggy_code_start_loc": [263, 79], "buggy_code_end_loc": [420, 344], "fixing_code_start_loc": [264, 80], "fixing_code_end_loc": [433, 408], "type": "CWE-617", "message": "TensorFlow is an open source platform for machine learning. When `tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient` receives input `min` or `max` of rank other than 1, it gives a `CHECK` fail that can trigger a denial of service attack. We have patched the issue in GitHub commit f3cf67ac5705f4f04721d15e485e192bb319feed. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range.There are no known workarounds for this issue.", "other": {"cve": {"id": "CVE-2022-35990", "sourceIdentifier": "security-advisories@github.com", "published": "2022-09-16T22:15:11.727", "lastModified": "2022-09-20T14:55:02.243", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. When `tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient` receives input `min` or `max` of rank other than 1, it gives a `CHECK` fail that can trigger a denial of service attack. We have patched the issue in GitHub commit f3cf67ac5705f4f04721d15e485e192bb319feed. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range.There are no known workarounds for this issue."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. Cuando \"tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient\" recibe una entrada \"min\" o \"max\" de rango distinto a 1, da un fallo de \"CHECK\" que puede desencadenar un ataque de denegaci\u00f3n de servicio. Hemos parcheado el problema en el commit f3cf67ac5705f4f04721d15e485e192bb319feed de GitHub. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.10.0. Tambi\u00e9n seleccionaremos este compromiso en TensorFlow versi\u00f3n 2.9.1, TensorFlow versi\u00f3n 2.8.1, y TensorFlow versi\u00f3n 2.7.2, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido. No se presentan mitigaciones conocidas para este problema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 3.6}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-617"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.7.2", "matchCriteriaId": "C6622D95-1C86-45C5-AB55-E6EEEA0996DF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.8.0", "versionEndExcluding": "2.8.1", "matchCriteriaId": "0F9D273D-02DC-441E-AA91-EAC8DEAA4B44"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.9.0", "versionEndExcluding": "2.9.1", "matchCriteriaId": "FE4F8A81-6CC2-4F7F-9602-C170FDD926E7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc0:*:*:*:*:*:*", "matchCriteriaId": "1DBFBCE2-0A01-4575-BE45-6775ABFB8B28"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc1:*:*:*:*:*:*", "matchCriteriaId": "89806CF9-E423-4CA6-A01A-8175C260CB24"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc2:*:*:*:*:*:*", "matchCriteriaId": "F2B80690-A257-4E16-BD27-9AE045BC56ED"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc3:*:*:*:*:*:*", "matchCriteriaId": "F335F9A4-5AB8-4E53-BC18-E01F7C653E5E"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/f3cf67ac5705f4f04721d15e485e192bb319feed", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-h7ff-cfc9-wmmh", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/f3cf67ac5705f4f04721d15e485e192bb319feed"}}