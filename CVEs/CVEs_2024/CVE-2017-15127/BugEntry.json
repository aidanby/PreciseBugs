{"buggy_code": ["/*\n * Generic hugetlb support.\n * (C) Nadia Yvette Chambers, April 2004\n */\n#include <linux/list.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/seq_file.h>\n#include <linux/sysctl.h>\n#include <linux/highmem.h>\n#include <linux/mmu_notifier.h>\n#include <linux/nodemask.h>\n#include <linux/pagemap.h>\n#include <linux/mempolicy.h>\n#include <linux/compiler.h>\n#include <linux/cpuset.h>\n#include <linux/mutex.h>\n#include <linux/bootmem.h>\n#include <linux/sysfs.h>\n#include <linux/slab.h>\n#include <linux/sched/signal.h>\n#include <linux/rmap.h>\n#include <linux/string_helpers.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/jhash.h>\n\n#include <asm/page.h>\n#include <asm/pgtable.h>\n#include <asm/tlb.h>\n\n#include <linux/io.h>\n#include <linux/hugetlb.h>\n#include <linux/hugetlb_cgroup.h>\n#include <linux/node.h>\n#include <linux/userfaultfd_k.h>\n#include \"internal.h\"\n\nint hugepages_treat_as_movable;\n\nint hugetlb_max_hstate __read_mostly;\nunsigned int default_hstate_idx;\nstruct hstate hstates[HUGE_MAX_HSTATE];\n/*\n * Minimum page order among possible hugepage sizes, set to a proper value\n * at boot time.\n */\nstatic unsigned int minimum_order __read_mostly = UINT_MAX;\n\n__initdata LIST_HEAD(huge_boot_pages);\n\n/* for command line parsing */\nstatic struct hstate * __initdata parsed_hstate;\nstatic unsigned long __initdata default_hstate_max_huge_pages;\nstatic unsigned long __initdata default_hstate_size;\nstatic bool __initdata parsed_valid_hugepagesz = true;\n\n/*\n * Protects updates to hugepage_freelists, hugepage_activelist, nr_huge_pages,\n * free_huge_pages, and surplus_huge_pages.\n */\nDEFINE_SPINLOCK(hugetlb_lock);\n\n/*\n * Serializes faults on the same logical page.  This is used to\n * prevent spurious OOMs when the hugepage pool is fully utilized.\n */\nstatic int num_fault_mutexes;\nstruct mutex *hugetlb_fault_mutex_table ____cacheline_aligned_in_smp;\n\n/* Forward declaration */\nstatic int hugetlb_acct_memory(struct hstate *h, long delta);\n\nstatic inline void unlock_or_release_subpool(struct hugepage_subpool *spool)\n{\n\tbool free = (spool->count == 0) && (spool->used_hpages == 0);\n\n\tspin_unlock(&spool->lock);\n\n\t/* If no pages are used, and no other handles to the subpool\n\t * remain, give up any reservations mased on minimum size and\n\t * free the subpool */\n\tif (free) {\n\t\tif (spool->min_hpages != -1)\n\t\t\thugetlb_acct_memory(spool->hstate,\n\t\t\t\t\t\t-spool->min_hpages);\n\t\tkfree(spool);\n\t}\n}\n\nstruct hugepage_subpool *hugepage_new_subpool(struct hstate *h, long max_hpages,\n\t\t\t\t\t\tlong min_hpages)\n{\n\tstruct hugepage_subpool *spool;\n\n\tspool = kzalloc(sizeof(*spool), GFP_KERNEL);\n\tif (!spool)\n\t\treturn NULL;\n\n\tspin_lock_init(&spool->lock);\n\tspool->count = 1;\n\tspool->max_hpages = max_hpages;\n\tspool->hstate = h;\n\tspool->min_hpages = min_hpages;\n\n\tif (min_hpages != -1 && hugetlb_acct_memory(h, min_hpages)) {\n\t\tkfree(spool);\n\t\treturn NULL;\n\t}\n\tspool->rsv_hpages = min_hpages;\n\n\treturn spool;\n}\n\nvoid hugepage_put_subpool(struct hugepage_subpool *spool)\n{\n\tspin_lock(&spool->lock);\n\tBUG_ON(!spool->count);\n\tspool->count--;\n\tunlock_or_release_subpool(spool);\n}\n\n/*\n * Subpool accounting for allocating and reserving pages.\n * Return -ENOMEM if there are not enough resources to satisfy the\n * the request.  Otherwise, return the number of pages by which the\n * global pools must be adjusted (upward).  The returned value may\n * only be different than the passed value (delta) in the case where\n * a subpool minimum size must be manitained.\n */\nstatic long hugepage_subpool_get_pages(struct hugepage_subpool *spool,\n\t\t\t\t      long delta)\n{\n\tlong ret = delta;\n\n\tif (!spool)\n\t\treturn ret;\n\n\tspin_lock(&spool->lock);\n\n\tif (spool->max_hpages != -1) {\t\t/* maximum size accounting */\n\t\tif ((spool->used_hpages + delta) <= spool->max_hpages)\n\t\t\tspool->used_hpages += delta;\n\t\telse {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto unlock_ret;\n\t\t}\n\t}\n\n\t/* minimum size accounting */\n\tif (spool->min_hpages != -1 && spool->rsv_hpages) {\n\t\tif (delta > spool->rsv_hpages) {\n\t\t\t/*\n\t\t\t * Asking for more reserves than those already taken on\n\t\t\t * behalf of subpool.  Return difference.\n\t\t\t */\n\t\t\tret = delta - spool->rsv_hpages;\n\t\t\tspool->rsv_hpages = 0;\n\t\t} else {\n\t\t\tret = 0;\t/* reserves already accounted for */\n\t\t\tspool->rsv_hpages -= delta;\n\t\t}\n\t}\n\nunlock_ret:\n\tspin_unlock(&spool->lock);\n\treturn ret;\n}\n\n/*\n * Subpool accounting for freeing and unreserving pages.\n * Return the number of global page reservations that must be dropped.\n * The return value may only be different than the passed value (delta)\n * in the case where a subpool minimum size must be maintained.\n */\nstatic long hugepage_subpool_put_pages(struct hugepage_subpool *spool,\n\t\t\t\t       long delta)\n{\n\tlong ret = delta;\n\n\tif (!spool)\n\t\treturn delta;\n\n\tspin_lock(&spool->lock);\n\n\tif (spool->max_hpages != -1)\t\t/* maximum size accounting */\n\t\tspool->used_hpages -= delta;\n\n\t /* minimum size accounting */\n\tif (spool->min_hpages != -1 && spool->used_hpages < spool->min_hpages) {\n\t\tif (spool->rsv_hpages + delta <= spool->min_hpages)\n\t\t\tret = 0;\n\t\telse\n\t\t\tret = spool->rsv_hpages + delta - spool->min_hpages;\n\n\t\tspool->rsv_hpages += delta;\n\t\tif (spool->rsv_hpages > spool->min_hpages)\n\t\t\tspool->rsv_hpages = spool->min_hpages;\n\t}\n\n\t/*\n\t * If hugetlbfs_put_super couldn't free spool due to an outstanding\n\t * quota reference, free it now.\n\t */\n\tunlock_or_release_subpool(spool);\n\n\treturn ret;\n}\n\nstatic inline struct hugepage_subpool *subpool_inode(struct inode *inode)\n{\n\treturn HUGETLBFS_SB(inode->i_sb)->spool;\n}\n\nstatic inline struct hugepage_subpool *subpool_vma(struct vm_area_struct *vma)\n{\n\treturn subpool_inode(file_inode(vma->vm_file));\n}\n\n/*\n * Region tracking -- allows tracking of reservations and instantiated pages\n *                    across the pages in a mapping.\n *\n * The region data structures are embedded into a resv_map and protected\n * by a resv_map's lock.  The set of regions within the resv_map represent\n * reservations for huge pages, or huge pages that have already been\n * instantiated within the map.  The from and to elements are huge page\n * indicies into the associated mapping.  from indicates the starting index\n * of the region.  to represents the first index past the end of  the region.\n *\n * For example, a file region structure with from == 0 and to == 4 represents\n * four huge pages in a mapping.  It is important to note that the to element\n * represents the first element past the end of the region. This is used in\n * arithmetic as 4(to) - 0(from) = 4 huge pages in the region.\n *\n * Interval notation of the form [from, to) will be used to indicate that\n * the endpoint from is inclusive and to is exclusive.\n */\nstruct file_region {\n\tstruct list_head link;\n\tlong from;\n\tlong to;\n};\n\n/*\n * Add the huge page range represented by [f, t) to the reserve\n * map.  In the normal case, existing regions will be expanded\n * to accommodate the specified range.  Sufficient regions should\n * exist for expansion due to the previous call to region_chg\n * with the same range.  However, it is possible that region_del\n * could have been called after region_chg and modifed the map\n * in such a way that no region exists to be expanded.  In this\n * case, pull a region descriptor from the cache associated with\n * the map and use that for the new range.\n *\n * Return the number of new huge pages added to the map.  This\n * number is greater than or equal to zero.\n */\nstatic long region_add(struct resv_map *resv, long f, long t)\n{\n\tstruct list_head *head = &resv->regions;\n\tstruct file_region *rg, *nrg, *trg;\n\tlong add = 0;\n\n\tspin_lock(&resv->lock);\n\t/* Locate the region we are either in or before. */\n\tlist_for_each_entry(rg, head, link)\n\t\tif (f <= rg->to)\n\t\t\tbreak;\n\n\t/*\n\t * If no region exists which can be expanded to include the\n\t * specified range, the list must have been modified by an\n\t * interleving call to region_del().  Pull a region descriptor\n\t * from the cache and use it for this range.\n\t */\n\tif (&rg->link == head || t < rg->from) {\n\t\tVM_BUG_ON(resv->region_cache_count <= 0);\n\n\t\tresv->region_cache_count--;\n\t\tnrg = list_first_entry(&resv->region_cache, struct file_region,\n\t\t\t\t\tlink);\n\t\tlist_del(&nrg->link);\n\n\t\tnrg->from = f;\n\t\tnrg->to = t;\n\t\tlist_add(&nrg->link, rg->link.prev);\n\n\t\tadd += t - f;\n\t\tgoto out_locked;\n\t}\n\n\t/* Round our left edge to the current segment if it encloses us. */\n\tif (f > rg->from)\n\t\tf = rg->from;\n\n\t/* Check for and consume any regions we now overlap with. */\n\tnrg = rg;\n\tlist_for_each_entry_safe(rg, trg, rg->link.prev, link) {\n\t\tif (&rg->link == head)\n\t\t\tbreak;\n\t\tif (rg->from > t)\n\t\t\tbreak;\n\n\t\t/* If this area reaches higher then extend our area to\n\t\t * include it completely.  If this is not the first area\n\t\t * which we intend to reuse, free it. */\n\t\tif (rg->to > t)\n\t\t\tt = rg->to;\n\t\tif (rg != nrg) {\n\t\t\t/* Decrement return value by the deleted range.\n\t\t\t * Another range will span this area so that by\n\t\t\t * end of routine add will be >= zero\n\t\t\t */\n\t\t\tadd -= (rg->to - rg->from);\n\t\t\tlist_del(&rg->link);\n\t\t\tkfree(rg);\n\t\t}\n\t}\n\n\tadd += (nrg->from - f);\t\t/* Added to beginning of region */\n\tnrg->from = f;\n\tadd += t - nrg->to;\t\t/* Added to end of region */\n\tnrg->to = t;\n\nout_locked:\n\tresv->adds_in_progress--;\n\tspin_unlock(&resv->lock);\n\tVM_BUG_ON(add < 0);\n\treturn add;\n}\n\n/*\n * Examine the existing reserve map and determine how many\n * huge pages in the specified range [f, t) are NOT currently\n * represented.  This routine is called before a subsequent\n * call to region_add that will actually modify the reserve\n * map to add the specified range [f, t).  region_chg does\n * not change the number of huge pages represented by the\n * map.  However, if the existing regions in the map can not\n * be expanded to represent the new range, a new file_region\n * structure is added to the map as a placeholder.  This is\n * so that the subsequent region_add call will have all the\n * regions it needs and will not fail.\n *\n * Upon entry, region_chg will also examine the cache of region descriptors\n * associated with the map.  If there are not enough descriptors cached, one\n * will be allocated for the in progress add operation.\n *\n * Returns the number of huge pages that need to be added to the existing\n * reservation map for the range [f, t).  This number is greater or equal to\n * zero.  -ENOMEM is returned if a new file_region structure or cache entry\n * is needed and can not be allocated.\n */\nstatic long region_chg(struct resv_map *resv, long f, long t)\n{\n\tstruct list_head *head = &resv->regions;\n\tstruct file_region *rg, *nrg = NULL;\n\tlong chg = 0;\n\nretry:\n\tspin_lock(&resv->lock);\nretry_locked:\n\tresv->adds_in_progress++;\n\n\t/*\n\t * Check for sufficient descriptors in the cache to accommodate\n\t * the number of in progress add operations.\n\t */\n\tif (resv->adds_in_progress > resv->region_cache_count) {\n\t\tstruct file_region *trg;\n\n\t\tVM_BUG_ON(resv->adds_in_progress - resv->region_cache_count > 1);\n\t\t/* Must drop lock to allocate a new descriptor. */\n\t\tresv->adds_in_progress--;\n\t\tspin_unlock(&resv->lock);\n\n\t\ttrg = kmalloc(sizeof(*trg), GFP_KERNEL);\n\t\tif (!trg) {\n\t\t\tkfree(nrg);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tspin_lock(&resv->lock);\n\t\tlist_add(&trg->link, &resv->region_cache);\n\t\tresv->region_cache_count++;\n\t\tgoto retry_locked;\n\t}\n\n\t/* Locate the region we are before or in. */\n\tlist_for_each_entry(rg, head, link)\n\t\tif (f <= rg->to)\n\t\t\tbreak;\n\n\t/* If we are below the current region then a new region is required.\n\t * Subtle, allocate a new region at the position but make it zero\n\t * size such that we can guarantee to record the reservation. */\n\tif (&rg->link == head || t < rg->from) {\n\t\tif (!nrg) {\n\t\t\tresv->adds_in_progress--;\n\t\t\tspin_unlock(&resv->lock);\n\t\t\tnrg = kmalloc(sizeof(*nrg), GFP_KERNEL);\n\t\t\tif (!nrg)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tnrg->from = f;\n\t\t\tnrg->to   = f;\n\t\t\tINIT_LIST_HEAD(&nrg->link);\n\t\t\tgoto retry;\n\t\t}\n\n\t\tlist_add(&nrg->link, rg->link.prev);\n\t\tchg = t - f;\n\t\tgoto out_nrg;\n\t}\n\n\t/* Round our left edge to the current segment if it encloses us. */\n\tif (f > rg->from)\n\t\tf = rg->from;\n\tchg = t - f;\n\n\t/* Check for and consume any regions we now overlap with. */\n\tlist_for_each_entry(rg, rg->link.prev, link) {\n\t\tif (&rg->link == head)\n\t\t\tbreak;\n\t\tif (rg->from > t)\n\t\t\tgoto out;\n\n\t\t/* We overlap with this area, if it extends further than\n\t\t * us then we must extend ourselves.  Account for its\n\t\t * existing reservation. */\n\t\tif (rg->to > t) {\n\t\t\tchg += rg->to - t;\n\t\t\tt = rg->to;\n\t\t}\n\t\tchg -= rg->to - rg->from;\n\t}\n\nout:\n\tspin_unlock(&resv->lock);\n\t/*  We already know we raced and no longer need the new region */\n\tkfree(nrg);\n\treturn chg;\nout_nrg:\n\tspin_unlock(&resv->lock);\n\treturn chg;\n}\n\n/*\n * Abort the in progress add operation.  The adds_in_progress field\n * of the resv_map keeps track of the operations in progress between\n * calls to region_chg and region_add.  Operations are sometimes\n * aborted after the call to region_chg.  In such cases, region_abort\n * is called to decrement the adds_in_progress counter.\n *\n * NOTE: The range arguments [f, t) are not needed or used in this\n * routine.  They are kept to make reading the calling code easier as\n * arguments will match the associated region_chg call.\n */\nstatic void region_abort(struct resv_map *resv, long f, long t)\n{\n\tspin_lock(&resv->lock);\n\tVM_BUG_ON(!resv->region_cache_count);\n\tresv->adds_in_progress--;\n\tspin_unlock(&resv->lock);\n}\n\n/*\n * Delete the specified range [f, t) from the reserve map.  If the\n * t parameter is LONG_MAX, this indicates that ALL regions after f\n * should be deleted.  Locate the regions which intersect [f, t)\n * and either trim, delete or split the existing regions.\n *\n * Returns the number of huge pages deleted from the reserve map.\n * In the normal case, the return value is zero or more.  In the\n * case where a region must be split, a new region descriptor must\n * be allocated.  If the allocation fails, -ENOMEM will be returned.\n * NOTE: If the parameter t == LONG_MAX, then we will never split\n * a region and possibly return -ENOMEM.  Callers specifying\n * t == LONG_MAX do not need to check for -ENOMEM error.\n */\nstatic long region_del(struct resv_map *resv, long f, long t)\n{\n\tstruct list_head *head = &resv->regions;\n\tstruct file_region *rg, *trg;\n\tstruct file_region *nrg = NULL;\n\tlong del = 0;\n\nretry:\n\tspin_lock(&resv->lock);\n\tlist_for_each_entry_safe(rg, trg, head, link) {\n\t\t/*\n\t\t * Skip regions before the range to be deleted.  file_region\n\t\t * ranges are normally of the form [from, to).  However, there\n\t\t * may be a \"placeholder\" entry in the map which is of the form\n\t\t * (from, to) with from == to.  Check for placeholder entries\n\t\t * at the beginning of the range to be deleted.\n\t\t */\n\t\tif (rg->to <= f && (rg->to != rg->from || rg->to != f))\n\t\t\tcontinue;\n\n\t\tif (rg->from >= t)\n\t\t\tbreak;\n\n\t\tif (f > rg->from && t < rg->to) { /* Must split region */\n\t\t\t/*\n\t\t\t * Check for an entry in the cache before dropping\n\t\t\t * lock and attempting allocation.\n\t\t\t */\n\t\t\tif (!nrg &&\n\t\t\t    resv->region_cache_count > resv->adds_in_progress) {\n\t\t\t\tnrg = list_first_entry(&resv->region_cache,\n\t\t\t\t\t\t\tstruct file_region,\n\t\t\t\t\t\t\tlink);\n\t\t\t\tlist_del(&nrg->link);\n\t\t\t\tresv->region_cache_count--;\n\t\t\t}\n\n\t\t\tif (!nrg) {\n\t\t\t\tspin_unlock(&resv->lock);\n\t\t\t\tnrg = kmalloc(sizeof(*nrg), GFP_KERNEL);\n\t\t\t\tif (!nrg)\n\t\t\t\t\treturn -ENOMEM;\n\t\t\t\tgoto retry;\n\t\t\t}\n\n\t\t\tdel += t - f;\n\n\t\t\t/* New entry for end of split region */\n\t\t\tnrg->from = t;\n\t\t\tnrg->to = rg->to;\n\t\t\tINIT_LIST_HEAD(&nrg->link);\n\n\t\t\t/* Original entry is trimmed */\n\t\t\trg->to = f;\n\n\t\t\tlist_add(&nrg->link, &rg->link);\n\t\t\tnrg = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (f <= rg->from && t >= rg->to) { /* Remove entire region */\n\t\t\tdel += rg->to - rg->from;\n\t\t\tlist_del(&rg->link);\n\t\t\tkfree(rg);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (f <= rg->from) {\t/* Trim beginning of region */\n\t\t\tdel += t - rg->from;\n\t\t\trg->from = t;\n\t\t} else {\t\t/* Trim end of region */\n\t\t\tdel += rg->to - f;\n\t\t\trg->to = f;\n\t\t}\n\t}\n\n\tspin_unlock(&resv->lock);\n\tkfree(nrg);\n\treturn del;\n}\n\n/*\n * A rare out of memory error was encountered which prevented removal of\n * the reserve map region for a page.  The huge page itself was free'ed\n * and removed from the page cache.  This routine will adjust the subpool\n * usage count, and the global reserve count if needed.  By incrementing\n * these counts, the reserve map entry which could not be deleted will\n * appear as a \"reserved\" entry instead of simply dangling with incorrect\n * counts.\n */\nvoid hugetlb_fix_reserve_counts(struct inode *inode)\n{\n\tstruct hugepage_subpool *spool = subpool_inode(inode);\n\tlong rsv_adjust;\n\n\trsv_adjust = hugepage_subpool_get_pages(spool, 1);\n\tif (rsv_adjust) {\n\t\tstruct hstate *h = hstate_inode(inode);\n\n\t\thugetlb_acct_memory(h, 1);\n\t}\n}\n\n/*\n * Count and return the number of huge pages in the reserve map\n * that intersect with the range [f, t).\n */\nstatic long region_count(struct resv_map *resv, long f, long t)\n{\n\tstruct list_head *head = &resv->regions;\n\tstruct file_region *rg;\n\tlong chg = 0;\n\n\tspin_lock(&resv->lock);\n\t/* Locate each segment we overlap with, and count that overlap. */\n\tlist_for_each_entry(rg, head, link) {\n\t\tlong seg_from;\n\t\tlong seg_to;\n\n\t\tif (rg->to <= f)\n\t\t\tcontinue;\n\t\tif (rg->from >= t)\n\t\t\tbreak;\n\n\t\tseg_from = max(rg->from, f);\n\t\tseg_to = min(rg->to, t);\n\n\t\tchg += seg_to - seg_from;\n\t}\n\tspin_unlock(&resv->lock);\n\n\treturn chg;\n}\n\n/*\n * Convert the address within this vma to the page offset within\n * the mapping, in pagecache page units; huge pages here.\n */\nstatic pgoff_t vma_hugecache_offset(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\treturn ((address - vma->vm_start) >> huge_page_shift(h)) +\n\t\t\t(vma->vm_pgoff >> huge_page_order(h));\n}\n\npgoff_t linear_hugepage_index(struct vm_area_struct *vma,\n\t\t\t\t     unsigned long address)\n{\n\treturn vma_hugecache_offset(hstate_vma(vma), vma, address);\n}\nEXPORT_SYMBOL_GPL(linear_hugepage_index);\n\n/*\n * Return the size of the pages allocated when backing a VMA. In the majority\n * cases this will be same size as used by the page table entries.\n */\nunsigned long vma_kernel_pagesize(struct vm_area_struct *vma)\n{\n\tstruct hstate *hstate;\n\n\tif (!is_vm_hugetlb_page(vma))\n\t\treturn PAGE_SIZE;\n\n\thstate = hstate_vma(vma);\n\n\treturn 1UL << huge_page_shift(hstate);\n}\nEXPORT_SYMBOL_GPL(vma_kernel_pagesize);\n\n/*\n * Return the page size being used by the MMU to back a VMA. In the majority\n * of cases, the page size used by the kernel matches the MMU size. On\n * architectures where it differs, an architecture-specific version of this\n * function is required.\n */\n#ifndef vma_mmu_pagesize\nunsigned long vma_mmu_pagesize(struct vm_area_struct *vma)\n{\n\treturn vma_kernel_pagesize(vma);\n}\n#endif\n\n/*\n * Flags for MAP_PRIVATE reservations.  These are stored in the bottom\n * bits of the reservation map pointer, which are always clear due to\n * alignment.\n */\n#define HPAGE_RESV_OWNER    (1UL << 0)\n#define HPAGE_RESV_UNMAPPED (1UL << 1)\n#define HPAGE_RESV_MASK (HPAGE_RESV_OWNER | HPAGE_RESV_UNMAPPED)\n\n/*\n * These helpers are used to track how many pages are reserved for\n * faults in a MAP_PRIVATE mapping. Only the process that called mmap()\n * is guaranteed to have their future faults succeed.\n *\n * With the exception of reset_vma_resv_huge_pages() which is called at fork(),\n * the reserve counters are updated with the hugetlb_lock held. It is safe\n * to reset the VMA at fork() time as it is not in use yet and there is no\n * chance of the global counters getting corrupted as a result of the values.\n *\n * The private mapping reservation is represented in a subtly different\n * manner to a shared mapping.  A shared mapping has a region map associated\n * with the underlying file, this region map represents the backing file\n * pages which have ever had a reservation assigned which this persists even\n * after the page is instantiated.  A private mapping has a region map\n * associated with the original mmap which is attached to all VMAs which\n * reference it, this region map represents those offsets which have consumed\n * reservation ie. where pages have been instantiated.\n */\nstatic unsigned long get_vma_private_data(struct vm_area_struct *vma)\n{\n\treturn (unsigned long)vma->vm_private_data;\n}\n\nstatic void set_vma_private_data(struct vm_area_struct *vma,\n\t\t\t\t\t\t\tunsigned long value)\n{\n\tvma->vm_private_data = (void *)value;\n}\n\nstruct resv_map *resv_map_alloc(void)\n{\n\tstruct resv_map *resv_map = kmalloc(sizeof(*resv_map), GFP_KERNEL);\n\tstruct file_region *rg = kmalloc(sizeof(*rg), GFP_KERNEL);\n\n\tif (!resv_map || !rg) {\n\t\tkfree(resv_map);\n\t\tkfree(rg);\n\t\treturn NULL;\n\t}\n\n\tkref_init(&resv_map->refs);\n\tspin_lock_init(&resv_map->lock);\n\tINIT_LIST_HEAD(&resv_map->regions);\n\n\tresv_map->adds_in_progress = 0;\n\n\tINIT_LIST_HEAD(&resv_map->region_cache);\n\tlist_add(&rg->link, &resv_map->region_cache);\n\tresv_map->region_cache_count = 1;\n\n\treturn resv_map;\n}\n\nvoid resv_map_release(struct kref *ref)\n{\n\tstruct resv_map *resv_map = container_of(ref, struct resv_map, refs);\n\tstruct list_head *head = &resv_map->region_cache;\n\tstruct file_region *rg, *trg;\n\n\t/* Clear out any active regions before we release the map. */\n\tregion_del(resv_map, 0, LONG_MAX);\n\n\t/* ... and any entries left in the cache */\n\tlist_for_each_entry_safe(rg, trg, head, link) {\n\t\tlist_del(&rg->link);\n\t\tkfree(rg);\n\t}\n\n\tVM_BUG_ON(resv_map->adds_in_progress);\n\n\tkfree(resv_map);\n}\n\nstatic inline struct resv_map *inode_resv_map(struct inode *inode)\n{\n\treturn inode->i_mapping->private_data;\n}\n\nstatic struct resv_map *vma_resv_map(struct vm_area_struct *vma)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\tstruct address_space *mapping = vma->vm_file->f_mapping;\n\t\tstruct inode *inode = mapping->host;\n\n\t\treturn inode_resv_map(inode);\n\n\t} else {\n\t\treturn (struct resv_map *)(get_vma_private_data(vma) &\n\t\t\t\t\t\t\t~HPAGE_RESV_MASK);\n\t}\n}\n\nstatic void set_vma_resv_map(struct vm_area_struct *vma, struct resv_map *map)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\tVM_BUG_ON_VMA(vma->vm_flags & VM_MAYSHARE, vma);\n\n\tset_vma_private_data(vma, (get_vma_private_data(vma) &\n\t\t\t\tHPAGE_RESV_MASK) | (unsigned long)map);\n}\n\nstatic void set_vma_resv_flags(struct vm_area_struct *vma, unsigned long flags)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\tVM_BUG_ON_VMA(vma->vm_flags & VM_MAYSHARE, vma);\n\n\tset_vma_private_data(vma, get_vma_private_data(vma) | flags);\n}\n\nstatic int is_vma_resv_set(struct vm_area_struct *vma, unsigned long flag)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\n\treturn (get_vma_private_data(vma) & flag) != 0;\n}\n\n/* Reset counters to 0 and clear all HPAGE_RESV_* flags */\nvoid reset_vma_resv_huge_pages(struct vm_area_struct *vma)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\tif (!(vma->vm_flags & VM_MAYSHARE))\n\t\tvma->vm_private_data = (void *)0;\n}\n\n/* Returns true if the VMA has associated reserve pages */\nstatic bool vma_has_reserves(struct vm_area_struct *vma, long chg)\n{\n\tif (vma->vm_flags & VM_NORESERVE) {\n\t\t/*\n\t\t * This address is already reserved by other process(chg == 0),\n\t\t * so, we should decrement reserved count. Without decrementing,\n\t\t * reserve count remains after releasing inode, because this\n\t\t * allocated page will go into page cache and is regarded as\n\t\t * coming from reserved pool in releasing step.  Currently, we\n\t\t * don't have any other solution to deal with this situation\n\t\t * properly, so add work-around here.\n\t\t */\n\t\tif (vma->vm_flags & VM_MAYSHARE && chg == 0)\n\t\t\treturn true;\n\t\telse\n\t\t\treturn false;\n\t}\n\n\t/* Shared mappings always use reserves */\n\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\t/*\n\t\t * We know VM_NORESERVE is not set.  Therefore, there SHOULD\n\t\t * be a region map for all pages.  The only situation where\n\t\t * there is no region map is if a hole was punched via\n\t\t * fallocate.  In this case, there really are no reverves to\n\t\t * use.  This situation is indicated if chg != 0.\n\t\t */\n\t\tif (chg)\n\t\t\treturn false;\n\t\telse\n\t\t\treturn true;\n\t}\n\n\t/*\n\t * Only the process that called mmap() has reserves for\n\t * private mappings.\n\t */\n\tif (is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {\n\t\t/*\n\t\t * Like the shared case above, a hole punch or truncate\n\t\t * could have been performed on the private mapping.\n\t\t * Examine the value of chg to determine if reserves\n\t\t * actually exist or were previously consumed.\n\t\t * Very Subtle - The value of chg comes from a previous\n\t\t * call to vma_needs_reserves().  The reserve map for\n\t\t * private mappings has different (opposite) semantics\n\t\t * than that of shared mappings.  vma_needs_reserves()\n\t\t * has already taken this difference in semantics into\n\t\t * account.  Therefore, the meaning of chg is the same\n\t\t * as in the shared case above.  Code could easily be\n\t\t * combined, but keeping it separate draws attention to\n\t\t * subtle differences.\n\t\t */\n\t\tif (chg)\n\t\t\treturn false;\n\t\telse\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void enqueue_huge_page(struct hstate *h, struct page *page)\n{\n\tint nid = page_to_nid(page);\n\tlist_move(&page->lru, &h->hugepage_freelists[nid]);\n\th->free_huge_pages++;\n\th->free_huge_pages_node[nid]++;\n}\n\nstatic struct page *dequeue_huge_page_node_exact(struct hstate *h, int nid)\n{\n\tstruct page *page;\n\n\tlist_for_each_entry(page, &h->hugepage_freelists[nid], lru)\n\t\tif (!PageHWPoison(page))\n\t\t\tbreak;\n\t/*\n\t * if 'non-isolated free hugepage' not found on the list,\n\t * the allocation fails.\n\t */\n\tif (&h->hugepage_freelists[nid] == &page->lru)\n\t\treturn NULL;\n\tlist_move(&page->lru, &h->hugepage_activelist);\n\tset_page_refcounted(page);\n\th->free_huge_pages--;\n\th->free_huge_pages_node[nid]--;\n\treturn page;\n}\n\nstatic struct page *dequeue_huge_page_nodemask(struct hstate *h, gfp_t gfp_mask, int nid,\n\t\tnodemask_t *nmask)\n{\n\tunsigned int cpuset_mems_cookie;\n\tstruct zonelist *zonelist;\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\tint node = -1;\n\n\tzonelist = node_zonelist(nid, gfp_mask);\n\nretry_cpuset:\n\tcpuset_mems_cookie = read_mems_allowed_begin();\n\tfor_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {\n\t\tstruct page *page;\n\n\t\tif (!cpuset_zone_allowed(zone, gfp_mask))\n\t\t\tcontinue;\n\t\t/*\n\t\t * no need to ask again on the same node. Pool is node rather than\n\t\t * zone aware\n\t\t */\n\t\tif (zone_to_nid(zone) == node)\n\t\t\tcontinue;\n\t\tnode = zone_to_nid(zone);\n\n\t\tpage = dequeue_huge_page_node_exact(h, node);\n\t\tif (page)\n\t\t\treturn page;\n\t}\n\tif (unlikely(read_mems_allowed_retry(cpuset_mems_cookie)))\n\t\tgoto retry_cpuset;\n\n\treturn NULL;\n}\n\n/* Movability of hugepages depends on migration support. */\nstatic inline gfp_t htlb_alloc_mask(struct hstate *h)\n{\n\tif (hugepages_treat_as_movable || hugepage_migration_supported(h))\n\t\treturn GFP_HIGHUSER_MOVABLE;\n\telse\n\t\treturn GFP_HIGHUSER;\n}\n\nstatic struct page *dequeue_huge_page_vma(struct hstate *h,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tunsigned long address, int avoid_reserve,\n\t\t\t\tlong chg)\n{\n\tstruct page *page;\n\tstruct mempolicy *mpol;\n\tgfp_t gfp_mask;\n\tnodemask_t *nodemask;\n\tint nid;\n\n\t/*\n\t * A child process with MAP_PRIVATE mappings created by their parent\n\t * have no page reserves. This check ensures that reservations are\n\t * not \"stolen\". The child may still get SIGKILLed\n\t */\n\tif (!vma_has_reserves(vma, chg) &&\n\t\t\th->free_huge_pages - h->resv_huge_pages == 0)\n\t\tgoto err;\n\n\t/* If reserves cannot be used, ensure enough pages are in the pool */\n\tif (avoid_reserve && h->free_huge_pages - h->resv_huge_pages == 0)\n\t\tgoto err;\n\n\tgfp_mask = htlb_alloc_mask(h);\n\tnid = huge_node(vma, address, gfp_mask, &mpol, &nodemask);\n\tpage = dequeue_huge_page_nodemask(h, gfp_mask, nid, nodemask);\n\tif (page && !avoid_reserve && vma_has_reserves(vma, chg)) {\n\t\tSetPagePrivate(page);\n\t\th->resv_huge_pages--;\n\t}\n\n\tmpol_cond_put(mpol);\n\treturn page;\n\nerr:\n\treturn NULL;\n}\n\n/*\n * common helper functions for hstate_next_node_to_{alloc|free}.\n * We may have allocated or freed a huge page based on a different\n * nodes_allowed previously, so h->next_node_to_{alloc|free} might\n * be outside of *nodes_allowed.  Ensure that we use an allowed\n * node for alloc or free.\n */\nstatic int next_node_allowed(int nid, nodemask_t *nodes_allowed)\n{\n\tnid = next_node_in(nid, *nodes_allowed);\n\tVM_BUG_ON(nid >= MAX_NUMNODES);\n\n\treturn nid;\n}\n\nstatic int get_valid_node_allowed(int nid, nodemask_t *nodes_allowed)\n{\n\tif (!node_isset(nid, *nodes_allowed))\n\t\tnid = next_node_allowed(nid, nodes_allowed);\n\treturn nid;\n}\n\n/*\n * returns the previously saved node [\"this node\"] from which to\n * allocate a persistent huge page for the pool and advance the\n * next node from which to allocate, handling wrap at end of node\n * mask.\n */\nstatic int hstate_next_node_to_alloc(struct hstate *h,\n\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n\tint nid;\n\n\tVM_BUG_ON(!nodes_allowed);\n\n\tnid = get_valid_node_allowed(h->next_nid_to_alloc, nodes_allowed);\n\th->next_nid_to_alloc = next_node_allowed(nid, nodes_allowed);\n\n\treturn nid;\n}\n\n/*\n * helper for free_pool_huge_page() - return the previously saved\n * node [\"this node\"] from which to free a huge page.  Advance the\n * next node id whether or not we find a free huge page to free so\n * that the next attempt to free addresses the next node.\n */\nstatic int hstate_next_node_to_free(struct hstate *h, nodemask_t *nodes_allowed)\n{\n\tint nid;\n\n\tVM_BUG_ON(!nodes_allowed);\n\n\tnid = get_valid_node_allowed(h->next_nid_to_free, nodes_allowed);\n\th->next_nid_to_free = next_node_allowed(nid, nodes_allowed);\n\n\treturn nid;\n}\n\n#define for_each_node_mask_to_alloc(hs, nr_nodes, node, mask)\t\t\\\n\tfor (nr_nodes = nodes_weight(*mask);\t\t\t\t\\\n\t\tnr_nodes > 0 &&\t\t\t\t\t\t\\\n\t\t((node = hstate_next_node_to_alloc(hs, mask)) || 1);\t\\\n\t\tnr_nodes--)\n\n#define for_each_node_mask_to_free(hs, nr_nodes, node, mask)\t\t\\\n\tfor (nr_nodes = nodes_weight(*mask);\t\t\t\t\\\n\t\tnr_nodes > 0 &&\t\t\t\t\t\t\\\n\t\t((node = hstate_next_node_to_free(hs, mask)) || 1);\t\\\n\t\tnr_nodes--)\n\n#ifdef CONFIG_ARCH_HAS_GIGANTIC_PAGE\nstatic void destroy_compound_gigantic_page(struct page *page,\n\t\t\t\t\tunsigned int order)\n{\n\tint i;\n\tint nr_pages = 1 << order;\n\tstruct page *p = page + 1;\n\n\tatomic_set(compound_mapcount_ptr(page), 0);\n\tfor (i = 1; i < nr_pages; i++, p = mem_map_next(p, page, i)) {\n\t\tclear_compound_head(p);\n\t\tset_page_refcounted(p);\n\t}\n\n\tset_compound_order(page, 0);\n\t__ClearPageHead(page);\n}\n\nstatic void free_gigantic_page(struct page *page, unsigned int order)\n{\n\tfree_contig_range(page_to_pfn(page), 1 << order);\n}\n\nstatic int __alloc_gigantic_page(unsigned long start_pfn,\n\t\t\t\tunsigned long nr_pages)\n{\n\tunsigned long end_pfn = start_pfn + nr_pages;\n\treturn alloc_contig_range(start_pfn, end_pfn, MIGRATE_MOVABLE,\n\t\t\t\t  GFP_KERNEL);\n}\n\nstatic bool pfn_range_valid_gigantic(struct zone *z,\n\t\t\tunsigned long start_pfn, unsigned long nr_pages)\n{\n\tunsigned long i, end_pfn = start_pfn + nr_pages;\n\tstruct page *page;\n\n\tfor (i = start_pfn; i < end_pfn; i++) {\n\t\tif (!pfn_valid(i))\n\t\t\treturn false;\n\n\t\tpage = pfn_to_page(i);\n\n\t\tif (page_zone(page) != z)\n\t\t\treturn false;\n\n\t\tif (PageReserved(page))\n\t\t\treturn false;\n\n\t\tif (page_count(page) > 0)\n\t\t\treturn false;\n\n\t\tif (PageHuge(page))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool zone_spans_last_pfn(const struct zone *zone,\n\t\t\tunsigned long start_pfn, unsigned long nr_pages)\n{\n\tunsigned long last_pfn = start_pfn + nr_pages - 1;\n\treturn zone_spans_pfn(zone, last_pfn);\n}\n\nstatic struct page *alloc_gigantic_page(int nid, unsigned int order)\n{\n\tunsigned long nr_pages = 1 << order;\n\tunsigned long ret, pfn, flags;\n\tstruct zone *z;\n\n\tz = NODE_DATA(nid)->node_zones;\n\tfor (; z - NODE_DATA(nid)->node_zones < MAX_NR_ZONES; z++) {\n\t\tspin_lock_irqsave(&z->lock, flags);\n\n\t\tpfn = ALIGN(z->zone_start_pfn, nr_pages);\n\t\twhile (zone_spans_last_pfn(z, pfn, nr_pages)) {\n\t\t\tif (pfn_range_valid_gigantic(z, pfn, nr_pages)) {\n\t\t\t\t/*\n\t\t\t\t * We release the zone lock here because\n\t\t\t\t * alloc_contig_range() will also lock the zone\n\t\t\t\t * at some point. If there's an allocation\n\t\t\t\t * spinning on this lock, it may win the race\n\t\t\t\t * and cause alloc_contig_range() to fail...\n\t\t\t\t */\n\t\t\t\tspin_unlock_irqrestore(&z->lock, flags);\n\t\t\t\tret = __alloc_gigantic_page(pfn, nr_pages);\n\t\t\t\tif (!ret)\n\t\t\t\t\treturn pfn_to_page(pfn);\n\t\t\t\tspin_lock_irqsave(&z->lock, flags);\n\t\t\t}\n\t\t\tpfn += nr_pages;\n\t\t}\n\n\t\tspin_unlock_irqrestore(&z->lock, flags);\n\t}\n\n\treturn NULL;\n}\n\nstatic void prep_new_huge_page(struct hstate *h, struct page *page, int nid);\nstatic void prep_compound_gigantic_page(struct page *page, unsigned int order);\n\nstatic struct page *alloc_fresh_gigantic_page_node(struct hstate *h, int nid)\n{\n\tstruct page *page;\n\n\tpage = alloc_gigantic_page(nid, huge_page_order(h));\n\tif (page) {\n\t\tprep_compound_gigantic_page(page, huge_page_order(h));\n\t\tprep_new_huge_page(h, page, nid);\n\t}\n\n\treturn page;\n}\n\nstatic int alloc_fresh_gigantic_page(struct hstate *h,\n\t\t\t\tnodemask_t *nodes_allowed)\n{\n\tstruct page *page = NULL;\n\tint nr_nodes, node;\n\n\tfor_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {\n\t\tpage = alloc_fresh_gigantic_page_node(h, node);\n\t\tif (page)\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n#else /* !CONFIG_ARCH_HAS_GIGANTIC_PAGE */\nstatic inline bool gigantic_page_supported(void) { return false; }\nstatic inline void free_gigantic_page(struct page *page, unsigned int order) { }\nstatic inline void destroy_compound_gigantic_page(struct page *page,\n\t\t\t\t\t\tunsigned int order) { }\nstatic inline int alloc_fresh_gigantic_page(struct hstate *h,\n\t\t\t\t\tnodemask_t *nodes_allowed) { return 0; }\n#endif\n\nstatic void update_and_free_page(struct hstate *h, struct page *page)\n{\n\tint i;\n\n\tif (hstate_is_gigantic(h) && !gigantic_page_supported())\n\t\treturn;\n\n\th->nr_huge_pages--;\n\th->nr_huge_pages_node[page_to_nid(page)]--;\n\tfor (i = 0; i < pages_per_huge_page(h); i++) {\n\t\tpage[i].flags &= ~(1 << PG_locked | 1 << PG_error |\n\t\t\t\t1 << PG_referenced | 1 << PG_dirty |\n\t\t\t\t1 << PG_active | 1 << PG_private |\n\t\t\t\t1 << PG_writeback);\n\t}\n\tVM_BUG_ON_PAGE(hugetlb_cgroup_from_page(page), page);\n\tset_compound_page_dtor(page, NULL_COMPOUND_DTOR);\n\tset_page_refcounted(page);\n\tif (hstate_is_gigantic(h)) {\n\t\tdestroy_compound_gigantic_page(page, huge_page_order(h));\n\t\tfree_gigantic_page(page, huge_page_order(h));\n\t} else {\n\t\t__free_pages(page, huge_page_order(h));\n\t}\n}\n\nstruct hstate *size_to_hstate(unsigned long size)\n{\n\tstruct hstate *h;\n\n\tfor_each_hstate(h) {\n\t\tif (huge_page_size(h) == size)\n\t\t\treturn h;\n\t}\n\treturn NULL;\n}\n\n/*\n * Test to determine whether the hugepage is \"active/in-use\" (i.e. being linked\n * to hstate->hugepage_activelist.)\n *\n * This function can be called for tail pages, but never returns true for them.\n */\nbool page_huge_active(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!PageHuge(page), page);\n\treturn PageHead(page) && PagePrivate(&page[1]);\n}\n\n/* never called for tail page */\nstatic void set_page_huge_active(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!PageHeadHuge(page), page);\n\tSetPagePrivate(&page[1]);\n}\n\nstatic void clear_page_huge_active(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!PageHeadHuge(page), page);\n\tClearPagePrivate(&page[1]);\n}\n\nvoid free_huge_page(struct page *page)\n{\n\t/*\n\t * Can't pass hstate in here because it is called from the\n\t * compound page destructor.\n\t */\n\tstruct hstate *h = page_hstate(page);\n\tint nid = page_to_nid(page);\n\tstruct hugepage_subpool *spool =\n\t\t(struct hugepage_subpool *)page_private(page);\n\tbool restore_reserve;\n\n\tset_page_private(page, 0);\n\tpage->mapping = NULL;\n\tVM_BUG_ON_PAGE(page_count(page), page);\n\tVM_BUG_ON_PAGE(page_mapcount(page), page);\n\trestore_reserve = PagePrivate(page);\n\tClearPagePrivate(page);\n\n\t/*\n\t * A return code of zero implies that the subpool will be under its\n\t * minimum size if the reservation is not restored after page is free.\n\t * Therefore, force restore_reserve operation.\n\t */\n\tif (hugepage_subpool_put_pages(spool, 1) == 0)\n\t\trestore_reserve = true;\n\n\tspin_lock(&hugetlb_lock);\n\tclear_page_huge_active(page);\n\thugetlb_cgroup_uncharge_page(hstate_index(h),\n\t\t\t\t     pages_per_huge_page(h), page);\n\tif (restore_reserve)\n\t\th->resv_huge_pages++;\n\n\tif (h->surplus_huge_pages_node[nid]) {\n\t\t/* remove the page from active list */\n\t\tlist_del(&page->lru);\n\t\tupdate_and_free_page(h, page);\n\t\th->surplus_huge_pages--;\n\t\th->surplus_huge_pages_node[nid]--;\n\t} else {\n\t\tarch_clear_hugepage_flags(page);\n\t\tenqueue_huge_page(h, page);\n\t}\n\tspin_unlock(&hugetlb_lock);\n}\n\nstatic void prep_new_huge_page(struct hstate *h, struct page *page, int nid)\n{\n\tINIT_LIST_HEAD(&page->lru);\n\tset_compound_page_dtor(page, HUGETLB_PAGE_DTOR);\n\tspin_lock(&hugetlb_lock);\n\tset_hugetlb_cgroup(page, NULL);\n\th->nr_huge_pages++;\n\th->nr_huge_pages_node[nid]++;\n\tspin_unlock(&hugetlb_lock);\n\tput_page(page); /* free it into the hugepage allocator */\n}\n\nstatic void prep_compound_gigantic_page(struct page *page, unsigned int order)\n{\n\tint i;\n\tint nr_pages = 1 << order;\n\tstruct page *p = page + 1;\n\n\t/* we rely on prep_new_huge_page to set the destructor */\n\tset_compound_order(page, order);\n\t__ClearPageReserved(page);\n\t__SetPageHead(page);\n\tfor (i = 1; i < nr_pages; i++, p = mem_map_next(p, page, i)) {\n\t\t/*\n\t\t * For gigantic hugepages allocated through bootmem at\n\t\t * boot, it's safer to be consistent with the not-gigantic\n\t\t * hugepages and clear the PG_reserved bit from all tail pages\n\t\t * too.  Otherwse drivers using get_user_pages() to access tail\n\t\t * pages may get the reference counting wrong if they see\n\t\t * PG_reserved set on a tail page (despite the head page not\n\t\t * having PG_reserved set).  Enforcing this consistency between\n\t\t * head and tail pages allows drivers to optimize away a check\n\t\t * on the head page when they need know if put_page() is needed\n\t\t * after get_user_pages().\n\t\t */\n\t\t__ClearPageReserved(p);\n\t\tset_page_count(p, 0);\n\t\tset_compound_head(p, page);\n\t}\n\tatomic_set(compound_mapcount_ptr(page), -1);\n}\n\n/*\n * PageHuge() only returns true for hugetlbfs pages, but not for normal or\n * transparent huge pages.  See the PageTransHuge() documentation for more\n * details.\n */\nint PageHuge(struct page *page)\n{\n\tif (!PageCompound(page))\n\t\treturn 0;\n\n\tpage = compound_head(page);\n\treturn page[1].compound_dtor == HUGETLB_PAGE_DTOR;\n}\nEXPORT_SYMBOL_GPL(PageHuge);\n\n/*\n * PageHeadHuge() only returns true for hugetlbfs head page, but not for\n * normal or transparent huge pages.\n */\nint PageHeadHuge(struct page *page_head)\n{\n\tif (!PageHead(page_head))\n\t\treturn 0;\n\n\treturn get_compound_page_dtor(page_head) == free_huge_page;\n}\n\npgoff_t __basepage_index(struct page *page)\n{\n\tstruct page *page_head = compound_head(page);\n\tpgoff_t index = page_index(page_head);\n\tunsigned long compound_idx;\n\n\tif (!PageHuge(page_head))\n\t\treturn page_index(page);\n\n\tif (compound_order(page_head) >= MAX_ORDER)\n\t\tcompound_idx = page_to_pfn(page) - page_to_pfn(page_head);\n\telse\n\t\tcompound_idx = page - page_head;\n\n\treturn (index << compound_order(page_head)) + compound_idx;\n}\n\nstatic struct page *alloc_fresh_huge_page_node(struct hstate *h, int nid)\n{\n\tstruct page *page;\n\n\tpage = __alloc_pages_node(nid,\n\t\thtlb_alloc_mask(h)|__GFP_COMP|__GFP_THISNODE|\n\t\t\t\t\t\t__GFP_RETRY_MAYFAIL|__GFP_NOWARN,\n\t\thuge_page_order(h));\n\tif (page) {\n\t\tprep_new_huge_page(h, page, nid);\n\t}\n\n\treturn page;\n}\n\nstatic int alloc_fresh_huge_page(struct hstate *h, nodemask_t *nodes_allowed)\n{\n\tstruct page *page;\n\tint nr_nodes, node;\n\tint ret = 0;\n\n\tfor_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {\n\t\tpage = alloc_fresh_huge_page_node(h, node);\n\t\tif (page) {\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (ret)\n\t\tcount_vm_event(HTLB_BUDDY_PGALLOC);\n\telse\n\t\tcount_vm_event(HTLB_BUDDY_PGALLOC_FAIL);\n\n\treturn ret;\n}\n\n/*\n * Free huge page from pool from next node to free.\n * Attempt to keep persistent huge pages more or less\n * balanced over allowed nodes.\n * Called with hugetlb_lock locked.\n */\nstatic int free_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed,\n\t\t\t\t\t\t\t bool acct_surplus)\n{\n\tint nr_nodes, node;\n\tint ret = 0;\n\n\tfor_each_node_mask_to_free(h, nr_nodes, node, nodes_allowed) {\n\t\t/*\n\t\t * If we're returning unused surplus pages, only examine\n\t\t * nodes with surplus pages.\n\t\t */\n\t\tif ((!acct_surplus || h->surplus_huge_pages_node[node]) &&\n\t\t    !list_empty(&h->hugepage_freelists[node])) {\n\t\t\tstruct page *page =\n\t\t\t\tlist_entry(h->hugepage_freelists[node].next,\n\t\t\t\t\t  struct page, lru);\n\t\t\tlist_del(&page->lru);\n\t\t\th->free_huge_pages--;\n\t\t\th->free_huge_pages_node[node]--;\n\t\t\tif (acct_surplus) {\n\t\t\t\th->surplus_huge_pages--;\n\t\t\t\th->surplus_huge_pages_node[node]--;\n\t\t\t}\n\t\t\tupdate_and_free_page(h, page);\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n/*\n * Dissolve a given free hugepage into free buddy pages. This function does\n * nothing for in-use (including surplus) hugepages. Returns -EBUSY if the\n * number of free hugepages would be reduced below the number of reserved\n * hugepages.\n */\nint dissolve_free_huge_page(struct page *page)\n{\n\tint rc = 0;\n\n\tspin_lock(&hugetlb_lock);\n\tif (PageHuge(page) && !page_count(page)) {\n\t\tstruct page *head = compound_head(page);\n\t\tstruct hstate *h = page_hstate(head);\n\t\tint nid = page_to_nid(head);\n\t\tif (h->free_huge_pages - h->resv_huge_pages == 0) {\n\t\t\trc = -EBUSY;\n\t\t\tgoto out;\n\t\t}\n\t\t/*\n\t\t * Move PageHWPoison flag from head page to the raw error page,\n\t\t * which makes any subpages rather than the error page reusable.\n\t\t */\n\t\tif (PageHWPoison(head) && page != head) {\n\t\t\tSetPageHWPoison(page);\n\t\t\tClearPageHWPoison(head);\n\t\t}\n\t\tlist_del(&head->lru);\n\t\th->free_huge_pages--;\n\t\th->free_huge_pages_node[nid]--;\n\t\th->max_huge_pages--;\n\t\tupdate_and_free_page(h, head);\n\t}\nout:\n\tspin_unlock(&hugetlb_lock);\n\treturn rc;\n}\n\n/*\n * Dissolve free hugepages in a given pfn range. Used by memory hotplug to\n * make specified memory blocks removable from the system.\n * Note that this will dissolve a free gigantic hugepage completely, if any\n * part of it lies within the given range.\n * Also note that if dissolve_free_huge_page() returns with an error, all\n * free hugepages that were dissolved before that error are lost.\n */\nint dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tunsigned long pfn;\n\tstruct page *page;\n\tint rc = 0;\n\n\tif (!hugepages_supported())\n\t\treturn rc;\n\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn += 1 << minimum_order) {\n\t\tpage = pfn_to_page(pfn);\n\t\tif (PageHuge(page) && !page_count(page)) {\n\t\t\trc = dissolve_free_huge_page(page);\n\t\t\tif (rc)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn rc;\n}\n\nstatic struct page *__hugetlb_alloc_buddy_huge_page(struct hstate *h,\n\t\tgfp_t gfp_mask, int nid, nodemask_t *nmask)\n{\n\tint order = huge_page_order(h);\n\n\tgfp_mask |= __GFP_COMP|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;\n\tif (nid == NUMA_NO_NODE)\n\t\tnid = numa_mem_id();\n\treturn __alloc_pages_nodemask(gfp_mask, order, nid, nmask);\n}\n\nstatic struct page *__alloc_buddy_huge_page(struct hstate *h, gfp_t gfp_mask,\n\t\tint nid, nodemask_t *nmask)\n{\n\tstruct page *page;\n\tunsigned int r_nid;\n\n\tif (hstate_is_gigantic(h))\n\t\treturn NULL;\n\n\t/*\n\t * Assume we will successfully allocate the surplus page to\n\t * prevent racing processes from causing the surplus to exceed\n\t * overcommit\n\t *\n\t * This however introduces a different race, where a process B\n\t * tries to grow the static hugepage pool while alloc_pages() is\n\t * called by process A. B will only examine the per-node\n\t * counters in determining if surplus huge pages can be\n\t * converted to normal huge pages in adjust_pool_surplus(). A\n\t * won't be able to increment the per-node counter, until the\n\t * lock is dropped by B, but B doesn't drop hugetlb_lock until\n\t * no more huge pages can be converted from surplus to normal\n\t * state (and doesn't try to convert again). Thus, we have a\n\t * case where a surplus huge page exists, the pool is grown, and\n\t * the surplus huge page still exists after, even though it\n\t * should just have been converted to a normal huge page. This\n\t * does not leak memory, though, as the hugepage will be freed\n\t * once it is out of use. It also does not allow the counters to\n\t * go out of whack in adjust_pool_surplus() as we don't modify\n\t * the node values until we've gotten the hugepage and only the\n\t * per-node value is checked there.\n\t */\n\tspin_lock(&hugetlb_lock);\n\tif (h->surplus_huge_pages >= h->nr_overcommit_huge_pages) {\n\t\tspin_unlock(&hugetlb_lock);\n\t\treturn NULL;\n\t} else {\n\t\th->nr_huge_pages++;\n\t\th->surplus_huge_pages++;\n\t}\n\tspin_unlock(&hugetlb_lock);\n\n\tpage = __hugetlb_alloc_buddy_huge_page(h, gfp_mask, nid, nmask);\n\n\tspin_lock(&hugetlb_lock);\n\tif (page) {\n\t\tINIT_LIST_HEAD(&page->lru);\n\t\tr_nid = page_to_nid(page);\n\t\tset_compound_page_dtor(page, HUGETLB_PAGE_DTOR);\n\t\tset_hugetlb_cgroup(page, NULL);\n\t\t/*\n\t\t * We incremented the global counters already\n\t\t */\n\t\th->nr_huge_pages_node[r_nid]++;\n\t\th->surplus_huge_pages_node[r_nid]++;\n\t\t__count_vm_event(HTLB_BUDDY_PGALLOC);\n\t} else {\n\t\th->nr_huge_pages--;\n\t\th->surplus_huge_pages--;\n\t\t__count_vm_event(HTLB_BUDDY_PGALLOC_FAIL);\n\t}\n\tspin_unlock(&hugetlb_lock);\n\n\treturn page;\n}\n\n/*\n * Use the VMA's mpolicy to allocate a huge page from the buddy.\n */\nstatic\nstruct page *__alloc_buddy_huge_page_with_mpol(struct hstate *h,\n\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\tstruct page *page;\n\tstruct mempolicy *mpol;\n\tgfp_t gfp_mask = htlb_alloc_mask(h);\n\tint nid;\n\tnodemask_t *nodemask;\n\n\tnid = huge_node(vma, addr, gfp_mask, &mpol, &nodemask);\n\tpage = __alloc_buddy_huge_page(h, gfp_mask, nid, nodemask);\n\tmpol_cond_put(mpol);\n\n\treturn page;\n}\n\n/*\n * This allocation function is useful in the context where vma is irrelevant.\n * E.g. soft-offlining uses this function because it only cares physical\n * address of error page.\n */\nstruct page *alloc_huge_page_node(struct hstate *h, int nid)\n{\n\tgfp_t gfp_mask = htlb_alloc_mask(h);\n\tstruct page *page = NULL;\n\n\tif (nid != NUMA_NO_NODE)\n\t\tgfp_mask |= __GFP_THISNODE;\n\n\tspin_lock(&hugetlb_lock);\n\tif (h->free_huge_pages - h->resv_huge_pages > 0)\n\t\tpage = dequeue_huge_page_nodemask(h, gfp_mask, nid, NULL);\n\tspin_unlock(&hugetlb_lock);\n\n\tif (!page)\n\t\tpage = __alloc_buddy_huge_page(h, gfp_mask, nid, NULL);\n\n\treturn page;\n}\n\n\nstruct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,\n\t\tnodemask_t *nmask)\n{\n\tgfp_t gfp_mask = htlb_alloc_mask(h);\n\n\tspin_lock(&hugetlb_lock);\n\tif (h->free_huge_pages - h->resv_huge_pages > 0) {\n\t\tstruct page *page;\n\n\t\tpage = dequeue_huge_page_nodemask(h, gfp_mask, preferred_nid, nmask);\n\t\tif (page) {\n\t\t\tspin_unlock(&hugetlb_lock);\n\t\t\treturn page;\n\t\t}\n\t}\n\tspin_unlock(&hugetlb_lock);\n\n\t/* No reservations, try to overcommit */\n\n\treturn __alloc_buddy_huge_page(h, gfp_mask, preferred_nid, nmask);\n}\n\n/*\n * Increase the hugetlb pool such that it can accommodate a reservation\n * of size 'delta'.\n */\nstatic int gather_surplus_pages(struct hstate *h, int delta)\n{\n\tstruct list_head surplus_list;\n\tstruct page *page, *tmp;\n\tint ret, i;\n\tint needed, allocated;\n\tbool alloc_ok = true;\n\n\tneeded = (h->resv_huge_pages + delta) - h->free_huge_pages;\n\tif (needed <= 0) {\n\t\th->resv_huge_pages += delta;\n\t\treturn 0;\n\t}\n\n\tallocated = 0;\n\tINIT_LIST_HEAD(&surplus_list);\n\n\tret = -ENOMEM;\nretry:\n\tspin_unlock(&hugetlb_lock);\n\tfor (i = 0; i < needed; i++) {\n\t\tpage = __alloc_buddy_huge_page(h, htlb_alloc_mask(h),\n\t\t\t\tNUMA_NO_NODE, NULL);\n\t\tif (!page) {\n\t\t\talloc_ok = false;\n\t\t\tbreak;\n\t\t}\n\t\tlist_add(&page->lru, &surplus_list);\n\t\tcond_resched();\n\t}\n\tallocated += i;\n\n\t/*\n\t * After retaking hugetlb_lock, we need to recalculate 'needed'\n\t * because either resv_huge_pages or free_huge_pages may have changed.\n\t */\n\tspin_lock(&hugetlb_lock);\n\tneeded = (h->resv_huge_pages + delta) -\n\t\t\t(h->free_huge_pages + allocated);\n\tif (needed > 0) {\n\t\tif (alloc_ok)\n\t\t\tgoto retry;\n\t\t/*\n\t\t * We were not able to allocate enough pages to\n\t\t * satisfy the entire reservation so we free what\n\t\t * we've allocated so far.\n\t\t */\n\t\tgoto free;\n\t}\n\t/*\n\t * The surplus_list now contains _at_least_ the number of extra pages\n\t * needed to accommodate the reservation.  Add the appropriate number\n\t * of pages to the hugetlb pool and free the extras back to the buddy\n\t * allocator.  Commit the entire reservation here to prevent another\n\t * process from stealing the pages as they are added to the pool but\n\t * before they are reserved.\n\t */\n\tneeded += allocated;\n\th->resv_huge_pages += delta;\n\tret = 0;\n\n\t/* Free the needed pages to the hugetlb pool */\n\tlist_for_each_entry_safe(page, tmp, &surplus_list, lru) {\n\t\tif ((--needed) < 0)\n\t\t\tbreak;\n\t\t/*\n\t\t * This page is now managed by the hugetlb allocator and has\n\t\t * no users -- drop the buddy allocator's reference.\n\t\t */\n\t\tput_page_testzero(page);\n\t\tVM_BUG_ON_PAGE(page_count(page), page);\n\t\tenqueue_huge_page(h, page);\n\t}\nfree:\n\tspin_unlock(&hugetlb_lock);\n\n\t/* Free unnecessary surplus pages to the buddy allocator */\n\tlist_for_each_entry_safe(page, tmp, &surplus_list, lru)\n\t\tput_page(page);\n\tspin_lock(&hugetlb_lock);\n\n\treturn ret;\n}\n\n/*\n * This routine has two main purposes:\n * 1) Decrement the reservation count (resv_huge_pages) by the value passed\n *    in unused_resv_pages.  This corresponds to the prior adjustments made\n *    to the associated reservation map.\n * 2) Free any unused surplus pages that may have been allocated to satisfy\n *    the reservation.  As many as unused_resv_pages may be freed.\n *\n * Called with hugetlb_lock held.  However, the lock could be dropped (and\n * reacquired) during calls to cond_resched_lock.  Whenever dropping the lock,\n * we must make sure nobody else can claim pages we are in the process of\n * freeing.  Do this by ensuring resv_huge_page always is greater than the\n * number of huge pages we plan to free when dropping the lock.\n */\nstatic void return_unused_surplus_pages(struct hstate *h,\n\t\t\t\t\tunsigned long unused_resv_pages)\n{\n\tunsigned long nr_pages;\n\n\t/* Cannot return gigantic pages currently */\n\tif (hstate_is_gigantic(h))\n\t\tgoto out;\n\n\t/*\n\t * Part (or even all) of the reservation could have been backed\n\t * by pre-allocated pages. Only free surplus pages.\n\t */\n\tnr_pages = min(unused_resv_pages, h->surplus_huge_pages);\n\n\t/*\n\t * We want to release as many surplus pages as possible, spread\n\t * evenly across all nodes with memory. Iterate across these nodes\n\t * until we can no longer free unreserved surplus pages. This occurs\n\t * when the nodes with surplus pages have no free pages.\n\t * free_pool_huge_page() will balance the the freed pages across the\n\t * on-line nodes with memory and will handle the hstate accounting.\n\t *\n\t * Note that we decrement resv_huge_pages as we free the pages.  If\n\t * we drop the lock, resv_huge_pages will still be sufficiently large\n\t * to cover subsequent pages we may free.\n\t */\n\twhile (nr_pages--) {\n\t\th->resv_huge_pages--;\n\t\tunused_resv_pages--;\n\t\tif (!free_pool_huge_page(h, &node_states[N_MEMORY], 1))\n\t\t\tgoto out;\n\t\tcond_resched_lock(&hugetlb_lock);\n\t}\n\nout:\n\t/* Fully uncommit the reservation */\n\th->resv_huge_pages -= unused_resv_pages;\n}\n\n\n/*\n * vma_needs_reservation, vma_commit_reservation and vma_end_reservation\n * are used by the huge page allocation routines to manage reservations.\n *\n * vma_needs_reservation is called to determine if the huge page at addr\n * within the vma has an associated reservation.  If a reservation is\n * needed, the value 1 is returned.  The caller is then responsible for\n * managing the global reservation and subpool usage counts.  After\n * the huge page has been allocated, vma_commit_reservation is called\n * to add the page to the reservation map.  If the page allocation fails,\n * the reservation must be ended instead of committed.  vma_end_reservation\n * is called in such cases.\n *\n * In the normal case, vma_commit_reservation returns the same value\n * as the preceding vma_needs_reservation call.  The only time this\n * is not the case is if a reserve map was changed between calls.  It\n * is the responsibility of the caller to notice the difference and\n * take appropriate action.\n *\n * vma_add_reservation is used in error paths where a reservation must\n * be restored when a newly allocated huge page must be freed.  It is\n * to be called after calling vma_needs_reservation to determine if a\n * reservation exists.\n */\nenum vma_resv_mode {\n\tVMA_NEEDS_RESV,\n\tVMA_COMMIT_RESV,\n\tVMA_END_RESV,\n\tVMA_ADD_RESV,\n};\nstatic long __vma_reservation_common(struct hstate *h,\n\t\t\t\tstruct vm_area_struct *vma, unsigned long addr,\n\t\t\t\tenum vma_resv_mode mode)\n{\n\tstruct resv_map *resv;\n\tpgoff_t idx;\n\tlong ret;\n\n\tresv = vma_resv_map(vma);\n\tif (!resv)\n\t\treturn 1;\n\n\tidx = vma_hugecache_offset(h, vma, addr);\n\tswitch (mode) {\n\tcase VMA_NEEDS_RESV:\n\t\tret = region_chg(resv, idx, idx + 1);\n\t\tbreak;\n\tcase VMA_COMMIT_RESV:\n\t\tret = region_add(resv, idx, idx + 1);\n\t\tbreak;\n\tcase VMA_END_RESV:\n\t\tregion_abort(resv, idx, idx + 1);\n\t\tret = 0;\n\t\tbreak;\n\tcase VMA_ADD_RESV:\n\t\tif (vma->vm_flags & VM_MAYSHARE)\n\t\t\tret = region_add(resv, idx, idx + 1);\n\t\telse {\n\t\t\tregion_abort(resv, idx, idx + 1);\n\t\t\tret = region_del(resv, idx, idx + 1);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\treturn ret;\n\telse if (is_vma_resv_set(vma, HPAGE_RESV_OWNER) && ret >= 0) {\n\t\t/*\n\t\t * In most cases, reserves always exist for private mappings.\n\t\t * However, a file associated with mapping could have been\n\t\t * hole punched or truncated after reserves were consumed.\n\t\t * As subsequent fault on such a range will not use reserves.\n\t\t * Subtle - The reserve map for private mappings has the\n\t\t * opposite meaning than that of shared mappings.  If NO\n\t\t * entry is in the reserve map, it means a reservation exists.\n\t\t * If an entry exists in the reserve map, it means the\n\t\t * reservation has already been consumed.  As a result, the\n\t\t * return value of this routine is the opposite of the\n\t\t * value returned from reserve map manipulation routines above.\n\t\t */\n\t\tif (ret)\n\t\t\treturn 0;\n\t\telse\n\t\t\treturn 1;\n\t}\n\telse\n\t\treturn ret < 0 ? ret : 0;\n}\n\nstatic long vma_needs_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\treturn __vma_reservation_common(h, vma, addr, VMA_NEEDS_RESV);\n}\n\nstatic long vma_commit_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\treturn __vma_reservation_common(h, vma, addr, VMA_COMMIT_RESV);\n}\n\nstatic void vma_end_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\t(void)__vma_reservation_common(h, vma, addr, VMA_END_RESV);\n}\n\nstatic long vma_add_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\treturn __vma_reservation_common(h, vma, addr, VMA_ADD_RESV);\n}\n\n/*\n * This routine is called to restore a reservation on error paths.  In the\n * specific error paths, a huge page was allocated (via alloc_huge_page)\n * and is about to be freed.  If a reservation for the page existed,\n * alloc_huge_page would have consumed the reservation and set PagePrivate\n * in the newly allocated page.  When the page is freed via free_huge_page,\n * the global reservation count will be incremented if PagePrivate is set.\n * However, free_huge_page can not adjust the reserve map.  Adjust the\n * reserve map here to be consistent with global reserve count adjustments\n * to be made by free_huge_page.\n */\nstatic void restore_reserve_on_error(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address,\n\t\t\tstruct page *page)\n{\n\tif (unlikely(PagePrivate(page))) {\n\t\tlong rc = vma_needs_reservation(h, vma, address);\n\n\t\tif (unlikely(rc < 0)) {\n\t\t\t/*\n\t\t\t * Rare out of memory condition in reserve map\n\t\t\t * manipulation.  Clear PagePrivate so that\n\t\t\t * global reserve count will not be incremented\n\t\t\t * by free_huge_page.  This will make it appear\n\t\t\t * as though the reservation for this page was\n\t\t\t * consumed.  This may prevent the task from\n\t\t\t * faulting in the page at a later time.  This\n\t\t\t * is better than inconsistent global huge page\n\t\t\t * accounting of reserve counts.\n\t\t\t */\n\t\t\tClearPagePrivate(page);\n\t\t} else if (rc) {\n\t\t\trc = vma_add_reservation(h, vma, address);\n\t\t\tif (unlikely(rc < 0))\n\t\t\t\t/*\n\t\t\t\t * See above comment about rare out of\n\t\t\t\t * memory condition.\n\t\t\t\t */\n\t\t\t\tClearPagePrivate(page);\n\t\t} else\n\t\t\tvma_end_reservation(h, vma, address);\n\t}\n}\n\nstruct page *alloc_huge_page(struct vm_area_struct *vma,\n\t\t\t\t    unsigned long addr, int avoid_reserve)\n{\n\tstruct hugepage_subpool *spool = subpool_vma(vma);\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct page *page;\n\tlong map_chg, map_commit;\n\tlong gbl_chg;\n\tint ret, idx;\n\tstruct hugetlb_cgroup *h_cg;\n\n\tidx = hstate_index(h);\n\t/*\n\t * Examine the region/reserve map to determine if the process\n\t * has a reservation for the page to be allocated.  A return\n\t * code of zero indicates a reservation exists (no change).\n\t */\n\tmap_chg = gbl_chg = vma_needs_reservation(h, vma, addr);\n\tif (map_chg < 0)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/*\n\t * Processes that did not create the mapping will have no\n\t * reserves as indicated by the region/reserve map. Check\n\t * that the allocation will not exceed the subpool limit.\n\t * Allocations for MAP_NORESERVE mappings also need to be\n\t * checked against any subpool limit.\n\t */\n\tif (map_chg || avoid_reserve) {\n\t\tgbl_chg = hugepage_subpool_get_pages(spool, 1);\n\t\tif (gbl_chg < 0) {\n\t\t\tvma_end_reservation(h, vma, addr);\n\t\t\treturn ERR_PTR(-ENOSPC);\n\t\t}\n\n\t\t/*\n\t\t * Even though there was no reservation in the region/reserve\n\t\t * map, there could be reservations associated with the\n\t\t * subpool that can be used.  This would be indicated if the\n\t\t * return value of hugepage_subpool_get_pages() is zero.\n\t\t * However, if avoid_reserve is specified we still avoid even\n\t\t * the subpool reservations.\n\t\t */\n\t\tif (avoid_reserve)\n\t\t\tgbl_chg = 1;\n\t}\n\n\tret = hugetlb_cgroup_charge_cgroup(idx, pages_per_huge_page(h), &h_cg);\n\tif (ret)\n\t\tgoto out_subpool_put;\n\n\tspin_lock(&hugetlb_lock);\n\t/*\n\t * glb_chg is passed to indicate whether or not a page must be taken\n\t * from the global free pool (global change).  gbl_chg == 0 indicates\n\t * a reservation exists for the allocation.\n\t */\n\tpage = dequeue_huge_page_vma(h, vma, addr, avoid_reserve, gbl_chg);\n\tif (!page) {\n\t\tspin_unlock(&hugetlb_lock);\n\t\tpage = __alloc_buddy_huge_page_with_mpol(h, vma, addr);\n\t\tif (!page)\n\t\t\tgoto out_uncharge_cgroup;\n\t\tif (!avoid_reserve && vma_has_reserves(vma, gbl_chg)) {\n\t\t\tSetPagePrivate(page);\n\t\t\th->resv_huge_pages--;\n\t\t}\n\t\tspin_lock(&hugetlb_lock);\n\t\tlist_move(&page->lru, &h->hugepage_activelist);\n\t\t/* Fall through */\n\t}\n\thugetlb_cgroup_commit_charge(idx, pages_per_huge_page(h), h_cg, page);\n\tspin_unlock(&hugetlb_lock);\n\n\tset_page_private(page, (unsigned long)spool);\n\n\tmap_commit = vma_commit_reservation(h, vma, addr);\n\tif (unlikely(map_chg > map_commit)) {\n\t\t/*\n\t\t * The page was added to the reservation map between\n\t\t * vma_needs_reservation and vma_commit_reservation.\n\t\t * This indicates a race with hugetlb_reserve_pages.\n\t\t * Adjust for the subpool count incremented above AND\n\t\t * in hugetlb_reserve_pages for the same page.  Also,\n\t\t * the reservation count added in hugetlb_reserve_pages\n\t\t * no longer applies.\n\t\t */\n\t\tlong rsv_adjust;\n\n\t\trsv_adjust = hugepage_subpool_put_pages(spool, 1);\n\t\thugetlb_acct_memory(h, -rsv_adjust);\n\t}\n\treturn page;\n\nout_uncharge_cgroup:\n\thugetlb_cgroup_uncharge_cgroup(idx, pages_per_huge_page(h), h_cg);\nout_subpool_put:\n\tif (map_chg || avoid_reserve)\n\t\thugepage_subpool_put_pages(spool, 1);\n\tvma_end_reservation(h, vma, addr);\n\treturn ERR_PTR(-ENOSPC);\n}\n\n/*\n * alloc_huge_page()'s wrapper which simply returns the page if allocation\n * succeeds, otherwise NULL. This function is called from new_vma_page(),\n * where no ERR_VALUE is expected to be returned.\n */\nstruct page *alloc_huge_page_noerr(struct vm_area_struct *vma,\n\t\t\t\tunsigned long addr, int avoid_reserve)\n{\n\tstruct page *page = alloc_huge_page(vma, addr, avoid_reserve);\n\tif (IS_ERR(page))\n\t\tpage = NULL;\n\treturn page;\n}\n\nint __weak alloc_bootmem_huge_page(struct hstate *h)\n{\n\tstruct huge_bootmem_page *m;\n\tint nr_nodes, node;\n\n\tfor_each_node_mask_to_alloc(h, nr_nodes, node, &node_states[N_MEMORY]) {\n\t\tvoid *addr;\n\n\t\taddr = memblock_virt_alloc_try_nid_nopanic(\n\t\t\t\thuge_page_size(h), huge_page_size(h),\n\t\t\t\t0, BOOTMEM_ALLOC_ACCESSIBLE, node);\n\t\tif (addr) {\n\t\t\t/*\n\t\t\t * Use the beginning of the huge page to store the\n\t\t\t * huge_bootmem_page struct (until gather_bootmem\n\t\t\t * puts them into the mem_map).\n\t\t\t */\n\t\t\tm = addr;\n\t\t\tgoto found;\n\t\t}\n\t}\n\treturn 0;\n\nfound:\n\tBUG_ON(!IS_ALIGNED(virt_to_phys(m), huge_page_size(h)));\n\t/* Put them into a private list first because mem_map is not up yet */\n\tlist_add(&m->list, &huge_boot_pages);\n\tm->hstate = h;\n\treturn 1;\n}\n\nstatic void __init prep_compound_huge_page(struct page *page,\n\t\tunsigned int order)\n{\n\tif (unlikely(order > (MAX_ORDER - 1)))\n\t\tprep_compound_gigantic_page(page, order);\n\telse\n\t\tprep_compound_page(page, order);\n}\n\n/* Put bootmem huge pages into the standard lists after mem_map is up */\nstatic void __init gather_bootmem_prealloc(void)\n{\n\tstruct huge_bootmem_page *m;\n\n\tlist_for_each_entry(m, &huge_boot_pages, list) {\n\t\tstruct hstate *h = m->hstate;\n\t\tstruct page *page;\n\n#ifdef CONFIG_HIGHMEM\n\t\tpage = pfn_to_page(m->phys >> PAGE_SHIFT);\n\t\tmemblock_free_late(__pa(m),\n\t\t\t\t   sizeof(struct huge_bootmem_page));\n#else\n\t\tpage = virt_to_page(m);\n#endif\n\t\tWARN_ON(page_count(page) != 1);\n\t\tprep_compound_huge_page(page, h->order);\n\t\tWARN_ON(PageReserved(page));\n\t\tprep_new_huge_page(h, page, page_to_nid(page));\n\t\t/*\n\t\t * If we had gigantic hugepages allocated at boot time, we need\n\t\t * to restore the 'stolen' pages to totalram_pages in order to\n\t\t * fix confusing memory reports from free(1) and another\n\t\t * side-effects, like CommitLimit going negative.\n\t\t */\n\t\tif (hstate_is_gigantic(h))\n\t\t\tadjust_managed_page_count(page, 1 << h->order);\n\t}\n}\n\nstatic void __init hugetlb_hstate_alloc_pages(struct hstate *h)\n{\n\tunsigned long i;\n\n\tfor (i = 0; i < h->max_huge_pages; ++i) {\n\t\tif (hstate_is_gigantic(h)) {\n\t\t\tif (!alloc_bootmem_huge_page(h))\n\t\t\t\tbreak;\n\t\t} else if (!alloc_fresh_huge_page(h,\n\t\t\t\t\t &node_states[N_MEMORY]))\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\tif (i < h->max_huge_pages) {\n\t\tchar buf[32];\n\n\t\tstring_get_size(huge_page_size(h), 1, STRING_UNITS_2, buf, 32);\n\t\tpr_warn(\"HugeTLB: allocating %lu of page size %s failed.  Only allocated %lu hugepages.\\n\",\n\t\t\th->max_huge_pages, buf, i);\n\t\th->max_huge_pages = i;\n\t}\n}\n\nstatic void __init hugetlb_init_hstates(void)\n{\n\tstruct hstate *h;\n\n\tfor_each_hstate(h) {\n\t\tif (minimum_order > huge_page_order(h))\n\t\t\tminimum_order = huge_page_order(h);\n\n\t\t/* oversize hugepages were init'ed in early boot */\n\t\tif (!hstate_is_gigantic(h))\n\t\t\thugetlb_hstate_alloc_pages(h);\n\t}\n\tVM_BUG_ON(minimum_order == UINT_MAX);\n}\n\nstatic void __init report_hugepages(void)\n{\n\tstruct hstate *h;\n\n\tfor_each_hstate(h) {\n\t\tchar buf[32];\n\n\t\tstring_get_size(huge_page_size(h), 1, STRING_UNITS_2, buf, 32);\n\t\tpr_info(\"HugeTLB registered %s page size, pre-allocated %ld pages\\n\",\n\t\t\tbuf, h->free_huge_pages);\n\t}\n}\n\n#ifdef CONFIG_HIGHMEM\nstatic void try_to_free_low(struct hstate *h, unsigned long count,\n\t\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n\tint i;\n\n\tif (hstate_is_gigantic(h))\n\t\treturn;\n\n\tfor_each_node_mask(i, *nodes_allowed) {\n\t\tstruct page *page, *next;\n\t\tstruct list_head *freel = &h->hugepage_freelists[i];\n\t\tlist_for_each_entry_safe(page, next, freel, lru) {\n\t\t\tif (count >= h->nr_huge_pages)\n\t\t\t\treturn;\n\t\t\tif (PageHighMem(page))\n\t\t\t\tcontinue;\n\t\t\tlist_del(&page->lru);\n\t\t\tupdate_and_free_page(h, page);\n\t\t\th->free_huge_pages--;\n\t\t\th->free_huge_pages_node[page_to_nid(page)]--;\n\t\t}\n\t}\n}\n#else\nstatic inline void try_to_free_low(struct hstate *h, unsigned long count,\n\t\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n}\n#endif\n\n/*\n * Increment or decrement surplus_huge_pages.  Keep node-specific counters\n * balanced by operating on them in a round-robin fashion.\n * Returns 1 if an adjustment was made.\n */\nstatic int adjust_pool_surplus(struct hstate *h, nodemask_t *nodes_allowed,\n\t\t\t\tint delta)\n{\n\tint nr_nodes, node;\n\n\tVM_BUG_ON(delta != -1 && delta != 1);\n\n\tif (delta < 0) {\n\t\tfor_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {\n\t\t\tif (h->surplus_huge_pages_node[node])\n\t\t\t\tgoto found;\n\t\t}\n\t} else {\n\t\tfor_each_node_mask_to_free(h, nr_nodes, node, nodes_allowed) {\n\t\t\tif (h->surplus_huge_pages_node[node] <\n\t\t\t\t\th->nr_huge_pages_node[node])\n\t\t\t\tgoto found;\n\t\t}\n\t}\n\treturn 0;\n\nfound:\n\th->surplus_huge_pages += delta;\n\th->surplus_huge_pages_node[node] += delta;\n\treturn 1;\n}\n\n#define persistent_huge_pages(h) (h->nr_huge_pages - h->surplus_huge_pages)\nstatic unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,\n\t\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n\tunsigned long min_count, ret;\n\n\tif (hstate_is_gigantic(h) && !gigantic_page_supported())\n\t\treturn h->max_huge_pages;\n\n\t/*\n\t * Increase the pool size\n\t * First take pages out of surplus state.  Then make up the\n\t * remaining difference by allocating fresh huge pages.\n\t *\n\t * We might race with __alloc_buddy_huge_page() here and be unable\n\t * to convert a surplus huge page to a normal huge page. That is\n\t * not critical, though, it just means the overall size of the\n\t * pool might be one hugepage larger than it needs to be, but\n\t * within all the constraints specified by the sysctls.\n\t */\n\tspin_lock(&hugetlb_lock);\n\twhile (h->surplus_huge_pages && count > persistent_huge_pages(h)) {\n\t\tif (!adjust_pool_surplus(h, nodes_allowed, -1))\n\t\t\tbreak;\n\t}\n\n\twhile (count > persistent_huge_pages(h)) {\n\t\t/*\n\t\t * If this allocation races such that we no longer need the\n\t\t * page, free_huge_page will handle it by freeing the page\n\t\t * and reducing the surplus.\n\t\t */\n\t\tspin_unlock(&hugetlb_lock);\n\n\t\t/* yield cpu to avoid soft lockup */\n\t\tcond_resched();\n\n\t\tif (hstate_is_gigantic(h))\n\t\t\tret = alloc_fresh_gigantic_page(h, nodes_allowed);\n\t\telse\n\t\t\tret = alloc_fresh_huge_page(h, nodes_allowed);\n\t\tspin_lock(&hugetlb_lock);\n\t\tif (!ret)\n\t\t\tgoto out;\n\n\t\t/* Bail for signals. Probably ctrl-c from user */\n\t\tif (signal_pending(current))\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * Decrease the pool size\n\t * First return free pages to the buddy allocator (being careful\n\t * to keep enough around to satisfy reservations).  Then place\n\t * pages into surplus state as needed so the pool will shrink\n\t * to the desired size as pages become free.\n\t *\n\t * By placing pages into the surplus state independent of the\n\t * overcommit value, we are allowing the surplus pool size to\n\t * exceed overcommit. There are few sane options here. Since\n\t * __alloc_buddy_huge_page() is checking the global counter,\n\t * though, we'll note that we're not allowed to exceed surplus\n\t * and won't grow the pool anywhere else. Not until one of the\n\t * sysctls are changed, or the surplus pages go out of use.\n\t */\n\tmin_count = h->resv_huge_pages + h->nr_huge_pages - h->free_huge_pages;\n\tmin_count = max(count, min_count);\n\ttry_to_free_low(h, min_count, nodes_allowed);\n\twhile (min_count < persistent_huge_pages(h)) {\n\t\tif (!free_pool_huge_page(h, nodes_allowed, 0))\n\t\t\tbreak;\n\t\tcond_resched_lock(&hugetlb_lock);\n\t}\n\twhile (count < persistent_huge_pages(h)) {\n\t\tif (!adjust_pool_surplus(h, nodes_allowed, 1))\n\t\t\tbreak;\n\t}\nout:\n\tret = persistent_huge_pages(h);\n\tspin_unlock(&hugetlb_lock);\n\treturn ret;\n}\n\n#define HSTATE_ATTR_RO(_name) \\\n\tstatic struct kobj_attribute _name##_attr = __ATTR_RO(_name)\n\n#define HSTATE_ATTR(_name) \\\n\tstatic struct kobj_attribute _name##_attr = \\\n\t\t__ATTR(_name, 0644, _name##_show, _name##_store)\n\nstatic struct kobject *hugepages_kobj;\nstatic struct kobject *hstate_kobjs[HUGE_MAX_HSTATE];\n\nstatic struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp);\n\nstatic struct hstate *kobj_to_hstate(struct kobject *kobj, int *nidp)\n{\n\tint i;\n\n\tfor (i = 0; i < HUGE_MAX_HSTATE; i++)\n\t\tif (hstate_kobjs[i] == kobj) {\n\t\t\tif (nidp)\n\t\t\t\t*nidp = NUMA_NO_NODE;\n\t\t\treturn &hstates[i];\n\t\t}\n\n\treturn kobj_to_node_hstate(kobj, nidp);\n}\n\nstatic ssize_t nr_hugepages_show_common(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h;\n\tunsigned long nr_huge_pages;\n\tint nid;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (nid == NUMA_NO_NODE)\n\t\tnr_huge_pages = h->nr_huge_pages;\n\telse\n\t\tnr_huge_pages = h->nr_huge_pages_node[nid];\n\n\treturn sprintf(buf, \"%lu\\n\", nr_huge_pages);\n}\n\nstatic ssize_t __nr_hugepages_store_common(bool obey_mempolicy,\n\t\t\t\t\t   struct hstate *h, int nid,\n\t\t\t\t\t   unsigned long count, size_t len)\n{\n\tint err;\n\tNODEMASK_ALLOC(nodemask_t, nodes_allowed, GFP_KERNEL | __GFP_NORETRY);\n\n\tif (hstate_is_gigantic(h) && !gigantic_page_supported()) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (nid == NUMA_NO_NODE) {\n\t\t/*\n\t\t * global hstate attribute\n\t\t */\n\t\tif (!(obey_mempolicy &&\n\t\t\t\tinit_nodemask_of_mempolicy(nodes_allowed))) {\n\t\t\tNODEMASK_FREE(nodes_allowed);\n\t\t\tnodes_allowed = &node_states[N_MEMORY];\n\t\t}\n\t} else if (nodes_allowed) {\n\t\t/*\n\t\t * per node hstate attribute: adjust count to global,\n\t\t * but restrict alloc/free to the specified node.\n\t\t */\n\t\tcount += h->nr_huge_pages - h->nr_huge_pages_node[nid];\n\t\tinit_nodemask_of_node(nodes_allowed, nid);\n\t} else\n\t\tnodes_allowed = &node_states[N_MEMORY];\n\n\th->max_huge_pages = set_max_huge_pages(h, count, nodes_allowed);\n\n\tif (nodes_allowed != &node_states[N_MEMORY])\n\t\tNODEMASK_FREE(nodes_allowed);\n\n\treturn len;\nout:\n\tNODEMASK_FREE(nodes_allowed);\n\treturn err;\n}\n\nstatic ssize_t nr_hugepages_store_common(bool obey_mempolicy,\n\t\t\t\t\t struct kobject *kobj, const char *buf,\n\t\t\t\t\t size_t len)\n{\n\tstruct hstate *h;\n\tunsigned long count;\n\tint nid;\n\tint err;\n\n\terr = kstrtoul(buf, 10, &count);\n\tif (err)\n\t\treturn err;\n\n\th = kobj_to_hstate(kobj, &nid);\n\treturn __nr_hugepages_store_common(obey_mempolicy, h, nid, count, len);\n}\n\nstatic ssize_t nr_hugepages_show(struct kobject *kobj,\n\t\t\t\t       struct kobj_attribute *attr, char *buf)\n{\n\treturn nr_hugepages_show_common(kobj, attr, buf);\n}\n\nstatic ssize_t nr_hugepages_store(struct kobject *kobj,\n\t       struct kobj_attribute *attr, const char *buf, size_t len)\n{\n\treturn nr_hugepages_store_common(false, kobj, buf, len);\n}\nHSTATE_ATTR(nr_hugepages);\n\n#ifdef CONFIG_NUMA\n\n/*\n * hstate attribute for optionally mempolicy-based constraint on persistent\n * huge page alloc/free.\n */\nstatic ssize_t nr_hugepages_mempolicy_show(struct kobject *kobj,\n\t\t\t\t       struct kobj_attribute *attr, char *buf)\n{\n\treturn nr_hugepages_show_common(kobj, attr, buf);\n}\n\nstatic ssize_t nr_hugepages_mempolicy_store(struct kobject *kobj,\n\t       struct kobj_attribute *attr, const char *buf, size_t len)\n{\n\treturn nr_hugepages_store_common(true, kobj, buf, len);\n}\nHSTATE_ATTR(nr_hugepages_mempolicy);\n#endif\n\n\nstatic ssize_t nr_overcommit_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\treturn sprintf(buf, \"%lu\\n\", h->nr_overcommit_huge_pages);\n}\n\nstatic ssize_t nr_overcommit_hugepages_store(struct kobject *kobj,\n\t\tstruct kobj_attribute *attr, const char *buf, size_t count)\n{\n\tint err;\n\tunsigned long input;\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\n\tif (hstate_is_gigantic(h))\n\t\treturn -EINVAL;\n\n\terr = kstrtoul(buf, 10, &input);\n\tif (err)\n\t\treturn err;\n\n\tspin_lock(&hugetlb_lock);\n\th->nr_overcommit_huge_pages = input;\n\tspin_unlock(&hugetlb_lock);\n\n\treturn count;\n}\nHSTATE_ATTR(nr_overcommit_hugepages);\n\nstatic ssize_t free_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h;\n\tunsigned long free_huge_pages;\n\tint nid;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (nid == NUMA_NO_NODE)\n\t\tfree_huge_pages = h->free_huge_pages;\n\telse\n\t\tfree_huge_pages = h->free_huge_pages_node[nid];\n\n\treturn sprintf(buf, \"%lu\\n\", free_huge_pages);\n}\nHSTATE_ATTR_RO(free_hugepages);\n\nstatic ssize_t resv_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\treturn sprintf(buf, \"%lu\\n\", h->resv_huge_pages);\n}\nHSTATE_ATTR_RO(resv_hugepages);\n\nstatic ssize_t surplus_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h;\n\tunsigned long surplus_huge_pages;\n\tint nid;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (nid == NUMA_NO_NODE)\n\t\tsurplus_huge_pages = h->surplus_huge_pages;\n\telse\n\t\tsurplus_huge_pages = h->surplus_huge_pages_node[nid];\n\n\treturn sprintf(buf, \"%lu\\n\", surplus_huge_pages);\n}\nHSTATE_ATTR_RO(surplus_hugepages);\n\nstatic struct attribute *hstate_attrs[] = {\n\t&nr_hugepages_attr.attr,\n\t&nr_overcommit_hugepages_attr.attr,\n\t&free_hugepages_attr.attr,\n\t&resv_hugepages_attr.attr,\n\t&surplus_hugepages_attr.attr,\n#ifdef CONFIG_NUMA\n\t&nr_hugepages_mempolicy_attr.attr,\n#endif\n\tNULL,\n};\n\nstatic struct attribute_group hstate_attr_group = {\n\t.attrs = hstate_attrs,\n};\n\nstatic int hugetlb_sysfs_add_hstate(struct hstate *h, struct kobject *parent,\n\t\t\t\t    struct kobject **hstate_kobjs,\n\t\t\t\t    struct attribute_group *hstate_attr_group)\n{\n\tint retval;\n\tint hi = hstate_index(h);\n\n\thstate_kobjs[hi] = kobject_create_and_add(h->name, parent);\n\tif (!hstate_kobjs[hi])\n\t\treturn -ENOMEM;\n\n\tretval = sysfs_create_group(hstate_kobjs[hi], hstate_attr_group);\n\tif (retval)\n\t\tkobject_put(hstate_kobjs[hi]);\n\n\treturn retval;\n}\n\nstatic void __init hugetlb_sysfs_init(void)\n{\n\tstruct hstate *h;\n\tint err;\n\n\thugepages_kobj = kobject_create_and_add(\"hugepages\", mm_kobj);\n\tif (!hugepages_kobj)\n\t\treturn;\n\n\tfor_each_hstate(h) {\n\t\terr = hugetlb_sysfs_add_hstate(h, hugepages_kobj,\n\t\t\t\t\t hstate_kobjs, &hstate_attr_group);\n\t\tif (err)\n\t\t\tpr_err(\"Hugetlb: Unable to add hstate %s\", h->name);\n\t}\n}\n\n#ifdef CONFIG_NUMA\n\n/*\n * node_hstate/s - associate per node hstate attributes, via their kobjects,\n * with node devices in node_devices[] using a parallel array.  The array\n * index of a node device or _hstate == node id.\n * This is here to avoid any static dependency of the node device driver, in\n * the base kernel, on the hugetlb module.\n */\nstruct node_hstate {\n\tstruct kobject\t\t*hugepages_kobj;\n\tstruct kobject\t\t*hstate_kobjs[HUGE_MAX_HSTATE];\n};\nstatic struct node_hstate node_hstates[MAX_NUMNODES];\n\n/*\n * A subset of global hstate attributes for node devices\n */\nstatic struct attribute *per_node_hstate_attrs[] = {\n\t&nr_hugepages_attr.attr,\n\t&free_hugepages_attr.attr,\n\t&surplus_hugepages_attr.attr,\n\tNULL,\n};\n\nstatic struct attribute_group per_node_hstate_attr_group = {\n\t.attrs = per_node_hstate_attrs,\n};\n\n/*\n * kobj_to_node_hstate - lookup global hstate for node device hstate attr kobj.\n * Returns node id via non-NULL nidp.\n */\nstatic struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp)\n{\n\tint nid;\n\n\tfor (nid = 0; nid < nr_node_ids; nid++) {\n\t\tstruct node_hstate *nhs = &node_hstates[nid];\n\t\tint i;\n\t\tfor (i = 0; i < HUGE_MAX_HSTATE; i++)\n\t\t\tif (nhs->hstate_kobjs[i] == kobj) {\n\t\t\t\tif (nidp)\n\t\t\t\t\t*nidp = nid;\n\t\t\t\treturn &hstates[i];\n\t\t\t}\n\t}\n\n\tBUG();\n\treturn NULL;\n}\n\n/*\n * Unregister hstate attributes from a single node device.\n * No-op if no hstate attributes attached.\n */\nstatic void hugetlb_unregister_node(struct node *node)\n{\n\tstruct hstate *h;\n\tstruct node_hstate *nhs = &node_hstates[node->dev.id];\n\n\tif (!nhs->hugepages_kobj)\n\t\treturn;\t\t/* no hstate attributes */\n\n\tfor_each_hstate(h) {\n\t\tint idx = hstate_index(h);\n\t\tif (nhs->hstate_kobjs[idx]) {\n\t\t\tkobject_put(nhs->hstate_kobjs[idx]);\n\t\t\tnhs->hstate_kobjs[idx] = NULL;\n\t\t}\n\t}\n\n\tkobject_put(nhs->hugepages_kobj);\n\tnhs->hugepages_kobj = NULL;\n}\n\n\n/*\n * Register hstate attributes for a single node device.\n * No-op if attributes already registered.\n */\nstatic void hugetlb_register_node(struct node *node)\n{\n\tstruct hstate *h;\n\tstruct node_hstate *nhs = &node_hstates[node->dev.id];\n\tint err;\n\n\tif (nhs->hugepages_kobj)\n\t\treturn;\t\t/* already allocated */\n\n\tnhs->hugepages_kobj = kobject_create_and_add(\"hugepages\",\n\t\t\t\t\t\t\t&node->dev.kobj);\n\tif (!nhs->hugepages_kobj)\n\t\treturn;\n\n\tfor_each_hstate(h) {\n\t\terr = hugetlb_sysfs_add_hstate(h, nhs->hugepages_kobj,\n\t\t\t\t\t\tnhs->hstate_kobjs,\n\t\t\t\t\t\t&per_node_hstate_attr_group);\n\t\tif (err) {\n\t\t\tpr_err(\"Hugetlb: Unable to add hstate %s for node %d\\n\",\n\t\t\t\th->name, node->dev.id);\n\t\t\thugetlb_unregister_node(node);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n/*\n * hugetlb init time:  register hstate attributes for all registered node\n * devices of nodes that have memory.  All on-line nodes should have\n * registered their associated device by this time.\n */\nstatic void __init hugetlb_register_all_nodes(void)\n{\n\tint nid;\n\n\tfor_each_node_state(nid, N_MEMORY) {\n\t\tstruct node *node = node_devices[nid];\n\t\tif (node->dev.id == nid)\n\t\t\thugetlb_register_node(node);\n\t}\n\n\t/*\n\t * Let the node device driver know we're here so it can\n\t * [un]register hstate attributes on node hotplug.\n\t */\n\tregister_hugetlbfs_with_node(hugetlb_register_node,\n\t\t\t\t     hugetlb_unregister_node);\n}\n#else\t/* !CONFIG_NUMA */\n\nstatic struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp)\n{\n\tBUG();\n\tif (nidp)\n\t\t*nidp = -1;\n\treturn NULL;\n}\n\nstatic void hugetlb_register_all_nodes(void) { }\n\n#endif\n\nstatic int __init hugetlb_init(void)\n{\n\tint i;\n\n\tif (!hugepages_supported())\n\t\treturn 0;\n\n\tif (!size_to_hstate(default_hstate_size)) {\n\t\tif (default_hstate_size != 0) {\n\t\t\tpr_err(\"HugeTLB: unsupported default_hugepagesz %lu. Reverting to %lu\\n\",\n\t\t\t       default_hstate_size, HPAGE_SIZE);\n\t\t}\n\n\t\tdefault_hstate_size = HPAGE_SIZE;\n\t\tif (!size_to_hstate(default_hstate_size))\n\t\t\thugetlb_add_hstate(HUGETLB_PAGE_ORDER);\n\t}\n\tdefault_hstate_idx = hstate_index(size_to_hstate(default_hstate_size));\n\tif (default_hstate_max_huge_pages) {\n\t\tif (!default_hstate.max_huge_pages)\n\t\t\tdefault_hstate.max_huge_pages = default_hstate_max_huge_pages;\n\t}\n\n\thugetlb_init_hstates();\n\tgather_bootmem_prealloc();\n\treport_hugepages();\n\n\thugetlb_sysfs_init();\n\thugetlb_register_all_nodes();\n\thugetlb_cgroup_file_init();\n\n#ifdef CONFIG_SMP\n\tnum_fault_mutexes = roundup_pow_of_two(8 * num_possible_cpus());\n#else\n\tnum_fault_mutexes = 1;\n#endif\n\thugetlb_fault_mutex_table =\n\t\tkmalloc(sizeof(struct mutex) * num_fault_mutexes, GFP_KERNEL);\n\tBUG_ON(!hugetlb_fault_mutex_table);\n\n\tfor (i = 0; i < num_fault_mutexes; i++)\n\t\tmutex_init(&hugetlb_fault_mutex_table[i]);\n\treturn 0;\n}\nsubsys_initcall(hugetlb_init);\n\n/* Should be called on processing a hugepagesz=... option */\nvoid __init hugetlb_bad_size(void)\n{\n\tparsed_valid_hugepagesz = false;\n}\n\nvoid __init hugetlb_add_hstate(unsigned int order)\n{\n\tstruct hstate *h;\n\tunsigned long i;\n\n\tif (size_to_hstate(PAGE_SIZE << order)) {\n\t\tpr_warn(\"hugepagesz= specified twice, ignoring\\n\");\n\t\treturn;\n\t}\n\tBUG_ON(hugetlb_max_hstate >= HUGE_MAX_HSTATE);\n\tBUG_ON(order == 0);\n\th = &hstates[hugetlb_max_hstate++];\n\th->order = order;\n\th->mask = ~((1ULL << (order + PAGE_SHIFT)) - 1);\n\th->nr_huge_pages = 0;\n\th->free_huge_pages = 0;\n\tfor (i = 0; i < MAX_NUMNODES; ++i)\n\t\tINIT_LIST_HEAD(&h->hugepage_freelists[i]);\n\tINIT_LIST_HEAD(&h->hugepage_activelist);\n\th->next_nid_to_alloc = first_memory_node;\n\th->next_nid_to_free = first_memory_node;\n\tsnprintf(h->name, HSTATE_NAME_LEN, \"hugepages-%lukB\",\n\t\t\t\t\thuge_page_size(h)/1024);\n\n\tparsed_hstate = h;\n}\n\nstatic int __init hugetlb_nrpages_setup(char *s)\n{\n\tunsigned long *mhp;\n\tstatic unsigned long *last_mhp;\n\n\tif (!parsed_valid_hugepagesz) {\n\t\tpr_warn(\"hugepages = %s preceded by \"\n\t\t\t\"an unsupported hugepagesz, ignoring\\n\", s);\n\t\tparsed_valid_hugepagesz = true;\n\t\treturn 1;\n\t}\n\t/*\n\t * !hugetlb_max_hstate means we haven't parsed a hugepagesz= parameter yet,\n\t * so this hugepages= parameter goes to the \"default hstate\".\n\t */\n\telse if (!hugetlb_max_hstate)\n\t\tmhp = &default_hstate_max_huge_pages;\n\telse\n\t\tmhp = &parsed_hstate->max_huge_pages;\n\n\tif (mhp == last_mhp) {\n\t\tpr_warn(\"hugepages= specified twice without interleaving hugepagesz=, ignoring\\n\");\n\t\treturn 1;\n\t}\n\n\tif (sscanf(s, \"%lu\", mhp) <= 0)\n\t\t*mhp = 0;\n\n\t/*\n\t * Global state is always initialized later in hugetlb_init.\n\t * But we need to allocate >= MAX_ORDER hstates here early to still\n\t * use the bootmem allocator.\n\t */\n\tif (hugetlb_max_hstate && parsed_hstate->order >= MAX_ORDER)\n\t\thugetlb_hstate_alloc_pages(parsed_hstate);\n\n\tlast_mhp = mhp;\n\n\treturn 1;\n}\n__setup(\"hugepages=\", hugetlb_nrpages_setup);\n\nstatic int __init hugetlb_default_setup(char *s)\n{\n\tdefault_hstate_size = memparse(s, &s);\n\treturn 1;\n}\n__setup(\"default_hugepagesz=\", hugetlb_default_setup);\n\nstatic unsigned int cpuset_mems_nr(unsigned int *array)\n{\n\tint node;\n\tunsigned int nr = 0;\n\n\tfor_each_node_mask(node, cpuset_current_mems_allowed)\n\t\tnr += array[node];\n\n\treturn nr;\n}\n\n#ifdef CONFIG_SYSCTL\nstatic int hugetlb_sysctl_handler_common(bool obey_mempolicy,\n\t\t\t struct ctl_table *table, int write,\n\t\t\t void __user *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp = h->max_huge_pages;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttable->data = &tmp;\n\ttable->maxlen = sizeof(unsigned long);\n\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write)\n\t\tret = __nr_hugepages_store_common(obey_mempolicy, h,\n\t\t\t\t\t\t  NUMA_NO_NODE, tmp, *length);\nout:\n\treturn ret;\n}\n\nint hugetlb_sysctl_handler(struct ctl_table *table, int write,\n\t\t\t  void __user *buffer, size_t *length, loff_t *ppos)\n{\n\n\treturn hugetlb_sysctl_handler_common(false, table, write,\n\t\t\t\t\t\t\tbuffer, length, ppos);\n}\n\n#ifdef CONFIG_NUMA\nint hugetlb_mempolicy_sysctl_handler(struct ctl_table *table, int write,\n\t\t\t  void __user *buffer, size_t *length, loff_t *ppos)\n{\n\treturn hugetlb_sysctl_handler_common(true, table, write,\n\t\t\t\t\t\t\tbuffer, length, ppos);\n}\n#endif /* CONFIG_NUMA */\n\nint hugetlb_overcommit_handler(struct ctl_table *table, int write,\n\t\t\tvoid __user *buffer,\n\t\t\tsize_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttmp = h->nr_overcommit_huge_pages;\n\n\tif (write && hstate_is_gigantic(h))\n\t\treturn -EINVAL;\n\n\ttable->data = &tmp;\n\ttable->maxlen = sizeof(unsigned long);\n\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write) {\n\t\tspin_lock(&hugetlb_lock);\n\t\th->nr_overcommit_huge_pages = tmp;\n\t\tspin_unlock(&hugetlb_lock);\n\t}\nout:\n\treturn ret;\n}\n\n#endif /* CONFIG_SYSCTL */\n\nvoid hugetlb_report_meminfo(struct seq_file *m)\n{\n\tstruct hstate *h = &default_hstate;\n\tif (!hugepages_supported())\n\t\treturn;\n\tseq_printf(m,\n\t\t\t\"HugePages_Total:   %5lu\\n\"\n\t\t\t\"HugePages_Free:    %5lu\\n\"\n\t\t\t\"HugePages_Rsvd:    %5lu\\n\"\n\t\t\t\"HugePages_Surp:    %5lu\\n\"\n\t\t\t\"Hugepagesize:   %8lu kB\\n\",\n\t\t\th->nr_huge_pages,\n\t\t\th->free_huge_pages,\n\t\t\th->resv_huge_pages,\n\t\t\th->surplus_huge_pages,\n\t\t\t1UL << (huge_page_order(h) + PAGE_SHIFT - 10));\n}\n\nint hugetlb_report_node_meminfo(int nid, char *buf)\n{\n\tstruct hstate *h = &default_hstate;\n\tif (!hugepages_supported())\n\t\treturn 0;\n\treturn sprintf(buf,\n\t\t\"Node %d HugePages_Total: %5u\\n\"\n\t\t\"Node %d HugePages_Free:  %5u\\n\"\n\t\t\"Node %d HugePages_Surp:  %5u\\n\",\n\t\tnid, h->nr_huge_pages_node[nid],\n\t\tnid, h->free_huge_pages_node[nid],\n\t\tnid, h->surplus_huge_pages_node[nid]);\n}\n\nvoid hugetlb_show_meminfo(void)\n{\n\tstruct hstate *h;\n\tint nid;\n\n\tif (!hugepages_supported())\n\t\treturn;\n\n\tfor_each_node_state(nid, N_MEMORY)\n\t\tfor_each_hstate(h)\n\t\t\tpr_info(\"Node %d hugepages_total=%u hugepages_free=%u hugepages_surp=%u hugepages_size=%lukB\\n\",\n\t\t\t\tnid,\n\t\t\t\th->nr_huge_pages_node[nid],\n\t\t\t\th->free_huge_pages_node[nid],\n\t\t\t\th->surplus_huge_pages_node[nid],\n\t\t\t\t1UL << (huge_page_order(h) + PAGE_SHIFT - 10));\n}\n\nvoid hugetlb_report_usage(struct seq_file *m, struct mm_struct *mm)\n{\n\tseq_printf(m, \"HugetlbPages:\\t%8lu kB\\n\",\n\t\t   atomic_long_read(&mm->hugetlb_usage) << (PAGE_SHIFT - 10));\n}\n\n/* Return the number pages of memory we physically have, in PAGE_SIZE units. */\nunsigned long hugetlb_total_pages(void)\n{\n\tstruct hstate *h;\n\tunsigned long nr_total_pages = 0;\n\n\tfor_each_hstate(h)\n\t\tnr_total_pages += h->nr_huge_pages * pages_per_huge_page(h);\n\treturn nr_total_pages;\n}\n\nstatic int hugetlb_acct_memory(struct hstate *h, long delta)\n{\n\tint ret = -ENOMEM;\n\n\tspin_lock(&hugetlb_lock);\n\t/*\n\t * When cpuset is configured, it breaks the strict hugetlb page\n\t * reservation as the accounting is done on a global variable. Such\n\t * reservation is completely rubbish in the presence of cpuset because\n\t * the reservation is not checked against page availability for the\n\t * current cpuset. Application can still potentially OOM'ed by kernel\n\t * with lack of free htlb page in cpuset that the task is in.\n\t * Attempt to enforce strict accounting with cpuset is almost\n\t * impossible (or too ugly) because cpuset is too fluid that\n\t * task or memory node can be dynamically moved between cpusets.\n\t *\n\t * The change of semantics for shared hugetlb mapping with cpuset is\n\t * undesirable. However, in order to preserve some of the semantics,\n\t * we fall back to check against current free page availability as\n\t * a best attempt and hopefully to minimize the impact of changing\n\t * semantics that cpuset has.\n\t */\n\tif (delta > 0) {\n\t\tif (gather_surplus_pages(h, delta) < 0)\n\t\t\tgoto out;\n\n\t\tif (delta > cpuset_mems_nr(h->free_huge_pages_node)) {\n\t\t\treturn_unused_surplus_pages(h, delta);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = 0;\n\tif (delta < 0)\n\t\treturn_unused_surplus_pages(h, (unsigned long) -delta);\n\nout:\n\tspin_unlock(&hugetlb_lock);\n\treturn ret;\n}\n\nstatic void hugetlb_vm_op_open(struct vm_area_struct *vma)\n{\n\tstruct resv_map *resv = vma_resv_map(vma);\n\n\t/*\n\t * This new VMA should share its siblings reservation map if present.\n\t * The VMA will only ever have a valid reservation map pointer where\n\t * it is being copied for another still existing VMA.  As that VMA\n\t * has a reference to the reservation map it cannot disappear until\n\t * after this open call completes.  It is therefore safe to take a\n\t * new reference here without additional locking.\n\t */\n\tif (resv && is_vma_resv_set(vma, HPAGE_RESV_OWNER))\n\t\tkref_get(&resv->refs);\n}\n\nstatic void hugetlb_vm_op_close(struct vm_area_struct *vma)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct resv_map *resv = vma_resv_map(vma);\n\tstruct hugepage_subpool *spool = subpool_vma(vma);\n\tunsigned long reserve, start, end;\n\tlong gbl_reserve;\n\n\tif (!resv || !is_vma_resv_set(vma, HPAGE_RESV_OWNER))\n\t\treturn;\n\n\tstart = vma_hugecache_offset(h, vma, vma->vm_start);\n\tend = vma_hugecache_offset(h, vma, vma->vm_end);\n\n\treserve = (end - start) - region_count(resv, start, end);\n\n\tkref_put(&resv->refs, resv_map_release);\n\n\tif (reserve) {\n\t\t/*\n\t\t * Decrement reserve counts.  The global reserve count may be\n\t\t * adjusted if the subpool has a minimum size.\n\t\t */\n\t\tgbl_reserve = hugepage_subpool_put_pages(spool, reserve);\n\t\thugetlb_acct_memory(h, -gbl_reserve);\n\t}\n}\n\n/*\n * We cannot handle pagefaults against hugetlb pages at all.  They cause\n * handle_mm_fault() to try to instantiate regular-sized pages in the\n * hugegpage VMA.  do_page_fault() is supposed to trap this, so BUG is we get\n * this far.\n */\nstatic int hugetlb_vm_op_fault(struct vm_fault *vmf)\n{\n\tBUG();\n\treturn 0;\n}\n\nconst struct vm_operations_struct hugetlb_vm_ops = {\n\t.fault = hugetlb_vm_op_fault,\n\t.open = hugetlb_vm_op_open,\n\t.close = hugetlb_vm_op_close,\n};\n\nstatic pte_t make_huge_pte(struct vm_area_struct *vma, struct page *page,\n\t\t\t\tint writable)\n{\n\tpte_t entry;\n\n\tif (writable) {\n\t\tentry = huge_pte_mkwrite(huge_pte_mkdirty(mk_huge_pte(page,\n\t\t\t\t\t vma->vm_page_prot)));\n\t} else {\n\t\tentry = huge_pte_wrprotect(mk_huge_pte(page,\n\t\t\t\t\t   vma->vm_page_prot));\n\t}\n\tentry = pte_mkyoung(entry);\n\tentry = pte_mkhuge(entry);\n\tentry = arch_make_huge_pte(entry, vma, page, writable);\n\n\treturn entry;\n}\n\nstatic void set_huge_ptep_writable(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long address, pte_t *ptep)\n{\n\tpte_t entry;\n\n\tentry = huge_pte_mkwrite(huge_pte_mkdirty(huge_ptep_get(ptep)));\n\tif (huge_ptep_set_access_flags(vma, address, ptep, entry, 1))\n\t\tupdate_mmu_cache(vma, address, ptep);\n}\n\nbool is_hugetlb_entry_migration(pte_t pte)\n{\n\tswp_entry_t swp;\n\n\tif (huge_pte_none(pte) || pte_present(pte))\n\t\treturn false;\n\tswp = pte_to_swp_entry(pte);\n\tif (non_swap_entry(swp) && is_migration_entry(swp))\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nstatic int is_hugetlb_entry_hwpoisoned(pte_t pte)\n{\n\tswp_entry_t swp;\n\n\tif (huge_pte_none(pte) || pte_present(pte))\n\t\treturn 0;\n\tswp = pte_to_swp_entry(pte);\n\tif (non_swap_entry(swp) && is_hwpoison_entry(swp))\n\t\treturn 1;\n\telse\n\t\treturn 0;\n}\n\nint copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,\n\t\t\t    struct vm_area_struct *vma)\n{\n\tpte_t *src_pte, *dst_pte, entry;\n\tstruct page *ptepage;\n\tunsigned long addr;\n\tint cow;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long sz = huge_page_size(h);\n\tunsigned long mmun_start;\t/* For mmu_notifiers */\n\tunsigned long mmun_end;\t\t/* For mmu_notifiers */\n\tint ret = 0;\n\n\tcow = (vma->vm_flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;\n\n\tmmun_start = vma->vm_start;\n\tmmun_end = vma->vm_end;\n\tif (cow)\n\t\tmmu_notifier_invalidate_range_start(src, mmun_start, mmun_end);\n\n\tfor (addr = vma->vm_start; addr < vma->vm_end; addr += sz) {\n\t\tspinlock_t *src_ptl, *dst_ptl;\n\t\tsrc_pte = huge_pte_offset(src, addr, sz);\n\t\tif (!src_pte)\n\t\t\tcontinue;\n\t\tdst_pte = huge_pte_alloc(dst, addr, sz);\n\t\tif (!dst_pte) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* If the pagetables are shared don't copy or take references */\n\t\tif (dst_pte == src_pte)\n\t\t\tcontinue;\n\n\t\tdst_ptl = huge_pte_lock(h, dst, dst_pte);\n\t\tsrc_ptl = huge_pte_lockptr(h, src, src_pte);\n\t\tspin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);\n\t\tentry = huge_ptep_get(src_pte);\n\t\tif (huge_pte_none(entry)) { /* skip none entry */\n\t\t\t;\n\t\t} else if (unlikely(is_hugetlb_entry_migration(entry) ||\n\t\t\t\t    is_hugetlb_entry_hwpoisoned(entry))) {\n\t\t\tswp_entry_t swp_entry = pte_to_swp_entry(entry);\n\n\t\t\tif (is_write_migration_entry(swp_entry) && cow) {\n\t\t\t\t/*\n\t\t\t\t * COW mappings require pages in both\n\t\t\t\t * parent and child to be set to read.\n\t\t\t\t */\n\t\t\t\tmake_migration_entry_read(&swp_entry);\n\t\t\t\tentry = swp_entry_to_pte(swp_entry);\n\t\t\t\tset_huge_swap_pte_at(src, addr, src_pte,\n\t\t\t\t\t\t     entry, sz);\n\t\t\t}\n\t\t\tset_huge_swap_pte_at(dst, addr, dst_pte, entry, sz);\n\t\t} else {\n\t\t\tif (cow) {\n\t\t\t\thuge_ptep_set_wrprotect(src, addr, src_pte);\n\t\t\t\tmmu_notifier_invalidate_range(src, mmun_start,\n\t\t\t\t\t\t\t\t   mmun_end);\n\t\t\t}\n\t\t\tentry = huge_ptep_get(src_pte);\n\t\t\tptepage = pte_page(entry);\n\t\t\tget_page(ptepage);\n\t\t\tpage_dup_rmap(ptepage, true);\n\t\t\tset_huge_pte_at(dst, addr, dst_pte, entry);\n\t\t\thugetlb_count_add(pages_per_huge_page(h), dst);\n\t\t}\n\t\tspin_unlock(src_ptl);\n\t\tspin_unlock(dst_ptl);\n\t}\n\n\tif (cow)\n\t\tmmu_notifier_invalidate_range_end(src, mmun_start, mmun_end);\n\n\treturn ret;\n}\n\nvoid __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,\n\t\t\t    unsigned long start, unsigned long end,\n\t\t\t    struct page *ref_page)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long address;\n\tpte_t *ptep;\n\tpte_t pte;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long sz = huge_page_size(h);\n\tconst unsigned long mmun_start = start;\t/* For mmu_notifiers */\n\tconst unsigned long mmun_end   = end;\t/* For mmu_notifiers */\n\n\tWARN_ON(!is_vm_hugetlb_page(vma));\n\tBUG_ON(start & ~huge_page_mask(h));\n\tBUG_ON(end & ~huge_page_mask(h));\n\n\t/*\n\t * This is a hugetlb vma, all the pte entries should point\n\t * to huge page.\n\t */\n\ttlb_remove_check_page_size_change(tlb, sz);\n\ttlb_start_vma(tlb, vma);\n\tmmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);\n\taddress = start;\n\tfor (; address < end; address += sz) {\n\t\tptep = huge_pte_offset(mm, address, sz);\n\t\tif (!ptep)\n\t\t\tcontinue;\n\n\t\tptl = huge_pte_lock(h, mm, ptep);\n\t\tif (huge_pmd_unshare(mm, &address, ptep)) {\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\n\t\tpte = huge_ptep_get(ptep);\n\t\tif (huge_pte_none(pte)) {\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Migrating hugepage or HWPoisoned hugepage is already\n\t\t * unmapped and its refcount is dropped, so just clear pte here.\n\t\t */\n\t\tif (unlikely(!pte_present(pte))) {\n\t\t\thuge_pte_clear(mm, address, ptep, sz);\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\n\t\tpage = pte_page(pte);\n\t\t/*\n\t\t * If a reference page is supplied, it is because a specific\n\t\t * page is being unmapped, not a range. Ensure the page we\n\t\t * are about to unmap is the actual page of interest.\n\t\t */\n\t\tif (ref_page) {\n\t\t\tif (page != ref_page) {\n\t\t\t\tspin_unlock(ptl);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Mark the VMA as having unmapped its page so that\n\t\t\t * future faults in this VMA will fail rather than\n\t\t\t * looking like data was lost\n\t\t\t */\n\t\t\tset_vma_resv_flags(vma, HPAGE_RESV_UNMAPPED);\n\t\t}\n\n\t\tpte = huge_ptep_get_and_clear(mm, address, ptep);\n\t\ttlb_remove_huge_tlb_entry(h, tlb, ptep, address);\n\t\tif (huge_pte_dirty(pte))\n\t\t\tset_page_dirty(page);\n\n\t\thugetlb_count_sub(pages_per_huge_page(h), mm);\n\t\tpage_remove_rmap(page, true);\n\n\t\tspin_unlock(ptl);\n\t\ttlb_remove_page_size(tlb, page, huge_page_size(h));\n\t\t/*\n\t\t * Bail out after unmapping reference page if supplied\n\t\t */\n\t\tif (ref_page)\n\t\t\tbreak;\n\t}\n\tmmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);\n\ttlb_end_vma(tlb, vma);\n}\n\nvoid __unmap_hugepage_range_final(struct mmu_gather *tlb,\n\t\t\t  struct vm_area_struct *vma, unsigned long start,\n\t\t\t  unsigned long end, struct page *ref_page)\n{\n\t__unmap_hugepage_range(tlb, vma, start, end, ref_page);\n\n\t/*\n\t * Clear this flag so that x86's huge_pmd_share page_table_shareable\n\t * test will fail on a vma being torn down, and not grab a page table\n\t * on its way out.  We're lucky that the flag has such an appropriate\n\t * name, and can in fact be safely cleared here. We could clear it\n\t * before the __unmap_hugepage_range above, but all that's necessary\n\t * is to clear it before releasing the i_mmap_rwsem. This works\n\t * because in the context this is called, the VMA is about to be\n\t * destroyed and the i_mmap_rwsem is held.\n\t */\n\tvma->vm_flags &= ~VM_MAYSHARE;\n}\n\nvoid unmap_hugepage_range(struct vm_area_struct *vma, unsigned long start,\n\t\t\t  unsigned long end, struct page *ref_page)\n{\n\tstruct mm_struct *mm;\n\tstruct mmu_gather tlb;\n\n\tmm = vma->vm_mm;\n\n\ttlb_gather_mmu(&tlb, mm, start, end);\n\t__unmap_hugepage_range(&tlb, vma, start, end, ref_page);\n\ttlb_finish_mmu(&tlb, start, end);\n}\n\n/*\n * This is called when the original mapper is failing to COW a MAP_PRIVATE\n * mappping it owns the reserve page for. The intention is to unmap the page\n * from other VMAs and let the children be SIGKILLed if they are faulting the\n * same region.\n */\nstatic void unmap_ref_private(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t      struct page *page, unsigned long address)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct vm_area_struct *iter_vma;\n\tstruct address_space *mapping;\n\tpgoff_t pgoff;\n\n\t/*\n\t * vm_pgoff is in PAGE_SIZE units, hence the different calculation\n\t * from page cache lookup which is in HPAGE_SIZE units.\n\t */\n\taddress = address & huge_page_mask(h);\n\tpgoff = ((address - vma->vm_start) >> PAGE_SHIFT) +\n\t\t\tvma->vm_pgoff;\n\tmapping = vma->vm_file->f_mapping;\n\n\t/*\n\t * Take the mapping lock for the duration of the table walk. As\n\t * this mapping should be shared between all the VMAs,\n\t * __unmap_hugepage_range() is called as the lock is already held\n\t */\n\ti_mmap_lock_write(mapping);\n\tvma_interval_tree_foreach(iter_vma, &mapping->i_mmap, pgoff, pgoff) {\n\t\t/* Do not unmap the current VMA */\n\t\tif (iter_vma == vma)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Shared VMAs have their own reserves and do not affect\n\t\t * MAP_PRIVATE accounting but it is possible that a shared\n\t\t * VMA is using the same page so check and skip such VMAs.\n\t\t */\n\t\tif (iter_vma->vm_flags & VM_MAYSHARE)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Unmap the page from other VMAs without their own reserves.\n\t\t * They get marked to be SIGKILLed if they fault in these\n\t\t * areas. This is because a future no-page fault on this VMA\n\t\t * could insert a zeroed page instead of the data existing\n\t\t * from the time of fork. This would look like data corruption\n\t\t */\n\t\tif (!is_vma_resv_set(iter_vma, HPAGE_RESV_OWNER))\n\t\t\tunmap_hugepage_range(iter_vma, address,\n\t\t\t\t\t     address + huge_page_size(h), page);\n\t}\n\ti_mmap_unlock_write(mapping);\n}\n\n/*\n * Hugetlb_cow() should be called with page lock of the original hugepage held.\n * Called with hugetlb_instantiation_mutex held and pte_page locked so we\n * cannot race with other handlers or page migration.\n * Keep the pte_same checks anyway to make transition from the mutex easier.\n */\nstatic int hugetlb_cow(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t       unsigned long address, pte_t *ptep,\n\t\t       struct page *pagecache_page, spinlock_t *ptl)\n{\n\tpte_t pte;\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct page *old_page, *new_page;\n\tint ret = 0, outside_reserve = 0;\n\tunsigned long mmun_start;\t/* For mmu_notifiers */\n\tunsigned long mmun_end;\t\t/* For mmu_notifiers */\n\n\tpte = huge_ptep_get(ptep);\n\told_page = pte_page(pte);\n\nretry_avoidcopy:\n\t/* If no-one else is actually using this page, avoid the copy\n\t * and just make the page writable */\n\tif (page_mapcount(old_page) == 1 && PageAnon(old_page)) {\n\t\tpage_move_anon_rmap(old_page, vma);\n\t\tset_huge_ptep_writable(vma, address, ptep);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * If the process that created a MAP_PRIVATE mapping is about to\n\t * perform a COW due to a shared page count, attempt to satisfy\n\t * the allocation without using the existing reserves. The pagecache\n\t * page is used to determine if the reserve at this address was\n\t * consumed or not. If reserves were used, a partial faulted mapping\n\t * at the time of fork() could consume its reserves on COW instead\n\t * of the full address range.\n\t */\n\tif (is_vma_resv_set(vma, HPAGE_RESV_OWNER) &&\n\t\t\told_page != pagecache_page)\n\t\toutside_reserve = 1;\n\n\tget_page(old_page);\n\n\t/*\n\t * Drop page table lock as buddy allocator may be called. It will\n\t * be acquired again before returning to the caller, as expected.\n\t */\n\tspin_unlock(ptl);\n\tnew_page = alloc_huge_page(vma, address, outside_reserve);\n\n\tif (IS_ERR(new_page)) {\n\t\t/*\n\t\t * If a process owning a MAP_PRIVATE mapping fails to COW,\n\t\t * it is due to references held by a child and an insufficient\n\t\t * huge page pool. To guarantee the original mappers\n\t\t * reliability, unmap the page from child processes. The child\n\t\t * may get SIGKILLed if it later faults.\n\t\t */\n\t\tif (outside_reserve) {\n\t\t\tput_page(old_page);\n\t\t\tBUG_ON(huge_pte_none(pte));\n\t\t\tunmap_ref_private(mm, vma, old_page, address);\n\t\t\tBUG_ON(huge_pte_none(pte));\n\t\t\tspin_lock(ptl);\n\t\t\tptep = huge_pte_offset(mm, address & huge_page_mask(h),\n\t\t\t\t\t       huge_page_size(h));\n\t\t\tif (likely(ptep &&\n\t\t\t\t   pte_same(huge_ptep_get(ptep), pte)))\n\t\t\t\tgoto retry_avoidcopy;\n\t\t\t/*\n\t\t\t * race occurs while re-acquiring page table\n\t\t\t * lock, and our job is done.\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\n\t\tret = (PTR_ERR(new_page) == -ENOMEM) ?\n\t\t\tVM_FAULT_OOM : VM_FAULT_SIGBUS;\n\t\tgoto out_release_old;\n\t}\n\n\t/*\n\t * When the original hugepage is shared one, it does not have\n\t * anon_vma prepared.\n\t */\n\tif (unlikely(anon_vma_prepare(vma))) {\n\t\tret = VM_FAULT_OOM;\n\t\tgoto out_release_all;\n\t}\n\n\tcopy_user_huge_page(new_page, old_page, address, vma,\n\t\t\t    pages_per_huge_page(h));\n\t__SetPageUptodate(new_page);\n\tset_page_huge_active(new_page);\n\n\tmmun_start = address & huge_page_mask(h);\n\tmmun_end = mmun_start + huge_page_size(h);\n\tmmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);\n\n\t/*\n\t * Retake the page table lock to check for racing updates\n\t * before the page tables are altered\n\t */\n\tspin_lock(ptl);\n\tptep = huge_pte_offset(mm, address & huge_page_mask(h),\n\t\t\t       huge_page_size(h));\n\tif (likely(ptep && pte_same(huge_ptep_get(ptep), pte))) {\n\t\tClearPagePrivate(new_page);\n\n\t\t/* Break COW */\n\t\thuge_ptep_clear_flush(vma, address, ptep);\n\t\tmmu_notifier_invalidate_range(mm, mmun_start, mmun_end);\n\t\tset_huge_pte_at(mm, address, ptep,\n\t\t\t\tmake_huge_pte(vma, new_page, 1));\n\t\tpage_remove_rmap(old_page, true);\n\t\thugepage_add_new_anon_rmap(new_page, vma, address);\n\t\t/* Make the old page be freed below */\n\t\tnew_page = old_page;\n\t}\n\tspin_unlock(ptl);\n\tmmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);\nout_release_all:\n\trestore_reserve_on_error(h, vma, address, new_page);\n\tput_page(new_page);\nout_release_old:\n\tput_page(old_page);\n\n\tspin_lock(ptl); /* Caller expects lock to be held */\n\treturn ret;\n}\n\n/* Return the pagecache page at a given address within a VMA */\nstatic struct page *hugetlbfs_pagecache_page(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tstruct address_space *mapping;\n\tpgoff_t idx;\n\n\tmapping = vma->vm_file->f_mapping;\n\tidx = vma_hugecache_offset(h, vma, address);\n\n\treturn find_lock_page(mapping, idx);\n}\n\n/*\n * Return whether there is a pagecache page to back given address within VMA.\n * Caller follow_hugetlb_page() holds page_table_lock so we cannot lock_page.\n */\nstatic bool hugetlbfs_pagecache_present(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tstruct address_space *mapping;\n\tpgoff_t idx;\n\tstruct page *page;\n\n\tmapping = vma->vm_file->f_mapping;\n\tidx = vma_hugecache_offset(h, vma, address);\n\n\tpage = find_get_page(mapping, idx);\n\tif (page)\n\t\tput_page(page);\n\treturn page != NULL;\n}\n\nint huge_add_to_page_cache(struct page *page, struct address_space *mapping,\n\t\t\t   pgoff_t idx)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct hstate *h = hstate_inode(inode);\n\tint err = add_to_page_cache(page, mapping, idx, GFP_KERNEL);\n\n\tif (err)\n\t\treturn err;\n\tClearPagePrivate(page);\n\n\tspin_lock(&inode->i_lock);\n\tinode->i_blocks += blocks_per_huge_page(h);\n\tspin_unlock(&inode->i_lock);\n\treturn 0;\n}\n\nstatic int hugetlb_no_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t   struct address_space *mapping, pgoff_t idx,\n\t\t\t   unsigned long address, pte_t *ptep, unsigned int flags)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tint ret = VM_FAULT_SIGBUS;\n\tint anon_rmap = 0;\n\tunsigned long size;\n\tstruct page *page;\n\tpte_t new_pte;\n\tspinlock_t *ptl;\n\n\t/*\n\t * Currently, we are forced to kill the process in the event the\n\t * original mapper has unmapped pages from the child due to a failed\n\t * COW. Warn that such a situation has occurred as it may not be obvious\n\t */\n\tif (is_vma_resv_set(vma, HPAGE_RESV_UNMAPPED)) {\n\t\tpr_warn_ratelimited(\"PID %d killed due to inadequate hugepage pool\\n\",\n\t\t\t   current->pid);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * Use page lock to guard against racing truncation\n\t * before we get page_table_lock.\n\t */\nretry:\n\tpage = find_lock_page(mapping, idx);\n\tif (!page) {\n\t\tsize = i_size_read(mapping->host) >> huge_page_shift(h);\n\t\tif (idx >= size)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * Check for page in userfault range\n\t\t */\n\t\tif (userfaultfd_missing(vma)) {\n\t\t\tu32 hash;\n\t\t\tstruct vm_fault vmf = {\n\t\t\t\t.vma = vma,\n\t\t\t\t.address = address,\n\t\t\t\t.flags = flags,\n\t\t\t\t/*\n\t\t\t\t * Hard to debug if it ends up being\n\t\t\t\t * used by a callee that assumes\n\t\t\t\t * something about the other\n\t\t\t\t * uninitialized fields... same as in\n\t\t\t\t * memory.c\n\t\t\t\t */\n\t\t\t};\n\n\t\t\t/*\n\t\t\t * hugetlb_fault_mutex must be dropped before\n\t\t\t * handling userfault.  Reacquire after handling\n\t\t\t * fault to make calling code simpler.\n\t\t\t */\n\t\t\thash = hugetlb_fault_mutex_hash(h, mm, vma, mapping,\n\t\t\t\t\t\t\tidx, address);\n\t\t\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\t\t\tret = handle_userfault(&vmf, VM_UFFD_MISSING);\n\t\t\tmutex_lock(&hugetlb_fault_mutex_table[hash]);\n\t\t\tgoto out;\n\t\t}\n\n\t\tpage = alloc_huge_page(vma, address, 0);\n\t\tif (IS_ERR(page)) {\n\t\t\tret = PTR_ERR(page);\n\t\t\tif (ret == -ENOMEM)\n\t\t\t\tret = VM_FAULT_OOM;\n\t\t\telse\n\t\t\t\tret = VM_FAULT_SIGBUS;\n\t\t\tgoto out;\n\t\t}\n\t\tclear_huge_page(page, address, pages_per_huge_page(h));\n\t\t__SetPageUptodate(page);\n\t\tset_page_huge_active(page);\n\n\t\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\t\tint err = huge_add_to_page_cache(page, mapping, idx);\n\t\t\tif (err) {\n\t\t\t\tput_page(page);\n\t\t\t\tif (err == -EEXIST)\n\t\t\t\t\tgoto retry;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else {\n\t\t\tlock_page(page);\n\t\t\tif (unlikely(anon_vma_prepare(vma))) {\n\t\t\t\tret = VM_FAULT_OOM;\n\t\t\t\tgoto backout_unlocked;\n\t\t\t}\n\t\t\tanon_rmap = 1;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If memory error occurs between mmap() and fault, some process\n\t\t * don't have hwpoisoned swap entry for errored virtual address.\n\t\t * So we need to block hugepage fault by PG_hwpoison bit check.\n\t\t */\n\t\tif (unlikely(PageHWPoison(page))) {\n\t\t\tret = VM_FAULT_HWPOISON |\n\t\t\t\tVM_FAULT_SET_HINDEX(hstate_index(h));\n\t\t\tgoto backout_unlocked;\n\t\t}\n\t}\n\n\t/*\n\t * If we are going to COW a private mapping later, we examine the\n\t * pending reservations for this page now. This will ensure that\n\t * any allocations necessary to record that reservation occur outside\n\t * the spinlock.\n\t */\n\tif ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {\n\t\tif (vma_needs_reservation(h, vma, address) < 0) {\n\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto backout_unlocked;\n\t\t}\n\t\t/* Just decrements count, does not deallocate */\n\t\tvma_end_reservation(h, vma, address);\n\t}\n\n\tptl = huge_pte_lock(h, mm, ptep);\n\tsize = i_size_read(mapping->host) >> huge_page_shift(h);\n\tif (idx >= size)\n\t\tgoto backout;\n\n\tret = 0;\n\tif (!huge_pte_none(huge_ptep_get(ptep)))\n\t\tgoto backout;\n\n\tif (anon_rmap) {\n\t\tClearPagePrivate(page);\n\t\thugepage_add_new_anon_rmap(page, vma, address);\n\t} else\n\t\tpage_dup_rmap(page, true);\n\tnew_pte = make_huge_pte(vma, page, ((vma->vm_flags & VM_WRITE)\n\t\t\t\t&& (vma->vm_flags & VM_SHARED)));\n\tset_huge_pte_at(mm, address, ptep, new_pte);\n\n\thugetlb_count_add(pages_per_huge_page(h), mm);\n\tif ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {\n\t\t/* Optimization, do the COW without a second fault */\n\t\tret = hugetlb_cow(mm, vma, address, ptep, page, ptl);\n\t}\n\n\tspin_unlock(ptl);\n\tunlock_page(page);\nout:\n\treturn ret;\n\nbackout:\n\tspin_unlock(ptl);\nbackout_unlocked:\n\tunlock_page(page);\n\trestore_reserve_on_error(h, vma, address, page);\n\tput_page(page);\n\tgoto out;\n}\n\n#ifdef CONFIG_SMP\nu32 hugetlb_fault_mutex_hash(struct hstate *h, struct mm_struct *mm,\n\t\t\t    struct vm_area_struct *vma,\n\t\t\t    struct address_space *mapping,\n\t\t\t    pgoff_t idx, unsigned long address)\n{\n\tunsigned long key[2];\n\tu32 hash;\n\n\tif (vma->vm_flags & VM_SHARED) {\n\t\tkey[0] = (unsigned long) mapping;\n\t\tkey[1] = idx;\n\t} else {\n\t\tkey[0] = (unsigned long) mm;\n\t\tkey[1] = address >> huge_page_shift(h);\n\t}\n\n\thash = jhash2((u32 *)&key, sizeof(key)/sizeof(u32), 0);\n\n\treturn hash & (num_fault_mutexes - 1);\n}\n#else\n/*\n * For uniprocesor systems we always use a single mutex, so just\n * return 0 and avoid the hashing overhead.\n */\nu32 hugetlb_fault_mutex_hash(struct hstate *h, struct mm_struct *mm,\n\t\t\t    struct vm_area_struct *vma,\n\t\t\t    struct address_space *mapping,\n\t\t\t    pgoff_t idx, unsigned long address)\n{\n\treturn 0;\n}\n#endif\n\nint hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tunsigned long address, unsigned int flags)\n{\n\tpte_t *ptep, entry;\n\tspinlock_t *ptl;\n\tint ret;\n\tu32 hash;\n\tpgoff_t idx;\n\tstruct page *page = NULL;\n\tstruct page *pagecache_page = NULL;\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct address_space *mapping;\n\tint need_wait_lock = 0;\n\n\taddress &= huge_page_mask(h);\n\n\tptep = huge_pte_offset(mm, address, huge_page_size(h));\n\tif (ptep) {\n\t\tentry = huge_ptep_get(ptep);\n\t\tif (unlikely(is_hugetlb_entry_migration(entry))) {\n\t\t\tmigration_entry_wait_huge(vma, mm, ptep);\n\t\t\treturn 0;\n\t\t} else if (unlikely(is_hugetlb_entry_hwpoisoned(entry)))\n\t\t\treturn VM_FAULT_HWPOISON_LARGE |\n\t\t\t\tVM_FAULT_SET_HINDEX(hstate_index(h));\n\t} else {\n\t\tptep = huge_pte_alloc(mm, address, huge_page_size(h));\n\t\tif (!ptep)\n\t\t\treturn VM_FAULT_OOM;\n\t}\n\n\tmapping = vma->vm_file->f_mapping;\n\tidx = vma_hugecache_offset(h, vma, address);\n\n\t/*\n\t * Serialize hugepage allocation and instantiation, so that we don't\n\t * get spurious allocation failures if two CPUs race to instantiate\n\t * the same page in the page cache.\n\t */\n\thash = hugetlb_fault_mutex_hash(h, mm, vma, mapping, idx, address);\n\tmutex_lock(&hugetlb_fault_mutex_table[hash]);\n\n\tentry = huge_ptep_get(ptep);\n\tif (huge_pte_none(entry)) {\n\t\tret = hugetlb_no_page(mm, vma, mapping, idx, address, ptep, flags);\n\t\tgoto out_mutex;\n\t}\n\n\tret = 0;\n\n\t/*\n\t * entry could be a migration/hwpoison entry at this point, so this\n\t * check prevents the kernel from going below assuming that we have\n\t * a active hugepage in pagecache. This goto expects the 2nd page fault,\n\t * and is_hugetlb_entry_(migration|hwpoisoned) check will properly\n\t * handle it.\n\t */\n\tif (!pte_present(entry))\n\t\tgoto out_mutex;\n\n\t/*\n\t * If we are going to COW the mapping later, we examine the pending\n\t * reservations for this page now. This will ensure that any\n\t * allocations necessary to record that reservation occur outside the\n\t * spinlock. For private mappings, we also lookup the pagecache\n\t * page now as it is used to determine if a reservation has been\n\t * consumed.\n\t */\n\tif ((flags & FAULT_FLAG_WRITE) && !huge_pte_write(entry)) {\n\t\tif (vma_needs_reservation(h, vma, address) < 0) {\n\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto out_mutex;\n\t\t}\n\t\t/* Just decrements count, does not deallocate */\n\t\tvma_end_reservation(h, vma, address);\n\n\t\tif (!(vma->vm_flags & VM_MAYSHARE))\n\t\t\tpagecache_page = hugetlbfs_pagecache_page(h,\n\t\t\t\t\t\t\t\tvma, address);\n\t}\n\n\tptl = huge_pte_lock(h, mm, ptep);\n\n\t/* Check for a racing update before calling hugetlb_cow */\n\tif (unlikely(!pte_same(entry, huge_ptep_get(ptep))))\n\t\tgoto out_ptl;\n\n\t/*\n\t * hugetlb_cow() requires page locks of pte_page(entry) and\n\t * pagecache_page, so here we need take the former one\n\t * when page != pagecache_page or !pagecache_page.\n\t */\n\tpage = pte_page(entry);\n\tif (page != pagecache_page)\n\t\tif (!trylock_page(page)) {\n\t\t\tneed_wait_lock = 1;\n\t\t\tgoto out_ptl;\n\t\t}\n\n\tget_page(page);\n\n\tif (flags & FAULT_FLAG_WRITE) {\n\t\tif (!huge_pte_write(entry)) {\n\t\t\tret = hugetlb_cow(mm, vma, address, ptep,\n\t\t\t\t\t  pagecache_page, ptl);\n\t\t\tgoto out_put_page;\n\t\t}\n\t\tentry = huge_pte_mkdirty(entry);\n\t}\n\tentry = pte_mkyoung(entry);\n\tif (huge_ptep_set_access_flags(vma, address, ptep, entry,\n\t\t\t\t\t\tflags & FAULT_FLAG_WRITE))\n\t\tupdate_mmu_cache(vma, address, ptep);\nout_put_page:\n\tif (page != pagecache_page)\n\t\tunlock_page(page);\n\tput_page(page);\nout_ptl:\n\tspin_unlock(ptl);\n\n\tif (pagecache_page) {\n\t\tunlock_page(pagecache_page);\n\t\tput_page(pagecache_page);\n\t}\nout_mutex:\n\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\t/*\n\t * Generally it's safe to hold refcount during waiting page lock. But\n\t * here we just wait to defer the next page fault to avoid busy loop and\n\t * the page is not used after unlocked before returning from the current\n\t * page fault. So we are safe from accessing freed page, even if we wait\n\t * here without taking refcount.\n\t */\n\tif (need_wait_lock)\n\t\twait_on_page_locked(page);\n\treturn ret;\n}\n\n/*\n * Used by userfaultfd UFFDIO_COPY.  Based on mcopy_atomic_pte with\n * modifications for huge pages.\n */\nint hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,\n\t\t\t    pte_t *dst_pte,\n\t\t\t    struct vm_area_struct *dst_vma,\n\t\t\t    unsigned long dst_addr,\n\t\t\t    unsigned long src_addr,\n\t\t\t    struct page **pagep)\n{\n\tint vm_shared = dst_vma->vm_flags & VM_SHARED;\n\tstruct hstate *h = hstate_vma(dst_vma);\n\tpte_t _dst_pte;\n\tspinlock_t *ptl;\n\tint ret;\n\tstruct page *page;\n\n\tif (!*pagep) {\n\t\tret = -ENOMEM;\n\t\tpage = alloc_huge_page(dst_vma, dst_addr, 0);\n\t\tif (IS_ERR(page))\n\t\t\tgoto out;\n\n\t\tret = copy_huge_page_from_user(page,\n\t\t\t\t\t\t(const void __user *) src_addr,\n\t\t\t\t\t\tpages_per_huge_page(h), false);\n\n\t\t/* fallback to copy_from_user outside mmap_sem */\n\t\tif (unlikely(ret)) {\n\t\t\tret = -EFAULT;\n\t\t\t*pagep = page;\n\t\t\t/* don't free the page */\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tpage = *pagep;\n\t\t*pagep = NULL;\n\t}\n\n\t/*\n\t * The memory barrier inside __SetPageUptodate makes sure that\n\t * preceding stores to the page contents become visible before\n\t * the set_pte_at() write.\n\t */\n\t__SetPageUptodate(page);\n\tset_page_huge_active(page);\n\n\t/*\n\t * If shared, add to page cache\n\t */\n\tif (vm_shared) {\n\t\tstruct address_space *mapping = dst_vma->vm_file->f_mapping;\n\t\tpgoff_t idx = vma_hugecache_offset(h, dst_vma, dst_addr);\n\n\t\tret = huge_add_to_page_cache(page, mapping, idx);\n\t\tif (ret)\n\t\t\tgoto out_release_nounlock;\n\t}\n\n\tptl = huge_pte_lockptr(h, dst_mm, dst_pte);\n\tspin_lock(ptl);\n\n\tret = -EEXIST;\n\tif (!huge_pte_none(huge_ptep_get(dst_pte)))\n\t\tgoto out_release_unlock;\n\n\tif (vm_shared) {\n\t\tpage_dup_rmap(page, true);\n\t} else {\n\t\tClearPagePrivate(page);\n\t\thugepage_add_new_anon_rmap(page, dst_vma, dst_addr);\n\t}\n\n\t_dst_pte = make_huge_pte(dst_vma, page, dst_vma->vm_flags & VM_WRITE);\n\tif (dst_vma->vm_flags & VM_WRITE)\n\t\t_dst_pte = huge_pte_mkdirty(_dst_pte);\n\t_dst_pte = pte_mkyoung(_dst_pte);\n\n\tset_huge_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte);\n\n\t(void)huge_ptep_set_access_flags(dst_vma, dst_addr, dst_pte, _dst_pte,\n\t\t\t\t\tdst_vma->vm_flags & VM_WRITE);\n\thugetlb_count_add(pages_per_huge_page(h), dst_mm);\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\n\n\tspin_unlock(ptl);\n\tif (vm_shared)\n\t\tunlock_page(page);\n\tret = 0;\nout:\n\treturn ret;\nout_release_unlock:\n\tspin_unlock(ptl);\nout_release_nounlock:\n\tif (vm_shared)\n\t\tunlock_page(page);\n\tput_page(page);\n\tgoto out;\n}\n\nlong follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t struct page **pages, struct vm_area_struct **vmas,\n\t\t\t unsigned long *position, unsigned long *nr_pages,\n\t\t\t long i, unsigned int flags, int *nonblocking)\n{\n\tunsigned long pfn_offset;\n\tunsigned long vaddr = *position;\n\tunsigned long remainder = *nr_pages;\n\tstruct hstate *h = hstate_vma(vma);\n\tint err = -EFAULT;\n\n\twhile (vaddr < vma->vm_end && remainder) {\n\t\tpte_t *pte;\n\t\tspinlock_t *ptl = NULL;\n\t\tint absent;\n\t\tstruct page *page;\n\n\t\t/*\n\t\t * If we have a pending SIGKILL, don't keep faulting pages and\n\t\t * potentially allocating memory.\n\t\t */\n\t\tif (unlikely(fatal_signal_pending(current))) {\n\t\t\tremainder = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Some archs (sparc64, sh*) have multiple pte_ts to\n\t\t * each hugepage.  We have to make sure we get the\n\t\t * first, for the page indexing below to work.\n\t\t *\n\t\t * Note that page table lock is not held when pte is null.\n\t\t */\n\t\tpte = huge_pte_offset(mm, vaddr & huge_page_mask(h),\n\t\t\t\t      huge_page_size(h));\n\t\tif (pte)\n\t\t\tptl = huge_pte_lock(h, mm, pte);\n\t\tabsent = !pte || huge_pte_none(huge_ptep_get(pte));\n\n\t\t/*\n\t\t * When coredumping, it suits get_dump_page if we just return\n\t\t * an error where there's an empty slot with no huge pagecache\n\t\t * to back it.  This way, we avoid allocating a hugepage, and\n\t\t * the sparse dumpfile avoids allocating disk blocks, but its\n\t\t * huge holes still show up with zeroes where they need to be.\n\t\t */\n\t\tif (absent && (flags & FOLL_DUMP) &&\n\t\t    !hugetlbfs_pagecache_present(h, vma, vaddr)) {\n\t\t\tif (pte)\n\t\t\t\tspin_unlock(ptl);\n\t\t\tremainder = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * We need call hugetlb_fault for both hugepages under migration\n\t\t * (in which case hugetlb_fault waits for the migration,) and\n\t\t * hwpoisoned hugepages (in which case we need to prevent the\n\t\t * caller from accessing to them.) In order to do this, we use\n\t\t * here is_swap_pte instead of is_hugetlb_entry_migration and\n\t\t * is_hugetlb_entry_hwpoisoned. This is because it simply covers\n\t\t * both cases, and because we can't follow correct pages\n\t\t * directly from any kind of swap entries.\n\t\t */\n\t\tif (absent || is_swap_pte(huge_ptep_get(pte)) ||\n\t\t    ((flags & FOLL_WRITE) &&\n\t\t      !huge_pte_write(huge_ptep_get(pte)))) {\n\t\t\tint ret;\n\t\t\tunsigned int fault_flags = 0;\n\n\t\t\tif (pte)\n\t\t\t\tspin_unlock(ptl);\n\t\t\tif (flags & FOLL_WRITE)\n\t\t\t\tfault_flags |= FAULT_FLAG_WRITE;\n\t\t\tif (nonblocking)\n\t\t\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY;\n\t\t\tif (flags & FOLL_NOWAIT)\n\t\t\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY |\n\t\t\t\t\tFAULT_FLAG_RETRY_NOWAIT;\n\t\t\tif (flags & FOLL_TRIED) {\n\t\t\t\tVM_WARN_ON_ONCE(fault_flags &\n\t\t\t\t\t\tFAULT_FLAG_ALLOW_RETRY);\n\t\t\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t\t\t}\n\t\t\tret = hugetlb_fault(mm, vma, vaddr, fault_flags);\n\t\t\tif (ret & VM_FAULT_ERROR) {\n\t\t\t\terr = vm_fault_to_errno(ret, flags);\n\t\t\t\tremainder = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ret & VM_FAULT_RETRY) {\n\t\t\t\tif (nonblocking)\n\t\t\t\t\t*nonblocking = 0;\n\t\t\t\t*nr_pages = 0;\n\t\t\t\t/*\n\t\t\t\t * VM_FAULT_RETRY must not return an\n\t\t\t\t * error, it will return zero\n\t\t\t\t * instead.\n\t\t\t\t *\n\t\t\t\t * No need to update \"position\" as the\n\t\t\t\t * caller will not check it after\n\t\t\t\t * *nr_pages is set to 0.\n\t\t\t\t */\n\t\t\t\treturn i;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tpfn_offset = (vaddr & ~huge_page_mask(h)) >> PAGE_SHIFT;\n\t\tpage = pte_page(huge_ptep_get(pte));\nsame_page:\n\t\tif (pages) {\n\t\t\tpages[i] = mem_map_offset(page, pfn_offset);\n\t\t\tget_page(pages[i]);\n\t\t}\n\n\t\tif (vmas)\n\t\t\tvmas[i] = vma;\n\n\t\tvaddr += PAGE_SIZE;\n\t\t++pfn_offset;\n\t\t--remainder;\n\t\t++i;\n\t\tif (vaddr < vma->vm_end && remainder &&\n\t\t\t\tpfn_offset < pages_per_huge_page(h)) {\n\t\t\t/*\n\t\t\t * We use pfn_offset to avoid touching the pageframes\n\t\t\t * of this compound page.\n\t\t\t */\n\t\t\tgoto same_page;\n\t\t}\n\t\tspin_unlock(ptl);\n\t}\n\t*nr_pages = remainder;\n\t/*\n\t * setting position is actually required only if remainder is\n\t * not zero but it's faster not to add a \"if (remainder)\"\n\t * branch.\n\t */\n\t*position = vaddr;\n\n\treturn i ? i : err;\n}\n\n#ifndef __HAVE_ARCH_FLUSH_HUGETLB_TLB_RANGE\n/*\n * ARCHes with special requirements for evicting HUGETLB backing TLB entries can\n * implement this.\n */\n#define flush_hugetlb_tlb_range(vma, addr, end)\tflush_tlb_range(vma, addr, end)\n#endif\n\nunsigned long hugetlb_change_protection(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned long end, pgprot_t newprot)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long start = address;\n\tpte_t *ptep;\n\tpte_t pte;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long pages = 0;\n\n\tBUG_ON(address >= end);\n\tflush_cache_range(vma, address, end);\n\n\tmmu_notifier_invalidate_range_start(mm, start, end);\n\ti_mmap_lock_write(vma->vm_file->f_mapping);\n\tfor (; address < end; address += huge_page_size(h)) {\n\t\tspinlock_t *ptl;\n\t\tptep = huge_pte_offset(mm, address, huge_page_size(h));\n\t\tif (!ptep)\n\t\t\tcontinue;\n\t\tptl = huge_pte_lock(h, mm, ptep);\n\t\tif (huge_pmd_unshare(mm, &address, ptep)) {\n\t\t\tpages++;\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\t\tpte = huge_ptep_get(ptep);\n\t\tif (unlikely(is_hugetlb_entry_hwpoisoned(pte))) {\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\t\tif (unlikely(is_hugetlb_entry_migration(pte))) {\n\t\t\tswp_entry_t entry = pte_to_swp_entry(pte);\n\n\t\t\tif (is_write_migration_entry(entry)) {\n\t\t\t\tpte_t newpte;\n\n\t\t\t\tmake_migration_entry_read(&entry);\n\t\t\t\tnewpte = swp_entry_to_pte(entry);\n\t\t\t\tset_huge_swap_pte_at(mm, address, ptep,\n\t\t\t\t\t\t     newpte, huge_page_size(h));\n\t\t\t\tpages++;\n\t\t\t}\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\t\tif (!huge_pte_none(pte)) {\n\t\t\tpte = huge_ptep_get_and_clear(mm, address, ptep);\n\t\t\tpte = pte_mkhuge(huge_pte_modify(pte, newprot));\n\t\t\tpte = arch_make_huge_pte(pte, vma, NULL, 0);\n\t\t\tset_huge_pte_at(mm, address, ptep, pte);\n\t\t\tpages++;\n\t\t}\n\t\tspin_unlock(ptl);\n\t}\n\t/*\n\t * Must flush TLB before releasing i_mmap_rwsem: x86's huge_pmd_unshare\n\t * may have cleared our pud entry and done put_page on the page table:\n\t * once we release i_mmap_rwsem, another task can do the final put_page\n\t * and that page table be reused and filled with junk.\n\t */\n\tflush_hugetlb_tlb_range(vma, start, end);\n\tmmu_notifier_invalidate_range(mm, start, end);\n\ti_mmap_unlock_write(vma->vm_file->f_mapping);\n\tmmu_notifier_invalidate_range_end(mm, start, end);\n\n\treturn pages << h->order;\n}\n\nint hugetlb_reserve_pages(struct inode *inode,\n\t\t\t\t\tlong from, long to,\n\t\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\t\tvm_flags_t vm_flags)\n{\n\tlong ret, chg;\n\tstruct hstate *h = hstate_inode(inode);\n\tstruct hugepage_subpool *spool = subpool_inode(inode);\n\tstruct resv_map *resv_map;\n\tlong gbl_reserve;\n\n\t/*\n\t * Only apply hugepage reservation if asked. At fault time, an\n\t * attempt will be made for VM_NORESERVE to allocate a page\n\t * without using reserves\n\t */\n\tif (vm_flags & VM_NORESERVE)\n\t\treturn 0;\n\n\t/*\n\t * Shared mappings base their reservation on the number of pages that\n\t * are already allocated on behalf of the file. Private mappings need\n\t * to reserve the full area even if read-only as mprotect() may be\n\t * called to make the mapping read-write. Assume !vma is a shm mapping\n\t */\n\tif (!vma || vma->vm_flags & VM_MAYSHARE) {\n\t\tresv_map = inode_resv_map(inode);\n\n\t\tchg = region_chg(resv_map, from, to);\n\n\t} else {\n\t\tresv_map = resv_map_alloc();\n\t\tif (!resv_map)\n\t\t\treturn -ENOMEM;\n\n\t\tchg = to - from;\n\n\t\tset_vma_resv_map(vma, resv_map);\n\t\tset_vma_resv_flags(vma, HPAGE_RESV_OWNER);\n\t}\n\n\tif (chg < 0) {\n\t\tret = chg;\n\t\tgoto out_err;\n\t}\n\n\t/*\n\t * There must be enough pages in the subpool for the mapping. If\n\t * the subpool has a minimum size, there may be some global\n\t * reservations already in place (gbl_reserve).\n\t */\n\tgbl_reserve = hugepage_subpool_get_pages(spool, chg);\n\tif (gbl_reserve < 0) {\n\t\tret = -ENOSPC;\n\t\tgoto out_err;\n\t}\n\n\t/*\n\t * Check enough hugepages are available for the reservation.\n\t * Hand the pages back to the subpool if there are not\n\t */\n\tret = hugetlb_acct_memory(h, gbl_reserve);\n\tif (ret < 0) {\n\t\t/* put back original number of pages, chg */\n\t\t(void)hugepage_subpool_put_pages(spool, chg);\n\t\tgoto out_err;\n\t}\n\n\t/*\n\t * Account for the reservations made. Shared mappings record regions\n\t * that have reservations as they are shared by multiple VMAs.\n\t * When the last VMA disappears, the region map says how much\n\t * the reservation was and the page cache tells how much of\n\t * the reservation was consumed. Private mappings are per-VMA and\n\t * only the consumed reservations are tracked. When the VMA\n\t * disappears, the original reservation is the VMA size and the\n\t * consumed reservations are stored in the map. Hence, nothing\n\t * else has to be done for private mappings here\n\t */\n\tif (!vma || vma->vm_flags & VM_MAYSHARE) {\n\t\tlong add = region_add(resv_map, from, to);\n\n\t\tif (unlikely(chg > add)) {\n\t\t\t/*\n\t\t\t * pages in this range were added to the reserve\n\t\t\t * map between region_chg and region_add.  This\n\t\t\t * indicates a race with alloc_huge_page.  Adjust\n\t\t\t * the subpool and reserve counts modified above\n\t\t\t * based on the difference.\n\t\t\t */\n\t\t\tlong rsv_adjust;\n\n\t\t\trsv_adjust = hugepage_subpool_put_pages(spool,\n\t\t\t\t\t\t\t\tchg - add);\n\t\t\thugetlb_acct_memory(h, -rsv_adjust);\n\t\t}\n\t}\n\treturn 0;\nout_err:\n\tif (!vma || vma->vm_flags & VM_MAYSHARE)\n\t\t/* Don't call region_abort if region_chg failed */\n\t\tif (chg >= 0)\n\t\t\tregion_abort(resv_map, from, to);\n\tif (vma && is_vma_resv_set(vma, HPAGE_RESV_OWNER))\n\t\tkref_put(&resv_map->refs, resv_map_release);\n\treturn ret;\n}\n\nlong hugetlb_unreserve_pages(struct inode *inode, long start, long end,\n\t\t\t\t\t\t\t\tlong freed)\n{\n\tstruct hstate *h = hstate_inode(inode);\n\tstruct resv_map *resv_map = inode_resv_map(inode);\n\tlong chg = 0;\n\tstruct hugepage_subpool *spool = subpool_inode(inode);\n\tlong gbl_reserve;\n\n\tif (resv_map) {\n\t\tchg = region_del(resv_map, start, end);\n\t\t/*\n\t\t * region_del() can fail in the rare case where a region\n\t\t * must be split and another region descriptor can not be\n\t\t * allocated.  If end == LONG_MAX, it will not fail.\n\t\t */\n\t\tif (chg < 0)\n\t\t\treturn chg;\n\t}\n\n\tspin_lock(&inode->i_lock);\n\tinode->i_blocks -= (blocks_per_huge_page(h) * freed);\n\tspin_unlock(&inode->i_lock);\n\n\t/*\n\t * If the subpool has a minimum size, the number of global\n\t * reservations to be released may be adjusted.\n\t */\n\tgbl_reserve = hugepage_subpool_put_pages(spool, (chg - freed));\n\thugetlb_acct_memory(h, -gbl_reserve);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE\nstatic unsigned long page_table_shareable(struct vm_area_struct *svma,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tunsigned long addr, pgoff_t idx)\n{\n\tunsigned long saddr = ((idx - svma->vm_pgoff) << PAGE_SHIFT) +\n\t\t\t\tsvma->vm_start;\n\tunsigned long sbase = saddr & PUD_MASK;\n\tunsigned long s_end = sbase + PUD_SIZE;\n\n\t/* Allow segments to share if only one is marked locked */\n\tunsigned long vm_flags = vma->vm_flags & VM_LOCKED_CLEAR_MASK;\n\tunsigned long svm_flags = svma->vm_flags & VM_LOCKED_CLEAR_MASK;\n\n\t/*\n\t * match the virtual addresses, permission and the alignment of the\n\t * page table page.\n\t */\n\tif (pmd_index(addr) != pmd_index(saddr) ||\n\t    vm_flags != svm_flags ||\n\t    sbase < svma->vm_start || svma->vm_end < s_end)\n\t\treturn 0;\n\n\treturn saddr;\n}\n\nstatic bool vma_shareable(struct vm_area_struct *vma, unsigned long addr)\n{\n\tunsigned long base = addr & PUD_MASK;\n\tunsigned long end = base + PUD_SIZE;\n\n\t/*\n\t * check on proper vm_flags and page table alignment\n\t */\n\tif (vma->vm_flags & VM_MAYSHARE &&\n\t    vma->vm_start <= base && end <= vma->vm_end)\n\t\treturn true;\n\treturn false;\n}\n\n/*\n * Search for a shareable pmd page for hugetlb. In any case calls pmd_alloc()\n * and returns the corresponding pte. While this is not necessary for the\n * !shared pmd case because we can allocate the pmd later as well, it makes the\n * code much cleaner. pmd allocation is essential for the shared case because\n * pud has to be populated inside the same i_mmap_rwsem section - otherwise\n * racing tasks could either miss the sharing (see huge_pte_offset) or select a\n * bad pmd for sharing.\n */\npte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud)\n{\n\tstruct vm_area_struct *vma = find_vma(mm, addr);\n\tstruct address_space *mapping = vma->vm_file->f_mapping;\n\tpgoff_t idx = ((addr - vma->vm_start) >> PAGE_SHIFT) +\n\t\t\tvma->vm_pgoff;\n\tstruct vm_area_struct *svma;\n\tunsigned long saddr;\n\tpte_t *spte = NULL;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\tif (!vma_shareable(vma, addr))\n\t\treturn (pte_t *)pmd_alloc(mm, pud, addr);\n\n\ti_mmap_lock_write(mapping);\n\tvma_interval_tree_foreach(svma, &mapping->i_mmap, idx, idx) {\n\t\tif (svma == vma)\n\t\t\tcontinue;\n\n\t\tsaddr = page_table_shareable(svma, vma, addr, idx);\n\t\tif (saddr) {\n\t\t\tspte = huge_pte_offset(svma->vm_mm, saddr,\n\t\t\t\t\t       vma_mmu_pagesize(svma));\n\t\t\tif (spte) {\n\t\t\t\tget_page(virt_to_page(spte));\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!spte)\n\t\tgoto out;\n\n\tptl = huge_pte_lock(hstate_vma(vma), mm, spte);\n\tif (pud_none(*pud)) {\n\t\tpud_populate(mm, pud,\n\t\t\t\t(pmd_t *)((unsigned long)spte & PAGE_MASK));\n\t\tmm_inc_nr_pmds(mm);\n\t} else {\n\t\tput_page(virt_to_page(spte));\n\t}\n\tspin_unlock(ptl);\nout:\n\tpte = (pte_t *)pmd_alloc(mm, pud, addr);\n\ti_mmap_unlock_write(mapping);\n\treturn pte;\n}\n\n/*\n * unmap huge page backed by shared pte.\n *\n * Hugetlb pte page is ref counted at the time of mapping.  If pte is shared\n * indicated by page_count > 1, unmap is achieved by clearing pud and\n * decrementing the ref count. If count == 1, the pte page is not shared.\n *\n * called with page table lock held.\n *\n * returns: 1 successfully unmapped a shared pte page\n *\t    0 the underlying pte page is not shared, or it is the last user\n */\nint huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep)\n{\n\tpgd_t *pgd = pgd_offset(mm, *addr);\n\tp4d_t *p4d = p4d_offset(pgd, *addr);\n\tpud_t *pud = pud_offset(p4d, *addr);\n\n\tBUG_ON(page_count(virt_to_page(ptep)) == 0);\n\tif (page_count(virt_to_page(ptep)) == 1)\n\t\treturn 0;\n\n\tpud_clear(pud);\n\tput_page(virt_to_page(ptep));\n\tmm_dec_nr_pmds(mm);\n\t*addr = ALIGN(*addr, HPAGE_SIZE * PTRS_PER_PTE) - HPAGE_SIZE;\n\treturn 1;\n}\n#define want_pmd_share()\t(1)\n#else /* !CONFIG_ARCH_WANT_HUGE_PMD_SHARE */\npte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud)\n{\n\treturn NULL;\n}\n\nint huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep)\n{\n\treturn 0;\n}\n#define want_pmd_share()\t(0)\n#endif /* CONFIG_ARCH_WANT_HUGE_PMD_SHARE */\n\n#ifdef CONFIG_ARCH_WANT_GENERAL_HUGETLB\npte_t *huge_pte_alloc(struct mm_struct *mm,\n\t\t\tunsigned long addr, unsigned long sz)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpte_t *pte = NULL;\n\n\tpgd = pgd_offset(mm, addr);\n\tp4d = p4d_offset(pgd, addr);\n\tpud = pud_alloc(mm, p4d, addr);\n\tif (pud) {\n\t\tif (sz == PUD_SIZE) {\n\t\t\tpte = (pte_t *)pud;\n\t\t} else {\n\t\t\tBUG_ON(sz != PMD_SIZE);\n\t\t\tif (want_pmd_share() && pud_none(*pud))\n\t\t\t\tpte = huge_pmd_share(mm, addr, pud);\n\t\t\telse\n\t\t\t\tpte = (pte_t *)pmd_alloc(mm, pud, addr);\n\t\t}\n\t}\n\tBUG_ON(pte && pte_present(*pte) && !pte_huge(*pte));\n\n\treturn pte;\n}\n\npte_t *huge_pte_offset(struct mm_struct *mm,\n\t\t       unsigned long addr, unsigned long sz)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\n\tpgd = pgd_offset(mm, addr);\n\tif (!pgd_present(*pgd))\n\t\treturn NULL;\n\tp4d = p4d_offset(pgd, addr);\n\tif (!p4d_present(*p4d))\n\t\treturn NULL;\n\tpud = pud_offset(p4d, addr);\n\tif (!pud_present(*pud))\n\t\treturn NULL;\n\tif (pud_huge(*pud))\n\t\treturn (pte_t *)pud;\n\tpmd = pmd_offset(pud, addr);\n\treturn (pte_t *) pmd;\n}\n\n#endif /* CONFIG_ARCH_WANT_GENERAL_HUGETLB */\n\n/*\n * These functions are overwritable if your architecture needs its own\n * behavior.\n */\nstruct page * __weak\nfollow_huge_addr(struct mm_struct *mm, unsigned long address,\n\t\t\t      int write)\n{\n\treturn ERR_PTR(-EINVAL);\n}\n\nstruct page * __weak\nfollow_huge_pd(struct vm_area_struct *vma,\n\t       unsigned long address, hugepd_t hpd, int flags, int pdshift)\n{\n\tWARN(1, \"hugepd follow called with no support for hugepage directory format\\n\");\n\treturn NULL;\n}\n\nstruct page * __weak\nfollow_huge_pmd(struct mm_struct *mm, unsigned long address,\n\t\tpmd_t *pmd, int flags)\n{\n\tstruct page *page = NULL;\n\tspinlock_t *ptl;\n\tpte_t pte;\nretry:\n\tptl = pmd_lockptr(mm, pmd);\n\tspin_lock(ptl);\n\t/*\n\t * make sure that the address range covered by this pmd is not\n\t * unmapped from other threads.\n\t */\n\tif (!pmd_huge(*pmd))\n\t\tgoto out;\n\tpte = huge_ptep_get((pte_t *)pmd);\n\tif (pte_present(pte)) {\n\t\tpage = pmd_page(*pmd) + ((address & ~PMD_MASK) >> PAGE_SHIFT);\n\t\tif (flags & FOLL_GET)\n\t\t\tget_page(page);\n\t} else {\n\t\tif (is_hugetlb_entry_migration(pte)) {\n\t\t\tspin_unlock(ptl);\n\t\t\t__migration_entry_wait(mm, (pte_t *)pmd, ptl);\n\t\t\tgoto retry;\n\t\t}\n\t\t/*\n\t\t * hwpoisoned entry is treated as no_page_table in\n\t\t * follow_page_mask().\n\t\t */\n\t}\nout:\n\tspin_unlock(ptl);\n\treturn page;\n}\n\nstruct page * __weak\nfollow_huge_pud(struct mm_struct *mm, unsigned long address,\n\t\tpud_t *pud, int flags)\n{\n\tif (flags & FOLL_GET)\n\t\treturn NULL;\n\n\treturn pte_page(*(pte_t *)pud) + ((address & ~PUD_MASK) >> PAGE_SHIFT);\n}\n\nstruct page * __weak\nfollow_huge_pgd(struct mm_struct *mm, unsigned long address, pgd_t *pgd, int flags)\n{\n\tif (flags & FOLL_GET)\n\t\treturn NULL;\n\n\treturn pte_page(*(pte_t *)pgd) + ((address & ~PGDIR_MASK) >> PAGE_SHIFT);\n}\n\nbool isolate_huge_page(struct page *page, struct list_head *list)\n{\n\tbool ret = true;\n\n\tVM_BUG_ON_PAGE(!PageHead(page), page);\n\tspin_lock(&hugetlb_lock);\n\tif (!page_huge_active(page) || !get_page_unless_zero(page)) {\n\t\tret = false;\n\t\tgoto unlock;\n\t}\n\tclear_page_huge_active(page);\n\tlist_move_tail(&page->lru, list);\nunlock:\n\tspin_unlock(&hugetlb_lock);\n\treturn ret;\n}\n\nvoid putback_active_hugepage(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!PageHead(page), page);\n\tspin_lock(&hugetlb_lock);\n\tset_page_huge_active(page);\n\tlist_move_tail(&page->lru, &(page_hstate(page))->hugepage_activelist);\n\tspin_unlock(&hugetlb_lock);\n\tput_page(page);\n}\n"], "fixing_code": ["/*\n * Generic hugetlb support.\n * (C) Nadia Yvette Chambers, April 2004\n */\n#include <linux/list.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/seq_file.h>\n#include <linux/sysctl.h>\n#include <linux/highmem.h>\n#include <linux/mmu_notifier.h>\n#include <linux/nodemask.h>\n#include <linux/pagemap.h>\n#include <linux/mempolicy.h>\n#include <linux/compiler.h>\n#include <linux/cpuset.h>\n#include <linux/mutex.h>\n#include <linux/bootmem.h>\n#include <linux/sysfs.h>\n#include <linux/slab.h>\n#include <linux/sched/signal.h>\n#include <linux/rmap.h>\n#include <linux/string_helpers.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/jhash.h>\n\n#include <asm/page.h>\n#include <asm/pgtable.h>\n#include <asm/tlb.h>\n\n#include <linux/io.h>\n#include <linux/hugetlb.h>\n#include <linux/hugetlb_cgroup.h>\n#include <linux/node.h>\n#include <linux/userfaultfd_k.h>\n#include \"internal.h\"\n\nint hugepages_treat_as_movable;\n\nint hugetlb_max_hstate __read_mostly;\nunsigned int default_hstate_idx;\nstruct hstate hstates[HUGE_MAX_HSTATE];\n/*\n * Minimum page order among possible hugepage sizes, set to a proper value\n * at boot time.\n */\nstatic unsigned int minimum_order __read_mostly = UINT_MAX;\n\n__initdata LIST_HEAD(huge_boot_pages);\n\n/* for command line parsing */\nstatic struct hstate * __initdata parsed_hstate;\nstatic unsigned long __initdata default_hstate_max_huge_pages;\nstatic unsigned long __initdata default_hstate_size;\nstatic bool __initdata parsed_valid_hugepagesz = true;\n\n/*\n * Protects updates to hugepage_freelists, hugepage_activelist, nr_huge_pages,\n * free_huge_pages, and surplus_huge_pages.\n */\nDEFINE_SPINLOCK(hugetlb_lock);\n\n/*\n * Serializes faults on the same logical page.  This is used to\n * prevent spurious OOMs when the hugepage pool is fully utilized.\n */\nstatic int num_fault_mutexes;\nstruct mutex *hugetlb_fault_mutex_table ____cacheline_aligned_in_smp;\n\n/* Forward declaration */\nstatic int hugetlb_acct_memory(struct hstate *h, long delta);\n\nstatic inline void unlock_or_release_subpool(struct hugepage_subpool *spool)\n{\n\tbool free = (spool->count == 0) && (spool->used_hpages == 0);\n\n\tspin_unlock(&spool->lock);\n\n\t/* If no pages are used, and no other handles to the subpool\n\t * remain, give up any reservations mased on minimum size and\n\t * free the subpool */\n\tif (free) {\n\t\tif (spool->min_hpages != -1)\n\t\t\thugetlb_acct_memory(spool->hstate,\n\t\t\t\t\t\t-spool->min_hpages);\n\t\tkfree(spool);\n\t}\n}\n\nstruct hugepage_subpool *hugepage_new_subpool(struct hstate *h, long max_hpages,\n\t\t\t\t\t\tlong min_hpages)\n{\n\tstruct hugepage_subpool *spool;\n\n\tspool = kzalloc(sizeof(*spool), GFP_KERNEL);\n\tif (!spool)\n\t\treturn NULL;\n\n\tspin_lock_init(&spool->lock);\n\tspool->count = 1;\n\tspool->max_hpages = max_hpages;\n\tspool->hstate = h;\n\tspool->min_hpages = min_hpages;\n\n\tif (min_hpages != -1 && hugetlb_acct_memory(h, min_hpages)) {\n\t\tkfree(spool);\n\t\treturn NULL;\n\t}\n\tspool->rsv_hpages = min_hpages;\n\n\treturn spool;\n}\n\nvoid hugepage_put_subpool(struct hugepage_subpool *spool)\n{\n\tspin_lock(&spool->lock);\n\tBUG_ON(!spool->count);\n\tspool->count--;\n\tunlock_or_release_subpool(spool);\n}\n\n/*\n * Subpool accounting for allocating and reserving pages.\n * Return -ENOMEM if there are not enough resources to satisfy the\n * the request.  Otherwise, return the number of pages by which the\n * global pools must be adjusted (upward).  The returned value may\n * only be different than the passed value (delta) in the case where\n * a subpool minimum size must be manitained.\n */\nstatic long hugepage_subpool_get_pages(struct hugepage_subpool *spool,\n\t\t\t\t      long delta)\n{\n\tlong ret = delta;\n\n\tif (!spool)\n\t\treturn ret;\n\n\tspin_lock(&spool->lock);\n\n\tif (spool->max_hpages != -1) {\t\t/* maximum size accounting */\n\t\tif ((spool->used_hpages + delta) <= spool->max_hpages)\n\t\t\tspool->used_hpages += delta;\n\t\telse {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto unlock_ret;\n\t\t}\n\t}\n\n\t/* minimum size accounting */\n\tif (spool->min_hpages != -1 && spool->rsv_hpages) {\n\t\tif (delta > spool->rsv_hpages) {\n\t\t\t/*\n\t\t\t * Asking for more reserves than those already taken on\n\t\t\t * behalf of subpool.  Return difference.\n\t\t\t */\n\t\t\tret = delta - spool->rsv_hpages;\n\t\t\tspool->rsv_hpages = 0;\n\t\t} else {\n\t\t\tret = 0;\t/* reserves already accounted for */\n\t\t\tspool->rsv_hpages -= delta;\n\t\t}\n\t}\n\nunlock_ret:\n\tspin_unlock(&spool->lock);\n\treturn ret;\n}\n\n/*\n * Subpool accounting for freeing and unreserving pages.\n * Return the number of global page reservations that must be dropped.\n * The return value may only be different than the passed value (delta)\n * in the case where a subpool minimum size must be maintained.\n */\nstatic long hugepage_subpool_put_pages(struct hugepage_subpool *spool,\n\t\t\t\t       long delta)\n{\n\tlong ret = delta;\n\n\tif (!spool)\n\t\treturn delta;\n\n\tspin_lock(&spool->lock);\n\n\tif (spool->max_hpages != -1)\t\t/* maximum size accounting */\n\t\tspool->used_hpages -= delta;\n\n\t /* minimum size accounting */\n\tif (spool->min_hpages != -1 && spool->used_hpages < spool->min_hpages) {\n\t\tif (spool->rsv_hpages + delta <= spool->min_hpages)\n\t\t\tret = 0;\n\t\telse\n\t\t\tret = spool->rsv_hpages + delta - spool->min_hpages;\n\n\t\tspool->rsv_hpages += delta;\n\t\tif (spool->rsv_hpages > spool->min_hpages)\n\t\t\tspool->rsv_hpages = spool->min_hpages;\n\t}\n\n\t/*\n\t * If hugetlbfs_put_super couldn't free spool due to an outstanding\n\t * quota reference, free it now.\n\t */\n\tunlock_or_release_subpool(spool);\n\n\treturn ret;\n}\n\nstatic inline struct hugepage_subpool *subpool_inode(struct inode *inode)\n{\n\treturn HUGETLBFS_SB(inode->i_sb)->spool;\n}\n\nstatic inline struct hugepage_subpool *subpool_vma(struct vm_area_struct *vma)\n{\n\treturn subpool_inode(file_inode(vma->vm_file));\n}\n\n/*\n * Region tracking -- allows tracking of reservations and instantiated pages\n *                    across the pages in a mapping.\n *\n * The region data structures are embedded into a resv_map and protected\n * by a resv_map's lock.  The set of regions within the resv_map represent\n * reservations for huge pages, or huge pages that have already been\n * instantiated within the map.  The from and to elements are huge page\n * indicies into the associated mapping.  from indicates the starting index\n * of the region.  to represents the first index past the end of  the region.\n *\n * For example, a file region structure with from == 0 and to == 4 represents\n * four huge pages in a mapping.  It is important to note that the to element\n * represents the first element past the end of the region. This is used in\n * arithmetic as 4(to) - 0(from) = 4 huge pages in the region.\n *\n * Interval notation of the form [from, to) will be used to indicate that\n * the endpoint from is inclusive and to is exclusive.\n */\nstruct file_region {\n\tstruct list_head link;\n\tlong from;\n\tlong to;\n};\n\n/*\n * Add the huge page range represented by [f, t) to the reserve\n * map.  In the normal case, existing regions will be expanded\n * to accommodate the specified range.  Sufficient regions should\n * exist for expansion due to the previous call to region_chg\n * with the same range.  However, it is possible that region_del\n * could have been called after region_chg and modifed the map\n * in such a way that no region exists to be expanded.  In this\n * case, pull a region descriptor from the cache associated with\n * the map and use that for the new range.\n *\n * Return the number of new huge pages added to the map.  This\n * number is greater than or equal to zero.\n */\nstatic long region_add(struct resv_map *resv, long f, long t)\n{\n\tstruct list_head *head = &resv->regions;\n\tstruct file_region *rg, *nrg, *trg;\n\tlong add = 0;\n\n\tspin_lock(&resv->lock);\n\t/* Locate the region we are either in or before. */\n\tlist_for_each_entry(rg, head, link)\n\t\tif (f <= rg->to)\n\t\t\tbreak;\n\n\t/*\n\t * If no region exists which can be expanded to include the\n\t * specified range, the list must have been modified by an\n\t * interleving call to region_del().  Pull a region descriptor\n\t * from the cache and use it for this range.\n\t */\n\tif (&rg->link == head || t < rg->from) {\n\t\tVM_BUG_ON(resv->region_cache_count <= 0);\n\n\t\tresv->region_cache_count--;\n\t\tnrg = list_first_entry(&resv->region_cache, struct file_region,\n\t\t\t\t\tlink);\n\t\tlist_del(&nrg->link);\n\n\t\tnrg->from = f;\n\t\tnrg->to = t;\n\t\tlist_add(&nrg->link, rg->link.prev);\n\n\t\tadd += t - f;\n\t\tgoto out_locked;\n\t}\n\n\t/* Round our left edge to the current segment if it encloses us. */\n\tif (f > rg->from)\n\t\tf = rg->from;\n\n\t/* Check for and consume any regions we now overlap with. */\n\tnrg = rg;\n\tlist_for_each_entry_safe(rg, trg, rg->link.prev, link) {\n\t\tif (&rg->link == head)\n\t\t\tbreak;\n\t\tif (rg->from > t)\n\t\t\tbreak;\n\n\t\t/* If this area reaches higher then extend our area to\n\t\t * include it completely.  If this is not the first area\n\t\t * which we intend to reuse, free it. */\n\t\tif (rg->to > t)\n\t\t\tt = rg->to;\n\t\tif (rg != nrg) {\n\t\t\t/* Decrement return value by the deleted range.\n\t\t\t * Another range will span this area so that by\n\t\t\t * end of routine add will be >= zero\n\t\t\t */\n\t\t\tadd -= (rg->to - rg->from);\n\t\t\tlist_del(&rg->link);\n\t\t\tkfree(rg);\n\t\t}\n\t}\n\n\tadd += (nrg->from - f);\t\t/* Added to beginning of region */\n\tnrg->from = f;\n\tadd += t - nrg->to;\t\t/* Added to end of region */\n\tnrg->to = t;\n\nout_locked:\n\tresv->adds_in_progress--;\n\tspin_unlock(&resv->lock);\n\tVM_BUG_ON(add < 0);\n\treturn add;\n}\n\n/*\n * Examine the existing reserve map and determine how many\n * huge pages in the specified range [f, t) are NOT currently\n * represented.  This routine is called before a subsequent\n * call to region_add that will actually modify the reserve\n * map to add the specified range [f, t).  region_chg does\n * not change the number of huge pages represented by the\n * map.  However, if the existing regions in the map can not\n * be expanded to represent the new range, a new file_region\n * structure is added to the map as a placeholder.  This is\n * so that the subsequent region_add call will have all the\n * regions it needs and will not fail.\n *\n * Upon entry, region_chg will also examine the cache of region descriptors\n * associated with the map.  If there are not enough descriptors cached, one\n * will be allocated for the in progress add operation.\n *\n * Returns the number of huge pages that need to be added to the existing\n * reservation map for the range [f, t).  This number is greater or equal to\n * zero.  -ENOMEM is returned if a new file_region structure or cache entry\n * is needed and can not be allocated.\n */\nstatic long region_chg(struct resv_map *resv, long f, long t)\n{\n\tstruct list_head *head = &resv->regions;\n\tstruct file_region *rg, *nrg = NULL;\n\tlong chg = 0;\n\nretry:\n\tspin_lock(&resv->lock);\nretry_locked:\n\tresv->adds_in_progress++;\n\n\t/*\n\t * Check for sufficient descriptors in the cache to accommodate\n\t * the number of in progress add operations.\n\t */\n\tif (resv->adds_in_progress > resv->region_cache_count) {\n\t\tstruct file_region *trg;\n\n\t\tVM_BUG_ON(resv->adds_in_progress - resv->region_cache_count > 1);\n\t\t/* Must drop lock to allocate a new descriptor. */\n\t\tresv->adds_in_progress--;\n\t\tspin_unlock(&resv->lock);\n\n\t\ttrg = kmalloc(sizeof(*trg), GFP_KERNEL);\n\t\tif (!trg) {\n\t\t\tkfree(nrg);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tspin_lock(&resv->lock);\n\t\tlist_add(&trg->link, &resv->region_cache);\n\t\tresv->region_cache_count++;\n\t\tgoto retry_locked;\n\t}\n\n\t/* Locate the region we are before or in. */\n\tlist_for_each_entry(rg, head, link)\n\t\tif (f <= rg->to)\n\t\t\tbreak;\n\n\t/* If we are below the current region then a new region is required.\n\t * Subtle, allocate a new region at the position but make it zero\n\t * size such that we can guarantee to record the reservation. */\n\tif (&rg->link == head || t < rg->from) {\n\t\tif (!nrg) {\n\t\t\tresv->adds_in_progress--;\n\t\t\tspin_unlock(&resv->lock);\n\t\t\tnrg = kmalloc(sizeof(*nrg), GFP_KERNEL);\n\t\t\tif (!nrg)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tnrg->from = f;\n\t\t\tnrg->to   = f;\n\t\t\tINIT_LIST_HEAD(&nrg->link);\n\t\t\tgoto retry;\n\t\t}\n\n\t\tlist_add(&nrg->link, rg->link.prev);\n\t\tchg = t - f;\n\t\tgoto out_nrg;\n\t}\n\n\t/* Round our left edge to the current segment if it encloses us. */\n\tif (f > rg->from)\n\t\tf = rg->from;\n\tchg = t - f;\n\n\t/* Check for and consume any regions we now overlap with. */\n\tlist_for_each_entry(rg, rg->link.prev, link) {\n\t\tif (&rg->link == head)\n\t\t\tbreak;\n\t\tif (rg->from > t)\n\t\t\tgoto out;\n\n\t\t/* We overlap with this area, if it extends further than\n\t\t * us then we must extend ourselves.  Account for its\n\t\t * existing reservation. */\n\t\tif (rg->to > t) {\n\t\t\tchg += rg->to - t;\n\t\t\tt = rg->to;\n\t\t}\n\t\tchg -= rg->to - rg->from;\n\t}\n\nout:\n\tspin_unlock(&resv->lock);\n\t/*  We already know we raced and no longer need the new region */\n\tkfree(nrg);\n\treturn chg;\nout_nrg:\n\tspin_unlock(&resv->lock);\n\treturn chg;\n}\n\n/*\n * Abort the in progress add operation.  The adds_in_progress field\n * of the resv_map keeps track of the operations in progress between\n * calls to region_chg and region_add.  Operations are sometimes\n * aborted after the call to region_chg.  In such cases, region_abort\n * is called to decrement the adds_in_progress counter.\n *\n * NOTE: The range arguments [f, t) are not needed or used in this\n * routine.  They are kept to make reading the calling code easier as\n * arguments will match the associated region_chg call.\n */\nstatic void region_abort(struct resv_map *resv, long f, long t)\n{\n\tspin_lock(&resv->lock);\n\tVM_BUG_ON(!resv->region_cache_count);\n\tresv->adds_in_progress--;\n\tspin_unlock(&resv->lock);\n}\n\n/*\n * Delete the specified range [f, t) from the reserve map.  If the\n * t parameter is LONG_MAX, this indicates that ALL regions after f\n * should be deleted.  Locate the regions which intersect [f, t)\n * and either trim, delete or split the existing regions.\n *\n * Returns the number of huge pages deleted from the reserve map.\n * In the normal case, the return value is zero or more.  In the\n * case where a region must be split, a new region descriptor must\n * be allocated.  If the allocation fails, -ENOMEM will be returned.\n * NOTE: If the parameter t == LONG_MAX, then we will never split\n * a region and possibly return -ENOMEM.  Callers specifying\n * t == LONG_MAX do not need to check for -ENOMEM error.\n */\nstatic long region_del(struct resv_map *resv, long f, long t)\n{\n\tstruct list_head *head = &resv->regions;\n\tstruct file_region *rg, *trg;\n\tstruct file_region *nrg = NULL;\n\tlong del = 0;\n\nretry:\n\tspin_lock(&resv->lock);\n\tlist_for_each_entry_safe(rg, trg, head, link) {\n\t\t/*\n\t\t * Skip regions before the range to be deleted.  file_region\n\t\t * ranges are normally of the form [from, to).  However, there\n\t\t * may be a \"placeholder\" entry in the map which is of the form\n\t\t * (from, to) with from == to.  Check for placeholder entries\n\t\t * at the beginning of the range to be deleted.\n\t\t */\n\t\tif (rg->to <= f && (rg->to != rg->from || rg->to != f))\n\t\t\tcontinue;\n\n\t\tif (rg->from >= t)\n\t\t\tbreak;\n\n\t\tif (f > rg->from && t < rg->to) { /* Must split region */\n\t\t\t/*\n\t\t\t * Check for an entry in the cache before dropping\n\t\t\t * lock and attempting allocation.\n\t\t\t */\n\t\t\tif (!nrg &&\n\t\t\t    resv->region_cache_count > resv->adds_in_progress) {\n\t\t\t\tnrg = list_first_entry(&resv->region_cache,\n\t\t\t\t\t\t\tstruct file_region,\n\t\t\t\t\t\t\tlink);\n\t\t\t\tlist_del(&nrg->link);\n\t\t\t\tresv->region_cache_count--;\n\t\t\t}\n\n\t\t\tif (!nrg) {\n\t\t\t\tspin_unlock(&resv->lock);\n\t\t\t\tnrg = kmalloc(sizeof(*nrg), GFP_KERNEL);\n\t\t\t\tif (!nrg)\n\t\t\t\t\treturn -ENOMEM;\n\t\t\t\tgoto retry;\n\t\t\t}\n\n\t\t\tdel += t - f;\n\n\t\t\t/* New entry for end of split region */\n\t\t\tnrg->from = t;\n\t\t\tnrg->to = rg->to;\n\t\t\tINIT_LIST_HEAD(&nrg->link);\n\n\t\t\t/* Original entry is trimmed */\n\t\t\trg->to = f;\n\n\t\t\tlist_add(&nrg->link, &rg->link);\n\t\t\tnrg = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (f <= rg->from && t >= rg->to) { /* Remove entire region */\n\t\t\tdel += rg->to - rg->from;\n\t\t\tlist_del(&rg->link);\n\t\t\tkfree(rg);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (f <= rg->from) {\t/* Trim beginning of region */\n\t\t\tdel += t - rg->from;\n\t\t\trg->from = t;\n\t\t} else {\t\t/* Trim end of region */\n\t\t\tdel += rg->to - f;\n\t\t\trg->to = f;\n\t\t}\n\t}\n\n\tspin_unlock(&resv->lock);\n\tkfree(nrg);\n\treturn del;\n}\n\n/*\n * A rare out of memory error was encountered which prevented removal of\n * the reserve map region for a page.  The huge page itself was free'ed\n * and removed from the page cache.  This routine will adjust the subpool\n * usage count, and the global reserve count if needed.  By incrementing\n * these counts, the reserve map entry which could not be deleted will\n * appear as a \"reserved\" entry instead of simply dangling with incorrect\n * counts.\n */\nvoid hugetlb_fix_reserve_counts(struct inode *inode)\n{\n\tstruct hugepage_subpool *spool = subpool_inode(inode);\n\tlong rsv_adjust;\n\n\trsv_adjust = hugepage_subpool_get_pages(spool, 1);\n\tif (rsv_adjust) {\n\t\tstruct hstate *h = hstate_inode(inode);\n\n\t\thugetlb_acct_memory(h, 1);\n\t}\n}\n\n/*\n * Count and return the number of huge pages in the reserve map\n * that intersect with the range [f, t).\n */\nstatic long region_count(struct resv_map *resv, long f, long t)\n{\n\tstruct list_head *head = &resv->regions;\n\tstruct file_region *rg;\n\tlong chg = 0;\n\n\tspin_lock(&resv->lock);\n\t/* Locate each segment we overlap with, and count that overlap. */\n\tlist_for_each_entry(rg, head, link) {\n\t\tlong seg_from;\n\t\tlong seg_to;\n\n\t\tif (rg->to <= f)\n\t\t\tcontinue;\n\t\tif (rg->from >= t)\n\t\t\tbreak;\n\n\t\tseg_from = max(rg->from, f);\n\t\tseg_to = min(rg->to, t);\n\n\t\tchg += seg_to - seg_from;\n\t}\n\tspin_unlock(&resv->lock);\n\n\treturn chg;\n}\n\n/*\n * Convert the address within this vma to the page offset within\n * the mapping, in pagecache page units; huge pages here.\n */\nstatic pgoff_t vma_hugecache_offset(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\treturn ((address - vma->vm_start) >> huge_page_shift(h)) +\n\t\t\t(vma->vm_pgoff >> huge_page_order(h));\n}\n\npgoff_t linear_hugepage_index(struct vm_area_struct *vma,\n\t\t\t\t     unsigned long address)\n{\n\treturn vma_hugecache_offset(hstate_vma(vma), vma, address);\n}\nEXPORT_SYMBOL_GPL(linear_hugepage_index);\n\n/*\n * Return the size of the pages allocated when backing a VMA. In the majority\n * cases this will be same size as used by the page table entries.\n */\nunsigned long vma_kernel_pagesize(struct vm_area_struct *vma)\n{\n\tstruct hstate *hstate;\n\n\tif (!is_vm_hugetlb_page(vma))\n\t\treturn PAGE_SIZE;\n\n\thstate = hstate_vma(vma);\n\n\treturn 1UL << huge_page_shift(hstate);\n}\nEXPORT_SYMBOL_GPL(vma_kernel_pagesize);\n\n/*\n * Return the page size being used by the MMU to back a VMA. In the majority\n * of cases, the page size used by the kernel matches the MMU size. On\n * architectures where it differs, an architecture-specific version of this\n * function is required.\n */\n#ifndef vma_mmu_pagesize\nunsigned long vma_mmu_pagesize(struct vm_area_struct *vma)\n{\n\treturn vma_kernel_pagesize(vma);\n}\n#endif\n\n/*\n * Flags for MAP_PRIVATE reservations.  These are stored in the bottom\n * bits of the reservation map pointer, which are always clear due to\n * alignment.\n */\n#define HPAGE_RESV_OWNER    (1UL << 0)\n#define HPAGE_RESV_UNMAPPED (1UL << 1)\n#define HPAGE_RESV_MASK (HPAGE_RESV_OWNER | HPAGE_RESV_UNMAPPED)\n\n/*\n * These helpers are used to track how many pages are reserved for\n * faults in a MAP_PRIVATE mapping. Only the process that called mmap()\n * is guaranteed to have their future faults succeed.\n *\n * With the exception of reset_vma_resv_huge_pages() which is called at fork(),\n * the reserve counters are updated with the hugetlb_lock held. It is safe\n * to reset the VMA at fork() time as it is not in use yet and there is no\n * chance of the global counters getting corrupted as a result of the values.\n *\n * The private mapping reservation is represented in a subtly different\n * manner to a shared mapping.  A shared mapping has a region map associated\n * with the underlying file, this region map represents the backing file\n * pages which have ever had a reservation assigned which this persists even\n * after the page is instantiated.  A private mapping has a region map\n * associated with the original mmap which is attached to all VMAs which\n * reference it, this region map represents those offsets which have consumed\n * reservation ie. where pages have been instantiated.\n */\nstatic unsigned long get_vma_private_data(struct vm_area_struct *vma)\n{\n\treturn (unsigned long)vma->vm_private_data;\n}\n\nstatic void set_vma_private_data(struct vm_area_struct *vma,\n\t\t\t\t\t\t\tunsigned long value)\n{\n\tvma->vm_private_data = (void *)value;\n}\n\nstruct resv_map *resv_map_alloc(void)\n{\n\tstruct resv_map *resv_map = kmalloc(sizeof(*resv_map), GFP_KERNEL);\n\tstruct file_region *rg = kmalloc(sizeof(*rg), GFP_KERNEL);\n\n\tif (!resv_map || !rg) {\n\t\tkfree(resv_map);\n\t\tkfree(rg);\n\t\treturn NULL;\n\t}\n\n\tkref_init(&resv_map->refs);\n\tspin_lock_init(&resv_map->lock);\n\tINIT_LIST_HEAD(&resv_map->regions);\n\n\tresv_map->adds_in_progress = 0;\n\n\tINIT_LIST_HEAD(&resv_map->region_cache);\n\tlist_add(&rg->link, &resv_map->region_cache);\n\tresv_map->region_cache_count = 1;\n\n\treturn resv_map;\n}\n\nvoid resv_map_release(struct kref *ref)\n{\n\tstruct resv_map *resv_map = container_of(ref, struct resv_map, refs);\n\tstruct list_head *head = &resv_map->region_cache;\n\tstruct file_region *rg, *trg;\n\n\t/* Clear out any active regions before we release the map. */\n\tregion_del(resv_map, 0, LONG_MAX);\n\n\t/* ... and any entries left in the cache */\n\tlist_for_each_entry_safe(rg, trg, head, link) {\n\t\tlist_del(&rg->link);\n\t\tkfree(rg);\n\t}\n\n\tVM_BUG_ON(resv_map->adds_in_progress);\n\n\tkfree(resv_map);\n}\n\nstatic inline struct resv_map *inode_resv_map(struct inode *inode)\n{\n\treturn inode->i_mapping->private_data;\n}\n\nstatic struct resv_map *vma_resv_map(struct vm_area_struct *vma)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\tstruct address_space *mapping = vma->vm_file->f_mapping;\n\t\tstruct inode *inode = mapping->host;\n\n\t\treturn inode_resv_map(inode);\n\n\t} else {\n\t\treturn (struct resv_map *)(get_vma_private_data(vma) &\n\t\t\t\t\t\t\t~HPAGE_RESV_MASK);\n\t}\n}\n\nstatic void set_vma_resv_map(struct vm_area_struct *vma, struct resv_map *map)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\tVM_BUG_ON_VMA(vma->vm_flags & VM_MAYSHARE, vma);\n\n\tset_vma_private_data(vma, (get_vma_private_data(vma) &\n\t\t\t\tHPAGE_RESV_MASK) | (unsigned long)map);\n}\n\nstatic void set_vma_resv_flags(struct vm_area_struct *vma, unsigned long flags)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\tVM_BUG_ON_VMA(vma->vm_flags & VM_MAYSHARE, vma);\n\n\tset_vma_private_data(vma, get_vma_private_data(vma) | flags);\n}\n\nstatic int is_vma_resv_set(struct vm_area_struct *vma, unsigned long flag)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\n\treturn (get_vma_private_data(vma) & flag) != 0;\n}\n\n/* Reset counters to 0 and clear all HPAGE_RESV_* flags */\nvoid reset_vma_resv_huge_pages(struct vm_area_struct *vma)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\tif (!(vma->vm_flags & VM_MAYSHARE))\n\t\tvma->vm_private_data = (void *)0;\n}\n\n/* Returns true if the VMA has associated reserve pages */\nstatic bool vma_has_reserves(struct vm_area_struct *vma, long chg)\n{\n\tif (vma->vm_flags & VM_NORESERVE) {\n\t\t/*\n\t\t * This address is already reserved by other process(chg == 0),\n\t\t * so, we should decrement reserved count. Without decrementing,\n\t\t * reserve count remains after releasing inode, because this\n\t\t * allocated page will go into page cache and is regarded as\n\t\t * coming from reserved pool in releasing step.  Currently, we\n\t\t * don't have any other solution to deal with this situation\n\t\t * properly, so add work-around here.\n\t\t */\n\t\tif (vma->vm_flags & VM_MAYSHARE && chg == 0)\n\t\t\treturn true;\n\t\telse\n\t\t\treturn false;\n\t}\n\n\t/* Shared mappings always use reserves */\n\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\t/*\n\t\t * We know VM_NORESERVE is not set.  Therefore, there SHOULD\n\t\t * be a region map for all pages.  The only situation where\n\t\t * there is no region map is if a hole was punched via\n\t\t * fallocate.  In this case, there really are no reverves to\n\t\t * use.  This situation is indicated if chg != 0.\n\t\t */\n\t\tif (chg)\n\t\t\treturn false;\n\t\telse\n\t\t\treturn true;\n\t}\n\n\t/*\n\t * Only the process that called mmap() has reserves for\n\t * private mappings.\n\t */\n\tif (is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {\n\t\t/*\n\t\t * Like the shared case above, a hole punch or truncate\n\t\t * could have been performed on the private mapping.\n\t\t * Examine the value of chg to determine if reserves\n\t\t * actually exist or were previously consumed.\n\t\t * Very Subtle - The value of chg comes from a previous\n\t\t * call to vma_needs_reserves().  The reserve map for\n\t\t * private mappings has different (opposite) semantics\n\t\t * than that of shared mappings.  vma_needs_reserves()\n\t\t * has already taken this difference in semantics into\n\t\t * account.  Therefore, the meaning of chg is the same\n\t\t * as in the shared case above.  Code could easily be\n\t\t * combined, but keeping it separate draws attention to\n\t\t * subtle differences.\n\t\t */\n\t\tif (chg)\n\t\t\treturn false;\n\t\telse\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void enqueue_huge_page(struct hstate *h, struct page *page)\n{\n\tint nid = page_to_nid(page);\n\tlist_move(&page->lru, &h->hugepage_freelists[nid]);\n\th->free_huge_pages++;\n\th->free_huge_pages_node[nid]++;\n}\n\nstatic struct page *dequeue_huge_page_node_exact(struct hstate *h, int nid)\n{\n\tstruct page *page;\n\n\tlist_for_each_entry(page, &h->hugepage_freelists[nid], lru)\n\t\tif (!PageHWPoison(page))\n\t\t\tbreak;\n\t/*\n\t * if 'non-isolated free hugepage' not found on the list,\n\t * the allocation fails.\n\t */\n\tif (&h->hugepage_freelists[nid] == &page->lru)\n\t\treturn NULL;\n\tlist_move(&page->lru, &h->hugepage_activelist);\n\tset_page_refcounted(page);\n\th->free_huge_pages--;\n\th->free_huge_pages_node[nid]--;\n\treturn page;\n}\n\nstatic struct page *dequeue_huge_page_nodemask(struct hstate *h, gfp_t gfp_mask, int nid,\n\t\tnodemask_t *nmask)\n{\n\tunsigned int cpuset_mems_cookie;\n\tstruct zonelist *zonelist;\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\tint node = -1;\n\n\tzonelist = node_zonelist(nid, gfp_mask);\n\nretry_cpuset:\n\tcpuset_mems_cookie = read_mems_allowed_begin();\n\tfor_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {\n\t\tstruct page *page;\n\n\t\tif (!cpuset_zone_allowed(zone, gfp_mask))\n\t\t\tcontinue;\n\t\t/*\n\t\t * no need to ask again on the same node. Pool is node rather than\n\t\t * zone aware\n\t\t */\n\t\tif (zone_to_nid(zone) == node)\n\t\t\tcontinue;\n\t\tnode = zone_to_nid(zone);\n\n\t\tpage = dequeue_huge_page_node_exact(h, node);\n\t\tif (page)\n\t\t\treturn page;\n\t}\n\tif (unlikely(read_mems_allowed_retry(cpuset_mems_cookie)))\n\t\tgoto retry_cpuset;\n\n\treturn NULL;\n}\n\n/* Movability of hugepages depends on migration support. */\nstatic inline gfp_t htlb_alloc_mask(struct hstate *h)\n{\n\tif (hugepages_treat_as_movable || hugepage_migration_supported(h))\n\t\treturn GFP_HIGHUSER_MOVABLE;\n\telse\n\t\treturn GFP_HIGHUSER;\n}\n\nstatic struct page *dequeue_huge_page_vma(struct hstate *h,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tunsigned long address, int avoid_reserve,\n\t\t\t\tlong chg)\n{\n\tstruct page *page;\n\tstruct mempolicy *mpol;\n\tgfp_t gfp_mask;\n\tnodemask_t *nodemask;\n\tint nid;\n\n\t/*\n\t * A child process with MAP_PRIVATE mappings created by their parent\n\t * have no page reserves. This check ensures that reservations are\n\t * not \"stolen\". The child may still get SIGKILLed\n\t */\n\tif (!vma_has_reserves(vma, chg) &&\n\t\t\th->free_huge_pages - h->resv_huge_pages == 0)\n\t\tgoto err;\n\n\t/* If reserves cannot be used, ensure enough pages are in the pool */\n\tif (avoid_reserve && h->free_huge_pages - h->resv_huge_pages == 0)\n\t\tgoto err;\n\n\tgfp_mask = htlb_alloc_mask(h);\n\tnid = huge_node(vma, address, gfp_mask, &mpol, &nodemask);\n\tpage = dequeue_huge_page_nodemask(h, gfp_mask, nid, nodemask);\n\tif (page && !avoid_reserve && vma_has_reserves(vma, chg)) {\n\t\tSetPagePrivate(page);\n\t\th->resv_huge_pages--;\n\t}\n\n\tmpol_cond_put(mpol);\n\treturn page;\n\nerr:\n\treturn NULL;\n}\n\n/*\n * common helper functions for hstate_next_node_to_{alloc|free}.\n * We may have allocated or freed a huge page based on a different\n * nodes_allowed previously, so h->next_node_to_{alloc|free} might\n * be outside of *nodes_allowed.  Ensure that we use an allowed\n * node for alloc or free.\n */\nstatic int next_node_allowed(int nid, nodemask_t *nodes_allowed)\n{\n\tnid = next_node_in(nid, *nodes_allowed);\n\tVM_BUG_ON(nid >= MAX_NUMNODES);\n\n\treturn nid;\n}\n\nstatic int get_valid_node_allowed(int nid, nodemask_t *nodes_allowed)\n{\n\tif (!node_isset(nid, *nodes_allowed))\n\t\tnid = next_node_allowed(nid, nodes_allowed);\n\treturn nid;\n}\n\n/*\n * returns the previously saved node [\"this node\"] from which to\n * allocate a persistent huge page for the pool and advance the\n * next node from which to allocate, handling wrap at end of node\n * mask.\n */\nstatic int hstate_next_node_to_alloc(struct hstate *h,\n\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n\tint nid;\n\n\tVM_BUG_ON(!nodes_allowed);\n\n\tnid = get_valid_node_allowed(h->next_nid_to_alloc, nodes_allowed);\n\th->next_nid_to_alloc = next_node_allowed(nid, nodes_allowed);\n\n\treturn nid;\n}\n\n/*\n * helper for free_pool_huge_page() - return the previously saved\n * node [\"this node\"] from which to free a huge page.  Advance the\n * next node id whether or not we find a free huge page to free so\n * that the next attempt to free addresses the next node.\n */\nstatic int hstate_next_node_to_free(struct hstate *h, nodemask_t *nodes_allowed)\n{\n\tint nid;\n\n\tVM_BUG_ON(!nodes_allowed);\n\n\tnid = get_valid_node_allowed(h->next_nid_to_free, nodes_allowed);\n\th->next_nid_to_free = next_node_allowed(nid, nodes_allowed);\n\n\treturn nid;\n}\n\n#define for_each_node_mask_to_alloc(hs, nr_nodes, node, mask)\t\t\\\n\tfor (nr_nodes = nodes_weight(*mask);\t\t\t\t\\\n\t\tnr_nodes > 0 &&\t\t\t\t\t\t\\\n\t\t((node = hstate_next_node_to_alloc(hs, mask)) || 1);\t\\\n\t\tnr_nodes--)\n\n#define for_each_node_mask_to_free(hs, nr_nodes, node, mask)\t\t\\\n\tfor (nr_nodes = nodes_weight(*mask);\t\t\t\t\\\n\t\tnr_nodes > 0 &&\t\t\t\t\t\t\\\n\t\t((node = hstate_next_node_to_free(hs, mask)) || 1);\t\\\n\t\tnr_nodes--)\n\n#ifdef CONFIG_ARCH_HAS_GIGANTIC_PAGE\nstatic void destroy_compound_gigantic_page(struct page *page,\n\t\t\t\t\tunsigned int order)\n{\n\tint i;\n\tint nr_pages = 1 << order;\n\tstruct page *p = page + 1;\n\n\tatomic_set(compound_mapcount_ptr(page), 0);\n\tfor (i = 1; i < nr_pages; i++, p = mem_map_next(p, page, i)) {\n\t\tclear_compound_head(p);\n\t\tset_page_refcounted(p);\n\t}\n\n\tset_compound_order(page, 0);\n\t__ClearPageHead(page);\n}\n\nstatic void free_gigantic_page(struct page *page, unsigned int order)\n{\n\tfree_contig_range(page_to_pfn(page), 1 << order);\n}\n\nstatic int __alloc_gigantic_page(unsigned long start_pfn,\n\t\t\t\tunsigned long nr_pages)\n{\n\tunsigned long end_pfn = start_pfn + nr_pages;\n\treturn alloc_contig_range(start_pfn, end_pfn, MIGRATE_MOVABLE,\n\t\t\t\t  GFP_KERNEL);\n}\n\nstatic bool pfn_range_valid_gigantic(struct zone *z,\n\t\t\tunsigned long start_pfn, unsigned long nr_pages)\n{\n\tunsigned long i, end_pfn = start_pfn + nr_pages;\n\tstruct page *page;\n\n\tfor (i = start_pfn; i < end_pfn; i++) {\n\t\tif (!pfn_valid(i))\n\t\t\treturn false;\n\n\t\tpage = pfn_to_page(i);\n\n\t\tif (page_zone(page) != z)\n\t\t\treturn false;\n\n\t\tif (PageReserved(page))\n\t\t\treturn false;\n\n\t\tif (page_count(page) > 0)\n\t\t\treturn false;\n\n\t\tif (PageHuge(page))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool zone_spans_last_pfn(const struct zone *zone,\n\t\t\tunsigned long start_pfn, unsigned long nr_pages)\n{\n\tunsigned long last_pfn = start_pfn + nr_pages - 1;\n\treturn zone_spans_pfn(zone, last_pfn);\n}\n\nstatic struct page *alloc_gigantic_page(int nid, unsigned int order)\n{\n\tunsigned long nr_pages = 1 << order;\n\tunsigned long ret, pfn, flags;\n\tstruct zone *z;\n\n\tz = NODE_DATA(nid)->node_zones;\n\tfor (; z - NODE_DATA(nid)->node_zones < MAX_NR_ZONES; z++) {\n\t\tspin_lock_irqsave(&z->lock, flags);\n\n\t\tpfn = ALIGN(z->zone_start_pfn, nr_pages);\n\t\twhile (zone_spans_last_pfn(z, pfn, nr_pages)) {\n\t\t\tif (pfn_range_valid_gigantic(z, pfn, nr_pages)) {\n\t\t\t\t/*\n\t\t\t\t * We release the zone lock here because\n\t\t\t\t * alloc_contig_range() will also lock the zone\n\t\t\t\t * at some point. If there's an allocation\n\t\t\t\t * spinning on this lock, it may win the race\n\t\t\t\t * and cause alloc_contig_range() to fail...\n\t\t\t\t */\n\t\t\t\tspin_unlock_irqrestore(&z->lock, flags);\n\t\t\t\tret = __alloc_gigantic_page(pfn, nr_pages);\n\t\t\t\tif (!ret)\n\t\t\t\t\treturn pfn_to_page(pfn);\n\t\t\t\tspin_lock_irqsave(&z->lock, flags);\n\t\t\t}\n\t\t\tpfn += nr_pages;\n\t\t}\n\n\t\tspin_unlock_irqrestore(&z->lock, flags);\n\t}\n\n\treturn NULL;\n}\n\nstatic void prep_new_huge_page(struct hstate *h, struct page *page, int nid);\nstatic void prep_compound_gigantic_page(struct page *page, unsigned int order);\n\nstatic struct page *alloc_fresh_gigantic_page_node(struct hstate *h, int nid)\n{\n\tstruct page *page;\n\n\tpage = alloc_gigantic_page(nid, huge_page_order(h));\n\tif (page) {\n\t\tprep_compound_gigantic_page(page, huge_page_order(h));\n\t\tprep_new_huge_page(h, page, nid);\n\t}\n\n\treturn page;\n}\n\nstatic int alloc_fresh_gigantic_page(struct hstate *h,\n\t\t\t\tnodemask_t *nodes_allowed)\n{\n\tstruct page *page = NULL;\n\tint nr_nodes, node;\n\n\tfor_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {\n\t\tpage = alloc_fresh_gigantic_page_node(h, node);\n\t\tif (page)\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n#else /* !CONFIG_ARCH_HAS_GIGANTIC_PAGE */\nstatic inline bool gigantic_page_supported(void) { return false; }\nstatic inline void free_gigantic_page(struct page *page, unsigned int order) { }\nstatic inline void destroy_compound_gigantic_page(struct page *page,\n\t\t\t\t\t\tunsigned int order) { }\nstatic inline int alloc_fresh_gigantic_page(struct hstate *h,\n\t\t\t\t\tnodemask_t *nodes_allowed) { return 0; }\n#endif\n\nstatic void update_and_free_page(struct hstate *h, struct page *page)\n{\n\tint i;\n\n\tif (hstate_is_gigantic(h) && !gigantic_page_supported())\n\t\treturn;\n\n\th->nr_huge_pages--;\n\th->nr_huge_pages_node[page_to_nid(page)]--;\n\tfor (i = 0; i < pages_per_huge_page(h); i++) {\n\t\tpage[i].flags &= ~(1 << PG_locked | 1 << PG_error |\n\t\t\t\t1 << PG_referenced | 1 << PG_dirty |\n\t\t\t\t1 << PG_active | 1 << PG_private |\n\t\t\t\t1 << PG_writeback);\n\t}\n\tVM_BUG_ON_PAGE(hugetlb_cgroup_from_page(page), page);\n\tset_compound_page_dtor(page, NULL_COMPOUND_DTOR);\n\tset_page_refcounted(page);\n\tif (hstate_is_gigantic(h)) {\n\t\tdestroy_compound_gigantic_page(page, huge_page_order(h));\n\t\tfree_gigantic_page(page, huge_page_order(h));\n\t} else {\n\t\t__free_pages(page, huge_page_order(h));\n\t}\n}\n\nstruct hstate *size_to_hstate(unsigned long size)\n{\n\tstruct hstate *h;\n\n\tfor_each_hstate(h) {\n\t\tif (huge_page_size(h) == size)\n\t\t\treturn h;\n\t}\n\treturn NULL;\n}\n\n/*\n * Test to determine whether the hugepage is \"active/in-use\" (i.e. being linked\n * to hstate->hugepage_activelist.)\n *\n * This function can be called for tail pages, but never returns true for them.\n */\nbool page_huge_active(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!PageHuge(page), page);\n\treturn PageHead(page) && PagePrivate(&page[1]);\n}\n\n/* never called for tail page */\nstatic void set_page_huge_active(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!PageHeadHuge(page), page);\n\tSetPagePrivate(&page[1]);\n}\n\nstatic void clear_page_huge_active(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!PageHeadHuge(page), page);\n\tClearPagePrivate(&page[1]);\n}\n\nvoid free_huge_page(struct page *page)\n{\n\t/*\n\t * Can't pass hstate in here because it is called from the\n\t * compound page destructor.\n\t */\n\tstruct hstate *h = page_hstate(page);\n\tint nid = page_to_nid(page);\n\tstruct hugepage_subpool *spool =\n\t\t(struct hugepage_subpool *)page_private(page);\n\tbool restore_reserve;\n\n\tset_page_private(page, 0);\n\tpage->mapping = NULL;\n\tVM_BUG_ON_PAGE(page_count(page), page);\n\tVM_BUG_ON_PAGE(page_mapcount(page), page);\n\trestore_reserve = PagePrivate(page);\n\tClearPagePrivate(page);\n\n\t/*\n\t * A return code of zero implies that the subpool will be under its\n\t * minimum size if the reservation is not restored after page is free.\n\t * Therefore, force restore_reserve operation.\n\t */\n\tif (hugepage_subpool_put_pages(spool, 1) == 0)\n\t\trestore_reserve = true;\n\n\tspin_lock(&hugetlb_lock);\n\tclear_page_huge_active(page);\n\thugetlb_cgroup_uncharge_page(hstate_index(h),\n\t\t\t\t     pages_per_huge_page(h), page);\n\tif (restore_reserve)\n\t\th->resv_huge_pages++;\n\n\tif (h->surplus_huge_pages_node[nid]) {\n\t\t/* remove the page from active list */\n\t\tlist_del(&page->lru);\n\t\tupdate_and_free_page(h, page);\n\t\th->surplus_huge_pages--;\n\t\th->surplus_huge_pages_node[nid]--;\n\t} else {\n\t\tarch_clear_hugepage_flags(page);\n\t\tenqueue_huge_page(h, page);\n\t}\n\tspin_unlock(&hugetlb_lock);\n}\n\nstatic void prep_new_huge_page(struct hstate *h, struct page *page, int nid)\n{\n\tINIT_LIST_HEAD(&page->lru);\n\tset_compound_page_dtor(page, HUGETLB_PAGE_DTOR);\n\tspin_lock(&hugetlb_lock);\n\tset_hugetlb_cgroup(page, NULL);\n\th->nr_huge_pages++;\n\th->nr_huge_pages_node[nid]++;\n\tspin_unlock(&hugetlb_lock);\n\tput_page(page); /* free it into the hugepage allocator */\n}\n\nstatic void prep_compound_gigantic_page(struct page *page, unsigned int order)\n{\n\tint i;\n\tint nr_pages = 1 << order;\n\tstruct page *p = page + 1;\n\n\t/* we rely on prep_new_huge_page to set the destructor */\n\tset_compound_order(page, order);\n\t__ClearPageReserved(page);\n\t__SetPageHead(page);\n\tfor (i = 1; i < nr_pages; i++, p = mem_map_next(p, page, i)) {\n\t\t/*\n\t\t * For gigantic hugepages allocated through bootmem at\n\t\t * boot, it's safer to be consistent with the not-gigantic\n\t\t * hugepages and clear the PG_reserved bit from all tail pages\n\t\t * too.  Otherwse drivers using get_user_pages() to access tail\n\t\t * pages may get the reference counting wrong if they see\n\t\t * PG_reserved set on a tail page (despite the head page not\n\t\t * having PG_reserved set).  Enforcing this consistency between\n\t\t * head and tail pages allows drivers to optimize away a check\n\t\t * on the head page when they need know if put_page() is needed\n\t\t * after get_user_pages().\n\t\t */\n\t\t__ClearPageReserved(p);\n\t\tset_page_count(p, 0);\n\t\tset_compound_head(p, page);\n\t}\n\tatomic_set(compound_mapcount_ptr(page), -1);\n}\n\n/*\n * PageHuge() only returns true for hugetlbfs pages, but not for normal or\n * transparent huge pages.  See the PageTransHuge() documentation for more\n * details.\n */\nint PageHuge(struct page *page)\n{\n\tif (!PageCompound(page))\n\t\treturn 0;\n\n\tpage = compound_head(page);\n\treturn page[1].compound_dtor == HUGETLB_PAGE_DTOR;\n}\nEXPORT_SYMBOL_GPL(PageHuge);\n\n/*\n * PageHeadHuge() only returns true for hugetlbfs head page, but not for\n * normal or transparent huge pages.\n */\nint PageHeadHuge(struct page *page_head)\n{\n\tif (!PageHead(page_head))\n\t\treturn 0;\n\n\treturn get_compound_page_dtor(page_head) == free_huge_page;\n}\n\npgoff_t __basepage_index(struct page *page)\n{\n\tstruct page *page_head = compound_head(page);\n\tpgoff_t index = page_index(page_head);\n\tunsigned long compound_idx;\n\n\tif (!PageHuge(page_head))\n\t\treturn page_index(page);\n\n\tif (compound_order(page_head) >= MAX_ORDER)\n\t\tcompound_idx = page_to_pfn(page) - page_to_pfn(page_head);\n\telse\n\t\tcompound_idx = page - page_head;\n\n\treturn (index << compound_order(page_head)) + compound_idx;\n}\n\nstatic struct page *alloc_fresh_huge_page_node(struct hstate *h, int nid)\n{\n\tstruct page *page;\n\n\tpage = __alloc_pages_node(nid,\n\t\thtlb_alloc_mask(h)|__GFP_COMP|__GFP_THISNODE|\n\t\t\t\t\t\t__GFP_RETRY_MAYFAIL|__GFP_NOWARN,\n\t\thuge_page_order(h));\n\tif (page) {\n\t\tprep_new_huge_page(h, page, nid);\n\t}\n\n\treturn page;\n}\n\nstatic int alloc_fresh_huge_page(struct hstate *h, nodemask_t *nodes_allowed)\n{\n\tstruct page *page;\n\tint nr_nodes, node;\n\tint ret = 0;\n\n\tfor_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {\n\t\tpage = alloc_fresh_huge_page_node(h, node);\n\t\tif (page) {\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (ret)\n\t\tcount_vm_event(HTLB_BUDDY_PGALLOC);\n\telse\n\t\tcount_vm_event(HTLB_BUDDY_PGALLOC_FAIL);\n\n\treturn ret;\n}\n\n/*\n * Free huge page from pool from next node to free.\n * Attempt to keep persistent huge pages more or less\n * balanced over allowed nodes.\n * Called with hugetlb_lock locked.\n */\nstatic int free_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed,\n\t\t\t\t\t\t\t bool acct_surplus)\n{\n\tint nr_nodes, node;\n\tint ret = 0;\n\n\tfor_each_node_mask_to_free(h, nr_nodes, node, nodes_allowed) {\n\t\t/*\n\t\t * If we're returning unused surplus pages, only examine\n\t\t * nodes with surplus pages.\n\t\t */\n\t\tif ((!acct_surplus || h->surplus_huge_pages_node[node]) &&\n\t\t    !list_empty(&h->hugepage_freelists[node])) {\n\t\t\tstruct page *page =\n\t\t\t\tlist_entry(h->hugepage_freelists[node].next,\n\t\t\t\t\t  struct page, lru);\n\t\t\tlist_del(&page->lru);\n\t\t\th->free_huge_pages--;\n\t\t\th->free_huge_pages_node[node]--;\n\t\t\tif (acct_surplus) {\n\t\t\t\th->surplus_huge_pages--;\n\t\t\t\th->surplus_huge_pages_node[node]--;\n\t\t\t}\n\t\t\tupdate_and_free_page(h, page);\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n/*\n * Dissolve a given free hugepage into free buddy pages. This function does\n * nothing for in-use (including surplus) hugepages. Returns -EBUSY if the\n * number of free hugepages would be reduced below the number of reserved\n * hugepages.\n */\nint dissolve_free_huge_page(struct page *page)\n{\n\tint rc = 0;\n\n\tspin_lock(&hugetlb_lock);\n\tif (PageHuge(page) && !page_count(page)) {\n\t\tstruct page *head = compound_head(page);\n\t\tstruct hstate *h = page_hstate(head);\n\t\tint nid = page_to_nid(head);\n\t\tif (h->free_huge_pages - h->resv_huge_pages == 0) {\n\t\t\trc = -EBUSY;\n\t\t\tgoto out;\n\t\t}\n\t\t/*\n\t\t * Move PageHWPoison flag from head page to the raw error page,\n\t\t * which makes any subpages rather than the error page reusable.\n\t\t */\n\t\tif (PageHWPoison(head) && page != head) {\n\t\t\tSetPageHWPoison(page);\n\t\t\tClearPageHWPoison(head);\n\t\t}\n\t\tlist_del(&head->lru);\n\t\th->free_huge_pages--;\n\t\th->free_huge_pages_node[nid]--;\n\t\th->max_huge_pages--;\n\t\tupdate_and_free_page(h, head);\n\t}\nout:\n\tspin_unlock(&hugetlb_lock);\n\treturn rc;\n}\n\n/*\n * Dissolve free hugepages in a given pfn range. Used by memory hotplug to\n * make specified memory blocks removable from the system.\n * Note that this will dissolve a free gigantic hugepage completely, if any\n * part of it lies within the given range.\n * Also note that if dissolve_free_huge_page() returns with an error, all\n * free hugepages that were dissolved before that error are lost.\n */\nint dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tunsigned long pfn;\n\tstruct page *page;\n\tint rc = 0;\n\n\tif (!hugepages_supported())\n\t\treturn rc;\n\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn += 1 << minimum_order) {\n\t\tpage = pfn_to_page(pfn);\n\t\tif (PageHuge(page) && !page_count(page)) {\n\t\t\trc = dissolve_free_huge_page(page);\n\t\t\tif (rc)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn rc;\n}\n\nstatic struct page *__hugetlb_alloc_buddy_huge_page(struct hstate *h,\n\t\tgfp_t gfp_mask, int nid, nodemask_t *nmask)\n{\n\tint order = huge_page_order(h);\n\n\tgfp_mask |= __GFP_COMP|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;\n\tif (nid == NUMA_NO_NODE)\n\t\tnid = numa_mem_id();\n\treturn __alloc_pages_nodemask(gfp_mask, order, nid, nmask);\n}\n\nstatic struct page *__alloc_buddy_huge_page(struct hstate *h, gfp_t gfp_mask,\n\t\tint nid, nodemask_t *nmask)\n{\n\tstruct page *page;\n\tunsigned int r_nid;\n\n\tif (hstate_is_gigantic(h))\n\t\treturn NULL;\n\n\t/*\n\t * Assume we will successfully allocate the surplus page to\n\t * prevent racing processes from causing the surplus to exceed\n\t * overcommit\n\t *\n\t * This however introduces a different race, where a process B\n\t * tries to grow the static hugepage pool while alloc_pages() is\n\t * called by process A. B will only examine the per-node\n\t * counters in determining if surplus huge pages can be\n\t * converted to normal huge pages in adjust_pool_surplus(). A\n\t * won't be able to increment the per-node counter, until the\n\t * lock is dropped by B, but B doesn't drop hugetlb_lock until\n\t * no more huge pages can be converted from surplus to normal\n\t * state (and doesn't try to convert again). Thus, we have a\n\t * case where a surplus huge page exists, the pool is grown, and\n\t * the surplus huge page still exists after, even though it\n\t * should just have been converted to a normal huge page. This\n\t * does not leak memory, though, as the hugepage will be freed\n\t * once it is out of use. It also does not allow the counters to\n\t * go out of whack in adjust_pool_surplus() as we don't modify\n\t * the node values until we've gotten the hugepage and only the\n\t * per-node value is checked there.\n\t */\n\tspin_lock(&hugetlb_lock);\n\tif (h->surplus_huge_pages >= h->nr_overcommit_huge_pages) {\n\t\tspin_unlock(&hugetlb_lock);\n\t\treturn NULL;\n\t} else {\n\t\th->nr_huge_pages++;\n\t\th->surplus_huge_pages++;\n\t}\n\tspin_unlock(&hugetlb_lock);\n\n\tpage = __hugetlb_alloc_buddy_huge_page(h, gfp_mask, nid, nmask);\n\n\tspin_lock(&hugetlb_lock);\n\tif (page) {\n\t\tINIT_LIST_HEAD(&page->lru);\n\t\tr_nid = page_to_nid(page);\n\t\tset_compound_page_dtor(page, HUGETLB_PAGE_DTOR);\n\t\tset_hugetlb_cgroup(page, NULL);\n\t\t/*\n\t\t * We incremented the global counters already\n\t\t */\n\t\th->nr_huge_pages_node[r_nid]++;\n\t\th->surplus_huge_pages_node[r_nid]++;\n\t\t__count_vm_event(HTLB_BUDDY_PGALLOC);\n\t} else {\n\t\th->nr_huge_pages--;\n\t\th->surplus_huge_pages--;\n\t\t__count_vm_event(HTLB_BUDDY_PGALLOC_FAIL);\n\t}\n\tspin_unlock(&hugetlb_lock);\n\n\treturn page;\n}\n\n/*\n * Use the VMA's mpolicy to allocate a huge page from the buddy.\n */\nstatic\nstruct page *__alloc_buddy_huge_page_with_mpol(struct hstate *h,\n\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\tstruct page *page;\n\tstruct mempolicy *mpol;\n\tgfp_t gfp_mask = htlb_alloc_mask(h);\n\tint nid;\n\tnodemask_t *nodemask;\n\n\tnid = huge_node(vma, addr, gfp_mask, &mpol, &nodemask);\n\tpage = __alloc_buddy_huge_page(h, gfp_mask, nid, nodemask);\n\tmpol_cond_put(mpol);\n\n\treturn page;\n}\n\n/*\n * This allocation function is useful in the context where vma is irrelevant.\n * E.g. soft-offlining uses this function because it only cares physical\n * address of error page.\n */\nstruct page *alloc_huge_page_node(struct hstate *h, int nid)\n{\n\tgfp_t gfp_mask = htlb_alloc_mask(h);\n\tstruct page *page = NULL;\n\n\tif (nid != NUMA_NO_NODE)\n\t\tgfp_mask |= __GFP_THISNODE;\n\n\tspin_lock(&hugetlb_lock);\n\tif (h->free_huge_pages - h->resv_huge_pages > 0)\n\t\tpage = dequeue_huge_page_nodemask(h, gfp_mask, nid, NULL);\n\tspin_unlock(&hugetlb_lock);\n\n\tif (!page)\n\t\tpage = __alloc_buddy_huge_page(h, gfp_mask, nid, NULL);\n\n\treturn page;\n}\n\n\nstruct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,\n\t\tnodemask_t *nmask)\n{\n\tgfp_t gfp_mask = htlb_alloc_mask(h);\n\n\tspin_lock(&hugetlb_lock);\n\tif (h->free_huge_pages - h->resv_huge_pages > 0) {\n\t\tstruct page *page;\n\n\t\tpage = dequeue_huge_page_nodemask(h, gfp_mask, preferred_nid, nmask);\n\t\tif (page) {\n\t\t\tspin_unlock(&hugetlb_lock);\n\t\t\treturn page;\n\t\t}\n\t}\n\tspin_unlock(&hugetlb_lock);\n\n\t/* No reservations, try to overcommit */\n\n\treturn __alloc_buddy_huge_page(h, gfp_mask, preferred_nid, nmask);\n}\n\n/*\n * Increase the hugetlb pool such that it can accommodate a reservation\n * of size 'delta'.\n */\nstatic int gather_surplus_pages(struct hstate *h, int delta)\n{\n\tstruct list_head surplus_list;\n\tstruct page *page, *tmp;\n\tint ret, i;\n\tint needed, allocated;\n\tbool alloc_ok = true;\n\n\tneeded = (h->resv_huge_pages + delta) - h->free_huge_pages;\n\tif (needed <= 0) {\n\t\th->resv_huge_pages += delta;\n\t\treturn 0;\n\t}\n\n\tallocated = 0;\n\tINIT_LIST_HEAD(&surplus_list);\n\n\tret = -ENOMEM;\nretry:\n\tspin_unlock(&hugetlb_lock);\n\tfor (i = 0; i < needed; i++) {\n\t\tpage = __alloc_buddy_huge_page(h, htlb_alloc_mask(h),\n\t\t\t\tNUMA_NO_NODE, NULL);\n\t\tif (!page) {\n\t\t\talloc_ok = false;\n\t\t\tbreak;\n\t\t}\n\t\tlist_add(&page->lru, &surplus_list);\n\t\tcond_resched();\n\t}\n\tallocated += i;\n\n\t/*\n\t * After retaking hugetlb_lock, we need to recalculate 'needed'\n\t * because either resv_huge_pages or free_huge_pages may have changed.\n\t */\n\tspin_lock(&hugetlb_lock);\n\tneeded = (h->resv_huge_pages + delta) -\n\t\t\t(h->free_huge_pages + allocated);\n\tif (needed > 0) {\n\t\tif (alloc_ok)\n\t\t\tgoto retry;\n\t\t/*\n\t\t * We were not able to allocate enough pages to\n\t\t * satisfy the entire reservation so we free what\n\t\t * we've allocated so far.\n\t\t */\n\t\tgoto free;\n\t}\n\t/*\n\t * The surplus_list now contains _at_least_ the number of extra pages\n\t * needed to accommodate the reservation.  Add the appropriate number\n\t * of pages to the hugetlb pool and free the extras back to the buddy\n\t * allocator.  Commit the entire reservation here to prevent another\n\t * process from stealing the pages as they are added to the pool but\n\t * before they are reserved.\n\t */\n\tneeded += allocated;\n\th->resv_huge_pages += delta;\n\tret = 0;\n\n\t/* Free the needed pages to the hugetlb pool */\n\tlist_for_each_entry_safe(page, tmp, &surplus_list, lru) {\n\t\tif ((--needed) < 0)\n\t\t\tbreak;\n\t\t/*\n\t\t * This page is now managed by the hugetlb allocator and has\n\t\t * no users -- drop the buddy allocator's reference.\n\t\t */\n\t\tput_page_testzero(page);\n\t\tVM_BUG_ON_PAGE(page_count(page), page);\n\t\tenqueue_huge_page(h, page);\n\t}\nfree:\n\tspin_unlock(&hugetlb_lock);\n\n\t/* Free unnecessary surplus pages to the buddy allocator */\n\tlist_for_each_entry_safe(page, tmp, &surplus_list, lru)\n\t\tput_page(page);\n\tspin_lock(&hugetlb_lock);\n\n\treturn ret;\n}\n\n/*\n * This routine has two main purposes:\n * 1) Decrement the reservation count (resv_huge_pages) by the value passed\n *    in unused_resv_pages.  This corresponds to the prior adjustments made\n *    to the associated reservation map.\n * 2) Free any unused surplus pages that may have been allocated to satisfy\n *    the reservation.  As many as unused_resv_pages may be freed.\n *\n * Called with hugetlb_lock held.  However, the lock could be dropped (and\n * reacquired) during calls to cond_resched_lock.  Whenever dropping the lock,\n * we must make sure nobody else can claim pages we are in the process of\n * freeing.  Do this by ensuring resv_huge_page always is greater than the\n * number of huge pages we plan to free when dropping the lock.\n */\nstatic void return_unused_surplus_pages(struct hstate *h,\n\t\t\t\t\tunsigned long unused_resv_pages)\n{\n\tunsigned long nr_pages;\n\n\t/* Cannot return gigantic pages currently */\n\tif (hstate_is_gigantic(h))\n\t\tgoto out;\n\n\t/*\n\t * Part (or even all) of the reservation could have been backed\n\t * by pre-allocated pages. Only free surplus pages.\n\t */\n\tnr_pages = min(unused_resv_pages, h->surplus_huge_pages);\n\n\t/*\n\t * We want to release as many surplus pages as possible, spread\n\t * evenly across all nodes with memory. Iterate across these nodes\n\t * until we can no longer free unreserved surplus pages. This occurs\n\t * when the nodes with surplus pages have no free pages.\n\t * free_pool_huge_page() will balance the the freed pages across the\n\t * on-line nodes with memory and will handle the hstate accounting.\n\t *\n\t * Note that we decrement resv_huge_pages as we free the pages.  If\n\t * we drop the lock, resv_huge_pages will still be sufficiently large\n\t * to cover subsequent pages we may free.\n\t */\n\twhile (nr_pages--) {\n\t\th->resv_huge_pages--;\n\t\tunused_resv_pages--;\n\t\tif (!free_pool_huge_page(h, &node_states[N_MEMORY], 1))\n\t\t\tgoto out;\n\t\tcond_resched_lock(&hugetlb_lock);\n\t}\n\nout:\n\t/* Fully uncommit the reservation */\n\th->resv_huge_pages -= unused_resv_pages;\n}\n\n\n/*\n * vma_needs_reservation, vma_commit_reservation and vma_end_reservation\n * are used by the huge page allocation routines to manage reservations.\n *\n * vma_needs_reservation is called to determine if the huge page at addr\n * within the vma has an associated reservation.  If a reservation is\n * needed, the value 1 is returned.  The caller is then responsible for\n * managing the global reservation and subpool usage counts.  After\n * the huge page has been allocated, vma_commit_reservation is called\n * to add the page to the reservation map.  If the page allocation fails,\n * the reservation must be ended instead of committed.  vma_end_reservation\n * is called in such cases.\n *\n * In the normal case, vma_commit_reservation returns the same value\n * as the preceding vma_needs_reservation call.  The only time this\n * is not the case is if a reserve map was changed between calls.  It\n * is the responsibility of the caller to notice the difference and\n * take appropriate action.\n *\n * vma_add_reservation is used in error paths where a reservation must\n * be restored when a newly allocated huge page must be freed.  It is\n * to be called after calling vma_needs_reservation to determine if a\n * reservation exists.\n */\nenum vma_resv_mode {\n\tVMA_NEEDS_RESV,\n\tVMA_COMMIT_RESV,\n\tVMA_END_RESV,\n\tVMA_ADD_RESV,\n};\nstatic long __vma_reservation_common(struct hstate *h,\n\t\t\t\tstruct vm_area_struct *vma, unsigned long addr,\n\t\t\t\tenum vma_resv_mode mode)\n{\n\tstruct resv_map *resv;\n\tpgoff_t idx;\n\tlong ret;\n\n\tresv = vma_resv_map(vma);\n\tif (!resv)\n\t\treturn 1;\n\n\tidx = vma_hugecache_offset(h, vma, addr);\n\tswitch (mode) {\n\tcase VMA_NEEDS_RESV:\n\t\tret = region_chg(resv, idx, idx + 1);\n\t\tbreak;\n\tcase VMA_COMMIT_RESV:\n\t\tret = region_add(resv, idx, idx + 1);\n\t\tbreak;\n\tcase VMA_END_RESV:\n\t\tregion_abort(resv, idx, idx + 1);\n\t\tret = 0;\n\t\tbreak;\n\tcase VMA_ADD_RESV:\n\t\tif (vma->vm_flags & VM_MAYSHARE)\n\t\t\tret = region_add(resv, idx, idx + 1);\n\t\telse {\n\t\t\tregion_abort(resv, idx, idx + 1);\n\t\t\tret = region_del(resv, idx, idx + 1);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\treturn ret;\n\telse if (is_vma_resv_set(vma, HPAGE_RESV_OWNER) && ret >= 0) {\n\t\t/*\n\t\t * In most cases, reserves always exist for private mappings.\n\t\t * However, a file associated with mapping could have been\n\t\t * hole punched or truncated after reserves were consumed.\n\t\t * As subsequent fault on such a range will not use reserves.\n\t\t * Subtle - The reserve map for private mappings has the\n\t\t * opposite meaning than that of shared mappings.  If NO\n\t\t * entry is in the reserve map, it means a reservation exists.\n\t\t * If an entry exists in the reserve map, it means the\n\t\t * reservation has already been consumed.  As a result, the\n\t\t * return value of this routine is the opposite of the\n\t\t * value returned from reserve map manipulation routines above.\n\t\t */\n\t\tif (ret)\n\t\t\treturn 0;\n\t\telse\n\t\t\treturn 1;\n\t}\n\telse\n\t\treturn ret < 0 ? ret : 0;\n}\n\nstatic long vma_needs_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\treturn __vma_reservation_common(h, vma, addr, VMA_NEEDS_RESV);\n}\n\nstatic long vma_commit_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\treturn __vma_reservation_common(h, vma, addr, VMA_COMMIT_RESV);\n}\n\nstatic void vma_end_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\t(void)__vma_reservation_common(h, vma, addr, VMA_END_RESV);\n}\n\nstatic long vma_add_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\treturn __vma_reservation_common(h, vma, addr, VMA_ADD_RESV);\n}\n\n/*\n * This routine is called to restore a reservation on error paths.  In the\n * specific error paths, a huge page was allocated (via alloc_huge_page)\n * and is about to be freed.  If a reservation for the page existed,\n * alloc_huge_page would have consumed the reservation and set PagePrivate\n * in the newly allocated page.  When the page is freed via free_huge_page,\n * the global reservation count will be incremented if PagePrivate is set.\n * However, free_huge_page can not adjust the reserve map.  Adjust the\n * reserve map here to be consistent with global reserve count adjustments\n * to be made by free_huge_page.\n */\nstatic void restore_reserve_on_error(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address,\n\t\t\tstruct page *page)\n{\n\tif (unlikely(PagePrivate(page))) {\n\t\tlong rc = vma_needs_reservation(h, vma, address);\n\n\t\tif (unlikely(rc < 0)) {\n\t\t\t/*\n\t\t\t * Rare out of memory condition in reserve map\n\t\t\t * manipulation.  Clear PagePrivate so that\n\t\t\t * global reserve count will not be incremented\n\t\t\t * by free_huge_page.  This will make it appear\n\t\t\t * as though the reservation for this page was\n\t\t\t * consumed.  This may prevent the task from\n\t\t\t * faulting in the page at a later time.  This\n\t\t\t * is better than inconsistent global huge page\n\t\t\t * accounting of reserve counts.\n\t\t\t */\n\t\t\tClearPagePrivate(page);\n\t\t} else if (rc) {\n\t\t\trc = vma_add_reservation(h, vma, address);\n\t\t\tif (unlikely(rc < 0))\n\t\t\t\t/*\n\t\t\t\t * See above comment about rare out of\n\t\t\t\t * memory condition.\n\t\t\t\t */\n\t\t\t\tClearPagePrivate(page);\n\t\t} else\n\t\t\tvma_end_reservation(h, vma, address);\n\t}\n}\n\nstruct page *alloc_huge_page(struct vm_area_struct *vma,\n\t\t\t\t    unsigned long addr, int avoid_reserve)\n{\n\tstruct hugepage_subpool *spool = subpool_vma(vma);\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct page *page;\n\tlong map_chg, map_commit;\n\tlong gbl_chg;\n\tint ret, idx;\n\tstruct hugetlb_cgroup *h_cg;\n\n\tidx = hstate_index(h);\n\t/*\n\t * Examine the region/reserve map to determine if the process\n\t * has a reservation for the page to be allocated.  A return\n\t * code of zero indicates a reservation exists (no change).\n\t */\n\tmap_chg = gbl_chg = vma_needs_reservation(h, vma, addr);\n\tif (map_chg < 0)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/*\n\t * Processes that did not create the mapping will have no\n\t * reserves as indicated by the region/reserve map. Check\n\t * that the allocation will not exceed the subpool limit.\n\t * Allocations for MAP_NORESERVE mappings also need to be\n\t * checked against any subpool limit.\n\t */\n\tif (map_chg || avoid_reserve) {\n\t\tgbl_chg = hugepage_subpool_get_pages(spool, 1);\n\t\tif (gbl_chg < 0) {\n\t\t\tvma_end_reservation(h, vma, addr);\n\t\t\treturn ERR_PTR(-ENOSPC);\n\t\t}\n\n\t\t/*\n\t\t * Even though there was no reservation in the region/reserve\n\t\t * map, there could be reservations associated with the\n\t\t * subpool that can be used.  This would be indicated if the\n\t\t * return value of hugepage_subpool_get_pages() is zero.\n\t\t * However, if avoid_reserve is specified we still avoid even\n\t\t * the subpool reservations.\n\t\t */\n\t\tif (avoid_reserve)\n\t\t\tgbl_chg = 1;\n\t}\n\n\tret = hugetlb_cgroup_charge_cgroup(idx, pages_per_huge_page(h), &h_cg);\n\tif (ret)\n\t\tgoto out_subpool_put;\n\n\tspin_lock(&hugetlb_lock);\n\t/*\n\t * glb_chg is passed to indicate whether or not a page must be taken\n\t * from the global free pool (global change).  gbl_chg == 0 indicates\n\t * a reservation exists for the allocation.\n\t */\n\tpage = dequeue_huge_page_vma(h, vma, addr, avoid_reserve, gbl_chg);\n\tif (!page) {\n\t\tspin_unlock(&hugetlb_lock);\n\t\tpage = __alloc_buddy_huge_page_with_mpol(h, vma, addr);\n\t\tif (!page)\n\t\t\tgoto out_uncharge_cgroup;\n\t\tif (!avoid_reserve && vma_has_reserves(vma, gbl_chg)) {\n\t\t\tSetPagePrivate(page);\n\t\t\th->resv_huge_pages--;\n\t\t}\n\t\tspin_lock(&hugetlb_lock);\n\t\tlist_move(&page->lru, &h->hugepage_activelist);\n\t\t/* Fall through */\n\t}\n\thugetlb_cgroup_commit_charge(idx, pages_per_huge_page(h), h_cg, page);\n\tspin_unlock(&hugetlb_lock);\n\n\tset_page_private(page, (unsigned long)spool);\n\n\tmap_commit = vma_commit_reservation(h, vma, addr);\n\tif (unlikely(map_chg > map_commit)) {\n\t\t/*\n\t\t * The page was added to the reservation map between\n\t\t * vma_needs_reservation and vma_commit_reservation.\n\t\t * This indicates a race with hugetlb_reserve_pages.\n\t\t * Adjust for the subpool count incremented above AND\n\t\t * in hugetlb_reserve_pages for the same page.  Also,\n\t\t * the reservation count added in hugetlb_reserve_pages\n\t\t * no longer applies.\n\t\t */\n\t\tlong rsv_adjust;\n\n\t\trsv_adjust = hugepage_subpool_put_pages(spool, 1);\n\t\thugetlb_acct_memory(h, -rsv_adjust);\n\t}\n\treturn page;\n\nout_uncharge_cgroup:\n\thugetlb_cgroup_uncharge_cgroup(idx, pages_per_huge_page(h), h_cg);\nout_subpool_put:\n\tif (map_chg || avoid_reserve)\n\t\thugepage_subpool_put_pages(spool, 1);\n\tvma_end_reservation(h, vma, addr);\n\treturn ERR_PTR(-ENOSPC);\n}\n\n/*\n * alloc_huge_page()'s wrapper which simply returns the page if allocation\n * succeeds, otherwise NULL. This function is called from new_vma_page(),\n * where no ERR_VALUE is expected to be returned.\n */\nstruct page *alloc_huge_page_noerr(struct vm_area_struct *vma,\n\t\t\t\tunsigned long addr, int avoid_reserve)\n{\n\tstruct page *page = alloc_huge_page(vma, addr, avoid_reserve);\n\tif (IS_ERR(page))\n\t\tpage = NULL;\n\treturn page;\n}\n\nint __weak alloc_bootmem_huge_page(struct hstate *h)\n{\n\tstruct huge_bootmem_page *m;\n\tint nr_nodes, node;\n\n\tfor_each_node_mask_to_alloc(h, nr_nodes, node, &node_states[N_MEMORY]) {\n\t\tvoid *addr;\n\n\t\taddr = memblock_virt_alloc_try_nid_nopanic(\n\t\t\t\thuge_page_size(h), huge_page_size(h),\n\t\t\t\t0, BOOTMEM_ALLOC_ACCESSIBLE, node);\n\t\tif (addr) {\n\t\t\t/*\n\t\t\t * Use the beginning of the huge page to store the\n\t\t\t * huge_bootmem_page struct (until gather_bootmem\n\t\t\t * puts them into the mem_map).\n\t\t\t */\n\t\t\tm = addr;\n\t\t\tgoto found;\n\t\t}\n\t}\n\treturn 0;\n\nfound:\n\tBUG_ON(!IS_ALIGNED(virt_to_phys(m), huge_page_size(h)));\n\t/* Put them into a private list first because mem_map is not up yet */\n\tlist_add(&m->list, &huge_boot_pages);\n\tm->hstate = h;\n\treturn 1;\n}\n\nstatic void __init prep_compound_huge_page(struct page *page,\n\t\tunsigned int order)\n{\n\tif (unlikely(order > (MAX_ORDER - 1)))\n\t\tprep_compound_gigantic_page(page, order);\n\telse\n\t\tprep_compound_page(page, order);\n}\n\n/* Put bootmem huge pages into the standard lists after mem_map is up */\nstatic void __init gather_bootmem_prealloc(void)\n{\n\tstruct huge_bootmem_page *m;\n\n\tlist_for_each_entry(m, &huge_boot_pages, list) {\n\t\tstruct hstate *h = m->hstate;\n\t\tstruct page *page;\n\n#ifdef CONFIG_HIGHMEM\n\t\tpage = pfn_to_page(m->phys >> PAGE_SHIFT);\n\t\tmemblock_free_late(__pa(m),\n\t\t\t\t   sizeof(struct huge_bootmem_page));\n#else\n\t\tpage = virt_to_page(m);\n#endif\n\t\tWARN_ON(page_count(page) != 1);\n\t\tprep_compound_huge_page(page, h->order);\n\t\tWARN_ON(PageReserved(page));\n\t\tprep_new_huge_page(h, page, page_to_nid(page));\n\t\t/*\n\t\t * If we had gigantic hugepages allocated at boot time, we need\n\t\t * to restore the 'stolen' pages to totalram_pages in order to\n\t\t * fix confusing memory reports from free(1) and another\n\t\t * side-effects, like CommitLimit going negative.\n\t\t */\n\t\tif (hstate_is_gigantic(h))\n\t\t\tadjust_managed_page_count(page, 1 << h->order);\n\t}\n}\n\nstatic void __init hugetlb_hstate_alloc_pages(struct hstate *h)\n{\n\tunsigned long i;\n\n\tfor (i = 0; i < h->max_huge_pages; ++i) {\n\t\tif (hstate_is_gigantic(h)) {\n\t\t\tif (!alloc_bootmem_huge_page(h))\n\t\t\t\tbreak;\n\t\t} else if (!alloc_fresh_huge_page(h,\n\t\t\t\t\t &node_states[N_MEMORY]))\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\tif (i < h->max_huge_pages) {\n\t\tchar buf[32];\n\n\t\tstring_get_size(huge_page_size(h), 1, STRING_UNITS_2, buf, 32);\n\t\tpr_warn(\"HugeTLB: allocating %lu of page size %s failed.  Only allocated %lu hugepages.\\n\",\n\t\t\th->max_huge_pages, buf, i);\n\t\th->max_huge_pages = i;\n\t}\n}\n\nstatic void __init hugetlb_init_hstates(void)\n{\n\tstruct hstate *h;\n\n\tfor_each_hstate(h) {\n\t\tif (minimum_order > huge_page_order(h))\n\t\t\tminimum_order = huge_page_order(h);\n\n\t\t/* oversize hugepages were init'ed in early boot */\n\t\tif (!hstate_is_gigantic(h))\n\t\t\thugetlb_hstate_alloc_pages(h);\n\t}\n\tVM_BUG_ON(minimum_order == UINT_MAX);\n}\n\nstatic void __init report_hugepages(void)\n{\n\tstruct hstate *h;\n\n\tfor_each_hstate(h) {\n\t\tchar buf[32];\n\n\t\tstring_get_size(huge_page_size(h), 1, STRING_UNITS_2, buf, 32);\n\t\tpr_info(\"HugeTLB registered %s page size, pre-allocated %ld pages\\n\",\n\t\t\tbuf, h->free_huge_pages);\n\t}\n}\n\n#ifdef CONFIG_HIGHMEM\nstatic void try_to_free_low(struct hstate *h, unsigned long count,\n\t\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n\tint i;\n\n\tif (hstate_is_gigantic(h))\n\t\treturn;\n\n\tfor_each_node_mask(i, *nodes_allowed) {\n\t\tstruct page *page, *next;\n\t\tstruct list_head *freel = &h->hugepage_freelists[i];\n\t\tlist_for_each_entry_safe(page, next, freel, lru) {\n\t\t\tif (count >= h->nr_huge_pages)\n\t\t\t\treturn;\n\t\t\tif (PageHighMem(page))\n\t\t\t\tcontinue;\n\t\t\tlist_del(&page->lru);\n\t\t\tupdate_and_free_page(h, page);\n\t\t\th->free_huge_pages--;\n\t\t\th->free_huge_pages_node[page_to_nid(page)]--;\n\t\t}\n\t}\n}\n#else\nstatic inline void try_to_free_low(struct hstate *h, unsigned long count,\n\t\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n}\n#endif\n\n/*\n * Increment or decrement surplus_huge_pages.  Keep node-specific counters\n * balanced by operating on them in a round-robin fashion.\n * Returns 1 if an adjustment was made.\n */\nstatic int adjust_pool_surplus(struct hstate *h, nodemask_t *nodes_allowed,\n\t\t\t\tint delta)\n{\n\tint nr_nodes, node;\n\n\tVM_BUG_ON(delta != -1 && delta != 1);\n\n\tif (delta < 0) {\n\t\tfor_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {\n\t\t\tif (h->surplus_huge_pages_node[node])\n\t\t\t\tgoto found;\n\t\t}\n\t} else {\n\t\tfor_each_node_mask_to_free(h, nr_nodes, node, nodes_allowed) {\n\t\t\tif (h->surplus_huge_pages_node[node] <\n\t\t\t\t\th->nr_huge_pages_node[node])\n\t\t\t\tgoto found;\n\t\t}\n\t}\n\treturn 0;\n\nfound:\n\th->surplus_huge_pages += delta;\n\th->surplus_huge_pages_node[node] += delta;\n\treturn 1;\n}\n\n#define persistent_huge_pages(h) (h->nr_huge_pages - h->surplus_huge_pages)\nstatic unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,\n\t\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n\tunsigned long min_count, ret;\n\n\tif (hstate_is_gigantic(h) && !gigantic_page_supported())\n\t\treturn h->max_huge_pages;\n\n\t/*\n\t * Increase the pool size\n\t * First take pages out of surplus state.  Then make up the\n\t * remaining difference by allocating fresh huge pages.\n\t *\n\t * We might race with __alloc_buddy_huge_page() here and be unable\n\t * to convert a surplus huge page to a normal huge page. That is\n\t * not critical, though, it just means the overall size of the\n\t * pool might be one hugepage larger than it needs to be, but\n\t * within all the constraints specified by the sysctls.\n\t */\n\tspin_lock(&hugetlb_lock);\n\twhile (h->surplus_huge_pages && count > persistent_huge_pages(h)) {\n\t\tif (!adjust_pool_surplus(h, nodes_allowed, -1))\n\t\t\tbreak;\n\t}\n\n\twhile (count > persistent_huge_pages(h)) {\n\t\t/*\n\t\t * If this allocation races such that we no longer need the\n\t\t * page, free_huge_page will handle it by freeing the page\n\t\t * and reducing the surplus.\n\t\t */\n\t\tspin_unlock(&hugetlb_lock);\n\n\t\t/* yield cpu to avoid soft lockup */\n\t\tcond_resched();\n\n\t\tif (hstate_is_gigantic(h))\n\t\t\tret = alloc_fresh_gigantic_page(h, nodes_allowed);\n\t\telse\n\t\t\tret = alloc_fresh_huge_page(h, nodes_allowed);\n\t\tspin_lock(&hugetlb_lock);\n\t\tif (!ret)\n\t\t\tgoto out;\n\n\t\t/* Bail for signals. Probably ctrl-c from user */\n\t\tif (signal_pending(current))\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * Decrease the pool size\n\t * First return free pages to the buddy allocator (being careful\n\t * to keep enough around to satisfy reservations).  Then place\n\t * pages into surplus state as needed so the pool will shrink\n\t * to the desired size as pages become free.\n\t *\n\t * By placing pages into the surplus state independent of the\n\t * overcommit value, we are allowing the surplus pool size to\n\t * exceed overcommit. There are few sane options here. Since\n\t * __alloc_buddy_huge_page() is checking the global counter,\n\t * though, we'll note that we're not allowed to exceed surplus\n\t * and won't grow the pool anywhere else. Not until one of the\n\t * sysctls are changed, or the surplus pages go out of use.\n\t */\n\tmin_count = h->resv_huge_pages + h->nr_huge_pages - h->free_huge_pages;\n\tmin_count = max(count, min_count);\n\ttry_to_free_low(h, min_count, nodes_allowed);\n\twhile (min_count < persistent_huge_pages(h)) {\n\t\tif (!free_pool_huge_page(h, nodes_allowed, 0))\n\t\t\tbreak;\n\t\tcond_resched_lock(&hugetlb_lock);\n\t}\n\twhile (count < persistent_huge_pages(h)) {\n\t\tif (!adjust_pool_surplus(h, nodes_allowed, 1))\n\t\t\tbreak;\n\t}\nout:\n\tret = persistent_huge_pages(h);\n\tspin_unlock(&hugetlb_lock);\n\treturn ret;\n}\n\n#define HSTATE_ATTR_RO(_name) \\\n\tstatic struct kobj_attribute _name##_attr = __ATTR_RO(_name)\n\n#define HSTATE_ATTR(_name) \\\n\tstatic struct kobj_attribute _name##_attr = \\\n\t\t__ATTR(_name, 0644, _name##_show, _name##_store)\n\nstatic struct kobject *hugepages_kobj;\nstatic struct kobject *hstate_kobjs[HUGE_MAX_HSTATE];\n\nstatic struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp);\n\nstatic struct hstate *kobj_to_hstate(struct kobject *kobj, int *nidp)\n{\n\tint i;\n\n\tfor (i = 0; i < HUGE_MAX_HSTATE; i++)\n\t\tif (hstate_kobjs[i] == kobj) {\n\t\t\tif (nidp)\n\t\t\t\t*nidp = NUMA_NO_NODE;\n\t\t\treturn &hstates[i];\n\t\t}\n\n\treturn kobj_to_node_hstate(kobj, nidp);\n}\n\nstatic ssize_t nr_hugepages_show_common(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h;\n\tunsigned long nr_huge_pages;\n\tint nid;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (nid == NUMA_NO_NODE)\n\t\tnr_huge_pages = h->nr_huge_pages;\n\telse\n\t\tnr_huge_pages = h->nr_huge_pages_node[nid];\n\n\treturn sprintf(buf, \"%lu\\n\", nr_huge_pages);\n}\n\nstatic ssize_t __nr_hugepages_store_common(bool obey_mempolicy,\n\t\t\t\t\t   struct hstate *h, int nid,\n\t\t\t\t\t   unsigned long count, size_t len)\n{\n\tint err;\n\tNODEMASK_ALLOC(nodemask_t, nodes_allowed, GFP_KERNEL | __GFP_NORETRY);\n\n\tif (hstate_is_gigantic(h) && !gigantic_page_supported()) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (nid == NUMA_NO_NODE) {\n\t\t/*\n\t\t * global hstate attribute\n\t\t */\n\t\tif (!(obey_mempolicy &&\n\t\t\t\tinit_nodemask_of_mempolicy(nodes_allowed))) {\n\t\t\tNODEMASK_FREE(nodes_allowed);\n\t\t\tnodes_allowed = &node_states[N_MEMORY];\n\t\t}\n\t} else if (nodes_allowed) {\n\t\t/*\n\t\t * per node hstate attribute: adjust count to global,\n\t\t * but restrict alloc/free to the specified node.\n\t\t */\n\t\tcount += h->nr_huge_pages - h->nr_huge_pages_node[nid];\n\t\tinit_nodemask_of_node(nodes_allowed, nid);\n\t} else\n\t\tnodes_allowed = &node_states[N_MEMORY];\n\n\th->max_huge_pages = set_max_huge_pages(h, count, nodes_allowed);\n\n\tif (nodes_allowed != &node_states[N_MEMORY])\n\t\tNODEMASK_FREE(nodes_allowed);\n\n\treturn len;\nout:\n\tNODEMASK_FREE(nodes_allowed);\n\treturn err;\n}\n\nstatic ssize_t nr_hugepages_store_common(bool obey_mempolicy,\n\t\t\t\t\t struct kobject *kobj, const char *buf,\n\t\t\t\t\t size_t len)\n{\n\tstruct hstate *h;\n\tunsigned long count;\n\tint nid;\n\tint err;\n\n\terr = kstrtoul(buf, 10, &count);\n\tif (err)\n\t\treturn err;\n\n\th = kobj_to_hstate(kobj, &nid);\n\treturn __nr_hugepages_store_common(obey_mempolicy, h, nid, count, len);\n}\n\nstatic ssize_t nr_hugepages_show(struct kobject *kobj,\n\t\t\t\t       struct kobj_attribute *attr, char *buf)\n{\n\treturn nr_hugepages_show_common(kobj, attr, buf);\n}\n\nstatic ssize_t nr_hugepages_store(struct kobject *kobj,\n\t       struct kobj_attribute *attr, const char *buf, size_t len)\n{\n\treturn nr_hugepages_store_common(false, kobj, buf, len);\n}\nHSTATE_ATTR(nr_hugepages);\n\n#ifdef CONFIG_NUMA\n\n/*\n * hstate attribute for optionally mempolicy-based constraint on persistent\n * huge page alloc/free.\n */\nstatic ssize_t nr_hugepages_mempolicy_show(struct kobject *kobj,\n\t\t\t\t       struct kobj_attribute *attr, char *buf)\n{\n\treturn nr_hugepages_show_common(kobj, attr, buf);\n}\n\nstatic ssize_t nr_hugepages_mempolicy_store(struct kobject *kobj,\n\t       struct kobj_attribute *attr, const char *buf, size_t len)\n{\n\treturn nr_hugepages_store_common(true, kobj, buf, len);\n}\nHSTATE_ATTR(nr_hugepages_mempolicy);\n#endif\n\n\nstatic ssize_t nr_overcommit_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\treturn sprintf(buf, \"%lu\\n\", h->nr_overcommit_huge_pages);\n}\n\nstatic ssize_t nr_overcommit_hugepages_store(struct kobject *kobj,\n\t\tstruct kobj_attribute *attr, const char *buf, size_t count)\n{\n\tint err;\n\tunsigned long input;\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\n\tif (hstate_is_gigantic(h))\n\t\treturn -EINVAL;\n\n\terr = kstrtoul(buf, 10, &input);\n\tif (err)\n\t\treturn err;\n\n\tspin_lock(&hugetlb_lock);\n\th->nr_overcommit_huge_pages = input;\n\tspin_unlock(&hugetlb_lock);\n\n\treturn count;\n}\nHSTATE_ATTR(nr_overcommit_hugepages);\n\nstatic ssize_t free_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h;\n\tunsigned long free_huge_pages;\n\tint nid;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (nid == NUMA_NO_NODE)\n\t\tfree_huge_pages = h->free_huge_pages;\n\telse\n\t\tfree_huge_pages = h->free_huge_pages_node[nid];\n\n\treturn sprintf(buf, \"%lu\\n\", free_huge_pages);\n}\nHSTATE_ATTR_RO(free_hugepages);\n\nstatic ssize_t resv_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\treturn sprintf(buf, \"%lu\\n\", h->resv_huge_pages);\n}\nHSTATE_ATTR_RO(resv_hugepages);\n\nstatic ssize_t surplus_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h;\n\tunsigned long surplus_huge_pages;\n\tint nid;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (nid == NUMA_NO_NODE)\n\t\tsurplus_huge_pages = h->surplus_huge_pages;\n\telse\n\t\tsurplus_huge_pages = h->surplus_huge_pages_node[nid];\n\n\treturn sprintf(buf, \"%lu\\n\", surplus_huge_pages);\n}\nHSTATE_ATTR_RO(surplus_hugepages);\n\nstatic struct attribute *hstate_attrs[] = {\n\t&nr_hugepages_attr.attr,\n\t&nr_overcommit_hugepages_attr.attr,\n\t&free_hugepages_attr.attr,\n\t&resv_hugepages_attr.attr,\n\t&surplus_hugepages_attr.attr,\n#ifdef CONFIG_NUMA\n\t&nr_hugepages_mempolicy_attr.attr,\n#endif\n\tNULL,\n};\n\nstatic struct attribute_group hstate_attr_group = {\n\t.attrs = hstate_attrs,\n};\n\nstatic int hugetlb_sysfs_add_hstate(struct hstate *h, struct kobject *parent,\n\t\t\t\t    struct kobject **hstate_kobjs,\n\t\t\t\t    struct attribute_group *hstate_attr_group)\n{\n\tint retval;\n\tint hi = hstate_index(h);\n\n\thstate_kobjs[hi] = kobject_create_and_add(h->name, parent);\n\tif (!hstate_kobjs[hi])\n\t\treturn -ENOMEM;\n\n\tretval = sysfs_create_group(hstate_kobjs[hi], hstate_attr_group);\n\tif (retval)\n\t\tkobject_put(hstate_kobjs[hi]);\n\n\treturn retval;\n}\n\nstatic void __init hugetlb_sysfs_init(void)\n{\n\tstruct hstate *h;\n\tint err;\n\n\thugepages_kobj = kobject_create_and_add(\"hugepages\", mm_kobj);\n\tif (!hugepages_kobj)\n\t\treturn;\n\n\tfor_each_hstate(h) {\n\t\terr = hugetlb_sysfs_add_hstate(h, hugepages_kobj,\n\t\t\t\t\t hstate_kobjs, &hstate_attr_group);\n\t\tif (err)\n\t\t\tpr_err(\"Hugetlb: Unable to add hstate %s\", h->name);\n\t}\n}\n\n#ifdef CONFIG_NUMA\n\n/*\n * node_hstate/s - associate per node hstate attributes, via their kobjects,\n * with node devices in node_devices[] using a parallel array.  The array\n * index of a node device or _hstate == node id.\n * This is here to avoid any static dependency of the node device driver, in\n * the base kernel, on the hugetlb module.\n */\nstruct node_hstate {\n\tstruct kobject\t\t*hugepages_kobj;\n\tstruct kobject\t\t*hstate_kobjs[HUGE_MAX_HSTATE];\n};\nstatic struct node_hstate node_hstates[MAX_NUMNODES];\n\n/*\n * A subset of global hstate attributes for node devices\n */\nstatic struct attribute *per_node_hstate_attrs[] = {\n\t&nr_hugepages_attr.attr,\n\t&free_hugepages_attr.attr,\n\t&surplus_hugepages_attr.attr,\n\tNULL,\n};\n\nstatic struct attribute_group per_node_hstate_attr_group = {\n\t.attrs = per_node_hstate_attrs,\n};\n\n/*\n * kobj_to_node_hstate - lookup global hstate for node device hstate attr kobj.\n * Returns node id via non-NULL nidp.\n */\nstatic struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp)\n{\n\tint nid;\n\n\tfor (nid = 0; nid < nr_node_ids; nid++) {\n\t\tstruct node_hstate *nhs = &node_hstates[nid];\n\t\tint i;\n\t\tfor (i = 0; i < HUGE_MAX_HSTATE; i++)\n\t\t\tif (nhs->hstate_kobjs[i] == kobj) {\n\t\t\t\tif (nidp)\n\t\t\t\t\t*nidp = nid;\n\t\t\t\treturn &hstates[i];\n\t\t\t}\n\t}\n\n\tBUG();\n\treturn NULL;\n}\n\n/*\n * Unregister hstate attributes from a single node device.\n * No-op if no hstate attributes attached.\n */\nstatic void hugetlb_unregister_node(struct node *node)\n{\n\tstruct hstate *h;\n\tstruct node_hstate *nhs = &node_hstates[node->dev.id];\n\n\tif (!nhs->hugepages_kobj)\n\t\treturn;\t\t/* no hstate attributes */\n\n\tfor_each_hstate(h) {\n\t\tint idx = hstate_index(h);\n\t\tif (nhs->hstate_kobjs[idx]) {\n\t\t\tkobject_put(nhs->hstate_kobjs[idx]);\n\t\t\tnhs->hstate_kobjs[idx] = NULL;\n\t\t}\n\t}\n\n\tkobject_put(nhs->hugepages_kobj);\n\tnhs->hugepages_kobj = NULL;\n}\n\n\n/*\n * Register hstate attributes for a single node device.\n * No-op if attributes already registered.\n */\nstatic void hugetlb_register_node(struct node *node)\n{\n\tstruct hstate *h;\n\tstruct node_hstate *nhs = &node_hstates[node->dev.id];\n\tint err;\n\n\tif (nhs->hugepages_kobj)\n\t\treturn;\t\t/* already allocated */\n\n\tnhs->hugepages_kobj = kobject_create_and_add(\"hugepages\",\n\t\t\t\t\t\t\t&node->dev.kobj);\n\tif (!nhs->hugepages_kobj)\n\t\treturn;\n\n\tfor_each_hstate(h) {\n\t\terr = hugetlb_sysfs_add_hstate(h, nhs->hugepages_kobj,\n\t\t\t\t\t\tnhs->hstate_kobjs,\n\t\t\t\t\t\t&per_node_hstate_attr_group);\n\t\tif (err) {\n\t\t\tpr_err(\"Hugetlb: Unable to add hstate %s for node %d\\n\",\n\t\t\t\th->name, node->dev.id);\n\t\t\thugetlb_unregister_node(node);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n/*\n * hugetlb init time:  register hstate attributes for all registered node\n * devices of nodes that have memory.  All on-line nodes should have\n * registered their associated device by this time.\n */\nstatic void __init hugetlb_register_all_nodes(void)\n{\n\tint nid;\n\n\tfor_each_node_state(nid, N_MEMORY) {\n\t\tstruct node *node = node_devices[nid];\n\t\tif (node->dev.id == nid)\n\t\t\thugetlb_register_node(node);\n\t}\n\n\t/*\n\t * Let the node device driver know we're here so it can\n\t * [un]register hstate attributes on node hotplug.\n\t */\n\tregister_hugetlbfs_with_node(hugetlb_register_node,\n\t\t\t\t     hugetlb_unregister_node);\n}\n#else\t/* !CONFIG_NUMA */\n\nstatic struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp)\n{\n\tBUG();\n\tif (nidp)\n\t\t*nidp = -1;\n\treturn NULL;\n}\n\nstatic void hugetlb_register_all_nodes(void) { }\n\n#endif\n\nstatic int __init hugetlb_init(void)\n{\n\tint i;\n\n\tif (!hugepages_supported())\n\t\treturn 0;\n\n\tif (!size_to_hstate(default_hstate_size)) {\n\t\tif (default_hstate_size != 0) {\n\t\t\tpr_err(\"HugeTLB: unsupported default_hugepagesz %lu. Reverting to %lu\\n\",\n\t\t\t       default_hstate_size, HPAGE_SIZE);\n\t\t}\n\n\t\tdefault_hstate_size = HPAGE_SIZE;\n\t\tif (!size_to_hstate(default_hstate_size))\n\t\t\thugetlb_add_hstate(HUGETLB_PAGE_ORDER);\n\t}\n\tdefault_hstate_idx = hstate_index(size_to_hstate(default_hstate_size));\n\tif (default_hstate_max_huge_pages) {\n\t\tif (!default_hstate.max_huge_pages)\n\t\t\tdefault_hstate.max_huge_pages = default_hstate_max_huge_pages;\n\t}\n\n\thugetlb_init_hstates();\n\tgather_bootmem_prealloc();\n\treport_hugepages();\n\n\thugetlb_sysfs_init();\n\thugetlb_register_all_nodes();\n\thugetlb_cgroup_file_init();\n\n#ifdef CONFIG_SMP\n\tnum_fault_mutexes = roundup_pow_of_two(8 * num_possible_cpus());\n#else\n\tnum_fault_mutexes = 1;\n#endif\n\thugetlb_fault_mutex_table =\n\t\tkmalloc(sizeof(struct mutex) * num_fault_mutexes, GFP_KERNEL);\n\tBUG_ON(!hugetlb_fault_mutex_table);\n\n\tfor (i = 0; i < num_fault_mutexes; i++)\n\t\tmutex_init(&hugetlb_fault_mutex_table[i]);\n\treturn 0;\n}\nsubsys_initcall(hugetlb_init);\n\n/* Should be called on processing a hugepagesz=... option */\nvoid __init hugetlb_bad_size(void)\n{\n\tparsed_valid_hugepagesz = false;\n}\n\nvoid __init hugetlb_add_hstate(unsigned int order)\n{\n\tstruct hstate *h;\n\tunsigned long i;\n\n\tif (size_to_hstate(PAGE_SIZE << order)) {\n\t\tpr_warn(\"hugepagesz= specified twice, ignoring\\n\");\n\t\treturn;\n\t}\n\tBUG_ON(hugetlb_max_hstate >= HUGE_MAX_HSTATE);\n\tBUG_ON(order == 0);\n\th = &hstates[hugetlb_max_hstate++];\n\th->order = order;\n\th->mask = ~((1ULL << (order + PAGE_SHIFT)) - 1);\n\th->nr_huge_pages = 0;\n\th->free_huge_pages = 0;\n\tfor (i = 0; i < MAX_NUMNODES; ++i)\n\t\tINIT_LIST_HEAD(&h->hugepage_freelists[i]);\n\tINIT_LIST_HEAD(&h->hugepage_activelist);\n\th->next_nid_to_alloc = first_memory_node;\n\th->next_nid_to_free = first_memory_node;\n\tsnprintf(h->name, HSTATE_NAME_LEN, \"hugepages-%lukB\",\n\t\t\t\t\thuge_page_size(h)/1024);\n\n\tparsed_hstate = h;\n}\n\nstatic int __init hugetlb_nrpages_setup(char *s)\n{\n\tunsigned long *mhp;\n\tstatic unsigned long *last_mhp;\n\n\tif (!parsed_valid_hugepagesz) {\n\t\tpr_warn(\"hugepages = %s preceded by \"\n\t\t\t\"an unsupported hugepagesz, ignoring\\n\", s);\n\t\tparsed_valid_hugepagesz = true;\n\t\treturn 1;\n\t}\n\t/*\n\t * !hugetlb_max_hstate means we haven't parsed a hugepagesz= parameter yet,\n\t * so this hugepages= parameter goes to the \"default hstate\".\n\t */\n\telse if (!hugetlb_max_hstate)\n\t\tmhp = &default_hstate_max_huge_pages;\n\telse\n\t\tmhp = &parsed_hstate->max_huge_pages;\n\n\tif (mhp == last_mhp) {\n\t\tpr_warn(\"hugepages= specified twice without interleaving hugepagesz=, ignoring\\n\");\n\t\treturn 1;\n\t}\n\n\tif (sscanf(s, \"%lu\", mhp) <= 0)\n\t\t*mhp = 0;\n\n\t/*\n\t * Global state is always initialized later in hugetlb_init.\n\t * But we need to allocate >= MAX_ORDER hstates here early to still\n\t * use the bootmem allocator.\n\t */\n\tif (hugetlb_max_hstate && parsed_hstate->order >= MAX_ORDER)\n\t\thugetlb_hstate_alloc_pages(parsed_hstate);\n\n\tlast_mhp = mhp;\n\n\treturn 1;\n}\n__setup(\"hugepages=\", hugetlb_nrpages_setup);\n\nstatic int __init hugetlb_default_setup(char *s)\n{\n\tdefault_hstate_size = memparse(s, &s);\n\treturn 1;\n}\n__setup(\"default_hugepagesz=\", hugetlb_default_setup);\n\nstatic unsigned int cpuset_mems_nr(unsigned int *array)\n{\n\tint node;\n\tunsigned int nr = 0;\n\n\tfor_each_node_mask(node, cpuset_current_mems_allowed)\n\t\tnr += array[node];\n\n\treturn nr;\n}\n\n#ifdef CONFIG_SYSCTL\nstatic int hugetlb_sysctl_handler_common(bool obey_mempolicy,\n\t\t\t struct ctl_table *table, int write,\n\t\t\t void __user *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp = h->max_huge_pages;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttable->data = &tmp;\n\ttable->maxlen = sizeof(unsigned long);\n\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write)\n\t\tret = __nr_hugepages_store_common(obey_mempolicy, h,\n\t\t\t\t\t\t  NUMA_NO_NODE, tmp, *length);\nout:\n\treturn ret;\n}\n\nint hugetlb_sysctl_handler(struct ctl_table *table, int write,\n\t\t\t  void __user *buffer, size_t *length, loff_t *ppos)\n{\n\n\treturn hugetlb_sysctl_handler_common(false, table, write,\n\t\t\t\t\t\t\tbuffer, length, ppos);\n}\n\n#ifdef CONFIG_NUMA\nint hugetlb_mempolicy_sysctl_handler(struct ctl_table *table, int write,\n\t\t\t  void __user *buffer, size_t *length, loff_t *ppos)\n{\n\treturn hugetlb_sysctl_handler_common(true, table, write,\n\t\t\t\t\t\t\tbuffer, length, ppos);\n}\n#endif /* CONFIG_NUMA */\n\nint hugetlb_overcommit_handler(struct ctl_table *table, int write,\n\t\t\tvoid __user *buffer,\n\t\t\tsize_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttmp = h->nr_overcommit_huge_pages;\n\n\tif (write && hstate_is_gigantic(h))\n\t\treturn -EINVAL;\n\n\ttable->data = &tmp;\n\ttable->maxlen = sizeof(unsigned long);\n\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write) {\n\t\tspin_lock(&hugetlb_lock);\n\t\th->nr_overcommit_huge_pages = tmp;\n\t\tspin_unlock(&hugetlb_lock);\n\t}\nout:\n\treturn ret;\n}\n\n#endif /* CONFIG_SYSCTL */\n\nvoid hugetlb_report_meminfo(struct seq_file *m)\n{\n\tstruct hstate *h = &default_hstate;\n\tif (!hugepages_supported())\n\t\treturn;\n\tseq_printf(m,\n\t\t\t\"HugePages_Total:   %5lu\\n\"\n\t\t\t\"HugePages_Free:    %5lu\\n\"\n\t\t\t\"HugePages_Rsvd:    %5lu\\n\"\n\t\t\t\"HugePages_Surp:    %5lu\\n\"\n\t\t\t\"Hugepagesize:   %8lu kB\\n\",\n\t\t\th->nr_huge_pages,\n\t\t\th->free_huge_pages,\n\t\t\th->resv_huge_pages,\n\t\t\th->surplus_huge_pages,\n\t\t\t1UL << (huge_page_order(h) + PAGE_SHIFT - 10));\n}\n\nint hugetlb_report_node_meminfo(int nid, char *buf)\n{\n\tstruct hstate *h = &default_hstate;\n\tif (!hugepages_supported())\n\t\treturn 0;\n\treturn sprintf(buf,\n\t\t\"Node %d HugePages_Total: %5u\\n\"\n\t\t\"Node %d HugePages_Free:  %5u\\n\"\n\t\t\"Node %d HugePages_Surp:  %5u\\n\",\n\t\tnid, h->nr_huge_pages_node[nid],\n\t\tnid, h->free_huge_pages_node[nid],\n\t\tnid, h->surplus_huge_pages_node[nid]);\n}\n\nvoid hugetlb_show_meminfo(void)\n{\n\tstruct hstate *h;\n\tint nid;\n\n\tif (!hugepages_supported())\n\t\treturn;\n\n\tfor_each_node_state(nid, N_MEMORY)\n\t\tfor_each_hstate(h)\n\t\t\tpr_info(\"Node %d hugepages_total=%u hugepages_free=%u hugepages_surp=%u hugepages_size=%lukB\\n\",\n\t\t\t\tnid,\n\t\t\t\th->nr_huge_pages_node[nid],\n\t\t\t\th->free_huge_pages_node[nid],\n\t\t\t\th->surplus_huge_pages_node[nid],\n\t\t\t\t1UL << (huge_page_order(h) + PAGE_SHIFT - 10));\n}\n\nvoid hugetlb_report_usage(struct seq_file *m, struct mm_struct *mm)\n{\n\tseq_printf(m, \"HugetlbPages:\\t%8lu kB\\n\",\n\t\t   atomic_long_read(&mm->hugetlb_usage) << (PAGE_SHIFT - 10));\n}\n\n/* Return the number pages of memory we physically have, in PAGE_SIZE units. */\nunsigned long hugetlb_total_pages(void)\n{\n\tstruct hstate *h;\n\tunsigned long nr_total_pages = 0;\n\n\tfor_each_hstate(h)\n\t\tnr_total_pages += h->nr_huge_pages * pages_per_huge_page(h);\n\treturn nr_total_pages;\n}\n\nstatic int hugetlb_acct_memory(struct hstate *h, long delta)\n{\n\tint ret = -ENOMEM;\n\n\tspin_lock(&hugetlb_lock);\n\t/*\n\t * When cpuset is configured, it breaks the strict hugetlb page\n\t * reservation as the accounting is done on a global variable. Such\n\t * reservation is completely rubbish in the presence of cpuset because\n\t * the reservation is not checked against page availability for the\n\t * current cpuset. Application can still potentially OOM'ed by kernel\n\t * with lack of free htlb page in cpuset that the task is in.\n\t * Attempt to enforce strict accounting with cpuset is almost\n\t * impossible (or too ugly) because cpuset is too fluid that\n\t * task or memory node can be dynamically moved between cpusets.\n\t *\n\t * The change of semantics for shared hugetlb mapping with cpuset is\n\t * undesirable. However, in order to preserve some of the semantics,\n\t * we fall back to check against current free page availability as\n\t * a best attempt and hopefully to minimize the impact of changing\n\t * semantics that cpuset has.\n\t */\n\tif (delta > 0) {\n\t\tif (gather_surplus_pages(h, delta) < 0)\n\t\t\tgoto out;\n\n\t\tif (delta > cpuset_mems_nr(h->free_huge_pages_node)) {\n\t\t\treturn_unused_surplus_pages(h, delta);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = 0;\n\tif (delta < 0)\n\t\treturn_unused_surplus_pages(h, (unsigned long) -delta);\n\nout:\n\tspin_unlock(&hugetlb_lock);\n\treturn ret;\n}\n\nstatic void hugetlb_vm_op_open(struct vm_area_struct *vma)\n{\n\tstruct resv_map *resv = vma_resv_map(vma);\n\n\t/*\n\t * This new VMA should share its siblings reservation map if present.\n\t * The VMA will only ever have a valid reservation map pointer where\n\t * it is being copied for another still existing VMA.  As that VMA\n\t * has a reference to the reservation map it cannot disappear until\n\t * after this open call completes.  It is therefore safe to take a\n\t * new reference here without additional locking.\n\t */\n\tif (resv && is_vma_resv_set(vma, HPAGE_RESV_OWNER))\n\t\tkref_get(&resv->refs);\n}\n\nstatic void hugetlb_vm_op_close(struct vm_area_struct *vma)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct resv_map *resv = vma_resv_map(vma);\n\tstruct hugepage_subpool *spool = subpool_vma(vma);\n\tunsigned long reserve, start, end;\n\tlong gbl_reserve;\n\n\tif (!resv || !is_vma_resv_set(vma, HPAGE_RESV_OWNER))\n\t\treturn;\n\n\tstart = vma_hugecache_offset(h, vma, vma->vm_start);\n\tend = vma_hugecache_offset(h, vma, vma->vm_end);\n\n\treserve = (end - start) - region_count(resv, start, end);\n\n\tkref_put(&resv->refs, resv_map_release);\n\n\tif (reserve) {\n\t\t/*\n\t\t * Decrement reserve counts.  The global reserve count may be\n\t\t * adjusted if the subpool has a minimum size.\n\t\t */\n\t\tgbl_reserve = hugepage_subpool_put_pages(spool, reserve);\n\t\thugetlb_acct_memory(h, -gbl_reserve);\n\t}\n}\n\n/*\n * We cannot handle pagefaults against hugetlb pages at all.  They cause\n * handle_mm_fault() to try to instantiate regular-sized pages in the\n * hugegpage VMA.  do_page_fault() is supposed to trap this, so BUG is we get\n * this far.\n */\nstatic int hugetlb_vm_op_fault(struct vm_fault *vmf)\n{\n\tBUG();\n\treturn 0;\n}\n\nconst struct vm_operations_struct hugetlb_vm_ops = {\n\t.fault = hugetlb_vm_op_fault,\n\t.open = hugetlb_vm_op_open,\n\t.close = hugetlb_vm_op_close,\n};\n\nstatic pte_t make_huge_pte(struct vm_area_struct *vma, struct page *page,\n\t\t\t\tint writable)\n{\n\tpte_t entry;\n\n\tif (writable) {\n\t\tentry = huge_pte_mkwrite(huge_pte_mkdirty(mk_huge_pte(page,\n\t\t\t\t\t vma->vm_page_prot)));\n\t} else {\n\t\tentry = huge_pte_wrprotect(mk_huge_pte(page,\n\t\t\t\t\t   vma->vm_page_prot));\n\t}\n\tentry = pte_mkyoung(entry);\n\tentry = pte_mkhuge(entry);\n\tentry = arch_make_huge_pte(entry, vma, page, writable);\n\n\treturn entry;\n}\n\nstatic void set_huge_ptep_writable(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long address, pte_t *ptep)\n{\n\tpte_t entry;\n\n\tentry = huge_pte_mkwrite(huge_pte_mkdirty(huge_ptep_get(ptep)));\n\tif (huge_ptep_set_access_flags(vma, address, ptep, entry, 1))\n\t\tupdate_mmu_cache(vma, address, ptep);\n}\n\nbool is_hugetlb_entry_migration(pte_t pte)\n{\n\tswp_entry_t swp;\n\n\tif (huge_pte_none(pte) || pte_present(pte))\n\t\treturn false;\n\tswp = pte_to_swp_entry(pte);\n\tif (non_swap_entry(swp) && is_migration_entry(swp))\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nstatic int is_hugetlb_entry_hwpoisoned(pte_t pte)\n{\n\tswp_entry_t swp;\n\n\tif (huge_pte_none(pte) || pte_present(pte))\n\t\treturn 0;\n\tswp = pte_to_swp_entry(pte);\n\tif (non_swap_entry(swp) && is_hwpoison_entry(swp))\n\t\treturn 1;\n\telse\n\t\treturn 0;\n}\n\nint copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,\n\t\t\t    struct vm_area_struct *vma)\n{\n\tpte_t *src_pte, *dst_pte, entry;\n\tstruct page *ptepage;\n\tunsigned long addr;\n\tint cow;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long sz = huge_page_size(h);\n\tunsigned long mmun_start;\t/* For mmu_notifiers */\n\tunsigned long mmun_end;\t\t/* For mmu_notifiers */\n\tint ret = 0;\n\n\tcow = (vma->vm_flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;\n\n\tmmun_start = vma->vm_start;\n\tmmun_end = vma->vm_end;\n\tif (cow)\n\t\tmmu_notifier_invalidate_range_start(src, mmun_start, mmun_end);\n\n\tfor (addr = vma->vm_start; addr < vma->vm_end; addr += sz) {\n\t\tspinlock_t *src_ptl, *dst_ptl;\n\t\tsrc_pte = huge_pte_offset(src, addr, sz);\n\t\tif (!src_pte)\n\t\t\tcontinue;\n\t\tdst_pte = huge_pte_alloc(dst, addr, sz);\n\t\tif (!dst_pte) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* If the pagetables are shared don't copy or take references */\n\t\tif (dst_pte == src_pte)\n\t\t\tcontinue;\n\n\t\tdst_ptl = huge_pte_lock(h, dst, dst_pte);\n\t\tsrc_ptl = huge_pte_lockptr(h, src, src_pte);\n\t\tspin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);\n\t\tentry = huge_ptep_get(src_pte);\n\t\tif (huge_pte_none(entry)) { /* skip none entry */\n\t\t\t;\n\t\t} else if (unlikely(is_hugetlb_entry_migration(entry) ||\n\t\t\t\t    is_hugetlb_entry_hwpoisoned(entry))) {\n\t\t\tswp_entry_t swp_entry = pte_to_swp_entry(entry);\n\n\t\t\tif (is_write_migration_entry(swp_entry) && cow) {\n\t\t\t\t/*\n\t\t\t\t * COW mappings require pages in both\n\t\t\t\t * parent and child to be set to read.\n\t\t\t\t */\n\t\t\t\tmake_migration_entry_read(&swp_entry);\n\t\t\t\tentry = swp_entry_to_pte(swp_entry);\n\t\t\t\tset_huge_swap_pte_at(src, addr, src_pte,\n\t\t\t\t\t\t     entry, sz);\n\t\t\t}\n\t\t\tset_huge_swap_pte_at(dst, addr, dst_pte, entry, sz);\n\t\t} else {\n\t\t\tif (cow) {\n\t\t\t\thuge_ptep_set_wrprotect(src, addr, src_pte);\n\t\t\t\tmmu_notifier_invalidate_range(src, mmun_start,\n\t\t\t\t\t\t\t\t   mmun_end);\n\t\t\t}\n\t\t\tentry = huge_ptep_get(src_pte);\n\t\t\tptepage = pte_page(entry);\n\t\t\tget_page(ptepage);\n\t\t\tpage_dup_rmap(ptepage, true);\n\t\t\tset_huge_pte_at(dst, addr, dst_pte, entry);\n\t\t\thugetlb_count_add(pages_per_huge_page(h), dst);\n\t\t}\n\t\tspin_unlock(src_ptl);\n\t\tspin_unlock(dst_ptl);\n\t}\n\n\tif (cow)\n\t\tmmu_notifier_invalidate_range_end(src, mmun_start, mmun_end);\n\n\treturn ret;\n}\n\nvoid __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,\n\t\t\t    unsigned long start, unsigned long end,\n\t\t\t    struct page *ref_page)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long address;\n\tpte_t *ptep;\n\tpte_t pte;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long sz = huge_page_size(h);\n\tconst unsigned long mmun_start = start;\t/* For mmu_notifiers */\n\tconst unsigned long mmun_end   = end;\t/* For mmu_notifiers */\n\n\tWARN_ON(!is_vm_hugetlb_page(vma));\n\tBUG_ON(start & ~huge_page_mask(h));\n\tBUG_ON(end & ~huge_page_mask(h));\n\n\t/*\n\t * This is a hugetlb vma, all the pte entries should point\n\t * to huge page.\n\t */\n\ttlb_remove_check_page_size_change(tlb, sz);\n\ttlb_start_vma(tlb, vma);\n\tmmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);\n\taddress = start;\n\tfor (; address < end; address += sz) {\n\t\tptep = huge_pte_offset(mm, address, sz);\n\t\tif (!ptep)\n\t\t\tcontinue;\n\n\t\tptl = huge_pte_lock(h, mm, ptep);\n\t\tif (huge_pmd_unshare(mm, &address, ptep)) {\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\n\t\tpte = huge_ptep_get(ptep);\n\t\tif (huge_pte_none(pte)) {\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Migrating hugepage or HWPoisoned hugepage is already\n\t\t * unmapped and its refcount is dropped, so just clear pte here.\n\t\t */\n\t\tif (unlikely(!pte_present(pte))) {\n\t\t\thuge_pte_clear(mm, address, ptep, sz);\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\n\t\tpage = pte_page(pte);\n\t\t/*\n\t\t * If a reference page is supplied, it is because a specific\n\t\t * page is being unmapped, not a range. Ensure the page we\n\t\t * are about to unmap is the actual page of interest.\n\t\t */\n\t\tif (ref_page) {\n\t\t\tif (page != ref_page) {\n\t\t\t\tspin_unlock(ptl);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Mark the VMA as having unmapped its page so that\n\t\t\t * future faults in this VMA will fail rather than\n\t\t\t * looking like data was lost\n\t\t\t */\n\t\t\tset_vma_resv_flags(vma, HPAGE_RESV_UNMAPPED);\n\t\t}\n\n\t\tpte = huge_ptep_get_and_clear(mm, address, ptep);\n\t\ttlb_remove_huge_tlb_entry(h, tlb, ptep, address);\n\t\tif (huge_pte_dirty(pte))\n\t\t\tset_page_dirty(page);\n\n\t\thugetlb_count_sub(pages_per_huge_page(h), mm);\n\t\tpage_remove_rmap(page, true);\n\n\t\tspin_unlock(ptl);\n\t\ttlb_remove_page_size(tlb, page, huge_page_size(h));\n\t\t/*\n\t\t * Bail out after unmapping reference page if supplied\n\t\t */\n\t\tif (ref_page)\n\t\t\tbreak;\n\t}\n\tmmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);\n\ttlb_end_vma(tlb, vma);\n}\n\nvoid __unmap_hugepage_range_final(struct mmu_gather *tlb,\n\t\t\t  struct vm_area_struct *vma, unsigned long start,\n\t\t\t  unsigned long end, struct page *ref_page)\n{\n\t__unmap_hugepage_range(tlb, vma, start, end, ref_page);\n\n\t/*\n\t * Clear this flag so that x86's huge_pmd_share page_table_shareable\n\t * test will fail on a vma being torn down, and not grab a page table\n\t * on its way out.  We're lucky that the flag has such an appropriate\n\t * name, and can in fact be safely cleared here. We could clear it\n\t * before the __unmap_hugepage_range above, but all that's necessary\n\t * is to clear it before releasing the i_mmap_rwsem. This works\n\t * because in the context this is called, the VMA is about to be\n\t * destroyed and the i_mmap_rwsem is held.\n\t */\n\tvma->vm_flags &= ~VM_MAYSHARE;\n}\n\nvoid unmap_hugepage_range(struct vm_area_struct *vma, unsigned long start,\n\t\t\t  unsigned long end, struct page *ref_page)\n{\n\tstruct mm_struct *mm;\n\tstruct mmu_gather tlb;\n\n\tmm = vma->vm_mm;\n\n\ttlb_gather_mmu(&tlb, mm, start, end);\n\t__unmap_hugepage_range(&tlb, vma, start, end, ref_page);\n\ttlb_finish_mmu(&tlb, start, end);\n}\n\n/*\n * This is called when the original mapper is failing to COW a MAP_PRIVATE\n * mappping it owns the reserve page for. The intention is to unmap the page\n * from other VMAs and let the children be SIGKILLed if they are faulting the\n * same region.\n */\nstatic void unmap_ref_private(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t      struct page *page, unsigned long address)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct vm_area_struct *iter_vma;\n\tstruct address_space *mapping;\n\tpgoff_t pgoff;\n\n\t/*\n\t * vm_pgoff is in PAGE_SIZE units, hence the different calculation\n\t * from page cache lookup which is in HPAGE_SIZE units.\n\t */\n\taddress = address & huge_page_mask(h);\n\tpgoff = ((address - vma->vm_start) >> PAGE_SHIFT) +\n\t\t\tvma->vm_pgoff;\n\tmapping = vma->vm_file->f_mapping;\n\n\t/*\n\t * Take the mapping lock for the duration of the table walk. As\n\t * this mapping should be shared between all the VMAs,\n\t * __unmap_hugepage_range() is called as the lock is already held\n\t */\n\ti_mmap_lock_write(mapping);\n\tvma_interval_tree_foreach(iter_vma, &mapping->i_mmap, pgoff, pgoff) {\n\t\t/* Do not unmap the current VMA */\n\t\tif (iter_vma == vma)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Shared VMAs have their own reserves and do not affect\n\t\t * MAP_PRIVATE accounting but it is possible that a shared\n\t\t * VMA is using the same page so check and skip such VMAs.\n\t\t */\n\t\tif (iter_vma->vm_flags & VM_MAYSHARE)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Unmap the page from other VMAs without their own reserves.\n\t\t * They get marked to be SIGKILLed if they fault in these\n\t\t * areas. This is because a future no-page fault on this VMA\n\t\t * could insert a zeroed page instead of the data existing\n\t\t * from the time of fork. This would look like data corruption\n\t\t */\n\t\tif (!is_vma_resv_set(iter_vma, HPAGE_RESV_OWNER))\n\t\t\tunmap_hugepage_range(iter_vma, address,\n\t\t\t\t\t     address + huge_page_size(h), page);\n\t}\n\ti_mmap_unlock_write(mapping);\n}\n\n/*\n * Hugetlb_cow() should be called with page lock of the original hugepage held.\n * Called with hugetlb_instantiation_mutex held and pte_page locked so we\n * cannot race with other handlers or page migration.\n * Keep the pte_same checks anyway to make transition from the mutex easier.\n */\nstatic int hugetlb_cow(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t       unsigned long address, pte_t *ptep,\n\t\t       struct page *pagecache_page, spinlock_t *ptl)\n{\n\tpte_t pte;\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct page *old_page, *new_page;\n\tint ret = 0, outside_reserve = 0;\n\tunsigned long mmun_start;\t/* For mmu_notifiers */\n\tunsigned long mmun_end;\t\t/* For mmu_notifiers */\n\n\tpte = huge_ptep_get(ptep);\n\told_page = pte_page(pte);\n\nretry_avoidcopy:\n\t/* If no-one else is actually using this page, avoid the copy\n\t * and just make the page writable */\n\tif (page_mapcount(old_page) == 1 && PageAnon(old_page)) {\n\t\tpage_move_anon_rmap(old_page, vma);\n\t\tset_huge_ptep_writable(vma, address, ptep);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * If the process that created a MAP_PRIVATE mapping is about to\n\t * perform a COW due to a shared page count, attempt to satisfy\n\t * the allocation without using the existing reserves. The pagecache\n\t * page is used to determine if the reserve at this address was\n\t * consumed or not. If reserves were used, a partial faulted mapping\n\t * at the time of fork() could consume its reserves on COW instead\n\t * of the full address range.\n\t */\n\tif (is_vma_resv_set(vma, HPAGE_RESV_OWNER) &&\n\t\t\told_page != pagecache_page)\n\t\toutside_reserve = 1;\n\n\tget_page(old_page);\n\n\t/*\n\t * Drop page table lock as buddy allocator may be called. It will\n\t * be acquired again before returning to the caller, as expected.\n\t */\n\tspin_unlock(ptl);\n\tnew_page = alloc_huge_page(vma, address, outside_reserve);\n\n\tif (IS_ERR(new_page)) {\n\t\t/*\n\t\t * If a process owning a MAP_PRIVATE mapping fails to COW,\n\t\t * it is due to references held by a child and an insufficient\n\t\t * huge page pool. To guarantee the original mappers\n\t\t * reliability, unmap the page from child processes. The child\n\t\t * may get SIGKILLed if it later faults.\n\t\t */\n\t\tif (outside_reserve) {\n\t\t\tput_page(old_page);\n\t\t\tBUG_ON(huge_pte_none(pte));\n\t\t\tunmap_ref_private(mm, vma, old_page, address);\n\t\t\tBUG_ON(huge_pte_none(pte));\n\t\t\tspin_lock(ptl);\n\t\t\tptep = huge_pte_offset(mm, address & huge_page_mask(h),\n\t\t\t\t\t       huge_page_size(h));\n\t\t\tif (likely(ptep &&\n\t\t\t\t   pte_same(huge_ptep_get(ptep), pte)))\n\t\t\t\tgoto retry_avoidcopy;\n\t\t\t/*\n\t\t\t * race occurs while re-acquiring page table\n\t\t\t * lock, and our job is done.\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\n\t\tret = (PTR_ERR(new_page) == -ENOMEM) ?\n\t\t\tVM_FAULT_OOM : VM_FAULT_SIGBUS;\n\t\tgoto out_release_old;\n\t}\n\n\t/*\n\t * When the original hugepage is shared one, it does not have\n\t * anon_vma prepared.\n\t */\n\tif (unlikely(anon_vma_prepare(vma))) {\n\t\tret = VM_FAULT_OOM;\n\t\tgoto out_release_all;\n\t}\n\n\tcopy_user_huge_page(new_page, old_page, address, vma,\n\t\t\t    pages_per_huge_page(h));\n\t__SetPageUptodate(new_page);\n\tset_page_huge_active(new_page);\n\n\tmmun_start = address & huge_page_mask(h);\n\tmmun_end = mmun_start + huge_page_size(h);\n\tmmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);\n\n\t/*\n\t * Retake the page table lock to check for racing updates\n\t * before the page tables are altered\n\t */\n\tspin_lock(ptl);\n\tptep = huge_pte_offset(mm, address & huge_page_mask(h),\n\t\t\t       huge_page_size(h));\n\tif (likely(ptep && pte_same(huge_ptep_get(ptep), pte))) {\n\t\tClearPagePrivate(new_page);\n\n\t\t/* Break COW */\n\t\thuge_ptep_clear_flush(vma, address, ptep);\n\t\tmmu_notifier_invalidate_range(mm, mmun_start, mmun_end);\n\t\tset_huge_pte_at(mm, address, ptep,\n\t\t\t\tmake_huge_pte(vma, new_page, 1));\n\t\tpage_remove_rmap(old_page, true);\n\t\thugepage_add_new_anon_rmap(new_page, vma, address);\n\t\t/* Make the old page be freed below */\n\t\tnew_page = old_page;\n\t}\n\tspin_unlock(ptl);\n\tmmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);\nout_release_all:\n\trestore_reserve_on_error(h, vma, address, new_page);\n\tput_page(new_page);\nout_release_old:\n\tput_page(old_page);\n\n\tspin_lock(ptl); /* Caller expects lock to be held */\n\treturn ret;\n}\n\n/* Return the pagecache page at a given address within a VMA */\nstatic struct page *hugetlbfs_pagecache_page(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tstruct address_space *mapping;\n\tpgoff_t idx;\n\n\tmapping = vma->vm_file->f_mapping;\n\tidx = vma_hugecache_offset(h, vma, address);\n\n\treturn find_lock_page(mapping, idx);\n}\n\n/*\n * Return whether there is a pagecache page to back given address within VMA.\n * Caller follow_hugetlb_page() holds page_table_lock so we cannot lock_page.\n */\nstatic bool hugetlbfs_pagecache_present(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tstruct address_space *mapping;\n\tpgoff_t idx;\n\tstruct page *page;\n\n\tmapping = vma->vm_file->f_mapping;\n\tidx = vma_hugecache_offset(h, vma, address);\n\n\tpage = find_get_page(mapping, idx);\n\tif (page)\n\t\tput_page(page);\n\treturn page != NULL;\n}\n\nint huge_add_to_page_cache(struct page *page, struct address_space *mapping,\n\t\t\t   pgoff_t idx)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct hstate *h = hstate_inode(inode);\n\tint err = add_to_page_cache(page, mapping, idx, GFP_KERNEL);\n\n\tif (err)\n\t\treturn err;\n\tClearPagePrivate(page);\n\n\tspin_lock(&inode->i_lock);\n\tinode->i_blocks += blocks_per_huge_page(h);\n\tspin_unlock(&inode->i_lock);\n\treturn 0;\n}\n\nstatic int hugetlb_no_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t   struct address_space *mapping, pgoff_t idx,\n\t\t\t   unsigned long address, pte_t *ptep, unsigned int flags)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tint ret = VM_FAULT_SIGBUS;\n\tint anon_rmap = 0;\n\tunsigned long size;\n\tstruct page *page;\n\tpte_t new_pte;\n\tspinlock_t *ptl;\n\n\t/*\n\t * Currently, we are forced to kill the process in the event the\n\t * original mapper has unmapped pages from the child due to a failed\n\t * COW. Warn that such a situation has occurred as it may not be obvious\n\t */\n\tif (is_vma_resv_set(vma, HPAGE_RESV_UNMAPPED)) {\n\t\tpr_warn_ratelimited(\"PID %d killed due to inadequate hugepage pool\\n\",\n\t\t\t   current->pid);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * Use page lock to guard against racing truncation\n\t * before we get page_table_lock.\n\t */\nretry:\n\tpage = find_lock_page(mapping, idx);\n\tif (!page) {\n\t\tsize = i_size_read(mapping->host) >> huge_page_shift(h);\n\t\tif (idx >= size)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * Check for page in userfault range\n\t\t */\n\t\tif (userfaultfd_missing(vma)) {\n\t\t\tu32 hash;\n\t\t\tstruct vm_fault vmf = {\n\t\t\t\t.vma = vma,\n\t\t\t\t.address = address,\n\t\t\t\t.flags = flags,\n\t\t\t\t/*\n\t\t\t\t * Hard to debug if it ends up being\n\t\t\t\t * used by a callee that assumes\n\t\t\t\t * something about the other\n\t\t\t\t * uninitialized fields... same as in\n\t\t\t\t * memory.c\n\t\t\t\t */\n\t\t\t};\n\n\t\t\t/*\n\t\t\t * hugetlb_fault_mutex must be dropped before\n\t\t\t * handling userfault.  Reacquire after handling\n\t\t\t * fault to make calling code simpler.\n\t\t\t */\n\t\t\thash = hugetlb_fault_mutex_hash(h, mm, vma, mapping,\n\t\t\t\t\t\t\tidx, address);\n\t\t\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\t\t\tret = handle_userfault(&vmf, VM_UFFD_MISSING);\n\t\t\tmutex_lock(&hugetlb_fault_mutex_table[hash]);\n\t\t\tgoto out;\n\t\t}\n\n\t\tpage = alloc_huge_page(vma, address, 0);\n\t\tif (IS_ERR(page)) {\n\t\t\tret = PTR_ERR(page);\n\t\t\tif (ret == -ENOMEM)\n\t\t\t\tret = VM_FAULT_OOM;\n\t\t\telse\n\t\t\t\tret = VM_FAULT_SIGBUS;\n\t\t\tgoto out;\n\t\t}\n\t\tclear_huge_page(page, address, pages_per_huge_page(h));\n\t\t__SetPageUptodate(page);\n\t\tset_page_huge_active(page);\n\n\t\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\t\tint err = huge_add_to_page_cache(page, mapping, idx);\n\t\t\tif (err) {\n\t\t\t\tput_page(page);\n\t\t\t\tif (err == -EEXIST)\n\t\t\t\t\tgoto retry;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else {\n\t\t\tlock_page(page);\n\t\t\tif (unlikely(anon_vma_prepare(vma))) {\n\t\t\t\tret = VM_FAULT_OOM;\n\t\t\t\tgoto backout_unlocked;\n\t\t\t}\n\t\t\tanon_rmap = 1;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If memory error occurs between mmap() and fault, some process\n\t\t * don't have hwpoisoned swap entry for errored virtual address.\n\t\t * So we need to block hugepage fault by PG_hwpoison bit check.\n\t\t */\n\t\tif (unlikely(PageHWPoison(page))) {\n\t\t\tret = VM_FAULT_HWPOISON |\n\t\t\t\tVM_FAULT_SET_HINDEX(hstate_index(h));\n\t\t\tgoto backout_unlocked;\n\t\t}\n\t}\n\n\t/*\n\t * If we are going to COW a private mapping later, we examine the\n\t * pending reservations for this page now. This will ensure that\n\t * any allocations necessary to record that reservation occur outside\n\t * the spinlock.\n\t */\n\tif ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {\n\t\tif (vma_needs_reservation(h, vma, address) < 0) {\n\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto backout_unlocked;\n\t\t}\n\t\t/* Just decrements count, does not deallocate */\n\t\tvma_end_reservation(h, vma, address);\n\t}\n\n\tptl = huge_pte_lock(h, mm, ptep);\n\tsize = i_size_read(mapping->host) >> huge_page_shift(h);\n\tif (idx >= size)\n\t\tgoto backout;\n\n\tret = 0;\n\tif (!huge_pte_none(huge_ptep_get(ptep)))\n\t\tgoto backout;\n\n\tif (anon_rmap) {\n\t\tClearPagePrivate(page);\n\t\thugepage_add_new_anon_rmap(page, vma, address);\n\t} else\n\t\tpage_dup_rmap(page, true);\n\tnew_pte = make_huge_pte(vma, page, ((vma->vm_flags & VM_WRITE)\n\t\t\t\t&& (vma->vm_flags & VM_SHARED)));\n\tset_huge_pte_at(mm, address, ptep, new_pte);\n\n\thugetlb_count_add(pages_per_huge_page(h), mm);\n\tif ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {\n\t\t/* Optimization, do the COW without a second fault */\n\t\tret = hugetlb_cow(mm, vma, address, ptep, page, ptl);\n\t}\n\n\tspin_unlock(ptl);\n\tunlock_page(page);\nout:\n\treturn ret;\n\nbackout:\n\tspin_unlock(ptl);\nbackout_unlocked:\n\tunlock_page(page);\n\trestore_reserve_on_error(h, vma, address, page);\n\tput_page(page);\n\tgoto out;\n}\n\n#ifdef CONFIG_SMP\nu32 hugetlb_fault_mutex_hash(struct hstate *h, struct mm_struct *mm,\n\t\t\t    struct vm_area_struct *vma,\n\t\t\t    struct address_space *mapping,\n\t\t\t    pgoff_t idx, unsigned long address)\n{\n\tunsigned long key[2];\n\tu32 hash;\n\n\tif (vma->vm_flags & VM_SHARED) {\n\t\tkey[0] = (unsigned long) mapping;\n\t\tkey[1] = idx;\n\t} else {\n\t\tkey[0] = (unsigned long) mm;\n\t\tkey[1] = address >> huge_page_shift(h);\n\t}\n\n\thash = jhash2((u32 *)&key, sizeof(key)/sizeof(u32), 0);\n\n\treturn hash & (num_fault_mutexes - 1);\n}\n#else\n/*\n * For uniprocesor systems we always use a single mutex, so just\n * return 0 and avoid the hashing overhead.\n */\nu32 hugetlb_fault_mutex_hash(struct hstate *h, struct mm_struct *mm,\n\t\t\t    struct vm_area_struct *vma,\n\t\t\t    struct address_space *mapping,\n\t\t\t    pgoff_t idx, unsigned long address)\n{\n\treturn 0;\n}\n#endif\n\nint hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tunsigned long address, unsigned int flags)\n{\n\tpte_t *ptep, entry;\n\tspinlock_t *ptl;\n\tint ret;\n\tu32 hash;\n\tpgoff_t idx;\n\tstruct page *page = NULL;\n\tstruct page *pagecache_page = NULL;\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct address_space *mapping;\n\tint need_wait_lock = 0;\n\n\taddress &= huge_page_mask(h);\n\n\tptep = huge_pte_offset(mm, address, huge_page_size(h));\n\tif (ptep) {\n\t\tentry = huge_ptep_get(ptep);\n\t\tif (unlikely(is_hugetlb_entry_migration(entry))) {\n\t\t\tmigration_entry_wait_huge(vma, mm, ptep);\n\t\t\treturn 0;\n\t\t} else if (unlikely(is_hugetlb_entry_hwpoisoned(entry)))\n\t\t\treturn VM_FAULT_HWPOISON_LARGE |\n\t\t\t\tVM_FAULT_SET_HINDEX(hstate_index(h));\n\t} else {\n\t\tptep = huge_pte_alloc(mm, address, huge_page_size(h));\n\t\tif (!ptep)\n\t\t\treturn VM_FAULT_OOM;\n\t}\n\n\tmapping = vma->vm_file->f_mapping;\n\tidx = vma_hugecache_offset(h, vma, address);\n\n\t/*\n\t * Serialize hugepage allocation and instantiation, so that we don't\n\t * get spurious allocation failures if two CPUs race to instantiate\n\t * the same page in the page cache.\n\t */\n\thash = hugetlb_fault_mutex_hash(h, mm, vma, mapping, idx, address);\n\tmutex_lock(&hugetlb_fault_mutex_table[hash]);\n\n\tentry = huge_ptep_get(ptep);\n\tif (huge_pte_none(entry)) {\n\t\tret = hugetlb_no_page(mm, vma, mapping, idx, address, ptep, flags);\n\t\tgoto out_mutex;\n\t}\n\n\tret = 0;\n\n\t/*\n\t * entry could be a migration/hwpoison entry at this point, so this\n\t * check prevents the kernel from going below assuming that we have\n\t * a active hugepage in pagecache. This goto expects the 2nd page fault,\n\t * and is_hugetlb_entry_(migration|hwpoisoned) check will properly\n\t * handle it.\n\t */\n\tif (!pte_present(entry))\n\t\tgoto out_mutex;\n\n\t/*\n\t * If we are going to COW the mapping later, we examine the pending\n\t * reservations for this page now. This will ensure that any\n\t * allocations necessary to record that reservation occur outside the\n\t * spinlock. For private mappings, we also lookup the pagecache\n\t * page now as it is used to determine if a reservation has been\n\t * consumed.\n\t */\n\tif ((flags & FAULT_FLAG_WRITE) && !huge_pte_write(entry)) {\n\t\tif (vma_needs_reservation(h, vma, address) < 0) {\n\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto out_mutex;\n\t\t}\n\t\t/* Just decrements count, does not deallocate */\n\t\tvma_end_reservation(h, vma, address);\n\n\t\tif (!(vma->vm_flags & VM_MAYSHARE))\n\t\t\tpagecache_page = hugetlbfs_pagecache_page(h,\n\t\t\t\t\t\t\t\tvma, address);\n\t}\n\n\tptl = huge_pte_lock(h, mm, ptep);\n\n\t/* Check for a racing update before calling hugetlb_cow */\n\tif (unlikely(!pte_same(entry, huge_ptep_get(ptep))))\n\t\tgoto out_ptl;\n\n\t/*\n\t * hugetlb_cow() requires page locks of pte_page(entry) and\n\t * pagecache_page, so here we need take the former one\n\t * when page != pagecache_page or !pagecache_page.\n\t */\n\tpage = pte_page(entry);\n\tif (page != pagecache_page)\n\t\tif (!trylock_page(page)) {\n\t\t\tneed_wait_lock = 1;\n\t\t\tgoto out_ptl;\n\t\t}\n\n\tget_page(page);\n\n\tif (flags & FAULT_FLAG_WRITE) {\n\t\tif (!huge_pte_write(entry)) {\n\t\t\tret = hugetlb_cow(mm, vma, address, ptep,\n\t\t\t\t\t  pagecache_page, ptl);\n\t\t\tgoto out_put_page;\n\t\t}\n\t\tentry = huge_pte_mkdirty(entry);\n\t}\n\tentry = pte_mkyoung(entry);\n\tif (huge_ptep_set_access_flags(vma, address, ptep, entry,\n\t\t\t\t\t\tflags & FAULT_FLAG_WRITE))\n\t\tupdate_mmu_cache(vma, address, ptep);\nout_put_page:\n\tif (page != pagecache_page)\n\t\tunlock_page(page);\n\tput_page(page);\nout_ptl:\n\tspin_unlock(ptl);\n\n\tif (pagecache_page) {\n\t\tunlock_page(pagecache_page);\n\t\tput_page(pagecache_page);\n\t}\nout_mutex:\n\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\t/*\n\t * Generally it's safe to hold refcount during waiting page lock. But\n\t * here we just wait to defer the next page fault to avoid busy loop and\n\t * the page is not used after unlocked before returning from the current\n\t * page fault. So we are safe from accessing freed page, even if we wait\n\t * here without taking refcount.\n\t */\n\tif (need_wait_lock)\n\t\twait_on_page_locked(page);\n\treturn ret;\n}\n\n/*\n * Used by userfaultfd UFFDIO_COPY.  Based on mcopy_atomic_pte with\n * modifications for huge pages.\n */\nint hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,\n\t\t\t    pte_t *dst_pte,\n\t\t\t    struct vm_area_struct *dst_vma,\n\t\t\t    unsigned long dst_addr,\n\t\t\t    unsigned long src_addr,\n\t\t\t    struct page **pagep)\n{\n\tint vm_shared = dst_vma->vm_flags & VM_SHARED;\n\tstruct hstate *h = hstate_vma(dst_vma);\n\tpte_t _dst_pte;\n\tspinlock_t *ptl;\n\tint ret;\n\tstruct page *page;\n\n\tif (!*pagep) {\n\t\tret = -ENOMEM;\n\t\tpage = alloc_huge_page(dst_vma, dst_addr, 0);\n\t\tif (IS_ERR(page))\n\t\t\tgoto out;\n\n\t\tret = copy_huge_page_from_user(page,\n\t\t\t\t\t\t(const void __user *) src_addr,\n\t\t\t\t\t\tpages_per_huge_page(h), false);\n\n\t\t/* fallback to copy_from_user outside mmap_sem */\n\t\tif (unlikely(ret)) {\n\t\t\tret = -EFAULT;\n\t\t\t*pagep = page;\n\t\t\t/* don't free the page */\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tpage = *pagep;\n\t\t*pagep = NULL;\n\t}\n\n\t/*\n\t * The memory barrier inside __SetPageUptodate makes sure that\n\t * preceding stores to the page contents become visible before\n\t * the set_pte_at() write.\n\t */\n\t__SetPageUptodate(page);\n\tset_page_huge_active(page);\n\n\t/*\n\t * If shared, add to page cache\n\t */\n\tif (vm_shared) {\n\t\tstruct address_space *mapping = dst_vma->vm_file->f_mapping;\n\t\tpgoff_t idx = vma_hugecache_offset(h, dst_vma, dst_addr);\n\n\t\tret = huge_add_to_page_cache(page, mapping, idx);\n\t\tif (ret)\n\t\t\tgoto out_release_nounlock;\n\t}\n\n\tptl = huge_pte_lockptr(h, dst_mm, dst_pte);\n\tspin_lock(ptl);\n\n\tret = -EEXIST;\n\tif (!huge_pte_none(huge_ptep_get(dst_pte)))\n\t\tgoto out_release_unlock;\n\n\tif (vm_shared) {\n\t\tpage_dup_rmap(page, true);\n\t} else {\n\t\tClearPagePrivate(page);\n\t\thugepage_add_new_anon_rmap(page, dst_vma, dst_addr);\n\t}\n\n\t_dst_pte = make_huge_pte(dst_vma, page, dst_vma->vm_flags & VM_WRITE);\n\tif (dst_vma->vm_flags & VM_WRITE)\n\t\t_dst_pte = huge_pte_mkdirty(_dst_pte);\n\t_dst_pte = pte_mkyoung(_dst_pte);\n\n\tset_huge_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte);\n\n\t(void)huge_ptep_set_access_flags(dst_vma, dst_addr, dst_pte, _dst_pte,\n\t\t\t\t\tdst_vma->vm_flags & VM_WRITE);\n\thugetlb_count_add(pages_per_huge_page(h), dst_mm);\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\n\n\tspin_unlock(ptl);\n\tif (vm_shared)\n\t\tunlock_page(page);\n\tret = 0;\nout:\n\treturn ret;\nout_release_unlock:\n\tspin_unlock(ptl);\n\tif (vm_shared)\n\t\tunlock_page(page);\nout_release_nounlock:\n\tput_page(page);\n\tgoto out;\n}\n\nlong follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t struct page **pages, struct vm_area_struct **vmas,\n\t\t\t unsigned long *position, unsigned long *nr_pages,\n\t\t\t long i, unsigned int flags, int *nonblocking)\n{\n\tunsigned long pfn_offset;\n\tunsigned long vaddr = *position;\n\tunsigned long remainder = *nr_pages;\n\tstruct hstate *h = hstate_vma(vma);\n\tint err = -EFAULT;\n\n\twhile (vaddr < vma->vm_end && remainder) {\n\t\tpte_t *pte;\n\t\tspinlock_t *ptl = NULL;\n\t\tint absent;\n\t\tstruct page *page;\n\n\t\t/*\n\t\t * If we have a pending SIGKILL, don't keep faulting pages and\n\t\t * potentially allocating memory.\n\t\t */\n\t\tif (unlikely(fatal_signal_pending(current))) {\n\t\t\tremainder = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Some archs (sparc64, sh*) have multiple pte_ts to\n\t\t * each hugepage.  We have to make sure we get the\n\t\t * first, for the page indexing below to work.\n\t\t *\n\t\t * Note that page table lock is not held when pte is null.\n\t\t */\n\t\tpte = huge_pte_offset(mm, vaddr & huge_page_mask(h),\n\t\t\t\t      huge_page_size(h));\n\t\tif (pte)\n\t\t\tptl = huge_pte_lock(h, mm, pte);\n\t\tabsent = !pte || huge_pte_none(huge_ptep_get(pte));\n\n\t\t/*\n\t\t * When coredumping, it suits get_dump_page if we just return\n\t\t * an error where there's an empty slot with no huge pagecache\n\t\t * to back it.  This way, we avoid allocating a hugepage, and\n\t\t * the sparse dumpfile avoids allocating disk blocks, but its\n\t\t * huge holes still show up with zeroes where they need to be.\n\t\t */\n\t\tif (absent && (flags & FOLL_DUMP) &&\n\t\t    !hugetlbfs_pagecache_present(h, vma, vaddr)) {\n\t\t\tif (pte)\n\t\t\t\tspin_unlock(ptl);\n\t\t\tremainder = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * We need call hugetlb_fault for both hugepages under migration\n\t\t * (in which case hugetlb_fault waits for the migration,) and\n\t\t * hwpoisoned hugepages (in which case we need to prevent the\n\t\t * caller from accessing to them.) In order to do this, we use\n\t\t * here is_swap_pte instead of is_hugetlb_entry_migration and\n\t\t * is_hugetlb_entry_hwpoisoned. This is because it simply covers\n\t\t * both cases, and because we can't follow correct pages\n\t\t * directly from any kind of swap entries.\n\t\t */\n\t\tif (absent || is_swap_pte(huge_ptep_get(pte)) ||\n\t\t    ((flags & FOLL_WRITE) &&\n\t\t      !huge_pte_write(huge_ptep_get(pte)))) {\n\t\t\tint ret;\n\t\t\tunsigned int fault_flags = 0;\n\n\t\t\tif (pte)\n\t\t\t\tspin_unlock(ptl);\n\t\t\tif (flags & FOLL_WRITE)\n\t\t\t\tfault_flags |= FAULT_FLAG_WRITE;\n\t\t\tif (nonblocking)\n\t\t\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY;\n\t\t\tif (flags & FOLL_NOWAIT)\n\t\t\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY |\n\t\t\t\t\tFAULT_FLAG_RETRY_NOWAIT;\n\t\t\tif (flags & FOLL_TRIED) {\n\t\t\t\tVM_WARN_ON_ONCE(fault_flags &\n\t\t\t\t\t\tFAULT_FLAG_ALLOW_RETRY);\n\t\t\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t\t\t}\n\t\t\tret = hugetlb_fault(mm, vma, vaddr, fault_flags);\n\t\t\tif (ret & VM_FAULT_ERROR) {\n\t\t\t\terr = vm_fault_to_errno(ret, flags);\n\t\t\t\tremainder = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ret & VM_FAULT_RETRY) {\n\t\t\t\tif (nonblocking)\n\t\t\t\t\t*nonblocking = 0;\n\t\t\t\t*nr_pages = 0;\n\t\t\t\t/*\n\t\t\t\t * VM_FAULT_RETRY must not return an\n\t\t\t\t * error, it will return zero\n\t\t\t\t * instead.\n\t\t\t\t *\n\t\t\t\t * No need to update \"position\" as the\n\t\t\t\t * caller will not check it after\n\t\t\t\t * *nr_pages is set to 0.\n\t\t\t\t */\n\t\t\t\treturn i;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tpfn_offset = (vaddr & ~huge_page_mask(h)) >> PAGE_SHIFT;\n\t\tpage = pte_page(huge_ptep_get(pte));\nsame_page:\n\t\tif (pages) {\n\t\t\tpages[i] = mem_map_offset(page, pfn_offset);\n\t\t\tget_page(pages[i]);\n\t\t}\n\n\t\tif (vmas)\n\t\t\tvmas[i] = vma;\n\n\t\tvaddr += PAGE_SIZE;\n\t\t++pfn_offset;\n\t\t--remainder;\n\t\t++i;\n\t\tif (vaddr < vma->vm_end && remainder &&\n\t\t\t\tpfn_offset < pages_per_huge_page(h)) {\n\t\t\t/*\n\t\t\t * We use pfn_offset to avoid touching the pageframes\n\t\t\t * of this compound page.\n\t\t\t */\n\t\t\tgoto same_page;\n\t\t}\n\t\tspin_unlock(ptl);\n\t}\n\t*nr_pages = remainder;\n\t/*\n\t * setting position is actually required only if remainder is\n\t * not zero but it's faster not to add a \"if (remainder)\"\n\t * branch.\n\t */\n\t*position = vaddr;\n\n\treturn i ? i : err;\n}\n\n#ifndef __HAVE_ARCH_FLUSH_HUGETLB_TLB_RANGE\n/*\n * ARCHes with special requirements for evicting HUGETLB backing TLB entries can\n * implement this.\n */\n#define flush_hugetlb_tlb_range(vma, addr, end)\tflush_tlb_range(vma, addr, end)\n#endif\n\nunsigned long hugetlb_change_protection(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned long end, pgprot_t newprot)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long start = address;\n\tpte_t *ptep;\n\tpte_t pte;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long pages = 0;\n\n\tBUG_ON(address >= end);\n\tflush_cache_range(vma, address, end);\n\n\tmmu_notifier_invalidate_range_start(mm, start, end);\n\ti_mmap_lock_write(vma->vm_file->f_mapping);\n\tfor (; address < end; address += huge_page_size(h)) {\n\t\tspinlock_t *ptl;\n\t\tptep = huge_pte_offset(mm, address, huge_page_size(h));\n\t\tif (!ptep)\n\t\t\tcontinue;\n\t\tptl = huge_pte_lock(h, mm, ptep);\n\t\tif (huge_pmd_unshare(mm, &address, ptep)) {\n\t\t\tpages++;\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\t\tpte = huge_ptep_get(ptep);\n\t\tif (unlikely(is_hugetlb_entry_hwpoisoned(pte))) {\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\t\tif (unlikely(is_hugetlb_entry_migration(pte))) {\n\t\t\tswp_entry_t entry = pte_to_swp_entry(pte);\n\n\t\t\tif (is_write_migration_entry(entry)) {\n\t\t\t\tpte_t newpte;\n\n\t\t\t\tmake_migration_entry_read(&entry);\n\t\t\t\tnewpte = swp_entry_to_pte(entry);\n\t\t\t\tset_huge_swap_pte_at(mm, address, ptep,\n\t\t\t\t\t\t     newpte, huge_page_size(h));\n\t\t\t\tpages++;\n\t\t\t}\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\t\tif (!huge_pte_none(pte)) {\n\t\t\tpte = huge_ptep_get_and_clear(mm, address, ptep);\n\t\t\tpte = pte_mkhuge(huge_pte_modify(pte, newprot));\n\t\t\tpte = arch_make_huge_pte(pte, vma, NULL, 0);\n\t\t\tset_huge_pte_at(mm, address, ptep, pte);\n\t\t\tpages++;\n\t\t}\n\t\tspin_unlock(ptl);\n\t}\n\t/*\n\t * Must flush TLB before releasing i_mmap_rwsem: x86's huge_pmd_unshare\n\t * may have cleared our pud entry and done put_page on the page table:\n\t * once we release i_mmap_rwsem, another task can do the final put_page\n\t * and that page table be reused and filled with junk.\n\t */\n\tflush_hugetlb_tlb_range(vma, start, end);\n\tmmu_notifier_invalidate_range(mm, start, end);\n\ti_mmap_unlock_write(vma->vm_file->f_mapping);\n\tmmu_notifier_invalidate_range_end(mm, start, end);\n\n\treturn pages << h->order;\n}\n\nint hugetlb_reserve_pages(struct inode *inode,\n\t\t\t\t\tlong from, long to,\n\t\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\t\tvm_flags_t vm_flags)\n{\n\tlong ret, chg;\n\tstruct hstate *h = hstate_inode(inode);\n\tstruct hugepage_subpool *spool = subpool_inode(inode);\n\tstruct resv_map *resv_map;\n\tlong gbl_reserve;\n\n\t/*\n\t * Only apply hugepage reservation if asked. At fault time, an\n\t * attempt will be made for VM_NORESERVE to allocate a page\n\t * without using reserves\n\t */\n\tif (vm_flags & VM_NORESERVE)\n\t\treturn 0;\n\n\t/*\n\t * Shared mappings base their reservation on the number of pages that\n\t * are already allocated on behalf of the file. Private mappings need\n\t * to reserve the full area even if read-only as mprotect() may be\n\t * called to make the mapping read-write. Assume !vma is a shm mapping\n\t */\n\tif (!vma || vma->vm_flags & VM_MAYSHARE) {\n\t\tresv_map = inode_resv_map(inode);\n\n\t\tchg = region_chg(resv_map, from, to);\n\n\t} else {\n\t\tresv_map = resv_map_alloc();\n\t\tif (!resv_map)\n\t\t\treturn -ENOMEM;\n\n\t\tchg = to - from;\n\n\t\tset_vma_resv_map(vma, resv_map);\n\t\tset_vma_resv_flags(vma, HPAGE_RESV_OWNER);\n\t}\n\n\tif (chg < 0) {\n\t\tret = chg;\n\t\tgoto out_err;\n\t}\n\n\t/*\n\t * There must be enough pages in the subpool for the mapping. If\n\t * the subpool has a minimum size, there may be some global\n\t * reservations already in place (gbl_reserve).\n\t */\n\tgbl_reserve = hugepage_subpool_get_pages(spool, chg);\n\tif (gbl_reserve < 0) {\n\t\tret = -ENOSPC;\n\t\tgoto out_err;\n\t}\n\n\t/*\n\t * Check enough hugepages are available for the reservation.\n\t * Hand the pages back to the subpool if there are not\n\t */\n\tret = hugetlb_acct_memory(h, gbl_reserve);\n\tif (ret < 0) {\n\t\t/* put back original number of pages, chg */\n\t\t(void)hugepage_subpool_put_pages(spool, chg);\n\t\tgoto out_err;\n\t}\n\n\t/*\n\t * Account for the reservations made. Shared mappings record regions\n\t * that have reservations as they are shared by multiple VMAs.\n\t * When the last VMA disappears, the region map says how much\n\t * the reservation was and the page cache tells how much of\n\t * the reservation was consumed. Private mappings are per-VMA and\n\t * only the consumed reservations are tracked. When the VMA\n\t * disappears, the original reservation is the VMA size and the\n\t * consumed reservations are stored in the map. Hence, nothing\n\t * else has to be done for private mappings here\n\t */\n\tif (!vma || vma->vm_flags & VM_MAYSHARE) {\n\t\tlong add = region_add(resv_map, from, to);\n\n\t\tif (unlikely(chg > add)) {\n\t\t\t/*\n\t\t\t * pages in this range were added to the reserve\n\t\t\t * map between region_chg and region_add.  This\n\t\t\t * indicates a race with alloc_huge_page.  Adjust\n\t\t\t * the subpool and reserve counts modified above\n\t\t\t * based on the difference.\n\t\t\t */\n\t\t\tlong rsv_adjust;\n\n\t\t\trsv_adjust = hugepage_subpool_put_pages(spool,\n\t\t\t\t\t\t\t\tchg - add);\n\t\t\thugetlb_acct_memory(h, -rsv_adjust);\n\t\t}\n\t}\n\treturn 0;\nout_err:\n\tif (!vma || vma->vm_flags & VM_MAYSHARE)\n\t\t/* Don't call region_abort if region_chg failed */\n\t\tif (chg >= 0)\n\t\t\tregion_abort(resv_map, from, to);\n\tif (vma && is_vma_resv_set(vma, HPAGE_RESV_OWNER))\n\t\tkref_put(&resv_map->refs, resv_map_release);\n\treturn ret;\n}\n\nlong hugetlb_unreserve_pages(struct inode *inode, long start, long end,\n\t\t\t\t\t\t\t\tlong freed)\n{\n\tstruct hstate *h = hstate_inode(inode);\n\tstruct resv_map *resv_map = inode_resv_map(inode);\n\tlong chg = 0;\n\tstruct hugepage_subpool *spool = subpool_inode(inode);\n\tlong gbl_reserve;\n\n\tif (resv_map) {\n\t\tchg = region_del(resv_map, start, end);\n\t\t/*\n\t\t * region_del() can fail in the rare case where a region\n\t\t * must be split and another region descriptor can not be\n\t\t * allocated.  If end == LONG_MAX, it will not fail.\n\t\t */\n\t\tif (chg < 0)\n\t\t\treturn chg;\n\t}\n\n\tspin_lock(&inode->i_lock);\n\tinode->i_blocks -= (blocks_per_huge_page(h) * freed);\n\tspin_unlock(&inode->i_lock);\n\n\t/*\n\t * If the subpool has a minimum size, the number of global\n\t * reservations to be released may be adjusted.\n\t */\n\tgbl_reserve = hugepage_subpool_put_pages(spool, (chg - freed));\n\thugetlb_acct_memory(h, -gbl_reserve);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE\nstatic unsigned long page_table_shareable(struct vm_area_struct *svma,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tunsigned long addr, pgoff_t idx)\n{\n\tunsigned long saddr = ((idx - svma->vm_pgoff) << PAGE_SHIFT) +\n\t\t\t\tsvma->vm_start;\n\tunsigned long sbase = saddr & PUD_MASK;\n\tunsigned long s_end = sbase + PUD_SIZE;\n\n\t/* Allow segments to share if only one is marked locked */\n\tunsigned long vm_flags = vma->vm_flags & VM_LOCKED_CLEAR_MASK;\n\tunsigned long svm_flags = svma->vm_flags & VM_LOCKED_CLEAR_MASK;\n\n\t/*\n\t * match the virtual addresses, permission and the alignment of the\n\t * page table page.\n\t */\n\tif (pmd_index(addr) != pmd_index(saddr) ||\n\t    vm_flags != svm_flags ||\n\t    sbase < svma->vm_start || svma->vm_end < s_end)\n\t\treturn 0;\n\n\treturn saddr;\n}\n\nstatic bool vma_shareable(struct vm_area_struct *vma, unsigned long addr)\n{\n\tunsigned long base = addr & PUD_MASK;\n\tunsigned long end = base + PUD_SIZE;\n\n\t/*\n\t * check on proper vm_flags and page table alignment\n\t */\n\tif (vma->vm_flags & VM_MAYSHARE &&\n\t    vma->vm_start <= base && end <= vma->vm_end)\n\t\treturn true;\n\treturn false;\n}\n\n/*\n * Search for a shareable pmd page for hugetlb. In any case calls pmd_alloc()\n * and returns the corresponding pte. While this is not necessary for the\n * !shared pmd case because we can allocate the pmd later as well, it makes the\n * code much cleaner. pmd allocation is essential for the shared case because\n * pud has to be populated inside the same i_mmap_rwsem section - otherwise\n * racing tasks could either miss the sharing (see huge_pte_offset) or select a\n * bad pmd for sharing.\n */\npte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud)\n{\n\tstruct vm_area_struct *vma = find_vma(mm, addr);\n\tstruct address_space *mapping = vma->vm_file->f_mapping;\n\tpgoff_t idx = ((addr - vma->vm_start) >> PAGE_SHIFT) +\n\t\t\tvma->vm_pgoff;\n\tstruct vm_area_struct *svma;\n\tunsigned long saddr;\n\tpte_t *spte = NULL;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\tif (!vma_shareable(vma, addr))\n\t\treturn (pte_t *)pmd_alloc(mm, pud, addr);\n\n\ti_mmap_lock_write(mapping);\n\tvma_interval_tree_foreach(svma, &mapping->i_mmap, idx, idx) {\n\t\tif (svma == vma)\n\t\t\tcontinue;\n\n\t\tsaddr = page_table_shareable(svma, vma, addr, idx);\n\t\tif (saddr) {\n\t\t\tspte = huge_pte_offset(svma->vm_mm, saddr,\n\t\t\t\t\t       vma_mmu_pagesize(svma));\n\t\t\tif (spte) {\n\t\t\t\tget_page(virt_to_page(spte));\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!spte)\n\t\tgoto out;\n\n\tptl = huge_pte_lock(hstate_vma(vma), mm, spte);\n\tif (pud_none(*pud)) {\n\t\tpud_populate(mm, pud,\n\t\t\t\t(pmd_t *)((unsigned long)spte & PAGE_MASK));\n\t\tmm_inc_nr_pmds(mm);\n\t} else {\n\t\tput_page(virt_to_page(spte));\n\t}\n\tspin_unlock(ptl);\nout:\n\tpte = (pte_t *)pmd_alloc(mm, pud, addr);\n\ti_mmap_unlock_write(mapping);\n\treturn pte;\n}\n\n/*\n * unmap huge page backed by shared pte.\n *\n * Hugetlb pte page is ref counted at the time of mapping.  If pte is shared\n * indicated by page_count > 1, unmap is achieved by clearing pud and\n * decrementing the ref count. If count == 1, the pte page is not shared.\n *\n * called with page table lock held.\n *\n * returns: 1 successfully unmapped a shared pte page\n *\t    0 the underlying pte page is not shared, or it is the last user\n */\nint huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep)\n{\n\tpgd_t *pgd = pgd_offset(mm, *addr);\n\tp4d_t *p4d = p4d_offset(pgd, *addr);\n\tpud_t *pud = pud_offset(p4d, *addr);\n\n\tBUG_ON(page_count(virt_to_page(ptep)) == 0);\n\tif (page_count(virt_to_page(ptep)) == 1)\n\t\treturn 0;\n\n\tpud_clear(pud);\n\tput_page(virt_to_page(ptep));\n\tmm_dec_nr_pmds(mm);\n\t*addr = ALIGN(*addr, HPAGE_SIZE * PTRS_PER_PTE) - HPAGE_SIZE;\n\treturn 1;\n}\n#define want_pmd_share()\t(1)\n#else /* !CONFIG_ARCH_WANT_HUGE_PMD_SHARE */\npte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud)\n{\n\treturn NULL;\n}\n\nint huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep)\n{\n\treturn 0;\n}\n#define want_pmd_share()\t(0)\n#endif /* CONFIG_ARCH_WANT_HUGE_PMD_SHARE */\n\n#ifdef CONFIG_ARCH_WANT_GENERAL_HUGETLB\npte_t *huge_pte_alloc(struct mm_struct *mm,\n\t\t\tunsigned long addr, unsigned long sz)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpte_t *pte = NULL;\n\n\tpgd = pgd_offset(mm, addr);\n\tp4d = p4d_offset(pgd, addr);\n\tpud = pud_alloc(mm, p4d, addr);\n\tif (pud) {\n\t\tif (sz == PUD_SIZE) {\n\t\t\tpte = (pte_t *)pud;\n\t\t} else {\n\t\t\tBUG_ON(sz != PMD_SIZE);\n\t\t\tif (want_pmd_share() && pud_none(*pud))\n\t\t\t\tpte = huge_pmd_share(mm, addr, pud);\n\t\t\telse\n\t\t\t\tpte = (pte_t *)pmd_alloc(mm, pud, addr);\n\t\t}\n\t}\n\tBUG_ON(pte && pte_present(*pte) && !pte_huge(*pte));\n\n\treturn pte;\n}\n\npte_t *huge_pte_offset(struct mm_struct *mm,\n\t\t       unsigned long addr, unsigned long sz)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\n\tpgd = pgd_offset(mm, addr);\n\tif (!pgd_present(*pgd))\n\t\treturn NULL;\n\tp4d = p4d_offset(pgd, addr);\n\tif (!p4d_present(*p4d))\n\t\treturn NULL;\n\tpud = pud_offset(p4d, addr);\n\tif (!pud_present(*pud))\n\t\treturn NULL;\n\tif (pud_huge(*pud))\n\t\treturn (pte_t *)pud;\n\tpmd = pmd_offset(pud, addr);\n\treturn (pte_t *) pmd;\n}\n\n#endif /* CONFIG_ARCH_WANT_GENERAL_HUGETLB */\n\n/*\n * These functions are overwritable if your architecture needs its own\n * behavior.\n */\nstruct page * __weak\nfollow_huge_addr(struct mm_struct *mm, unsigned long address,\n\t\t\t      int write)\n{\n\treturn ERR_PTR(-EINVAL);\n}\n\nstruct page * __weak\nfollow_huge_pd(struct vm_area_struct *vma,\n\t       unsigned long address, hugepd_t hpd, int flags, int pdshift)\n{\n\tWARN(1, \"hugepd follow called with no support for hugepage directory format\\n\");\n\treturn NULL;\n}\n\nstruct page * __weak\nfollow_huge_pmd(struct mm_struct *mm, unsigned long address,\n\t\tpmd_t *pmd, int flags)\n{\n\tstruct page *page = NULL;\n\tspinlock_t *ptl;\n\tpte_t pte;\nretry:\n\tptl = pmd_lockptr(mm, pmd);\n\tspin_lock(ptl);\n\t/*\n\t * make sure that the address range covered by this pmd is not\n\t * unmapped from other threads.\n\t */\n\tif (!pmd_huge(*pmd))\n\t\tgoto out;\n\tpte = huge_ptep_get((pte_t *)pmd);\n\tif (pte_present(pte)) {\n\t\tpage = pmd_page(*pmd) + ((address & ~PMD_MASK) >> PAGE_SHIFT);\n\t\tif (flags & FOLL_GET)\n\t\t\tget_page(page);\n\t} else {\n\t\tif (is_hugetlb_entry_migration(pte)) {\n\t\t\tspin_unlock(ptl);\n\t\t\t__migration_entry_wait(mm, (pte_t *)pmd, ptl);\n\t\t\tgoto retry;\n\t\t}\n\t\t/*\n\t\t * hwpoisoned entry is treated as no_page_table in\n\t\t * follow_page_mask().\n\t\t */\n\t}\nout:\n\tspin_unlock(ptl);\n\treturn page;\n}\n\nstruct page * __weak\nfollow_huge_pud(struct mm_struct *mm, unsigned long address,\n\t\tpud_t *pud, int flags)\n{\n\tif (flags & FOLL_GET)\n\t\treturn NULL;\n\n\treturn pte_page(*(pte_t *)pud) + ((address & ~PUD_MASK) >> PAGE_SHIFT);\n}\n\nstruct page * __weak\nfollow_huge_pgd(struct mm_struct *mm, unsigned long address, pgd_t *pgd, int flags)\n{\n\tif (flags & FOLL_GET)\n\t\treturn NULL;\n\n\treturn pte_page(*(pte_t *)pgd) + ((address & ~PGDIR_MASK) >> PAGE_SHIFT);\n}\n\nbool isolate_huge_page(struct page *page, struct list_head *list)\n{\n\tbool ret = true;\n\n\tVM_BUG_ON_PAGE(!PageHead(page), page);\n\tspin_lock(&hugetlb_lock);\n\tif (!page_huge_active(page) || !get_page_unless_zero(page)) {\n\t\tret = false;\n\t\tgoto unlock;\n\t}\n\tclear_page_huge_active(page);\n\tlist_move_tail(&page->lru, list);\nunlock:\n\tspin_unlock(&hugetlb_lock);\n\treturn ret;\n}\n\nvoid putback_active_hugepage(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!PageHead(page), page);\n\tspin_lock(&hugetlb_lock);\n\tset_page_huge_active(page);\n\tlist_move_tail(&page->lru, &(page_hstate(page))->hugepage_activelist);\n\tspin_unlock(&hugetlb_lock);\n\tput_page(page);\n}\n"], "filenames": ["mm/hugetlb.c"], "buggy_code_start_loc": [4065], "buggy_code_end_loc": [4067], "fixing_code_start_loc": [4064], "fixing_code_end_loc": [4068], "type": "CWE-460", "message": "A flaw was found in the hugetlb_mcopy_atomic_pte function in mm/hugetlb.c in the Linux kernel before 4.13. A superfluous implicit page unlock for VM_SHARED hugetlbfs mapping could trigger a local denial of service (BUG).", "other": {"cve": {"id": "CVE-2017-15127", "sourceIdentifier": "secalert@redhat.com", "published": "2018-01-14T06:29:00.263", "lastModified": "2023-02-12T23:28:50.603", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "A flaw was found in the hugetlb_mcopy_atomic_pte function in mm/hugetlb.c in the Linux kernel before 4.13. A superfluous implicit page unlock for VM_SHARED hugetlbfs mapping could trigger a local denial of service (BUG)."}, {"lang": "es", "value": "Se encontr\u00f3 un error en la funci\u00f3n hugetlb_mcopy_atomic_pte en mm/hugetlb.c en el kernel de Linux en versiones anteriores a la 4.13. Un desbloqueo superfluo impl\u00edcito de p\u00e1gina para la representaci\u00f3n hugetlbfs de VM_SHARED podr\u00eda desembocar una denegaci\u00f3n de servicio local (error)."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "secalert@redhat.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-460"}]}, {"source": "nvd@nist.gov", "type": "Secondary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.13", "matchCriteriaId": "D2894F22-5448-4402-AE27-6E43BD08E14E"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "142AD0DD-4CF3-4D74-9442-459CE3347E3A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_mrg:2.0:*:*:*:*:*:*:*", "matchCriteriaId": "C60FA8B1-1802-4522-A088-22171DCF7A93"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=5af10dfd0afc559bb4b0f7e3e8227a1578333995", "source": "secalert@redhat.com", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://www.securityfocus.com/bid/102517", "source": "secalert@redhat.com"}, {"url": "https://access.redhat.com/errata/RHSA-2018:0676", "source": "secalert@redhat.com"}, {"url": "https://access.redhat.com/errata/RHSA-2018:1062", "source": "secalert@redhat.com"}, {"url": "https://access.redhat.com/security/cve/CVE-2017-15127", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1525218", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/5af10dfd0afc559bb4b0f7e3e8227a1578333995", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/5af10dfd0afc559bb4b0f7e3e8227a1578333995"}}