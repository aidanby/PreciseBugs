{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (c) 2003-2006, Cluster File Systems, Inc, info@clusterfs.com\n * Written by Alex Tomas <alex@clusterfs.com>\n *\n * Architecture independence:\n *   Copyright (c) 2005, Bull S.A.\n *   Written by Pierre Peiffer <pierre.peiffer@bull.net>\n */\n\n/*\n * Extents support for EXT4\n *\n * TODO:\n *   - ext4*_error() should be used in some situations\n *   - analyze all BUG()/BUG_ON(), use -EIO where appropriate\n *   - smart tree reduction\n */\n\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/jbd2.h>\n#include <linux/highuid.h>\n#include <linux/pagemap.h>\n#include <linux/quotaops.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#include <linux/fiemap.h>\n#include <linux/backing-dev.h>\n#include \"ext4_jbd2.h\"\n#include \"ext4_extents.h\"\n#include \"xattr.h\"\n\n#include <trace/events/ext4.h>\n\n/*\n * used by extent splitting.\n */\n#define EXT4_EXT_MAY_ZEROOUT\t0x1  /* safe to zeroout if split fails \\\n\t\t\t\t\tdue to ENOSPC */\n#define EXT4_EXT_MARK_UNWRIT1\t0x2  /* mark first half unwritten */\n#define EXT4_EXT_MARK_UNWRIT2\t0x4  /* mark second half unwritten */\n\n#define EXT4_EXT_DATA_VALID1\t0x8  /* first half contains valid data */\n#define EXT4_EXT_DATA_VALID2\t0x10 /* second half contains valid data */\n\nstatic __le32 ext4_extent_block_csum(struct inode *inode,\n\t\t\t\t     struct ext4_extent_header *eh)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t__u32 csum;\n\n\tcsum = ext4_chksum(sbi, ei->i_csum_seed, (__u8 *)eh,\n\t\t\t   EXT4_EXTENT_TAIL_OFFSET(eh));\n\treturn cpu_to_le32(csum);\n}\n\nstatic int ext4_extent_block_csum_verify(struct inode *inode,\n\t\t\t\t\t struct ext4_extent_header *eh)\n{\n\tstruct ext4_extent_tail *et;\n\n\tif (!ext4_has_metadata_csum(inode->i_sb))\n\t\treturn 1;\n\n\tet = find_ext4_extent_tail(eh);\n\tif (et->et_checksum != ext4_extent_block_csum(inode, eh))\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic void ext4_extent_block_csum_set(struct inode *inode,\n\t\t\t\t       struct ext4_extent_header *eh)\n{\n\tstruct ext4_extent_tail *et;\n\n\tif (!ext4_has_metadata_csum(inode->i_sb))\n\t\treturn;\n\n\tet = find_ext4_extent_tail(eh);\n\tet->et_checksum = ext4_extent_block_csum(inode, eh);\n}\n\nstatic int ext4_split_extent(handle_t *handle,\n\t\t\t\tstruct inode *inode,\n\t\t\t\tstruct ext4_ext_path **ppath,\n\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\tint split_flag,\n\t\t\t\tint flags);\n\nstatic int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path **ppath,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags);\n\nstatic int ext4_find_delayed_extent(struct inode *inode,\n\t\t\t\t    struct extent_status *newes);\n\nstatic int ext4_ext_truncate_extend_restart(handle_t *handle,\n\t\t\t\t\t    struct inode *inode,\n\t\t\t\t\t    int needed)\n{\n\tint err;\n\n\tif (!ext4_handle_valid(handle))\n\t\treturn 0;\n\tif (handle->h_buffer_credits >= needed)\n\t\treturn 0;\n\t/*\n\t * If we need to extend the journal get a few extra blocks\n\t * while we're at it for efficiency's sake.\n\t */\n\tneeded += 3;\n\terr = ext4_journal_extend(handle, needed - handle->h_buffer_credits);\n\tif (err <= 0)\n\t\treturn err;\n\terr = ext4_truncate_restart_trans(handle, inode, needed);\n\tif (err == 0)\n\t\terr = -EAGAIN;\n\n\treturn err;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n */\nstatic int ext4_ext_get_access(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path->p_bh) {\n\t\t/* path points to block */\n\t\tBUFFER_TRACE(path->p_bh, \"get_write_access\");\n\t\treturn ext4_journal_get_write_access(handle, path->p_bh);\n\t}\n\t/* path points to leaf/index in inode body */\n\t/* we use in-core data, no need to protect them */\n\treturn 0;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n *  - EIO\n */\nint __ext4_ext_dirty(const char *where, unsigned int line, handle_t *handle,\n\t\t     struct inode *inode, struct ext4_ext_path *path)\n{\n\tint err;\n\n\tWARN_ON(!rwsem_is_locked(&EXT4_I(inode)->i_data_sem));\n\tif (path->p_bh) {\n\t\text4_extent_block_csum_set(inode, ext_block_hdr(path->p_bh));\n\t\t/* path points to block */\n\t\terr = __ext4_handle_dirty_metadata(where, line, handle,\n\t\t\t\t\t\t   inode, path->p_bh);\n\t} else {\n\t\t/* path points to leaf/index in inode body */\n\t\terr = ext4_mark_inode_dirty(handle, inode);\n\t}\n\treturn err;\n}\n\nstatic ext4_fsblk_t ext4_ext_find_goal(struct inode *inode,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      ext4_lblk_t block)\n{\n\tif (path) {\n\t\tint depth = path->p_depth;\n\t\tstruct ext4_extent *ex;\n\n\t\t/*\n\t\t * Try to predict block placement assuming that we are\n\t\t * filling in a file which will eventually be\n\t\t * non-sparse --- i.e., in the case of libbfd writing\n\t\t * an ELF object sections out-of-order but in a way\n\t\t * the eventually results in a contiguous object or\n\t\t * executable file, or some database extending a table\n\t\t * space file.  However, this is actually somewhat\n\t\t * non-ideal if we are writing a sparse file such as\n\t\t * qemu or KVM writing a raw image file that is going\n\t\t * to stay fairly sparse, since it will end up\n\t\t * fragmenting the file system's free space.  Maybe we\n\t\t * should have some hueristics or some way to allow\n\t\t * userspace to pass a hint to file system,\n\t\t * especially if the latter case turns out to be\n\t\t * common.\n\t\t */\n\t\tex = path[depth].p_ext;\n\t\tif (ex) {\n\t\t\text4_fsblk_t ext_pblk = ext4_ext_pblock(ex);\n\t\t\text4_lblk_t ext_block = le32_to_cpu(ex->ee_block);\n\n\t\t\tif (block > ext_block)\n\t\t\t\treturn ext_pblk + (block - ext_block);\n\t\t\telse\n\t\t\t\treturn ext_pblk - (ext_block - block);\n\t\t}\n\n\t\t/* it looks like index is empty;\n\t\t * try to find starting block from index itself */\n\t\tif (path[depth].p_bh)\n\t\t\treturn path[depth].p_bh->b_blocknr;\n\t}\n\n\t/* OK. use inode's group */\n\treturn ext4_inode_to_goal_block(inode);\n}\n\n/*\n * Allocation for a meta data block\n */\nstatic ext4_fsblk_t\next4_ext_new_meta_block(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path,\n\t\t\tstruct ext4_extent *ex, int *err, unsigned int flags)\n{\n\text4_fsblk_t goal, newblock;\n\n\tgoal = ext4_ext_find_goal(inode, path, le32_to_cpu(ex->ee_block));\n\tnewblock = ext4_new_meta_blocks(handle, inode, goal, flags,\n\t\t\t\t\tNULL, err);\n\treturn newblock;\n}\n\nstatic inline int ext4_ext_space_block(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 6)\n\t\tsize = 6;\n#endif\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_block_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent_idx);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 5)\n\t\tsize = 5;\n#endif\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 3)\n\t\tsize = 3;\n#endif\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent_idx);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 4)\n\t\tsize = 4;\n#endif\n\treturn size;\n}\n\nstatic inline int\next4_force_split_extent_at(handle_t *handle, struct inode *inode,\n\t\t\t   struct ext4_ext_path **ppath, ext4_lblk_t lblk,\n\t\t\t   int nofail)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tint unwritten = ext4_ext_is_unwritten(path[path->p_depth].p_ext);\n\n\treturn ext4_split_extent_at(handle, inode, ppath, lblk, unwritten ?\n\t\t\tEXT4_EXT_MARK_UNWRIT1|EXT4_EXT_MARK_UNWRIT2 : 0,\n\t\t\tEXT4_EX_NOCACHE | EXT4_GET_BLOCKS_PRE_IO |\n\t\t\t(nofail ? EXT4_GET_BLOCKS_METADATA_NOFAIL:0));\n}\n\n/*\n * Calculate the number of metadata blocks needed\n * to allocate @blocks\n * Worse case is one block per extent\n */\nint ext4_ext_calc_metadata_amount(struct inode *inode, ext4_lblk_t lblock)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint idxs;\n\n\tidxs = ((inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t/ sizeof(struct ext4_extent_idx));\n\n\t/*\n\t * If the new delayed allocation block is contiguous with the\n\t * previous da block, it can share index blocks with the\n\t * previous block, so we only need to allocate a new index\n\t * block every idxs leaf blocks.  At ldxs**2 blocks, we need\n\t * an additional index block, and at ldxs**3 blocks, yet\n\t * another index blocks.\n\t */\n\tif (ei->i_da_metadata_calc_len &&\n\t    ei->i_da_metadata_calc_last_lblock+1 == lblock) {\n\t\tint num = 0;\n\n\t\tif ((ei->i_da_metadata_calc_len % idxs) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs)) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs*idxs)) == 0) {\n\t\t\tnum++;\n\t\t\tei->i_da_metadata_calc_len = 0;\n\t\t} else\n\t\t\tei->i_da_metadata_calc_len++;\n\t\tei->i_da_metadata_calc_last_lblock++;\n\t\treturn num;\n\t}\n\n\t/*\n\t * In the worst case we need a new set of index blocks at\n\t * every level of the inode's extent tree.\n\t */\n\tei->i_da_metadata_calc_len = 1;\n\tei->i_da_metadata_calc_last_lblock = lblock;\n\treturn ext_depth(inode) + 1;\n}\n\nstatic int\next4_ext_max_entries(struct inode *inode, int depth)\n{\n\tint max;\n\n\tif (depth == ext_depth(inode)) {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_root(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_root_idx(inode, 1);\n\t} else {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_block(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_block_idx(inode, 1);\n\t}\n\n\treturn max;\n}\n\nstatic int ext4_valid_extent(struct inode *inode, struct ext4_extent *ext)\n{\n\text4_fsblk_t block = ext4_ext_pblock(ext);\n\tint len = ext4_ext_get_actual_len(ext);\n\text4_lblk_t lblock = le32_to_cpu(ext->ee_block);\n\n\t/*\n\t * We allow neither:\n\t *  - zero length\n\t *  - overflow/wrap-around\n\t */\n\tif (lblock + len <= lblock)\n\t\treturn 0;\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, len);\n}\n\nstatic int ext4_valid_extent_idx(struct inode *inode,\n\t\t\t\tstruct ext4_extent_idx *ext_idx)\n{\n\text4_fsblk_t block = ext4_idx_pblock(ext_idx);\n\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, 1);\n}\n\nstatic int ext4_valid_extent_entries(struct inode *inode,\n\t\t\t\tstruct ext4_extent_header *eh,\n\t\t\t\tint depth)\n{\n\tunsigned short entries;\n\tif (eh->eh_entries == 0)\n\t\treturn 1;\n\n\tentries = le16_to_cpu(eh->eh_entries);\n\n\tif (depth == 0) {\n\t\t/* leaf entries */\n\t\tstruct ext4_extent *ext = EXT_FIRST_EXTENT(eh);\n\t\tstruct ext4_super_block *es = EXT4_SB(inode->i_sb)->s_es;\n\t\text4_fsblk_t pblock = 0;\n\t\text4_lblk_t lblock = 0;\n\t\text4_lblk_t prev = 0;\n\t\tint len = 0;\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent(inode, ext))\n\t\t\t\treturn 0;\n\n\t\t\t/* Check for overlapping extents */\n\t\t\tlblock = le32_to_cpu(ext->ee_block);\n\t\t\tlen = ext4_ext_get_actual_len(ext);\n\t\t\tif ((lblock <= prev) && prev) {\n\t\t\t\tpblock = ext4_ext_pblock(ext);\n\t\t\t\tes->s_last_error_block = cpu_to_le64(pblock);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\text++;\n\t\t\tentries--;\n\t\t\tprev = lblock + len - 1;\n\t\t}\n\t} else {\n\t\tstruct ext4_extent_idx *ext_idx = EXT_FIRST_INDEX(eh);\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent_idx(inode, ext_idx))\n\t\t\t\treturn 0;\n\t\t\text_idx++;\n\t\t\tentries--;\n\t\t}\n\t}\n\treturn 1;\n}\n\nstatic int __ext4_ext_check(const char *function, unsigned int line,\n\t\t\t    struct inode *inode, struct ext4_extent_header *eh,\n\t\t\t    int depth, ext4_fsblk_t pblk)\n{\n\tconst char *error_msg;\n\tint max = 0, err = -EFSCORRUPTED;\n\n\tif (unlikely(eh->eh_magic != EXT4_EXT_MAGIC)) {\n\t\terror_msg = \"invalid magic\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_depth) != depth)) {\n\t\terror_msg = \"unexpected eh_depth\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(eh->eh_max == 0)) {\n\t\terror_msg = \"invalid eh_max\";\n\t\tgoto corrupted;\n\t}\n\tmax = ext4_ext_max_entries(inode, depth);\n\tif (unlikely(le16_to_cpu(eh->eh_max) > max)) {\n\t\terror_msg = \"too large eh_max\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_entries) > le16_to_cpu(eh->eh_max))) {\n\t\terror_msg = \"invalid eh_entries\";\n\t\tgoto corrupted;\n\t}\n\tif (!ext4_valid_extent_entries(inode, eh, depth)) {\n\t\terror_msg = \"invalid extent entries\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(depth > 32)) {\n\t\terror_msg = \"too large eh_depth\";\n\t\tgoto corrupted;\n\t}\n\t/* Verify checksum on non-root extent tree nodes */\n\tif (ext_depth(inode) != depth &&\n\t    !ext4_extent_block_csum_verify(inode, eh)) {\n\t\terror_msg = \"extent tree corrupted\";\n\t\terr = -EFSBADCRC;\n\t\tgoto corrupted;\n\t}\n\treturn 0;\n\ncorrupted:\n\text4_error_inode(inode, function, line, 0,\n\t\t\t \"pblk %llu bad header/extent: %s - magic %x, \"\n\t\t\t \"entries %u, max %u(%u), depth %u(%u)\",\n\t\t\t (unsigned long long) pblk, error_msg,\n\t\t\t le16_to_cpu(eh->eh_magic),\n\t\t\t le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max),\n\t\t\t max, le16_to_cpu(eh->eh_depth), depth);\n\treturn err;\n}\n\n#define ext4_ext_check(inode, eh, depth, pblk)\t\t\t\\\n\t__ext4_ext_check(__func__, __LINE__, (inode), (eh), (depth), (pblk))\n\nint ext4_ext_check_inode(struct inode *inode)\n{\n\treturn ext4_ext_check(inode, ext_inode_hdr(inode), ext_depth(inode), 0);\n}\n\nstatic struct buffer_head *\n__read_extent_tree_block(const char *function, unsigned int line,\n\t\t\t struct inode *inode, ext4_fsblk_t pblk, int depth,\n\t\t\t int flags)\n{\n\tstruct buffer_head\t\t*bh;\n\tint\t\t\t\terr;\n\n\tbh = sb_getblk_gfp(inode->i_sb, pblk, __GFP_MOVABLE | GFP_NOFS);\n\tif (unlikely(!bh))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (!bh_uptodate_or_lock(bh)) {\n\t\ttrace_ext4_ext_load_extent(inode, pblk, _RET_IP_);\n\t\terr = bh_submit_read(bh);\n\t\tif (err < 0)\n\t\t\tgoto errout;\n\t}\n\tif (buffer_verified(bh) && !(flags & EXT4_EX_FORCE_CACHE))\n\t\treturn bh;\n\terr = __ext4_ext_check(function, line, inode,\n\t\t\t       ext_block_hdr(bh), depth, pblk);\n\tif (err)\n\t\tgoto errout;\n\tset_buffer_verified(bh);\n\t/*\n\t * If this is a leaf block, cache all of its entries\n\t */\n\tif (!(flags & EXT4_EX_NOCACHE) && depth == 0) {\n\t\tstruct ext4_extent_header *eh = ext_block_hdr(bh);\n\t\tstruct ext4_extent *ex = EXT_FIRST_EXTENT(eh);\n\t\text4_lblk_t prev = 0;\n\t\tint i;\n\n\t\tfor (i = le16_to_cpu(eh->eh_entries); i > 0; i--, ex++) {\n\t\t\tunsigned int status = EXTENT_STATUS_WRITTEN;\n\t\t\text4_lblk_t lblk = le32_to_cpu(ex->ee_block);\n\t\t\tint len = ext4_ext_get_actual_len(ex);\n\n\t\t\tif (prev && (prev != lblk))\n\t\t\t\text4_es_cache_extent(inode, prev,\n\t\t\t\t\t\t     lblk - prev, ~0,\n\t\t\t\t\t\t     EXTENT_STATUS_HOLE);\n\n\t\t\tif (ext4_ext_is_unwritten(ex))\n\t\t\t\tstatus = EXTENT_STATUS_UNWRITTEN;\n\t\t\text4_es_cache_extent(inode, lblk, len,\n\t\t\t\t\t     ext4_ext_pblock(ex), status);\n\t\t\tprev = lblk + len;\n\t\t}\n\t}\n\treturn bh;\nerrout:\n\tput_bh(bh);\n\treturn ERR_PTR(err);\n\n}\n\n#define read_extent_tree_block(inode, pblk, depth, flags)\t\t\\\n\t__read_extent_tree_block(__func__, __LINE__, (inode), (pblk),   \\\n\t\t\t\t (depth), (flags))\n\n/*\n * This function is called to cache a file's extent information in the\n * extent status tree\n */\nint ext4_ext_precache(struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct ext4_ext_path *path = NULL;\n\tstruct buffer_head *bh;\n\tint i = 0, depth, ret = 0;\n\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\treturn 0;\t/* not an extent-mapped inode */\n\n\tdown_read(&ei->i_data_sem);\n\tdepth = ext_depth(inode);\n\n\tpath = kcalloc(depth + 1, sizeof(struct ext4_ext_path),\n\t\t       GFP_NOFS);\n\tif (path == NULL) {\n\t\tup_read(&ei->i_data_sem);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* Don't cache anything if there are no external extent blocks */\n\tif (depth == 0)\n\t\tgoto out;\n\tpath[0].p_hdr = ext_inode_hdr(inode);\n\tret = ext4_ext_check(inode, path[0].p_hdr, depth, 0);\n\tif (ret)\n\t\tgoto out;\n\tpath[0].p_idx = EXT_FIRST_INDEX(path[0].p_hdr);\n\twhile (i >= 0) {\n\t\t/*\n\t\t * If this is a leaf block or we've reached the end of\n\t\t * the index block, go up\n\t\t */\n\t\tif ((i == depth) ||\n\t\t    path[i].p_idx > EXT_LAST_INDEX(path[i].p_hdr)) {\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\t\tbh = read_extent_tree_block(inode,\n\t\t\t\t\t    ext4_idx_pblock(path[i].p_idx++),\n\t\t\t\t\t    depth - i - 1,\n\t\t\t\t\t    EXT4_EX_FORCE_CACHE);\n\t\tif (IS_ERR(bh)) {\n\t\t\tret = PTR_ERR(bh);\n\t\t\tbreak;\n\t\t}\n\t\ti++;\n\t\tpath[i].p_bh = bh;\n\t\tpath[i].p_hdr = ext_block_hdr(bh);\n\t\tpath[i].p_idx = EXT_FIRST_INDEX(path[i].p_hdr);\n\t}\n\text4_set_inode_state(inode, EXT4_STATE_EXT_PRECACHED);\nout:\n\tup_read(&ei->i_data_sem);\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\treturn ret;\n}\n\n#ifdef EXT_DEBUG\nstatic void ext4_ext_show_path(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint k, l = path->p_depth;\n\n\text_debug(\"path:\");\n\tfor (k = 0; k <= l; k++, path++) {\n\t\tif (path->p_idx) {\n\t\t  ext_debug(\"  %d->%llu\", le32_to_cpu(path->p_idx->ei_block),\n\t\t\t    ext4_idx_pblock(path->p_idx));\n\t\t} else if (path->p_ext) {\n\t\t\text_debug(\"  %d:[%d]%d:%llu \",\n\t\t\t\t  le32_to_cpu(path->p_ext->ee_block),\n\t\t\t\t  ext4_ext_is_unwritten(path->p_ext),\n\t\t\t\t  ext4_ext_get_actual_len(path->p_ext),\n\t\t\t\t  ext4_ext_pblock(path->p_ext));\n\t\t} else\n\t\t\text_debug(\"  []\");\n\t}\n\text_debug(\"\\n\");\n}\n\nstatic void ext4_ext_show_leaf(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex;\n\tint i;\n\n\tif (!path)\n\t\treturn;\n\n\teh = path[depth].p_hdr;\n\tex = EXT_FIRST_EXTENT(eh);\n\n\text_debug(\"Displaying leaf extents for inode %lu\\n\", inode->i_ino);\n\n\tfor (i = 0; i < le16_to_cpu(eh->eh_entries); i++, ex++) {\n\t\text_debug(\"%d:[%d]%d:%llu \", le32_to_cpu(ex->ee_block),\n\t\t\t  ext4_ext_is_unwritten(ex),\n\t\t\t  ext4_ext_get_actual_len(ex), ext4_ext_pblock(ex));\n\t}\n\text_debug(\"\\n\");\n}\n\nstatic void ext4_ext_show_move(struct inode *inode, struct ext4_ext_path *path,\n\t\t\text4_fsblk_t newblock, int level)\n{\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent *ex;\n\n\tif (depth != level) {\n\t\tstruct ext4_extent_idx *idx;\n\t\tidx = path[level].p_idx;\n\t\twhile (idx <= EXT_MAX_INDEX(path[level].p_hdr)) {\n\t\t\text_debug(\"%d: move %d:%llu in new index %llu\\n\", level,\n\t\t\t\t\tle32_to_cpu(idx->ei_block),\n\t\t\t\t\text4_idx_pblock(idx),\n\t\t\t\t\tnewblock);\n\t\t\tidx++;\n\t\t}\n\n\t\treturn;\n\t}\n\n\tex = path[depth].p_ext;\n\twhile (ex <= EXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\text_debug(\"move %d:%llu:[%d]%d in new leaf %llu\\n\",\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\text4_ext_pblock(ex),\n\t\t\t\text4_ext_is_unwritten(ex),\n\t\t\t\text4_ext_get_actual_len(ex),\n\t\t\t\tnewblock);\n\t\tex++;\n\t}\n}\n\n#else\n#define ext4_ext_show_path(inode, path)\n#define ext4_ext_show_leaf(inode, path)\n#define ext4_ext_show_move(inode, path, newblock, level)\n#endif\n\nvoid ext4_ext_drop_refs(struct ext4_ext_path *path)\n{\n\tint depth, i;\n\n\tif (!path)\n\t\treturn;\n\tdepth = path->p_depth;\n\tfor (i = 0; i <= depth; i++, path++)\n\t\tif (path->p_bh) {\n\t\t\tbrelse(path->p_bh);\n\t\t\tpath->p_bh = NULL;\n\t\t}\n}\n\n/*\n * ext4_ext_binsearch_idx:\n * binary search for the closest index of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch_idx(struct inode *inode,\n\t\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent_idx *r, *l, *m;\n\n\n\text_debug(\"binsearch for %u(idx):  \", block);\n\n\tl = EXT_FIRST_INDEX(eh) + 1;\n\tr = EXT_LAST_INDEX(eh);\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ei_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ei_block),\n\t\t\t\tm, le32_to_cpu(m->ei_block),\n\t\t\t\tr, le32_to_cpu(r->ei_block));\n\t}\n\n\tpath->p_idx = l - 1;\n\text_debug(\"  -> %u->%lld \", le32_to_cpu(path->p_idx->ei_block),\n\t\t  ext4_idx_pblock(path->p_idx));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent_idx *chix, *ix;\n\t\tint k;\n\n\t\tchix = ix = EXT_FIRST_INDEX(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ix++) {\n\t\t  if (k != 0 &&\n\t\t      le32_to_cpu(ix->ei_block) <= le32_to_cpu(ix[-1].ei_block)) {\n\t\t\t\tprintk(KERN_DEBUG \"k=%d, ix=0x%p, \"\n\t\t\t\t       \"first=0x%p\\n\", k,\n\t\t\t\t       ix, EXT_FIRST_INDEX(eh));\n\t\t\t\tprintk(KERN_DEBUG \"%u <= %u\\n\",\n\t\t\t\t       le32_to_cpu(ix->ei_block),\n\t\t\t\t       le32_to_cpu(ix[-1].ei_block));\n\t\t\t}\n\t\t\tBUG_ON(k && le32_to_cpu(ix->ei_block)\n\t\t\t\t\t   <= le32_to_cpu(ix[-1].ei_block));\n\t\t\tif (block < le32_to_cpu(ix->ei_block))\n\t\t\t\tbreak;\n\t\t\tchix = ix;\n\t\t}\n\t\tBUG_ON(chix != path->p_idx);\n\t}\n#endif\n\n}\n\n/*\n * ext4_ext_binsearch:\n * binary search for closest extent of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch(struct inode *inode,\n\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent *r, *l, *m;\n\n\tif (eh->eh_entries == 0) {\n\t\t/*\n\t\t * this leaf is empty:\n\t\t * we get such a leaf in split/add case\n\t\t */\n\t\treturn;\n\t}\n\n\text_debug(\"binsearch for %u:  \", block);\n\n\tl = EXT_FIRST_EXTENT(eh) + 1;\n\tr = EXT_LAST_EXTENT(eh);\n\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ee_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ee_block),\n\t\t\t\tm, le32_to_cpu(m->ee_block),\n\t\t\t\tr, le32_to_cpu(r->ee_block));\n\t}\n\n\tpath->p_ext = l - 1;\n\text_debug(\"  -> %d:%llu:[%d]%d \",\n\t\t\tle32_to_cpu(path->p_ext->ee_block),\n\t\t\text4_ext_pblock(path->p_ext),\n\t\t\text4_ext_is_unwritten(path->p_ext),\n\t\t\text4_ext_get_actual_len(path->p_ext));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent *chex, *ex;\n\t\tint k;\n\n\t\tchex = ex = EXT_FIRST_EXTENT(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ex++) {\n\t\t\tBUG_ON(k && le32_to_cpu(ex->ee_block)\n\t\t\t\t\t  <= le32_to_cpu(ex[-1].ee_block));\n\t\t\tif (block < le32_to_cpu(ex->ee_block))\n\t\t\t\tbreak;\n\t\t\tchex = ex;\n\t\t}\n\t\tBUG_ON(chex != path->p_ext);\n\t}\n#endif\n\n}\n\nint ext4_ext_tree_init(handle_t *handle, struct inode *inode)\n{\n\tstruct ext4_extent_header *eh;\n\n\teh = ext_inode_hdr(inode);\n\teh->eh_depth = 0;\n\teh->eh_entries = 0;\n\teh->eh_magic = EXT4_EXT_MAGIC;\n\teh->eh_max = cpu_to_le16(ext4_ext_space_root(inode, 0));\n\text4_mark_inode_dirty(handle, inode);\n\treturn 0;\n}\n\nstruct ext4_ext_path *\next4_find_extent(struct inode *inode, ext4_lblk_t block,\n\t\t struct ext4_ext_path **orig_path, int flags)\n{\n\tstruct ext4_extent_header *eh;\n\tstruct buffer_head *bh;\n\tstruct ext4_ext_path *path = orig_path ? *orig_path : NULL;\n\tshort int depth, i, ppos = 0;\n\tint ret;\n\n\teh = ext_inode_hdr(inode);\n\tdepth = ext_depth(inode);\n\tif (depth < 0 || depth > EXT4_MAX_EXTENT_DEPTH) {\n\t\tEXT4_ERROR_INODE(inode, \"inode has invalid extent depth: %d\",\n\t\t\t\t depth);\n\t\tret = -EFSCORRUPTED;\n\t\tgoto err;\n\t}\n\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tif (depth > path[0].p_maxdepth) {\n\t\t\tkfree(path);\n\t\t\t*orig_path = path = NULL;\n\t\t}\n\t}\n\tif (!path) {\n\t\t/* account possible depth increase */\n\t\tpath = kcalloc(depth + 2, sizeof(struct ext4_ext_path),\n\t\t\t\tGFP_NOFS);\n\t\tif (unlikely(!path))\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\tpath[0].p_maxdepth = depth + 1;\n\t}\n\tpath[0].p_hdr = eh;\n\tpath[0].p_bh = NULL;\n\n\ti = depth;\n\t/* walk through the tree */\n\twhile (i) {\n\t\text_debug(\"depth %d: num %d, max %d\\n\",\n\t\t\t  ppos, le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\n\t\text4_ext_binsearch_idx(inode, path + ppos, block);\n\t\tpath[ppos].p_block = ext4_idx_pblock(path[ppos].p_idx);\n\t\tpath[ppos].p_depth = i;\n\t\tpath[ppos].p_ext = NULL;\n\n\t\tbh = read_extent_tree_block(inode, path[ppos].p_block, --i,\n\t\t\t\t\t    flags);\n\t\tif (IS_ERR(bh)) {\n\t\t\tret = PTR_ERR(bh);\n\t\t\tgoto err;\n\t\t}\n\n\t\teh = ext_block_hdr(bh);\n\t\tppos++;\n\t\tpath[ppos].p_bh = bh;\n\t\tpath[ppos].p_hdr = eh;\n\t}\n\n\tpath[ppos].p_depth = i;\n\tpath[ppos].p_ext = NULL;\n\tpath[ppos].p_idx = NULL;\n\n\t/* find extent */\n\text4_ext_binsearch(inode, path + ppos, block);\n\t/* if not an empty leaf */\n\tif (path[ppos].p_ext)\n\t\tpath[ppos].p_block = ext4_ext_pblock(path[ppos].p_ext);\n\n\text4_ext_show_path(inode, path);\n\n\treturn path;\n\nerr:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\tif (orig_path)\n\t\t*orig_path = NULL;\n\treturn ERR_PTR(ret);\n}\n\n/*\n * ext4_ext_insert_index:\n * insert new index [@logical;@ptr] into the block at @curp;\n * check where to insert: before @curp or after @curp\n */\nstatic int ext4_ext_insert_index(handle_t *handle, struct inode *inode,\n\t\t\t\t struct ext4_ext_path *curp,\n\t\t\t\t int logical, ext4_fsblk_t ptr)\n{\n\tstruct ext4_extent_idx *ix;\n\tint len, err;\n\n\terr = ext4_ext_get_access(handle, inode, curp);\n\tif (err)\n\t\treturn err;\n\n\tif (unlikely(logical == le32_to_cpu(curp->p_idx->ei_block))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d == ei_block %d!\",\n\t\t\t\t logical, le32_to_cpu(curp->p_idx->ei_block));\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\tif (unlikely(le16_to_cpu(curp->p_hdr->eh_entries)\n\t\t\t     >= le16_to_cpu(curp->p_hdr->eh_max))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"eh_entries %d >= eh_max %d!\",\n\t\t\t\t le16_to_cpu(curp->p_hdr->eh_entries),\n\t\t\t\t le16_to_cpu(curp->p_hdr->eh_max));\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\tif (logical > le32_to_cpu(curp->p_idx->ei_block)) {\n\t\t/* insert after */\n\t\text_debug(\"insert new index %d after: %llu\\n\", logical, ptr);\n\t\tix = curp->p_idx + 1;\n\t} else {\n\t\t/* insert before */\n\t\text_debug(\"insert new index %d before: %llu\\n\", logical, ptr);\n\t\tix = curp->p_idx;\n\t}\n\n\tlen = EXT_LAST_INDEX(curp->p_hdr) - ix + 1;\n\tBUG_ON(len < 0);\n\tif (len > 0) {\n\t\text_debug(\"insert new index %d: \"\n\t\t\t\t\"move %d indices from 0x%p to 0x%p\\n\",\n\t\t\t\tlogical, len, ix, ix + 1);\n\t\tmemmove(ix + 1, ix, len * sizeof(struct ext4_extent_idx));\n\t}\n\n\tif (unlikely(ix > EXT_MAX_INDEX(curp->p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"ix > EXT_MAX_INDEX!\");\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\tix->ei_block = cpu_to_le32(logical);\n\text4_idx_store_pblock(ix, ptr);\n\tle16_add_cpu(&curp->p_hdr->eh_entries, 1);\n\n\tif (unlikely(ix > EXT_LAST_INDEX(curp->p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"ix > EXT_LAST_INDEX!\");\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\terr = ext4_ext_dirty(handle, inode, curp);\n\text4_std_error(inode->i_sb, err);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_split:\n * inserts new subtree into the path, using free index entry\n * at depth @at:\n * - allocates all needed blocks (new leaf and all intermediate index blocks)\n * - makes decision where to split\n * - moves remaining extents and index entries (right to the split point)\n *   into the newly allocated blocks\n * - initializes subtree\n */\nstatic int ext4_ext_split(handle_t *handle, struct inode *inode,\n\t\t\t  unsigned int flags,\n\t\t\t  struct ext4_ext_path *path,\n\t\t\t  struct ext4_extent *newext, int at)\n{\n\tstruct buffer_head *bh = NULL;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *neh;\n\tstruct ext4_extent_idx *fidx;\n\tint i = at, k, m, a;\n\text4_fsblk_t newblock, oldblock;\n\t__le32 border;\n\text4_fsblk_t *ablocks = NULL; /* array of allocated blocks */\n\tint err = 0;\n\n\t/* make decision: where to split? */\n\t/* FIXME: now decision is simplest: at current extent */\n\n\t/* if current leaf will be split, then we should use\n\t * border from split point */\n\tif (unlikely(path[depth].p_ext > EXT_MAX_EXTENT(path[depth].p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"p_ext > EXT_MAX_EXTENT!\");\n\t\treturn -EFSCORRUPTED;\n\t}\n\tif (path[depth].p_ext != EXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\tborder = path[depth].p_ext[1].ee_block;\n\t\text_debug(\"leaf will be split.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\t  le32_to_cpu(border));\n\t} else {\n\t\tborder = newext->ee_block;\n\t\text_debug(\"leaf will be added.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\tle32_to_cpu(border));\n\t}\n\n\t/*\n\t * If error occurs, then we break processing\n\t * and mark filesystem read-only. index won't\n\t * be inserted and tree will be in consistent\n\t * state. Next mount will repair buffers too.\n\t */\n\n\t/*\n\t * Get array to track all allocated blocks.\n\t * We need this to handle errors and free blocks\n\t * upon them.\n\t */\n\tablocks = kcalloc(depth, sizeof(ext4_fsblk_t), GFP_NOFS);\n\tif (!ablocks)\n\t\treturn -ENOMEM;\n\n\t/* allocate all needed blocks */\n\text_debug(\"allocate %d blocks for indexes/leaf\\n\", depth - at);\n\tfor (a = 0; a < depth - at; a++) {\n\t\tnewblock = ext4_ext_new_meta_block(handle, inode, path,\n\t\t\t\t\t\t   newext, &err, flags);\n\t\tif (newblock == 0)\n\t\t\tgoto cleanup;\n\t\tablocks[a] = newblock;\n\t}\n\n\t/* initialize new leaf */\n\tnewblock = ablocks[--a];\n\tif (unlikely(newblock == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"newblock == 0!\");\n\t\terr = -EFSCORRUPTED;\n\t\tgoto cleanup;\n\t}\n\tbh = sb_getblk_gfp(inode->i_sb, newblock, __GFP_MOVABLE | GFP_NOFS);\n\tif (unlikely(!bh)) {\n\t\terr = -ENOMEM;\n\t\tgoto cleanup;\n\t}\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err)\n\t\tgoto cleanup;\n\n\tneh = ext_block_hdr(bh);\n\tneh->eh_entries = 0;\n\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\tneh->eh_depth = 0;\n\n\t/* move remainder of path[depth] to the new leaf */\n\tif (unlikely(path[depth].p_hdr->eh_entries !=\n\t\t     path[depth].p_hdr->eh_max)) {\n\t\tEXT4_ERROR_INODE(inode, \"eh_entries %d != eh_max %d!\",\n\t\t\t\t path[depth].p_hdr->eh_entries,\n\t\t\t\t path[depth].p_hdr->eh_max);\n\t\terr = -EFSCORRUPTED;\n\t\tgoto cleanup;\n\t}\n\t/* start copy from next extent */\n\tm = EXT_MAX_EXTENT(path[depth].p_hdr) - path[depth].p_ext++;\n\text4_ext_show_move(inode, path, newblock, depth);\n\tif (m) {\n\t\tstruct ext4_extent *ex;\n\t\tex = EXT_FIRST_EXTENT(neh);\n\t\tmemmove(ex, path[depth].p_ext, sizeof(struct ext4_extent) * m);\n\t\tle16_add_cpu(&neh->eh_entries, m);\n\t}\n\n\text4_extent_block_csum_set(inode, neh);\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto cleanup;\n\tbrelse(bh);\n\tbh = NULL;\n\n\t/* correct old leaf */\n\tif (m) {\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tle16_add_cpu(&path[depth].p_hdr->eh_entries, -m);\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t}\n\n\t/* create intermediate indexes */\n\tk = depth - at - 1;\n\tif (unlikely(k < 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"k %d < 0!\", k);\n\t\terr = -EFSCORRUPTED;\n\t\tgoto cleanup;\n\t}\n\tif (k)\n\t\text_debug(\"create %d intermediate indices\\n\", k);\n\t/* insert new index into current index block */\n\t/* current depth stored in i var */\n\ti = depth - 1;\n\twhile (k--) {\n\t\toldblock = newblock;\n\t\tnewblock = ablocks[--a];\n\t\tbh = sb_getblk(inode->i_sb, newblock);\n\t\tif (unlikely(!bh)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tlock_buffer(bh);\n\n\t\terr = ext4_journal_get_create_access(handle, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t\tneh = ext_block_hdr(bh);\n\t\tneh->eh_entries = cpu_to_le16(1);\n\t\tneh->eh_magic = EXT4_EXT_MAGIC;\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\t\tneh->eh_depth = cpu_to_le16(depth - i);\n\t\tfidx = EXT_FIRST_INDEX(neh);\n\t\tfidx->ei_block = border;\n\t\text4_idx_store_pblock(fidx, oldblock);\n\n\t\text_debug(\"int.index at %d (block %llu): %u -> %llu\\n\",\n\t\t\t\ti, newblock, le32_to_cpu(border), oldblock);\n\n\t\t/* move remainder of path[i] to the new index block */\n\t\tif (unlikely(EXT_MAX_INDEX(path[i].p_hdr) !=\n\t\t\t\t\tEXT_LAST_INDEX(path[i].p_hdr))) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"EXT_MAX_INDEX != EXT_LAST_INDEX ee_block %d!\",\n\t\t\t\t\t le32_to_cpu(path[i].p_ext->ee_block));\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tgoto cleanup;\n\t\t}\n\t\t/* start copy indexes */\n\t\tm = EXT_MAX_INDEX(path[i].p_hdr) - path[i].p_idx++;\n\t\text_debug(\"cur 0x%p, last 0x%p\\n\", path[i].p_idx,\n\t\t\t\tEXT_MAX_INDEX(path[i].p_hdr));\n\t\text4_ext_show_move(inode, path, newblock, i);\n\t\tif (m) {\n\t\t\tmemmove(++fidx, path[i].p_idx,\n\t\t\t\tsizeof(struct ext4_extent_idx) * m);\n\t\t\tle16_add_cpu(&neh->eh_entries, m);\n\t\t}\n\t\text4_extent_block_csum_set(inode, neh);\n\t\tset_buffer_uptodate(bh);\n\t\tunlock_buffer(bh);\n\n\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tbrelse(bh);\n\t\tbh = NULL;\n\n\t\t/* correct old index */\n\t\tif (m) {\n\t\t\terr = ext4_ext_get_access(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t\tle16_add_cpu(&path[i].p_hdr->eh_entries, -m);\n\t\t\terr = ext4_ext_dirty(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t}\n\n\t\ti--;\n\t}\n\n\t/* insert new index */\n\terr = ext4_ext_insert_index(handle, inode, path + at,\n\t\t\t\t    le32_to_cpu(border), newblock);\n\ncleanup:\n\tif (bh) {\n\t\tif (buffer_locked(bh))\n\t\t\tunlock_buffer(bh);\n\t\tbrelse(bh);\n\t}\n\n\tif (err) {\n\t\t/* free all allocated blocks in error case */\n\t\tfor (i = 0; i < depth; i++) {\n\t\t\tif (!ablocks[i])\n\t\t\t\tcontinue;\n\t\t\text4_free_blocks(handle, inode, NULL, ablocks[i], 1,\n\t\t\t\t\t EXT4_FREE_BLOCKS_METADATA);\n\t\t}\n\t}\n\tkfree(ablocks);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_grow_indepth:\n * implements tree growing procedure:\n * - allocates new block\n * - moves top-level data (index block or leaf) into the new block\n * - initializes new top-level, creating index that points to the\n *   just created block\n */\nstatic int ext4_ext_grow_indepth(handle_t *handle, struct inode *inode,\n\t\t\t\t unsigned int flags)\n{\n\tstruct ext4_extent_header *neh;\n\tstruct buffer_head *bh;\n\text4_fsblk_t newblock, goal = 0;\n\tstruct ext4_super_block *es = EXT4_SB(inode->i_sb)->s_es;\n\tint err = 0;\n\n\t/* Try to prepend new index to old one */\n\tif (ext_depth(inode))\n\t\tgoal = ext4_idx_pblock(EXT_FIRST_INDEX(ext_inode_hdr(inode)));\n\tif (goal > le32_to_cpu(es->s_first_data_block)) {\n\t\tflags |= EXT4_MB_HINT_TRY_GOAL;\n\t\tgoal--;\n\t} else\n\t\tgoal = ext4_inode_to_goal_block(inode);\n\tnewblock = ext4_new_meta_blocks(handle, inode, goal, flags,\n\t\t\t\t\tNULL, &err);\n\tif (newblock == 0)\n\t\treturn err;\n\n\tbh = sb_getblk_gfp(inode->i_sb, newblock, __GFP_MOVABLE | GFP_NOFS);\n\tif (unlikely(!bh))\n\t\treturn -ENOMEM;\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err) {\n\t\tunlock_buffer(bh);\n\t\tgoto out;\n\t}\n\n\t/* move top-level index/leaf into new block */\n\tmemmove(bh->b_data, EXT4_I(inode)->i_data,\n\t\tsizeof(EXT4_I(inode)->i_data));\n\n\t/* set size of new block */\n\tneh = ext_block_hdr(bh);\n\t/* old root could have indexes or leaves\n\t * so calculate e_max right way */\n\tif (ext_depth(inode))\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\telse\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\text4_extent_block_csum_set(inode, neh);\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto out;\n\n\t/* Update top-level index: num,max,pointer */\n\tneh = ext_inode_hdr(inode);\n\tneh->eh_entries = cpu_to_le16(1);\n\text4_idx_store_pblock(EXT_FIRST_INDEX(neh), newblock);\n\tif (neh->eh_depth == 0) {\n\t\t/* Root extent block becomes index block */\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_root_idx(inode, 0));\n\t\tEXT_FIRST_INDEX(neh)->ei_block =\n\t\t\tEXT_FIRST_EXTENT(neh)->ee_block;\n\t}\n\text_debug(\"new root: num %d(%d), lblock %d, ptr %llu\\n\",\n\t\t  le16_to_cpu(neh->eh_entries), le16_to_cpu(neh->eh_max),\n\t\t  le32_to_cpu(EXT_FIRST_INDEX(neh)->ei_block),\n\t\t  ext4_idx_pblock(EXT_FIRST_INDEX(neh)));\n\n\tle16_add_cpu(&neh->eh_depth, 1);\n\text4_mark_inode_dirty(handle, inode);\nout:\n\tbrelse(bh);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_create_new_leaf:\n * finds empty index and adds new leaf.\n * if no free index is found, then it requests in-depth growing.\n */\nstatic int ext4_ext_create_new_leaf(handle_t *handle, struct inode *inode,\n\t\t\t\t    unsigned int mb_flags,\n\t\t\t\t    unsigned int gb_flags,\n\t\t\t\t    struct ext4_ext_path **ppath,\n\t\t\t\t    struct ext4_extent *newext)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_ext_path *curp;\n\tint depth, i, err = 0;\n\nrepeat:\n\ti = depth = ext_depth(inode);\n\n\t/* walk up to the tree and look for free index entry */\n\tcurp = path + depth;\n\twhile (i > 0 && !EXT_HAS_FREE_INDEX(curp)) {\n\t\ti--;\n\t\tcurp--;\n\t}\n\n\t/* we use already allocated block for index block,\n\t * so subsequent data blocks should be contiguous */\n\tif (EXT_HAS_FREE_INDEX(curp)) {\n\t\t/* if we found index with free entry, then use that\n\t\t * entry: create all needed subtree and add new leaf */\n\t\terr = ext4_ext_split(handle, inode, mb_flags, path, newext, i);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\tpath = ext4_find_extent(inode,\n\t\t\t\t    (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    ppath, gb_flags);\n\t\tif (IS_ERR(path))\n\t\t\terr = PTR_ERR(path);\n\t} else {\n\t\t/* tree is full, time to grow in depth */\n\t\terr = ext4_ext_grow_indepth(handle, inode, mb_flags);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\tpath = ext4_find_extent(inode,\n\t\t\t\t   (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    ppath, gb_flags);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * only first (depth 0 -> 1) produces free space;\n\t\t * in all other cases we have to split the grown tree\n\t\t */\n\t\tdepth = ext_depth(inode);\n\t\tif (path[depth].p_hdr->eh_entries == path[depth].p_hdr->eh_max) {\n\t\t\t/* now we need to split */\n\t\t\tgoto repeat;\n\t\t}\n\t}\n\nout:\n\treturn err;\n}\n\n/*\n * search the closest allocated block to the left for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the smallest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nstatic int ext4_ext_search_left(struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\text4_lblk_t *logical, ext4_fsblk_t *phys)\n{\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\tint depth, ee_len;\n\n\tif (unlikely(path == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path == NULL *logical %d!\", *logical);\n\t\treturn -EFSCORRUPTED;\n\t}\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tif (unlikely(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"EXT_FIRST_EXTENT != ex *logical %d ee_block %d!\",\n\t\t\t\t\t *logical, le32_to_cpu(ex->ee_block));\n\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tif (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t  \"ix (%d) != EXT_FIRST_INDEX (%d) (depth %d)!\",\n\t\t\t\t  ix != NULL ? le32_to_cpu(ix->ei_block) : 0,\n\t\t\t\t  EXT_FIRST_INDEX(path[depth].p_hdr) != NULL ?\n\t\tle32_to_cpu(EXT_FIRST_INDEX(path[depth].p_hdr)->ei_block) : 0,\n\t\t\t\t  depth);\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t\t}\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d < ee_block %d + ee_len %d!\",\n\t\t\t\t *logical, le32_to_cpu(ex->ee_block), ee_len);\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\t*logical = le32_to_cpu(ex->ee_block) + ee_len - 1;\n\t*phys = ext4_ext_pblock(ex) + ee_len - 1;\n\treturn 0;\n}\n\n/*\n * search the closest allocated block to the right for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the largest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nstatic int ext4_ext_search_right(struct inode *inode,\n\t\t\t\t struct ext4_ext_path *path,\n\t\t\t\t ext4_lblk_t *logical, ext4_fsblk_t *phys,\n\t\t\t\t struct ext4_extent **ret_ex)\n{\n\tstruct buffer_head *bh = NULL;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\text4_fsblk_t block;\n\tint depth;\t/* Note, NOT eh_depth; depth from top of tree */\n\tint ee_len;\n\n\tif (unlikely(path == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path == NULL *logical %d!\", *logical);\n\t\treturn -EFSCORRUPTED;\n\t}\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tif (unlikely(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"first_extent(path[%d].p_hdr) != ex\",\n\t\t\t\t\t depth);\n\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tif (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t\t \"ix != EXT_FIRST_INDEX *logical %d!\",\n\t\t\t\t\t\t *logical);\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t\t}\n\t\t}\n\t\tgoto found_extent;\n\t}\n\n\tif (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d < ee_block %d + ee_len %d!\",\n\t\t\t\t *logical, le32_to_cpu(ex->ee_block), ee_len);\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\tif (ex != EXT_LAST_EXTENT(path[depth].p_hdr)) {\n\t\t/* next allocated block in this leaf */\n\t\tex++;\n\t\tgoto found_extent;\n\t}\n\n\t/* go up and search for index to the right */\n\twhile (--depth >= 0) {\n\t\tix = path[depth].p_idx;\n\t\tif (ix != EXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\tgoto got_index;\n\t}\n\n\t/* we've gone up to the root and found no index to the right */\n\treturn 0;\n\ngot_index:\n\t/* we've found index to the right, let's\n\t * follow it and find the closest allocated\n\t * block to the right */\n\tix++;\n\tblock = ext4_idx_pblock(ix);\n\twhile (++depth < path->p_depth) {\n\t\t/* subtract from p_depth to get proper eh_depth */\n\t\tbh = read_extent_tree_block(inode, block,\n\t\t\t\t\t    path->p_depth - depth, 0);\n\t\tif (IS_ERR(bh))\n\t\t\treturn PTR_ERR(bh);\n\t\teh = ext_block_hdr(bh);\n\t\tix = EXT_FIRST_INDEX(eh);\n\t\tblock = ext4_idx_pblock(ix);\n\t\tput_bh(bh);\n\t}\n\n\tbh = read_extent_tree_block(inode, block, path->p_depth - depth, 0);\n\tif (IS_ERR(bh))\n\t\treturn PTR_ERR(bh);\n\teh = ext_block_hdr(bh);\n\tex = EXT_FIRST_EXTENT(eh);\nfound_extent:\n\t*logical = le32_to_cpu(ex->ee_block);\n\t*phys = ext4_ext_pblock(ex);\n\t*ret_ex = ex;\n\tif (bh)\n\t\tput_bh(bh);\n\treturn 0;\n}\n\n/*\n * ext4_ext_next_allocated_block:\n * returns allocated block in subsequent extent or EXT_MAX_BLOCKS.\n * NOTE: it considers block number from index entry as\n * allocated block. Thus, index entries have to be consistent\n * with leaves.\n */\next4_lblk_t\next4_ext_next_allocated_block(struct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn EXT_MAX_BLOCKS;\n\n\twhile (depth >= 0) {\n\t\tif (depth == path->p_depth) {\n\t\t\t/* leaf */\n\t\t\tif (path[depth].p_ext &&\n\t\t\t\tpath[depth].p_ext !=\n\t\t\t\t\tEXT_LAST_EXTENT(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_ext[1].ee_block);\n\t\t} else {\n\t\t\t/* index */\n\t\t\tif (path[depth].p_idx !=\n\t\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\t}\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCKS;\n}\n\n/*\n * ext4_ext_next_leaf_block:\n * returns first allocated block from next leaf or EXT_MAX_BLOCKS\n */\nstatic ext4_lblk_t ext4_ext_next_leaf_block(struct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\t/* zero-tree has no leaf blocks at all */\n\tif (depth == 0)\n\t\treturn EXT_MAX_BLOCKS;\n\n\t/* go to index block */\n\tdepth--;\n\n\twhile (depth >= 0) {\n\t\tif (path[depth].p_idx !=\n\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\treturn (ext4_lblk_t)\n\t\t\t\tle32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCKS;\n}\n\n/*\n * ext4_ext_correct_indexes:\n * if leaf gets modified and modified extent is first in the leaf,\n * then we have to correct all indexes above.\n * TODO: do we need to correct tree in all cases?\n */\nstatic int ext4_ext_correct_indexes(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tstruct ext4_extent_header *eh;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent *ex;\n\t__le32 border;\n\tint k, err = 0;\n\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\n\tif (unlikely(ex == NULL || eh == NULL)) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"ex %p == NULL or eh %p == NULL\", ex, eh);\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\tif (depth == 0) {\n\t\t/* there is no tree at all */\n\t\treturn 0;\n\t}\n\n\tif (ex != EXT_FIRST_EXTENT(eh)) {\n\t\t/* we correct tree if first leaf got modified only */\n\t\treturn 0;\n\t}\n\n\t/*\n\t * TODO: we need correction if border is smaller than current one\n\t */\n\tk = depth - 1;\n\tborder = path[depth].p_ext->ee_block;\n\terr = ext4_ext_get_access(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\tpath[k].p_idx->ei_block = border;\n\terr = ext4_ext_dirty(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\n\twhile (k--) {\n\t\t/* change all left-side indexes */\n\t\tif (path[k+1].p_idx != EXT_FIRST_INDEX(path[k+1].p_hdr))\n\t\t\tbreak;\n\t\terr = ext4_ext_get_access(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t\tpath[k].p_idx->ei_block = border;\n\t\terr = ext4_ext_dirty(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nint\next4_can_extents_be_merged(struct inode *inode, struct ext4_extent *ex1,\n\t\t\t\tstruct ext4_extent *ex2)\n{\n\tunsigned short ext1_ee_len, ext2_ee_len;\n\n\tif (ext4_ext_is_unwritten(ex1) != ext4_ext_is_unwritten(ex2))\n\t\treturn 0;\n\n\text1_ee_len = ext4_ext_get_actual_len(ex1);\n\text2_ee_len = ext4_ext_get_actual_len(ex2);\n\n\tif (le32_to_cpu(ex1->ee_block) + ext1_ee_len !=\n\t\t\tle32_to_cpu(ex2->ee_block))\n\t\treturn 0;\n\n\t/*\n\t * To allow future support for preallocated extents to be added\n\t * as an RO_COMPAT feature, refuse to merge to extents if\n\t * this can result in the top bit of ee_len being set.\n\t */\n\tif (ext1_ee_len + ext2_ee_len > EXT_INIT_MAX_LEN)\n\t\treturn 0;\n\t/*\n\t * The check for IO to unwritten extent is somewhat racy as we\n\t * increment i_unwritten / set EXT4_STATE_DIO_UNWRITTEN only after\n\t * dropping i_data_sem. But reserved blocks should save us in that\n\t * case.\n\t */\n\tif (ext4_ext_is_unwritten(ex1) &&\n\t    (ext4_test_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN) ||\n\t     atomic_read(&EXT4_I(inode)->i_unwritten) ||\n\t     (ext1_ee_len + ext2_ee_len > EXT_UNWRITTEN_MAX_LEN)))\n\t\treturn 0;\n#ifdef AGGRESSIVE_TEST\n\tif (ext1_ee_len >= 4)\n\t\treturn 0;\n#endif\n\n\tif (ext4_ext_pblock(ex1) + ext1_ee_len == ext4_ext_pblock(ex2))\n\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * This function tries to merge the \"ex\" extent to the next extent in the tree.\n * It always tries to merge towards right. If you want to merge towards\n * left, pass \"ex - 1\" as argument instead of \"ex\".\n * Returns 0 if the extents (ex and ex+1) were _not_ merged and returns\n * 1 if they got merged.\n */\nstatic int ext4_ext_try_to_merge_right(struct inode *inode,\n\t\t\t\t struct ext4_ext_path *path,\n\t\t\t\t struct ext4_extent *ex)\n{\n\tstruct ext4_extent_header *eh;\n\tunsigned int depth, len;\n\tint merge_done = 0, unwritten;\n\n\tdepth = ext_depth(inode);\n\tBUG_ON(path[depth].p_hdr == NULL);\n\teh = path[depth].p_hdr;\n\n\twhile (ex < EXT_LAST_EXTENT(eh)) {\n\t\tif (!ext4_can_extents_be_merged(inode, ex, ex + 1))\n\t\t\tbreak;\n\t\t/* merge with next extent! */\n\t\tunwritten = ext4_ext_is_unwritten(ex);\n\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t+ ext4_ext_get_actual_len(ex + 1));\n\t\tif (unwritten)\n\t\t\text4_ext_mark_unwritten(ex);\n\n\t\tif (ex + 1 < EXT_LAST_EXTENT(eh)) {\n\t\t\tlen = (EXT_LAST_EXTENT(eh) - ex - 1)\n\t\t\t\t* sizeof(struct ext4_extent);\n\t\t\tmemmove(ex + 1, ex + 2, len);\n\t\t}\n\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\tmerge_done = 1;\n\t\tWARN_ON(eh->eh_entries == 0);\n\t\tif (!eh->eh_entries)\n\t\t\tEXT4_ERROR_INODE(inode, \"eh->eh_entries = 0!\");\n\t}\n\n\treturn merge_done;\n}\n\n/*\n * This function does a very simple check to see if we can collapse\n * an extent tree with a single extent tree leaf block into the inode.\n */\nstatic void ext4_ext_try_to_merge_up(handle_t *handle,\n\t\t\t\t     struct inode *inode,\n\t\t\t\t     struct ext4_ext_path *path)\n{\n\tsize_t s;\n\tunsigned max_root = ext4_ext_space_root(inode, 0);\n\text4_fsblk_t blk;\n\n\tif ((path[0].p_depth != 1) ||\n\t    (le16_to_cpu(path[0].p_hdr->eh_entries) != 1) ||\n\t    (le16_to_cpu(path[1].p_hdr->eh_entries) > max_root))\n\t\treturn;\n\n\t/*\n\t * We need to modify the block allocation bitmap and the block\n\t * group descriptor to release the extent tree block.  If we\n\t * can't get the journal credits, give up.\n\t */\n\tif (ext4_journal_extend(handle, 2))\n\t\treturn;\n\n\t/*\n\t * Copy the extent data up to the inode\n\t */\n\tblk = ext4_idx_pblock(path[0].p_idx);\n\ts = le16_to_cpu(path[1].p_hdr->eh_entries) *\n\t\tsizeof(struct ext4_extent_idx);\n\ts += sizeof(struct ext4_extent_header);\n\n\tpath[1].p_maxdepth = path[0].p_maxdepth;\n\tmemcpy(path[0].p_hdr, path[1].p_hdr, s);\n\tpath[0].p_depth = 0;\n\tpath[0].p_ext = EXT_FIRST_EXTENT(path[0].p_hdr) +\n\t\t(path[1].p_ext - EXT_FIRST_EXTENT(path[1].p_hdr));\n\tpath[0].p_hdr->eh_max = cpu_to_le16(max_root);\n\n\tbrelse(path[1].p_bh);\n\text4_free_blocks(handle, inode, NULL, blk, 1,\n\t\t\t EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);\n}\n\n/*\n * This function tries to merge the @ex extent to neighbours in the tree.\n * return 1 if merge left else 0.\n */\nstatic void ext4_ext_try_to_merge(handle_t *handle,\n\t\t\t\t  struct inode *inode,\n\t\t\t\t  struct ext4_ext_path *path,\n\t\t\t\t  struct ext4_extent *ex) {\n\tstruct ext4_extent_header *eh;\n\tunsigned int depth;\n\tint merge_done = 0;\n\n\tdepth = ext_depth(inode);\n\tBUG_ON(path[depth].p_hdr == NULL);\n\teh = path[depth].p_hdr;\n\n\tif (ex > EXT_FIRST_EXTENT(eh))\n\t\tmerge_done = ext4_ext_try_to_merge_right(inode, path, ex - 1);\n\n\tif (!merge_done)\n\t\t(void) ext4_ext_try_to_merge_right(inode, path, ex);\n\n\text4_ext_try_to_merge_up(handle, inode, path);\n}\n\n/*\n * check if a portion of the \"newext\" extent overlaps with an\n * existing extent.\n *\n * If there is an overlap discovered, it updates the length of the newext\n * such that there will be no overlap, and then returns 1.\n * If there is no overlap found, it returns 0.\n */\nstatic unsigned int ext4_ext_check_overlap(struct ext4_sb_info *sbi,\n\t\t\t\t\t   struct inode *inode,\n\t\t\t\t\t   struct ext4_extent *newext,\n\t\t\t\t\t   struct ext4_ext_path *path)\n{\n\text4_lblk_t b1, b2;\n\tunsigned int depth, len1;\n\tunsigned int ret = 0;\n\n\tb1 = le32_to_cpu(newext->ee_block);\n\tlen1 = ext4_ext_get_actual_len(newext);\n\tdepth = ext_depth(inode);\n\tif (!path[depth].p_ext)\n\t\tgoto out;\n\tb2 = EXT4_LBLK_CMASK(sbi, le32_to_cpu(path[depth].p_ext->ee_block));\n\n\t/*\n\t * get the next allocated block if the extent in the path\n\t * is before the requested block(s)\n\t */\n\tif (b2 < b1) {\n\t\tb2 = ext4_ext_next_allocated_block(path);\n\t\tif (b2 == EXT_MAX_BLOCKS)\n\t\t\tgoto out;\n\t\tb2 = EXT4_LBLK_CMASK(sbi, b2);\n\t}\n\n\t/* check for wrap through zero on extent logical start block*/\n\tif (b1 + len1 < b1) {\n\t\tlen1 = EXT_MAX_BLOCKS - b1;\n\t\tnewext->ee_len = cpu_to_le16(len1);\n\t\tret = 1;\n\t}\n\n\t/* check for overlap */\n\tif (b1 + len1 > b2) {\n\t\tnewext->ee_len = cpu_to_le16(b2 - b1);\n\t\tret = 1;\n\t}\nout:\n\treturn ret;\n}\n\n/*\n * ext4_ext_insert_extent:\n * tries to merge requsted extent into the existing extent or\n * inserts requested extent as new one into the tree,\n * creating new leaf in the no-space case.\n */\nint ext4_ext_insert_extent(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path **ppath,\n\t\t\t\tstruct ext4_extent *newext, int gb_flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex, *fex;\n\tstruct ext4_extent *nearex; /* nearest extent */\n\tstruct ext4_ext_path *npath = NULL;\n\tint depth, len, err;\n\text4_lblk_t next;\n\tint mb_flags = 0, unwritten;\n\n\tif (gb_flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\tmb_flags |= EXT4_MB_DELALLOC_RESERVED;\n\tif (unlikely(ext4_ext_get_actual_len(newext) == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"ext4_ext_get_actual_len(newext) == 0\");\n\t\treturn -EFSCORRUPTED;\n\t}\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\teh = path[depth].p_hdr;\n\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\t/* try to insert block into found extent and return */\n\tif (ex && !(gb_flags & EXT4_GET_BLOCKS_PRE_IO)) {\n\n\t\t/*\n\t\t * Try to see whether we should rather test the extent on\n\t\t * right from ex, or from the left of ex. This is because\n\t\t * ext4_find_extent() can return either extent on the\n\t\t * left, or on the right from the searched position. This\n\t\t * will make merging more effective.\n\t\t */\n\t\tif (ex < EXT_LAST_EXTENT(eh) &&\n\t\t    (le32_to_cpu(ex->ee_block) +\n\t\t    ext4_ext_get_actual_len(ex) <\n\t\t    le32_to_cpu(newext->ee_block))) {\n\t\t\tex += 1;\n\t\t\tgoto prepend;\n\t\t} else if ((ex > EXT_FIRST_EXTENT(eh)) &&\n\t\t\t   (le32_to_cpu(newext->ee_block) +\n\t\t\t   ext4_ext_get_actual_len(newext) <\n\t\t\t   le32_to_cpu(ex->ee_block)))\n\t\t\tex -= 1;\n\n\t\t/* Try to append newex to the ex */\n\t\tif (ext4_can_extents_be_merged(inode, ex, newext)) {\n\t\t\text_debug(\"append [%d]%d block to %u:[%d]%d\"\n\t\t\t\t  \"(from %llu)\\n\",\n\t\t\t\t  ext4_ext_is_unwritten(newext),\n\t\t\t\t  ext4_ext_get_actual_len(newext),\n\t\t\t\t  le32_to_cpu(ex->ee_block),\n\t\t\t\t  ext4_ext_is_unwritten(ex),\n\t\t\t\t  ext4_ext_get_actual_len(ex),\n\t\t\t\t  ext4_ext_pblock(ex));\n\t\t\terr = ext4_ext_get_access(handle, inode,\n\t\t\t\t\t\t  path + depth);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tunwritten = ext4_ext_is_unwritten(ex);\n\t\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t\t+ ext4_ext_get_actual_len(newext));\n\t\t\tif (unwritten)\n\t\t\t\text4_ext_mark_unwritten(ex);\n\t\t\teh = path[depth].p_hdr;\n\t\t\tnearex = ex;\n\t\t\tgoto merge;\n\t\t}\n\nprepend:\n\t\t/* Try to prepend newex to the ex */\n\t\tif (ext4_can_extents_be_merged(inode, newext, ex)) {\n\t\t\text_debug(\"prepend %u[%d]%d block to %u:[%d]%d\"\n\t\t\t\t  \"(from %llu)\\n\",\n\t\t\t\t  le32_to_cpu(newext->ee_block),\n\t\t\t\t  ext4_ext_is_unwritten(newext),\n\t\t\t\t  ext4_ext_get_actual_len(newext),\n\t\t\t\t  le32_to_cpu(ex->ee_block),\n\t\t\t\t  ext4_ext_is_unwritten(ex),\n\t\t\t\t  ext4_ext_get_actual_len(ex),\n\t\t\t\t  ext4_ext_pblock(ex));\n\t\t\terr = ext4_ext_get_access(handle, inode,\n\t\t\t\t\t\t  path + depth);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tunwritten = ext4_ext_is_unwritten(ex);\n\t\t\tex->ee_block = newext->ee_block;\n\t\t\text4_ext_store_pblock(ex, ext4_ext_pblock(newext));\n\t\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t\t+ ext4_ext_get_actual_len(newext));\n\t\t\tif (unwritten)\n\t\t\t\text4_ext_mark_unwritten(ex);\n\t\t\teh = path[depth].p_hdr;\n\t\t\tnearex = ex;\n\t\t\tgoto merge;\n\t\t}\n\t}\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max))\n\t\tgoto has_space;\n\n\t/* probably next leaf has space for us? */\n\tfex = EXT_LAST_EXTENT(eh);\n\tnext = EXT_MAX_BLOCKS;\n\tif (le32_to_cpu(newext->ee_block) > le32_to_cpu(fex->ee_block))\n\t\tnext = ext4_ext_next_leaf_block(path);\n\tif (next != EXT_MAX_BLOCKS) {\n\t\text_debug(\"next leaf block - %u\\n\", next);\n\t\tBUG_ON(npath != NULL);\n\t\tnpath = ext4_find_extent(inode, next, NULL, 0);\n\t\tif (IS_ERR(npath))\n\t\t\treturn PTR_ERR(npath);\n\t\tBUG_ON(npath->p_depth != path->p_depth);\n\t\teh = npath[depth].p_hdr;\n\t\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max)) {\n\t\t\text_debug(\"next leaf isn't full(%d)\\n\",\n\t\t\t\t  le16_to_cpu(eh->eh_entries));\n\t\t\tpath = npath;\n\t\t\tgoto has_space;\n\t\t}\n\t\text_debug(\"next leaf has no free space(%d,%d)\\n\",\n\t\t\t  le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\t}\n\n\t/*\n\t * There is no free space in the found leaf.\n\t * We're gonna add a new leaf in the tree.\n\t */\n\tif (gb_flags & EXT4_GET_BLOCKS_METADATA_NOFAIL)\n\t\tmb_flags |= EXT4_MB_USE_RESERVED;\n\terr = ext4_ext_create_new_leaf(handle, inode, mb_flags, gb_flags,\n\t\t\t\t       ppath, newext);\n\tif (err)\n\t\tgoto cleanup;\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\nhas_space:\n\tnearex = path[depth].p_ext;\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto cleanup;\n\n\tif (!nearex) {\n\t\t/* there is no extent in this leaf, create first one */\n\t\text_debug(\"first extent in the leaf: %u:%llu:[%d]%d\\n\",\n\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\text4_ext_pblock(newext),\n\t\t\t\text4_ext_is_unwritten(newext),\n\t\t\t\text4_ext_get_actual_len(newext));\n\t\tnearex = EXT_FIRST_EXTENT(eh);\n\t} else {\n\t\tif (le32_to_cpu(newext->ee_block)\n\t\t\t   > le32_to_cpu(nearex->ee_block)) {\n\t\t\t/* Insert after */\n\t\t\text_debug(\"insert %u:%llu:[%d]%d before: \"\n\t\t\t\t\t\"nearest %p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_unwritten(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tnearex);\n\t\t\tnearex++;\n\t\t} else {\n\t\t\t/* Insert before */\n\t\t\tBUG_ON(newext->ee_block == nearex->ee_block);\n\t\t\text_debug(\"insert %u:%llu:[%d]%d after: \"\n\t\t\t\t\t\"nearest %p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_unwritten(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tnearex);\n\t\t}\n\t\tlen = EXT_LAST_EXTENT(eh) - nearex + 1;\n\t\tif (len > 0) {\n\t\t\text_debug(\"insert %u:%llu:[%d]%d: \"\n\t\t\t\t\t\"move %d extents from 0x%p to 0x%p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_unwritten(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tlen, nearex, nearex + 1);\n\t\t\tmemmove(nearex + 1, nearex,\n\t\t\t\tlen * sizeof(struct ext4_extent));\n\t\t}\n\t}\n\n\tle16_add_cpu(&eh->eh_entries, 1);\n\tpath[depth].p_ext = nearex;\n\tnearex->ee_block = newext->ee_block;\n\text4_ext_store_pblock(nearex, ext4_ext_pblock(newext));\n\tnearex->ee_len = newext->ee_len;\n\nmerge:\n\t/* try to merge extents */\n\tif (!(gb_flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\text4_ext_try_to_merge(handle, inode, path, nearex);\n\n\n\t/* time to correct all indexes above */\n\terr = ext4_ext_correct_indexes(handle, inode, path);\n\tif (err)\n\t\tgoto cleanup;\n\n\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\ncleanup:\n\text4_ext_drop_refs(npath);\n\tkfree(npath);\n\treturn err;\n}\n\nstatic int ext4_fill_fiemap_extents(struct inode *inode,\n\t\t\t\t    ext4_lblk_t block, ext4_lblk_t num,\n\t\t\t\t    struct fiemap_extent_info *fieinfo)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_extent *ex;\n\tstruct extent_status es;\n\text4_lblk_t next, next_del, start = 0, end = 0;\n\text4_lblk_t last = block + num;\n\tint exists, depth = 0, err = 0;\n\tunsigned int flags = 0;\n\tunsigned char blksize_bits = inode->i_sb->s_blocksize_bits;\n\n\twhile (block < last && block != EXT_MAX_BLOCKS) {\n\t\tnum = last - block;\n\t\t/* find extent for this block */\n\t\tdown_read(&EXT4_I(inode)->i_data_sem);\n\n\t\tpath = ext4_find_extent(inode, block, &path, 0);\n\t\tif (IS_ERR(path)) {\n\t\t\tup_read(&EXT4_I(inode)->i_data_sem);\n\t\t\terr = PTR_ERR(path);\n\t\t\tpath = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tdepth = ext_depth(inode);\n\t\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\t\tup_read(&EXT4_I(inode)->i_data_sem);\n\t\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tbreak;\n\t\t}\n\t\tex = path[depth].p_ext;\n\t\tnext = ext4_ext_next_allocated_block(path);\n\n\t\tflags = 0;\n\t\texists = 0;\n\t\tif (!ex) {\n\t\t\t/* there is no extent yet, so try to allocate\n\t\t\t * all requested space */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t} else if (le32_to_cpu(ex->ee_block) > block) {\n\t\t\t/* need to allocate space before found extent */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)\n\t\t\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\t\t/* need to allocate space after found extent */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t\tif (end >= next)\n\t\t\t\tend = next;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)) {\n\t\t\t/*\n\t\t\t * some part of requested space is covered\n\t\t\t * by found extent\n\t\t\t */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block)\n\t\t\t\t+ ext4_ext_get_actual_len(ex);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t\texists = 1;\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n\t\tBUG_ON(end <= start);\n\n\t\tif (!exists) {\n\t\t\tes.es_lblk = start;\n\t\t\tes.es_len = end - start;\n\t\t\tes.es_pblk = 0;\n\t\t} else {\n\t\t\tes.es_lblk = le32_to_cpu(ex->ee_block);\n\t\t\tes.es_len = ext4_ext_get_actual_len(ex);\n\t\t\tes.es_pblk = ext4_ext_pblock(ex);\n\t\t\tif (ext4_ext_is_unwritten(ex))\n\t\t\t\tflags |= FIEMAP_EXTENT_UNWRITTEN;\n\t\t}\n\n\t\t/*\n\t\t * Find delayed extent and update es accordingly. We call\n\t\t * it even in !exists case to find out whether es is the\n\t\t * last existing extent or not.\n\t\t */\n\t\tnext_del = ext4_find_delayed_extent(inode, &es);\n\t\tif (!exists && next_del) {\n\t\t\texists = 1;\n\t\t\tflags |= (FIEMAP_EXTENT_DELALLOC |\n\t\t\t\t  FIEMAP_EXTENT_UNKNOWN);\n\t\t}\n\t\tup_read(&EXT4_I(inode)->i_data_sem);\n\n\t\tif (unlikely(es.es_len == 0)) {\n\t\t\tEXT4_ERROR_INODE(inode, \"es.es_len == 0\");\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * This is possible iff next == next_del == EXT_MAX_BLOCKS.\n\t\t * we need to check next == EXT_MAX_BLOCKS because it is\n\t\t * possible that an extent is with unwritten and delayed\n\t\t * status due to when an extent is delayed allocated and\n\t\t * is allocated by fallocate status tree will track both of\n\t\t * them in a extent.\n\t\t *\n\t\t * So we could return a unwritten and delayed extent, and\n\t\t * its block is equal to 'next'.\n\t\t */\n\t\tif (next == next_del && next == EXT_MAX_BLOCKS) {\n\t\t\tflags |= FIEMAP_EXTENT_LAST;\n\t\t\tif (unlikely(next_del != EXT_MAX_BLOCKS ||\n\t\t\t\t     next != EXT_MAX_BLOCKS)) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t\t \"next extent == %u, next \"\n\t\t\t\t\t\t \"delalloc extent = %u\",\n\t\t\t\t\t\t next, next_del);\n\t\t\t\terr = -EFSCORRUPTED;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (exists) {\n\t\t\terr = fiemap_fill_next_extent(fieinfo,\n\t\t\t\t(__u64)es.es_lblk << blksize_bits,\n\t\t\t\t(__u64)es.es_pblk << blksize_bits,\n\t\t\t\t(__u64)es.es_len << blksize_bits,\n\t\t\t\tflags);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t\tif (err == 1) {\n\t\t\t\terr = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tblock = es.es_lblk + es.es_len;\n\t}\n\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\treturn err;\n}\n\n/*\n * ext4_ext_determine_hole - determine hole around given block\n * @inode:\tinode we lookup in\n * @path:\tpath in extent tree to @lblk\n * @lblk:\tpointer to logical block around which we want to determine hole\n *\n * Determine hole length (and start if easily possible) around given logical\n * block. We don't try too hard to find the beginning of the hole but @path\n * actually points to extent before @lblk, we provide it.\n *\n * The function returns the length of a hole starting at @lblk. We update @lblk\n * to the beginning of the hole if we managed to find it.\n */\nstatic ext4_lblk_t ext4_ext_determine_hole(struct inode *inode,\n\t\t\t\t\t   struct ext4_ext_path *path,\n\t\t\t\t\t   ext4_lblk_t *lblk)\n{\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent *ex;\n\text4_lblk_t len;\n\n\tex = path[depth].p_ext;\n\tif (ex == NULL) {\n\t\t/* there is no extent yet, so gap is [0;-] */\n\t\t*lblk = 0;\n\t\tlen = EXT_MAX_BLOCKS;\n\t} else if (*lblk < le32_to_cpu(ex->ee_block)) {\n\t\tlen = le32_to_cpu(ex->ee_block) - *lblk;\n\t} else if (*lblk >= le32_to_cpu(ex->ee_block)\n\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\text4_lblk_t next;\n\n\t\t*lblk = le32_to_cpu(ex->ee_block) + ext4_ext_get_actual_len(ex);\n\t\tnext = ext4_ext_next_allocated_block(path);\n\t\tBUG_ON(next == *lblk);\n\t\tlen = next - *lblk;\n\t} else {\n\t\tBUG();\n\t}\n\treturn len;\n}\n\n/*\n * ext4_ext_put_gap_in_cache:\n * calculate boundaries of the gap that the requested block fits into\n * and cache this gap\n */\nstatic void\next4_ext_put_gap_in_cache(struct inode *inode, ext4_lblk_t hole_start,\n\t\t\t  ext4_lblk_t hole_len)\n{\n\tstruct extent_status es;\n\n\text4_es_find_extent_range(inode, &ext4_es_is_delayed, hole_start,\n\t\t\t\t  hole_start + hole_len - 1, &es);\n\tif (es.es_len) {\n\t\t/* There's delayed extent containing lblock? */\n\t\tif (es.es_lblk <= hole_start)\n\t\t\treturn;\n\t\thole_len = min(es.es_lblk - hole_start, hole_len);\n\t}\n\text_debug(\" -> %u:%u\\n\", hole_start, hole_len);\n\text4_es_insert_extent(inode, hole_start, hole_len, ~0,\n\t\t\t      EXTENT_STATUS_HOLE);\n}\n\n/*\n * ext4_ext_rm_idx:\n * removes index from the index block.\n */\nstatic int ext4_ext_rm_idx(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path, int depth)\n{\n\tint err;\n\text4_fsblk_t leaf;\n\n\t/* free index block */\n\tdepth--;\n\tpath = path + depth;\n\tleaf = ext4_idx_pblock(path->p_idx);\n\tif (unlikely(path->p_hdr->eh_entries == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"path->p_hdr->eh_entries == 0\");\n\t\treturn -EFSCORRUPTED;\n\t}\n\terr = ext4_ext_get_access(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\n\tif (path->p_idx != EXT_LAST_INDEX(path->p_hdr)) {\n\t\tint len = EXT_LAST_INDEX(path->p_hdr) - path->p_idx;\n\t\tlen *= sizeof(struct ext4_extent_idx);\n\t\tmemmove(path->p_idx, path->p_idx + 1, len);\n\t}\n\n\tle16_add_cpu(&path->p_hdr->eh_entries, -1);\n\terr = ext4_ext_dirty(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\text_debug(\"index is empty, remove it, free block %llu\\n\", leaf);\n\ttrace_ext4_ext_rm_idx(inode, leaf);\n\n\text4_free_blocks(handle, inode, NULL, leaf, 1,\n\t\t\t EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);\n\n\twhile (--depth >= 0) {\n\t\tif (path->p_idx != EXT_FIRST_INDEX(path->p_hdr))\n\t\t\tbreak;\n\t\tpath--;\n\t\terr = ext4_ext_get_access(handle, inode, path);\n\t\tif (err)\n\t\t\tbreak;\n\t\tpath->p_idx->ei_block = (path+1)->p_idx->ei_block;\n\t\terr = ext4_ext_dirty(handle, inode, path);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\treturn err;\n}\n\n/*\n * ext4_ext_calc_credits_for_single_extent:\n * This routine returns max. credits that needed to insert an extent\n * to the extent tree.\n * When pass the actual path, the caller should calculate credits\n * under i_data_sem.\n */\nint ext4_ext_calc_credits_for_single_extent(struct inode *inode, int nrblocks,\n\t\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path) {\n\t\tint depth = ext_depth(inode);\n\t\tint ret = 0;\n\n\t\t/* probably there is space in leaf? */\n\t\tif (le16_to_cpu(path[depth].p_hdr->eh_entries)\n\t\t\t\t< le16_to_cpu(path[depth].p_hdr->eh_max)) {\n\n\t\t\t/*\n\t\t\t *  There are some space in the leaf tree, no\n\t\t\t *  need to account for leaf block credit\n\t\t\t *\n\t\t\t *  bitmaps and block group descriptor blocks\n\t\t\t *  and other metadata blocks still need to be\n\t\t\t *  accounted.\n\t\t\t */\n\t\t\t/* 1 bitmap, 1 block group descriptor */\n\t\t\tret = 2 + EXT4_META_TRANS_BLOCKS(inode->i_sb);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn ext4_chunk_trans_blocks(inode, nrblocks);\n}\n\n/*\n * How many index/leaf blocks need to change/allocate to add @extents extents?\n *\n * If we add a single extent, then in the worse case, each tree level\n * index/leaf need to be changed in case of the tree split.\n *\n * If more extents are inserted, they could cause the whole tree split more\n * than once, but this is really rare.\n */\nint ext4_ext_index_trans_blocks(struct inode *inode, int extents)\n{\n\tint index;\n\tint depth;\n\n\t/* If we are converting the inline data, only one is needed here. */\n\tif (ext4_has_inline_data(inode))\n\t\treturn 1;\n\n\tdepth = ext_depth(inode);\n\n\tif (extents <= 1)\n\t\tindex = depth * 2;\n\telse\n\t\tindex = depth * 3;\n\n\treturn index;\n}\n\nstatic inline int get_default_free_blocks_flags(struct inode *inode)\n{\n\tif (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode) ||\n\t    ext4_test_inode_flag(inode, EXT4_INODE_EA_INODE))\n\t\treturn EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET;\n\telse if (ext4_should_journal_data(inode))\n\t\treturn EXT4_FREE_BLOCKS_FORGET;\n\treturn 0;\n}\n\n/*\n * ext4_rereserve_cluster - increment the reserved cluster count when\n *                          freeing a cluster with a pending reservation\n *\n * @inode - file containing the cluster\n * @lblk - logical block in cluster to be reserved\n *\n * Increments the reserved cluster count and adjusts quota in a bigalloc\n * file system when freeing a partial cluster containing at least one\n * delayed and unwritten block.  A partial cluster meeting that\n * requirement will have a pending reservation.  If so, the\n * RERESERVE_CLUSTER flag is used when calling ext4_free_blocks() to\n * defer reserved and allocated space accounting to a subsequent call\n * to this function.\n */\nstatic void ext4_rereserve_cluster(struct inode *inode, ext4_lblk_t lblk)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\n\tdquot_reclaim_block(inode, EXT4_C2B(sbi, 1));\n\n\tspin_lock(&ei->i_block_reservation_lock);\n\tei->i_reserved_data_blocks++;\n\tpercpu_counter_add(&sbi->s_dirtyclusters_counter, 1);\n\tspin_unlock(&ei->i_block_reservation_lock);\n\n\tpercpu_counter_add(&sbi->s_freeclusters_counter, 1);\n\text4_remove_pending(inode, lblk);\n}\n\nstatic int ext4_remove_blocks(handle_t *handle, struct inode *inode,\n\t\t\t      struct ext4_extent *ex,\n\t\t\t      struct partial_cluster *partial,\n\t\t\t      ext4_lblk_t from, ext4_lblk_t to)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tunsigned short ee_len = ext4_ext_get_actual_len(ex);\n\text4_fsblk_t last_pblk, pblk;\n\text4_lblk_t num;\n\tint flags;\n\n\t/* only extent tail removal is allowed */\n\tif (from < le32_to_cpu(ex->ee_block) ||\n\t    to != le32_to_cpu(ex->ee_block) + ee_len - 1) {\n\t\text4_error(sbi->s_sb,\n\t\t\t   \"strange request: removal(2) %u-%u from %u:%u\",\n\t\t\t   from, to, le32_to_cpu(ex->ee_block), ee_len);\n\t\treturn 0;\n\t}\n\n#ifdef EXTENTS_STATS\n\tspin_lock(&sbi->s_ext_stats_lock);\n\tsbi->s_ext_blocks += ee_len;\n\tsbi->s_ext_extents++;\n\tif (ee_len < sbi->s_ext_min)\n\t\tsbi->s_ext_min = ee_len;\n\tif (ee_len > sbi->s_ext_max)\n\t\tsbi->s_ext_max = ee_len;\n\tif (ext_depth(inode) > sbi->s_depth_max)\n\t\tsbi->s_depth_max = ext_depth(inode);\n\tspin_unlock(&sbi->s_ext_stats_lock);\n#endif\n\n\ttrace_ext4_remove_blocks(inode, ex, from, to, partial);\n\n\t/*\n\t * if we have a partial cluster, and it's different from the\n\t * cluster of the last block in the extent, we free it\n\t */\n\tlast_pblk = ext4_ext_pblock(ex) + ee_len - 1;\n\n\tif (partial->state != initial &&\n\t    partial->pclu != EXT4_B2C(sbi, last_pblk)) {\n\t\tif (partial->state == tofree) {\n\t\t\tflags = get_default_free_blocks_flags(inode);\n\t\t\tif (ext4_is_pending(inode, partial->lblk))\n\t\t\t\tflags |= EXT4_FREE_BLOCKS_RERESERVE_CLUSTER;\n\t\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t\t EXT4_C2B(sbi, partial->pclu),\n\t\t\t\t\t sbi->s_cluster_ratio, flags);\n\t\t\tif (flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)\n\t\t\t\text4_rereserve_cluster(inode, partial->lblk);\n\t\t}\n\t\tpartial->state = initial;\n\t}\n\n\tnum = le32_to_cpu(ex->ee_block) + ee_len - from;\n\tpblk = ext4_ext_pblock(ex) + ee_len - num;\n\n\t/*\n\t * We free the partial cluster at the end of the extent (if any),\n\t * unless the cluster is used by another extent (partial_cluster\n\t * state is nofree).  If a partial cluster exists here, it must be\n\t * shared with the last block in the extent.\n\t */\n\tflags = get_default_free_blocks_flags(inode);\n\n\t/* partial, left end cluster aligned, right end unaligned */\n\tif ((EXT4_LBLK_COFF(sbi, to) != sbi->s_cluster_ratio - 1) &&\n\t    (EXT4_LBLK_CMASK(sbi, to) >= from) &&\n\t    (partial->state != nofree)) {\n\t\tif (ext4_is_pending(inode, to))\n\t\t\tflags |= EXT4_FREE_BLOCKS_RERESERVE_CLUSTER;\n\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t EXT4_PBLK_CMASK(sbi, last_pblk),\n\t\t\t\t sbi->s_cluster_ratio, flags);\n\t\tif (flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)\n\t\t\text4_rereserve_cluster(inode, to);\n\t\tpartial->state = initial;\n\t\tflags = get_default_free_blocks_flags(inode);\n\t}\n\n\tflags |= EXT4_FREE_BLOCKS_NOFREE_LAST_CLUSTER;\n\n\t/*\n\t * For bigalloc file systems, we never free a partial cluster\n\t * at the beginning of the extent.  Instead, we check to see if we\n\t * need to free it on a subsequent call to ext4_remove_blocks,\n\t * or at the end of ext4_ext_rm_leaf or ext4_ext_remove_space.\n\t */\n\tflags |= EXT4_FREE_BLOCKS_NOFREE_FIRST_CLUSTER;\n\text4_free_blocks(handle, inode, NULL, pblk, num, flags);\n\n\t/* reset the partial cluster if we've freed past it */\n\tif (partial->state != initial && partial->pclu != EXT4_B2C(sbi, pblk))\n\t\tpartial->state = initial;\n\n\t/*\n\t * If we've freed the entire extent but the beginning is not left\n\t * cluster aligned and is not marked as ineligible for freeing we\n\t * record the partial cluster at the beginning of the extent.  It\n\t * wasn't freed by the preceding ext4_free_blocks() call, and we\n\t * need to look farther to the left to determine if it's to be freed\n\t * (not shared with another extent). Else, reset the partial\n\t * cluster - we're either  done freeing or the beginning of the\n\t * extent is left cluster aligned.\n\t */\n\tif (EXT4_LBLK_COFF(sbi, from) && num == ee_len) {\n\t\tif (partial->state == initial) {\n\t\t\tpartial->pclu = EXT4_B2C(sbi, pblk);\n\t\t\tpartial->lblk = from;\n\t\t\tpartial->state = tofree;\n\t\t}\n\t} else {\n\t\tpartial->state = initial;\n\t}\n\n\treturn 0;\n}\n\n/*\n * ext4_ext_rm_leaf() Removes the extents associated with the\n * blocks appearing between \"start\" and \"end\".  Both \"start\"\n * and \"end\" must appear in the same extent or EIO is returned.\n *\n * @handle: The journal handle\n * @inode:  The files inode\n * @path:   The path to the leaf\n * @partial_cluster: The cluster which we'll have to free if all extents\n *                   has been released from it.  However, if this value is\n *                   negative, it's a cluster just to the right of the\n *                   punched region and it must not be freed.\n * @start:  The first block to remove\n * @end:   The last block to remove\n */\nstatic int\next4_ext_rm_leaf(handle_t *handle, struct inode *inode,\n\t\t struct ext4_ext_path *path,\n\t\t struct partial_cluster *partial,\n\t\t ext4_lblk_t start, ext4_lblk_t end)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tint err = 0, correct_index = 0;\n\tint depth = ext_depth(inode), credits;\n\tstruct ext4_extent_header *eh;\n\text4_lblk_t a, b;\n\tunsigned num;\n\text4_lblk_t ex_ee_block;\n\tunsigned short ex_ee_len;\n\tunsigned unwritten = 0;\n\tstruct ext4_extent *ex;\n\text4_fsblk_t pblk;\n\n\t/* the header must be checked already in ext4_ext_remove_space() */\n\text_debug(\"truncate since %u in leaf to %u\\n\", start, end);\n\tif (!path[depth].p_hdr)\n\t\tpath[depth].p_hdr = ext_block_hdr(path[depth].p_bh);\n\teh = path[depth].p_hdr;\n\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\treturn -EFSCORRUPTED;\n\t}\n\t/* find where to start removing */\n\tex = path[depth].p_ext;\n\tif (!ex)\n\t\tex = EXT_LAST_EXTENT(eh);\n\n\tex_ee_block = le32_to_cpu(ex->ee_block);\n\tex_ee_len = ext4_ext_get_actual_len(ex);\n\n\ttrace_ext4_ext_rm_leaf(inode, start, ex, partial);\n\n\twhile (ex >= EXT_FIRST_EXTENT(eh) &&\n\t\t\tex_ee_block + ex_ee_len > start) {\n\n\t\tif (ext4_ext_is_unwritten(ex))\n\t\t\tunwritten = 1;\n\t\telse\n\t\t\tunwritten = 0;\n\n\t\text_debug(\"remove ext %u:[%d]%d\\n\", ex_ee_block,\n\t\t\t  unwritten, ex_ee_len);\n\t\tpath[depth].p_ext = ex;\n\n\t\ta = ex_ee_block > start ? ex_ee_block : start;\n\t\tb = ex_ee_block+ex_ee_len - 1 < end ?\n\t\t\tex_ee_block+ex_ee_len - 1 : end;\n\n\t\text_debug(\"  border %u:%u\\n\", a, b);\n\n\t\t/* If this extent is beyond the end of the hole, skip it */\n\t\tif (end < ex_ee_block) {\n\t\t\t/*\n\t\t\t * We're going to skip this extent and move to another,\n\t\t\t * so note that its first cluster is in use to avoid\n\t\t\t * freeing it when removing blocks.  Eventually, the\n\t\t\t * right edge of the truncated/punched region will\n\t\t\t * be just to the left.\n\t\t\t */\n\t\t\tif (sbi->s_cluster_ratio > 1) {\n\t\t\t\tpblk = ext4_ext_pblock(ex);\n\t\t\t\tpartial->pclu = EXT4_B2C(sbi, pblk);\n\t\t\t\tpartial->state = nofree;\n\t\t\t}\n\t\t\tex--;\n\t\t\tex_ee_block = le32_to_cpu(ex->ee_block);\n\t\t\tex_ee_len = ext4_ext_get_actual_len(ex);\n\t\t\tcontinue;\n\t\t} else if (b != ex_ee_block + ex_ee_len - 1) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"can not handle truncate %u:%u \"\n\t\t\t\t\t \"on extent %u:%u\",\n\t\t\t\t\t start, end, ex_ee_block,\n\t\t\t\t\t ex_ee_block + ex_ee_len - 1);\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tgoto out;\n\t\t} else if (a != ex_ee_block) {\n\t\t\t/* remove tail of the extent */\n\t\t\tnum = a - ex_ee_block;\n\t\t} else {\n\t\t\t/* remove whole extent: excellent! */\n\t\t\tnum = 0;\n\t\t}\n\t\t/*\n\t\t * 3 for leaf, sb, and inode plus 2 (bmap and group\n\t\t * descriptor) for each block group; assume two block\n\t\t * groups plus ex_ee_len/blocks_per_block_group for\n\t\t * the worst case\n\t\t */\n\t\tcredits = 7 + 2*(ex_ee_len/EXT4_BLOCKS_PER_GROUP(inode->i_sb));\n\t\tif (ex == EXT_FIRST_EXTENT(eh)) {\n\t\t\tcorrect_index = 1;\n\t\t\tcredits += (ext_depth(inode)) + 1;\n\t\t}\n\t\tcredits += EXT4_MAXQUOTAS_TRANS_BLOCKS(inode->i_sb);\n\n\t\terr = ext4_ext_truncate_extend_restart(handle, inode, credits);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_remove_blocks(handle, inode, ex, partial, a, b);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (num == 0)\n\t\t\t/* this extent is removed; mark slot entirely unused */\n\t\t\text4_ext_store_pblock(ex, 0);\n\n\t\tex->ee_len = cpu_to_le16(num);\n\t\t/*\n\t\t * Do not mark unwritten if all the blocks in the\n\t\t * extent have been removed.\n\t\t */\n\t\tif (unwritten && num)\n\t\t\text4_ext_mark_unwritten(ex);\n\t\t/*\n\t\t * If the extent was completely released,\n\t\t * we need to remove it from the leaf\n\t\t */\n\t\tif (num == 0) {\n\t\t\tif (end != EXT_MAX_BLOCKS - 1) {\n\t\t\t\t/*\n\t\t\t\t * For hole punching, we need to scoot all the\n\t\t\t\t * extents up when an extent is removed so that\n\t\t\t\t * we dont have blank extents in the middle\n\t\t\t\t */\n\t\t\t\tmemmove(ex, ex+1, (EXT_LAST_EXTENT(eh) - ex) *\n\t\t\t\t\tsizeof(struct ext4_extent));\n\n\t\t\t\t/* Now get rid of the one at the end */\n\t\t\t\tmemset(EXT_LAST_EXTENT(eh), 0,\n\t\t\t\t\tsizeof(struct ext4_extent));\n\t\t\t}\n\t\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\t}\n\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\text_debug(\"new extent: %u:%u:%llu\\n\", ex_ee_block, num,\n\t\t\t\text4_ext_pblock(ex));\n\t\tex--;\n\t\tex_ee_block = le32_to_cpu(ex->ee_block);\n\t\tex_ee_len = ext4_ext_get_actual_len(ex);\n\t}\n\n\tif (correct_index && eh->eh_entries)\n\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\n\t/*\n\t * If there's a partial cluster and at least one extent remains in\n\t * the leaf, free the partial cluster if it isn't shared with the\n\t * current extent.  If it is shared with the current extent\n\t * we reset the partial cluster because we've reached the start of the\n\t * truncated/punched region and we're done removing blocks.\n\t */\n\tif (partial->state == tofree && ex >= EXT_FIRST_EXTENT(eh)) {\n\t\tpblk = ext4_ext_pblock(ex) + ex_ee_len - 1;\n\t\tif (partial->pclu != EXT4_B2C(sbi, pblk)) {\n\t\t\tint flags = get_default_free_blocks_flags(inode);\n\n\t\t\tif (ext4_is_pending(inode, partial->lblk))\n\t\t\t\tflags |= EXT4_FREE_BLOCKS_RERESERVE_CLUSTER;\n\t\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t\t EXT4_C2B(sbi, partial->pclu),\n\t\t\t\t\t sbi->s_cluster_ratio, flags);\n\t\t\tif (flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)\n\t\t\t\text4_rereserve_cluster(inode, partial->lblk);\n\t\t}\n\t\tpartial->state = initial;\n\t}\n\n\t/* if this leaf is free, then we should\n\t * remove it from index block above */\n\tif (err == 0 && eh->eh_entries == 0 && path[depth].p_bh != NULL)\n\t\terr = ext4_ext_rm_idx(handle, inode, path, depth);\n\nout:\n\treturn err;\n}\n\n/*\n * ext4_ext_more_to_rm:\n * returns 1 if current index has to be freed (even partial)\n */\nstatic int\next4_ext_more_to_rm(struct ext4_ext_path *path)\n{\n\tBUG_ON(path->p_idx == NULL);\n\n\tif (path->p_idx < EXT_FIRST_INDEX(path->p_hdr))\n\t\treturn 0;\n\n\t/*\n\t * if truncate on deeper level happened, it wasn't partial,\n\t * so we have to consider current index for truncation\n\t */\n\tif (le16_to_cpu(path->p_hdr->eh_entries) == path->p_block)\n\t\treturn 0;\n\treturn 1;\n}\n\nint ext4_ext_remove_space(struct inode *inode, ext4_lblk_t start,\n\t\t\t  ext4_lblk_t end)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tint depth = ext_depth(inode);\n\tstruct ext4_ext_path *path = NULL;\n\tstruct partial_cluster partial;\n\thandle_t *handle;\n\tint i = 0, err = 0;\n\n\tpartial.pclu = 0;\n\tpartial.lblk = 0;\n\tpartial.state = initial;\n\n\text_debug(\"truncate since %u to %u\\n\", start, end);\n\n\t/* probably first extent we're gonna free will be last in block */\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, depth + 1);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\nagain:\n\ttrace_ext4_ext_remove_space(inode, start, end, depth);\n\n\t/*\n\t * Check if we are removing extents inside the extent tree. If that\n\t * is the case, we are going to punch a hole inside the extent tree\n\t * so we have to check whether we need to split the extent covering\n\t * the last block to remove so we can easily remove the part of it\n\t * in ext4_ext_rm_leaf().\n\t */\n\tif (end < EXT_MAX_BLOCKS - 1) {\n\t\tstruct ext4_extent *ex;\n\t\text4_lblk_t ee_block, ex_end, lblk;\n\t\text4_fsblk_t pblk;\n\n\t\t/* find extent for or closest extent to this block */\n\t\tpath = ext4_find_extent(inode, end, NULL, EXT4_EX_NOCACHE);\n\t\tif (IS_ERR(path)) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn PTR_ERR(path);\n\t\t}\n\t\tdepth = ext_depth(inode);\n\t\t/* Leaf not may not exist only if inode has no blocks at all */\n\t\tex = path[depth].p_ext;\n\t\tif (!ex) {\n\t\t\tif (depth) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t\t \"path[%d].p_hdr == NULL\",\n\t\t\t\t\t\t depth);\n\t\t\t\terr = -EFSCORRUPTED;\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\n\t\tee_block = le32_to_cpu(ex->ee_block);\n\t\tex_end = ee_block + ext4_ext_get_actual_len(ex) - 1;\n\n\t\t/*\n\t\t * See if the last block is inside the extent, if so split\n\t\t * the extent at 'end' block so we can easily remove the\n\t\t * tail of the first part of the split extent in\n\t\t * ext4_ext_rm_leaf().\n\t\t */\n\t\tif (end >= ee_block && end < ex_end) {\n\n\t\t\t/*\n\t\t\t * If we're going to split the extent, note that\n\t\t\t * the cluster containing the block after 'end' is\n\t\t\t * in use to avoid freeing it when removing blocks.\n\t\t\t */\n\t\t\tif (sbi->s_cluster_ratio > 1) {\n\t\t\t\tpblk = ext4_ext_pblock(ex) + end - ee_block + 2;\n\t\t\t\tpartial.pclu = EXT4_B2C(sbi, pblk);\n\t\t\t\tpartial.state = nofree;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Split the extent in two so that 'end' is the last\n\t\t\t * block in the first new extent. Also we should not\n\t\t\t * fail removing space due to ENOSPC so try to use\n\t\t\t * reserved block if that happens.\n\t\t\t */\n\t\t\terr = ext4_force_split_extent_at(handle, inode, &path,\n\t\t\t\t\t\t\t end + 1, 1);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t} else if (sbi->s_cluster_ratio > 1 && end >= ex_end &&\n\t\t\t   partial.state == initial) {\n\t\t\t/*\n\t\t\t * If we're punching, there's an extent to the right.\n\t\t\t * If the partial cluster hasn't been set, set it to\n\t\t\t * that extent's first cluster and its state to nofree\n\t\t\t * so it won't be freed should it contain blocks to be\n\t\t\t * removed. If it's already set (tofree/nofree), we're\n\t\t\t * retrying and keep the original partial cluster info\n\t\t\t * so a cluster marked tofree as a result of earlier\n\t\t\t * extent removal is not lost.\n\t\t\t */\n\t\t\tlblk = ex_end + 1;\n\t\t\terr = ext4_ext_search_right(inode, path, &lblk, &pblk,\n\t\t\t\t\t\t    &ex);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tif (pblk) {\n\t\t\t\tpartial.pclu = EXT4_B2C(sbi, pblk);\n\t\t\t\tpartial.state = nofree;\n\t\t\t}\n\t\t}\n\t}\n\t/*\n\t * We start scanning from right side, freeing all the blocks\n\t * after i_size and walking into the tree depth-wise.\n\t */\n\tdepth = ext_depth(inode);\n\tif (path) {\n\t\tint k = i = depth;\n\t\twhile (--k > 0)\n\t\t\tpath[k].p_block =\n\t\t\t\tle16_to_cpu(path[k].p_hdr->eh_entries)+1;\n\t} else {\n\t\tpath = kcalloc(depth + 1, sizeof(struct ext4_ext_path),\n\t\t\t       GFP_NOFS);\n\t\tif (path == NULL) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tpath[0].p_maxdepth = path[0].p_depth = depth;\n\t\tpath[0].p_hdr = ext_inode_hdr(inode);\n\t\ti = 0;\n\n\t\tif (ext4_ext_check(inode, path[0].p_hdr, depth, 0)) {\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tgoto out;\n\t\t}\n\t}\n\terr = 0;\n\n\twhile (i >= 0 && err == 0) {\n\t\tif (i == depth) {\n\t\t\t/* this is leaf block */\n\t\t\terr = ext4_ext_rm_leaf(handle, inode, path,\n\t\t\t\t\t       &partial, start, end);\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* this is index block */\n\t\tif (!path[i].p_hdr) {\n\t\t\text_debug(\"initialize header\\n\");\n\t\t\tpath[i].p_hdr = ext_block_hdr(path[i].p_bh);\n\t\t}\n\n\t\tif (!path[i].p_idx) {\n\t\t\t/* this level hasn't been touched yet */\n\t\t\tpath[i].p_idx = EXT_LAST_INDEX(path[i].p_hdr);\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries)+1;\n\t\t\text_debug(\"init index ptr: hdr 0x%p, num %d\\n\",\n\t\t\t\t  path[i].p_hdr,\n\t\t\t\t  le16_to_cpu(path[i].p_hdr->eh_entries));\n\t\t} else {\n\t\t\t/* we were already here, see at next index */\n\t\t\tpath[i].p_idx--;\n\t\t}\n\n\t\text_debug(\"level %d - index, first 0x%p, cur 0x%p\\n\",\n\t\t\t\ti, EXT_FIRST_INDEX(path[i].p_hdr),\n\t\t\t\tpath[i].p_idx);\n\t\tif (ext4_ext_more_to_rm(path + i)) {\n\t\t\tstruct buffer_head *bh;\n\t\t\t/* go to the next level */\n\t\t\text_debug(\"move to level %d (block %llu)\\n\",\n\t\t\t\t  i + 1, ext4_idx_pblock(path[i].p_idx));\n\t\t\tmemset(path + i + 1, 0, sizeof(*path));\n\t\t\tbh = read_extent_tree_block(inode,\n\t\t\t\text4_idx_pblock(path[i].p_idx), depth - i - 1,\n\t\t\t\tEXT4_EX_NOCACHE);\n\t\t\tif (IS_ERR(bh)) {\n\t\t\t\t/* should we reset i_size? */\n\t\t\t\terr = PTR_ERR(bh);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* Yield here to deal with large extent trees.\n\t\t\t * Should be a no-op if we did IO above. */\n\t\t\tcond_resched();\n\t\t\tif (WARN_ON(i + 1 > depth)) {\n\t\t\t\terr = -EFSCORRUPTED;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpath[i + 1].p_bh = bh;\n\n\t\t\t/* save actual number of indexes since this\n\t\t\t * number is changed at the next iteration */\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries);\n\t\t\ti++;\n\t\t} else {\n\t\t\t/* we finished processing this index, go up */\n\t\t\tif (path[i].p_hdr->eh_entries == 0 && i > 0) {\n\t\t\t\t/* index is empty, remove it;\n\t\t\t\t * handle must be already prepared by the\n\t\t\t\t * truncatei_leaf() */\n\t\t\t\terr = ext4_ext_rm_idx(handle, inode, path, i);\n\t\t\t}\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\text_debug(\"return to level %d\\n\", i);\n\t\t}\n\t}\n\n\ttrace_ext4_ext_remove_space_done(inode, start, end, depth, &partial,\n\t\t\t\t\t path->p_hdr->eh_entries);\n\n\t/*\n\t * if there's a partial cluster and we have removed the first extent\n\t * in the file, then we also free the partial cluster, if any\n\t */\n\tif (partial.state == tofree && err == 0) {\n\t\tint flags = get_default_free_blocks_flags(inode);\n\n\t\tif (ext4_is_pending(inode, partial.lblk))\n\t\t\tflags |= EXT4_FREE_BLOCKS_RERESERVE_CLUSTER;\n\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t EXT4_C2B(sbi, partial.pclu),\n\t\t\t\t sbi->s_cluster_ratio, flags);\n\t\tif (flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)\n\t\t\text4_rereserve_cluster(inode, partial.lblk);\n\t\tpartial.state = initial;\n\t}\n\n\t/* TODO: flexible tree reduction should be here */\n\tif (path->p_hdr->eh_entries == 0) {\n\t\t/*\n\t\t * truncate to zero freed all the tree,\n\t\t * so we need to correct eh_depth\n\t\t */\n\t\terr = ext4_ext_get_access(handle, inode, path);\n\t\tif (err == 0) {\n\t\t\text_inode_hdr(inode)->eh_depth = 0;\n\t\t\text_inode_hdr(inode)->eh_max =\n\t\t\t\tcpu_to_le16(ext4_ext_space_root(inode, 0));\n\t\t\terr = ext4_ext_dirty(handle, inode, path);\n\t\t}\n\t}\nout:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\tpath = NULL;\n\tif (err == -EAGAIN)\n\t\tgoto again;\n\text4_journal_stop(handle);\n\n\treturn err;\n}\n\n/*\n * called at mount time\n */\nvoid ext4_ext_init(struct super_block *sb)\n{\n\t/*\n\t * possible initialization would be here\n\t */\n\n\tif (ext4_has_feature_extents(sb)) {\n#if defined(AGGRESSIVE_TEST) || defined(CHECK_BINSEARCH) || defined(EXTENTS_STATS)\n\t\tprintk(KERN_INFO \"EXT4-fs: file extents enabled\"\n#ifdef AGGRESSIVE_TEST\n\t\t       \", aggressive tests\"\n#endif\n#ifdef CHECK_BINSEARCH\n\t\t       \", check binsearch\"\n#endif\n#ifdef EXTENTS_STATS\n\t\t       \", stats\"\n#endif\n\t\t       \"\\n\");\n#endif\n#ifdef EXTENTS_STATS\n\t\tspin_lock_init(&EXT4_SB(sb)->s_ext_stats_lock);\n\t\tEXT4_SB(sb)->s_ext_min = 1 << 30;\n\t\tEXT4_SB(sb)->s_ext_max = 0;\n#endif\n\t}\n}\n\n/*\n * called at umount time\n */\nvoid ext4_ext_release(struct super_block *sb)\n{\n\tif (!ext4_has_feature_extents(sb))\n\t\treturn;\n\n#ifdef EXTENTS_STATS\n\tif (EXT4_SB(sb)->s_ext_blocks && EXT4_SB(sb)->s_ext_extents) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\t\tprintk(KERN_ERR \"EXT4-fs: %lu blocks in %lu extents (%lu ave)\\n\",\n\t\t\tsbi->s_ext_blocks, sbi->s_ext_extents,\n\t\t\tsbi->s_ext_blocks / sbi->s_ext_extents);\n\t\tprintk(KERN_ERR \"EXT4-fs: extents: %lu min, %lu max, max depth %lu\\n\",\n\t\t\tsbi->s_ext_min, sbi->s_ext_max, sbi->s_depth_max);\n\t}\n#endif\n}\n\nstatic int ext4_zeroout_es(struct inode *inode, struct ext4_extent *ex)\n{\n\text4_lblk_t  ee_block;\n\text4_fsblk_t ee_pblock;\n\tunsigned int ee_len;\n\n\tee_block  = le32_to_cpu(ex->ee_block);\n\tee_len    = ext4_ext_get_actual_len(ex);\n\tee_pblock = ext4_ext_pblock(ex);\n\n\tif (ee_len == 0)\n\t\treturn 0;\n\n\treturn ext4_es_insert_extent(inode, ee_block, ee_len, ee_pblock,\n\t\t\t\t     EXTENT_STATUS_WRITTEN);\n}\n\n/* FIXME!! we need to try to merge to left or right after zero-out  */\nstatic int ext4_ext_zeroout(struct inode *inode, struct ext4_extent *ex)\n{\n\text4_fsblk_t ee_pblock;\n\tunsigned int ee_len;\n\n\tee_len    = ext4_ext_get_actual_len(ex);\n\tee_pblock = ext4_ext_pblock(ex);\n\treturn ext4_issue_zeroout(inode, le32_to_cpu(ex->ee_block), ee_pblock,\n\t\t\t\t  ee_len);\n}\n\n/*\n * ext4_split_extent_at() splits an extent at given block.\n *\n * @handle: the journal handle\n * @inode: the file inode\n * @path: the path to the extent\n * @split: the logical block where the extent is splitted.\n * @split_flags: indicates if the extent could be zeroout if split fails, and\n *\t\t the states(init or unwritten) of new extents.\n * @flags: flags used to insert new extent to extent tree.\n *\n *\n * Splits extent [a, b] into two extents [a, @split) and [@split, b], states\n * of which are deterimined by split_flag.\n *\n * There are two cases:\n *  a> the extent are splitted into two extent.\n *  b> split is not needed, and just mark the extent.\n *\n * return 0 on success.\n */\nstatic int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path **ppath,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\text4_fsblk_t newblock;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex, newex, orig_ex, zero_ex;\n\tstruct ext4_extent *ex2 = NULL;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\n\tBUG_ON((split_flag & (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2)) ==\n\t       (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2));\n\n\text_debug(\"ext4_split_extents_at: inode %lu, logical\"\n\t\t\"block %llu\\n\", inode->i_ino, (unsigned long long)split);\n\n\text4_ext_show_leaf(inode, path);\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tnewblock = split - ee_block + ext4_ext_pblock(ex);\n\n\tBUG_ON(split < ee_block || split >= (ee_block + ee_len));\n\tBUG_ON(!ext4_ext_is_unwritten(ex) &&\n\t       split_flag & (EXT4_EXT_MAY_ZEROOUT |\n\t\t\t     EXT4_EXT_MARK_UNWRIT1 |\n\t\t\t     EXT4_EXT_MARK_UNWRIT2));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\n\tif (split == ee_block) {\n\t\t/*\n\t\t * case b: block @split is the block that the extent begins with\n\t\t * then we just change the state of the extent, and splitting\n\t\t * is not needed.\n\t\t */\n\t\tif (split_flag & EXT4_EXT_MARK_UNWRIT2)\n\t\t\text4_ext_mark_unwritten(ex);\n\t\telse\n\t\t\text4_ext_mark_initialized(ex);\n\n\t\tif (!(flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t}\n\n\t/* case a */\n\tmemcpy(&orig_ex, ex, sizeof(orig_ex));\n\tex->ee_len = cpu_to_le16(split - ee_block);\n\tif (split_flag & EXT4_EXT_MARK_UNWRIT1)\n\t\text4_ext_mark_unwritten(ex);\n\n\t/*\n\t * path may lead to new leaf, not to original leaf any more\n\t * after ext4_ext_insert_extent() returns,\n\t */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\tif (err)\n\t\tgoto fix_extent_len;\n\n\tex2 = &newex;\n\tex2->ee_block = cpu_to_le32(split);\n\tex2->ee_len   = cpu_to_le16(ee_len - (split - ee_block));\n\text4_ext_store_pblock(ex2, newblock);\n\tif (split_flag & EXT4_EXT_MARK_UNWRIT2)\n\t\text4_ext_mark_unwritten(ex2);\n\n\terr = ext4_ext_insert_extent(handle, inode, ppath, &newex, flags);\n\tif (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n\t\tif (split_flag & (EXT4_EXT_DATA_VALID1|EXT4_EXT_DATA_VALID2)) {\n\t\t\tif (split_flag & EXT4_EXT_DATA_VALID1) {\n\t\t\t\terr = ext4_ext_zeroout(inode, ex2);\n\t\t\t\tzero_ex.ee_block = ex2->ee_block;\n\t\t\t\tzero_ex.ee_len = cpu_to_le16(\n\t\t\t\t\t\text4_ext_get_actual_len(ex2));\n\t\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\t\t\t      ext4_ext_pblock(ex2));\n\t\t\t} else {\n\t\t\t\terr = ext4_ext_zeroout(inode, ex);\n\t\t\t\tzero_ex.ee_block = ex->ee_block;\n\t\t\t\tzero_ex.ee_len = cpu_to_le16(\n\t\t\t\t\t\text4_ext_get_actual_len(ex));\n\t\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\t\t\t      ext4_ext_pblock(ex));\n\t\t\t}\n\t\t} else {\n\t\t\terr = ext4_ext_zeroout(inode, &orig_ex);\n\t\t\tzero_ex.ee_block = orig_ex.ee_block;\n\t\t\tzero_ex.ee_len = cpu_to_le16(\n\t\t\t\t\t\text4_ext_get_actual_len(&orig_ex));\n\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\t\t      ext4_ext_pblock(&orig_ex));\n\t\t}\n\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_len = cpu_to_le16(ee_len);\n\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\n\t\t/* update extent status tree */\n\t\terr = ext4_zeroout_es(inode, &zero_ex);\n\n\t\tgoto out;\n\t} else if (err)\n\t\tgoto fix_extent_len;\n\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n\nfix_extent_len:\n\tex->ee_len = orig_ex.ee_len;\n\text4_ext_dirty(handle, inode, path + path->p_depth);\n\treturn err;\n}\n\n/*\n * ext4_split_extents() splits an extent and mark extent which is covered\n * by @map as split_flags indicates\n *\n * It may result in splitting the extent into multiple extents (up to three)\n * There are three possibilities:\n *   a> There is no split required\n *   b> Splits in two extents: Split is happening at either end of the extent\n *   c> Splits in three extents: Somone is splitting in middle of the extent\n *\n */\nstatic int ext4_split_extent(handle_t *handle,\n\t\t\t      struct inode *inode,\n\t\t\t      struct ext4_ext_path **ppath,\n\t\t\t      struct ext4_map_blocks *map,\n\t\t\t      int split_flag,\n\t\t\t      int flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\tint unwritten;\n\tint split_flag1, flags1;\n\tint allocated = map->m_len;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tunwritten = ext4_ext_is_unwritten(ex);\n\n\tif (map->m_lblk + map->m_len < ee_block + ee_len) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT;\n\t\tflags1 = flags | EXT4_GET_BLOCKS_PRE_IO;\n\t\tif (unwritten)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNWRIT1 |\n\t\t\t\t       EXT4_EXT_MARK_UNWRIT2;\n\t\tif (split_flag & EXT4_EXT_DATA_VALID2)\n\t\t\tsplit_flag1 |= EXT4_EXT_DATA_VALID1;\n\t\terr = ext4_split_extent_at(handle, inode, ppath,\n\t\t\t\tmap->m_lblk + map->m_len, split_flag1, flags1);\n\t\tif (err)\n\t\t\tgoto out;\n\t} else {\n\t\tallocated = ee_len - (map->m_lblk - ee_block);\n\t}\n\t/*\n\t * Update path is required because previous ext4_split_extent_at() may\n\t * result in split of original leaf or extent zeroout.\n\t */\n\tpath = ext4_find_extent(inode, map->m_lblk, ppath, 0);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tif (!ex) {\n\t\tEXT4_ERROR_INODE(inode, \"unexpected hole at %lu\",\n\t\t\t\t (unsigned long) map->m_lblk);\n\t\treturn -EFSCORRUPTED;\n\t}\n\tunwritten = ext4_ext_is_unwritten(ex);\n\tsplit_flag1 = 0;\n\n\tif (map->m_lblk >= ee_block) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_DATA_VALID2;\n\t\tif (unwritten) {\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNWRIT1;\n\t\t\tsplit_flag1 |= split_flag & (EXT4_EXT_MAY_ZEROOUT |\n\t\t\t\t\t\t     EXT4_EXT_MARK_UNWRIT2);\n\t\t}\n\t\terr = ext4_split_extent_at(handle, inode, ppath,\n\t\t\t\tmap->m_lblk, split_flag1, flags);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\text4_ext_show_leaf(inode, path);\nout:\n\treturn err ? err : allocated;\n}\n\n/*\n * This function is called by ext4_ext_map_blocks() if someone tries to write\n * to an unwritten extent. It may result in splitting the unwritten\n * extent into multiple extents (up to three - one initialized and two\n * unwritten).\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be initialized\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n *\n * Pre-conditions:\n *  - The extent pointed to by 'path' is unwritten.\n *  - The extent pointed to by 'path' contains a superset\n *    of the logical span [map->m_lblk, map->m_lblk + map->m_len).\n *\n * Post-conditions on success:\n *  - the returned value is the number of blocks beyond map->l_lblk\n *    that are allocated and initialized.\n *    It is guaranteed to be >= map->m_len.\n */\nstatic int ext4_ext_convert_to_initialized(handle_t *handle,\n\t\t\t\t\t   struct inode *inode,\n\t\t\t\t\t   struct ext4_map_blocks *map,\n\t\t\t\t\t   struct ext4_ext_path **ppath,\n\t\t\t\t\t   int flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_sb_info *sbi;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_map_blocks split_map;\n\tstruct ext4_extent zero_ex1, zero_ex2;\n\tstruct ext4_extent *ex, *abut_ex;\n\text4_lblk_t ee_block, eof_block;\n\tunsigned int ee_len, depth, map_len = map->m_len;\n\tint allocated = 0, max_zeroout = 0;\n\tint err = 0;\n\tint split_flag = EXT4_EXT_DATA_VALID2;\n\n\text_debug(\"ext4_ext_convert_to_initialized: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)map->m_lblk, map_len);\n\n\tsbi = EXT4_SB(inode->i_sb);\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map_len)\n\t\teof_block = map->m_lblk + map_len;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tzero_ex1.ee_len = 0;\n\tzero_ex2.ee_len = 0;\n\n\ttrace_ext4_ext_convert_to_initialized_enter(inode, map, ex);\n\n\t/* Pre-conditions */\n\tBUG_ON(!ext4_ext_is_unwritten(ex));\n\tBUG_ON(!in_range(map->m_lblk, ee_block, ee_len));\n\n\t/*\n\t * Attempt to transfer newly initialized blocks from the currently\n\t * unwritten extent to its neighbor. This is much cheaper\n\t * than an insertion followed by a merge as those involve costly\n\t * memmove() calls. Transferring to the left is the common case in\n\t * steady state for workloads doing fallocate(FALLOC_FL_KEEP_SIZE)\n\t * followed by append writes.\n\t *\n\t * Limitations of the current logic:\n\t *  - L1: we do not deal with writes covering the whole extent.\n\t *    This would require removing the extent if the transfer\n\t *    is possible.\n\t *  - L2: we only attempt to merge with an extent stored in the\n\t *    same extent tree node.\n\t */\n\tif ((map->m_lblk == ee_block) &&\n\t\t/* See if we can merge left */\n\t\t(map_len < ee_len) &&\t\t/*L1*/\n\t\t(ex > EXT_FIRST_EXTENT(eh))) {\t/*L2*/\n\t\text4_lblk_t prev_lblk;\n\t\text4_fsblk_t prev_pblk, ee_pblk;\n\t\tunsigned int prev_len;\n\n\t\tabut_ex = ex - 1;\n\t\tprev_lblk = le32_to_cpu(abut_ex->ee_block);\n\t\tprev_len = ext4_ext_get_actual_len(abut_ex);\n\t\tprev_pblk = ext4_ext_pblock(abut_ex);\n\t\tee_pblk = ext4_ext_pblock(ex);\n\n\t\t/*\n\t\t * A transfer of blocks from 'ex' to 'abut_ex' is allowed\n\t\t * upon those conditions:\n\t\t * - C1: abut_ex is initialized,\n\t\t * - C2: abut_ex is logically abutting ex,\n\t\t * - C3: abut_ex is physically abutting ex,\n\t\t * - C4: abut_ex can receive the additional blocks without\n\t\t *   overflowing the (initialized) length limit.\n\t\t */\n\t\tif ((!ext4_ext_is_unwritten(abut_ex)) &&\t\t/*C1*/\n\t\t\t((prev_lblk + prev_len) == ee_block) &&\t\t/*C2*/\n\t\t\t((prev_pblk + prev_len) == ee_pblk) &&\t\t/*C3*/\n\t\t\t(prev_len < (EXT_INIT_MAX_LEN - map_len))) {\t/*C4*/\n\t\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\ttrace_ext4_ext_convert_to_initialized_fastpath(inode,\n\t\t\t\tmap, ex, abut_ex);\n\n\t\t\t/* Shift the start of ex by 'map_len' blocks */\n\t\t\tex->ee_block = cpu_to_le32(ee_block + map_len);\n\t\t\text4_ext_store_pblock(ex, ee_pblk + map_len);\n\t\t\tex->ee_len = cpu_to_le16(ee_len - map_len);\n\t\t\text4_ext_mark_unwritten(ex); /* Restore the flag */\n\n\t\t\t/* Extend abut_ex by 'map_len' blocks */\n\t\t\tabut_ex->ee_len = cpu_to_le16(prev_len + map_len);\n\n\t\t\t/* Result: number of initialized blocks past m_lblk */\n\t\t\tallocated = map_len;\n\t\t}\n\t} else if (((map->m_lblk + map_len) == (ee_block + ee_len)) &&\n\t\t   (map_len < ee_len) &&\t/*L1*/\n\t\t   ex < EXT_LAST_EXTENT(eh)) {\t/*L2*/\n\t\t/* See if we can merge right */\n\t\text4_lblk_t next_lblk;\n\t\text4_fsblk_t next_pblk, ee_pblk;\n\t\tunsigned int next_len;\n\n\t\tabut_ex = ex + 1;\n\t\tnext_lblk = le32_to_cpu(abut_ex->ee_block);\n\t\tnext_len = ext4_ext_get_actual_len(abut_ex);\n\t\tnext_pblk = ext4_ext_pblock(abut_ex);\n\t\tee_pblk = ext4_ext_pblock(ex);\n\n\t\t/*\n\t\t * A transfer of blocks from 'ex' to 'abut_ex' is allowed\n\t\t * upon those conditions:\n\t\t * - C1: abut_ex is initialized,\n\t\t * - C2: abut_ex is logically abutting ex,\n\t\t * - C3: abut_ex is physically abutting ex,\n\t\t * - C4: abut_ex can receive the additional blocks without\n\t\t *   overflowing the (initialized) length limit.\n\t\t */\n\t\tif ((!ext4_ext_is_unwritten(abut_ex)) &&\t\t/*C1*/\n\t\t    ((map->m_lblk + map_len) == next_lblk) &&\t\t/*C2*/\n\t\t    ((ee_pblk + ee_len) == next_pblk) &&\t\t/*C3*/\n\t\t    (next_len < (EXT_INIT_MAX_LEN - map_len))) {\t/*C4*/\n\t\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\ttrace_ext4_ext_convert_to_initialized_fastpath(inode,\n\t\t\t\tmap, ex, abut_ex);\n\n\t\t\t/* Shift the start of abut_ex by 'map_len' blocks */\n\t\t\tabut_ex->ee_block = cpu_to_le32(next_lblk - map_len);\n\t\t\text4_ext_store_pblock(abut_ex, next_pblk - map_len);\n\t\t\tex->ee_len = cpu_to_le16(ee_len - map_len);\n\t\t\text4_ext_mark_unwritten(ex); /* Restore the flag */\n\n\t\t\t/* Extend abut_ex by 'map_len' blocks */\n\t\t\tabut_ex->ee_len = cpu_to_le16(next_len + map_len);\n\n\t\t\t/* Result: number of initialized blocks past m_lblk */\n\t\t\tallocated = map_len;\n\t\t}\n\t}\n\tif (allocated) {\n\t\t/* Mark the block containing both extents as dirty */\n\t\text4_ext_dirty(handle, inode, path + depth);\n\n\t\t/* Update path to point to the right extent */\n\t\tpath[depth].p_ext = abut_ex;\n\t\tgoto out;\n\t} else\n\t\tallocated = ee_len - (map->m_lblk - ee_block);\n\n\tWARN_ON(map->m_lblk < ee_block);\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully inside i_size or new_size.\n\t */\n\tsplit_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;\n\n\tif (EXT4_EXT_MAY_ZEROOUT & split_flag)\n\t\tmax_zeroout = sbi->s_extent_max_zeroout_kb >>\n\t\t\t(inode->i_sb->s_blocksize_bits - 10);\n\n\tif (IS_ENCRYPTED(inode))\n\t\tmax_zeroout = 0;\n\n\t/*\n\t * five cases:\n\t * 1. split the extent into three extents.\n\t * 2. split the extent into two extents, zeroout the head of the first\n\t *    extent.\n\t * 3. split the extent into two extents, zeroout the tail of the second\n\t *    extent.\n\t * 4. split the extent into two extents with out zeroout.\n\t * 5. no splitting needed, just possibly zeroout the head and / or the\n\t *    tail of the extent.\n\t */\n\tsplit_map.m_lblk = map->m_lblk;\n\tsplit_map.m_len = map->m_len;\n\n\tif (max_zeroout && (allocated > split_map.m_len)) {\n\t\tif (allocated <= max_zeroout) {\n\t\t\t/* case 3 or 5 */\n\t\t\tzero_ex1.ee_block =\n\t\t\t\t cpu_to_le32(split_map.m_lblk +\n\t\t\t\t\t     split_map.m_len);\n\t\t\tzero_ex1.ee_len =\n\t\t\t\tcpu_to_le16(allocated - split_map.m_len);\n\t\t\text4_ext_store_pblock(&zero_ex1,\n\t\t\t\text4_ext_pblock(ex) + split_map.m_lblk +\n\t\t\t\tsplit_map.m_len - ee_block);\n\t\t\terr = ext4_ext_zeroout(inode, &zero_ex1);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tsplit_map.m_len = allocated;\n\t\t}\n\t\tif (split_map.m_lblk - ee_block + split_map.m_len <\n\t\t\t\t\t\t\t\tmax_zeroout) {\n\t\t\t/* case 2 or 5 */\n\t\t\tif (split_map.m_lblk != ee_block) {\n\t\t\t\tzero_ex2.ee_block = ex->ee_block;\n\t\t\t\tzero_ex2.ee_len = cpu_to_le16(split_map.m_lblk -\n\t\t\t\t\t\t\tee_block);\n\t\t\t\text4_ext_store_pblock(&zero_ex2,\n\t\t\t\t\t\t      ext4_ext_pblock(ex));\n\t\t\t\terr = ext4_ext_zeroout(inode, &zero_ex2);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tsplit_map.m_len += split_map.m_lblk - ee_block;\n\t\t\tsplit_map.m_lblk = ee_block;\n\t\t\tallocated = map->m_len;\n\t\t}\n\t}\n\n\terr = ext4_split_extent(handle, inode, ppath, &split_map, split_flag,\n\t\t\t\tflags);\n\tif (err > 0)\n\t\terr = 0;\nout:\n\t/* If we have gotten a failure, don't zero out status tree */\n\tif (!err) {\n\t\terr = ext4_zeroout_es(inode, &zero_ex1);\n\t\tif (!err)\n\t\t\terr = ext4_zeroout_es(inode, &zero_ex2);\n\t}\n\treturn err ? err : allocated;\n}\n\n/*\n * This function is called by ext4_ext_map_blocks() from\n * ext4_get_blocks_dio_write() when DIO to write\n * to an unwritten extent.\n *\n * Writing to an unwritten extent may result in splitting the unwritten\n * extent into multiple initialized/unwritten extents (up to three)\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be unwritten\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n *\n * This works the same way in the case of initialized -> unwritten conversion.\n *\n * One of more index blocks maybe needed if the extent tree grow after\n * the unwritten extent split. To prevent ENOSPC occur at the IO\n * complete, we need to split the unwritten extent before DIO submit\n * the IO. The unwritten extent called at this time will be split\n * into three unwritten extent(at most). After IO complete, the part\n * being filled will be convert to initialized by the end_io callback function\n * via ext4_convert_unwritten_extents().\n *\n * Returns the size of unwritten extent to be written on success.\n */\nstatic int ext4_split_convert_extents(handle_t *handle,\n\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\t\tstruct ext4_ext_path **ppath,\n\t\t\t\t\tint flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\text4_lblk_t eof_block;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len;\n\tint split_flag = 0, depth;\n\n\text_debug(\"%s: inode %lu, logical block %llu, max_blocks %u\\n\",\n\t\t  __func__, inode->i_ino,\n\t\t  (unsigned long long)map->m_lblk, map->m_len);\n\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map->m_len)\n\t\teof_block = map->m_lblk + map->m_len;\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully insde i_size or new_size.\n\t */\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\t/* Convert to unwritten */\n\tif (flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN) {\n\t\tsplit_flag |= EXT4_EXT_DATA_VALID1;\n\t/* Convert to initialized */\n\t} else if (flags & EXT4_GET_BLOCKS_CONVERT) {\n\t\tsplit_flag |= ee_block + ee_len <= eof_block ?\n\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;\n\t\tsplit_flag |= (EXT4_EXT_MARK_UNWRIT2 | EXT4_EXT_DATA_VALID2);\n\t}\n\tflags |= EXT4_GET_BLOCKS_PRE_IO;\n\treturn ext4_split_extent(handle, inode, ppath, map, split_flag, flags);\n}\n\nstatic int ext4_convert_unwritten_extents_endio(handle_t *handle,\n\t\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\t\t\tstruct ext4_ext_path **ppath)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_extent *ex;\n\text4_lblk_t ee_block;\n\tunsigned int ee_len;\n\tint depth;\n\tint err = 0;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\text_debug(\"ext4_convert_unwritten_extents_endio: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t  (unsigned long long)ee_block, ee_len);\n\n\t/* If extent is larger than requested it is a clear sign that we still\n\t * have some extent state machine issues left. So extent_split is still\n\t * required.\n\t * TODO: Once all related issues will be fixed this situation should be\n\t * illegal.\n\t */\n\tif (ee_block != map->m_lblk || ee_len > map->m_len) {\n#ifdef EXT4_DEBUG\n\t\text4_warning(\"Inode (%ld) finished: extent logical block %llu,\"\n\t\t\t     \" len %u; IO logical block %llu, len %u\",\n\t\t\t     inode->i_ino, (unsigned long long)ee_block, ee_len,\n\t\t\t     (unsigned long long)map->m_lblk, map->m_len);\n#endif\n\t\terr = ext4_split_convert_extents(handle, inode, map, ppath,\n\t\t\t\t\t\t EXT4_GET_BLOCKS_CONVERT);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tpath = ext4_find_extent(inode, map->m_lblk, ppath, 0);\n\t\tif (IS_ERR(path))\n\t\t\treturn PTR_ERR(path);\n\t\tdepth = ext_depth(inode);\n\t\tex = path[depth].p_ext;\n\t}\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\t/* first mark the extent as initialized */\n\text4_ext_mark_initialized(ex);\n\n\t/* note: ext4_ext_correct_indexes() isn't needed here because\n\t * borders are not changed\n\t */\n\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n}\n\n/*\n * Handle EOFBLOCKS_FL flag, clearing it if necessary\n */\nstatic int check_eofblocks_fl(handle_t *handle, struct inode *inode,\n\t\t\t      ext4_lblk_t lblk,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      unsigned int len)\n{\n\tint i, depth;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *last_ex;\n\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EOFBLOCKS))\n\t\treturn 0;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\n\t/*\n\t * We're going to remove EOFBLOCKS_FL entirely in future so we\n\t * do not care for this case anymore. Simply remove the flag\n\t * if there are no extents.\n\t */\n\tif (unlikely(!eh->eh_entries))\n\t\tgoto out;\n\tlast_ex = EXT_LAST_EXTENT(eh);\n\t/*\n\t * We should clear the EOFBLOCKS_FL flag if we are writing the\n\t * last block in the last extent in the file.  We test this by\n\t * first checking to see if the caller to\n\t * ext4_ext_get_blocks() was interested in the last block (or\n\t * a block beyond the last block) in the current extent.  If\n\t * this turns out to be false, we can bail out from this\n\t * function immediately.\n\t */\n\tif (lblk + len < le32_to_cpu(last_ex->ee_block) +\n\t    ext4_ext_get_actual_len(last_ex))\n\t\treturn 0;\n\t/*\n\t * If the caller does appear to be planning to write at or\n\t * beyond the end of the current extent, we then test to see\n\t * if the current extent is the last extent in the file, by\n\t * checking to make sure it was reached via the rightmost node\n\t * at each level of the tree.\n\t */\n\tfor (i = depth-1; i >= 0; i--)\n\t\tif (path[i].p_idx != EXT_LAST_INDEX(path[i].p_hdr))\n\t\t\treturn 0;\nout:\n\text4_clear_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\treturn ext4_mark_inode_dirty(handle, inode);\n}\n\nstatic int\nconvert_initialized_extent(handle_t *handle, struct inode *inode,\n\t\t\t   struct ext4_map_blocks *map,\n\t\t\t   struct ext4_ext_path **ppath,\n\t\t\t   unsigned int allocated)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_extent *ex;\n\text4_lblk_t ee_block;\n\tunsigned int ee_len;\n\tint depth;\n\tint err = 0;\n\n\t/*\n\t * Make sure that the extent is no bigger than we support with\n\t * unwritten extent\n\t */\n\tif (map->m_len > EXT_UNWRITTEN_MAX_LEN)\n\t\tmap->m_len = EXT_UNWRITTEN_MAX_LEN / 2;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\text_debug(\"%s: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", __func__, inode->i_ino,\n\t\t  (unsigned long long)ee_block, ee_len);\n\n\tif (ee_block != map->m_lblk || ee_len > map->m_len) {\n\t\terr = ext4_split_convert_extents(handle, inode, map, ppath,\n\t\t\t\tEXT4_GET_BLOCKS_CONVERT_UNWRITTEN);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tpath = ext4_find_extent(inode, map->m_lblk, ppath, 0);\n\t\tif (IS_ERR(path))\n\t\t\treturn PTR_ERR(path);\n\t\tdepth = ext_depth(inode);\n\t\tex = path[depth].p_ext;\n\t\tif (!ex) {\n\t\t\tEXT4_ERROR_INODE(inode, \"unexpected hole at %lu\",\n\t\t\t\t\t (unsigned long) map->m_lblk);\n\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t}\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\treturn err;\n\t/* first mark the extent as unwritten */\n\text4_ext_mark_unwritten(ex);\n\n\t/* note: ext4_ext_correct_indexes() isn't needed here because\n\t * borders are not changed\n\t */\n\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\tif (err)\n\t\treturn err;\n\text4_ext_show_leaf(inode, path);\n\n\text4_update_inode_fsync_trans(handle, inode, 1);\n\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path, map->m_len);\n\tif (err)\n\t\treturn err;\n\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\tmap->m_len = allocated;\n\treturn allocated;\n}\n\nstatic int\next4_ext_handle_unwritten_extents(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map,\n\t\t\tstruct ext4_ext_path **ppath, int flags,\n\t\t\tunsigned int allocated, ext4_fsblk_t newblock)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tint ret = 0;\n\tint err = 0;\n\n\text_debug(\"ext4_ext_handle_unwritten_extents: inode %lu, logical \"\n\t\t  \"block %llu, max_blocks %u, flags %x, allocated %u\\n\",\n\t\t  inode->i_ino, (unsigned long long)map->m_lblk, map->m_len,\n\t\t  flags, allocated);\n\text4_ext_show_leaf(inode, path);\n\n\t/*\n\t * When writing into unwritten space, we should not fail to\n\t * allocate metadata blocks for the new extent block if needed.\n\t */\n\tflags |= EXT4_GET_BLOCKS_METADATA_NOFAIL;\n\n\ttrace_ext4_ext_handle_unwritten_extents(inode, map, flags,\n\t\t\t\t\t\t    allocated, newblock);\n\n\t/* get_block() before submit the IO, split the extent */\n\tif (flags & EXT4_GET_BLOCKS_PRE_IO) {\n\t\tret = ext4_split_convert_extents(handle, inode, map, ppath,\n\t\t\t\t\t flags | EXT4_GET_BLOCKS_CONVERT);\n\t\tif (ret <= 0)\n\t\t\tgoto out;\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto out;\n\t}\n\t/* IO end_io complete, convert the filled extent to written */\n\tif (flags & EXT4_GET_BLOCKS_CONVERT) {\n\t\tif (flags & EXT4_GET_BLOCKS_ZERO) {\n\t\t\tif (allocated > map->m_len)\n\t\t\t\tallocated = map->m_len;\n\t\t\terr = ext4_issue_zeroout(inode, map->m_lblk, newblock,\n\t\t\t\t\t\t allocated);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out2;\n\t\t}\n\t\tret = ext4_convert_unwritten_extents_endio(handle, inode, map,\n\t\t\t\t\t\t\t   ppath);\n\t\tif (ret >= 0) {\n\t\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t\t path, map->m_len);\n\t\t} else\n\t\t\terr = ret;\n\t\tmap->m_flags |= EXT4_MAP_MAPPED;\n\t\tmap->m_pblk = newblock;\n\t\tif (allocated > map->m_len)\n\t\t\tallocated = map->m_len;\n\t\tmap->m_len = allocated;\n\t\tgoto out2;\n\t}\n\t/* buffered IO case */\n\t/*\n\t * repeat fallocate creation request\n\t * we already have an unwritten extent\n\t */\n\tif (flags & EXT4_GET_BLOCKS_UNWRIT_EXT) {\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto map_out;\n\t}\n\n\t/* buffered READ or buffered write_begin() lookup */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * We have blocks reserved already.  We\n\t\t * return allocated blocks so that delalloc\n\t\t * won't do block reservation for us.  But\n\t\t * the buffer head will be unmapped so that\n\t\t * a read from the block returns 0s.\n\t\t */\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto out1;\n\t}\n\n\t/* buffered write, writepage time, convert*/\n\tret = ext4_ext_convert_to_initialized(handle, inode, map, ppath, flags);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\nout:\n\tif (ret <= 0) {\n\t\terr = ret;\n\t\tgoto out2;\n\t} else\n\t\tallocated = ret;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\tmap->m_len = allocated;\n\nmap_out:\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tif ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0) {\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path,\n\t\t\t\t\t map->m_len);\n\t\tif (err < 0)\n\t\t\tgoto out2;\n\t}\nout1:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\treturn err ? err : allocated;\n}\n\n/*\n * get_implied_cluster_alloc - check to see if the requested\n * allocation (in the map structure) overlaps with a cluster already\n * allocated in an extent.\n *\t@sb\tThe filesystem superblock structure\n *\t@map\tThe requested lblk->pblk mapping\n *\t@ex\tThe extent structure which might contain an implied\n *\t\t\tcluster allocation\n *\n * This function is called by ext4_ext_map_blocks() after we failed to\n * find blocks that were already in the inode's extent tree.  Hence,\n * we know that the beginning of the requested region cannot overlap\n * the extent from the inode's extent tree.  There are three cases we\n * want to catch.  The first is this case:\n *\n *\t\t |--- cluster # N--|\n *    |--- extent ---|\t|---- requested region ---|\n *\t\t\t|==========|\n *\n * The second case that we need to test for is this one:\n *\n *   |--------- cluster # N ----------------|\n *\t   |--- requested region --|   |------- extent ----|\n *\t   |=======================|\n *\n * The third case is when the requested region lies between two extents\n * within the same cluster:\n *          |------------- cluster # N-------------|\n * |----- ex -----|                  |---- ex_right ----|\n *                  |------ requested region ------|\n *                  |================|\n *\n * In each of the above cases, we need to set the map->m_pblk and\n * map->m_len so it corresponds to the return the extent labelled as\n * \"|====|\" from cluster #N, since it is already in use for data in\n * cluster EXT4_B2C(sbi, map->m_lblk).\tWe will then return 1 to\n * signal to ext4_ext_map_blocks() that map->m_pblk should be treated\n * as a new \"allocated\" block region.  Otherwise, we will return 0 and\n * ext4_ext_map_blocks() will then allocate one or more new clusters\n * by calling ext4_mb_new_blocks().\n */\nstatic int get_implied_cluster_alloc(struct super_block *sb,\n\t\t\t\t     struct ext4_map_blocks *map,\n\t\t\t\t     struct ext4_extent *ex,\n\t\t\t\t     struct ext4_ext_path *path)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_lblk_t c_offset = EXT4_LBLK_COFF(sbi, map->m_lblk);\n\text4_lblk_t ex_cluster_start, ex_cluster_end;\n\text4_lblk_t rr_cluster_start;\n\text4_lblk_t ee_block = le32_to_cpu(ex->ee_block);\n\text4_fsblk_t ee_start = ext4_ext_pblock(ex);\n\tunsigned short ee_len = ext4_ext_get_actual_len(ex);\n\n\t/* The extent passed in that we are trying to match */\n\tex_cluster_start = EXT4_B2C(sbi, ee_block);\n\tex_cluster_end = EXT4_B2C(sbi, ee_block + ee_len - 1);\n\n\t/* The requested region passed into ext4_map_blocks() */\n\trr_cluster_start = EXT4_B2C(sbi, map->m_lblk);\n\n\tif ((rr_cluster_start == ex_cluster_end) ||\n\t    (rr_cluster_start == ex_cluster_start)) {\n\t\tif (rr_cluster_start == ex_cluster_end)\n\t\t\tee_start += ee_len - 1;\n\t\tmap->m_pblk = EXT4_PBLK_CMASK(sbi, ee_start) + c_offset;\n\t\tmap->m_len = min(map->m_len,\n\t\t\t\t (unsigned) sbi->s_cluster_ratio - c_offset);\n\t\t/*\n\t\t * Check for and handle this case:\n\t\t *\n\t\t *   |--------- cluster # N-------------|\n\t\t *\t\t       |------- extent ----|\n\t\t *\t   |--- requested region ---|\n\t\t *\t   |===========|\n\t\t */\n\n\t\tif (map->m_lblk < ee_block)\n\t\t\tmap->m_len = min(map->m_len, ee_block - map->m_lblk);\n\n\t\t/*\n\t\t * Check for the case where there is already another allocated\n\t\t * block to the right of 'ex' but before the end of the cluster.\n\t\t *\n\t\t *          |------------- cluster # N-------------|\n\t\t * |----- ex -----|                  |---- ex_right ----|\n\t\t *                  |------ requested region ------|\n\t\t *                  |================|\n\t\t */\n\t\tif (map->m_lblk > ee_block) {\n\t\t\text4_lblk_t next = ext4_ext_next_allocated_block(path);\n\t\t\tmap->m_len = min(map->m_len, next - map->m_lblk);\n\t\t}\n\n\t\ttrace_ext4_get_implied_cluster_alloc_exit(sb, map, 1);\n\t\treturn 1;\n\t}\n\n\ttrace_ext4_get_implied_cluster_alloc_exit(sb, map, 0);\n\treturn 0;\n}\n\n\n/*\n * Block allocation/map/preallocation routine for extents based files\n *\n *\n * Need to be called with\n * down_read(&EXT4_I(inode)->i_data_sem) if not allocating file system block\n * (ie, create is zero). Otherwise down_write(&EXT4_I(inode)->i_data_sem)\n *\n * return > 0, number of of blocks already mapped/allocated\n *          if create == 0 and these are pre-allocated blocks\n *          \tbuffer head is unmapped\n *          otherwise blocks are mapped\n *\n * return = 0, if plain look up failed (blocks have not been allocated)\n *          buffer head is unmapped\n *\n * return < 0, error case.\n */\nint ext4_ext_map_blocks(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map, int flags)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_extent newex, *ex, *ex2;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_fsblk_t newblock = 0;\n\tint free_on_err = 0, err = 0, depth, ret;\n\tunsigned int allocated = 0, offset = 0;\n\tunsigned int allocated_clusters = 0;\n\tstruct ext4_allocation_request ar;\n\text4_lblk_t cluster_offset;\n\tbool map_from_cluster = false;\n\n\text_debug(\"blocks %u/%u requested for inode %lu\\n\",\n\t\t  map->m_lblk, map->m_len, inode->i_ino);\n\ttrace_ext4_ext_map_blocks_enter(inode, map->m_lblk, map->m_len, flags);\n\n\t/* find extent for this block */\n\tpath = ext4_find_extent(inode, map->m_lblk, NULL, 0);\n\tif (IS_ERR(path)) {\n\t\terr = PTR_ERR(path);\n\t\tpath = NULL;\n\t\tgoto out2;\n\t}\n\n\tdepth = ext_depth(inode);\n\n\t/*\n\t * consistent leaf must not be empty;\n\t * this situation is possible, though, _during_ tree modification;\n\t * this is why assert can't be put in ext4_find_extent()\n\t */\n\tif (unlikely(path[depth].p_ext == NULL && depth != 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"bad extent address \"\n\t\t\t\t \"lblock: %lu, depth: %d pblock %lld\",\n\t\t\t\t (unsigned long) map->m_lblk, depth,\n\t\t\t\t path[depth].p_block);\n\t\terr = -EFSCORRUPTED;\n\t\tgoto out2;\n\t}\n\n\tex = path[depth].p_ext;\n\tif (ex) {\n\t\text4_lblk_t ee_block = le32_to_cpu(ex->ee_block);\n\t\text4_fsblk_t ee_start = ext4_ext_pblock(ex);\n\t\tunsigned short ee_len;\n\n\n\t\t/*\n\t\t * unwritten extents are treated as holes, except that\n\t\t * we split out initialized portions during a write.\n\t\t */\n\t\tee_len = ext4_ext_get_actual_len(ex);\n\n\t\ttrace_ext4_ext_show_extent(inode, ee_block, ee_start, ee_len);\n\n\t\t/* if found extent covers block, simply return it */\n\t\tif (in_range(map->m_lblk, ee_block, ee_len)) {\n\t\t\tnewblock = map->m_lblk - ee_block + ee_start;\n\t\t\t/* number of remaining blocks in the extent */\n\t\t\tallocated = ee_len - (map->m_lblk - ee_block);\n\t\t\text_debug(\"%u fit into %u:%d -> %llu\\n\", map->m_lblk,\n\t\t\t\t  ee_block, ee_len, newblock);\n\n\t\t\t/*\n\t\t\t * If the extent is initialized check whether the\n\t\t\t * caller wants to convert it to unwritten.\n\t\t\t */\n\t\t\tif ((!ext4_ext_is_unwritten(ex)) &&\n\t\t\t    (flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN)) {\n\t\t\t\tallocated = convert_initialized_extent(\n\t\t\t\t\t\thandle, inode, map, &path,\n\t\t\t\t\t\tallocated);\n\t\t\t\tgoto out2;\n\t\t\t} else if (!ext4_ext_is_unwritten(ex))\n\t\t\t\tgoto out;\n\n\t\t\tret = ext4_ext_handle_unwritten_extents(\n\t\t\t\thandle, inode, map, &path, flags,\n\t\t\t\tallocated, newblock);\n\t\t\tif (ret < 0)\n\t\t\t\terr = ret;\n\t\t\telse\n\t\t\t\tallocated = ret;\n\t\t\tgoto out2;\n\t\t}\n\t}\n\n\t/*\n\t * requested block isn't allocated yet;\n\t * we couldn't try to create block if create flag is zero\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\text4_lblk_t hole_start, hole_len;\n\n\t\thole_start = map->m_lblk;\n\t\thole_len = ext4_ext_determine_hole(inode, path, &hole_start);\n\t\t/*\n\t\t * put just found gap into cache to speed up\n\t\t * subsequent requests\n\t\t */\n\t\text4_ext_put_gap_in_cache(inode, hole_start, hole_len);\n\n\t\t/* Update hole_len to reflect hole size after map->m_lblk */\n\t\tif (hole_start != map->m_lblk)\n\t\t\thole_len -= map->m_lblk - hole_start;\n\t\tmap->m_pblk = 0;\n\t\tmap->m_len = min_t(unsigned int, map->m_len, hole_len);\n\n\t\tgoto out2;\n\t}\n\n\t/*\n\t * Okay, we need to do block allocation.\n\t */\n\tnewex.ee_block = cpu_to_le32(map->m_lblk);\n\tcluster_offset = EXT4_LBLK_COFF(sbi, map->m_lblk);\n\n\t/*\n\t * If we are doing bigalloc, check to see if the extent returned\n\t * by ext4_find_extent() implies a cluster we can use.\n\t */\n\tif (cluster_offset && ex &&\n\t    get_implied_cluster_alloc(inode->i_sb, map, ex, path)) {\n\t\tar.len = allocated = map->m_len;\n\t\tnewblock = map->m_pblk;\n\t\tmap_from_cluster = true;\n\t\tgoto got_allocated_blocks;\n\t}\n\n\t/* find neighbour allocated blocks */\n\tar.lleft = map->m_lblk;\n\terr = ext4_ext_search_left(inode, path, &ar.lleft, &ar.pleft);\n\tif (err)\n\t\tgoto out2;\n\tar.lright = map->m_lblk;\n\tex2 = NULL;\n\terr = ext4_ext_search_right(inode, path, &ar.lright, &ar.pright, &ex2);\n\tif (err)\n\t\tgoto out2;\n\n\t/* Check if the extent after searching to the right implies a\n\t * cluster we can use. */\n\tif ((sbi->s_cluster_ratio > 1) && ex2 &&\n\t    get_implied_cluster_alloc(inode->i_sb, map, ex2, path)) {\n\t\tar.len = allocated = map->m_len;\n\t\tnewblock = map->m_pblk;\n\t\tmap_from_cluster = true;\n\t\tgoto got_allocated_blocks;\n\t}\n\n\t/*\n\t * See if request is beyond maximum number of blocks we can have in\n\t * a single extent. For an initialized extent this limit is\n\t * EXT_INIT_MAX_LEN and for an unwritten extent this limit is\n\t * EXT_UNWRITTEN_MAX_LEN.\n\t */\n\tif (map->m_len > EXT_INIT_MAX_LEN &&\n\t    !(flags & EXT4_GET_BLOCKS_UNWRIT_EXT))\n\t\tmap->m_len = EXT_INIT_MAX_LEN;\n\telse if (map->m_len > EXT_UNWRITTEN_MAX_LEN &&\n\t\t (flags & EXT4_GET_BLOCKS_UNWRIT_EXT))\n\t\tmap->m_len = EXT_UNWRITTEN_MAX_LEN;\n\n\t/* Check if we can really insert (m_lblk)::(m_lblk + m_len) extent */\n\tnewex.ee_len = cpu_to_le16(map->m_len);\n\terr = ext4_ext_check_overlap(sbi, inode, &newex, path);\n\tif (err)\n\t\tallocated = ext4_ext_get_actual_len(&newex);\n\telse\n\t\tallocated = map->m_len;\n\n\t/* allocate new block */\n\tar.inode = inode;\n\tar.goal = ext4_ext_find_goal(inode, path, map->m_lblk);\n\tar.logical = map->m_lblk;\n\t/*\n\t * We calculate the offset from the beginning of the cluster\n\t * for the logical block number, since when we allocate a\n\t * physical cluster, the physical block should start at the\n\t * same offset from the beginning of the cluster.  This is\n\t * needed so that future calls to get_implied_cluster_alloc()\n\t * work correctly.\n\t */\n\toffset = EXT4_LBLK_COFF(sbi, map->m_lblk);\n\tar.len = EXT4_NUM_B2C(sbi, offset+allocated);\n\tar.goal -= offset;\n\tar.logical -= offset;\n\tif (S_ISREG(inode->i_mode))\n\t\tar.flags = EXT4_MB_HINT_DATA;\n\telse\n\t\t/* disable in-core preallocation for non-regular files */\n\t\tar.flags = 0;\n\tif (flags & EXT4_GET_BLOCKS_NO_NORMALIZE)\n\t\tar.flags |= EXT4_MB_HINT_NOPREALLOC;\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\tar.flags |= EXT4_MB_DELALLOC_RESERVED;\n\tif (flags & EXT4_GET_BLOCKS_METADATA_NOFAIL)\n\t\tar.flags |= EXT4_MB_USE_RESERVED;\n\tnewblock = ext4_mb_new_blocks(handle, &ar, &err);\n\tif (!newblock)\n\t\tgoto out2;\n\text_debug(\"allocate new block: goal %llu, found %llu/%u\\n\",\n\t\t  ar.goal, newblock, allocated);\n\tfree_on_err = 1;\n\tallocated_clusters = ar.len;\n\tar.len = EXT4_C2B(sbi, ar.len) - offset;\n\tif (ar.len > allocated)\n\t\tar.len = allocated;\n\ngot_allocated_blocks:\n\t/* try to insert new extent into found leaf and return */\n\text4_ext_store_pblock(&newex, newblock + offset);\n\tnewex.ee_len = cpu_to_le16(ar.len);\n\t/* Mark unwritten */\n\tif (flags & EXT4_GET_BLOCKS_UNWRIT_EXT){\n\t\text4_ext_mark_unwritten(&newex);\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t}\n\n\terr = 0;\n\tif ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0)\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t path, ar.len);\n\tif (!err)\n\t\terr = ext4_ext_insert_extent(handle, inode, &path,\n\t\t\t\t\t     &newex, flags);\n\n\tif (err && free_on_err) {\n\t\tint fb_flags = flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE ?\n\t\t\tEXT4_FREE_BLOCKS_NO_QUOT_UPDATE : 0;\n\t\t/* free data blocks we just allocated */\n\t\t/* not a good idea to call discard here directly,\n\t\t * but otherwise we'd need to call it every free() */\n\t\text4_discard_preallocations(inode);\n\t\text4_free_blocks(handle, inode, NULL, newblock,\n\t\t\t\t EXT4_C2B(sbi, allocated_clusters), fb_flags);\n\t\tgoto out2;\n\t}\n\n\t/* previous routine could use block we allocated */\n\tnewblock = ext4_ext_pblock(&newex);\n\tallocated = ext4_ext_get_actual_len(&newex);\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\n\t/*\n\t * Reduce the reserved cluster count to reflect successful deferred\n\t * allocation of delayed allocated clusters or direct allocation of\n\t * clusters discovered to be delayed allocated.  Once allocated, a\n\t * cluster is not included in the reserved count.\n\t */\n\tif (test_opt(inode->i_sb, DELALLOC) && !map_from_cluster) {\n\t\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) {\n\t\t\t/*\n\t\t\t * When allocating delayed allocated clusters, simply\n\t\t\t * reduce the reserved cluster count and claim quota\n\t\t\t */\n\t\t\text4_da_update_reserve_space(inode, allocated_clusters,\n\t\t\t\t\t\t\t1);\n\t\t} else {\n\t\t\text4_lblk_t lblk, len;\n\t\t\tunsigned int n;\n\n\t\t\t/*\n\t\t\t * When allocating non-delayed allocated clusters\n\t\t\t * (from fallocate, filemap, DIO, or clusters\n\t\t\t * allocated when delalloc has been disabled by\n\t\t\t * ext4_nonda_switch), reduce the reserved cluster\n\t\t\t * count by the number of allocated clusters that\n\t\t\t * have previously been delayed allocated.  Quota\n\t\t\t * has been claimed by ext4_mb_new_blocks() above,\n\t\t\t * so release the quota reservations made for any\n\t\t\t * previously delayed allocated clusters.\n\t\t\t */\n\t\t\tlblk = EXT4_LBLK_CMASK(sbi, map->m_lblk);\n\t\t\tlen = allocated_clusters << sbi->s_cluster_bits;\n\t\t\tn = ext4_es_delayed_clu(inode, lblk, len);\n\t\t\tif (n > 0)\n\t\t\t\text4_da_update_reserve_space(inode, (int) n, 0);\n\t\t}\n\t}\n\n\t/*\n\t * Cache the extent and update transaction to commit on fdatasync only\n\t * when it is _not_ an unwritten extent.\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_UNWRIT_EXT) == 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\telse\n\t\text4_update_inode_fsync_trans(handle, inode, 0);\nout:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\n\ttrace_ext4_ext_map_blocks_exit(inode, flags, map,\n\t\t\t\t       err ? err : allocated);\n\treturn err ? err : allocated;\n}\n\nint ext4_ext_truncate(handle_t *handle, struct inode *inode)\n{\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t last_block;\n\tint err = 0;\n\n\t/*\n\t * TODO: optimization is possible here.\n\t * Probably we need not scan at all,\n\t * because page truncation is enough.\n\t */\n\n\t/* we have to know where to truncate from in crash case */\n\tEXT4_I(inode)->i_disksize = inode->i_size;\n\terr = ext4_mark_inode_dirty(handle, inode);\n\tif (err)\n\t\treturn err;\n\n\tlast_block = (inode->i_size + sb->s_blocksize - 1)\n\t\t\t>> EXT4_BLOCK_SIZE_BITS(sb);\nretry:\n\terr = ext4_es_remove_extent(inode, last_block,\n\t\t\t\t    EXT_MAX_BLOCKS - last_block);\n\tif (err == -ENOMEM) {\n\t\tcond_resched();\n\t\tcongestion_wait(BLK_RW_ASYNC, HZ/50);\n\t\tgoto retry;\n\t}\n\tif (err)\n\t\treturn err;\n\treturn ext4_ext_remove_space(inode, last_block, EXT_MAX_BLOCKS - 1);\n}\n\nstatic int ext4_alloc_file_blocks(struct file *file, ext4_lblk_t offset,\n\t\t\t\t  ext4_lblk_t len, loff_t new_size,\n\t\t\t\t  int flags)\n{\n\tstruct inode *inode = file_inode(file);\n\thandle_t *handle;\n\tint ret = 0;\n\tint ret2 = 0;\n\tint retries = 0;\n\tint depth = 0;\n\tstruct ext4_map_blocks map;\n\tunsigned int credits;\n\tloff_t epos;\n\n\tBUG_ON(!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS));\n\tmap.m_lblk = offset;\n\tmap.m_len = len;\n\t/*\n\t * Don't normalize the request if it can fit in one extent so\n\t * that it doesn't get unnecessarily split into multiple\n\t * extents.\n\t */\n\tif (len <= EXT_UNWRITTEN_MAX_LEN)\n\t\tflags |= EXT4_GET_BLOCKS_NO_NORMALIZE;\n\n\t/*\n\t * credits to insert 1 extent into extent tree\n\t */\n\tcredits = ext4_chunk_trans_blocks(inode, len);\n\tdepth = ext_depth(inode);\n\nretry:\n\twhile (ret >= 0 && len) {\n\t\t/*\n\t\t * Recalculate credits when extent tree depth changes.\n\t\t */\n\t\tif (depth != ext_depth(inode)) {\n\t\t\tcredits = ext4_chunk_trans_blocks(inode, len);\n\t\t\tdepth = ext_depth(inode);\n\t\t}\n\n\t\thandle = ext4_journal_start(inode, EXT4_HT_MAP_BLOCKS,\n\t\t\t\t\t    credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tbreak;\n\t\t}\n\t\tret = ext4_map_blocks(handle, inode, &map, flags);\n\t\tif (ret <= 0) {\n\t\t\text4_debug(\"inode #%lu: block %u: len %u: \"\n\t\t\t\t   \"ext4_ext_map_blocks returned %d\",\n\t\t\t\t   inode->i_ino, map.m_lblk,\n\t\t\t\t   map.m_len, ret);\n\t\t\text4_mark_inode_dirty(handle, inode);\n\t\t\tret2 = ext4_journal_stop(handle);\n\t\t\tbreak;\n\t\t}\n\t\tmap.m_lblk += ret;\n\t\tmap.m_len = len = len - ret;\n\t\tepos = (loff_t)map.m_lblk << inode->i_blkbits;\n\t\tinode->i_ctime = current_time(inode);\n\t\tif (new_size) {\n\t\t\tif (epos > new_size)\n\t\t\t\tepos = new_size;\n\t\t\tif (ext4_update_inode_size(inode, epos) & 0x1)\n\t\t\t\tinode->i_mtime = inode->i_ctime;\n\t\t} else {\n\t\t\tif (epos > inode->i_size)\n\t\t\t\text4_set_inode_flag(inode,\n\t\t\t\t\t\t    EXT4_INODE_EOFBLOCKS);\n\t\t}\n\t\text4_mark_inode_dirty(handle, inode);\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret2)\n\t\t\tbreak;\n\t}\n\tif (ret == -ENOSPC &&\n\t\t\text4_should_retry_alloc(inode->i_sb, &retries)) {\n\t\tret = 0;\n\t\tgoto retry;\n\t}\n\n\treturn ret > 0 ? ret2 : ret;\n}\n\nstatic long ext4_zero_range(struct file *file, loff_t offset,\n\t\t\t    loff_t len, int mode)\n{\n\tstruct inode *inode = file_inode(file);\n\thandle_t *handle = NULL;\n\tunsigned int max_blocks;\n\tloff_t new_size = 0;\n\tint ret = 0;\n\tint flags;\n\tint credits;\n\tint partial_begin, partial_end;\n\tloff_t start, end;\n\text4_lblk_t lblk;\n\tunsigned int blkbits = inode->i_blkbits;\n\n\ttrace_ext4_zero_range(inode, offset, len, mode);\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EINVAL;\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal. */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Round up offset. This is not fallocate, we neet to zero out\n\t * blocks, so convert interior block aligned part of the range to\n\t * unwritten and possibly manually zero out unaligned parts of the\n\t * range.\n\t */\n\tstart = round_up(offset, 1 << blkbits);\n\tend = round_down((offset + len), 1 << blkbits);\n\n\tif (start < offset || end > offset + len)\n\t\treturn -EINVAL;\n\tpartial_begin = offset & ((1 << blkbits) - 1);\n\tpartial_end = (offset + len) & ((1 << blkbits) - 1);\n\n\tlblk = start >> blkbits;\n\tmax_blocks = (end >> blkbits);\n\tif (max_blocks < lblk)\n\t\tmax_blocks = 0;\n\telse\n\t\tmax_blocks -= lblk;\n\n\tinode_lock(inode);\n\n\t/*\n\t * Indirect files do not support unwritten extnets\n\t */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\tif (!(mode & FALLOC_FL_KEEP_SIZE) &&\n\t    (offset + len > i_size_read(inode) ||\n\t     offset + len > EXT4_I(inode)->i_disksize)) {\n\t\tnew_size = offset + len;\n\t\tret = inode_newsize_ok(inode, new_size);\n\t\tif (ret)\n\t\t\tgoto out_mutex;\n\t}\n\n\tflags = EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT;\n\tif (mode & FALLOC_FL_KEEP_SIZE)\n\t\tflags |= EXT4_GET_BLOCKS_KEEP_SIZE;\n\n\t/* Wait all existing dio workers, newcomers will block on i_mutex */\n\tinode_dio_wait(inode);\n\n\t/* Preallocate the range including the unaligned edges */\n\tif (partial_begin || partial_end) {\n\t\tret = ext4_alloc_file_blocks(file,\n\t\t\t\tround_down(offset, 1 << blkbits) >> blkbits,\n\t\t\t\t(round_up((offset + len), 1 << blkbits) -\n\t\t\t\t round_down(offset, 1 << blkbits)) >> blkbits,\n\t\t\t\tnew_size, flags);\n\t\tif (ret)\n\t\t\tgoto out_mutex;\n\n\t}\n\n\t/* Zero range excluding the unaligned edges */\n\tif (max_blocks > 0) {\n\t\tflags |= (EXT4_GET_BLOCKS_CONVERT_UNWRITTEN |\n\t\t\t  EXT4_EX_NOCACHE);\n\n\t\t/*\n\t\t * Prevent page faults from reinstantiating pages we have\n\t\t * released from page cache.\n\t\t */\n\t\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\n\t\tret = ext4_break_layouts(inode);\n\t\tif (ret) {\n\t\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\t\t\tgoto out_mutex;\n\t\t}\n\n\t\tret = ext4_update_disksize_before_punch(inode, offset, len);\n\t\tif (ret) {\n\t\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\t\t\tgoto out_mutex;\n\t\t}\n\t\t/* Now release the pages and zero block aligned part of pages */\n\t\ttruncate_pagecache_range(inode, start, end - 1);\n\t\tinode->i_mtime = inode->i_ctime = current_time(inode);\n\n\t\tret = ext4_alloc_file_blocks(file, lblk, max_blocks, new_size,\n\t\t\t\t\t     flags);\n\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\t\tif (ret)\n\t\t\tgoto out_mutex;\n\t}\n\tif (!partial_begin && !partial_end)\n\t\tgoto out_mutex;\n\n\t/*\n\t * In worst case we have to writeout two nonadjacent unwritten\n\t * blocks and update the inode\n\t */\n\tcredits = (2 * ext4_ext_index_trans_blocks(inode, 2)) + 1;\n\tif (ext4_should_journal_data(inode))\n\t\tcredits += 2;\n\thandle = ext4_journal_start(inode, EXT4_HT_MISC, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\text4_std_error(inode->i_sb, ret);\n\t\tgoto out_mutex;\n\t}\n\n\tinode->i_mtime = inode->i_ctime = current_time(inode);\n\tif (new_size) {\n\t\text4_update_inode_size(inode, new_size);\n\t} else {\n\t\t/*\n\t\t* Mark that we allocate beyond EOF so the subsequent truncate\n\t\t* can proceed even if the new size is the same as i_size.\n\t\t*/\n\t\tif ((offset + len) > i_size_read(inode))\n\t\t\text4_set_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\t}\n\text4_mark_inode_dirty(handle, inode);\n\n\t/* Zero out partial block at the edges of the range */\n\tret = ext4_zero_partial_blocks(handle, inode, offset, len);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\n\tif (file->f_flags & O_SYNC)\n\t\text4_handle_sync(handle);\n\n\text4_journal_stop(handle);\nout_mutex:\n\tinode_unlock(inode);\n\treturn ret;\n}\n\n/*\n * preallocate space for a file. This implements ext4's fallocate file\n * operation, which gets called from sys_fallocate system call.\n * For block-mapped files, posix_fallocate should fall back to the method\n * of writing zeroes to the required new blocks (the same behavior which is\n * expected for file systems which do not support fallocate() system call).\n */\nlong ext4_fallocate(struct file *file, int mode, loff_t offset, loff_t len)\n{\n\tstruct inode *inode = file_inode(file);\n\tloff_t new_size = 0;\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint flags;\n\text4_lblk_t lblk;\n\tunsigned int blkbits = inode->i_blkbits;\n\n\t/*\n\t * Encrypted inodes can't handle collapse range or insert\n\t * range since we would need to re-encrypt blocks with a\n\t * different IV or XTS tweak (which are based on the logical\n\t * block number).\n\t *\n\t * XXX It's not clear why zero range isn't working, but we'll\n\t * leave it disabled for encrypted inodes for now.  This is a\n\t * bug we should fix....\n\t */\n\tif (IS_ENCRYPTED(inode) &&\n\t    (mode & (FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_INSERT_RANGE |\n\t\t     FALLOC_FL_ZERO_RANGE)))\n\t\treturn -EOPNOTSUPP;\n\n\t/* Return error if mode is not supported */\n\tif (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE |\n\t\t     FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_ZERO_RANGE |\n\t\t     FALLOC_FL_INSERT_RANGE))\n\t\treturn -EOPNOTSUPP;\n\n\tif (mode & FALLOC_FL_PUNCH_HOLE)\n\t\treturn ext4_punch_hole(inode, offset, len);\n\n\tret = ext4_convert_inline_data(inode);\n\tif (ret)\n\t\treturn ret;\n\n\tif (mode & FALLOC_FL_COLLAPSE_RANGE)\n\t\treturn ext4_collapse_range(inode, offset, len);\n\n\tif (mode & FALLOC_FL_INSERT_RANGE)\n\t\treturn ext4_insert_range(inode, offset, len);\n\n\tif (mode & FALLOC_FL_ZERO_RANGE)\n\t\treturn ext4_zero_range(file, offset, len, mode);\n\n\ttrace_ext4_fallocate_enter(inode, offset, len, mode);\n\tlblk = offset >> blkbits;\n\n\tmax_blocks = EXT4_MAX_BLOCKS(len, offset, blkbits);\n\tflags = EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT;\n\tif (mode & FALLOC_FL_KEEP_SIZE)\n\t\tflags |= EXT4_GET_BLOCKS_KEEP_SIZE;\n\n\tinode_lock(inode);\n\n\t/*\n\t * We only support preallocation for extent-based files only\n\t */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (!(mode & FALLOC_FL_KEEP_SIZE) &&\n\t    (offset + len > i_size_read(inode) ||\n\t     offset + len > EXT4_I(inode)->i_disksize)) {\n\t\tnew_size = offset + len;\n\t\tret = inode_newsize_ok(inode, new_size);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\t/* Wait all existing dio workers, newcomers will block on i_mutex */\n\tinode_dio_wait(inode);\n\n\tret = ext4_alloc_file_blocks(file, lblk, max_blocks, new_size, flags);\n\tif (ret)\n\t\tgoto out;\n\n\tif (file->f_flags & O_SYNC && EXT4_SB(inode->i_sb)->s_journal) {\n\t\tret = jbd2_complete_transaction(EXT4_SB(inode->i_sb)->s_journal,\n\t\t\t\t\t\tEXT4_I(inode)->i_sync_tid);\n\t}\nout:\n\tinode_unlock(inode);\n\ttrace_ext4_fallocate_exit(inode, offset, max_blocks, ret);\n\treturn ret;\n}\n\n/*\n * This function convert a range of blocks to written extents\n * The caller of this function will pass the start offset and the size.\n * all unwritten extents within this range will be converted to\n * written extents.\n *\n * This function is called from the direct IO end io call back\n * function, to convert the fallocated extents after IO is completed.\n * Returns 0 on success.\n */\nint ext4_convert_unwritten_extents(handle_t *handle, struct inode *inode,\n\t\t\t\t   loff_t offset, ssize_t len)\n{\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint ret2 = 0;\n\tstruct ext4_map_blocks map;\n\tunsigned int credits, blkbits = inode->i_blkbits;\n\n\tmap.m_lblk = offset >> blkbits;\n\tmax_blocks = EXT4_MAX_BLOCKS(len, offset, blkbits);\n\n\t/*\n\t * This is somewhat ugly but the idea is clear: When transaction is\n\t * reserved, everything goes into it. Otherwise we rather start several\n\t * smaller transactions for conversion of each extent separately.\n\t */\n\tif (handle) {\n\t\thandle = ext4_journal_start_reserved(handle,\n\t\t\t\t\t\t     EXT4_HT_EXT_CONVERT);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tcredits = 0;\n\t} else {\n\t\t/*\n\t\t * credits to insert 1 extent into extent tree\n\t\t */\n\t\tcredits = ext4_chunk_trans_blocks(inode, max_blocks);\n\t}\n\twhile (ret >= 0 && ret < max_blocks) {\n\t\tmap.m_lblk += ret;\n\t\tmap.m_len = (max_blocks -= ret);\n\t\tif (credits) {\n\t\t\thandle = ext4_journal_start(inode, EXT4_HT_MAP_BLOCKS,\n\t\t\t\t\t\t    credits);\n\t\t\tif (IS_ERR(handle)) {\n\t\t\t\tret = PTR_ERR(handle);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tret = ext4_map_blocks(handle, inode, &map,\n\t\t\t\t      EXT4_GET_BLOCKS_IO_CONVERT_EXT);\n\t\tif (ret <= 0)\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"inode #%lu: block %u: len %u: \"\n\t\t\t\t     \"ext4_ext_map_blocks returned %d\",\n\t\t\t\t     inode->i_ino, map.m_lblk,\n\t\t\t\t     map.m_len, ret);\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tif (credits)\n\t\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret <= 0 || ret2)\n\t\t\tbreak;\n\t}\n\tif (!credits)\n\t\tret2 = ext4_journal_stop(handle);\n\treturn ret > 0 ? ret2 : ret;\n}\n\n/*\n * If newes is not existing extent (newes->ec_pblk equals zero) find\n * delayed extent at start of newes and update newes accordingly and\n * return start of the next delayed extent.\n *\n * If newes is existing extent (newes->ec_pblk is not equal zero)\n * return start of next delayed extent or EXT_MAX_BLOCKS if no delayed\n * extent found. Leave newes unmodified.\n */\nstatic int ext4_find_delayed_extent(struct inode *inode,\n\t\t\t\t    struct extent_status *newes)\n{\n\tstruct extent_status es;\n\text4_lblk_t block, next_del;\n\n\tif (newes->es_pblk == 0) {\n\t\text4_es_find_extent_range(inode, &ext4_es_is_delayed,\n\t\t\t\t\t  newes->es_lblk,\n\t\t\t\t\t  newes->es_lblk + newes->es_len - 1,\n\t\t\t\t\t  &es);\n\n\t\t/*\n\t\t * No extent in extent-tree contains block @newes->es_pblk,\n\t\t * then the block may stay in 1)a hole or 2)delayed-extent.\n\t\t */\n\t\tif (es.es_len == 0)\n\t\t\t/* A hole found. */\n\t\t\treturn 0;\n\n\t\tif (es.es_lblk > newes->es_lblk) {\n\t\t\t/* A hole found. */\n\t\t\tnewes->es_len = min(es.es_lblk - newes->es_lblk,\n\t\t\t\t\t    newes->es_len);\n\t\t\treturn 0;\n\t\t}\n\n\t\tnewes->es_len = es.es_lblk + es.es_len - newes->es_lblk;\n\t}\n\n\tblock = newes->es_lblk + newes->es_len;\n\text4_es_find_extent_range(inode, &ext4_es_is_delayed, block,\n\t\t\t\t  EXT_MAX_BLOCKS, &es);\n\tif (es.es_len == 0)\n\t\tnext_del = EXT_MAX_BLOCKS;\n\telse\n\t\tnext_del = es.es_lblk;\n\n\treturn next_del;\n}\n/* fiemap flags we can handle specified here */\n#define EXT4_FIEMAP_FLAGS\t(FIEMAP_FLAG_SYNC|FIEMAP_FLAG_XATTR)\n\nstatic int ext4_xattr_fiemap(struct inode *inode,\n\t\t\t\tstruct fiemap_extent_info *fieinfo)\n{\n\t__u64 physical = 0;\n\t__u64 length;\n\t__u32 flags = FIEMAP_EXTENT_LAST;\n\tint blockbits = inode->i_sb->s_blocksize_bits;\n\tint error = 0;\n\n\t/* in-inode? */\n\tif (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {\n\t\tstruct ext4_iloc iloc;\n\t\tint offset;\t/* offset of xattr in inode */\n\n\t\terror = ext4_get_inode_loc(inode, &iloc);\n\t\tif (error)\n\t\t\treturn error;\n\t\tphysical = (__u64)iloc.bh->b_blocknr << blockbits;\n\t\toffset = EXT4_GOOD_OLD_INODE_SIZE +\n\t\t\t\tEXT4_I(inode)->i_extra_isize;\n\t\tphysical += offset;\n\t\tlength = EXT4_SB(inode->i_sb)->s_inode_size - offset;\n\t\tflags |= FIEMAP_EXTENT_DATA_INLINE;\n\t\tbrelse(iloc.bh);\n\t} else { /* external block */\n\t\tphysical = (__u64)EXT4_I(inode)->i_file_acl << blockbits;\n\t\tlength = inode->i_sb->s_blocksize;\n\t}\n\n\tif (physical)\n\t\terror = fiemap_fill_next_extent(fieinfo, 0, physical,\n\t\t\t\t\t\tlength, flags);\n\treturn (error < 0 ? error : 0);\n}\n\nint ext4_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t__u64 start, __u64 len)\n{\n\text4_lblk_t start_blk;\n\tint error = 0;\n\n\tif (ext4_has_inline_data(inode)) {\n\t\tint has_inline = 1;\n\n\t\terror = ext4_inline_data_fiemap(inode, fieinfo, &has_inline,\n\t\t\t\t\t\tstart, len);\n\n\t\tif (has_inline)\n\t\t\treturn error;\n\t}\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_CACHE) {\n\t\terror = ext4_ext_precache(inode);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\t/* fallback to generic here if not in extents fmt */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))\n\t\treturn generic_block_fiemap(inode, fieinfo, start, len,\n\t\t\text4_get_block);\n\n\tif (fiemap_check_flags(fieinfo, EXT4_FIEMAP_FLAGS))\n\t\treturn -EBADR;\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_XATTR) {\n\t\terror = ext4_xattr_fiemap(inode, fieinfo);\n\t} else {\n\t\text4_lblk_t len_blks;\n\t\t__u64 last_blk;\n\n\t\tstart_blk = start >> inode->i_sb->s_blocksize_bits;\n\t\tlast_blk = (start + len - 1) >> inode->i_sb->s_blocksize_bits;\n\t\tif (last_blk >= EXT_MAX_BLOCKS)\n\t\t\tlast_blk = EXT_MAX_BLOCKS-1;\n\t\tlen_blks = ((ext4_lblk_t) last_blk) - start_blk + 1;\n\n\t\t/*\n\t\t * Walk the extent tree gathering extent information\n\t\t * and pushing extents back to the user.\n\t\t */\n\t\terror = ext4_fill_fiemap_extents(inode, start_blk,\n\t\t\t\t\t\t len_blks, fieinfo);\n\t}\n\treturn error;\n}\n\n/*\n * ext4_access_path:\n * Function to access the path buffer for marking it dirty.\n * It also checks if there are sufficient credits left in the journal handle\n * to update path.\n */\nstatic int\next4_access_path(handle_t *handle, struct inode *inode,\n\t\tstruct ext4_ext_path *path)\n{\n\tint credits, err;\n\n\tif (!ext4_handle_valid(handle))\n\t\treturn 0;\n\n\t/*\n\t * Check if need to extend journal credits\n\t * 3 for leaf, sb, and inode plus 2 (bmap and group\n\t * descriptor) for each block group; assume two block\n\t * groups\n\t */\n\tif (handle->h_buffer_credits < 7) {\n\t\tcredits = ext4_writepage_trans_blocks(inode);\n\t\terr = ext4_ext_truncate_extend_restart(handle, inode, credits);\n\t\t/* EAGAIN is success */\n\t\tif (err && err != -EAGAIN)\n\t\t\treturn err;\n\t}\n\n\terr = ext4_ext_get_access(handle, inode, path);\n\treturn err;\n}\n\n/*\n * ext4_ext_shift_path_extents:\n * Shift the extents of a path structure lying between path[depth].p_ext\n * and EXT_LAST_EXTENT(path[depth].p_hdr), by @shift blocks. @SHIFT tells\n * if it is right shift or left shift operation.\n */\nstatic int\next4_ext_shift_path_extents(struct ext4_ext_path *path, ext4_lblk_t shift,\n\t\t\t    struct inode *inode, handle_t *handle,\n\t\t\t    enum SHIFT_DIRECTION SHIFT)\n{\n\tint depth, err = 0;\n\tstruct ext4_extent *ex_start, *ex_last;\n\tbool update = 0;\n\tdepth = path->p_depth;\n\n\twhile (depth >= 0) {\n\t\tif (depth == path->p_depth) {\n\t\t\tex_start = path[depth].p_ext;\n\t\t\tif (!ex_start)\n\t\t\t\treturn -EFSCORRUPTED;\n\n\t\t\tex_last = EXT_LAST_EXTENT(path[depth].p_hdr);\n\n\t\t\terr = ext4_access_path(handle, inode, path + depth);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\tif (ex_start == EXT_FIRST_EXTENT(path[depth].p_hdr))\n\t\t\t\tupdate = 1;\n\n\t\t\twhile (ex_start <= ex_last) {\n\t\t\t\tif (SHIFT == SHIFT_LEFT) {\n\t\t\t\t\tle32_add_cpu(&ex_start->ee_block,\n\t\t\t\t\t\t-shift);\n\t\t\t\t\t/* Try to merge to the left. */\n\t\t\t\t\tif ((ex_start >\n\t\t\t\t\t    EXT_FIRST_EXTENT(path[depth].p_hdr))\n\t\t\t\t\t    &&\n\t\t\t\t\t    ext4_ext_try_to_merge_right(inode,\n\t\t\t\t\t    path, ex_start - 1))\n\t\t\t\t\t\tex_last--;\n\t\t\t\t\telse\n\t\t\t\t\t\tex_start++;\n\t\t\t\t} else {\n\t\t\t\t\tle32_add_cpu(&ex_last->ee_block, shift);\n\t\t\t\t\text4_ext_try_to_merge_right(inode, path,\n\t\t\t\t\t\tex_last);\n\t\t\t\t\tex_last--;\n\t\t\t\t}\n\t\t\t}\n\t\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\tif (--depth < 0 || !update)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t/* Update index too */\n\t\terr = ext4_access_path(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (SHIFT == SHIFT_LEFT)\n\t\t\tle32_add_cpu(&path[depth].p_idx->ei_block, -shift);\n\t\telse\n\t\t\tle32_add_cpu(&path[depth].p_idx->ei_block, shift);\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* we are done if current index is not a starting index */\n\t\tif (path[depth].p_idx != EXT_FIRST_INDEX(path[depth].p_hdr))\n\t\t\tbreak;\n\n\t\tdepth--;\n\t}\n\nout:\n\treturn err;\n}\n\n/*\n * ext4_ext_shift_extents:\n * All the extents which lies in the range from @start to the last allocated\n * block for the @inode are shifted either towards left or right (depending\n * upon @SHIFT) by @shift blocks.\n * On success, 0 is returned, error otherwise.\n */\nstatic int\next4_ext_shift_extents(struct inode *inode, handle_t *handle,\n\t\t       ext4_lblk_t start, ext4_lblk_t shift,\n\t\t       enum SHIFT_DIRECTION SHIFT)\n{\n\tstruct ext4_ext_path *path;\n\tint ret = 0, depth;\n\tstruct ext4_extent *extent;\n\text4_lblk_t stop, *iterator, ex_start, ex_end;\n\n\t/* Let path point to the last extent */\n\tpath = ext4_find_extent(inode, EXT_MAX_BLOCKS - 1, NULL,\n\t\t\t\tEXT4_EX_NOCACHE);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\n\tdepth = path->p_depth;\n\textent = path[depth].p_ext;\n\tif (!extent)\n\t\tgoto out;\n\n\tstop = le32_to_cpu(extent->ee_block);\n\n       /*\n\t* For left shifts, make sure the hole on the left is big enough to\n\t* accommodate the shift.  For right shifts, make sure the last extent\n\t* won't be shifted beyond EXT_MAX_BLOCKS.\n\t*/\n\tif (SHIFT == SHIFT_LEFT) {\n\t\tpath = ext4_find_extent(inode, start - 1, &path,\n\t\t\t\t\tEXT4_EX_NOCACHE);\n\t\tif (IS_ERR(path))\n\t\t\treturn PTR_ERR(path);\n\t\tdepth = path->p_depth;\n\t\textent =  path[depth].p_ext;\n\t\tif (extent) {\n\t\t\tex_start = le32_to_cpu(extent->ee_block);\n\t\t\tex_end = le32_to_cpu(extent->ee_block) +\n\t\t\t\text4_ext_get_actual_len(extent);\n\t\t} else {\n\t\t\tex_start = 0;\n\t\t\tex_end = 0;\n\t\t}\n\n\t\tif ((start == ex_start && shift > ex_start) ||\n\t\t    (shift > start - ex_end)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tif (shift > EXT_MAX_BLOCKS -\n\t\t    (stop + ext4_ext_get_actual_len(extent))) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * In case of left shift, iterator points to start and it is increased\n\t * till we reach stop. In case of right shift, iterator points to stop\n\t * and it is decreased till we reach start.\n\t */\n\tif (SHIFT == SHIFT_LEFT)\n\t\titerator = &start;\n\telse\n\t\titerator = &stop;\n\n\t/*\n\t * Its safe to start updating extents.  Start and stop are unsigned, so\n\t * in case of right shift if extent with 0 block is reached, iterator\n\t * becomes NULL to indicate the end of the loop.\n\t */\n\twhile (iterator && start <= stop) {\n\t\tpath = ext4_find_extent(inode, *iterator, &path,\n\t\t\t\t\tEXT4_EX_NOCACHE);\n\t\tif (IS_ERR(path))\n\t\t\treturn PTR_ERR(path);\n\t\tdepth = path->p_depth;\n\t\textent = path[depth].p_ext;\n\t\tif (!extent) {\n\t\t\tEXT4_ERROR_INODE(inode, \"unexpected hole at %lu\",\n\t\t\t\t\t (unsigned long) *iterator);\n\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t\tif (SHIFT == SHIFT_LEFT && *iterator >\n\t\t    le32_to_cpu(extent->ee_block)) {\n\t\t\t/* Hole, move to the next extent */\n\t\t\tif (extent < EXT_LAST_EXTENT(path[depth].p_hdr)) {\n\t\t\t\tpath[depth].p_ext++;\n\t\t\t} else {\n\t\t\t\t*iterator = ext4_ext_next_allocated_block(path);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (SHIFT == SHIFT_LEFT) {\n\t\t\textent = EXT_LAST_EXTENT(path[depth].p_hdr);\n\t\t\t*iterator = le32_to_cpu(extent->ee_block) +\n\t\t\t\t\text4_ext_get_actual_len(extent);\n\t\t} else {\n\t\t\textent = EXT_FIRST_EXTENT(path[depth].p_hdr);\n\t\t\tif (le32_to_cpu(extent->ee_block) > 0)\n\t\t\t\t*iterator = le32_to_cpu(extent->ee_block) - 1;\n\t\t\telse\n\t\t\t\t/* Beginning is reached, end of the loop */\n\t\t\t\titerator = NULL;\n\t\t\t/* Update path extent in case we need to stop */\n\t\t\twhile (le32_to_cpu(extent->ee_block) < start)\n\t\t\t\textent++;\n\t\t\tpath[depth].p_ext = extent;\n\t\t}\n\t\tret = ext4_ext_shift_path_extents(path, shift, inode,\n\t\t\t\thandle, SHIFT);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\nout:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\treturn ret;\n}\n\n/*\n * ext4_collapse_range:\n * This implements the fallocate's collapse range functionality for ext4\n * Returns: 0 and non-zero on error.\n */\nint ext4_collapse_range(struct inode *inode, loff_t offset, loff_t len)\n{\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t punch_start, punch_stop;\n\thandle_t *handle;\n\tunsigned int credits;\n\tloff_t new_size, ioffset;\n\tint ret;\n\n\t/*\n\t * We need to test this early because xfstests assumes that a\n\t * collapse range of (0, 1) will return EOPNOTSUPP if the file\n\t * system does not support collapse range.\n\t */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\treturn -EOPNOTSUPP;\n\n\t/* Collapse range works only on fs block size aligned offsets. */\n\tif (offset & (EXT4_CLUSTER_SIZE(sb) - 1) ||\n\t    len & (EXT4_CLUSTER_SIZE(sb) - 1))\n\t\treturn -EINVAL;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EINVAL;\n\n\ttrace_ext4_collapse_range(inode, offset, len);\n\n\tpunch_start = offset >> EXT4_BLOCK_SIZE_BITS(sb);\n\tpunch_stop = (offset + len) >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal. */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tinode_lock(inode);\n\t/*\n\t * There is no need to overlap collapse range with EOF, in which case\n\t * it is effectively a truncate operation\n\t */\n\tif (offset + len >= i_size_read(inode)) {\n\t\tret = -EINVAL;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Currently just for extent based files */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Wait for existing dio to complete */\n\tinode_dio_wait(inode);\n\n\t/*\n\t * Prevent page faults from reinstantiating pages we have released from\n\t * page cache.\n\t */\n\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\n\tret = ext4_break_layouts(inode);\n\tif (ret)\n\t\tgoto out_mmap;\n\n\t/*\n\t * Need to round down offset to be aligned with page size boundary\n\t * for page size > block size.\n\t */\n\tioffset = round_down(offset, PAGE_SIZE);\n\t/*\n\t * Write tail of the last page before removed range since it will get\n\t * removed from the page cache below.\n\t */\n\tret = filemap_write_and_wait_range(inode->i_mapping, ioffset, offset);\n\tif (ret)\n\t\tgoto out_mmap;\n\t/*\n\t * Write data that will be shifted to preserve them when discarding\n\t * page cache below. We are also protected from pages becoming dirty\n\t * by i_mmap_sem.\n\t */\n\tret = filemap_write_and_wait_range(inode->i_mapping, offset + len,\n\t\t\t\t\t   LLONG_MAX);\n\tif (ret)\n\t\tgoto out_mmap;\n\ttruncate_pagecache(inode, ioffset);\n\n\tcredits = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out_mmap;\n\t}\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_discard_preallocations(inode);\n\n\tret = ext4_es_remove_extent(inode, punch_start,\n\t\t\t\t    EXT_MAX_BLOCKS - punch_start);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tret = ext4_ext_remove_space(inode, punch_start, punch_stop - 1);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\text4_discard_preallocations(inode);\n\n\tret = ext4_ext_shift_extents(inode, handle, punch_stop,\n\t\t\t\t     punch_stop - punch_start, SHIFT_LEFT);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tnew_size = i_size_read(inode) - len;\n\ti_size_write(inode, new_size);\n\tEXT4_I(inode)->i_disksize = new_size;\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\tinode->i_mtime = inode->i_ctime = current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\text4_update_inode_fsync_trans(handle, inode, 1);\n\nout_stop:\n\text4_journal_stop(handle);\nout_mmap:\n\tup_write(&EXT4_I(inode)->i_mmap_sem);\nout_mutex:\n\tinode_unlock(inode);\n\treturn ret;\n}\n\n/*\n * ext4_insert_range:\n * This function implements the FALLOC_FL_INSERT_RANGE flag of fallocate.\n * The data blocks starting from @offset to the EOF are shifted by @len\n * towards right to create a hole in the @inode. Inode size is increased\n * by len bytes.\n * Returns 0 on success, error otherwise.\n */\nint ext4_insert_range(struct inode *inode, loff_t offset, loff_t len)\n{\n\tstruct super_block *sb = inode->i_sb;\n\thandle_t *handle;\n\tstruct ext4_ext_path *path;\n\tstruct ext4_extent *extent;\n\text4_lblk_t offset_lblk, len_lblk, ee_start_lblk = 0;\n\tunsigned int credits, ee_len;\n\tint ret = 0, depth, split_flag = 0;\n\tloff_t ioffset;\n\n\t/*\n\t * We need to test this early because xfstests assumes that an\n\t * insert range of (0, 1) will return EOPNOTSUPP if the file\n\t * system does not support insert range.\n\t */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\treturn -EOPNOTSUPP;\n\n\t/* Insert range works only on fs block size aligned offsets. */\n\tif (offset & (EXT4_CLUSTER_SIZE(sb) - 1) ||\n\t\t\tlen & (EXT4_CLUSTER_SIZE(sb) - 1))\n\t\treturn -EINVAL;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EOPNOTSUPP;\n\n\ttrace_ext4_insert_range(inode, offset, len);\n\n\toffset_lblk = offset >> EXT4_BLOCK_SIZE_BITS(sb);\n\tlen_lblk = len >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tinode_lock(inode);\n\t/* Currently just for extent based files */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Check for wrap through zero */\n\tif (inode->i_size + len > inode->i_sb->s_maxbytes) {\n\t\tret = -EFBIG;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Offset should be less than i_size */\n\tif (offset >= i_size_read(inode)) {\n\t\tret = -EINVAL;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Wait for existing dio to complete */\n\tinode_dio_wait(inode);\n\n\t/*\n\t * Prevent page faults from reinstantiating pages we have released from\n\t * page cache.\n\t */\n\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\n\tret = ext4_break_layouts(inode);\n\tif (ret)\n\t\tgoto out_mmap;\n\n\t/*\n\t * Need to round down to align start offset to page size boundary\n\t * for page size > block size.\n\t */\n\tioffset = round_down(offset, PAGE_SIZE);\n\t/* Write out all dirty pages */\n\tret = filemap_write_and_wait_range(inode->i_mapping, ioffset,\n\t\t\tLLONG_MAX);\n\tif (ret)\n\t\tgoto out_mmap;\n\ttruncate_pagecache(inode, ioffset);\n\n\tcredits = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out_mmap;\n\t}\n\n\t/* Expand file to avoid data loss if there is error while shifting */\n\tinode->i_size += len;\n\tEXT4_I(inode)->i_disksize += len;\n\tinode->i_mtime = inode->i_ctime = current_time(inode);\n\tret = ext4_mark_inode_dirty(handle, inode);\n\tif (ret)\n\t\tgoto out_stop;\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_discard_preallocations(inode);\n\n\tpath = ext4_find_extent(inode, offset_lblk, NULL, 0);\n\tif (IS_ERR(path)) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tdepth = ext_depth(inode);\n\textent = path[depth].p_ext;\n\tif (extent) {\n\t\tee_start_lblk = le32_to_cpu(extent->ee_block);\n\t\tee_len = ext4_ext_get_actual_len(extent);\n\n\t\t/*\n\t\t * If offset_lblk is not the starting block of extent, split\n\t\t * the extent @offset_lblk\n\t\t */\n\t\tif ((offset_lblk > ee_start_lblk) &&\n\t\t\t\t(offset_lblk < (ee_start_lblk + ee_len))) {\n\t\t\tif (ext4_ext_is_unwritten(extent))\n\t\t\t\tsplit_flag = EXT4_EXT_MARK_UNWRIT1 |\n\t\t\t\t\tEXT4_EXT_MARK_UNWRIT2;\n\t\t\tret = ext4_split_extent_at(handle, inode, &path,\n\t\t\t\t\toffset_lblk, split_flag,\n\t\t\t\t\tEXT4_EX_NOCACHE |\n\t\t\t\t\tEXT4_GET_BLOCKS_PRE_IO |\n\t\t\t\t\tEXT4_GET_BLOCKS_METADATA_NOFAIL);\n\t\t}\n\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t\tif (ret < 0) {\n\t\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\t\tgoto out_stop;\n\t\t}\n\t} else {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\n\tret = ext4_es_remove_extent(inode, offset_lblk,\n\t\t\tEXT_MAX_BLOCKS - offset_lblk);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\t/*\n\t * if offset_lblk lies in a hole which is at start of file, use\n\t * ee_start_lblk to shift extents\n\t */\n\tret = ext4_ext_shift_extents(inode, handle,\n\t\tee_start_lblk > offset_lblk ? ee_start_lblk : offset_lblk,\n\t\tlen_lblk, SHIFT_RIGHT);\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\nout_stop:\n\text4_journal_stop(handle);\nout_mmap:\n\tup_write(&EXT4_I(inode)->i_mmap_sem);\nout_mutex:\n\tinode_unlock(inode);\n\treturn ret;\n}\n\n/**\n * ext4_swap_extents - Swap extents between two inodes\n *\n * @inode1:\tFirst inode\n * @inode2:\tSecond inode\n * @lblk1:\tStart block for first inode\n * @lblk2:\tStart block for second inode\n * @count:\tNumber of blocks to swap\n * @unwritten: Mark second inode's extents as unwritten after swap\n * @erp:\tPointer to save error value\n *\n * This helper routine does exactly what is promise \"swap extents\". All other\n * stuff such as page-cache locking consistency, bh mapping consistency or\n * extent's data copying must be performed by caller.\n * Locking:\n * \t\ti_mutex is held for both inodes\n * \t\ti_data_sem is locked for write for both inodes\n * Assumptions:\n *\t\tAll pages from requested range are locked for both inodes\n */\nint\next4_swap_extents(handle_t *handle, struct inode *inode1,\n\t\t  struct inode *inode2, ext4_lblk_t lblk1, ext4_lblk_t lblk2,\n\t\t  ext4_lblk_t count, int unwritten, int *erp)\n{\n\tstruct ext4_ext_path *path1 = NULL;\n\tstruct ext4_ext_path *path2 = NULL;\n\tint replaced_count = 0;\n\n\tBUG_ON(!rwsem_is_locked(&EXT4_I(inode1)->i_data_sem));\n\tBUG_ON(!rwsem_is_locked(&EXT4_I(inode2)->i_data_sem));\n\tBUG_ON(!inode_is_locked(inode1));\n\tBUG_ON(!inode_is_locked(inode2));\n\n\t*erp = ext4_es_remove_extent(inode1, lblk1, count);\n\tif (unlikely(*erp))\n\t\treturn 0;\n\t*erp = ext4_es_remove_extent(inode2, lblk2, count);\n\tif (unlikely(*erp))\n\t\treturn 0;\n\n\twhile (count) {\n\t\tstruct ext4_extent *ex1, *ex2, tmp_ex;\n\t\text4_lblk_t e1_blk, e2_blk;\n\t\tint e1_len, e2_len, len;\n\t\tint split = 0;\n\n\t\tpath1 = ext4_find_extent(inode1, lblk1, NULL, EXT4_EX_NOCACHE);\n\t\tif (IS_ERR(path1)) {\n\t\t\t*erp = PTR_ERR(path1);\n\t\t\tpath1 = NULL;\n\t\tfinish:\n\t\t\tcount = 0;\n\t\t\tgoto repeat;\n\t\t}\n\t\tpath2 = ext4_find_extent(inode2, lblk2, NULL, EXT4_EX_NOCACHE);\n\t\tif (IS_ERR(path2)) {\n\t\t\t*erp = PTR_ERR(path2);\n\t\t\tpath2 = NULL;\n\t\t\tgoto finish;\n\t\t}\n\t\tex1 = path1[path1->p_depth].p_ext;\n\t\tex2 = path2[path2->p_depth].p_ext;\n\t\t/* Do we have somthing to swap ? */\n\t\tif (unlikely(!ex2 || !ex1))\n\t\t\tgoto finish;\n\n\t\te1_blk = le32_to_cpu(ex1->ee_block);\n\t\te2_blk = le32_to_cpu(ex2->ee_block);\n\t\te1_len = ext4_ext_get_actual_len(ex1);\n\t\te2_len = ext4_ext_get_actual_len(ex2);\n\n\t\t/* Hole handling */\n\t\tif (!in_range(lblk1, e1_blk, e1_len) ||\n\t\t    !in_range(lblk2, e2_blk, e2_len)) {\n\t\t\text4_lblk_t next1, next2;\n\n\t\t\t/* if hole after extent, then go to next extent */\n\t\t\tnext1 = ext4_ext_next_allocated_block(path1);\n\t\t\tnext2 = ext4_ext_next_allocated_block(path2);\n\t\t\t/* If hole before extent, then shift to that extent */\n\t\t\tif (e1_blk > lblk1)\n\t\t\t\tnext1 = e1_blk;\n\t\t\tif (e2_blk > lblk2)\n\t\t\t\tnext2 = e2_blk;\n\t\t\t/* Do we have something to swap */\n\t\t\tif (next1 == EXT_MAX_BLOCKS || next2 == EXT_MAX_BLOCKS)\n\t\t\t\tgoto finish;\n\t\t\t/* Move to the rightest boundary */\n\t\t\tlen = next1 - lblk1;\n\t\t\tif (len < next2 - lblk2)\n\t\t\t\tlen = next2 - lblk2;\n\t\t\tif (len > count)\n\t\t\t\tlen = count;\n\t\t\tlblk1 += len;\n\t\t\tlblk2 += len;\n\t\t\tcount -= len;\n\t\t\tgoto repeat;\n\t\t}\n\n\t\t/* Prepare left boundary */\n\t\tif (e1_blk < lblk1) {\n\t\t\tsplit = 1;\n\t\t\t*erp = ext4_force_split_extent_at(handle, inode1,\n\t\t\t\t\t\t&path1, lblk1, 0);\n\t\t\tif (unlikely(*erp))\n\t\t\t\tgoto finish;\n\t\t}\n\t\tif (e2_blk < lblk2) {\n\t\t\tsplit = 1;\n\t\t\t*erp = ext4_force_split_extent_at(handle, inode2,\n\t\t\t\t\t\t&path2,  lblk2, 0);\n\t\t\tif (unlikely(*erp))\n\t\t\t\tgoto finish;\n\t\t}\n\t\t/* ext4_split_extent_at() may result in leaf extent split,\n\t\t * path must to be revalidated. */\n\t\tif (split)\n\t\t\tgoto repeat;\n\n\t\t/* Prepare right boundary */\n\t\tlen = count;\n\t\tif (len > e1_blk + e1_len - lblk1)\n\t\t\tlen = e1_blk + e1_len - lblk1;\n\t\tif (len > e2_blk + e2_len - lblk2)\n\t\t\tlen = e2_blk + e2_len - lblk2;\n\n\t\tif (len != e1_len) {\n\t\t\tsplit = 1;\n\t\t\t*erp = ext4_force_split_extent_at(handle, inode1,\n\t\t\t\t\t\t&path1, lblk1 + len, 0);\n\t\t\tif (unlikely(*erp))\n\t\t\t\tgoto finish;\n\t\t}\n\t\tif (len != e2_len) {\n\t\t\tsplit = 1;\n\t\t\t*erp = ext4_force_split_extent_at(handle, inode2,\n\t\t\t\t\t\t&path2, lblk2 + len, 0);\n\t\t\tif (*erp)\n\t\t\t\tgoto finish;\n\t\t}\n\t\t/* ext4_split_extent_at() may result in leaf extent split,\n\t\t * path must to be revalidated. */\n\t\tif (split)\n\t\t\tgoto repeat;\n\n\t\tBUG_ON(e2_len != e1_len);\n\t\t*erp = ext4_ext_get_access(handle, inode1, path1 + path1->p_depth);\n\t\tif (unlikely(*erp))\n\t\t\tgoto finish;\n\t\t*erp = ext4_ext_get_access(handle, inode2, path2 + path2->p_depth);\n\t\tif (unlikely(*erp))\n\t\t\tgoto finish;\n\n\t\t/* Both extents are fully inside boundaries. Swap it now */\n\t\ttmp_ex = *ex1;\n\t\text4_ext_store_pblock(ex1, ext4_ext_pblock(ex2));\n\t\text4_ext_store_pblock(ex2, ext4_ext_pblock(&tmp_ex));\n\t\tex1->ee_len = cpu_to_le16(e2_len);\n\t\tex2->ee_len = cpu_to_le16(e1_len);\n\t\tif (unwritten)\n\t\t\text4_ext_mark_unwritten(ex2);\n\t\tif (ext4_ext_is_unwritten(&tmp_ex))\n\t\t\text4_ext_mark_unwritten(ex1);\n\n\t\text4_ext_try_to_merge(handle, inode2, path2, ex2);\n\t\text4_ext_try_to_merge(handle, inode1, path1, ex1);\n\t\t*erp = ext4_ext_dirty(handle, inode2, path2 +\n\t\t\t\t      path2->p_depth);\n\t\tif (unlikely(*erp))\n\t\t\tgoto finish;\n\t\t*erp = ext4_ext_dirty(handle, inode1, path1 +\n\t\t\t\t      path1->p_depth);\n\t\t/*\n\t\t * Looks scarry ah..? second inode already points to new blocks,\n\t\t * and it was successfully dirtied. But luckily error may happen\n\t\t * only due to journal error, so full transaction will be\n\t\t * aborted anyway.\n\t\t */\n\t\tif (unlikely(*erp))\n\t\t\tgoto finish;\n\t\tlblk1 += len;\n\t\tlblk2 += len;\n\t\treplaced_count += len;\n\t\tcount -= len;\n\n\trepeat:\n\t\text4_ext_drop_refs(path1);\n\t\tkfree(path1);\n\t\text4_ext_drop_refs(path2);\n\t\tkfree(path2);\n\t\tpath1 = path2 = NULL;\n\t}\n\treturn replaced_count;\n}\n\n/*\n * ext4_clu_mapped - determine whether any block in a logical cluster has\n *                   been mapped to a physical cluster\n *\n * @inode - file containing the logical cluster\n * @lclu - logical cluster of interest\n *\n * Returns 1 if any block in the logical cluster is mapped, signifying\n * that a physical cluster has been allocated for it.  Otherwise,\n * returns 0.  Can also return negative error codes.  Derived from\n * ext4_ext_map_blocks().\n */\nint ext4_clu_mapped(struct inode *inode, ext4_lblk_t lclu)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_ext_path *path;\n\tint depth, mapped = 0, err = 0;\n\tstruct ext4_extent *extent;\n\text4_lblk_t first_lblk, first_lclu, last_lclu;\n\n\t/* search for the extent closest to the first block in the cluster */\n\tpath = ext4_find_extent(inode, EXT4_C2B(sbi, lclu), NULL, 0);\n\tif (IS_ERR(path)) {\n\t\terr = PTR_ERR(path);\n\t\tpath = NULL;\n\t\tgoto out;\n\t}\n\n\tdepth = ext_depth(inode);\n\n\t/*\n\t * A consistent leaf must not be empty.  This situation is possible,\n\t * though, _during_ tree modification, and it's why an assert can't\n\t * be put in ext4_find_extent().\n\t */\n\tif (unlikely(path[depth].p_ext == NULL && depth != 0)) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t    \"bad extent address - lblock: %lu, depth: %d, pblock: %lld\",\n\t\t\t\t (unsigned long) EXT4_C2B(sbi, lclu),\n\t\t\t\t depth, path[depth].p_block);\n\t\terr = -EFSCORRUPTED;\n\t\tgoto out;\n\t}\n\n\textent = path[depth].p_ext;\n\n\t/* can't be mapped if the extent tree is empty */\n\tif (extent == NULL)\n\t\tgoto out;\n\n\tfirst_lblk = le32_to_cpu(extent->ee_block);\n\tfirst_lclu = EXT4_B2C(sbi, first_lblk);\n\n\t/*\n\t * Three possible outcomes at this point - found extent spanning\n\t * the target cluster, to the left of the target cluster, or to the\n\t * right of the target cluster.  The first two cases are handled here.\n\t * The last case indicates the target cluster is not mapped.\n\t */\n\tif (lclu >= first_lclu) {\n\t\tlast_lclu = EXT4_B2C(sbi, first_lblk +\n\t\t\t\t     ext4_ext_get_actual_len(extent) - 1);\n\t\tif (lclu <= last_lclu) {\n\t\t\tmapped = 1;\n\t\t} else {\n\t\t\tfirst_lblk = ext4_ext_next_allocated_block(path);\n\t\t\tfirst_lclu = EXT4_B2C(sbi, first_lblk);\n\t\t\tif (lclu == first_lclu)\n\t\t\t\tmapped = 1;\n\t\t}\n\t}\n\nout:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\n\treturn err ? err : mapped;\n}\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (c) 2003-2006, Cluster File Systems, Inc, info@clusterfs.com\n * Written by Alex Tomas <alex@clusterfs.com>\n *\n * Architecture independence:\n *   Copyright (c) 2005, Bull S.A.\n *   Written by Pierre Peiffer <pierre.peiffer@bull.net>\n */\n\n/*\n * Extents support for EXT4\n *\n * TODO:\n *   - ext4*_error() should be used in some situations\n *   - analyze all BUG()/BUG_ON(), use -EIO where appropriate\n *   - smart tree reduction\n */\n\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/jbd2.h>\n#include <linux/highuid.h>\n#include <linux/pagemap.h>\n#include <linux/quotaops.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#include <linux/fiemap.h>\n#include <linux/backing-dev.h>\n#include \"ext4_jbd2.h\"\n#include \"ext4_extents.h\"\n#include \"xattr.h\"\n\n#include <trace/events/ext4.h>\n\n/*\n * used by extent splitting.\n */\n#define EXT4_EXT_MAY_ZEROOUT\t0x1  /* safe to zeroout if split fails \\\n\t\t\t\t\tdue to ENOSPC */\n#define EXT4_EXT_MARK_UNWRIT1\t0x2  /* mark first half unwritten */\n#define EXT4_EXT_MARK_UNWRIT2\t0x4  /* mark second half unwritten */\n\n#define EXT4_EXT_DATA_VALID1\t0x8  /* first half contains valid data */\n#define EXT4_EXT_DATA_VALID2\t0x10 /* second half contains valid data */\n\nstatic __le32 ext4_extent_block_csum(struct inode *inode,\n\t\t\t\t     struct ext4_extent_header *eh)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t__u32 csum;\n\n\tcsum = ext4_chksum(sbi, ei->i_csum_seed, (__u8 *)eh,\n\t\t\t   EXT4_EXTENT_TAIL_OFFSET(eh));\n\treturn cpu_to_le32(csum);\n}\n\nstatic int ext4_extent_block_csum_verify(struct inode *inode,\n\t\t\t\t\t struct ext4_extent_header *eh)\n{\n\tstruct ext4_extent_tail *et;\n\n\tif (!ext4_has_metadata_csum(inode->i_sb))\n\t\treturn 1;\n\n\tet = find_ext4_extent_tail(eh);\n\tif (et->et_checksum != ext4_extent_block_csum(inode, eh))\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic void ext4_extent_block_csum_set(struct inode *inode,\n\t\t\t\t       struct ext4_extent_header *eh)\n{\n\tstruct ext4_extent_tail *et;\n\n\tif (!ext4_has_metadata_csum(inode->i_sb))\n\t\treturn;\n\n\tet = find_ext4_extent_tail(eh);\n\tet->et_checksum = ext4_extent_block_csum(inode, eh);\n}\n\nstatic int ext4_split_extent(handle_t *handle,\n\t\t\t\tstruct inode *inode,\n\t\t\t\tstruct ext4_ext_path **ppath,\n\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\tint split_flag,\n\t\t\t\tint flags);\n\nstatic int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path **ppath,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags);\n\nstatic int ext4_find_delayed_extent(struct inode *inode,\n\t\t\t\t    struct extent_status *newes);\n\nstatic int ext4_ext_truncate_extend_restart(handle_t *handle,\n\t\t\t\t\t    struct inode *inode,\n\t\t\t\t\t    int needed)\n{\n\tint err;\n\n\tif (!ext4_handle_valid(handle))\n\t\treturn 0;\n\tif (handle->h_buffer_credits >= needed)\n\t\treturn 0;\n\t/*\n\t * If we need to extend the journal get a few extra blocks\n\t * while we're at it for efficiency's sake.\n\t */\n\tneeded += 3;\n\terr = ext4_journal_extend(handle, needed - handle->h_buffer_credits);\n\tif (err <= 0)\n\t\treturn err;\n\terr = ext4_truncate_restart_trans(handle, inode, needed);\n\tif (err == 0)\n\t\terr = -EAGAIN;\n\n\treturn err;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n */\nstatic int ext4_ext_get_access(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path->p_bh) {\n\t\t/* path points to block */\n\t\tBUFFER_TRACE(path->p_bh, \"get_write_access\");\n\t\treturn ext4_journal_get_write_access(handle, path->p_bh);\n\t}\n\t/* path points to leaf/index in inode body */\n\t/* we use in-core data, no need to protect them */\n\treturn 0;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n *  - EIO\n */\nint __ext4_ext_dirty(const char *where, unsigned int line, handle_t *handle,\n\t\t     struct inode *inode, struct ext4_ext_path *path)\n{\n\tint err;\n\n\tWARN_ON(!rwsem_is_locked(&EXT4_I(inode)->i_data_sem));\n\tif (path->p_bh) {\n\t\text4_extent_block_csum_set(inode, ext_block_hdr(path->p_bh));\n\t\t/* path points to block */\n\t\terr = __ext4_handle_dirty_metadata(where, line, handle,\n\t\t\t\t\t\t   inode, path->p_bh);\n\t} else {\n\t\t/* path points to leaf/index in inode body */\n\t\terr = ext4_mark_inode_dirty(handle, inode);\n\t}\n\treturn err;\n}\n\nstatic ext4_fsblk_t ext4_ext_find_goal(struct inode *inode,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      ext4_lblk_t block)\n{\n\tif (path) {\n\t\tint depth = path->p_depth;\n\t\tstruct ext4_extent *ex;\n\n\t\t/*\n\t\t * Try to predict block placement assuming that we are\n\t\t * filling in a file which will eventually be\n\t\t * non-sparse --- i.e., in the case of libbfd writing\n\t\t * an ELF object sections out-of-order but in a way\n\t\t * the eventually results in a contiguous object or\n\t\t * executable file, or some database extending a table\n\t\t * space file.  However, this is actually somewhat\n\t\t * non-ideal if we are writing a sparse file such as\n\t\t * qemu or KVM writing a raw image file that is going\n\t\t * to stay fairly sparse, since it will end up\n\t\t * fragmenting the file system's free space.  Maybe we\n\t\t * should have some hueristics or some way to allow\n\t\t * userspace to pass a hint to file system,\n\t\t * especially if the latter case turns out to be\n\t\t * common.\n\t\t */\n\t\tex = path[depth].p_ext;\n\t\tif (ex) {\n\t\t\text4_fsblk_t ext_pblk = ext4_ext_pblock(ex);\n\t\t\text4_lblk_t ext_block = le32_to_cpu(ex->ee_block);\n\n\t\t\tif (block > ext_block)\n\t\t\t\treturn ext_pblk + (block - ext_block);\n\t\t\telse\n\t\t\t\treturn ext_pblk - (ext_block - block);\n\t\t}\n\n\t\t/* it looks like index is empty;\n\t\t * try to find starting block from index itself */\n\t\tif (path[depth].p_bh)\n\t\t\treturn path[depth].p_bh->b_blocknr;\n\t}\n\n\t/* OK. use inode's group */\n\treturn ext4_inode_to_goal_block(inode);\n}\n\n/*\n * Allocation for a meta data block\n */\nstatic ext4_fsblk_t\next4_ext_new_meta_block(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path,\n\t\t\tstruct ext4_extent *ex, int *err, unsigned int flags)\n{\n\text4_fsblk_t goal, newblock;\n\n\tgoal = ext4_ext_find_goal(inode, path, le32_to_cpu(ex->ee_block));\n\tnewblock = ext4_new_meta_blocks(handle, inode, goal, flags,\n\t\t\t\t\tNULL, err);\n\treturn newblock;\n}\n\nstatic inline int ext4_ext_space_block(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 6)\n\t\tsize = 6;\n#endif\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_block_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent_idx);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 5)\n\t\tsize = 5;\n#endif\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 3)\n\t\tsize = 3;\n#endif\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent_idx);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 4)\n\t\tsize = 4;\n#endif\n\treturn size;\n}\n\nstatic inline int\next4_force_split_extent_at(handle_t *handle, struct inode *inode,\n\t\t\t   struct ext4_ext_path **ppath, ext4_lblk_t lblk,\n\t\t\t   int nofail)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tint unwritten = ext4_ext_is_unwritten(path[path->p_depth].p_ext);\n\n\treturn ext4_split_extent_at(handle, inode, ppath, lblk, unwritten ?\n\t\t\tEXT4_EXT_MARK_UNWRIT1|EXT4_EXT_MARK_UNWRIT2 : 0,\n\t\t\tEXT4_EX_NOCACHE | EXT4_GET_BLOCKS_PRE_IO |\n\t\t\t(nofail ? EXT4_GET_BLOCKS_METADATA_NOFAIL:0));\n}\n\n/*\n * Calculate the number of metadata blocks needed\n * to allocate @blocks\n * Worse case is one block per extent\n */\nint ext4_ext_calc_metadata_amount(struct inode *inode, ext4_lblk_t lblock)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint idxs;\n\n\tidxs = ((inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t/ sizeof(struct ext4_extent_idx));\n\n\t/*\n\t * If the new delayed allocation block is contiguous with the\n\t * previous da block, it can share index blocks with the\n\t * previous block, so we only need to allocate a new index\n\t * block every idxs leaf blocks.  At ldxs**2 blocks, we need\n\t * an additional index block, and at ldxs**3 blocks, yet\n\t * another index blocks.\n\t */\n\tif (ei->i_da_metadata_calc_len &&\n\t    ei->i_da_metadata_calc_last_lblock+1 == lblock) {\n\t\tint num = 0;\n\n\t\tif ((ei->i_da_metadata_calc_len % idxs) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs)) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs*idxs)) == 0) {\n\t\t\tnum++;\n\t\t\tei->i_da_metadata_calc_len = 0;\n\t\t} else\n\t\t\tei->i_da_metadata_calc_len++;\n\t\tei->i_da_metadata_calc_last_lblock++;\n\t\treturn num;\n\t}\n\n\t/*\n\t * In the worst case we need a new set of index blocks at\n\t * every level of the inode's extent tree.\n\t */\n\tei->i_da_metadata_calc_len = 1;\n\tei->i_da_metadata_calc_last_lblock = lblock;\n\treturn ext_depth(inode) + 1;\n}\n\nstatic int\next4_ext_max_entries(struct inode *inode, int depth)\n{\n\tint max;\n\n\tif (depth == ext_depth(inode)) {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_root(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_root_idx(inode, 1);\n\t} else {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_block(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_block_idx(inode, 1);\n\t}\n\n\treturn max;\n}\n\nstatic int ext4_valid_extent(struct inode *inode, struct ext4_extent *ext)\n{\n\text4_fsblk_t block = ext4_ext_pblock(ext);\n\tint len = ext4_ext_get_actual_len(ext);\n\text4_lblk_t lblock = le32_to_cpu(ext->ee_block);\n\n\t/*\n\t * We allow neither:\n\t *  - zero length\n\t *  - overflow/wrap-around\n\t */\n\tif (lblock + len <= lblock)\n\t\treturn 0;\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, len);\n}\n\nstatic int ext4_valid_extent_idx(struct inode *inode,\n\t\t\t\tstruct ext4_extent_idx *ext_idx)\n{\n\text4_fsblk_t block = ext4_idx_pblock(ext_idx);\n\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, 1);\n}\n\nstatic int ext4_valid_extent_entries(struct inode *inode,\n\t\t\t\tstruct ext4_extent_header *eh,\n\t\t\t\tint depth)\n{\n\tunsigned short entries;\n\tif (eh->eh_entries == 0)\n\t\treturn 1;\n\n\tentries = le16_to_cpu(eh->eh_entries);\n\n\tif (depth == 0) {\n\t\t/* leaf entries */\n\t\tstruct ext4_extent *ext = EXT_FIRST_EXTENT(eh);\n\t\tstruct ext4_super_block *es = EXT4_SB(inode->i_sb)->s_es;\n\t\text4_fsblk_t pblock = 0;\n\t\text4_lblk_t lblock = 0;\n\t\text4_lblk_t prev = 0;\n\t\tint len = 0;\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent(inode, ext))\n\t\t\t\treturn 0;\n\n\t\t\t/* Check for overlapping extents */\n\t\t\tlblock = le32_to_cpu(ext->ee_block);\n\t\t\tlen = ext4_ext_get_actual_len(ext);\n\t\t\tif ((lblock <= prev) && prev) {\n\t\t\t\tpblock = ext4_ext_pblock(ext);\n\t\t\t\tes->s_last_error_block = cpu_to_le64(pblock);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\text++;\n\t\t\tentries--;\n\t\t\tprev = lblock + len - 1;\n\t\t}\n\t} else {\n\t\tstruct ext4_extent_idx *ext_idx = EXT_FIRST_INDEX(eh);\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent_idx(inode, ext_idx))\n\t\t\t\treturn 0;\n\t\t\text_idx++;\n\t\t\tentries--;\n\t\t}\n\t}\n\treturn 1;\n}\n\nstatic int __ext4_ext_check(const char *function, unsigned int line,\n\t\t\t    struct inode *inode, struct ext4_extent_header *eh,\n\t\t\t    int depth, ext4_fsblk_t pblk)\n{\n\tconst char *error_msg;\n\tint max = 0, err = -EFSCORRUPTED;\n\n\tif (unlikely(eh->eh_magic != EXT4_EXT_MAGIC)) {\n\t\terror_msg = \"invalid magic\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_depth) != depth)) {\n\t\terror_msg = \"unexpected eh_depth\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(eh->eh_max == 0)) {\n\t\terror_msg = \"invalid eh_max\";\n\t\tgoto corrupted;\n\t}\n\tmax = ext4_ext_max_entries(inode, depth);\n\tif (unlikely(le16_to_cpu(eh->eh_max) > max)) {\n\t\terror_msg = \"too large eh_max\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_entries) > le16_to_cpu(eh->eh_max))) {\n\t\terror_msg = \"invalid eh_entries\";\n\t\tgoto corrupted;\n\t}\n\tif (!ext4_valid_extent_entries(inode, eh, depth)) {\n\t\terror_msg = \"invalid extent entries\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(depth > 32)) {\n\t\terror_msg = \"too large eh_depth\";\n\t\tgoto corrupted;\n\t}\n\t/* Verify checksum on non-root extent tree nodes */\n\tif (ext_depth(inode) != depth &&\n\t    !ext4_extent_block_csum_verify(inode, eh)) {\n\t\terror_msg = \"extent tree corrupted\";\n\t\terr = -EFSBADCRC;\n\t\tgoto corrupted;\n\t}\n\treturn 0;\n\ncorrupted:\n\text4_error_inode(inode, function, line, 0,\n\t\t\t \"pblk %llu bad header/extent: %s - magic %x, \"\n\t\t\t \"entries %u, max %u(%u), depth %u(%u)\",\n\t\t\t (unsigned long long) pblk, error_msg,\n\t\t\t le16_to_cpu(eh->eh_magic),\n\t\t\t le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max),\n\t\t\t max, le16_to_cpu(eh->eh_depth), depth);\n\treturn err;\n}\n\n#define ext4_ext_check(inode, eh, depth, pblk)\t\t\t\\\n\t__ext4_ext_check(__func__, __LINE__, (inode), (eh), (depth), (pblk))\n\nint ext4_ext_check_inode(struct inode *inode)\n{\n\treturn ext4_ext_check(inode, ext_inode_hdr(inode), ext_depth(inode), 0);\n}\n\nstatic struct buffer_head *\n__read_extent_tree_block(const char *function, unsigned int line,\n\t\t\t struct inode *inode, ext4_fsblk_t pblk, int depth,\n\t\t\t int flags)\n{\n\tstruct buffer_head\t\t*bh;\n\tint\t\t\t\terr;\n\n\tbh = sb_getblk_gfp(inode->i_sb, pblk, __GFP_MOVABLE | GFP_NOFS);\n\tif (unlikely(!bh))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (!bh_uptodate_or_lock(bh)) {\n\t\ttrace_ext4_ext_load_extent(inode, pblk, _RET_IP_);\n\t\terr = bh_submit_read(bh);\n\t\tif (err < 0)\n\t\t\tgoto errout;\n\t}\n\tif (buffer_verified(bh) && !(flags & EXT4_EX_FORCE_CACHE))\n\t\treturn bh;\n\terr = __ext4_ext_check(function, line, inode,\n\t\t\t       ext_block_hdr(bh), depth, pblk);\n\tif (err)\n\t\tgoto errout;\n\tset_buffer_verified(bh);\n\t/*\n\t * If this is a leaf block, cache all of its entries\n\t */\n\tif (!(flags & EXT4_EX_NOCACHE) && depth == 0) {\n\t\tstruct ext4_extent_header *eh = ext_block_hdr(bh);\n\t\tstruct ext4_extent *ex = EXT_FIRST_EXTENT(eh);\n\t\text4_lblk_t prev = 0;\n\t\tint i;\n\n\t\tfor (i = le16_to_cpu(eh->eh_entries); i > 0; i--, ex++) {\n\t\t\tunsigned int status = EXTENT_STATUS_WRITTEN;\n\t\t\text4_lblk_t lblk = le32_to_cpu(ex->ee_block);\n\t\t\tint len = ext4_ext_get_actual_len(ex);\n\n\t\t\tif (prev && (prev != lblk))\n\t\t\t\text4_es_cache_extent(inode, prev,\n\t\t\t\t\t\t     lblk - prev, ~0,\n\t\t\t\t\t\t     EXTENT_STATUS_HOLE);\n\n\t\t\tif (ext4_ext_is_unwritten(ex))\n\t\t\t\tstatus = EXTENT_STATUS_UNWRITTEN;\n\t\t\text4_es_cache_extent(inode, lblk, len,\n\t\t\t\t\t     ext4_ext_pblock(ex), status);\n\t\t\tprev = lblk + len;\n\t\t}\n\t}\n\treturn bh;\nerrout:\n\tput_bh(bh);\n\treturn ERR_PTR(err);\n\n}\n\n#define read_extent_tree_block(inode, pblk, depth, flags)\t\t\\\n\t__read_extent_tree_block(__func__, __LINE__, (inode), (pblk),   \\\n\t\t\t\t (depth), (flags))\n\n/*\n * This function is called to cache a file's extent information in the\n * extent status tree\n */\nint ext4_ext_precache(struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct ext4_ext_path *path = NULL;\n\tstruct buffer_head *bh;\n\tint i = 0, depth, ret = 0;\n\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\treturn 0;\t/* not an extent-mapped inode */\n\n\tdown_read(&ei->i_data_sem);\n\tdepth = ext_depth(inode);\n\n\tpath = kcalloc(depth + 1, sizeof(struct ext4_ext_path),\n\t\t       GFP_NOFS);\n\tif (path == NULL) {\n\t\tup_read(&ei->i_data_sem);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* Don't cache anything if there are no external extent blocks */\n\tif (depth == 0)\n\t\tgoto out;\n\tpath[0].p_hdr = ext_inode_hdr(inode);\n\tret = ext4_ext_check(inode, path[0].p_hdr, depth, 0);\n\tif (ret)\n\t\tgoto out;\n\tpath[0].p_idx = EXT_FIRST_INDEX(path[0].p_hdr);\n\twhile (i >= 0) {\n\t\t/*\n\t\t * If this is a leaf block or we've reached the end of\n\t\t * the index block, go up\n\t\t */\n\t\tif ((i == depth) ||\n\t\t    path[i].p_idx > EXT_LAST_INDEX(path[i].p_hdr)) {\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\t\tbh = read_extent_tree_block(inode,\n\t\t\t\t\t    ext4_idx_pblock(path[i].p_idx++),\n\t\t\t\t\t    depth - i - 1,\n\t\t\t\t\t    EXT4_EX_FORCE_CACHE);\n\t\tif (IS_ERR(bh)) {\n\t\t\tret = PTR_ERR(bh);\n\t\t\tbreak;\n\t\t}\n\t\ti++;\n\t\tpath[i].p_bh = bh;\n\t\tpath[i].p_hdr = ext_block_hdr(bh);\n\t\tpath[i].p_idx = EXT_FIRST_INDEX(path[i].p_hdr);\n\t}\n\text4_set_inode_state(inode, EXT4_STATE_EXT_PRECACHED);\nout:\n\tup_read(&ei->i_data_sem);\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\treturn ret;\n}\n\n#ifdef EXT_DEBUG\nstatic void ext4_ext_show_path(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint k, l = path->p_depth;\n\n\text_debug(\"path:\");\n\tfor (k = 0; k <= l; k++, path++) {\n\t\tif (path->p_idx) {\n\t\t  ext_debug(\"  %d->%llu\", le32_to_cpu(path->p_idx->ei_block),\n\t\t\t    ext4_idx_pblock(path->p_idx));\n\t\t} else if (path->p_ext) {\n\t\t\text_debug(\"  %d:[%d]%d:%llu \",\n\t\t\t\t  le32_to_cpu(path->p_ext->ee_block),\n\t\t\t\t  ext4_ext_is_unwritten(path->p_ext),\n\t\t\t\t  ext4_ext_get_actual_len(path->p_ext),\n\t\t\t\t  ext4_ext_pblock(path->p_ext));\n\t\t} else\n\t\t\text_debug(\"  []\");\n\t}\n\text_debug(\"\\n\");\n}\n\nstatic void ext4_ext_show_leaf(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex;\n\tint i;\n\n\tif (!path)\n\t\treturn;\n\n\teh = path[depth].p_hdr;\n\tex = EXT_FIRST_EXTENT(eh);\n\n\text_debug(\"Displaying leaf extents for inode %lu\\n\", inode->i_ino);\n\n\tfor (i = 0; i < le16_to_cpu(eh->eh_entries); i++, ex++) {\n\t\text_debug(\"%d:[%d]%d:%llu \", le32_to_cpu(ex->ee_block),\n\t\t\t  ext4_ext_is_unwritten(ex),\n\t\t\t  ext4_ext_get_actual_len(ex), ext4_ext_pblock(ex));\n\t}\n\text_debug(\"\\n\");\n}\n\nstatic void ext4_ext_show_move(struct inode *inode, struct ext4_ext_path *path,\n\t\t\text4_fsblk_t newblock, int level)\n{\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent *ex;\n\n\tif (depth != level) {\n\t\tstruct ext4_extent_idx *idx;\n\t\tidx = path[level].p_idx;\n\t\twhile (idx <= EXT_MAX_INDEX(path[level].p_hdr)) {\n\t\t\text_debug(\"%d: move %d:%llu in new index %llu\\n\", level,\n\t\t\t\t\tle32_to_cpu(idx->ei_block),\n\t\t\t\t\text4_idx_pblock(idx),\n\t\t\t\t\tnewblock);\n\t\t\tidx++;\n\t\t}\n\n\t\treturn;\n\t}\n\n\tex = path[depth].p_ext;\n\twhile (ex <= EXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\text_debug(\"move %d:%llu:[%d]%d in new leaf %llu\\n\",\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\text4_ext_pblock(ex),\n\t\t\t\text4_ext_is_unwritten(ex),\n\t\t\t\text4_ext_get_actual_len(ex),\n\t\t\t\tnewblock);\n\t\tex++;\n\t}\n}\n\n#else\n#define ext4_ext_show_path(inode, path)\n#define ext4_ext_show_leaf(inode, path)\n#define ext4_ext_show_move(inode, path, newblock, level)\n#endif\n\nvoid ext4_ext_drop_refs(struct ext4_ext_path *path)\n{\n\tint depth, i;\n\n\tif (!path)\n\t\treturn;\n\tdepth = path->p_depth;\n\tfor (i = 0; i <= depth; i++, path++)\n\t\tif (path->p_bh) {\n\t\t\tbrelse(path->p_bh);\n\t\t\tpath->p_bh = NULL;\n\t\t}\n}\n\n/*\n * ext4_ext_binsearch_idx:\n * binary search for the closest index of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch_idx(struct inode *inode,\n\t\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent_idx *r, *l, *m;\n\n\n\text_debug(\"binsearch for %u(idx):  \", block);\n\n\tl = EXT_FIRST_INDEX(eh) + 1;\n\tr = EXT_LAST_INDEX(eh);\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ei_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ei_block),\n\t\t\t\tm, le32_to_cpu(m->ei_block),\n\t\t\t\tr, le32_to_cpu(r->ei_block));\n\t}\n\n\tpath->p_idx = l - 1;\n\text_debug(\"  -> %u->%lld \", le32_to_cpu(path->p_idx->ei_block),\n\t\t  ext4_idx_pblock(path->p_idx));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent_idx *chix, *ix;\n\t\tint k;\n\n\t\tchix = ix = EXT_FIRST_INDEX(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ix++) {\n\t\t  if (k != 0 &&\n\t\t      le32_to_cpu(ix->ei_block) <= le32_to_cpu(ix[-1].ei_block)) {\n\t\t\t\tprintk(KERN_DEBUG \"k=%d, ix=0x%p, \"\n\t\t\t\t       \"first=0x%p\\n\", k,\n\t\t\t\t       ix, EXT_FIRST_INDEX(eh));\n\t\t\t\tprintk(KERN_DEBUG \"%u <= %u\\n\",\n\t\t\t\t       le32_to_cpu(ix->ei_block),\n\t\t\t\t       le32_to_cpu(ix[-1].ei_block));\n\t\t\t}\n\t\t\tBUG_ON(k && le32_to_cpu(ix->ei_block)\n\t\t\t\t\t   <= le32_to_cpu(ix[-1].ei_block));\n\t\t\tif (block < le32_to_cpu(ix->ei_block))\n\t\t\t\tbreak;\n\t\t\tchix = ix;\n\t\t}\n\t\tBUG_ON(chix != path->p_idx);\n\t}\n#endif\n\n}\n\n/*\n * ext4_ext_binsearch:\n * binary search for closest extent of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch(struct inode *inode,\n\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent *r, *l, *m;\n\n\tif (eh->eh_entries == 0) {\n\t\t/*\n\t\t * this leaf is empty:\n\t\t * we get such a leaf in split/add case\n\t\t */\n\t\treturn;\n\t}\n\n\text_debug(\"binsearch for %u:  \", block);\n\n\tl = EXT_FIRST_EXTENT(eh) + 1;\n\tr = EXT_LAST_EXTENT(eh);\n\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ee_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ee_block),\n\t\t\t\tm, le32_to_cpu(m->ee_block),\n\t\t\t\tr, le32_to_cpu(r->ee_block));\n\t}\n\n\tpath->p_ext = l - 1;\n\text_debug(\"  -> %d:%llu:[%d]%d \",\n\t\t\tle32_to_cpu(path->p_ext->ee_block),\n\t\t\text4_ext_pblock(path->p_ext),\n\t\t\text4_ext_is_unwritten(path->p_ext),\n\t\t\text4_ext_get_actual_len(path->p_ext));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent *chex, *ex;\n\t\tint k;\n\n\t\tchex = ex = EXT_FIRST_EXTENT(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ex++) {\n\t\t\tBUG_ON(k && le32_to_cpu(ex->ee_block)\n\t\t\t\t\t  <= le32_to_cpu(ex[-1].ee_block));\n\t\t\tif (block < le32_to_cpu(ex->ee_block))\n\t\t\t\tbreak;\n\t\t\tchex = ex;\n\t\t}\n\t\tBUG_ON(chex != path->p_ext);\n\t}\n#endif\n\n}\n\nint ext4_ext_tree_init(handle_t *handle, struct inode *inode)\n{\n\tstruct ext4_extent_header *eh;\n\n\teh = ext_inode_hdr(inode);\n\teh->eh_depth = 0;\n\teh->eh_entries = 0;\n\teh->eh_magic = EXT4_EXT_MAGIC;\n\teh->eh_max = cpu_to_le16(ext4_ext_space_root(inode, 0));\n\text4_mark_inode_dirty(handle, inode);\n\treturn 0;\n}\n\nstruct ext4_ext_path *\next4_find_extent(struct inode *inode, ext4_lblk_t block,\n\t\t struct ext4_ext_path **orig_path, int flags)\n{\n\tstruct ext4_extent_header *eh;\n\tstruct buffer_head *bh;\n\tstruct ext4_ext_path *path = orig_path ? *orig_path : NULL;\n\tshort int depth, i, ppos = 0;\n\tint ret;\n\n\teh = ext_inode_hdr(inode);\n\tdepth = ext_depth(inode);\n\tif (depth < 0 || depth > EXT4_MAX_EXTENT_DEPTH) {\n\t\tEXT4_ERROR_INODE(inode, \"inode has invalid extent depth: %d\",\n\t\t\t\t depth);\n\t\tret = -EFSCORRUPTED;\n\t\tgoto err;\n\t}\n\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tif (depth > path[0].p_maxdepth) {\n\t\t\tkfree(path);\n\t\t\t*orig_path = path = NULL;\n\t\t}\n\t}\n\tif (!path) {\n\t\t/* account possible depth increase */\n\t\tpath = kcalloc(depth + 2, sizeof(struct ext4_ext_path),\n\t\t\t\tGFP_NOFS);\n\t\tif (unlikely(!path))\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\tpath[0].p_maxdepth = depth + 1;\n\t}\n\tpath[0].p_hdr = eh;\n\tpath[0].p_bh = NULL;\n\n\ti = depth;\n\t/* walk through the tree */\n\twhile (i) {\n\t\text_debug(\"depth %d: num %d, max %d\\n\",\n\t\t\t  ppos, le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\n\t\text4_ext_binsearch_idx(inode, path + ppos, block);\n\t\tpath[ppos].p_block = ext4_idx_pblock(path[ppos].p_idx);\n\t\tpath[ppos].p_depth = i;\n\t\tpath[ppos].p_ext = NULL;\n\n\t\tbh = read_extent_tree_block(inode, path[ppos].p_block, --i,\n\t\t\t\t\t    flags);\n\t\tif (IS_ERR(bh)) {\n\t\t\tret = PTR_ERR(bh);\n\t\t\tgoto err;\n\t\t}\n\n\t\teh = ext_block_hdr(bh);\n\t\tppos++;\n\t\tpath[ppos].p_bh = bh;\n\t\tpath[ppos].p_hdr = eh;\n\t}\n\n\tpath[ppos].p_depth = i;\n\tpath[ppos].p_ext = NULL;\n\tpath[ppos].p_idx = NULL;\n\n\t/* find extent */\n\text4_ext_binsearch(inode, path + ppos, block);\n\t/* if not an empty leaf */\n\tif (path[ppos].p_ext)\n\t\tpath[ppos].p_block = ext4_ext_pblock(path[ppos].p_ext);\n\n\text4_ext_show_path(inode, path);\n\n\treturn path;\n\nerr:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\tif (orig_path)\n\t\t*orig_path = NULL;\n\treturn ERR_PTR(ret);\n}\n\n/*\n * ext4_ext_insert_index:\n * insert new index [@logical;@ptr] into the block at @curp;\n * check where to insert: before @curp or after @curp\n */\nstatic int ext4_ext_insert_index(handle_t *handle, struct inode *inode,\n\t\t\t\t struct ext4_ext_path *curp,\n\t\t\t\t int logical, ext4_fsblk_t ptr)\n{\n\tstruct ext4_extent_idx *ix;\n\tint len, err;\n\n\terr = ext4_ext_get_access(handle, inode, curp);\n\tif (err)\n\t\treturn err;\n\n\tif (unlikely(logical == le32_to_cpu(curp->p_idx->ei_block))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d == ei_block %d!\",\n\t\t\t\t logical, le32_to_cpu(curp->p_idx->ei_block));\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\tif (unlikely(le16_to_cpu(curp->p_hdr->eh_entries)\n\t\t\t     >= le16_to_cpu(curp->p_hdr->eh_max))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"eh_entries %d >= eh_max %d!\",\n\t\t\t\t le16_to_cpu(curp->p_hdr->eh_entries),\n\t\t\t\t le16_to_cpu(curp->p_hdr->eh_max));\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\tif (logical > le32_to_cpu(curp->p_idx->ei_block)) {\n\t\t/* insert after */\n\t\text_debug(\"insert new index %d after: %llu\\n\", logical, ptr);\n\t\tix = curp->p_idx + 1;\n\t} else {\n\t\t/* insert before */\n\t\text_debug(\"insert new index %d before: %llu\\n\", logical, ptr);\n\t\tix = curp->p_idx;\n\t}\n\n\tlen = EXT_LAST_INDEX(curp->p_hdr) - ix + 1;\n\tBUG_ON(len < 0);\n\tif (len > 0) {\n\t\text_debug(\"insert new index %d: \"\n\t\t\t\t\"move %d indices from 0x%p to 0x%p\\n\",\n\t\t\t\tlogical, len, ix, ix + 1);\n\t\tmemmove(ix + 1, ix, len * sizeof(struct ext4_extent_idx));\n\t}\n\n\tif (unlikely(ix > EXT_MAX_INDEX(curp->p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"ix > EXT_MAX_INDEX!\");\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\tix->ei_block = cpu_to_le32(logical);\n\text4_idx_store_pblock(ix, ptr);\n\tle16_add_cpu(&curp->p_hdr->eh_entries, 1);\n\n\tif (unlikely(ix > EXT_LAST_INDEX(curp->p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"ix > EXT_LAST_INDEX!\");\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\terr = ext4_ext_dirty(handle, inode, curp);\n\text4_std_error(inode->i_sb, err);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_split:\n * inserts new subtree into the path, using free index entry\n * at depth @at:\n * - allocates all needed blocks (new leaf and all intermediate index blocks)\n * - makes decision where to split\n * - moves remaining extents and index entries (right to the split point)\n *   into the newly allocated blocks\n * - initializes subtree\n */\nstatic int ext4_ext_split(handle_t *handle, struct inode *inode,\n\t\t\t  unsigned int flags,\n\t\t\t  struct ext4_ext_path *path,\n\t\t\t  struct ext4_extent *newext, int at)\n{\n\tstruct buffer_head *bh = NULL;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *neh;\n\tstruct ext4_extent_idx *fidx;\n\tint i = at, k, m, a;\n\text4_fsblk_t newblock, oldblock;\n\t__le32 border;\n\text4_fsblk_t *ablocks = NULL; /* array of allocated blocks */\n\tint err = 0;\n\tsize_t ext_size = 0;\n\n\t/* make decision: where to split? */\n\t/* FIXME: now decision is simplest: at current extent */\n\n\t/* if current leaf will be split, then we should use\n\t * border from split point */\n\tif (unlikely(path[depth].p_ext > EXT_MAX_EXTENT(path[depth].p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"p_ext > EXT_MAX_EXTENT!\");\n\t\treturn -EFSCORRUPTED;\n\t}\n\tif (path[depth].p_ext != EXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\tborder = path[depth].p_ext[1].ee_block;\n\t\text_debug(\"leaf will be split.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\t  le32_to_cpu(border));\n\t} else {\n\t\tborder = newext->ee_block;\n\t\text_debug(\"leaf will be added.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\tle32_to_cpu(border));\n\t}\n\n\t/*\n\t * If error occurs, then we break processing\n\t * and mark filesystem read-only. index won't\n\t * be inserted and tree will be in consistent\n\t * state. Next mount will repair buffers too.\n\t */\n\n\t/*\n\t * Get array to track all allocated blocks.\n\t * We need this to handle errors and free blocks\n\t * upon them.\n\t */\n\tablocks = kcalloc(depth, sizeof(ext4_fsblk_t), GFP_NOFS);\n\tif (!ablocks)\n\t\treturn -ENOMEM;\n\n\t/* allocate all needed blocks */\n\text_debug(\"allocate %d blocks for indexes/leaf\\n\", depth - at);\n\tfor (a = 0; a < depth - at; a++) {\n\t\tnewblock = ext4_ext_new_meta_block(handle, inode, path,\n\t\t\t\t\t\t   newext, &err, flags);\n\t\tif (newblock == 0)\n\t\t\tgoto cleanup;\n\t\tablocks[a] = newblock;\n\t}\n\n\t/* initialize new leaf */\n\tnewblock = ablocks[--a];\n\tif (unlikely(newblock == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"newblock == 0!\");\n\t\terr = -EFSCORRUPTED;\n\t\tgoto cleanup;\n\t}\n\tbh = sb_getblk_gfp(inode->i_sb, newblock, __GFP_MOVABLE | GFP_NOFS);\n\tif (unlikely(!bh)) {\n\t\terr = -ENOMEM;\n\t\tgoto cleanup;\n\t}\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err)\n\t\tgoto cleanup;\n\n\tneh = ext_block_hdr(bh);\n\tneh->eh_entries = 0;\n\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\tneh->eh_depth = 0;\n\n\t/* move remainder of path[depth] to the new leaf */\n\tif (unlikely(path[depth].p_hdr->eh_entries !=\n\t\t     path[depth].p_hdr->eh_max)) {\n\t\tEXT4_ERROR_INODE(inode, \"eh_entries %d != eh_max %d!\",\n\t\t\t\t path[depth].p_hdr->eh_entries,\n\t\t\t\t path[depth].p_hdr->eh_max);\n\t\terr = -EFSCORRUPTED;\n\t\tgoto cleanup;\n\t}\n\t/* start copy from next extent */\n\tm = EXT_MAX_EXTENT(path[depth].p_hdr) - path[depth].p_ext++;\n\text4_ext_show_move(inode, path, newblock, depth);\n\tif (m) {\n\t\tstruct ext4_extent *ex;\n\t\tex = EXT_FIRST_EXTENT(neh);\n\t\tmemmove(ex, path[depth].p_ext, sizeof(struct ext4_extent) * m);\n\t\tle16_add_cpu(&neh->eh_entries, m);\n\t}\n\n\t/* zero out unused area in the extent block */\n\text_size = sizeof(struct ext4_extent_header) +\n\t\tsizeof(struct ext4_extent) * le16_to_cpu(neh->eh_entries);\n\tmemset(bh->b_data + ext_size, 0, inode->i_sb->s_blocksize - ext_size);\n\text4_extent_block_csum_set(inode, neh);\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto cleanup;\n\tbrelse(bh);\n\tbh = NULL;\n\n\t/* correct old leaf */\n\tif (m) {\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tle16_add_cpu(&path[depth].p_hdr->eh_entries, -m);\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t}\n\n\t/* create intermediate indexes */\n\tk = depth - at - 1;\n\tif (unlikely(k < 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"k %d < 0!\", k);\n\t\terr = -EFSCORRUPTED;\n\t\tgoto cleanup;\n\t}\n\tif (k)\n\t\text_debug(\"create %d intermediate indices\\n\", k);\n\t/* insert new index into current index block */\n\t/* current depth stored in i var */\n\ti = depth - 1;\n\twhile (k--) {\n\t\toldblock = newblock;\n\t\tnewblock = ablocks[--a];\n\t\tbh = sb_getblk(inode->i_sb, newblock);\n\t\tif (unlikely(!bh)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tlock_buffer(bh);\n\n\t\terr = ext4_journal_get_create_access(handle, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t\tneh = ext_block_hdr(bh);\n\t\tneh->eh_entries = cpu_to_le16(1);\n\t\tneh->eh_magic = EXT4_EXT_MAGIC;\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\t\tneh->eh_depth = cpu_to_le16(depth - i);\n\t\tfidx = EXT_FIRST_INDEX(neh);\n\t\tfidx->ei_block = border;\n\t\text4_idx_store_pblock(fidx, oldblock);\n\n\t\text_debug(\"int.index at %d (block %llu): %u -> %llu\\n\",\n\t\t\t\ti, newblock, le32_to_cpu(border), oldblock);\n\n\t\t/* move remainder of path[i] to the new index block */\n\t\tif (unlikely(EXT_MAX_INDEX(path[i].p_hdr) !=\n\t\t\t\t\tEXT_LAST_INDEX(path[i].p_hdr))) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"EXT_MAX_INDEX != EXT_LAST_INDEX ee_block %d!\",\n\t\t\t\t\t le32_to_cpu(path[i].p_ext->ee_block));\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tgoto cleanup;\n\t\t}\n\t\t/* start copy indexes */\n\t\tm = EXT_MAX_INDEX(path[i].p_hdr) - path[i].p_idx++;\n\t\text_debug(\"cur 0x%p, last 0x%p\\n\", path[i].p_idx,\n\t\t\t\tEXT_MAX_INDEX(path[i].p_hdr));\n\t\text4_ext_show_move(inode, path, newblock, i);\n\t\tif (m) {\n\t\t\tmemmove(++fidx, path[i].p_idx,\n\t\t\t\tsizeof(struct ext4_extent_idx) * m);\n\t\t\tle16_add_cpu(&neh->eh_entries, m);\n\t\t}\n\t\t/* zero out unused area in the extent block */\n\t\text_size = sizeof(struct ext4_extent_header) +\n\t\t   (sizeof(struct ext4_extent) * le16_to_cpu(neh->eh_entries));\n\t\tmemset(bh->b_data + ext_size, 0,\n\t\t\tinode->i_sb->s_blocksize - ext_size);\n\t\text4_extent_block_csum_set(inode, neh);\n\t\tset_buffer_uptodate(bh);\n\t\tunlock_buffer(bh);\n\n\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tbrelse(bh);\n\t\tbh = NULL;\n\n\t\t/* correct old index */\n\t\tif (m) {\n\t\t\terr = ext4_ext_get_access(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t\tle16_add_cpu(&path[i].p_hdr->eh_entries, -m);\n\t\t\terr = ext4_ext_dirty(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t}\n\n\t\ti--;\n\t}\n\n\t/* insert new index */\n\terr = ext4_ext_insert_index(handle, inode, path + at,\n\t\t\t\t    le32_to_cpu(border), newblock);\n\ncleanup:\n\tif (bh) {\n\t\tif (buffer_locked(bh))\n\t\t\tunlock_buffer(bh);\n\t\tbrelse(bh);\n\t}\n\n\tif (err) {\n\t\t/* free all allocated blocks in error case */\n\t\tfor (i = 0; i < depth; i++) {\n\t\t\tif (!ablocks[i])\n\t\t\t\tcontinue;\n\t\t\text4_free_blocks(handle, inode, NULL, ablocks[i], 1,\n\t\t\t\t\t EXT4_FREE_BLOCKS_METADATA);\n\t\t}\n\t}\n\tkfree(ablocks);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_grow_indepth:\n * implements tree growing procedure:\n * - allocates new block\n * - moves top-level data (index block or leaf) into the new block\n * - initializes new top-level, creating index that points to the\n *   just created block\n */\nstatic int ext4_ext_grow_indepth(handle_t *handle, struct inode *inode,\n\t\t\t\t unsigned int flags)\n{\n\tstruct ext4_extent_header *neh;\n\tstruct buffer_head *bh;\n\text4_fsblk_t newblock, goal = 0;\n\tstruct ext4_super_block *es = EXT4_SB(inode->i_sb)->s_es;\n\tint err = 0;\n\tsize_t ext_size = 0;\n\n\t/* Try to prepend new index to old one */\n\tif (ext_depth(inode))\n\t\tgoal = ext4_idx_pblock(EXT_FIRST_INDEX(ext_inode_hdr(inode)));\n\tif (goal > le32_to_cpu(es->s_first_data_block)) {\n\t\tflags |= EXT4_MB_HINT_TRY_GOAL;\n\t\tgoal--;\n\t} else\n\t\tgoal = ext4_inode_to_goal_block(inode);\n\tnewblock = ext4_new_meta_blocks(handle, inode, goal, flags,\n\t\t\t\t\tNULL, &err);\n\tif (newblock == 0)\n\t\treturn err;\n\n\tbh = sb_getblk_gfp(inode->i_sb, newblock, __GFP_MOVABLE | GFP_NOFS);\n\tif (unlikely(!bh))\n\t\treturn -ENOMEM;\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err) {\n\t\tunlock_buffer(bh);\n\t\tgoto out;\n\t}\n\n\text_size = sizeof(EXT4_I(inode)->i_data);\n\t/* move top-level index/leaf into new block */\n\tmemmove(bh->b_data, EXT4_I(inode)->i_data, ext_size);\n\t/* zero out unused area in the extent block */\n\tmemset(bh->b_data + ext_size, 0, inode->i_sb->s_blocksize - ext_size);\n\n\t/* set size of new block */\n\tneh = ext_block_hdr(bh);\n\t/* old root could have indexes or leaves\n\t * so calculate e_max right way */\n\tif (ext_depth(inode))\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\telse\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\text4_extent_block_csum_set(inode, neh);\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto out;\n\n\t/* Update top-level index: num,max,pointer */\n\tneh = ext_inode_hdr(inode);\n\tneh->eh_entries = cpu_to_le16(1);\n\text4_idx_store_pblock(EXT_FIRST_INDEX(neh), newblock);\n\tif (neh->eh_depth == 0) {\n\t\t/* Root extent block becomes index block */\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_root_idx(inode, 0));\n\t\tEXT_FIRST_INDEX(neh)->ei_block =\n\t\t\tEXT_FIRST_EXTENT(neh)->ee_block;\n\t}\n\text_debug(\"new root: num %d(%d), lblock %d, ptr %llu\\n\",\n\t\t  le16_to_cpu(neh->eh_entries), le16_to_cpu(neh->eh_max),\n\t\t  le32_to_cpu(EXT_FIRST_INDEX(neh)->ei_block),\n\t\t  ext4_idx_pblock(EXT_FIRST_INDEX(neh)));\n\n\tle16_add_cpu(&neh->eh_depth, 1);\n\text4_mark_inode_dirty(handle, inode);\nout:\n\tbrelse(bh);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_create_new_leaf:\n * finds empty index and adds new leaf.\n * if no free index is found, then it requests in-depth growing.\n */\nstatic int ext4_ext_create_new_leaf(handle_t *handle, struct inode *inode,\n\t\t\t\t    unsigned int mb_flags,\n\t\t\t\t    unsigned int gb_flags,\n\t\t\t\t    struct ext4_ext_path **ppath,\n\t\t\t\t    struct ext4_extent *newext)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_ext_path *curp;\n\tint depth, i, err = 0;\n\nrepeat:\n\ti = depth = ext_depth(inode);\n\n\t/* walk up to the tree and look for free index entry */\n\tcurp = path + depth;\n\twhile (i > 0 && !EXT_HAS_FREE_INDEX(curp)) {\n\t\ti--;\n\t\tcurp--;\n\t}\n\n\t/* we use already allocated block for index block,\n\t * so subsequent data blocks should be contiguous */\n\tif (EXT_HAS_FREE_INDEX(curp)) {\n\t\t/* if we found index with free entry, then use that\n\t\t * entry: create all needed subtree and add new leaf */\n\t\terr = ext4_ext_split(handle, inode, mb_flags, path, newext, i);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\tpath = ext4_find_extent(inode,\n\t\t\t\t    (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    ppath, gb_flags);\n\t\tif (IS_ERR(path))\n\t\t\terr = PTR_ERR(path);\n\t} else {\n\t\t/* tree is full, time to grow in depth */\n\t\terr = ext4_ext_grow_indepth(handle, inode, mb_flags);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\tpath = ext4_find_extent(inode,\n\t\t\t\t   (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    ppath, gb_flags);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * only first (depth 0 -> 1) produces free space;\n\t\t * in all other cases we have to split the grown tree\n\t\t */\n\t\tdepth = ext_depth(inode);\n\t\tif (path[depth].p_hdr->eh_entries == path[depth].p_hdr->eh_max) {\n\t\t\t/* now we need to split */\n\t\t\tgoto repeat;\n\t\t}\n\t}\n\nout:\n\treturn err;\n}\n\n/*\n * search the closest allocated block to the left for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the smallest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nstatic int ext4_ext_search_left(struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\text4_lblk_t *logical, ext4_fsblk_t *phys)\n{\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\tint depth, ee_len;\n\n\tif (unlikely(path == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path == NULL *logical %d!\", *logical);\n\t\treturn -EFSCORRUPTED;\n\t}\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tif (unlikely(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"EXT_FIRST_EXTENT != ex *logical %d ee_block %d!\",\n\t\t\t\t\t *logical, le32_to_cpu(ex->ee_block));\n\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tif (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t  \"ix (%d) != EXT_FIRST_INDEX (%d) (depth %d)!\",\n\t\t\t\t  ix != NULL ? le32_to_cpu(ix->ei_block) : 0,\n\t\t\t\t  EXT_FIRST_INDEX(path[depth].p_hdr) != NULL ?\n\t\tle32_to_cpu(EXT_FIRST_INDEX(path[depth].p_hdr)->ei_block) : 0,\n\t\t\t\t  depth);\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t\t}\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d < ee_block %d + ee_len %d!\",\n\t\t\t\t *logical, le32_to_cpu(ex->ee_block), ee_len);\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\t*logical = le32_to_cpu(ex->ee_block) + ee_len - 1;\n\t*phys = ext4_ext_pblock(ex) + ee_len - 1;\n\treturn 0;\n}\n\n/*\n * search the closest allocated block to the right for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the largest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nstatic int ext4_ext_search_right(struct inode *inode,\n\t\t\t\t struct ext4_ext_path *path,\n\t\t\t\t ext4_lblk_t *logical, ext4_fsblk_t *phys,\n\t\t\t\t struct ext4_extent **ret_ex)\n{\n\tstruct buffer_head *bh = NULL;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\text4_fsblk_t block;\n\tint depth;\t/* Note, NOT eh_depth; depth from top of tree */\n\tint ee_len;\n\n\tif (unlikely(path == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path == NULL *logical %d!\", *logical);\n\t\treturn -EFSCORRUPTED;\n\t}\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tif (unlikely(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"first_extent(path[%d].p_hdr) != ex\",\n\t\t\t\t\t depth);\n\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tif (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t\t \"ix != EXT_FIRST_INDEX *logical %d!\",\n\t\t\t\t\t\t *logical);\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t\t}\n\t\t}\n\t\tgoto found_extent;\n\t}\n\n\tif (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d < ee_block %d + ee_len %d!\",\n\t\t\t\t *logical, le32_to_cpu(ex->ee_block), ee_len);\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\tif (ex != EXT_LAST_EXTENT(path[depth].p_hdr)) {\n\t\t/* next allocated block in this leaf */\n\t\tex++;\n\t\tgoto found_extent;\n\t}\n\n\t/* go up and search for index to the right */\n\twhile (--depth >= 0) {\n\t\tix = path[depth].p_idx;\n\t\tif (ix != EXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\tgoto got_index;\n\t}\n\n\t/* we've gone up to the root and found no index to the right */\n\treturn 0;\n\ngot_index:\n\t/* we've found index to the right, let's\n\t * follow it and find the closest allocated\n\t * block to the right */\n\tix++;\n\tblock = ext4_idx_pblock(ix);\n\twhile (++depth < path->p_depth) {\n\t\t/* subtract from p_depth to get proper eh_depth */\n\t\tbh = read_extent_tree_block(inode, block,\n\t\t\t\t\t    path->p_depth - depth, 0);\n\t\tif (IS_ERR(bh))\n\t\t\treturn PTR_ERR(bh);\n\t\teh = ext_block_hdr(bh);\n\t\tix = EXT_FIRST_INDEX(eh);\n\t\tblock = ext4_idx_pblock(ix);\n\t\tput_bh(bh);\n\t}\n\n\tbh = read_extent_tree_block(inode, block, path->p_depth - depth, 0);\n\tif (IS_ERR(bh))\n\t\treturn PTR_ERR(bh);\n\teh = ext_block_hdr(bh);\n\tex = EXT_FIRST_EXTENT(eh);\nfound_extent:\n\t*logical = le32_to_cpu(ex->ee_block);\n\t*phys = ext4_ext_pblock(ex);\n\t*ret_ex = ex;\n\tif (bh)\n\t\tput_bh(bh);\n\treturn 0;\n}\n\n/*\n * ext4_ext_next_allocated_block:\n * returns allocated block in subsequent extent or EXT_MAX_BLOCKS.\n * NOTE: it considers block number from index entry as\n * allocated block. Thus, index entries have to be consistent\n * with leaves.\n */\next4_lblk_t\next4_ext_next_allocated_block(struct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn EXT_MAX_BLOCKS;\n\n\twhile (depth >= 0) {\n\t\tif (depth == path->p_depth) {\n\t\t\t/* leaf */\n\t\t\tif (path[depth].p_ext &&\n\t\t\t\tpath[depth].p_ext !=\n\t\t\t\t\tEXT_LAST_EXTENT(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_ext[1].ee_block);\n\t\t} else {\n\t\t\t/* index */\n\t\t\tif (path[depth].p_idx !=\n\t\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\t}\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCKS;\n}\n\n/*\n * ext4_ext_next_leaf_block:\n * returns first allocated block from next leaf or EXT_MAX_BLOCKS\n */\nstatic ext4_lblk_t ext4_ext_next_leaf_block(struct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\t/* zero-tree has no leaf blocks at all */\n\tif (depth == 0)\n\t\treturn EXT_MAX_BLOCKS;\n\n\t/* go to index block */\n\tdepth--;\n\n\twhile (depth >= 0) {\n\t\tif (path[depth].p_idx !=\n\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\treturn (ext4_lblk_t)\n\t\t\t\tle32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCKS;\n}\n\n/*\n * ext4_ext_correct_indexes:\n * if leaf gets modified and modified extent is first in the leaf,\n * then we have to correct all indexes above.\n * TODO: do we need to correct tree in all cases?\n */\nstatic int ext4_ext_correct_indexes(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tstruct ext4_extent_header *eh;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent *ex;\n\t__le32 border;\n\tint k, err = 0;\n\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\n\tif (unlikely(ex == NULL || eh == NULL)) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"ex %p == NULL or eh %p == NULL\", ex, eh);\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\tif (depth == 0) {\n\t\t/* there is no tree at all */\n\t\treturn 0;\n\t}\n\n\tif (ex != EXT_FIRST_EXTENT(eh)) {\n\t\t/* we correct tree if first leaf got modified only */\n\t\treturn 0;\n\t}\n\n\t/*\n\t * TODO: we need correction if border is smaller than current one\n\t */\n\tk = depth - 1;\n\tborder = path[depth].p_ext->ee_block;\n\terr = ext4_ext_get_access(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\tpath[k].p_idx->ei_block = border;\n\terr = ext4_ext_dirty(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\n\twhile (k--) {\n\t\t/* change all left-side indexes */\n\t\tif (path[k+1].p_idx != EXT_FIRST_INDEX(path[k+1].p_hdr))\n\t\t\tbreak;\n\t\terr = ext4_ext_get_access(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t\tpath[k].p_idx->ei_block = border;\n\t\terr = ext4_ext_dirty(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nint\next4_can_extents_be_merged(struct inode *inode, struct ext4_extent *ex1,\n\t\t\t\tstruct ext4_extent *ex2)\n{\n\tunsigned short ext1_ee_len, ext2_ee_len;\n\n\tif (ext4_ext_is_unwritten(ex1) != ext4_ext_is_unwritten(ex2))\n\t\treturn 0;\n\n\text1_ee_len = ext4_ext_get_actual_len(ex1);\n\text2_ee_len = ext4_ext_get_actual_len(ex2);\n\n\tif (le32_to_cpu(ex1->ee_block) + ext1_ee_len !=\n\t\t\tle32_to_cpu(ex2->ee_block))\n\t\treturn 0;\n\n\t/*\n\t * To allow future support for preallocated extents to be added\n\t * as an RO_COMPAT feature, refuse to merge to extents if\n\t * this can result in the top bit of ee_len being set.\n\t */\n\tif (ext1_ee_len + ext2_ee_len > EXT_INIT_MAX_LEN)\n\t\treturn 0;\n\t/*\n\t * The check for IO to unwritten extent is somewhat racy as we\n\t * increment i_unwritten / set EXT4_STATE_DIO_UNWRITTEN only after\n\t * dropping i_data_sem. But reserved blocks should save us in that\n\t * case.\n\t */\n\tif (ext4_ext_is_unwritten(ex1) &&\n\t    (ext4_test_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN) ||\n\t     atomic_read(&EXT4_I(inode)->i_unwritten) ||\n\t     (ext1_ee_len + ext2_ee_len > EXT_UNWRITTEN_MAX_LEN)))\n\t\treturn 0;\n#ifdef AGGRESSIVE_TEST\n\tif (ext1_ee_len >= 4)\n\t\treturn 0;\n#endif\n\n\tif (ext4_ext_pblock(ex1) + ext1_ee_len == ext4_ext_pblock(ex2))\n\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * This function tries to merge the \"ex\" extent to the next extent in the tree.\n * It always tries to merge towards right. If you want to merge towards\n * left, pass \"ex - 1\" as argument instead of \"ex\".\n * Returns 0 if the extents (ex and ex+1) were _not_ merged and returns\n * 1 if they got merged.\n */\nstatic int ext4_ext_try_to_merge_right(struct inode *inode,\n\t\t\t\t struct ext4_ext_path *path,\n\t\t\t\t struct ext4_extent *ex)\n{\n\tstruct ext4_extent_header *eh;\n\tunsigned int depth, len;\n\tint merge_done = 0, unwritten;\n\n\tdepth = ext_depth(inode);\n\tBUG_ON(path[depth].p_hdr == NULL);\n\teh = path[depth].p_hdr;\n\n\twhile (ex < EXT_LAST_EXTENT(eh)) {\n\t\tif (!ext4_can_extents_be_merged(inode, ex, ex + 1))\n\t\t\tbreak;\n\t\t/* merge with next extent! */\n\t\tunwritten = ext4_ext_is_unwritten(ex);\n\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t+ ext4_ext_get_actual_len(ex + 1));\n\t\tif (unwritten)\n\t\t\text4_ext_mark_unwritten(ex);\n\n\t\tif (ex + 1 < EXT_LAST_EXTENT(eh)) {\n\t\t\tlen = (EXT_LAST_EXTENT(eh) - ex - 1)\n\t\t\t\t* sizeof(struct ext4_extent);\n\t\t\tmemmove(ex + 1, ex + 2, len);\n\t\t}\n\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\tmerge_done = 1;\n\t\tWARN_ON(eh->eh_entries == 0);\n\t\tif (!eh->eh_entries)\n\t\t\tEXT4_ERROR_INODE(inode, \"eh->eh_entries = 0!\");\n\t}\n\n\treturn merge_done;\n}\n\n/*\n * This function does a very simple check to see if we can collapse\n * an extent tree with a single extent tree leaf block into the inode.\n */\nstatic void ext4_ext_try_to_merge_up(handle_t *handle,\n\t\t\t\t     struct inode *inode,\n\t\t\t\t     struct ext4_ext_path *path)\n{\n\tsize_t s;\n\tunsigned max_root = ext4_ext_space_root(inode, 0);\n\text4_fsblk_t blk;\n\n\tif ((path[0].p_depth != 1) ||\n\t    (le16_to_cpu(path[0].p_hdr->eh_entries) != 1) ||\n\t    (le16_to_cpu(path[1].p_hdr->eh_entries) > max_root))\n\t\treturn;\n\n\t/*\n\t * We need to modify the block allocation bitmap and the block\n\t * group descriptor to release the extent tree block.  If we\n\t * can't get the journal credits, give up.\n\t */\n\tif (ext4_journal_extend(handle, 2))\n\t\treturn;\n\n\t/*\n\t * Copy the extent data up to the inode\n\t */\n\tblk = ext4_idx_pblock(path[0].p_idx);\n\ts = le16_to_cpu(path[1].p_hdr->eh_entries) *\n\t\tsizeof(struct ext4_extent_idx);\n\ts += sizeof(struct ext4_extent_header);\n\n\tpath[1].p_maxdepth = path[0].p_maxdepth;\n\tmemcpy(path[0].p_hdr, path[1].p_hdr, s);\n\tpath[0].p_depth = 0;\n\tpath[0].p_ext = EXT_FIRST_EXTENT(path[0].p_hdr) +\n\t\t(path[1].p_ext - EXT_FIRST_EXTENT(path[1].p_hdr));\n\tpath[0].p_hdr->eh_max = cpu_to_le16(max_root);\n\n\tbrelse(path[1].p_bh);\n\text4_free_blocks(handle, inode, NULL, blk, 1,\n\t\t\t EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);\n}\n\n/*\n * This function tries to merge the @ex extent to neighbours in the tree.\n * return 1 if merge left else 0.\n */\nstatic void ext4_ext_try_to_merge(handle_t *handle,\n\t\t\t\t  struct inode *inode,\n\t\t\t\t  struct ext4_ext_path *path,\n\t\t\t\t  struct ext4_extent *ex) {\n\tstruct ext4_extent_header *eh;\n\tunsigned int depth;\n\tint merge_done = 0;\n\n\tdepth = ext_depth(inode);\n\tBUG_ON(path[depth].p_hdr == NULL);\n\teh = path[depth].p_hdr;\n\n\tif (ex > EXT_FIRST_EXTENT(eh))\n\t\tmerge_done = ext4_ext_try_to_merge_right(inode, path, ex - 1);\n\n\tif (!merge_done)\n\t\t(void) ext4_ext_try_to_merge_right(inode, path, ex);\n\n\text4_ext_try_to_merge_up(handle, inode, path);\n}\n\n/*\n * check if a portion of the \"newext\" extent overlaps with an\n * existing extent.\n *\n * If there is an overlap discovered, it updates the length of the newext\n * such that there will be no overlap, and then returns 1.\n * If there is no overlap found, it returns 0.\n */\nstatic unsigned int ext4_ext_check_overlap(struct ext4_sb_info *sbi,\n\t\t\t\t\t   struct inode *inode,\n\t\t\t\t\t   struct ext4_extent *newext,\n\t\t\t\t\t   struct ext4_ext_path *path)\n{\n\text4_lblk_t b1, b2;\n\tunsigned int depth, len1;\n\tunsigned int ret = 0;\n\n\tb1 = le32_to_cpu(newext->ee_block);\n\tlen1 = ext4_ext_get_actual_len(newext);\n\tdepth = ext_depth(inode);\n\tif (!path[depth].p_ext)\n\t\tgoto out;\n\tb2 = EXT4_LBLK_CMASK(sbi, le32_to_cpu(path[depth].p_ext->ee_block));\n\n\t/*\n\t * get the next allocated block if the extent in the path\n\t * is before the requested block(s)\n\t */\n\tif (b2 < b1) {\n\t\tb2 = ext4_ext_next_allocated_block(path);\n\t\tif (b2 == EXT_MAX_BLOCKS)\n\t\t\tgoto out;\n\t\tb2 = EXT4_LBLK_CMASK(sbi, b2);\n\t}\n\n\t/* check for wrap through zero on extent logical start block*/\n\tif (b1 + len1 < b1) {\n\t\tlen1 = EXT_MAX_BLOCKS - b1;\n\t\tnewext->ee_len = cpu_to_le16(len1);\n\t\tret = 1;\n\t}\n\n\t/* check for overlap */\n\tif (b1 + len1 > b2) {\n\t\tnewext->ee_len = cpu_to_le16(b2 - b1);\n\t\tret = 1;\n\t}\nout:\n\treturn ret;\n}\n\n/*\n * ext4_ext_insert_extent:\n * tries to merge requsted extent into the existing extent or\n * inserts requested extent as new one into the tree,\n * creating new leaf in the no-space case.\n */\nint ext4_ext_insert_extent(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path **ppath,\n\t\t\t\tstruct ext4_extent *newext, int gb_flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex, *fex;\n\tstruct ext4_extent *nearex; /* nearest extent */\n\tstruct ext4_ext_path *npath = NULL;\n\tint depth, len, err;\n\text4_lblk_t next;\n\tint mb_flags = 0, unwritten;\n\n\tif (gb_flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\tmb_flags |= EXT4_MB_DELALLOC_RESERVED;\n\tif (unlikely(ext4_ext_get_actual_len(newext) == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"ext4_ext_get_actual_len(newext) == 0\");\n\t\treturn -EFSCORRUPTED;\n\t}\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\teh = path[depth].p_hdr;\n\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\t/* try to insert block into found extent and return */\n\tif (ex && !(gb_flags & EXT4_GET_BLOCKS_PRE_IO)) {\n\n\t\t/*\n\t\t * Try to see whether we should rather test the extent on\n\t\t * right from ex, or from the left of ex. This is because\n\t\t * ext4_find_extent() can return either extent on the\n\t\t * left, or on the right from the searched position. This\n\t\t * will make merging more effective.\n\t\t */\n\t\tif (ex < EXT_LAST_EXTENT(eh) &&\n\t\t    (le32_to_cpu(ex->ee_block) +\n\t\t    ext4_ext_get_actual_len(ex) <\n\t\t    le32_to_cpu(newext->ee_block))) {\n\t\t\tex += 1;\n\t\t\tgoto prepend;\n\t\t} else if ((ex > EXT_FIRST_EXTENT(eh)) &&\n\t\t\t   (le32_to_cpu(newext->ee_block) +\n\t\t\t   ext4_ext_get_actual_len(newext) <\n\t\t\t   le32_to_cpu(ex->ee_block)))\n\t\t\tex -= 1;\n\n\t\t/* Try to append newex to the ex */\n\t\tif (ext4_can_extents_be_merged(inode, ex, newext)) {\n\t\t\text_debug(\"append [%d]%d block to %u:[%d]%d\"\n\t\t\t\t  \"(from %llu)\\n\",\n\t\t\t\t  ext4_ext_is_unwritten(newext),\n\t\t\t\t  ext4_ext_get_actual_len(newext),\n\t\t\t\t  le32_to_cpu(ex->ee_block),\n\t\t\t\t  ext4_ext_is_unwritten(ex),\n\t\t\t\t  ext4_ext_get_actual_len(ex),\n\t\t\t\t  ext4_ext_pblock(ex));\n\t\t\terr = ext4_ext_get_access(handle, inode,\n\t\t\t\t\t\t  path + depth);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tunwritten = ext4_ext_is_unwritten(ex);\n\t\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t\t+ ext4_ext_get_actual_len(newext));\n\t\t\tif (unwritten)\n\t\t\t\text4_ext_mark_unwritten(ex);\n\t\t\teh = path[depth].p_hdr;\n\t\t\tnearex = ex;\n\t\t\tgoto merge;\n\t\t}\n\nprepend:\n\t\t/* Try to prepend newex to the ex */\n\t\tif (ext4_can_extents_be_merged(inode, newext, ex)) {\n\t\t\text_debug(\"prepend %u[%d]%d block to %u:[%d]%d\"\n\t\t\t\t  \"(from %llu)\\n\",\n\t\t\t\t  le32_to_cpu(newext->ee_block),\n\t\t\t\t  ext4_ext_is_unwritten(newext),\n\t\t\t\t  ext4_ext_get_actual_len(newext),\n\t\t\t\t  le32_to_cpu(ex->ee_block),\n\t\t\t\t  ext4_ext_is_unwritten(ex),\n\t\t\t\t  ext4_ext_get_actual_len(ex),\n\t\t\t\t  ext4_ext_pblock(ex));\n\t\t\terr = ext4_ext_get_access(handle, inode,\n\t\t\t\t\t\t  path + depth);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tunwritten = ext4_ext_is_unwritten(ex);\n\t\t\tex->ee_block = newext->ee_block;\n\t\t\text4_ext_store_pblock(ex, ext4_ext_pblock(newext));\n\t\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t\t+ ext4_ext_get_actual_len(newext));\n\t\t\tif (unwritten)\n\t\t\t\text4_ext_mark_unwritten(ex);\n\t\t\teh = path[depth].p_hdr;\n\t\t\tnearex = ex;\n\t\t\tgoto merge;\n\t\t}\n\t}\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max))\n\t\tgoto has_space;\n\n\t/* probably next leaf has space for us? */\n\tfex = EXT_LAST_EXTENT(eh);\n\tnext = EXT_MAX_BLOCKS;\n\tif (le32_to_cpu(newext->ee_block) > le32_to_cpu(fex->ee_block))\n\t\tnext = ext4_ext_next_leaf_block(path);\n\tif (next != EXT_MAX_BLOCKS) {\n\t\text_debug(\"next leaf block - %u\\n\", next);\n\t\tBUG_ON(npath != NULL);\n\t\tnpath = ext4_find_extent(inode, next, NULL, 0);\n\t\tif (IS_ERR(npath))\n\t\t\treturn PTR_ERR(npath);\n\t\tBUG_ON(npath->p_depth != path->p_depth);\n\t\teh = npath[depth].p_hdr;\n\t\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max)) {\n\t\t\text_debug(\"next leaf isn't full(%d)\\n\",\n\t\t\t\t  le16_to_cpu(eh->eh_entries));\n\t\t\tpath = npath;\n\t\t\tgoto has_space;\n\t\t}\n\t\text_debug(\"next leaf has no free space(%d,%d)\\n\",\n\t\t\t  le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\t}\n\n\t/*\n\t * There is no free space in the found leaf.\n\t * We're gonna add a new leaf in the tree.\n\t */\n\tif (gb_flags & EXT4_GET_BLOCKS_METADATA_NOFAIL)\n\t\tmb_flags |= EXT4_MB_USE_RESERVED;\n\terr = ext4_ext_create_new_leaf(handle, inode, mb_flags, gb_flags,\n\t\t\t\t       ppath, newext);\n\tif (err)\n\t\tgoto cleanup;\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\nhas_space:\n\tnearex = path[depth].p_ext;\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto cleanup;\n\n\tif (!nearex) {\n\t\t/* there is no extent in this leaf, create first one */\n\t\text_debug(\"first extent in the leaf: %u:%llu:[%d]%d\\n\",\n\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\text4_ext_pblock(newext),\n\t\t\t\text4_ext_is_unwritten(newext),\n\t\t\t\text4_ext_get_actual_len(newext));\n\t\tnearex = EXT_FIRST_EXTENT(eh);\n\t} else {\n\t\tif (le32_to_cpu(newext->ee_block)\n\t\t\t   > le32_to_cpu(nearex->ee_block)) {\n\t\t\t/* Insert after */\n\t\t\text_debug(\"insert %u:%llu:[%d]%d before: \"\n\t\t\t\t\t\"nearest %p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_unwritten(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tnearex);\n\t\t\tnearex++;\n\t\t} else {\n\t\t\t/* Insert before */\n\t\t\tBUG_ON(newext->ee_block == nearex->ee_block);\n\t\t\text_debug(\"insert %u:%llu:[%d]%d after: \"\n\t\t\t\t\t\"nearest %p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_unwritten(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tnearex);\n\t\t}\n\t\tlen = EXT_LAST_EXTENT(eh) - nearex + 1;\n\t\tif (len > 0) {\n\t\t\text_debug(\"insert %u:%llu:[%d]%d: \"\n\t\t\t\t\t\"move %d extents from 0x%p to 0x%p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_unwritten(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tlen, nearex, nearex + 1);\n\t\t\tmemmove(nearex + 1, nearex,\n\t\t\t\tlen * sizeof(struct ext4_extent));\n\t\t}\n\t}\n\n\tle16_add_cpu(&eh->eh_entries, 1);\n\tpath[depth].p_ext = nearex;\n\tnearex->ee_block = newext->ee_block;\n\text4_ext_store_pblock(nearex, ext4_ext_pblock(newext));\n\tnearex->ee_len = newext->ee_len;\n\nmerge:\n\t/* try to merge extents */\n\tif (!(gb_flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\text4_ext_try_to_merge(handle, inode, path, nearex);\n\n\n\t/* time to correct all indexes above */\n\terr = ext4_ext_correct_indexes(handle, inode, path);\n\tif (err)\n\t\tgoto cleanup;\n\n\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\ncleanup:\n\text4_ext_drop_refs(npath);\n\tkfree(npath);\n\treturn err;\n}\n\nstatic int ext4_fill_fiemap_extents(struct inode *inode,\n\t\t\t\t    ext4_lblk_t block, ext4_lblk_t num,\n\t\t\t\t    struct fiemap_extent_info *fieinfo)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_extent *ex;\n\tstruct extent_status es;\n\text4_lblk_t next, next_del, start = 0, end = 0;\n\text4_lblk_t last = block + num;\n\tint exists, depth = 0, err = 0;\n\tunsigned int flags = 0;\n\tunsigned char blksize_bits = inode->i_sb->s_blocksize_bits;\n\n\twhile (block < last && block != EXT_MAX_BLOCKS) {\n\t\tnum = last - block;\n\t\t/* find extent for this block */\n\t\tdown_read(&EXT4_I(inode)->i_data_sem);\n\n\t\tpath = ext4_find_extent(inode, block, &path, 0);\n\t\tif (IS_ERR(path)) {\n\t\t\tup_read(&EXT4_I(inode)->i_data_sem);\n\t\t\terr = PTR_ERR(path);\n\t\t\tpath = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tdepth = ext_depth(inode);\n\t\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\t\tup_read(&EXT4_I(inode)->i_data_sem);\n\t\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tbreak;\n\t\t}\n\t\tex = path[depth].p_ext;\n\t\tnext = ext4_ext_next_allocated_block(path);\n\n\t\tflags = 0;\n\t\texists = 0;\n\t\tif (!ex) {\n\t\t\t/* there is no extent yet, so try to allocate\n\t\t\t * all requested space */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t} else if (le32_to_cpu(ex->ee_block) > block) {\n\t\t\t/* need to allocate space before found extent */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)\n\t\t\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\t\t/* need to allocate space after found extent */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t\tif (end >= next)\n\t\t\t\tend = next;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)) {\n\t\t\t/*\n\t\t\t * some part of requested space is covered\n\t\t\t * by found extent\n\t\t\t */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block)\n\t\t\t\t+ ext4_ext_get_actual_len(ex);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t\texists = 1;\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n\t\tBUG_ON(end <= start);\n\n\t\tif (!exists) {\n\t\t\tes.es_lblk = start;\n\t\t\tes.es_len = end - start;\n\t\t\tes.es_pblk = 0;\n\t\t} else {\n\t\t\tes.es_lblk = le32_to_cpu(ex->ee_block);\n\t\t\tes.es_len = ext4_ext_get_actual_len(ex);\n\t\t\tes.es_pblk = ext4_ext_pblock(ex);\n\t\t\tif (ext4_ext_is_unwritten(ex))\n\t\t\t\tflags |= FIEMAP_EXTENT_UNWRITTEN;\n\t\t}\n\n\t\t/*\n\t\t * Find delayed extent and update es accordingly. We call\n\t\t * it even in !exists case to find out whether es is the\n\t\t * last existing extent or not.\n\t\t */\n\t\tnext_del = ext4_find_delayed_extent(inode, &es);\n\t\tif (!exists && next_del) {\n\t\t\texists = 1;\n\t\t\tflags |= (FIEMAP_EXTENT_DELALLOC |\n\t\t\t\t  FIEMAP_EXTENT_UNKNOWN);\n\t\t}\n\t\tup_read(&EXT4_I(inode)->i_data_sem);\n\n\t\tif (unlikely(es.es_len == 0)) {\n\t\t\tEXT4_ERROR_INODE(inode, \"es.es_len == 0\");\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * This is possible iff next == next_del == EXT_MAX_BLOCKS.\n\t\t * we need to check next == EXT_MAX_BLOCKS because it is\n\t\t * possible that an extent is with unwritten and delayed\n\t\t * status due to when an extent is delayed allocated and\n\t\t * is allocated by fallocate status tree will track both of\n\t\t * them in a extent.\n\t\t *\n\t\t * So we could return a unwritten and delayed extent, and\n\t\t * its block is equal to 'next'.\n\t\t */\n\t\tif (next == next_del && next == EXT_MAX_BLOCKS) {\n\t\t\tflags |= FIEMAP_EXTENT_LAST;\n\t\t\tif (unlikely(next_del != EXT_MAX_BLOCKS ||\n\t\t\t\t     next != EXT_MAX_BLOCKS)) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t\t \"next extent == %u, next \"\n\t\t\t\t\t\t \"delalloc extent = %u\",\n\t\t\t\t\t\t next, next_del);\n\t\t\t\terr = -EFSCORRUPTED;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (exists) {\n\t\t\terr = fiemap_fill_next_extent(fieinfo,\n\t\t\t\t(__u64)es.es_lblk << blksize_bits,\n\t\t\t\t(__u64)es.es_pblk << blksize_bits,\n\t\t\t\t(__u64)es.es_len << blksize_bits,\n\t\t\t\tflags);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t\tif (err == 1) {\n\t\t\t\terr = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tblock = es.es_lblk + es.es_len;\n\t}\n\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\treturn err;\n}\n\n/*\n * ext4_ext_determine_hole - determine hole around given block\n * @inode:\tinode we lookup in\n * @path:\tpath in extent tree to @lblk\n * @lblk:\tpointer to logical block around which we want to determine hole\n *\n * Determine hole length (and start if easily possible) around given logical\n * block. We don't try too hard to find the beginning of the hole but @path\n * actually points to extent before @lblk, we provide it.\n *\n * The function returns the length of a hole starting at @lblk. We update @lblk\n * to the beginning of the hole if we managed to find it.\n */\nstatic ext4_lblk_t ext4_ext_determine_hole(struct inode *inode,\n\t\t\t\t\t   struct ext4_ext_path *path,\n\t\t\t\t\t   ext4_lblk_t *lblk)\n{\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent *ex;\n\text4_lblk_t len;\n\n\tex = path[depth].p_ext;\n\tif (ex == NULL) {\n\t\t/* there is no extent yet, so gap is [0;-] */\n\t\t*lblk = 0;\n\t\tlen = EXT_MAX_BLOCKS;\n\t} else if (*lblk < le32_to_cpu(ex->ee_block)) {\n\t\tlen = le32_to_cpu(ex->ee_block) - *lblk;\n\t} else if (*lblk >= le32_to_cpu(ex->ee_block)\n\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\text4_lblk_t next;\n\n\t\t*lblk = le32_to_cpu(ex->ee_block) + ext4_ext_get_actual_len(ex);\n\t\tnext = ext4_ext_next_allocated_block(path);\n\t\tBUG_ON(next == *lblk);\n\t\tlen = next - *lblk;\n\t} else {\n\t\tBUG();\n\t}\n\treturn len;\n}\n\n/*\n * ext4_ext_put_gap_in_cache:\n * calculate boundaries of the gap that the requested block fits into\n * and cache this gap\n */\nstatic void\next4_ext_put_gap_in_cache(struct inode *inode, ext4_lblk_t hole_start,\n\t\t\t  ext4_lblk_t hole_len)\n{\n\tstruct extent_status es;\n\n\text4_es_find_extent_range(inode, &ext4_es_is_delayed, hole_start,\n\t\t\t\t  hole_start + hole_len - 1, &es);\n\tif (es.es_len) {\n\t\t/* There's delayed extent containing lblock? */\n\t\tif (es.es_lblk <= hole_start)\n\t\t\treturn;\n\t\thole_len = min(es.es_lblk - hole_start, hole_len);\n\t}\n\text_debug(\" -> %u:%u\\n\", hole_start, hole_len);\n\text4_es_insert_extent(inode, hole_start, hole_len, ~0,\n\t\t\t      EXTENT_STATUS_HOLE);\n}\n\n/*\n * ext4_ext_rm_idx:\n * removes index from the index block.\n */\nstatic int ext4_ext_rm_idx(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path, int depth)\n{\n\tint err;\n\text4_fsblk_t leaf;\n\n\t/* free index block */\n\tdepth--;\n\tpath = path + depth;\n\tleaf = ext4_idx_pblock(path->p_idx);\n\tif (unlikely(path->p_hdr->eh_entries == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"path->p_hdr->eh_entries == 0\");\n\t\treturn -EFSCORRUPTED;\n\t}\n\terr = ext4_ext_get_access(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\n\tif (path->p_idx != EXT_LAST_INDEX(path->p_hdr)) {\n\t\tint len = EXT_LAST_INDEX(path->p_hdr) - path->p_idx;\n\t\tlen *= sizeof(struct ext4_extent_idx);\n\t\tmemmove(path->p_idx, path->p_idx + 1, len);\n\t}\n\n\tle16_add_cpu(&path->p_hdr->eh_entries, -1);\n\terr = ext4_ext_dirty(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\text_debug(\"index is empty, remove it, free block %llu\\n\", leaf);\n\ttrace_ext4_ext_rm_idx(inode, leaf);\n\n\text4_free_blocks(handle, inode, NULL, leaf, 1,\n\t\t\t EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);\n\n\twhile (--depth >= 0) {\n\t\tif (path->p_idx != EXT_FIRST_INDEX(path->p_hdr))\n\t\t\tbreak;\n\t\tpath--;\n\t\terr = ext4_ext_get_access(handle, inode, path);\n\t\tif (err)\n\t\t\tbreak;\n\t\tpath->p_idx->ei_block = (path+1)->p_idx->ei_block;\n\t\terr = ext4_ext_dirty(handle, inode, path);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\treturn err;\n}\n\n/*\n * ext4_ext_calc_credits_for_single_extent:\n * This routine returns max. credits that needed to insert an extent\n * to the extent tree.\n * When pass the actual path, the caller should calculate credits\n * under i_data_sem.\n */\nint ext4_ext_calc_credits_for_single_extent(struct inode *inode, int nrblocks,\n\t\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path) {\n\t\tint depth = ext_depth(inode);\n\t\tint ret = 0;\n\n\t\t/* probably there is space in leaf? */\n\t\tif (le16_to_cpu(path[depth].p_hdr->eh_entries)\n\t\t\t\t< le16_to_cpu(path[depth].p_hdr->eh_max)) {\n\n\t\t\t/*\n\t\t\t *  There are some space in the leaf tree, no\n\t\t\t *  need to account for leaf block credit\n\t\t\t *\n\t\t\t *  bitmaps and block group descriptor blocks\n\t\t\t *  and other metadata blocks still need to be\n\t\t\t *  accounted.\n\t\t\t */\n\t\t\t/* 1 bitmap, 1 block group descriptor */\n\t\t\tret = 2 + EXT4_META_TRANS_BLOCKS(inode->i_sb);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn ext4_chunk_trans_blocks(inode, nrblocks);\n}\n\n/*\n * How many index/leaf blocks need to change/allocate to add @extents extents?\n *\n * If we add a single extent, then in the worse case, each tree level\n * index/leaf need to be changed in case of the tree split.\n *\n * If more extents are inserted, they could cause the whole tree split more\n * than once, but this is really rare.\n */\nint ext4_ext_index_trans_blocks(struct inode *inode, int extents)\n{\n\tint index;\n\tint depth;\n\n\t/* If we are converting the inline data, only one is needed here. */\n\tif (ext4_has_inline_data(inode))\n\t\treturn 1;\n\n\tdepth = ext_depth(inode);\n\n\tif (extents <= 1)\n\t\tindex = depth * 2;\n\telse\n\t\tindex = depth * 3;\n\n\treturn index;\n}\n\nstatic inline int get_default_free_blocks_flags(struct inode *inode)\n{\n\tif (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode) ||\n\t    ext4_test_inode_flag(inode, EXT4_INODE_EA_INODE))\n\t\treturn EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET;\n\telse if (ext4_should_journal_data(inode))\n\t\treturn EXT4_FREE_BLOCKS_FORGET;\n\treturn 0;\n}\n\n/*\n * ext4_rereserve_cluster - increment the reserved cluster count when\n *                          freeing a cluster with a pending reservation\n *\n * @inode - file containing the cluster\n * @lblk - logical block in cluster to be reserved\n *\n * Increments the reserved cluster count and adjusts quota in a bigalloc\n * file system when freeing a partial cluster containing at least one\n * delayed and unwritten block.  A partial cluster meeting that\n * requirement will have a pending reservation.  If so, the\n * RERESERVE_CLUSTER flag is used when calling ext4_free_blocks() to\n * defer reserved and allocated space accounting to a subsequent call\n * to this function.\n */\nstatic void ext4_rereserve_cluster(struct inode *inode, ext4_lblk_t lblk)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\n\tdquot_reclaim_block(inode, EXT4_C2B(sbi, 1));\n\n\tspin_lock(&ei->i_block_reservation_lock);\n\tei->i_reserved_data_blocks++;\n\tpercpu_counter_add(&sbi->s_dirtyclusters_counter, 1);\n\tspin_unlock(&ei->i_block_reservation_lock);\n\n\tpercpu_counter_add(&sbi->s_freeclusters_counter, 1);\n\text4_remove_pending(inode, lblk);\n}\n\nstatic int ext4_remove_blocks(handle_t *handle, struct inode *inode,\n\t\t\t      struct ext4_extent *ex,\n\t\t\t      struct partial_cluster *partial,\n\t\t\t      ext4_lblk_t from, ext4_lblk_t to)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tunsigned short ee_len = ext4_ext_get_actual_len(ex);\n\text4_fsblk_t last_pblk, pblk;\n\text4_lblk_t num;\n\tint flags;\n\n\t/* only extent tail removal is allowed */\n\tif (from < le32_to_cpu(ex->ee_block) ||\n\t    to != le32_to_cpu(ex->ee_block) + ee_len - 1) {\n\t\text4_error(sbi->s_sb,\n\t\t\t   \"strange request: removal(2) %u-%u from %u:%u\",\n\t\t\t   from, to, le32_to_cpu(ex->ee_block), ee_len);\n\t\treturn 0;\n\t}\n\n#ifdef EXTENTS_STATS\n\tspin_lock(&sbi->s_ext_stats_lock);\n\tsbi->s_ext_blocks += ee_len;\n\tsbi->s_ext_extents++;\n\tif (ee_len < sbi->s_ext_min)\n\t\tsbi->s_ext_min = ee_len;\n\tif (ee_len > sbi->s_ext_max)\n\t\tsbi->s_ext_max = ee_len;\n\tif (ext_depth(inode) > sbi->s_depth_max)\n\t\tsbi->s_depth_max = ext_depth(inode);\n\tspin_unlock(&sbi->s_ext_stats_lock);\n#endif\n\n\ttrace_ext4_remove_blocks(inode, ex, from, to, partial);\n\n\t/*\n\t * if we have a partial cluster, and it's different from the\n\t * cluster of the last block in the extent, we free it\n\t */\n\tlast_pblk = ext4_ext_pblock(ex) + ee_len - 1;\n\n\tif (partial->state != initial &&\n\t    partial->pclu != EXT4_B2C(sbi, last_pblk)) {\n\t\tif (partial->state == tofree) {\n\t\t\tflags = get_default_free_blocks_flags(inode);\n\t\t\tif (ext4_is_pending(inode, partial->lblk))\n\t\t\t\tflags |= EXT4_FREE_BLOCKS_RERESERVE_CLUSTER;\n\t\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t\t EXT4_C2B(sbi, partial->pclu),\n\t\t\t\t\t sbi->s_cluster_ratio, flags);\n\t\t\tif (flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)\n\t\t\t\text4_rereserve_cluster(inode, partial->lblk);\n\t\t}\n\t\tpartial->state = initial;\n\t}\n\n\tnum = le32_to_cpu(ex->ee_block) + ee_len - from;\n\tpblk = ext4_ext_pblock(ex) + ee_len - num;\n\n\t/*\n\t * We free the partial cluster at the end of the extent (if any),\n\t * unless the cluster is used by another extent (partial_cluster\n\t * state is nofree).  If a partial cluster exists here, it must be\n\t * shared with the last block in the extent.\n\t */\n\tflags = get_default_free_blocks_flags(inode);\n\n\t/* partial, left end cluster aligned, right end unaligned */\n\tif ((EXT4_LBLK_COFF(sbi, to) != sbi->s_cluster_ratio - 1) &&\n\t    (EXT4_LBLK_CMASK(sbi, to) >= from) &&\n\t    (partial->state != nofree)) {\n\t\tif (ext4_is_pending(inode, to))\n\t\t\tflags |= EXT4_FREE_BLOCKS_RERESERVE_CLUSTER;\n\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t EXT4_PBLK_CMASK(sbi, last_pblk),\n\t\t\t\t sbi->s_cluster_ratio, flags);\n\t\tif (flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)\n\t\t\text4_rereserve_cluster(inode, to);\n\t\tpartial->state = initial;\n\t\tflags = get_default_free_blocks_flags(inode);\n\t}\n\n\tflags |= EXT4_FREE_BLOCKS_NOFREE_LAST_CLUSTER;\n\n\t/*\n\t * For bigalloc file systems, we never free a partial cluster\n\t * at the beginning of the extent.  Instead, we check to see if we\n\t * need to free it on a subsequent call to ext4_remove_blocks,\n\t * or at the end of ext4_ext_rm_leaf or ext4_ext_remove_space.\n\t */\n\tflags |= EXT4_FREE_BLOCKS_NOFREE_FIRST_CLUSTER;\n\text4_free_blocks(handle, inode, NULL, pblk, num, flags);\n\n\t/* reset the partial cluster if we've freed past it */\n\tif (partial->state != initial && partial->pclu != EXT4_B2C(sbi, pblk))\n\t\tpartial->state = initial;\n\n\t/*\n\t * If we've freed the entire extent but the beginning is not left\n\t * cluster aligned and is not marked as ineligible for freeing we\n\t * record the partial cluster at the beginning of the extent.  It\n\t * wasn't freed by the preceding ext4_free_blocks() call, and we\n\t * need to look farther to the left to determine if it's to be freed\n\t * (not shared with another extent). Else, reset the partial\n\t * cluster - we're either  done freeing or the beginning of the\n\t * extent is left cluster aligned.\n\t */\n\tif (EXT4_LBLK_COFF(sbi, from) && num == ee_len) {\n\t\tif (partial->state == initial) {\n\t\t\tpartial->pclu = EXT4_B2C(sbi, pblk);\n\t\t\tpartial->lblk = from;\n\t\t\tpartial->state = tofree;\n\t\t}\n\t} else {\n\t\tpartial->state = initial;\n\t}\n\n\treturn 0;\n}\n\n/*\n * ext4_ext_rm_leaf() Removes the extents associated with the\n * blocks appearing between \"start\" and \"end\".  Both \"start\"\n * and \"end\" must appear in the same extent or EIO is returned.\n *\n * @handle: The journal handle\n * @inode:  The files inode\n * @path:   The path to the leaf\n * @partial_cluster: The cluster which we'll have to free if all extents\n *                   has been released from it.  However, if this value is\n *                   negative, it's a cluster just to the right of the\n *                   punched region and it must not be freed.\n * @start:  The first block to remove\n * @end:   The last block to remove\n */\nstatic int\next4_ext_rm_leaf(handle_t *handle, struct inode *inode,\n\t\t struct ext4_ext_path *path,\n\t\t struct partial_cluster *partial,\n\t\t ext4_lblk_t start, ext4_lblk_t end)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tint err = 0, correct_index = 0;\n\tint depth = ext_depth(inode), credits;\n\tstruct ext4_extent_header *eh;\n\text4_lblk_t a, b;\n\tunsigned num;\n\text4_lblk_t ex_ee_block;\n\tunsigned short ex_ee_len;\n\tunsigned unwritten = 0;\n\tstruct ext4_extent *ex;\n\text4_fsblk_t pblk;\n\n\t/* the header must be checked already in ext4_ext_remove_space() */\n\text_debug(\"truncate since %u in leaf to %u\\n\", start, end);\n\tif (!path[depth].p_hdr)\n\t\tpath[depth].p_hdr = ext_block_hdr(path[depth].p_bh);\n\teh = path[depth].p_hdr;\n\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\treturn -EFSCORRUPTED;\n\t}\n\t/* find where to start removing */\n\tex = path[depth].p_ext;\n\tif (!ex)\n\t\tex = EXT_LAST_EXTENT(eh);\n\n\tex_ee_block = le32_to_cpu(ex->ee_block);\n\tex_ee_len = ext4_ext_get_actual_len(ex);\n\n\ttrace_ext4_ext_rm_leaf(inode, start, ex, partial);\n\n\twhile (ex >= EXT_FIRST_EXTENT(eh) &&\n\t\t\tex_ee_block + ex_ee_len > start) {\n\n\t\tif (ext4_ext_is_unwritten(ex))\n\t\t\tunwritten = 1;\n\t\telse\n\t\t\tunwritten = 0;\n\n\t\text_debug(\"remove ext %u:[%d]%d\\n\", ex_ee_block,\n\t\t\t  unwritten, ex_ee_len);\n\t\tpath[depth].p_ext = ex;\n\n\t\ta = ex_ee_block > start ? ex_ee_block : start;\n\t\tb = ex_ee_block+ex_ee_len - 1 < end ?\n\t\t\tex_ee_block+ex_ee_len - 1 : end;\n\n\t\text_debug(\"  border %u:%u\\n\", a, b);\n\n\t\t/* If this extent is beyond the end of the hole, skip it */\n\t\tif (end < ex_ee_block) {\n\t\t\t/*\n\t\t\t * We're going to skip this extent and move to another,\n\t\t\t * so note that its first cluster is in use to avoid\n\t\t\t * freeing it when removing blocks.  Eventually, the\n\t\t\t * right edge of the truncated/punched region will\n\t\t\t * be just to the left.\n\t\t\t */\n\t\t\tif (sbi->s_cluster_ratio > 1) {\n\t\t\t\tpblk = ext4_ext_pblock(ex);\n\t\t\t\tpartial->pclu = EXT4_B2C(sbi, pblk);\n\t\t\t\tpartial->state = nofree;\n\t\t\t}\n\t\t\tex--;\n\t\t\tex_ee_block = le32_to_cpu(ex->ee_block);\n\t\t\tex_ee_len = ext4_ext_get_actual_len(ex);\n\t\t\tcontinue;\n\t\t} else if (b != ex_ee_block + ex_ee_len - 1) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"can not handle truncate %u:%u \"\n\t\t\t\t\t \"on extent %u:%u\",\n\t\t\t\t\t start, end, ex_ee_block,\n\t\t\t\t\t ex_ee_block + ex_ee_len - 1);\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tgoto out;\n\t\t} else if (a != ex_ee_block) {\n\t\t\t/* remove tail of the extent */\n\t\t\tnum = a - ex_ee_block;\n\t\t} else {\n\t\t\t/* remove whole extent: excellent! */\n\t\t\tnum = 0;\n\t\t}\n\t\t/*\n\t\t * 3 for leaf, sb, and inode plus 2 (bmap and group\n\t\t * descriptor) for each block group; assume two block\n\t\t * groups plus ex_ee_len/blocks_per_block_group for\n\t\t * the worst case\n\t\t */\n\t\tcredits = 7 + 2*(ex_ee_len/EXT4_BLOCKS_PER_GROUP(inode->i_sb));\n\t\tif (ex == EXT_FIRST_EXTENT(eh)) {\n\t\t\tcorrect_index = 1;\n\t\t\tcredits += (ext_depth(inode)) + 1;\n\t\t}\n\t\tcredits += EXT4_MAXQUOTAS_TRANS_BLOCKS(inode->i_sb);\n\n\t\terr = ext4_ext_truncate_extend_restart(handle, inode, credits);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_remove_blocks(handle, inode, ex, partial, a, b);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (num == 0)\n\t\t\t/* this extent is removed; mark slot entirely unused */\n\t\t\text4_ext_store_pblock(ex, 0);\n\n\t\tex->ee_len = cpu_to_le16(num);\n\t\t/*\n\t\t * Do not mark unwritten if all the blocks in the\n\t\t * extent have been removed.\n\t\t */\n\t\tif (unwritten && num)\n\t\t\text4_ext_mark_unwritten(ex);\n\t\t/*\n\t\t * If the extent was completely released,\n\t\t * we need to remove it from the leaf\n\t\t */\n\t\tif (num == 0) {\n\t\t\tif (end != EXT_MAX_BLOCKS - 1) {\n\t\t\t\t/*\n\t\t\t\t * For hole punching, we need to scoot all the\n\t\t\t\t * extents up when an extent is removed so that\n\t\t\t\t * we dont have blank extents in the middle\n\t\t\t\t */\n\t\t\t\tmemmove(ex, ex+1, (EXT_LAST_EXTENT(eh) - ex) *\n\t\t\t\t\tsizeof(struct ext4_extent));\n\n\t\t\t\t/* Now get rid of the one at the end */\n\t\t\t\tmemset(EXT_LAST_EXTENT(eh), 0,\n\t\t\t\t\tsizeof(struct ext4_extent));\n\t\t\t}\n\t\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\t}\n\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\text_debug(\"new extent: %u:%u:%llu\\n\", ex_ee_block, num,\n\t\t\t\text4_ext_pblock(ex));\n\t\tex--;\n\t\tex_ee_block = le32_to_cpu(ex->ee_block);\n\t\tex_ee_len = ext4_ext_get_actual_len(ex);\n\t}\n\n\tif (correct_index && eh->eh_entries)\n\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\n\t/*\n\t * If there's a partial cluster and at least one extent remains in\n\t * the leaf, free the partial cluster if it isn't shared with the\n\t * current extent.  If it is shared with the current extent\n\t * we reset the partial cluster because we've reached the start of the\n\t * truncated/punched region and we're done removing blocks.\n\t */\n\tif (partial->state == tofree && ex >= EXT_FIRST_EXTENT(eh)) {\n\t\tpblk = ext4_ext_pblock(ex) + ex_ee_len - 1;\n\t\tif (partial->pclu != EXT4_B2C(sbi, pblk)) {\n\t\t\tint flags = get_default_free_blocks_flags(inode);\n\n\t\t\tif (ext4_is_pending(inode, partial->lblk))\n\t\t\t\tflags |= EXT4_FREE_BLOCKS_RERESERVE_CLUSTER;\n\t\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t\t EXT4_C2B(sbi, partial->pclu),\n\t\t\t\t\t sbi->s_cluster_ratio, flags);\n\t\t\tif (flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)\n\t\t\t\text4_rereserve_cluster(inode, partial->lblk);\n\t\t}\n\t\tpartial->state = initial;\n\t}\n\n\t/* if this leaf is free, then we should\n\t * remove it from index block above */\n\tif (err == 0 && eh->eh_entries == 0 && path[depth].p_bh != NULL)\n\t\terr = ext4_ext_rm_idx(handle, inode, path, depth);\n\nout:\n\treturn err;\n}\n\n/*\n * ext4_ext_more_to_rm:\n * returns 1 if current index has to be freed (even partial)\n */\nstatic int\next4_ext_more_to_rm(struct ext4_ext_path *path)\n{\n\tBUG_ON(path->p_idx == NULL);\n\n\tif (path->p_idx < EXT_FIRST_INDEX(path->p_hdr))\n\t\treturn 0;\n\n\t/*\n\t * if truncate on deeper level happened, it wasn't partial,\n\t * so we have to consider current index for truncation\n\t */\n\tif (le16_to_cpu(path->p_hdr->eh_entries) == path->p_block)\n\t\treturn 0;\n\treturn 1;\n}\n\nint ext4_ext_remove_space(struct inode *inode, ext4_lblk_t start,\n\t\t\t  ext4_lblk_t end)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tint depth = ext_depth(inode);\n\tstruct ext4_ext_path *path = NULL;\n\tstruct partial_cluster partial;\n\thandle_t *handle;\n\tint i = 0, err = 0;\n\n\tpartial.pclu = 0;\n\tpartial.lblk = 0;\n\tpartial.state = initial;\n\n\text_debug(\"truncate since %u to %u\\n\", start, end);\n\n\t/* probably first extent we're gonna free will be last in block */\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, depth + 1);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\nagain:\n\ttrace_ext4_ext_remove_space(inode, start, end, depth);\n\n\t/*\n\t * Check if we are removing extents inside the extent tree. If that\n\t * is the case, we are going to punch a hole inside the extent tree\n\t * so we have to check whether we need to split the extent covering\n\t * the last block to remove so we can easily remove the part of it\n\t * in ext4_ext_rm_leaf().\n\t */\n\tif (end < EXT_MAX_BLOCKS - 1) {\n\t\tstruct ext4_extent *ex;\n\t\text4_lblk_t ee_block, ex_end, lblk;\n\t\text4_fsblk_t pblk;\n\n\t\t/* find extent for or closest extent to this block */\n\t\tpath = ext4_find_extent(inode, end, NULL, EXT4_EX_NOCACHE);\n\t\tif (IS_ERR(path)) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn PTR_ERR(path);\n\t\t}\n\t\tdepth = ext_depth(inode);\n\t\t/* Leaf not may not exist only if inode has no blocks at all */\n\t\tex = path[depth].p_ext;\n\t\tif (!ex) {\n\t\t\tif (depth) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t\t \"path[%d].p_hdr == NULL\",\n\t\t\t\t\t\t depth);\n\t\t\t\terr = -EFSCORRUPTED;\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\n\t\tee_block = le32_to_cpu(ex->ee_block);\n\t\tex_end = ee_block + ext4_ext_get_actual_len(ex) - 1;\n\n\t\t/*\n\t\t * See if the last block is inside the extent, if so split\n\t\t * the extent at 'end' block so we can easily remove the\n\t\t * tail of the first part of the split extent in\n\t\t * ext4_ext_rm_leaf().\n\t\t */\n\t\tif (end >= ee_block && end < ex_end) {\n\n\t\t\t/*\n\t\t\t * If we're going to split the extent, note that\n\t\t\t * the cluster containing the block after 'end' is\n\t\t\t * in use to avoid freeing it when removing blocks.\n\t\t\t */\n\t\t\tif (sbi->s_cluster_ratio > 1) {\n\t\t\t\tpblk = ext4_ext_pblock(ex) + end - ee_block + 2;\n\t\t\t\tpartial.pclu = EXT4_B2C(sbi, pblk);\n\t\t\t\tpartial.state = nofree;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Split the extent in two so that 'end' is the last\n\t\t\t * block in the first new extent. Also we should not\n\t\t\t * fail removing space due to ENOSPC so try to use\n\t\t\t * reserved block if that happens.\n\t\t\t */\n\t\t\terr = ext4_force_split_extent_at(handle, inode, &path,\n\t\t\t\t\t\t\t end + 1, 1);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t} else if (sbi->s_cluster_ratio > 1 && end >= ex_end &&\n\t\t\t   partial.state == initial) {\n\t\t\t/*\n\t\t\t * If we're punching, there's an extent to the right.\n\t\t\t * If the partial cluster hasn't been set, set it to\n\t\t\t * that extent's first cluster and its state to nofree\n\t\t\t * so it won't be freed should it contain blocks to be\n\t\t\t * removed. If it's already set (tofree/nofree), we're\n\t\t\t * retrying and keep the original partial cluster info\n\t\t\t * so a cluster marked tofree as a result of earlier\n\t\t\t * extent removal is not lost.\n\t\t\t */\n\t\t\tlblk = ex_end + 1;\n\t\t\terr = ext4_ext_search_right(inode, path, &lblk, &pblk,\n\t\t\t\t\t\t    &ex);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tif (pblk) {\n\t\t\t\tpartial.pclu = EXT4_B2C(sbi, pblk);\n\t\t\t\tpartial.state = nofree;\n\t\t\t}\n\t\t}\n\t}\n\t/*\n\t * We start scanning from right side, freeing all the blocks\n\t * after i_size and walking into the tree depth-wise.\n\t */\n\tdepth = ext_depth(inode);\n\tif (path) {\n\t\tint k = i = depth;\n\t\twhile (--k > 0)\n\t\t\tpath[k].p_block =\n\t\t\t\tle16_to_cpu(path[k].p_hdr->eh_entries)+1;\n\t} else {\n\t\tpath = kcalloc(depth + 1, sizeof(struct ext4_ext_path),\n\t\t\t       GFP_NOFS);\n\t\tif (path == NULL) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tpath[0].p_maxdepth = path[0].p_depth = depth;\n\t\tpath[0].p_hdr = ext_inode_hdr(inode);\n\t\ti = 0;\n\n\t\tif (ext4_ext_check(inode, path[0].p_hdr, depth, 0)) {\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tgoto out;\n\t\t}\n\t}\n\terr = 0;\n\n\twhile (i >= 0 && err == 0) {\n\t\tif (i == depth) {\n\t\t\t/* this is leaf block */\n\t\t\terr = ext4_ext_rm_leaf(handle, inode, path,\n\t\t\t\t\t       &partial, start, end);\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* this is index block */\n\t\tif (!path[i].p_hdr) {\n\t\t\text_debug(\"initialize header\\n\");\n\t\t\tpath[i].p_hdr = ext_block_hdr(path[i].p_bh);\n\t\t}\n\n\t\tif (!path[i].p_idx) {\n\t\t\t/* this level hasn't been touched yet */\n\t\t\tpath[i].p_idx = EXT_LAST_INDEX(path[i].p_hdr);\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries)+1;\n\t\t\text_debug(\"init index ptr: hdr 0x%p, num %d\\n\",\n\t\t\t\t  path[i].p_hdr,\n\t\t\t\t  le16_to_cpu(path[i].p_hdr->eh_entries));\n\t\t} else {\n\t\t\t/* we were already here, see at next index */\n\t\t\tpath[i].p_idx--;\n\t\t}\n\n\t\text_debug(\"level %d - index, first 0x%p, cur 0x%p\\n\",\n\t\t\t\ti, EXT_FIRST_INDEX(path[i].p_hdr),\n\t\t\t\tpath[i].p_idx);\n\t\tif (ext4_ext_more_to_rm(path + i)) {\n\t\t\tstruct buffer_head *bh;\n\t\t\t/* go to the next level */\n\t\t\text_debug(\"move to level %d (block %llu)\\n\",\n\t\t\t\t  i + 1, ext4_idx_pblock(path[i].p_idx));\n\t\t\tmemset(path + i + 1, 0, sizeof(*path));\n\t\t\tbh = read_extent_tree_block(inode,\n\t\t\t\text4_idx_pblock(path[i].p_idx), depth - i - 1,\n\t\t\t\tEXT4_EX_NOCACHE);\n\t\t\tif (IS_ERR(bh)) {\n\t\t\t\t/* should we reset i_size? */\n\t\t\t\terr = PTR_ERR(bh);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* Yield here to deal with large extent trees.\n\t\t\t * Should be a no-op if we did IO above. */\n\t\t\tcond_resched();\n\t\t\tif (WARN_ON(i + 1 > depth)) {\n\t\t\t\terr = -EFSCORRUPTED;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpath[i + 1].p_bh = bh;\n\n\t\t\t/* save actual number of indexes since this\n\t\t\t * number is changed at the next iteration */\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries);\n\t\t\ti++;\n\t\t} else {\n\t\t\t/* we finished processing this index, go up */\n\t\t\tif (path[i].p_hdr->eh_entries == 0 && i > 0) {\n\t\t\t\t/* index is empty, remove it;\n\t\t\t\t * handle must be already prepared by the\n\t\t\t\t * truncatei_leaf() */\n\t\t\t\terr = ext4_ext_rm_idx(handle, inode, path, i);\n\t\t\t}\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\text_debug(\"return to level %d\\n\", i);\n\t\t}\n\t}\n\n\ttrace_ext4_ext_remove_space_done(inode, start, end, depth, &partial,\n\t\t\t\t\t path->p_hdr->eh_entries);\n\n\t/*\n\t * if there's a partial cluster and we have removed the first extent\n\t * in the file, then we also free the partial cluster, if any\n\t */\n\tif (partial.state == tofree && err == 0) {\n\t\tint flags = get_default_free_blocks_flags(inode);\n\n\t\tif (ext4_is_pending(inode, partial.lblk))\n\t\t\tflags |= EXT4_FREE_BLOCKS_RERESERVE_CLUSTER;\n\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t EXT4_C2B(sbi, partial.pclu),\n\t\t\t\t sbi->s_cluster_ratio, flags);\n\t\tif (flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)\n\t\t\text4_rereserve_cluster(inode, partial.lblk);\n\t\tpartial.state = initial;\n\t}\n\n\t/* TODO: flexible tree reduction should be here */\n\tif (path->p_hdr->eh_entries == 0) {\n\t\t/*\n\t\t * truncate to zero freed all the tree,\n\t\t * so we need to correct eh_depth\n\t\t */\n\t\terr = ext4_ext_get_access(handle, inode, path);\n\t\tif (err == 0) {\n\t\t\text_inode_hdr(inode)->eh_depth = 0;\n\t\t\text_inode_hdr(inode)->eh_max =\n\t\t\t\tcpu_to_le16(ext4_ext_space_root(inode, 0));\n\t\t\terr = ext4_ext_dirty(handle, inode, path);\n\t\t}\n\t}\nout:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\tpath = NULL;\n\tif (err == -EAGAIN)\n\t\tgoto again;\n\text4_journal_stop(handle);\n\n\treturn err;\n}\n\n/*\n * called at mount time\n */\nvoid ext4_ext_init(struct super_block *sb)\n{\n\t/*\n\t * possible initialization would be here\n\t */\n\n\tif (ext4_has_feature_extents(sb)) {\n#if defined(AGGRESSIVE_TEST) || defined(CHECK_BINSEARCH) || defined(EXTENTS_STATS)\n\t\tprintk(KERN_INFO \"EXT4-fs: file extents enabled\"\n#ifdef AGGRESSIVE_TEST\n\t\t       \", aggressive tests\"\n#endif\n#ifdef CHECK_BINSEARCH\n\t\t       \", check binsearch\"\n#endif\n#ifdef EXTENTS_STATS\n\t\t       \", stats\"\n#endif\n\t\t       \"\\n\");\n#endif\n#ifdef EXTENTS_STATS\n\t\tspin_lock_init(&EXT4_SB(sb)->s_ext_stats_lock);\n\t\tEXT4_SB(sb)->s_ext_min = 1 << 30;\n\t\tEXT4_SB(sb)->s_ext_max = 0;\n#endif\n\t}\n}\n\n/*\n * called at umount time\n */\nvoid ext4_ext_release(struct super_block *sb)\n{\n\tif (!ext4_has_feature_extents(sb))\n\t\treturn;\n\n#ifdef EXTENTS_STATS\n\tif (EXT4_SB(sb)->s_ext_blocks && EXT4_SB(sb)->s_ext_extents) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\t\tprintk(KERN_ERR \"EXT4-fs: %lu blocks in %lu extents (%lu ave)\\n\",\n\t\t\tsbi->s_ext_blocks, sbi->s_ext_extents,\n\t\t\tsbi->s_ext_blocks / sbi->s_ext_extents);\n\t\tprintk(KERN_ERR \"EXT4-fs: extents: %lu min, %lu max, max depth %lu\\n\",\n\t\t\tsbi->s_ext_min, sbi->s_ext_max, sbi->s_depth_max);\n\t}\n#endif\n}\n\nstatic int ext4_zeroout_es(struct inode *inode, struct ext4_extent *ex)\n{\n\text4_lblk_t  ee_block;\n\text4_fsblk_t ee_pblock;\n\tunsigned int ee_len;\n\n\tee_block  = le32_to_cpu(ex->ee_block);\n\tee_len    = ext4_ext_get_actual_len(ex);\n\tee_pblock = ext4_ext_pblock(ex);\n\n\tif (ee_len == 0)\n\t\treturn 0;\n\n\treturn ext4_es_insert_extent(inode, ee_block, ee_len, ee_pblock,\n\t\t\t\t     EXTENT_STATUS_WRITTEN);\n}\n\n/* FIXME!! we need to try to merge to left or right after zero-out  */\nstatic int ext4_ext_zeroout(struct inode *inode, struct ext4_extent *ex)\n{\n\text4_fsblk_t ee_pblock;\n\tunsigned int ee_len;\n\n\tee_len    = ext4_ext_get_actual_len(ex);\n\tee_pblock = ext4_ext_pblock(ex);\n\treturn ext4_issue_zeroout(inode, le32_to_cpu(ex->ee_block), ee_pblock,\n\t\t\t\t  ee_len);\n}\n\n/*\n * ext4_split_extent_at() splits an extent at given block.\n *\n * @handle: the journal handle\n * @inode: the file inode\n * @path: the path to the extent\n * @split: the logical block where the extent is splitted.\n * @split_flags: indicates if the extent could be zeroout if split fails, and\n *\t\t the states(init or unwritten) of new extents.\n * @flags: flags used to insert new extent to extent tree.\n *\n *\n * Splits extent [a, b] into two extents [a, @split) and [@split, b], states\n * of which are deterimined by split_flag.\n *\n * There are two cases:\n *  a> the extent are splitted into two extent.\n *  b> split is not needed, and just mark the extent.\n *\n * return 0 on success.\n */\nstatic int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path **ppath,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\text4_fsblk_t newblock;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex, newex, orig_ex, zero_ex;\n\tstruct ext4_extent *ex2 = NULL;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\n\tBUG_ON((split_flag & (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2)) ==\n\t       (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2));\n\n\text_debug(\"ext4_split_extents_at: inode %lu, logical\"\n\t\t\"block %llu\\n\", inode->i_ino, (unsigned long long)split);\n\n\text4_ext_show_leaf(inode, path);\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tnewblock = split - ee_block + ext4_ext_pblock(ex);\n\n\tBUG_ON(split < ee_block || split >= (ee_block + ee_len));\n\tBUG_ON(!ext4_ext_is_unwritten(ex) &&\n\t       split_flag & (EXT4_EXT_MAY_ZEROOUT |\n\t\t\t     EXT4_EXT_MARK_UNWRIT1 |\n\t\t\t     EXT4_EXT_MARK_UNWRIT2));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\n\tif (split == ee_block) {\n\t\t/*\n\t\t * case b: block @split is the block that the extent begins with\n\t\t * then we just change the state of the extent, and splitting\n\t\t * is not needed.\n\t\t */\n\t\tif (split_flag & EXT4_EXT_MARK_UNWRIT2)\n\t\t\text4_ext_mark_unwritten(ex);\n\t\telse\n\t\t\text4_ext_mark_initialized(ex);\n\n\t\tif (!(flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t}\n\n\t/* case a */\n\tmemcpy(&orig_ex, ex, sizeof(orig_ex));\n\tex->ee_len = cpu_to_le16(split - ee_block);\n\tif (split_flag & EXT4_EXT_MARK_UNWRIT1)\n\t\text4_ext_mark_unwritten(ex);\n\n\t/*\n\t * path may lead to new leaf, not to original leaf any more\n\t * after ext4_ext_insert_extent() returns,\n\t */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\tif (err)\n\t\tgoto fix_extent_len;\n\n\tex2 = &newex;\n\tex2->ee_block = cpu_to_le32(split);\n\tex2->ee_len   = cpu_to_le16(ee_len - (split - ee_block));\n\text4_ext_store_pblock(ex2, newblock);\n\tif (split_flag & EXT4_EXT_MARK_UNWRIT2)\n\t\text4_ext_mark_unwritten(ex2);\n\n\terr = ext4_ext_insert_extent(handle, inode, ppath, &newex, flags);\n\tif (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n\t\tif (split_flag & (EXT4_EXT_DATA_VALID1|EXT4_EXT_DATA_VALID2)) {\n\t\t\tif (split_flag & EXT4_EXT_DATA_VALID1) {\n\t\t\t\terr = ext4_ext_zeroout(inode, ex2);\n\t\t\t\tzero_ex.ee_block = ex2->ee_block;\n\t\t\t\tzero_ex.ee_len = cpu_to_le16(\n\t\t\t\t\t\text4_ext_get_actual_len(ex2));\n\t\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\t\t\t      ext4_ext_pblock(ex2));\n\t\t\t} else {\n\t\t\t\terr = ext4_ext_zeroout(inode, ex);\n\t\t\t\tzero_ex.ee_block = ex->ee_block;\n\t\t\t\tzero_ex.ee_len = cpu_to_le16(\n\t\t\t\t\t\text4_ext_get_actual_len(ex));\n\t\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\t\t\t      ext4_ext_pblock(ex));\n\t\t\t}\n\t\t} else {\n\t\t\terr = ext4_ext_zeroout(inode, &orig_ex);\n\t\t\tzero_ex.ee_block = orig_ex.ee_block;\n\t\t\tzero_ex.ee_len = cpu_to_le16(\n\t\t\t\t\t\text4_ext_get_actual_len(&orig_ex));\n\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\t\t      ext4_ext_pblock(&orig_ex));\n\t\t}\n\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_len = cpu_to_le16(ee_len);\n\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\n\t\t/* update extent status tree */\n\t\terr = ext4_zeroout_es(inode, &zero_ex);\n\n\t\tgoto out;\n\t} else if (err)\n\t\tgoto fix_extent_len;\n\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n\nfix_extent_len:\n\tex->ee_len = orig_ex.ee_len;\n\text4_ext_dirty(handle, inode, path + path->p_depth);\n\treturn err;\n}\n\n/*\n * ext4_split_extents() splits an extent and mark extent which is covered\n * by @map as split_flags indicates\n *\n * It may result in splitting the extent into multiple extents (up to three)\n * There are three possibilities:\n *   a> There is no split required\n *   b> Splits in two extents: Split is happening at either end of the extent\n *   c> Splits in three extents: Somone is splitting in middle of the extent\n *\n */\nstatic int ext4_split_extent(handle_t *handle,\n\t\t\t      struct inode *inode,\n\t\t\t      struct ext4_ext_path **ppath,\n\t\t\t      struct ext4_map_blocks *map,\n\t\t\t      int split_flag,\n\t\t\t      int flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\tint unwritten;\n\tint split_flag1, flags1;\n\tint allocated = map->m_len;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tunwritten = ext4_ext_is_unwritten(ex);\n\n\tif (map->m_lblk + map->m_len < ee_block + ee_len) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT;\n\t\tflags1 = flags | EXT4_GET_BLOCKS_PRE_IO;\n\t\tif (unwritten)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNWRIT1 |\n\t\t\t\t       EXT4_EXT_MARK_UNWRIT2;\n\t\tif (split_flag & EXT4_EXT_DATA_VALID2)\n\t\t\tsplit_flag1 |= EXT4_EXT_DATA_VALID1;\n\t\terr = ext4_split_extent_at(handle, inode, ppath,\n\t\t\t\tmap->m_lblk + map->m_len, split_flag1, flags1);\n\t\tif (err)\n\t\t\tgoto out;\n\t} else {\n\t\tallocated = ee_len - (map->m_lblk - ee_block);\n\t}\n\t/*\n\t * Update path is required because previous ext4_split_extent_at() may\n\t * result in split of original leaf or extent zeroout.\n\t */\n\tpath = ext4_find_extent(inode, map->m_lblk, ppath, 0);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tif (!ex) {\n\t\tEXT4_ERROR_INODE(inode, \"unexpected hole at %lu\",\n\t\t\t\t (unsigned long) map->m_lblk);\n\t\treturn -EFSCORRUPTED;\n\t}\n\tunwritten = ext4_ext_is_unwritten(ex);\n\tsplit_flag1 = 0;\n\n\tif (map->m_lblk >= ee_block) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_DATA_VALID2;\n\t\tif (unwritten) {\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNWRIT1;\n\t\t\tsplit_flag1 |= split_flag & (EXT4_EXT_MAY_ZEROOUT |\n\t\t\t\t\t\t     EXT4_EXT_MARK_UNWRIT2);\n\t\t}\n\t\terr = ext4_split_extent_at(handle, inode, ppath,\n\t\t\t\tmap->m_lblk, split_flag1, flags);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\text4_ext_show_leaf(inode, path);\nout:\n\treturn err ? err : allocated;\n}\n\n/*\n * This function is called by ext4_ext_map_blocks() if someone tries to write\n * to an unwritten extent. It may result in splitting the unwritten\n * extent into multiple extents (up to three - one initialized and two\n * unwritten).\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be initialized\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n *\n * Pre-conditions:\n *  - The extent pointed to by 'path' is unwritten.\n *  - The extent pointed to by 'path' contains a superset\n *    of the logical span [map->m_lblk, map->m_lblk + map->m_len).\n *\n * Post-conditions on success:\n *  - the returned value is the number of blocks beyond map->l_lblk\n *    that are allocated and initialized.\n *    It is guaranteed to be >= map->m_len.\n */\nstatic int ext4_ext_convert_to_initialized(handle_t *handle,\n\t\t\t\t\t   struct inode *inode,\n\t\t\t\t\t   struct ext4_map_blocks *map,\n\t\t\t\t\t   struct ext4_ext_path **ppath,\n\t\t\t\t\t   int flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_sb_info *sbi;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_map_blocks split_map;\n\tstruct ext4_extent zero_ex1, zero_ex2;\n\tstruct ext4_extent *ex, *abut_ex;\n\text4_lblk_t ee_block, eof_block;\n\tunsigned int ee_len, depth, map_len = map->m_len;\n\tint allocated = 0, max_zeroout = 0;\n\tint err = 0;\n\tint split_flag = EXT4_EXT_DATA_VALID2;\n\n\text_debug(\"ext4_ext_convert_to_initialized: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)map->m_lblk, map_len);\n\n\tsbi = EXT4_SB(inode->i_sb);\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map_len)\n\t\teof_block = map->m_lblk + map_len;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tzero_ex1.ee_len = 0;\n\tzero_ex2.ee_len = 0;\n\n\ttrace_ext4_ext_convert_to_initialized_enter(inode, map, ex);\n\n\t/* Pre-conditions */\n\tBUG_ON(!ext4_ext_is_unwritten(ex));\n\tBUG_ON(!in_range(map->m_lblk, ee_block, ee_len));\n\n\t/*\n\t * Attempt to transfer newly initialized blocks from the currently\n\t * unwritten extent to its neighbor. This is much cheaper\n\t * than an insertion followed by a merge as those involve costly\n\t * memmove() calls. Transferring to the left is the common case in\n\t * steady state for workloads doing fallocate(FALLOC_FL_KEEP_SIZE)\n\t * followed by append writes.\n\t *\n\t * Limitations of the current logic:\n\t *  - L1: we do not deal with writes covering the whole extent.\n\t *    This would require removing the extent if the transfer\n\t *    is possible.\n\t *  - L2: we only attempt to merge with an extent stored in the\n\t *    same extent tree node.\n\t */\n\tif ((map->m_lblk == ee_block) &&\n\t\t/* See if we can merge left */\n\t\t(map_len < ee_len) &&\t\t/*L1*/\n\t\t(ex > EXT_FIRST_EXTENT(eh))) {\t/*L2*/\n\t\text4_lblk_t prev_lblk;\n\t\text4_fsblk_t prev_pblk, ee_pblk;\n\t\tunsigned int prev_len;\n\n\t\tabut_ex = ex - 1;\n\t\tprev_lblk = le32_to_cpu(abut_ex->ee_block);\n\t\tprev_len = ext4_ext_get_actual_len(abut_ex);\n\t\tprev_pblk = ext4_ext_pblock(abut_ex);\n\t\tee_pblk = ext4_ext_pblock(ex);\n\n\t\t/*\n\t\t * A transfer of blocks from 'ex' to 'abut_ex' is allowed\n\t\t * upon those conditions:\n\t\t * - C1: abut_ex is initialized,\n\t\t * - C2: abut_ex is logically abutting ex,\n\t\t * - C3: abut_ex is physically abutting ex,\n\t\t * - C4: abut_ex can receive the additional blocks without\n\t\t *   overflowing the (initialized) length limit.\n\t\t */\n\t\tif ((!ext4_ext_is_unwritten(abut_ex)) &&\t\t/*C1*/\n\t\t\t((prev_lblk + prev_len) == ee_block) &&\t\t/*C2*/\n\t\t\t((prev_pblk + prev_len) == ee_pblk) &&\t\t/*C3*/\n\t\t\t(prev_len < (EXT_INIT_MAX_LEN - map_len))) {\t/*C4*/\n\t\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\ttrace_ext4_ext_convert_to_initialized_fastpath(inode,\n\t\t\t\tmap, ex, abut_ex);\n\n\t\t\t/* Shift the start of ex by 'map_len' blocks */\n\t\t\tex->ee_block = cpu_to_le32(ee_block + map_len);\n\t\t\text4_ext_store_pblock(ex, ee_pblk + map_len);\n\t\t\tex->ee_len = cpu_to_le16(ee_len - map_len);\n\t\t\text4_ext_mark_unwritten(ex); /* Restore the flag */\n\n\t\t\t/* Extend abut_ex by 'map_len' blocks */\n\t\t\tabut_ex->ee_len = cpu_to_le16(prev_len + map_len);\n\n\t\t\t/* Result: number of initialized blocks past m_lblk */\n\t\t\tallocated = map_len;\n\t\t}\n\t} else if (((map->m_lblk + map_len) == (ee_block + ee_len)) &&\n\t\t   (map_len < ee_len) &&\t/*L1*/\n\t\t   ex < EXT_LAST_EXTENT(eh)) {\t/*L2*/\n\t\t/* See if we can merge right */\n\t\text4_lblk_t next_lblk;\n\t\text4_fsblk_t next_pblk, ee_pblk;\n\t\tunsigned int next_len;\n\n\t\tabut_ex = ex + 1;\n\t\tnext_lblk = le32_to_cpu(abut_ex->ee_block);\n\t\tnext_len = ext4_ext_get_actual_len(abut_ex);\n\t\tnext_pblk = ext4_ext_pblock(abut_ex);\n\t\tee_pblk = ext4_ext_pblock(ex);\n\n\t\t/*\n\t\t * A transfer of blocks from 'ex' to 'abut_ex' is allowed\n\t\t * upon those conditions:\n\t\t * - C1: abut_ex is initialized,\n\t\t * - C2: abut_ex is logically abutting ex,\n\t\t * - C3: abut_ex is physically abutting ex,\n\t\t * - C4: abut_ex can receive the additional blocks without\n\t\t *   overflowing the (initialized) length limit.\n\t\t */\n\t\tif ((!ext4_ext_is_unwritten(abut_ex)) &&\t\t/*C1*/\n\t\t    ((map->m_lblk + map_len) == next_lblk) &&\t\t/*C2*/\n\t\t    ((ee_pblk + ee_len) == next_pblk) &&\t\t/*C3*/\n\t\t    (next_len < (EXT_INIT_MAX_LEN - map_len))) {\t/*C4*/\n\t\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\ttrace_ext4_ext_convert_to_initialized_fastpath(inode,\n\t\t\t\tmap, ex, abut_ex);\n\n\t\t\t/* Shift the start of abut_ex by 'map_len' blocks */\n\t\t\tabut_ex->ee_block = cpu_to_le32(next_lblk - map_len);\n\t\t\text4_ext_store_pblock(abut_ex, next_pblk - map_len);\n\t\t\tex->ee_len = cpu_to_le16(ee_len - map_len);\n\t\t\text4_ext_mark_unwritten(ex); /* Restore the flag */\n\n\t\t\t/* Extend abut_ex by 'map_len' blocks */\n\t\t\tabut_ex->ee_len = cpu_to_le16(next_len + map_len);\n\n\t\t\t/* Result: number of initialized blocks past m_lblk */\n\t\t\tallocated = map_len;\n\t\t}\n\t}\n\tif (allocated) {\n\t\t/* Mark the block containing both extents as dirty */\n\t\text4_ext_dirty(handle, inode, path + depth);\n\n\t\t/* Update path to point to the right extent */\n\t\tpath[depth].p_ext = abut_ex;\n\t\tgoto out;\n\t} else\n\t\tallocated = ee_len - (map->m_lblk - ee_block);\n\n\tWARN_ON(map->m_lblk < ee_block);\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully inside i_size or new_size.\n\t */\n\tsplit_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;\n\n\tif (EXT4_EXT_MAY_ZEROOUT & split_flag)\n\t\tmax_zeroout = sbi->s_extent_max_zeroout_kb >>\n\t\t\t(inode->i_sb->s_blocksize_bits - 10);\n\n\tif (IS_ENCRYPTED(inode))\n\t\tmax_zeroout = 0;\n\n\t/*\n\t * five cases:\n\t * 1. split the extent into three extents.\n\t * 2. split the extent into two extents, zeroout the head of the first\n\t *    extent.\n\t * 3. split the extent into two extents, zeroout the tail of the second\n\t *    extent.\n\t * 4. split the extent into two extents with out zeroout.\n\t * 5. no splitting needed, just possibly zeroout the head and / or the\n\t *    tail of the extent.\n\t */\n\tsplit_map.m_lblk = map->m_lblk;\n\tsplit_map.m_len = map->m_len;\n\n\tif (max_zeroout && (allocated > split_map.m_len)) {\n\t\tif (allocated <= max_zeroout) {\n\t\t\t/* case 3 or 5 */\n\t\t\tzero_ex1.ee_block =\n\t\t\t\t cpu_to_le32(split_map.m_lblk +\n\t\t\t\t\t     split_map.m_len);\n\t\t\tzero_ex1.ee_len =\n\t\t\t\tcpu_to_le16(allocated - split_map.m_len);\n\t\t\text4_ext_store_pblock(&zero_ex1,\n\t\t\t\text4_ext_pblock(ex) + split_map.m_lblk +\n\t\t\t\tsplit_map.m_len - ee_block);\n\t\t\terr = ext4_ext_zeroout(inode, &zero_ex1);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tsplit_map.m_len = allocated;\n\t\t}\n\t\tif (split_map.m_lblk - ee_block + split_map.m_len <\n\t\t\t\t\t\t\t\tmax_zeroout) {\n\t\t\t/* case 2 or 5 */\n\t\t\tif (split_map.m_lblk != ee_block) {\n\t\t\t\tzero_ex2.ee_block = ex->ee_block;\n\t\t\t\tzero_ex2.ee_len = cpu_to_le16(split_map.m_lblk -\n\t\t\t\t\t\t\tee_block);\n\t\t\t\text4_ext_store_pblock(&zero_ex2,\n\t\t\t\t\t\t      ext4_ext_pblock(ex));\n\t\t\t\terr = ext4_ext_zeroout(inode, &zero_ex2);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tsplit_map.m_len += split_map.m_lblk - ee_block;\n\t\t\tsplit_map.m_lblk = ee_block;\n\t\t\tallocated = map->m_len;\n\t\t}\n\t}\n\n\terr = ext4_split_extent(handle, inode, ppath, &split_map, split_flag,\n\t\t\t\tflags);\n\tif (err > 0)\n\t\terr = 0;\nout:\n\t/* If we have gotten a failure, don't zero out status tree */\n\tif (!err) {\n\t\terr = ext4_zeroout_es(inode, &zero_ex1);\n\t\tif (!err)\n\t\t\terr = ext4_zeroout_es(inode, &zero_ex2);\n\t}\n\treturn err ? err : allocated;\n}\n\n/*\n * This function is called by ext4_ext_map_blocks() from\n * ext4_get_blocks_dio_write() when DIO to write\n * to an unwritten extent.\n *\n * Writing to an unwritten extent may result in splitting the unwritten\n * extent into multiple initialized/unwritten extents (up to three)\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be unwritten\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n *\n * This works the same way in the case of initialized -> unwritten conversion.\n *\n * One of more index blocks maybe needed if the extent tree grow after\n * the unwritten extent split. To prevent ENOSPC occur at the IO\n * complete, we need to split the unwritten extent before DIO submit\n * the IO. The unwritten extent called at this time will be split\n * into three unwritten extent(at most). After IO complete, the part\n * being filled will be convert to initialized by the end_io callback function\n * via ext4_convert_unwritten_extents().\n *\n * Returns the size of unwritten extent to be written on success.\n */\nstatic int ext4_split_convert_extents(handle_t *handle,\n\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\t\tstruct ext4_ext_path **ppath,\n\t\t\t\t\tint flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\text4_lblk_t eof_block;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len;\n\tint split_flag = 0, depth;\n\n\text_debug(\"%s: inode %lu, logical block %llu, max_blocks %u\\n\",\n\t\t  __func__, inode->i_ino,\n\t\t  (unsigned long long)map->m_lblk, map->m_len);\n\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map->m_len)\n\t\teof_block = map->m_lblk + map->m_len;\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully insde i_size or new_size.\n\t */\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\t/* Convert to unwritten */\n\tif (flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN) {\n\t\tsplit_flag |= EXT4_EXT_DATA_VALID1;\n\t/* Convert to initialized */\n\t} else if (flags & EXT4_GET_BLOCKS_CONVERT) {\n\t\tsplit_flag |= ee_block + ee_len <= eof_block ?\n\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;\n\t\tsplit_flag |= (EXT4_EXT_MARK_UNWRIT2 | EXT4_EXT_DATA_VALID2);\n\t}\n\tflags |= EXT4_GET_BLOCKS_PRE_IO;\n\treturn ext4_split_extent(handle, inode, ppath, map, split_flag, flags);\n}\n\nstatic int ext4_convert_unwritten_extents_endio(handle_t *handle,\n\t\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\t\t\tstruct ext4_ext_path **ppath)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_extent *ex;\n\text4_lblk_t ee_block;\n\tunsigned int ee_len;\n\tint depth;\n\tint err = 0;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\text_debug(\"ext4_convert_unwritten_extents_endio: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t  (unsigned long long)ee_block, ee_len);\n\n\t/* If extent is larger than requested it is a clear sign that we still\n\t * have some extent state machine issues left. So extent_split is still\n\t * required.\n\t * TODO: Once all related issues will be fixed this situation should be\n\t * illegal.\n\t */\n\tif (ee_block != map->m_lblk || ee_len > map->m_len) {\n#ifdef EXT4_DEBUG\n\t\text4_warning(\"Inode (%ld) finished: extent logical block %llu,\"\n\t\t\t     \" len %u; IO logical block %llu, len %u\",\n\t\t\t     inode->i_ino, (unsigned long long)ee_block, ee_len,\n\t\t\t     (unsigned long long)map->m_lblk, map->m_len);\n#endif\n\t\terr = ext4_split_convert_extents(handle, inode, map, ppath,\n\t\t\t\t\t\t EXT4_GET_BLOCKS_CONVERT);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tpath = ext4_find_extent(inode, map->m_lblk, ppath, 0);\n\t\tif (IS_ERR(path))\n\t\t\treturn PTR_ERR(path);\n\t\tdepth = ext_depth(inode);\n\t\tex = path[depth].p_ext;\n\t}\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\t/* first mark the extent as initialized */\n\text4_ext_mark_initialized(ex);\n\n\t/* note: ext4_ext_correct_indexes() isn't needed here because\n\t * borders are not changed\n\t */\n\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n}\n\n/*\n * Handle EOFBLOCKS_FL flag, clearing it if necessary\n */\nstatic int check_eofblocks_fl(handle_t *handle, struct inode *inode,\n\t\t\t      ext4_lblk_t lblk,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      unsigned int len)\n{\n\tint i, depth;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *last_ex;\n\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EOFBLOCKS))\n\t\treturn 0;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\n\t/*\n\t * We're going to remove EOFBLOCKS_FL entirely in future so we\n\t * do not care for this case anymore. Simply remove the flag\n\t * if there are no extents.\n\t */\n\tif (unlikely(!eh->eh_entries))\n\t\tgoto out;\n\tlast_ex = EXT_LAST_EXTENT(eh);\n\t/*\n\t * We should clear the EOFBLOCKS_FL flag if we are writing the\n\t * last block in the last extent in the file.  We test this by\n\t * first checking to see if the caller to\n\t * ext4_ext_get_blocks() was interested in the last block (or\n\t * a block beyond the last block) in the current extent.  If\n\t * this turns out to be false, we can bail out from this\n\t * function immediately.\n\t */\n\tif (lblk + len < le32_to_cpu(last_ex->ee_block) +\n\t    ext4_ext_get_actual_len(last_ex))\n\t\treturn 0;\n\t/*\n\t * If the caller does appear to be planning to write at or\n\t * beyond the end of the current extent, we then test to see\n\t * if the current extent is the last extent in the file, by\n\t * checking to make sure it was reached via the rightmost node\n\t * at each level of the tree.\n\t */\n\tfor (i = depth-1; i >= 0; i--)\n\t\tif (path[i].p_idx != EXT_LAST_INDEX(path[i].p_hdr))\n\t\t\treturn 0;\nout:\n\text4_clear_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\treturn ext4_mark_inode_dirty(handle, inode);\n}\n\nstatic int\nconvert_initialized_extent(handle_t *handle, struct inode *inode,\n\t\t\t   struct ext4_map_blocks *map,\n\t\t\t   struct ext4_ext_path **ppath,\n\t\t\t   unsigned int allocated)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_extent *ex;\n\text4_lblk_t ee_block;\n\tunsigned int ee_len;\n\tint depth;\n\tint err = 0;\n\n\t/*\n\t * Make sure that the extent is no bigger than we support with\n\t * unwritten extent\n\t */\n\tif (map->m_len > EXT_UNWRITTEN_MAX_LEN)\n\t\tmap->m_len = EXT_UNWRITTEN_MAX_LEN / 2;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\text_debug(\"%s: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", __func__, inode->i_ino,\n\t\t  (unsigned long long)ee_block, ee_len);\n\n\tif (ee_block != map->m_lblk || ee_len > map->m_len) {\n\t\terr = ext4_split_convert_extents(handle, inode, map, ppath,\n\t\t\t\tEXT4_GET_BLOCKS_CONVERT_UNWRITTEN);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tpath = ext4_find_extent(inode, map->m_lblk, ppath, 0);\n\t\tif (IS_ERR(path))\n\t\t\treturn PTR_ERR(path);\n\t\tdepth = ext_depth(inode);\n\t\tex = path[depth].p_ext;\n\t\tif (!ex) {\n\t\t\tEXT4_ERROR_INODE(inode, \"unexpected hole at %lu\",\n\t\t\t\t\t (unsigned long) map->m_lblk);\n\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t}\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\treturn err;\n\t/* first mark the extent as unwritten */\n\text4_ext_mark_unwritten(ex);\n\n\t/* note: ext4_ext_correct_indexes() isn't needed here because\n\t * borders are not changed\n\t */\n\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\tif (err)\n\t\treturn err;\n\text4_ext_show_leaf(inode, path);\n\n\text4_update_inode_fsync_trans(handle, inode, 1);\n\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path, map->m_len);\n\tif (err)\n\t\treturn err;\n\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\tmap->m_len = allocated;\n\treturn allocated;\n}\n\nstatic int\next4_ext_handle_unwritten_extents(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map,\n\t\t\tstruct ext4_ext_path **ppath, int flags,\n\t\t\tunsigned int allocated, ext4_fsblk_t newblock)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tint ret = 0;\n\tint err = 0;\n\n\text_debug(\"ext4_ext_handle_unwritten_extents: inode %lu, logical \"\n\t\t  \"block %llu, max_blocks %u, flags %x, allocated %u\\n\",\n\t\t  inode->i_ino, (unsigned long long)map->m_lblk, map->m_len,\n\t\t  flags, allocated);\n\text4_ext_show_leaf(inode, path);\n\n\t/*\n\t * When writing into unwritten space, we should not fail to\n\t * allocate metadata blocks for the new extent block if needed.\n\t */\n\tflags |= EXT4_GET_BLOCKS_METADATA_NOFAIL;\n\n\ttrace_ext4_ext_handle_unwritten_extents(inode, map, flags,\n\t\t\t\t\t\t    allocated, newblock);\n\n\t/* get_block() before submit the IO, split the extent */\n\tif (flags & EXT4_GET_BLOCKS_PRE_IO) {\n\t\tret = ext4_split_convert_extents(handle, inode, map, ppath,\n\t\t\t\t\t flags | EXT4_GET_BLOCKS_CONVERT);\n\t\tif (ret <= 0)\n\t\t\tgoto out;\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto out;\n\t}\n\t/* IO end_io complete, convert the filled extent to written */\n\tif (flags & EXT4_GET_BLOCKS_CONVERT) {\n\t\tif (flags & EXT4_GET_BLOCKS_ZERO) {\n\t\t\tif (allocated > map->m_len)\n\t\t\t\tallocated = map->m_len;\n\t\t\terr = ext4_issue_zeroout(inode, map->m_lblk, newblock,\n\t\t\t\t\t\t allocated);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out2;\n\t\t}\n\t\tret = ext4_convert_unwritten_extents_endio(handle, inode, map,\n\t\t\t\t\t\t\t   ppath);\n\t\tif (ret >= 0) {\n\t\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t\t path, map->m_len);\n\t\t} else\n\t\t\terr = ret;\n\t\tmap->m_flags |= EXT4_MAP_MAPPED;\n\t\tmap->m_pblk = newblock;\n\t\tif (allocated > map->m_len)\n\t\t\tallocated = map->m_len;\n\t\tmap->m_len = allocated;\n\t\tgoto out2;\n\t}\n\t/* buffered IO case */\n\t/*\n\t * repeat fallocate creation request\n\t * we already have an unwritten extent\n\t */\n\tif (flags & EXT4_GET_BLOCKS_UNWRIT_EXT) {\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto map_out;\n\t}\n\n\t/* buffered READ or buffered write_begin() lookup */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * We have blocks reserved already.  We\n\t\t * return allocated blocks so that delalloc\n\t\t * won't do block reservation for us.  But\n\t\t * the buffer head will be unmapped so that\n\t\t * a read from the block returns 0s.\n\t\t */\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto out1;\n\t}\n\n\t/* buffered write, writepage time, convert*/\n\tret = ext4_ext_convert_to_initialized(handle, inode, map, ppath, flags);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\nout:\n\tif (ret <= 0) {\n\t\terr = ret;\n\t\tgoto out2;\n\t} else\n\t\tallocated = ret;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\tmap->m_len = allocated;\n\nmap_out:\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tif ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0) {\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path,\n\t\t\t\t\t map->m_len);\n\t\tif (err < 0)\n\t\t\tgoto out2;\n\t}\nout1:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\treturn err ? err : allocated;\n}\n\n/*\n * get_implied_cluster_alloc - check to see if the requested\n * allocation (in the map structure) overlaps with a cluster already\n * allocated in an extent.\n *\t@sb\tThe filesystem superblock structure\n *\t@map\tThe requested lblk->pblk mapping\n *\t@ex\tThe extent structure which might contain an implied\n *\t\t\tcluster allocation\n *\n * This function is called by ext4_ext_map_blocks() after we failed to\n * find blocks that were already in the inode's extent tree.  Hence,\n * we know that the beginning of the requested region cannot overlap\n * the extent from the inode's extent tree.  There are three cases we\n * want to catch.  The first is this case:\n *\n *\t\t |--- cluster # N--|\n *    |--- extent ---|\t|---- requested region ---|\n *\t\t\t|==========|\n *\n * The second case that we need to test for is this one:\n *\n *   |--------- cluster # N ----------------|\n *\t   |--- requested region --|   |------- extent ----|\n *\t   |=======================|\n *\n * The third case is when the requested region lies between two extents\n * within the same cluster:\n *          |------------- cluster # N-------------|\n * |----- ex -----|                  |---- ex_right ----|\n *                  |------ requested region ------|\n *                  |================|\n *\n * In each of the above cases, we need to set the map->m_pblk and\n * map->m_len so it corresponds to the return the extent labelled as\n * \"|====|\" from cluster #N, since it is already in use for data in\n * cluster EXT4_B2C(sbi, map->m_lblk).\tWe will then return 1 to\n * signal to ext4_ext_map_blocks() that map->m_pblk should be treated\n * as a new \"allocated\" block region.  Otherwise, we will return 0 and\n * ext4_ext_map_blocks() will then allocate one or more new clusters\n * by calling ext4_mb_new_blocks().\n */\nstatic int get_implied_cluster_alloc(struct super_block *sb,\n\t\t\t\t     struct ext4_map_blocks *map,\n\t\t\t\t     struct ext4_extent *ex,\n\t\t\t\t     struct ext4_ext_path *path)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_lblk_t c_offset = EXT4_LBLK_COFF(sbi, map->m_lblk);\n\text4_lblk_t ex_cluster_start, ex_cluster_end;\n\text4_lblk_t rr_cluster_start;\n\text4_lblk_t ee_block = le32_to_cpu(ex->ee_block);\n\text4_fsblk_t ee_start = ext4_ext_pblock(ex);\n\tunsigned short ee_len = ext4_ext_get_actual_len(ex);\n\n\t/* The extent passed in that we are trying to match */\n\tex_cluster_start = EXT4_B2C(sbi, ee_block);\n\tex_cluster_end = EXT4_B2C(sbi, ee_block + ee_len - 1);\n\n\t/* The requested region passed into ext4_map_blocks() */\n\trr_cluster_start = EXT4_B2C(sbi, map->m_lblk);\n\n\tif ((rr_cluster_start == ex_cluster_end) ||\n\t    (rr_cluster_start == ex_cluster_start)) {\n\t\tif (rr_cluster_start == ex_cluster_end)\n\t\t\tee_start += ee_len - 1;\n\t\tmap->m_pblk = EXT4_PBLK_CMASK(sbi, ee_start) + c_offset;\n\t\tmap->m_len = min(map->m_len,\n\t\t\t\t (unsigned) sbi->s_cluster_ratio - c_offset);\n\t\t/*\n\t\t * Check for and handle this case:\n\t\t *\n\t\t *   |--------- cluster # N-------------|\n\t\t *\t\t       |------- extent ----|\n\t\t *\t   |--- requested region ---|\n\t\t *\t   |===========|\n\t\t */\n\n\t\tif (map->m_lblk < ee_block)\n\t\t\tmap->m_len = min(map->m_len, ee_block - map->m_lblk);\n\n\t\t/*\n\t\t * Check for the case where there is already another allocated\n\t\t * block to the right of 'ex' but before the end of the cluster.\n\t\t *\n\t\t *          |------------- cluster # N-------------|\n\t\t * |----- ex -----|                  |---- ex_right ----|\n\t\t *                  |------ requested region ------|\n\t\t *                  |================|\n\t\t */\n\t\tif (map->m_lblk > ee_block) {\n\t\t\text4_lblk_t next = ext4_ext_next_allocated_block(path);\n\t\t\tmap->m_len = min(map->m_len, next - map->m_lblk);\n\t\t}\n\n\t\ttrace_ext4_get_implied_cluster_alloc_exit(sb, map, 1);\n\t\treturn 1;\n\t}\n\n\ttrace_ext4_get_implied_cluster_alloc_exit(sb, map, 0);\n\treturn 0;\n}\n\n\n/*\n * Block allocation/map/preallocation routine for extents based files\n *\n *\n * Need to be called with\n * down_read(&EXT4_I(inode)->i_data_sem) if not allocating file system block\n * (ie, create is zero). Otherwise down_write(&EXT4_I(inode)->i_data_sem)\n *\n * return > 0, number of of blocks already mapped/allocated\n *          if create == 0 and these are pre-allocated blocks\n *          \tbuffer head is unmapped\n *          otherwise blocks are mapped\n *\n * return = 0, if plain look up failed (blocks have not been allocated)\n *          buffer head is unmapped\n *\n * return < 0, error case.\n */\nint ext4_ext_map_blocks(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map, int flags)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_extent newex, *ex, *ex2;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_fsblk_t newblock = 0;\n\tint free_on_err = 0, err = 0, depth, ret;\n\tunsigned int allocated = 0, offset = 0;\n\tunsigned int allocated_clusters = 0;\n\tstruct ext4_allocation_request ar;\n\text4_lblk_t cluster_offset;\n\tbool map_from_cluster = false;\n\n\text_debug(\"blocks %u/%u requested for inode %lu\\n\",\n\t\t  map->m_lblk, map->m_len, inode->i_ino);\n\ttrace_ext4_ext_map_blocks_enter(inode, map->m_lblk, map->m_len, flags);\n\n\t/* find extent for this block */\n\tpath = ext4_find_extent(inode, map->m_lblk, NULL, 0);\n\tif (IS_ERR(path)) {\n\t\terr = PTR_ERR(path);\n\t\tpath = NULL;\n\t\tgoto out2;\n\t}\n\n\tdepth = ext_depth(inode);\n\n\t/*\n\t * consistent leaf must not be empty;\n\t * this situation is possible, though, _during_ tree modification;\n\t * this is why assert can't be put in ext4_find_extent()\n\t */\n\tif (unlikely(path[depth].p_ext == NULL && depth != 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"bad extent address \"\n\t\t\t\t \"lblock: %lu, depth: %d pblock %lld\",\n\t\t\t\t (unsigned long) map->m_lblk, depth,\n\t\t\t\t path[depth].p_block);\n\t\terr = -EFSCORRUPTED;\n\t\tgoto out2;\n\t}\n\n\tex = path[depth].p_ext;\n\tif (ex) {\n\t\text4_lblk_t ee_block = le32_to_cpu(ex->ee_block);\n\t\text4_fsblk_t ee_start = ext4_ext_pblock(ex);\n\t\tunsigned short ee_len;\n\n\n\t\t/*\n\t\t * unwritten extents are treated as holes, except that\n\t\t * we split out initialized portions during a write.\n\t\t */\n\t\tee_len = ext4_ext_get_actual_len(ex);\n\n\t\ttrace_ext4_ext_show_extent(inode, ee_block, ee_start, ee_len);\n\n\t\t/* if found extent covers block, simply return it */\n\t\tif (in_range(map->m_lblk, ee_block, ee_len)) {\n\t\t\tnewblock = map->m_lblk - ee_block + ee_start;\n\t\t\t/* number of remaining blocks in the extent */\n\t\t\tallocated = ee_len - (map->m_lblk - ee_block);\n\t\t\text_debug(\"%u fit into %u:%d -> %llu\\n\", map->m_lblk,\n\t\t\t\t  ee_block, ee_len, newblock);\n\n\t\t\t/*\n\t\t\t * If the extent is initialized check whether the\n\t\t\t * caller wants to convert it to unwritten.\n\t\t\t */\n\t\t\tif ((!ext4_ext_is_unwritten(ex)) &&\n\t\t\t    (flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN)) {\n\t\t\t\tallocated = convert_initialized_extent(\n\t\t\t\t\t\thandle, inode, map, &path,\n\t\t\t\t\t\tallocated);\n\t\t\t\tgoto out2;\n\t\t\t} else if (!ext4_ext_is_unwritten(ex))\n\t\t\t\tgoto out;\n\n\t\t\tret = ext4_ext_handle_unwritten_extents(\n\t\t\t\thandle, inode, map, &path, flags,\n\t\t\t\tallocated, newblock);\n\t\t\tif (ret < 0)\n\t\t\t\terr = ret;\n\t\t\telse\n\t\t\t\tallocated = ret;\n\t\t\tgoto out2;\n\t\t}\n\t}\n\n\t/*\n\t * requested block isn't allocated yet;\n\t * we couldn't try to create block if create flag is zero\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\text4_lblk_t hole_start, hole_len;\n\n\t\thole_start = map->m_lblk;\n\t\thole_len = ext4_ext_determine_hole(inode, path, &hole_start);\n\t\t/*\n\t\t * put just found gap into cache to speed up\n\t\t * subsequent requests\n\t\t */\n\t\text4_ext_put_gap_in_cache(inode, hole_start, hole_len);\n\n\t\t/* Update hole_len to reflect hole size after map->m_lblk */\n\t\tif (hole_start != map->m_lblk)\n\t\t\thole_len -= map->m_lblk - hole_start;\n\t\tmap->m_pblk = 0;\n\t\tmap->m_len = min_t(unsigned int, map->m_len, hole_len);\n\n\t\tgoto out2;\n\t}\n\n\t/*\n\t * Okay, we need to do block allocation.\n\t */\n\tnewex.ee_block = cpu_to_le32(map->m_lblk);\n\tcluster_offset = EXT4_LBLK_COFF(sbi, map->m_lblk);\n\n\t/*\n\t * If we are doing bigalloc, check to see if the extent returned\n\t * by ext4_find_extent() implies a cluster we can use.\n\t */\n\tif (cluster_offset && ex &&\n\t    get_implied_cluster_alloc(inode->i_sb, map, ex, path)) {\n\t\tar.len = allocated = map->m_len;\n\t\tnewblock = map->m_pblk;\n\t\tmap_from_cluster = true;\n\t\tgoto got_allocated_blocks;\n\t}\n\n\t/* find neighbour allocated blocks */\n\tar.lleft = map->m_lblk;\n\terr = ext4_ext_search_left(inode, path, &ar.lleft, &ar.pleft);\n\tif (err)\n\t\tgoto out2;\n\tar.lright = map->m_lblk;\n\tex2 = NULL;\n\terr = ext4_ext_search_right(inode, path, &ar.lright, &ar.pright, &ex2);\n\tif (err)\n\t\tgoto out2;\n\n\t/* Check if the extent after searching to the right implies a\n\t * cluster we can use. */\n\tif ((sbi->s_cluster_ratio > 1) && ex2 &&\n\t    get_implied_cluster_alloc(inode->i_sb, map, ex2, path)) {\n\t\tar.len = allocated = map->m_len;\n\t\tnewblock = map->m_pblk;\n\t\tmap_from_cluster = true;\n\t\tgoto got_allocated_blocks;\n\t}\n\n\t/*\n\t * See if request is beyond maximum number of blocks we can have in\n\t * a single extent. For an initialized extent this limit is\n\t * EXT_INIT_MAX_LEN and for an unwritten extent this limit is\n\t * EXT_UNWRITTEN_MAX_LEN.\n\t */\n\tif (map->m_len > EXT_INIT_MAX_LEN &&\n\t    !(flags & EXT4_GET_BLOCKS_UNWRIT_EXT))\n\t\tmap->m_len = EXT_INIT_MAX_LEN;\n\telse if (map->m_len > EXT_UNWRITTEN_MAX_LEN &&\n\t\t (flags & EXT4_GET_BLOCKS_UNWRIT_EXT))\n\t\tmap->m_len = EXT_UNWRITTEN_MAX_LEN;\n\n\t/* Check if we can really insert (m_lblk)::(m_lblk + m_len) extent */\n\tnewex.ee_len = cpu_to_le16(map->m_len);\n\terr = ext4_ext_check_overlap(sbi, inode, &newex, path);\n\tif (err)\n\t\tallocated = ext4_ext_get_actual_len(&newex);\n\telse\n\t\tallocated = map->m_len;\n\n\t/* allocate new block */\n\tar.inode = inode;\n\tar.goal = ext4_ext_find_goal(inode, path, map->m_lblk);\n\tar.logical = map->m_lblk;\n\t/*\n\t * We calculate the offset from the beginning of the cluster\n\t * for the logical block number, since when we allocate a\n\t * physical cluster, the physical block should start at the\n\t * same offset from the beginning of the cluster.  This is\n\t * needed so that future calls to get_implied_cluster_alloc()\n\t * work correctly.\n\t */\n\toffset = EXT4_LBLK_COFF(sbi, map->m_lblk);\n\tar.len = EXT4_NUM_B2C(sbi, offset+allocated);\n\tar.goal -= offset;\n\tar.logical -= offset;\n\tif (S_ISREG(inode->i_mode))\n\t\tar.flags = EXT4_MB_HINT_DATA;\n\telse\n\t\t/* disable in-core preallocation for non-regular files */\n\t\tar.flags = 0;\n\tif (flags & EXT4_GET_BLOCKS_NO_NORMALIZE)\n\t\tar.flags |= EXT4_MB_HINT_NOPREALLOC;\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\tar.flags |= EXT4_MB_DELALLOC_RESERVED;\n\tif (flags & EXT4_GET_BLOCKS_METADATA_NOFAIL)\n\t\tar.flags |= EXT4_MB_USE_RESERVED;\n\tnewblock = ext4_mb_new_blocks(handle, &ar, &err);\n\tif (!newblock)\n\t\tgoto out2;\n\text_debug(\"allocate new block: goal %llu, found %llu/%u\\n\",\n\t\t  ar.goal, newblock, allocated);\n\tfree_on_err = 1;\n\tallocated_clusters = ar.len;\n\tar.len = EXT4_C2B(sbi, ar.len) - offset;\n\tif (ar.len > allocated)\n\t\tar.len = allocated;\n\ngot_allocated_blocks:\n\t/* try to insert new extent into found leaf and return */\n\text4_ext_store_pblock(&newex, newblock + offset);\n\tnewex.ee_len = cpu_to_le16(ar.len);\n\t/* Mark unwritten */\n\tif (flags & EXT4_GET_BLOCKS_UNWRIT_EXT){\n\t\text4_ext_mark_unwritten(&newex);\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t}\n\n\terr = 0;\n\tif ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0)\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t path, ar.len);\n\tif (!err)\n\t\terr = ext4_ext_insert_extent(handle, inode, &path,\n\t\t\t\t\t     &newex, flags);\n\n\tif (err && free_on_err) {\n\t\tint fb_flags = flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE ?\n\t\t\tEXT4_FREE_BLOCKS_NO_QUOT_UPDATE : 0;\n\t\t/* free data blocks we just allocated */\n\t\t/* not a good idea to call discard here directly,\n\t\t * but otherwise we'd need to call it every free() */\n\t\text4_discard_preallocations(inode);\n\t\text4_free_blocks(handle, inode, NULL, newblock,\n\t\t\t\t EXT4_C2B(sbi, allocated_clusters), fb_flags);\n\t\tgoto out2;\n\t}\n\n\t/* previous routine could use block we allocated */\n\tnewblock = ext4_ext_pblock(&newex);\n\tallocated = ext4_ext_get_actual_len(&newex);\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\n\t/*\n\t * Reduce the reserved cluster count to reflect successful deferred\n\t * allocation of delayed allocated clusters or direct allocation of\n\t * clusters discovered to be delayed allocated.  Once allocated, a\n\t * cluster is not included in the reserved count.\n\t */\n\tif (test_opt(inode->i_sb, DELALLOC) && !map_from_cluster) {\n\t\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) {\n\t\t\t/*\n\t\t\t * When allocating delayed allocated clusters, simply\n\t\t\t * reduce the reserved cluster count and claim quota\n\t\t\t */\n\t\t\text4_da_update_reserve_space(inode, allocated_clusters,\n\t\t\t\t\t\t\t1);\n\t\t} else {\n\t\t\text4_lblk_t lblk, len;\n\t\t\tunsigned int n;\n\n\t\t\t/*\n\t\t\t * When allocating non-delayed allocated clusters\n\t\t\t * (from fallocate, filemap, DIO, or clusters\n\t\t\t * allocated when delalloc has been disabled by\n\t\t\t * ext4_nonda_switch), reduce the reserved cluster\n\t\t\t * count by the number of allocated clusters that\n\t\t\t * have previously been delayed allocated.  Quota\n\t\t\t * has been claimed by ext4_mb_new_blocks() above,\n\t\t\t * so release the quota reservations made for any\n\t\t\t * previously delayed allocated clusters.\n\t\t\t */\n\t\t\tlblk = EXT4_LBLK_CMASK(sbi, map->m_lblk);\n\t\t\tlen = allocated_clusters << sbi->s_cluster_bits;\n\t\t\tn = ext4_es_delayed_clu(inode, lblk, len);\n\t\t\tif (n > 0)\n\t\t\t\text4_da_update_reserve_space(inode, (int) n, 0);\n\t\t}\n\t}\n\n\t/*\n\t * Cache the extent and update transaction to commit on fdatasync only\n\t * when it is _not_ an unwritten extent.\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_UNWRIT_EXT) == 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\telse\n\t\text4_update_inode_fsync_trans(handle, inode, 0);\nout:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\n\ttrace_ext4_ext_map_blocks_exit(inode, flags, map,\n\t\t\t\t       err ? err : allocated);\n\treturn err ? err : allocated;\n}\n\nint ext4_ext_truncate(handle_t *handle, struct inode *inode)\n{\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t last_block;\n\tint err = 0;\n\n\t/*\n\t * TODO: optimization is possible here.\n\t * Probably we need not scan at all,\n\t * because page truncation is enough.\n\t */\n\n\t/* we have to know where to truncate from in crash case */\n\tEXT4_I(inode)->i_disksize = inode->i_size;\n\terr = ext4_mark_inode_dirty(handle, inode);\n\tif (err)\n\t\treturn err;\n\n\tlast_block = (inode->i_size + sb->s_blocksize - 1)\n\t\t\t>> EXT4_BLOCK_SIZE_BITS(sb);\nretry:\n\terr = ext4_es_remove_extent(inode, last_block,\n\t\t\t\t    EXT_MAX_BLOCKS - last_block);\n\tif (err == -ENOMEM) {\n\t\tcond_resched();\n\t\tcongestion_wait(BLK_RW_ASYNC, HZ/50);\n\t\tgoto retry;\n\t}\n\tif (err)\n\t\treturn err;\n\treturn ext4_ext_remove_space(inode, last_block, EXT_MAX_BLOCKS - 1);\n}\n\nstatic int ext4_alloc_file_blocks(struct file *file, ext4_lblk_t offset,\n\t\t\t\t  ext4_lblk_t len, loff_t new_size,\n\t\t\t\t  int flags)\n{\n\tstruct inode *inode = file_inode(file);\n\thandle_t *handle;\n\tint ret = 0;\n\tint ret2 = 0;\n\tint retries = 0;\n\tint depth = 0;\n\tstruct ext4_map_blocks map;\n\tunsigned int credits;\n\tloff_t epos;\n\n\tBUG_ON(!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS));\n\tmap.m_lblk = offset;\n\tmap.m_len = len;\n\t/*\n\t * Don't normalize the request if it can fit in one extent so\n\t * that it doesn't get unnecessarily split into multiple\n\t * extents.\n\t */\n\tif (len <= EXT_UNWRITTEN_MAX_LEN)\n\t\tflags |= EXT4_GET_BLOCKS_NO_NORMALIZE;\n\n\t/*\n\t * credits to insert 1 extent into extent tree\n\t */\n\tcredits = ext4_chunk_trans_blocks(inode, len);\n\tdepth = ext_depth(inode);\n\nretry:\n\twhile (ret >= 0 && len) {\n\t\t/*\n\t\t * Recalculate credits when extent tree depth changes.\n\t\t */\n\t\tif (depth != ext_depth(inode)) {\n\t\t\tcredits = ext4_chunk_trans_blocks(inode, len);\n\t\t\tdepth = ext_depth(inode);\n\t\t}\n\n\t\thandle = ext4_journal_start(inode, EXT4_HT_MAP_BLOCKS,\n\t\t\t\t\t    credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tbreak;\n\t\t}\n\t\tret = ext4_map_blocks(handle, inode, &map, flags);\n\t\tif (ret <= 0) {\n\t\t\text4_debug(\"inode #%lu: block %u: len %u: \"\n\t\t\t\t   \"ext4_ext_map_blocks returned %d\",\n\t\t\t\t   inode->i_ino, map.m_lblk,\n\t\t\t\t   map.m_len, ret);\n\t\t\text4_mark_inode_dirty(handle, inode);\n\t\t\tret2 = ext4_journal_stop(handle);\n\t\t\tbreak;\n\t\t}\n\t\tmap.m_lblk += ret;\n\t\tmap.m_len = len = len - ret;\n\t\tepos = (loff_t)map.m_lblk << inode->i_blkbits;\n\t\tinode->i_ctime = current_time(inode);\n\t\tif (new_size) {\n\t\t\tif (epos > new_size)\n\t\t\t\tepos = new_size;\n\t\t\tif (ext4_update_inode_size(inode, epos) & 0x1)\n\t\t\t\tinode->i_mtime = inode->i_ctime;\n\t\t} else {\n\t\t\tif (epos > inode->i_size)\n\t\t\t\text4_set_inode_flag(inode,\n\t\t\t\t\t\t    EXT4_INODE_EOFBLOCKS);\n\t\t}\n\t\text4_mark_inode_dirty(handle, inode);\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret2)\n\t\t\tbreak;\n\t}\n\tif (ret == -ENOSPC &&\n\t\t\text4_should_retry_alloc(inode->i_sb, &retries)) {\n\t\tret = 0;\n\t\tgoto retry;\n\t}\n\n\treturn ret > 0 ? ret2 : ret;\n}\n\nstatic long ext4_zero_range(struct file *file, loff_t offset,\n\t\t\t    loff_t len, int mode)\n{\n\tstruct inode *inode = file_inode(file);\n\thandle_t *handle = NULL;\n\tunsigned int max_blocks;\n\tloff_t new_size = 0;\n\tint ret = 0;\n\tint flags;\n\tint credits;\n\tint partial_begin, partial_end;\n\tloff_t start, end;\n\text4_lblk_t lblk;\n\tunsigned int blkbits = inode->i_blkbits;\n\n\ttrace_ext4_zero_range(inode, offset, len, mode);\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EINVAL;\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal. */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Round up offset. This is not fallocate, we neet to zero out\n\t * blocks, so convert interior block aligned part of the range to\n\t * unwritten and possibly manually zero out unaligned parts of the\n\t * range.\n\t */\n\tstart = round_up(offset, 1 << blkbits);\n\tend = round_down((offset + len), 1 << blkbits);\n\n\tif (start < offset || end > offset + len)\n\t\treturn -EINVAL;\n\tpartial_begin = offset & ((1 << blkbits) - 1);\n\tpartial_end = (offset + len) & ((1 << blkbits) - 1);\n\n\tlblk = start >> blkbits;\n\tmax_blocks = (end >> blkbits);\n\tif (max_blocks < lblk)\n\t\tmax_blocks = 0;\n\telse\n\t\tmax_blocks -= lblk;\n\n\tinode_lock(inode);\n\n\t/*\n\t * Indirect files do not support unwritten extnets\n\t */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\tif (!(mode & FALLOC_FL_KEEP_SIZE) &&\n\t    (offset + len > i_size_read(inode) ||\n\t     offset + len > EXT4_I(inode)->i_disksize)) {\n\t\tnew_size = offset + len;\n\t\tret = inode_newsize_ok(inode, new_size);\n\t\tif (ret)\n\t\t\tgoto out_mutex;\n\t}\n\n\tflags = EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT;\n\tif (mode & FALLOC_FL_KEEP_SIZE)\n\t\tflags |= EXT4_GET_BLOCKS_KEEP_SIZE;\n\n\t/* Wait all existing dio workers, newcomers will block on i_mutex */\n\tinode_dio_wait(inode);\n\n\t/* Preallocate the range including the unaligned edges */\n\tif (partial_begin || partial_end) {\n\t\tret = ext4_alloc_file_blocks(file,\n\t\t\t\tround_down(offset, 1 << blkbits) >> blkbits,\n\t\t\t\t(round_up((offset + len), 1 << blkbits) -\n\t\t\t\t round_down(offset, 1 << blkbits)) >> blkbits,\n\t\t\t\tnew_size, flags);\n\t\tif (ret)\n\t\t\tgoto out_mutex;\n\n\t}\n\n\t/* Zero range excluding the unaligned edges */\n\tif (max_blocks > 0) {\n\t\tflags |= (EXT4_GET_BLOCKS_CONVERT_UNWRITTEN |\n\t\t\t  EXT4_EX_NOCACHE);\n\n\t\t/*\n\t\t * Prevent page faults from reinstantiating pages we have\n\t\t * released from page cache.\n\t\t */\n\t\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\n\t\tret = ext4_break_layouts(inode);\n\t\tif (ret) {\n\t\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\t\t\tgoto out_mutex;\n\t\t}\n\n\t\tret = ext4_update_disksize_before_punch(inode, offset, len);\n\t\tif (ret) {\n\t\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\t\t\tgoto out_mutex;\n\t\t}\n\t\t/* Now release the pages and zero block aligned part of pages */\n\t\ttruncate_pagecache_range(inode, start, end - 1);\n\t\tinode->i_mtime = inode->i_ctime = current_time(inode);\n\n\t\tret = ext4_alloc_file_blocks(file, lblk, max_blocks, new_size,\n\t\t\t\t\t     flags);\n\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\t\tif (ret)\n\t\t\tgoto out_mutex;\n\t}\n\tif (!partial_begin && !partial_end)\n\t\tgoto out_mutex;\n\n\t/*\n\t * In worst case we have to writeout two nonadjacent unwritten\n\t * blocks and update the inode\n\t */\n\tcredits = (2 * ext4_ext_index_trans_blocks(inode, 2)) + 1;\n\tif (ext4_should_journal_data(inode))\n\t\tcredits += 2;\n\thandle = ext4_journal_start(inode, EXT4_HT_MISC, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\text4_std_error(inode->i_sb, ret);\n\t\tgoto out_mutex;\n\t}\n\n\tinode->i_mtime = inode->i_ctime = current_time(inode);\n\tif (new_size) {\n\t\text4_update_inode_size(inode, new_size);\n\t} else {\n\t\t/*\n\t\t* Mark that we allocate beyond EOF so the subsequent truncate\n\t\t* can proceed even if the new size is the same as i_size.\n\t\t*/\n\t\tif ((offset + len) > i_size_read(inode))\n\t\t\text4_set_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\t}\n\text4_mark_inode_dirty(handle, inode);\n\n\t/* Zero out partial block at the edges of the range */\n\tret = ext4_zero_partial_blocks(handle, inode, offset, len);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\n\tif (file->f_flags & O_SYNC)\n\t\text4_handle_sync(handle);\n\n\text4_journal_stop(handle);\nout_mutex:\n\tinode_unlock(inode);\n\treturn ret;\n}\n\n/*\n * preallocate space for a file. This implements ext4's fallocate file\n * operation, which gets called from sys_fallocate system call.\n * For block-mapped files, posix_fallocate should fall back to the method\n * of writing zeroes to the required new blocks (the same behavior which is\n * expected for file systems which do not support fallocate() system call).\n */\nlong ext4_fallocate(struct file *file, int mode, loff_t offset, loff_t len)\n{\n\tstruct inode *inode = file_inode(file);\n\tloff_t new_size = 0;\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint flags;\n\text4_lblk_t lblk;\n\tunsigned int blkbits = inode->i_blkbits;\n\n\t/*\n\t * Encrypted inodes can't handle collapse range or insert\n\t * range since we would need to re-encrypt blocks with a\n\t * different IV or XTS tweak (which are based on the logical\n\t * block number).\n\t *\n\t * XXX It's not clear why zero range isn't working, but we'll\n\t * leave it disabled for encrypted inodes for now.  This is a\n\t * bug we should fix....\n\t */\n\tif (IS_ENCRYPTED(inode) &&\n\t    (mode & (FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_INSERT_RANGE |\n\t\t     FALLOC_FL_ZERO_RANGE)))\n\t\treturn -EOPNOTSUPP;\n\n\t/* Return error if mode is not supported */\n\tif (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE |\n\t\t     FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_ZERO_RANGE |\n\t\t     FALLOC_FL_INSERT_RANGE))\n\t\treturn -EOPNOTSUPP;\n\n\tif (mode & FALLOC_FL_PUNCH_HOLE)\n\t\treturn ext4_punch_hole(inode, offset, len);\n\n\tret = ext4_convert_inline_data(inode);\n\tif (ret)\n\t\treturn ret;\n\n\tif (mode & FALLOC_FL_COLLAPSE_RANGE)\n\t\treturn ext4_collapse_range(inode, offset, len);\n\n\tif (mode & FALLOC_FL_INSERT_RANGE)\n\t\treturn ext4_insert_range(inode, offset, len);\n\n\tif (mode & FALLOC_FL_ZERO_RANGE)\n\t\treturn ext4_zero_range(file, offset, len, mode);\n\n\ttrace_ext4_fallocate_enter(inode, offset, len, mode);\n\tlblk = offset >> blkbits;\n\n\tmax_blocks = EXT4_MAX_BLOCKS(len, offset, blkbits);\n\tflags = EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT;\n\tif (mode & FALLOC_FL_KEEP_SIZE)\n\t\tflags |= EXT4_GET_BLOCKS_KEEP_SIZE;\n\n\tinode_lock(inode);\n\n\t/*\n\t * We only support preallocation for extent-based files only\n\t */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (!(mode & FALLOC_FL_KEEP_SIZE) &&\n\t    (offset + len > i_size_read(inode) ||\n\t     offset + len > EXT4_I(inode)->i_disksize)) {\n\t\tnew_size = offset + len;\n\t\tret = inode_newsize_ok(inode, new_size);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\t/* Wait all existing dio workers, newcomers will block on i_mutex */\n\tinode_dio_wait(inode);\n\n\tret = ext4_alloc_file_blocks(file, lblk, max_blocks, new_size, flags);\n\tif (ret)\n\t\tgoto out;\n\n\tif (file->f_flags & O_SYNC && EXT4_SB(inode->i_sb)->s_journal) {\n\t\tret = jbd2_complete_transaction(EXT4_SB(inode->i_sb)->s_journal,\n\t\t\t\t\t\tEXT4_I(inode)->i_sync_tid);\n\t}\nout:\n\tinode_unlock(inode);\n\ttrace_ext4_fallocate_exit(inode, offset, max_blocks, ret);\n\treturn ret;\n}\n\n/*\n * This function convert a range of blocks to written extents\n * The caller of this function will pass the start offset and the size.\n * all unwritten extents within this range will be converted to\n * written extents.\n *\n * This function is called from the direct IO end io call back\n * function, to convert the fallocated extents after IO is completed.\n * Returns 0 on success.\n */\nint ext4_convert_unwritten_extents(handle_t *handle, struct inode *inode,\n\t\t\t\t   loff_t offset, ssize_t len)\n{\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint ret2 = 0;\n\tstruct ext4_map_blocks map;\n\tunsigned int credits, blkbits = inode->i_blkbits;\n\n\tmap.m_lblk = offset >> blkbits;\n\tmax_blocks = EXT4_MAX_BLOCKS(len, offset, blkbits);\n\n\t/*\n\t * This is somewhat ugly but the idea is clear: When transaction is\n\t * reserved, everything goes into it. Otherwise we rather start several\n\t * smaller transactions for conversion of each extent separately.\n\t */\n\tif (handle) {\n\t\thandle = ext4_journal_start_reserved(handle,\n\t\t\t\t\t\t     EXT4_HT_EXT_CONVERT);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tcredits = 0;\n\t} else {\n\t\t/*\n\t\t * credits to insert 1 extent into extent tree\n\t\t */\n\t\tcredits = ext4_chunk_trans_blocks(inode, max_blocks);\n\t}\n\twhile (ret >= 0 && ret < max_blocks) {\n\t\tmap.m_lblk += ret;\n\t\tmap.m_len = (max_blocks -= ret);\n\t\tif (credits) {\n\t\t\thandle = ext4_journal_start(inode, EXT4_HT_MAP_BLOCKS,\n\t\t\t\t\t\t    credits);\n\t\t\tif (IS_ERR(handle)) {\n\t\t\t\tret = PTR_ERR(handle);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tret = ext4_map_blocks(handle, inode, &map,\n\t\t\t\t      EXT4_GET_BLOCKS_IO_CONVERT_EXT);\n\t\tif (ret <= 0)\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"inode #%lu: block %u: len %u: \"\n\t\t\t\t     \"ext4_ext_map_blocks returned %d\",\n\t\t\t\t     inode->i_ino, map.m_lblk,\n\t\t\t\t     map.m_len, ret);\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tif (credits)\n\t\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret <= 0 || ret2)\n\t\t\tbreak;\n\t}\n\tif (!credits)\n\t\tret2 = ext4_journal_stop(handle);\n\treturn ret > 0 ? ret2 : ret;\n}\n\n/*\n * If newes is not existing extent (newes->ec_pblk equals zero) find\n * delayed extent at start of newes and update newes accordingly and\n * return start of the next delayed extent.\n *\n * If newes is existing extent (newes->ec_pblk is not equal zero)\n * return start of next delayed extent or EXT_MAX_BLOCKS if no delayed\n * extent found. Leave newes unmodified.\n */\nstatic int ext4_find_delayed_extent(struct inode *inode,\n\t\t\t\t    struct extent_status *newes)\n{\n\tstruct extent_status es;\n\text4_lblk_t block, next_del;\n\n\tif (newes->es_pblk == 0) {\n\t\text4_es_find_extent_range(inode, &ext4_es_is_delayed,\n\t\t\t\t\t  newes->es_lblk,\n\t\t\t\t\t  newes->es_lblk + newes->es_len - 1,\n\t\t\t\t\t  &es);\n\n\t\t/*\n\t\t * No extent in extent-tree contains block @newes->es_pblk,\n\t\t * then the block may stay in 1)a hole or 2)delayed-extent.\n\t\t */\n\t\tif (es.es_len == 0)\n\t\t\t/* A hole found. */\n\t\t\treturn 0;\n\n\t\tif (es.es_lblk > newes->es_lblk) {\n\t\t\t/* A hole found. */\n\t\t\tnewes->es_len = min(es.es_lblk - newes->es_lblk,\n\t\t\t\t\t    newes->es_len);\n\t\t\treturn 0;\n\t\t}\n\n\t\tnewes->es_len = es.es_lblk + es.es_len - newes->es_lblk;\n\t}\n\n\tblock = newes->es_lblk + newes->es_len;\n\text4_es_find_extent_range(inode, &ext4_es_is_delayed, block,\n\t\t\t\t  EXT_MAX_BLOCKS, &es);\n\tif (es.es_len == 0)\n\t\tnext_del = EXT_MAX_BLOCKS;\n\telse\n\t\tnext_del = es.es_lblk;\n\n\treturn next_del;\n}\n/* fiemap flags we can handle specified here */\n#define EXT4_FIEMAP_FLAGS\t(FIEMAP_FLAG_SYNC|FIEMAP_FLAG_XATTR)\n\nstatic int ext4_xattr_fiemap(struct inode *inode,\n\t\t\t\tstruct fiemap_extent_info *fieinfo)\n{\n\t__u64 physical = 0;\n\t__u64 length;\n\t__u32 flags = FIEMAP_EXTENT_LAST;\n\tint blockbits = inode->i_sb->s_blocksize_bits;\n\tint error = 0;\n\n\t/* in-inode? */\n\tif (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {\n\t\tstruct ext4_iloc iloc;\n\t\tint offset;\t/* offset of xattr in inode */\n\n\t\terror = ext4_get_inode_loc(inode, &iloc);\n\t\tif (error)\n\t\t\treturn error;\n\t\tphysical = (__u64)iloc.bh->b_blocknr << blockbits;\n\t\toffset = EXT4_GOOD_OLD_INODE_SIZE +\n\t\t\t\tEXT4_I(inode)->i_extra_isize;\n\t\tphysical += offset;\n\t\tlength = EXT4_SB(inode->i_sb)->s_inode_size - offset;\n\t\tflags |= FIEMAP_EXTENT_DATA_INLINE;\n\t\tbrelse(iloc.bh);\n\t} else { /* external block */\n\t\tphysical = (__u64)EXT4_I(inode)->i_file_acl << blockbits;\n\t\tlength = inode->i_sb->s_blocksize;\n\t}\n\n\tif (physical)\n\t\terror = fiemap_fill_next_extent(fieinfo, 0, physical,\n\t\t\t\t\t\tlength, flags);\n\treturn (error < 0 ? error : 0);\n}\n\nint ext4_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t__u64 start, __u64 len)\n{\n\text4_lblk_t start_blk;\n\tint error = 0;\n\n\tif (ext4_has_inline_data(inode)) {\n\t\tint has_inline = 1;\n\n\t\terror = ext4_inline_data_fiemap(inode, fieinfo, &has_inline,\n\t\t\t\t\t\tstart, len);\n\n\t\tif (has_inline)\n\t\t\treturn error;\n\t}\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_CACHE) {\n\t\terror = ext4_ext_precache(inode);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\t/* fallback to generic here if not in extents fmt */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))\n\t\treturn generic_block_fiemap(inode, fieinfo, start, len,\n\t\t\text4_get_block);\n\n\tif (fiemap_check_flags(fieinfo, EXT4_FIEMAP_FLAGS))\n\t\treturn -EBADR;\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_XATTR) {\n\t\terror = ext4_xattr_fiemap(inode, fieinfo);\n\t} else {\n\t\text4_lblk_t len_blks;\n\t\t__u64 last_blk;\n\n\t\tstart_blk = start >> inode->i_sb->s_blocksize_bits;\n\t\tlast_blk = (start + len - 1) >> inode->i_sb->s_blocksize_bits;\n\t\tif (last_blk >= EXT_MAX_BLOCKS)\n\t\t\tlast_blk = EXT_MAX_BLOCKS-1;\n\t\tlen_blks = ((ext4_lblk_t) last_blk) - start_blk + 1;\n\n\t\t/*\n\t\t * Walk the extent tree gathering extent information\n\t\t * and pushing extents back to the user.\n\t\t */\n\t\terror = ext4_fill_fiemap_extents(inode, start_blk,\n\t\t\t\t\t\t len_blks, fieinfo);\n\t}\n\treturn error;\n}\n\n/*\n * ext4_access_path:\n * Function to access the path buffer for marking it dirty.\n * It also checks if there are sufficient credits left in the journal handle\n * to update path.\n */\nstatic int\next4_access_path(handle_t *handle, struct inode *inode,\n\t\tstruct ext4_ext_path *path)\n{\n\tint credits, err;\n\n\tif (!ext4_handle_valid(handle))\n\t\treturn 0;\n\n\t/*\n\t * Check if need to extend journal credits\n\t * 3 for leaf, sb, and inode plus 2 (bmap and group\n\t * descriptor) for each block group; assume two block\n\t * groups\n\t */\n\tif (handle->h_buffer_credits < 7) {\n\t\tcredits = ext4_writepage_trans_blocks(inode);\n\t\terr = ext4_ext_truncate_extend_restart(handle, inode, credits);\n\t\t/* EAGAIN is success */\n\t\tif (err && err != -EAGAIN)\n\t\t\treturn err;\n\t}\n\n\terr = ext4_ext_get_access(handle, inode, path);\n\treturn err;\n}\n\n/*\n * ext4_ext_shift_path_extents:\n * Shift the extents of a path structure lying between path[depth].p_ext\n * and EXT_LAST_EXTENT(path[depth].p_hdr), by @shift blocks. @SHIFT tells\n * if it is right shift or left shift operation.\n */\nstatic int\next4_ext_shift_path_extents(struct ext4_ext_path *path, ext4_lblk_t shift,\n\t\t\t    struct inode *inode, handle_t *handle,\n\t\t\t    enum SHIFT_DIRECTION SHIFT)\n{\n\tint depth, err = 0;\n\tstruct ext4_extent *ex_start, *ex_last;\n\tbool update = 0;\n\tdepth = path->p_depth;\n\n\twhile (depth >= 0) {\n\t\tif (depth == path->p_depth) {\n\t\t\tex_start = path[depth].p_ext;\n\t\t\tif (!ex_start)\n\t\t\t\treturn -EFSCORRUPTED;\n\n\t\t\tex_last = EXT_LAST_EXTENT(path[depth].p_hdr);\n\n\t\t\terr = ext4_access_path(handle, inode, path + depth);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\tif (ex_start == EXT_FIRST_EXTENT(path[depth].p_hdr))\n\t\t\t\tupdate = 1;\n\n\t\t\twhile (ex_start <= ex_last) {\n\t\t\t\tif (SHIFT == SHIFT_LEFT) {\n\t\t\t\t\tle32_add_cpu(&ex_start->ee_block,\n\t\t\t\t\t\t-shift);\n\t\t\t\t\t/* Try to merge to the left. */\n\t\t\t\t\tif ((ex_start >\n\t\t\t\t\t    EXT_FIRST_EXTENT(path[depth].p_hdr))\n\t\t\t\t\t    &&\n\t\t\t\t\t    ext4_ext_try_to_merge_right(inode,\n\t\t\t\t\t    path, ex_start - 1))\n\t\t\t\t\t\tex_last--;\n\t\t\t\t\telse\n\t\t\t\t\t\tex_start++;\n\t\t\t\t} else {\n\t\t\t\t\tle32_add_cpu(&ex_last->ee_block, shift);\n\t\t\t\t\text4_ext_try_to_merge_right(inode, path,\n\t\t\t\t\t\tex_last);\n\t\t\t\t\tex_last--;\n\t\t\t\t}\n\t\t\t}\n\t\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\tif (--depth < 0 || !update)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t/* Update index too */\n\t\terr = ext4_access_path(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (SHIFT == SHIFT_LEFT)\n\t\t\tle32_add_cpu(&path[depth].p_idx->ei_block, -shift);\n\t\telse\n\t\t\tle32_add_cpu(&path[depth].p_idx->ei_block, shift);\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* we are done if current index is not a starting index */\n\t\tif (path[depth].p_idx != EXT_FIRST_INDEX(path[depth].p_hdr))\n\t\t\tbreak;\n\n\t\tdepth--;\n\t}\n\nout:\n\treturn err;\n}\n\n/*\n * ext4_ext_shift_extents:\n * All the extents which lies in the range from @start to the last allocated\n * block for the @inode are shifted either towards left or right (depending\n * upon @SHIFT) by @shift blocks.\n * On success, 0 is returned, error otherwise.\n */\nstatic int\next4_ext_shift_extents(struct inode *inode, handle_t *handle,\n\t\t       ext4_lblk_t start, ext4_lblk_t shift,\n\t\t       enum SHIFT_DIRECTION SHIFT)\n{\n\tstruct ext4_ext_path *path;\n\tint ret = 0, depth;\n\tstruct ext4_extent *extent;\n\text4_lblk_t stop, *iterator, ex_start, ex_end;\n\n\t/* Let path point to the last extent */\n\tpath = ext4_find_extent(inode, EXT_MAX_BLOCKS - 1, NULL,\n\t\t\t\tEXT4_EX_NOCACHE);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\n\tdepth = path->p_depth;\n\textent = path[depth].p_ext;\n\tif (!extent)\n\t\tgoto out;\n\n\tstop = le32_to_cpu(extent->ee_block);\n\n       /*\n\t* For left shifts, make sure the hole on the left is big enough to\n\t* accommodate the shift.  For right shifts, make sure the last extent\n\t* won't be shifted beyond EXT_MAX_BLOCKS.\n\t*/\n\tif (SHIFT == SHIFT_LEFT) {\n\t\tpath = ext4_find_extent(inode, start - 1, &path,\n\t\t\t\t\tEXT4_EX_NOCACHE);\n\t\tif (IS_ERR(path))\n\t\t\treturn PTR_ERR(path);\n\t\tdepth = path->p_depth;\n\t\textent =  path[depth].p_ext;\n\t\tif (extent) {\n\t\t\tex_start = le32_to_cpu(extent->ee_block);\n\t\t\tex_end = le32_to_cpu(extent->ee_block) +\n\t\t\t\text4_ext_get_actual_len(extent);\n\t\t} else {\n\t\t\tex_start = 0;\n\t\t\tex_end = 0;\n\t\t}\n\n\t\tif ((start == ex_start && shift > ex_start) ||\n\t\t    (shift > start - ex_end)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tif (shift > EXT_MAX_BLOCKS -\n\t\t    (stop + ext4_ext_get_actual_len(extent))) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * In case of left shift, iterator points to start and it is increased\n\t * till we reach stop. In case of right shift, iterator points to stop\n\t * and it is decreased till we reach start.\n\t */\n\tif (SHIFT == SHIFT_LEFT)\n\t\titerator = &start;\n\telse\n\t\titerator = &stop;\n\n\t/*\n\t * Its safe to start updating extents.  Start and stop are unsigned, so\n\t * in case of right shift if extent with 0 block is reached, iterator\n\t * becomes NULL to indicate the end of the loop.\n\t */\n\twhile (iterator && start <= stop) {\n\t\tpath = ext4_find_extent(inode, *iterator, &path,\n\t\t\t\t\tEXT4_EX_NOCACHE);\n\t\tif (IS_ERR(path))\n\t\t\treturn PTR_ERR(path);\n\t\tdepth = path->p_depth;\n\t\textent = path[depth].p_ext;\n\t\tif (!extent) {\n\t\t\tEXT4_ERROR_INODE(inode, \"unexpected hole at %lu\",\n\t\t\t\t\t (unsigned long) *iterator);\n\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t\tif (SHIFT == SHIFT_LEFT && *iterator >\n\t\t    le32_to_cpu(extent->ee_block)) {\n\t\t\t/* Hole, move to the next extent */\n\t\t\tif (extent < EXT_LAST_EXTENT(path[depth].p_hdr)) {\n\t\t\t\tpath[depth].p_ext++;\n\t\t\t} else {\n\t\t\t\t*iterator = ext4_ext_next_allocated_block(path);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (SHIFT == SHIFT_LEFT) {\n\t\t\textent = EXT_LAST_EXTENT(path[depth].p_hdr);\n\t\t\t*iterator = le32_to_cpu(extent->ee_block) +\n\t\t\t\t\text4_ext_get_actual_len(extent);\n\t\t} else {\n\t\t\textent = EXT_FIRST_EXTENT(path[depth].p_hdr);\n\t\t\tif (le32_to_cpu(extent->ee_block) > 0)\n\t\t\t\t*iterator = le32_to_cpu(extent->ee_block) - 1;\n\t\t\telse\n\t\t\t\t/* Beginning is reached, end of the loop */\n\t\t\t\titerator = NULL;\n\t\t\t/* Update path extent in case we need to stop */\n\t\t\twhile (le32_to_cpu(extent->ee_block) < start)\n\t\t\t\textent++;\n\t\t\tpath[depth].p_ext = extent;\n\t\t}\n\t\tret = ext4_ext_shift_path_extents(path, shift, inode,\n\t\t\t\thandle, SHIFT);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\nout:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\treturn ret;\n}\n\n/*\n * ext4_collapse_range:\n * This implements the fallocate's collapse range functionality for ext4\n * Returns: 0 and non-zero on error.\n */\nint ext4_collapse_range(struct inode *inode, loff_t offset, loff_t len)\n{\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t punch_start, punch_stop;\n\thandle_t *handle;\n\tunsigned int credits;\n\tloff_t new_size, ioffset;\n\tint ret;\n\n\t/*\n\t * We need to test this early because xfstests assumes that a\n\t * collapse range of (0, 1) will return EOPNOTSUPP if the file\n\t * system does not support collapse range.\n\t */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\treturn -EOPNOTSUPP;\n\n\t/* Collapse range works only on fs block size aligned offsets. */\n\tif (offset & (EXT4_CLUSTER_SIZE(sb) - 1) ||\n\t    len & (EXT4_CLUSTER_SIZE(sb) - 1))\n\t\treturn -EINVAL;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EINVAL;\n\n\ttrace_ext4_collapse_range(inode, offset, len);\n\n\tpunch_start = offset >> EXT4_BLOCK_SIZE_BITS(sb);\n\tpunch_stop = (offset + len) >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal. */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tinode_lock(inode);\n\t/*\n\t * There is no need to overlap collapse range with EOF, in which case\n\t * it is effectively a truncate operation\n\t */\n\tif (offset + len >= i_size_read(inode)) {\n\t\tret = -EINVAL;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Currently just for extent based files */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Wait for existing dio to complete */\n\tinode_dio_wait(inode);\n\n\t/*\n\t * Prevent page faults from reinstantiating pages we have released from\n\t * page cache.\n\t */\n\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\n\tret = ext4_break_layouts(inode);\n\tif (ret)\n\t\tgoto out_mmap;\n\n\t/*\n\t * Need to round down offset to be aligned with page size boundary\n\t * for page size > block size.\n\t */\n\tioffset = round_down(offset, PAGE_SIZE);\n\t/*\n\t * Write tail of the last page before removed range since it will get\n\t * removed from the page cache below.\n\t */\n\tret = filemap_write_and_wait_range(inode->i_mapping, ioffset, offset);\n\tif (ret)\n\t\tgoto out_mmap;\n\t/*\n\t * Write data that will be shifted to preserve them when discarding\n\t * page cache below. We are also protected from pages becoming dirty\n\t * by i_mmap_sem.\n\t */\n\tret = filemap_write_and_wait_range(inode->i_mapping, offset + len,\n\t\t\t\t\t   LLONG_MAX);\n\tif (ret)\n\t\tgoto out_mmap;\n\ttruncate_pagecache(inode, ioffset);\n\n\tcredits = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out_mmap;\n\t}\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_discard_preallocations(inode);\n\n\tret = ext4_es_remove_extent(inode, punch_start,\n\t\t\t\t    EXT_MAX_BLOCKS - punch_start);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tret = ext4_ext_remove_space(inode, punch_start, punch_stop - 1);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\text4_discard_preallocations(inode);\n\n\tret = ext4_ext_shift_extents(inode, handle, punch_stop,\n\t\t\t\t     punch_stop - punch_start, SHIFT_LEFT);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tnew_size = i_size_read(inode) - len;\n\ti_size_write(inode, new_size);\n\tEXT4_I(inode)->i_disksize = new_size;\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\tinode->i_mtime = inode->i_ctime = current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\text4_update_inode_fsync_trans(handle, inode, 1);\n\nout_stop:\n\text4_journal_stop(handle);\nout_mmap:\n\tup_write(&EXT4_I(inode)->i_mmap_sem);\nout_mutex:\n\tinode_unlock(inode);\n\treturn ret;\n}\n\n/*\n * ext4_insert_range:\n * This function implements the FALLOC_FL_INSERT_RANGE flag of fallocate.\n * The data blocks starting from @offset to the EOF are shifted by @len\n * towards right to create a hole in the @inode. Inode size is increased\n * by len bytes.\n * Returns 0 on success, error otherwise.\n */\nint ext4_insert_range(struct inode *inode, loff_t offset, loff_t len)\n{\n\tstruct super_block *sb = inode->i_sb;\n\thandle_t *handle;\n\tstruct ext4_ext_path *path;\n\tstruct ext4_extent *extent;\n\text4_lblk_t offset_lblk, len_lblk, ee_start_lblk = 0;\n\tunsigned int credits, ee_len;\n\tint ret = 0, depth, split_flag = 0;\n\tloff_t ioffset;\n\n\t/*\n\t * We need to test this early because xfstests assumes that an\n\t * insert range of (0, 1) will return EOPNOTSUPP if the file\n\t * system does not support insert range.\n\t */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\treturn -EOPNOTSUPP;\n\n\t/* Insert range works only on fs block size aligned offsets. */\n\tif (offset & (EXT4_CLUSTER_SIZE(sb) - 1) ||\n\t\t\tlen & (EXT4_CLUSTER_SIZE(sb) - 1))\n\t\treturn -EINVAL;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EOPNOTSUPP;\n\n\ttrace_ext4_insert_range(inode, offset, len);\n\n\toffset_lblk = offset >> EXT4_BLOCK_SIZE_BITS(sb);\n\tlen_lblk = len >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tinode_lock(inode);\n\t/* Currently just for extent based files */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Check for wrap through zero */\n\tif (inode->i_size + len > inode->i_sb->s_maxbytes) {\n\t\tret = -EFBIG;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Offset should be less than i_size */\n\tif (offset >= i_size_read(inode)) {\n\t\tret = -EINVAL;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Wait for existing dio to complete */\n\tinode_dio_wait(inode);\n\n\t/*\n\t * Prevent page faults from reinstantiating pages we have released from\n\t * page cache.\n\t */\n\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\n\tret = ext4_break_layouts(inode);\n\tif (ret)\n\t\tgoto out_mmap;\n\n\t/*\n\t * Need to round down to align start offset to page size boundary\n\t * for page size > block size.\n\t */\n\tioffset = round_down(offset, PAGE_SIZE);\n\t/* Write out all dirty pages */\n\tret = filemap_write_and_wait_range(inode->i_mapping, ioffset,\n\t\t\tLLONG_MAX);\n\tif (ret)\n\t\tgoto out_mmap;\n\ttruncate_pagecache(inode, ioffset);\n\n\tcredits = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out_mmap;\n\t}\n\n\t/* Expand file to avoid data loss if there is error while shifting */\n\tinode->i_size += len;\n\tEXT4_I(inode)->i_disksize += len;\n\tinode->i_mtime = inode->i_ctime = current_time(inode);\n\tret = ext4_mark_inode_dirty(handle, inode);\n\tif (ret)\n\t\tgoto out_stop;\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_discard_preallocations(inode);\n\n\tpath = ext4_find_extent(inode, offset_lblk, NULL, 0);\n\tif (IS_ERR(path)) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tdepth = ext_depth(inode);\n\textent = path[depth].p_ext;\n\tif (extent) {\n\t\tee_start_lblk = le32_to_cpu(extent->ee_block);\n\t\tee_len = ext4_ext_get_actual_len(extent);\n\n\t\t/*\n\t\t * If offset_lblk is not the starting block of extent, split\n\t\t * the extent @offset_lblk\n\t\t */\n\t\tif ((offset_lblk > ee_start_lblk) &&\n\t\t\t\t(offset_lblk < (ee_start_lblk + ee_len))) {\n\t\t\tif (ext4_ext_is_unwritten(extent))\n\t\t\t\tsplit_flag = EXT4_EXT_MARK_UNWRIT1 |\n\t\t\t\t\tEXT4_EXT_MARK_UNWRIT2;\n\t\t\tret = ext4_split_extent_at(handle, inode, &path,\n\t\t\t\t\toffset_lblk, split_flag,\n\t\t\t\t\tEXT4_EX_NOCACHE |\n\t\t\t\t\tEXT4_GET_BLOCKS_PRE_IO |\n\t\t\t\t\tEXT4_GET_BLOCKS_METADATA_NOFAIL);\n\t\t}\n\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t\tif (ret < 0) {\n\t\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\t\tgoto out_stop;\n\t\t}\n\t} else {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\n\tret = ext4_es_remove_extent(inode, offset_lblk,\n\t\t\tEXT_MAX_BLOCKS - offset_lblk);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\t/*\n\t * if offset_lblk lies in a hole which is at start of file, use\n\t * ee_start_lblk to shift extents\n\t */\n\tret = ext4_ext_shift_extents(inode, handle,\n\t\tee_start_lblk > offset_lblk ? ee_start_lblk : offset_lblk,\n\t\tlen_lblk, SHIFT_RIGHT);\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\nout_stop:\n\text4_journal_stop(handle);\nout_mmap:\n\tup_write(&EXT4_I(inode)->i_mmap_sem);\nout_mutex:\n\tinode_unlock(inode);\n\treturn ret;\n}\n\n/**\n * ext4_swap_extents - Swap extents between two inodes\n *\n * @inode1:\tFirst inode\n * @inode2:\tSecond inode\n * @lblk1:\tStart block for first inode\n * @lblk2:\tStart block for second inode\n * @count:\tNumber of blocks to swap\n * @unwritten: Mark second inode's extents as unwritten after swap\n * @erp:\tPointer to save error value\n *\n * This helper routine does exactly what is promise \"swap extents\". All other\n * stuff such as page-cache locking consistency, bh mapping consistency or\n * extent's data copying must be performed by caller.\n * Locking:\n * \t\ti_mutex is held for both inodes\n * \t\ti_data_sem is locked for write for both inodes\n * Assumptions:\n *\t\tAll pages from requested range are locked for both inodes\n */\nint\next4_swap_extents(handle_t *handle, struct inode *inode1,\n\t\t  struct inode *inode2, ext4_lblk_t lblk1, ext4_lblk_t lblk2,\n\t\t  ext4_lblk_t count, int unwritten, int *erp)\n{\n\tstruct ext4_ext_path *path1 = NULL;\n\tstruct ext4_ext_path *path2 = NULL;\n\tint replaced_count = 0;\n\n\tBUG_ON(!rwsem_is_locked(&EXT4_I(inode1)->i_data_sem));\n\tBUG_ON(!rwsem_is_locked(&EXT4_I(inode2)->i_data_sem));\n\tBUG_ON(!inode_is_locked(inode1));\n\tBUG_ON(!inode_is_locked(inode2));\n\n\t*erp = ext4_es_remove_extent(inode1, lblk1, count);\n\tif (unlikely(*erp))\n\t\treturn 0;\n\t*erp = ext4_es_remove_extent(inode2, lblk2, count);\n\tif (unlikely(*erp))\n\t\treturn 0;\n\n\twhile (count) {\n\t\tstruct ext4_extent *ex1, *ex2, tmp_ex;\n\t\text4_lblk_t e1_blk, e2_blk;\n\t\tint e1_len, e2_len, len;\n\t\tint split = 0;\n\n\t\tpath1 = ext4_find_extent(inode1, lblk1, NULL, EXT4_EX_NOCACHE);\n\t\tif (IS_ERR(path1)) {\n\t\t\t*erp = PTR_ERR(path1);\n\t\t\tpath1 = NULL;\n\t\tfinish:\n\t\t\tcount = 0;\n\t\t\tgoto repeat;\n\t\t}\n\t\tpath2 = ext4_find_extent(inode2, lblk2, NULL, EXT4_EX_NOCACHE);\n\t\tif (IS_ERR(path2)) {\n\t\t\t*erp = PTR_ERR(path2);\n\t\t\tpath2 = NULL;\n\t\t\tgoto finish;\n\t\t}\n\t\tex1 = path1[path1->p_depth].p_ext;\n\t\tex2 = path2[path2->p_depth].p_ext;\n\t\t/* Do we have somthing to swap ? */\n\t\tif (unlikely(!ex2 || !ex1))\n\t\t\tgoto finish;\n\n\t\te1_blk = le32_to_cpu(ex1->ee_block);\n\t\te2_blk = le32_to_cpu(ex2->ee_block);\n\t\te1_len = ext4_ext_get_actual_len(ex1);\n\t\te2_len = ext4_ext_get_actual_len(ex2);\n\n\t\t/* Hole handling */\n\t\tif (!in_range(lblk1, e1_blk, e1_len) ||\n\t\t    !in_range(lblk2, e2_blk, e2_len)) {\n\t\t\text4_lblk_t next1, next2;\n\n\t\t\t/* if hole after extent, then go to next extent */\n\t\t\tnext1 = ext4_ext_next_allocated_block(path1);\n\t\t\tnext2 = ext4_ext_next_allocated_block(path2);\n\t\t\t/* If hole before extent, then shift to that extent */\n\t\t\tif (e1_blk > lblk1)\n\t\t\t\tnext1 = e1_blk;\n\t\t\tif (e2_blk > lblk2)\n\t\t\t\tnext2 = e2_blk;\n\t\t\t/* Do we have something to swap */\n\t\t\tif (next1 == EXT_MAX_BLOCKS || next2 == EXT_MAX_BLOCKS)\n\t\t\t\tgoto finish;\n\t\t\t/* Move to the rightest boundary */\n\t\t\tlen = next1 - lblk1;\n\t\t\tif (len < next2 - lblk2)\n\t\t\t\tlen = next2 - lblk2;\n\t\t\tif (len > count)\n\t\t\t\tlen = count;\n\t\t\tlblk1 += len;\n\t\t\tlblk2 += len;\n\t\t\tcount -= len;\n\t\t\tgoto repeat;\n\t\t}\n\n\t\t/* Prepare left boundary */\n\t\tif (e1_blk < lblk1) {\n\t\t\tsplit = 1;\n\t\t\t*erp = ext4_force_split_extent_at(handle, inode1,\n\t\t\t\t\t\t&path1, lblk1, 0);\n\t\t\tif (unlikely(*erp))\n\t\t\t\tgoto finish;\n\t\t}\n\t\tif (e2_blk < lblk2) {\n\t\t\tsplit = 1;\n\t\t\t*erp = ext4_force_split_extent_at(handle, inode2,\n\t\t\t\t\t\t&path2,  lblk2, 0);\n\t\t\tif (unlikely(*erp))\n\t\t\t\tgoto finish;\n\t\t}\n\t\t/* ext4_split_extent_at() may result in leaf extent split,\n\t\t * path must to be revalidated. */\n\t\tif (split)\n\t\t\tgoto repeat;\n\n\t\t/* Prepare right boundary */\n\t\tlen = count;\n\t\tif (len > e1_blk + e1_len - lblk1)\n\t\t\tlen = e1_blk + e1_len - lblk1;\n\t\tif (len > e2_blk + e2_len - lblk2)\n\t\t\tlen = e2_blk + e2_len - lblk2;\n\n\t\tif (len != e1_len) {\n\t\t\tsplit = 1;\n\t\t\t*erp = ext4_force_split_extent_at(handle, inode1,\n\t\t\t\t\t\t&path1, lblk1 + len, 0);\n\t\t\tif (unlikely(*erp))\n\t\t\t\tgoto finish;\n\t\t}\n\t\tif (len != e2_len) {\n\t\t\tsplit = 1;\n\t\t\t*erp = ext4_force_split_extent_at(handle, inode2,\n\t\t\t\t\t\t&path2, lblk2 + len, 0);\n\t\t\tif (*erp)\n\t\t\t\tgoto finish;\n\t\t}\n\t\t/* ext4_split_extent_at() may result in leaf extent split,\n\t\t * path must to be revalidated. */\n\t\tif (split)\n\t\t\tgoto repeat;\n\n\t\tBUG_ON(e2_len != e1_len);\n\t\t*erp = ext4_ext_get_access(handle, inode1, path1 + path1->p_depth);\n\t\tif (unlikely(*erp))\n\t\t\tgoto finish;\n\t\t*erp = ext4_ext_get_access(handle, inode2, path2 + path2->p_depth);\n\t\tif (unlikely(*erp))\n\t\t\tgoto finish;\n\n\t\t/* Both extents are fully inside boundaries. Swap it now */\n\t\ttmp_ex = *ex1;\n\t\text4_ext_store_pblock(ex1, ext4_ext_pblock(ex2));\n\t\text4_ext_store_pblock(ex2, ext4_ext_pblock(&tmp_ex));\n\t\tex1->ee_len = cpu_to_le16(e2_len);\n\t\tex2->ee_len = cpu_to_le16(e1_len);\n\t\tif (unwritten)\n\t\t\text4_ext_mark_unwritten(ex2);\n\t\tif (ext4_ext_is_unwritten(&tmp_ex))\n\t\t\text4_ext_mark_unwritten(ex1);\n\n\t\text4_ext_try_to_merge(handle, inode2, path2, ex2);\n\t\text4_ext_try_to_merge(handle, inode1, path1, ex1);\n\t\t*erp = ext4_ext_dirty(handle, inode2, path2 +\n\t\t\t\t      path2->p_depth);\n\t\tif (unlikely(*erp))\n\t\t\tgoto finish;\n\t\t*erp = ext4_ext_dirty(handle, inode1, path1 +\n\t\t\t\t      path1->p_depth);\n\t\t/*\n\t\t * Looks scarry ah..? second inode already points to new blocks,\n\t\t * and it was successfully dirtied. But luckily error may happen\n\t\t * only due to journal error, so full transaction will be\n\t\t * aborted anyway.\n\t\t */\n\t\tif (unlikely(*erp))\n\t\t\tgoto finish;\n\t\tlblk1 += len;\n\t\tlblk2 += len;\n\t\treplaced_count += len;\n\t\tcount -= len;\n\n\trepeat:\n\t\text4_ext_drop_refs(path1);\n\t\tkfree(path1);\n\t\text4_ext_drop_refs(path2);\n\t\tkfree(path2);\n\t\tpath1 = path2 = NULL;\n\t}\n\treturn replaced_count;\n}\n\n/*\n * ext4_clu_mapped - determine whether any block in a logical cluster has\n *                   been mapped to a physical cluster\n *\n * @inode - file containing the logical cluster\n * @lclu - logical cluster of interest\n *\n * Returns 1 if any block in the logical cluster is mapped, signifying\n * that a physical cluster has been allocated for it.  Otherwise,\n * returns 0.  Can also return negative error codes.  Derived from\n * ext4_ext_map_blocks().\n */\nint ext4_clu_mapped(struct inode *inode, ext4_lblk_t lclu)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_ext_path *path;\n\tint depth, mapped = 0, err = 0;\n\tstruct ext4_extent *extent;\n\text4_lblk_t first_lblk, first_lclu, last_lclu;\n\n\t/* search for the extent closest to the first block in the cluster */\n\tpath = ext4_find_extent(inode, EXT4_C2B(sbi, lclu), NULL, 0);\n\tif (IS_ERR(path)) {\n\t\terr = PTR_ERR(path);\n\t\tpath = NULL;\n\t\tgoto out;\n\t}\n\n\tdepth = ext_depth(inode);\n\n\t/*\n\t * A consistent leaf must not be empty.  This situation is possible,\n\t * though, _during_ tree modification, and it's why an assert can't\n\t * be put in ext4_find_extent().\n\t */\n\tif (unlikely(path[depth].p_ext == NULL && depth != 0)) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t    \"bad extent address - lblock: %lu, depth: %d, pblock: %lld\",\n\t\t\t\t (unsigned long) EXT4_C2B(sbi, lclu),\n\t\t\t\t depth, path[depth].p_block);\n\t\terr = -EFSCORRUPTED;\n\t\tgoto out;\n\t}\n\n\textent = path[depth].p_ext;\n\n\t/* can't be mapped if the extent tree is empty */\n\tif (extent == NULL)\n\t\tgoto out;\n\n\tfirst_lblk = le32_to_cpu(extent->ee_block);\n\tfirst_lclu = EXT4_B2C(sbi, first_lblk);\n\n\t/*\n\t * Three possible outcomes at this point - found extent spanning\n\t * the target cluster, to the left of the target cluster, or to the\n\t * right of the target cluster.  The first two cases are handled here.\n\t * The last case indicates the target cluster is not mapped.\n\t */\n\tif (lclu >= first_lclu) {\n\t\tlast_lclu = EXT4_B2C(sbi, first_lblk +\n\t\t\t\t     ext4_ext_get_actual_len(extent) - 1);\n\t\tif (lclu <= last_lclu) {\n\t\t\tmapped = 1;\n\t\t} else {\n\t\t\tfirst_lblk = ext4_ext_next_allocated_block(path);\n\t\t\tfirst_lclu = EXT4_B2C(sbi, first_lblk);\n\t\t\tif (lclu == first_lclu)\n\t\t\t\tmapped = 1;\n\t\t}\n\t}\n\nout:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\n\treturn err ? err : mapped;\n}\n"], "filenames": ["fs/ext4/extents.c"], "buggy_code_start_loc": [1037], "buggy_code_end_loc": [1301], "fixing_code_start_loc": [1038], "fixing_code_end_loc": [1314], "type": "CWE-908", "message": "fs/ext4/extents.c in the Linux kernel through 5.1.2 does not zero out the unused memory region in the extent tree block, which might allow local users to obtain sensitive information by reading uninitialized data in the filesystem.", "other": {"cve": {"id": "CVE-2019-11833", "sourceIdentifier": "cve@mitre.org", "published": "2019-05-15T13:29:00.197", "lastModified": "2023-03-01T15:28:40.643", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "fs/ext4/extents.c in the Linux kernel through 5.1.2 does not zero out the unused memory region in the extent tree block, which might allow local users to obtain sensitive information by reading uninitialized data in the filesystem."}, {"lang": "es", "value": "fs / ext4 / extents.c en el kernel de Linux hasta 5.1.2 no pone a cero la regi\u00f3n de memoria no utilizada en el bloque del \u00e1rbol de extensi\u00f3n, lo que podr\u00eda permitir a los usuarios locales obtener informaci\u00f3n confidencial al leer datos no inicializados en el sistema de archivos."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:N/A:N", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-908"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "5.1.2", "matchCriteriaId": "28B34C9A-7B60-4681-97D4-06BC8751711B"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:29:*:*:*:*:*:*:*", "matchCriteriaId": "D100F7CE-FC64-4CC6-852A-6136D72DA419"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:esm:*:*:*", "matchCriteriaId": "815D70A8-47D3-459C-A32C-9FEACA0659D1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:esm:*:*:*", "matchCriteriaId": "7A5301BF-1402-4BE0-A0F8-69FBE79BC6D6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:lts:*:*:*", "matchCriteriaId": "23A7C53F-B80F-4E6A-AFA9-58EEA84BE11D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:19.04:*:*:*:*:*:*:*", "matchCriteriaId": "CD783B0C-9246-47D9-A937-6144FE8BFF0F"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "F4CFF558-3C47-480D-A2F0-BABF26042943"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_desktop:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "33C068A4-3780-4EAB-A937-6082DF847564"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_eus:8.1:*:*:*:*:*:*:*", "matchCriteriaId": "92BC9265-6959-4D37-BE5E-8C45E98992F8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_eus:8.2:*:*:*:*:*:*:*", "matchCriteriaId": "831F0F47-3565-4763-B16F-C87B1FF2035E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_eus:8.4:*:*:*:*:*:*:*", "matchCriteriaId": "0E3F09B5-569F-4C58-9FCA-3C0953D107B5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_eus:8.6:*:*:*:*:*:*:*", "matchCriteriaId": "6C3741B8-851F-475D-B428-523F4F722350"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time:7:*:*:*:*:*:*:*", "matchCriteriaId": "C2B15608-BABC-4663-A58F-B74BD2D1A734"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "5487EF77-D23A-4CC0-851C-E330B4485D8A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time_for_nfv:7:*:*:*:*:*:*:*", "matchCriteriaId": "36E85B24-30F2-42AB-9F68-8668C0FCC5E3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time_for_nfv:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "782C86CD-1B68-410A-A096-E5170AD24DA2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time_for_nfv_tus:8.2:*:*:*:*:*:*:*", "matchCriteriaId": "77C61DDC-81F3-4E2D-9CAA-17A256C85443"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time_for_nfv_tus:8.4:*:*:*:*:*:*:*", "matchCriteriaId": "B6B0DA79-DF12-4418-B075-F048C9E2979A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time_for_nfv_tus:8.6:*:*:*:*:*:*:*", "matchCriteriaId": "6D5DE3C5-B090-4CE7-9AF2-DEB379D7D5FC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time_tus:8.2:*:*:*:*:*:*:*", "matchCriteriaId": "B92409A9-0D6B-4B7E-8847-1B63837D201F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time_tus:8.4:*:*:*:*:*:*:*", "matchCriteriaId": "C5C5860E-9FEB-4259-92FD-A85911E2F99E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_for_real_time_tus:8.6:*:*:*:*:*:*:*", "matchCriteriaId": "CCE99A08-D6F7-4937-8154-65062BC88009"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "51EF4996-72F4-4FA4-814F-F5991E7A8318"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_aus:8.2:*:*:*:*:*:*:*", "matchCriteriaId": "6897676D-53F9-45B3-B27F-7FF9A4C58D33"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_aus:8.4:*:*:*:*:*:*:*", "matchCriteriaId": "E28F226A-CBC7-4A32-BE58-398FA5B42481"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_aus:8.6:*:*:*:*:*:*:*", "matchCriteriaId": "76C24D94-834A-4E9D-8F73-624AFA99AAA2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_tus:8.2:*:*:*:*:*:*:*", "matchCriteriaId": "B09ACF2D-D83F-4A86-8185-9569605D8EE1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_tus:8.4:*:*:*:*:*:*:*", "matchCriteriaId": "AC10D919-57FD-4725-B8D2-39ECB476902F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_tus:8.6:*:*:*:*:*:*:*", "matchCriteriaId": "1272DF03-7674-4BD4-8E64-94004B195448"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_workstation:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "825ECE2D-E232-46E0-A047-074B34DB1E97"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2019-05/msg00071.html", "source": "cve@mitre.org", "tags": ["Broken Link"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2019-06/msg00039.html", "source": "cve@mitre.org", "tags": ["Broken Link"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2019-06/msg00048.html", "source": "cve@mitre.org", "tags": ["Broken Link"]}, {"url": "http://packetstormsecurity.com/files/154951/Kernel-Live-Patch-Security-Notice-LSN-0058-1.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securityfocus.com/bid/108372", "source": "cve@mitre.org", "tags": ["Broken Link", "Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:2029", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:2043", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:3309", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:3517", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/592acbf16821288ecdc4192c47e3774a4c48bb64", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2019/06/msg00010.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2019/06/msg00011.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/GJGZIMGB72TL7OGWRMHIL43WHXFQWU4X/", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://seclists.org/bugtraq/2019/Jun/26", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4068-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4068-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4069-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4069-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4076-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4095-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4118-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.debian.org/security/2019/dsa-4465", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/592acbf16821288ecdc4192c47e3774a4c48bb64"}}